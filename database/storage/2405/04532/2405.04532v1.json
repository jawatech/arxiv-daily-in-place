{"2405.04532": {"publish_time": "2024-05-07", "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving", "paper_summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.", "paper_summary_zh": "\u91cf\u5316\u53ef\u4ee5\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u63a8\u8bba\u3002\u8d85\u8d8a INT8 \u91cf\u5316\uff0c\u7814\u7a76\u754c\u6b63\u5728\u79ef\u6781\u63a2\u7d22\u66f4\u4f4e\u7cbe\u5ea6\uff0c\u4f8b\u5982 INT4\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u6700\u5148\u8fdb\u7684 INT4 \u91cf\u5316\u6280\u672f\u4ec5\u52a0\u901f\u5c0f\u6279\u91cf\u8fb9\u7f18 LLM \u63a8\u8bba\uff0c\u65e0\u6cd5\u5728\u5927\u6279\u91cf\u57fa\u4e8e\u4e91\u7684 LLM \u670d\u52a1\u4e2d\u63d0\u4f9b\u6027\u80fd\u63d0\u5347\u3002\u6211\u4eec\u53d1\u73b0\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u73b0\u6709\u7684 INT4 \u91cf\u5316\u65b9\u6cd5\u5728 GPU \u4e0a\u5bf9\u6743\u91cd\u6216\u90e8\u5206\u548c\u8fdb\u884c\u53bb\u91cf\u5316\u65f6\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u8fd0\u884c\u65f6\u5f00\u9500 (20-90%)\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 QoQ\uff0c\u4e00\u79cd\u5177\u6709 4 \u4f4d\u6743\u91cd\u30018 \u4f4d\u6fc0\u6d3b\u548c 4 \u4f4d KV \u7f13\u5b58\u7684 W4A8KV4 \u91cf\u5316\u7b97\u6cd5\u3002QoQ \u4ee3\u8868 quattuor-octo-quattuor\uff0c\u5728\u62c9\u4e01\u8bed\u4e2d\u8868\u793a 4-8-4\u3002QoQ \u7531 QServe \u63a8\u8bba\u5e93\u5b9e\u73b0\uff0c\u8be5\u5e93\u5b9e\u73b0\u4e86\u5df2\u6d4b\u91cf\u7684\u52a0\u901f\u3002\u63a8\u52a8 QServe \u7684\u5173\u952e\u89c1\u89e3\u662f\uff0cGPU \u4e0a\u7684 LLM \u670d\u52a1\u7684\u6548\u7387\u53d7\u5230\u4f4e\u541e\u5410\u91cf CUDA \u6838\u5fc3\u4e0a\u7684\u64cd\u4f5c\u7684\u4e25\u91cd\u5f71\u54cd\u3002\u57fa\u4e8e\u6b64\u89c1\u89e3\uff0c\u5728 QoQ \u7b97\u6cd5\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u6e10\u8fdb\u5f0f\u91cf\u5316\uff0c\u53ef\u4ee5\u5728 W4A8 GEMM \u4e2d\u5141\u8bb8\u8f83\u4f4e\u7684\u53bb\u91cf\u5316\u5f00\u9500\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 SmoothAttention \u4ee5\u6709\u6548\u51cf\u8f7b 4 \u4f4d KV \u91cf\u5316\u5e26\u6765\u7684\u7cbe\u5ea6\u4e0b\u964d\u3002\u5728 QServe \u7cfb\u7edf\u4e2d\uff0c\u6211\u4eec\u6267\u884c\u8ba1\u7b97\u611f\u77e5\u6743\u91cd\u91cd\u65b0\u6392\u5e8f\u5e76\u5229\u7528\u5bc4\u5b58\u5668\u7ea7\u5e76\u884c\u6027\u6765\u51cf\u5c11\u53bb\u91cf\u5316\u5ef6\u8fdf\u3002\u6211\u4eec\u8fd8\u4f7f\u878d\u5408\u6ce8\u610f\u5185\u5b58\u53d7\u9650\uff0c\u5229\u7528 KV4 \u91cf\u5316\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u3002\u7ed3\u679c\uff0cQServe \u5c06 Llama-3-8B \u7684\u6700\u5927\u53ef\u5b9e\u73b0\u670d\u52a1\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 1.2 \u500d\uff08A100 \u4e0a\uff09\uff0c1.4 \u500d\uff08L40S \u4e0a\uff09\uff1b\u4e0e TensorRT-LLM \u76f8\u6bd4\uff0cQwen1.5-72B \u5728 A100 \u4e0a\u63d0\u9ad8\u4e86 2.4 \u500d\uff0c\u5728 L40S \u4e0a\u63d0\u9ad8\u4e86 3.5 \u500d\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cL40S GPU \u4e0a\u7684 QServe \u751a\u81f3\u53ef\u4ee5\u5b9e\u73b0\u6bd4 A100 \u4e0a\u7684 TensorRT-LLM \u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002\u56e0\u6b64\uff0cQServe \u6709\u6548\u5730\u5c06 LLM \u670d\u52a1\u7684\u7f8e\u5143\u6210\u672c\u964d\u4f4e\u4e86 3 \u500d\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/mit-han-lab/qserve \u83b7\u5f97\u3002", "author": "Yujun Lin et.al.", "authors": "Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han", "id": "2405.04532v1", "paper_url": "http://arxiv.org/abs/2405.04532v1", "repo": "https://github.com/mit-han-lab/qserve"}}