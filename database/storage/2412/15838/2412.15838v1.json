{"2412.15838": {"publish_time": "2024-12-20", "title": "Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback", "paper_summary": "Reinforcement learning from human feedback (RLHF) has proven effective in\nenhancing the instruction-following capabilities of large language models;\nhowever, it remains underexplored in the cross-modality domain. As the number\nof modalities increases, aligning all-modality models with human intentions --\nsuch as instruction following -- becomes a pressing challenge. In this work, we\nmake the first attempt to fine-tune all-modality models (i.e. input and output\nwith any modality, also named any-to-any models) using human preference data\nacross all modalities (including text, image, audio, and video), ensuring its\nbehavior aligns with human intentions. This endeavor presents several\nchallenges. First, there is no large-scale all-modality human preference data\nin existing open-source resources, as most datasets are limited to specific\nmodalities, predominantly text and image. Secondly, the effectiveness of binary\npreferences in RLHF for post-training alignment in complex all-modality\nscenarios remains an unexplored area. Finally, there is a lack of a systematic\nframework to evaluate the capabilities of all-modality models, particularly\nregarding modality selection and synergy. To address these challenges, we\npropose the align-anything framework, which includes meticulously annotated\n200k all-modality human preference data. Then, we introduce an alignment method\nthat learns from unified language feedback, effectively capturing complex\nmodality-specific human preferences and enhancing the model's\ninstruction-following capabilities. Furthermore, to assess performance\nimprovements in all-modality models after post-training alignment, we construct\na challenging all-modality capability evaluation framework -- eval-anything.\nAll data, models, and code frameworks have been open-sourced for the community.\nFor more details, please refer to\nhttps://github.com/PKU-Alignment/align-anything.", "paper_summary_zh": "<paragraph>\u900f\u904e\u4eba\u985e\u56de\u994b\u7684\u589e\u5f37\u5b78\u7fd2 (RLHF) \u5df2\u88ab\u8b49\u5be6\u80fd\u6709\u6548\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\uff1b\u7136\u800c\uff0c\u5728\u8de8\u6a21\u614b\u9818\u57df\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u96a8\u8457\u6a21\u614b\u6578\u91cf\u7684\u589e\u52a0\uff0c\u4f7f\u6240\u6709\u6a21\u614b\u6a21\u578b\u8207\u4eba\u985e\u610f\u5716\u4fdd\u6301\u4e00\u81f4\uff08\u4f8b\u5982\u9075\u5faa\u6307\u4ee4\uff09\u6210\u4e86\u4e00\u9805\u8feb\u5207\u7684\u6311\u6230\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u6b21\u5617\u8a66\u4f7f\u7528\u6240\u6709\u6a21\u614b\uff08\u5305\u62ec\u6587\u5b57\u3001\u5f71\u50cf\u3001\u97f3\u8a0a\u548c\u5f71\u7247\uff09\u7684\u4eba\u985e\u504f\u597d\u8cc7\u6599\u5fae\u8abf\u6240\u6709\u6a21\u614b\u6a21\u578b\uff08\u5373\u8f38\u5165\u548c\u8f38\u51fa\u5177\u6709\u4efb\u4f55\u6a21\u614b\uff0c\u4e5f\u7a31\u70ba\u4efb\u610f\u5c0d\u4efb\u610f\u6a21\u578b\uff09\uff0c\u78ba\u4fdd\u5176\u884c\u70ba\u8207\u4eba\u985e\u610f\u5716\u4fdd\u6301\u4e00\u81f4\u3002\u9019\u9805\u52aa\u529b\u63d0\u51fa\u4e86\u5e7e\u500b\u6311\u6230\u3002\u9996\u5148\uff0c\u73fe\u6709\u7684\u958b\u653e\u539f\u59cb\u78bc\u8cc7\u6e90\u4e2d\u6c92\u6709\u5927\u898f\u6a21\u7684\u6240\u6709\u6a21\u614b\u4eba\u985e\u504f\u597d\u8cc7\u6599\uff0c\u56e0\u70ba\u5927\u591a\u6578\u8cc7\u6599\u96c6\u90fd\u9650\u65bc\u7279\u5b9a\u6a21\u614b\uff0c\u4e3b\u8981\u662f\u6587\u5b57\u548c\u5f71\u50cf\u3002\u5176\u6b21\uff0c\u5728\u8907\u96dc\u7684\u6240\u6709\u6a21\u614b\u60c5\u5883\u4e2d\uff0cRLHF \u4e2d\u4e8c\u5143\u504f\u597d\u5c0d\u5f8c\u8a13\u7df4\u5c0d\u9f4a\u7684\u6709\u6548\u6027\u4ecd\u662f\u4e00\u500b\u672a\u7d93\u63a2\u7d22\u7684\u9818\u57df\u3002\u6700\u5f8c\uff0c\u7f3a\u4e4f\u7cfb\u7d71\u6027\u7684\u67b6\u69cb\u4f86\u8a55\u4f30\u6240\u6709\u6a21\u614b\u6a21\u578b\u7684\u80fd\u529b\uff0c\u7279\u5225\u662f\u95dc\u65bc\u6a21\u614b\u9078\u64c7\u548c\u5354\u540c\u6548\u61c9\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 align-anything \u67b6\u69cb\uff0c\u5176\u4e2d\u5305\u542b\u4ed4\u7d30\u8a3b\u89e3\u7684 20 \u842c\u500b\u6240\u6709\u6a21\u614b\u4eba\u985e\u504f\u597d\u8cc7\u6599\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u5f9e\u7d71\u4e00\u7684\u8a9e\u8a00\u56de\u994b\u4e2d\u5b78\u7fd2\uff0c\u6709\u6548\u6355\u6349\u8907\u96dc\u7684\u7279\u5b9a\u65bc\u6a21\u614b\u7684\u4eba\u985e\u504f\u597d\uff0c\u4e26\u589e\u5f37\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u8a55\u4f30\u5f8c\u8a13\u7df4\u5c0d\u9f4a\u5f8c\u6240\u6709\u6a21\u614b\u6a21\u578b\u7684\u6548\u80fd\u6539\u5584\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u6240\u6709\u6a21\u614b\u80fd\u529b\u8a55\u4f30\u67b6\u69cb\u2014\u2014eval-anything\u3002\u6240\u6709\u8cc7\u6599\u3001\u6a21\u578b\u548c\u7a0b\u5f0f\u78bc\u67b6\u69cb\u90fd\u5df2\u958b\u653e\u539f\u59cb\u78bc\u4f9b\u793e\u7fa4\u4f7f\u7528\u3002\u5982\u9700\u66f4\u591a\u8a73\u7d30\u8cc7\u8a0a\uff0c\u8acb\u53c3\u95b1 https://github.com/PKU-Alignment/align-anything\u3002</paragraph>", "author": "Jiaming Ji et.al.", "authors": "Jiaming Ji, Jiayi Zhou, Hantao Lou, Boyuan Chen, Donghai Hong, Xuyao Wang, Wenqi Chen, Kaile Wang, Rui Pan, Jiahao Li, Mohan Wang, Josef Dai, Tianyi Qiu, Hua Xu, Dong Li, Weipeng Chen, Jun Song, Bo Zheng, Yaodong Yang", "id": "2412.15838v1", "paper_url": "http://arxiv.org/abs/2412.15838v1", "repo": "https://github.com/pku-alignment/align-anything"}}