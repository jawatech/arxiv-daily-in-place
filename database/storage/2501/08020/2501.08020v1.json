{"2501.08020": {"publish_time": "2025-01-14", "title": "Cooperative Patrol Routing: Optimizing Urban Crime Surveillance through Multi-Agent Reinforcement Learning", "paper_summary": "The effective design of patrol strategies is a difficult and complex problem,\nespecially in medium and large areas. The objective is to plan, in a\ncoordinated manner, the optimal routes for a set of patrols in a given area, in\norder to achieve maximum coverage of the area, while also trying to minimize\nthe number of patrols. In this paper, we propose a multi-agent reinforcement\nlearning (MARL) model, based on a decentralized partially observable Markov\ndecision process, to plan unpredictable patrol routes within an urban\nenvironment represented as an undirected graph. The model attempts to maximize\na target function that characterizes the environment within a given time frame.\nOur model has been tested to optimize police patrol routes in three\nmedium-sized districts of the city of Malaga. The aim was to maximize\nsurveillance coverage of the most crime-prone areas, based on actual crime data\nin the city. To address this problem, several MARL algorithms have been\nstudied, and among these the Value Decomposition Proximal Policy Optimization\n(VDPPO) algorithm exhibited the best performance. We also introduce a novel\nmetric, the coverage index, for the evaluation of the coverage performance of\nthe routes generated by our model. This metric is inspired by the predictive\naccuracy index (PAI), which is commonly used in criminology to detect hotspots.\nUsing this metric, we have evaluated the model under various scenarios in which\nthe number of agents (or patrols), their starting positions, and the level of\ninformation they can observe in the environment have been modified. Results\nshow that the coordinated routes generated by our model achieve a coverage of\nmore than $90\\%$ of the $3\\%$ of graph nodes with the highest crime incidence,\nand $65\\%$ for $20\\%$ of these nodes; $3\\%$ and $20\\%$ represent the coverage\nstandards for police resource allocation.", "paper_summary_zh": "\u5de1\u908f\u7b56\u7565\u7684\u6709\u6548\u8a2d\u8a08\u662f\u4e00\u500b\u56f0\u96e3\u4e14\u8907\u96dc\u7684\u554f\u984c\uff0c\u7279\u5225\u662f\u5728\u4e2d\u5927\u578b\u5340\u57df\u3002\u76ee\u6a19\u662f\u4ee5\u5354\u8abf\u7684\u65b9\u5f0f\u898f\u5283\u7279\u5b9a\u5340\u57df\u5167\u4e00\u7d44\u5de1\u908f\u7684\u6700\u4f73\u8def\u7dda\uff0c\u4ee5\u5be6\u73fe\u5340\u57df\u7684\u6700\u5927\u8986\u84cb\u7387\uff0c\u540c\u6642\u4e5f\u8a66\u5716\u5c07\u5de1\u908f\u6b21\u6578\u6e1b\u81f3\u6700\u5c11\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u5206\u6563\u90e8\u5206\u53ef\u89c0\u5bdf\u99ac\u53ef\u592b\u6c7a\u7b56\u904e\u7a0b\u7684\u591a\u4e3b\u9ad4\u5f37\u5316\u5b78\u7fd2 (MARL) \u6a21\u578b\uff0c\u4ee5\u898f\u5283\u5728\u8868\u793a\u70ba\u7121\u5411\u5716\u7684\u57ce\u5e02\u74b0\u5883\u4e2d\u4e0d\u53ef\u9810\u6e2c\u7684\u5de1\u908f\u8def\u7dda\u3002\u8a72\u6a21\u578b\u5617\u8a66\u6700\u5927\u5316\u8868\u5fb5\u7d66\u5b9a\u6642\u9593\u7bc4\u570d\u5167\u74b0\u5883\u7684\u76ee\u6a19\u51fd\u6578\u3002\u6211\u5011\u7684\u6a21\u578b\u5df2\u7d93\u904e\u6e2c\u8a66\uff0c\u4ee5\u512a\u5316\u897f\u73ed\u7259\u99ac\u62c9\u52a0\u5e02\u4e09\u500b\u4e2d\u7b49\u898f\u6a21\u5340\u57df\u7684\u8b66\u5bdf\u5de1\u908f\u8def\u7dda\u3002\u76ee\u6a19\u662f\u6839\u64da\u8a72\u5e02\u7684\u5be6\u969b\u72af\u7f6a\u6578\u64da\uff0c\u6700\u5927\u5316\u5c0d\u72af\u7f6a\u591a\u767c\u5340\u57df\u7684\u76e3\u63a7\u8986\u84cb\u7387\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u5df2\u7d93\u7814\u7a76\u4e86\u591a\u500b MARL \u6f14\u7b97\u6cd5\uff0c\u5176\u4e2d Value Decomposition Proximal Policy Optimization (VDPPO) \u6f14\u7b97\u6cd5\u8868\u73fe\u6700\u4f73\u3002\u6211\u5011\u9084\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7684\u6307\u6a19\uff0c\u5373\u8986\u84cb\u7387\u6307\u6578\uff0c\u7528\u65bc\u8a55\u4f30\u6211\u5011\u6a21\u578b\u751f\u6210\u7684\u8def\u7dda\u7684\u8986\u84cb\u7387\u8868\u73fe\u3002\u6b64\u6307\u6a19\u7684\u9748\u611f\u4f86\u81ea\u9810\u6e2c\u6e96\u78ba\u5ea6\u6307\u6578 (PAI)\uff0c\u5b83\u901a\u5e38\u7528\u65bc\u72af\u7f6a\u5b78\u4e2d\u5075\u6e2c\u71b1\u9ede\u3002\u4f7f\u7528\u6b64\u6307\u6a19\uff0c\u6211\u5011\u5728\u5404\u7a2e\u5834\u666f\u4e0b\u8a55\u4f30\u4e86\u6a21\u578b\uff0c\u5176\u4e2d\u4fee\u6539\u4e86\u4e3b\u9ad4\uff08\u6216\u5de1\u908f\uff09\u7684\u6578\u91cf\u3001\u4ed6\u5011\u7684\u8d77\u59cb\u4f4d\u7f6e\u4ee5\u53ca\u4ed6\u5011\u53ef\u4ee5\u5728\u74b0\u5883\u4e2d\u89c0\u5bdf\u5230\u7684\u8cc7\u8a0a\u5c64\u7d1a\u3002\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u751f\u6210\u7684\u5354\u8abf\u8def\u7dda\u9054\u5230\u4e86\u5c0d\u72af\u7f6a\u767c\u751f\u7387\u6700\u9ad8\u7684 $3\\%$ \u5716\u7bc0\u9ede\u7684 $90\\%$ \u4ee5\u4e0a\u7684\u8986\u84cb\u7387\uff0c\u800c\u5c0d\u65bc $20\\%$ \u7684\u9019\u4e9b\u7bc0\u9ede\u5247\u9054\u5230\u4e86 $65\\%$\uff1b$3\\%$ \u548c $20\\%$ \u4ee3\u8868\u4e86\u8b66\u5bdf\u8cc7\u6e90\u914d\u7f6e\u7684\u8986\u84cb\u7387\u6a19\u6e96\u3002", "author": "Juan Palma-Borda et.al.", "authors": "Juan Palma-Borda, Eduardo Guzm\u00e1n, Mar\u00eda-Victoria Belmonte", "id": "2501.08020v1", "paper_url": "http://arxiv.org/abs/2501.08020v1", "repo": "null"}}