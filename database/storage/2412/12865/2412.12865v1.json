{"2412.12865": {"publish_time": "2024-12-17", "title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models", "paper_summary": "Alignment, endowing a pre-trained Large language model (LLM) with the ability\nto follow instructions, is crucial for its real-world applications.\nConventional supervised fine-tuning (SFT) methods formalize it as causal\nlanguage modeling typically with a cross-entropy objective, requiring a large\namount of high-quality instruction-response pairs. However, the quality of\nwidely used SFT datasets can not be guaranteed due to the high cost and\nintensive labor for the creation and maintenance in practice. To overcome the\nlimitations associated with the quality of SFT datasets, we introduce a novel\n\\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning\napproach, namely PoFT. The intuition is to boost SFT by imposing a particular\npreference: \\textit{favoring the target model over aligned LLMs on the same SFT\ndata.} This preference encourages the target model to predict a higher\nlikelihood than that predicted by the aligned LLMs, incorporating assessment\ninformation on data quality (i.e., predicted likelihood by the aligned LLMs)\ninto the training process. Extensive experiments are conducted, and the results\nvalidate the effectiveness of the proposed method. PoFT achieves stable and\nconsistent improvements over the SFT baselines across different training\ndatasets and base models. Moreover, we prove that PoFT can be integrated with\nexisting SFT data filtering methods to achieve better performance, and further\nimproved by following preference optimization procedures, such as DPO.", "paper_summary_zh": "\u5c0d\u9f4a\u662f\u8ce6\u4e88\u9810\u5148\u8a13\u7df4\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u9019\u5c0d\u5176\u771f\u5be6\u4e16\u754c\u7684\u61c9\u7528\u81f3\u95dc\u91cd\u8981\u3002\n\u50b3\u7d71\u7684\u76e3\u7763\u5f0f\u5fae\u8abf (SFT) \u65b9\u6cd5\u5c07\u5176\u5f62\u5f0f\u5316\u70ba\u56e0\u679c\u8a9e\u8a00\u6a21\u578b\uff0c\u901a\u5e38\u4f7f\u7528\u4ea4\u53c9\u71b5\u76ee\u6a19\uff0c\u9700\u8981\u5927\u91cf\u9ad8\u54c1\u8cea\u7684\u6307\u4ee4\u56de\u61c9\u5c0d\u3002\u7136\u800c\uff0c\u7531\u65bc\u5be6\u969b\u4e2d\u5efa\u7acb\u548c\u7dad\u8b77\u7684\u9ad8\u6210\u672c\u548c\u5927\u91cf\u52de\u52d5\u529b\uff0c\u5ee3\u6cdb\u4f7f\u7528\u7684 SFT \u8cc7\u6599\u96c6\u7684\u54c1\u8cea\u7121\u6cd5\u5f97\u5230\u4fdd\u8b49\u3002\u70ba\u4e86\u514b\u670d\u8207 SFT \u8cc7\u6599\u96c6\u54c1\u8cea\u76f8\u95dc\u7684\u9650\u5236\uff0c\u6211\u5011\u5f15\u9032\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684 \\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning \u65b9\u6cd5\uff0c\u5373 PoFT\u3002\u76f4\u89ba\u4e0a\uff0c\u6211\u5011\u900f\u904e\u65bd\u52a0\u7279\u5b9a\u7684\u504f\u597d\u4f86\u63d0\u5347 SFT\uff1a\\textit{\u5728\u76f8\u540c\u7684 SFT \u8cc7\u6599\u4e0a\u504f\u597d\u76ee\u6a19\u6a21\u578b\u52dd\u904e\u5c0d\u9f4a\u7684 LLM\u3002}\u6b64\u504f\u597d\u9f13\u52f5\u76ee\u6a19\u6a21\u578b\u9810\u6e2c\u6bd4\u5c0d\u9f4a\u7684 LLM \u9810\u6e2c\u66f4\u9ad8\u7684\u53ef\u80fd\u6027\uff0c\u5c07\u8cc7\u6599\u54c1\u8cea\u7684\u8a55\u4f30\u8cc7\u8a0a\uff08\u5373\u5c0d\u9f4a\u7684 LLM \u9810\u6e2c\u7684\u53ef\u80fd\u6027\uff09\u7d0d\u5165\u8a13\u7df4\u7a0b\u5e8f\u4e2d\u3002\u6211\u5011\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u7d50\u679c\u9a57\u8b49\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002PoFT \u5728\u4e0d\u540c\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\u548c\u57fa\u790e\u6a21\u578b\u4e2d\uff0c\u90fd\u7372\u5f97\u4e86\u7a69\u5b9a\u4e14\u6301\u7e8c\u512a\u65bc SFT \u57fa\u6e96\u7684\u6539\u9032\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e PoFT \u53ef\u4ee5\u8207\u73fe\u6709\u7684 SFT \u8cc7\u6599\u904e\u6ffe\u65b9\u6cd5\u6574\u5408\uff0c\u4ee5\u9054\u5230\u66f4\u597d\u7684\u6548\u80fd\uff0c\u4e26\u900f\u904e\u9075\u5faa\u504f\u597d\u6700\u4f73\u5316\u7a0b\u5e8f\uff08\u4f8b\u5982 DPO\uff09\u9032\u4e00\u6b65\u63d0\u5347\u3002", "author": "Yuchen Fan et.al.", "authors": "Yuchen Fan, Yuzhong Hong, Qiushi Wang, Junwei Bao, Hongfei Jiang, Yang Song", "id": "2412.12865v1", "paper_url": "http://arxiv.org/abs/2412.12865v1", "repo": "https://github.com/Savannah120/alignment-handbook-PoFT"}}