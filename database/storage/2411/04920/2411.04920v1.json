{"2411.04920": {"publish_time": "2024-11-07", "title": "GPTKB: Building Very Large Knowledge Bases from Language Models", "paper_summary": "General-domain knowledge bases (KB), in particular the \"big three\" --\nWikidata, Yago and DBpedia -- are the backbone of many intelligent\napplications. While these three have seen steady development, comprehensive KB\nconstruction at large has seen few fresh attempts. In this work, we propose to\nbuild a large general-domain KB entirely from a large language model (LLM). We\ndemonstrate the feasibility of large-scale KB construction from LLMs, while\nhighlighting specific challenges arising around entity recognition, entity and\nproperty canonicalization, and taxonomy construction. As a prototype, we use\nGPT-4o-mini to construct GPTKB, which contains 105 million triples for more\nthan 2.9 million entities, at a cost 100x less than previous KBC projects. Our\nwork is a landmark for two fields: For NLP, for the first time, it provides\n\\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the\nSemantic Web, it shows novel ways forward for the long-standing challenge of\ngeneral-domain KB construction. GPTKB is accessible at https://gptkb.org.", "paper_summary_zh": "\u4e00\u822c\u9818\u57df\u77e5\u8b58\u5eab (KB)\uff0c\u7279\u5225\u662f\u300c\u4e09\u5927\u77e5\u8b58\u5eab\u300d-- Wikidata\u3001Yago \u548c DBpedia -- \u662f\u8a31\u591a\u667a\u6167\u578b\u61c9\u7528\u7a0b\u5f0f\u7684\u9aa8\u5e79\u3002\u5118\u7ba1\u9019\u4e09\u500b\u77e5\u8b58\u5eab\u6301\u7e8c\u767c\u5c55\uff0c\u4f46\u6574\u9ad4\u800c\u8a00\uff0c\u5168\u9762\u7684\u77e5\u8b58\u5eab\u5efa\u69cb\u9bae\u5c11\u6709\u65b0\u7684\u5617\u8a66\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u8b70\u5b8c\u5168\u5f9e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5efa\u7acb\u4e00\u500b\u5927\u578b\u4e00\u822c\u9818\u57df\u77e5\u8b58\u5eab\u3002\u6211\u5011\u5c55\u793a\u4e86\u5f9e LLM \u5efa\u69cb\u5927\u898f\u6a21\u77e5\u8b58\u5eab\u7684\u53ef\u884c\u6027\uff0c\u540c\u6642\u5f37\u8abf\u4e86\u5be6\u9ad4\u8fa8\u8b58\u3001\u5be6\u9ad4\u548c\u5c6c\u6027\u6b63\u898f\u5316\u4ee5\u53ca\u5206\u985e\u6cd5\u5efa\u69cb\u7b49\u7279\u5b9a\u6311\u6230\u3002\u4f5c\u70ba\u539f\u578b\uff0c\u6211\u5011\u4f7f\u7528 GPT-4o-mini \u5efa\u69cb GPTKB\uff0c\u5176\u4e2d\u5305\u542b\u8d85\u904e 290 \u842c\u500b\u5be6\u9ad4\u7684 1.05 \u5104\u500b\u4e09\u5143\u7d44\uff0c\u6210\u672c\u6bd4\u5148\u524d\u7684 KBC \u5c08\u6848\u4f4e 100 \u500d\u3002\u6211\u5011\u7684\u7814\u7a76\u662f\u5169\u500b\u9818\u57df\u7684\u91cc\u7a0b\u7891\uff1a\u5c0d\u65bc\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP)\uff0c\u5b83\u9996\u6b21\u63d0\u4f9b\u4e86 LLM \u7684\u77e5\u8b58\uff08\u6216\u4fe1\u5ff5\uff09\u7684\u300c\u5efa\u69cb\u6027\u300d\u898b\u89e3\u3002\u5c0d\u65bc\u8a9e\u610f\u7db2\u8def\uff0c\u5b83\u5c55\u793a\u4e86\u9577\u671f\u5b58\u5728\u7684\u4e00\u822c\u9818\u57df\u77e5\u8b58\u5eab\u5efa\u69cb\u6311\u6230\u7684\u65b0\u7a4e\u65b9\u6cd5\u3002GPTKB \u53ef\u5728 https://gptkb.org \u53d6\u5f97\u3002", "author": "Yujia Hu et.al.", "authors": "Yujia Hu, Shrestha Ghosh, Tuan-Phong Nugyen, Simon Razniewski", "id": "2411.04920v1", "paper_url": "http://arxiv.org/abs/2411.04920v1", "repo": "null"}}