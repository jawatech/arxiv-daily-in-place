{"2407.13952": {"publish_time": "2024-07-19", "title": "Knowledge Distillation Approaches for Accurate and Efficient Recommender System", "paper_summary": "Despite its breakthrough in classification problems, Knowledge distillation\n(KD) to recommendation models and ranking problems has not been studied well in\nthe previous literature. This dissertation is devoted to developing knowledge\ndistillation methods for recommender systems to fully improve the performance\nof a compact model. We propose novel distillation methods designed for\nrecommender systems. The proposed methods are categorized according to their\nknowledge sources as follows: (1) Latent knowledge: we propose two methods that\ntransfer latent knowledge of user/item representation. They effectively\ntransfer knowledge of niche tastes with a balanced distillation strategy that\nprevents the KD process from being biased towards a small number of large\npreference groups. Also, we propose a new method that transfers user/item\nrelations in the representation space. The proposed method selectively\ntransfers essential relations considering the limited capacity of the compact\nmodel. (2) Ranking knowledge: we propose three methods that transfer ranking\nknowledge from the recommendation results. They formulate the KD process as a\nranking matching problem and transfer the knowledge via a listwise learning\nstrategy. Further, we present a new learning framework that compresses the\nranking knowledge of heterogeneous recommendation models. The proposed\nframework is developed to ease the computational burdens of model ensemble\nwhich is a dominant solution for many recommendation applications. We validate\nthe benefit of our proposed methods and frameworks through extensive\nexperiments. To summarize, this dissertation sheds light on knowledge\ndistillation approaches for a better accuracy-efficiency trade-off of the\nrecommendation models.", "paper_summary_zh": "\u5118\u7ba1\u5728\u5206\u985e\u554f\u984c\u4e0a\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u77e5\u8b58\u84b8\u993e (KD) \u5728\u63a8\u85a6\u6a21\u578b\u548c\u6392\u540d\u554f\u984c\u4e0a\u7684\u61c9\u7528\u5c1a\u672a\u5728\u5148\u524d\u7684\u6587\u737b\u4e2d\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u672c\u8ad6\u6587\u81f4\u529b\u65bc\u958b\u767c\u63a8\u85a6\u7cfb\u7d71\u7684\u77e5\u8b58\u84b8\u993e\u65b9\u6cd5\uff0c\u4ee5\u5168\u9762\u63d0\u5347\u7cbe\u7c21\u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u63d0\u51fa\u5c08\u70ba\u63a8\u85a6\u7cfb\u7d71\u8a2d\u8a08\u7684\u65b0\u7a4e\u84b8\u993e\u65b9\u6cd5\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6839\u64da\u5176\u77e5\u8b58\u4f86\u6e90\u5206\u985e\u5982\u4e0b\uff1a(1) \u6f5b\u5728\u77e5\u8b58\uff1a\u6211\u5011\u63d0\u51fa\u5169\u7a2e\u65b9\u6cd5\uff0c\u7528\u65bc\u50b3\u8f38\u4f7f\u7528\u8005/\u5c08\u6848\u8868\u5fb5\u7684\u6f5b\u5728\u77e5\u8b58\u3002\u5b83\u5011\u900f\u904e\u5e73\u8861\u84b8\u993e\u7b56\u7565\u6709\u6548\u50b3\u8f38\u5229\u57fa\u54c1\u5473\u7684\u77e5\u8b58\uff0c\u9632\u6b62 KD \u7a0b\u5e8f\u504f\u5411\u5c11\u6578\u5927\u578b\u504f\u597d\u7fa4\u7d44\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u50b3\u8f38\u8868\u5fb5\u7a7a\u9593\u4e2d\u7684\u4f7f\u7528\u8005/\u5c08\u6848\u95dc\u4fc2\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8003\u616e\u7cbe\u7c21\u6a21\u578b\u7684\u6709\u9650\u5bb9\u91cf\u4e0b\uff0c\u6709\u9078\u64c7\u6027\u5730\u50b3\u8f38\u5fc5\u8981\u7684\u95dc\u4fc2\u3002(2) \u6392\u540d\u77e5\u8b58\uff1a\u6211\u5011\u63d0\u51fa\u4e09\u7a2e\u65b9\u6cd5\uff0c\u7528\u65bc\u50b3\u8f38\u63a8\u85a6\u7d50\u679c\u4e2d\u7684\u6392\u540d\u77e5\u8b58\u3002\u5b83\u5011\u5c07 KD \u7a0b\u5e8f\u5236\u5b9a\u70ba\u6392\u540d\u914d\u5c0d\u554f\u984c\uff0c\u4e26\u900f\u904e\u6e05\u55ae\u5b78\u7fd2\u7b56\u7565\u50b3\u8f38\u77e5\u8b58\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u5b78\u7fd2\u67b6\u69cb\uff0c\u7528\u65bc\u58d3\u7e2e\u7570\u8cea\u63a8\u85a6\u6a21\u578b\u7684\u6392\u540d\u77e5\u8b58\u3002\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u65e8\u5728\u6e1b\u8f15\u6a21\u578b\u6574\u5408\u7684\u904b\u7b97\u8ca0\u64d4\uff0c\u800c\u6a21\u578b\u6574\u5408\u662f\u8a31\u591a\u63a8\u85a6\u61c9\u7528\u7a0b\u5f0f\u7684\u4e3b\u6d41\u89e3\u6c7a\u65b9\u6848\u3002\u6211\u5011\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\u9a57\u8b49\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u67b6\u69cb\u7684\u512a\u9ede\u3002\u7e3d\u4e4b\uff0c\u672c\u8ad6\u6587\u95e1\u660e\u4e86\u77e5\u8b58\u84b8\u993e\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u63a8\u85a6\u6a21\u578b\u7684\u6e96\u78ba\u6027\u8207\u6548\u7387\u6298\u8877\u3002", "author": "SeongKu Kang et.al.", "authors": "SeongKu Kang", "id": "2407.13952v1", "paper_url": "http://arxiv.org/abs/2407.13952v1", "repo": "null"}}