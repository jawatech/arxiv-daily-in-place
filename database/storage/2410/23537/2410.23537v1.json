{"2410.23537": {"publish_time": "2024-10-31", "title": "ALISE: Accelerating Large Language Model Serving with Speculative Scheduling", "paper_summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee3\u8868\u4e86\u7576\u4ee3\u4eba\u5de5\u901a\u7528\u667a\u80fd (AGI) \u9818\u57df\u7684\u4e00\u5834\u9769\u547d\u6027\u9032\u5c55\u3002\u4ee5 ChatGPT \u70ba\u4f8b\uff0c\u57fa\u65bc LLM \u7684\u61c9\u7528\u9700\u8981\u6700\u5c0f\u7684\u56de\u61c9\u5ef6\u9072\u548c\u6700\u5927\u7684\u63a8\u8ad6\u670d\u52d9\u50b3\u8f38\u91cf\u3002\u7136\u800c\uff0c\u7531\u65bc LLM \u57f7\u884c\u4e0d\u53ef\u9810\u6e2c\uff0c\u7576\u524d LLM \u670d\u52d9\u7cfb\u7d71\u63a1\u7528\u7684\u5148\u5230\u5148\u670d\u52d9 (FCFS) \u6392\u7a0b\u653f\u7b56\u6703\u51fa\u73fe\u968a\u982d (HoL) \u963b\u585e\u554f\u984c\u548c\u9577\u7684\u4f5c\u696d\u56de\u61c9\u6642\u9593\u3002\n\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u9ad8\u6548 LLM \u63a8\u8ad6\u670d\u52d9\u67b6\u69cb\uff0c\u540d\u70ba ALISE\u3002ALISE \u7684\u4e3b\u8981\u8a2d\u8a08\u7bc4\u4f8b\u662f\u5229\u7528\u4e00\u500b\u65b0\u7684\u63a8\u6e2c\u6392\u7a0b\u5668\uff0c\u900f\u904e\u4f30\u8a08\u6bcf\u500b\u4f5c\u696d\u7684\u57f7\u884c\u6642\u9593\uff0c\u4e26\u5229\u7528\u6b64\u985e\u5148\u9a57\u77e5\u8b58\u4f86\u6307\u5b9a\u9069\u7576\u7684\u4f5c\u696d\u512a\u5148\u9806\u5e8f\uff0c\u5f9e\u800c\u6700\u5927\u7a0b\u5ea6\u5730\u6e1b\u5c11\u7570\u8cea\u5de5\u4f5c\u8ca0\u8f09\u7684\u6f5b\u5728\u6392\u968a\u5ef6\u9072\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u6e1b\u8f15\u4e2d\u9593\u9375\u503c (KV) \u5feb\u53d6\u7684\u8a18\u61b6\u9ad4\u958b\u92b7\uff0c\u6211\u5011\u63a1\u7528\u57fa\u65bc\u512a\u5148\u9806\u5e8f\u7684\u81ea\u9069\u61c9\u8a18\u61b6\u9ad4\u7ba1\u7406\u5354\u5b9a\u548c\u57fa\u65bc\u91cf\u5316\u7684\u58d3\u7e2e\u6280\u8853\u3002\u8a55\u4f30\u8868\u660e\uff0c\u8207\u6700\u5148\u9032\u7684\u89e3\u6c7a\u65b9\u6848 vLLM \u76f8\u6bd4\uff0cALISE \u5728 Alpaca \u548c ShareGPT \u8cc7\u6599\u96c6\u4e0a\u5206\u5225\u5728\u76f8\u540c\u7684\u5ef6\u9072\u9650\u5236\u4e0b\u5c07\u63a8\u8ad6\u670d\u52d9\u7684\u50b3\u8f38\u91cf\u63d0\u5347\u4e86 1.8 \u500d\u548c 2.1 \u500d\u3002", "author": "Youpeng Zhao et.al.", "authors": "Youpeng Zhao, Jun Wang", "id": "2410.23537v1", "paper_url": "http://arxiv.org/abs/2410.23537v1", "repo": "null"}}