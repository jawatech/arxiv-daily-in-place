{"2405.05957": {"publish_time": "2024-05-09", "title": "OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage Pruning", "paper_summary": "Large Language Models (LLMs) have played an important role in many fields due\nto their powerful capabilities.However, their massive number of parameters\nleads to high deployment requirements and incurs significant inference costs,\nwhich impedes their practical applications. Training smaller models is an\neffective way to address this problem. Therefore, we introduce OpenBA-V2, a\n3.4B model derived from multi-stage compression and continual pre-training from\nthe original 15B OpenBA model. OpenBA-V2 utilizes more data, more flexible\ntraining objectives, and techniques such as layer pruning, neural pruning, and\nvocabulary pruning to achieve a compression rate of 77.3\\% with minimal\nperformance loss. OpenBA-V2 demonstrates competitive performance compared to\nother open-source models of similar size, achieving results close to or on par\nwith the 15B OpenBA model in downstream tasks such as common sense reasoning\nand Named Entity Recognition (NER). OpenBA-V2 illustrates that LLMs can be\ncompressed into smaller ones with minimal performance loss by employing\nadvanced training objectives and data strategies, which may help deploy LLMs in\nresource-limited scenarios.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7531\u65bc\u5176\u5f37\u5927\u7684\u529f\u80fd\uff0c\u5728\u8a31\u591a\u9818\u57df\u4e2d\u626e\u6f14\u8457\u91cd\u8981\u7684\u89d2\u8272\u3002\u7136\u800c\uff0c\u5176\u9f90\u5927\u7684\u53c3\u6578\u91cf\u5c0e\u81f4\u4e86\u9ad8\u90e8\u7f72\u9700\u6c42\uff0c\u4e26\u7522\u751f\u986f\u8457\u7684\u63a8\u8ad6\u6210\u672c\uff0c\u9019\u963b\u7919\u4e86\u5176\u5be6\u969b\u61c9\u7528\u3002\u8a13\u7df4\u8f03\u5c0f\u7684\u6a21\u578b\u662f\u89e3\u6c7a\u6b64\u554f\u984c\u7684\u6709\u6548\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 OpenBA-V2\uff0c\u4e00\u500b\u5f9e\u539f\u59cb 15B OpenBA \u6a21\u578b\u7684\u591a\u968e\u6bb5\u58d3\u7e2e\u548c\u6301\u7e8c\u9810\u8a13\u7df4\u884d\u751f\u7684 3.4B \u6a21\u578b\u3002OpenBA-V2 \u5229\u7528\u66f4\u591a\u6578\u64da\u3001\u66f4\u9748\u6d3b\u7684\u8a13\u7df4\u76ee\u6a19\uff0c\u4ee5\u53ca\u5c64\u4fee\u526a\u3001\u795e\u7d93\u4fee\u526a\u548c\u8a5e\u5f59\u4fee\u526a\u7b49\u6280\u8853\uff0c\u4ee5\u6700\u5c0f\u7684\u6548\u80fd\u640d\u5931\u5be6\u73fe 77.3% \u7684\u58d3\u7e2e\u7387\u3002\u8207\u5176\u4ed6\u985e\u4f3c\u898f\u6a21\u7684\u958b\u6e90\u6a21\u578b\u76f8\u6bd4\uff0cOpenBA-V2 \u8868\u73fe\u51fa\u7af6\u722d\u529b\u7684\u6548\u80fd\uff0c\u5728\u5e38\u8b58\u63a8\u7406\u548c\u547d\u540d\u5be6\u9ad4\u8b58\u5225 (NER) \u7b49\u4e0b\u6e38\u4efb\u52d9\u4e2d\uff0c\u7372\u5f97\u63a5\u8fd1\u6216\u7b49\u540c\u65bc 15B OpenBA \u6a21\u578b\u7684\u7d50\u679c\u3002OpenBA-V2 \u8aaa\u660e\u4e86 LLM \u53ef\u4ee5\u900f\u904e\u63a1\u7528\u9032\u968e\u8a13\u7df4\u76ee\u6a19\u548c\u6578\u64da\u7b56\u7565\uff0c\u58d3\u7e2e\u6210\u8f03\u5c0f\u7684\u6a21\u578b\uff0c\u4e14\u6548\u80fd\u640d\u5931\u6700\u5c0f\uff0c\u9019\u6709\u52a9\u65bc\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u60c5\u6cc1\u4e0b\u90e8\u7f72 LLM\u3002", "author": "Dan Qiao et.al.", "authors": "Dan Qiao, Yi Su, Pinzheng Wang, Jing Ye, Wenjing Xie, Yuechi Zhou, Yuyang Ding, Zecheng Tang, Jikai Wang, Yixin Ji, Yue Wang, Pei Guo, Zechen Sun, Zikang Zhang, Juntao Li, Pingfu Chao, Wenliang Chen, Guohong Fu, Guodong Zhou, Qiaoming Zhu, Min Zhang", "id": "2405.05957v1", "paper_url": "http://arxiv.org/abs/2405.05957v1", "repo": "https://github.com/opennlg/openba-v2"}}