{"2410.05193": {"publish_time": "2024-10-07", "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References", "paper_summary": "With significant efforts in recent studies, LLM-as-a-Judge has become a\ncost-effective alternative to human evaluation for assessing the text\ngeneration quality in a wide range of tasks. However, there still remains a\nreliability gap between LLM-as-a-Judge and human evaluation. One important\nreason is the lack of guided oracles in the evaluation process. Motivated by\nthe role of reference pervasively used in classic text evaluation, we introduce\nRevisEval, a novel text generation evaluation paradigm via the response-adapted\nreferences. RevisEval is driven by the key observation that an ideal reference\nshould maintain the necessary relevance to the response to be evaluated.\nSpecifically, RevisEval leverages the text revision capabilities of large\nlanguage models (LLMs) to adaptively revise the response, then treat the\nrevised text as the reference (response-adapted reference) for the subsequent\nevaluation. Extensive experiments demonstrate that RevisEval outperforms\ntraditional reference-free and reference-based evaluation paradigms that use\nLLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks.\nMore importantly, our response-adapted references can further boost the\nclassical text metrics, e.g., BLEU and BERTScore, compared to traditional\nreferences and even rival the LLM-as-a-Judge. A detailed analysis is also\nconducted to confirm RevisEval's effectiveness in bias reduction, the impact of\ninference cost, and reference relevance.", "paper_summary_zh": "<paragraph>\u5728\u6700\u8fd1\u7684\u7814\u7a76\u4e2d\uff0cLLM-as-a-Judge \u5df2\u6210\u70ba\u4e00\u7a2e\u8a55\u4f30\u5404\u7a2e\u4efb\u52d9\u4e2d\u6587\u5b57\u751f\u6210\u54c1\u8cea\u7684\u5177\u6210\u672c\u6548\u76ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u4ee5\u53d6\u4ee3\u4eba\u5de5\u8a55\u4f30\u3002\u7136\u800c\uff0cLLM-as-a-Judge \u8207\u4eba\u5de5\u8a55\u4f30\u4e4b\u9593\u4ecd\u5b58\u5728\u53ef\u9760\u6027\u5dee\u8ddd\u3002\u4e00\u500b\u91cd\u8981\u7684\u539f\u56e0\u662f\u8a55\u4f30\u904e\u7a0b\u4e2d\u7f3a\u4e4f\u5f15\u5c0e\u5f0f\u795e\u8aed\u3002\u5728\u7d93\u5178\u6587\u5b57\u8a55\u4f30\u4e2d\u666e\u904d\u4f7f\u7528\u53c3\u8003\u7684\u555f\u767c\u4e0b\uff0c\u6211\u5011\u5f15\u5165\u4e86 RevisEval\uff0c\u4e00\u7a2e\u900f\u904e\u56de\u61c9\u9069\u61c9\u53c3\u8003\u7684\u65b0\u6587\u5b57\u751f\u6210\u8a55\u4f30\u7bc4\u4f8b\u3002RevisEval \u53d7\u5230\u4e00\u500b\u95dc\u9375\u89c0\u5bdf\u7684\u9a45\u52d5\uff0c\u5373\u7406\u60f3\u7684\u53c3\u8003\u61c9\u8207\u8981\u8a55\u4f30\u7684\u56de\u61c9\u4fdd\u6301\u5fc5\u8981\u7684\u76f8\u95dc\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cRevisEval \u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6587\u5b57\u4fee\u6539\u80fd\u529b\u4f86\u9069\u61c9\u6027\u5730\u4fee\u6539\u56de\u61c9\uff0c\u7136\u5f8c\u5c07\u4fee\u6539\u5f8c\u7684\u6587\u5b57\u4f5c\u70ba\u5f8c\u7e8c\u8a55\u4f30\u7684\u53c3\u8003\uff08\u56de\u61c9\u9069\u61c9\u53c3\u8003\uff09\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cRevisEval \u5728 NLG \u4efb\u52d9\u548c\u958b\u653e\u5f0f\u6307\u4ee4\u9075\u5faa\u4efb\u52d9\u4e2d\u512a\u65bc\u4f7f\u7528 LLM-as-a-Judge \u7684\u50b3\u7d71\u7121\u53c3\u8003\u548c\u57fa\u65bc\u53c3\u8003\u7684\u8a55\u4f30\u7bc4\u4f8b\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8207\u50b3\u7d71\u53c3\u8003\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u56de\u61c9\u9069\u61c9\u53c3\u8003\u53ef\u4ee5\u9032\u4e00\u6b65\u63d0\u5347\u7d93\u5178\u6587\u5b57\u6307\u6a19\uff0c\u4f8b\u5982 BLEU \u548c BERTScore\uff0c\u751a\u81f3\u8207 LLM-as-a-Judge \u76f8\u5ab2\u7f8e\u3002\u9084\u9032\u884c\u4e86\u8a73\u7d30\u5206\u6790\u4ee5\u78ba\u8a8d RevisEval \u5728\u6e1b\u5c11\u504f\u5dee\u3001\u63a8\u7406\u6210\u672c\u5f71\u97ff\u548c\u53c3\u8003\u76f8\u95dc\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002</paragraph>", "author": "Qiyuan Zhang et.al.", "authors": "Qiyuan Zhang, Yufei Wang, Tiezheng YU, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma", "id": "2410.05193v1", "paper_url": "http://arxiv.org/abs/2410.05193v1", "repo": "null"}}