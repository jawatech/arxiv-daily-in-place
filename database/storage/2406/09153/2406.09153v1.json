{"2406.09153": {"publish_time": "2024-06-13", "title": "LASER: Learning by Aligning Self-supervised Representations of Speech for Improving Content-related Tasks", "paper_summary": "Self-supervised learning (SSL)-based speech models are extensively used for\nfull-stack speech processing. However, it has been observed that improving\nSSL-based speech representations using unlabeled speech for content-related\ntasks is challenging and computationally expensive. Recent attempts have been\nmade to address this issue with cost-effective self-supervised fine-tuning\n(SSFT) approaches. Continuing in this direction, a cost-effective SSFT method\nnamed \"LASER: Learning by Aligning Self-supervised Representations\" is\npresented. LASER is based on the soft-DTW alignment loss with temporal\nregularisation term. Experiments are conducted with HuBERT and WavLM models and\nevaluated on the SUPERB benchmark for two content-related tasks: automatic\nspeech recognition (ASR) and phoneme recognition (PR). A relative improvement\nof 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the\nASR and PR tasks respectively, with only < 3 hours of fine-tuning on a single\nGPU.", "paper_summary_zh": "\u57fa\u65bc\u81ea\u6211\u76e3\u7763\u5b78\u7fd2 (SSL) \u7684\u8a9e\u97f3\u6a21\u578b\u5ee3\u6cdb\u7528\u65bc\u5168\u7aef\u8a9e\u97f3\u8655\u7406\u3002\u7136\u800c\uff0c\u5df2\u89c0\u5bdf\u5230\u4f7f\u7528\u8207\u5167\u5bb9\u76f8\u95dc\u4efb\u52d9\u7684\u672a\u6a19\u8a18\u8a9e\u97f3\u4f86\u6539\u9032\u57fa\u65bc SSL \u7684\u8a9e\u97f3\u8868\u793a\u5177\u6709\u6311\u6230\u6027\u4e14\u5728\u8a08\u7b97\u4e0a\u5f88\u6602\u8cb4\u3002\u6700\u8fd1\u5df2\u5617\u8a66\u4f7f\u7528\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u81ea\u6211\u76e3\u7763\u5fae\u8abf (SSFT) \u65b9\u6cd5\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\u3002\u7e7c\u7e8c\u671d\u9019\u500b\u65b9\u5411\uff0c\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba\u300cLASER\uff1a\u900f\u904e\u6bd4\u5c0d\u81ea\u6211\u76e3\u7763\u8868\u793a\u9032\u884c\u5b78\u7fd2\u300d\u7684\u5177\u6709\u6210\u672c\u6548\u76ca\u7684 SSFT \u65b9\u6cd5\u3002LASER \u57fa\u65bc\u5177\u6709\u6642\u9593\u6b63\u5247\u5316\u9805\u7684\u8edf DTW \u5c0d\u9f4a\u640d\u5931\u3002\u4f7f\u7528 HuBERT \u548c WavLM \u6a21\u578b\u9032\u884c\u5be6\u9a57\uff0c\u4e26\u91dd\u5c0d\u5169\u500b\u8207\u5167\u5bb9\u76f8\u95dc\u7684\u4efb\u52d9\uff08\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u548c\u97f3\u7d20\u8fa8\u8b58 (PR)\uff09\u5728 SUPERB \u57fa\u6e96\u4e0a\u9032\u884c\u8a55\u4f30\u3002\u5c0d\u65bc HuBERT\uff0cASR \u548c PR \u4efb\u52d9\u5206\u5225\u89c0\u5bdf\u5230 3.7% \u548c 8.2% \u7684\u76f8\u5c0d\u6539\u9032\uff0c\u800c WavLM \u5247\u89c0\u5bdf\u5230 4.1% \u548c 11.7%\uff0c\u800c\u50c5\u5728\u55ae\u4e00 GPU \u4e0a\u9032\u884c\u4e0d\u5230 3 \u5c0f\u6642\u7684\u5fae\u8abf\u3002", "author": "Amit Meghanani et.al.", "authors": "Amit Meghanani, Thomas Hain", "id": "2406.09153v1", "paper_url": "http://arxiv.org/abs/2406.09153v1", "repo": "null"}}