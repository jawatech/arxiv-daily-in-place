{"2411.14982": {"publish_time": "2024-11-22", "title": "Large Multi-modal Models Can Interpret Features in Large Multi-modal Models", "paper_summary": "Recent advances in Large Multimodal Models (LMMs) lead to significant\nbreakthroughs in both academia and industry. One question that arises is how\nwe, as humans, can understand their internal neural representations. This paper\ntakes an initial step towards addressing this question by presenting a\nversatile framework to identify and interpret the semantics within LMMs.\nSpecifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the\nrepresentations into human understandable features. 2) We then present an\nautomatic interpretation framework to interpreted the open-semantic features\nlearned in SAE by the LMMs themselves. We employ this framework to analyze the\nLLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these\nfeatures can effectively steer the model's behavior. Our results contribute to\na deeper understanding of why LMMs excel in specific tasks, including EQ tests,\nand illuminate the nature of their mistakes along with potential strategies for\ntheir rectification. These findings offer new insights into the internal\nmechanisms of LMMs and suggest parallels with the cognitive processes of the\nhuman brain.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u5b66\u672f\u754c\u548c\u4ea7\u4e1a\u754c\u5e26\u6765\u4e86\u91cd\u5927\u7a81\u7834\u3002\u4e00\u4e2a\u51fa\u73b0\u7684\u95ee\u9898\u662f\u6211\u4eec\u4f5c\u4e3a\u4eba\u7c7b\u5982\u4f55\u7406\u89e3\u5176\u5185\u90e8\u795e\u7ecf\u8868\u5f81\u3002\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\u6765\u8bc6\u522b\u548c\u89e3\u91ca LMM \u4e2d\u7684\u8bed\u4e49\uff0c\u8fc8\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u7684\u7b2c\u4e00\u6b65\u3002\u5177\u4f53\u6765\u8bf4\uff0c1) \u6211\u4eec\u9996\u5148\u5e94\u7528\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668 (SAE) \u5c06\u8868\u5f81\u5206\u89e3\u4e3a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7279\u5f81\u30022) \u7136\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u81ea\u52a8\u89e3\u91ca\u6846\u67b6\uff0c\u7531 LMM \u672c\u8eab\u89e3\u91ca\u5728 SAE \u4e2d\u5b66\u4e60\u7684\u5f00\u653e\u8bed\u4e49\u7279\u5f81\u3002\u6211\u4eec\u4f7f\u7528 LLaVA-OV-72B \u6a21\u578b\u5206\u6790 LLaVA-NeXT-8B \u6a21\u578b\uff0c\u8bc1\u660e\u8fd9\u4e9b\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u5730\u5f15\u5bfc\u6a21\u578b\u7684\u884c\u4e3a\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u5730\u7406\u89e3 LMM \u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5305\u62ec\u60c5\u5546\u6d4b\u8bd5\uff09\u4e2d\u8868\u73b0\u51fa\u8272\u7684\u539f\u56e0\uff0c\u5e76\u9610\u660e\u5176\u9519\u8bef\u7684\u672c\u8d28\u4ee5\u53ca\u6f5c\u5728\u7684\u7ea0\u6b63\u7b56\u7565\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a LMM \u7684\u5185\u90e8\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u6697\u793a\u4e86\u4e0e\u4eba\u8111\u8ba4\u77e5\u8fc7\u7a0b\u7684\u76f8\u4f3c\u4e4b\u5904\u3002", "author": "Kaichen Zhang et.al.", "authors": "Kaichen Zhang, Yifei Shen, Bo Li, Ziwei Liu", "id": "2411.14982v1", "paper_url": "http://arxiv.org/abs/2411.14982v1", "repo": "https://github.com/EvolvingLMMs-Lab/multimodal-sae"}}