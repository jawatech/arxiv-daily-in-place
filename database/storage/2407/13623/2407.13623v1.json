{"2407.13623": {"publish_time": "2024-07-18", "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies", "paper_summary": "Research on scaling large language models (LLMs) has primarily focused on\nmodel parameters and training data size, overlooking the role of vocabulary\nsize. % Intuitively, larger vocabularies enable more efficient tokenization by\nrepresenting sentences with fewer tokens, but they also increase the risk of\nunder-fitting representations for rare tokens. We investigate how vocabulary\nsize impacts LLM scaling laws by training models ranging from 33M to 3B\nparameters on up to 500B characters with various vocabulary configurations. We\npropose three complementary approaches for predicting the compute-optimal\nvocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit\nof the loss function. Our approaches converge on the same result that the\noptimal vocabulary size depends on the available compute budget and that larger\nmodels deserve larger vocabularies. However, most LLMs use too small vocabulary\nsizes. For example, we predict that the optimal vocabulary size of Llama2-70B\nshould have been at least 216K, 7 times larger than its vocabulary of 32K. We\nvalidate our predictions empirically by training models with 3B parameters\nacross different FLOPs budgets. Adopting our predicted optimal vocabulary size\nconsistently improves downstream performance over commonly used vocabulary\nsizes. By increasing the vocabulary size from the conventional 32K to 43K, we\nimprove performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21\nFLOPs. Our work emphasizes the necessity of jointly considering model\nparameters and vocabulary size for efficient scaling.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u64f4\u5145\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\n\u6a21\u578b\u53c3\u6578\u548c\u8a13\u7df4\u8cc7\u6599\u5927\u5c0f\uff0c\u5ffd\u7565\u4e86\u8a5e\u5f59\u91cf\u7684\u89d2\u8272\n\u5927\u5c0f\u3002% \u76f4\u89ba\u4e0a\uff0c\u8f03\u5927\u7684\u8a5e\u5f59\u91cf\u80fd\u900f\u904e\n\u4f7f\u7528\u8f03\u5c11\u7b26\u865f\u8868\u793a\u53e5\u5b50\u4f86\u66f4\u6709\u6548\u7387\u5730\u9032\u884c\u7b26\u865f\u5316\uff0c\u4f46\u5b83\u5011\u4e5f\u589e\u52a0\u4e86\n\u4f4e\u64ec\u5408\u7f55\u898b\u7b26\u865f\u8868\u793a\u7684\u98a8\u96aa\u3002\u6211\u5011\u7814\u7a76\u8a5e\u5f59\u91cf\n\u5927\u5c0f\u5982\u4f55\u5f71\u97ff LLM \u64f4\u5145\u5b9a\u5f8b\uff0c\u8a13\u7df4\u6a21\u578b\u7bc4\u570d\u5f9e 33M \u5230 3B\n\u53c3\u6578\uff0c\u5728\u6700\u591a 500B \u500b\u5b57\u5143\u4e0a\uff0c\u642d\u914d\u5404\u7a2e\u8a5e\u5f59\u91cf\u7d44\u614b\u3002\u6211\u5011\n\u63d0\u51fa\u4e09\u7a2e\u4e92\u88dc\u7684\u65b9\u6cd5\u4f86\u9810\u6e2c\u8a08\u7b97\u6700\u4f73\u7684\n\u8a5e\u5f59\u91cf\u5927\u5c0f\uff1aIsoFLOPs \u5206\u6790\u3001\u5c0e\u6578\u4f30\u8a08\u548c\u53c3\u6578\u64ec\u5408\n\u640d\u5931\u51fd\u6578\u3002\u6211\u5011\u7684\u505a\u6cd5\u6536\u6582\u65bc\u76f8\u540c\u7684\u7d50\u679c\uff0c\u5373\n\u6700\u4f73\u8a5e\u5f59\u91cf\u5927\u5c0f\u53d6\u6c7a\u65bc\u53ef\u7528\u7684\u8a08\u7b97\u9810\u7b97\uff0c\u4e26\u4e14\u8f03\u5927\u7684\n\u6a21\u578b\u9700\u8981\u8f03\u5927\u7684\u8a5e\u5f59\u91cf\u3002\u7136\u800c\uff0c\u5927\u591a\u6578 LLM \u4f7f\u7528\u592a\u5c0f\u7684\u8a5e\u5f59\u91cf\n\u5927\u5c0f\u3002\u4f8b\u5982\uff0c\u6211\u5011\u9810\u6e2c Llama2-70B \u7684\u6700\u4f73\u8a5e\u5f59\u91cf\u5927\u5c0f\n\u61c9\u8a72\u81f3\u5c11\u70ba 216K\uff0c\u6bd4\u5176 32K \u7684\u8a5e\u5f59\u91cf\u5927 7 \u500d\u3002\u6211\u5011\n\u900f\u904e\u4f7f\u7528 3B \u53c3\u6578\u5728\u4e0d\u540c\u7684 FLOPs \u9810\u7b97\u4e2d\u8a13\u7df4\u6a21\u578b\u4f86\u9a57\u8b49\u6211\u5011\u7684\u9810\u6e2c\n\u3002\u63a1\u7528\u6211\u5011\u9810\u6e2c\u7684\u6700\u4f73\u8a5e\u5f59\u91cf\u5927\u5c0f\u53ef\u4ee5\u6301\u7e8c\u6539\u5584\u4e0b\u6e38\u6548\u80fd\n\u8d85\u904e\u5e38\u7528\u7684\u8a5e\u5f59\u91cf\u5927\u5c0f\u3002\u900f\u904e\u5c07\u8a5e\u5f59\u91cf\u5927\u5c0f\u5f9e\u50b3\u7d71\u7684 32K \u589e\u52a0\u5230 43K\uff0c\u6211\u5011\n\u4f7f\u7528\u76f8\u540c\u7684 2.3e21 FLOPs \u5c07 ARC-Challenge \u4e0a\u7684\u6548\u80fd\u5f9e 29.1 \u63d0\u5347\u5230 32.0\u3002\u6211\u5011\u7684\u7814\u7a76\u5f37\u8abf\u4e86\u5728\u6709\u6548\u64f4\u5145\u4e2d\u5171\u540c\u8003\u91cf\u6a21\u578b\n\u53c3\u6578\u548c\u8a5e\u5f59\u91cf\u5927\u5c0f\u7684\u5fc5\u8981\u6027\u3002", "author": "Chaofan Tao et.al.", "authors": "Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, Ngai Wong", "id": "2407.13623v1", "paper_url": "http://arxiv.org/abs/2407.13623v1", "repo": "null"}}