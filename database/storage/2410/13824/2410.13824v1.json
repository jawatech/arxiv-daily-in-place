{"2410.13824": {"publish_time": "2024-10-17", "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding", "paper_summary": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48\\% improvement on\nVisualWebBench and a 19.1\\% boost in action accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.", "paper_summary_zh": "\u6587\u672c\u4e30\u5bcc\u7684\u89c6\u89c9\u7406\u89e3\u2014\u2014\u5904\u7406\u5bc6\u96c6\u6587\u672c\u5185\u5bb9\u4e0e\u89c6\u89c9\u5143\u7d20\u76f8\u7ed3\u5408\u7684\u73af\u5883\u7684\u80fd\u529b\u2014\u2014\u5bf9\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u4e0e\u7ed3\u6784\u5316\u73af\u5883\u6709\u6548\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u589e\u5f3a\u6b64\u529f\u80fd\uff0c\u6211\u4eec\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ece\u7f51\u9875 UI \u4e2d\u5408\u6210\u901a\u7528\u7684\u591a\u6a21\u6001\u6307\u4ee4\u3002\u5c3d\u7ba1\u7f3a\u4e4f\u76f4\u63a5\u7684\u89c6\u89c9\u8f93\u5165\uff0c\u4f46\u57fa\u4e8e\u6587\u672c\u7684 LLM \u80fd\u591f\u5904\u7406\u6765\u81ea\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u6811\u7684\u7ed3\u6784\u5316\u6587\u672c\u8868\u793a\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u6307\u4ee4\u4e0e UI \u5c4f\u5e55\u622a\u56fe\u914d\u5bf9\uff0c\u4ee5\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u3002\u6211\u4eec\u5f15\u5165\u4e86 MultiUI\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u6765\u81ea 100 \u4e07\u4e2a\u7f51\u7ad9\u7684 730 \u4e07\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u591a\u6a21\u6001\u4efb\u52a1\u548c UI \u5e03\u5c40\u3002\u5728 MultiUI \u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4e0d\u4ec5\u5728 Web UI \u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u2014\u2014\u5728 VisualWebBench \u4e0a\u7684\u6539\u8fdb\u5e45\u5ea6\u9ad8\u8fbe 48%\uff0c\u5728 Web \u4ee3\u7406\u6570\u636e\u96c6 Mind2Web \u4e0a\u7684\u52a8\u4f5c\u51c6\u786e\u5ea6\u63d0\u5347\u4e86 19.1%\u2014\u2014\u800c\u4e14\u5728\u975e Web UI \u4efb\u52a1\u751a\u81f3\u975e UI \u9886\u57df\uff08\u4f8b\u5982\u6587\u6863\u7406\u89e3\u3001OCR \u548c\u56fe\u8868\u89e3\u91ca\uff09\u4e2d\u4e5f\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86 Web UI \u6570\u636e\u5728\u5404\u79cd\u573a\u666f\u4e2d\u63a8\u8fdb\u6587\u672c\u4e30\u5bcc\u7684\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "author": "Junpeng Liu et.al.", "authors": "Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue", "id": "2410.13824v1", "paper_url": "http://arxiv.org/abs/2410.13824v1", "repo": "null"}}