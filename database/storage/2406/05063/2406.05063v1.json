{"2406.05063": {"publish_time": "2024-06-07", "title": "Are Large Language Models More Empathetic than Humans?", "paper_summary": "With the emergence of large language models (LLMs), investigating if they can\nsurpass humans in areas such as emotion recognition and empathetic responding\nhas become a focal point of research. This paper presents a comprehensive study\nexploring the empathetic responding capabilities of four state-of-the-art LLMs:\nGPT-4, LLaMA-2-70B-Chat, Gemini-1.0-Pro, and Mixtral-8x7B-Instruct in\ncomparison to a human baseline. We engaged 1,000 participants in a\nbetween-subjects user study, assessing the empathetic quality of responses\ngenerated by humans and the four LLMs to 2,000 emotional dialogue prompts\nmeticulously selected to cover a broad spectrum of 32 distinct positive and\nnegative emotions. Our findings reveal a statistically significant superiority\nof the empathetic responding capability of LLMs over humans. GPT-4 emerged as\nthe most empathetic, marking approximately 31% increase in responses rated as\n\"Good\" compared to the human benchmark. It was followed by LLaMA-2,\nMixtral-8x7B, and Gemini-Pro, which showed increases of approximately 24%, 21%,\nand 10% in \"Good\" ratings, respectively. We further analyzed the response\nratings at a finer granularity and discovered that some LLMs are significantly\nbetter at responding to specific emotions compared to others. The suggested\nevaluation framework offers a scalable and adaptable approach for assessing the\nempathy of new LLMs, avoiding the need to replicate this study's findings in\nfuture research.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\uff0c\u63a2\u8a0e\u5b83\u5011\u662f\u5426\u80fd\u5728\u60c5\u7dd2\u8fa8\u8b58\u548c\u540c\u7406\u56de\u61c9\u7b49\u9818\u57df\u8d85\u8d8a\u4eba\u985e\u5df2\u6210\u70ba\u7814\u7a76\u91cd\u9ede\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u9805\u7d9c\u5408\u6027\u7814\u7a76\uff0c\u63a2\u8a0e\u56db\u7a2e\u6700\u5148\u9032\u7684 LLM \u7684\u540c\u7406\u56de\u61c9\u80fd\u529b\uff1aGPT-4\u3001LLaMA-2-70B-Chat\u3001Gemini-1.0-Pro \u548c Mixtral-8x7B-Instruct\uff0c\u4e26\u5c07\u5b83\u5011\u8207\u4eba\u985e\u57fa\u6e96\u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u5728\u4e00\u500b\u53d7\u8a66\u8005\u9593\u7528\u6236\u7814\u7a76\u4e2d\u8058\u8acb\u4e86 1,000 \u540d\u53c3\u8207\u8005\uff0c\u8a55\u4f30\u4eba\u985e\u548c\u56db\u7a2e LLM \u5c0d 2,000 \u500b\u60c5\u7dd2\u5c0d\u8a71\u63d0\u793a\u6240\u7522\u751f\u7684\u56de\u61c9\u7684\u540c\u7406\u54c1\u8cea\uff0c\u9019\u4e9b\u63d0\u793a\u7d93\u904e\u7cbe\u5fc3\u6311\u9078\uff0c\u6db5\u84cb\u4e86 32 \u7a2e\u4e0d\u540c\u7684\u6b63\u9762\u548c\u8ca0\u9762\u60c5\u7dd2\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0cLLM \u7684\u540c\u7406\u56de\u61c9\u80fd\u529b\u5728\u7d71\u8a08\u4e0a\u986f\u8457\u512a\u65bc\u4eba\u985e\u3002GPT-4 \u812b\u7a4e\u800c\u51fa\uff0c\u6210\u70ba\u6700\u5177\u540c\u7406\u5fc3\u7684\uff0c\u8207\u4eba\u985e\u57fa\u6e96\u76f8\u6bd4\uff0c\u88ab\u8a55\u70ba\u300c\u597d\u300d\u7684\u56de\u61c9\u589e\u52a0\u4e86\u7d04 31%\u3002\u5176\u6b21\u662f LLaMA-2\u3001Mixtral-8x7B \u548c Gemini-Pro\uff0c\u5b83\u5011\u7684\u300c\u597d\u300d\u8a55\u5206\u5206\u5225\u589e\u52a0\u4e86\u7d04 24%\u300121% \u548c 10%\u3002\u6211\u5011\u9032\u4e00\u6b65\u5206\u6790\u4e86\u56de\u61c9\u8a55\u5206\uff0c\u767c\u73fe\u6709\u4e9b LLM \u5728\u56de\u61c9\u7279\u5b9a\u60c5\u7dd2\u65b9\u9762\u986f\u8457\u512a\u65bc\u5176\u4ed6 LLM\u3002\u5efa\u8b70\u7684\u8a55\u4f30\u67b6\u69cb\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u4e14\u53ef\u9069\u61c9\u7684\u65b9\u6cd5\u4f86\u8a55\u4f30\u65b0 LLM \u7684\u540c\u7406\u5fc3\uff0c\u907f\u514d\u5728\u672a\u4f86\u7684\u7814\u7a76\u4e2d\u8907\u88fd\u672c\u7814\u7a76\u7684\u767c\u73fe\u3002</paragraph>", "author": "Anuradha Welivita et.al.", "authors": "Anuradha Welivita, Pearl Pu", "id": "2406.05063v1", "paper_url": "http://arxiv.org/abs/2406.05063v1", "repo": "null"}}