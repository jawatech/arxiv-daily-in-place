{"2405.02814": {"publish_time": "2024-05-05", "title": "NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli", "paper_summary": "Large Language Models (LLMs) have become integral to a wide spectrum of\napplications, ranging from traditional computing tasks to advanced artificial\nintelligence (AI) applications. This widespread adoption has spurred extensive\nresearch into LLMs across various disciplines, including the social sciences.\nNotably, studies have revealed that LLMs possess emotional intelligence, which\ncan be further developed through positive emotional stimuli. This discovery\nraises an intriguing question: can negative emotions similarly influence LLMs,\npotentially enhancing their performance? In response to this question, we\nintroduce NegativePrompt, a novel approach underpinned by psychological\nprinciples, involving ten specifically designed negative emotional stimuli. We\nembark on rigorous experimental evaluations of five LLMs including\nFlan-T5-Large, Vicuna, Llama 2, ChatGPT, and GPT-4, across a set of 45 tasks.\nThe results are revealing: NegativePrompt markedly enhances the performance of\nLLMs, evidenced by relative improvements of 12.89% in Instruction Induction\ntasks and 46.25% in BIG-Bench tasks. Moreover, we conduct attention\nvisualization experiments to decipher the underlying mechanisms of\nNegativePrompt's influence. Our research contributes significantly to the\nunderstanding of LLMs and emotion interaction, demonstrating the practical\nefficacy of NegativePrompt as an emotion-driven method and offering novel\ninsights for the enhancement of LLMs in real-world applications. The code is\navailable at https://github.com/wangxu0820/NegativePrompt.", "paper_summary_zh": "", "author": "Xu Wang et.al.", "authors": "Xu Wang,Cheng Li,Yi Chang,Jindong Wang,Yuan Wu", "id": "2405.02814v1", "paper_url": "http://arxiv.org/abs/2405.02814v1", "repo": "https://github.com/wangxu0820/negativeprompt"}}