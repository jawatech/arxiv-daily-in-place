{"2501.13264": {"publish_time": "2025-01-22", "title": "RAG-Reward: Optimizing RAG with Reward Modeling and RLHF", "paper_summary": "Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs)\nwith relevant and up-to-date knowledge, improving their ability to answer\nknowledge-intensive questions. It has been shown to enhance both generation\nquality and trustworthiness. While numerous works have focused on improving\nretrieval, generation, and evaluation, the role of reward models in\nreinforcement learning for optimizing RAG and establishing automated\nbenchmarking pipelines remains underexplored. In this paper, we introduce\n\\textbf{RAG-Reward}, a dataset designed to enable \\textit{hallucination-free,\ncomprehensive, reliable, and efficient RAG}. We define four key metrics for\nassessing generation quality and develop an automated annotation pipeline that\nleverages multiple LLMs to generate outputs across diverse RAG scenarios.\nGPT-4o is used to evaluate and construct preference data. Using\n\\textbf{RAG-Reward}, we train reward models and apply reinforcement learning\nwith human feedback (RLHF) to improve LLMs' effectiveness in RAG. Experimental\nresults show that our reward model achieves state-of-the-art performance on a\nheld-out test set, demonstrating both the effectiveness of our approach and the\nquality of our dataset. Furthermore, the improved generation quality of the\ntrained policy model highlights the feasibility of using RLHF to enhance RAG\npipelines.", "paper_summary_zh": "<paragraph>\u64b7\u53d6\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5f3a\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\n\u5177\u5907\u76f8\u5173\u4e14\u6700\u65b0\u7684\u77e5\u8bc6\uff0c\u63d0\u5347\u5176\u56de\u7b54\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u9898\u7684\u80fd\u8010\u3002\u5b83\u5df2\u88ab\u8bc1\u660e\u80fd\u540c\u65f6\u5f3a\u5316\u751f\u6210\n\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\u3002\u867d\u7136\u8bb8\u591a\u7814\u7a76\u4e13\u6ce8\u4e8e\u6539\u8fdb\u64b7\u53d6\u3001\u751f\u6210\u548c\u8bc4\u4f30\uff0c\u4f46\u5956\u52b1\u6a21\u578b\u5728\n\u5f3a\u5316\u5b66\u4e60\u4e2d\u626e\u6f14\u7684\u89d2\u8272\uff0c\u4ee5\u4f18\u5316 RAG \u548c\u5efa\u7acb\u81ea\u52a8\u5316\u57fa\u51c6\u7ba1\u9053\u4ecd\u672a\u53d7\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\n\\textbf{RAG-Reward}\uff0c\u4e00\u4e2a\u6570\u636e\u96c6\u65e8\u5728\u5b9e\u73b0\\textit{\u65e0\u5e7b\u89c9\u3001\u5168\u9762\u3001\u53ef\u9760\u4e14\u9ad8\u6548\u7684 RAG}\u3002\u6211\u4eec\u5b9a\u4e49\u4e86\u56db\u4e2a\u5173\u952e\u6307\u6807\u6765\n\u8bc4\u4f30\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6ce8\u91ca\u7ba1\u9053\uff0c\u5229\u7528\u591a\u4e2a LLM \u5728\u4e0d\u540c\u7684 RAG \u573a\u666f\u4e2d\u751f\u6210\u8f93\u51fa\u3002\nGPT-4o \u7528\u4e8e\u8bc4\u4f30\u548c\u6784\u5efa\u504f\u597d\u6570\u636e\u3002\u4f7f\u7528\n\\textbf{RAG-Reward}\uff0c\u6211\u4eec\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u5e76\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\n\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u6765\u63d0\u5347 LLM \u5728 RAG \u4e2d\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\n\u7ed3\u679c\u663e\u793a\u6211\u4eec\u7684\u5956\u52b1\u6a21\u578b\u5728\u7559\u5b58\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4ee5\u53ca\n\u6211\u4eec\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u597d\u7684\u7b56\u7565\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u63d0\u5347\u7a81\u663e\u4e86\u4f7f\u7528 RLHF \u589e\u5f3a RAG\n\u7ba1\u9053\u7684\u53ef\u884c\u6027\u3002</paragraph>", "author": "Hanning Zhang et.al.", "authors": "Hanning Zhang, Juntong Song, Juno Zhu, Yuanhao Wu, Tong Zhang, Cheng Niu", "id": "2501.13264v1", "paper_url": "http://arxiv.org/abs/2501.13264v1", "repo": "null"}}