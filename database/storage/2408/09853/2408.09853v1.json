{"2408.09853": {"publish_time": "2024-08-19", "title": "Self-Directed Turing Test for Large Language Models", "paper_summary": "The Turing test examines whether AIs can exhibit human-like behaviour in\nnatural language conversations. Traditional Turing tests adopt a rigid dialogue\nformat where each participant sends only one message each time and require\ncontinuous human involvement to direct the entire interaction with the test\nsubject. This fails to reflect a natural conversational style and hinders the\nevaluation of Large Language Models (LLMs) in complex and prolonged dialogues.\nThis paper proposes the Self-Directed Turing Test, which extends the original\ntest with a burst dialogue format, allowing more dynamic exchanges by multiple\nconsecutive messages. It further efficiently reduces human workload by having\nthe LLM self-direct the majority of the test process, iteratively generating\ndialogues that simulate its interaction with humans. With the pseudo-dialogue\nhistory, the model then engages in a shorter dialogue with a human, which is\npaired with a human-human conversation on the same topic to be judged using\nquestionnaires. We introduce the X-Turn Pass-Rate metric to assess the human\nlikeness of LLMs across varying durations. While LLMs like GPT-4 initially\nperform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10\nturns of dialogues respectively, their performance drops as the dialogue\nprogresses, which underscores the difficulty in maintaining consistency in the\nlong term.", "paper_summary_zh": "\u5716\u9748\u6e2c\u8a66\u63a2\u8a0e AI \u662f\u5426\u80fd\u5728\u81ea\u7136\u8a9e\u8a00\u5c0d\u8a71\u4e2d\u8868\u73fe\u51fa\u985e\u4f3c\u4eba\u985e\u7684\u884c\u70ba\u3002\u50b3\u7d71\u7684\u5716\u9748\u6e2c\u8a66\u63a1\u7528\u56b4\u683c\u7684\u5c0d\u8a71\u683c\u5f0f\uff0c\u5176\u4e2d\u6bcf\u500b\u53c3\u8207\u8005\u6bcf\u6b21\u53ea\u767c\u9001\u4e00\u689d\u8a0a\u606f\uff0c\u4e26\u4e14\u9700\u8981\u6301\u7e8c\u7684\u4eba\u70ba\u4ecb\u5165\u4f86\u6307\u5c0e\u8207\u53d7\u8a66\u8005\u7684\u6574\u500b\u4e92\u52d5\u3002\u9019\u7121\u6cd5\u53cd\u6620\u81ea\u7136\u7684\u5c0d\u8a71\u98a8\u683c\uff0c\u4e26\u963b\u7919\u4e86\u5728\u8907\u96dc\u4e14\u6301\u4e45\u7684\u5c0d\u8a71\u4e2d\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u5c0e\u5f0f\u5716\u9748\u6e2c\u8a66\uff0c\u5b83\u901a\u904e\u7206\u767c\u5c0d\u8a71\u683c\u5f0f\u64f4\u5c55\u4e86\u539f\u59cb\u6e2c\u8a66\uff0c\u5141\u8a31\u901a\u904e\u591a\u689d\u9023\u7e8c\u8a0a\u606f\u9032\u884c\u66f4\u52d5\u614b\u7684\u4ea4\u6d41\u3002\u5b83\u9032\u4e00\u6b65\u901a\u904e\u8b93 LLM \u81ea\u6211\u6307\u5c0e\u5927\u90e8\u5206\u6e2c\u8a66\u904e\u7a0b\u4f86\u6709\u6548\u6e1b\u5c11\u4eba\u985e\u5de5\u4f5c\u91cf\uff0c\u53cd\u8986\u751f\u6210\u6a21\u64ec\u5176\u8207\u4eba\u985e\u4e92\u52d5\u7684\u5c0d\u8a71\u3002\u6709\u4e86\u507d\u5c0d\u8a71\u6b77\u53f2\uff0c\u8a72\u6a21\u578b\u96a8\u5f8c\u8207\u4eba\u985e\u9032\u884c\u8f03\u77ed\u7684\u5c0d\u8a71\uff0c\u8a72\u5c0d\u8a71\u8207\u540c\u4e00\u500b\u4e3b\u984c\u7684\u4eba\u985e\u5c0d\u8a71\u914d\u5c0d\uff0c\u4e26\u4f7f\u7528\u554f\u5377\u9032\u884c\u5224\u65b7\u3002\u6211\u5011\u5f15\u5165\u4e86 X-Turn \u901a\u904e\u7387\u6307\u6a19\u4f86\u8a55\u4f30 LLM \u5728\u4e0d\u540c\u6301\u7e8c\u6642\u9593\u5167\u7684\u985e\u4eba\u7a0b\u5ea6\u3002\u96d6\u7136\u50cf GPT-4 \u9019\u6a23\u7684 LLM \u6700\u521d\u8868\u73fe\u826f\u597d\uff0c\u5728 3 \u8f2a\u548c 10 \u8f2a\u5c0d\u8a71\u4e2d\u5206\u5225\u9054\u5230 51.9% \u548c 38.9% \u7684\u901a\u904e\u7387\uff0c\u4f46\u96a8\u8457\u5c0d\u8a71\u7684\u9032\u884c\uff0c\u5b83\u5011\u7684\u8868\u73fe\u6703\u4e0b\u964d\uff0c\u9019\u7a81\u986f\u4e86\u9577\u671f\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u96e3\u5ea6\u3002", "author": "Weiqi Wu et.al.", "authors": "Weiqi Wu, Hongqiu Wu, Hai Zhao", "id": "2408.09853v1", "paper_url": "http://arxiv.org/abs/2408.09853v1", "repo": "null"}}