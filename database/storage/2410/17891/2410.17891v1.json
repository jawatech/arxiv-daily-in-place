{"2410.17891": {"publish_time": "2024-10-23", "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models", "paper_summary": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for\ntext generative modeling, potentially addressing limitations of autoregressive\n(AR) models. However, current DLMs have been studied at a smaller scale\ncompared to their AR counterparts and lack fair comparison on language modeling\nbenchmarks. Additionally, training diffusion models from scratch at scale\nremains challenging. Given the prevalence of open-source AR language models, we\npropose adapting these models to build text diffusion models. We demonstrate\nconnections between AR and diffusion modeling objectives and introduce a simple\ncontinual pre-training approach for training diffusion models. Through\nsystematic evaluation on language modeling, reasoning, and commonsense\nbenchmarks, we show that we can convert AR models ranging from 127M to 7B\nparameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA,\nusing less than 200B tokens for training. Our experimental results reveal that\nthese models outperform earlier DLMs and are competitive with their AR\ncounterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters)\ncapable of generating fluent text, performing in-context learning, filling in\nthe middle without prompt re-ordering, and following instructions\n\\url{https://github.com/HKUNLP/DiffuLLaMA}.", "paper_summary_zh": "\u64f4\u6563\u8a9e\u8a00\u6a21\u578b (DLM) \u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u524d\u666f\u7684\u65b0\u7bc4\u4f8b\uff0c\u7528\u65bc\u6587\u5b57\u751f\u6210\u6a21\u578b\uff0c\u53ef\u80fd\u6703\u89e3\u6c7a\u81ea\u56de\u6b78 (AR) \u6a21\u578b\u7684\u9650\u5236\u3002\u7136\u800c\uff0c\u8207 AR \u5c0d\u61c9\u6a21\u578b\u76f8\u6bd4\uff0c\u76ee\u524d DLM \u7684\u7814\u7a76\u898f\u6a21\u8f03\u5c0f\uff0c\u4e14\u5728\u8a9e\u8a00\u6a21\u578b\u57fa\u6e96\u4e0a\u7f3a\u4e4f\u516c\u5e73\u7684\u6bd4\u8f03\u3002\u6b64\u5916\uff0c\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u64f4\u6563\u6a21\u578b\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u9451\u65bc\u958b\u6e90 AR \u8a9e\u8a00\u6a21\u578b\u7684\u666e\u904d\u6027\uff0c\u6211\u5011\u5efa\u8b70\u8abf\u6574\u9019\u4e9b\u6a21\u578b\u4ee5\u5efa\u7acb\u6587\u5b57\u64f4\u6563\u6a21\u578b\u3002\u6211\u5011\u5c55\u793a\u4e86 AR \u8207\u64f4\u6563\u5efa\u6a21\u76ee\u6a19\u4e4b\u9593\u7684\u95dc\u806f\uff0c\u4e26\u4ecb\u7d39\u4e86\u4e00\u7a2e\u7c21\u55ae\u7684\u6301\u7e8c\u9810\u8a13\u7df4\u65b9\u6cd5\uff0c\u7528\u65bc\u8a13\u7df4\u64f4\u6563\u6a21\u578b\u3002\u900f\u904e\u5728\u8a9e\u8a00\u5efa\u6a21\u3001\u63a8\u7406\u548c\u5e38\u8b58\u57fa\u6e96\u4e0a\u7684\u7cfb\u7d71\u6027\u8a55\u4f30\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u53ef\u4ee5\u5c07\u53c3\u6578\u5f9e 127M \u5230 7B \u7684 AR \u6a21\u578b\uff08GPT2 \u548c LLaMA\uff09\u8f49\u63db\u70ba\u64f4\u6563\u6a21\u578b DiffuGPT \u548c DiffuLLaMA\uff0c\u4f7f\u7528\u4e0d\u5230 200B \u500b token \u9032\u884c\u8a13\u7df4\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u9019\u4e9b\u6a21\u578b\u512a\u65bc\u65e9\u671f\u7684 DLM\uff0c\u4e26\u4e14\u8207\u5176 AR \u5c0d\u61c9\u6a21\u578b\u5177\u6709\u7af6\u722d\u529b\u3002\u6211\u5011\u767c\u5e03\u4e86\u4e00\u5957 DLM\uff08\u5177\u6709 127M\u3001355M \u548c 7B \u53c3\u6578\uff09\uff0c\u80fd\u5920\u751f\u6210\u6d41\u5229\u7684\u6587\u5b57\u3001\u9032\u884c\u60c5\u5883\u5b78\u7fd2\u3001\u5728\u4e0d\u91cd\u65b0\u6392\u5e8f\u63d0\u793a\u7684\u60c5\u6cc1\u4e0b\u586b\u88dc\u4e2d\u9593\u90e8\u5206\uff0c\u4e26\u9075\u5faa\u6307\u4ee4 https://github.com/HKUNLP/DiffuLLaMA\u3002", "author": "Shansan Gong et.al.", "authors": "Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong", "id": "2410.17891v1", "paper_url": "http://arxiv.org/abs/2410.17891v1", "repo": "https://github.com/hkunlp/diffullama"}}