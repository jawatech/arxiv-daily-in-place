{"2406.16554": {"publish_time": "2024-06-24", "title": "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training", "paper_summary": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising\nframework for scaling up large language models (LLMs). However, training MoE\nfrom scratch in a large-scale setting still suffers from data-hungry and\ninstability problems. Motivated by this limit, we investigate building MoE\nmodels from existing dense large language models. Specifically, based on the\nwell-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert\nConstruction, which partitions the parameters of original Feed-Forward Networks\n(FFNs) into multiple experts; (2) Continual Pre-training, which further trains\nthe transformed MoE model and additional gate networks. In this paper, we\ncomprehensively explore different methods for expert construction and various\ndata sampling strategies for continual pre-training. After these stages, our\nLLaMA-MoE models could maintain language abilities and route the input tokens\nto specific experts with part of the parameters activated. Empirically, by\ntraining 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense\nmodels that contain similar activation parameters. The source codes and models\nare available at https://github.com/pjlab-sys4nlp/llama-moe .", "paper_summary_zh": "\u6df7\u5408\u4e13\u5bb6 (MoE) \u4f5c\u70ba\u4e00\u7a2e\u6709\u524d\u9014\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u64f4\u5c55\u6846\u67b6\uff0c\u7372\u5f97\u8d8a\u4f86\u8d8a\u9ad8\u7684\u95dc\u6ce8\u5ea6\u3002\u7136\u800c\uff0c\u5f9e\u982d\u958b\u59cb\u5728\u5927\u578b\u74b0\u5883\u4e2d\u8a13\u7df4 MoE \u4ecd\u7136\u6703\u9047\u5230\u8cc7\u6599\u9700\u6c42\u5927\u4e14\u4e0d\u7a69\u5b9a\u7684\u554f\u984c\u3002\u57fa\u65bc\u6b64\u9650\u5236\uff0c\u6211\u5011\u63a2\u8a0e\u5f9e\u73fe\u6709\u7684\u7a20\u5bc6\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\u5efa\u7acb MoE \u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u57fa\u65bc\u8457\u540d\u7684 LLaMA-2 7B \u6a21\u578b\uff0c\u6211\u5011\u900f\u904e\u4ee5\u4e0b\u65b9\u5f0f\u53d6\u5f97 MoE \u6a21\u578b\uff1a(1) \u5c08\u5bb6\u69cb\u9020\uff0c\u5c07\u539f\u59cb\u524d\u994b\u7db2\u8def (FFN) \u7684\u53c3\u6578\u5206\u5272\u6210\u591a\u500b\u5c08\u5bb6\uff1b(2) \u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u9032\u4e00\u6b65\u8a13\u7df4\u8f49\u63db\u5f8c\u7684 MoE \u6a21\u578b\u548c\u984d\u5916\u7684\u9598\u7db2\u8def\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5168\u9762\u63a2\u8a0e\u4e86\u5c08\u5bb6\u69cb\u9020\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u4ee5\u53ca\u6301\u7e8c\u9810\u8a13\u7df4\u7684\u4e0d\u540c\u8cc7\u6599\u53d6\u6a23\u7b56\u7565\u3002\u5728\u9019\u4e9b\u968e\u6bb5\u4e4b\u5f8c\uff0c\u6211\u5011\u7684 LLaMA-MoE \u6a21\u578b\u53ef\u4ee5\u7dad\u8b77\u8a9e\u8a00\u80fd\u529b\uff0c\u4e26\u5c07\u8f38\u5165\u7b26\u865f\u8def\u7531\u5230\u7279\u5b9a\u5c08\u5bb6\uff0c\u4e26\u555f\u7528\u90e8\u5206\u53c3\u6578\u3002\u6839\u64da\u7d93\u9a57\uff0c\u900f\u904e\u8a13\u7df4 200B \u500b\u7b26\u865f\uff0cLLaMA-MoE-3.5B \u6a21\u578b\u660e\u986f\u512a\u65bc\u5305\u542b\u985e\u4f3c\u555f\u7528\u53c3\u6578\u7684\u7a20\u5bc6\u6a21\u578b\u3002\u539f\u59cb\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u4ee5\u5728 https://github.com/pjlab-sys4nlp/llama-moe \u53d6\u5f97\u3002", "author": "Tong Zhu et.al.", "authors": "Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, Yu Cheng", "id": "2406.16554v1", "paper_url": "http://arxiv.org/abs/2406.16554v1", "repo": "https://github.com/pjlab-sys4nlp/llama-moe"}}