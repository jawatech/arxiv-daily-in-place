{"2501.08523": {"publish_time": "2025-01-15", "title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation", "paper_summary": "The field of artificial intelligence has witnessed significant advancements\nin natural language processing, largely attributed to the capabilities of Large\nLanguage Models (LLMs). These models form the backbone of Agents designed to\naddress long-context dependencies, particularly in Document-level Machine\nTranslation (DocMT). DocMT presents unique challenges, with quality,\nconsistency, and fluency being the key metrics for evaluation. Existing\napproaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise\nfluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an\nincremental sentence-level forced decoding strategy \\textbf{to ensure every\nsentence is translated while enhancing the fluency of adjacent sentences.} Our\nAgent leverages a Doc-Guided Memory, focusing solely on the summary and its\ntranslation, which we find to be an efficient approach to maintaining\nconsistency. Through extensive testing across multiple languages and domains,\nwe demonstrate that Sent2Sent++ outperforms other methods in terms of quality,\nconsistency, and fluency. The results indicate that, our approach has achieved\nsignificant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and\ndocument-level perplexity (d-ppl). The contributions of this paper include a\ndetailed analysis of current DocMT research, the introduction of the\nSent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of\nits effectiveness across languages and domains.", "paper_summary_zh": "\u4eba\u5de5\u667a\u80fd\u9886\u57df\u89c1\u8bc1\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u8fd9\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5f52\u529f\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u80fd\u529b\u3002\u8fd9\u4e9b\u6a21\u578b\u6784\u6210\u4e86\u4ee3\u7406\u7684\u57fa\u7840\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u5c24\u5176\u662f\u5728\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1 (DocMT) \u4e2d\u3002DocMT \u5448\u73b0\u51fa\u72ec\u7279\u7684\u6311\u6218\uff0c\u5176\u4e2d\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u548c\u6d41\u7545\u6027\u662f\u8bc4\u4f30\u7684\u5173\u952e\u6307\u6807\u3002\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982 Doc2Doc \u548c Doc2Sent\uff0c\u8981\u4e48\u7701\u7565\u53e5\u5b50\uff0c\u8981\u4e48\u5f71\u54cd\u6d41\u7545\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86 Doc-Guided Sent2Sent++\uff0c\u8fd9\u662f\u4e00\u4e2a\u91c7\u7528\u589e\u91cf\u53e5\u5b50\u7ea7\u5f3a\u5236\u89e3\u7801\u7b56\u7565\u7684\u4ee3\u7406\uff0c\u4ee5\u786e\u4fdd\u7ffb\u8bd1\u6bcf\u4e2a\u53e5\u5b50\uff0c\u540c\u65f6\u589e\u5f3a\u76f8\u90bb\u53e5\u5b50\u7684\u6d41\u7545\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7406\u5229\u7528 Doc-Guided Memory\uff0c\u4ec5\u5173\u6ce8\u6458\u8981\u53ca\u5176\u7ffb\u8bd1\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u662f\u4e00\u79cd\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002\u901a\u8fc7\u8de8\u591a\u4e2a\u8bed\u8a00\u548c\u9886\u57df\u7684\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u6211\u4eec\u8bc1\u660e Sent2Sent++ \u5728\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u548c\u6d41\u7545\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 s-COMET\u3001d-COMET\u3001LTCR-$1_f$ \u548c\u6587\u6863\u7ea7\u56f0\u60d1\u5ea6 (d-ppl) \u7b49\u6307\u6807\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\u3002\u672c\u6587\u7684\u8d21\u732e\u5305\u62ec\u5bf9\u5f53\u524d DocMT \u7814\u7a76\u7684\u8be6\u7ec6\u5206\u6790\u3001Sent2Sent++ \u89e3\u7801\u65b9\u6cd5\u7684\u4ecb\u7ecd\u3001Doc-Guided Memory \u673a\u5236\uff0c\u4ee5\u53ca\u8de8\u8bed\u8a00\u548c\u9886\u57df\u7684\u6709\u6548\u6027\u9a8c\u8bc1\u3002", "author": "Jiaxin Guo et.al.", "authors": "Jiaxin Guo, Yuanchang Luo, Daimeng Wei, Ling Zhang, Zongyao Li, Hengchao Shang, Zhiqiang Rao, Shaojun Li, Jinlong Yang, Zhanglin Wu, Hao Yang", "id": "2501.08523v1", "paper_url": "http://arxiv.org/abs/2501.08523v1", "repo": "null"}}