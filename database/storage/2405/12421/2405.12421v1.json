{"2405.12421": {"publish_time": "2024-05-20", "title": "A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback", "paper_summary": "Inverse Reinforcement Learning (IRL) and Reinforcement Learning from Human\nFeedback (RLHF) are pivotal methodologies in reward learning, which involve\ninferring and shaping the underlying reward function of sequential\ndecision-making problems based on observed human demonstrations and feedback.\nMost prior work in reward learning has relied on prior knowledge or assumptions\nabout decision or preference models, potentially leading to robustness issues.\nIn response, this paper introduces a novel linear programming (LP) framework\ntailored for offline reward learning. Utilizing pre-collected trajectories\nwithout online exploration, this framework estimates a feasible reward set from\nthe primal-dual optimality conditions of a suitably designed LP, and offers an\noptimality guarantee with provable sample efficiency. Our LP framework also\nenables aligning the reward functions with human feedback, such as pairwise\ntrajectory comparison data, while maintaining computational tractability and\nsample efficiency. We demonstrate that our framework potentially achieves\nbetter performance compared to the conventional maximum likelihood estimation\n(MLE) approach through analytical examples and numerical experiments.", "paper_summary_zh": "\u9006\u5411\u5f37\u5316\u5b78\u7fd2 (IRL) \u8207\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u662f\u734e\u52f5\u5b78\u7fd2\u4e2d\u7684\u95dc\u9375\u65b9\u6cd5\uff0c\u6d89\u53ca\u6839\u64da\u89c0\u5bdf\u5230\u7684\u4eba\u985e\u793a\u7bc4\u548c\u56de\u994b\u63a8\u8ad6\u548c\u5851\u9020\u5e8f\u8cab\u6c7a\u7b56\u554f\u984c\u7684\u57fa\u790e\u734e\u52f5\u51fd\u6578\u3002\u5927\u591a\u6578\u5148\u524d\u7684\u734e\u52f5\u5b78\u7fd2\u5de5\u4f5c\u4f9d\u8cf4\u65bc\u6c7a\u7b56\u6216\u504f\u597d\u6a21\u578b\u7684\u5148\u9a57\u77e5\u8b58\u6216\u5047\u8a2d\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u7a69\u5065\u6027\u554f\u984c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u500b\u91dd\u5c0d\u96e2\u7dda\u734e\u52f5\u5b78\u7fd2\u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u578b\u7dda\u6027\u898f\u5283 (LP) \u6846\u67b6\u3002\u5229\u7528\u9810\u5148\u6536\u96c6\u7684\u8ecc\u8de1\u800c\u7121\u9700\u5728\u7dda\u63a2\u7d22\uff0c\u6b64\u6846\u67b6\u5f9e\u9069\u7576\u8a2d\u8a08\u7684 LP \u7684\u539f\u59cb\u5c0d\u5076\u6700\u512a\u689d\u4ef6\u4f30\u8a08\u4e00\u500b\u53ef\u884c\u7684\u734e\u52f5\u96c6\uff0c\u4e26\u63d0\u4f9b\u5177\u6709\u53ef\u8b49\u660e\u6a23\u672c\u6548\u7387\u7684\u6700\u512a\u4fdd\u8b49\u3002\u6211\u5011\u7684 LP \u6846\u67b6\u9084\u80fd\u5920\u5c07\u734e\u52f5\u51fd\u6578\u8207\u4eba\u985e\u56de\u994b\u5c0d\u9f4a\uff0c\u4f8b\u5982\u6210\u5c0d\u8ecc\u8de1\u6bd4\u8f03\u6578\u64da\uff0c\u540c\u6642\u4fdd\u6301\u8a08\u7b97\u53ef\u8655\u7406\u6027\u548c\u6a23\u672c\u6548\u7387\u3002\u6211\u5011\u8b49\u660e\uff0c\u8207\u50b3\u7d71\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8a08 (MLE) \u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6846\u67b6\u6709\u53ef\u80fd\u901a\u904e\u5206\u6790\u7bc4\u4f8b\u548c\u6578\u503c\u5be6\u9a57\u5be6\u73fe\u66f4\u597d\u7684\u6027\u80fd\u3002", "author": "Kihyun Kim et.al.", "authors": "Kihyun Kim, Jiawei Zhang, Pablo A. Parrilo, Asuman Ozdaglar", "id": "2405.12421v1", "paper_url": "http://arxiv.org/abs/2405.12421v1", "repo": "null"}}