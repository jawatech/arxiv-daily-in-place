{"2408.13909": {"publish_time": "2024-08-25", "title": "LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages in Multimodal Image Retrieval Task", "paper_summary": "This research explores the development of multimodal vision-language models\nfor image retrieval in low-resource languages, specifically Azerbaijani.\nExisting vision-language models primarily support high-resource languages, and\nfine-tuning them remains computationally demanding. To address challenges in\nvision-language retrieval for low-resource languages, we integrated the CLIP\nmodel architecture and employed several techniques to balance computational\nefficiency with performance. These techniques include synthetic data generation\nthrough machine translation, image augmentation, and further training the\nattention mechanisms of transformer-based models with domain-specific data. We\nintegrated Multilingual BERT as a text encoder with image encoders like\nResNet50, EfficientNet0, Vision Transformer (ViT), and Tiny Swin Transformer.\nOur study found that models like EfficientNet0 and Tiny Swin Transformer\nperform best on the datasets they were trained on, such as COCO, Flickr30k, and\nFlickr8k. Augmentation techniques boosted EfficientNet0 MAP on Flickr30k from\n0.84 to 0.87 and ResNet50 MAP on MSCOCO from 0.70 to 0.80, contributing to a\nnew state of the art in vision-language retrieval. We share our configurations\nand results to support further research. Code and pre-trained models are\navailable at https://github.com/aliasgerovs/azclip.", "paper_summary_zh": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u6a21\u614b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u5728\u4f4e\u8cc7\u6e90\u8a9e\u8a00\uff08\u7279\u5225\u662f\u4e9e\u585e\u62dc\u7136\u8a9e\uff09\u4e2d\u9032\u884c\u5f71\u50cf\u6aa2\u7d22\u7684\u767c\u5c55\u3002\u73fe\u6709\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4e3b\u8981\u652f\u63f4\u9ad8\u8cc7\u6e90\u8a9e\u8a00\uff0c\u4e14\u5fae\u8abf\u5b83\u5011\u5728\u904b\u7b97\u4e0a\u4ecd\u7136\u8981\u6c42\u5f88\u9ad8\u3002\u70ba\u4e86\u61c9\u5c0d\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u8996\u89ba\u8a9e\u8a00\u6aa2\u7d22\u7684\u6311\u6230\uff0c\u6211\u5011\u6574\u5408\u4e86 CLIP \u6a21\u578b\u67b6\u69cb\uff0c\u4e26\u63a1\u7528\u4e86\u5e7e\u7a2e\u6280\u8853\u4f86\u5e73\u8861\u904b\u7b97\u6548\u7387\u8207\u6548\u80fd\u3002\u9019\u4e9b\u6280\u8853\u5305\u62ec\u900f\u904e\u6a5f\u5668\u7ffb\u8b6f\u3001\u5f71\u50cf\u64f4\u5145\uff0c\u4ee5\u53ca\u4f7f\u7528\u7279\u5b9a\u9818\u57df\u8cc7\u6599\u9032\u4e00\u6b65\u8a13\u7df4\u57fa\u65bc\u8f49\u63db\u5668\u7684\u6a21\u578b\u7684\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4f86\u751f\u6210\u5408\u6210\u8cc7\u6599\u3002\u6211\u5011\u5c07\u591a\u8a9e\u8a00 BERT \u6574\u5408\u70ba\u6587\u672c\u7de8\u78bc\u5668\uff0c\u4e26\u642d\u914d ResNet50\u3001EfficientNet0\u3001\u8996\u89ba\u8f49\u63db\u5668 (ViT) \u548c Tiny Swin Transformer \u7b49\u5f71\u50cf\u7de8\u78bc\u5668\u3002\u6211\u5011\u7684\u7814\u7a76\u767c\u73fe\uff0cEfficientNet0 \u548c Tiny Swin Transformer \u7b49\u6a21\u578b\u5728\u5176\u53d7\u8a13\u7684\u8cc7\u6599\u96c6\uff08\u4f8b\u5982 COCO\u3001Flickr30k \u548c Flickr8k\uff09\u4e0a\u8868\u73fe\u6700\u4f73\u3002\u64f4\u5145\u6280\u8853\u5c07 Flickr30k \u4e0a\u7684 EfficientNet0 MAP \u5f9e 0.84 \u63d0\u5347\u5230 0.87\uff0c\u4e26\u5c07 MSCOCO \u4e0a\u7684 ResNet50 MAP \u5f9e 0.70 \u63d0\u5347\u5230 0.80\uff0c\u70ba\u8996\u89ba\u8a9e\u8a00\u6aa2\u7d22\u6a39\u7acb\u4e86\u65b0\u7684\u6280\u8853\u6a19\u6e96\u3002\u6211\u5011\u5206\u4eab\u6211\u5011\u7684\u8a2d\u5b9a\u548c\u7d50\u679c\uff0c\u4ee5\u652f\u6301\u9032\u4e00\u6b65\u7684\u7814\u7a76\u3002\u7a0b\u5f0f\u78bc\u548c\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\u53ef\u65bc https://github.com/aliasgerovs/azclip \u53d6\u5f97\u3002", "author": "Ali Asgarov et.al.", "authors": "Ali Asgarov, Samir Rustamov", "id": "2408.13909v1", "paper_url": "http://arxiv.org/abs/2408.13909v1", "repo": "https://github.com/aliasgerovs/azclip"}}