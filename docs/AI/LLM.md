
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-12-23**|**Cross-View Referring Multi-Object Tracking**|Sijia Chen et.al.|[2412.17807v1](http://arxiv.org/abs/2412.17807v1)|[link](https://github.com/chen-si-jia/crmot)|
|**2024-12-23**|**Automating the Search for Artificial Life with Foundation Models**|Akarsh Kumar et.al.|[2412.17799v1](http://arxiv.org/abs/2412.17799v1)|[link](https://github.com/sakanaai/asal)|
|**2024-12-23**|**Observation Interference in Partially Observable Assistance Games**|Scott Emmons et.al.|[2412.17797v1](http://arxiv.org/abs/2412.17797v1)|null|
|**2024-12-23**|**Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective**|Xinmiao Yu et.al.|[2412.17787v1](http://arxiv.org/abs/2412.17787v1)|null|
|**2024-12-23**|**PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion**|Sophia Tang et.al.|[2412.17780v1](http://arxiv.org/abs/2412.17780v1)|null|
|**2024-12-23**|**An Investigation on the Potential of KAN in Speech Enhancement**|Haoyang Li et.al.|[2412.17778v1](http://arxiv.org/abs/2412.17778v1)|null|
|**2024-12-23**|**ResearchTown: Simulator of Human Research Community**|Haofei Yu et.al.|[2412.17767v1](http://arxiv.org/abs/2412.17767v1)|[link](https://github.com/ulab-uiuc/research-town)|
|**2024-12-23**|**Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy**|Priyaranjan Pattnayak et.al.|[2412.17759v1](http://arxiv.org/abs/2412.17759v1)|null|
|**2024-12-23**|**In Case You Missed It: ARC 'Challenge' Is Not That Challenging**|Łukasz Borchmann et.al.|[2412.17758v1](http://arxiv.org/abs/2412.17758v1)|null|
|**2024-12-23**|**Deliberation in Latent Space via Differentiable Cache Augmentation**|Luyang Liu et.al.|[2412.17747v1](http://arxiv.org/abs/2412.17747v1)|null|
|**2024-12-23**|**RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation**|Yanli Wang et.al.|[2412.17744v1](http://arxiv.org/abs/2412.17744v1)|null|
|**2024-12-23**|**YuLan-Mini: An Open Data-efficient Language Model**|Yiwen Hu et.al.|[2412.17743v1](http://arxiv.org/abs/2412.17743v1)|[link](https://github.com/ruc-gsai/yulan-mini)|
|**2024-12-23**|**Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization**|Ermo Hua et.al.|[2412.17739v1](http://arxiv.org/abs/2412.17739v1)|null|
|**2024-12-23**|**Chumor 2.0: Towards Benchmarking Chinese Humor Understanding**|Ruiqi He et.al.|[2412.17729v1](http://arxiv.org/abs/2412.17729v1)|[link](https://github.com/dnaihao/chumor-dataset)|
|**2024-12-23**|**Knowledge Editing through Chain-of-Thought**|Changyue Wang et.al.|[2412.17727v1](http://arxiv.org/abs/2412.17727v1)|[link](https://github.com/bebr2/editcot)|
|**2024-12-23**|**VidTwin: Video VAE with Decoupled Structure and Dynamics**|Yuchi Wang et.al.|[2412.17726v1](http://arxiv.org/abs/2412.17726v1)|[link](https://github.com/microsoft/vidtok)|
|**2024-12-23**|**SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC**|Yue Deng et.al.|[2412.17707v1](http://arxiv.org/abs/2412.17707v1)|[link](https://github.com/devindeng94/smac-hard)|
|**2024-12-23**|**From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question Answering**|Nathaniel Weir et.al.|[2412.17701v1](http://arxiv.org/abs/2412.17701v1)|[link](https://github.com/nweir127/microtheories)|
|**2024-12-23**|**Understanding the Logic of Direct Preference Alignment through Logic**|Kyle Richardson et.al.|[2412.17696v1](http://arxiv.org/abs/2412.17696v1)|null|
|**2024-12-23**|**FedTLU: Federated Learning with Targeted Layer Updates**|Jong-Ik Park et.al.|[2412.17692v1](http://arxiv.org/abs/2412.17692v1)|null|
|**2024-12-23**|**RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF for Conversational QA over KGs with RAG**|Rishiraj Saha Roy et.al.|[2412.17690v1](http://arxiv.org/abs/2412.17690v1)|[link](https://github.com/fraunhofer-iis/ragonite)|
|**2024-12-23**|**Large Language Model Safety: A Holistic Survey**|Dan Shi et.al.|[2412.17686v1](http://arxiv.org/abs/2412.17686v1)|null|
|**2024-12-23**|**Generating Completions for Fragmented Broca's Aphasic Sentences Using Large Language Models**|Sijbren van Vaals et.al.|[2412.17669v1](http://arxiv.org/abs/2412.17669v1)|[link](https://github.com/sijbrenvv/completions_for_broca-s_aphasia)|
|**2024-12-23**|**Enhanced Temporal Processing in Spiking Neural Networks for Static Object Detection Using 3D Convolutions**|Huaxu He et.al.|[2412.17654v1](http://arxiv.org/abs/2412.17654v1)|null|
|**2024-12-23**|**Detecting anxiety and depression in dialogues: a multi-label and explainable approach**|Francisco de Arriba-Pérez et.al.|[2412.17651v1](http://arxiv.org/abs/2412.17651v1)|null|
|**2024-12-23**|**An Adaptive Framework for Multi-View Clustering Leveraging Conditional Entropy Optimization**|Lijian Li et.al.|[2412.17647v1](http://arxiv.org/abs/2412.17647v1)|null|
|**2024-12-23**|**SCBench: A Sports Commentary Benchmark for Video LLMs**|Kuangzhi Ge et.al.|[2412.17637v1](http://arxiv.org/abs/2412.17637v1)|null|
|**2024-12-23**|**ANID: How Far Are We? Evaluating the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance**|Renyang Liu et.al.|[2412.17632v1](http://arxiv.org/abs/2412.17632v1)|null|
|**2024-12-23**|**Graph Neural Networks Are Evolutionary Algorithms**|Kaichen Ouyang et.al.|[2412.17629v1](http://arxiv.org/abs/2412.17629v1)|null|
|**2024-12-23**|**Tracking the Feature Dynamics in LLM Training: A Mechanistic Study**|Yang Xu et.al.|[2412.17626v1](http://arxiv.org/abs/2412.17626v1)|null|
|**2024-12-23**|**Emerging Security Challenges of Large Language Models**|Herve Debar et.al.|[2412.17614v1](http://arxiv.org/abs/2412.17614v1)|null|
|**2024-12-23**|**AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation**|Jiaqi Ma et.al.|[2412.17601v1](http://arxiv.org/abs/2412.17601v1)|[link](https://github.com/jarch-ma/AFANet)|
|**2024-12-23**|**LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea Generation with Minimal Context**|Kai Ruan et.al.|[2412.17596v1](http://arxiv.org/abs/2412.17596v1)|null|
|**2024-12-23**|**Improved Cotton Leaf Disease Classification Using Parameter-Efficient Deep Learning Framework**|Aswini Kumar Patra et.al.|[2412.17587v1](http://arxiv.org/abs/2412.17587v1)|null|
|**2024-12-23**|**HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data**|Ting Zhou et.al.|[2412.17574v1](http://arxiv.org/abs/2412.17574v1)|null|
|**2024-12-23**|**Empathetic Response in Audio-Visual Conversations Using Emotion Preference Optimization and MambaCompressor**|Yeonju Kim et.al.|[2412.17572v1](http://arxiv.org/abs/2412.17572v1)|null|
|**2024-12-23**|**The Dynamic Duo of Collaborative Masking and Target for Advanced Masked Autoencoder Learning**|Shentong Mo et.al.|[2412.17566v1](http://arxiv.org/abs/2412.17566v1)|null|
|**2024-12-23**|**Evaluation of Bio-Inspired Models under Different Learning Settings For Energy Efficiency in Network Traffic Prediction**|Theodoros Tsiolakis et.al.|[2412.17565v1](http://arxiv.org/abs/2412.17565v1)|null|
|**2024-12-23**|**ERUPD -- English to Roman Urdu Parallel Dataset**|Mohammed Furqan et.al.|[2412.17562v1](http://arxiv.org/abs/2412.17562v1)|null|
|**2024-12-23**|**A Survey of Query Optimization in Large Language Models**|Mingyang Song et.al.|[2412.17558v1](http://arxiv.org/abs/2412.17558v1)|null|
|**2024-12-23**|**Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and Multi-Domain Testing**|Prakash Aryan et.al.|[2412.17548v1](http://arxiv.org/abs/2412.17548v1)|[link](https://github.com/prakash-aryan/qwen-arabic-project)|
|**2024-12-23**|**Retention Score: Quantifying Jailbreak Risks for Vision Language Models**|Zaitang Li et.al.|[2412.17544v1](http://arxiv.org/abs/2412.17544v1)|null|
|**2024-12-23**|**Concept Discovery in Deep Neural Networks for Explainable Face Anti-Spoofing**|Haoyuan Zhang et.al.|[2412.17541v1](http://arxiv.org/abs/2412.17541v1)|null|
|**2024-12-23**|**Domain adapted machine translation: What does catastrophic forgetting forget and why?**|Danielle Saunders et.al.|[2412.17537v1](http://arxiv.org/abs/2412.17537v1)|null|
|**2024-12-23**|**Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse**|Anna Kołos et.al.|[2412.17533v1](http://arxiv.org/abs/2412.17533v1)|null|
|**2024-12-23**|**Double Landmines: Invisible Textual Backdoor Attacks based on Dual-Trigger**|Yang Hou et.al.|[2412.17531v1](http://arxiv.org/abs/2412.17531v1)|null|
|**2024-12-23**|**Enhancing Cancer Diagnosis with Explainable & Trustworthy Deep Learning Models**|Badaru I. Olumuyiwa et.al.|[2412.17527v1](http://arxiv.org/abs/2412.17527v1)|null|
|**2024-12-23**|**STAHGNet: Modeling Hybrid-grained Heterogenous Dependency Efficiently for Traffic Prediction**|Jiyao Wang et.al.|[2412.17524v1](http://arxiv.org/abs/2412.17524v1)|null|
|**2024-12-23**|**Constructing Fair Latent Space for Intersection of Fairness and Explainability**|Hyungjun Joo et.al.|[2412.17523v1](http://arxiv.org/abs/2412.17523v1)|null|
|**2024-12-23**|**DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak**|Hao Wang et.al.|[2412.17522v1](http://arxiv.org/abs/2412.17522v1)|null|
|**2024-12-23**|**BEE: Metric-Adapted Explanations via Baseline Exploration-Exploitation**|Oren Barkan et.al.|[2412.17512v1](http://arxiv.org/abs/2412.17512v1)|null|
|**2024-12-23**|**An Evaluation Framework for Product Images Background Inpainting based on Human Feedback and Product Consistency**|Yuqi Liang et.al.|[2412.17504v1](http://arxiv.org/abs/2412.17504v1)|null|
|**2024-12-23**|**DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought**|Jiaan Wang et.al.|[2412.17498v1](http://arxiv.org/abs/2412.17498v1)|[link](https://github.com/krystalan/drt-o1)|
|**2024-12-23**|**A Toolkit for Virtual Reality Data Collection**|Tim Rolff et.al.|[2412.17490v1](http://arxiv.org/abs/2412.17490v1)|null|
|**2024-12-23**|**Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of Large Language Models such as ChatGPT in Educational Settings**|Jérémie Sublime et.al.|[2412.17486v1](http://arxiv.org/abs/2412.17486v1)|null|
|**2024-12-23**|**Power- and Fragmentation-aware Online Scheduling for GPU Datacenters**|Francesco Lettich et.al.|[2412.17484v1](http://arxiv.org/abs/2412.17484v1)|[link](https://github.com/fr4nz83/pwr-plugin-kubernetes)|
|**2024-12-23**|**A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression**|Chenlong Deng et.al.|[2412.17483v1](http://arxiv.org/abs/2412.17483v1)|null|
|**2024-12-23**|**A Survey on Multi-Generative Agent System: Recent Advances and New Frontiers**|Shuaihang Chen et.al.|[2412.17481v1](http://arxiv.org/abs/2412.17481v1)|null|
|**2024-12-23**|**Signal Transformation for Effective Multi-Channel Signal Processing**|Sunil Kumar Kopparapu et.al.|[2412.17478v1](http://arxiv.org/abs/2412.17478v1)|null|
|**2024-12-23**|**Line Graph Vietoris-Rips Persistence Diagram for Topological Graph Representation Learning**|Jaesun Shin et.al.|[2412.17468v1](http://arxiv.org/abs/2412.17468v1)|[link](https://github.com/samsungsds-research-papers/lgvr)|
|**2024-12-23**|**Developmental Predictive Coding Model for Early Infancy Mono and Bilingual Vocal Continual Learning**|Xiaodan Chen et.al.|[2412.17456v1](http://arxiv.org/abs/2412.17456v1)|null|
|**2024-12-23**|**Diving into Self-Evolving Training for Multimodal Reasoning**|Wei Liu et.al.|[2412.17451v1](http://arxiv.org/abs/2412.17451v1)|null|
|**2024-12-23**|**Applying LLM and Topic Modelling in Psychotherapeutic Contexts**|Alexander Vanin et.al.|[2412.17449v1](http://arxiv.org/abs/2412.17449v1)|null|
|**2024-12-23**|**The Role of XAI in Transforming Aeronautics and Aerospace Systems**|Francisco Javier Cantero Zorita et.al.|[2412.17440v1](http://arxiv.org/abs/2412.17440v1)|null|
|**2024-12-23**|**Markov Process-Based Graph Convolutional Networks for Entity Classification in Knowledge Graphs**|Johannes Mäkelburg et.al.|[2412.17438v1](http://arxiv.org/abs/2412.17438v1)|null|
|**2024-12-23**|**Measuring Contextual Informativeness in Child-Directed Text**|Maria Valentini et.al.|[2412.17427v1](http://arxiv.org/abs/2412.17427v1)|[link](https://github.com/mariavale/contextual_inform)|
|**2024-12-23**|**VidCtx: Context-aware Video Question Answering with Image Models**|Andreas Goulas et.al.|[2412.17415v1](http://arxiv.org/abs/2412.17415v1)|null|
|**2024-12-23**|**Pretraining with random noise for uncertainty calibration**|Jeonghwan Cheon et.al.|[2412.17411v1](http://arxiv.org/abs/2412.17411v1)|null|
|**2024-12-23**|**Just What You Desire: Constrained Timeline Summarization with Self-Reflection for Enhanced Relevance**|Muhammad Reza Qorib et.al.|[2412.17408v1](http://arxiv.org/abs/2412.17408v1)|[link](https://github.com/nusnlp/reacts)|
|**2024-12-23**|**BrainMAP: Learning Multiple Activation Pathways in Brain Networks**|Song Wang et.al.|[2412.17404v1](http://arxiv.org/abs/2412.17404v1)|null|
|**2024-12-23**|**WarriorCoder: Learning from Expert Battles to Augment Code Large Language Models**|Huawen Feng et.al.|[2412.17395v1](http://arxiv.org/abs/2412.17395v1)|null|
|**2024-12-23**|**Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement**|Hyeonjin Kim et.al.|[2412.17387v1](http://arxiv.org/abs/2412.17387v1)|null|
|**2024-12-23**|**Interweaving Memories of a Siamese Large Language Model**|Xin Song et.al.|[2412.17383v1](http://arxiv.org/abs/2412.17383v1)|[link](https://github.com/ecnu-text-computing/imsm)|
|**2024-12-23**|**A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions**|Youliang Zhang et.al.|[2412.17377v1](http://arxiv.org/abs/2412.17377v1)|null|
|**2024-12-23**|**FRTP: Federating Route Search Records to Enhance Long-term Traffic Prediction**|Hangli Ge et.al.|[2412.17373v1](http://arxiv.org/abs/2412.17373v1)|null|
|**2024-12-23**|**Boosting LLM via Learning from Data Iteratively and Selectively**|Qi Jia et.al.|[2412.17365v1](http://arxiv.org/abs/2412.17365v1)|null|
|**2024-12-23**|**Efficient fine-tuning methodology of text embedding models for information retrieval: contrastive learning penalty (clp)**|Jeongsu Yu et.al.|[2412.17364v1](http://arxiv.org/abs/2412.17364v1)|[link](https://github.com/crealabs/enhanced-bge-m3-with-clp-and-moe)|
|**2024-12-23**|**Three-Class Text Sentiment Analysis Based on LSTM**|Yin Qixuan et.al.|[2412.17347v1](http://arxiv.org/abs/2412.17347v1)|null|
|**2024-12-23**|**FFA Sora, video generation as fundus fluorescein angiography simulator**|Xinyuan Wu et.al.|[2412.17346v1](http://arxiv.org/abs/2412.17346v1)|null|
|**2024-12-23**|**MineAgent: Towards Remote-Sensing Mineral Exploration with Multimodal Large Language Models**|Beibei Yu et.al.|[2412.17339v1](http://arxiv.org/abs/2412.17339v1)|null|
|**2024-12-23**|**Enhancing Topic Interpretability for Neural Topic Modeling through Topic-wise Contrastive Learning**|Xin Gao et.al.|[2412.17338v1](http://arxiv.org/abs/2412.17338v1)|null|
|**2024-12-23**|**Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition**|Jaeheun Jung et.al.|[2412.17333v1](http://arxiv.org/abs/2412.17333v1)|null|
|**2024-12-23**|**A Dual-Perspective Metaphor Detection Framework Using Large Language Models**|Yujie Lin et.al.|[2412.17332v1](http://arxiv.org/abs/2412.17332v1)|[link](https://github.com/deeplearnxmu/dmd)|
|**2024-12-23**|**xPatch: Dual-Stream Time Series Forecasting with Exponential Seasonal-Trend Decomposition**|Artyom Stitsyuk et.al.|[2412.17323v1](http://arxiv.org/abs/2412.17323v1)|[link](https://github.com/stitsyuk/xpatch)|
|**2024-12-23**|**Assessing Human Editing Effort on LLM-Generated Texts via Compression-Based Edit Distance**|Nicolas Devatine et.al.|[2412.17321v1](http://arxiv.org/abs/2412.17321v1)|null|
|**2024-12-23**|**Fast Gradient Computation for RoPE Attention in Almost Linear Time**|Yifang Chen et.al.|[2412.17316v1](http://arxiv.org/abs/2412.17316v1)|null|
|**2024-12-23**|**CodeV: Issue Resolving with Visual Data**|Linhao Zhang et.al.|[2412.17315v1](http://arxiv.org/abs/2412.17315v1)|[link](https://github.com/luolin101/codev)|
|**2024-12-23**|**On the Feasibility of Vision-Language Models for Time-Series Classification**|Vinay Prithyani et.al.|[2412.17304v1](http://arxiv.org/abs/2412.17304v1)|null|
|**2024-12-23**|**Dynamic Scheduling Strategies for Resource Optimization in Computing Environments**|Xiaoye Wang et.al.|[2412.17301v1](http://arxiv.org/abs/2412.17301v1)|null|
|**2024-12-23**|**Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding**|Yueqian Wang et.al.|[2412.17295v1](http://arxiv.org/abs/2412.17295v1)|[link](https://github.com/yellow-binary-tree/friends-mmc)|
|**2024-12-23**|**AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues**|Se Jin Park et.al.|[2412.17292v1](http://arxiv.org/abs/2412.17292v1)|null|
|**2024-12-23**|**Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with A Few Examples**|Taewoong Kim et.al.|[2412.17288v1](http://arxiv.org/abs/2412.17288v1)|null|
|**2024-12-23**|**LLM4AD: A Platform for Algorithm Design with Large Language Model**|Fei Liu et.al.|[2412.17287v1](http://arxiv.org/abs/2412.17287v1)|null|
|**2024-12-23**|**Enabling Time-series Foundation Model for Building Energy Forecasting via Contrastive Curriculum Learning**|Rui Liang et.al.|[2412.17285v1](http://arxiv.org/abs/2412.17285v1)|null|
|**2024-12-23**|**Learning from Mistakes: Self-correct Adversarial Training for Chinese Unnatural Text Correction**|Xuan Feng et.al.|[2412.17279v1](http://arxiv.org/abs/2412.17279v1)|null|
|**2024-12-23**|**LegalAgentBench: Evaluating LLM Agents in Legal Domain**|Haitao Li et.al.|[2412.17259v1](http://arxiv.org/abs/2412.17259v1)|[link](https://github.com/cshaitao/legalagentbench)|
|**2024-12-23**|**B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners**|Weihao Zeng et.al.|[2412.17256v1](http://arxiv.org/abs/2412.17256v1)|[link](https://github.com/hkust-nlp/b-star)|
|**2024-12-23**|**Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation: A Multimodal Generative AI Approach**|Rafid Ishrak Jahan et.al.|[2412.17255v1](http://arxiv.org/abs/2412.17255v1)|[link](https://github.com/responsibleailab/emoji-universal-sentiment)|
|**2024-12-23**|**Enhancing Multi-Text Long Video Generation Consistency without Tuning: Time-Frequency Analysis, Prompt Alignment, and Theory**|Xingyao Li et.al.|[2412.17254v1](http://arxiv.org/abs/2412.17254v1)|null|
|**2024-12-23**|**On the Generalization Ability of Machine-Generated Text Detectors**|Yule Liu et.al.|[2412.17242v1](http://arxiv.org/abs/2412.17242v1)|[link](https://github.com/Y-L-LIU/MGTBench-2.0)|

#### Abstracts
##### **Cross-View Referring Multi-Object Tracking**
2412.17807v1 by Sijia Chen, En Yu, Wenbing Tao

Referring Multi-Object Tracking (RMOT) is an important topic in the current
tracking field. Its task form is to guide the tracker to track objects that
match the language description. Current research mainly focuses on referring
multi-object tracking under single-view, which refers to a view sequence or
multiple unrelated view sequences. However, in the single-view, some
appearances of objects are easily invisible, resulting in incorrect matching of
objects with the language description. In this work, we propose a new task,
called Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the
cross-view to obtain the appearances of objects from multiple views, avoiding
the problem of the invisible appearances of objects in RMOT task. CRMOT is a
more challenging task of accurately tracking the objects that match the
language description and maintaining the identity consistency of objects in
each cross-view. To advance CRMOT task, we construct a cross-view referring
multi-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named
CRTrack. Specifically, it provides 13 different scenes and 221 language
descriptions. Furthermore, we propose an end-to-end cross-view referring
multi-object tracking method, named CRTracker. Extensive experiments on the
CRTrack benchmark verify the effectiveness of our method. The dataset and code
are available at https://github.com/chen-si-jia/CRMOT.

摘要：多目標追蹤 (RMOT) 是目前追蹤領域中一個重要的課題。其任務形式是引導追蹤器追蹤符合語言描述的目標。目前的研究主要集中於單視角下的多目標追蹤，指的是一個視角序列或多個無關的視角序列。然而，在單視角中，目標的一些外觀很容易看不見，導致目標與語言描述不正確地匹配。在這項工作中，我們提出了新任務，稱為跨視角多目標追蹤 (CRMOT)。它引入了跨視角，以從多個視角獲得目標的外觀，避免了 RMOT 任務中目標外觀不可見的問題。CRMOT 是一項更具挑戰性的任務，準確追蹤符合語言描述的目標，並維持每個跨視角中目標的身份一致性。為了推進 CRMOT 任務，我們基於 CAMPUS 和 DIVOTrack 資料集構建了一個跨視角多目標追蹤基準，稱為 CRTrack。具體來說，它提供了 13 個不同的場景和 221 個語言描述。此外，我們提出了一種端到端的跨視角多目標追蹤方法，稱為 CRTracker。在 CRTrack 基準上進行的廣泛實驗驗證了我們方法的有效性。資料集和程式碼可在 https://github.com/chen-si-jia/CRMOT 中取得。

##### **Automating the Search for Artificial Life with Foundation Models**
2412.17799v1 by Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip Isola, David Ha

With the recent Nobel Prize awarded for radical advances in protein
discovery, foundation models (FMs) for exploring large combinatorial spaces
promise to revolutionize many scientific fields. Artificial Life (ALife) has
not yet integrated FMs, thus presenting a major opportunity for the field to
alleviate the historical burden of relying chiefly on manual design and
trial-and-error to discover the configurations of lifelike simulations. This
paper presents, for the first time, a successful realization of this
opportunity using vision-language FMs. The proposed approach, called Automated
Search for Artificial Life (ASAL), (1) finds simulations that produce target
phenomena, (2) discovers simulations that generate temporally open-ended
novelty, and (3) illuminates an entire space of interestingly diverse
simulations. Because of the generality of FMs, ASAL works effectively across a
diverse range of ALife substrates including Boids, Particle Life, Game of Life,
Lenia, and Neural Cellular Automata. A major result highlighting the potential
of this technique is the discovery of previously unseen Lenia and Boids
lifeforms, as well as cellular automata that are open-ended like Conway's Game
of Life. Additionally, the use of FMs allows for the quantification of
previously qualitative phenomena in a human-aligned way. This new paradigm
promises to accelerate ALife research beyond what is possible through human
ingenuity alone.

摘要：隨著最近諾貝爾獎授予蛋白質發現的激進進展，探索大型組合空間的基礎模型 (FM) 承諾將徹底改變許多科學領域。人工生命 (ALife) 尚未整合 FM，因此為該領域提供了一個重大的機會，以減輕主要依賴手動設計和試錯來發現逼真模擬的配置的歷史負擔。本文首次提出使用視覺語言 FM 成功實現這一機會。所提出的方法稱為人工生命自動搜尋 (ASAL)，（1）找到產生目標現象的模擬，（2）發現產生時間開放式創新的模擬，以及（3）說明整個有趣多樣性模擬空間。由於 FM 的普遍性，ASAL 在包括 Boids、Particle Life、生命遊戲、Lenia 和神經元細胞自動機在內的各種 ALife 基質中有效運行。強調此技術潛力的主要結果是發現以前未見過的 Lenia 和 Boids 生命形式，以及像康威生命遊戲一樣開放式的細胞自動機。此外，FM 的使用允許以與人類一致的方式量化先前的定性現象。這種新的範例有望加速 ALife 研究，超越僅通過人類智慧所能達到的範圍。

##### **Observation Interference in Partially Observable Assistance Games**
2412.17797v1 by Scott Emmons, Caspar Oesterheld, Vincent Conitzer, Stuart Russell

We study partially observable assistance games (POAGs), a model of the
human-AI value alignment problem which allows the human and the AI assistant to
have partial observations. Motivated by concerns of AI deception, we study a
qualitatively new phenomenon made possible by partial observability: would an
AI assistant ever have an incentive to interfere with the human's observations?
First, we prove that sometimes an optimal assistant must take
observation-interfering actions, even when the human is playing optimally, and
even when there are otherwise-equivalent actions available that do not
interfere with observations. Though this result seems to contradict the classic
theorem from single-agent decision making that the value of perfect information
is nonnegative, we resolve this seeming contradiction by developing a notion of
interference defined on entire policies. This can be viewed as an extension of
the classic result that the value of perfect information is nonnegative into
the cooperative multiagent setting. Second, we prove that if the human is
simply making decisions based on their immediate outcomes, the assistant might
need to interfere with observations as a way to query the human's preferences.
We show that this incentive for interference goes away if the human is playing
optimally, or if we introduce a communication channel for the human to
communicate their preferences to the assistant. Third, we show that if the
human acts according to the Boltzmann model of irrationality, this can create
an incentive for the assistant to interfere with observations. Finally, we use
an experimental model to analyze tradeoffs faced by the AI assistant in
practice when considering whether or not to take observation-interfering
actions.

摘要：<paragraph>我們研究部分可觀察協助遊戲 (POAG)，這是一種人類-AI 價值對齊問題模型，允許人類和 AI 助理進行部分觀察。由於擔心 AI 欺騙，我們研究了一個由部分可觀察性促成的質性新現象：AI 助理是否會誘使干擾人類的觀察？首先，我們證明有時最佳助理必須採取觀察干擾行動，即使人類表現最佳，即使有其他等效行動不會干擾觀察。儘管這個結果看似與單一代理決策制定中的經典定理相矛盾，即完美資訊的價值是非負的，但我們透過制定定義在整個政策上的干擾概念來解決這個看似矛盾的現象。這可以視為將完美資訊的價值是非負的經典結果延伸到合作的多代理設定中。其次，我們證明如果人類只是根據其立即結果做出決策，助理可能需要干擾觀察作為查詢人類偏好的方法。我們表明，如果人類表現最佳，或者我們引入一個讓人類將其偏好傳達給助理的溝通管道，這種干擾誘因就會消失。第三，我們表明如果人類根據波茲曼非理性模型行事，這可能會誘使助理干擾觀察。最後，我們使用一個實驗模型來分析 AI 助理在實務上考慮是否採取觀察干擾行動時所面臨的權衡。</paragraph>

##### **Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective**
2412.17787v1 by Xinmiao Yu, Xiaocheng Feng, Yun Li, Minghui Liao, Ya-Qi Yu, Xiachong Feng, Weihong Zhong, Ruihan Chen, Mengkang Hu, Jihao Wu, Dandan Tu, Duyu Tang, Bing Qin

Recent Large Vision-Language Models (LVLMs) have shown promising reasoning
capabilities on text-rich images from charts, tables, and documents. However,
the abundant text within such images may increase the model's sensitivity to
language. This raises the need to evaluate LVLM performance on cross-lingual
text-rich visual inputs, where the language in the image differs from the
language of the instructions. To address this, we introduce XT-VQA
(Cross-Lingual Text-Rich Visual Question Answering), a benchmark designed to
assess how LVLMs handle language inconsistency between image text and
questions. XT-VQA integrates five existing text-rich VQA datasets and a newly
collected dataset, XPaperQA, covering diverse scenarios that require faithful
recognition and comprehension of visual information despite language
inconsistency. Our evaluation of prominent LVLMs on XT-VQA reveals a
significant drop in performance for cross-lingual scenarios, even for models
with multilingual capabilities. A mutual information analysis suggests that
this performance gap stems from cross-lingual questions failing to adequately
activate relevant visual information. To mitigate this issue, we propose
MVCL-MI (Maximization of Vision-Language Cross-Lingual Mutual Information),
where a visual-text cross-lingual alignment is built by maximizing mutual
information between the model's outputs and visual information. This is
achieved by distilling knowledge from monolingual to cross-lingual settings
through KL divergence minimization, where monolingual output logits serve as a
teacher. Experimental results on the XT-VQA demonstrate that MVCL-MI
effectively reduces the visual-text cross-lingual performance disparity while
preserving the inherent capabilities of LVLMs, shedding new light on the
potential practice for improving LVLMs. Codes are available at:
https://github.com/Stardust-y/XTVQA.git

摘要：<paragraph>最近的大型视觉语言模型 (LVLMs) 在图表、表格和文档中的文本丰富的图像上表现出有希望的推理能力。然而，此类图像中丰富的文本可能会增加模型对语言的敏感性。这引发了需要评估 LVLM 在跨语言文本丰富的视觉输入上的性能，其中图像中的语言与指令的语言不同。为了解决这个问题，我们引入了 XT-VQA（跨语言文本丰富的视觉问题解答），这是一个旨在评估 LVLM 如何处理图像文本和问题之间的语言不一致的基准。XT-VQA 集成了五个现有的文本丰富的 VQA 数据集和一个新收集的数据集 XPaperQA，涵盖了需要忠实识别和理解视觉信息的不同场景，尽管存在语言不一致。我们对 XT-VQA 上的突出 LVLM 的评估表明，即使对于具有多语言能力的模型，跨语言场景的性能也大幅下降。互信息分析表明，这种性能差距源于跨语言问题未能充分激活相关的视觉信息。为了缓解这个问题，我们提出了 MVCL-MI（视觉语言跨语言互信息的最大化），其中通过最大化模型输出和视觉信息之间的互信息来构建视觉文本跨语言对齐。这是通过 KL 散度最小化从单语言到跨语言设置提取知识来实现的，其中单语言输出 logit 充当教师。XT-VQA 上的实验结果表明，MVCL-MI 有效地减少了视觉文本跨语言性能差异，同时保留了 LVLMs 的固有能力，为改进 LVLMs 的潜在实践提供了新的思路。代码可在以下位置获得：
https://github.com/Stardust-y/XTVQA.git</paragraph>

##### **PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion**
2412.17780v1 by Sophia Tang, Yinuo Zhang, Pranam Chatterjee

Peptide therapeutics, a major class of medicines, have achieved remarkable
success across diseases such as diabetes and cancer, with landmark examples
such as GLP-1 receptor agonists revolutionizing the treatment of type-2
diabetes and obesity. Despite their success, designing peptides that satisfy
multiple conflicting objectives, such as target binding affinity, solubility,
and membrane permeability, remains a major challenge. Classical drug
development and structure-based design are ineffective for such tasks, as they
fail to optimize global functional properties critical for therapeutic
efficacy. Existing generative frameworks are largely limited to continuous
spaces, unconditioned outputs, or single-objective guidance, making them
unsuitable for discrete sequence optimization across multiple properties. To
address this, we present PepTune, a multi-objective discrete diffusion model
for the simultaneous generation and optimization of therapeutic peptide SMILES.
Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensures
valid peptide structures with state-dependent masking schedules and
penalty-based objectives. To guide the diffusion process, we propose a Monte
Carlo Tree Search (MCTS)-based strategy that balances exploration and
exploitation to iteratively refine Pareto-optimal sequences. MCTS integrates
classifier-based rewards with search-tree expansion, overcoming gradient
estimation challenges and data sparsity inherent to discrete spaces. Using
PepTune, we generate diverse, chemically-modified peptides optimized for
multiple therapeutic properties, including target binding affinity, membrane
permeability, solubility, hemolysis, and non-fouling characteristics on various
disease-relevant targets. In total, our results demonstrate that MCTS-guided
discrete diffusion is a powerful and modular approach for multi-objective
sequence design in discrete state spaces.

摘要：胜肽疗法是一类主要的药物，在糖尿病和癌症等疾病中取得了显著的成功，其中具有里程碑意义的例子如 GLP-1 受体激动剂彻底改变了 2 型糖尿病和肥胖症的治疗。尽管取得了成功，但设计满足多种相互冲突的目标（如靶向结合亲和力、溶解性和膜渗透性）的胜肽仍然是一项重大挑战。经典药物开发和基于结构的设计对于此类任务无效，因为它们无法优化对治疗效果至关重要的全局功能特性。现有的生成框架在很大程度上仅限于连续空间、无条件输出或单目标指导，这使得它们不适合跨多个属性进行离散序列优化。为了解决这个问题，我们提出了 PepTune，一种用于同时生成和优化治疗性胜肽 SMILES 的多目标离散扩散模型。基于掩码离散语言模型 (MDLM) 框架，PepTune 通过状态相关掩码调度和基于惩罚的目标确保有效的胜肽结构。为了指导扩散过程，我们提出了一种基于蒙特卡罗树搜索 (MCTS) 的策略，该策略平衡探索和利用以迭代优化帕累托最优序列。MCTS 将基于分类器的奖励与搜索树扩展相结合，克服了离散空间固有的梯度估计挑战和数据稀疏性。使用 PepTune，我们生成了针对多种治疗特性（包括靶向结合亲和力、膜渗透性、溶解性、溶血和在各种疾病相关靶标上的非污染特性）进行优化的多样化化学修饰胜肽。总体而言，我们的结果表明，MCTS 指导的离散扩散是一种用于离散状态空间中多目标序列设计的强大且模块化的方法。

##### **An Investigation on the Potential of KAN in Speech Enhancement**
2412.17778v1 by Haoyang Li, Yuchen Hu, Chen Chen, Eng Siong Chng

High-fidelity speech enhancement often requires sophisticated modeling to
capture intricate, multiscale patterns. Standard activation functions, while
introducing nonlinearity, lack the flexibility to fully address this
complexity. Kolmogorov-Arnold Networks (KAN), an emerging methodology that
employs learnable activation functions on graph edges, present a promising
alternative. This work investigates two novel KAN variants based on rational
and radial basis functions for speech enhancement. We integrate the rational
variant into the 1D CNN blocks of Demucs and the GRU-Transformer blocks of
MP-SENet, while the radial variant is adapted to the 2D CNN-based decoders of
MP-SENet. Experiments on the VoiceBank-DEMAND dataset show that replacing
standard activations with KAN-based activations improves speech quality across
both the time-domain and time-frequency domain methods with minimal impact on
model size and FLOP, underscoring KAN's potential to improve speech enhancement
models.

摘要：高保真语音增强通常需要复杂的建模来捕捉复杂的多尺度模式。标准激活函数虽然引入了非线性，但缺乏完全解决这种复杂性的灵活性。Kolmogorov-Arnold 网络 (KAN) 是一种新兴的方法，它在图边缘上使用可学习的激活函数，提供了一个有前途的替代方案。这项工作研究了两种基于有理函数和径向基函数的用于语音增强的 KAN 变体。我们将有理变体集成到 Demucs 的一维卷积神经网络块和 MP-SENet 的 GRU-Transformer 块中，而径向变体则适用于 MP-SENet 基于二维卷积神经网络的解码器。在 VoiceBank-DEMAND 数据集上的实验表明，用基于 KAN 的激活函数替换标准激活函数可以提高语音质量，同时对模型大小和浮点运算的影响很小，这突显了 KAN 改进语音增强模型的潜力。

##### **ResearchTown: Simulator of Human Research Community**
2412.17767v1 by Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, Jiaxuan You

Large Language Models (LLMs) have demonstrated remarkable potential in
scientific domains, yet a fundamental question remains unanswered: Can we
simulate human research communities with LLMs? Addressing this question can
deepen our understanding of the processes behind idea brainstorming and inspire
the automatic discovery of novel scientific insights. In this work, we propose
ResearchTown, a multi-agent framework for research community simulation. Within
this framework, the human research community is simplified and modeled as an
agent-data graph, where researchers and papers are represented as agent-type
and data-type nodes, respectively, and connected based on their collaboration
relationships. We also introduce TextGNN, a text-based inference framework that
models various research activities (e.g., paper reading, paper writing, and
review writing) as special forms of a unified message-passing process on the
agent-data graph. To evaluate the quality of the research simulation, we
present ResearchBench, a benchmark that uses a node-masking prediction task for
scalable and objective assessment based on similarity. Our experiments reveal
three key findings: (1) ResearchTown can provide a realistic simulation of
collaborative research activities, including paper writing and review writing;
(2) ResearchTown can maintain robust simulation with multiple researchers and
diverse papers; (3) ResearchTown can generate interdisciplinary research ideas
that potentially inspire novel research directions.

摘要：大型語言模型 (LLM) 在科學領域展現了非凡的潛力，但仍有一個基本問題尚未解答：我們能用 LLM 模擬人類研究社群嗎？探討這個問題能加深我們對腦力激盪背後流程的理解，並激發自動發現新科學見解。在這項工作中，我們提出 ResearchTown，一個用於研究社群模擬的多代理架構。在這個架構中，人類研究社群被簡化並建模為代理資料圖，其中研究人員和論文分別表示為代理類型節點和資料類型節點，並根據他們的合作關係進行連接。我們還介紹了 TextGNN，一個基於文字的推論架構，它將各種研究活動（例如，閱讀論文、撰寫論文和撰寫評論）建模為代理資料圖上統一訊息傳遞過程的特殊形式。為了評估研究模擬的品質，我們提出了 ResearchBench，一個使用節點遮罩預測任務進行基於相似性的可擴充且客觀評估的基準。我們的實驗揭示了三個關鍵發現：(1) ResearchTown 可以提供協作研究活動的逼真模擬，包括撰寫論文和撰寫評論；(2) ResearchTown 可以維持多位研究人員和不同論文的穩健模擬；(3) ResearchTown 可以產生跨學科研究構想，潛在激發新的研究方向。

##### **Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy**
2412.17759v1 by Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal, Ishan Banerjee, Srikant Panda, Tejaswini Kumar

Multimodal learning, a rapidly evolving field in artificial intelligence,
seeks to construct more versatile and robust systems by integrating and
analyzing diverse types of data, including text, images, audio, and video.
Inspired by the human ability to assimilate information through many senses,
this method enables applications such as text-to-video conversion, visual
question answering, and image captioning. Recent developments in datasets that
support multimodal language models (MLLMs) are highlighted in this overview.
Large-scale multimodal datasets are essential because they allow for thorough
testing and training of these models. With an emphasis on their contributions
to the discipline, the study examines a variety of datasets, including those
for training, domain-specific tasks, and real-world applications. It also
emphasizes how crucial benchmark datasets are for assessing models' performance
in a range of scenarios, scalability, and applicability. Since multimodal
learning is always changing, overcoming these obstacles will help AI research
and applications reach new heights.

摘要：多模态学习是人工智能领域中快速发展的领域，
它通过整合和分析包括文本、图像、音频和视频在内的多种类型的数据，
旨在构建更多功能强大且健壮的系统。
受人类通过多种感官吸收信息的能力的启发，
这种方法支持文本转视频转换、视觉问答和图像标题等应用程序。
本文重点介绍了支持多模态语言模型 (MLLM) 的数据集的最新发展。
大规模多模态数据集至关重要，因为它们允许对这些模型进行彻底的测试和训练。
本研究重点介绍了它们对该学科的贡献，考察了各种数据集，包括用于训练、特定领域的任务和实际应用程序的数据集。
它还强调了基准数据集对于评估模型在各种场景、可扩展性和适用性中的性能至关重要。
由于多模态学习总是在变化，克服这些障碍将有助于人工智能研究和应用达到新的高度。

##### **In Case You Missed It: ARC 'Challenge' Is Not That Challenging**
2412.17758v1 by Łukasz Borchmann

ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily
due to an evaluation setup that prevents direct comparison of answer choices
rather than inherent complexity. Although some researchers have quietly shifted
to a more appropriate scheme over the last year, the implications of this
change have yet to be widely acknowledged. We highlight this overlooked shift,
show how similar evaluation practices falsely imply reasoning deficits in other
benchmarks, and demonstrate that fairer methods dramatically reduce performance
gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing
so, we reveal how evaluation shapes perceived difficulty and offer guidelines
to ensure that multiple-choice evaluations accurately reflect actual model
capabilities.

摘要：ARC Challenge 比 ARC Easy 更難，主要是因為評估設定，它會阻止直接比較答案選擇，而不是固有的複雜性。儘管一些研究人員在過去一年中已悄悄轉向更適當的方案，但這種改變的影響尚未得到廣泛認可。我們強調這個被忽視的轉變，展示類似的評估實務如何錯誤地暗示其他基準中的推理缺陷，並證明更公平的方法顯著縮小了效能差距（例如在 SIQA 上），甚至產生了超人的結果（OpenBookQA）。在這樣做的過程中，我們揭示了評估如何塑造感知的難度，並提供準則以確保多重選擇評估準確反映實際模型的能力。

##### **Deliberation in Latent Space via Differentiable Cache Augmentation**
2412.17747v1 by Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, Arthur Szlam

Techniques enabling large language models (LLMs) to "think more" by
generating and attending to intermediate reasoning steps have shown promise in
solving complex problems. However, the standard approaches generate sequences
of discrete tokens immediately before responding, and so they can incur
significant latency costs and be challenging to optimize. In this work, we
demonstrate that a frozen LLM can be augmented with an offline coprocessor that
operates on the model's key-value (kv) cache. This coprocessor augments the
cache with a set of latent embeddings designed to improve the fidelity of
subsequent decoding. We train this coprocessor using the language modeling loss
from the decoder on standard pretraining data, while keeping the decoder itself
frozen. This approach enables the model to learn, in an end-to-end
differentiable fashion, how to distill additional computation into its
kv-cache. Because the decoder remains unchanged, the coprocessor can operate
offline and asynchronously, and the language model can function normally if the
coprocessor is unavailable or if a given cache is deemed not to require extra
computation. We show experimentally that when a cache is augmented, the decoder
achieves lower perplexity on numerous subsequent tokens. Furthermore, even
without any task-specific training, our experiments demonstrate that cache
augmentation consistently reduces perplexity and improves performance across a
range of reasoning-intensive tasks.

摘要：大型語言模型 (LLM) 能夠透過產生和關注中間推理步驟來「更深入思考」的技術已在解決複雜問題方面展現前景。然而，標準方法會在回應前立即產生離散符號的序列，因此可能會產生顯著的延遲成本，且難以最佳化。在這項工作中，我們證明了一個凍結的 LLM 可以透過一個離線協同處理器進行擴充，該協同處理器在模型的快取值 (kv) 快取中執行。此協同處理器會針對快取進行擴充，加入一組潛在嵌入，以提升後續解碼的保真度。我們使用解碼器中來自標準預訓練資料的語言模型損失訓練此協同處理器，同時保持解碼器本身處於凍結狀態。此方法讓模型能夠以端到端可微分的方式學習，如何將額外的運算納入其 kv 快取中。由於解碼器保持不變，因此協同處理器可以在離線和非同步的情況下執行，且如果協同處理器不可用或某個快取被認為不需要額外運算，語言模型可以正常運作。我們透過實驗證明，當快取經過擴充時，解碼器在後續的許多符號上都能達到較低的困惑度。此外，我們的實驗證明，即使沒有任何特定任務的訓練，快取擴充都能持續降低困惑度，並提升各種推理密集型任務的效能。

##### **RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation**
2412.17744v1 by Yanli Wang, Yanlin Wang, Suiquan Wang, Daya Guo, Jiachi Chen, John Grundy, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng

Repository-level code translation refers to translating an entire code
repository from one programming language to another while preserving the
functionality of the source repository. Many benchmarks have been proposed to
evaluate the performance of such code translators. However, previous benchmarks
mostly provide fine-grained samples, focusing at either code snippet, function,
or file-level code translation. Such benchmarks do not accurately reflect
real-world demands, where entire repositories often need to be translated,
involving longer code length and more complex functionalities. To address this
gap, we propose a new benchmark, named RepoTransBench, which is a real-world
repository-level code translation benchmark with an automatically executable
test suite. We conduct experiments on RepoTransBench to evaluate the
translation performance of 11 advanced LLMs. We find that the Success@1 score
(test success in one attempt) of the best-performing LLM is only 7.33%. To
further explore the potential of LLMs for repository-level code translation, we
provide LLMs with error-related feedback to perform iterative debugging and
observe an average 7.09% improvement on Success@1. However, even with this
improvement, the Success@1 score of the best-performing LLM is only 21%, which
may not meet the need for reliable automatic repository-level code translation.
Finally, we conduct a detailed error analysis and highlight current LLMs'
deficiencies in repository-level code translation, which could provide a
reference for further improvements.

摘要：<paragraph>儲存庫層級程式碼翻譯是指將整個程式碼儲存庫從一種程式語言翻譯成另一種，同時保留來源儲存庫的功能。已經提出了許多基準來評估此類程式碼翻譯器的效能。然而，以前的基準大多提供細粒度的範例，專注於程式碼片段、函式或檔案層級程式碼翻譯。此類基準並未準確反映實際需求，在實際需求中，通常需要翻譯整個儲存庫，涉及較長的程式碼長度和更複雜的功能。為了解決這個差距，我們提出一個新的基準，稱為 RepoTransBench，這是一個真實世界的儲存庫層級程式碼翻譯基準，並具有自動可執行測試套件。我們在 RepoTransBench 上進行實驗，以評估 11 個先進 LLM 的翻譯效能。我們發現，效能最佳的 LLM 的 Success@1 分數（一次嘗試中的測試成功率）僅為 7.33%。為了進一步探索 LLM 在儲存庫層級程式碼翻譯方面的潛力，我們為 LLM 提供與錯誤相關的回饋，以執行反覆除錯，並觀察到 Success@1 平均提升 7.09%。然而，即使有此提升，效能最佳的 LLM 的 Success@1 分數也僅為 21%，可能無法滿足可靠的自動儲存庫層級程式碼翻譯需求。最後，我們進行詳細的錯誤分析，並強調當前 LLM 在儲存庫層級程式碼翻譯中的缺陷，這可以作為進一步改進的參考。</paragraph>

##### **YuLan-Mini: An Open Data-efficient Language Model**
2412.17743v1 by Yiwen Hu, Huatong Song, Jia Deng, Jiapeng Wang, Jie Chen, Kun Zhou, Yutao Zhu, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Ji-Rong Wen

Effective pre-training of large language models (LLMs) has been challenging
due to the immense resource demands and the complexity of the technical
processes involved. This paper presents a detailed technical report on
YuLan-Mini, a highly capable base model with 2.42B parameters that achieves
top-tier performance among models of similar parameter scale. Our pre-training
approach focuses on enhancing training efficacy through three key technical
contributions: an elaborate data pipeline combines data cleaning with data
schedule strategies, a robust optimization method to mitigate training
instability, and an effective annealing approach that incorporates targeted
data selection and long context training. Remarkably, YuLan-Mini, trained on
1.08T tokens, achieves performance comparable to industry-leading models that
require significantly more data. To facilitate reproduction, we release the
full details of the data composition for each training phase. Project details
can be accessed at the following link: https://github.com/RUC-GSAI/YuLan-Mini.

摘要：由於龐大的資源需求和技術流程的複雜性，大型語言模型 (LLM) 的有效預訓練一直具有挑戰性。本文針對 YuLan-Mini 提出了一份詳細的技術報告，這是一個功能強大的基礎模型，擁有 2.42B 個參數，在類似參數規模的模型中取得了頂尖的效能。我們的預訓練方法專注於透過三個關鍵技術貢獻來增強訓練效能：精緻的資料管道將資料清理與資料排程策略結合在一起，穩健的最佳化方法來減輕訓練的不穩定性，以及結合目標資料選取和長脈絡訓練的有效退火方法。值得注意的是，在 1.08T 個符號上訓練的 YuLan-Mini，其效能可與需要更多資料的業界領先模型相媲美。為了促進重現，我們公開了每個訓練階段的資料組成的完整詳細資料。專案詳細資訊可透過以下連結取得：https://github.com/RUC-GSAI/YuLan-Mini。

##### **Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization**
2412.17739v1 by Ermo Hua, Che Jiang, Xingtai Lv, Kaiyan Zhang, Ning Ding, Youbang Sun, Biqing Qi, Yuchen Fan, Xue Kai Zhu, Bowen Zhou

Extending the context length of Language Models (LMs) by improving Rotary
Position Embedding (RoPE) has become a trend. While existing works mainly
address RoPE's limitations within attention mechanism, this paper provides an
analysis across nearly all parts of LMs, uncovering their adverse effects on
length generalization for RoPE-based attention. Using Discrete Signal
Processing theory, we show that RoPE enables periodic attention by implicitly
achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is
undermined by the spectral damage caused by: 1) linear layers and activation
functions outside of attention; 2) insufficiently trained frequency components
brought by time-domain truncation. Building on our observations, we propose
Fourier Position Embedding (FoPE), which enhances attention's frequency-domain
properties to improve both its periodic extension and length generalization.
FoPE constructs Fourier Series and zero-outs the destructive frequency
components, increasing model robustness against the spectrum damage.
Experiments across various model scales show that, within varying context
windows, FoPE can maintain a more stable perplexity and a more consistent
accuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several
analyses and ablations bring further support to our method and theoretical
modeling.

摘要：借由改良旋轉位置嵌入 (RoPE)，進而延伸語言模型 (LM) 的內容長度，已成為一種趨勢。雖然現有研究主要探討 RoPE 在注意機制中的限制，但本文提供了一個幾乎涵蓋 LM 所有部分的分析，揭露 RoPE 基於注意的長度概化對其產生的不利影響。我們使用離散信號處理理論，證明 RoPE 透過隱含地達成非均勻離散傅立葉轉換，進而實現週期性注意。然而，這種週期性受到以下因素造成的頻譜損害所破壞：1) 注意之外的線性層和激活函數；2) 時域截斷所帶來的頻率組成訓練不足。根據我們的觀察，我們提出傅立葉位置嵌入 (FoPE)，它增強了注意的頻域特性，以改善其週期性延伸和長度概化。FoPE 建構傅立葉級數，並將破壞性的頻率組成零化，增加模型對頻譜損害的穩健性。在各種模型規模的實驗中顯示，在不同的內容視窗中，與 RoPE 和 ALiBi 相比，FoPE 能夠維持更穩定的困惑度，以及在針頭大海任務中更一致的準確度。多項分析和消融進一步支持了我們的方法和理論建模。

##### **Chumor 2.0: Towards Benchmarking Chinese Humor Understanding**
2412.17729v1 by Ruiqi He, Yushu He, Longju Bai, Jiarui Liu, Zhenjie Sun, Zenghao Tang, He Wang, Hanchen Xia, Rada Mihalcea, Naihao Deng

Existing humor datasets and evaluations predominantly focus on English,
leaving limited resources for culturally nuanced humor in non-English languages
like Chinese. To address this gap, we construct Chumor, the first Chinese humor
explanation dataset that exceeds the size of existing humor datasets. Chumor is
sourced from Ruo Zhi Ba, a Chinese Reddit-like platform known for sharing
intellectually challenging and culturally specific jokes. We test ten LLMs
through direct and chain-of-thought prompting, revealing that Chumor poses
significant challenges to existing LLMs, with their accuracy slightly above
random and far below human. In addition, our analysis highlights that
human-annotated humor explanations are significantly better than those
generated by GPT-4o and ERNIE-4-turbo. We release Chumor at
https://huggingface.co/datasets/dnaihao/Chumor, our project page is at
https://dnaihao.github.io/Chumor-dataset/, our leaderboard is at
https://huggingface.co/spaces/dnaihao/Chumor, and our codebase is at
https://github.com/dnaihao/Chumor-dataset.

摘要：現有的幽默數據集和評估主要集中在英語上，
為非英語語言（如中文）中具有文化差異的幽默留下了有限的資源。為了解決這個差距，我們構建了 Chumor，這是第一個中文幽默
解釋數據集，其規模超過了現有的幽默數據集。Chumor 來自 Ruo Zhi Ba，一個類似於 Reddit 的中文平台，以分享具有挑戰性的智力題和具有文化特色的笑話而聞名。我們通過直接和思想鏈提示測試了十個 LLM，結果表明 Chumor 對現有的 LLM 構成了重大挑戰，其準確性僅略高於隨機，遠低於人類。此外，我們的分析強調，
人工註釋的幽默解釋顯著優於 GPT-4o 和 ERNIE-4-turbo 生成的解釋。我們在
https://huggingface.co/datasets/dnaihao/Chumor 上發布了 Chumor，我們的項目頁面在
https://dnaihao.github.io/Chumor-dataset/，我們的排行榜在
https://huggingface.co/spaces/dnaihao/Chumor，我們的代碼庫在
https://github.com/dnaihao/Chumor-dataset。

##### **Knowledge Editing through Chain-of-Thought**
2412.17727v1 by Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu

Large Language Models (LLMs) have demonstrated exceptional capabilities
across a wide range of natural language processing (NLP) tasks. However,
keeping these models up-to-date with evolving world knowledge remains a
significant challenge due to the high costs of frequent retraining. To address
this challenge, knowledge editing techniques have emerged to update LLMs with
new information without rebuilding the model from scratch. Among these, the
in-context editing paradigm stands out for its effectiveness in integrating new
knowledge while preserving the model's original capabilities. Despite its
potential, existing in-context knowledge editing methods are often
task-specific, focusing primarily on multi-hop QA tasks using structured
knowledge triples. Moreover, their reliance on few-shot prompting for task
decomposition makes them unstable and less effective in generalizing across
diverse tasks.
  In response to these limitations, we propose EditCoT, a novel knowledge
editing framework that flexibly and efficiently updates LLMs across various
tasks without retraining. EditCoT works by generating a chain-of-thought (CoT)
for a given input and then iteratively refining this CoT process using a CoT
editor based on updated knowledge. We evaluate EditCoT across a diverse range
of benchmarks, covering multiple languages and tasks. The results demonstrate
that our approach achieves state-of-the-art performance while offering superior
generalization, effectiveness, and stability compared to existing methods,
marking a significant advancement in the field of knowledge updating. Code and
data are available at: https://github.com/bebr2/EditCoT.

摘要：大型語言模型 (LLM) 已在廣泛的自然語言處理 (NLP) 任務中展現出非凡的能力。然而，由於頻繁重新訓練的成本高昂，讓這些模型與不斷演進的世界知識保持同步仍是一項重大挑戰。為了應對此挑戰，知識編輯技術應運而生，可以在不從頭重建模型的情況下，使用新資訊更新 LLM。其中，情境編輯範例因其在整合新知識的同時，還能保留模型的原始能力而脫穎而出。儘管有其潛力，現有的情境知識編輯方法通常是特定於任務的，主要專注於使用結構化知識三元組的多跳問答任務。此外，它們依賴於少次提示來進行任務分解，這使得它們不穩定，且在跨不同任務進行概括時效果較差。
為了回應這些限制，我們提出了 EditCoT，這是一個新穎的知識編輯架構，可以在不重新訓練的情況下，靈活且有效地跨各種任務更新 LLM。EditCoT 的運作方式是為給定的輸入產生一個思考鏈 (CoT)，然後使用基於更新知識的 CoT 編輯器，反覆優化這個 CoT 程序。我們在涵蓋多種語言和任務的多元基準上評估 EditCoT。結果表明，與現有方法相比，我們的做法達到了最先進的效能，同時提供了優異的概括性、有效性和穩定性，標誌著知識更新領域的重大進展。程式碼和資料可於以下網址取得：https://github.com/bebr2/EditCoT。

##### **VidTwin: Video VAE with Decoupled Structure and Dynamics**
2412.17726v1 by Yuchi Wang, Junliang Guo, Xinyi Xie, Tianyu He, Xu Sun, Jiang Bian

Recent advancements in video autoencoders (Video AEs) have significantly
improved the quality and efficiency of video generation. In this paper, we
propose a novel and compact video autoencoder, VidTwin, that decouples video
into two distinct latent spaces: Structure latent vectors, which capture
overall content and global movement, and Dynamics latent vectors, which
represent fine-grained details and rapid movements. Specifically, our approach
leverages an Encoder-Decoder backbone, augmented with two submodules for
extracting these latent spaces, respectively. The first submodule employs a
Q-Former to extract low-frequency motion trends, followed by downsampling
blocks to remove redundant content details. The second averages the latent
vectors along the spatial dimension to capture rapid motion. Extensive
experiments show that VidTwin achieves a high compression rate of 0.20% with
high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and
performs efficiently and effectively in downstream generative tasks. Moreover,
our model demonstrates explainability and scalability, paving the way for
future research in video latent representation and generation. Our code has
been released at https://github.com/microsoft/VidTok/tree/main/vidtwin.

摘要：最近影片自動編碼器 (影片 AE) 的進步大幅提升了影片生成的品質和效率。在本文中，我們提出一個新穎且精簡的影片自動編碼器 VidTwin，它將影片解耦成兩個不同的潛在空間：結構潛在向量，用於擷取整體內容和全局動作；以及動態潛在向量，用於表示細微的細節和快速動作。具體來說，我們的做法利用一個編碼器-解碼器主幹，並擴充兩個子模組來分別萃取這些潛在空間。第一個子模組採用 Q-Former 來萃取低頻率動作趨勢，接著是降採樣區塊以移除多餘的內容細節。第二個子模組沿著空間維度平均潛在向量以擷取快速動作。廣泛的實驗顯示，VidTwin 達到 0.20% 的高壓縮率，且重建品質很高（在 MCL-JCV 資料集上 PSNR 為 28.14），而且在下游生成任務中表現得有效率且有效。此外，我們的模型展現了解釋性和可擴充性，為未來的影片潛在表示和生成研究鋪路。我們的程式碼已在 https://github.com/microsoft/VidTok/tree/main/vidtwin 發布。

##### **SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC**
2412.17707v1 by Yue Deng, Yan Yu, Weiyu Ma, Zirui Wang, Wenhui Zhu, Jian Zhao, Yin Zhang

The availability of challenging simulation environments is pivotal for
advancing the field of Multi-Agent Reinforcement Learning (MARL). In
cooperative MARL settings, the StarCraft Multi-Agent Challenge (SMAC) has
gained prominence as a benchmark for algorithms following centralized training
with decentralized execution paradigm. However, with continual advancements in
SMAC, many algorithms now exhibit near-optimal performance, complicating the
evaluation of their true effectiveness. To alleviate this problem, in this
work, we highlight a critical issue: the default opponent policy in these
environments lacks sufficient diversity, leading MARL algorithms to overfit and
exploit unintended vulnerabilities rather than learning robust strategies. To
overcome these limitations, we propose SMAC-HARD, a novel benchmark designed to
enhance training robustness and evaluation comprehensiveness. SMAC-HARD
supports customizable opponent strategies, randomization of adversarial
policies, and interfaces for MARL self-play, enabling agents to generalize to
varying opponent behaviors and improve model stability. Furthermore, we
introduce a black-box testing framework wherein agents are trained without
exposure to the edited opponent scripts but are tested against these scripts to
evaluate the policy coverage and adaptability of MARL algorithms. We conduct
extensive evaluations of widely used and state-of-the-art algorithms on
SMAC-HARD, revealing the substantial challenges posed by edited and mixed
strategy opponents. Additionally, the black-box strategy tests illustrate the
difficulty of transferring learned policies to unseen adversaries. We envision
SMAC-HARD as a critical step toward benchmarking the next generation of MARL
algorithms, fostering progress in self-play methods for multi-agent systems.
Our code is available at https://github.com/devindeng94/smac-hard.

摘要：<paragraph>具有挑戰性的模擬環境的可用性對於推進多智能體強化學習 (MARL) 領域至關重要。在合作式 MARL 設置中，星海爭霸多智能體挑戰 (SMAC) 已作為遵循集中訓練和分散執行範例的演算法的基準而聲名大噪。然而，隨著 SMAC 的持續進步，許多演算法現在表現出接近最佳的效能，這使得評估其真實效能變得複雜。為了緩解這個問題，在這項工作中，我們強調了一個關鍵問題：這些環境中的預設對手策略缺乏足夠的多樣性，導致 MARL 演算法過度擬合並利用無意的漏洞，而不是學習穩健的策略。為了克服這些限制，我們提出了 SMAC-HARD，這是一個新基準，旨在增強訓練的穩健性和評估的全面性。SMAC-HARD 支援自訂對手策略、對抗策略隨機化，以及 MARL 自我對弈的介面，使智能體能夠概化到不同的對手行為並改善模型的穩定性。此外，我們引入了一個黑盒測試架構，其中智能體在沒有接觸到已編輯對手腳本的情況下接受訓練，但會針對這些腳本進行測試，以評估 MARL 演算法的政策涵蓋範圍和適應性。我們對 SMAC-HARD 上廣泛使用和最先進的演算法進行了廣泛的評估，揭示了已編輯和混合策略對手帶來的重大挑戰。此外，黑盒策略測試說明了將學習到的策略轉移到未見過的對手的難度。我們將 SMAC-HARD 視為對下一代 MARL 演算法進行基準測試的關鍵一步，促進多智能體系統自我對弈方法的進展。我們的程式碼可在 https://github.com/devindeng94/smac-hard 取得。</paragraph>

##### **From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question Answering**
2412.17701v1 by Nathaniel Weir, Bhavana Dalvi Mishra, Orion Weller, Oyvind Tafjord, Sam Hornstein, Alexander Sabol, Peter Jansen, Benjamin Van Durme, Peter Clark

Recent reasoning methods (e.g., chain-of-thought, entailment reasoning) help
users understand how language models (LMs) answer a single question, but they
do little to reveal the LM's overall understanding, or "theory," about the
question's $\textit{topic}$, making it still hard to trust the model. Our goal
is to materialize such theories - here called $\textit{microtheories}$ (a
linguistic analog of logical microtheories) - as a set of sentences
encapsulating an LM's core knowledge about a topic. These statements
systematically work together to entail answers to a $\textit{set}$ of questions
to both engender trust and improve performance. Our approach is to first
populate a knowledge store with (model-generated) sentences that entail answers
to training questions and then distill those down to a core microtheory that is
concise, general, and non-redundant. We show that, when added to a general
corpus (e.g., Wikipedia), microtheories can supply critical, topical
information not necessarily present in the corpus, improving both a model's
ability to ground its answers to verifiable knowledge (i.e., show how answers
are systematically entailed by documents in the corpus, fully grounding up to
+8% more answers), and the accuracy of those grounded answers (up to +8%
absolute). We also show that, in a human evaluation in the medical domain, our
distilled microtheories contain a significantly higher concentration of
topically critical facts than the non-distilled knowledge store. Finally, we
show we can quantify the coverage of a microtheory for a topic (characterized
by a dataset) using a notion of $p$-relevance. Together, these suggest that
microtheories are an efficient distillation of an LM's topic-relevant
knowledge, that they can usefully augment existing corpora, and can provide
both performance gains and an interpretable, verifiable window into the model's
knowledge of a topic.

摘要：<paragraph>最近的推理方法（例如，思维链、蕴含推理）有助于
用户了解语言模型 (LM) 如何回答单个问题，但它们
几乎没有揭示 LM 对问题的整体理解或“理论”，$\textit{主题}$，这使得仍然难以信任该模型。我们的目标
是将这些理论具体化 - 这里称为 $\textit{微理论}$（逻辑微理论的语言类比） - 作为一组句子
封装 LM 关于某个主题的核心知识。这些陈述
系统地协同工作，以得出对 $\textit{一组}$ 问题的答案，既能培养信任又能提高性能。我们的方法首先
使用（模型生成的）句子填充知识库，这些句子蕴含对训练问题的答案，然后将这些句子提炼成一个核心微理论，该理论简洁、通用且不冗余。我们表明，当添加到通用
语料库（例如，维基百科）中时，微理论可以提供语料库中不一定存在的关键主题信息，从而提高模型
根据可验证知识回答问题的能力（即，展示答案如何被语料库中的文档系统地蕴含，完全支持高达
+8% 的答案），以及这些有根据的答案的准确性（高达 +8%
绝对）。我们还表明，在医学领域的评估中，我们的
提炼微理论包含的主题关键事实浓度显着高于未提炼的知识库。最后，我们
表明我们可以使用 $p$-相关性的概念量化微理论对某个主题（由数据集表征）的覆盖范围。总之，这些表明
微理论是 LM 与主题相关的知识的有效提炼，它们可以有用地扩充现有语料库，并且可以提供
性能提升和可解释的、可验证的窗口，以便了解模型对某个主题的知识。</paragraph>

##### **Understanding the Logic of Direct Preference Alignment through Logic**
2412.17696v1 by Kyle Richardson, Vivek Srikumar, Ashish Sabharwal

Recent direct preference alignment algorithms (DPA), such as DPO, have shown
great promise in aligning large language models to human preferences. While
this has motivated the development of many new variants of the original DPO
loss, understanding the differences between these recent proposals, as well as
developing new DPA loss functions, remains difficult given the lack of a
technical and conceptual framework for reasoning about the underlying semantics
of these algorithms. In this paper, we attempt to remedy this by formalizing
DPA losses in terms of discrete reasoning problems. Specifically, we ask: Given
an existing DPA loss, can we systematically derive a symbolic expression that
characterizes its semantics? How do the semantics of two losses relate to each
other? We propose a novel formalism for characterizing preference losses for
single model and reference model based approaches, and identify symbolic forms
for a number of commonly used DPA variants. Further, we show how this formal
view of preference learning sheds new light on both the size and structure of
the DPA loss landscape, making it possible to not only rigorously characterize
the relationships between recent loss proposals but also to systematically
explore the landscape and derive new loss functions from first principles. We
hope our framework and findings will help provide useful guidance to those
working on human AI alignment.

摘要：最近的直接偏好對齊演算法（DPA），例如 DPO，已顯示出將大型語言模型與人類偏好對齊的巨大希望。雖然這激勵了許多原始 DPO 損失的新變體的開發，但由於缺乏技術和概念框架來推理這些演算法的基礎語義，因此了解這些最近的提案之間的差異以及開發新的 DPA 損失函數仍然很困難。在本文中，我們嘗試通過形式化 DPA 損失來補救這個問題，即離散推理問題。具體來說，我們問：給定現有的 DPA 損失，我們能系統地推導出一個符號表達式來表徵其語義嗎？兩個損失的語義如何相互關聯？我們提出了一種新穎的形式主義來表徵基於單一模型和參考模型方法的偏好損失，並識別了許多常用的 DPA 變體的符號形式。此外，我們展示了這種偏好學習的形式觀點如何為 DPA 損失景觀的大小和結構提供新的見解，這使得不僅可以嚴格表徵最近的損失提案之間的關係，還可以系統地探索景觀並從第一原理推導出新的損失函數。我們希望我們的框架和發現將有助於為從事人類 AI 對齊的人們提供有用的指導。

##### **FedTLU: Federated Learning with Targeted Layer Updates**
2412.17692v1 by Jong-Ik Park, Carlee Joe-Wong

Federated learning (FL) addresses privacy concerns in language modeling by
enabling multiple clients to contribute to training language models. However,
non-IID (identically and independently distributed) data across clients often
limits FL's performance. This issue is especially challenging during model
fine-tuning, as noise due to variations in clients' data distributions can harm
model convergence near the optimum. This paper proposes a targeted layer update
strategy for fine-tuning in FL. Instead of randomly updating layers of the
language model, as often done in practice, we use a scoring mechanism to
identify and update the most critical layers, avoiding excessively noisy or
even poisoned updates by freezing the parameters in other layers. We show in
extensive experiments that our method improves convergence and performance in
non-IID settings, offering a more efficient approach to fine-tuning federated
language models.

摘要：聯邦學習 (FL) 透過讓多個用戶參與訓練語言模型，來解決語言建模中的隱私問題。然而，用戶之間的非 IID（獨立同分布）資料通常會限制 FL 的效能。這個問題在模型微調期間特別具有挑戰性，因為用戶資料分佈的差異所造成的雜訊會損害模型在最佳值附近的收斂性。本文針對 FL 中的微調提出了一個目標層更新策略。我們使用評分機制來識別和更新最關鍵的層，而不是像實際操作中常見的那樣隨機更新語言模型的層，藉此避免過度雜訊或甚至透過凍結其他層中的參數來避免中毒更新。我們在廣泛的實驗中表明，我們的模型在非 IID 設定中改善了收斂性和效能，提供了一個更有效率的聯邦語言模型微調方法。

##### **RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF for Conversational QA over KGs with RAG**
2412.17690v1 by Rishiraj Saha Roy, Chris Hinze, Joel Schlotthauer, Farzad Naderi, Viktor Hangya, Andreas Foltyn, Luzian Hahn, Fabian Kuech

Conversational question answering (ConvQA) is a convenient means of searching
over RDF knowledge graphs (KGs), where a prevalent approach is to translate
natural language questions to SPARQL queries. However, SPARQL has certain
shortcomings: (i) it is brittle for complex intents and conversational
questions, and (ii) it is not suitable for more abstract needs. Instead, we
propose a novel two-pronged system where we fuse: (i) SQL-query results over a
database automatically derived from the KG, and (ii) text-search results over
verbalizations of KG facts. Our pipeline supports iterative retrieval: when the
results of any branch are found to be unsatisfactory, the system can
automatically opt for further rounds. We put everything together in a retrieval
augmented generation (RAG) setup, where an LLM generates a coherent response
from accumulated search results. We demonstrate the superiority of our proposed
system over several baselines on a knowledge graph of BMW automobiles.

摘要：對話式問答 (ConvQA) 是在 RDF 知識圖譜 (KG) 上進行搜尋的便利方式，其中一種普遍的方法是將自然語言問題轉換成 SPARQL 查詢。然而，SPARQL 有某些缺點：(i) 對於複雜的意圖和對話式問題而言，它很脆弱，而且 (ii) 它不適合更抽象的需求。我們提出一個新穎的雙管齊下的系統，其中我們融合：(i) 從自動從 KG 衍生的資料庫中取得的 SQL 查詢結果，以及 (ii) KG 事實的口頭表達上的文字搜尋結果。我們的管道支援反覆檢索：當發現任何分支的結果不令人滿意時，系統可以自動選擇進行進一步的回合。我們將所有內容組合在檢索擴充產生 (RAG) 設定中，其中 LLM 從累積的搜尋結果中產生連貫的回應。我們在 BMW 汽車的知識圖譜上展示了我們提出的系統優於多個基準的優越性。

##### **Large Language Model Safety: A Holistic Survey**
2412.17686v1 by Dan Shi, Tianhao Shen, Yufei Huang, Zhigen Li, Yongqi Leng, Renren Jin, Chuang Liu, Xinwei Wu, Zishan Guo, Linhao Yu, Ling Shi, Bojian Jiang, Deyi Xiong

The rapid development and deployment of large language models (LLMs) have
introduced a new frontier in artificial intelligence, marked by unprecedented
capabilities in natural language understanding and generation. However, the
increasing integration of these models into critical applications raises
substantial safety concerns, necessitating a thorough examination of their
potential risks and associated mitigation strategies.
  This survey provides a comprehensive overview of the current landscape of LLM
safety, covering four major categories: value misalignment, robustness to
adversarial attacks, misuse, and autonomous AI risks. In addition to the
comprehensive review of the mitigation methodologies and evaluation resources
on these four aspects, we further explore four topics related to LLM safety:
the safety implications of LLM agents, the role of interpretability in
enhancing LLM safety, the technology roadmaps proposed and abided by a list of
AI companies and institutes for LLM safety, and AI governance aimed at LLM
safety with discussions on international cooperation, policy proposals, and
prospective regulatory directions.
  Our findings underscore the necessity for a proactive, multifaceted approach
to LLM safety, emphasizing the integration of technical solutions, ethical
considerations, and robust governance frameworks. This survey is intended to
serve as a foundational resource for academy researchers, industry
practitioners, and policymakers, offering insights into the challenges and
opportunities associated with the safe integration of LLMs into society.
Ultimately, it seeks to contribute to the safe and beneficial development of
LLMs, aligning with the overarching goal of harnessing AI for societal
advancement and well-being. A curated list of related papers has been publicly
available at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.

摘要：大型語言模型 (LLM) 的快速發展和部署引進了人工智慧的新領域，以自然語言理解和生成的空前能力為標誌。然而，這些模型與關鍵應用程式整合的增加引發了重大的安全問題，需要徹底檢查其潛在風險和相關的緩解策略。
這項調查提供了 LLM 安全現況的全面概述，涵蓋四個主要類別：價值錯位、對抗攻擊的穩健性、誤用和自主 AI 風險。除了對這四個方面的緩解方法和評估資源進行全面回顧外，我們進一步探討了與 LLM 安全相關的四個主題：LLM 代理的安全性影響、可解釋性在增強 LLM 安全性中的作用、AI 公司和機構為 LLM 安全性提出的和遵守的技术路線圖，以及針對 LLM 安全性的 AI 治理，討論了國際合作、政策提案和潛在的法規方向。
我們的研究結果強調了主動、多方面的 LLM 安全方法的必要性，強調技術解決方案、倫理考量和穩健的治理架構的整合。這項調查旨在作為學術研究人員、產業從業者和政策制定者的基礎資源，提供對 LLM 安全整合到社會中相關挑戰和機會的見解。最終，它旨在促進 LLM 的安全和有益發展，與利用 AI 促進社會進步和福祉的總體目標保持一致。已公開整理的相關論文清單 https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers。

##### **Generating Completions for Fragmented Broca's Aphasic Sentences Using Large Language Models**
2412.17669v1 by Sijbren van Vaals, Yevgen Matusevych, Frank Tsiwah

Broca's aphasia is a type of aphasia characterized by non-fluent, effortful
and fragmented speech production with relatively good comprehension. Since
traditional aphasia treatment methods are often time-consuming,
labour-intensive, and do not reflect real-world conversations, applying natural
language processing based approaches such as Large Language Models (LLMs) could
potentially contribute to improving existing treatment approaches. To address
this issue, we explore the use of sequence-to-sequence LLMs for completing
fragmented Broca's aphasic sentences. We first generate synthetic Broca's
aphasic data using a rule-based system designed to mirror the linguistic
characteristics of Broca's aphasic speech. Using this synthetic data, we then
fine-tune four pre-trained LLMs on the task of completing fragmented sentences.
We evaluate our fine-tuned models on both synthetic and authentic Broca's
aphasic data. We demonstrate LLMs' capability for reconstructing fragmented
sentences, with the models showing improved performance with longer input
utterances. Our result highlights the LLMs' potential in advancing
communication aids for individuals with Broca's aphasia and possibly other
clinical populations.

摘要：布羅卡失語症是一種失語症，其特徵是非流暢、費力且斷斷續續的言語產生，但理解力相對良好。由於傳統的失語症治療方法通常耗時、費力且無法反映現實世界的對話，因此應用基於自然語言處理的方法（例如大型語言模型 (LLM)）可能會對改善現有的治療方法有所幫助。為了解決這個問題，我們探討使用序列到序列 LLM 來完成斷斷續續的布羅卡失語症句子。我們首先使用基於規則的系統產生合成布羅卡失語症數據，該系統旨在反映布羅卡失語症言語的語言特徵。使用這些合成數據，我們隨後對四個預訓練的 LLM 進行微調，以完成斷句的任務。我們對微調模型在合成和真實的布羅卡失語症數據上進行評估。我們展示了 LLM 重建斷句的能力，這些模型在輸入語句較長時顯示出更好的性能。我們的結果突出了 LLM 在推進布羅卡失語症患者的溝通輔具方面的潛力，以及其他臨床人群的潛力。

##### **Enhanced Temporal Processing in Spiking Neural Networks for Static Object Detection Using 3D Convolutions**
2412.17654v1 by Huaxu He

Spiking Neural Networks (SNNs) are a class of network models capable of
processing spatiotemporal information, with event-driven characteristics and
energy efficiency advantages. Recently, directly trained SNNs have shown
potential to match or surpass the performance of traditional Artificial Neural
Networks (ANNs) in classification tasks. However, in object detection tasks,
directly trained SNNs still exhibit a significant performance gap compared to
ANNs when tested on frame-based static object datasets (such as COCO2017).
Therefore, bridging this performance gap and enabling directly trained SNNs to
achieve performance comparable to ANNs on these static datasets has become one
of the key challenges in the development of SNNs.To address this challenge,
this paper focuses on enhancing the SNN's unique ability to process
spatiotemporal information. Spiking neurons, as the core components of SNNs,
facilitate the exchange of information between different temporal channels
during the process of converting input floating-point data into binary spike
signals. However, existing neuron models still have certain limitations in the
communication of temporal information. Some studies have even suggested that
disabling the backpropagation in the time dimension during SNN training can
still yield good training results. To improve the SNN handling of temporal
information, this paper proposes replacing traditional 2D convolutions with 3D
convolutions, thus directly incorporating temporal information into the
convolutional process. Additionally, temporal information recurrence mechanism
is introduced within the neurons to further enhance the neurons' efficiency in
utilizing temporal information.Experimental results show that the proposed
method enables directly trained SNNs to achieve performance levels comparable
to ANNs on the COCO2017 and VOC datasets.

摘要：尖峰神经網路 (SNN) 是一種網路模型，具有處理時空資訊的能力，具備事件驅動特性和能源效率優勢。最近，直接訓練的 SNN 已展現出在分類任務中與傳統人工神經網路 (ANN) 相匹配或超越其效能的潛力。然而，在目標偵測任務中，直接訓練的 SNN 在基於幀的靜態目標資料集（例如 COCO2017）上測試時，與 ANN 相比仍存在顯著的效能差距。因此，彌合此效能差距並使直接訓練的 SNN 能在這些靜態資料集上達到與 ANN 相當的效能，已成為 SNN 發展中的關鍵挑戰之一。為了應對此挑戰，本文重點在於增強 SNN 處理時空資訊的獨特能力。尖峰神經元作為 SNN 的核心元件，在將輸入浮點資料轉換為二進制尖峰訊號的過程中，促進不同時間通道之間的資訊交換。然而，現有的神經元模型在時間資訊的傳遞上仍有某些限制。一些研究甚至表明，在 SNN 訓練期間停用時間維度的反向傳播仍然可以產生良好的訓練結果。為了改善 SNN 對時間資訊的處理，本文建議以 3D 捲積取代傳統的 2D 捲積，從而將時間資訊直接納入捲積過程中。此外，在神經元中引入時間資訊遞迴機制，以進一步增強神經元利用時間資訊的效率。實驗結果表明，所提出的方法使直接訓練的 SNN 能在 COCO2017 和 VOC 資料集上達到與 ANN 相當的效能水準。

##### **Detecting anxiety and depression in dialogues: a multi-label and explainable approach**
2412.17651v1 by Francisco de Arriba-Pérez, Silvia García-Méndez

Anxiety and depression are the most common mental health issues worldwide,
affecting a non-negligible part of the population. Accordingly, stakeholders,
including governments' health systems, are developing new strategies to promote
early detection and prevention from a holistic perspective (i.e., addressing
several disorders simultaneously). In this work, an entirely novel system for
the multi-label classification of anxiety and depression is proposed. The input
data consists of dialogues from user interactions with an assistant chatbot.
Another relevant contribution lies in using Large Language Models (LLMs) for
feature extraction, provided the complexity and variability of language. The
combination of LLMs, given their high capability for language understanding,
and Machine Learning (ML) models, provided their contextual knowledge about the
classification problem thanks to the labeled data, constitute a promising
approach towards mental health assessment. To promote the solution's
trustworthiness, reliability, and accountability, explainability descriptions
of the model's decision are provided in a graphical dashboard. Experimental
results on a real dataset attain 90 % accuracy, improving those in the prior
literature. The ultimate objective is to contribute in an accessible and
scalable way before formal treatment occurs in the healthcare systems.

摘要：焦慮和憂鬱症是全球最常見的心理健康問題，
影響著人口中不可忽視的一部分。因此，利益相關者，
包括政府的衛生系統，正在制定新的策略來促進
從整體角度及早發現和預防（即同時解決
多種疾病）。在這項工作中，一個完全新穎的系統
用於焦慮和憂鬱症的多標籤分類。輸入
資料包含使用者與助理聊天機器人互動的對話。
另一個相關的貢獻在於使用大型語言模型 (LLM) 進行
特徵萃取，提供了語言的複雜性和可變性。LLM 的組合，
由於它們對語言理解的高能力，以及機器學習 (ML) 模型，
由於它們對分類問題的背景知識，這要歸功於標籤資料，
構成了一種有希望的心理健康評估方法。為了促進解決方案的
可信度、可靠性和問責制，模型決策的可解釋性描述
在圖形儀表板中提供。在真實資料集上的實驗結果達到 90% 的準確度，
改進了先前文獻中的準確度。最終目標是以一種可訪問且
可擴展的方式做出貢獻，在醫療保健系統中進行正式治療之前。

##### **An Adaptive Framework for Multi-View Clustering Leveraging Conditional Entropy Optimization**
2412.17647v1 by Lijian Li

Multi-view clustering (MVC) has emerged as a powerful technique for
extracting valuable insights from data characterized by multiple perspectives
or modalities. Despite significant advancements, existing MVC methods struggle
with effectively quantifying the consistency and complementarity among views,
and are particularly susceptible to the adverse effects of noisy views, known
as the Noisy-View Drawback (NVD). To address these challenges, we propose
CE-MVC, a novel framework that integrates an adaptive weighting algorithm with
a parameter-decoupled deep model. Leveraging the concept of conditional entropy
and normalized mutual information, CE-MVC quantitatively assesses and weights
the informative contribution of each view, facilitating the construction of
robust unified representations. The parameter-decoupled design enables
independent processing of each view, effectively mitigating the influence of
noise and enhancing overall clustering performance. Extensive experiments
demonstrate that CE-MVC outperforms existing approaches, offering a more
resilient and accurate solution for multi-view clustering tasks.

摘要：多視角聚類 (MVC) 已成為一種強大的技術，可用於從以多重觀點或模式為特徵的數據中萃取出有價值的見解。儘管有顯著的進展，現有的 MVC 方法仍難以有效量化視角之間的一致性和互補性，而且特別容易受到雜訊視角的不利影響，這稱為雜訊視角缺點 (NVD)。為了應對這些挑戰，我們提出了 CE-MVC，一個將自適應加權演算法與參數解耦深度模型整合在一起的新穎架構。透過利用條件熵和正規化互惠資訊的概念，CE-MVC 定量評估和加權每個視角的資訊貢獻，進而促進建立穩健的統一表徵。參數解耦設計能獨立處理每個視角，有效減輕雜訊的影響，並提升整體聚類效能。廣泛的實驗證明，CE-MVC 優於現有方法，為多視角聚類任務提供了更具韌性和準確性的解決方案。

##### **SCBench: A Sports Commentary Benchmark for Video LLMs**
2412.17637v1 by Kuangzhi Ge, Lingjun Chen, Kevin Zhang, Yulin Luo, Tianyu Shi, Liaoyuan Fan, Xiang Li, Guanqun Wang, Shanghang Zhang

Recently, significant advances have been made in Video Large Language Models
(Video LLMs) in both academia and industry. However, methods to evaluate and
benchmark the performance of different Video LLMs, especially their
fine-grained, temporal visual capabilities, remain very limited. On one hand,
current benchmarks use relatively simple videos (e.g., subtitled movie clips)
where the model can understand the entire video by processing just a few
frames. On the other hand, their datasets lack diversity in task format,
comprising only QA or multi-choice QA, which overlooks the models' capacity for
generating in-depth and precise texts. Sports videos, which feature intricate
visual information, sequential events, and emotionally charged commentary,
present a critical challenge for Video LLMs, making sports commentary an ideal
benchmarking task. Inspired by these challenges, we propose a novel task:
sports video commentary generation, developed $\textbf{SCBench}$ for Video
LLMs. To construct such a benchmark, we introduce (1) $\textbf{SCORES}$, a
six-dimensional metric specifically designed for our task, upon which we
propose a GPT-based evaluation method, and (2) $\textbf{CommentarySet}$, a
dataset consisting of 5,775 annotated video clips and ground-truth labels
tailored to our metric. Based on SCBench, we conduct comprehensive evaluations
on multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought
baseline methods. Our results found that InternVL-Chat-2 achieves the best
performance with 5.44, surpassing the second-best by 1.04. Our work provides a
fresh perspective for future research, aiming to enhance models' overall
capabilities in complex visual understanding tasks. Our dataset will be
released soon.

摘要：<paragraph>最近，在学术界和产业界中，视频大语言模型（Video LLM）取得了重大进展。然而，评估和比较不同视频 LLM 性能的方法，尤其是它们细粒度的时序视觉能力，仍然非常有限。一方面，当前的基准使用相对简单的视频（例如，带字幕的电影剪辑），其中模型可以通过处理少数帧来理解整个视频。另一方面，它们的数据集缺乏任务格式的多样性，仅包括问答或多项选择问答，这忽略了模型生成深入且精确文本的能力。体育视频具有复杂的可视信息、顺序事件和充满情感的评论，这对视频 LLM 来说是一个严峻的挑战，使得体育评论成为一个理想的基准任务。受这些挑战的启发，我们提出了一项新任务：体育视频评论生成，为视频 LLM 开发了 $\textbf{SCBench}$。为了构建这样一个基准，我们引入了 (1) $\textbf{SCORES}$，一个专门为我们的任务设计的六维指标，在此基础上我们提出了一种基于 GPT 的评估方法，以及 (2) $\textbf{CommentarySet}$，一个由 5,775 个带注释的视频剪辑和针对我们的指标定制的地面实况标签组成的数据集。基于 SCBench，我们对多个视频 LLM（例如 VILA、Video-LLaVA 等）和思维链基线方法进行了综合评估。我们的结果发现 InternVL-Chat-2 以 5.44 的成绩取得了最佳性能，比第二名高出 1.04。我们的工作为未来的研究提供了新的视角，旨在增强模型在复杂视觉理解任务中的整体能力。我们的数据集将很快发布。</paragraph>

##### **ANID: How Far Are We? Evaluating the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance**
2412.17632v1 by Renyang Liu, Ziyu Lyu, Wei Zhou, See-Kiong Ng

In the rapidly evolving field of Artificial Intelligence Generated Content
(AIGC), one of the key challenges is distinguishing AI-synthesized images from
natural images. Despite the remarkable capabilities of advanced AI generative
models in producing visually compelling images, significant discrepancies
remain when these images are compared to natural ones. To systematically
investigate and quantify these discrepancies, we introduce an AI-Natural Image
Discrepancy Evaluation benchmark aimed at addressing the critical question:
\textit{how far are AI-generated images (AIGIs) from truly realistic images?}
We have constructed a large-scale multimodal dataset, the Distinguishing
Natural and AI-generated Images (DNAI) dataset, which includes over 440,000
AIGI samples generated by 8 representative models using both unimodal and
multimodal prompts, such as Text-to-Image (T2I), Image-to-Image (I2I), and Text
\textit{vs.} Image-to-Image (TI2I). Our fine-grained assessment framework
provides a comprehensive evaluation of the DNAI dataset across five key
dimensions: naive visual feature quality, semantic alignment in multimodal
generation, aesthetic appeal, downstream task applicability, and coordinated
human validation. Extensive evaluation results highlight significant
discrepancies across these dimensions, underscoring the necessity of aligning
quantitative metrics with human judgment to achieve a holistic understanding of
AI-generated image quality. Code is available at
\href{https://github.com/ryliu68/ANID}{https://github.com/ryliu68/ANID}.

摘要：<paragraph>在快速發展的人工智慧生成內容 (AIGC) 領域中，其中一個主要挑戰是區分 AI 合成影像與自然影像。儘管先進 AI 生成模型在產生視覺上引人注目的影像方面具有非凡的能力，但在將這些影像與自然影像進行比較時，仍存在顯著差異。為了系統地研究和量化這些差異，我們引入了 AI 自然影像差異評估基準，旨在解決這個關鍵問題：\textit{AI 生成的影像 (AIGIs) 與真正逼真的影像相差多遠？}我們構建了一個大型多模態資料集，即區分自然和 AI 生成的影像 (DNAI) 資料集，其中包含超過 440,000 個 AIGI 樣本，這些樣本是由 8 個代表性模型使用單模態和多模態提示生成的，例如文字轉影像 (T2I)、影像轉影像 (I2I) 和文字\textit{vs.}影像轉影像 (TI2I)。我們細緻的評估架構對 DNAI 資料集進行了全面的評估，涵蓋了五個關鍵面向：樸實的視覺特徵品質、多模態生成中的語義對齊、美學吸引力、下游任務適用性以及協調的人類驗證。廣泛的評估結果突顯了這些面向之間的顯著差異，強調了將量化指標與人類判斷相結合以全面了解 AI 生成的影像品質的必要性。程式碼可在\href{https://github.com/ryliu68/ANID}{https://github.com/ryliu68/ANID}取得。</paragraph>

##### **Graph Neural Networks Are Evolutionary Algorithms**
2412.17629v1 by Kaichen Ouyang, Shengwei Fu

In this paper, we reveal the intrinsic duality between graph neural networks
(GNNs) and evolutionary algorithms (EAs), bridging two traditionally distinct
fields. Building on this insight, we propose Graph Neural Evolution (GNE), a
novel evolutionary algorithm that models individuals as nodes in a graph and
leverages designed frequency-domain filters to balance global exploration and
local exploitation. Through the use of these filters, GNE aggregates
high-frequency (diversity-enhancing) and low-frequency (stability-promoting)
information, transforming EAs into interpretable and tunable mechanisms in the
frequency domain. Extensive experiments on benchmark functions demonstrate that
GNE consistently outperforms state-of-the-art algorithms such as GA, DE,
CMA-ES, SDAES, and RL-SHADE, excelling in complex landscapes, optimal solution
shifts, and noisy environments. Its robustness, adaptability, and superior
convergence highlight its practical and theoretical value. Beyond optimization,
GNE establishes a conceptual and mathematical foundation linking EAs and GNNs,
offering new perspectives for both fields. Its framework encourages the
development of task-adaptive filters and hybrid approaches for EAs, while its
insights can inspire advances in GNNs, such as improved global information
propagation and mitigation of oversmoothing. GNE's versatility extends to
solving challenges in machine learning, including hyperparameter tuning and
neural architecture search, as well as real-world applications in engineering
and operations research. By uniting the dynamics of EAs with the structural
insights of GNNs, this work provides a foundation for interdisciplinary
innovation, paving the way for scalable and interpretable solutions to complex
optimization problems.

摘要：<paragraph>在本文中，我們揭示了圖神經網路 (GNN) 和演化演算法 (EA) 之間的內在二元性，連結了兩個傳統上截然不同的領域。基於此見解，我們提出圖神經演化 (GNE)，一種新穎的演化演算法，它將個體建模為圖中的節點，並利用設計的頻域濾波器來平衡整體探索和局部開發。透過使用這些濾波器，GNE 匯聚高頻 (增強多樣性) 和低頻 (促進穩定性) 資訊，將 EA 轉變為頻域中可解釋且可調整的機制。在基準函數上的廣泛實驗證明，GNE 持續優於 GA、DE、CMA-ES、SDAES 和 RL-SHADE 等最先進的演算法，在複雜環境、最佳解位移和雜訊環境中表現出色。其穩健性、適應性和優異的收斂性突顯了其實用和理論價值。除了最佳化之外，GNE 還建立了一個連結 EA 和 GNN 的概念和數學基礎，為這兩個領域提供了新的觀點。其架構鼓勵開發任務自適應濾波器和 EA 的混合方法，而其見解可以激勵 GNN 的進步，例如改善整體資訊傳播和減輕過度平滑。GNE 的多功能性延伸到解決機器學習中的挑戰，包括超參數調整和神經架構搜尋，以及工程和作業研究中的實際應用。透過將 EA 的動態與 GNN 的結構見解結合起來，這項工作為跨領域創新奠定了基礎，為複雜最佳化問題的具可擴充性和可解釋性解決方案鋪平了道路。</paragraph>

##### **Tracking the Feature Dynamics in LLM Training: A Mechanistic Study**
2412.17626v1 by Yang Xu, Yi Wang, Hao Wang

Understanding training dynamics and feature evolution is crucial for the
mechanistic interpretability of large language models (LLMs). Although sparse
autoencoders (SAEs) have been used to identify features within LLMs, a clear
picture of how these features evolve during training remains elusive. In this
study, we: (1) introduce SAE-Track, a method to efficiently obtain a continual
series of SAEs; (2) formulate the process of feature formation and conduct a
mechanistic analysis; and (3) analyze and visualize feature drift during
training. Our work provides new insights into the dynamics of features in LLMs,
enhancing our understanding of training mechanisms and feature evolution.

摘要：了解訓練動態和特徵演變對於大型語言模型 (LLM) 的機制可解釋性至關重要。儘管稀疏自動編碼器 (SAE) 已用於識別 LLM 中的特徵，但這些特徵在訓練期間如何演變的清晰圖像仍然難以捉摸。在這個研究中，我們：(1) 介紹 SAE-Track，一種有效獲取連續 SAE 系列的方法；(2) 制定特徵形成的過程並進行機制分析；以及 (3) 分析和視覺化訓練期間的特徵漂移。我們的研究為 LLM 中特徵的動態提供了新的見解，增強了我們對訓練機制和特徵演變的理解。

##### **Emerging Security Challenges of Large Language Models**
2412.17614v1 by Herve Debar, Sven Dietrich, Pavel Laskov, Emil C. Lupu, Eirini Ntoutsi

Large language models (LLMs) have achieved record adoption in a short period
of time across many different sectors including high importance areas such as
education [4] and healthcare [23]. LLMs are open-ended models trained on
diverse data without being tailored for specific downstream tasks, enabling
broad applicability across various domains. They are commonly used for text
generation, but also widely used to assist with code generation [3], and even
analysis of security information, as Microsoft Security Copilot demonstrates
[18]. Traditional Machine Learning (ML) models are vulnerable to adversarial
attacks [9]. So the concerns on the potential security implications of such
wide scale adoption of LLMs have led to the creation of this working group on
the security of LLMs. During the Dagstuhl seminar on "Network Attack Detection
and Defense - AI-Powered Threats and Responses", the working group discussions
focused on the vulnerability of LLMs to adversarial attacks, rather than their
potential use in generating malware or enabling cyberattacks. Although we note
the potential threat represented by the latter, the role of the LLMs in such
uses is mostly as an accelerator for development, similar to what it is in
benign use. To make the analysis more specific, the working group employed
ChatGPT as a concrete example of an LLM and addressed the following points,
which also form the structure of this report: 1. How do LLMs differ in
vulnerabilities from traditional ML models? 2. What are the attack objectives
in LLMs? 3. How complex it is to assess the risks posed by the vulnerabilities
of LLMs? 4. What is the supply chain in LLMs, how data flow in and out of
systems and what are the security implications? We conclude with an overview of
open challenges and outlook.

摘要：大型語言模型 (LLM) 在短時間內於許多不同產業中獲得創紀錄的採用率，包括教育 [4] 和醫療保健 [23] 等重要領域。LLM 是開放式模型，在多樣化資料上訓練，未針對特定下游任務進行調整，可在各種領域廣泛應用。它們通常用於文字生成，但也廣泛用於協助程式碼生成 [3]，甚至分析安全資訊，就像 Microsoft Security Copilot 所展示的 [18]。傳統機器學習 (ML) 模型容易受到對抗性攻擊 [9]。因此，對於 LLM 廣泛採用潛在安全影響的疑慮，導致成立這個 LLM 安全工作小組。在「網路攻擊偵測與防禦 - AI 驅動的威脅與回應」達格斯圖爾研討會期間，工作小組討論的重點在於 LLM 對抗性攻擊的脆弱性，而非它們在產生惡意軟體或協助網路攻擊中的潛在用途。儘管我們注意到後者所代表的潛在威脅，但 LLM 在此類用途中的角色主要作為開發的加速器，類似於它在良性用途中的角色。為了讓分析更具體，工作小組以 ChatGPT 為 LLM 的具體範例，並探討以下幾點，這些點也構成這份報告的結構：1. LLM 在脆弱性方面與傳統 ML 模型有何不同？2. LLM 中的攻擊目標是什麼？3. 評估 LLM 脆弱性所帶來的風險有多複雜？4. LLM 中的供應鏈是什麼？資料如何流入和流出系統，以及安全影響是什麼？我們最後概述開放式挑戰和展望。

##### **AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation**
2412.17601v1 by Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li

Few-shot learning aims to recognize novel concepts by leveraging prior
knowledge learned from a few samples. However, for visually intensive tasks
such as few-shot semantic segmentation, pixel-level annotations are
time-consuming and costly. Therefore, in this paper, we utilize the more
challenging image-level annotations and propose an adaptive frequency-aware
network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS).
Specifically, we first propose a cross-granularity frequency-aware module (CFM)
that decouples RGB images into high-frequency and low-frequency distributions
and further optimizes semantic structural information by realigning them.
Unlike most existing WFSS methods using the textual information from the
multi-modal language-vision model, e.g., CLIP, in an offline learning manner,
we further propose a CLIP-guided spatial-adapter module (CSM), which performs
spatial domain adaptive transformation on textual information through online
learning, thus providing enriched cross-modal semantic information for CFM.
Extensive experiments on the Pascal-5\textsuperscript{i} and
COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved
state-of-the-art performance. The code is available at
https://github.com/jarch-ma/AFANet.

摘要：小样本学习旨在通过利用从少数样本中学到的先验知识来识别新概念。然而，对于视觉密集型任务（如小样本语义分割），像素级注释既耗时又昂贵。因此，在本文中，我们利用更具挑战性的图像级注释，并提出了一种用于弱监督小样本语义分割（WFSS）的自适应频率感知网络（AFANet）。具体来说，我们首先提出了一种跨粒度频率感知模块（CFM），将 RGB 图像解耦为高频和低频分布，并通过重新对齐进一步优化语义结构信息。与大多数现有的 WFSS 方法不同，后者以离线学习的方式使用来自多模态语言视觉模型（例如 CLIP）的文本信息，我们进一步提出了一种 CLIP 指导的空间适配器模块（CSM），该模块通过在线学习对文本信息执行空间域自适应变换，从而为 CFM 提供丰富的跨模态语义信息。在 Pascal-5i 和 COCO-20i 数据集上的大量实验表明，AFANet 已达到最先进的性能。代码可在 https://github.com/jarch-ma/AFANet 获得。

##### **LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea Generation with Minimal Context**
2412.17596v1 by Kai Ruan, Xuan Wang, Jixiang Hong, Hao Sun

While Large Language Models (LLMs) have demonstrated remarkable capabilities
in scientific tasks, existing evaluation frameworks primarily assess their
performance using rich contextual inputs, overlooking their ability to generate
novel ideas from minimal information. We introduce LiveIdeaBench, a
comprehensive benchmark that evaluates LLMs' scientific creativity and
divergent thinking capabilities using single-keyword prompts. Drawing from
Guilford's creativity theory, our framework employs a dynamic panel of
state-of-the-art LLMs to assess generated ideas across four key dimensions:
originality, feasibility, fluency, and flexibility. Through extensive
experimentation with 20 leading models across 1,180 keywords spanning 18
scientific domains, we reveal that scientific creative ability shows distinct
patterns from general intelligence metrics. Notably, our results demonstrate
that models like QwQ-32B-preview achieve comparable creative performance to
top-tier models like o1-preview, despite significant gaps in their general
intelligence scores. These findings highlight the importance of specialized
evaluation frameworks for scientific creativity and suggest that the
development of creative capabilities in LLMs may follow different trajectories
than traditional problem-solving abilities.

摘要：儘管大型語言模型 (LLM) 在科學任務中展現了非凡的能力，現有的評估架構主要使用豐富的上下文輸入來評估其效能，卻忽略了它們從極少資訊中產生新穎想法的能力。我們引入了 LiveIdeaBench，這是一個綜合基準，它使用單一關鍵字提示來評估 LLM 的科學創造力和發散性思維能力。我們的架構從 Guilford 的創造力理論中汲取靈感，採用最先進的 LLM 動態小組，以四個關鍵面向來評估產生的想法：獨創性、可行性、流暢性和靈活性。透過對 18 個科學領域中 1,180 個關鍵字的 20 個領先模型進行廣泛的實驗，我們發現科學創造力展現出與一般智慧指標不同的明確模式。值得注意的是，我們的結果證明了 QwQ-32B-preview 等模型達到了與 o1-preview 等頂級模型相當的創造力表現，儘管它們的一般智慧評分有顯著差距。這些發現突顯了專門用於科學創造力的評估架構的重要性，並表明 LLM 中創造力能力的發展可能與傳統的解決問題能力遵循不同的軌跡。

##### **Improved Cotton Leaf Disease Classification Using Parameter-Efficient Deep Learning Framework**
2412.17587v1 by Aswini Kumar Patra, Tejashwini Gajurel

Cotton crops, often called "white gold," face significant production
challenges, primarily due to various leaf-affecting diseases. As a major global
source of fiber, timely and accurate disease identification is crucial to
ensure optimal yields and maintain crop health. While deep learning and machine
learning techniques have been explored to address this challenge, there remains
a gap in developing lightweight models with fewer parameters which could be
computationally effective for agricultural practitioners. To address this, we
propose an innovative deep learning framework integrating a subset of trainable
layers from MobileNet, transfer learning, data augmentation, a learning rate
decay schedule, model checkpoints, and early stopping mechanisms. Our model
demonstrates exceptional performance, accurately classifying seven cotton
disease types with an overall accuracy of 98.42% and class-wise precision
ranging from 96% to 100%. This results in significantly enhanced efficiency,
surpassing recent approaches in accuracy and model complexity. The existing
models in the literature have yet to attain such high accuracy, even when
tested on data sets with fewer disease types. The substantial performance
improvement, combined with the lightweight nature of the model, makes it
practically suitable for real-world applications in smart farming. By offering
a high-performing and efficient solution, our framework can potentially address
challenges in cotton cultivation, contributing to sustainable agricultural
practices.

摘要：棉花作物常被称为「白色黄金」，但會面臨重大的生產挑戰，主要是因為各種影響葉子的疾病。作為全球主要的纖維來源，及時且準確地辨識疾病對於確保最佳產量和維持作物健康至關重要。儘管深度學習和機器學習技術已被探索用於解決此挑戰，但對於開發具有較少參數的輕量級模型仍存在差距，而這些模型對於農業從業者而言在運算上可能有效。為了解決此問題，我們提出一個創新的深度學習架構，整合了來自 MobileNet 的可訓練層子集、遷移學習、資料擴充、學習率衰減時間表、模型檢查點和早期停止機制。我們的模型展現了非凡的效能，準確地分類了七種棉花疾病類型，整體準確度為 98.42%，且類別準確度介於 96% 到 100% 之間。這導致顯著提升的效率，在準確度和模型複雜度上超越了最近的方法。文獻中的現有模型尚未達到如此高的準確度，即使在較少疾病類型的資料集上進行測試時也是如此。大幅提升的效能，加上模型的輕量級特性，使其在智慧農業的實際應用中具有實用性。透過提供高性能且高效的解決方案，我們的架構有可能解決棉花種植中的挑戰，進而促成永續的農業實務。

##### **HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data**
2412.17574v1 by Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen

In the domain of Multimodal Large Language Models (MLLMs), achieving
human-centric video understanding remains a formidable challenge. Existing
benchmarks primarily emphasize object and action recognition, often neglecting
the intricate nuances of human emotions, behaviors, and speech visual alignment
within video content. We present HumanVBench, an innovative benchmark
meticulously crafted to bridge these gaps in the evaluation of video MLLMs.
HumanVBench comprises 17 carefully designed tasks that explore two primary
dimensions: inner emotion and outer manifestations, spanning static and
dynamic, basic and complex, as well as single-modal and cross-modal aspects.
With two advanced automated pipelines for video annotation and
distractor-included QA generation, HumanVBench utilizes diverse
state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and
quality assessment, minimizing human annotation dependency tailored to
human-centric multimodal attributes. A comprehensive evaluation across 16 SOTA
video MLLMs reveals notable limitations in current performance, especially in
cross-modal and temporal alignment, underscoring the necessity for further
refinement toward achieving more human-like understanding. HumanVBench is
open-sourced to facilitate future advancements and real-world applications in
video MLLMs.

摘要：在多模态大语言模型（MLLM）领域，实现以人为本的视频理解仍然是一项艰巨的挑战。现有的基准主要强调对象和动作识别，通常忽略了视频内容中人类情感、行为和言语视觉对齐的细微差别。我们提出了 HumanVBench，这是一个经过精心设计的创新基准，旨在弥合视频 MLLM 评估中的这些差距。HumanVBench 包含 17 项精心设计的任务，探索了两个主要维度：内在情感和外在表现，涵盖静态和动态、基本和复杂以及单模态和跨模态方面。借助两个用于视频注释和包含干扰项的 QA 生成的先进自动化管道，HumanVBench 利用了多种最先进 (SOTA) 技术来简化基准数据合成和质量评估，最大程度地减少了针对以人为本的多模态属性的人工注释依赖性。对 16 个 SOTA 视频 MLLM 的全面评估揭示了当前性能的显着局限性，尤其是在跨模态和时间对齐方面，强调了进一步完善以实现更接近人类的理解的必要性。HumanVBench 是开源的，旨在促进视频 MLLM 中的未来进步和实际应用。

##### **Empathetic Response in Audio-Visual Conversations Using Emotion Preference Optimization and MambaCompressor**
2412.17572v1 by Yeonju Kim, Se Jin Park, Yong Man Ro

Chatbot research is advancing with the growing importance of chatbots in
fields that require human interactions, such as customer support and mental
health care. Despite these advancements, chatbots still face significant
challenges in understanding subtle nuances and managing long conversation
histories. To address these issues, our study introduces a dual approach:
firstly, we employ Emotional Preference Optimization (EPO) to train chatbots
not only with correct responses but also with counter-emotional responses-those
that are contextually similar but emotionally divergent. This training enables
the model to discern fine nuance distinctions between correct and
counter-emotional responses, thereby enhancing the quality of its responses.
Secondly, we introduce MambaCompressor to effectively compress and manage
extensive conversation histories, significantly reducing time and memory
complexities while improving the chatbot's contextual understanding. Our
comprehensive experiments across multiple datasets demonstrate that our model
significantly outperforms existing models in generating empathetic responses
and efficiently managing lengthy dialogues.

摘要：聊天機器人的研究隨著聊天機器人在需要人類互動的領域（例如客戶支援和心理保健）中日益重要而進展。儘管有這些進展，聊天機器人在理解微妙的細微差別和管理冗長的對話記錄方面仍然面臨重大挑戰。為了解決這些問題，我們的研究引入了一個雙重方法：首先，我們採用情緒偏好最佳化 (EPO) 來訓練聊天機器人，不僅使用正確的回應，還使用反情緒回應——那些在語境上相似但在情緒上不同的回應。這種訓練使模型能夠辨別正確和反情緒回應之間的細微差別，從而提高其回應的品質。其次，我們引入了 MambaCompressor 來有效壓縮和管理廣泛的對話記錄，大幅減少時間和記憶體複雜度，同時改善聊天機器人的語境理解。我們在多個資料集上的全面實驗表明，我們的模型在產生同理心回應和有效管理冗長的對話方面明顯優於現有的模型。

##### **The Dynamic Duo of Collaborative Masking and Target for Advanced Masked Autoencoder Learning**
2412.17566v1 by Shentong Mo

Masked autoencoders (MAE) have recently succeeded in self-supervised vision
representation learning. Previous work mainly applied custom-designed (e.g.,
random, block-wise) masking or teacher (e.g., CLIP)-guided masking and targets.
However, they ignore the potential role of the self-training (student) model in
giving feedback to the teacher for masking and targets. In this work, we
present to integrate Collaborative Masking and Targets for boosting Masked
AutoEncoders, namely CMT-MAE. Specifically, CMT-MAE leverages a simple
collaborative masking mechanism through linear aggregation across attentions
from both teacher and student models. We further propose using the output
features from those two models as the collaborative target of the decoder. Our
simple and effective framework pre-trained on ImageNet-1K achieves
state-of-the-art linear probing and fine-tuning performance. In particular,
using ViT-base, we improve the fine-tuning results of the vanilla MAE from
83.6% to 85.7%.

摘要：遮罩式自动编码器 (MAE) 最近在自监督视觉表示学习中取得了成功。先前的工作主要应用自定义设计（例如，随机、区块式）遮罩或教师（例如，CLIP）指导的遮罩和目标。然而，他们忽略了自训练（学生）模型在向教师提供遮罩和目标反馈方面的潜在作用。在这项工作中，我们提出了集成协作遮罩和目标以提升遮罩自动编码器，即 CMT-MAE。具体来说，CMT-MAE 通过教师和学生模型的注意力跨线性聚合利用了一种简单的协作遮罩机制。我们进一步提出使用这两个模型的输出特征作为解码器的协作目标。我们简单而有效的框架在 ImageNet-1K 上经过预训练，实现了最先进的线性探测和微调性能。特别是，使用 ViT-base，我们将原始 MAE 的微调结果从 83.6% 提高到 85.7%。

##### **Evaluation of Bio-Inspired Models under Different Learning Settings For Energy Efficiency in Network Traffic Prediction**
2412.17565v1 by Theodoros Tsiolakis, Nikolaos Pavlidis, Vasileios Perifanis, Pavlos Efraimidis

Cellular traffic forecasting is a critical task that enables network
operators to efficiently allocate resources and address anomalies in rapidly
evolving environments. The exponential growth of data collected from base
stations poses significant challenges to processing and analysis. While machine
learning (ML) algorithms have emerged as powerful tools for handling these
large datasets and providing accurate predictions, their environmental impact,
particularly in terms of energy consumption, is often overlooked in favor of
their predictive capabilities. This study investigates the potential of two
bio-inspired models: Spiking Neural Networks (SNNs) and Reservoir Computing
through Echo State Networks (ESNs) for cellular traffic forecasting. The
evaluation focuses on both their predictive performance and energy efficiency.
These models are implemented in both centralized and federated settings to
analyze their effectiveness and energy consumption in decentralized systems.
Additionally, we compare bio-inspired models with traditional architectures,
such as Convolutional Neural Networks (CNNs) and Multi-Layer Perceptrons
(MLPs), to provide a comprehensive evaluation. Using data collected from three
diverse locations in Barcelona, Spain, we examine the trade-offs between
predictive accuracy and energy demands across these approaches. The results
indicate that bio-inspired models, such as SNNs and ESNs, can achieve
significant energy savings while maintaining predictive accuracy comparable to
traditional architectures. Furthermore, federated implementations were tested
to evaluate their energy efficiency in decentralized settings compared to
centralized systems, particularly in combination with bio-inspired models.
These findings offer valuable insights into the potential of bio-inspired
models for sustainable and privacy-preserving cellular traffic forecasting.

摘要：行動通訊流量預測是一項重要的任務，使網路營運商能夠在快速變化的環境中有效分配資源並解決異常問題。從基地台收集的資料呈指數成長，對處理和分析構成重大的挑戰。雖然機器學習 (ML) 演算法已成為處理這些大型資料集和提供準確預測的強大工具，但其環境影響，特別是在能源消耗方面，常常被忽視，而偏好其預測能力。本研究探討了兩種生物啟發模型的潛力：尖峰神經網路 (SNN) 和透過迴音狀態網路 (ESN) 的儲存器運算，用於行動通訊流量預測。評估重點在於其預測效能和能源效率。這些模型在集中式和聯邦式設定中實作，以分析其在分散式系統中的有效性和能源消耗。此外，我們將生物啟發模型與傳統架構進行比較，例如卷積神經網路 (CNN) 和多層感知器 (MLP)，以提供全面的評估。使用從西班牙巴塞隆納三個不同地點收集的資料，我們檢視了這些方法在預測準確度和能源需求之間的權衡。結果表明，生物啟發模型，例如 SNN 和 ESN，可以在維持與傳統架構相當的預測準確度的同時，實現顯著的節能。此外，我們測試了聯邦式實作，以評估其在分散式設定中的能源效率，與集中式系統相比，特別是與生物啟發模型結合時。這些發現為生物啟發模型在永續且隱私保護的行動通訊流量預測中的潛力提供了有價值的見解。

##### **ERUPD -- English to Roman Urdu Parallel Dataset**
2412.17562v1 by Mohammed Furqan, Raahid Bin Khaja, Rayyan Habeeb

Bridging linguistic gaps fosters global growth and cultural exchange. This
study addresses the challenges of Roman Urdu -- a Latin-script adaptation of
Urdu widely used in digital communication -- by creating a novel parallel
dataset comprising 75,146 sentence pairs. Roman Urdu's lack of standardization,
phonetic variability, and code-switching with English complicates language
processing. We tackled this by employing a hybrid approach that combines
synthetic data generated via advanced prompt engineering with real-world
conversational data from personal messaging groups. We further refined the
dataset through a human evaluation phase, addressing linguistic inconsistencies
and ensuring accuracy in code-switching, phonetic representations, and synonym
variability. The resulting dataset captures Roman Urdu's diverse linguistic
features and serves as a critical resource for machine translation, sentiment
analysis, and multilingual education.

摘要：<paragraph>縮小語言差距有助於全球成長和文化交流。這項研究探討了羅馬烏爾都語的挑戰，羅馬烏爾都語是烏爾都語的拉丁文字改編，廣泛用於數位通訊，方法是建立一個包含 75,146 個句子對的新穎平行資料集。羅馬烏爾都語缺乏標準化、語音變異性，以及與英語的代碼切換，使語言處理變得複雜。我們採用一種混合方法來解決這個問題，該方法結合了透過進階提示工程產生的合成資料，以及來自個人訊息群組的真實對話資料。我們進一步透過人工評估階段來改善資料集，解決語言不一致的問題，並確保代碼切換、語音表示和同義詞變異的準確性。產生的資料集擷取了羅馬烏爾都語多樣化的語言特徵，並作為機器翻譯、情緒分析和多語言教育的重要資源。</paragraph>

##### **A Survey of Query Optimization in Large Language Models**
2412.17558v1 by Mingyang Song, Mao Zheng

\textit{Query Optimization} (QO) refers to techniques aimed at enhancing the
efficiency and quality of Large Language Models (LLMs) in understanding and
answering queries, especially complex ones in scenarios like
Retrieval-Augmented Generation (RAG). Specifically, RAG mitigates the
limitations of LLMs by dynamically retrieving and leveraging up-to-date
relevant information, which provides a cost-effective solution to the challenge
of LLMs producing plausible but potentially inaccurate responses. Recently, as
RAG evolves and incorporates multiple components that influence its
performance, QO has emerged as a critical element, playing a pivotal role in
determining the effectiveness of RAG's retrieval stage in accurately sourcing
the necessary multiple pieces of evidence to answer queries correctly. In this
paper, we trace the evolution of QO techniques by summarizing and analyzing
significant studies. Through an organized framework and categorization, we aim
to consolidate existing QO techniques in RAG, elucidate their technological
foundations, and highlight their potential to enhance the versatility and
applications of LLMs.

摘要：**查詢最佳化** (QO) 係指旨在提升大型語言模型 (LLM) 理解和回答查詢的效率和品質的技術，特別是像檢索增強生成 (RAG) 等情境中的複雜查詢。具體來說，RAG 透過動態檢索和利用最新的相關資訊來緩解 LLM 的限制，這為 LLM 產生看似合理但潛在不準確的回應的挑戰提供了具成本效益的解決方案。最近，隨著 RAG 的演進並整合影響其效能的多個元件，QO 已成為一個重要的元素，在決定 RAG 檢索階段準確取得回答查詢所需要的多個證據片段的有效性方面扮演關鍵角色。在本文中，我們透過摘要和分析重要的研究來追溯 QO 技術的演進。透過有組織的架構和分類，我們旨在整合 RAG 中現有的 QO 技術，闡明其技術基礎，並強調其提升 LLM 的多功能性和應用程式的潛力。

##### **Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and Multi-Domain Testing**
2412.17548v1 by Prakash Aryan

This paper presents a novel approach to fine-tuning the Qwen2-1.5B model for
Arabic language processing using Quantized Low-Rank Adaptation (QLoRA) on a
system with only 4GB VRAM. We detail the process of adapting this large
language model to the Arabic domain, using diverse datasets including Bactrian,
OpenAssistant, and Wikipedia Arabic corpora. Our methodology involves custom
data preprocessing, model configuration, and training optimization techniques
such as gradient accumulation and mixed-precision training. We address specific
challenges in Arabic NLP, including morphological complexity, dialectal
variations, and diacritical mark handling. Experimental results over 10,000
training steps show significant performance improvements, with the final loss
converging to 0.1083. We provide comprehensive analysis of GPU memory usage,
training dynamics, and model evaluation across various Arabic language tasks,
including text classification, question answering, and dialect identification.
The fine-tuned model demonstrates robustness to input perturbations and
improved handling of Arabic-specific linguistic phenomena. This research
contributes to multilingual AI by demonstrating a resource-efficient approach
for creating specialized language models, potentially democratizing access to
advanced NLP technologies for diverse linguistic communities. Our work paves
the way for future research in low-resource language adaptation and efficient
fine-tuning of large language models.

摘要：本文提出了一種新穎的方法，使用量化低秩適應 (QLoRA) 在僅有 4GB VRAM 的系統上，微調 Qwen2-1.5B 模型以進行阿拉伯語處理。我們詳細說明了將這個大型語言模型適應到阿拉伯語領域的過程，使用了包括 Bactrian、OpenAssistant 和 Wikipedia 阿拉伯語語料庫在內的各種數據集。我們的做法包括自定義資料預處理、模型組態和訓練最佳化技術，例如梯度累積和混合精度訓練。我們解決了阿拉伯語 NLP 中的特定挑戰，包括形態複雜性、方言變異和音標符號處理。超過 10,000 個訓練步驟的實驗結果顯示出顯著的效能提升，最終損失收斂至 0.1083。我們提供了對 GPU 記憶體使用、訓練動態和模型評估的全面分析，涵蓋各種阿拉伯語語言任務，包括文字分類、問題解答和方言辨識。微調後的模型展示了對輸入擾動的穩健性，並改進了對阿拉伯語特定語言現象的處理。這項研究透過展示一種資源有效的方法來建立專門的語言模型，對多語言 AI 有所貢獻，有可能讓不同語言社群都能使用進階的 NLP 技術。我們的研究為低資源語言適應和大型語言模型的有效微調鋪平了道路。

##### **Retention Score: Quantifying Jailbreak Risks for Vision Language Models**
2412.17544v1 by Zaitang Li, Pin-Yu Chen, Tsung-Yi Ho

The emergence of Vision-Language Models (VLMs) is a significant advancement
in integrating computer vision with Large Language Models (LLMs) to enhance
multi-modal machine learning capabilities. However, this progress has also made
VLMs vulnerable to sophisticated adversarial attacks, raising concerns about
their reliability. The objective of this paper is to assess the resilience of
VLMs against jailbreak attacks that can compromise model safety compliance and
result in harmful outputs. To evaluate a VLM's ability to maintain its
robustness against adversarial input perturbations, we propose a novel metric
called the \textbf{Retention Score}. Retention Score is a multi-modal
evaluation metric that includes Retention-I and Retention-T scores for
quantifying jailbreak risks in visual and textual components of VLMs. Our
process involves generating synthetic image-text pairs using a conditional
diffusion model. These pairs are then predicted for toxicity score by a VLM
alongside a toxicity judgment classifier. By calculating the margin in toxicity
scores, we can quantify the robustness of the VLM in an attack-agnostic manner.
Our work has four main contributions. First, we prove that Retention Score can
serve as a certified robustness metric. Second, we demonstrate that most VLMs
with visual components are less robust against jailbreak attacks than the
corresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find
that the security settings in Google Gemini significantly affect the score and
robustness. Moreover, the robustness of GPT4V is similar to the medium settings
of Gemini. Finally, our approach offers a time-efficient alternative to
existing adversarial attack methods and provides consistent model robustness
rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA.

摘要：視覺語言模型 (VLM) 的出現是將電腦視覺與大型語言模型 (LLM) 整合的重大進展，以增強多模態機器學習功能。然而，此進展也讓 VLM 容易受到精密對抗攻擊，引起人們對其可靠性的擔憂。本文的目標是評估 VLM 對越獄攻擊的韌性，越獄攻擊可能會損害模型安全合規性，並導致有害的輸出。為了評估 VLM 在對抗輸入擾動下維持其穩健性的能力，我們提出了一種名為「保留分數」的新指標。保留分數是一種多模態評估指標，包含保留分數 I 和保留分數 T，用於量化 VLM 視覺和文本組件中的越獄風險。我們的流程涉及使用條件擴散模型生成合成圖像文字對。接著，VLM 會預測這些對的毒性分數，同時使用毒性判斷分類器。透過計算毒性分數中的邊際值，我們可以量化 VLM 在不可知攻擊的方式中的穩健性。我們的研究有四項主要貢獻。首先，我們證明保留分數可用作認證穩健性指標。其次，我們證明大多數具有視覺組件的 VLM 比對應的純粹 VLM 對越獄攻擊的穩健性較低。此外，我們評估黑盒 VLM API，發現 Google Gemini 中的安全設定會顯著影響分數和穩健性。此外，GPT4V 的穩健性類似於 Gemini 的中階設定。最後，我們的方法提供了一個比現有對抗攻擊方法更省時的方法，並在評估包括 MiniGPT-4、InstructBLIP 和 LLaVA 在內的 VLM 時提供一致的模型穩健性排名。

##### **Concept Discovery in Deep Neural Networks for Explainable Face Anti-Spoofing**
2412.17541v1 by Haoyuan Zhang, Xiangyu Zhu, Li Gao, Jiawei Pan, Kai Pang, Guoying Zhao, Stan Z. Li, Zhen Lei

With the rapid growth usage of face recognition in people's daily life, face
anti-spoofing becomes increasingly important to avoid malicious attacks. Recent
face anti-spoofing models can reach a high classification accuracy on multiple
datasets but these models can only tell people ``this face is fake'' while
lacking the explanation to answer ``why it is fake''. Such a system undermines
trustworthiness and causes user confusion, as it denies their requests without
providing any explanations. In this paper, we incorporate XAI into face
anti-spoofing and propose a new problem termed X-FAS (eXplainable Face
Anti-Spoofing) empowering face anti-spoofing models to provide an explanation.
We propose SPED (SPoofing Evidence Discovery), an X-FAS method which can
discover spoof concepts and provide reliable explanations on the basis of
discovered concepts. To evaluate the quality of X-FAS methods, we propose an
X-FAS benchmark with annotated spoofing evidence by experts. We analyze SPED
explanations on face anti-spoofing dataset and compare SPED quantitatively and
qualitatively with previous XAI methods on proposed X-FAS benchmark.
Experimental results demonstrate SPED's ability to generate reliable
explanations.

摘要：隨著人臉識別在人們日常生活中使用率的快速增長，人臉防欺騙變得越來越重要，以避免惡意攻擊。最近的人臉防欺騙模型可以在多個數據集上達到很高的分類準確度，但這些模型只能告訴人們「這張臉是假的」，而無法解釋「為什麼它是假的」。這種系統會破壞可信度並造成用戶混淆，因為它會在不提供任何解釋的情況下拒絕他們的請求。在本文中，我們將 XAI 納入人臉防欺騙，並提出一個稱為 X-FAS（可解釋人臉防欺騙）的新問題，賦予人臉防欺騙模型提供解釋的能力。我們提出 SPED（欺騙證據發現），這是一種 X-FAS 方法，它可以發現欺騙概念，並根據發現的概念提供可靠的解釋。為了評估 X-FAS 方法的品質，我們提出了一個由專家註解欺騙證據的 X-FAS 基準。我們分析了人臉防欺騙數據集上的 SPED 解釋，並在提出的 X-FAS 基準上對 SPED 與先前的 XAI 方法進行定量和定性比較。實驗結果證明了 SPED 產生可靠解釋的能力。

##### **Domain adapted machine translation: What does catastrophic forgetting forget and why?**
2412.17537v1 by Danielle Saunders, Steve DeNeefe

Neural Machine Translation (NMT) models can be specialized by domain
adaptation, often involving fine-tuning on a dataset of interest. This process
risks catastrophic forgetting: rapid loss of generic translation quality.
Forgetting has been widely observed, with many mitigation methods proposed.
However, the causes of forgetting and the relationship between forgetting and
adaptation data are under-explored.
  This paper takes a novel approach to understanding catastrophic forgetting
during NMT adaptation by investigating the impact of the data. We provide a
first investigation of what is forgotten, and why. We examine the relationship
between forgetting and the in-domain data, and show that the amount and type of
forgetting is linked to that data's target vocabulary coverage. Our findings
pave the way toward better informed NMT domain adaptation.

摘要：神經機器翻譯 (NMT) 模型可以透過領域適應進行專業化，這通常涉及對感興趣的資料集進行微調。這個過程有災難性遺忘的風險：一般翻譯品質快速下降。遺忘已被廣泛觀察到，並提出了許多減緩方法。然而，遺忘的原因以及遺忘與適應資料之間的關係尚未得到充分探討。本文採取一種新穎的方法，透過探討資料的影響，來了解 NMT 適應期間的災難性遺忘。我們提供了對遺忘內容和原因的首次調查。我們探討了遺忘與領域內資料之間的關係，並證明遺忘的數量和類型與該資料的目標詞彙覆蓋率有關。我們的發現為更完善的 NMT 領域適應鋪平了道路。

##### **Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse**
2412.17533v1 by Anna Kołos, Katarzyna Lorenc, Emilia Wiśnios, Agnieszka Karlińska

The surge in online content has created an urgent demand for robust detection
systems, especially in non-English contexts where current tools demonstrate
significant limitations. We present forePLay, a novel Polish language dataset
for erotic content detection, featuring over 24k annotated sentences with a
multidimensional taxonomy encompassing ambiguity, violence, and social
unacceptability dimensions. Our comprehensive evaluation demonstrates that
specialized Polish language models achieve superior performance compared to
multilingual alternatives, with transformer-based architectures showing
particular strength in handling imbalanced categories. The dataset and
accompanying analysis establish essential frameworks for developing
linguistically-aware content moderation systems, while highlighting critical
considerations for extending such capabilities to morphologically complex
languages.

摘要：線上內容激增，導致對強健的偵測系統產生迫切需求，特別是在非英語語境中，現有工具顯示出顯著的限制。我們提出 forePLay，一個新穎的波蘭語色情內容偵測資料集，包含超過 24k 個帶註解的句子，採用涵蓋歧義、暴力和社會不可接受性面向的多維分類法。我們的全面評估證明，與多語言替代方案相比，專門的波蘭語語言模型可達成優異的效能，而基於轉換器的架構在處理不平衡類別方面展現出特別的優勢。該資料集和附帶分析建立了開發語言感知內容審核系統的基本架構，同時強調了將此類能力擴展到形態複雜語言的重要考量。

##### **Double Landmines: Invisible Textual Backdoor Attacks based on Dual-Trigger**
2412.17531v1 by Yang Hou, Qiuling Yue, Lujia Chai, Guozhao Liao, Wenbao Han, Wei Ou

At present, all textual backdoor attack methods are based on single triggers:
for example, inserting specific content into the text to activate the backdoor;
or changing the abstract text features. The former is easier to be identified
by existing defense strategies due to its obvious characteristics; the latter,
although improved in invisibility, has certain shortcomings in terms of attack
performance, construction of poisoned datasets, and selection of the final
poisoning rate. On this basis, this paper innovatively proposes a Dual-Trigger
backdoor attack based on syntax and mood, and optimizes the construction of the
poisoned dataset and the selection strategy of the final poisoning rate. A
large number of experimental results show that this method significantly
outperforms the previous methods based on abstract features in attack
performance, and achieves comparable attack performance (almost 100% attack
success rate) with the insertion-based method. In addition, the two trigger
mechanisms included in this method can be activated independently in the
application phase of the model, which not only improves the flexibility of the
trigger style, but also enhances its robustness against defense strategies.
These results profoundly reveal that textual backdoor attacks are extremely
harmful and provide a new perspective for security protection in this field.

摘要：目前，所有文本后门攻击方法都基于单一触发器：
例如，将特定内容插入文本以激活后门；
或更改抽象文本特征。前者由于其明显的特征，更容易被现有的防御策略识别；后者，虽然在不可见性方面有所改进，但在攻击性能、中毒数据集的构建和最终中毒率的选择方面存在一定的缺点。在此基础上，本文创新性地提出了一种基于语法和情绪的双触发后门攻击，并优化了中毒数据集的构建和最终中毒率的选择策略。大量的实验结果表明，该方法在攻击性能上明显优于以前基于抽象特征的方法，并且与基于插入的方法实现了相当的攻击性能（几乎 100% 的攻击成功率）。此外，该方法中包含的两个触发机制可以在模型的应用阶段独立激活，这不仅提高了触发方式的灵活性，而且增强了其对防御策略的鲁棒性。这些结果深刻地揭示了文本后门攻击的极端危害性，并为该领域的安全性保护提供了新的视角。

##### **Enhancing Cancer Diagnosis with Explainable & Trustworthy Deep Learning Models**
2412.17527v1 by Badaru I. Olumuyiwa, The Anh Han, Zia U. Shamszaman

This research presents an innovative approach to cancer diagnosis and
prediction using explainable Artificial Intelligence (XAI) and deep learning
techniques. With cancer causing nearly 10 million deaths globally in 2020,
early and accurate diagnosis is crucial. Traditional methods often face
challenges in cost, accuracy, and efficiency. Our study develops an AI model
that provides precise outcomes and clear insights into its decision-making
process, addressing the "black box" problem of deep learning models. By
employing XAI techniques, we enhance interpretability and transparency,
building trust among healthcare professionals and patients. Our approach
leverages neural networks to analyse extensive datasets, identifying patterns
for cancer detection. This model has the potential to revolutionise diagnosis
by improving accuracy, accessibility, and clarity in medical decision-making,
possibly leading to earlier detection and more personalised treatment
strategies. Furthermore, it could democratise access to high-quality
diagnostics, particularly in resource-limited settings, contributing to global
health equity. The model's applications extend beyond cancer diagnosis,
potentially transforming various aspects of medical decision-making and saving
millions of lives worldwide.

摘要：本研究提出了一個創新的癌症診斷和預測方法，使用可解釋的人工智慧 (XAI) 和深度學習技術。由於癌症在 2020 年造成全球近 1,000 萬人死亡，因此早期準確的診斷至關重要。傳統方法通常面臨成本、準確性和效率方面的挑戰。我們的研究開發了一個 AI 模型，它提供精確的結果並清楚地了解其決策過程，解決了深度學習模型的「黑箱」問題。通過採用 XAI 技術，我們增強了解釋性和透明度，在醫療專業人員和患者之間建立信任。我們的做法利用神經網路分析廣泛的數據集，識別癌症檢測模式。這個模型有可能通過提高醫療決策的準確性、可及性和清晰度來革新診斷，可能導致更早的檢測和更個性化的治療策略。此外，它可以使更多人獲得高品質的診斷，特別是在資源有限的環境中，有助於全球健康公平。該模型的應用範圍不僅限於癌症診斷，還可能轉變醫療決策的各個方面，並拯救全球數百萬人的生命。

##### **STAHGNet: Modeling Hybrid-grained Heterogenous Dependency Efficiently for Traffic Prediction**
2412.17524v1 by Jiyao Wang, Zehua Peng, Yijia Zhang, Dengbo He, Lei Chen

Traffic flow prediction plays a critical role in the intelligent
transportation system, and it is also a challenging task because of the
underlying complex Spatio-temporal patterns and heterogeneities evolving across
time. However, most present works mostly concentrate on solely capturing
Spatial-temporal dependency or extracting implicit similarity graphs, but the
hybrid-granularity evolution is ignored in their modeling process. In this
paper, we proposed a novel data-driven end-to-end framework, named
Spatio-Temporal Aware Hybrid Graph Network (STAHGNet), to couple the
hybrid-grained heterogeneous correlations in series simultaneously through an
elaborately Hybrid Graph Attention Module (HGAT) and Coarse-granularity
Temporal Graph (CTG) generator. Furthermore, an automotive feature engineering
with domain knowledge and a random neighbor sampling strategy is utilized to
improve efficiency and reduce computational complexity. The MAE, RMSE, and MAPE
are used for evaluation metrics. Tested on four real-life datasets, our
proposal outperforms eight classical baselines and four state-of-the-art (SOTA)
methods (e.g., MAE 14.82 on PeMSD3; MAE 18.92 on PeMSD4). Besides, extensive
experiments and visualizations verify the effectiveness of each component in
STAHGNet. In terms of computational cost, STAHGNet saves at least four times
the space compared to the previous SOTA models. The proposed model will be
beneficial for more efficient TFP as well as intelligent transport system
construction.

摘要：交通流量預測在智慧運輸系統中扮演著至關重要的角色，同時也是一項艱鉅的任務，原因在於其複雜的時空模式和異質性隨著時間演變。然而，現有的研究大多僅專注於捕捉時空依賴性或萃取隱含的相似性圖，而忽略了混合粒度演化在建模過程中的影響。在本文中，我們提出一個新穎的資料驅動端到端架構，稱為時空感知混合圖網路 (STAHGNet)，透過精心設計的混合圖注意力模組 (HGAT) 和粗粒度時序圖 (CTG) 生成器，同時串聯混合粒度異質相關性。此外，利用具備領域知識的汽車特徵工程和隨機鄰域取樣策略來提升效率並降低運算複雜度。MAE、RMSE 和 MAPE 作為評估指標。在四個真實世界資料集上的測試結果顯示，我們的提案優於八個傳統基準和四個最先進 (SOTA) 方法（例如，PeMSD3 上的 MAE 為 14.82；PeMSD4 上的 MAE 為 18.92）。此外，廣泛的實驗和可視化驗證了 STAHGNet 中每個組件的有效性。在運算成本方面，與先前的 SOTA 模型相比，STAHGNet 至少節省了四倍的空間。所提出的模型將有利於更有效率的 TFP 以及智慧運輸系統的建構。

##### **Constructing Fair Latent Space for Intersection of Fairness and Explainability**
2412.17523v1 by Hyungjun Joo, Hyeonggeun Han, Sehwan Kim, Sangwoo Hong, Jungwoo Lee

As the use of machine learning models has increased, numerous studies have
aimed to enhance fairness. However, research on the intersection of fairness
and explainability remains insufficient, leading to potential issues in gaining
the trust of actual users. Here, we propose a novel module that constructs a
fair latent space, enabling faithful explanation while ensuring fairness. The
fair latent space is constructed by disentangling and redistributing labels and
sensitive attributes, allowing the generation of counterfactual explanations
for each type of information. Our module is attached to a pretrained generative
model, transforming its biased latent space into a fair latent space.
Additionally, since only the module needs to be trained, there are advantages
in terms of time and cost savings, without the need to train the entire
generative model. We validate the fair latent space with various fairness
metrics and demonstrate that our approach can effectively provide explanations
for biased decisions and assurances of fairness.

摘要：隨著機器學習模型的使用增加，許多研究旨在提升公平性。然而，公平性與可解釋性之間的交叉研究仍不足，導致在獲得實際用戶信任方面出現潛在問題。在此，我們提出一個創新的模組，用於建構一個公平的潛在空間，在確保公平性的同時，提供忠實的解釋。公平的潛在空間是透過解開和重新分配標籤和敏感屬性來建構的，允許為每種類型的資訊產生反事實的解釋。我們的模組附加到一個預先訓練的生成模型，將其有偏差的潛在空間轉換成一個公平的潛在空間。此外，由於只需要訓練模組，因此在時間和成本節省方面具有優勢，無需訓練整個生成模型。我們使用各種公平性指標驗證公平的潛在空間，並證明我們的做法可以有效地為有偏差的決策提供解釋，並保證公平性。

##### **DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak**
2412.17522v1 by Hao Wang, Hao Li, Junda Zhu, Xinyuan Wang, Chengwei Pan, MinLie Huang, Lei Sha

Large Language Models (LLMs) are susceptible to generating harmful content
when prompted with carefully crafted inputs, a vulnerability known as LLM
jailbreaking. As LLMs become more powerful, studying jailbreak methods is
critical to enhancing security and aligning models with human values.
Traditionally, jailbreak techniques have relied on suffix addition or prompt
templates, but these methods suffer from limited attack diversity. This paper
introduces DiffusionAttacker, an end-to-end generative approach for jailbreak
rewriting inspired by diffusion models. Our method employs a
sequence-to-sequence (seq2seq) text diffusion model as a generator,
conditioning on the original prompt and guiding the denoising process with a
novel attack loss. Unlike previous approaches that use autoregressive LLMs to
generate jailbreak prompts, which limit the modification of already generated
tokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq
diffusion model, allowing more flexible token modifications. This approach
preserves the semantic content of the original prompt while producing harmful
content. Additionally, we leverage the Gumbel-Softmax technique to make the
sampling process from the diffusion model's output distribution differentiable,
eliminating the need for iterative token search. Extensive experiments on
Advbench and Harmbench demonstrate that DiffusionAttacker outperforms previous
methods across various evaluation metrics, including attack success rate (ASR),
fluency, and diversity.

摘要：大型語言模型 (LLM) 在收到精心設計的輸入時容易產生有害內容，此漏洞稱為 LLM 越獄。隨著 LLM 變得更強大，研究越獄方法對於增強安全性並使模型與人類價值觀保持一致至關重要。傳統上，越獄技術依賴於後綴添加或提示範本，但這些方法的攻擊多樣性有限。本文介紹了 DiffusionAttacker，一種靈感來自擴散模型的端到端生成式越獄重寫方法。我們的技術採用序列到序列 (seq2seq) 文本擴散模型作為生成器，以原始提示為條件，並使用新穎的攻擊損失來引導去噪過程。與使用自迴歸 LLM 來產生越獄提示的不同，這會限制已經產生的標記的修改並限制重寫空間，DiffusionAttacker 使用 seq2seq 擴散模型，允許更靈活的標記修改。這種方法保留了原始提示的語義內容，同時產生有害內容。此外，我們利用 Gumbel-Softmax 技術使從擴散模型輸出分佈中採樣的過程可微分，從而消除了對反覆標記搜索的需求。在 Advbench 和 Harmbench 上進行的廣泛實驗表明，DiffusionAttacker 在各種評估指標（包括攻擊成功率 (ASR)、流暢度和多樣性）上優於以前的方法。

##### **BEE: Metric-Adapted Explanations via Baseline Exploration-Exploitation**
2412.17512v1 by Oren Barkan, Yehonatan Elisha, Jonathan Weill, Noam Koenigstein

Two prominent challenges in explainability research involve 1) the nuanced
evaluation of explanations and 2) the modeling of missing information through
baseline representations. The existing literature introduces diverse evaluation
metrics, each scrutinizing the quality of explanations through distinct lenses.
Additionally, various baseline representations have been proposed, each
modeling the notion of missingness differently. Yet, a consensus on the
ultimate evaluation metric and baseline representation remains elusive. This
work acknowledges the diversity in explanation metrics and baselines,
demonstrating that different metrics exhibit preferences for distinct
explanation maps resulting from the utilization of different baseline
representations and distributions. To address the diversity in metrics and
accommodate the variety of baseline representations in a unified manner, we
propose Baseline Exploration-Exploitation (BEE) - a path-integration method
that introduces randomness to the integration process by modeling the baseline
as a learned random tensor. This tensor follows a learned mixture of baseline
distributions optimized through a contextual exploration-exploitation procedure
to enhance performance on the specific metric of interest. By resampling the
baseline from the learned distribution, BEE generates a comprehensive set of
explanation maps, facilitating the selection of the best-performing explanation
map in this broad set for the given metric. Extensive evaluations across
various model architectures showcase the superior performance of BEE in
comparison to state-of-the-art explanation methods on a variety of objective
evaluation metrics.

摘要：可解釋性研究中的兩個突出挑戰涉及 1) 對解釋的細緻評估和 2) 透過基準表示對遺失資訊的建模。現有文獻引入了多樣化的評估指標，每個指標都透過不同的視角檢視解釋的品質。此外，已經提出了各種基準表示，每個基準表示都以不同的方式對缺失的概念進行建模。然而，對於最終的評估指標和基準表示尚未達成共識。這項工作承認解釋指標和基準的多樣性，證明不同的指標展現出對不同基準表示和分佈所產生的不同解釋圖的偏好。為了解決指標的多樣性，並以統一的方式容納各種基準表示，我們提出了基準探索-利用 (BEE) - 一種路徑整合方法，透過將基準建模為學習到的隨機張量，在整合過程中引入隨機性。此張量遵循透過情境探索-利用程序最佳化的基準分佈的學習混合，以增強特定感興趣指標的效能。透過從學習到的分佈中重新抽樣基準，BEE 產生一組全面的解釋圖，促進在這個廣泛的集合中選擇給定指標的最佳效能解釋圖。跨越各種模型架構的廣泛評估顯示，與各種客觀評估指標上的最新解釋方法相比，BEE 具有優越的效能。

##### **An Evaluation Framework for Product Images Background Inpainting based on Human Feedback and Product Consistency**
2412.17504v1 by Yuqi Liang, Jun Luo, Xiaoxi Guo, Jianqi Bi

In product advertising applications, the automated inpainting of backgrounds
utilizing AI techniques in product images has emerged as a significant task.
However, the techniques still suffer from issues such as inappropriate
background and inconsistent product in generated product images, and existing
approaches for evaluating the quality of generated product images are mostly
inconsistent with human feedback causing the evaluation for this task to depend
on manual annotation. To relieve the issues above, this paper proposes Human
Feedback and Product Consistency (HFPC), which can automatically assess the
generated product images based on two modules. Firstly, to solve inappropriate
backgrounds, human feedback on 44,000 automated inpainting product images is
collected to train a reward model based on multi-modal features extracted from
BLIP and comparative learning. Secondly, to filter generated product images
containing inconsistent products, a fine-tuned segmentation model is employed
to segment the product of the original and generated product images and then
compare the differences between the above two. Extensive experiments have
demonstrated that HFPC can effectively evaluate the quality of generated
product images and significantly reduce the expense of manual annotation.
Moreover, HFPC achieves state-of-the-art(96.4% in precision) in comparison to
other open-source visual-quality-assessment models. Dataset and code are
available at: https://github.com/created-Bi/background inpainting products
dataset/.

摘要：<paragraph>在商品廣告應用中，利用 AI 技術自動修復商品圖片背景已成為一項重要任務。然而，這些技術仍存在著生成商品圖片中背景不適當和商品不一致等問題，而現有的評估生成商品圖片品質的方法大多與人為回饋不一致，導致此任務的評估依賴於人工標註。為了解決上述問題，本文提出人為回饋與商品一致性 (HFPC)，它可以根據兩個模組自動評估生成的商品圖片。首先，為了解決背景不適當的問題，收集了 44,000 張自動修復商品圖片的人為回饋，以訓練一個獎勵模型，該模型基於從 BLIP 和比較學習中提取的多模式特徵。其次，為了過濾包含不一致商品的生成商品圖片，採用微調分割模型分割原始商品圖片和生成商品圖片的商品，然後比較兩者的差異。大量的實驗證明，HFPC 可以有效評估生成商品圖片的品質，並顯著降低人工標註的成本。此外，與其他開源視覺品質評估模型相比，HFPC 達到了最先進的水平（準確率為 96.4%）。資料集和程式碼可在以下網址取得：https://github.com/created-Bi/background inpainting products dataset/。</paragraph>

##### **DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought**
2412.17498v1 by Jiaan Wang, Fandong Meng, Yunlong Liang, Jie Zhou

Recently, O1-like models have emerged as representative examples,
illustrating the effectiveness of long chain-of-thought (CoT) in reasoning
tasks such as math and coding tasks. In this paper, we introduce DRT-o1, an
attempt to bring the success of long CoT to neural machine translation (MT).
Specifically, in view of the literature books that might involve similes and
metaphors, translating these texts to a target language is very difficult in
practice due to cultural differences. In such cases, literal translation often
fails to convey the intended meaning effectively. Even for professional human
translators, considerable thought must be given to preserving semantics
throughout the translation process. To simulate LLMs' long thought ability in
MT, we first mine sentences containing similes or metaphors from existing
literature books, and then develop a multi-agent framework to translate these
sentences via long thought. In the multi-agent framework, a translator is used
to iteratively translate the source sentence under the suggestions provided by
an advisor. To ensure the effectiveness of the long thoughts, an evaluator is
also employed to judge whether the translation in the current round is better
than the previous one or not. In this manner, we collect tens of thousands of
long-thought MT data, which is used to train our DRT-o1. The experimental
results on literature translation demonstrate the effectiveness of the DRT-o1.
Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by
DRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can
outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its
effectiveness. The project is available at https://github.com/krystalan/DRT-o1

摘要：<paragraph>最近，O1 類模型已成為代表性範例，說明長思維鏈 (CoT) 在推理任務（例如數學和編碼任務）中的有效性。在本文中，我們介紹 DRT-o1，嘗試將長 CoT 的成功帶入神經機器翻譯 (MT)。具體來說，有鑑於可能涉及明喻和隱喻的文學書籍，由於文化差異，將這些文本翻譯成目標語言在實務上非常困難。在這種情況下，逐字翻譯通常無法有效傳達預期的意義。即使對於專業的人類翻譯人員，在整個翻譯過程中也必須仔細考慮如何保留語義。為了模擬 LLM 在 MT 中的長思維能力，我們首先從現有的文學書籍中挖掘包含明喻或隱喻的句子，然後開發一個多代理架構，透過長思維翻譯這些句子。在多代理架構中，翻譯人員用於在顧問提供的建議下反覆翻譯原始句子。為了確保長思維的有效性，也使用評估器來判斷當前回合的翻譯是否優於前一個回合。透過這種方式，我們收集了數萬個長思維 MT 資料，用於訓練我們的 DRT-o1。文學翻譯的實驗結果證明了 DRT-o1 的有效性。使用 Qwen2.5-7B 和 Qwen2.5-14B 作為主幹，DRT-o1 帶來的改進達到了 7.33~8.26 BLEU 和 1.66~3.36 CometScore。此外，DRT-o1-7B 可以比 QwQ-32B-Preview 高出 7.82 BLEU 和 1.46 CometScore，顯示其有效性。專案可在 https://github.com/krystalan/DRT-o1 取得</paragraph>

##### **A Toolkit for Virtual Reality Data Collection**
2412.17490v1 by Tim Rolff, Niklas Hypki, Markus Lappe, Frank Steinicke

Due to the still relatively low number of users, acquiring large-scale and
multidimensional virtual reality datasets remains a significant challenge.
Consequently, VR datasets comparable in size to state-of-the-art collections in
natural language processing or computer vision are rare or absent. However, the
availability of such datasets could unlock groundbreaking advancements in
deep-learning, psychological modeling, and data analysis in the context of VR.
In this paper, we present a versatile data collection toolkit designed to
facilitate the capturing of extensive VR datasets. Our toolkit seamlessly
integrates with any device, either directly via OpenXR or through the use of a
virtual device. Additionally, we introduce a robust data collection pipeline
that emphasizes ethical practices (e.g., ensuring data protection and
regulation) and ensures a standardized, reproducible methodology.

摘要：由於使用者數量仍然相對較少，因此取得大規模且多面向的虛擬實境資料集仍然是一項重大的挑戰。
因此，與自然語言處理或電腦視覺領域中最先進的資料集規模相當的 VR 資料集十分罕見，甚至不存在。然而，此類資料集的取得可以解鎖 VR 領域中深度學習、心理建模和資料分析的突破性進展。
在本文中，我們提出了一個多功能資料收集工具包，旨在促進廣泛 VR 資料集的擷取。我們的工具包可以透過 OpenXR 或使用虛擬裝置，與任何裝置無縫整合。此外，我們還導入了一個穩健的資料收集管線，強調道德實務（例如確保資料保護和法規遵循），並確保標準化且可複製的方法。

##### **Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of Large Language Models such as ChatGPT in Educational Settings**
2412.17486v1 by Jérémie Sublime, Ilaria Renna

The rapid adoption of Generative AI (GenAI) based on Large Language Models
(LLMs) such as ChatGPT has recently and profoundly impacted education, offering
transformative opportunities while raising significant concerns. In this study
we present the results of a survey that investigates how 395 students aged 13
to 25 years old in France and Italy integrate LLMs into their educational
routines.
  Key findings include the widespread use of these tools across all age groups
and disciplines, with older students and male students demonstrating higher
usage frequencies, particularly in scientific contexts. The results also show
gender disparities, raising concerns about an emerging AI literacy and
technological gender gap. Additionally, while most students utilise LLMs
constructively, the lack of systematic proofreading and critical evaluation
among younger users suggests potential risks to cognitive skills development,
including critical thinking and foundational knowledge. The survey results
underscore the need for educational institutions to adapt their curricula to
integrate AI tools effectively, promoting ethical use, critical thinking, and
awareness of AI limitations and environmental costs. This paper provides
actionable recommendations for fostering equitable and effective cohabitation
of LLMs and education while addressing emerging challenges.

摘要：生成式 AI (GenAI) 的快速採納，其基礎是大語言模型 (LLM)，例如 ChatGPT，最近對教育產生了深遠的影響，提供了轉型機會，同時也引發了重大的疑慮。在這項研究中，我們呈現了一項調查結果，調查探討了 395 名 13 至 25 歲的法國和義大利學生如何將 LLM 整合到他們的教育常規中。
主要發現包括所有年齡層和學科都廣泛使用這些工具，年紀較大的學生和男性學生表現出更高的使用頻率，特別是在科學背景下。結果也顯示出性別差異，引發了對新興的 AI 素養和技術性別差距的擔憂。此外，雖然大多數學生建設性地利用 LLM，但年輕使用者缺乏系統性的校對和批判性評估，這表明對認知技能發展的潛在風險，包括批判性思考和基礎知識。調查結果強調了教育機構必須調整其課程以有效整合 AI 工具、促進道德使用、批判性思考以及認識 AI 的限制和環境成本。本文提供了可行的建議，以促進 LLM 和教育的公平且有效的共存，同時應對新興的挑戰。

##### **Power- and Fragmentation-aware Online Scheduling for GPU Datacenters**
2412.17484v1 by Francesco Lettich, Emanuele Carlini, Franco Maria Nardini, Raffaele Perego, Salvatore Trani

The rise of Artificial Intelligence and Large Language Models is driving
increased GPU usage in data centers for complex training and inference tasks,
impacting operational costs, energy demands, and the environmental footprint of
large-scale computing infrastructures. This work addresses the online
scheduling problem in GPU datacenters, which involves scheduling tasks without
knowledge of their future arrivals. We focus on two objectives: minimizing GPU
fragmentation and reducing power consumption. GPU fragmentation occurs when
partial GPU allocations hinder the efficient use of remaining resources,
especially as the datacenter nears full capacity. A recent scheduling policy,
Fragmentation Gradient Descent (FGD), leverages a fragmentation metric to
address this issue. Reducing power consumption is also crucial due to the
significant power demands of GPUs. To this end, we propose PWR, a novel
scheduling policy to minimize power usage by selecting power-efficient GPU and
CPU combinations. This involves a simplified model for measuring power
consumption integrated into a Kubernetes score plugin. Through an extensive
experimental evaluation in a simulated cluster, we show how PWR, when combined
with FGD, achieves a balanced trade-off between reducing power consumption and
minimizing GPU fragmentation.

摘要：人工智慧與大型語言模型的崛起，推動了資料中心中 GPU 的使用量，以進行複雜的訓練和推理任務，影響了營運成本、能源需求，以及大型運算基礎設施的環境足跡。這項工作探討了 GPU 資料中心中的線上排程問題，這涉及在不知道未來到達情況下排程任務。我們專注於兩個目標：最小化 GPU 分割和減少耗電量。當部分 GPU 分配阻礙了剩餘資源的有效使用時，就會發生 GPU 分割，特別是在資料中心接近滿載時。最近的排程政策，分割梯度下降 (FGD)，利用分割指標來解決這個問題。由於 GPU 的龐大電力需求，減少耗電量也至關重要。為此，我們提出了 PWR，一種新的排程政策，透過選擇省電的 GPU 和 CPU 組合來最小化電力使用。這涉及簡化的電力消耗測量模型，整合到 Kubernetes 分數外掛程式中。透過在模擬叢集中進行廣泛的實驗評估，我們展示了 PWR 與 FGD 結合使用時，如何在減少耗電量和最小化 GPU 分割之間取得平衡的折衷。

##### **A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression**
2412.17483v1 by Chenlong Deng, Zhisong Zhang, Kelong Mao, Shuaiyi Li, Xinting Huang, Dong Yu, Zhicheng Dou

In this work, we provide a thorough investigation of gist-based context
compression methods to improve long-context processing in large language
models. We focus on two key questions: (1) How well can these methods replace
full attention models? and (2) What potential failure patterns arise due to
compression? Through extensive experiments, we show that while gist-based
compression can achieve near-lossless performance on tasks like
retrieval-augmented generation and long-document QA, it faces challenges in
tasks like synthetic recall. Furthermore, we identify three key failure
patterns: lost by the boundary, lost if surprise, and lost along the way. To
mitigate these issues, we propose two effective strategies: fine-grained
autoencoding, which enhances the reconstruction of original token information,
and segment-wise token importance estimation, which adjusts optimization based
on token dependencies. Our work provides valuable insights into the
understanding of gist token-based context compression and offers practical
strategies for improving compression capabilities.

摘要：在本文中，我們對基於要點的語境壓縮方法進行了深入研究，以改善大型語言模型中的長語境處理。我們專注於兩個關鍵問題：(1) 這些方法在多大程度上可以取代完全注意模型？(2) 由於壓縮而產生的潛在失敗模式是什麼？通過廣泛的實驗，我們表明，儘管基於要點的壓縮可以在檢索增強生成和長文檔問答等任務上實現近乎無損的性能，但在合成召回等任務中面臨挑戰。此外，我們識別出三種關鍵的失敗模式：被邊界丟失、因驚喜而丟失以及在途中丟失。為了減輕這些問題，我們提出了兩種有效的策略：細粒度自動編碼，它增強了原始令牌信息的重建，以及分段令牌重要性估計，它根據令牌依賴關係調整優化。我們的研究為理解基於要點令牌的語境壓縮提供了寶貴的見解，並為改進壓縮能力提供了實用的策略。

##### **A Survey on Multi-Generative Agent System: Recent Advances and New Frontiers**
2412.17481v1 by Shuaihang Chen, Yuanxing Liu, Wei Han, Weinan Zhang, Ting Liu

Multi-generative agent systems (MGASs) have become a research hotspot since
the rise of large language models (LLMs). However, with the continuous influx
of new related works, the existing reviews struggle to capture them
comprehensively. This paper presents a comprehensive survey of these studies.
We first discuss the definition of MGAS, a framework encompassing much of
previous work. We provide an overview of the various applications of MGAS in
(i) solving complex tasks, (ii) simulating specific scenarios, and (iii)
evaluating generative agents. Building on previous studies, we also highlight
several challenges and propose future directions for research in this field.

摘要：多世代代理系統（MGAS）自大型語言模型（LLM）興起以來已成為研究熱點。然而，隨著大量相關新作品的持續湧入，現有的評論難以全面涵蓋它們。本文對這些研究進行了全面的調查。我們首先討論了 MGAS 的定義，一個涵蓋許多先前工作的框架。我們概述了 MGAS 在以下方面的各種應用：（i）解決複雜任務，（ii）模擬特定場景，以及（iii）評估生成代理。在先前的研究基礎上，我們還強調了幾個挑戰，並提出了該領域未來的研究方向。

##### **Signal Transformation for Effective Multi-Channel Signal Processing**
2412.17478v1 by Sunil Kumar Kopparapu

Electroencephalography (EEG) is an non-invasive method to record the
electrical activity of the brain. The EEG signals are low bandwidth and
recorded from multiple electrodes simultaneously in a time synchronized manner.
Typical EEG signal processing involves extracting features from all the
individual channels separately and then fusing these features for downstream
applications. In this paper, we propose a signal transformation, using basic
signal processing, to combine the individual channels of a low-bandwidth
signal, like the EEG into a single-channel high-bandwidth signal, like audio.
Further this signal transformation is bi-directional, namely the high-bandwidth
single-channel can be transformed to generate the individual low-bandwidth
signals without any loss of information. Such a transformation when applied to
EEG signals overcomes the need to process multiple signals and allows for a
single-channel processing. The advantage of this signal transformation is that
it allows the use of pre-trained single-channel pre-trained models, for
multi-channel signal processing and analysis. We further show the utility of
the signal transformation on publicly available EEG dataset.

摘要：腦電圖 (EEG) 是一種非侵入性的方法，用於記錄大腦的電氣活動。EEG 訊號頻寬低，並以時間同步的方式同時從多個電極記錄。典型的 EEG 訊號處理涉及從所有個別通道中分別提取特徵，然後融合這些特徵以供下游應用。在本文中，我們提出一個訊號轉換，使用基本的訊號處理，將低頻寬訊號（例如 EEG）的個別通道組合成單通道高頻寬訊號（例如音訊）。此外，此訊號轉換是雙向的，即可以轉換高頻寬單通道以產生個別低頻寬訊號，而不會有任何資訊遺失。當這種轉換應用於 EEG 訊號時，可以克服處理多個訊號的需求，並允許單通道處理。這種訊號轉換的優點在於，它允許使用預先訓練的單通道預先訓練模型，進行多通道訊號處理和分析。我們進一步展示了訊號轉換在公開可用 EEG 資料集上的效用。

##### **Line Graph Vietoris-Rips Persistence Diagram for Topological Graph Representation Learning**
2412.17468v1 by Jaesun Shin, Eunjoo Jeon, Taewon Cho, Namkyeong Cho, Youngjune Gwon

While message passing graph neural networks result in informative node
embeddings, they may suffer from describing the topological properties of
graphs. To this end, node filtration has been widely used as an attempt to
obtain the topological information of a graph using persistence diagrams.
However, these attempts have faced the problem of losing node embedding
information, which in turn prevents them from providing a more expressive graph
representation. To tackle this issue, we shift our focus to edge filtration and
introduce a novel edge filtration-based persistence diagram, named Topological
Edge Diagram (TED), which is mathematically proven to preserve node embedding
information as well as contain additional topological information. To implement
TED, we propose a neural network based algorithm, named Line Graph
Vietoris-Rips (LGVR) Persistence Diagram, that extracts edge information by
transforming a graph into its line graph. Through LGVR, we propose two model
frameworks that can be applied to any message passing GNNs, and prove that they
are strictly more powerful than Weisfeiler-Lehman type colorings. Finally we
empirically validate superior performance of our models on several graph
classification and regression benchmarks.

摘要：虽然信息传递图神经网络会产生信息丰富的节点嵌入，但它们可能无法描述图的拓扑属性。为此，节点过滤已广泛用作尝试使用持久性图来获取图的拓扑信息。然而，这些尝试面临着丢失节点嵌入信息的问题，这反过来又阻止它们提供更具表现力的图表示。为了解决这个问题，我们将重点转移到边缘过滤，并引入了一种新颖的基于边缘过滤的持久性图，称为拓扑边缘图 (TED)，它在数学上被证明可以保留节点嵌入信息以及包含额外的拓扑信息。为了实现 TED，我们提出了一种基于神经网络的算法，称为线图 Vietoris-Rips (LGVR) 持久性图，它通过将图转换为其线图来提取边缘信息。通过 LGVR，我们提出了两个可以应用于任何信息传递 GNN 的模型框架，并证明它们严格比 Weisfeiler-Lehman 类型着色更强大。最后，我们在几个图分类和回归基准上凭经验验证了我们模型的卓越性能。

##### **Developmental Predictive Coding Model for Early Infancy Mono and Bilingual Vocal Continual Learning**
2412.17456v1 by Xiaodan Chen, Alexandre Pitti, Mathias Quoy, Nancy F Chen

Understanding how infants perceive speech sounds and language structures is
still an open problem. Previous research in artificial neural networks has
mainly focused on large dataset-dependent generative models, aiming to
replicate language-related phenomena such as ''perceptual narrowing''. In this
paper, we propose a novel approach using a small-sized generative neural
network equipped with a continual learning mechanism based on predictive coding
for mono-and bilingual speech sound learning (referred to as language sound
acquisition during ''critical period'') and a compositional optimization
mechanism for generation where no learning is involved (later infancy sound
imitation). Our model prioritizes interpretability and demonstrates the
advantages of online learning: Unlike deep networks requiring substantial
offline training, our model continuously updates with new data, making it
adaptable and responsive to changing inputs. Through experiments, we
demonstrate that if second language acquisition occurs during later infancy,
the challenges associated with learning a foreign language after the critical
period amplify, replicating the perceptual narrowing effect.

摘要：了解婴儿如何感知语音和语言结构仍然是一个悬而未决的问题。先前的人工神经网络研究主要集中在大型数据集相关的生成模型上，旨在复制语言相关的现象，例如“感知狭窄”。在本文中，我们提出了一种新方法，使用配备了基于预测编码的持续学习机制的小型生成神经网络，用于单语和双语语音声音学习（称为“关键时期”期间的语言声音习得）和生成的组合优化机制，其中不涉及学习（后来的婴儿声音模仿）。我们的模型优先考虑可解释性，并展示了在线学习的优势：与需要大量离线训练的深度网络不同，我们的模型会不断更新新数据，使其适应并响应不断变化的输入。通过实验，我们证明，如果第二语言习得发生在婴儿后期，那么在关键时期之后学习外语相关的挑战会放大，复制感知狭窄效应。

##### **Diving into Self-Evolving Training for Multimodal Reasoning**
2412.17451v1 by Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, Junxian He

Reasoning ability is essential for Large Multimodal Models (LMMs). In the
absence of multimodal chain-of-thought annotated data, self-evolving training,
where the model learns from its own outputs, has emerged as an effective and
scalable approach for enhancing reasoning abilities. Despite its growing usage,
a comprehensive understanding of self-evolving training, particularly in the
context of multimodal reasoning, remains limited. In this paper, we delve into
the intricacies of self-evolving training for multimodal reasoning, pinpointing
three key factors: Training Method, Reward Model, and Prompt Variation. We
systematically examine each factor and explore how various configurations
affect the training's effectiveness. Our analysis leads to a set of best
practices for each factor, aimed at optimizing multimodal reasoning.
Furthermore, we explore the Self-Evolution Dynamics during training and the
impact of automatic balancing mechanisms in boosting performance. After all the
investigations, we present a final recipe for self-evolving training in
multimodal reasoning, encapsulating these design choices into a framework we
call MSTaR (Multimodal Self-evolving Training for Reasoning), which is
universally effective for models with different sizes on various benchmarks,
e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning
benchmarks without using additional human annotations, as demonstrated on
MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this
study fills a significant gap in the understanding of self-evolving training
for multimodal reasoning and offers a robust framework for future research. Our
policy and reward models, as well as the collected data, is released to
facilitate further investigation in multimodal reasoning.

摘要：推理能力对于大型多模态模型 (LMM) 至关重要。在缺乏多模态思维链注释数据的情况下，自我进化训练（模型从其自身的输出中学习）已成为一种有效且可扩展的方法，用于增强推理能力。尽管使用越来越广泛，但对自我进化训练的全面理解，尤其是在多模态推理的背景下，仍然有限。在本文中，我们深入研究了多模态推理的自我进化训练的复杂性，指出了三个关键因素：训练方法、奖励模型和提示变化。我们系统地检查了每个因素，并探讨了各种配置如何影响训练的有效性。我们的分析针对每个因素得出了一组最佳实践，旨在优化多模态推理。此外，我们探讨了训练期间的自我进化动态以及自动平衡机制对提升性能的影响。在所有调查之后，我们提出了多模态推理中自我进化训练的最终秘诀，将这些设计选择封装到一个我们称之为 MSTaR（推理多模态自我进化训练）的框架中，该框架对不同规模的模型在各种基准上都具有普遍的有效性，例如，在 5 个多模态推理基准上明显超越了预先进化的模型，而无需使用额外的注释，如在 MiniCPM-V-2.5 (8B)、Phi-3.5-Vision (4B) 和 InternVL2 (2B) 上所展示的。我们相信，这项研究填补了对多模态推理的自我进化训练理解方面的重大空白，并为未来的研究提供了稳健的框架。我们的策略和奖励模型以及收集的数据已发布，以促进对多模态推理的进一步调查。

##### **Applying LLM and Topic Modelling in Psychotherapeutic Contexts**
2412.17449v1 by Alexander Vanin, Vadim Bolshev, Anastasia Panfilova

This study explores the use of Large language models to analyze therapist
remarks in a psychotherapeutic setting. The paper focuses on the application of
BERTopic, a machine learning-based topic modeling tool, to the dialogue of two
different groups of therapists (classical and modern), which makes it possible
to identify and describe a set of topics that consistently emerge across these
groups. The paper describes in detail the chosen algorithm for BERTopic, which
included creating a vector space from a corpus of therapist remarks, reducing
its dimensionality, clustering the space, and creating and optimizing topic
representation. Along with the automatic topical modeling by the BERTopic, the
research involved an expert assessment of the findings and manual topic
structure optimization. The topic modeling results highlighted the most common
and stable topics in therapists speech, offering insights into how language
patterns in therapy develop and remain stable across different therapeutic
styles. This work contributes to the growing field of machine learning in
psychotherapy by demonstrating the potential of automated methods to improve
both the practice and training of therapists. The study highlights the value of
topic modeling as a tool for gaining a deeper understanding of therapeutic
dialogue and offers new opportunities for improving therapeutic effectiveness
and clinical supervision.

摘要：本研究探討使用大型語言模型來分析心理治療環境中的治療師評論。本文重點關注將基於機器學習的主題建模工具 BERTopic 應用於兩組不同治療師（古典和現代）的對話，這使得識別和描述一組在這些組中持續出現的主題成為可能。本文詳細描述了 BERTopic 的選擇演算法，其中包括從治療師評論語料庫建立向量空間、降低其維度、對空間進行分群，以及建立和最佳化主題表示。除了 BERTopic 的自動主題建模外，該研究還包括對研究結果的專家評估和手動主題結構最佳化。主題建模結果突出了治療師言語中最常見且穩定的主題，提供了有關治療中的語言模式如何發展並在不同的治療風格中保持穩定的見解。這項工作透過展示自動化方法在改善治療師實務和訓練方面的潛力，為心理治療中機器學習的成長領域做出貢獻。本研究強調了主題建模作為深入了解治療對話的工具的價值，並為改善治療效果和臨床督導提供了新的機會。

##### **The Role of XAI in Transforming Aeronautics and Aerospace Systems**
2412.17440v1 by Francisco Javier Cantero Zorita, Mikel Galafate, Javier M. Moguerza, Isaac Martín de Diego, M. Teresa Gonzalez, Gema Gutierrez Peña

Recent advancements in Artificial Intelligence (AI) have transformed
decision-making in aeronautics and aerospace. These advancements in AI have
brought with them the need to understand the reasons behind the predictions
generated by AI systems and models, particularly by professionals in these
sectors. In this context, the emergence of eXplainable Artificial Intelligence
(XAI) has helped bridge the gap between professionals in the aeronautical and
aerospace sectors and the AI systems and models they work with. For this
reason, this paper provides a review of the concept of XAI is carried out
defining the term and the objectives it aims to achieve. Additionally, the
paper discusses the types of models defined within it and the properties these
models must fulfill to be considered transparent, as well as the post-hoc
techniques used to understand AI systems and models after their training.
Finally, various application areas within the aeronautical and aerospace
sectors will be presented, highlighting how XAI is used in these fields to help
professionals understand the functioning of AI systems and models.

摘要：人工智能 (AI) 的最新進展已轉變航空和航太領域的決策制定。這些 AI 進展帶來了解 AI 系統和模型所產生預測背後原因的需求，特別是這些領域的專業人士。在此背景下，可解釋人工智慧 (XAI) 的出現有助於縮小航空和航太領域專業人士與他們所使用的 AI 系統和模型之間的差距。基於這個原因，本文回顧了 XAI 的概念，定義了這個術語及其旨在達成的目標。此外，本文討論了在其中定義的模型類型，以及這些模型必須具備的特性才能被視為透明，以及在訓練後用於了解 AI 系統和模型的事後技術。最後，將介紹航空和航太領域內的各種應用領域，重點說明 XAI 如何用於這些領域，以幫助專業人士了解 AI 系統和模型的功能。

##### **Markov Process-Based Graph Convolutional Networks for Entity Classification in Knowledge Graphs**
2412.17438v1 by Johannes Mäkelburg, Yiwen Peng, Mehwish Alam, Tobias Weller, Maribel Acosta

Despite the vast amount of information encoded in Knowledge Graphs (KGs),
information about the class affiliation of entities remains often incomplete.
Graph Convolutional Networks (GCNs) have been shown to be effective predictors
of complete information about the class affiliation of entities in KGs.
However, these models do not learn the class affiliation of entities in KGs
incorporating the complexity of the task, which negatively affects the models
prediction capabilities. To address this problem, we introduce a Markov
process-based architecture into well-known GCN architectures. This end-to-end
network learns the prediction of class affiliation of entities in KGs within a
Markov process. The number of computational steps is learned during training
using a geometric distribution. At the same time, the loss function combines
insights from the field of evidential learning. The experiments show a
performance improvement over existing models in several studied architectures
and datasets. Based on the chosen hyperparameters for the geometric
distribution, the expected number of computation steps can be adjusted to
improve efficiency and accuracy during training.

摘要：儘管知識圖譜 (KGs) 中編碼了大量的資訊，
關於實體類別關聯的資訊常常是不完整的。
圖形卷積網路 (GCNs) 已被證明是有效預測 KGs 中實體類別關聯的完整資訊。
然而，這些模型並未學習 KGs 中實體的類別關聯，
結合任務的複雜性，這對模型的預測能力產生了負面影響。
為了解決這個問題，我們將基於馬可夫過程的架構引入眾所周知的 GCN 架構中。
這個端對端網路在馬可夫過程中學習預測 KGs 中實體的類別關聯。
計算步驟的數量在訓練期間使用幾何分佈學習。
同時，損失函數結合了證據學習領域的見解。
實驗顯示，在幾個研究的架構和資料集中的效能優於現有模型。
根據幾何分佈所選取的超參數，預期的計算步驟數量可以調整，
以提高訓練期間的效率和準確度。

##### **Measuring Contextual Informativeness in Child-Directed Text**
2412.17427v1 by Maria Valentini, Téa Wright, Ali Marashian, Jennifer Weber, Eliana Colunga, Katharina von der Wense

To address an important gap in creating children's stories for vocabulary
enrichment, we investigate the automatic evaluation of how well stories convey
the semantics of target vocabulary words, a task with substantial implications
for generating educational content. We motivate this task, which we call
measuring contextual informativeness in children's stories, and provide a
formal task definition as well as a dataset for the task. We further propose a
method for automating the task using a large language model (LLM). Our
experiments show that our approach reaches a Spearman correlation of 0.4983
with human judgments of informativeness, while the strongest baseline only
obtains a correlation of 0.3534. An additional analysis shows that the
LLM-based approach is able to generalize to measuring contextual
informativeness in adult-directed text, on which it also outperforms all
baselines.

摘要：為了彌補在為兒童創作故事以豐富詞彙上的重要差距，我們探討如何自動評估故事傳達目標詞彙語義的程度，這項任務對產生教育內容有重大的影響。我們激勵這項任務，我們稱之為衡量兒童故事中的脈絡資訊性，並提供正式的任務定義以及任務的資料集。我們進一步提出一個使用大型語言模型 (LLM) 自動化任務的方法。我們的實驗顯示，我們的做法與人類的資訊性判斷達到 0.4983 的 Spearman 相關性，而最強的基準線僅獲得 0.3534 的相關性。額外的分析顯示，基於 LLM 的方法能夠概括為衡量成人導向文本中的脈絡資訊性，在該文本中，它也優於所有基準線。

##### **VidCtx: Context-aware Video Question Answering with Image Models**
2412.17415v1 by Andreas Goulas, Vasileios Mezaris, Ioannis Patras

To address computational and memory limitations of Large Multimodal Models in
the Video Question-Answering task, several recent methods extract textual
representations per frame (e.g., by captioning) and feed them to a Large
Language Model (LLM) that processes them to produce the final response.
However, in this way, the LLM does not have access to visual information and
often has to process repetitive textual descriptions of nearby frames. To
address those shortcomings, in this paper, we introduce VidCtx, a novel
training-free VideoQA framework which integrates both modalities, i.e. both
visual information from input frames and textual descriptions of others frames
that give the appropriate context. More specifically, in the proposed framework
a pre-trained Large Multimodal Model (LMM) is prompted to extract at regular
intervals, question-aware textual descriptions (captions) of video frames.
Those will be used as context when the same LMM will be prompted to answer the
question at hand given as input a) a certain frame, b) the question and c) the
context/caption of an appropriate frame. To avoid redundant information, we
chose as context the descriptions of distant frames. Finally, a simple yet
effective max pooling mechanism is used to aggregate the frame-level decisions.
This methodology enables the model to focus on the relevant segments of the
video and scale to a high number of frames. Experiments show that VidCtx
achieves competitive performance among approaches that rely on open models on
three public Video QA benchmarks, NExT-QA, IntentQA and STAR.

摘要：<paragraph>為了解決大型多模態模型在影片問答任務中的運算和記憶限制，最近有幾種方法每幀提取文字表徵（例如，透過標題），並將其提供給大型語言模型（LLM），由其處理以產生最後的回應。
然而，這樣一來，LLM 無法存取視覺資訊，而且常常必須處理附近幀的重複文字描述。為了解決這些缺點，我們在這篇論文中介紹 VidCtx，一種新穎的免訓練 VideoQA 架構，它整合了兩種模式，也就是輸入幀的視覺資訊和提供適當脈絡的其他幀的文字描述。更具體地說，在建議的架構中，會提示預先訓練好的大型多模態模型（LMM）定期提取影片幀的與問題相關的文字描述（標題）。
當提示相同的 LMM 回答手邊的問題時，這些描述將用作脈絡，輸入為 a）某個幀、b）問題，以及 c）適當幀的脈絡/標題。為了避免冗餘資訊，我們選擇遠距離幀的描述作為脈絡。最後，使用一個簡單但有效的最大池化機制來彙總幀層級的決策。這種方法使模型能夠專注於影片中的相關片段，並擴展到大量的幀。實驗顯示，VidCtx 在依賴開放模型的方法中，於三個公開的影片問答基準，NExT-QA、IntentQA 和 STAR，取得了有競爭力的表現。</paragraph>

##### **Pretraining with random noise for uncertainty calibration**
2412.17411v1 by Jeonghwan Cheon, Se-Bum Paik

Uncertainty calibration, the process of aligning confidence with accuracy, is
a hallmark of human intelligence. However, most machine learning models
struggle to achieve this alignment, particularly when the training dataset is
small relative to the network's capacity. Here, we demonstrate that uncertainty
calibration can be effectively achieved through a pretraining method inspired
by developmental neuroscience. Specifically, training with random noise before
data training allows neural networks to calibrate their uncertainty, ensuring
that confidence levels are aligned with actual accuracy. We show that randomly
initialized, untrained networks tend to exhibit erroneously high confidence,
but pretraining with random noise effectively calibrates these networks,
bringing their confidence down to chance levels across input spaces. As a
result, networks pretrained with random noise exhibit optimal calibration, with
confidence closely aligned with accuracy throughout subsequent data training.
These pre-calibrated networks also perform better at identifying "unknown data"
by exhibiting lower confidence for out-of-distribution samples. Our findings
provide a fundamental solution for uncertainty calibration in both
in-distribution and out-of-distribution contexts.

摘要：不確定性校準，也就是將信心與準確性對齊的過程，是人類智慧的標誌。然而，大多數機器學習模型難以實現這種對齊，特別是在訓練資料集相對於網路容量較小的時候。在此，我們展示了不確定性校準可以透過受發展神經科學啟發的預訓練方法有效實現。具體來說，在資料訓練之前使用隨機雜訊進行訓練，可以讓神經網路校準其不確定性，確保信心水準與實際準確性對齊。我們展示隨機初始化的未訓練網路往往表現出錯誤的高信心，但使用隨機雜訊進行預訓練可有效校準這些網路，將其信心降低到輸入空間中的機率水準。因此，使用隨機雜訊預訓練的網路表現出最佳校準，信心與準確性在後續資料訓練中緊密對齊。這些預先校準的網路在識別「未知資料」方面也表現得更好，對分布外樣本表現出較低的信心。我們的發現為分佈內和分佈外背景中的不確定性校準提供了一個基本的解決方案。

##### **Just What You Desire: Constrained Timeline Summarization with Self-Reflection for Enhanced Relevance**
2412.17408v1 by Muhammad Reza Qorib, Qisheng Hu, Hwee Tou Ng

Given news articles about an entity, such as a public figure or organization,
timeline summarization (TLS) involves generating a timeline that summarizes the
key events about the entity. However, the TLS task is too underspecified, since
what is of interest to each reader may vary, and hence there is not a single
ideal or optimal timeline. In this paper, we introduce a novel task, called
Constrained Timeline Summarization (CTLS), where a timeline is generated in
which all events in the timeline meet some constraint. An example of a
constrained timeline concerns the legal battles of Tiger Woods, where only
events related to his legal problems are selected to appear in the timeline. We
collected a new human-verified dataset of constrained timelines involving 47
entities and 5 constraints per entity. We propose an approach that employs a
large language model (LLM) to summarize news articles according to a specified
constraint and cluster them to identify key events to include in a constrained
timeline. In addition, we propose a novel self-reflection method during summary
generation, demonstrating that this approach successfully leads to improved
performance.

摘要：给定有关实体（例如公众人物或组织）的新闻文章，时间线摘要 (TLS) 涉及生成一条总结该实体关键事件的时间线。然而，TLS 任务过于不具体，因为每个读者的兴趣可能不同，因此没有单一的理想或最佳时间线。在本文中，我们引入了一项新任务，称为受限时间线摘要 (CTLS)，其中生成一条时间线，其中时间线中的所有事件都满足某些约束。受限时间线的一个示例涉及老虎伍兹的法律纠纷，其中仅选择与他的法律问题相关的时间出现在时间线中。我们收集了一个新的经过人工验证的受限时间线数据集，涉及 47 个实体和每个实体 5 个约束。我们提出了一种方法，该方法采用大型语言模型 (LLM) 根据指定约束总结新闻文章，并将它们聚类以识别要包含在受限时间线中的关键事件。此外，我们在摘要生成期间提出了一种新颖的自省方法，证明这种方法成功地带来了性能改进。

##### **BrainMAP: Learning Multiple Activation Pathways in Brain Networks**
2412.17404v1 by Song Wang, Zhenyu Lei, Zhen Tan, Jiaqi Ding, Xinyu Zhao, Yushun Dong, Guorong Wu, Tianlong Chen, Chen Chen, Aiying Zhang, Jundong Li

Functional Magnetic Resonance Image (fMRI) is commonly employed to study
human brain activity, since it offers insight into the relationship between
functional fluctuations and human behavior. To enhance analysis and
comprehension of brain activity, Graph Neural Networks (GNNs) have been widely
applied to the analysis of functional connectivities (FC) derived from fMRI
data, due to their ability to capture the synergistic interactions among brain
regions. However, in the human brain, performing complex tasks typically
involves the activation of certain pathways, which could be represented as
paths across graphs. As such, conventional GNNs struggle to learn from these
pathways due to the long-range dependencies of multiple pathways. To address
these challenges, we introduce a novel framework BrainMAP to learn Multiple
Activation Pathways in Brain networks. BrainMAP leverages sequential models to
identify long-range correlations among sequentialized brain regions and
incorporates an aggregation module based on Mixture of Experts (MoE) to learn
from multiple pathways. Our comprehensive experiments highlight BrainMAP's
superior performance. Furthermore, our framework enables explanatory analyses
of crucial brain regions involved in tasks. Our code is provided at
https://github.com/LzyFischer/Graph-Mamba.

摘要：功能性磁振造影（fMRI）通常用于研究人脑活动，因为它提供了功能性波动与人类行为之间关系的见解。为了增强对脑活动进行分析和理解，图神经网络（GNN）已被广泛应用于分析功能性连通性（FC），该连通性源自 fMRI 数据，这是因为图神经网络能够捕捉大脑区域之间的协同交互。然而，在人脑中，执行复杂任务通常涉及激活某些通路，这些通路可以表示为跨越图表的路径。因此，传统的 GNN 由于多个路径的长程依赖性而难以从这些路径中学习。为了应对这些挑战，我们引入了一个新的框架 BrainMAP 来学习大脑网络中的多重激活通路。BrainMAP 利用顺序模型来识别顺序化大脑区域之间的长程相关性，并结合基于专家混合（MoE）的聚合模块来学习多重通路。我们全面的实验突出了 BrainMAP 的卓越性能。此外，我们的框架能够对参与任务的关键大脑区域进行解释性分析。我们的代码可在 https://github.com/LzyFischer/Graph-Mamba 中获得。

##### **WarriorCoder: Learning from Expert Battles to Augment Code Large Language Models**
2412.17395v1 by Huawen Feng, Pu Zhao, Qingfeng Sun, Can Xu, Fangkai Yang, Lu Wang, Qianli Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

Despite recent progress achieved by code large language models (LLMs), their
remarkable abilities are largely dependent on fine-tuning on the high-quality
data, posing challenges for data collection and annotation. To address this,
current methods often design various data flywheels to gather complex code
instructions, enabling models to handle more intricate tasks. However, these
approaches typically rely on off-the-shelf datasets and data augmentation from
the limited pool of proprietary LLMs (e.g., Claude, GPT4, and so on), which
limits the diversity of the constructed data and makes it prone to systemic
biases. In this paper, we propose WarriorCoder which learns from expert battles
to address these limitations. Specifically, we create an arena for current
expert code LLMs, where each model challenges and responds to others'
challenges, with evaluations conducted by uninvolved judge models. This
competitive framework generates novel training data constructed from scratch,
harnessing the strengths of all participants. Experimental results demonstrate
that WarriorCoder achieves competitive performance compared to previous
methods, even without relying on proprietary LLMs.

摘要：儘管大型語言模型 (LLM) 的程式碼最近取得進展，但它們卓越的能力在很大程度上依賴於對高品質資料進行微調，這對資料收集和註解構成挑戰。為了解決這個問題，目前的各種方法通常會設計各種資料飛輪來收集複雜的程式碼指令，使模型能夠處理更複雜的任務。然而，這些方法通常依賴於現成的資料集和來自有限專有 LLM（例如 Claude、GPT4 等）的資料擴充，這限制了建構資料的多樣性，並使其容易產生系統性偏差。在本文中，我們提出 WarriorCoder，它從專家戰鬥中學習以解決這些限制。具體來說，我們為目前的專家程式碼 LLM 建立了一個競技場，每個模型都會挑戰並回應其他模型的挑戰，並由未參與的評審模型進行評估。這個競爭框架會產生從頭建構的新訓練資料，利用所有參與者的優勢。實驗結果表明，即使不依賴專有 LLM，WarriorCoder 也能達到與先前方法相當的競爭力。

##### **Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement**
2412.17387v1 by Hyeonjin Kim, Jaejun Yoo

While pruning methods effectively maintain model performance without extra
training costs, they often focus solely on preserving crucial connections,
overlooking the impact of pruned weights on subsequent fine-tuning or
distillation, leading to inefficiencies. Moreover, most compression techniques
for generative models have been developed primarily for GANs, tailored to
specific architectures like StyleGAN, and research into compressing Diffusion
models has just begun. Even more, these methods are often applicable only to
GANs or Diffusion models, highlighting the need for approaches that work across
both model types. In this paper, we introduce Singular Value Scaling (SVS), a
versatile technique for refining pruned weights, applicable to both model
types. Our analysis reveals that pruned weights often exhibit dominant singular
vectors, hindering fine-tuning efficiency and leading to suboptimal performance
compared to random initialization. Our method enhances weight initialization by
minimizing the disparities between singular values of pruned weights, thereby
improving the fine-tuning process. This approach not only guides the compressed
model toward superior solutions but also significantly speeds up fine-tuning.
Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS
improves compression performance across model types without additional training
costs. Our code is available at:
https://github.com/LAIT-CVLab/Singular_Value_Scaling.

摘要：雖然剪枝方法有效地維持模型效能而無額外的訓練成本，它們通常只專注於保留關鍵連接，而忽略修剪權重對後續微調或蒸餾的影響，導致效率低下。此外，大多數用於生成模型的壓縮技術主要針對 GAN 而開發，專門針對 StyleGAN 等特定架構，而對擴散模型的壓縮研究才剛開始。更重要的是，這些方法通常僅適用於 GAN 或擴散模型，這突顯了需要跨這兩種模型類型運作的方法。在本文中，我們引入了奇異值縮放 (SVS)，這是一種適用於這兩種模型類型的精煉修剪權重的通用技術。我們的分析揭示，修剪權重通常表現出主要的奇異向量，阻礙微調效率，並導致與隨機初始化相比次佳的效能。我們的技術通過最小化修剪權重的奇異值之間的差異來增強權重初始化，從而改善微調過程。這種方法不僅引導壓縮模型走向更好的解，而且還顯著加快了微調速度。對 StyleGAN2、StyleGAN3 和 DDPM 的大量實驗表明，SVS 改善了跨模型類型的壓縮效能，而無需額外的訓練成本。我們的程式碼可在以下網址取得：
https://github.com/LAIT-CVLab/Singular_Value_Scaling。

##### **Interweaving Memories of a Siamese Large Language Model**
2412.17383v1 by Xin Song, Zhikai Xue, Guoxiu He, Jiawei Liu, Wei Lu

Parameter-efficient fine-tuning (PEFT) methods optimize large language models
(LLMs) by modifying or introducing a small number of parameters to enhance
alignment with downstream tasks. However, they can result in catastrophic
forgetting, where LLMs prioritize new knowledge at the expense of comprehensive
world knowledge. A promising approach to mitigate this issue is to recall prior
memories based on the original knowledge. To this end, we propose a
model-agnostic PEFT framework, IMSM, which Interweaves Memories of a Siamese
Large Language Model. Specifically, our siamese LLM is equipped with an
existing PEFT method. Given an incoming query, it generates two distinct
memories based on the pre-trained and fine-tuned parameters. IMSM then
incorporates an interweaving mechanism that regulates the contributions of both
original and enhanced memories when generating the next token. This framework
is theoretically applicable to all open-source LLMs and existing PEFT methods.
We conduct extensive experiments across various benchmark datasets, evaluating
the performance of popular open-source LLMs using the proposed IMSM, in
comparison to both classical and leading PEFT methods. Our findings indicate
that IMSM maintains comparable time and space efficiency to backbone PEFT
methods while significantly improving performance and effectively mitigating
catastrophic forgetting.

摘要：參數高效微調 (PEFT) 方法透過修改或引入少數參數來最佳化大型語言模型 (LLM)，以增強與下游任務的對齊。然而，它們可能會導致災難性遺忘，其中 LLM 以犧牲全面世界知識為代價優先考慮新知識。減輕此問題的一個有希望的方法是根據原始知識回想起先前的記憶。為此，我們提出了一個與模型無關的 PEFT 框架 IMSM，它交織了連體大型語言模型的記憶。具體來說，我們的連體 LLM 配備了現有的 PEFT 方法。給定一個輸入查詢，它會根據預訓練和微調參數產生兩個不同的記憶。然後，IMSM 會納入一個交織機制，在產生下一個標記時調節原始記憶和增強記憶的貢獻。此框架在理論上適用於所有開源 LLM 和現有的 PEFT 方法。我們對各種基準數據集進行了廣泛的實驗，使用提議的 IMSM 評估了使用流行開源 LLM 的效能，並與傳統和領先的 PEFT 方法進行比較。我們的研究結果表明，IMSM 在保持與主幹 PEFT 方法相當的時間和空間效率的同時，顯著提升了效能，並有效減輕了災難性遺忘。

##### **A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions**
2412.17377v1 by Youliang Zhang, Ronghui Li, Yachao Zhang, Liang Pan, Jingbo Wang, Yebin Liu, Xiu Li

Extracting physically plausible 3D human motion from videos is a critical
task. Although existing simulation-based motion imitation methods can enhance
the physical quality of daily motions estimated from monocular video capture,
extending this capability to high-difficulty motions remains an open challenge.
This can be attributed to some flawed motion clips in video-based motion
capture results and the inherent complexity in modeling high-difficulty
motions. Therefore, sensing the advantage of segmentation in localizing human
body, we introduce a mask-based motion correction module (MCM) that leverages
motion context and video mask to repair flawed motions, producing
imitation-friendly motions; and propose a physics-based motion transfer module
(PTM), which employs a pretrain and adapt approach for motion imitation,
improving physical plausibility with the ability to handle in-the-wild and
challenging motions. Our approach is designed as a plug-and-play module to
physically refine the video motion capture results, including high-difficulty
in-the-wild motions. Finally, to validate our approach, we collected a
challenging in-the-wild test set to establish a benchmark, and our method has
demonstrated effectiveness on both the new benchmark and existing public
datasets.https://physicalmotionrestoration.github.io

摘要：從影片中擷取物理上合理的 3D 人體動作是一項重要的任務。儘管現有的基於模擬的動作模仿方法可以提升從單眼影片擷取的日常動作之物理品質，將此能力擴展到高難度動作仍然是一項公開的挑戰。這可以歸因於影片動作擷取結果中一些有缺陷的動作片段，以及建模高難度動作的複雜性。因此，我們感知到分割在定位人體中的優勢，我們引入了一個基於遮罩的動作校正模組 (MCM)，它利用動作內容和影片遮罩來修復有缺陷的動作，產生友善的模仿動作；並提出了一個基於物理的動作轉移模組 (PTM)，它採用預訓練和適應的方式進行動作模仿，透過處理野外和具挑戰性的動作來提升物理合理性。我們的做法被設計成一個即插即用的模組，用於物理精煉影片動作擷取結果，包括野外的高難度動作。最後，為了驗證我們的做法，我們收集了一個具挑戰性的野外測試集來建立基準，我們的做法已在新的基準和現有的公開資料集上證明其有效性。https://physicalmotionrestoration.github.io

##### **FRTP: Federating Route Search Records to Enhance Long-term Traffic Prediction**
2412.17373v1 by Hangli Ge, Xiaojie Yang, Itsuki Matsunaga, Dizhi Huang, Noboru Koshizuka

Accurate traffic prediction, especially predicting traffic conditions several
days in advance is essential for intelligent transportation systems (ITS). Such
predictions enable mid- and long-term traffic optimization, which is crucial
for efficient transportation planning. However, the inclusion of diverse
external features, alongside the complexities of spatial relationships and
temporal uncertainties, significantly increases the complexity of forecasting
models. Additionally, traditional approaches have handled data preprocessing
separately from the learning model, leading to inefficiencies caused by
repeated trials of preprocessing and training. In this study, we propose a
federated architecture capable of learning directly from raw data with varying
features and time granularities or lengths. The model adopts a unified design
that accommodates different feature types, time scales, and temporal periods.
Our experiments focus on federating route search records and begin by
processing raw data within the model framework. Unlike traditional models, this
approach integrates the data federation phase into the learning process,
enabling compatibility with various time frequencies and input/output
configurations. The accuracy of the proposed model is demonstrated through
evaluations using diverse learning patterns and parameter settings. The results
show that online search log data is useful for forecasting long-term traffic,
highlighting the model's adaptability and efficiency.

摘要：精準的交通預測，特別是預測幾天後的交通狀況，對於智慧型運輸系統 (ITS) 至關重要。此類預測能讓中長期交通最佳化，這對於運輸規劃的有效性至關重要。然而，納入各種外部功能，以及空間關係和時間不確定性的複雜性，大幅增加了預測模型的複雜性。此外，傳統方法已將資料前處理與學習模型分開處理，導致重複的預處理和訓練試驗造成效率不彰。在本研究中，我們提出一個聯盟式架構，能夠直接從具有不同功能和時間粒度或長度的原始資料中學習。該模型採用統一的設計，可容納不同的功能類型、時間尺度和時間週期。我們的實驗重點在於聯邦化路徑搜尋記錄，並從模型架構內處理原始資料開始。與傳統模型不同，此方法將資料聯盟階段整合到學習過程中，讓其與各種時間頻率和輸入/輸出組態相容。所提出模型的準確性已透過使用不同的學習模式和參數設定進行評估來證明。結果顯示線上搜尋記錄資料對於預測長期交通狀況很有用，突顯了模型的適應性和效率。

##### **Boosting LLM via Learning from Data Iteratively and Selectively**
2412.17365v1 by Qi Jia, Siyu Ren, Ziheng Qin, Fuzhao Xue, Jinjie Ni, Yang You

Datasets nowadays are generally constructed from multiple sources and using
different synthetic techniques, making data de-noising and de-duplication
crucial before being used for post-training. In this work, we propose to
perform instruction tuning by iterative data selection (\ApproachName{}). We
measure the quality of a sample from complexity and diversity simultaneously.
Instead of calculating the complexity score once for all before fine-tuning, we
highlight the importance of updating this model-specific score during
fine-tuning to accurately accommodate the dynamic changes of the model. On the
other hand, the diversity score is defined on top of the samples' responses
under the consideration of their informativeness. IterIT integrates the
strengths of both worlds by iteratively updating the complexity score for the
top-ranked samples and greedily selecting the ones with the highest
complexity-diversity score. Experiments on multiple instruction-tuning data
demonstrate consistent improvements of IterIT over strong baselines. Moreover,
our approach also generalizes well to domain-specific scenarios and different
backbone models. All resources will be available at
https://github.com/JiaQiSJTU/IterIT.

摘要：現今的資料集通常由多個來源建構，並使用不同的合成技術，這使得資料去雜訊和去重在用於後訓練之前至關重要。在這項工作中，我們建議透過反覆資料選擇來執行指令微調（\ApproachName{}）。我們同時從複雜性和多樣性來衡量樣本的品質。我們並非在微調之前一次計算所有複雜性分數，而是強調在微調期間更新此模型特定分數以準確適應模型的動態變化。另一方面，多樣性分數是在樣本回應的基礎上定義的，並考慮到它們的資訊性。IterIT 整合了這兩個世界的優點，透過反覆更新排名前幾名的樣本的複雜性分數，並貪婪地選擇複雜性多樣性分數最高的樣本。對多個指令微調資料進行的實驗證明了 IterIT 在強大的基線上的一致改進。此外，我們的做法也很好地推廣到特定領域的場景和不同的主幹模型。所有資源都可以在 https://github.com/JiaQiSJTU/IterIT 獲得。

##### **Efficient fine-tuning methodology of text embedding models for information retrieval: contrastive learning penalty (clp)**
2412.17364v1 by Jeongsu Yu

Text embedding models play a crucial role in natural language processing,
particularly in information retrieval, and their importance is further
highlighted with the recent utilization of RAG (Retrieval- Augmented
Generation). This study presents an efficient fine-tuning methodology
encompassing data selection, loss function, and model architecture to enhance
the information retrieval performance of pre-trained text embedding models. In
particular, this study proposes a novel Contrastive Learning Penalty function
that overcomes the limitations of existing Contrastive Learning. The proposed
methodology achieves significant performance improvements over existing methods
in document retrieval tasks. This study is expected to contribute to improving
the performance of information retrieval systems through fine-tuning of text
embedding models. The code for this study can be found at
https://github.com/CreaLabs/Enhanced-BGE-M3-with-CLP-and-MoE, and the
best-performing model can be found at https://huggingface.co/CreaLabs.

摘要：文本嵌入模型在自然語言處理中扮演著至關重要的角色，特別是在資訊檢索中，而其重要性隨著最近利用 RAG（檢索增強生成）而進一步凸顯。本研究提出了一種有效微調方法，包含資料選擇、損失函數和模型架構，以增強預訓練文本嵌入模型的資訊檢索效能。特別是，本研究提出了一個新穎的對比學習懲罰函數，克服了現有對比學習的限制。所提出的方法在文件檢索任務中，比現有方法獲得顯著的效能提升。預期本研究將有助於透過微調文本嵌入模型來提升資訊檢索系統的效能。本研究的程式碼可以在 https://github.com/CreaLabs/Enhanced-BGE-M3-with-CLP-and-MoE 中找到，而效能最佳的模型可以在 https://huggingface.co/CreaLabs 中找到。

##### **Three-Class Text Sentiment Analysis Based on LSTM**
2412.17347v1 by Yin Qixuan

Sentiment analysis is a crucial task in natural language processing (NLP)
with applications in public opinion monitoring, market research, and beyond.
This paper introduces a three-class sentiment classification method for Weibo
comments using Long Short-Term Memory (LSTM) networks to discern positive,
neutral, and negative sentiments. LSTM, as a deep learning model, excels at
capturing long-distance dependencies in text data, providing significant
advantages over traditional machine learning approaches. Through preprocessing
and feature extraction from Weibo comment texts, our LSTM model achieves
precise sentiment prediction. Experimental results demonstrate superior
performance, achieving an accuracy of 98.31% and an F1 score of 98.28%, notably
outperforming conventional models and other deep learning methods. This
underscores the effectiveness of LSTM in capturing nuanced sentiment
information within text, thereby enhancing classification accuracy. Despite its
strengths, the LSTM model faces challenges such as high computational
complexity and slower processing times for lengthy texts. Moreover, complex
emotional expressions like sarcasm and humor pose additional difficulties.
Future work could explore combining pre-trained models or advancing feature
engineering techniques to further improve both accuracy and practicality.
Overall, this study provides an effective solution for sentiment analysis on
Weibo comments.

摘要：<paragraph>情緒分析是自然語言處理 (NLP) 中的一項關鍵任務，在公眾輿論監控、市場調查等領域都有應用。本文提出了一種使用長短期記憶 (LSTM) 網路對微博評論進行三類情緒分類的方法，以辨別正面、中立和負面情緒。LSTM 作為一種深度學習模型，擅長捕捉文本資料中的長距離依賴關係，比傳統機器學習方法具有顯著優勢。通過對微博評論文本進行預處理和特徵提取，我們的 LSTM 模型實現了精確的情緒預測。實驗結果證明了其優越的性能，達到了 98.31% 的準確率和 98.28% 的 F1 分數，顯著優於傳統模型和其他深度學習方法。這凸顯了 LSTM 在捕捉文本中細微的情緒資訊方面的有效性，從而提高了分類準確率。儘管有這些優點，但 LSTM 模型也面臨著計算複雜度高、處理較長文本時速度較慢等挑戰。此外，諷刺和幽默等複雜的情緒表達也帶來了額外的難題。未來的研究可以探索結合預訓練模型或改進特徵工程技術，以進一步提高準確率和實用性。總的來說，本研究為微博評論的情緒分析提供了一個有效的解決方案。</paragraph>

##### **FFA Sora, video generation as fundus fluorescein angiography simulator**
2412.17346v1 by Xinyuan Wu, Lili Wang, Ruoyu Chen, Bowen Liu, Weiyi Zhang, Xi Yang, Yifan Feng, Mingguang He, Danli Shi

Fundus fluorescein angiography (FFA) is critical for diagnosing retinal
vascular diseases, but beginners often struggle with image interpretation. This
study develops FFA Sora, a text-to-video model that converts FFA reports into
dynamic videos via a Wavelet-Flow Variational Autoencoder (WF-VAE) and a
diffusion transformer (DiT). Trained on an anonymized dataset, FFA Sora
accurately simulates disease features from the input text, as confirmed by
objective metrics: Frechet Video Distance (FVD) = 329.78, Learned Perceptual
Image Patch Similarity (LPIPS) = 0.48, and Visual-question-answering Score
(VQAScore) = 0.61. Specific evaluations showed acceptable alignment between the
generated videos and textual prompts, with BERTScore of 0.35. Additionally, the
model demonstrated strong privacy-preserving performance in retrieval
evaluations, achieving an average Recall@K of 0.073. Human assessments
indicated satisfactory visual quality, with an average score of 1.570(scale: 1
= best, 5 = worst). This model addresses privacy concerns associated with
sharing large-scale FFA data and enhances medical education.

摘要：眼底螢光血管攝影 (FFA) 對診斷視網膜血管疾病至關重要，但初學者常常難以解讀影像。本研究開發出 FFA Sora，一個文字轉影片模型，可透過小波流變動自編碼器 (WF-VAE) 和擴散轉換器 (DiT) 將 FFA 報告轉換成動態影片。FFA Sora 在匿名資料集上訓練，能準確模擬輸入文字中的疾病特徵，並由客觀指標證實：Fréchet 影片距離 (FVD) = 329.78、學習知覺影像區塊相似度 (LPIPS) = 0.48 和視覺問答分數 (VQAScore) = 0.61。特定評估顯示產生的影片和文字提示之間有可接受的一致性，BERTScore 為 0.35。此外，該模型在檢索評估中展現出強大的隱私保護效能，平均 Recall@K 達到 0.073。人類評估顯示令人滿意的視覺品質，平均分數為 1.570（量表：1 = 最佳，5 = 最差）。此模型解決了與分享大規模 FFA 資料相關的隱私疑慮，並增進醫學教育。

##### **MineAgent: Towards Remote-Sensing Mineral Exploration with Multimodal Large Language Models**
2412.17339v1 by Beibei Yu, Tao Shen, Hongbin Na, Ling Chen, Denqi Li

Remote-sensing mineral exploration is critical for identifying economically
viable mineral deposits, yet it poses significant challenges for multimodal
large language models (MLLMs). These include limitations in domain-specific
geological knowledge and difficulties in reasoning across multiple
remote-sensing images, further exacerbating long-context issues. To address
these, we present MineAgent, a modular framework leveraging hierarchical
judging and decision-making modules to improve multi-image reasoning and
spatial-spectral integration. Complementing this, we propose MineBench, a
benchmark specific for evaluating MLLMs in domain-specific mineral exploration
tasks using geological and hyperspectral data. Extensive experiments
demonstrate the effectiveness of MineAgent, highlighting its potential to
advance MLLMs in remote-sensing mineral exploration.

摘要：遙測礦物勘探對於辨識經濟上可行的礦藏至關重要，但它對多模態大型語言模型 (MLLM) 而言卻構成重大挑戰。這些挑戰包括特定領域地質知識的限制，以及在多個遙測影像中進行推理的困難，進一步加劇了長語境問題。為了解決這些問題，我們提出了 MineAgent，一個利用階層判斷和決策模組來改善多影像推理和空間光譜整合的模組化框架。作為補充，我們提出了 MineBench，一個專門用於評估 MLLM 在特定領域礦物勘探任務中使用地質和高光譜資料的基準。廣泛的實驗證明了 MineAgent 的有效性，突顯了它在遙測礦物勘探中推進 MLLM 的潛力。

##### **Enhancing Topic Interpretability for Neural Topic Modeling through Topic-wise Contrastive Learning**
2412.17338v1 by Xin Gao, Yang Lin, Ruiqing Li, Yasha Wang, Xu Chu, Xinyu Ma, Hailong Yu

Data mining and knowledge discovery are essential aspects of extracting
valuable insights from vast datasets. Neural topic models (NTMs) have emerged
as a valuable unsupervised tool in this field. However, the predominant
objective in NTMs, which aims to discover topics maximizing data likelihood,
often lacks alignment with the central goals of data mining and knowledge
discovery which is to reveal interpretable insights from large data
repositories. Overemphasizing likelihood maximization without incorporating
topic regularization can lead to an overly expansive latent space for topic
modeling. In this paper, we present an innovative approach to NTMs that
addresses this misalignment by introducing contrastive learning measures to
assess topic interpretability. We propose a novel NTM framework, named
ContraTopic, that integrates a differentiable regularizer capable of evaluating
multiple facets of topic interpretability throughout the training process. Our
regularizer adopts a unique topic-wise contrastive methodology, fostering both
internal coherence within topics and clear external distinctions among them.
Comprehensive experiments conducted on three diverse datasets demonstrate that
our approach consistently produces topics with superior interpretability
compared to state-of-the-art NTMs.

摘要：資料探勘與知識發現是從龐大資料集中萃取有價值見解的必要面向。神經主題模型 (NTM) 已成為此領域有價值的非監督式工具。然而，NTM 中的主要目標在於發現最大化資料機率的主題，這通常與資料探勘和知識發現的核心目標不一致，而後者旨在從大型資料儲存庫中揭示可詮釋的見解。過度強調機率極大化而不納入主題正規化，可能會導致主題建模的潛在空間過度擴張。在本文中，我們提出了一種創新的 NTM 方法，透過引入對比學習測量來評估主題可詮釋性，以解決這個不一致性。我們提出了一個名為 ContraTopic 的新穎 NTM 架構，它整合了一個可微分正規化器，能夠在整個訓練過程中評估主題可詮釋性的多個面向。我們的正規化器採用獨特的主題對比方法，既促進主題內部的內部一致性，又促進主題之間的外部區別。在三個不同的資料集上進行的全面實驗證明，與現有最先進的 NTM 相比，我們的做法持續產生可詮釋性更高的主題。

##### **Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition**
2412.17333v1 by Jaeheun Jung, Jaehyuk Lee, Chang-Hae Jung, Hanyoung Kim, Bosung Jung, Donghun Lee

Earthquakes are rare. Hence there is a fundamental call for reliable methods
to generate realistic ground motion data for data-driven approaches in
seismology. Recent GAN-based methods fall short of the call, as the methods
either require special information such as geological traits or generate subpar
waveforms that fail to satisfy seismological constraints such as phase arrival
times. We propose a specialized Latent Diffusion Model (LDM) that reliably
generates realistic waveforms after learning from real earthquake data with
minimal conditions: location and magnitude. We also design a domain-specific
training method that exploits the traits of earthquake dataset: multiple
observed waveforms time-aligned and paired to each earthquake source that are
tagged with seismological metadata comprised of earthquake magnitude, depth of
focus, and the locations of epicenter and seismometers. We construct the
time-aligned earthquake dataset using Southern California Earthquake Data
Center (SCEDC) API, and train our model with the dataset and our proposed
training method for performance evaluation. Our model surpasses all comparable
data-driven methods in various test criteria not only from waveform generation
domain but also from seismology such as phase arrival time, GMPE analysis, and
spectrum analysis. Our result opens new future research directions for deep
learning applications in seismology.

摘要：地震罕見。因此，迫切需要可靠的方法來生成逼真的地面運動數據，以用於地震學中的數據驅動方法。基於 GAN 的近期方法未能滿足要求，因為這些方法需要特殊信息（例如地質特徵）或生成無法滿足地震學約束（例如相位到達時間）的次級波形。我們提出了一種專門的潛在擴散模型 (LDM)，該模型在從真實地震數據中學習後，在最少的條件下（位置和震級）可靠地生成逼真的波形。我們還設計了一種特定於領域的訓練方法，該方法利用了地震數據集的特徵：多個觀測波形與每個地震源的時間對齊並配對，這些波形標記有地震學元數據，包括地震震級、震源深度以及震中和地震儀的位置。我們使用南加州地震數據中心 (SCEDC) API 構建了時間對齊的地震數據集，並使用該數據集和我們提出的訓練方法訓練我們的模型以進行性能評估。我們的模型在各種測試標準中超越了所有可比較的數據驅動方法，不僅來自波形生成領域，還來自地震學，例如相位到達時間、GMPE 分析和譜分析。我們的結果為地震學中的深度學習應用開闢了新的未來研究方向。

##### **A Dual-Perspective Metaphor Detection Framework Using Large Language Models**
2412.17332v1 by Yujie Lin, Jingyao Liu, Yan Gao, Ante Wang, Jinsong Su

Metaphor detection, a critical task in natural language processing, involves
identifying whether a particular word in a sentence is used metaphorically.
Traditional approaches often rely on supervised learning models that implicitly
encode semantic relationships based on metaphor theories. However, these
methods often suffer from a lack of transparency in their decision-making
processes, which undermines the reliability of their predictions. Recent
research indicates that LLMs (large language models) exhibit significant
potential in metaphor detection. Nevertheless, their reasoning capabilities are
constrained by predefined knowledge graphs. To overcome these limitations, we
propose DMD, a novel dual-perspective framework that harnesses both implicit
and explicit applications of metaphor theories to guide LLMs in metaphor
detection and adopts a self-judgment mechanism to validate the responses from
the aforementioned forms of guidance. In comparison to previous methods, our
framework offers more transparent reasoning processes and delivers more
reliable predictions. Experimental results prove the effectiveness of DMD,
demonstrating state-of-the-art performance across widely-used datasets.

摘要：隱喻偵測，在自然語言處理中是一項重要的任務，涉及辨識句子中特定字詞是否被隱喻使用。
傳統方法通常仰賴監督式學習模型，該模型會根據隱喻理論隱含編碼語意關係。
然而，這些方法通常在決策過程中缺乏透明度，這會損害其預測的可靠性。
最近的研究指出，LLM（大型語言模型）在隱喻偵測中展現出顯著的潛力。
儘管如此，其推理能力仍受到預先定義的知識圖表的限制。
為了克服這些限制，我們提出 DMD，一個新穎的雙重觀點架構，它利用隱喻理論的隱含和明確應用來引導 LLM 進行隱喻偵測，並採用自我判斷機制來驗證上述形式指導的回應。
與先前的模型相比，我們的架構提供了更透明的推理過程，並提供更可靠的預測。
實驗結果證明了 DMD 的有效性，在廣泛使用的資料集上展現出最先進的效能。

##### **xPatch: Dual-Stream Time Series Forecasting with Exponential Seasonal-Trend Decomposition**
2412.17323v1 by Artyom Stitsyuk, Jaesik Choi

In recent years, the application of transformer-based models in time-series
forecasting has received significant attention. While often demonstrating
promising results, the transformer architecture encounters challenges in fully
exploiting the temporal relations within time series data due to its attention
mechanism. In this work, we design eXponential Patch (xPatch for short), a
novel dual-stream architecture that utilizes exponential decomposition.
Inspired by the classical exponential smoothing approaches, xPatch introduces
the innovative seasonal-trend exponential decomposition module. Additionally,
we propose a dual-flow architecture that consists of an MLP-based linear stream
and a CNN-based non-linear stream. This model investigates the benefits of
employing patching and channel-independence techniques within a non-transformer
model. Finally, we develop a robust arctangent loss function and a sigmoid
learning rate adjustment scheme, which prevent overfitting and boost
forecasting performance. The code is available at the following repository:
https://github.com/stitsyuk/xPatch.

摘要：近年來，基於 Transformer 的模型在時間序列預測中的應用備受關注。儘管通常展示出有希望的結果，但 Transformer 架構在充分利用時間序列資料中的時間關係方面遇到了挑戰，這是因為它的注意力機制。在這項工作中，我們設計了指數貼片（簡稱 xPatch），一種利用指數分解的新型雙流架構。受經典指數平滑方法的啟發，xPatch 引入了創新的季節趨勢指數分解模組。此外，我們提出了一種雙流架構，它包含一個基於 MLP 的線性流和一個基於 CNN 的非線性流。這個模型探討了在非 Transformer 模型中採用貼片和通道獨立技術的好處。最後，我們開發了一個穩健的反正切損失函數和一個 sigmoid 學習率調整方案，它們可以防止過度擬合並提升預測性能。程式碼可在以下儲存庫中取得：https://github.com/stitsyuk/xPatch。

##### **Assessing Human Editing Effort on LLM-Generated Texts via Compression-Based Edit Distance**
2412.17321v1 by Nicolas Devatine, Louis Abraham

Assessing the extent of human edits on texts generated by Large Language
Models (LLMs) is crucial to understanding the human-AI interactions and
improving the quality of automated text generation systems. Existing edit
distance metrics, such as Levenshtein, BLEU, ROUGE, and TER, often fail to
accurately measure the effort required for post-editing, especially when edits
involve substantial modifications, such as block operations. In this paper, we
introduce a novel compression-based edit distance metric grounded in the
Lempel-Ziv-77 algorithm, designed to quantify the amount of post-editing
applied to LLM-generated texts. Our method leverages the properties of text
compression to measure the informational difference between the original and
edited texts. Through experiments on real-world human edits datasets, we
demonstrate that our proposed metric is highly correlated with actual edit time
and effort. We also show that LLMs exhibit an implicit understanding of editing
speed, that aligns well with our metric. Furthermore, we compare our metric
with existing ones, highlighting its advantages in capturing complex edits with
linear computational efficiency. Our code and data are available at:
https://github.com/NDV-tiime/CompressionDistance

摘要：評估大型語言模型 (LLM) 所產生文字上的人工編輯程度，對於理解人機互動以及提升自動化文字產生系統的品質至關重要。現有的編輯距離量度，例如 Levenshtein、BLEU、ROUGE 和 TER，通常無法準確測量後編輯所需的功夫，特別是在編輯涉及大量修改（例如區塊操作）的情況下。在本文中，我們引入一種新的基於壓縮的編輯距離量度，其基礎在於 Lempel-Ziv-77 演算法，旨在量化應用於 LLM 所產生文字的後編輯量。我們的做法利用文字壓縮的特性來測量原始文字和已編輯文字之間的資訊差異。透過對真實世界人工編輯資料集的實驗，我們證明所提出的量度與實際編輯時間和功夫高度相關。我們也顯示 LLM 展現對編輯速度的隱含理解，這與我們的量度非常吻合。此外，我們將我們的量度與現有量度進行比較，強調其在以線性運算效率擷取複雜編輯方面的優勢。我們的程式碼和資料可在以下網址取得：https://github.com/NDV-tiime/CompressionDistance

##### **Fast Gradient Computation for RoPE Attention in Almost Linear Time**
2412.17316v1 by Yifang Chen, Jiayan Huo, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song

The Rotary Position Embedding (RoPE) mechanism has become a powerful
enhancement to the Transformer architecture, which enables models to capture
token relationships when encoding positional information. However, the RoPE
mechanisms make the computations of attention mechanisms more complicated,
which makes efficient algorithms challenging. Earlier research introduced
almost linear time, i.e., $n^{1+o(1)}$ where $n$ is the number of input tokens,
algorithms for the forward computation under specific parameter settings.
However, achieving a subquadratic time algorithm for other parameter regimes
remains impossible unless the widely accepted Strong Exponential Time
Hypothesis (SETH) is disproven. In this work, we develop the first almost
linear time algorithm for backward computations in the RoPE-based attention
under bounded entries. Our approach builds on recent advancements in fast RoPE
attention computations, utilizing a novel combination of the polynomial method
and the Fast Fourier Transform. Furthermore, we show that with lower bounds
derived from the SETH, the bounded entry condition is necessary for
subquadratic performance.

摘要：旋轉位置嵌入 (RoPE) 機制已成為 Transformer 架構的一項強大強化功能，它能讓模型在編碼位置資訊時擷取標記關聯。然而，RoPE 機制讓注意力機制的運算更為複雜，這使得高效演算法充滿挑戰。稍早的研究引入了近似線性時間，即 $n^{1+o(1)}$，其中 $n$ 是輸入標記的數量，演算法用於在特定參數設定下的前向運算。然而，除非廣泛接受的強指數時間假設 (SETH) 被證偽，否則無法為其他參數範圍達成次平方時間演算法。在這項工作中，我們開發出第一個近似線性時間演算法，用於在有界輸入下的 RoPE 基礎注意力中進行後向運算。我們的做法建立在快速 RoPE 注意力運算的最新進展上，利用多項式方法和快速傅立葉轉換的新穎組合。此外，我們證明，根據 SETH 推導出的下界，有界輸入條件對於次平方效能而言是必要的。

##### **CodeV: Issue Resolving with Visual Data**
2412.17315v1 by Linhao Zhang, Daoguang Zan, Quanshun Yang, Zhirong Huang, Dong Chen, Bo Shen, Tianyu Liu, Yongshun Gong, Pengjie Huang, Xudong Lu, Guangtai Liang, Lizhen Cui, Qianxiang Wang

Large Language Models (LLMs) have advanced rapidly in recent years, with
their applications in software engineering expanding to more complex
repository-level tasks. GitHub issue resolving is a key challenge among these
tasks. While recent approaches have made progress on this task, they focus on
textual data within issues, neglecting visual data. However, this visual data
is crucial for resolving issues as it conveys additional knowledge that text
alone cannot. We propose CodeV, the first approach to leveraging visual data to
enhance the issue-resolving capabilities of LLMs. CodeV resolves each issue by
following a two-phase process: data processing and patch generation. To
evaluate CodeV, we construct a benchmark for visual issue resolving, namely
Visual SWE-bench. Through extensive experiments, we demonstrate the
effectiveness of CodeV, as well as provide valuable insights into leveraging
visual data to resolve GitHub issues.

摘要：大型語言模型 (LLM) 近年來進步神速，其在軟體工程中的應用已擴展到更複雜的儲存庫層級任務。GitHub 問題解決是這些任務中的關鍵挑戰。雖然最近的方法已針對此任務取得進展，但它們專注於問題中的文字資料，而忽略了視覺資料。然而，此視覺資料對於解決問題至關重要，因為它傳達了單靠文字無法傳達的額外知識。我們提出 CodeV，這是第一個利用視覺資料來增強 LLM 問題解決能力的方法。CodeV 透過遵循資料處理和修補程式產生這兩個階段的流程來解決每個問題。為了評估 CodeV，我們針對視覺問題解決建立了一個基準，即 Visual SWE-bench。透過廣泛的實驗，我們證明了 CodeV 的有效性，並提供了利用視覺資料解決 GitHub 問題的寶貴見解。

##### **On the Feasibility of Vision-Language Models for Time-Series Classification**
2412.17304v1 by Vinay Prithyani, Mohsin Mohammed, Richa Gadgil, Ricardo Buitrago, Vinija Jain, Aman Chadha

We build upon time-series classification by leveraging the capabilities of
Vision Language Models (VLMs). We find that VLMs produce competitive results
after two or less epochs of fine-tuning. We develop a novel approach that
incorporates graphical data representations as images in conjunction with
numerical data. This approach is rooted in the hypothesis that graphical
representations can provide additional contextual information that numerical
data alone may not capture. Additionally, providing a graphical representation
can circumvent issues such as limited context length faced by LLMs. To further
advance this work, we implemented a scalable end-to-end pipeline for training
on different scenarios, allowing us to isolate the most effective strategies
for transferring learning capabilities from LLMs to Time Series Classification
(TSC) tasks. Our approach works with univariate and multivariate time-series
data. In addition, we conduct extensive and practical experiments to show how
this approach works for time-series classification and generative labels.

摘要：我們透過利用視覺語言模型 (VLM) 的功能，建立在時間序列分類之上。我們發現 VLM 在經過兩次或更少次微調時期後，會產生具有競爭力的結果。我們開發出一種新穎的方法，將圖形數據表示法作為圖像與數值數據結合在一起。此方法植基於圖形表示法可以提供數值數據本身可能無法擷取的額外脈絡資訊的假設。此外，提供圖形表示法可以迴避 LLM 所面臨的有限脈絡長度等問題。為了進一步推進這項工作，我們實作了一個可擴充的端對端管線，用於針對不同的場景進行訓練，這讓我們能夠找出最有效的策略，將學習能力從 LLM 轉移到時間序列分類 (TSC) 任務。我們的做法適用於單變量和多變量時間序列數據。此外，我們進行廣泛且實用的實驗，以展示這種方法如何用於時間序列分類和生成式標籤。

##### **Dynamic Scheduling Strategies for Resource Optimization in Computing Environments**
2412.17301v1 by Xiaoye Wang

The rapid development of cloud-native architecture has promoted the
widespread application of container technology, but the optimization problems
in container scheduling and resource management still face many challenges.
This paper proposes a container scheduling method based on multi-objective
optimization, which aims to balance key performance indicators such as resource
utilization, load balancing and task completion efficiency. By introducing
optimization models and heuristic algorithms, the scheduling strategy is
comprehensively improved, and experimental verification is carried out using
the real Google Cluster Data dataset. The experimental results show that
compared with traditional static rule algorithms and heuristic algorithms, the
optimized scheduling scheme shows significant advantages in resource
utilization, load balancing and burst task completion efficiency. This shows
that the proposed method can effectively improve resource management efficiency
and ensure service quality and system stability in complex dynamic cloud
environments. At the same time, this paper also explores the future development
direction of scheduling algorithms in multi-tenant environments, heterogeneous
cloud computing, and cross-edge and cloud collaborative computing scenarios,
and proposes research prospects for energy consumption optimization, adaptive
scheduling and fairness. The research results not only provide a theoretical
basis and practical reference for container scheduling under cloud-native
architecture, but also lay a foundation for further realizing intelligent and
efficient resource management.

摘要：雲原生架構的快速發展促进了容器技術的廣泛應用，但容器調度和資源管理中的優化問題仍面臨諸多挑戰。本文提出了一種基於多目標優化的容器調度方法，旨在平衡資源利用率、負載均衡和任務完成效率等關鍵性能指標。通過引入優化模型和啟發式演算法，對調度策略進行了全面的改進，並使用真實的 Google Cluster Data 資料集進行了實驗驗證。實驗結果表明，與傳統的靜態規則演算法和啟發式演算法相比，優化的調度方案在資源利用率、負載均衡和突發任務完成效率方面表現出顯著優勢。這表明所提出的方法可以有效提高資源管理效率，並確保複雜動態雲環境中的服務品質和系統穩定性。同時，本文還探討了多租戶環境、異構雲計算、邊緣雲協同計算場景下調度演算法的未來發展方向，並提出了能耗優化、自適應調度和公平性的研究展望。研究成果不僅為雲原生架構下的容器調度提供了理論依據和實踐參考，也為進一步實現智慧高效的資源管理奠定了基礎。

##### **Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding**
2412.17295v1 by Yueqian Wang, Xiaojun Meng, Yuxuan Wang, Jianxin Liang, Qun Liu, Dongyan Zhao

Multi-modal multi-party conversation (MMC) is a less studied yet important
topic of research due to that it well fits real-world scenarios and thus
potentially has more widely-used applications. Compared with the traditional
multi-modal conversations, MMC requires stronger character-centered
understanding abilities as there are many interlocutors appearing in both the
visual and textual context. To facilitate the study of this problem, we present
Friends-MMC in this paper, an MMC dataset that contains 24,000+ unique
utterances paired with video context. To explore the character-centered
understanding of the dialogue, we also annotate the speaker of each utterance,
the names and bounding bboxes of faces that appear in the video. Based on this
Friends-MMC dataset, we further study two fundamental MMC tasks: conversation
speaker identification and conversation response prediction, both of which have
the multi-party nature with the video or image as visual context. For
conversation speaker identification, we demonstrate the inefficiencies of
existing methods such as pre-trained models, and propose a simple yet effective
baseline method that leverages an optimization solver to utilize the context of
two modalities to achieve better performance. For conversation response
prediction, we fine-tune generative dialogue models on Friend-MMC, and analyze
the benefits of speaker information. The code and dataset is publicly available
at https://github.com/yellow-binary-tree/Friends-MMC and thus we call for more
attention on modeling speaker information when understanding conversations.

摘要：多模态多方對話 (MMC) 是一個研究較少但重要的研究主題，因為它很符合現實世界的場景，因此潛在有更廣泛的應用。與傳統的多模態對話相比，MMC 需要更強的人物為中心理解能力，因為在視覺和文本語境中出現許多對話者。為了促進對這個問題的研究，我們在本文中提出 Friends-MMC，一個包含 24,000 多個與影片語境配對的獨特話語的 MMC 資料集。為了探索對話的人物為中心理解，我們也註解每個話語的說話者、影片中出現的臉部名稱和邊界框。基於這個 Friends-MMC 資料集，我們進一步研究兩個基本的 MMC 任務：對話說話者識別和對話回應預測，這兩個任務都具有多方性質，影片或影像作為視覺語境。對於對話說話者識別，我們展示現有方法（例如預先訓練的模型）的低效率，並提出一個簡單但有效的基本方法，該方法利用最佳化求解器來利用兩個模態的語境以達成更好的效能。對於對話回應預測，我們在 Friend-MMC 上微調生成式對話模型，並分析說話者資訊的好處。程式碼和資料集在 https://github.com/yellow-binary-tree/Friends-MMC 公開提供，因此我們呼籲在理解對話時更注意對說話者資訊的建模。

##### **AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues**
2412.17292v1 by Se Jin Park, Yeonju Kim, Hyeongseop Rha, Bella Godiva, Yong Man Ro

In human communication, both verbal and non-verbal cues play a crucial role
in conveying emotions, intentions, and meaning beyond words alone. These
non-linguistic information, such as facial expressions, eye contact, voice
tone, and pitch, are fundamental elements of effective interactions, enriching
conversations by adding emotional and contextual depth. Recognizing the
importance of non-linguistic content in communication, we present AV-EmoDialog,
a dialogue system designed to exploit verbal and non-verbal information from
users' audio-visual inputs to generate more responsive and empathetic
interactions. AV-EmoDialog systematically exploits the emotional cues in
audio-visual dialogues; extracting speech content and emotional tones from
speech, analyzing fine-grained facial expressions from visuals, and integrating
these cues to generate emotionally aware responses in an end-to-end manner.
Through extensive experiments, we validate that the proposed AV-EmoDialog
outperforms existing multimodal LLMs in generating not only emotionally
appropriate but also contextually appropriate responses.

摘要：在人類溝通中，言語和非言語線索在傳達情緒、意圖和意義方面都扮演著至關重要的角色，而這些意義並非言語本身所能傳達的。這些非語言資訊，例如面部表情、眼神接觸、語調和音高，是有效互動的基本要素，它們通過增加情緒和語境深度來豐富對話。認識到非語言內容在溝通中的重要性，我們提出了 AV-EmoDialog，這是一個對話系統，旨在利用使用者音訊視訊輸入中的言語和非言語資訊，以產生更具回應性和同理心的互動。AV-EmoDialog 系統性地利用音訊視訊對話中的情緒線索；從語音中提取語音內容和情緒語調，從視覺中分析細緻的面部表情，並將這些線索整合起來，以端到端的方式產生具有情緒感知的回應。通過廣泛的實驗，我們驗證了所提出的 AV-EmoDialog 在產生不僅在情緒上適當，而且在語境上適當的回應方面，優於現有的多模態 LLM。

##### **Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with A Few Examples**
2412.17288v1 by Taewoong Kim, Byeonghwi Kim, Jonghyun Choi

Learning a perception and reasoning module for robotic assistants to plan
steps to perform complex tasks based on natural language instructions often
requires large free-form language annotations, especially for short high-level
instructions. To reduce the cost of annotation, large language models (LLMs)
are used as a planner with few data. However, when elaborating the steps, even
the state-of-the-art planner that uses LLMs mostly relies on linguistic common
sense, often neglecting the status of the environment at command reception,
resulting in inappropriate plans. To generate plans grounded in the
environment, we propose FLARE (Few-shot Language with environmental Adaptive
Replanning Embodied agent), which improves task planning using both language
command and environmental perception. As language instructions often contain
ambiguities or incorrect expressions, we additionally propose to correct the
mistakes using visual cues from the agent. The proposed scheme allows us to use
a few language pairs thanks to the visual cues and outperforms state-of-the-art
approaches. Our code is available at https://github.com/snumprlab/flare.

摘要：學習一個知覺和推理模組，讓機器人助理根據自然語言指令，規劃執行複雜任務的步驟，通常需要大量的自由形式語言註解，尤其是對於簡短的高階指令。為了降低註解的成本，大型語言模型 (LLM) 被用作具有少量資料的規劃器。然而，在詳細說明步驟時，即使是使用 LLM 的最先進規劃器，也大多依賴於語言常識，常常忽略在接收命令時的環境狀態，導致不適當的計畫。為了產生以環境為基礎的計畫，我們提出 FLARE（少次語言與環境適應性重新規劃具身代理），它使用語言命令和環境感知來改善任務規劃。由於語言指令通常包含歧義或不正確的表達，我們另外提出使用來自代理的視覺線索來更正錯誤。所提出的方案允許我們使用少量的語言對，這要歸功於視覺線索，並且優於最先進的方法。我們的程式碼可在 https://github.com/snumprlab/flare 取得。

##### **LLM4AD: A Platform for Algorithm Design with Large Language Model**
2412.17287v1 by Fei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li, Xi Lin, Zhenkun Wang, Zhichao Lu, Qingfu Zhang

We introduce LLM4AD, a unified Python platform for algorithm design (AD) with
large language models (LLMs). LLM4AD is a generic framework with modularized
blocks for search methods, algorithm design tasks, and LLM interface. The
platform integrates numerous key methods and supports a wide range of algorithm
design tasks across various domains including optimization, machine learning,
and scientific discovery. We have also designed a unified evaluation sandbox to
ensure a secure and robust assessment of algorithms. Additionally, we have
compiled a comprehensive suite of support resources, including tutorials,
examples, a user manual, online resources, and a dedicated graphical user
interface (GUI) to enhance the usage of LLM4AD. We believe this platform will
serve as a valuable tool for fostering future development in the merging
research direction of LLM-assisted algorithm design.

摘要：我們介紹 LLM4AD，一個統一的 Python 平台，用於演算法設計 (AD) 和大型語言模型 (LLM)。LLM4AD 是個通用架構，具有模組化區塊，用於搜尋方法、演算法設計任務和 LLM 介面。該平台整合了許多關鍵方法，並支援各種領域的演算法設計任務，包括最佳化、機器學習和科學發現。我們還設計了一個統一的評估沙盒，以確保演算法的安全且穩健的評估。此外，我們編制了一套全面的支援資源，包括教學課程、範例、使用者手冊、線上資源和專用的圖形使用者介面 (GUI)，以增強 LLM4AD 的使用。我們相信這個平台將成為促進 LLM 輔助演算法設計合併研究方向未來發展的寶貴工具。

##### **Enabling Time-series Foundation Model for Building Energy Forecasting via Contrastive Curriculum Learning**
2412.17285v1 by Rui Liang, Yang Deng, Donghua Xie, Fang He, Dan Wang

Advances in time-series forecasting are driving a shift from conventional
machine learning models to foundation models (FMs) that are trained with
generalized knowledge. However, existing FMs still perform poorly in the energy
fields, such as building energy forecasting (BEF). This paper studies the
adaptation of FM to BEF tasks. We demonstrate the shortcomings of fine-tuning
FM straightforwardly from both the perspectives of FM and the data. To overcome
these limitations, we propose a new \textit{contrastive curriculum
learning}-based training method. Our method optimizes the ordering of training
data in the context of TSFM adaptation. Experiments show that our method can
improve the zero/few-shot performance by 14.6\% compared to the existing FMs.
Our code and new TSFM will be available at <Anonymous Github Repo>.

摘要：時序預測的進步正推動傳統機器學習模型轉向以廣泛知識訓練的基礎模型 (FM)。然而，現有的 FM 在能源領域的表現仍然不佳，例如建築能源預測 (BEF)。本文研究了 FM 適應 BEF 任務。我們從 FM 和數據的角度展示了微調 FM 的缺點。為了克服這些限制，我們提出了一種新的基於對比課程學習的訓練方法。我們的模型優化了 TSFM 適應過程中訓練數據的排序。實驗表明，與現有的 FM 相比，我們的模型可以將零/少次學習的表現提高 14.6%。我們的程式碼和新的 TSFM 將在 <匿名 Github Repo> 中提供。

##### **Learning from Mistakes: Self-correct Adversarial Training for Chinese Unnatural Text Correction**
2412.17279v1 by Xuan Feng, Tianlong Gu, Xiaoli Liu, Liang Chang

Unnatural text correction aims to automatically detect and correct spelling
errors or adversarial perturbation errors in sentences. Existing methods
typically rely on fine-tuning or adversarial training to correct errors, which
have achieved significant success. However, these methods exhibit poor
generalization performance due to the difference in data distribution between
training data and real-world scenarios, known as the exposure bias problem. In
this paper, we propose a self-correct adversarial training framework for
\textbf{L}earn\textbf{I}ng from \textbf{MI}s\textbf{T}akes (\textbf{LIMIT}),
which is a task- and model-independent framework to correct unnatural errors or
mistakes. Specifically, we fully utilize errors generated by the model that are
actively exposed during the inference phase, i.e., predictions that are
inconsistent with the target. This training method not only simulates potential
errors in real application scenarios, but also mitigates the exposure bias of
the traditional training process. Meanwhile, we design a novel decoding
intervention strategy to maintain semantic consistency. Extensive experimental
results on Chinese unnatural text error correction datasets show that our
proposed method can correct multiple forms of errors and outperforms the
state-of-the-art text correction methods. In addition, extensive results on
Chinese and English datasets validate that LIMIT can serve as a plug-and-play
defense module and can extend to new models and datasets without further
training.

摘要：非自然文本修正旨在自動偵測並修正句子中的拼寫錯誤或對抗性擾動錯誤。現有方法通常依賴微調或對抗性訓練來修正錯誤，這已取得重大成功。然而，由於訓練資料與真實世界場景之間的資料分佈差異，這些方法展現出差勁的泛化效能，這稱為曝光偏差問題。在本文中，我們提出一個用於從錯誤中學習的自修正對抗性訓練架構（LIMIT），這是一個與任務和模型無關的架構，用於修正非自然錯誤或失誤。具體來說，我們充分利用模型在推論階段主動暴露的錯誤，也就是與目標不一致的預測。這種訓練方法不僅模擬真實應用場景中的潛在錯誤，還減輕傳統訓練流程的曝光偏差。同時，我們設計一種新穎的解碼介入策略來維持語義一致性。在中文非自然文本錯誤修正資料集上的廣泛實驗結果顯示，我們提出的方法可以修正多種形式的錯誤，並且優於最先進的文本修正方法。此外，在中文和英文資料集上的廣泛結果驗證了 LIMIT 可以作為即插即用防禦模組，並且可以在不進一步訓練的情況下擴展到新的模型和資料集。

##### **LegalAgentBench: Evaluating LLM Agents in Legal Domain**
2412.17259v1 by Haitao Li, Junjie Chen, Jingli Yang, Qingyao Ai, Wei Jia, Youfeng Liu, Kai Lin, Yueyue Wu, Guozhi Yuan, Yiran Hu, Wuyue Wang, Yiqun Liu, Minlie Huang

With the increasing intelligence and autonomy of LLM agents, their potential
applications in the legal domain are becoming increasingly apparent. However,
existing general-domain benchmarks cannot fully capture the complexity and
subtle nuances of real-world judicial cognition and decision-making. Therefore,
we propose LegalAgentBench, a comprehensive benchmark specifically designed to
evaluate LLM Agents in the Chinese legal domain. LegalAgentBench includes 17
corpora from real-world legal scenarios and provides 37 tools for interacting
with external knowledge. We designed a scalable task construction framework and
carefully annotated 300 tasks. These tasks span various types, including
multi-hop reasoning and writing, and range across different difficulty levels,
effectively reflecting the complexity of real-world legal scenarios. Moreover,
beyond evaluating final success, LegalAgentBench incorporates keyword analysis
during intermediate processes to calculate progress rates, enabling more
fine-grained evaluation. We evaluated eight popular LLMs, highlighting the
strengths, limitations, and potential areas for improvement of existing models
and methods. LegalAgentBench sets a new benchmark for the practical application
of LLMs in the legal domain, with its code and data available at
\url{https://github.com/CSHaitao/LegalAgentBench}.

摘要：隨著 LLM 代理的智慧與自主性日益提升，它們在法律領域的潛在應用也變得越來越明顯。然而，現有的通用領域基準無法完全捕捉到現實世界司法認知和決策的複雜性和微妙差別。因此，我們提出 LegalAgentBench，一個專門設計用於評估 LLM 代理在中國法律領域的綜合基準。LegalAgentBench 包含來自現實世界法律場景的 17 個語料庫，並提供 37 個與外部知識互動的工具。我們設計了一個可擴充的任務建構框架，並仔細標註了 300 個任務。這些任務涵蓋各種類型，包括多跳推理和寫作，並跨越不同的難度等級，有效地反映了現實世界法律場景的複雜性。此外，除了評估最終成功之外，LegalAgentBench 還結合了中間過程中的關鍵字分析來計算進度率，從而實現更精細的評估。我們評估了八個流行的 LLM，突出了現有模型和方法的優勢、限制和潛在改進領域。LegalAgentBench 為 LLM 在法律領域的實際應用設定了一個新的基準，其程式碼和資料可在 \url{https://github.com/CSHaitao/LegalAgentBench} 取得。

##### **B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners**
2412.17256v1 by Weihao Zeng, Yuzhen Huang, Lulu Zhao, Yijun Wang, Zifei Shan, Junxian He

In the absence of extensive human-annotated data for complex reasoning tasks,
self-improvement -- where models are trained on their own outputs -- has
emerged as a primary method for enhancing performance. However, the critical
factors underlying the mechanism of these iterative self-improving methods
remain poorly understood, such as under what conditions self-improvement is
effective, and what are the bottlenecks in the current iterations. In this
work, we identify and propose methods to monitor two pivotal factors in this
iterative process: (1) the model's ability to generate sufficiently diverse
responses (exploration); and (2) the effectiveness of external rewards in
distinguishing high-quality candidates from lower-quality ones (exploitation).
Using mathematical reasoning as a case study, we begin with a quantitative
analysis to track the dynamics of exploration and exploitation, discovering
that a model's exploratory capabilities rapidly deteriorate over iterations,
and the effectiveness of exploiting external rewards diminishes as well.
Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning
framework that autonomously adjusts configurations across iterations to Balance
exploration and exploitation, thereby optimizing the self-improving
effectiveness based on the current policy model and available rewards. Our
experiments on mathematical reasoning, coding, and commonsense reasoning
demonstrate that B-STaR not only enhances the model's exploratory capabilities
throughout training but also achieves a more effective balance between
exploration and exploitation, leading to superior performance.

摘要：在缺乏廣泛的人類標註資料的情況下，複雜的推理任務，
自我提升——模型根據自己的輸出進行訓練——已成為增強效能的主要方法。然而，這些迭代自我提升方法機制背後的重要因素仍然知之甚少，例如在什麼條件下自我提升有效，以及當前迭代的瓶頸是什麼。在這項工作中，我們找出並提出方法來監控此迭代過程中兩個關鍵因素：(1) 模型產生足夠多樣化回應的能力（探索）；以及 (2) 外部獎勵在區分高品質候選者和低品質候選者方面的有效性（利用）。使用數學推理作為案例研究，我們從定量分析開始，以追蹤探索和利用的動態，發現模型的探索能力在迭代過程中迅速惡化，利用外部獎勵的有效性也隨之降低。受到這些發現的啟發，我們引入了 B-STaR，一個自我學習推理框架，它可以自主調整配置以平衡探索和利用，從而根據當前的策略模型和可用獎勵來優化自我提升的有效性。我們在數學推理、編碼和常識推理方面的實驗表明，B-STaR 不僅增強了模型在整個訓練過程中的探索能力，而且在探索和利用之間達到了更有效的平衡，從而帶來了卓越的效能。

##### **Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation: A Multimodal Generative AI Approach**
2412.17255v1 by Rafid Ishrak Jahan, Heng Fan, Haihua Chen, Yunhe Feng

Emojis have become ubiquitous in online communication, serving as a universal
medium to convey emotions and decorative elements. Their widespread use
transcends language and cultural barriers, enhancing understanding and
fostering more inclusive interactions. While existing work gained valuable
insight into emojis understanding, exploring emojis' capability to serve as a
universal sentiment indicator leveraging large language models (LLMs) has not
been thoroughly examined. Our study aims to investigate the capacity of emojis
to serve as reliable sentiment markers through LLMs across languages and
cultures. We leveraged the multimodal capabilities of ChatGPT to explore the
sentiments of various representations of emojis and evaluated how well
emoji-conveyed sentiment aligned with text sentiment on a multi-lingual dataset
collected from 32 countries. Our analysis reveals that the accuracy of
LLM-based emoji-conveyed sentiment is 81.43%, underscoring emojis' significant
potential to serve as a universal sentiment marker. We also found a consistent
trend that the accuracy of sentiment conveyed by emojis increased as the number
of emojis grew in text. The results reinforce the potential of emojis to serve
as global sentiment indicators, offering insight into fields such as
cross-lingual and cross-cultural sentiment analysis on social media platforms.
Code: https://github.com/ResponsibleAILab/emoji-universal-sentiment.

摘要：表情符號已在網路溝通中無處不在，作為傳達情緒和裝飾元素的通用媒介。它們廣泛的使用超越了語言和文化障礙，增強了理解並促進了更具包容性的互動。雖然現有工作對表情符號的理解獲得了寶貴的見解，但探索表情符號作為通用情緒指標的能力，利用大型語言模型 (LLM) 尚未得到徹底檢驗。我們的研究旨在調查表情符號通過跨語言和文化的 LLM 作為可靠情緒標記的能力。我們利用 ChatGPT 的多模態能力來探索表情符號的各種表示形式的情緒，並評估表情符號傳達的情緒在從 32 個國家/地區收集的多語言資料集上與文字情緒的吻合程度。我們的分析表明，基於 LLM 的表情符號傳達情緒的準確度為 81.43%，強調了表情符號作為通用情緒標記的巨大潛力。我們還發現了一個一致的趨勢，即隨著文本中表情符號數量增加，表情符號傳達的情緒準確度也會提高。結果強化了表情符號作為全球情緒指標的潛力，為跨語言和跨文化社交媒體平台上的情緒分析等領域提供了見解。程式碼：https://github.com/ResponsibleAILab/emoji-universal-sentiment。

##### **Enhancing Multi-Text Long Video Generation Consistency without Tuning: Time-Frequency Analysis, Prompt Alignment, and Theory**
2412.17254v1 by Xingyao Li, Fengzhuo Zhang, Jiachun Pan, Yunlong Hou, Vincent Y. F. Tan, Zhuoran Yang

Despite the considerable progress achieved in the long video generation
problem, there is still significant room to improve the consistency of the
videos, particularly in terms of smoothness and transitions between scenes. We
address these issues to enhance the consistency and coherence of videos
generated with either single or multiple prompts. We propose the Time-frequency
based temporal Attention Reweighting Algorithm (TiARA), which meticulously
edits the attention score matrix based on the Discrete Short-Time Fourier
Transform. Our method is supported by a theoretical guarantee, the
first-of-its-kind for frequency-based methods in diffusion models. For videos
generated by multiple prompts, we further investigate key factors affecting
prompt interpolation quality and propose PromptBlend, an advanced prompt
interpolation pipeline. The efficacy of our proposed method is validated via
extensive experimental results, exhibiting consistent and impressive
improvements over baseline methods. The code will be released upon acceptance.

摘要：儘管長影片生成問題已取得顯著進展，但影片的一致性仍有很大的進步空間，特別是在場景間的順暢度和轉場方面。我們解決這些問題，以增強使用單一或多個提示所生成影片的一致性和連貫性。我們提出基於時頻的時序注意力重新加權演算法 (TiARA)，它根據離散短時距傅立葉轉換，仔細編輯注意力分數矩陣。我們的演算法受到理論保證的支持，這是擴散模型中基於頻率的方法中首創的。對於由多個提示生成的影片，我們進一步探討影響提示內插品質的主要因素，並提出 PromptBlend，一種進階提示內插管道。我們提出的方法的效能已透過廣泛的實驗結果驗證，展示出與基準方法相比，一致且令人印象深刻的改進。程式碼將在接受後釋出。

##### **On the Generalization Ability of Machine-Generated Text Detectors**
2412.17242v1 by Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, Xinlei He

The rise of large language models (LLMs) has raised concerns about
machine-generated text (MGT), including ethical and practical issues like
plagiarism and misinformation. Building a robust and highly generalizable MGT
detection system has become increasingly important. This work investigates the
generalization capabilities of MGT detectors in three aspects: First, we
construct MGTAcademic, a large-scale dataset focused on academic writing,
featuring human-written texts (HWTs) and MGTs across STEM, Humanities, and
Social Sciences, paired with an extensible code framework for efficient
benchmarking. Second, we investigate the transferability of detectors across
domains and LLMs, leveraging fine-grained datasets to reveal insights into
domain transferring and implementing few-shot techniques to improve the
performance by roughly 13.2%. Third, we introduce a novel attribution task
where models must adapt to new classes over time without (or with very limited)
access to prior training data and benchmark detectors. We implement several
adapting techniques to improve the performance by roughly 10% and highlight the
inherent complexity of the task. Our findings provide insights into the
generalization ability of MGT detectors across diverse scenarios and lay the
foundation for building robust, adaptive detection systems.

摘要：大型語言模型 (LLM) 的興起引發了對機器產生的文本 (MGT) 的擔憂，包括抄襲和錯誤資訊等道德和實務問題。建構一個強健且高度可概括的 MGT 偵測系統變得越來越重要。這項研究從三個方面探討 MGT 偵測器的概括能力：首先，我們建構了 MGTAcademic，一個專注於學術寫作的大規模資料集，其中包含 STEM、人文學科和社會科學領域的人類撰寫文本 (HWT) 和 MGT，並配備可延伸的程式碼架構以進行有效基準測試。其次，我們探討偵測器在不同領域和 LLM 之間的可轉移性，利用細粒度的資料集揭露領域轉移的見解，並實作少量樣本技術，將效能提升約 13.2%。第三，我們引入一項新穎的歸因任務，其中模型必須隨著時間適應新類別，而不會（或僅能非常有限地）存取先前的訓練資料和基準偵測器。我們實作了多種適應技術，將效能提升約 10%，並強調了這項任務的內在複雜性。我們的研究結果提供了對 MGT 偵測器在不同場景中的概括能力的見解，並為建構強健的適應性偵測系統奠定了基礎。

