{"2502.01158": {"publish_time": "2025-02-03", "title": "MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks", "paper_summary": "Multimodal fusion leverages information across modalities to learn better\nfeature representations with the goal of improving performance in fusion-based\ntasks. However, multimodal datasets, especially in medical settings, are\ntypically smaller than their unimodal counterparts, which can impede the\nperformance of multimodal models. Additionally, the increase in the number of\nmodalities is often associated with an overall increase in the size of the\nmultimodal network, which may be undesirable in medical use cases. Utilizing\nsmaller unimodal encoders may lead to sub-optimal performance, particularly\nwhen dealing with high-dimensional clinical data. In this paper, we propose the\nModality-INformed knowledge Distillation (MIND) framework, a multimodal model\ncompression approach based on knowledge distillation that transfers knowledge\nfrom ensembles of pre-trained deep neural networks of varying sizes into a\nsmaller multimodal student. The teacher models consist of unimodal networks,\nallowing the student to learn from diverse representations. MIND employs\nmulti-head joint fusion models, as opposed to single-head models, enabling the\nuse of unimodal encoders in the case of unimodal samples without requiring\nimputation or masking of absent modalities. As a result, MIND generates an\noptimized multimodal model, enhancing both multimodal and unimodal\nrepresentations. It can also be leveraged to balance multimodal learning during\ntraining. We evaluate MIND on binary and multilabel clinical prediction tasks\nusing time series data and chest X-ray images. Additionally, we assess the\ngeneralizability of the MIND framework on three non-medical multimodal\nmulticlass datasets. Experimental results demonstrate that MIND enhances the\nperformance of the smaller multimodal network across all five tasks, as well as\nvarious fusion methods and multimodal architectures, compared to\nstate-of-the-art baselines.", "paper_summary_zh": "\u591a\u6a21\u6001\u878d\u5408\u5229\u7528\u8de8\u6a21\u6001\u7684\u4fe1\u606f\u6765\u5b66\u4e60\u66f4\u597d\u7684\u7279\u5f81\u8868\u793a\uff0c\u76ee\u6807\u662f\u63d0\u5347\u57fa\u4e8e\u878d\u5408\u7684\u4efb\u52a1\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u73af\u5883\u4e2d\uff0c\u901a\u5e38\u6bd4\u5b83\u4eec\u7684\u5355\u6a21\u6001\u5bf9\u5e94\u6570\u636e\u96c6\u5c0f\uff0c\u8fd9\u4f1a\u963b\u788d\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6a21\u6001\u6570\u91cf\u7684\u589e\u52a0\u901a\u5e38\u4e0e\u591a\u6a21\u6001\u7f51\u7edc\u5c3a\u5bf8\u7684\u6574\u4f53\u589e\u52a0\u76f8\u5173\uff0c\u8fd9\u5728\u533b\u7597\u7528\u4f8b\u4e2d\u53ef\u80fd\u662f\u4e0d\u53ef\u53d6\u7684\u3002\u5229\u7528\u8f83\u5c0f\u7684\u5355\u6a21\u6001\u7f16\u7801\u5668\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u7ef4\u4e34\u5e8a\u6570\u636e\u65f6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6a21\u6001\u4fe1\u606f\u77e5\u8bc6\u84b8\u998f (MIND) \u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u591a\u6a21\u6001\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u5b83\u5c06\u6765\u81ea\u4e0d\u540c\u5927\u5c0f\u7684\u9884\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u96c6\u5408\u4e2d\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u4e00\u4e2a\u8f83\u5c0f\u7684\u591a\u6a21\u6001\u5b66\u751f\u4e2d\u3002\u6559\u5e08\u6a21\u578b\u7531\u5355\u6a21\u6001\u7f51\u7edc\u7ec4\u6210\uff0c\u5141\u8bb8\u5b66\u751f\u4ece\u4e0d\u540c\u7684\u8868\u793a\u4e2d\u5b66\u4e60\u3002MIND \u91c7\u7528\u591a\u5934\u8054\u5408\u878d\u5408\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u5355\u5934\u6a21\u578b\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5355\u6a21\u6001\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u5355\u6a21\u6001\u7f16\u7801\u5668\uff0c\u800c\u4e0d\u9700\u8981\u7f3a\u5931\u6a21\u6001\u7684\u63d2\u8865\u6216\u63a9\u853d\u3002\u56e0\u6b64\uff0cMIND \u751f\u6210\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u4f18\u5316\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u548c\u5355\u6a21\u6001\u8868\u793a\u3002\u5b83\u8fd8\u53ef\u4ee5\u7528\u6765\u5728\u8bad\u7ec3\u671f\u95f4\u5e73\u8861\u591a\u6a21\u6001\u5b66\u4e60\u3002\u6211\u4eec\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u80f8\u90e8 X \u5c04\u7ebf\u56fe\u50cf\u5bf9\u4e8c\u5143\u548c\u591a\u6807\u7b7e\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u8bc4\u4f30\u4e86 MIND\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86 MIND \u6846\u67b6\u5728\u4e09\u4e2a\u975e\u533b\u7597\u591a\u6a21\u6001\u591a\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cMIND \u589e\u5f3a\u4e86\u8f83\u5c0f\u7684\u591a\u6a21\u6001\u7f51\u7edc\u5728\u6240\u6709\u4e94\u4e2a\u4efb\u52a1\u4ee5\u53ca\u5404\u79cd\u878d\u5408\u65b9\u6cd5\u548c\u591a\u6a21\u6001\u67b6\u6784\u4e2d\u7684\u6027\u80fd\u3002", "author": "Alejandro Guerra-Manzanares et.al.", "authors": "Alejandro Guerra-Manzanares, Farah E. Shamout", "id": "2502.01158v1", "paper_url": "http://arxiv.org/abs/2502.01158v1", "repo": "null"}}