{"2406.09403": {"publish_time": "2024-06-13", "title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models", "paper_summary": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving\ngeometry problems; we mark and circle when reasoning on maps; we use sketches\nto amplify our ideas and relieve our limited-capacity working memory. However,\nsuch actions are missing in current multimodal language models (LMs). Current\nchain-of-thought and tool-use paradigms only use text as intermediate reasoning\nsteps. In this work, we introduce Sketchpad, a framework that gives multimodal\nLMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts\nplanning and reasoning according to the visual artifacts it has drawn.\nDifferent from prior work, which uses text-to-image models to enable LMs to\ndraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is\ncloser to human sketching and better facilitates reasoning. Sketchpad can also\nuse specialist vision models during the sketching process (e.g., draw bounding\nboxes with object detection models, draw masks with segmentation models), to\nfurther enhance visual perception and reasoning. We experiment with a wide\nrange of math tasks (including geometry, functions, graphs, and chess) and\ncomplex visual reasoning tasks. Sketchpad substantially improves performance on\nall tasks over strong base models with no sketching, yielding an average gain\nof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a\nnew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial\nreasoning (83.9%), and visual correspondence (80.8%). All codes and data are in\nhttps://visualsketchpad.github.io/.", "paper_summary_zh": "<paragraph>\u4eba\u985e\u5229\u7528\u7e6a\u756b\u4f86\u4fc3\u9032\u63a8\u7406\uff1a\u6211\u5011\u5728\u89e3\u6c7a\u5e7e\u4f55\u554f\u984c\u6642\u6703\u756b\u8f14\u52a9\u7dda\uff1b\u5728\u7814\u7a76\u5730\u5716\u6642\u6703\u6a19\u8a18\u548c\u756b\u5708\uff1b\u6211\u5011\u4f7f\u7528\u8349\u5716\u4f86\u64f4\u5c55\u6211\u5011\u7684\u60f3\u6cd5\u4e26\u6e1b\u8f15\u6211\u5011\u5bb9\u91cf\u6709\u9650\u7684\u5de5\u4f5c\u8a18\u61b6\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u6a21\u614b\u8a9e\u8a00\u6a21\u578b (LM) \u4e2d\u7f3a\u5c11\u6b64\u985e\u52d5\u4f5c\u3002\u76ee\u524d\u7684\u601d\u8003\u93c8\u548c\u5de5\u5177\u4f7f\u7528\u7bc4\u4f8b\u50c5\u5c07\u6587\u5b57\u7528\u4f5c\u4e2d\u9593\u63a8\u7406\u6b65\u9a5f\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 Sketchpad\uff0c\u4e00\u500b\u6846\u67b6\uff0c\u5b83\u70ba\u591a\u6a21\u614b LM \u63d0\u4f9b\u4e86\u4e00\u500b\u8996\u89ba\u8349\u5716\u672c\u548c\u53ef\u4ee5\u5728\u8349\u5716\u672c\u4e0a\u7e6a\u756b\u7684\u5de5\u5177\u3002LM \u6839\u64da\u5b83\u7e6a\u88fd\u7684\u8996\u89ba\u5de5\u4ef6\u9032\u884c\u898f\u5283\u548c\u63a8\u7406\u3002\u4e0d\u540c\u65bc\u4ee5\u524d\u4f7f\u7528\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\u4f7f LM \u80fd\u5920\u7e6a\u756b\u7684\u5148\u524d\u5de5\u4f5c\uff0cSketchpad \u4f7f LM \u80fd\u5920\u4f7f\u7528\u7dda\u689d\u3001\u65b9\u584a\u3001\u6a19\u8a18\u7b49\u9032\u884c\u7e6a\u756b\uff0c\u9019\u66f4\u63a5\u8fd1\u65bc\u4eba\u985e\u7684\u7d20\u63cf\uff0c\u4e26\u4e14\u66f4\u597d\u5730\u4fc3\u9032\u4e86\u63a8\u7406\u3002Sketchpad \u4e5f\u53ef\u4ee5\u5728\u7d20\u63cf\u904e\u7a0b\u4e2d\u4f7f\u7528\u5c08\u5bb6\u8996\u89ba\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u4f7f\u7528\u7269\u4ef6\u5075\u6e2c\u6a21\u578b\u7e6a\u88fd\u908a\u754c\u6846\uff0c\u4f7f\u7528\u5206\u5272\u6a21\u578b\u7e6a\u88fd\u906e\u7f69\uff09\uff0c\u4ee5\u9032\u4e00\u6b65\u589e\u5f37\u8996\u89ba\u611f\u77e5\u548c\u63a8\u7406\u3002\u6211\u5011\u4f7f\u7528\u5ee3\u6cdb\u7684\u6578\u5b78\u4efb\u52d9\uff08\u5305\u62ec\u5e7e\u4f55\u3001\u51fd\u6578\u3001\u5716\u5f62\u548c\u897f\u6d0b\u68cb\uff09\u548c\u8907\u96dc\u7684\u8996\u89ba\u63a8\u7406\u4efb\u52d9\u9032\u884c\u4e86\u5be6\u9a57\u3002Sketchpad \u5927\u5e45\u63d0\u5347\u4e86\u6240\u6709\u4efb\u52d9\u5728\u6c92\u6709\u7d20\u63cf\u7684\u60c5\u6cc1\u4e0b\u5f37\u5927\u7684\u57fa\u790e\u6a21\u578b\u7684\u6548\u80fd\uff0c\u5728\u6578\u5b78\u4efb\u52d9\u4e0a\u5e73\u5747\u63d0\u5347 12.7%\uff0c\u5728\u8996\u89ba\u4efb\u52d9\u4e0a\u63d0\u5347 8.6%\u3002\u914d\u5099 Sketchpad \u7684 GPT-4o \u5728\u6240\u6709\u4efb\u52d9\u4e0a\u90fd\u5275\u4e0b\u4e86\u65b0\u7684\u6280\u8853\u6c34\u6e96\uff0c\u5305\u62ec V*Bench (80.3%)\u3001BLINK \u7a7a\u9593\u63a8\u7406 (83.9%) \u548c\u8996\u89ba\u5c0d\u61c9 (80.8%)\u3002\u6240\u6709\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u90fd\u53ef\u4ee5\u5728 https://visualsketchpad.github.io/ \u4e2d\u627e\u5230\u3002</paragraph>", "author": "Yushi Hu et.al.", "authors": "Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna", "id": "2406.09403v1", "paper_url": "http://arxiv.org/abs/2406.09403v1", "repo": "null"}}