{"2405.05444": {"publish_time": "2024-05-08", "title": "Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large", "paper_summary": "Evaluating open-ended written examination responses from students is an\nessential yet time-intensive task for educators, requiring a high degree of\neffort, consistency, and precision. Recent developments in Large Language\nModels (LLMs) present a promising opportunity to balance the need for thorough\nevaluation with efficient use of educators' time. In our study, we explore the\neffectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in\nassessing university students' open-ended answers to questions made about\nreference material they have studied. Each model was instructed to evaluate 54\nanswers repeatedly under two conditions: 10 times (10-shot) with a temperature\nsetting of 0.0 and 10 times with a temperature of 0.5, expecting a total of\n1,080 evaluations per model and 4,320 evaluations across all models. The RAG\n(Retrieval Augmented Generation) framework was used as the framework to make\nthe LLMs to process the evaluation of the answers. As of spring 2024, our\nanalysis revealed notable variations in consistency and the grading outcomes\nprovided by studied LLMs. There is a need to comprehend strengths and\nweaknesses of LLMs in educational settings for evaluating open-ended written\nresponses. Further comparative research is essential to determine the accuracy\nand cost-effectiveness of using LLMs for educational assessments.", "paper_summary_zh": "\u8a55\u91cf\u5b78\u751f\u958b\u653e\u5f0f\u66f8\u9762\u8003\u8a66\u7684\u56de\u7b54\uff0c\u5c0d\u6559\u80b2\u5de5\u4f5c\u8005\u4f86\u8aaa\u662f\u4e00\u9805\u91cd\u8981\u4f46\u8017\u6642\u7684\u4efb\u52d9\uff0c\u9700\u8981\u9ad8\u5ea6\u7684\u52aa\u529b\u3001\u4e00\u81f4\u6027\u548c\u6e96\u78ba\u6027\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u767c\u5c55\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u5e0c\u671b\u7684\u6a5f\u6703\uff0c\u53ef\u4ee5\u5728\u5fb9\u5e95\u8a55\u91cf\u8207\u6709\u6548\u5229\u7528\u6559\u80b2\u5de5\u4f5c\u8005\u6642\u9593\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86 LLM ChatGPT-3.5\u3001ChatGPT-4\u3001Claude-3 \u548c Mistral-Large \u5728\u8a55\u91cf\u5927\u5b78\u751f\u91dd\u5c0d\u5176\u5df2\u7814\u8b80\u7684\u53c3\u8003\u8cc7\u6599\u6240\u63d0\u51fa\u7684\u958b\u653e\u5f0f\u554f\u984c\u56de\u7b54\u7684\u6709\u6548\u6027\u3002\u6bcf\u500b\u6a21\u578b\u90fd\u88ab\u6307\u793a\u5728\u5169\u7a2e\u689d\u4ef6\u4e0b\u91cd\u8907\u8a55\u91cf 54 \u500b\u7b54\u6848\uff1a10 \u6b21\uff0810 \u6b21\u5617\u8a66\uff09\u6eab\u5ea6\u8a2d\u5b9a\u70ba 0.0\uff0c\u4ee5\u53ca 10 \u6b21\u6eab\u5ea6\u8a2d\u5b9a\u70ba 0.5\uff0c\u9810\u671f\u6bcf\u500b\u6a21\u578b\u5171\u8a55\u91cf 1,080 \u6b21\uff0c\u6240\u6709\u6a21\u578b\u5171\u8a55\u91cf 4,320 \u6b21\u3002RAG\uff08\u6aa2\u7d22\u64f4\u5145\u751f\u6210\uff09\u67b6\u69cb\u88ab\u7528\u4f5c\u8b93 LLM \u8655\u7406\u7b54\u6848\u8a55\u91cf\u7684\u67b6\u69cb\u3002\u622a\u81f3 2024 \u5e74\u6625\u5b63\uff0c\u6211\u5011\u7684\u5206\u6790\u63ed\u793a\u4e86\u6240\u7814\u7a76\u7684 LLM \u5728\u4e00\u81f4\u6027\u548c\u8a55\u5206\u7d50\u679c\u65b9\u9762\u6709\u986f\u8457\u7684\u5dee\u7570\u3002\u6709\u5fc5\u8981\u4e86\u89e3 LLM \u5728\u6559\u80b2\u74b0\u5883\u4e2d\u8a55\u91cf\u958b\u653e\u5f0f\u66f8\u9762\u56de\u7b54\u7684\u512a\u9ede\u548c\u7f3a\u9ede\u3002\u9032\u4e00\u6b65\u7684\u6bd4\u8f03\u7814\u7a76\u5c0d\u65bc\u78ba\u5b9a\u4f7f\u7528 LLM \u9032\u884c\u6559\u80b2\u8a55\u91cf\u7684\u6e96\u78ba\u6027\u548c\u6210\u672c\u6548\u76ca\u81f3\u95dc\u91cd\u8981\u3002", "author": "Jussi S. Jauhiainen et.al.", "authors": "Jussi S. Jauhiainen, Agust\u00edn Garagorry Guerra", "id": "2405.05444v1", "paper_url": "http://arxiv.org/abs/2405.05444v1", "repo": "null"}}