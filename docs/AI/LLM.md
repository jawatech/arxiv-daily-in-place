
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-02-17**|**Diffusion Models without Classifier-free Guidance**|Zhicong Tang et.al.|[2502.12154v1](http://arxiv.org/abs/2502.12154v1)|[link](https://github.com/tzco/Diffusion-wo-CFG)|
|**2025-02-17**|**Idiosyncrasies in Large Language Models**|Mingjie Sun et.al.|[2502.12150v1](http://arxiv.org/abs/2502.12150v1)|null|
|**2025-02-17**|**HARBOR: Exploring Persona Dynamics in Multi-Agent Competition**|Kenan Jiang et.al.|[2502.12149v1](http://arxiv.org/abs/2502.12149v1)|null|
|**2025-02-17**|**Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control**|Jinyan Su et.al.|[2502.12145v1](http://arxiv.org/abs/2502.12145v1)|null|
|**2025-02-17**|**Small Models Struggle to Learn from Strong Reasoners**|Yuetai Li et.al.|[2502.12143v1](http://arxiv.org/abs/2502.12143v1)|null|
|**2025-02-17**|**SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs**|Yige Xu et.al.|[2502.12134v1](http://arxiv.org/abs/2502.12134v1)|null|
|**2025-02-17**|**Transformer Dynamics: A neuroscientific approach to interpretability of large language models**|Jesseba Fernando et.al.|[2502.12131v1](http://arxiv.org/abs/2502.12131v1)|null|
|**2025-02-17**|**Scaling Autonomous Agents via Automatic Reward Modeling And Planning**|Zhenfang Chen et.al.|[2502.12130v1](http://arxiv.org/abs/2502.12130v1)|null|
|**2025-02-17**|**LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities**|Florian Sestak et.al.|[2502.12128v1](http://arxiv.org/abs/2502.12128v1)|null|
|**2025-02-17**|**On the Query Complexity of Verifier-Assisted Language Generation**|Edoardo Botta et.al.|[2502.12123v1](http://arxiv.org/abs/2502.12123v1)|null|
|**2025-02-17**|**LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws**|Prasanna Mayilvahanan et.al.|[2502.12120v1](http://arxiv.org/abs/2502.12120v1)|null|
|**2025-02-17**|**PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection**|Jinhe Bi et.al.|[2502.12119v1](http://arxiv.org/abs/2502.12119v1)|null|
|**2025-02-17**|**Scaling Test-Time Compute Without Verification or RL is Suboptimal**|Amrith Setlur et.al.|[2502.12118v1](http://arxiv.org/abs/2502.12118v1)|null|
|**2025-02-17**|**A-MEM: Agentic Memory for LLM Agents**|Wujiang Xu et.al.|[2502.12110v1](http://arxiv.org/abs/2502.12110v1)|null|
|**2025-02-17**|**Personality Structured Interview for Large Language Model Simulation in Personality Research**|Pengda Wang et.al.|[2502.12109v1](http://arxiv.org/abs/2502.12109v1)|null|
|**2025-02-17**|**Using the Path of Least Resistance to Explain Deep Networks**|Sina Salek et.al.|[2502.12108v1](http://arxiv.org/abs/2502.12108v1)|null|
|**2025-02-17**|**Relational Norms for Human-AI Cooperation**|Brian D. Earp et.al.|[2502.12102v1](http://arxiv.org/abs/2502.12102v1)|null|
|**2025-02-17**|**A Study on Leveraging Search and Self-Feedback for Agent Reasoning**|Karthikeyan K et.al.|[2502.12094v1](http://arxiv.org/abs/2502.12094v1)|null|
|**2025-02-17**|**Meta-Statistical Learning: Supervised Learning of Statistical Inference**|Maxime Peyrard et.al.|[2502.12088v1](http://arxiv.org/abs/2502.12088v1)|null|
|**2025-02-17**|**APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs**|Yuxiang Huang et.al.|[2502.12085v1](http://arxiv.org/abs/2502.12085v1)|null|
|**2025-02-17**|**VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues**|Jianshu Zhang et.al.|[2502.12084v1](http://arxiv.org/abs/2502.12084v1)|null|
|**2025-02-17**|**AdaSplash: Adaptive Sparse Flash Attention**|Nuno Gonçalves et.al.|[2502.12082v1](http://arxiv.org/abs/2502.12082v1)|null|
|**2025-02-17**|**Unhackable Temporal Rewarding for Scalable Video MLLMs**|En Yu et.al.|[2502.12081v1](http://arxiv.org/abs/2502.12081v1)|null|
|**2025-02-17**|**Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation**|Zhongyi Qiu et.al.|[2502.12073v1](http://arxiv.org/abs/2502.12073v1)|null|
|**2025-02-17**|**TokenSkip: Controllable Chain-of-Thought Compression in LLMs**|Heming Xia et.al.|[2502.12067v1](http://arxiv.org/abs/2502.12067v1)|null|
|**2025-02-17**|**CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication Facilities with Large Language Models**|Yifan Zhang et.al.|[2502.12066v1](http://arxiv.org/abs/2502.12066v1)|null|
|**2025-02-17**|**Formalizing Complex Mathematical Statements with LLMs: A Study on Mathematical Definitions**|Lan Zhang et.al.|[2502.12065v1](http://arxiv.org/abs/2502.12065v1)|null|
|**2025-02-17**|**AI-generated Text Detection with a GLTR-based Approach**|Lucía Yan Wu et.al.|[2502.12064v1](http://arxiv.org/abs/2502.12064v1)|null|
|**2025-02-17**|**Culture is Not Trivia: Sociocultural Theory for Cultural NLP**|Naitian Zhou et.al.|[2502.12057v1](http://arxiv.org/abs/2502.12057v1)|null|
|**2025-02-17**|**Designing Role Vectors to Improve LLM Inference Behaviour**|Daniele Potertì et.al.|[2502.12055v1](http://arxiv.org/abs/2502.12055v1)|null|
|**2025-02-17**|**PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning**|Xinyu Zhang et.al.|[2502.12054v1](http://arxiv.org/abs/2502.12054v1)|null|
|**2025-02-17**|**A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability**|Xinyu Hu et.al.|[2502.12052v1](http://arxiv.org/abs/2502.12052v1)|null|
|**2025-02-17**|**How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines**|Ayan Sengupta et.al.|[2502.12051v1](http://arxiv.org/abs/2502.12051v1)|null|
|**2025-02-17**|**SpeechT: Findings of the First Mentorship in Speech Translation**|Yasmin Moslem et.al.|[2502.12050v1](http://arxiv.org/abs/2502.12050v1)|null|
|**2025-02-17**|**A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond**|Shreya Shukla et.al.|[2502.12048v1](http://arxiv.org/abs/2502.12048v1)|null|
|**2025-02-17**|**KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs**|Qi Zhao et.al.|[2502.12029v1](http://arxiv.org/abs/2502.12029v1)|null|
|**2025-02-17**|**SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities**|Fengqing Jiang et.al.|[2502.12025v1](http://arxiv.org/abs/2502.12025v1)|null|
|**2025-02-17**|**Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving**|Xin Xu et.al.|[2502.12022v1](http://arxiv.org/abs/2502.12022v1)|null|
|**2025-02-17**|**Atom of Thoughts for Markov LLM Test-Time Scaling**|Fengwei Teng et.al.|[2502.12018v1](http://arxiv.org/abs/2502.12018v1)|null|
|**2025-02-17**|**Demographic Attributes Prediction from Speech Using WavLM Embeddings**|Yuchen Yang et.al.|[2502.12007v1](http://arxiv.org/abs/2502.12007v1)|null|
|**2025-02-17**|**Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition**|Thibault Rousset et.al.|[2502.12001v1](http://arxiv.org/abs/2502.12001v1)|null|
|**2025-02-17**|**Presumed Cultural Identity: How Names Shape LLM Responses**|Siddhesh Pawar et.al.|[2502.11995v1](http://arxiv.org/abs/2502.11995v1)|null|
|**2025-02-17**|**Characterizing Photorealism and Artifacts in Diffusion Model-Generated Images**|Negar Kamali et.al.|[2502.11989v1](http://arxiv.org/abs/2502.11989v1)|null|
|**2025-02-17**|**Generating Text from Uniform Meaning Representation**|Emma Markle et.al.|[2502.11973v1](http://arxiv.org/abs/2502.11973v1)|null|
|**2025-02-17**|**Learning Generalizable Prompt for CLIP with Class Similarity Knowledge**|Sehun Jung et.al.|[2502.11969v1](http://arxiv.org/abs/2502.11969v1)|null|
|**2025-02-17**|**A MIMO Wireless Channel Foundation Model via CIR-CSI Consistency**|Jun Jiang et.al.|[2502.11965v1](http://arxiv.org/abs/2502.11965v1)|null|
|**2025-02-17**|**Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning**|Tianyi Wu et.al.|[2502.11962v1](http://arxiv.org/abs/2502.11962v1)|null|
|**2025-02-17**|**STRIVE: Structured Reasoning for Self-Improvement in Claim Verification**|Haisong Gong et.al.|[2502.11959v1](http://arxiv.org/abs/2502.11959v1)|null|
|**2025-02-17**|**Can Your Uncertainty Scores Detect Hallucinated Entity?**|Min-Hsuan Yeh et.al.|[2502.11948v1](http://arxiv.org/abs/2502.11948v1)|null|
|**2025-02-17**|**Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction**|Ailin Huang et.al.|[2502.11946v1](http://arxiv.org/abs/2502.11946v1)|null|
|**2025-02-17**|**Deep Spatio-Temporal Neural Network for Air Quality Reanalysis**|Ammar Kheder et.al.|[2502.11941v1](http://arxiv.org/abs/2502.11941v1)|null|
|**2025-02-17**|**FitLight: Federated Imitation Learning for Plug-and-Play Autonomous Traffic Signal Control**|Yutong Ye et.al.|[2502.11937v1](http://arxiv.org/abs/2502.11937v1)|null|
|**2025-02-17**|**On Representational Dissociation of Language and Arithmetic in Large Language Models**|Riku Kisako et.al.|[2502.11932v1](http://arxiv.org/abs/2502.11932v1)|null|
|**2025-02-17**|**BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages**|Shamsuddeen Hassan Muhammad et.al.|[2502.11926v1](http://arxiv.org/abs/2502.11926v1)|null|
|**2025-02-17**|**GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs**|Yi Fang et.al.|[2502.11925v1](http://arxiv.org/abs/2502.11925v1)|null|
|**2025-02-17**|**From Text to Trust: Empowering AI-assisted Decision Making with Adaptive LLM-powered Analysis**|Zhuoyan Li et.al.|[2502.11919v1](http://arxiv.org/abs/2502.11919v1)|null|
|**2025-02-17**|**EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models**|Jiamin Su et.al.|[2502.11916v1](http://arxiv.org/abs/2502.11916v1)|null|
|**2025-02-17**|**On the robustness of ChatGPT in teaching Korean Mathematics**|Phuong-Nam Nguyen et.al.|[2502.11915v1](http://arxiv.org/abs/2502.11915v1)|null|
|**2025-02-17**|**MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation**|Haochen Xue et.al.|[2502.11903v1](http://arxiv.org/abs/2502.11903v1)|null|
|**2025-02-17**|**Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity**|Dylan Zhang et.al.|[2502.11901v1](http://arxiv.org/abs/2502.11901v1)|null|
|**2025-02-17**|**DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation**|Zhihang Yuan et.al.|[2502.11897v1](http://arxiv.org/abs/2502.11897v1)|null|
|**2025-02-17**|**CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning**|Yanxiao Zhao et.al.|[2502.11896v1](http://arxiv.org/abs/2502.11896v1)|null|
|**2025-02-17**|**Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?**|Jacob Nielsen et.al.|[2502.11895v1](http://arxiv.org/abs/2502.11895v1)|null|
|**2025-02-17**|**Revisiting Classification Taxonomy for Grammatical Errors**|Deqing Zou et.al.|[2502.11890v1](http://arxiv.org/abs/2502.11890v1)|null|
|**2025-02-17**|**Stonefish: Supporting Machine Learning Research in Marine Robotics**|Michele Grimaldi et.al.|[2502.11887v1](http://arxiv.org/abs/2502.11887v1)|null|
|**2025-02-17**|**LIMR: Less is More for RL Scaling**|Xuefeng Li et.al.|[2502.11886v1](http://arxiv.org/abs/2502.11886v1)|null|
|**2025-02-17**|**Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration**|Shao Zhang et.al.|[2502.11882v1](http://arxiv.org/abs/2502.11882v1)|null|
|**2025-02-17**|**Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models**|Hyunwoo Kim et.al.|[2502.11881v1](http://arxiv.org/abs/2502.11881v1)|null|
|**2025-02-17**|**Bitnet.cpp: Efficient Edge Inference for Ternary LLMs**|Jinheng Wang et.al.|[2502.11880v1](http://arxiv.org/abs/2502.11880v1)|null|
|**2025-02-17**|**VAQUUM: Are Vague Quantifiers Grounded in Visual Data?**|Hugh Mee Wong et.al.|[2502.11874v1](http://arxiv.org/abs/2502.11874v1)|null|
|**2025-02-17**|**Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire Articles Beyond the Front Page**|Michael McRae et.al.|[2502.11866v1](http://arxiv.org/abs/2502.11866v1)|null|
|**2025-02-17**|**FedEAT: A Robustness Optimization Framework for Federated LLMs**|Yahao Pang et.al.|[2502.11863v1](http://arxiv.org/abs/2502.11863v1)|null|
|**2025-02-17**|**Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu**|Renhao Pei et.al.|[2502.11862v1](http://arxiv.org/abs/2502.11862v1)|null|
|**2025-02-17**|**Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics**|Shuqi Yang et.al.|[2502.11861v1](http://arxiv.org/abs/2502.11861v1)|null|
|**2025-02-17**|**Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics**|Wenrui Xu et.al.|[2502.11859v1](http://arxiv.org/abs/2502.11859v1)|null|
|**2025-02-17**|**LLMs as a synthesis between symbolic and continuous approaches to language**|Gemma Boleda et.al.|[2502.11856v1](http://arxiv.org/abs/2502.11856v1)|null|
|**2025-02-17**|**BaxBench: Can LLMs Generate Correct and Secure Backends?**|Mark Vero et.al.|[2502.11844v1](http://arxiv.org/abs/2502.11844v1)|null|
|**2025-02-17**|**Can LLM Agents Maintain a Persona in Discourse?**|Pranav Bhandari et.al.|[2502.11843v1](http://arxiv.org/abs/2502.11843v1)|null|
|**2025-02-17**|**ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio Chord Recognition**|Muhammad Waseem Akram et.al.|[2502.11840v1](http://arxiv.org/abs/2502.11840v1)|null|
|**2025-02-17**|**Intuitive physics understanding emerges from self-supervised pretraining on natural videos**|Quentin Garrido et.al.|[2502.11831v1](http://arxiv.org/abs/2502.11831v1)|null|
|**2025-02-17**|**Text Classification in the LLM Era - Where do we stand?**|Sowmya Vajjala et.al.|[2502.11830v1](http://arxiv.org/abs/2502.11830v1)|null|
|**2025-02-17**|**Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities**|Hanbin Wang et.al.|[2502.11829v1](http://arxiv.org/abs/2502.11829v1)|null|
|**2025-02-17**|**M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis**|Chengyan Wu et.al.|[2502.11824v1](http://arxiv.org/abs/2502.11824v1)|null|
|**2025-02-17**|**AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling**|Hao Zhou et.al.|[2502.11817v1](http://arxiv.org/abs/2502.11817v1)|null|
|**2025-02-17**|**Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis**|Xu Wang et.al.|[2502.11812v1](http://arxiv.org/abs/2502.11812v1)|null|
|**2025-02-17**|**FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models**|Qianchi Zhang et.al.|[2502.11811v1](http://arxiv.org/abs/2502.11811v1)|null|
|**2025-02-17**|**Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling**|Yanbiao Ma et.al.|[2502.11809v1](http://arxiv.org/abs/2502.11809v1)|null|
|**2025-02-17**|**Exploring Translation Mechanism of Large Language Models**|Hongbin Zhang et.al.|[2502.11806v1](http://arxiv.org/abs/2502.11806v1)|null|
|**2025-02-17**|**Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning**|Peiying Yu et.al.|[2502.11799v1](http://arxiv.org/abs/2502.11799v1)|null|
|**2025-02-17**|**Personality Editing for Language Models through Relevant Knowledge Editing**|Seojin Hwang et.al.|[2502.11789v1](http://arxiv.org/abs/2502.11789v1)|null|
|**2025-02-17**|**Efficient Response Generation Method Selection for Fine-Tuning Large Language Models**|Xuan Ren et.al.|[2502.11779v1](http://arxiv.org/abs/2502.11779v1)|null|
|**2025-02-17**|**Deep Neural Networks for Accurate Depth Estimation with Latent Space Features**|Siddiqui Muhammad Yasir et.al.|[2502.11777v1](http://arxiv.org/abs/2502.11777v1)|null|
|**2025-02-17**|**The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It**|Leonardo Bertolazzi et.al.|[2502.11771v1](http://arxiv.org/abs/2502.11771v1)|null|
|**2025-02-17**|**Cognitive-Aligned Document Selection for Retrieval-augmented Generation**|Bingyu Wan et.al.|[2502.11770v1](http://arxiv.org/abs/2502.11770v1)|null|
|**2025-02-17**|**From Selection to Generation: A Survey of LLM-based Active Learning**|Yu Xia et.al.|[2502.11767v1](http://arxiv.org/abs/2502.11767v1)|null|
|**2025-02-17**|**Warmup-Distill: Bridge the Distribution Mismatch between Teacher and Student before Knowledge Distillation**|Zengkui Sun et.al.|[2502.11766v1](http://arxiv.org/abs/2502.11766v1)|null|
|**2025-02-17**|**Lightweight Deepfake Detection Based on Multi-Feature Fusion**|Siddiqui Muhammad Yasir et.al.|[2502.11763v1](http://arxiv.org/abs/2502.11763v1)|null|
|**2025-02-17**|**HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims**|Michiel van der Meer et.al.|[2502.11753v1](http://arxiv.org/abs/2502.11753v1)|null|
|**2025-02-17**|**Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning**|Yuqi Pang et.al.|[2502.11751v1](http://arxiv.org/abs/2502.11751v1)|null|
|**2025-02-17**|**SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL**|Shuai Lyu et.al.|[2502.11741v1](http://arxiv.org/abs/2502.11741v1)|null|

#### Abstracts
##### **Diffusion Models without Classifier-free Guidance**
2502.12154v1 by Zhicong Tang, Jianmin Bao, Dong Chen, Baining Guo

This paper presents Model-guidance (MG), a novel objective for training
diffusion model that addresses and removes of the commonly used Classifier-free
guidance (CFG). Our innovative approach transcends the standard modeling of
solely data distribution to incorporating the posterior probability of
conditions. The proposed technique originates from the idea of CFG and is easy
yet effective, making it a plug-and-play module for existing models. Our method
significantly accelerates the training process, doubles the inference speed,
and achieve exceptional quality that parallel and even surpass concurrent
diffusion models with CFG. Extensive experiments demonstrate the effectiveness,
efficiency, scalability on different models and datasets. Finally, we establish
state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.
Our code is available at https://github.com/tzco/Diffusion-wo-CFG.

摘要：本文提出模型指導 (MG)，一種用於訓練擴散模型的新目標，它解決並消除了常用的無分類器指導 (CFG)。我們的創新方法超越了僅數據分佈的標準建模，並納入了條件的後驗機率。提議的技術源自 CFG 的概念，既簡單又有效，使其成為現有模型的即插即用模組。我們的技術顯著加速了訓練過程，將推論速度提高了一倍，並取得了與 CFG 並行甚至超越並行擴散模型的出色品質。廣泛的實驗證明了該技術在不同模型和資料集上的有效性、效率和可擴充性。最後，我們在 ImageNet 256 基準上建立了最先進的效能，FID 為 1.34。我們的程式碼可在 https://github.com/tzco/Diffusion-wo-CFG 取得。

##### **Idiosyncrasies in Large Language Models**
2502.12150v1 by Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu

In this work, we unveil and study idiosyncrasies in Large Language Models
(LLMs) -- unique patterns in their outputs that can be used to distinguish the
models. To do so, we consider a simple classification task: given a particular
text output, the objective is to predict the source LLM that generates the
text. We evaluate this synthetic task across various groups of LLMs and find
that simply fine-tuning existing text embedding models on LLM-generated texts
yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on
held-out validation data in the five-way classification problem involving
ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals
that these idiosyncrasies are rooted in word-level distributions. These
patterns persist even when the texts are rewritten, translated, or summarized
by an external LLM, suggesting that they are also encoded in the semantic
content. Additionally, we leverage LLM as judges to generate detailed,
open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the
broader implications of our findings, particularly for training on synthetic
data and inferring model similarity. Code is available at
https://github.com/locuslab/llm-idiosyncrasies.

摘要：在這項工作中，我們揭示並研究了大型語言模型 (LLM) 中的特殊性，也就是其輸出中可區分模型的獨特模式。為此，我們考慮了一項簡單的分類任務：給定一個特定文本輸出，目標是預測產生該文本的來源 LLM。我們在各種 LLM 組合中評估這個合成任務，並發現僅微調現有的文本嵌入模型在 LLM 生成的文本上即可產生極佳的分類準確度。值得注意的是，在涉及 ChatGPT、Claude、Grok、Gemini 和 DeepSeek 的五向分類問題中，我們在留存驗證資料上達到了 97.1% 的準確度。我們的進一步調查顯示，這些特殊性根植於詞彙層級的分布。即使文本是由外部 LLM 改寫、翻譯或摘要，這些模式仍然存在，這表明它們也編碼在語義內容中。此外，我們利用 LLM 作為評審，為每個模型的特殊性產生詳細、開放式的描述。最後，我們討論了我們發現的更廣泛含意，特別是對於合成資料的訓練和推斷模型相似性。程式碼可在 https://github.com/locuslab/llm-idiosyncrasies 取得。

##### **HARBOR: Exploring Persona Dynamics in Multi-Agent Competition**
2502.12149v1 by Kenan Jiang, Li Xiong, Fei Liu

We investigate factors contributing to LLM agents' success in competitive
multi-agent environments, using auctions as a testbed where agents bid to
maximize profit. The agents are equipped with bidding domain knowledge,
distinct personas that reflect item preferences, and a memory of auction
history. Our work extends the classic auction scenario by creating a realistic
environment where multiple agents bid on houses, weighing aspects such as size,
location, and budget to secure the most desirable homes at the lowest prices.
Particularly, we investigate three key questions: (a) How does a persona
influence an agent's behavior in a competitive setting? (b) Can an agent
effectively profile its competitors' behavior during auctions? (c) How can
persona profiling be leveraged to create an advantage using strategies such as
theory of mind? Through a series of experiments, we analyze the behaviors of
LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a
valuable platform for deepening our understanding of multi-agent workflows in
competitive environments.

摘要：我們研究促成 LLM 代理在競爭性多代理環境中成功的因素，使用拍賣作為測試平台，其中代理出價以最大化利潤。這些代理配備了競標領域知識、反映物品偏好的不同角色以及拍賣歷史的記憶。我們的研究透過創造一個現實的環境來擴展經典的拍賣場景，在該環境中，多個代理對房屋出價，權衡大小、位置和預算等方面以最低價格確保最理想的房屋。特別是，我們研究了三個關鍵問題：(a) 角色如何在競爭環境中影響代理的行為？(b) 代理是否可以在拍賣期間有效地分析其競爭對手的行為？(c) 如何利用角色分析來利用心智理論等策略創造優勢？透過一系列實驗，我們分析 LLM 代理的行為並闡明新的發現。我們的測試平台稱為 HARBOR，它提供了一個有價值的平台，用於加深我們對競爭環境中多代理工作流程的理解。

##### **Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control**
2502.12145v1 by Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie

Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to
mitigate large language model (LLM) hallucinations by incorporating external
knowledge retrieval. However, existing RAG frameworks often apply retrieval
indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or
failing to retrieve iteratively when required for complex reasoning. Recent
adaptive retrieval strategies, though adaptively navigates these retrieval
strategies, predict only based on query complexity and lacks user-driven
flexibility, making them infeasible for diverse user application needs. In this
paper, we introduce a novel user-controllable RAG framework that enables
dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two
classifiers: one trained to prioritize accuracy and another to prioritize
retrieval efficiency. Via an interpretable control parameter $\alpha$, users
can seamlessly navigate between minimal-cost retrieval and high-accuracy
retrieval based on their specific requirements. We empirically demonstrate that
our approach effectively balances accuracy, retrieval cost, and user
controllability, making it a practical and adaptable solution for real-world
applications.

摘要：檢索增強生成 (RAG) 已成為一種強大的方法，可透過整合外部知識檢索來減輕大型語言模型 (LLM) 的幻覺。然而，現有的 RAG 框架經常不加區別地應用檢索，導致低效率，在不必要時過度檢索，或在複雜推理時無法反覆檢索。最近的自適應檢索策略，儘管自適應地導航這些檢索策略，但僅根據查詢複雜性進行預測，並且缺乏使用者驅動的靈活性，這使得它們無法滿足多樣化的使用者應用需求。在本文中，我們引入了一個新穎的使用者可控制 RAG 框架，它可以動態調整準確度成本權衡。我們的做法利用兩個分類器：一個訓練用於優先考慮準確度，另一個用於優先考慮檢索效率。透過可解釋的控制參數 $\alpha$，使用者可以在最低成本檢索和基於其特定需求的高準確度檢索之間無縫導航。我們通過實證證明，我們的做法有效地平衡了準確度、檢索成本和使用者可控性，使其成為現實世界應用中實用且適應性強的解決方案。

##### **Small Models Struggle to Learn from Strong Reasoners**
2502.12143v1 by Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, Radha Poovendran

Large language models (LLMs) excel in complex reasoning tasks, and distilling
their reasoning capabilities into smaller models has shown promise. However, we
uncover an interesting phenomenon, which we term the Small Model Learnability
Gap: small models ($\leq$3B parameters) do not consistently benefit from long
chain-of-thought (CoT) reasoning or distillation from larger models. Instead,
they perform better when fine-tuned on shorter, simpler reasoning chains that
better align with their intrinsic learning capacity. To address this, we
propose Mix Distillation, a simple yet effective strategy that balances
reasoning complexity by combining long and short CoT examples or reasoning from
both larger and smaller models. Our experiments demonstrate that Mix
Distillation significantly improves small model reasoning performance compared
to training on either data alone. These findings highlight the limitations of
direct strong model distillation and underscore the importance of adapting
reasoning complexity for effective reasoning capability transfer.

摘要：大型語言模型 (LLM) 在複雜推理任務中表現出色，且將其推理能力提煉成較小的模型已展現前景。然而，我們發現了一個有趣的現象，我們稱之為小型模型可學習性差距：小型模型（參數數目 ≤ 3B）並非總能從大型模型的長鏈條思考 (CoT) 推理或提煉中受益。相反地，當針對較短、較簡單的推理鏈進行微調時，它們的表現會更好，而這更符合其內在學習能力。為了解決此問題，我們提出混合提煉，這是一種簡單但有效的策略，透過結合長短 CoT 範例或從較大及較小模型進行推理，來平衡推理的複雜性。我們的實驗證明，與僅針對任一資料進行訓練相比，混合提煉顯著改善了小型模型的推理效能。這些發現突顯了直接強模型提煉的限制，並強調了調整推理複雜性以有效轉移推理能力的重要性。

##### **SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs**
2502.12134v1 by Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao

Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to
solve complex reasoning tasks by generating intermediate reasoning steps.
However, most existing approaches focus on hard token decoding, which
constrains reasoning within the discrete vocabulary space and may not always be
optimal. While recent efforts explore continuous-space reasoning, they often
suffer from catastrophic forgetting, limiting their applicability to
state-of-the-art LLMs that already perform well in zero-shot settings with a
proper instruction. To address this challenge, we propose a novel approach for
continuous-space reasoning that does not require modifying the underlying LLM.
Specifically, we employ a lightweight assistant model to generate
instance-specific soft thought tokens speculatively as the initial chain of
thoughts, which are then mapped into the LLM's representation space via a
projection module. Experimental results on five reasoning benchmarks
demonstrate that our method enhances LLM reasoning performance through
supervised, parameter-efficient fine-tuning.

摘要：鏈式思考 (CoT) 推理讓大型語言模型 (LLM) 能夠透過產生中間推理步驟來解決複雜的推理任務。然而，現有的大多數方法都專注於硬標記解碼，這會將推理限制在離散的詞彙空間內，而且可能並非總是最佳。雖然最近的研究探索了連續空間推理，但它們經常會遭遇災難性遺忘，這限制了它們在零次學習設置中表現良好的最先進 LLM 的適用性，且需要適當的說明。為了應對這項挑戰，我們提出了一種創新的連續空間推理方法，不需要修改底層的 LLM。具體來說，我們採用一個輕量級的輔助模型來產生特定於實例的軟思考標記，作為思考的初始鏈，然後透過投影模組將它們映射到 LLM 的表示空間。在五個推理基準上的實驗結果表明，我們的模型透過監督式、參數高效的微調，增強了 LLM 的推理效能。

##### **Transformer Dynamics: A neuroscientific approach to interpretability of large language models**
2502.12131v1 by Jesseba Fernando, Grigori Guitchounts

As artificial intelligence models have exploded in scale and capability,
understanding of their internal mechanisms remains a critical challenge.
Inspired by the success of dynamical systems approaches in neuroscience, here
we propose a novel framework for studying computations in deep learning
systems. We focus on the residual stream (RS) in transformer models,
conceptualizing it as a dynamical system evolving across layers. We find that
activations of individual RS units exhibit strong continuity across layers,
despite the RS being a non-privileged basis. Activations in the RS accelerate
and grow denser over layers, while individual units trace unstable periodic
orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with
attractor-like dynamics in the lower layers. These insights bridge dynamical
systems theory and mechanistic interpretability, establishing a foundation for
a "neuroscience of AI" that combines theoretical rigor with large-scale data
analysis to advance our understanding of modern neural networks.

摘要：隨著人工智慧模型在規模和能力上爆炸式增長，
理解其內部機制仍然是一項嚴峻的挑戰。
受到神經科學中動力系統方法成功的啟發，我們在此
提出了一個新的框架來研究深度學習系統中的運算。我們專注於Transformer模型中的殘差流 (RS)，
將其概念化為一個跨層演化的動態系統。我們發現
儘管 RS 不是一個特權基礎，但個別 RS 單元的激活在各層之間表現出很強的連續性。RS 中的激活
隨著層數的增加而加速並變得更密集，而個別單元則追蹤不穩定的週期
軌道。在降維空間中，RS 遵循一個曲線軌跡，在較低層中具有類吸引子的動力學。這些見解橋接了動力
系統理論和機制可解釋性，為「AI 神經科學」奠定了基礎，結合了理論嚴謹性和大規模數據
分析，以增進我們對現代神經網路的理解。

##### **Scaling Autonomous Agents via Automatic Reward Modeling And Planning**
2502.12130v1 by Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan

Large language models (LLMs) have demonstrated remarkable capabilities across
a range of text-generation tasks. However, LLMs still struggle with problems
requiring multi-step decision-making and environmental feedback, such as online
shopping, scientific reasoning, and mathematical problem-solving. Unlike pure
text data, collecting large-scale decision-making data is challenging.
Moreover, many powerful LLMs are only accessible through APIs, which hinders
their fine-tuning for agent tasks due to cost and complexity. To address LLM
agents' limitations, we propose a framework that can automatically learn a
reward model from the environment without human annotations. This model can be
used to evaluate the action trajectories of LLM agents and provide heuristics
for task planning. Specifically, our approach involves employing one LLM-based
agent to navigate an environment randomly, generating diverse action
trajectories. Subsequently, a separate LLM is leveraged to assign a task intent
and synthesize a negative response alongside the correct response for each
trajectory. These triplets (task intent, positive response, and negative
response) are then utilized as training data to optimize a reward model capable
of scoring action trajectories. The effectiveness and generalizability of our
framework are demonstrated through evaluations conducted on different agent
benchmarks. In conclusion, our proposed framework represents a significant
advancement in enhancing LLM agents' decision-making capabilities. By
automating the learning of reward models, we overcome the challenges of data
scarcity and API limitations, potentially revolutionizing the application of
LLMs in complex and interactive environments. This research paves the way for
more sophisticated AI agents capable of tackling a wide range of real-world
problems requiring multi-step decision-making.

摘要：大型語言模型 (LLM) 已在各種文字生成任務中展示出非凡的能力。然而，LLM 仍然在需要多步驟決策制定和環境回饋的問題上苦苦掙扎，例如網上購物、科學推理和數學問題求解。與純文本數據不同，收集大規模決策制定數據具有挑戰性。此外，許多強大的 LLM 只能通過 API 訪問，這由於成本和複雜性而阻礙了它們對代理任務的微調。為了解決 LLM 代理的局限性，我們提出了一個框架，該框架可以從環境中自動學習獎勵模型，而無需人工註釋。此模型可用于評估 LLM 代理的動作軌跡並為任務規劃提供啟發式方法。具體來說，我們的方法涉及使用一個基於 LLM 的代理隨機導航環境，生成不同的動作軌跡。隨後，利用一個單獨的 LLM 為每個軌跡分配任務意圖並合成一個負面響應以及正確的響應。然後將這些三元組（任務意圖、正面響應和負面響應）用作訓練數據，以優化能夠評分動作軌跡的獎勵模型。我們框架的有效性和普遍性通過在不同代理基準上進行的評估得到證明。總之，我們提出的框架代表了加強 LLM 代理決策能力的重大進步。通過自動化獎勵模型的學習，我們克服了數據稀缺和 API 限制的挑戰，有可能徹底改變 LLM 在複雜和互動環境中的應用。這項研究為更複雜的 AI 代理鋪平了道路，這些代理能夠解決需要多步驟決策制定的大量現實世界問題。

##### **LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities**
2502.12128v1 by Florian Sestak, Artur Toshev, Andreas Fürst, Günter Klambauer, Andreas Mayr, Johannes Brandstetter

Generative models are spearheading recent progress in deep learning, showing
strong promise for trajectory sampling in dynamical systems as well. However,
while latent space modeling paradigms have transformed image and video
generation, similar approaches are more difficult for most dynamical systems.
Such systems -- from chemical molecule structures to collective human behavior
-- are described by interactions of entities, making them inherently linked to
connectivity patterns and the traceability of entities over time. Our approach,
LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked
Entities), combines the advantages of graph neural networks, i.e., the
traceability of entities across time-steps, with the efficiency and scalability
of recent advances in image and video generation, where pre-trained encoder and
decoder are frozen to enable generative modeling in the latent space. The core
idea of LaM-SLidE is to introduce identifier representations (IDs) to allow for
retrieval of entity properties, e.g., entity coordinates, from latent system
representations and thus enables traceability. Experimentally, across different
domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy,
and generalizability. (Code is available at
https://github.com/ml-jku/LaM-SLidE)

摘要：生成模型引領深度學習的最新進展，也展現出在動態系統中進行軌跡取樣的強大前景。然而，儘管潛在空間建模範例已轉變圖像和影片生成，但對於大多數動態系統來說，類似的做法較為困難。此類系統（從化學分子結構到人類集體行為）由實體的交互作用所描述，使它們與連接模式和實體隨時間的追溯性產生固有聯繫。我們的做法 LaM-SLidE（透過連結實體進行空間動態系統的潛在空間建模）結合圖形神經網路的優點，亦即跨時間步長的實體追溯性，以及圖像和影片生成中近期進展的高效率和可擴充性，其中預先訓練的編碼器和解碼器被凍結以在潛在空間中啟用生成模型。LaM-SLidE 的核心概念是導入識別符號表示（ID），以允許從潛在系統表示中擷取實體屬性（例如實體座標），從而實現追溯性。透過不同領域的實驗，我們證明 LaM-SLidE 在速度、準確度和可概括性方面表現良好。（程式碼可在 https://github.com/ml-jku/LaM-SLidE 取得）

##### **On the Query Complexity of Verifier-Assisted Language Generation**
2502.12123v1 by Edoardo Botta, Yuchen Li, Aashay Mehta, Jordan T. Ash, Cyril Zhang, Andrej Risteski

Recently, a plethora of works have proposed inference-time algorithms (e.g.
best-of-n), which incorporate verifiers to assist the generation process. Their
quality-efficiency trade-offs have been empirically benchmarked on a variety of
constrained generation tasks, but the algorithmic design landscape is still
largely poorly understood. In this paper, we develop a mathematical framework
for reasoning about constrained generation using a pre-trained language model
generator oracle and a process verifier--which can decide whether a prefix can
be extended to a string which satisfies the constraints of choice. We show that
even in very simple settings, access to a verifier can render an intractable
problem (information-theoretically or computationally) to a tractable one. In
fact, we show even simple algorithms, like tokenwise rejection sampling, can
enjoy significant benefits from access to a verifier. Empirically, we show that
a natural modification of tokenwise rejection sampling, in which the sampler is
allowed to "backtrack" (i.e., erase the final few generated tokens) has robust
and substantive benefits over natural baselines (e.g. (blockwise) rejection
sampling, nucleus sampling)--both in terms of computational efficiency,
accuracy and diversity.

摘要：<paragraph>最近，许多作品提出了推理时间算法（例如 best-of-n），其中包含验证器以协助生成过程。它们的质量效率权衡已在各种受限生成任务中得到经验基准测试，但算法设计格局仍然很大程度上难以理解。在本文中，我们开发了一个数学框架，用于使用预训练语言模型生成器预言机和过程验证器推理受限生成——它可以决定是否可以将前缀扩展为满足选择约束的字符串。我们表明，即使在非常简单的设置中，访问验证器也可以将一个棘手的问题（信息论或计算）转换为一个易处理的问题。事实上，我们表明即使是简单的算法，如逐个标记拒绝采样，也可以从访问验证器中受益匪浅。凭经验，我们表明逐个标记拒绝采样的自然修改，其中允许采样器“回溯”（即，擦除最后几个生成的标记）比自然基线（例如（按块）拒绝采样、核采样）具有强大而实质性的优势——无论是在计算效率、准确性还是多样性方面。</paragraph>

##### **LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws**
2502.12120v1 by Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel

Scaling laws guide the development of large language models (LLMs) by
offering estimates for the optimal balance of model size, tokens, and compute.
More recently, loss-to-loss scaling laws that relate losses across pretraining
datasets and downstream tasks have emerged as a powerful tool for understanding
and improving LLM performance. In this work, we investigate which factors most
strongly influence loss-to-loss scaling. Our experiments reveal that the
pretraining data and tokenizer determine the scaling trend. In contrast, model
size, optimization hyperparameters, and even significant architectural
differences, such as between transformer-based models like Llama and
state-space models like Mamba, have limited impact. Consequently, practitioners
should carefully curate suitable pretraining datasets for optimal downstream
performance, while architectures and other settings can be freely optimized for
training efficiency.

摘要：規模化定律透過提供模型大小、符號和運算的最佳平衡估計，引導大型語言模型 (LLM) 的開發。最近，與預訓練資料集和下游任務相關的損失到損失縮放定律已成為了解和改善 LLM 效能的強大工具。在這項工作中，我們探討哪些因素最能影響損失到損失縮放。我們的實驗顯示，預訓練資料和分詞器會決定縮放趨勢。相反地，模型大小、最佳化超參數，甚至重大的架構差異（例如基於Transformer的模型，如 Llama，和狀態空間模型，如 Mamba 之間的差異）影響有限。因此，從業人員應仔細策劃適當的預訓練資料集以獲得最佳的下游效能，而架構和其他設定可以自由最佳化以提升訓練效率。

##### **PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection**
2502.12119v1 by Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma

Visual instruction tuning refines pre-trained Multimodal Large Language
Models (MLLMs) to enhance their real-world task performance. However, the rapid
expansion of visual instruction datasets introduces significant data
redundancy, leading to excessive computational costs. Existing data selection
methods predominantly rely on proxy models or loss-based metrics, both of which
impose substantial computational overheads due to the necessity of model
inference and backpropagation. To address this challenge, we propose PRISM, a
novel training-free approach for efficient multimodal data selection. Unlike
existing methods, PRISM eliminates the reliance on proxy models, warm-up
pretraining, and gradient-based optimization. Instead, it leverages Pearson
correlation analysis to quantify the intrinsic visual encoding properties of
MLLMs, computing a task-specific correlation score to identify high-value
instances. This not only enbles data-efficient selection,but maintains the
original performance. Empirical evaluations across multiple MLLMs demonstrate
that PRISM reduces the overall time required for visual instruction tuning and
data selection to just 30% of conventional methods, while surpassing fully
fine-tuned models across eight multimodal and three language understanding
benchmarks, achieving a 101.7% relative improvement in final performance.

摘要：視覺指令調整優化預先訓練的多模態大型語言模型 (MLLM)，以增強其真實世界的任務表現。然而，視覺指令資料集的快速擴展引入了顯著的資料冗餘，導致過度的運算成本。現有的資料選取方法主要依賴於代理模型或基於損失的指標，這兩者由於模型推理和反向傳播的必要性而造成大量的運算負擔。為了應對這一挑戰，我們提出了 PRISM，一種用於高效多模態資料選取的新型無訓練方法。與現有方法不同，PRISM 消除了對代理模型、熱身預訓練和基於梯度的優化的依賴。相反，它利用 Pearson 相關分析來量化 MLLM 的內在視覺編碼特性，計算特定任務相關性分數以識別高價值實例。這不僅能選擇資料效率，而且能保持原始效能。跨多個 MLLM 的經驗評估表明，PRISM 將視覺指令調整和資料選取所需的總時間減少到傳統方法的 30%，同時在八個多模態和三個語言理解基準中超越了完全微調的模型，在最終效能上實現了 101.7% 的相對改進。

##### **Scaling Test-Time Compute Without Verification or RL is Suboptimal**
2502.12118v1 by Amrith Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar

Despite substantial advances in scaling test-time compute, an ongoing debate
in the community is how it should be scaled up to enable continued and
efficient improvements with scaling. There are largely two approaches: first,
distilling successful search or thinking traces; and second, using verification
(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement
learning (RL) and search algorithms. In this paper, we prove that finetuning
LLMs with verifier-based (VB) methods based on RL or search is far superior to
verifier-free (VF) approaches based on distilling or cloning search traces,
given a fixed amount of compute/data budget. Further, we show that as we scale
test-time compute (measured as the output token length) and training data,
suboptimality of VF methods scales poorly compared to VB when the base
pre-trained LLM presents a heterogeneous distribution over correct solution
traces (e.g., different lengths, styles, etc.) and admits a non-sharp
distribution over rewards on traces sampled from it. We formalize this
condition using anti-concentration [Erd\H{o}s, 1945]. This implies a stronger
result that VB methods scale better asymptotically, with the performance gap
between VB and VF methods widening as test-time budget grows. We corroborate
our theory empirically on both didactic and math reasoning problems with
3/8/32B-sized pre-trained LLMs, where we find verification is crucial for
scaling test-time compute.

摘要：儘管在擴展測試時間計算方面取得了重大進展，但社群中持續的辯論是如何擴展它以持續有效地改善擴展。大致有兩種方法：首先，提煉成功的搜尋或思考軌跡；其次，使用驗證（例如，0/1 結果獎勵、獎勵模型或驗證器）來指導強化學習 (RL) 和搜尋演算法。在本文中，我們證明使用基於 RL 或搜尋的驗證器為基礎 (VB) 方法微調 LLM 遠優於基於提煉或複製搜尋軌跡的驗證器免費 (VF) 方法，給定固定數量的計算/資料預算。此外，我們表明，當我們擴展測試時間計算（以輸出標記長度衡量）和訓練資料時，與 VB 相比，VF 方法的次最佳性擴展效果不佳，當基礎預先訓練的 LLM 在正確的解決方案軌跡上呈現異質分佈（例如，不同的長度、樣式等）並承認從其中取樣的軌跡上獎勵的分佈不尖銳時。我們使用反集中 [Erd\H{o}s，1945] 將此條件形式化。這暗示了一個更強的結果，即 VB 方法在漸近上擴展得更好，VB 和 VF 方法之間的效能差距隨著測試時間預算的增加而擴大。我們在具有 3/8/32B 大小的預先訓練 LLM 的教學和數學推理問題上對我們的理論進行實證驗證，我們發現驗證對於擴展測試時間計算至關重要。

##### **A-MEM: Agentic Memory for LLM Agents**
2502.12110v1 by Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang

While large language model (LLM) agents can effectively use external tools
for complex real-world tasks, they require memory systems to leverage
historical experiences. Current memory systems enable basic storage and
retrieval but lack sophisticated memory organization, despite recent attempts
to incorporate graph databases. Moreover, these systems' fixed operations and
structures limit their adaptability across diverse tasks. To address this
limitation, this paper proposes a novel agentic memory system for LLM agents
that can dynamically organize memories in an agentic way. Following the basic
principles of the Zettelkasten method, we designed our memory system to create
interconnected knowledge networks through dynamic indexing and linking. When a
new memory is added, we generate a comprehensive note containing multiple
structured attributes, including contextual descriptions, keywords, and tags.
The system then analyzes historical memories to identify relevant connections,
establishing links where meaningful similarities exist. Additionally, this
process enables memory evolution - as new memories are integrated, they can
trigger updates to the contextual representations and attributes of existing
historical memories, allowing the memory network to continuously refine its
understanding. Our approach combines the structured organization principles of
Zettelkasten with the flexibility of agent-driven decision making, allowing for
more adaptive and context-aware memory management. Empirical experiments on six
foundation models show superior improvement against existing SOTA baselines.
The source code is available at https://github.com/WujiangXu/AgenticMemory.

摘要：大型語言模型 (LLM) 代理雖然能有效地使用外部工具來執行複雜的真實世界任務，但它們需要記憶體系統來利用歷史經驗。目前的記憶體系統能進行基本的儲存和檢索，但缺乏精密的記憶體組織，儘管最近嘗試納入圖形資料庫。此外，這些系統固定的運作和結構限制了它們在不同任務中的適應性。為了解決這個限制，本文提出了一種新的代理記憶體系統，供 LLM 代理動態地以代理的方式組織記憶體。遵循 Zettelkasten 方法的基本原則，我們設計我們的記憶體系統，透過動態索引和連結來建立相互連結的知識網路。當加入新的記憶體時，我們會產生包含多個結構化屬性的綜合筆記，包括脈絡描述、關鍵字和標籤。然後，系統會分析歷史記憶體以找出相關連結，在有意義的相似性時建立連結。此外，這個程序能讓記憶體演化，因為當整合新的記憶體時，它們會觸發對現有歷史記憶體的脈絡表示和屬性的更新，讓記憶體網路能持續精進它的理解。我們的做法結合了 Zettelkasten 的結構化組織原則和代理驅動決策制定的靈活性，能進行更具適應性和脈絡感知的記憶體管理。在六個基礎模型上的經驗實驗顯示出比現有的 SOTA 基準線有顯著的進步。原始碼可以在 https://github.com/WujiangXu/AgenticMemory 找到。

##### **Personality Structured Interview for Large Language Model Simulation in Personality Research**
2502.12109v1 by Pengda Wang, Huiqi Zou, Hanjie Chen, Tianjun Sun, Ziang Xiao, Frederick L. Oswald

Although psychometrics researchers have recently explored the use of large
language models (LLMs) as proxies for human participants, LLMs often fail to
generate heterogeneous data with human-like diversity, which diminishes their
value in advancing social science research. To address these challenges, we
explored the potential of the theory-informed Personality Structured Interview
(PSI) as a tool for simulating human responses in personality research. In this
approach, the simulation is grounded in nuanced real-human interview
transcripts that target the personality construct of interest. We have provided
a growing set of 357 structured interview transcripts from a representative
sample, each containing an individual's response to 32 open-ended questions
carefully designed to gather theory-based personality evidence. Additionally,
grounded in psychometric research, we have summarized an evaluation framework
to systematically validate LLM-generated psychometric data. Results from three
experiments demonstrate that well-designed structured interviews could improve
human-like heterogeneity in LLM-simulated personality data and predict
personality-related behavioral outcomes (i.e., organizational citizenship
behaviors and counterproductive work behavior). We further discuss the role of
theory-informed structured interviews in LLM-based simulation and outline a
general framework for designing structured interviews to simulate human-like
data for psychometric research.

摘要：儘管心理測量研究人員最近已探討將大型語言模型 (LLM) 用作人類參與者的代理，但 LLM 經常無法產生具有類似人類多樣性的異質資料，這降低了它們在推進社會科學研究中的價值。為了應對這些挑戰，我們探討了理論知情的個性結構化訪談 (PSI) 作為模擬人格研究中人類反應的工具的潛力。在此方法中，模擬基於針對目標人格建構的細緻真實人類訪談記錄。我們提供了一組不斷增加的 357 個結構化訪談記錄，來自一個具代表性的樣本，每個記錄都包含個人對 32 個開放式問題的回答，這些問題經過仔細設計，用於收集基於理論的人格證據。此外，基於心理測量研究，我們總結了一個評估架構，以系統性驗證 LLM 生成的精神測量資料。三個實驗的結果表明，設計良好的結構化訪談可以改善 LLM 模擬的人格資料中類似人類的異質性，並預測與人格相關的行為結果（例如，組織公民行為和適得其反的工作行為）。我們進一步討論了理論知情的結構化訪談在基於 LLM 的模擬中的作用，並概述了一個通用框架，用於設計結構化訪談以模擬類似人類的資料，以進行心理測量研究。

##### **Using the Path of Least Resistance to Explain Deep Networks**
2502.12108v1 by Sina Salek, Joseph Enguehard

Integrated Gradients (IG), a widely used axiomatic path-based attribution
method, assigns importance scores to input features by integrating model
gradients along a straight path from a baseline to the input. While effective
in some cases, we show that straight paths can lead to flawed attributions. In
this paper, we identify the cause of these misattributions and propose an
alternative approach that treats the input space as a Riemannian manifold,
computing attributions by integrating gradients along geodesics. We call this
method Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we
introduce two techniques: a k-Nearest Neighbours-based approach for smaller
models and a Stochastic Variational Inference-based method for larger ones.
Additionally, we propose a new axiom, Strong Completeness, extending the axioms
satisfied by IG. We show that this property is desirable for attribution
methods and that GIG is the only method that satisfies it. Through experiments
on both synthetic and real-world data, we demonstrate that GIG outperforms
existing explainability methods, including IG.

摘要：整合梯度 (IG) 是一種廣泛使用的公理路徑歸因方法，它透過整合從基線到輸入的直線路徑上的模型梯度，為輸入特徵分配重要性分數。雖然在某些情況下有效，但我們表明直線路徑可能會導致錯誤的歸因。在本文中，我們找出這些錯誤歸因的原因，並提出將輸入空間視為黎曼流形的替代方法，透過整合測地線上的梯度來計算歸因。我們將此方法稱為測地線整合梯度 (GIG)。為了近似測地線路徑，我們引入了兩種技術：一種基於 k 最近鄰的方法，適用於較小的模型；一種基於隨機變異推論的方法，適用於較大的模型。此外，我們提出了新的公理，即強完整性，擴展了 IG 滿足的公理。我們表明此屬性對於歸因方法而言是理想的，並且 GIG 是唯一滿足此屬性的方法。透過對合成資料和真實世界資料進行的實驗，我們證明 GIG 優於現有的可解釋性方法，包括 IG。

##### **Relational Norms for Human-AI Cooperation**
2502.12102v1 by Brian D. Earp, Sebastian Porsdam Mann, Mateo Aboy, Edmond Awad, Monika Betzler, Marietjie Botes, Rachel Calcott, Mina Caraccio, Nick Chater, Mark Coeckelbergh, Mihaela Constantinescu, Hossein Dabbagh, Kate Devlin, Xiaojun Ding, Vilius Dranseika, Jim A. C. Everett, Ruiping Fan, Faisal Feroz, Kathryn B. Francis, Cindy Friedman, Orsolya Friedrich, Iason Gabriel, Ivar Hannikainen, Julie Hellmann, Arasj Khodadade Jahrome, Niranjan S. Janardhanan, Paul Jurcys, Andreas Kappes, Maryam Ali Khan, Gordon Kraft-Todd, Maximilian Kroner Dale, Simon M. Laham, Benjamin Lange, Muriel Leuenberger, Jonathan Lewis, Peng Liu, David M. Lyreskog, Matthijs Maas, John McMillan, Emilian Mihailov, Timo Minssen, Joshua Teperowski Monrad, Kathryn Muyskens, Simon Myers, Sven Nyholm, Alexa M. Owen, Anna Puzio, Christopher Register, Madeline G. Reinecke, Adam Safron, Henry Shevlin, Hayate Shimizu, Peter V. Treit, Cristina Voinea, Karen Yan, Anda Zahiu, Renwen Zhang, Hazem Zohny, Walter Sinnott-Armstrong, Ilina Singh, Julian Savulescu, Margaret S. Clark

How we should design and interact with social artificial intelligence depends
on the socio-relational role the AI is meant to emulate or occupy. In human
society, relationships such as teacher-student, parent-child, neighbors,
siblings, or employer-employee are governed by specific norms that prescribe or
proscribe cooperative functions including hierarchy, care, transaction, and
mating. These norms shape our judgments of what is appropriate for each
partner. For example, workplace norms may allow a boss to give orders to an
employee, but not vice versa, reflecting hierarchical and transactional
expectations. As AI agents and chatbots powered by large language models are
increasingly designed to serve roles analogous to human positions - such as
assistant, mental health provider, tutor, or romantic partner - it is
imperative to examine whether and how human relational norms should extend to
human-AI interactions. Our analysis explores how differences between AI systems
and humans, such as the absence of conscious experience and immunity to
fatigue, may affect an AI's capacity to fulfill relationship-specific functions
and adhere to corresponding norms. This analysis, which is a collaborative
effort by philosophers, psychologists, relationship scientists, ethicists,
legal experts, and AI researchers, carries important implications for AI
systems design, user behavior, and regulation. While we accept that AI systems
can offer significant benefits such as increased availability and consistency
in certain socio-relational roles, they also risk fostering unhealthy
dependencies or unrealistic expectations that could spill over into human-human
relationships. We propose that understanding and thoughtfully shaping (or
implementing) suitable human-AI relational norms will be crucial for ensuring
that human-AI interactions are ethical, trustworthy, and favorable to human
well-being.

摘要：<paragraph>我們應如何設計和與社交人工智慧互動，取決於人工智慧預期要模仿或扮演的社會關係角色。在人類社會中，師生、父母子女、鄰居、兄弟姐妹或雇主員工等關係受特定規範所支配，這些規範規定或禁止包括等級、照顧、交易和交配在內的合作功能。這些規範形塑我們對每個夥伴適當行為的判斷。例如，職場規範可能允許老闆對員工發號施令，但反之則不行，這反映了等級和交易的期望。隨著由大型語言模型驅動的人工智慧代理程式和聊天機器人日益被設計為服務類似於人類職位的角色，例如助理、心理健康提供者、導師或浪漫伴侶，審查人類關係規範是否以及如何延伸至人類與人工智慧的互動至關重要。我們的分析探討了人工智慧系統和人類之間的差異，例如缺乏意識體驗和對疲勞的免疫力，如何影響人工智慧履行特定關係功能和遵守相應規範的能力。這項分析是由哲學家、心理學家、關係科學家、倫理學家、法律專家和人工智慧研究人員共同合作的成果，對人工智慧系統設計、使用者行為和法規具有重要的意義。雖然我們接受人工智慧系統可以在某些社會關係角色中提供顯著的好處，例如增加可用性和一致性，但它們也可能助長不健康的依賴關係或不切實際的期望，這些期望可能會蔓延到人際關係中。我們提出，理解和深思熟慮地塑造（或實施）適當的人類與人工智慧關係規範，對於確保人類與人工智慧的互動具有倫理性、可信賴性和有利於人類福祉至關重要。</paragraph>

##### **A Study on Leveraging Search and Self-Feedback for Agent Reasoning**
2502.12094v1 by Karthikeyan K, Michelle Yuan, Elman Mansimov, Katerina Margatina, Anurag Pratik, Daniele Bonadiman, Monica Sunkara, Yi Zhang, Yassine Benajiba

Recent works have demonstrated that incorporating search during inference can
significantly improve reasoning capabilities of language agents. Some
approaches may make use of the ground truth or rely on model's own generated
feedback. The search algorithm uses this feedback to then produce values that
will update its criterion for exploring and exploiting various reasoning paths.
In this study, we investigate how search and model's self-feedback can be
leveraged for reasoning tasks. First, we explore differences in ground-truth
feedback and self-feedback during search for math reasoning. Second, we observe
limitations in applying search techniques to more complex tasks like
tool-calling and design domain-specific approaches to address these gaps. Our
experiments reveal challenges related to generalization when solely relying on
self-feedback during search. For search to work effectively, either access to
the ground-truth is needed or feedback mechanisms need to be carefully designed
for the specific task.

摘要：最近的研究表明，在推理过程中加入搜索功能可以显著提升语言代理的推理能力。一些方法可能会利用基本事实或依赖模型本身产生的反馈。搜索算法使用此反馈，然后生成值，以更新其探索和利用各种推理路径的标准。在本研究中，我们调查了如何利用搜索和模型的自反馈来进行推理任务。首先，我们探讨了数学推理搜索过程中基本事实反馈和自反馈的差异。其次，我们观察到在将搜索技术应用于更复杂的任务（如工具调用和设计特定于领域的解决方案）时存在的局限性，并提出针对这些差距的解决方案。我们的实验揭示了在搜索过程中仅依赖自反馈时与泛化相关的挑战。要使搜索有效，需要访问基本事实或需要针对特定任务仔细设计反馈机制。

##### **Meta-Statistical Learning: Supervised Learning of Statistical Inference**
2502.12088v1 by Maxime Peyrard, Kyunghyun Cho

This work demonstrates that the tools and principles driving the success of
large language models (LLMs) can be repurposed to tackle distribution-level
tasks, where the goal is to predict properties of the data-generating
distribution rather than labels for individual datapoints. These tasks
encompass statistical inference problems such as parameter estimation,
hypothesis testing, or mutual information estimation. Framing these tasks
within traditional machine learning pipelines is challenging, as supervision is
typically tied to individual datapoint. We propose meta-statistical learning, a
framework inspired by multi-instance learning that reformulates statistical
inference tasks as supervised learning problems. In this approach, entire
datasets are treated as single inputs to neural networks, which predict
distribution-level parameters. Transformer-based architectures, without
positional encoding, provide a natural fit due to their permutation-invariance
properties. By training on large-scale synthetic datasets, meta-statistical
models can leverage the scalability and optimization infrastructure of
Transformer-based LLMs. We demonstrate the framework's versatility with
applications in hypothesis testing and mutual information estimation, showing
strong performance, particularly for small datasets where traditional neural
methods struggle.

摘要：这项工作表明，推动大型语言模型 (LLM) 成功发展的工具和原则可以重新用于解决分布级别任务，其中目标是预测数据生成分布的属性，而不是单个数据点的标签。这些任务包括统计推断问题，例如参数估计、假设检验或互信息估计。在传统的机器学习管道中构建这些任务具有挑战性，因为监督通常与单个数据点相关联。我们提出了元统计学习，这是一个受多实例学习启发的框架，它将统计推断任务重新表述为监督学习问题。在此方法中，整个数据集被视为神经网络的单个输入，该神经网络预测分布级别参数。基于 Transformer 的架构在没有位置编码的情况下提供了自然拟合，因为它们具有置换不变性。通过在大型合成数据集上进行训练，元统计模型可以利用基于 Transformer 的 LLM 的可扩展性和优化基础设施。我们通过在假设检验和互信息估计中的应用展示了该框架的多功能性，显示出强大的性能，特别是对于传统神经方法难以处理的小型数据集。

##### **APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs**
2502.12085v1 by Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Sun Ao, Hao Zhou, Jie Zhou, Zhiyuan Liu, Maosong Sun

While long-context inference is crucial for advancing large language model
(LLM) applications, its prefill speed remains a significant bottleneck. Current
approaches, including sequence parallelism strategies and compute reduction
through approximate attention mechanisms, still fall short of delivering
optimal inference efficiency. This hinders scaling the inputs to longer
sequences and processing long-context queries in a timely manner. To address
this, we introduce APB, an efficient long-context inference framework that
leverages multi-host approximate attention to enhance prefill speed by reducing
compute and enhancing parallelism simultaneously. APB introduces a
communication mechanism for essential key-value pairs within a sequence
parallelism framework, enabling a faster inference speed while maintaining task
performance. We implement APB by incorporating a tailored FlashAttn kernel
alongside optimized distribution strategies, supporting diverse models and
parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x
compared with FlashAttn, RingAttn, and StarAttn, respectively, without any
observable task performance degradation. We provide the implementation and
experiment code of APB in https://github.com/thunlp/APB.

摘要：雖然長文本推理對於推進大型語言模型 (LLM) 應用至關重要，但其預填充速度仍然是一個重大的瓶頸。目前的各種方法，包括序列並行策略和透過近似注意力機制減少運算，仍然無法提供最佳的推理效率。這會阻礙將輸入擴展到更長的序列，以及及時處理長文本查詢。為了解決這個問題，我們引入了 APB，這是一個高效的長文本推理架構，它利用多主機近似注意力來減少運算並同時提高並行性，從而提高預填充速度。APB 在序列並行架構中引入了一個用於基本鍵值對的通訊機制，在維持任務效能的同時，實現更快的推理速度。我們透過整合一個量身打造的 FlashAttn 核心以及最佳化的分佈策略來實作 APB，支援各種模型和並行配置。與 FlashAttn、RingAttn 和 StarAttn 相比，APB 分別實現了高達 9.2 倍、4.2 倍和 1.6 倍的加速，同時沒有任何可觀察到的任務效能下降。我們在 https://github.com/thunlp/APB 中提供了 APB 的實作和實驗程式碼。

##### **VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues**
2502.12084v1 by Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R., Fung

Visually linking matching cues is a crucial ability in daily life, such as
identifying the same person in multiple photos based on their cues, even
without knowing who they are. Despite the extensive knowledge that
vision-language models (VLMs) possess, it remains largely unexplored whether
they are capable of performing this fundamental task. To address this, we
introduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can
Visually Link Matching cues, with 9 subtasks and over 3,000 test cases.
Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with
further analysis of various language-side and vision-side prompting methods,
leads to a total of eight key findings. We identify critical challenges in
models' ability to link visual cues, highlighting a significant performance gap
where even GPT-4o lags 34.80% behind humans. Based on these insights, we
advocate for (i) enhancing core visual capabilities to improve adaptability and
reduce reliance on prior knowledge, (ii) establishing clearer principles for
integrating language-based reasoning in vision-centric tasks to prevent
unnecessary biases, and (iii) shifting vision-text training paradigms toward
fostering models' ability to independently structure and infer relationships
among visual cues.

摘要：視覺連結匹配線索是日常生活中的關鍵能力，例如在多張照片中根據線索辨識同一個人，即使不知道他們是誰。儘管視覺語言模型 (VLM) 擁有廣泛的知識，但它們是否能執行這項基本任務，在很大程度上仍未被探討。為了解決這個問題，我們引入了 VLM$^2$-Bench，一個基準測試，旨在評估 VLM 是否能視覺連結匹配線索，包含 9 個子任務和超過 3,000 個測試案例。對八個開源 VLM 和 GPT-4o 的全面評估，以及對各種語言側和視覺側提示方法的進一步分析，得出總共八項關鍵發現。我們找出模型連結視覺線索能力的關鍵挑戰，強調一個顯著的效能差距，即使是 GPT-4o 也落後人類 34.80%。根據這些見解，我們提倡 (i) 提升核心視覺能力以改善適應性並減少對先驗知識的依賴，(ii) 為整合基於語言的推理到以視覺為中心的任務中建立更明確的原則，以防止不必要的偏見，以及 (iii) 將視覺文字訓練範例轉移到培養模型獨立建構和推論視覺線索之間關係的能力。

##### **AdaSplash: Adaptive Sparse Flash Attention**
2502.12082v1 by Nuno Gonçalves, Marcos Treviso, André F. T. Martins

The computational cost of softmax-based attention in transformers limits
their applicability to long-context tasks. Adaptive sparsity, of which
$\alpha$-entmax attention is an example, offers a flexible data-dependent
alternative, but existing implementations are inefficient and do not leverage
the sparsity to obtain runtime and memory gains. In this work, we propose
AdaSplash, which combines the efficiency of GPU-optimized algorithms with the
sparsity benefits of $\alpha$-entmax. We first introduce a hybrid
Halley-bisection algorithm, resulting in a 7-fold reduction in the number of
iterations needed to compute the $\alpha$-entmax transformation. Then, we
implement custom Triton kernels to efficiently handle adaptive sparsity.
Experiments with RoBERTa and ModernBERT for text classification and
single-vector retrieval, along with GPT-2 for language modeling, show that our
method achieves substantial improvements in runtime and memory efficiency
compared to existing $\alpha$-entmax implementations. It approaches -- and in
some cases surpasses -- the efficiency of highly optimized softmax
implementations like FlashAttention-2, enabling long-context training while
maintaining strong task performance.

摘要：基於 softmax 的注意力在 Transformer 中的運算成本限制了它們在長內容任務中的應用性。適應性稀疏性，其中 $\alpha$-entmax 注意力是一個例子，提供了一個靈活的資料相關替代方案，但現有的實作效率低下，且無法利用稀疏性來獲得執行時間和記憶體的增益。在這項工作中，我們提出了 AdaSplash，它結合了 GPU 最佳化演算法的效率和 $\alpha$-entmax 的稀疏性優點。我們首先引入了一個混合 Halley-二分法演算法，導致計算 $\alpha$-entmax 轉換所需的迭代次數減少了 7 倍。然後，我們實作自訂 Triton 核心，以有效處理適應性稀疏性。針對文字分類和單一向量擷取的 RoBERTa 和 ModernBERT，以及用於語言建模的 GPT-2 的實驗顯示，與現有的 $\alpha$-entmax 實作相比，我們的方法在執行時間和記憶體效率方面獲得了顯著的改善。它接近了 -- 在某些情況下超越了 -- 高度最佳化 softmax 實作（例如 FlashAttention-2）的效率，同時在維持強大任務效能的同時，能夠進行長內容訓練。

##### **Unhackable Temporal Rewarding for Scalable Video MLLMs**
2502.12081v1 by En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, Wenbing Tao

In the pursuit of superior video-processing MLLMs, we have encountered a
perplexing paradox: the "anti-scaling law", where more data and larger models
lead to worse performance. This study unmasks the culprit: "temporal hacking",
a phenomenon where models shortcut by fixating on select frames, missing the
full video narrative. In this work, we systematically establish a comprehensive
theory of temporal hacking, defining it from a reinforcement learning
perspective, introducing the Temporal Perplexity (TPL) score to assess this
misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework
to mitigate the temporal hacking. Both theoretically and empirically, TPL
proves to be a reliable indicator of temporal modeling quality, correlating
strongly with frame activation patterns. Extensive experiments reveal that UTR
not only counters temporal hacking but significantly elevates video
comprehension capabilities. This work not only advances video-AI systems but
also illuminates the critical importance of aligning proxy rewards with true
objectives in MLLM development.

摘要：在追求卓越的影片處理 MLLM 時，我們遭遇了一個令人費解的矛盾現象：「反規模化定律」，也就是更多資料和更大的模型會導致更差的效能。本研究揭露了罪魁禍首：「時間駭客」，這是一種模型透過專注於特定影格來簡化的現象，錯失了完整的影片敘事。在這項研究中，我們系統性地建立了一個關於時間駭客的全面理論，從強化學習的角度定義它，並引入了時間困惑度 (TPL) 分數來評估這種失衡，並提出了無法破解的時間獎勵 (UTR) 架構來減輕時間駭客現象。從理論和經驗上來說，TPL 被證明是時間建模品質的可靠指標，與影格啟動模式有很強的相關性。大量的實驗顯示，UTR 不僅對抗時間駭客，還能顯著提升影片理解能力。這項研究不僅推動了影片 AI 系統，也闡明了在 MLLM 開發中，將代理獎勵與真實目標對齊的重要性。

##### **Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation**
2502.12073v1 by Zhongyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo

Social media enables dynamic user engagement with trending topics, and recent
research has explored the potential of large language models (LLMs) for
response generation. While some studies investigate LLMs as agents for
simulating user behavior on social media, their focus remains on practical
viability and scalability rather than a deeper understanding of how well LLM
aligns with human behavior. This paper analyzes LLMs' ability to simulate
social media engagement through action guided response generation, where a
model first predicts a user's most likely engagement action-retweet, quote, or
rewrite-towards a trending post before generating a personalized response
conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and
DeepSeek-R1 in social media engagement simulation regarding a major societal
event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT
in action prediction, while few-shot prompting initially degrades the
prediction accuracy of LLMs with limited examples. However, in response
generation, few-shot LLMs achieve stronger semantic alignment with ground truth
posts.

摘要：社交媒體讓使用者能夠動態參與熱門話題，而最近的研究探索了大型語言模型 (LLM) 在回應生成方面的潛力。儘管有些研究將 LLM 視為模擬社交媒體使用者行為的代理，但其重點仍放在實務可行性和可擴充性，而非深入了解 LLM 如何與人類行為相符。本文分析了 LLM 透過動作引導回應生成來模擬社交媒體參與的能力，其中一個模型首先預測使用者最有可能的參與動作（轉推、引用或改寫）對熱門貼文的參與，然後根據預測的動作產生個人化回應。我們在 X 上討論的一個重大社會事件中，對 GPT-4o-mini、O1-mini 和 DeepSeek-R1 進行社交媒體參與模擬的基準測試。我們的研究結果顯示，零次學習 LLM 在動作預測方面表現不如 BERT，而少次學習提示最初會降低範例有限的 LLM 預測準確度。然而，在回應生成方面，少次學習 LLM 與真實貼文達到了更強的語義對齊。

##### **TokenSkip: Controllable Chain-of-Thought Compression in LLMs**
2502.12067v1 by Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, Wenjie Li

Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning
capabilities of large language models (LLMs). Recent advancements, such as
OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT
sequences during inference could further boost LLM reasoning performance.
However, due to the autoregressive nature of LLM decoding, longer CoT outputs
lead to a linear increase in inference latency, adversely affecting user
experience, particularly when the CoT exceeds 10,000 tokens. To address this
limitation, we analyze the semantic importance of tokens within CoT outputs and
reveal that their contributions to reasoning vary. Building on this insight, we
propose TokenSkip, a simple yet effective approach that enables LLMs to
selectively skip less important tokens, allowing for controllable CoT
compression. Extensive experiments across various models and tasks demonstrate
the effectiveness of TokenSkip in reducing CoT token usage while preserving
strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,
TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less
than a 0.4% performance drop.

摘要：<paragraph>鏈式思維 (CoT) 已被證明能有效提升大型語言模型 (LLM) 的推理能力。最近的進展，例如 OpenAI 的 o1 和 DeepSeek-R1，表明在推理過程中擴展 CoT 序列的長度可以進一步提升 LLM 的推理效能。然而，由於 LLM 解碼的自動回歸特性，較長的 CoT 輸出會導致推理延遲線性增加，對使用者體驗造成負面影響，特別是在 CoT 超過 10,000 個符號時。為了解決這個限制，我們分析了 CoT 輸出中符號的語義重要性，並揭示了它們對推理的貢獻度不同。基於這個見解，我們提出了 TokenSkip，一種簡單但有效的技術，使 LLM 能有選擇地略過較不重要的符號，從而實現可控的 CoT 壓縮。跨越各種模型和任務的廣泛實驗證明了 TokenSkip 在減少 CoT 符號使用量同時保持強大推理效能方面的有效性。值得注意的是，當應用於 Qwen2.5-14B-Instruct 時，TokenSkip 將 GSM8K 上的推理符號減少了 40%（從 313 個減少到 181 個），效能下降不到 0.4%。</paragraph>

##### **CONSTRUCTA: Automating Commercial Construction Schedules in Fabrication Facilities with Large Language Models**
2502.12066v1 by Yifan Zhang, Xue Yang

Automating planning with LLMs presents transformative opportunities for
traditional industries, yet remains underexplored. In commercial construction,
the complexity of automated scheduling often requires manual intervention to
ensure precision. We propose CONSTRUCTA, a novel framework leveraging LLMs to
optimize construction schedules in complex projects like semiconductor
fabrication. CONSTRUCTA addresses key challenges by: (1) integrating
construction-specific knowledge through static RAG; (2) employing
context-sampling techniques inspired by architectural expertise to provide
relevant input; and (3) deploying Construction DPO to align schedules with
expert preferences using RLHF. Experiments on proprietary data demonstrate
performance improvements of +42.3% in missing value prediction, +79.1% in
dependency analysis, and +28.9% in automated planning compared to baseline
methods, showcasing its potential to revolutionize construction workflows and
inspire domain-specific LLM advancements.

摘要：利用 LLM 自動化規劃為傳統產業帶來轉型契機，但仍有待進一步探索。在商業建築中，自動化排程的複雜性通常需要手動介入以確保精確度。我們提出 CONSTRUCTA，一個利用 LLM 優化複雜專案（如半導體製造）建築排程的新穎架構。CONSTRUCTA 透過下列方式解決關鍵挑戰：(1) 整合靜態 RAG 的建築特定知識；(2) 採用受建築專業知識啟發的脈絡取樣技術，提供相關輸入；(3) 部署建築 DPO，使用 RLHF 將排程與專家偏好對齊。專利數據的實驗顯示，與基準方法相比，遺失值預測的效能提升 +42.3%、相依性分析提升 +79.1%、自動化規劃提升 +28.9%，展示其革新建築工作流程和激勵領域特定 LLM 進展的潛力。

##### **Formalizing Complex Mathematical Statements with LLMs: A Study on Mathematical Definitions**
2502.12065v1 by Lan Zhang, Marco Valentino, Andre Freitas

Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge
the gap between informal mathematics and formal languages through
autoformalization. However, it is still unclear how well LLMs generalize to
sophisticated and naturally occurring mathematical statements. To address this
gap, we investigate the task of autoformalizing real-world mathematical
definitions -- a critical component of mathematical discourse. Specifically, we
introduce two novel resources for autoformalisation, collecting definitions
from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically
evaluate a range of LLMs, analyzing their ability to formalize definitions into
Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'
performance including refinement through external feedback from Proof
Assistants, and formal definition grounding, where we guide LLMs through
relevant contextual elements from formal mathematical libraries. Our findings
reveal that definitions present a greater challenge compared to existing
benchmarks, such as miniF2F. In particular, we found that LLMs still struggle
with self-correction, and aligning with relevant mathematical libraries. At the
same time, structured refinement methods and definition grounding strategies
yield notable improvements of up to 16% on self-correction capabilities and 43%
on the reduction of undefined errors, highlighting promising directions for
enhancing LLM-based autoformalization in real-world scenarios.

摘要：由於語言能力，LLM 提供了一個機會，透過自動形式化來彌合非正式數學和形式語言之間的差距。然而，LLM 在多麼精巧且自然發生的數學陳述中概化，這仍不清楚。為了解決這個差距，我們探討了自動形式化真實世界數學定義的任務，這是數學論述中的關鍵組成部分。具體來說，我們介紹了自動形式化的兩個新資源，收集來自維基百科（Def_Wiki）和 arXiv 論文（Def_ArXiv）的定義。然後，我們系統性地評估了一系列 LLM，分析它們將定義形式化為 Isabelle/HOL 的能力。此外，我們探討了增強 LLM 效能的策略，包括透過證明輔助工具的外部回饋進行精煉，以及形式定義基礎，其中我們透過形式數學函式庫中的相關脈絡元素來引導 LLM。我們的發現顯示，與現有的基準（例如 miniF2F）相比，定義提出了更大的挑戰。特別是，我們發現 LLM 在自我修正和與相關數學函式庫對齊方面仍然有困難。同時，結構化的精煉方法和定義基礎策略在自我修正能力上產生了顯著的改善，高達 16%，在減少未定義錯誤方面改善了 43%，突顯了在真實世界場景中增強基於 LLM 的自動形式化的有希望的方向。

##### **AI-generated Text Detection with a GLTR-based Approach**
2502.12064v1 by Lucía Yan Wu, Isabel Segura-Bedmar

The rise of LLMs (Large Language Models) has contributed to the improved
performance and development of cutting-edge NLP applications. However, these
can also pose risks when used maliciously, such as spreading fake news, harmful
content, impersonating individuals, or facilitating school plagiarism, among
others. This is because LLMs can generate high-quality texts, which are
challenging to differentiate from those written by humans. GLTR, which stands
for Giant Language Model Test Room and was developed jointly by the MIT-IBM
Watson AI Lab and HarvardNLP, is a visual tool designed to help detect
machine-generated texts based on GPT-2, that highlights the words in text
depending on the probability that they were machine-generated. One limitation
of GLTR is that the results it returns can sometimes be ambiguous and lead to
confusion. This study aims to explore various ways to improve GLTR's
effectiveness for detecting AI-generated texts within the context of the
IberLef-AuTexTification 2023 shared task, in both English and Spanish
languages. Experiment results show that our GLTR-based GPT-2 model overcomes
the state-of-the-art models on the English dataset with a macro F1-score of
80.19%, except for the first ranking model (80.91%). However, for the Spanish
dataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%
compared to the top-performing model.

摘要：大型語言模型 (LLM) 的興起有助於改進尖端 NLP 應用程式的效能和開發。不過，這些應用程式若遭惡意使用，例如散布假新聞、有害內容、冒充個人或協助學校抄襲等，也可能造成風險。這是因為 LLM 可以產生高品質的文字，而這些文字難以與人類所寫的文字區分。GLTR（代表大型語言模型測試室）是由麻省理工學院-IBM Watson AI 實驗室和 HarvardNLP 共同開發的視覺工具，旨在協助偵測基於 GPT-2 的機器產生的文字，它會根據文字中每個字詞機器產生的機率來標示。GLTR 的一個限制在於，它回傳的結果有時可能模稜兩可，容易造成混淆。本研究旨在探討各種方法來改善 GLTR 在 IberLef-AuTexTification 2023 共享任務中偵測 AI 生成的文字的效能，任務中包含英文和西班牙文兩種語言。實驗結果顯示，我們的基於 GLTR 的 GPT-2 模型在英文資料集上以 80.19% 的巨觀 F1 分數超越了最先進的模型，僅次於第一名排名模型 (80.91%)。不過，在西班牙文資料集上，我們獲得的巨觀 F1 分數為 66.20%，與表現最佳的模型相比，相差 4.57%。

##### **Culture is Not Trivia: Sociocultural Theory for Cultural NLP**
2502.12057v1 by Naitian Zhou, David Bamman, Isaac L. Bleaman

The field of cultural NLP has recently experienced rapid growth, driven by a
pressing need to ensure that language technologies are effective and safe
across a pluralistic user base. This work has largely progressed without a
shared conception of culture, instead choosing to rely on a wide array of
cultural proxies. However, this leads to a number of recurring limitations:
coarse national boundaries fail to capture nuanced differences that lay within
them, limited coverage restricts datasets to only a subset of usually
highly-represented cultures, and a lack of dynamicity results in static
cultural benchmarks that do not change as culture evolves. In this position
paper, we argue that these methodological limitations are symptomatic of a
theoretical gap. We draw on a well-developed theory of culture from
sociocultural linguistics to fill this gap by 1) demonstrating in a case study
how it can clarify methodological constraints and affordances, 2) offering
theoretically-motivated paths forward to achieving cultural competence, and 3)
arguing that localization is a more useful framing for the goals of much
current work in cultural NLP.

摘要：文化 NLP 領域最近經歷了快速成長，這是因為迫切需要確保語言技術對於多元化的使用者基礎而言是有效且安全的。這項工作在很大程度上沒有文化共識，而是選擇依賴各種文化代理。然而，這導致了許多重複性的限制：粗略的國家界線無法捕捉到其中的細微差異，有限的涵蓋範圍將資料集限制在通常高度代表的文化子集，而且缺乏動態性導致靜態文化基準無法隨著文化演變而改變。在這篇立場文件中，我們認為這些方法論限制是理論差距的徵兆。我們從社會文化語言學中汲取一個發展良好的文化理論，透過 1) 在個案研究中展示它如何釐清方法論限制和可負擔性，2) 提供理論上合理的途徑來實現文化能力，以及 3) 主張在地化對於文化 NLP 中許多當前工作的目標而言是一個更有用的框架，來填補這個差距。

##### **Designing Role Vectors to Improve LLM Inference Behaviour**
2502.12055v1 by Daniele Potertì, Andrea Seveso, Fabio Mercorio

The influence of personas on Large Language Models (LLMs) has been widely
studied, yet their direct impact on performance remains uncertain. This work
explores a novel approach to guiding LLM behaviour through role vectors, an
alternative to persona-based prompting. We construct 29 role vectors derived
from model activations and evaluate their impact on benchmark performance
across multiple domains. Our analysis investigates whether these vectors can
effectively steer models toward domain-specific expertise. We measure two key
interventions: (i) activation addition, which reinforces role-specific
directions, and (ii) directional ablation, which removes them. Results on
well-established benchmarks indicate that role vectors do, in fact, influence
model behaviour, improving task performance in relevant domains while
marginally affecting unrelated tasks. This, in turn, suggests that manipulating
internal model representations has a greater impact on outcomes than
persona-based prompting.

摘要：大型語言模型 (LLM) 中角色的影響已被廣泛研究，但它們對效能的直接影響仍然不確定。本研究探討了一種透過角色向量引導 LLM 行為的新方法，這是一種基於角色提示的替代方案。我們從模型激活中建構了 29 個角色向量，並評估它們對多個領域基準效能的影響。我們的分析探討了這些向量是否能有效地引導模型朝向特定領域的專業知識。我們衡量了兩個關鍵干預措施：(i) 激活新增，它加強了特定角色的方向，以及 (ii) 方向消融，它移除了這些方向。在既定基準上的結果表明，角色向量確實會影響模型行為，在相關領域中改善任務效能，同時對不相關任務的影響很小。這反過來表明，操縱內部模型表示對結果的影響比基於角色的提示更大。

##### **PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning**
2502.12054v1 by Xinyu Zhang, Yuxuan Dong, Yanrui Wu, Jiaxing Huang, Chengyou Jia, Basura Fernando, Mike Zheng Shou, Lingling Zhang, Jun Liu

Large language models demonstrate remarkable capabilities across various
domains, especially mathematics and logic reasoning. However, current
evaluations overlook physics-based reasoning - a complex task requiring physics
theorems and constraints. We present PhysReason, a 1,200-problem benchmark
comprising knowledge-based (25%) and reasoning-based (75%) problems, where the
latter are divided into three difficulty levels (easy, medium, hard). Notably,
problems require an average of 8.1 solution steps, with hard requiring 15.6,
reflecting the complexity of physics-based reasoning. We propose the Physics
Solution Auto Scoring Framework, incorporating efficient answer-level and
comprehensive step-level evaluations. Top-performing models like Deepseek-R1,
Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on
answer-level evaluation, with performance dropping from knowledge questions
(75.11%) to hard problems (31.95%). Through step-level evaluation, we
identified four key bottlenecks: Physics Theorem Application, Physics Process
Understanding, Calculation, and Physics Condition Analysis. These findings
position PhysReason as a novel and comprehensive benchmark for evaluating
physics-based reasoning capabilities in large language models. Our code and
data will be published at https:/dxzxy12138.github.io/PhysReason.

摘要：大型語言模型展示了在各個領域的非凡能力，特別是數學和邏輯推理。然而，目前的評估忽略了基於物理的推理——這是一項複雜的任務，需要物理定理和約束。我們提出了 PhysReason，一個包含 1,200 題的基準，包含基於知識的（25%）和基於推理的（75%）問題，後者分為三個難度等級（容易、中等、困難）。值得注意的是，問題需要平均 8.1 個求解步驟，困難的需要 15.6 個，反映了基於物理的推理的複雜性。我們提出了物理解決方案自動評分框架，結合了高效的答案級別和全面的步驟級別評估。Deepseek-R1、Gemini-2.0-Flash-Thinking 和 o3-mini-high 等表現最佳的模型在答案級別評估中獲得低於 60% 的分數，性能從知識問題（75.11%）下降到困難問題（31.95%）。通過步驟級別評估，我們確定了四個關鍵瓶頸：物理定理應用、物理過程理解、計算和物理條件分析。這些發現將 PhysReason 定位為一個新穎且全面的基準，用於評估大型語言模型中基於物理的推理能力。我們的代碼和數據將發布在 https:/dxzxy12138.github.io/PhysReason。

##### **A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability**
2502.12052v1 by Xinyu Hu, Mingqi Gao, Li Lin, Zhenghan Yu, Xiaojun Wan

In NLG meta-evaluation, evaluation metrics are typically assessed based on
their consistency with humans. However, we identify some limitations in
traditional NLG meta-evaluation approaches, such as issues in handling human
ratings and ambiguous selections of correlation measures, which undermine the
effectiveness of meta-evaluation. In this work, we propose a dual-perspective
NLG meta-evaluation framework that focuses on different evaluation
capabilities, thereby providing better interpretability. In addition, we
introduce a method of automatically constructing the corresponding benchmarks
without requiring new human annotations. Furthermore, we conduct experiments
with 16 representative LLMs as the evaluators based on our proposed framework,
comprehensively analyzing their evaluation performance from different
perspectives.

摘要：在 NLG 元評估中，評估指標通常根據其與人類的一致性進行評估。然而，我們在傳統的 NLG 元評估方法中發現了一些限制，例如在處理人類評分和模稜兩可的相關性測量選擇方面存在問題，這會損害元評估的有效性。在這項工作中，我們提出了一個雙視角 NLG 元評估框架，該框架專注於不同的評估能力，從而提供更好的可解釋性。此外，我們引入了一種自動構建相應基準的方法，而不需要新的手動註釋。此外，我們根據我們提出的框架對 16 個具有代表性的 LLM 作為評估器進行了實驗，從不同的角度全面分析了它們的評估性能。

##### **How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines**
2502.12051v1 by Ayan Sengupta, Yash Goel, Tanmoy Chakraborty

Neural scaling laws have revolutionized the design and optimization of
large-scale AI models by revealing predictable relationships between model
size, dataset volume, and computational resources. Early research established
power-law relationships in model performance, leading to compute-optimal
scaling strategies. However, recent studies highlighted their limitations
across architectures, modalities, and deployment contexts. Sparse models,
mixture-of-experts, retrieval-augmented learning, and multimodal models often
deviate from traditional scaling patterns. Moreover, scaling behaviors vary
across domains such as vision, reinforcement learning, and fine-tuning,
underscoring the need for more nuanced approaches. In this survey, we
synthesize insights from over 50 studies, examining the theoretical
foundations, empirical findings, and practical implications of scaling laws. We
also explore key challenges, including data efficiency, inference scaling, and
architecture-specific constraints, advocating for adaptive scaling strategies
tailored to real-world applications. We suggest that while scaling laws provide
a useful guide, they do not always generalize across all architectures and
training strategies.

摘要：神經網路規模定律透過揭示模型規模、資料集體積和計算資源之間可預測的關係，徹底革新了大型 AI 模型的設計和最佳化。早期研究建立了模型效能中的冪次定律關係，進而產生最佳化的運算規模策略。然而，最近的研究突出了它們在架構、模態和部署脈絡中的限制。稀疏模型、專家混合、檢索增強式學習和多模態模型通常偏離傳統的規模模式。此外，規模行為因視覺、強化學習和微調等領域而異，強調需要更細緻的方法。在這項調查中，我們綜合了 50 多項研究的見解，探討規模定律的理論基礎、實證發現和實務意涵。我們也探討了關鍵挑戰，包括資料效率、推論規模和特定於架構的限制，提倡針對實際應用量身打造的自適應規模策略。我們建議，儘管規模定律提供了有用的指南，但它們並不總是能概括到所有架構和訓練策略。

##### **SpeechT: Findings of the First Mentorship in Speech Translation**
2502.12050v1 by Yasmin Moslem, Juan Julián Cea Morán, Mariano Gonzalez-Gomez, Muhammad Hazim Al Farouq, Farah Abdou, Satarupa Deb

This work presents the details and findings of the first mentorship in speech
translation (SpeechT), which took place in December 2024 and January 2025. To
fulfil the requirements of the mentorship, the participants engaged in key
activities, including data preparation, modelling, and advanced research.

摘要：本研究報告了 2024 年 12 月和 2025 年 1 月舉行的首次語音翻譯 (SpeechT) 指導計畫的詳細資訊和發現。為了滿足指導計畫的要求，參與者參與了關鍵活動，包括資料準備、建模和進階研究。

##### **A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond**
2502.12048v1 by Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury

Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial
Intelligence (GenAI) has opened new frontiers in brain signal decoding,
enabling assistive communication, neural representation learning, and
multimodal integration. BCIs, particularly those leveraging
Electroencephalography (EEG), provide a non-invasive means of translating
neural activity into meaningful outputs. Recent advances in deep learning,
including Generative Adversarial Networks (GANs) and Transformer-based Large
Language Models (LLMs), have significantly improved EEG-based generation of
images, text, and speech. This paper provides a literature review of the
state-of-the-art in EEG-based multimodal generation, focusing on (i)
EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and
Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based
language models and contrastive learning methods. Additionally, we discuss the
emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We
highlight key datasets, use cases, challenges, and EEG feature encoding methods
that underpin generative approaches. By providing a structured overview of
EEG-based generative AI, this survey aims to equip researchers and
practitioners with insights to advance neural decoding, enhance assistive
technologies, and expand the frontiers of brain-computer interaction.

摘要：腦機介面（BCIs）與生成式人工智慧（GenAI）的整合為腦信號解碼開啟了新領域，能協助溝通、神經表徵學習與多模式整合。BCIs，特別是利用腦電圖（EEG）的 BCIs，提供了一種非侵入性的方式，可將神經活動轉換為有意義的輸出。深度學習的最新進展，包括生成對抗網路（GANs）與基於 Transformer 的大型語言模型（LLMs），大幅改善了基於 EEG 的影像、文字與語音生成。本文提供了一份基於 EEG 的多模式生成的最新文獻回顧，重點在於（一）透過 GANs、變異自動編碼器（VAEs）與擴散模型進行 EEG 到影像的生成，以及（二）利用基於 Transformer 的語言模型與對比學習方法進行 EEG 到文字的生成。此外，我們討論了 EEG 到語音合成的新興領域，這是一個不斷演進的多模式領域。我們重點介紹了關鍵的資料集、用例、挑戰與支撐生成方法的 EEG 特徵編碼方法。透過提供基於 EEG 的生成式 AI 的結構化概觀，本調查旨在為研究人員與從業人員提供見解，以推進神經解碼、增強輔助技術並擴展腦機互動的領域。

##### **KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs**
2502.12029v1 by Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, Xiangyang Li

Large language models (LLMs) have demonstrated remarkable capabilities in
various complex tasks, yet they still suffer from hallucinations. Introducing
external knowledge, such as knowledge graph, can enhance the LLMs' ability to
provide factual answers. LLMs have the ability to interactively explore
knowledge graphs. However, most approaches have been affected by insufficient
internal knowledge excavation in LLMs, limited generation of trustworthy
knowledge reasoning paths, and a vague integration between internal and
external knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large
model framework driven by the collaboration of internal and external knowledge.
It relies on the internal knowledge of the LLM to guide the exploration of
interpretable directed subgraphs in external knowledge graphs, better
integrating the two knowledge sources for more accurate reasoning. Extensive
experiments on multiple real-world datasets confirm the superiority of
KnowPath.

摘要：大型語言模型 (LLM) 已在各種複雜任務中展現出卓越的能力，但仍會出現幻覺。引入外部知識（例如知識圖譜）可以增強 LLM 提供事實答案的能力。LLM 有能力互動式地探索知識圖譜。然而，大多數方法都受到 LLM 中內部知識挖掘不足、可信賴知識推理路徑生成受限，以及內部和外部知識之間的整合模糊的影響。因此，我們提出 KnowPath，這是一個由內部和外部知識的協作驅動的知識增強型大型模型框架。它依賴於 LLM 的內部知識來指導對外部知識圖譜中可解釋的有向子圖的探索，更好地整合兩個知識來源以進行更準確的推理。對多個真實世界資料集進行的大量實驗證實了 KnowPath 的優越性。

##### **SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities**
2502.12025v1 by Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran

Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage
long chain-of-thought (CoT) reasoning to generate structured intermediate
steps, enhancing their reasoning capabilities. However, long CoT does not
inherently guarantee safe outputs, potentially leading to harmful consequences
such as the introduction of security vulnerabilities in code or the spread of
misinformation. Current research on large language model (LLM) safety usually
focuses on short-answer responses, overlooking the long CoT style outputs of
LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First,
we investigate safety evaluators calibrated against human annotations. Using
our newly developed metrics, we thoroughly assess the safety of 12
state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results
show that LRMs are not safe compared to their reasoning advance. Further, we
perform a fine-grained analysis of the reasoning trace and final answer. We
find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can
improve model safety without additional training. However, these strategies
either use constrained reasoning traces or incur high inference costs. To
better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind
safety training dataset in CoT style. We fine-tune two LRMs with SafeChain,
showing that it not only enhances model safety but also preserves performance
across 6 reasoning benchmarks.

摘要：新興的大型推理模型（LRM），例如 DeepSeek-R1 模型，利用長鏈思考（CoT）推理來生成結構化的中間步驟，增強其推理能力。然而，長 CoT 本質上並不能保證安全的輸出，可能會導致有害的後果，例如在程式碼中引入安全漏洞或散佈錯誤訊息。目前針對大型語言模型（LLM）安全性的研究通常側重於簡短的回答回應，忽略了 LRM 的長 CoT 風格輸出。為了彌補這個差距，我們對 LRM 安全性進行系統性研究。首先，我們研究根據人類註解校正的安全評估器。使用我們新開發的指標，我們徹底評估了 12 個最先進的 LRM 在 StrongReject 和 WildJailbreak 資料集上的安全性。我們的結果表明，與其推理進度相比，LRM 並不安全。此外，我們對推理軌跡和最終答案進行了細粒度分析。我們發現三種解碼策略（ZeroThink、LessThink 和 MoreThink）可以在不額外訓練的情況下提高模型安全性。然而，這些策略要么使用受約束的推理軌跡，要么會產生高昂的推論成本。為了進一步加強 LRM 安全性，我們引入了 SafeChain，這是第一個 CoT 風格的安全訓練資料集。我們使用 SafeChain 微調了兩個 LRM，表明它不僅增強了模型安全性，而且在 6 個推理基準測試中都保持了效能。

##### **Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving**
2502.12022v1 by Xin Xu, Yan Xu, Tianhao Chen, Yuchen Yan, Chengwu Liu, Zaoyu Chen, Yufei Wang, Yichun Yin, Yasheng Wang, Lifeng Shang, Qun Liu

Existing approaches to mathematical reasoning with large language models
(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated
Reasoning (TIR) for precise computation. While efforts have been made to
combine these methods, they primarily rely on post-selection or predefined
strategies, leaving an open question: whether LLMs can autonomously adapt their
reasoning strategy based on their inherent capabilities. In this work, we
propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework
that enables LLMs to personalize their reasoning strategy spontaneously,
aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware
data selection during supervised fine-tuning (SFT) to tailor training data to
the model's unique abilities. This approach equips LLMs to autonomously
determine and apply the appropriate reasoning strategy at test time. We
evaluate TATA through extensive experiments on six mathematical reasoning
benchmarks, using both general-purpose and math-specialized LLMs. Empirical
results demonstrate that TATA effectively combines the complementary strengths
of CoT and TIR, achieving superior or comparable performance with improved
inference efficiency compared to TIR alone. Further analysis underscores the
critical role of aptitude-aware data selection in enabling LLMs to make
effective and adaptive reasoning decisions and align reasoning strategies with
model capabilities.

摘要：現有的數學推理方法使用大型語言模型 (LLM) 仰賴思考鏈 (CoT) 來達到泛化性，或使用工具整合推理 (TIR) 來進行精確運算。儘管已有人嘗試結合這些方法，但它們主要依賴後選取或預定義策略，留下一個開放性的問題：LLM 是否能根據其內在能力自主調整其推理策略。在這項工作中，我們提出 TATA（根據其天賦來教授 LLM），這是一個適應性架構，讓 LLM 能夠自發地個人化其推理策略，並與其內在的天賦保持一致。TATA 在監督微調 (SFT) 期間納入了基礎 LLM 感知資料選取，以根據模型的獨特能力調整訓練資料。此方法讓 LLM 能夠在測試時自主決定並套用適當的推理策略。我們透過對六個數學推理基準進行廣泛的實驗來評估 TATA，使用通用和數學專用 LLM。經驗結果顯示，TATA 有效地結合了 CoT 和 TIR 的互補優勢，與僅使用 TIR 相比，達到了優越或相當的效能，並改善了推論效率。進一步的分析強調了天賦感知資料選取在讓 LLM 能夠做出有效且適應性的推理決策，並將推理策略與模型能力保持一致時所扮演的關鍵角色。

##### **Atom of Thoughts for Markov LLM Test-Time Scaling**
2502.12018v1 by Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo

Large Language Models (LLMs) achieve superior performance through
training-time scaling, and test-time scaling further enhances their
capabilities by conducting effective reasoning during inference. However, as
the scale of reasoning increases, existing test-time scaling methods suffer
from accumulated historical information, which not only wastes computational
resources but also interferes with effective reasoning. To address this issue,
we observe that complex reasoning progress is often achieved by solving a
sequence of independent subquestions, each being self-contained and verifiable.
These subquestions are essentially atomic questions, relying primarily on their
current state rather than accumulated history, similar to the memoryless
transitions in a Markov process. Based on this observation, we propose Atom of
Thoughts (AoT), where each state transition in the reasoning process consists
of decomposing the current question into a dependency-based directed acyclic
graph and contracting its subquestions, forming a new atomic question state.
This iterative decomposition-contraction process continues until reaching
directly solvable atomic questions, naturally realizing Markov transitions
between question states. Furthermore, these atomic questions can be seamlessly
integrated into existing test-time scaling methods, enabling AoT to serve as a
plug-in enhancement for improving reasoning capabilities. Experiments across
six benchmarks demonstrate the effectiveness of AoT both as a standalone
framework and a plug-in enhancement. Notably, on HotpotQA, when applied to
gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and
DeepSeek-R1 by 10.6%. The code will be available at
https://github.com/qixucen/atom.

摘要：大型語言模型 (LLM) 透過訓練時間擴充來達成卓越的效能，而測試時間擴充透過在推論期間進行有效的推理，進一步提升其能力。然而，隨著推理規模的擴大，現有的測試時間擴充方法會受到累積的歷史資訊影響，這不僅會浪費運算資源，還會干擾有效的推理。為了解決這個問題，我們觀察到複雜的推理進程通常是透過解決一系列獨立的子問題來達成，每個子問題都是獨立且可驗證的。這些子問題本質上是原子問題，主要依賴於它們的當前狀態，而不是累積的歷史，類似於馬可夫過程中的無記憶轉換。基於這個觀察，我們提出了思想原子 (AoT)，其中推理過程中每個狀態轉換都包含將當前問題分解為基於依賴關係的有向無環圖，並收縮其子問題，形成新的原子問題狀態。這個反覆的分解收縮過程會持續進行，直到達到可直接解決的原子問題，自然地實現問題狀態之間的馬可夫轉換。此外，這些原子問題可以無縫整合到現有的測試時間擴充方法中，讓 AoT 可以作為外掛程式強化功能，以改善推理能力。橫跨六個基準的實驗證明了 AoT 作為獨立架構和外掛程式強化的有效性。值得注意的是，在 HotpotQA 上，當應用於 gpt-4o-mini 時，AoT 達到了 80.6% 的 F1 分數，比 o3-mini 高出 3.4%，比 DeepSeek-R1 高出 10.6%。程式碼將在 https://github.com/qixucen/atom 上提供。

##### **Demographic Attributes Prediction from Speech Using WavLM Embeddings**
2502.12007v1 by Yuchen Yang, Thomas Thebaud, Najim Dehak

This paper introduces a general classifier based on WavLM features, to infer
demographic characteristics, such as age, gender, native language, education,
and country, from speech. Demographic feature prediction plays a crucial role
in applications like language learning, accessibility, and digital forensics,
enabling more personalized and inclusive technologies. Leveraging pretrained
models for embedding extraction, the proposed framework identifies key acoustic
and linguistic fea-tures associated with demographic attributes, achieving a
Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy
for gender classification across various datasets. Our system improves upon
existing models by up to relative 30% in MAE and up to relative 10% in accuracy
and F1 scores across tasks, leveraging a diverse range of datasets and large
pretrained models to ensure robustness and generalizability. This study offers
new insights into speaker diversity and provides a strong foundation for future
research in speech-based demographic profiling.

摘要：本文介紹一個基於 WavLM 特徵的一般分類器，用於從語音中推斷人口特徵，例如年齡、性別、母語、教育和國家。人口特徵預測在語言學習、無障礙性和數位鑑識等應用中扮演著至關重要的角色，能實現更個人化且包容性的技術。利用預先訓練的模型進行嵌入式萃取，提出的架構識別與人口屬性相關的主要音訊和語言特徵，在年齡預測中達到 4.94 的平均絕對誤差 (MAE)，在各種資料集中的性別分類中準確率超過 99.81%。我們的系統在平均絕對誤差上比現有模型提升了相對 30%，在準確率和 F1 分數上提升了相對 10%，利用各種資料集和大型預先訓練模型來確保穩健性和概括性。本研究提供了對說話者多元性的新見解，並為未來基於語音的人口特徵分析研究奠定了堅實的基礎。

##### **Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition**
2502.12001v1 by Thibault Rousset, Taisei Kakibuchi, Yusuke Sasaki, Yoshihide Nomura

This paper investigates the integration of technical vocabulary in merged
language models. We explore the knowledge transfer mechanisms involved when
combining a general-purpose language-specific model with a domain-specific
model, focusing on the resulting model's comprehension of technical jargon. Our
experiments analyze the impact of this merging process on the target model's
proficiency in handling specialized terminology. We present a quantitative
evaluation of the performance of the merged model, comparing it with that of
the individual constituent models. The findings offer insights into the
effectiveness of different model merging methods for enhancing domain-specific
knowledge and highlight potential challenges and future directions in
leveraging these methods for cross-lingual knowledge transfer in Natural
Language Processing.

摘要：本文探討了技術詞彙在合併語言模型中的整合。我們探討了結合一般用途語言特定模型與特定領域模型時所涉及的知識轉移機制，重點在於所產生模型對技術術語的理解。我們的實驗分析了此合併程序對目標模型處理專業術語能力的影響。我們提出了合併模型效能的量化評估，並將其與個別組成模型的效能進行比較。這些發現提供了見解，說明了不同模型合併方法在增強特定領域知識方面的效能，並強調了利用這些方法進行自然語言處理中跨語言知識轉移的潛在挑戰和未來方向。

##### **Presumed Cultural Identity: How Names Shape LLM Responses**
2502.11995v1 by Siddhesh Pawar, Arnav Arora, Lucie-Aimée Kaffee, Isabelle Augenstein

Names are deeply tied to human identity. They can serve as markers of
individuality, cultural heritage, and personal history. However, using names as
a core indicator of identity can lead to over-simplification of complex
identities. When interacting with LLMs, user names are an important point of
information for personalisation. Names can enter chatbot conversations through
direct user input (requested by chatbots), as part of task contexts such as CV
reviews, or as built-in memory features that store user information for
personalisation. We study biases associated with names by measuring cultural
presumptions in the responses generated by LLMs when presented with common
suggestion-seeking queries, which might involve making assumptions about the
user. Our analyses demonstrate strong assumptions about cultural identity
associated with names present in LLM generations across multiple cultures. Our
work has implications for designing more nuanced personalisation systems that
avoid reinforcing stereotypes while maintaining meaningful customisation.

摘要：姓名與人類身分密不可分。它們可以作為個人特質、文化遺產和個人歷史的標記。然而，將姓名作為身分的核心指標可能會導致複雜身分的過度簡化。在與 LLM 互動時，使用者名稱是個人化的重要資訊點。姓名可以透過直接使用者輸入（聊天機器人要求）、作為履歷審查等任務情境的其中一部分，或作為儲存使用者資訊以供個人化的內建記憶功能，進入聊天機器人對話。我們透過衡量 LLM 在面對常見的建議尋求查詢時所產生的回應中的文化預設，來研究與姓名相關的偏見，這可能涉及對使用者的假設。我們的分析顯示，在跨多種文化的 LLM 世代中，與姓名相關的文化身分有強烈的假設。我們的研究對於設計更細緻的個人化系統有影響，這些系統避免強化刻板印象，同時維持有意義的客製化。

##### **Characterizing Photorealism and Artifacts in Diffusion Model-Generated Images**
2502.11989v1 by Negar Kamali, Karyn Nakamura, Aakriti Kumar, Angelos Chatzimparmpas, Jessica Hullman, Matthew Groh

Diffusion model-generated images can appear indistinguishable from authentic
photographs, but these images often contain artifacts and implausibilities that
reveal their AI-generated provenance. Given the challenge to public trust in
media posed by photorealistic AI-generated images, we conducted a large-scale
experiment measuring human detection accuracy on 450 diffusion-model generated
images and 149 real images. Based on collecting 749,828 observations and 34,675
comments from 50,444 participants, we find that scene complexity of an image,
artifact types within an image, display time of an image, and human curation of
AI-generated images all play significant roles in how accurately people
distinguish real from AI-generated images. Additionally, we propose a taxonomy
characterizing artifacts often appearing in images generated by diffusion
models. Our empirical observations and taxonomy offer nuanced insights into the
capabilities and limitations of diffusion models to generate photorealistic
images in 2024.

摘要：擴散模型生成的影像看起來可能與真實照片無異，但這些影像通常包含人工智慧生成來源的瑕疵和不合理之處。由於寫實的人工智慧生成影像對公眾對媒體的信任構成挑戰，我們進行了一項大規模實驗，測量人類對 450 張擴散模型生成影像和 149 張真實影像的檢測準確度。根據收集自 50,444 位參與者的 749,828 次觀察和 34,675 則評論，我們發現影像的場景複雜性、影像中的瑕疵類型、影像的顯示時間，以及人類對人工智慧生成影像的策展，在人們準確區分真實影像和人工智慧生成影像方面都扮演重要的角色。此外，我們提出了一種分類法，用於描述經常出現在擴散模型生成的影像中的瑕疵。我們的經驗觀察和分類法為擴散模型在 2024 年生成寫實影像的能力和限制提供了細緻的見解。

##### **Generating Text from Uniform Meaning Representation**
2502.11973v1 by Emma Markle, Reihaneh Iranmanesh, Shira Wein

Uniform Meaning Representation (UMR) is a recently developed graph-based
semantic representation, which expands on Abstract Meaning Representation (AMR)
in a number of ways, in particular through the inclusion of document-level
information and multilingual flexibility. In order to effectively adopt and
leverage UMR for downstream tasks, efforts must be placed toward developing a
UMR technological ecosystem. Though still limited amounts of UMR annotations
have been produced to date, in this work, we investigate the first approaches
to producing text from multilingual UMR graphs: (1) a pipeline conversion of
UMR to AMR, then using AMR-to-text generation models, (2) fine-tuning large
language models with UMR data, and (3) fine-tuning existing AMR-to-text
generation models with UMR data. Our best performing model achieves a
multilingual BERTscore of 0.825 for English and 0.882 for Chinese when compared
to the reference, which is a promising indication of the effectiveness of
fine-tuning approaches for UMR-to-text generation with even limited amounts of
UMR data.

摘要：統一語意表示 (UMR) 是一種最近開發的基於圖形的語意表示，它在許多方面擴展了抽象語意表示 (AMR)，特別是透過納入文件層級資訊和多語言靈活性。為了有效採用和利用下游任務的 UMR，必須投入精力開發 UMR 技術生態系統。雖然到目前為止產生的 UMR 標註數量仍然有限，但在這項工作中，我們探討了從多語言 UMR 圖形產生文字的第一種方法：(1) 將 UMR 轉換為 AMR 的管道，然後使用 AMR 轉文字生成模型，(2) 使用 UMR 資料微調大型語言模型，以及 (3) 使用 UMR 資料微調現有的 AMR 轉文字生成模型。與參考相比，我們效能最好的模型在英文中達到 0.825 的多語言 BERT 分數，在中文中達到 0.882，這表示使用 UMR 資料進行 UMR 轉文字生成的微調方法具有良好的效果，即使 UMR 資料數量有限。

##### **Learning Generalizable Prompt for CLIP with Class Similarity Knowledge**
2502.11969v1 by Sehun Jung, Hyang-won Lee

In vision-language models (VLMs), prompt tuning has shown its effectiveness
in adapting models to downstream tasks. However, learned prompts struggle to
generalize to unseen classes, as they tend to overfit to the classes that are
targeted during prompt tuning. Examining failure cases, we observed that
learned prompts disrupt the semantics of unseen classes, generating text
embeddings with incorrect semantic relationships among classes. To address
this, we propose Similarity Alignment Regularization (SAR), which regularizes
learnable prompts to preserve the semantic relationships among classes captured
by hand-crafted prompts. Specifically, we first obtain novel classes related to
base classes using ChatGPT-4o and utilize them as potential unseen classes
during prompt tuning. Then, by targeting both base and novel classes, SAR
aligns the similarity relationships among text embeddings generated by
learnable prompts with the similarity relationships from hand-crafted prompts.
Extensive experiments applying SAR to existing prompt tuning methods
demonstrate its effectiveness in improving generalization to unseen classes.

摘要：在視覺語言模型 (VLM) 中，提示調整已展現其在調整模型至下游任務上的效能。然而，已學習的提示難以推廣至未見類別，因為它們傾向於過度擬合提示調整期間所鎖定的類別。在檢視失敗案例時，我們觀察到已學習的提示會擾亂未見類別的語義，產生具有類別間不正確語義關係的文字嵌入。為了解決此問題，我們提出相似度對齊正則化 (SAR)，它會對可學習提示進行正則化，以保留由手工提示捕捉到的類別間語義關係。具體來說，我們首先使用 ChatGPT-4o 取得與基本類別相關的新穎類別，並在提示調整期間將它們用作潛在的未見類別。然後，透過鎖定基本類別和新穎類別，SAR 會將可學習提示產生的文字嵌入之間的相似度關係與手工提示的相似度關係對齊。將 SAR 應用於現有提示調整方法的廣泛實驗證明了其在改善對未見類別的概括上的效能。

##### **A MIMO Wireless Channel Foundation Model via CIR-CSI Consistency**
2502.11965v1 by Jun Jiang, Wenjun Yu, Yunfan Li, Yuan Gao, Shugong Xu

In the field of artificial intelligence, self-supervised learning has
demonstrated superior generalization capabilities by leveraging large-scale
unlabeled datasets for pretraining, which is especially critical for wireless
communication models to adapt to a variety of scenarios. This paper
innovatively treats Channel State Information (CSI) and Channel Impulse
Response (CIR) as naturally aligned multi-modal data and proposes the first
MIMO wireless channel foundation model, named CSI-CLIP. By effectively
capturing the joint representations of both CIR and CSI, CSI-CLIP exhibits
remarkable adaptability across scenarios and robust feature extraction
capabilities. Experimental results show that in positioning task, CSI-CLIP
reduces the mean error distance by 22%; in beam management task, it increases
accuracy by 1% compared to traditional supervised methods, as well as in the
channel identification task. These improvements not only highlight the
potential and value of CSI-CLIP in integrating sensing and communication but
also demonstrate its significant advantages over existing techniques. Moreover,
viewing CSI and CIR as multi-modal pairs and contrastive learning for wireless
channel foundation model open up new research directions in the domain of MIMO
wireless communications.

摘要：在人工智能领域，自监督学习通过利用大规模无标签数据集进行预训练，展示了卓越的泛化能力，这对于无线通信模型适应各种场景尤为关键。本文创新地将信道状态信息 (CSI) 和信道脉冲响应 (CIR) 视为自然对齐的多模态数据，并提出了第一个 MIMO 无线信道基础模型，名为 CSI-CLIP。通过有效捕获 CIR 和 CSI 的联合表示，CSI-CLIP 在各种场景中表现出卓越的适应性和强大的特征提取能力。实验结果表明，在定位任务中，CSI-CLIP 将平均误差距离减少了 22%；在波束管理任务中，与传统的监督方法相比，其准确度提高了 1%，以及在信道识别任务中。这些改进不仅突出了 CSI-CLIP 在集成感知和通信方面的潜力和价值，而且还展示了其相对于现有技术的显着优势。此外，将 CSI 和 CIR 视为多模态对，并对比学习无线信道基础模型，为 MIMO 无线通信领域开辟了新的研究方向。

##### **Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning**
2502.11962v1 by Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold

Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language
Models (LLMs), but it may lower their truthfulness. This trade-off arises
because IFT steers LLMs to generate responses with long-tail knowledge that is
not well covered during pre-training, leading to more informative but less
truthful answers when generalizing to unseen tasks. In this paper, we
empirically demonstrate this helpfulness-truthfulness trade-off in IFT and
propose $\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs
to recognize their uncertainty and explicitly reflect it at the end of their
responses. Experimental results show that UNIT-tuned models maintain their
helpfulness while distinguishing between certain and uncertain claims, thereby
reducing hallucinations.

摘要：指令微調 (IFT) 可以提升大型語言模型 (LLM) 的實用性，但可能會降低其真實性。這種取捨會出現，是因為 IFT 引導 LLM 生成具有長尾知識的回應，而這些知識在預訓練期間並未充分涵蓋，導致在推廣到未見任務時，答案更具資訊性，但真實性較低。在本文中，我們透過實證展示 IFT 中的這種實用性與真實性取捨，並提出一個新穎的 IFT 典範 $\textbf{UNIT}$ 來解決這個問題。UNIT 教導 LLM 辨識其不確定性，並明確反映在其回應的結尾。實驗結果顯示，經過 UNIT 微調的模型維持其實用性，同時區分確定和不確定的說法，從而減少幻覺。

##### **STRIVE: Structured Reasoning for Self-Improvement in Claim Verification**
2502.11959v1 by Haisong Gong, Jing Li, Junfei Wu, Qiang Liu, Shu Wu, Liang Wang

Claim verification is the task of determining whether a claim is supported or
refuted by evidence. Self-improvement methods, where reasoning chains are
generated and those leading to correct results are selected for training, have
succeeded in tasks like mathematical problem solving. However, in claim
verification, this approach struggles. Low-quality reasoning chains may falsely
match binary truth labels, introducing faulty reasoning into the
self-improvement process and ultimately degrading performance. To address this,
we propose STRIVE: Structured Reasoning for Self-Improved Verification. Our
method introduces a structured reasoning design with Claim Decomposition,
Entity Analysis, and Evidence Grounding Verification. These components improve
reasoning quality, reduce errors, and provide additional supervision signals
for self-improvement. STRIVE begins with a warm-up phase, where the base model
is fine-tuned on a small number of annotated examples to learn the structured
reasoning design. It is then applied to generate reasoning chains for all
training examples, selecting only those that are correct and structurally sound
for subsequent self-improvement training. We demonstrate that STRIVE achieves
significant improvements over baseline models, with a 31.4% performance gain
over the base model and 20.7% over Chain of Thought on the HOVER datasets,
highlighting its effectiveness.

摘要：聲明驗證的任務是確定聲明是否受到證據支持或反駁。自改善方法（產生推理鏈並選擇導致正確結果的鏈進行訓練）已成功應用於數學問題求解等任務。然而，在聲明驗證中，此方法會遇到困難。低品質的推理鏈可能錯誤地匹配二元真值標籤，將錯誤的推理引入自改善流程並最終降低效能。為了解決此問題，我們提出 STRIVE：結構化推理自改善驗證。我們的模型引入了結構化推理設計，包含聲明分解、實體分析和證據依據驗證。這些組件改善了推理品質、減少了錯誤，並為自改善提供了額外的監督訊號。STRIVE 從熱身階段開始，在少數標註範例上微調基礎模型以學習結構化推理設計。接著將其應用於為所有訓練範例產生推理鏈，僅選擇正確且結構上合理的推理鏈進行後續的自改善訓練。我們證明 STRIVE 獲得了顯著的改善，在 HOVER 資料集上，效能比基礎模型提升了 31.4%，比 Chain of Thought 提升了 20.7%，突顯了其有效性。

##### **Can Your Uncertainty Scores Detect Hallucinated Entity?**
2502.11948v1 by Min-Hsuan Yeh, Max Kamachee, Seongheon Park, Yixuan Li

To mitigate the impact of hallucination nature of LLMs, many studies propose
detecting hallucinated generation through uncertainty estimation. However,
these approaches predominantly operate at the sentence or paragraph level,
failing to pinpoint specific spans or entities responsible for hallucinated
content. This lack of granularity is especially problematic for long-form
outputs that mix accurate and fabricated information. To address this
limitation, we explore entity-level hallucination detection. We propose a new
data set, HalluEntity, which annotates hallucination at the entity level. Based
on the dataset, we comprehensively evaluate uncertainty-based hallucination
detection approaches across 17 modern LLMs. Our experimental results show that
uncertainty estimation approaches focusing on individual token probabilities
tend to over-predict hallucinations, while context-aware methods show better
but still suboptimal performance. Through an in-depth qualitative study, we
identify relationships between hallucination tendencies and linguistic
properties and highlight important directions for future research.

摘要：為了減輕 LLM 幻覺性質的影響，許多研究提出透過不確定性估計來偵測幻覺產生的內容。然而，這些方法主要是在句子或段落層級運作，無法精確找出對幻覺內容負責的特定區間或實體。這種缺乏粒度的現象對於混合了準確和虛構資訊的長篇輸出內容來說尤其成問題。為了解決這個限制，我們探討了實體層級的幻覺偵測。我們提出了一個新的資料集 HalluEntity，其中註解了實體層級的幻覺。根據該資料集，我們全面評估了 17 種現代 LLM 的基於不確定性的幻覺偵測方法。我們的實驗結果顯示，專注於個別代幣機率的不確定性估計方法傾向於過度預測幻覺，而具備背景感知能力的方法則表現得更好，但仍未達到最佳狀態。透過深入的定性研究，我們找出幻覺傾向與語言特徵之間的關係，並強調未來研究的重要方向。

##### **Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction**
2502.11946v1 by Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Jianchang Wu, Jiahong Liu, Jianjian Sun, Jiangjie Zhen, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong, Yu Luo, Yuanwei Lu, Yuhe Yin, Yuting Yan, Yuxiang Yang, Zhe Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zidong Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu

Real-time speech interaction, serving as a fundamental interface for
human-machine collaboration, holds immense potential. However, current
open-source models face limitations such as high costs in voice data
collection, weakness in dynamic control, and limited intelligence. To address
these challenges, this paper introduces Step-Audio, the first production-ready
open-source solution. Key contributions include: 1) a 130B-parameter unified
speech-text multi-modal model that achieves unified understanding and
generation, with the Step-Audio-Chat version open-sourced; 2) a generative
speech data engine that establishes an affordable voice cloning framework and
produces the open-sourced lightweight Step-Audio-TTS-3B model through
distillation; 3) an instruction-driven fine control system enabling dynamic
adjustments across dialects, emotions, singing, and RAP; 4) an enhanced
cognitive architecture augmented with tool calling and role-playing abilities
to manage complex tasks effectively. Based on our new StepEval-Audio-360
evaluation benchmark, Step-Audio achieves state-of-the-art performance in human
evaluations, especially in terms of instruction following. On open-source
benchmarks like LLaMA Question, shows 9.3% average performance improvement,
demonstrating our commitment to advancing the development of open-source
multi-modal language technologies. Our code and models are available at
https://github.com/stepfun-ai/Step-Audio.

摘要：<paragraph>即時語音互動作為人機協作的基本介面，蘊含著巨大的潛力。然而，目前的開源模型面臨著語音數據收集成本高、動態控制能力弱、智慧有限等限制。為了應對這些挑戰，本文介紹了 Step-Audio，這是第一個可投入生產的開源解決方案。主要貢獻包括：1) 一個 130B 參數的統一語音文字多模態模型，實現了統一的理解和生成，其中 Step-Audio-Chat 版本已開源；2) 一個生成式語音數據引擎，建立了一個經濟實惠的語音克隆框架，並通過蒸餾技術產生了開源的輕量級 Step-Audio-TTS-3B 模型；3) 一個指令驅動的精細控制系統，實現了跨方言、情緒、唱歌和饒舌的動態調整；4) 一個增強的認知架構，增加了工具呼叫和角色扮演的能力，以有效地管理複雜的任務。根據我們新的 StepEval-Audio-360 評估基準，Step-Audio 在人類評估中實現了最先進的性能，特別是在指令遵循方面。在 LLaMA Question 等開源基準測試中，表現出平均提升了 9.3%，證明了我們致力於推進開源多模態語言技術的發展。我們的程式碼和模型可在 https://github.com/stepfun-ai/Step-Audio 取得。</paragraph>

##### **Deep Spatio-Temporal Neural Network for Air Quality Reanalysis**
2502.11941v1 by Ammar Kheder, Benjamin Foreback, Lili Wang, Zhi-Song Liu, Michael Boy

Air quality prediction is key to mitigating health impacts and guiding
decisions, yet existing models tend to focus on temporal trends while
overlooking spatial generalization. We propose AQ-Net, a spatiotemporal
reanalysis model for both observed and unobserved stations in the near future.
AQ-Net utilizes the LSTM and multi-head attention for the temporal regression.
We also propose a cyclic encoding technique to ensure continuous time
representation. To learn fine-grained spatial air quality estimation, we
incorporate AQ-Net with the neural kNN to explore feature-based interpolation,
such that we can fill the spatial gaps given coarse observation stations. To
demonstrate the efficiency of our model for spatiotemporal reanalysis, we use
data from 2013-2017 collected in northern China for PM2.5 analysis. Extensive
experiments show that AQ-Net excels in air quality reanalysis, highlighting the
potential of hybrid spatio-temporal models to better capture environmental
dynamics, especially in urban areas where both spatial and temporal variability
are critical.

摘要：空气品质预测是减轻健康影响和指导决策的关键，但现有的模型倾向于关注时间趋势，而忽略空间概化。我们提出了 AQ-Net，这是一种时空再分析模型，适用于近期内已观测和未观测到的站点。AQ-Net 利用 LSTM 和多头注意力进行时间回归。我们还提出了一种循环编码技术来确保时间表示的连续性。为了学习细粒度的空间空气质量估计，我们将 AQ-Net 与神经 kNN 结合起来，以探索基于特征的插值，以便我们能够填充给定粗略观测站的空间空白。为了展示我们的模型在时空再分析中的效率，我们使用了 2013-2017 年在中国北部收集的 PM2.5 分析数据。大量的实验表明，AQ-Net 在空气质量再分析中表现出色，突出了混合时空模型在更好地捕捉环境动态方面的潜力，尤其是在空间和时间变异性都很关键的城市地区。

##### **FitLight: Federated Imitation Learning for Plug-and-Play Autonomous Traffic Signal Control**
2502.11937v1 by Yutong Ye, Yingbo Zhou, Zhusen Liu, Xiao Du, Hao Zhou, Xiang Lian, Mingsong Chen

Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC)
methods have been extensively studied, their practical applications still raise
some serious issues such as high learning cost and poor generalizability. This
is because the ``trial-and-error'' training style makes RL agents extremely
dependent on the specific traffic environment, which also requires a long
convergence time. To address these issues, we propose a novel Federated
Imitation Learning (FIL)-based framework for multi-intersection TSC, named
FitLight, which allows RL agents to plug-and-play for any traffic environment
without additional pre-training cost. Unlike existing imitation learning
approaches that rely on pre-training RL agents with demonstrations, FitLight
allows real-time imitation learning and seamless transition to reinforcement
learning. Due to our proposed knowledge-sharing mechanism and novel hybrid
pressure-based agent design, RL agents can quickly find a best control policy
with only a few episodes. Moreover, for resource-constrained TSC scenarios,
FitLight supports model pruning and heterogeneous model aggregation, such that
RL agents can work on a micro-controller with merely 16{\it KB} RAM and 32{\it
KB} ROM. Extensive experiments demonstrate that, compared to state-of-the-art
methods, FitLight not only provides a superior starting point but also
converges to a better final solution on both real-world and synthetic datasets,
even under extreme resource limitations.

摘要：儘管基於強化學習 (RL) 的交通號誌控制 (TSC) 方法已經廣泛研究，但其實際應用仍會產生一些嚴重的問題，例如學習成本高和泛化能力差。這是因為「試錯法」訓練風格讓 RL 代理極度依賴特定的交通環境，這也需要很長的收斂時間。為了解決這些問題，我們提出一個名為 FitLight 的基於聯邦模仿學習 (FIL) 的多路口 TSC 框架，讓 RL 代理可以即插即用於任何交通環境，而無需額外的預訓練成本。與依賴使用示範預訓練 RL 代理的現有模仿學習方法不同，FitLight 允許即時模仿學習和無縫過渡到強化學習。由於我們提出的知識共享機制和新穎的基於壓力的混合代理設計，RL 代理只需幾個回合即可快速找到最佳控制策略。此外，對於資源受限的 TSC 場景，FitLight 支援模型剪枝和異質模型聚合，讓 RL 代理可以在僅有 16{\it KB} RAM 和 32{\it KB} ROM 的微控制器上運行。廣泛的實驗證明，與最先進的方法相比，FitLight 不僅提供了更好的起點，而且在實際和合成資料集上都能收斂到更好的最終解決方案，即使在極端的資源限制下也是如此。

##### **On Representational Dissociation of Language and Arithmetic in Large Language Models**
2502.11932v1 by Riku Kisako, Tatsuki Kuribayashi, Ryohei Sasano

The association between language and (non-linguistic) thinking ability in
humans has long been debated, and recently, neuroscientific evidence of brain
activity patterns has been considered. Such a scientific context naturally
raises an interdisciplinary question -- what about such a language-thought
dissociation in large language models (LLMs)? In this paper, as an initial
foray, we explore this question by focusing on simple arithmetic skills (e.g.,
$1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in
LLMs' representation space. Our experiments with linear classifiers and cluster
separability tests demonstrate that simple arithmetic equations and general
language input are encoded in completely separated regions in LLMs' internal
representation space across all the layers, which is also supported with more
controlled stimuli (e.g., spelled-out equations). These tentatively suggest
that arithmetic reasoning is mapped into a distinct region from general
language input, which is in line with the neuroscientific observations of human
brain activations, while we also point out their somewhat cognitively
implausible geometric properties.

摘要：人類語言與（非語言）思考能力之間的關聯性長期以來一直備受爭論，而最近，神經科學證據中的大腦活動模式也已受到考量。這樣一個科學背景自然會引發一個跨領域問題——大型語言模型（LLM）中這種語言與思考的分離又是如何？在本文中，作為初步探討，我們透過專注於簡單的算術技能（例如 $1+2=$？）作為思考能力，並分析它們在 LLM 表徵空間中的編碼幾何形狀來探討這個問題。我們透過線性分類器和群集可分性測試進行的實驗證明，簡單的算術方程式和一般語言輸入在 LLM 的內部表徵空間中所有層中都是以完全分離的區域編碼，這也獲得了更受控刺激（例如，拼寫出的方程式）的支持。這些初步表明算術推理被映射到與一般語言輸入不同的區域，這與人類大腦活化的神經科學觀察結果一致，同時我們也指出了它們在認知上有些難以置信的幾何屬性。

##### **BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages**
2502.11926v1 by Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya, Rahmad Mahendra, Vukosi Marivate, Andrew Piper, Alexander Panchenko, Charles Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava, Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad

People worldwide use language in subtle and complex ways to express emotions.
While emotion recognition -- an umbrella term for several NLP tasks --
significantly impacts different applications in NLP and other fields, most work
in the area is focused on high-resource languages. Therefore, this has led to
major disparities in research and proposed solutions, especially for
low-resource languages that suffer from the lack of high-quality datasets. In
this paper, we present BRIGHTER-- a collection of multilabeled
emotion-annotated datasets in 28 different languages. BRIGHTER covers
predominantly low-resource languages from Africa, Asia, Eastern Europe, and
Latin America, with instances from various domains annotated by fluent
speakers. We describe the data collection and annotation processes and the
challenges of building these datasets. Then, we report different experimental
results for monolingual and crosslingual multi-label emotion identification, as
well as intensity-level emotion recognition. We investigate results with and
without using LLMs and analyse the large variability in performance across
languages and text domains. We show that BRIGHTER datasets are a step towards
bridging the gap in text-based emotion recognition and discuss their impact and
utility.

摘要：全球各地的人們都以微妙且複雜的方式使用語言來表達情感。
雖然情緒辨識——幾個 NLP 任務的總稱——
顯著影響 NLP 及其他領域中的不同應用，但該領域中的大部分工作
都集中於高資源語言。因此，這導致研究和提出的解決方案出現重大差異，特別是
對於缺乏高品質資料集的低資源語言。在本文中，我們提出 BRIGHTER——一個
由 28 種不同語言組成的多標記情緒標註資料集。BRIGHTER 主要涵蓋來自非洲、亞洲、東歐和
拉丁美洲的低資源語言，其中包含由流利講者標註的來自不同領域的實例。我們描述了資料收集和標註流程以及
建立這些資料集的挑戰。然後，我們報告了單語和跨語言多標籤情緒識別的不同實驗結果，以及
強度級別的情緒識別。我們研究了使用和不使用 LLM 的結果，並分析了跨語言和文字領域的性能的巨大變異。我們表明，BRIGHTER 資料集是縮小基於文字的情緒識別差距的一步，並討論了它們的影響和
效用。

##### **GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs**
2502.11925v1 by Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, Jiawei Han

The rapid development of Multimodal Large Language Models (MLLMs) has enabled
the integration of multiple modalities, including texts and images, within the
large language model (LLM) framework. However, texts and images are usually
interconnected, forming a multimodal attributed graph (MMAG). It is
underexplored how MLLMs can incorporate the relational information
(\textit{i.e.}, graph structure) and semantic information (\textit{i.e.,} texts
and images) on such graphs for multimodal comprehension and generation. In this
paper, we propose GraphGPT-o, which supports omni-multimodal understanding and
creation on MMAGs. We first comprehensively study linearization variants to
transform semantic and structural information as input for MLLMs. Then, we
propose a hierarchical aligner that enables deep graph encoding, bridging the
gap between MMAGs and MLLMs. Finally, we explore the inference choices,
adapting MLLM to interleaved text and image generation in graph scenarios.
Extensive experiments on three datasets from different domains demonstrate the
effectiveness of our proposed method. Datasets and codes will be open-sourced
upon acceptance.

摘要：多模态大语言模型 (MLLM) 的快速发展，促进了文本和图像等多种模态在大型语言模型 (LLM) 框架内的整合。然而，文本和图像通常是相互关联的，形成多模态属性图 (MMAG)。对于 MLLM 如何整合此类图上的关系信息（即图结构）和语义信息（即文本和图像）以进行多模态理解和生成，目前仍未得到充分探索。在本文中，我们提出了 GraphGPT-o，它支持在 MMAG 上进行全方位多模态理解和创建。我们首先全面研究了线性化变体，以将语义和结构信息转换为 MLLM 的输入。然后，我们提出了一个分层对齐器，它支持深度图编码，弥合了 MMAG 和 MLLM 之间的差距。最后，我们探索了推理选择，使 MLLM 适应图场景中交错的文本和图像生成。来自不同领域的三组数据集上的大量实验表明了我们提出的方法的有效性。数据集和代码将在被接受后开源。

##### **From Text to Trust: Empowering AI-assisted Decision Making with Adaptive LLM-powered Analysis**
2502.11919v1 by Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ziang Xiao, Ming Yin

AI-assisted decision making becomes increasingly prevalent, yet individuals
often fail to utilize AI-based decision aids appropriately especially when the
AI explanations are absent, potentially as they do not %understand reflect on
AI's decision recommendations critically. Large language models (LLMs), with
their exceptional conversational and analytical capabilities, present great
opportunities to enhance AI-assisted decision making in the absence of AI
explanations by providing natural-language-based analysis of AI's decision
recommendation, e.g., how each feature of a decision making task might
contribute to the AI recommendation. In this paper, via a randomized
experiment, we first show that presenting LLM-powered analysis of each task
feature, either sequentially or concurrently, does not significantly improve
people's AI-assisted decision performance. To enable decision makers to better
leverage LLM-powered analysis, we then propose an algorithmic framework to
characterize the effects of LLM-powered analysis on human decisions and
dynamically decide which analysis to present. Our evaluation with human
subjects shows that this approach effectively improves decision makers'
appropriate reliance on AI in AI-assisted decision making.

摘要：隨著 AI 輔助決策越來越普遍，但個人常常無法適當地利用 AI 決策輔助，特別是在沒有 AI 解釋的情況下，潛在原因是他們無法批判性地理解 AI 的決策建議。大型語言模型 (LLM) 擁有卓越的對話和分析能力，在沒有 AI 解釋的情況下，透過提供基於自然語言的 AI 決策建議分析，例如決策任務的每個特徵如何影響 AI 建議，為增強 AI 輔助決策提供了絕佳的機會。在本文中，我們透過隨機實驗，首先展示了以循序或並行的方式呈現 LLM 分析的每個任務特徵，並未顯著改善人們的 AI 輔助決策表現。為了讓決策者能更好地利用 LLM 分析，我們接著提出了演算法架構，用於描述 LLM 分析對人類決策的影響，並動態決定要呈現哪種分析。我們對人類受試者的評估顯示，這種方法有效地改善了決策者在 AI 輔助決策中對 AI 的適當依賴性。

##### **EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models**
2502.11916v1 by Jiamin Su, Yibo Yan, Fangteng Fu, Han Zhang, Jingheng Ye, Xiang Liu, Jiahao Huo, Huiyu Zhou, Xuming Hu

Automated Essay Scoring (AES) plays a crucial role in educational assessment
by providing scalable and consistent evaluations of writing tasks. However,
traditional AES systems face three major challenges: (1) reliance on
handcrafted features that limit generalizability, (2) difficulty in capturing
fine-grained traits like coherence and argumentation, and (3) inability to
handle multimodal contexts. In the era of Multimodal Large Language Models
(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES
capabilities across lexical-, sentence-, and discourse-level traits. By
leveraging MLLMs' strengths in trait-specific scoring and multimodal context
understanding, EssayJudge aims to offer precise, context-rich evaluations
without manual feature engineering, addressing longstanding AES limitations.
Our experiments with 18 representative MLLMs reveal gaps in AES performance
compared to human evaluation, particularly in discourse-level traits,
highlighting the need for further advancements in MLLM-based AES research. Our
dataset and code will be available upon acceptance.

摘要：自動化論文評分 (AES) 在教育評量中扮演著重要的角色，它能提供可擴充且一致的寫作任務評量。然而，傳統的 AES 系統面臨了三個主要的挑戰：(1) 依賴於限制泛用性的手工特徵，(2) 難以捕捉連貫性和論證等細微特徵，以及 (3) 無法處理多模態的脈絡。在多模態大型語言模型 (MLLM) 的時代，我們提出了 EssayJudge，這是第一個評估 AES 能力的多模態基準，橫跨詞彙、句子和篇章層級的特徵。EssayJudge 透過利用 MLLM 在特定特徵評分和多模態脈絡理解方面的優勢，旨在提供精確且富含脈絡的評量，而無需手動特徵工程，進而解決長久以來的 AES 限制。我們針對 18 個具代表性的 MLLM 進行的實驗揭露了 AES 效能與人類評量之間的差距，特別是在篇章層級的特徵，這凸顯了 MLLM 為基礎的 AES 研究需要進一步的進展。我們的資料集和程式碼將在通過驗證後提供。

##### **On the robustness of ChatGPT in teaching Korean Mathematics**
2502.11915v1 by Phuong-Nam Nguyen, Quang Nguyen-The, An Vu-Minh, Diep-Anh Nguyen, Xuan-Lam Pham

ChatGPT, an Artificial Intelligence model, has the potential to revolutionize
education. However, its effectiveness in solving non-English questions remains
uncertain. This study evaluates ChatGPT's robustness using 586 Korean
mathematics questions. ChatGPT achieves 66.72% accuracy, correctly answering
391 out of 586 questions. We also assess its ability to rate mathematics
questions based on eleven criteria and perform a topic analysis. Our findings
show that ChatGPT's ratings align with educational theory and test-taker
perspectives. While ChatGPT performs well in question classification, it
struggles with non-English contexts, highlighting areas for improvement. Future
research should address linguistic biases and enhance accuracy across diverse
languages. Domain-specific optimizations and multilingual training could
improve ChatGPT's role in personalized education.

摘要：ChatGPT，一種人工智慧模型，具有革新教育的潛力。然而，其解決非英語問題的有效性仍不確定。本研究使用 586 個韓語數學問題評估 ChatGPT 的健壯性。ChatGPT 達到 66.72% 的準確率，正確回答了 586 個問題中的 391 個。我們也評估其根據 11 個標準對數學問題進行評分並執行主題分析的能力。我們的研究結果顯示，ChatGPT 的評分與教育理論和應試者的觀點一致。儘管 ChatGPT 在問題分類中表現良好，但它在非英語語境中表現不佳，突顯出需要改進的地方。未來的研究應解決語言偏見並提高跨不同語言的準確性。特定領域的優化和多語言訓練可以提升 ChatGPT 在個人化教育中的作用。

##### **MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation**
2502.11903v1 by Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chun-Mei Feng, Yutong Xie, Imran Razzak, Zongyuan Ge, Jionglong Su, Junjun He, Yu Qiao

Recent multimodal large language models (MLLMs) have demonstrated significant
potential in open-ended conversation, generating more accurate and personalized
responses. However, their abilities to memorize, recall, and reason in
sustained interactions within real-world scenarios remain underexplored. This
paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for
evaluating six core open-ended abilities of MLLMs: information extraction,
multi-turn reasoning, information update, image management, memory recall, and
answer refusal. With data collected from real-world scenarios, MMRC comprises
5,120 conversations and 28,720 corresponding manually labeled questions, posing
a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC
indicate an accuracy drop during open-ended interactions. We identify four
common failure patterns: long-term memory degradation, inadequacies in updating
factual knowledge, accumulated assumption of error propagation, and reluctance
to say no. To mitigate these issues, we propose a simple yet effective
NOTE-TAKING strategy, which can record key information from the conversation
and remind the model during its responses, enhancing conversational
capabilities. Experiments across six MLLMs demonstrate significant performance
improvements.

摘要：最近的多模态大型语言模型 (MLLM) 已在开放式对话中展现出显著的潜力，产生更准确且个性化的回应。然而，它们在现实世界场景中持续互动中的记忆、回忆和推理能力仍未得到充分探索。本文介绍了 MMRC，一个多模态现实世界对话基准，用于评估 MLLM 的六项核心开放式能力：信息提取、多轮推理、信息更新、图像管理、记忆回忆和答案拒绝。通过从现实世界场景中收集的数据，MMRC 包含 5,120 个对话和 28,720 个相应的手动标记问题，对现有的 MLLM 构成了重大挑战。在 MMRC 中对 20 个 MLLM 的评估表明，在开放式互动期间准确性下降。我们确定了四种常见的故障模式：长期记忆退化、更新事实知识的不足、累积的错误传播假设以及不愿说不。为了减轻这些问题，我们提出了一种简单但有效的笔记策略，它可以记录对话中的关键信息并在模型响应期间提醒模型，从而增强对话能力。六个 MLLM 的实验表明了显著的性能改进。

##### **Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity**
2502.11901v1 by Dylan Zhang, Justin Wang, Tianran Sun

Existing LMs struggle with proof-oriented programming due to data scarcity,
which manifest in two key ways: (1) a lack of sufficient corpora for
proof-oriented programming languages such as F*, and (2) the absence of
large-scale, project-level proof-oriented implementations that can teach the
model the intricate reasoning process when performing proof-oriented
programming. We present the first on synthetic data augmentation for project
level proof oriented programming for both generation and repair. Our method
addresses data scarcity by synthesizing basic proof-oriented programming
problems for proficiency in that language; incorporating diverse coding data
for reasoning capability elicitation and creating new proofs and repair data
within existing repositories. This approach enables language models to both
synthesize and repair proofs for function- and repository-level code. We show
that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of
the models that outperforms GPT-4o in project-level proof-oriented programming
by 64% relative margin, and can improve GPT-4o's performance by 54% by
repairing its outputs over GPT-4o's self-repair.

摘要：現有的語言模型在基於證明編程時會因資料稀少而有困難，
這會以兩種關鍵方式表現出來：(1) 缺乏足夠的語料庫，例如 F* 等面向證明的程式語言，以及 (2) 缺乏大型的專案層級面向證明實作，這些實作可以在執行面向證明編程時，教導模型複雜的推理程序。我們提出第一個面向專案層級面向證明編程的合成資料擴充，用於產生和修復。我們的做法透過合成基本的面向證明編程問題來解決資料稀少的問題，以精通該語言；納入不同的編碼資料，以引出推理能力，並在現有的儲存庫中建立新的證明和修復資料。這個方法讓語言模型能夠為函數層級和儲存庫層級的程式碼合成和修復證明。我們展示經過微調的 14B 參數模型 PoPilot，可以超過在專案層級面向證明編程中表現優於 GPT-4o 的模型 64% 的相對差距，並且可以透過修復 GPT-4o 自我修復的輸出，將 GPT-4o 的效能提升 54%。

##### **DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation**
2502.11897v1 by Zhihang Yuan, Siyuan Wang, Rui Xie, Hanling Zhang, Tongcheng Fang, Yuzhang Shang, Shengen Yan, Guohao Dai, Yu Wang

In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a
training-free paradigm that can make use of adaptive temporal compression in
latent space. While existing video generative models apply fixed compression
rates via pretrained VAE, we observe that real-world video content exhibits
substantial temporal non-uniformity, with high-motion segments containing more
information than static scenes. Based on this insight, DLFR-VAE dynamically
adjusts the latent frame rate according to the content complexity.
Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent
Frame Rate Scheduler that partitions videos into temporal chunks and adaptively
determines optimal frame rates based on information-theoretic content
complexity, and (2) A training-free adaptation mechanism that transforms
pretrained VAE architectures into a dynamic VAE that can process features with
variable frame rates. Our simple but effective DLFR-VAE can function as a
plug-and-play module, seamlessly integrating with existing video generation
models and accelerating the video generation process.

摘要：在本文中，我們提出動態潛在幀率 VAE (DLFR-VAE)，一種無需訓練的範例，它可以在潛在空間中使用自適應時間壓縮。現有的影片生成模型透過預訓練的 VAE 應用固定壓縮率，但我們觀察到真實世界的影片內容展現出大量的時間非一致性，其中高動作片段包含比靜態場景更多的資訊。基於這個見解，DLFR-VAE 會根據內容複雜度動態調整潛在幀率。具體來說，DLFR-VAE 包含兩項核心創新：(1) 一個動態潛在幀率排程器，它將影片分割成時間區塊，並根據資訊理論內容複雜度自適應地決定最佳幀率，以及 (2) 一個無需訓練的適應機制，它將預訓練的 VAE 架構轉換成一個動態 VAE，它可以處理具有可變幀率的特色。我們簡單但有效的 DLFR-VAE 可以作為一個即插即用的模組，與現有的影片生成模型無縫整合，並加速影片生成過程。

##### **CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning**
2502.11896v1 by Yanxiao Zhao, Yangge Qian, Jingyang Shan, Xiaolin Qin

Reinforcement learning (RL) in continuous action spaces encounters persistent
challenges, such as inefficient exploration and convergence to suboptimal
solutions. To address these limitations, we propose CAMEL, a novel framework
integrating LLM-generated suboptimal policies into the RL training pipeline.
CAMEL leverages dynamic action masking and an adaptive epsilon-masking
mechanism to guide exploration during early training stages while gradually
enabling agents to optimize policies independently. At the core of CAMEL lies
the integration of Python-executable suboptimal policies generated by LLMs
based on environment descriptions and task objectives. Although simplistic and
hard-coded, these policies offer valuable initial guidance for RL agents. To
effectively utilize these priors, CAMEL employs masking-aware optimization to
dynamically constrain the action space based on LLM outputs. Additionally,
epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling
agents to transition from constrained exploration to autonomous policy
refinement. Experimental validation on Gymnasium MuJoCo environments
demonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated
policies significantly improve sample efficiency, achieving performance
comparable to or surpassing expert masking baselines. For Walker2d-v4, where
LLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust
RL performance without notable degradation, highlighting the framework's
adaptability across diverse tasks. While CAMEL shows promise in enhancing
sample efficiency and mitigating convergence challenges, these issues remain
open for further research. Future work aims to generalize CAMEL to multimodal
LLMs for broader observation-action spaces and automate policy evaluation,
reducing human intervention and enhancing scalability in RL training pipelines.

摘要：<paragraph>在連續動作空間中的強化學習 (RL) 會遇到持續的挑戰，例如探索效率低落和收斂至次佳解。為了解決這些限制，我們提出 CAMEL，一個將 LLM 生成的次佳策略整合到 RL 訓練管線中的新框架。CAMEL 透過動態動作遮罩和自適應 epsilon 遮罩機制來引導探索，同時逐漸讓代理程式能夠獨立最佳化策略。CAMEL 的核心在於整合由 LLM 生成的 Python 可執行次佳策略，這些策略基於環境描述和任務目標。儘管這些策略過於簡化且硬編碼，但它們為 RL 代理程式提供了有價值的初始指導。為了有效利用這些先驗知識，CAMEL 採用遮罩感知最佳化來根據 LLM 輸出動態限制動作空間。此外，epsilon 遮罩逐漸減少對 LLM 生成的指導依賴，讓代理程式能夠從受限探索轉換為自主策略改善。在 Gymnasium MuJoCo 環境上的實驗驗證證明了 CAMEL 的有效性。在 Hopper-v4 和 Ant-v4 中，LLM 生成的策略顯著提升了樣本效率，達到了與專家遮罩基準相近或超越的效能。對於 LLM 難以準確建模雙足步態動態的 Walker2d-v4，CAMEL 維持穩健的 RL 效能，且沒有顯著降低，突顯了該框架在不同任務中的適應性。儘管 CAMEL 在提升樣本效率和緩解收斂挑戰方面顯示出前景，但這些問題仍有待進一步研究。未來的研究工作旨在將 CAMEL 推廣到多模態 LLM，以涵蓋更廣泛的觀察動作空間，並自動化策略評估，減少人工介入並提升 RL 訓練管線的可擴充性。</paragraph>

##### **Continual Quantization-Aware Pre-Training: When to transition from 16-bit to 1.58-bit pre-training for BitNet language models?**
2502.11895v1 by Jacob Nielsen, Peter Schneider-Kamp, Lukas Galke

Large language models (LLMs) require immense resources for training and
inference. Quantization, a technique that reduces the precision of model
parameters, offers a promising solution for improving LLM efficiency and
sustainability. While post-training quantization methods typically achieve 4-8
bits per parameter, recent research suggests that training LLMs with 1.58 bits
per weight parameter from scratch can maintain model accuracy while greatly
reducing memory requirements and energy consumption at inference time. Here, we
investigate a training strategy for quantization-aware pre-training, where the
models are first trained with 16-bit precision and then transition into
1.58-bit quantization-aware training. Our results on 11 downstream tasks show
that this 16-to-1.58-bit training strategy is preferable over full 1.58-bit
training and leaves models closer to those which have undergone 16-bit
training. We further investigate the effects of retaining the optimizer state
at the transition point and gradually phasing in quantization strength --
finding that both techniques alleviate the magnitude of loss spikes, but also
that these effects can be compensated through further training.

摘要：大型語言模型 (LLM) 需要大量的資源來進行訓練和推理。量化是一種降低模型參數精度的技術，為提高 LLM 效率和可持續性提供了一個有希望的解決方案。雖然訓練後量化方法通常每參數達到 4-8 位元，但最近的研究表明，從頭開始使用每權重參數 1.58 位元訓練 LLM 可以維持模型準確性，同時大幅減少推理時間的記憶體需求和能源消耗。在此，我們探討量化感知預訓練的訓練策略，其中模型首先使用 16 位元精度訓練，然後轉換為 1.58 位元量化感知訓練。我們在 11 個下游任務上的結果表明，這種 16 位元到 1.58 位元的訓練策略優於完全 1.58 位元訓練，並且使模型更接近經過 16 位元訓練的模型。我們進一步探討了在轉換點保留最佳化器狀態和逐漸調整量化強度的影響——發現這兩種技術都可以減輕損失尖峰的大小，但這些影響也可以透過進一步訓練來補償。

##### **Revisiting Classification Taxonomy for Grammatical Errors**
2502.11890v1 by Deqing Zou, Jingheng Ye, Yulu Liu, Yu Wu, Zishan Xu, Yinghui Li, Hai-Tao Zheng, Bingxu An, Zhao Wei, Yong Xu

Grammatical error classification plays a crucial role in language learning
systems, but existing classification taxonomies often lack rigorous validation,
leading to inconsistencies and unreliable feedback. In this paper, we revisit
previous classification taxonomies for grammatical errors by introducing a
systematic and qualitative evaluation framework. Our approach examines four
aspects of a taxonomy, i.e., exclusivity, coverage, balance, and usability.
Then, we construct a high-quality grammatical error classification dataset
annotated with multiple classification taxonomies and evaluate them grounding
on our proposed evaluation framework. Our experiments reveal the drawbacks of
existing taxonomies. Our contributions aim to improve the precision and
effectiveness of error analysis, providing more understandable and actionable
feedback for language learners.

摘要：語法錯誤分類在語言學習系統中扮演至關重要的角色，但現有的分類法常常缺乏嚴謹的驗證，導致不一致且不可靠的回饋。在本文中，我們透過引入一個系統且定性的評估架構，重新檢視先前的語法錯誤分類法。我們的做法檢視分類法的四個面向，即排他性、涵蓋性、平衡性和可用性。接著，我們建構一個高品質的語法錯誤分類資料集，並用多個分類法進行標註，並根據我們提出的評估架構對其進行評估。我們的實驗揭露了現有分類法的缺點。我們的貢獻旨在改善錯誤分析的準確性和有效性，為語言學習者提供更易於理解且可操作的回饋。

##### **Stonefish: Supporting Machine Learning Research in Marine Robotics**
2502.11887v1 by Michele Grimaldi, Patryk Cieslak, Eduardo Ochoa, Vibhav Bharti, Hayat Rajani, Ignacio Carlucho, Maria Koskinopoulou, Yvan R. Petillot, Nuno Gracias

Simulations are highly valuable in marine robotics, offering a cost-effective
and controlled environment for testing in the challenging conditions of
underwater and surface operations. Given the high costs and logistical
difficulties of real-world trials, simulators capable of capturing the
operational conditions of subsea environments have become key in developing and
refining algorithms for remotely-operated and autonomous underwater vehicles.
This paper highlights recent enhancements to the Stonefish simulator, an
advanced open-source platform supporting development and testing of marine
robotics solutions. Key updates include a suite of additional sensors, such as
an event-based camera, a thermal camera, and an optical flow camera, as well
as, visual light communication, support for tethered operations, improved
thruster modelling, more flexible hydrodynamics, and enhanced sonar accuracy.
These developments and an automated annotation tool significantly bolster
Stonefish's role in marine robotics research, especially in the field of
machine learning, where training data with a known ground truth is hard or
impossible to collect.

摘要：模擬在海洋機器人中極具價值，提供具成本效益且受控的環境，用於在水下和水面作業的挑戰性條件下進行測試。鑑於現實世界試驗的高成本和後勤困難，能夠捕捉海底環境作業條件的模擬器已成為開發和改進遠程操作和自主水下載具演算法的關鍵。本文重點介紹了 Stonefish 模擬器最近的增強功能，這是一個先進的開源平台，支援海洋機器人解決方案的開發和測試。主要更新包括一系列額外的感測器，例如事件式相機、熱像儀和光流相機，以及可見光通訊、對繫繩操作的支援、改進的推進器建模、更靈活的水動力學和增強的聲納準確度。這些開發和自動化標註工具顯著提升了 Stonefish 在海洋機器人研究中的作用，特別是在機器學習領域，其中具有已知基本事實的訓練資料難以或無法收集。

##### **LIMR: Less is More for RL Scaling**
2502.11886v1 by Xuefeng Li, Haoyang Zou, Pengfei Liu

In this paper, we ask: what truly determines the effectiveness of RL training
data for enhancing language models' reasoning capabilities? While recent
advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack
of transparency about training data requirements has hindered systematic
progress. Starting directly from base models without distillation, we challenge
the assumption that scaling up RL training data inherently improves
performance. we demonstrate that a strategically selected subset of just 1,389
samples can outperform the full 8,523-sample dataset. We introduce Learning
Impact Measurement (LIM), an automated method to evaluate and prioritize
training samples based on their alignment with model learning trajectories,
enabling efficient resource utilization and scalable implementation. Our method
achieves comparable or even superior performance using only 1,389 samples
versus the full 8,523 samples dataset. Notably, while recent data-efficient
approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it
significantly underperforms at 7B-scale through supervised fine-tuning (SFT).
In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and
outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results
fundamentally reshape our understanding of RL scaling in LLMs, demonstrating
that precise sample selection, rather than data scale, may be the key to
unlocking enhanced reasoning capabilities. For reproducible research and future
innovation, we are open-sourcing LIMR, including implementation of LIM,
training and evaluation code, curated datasets, and trained models at
https://github.com/GAIR-NLP/LIMR.

摘要：<paragraph>在這篇論文中，我們提出一個問題：究竟是什麼決定了 RL 訓練資料增強語言模型推理能力的有效性？雖然最近的進展，例如 o1、Deepseek R1 和 Kimi1.5，展示了 RL 的潛力，但缺乏關於訓練資料需求的透明度阻礙了系統化的進展。從沒有蒸餾的基本模型直接開始，我們挑戰了擴充 RL 訓練資料本質上就會提升效能的假設。我們證明，策略性地選出僅 1,389 個樣本的子集就能勝過完整的 8,523 個樣本資料集。我們引入了學習影響力測量 (LIM)，這是一種自動化方法，用來評估和優先處理訓練樣本，根據它們與模型學習軌跡的一致性，能有效利用資源和擴充實作。我們的方法使用僅 1,389 個樣本就能達到與使用完整的 8,523 個樣本資料集相當甚至更佳的效能。值得注意的是，雖然最近資料有效率的方法（例如 LIMO 和 s1）在 32B 規模的模型上展現了前景，但我們發現它在 7B 規模上透過監督微調 (SFT) 的表現大幅落後。相比之下，我們基於 RL 的 LIMR 在 AIME24 上達到了高出 16.7% 的準確度，並在 MATH500 上比 LIMO 和 s1 分別高出 13.0% 和 22.2%。這些結果從根本上改變了我們對 LLM 中 RL 擴充的理解，證明精確的樣本選取，而非資料規模，可能是解鎖增強推理能力的關鍵。為了可重製的研究和未來的創新，我們開放原始碼 LIMR，包括 LIM 的實作、訓練和評估程式碼、策展的資料集，以及在 https://github.com/GAIR-NLP/LIMR 上訓練的模型。</paragraph>

##### **Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration**
2502.11882v1 by Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen

Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. To the best of our
knowledge, DPT-Agent is the first language agent framework that achieves
successful real-time simultaneous human-AI collaboration autonomously. Code of
DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.

摘要：建立在大语言模型（LLM）上的代理在回合制人机协作方面表现出色，但在需要实时交互的同时任务中却举步维艰。延迟问题和推断可变人类策略的挑战阻碍了他们在没有明确指示的情况下做出自主决策的能力。通过使用当前独立的系统 1 和系统 2 方法进行的实验，我们验证了在实时任务中使用双重过程理论 (DPT) 的必要性。我们提出了 DPT-Agent，这是一个新颖的语言代理框架，它集成了系统 1 和系统 2，以实现高效的实时同时人机协作。DPT-Agent 的系统 1 使用有限状态机 (FSM) 和代码作为策略，以进行快速、直观且可控的决策。DPT-Agent 的系统 2 集成了心智理论 (ToM) 和异步反射，以推断人类意图并执行基于推理的自主决策。我们通过与基于规则的代理和人类合作者进行进一步的实验来证明 DPT-Agent 的有效性，展示了对主流基于 LLM 的框架的重大改进。据我们所知，DPT-Agent 是第一个实现自主的实时同时人机协作的语言代理框架。DPT-Agent 的代码可以在 https://github.com/sjtu-marl/DPT-Agent 中找到。

##### **Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models**
2502.11881v1 by Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi

Existing LLM reasoning methods have shown impressive capabilities across
various tasks, such as solving math and coding problems. However, applying
these methods to scenarios without ground-truth answers or rule-based
verification methods - such as tracking the mental states of an agent - remains
challenging. Inspired by the sequential Monte Carlo algorithm, we introduce
thought-tracing, an inference-time reasoning algorithm designed to trace the
mental states of specific agents by generating hypotheses and weighting them
based on observations without relying on ground-truth solutions to questions in
datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,
using LLMs to approximate probabilistic inference over agents' evolving mental
states based on their perceptions and actions. We evaluate thought-tracing on
diverse theory-of-mind benchmarks, demonstrating significant performance
improvements compared to baseline LLMs. Our experiments also reveal interesting
behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind,
highlighting the difference of social reasoning compared to other domains.

摘要：現有的 LLM 推理方法已在各種任務中展現出令人印象深刻的能力，例如解決數學和編碼問題。然而，將這些方法應用於沒有正解答案或基於規則的驗證方法的情境中 - 例如追蹤代理人的心智狀態 - 仍然具有挑戰性。受到序貫蒙地卡羅演算法的啟發，我們引入了思想追蹤，這是一種在推理時間進行推理的演算法，旨在透過產生假設並根據觀察加權這些假設來追蹤特定代理人的心智狀態，而無需依賴資料集中的問題正解。我們的演算法是以貝氏心智理論架構為範本，使用 LLM 根據代理人的感知和行動來近似代理人不斷演變的心智狀態的機率推論。我們在各種心智理論基準上評估思想追蹤，與基準 LLM 相比，證明了顯著的效能提升。我們的實驗也揭露了近期推理模型在心智理論上的有趣行為 - 例如 o1 和 R1 - 突顯了社會推理與其他領域的差異。

##### **Bitnet.cpp: Efficient Edge Inference for Ternary LLMs**
2502.11880v1 by Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, Hongyu Wang, Furu Wei

The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has
spurred interest in ternary LLMs. Despite this, research and practical
applications focusing on efficient edge inference for ternary LLMs remain
scarce. To bridge this gap, we introduce Bitnet.cpp, an inference system
optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix
multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,
Bitnet.cpp incorporates a novel mpGEMM library to facilitate
sub-2-bits-per-weight, efficient and lossless inference. The library features
two core solutions: Ternary Lookup Table (TL), which addresses spatial
inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),
which ensures lossless edge inference, both enabling high-speed inference. Our
experiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over
full-precision baselines and up to 2.32x over low-bit baselines, setting new
benchmarks in the field. Additionally, we expand TL to element-wise lookup
table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and
empirical evidence of its considerable potential. Bitnet.cpp is publicly
available at https://github.com/microsoft/BitNet/tree/paper , offering a
sophisticated solution for the efficient and practical deployment of edge LLMs.

摘要：隨著由 BitNet b1.58 領先的 1 位元大型語言模型 (LLM) 出現，已激發了對三元 LLM 的興趣。儘管如此，專注於三元 LLM 的高效能邊緣推論的研究和實際應用仍然很少見。為了彌補這個差距，我們引入了 Bitnet.cpp，這是一個針對 BitNet b1.58 和三元 LLM 最佳化的推論系統。由於混合精度矩陣乘法 (mpGEMM) 構成三元 LLM 中推論時間的大部分，Bitnet.cpp 結合了一個新穎的 mpGEMM 函式庫，以利於每權重低於 2 位元、高效能且無損失的推論。該函式庫具有兩個核心解決方案：三元查詢表 (TL)，它解決了先前逐位元方法的空間低效率，以及具有比例的 Int2 (I2_S)，它確保無損失的邊緣推論，兩者都能實現高速推論。我們的實驗顯示，Bitnet.cpp 的速度比全精度的基準快了 6.25 倍，比低位元基準快了 2.32 倍，樹立了該領域的新基準。此外，我們在附錄中將 TL 擴充到逐元素查詢表 (ELUT) 以用於低位元 LLM，並提出其巨大潛力的理論和實證證據。Bitnet.cpp 已公開於 https://github.com/microsoft/BitNet/tree/paper，提供了一個精密的解決方案，用於邊緣 LLM 的高效能和實際部署。

##### **VAQUUM: Are Vague Quantifiers Grounded in Visual Data?**
2502.11874v1 by Hugh Mee Wong, Rick Nouwen, Albert Gatt

Vague quantifiers such as "a few" and "many" are influenced by many
contextual factors, including how many objects are present in a given context.
In this work, we evaluate the extent to which vision-and-language models (VLMs)
are compatible with humans when producing or judging the appropriateness of
vague quantifiers in visual contexts. We release a novel dataset, VAQUUM,
containing 20300 human ratings on quantified statements across a total of 1089
images. Using this dataset, we compare human judgments and VLM predictions
using three different evaluation methods. Our findings show that VLMs, like
humans, are influenced by object counts in vague quantifier use. However, we
find significant inconsistencies across models in different evaluation
settings, suggesting that judging and producing vague quantifiers rely on two
different processes.

摘要：模糊量词，例如「一些」和「许多」，会受到许多语境因素的影响，包括在给定语境中出现的对象数量。在这项工作中，我们评估视觉语言模型 (VLM) 在视觉语境中产生或判断模糊量词的适当性时，与人类的兼容程度。我们发布了一个新数据集 VAQUUM，其中包含对 1089 张图像中的量化陈述的 20300 个人类评级。使用此数据集，我们使用三种不同的评估方法来比较人类判断和 VLM 预测。我们的研究结果表明，VLM 与人类一样，在模糊量词的使用中会受到对象数量的影响。然而，我们发现不同评估设置中的模型之间存在显着的不一致性，这表明判断和产生模糊量词依赖于两个不同的过程。

##### **Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire Articles Beyond the Front Page**
2502.11866v1 by Michael McRae

I introduce a new large-scale dataset of historical wire articles from U.S.
Southern newspapers, spanning 1960-1975 and covering multiple wire services:
The Associated Press, United Press International, Newspaper Enterprise
Association. Unlike prior work focusing on front-page content, this dataset
captures articles across the entire newspaper, offering broader insight into
mid-century Southern coverage. The dataset includes a version that has
undergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its
suitability for quantitative text analysis. Additionally, duplicate versions of
articles are retained to enable analysis of editorial differences in language
and framing across newspapers. Each article is tagged by wire service,
facilitating comparative studies of editorial patterns across agencies. This
resource opens new avenues for research in computational social science,
digital humanities, and historical linguistics, providing a detailed
perspective on how Southern newspapers relayed national and international news
during a transformative period in American history. The dataset will be made
available upon publication or request for research purposes.

摘要：我介紹一個新的美國歷史電訊文章大型資料集，時間跨度為 1960-1975 年，涵蓋多個電訊服務：美聯社、美聯國際社、報業企業協會。與先前專注於頭版內容的研究不同，此資料集擷取了整份報紙的文章，提供更廣泛的見解，深入探討世紀中葉的南方報導。該資料集包含一個經過 LLM 文字清理管線處理的版本，以減少 OCR 雜訊，提升其適用於量化文字分析。此外，保留文章的重複版本，以利分析報紙間語言和架構的編輯差異。每篇文章都標記電訊服務，便於比較各家機構的編輯模式。此資源為計算社會科學、數位人文和歷史語言學的研究開啟了新的途徑，提供一個詳細的觀點，探討南方報紙在美國歷史的轉型時期如何傳遞國內和國際新聞。該資料集將在出版或研究目的請求後提供。

##### **FedEAT: A Robustness Optimization Framework for Federated LLMs**
2502.11863v1 by Yahao Pang, Xingyuan Wu, Xiaojin Zhang, Wei Chen, Hai Jin

Significant advancements have been made by Large Language Models (LLMs) in
the domains of natural language understanding and automated content creation.
However, they still face persistent problems, including substantial
computational costs and inadequate availability of training data. The
combination of Federated Learning (FL) and LLMs (federated LLMs) offers a
solution by leveraging distributed data while protecting privacy, which
positions it as an ideal choice for sensitive domains. However, Federated LLMs
still suffer from robustness challenges, including data heterogeneity,
malicious clients, and adversarial attacks, which greatly hinder their
applications. We first introduce the robustness problems in federated LLMs, to
address these challenges, we propose FedEAT (Federated Embedding space
Adversarial Training), a novel framework that applies adversarial training in
the embedding space of client LLM and employs a robust aggregation approach,
specifically geometric median aggregation, to enhance the robustness of
Federated LLMs. Our experiments demonstrate that FedEAT effectively improves
the robustness of Federated LLMs with minimal performance loss.

摘要：大型語言模型 (LLM) 在自然語言理解和自動化內容創作領域取得了重大進展。
然而，它們仍然面臨持續的問題，包括大量的運算成本和訓練數據的可用性不足。
聯合學習 (FL) 和 LLM（聯合 LLM）的結合提供了一個解決方案，在保護隱私的同時利用分佈式數據，這使其成為敏感領域的理想選擇。
然而，聯合 LLM 仍然面臨著穩健性的挑戰，包括數據異質性、惡意用戶和對抗性攻擊，這極大地阻礙了它們的應用。
我們首先介紹了聯合 LLM 中的穩健性問題，為了應對這些挑戰，我們提出了 FedEAT（聯合嵌入空間對抗訓練），這是一個新穎的框架，它在用戶端 LLM 的嵌入空間中應用對抗訓練，並採用穩健的聚合方法，特別是幾何中值聚合，以增強聯合 LLM 的穩健性。
我們的實驗表明，FedEAT 有效地提高了聯合 LLM 的穩健性，同時性能損失最小。

##### **Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu**
2502.11862v1 by Renhao Pei, Yihong Liu, Peiqin Lin, François Yvon, Hinrich Schütze

In-context machine translation (MT) with large language models (LLMs) is a
promising approach for low-resource MT, as it can readily take advantage of
linguistic resources such as grammar books and dictionaries. Such resources are
usually selectively integrated into the prompt so that LLMs can directly
perform translation without any specific training, via their in-context
learning capability (ICL). However, the relative importance of each type of
resource e.g., dictionary, grammar book, and retrieved parallel examples, is
not entirely clear. To address this gap, this study systematically investigates
how each resource and its quality affects the translation performance, with the
Manchu language as our case study. To remove any prior knowledge of Manchu
encoded in the LLM parameters and single out the effect of ICL, we also
experiment with an encrypted version of Manchu texts. Our results indicate that
high-quality dictionaries and good parallel examples are very helpful, while
grammars hardly help. In a follow-up study, we showcase a promising application
of in-context MT: parallel data augmentation as a way to bootstrap the
conventional MT model. When monolingual data abound, generating synthetic
parallel data through in-context MT offers a pathway to mitigate data scarcity
and build effective and efficient low-resource neural MT systems.

摘要：語境機器翻譯 (MT) 與大型語言模型 (LLM) 結合，對於低資源 MT 來說是一種有前景的方法，因為它可以輕易利用語法書和字典等語言資源。此類資源通常會選擇性地整合到提示中，讓 LLM 能夠透過其語境學習能力 (ICL) 直接執行翻譯，而無需任何特定訓練。然而，每種類型的資源（例如字典、語法書和擷取的平行範例）的相對重要性並不明確。為了解決這個問題，本研究系統性地探討每項資源及其品質如何影響翻譯效能，並以滿語作為我們的案例研究。為了移除 LLM 參數中編碼的任何滿語先備知識，並找出 ICL 的影響，我們也對滿語文本的加密版本進行實驗。我們的結果顯示，高品質的字典和良好的平行範例非常有幫助，而語法幾乎沒有幫助。在後續研究中，我們展示了語境 MT 的一個有前景的應用：平行數據擴充，作為引導傳統 MT 模型的一種方式。當單語資料豐富時，透過語境 MT 產生合成平行資料提供了一條途徑，可以減輕資料短缺，並建構有效且高效的低資源神經 MT 系統。

##### **Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics**
2502.11861v1 by Shuqi Yang, Mingrui Jing, Shuai Wang, Jiaxin Kou, Manfei Shi, Weijie Xing, Yan Hu, Zheng Zhu

This study reviewed the use of Large Language Models (LLMs) in healthcare,
focusing on their training corpora, customization techniques, and evaluation
metrics. A systematic search of studies from 2021 to 2024 identified 61
articles. Four types of corpora were used: clinical resources, literature,
open-source datasets, and web-crawled data. Common construction techniques
included pre-training, prompt engineering, and retrieval-augmented generation,
with 44 studies combining multiple methods. Evaluation metrics were categorized
into process, usability, and outcome metrics, with outcome metrics divided into
model-based and expert-assessed outcomes. The study identified critical gaps in
corpus fairness, which contributed to biases from geographic, cultural, and
socio-economic factors. The reliance on unverified or unstructured data
highlighted the need for better integration of evidence-based clinical
guidelines. Future research should focus on developing a tiered corpus
architecture with vetted sources and dynamic weighting, while ensuring model
transparency. Additionally, the lack of standardized evaluation frameworks for
domain-specific models called for comprehensive validation of LLMs in
real-world healthcare settings.

摘要：本研究回顧了大型語言模型 (LLM) 在醫療保健中的使用，重點在於其訓練語料庫、自訂技術和評估指標。針對 2021 年至 2024 年的研究進行系統性搜尋，找出 61 篇文章。語料庫類型有四種：臨床資源、文獻、開放原始碼資料集和網路爬取資料。常見的建構技術包括預訓練、提示工程和檢索增強生成，其中有 44 項研究結合多種方法。評估指標分為流程、可用性和成果指標，其中成果指標又分為基於模型和專家評估的成果。本研究發現語料庫公平性存在重大差距，這會導致地理、文化和社會經濟因素的偏見。對未驗證或非結構化資料的依賴性突顯出更佳整合循證臨床指南的必要性。未來的研究應專注於開發具有審查來源和動態加權的分層語料庫架構，同時確保模型透明性。此外，缺乏針對特定領域模型的標準化評估架構，因此需要對 LLM 在實際醫療保健環境中進行全面驗證。

##### **Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics**
2502.11859v1 by Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li

The Theory of Multiple Intelligences underscores the hierarchical nature of
cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer
a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual
Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial
Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13
mainstream VLMs through nine validated psychometric experiments reveals
significant gaps versus humans (average score 24.95 vs. 68.38), with three key
findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,
weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller
models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading
(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought
(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from
architectural constraints. Identified barriers include weak geometry encoding
and missing dynamic simulation. By linking psychometric BSAs to VLM
capabilities, we provide a diagnostic toolkit for spatial intelligence
evaluation, methodological foundations for embodied AI development, and a
cognitive science-informed roadmap for achieving human-like spatial
intelligence.

摘要：多元智能理論強調認知能力的層次性質。為了推進空間人工智慧，我們開創了一個心理測量框架，在視覺語言模型 (VLM) 中定義了五種基本空間能力 (BSA)：空間知覺、空間關係、空間定向、心智旋轉和空間視覺化。通過九項經過驗證的心理測量實驗對 13 個主流 VLM 進行基準測試，揭示了與人類相比的顯著差距（平均分數 24.95 對 68.38），並得出三個關鍵發現：1) VLM 反映人類層次結構（2D 定向最強，3D 旋轉最弱）具有獨立的 BSA（Pearson's r<0.4）；2) Qwen2-VL-7B 等較小的模型超越了較大的模型，其中 Qwen 領先（30.82），InternVL2 落後（19.6）；3) 思想鏈等干預措施（0.100  accuracy gain）和 5 次訓練（0.259 提升）顯示了架構約束的限制。已識別的障礙包括弱幾何編碼和缺少動態模擬。通過將心理測量 BSA 與 VLM 能力聯繫起來，我們提供了一個用於空間智能評估的診斷工具包、具身 AI 開發的方法論基礎，以及實現類人空間智能的認知科學信息路標。

##### **LLMs as a synthesis between symbolic and continuous approaches to language**
2502.11856v1 by Gemma Boleda

Since the middle of the 20th century, a fierce battle is being fought between
symbolic and continuous approaches to language and cognition. The success of
deep learning models, and LLMs in particular, has been alternatively taken as
showing that the continuous camp has won, or dismissed as an irrelevant
engineering development. However, in this position paper I argue that deep
learning models for language actually represent a synthesis between the two
traditions. This is because 1) deep learning architectures allow for both
continuous/distributed and symbolic/discrete-like representations and
computations; 2) models trained on language make use this flexibility. In
particular, I review recent research in mechanistic interpretability that
showcases how a substantial part of morphosyntactic knowledge is encoded in a
near-discrete fashion in LLMs. This line of research suggests that different
behaviors arise in an emergent fashion, and models flexibly alternate between
the two modes (and everything in between) as needed. This is possibly one of
the main reasons for their wild success; and it is also what makes them
particularly interesting for the study of language and cognition. Is it time
for peace?

摘要：自 20 世紀中葉以來，象徵與連續的語言和認知方法之間展開了一場激烈的戰鬥。深度學習模型，特別是 LLM 的成功，被交替視為連續陣營獲勝的證明，或被視為無關的工程發展而被忽視。然而，在本文中，我認為用於語言的深度學習模型實際上代表了這兩種傳統之間的綜合。這是因為 1) 深度學習架構允許連續/分佈式和符號/離散式表示和計算；2) 在語言上訓練的模型利用了這種靈活性。特別是，我回顧了機制可解釋性的最新研究，展示了形態句法知識的實質部分是如何以近乎離散的方式編碼在 LLM 中的。這條研究線表明，不同的行為以一種新興的方式出現，並且模型根據需要在兩種模式（以及介於兩者之間的所有內容）之間靈活地交替。這可能是它們獲得巨大成功的主要原因之一；這也是它們對語言和認知研究特別有趣的原因。和平的時刻到了嗎？

##### **BaxBench: Can LLMs Generate Correct and Secure Backends?**
2502.11844v1 by Mark Vero, Niels Mündler, Victor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanović, Jingxuan He, Martin Vechev

The automatic generation of programs has long been a fundamental challenge in
computer science. Recent benchmarks have shown that large language models
(LLMs) can effectively generate code at the function level, make code edits,
and solve algorithmic coding tasks. However, to achieve full automation, LLMs
should be able to generate production-quality, self-contained application
modules. To evaluate the capabilities of LLMs in solving this challenge, we
introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for
the generation of backend applications. We focus on backends for three critical
reasons: (i) they are practically relevant, building the core components of
most modern web and cloud software, (ii) they are difficult to get right,
requiring multiple functions and files to achieve the desired functionality,
and (iii) they are security-critical, as they are exposed to untrusted
third-parties, making secure solutions that prevent deployment-time attacks an
imperative. BaxBench validates the functionality of the generated applications
with comprehensive test cases, and assesses their security exposure by
executing end-to-end exploits. Our experiments reveal key limitations of
current LLMs in both functionality and security: (i) even the best model,
OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could
successfully execute security exploits on more than half of the correct
programs generated by each LLM; and (iii) in less popular backend frameworks,
models further struggle to generate correct and secure applications. Progress
on BaxBench signifies important steps towards autonomous and secure software
development with LLMs.

摘要：<paragraph>程式自動產生一直是電腦科學中的基本挑戰。最近的基準測試顯示，大型語言模型 (LLM) 能夠有效產生函數層級的程式碼、進行程式碼編輯，以及解決演算法編碼任務。然而，若要達成完全自動化，LLM 應能夠產生生產品質、獨立的應用程式模組。為了評估 LLM 在解決此挑戰的能力，我們引入了 BaxBench，這是一個包含 392 個後端應用程式產生任務的新評估基準。我們專注於後端有三個關鍵原因：(i) 它們在實務上有其相關性，建構了大多數現代網路和雲端軟體的核心元件；(ii) 它們難以正確執行，需要多個函數和檔案才能達成所需的運作功能；(iii) 它們與安全性息息相關，因為它們會暴露於不受信任的第三方，使得預防部署時攻擊的安全解決方案成為當務之急。BaxBench 使用全面的測試案例驗證產生應用程式的功能，並透過執行端對端漏洞利用來評估其安全性風險。我們的實驗揭露了目前 LLM 在功能和安全性上的主要限制：(i) 即使是最好的模型 OpenAI o1，在程式碼正確性上也僅達到 60%；(ii) 平均而言，我們能夠在每個 LLM 產生的正確程式中成功執行超過一半的安全漏洞利用；(iii) 在較不受歡迎的後端框架中，模型在產生正確且安全的應用程式上更加困難。在 BaxBench 上的進展代表著使用 LLM 朝向自主且安全的軟體開發邁出了重要的一步。</paragraph>

##### **Can LLM Agents Maintain a Persona in Discourse?**
2502.11843v1 by Pranav Bhandari, Nicolas Fay, Michael Wise, Amitava Datta, Stephanie Meek, Usman Naseem, Mehwish Nasim

Large Language Models (LLMs) are widely used as conversational agents,
exploiting their capabilities in various sectors such as education, law,
medicine, and more. However, LLMs are often subjected to context-shifting
behaviour, resulting in a lack of consistent and interpretable
personality-aligned interactions. Adherence to psychological traits lacks
comprehensive analysis, especially in the case of dyadic (pairwise)
conversations. We examine this challenge from two viewpoints, initially using
two conversation agents to generate a discourse on a certain topic with an
assigned personality from the OCEAN framework (Openness, Conscientiousness,
Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This
is followed by using multiple judge agents to infer the original traits
assigned to explore prediction consistency, inter-model agreement, and
alignment with the assigned personality. Our findings indicate that while LLMs
can be guided toward personality-driven dialogue, their ability to maintain
personality traits varies significantly depending on the combination of models
and discourse settings. These inconsistencies emphasise the challenges in
achieving stable and interpretable personality-aligned interactions in LLMs.

摘要：大型語言模型 (LLM) 被廣泛用作對話代理，
在教育、法律、
醫學等各個領域發揮其能力。然而，LLM 經常受到情境轉換
行為的影響，導致缺乏一致且可解釋的
與人格一致的互動。對心理特質的堅持缺乏
全面的分析，特別是在二元 (成對)
對話的情況下。我們從兩個觀點審視這個挑戰，最初使用
兩個對話代理在特定主題上產生論述，並從 OCEAN 框架 (開放性、盡責性、
外向性、宜人性、神經質) 中分配人格，每個特質為高/低。這
接著使用多個評審代理來推斷分配給探索預測一致性、模型間協議的原始特質，
以及與分配人格的一致性。我們的研究結果表明，雖然 LLM
可以引導至以人格為導向的對話，但它們維持
人格特質的能力會根據模型和論述設定的組合而有顯著差異。這些不一致強調了
在 LLM 中實現穩定且可解釋的與人格一致的互動的挑戰。

##### **ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio Chord Recognition**
2502.11840v1 by Muhammad Waseem Akram, Stefano Dettori, Valentina Colla, Giorgio Carlo Buttazzo

Chord recognition serves as a critical task in music information retrieval
due to the abstract and descriptive nature of chords in music analysis. While
audio chord recognition systems have achieved significant accuracy for small
vocabularies (e.g., major/minor chords), large-vocabulary chord recognition
remains a challenging problem. This complexity also arises from the inherent
long-tail distribution of chords, where rare chord types are underrepresented
in most datasets, leading to insufficient training samples. Effective chord
recognition requires leveraging contextual information from audio sequences,
yet existing models, such as combinations of convolutional neural networks,
bidirectional long short-term memory networks, and bidirectional transformers,
face limitations in capturing long-term dependencies and exhibit suboptimal
performance on large-vocabulary chord recognition tasks. This work proposes
ChordFormer, a novel conformer-based architecture designed to tackle structural
chord recognition (e.g., triads, bass, sevenths) for large vocabularies.
ChordFormer leverages conformer blocks that integrate convolutional neural
networks with transformers, thus enabling the model to capture both local
patterns and global dependencies effectively. By addressing challenges such as
class imbalance through a reweighted loss function and structured chord
representations, ChordFormer outperforms state-of-the-art models, achieving a
2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy
on large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling
class imbalance, providing robust and balanced recognition across chord types.
This approach bridges the gap between theoretical music knowledge and practical
applications, advancing the field of large-vocabulary chord recognition.

摘要：和弦辨識由於和弦在音樂分析中具有抽象性和描述性，因此在音樂資訊檢索中扮演著重要的任務。雖然音訊和弦辨識系統已在小型詞彙（例如，大調/小調和弦）中達到顯著的準確度，但大型詞彙和弦辨識仍然是一個具有挑戰性的問題。這種複雜性也來自和弦固有的長尾分佈，其中在大多數資料集中罕見的和弦類型代表性不足，導致訓練樣本不足。有效的和弦辨識需要利用音訊序列中的上下文資訊，但現有的模型，例如卷積神經網路、雙向長短期記憶網路和雙向轉換器的組合，在捕捉長期依賴關係方面面臨限制，並且在大詞彙和弦辨識任務上表現不佳。這項工作提出了 ChordFormer，這是一種新穎的基於變形器的架構，旨在解決大型詞彙的結構和弦辨識（例如，三和弦、低音、七和弦）。ChordFormer 利用變形器區塊將卷積神經網路與變形器整合在一起，從而使模型能夠有效地捕捉局部模式和全局依賴關係。透過重新加權損失函數和結構化和弦表示來解決類別不平衡等挑戰，ChordFormer 優於最先進的模型，在大詞彙和弦資料集上實現了幀準確度提高 2% 和類準確度提高 6%。此外，ChordFormer 在處理類別不平衡方面表現出色，在和弦類型中提供穩健且平衡的辨識。這種方法彌合了理論音樂知識與實際應用之間的差距，推動了大型詞彙和弦辨識領域的發展。

##### **Intuitive physics understanding emerges from self-supervised pretraining on natural videos**
2502.11831v1 by Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, Yann LeCun

We investigate the emergence of intuitive physics understanding in
general-purpose deep neural network models trained to predict masked regions in
natural videos. Leveraging the violation-of-expectation framework, we find that
video prediction models trained to predict outcomes in a learned representation
space demonstrate an understanding of various intuitive physics properties,
such as object permanence and shape consistency. In contrast, video prediction
in pixel space and multimodal large language models, which reason through text,
achieve performance closer to chance. Our comparisons of these architectures
reveal that jointly learning an abstract representation space while predicting
missing parts of sensory input, akin to predictive coding, is sufficient to
acquire an understanding of intuitive physics, and that even models trained on
one week of unique video achieve above chance performance. This challenges the
idea that core knowledge -- a set of innate systems to help understand the
world -- needs to be hardwired to develop an understanding of intuitive
physics.

摘要：我們探討了在經過訓練以預測自然影片中遮蔽區域的通用深度神經網路模型中，直覺物理理解的出現。利用違反預期框架，我們發現經過訓練以預測學習表徵空間中結果的影片預測模型，展現了對各種直覺物理特性的理解，例如物體恆存和形狀一致性。相反地，影片在像素空間和多模態大型語言模型中的預測，透過文字推理，達到的效能接近隨機。我們對這些架構的比較揭示了在預測感官輸入的遺失部分時，同時學習抽象表徵空間，類似於預測編碼，足以獲得對直覺物理的理解，而且即使在獨特影片上訓練一週的模型，也達到了高於隨機的效能。這挑戰了核心知識（一套幫助理解世界的先天系統）需要硬連線才能發展對直覺物理的理解這個想法。

##### **Text Classification in the LLM Era - Where do we stand?**
2502.11830v1 by Sowmya Vajjala, Shwetali Shimangaud

Large Language Models revolutionized NLP and showed dramatic performance
improvements across several tasks. In this paper, we investigated the role of
such language models in text classification and how they compare with other
approaches relying on smaller pre-trained language models. Considering 32
datasets spanning 8 languages, we compared zero-shot classification, few-shot
fine-tuning and synthetic data based classifiers with classifiers built using
the complete human labeled dataset. Our results show that zero-shot approaches
do well for sentiment classification, but are outperformed by other approaches
for the rest of the tasks, and synthetic data sourced from multiple LLMs can
build better classifiers than zero-shot open LLMs. We also see wide performance
disparities across languages in all the classification scenarios. We expect
that these findings would guide practitioners working on developing text
classification systems across languages.

摘要：大型語言模型革新了自然語言處理，並在多項任務中展現出顯著的效能提升。在本文中，我們探討了此類語言模型在文字分類中的角色，以及它們與依賴較小規模預先訓練語言模型的其他方法相比如何。考量涵蓋 8 種語言的 32 個資料集，我們比較了零次學習分類、少次學習微調和合成資料分類器，以及使用完整人工標記資料集建置的分類器。我們的結果顯示，零次學習方法在情緒分類中表現良好，但在其他任務中則不如其他方法，而來自多個大型語言模型的合成資料可以建置比零次學習開放大型語言模型更好的分類器。我們也看到在所有分類情境中，不同語言之間的效能差異很大。我們預期這些發現將引導從事跨語言文字分類系統開發的實務工作者。

##### **Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities**
2502.11829v1 by Hanbin Wang, Xiaoxuan Zhou, Zhipeng Xu, Keyuan Cheng, Yuxin Zuo, Kai Tian, Jingwei Song, Junting Lu, Wenhui Hu, Xueyang Liu

This paper introduces Code-Vision, a benchmark designed to evaluate the
logical understanding and code generation capabilities of Multimodal Large
Language Models (MLLMs). It challenges MLLMs to generate a correct program that
fulfills specific functionality requirements based on a given flowchart, which
visually represents the desired algorithm or process. Code-Vision comprises
three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding
abilities across basic programming, algorithmic, and mathematical
problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.
Experimental results demonstrate that there is a large performance difference
between proprietary and open-source models. On Hard problems, GPT-4o can
achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further
experiments reveal that Code-Vision can pose unique challenges compared to
other multimodal reasoning benchmarks MMCode and MathVista. We also explore the
reason for the poor performance of the open-source models. All data and codes
are available at https://github.com/wanghanbinpanda/CodeVision.

摘要：本文介绍 Code-Vision，此基准测试旨在评估多模态大型语言模型 (MLLM) 的逻辑理解和代码生成能力。它要求 MLLM 根据给定的流程图生成一个正确的程序，以满足特定的功能需求，而流程图直观地表示所需的算法或流程。Code-Vision 包含三个子集：HumanEval-V、Algorithm 和 MATH，它们评估 MLLM 在基本编程、算法和数学问题解决域中的编码能力。我们的实验对 Code-Vision 上的 12 个 MLLM 进行了评估。实验结果表明，专有模型和开源模型之间的性能差异很大。在困难问题上，GPT-4o 可以达到 79.3% 的 pass@1，但最好的开源模型只能达到 15%。进一步的实验表明，与其他多模态推理基准 MMCode 和 MathVista 相比，Code-Vision 可能会带来独特的挑战。我们还探讨了开源模型性能不佳的原因。所有数据和代码均可在 https://github.com/wanghanbinpanda/CodeVision 中获得。

##### **M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis**
2502.11824v1 by Chengyan Wu, Bolei Ma, Yihong Liu, Zheyu Zhang, Ningyuan Deng, Yanshu Li, Baolan Chen, Yi Zhang, Barbara Plank, Yun Xue

Aspect-based sentiment analysis (ABSA) is a crucial task in information
extraction and sentiment analysis, aiming to identify aspects with associated
sentiment elements in text. However, existing ABSA datasets are predominantly
English-centric, limiting the scope for multilingual evaluation and research.
To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7
domains and 21 languages, making it the most extensive multilingual parallel
dataset for ABSA to date. Our primary focus is on triplet extraction, which
involves identifying aspect terms, aspect categories, and sentiment polarities.
The dataset is constructed through an automatic translation process with human
review to ensure quality. We perform extensive experiments using various
baselines to assess performance and compatibility on M-ABSA. Our empirical
findings highlight that the dataset enables diverse evaluation tasks, such as
multilingual and multi-domain transfer learning, and large language model
evaluation, underscoring its inclusivity and its potential to drive
advancements in multilingual ABSA research.

摘要：面向方面的观点分析 (ABSA) 是資訊萃取和觀點分析中的一項重要任務，旨在識別文本中帶有相關觀點元素的方面。然而，現有的 ABSA 資料集以英語為中心，限制了多語言評估和研究的範圍。為了彌補這個差距，我們提出了 M-ABSA，這是一個涵蓋 7 個領域和 21 種語言的綜合性資料集，使其成為迄今為止最廣泛的多語言平行資料集，適用於 ABSA。我們的重點是三元組萃取，其中涉及識別方面術語、方面類別和觀點極性。該資料集是透過自動翻譯過程構建的，並經過人工審查以確保品質。我們使用各種基線進行廣泛的實驗，以評估 M-ABSA 上的效能和相容性。我們的實證結果強調，該資料集支援多樣化的評估任務，例如多語言和多領域遷移學習，以及大型語言模型評估，凸顯其包容性和推動多語言 ABSA 研究進展的潛力。

##### **AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling**
2502.11817v1 by Hao Zhou, Wenge Rong, Jianfei Zhang, Qing Sun, Yuanxin Ouyang, Zhang Xiong

Knowledge Tracing (KT) aims to predict students' future performances based on
their former exercises and additional information in educational settings. KT
has received significant attention since it facilitates personalized
experiences in educational situations. Simultaneously, the autoregressive
modeling on the sequence of former exercises has been proven effective for this
task. One of the primary challenges in autoregressive modeling for Knowledge
Tracing is effectively representing the anterior (pre-response) and posterior
(post-response) states of learners across exercises. Existing methods often
employ complex model architectures to update learner states using question and
response records. In this study, we propose a novel perspective on knowledge
tracing task by treating it as a generative process, consistent with the
principles of autoregressive models. We demonstrate that knowledge states can
be directly represented through autoregressive encodings on a question-response
alternate sequence, where model generate the most probable representation in
hidden state space by analyzing history interactions. This approach underpins
our framework, termed Alternate Autoregressive Knowledge Tracing (AAKT).
Additionally, we incorporate supplementary educational information, such as
question-related skills, into our framework through an auxiliary task, and
include extra exercise details, like response time, as additional inputs. Our
proposed framework is implemented using advanced autoregressive technologies
from Natural Language Generation (NLG) for both training and prediction.
Empirical evaluations on four real-world KT datasets indicate that AAKT
consistently outperforms all baseline models in terms of AUC, ACC, and RMSE.
Furthermore, extensive ablation studies and visualized analysis validate the
effectiveness of key components in AAKT.

摘要：<paragraph>知識追蹤 (KT) 旨在根據學生的前次練習和教育環境中的額外資訊，預測學生的未來表現。KT 自從促進教育情境中的個人化體驗後，便備受關注。同時，前次練習序列上的自迴歸模型已被證明對此任務有效。知識追蹤中自迴歸模型的主要挑戰之一，是有效表示學習者在各項練習中的先驗 (反應前) 和後驗 (反應後) 狀態。現有方法通常採用複雜的模型架構，使用問題和反應記錄來更新學習者狀態。在本研究中，我們提出了一個關於知識追蹤任務的新觀點，將其視為一個生成過程，與自迴歸模型的原理一致。我們證明了知識狀態可以直接透過問答交替序列上的自迴歸編碼來表示，其中模型透過分析歷史互動來生成隱藏狀態空間中最可能的表示。此方法支撐了我們的架構，稱為交替自迴歸知識追蹤 (AAKT)。此外，我們透過輔助任務將補充教育資訊（例如與問題相關的技能）納入我們的架構，並將額外練習細節（例如反應時間）納入額外輸入。我們提出的架構是使用自然語言生成 (NLG) 的先進自迴歸技術，用於訓練和預測。對四個真實世界的 KT 資料集進行的經驗評估表明，AAKT 在 AUC、ACC 和 RMSE 方面始終優於所有基準模型。此外，廣泛的消融研究和視覺化分析驗證了 AAKT 中關鍵組件的有效性。</paragraph>

##### **Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis**
2502.11812v1 by Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou

Fine-tuning significantly improves the performance of Large Language Models
(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims
to provide an in-depth interpretation of the fine-tuning process through
circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike
previous studies
\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}
that focus on tasks where pre-trained models already perform well, we develop a
set of mathematical tasks where fine-tuning yields substantial performance
gains, which are closer to the practical setting. In our experiments, we
identify circuits at various checkpoints during fine-tuning and examine the
interplay between circuit analysis, fine-tuning methods, and task complexities.
First, we find that while circuits maintain high node similarity before and
after fine-tuning, their edges undergo significant changes, which is in
contrast to the previous work
\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}
that show circuits only add some additional components after fine-tuning. Based
on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)
method, which assigns ranks to layers based on edge changes in the circuits.
Experimental results demonstrate that our circuit-based LoRA algorithm achieves
an average performance improvement of 2.46\% over standard LoRA with similar
parameter sizes. Furthermore, we explore how combining circuits from subtasks
can enhance fine-tuning in compositional tasks, providing new insights into the
design of such tasks and deepening the understanding of circuit dynamics and
fine-tuning mechanisms.

摘要：微調大幅提升大型語言模型 (LLM) 的效能，但其底層機制仍鮮為人知。本文旨在透過電路分析，一種機械可解釋性 (MI) 中廣泛使用的工具，提供微調過程的深入詮釋。不同於先前的研究
\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}
專注於預訓練模型已表現良好的任務，我們開發了一組數學任務，其中微調產生顯著的效能提升，更接近實際設定。在我們的實驗中，我們在微調期間的各種檢查點識別電路，並探討電路分析、微調方法和任務複雜度之間的交互作用。首先，我們發現電路在微調前後雖然維持高節點相似度，但其邊緣卻經歷顯著變化，這與先前的研究
\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}
顯示電路僅在微調後新增一些額外組件的結果相反。基於這些觀察，我們開發了一個電路感知低秩適應 (LoRA) 方法，根據電路中的邊緣變化為層級分配秩。實驗結果證明，我們的基於電路的 LoRA 演算法在參數大小相似的條件下，比標準 LoRA 平均提升了 2.46% 的效能。此外，我們探討如何結合子任務的電路來增強組合任務中的微調，為此類任務的設計提供新的見解，並加深對電路動態和微調機制的理解。

##### **FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models**
2502.11811v1 by Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng

Retrieved documents containing noise will hinder Retrieval-Augmented
Generation (RAG) from detecting answer clues, necessitating noise filtering
mechanisms to enhance accuracy.Existing methods use re-ranking or summarization
to identify the most relevant sentences, but directly and accurately locating
answer clues from these large-scale and complex documents remains challenging.
Unlike these document-level operations, we treat noise filtering as a
sentence-level MinMax optimization problem: first identifying the potential
clues from multiple documents using contextual information, then ranking them
by relevance, and finally retaining the least clues through truncation. In this
paper, we propose FineFilter, a novel fine-grained noise filtering mechanism
for RAG consisting of a clue extractor, a re-ranker, and a truncator. We
optimize each module to tackle complex reasoning challenges: (1) Clue extractor
firstly uses sentences containing the answer and similar ones as fine-tuned
targets, aiming at extracting sufficient potential clues; (2) Re-ranker is
trained to prioritize effective clues based on the real feedback from
generation module, with clues capable of generating correct answer as positive
samples and others as negative; (3) Truncator takes the minimum clues needed to
answer the question (truncation point) as fine-tuned targets, and performs
truncation on the re-ranked clues to achieve fine-grained noise filtering.
Experiments on three QA datasets demonstrate that FineFilter significantly
outperforms baselines in terms of performance and inference cost. Further
analysis on each module shows the effectiveness of our optimizations for
complex reasoning.

摘要：<paragraph>檢索到含有雜訊的文件會阻礙檢索增強生成 (RAG) 偵測答案線索，因此需要雜訊過濾機制來增強準確性。現有方法使用重新排序或摘要來找出最相關的句子，但從這些大規模且複雜的文件中直接且準確地找出答案線索仍然具有挑戰性。與這些文件層級的操作不同，我們將雜訊過濾視為一個句子層級的 MinMax 最佳化問題：首先使用脈絡資訊從多個文件中找出潛在線索，接著依據相關性對它們進行排序，最後透過截斷保留最少的線索。在本文中，我們提出 FineFilter，一種創新的細緻雜訊過濾機制，用於 RAG，它包含一個線索萃取器、一個重新排序器和一個截斷器。我們最佳化每個模組來應對複雜的推理挑戰：(1) 線索萃取器首先使用包含答案和類似答案的句子作為微調的目標，旨在萃取足夠的潛在線索；(2) 重新排序器經過訓練，根據生成模組的真實回饋來優先處理有效的線索，其中能夠生成正確答案的線索為正樣本，其他則為負樣本；(3) 截斷器將回答問題所需的最小線索 (截斷點) 視為微調的目標，並對重新排序的線索執行截斷，以達成細緻的雜訊過濾。在三個問答資料集上的實驗證實，FineFilter 在效能和推論成本方面都明顯優於基線。進一步分析每個模組顯示，我們的最佳化對於複雜推理而言是有效的。</paragraph>

##### **Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling**
2502.11809v1 by Yanbiao Ma, Bowei Liu, Wei Dai, Jiayi Chen, Shuo Li

Deep neural networks (DNNs) often exhibit biases toward certain categories
during object recognition, even under balanced training data conditions. The
intrinsic mechanisms underlying these biases remain unclear. Inspired by the
human visual system, which decouples object manifolds through hierarchical
processing to achieve object recognition, we propose a geometric analysis
framework linking the geometric complexity of class-specific perceptual
manifolds in DNNs to model bias. Our findings reveal that differences in
geometric complexity can lead to varying recognition capabilities across
categories, introducing biases. To support this analysis, we present the
Perceptual-Manifold-Geometry library, designed for calculating the geometric
properties of perceptual manifolds.

摘要：深度神經網路 (DNN) 在物件辨識過程中，即使在平衡的訓練資料條件下，通常會對特定類別表現出偏見。這些偏見背後的基本機制仍然不清楚。受人類視覺系統的啟發，人類視覺系統透過階層化處理來解耦物件流形以達成物件辨識，我們提出一個幾何分析架構，將 DNN 中特定類別感知流形的幾何複雜度與模型偏見連結起來。我們的研究結果顯示，幾何複雜度的差異會導致不同類別的辨識能力有所不同，進而造成偏見。為了支持這個分析，我們提出感知流形幾何函式庫，用於計算感知流形的幾何屬性。

##### **Exploring Translation Mechanism of Large Language Models**
2502.11806v1 by Hongbin Zhang, Kehai Chen, Xuefeng Bai, Xiucheng Li, Min Zhang

Large language models (LLMs) have succeeded remarkably in multilingual
translation tasks. However, the inherent translation mechanisms of LLMs remain
poorly understood, largely due to sophisticated architectures and vast
parameter scales. In response to this issue, this study explores the
translation mechanism of LLM from the perspective of computational components
(e.g., attention heads and MLPs). Path patching is utilized to explore causal
relationships between components, detecting those crucial for translation tasks
and subsequently analyzing their behavioral patterns in human-interpretable
terms. Comprehensive analysis reveals that translation is predominantly
facilitated by a sparse subset of specialized attention heads (less than 5\%),
which extract source language, indicator, and positional features. MLPs
subsequently integrate and process these features by transiting towards
English-centric latent representations. Notably, building on the above
findings, targeted fine-tuning of only 64 heads achieves translation
improvement comparable to full-parameter tuning while preserving general
capabilities.

摘要：大型語言模型 (LLM) 在多語言翻譯任務中取得了顯著的成功。然而，LLM 內在的翻譯機制仍未被很好地理解，這主要是由於複雜的架構和龐大的參數規模。為了應對這個問題，本研究從計算元件（例如注意力頭和 MLP）的角度探討了 LLM 的翻譯機制。路徑修補用於探索元件之間的因果關係，檢測對翻譯任務至關重要的元件，並隨後以人類可解釋的方式分析它們的行為模式。綜合分析表明，翻譯主要由稀疏的專門注意力頭（不到 5%）促進，這些注意力頭提取源語言、指標和位置特徵。MLPs 隨後通過轉換為以英語為中心的潛在表示來整合和處理這些特徵。值得注意的是，根據上述發現，僅對 64 個頭進行有針對性的微調，即可實現與全參數調整相當的翻譯改進，同時保留一般能力。

##### **Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning**
2502.11799v1 by Peiying Yu, Guoxin Chen, Jingjing Wang

Despite the remarkable capabilities of large language models (LLMs) in
various reasoning tasks, they still struggle with table reasoning tasks,
particularly in maintaining consistency throughout multi-step reasoning
processes. While existing approaches have explored various decomposition
strategies, they often lack effective mechanisms to identify and correct errors
in intermediate reasoning steps, leading to cascading error propagation. To
address these issues, we propose Table-Critic, a novel multi-agent framework
that facilitates collaborative criticism and iterative refinement of the
reasoning process until convergence to correct solutions. Our framework
consists of four specialized agents: a Judge for error identification, a Critic
for comprehensive critiques, a Refiner for process improvement, and a Curator
for pattern distillation. To effectively deal with diverse and unpredictable
error types, we introduce a self-evolving template tree that systematically
accumulates critique knowledge through experience-driven learning and guides
future reflections. Extensive experiments have demonstrated that Table-Critic
achieves substantial improvements over existing methods, achieving superior
accuracy and error correction rates while maintaining computational efficiency
and lower solution degradation rate.

摘要：儘管大型語言模型 (LLM) 在各種推理任務中展現出非凡的能力，它們在表格推理任務中仍面臨挑戰，特別是在多步驟推理過程中維持一致性方面。現有方法雖然探索了各種分解策略，但它們通常缺乏有效機制來識別和修正中間推理步驟中的錯誤，導致錯誤遞增。為了解決這些問題，我們提出 Table-Critic，一個新穎的多代理架構，它促進協作批評和反覆改進推理過程，直到收斂到正確的解決方案。我們的架構包含四個專業代理：用於錯誤識別的法官、用於全面批評的批評者、用於流程改進的精煉器，以及用於模式萃取的策展人。為了有效處理多樣且不可預測的錯誤類型，我們引入了一個自演化範本樹，它透過經驗驅動的學習系統性地累積批評知識，並引導未來的反思。廣泛的實驗證明，Table-Critic 在現有方法的基礎上取得了顯著的進步，在維持運算效率和較低解決方案劣化率的同時，達到了更高的準確度和錯誤修正率。

##### **Personality Editing for Language Models through Relevant Knowledge Editing**
2502.11789v1 by Seojin Hwang, Yumin Kim, Byeongjeong Kim, Hwanhee Lee

Large Language Models (LLMs) play a vital role in applications like
conversational agents and content creation, where controlling a model's
personality is crucial for maintaining tone, consistency, and engagement.
However, traditional prompt-based techniques for controlling personality often
fall short, as they do not effectively mitigate the model's inherent biases. In
this paper, we introduce a novel method PALETTE that enhances personality
control through knowledge editing. By generating adjustment queries inspired by
psychological assessments, our approach systematically adjusts responses to
personality-related queries similar to modifying factual knowledge, thereby
achieving controlled shifts in personality traits. Experimental results from
both automatic and human evaluations demonstrate that our method enables more
stable and well-balanced personality control in LLMs.

摘要：大型語言模型 (LLM) 在會話代理和內容創作等應用程式中扮演至關重要的角色，其中控制模型的人格特質對於維持語氣、一致性和參與度至關重要。然而，傳統基於提示的控制人格技術通常無法達到預期效果，因為它們無法有效減輕模型固有的偏差。在本文中，我們介紹一種創新的方法 PALETTE，它通過知識編輯來增強人格控制。透過產生受心理評量啟發的調整查詢，我們的做法系統性地調整對人格相關查詢的回應，類似於修改事實知識，從而實現人格特質的受控轉變。來自自動和人工評估的實驗結果表明，我們的模型能夠在 LLM 中實現更穩定且均衡的人格控制。

##### **Efficient Response Generation Method Selection for Fine-Tuning Large Language Models**
2502.11779v1 by Xuan Ren, Qi Chen, Lingqiao Liu

The training data for fine-tuning large language models (LLMs) is typically
structured as input-output pairs. However, for many tasks, there can be
multiple equally valid output variations for the same input. Recent studies
have observed that the choice of output variation used in training can affect
the model's performance. This raises an important question: how can we generate
the most effective output from the many possible response generation strategy
options? Rather than relying on the traditional but resource-intensive
train-and-evaluate approach, this paper proposes a scalable, approximate method
for estimating the quality of a small subset of generated training data derived
from the same input. We then evaluate how well this small subset of generated
output fits the target model we are trying to train. We present a large-scale
benchmark covering diverse reasoning-based datasets to support our study.
  The central idea is that a good output should closely resemble the output
generated by the target LLM. We formalize this 'closeness' as the expected
alignment score between a candidate output and the output sampled from the
target LLM. We connect this measurement to the perplexity metric used in
previous literature and demonstrate that leveraging an alignment-based metric
can provide better predictions of model performance. Using this strategy, we
can evaluate a small subset of the generated output from each response
generation strategy option, then select the most effective strategy. We show
that an LLM trained on data generated by the selected strategy could lead to a
significant performance gain in many cases.

摘要：大型語言模型 (LLM) 的微調訓練資料通常
以輸入輸出配對結構化。然而，對於許多任務而言，相同的輸入可能有多個同樣有效的輸出變化。最近的研究
觀察到訓練中使用的輸出變化選擇會影響模型的效能。這引發了一個重要問題：我們如何從許多可能的回應產生策略選項中產生最有效的輸出？本文提出一個可擴充、近似的方法，用於估計從相同輸入衍生的訓練資料小子集的品質，而非依賴傳統但資源密集的訓練和評估方法。然後我們評估這個產生輸出的小子集與我們嘗試訓練的目標模型的契合程度。我們提出一個涵蓋各種基於推理的資料集的大規模基準，以支持我們的研究。
核心概念是良好的輸出應與目標 LLM 產生的輸出密切相似。我們將這種「接近度」形式化為候選輸出與從目標 LLM 取樣的輸出之間的預期對齊分數。我們將此測量連接到先前文獻中使用的困惑度指標，並證明利用基於對齊的指標可以提供更好的模型效能預測。使用此策略，我們可以評估每個回應產生策略選項所產生輸出的小子集，然後選擇最有效的策略。我們展示在由所選策略產生的資料上訓練的 LLM，在許多情況下可能導致顯著的效能提升。

##### **Deep Neural Networks for Accurate Depth Estimation with Latent Space Features**
2502.11777v1 by Siddiqui Muhammad Yasir, Hyunsik Ahn

Depth estimation plays a pivotal role in advancing human-robot interactions,
especially in indoor environments where accurate 3D scene reconstruction is
essential for tasks like navigation and object handling. Monocular depth
estimation, which relies on a single RGB camera, offers a more affordable
solution compared to traditional methods that use stereo cameras or LiDAR.
However, despite recent progress, many monocular approaches struggle with
accurately defining depth boundaries, leading to less precise reconstructions.
In response to these challenges, this study introduces a novel depth estimation
framework that leverages latent space features within a deep convolutional
neural network to enhance the precision of monocular depth maps. The proposed
model features dual encoder-decoder architecture, enabling both color-to-depth
and depth-to-depth transformations. This structure allows for refined depth
estimation through latent space encoding. To further improve the accuracy of
depth boundaries and local features, a new loss function is introduced. This
function combines latent loss with gradient loss, helping the model maintain
the integrity of depth boundaries. The framework is thoroughly tested using the
NYU Depth V2 dataset, where it sets a new benchmark, particularly excelling in
complex indoor scenarios. The results clearly show that this approach
effectively reduces depth ambiguities and blurring, making it a promising
solution for applications in human-robot interaction and 3D scene
reconstruction.

摘要：深度估計在推進人機互動方面發揮著至關重要的作用，特別是在室內環境中，準確的 3D 場景重建對於導航和物體處理等任務至關重要。單目深度估計依賴於單個 RGB 相機，與使用立體相機或 LiDAR 的傳統方法相比，它提供了一個更經濟的解決方案。然而，儘管最近取得了進展，許多單目方法在準確定義深度邊界方面仍然存在困難，從而導致重建精度降低。為了應對這些挑戰，本研究引入了一個新穎的深度估計框架，該框架利用深度卷積神經網路中的潛在空間特徵來增強單目深度圖的精度。所提出的模型採用雙編碼器-解碼器架構，既能進行顏色到深度的轉換，又能進行深度到深度的轉換。這種結構允許通過潛在空間編碼進行精確的深度估計。為了進一步提高深度邊界和局部特徵的精度，引入了一個新的損失函數。此函數將潛在損失與梯度損失相結合，幫助模型維護深度邊界的完整性。使用 NYU Depth V2 數據集對該框架進行了全面測試，在該數據集上，它設定了一個新的基準，特別是在複雜的室內場景中表現出色。結果清楚地表明，這種方法有效地減少了深度模糊和模糊，使其成為人機互動和 3D 場景重建應用中一種有前途的解決方案。

##### **The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It**
2502.11771v1 by Leonardo Bertolazzi, Philipp Mondorf, Barbara Plank, Raffaella Bernardi

The ability of large language models (LLMs) to validate their output and
identify potential errors is crucial for ensuring robustness and reliability.
However, current research indicates that LLMs struggle with self-correction,
encountering significant challenges in detecting errors. While studies have
explored methods to enhance self-correction in LLMs, relatively little
attention has been given to understanding the models' internal mechanisms
underlying error detection. In this paper, we present a mechanistic analysis of
error detection in LLMs, focusing on simple arithmetic problems. Through
circuit analysis, we identify the computational subgraphs responsible for
detecting arithmetic errors across four smaller-sized LLMs. Our findings reveal
that all models heavily rely on $\textit{consistency heads}$--attention heads
that assess surface-level alignment of numerical values in arithmetic
solutions. Moreover, we observe that the models' internal arithmetic
computation primarily occurs in higher layers, whereas validation takes place
in middle layers, before the final arithmetic results are fully encoded. This
structural dissociation between arithmetic computation and validation seems to
explain why current LLMs struggle to detect even simple arithmetic errors.

摘要：大型語言模型 (LLM) 驗證其輸出並識別潛在錯誤的能力對於確保穩健性和可靠性至關重要。
然而，目前的研究所示，LLM 難以進行自我修正，在檢測錯誤時遇到重大挑戰。儘管研究已探討增強 LLM 自我修正的方法，但對於瞭解模型內部錯誤檢測機制卻關注較少。在本文中，我們提出對 LLM 中錯誤檢測的機制分析，重點關注簡單的算術問題。通過電路分析，我們識別出負責檢測四個較小規模 LLM 中算術錯誤的計算子圖。我們的研究結果表明，所有模型都嚴重依賴於「一致性頭部」--注意頭部，用於評估算術解中數值表面的對齊方式。此外，我們觀察到模型的內部算術運算主要發生在較高層，而驗證則發生在中間層，在最終算術結果完全編碼之前。算術運算和驗證之間的這種結構性分離似乎解釋了為什麼當前的 LLM 難以檢測到即使是簡單的算術錯誤。

##### **Cognitive-Aligned Document Selection for Retrieval-augmented Generation**
2502.11770v1 by Bingyu Wan, Fuxi Zhang, Zhongpeng Qi, Jiayi Ding, Jijun Li, Baoshi Fan, Yijia Zhang, Jun Zhang

Large language models (LLMs) inherently display hallucinations since the
precision of generated texts cannot be guaranteed purely by the parametric
knowledge they include. Although retrieval-augmented generation (RAG) systems
enhance the accuracy and reliability of generative models by incorporating
external documents, these retrieved documents often fail to adequately support
the model's responses in practical applications. To address this issue, we
propose GGatrieval (Fine-\textbf{G}rained \textbf{G}rounded \textbf{A}lignment
Re\textbf{trieval} for verifiable generation), which leverages an LLM to
dynamically update queries and filter high-quality, reliable retrieval
documents. Specifically, we parse the user query into its syntactic components
and perform fine-grained grounded alignment with the retrieved documents. For
query components that cannot be individually aligned, we propose a dynamic
semantic compensation mechanism that iteratively refines and rewrites the query
while continuously updating the retrieval results. This iterative process
continues until the retrieved documents sufficiently support the query's
response. Our approach introduces a novel criterion for filtering retrieved
documents, closely emulating human strategies for acquiring targeted
information. This ensures that the retrieved content effectively supports and
verifies the generated outputs. On the ALCE benchmark, our method significantly
surpasses a wide range of baselines, achieving state-of-the-art performance.

摘要：大型語言模型 (LLM) 本質上會出現幻覺，因為生成的文本的準確性無法僅透過它們包含的參數化知識來保證。儘管檢索增強生成 (RAG) 系統透過納入外部文件來提升生成模型的準確性和可靠性，但這些檢索的文件在實際應用中常常無法充分支援模型的回應。為了解決這個問題，我們提出 GGatrieval（用於可驗證生成的精細化粒度化基礎對齊檢索），它利用 LLM 來動態更新查詢並過濾高品質、可靠的檢索文件。具體來說，我們將使用者查詢分析成其語法組成部分，並對檢索文件執行精細化粒度化基礎對齊。對於無法個別對齊的查詢組成部分，我們提出一個動態語義補償機制，在持續更新檢索結果的同時，反覆修正和重寫查詢。這個反覆的程序會持續到檢索的文件充分支援查詢的回應為止。我們的做法引進了一個新的檢索文件過濾標準，嚴密地模擬人類獲取目標資訊的策略。這確保檢索的內容有效地支援和驗證生成的輸出。在 ALCE 基準測試中，我們的做法顯著超越各種基線，達成最先進的效能。

##### **From Selection to Generation: A Survey of LLM-based Active Learning**
2502.11767v1 by Yu Xia, Subhojyoti Mukherjee, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joe Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, Seunghyun Yoon, Ting-Hao Kenneth Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian McAuley

Active Learning (AL) has been a powerful paradigm for improving model
efficiency and performance by selecting the most informative data points for
labeling and training. In recent active learning frameworks, Large Language
Models (LLMs) have been employed not only for selection but also for generating
entirely new data instances and providing more cost-effective annotations.
Motivated by the increasing importance of high-quality data and efficient model
training in the era of LLMs, we present a comprehensive survey on LLM-based
Active Learning. We introduce an intuitive taxonomy that categorizes these
techniques and discuss the transformative roles LLMs can play in the active
learning loop. We further examine the impact of AL on LLM learning paradigms
and its applications across various domains. Finally, we identify open
challenges and propose future research directions. This survey aims to serve as
an up-to-date resource for researchers and practitioners seeking to gain an
intuitive understanding of LLM-based AL techniques and deploy them to new
applications.

摘要：主動學習 (AL) 透過挑選最具資訊性的資料點來標記和訓練，已成為一種強大的範例，用以提升模型效率和效能。在最近的主動學習架構中，大型語言模型 (LLM) 不僅用於挑選，也用於產生全新的資料實例，並提供更具成本效益的註解。在大型語言模型時代，由於高品質資料和高效能模型訓練日益重要，我們針對基於大型語言模型的主動學習提出了一項全面的調查。我們提出一個直覺式的分類法，用以分類這些技術，並探討大型語言模型在主動學習迴圈中可以扮演的轉型角色。我們進一步探討主動學習對大型語言模型學習範例的影響，以及它在各種領域中的應用。最後，我們找出開放式挑戰，並提出未來的研究方向。本調查旨在作為研究人員和實務工作者的最新資源，用以獲得對基於大型語言模型的主動學習技術的直覺式理解，並將其部署至新的應用程式。

##### **Warmup-Distill: Bridge the Distribution Mismatch between Teacher and Student before Knowledge Distillation**
2502.11766v1 by Zengkui Sun, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou

The widespread deployment of Large Language Models (LLMs) is hindered by the
high computational demands, making knowledge distillation (KD) crucial for
developing compact smaller ones. However, the conventional KD methods endure
the distribution mismatch issue between the teacher and student models, leading
to the poor performance of distillation. For instance, the widely-used KL-based
methods suffer the mode-averaging and mode-collapsing problems, since the
mismatched probabitliy distribution between both models. Previous studies
mainly optimize this issue via different distance calculations towards the
distribution of both models. Unfortunately, the distribution mismatch issue
still exists in the early stage of the distillation. Hence, to reduce the
impact of distribution mismatch, we propose a simple yet efficient method,
named Warmup-Distill, which aligns the distillation of the student to that of
the teacher in advance of distillation. Specifically, we first detect the
distribution of the student model in practical scenarios with its internal
knowledge, and then modify the knowledge with low probability via the teacher
as the checker. Consequently, Warmup-Distill aligns the internal student's
knowledge to that of the teacher, which expands the distribution of the student
with the teacher's, and assists the student model to learn better in the
subsequent distillation. Experiments on the seven benchmarks demonstrate that
Warmup-Distill could provide a warmup student more suitable for distillation,
which outperforms the vanilla student by as least +0.4 averaged score among all
benchmarks. Noteably, with the assistance of Warmup-Distill, the distillation
on the math task could yield a further improvement, at most +1.9% accuracy.

摘要：大型語言模型 (LLM) 的廣泛部署受到高運算需求的阻礙，這使得知識蒸餾 (KD) 對於開發緊湊型的小型模型至關重要。然而，傳統的 KD 方法忍受了教師和學生模型之間的分布不匹配問題，導致蒸餾效果不佳。例如，廣泛使用的基於 KL 的方法會出現模式平均和模式崩潰問題，因為兩個模型之間的機率分佈不匹配。先前的研究主要透過不同的距離計算來最佳化這個問題，以朝向兩個模型的分布。不幸的是，分布不匹配的問題仍然存在於蒸餾的早期階段。因此，為了減少分布不匹配的影響，我們提出了一種簡單但有效的方法，稱為 Warmup-Distill，它在蒸餾之前將學生的蒸餾與教師的蒸餾對齊。具體來說，我們首先使用其內部知識在實際場景中檢測學生的分布，然後透過教師作為檢查員修改低機率的知識。因此，Warmup-Distill 將學生的內部知識與教師的知識對齊，這會將學生的分布擴展到教師的分布，並協助學生模型在後續的蒸餾中學習得更好。在七個基準測試上的實驗表明，Warmup-Distill 可以提供更適合蒸餾的熱身學生，在所有基準測試中，其表現優於香草學生至少 +0.4 的平均分數。值得注意的是，在 Warmup-Distill 的協助下，數學任務上的蒸餾可以進一步提升，最多可提升 +1.9% 的準確度。

##### **Lightweight Deepfake Detection Based on Multi-Feature Fusion**
2502.11763v1 by Siddiqui Muhammad Yasir, Hyun Kim

Deepfake technology utilizes deep learning based face manipulation techniques
to seamlessly replace faces in videos creating highly realistic but
artificially generated content. Although this technology has beneficial
applications in media and entertainment misuse of its capabilities may lead to
serious risks including identity theft cyberbullying and false information. The
integration of DL with visual cognition has resulted in important technological
improvements particularly in addressing privacy risks caused by artificially
generated deepfake images on digital media platforms. In this study we propose
an efficient and lightweight method for detecting deepfake images and videos
making it suitable for devices with limited computational resources. In order
to reduce the computational burden usually associated with DL models our method
integrates machine learning classifiers in combination with keyframing
approaches and texture analysis. Moreover the features extracted with a
histogram of oriented gradients (HOG) local binary pattern (LBP) and KAZE bands
were integrated to evaluate using random forest extreme gradient boosting extra
trees and support vector classifier algorithms. Our findings show a
feature-level fusion of HOG LBP and KAZE features improves accuracy to 92% and
96% on FaceForensics++ and Celeb-DFv2 respectively.

摘要：深度偽造技術利用基於深度學習的換臉技術，可無縫替換影片中的臉孔，創造出高度逼真但人工產生的內容。儘管這項技術在媒體和娛樂方面有益，但若誤用其功能可能會導致嚴重的風險，包括身分盜用、網路霸凌和虛假訊息。深度學習與視覺認知的整合已帶來重要的技術進步，特別是在解決由數位媒體平台上的人工深度偽造影像所造成的隱私風險方面。在本研究中，我們提出了一種用於偵測深度偽造影像和影片的有效且輕量級的方法，使其適用於運算資源有限的裝置。為了降低通常與深度學習模型相關的運算負擔，我們的做法結合了機器學習分類器、關鍵影格方法和紋理分析。此外，我們整合了使用方向梯度直方圖 (HOG)、局部二進位模式 (LBP) 和 KAZE 頻段所萃取出的特徵，並使用隨機森林、極端梯度提升、額外樹木和支援向量分類器演算法進行評估。我們的研究結果顯示，HOG、LBP 和 KAZE 特徵的層級融合將準確度提升至 92%，分別在 FaceForensics++ 和 Celeb-DFv2 上達到 96%。

##### **HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims**
2502.11753v1 by Michiel van der Meer, Pavel Korshunov, Sébastien Marcel, Lonneke van der Plas

Misinformation can be countered with fact-checking, but the process is costly
and slow. Identifying checkworthy claims is the first step, where automation
can help scale fact-checkers' efforts. However, detection methods struggle with
content that is 1) multimodal, 2) from diverse domains, and 3) synthetic. We
introduce HintsOfTruth, a public dataset for multimodal checkworthiness
detection with $27$K real-world and synthetic image/claim pairs. The mix of
real and synthetic data makes this dataset unique and ideal for benchmarking
detection methods. We compare fine-tuned and prompted Large Language Models
(LLMs). We find that well-configured lightweight text-based encoders perform
comparably to multimodal models but the first only focus on identifying
non-claim-like content. Multimodal LLMs can be more accurate but come at a
significant computational cost, making them impractical for large-scale
applications. When faced with synthetic data, multimodal models perform more
robustly

摘要：錯誤訊息可以透過事實查核來反駁，但這個過程既昂貴又緩慢。辨識需要查核的說法是第一步，自動化可以幫助擴大事實查核人員的努力。然而，偵測方法會在處理 1) 多模態、2) 來自不同領域，以及 3) 合成的內容時遇到困難。我們引進 HintsOfTruth，一個用於多模態查核價值偵測的公開資料集，其中包含 27K 個真實世界和合成的影像/說法配對。真實和合成資料的組合讓這個資料集獨一無二，非常適合用於基準偵測方法。我們比較微調和提示的大語言模型 (LLM)。我們發現，設定良好的輕量級文字編碼器的表現與多模態模型相當，但前者只專注於辨識非說法類型的內容。多模態 LLM 可能更準確，但需要大量的運算成本，這讓它們不適用於大規模的應用。在面對合成資料時，多模態模型的表現更強健。

##### **Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning**
2502.11751v1 by Yuqi Pang, Bowen Yang, Haoqin Tu, Yun Cao, Zeyu Zhang

Although Large Language Models (LLMs) excel in reasoning and generation for
language tasks, they are not specifically designed for multimodal challenges.
Training Multimodal Large Language Models (MLLMs), however, is
resource-intensive and constrained by various training limitations. In this
paper, we propose the Modular-based Visual Contrastive Decoding (MVCD)
framework to move this obstacle. Our framework leverages LLMs' In-Context
Learning (ICL) capability and the proposed visual contrastive-example decoding
(CED), specifically tailored for this framework, without requiring any
additional training. By converting visual signals into text and focusing on
contrastive output distributions during decoding, we can highlight the new
information introduced by contextual examples, explore their connections, and
avoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual
perception to make it see and reason over the input visuals. To demonstrate
MVCD's effectiveness, we conduct experiments with four LLMs across five
question answering datasets. Our results not only show consistent improvement
in model accuracy but well explain the effective components inside our decoding
strategy. Our code will be available at https://github.com/Pbhgit/MVCD.

摘要：儘管大型語言模型 (LLM) 在語言任務的推理和生成方面表現優異，但它們並非專門針對多模態挑戰而設計。然而，訓練多模態大型語言模型 (MLLM) 十分耗費資源，並受到各種訓練限制。在本文中，我們提出基於模組的視覺對比解碼 (MVCD) 架構來克服這個障礙。我們的架構利用 LLM 的情境學習 (ICL) 能力和專門為此架構量身打造的視覺對比範例解碼 (CED)，而無需任何額外訓練。透過將視覺信號轉換為文字，並在解碼過程中專注於對比輸出分佈，我們可以突顯情境範例引入的新資訊，探索它們的關聯性，並避免過度依賴先前編碼的知識。MVCD 增強了 LLM 的視覺感知能力，使其能夠觀察並推論輸入視覺效果。為了證明 MVCD 的有效性，我們使用四個 LLM 在五個問答資料集上進行實驗。我們的結果不僅顯示模型準確度持續提升，還能清楚說明我們的解碼策略中的有效組成部分。我們的程式碼將在 https://github.com/Pbhgit/MVCD 公開。

##### **SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL**
2502.11741v1 by Shuai Lyu, Haoran Luo, Zhonghong Ou, Yifan Zhu, Xiaoran Shang, Yang Qin, Meina Song

The Text-to-SQL(Text2SQL) task aims to convert natural language queries into
executable SQL queries. Thanks to the application of large language models
(LLMs), significant progress has been made in this field. However, challenges
such as model scalability, limited generation space, and coherence issues in
SQL generation still persist. To address these issues, we propose SQL-o1, a
Self-Reward-based heuristic search method designed to enhance the reasoning
ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree
Search (MCTS) for heuristic process-level search and constructs a Schema-Aware
dataset to help the model better understand database schemas. Extensive
experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves
execution accuracy by 10.8\% on the complex Bird dataset compared to the latest
baseline methods, even outperforming GPT-4-based approaches. Additionally,
SQL-o1 excels in few-shot learning scenarios and shows strong cross-model
transferability. Our code is publicly available
at:https://github.com/ShuaiLyu0110/SQL-o1.

摘要：文本转 SQL（Text2SQL）任务旨在将自然语言查询转换为可执行的 SQL 查询。得益于大型语言模型（LLM）的应用，该领域取得了显著进展。然而，模型可扩展性、生成空间受限和 SQL 生成的连贯性问题等挑战仍然存在。为了解决这些问题，我们提出了 SQL-o1，这是一种基于自我奖励的启发式搜索方法，旨在增强 LLM 在 SQL 查询生成中的推理能力。SQL-o1 结合了蒙特卡罗树搜索（MCTS）用于启发式过程级搜索，并构建了一个模式感知数据集，以帮助模型更好地理解数据库模式。在 Bird 和 Spider 数据集上的大量实验表明，与最新的基准方法相比，SQL-o1 将复杂 Bird 数据集上的执行准确率提高了 10.8%，甚至优于基于 GPT-4 的方法。此外，SQL-o1 在少样本学习场景中表现出色，并显示出强大的跨模型可迁移性。我们的代码已公开发布在：https://github.com/ShuaiLyu0110/SQL-o1。

