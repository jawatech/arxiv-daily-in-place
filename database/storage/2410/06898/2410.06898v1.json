{"2410.06898": {"publish_time": "2024-10-09", "title": "Generative Model for Less-Resourced Language with 1 billion parameters", "paper_summary": "Large language models (LLMs) are a basic infrastructure for modern natural\nlanguage processing. Many commercial and open-source LLMs exist for English,\ne.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on\nmostly English texts, their fluency and knowledge of low-resource languages and\nsocieties are superficial. We present the development of large generative\nlanguage models for a less-resourced language. GaMS 1B - Generative Model for\nSlovene with 1 billion parameters was created by continuing pretraining of the\nexisting English OPT model. We developed a new tokenizer adapted to Slovene,\nCroatian, and English languages and used embedding initialization methods FOCUS\nand WECHSEL to transfer the embeddings from the English OPT model. We evaluate\nour models on several classification datasets from the Slovene suite of\nbenchmarks and generative sentence simplification task SENTA. We only used a\nfew-shot in-context learning of our models, which are not yet\ninstruction-tuned. For classification tasks, in this mode, the generative\nmodels lag behind the existing Slovene BERT-type models fine-tuned for specific\ntasks. On a sentence simplification task, the GaMS models achieve comparable or\nbetter performance than the GPT-3.5-Turbo model.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u662f\u73fe\u4ee3\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u57fa\u672c\u57fa\u790e\u8a2d\u65bd\u3002\u6709\u8a31\u591a\u5546\u696d\u548c\u958b\u653e\u539f\u59cb\u78bc\u7684 LLM \u9069\u7528\u65bc\u82f1\u8a9e\uff0c\u4f8b\u5982 ChatGPT\u3001Llama\u3001Falcon \u548c Mistral\u3002\u7531\u65bc\u9019\u4e9b\u6a21\u578b\u5927\u591a\u8a13\u7df4\u65bc\u82f1\u8a9e\u6587\u672c\uff0c\u56e0\u6b64\u5b83\u5011\u5c0d\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u548c\u793e\u6703\u7684\u6d41\u66a2\u5ea6\u548c\u77e5\u8b58\u90fd\u5f88\u819a\u6dfa\u3002\u6211\u5011\u5c55\u793a\u4e86\u91dd\u5c0d\u8cc7\u6e90\u8f03\u5c11\u7684\u8a9e\u8a00\u958b\u767c\u5927\u578b\u751f\u6210\u8a9e\u8a00\u6a21\u578b\u3002GaMS 1B - 10 \u5104\u500b\u53c3\u6578\u7684\u65af\u6d1b\u7dad\u5c3c\u4e9e\u751f\u6210\u6a21\u578b\u662f\u900f\u904e\u6301\u7e8c\u9810\u8a13\u7df4\u73fe\u6709\u7684\u82f1\u8a9e OPT \u6a21\u578b\u800c\u5efa\u7acb\u7684\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u9069\u7528\u65bc\u65af\u6d1b\u7dad\u5c3c\u4e9e\u8a9e\u3001\u514b\u7f85\u57c3\u897f\u4e9e\u8a9e\u548c\u82f1\u8a9e\u7684\u65b0\u5206\u8a5e\u5668\uff0c\u4e26\u4f7f\u7528\u5d4c\u5165\u521d\u59cb\u5316\u65b9\u6cd5 FOCUS \u548c WECHSEL \u5f9e\u82f1\u8a9e OPT \u6a21\u578b\u50b3\u8f38\u5d4c\u5165\u3002\u6211\u5011\u5728\u65af\u6d1b\u7dad\u5c3c\u4e9e\u57fa\u6e96\u5957\u4ef6\u4e2d\u7684\u5e7e\u500b\u5206\u985e\u8cc7\u6599\u96c6\u548c\u751f\u6210\u5f0f\u53e5\u5b50\u7c21\u5316\u4efb\u52d9 SENTA \u4e0a\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\u3002\u6211\u5011\u50c5\u4f7f\u7528\u6211\u5011\u6a21\u578b\u7684\u5c11\u91cf\u60c5\u5883\u5b78\u7fd2\uff0c\u9019\u4e9b\u6a21\u578b\u5c1a\u672a\u91dd\u5c0d\u6307\u4ee4\u9032\u884c\u8abf\u6574\u3002\u5c0d\u65bc\u5206\u985e\u4efb\u52d9\uff0c\u5728\u6b64\u6a21\u5f0f\u4e2d\uff0c\u751f\u6210\u5f0f\u6a21\u578b\u843d\u5f8c\u65bc\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u9032\u884c\u5fae\u8abf\u7684\u73fe\u6709\u65af\u6d1b\u7dad\u5c3c\u4e9e BERT \u985e\u578b\u6a21\u578b\u3002\u5728\u53e5\u5b50\u7c21\u5316\u4efb\u52d9\u4e2d\uff0cGaMS \u6a21\u578b\u9054\u5230\u4e86\u8207 GPT-3.5-Turbo \u6a21\u578b\u76f8\u7576\u6216\u66f4\u597d\u7684\u6548\u80fd\u3002", "author": "Domen Vre\u0161 et.al.", "authors": "Domen Vre\u0161, Martin Bo\u017ei\u010d, Alja\u017e Poto\u010dnik, Toma\u017e Martin\u010di\u010d, Marko Robnik-\u0160ikonja", "id": "2410.06898v1", "paper_url": "http://arxiv.org/abs/2410.06898v1", "repo": "null"}}