{"2411.03729": {"publish_time": "2024-11-06", "title": "Relation Learning and Aggregate-attention for Multi-person Motion Prediction", "paper_summary": "Multi-person motion prediction is an emerging and intricate task with broad\nreal-world applications. Unlike single person motion prediction, it considers\nnot just the skeleton structures or human trajectories but also the\ninteractions between others. Previous methods use various networks to achieve\nimpressive predictions but often overlook that the joints relations within an\nindividual (intra-relation) and interactions among groups (inter-relation) are\ndistinct types of representations. These methods often lack explicit\nrepresentation of inter&intra-relations, and inevitably introduce undesired\ndependencies. To address this issue, we introduce a new collaborative framework\nfor multi-person motion prediction that explicitly modeling these relations:a\nGCN-based network for intra-relations and a novel reasoning network for\ninter-relations.Moreover, we propose a novel plug-and-play aggregation module\ncalled the Interaction Aggregation Module (IAM), which employs an\naggregate-attention mechanism to seamlessly integrate these relations.\nExperiments indicate that the module can also be applied to other dual-path\nmodels. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as\nwell as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that\nour method achieves state-of-the-art performance.", "paper_summary_zh": "\u591a\u4eba\u52d5\u4f5c\u9810\u6e2c\u662f\u4e00\u9805\u65b0\u8208\u4e14\u8907\u96dc\u7684\u4efb\u52d9\uff0c\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u6709\u5ee3\u6cdb\u7684\u61c9\u7528\u3002\u5b83\u4e0d\u540c\u65bc\u55ae\u4eba\u52d5\u4f5c\u9810\u6e2c\uff0c\u5b83\u4e0d\u50c5\u8003\u616e\u9aa8\u67b6\u7d50\u69cb\u6216\u4eba\u9ad4\u8ecc\u8de1\uff0c\u9084\u8003\u616e\u4eba\u8207\u4eba\u4e4b\u9593\u7684\u4e92\u52d5\u3002\u5148\u524d\u7684\u5404\u7a2e\u65b9\u6cd5\u4f7f\u7528\u5404\u7a2e\u7db2\u8def\u4f86\u5be6\u73fe\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9810\u6e2c\uff0c\u4f46\u5e38\u5e38\u5ffd\u8996\u500b\u4eba\u5167\u90e8\u7684\u95dc\u7bc0\u95dc\u4fc2\uff08\u5167\u90e8\u95dc\u4fc2\uff09\u548c\u7fa4\u7d44\u4e4b\u9593\u7684\u4e92\u52d5\uff08\u76f8\u4e92\u95dc\u4fc2\uff09\u662f\u4e0d\u540c\u985e\u578b\u7684\u8868\u793a\u3002\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u5c0d\u5167\u90e8\u548c\u5916\u90e8\u95dc\u4fc2\u7684\u660e\u78ba\u8868\u793a\uff0c\u4e26\u4e0d\u53ef\u907f\u514d\u5730\u5f15\u5165\u4e86\u4e0d\u9700\u8981\u7684\u4f9d\u8cf4\u95dc\u4fc2\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7684\u5354\u4f5c\u6846\u67b6\uff0c\u7528\u65bc\u591a\u4eba\u54e1\u52d5\u4f5c\u9810\u6e2c\uff0c\u8a72\u6846\u67b6\u660e\u78ba\u5730\u5c0d\u9019\u4e9b\u95dc\u4fc2\u5efa\u6a21\uff1a\u4e00\u500b\u57fa\u65bc GCN \u7684\u7db2\u8def\u7528\u65bc\u5167\u90e8\u95dc\u4fc2\uff0c\u4e00\u500b\u65b0\u7684\u63a8\u7406\u7db2\u8def\u7528\u65bc\u76f8\u4e92\u95dc\u4fc2\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u5373\u63d2\u5373\u7528\u805a\u5408\u6a21\u7d44\uff0c\u7a31\u70ba\u4e92\u52d5\u805a\u5408\u6a21\u7d44 (IAM)\uff0c\u5b83\u63a1\u7528\u805a\u5408\u6ce8\u610f\u529b\u6a5f\u5236\u4f86\u7121\u7e2b\u6574\u5408\u9019\u4e9b\u95dc\u4fc2\u3002\u5be6\u9a57\u8868\u660e\uff0c\u8a72\u6a21\u7d44\u4e5f\u53ef\u4ee5\u61c9\u7528\u65bc\u5176\u4ed6\u96d9\u8def\u5f91\u6a21\u578b\u3002\u5728 3DPW\u30013DPW-RC\u3001CMU-Mocap\u3001MuPoTS-3D \u4ee5\u53ca\u5408\u6210\u8cc7\u6599\u96c6 Mix1 \u548c Mix2\uff089 \u5230 15 \u4eba\uff09\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002", "author": "Kehua Qu et.al.", "authors": "Kehua Qu, Rui Ding, Jin Tang", "id": "2411.03729v1", "paper_url": "http://arxiv.org/abs/2411.03729v1", "repo": "null"}}