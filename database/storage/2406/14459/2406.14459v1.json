{"2406.14459": {"publish_time": "2024-06-20", "title": "Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models", "paper_summary": "Language models like BERT excel at sentence classification tasks due to\nextensive pre-training on general data, but their robustness to parameter\ncorruption is unexplored. To understand this better, we look at what happens if\na language model is \"broken\", in the sense that some of its parameters are\ncorrupted and then recovered by fine-tuning. Strategically corrupting BERT\nvariants at different levels, we find corrupted models struggle to fully\nrecover their original performance, with higher corruption causing more severe\ndegradation. Notably, bottom-layer corruption affecting fundamental linguistic\nfeatures is more detrimental than top-layer corruption. Our insights contribute\nto understanding language model robustness and adaptability under adverse\nconditions, informing strategies for developing resilient NLP systems against\nparameter perturbations.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\uff0c\u4f8b\u5982 BERT\uff0c\u5728\u53e5\u5b50\u5206\u985e\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u56e0\u70ba\u5728\u4e00\u822c\u8cc7\u6599\u4e0a\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u9810\u5148\u8a13\u7df4\uff0c\u4f46\u5b83\u5011\u5c0d\u53c3\u6578\u640d\u6bc0\u7684\u7a69\u5065\u6027\u5c1a\u672a\u63a2\u7d22\u3002\u70ba\u4e86\u66f4\u597d\u5730\u7406\u89e3\u9019\u4e00\u9ede\uff0c\u6211\u5011\u89c0\u5bdf\u5982\u679c\u8a9e\u8a00\u6a21\u578b\u300c\u640d\u6bc0\u300d\uff0c\u5373\u67d0\u4e9b\u53c3\u6578\u640d\u6bc0\uff0c\u7136\u5f8c\u900f\u904e\u5fae\u8abf\u6062\u5fa9\uff0c\u6703\u767c\u751f\u4ec0\u9ebc\u4e8b\u3002\u5728\u4e0d\u540c\u5c64\u7d1a\u7b56\u7565\u6027\u5730\u640d\u6bc0 BERT \u8b8a\u9ad4\uff0c\u6211\u5011\u767c\u73fe\u640d\u6bc0\u7684\u6a21\u578b\u96e3\u4ee5\u5b8c\u5168\u6062\u5fa9\u5176\u539f\u59cb\u6548\u80fd\uff0c\u640d\u6bc0\u7a0b\u5ea6\u8d8a\u9ad8\uff0c\u6548\u80fd\u4e0b\u964d\u8d8a\u56b4\u91cd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5f71\u97ff\u57fa\u672c\u8a9e\u8a00\u7279\u5fb5\u7684\u5e95\u5c64\u640d\u6bc0\u6bd4\u9802\u5c64\u640d\u6bc0\u66f4\u5177\u7834\u58de\u6027\u3002\u6211\u5011\u7684\u898b\u89e3\u6709\u52a9\u65bc\u4e86\u89e3\u8a9e\u8a00\u6a21\u578b\u5728\u4e0d\u5229\u689d\u4ef6\u4e0b\u7684\u7a69\u5065\u6027\u548c\u9069\u61c9\u6027\uff0c\u4e26\u70ba\u958b\u767c\u80fd\u62b5\u79a6\u53c3\u6578\u64fe\u52d5\u7684\u5177\u97cc\u6027 NLP \u7cfb\u7d71\u63d0\u4f9b\u7b56\u7565\u3002", "author": "Shijie Han et.al.", "authors": "Shijie Han, Zhenyu Zhang, Andrei Arsene Simion", "id": "2406.14459v1", "paper_url": "http://arxiv.org/abs/2406.14459v1", "repo": "null"}}