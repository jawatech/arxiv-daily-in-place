{"2406.14343": {"publish_time": "2024-06-20", "title": "iWISDM: Assessing instruction following in multimodal models at scale", "paper_summary": "The ability to perform complex tasks from detailed instructions is a key to\nmany remarkable achievements of our species. As humans, we are not only capable\nof performing a wide variety of tasks but also very complex ones that may\nentail hundreds or thousands of steps to complete. Large language models and\ntheir more recent multimodal counterparts that integrate textual and visual\ninputs have achieved unprecedented success in performing complex tasks. Yet,\nmost existing benchmarks are largely confined to single-modality inputs (either\ntext or vision), narrowing the scope of multimodal assessments, particularly\nfor instruction-following in multimodal contexts. To bridge this gap, we\nintroduce the instructed-Virtual VISual Decision Making (iWISDM) environment\nengineered to generate a limitless array of vision-language tasks of varying\ncomplexity. Using iWISDM, we compiled three distinct benchmarks of instruction\nfollowing visual tasks across varying complexity levels and evaluated several\nnewly developed multimodal models on these benchmarks. Our findings establish\niWISDM as a robust benchmark for assessing the instructional adherence of both\nexisting and emergent multimodal models and highlight a large gap between these\nmodels' ability to precisely follow instructions with that of humans.", "paper_summary_zh": "\u80fd\u5920\u6839\u64da\u8a73\u7d30\u7684\u8aaa\u660e\u57f7\u884c\u8907\u96dc\u4efb\u52d9\uff0c\u662f\u6211\u5011\u4eba\u985e\u8a31\u591a\u975e\u51e1\u6210\u5c31\u7684\u95dc\u9375\u3002\u4f5c\u70ba\u4eba\u985e\uff0c\u6211\u5011\u4e0d\u50c5\u80fd\u5920\u57f7\u884c\u7a2e\u985e\u7e41\u591a\u7684\u4efb\u52d9\uff0c\u9084\u80fd\u57f7\u884c\u975e\u5e38\u8907\u96dc\u7684\u4efb\u52d9\uff0c\u9019\u4e9b\u4efb\u52d9\u53ef\u80fd\u9700\u8981\u6578\u767e\u6216\u6578\u5343\u500b\u6b65\u9a5f\u624d\u80fd\u5b8c\u6210\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u53ca\u5176\u6574\u5408\u6587\u5b57\u548c\u8996\u89ba\u8f38\u5165\u7684\u6700\u65b0\u591a\u6a21\u614b\u5c0d\u61c9\u7269\uff0c\u5728\u57f7\u884c\u8907\u96dc\u4efb\u52d9\u65b9\u9762\u53d6\u5f97\u4e86\u524d\u6240\u672a\u6709\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u6e96\u6e2c\u8a66\u5927\u591a\u50c5\u9650\u65bc\u55ae\u4e00\u6a21\u614b\u8f38\u5165\uff08\u6587\u5b57\u6216\u8996\u89ba\uff09\uff0c\u9019\u7e2e\u5c0f\u4e86\u591a\u6a21\u614b\u8a55\u4f30\u7684\u7bc4\u570d\uff0c\u7279\u5225\u662f\u91dd\u5c0d\u591a\u6a21\u614b\u74b0\u5883\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6307\u4ee4\u5f0f\u865b\u64ec\u8996\u89ba\u6c7a\u7b56\u5236\u5b9a (iWISDM) \u74b0\u5883\uff0c\u65e8\u5728\u751f\u6210\u7121\u6578\u7a2e\u985e\u3001\u8907\u96dc\u5ea6\u5404\u7570\u7684\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u3002\u4f7f\u7528 iWISDM\uff0c\u6211\u5011\u7de8\u5236\u4e86\u4e09\u500b\u4e0d\u540c\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u6db5\u84cb\u4e86\u4e0d\u540c\u8907\u96dc\u7a0b\u5ea6\u7684\u8996\u89ba\u4efb\u52d9\u6307\u4ee4\u9075\u5faa\uff0c\u4e26\u6839\u64da\u9019\u4e9b\u57fa\u6e96\u6e2c\u8a66\u8a55\u4f30\u4e86\u5e7e\u500b\u65b0\u958b\u767c\u7684\u591a\u6a21\u614b\u6a21\u578b\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u78ba\u7acb\u4e86 iWISDM \u6210\u70ba\u8a55\u4f30\u73fe\u6709\u548c\u65b0\u8208\u591a\u6a21\u614b\u6a21\u578b\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u5f37\u5927\u57fa\u6e96\u6e2c\u8a66\uff0c\u4e26\u5f37\u8abf\u4e86\u9019\u4e9b\u6a21\u578b\u7cbe\u78ba\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\u8207\u4eba\u985e\u4e4b\u9593\u7684\u5de8\u5927\u5dee\u8ddd\u3002", "author": "Xiaoxuan Lei et.al.", "authors": "Xiaoxuan Lei, Lucas Gomez, Hao Yuan Bai, Pouya Bashivan", "id": "2406.14343v1", "paper_url": "http://arxiv.org/abs/2406.14343v1", "repo": "null"}}