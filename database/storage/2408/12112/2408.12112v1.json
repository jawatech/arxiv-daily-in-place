{"2408.12112": {"publish_time": "2024-08-22", "title": "Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards", "paper_summary": "LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.", "paper_summary_zh": "LLM  zunehmend verwendet werden, um Belohnungsfunktionen basierend auf menschlichen Pr\u00e4ferenzen in Reinforcement Learning (RL) zu entwerfen. Wir konzentrieren uns auf LLM-entworfene Belohnungen f\u00fcr Restless Multi-Armed Bandits, ein Framework zur Zuweisung begrenzter Ressourcen unter Agenten. In Anwendungen wie dem \u00f6ffentlichen Gesundheitswesen erm\u00f6glicht dieser Ansatz Basisgesundheitsfachkr\u00e4ften, automatisierte Zuweisungsentscheidungen auf die Bed\u00fcrfnisse der Gemeinschaft zuzuschneiden. In Gegenwart mehrerer Agenten kann die \u00c4nderung der Belohnungsfunktion basierend auf menschlichen Pr\u00e4ferenzen Untergruppen sehr unterschiedlich beeinflussen, was zu komplexen Kompromissen und einem mehrzielgerichteten Ressourcenzuweisungsproblem f\u00fchrt. Wir sind die Ersten, die eine prinzipielle Methode namens Social Choice Language Model f\u00fcr den Umgang mit diesen Kompromissen f\u00fcr LLM-entworfene Belohnungen f\u00fcr Multiagentenplaner im Allgemeinen und unruhige Banditen im Besonderen vorstellen. Der neuartige Teil unseres Modells ist eine transparente und konfigurierbare Auswahlkomponente, die als Schiedsrichter bezeichnet wird und sich au\u00dferhalb des LLM befindet und komplexe Kompromisse \u00fcber eine vom Benutzer ausgew\u00e4hlte soziale Wohlfahrtsfunktion steuert. Unsere Experimente zeigen, dass unser Modell im Vergleich zu rein LLM-basierten Ans\u00e4tzen zuverl\u00e4ssig effektivere, ausgerichtete und ausgewogene Belohnungsfunktionen ausw\u00e4hlt.", "author": "Shresth Verma et.al.", "authors": "Shresth Verma, Niclas Boehmer, Lingkai Kong, Milind Tambe", "id": "2408.12112v1", "paper_url": "http://arxiv.org/abs/2408.12112v1", "repo": "null"}}