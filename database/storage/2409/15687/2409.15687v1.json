{"2409.15687": {"publish_time": "2024-09-24", "title": "A Comprehensive Evaluation of Large Language Models on Mental Illnesses", "paper_summary": "Large language models have shown promise in various domains, including\nhealthcare. In this study, we conduct a comprehensive evaluation of LLMs in the\ncontext of mental health tasks using social media data. We explore the\nzero-shot (ZS) and few-shot (FS) capabilities of various LLMs, including GPT-4,\nLlama 3, Gemini, and others, on tasks such as binary disorder detection,\ndisorder severity evaluation, and psychiatric knowledge assessment. Our\nevaluation involved 33 models testing 9 main prompt templates across the tasks.\nKey findings revealed that models like GPT-4 and Llama 3 exhibited superior\nperformance in binary disorder detection, with accuracies reaching up to 85% on\ncertain datasets. Moreover, prompt engineering played a crucial role in\nenhancing model performance. Notably, the Mixtral 8x22b model showed an\nimprovement of over 20%, while Gemma 7b experienced a similar boost in\nperformance. In the task of disorder severity evaluation, we observed that FS\nlearning significantly improved the model's accuracy, highlighting the\nimportance of contextual examples in complex assessments. Notably, the\nPhi-3-mini model exhibited a substantial increase in performance, with balanced\naccuracy improving by over 6.80% and mean average error dropping by nearly 1.3\nwhen moving from ZS to FS learning. In the psychiatric knowledge task, recent\nmodels generally outperformed older, larger counterparts, with the Llama 3.1\n405b achieving an accuracy of 91.2%. Despite promising results, our analysis\nidentified several challenges, including variability in performance across\ndatasets and the need for careful prompt engineering. Furthermore, the ethical\nguards imposed by many LLM providers hamper the ability to accurately evaluate\ntheir performance, due to tendency to not respond to potentially sensitive\nqueries.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5df2\u5728\u91ab\u7642\u4fdd\u5065\u7b49\u5404\u500b\u9818\u57df\u5c55\u73fe\u6f5b\u529b\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u793e\u7fa4\u5a92\u9ad4\u8cc7\u6599\u5c0d LLM \u5728\u5fc3\u7406\u5065\u5eb7\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u9032\u884c\u5168\u9762\u8a55\u4f30\u3002\u6211\u5011\u63a2\u8a0e\u5404\u7a2e LLM\uff0c\u5305\u62ec GPT-4\u3001Llama 3\u3001Gemini \u7b49\uff0c\u5728\u4e8c\u5143\u969c\u7919\u5075\u6e2c\u3001\u969c\u7919\u56b4\u91cd\u6027\u8a55\u4f30\u548c\u7cbe\u795e\u75be\u75c5\u77e5\u8b58\u8a55\u4f30\u7b49\u4efb\u52d9\u4e0a\u7684\u96f6\u6b21\u5b78\u7fd2 (ZS) \u548c\u5c11\u6b21\u5b78\u7fd2 (FS) \u80fd\u529b\u3002\u6211\u5011\u7684\u8a55\u4f30\u6d89\u53ca 33 \u500b\u6a21\u578b\uff0c\u5728\u5404\u9805\u4efb\u52d9\u4e2d\u6e2c\u8a66 9 \u500b\u4e3b\u8981\u63d0\u793a\u7bc4\u672c\u3002\u4e3b\u8981\u767c\u73fe\u986f\u793a\uff0cGPT-4 \u548c Llama 3 \u7b49\u6a21\u578b\u5728\u4e8c\u5143\u969c\u7919\u5075\u6e2c\u4e2d\u8868\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\uff0c\u5728\u7279\u5b9a\u8cc7\u6599\u96c6\u4e0a\u7684\u6e96\u78ba\u7387\u9ad8\u9054 85%\u3002\u6b64\u5916\uff0c\u63d0\u793a\u5de5\u7a0b\u5728\u63d0\u5347\u6a21\u578b\u6548\u80fd\u65b9\u9762\u626e\u6f14\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMixtral 8x22b \u6a21\u578b\u7684\u9032\u6b65\u5e45\u5ea6\u8d85\u904e 20%\uff0c\u800c Gemma 7b \u7684\u6548\u80fd\u4e5f\u7372\u5f97\u985e\u4f3c\u7684\u63d0\u5347\u3002\u5728\u969c\u7919\u56b4\u91cd\u6027\u8a55\u4f30\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u89c0\u5bdf\u5230 FS \u5b78\u7fd2\u986f\u8457\u63d0\u5347\u6a21\u578b\u7684\u6e96\u78ba\u5ea6\uff0c\u7a81\u986f\u51fa\u80cc\u666f\u7bc4\u4f8b\u5728\u8907\u96dc\u8a55\u4f30\u4e2d\u7684\u91cd\u8981\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cPhi-3-mini \u6a21\u578b\u7684\u6548\u80fd\u5927\u5e45\u63d0\u5347\uff0c\u5f9e ZS \u5b78\u7fd2\u8f49\u63db\u5230 FS \u5b78\u7fd2\u5f8c\uff0c\u5e73\u8861\u6e96\u78ba\u5ea6\u63d0\u5347\u8d85\u904e 6.80%\uff0c\u5e73\u5747\u5e73\u5747\u8aa4\u5dee\u964d\u4f4e\u8fd1 1.3\u3002\u5728\u7cbe\u795e\u75be\u75c5\u77e5\u8b58\u4efb\u52d9\u4e2d\uff0c\u8f03\u65b0\u7684\u6a21\u578b\u901a\u5e38\u512a\u65bc\u8f03\u820a\u3001\u8f03\u5927\u7684\u6a21\u578b\uff0c\u5176\u4e2d Llama 3.1 405b \u9054\u5230 91.2% \u7684\u6e96\u78ba\u5ea6\u3002\u5118\u7ba1\u6709\u4ee4\u4eba\u632f\u596e\u7684\u7d50\u679c\uff0c\u6211\u5011\u7684\u5206\u6790\u767c\u73fe\u4e86\u5e7e\u500b\u6311\u6230\uff0c\u5305\u62ec\u8de8\u8cc7\u6599\u96c6\u7684\u6548\u80fd\u8b8a\u7570\u6027\uff0c\u4ee5\u53ca\u4ed4\u7d30\u63d0\u793a\u5de5\u7a0b\u7684\u9700\u6c42\u3002\u6b64\u5916\uff0c\u8a31\u591a LLM \u63d0\u4f9b\u8005\u65bd\u52a0\u7684\u9053\u5fb7\u5b88\u5247\u963b\u7919\u4e86\u6e96\u78ba\u8a55\u4f30\u5176\u6548\u80fd\u7684\u80fd\u529b\uff0c\u56e0\u70ba\u5b83\u5011\u50be\u5411\u65bc\u4e0d\u56de\u61c9\u6f5b\u5728\u7684\u654f\u611f\u67e5\u8a62\u3002", "author": "Abdelrahman Hanafi et.al.", "authors": "Abdelrahman Hanafi, Mohammed Saad, Noureldin Zahran, Radwa J. Hanafy, Mohammed E. Fouda", "id": "2409.15687v1", "paper_url": "http://arxiv.org/abs/2409.15687v1", "repo": "null"}}