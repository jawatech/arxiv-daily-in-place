{"2410.23254": {"publish_time": "2024-10-30", "title": "Keypoint Abstraction using Large Models for Object-Relative Imitation Learning", "paper_summary": "Generalization to novel object configurations and instances across diverse\ntasks and environments is a critical challenge in robotics. Keypoint-based\nrepresentations have been proven effective as a succinct representation for\ncapturing essential object features, and for establishing a reference frame in\naction prediction, enabling data-efficient learning of robot skills. However,\ntheir manual design nature and reliance on additional human labels limit their\nscalability. In this paper, we propose KALM, a framework that leverages large\npre-trained vision-language models (LMs) to automatically generate\ntask-relevant and cross-instance consistent keypoints. KALM distills robust and\nconsistent keypoints across views and objects by generating proposals using LMs\nand verifies them against a small set of robot demonstration data. Based on the\ngenerated keypoints, we can train keypoint-conditioned policy models that\npredict actions in keypoint-centric frames, enabling robots to generalize\neffectively across varying object poses, camera views, and object instances\nwith similar functional shapes. Our method demonstrates strong performance in\nthe real world, adapting to different tasks and environments from only a\nhandful of demonstrations while requiring no additional labels. Website:\nhttps://kalm-il.github.io/", "paper_summary_zh": "\u5728\u6a5f\u5668\u4eba\u9818\u57df\u4e2d\uff0c\u6cdb\u5316\u81f3\u5404\u7a2e\u4efb\u52d9\u8207\u74b0\u5883\u4e2d\u7684\u65b0\u7a4e\u7269\u9ad4\u914d\u7f6e\u548c\u5be6\u4f8b\u662f\u4e00\u9805\u56b4\u5cfb\u7684\u6311\u6230\u3002\u57fa\u65bc\u95dc\u9375\u9ede\u7684\u8868\u793a\u6cd5\u5df2\u88ab\u8b49\u660e\u662f\u4e00\u7a2e\u7c21\u6f54\u7684\u8868\u793a\u6cd5\uff0c\u7528\u65bc\u64f7\u53d6\u5fc5\u8981\u7684\u7269\u9ad4\u7279\u5fb5\uff0c\u4e26\u5728\u52d5\u4f5c\u9810\u6e2c\u4e2d\u5efa\u7acb\u53c3\u8003\u67b6\u69cb\uff0c\u9032\u800c\u5be6\u73fe\u6a5f\u5668\u4eba\u6280\u80fd\u7684\u8cc7\u6599\u6709\u6548\u5b78\u7fd2\u3002\u7136\u800c\uff0c\u5176\u624b\u52d5\u8a2d\u8a08\u6027\u8cea\u548c\u5c0d\u984d\u5916\u4eba\u5de5\u6a19\u7c64\u7684\u4f9d\u8cf4\u9650\u5236\u4e86\u5176\u53ef\u64f4\u5145\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa KALM\uff0c\u9019\u662f\u4e00\u7a2e\u5229\u7528\u5927\u578b\u9810\u8a13\u7df4\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LM) \u81ea\u52d5\u751f\u6210\u8207\u4efb\u52d9\u76f8\u95dc\u4e14\u8de8\u5be6\u4f8b\u4e00\u81f4\u7684\u95dc\u9375\u9ede\u7684\u67b6\u69cb\u3002KALM \u900f\u904e\u4f7f\u7528 LM \u7522\u751f\u5efa\u8b70\uff0c\u4e26\u6839\u64da\u4e00\u7d44\u5c0f\u578b\u6a5f\u5668\u4eba\u793a\u7bc4\u8cc7\u6599\u9a57\u8b49\u5b83\u5011\uff0c\u5f9e\u800c\u63d0\u53d6\u8de8\u8996\u5716\u548c\u7269\u9ad4\u7684\u7a69\u5065\u4e14\u4e00\u81f4\u7684\u95dc\u9375\u9ede\u3002\u6839\u64da\u7522\u751f\u7684\u95dc\u9375\u9ede\uff0c\u6211\u5011\u53ef\u4ee5\u8a13\u7df4\u95dc\u9375\u9ede\u689d\u4ef6\u7b56\u7565\u6a21\u578b\uff0c\u8a72\u6a21\u578b\u9810\u6e2c\u95dc\u9375\u9ede\u4e2d\u5fc3\u6846\u67b6\u4e2d\u7684\u52d5\u4f5c\uff0c\u8b93\u6a5f\u5668\u4eba\u5728\u4e0d\u540c\u7684\u7269\u9ad4\u59ff\u52e2\u3001\u76f8\u6a5f\u8996\u5716\u548c\u5177\u6709\u985e\u4f3c\u529f\u80fd\u5f62\u72c0\u7684\u7269\u9ad4\u5be6\u4f8b\u4e2d\u6709\u6548\u6cdb\u5316\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u771f\u5be6\u4e16\u754c\u4e2d\u8868\u73fe\u51fa\u5f37\u5927\u7684\u6548\u80fd\uff0c\u53ea\u9700\u5c11\u6578\u793a\u7bc4\u5373\u53ef\u9069\u61c9\u4e0d\u540c\u7684\u4efb\u52d9\u548c\u74b0\u5883\uff0c\u540c\u6642\u4e0d\u9700\u8981\u984d\u5916\u7684\u6a19\u7c64\u3002\u7db2\u7ad9\uff1ahttps://kalm-il.github.io/", "author": "Xiaolin Fang et.al.", "authors": "Xiaolin Fang, Bo-Ruei Huang, Jiayuan Mao, Jasmine Shone, Joshua B. Tenenbaum, Tom\u00e1s Lozano-P\u00e9rez, Leslie Pack Kaelbling", "id": "2410.23254v1", "paper_url": "http://arxiv.org/abs/2410.23254v1", "repo": "null"}}