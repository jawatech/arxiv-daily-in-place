{"2411.05193": {"publish_time": "2024-11-07", "title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning", "paper_summary": "Value-based reinforcement learning (RL) can in principle learn effective\npolicies for a wide range of multi-turn problems, from games to dialogue to\nrobotic control, including via offline RL from static previously collected\ndatasets. However, despite the widespread use of policy gradient methods to\ntrain large language models for single turn tasks (e.g., question answering),\nvalue-based methods for multi-turn RL in an off-policy or offline setting have\nproven particularly challenging to scale to the setting of large language\nmodels. This setting requires effectively leveraging pretraining, scaling to\nlarge architectures with billions of parameters, and training on large\ndatasets, all of which represent major challenges for current value-based RL\nmethods. In this work, we propose a novel offline RL algorithm that addresses\nthese drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)\nproblem where the probabilities of tokens directly translate to Q-values. In\nthis way we obtain an algorithm that smoothly transitions from maximizing the\nlikelihood of the data during pretraining to learning a near-optimal Q-function\nduring finetuning. Our algorithm has strong theoretical foundations, enjoying\nperformance bounds similar to state-of-the-art Q-learning methods, while in\npractice utilizing an objective that closely resembles SFT. Because of this,\nour approach can enjoy the full benefits of the pretraining of language models,\nwithout the need to reinitialize any weights before RL finetuning, and without\nthe need to initialize new heads for predicting values or advantages.\nEmpirically, we evaluate our method on both pretrained LLMs and VLMs, on a\nvariety of tasks including both natural language dialogue and robotic\nmanipulation and navigation from images.", "paper_summary_zh": "\u57fa\u65bc\u50f9\u503c\u7684\u5f37\u5316\u5b78\u7fd2 (RL) \u7406\u8ad6\u4e0a\u53ef\u4ee5\u5b78\u7fd2\u5404\u7a2e\u591a\u8f2a\u554f\u984c\u7684\u6709\u6548\u653f\u7b56\uff0c\u5f9e\u904a\u6232\u5230\u5c0d\u8a71\u518d\u5230\u6a5f\u5668\u4eba\u63a7\u5236\uff0c\u5305\u62ec\u5f9e\u975c\u614b\u5148\u524d\u6536\u96c6\u7684\u8cc7\u6599\u96c6\u9032\u884c\u96e2\u7dda RL\u3002\u7136\u800c\uff0c\u5118\u7ba1\u5ee3\u6cdb\u4f7f\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4f86\u8a13\u7df4\u55ae\u8f2a\u4efb\u52d9\u7684\u5927\u8a9e\u8a00\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u554f\u984c\u89e3\u7b54\uff09\uff0c\u4f46\u591a\u8f2a RL \u5728\u975e\u7b56\u7565\u6216\u96e2\u7dda\u8a2d\u5b9a\u4e2d\u7684\u57fa\u65bc\u50f9\u503c\u7684\u65b9\u6cd5\u5df2\u88ab\u8b49\u660e\u7279\u5225\u96e3\u4ee5\u64f4\u5c55\u5230\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u8a2d\u5b9a\u3002\u6b64\u8a2d\u5b9a\u9700\u8981\u6709\u6548\u5730\u5229\u7528\u9810\u8a13\u7df4\uff0c\u64f4\u5c55\u5230\u5177\u6709\u6578\u5341\u5104\u500b\u53c3\u6578\u7684\u5927\u578b\u67b6\u69cb\uff0c\u4e26\u5728\u5927\u578b\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\uff0c\u6240\u6709\u9019\u4e9b\u90fd\u5c0d\u7576\u524d\u7684\u57fa\u65bc\u50f9\u503c\u7684 RL \u65b9\u6cd5\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u96e2\u7dda RL \u6f14\u7b97\u6cd5\u4f86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9ede\uff0c\u5c07 Q \u5b78\u7fd2\u8f49\u63db\u70ba\u4fee\u6539\u904e\u7684\u76e3\u7763\u5fae\u8abf (SFT) \u554f\u984c\uff0c\u5176\u4e2d\u7b26\u865f\u7684\u6a5f\u7387\u76f4\u63a5\u8f49\u63db\u70ba Q \u503c\u3002\u9019\u6a23\uff0c\u6211\u5011\u7372\u5f97\u4e86\u4e00\u7a2e\u6f14\u7b97\u6cd5\uff0c\u8a72\u6f14\u7b97\u6cd5\u53ef\u4ee5\u5728\u9810\u8a13\u7df4\u671f\u9593\u6700\u5927\u5316\u8cc7\u6599\u7684\u53ef\u80fd\u6027\u8207\u5fae\u8abf\u671f\u9593\u5b78\u7fd2\u8fd1\u4e4e\u6700\u4f73\u7684 Q \u51fd\u6578\u4e4b\u9593\u9806\u5229\u8f49\u63db\u3002\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u5177\u6709\u5f37\u5927\u7684\u7406\u8ad6\u57fa\u790e\uff0c\u4eab\u6709\u8207\u6700\u5148\u9032\u7684 Q \u5b78\u7fd2\u65b9\u6cd5\u985e\u4f3c\u7684\u6548\u80fd\u754c\u9650\uff0c\u540c\u6642\u5728\u5be6\u52d9\u4e0a\u4f7f\u7528\u8207 SFT \u975e\u5e38\u76f8\u4f3c\u7684\u76ee\u6a19\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\u505a\u6cd5\u53ef\u4ee5\u5145\u5206\u4eab\u53d7\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u7684\u5168\u90e8\u597d\u8655\uff0c\u800c\u7121\u9700\u5728 RL \u5fae\u8abf\u4e4b\u524d\u91cd\u65b0\u521d\u59cb\u5316\u4efb\u4f55\u6b0a\u91cd\uff0c\u4e5f\u7121\u9700\u521d\u59cb\u5316\u65b0\u7684\u982d\u90e8\u4f86\u9810\u6e2c\u503c\u6216\u512a\u52e2\u3002\u6839\u64da\u7d93\u9a57\uff0c\u6211\u5011\u5728\u9810\u8a13\u7df4\u7684 LLM \u548c VLM \u4e0a\u8a55\u4f30\u4e86\u6211\u5011\u7684\u65b9\u6cd5\uff0c\u5728\u5404\u7a2e\u4efb\u52d9\u4e0a\uff0c\u5305\u62ec\u81ea\u7136\u8a9e\u8a00\u5c0d\u8a71\u548c\u6a5f\u5668\u4eba\u5f9e\u5f71\u50cf\u9032\u884c\u64cd\u4f5c\u548c\u5c0e\u822a\u3002", "author": "Joey Hong et.al.", "authors": "Joey Hong, Anca Dragan, Sergey Levine", "id": "2411.05193v1", "paper_url": "http://arxiv.org/abs/2411.05193v1", "repo": "null"}}