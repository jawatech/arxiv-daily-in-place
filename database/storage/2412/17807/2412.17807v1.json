{"2412.17807": {"publish_time": "2024-12-23", "title": "Cross-View Referring Multi-Object Tracking", "paper_summary": "Referring Multi-Object Tracking (RMOT) is an important topic in the current\ntracking field. Its task form is to guide the tracker to track objects that\nmatch the language description. Current research mainly focuses on referring\nmulti-object tracking under single-view, which refers to a view sequence or\nmultiple unrelated view sequences. However, in the single-view, some\nappearances of objects are easily invisible, resulting in incorrect matching of\nobjects with the language description. In this work, we propose a new task,\ncalled Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the\ncross-view to obtain the appearances of objects from multiple views, avoiding\nthe problem of the invisible appearances of objects in RMOT task. CRMOT is a\nmore challenging task of accurately tracking the objects that match the\nlanguage description and maintaining the identity consistency of objects in\neach cross-view. To advance CRMOT task, we construct a cross-view referring\nmulti-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named\nCRTrack. Specifically, it provides 13 different scenes and 221 language\ndescriptions. Furthermore, we propose an end-to-end cross-view referring\nmulti-object tracking method, named CRTracker. Extensive experiments on the\nCRTrack benchmark verify the effectiveness of our method. The dataset and code\nare available at https://github.com/chen-si-jia/CRMOT.", "paper_summary_zh": "\u591a\u76ee\u6a19\u8ffd\u8e64 (RMOT) \u662f\u76ee\u524d\u8ffd\u8e64\u9818\u57df\u4e2d\u4e00\u500b\u91cd\u8981\u7684\u8ab2\u984c\u3002\u5176\u4efb\u52d9\u5f62\u5f0f\u662f\u5f15\u5c0e\u8ffd\u8e64\u5668\u8ffd\u8e64\u7b26\u5408\u8a9e\u8a00\u63cf\u8ff0\u7684\u76ee\u6a19\u3002\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u55ae\u8996\u89d2\u4e0b\u7684\u591a\u76ee\u6a19\u8ffd\u8e64\uff0c\u6307\u7684\u662f\u4e00\u500b\u8996\u89d2\u5e8f\u5217\u6216\u591a\u500b\u7121\u95dc\u7684\u8996\u89d2\u5e8f\u5217\u3002\u7136\u800c\uff0c\u5728\u55ae\u8996\u89d2\u4e2d\uff0c\u76ee\u6a19\u7684\u4e00\u4e9b\u5916\u89c0\u5f88\u5bb9\u6613\u770b\u4e0d\u898b\uff0c\u5c0e\u81f4\u76ee\u6a19\u8207\u8a9e\u8a00\u63cf\u8ff0\u4e0d\u6b63\u78ba\u5730\u5339\u914d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u65b0\u4efb\u52d9\uff0c\u7a31\u70ba\u8de8\u8996\u89d2\u591a\u76ee\u6a19\u8ffd\u8e64 (CRMOT)\u3002\u5b83\u5f15\u5165\u4e86\u8de8\u8996\u89d2\uff0c\u4ee5\u5f9e\u591a\u500b\u8996\u89d2\u7372\u5f97\u76ee\u6a19\u7684\u5916\u89c0\uff0c\u907f\u514d\u4e86 RMOT \u4efb\u52d9\u4e2d\u76ee\u6a19\u5916\u89c0\u4e0d\u53ef\u898b\u7684\u554f\u984c\u3002CRMOT \u662f\u4e00\u9805\u66f4\u5177\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u6e96\u78ba\u8ffd\u8e64\u7b26\u5408\u8a9e\u8a00\u63cf\u8ff0\u7684\u76ee\u6a19\uff0c\u4e26\u7dad\u6301\u6bcf\u500b\u8de8\u8996\u89d2\u4e2d\u76ee\u6a19\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u70ba\u4e86\u63a8\u9032 CRMOT \u4efb\u52d9\uff0c\u6211\u5011\u57fa\u65bc CAMPUS \u548c DIVOTrack \u8cc7\u6599\u96c6\u69cb\u5efa\u4e86\u4e00\u500b\u8de8\u8996\u89d2\u591a\u76ee\u6a19\u8ffd\u8e64\u57fa\u6e96\uff0c\u7a31\u70ba CRTrack\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5b83\u63d0\u4f9b\u4e86 13 \u500b\u4e0d\u540c\u7684\u5834\u666f\u548c 221 \u500b\u8a9e\u8a00\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7aef\u5230\u7aef\u7684\u8de8\u8996\u89d2\u591a\u76ee\u6a19\u8ffd\u8e64\u65b9\u6cd5\uff0c\u7a31\u70ba CRTracker\u3002\u5728 CRTrack \u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u8cc7\u6599\u96c6\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/chen-si-jia/CRMOT \u4e2d\u53d6\u5f97\u3002", "author": "Sijia Chen et.al.", "authors": "Sijia Chen, En Yu, Wenbing Tao", "id": "2412.17807v1", "paper_url": "http://arxiv.org/abs/2412.17807v1", "repo": "https://github.com/chen-si-jia/crmot"}}