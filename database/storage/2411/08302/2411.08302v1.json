{"2411.08302": {"publish_time": "2024-11-13", "title": "R3HF: Reward Redistribution for Enhancing Reinforcement Learning from Human Feedback", "paper_summary": "Reinforcement learning from human feedback (RLHF) provides a paradigm for\naligning large language models (LLMs) with human preferences. This involves the\ninitial training of a reward model based on pairwise human feedback. The reward\nmodel is subsequently utilized in reinforcement learning to assess the scores\nof each generated sentence as a whole, further guiding the optimization of\nLLMs. However, current approaches have a significant shortcoming: \\emph{They\nallocate a single, sparse, and delayed reward to an entire sequence of output}.\nThis may overlook some significant individual contributions of each token\ntowards the desired outcome. To overcome this limitation, our paper proposes a\nnovel reward redistribution method called R3HF, which facilitates a more\nfine-grained, token-level reward allocation. Specifically, our method treats\nthe reward prediction task of the reward model as a regression problem. As a\nresult, the redistributed rewards are computed by evaluating the specific\ncontribution of each token to the reward model's output. This detailed approach\nimproves the model's understanding of language nuances, leading to more precise\nenhancements in its performance. Our method is crafted to integrate seamlessly\nwith most current techniques while incurring minimal computational costs.\nThrough comprehensive experiments across diverse datasets and tasks, we have\nverified the effectiveness and superiority of our approach.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF) \u63d0\u4f9b\u4e86\u4e00\u500b\u5178\u7bc4\uff0c\u53ef\u4ee5\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u504f\u597d\u76f8\u7d50\u5408\u3002\u9019\u5305\u62ec\u6839\u64da\u4eba\u985e\u6210\u5c0d\u56de\u994b\uff0c\u5c0d\u734e\u52f5\u6a21\u578b\u9032\u884c\u521d\u59cb\u8a13\u7df4\u3002\u96a8\u5f8c\u5728\u5f37\u5316\u5b78\u7fd2\u4e2d\u5229\u7528\u734e\u52f5\u6a21\u578b\u4f86\u8a55\u4f30\u6bcf\u500b\u751f\u6210\u53e5\u5b50\u7684\u6574\u9ad4\u5206\u6578\uff0c\u9032\u4e00\u6b65\u6307\u5c0e LLM \u7684\u6700\u4f73\u5316\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u505a\u6cd5\u6709\u4e00\u500b\u91cd\u5927\u7684\u7f3a\u9ede\uff1a\\emph{\u5b83\u5011\u5c07\u55ae\u4e00\u3001\u7a00\u758f\u4e14\u5ef6\u9072\u7684\u734e\u52f5\u5206\u914d\u7d66\u6574\u500b\u8f38\u51fa\u5e8f\u5217}\u3002\u9019\u53ef\u80fd\u6703\u5ffd\u7565\u6bcf\u500b\u7b26\u865f\u5c0d\u6240\u9700\u7d50\u679c\u7684\u4e00\u4e9b\u91cd\u8981\u500b\u5225\u8ca2\u737b\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u7684\u8ad6\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba R3HF \u7684\u65b0\u734e\u52f5\u91cd\u65b0\u5206\u914d\u65b9\u6cd5\uff0c\u5b83\u6709\u52a9\u65bc\u66f4\u7d30\u7dfb\u7684\u7b26\u865f\u7d1a\u5225\u734e\u52f5\u5206\u914d\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c07\u734e\u52f5\u6a21\u578b\u7684\u734e\u52f5\u9810\u6e2c\u4efb\u52d9\u8996\u70ba\u56de\u6b78\u554f\u984c\u3002\u56e0\u6b64\uff0c\u91cd\u65b0\u5206\u914d\u7684\u734e\u52f5\u662f\u901a\u904e\u8a55\u4f30\u6bcf\u500b\u7b26\u865f\u5c0d\u734e\u52f5\u6a21\u578b\u8f38\u51fa\u7684\u5177\u9ad4\u8ca2\u737b\u4f86\u8a08\u7b97\u7684\u3002\u9019\u7a2e\u8a73\u7d30\u7684\u65b9\u6cd5\u6539\u9032\u4e86\u6a21\u578b\u5c0d\u8a9e\u8a00\u7d30\u5fae\u5dee\u7684\u7406\u89e3\uff0c\u5f9e\u800c\u66f4\u7cbe\u78ba\u5730\u63d0\u5347\u4e86\u5b83\u7684\u6027\u80fd\u3002\u6211\u5011\u7684\u6a21\u578b\u88ab\u8a2d\u8a08\u6210\u8207\u5927\u591a\u6578\u7576\u524d\u6280\u8853\u7121\u7e2b\u6574\u5408\uff0c\u540c\u6642\u7522\u751f\u6700\u5c0f\u7684\u8a08\u7b97\u6210\u672c\u3002\u901a\u904e\u5c0d\u4e0d\u540c\u8cc7\u6599\u96c6\u548c\u4efb\u52d9\u9032\u884c\u5168\u9762\u7684\u5be6\u9a57\uff0c\u6211\u5011\u9a57\u8b49\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u512a\u8d8a\u6027\u3002", "author": "Jiahui Li et.al.", "authors": "Jiahui Li, Tai-wei Chang, Fengda Zhang, Kun Kuang, Long Chen", "id": "2411.08302v1", "paper_url": "http://arxiv.org/abs/2411.08302v1", "repo": "null"}}