{"2410.08211": {"publish_time": "2024-10-10", "title": "LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts", "paper_summary": "Large-scale vision-language pre-trained (VLP) models (e.g., CLIP) are\nrenowned for their versatility, as they can be applied to diverse applications\nin a zero-shot setup. However, when these models are used in specific domains,\ntheir performance often falls short due to domain gaps or the\nunder-representation of these domains in the training data. While fine-tuning\nVLP models on custom datasets with human-annotated labels can address this\nissue, annotating even a small-scale dataset (e.g., 100k samples) can be an\nexpensive endeavor, often requiring expert annotators if the task is complex.\nTo address these challenges, we propose LatteCLIP, an unsupervised method for\nfine-tuning CLIP models on classification with known class names in custom\ndomains, without relying on human annotations. Our method leverages Large\nMultimodal Models (LMMs) to generate expressive textual descriptions for both\nindividual images and groups of images. These provide additional contextual\ninformation to guide the fine-tuning process in the custom domains. Since\nLMM-generated descriptions are prone to hallucination or missing details, we\nintroduce a novel strategy to distill only the useful information and stabilize\nthe training. Specifically, we learn rich per-class prototype representations\nfrom noisy generated texts and dual pseudo-labels. Our experiments on 10\ndomain-specific datasets show that LatteCLIP outperforms pre-trained zero-shot\nmethods by an average improvement of +4.74 points in top-1 accuracy and other\nstate-of-the-art unsupervised methods by +3.45 points.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4 (VLP) \u6a21\u578b (\u4f8b\u5982 CLIP) \u4ee5\u5176\u591a\u529f\u80fd\u6027\u800c\u805e\u540d\uff0c\u56e0\u70ba\u5b83\u5011\u53ef\u4ee5\u7528\u65bc\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u4e2d\uff0c\u800c\u7121\u9700\u9032\u884c\u4efb\u4f55\u8a2d\u5b9a\u3002\u7136\u800c\uff0c\u7576\u9019\u4e9b\u6a21\u578b\u7528\u65bc\u7279\u5b9a\u9818\u57df\u6642\uff0c\u5b83\u5011\u7684\u6548\u80fd\u901a\u5e38\u6703\u56e0\u9818\u57df\u5dee\u8ddd\u6216\u8a13\u7df4\u8cc7\u6599\u4e2d\u9019\u4e9b\u9818\u57df\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u800c\u4e0b\u964d\u3002\u96d6\u7136\u5728\u5177\u6709\u4eba\u5de5\u6a19\u8a18\u6a19\u7c64\u7684\u5ba2\u88fd\u5316\u8cc7\u6599\u96c6\u4e0a\u5fae\u8abf VLP \u6a21\u578b\u53ef\u4ee5\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u4f46\u5373\u4f7f\u662f\u6a19\u8a18\u5c0f\u898f\u6a21\u8cc7\u6599\u96c6 (\u4f8b\u5982 100k \u500b\u6a23\u672c) \u4e5f\u53ef\u80fd\u662f\u4e00\u9805\u6602\u8cb4\u7684\u5de5\u4f5c\uff0c\u5982\u679c\u4efb\u52d9\u5f88\u8907\u96dc\uff0c\u901a\u5e38\u9700\u8981\u5c08\u5bb6\u6a19\u8a18\u54e1\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LatteCLIP\uff0c\u9019\u662f\u4e00\u7a2e\u7121\u76e3\u7763\u65b9\u6cd5\uff0c\u7528\u65bc\u5728\u5ba2\u88fd\u5316\u9818\u57df\u4e2d\u5c0d CLIP \u6a21\u578b\u9032\u884c\u5fae\u8abf\uff0c\u4ee5\u5c0d\u5df2\u77e5\u7684\u985e\u5225\u540d\u7a31\u9032\u884c\u5206\u985e\uff0c\u800c\u4e0d\u9700\u8981\u4f9d\u8cf4\u4eba\u5de5\u6a19\u8a18\u3002\u6211\u5011\u7684\u6a21\u578b\u5229\u7528\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u70ba\u500b\u5225\u5f71\u50cf\u548c\u5f71\u50cf\u7fa4\u7d44\u7522\u751f\u5177\u8868\u73fe\u529b\u7684\u6587\u5b57\u63cf\u8ff0\u3002\u9019\u4e9b\u63cf\u8ff0\u63d0\u4f9b\u4e86\u984d\u5916\u7684\u8108\u7d61\u8cc7\u8a0a\uff0c\u4ee5\u6307\u5c0e\u5ba2\u88fd\u5316\u9818\u57df\u4e2d\u7684\u5fae\u8abf\u904e\u7a0b\u3002\u7531\u65bc LMM \u751f\u6210\u7684\u63cf\u8ff0\u5bb9\u6613\u51fa\u73fe\u5e7b\u89ba\u6216\u907a\u6f0f\u7d30\u7bc0\uff0c\u56e0\u6b64\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7b56\u7565\uff0c\u50c5\u63d0\u53d6\u6709\u7528\u7684\u8cc7\u8a0a\u4e26\u7a69\u5b9a\u8a13\u7df4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f9e\u96dc\u8a0a\u7522\u751f\u7684\u6587\u5b57\u548c\u96d9\u91cd\u507d\u6a19\u7c64\u4e2d\u5b78\u7fd2\u8c50\u5bcc\u7684\u6bcf\u500b\u985e\u5225\u539f\u578b\u8868\u793a\u3002\u6211\u5011\u5728 10 \u500b\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\uff0cLatteCLIP \u5728\u524d 1 \u540d\u6e96\u78ba\u7387\u4e0a\u6bd4\u9810\u5148\u8a13\u7df4\u7684\u96f6\u6b21\u5b78\u7fd2\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad8\u4e86 +4.74 \u500b\u767e\u5206\u9ede\uff0c\u6bd4\u5176\u4ed6\u6700\u5148\u9032\u7684\u7121\u76e3\u7763\u65b9\u6cd5\u63d0\u9ad8\u4e86 +3.45 \u500b\u767e\u5206\u9ede\u3002", "author": "Anh-Quan Cao et.al.", "authors": "Anh-Quan Cao, Maximilian Jaritz, Matthieu Guillaumin, Raoul de Charette, Loris Bazzani", "id": "2410.08211v1", "paper_url": "http://arxiv.org/abs/2410.08211v1", "repo": "null"}}