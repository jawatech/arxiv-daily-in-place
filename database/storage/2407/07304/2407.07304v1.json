{"2407.07304": {"publish_time": "2024-07-10", "title": "Inference Performance Optimization for Large Language Models on CPUs", "paper_summary": "Large language models (LLMs) have shown exceptional performance and vast\npotential across diverse tasks. However, the deployment of LLMs with high\nperformance in low-resource environments has garnered significant attention in\nthe industry. When GPU hardware resources are limited, we can explore\nalternative options on CPUs. To mitigate the financial burden and alleviate\nconstraints imposed by hardware resources, optimizing inference performance is\nnecessary. In this paper, we introduce an easily deployable inference\nperformance optimization solution aimed at accelerating LLMs on CPUs. In this\nsolution, we implement an effective way to reduce the KV cache size while\nensuring precision. We propose a distributed inference optimization approach\nand implement it based on oneAPI Collective Communications Library.\nFurthermore, we propose optimization approaches for LLMs on CPU, and conduct\ntailored optimizations for the most commonly used models. The code is\nopen-sourced at https://github.com/intel/xFasterTransformer.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u6548\u80fd\u548c\u5ee3\u6cdb\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u5728\u4f4e\u8cc7\u6e90\u74b0\u5883\u4e2d\u90e8\u7f72\u9ad8\u6548\u80fd\u7684 LLM \u5df2\u5f15\u8d77\u696d\u754c\u7684\u6975\u5927\u95dc\u6ce8\u3002\u7576 GPU \u786c\u9ad4\u8cc7\u6e90\u6709\u9650\u6642\uff0c\u6211\u5011\u53ef\u4ee5\u5728 CPU \u4e0a\u63a2\u7d22\u5176\u4ed6\u9078\u9805\u3002\u70ba\u4e86\u6e1b\u8f15\u8ca1\u52d9\u8ca0\u64d4\u4e26\u7de9\u89e3\u786c\u9ad4\u8cc7\u6e90\u5e36\u4f86\u7684\u9650\u5236\uff0c\u6700\u4f73\u5316\u63a8\u7406\u6548\u80fd\u662f\u5fc5\u8981\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u6613\u65bc\u90e8\u7f72\u7684\u63a8\u7406\u6548\u80fd\u6700\u4f73\u5316\u89e3\u6c7a\u65b9\u6848\uff0c\u65e8\u5728\u52a0\u901f CPU \u4e0a\u7684 LLM\u3002\u5728\u6b64\u89e3\u6c7a\u65b9\u6848\u4e2d\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\u4f86\u6e1b\u5c11 KV \u5feb\u53d6\u5927\u5c0f\uff0c\u540c\u6642\u78ba\u4fdd\u7cbe\u78ba\u5ea6\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5206\u6563\u5f0f\u63a8\u7406\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u4e26\u6839\u64da oneAPI Collective Communications Library \u4f86\u5be6\u4f5c\u5b83\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u91dd\u5c0d CPU \u4e0a LLM \u7684\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u4e26\u91dd\u5c0d\u6700\u5e38\u7528\u7684\u6a21\u578b\u9032\u884c\u5ba2\u88fd\u5316\u6700\u4f73\u5316\u3002\u7a0b\u5f0f\u78bc\u5df2\u5728 https://github.com/intel/xFasterTransformer \u958b\u6e90\u3002", "author": "Pujiang He et.al.", "authors": "Pujiang He, Shan Zhou, Wenhuan Huang, Changqing Li, Duyi Wang, Bin Guo, Chen Meng, Sheng Gui, Weifei Yu, Yi Xie", "id": "2407.07304v1", "paper_url": "http://arxiv.org/abs/2407.07304v1", "repo": "https://github.com/intel/xfastertransformer"}}