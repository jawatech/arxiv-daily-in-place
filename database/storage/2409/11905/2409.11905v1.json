{"2409.11905": {"publish_time": "2024-09-18", "title": "AlignBot: Aligning VLM-powered Customized Task Planning with User Reminders Through Fine-Tuning for Household Robots", "paper_summary": "This paper presents AlignBot, a novel framework designed to optimize\nVLM-powered customized task planning for household robots by effectively\naligning with user reminders. In domestic settings, aligning task planning with\nuser reminders poses significant challenges due to the limited quantity,\ndiversity, and multimodal nature of the reminders. To address these challenges,\nAlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for\nGPT-4o. This adapter model internalizes diverse forms of user reminders-such as\npersonalized preferences, corrective guidance, and contextual assistance-into\nstructured instruction-formatted cues that prompt GPT-4o in generating\ncustomized task plans. Additionally, AlignBot integrates a dynamic retrieval\nmechanism that selects task-relevant historical successes as prompts for\nGPT-4o, further enhancing task planning accuracy. To validate the effectiveness\nof AlignBot, experiments are conducted in real-world household environments,\nwhich are constructed within the laboratory to replicate typical household\nsettings. A multimodal dataset with over 1,500 entries derived from volunteer\nreminders is used for training and evaluation. The results demonstrate that\nAlignBot significantly improves customized task planning, outperforming\nexisting LLM- and VLM-powered planners by interpreting and aligning with user\nreminders, achieving 86.8% success rate compared to the vanilla GPT-4o baseline\nat 21.6%, reflecting a 65% improvement and over four times greater\neffectiveness. Supplementary materials are available at:\nhttps://yding25.com/AlignBot/", "paper_summary_zh": "\u672c\u6587\u63d0\u51fa AlignBot\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u65e8\u5728\u900f\u904e\u6709\u6548\u5730\u8207\u4f7f\u7528\u8005\u63d0\u9192\u76f8\u7b26\uff0c\u4f86\u6700\u4f73\u5316\u5927\u578b\u8a9e\u8a00\u6a21\u578b (VLM) \u9a45\u52d5\u7684\u5ba2\u88fd\u5316\u4efb\u52d9\u898f\u5283\uff0c\u4ee5\u4f9b\u5bb6\u7528\u6a5f\u5668\u4eba\u4f7f\u7528\u3002\u5728\u5bb6\u5ead\u74b0\u5883\u4e2d\uff0c\u5c07\u4efb\u52d9\u898f\u5283\u8207\u4f7f\u7528\u8005\u63d0\u9192\u76f8\u7b26\u6703\u9020\u6210\u91cd\u5927\u6311\u6230\uff0c\u539f\u56e0\u5728\u65bc\u63d0\u9192\u7684\u6578\u91cf\u6709\u9650\u3001\u591a\u6a23\u4e14\u5177\u6709\u591a\u6a21\u614b\u7684\u6027\u8cea\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0cAlignBot \u63a1\u7528\u5fae\u8abf\u904e\u7684 LLaVA-7B \u6a21\u578b\uff0c\u4f5c\u70ba GPT-4o \u7684\u9069\u914d\u5668\u3002\u9019\u500b\u9069\u914d\u5668\u6a21\u578b\u5c07\u4f7f\u7528\u8005\u63d0\u9192\u7684\u5404\u7a2e\u5f62\u5f0f\uff08\u4f8b\u5982\u500b\u4eba\u5316\u504f\u597d\u3001\u4fee\u6b63\u6307\u5c0e\u548c\u60c5\u5883\u5354\u52a9\uff09\u5167\u5316\u70ba\u7d50\u69cb\u5316\u7684\u6307\u4ee4\u683c\u5f0f\u63d0\u793a\uff0c\u4ee5\u63d0\u793a GPT-4o \u7522\u751f\u5ba2\u88fd\u5316\u7684\u4efb\u52d9\u8a08\u756b\u3002\u6b64\u5916\uff0cAlignBot \u6574\u5408\u4e86\u4e00\u500b\u52d5\u614b\u6aa2\u7d22\u6a5f\u5236\uff0c\u5b83\u6703\u9078\u64c7\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u6b77\u53f2\u6210\u529f\u7d93\u9a57\u4f5c\u70ba GPT-4o \u7684\u63d0\u793a\uff0c\u9032\u4e00\u6b65\u63d0\u5347\u4efb\u52d9\u898f\u5283\u7684\u6e96\u78ba\u5ea6\u3002\u70ba\u4e86\u9a57\u8b49 AlignBot \u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5728\u73fe\u5be6\u4e16\u754c\u7684\u5bb6\u5ead\u74b0\u5883\u4e2d\u9032\u884c\u5be6\u9a57\uff0c\u9019\u4e9b\u74b0\u5883\u662f\u5728\u5be6\u9a57\u5ba4\u4e2d\u5efa\u69cb\u7684\uff0c\u4ee5\u8907\u88fd\u5178\u578b\u7684\u5bb6\u5ead\u74b0\u5883\u3002\u6211\u5011\u4f7f\u7528\u4e00\u500b\u591a\u6a21\u614b\u7684\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea\u5fd7\u5de5\u63d0\u9192\u7684 1,500 \u591a\u500b\u689d\u76ee\uff0c\u7528\u65bc\u8a13\u7df4\u548c\u8a55\u4f30\u3002\u7d50\u679c\u986f\u793a\uff0cAlignBot \u5927\u5e45\u6539\u5584\u4e86\u5ba2\u88fd\u5316\u7684\u4efb\u52d9\u898f\u5283\uff0c\u900f\u904e\u8a6e\u91cb\u548c\u8207\u4f7f\u7528\u8005\u63d0\u9192\u76f8\u7b26\uff0c\u5176\u6548\u80fd\u512a\u65bc\u73fe\u6709\u7684 LLM \u548c VLM \u9a45\u52d5\u7684\u898f\u5283\u5668\uff0c\u6210\u529f\u7387\u9054\u5230 86.8%\uff0c\u800c\u9999\u8349 GPT-4o \u57fa\u6e96\u5247\u70ba 21.6%\uff0c\u986f\u793a\u6539\u5584\u4e86 65%\uff0c\u4e14\u6709\u6548\u6027\u63d0\u9ad8\u4e86\u56db\u500d\u4ee5\u4e0a\u3002\u88dc\u5145\u6750\u6599\u53ef\u65bc\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1ahttps://yding25.com/AlignBot/", "author": "Zhaxizhuoma et.al.", "authors": "Zhaxizhuoma, Pengan Chen, Ziniu Wu, Jiawei Sun, Dong Wang, Peng Zhou, Nieqing Cao, Yan Ding, Bin Zhao, Xuelong Li", "id": "2409.11905v1", "paper_url": "http://arxiv.org/abs/2409.11905v1", "repo": "null"}}