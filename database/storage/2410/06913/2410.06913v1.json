{"2410.06913": {"publish_time": "2024-10-09", "title": "Utilize the Flow before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal-Aware Instruction Tuning", "paper_summary": "Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)\nto refuse to answer unknown questions. By modifying responses of unknown\nquestions in the training data to refusal responses such as \"I don't know\",\nRAIT enhances the reliability of LLMs and reduces their hallucination.\nGenerally, RAIT modifies training samples based on the correctness of the\ninitial LLM's response. However, this crude approach can cause LLMs to\nexcessively refuse answering questions they could have correctly answered, the\nproblem we call over-refusal. In this paper, we explore two primary causes of\nover-refusal: Static conflict emerges when the RAIT data is constructed solely\non correctness criteria, causing similar samples in the LLM's feature space to\nbe assigned different labels (original vs. modified \"I don't know\"). Dynamic\nconflict occurs due to the changes of LLM's knowledge state during fine-tuning,\nwhich transforms previous unknown questions into knowns, while the training\ndata, which is constructed based on the initial LLM, remains unchanged. These\nconflicts cause the trained LLM to misclassify known questions as unknown,\nresulting in over-refusal. To address this issue, we introduce Certainty\nRepresented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).\nCRaFT centers on two main contributions: First, we additionally incorporate\nresponse certainty to selectively filter and modify data, reducing static\nconflicts. Second, we implement preliminary rehearsal training to characterize\nchanges in the LLM's knowledge state, which helps mitigate dynamic conflicts\nduring the fine-tuning process. We conducted extensive experiments on\nopen-ended question answering and multiple-choice question task. Experiment\nresults show that CRaFT can improve LLM's overall performance during the RAIT\nprocess. Source code and training data will be released at Github.", "paper_summary_zh": "\u62d2\u7d55\u611f\u77e5\u6307\u4ee4\u8abf\u6821 (RAIT) \u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u62d2\u7d55\u56de\u7b54\u672a\u77e5\u554f\u984c\u3002\u900f\u904e\u4fee\u6539\u8a13\u7df4\u8cc7\u6599\u4e2d\u672a\u77e5\u554f\u984c\u7684\u56de\u61c9\u70ba\u62d2\u7d55\u56de\u61c9\uff0c\u4f8b\u5982\u300c\u6211\u4e0d\u77e5\u9053\u300d\uff0cRAIT \u63d0\u5347\u4e86 LLM \u7684\u53ef\u9760\u6027\u4e26\u6e1b\u5c11\u4e86\u5b83\u5011\u7684\u5e7b\u89ba\u3002\u4e00\u822c\u4f86\u8aaa\uff0cRAIT \u6703\u6839\u64da LLM \u521d\u59cb\u56de\u61c9\u7684\u6b63\u78ba\u6027\u4fee\u6539\u8a13\u7df4\u6a23\u672c\u3002\u7136\u800c\uff0c\u9019\u7a2e\u7c97\u7cd9\u7684\u65b9\u6cd5\u6703\u5c0e\u81f4 LLM \u904e\u5ea6\u62d2\u7d55\u56de\u7b54\u5b83\u5011\u539f\u672c\u53ef\u4ee5\u6b63\u78ba\u56de\u7b54\u7684\u554f\u984c\uff0c\u6211\u5011\u7a31\u4e4b\u70ba\u904e\u5ea6\u62d2\u7d55\u7684\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u904e\u5ea6\u62d2\u7d55\u7684\u5169\u500b\u4e3b\u8981\u539f\u56e0\uff1a\u975c\u614b\u885d\u7a81\u51fa\u73fe\u5728 RAIT \u8cc7\u6599\u50c5\u6839\u64da\u6b63\u78ba\u6027\u6a19\u6e96\u5efa\u69cb\u6642\uff0c\u5c0e\u81f4 LLM \u7279\u5fb5\u7a7a\u9593\u4e2d\u985e\u4f3c\u7684\u6a23\u672c\u88ab\u6307\u5b9a\u70ba\u4e0d\u540c\u7684\u6a19\u7c64\uff08\u539f\u59cb\u8207\u4fee\u6539\u5f8c\u7684\u300c\u6211\u4e0d\u77e5\u9053\u300d\uff09\u3002\u52d5\u614b\u885d\u7a81\u5247\u56e0\u70ba\u5fae\u8abf\u671f\u9593 LLM \u77e5\u8b58\u72c0\u614b\u7684\u6539\u8b8a\u800c\u767c\u751f\uff0c\u9019\u6703\u5c07\u5148\u524d\u7684\u672a\u77e5\u554f\u984c\u8f49\u8b8a\u70ba\u5df2\u77e5\uff0c\u800c\u6839\u64da\u521d\u59cb LLM \u5efa\u69cb\u7684\u8a13\u7df4\u8cc7\u6599\u5247\u4fdd\u6301\u4e0d\u8b8a\u3002\u9019\u4e9b\u885d\u7a81\u5c0e\u81f4\u8a13\u7df4\u5f8c\u7684 LLM \u5c07\u5df2\u77e5\u554f\u984c\u8aa4\u5206\u985e\u70ba\u672a\u77e5\uff0c\u9032\u800c\u5c0e\u81f4\u904e\u5ea6\u62d2\u7d55\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u78ba\u5b9a\u6027\u8868\u793a\u77e5\u8b58\u6d41\uff0c\u7528\u65bc\u62d2\u7d55\u611f\u77e5\u6307\u4ee4\u5efa\u69cb (CRaFT)\u3002CRaFT \u5c08\u6ce8\u65bc\u5169\u500b\u4e3b\u8981\u8ca2\u737b\uff1a\u9996\u5148\uff0c\u6211\u5011\u984d\u5916\u7d0d\u5165\u56de\u61c9\u78ba\u5b9a\u6027\u4f86\u9078\u64c7\u6027\u5730\u904e\u6ffe\u548c\u4fee\u6539\u8cc7\u6599\uff0c\u6e1b\u5c11\u975c\u614b\u885d\u7a81\u3002\u5176\u6b21\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u521d\u6b65\u6392\u7df4\u8a13\u7df4\uff0c\u4ee5\u63cf\u8ff0 LLM \u77e5\u8b58\u72c0\u614b\u7684\u6539\u8b8a\uff0c\u9019\u6709\u52a9\u65bc\u5728\u5fae\u8abf\u904e\u7a0b\u4e2d\u6e1b\u8f15\u52d5\u614b\u885d\u7a81\u3002\u6211\u5011\u5728\u958b\u653e\u5f0f\u554f\u984c\u89e3\u7b54\u548c\u591a\u91cd\u9078\u64c7\u984c\u4efb\u52d9\u4e0a\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cCRaFT \u53ef\u4ee5\u6539\u5584 LLM \u5728 RAIT \u904e\u7a0b\u4e2d\u7684\u6574\u9ad4\u8868\u73fe\u3002\u539f\u59cb\u78bc\u548c\u8a13\u7df4\u8cc7\u6599\u5c07\u5728 Github \u4e0a\u767c\u5e03\u3002", "author": "Runchuan Zhu et.al.", "authors": "Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, Conghui He", "id": "2410.06913v1", "paper_url": "http://arxiv.org/abs/2410.06913v1", "repo": "null"}}