{"2412.13602": {"publish_time": "2024-12-18", "title": "Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games", "paper_summary": "Large Language Models (LLMs) are increasingly deployed in real-world\napplications that demand complex reasoning. To track progress, robust\nbenchmarks are required to evaluate their capabilities beyond superficial\npattern recognition. However, current LLM reasoning benchmarks often face\nchallenges such as insufficient interpretability, performance saturation or\ndata contamination. To address these challenges, we introduce GAMEBoT, a gaming\narena designed for rigorous and transparent assessment of LLM reasoning\ncapabilities. GAMEBoT decomposes complex reasoning in games into predefined\nmodular subproblems. This decomposition allows us to design a suite of\nChain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in\naddressing these subproblems before action selection. Furthermore, we develop a\nsuite of rule-based algorithms to generate ground truth for these subproblems,\nenabling rigorous validation of the LLMs' intermediate reasoning steps. This\napproach facilitates evaluation of both the quality of final actions and the\naccuracy of the underlying reasoning process. GAMEBoT also naturally alleviates\nthe risk of data contamination through dynamic games and head-to-head LLM\ncompetitions. We benchmark 17 prominent LLMs across eight games, encompassing\nvarious strategic abilities and game characteristics. Our results suggest that\nGAMEBoT presents a significant challenge, even when LLMs are provided with\ndetailed CoT prompts. Project page: \\url{https://visual-ai.github.io/gamebot}", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)  zunehmend in realen Anwendungen eingesetzt, die komplexe \u00dcberlegungen erfordern. Um den Fortschritt zu verfolgen, sind robuste Benchmarks erforderlich, um ihre F\u00e4higkeiten \u00fcber die oberfl\u00e4chliche Mustererkennung hinaus zu bewerten. Allerdings stehen aktuelle LLM-Begr\u00fcndungs-Benchmarks oft vor Herausforderungen wie unzureichender Interpretierbarkeit, Leistungss\u00e4ttigung oder Datenkontamination. Um diese Herausforderungen anzugehen, stellen wir GAMEBoT vor, eine Spielarena, die f\u00fcr eine strenge und transparente Bewertung der LLM-Begr\u00fcndungsf\u00e4higkeiten entwickelt wurde. GAMEBoT zerlegt komplexe \u00dcberlegungen in Spielen in vordefinierte modulare Teilprobleme. Diese Zerlegung erm\u00f6glicht es uns, eine Reihe von Chain-of-Thought (CoT)-Eingabeaufforderungen zu entwerfen, die Dom\u00e4nenwissen nutzen, um LLMs bei der L\u00f6sung dieser Teilprobleme vor der Aktionsauswahl zu unterst\u00fctzen. Dar\u00fcber hinaus entwickeln wir eine Reihe regelbasierter Algorithmen, um eine Grundwahrheit f\u00fcr diese Teilprobleme zu generieren, die eine strenge Validierung der Zwischenbegr\u00fcndungsschritte der LLMs erm\u00f6glicht. Dieser Ansatz erleichtert die Bewertung sowohl der Qualit\u00e4t der endg\u00fcltigen Aktionen als auch der Genauigkeit des zugrunde liegenden Begr\u00fcndungsprozesses. GAMEBoT verringert au\u00dferdem auf nat\u00fcrliche Weise das Risiko einer Datenkontamination durch dynamische Spiele und Kopf-an-Kopf-LLM-Wettbewerbe. Wir vergleichen 17 herausragende LLMs in acht Spielen, die verschiedene strategische F\u00e4higkeiten und Spieleigenschaften umfassen. Unsere Ergebnisse deuten darauf hin, dass GAMEBoT eine erhebliche Herausforderung darstellt, selbst wenn LLMs mit detaillierten CoT-Eingabeaufforderungen versehen werden. Projektseite: \\url{https://visual-ai.github.io/gamebot}", "author": "Wenye Lin et.al.", "authors": "Wenye Lin, Jonathan Roberts, Yunhan Yang, Samuel Albanie, Zongqing Lu, Kai Han", "id": "2412.13602v1", "paper_url": "http://arxiv.org/abs/2412.13602v1", "repo": "null"}}