{"2501.08102": {"publish_time": "2025-01-14", "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media", "paper_summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\ngeneration, yet their emotional consistency and semantic coherence in social\nmedia contexts remain insufficiently understood. This study investigates how\nLLMs handle emotional content and maintain semantic relationships through\ncontinuation and response tasks using two open-source models: Gemma and Llama.\nBy analyzing climate change discussions from Twitter and Reddit, we examine\nemotional transitions, intensity patterns, and semantic similarity between\nhuman-authored and LLM-generated content. Our findings reveal that while both\nmodels maintain high semantic coherence, they exhibit distinct emotional\npatterns: Gemma shows a tendency toward negative emotion amplification,\nparticularly anger, while maintaining certain positive emotions like optimism.\nLlama demonstrates superior emotional preservation across a broader spectrum of\naffects. Both models systematically generate responses with attenuated\nemotional intensity compared to human-authored content and show a bias toward\npositive emotions in response tasks. Additionally, both models maintain strong\nsemantic similarity with original texts, though performance varies between\ncontinuation and response tasks. These findings provide insights into LLMs'\nemotional and semantic processing capabilities, with implications for their\ndeployment in social media contexts and human-AI interaction design.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4f46\u5728\u793e\u4ea4\u5a92\u9ad4\u60c5\u5883\u4e2d\u7684\u60c5\u7dd2\u4e00\u81f4\u6027\u548c\u8a9e\u7fa9\u9023\u8cab\u6027\u4ecd\u6709\u5f85\u9032\u4e00\u6b65\u4e86\u89e3\u3002\u672c\u7814\u7a76\u63a2\u8a0e LLM \u5982\u4f55\u900f\u904e\u4f7f\u7528\u5169\u500b\u958b\u6e90\u6a21\u578b\uff1aGemma \u548c Llama\uff0c\u5728\u5ef6\u7e8c\u548c\u56de\u61c9\u4efb\u52d9\u4e2d\u8655\u7406\u60c5\u7dd2\u5167\u5bb9\u4e26\u7dad\u6301\u8a9e\u7fa9\u95dc\u4fc2\u3002\u900f\u904e\u5206\u6790\u4f86\u81ea Twitter \u548c Reddit \u7684\u6c23\u5019\u8b8a\u9077\u8a0e\u8ad6\uff0c\u6211\u5011\u6aa2\u8996\u4e86\u4eba\u985e\u64b0\u5beb\u548c LLM \u751f\u6210\u7684\u5167\u5bb9\u4e4b\u9593\u7684\u60c5\u7dd2\u8f49\u63db\u3001\u5f37\u5ea6\u6a21\u5f0f\u548c\u8a9e\u7fa9\u76f8\u4f3c\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u5118\u7ba1\u9019\u5169\u500b\u6a21\u578b\u90fd\u7dad\u6301\u4e86\u9ad8\u5ea6\u7684\u8a9e\u7fa9\u9023\u8cab\u6027\uff0c\u4f46\u5b83\u5011\u8868\u73fe\u51fa\u622a\u7136\u4e0d\u540c\u7684\u60c5\u7dd2\u6a21\u5f0f\uff1aGemma \u50be\u5411\u65bc\u653e\u5927\u8ca0\u9762\u60c5\u7dd2\uff0c\u5c24\u5176\u662f\u61a4\u6012\uff0c\u540c\u6642\u7dad\u6301\u67d0\u4e9b\u7a4d\u6975\u60c5\u7dd2\uff0c\u4f8b\u5982\u6a02\u89c0\u3002Llama \u5247\u5c55\u73fe\u51fa\u5728\u66f4\u5ee3\u6cdb\u7684\u60c5\u7dd2\u7bc4\u570d\u5167\u7dad\u6301\u60c5\u7dd2\u7684\u512a\u8d8a\u6027\u3002\u9019\u5169\u500b\u6a21\u578b\u90fd\u7cfb\u7d71\u6027\u5730\u7522\u751f\u8207\u4eba\u985e\u64b0\u5beb\u5167\u5bb9\u76f8\u6bd4\uff0c\u60c5\u7dd2\u5f37\u5ea6\u8f03\u5f31\u7684\u56de\u61c9\uff0c\u4e26\u5728\u56de\u61c9\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u504f\u597d\u6b63\u5411\u60c5\u7dd2\u3002\u6b64\u5916\uff0c\u9019\u5169\u500b\u6a21\u578b\u90fd\u8207\u539f\u59cb\u6587\u5b57\u7dad\u6301\u5f37\u70c8\u7684\u8a9e\u7fa9\u76f8\u4f3c\u6027\uff0c\u5118\u7ba1\u5728\u5ef6\u7e8c\u548c\u56de\u61c9\u4efb\u52d9\u4e4b\u9593\u7684\u8868\u73fe\u6709\u6240\u4e0d\u540c\u3002\u9019\u4e9b\u7814\u7a76\u7d50\u679c\u63d0\u4f9b\u4e86\u5c0d LLM \u60c5\u7dd2\u548c\u8a9e\u7fa9\u8655\u7406\u80fd\u529b\u7684\u898b\u89e3\uff0c\u4e26\u5c0d\u5176\u5728\u793e\u4ea4\u5a92\u9ad4\u60c5\u5883\u4e2d\u7684\u90e8\u7f72\u548c\u4eba\u6a5f\u4e92\u52d5\u8a2d\u8a08\u7522\u751f\u5f71\u97ff\u3002", "author": "Wenlu Fan et.al.", "authors": "Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu", "id": "2501.08102v1", "paper_url": "http://arxiv.org/abs/2501.08102v1", "repo": "null"}}