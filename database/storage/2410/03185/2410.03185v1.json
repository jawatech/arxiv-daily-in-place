{"2410.03185": {"publish_time": "2024-10-04", "title": "EXAQ: Exponent Aware Quantization For LLMs Acceleration", "paper_summary": "Quantization has established itself as the primary approach for decreasing\nthe computational and storage expenses associated with Large Language Models\n(LLMs) inference. The majority of current research emphasizes quantizing\nweights and activations to enable low-bit general-matrix-multiply (GEMM)\noperations, with the remaining non-linear operations executed at higher\nprecision. In our study, we discovered that following the application of these\ntechniques, the primary bottleneck in LLMs inference lies in the softmax layer.\nThe softmax operation comprises three phases: exponent calculation,\naccumulation, and normalization, Our work focuses on optimizing the first two\nphases. We propose an analytical approach to determine the optimal clipping\nvalue for the input to the softmax function, enabling sub-4-bit quantization\nfor LLMs inference. This method accelerates the calculations of both $e^x$ and\n$\\sum(e^x)$ with minimal to no accuracy degradation. For example, in\nLLaMA1-30B, we achieve baseline performance with 2-bit quantization on the\nwell-known \"Physical Interaction: Question Answering\" (PIQA) dataset\nevaluation. This ultra-low bit quantization allows, for the first time, an\nacceleration of approximately 4x in the accumulation phase. The combination of\naccelerating both $e^x$ and $\\sum(e^x)$ results in a 36.9% acceleration in the\nsoftmax operation.", "paper_summary_zh": "\u91cf\u5316\u5df2\u6210\u70ba\u6e1b\u5c11\u8207\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u63a8\u8ad6\u76f8\u95dc\u7684\u904b\u7b97\u548c\u5132\u5b58\u958b\u92b7\u7684\u4e3b\u8981\u65b9\u6cd5\u3002\u76ee\u524d\u5927\u591a\u6578\u7684\u7814\u7a76\u90fd\u5f37\u8abf\u91cf\u5316\u6b0a\u91cd\u548c\u555f\u7528\uff0c\u4ee5\u5be6\u73fe\u4f4e\u4f4d\u5143\u901a\u7528\u77e9\u9663\u4e58\u6cd5 (GEMM) \u904b\u7b97\uff0c\u800c\u5176\u9918\u975e\u7dda\u6027\u904b\u7b97\u5247\u4ee5\u8f03\u9ad8\u7cbe\u5ea6\u57f7\u884c\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5728\u61c9\u7528\u9019\u4e9b\u6280\u8853\u5f8c\uff0cLLM \u63a8\u8ad6\u4e2d\u7684\u4e3b\u8981\u74f6\u9838\u5728\u65bc softmax \u5c64\u3002softmax \u904b\u7b97\u5305\u542b\u4e09\u500b\u968e\u6bb5\uff1a\u6307\u6578\u8a08\u7b97\u3001\u7d2f\u7a4d\u548c\u6b63\u898f\u5316\uff0c\u6211\u5011\u7684\u7814\u7a76\u91cd\u9ede\u5728\u65bc\u6700\u4f73\u5316\u524d\u5169\u500b\u968e\u6bb5\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5206\u6790\u65b9\u6cd5\u4f86\u78ba\u5b9a softmax \u51fd\u6578\u8f38\u5165\u7684\u6700\u4f73\u88c1\u526a\u503c\uff0c\u5f9e\u800c\u5be6\u73fe LLM \u63a8\u8ad6\u7684 4 \u4f4d\u5143\u4ee5\u4e0b\u91cf\u5316\u3002\u6b64\u65b9\u6cd5\u53ef\u52a0\u901f\u8a08\u7b97 $e^x$ \u548c $\\sum(e^x)$\uff0c\u540c\u6642\u5c07\u7cbe\u78ba\u5ea6\u964d\u4f4e\u5230\u6700\u5c0f\u751a\u81f3\u4e0d\u964d\u4f4e\u3002\u4f8b\u5982\uff0c\u5728 LLaMA1-30B \u4e2d\uff0c\u6211\u5011\u5728\u8457\u540d\u7684\u300c\u7269\u7406\u4e92\u52d5\uff1a\u554f\u7b54\u300d(PIQA) \u8cc7\u6599\u96c6\u8a55\u4f30\u4e2d\uff0c\u4ee5 2 \u4f4d\u5143\u91cf\u5316\u9054\u5230\u4e86\u57fa\u6e96\u6548\u80fd\u3002\u9019\u7a2e\u8d85\u4f4e\u4f4d\u5143\u91cf\u5316\u9996\u6b21\u5141\u8a31\u5728\u7d2f\u7a4d\u968e\u6bb5\u52a0\u901f\u7d04 4 \u500d\u3002\u52a0\u901f $e^x$ \u548c $\\sum(e^x)$ \u7684\u7d50\u5408\uff0c\u4f7f softmax \u904b\u7b97\u52a0\u901f\u4e86 36.9%\u3002", "author": "Moran Shkolnik et.al.", "authors": "Moran Shkolnik, Maxim Fishman, Brian Chmiel, Hilla Ben-Yaacov, Ron Banner, Kfir Yehuda Levy", "id": "2410.03185v1", "paper_url": "http://arxiv.org/abs/2410.03185v1", "repo": "https://github.com/anonymous1252022/exaq"}}