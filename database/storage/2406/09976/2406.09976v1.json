{"2406.09976": {"publish_time": "2024-06-14", "title": "Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary Model", "paper_summary": "Reinforcement learning has demonstrated impressive performance in various\nchallenging problems such as robotics, board games, and classical arcade games.\nHowever, its real-world applications can be hindered by the absence of\nrobustness and safety in the learned policies. More specifically, an RL agent\nthat trains in a certain Markov decision process (MDP) often struggles to\nperform well in nearly identical MDPs. To address this issue, we employ the\nframework of Robust MDPs (RMDPs) in a model-based setting and introduce a novel\nlearned transition model. Our method specifically incorporates an auxiliary\npessimistic model, updated adversarially, to estimate the worst-case MDP within\na Kullback-Leibler uncertainty set. In comparison to several existing works,\nour work does not impose any additional conditions on the training environment,\nsuch as the need for a parametric simulator. To test the effectiveness of the\nproposed pessimistic model in enhancing policy robustness, we integrate it into\na practical RL algorithm, called Robust Model-Based Policy Optimization\n(RMBPO). Our experimental results indicate a notable improvement in policy\nrobustness on high-dimensional MuJoCo control tasks, with the auxiliary model\nenhancing the performance of the learned policy in distorted MDPs. We further\nexplore the learned deviation between the proposed auxiliary world model and\nthe nominal model, to examine how pessimism is achieved. By learning a\npessimistic world model and demonstrating its role in improving policy\nrobustness, our research contributes towards making (model-based) RL more\nrobust.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2\u5df2\u5728\u5404\u7a2e\u5177\u6709\u6311\u6230\u6027\u7684\u554f\u984c\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8868\u73fe\uff0c\u4f8b\u5982\u6a5f\u5668\u4eba\u6280\u8853\u3001\u68cb\u76e4\u904a\u6232\u548c\u7d93\u5178\u8857\u6a5f\u904a\u6232\u3002\u7136\u800c\uff0c\u5176\u771f\u5be6\u4e16\u754c\u7684\u61c9\u7528\u53ef\u80fd\u6703\u53d7\u5230\u5b78\u7fd2\u653f\u7b56\u4e2d\u7f3a\u4e4f\u7a69\u5065\u6027\u548c\u5b89\u5168\u6027\u6240\u963b\u7919\u3002\u66f4\u5177\u9ad4\u5730\u8aaa\uff0c\u5728\u7279\u5b9a\u99ac\u53ef\u592b\u6c7a\u7b56\u904e\u7a0b\u4e2d (MDP) \u9032\u884c\u8a13\u7df4\u7684 RL \u4ee3\u7406\u901a\u5e38\u96e3\u4ee5\u5728\u5e7e\u4e4e\u76f8\u540c\u7684 MDP \u4e2d\u8868\u73fe\u826f\u597d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5728\u57fa\u65bc\u6a21\u578b\u7684\u8a2d\u7f6e\u4e2d\u63a1\u7528\u7a69\u5065 MDP (RMDP) \u7684\u6846\u67b6\uff0c\u4e26\u5f15\u5165\u4e00\u500b\u65b0\u7a4e\u7684\u5b78\u7fd2\u8f49\u63db\u6a21\u578b\u3002\u6211\u5011\u7684\u6a21\u578b\u7279\u5225\u7d50\u5408\u4e86\u4e00\u500b\u8f14\u52a9\u60b2\u89c0\u6a21\u578b\uff0c\u4ee5\u5c0d\u6297\u7684\u65b9\u5f0f\u9032\u884c\u66f4\u65b0\uff0c\u4ee5\u4f30\u8a08 Kullback-Leibler \u4e0d\u78ba\u5b9a\u6027\u96c6\u5167\u7684\u6700\u60aa\u60c5\u6cc1 MDP\u3002\u8207\u73fe\u6709\u7684\u5e7e\u9805\u5de5\u4f5c\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u4e0d\u6703\u5c0d\u8a13\u7df4\u74b0\u5883\u65bd\u52a0\u4efb\u4f55\u984d\u5916\u7684\u689d\u4ef6\uff0c\u4f8b\u5982\u5c0d\u53c3\u6578\u5316\u6a21\u64ec\u5668\u7684\u9700\u6c42\u3002\u70ba\u4e86\u6e2c\u8a66\u6240\u63d0\u51fa\u7684\u60b2\u89c0\u6a21\u578b\u5728\u589e\u5f37\u7b56\u7565\u7a69\u5065\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5c07\u5176\u6574\u5408\u5230\u4e00\u500b\u5be6\u7528\u7684 RL \u6f14\u7b97\u6cd5\u4e2d\uff0c\u7a31\u70ba\u7a69\u5065\u57fa\u65bc\u6a21\u578b\u7684\u7b56\u7565\u6700\u4f73\u5316 (RMBPO)\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5728\u9ad8\u7dad MuJoCo \u63a7\u5236\u4efb\u52d9\u4e2d\uff0c\u7b56\u7565\u7a69\u5065\u6027\u6709\u4e86\u986f\u8457\u7684\u63d0\u5347\uff0c\u8f14\u52a9\u6a21\u578b\u589e\u5f37\u4e86\u5b78\u7fd2\u7b56\u7565\u5728\u626d\u66f2 MDP \u4e2d\u7684\u8868\u73fe\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a2\u8a0e\u4e86\u6240\u63d0\u51fa\u7684\u8f14\u52a9\u4e16\u754c\u6a21\u578b\u548c\u6a19\u7a31\u6a21\u578b\u4e4b\u9593\u7684\u5b78\u7fd2\u504f\u5dee\uff0c\u4ee5\u6aa2\u8996\u5982\u4f55\u5be6\u73fe\u60b2\u89c0\u4e3b\u7fa9\u3002\u900f\u904e\u5b78\u7fd2\u60b2\u89c0\u7684\u4e16\u754c\u6a21\u578b\u4e26\u5c55\u793a\u5176\u5728\u6539\u5584\u7b56\u7565\u7a69\u5065\u6027\u4e2d\u7684\u4f5c\u7528\uff0c\u6211\u5011\u7684\u7814\u7a76\u6709\u52a9\u65bc\u8b93\uff08\u57fa\u65bc\u6a21\u578b\u7684\uff09RL \u66f4\u52a0\u7a69\u5065\u3002", "author": "Siemen Herremans et.al.", "authors": "Siemen Herremans, Ali Anwar, Siegfried Mercelis", "id": "2406.09976v1", "paper_url": "http://arxiv.org/abs/2406.09976v1", "repo": "https://github.com/rmbpo-eval/rmbpo-eval"}}