{"2405.10160": {"publish_time": "2024-05-16", "title": "PIR: Remote Sensing Image-Text Retrieval with Prior Instruction Representation Learning", "paper_summary": "Remote sensing image-text retrieval constitutes a foundational aspect of\nremote sensing interpretation tasks, facilitating the alignment of vision and\nlanguage representations. This paper introduces a prior instruction\nrepresentation (PIR) learning paradigm that draws on prior knowledge to\ninstruct adaptive learning of vision and text representations. Based on PIR, a\ndomain-adapted remote sensing image-text retrieval framework PIR-ITR is\ndesigned to address semantic noise issues in vision-language understanding\ntasks. However, with massive additional data for pre-training the\nvision-language foundation model, remote sensing image-text retrieval is\nfurther developed into an open-domain retrieval task. Continuing with the\nabove, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote\nsensing image-text retrieval, to address semantic noise in remote sensing\nvision-language representations and further improve open-domain retrieval\nperformance. In vision representation, Vision Instruction Representation (VIR)\nbased on Spatial-PAE utilizes the prior-guided knowledge of the remote sensing\nscene recognition by building a belief matrix to select key features for\nreducing the impact of semantic noise. In text representation, Language Cycle\nAttention (LCA) based on Temporal-PAE uses the previous time step to cyclically\nactivate the current time step to enhance text representation capability. A\ncluster-wise Affiliation Loss (AL) is proposed to constrain the inter-classes\nand to reduce the semantic confusion zones in the common subspace.\nComprehensive experiments demonstrate that PIR could enhance vision and text\nrepresentations and outperform the state-of-the-art methods of closed-domain\nand open-domain retrieval on two benchmark datasets, RSICD and RSITMD.", "paper_summary_zh": "\u9059\u611f\u5f71\u50cf\u6587\u5b57\u6aa2\u7d22\u69cb\u6210\u9059\u611f\u89e3\u8b6f\u4efb\u52d9\u7684\u57fa\u790e\u5c64\u9762\uff0c\u4fc3\u9032\u8996\u89ba\u8207\u8a9e\u8a00\u8868\u5fb5\u7684\u4e00\u81f4\u6027\u3002\u672c\u6587\u4ecb\u7d39\u4e00\u7a2e\u4e8b\u524d\u6307\u4ee4\u8868\u5fb5 (PIR) \u5b78\u7fd2\u7bc4\u4f8b\uff0c\u5229\u7528\u4e8b\u524d\u77e5\u8b58\u6307\u5c0e\u8996\u89ba\u8207\u6587\u5b57\u8868\u5fb5\u7684\u81ea\u9069\u61c9\u5b78\u7fd2\u3002\u6839\u64da PIR\uff0c\u8a2d\u8a08\u4e86\u4e00\u500b\u9818\u57df\u9069\u61c9\u7684\u9059\u611f\u5f71\u50cf\u6587\u5b57\u6aa2\u7d22\u67b6\u69cb PIR-ITR\uff0c\u7528\u65bc\u89e3\u6c7a\u8996\u89ba\u8a9e\u8a00\u7406\u89e3\u4efb\u52d9\u4e2d\u7684\u8a9e\u7fa9\u96dc\u8a0a\u554f\u984c\u3002\u7136\u800c\uff0c\u96a8\u8457\u9810\u8a13\u7df4\u8996\u89ba\u8a9e\u8a00\u57fa\u790e\u6a21\u578b\u7684\u5927\u91cf\u984d\u5916\u8cc7\u6599\uff0c\u9059\u611f\u5f71\u50cf\u6587\u5b57\u6aa2\u7d22\u9032\u4e00\u6b65\u767c\u5c55\u6210\u958b\u653e\u9818\u57df\u6aa2\u7d22\u4efb\u52d9\u3002\u5ef6\u7e8c\u4e0a\u8ff0\uff0c\u6211\u5011\u63d0\u51fa PIR-CLIP\uff0c\u4e00\u500b\u57fa\u65bc\u9818\u57df\u7279\u5b9a CLIP \u7684\u9059\u611f\u5f71\u50cf\u6587\u5b57\u6aa2\u7d22\u67b6\u69cb\uff0c\u7528\u65bc\u89e3\u6c7a\u9059\u611f\u8996\u89ba\u8a9e\u8a00\u8868\u5fb5\u4e2d\u7684\u8a9e\u7fa9\u96dc\u8a0a\uff0c\u4e26\u9032\u4e00\u6b65\u63d0\u5347\u958b\u653e\u9818\u57df\u6aa2\u7d22\u6548\u80fd\u3002\u5728\u8996\u89ba\u8868\u5fb5\u4e2d\uff0c\u57fa\u65bc\u7a7a\u9593 PAE \u7684\u8996\u89ba\u6307\u4ee4\u8868\u5fb5 (VIR) \u900f\u904e\u5efa\u7acb\u4e00\u500b\u4fe1\u5ff5\u77e9\u9663\uff0c\u5229\u7528\u9059\u611f\u5834\u666f\u8fa8\u8b58\u7684\u4e8b\u524d\u5f15\u5c0e\u77e5\u8b58\uff0c\u4f86\u9078\u64c7\u95dc\u9375\u7279\u5fb5\uff0c\u4ee5\u964d\u4f4e\u8a9e\u7fa9\u96dc\u8a0a\u7684\u5f71\u97ff\u3002\u5728\u6587\u5b57\u8868\u5fb5\u4e2d\uff0c\u57fa\u65bc\u6642\u9593 PAE \u7684\u8a9e\u8a00\u5faa\u74b0\u6ce8\u610f\u529b (LCA) \u4f7f\u7528\u524d\u4e00\u500b\u6642\u9593\u6b65\u9a5f\uff0c\u4ee5\u5faa\u74b0\u65b9\u5f0f\u555f\u52d5\u7576\u524d\u6642\u9593\u6b65\u9a5f\uff0c\u4ee5\u589e\u5f37\u6587\u5b57\u8868\u5fb5\u80fd\u529b\u3002\u63d0\u51fa\u4e00\u500b\u7fa4\u96c6\u5f0f\u95dc\u806f\u640d\u5931 (AL)\uff0c\u7528\u65bc\u7d04\u675f\u985e\u9593\uff0c\u4e26\u6e1b\u5c11\u5171\u540c\u5b50\u7a7a\u9593\u4e2d\u7684\u8a9e\u7fa9\u6df7\u6dc6\u5340\u57df\u3002\u5168\u9762\u7684\u5be6\u9a57\u8b49\u660e\uff0cPIR \u53ef\u4ee5\u589e\u5f37\u8996\u89ba\u548c\u6587\u5b57\u8868\u5fb5\uff0c\u4e26\u5728\u5169\u500b\u57fa\u6e96\u8cc7\u6599\u96c6 RSICD \u548c RSITMD \u4e0a\u512a\u65bc\u5c01\u9589\u9818\u57df\u548c\u958b\u653e\u9818\u57df\u6aa2\u7d22\u7684\u6700\u65b0\u65b9\u6cd5\u3002", "author": "Jiancheng Pan et.al.", "authors": "Jiancheng Pan, Muyuan Ma, Qing Ma, Cong Bai, Shengyong Chen", "id": "2405.10160v1", "paper_url": "http://arxiv.org/abs/2405.10160v1", "repo": "https://github.com/jaychempan/pir-clip"}}