{"2501.18101": {"publish_time": "2025-01-30", "title": "Diverse Preference Optimization", "paper_summary": "Post-training of language models, either through reinforcement learning,\npreference optimization or supervised finetuning, tends to sharpen the output\nprobability distribution and reduce the diversity of generated responses. This\nis particularly a problem for creative generative tasks where varied responses\nare desired. %This impacts the ability to generate high quality synthetic data\nwhich is becoming a vital component of model training. In this work we\nintroduce Diverse Preference Optimization (DivPO), an online optimization\nmethod which learns to generate much more diverse responses than standard\npipelines, while maintaining the quality of the generations. In DivPO,\npreference pairs are selected by first considering a pool of responses, and a\nmeasure of diversity among them, and selecting chosen examples as being more\nrare but high quality, while rejected examples are more common, but low\nquality. DivPO results in generating 45.6% more diverse persona attributes, and\nan 74.6% increase in story diversity, while maintaining similar win rates as\nstandard baselines.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u7684\u5f8c\u7e8c\u8a13\u7df4\uff0c\u4e0d\u8ad6\u662f\u900f\u904e\u5f37\u5316\u5b78\u7fd2\u3001\u504f\u597d\u6700\u4f73\u5316\u6216\u76e3\u7763\u5fae\u8abf\uff0c\u5f80\u5f80\u6703\u92b3\u5316\u8f38\u51fa\u6a5f\u7387\u5206\u4f48\uff0c\u4e26\u964d\u4f4e\u7522\u51fa\u56de\u61c9\u7684\u591a\u6a23\u6027\u3002\u9019\u7279\u5225\u662f\u5275\u610f\u751f\u6210\u4efb\u52d9\u7684\u554f\u984c\uff0c\u56e0\u70ba\u9019\u4e9b\u4efb\u52d9\u9700\u8981\u591a\u6a23\u5316\u7684\u56de\u61c9\u3002\u9019\u6703\u5f71\u97ff\u7522\u751f\u9ad8\u54c1\u8cea\u5408\u6210\u8cc7\u6599\u7684\u80fd\u529b\uff0c\u800c\u5408\u6210\u8cc7\u6599\u6b63\u6210\u70ba\u6a21\u578b\u8a13\u7df4\u7684\u91cd\u8981\u7d44\u6210\u90e8\u5206\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u591a\u6a23\u5316\u504f\u597d\u6700\u4f73\u5316 (DivPO)\uff0c\u9019\u662f\u4e00\u7a2e\u7dda\u4e0a\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u5b83\u6703\u5b78\u7fd2\u7522\u751f\u6bd4\u6a19\u6e96\u7ba1\u7dda\u66f4\u70ba\u591a\u6a23\u5316\u7684\u56de\u61c9\uff0c\u540c\u6642\u7dad\u6301\u7522\u51fa\u7684\u54c1\u8cea\u3002\u5728 DivPO \u4e2d\uff0c\u504f\u597d\u5c0d\u6703\u5148\u5f9e\u56de\u61c9\u6c60\u4e2d\u9078\u51fa\uff0c\u4e26\u5f9e\u4e2d\u8861\u91cf\u591a\u6a23\u6027\uff0c\u7136\u5f8c\u9078\u64c7\u8f03\u7f55\u898b\u4f46\u54c1\u8cea\u8f03\u9ad8\u7684\u7bc4\u4f8b\uff0c\u800c\u88ab\u62d2\u7d55\u7684\u7bc4\u4f8b\u5247\u8f03\u5e38\u898b\uff0c\u4f46\u54c1\u8cea\u8f03\u4f4e\u3002DivPO \u7522\u751f\u4e86\u591a\u6a23\u5316\u89d2\u8272\u5c6c\u6027\u589e\u52a0\u4e86 45.6%\uff0c\u6545\u4e8b\u591a\u6a23\u6027\u589e\u52a0\u4e86 74.6%\uff0c\u540c\u6642\u7dad\u6301\u8207\u6a19\u6e96\u57fa\u6e96\u7dda\u985e\u4f3c\u7684\u7372\u52dd\u7387\u3002", "author": "Jack Lanchantin et.al.", "authors": "Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, Ilia Kulikov", "id": "2501.18101v1", "paper_url": "http://arxiv.org/abs/2501.18101v1", "repo": "null"}}