{"2412.08946": {"publish_time": "2024-12-12", "title": "MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning", "paper_summary": "Recently, LoRA has emerged as a crucial technique for fine-tuning large\npre-trained models, yet its performance in multi-task learning scenarios often\nfalls short. In contrast, the MoE architecture presents a natural solution to\nthis issue. However, it introduces challenges such as mutual interference of\ndata across multiple domains and knowledge forgetting of various tasks.\nAdditionally, MoE significantly increases the number of parameters, posing a\ncomputational cost challenge. Therefore, in this paper, we propose MoSLD, a\nmixture-of-shared-LoRAs model with a dropout strategy. MoSLD addresses these\nchallenges by sharing the upper projection matrix in LoRA among different\nexperts, encouraging the model to learn general knowledge across tasks, while\nstill allowing the lower projection matrix to focus on the unique features of\neach task. The application of dropout alleviates the imbalanced update of\nparameter matrix and mitigates parameter overfitting in LoRA. Extensive\nexperiments demonstrate that our model exhibits excellent performance in both\nsingle-task and multi-task scenarios, with robust out-of-domain generalization\ncapabilities.", "paper_summary_zh": "\u6700\u8fd1\uff0cLoRA \u5df2\u6210\u4e3a\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u8868\u73b0\u5f80\u5f80\u4e0d\u8db3\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cMoE \u67b6\u6784\u4e3a\u8fd9\u4e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u7136\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5b83\u5e26\u6765\u4e86\u8bf8\u5982\u8de8\u591a\u4e2a\u57df\u7684\u6570\u636e\u76f8\u4e92\u5e72\u6270\u548c\u5404\u79cd\u4efb\u52a1\u77e5\u8bc6\u9057\u5fd8\u7b49\u6311\u6218\u3002\u6b64\u5916\uff0cMoE \u5927\u5927\u589e\u52a0\u4e86\u53c2\u6570\u7684\u6570\u91cf\uff0c\u5e26\u6765\u4e86\u8ba1\u7b97\u6210\u672c\u6311\u6218\u3002\u56e0\u6b64\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MoSLD\uff0c\u4e00\u79cd\u5177\u6709 dropout \u7b56\u7565\u7684\u6df7\u5408\u5171\u4eab LoRA \u6a21\u578b\u3002MoSLD \u901a\u8fc7\u5728\u4e0d\u540c\u4e13\u5bb6\u4e4b\u95f4\u5171\u4eab LoRA \u4e2d\u7684\u4e0a\u5c42\u6295\u5f71\u77e9\u9635\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u9f13\u52b1\u6a21\u578b\u8de8\u4efb\u52a1\u5b66\u4e60\u4e00\u822c\u77e5\u8bc6\uff0c\u540c\u65f6\u4ecd\u7136\u5141\u8bb8\u4e0b\u5c42\u6295\u5f71\u77e9\u9635\u4e13\u6ce8\u4e8e\u6bcf\u4e2a\u4efb\u52a1\u7684\u72ec\u7279\u7279\u5f81\u3002dropout \u7684\u5e94\u7528\u51cf\u8f7b\u4e86\u53c2\u6570\u77e9\u9635\u7684\u4e0d\u5e73\u8861\u66f4\u65b0\uff0c\u5e76\u51cf\u8f7b\u4e86 LoRA \u4e2d\u7684\u53c2\u6570\u8fc7\u62df\u5408\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5177\u6709\u5f3a\u5927\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "author": "Lulu Zhao et.al.", "authors": "Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou", "id": "2412.08946v1", "paper_url": "http://arxiv.org/abs/2412.08946v1", "repo": "null"}}