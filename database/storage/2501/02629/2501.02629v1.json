{"2501.02629": {"publish_time": "2025-01-05", "title": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense", "paper_summary": "As large language models (LLMs) are increasingly deployed in diverse\napplications, including chatbot assistants and code generation, aligning their\nbehavior with safety and ethical standards has become paramount. However,\njailbreak attacks, which exploit vulnerabilities to elicit unintended or\nharmful outputs, threaten LLMs' safety significantly. In this paper, we\nintroduce Layer-AdvPatcher, a novel methodology designed to defend against\njailbreak attacks by utilizing an unlearning strategy to patch specific layers\nwithin LLMs through self-augmented datasets. Our insight is that certain\nlayer(s), tend to produce affirmative tokens when faced with harmful prompts.\nBy identifying these layers and adversarially exposing them to generate more\nharmful data, one can understand their inherent and diverse vulnerabilities to\nattacks. With these exposures, we then \"unlearn\" these issues, reducing the\nimpact of affirmative tokens and hence minimizing jailbreak risks while keeping\nthe model's responses to safe queries intact. We conduct extensive experiments\non two models, four benchmark datasets, and multiple state-of-the-art jailbreak\nbenchmarks to demonstrate the efficacy of our approach. Results indicate that\nour framework reduces the harmfulness and attack success rate of jailbreak\nattacks without compromising utility for benign queries compared to recent\ndefense methods.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u4e2d\u8d8a\u4f86\u8d8a\u5ee3\u6cdb\u5730\u90e8\u7f72\uff0c\u5305\u62ec\u804a\u5929\u6a5f\u5668\u4eba\u52a9\u7406\u548c\u7a0b\u5f0f\u78bc\u7522\u751f\uff0c\u4f7f\u5176\u884c\u70ba\u8207\u5b89\u5168\u548c\u502b\u7406\u6a19\u6e96\u4fdd\u6301\u4e00\u81f4\u5df2\u8b8a\u5f97\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u5229\u7528\u6f0f\u6d1e\u5f15\u767c\u610f\u5916\u6216\u6709\u5bb3\u8f38\u51fa\u7684\u8d8a\u7344\u653b\u64ca\uff0c\u56b4\u91cd\u5a01\u8105\u5230 LLM \u7684\u5b89\u5168\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 Layer-AdvPatcher\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u904e\u5229\u7528\u975e\u5b78\u7fd2\u7b56\u7565\u4f86\u4fee\u88dc LLM \u4e2d\u7684\u7279\u5b9a\u5c64\uff0c\u5f9e\u800c\u9632\u79a6\u8d8a\u7344\u653b\u64ca\u901a\u904e\u81ea\u6211\u64f4\u5145\u7684\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7684\u898b\u89e3\u662f\uff0c\u67d0\u4e9b\u5c64\u5728\u9762\u5c0d\u6709\u5bb3\u63d0\u793a\u6642\u5f80\u5f80\u6703\u7522\u751f\u80af\u5b9a\u7684\u6a19\u8a18\u3002\u901a\u904e\u8b58\u5225\u9019\u4e9b\u5c64\u4e26\u5c0d\u6297\u6027\u5730\u8b93\u5b83\u5011\u7522\u751f\u66f4\u591a\u6709\u5bb3\u8cc7\u6599\uff0c\u4eba\u5011\u53ef\u4ee5\u4e86\u89e3\u5b83\u5011\u56fa\u6709\u7684\u548c\u5c0d\u653b\u64ca\u7684\u591a\u7a2e\u6f0f\u6d1e\u3002\u6709\u4e86\u9019\u4e9b\u66dd\u5149\uff0c\u6211\u5011\u63a5\u8457\u300c\u5fd8\u8a18\u300d\u9019\u4e9b\u554f\u984c\uff0c\u6e1b\u5c11\u80af\u5b9a\u6a19\u8a18\u7684\u5f71\u97ff\uff0c\u5f9e\u800c\u6700\u5927\u7a0b\u5ea6\u5730\u964d\u4f4e\u8d8a\u7344\u98a8\u96aa\uff0c\u540c\u6642\u4fdd\u6301\u6a21\u578b\u5c0d\u5b89\u5168\u67e5\u8a62\u7684\u56de\u61c9\u4e0d\u8b8a\u3002\u6211\u5011\u5c0d\u5169\u500b\u6a21\u578b\u3001\u56db\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u548c\u591a\u500b\u6700\u5148\u9032\u7684\u8d8a\u7344\u57fa\u6e96\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4ee5\u8b49\u660e\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7d50\u679c\u8868\u660e\uff0c\u8207\u6700\u8fd1\u7684\u9632\u79a6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6846\u67b6\u964d\u4f4e\u4e86\u8d8a\u7344\u653b\u64ca\u7684\u5371\u5bb3\u6027\u548c\u653b\u64ca\u6210\u529f\u7387\uff0c\u540c\u6642\u4e0d\u640d\u5bb3\u826f\u6027\u67e5\u8a62\u7684\u6548\u7528\u3002", "author": "Yang Ouyang et.al.", "authors": "Yang Ouyang, Hengrui Gu, Shuhang Lin, Wenyue Hua, Jie Peng, Bhavya Kailkhura, Tianlong Chen, Kaixiong Zhou", "id": "2501.02629v1", "paper_url": "http://arxiv.org/abs/2501.02629v1", "repo": "null"}}