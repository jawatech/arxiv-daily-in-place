{"2406.02030": {"publish_time": "2024-06-04", "title": "Multimodal Reasoning with Multimodal Knowledge Graph", "paper_summary": "Multimodal reasoning with large language models (LLMs) often suffers from\nhallucinations and the presence of deficient or outdated knowledge within LLMs.\nSome approaches have sought to mitigate these issues by employing textual\nknowledge graphs, but their singular modality of knowledge limits comprehensive\ncross-modal understanding. In this paper, we propose the Multimodal Reasoning\nwith Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal\nknowledge graphs (MMKGs) to learn rich and semantic knowledge across\nmodalities, significantly enhancing the multimodal reasoning capabilities of\nLLMs. In particular, a relation graph attention network is utilized for\nencoding MMKGs and a cross-modal alignment module is designed for optimizing\nimage-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with\ninitial expertise in multimodal reasoning through pretraining. Remarkably,\nMR-MKG achieves superior performance while training on only a small fraction of\nparameters, approximately 2.25% of the LLM's parameter size. Experimental\nresults on multimodal question answering and multimodal analogy reasoning tasks\ndemonstrate that our MR-MKG method outperforms previous state-of-the-art\nmodels.", "paper_summary_zh": "\u591a\u6a21\u6001\u63a8\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\uff0c\u5e76\u4e14 LLM \u4e2d\u5b58\u5728\u77e5\u8bc6\u4e0d\u8db3\u6216\u8fc7\u65f6\u7684\u95ee\u9898\u3002\u4e00\u4e9b\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u4f7f\u7528\u6587\u672c\u77e5\u8bc6\u56fe\u8c31\u6765\u51cf\u8f7b\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u5355\u4e00\u7684\u77e5\u8bc6\u6a21\u5f0f\u9650\u5236\u4e86\u5168\u9762\u7684\u8de8\u6a21\u6001\u7406\u89e3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u591a\u6a21\u6001\u63a8\u7406 (MR-MKG) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31 (MMKG) \u6765\u5b66\u4e60\u8de8\u6a21\u6001\u7684\u4e30\u5bcc\u8bed\u4e49\u77e5\u8bc6\uff0c\u663e\u8457\u589e\u5f3a\u4e86 LLM \u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002\u7279\u522b\u662f\uff0c\u5173\u7cfb\u56fe\u8c31\u6ce8\u610f\u529b\u7f51\u7edc\u7528\u4e8e\u7f16\u7801 MMKG\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u6765\u4f18\u5316\u56fe\u50cf\u6587\u672c\u5bf9\u9f50\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e MMKG \u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u4e3a LLM \u63d0\u4f9b\u591a\u6a21\u6001\u63a8\u7406\u7684\u521d\u59cb\u4e13\u4e1a\u77e5\u8bc6\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMR-MKG \u5728\u4ec5\u8bad\u7ec3 LLM \u53c2\u6570\u89c4\u6a21\u7ea6 2.25% \u7684\u4e00\u5c0f\u90e8\u5206\u53c2\u6570\u65f6\u5c31\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u5728\u591a\u6a21\u6001\u95ee\u9898\u89e3\u7b54\u548c\u591a\u6a21\u6001\u7c7b\u6bd4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684 MR-MKG \u65b9\u6cd5\u4f18\u4e8e\u4ee5\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "author": "Junlin Lee et.al.", "authors": "Junlin Lee, Yequan Wang, Jing Li, Min Zhang", "id": "2406.02030v1", "paper_url": "http://arxiv.org/abs/2406.02030v1", "repo": "null"}}