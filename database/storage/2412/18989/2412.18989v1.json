{"2412.18989": {"publish_time": "2024-12-25", "title": "How Propense Are Large Language Models at Producing Code Smells? A Benchmarking Study", "paper_summary": "Large Language Models (LLMs) have shown significant potential in automating\nsoftware engineering tasks, particularly in code generation. However, current\nevaluation benchmarks, which primarily focus on accuracy, fall short in\nassessing the quality of the code generated by these models, specifically their\ntendency to produce code smells. To address this limitation, we introduce\nCodeSmellEval, a benchmark designed to evaluate the propensity of LLMs for\ngenerating code smells. Our benchmark includes a novel metric: Propensity\nSmelly Score (PSC), and a curated dataset of method-level code smells:\nCodeSmellData. To demonstrate the use of CodeSmellEval, we conducted a case\nstudy with two state-of-the-art LLMs, CodeLlama and Mistral. The results reveal\nthat both models tend to generate code smells, such as simplifiable-condition\nand consider-merging-isinstance. These findings highlight the effectiveness of\nour benchmark in evaluating LLMs, providing valuable insights into their\nreliability and their propensity to introduce code smells in code generation\ntasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u81ea\u52d5\u5316\u8edf\u9ad4\u5de5\u7a0b\u4efb\u52d9\uff0c\u7279\u5225\u662f\u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u65b9\u9762\uff0c\u5c55\u73fe\u4e86\u986f\u8457\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u4e3b\u8981\u8457\u91cd\u65bc\u6e96\u78ba\u6027\u7684\u8a55\u4f30\u57fa\u6e96\uff0c\u5728\u8a55\u4f30\u9019\u4e9b\u6a21\u578b\u6240\u7522\u751f\u7684\u7a0b\u5f0f\u78bc\u54c1\u8cea\u65b9\u9762\u6709\u6240\u4e0d\u8db3\uff0c\u7279\u5225\u662f\u5b83\u5011\u7522\u751f\u7a0b\u5f0f\u78bc\u7570\u5473\u7684\u50be\u5411\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 CodeSmellEval\uff0c\u4e00\u500b\u5c08\u9580\u7528\u65bc\u8a55\u4f30 LLM \u7522\u751f\u7a0b\u5f0f\u78bc\u7570\u5473\u7684\u50be\u5411\u7684\u57fa\u6e96\u3002\u6211\u5011\u7684\u57fa\u6e96\u5305\u542b\u4e00\u500b\u65b0\u7a4e\u7684\u6307\u6a19\uff1a\u50be\u5411\u7570\u5473\u5206\u6578 (PSC)\uff0c\u4ee5\u53ca\u4e00\u500b\u7d93\u904e\u6574\u7406\u7684\u51fd\u5f0f\u7d1a\u5225\u7a0b\u5f0f\u78bc\u7570\u5473\u8cc7\u6599\u96c6\uff1aCodeSmellData\u3002\u70ba\u4e86\u5c55\u793a CodeSmellEval \u7684\u7528\u6cd5\uff0c\u6211\u5011\u91dd\u5c0d\u5169\u500b\u6700\u5148\u9032\u7684 LLM\uff0cCodeLlama \u548c Mistral\uff0c\u9032\u884c\u4e86\u4e00\u500b\u6848\u4f8b\u7814\u7a76\u3002\u7d50\u679c\u986f\u793a\uff0c\u9019\u5169\u500b\u6a21\u578b\u90fd\u50be\u5411\u65bc\u7522\u751f\u7a0b\u5f0f\u78bc\u7570\u5473\uff0c\u4f8b\u5982\u7c21\u5316\u689d\u4ef6\u548c\u8003\u616e\u5408\u4f75 isinstance\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u6211\u5011\u57fa\u6e96\u5728\u8a55\u4f30 LLM \u6642\u7684\u6709\u6548\u6027\uff0c\u4e26\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\uff0c\u4e86\u89e3\u5b83\u5011\u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u4efb\u52d9\u4e2d\u5f15\u5165\u7a0b\u5f0f\u78bc\u7570\u5473\u7684\u53ef\u9760\u6027\u548c\u50be\u5411\u3002", "author": "Alejandro Velasco et.al.", "authors": "Alejandro Velasco, Daniel Rodriguez-Cardenas, David N. Palacio, Luftar Rahman Alif, Denys Poshyvanyk", "id": "2412.18989v1", "paper_url": "http://arxiv.org/abs/2412.18989v1", "repo": "null"}}