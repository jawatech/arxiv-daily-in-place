{"2502.13725": {"publish_time": "2025-02-19", "title": "Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method", "paper_summary": "Time series modeling holds significant importance in many real-world\napplications and has been extensively studied. While pre-trained foundation\nmodels have made impressive strides in the fields of natural language\nprocessing (NLP) and computer vision (CV), their development in time series\ndomains has been constrained by data sparsity. A series of recent studies have\ndemonstrated that large language models (LLMs) possess robust pattern\nrecognition and reasoning abilities over complex sequences of tokens. However,\nthe current literature have yet striked a high-quality balance between (a)\neffectively aligning the time series and natural language modalities, and (b)\nkeeping the inference efficiency. To address the above issues, we now propose\nthe Time-LlaMA framework. Time-LlaMA first converts the time series input into\ntoken embeddings through a linear tokenization mechanism. Second, the time\nseries token embeddings are aligned with the text prompts. Third, to further\nadapt the LLM backbone for time series modeling, we have developed a dynamic\nlow-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most\nsuitable LoRA modules at each layer of the Transformer backbone for each time\nseries input, enhancing the model's predictive capabilities. Our experimental\nresults on an extensive collection of challenging real-world time series tasks\nconfirm that our proposed method achieves the state-of-the-art (SOTA)\nperformance.", "paper_summary_zh": "\u6642\u5e8f\u5efa\u6a21\u5728\u8a31\u591a\u5be6\u969b\u61c9\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u7fa9\uff0c\u4e26\u4e14\u5df2\u88ab\u5ee3\u6cdb\u7814\u7a76\u3002\u96d6\u7136\u9810\u5148\u8a13\u7df4\u597d\u7684\u57fa\u790e\u6a21\u578b\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u548c\u96fb\u8166\u8996\u89ba (CV) \u9818\u57df\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u5728\u6642\u5e8f\u9818\u57df\u7684\u767c\u5c55\u53d7\u5230\u8cc7\u6599\u7a00\u758f\u6027\u7684\u9650\u5236\u3002\u4e00\u7cfb\u5217\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0d\u65bc\u8907\u96dc\u7684\u4ee3\u5e63\u5e8f\u5217\u5177\u5099\u5f37\u5927\u7684\u6a21\u5f0f\u8fa8\u8b58\u548c\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u6587\u737b\u5c1a\u672a\u5728 (a) \u6709\u6548\u5c0d\u9f4a\u6642\u5e8f\u548c\u81ea\u7136\u8a9e\u8a00\u6a21\u614b\uff0c\u4ee5\u53ca (b) \u4fdd\u6301\u63a8\u7406\u6548\u7387\u4e4b\u9593\u53d6\u5f97\u9ad8\u54c1\u8cea\u7684\u5e73\u8861\u3002\u70ba\u4e86\u89e3\u6c7a\u4e0a\u8ff0\u554f\u984c\uff0c\u6211\u5011\u73fe\u5728\u63d0\u51fa Time-LlaMA \u6846\u67b6\u3002Time-LlaMA \u9996\u5148\u900f\u904e\u7dda\u6027\u6a19\u8a18\u5316\u6a5f\u5236\u5c07\u6642\u5e8f\u8f38\u5165\u8f49\u63db\u70ba\u4ee3\u5e63\u5d4c\u5165\u3002\u5176\u6b21\uff0c\u6642\u5e8f\u4ee3\u5e63\u5d4c\u5165\u8207\u6587\u5b57\u63d0\u793a\u5c0d\u9f4a\u3002\u7b2c\u4e09\uff0c\u70ba\u4e86\u9032\u4e00\u6b65\u8abf\u6574 LLM \u4e3b\u5e79\u4ee5\u9032\u884c\u6642\u5e8f\u5efa\u6a21\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u52d5\u614b\u4f4e\u79e9\u9069\u61c9\u6280\u8853 (D-LoRA)\u3002D-LoRA \u6703\u52d5\u614b\u9078\u64c7 Transformer \u4e3b\u5e79\u4e2d\u6bcf\u4e00\u5c64\u6700\u5408\u9069\u7684 LoRA \u6a21\u7d44\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u7684\u9810\u6e2c\u80fd\u529b\u3002\u6211\u5011\u5728\u5927\u91cf\u5177\u6709\u6311\u6230\u6027\u7684\u771f\u5be6\u4e16\u754c\u6642\u5e8f\u4efb\u52d9\u96c6\u5408\u4e2d\u9032\u884c\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u5be6\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u9054\u5230\u4e86\u6700\u5148\u9032 (SOTA) \u7684\u6548\u80fd\u3002", "author": "Juyuan Zhang et.al.", "authors": "Juyuan Zhang, Wei Zhu, Jiechao Gao", "id": "2502.13725v1", "paper_url": "http://arxiv.org/abs/2502.13725v1", "repo": "null"}}