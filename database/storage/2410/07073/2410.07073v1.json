{"2410.07073": {"publish_time": "2024-10-09", "title": "Pixtral 12B", "paper_summary": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.\nPixtral-12B is trained to understand both natural images and documents,\nachieving leading performance on various multimodal benchmarks, surpassing a\nnumber of larger models. Unlike many open-source models, Pixtral is also a\ncutting-edge text model for its size, and does not compromise on natural\nlanguage performance to excel in multimodal tasks. Pixtral uses a new vision\nencoder trained from scratch, which allows it to ingest images at their natural\nresolution and aspect ratio. This gives users flexibility on the number of\ntokens used to process an image. Pixtral is also able to process any number of\nimages in its long context window of 128K tokens. Pixtral 12B substanially\noutperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B).\nIt also outperforms much larger open models like Llama-3.2 90B while being 7x\nsmaller. We further contribute an open-source benchmark, MM-MT-Bench, for\nevaluating vision-language models in practical scenarios, and provide detailed\nanalysis and code for standardized evaluation protocols for multimodal LLMs.\nPixtral-12B is released under Apache 2.0 license.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39\u4e86 Pixtral-12B\uff0c\u4e00\u500b\u64c1\u6709 120 \u5104\u500b\u53c3\u6578\u7684\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b\u3002\nPixtral-12B \u7d93\u904e\u8a13\u7df4\uff0c\u53ef\u4ee5\u7406\u89e3\u81ea\u7136\u5f71\u50cf\u548c\u6587\u4ef6\uff0c\n\u5728\u5404\u7a2e\u591a\u6a21\u614b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u53d6\u5f97\u9818\u5148\u7684\u6548\u80fd\uff0c\u8d85\u8d8a\u4e86\n\u8a31\u591a\u8f03\u5927\u578b\u7684\u6a21\u578b\u3002\u8207\u8a31\u591a\u958b\u6e90\u6a21\u578b\u4e0d\u540c\uff0cPixtral \u4e5f\u662f\n\u4e00\u500b\u4ee5\u5176\u898f\u6a21\u800c\u8a00\u6700\u5148\u9032\u7684\u6587\u5b57\u6a21\u578b\uff0c\u4e26\u4e14\u4e0d\u6703\n\u70ba\u4e86\u5728\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u800c\u72a7\u7272\u81ea\u7136\n\u8a9e\u8a00\u6548\u80fd\u3002Pixtral \u4f7f\u7528\u4e00\u500b\u5f9e\u982d\u8a13\u7df4\u7684\u65b0\u8996\u89ba\n\u7de8\u78bc\u5668\uff0c\u5b83\u5141\u8a31\u4ee5\u5176\u81ea\u7136\u89e3\u6790\u5ea6\u548c\u9577\u5bec\u6bd4\u64f7\u53d6\u5f71\u50cf\u3002\u9019\u8b93\u4f7f\u7528\u8005\u5728\u7528\u65bc\u8655\u7406\u5f71\u50cf\u7684\u4ee3\u78bc\u6578\u91cf\u4e0a\u66f4\u5177\u5f48\u6027\u3002Pixtral \u4e5f\u80fd\u5920\u5728\u5176 128K \u4ee3\u78bc\u7684\u9577\u8108\u7d61\u8996\u7a97\u4e2d\u8655\u7406\u4efb\u610f\u6578\u91cf\u7684\u5f71\u50cf\u3002Pixtral 12B \u5728\u8868\u73fe\u4e0a\u5927\u5e45\u8d85\u8d8a\u5176\u4ed6\u985e\u4f3c\u898f\u6a21\u7684\u958b\u653e\u6a21\u578b\uff08Llama-3.2 11B \u548c Qwen-2-VL 7B\uff09\u3002\n\u5b83\u4e5f\u8d85\u8d8a\u4e86\u8a31\u591a\u5927\u5f97\u591a\u7684\u958b\u653e\u6a21\u578b\uff0c\u4f8b\u5982 Llama-3.2 90B\uff0c\u540c\u6642\u537b\u5c0f\u4e86 7 \u500d\u3002\u6211\u5011\u9032\u4e00\u6b65\u8ca2\u737b\u4e86\u4e00\u500b\u958b\u653e\u539f\u59cb\u78bc\u57fa\u6e96\u6e2c\u8a66 MM-MT-Bench\uff0c\u7528\u65bc\u5728\u5be6\u969b\u60c5\u6cc1\u4e0b\u8a55\u4f30\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u4e26\u63d0\u4f9b\u8a73\u7d30\u7684\u5206\u6790\u548c\u7528\u65bc\u591a\u6a21\u614b LLM \u7684\u6a19\u6e96\u5316\u8a55\u4f30\u5354\u5b9a\u7684\u7a0b\u5f0f\u78bc\u3002\nPixtral-12B \u5728 Apache 2.0 \u6388\u6b0a\u4e0b\u767c\u5e03\u3002", "author": "Pravesh Agrawal et.al.", "authors": "Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Am\u00e9lie H\u00e9liou, Paul Jacob, Albert Q. Jiang, Timoth\u00e9e Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozi\u00e8re, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang", "id": "2410.07073v1", "paper_url": "http://arxiv.org/abs/2410.07073v1", "repo": "https://github.com/mistralai/mistral-inference"}}