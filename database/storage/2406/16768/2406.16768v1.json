{"2406.16768": {"publish_time": "2024-06-24", "title": "WARP: On the Benefits of Weight Averaged Rewarded Policies", "paper_summary": "Reinforcement learning from human feedback (RLHF) aligns large language\nmodels (LLMs) by encouraging their generations to have high rewards, using a\nreward model trained on human preferences. To prevent the forgetting of\npre-trained knowledge, RLHF usually incorporates a KL regularization; this\nforces the policy to remain close to its supervised fine-tuned initialization,\nthough it hinders the reward optimization. To tackle the trade-off between KL\nand reward, in this paper we introduce a novel alignment strategy named Weight\nAveraged Rewarded Policies (WARP). WARP merges policies in the weight space at\nthree distinct stages. First, it uses the exponential moving average of the\npolicy as a dynamic anchor in the KL regularization. Second, it applies\nspherical interpolation to merge independently fine-tuned policies into a new\nenhanced one. Third, it linearly interpolates between this merged model and the\ninitialization, to recover features from pre-training. This procedure is then\napplied iteratively, with each iteration's final model used as an advanced\ninitialization for the next, progressively refining the KL-reward Pareto front,\nachieving superior rewards at fixed KL. Experiments with GEMMA policies\nvalidate that WARP improves their quality and alignment, outperforming other\nopen-source LLMs.", "paper_summary_zh": "\u900f\u904e\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF) \u6703\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4eba\u985e\u504f\u597d\u4e0a\u8a13\u7df4\u51fa\u7684\u734e\u52f5\u6a21\u578b\u5c0d\u9f4a\uff0c\u9f13\u52f5\u5b83\u5011\u7522\u751f\u9ad8\u734e\u52f5\u3002\u70ba\u4e86\u9632\u6b62\u907a\u5fd8\u9810\u5148\u8a13\u7df4\u7684\u77e5\u8b58\uff0cRLHF \u901a\u5e38\u6703\u7d50\u5408 KL \u6b63\u898f\u5316\uff1b\u9019\u6703\u5f37\u8feb\u7b56\u7565\u8cbc\u8fd1\u5176\u76e3\u7763\u5fae\u8abf\u7684\u521d\u59cb\u5316\uff0c\u5118\u7ba1\u9019\u6703\u963b\u7919\u734e\u52f5\u6700\u4f73\u5316\u3002\u70ba\u4e86\u8655\u7406 KL \u8207\u734e\u52f5\u4e4b\u9593\u7684\u53d6\u6368\uff0c\u6211\u5011\u5728\u672c\u6587\u4e2d\u5f15\u5165\u4e86\u4e00\u7a2e\u540d\u70ba\u6b0a\u91cd\u5e73\u5747\u734e\u52f5\u7b56\u7565 (WARP) \u7684\u65b0\u5c0d\u9f4a\u7b56\u7565\u3002WARP \u5728\u4e09\u500b\u4e0d\u540c\u7684\u968e\u6bb5\u5408\u4f75\u6b0a\u91cd\u7a7a\u9593\u4e2d\u7684\u7b56\u7565\u3002\u9996\u5148\uff0c\u5b83\u4f7f\u7528\u7b56\u7565\u7684\u6307\u6578\u79fb\u52d5\u5e73\u5747\u4f5c\u70ba KL \u6b63\u898f\u5316\u4e2d\u7684\u52d5\u614b\u9328\u9ede\u3002\u5176\u6b21\uff0c\u5b83\u61c9\u7528\u7403\u9762\u63d2\u503c\u5c07\u7368\u7acb\u5fae\u8abf\u7684\u7b56\u7565\u5408\u4f75\u6210\u4e00\u500b\u65b0\u7684\u589e\u5f37\u7b56\u7565\u3002\u7b2c\u4e09\uff0c\u5b83\u5728\u9019\u500b\u5408\u4f75\u6a21\u578b\u548c\u521d\u59cb\u5316\u4e4b\u9593\u9032\u884c\u7dda\u6027\u63d2\u503c\uff0c\u4ee5\u5f9e\u9810\u8a13\u7df4\u4e2d\u6062\u5fa9\u7279\u5fb5\u3002\u7136\u5f8c\u53cd\u8986\u5957\u7528\u6b64\u7a0b\u5e8f\uff0c\u6bcf\u6b21\u8fed\u4ee3\u7684\u6700\u7d42\u6a21\u578b\u90fd\u7528\u4f5c\u4e0b\u4e00\u6b21\u7684\u9ad8\u7d1a\u521d\u59cb\u5316\uff0c\u9010\u6b65\u512a\u5316 KL-\u734e\u52f5\u5e15\u7d2f\u6258\u524d\u7de3\uff0c\u5728\u56fa\u5b9a\u7684 KL \u4e2d\u7372\u5f97\u66f4\u9ad8\u7684\u734e\u52f5\u3002\u4f7f\u7528 GEMMA \u7b56\u7565\u7684\u5be6\u9a57\u9a57\u8b49\u4e86 WARP \u6539\u5584\u4e86\u5b83\u5011\u7684\u54c1\u8cea\u548c\u5c0d\u9f4a\uff0c\u512a\u65bc\u5176\u4ed6\u958b\u6e90 LLM\u3002", "author": "Alexandre Ram\u00e9 et.al.", "authors": "Alexandre Ram\u00e9, Johan Ferret, Nino Vieillard, Robert Dadashi, L\u00e9onard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, Olivier Bachem", "id": "2406.16768v1", "paper_url": "http://arxiv.org/abs/2406.16768v1", "repo": "null"}}