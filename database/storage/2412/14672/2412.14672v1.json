{"2412.14672": {"publish_time": "2024-12-19", "title": "FiVL: A Framework for Improved Vision-Language Alignment", "paper_summary": "Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. This issue extends to\nvision-language benchmarks, where it is difficult to make the image\nindispensable for accurate answer generation, particularly in vision\nquestion-answering tasks. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nto evaluate their effectiveness in achieving it. These datasets can be utilized\nfor both training and assessing an LVLM's ability to use image content as\nsubstantive evidence rather than relying solely on linguistic priors, providing\ninsights into the model's reliance on visual information. To demonstrate the\nutility of our dataset, we introduce an innovative training task that\noutperforms baselines alongside a validation method and application for\nexplainability. The code is available at https://github.com/IntelLabs/fivl.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5728\u6574\u5408\u8996\u89ba\u548c\u6587\u5b57\u8f38\u5165\u4ee5\u9032\u884c\u591a\u6a21\u614b\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u9032\u5c55\u3002\u7136\u800c\uff0c\u4e00\u500b\u7d93\u5e38\u9047\u5230\u7684\u6311\u6230\u662f\u78ba\u4fdd\u9019\u4e9b\u6a21\u578b\u5728\u9700\u8981\u5169\u7a2e\u6a21\u614b\u624d\u80fd\u5236\u5b9a\u6e96\u78ba\u7b54\u6848\u6642\uff0c\u80fd\u50cf\u4f7f\u7528\u8a9e\u8a00\u5167\u5bb9\u4e00\u6a23\u6709\u6548\u5730\u5229\u7528\u8996\u89ba\u8cc7\u8a0a\u3002\u6211\u5011\u5047\u8a2d\u5e7b\u89ba\u7684\u7522\u751f\u662f\u56e0\u70ba\u7576\u524d LVLMs \u7f3a\u4e4f\u6709\u6548\u7684\u8996\u89ba\u57fa\u790e\u3002\u9019\u500b\u554f\u984c\u5ef6\u4f38\u5230\u8996\u89ba\u8a9e\u8a00\u57fa\u6e96\uff0c\u5728\u8996\u89ba\u8a9e\u8a00\u57fa\u6e96\u4e2d\uff0c\u5f88\u96e3\u8b93\u5f71\u50cf\u5c0d\u65bc\u7522\u751f\u6e96\u78ba\u7b54\u6848\u4f86\u8aaa\u662f\u4e0d\u53ef\u6216\u7f3a\u7684\uff0c\u7279\u5225\u662f\u5728\u8996\u89ba\u554f\u7b54\u4efb\u52d9\u4e2d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 FiVL\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u5efa\u69cb\u8cc7\u6599\u96c6\uff0c\u4ee5\u8a13\u7df4 LVLMs \u4ee5\u589e\u5f37\u8996\u89ba\u57fa\u790e\uff0c\u4e26\u8a55\u4f30\u5b83\u5011\u5728\u5be6\u73fe\u6b64\u76ee\u6a19\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u9019\u4e9b\u8cc7\u6599\u96c6\u53ef\u7528\u65bc\u8a13\u7df4\u548c\u8a55\u4f30 LVLM \u4f7f\u7528\u5f71\u50cf\u5167\u5bb9\u4f5c\u70ba\u5be6\u8cea\u8b49\u64da\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u50c5\u4f9d\u8cf4\u8a9e\u8a00\u5148\u9a57\uff0c\u5f9e\u800c\u6df1\u5165\u4e86\u89e3\u6a21\u578b\u5c0d\u8996\u89ba\u8cc7\u8a0a\u7684\u4f9d\u8cf4\u6027\u3002\u70ba\u4e86\u8b49\u660e\u6211\u5011\u8cc7\u6599\u96c6\u7684\u6548\u7528\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u9805\u5275\u65b0\u7684\u8a13\u7df4\u4efb\u52d9\uff0c\u8a72\u4efb\u52d9\u512a\u65bc\u57fa\u6e96\uff0c\u4e26\u9644\u5e36\u9a57\u8b49\u65b9\u6cd5\u548c\u53ef\u89e3\u91cb\u6027\u61c9\u7528\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/IntelLabs/fivl \u53d6\u5f97\u3002", "author": "Estelle Aflalo et.al.", "authors": "Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal", "id": "2412.14672v1", "paper_url": "http://arxiv.org/abs/2412.14672v1", "repo": "null"}}