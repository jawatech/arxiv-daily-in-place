{"2312.06032": {"publish_time": "2023-12-10", "title": "Evaluating the Utility of Model Explanations for Model Development", "paper_summary": "One of the motivations for explainable AI is to allow humans to make better\nand more informed decisions regarding the use and deployment of AI models. But\ncareful evaluations are needed to assess whether this expectation has been\nfulfilled. Current evaluations mainly focus on algorithmic properties of\nexplanations, and those that involve human subjects often employ subjective\nquestions to test human's perception of explanation usefulness, without being\ngrounded in objective metrics and measurements. In this work, we evaluate\nwhether explanations can improve human decision-making in practical scenarios\nof machine learning model development. We conduct a mixed-methods user study\ninvolving image data to evaluate saliency maps generated by SmoothGrad,\nGradCAM, and an oracle explanation on two tasks: model selection and\ncounterfactual simulation. To our surprise, we did not find evidence of\nsignificant improvement on these tasks when users were provided with any of the\nsaliency maps, even the synthetic oracle explanation designed to be simple to\nunderstand and highly indicative of the answer. Nonetheless, explanations did\nhelp users more accurately describe the models. These findings suggest caution\nregarding the usefulness and potential for misunderstanding in saliency-based\nexplanations.", "paper_summary_zh": "", "author": "Shawn Im et.al.", "authors": "Shawn Im,Jacob Andreas,Yilun Zhou", "id": "2312.06032v1", "paper_url": "http://arxiv.org/abs/2312.06032v1", "repo": "null"}}