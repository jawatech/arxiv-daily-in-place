{"2406.06366": {"publish_time": "2024-06-10", "title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models", "paper_summary": "Initially introduced as a machine translation model, the Transformer\narchitecture has now become the foundation for modern deep learning\narchitecture, with applications in a wide range of fields, from computer vision\nto natural language processing. Nowadays, to tackle increasingly more complex\ntasks, Transformer-based models are stretched to enormous sizes, requiring\nincreasingly larger training datasets, and unsustainable amount of compute\nresources. The ubiquitous nature of the Transformer and its core component, the\nattention mechanism, are thus prime targets for efficiency research. In this\nwork, we propose an alternative compatibility function for the self-attention\nmechanism introduced by the Transformer architecture. This compatibility\nfunction exploits an overlap in the learned representation of the traditional\nscaled dot-product attention, leading to a symmetric with pairwise coefficient\ndot-product attention. When applied to the pre-training of BERT-like models,\nthis new symmetric attention mechanism reaches a score of 79.36 on the GLUE\nbenchmark against 78.74 for the traditional implementation, leads to a\nreduction of 6% in the number of trainable parameters, and reduces the number\nof training steps required before convergence by half.", "paper_summary_zh": "\u6700\u521d\u4ee5\u6a5f\u5668\u7ffb\u8b6f\u6a21\u578b\u63a8\u51fa\u7684 Transformer \u67b6\u69cb\uff0c\u73fe\u5df2\u6210\u70ba\u73fe\u4ee3\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\u7684\u57fa\u790e\uff0c\u5728\u5f9e\u96fb\u8166\u8996\u89ba\u5230\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7b49\u5ee3\u6cdb\u9818\u57df\u4e2d\u90fd\u6709\u61c9\u7528\u3002\u5982\u4eca\uff0c\u70ba\u4e86\u61c9\u5c0d\u65e5\u76ca\u8907\u96dc\u7684\u4efb\u52d9\uff0cTransformer \u6a21\u578b\u88ab\u64f4\u5c55\u5230\u6975\u5927\u7684\u898f\u6a21\uff0c\u9700\u8981\u8d8a\u4f86\u8d8a\u5927\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\u548c\u96e3\u4ee5\u6301\u7e8c\u7684\u904b\u7b97\u8cc7\u6e90\u3002Transformer \u548c\u5176\u6838\u5fc3\u7d44\u6210\u90e8\u5206\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u666e\u904d\u6027\uff0c\u56e0\u6b64\u6210\u70ba\u6548\u7387\u7814\u7a76\u7684\u4e3b\u8981\u76ee\u6a19\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u70ba Transformer \u67b6\u69cb\u5f15\u5165\u7684\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u63d0\u51fa\u4e86\u53e6\u4e00\u7a2e\u76f8\u5bb9\u6027\u51fd\u6578\u3002\u6b64\u76f8\u5bb9\u6027\u51fd\u6578\u5229\u7528\u50b3\u7d71\u7e2e\u653e\u9ede\u7a4d\u6ce8\u610f\u529b\u7684\u5b78\u7fd2\u8868\u5fb5\u4e2d\u7684\u91cd\u758a\uff0c\u5c0e\u81f4\u8207\u6210\u5c0d\u4fc2\u6578\u9ede\u7a4d\u6ce8\u610f\u529b\u5c0d\u7a31\u3002\u7576\u61c9\u7528\u65bc BERT \u985e\u6a21\u578b\u7684\u9810\u8a13\u7df4\u6642\uff0c\u9019\u7a2e\u65b0\u7684\u5c0d\u7a31\u6ce8\u610f\u529b\u6a5f\u5236\u5728 GLUE \u57fa\u6e96\u4e0a\u9054\u5230 79.36 \u5206\uff0c\u800c\u50b3\u7d71\u5be6\u4f5c\u5247\u70ba 78.74 \u5206\uff0c\u5c0e\u81f4\u53ef\u8a13\u7df4\u53c3\u6578\u6578\u91cf\u6e1b\u5c11 6%\uff0c\u4e26\u5c07\u6536\u6582\u524d\u6240\u9700\u7684\u8a13\u7df4\u6b65\u9a5f\u6578\u91cf\u6e1b\u534a\u3002", "author": "Martin Courtois et.al.", "authors": "Martin Courtois, Malte Ostendorff, Leonhard Hennig, Georg Rehm", "id": "2406.06366v1", "paper_url": "http://arxiv.org/abs/2406.06366v1", "repo": "null"}}