{"2407.13089": {"publish_time": "2024-07-18", "title": "MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking", "paper_summary": "Fact-checking real-world claims often requires reviewing multiple multimodal\ndocuments to assess a claim's truthfulness, which is a highly laborious and\ntime-consuming task. In this paper, we present a summarization model designed\nto generate claim-specific summaries useful for fact-checking from multimodal,\nmulti-document datasets. The model takes inputs in the form of documents,\nimages, and a claim, with the objective of assisting in fact-checking tasks. We\nintroduce a dynamic perceiver-based model that can handle inputs from multiple\nmodalities of arbitrary lengths. To train our model, we leverage a novel\nreinforcement learning-based entailment objective to generate summaries that\nprovide evidence distinguishing between different truthfulness labels. To\nassess the efficacy of our approach, we conduct experiments on both an existing\nbenchmark and a new dataset of multi-document claims that we contribute. Our\napproach outperforms the SOTA approach by 4.6% in the claim verification task\non the MOCHEG dataset and demonstrates strong performance on our new\nMulti-News-Fact-Checking dataset.", "paper_summary_zh": "\u4e8b\u5be6\u67e5\u6838\u73fe\u5be6\u4e16\u754c\u7684\u8aaa\u6cd5\u901a\u5e38\u9700\u8981\u6aa2\u95b1\u591a\u7a2e\u591a\u6a21\u614b\u6587\u4ef6\u4ee5\u8a55\u4f30\u8aaa\u6cd5\u7684\u771f\u5be6\u6027\uff0c\u9019\u662f\u4e00\u9805\u9ad8\u5ea6\u8cbb\u529b\u548c\u8017\u6642\u7684\u4efb\u52d9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u6458\u8981\u6a21\u578b\uff0c\u65e8\u5728\u5f9e\u591a\u6a21\u614b\u3001\u591a\u6587\u4ef6\u6578\u64da\u96c6\u4e2d\u751f\u6210\u5c0d\u4e8b\u5be6\u67e5\u6838\u6709\u7528\u7684\u7279\u5b9a\u65bc\u8aaa\u6cd5\u7684\u6458\u8981\u3002\u8a72\u6a21\u578b\u4ee5\u6587\u4ef6\u3001\u5716\u50cf\u548c\u8aaa\u6cd5\u5f62\u5f0f\u63a5\u6536\u8f38\u5165\uff0c\u76ee\u7684\u662f\u5354\u52a9\u9032\u884c\u4e8b\u5be6\u67e5\u6838\u4efb\u52d9\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u57fa\u65bc\u52d5\u614b\u611f\u77e5\u5668\u7684\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u8655\u7406\u4f86\u81ea\u591a\u7a2e\u6a21\u614b\u7684\u4efb\u610f\u9577\u5ea6\u7684\u8f38\u5165\u3002\u70ba\u4e86\u8a13\u7df4\u6211\u5011\u7684\u6a21\u578b\uff0c\u6211\u5011\u5229\u7528\u4e00\u7a2e\u65b0\u7a4e\u7684\u57fa\u65bc\u5f37\u5316\u5b78\u7fd2\u7684\u860a\u6db5\u76ee\u6a19\u4f86\u751f\u6210\u6458\u8981\uff0c\u9019\u4e9b\u6458\u8981\u63d0\u4f9b\u4e86\u5340\u5206\u4e0d\u540c\u771f\u5be6\u6027\u6a19\u7c64\u7684\u8b49\u64da\u3002\u70ba\u4e86\u8a55\u4f30\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5728\u73fe\u6709\u57fa\u6e96\u548c\u6211\u5011\u8ca2\u737b\u7684\u591a\u6587\u4ef6\u8aaa\u6cd5\u7684\u65b0\u6578\u64da\u96c6\u4e0a\u9032\u884c\u4e86\u5be6\u9a57\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728 MOCHEG \u6578\u64da\u96c6\u4e0a\u7684\u8aaa\u6cd5\u9a57\u8b49\u4efb\u52d9\u4e2d\u6bd4 SOTA \u65b9\u6cd5\u9ad8\u51fa 4.6%\uff0c\u4e26\u5728\u6211\u5011\u65b0\u7684 Multi-News-Fact-Checking \u6578\u64da\u96c6\u4e0a\u5c55\u793a\u4e86\u5f37\u52c1\u7684\u6027\u80fd\u3002", "author": "Ting-Chih Chen et.al.", "authors": "Ting-Chih Chen, Chia-Wei Tang, Chris Thomas", "id": "2407.13089v1", "paper_url": "http://arxiv.org/abs/2407.13089v1", "repo": "null"}}