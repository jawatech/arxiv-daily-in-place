{"2412.15200": {"publish_time": "2024-12-19", "title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation", "paper_summary": "Procedural Content Generation (PCG) is powerful in creating high-quality 3D\ncontents, yet controlling it to produce desired shapes is difficult and often\nrequires extensive parameter tuning. Inverse Procedural Content Generation aims\nto automatically find the best parameters under the input condition. However,\nexisting sampling-based and neural network-based methods still suffer from\nnumerous sample iterations or limited controllability. In this work, we present\nDI-PCG, a novel and efficient method for Inverse PCG from general image\nconditions. At its core is a lightweight diffusion transformer model, where PCG\nparameters are directly treated as the denoising target and the observed images\nas conditions to control parameter generation. DI-PCG is efficient and\neffective. With only 7.6M network parameters and 30 GPU hours to train, it\ndemonstrates superior performance in recovering parameters accurately, and\ngeneralizing well to in-the-wild images. Quantitative and qualitative\nexperiment results validate the effectiveness of DI-PCG in inverse PCG and\nimage-to-3D generation tasks. DI-PCG offers a promising approach for efficient\ninverse PCG and represents a valuable exploration step towards a 3D generation\npath that models how to construct a 3D asset using parametric models.", "paper_summary_zh": "\u7a0b\u5e8f\u5167\u5bb9\u7522\u751f\uff08PCG\uff09\u5728\u5efa\u7acb\u9ad8\u54c1\u8cea 3D \u5167\u5bb9\u65b9\u9762\u5f88\u5f37\u5927\uff0c\u4f46\u8981\u63a7\u5236\u5b83\u4f86\u7522\u751f\u6240\u9700\u7684\u5f62\u72c0\u5f88\u56f0\u96e3\uff0c\u800c\u4e14\u901a\u5e38\u9700\u8981\u5ee3\u6cdb\u7684\u53c3\u6578\u8abf\u6574\u3002\u53cd\u5411\u7a0b\u5e8f\u5167\u5bb9\u7522\u751f\u65e8\u5728\u81ea\u52d5\u627e\u51fa\u8f38\u5165\u689d\u4ef6\u4e0b\u7684\u6700\u4f73\u53c3\u6578\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u65bc\u63a1\u6a23\u548c\u57fa\u65bc\u795e\u7d93\u7db2\u8def\u7684\u65b9\u6cd5\u4ecd\u7136\u6709\u8a31\u591a\u6a23\u672c\u8fed\u4ee3\u6216\u63a7\u5236\u80fd\u529b\u6709\u9650\u7684\u554f\u984c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa DI-PCG\uff0c\u9019\u662f\u4e00\u7a2e\u5f9e\u4e00\u822c\u5f71\u50cf\u689d\u4ef6\u4e2d\u9032\u884c\u53cd\u5411 PCG \u7684\u65b0\u7a4e\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002\u5176\u6838\u5fc3\u662f\u4e00\u500b\u8f15\u91cf\u7d1a\u7684\u64f4\u6563\u8b8a\u63db\u5668\u6a21\u578b\uff0c\u5176\u4e2d PCG \u53c3\u6578\u88ab\u76f4\u63a5\u8996\u70ba\u53bb\u566a\u76ee\u6a19\uff0c\u800c\u89c0\u5bdf\u5230\u7684\u5f71\u50cf\u5247\u4f5c\u70ba\u63a7\u5236\u53c3\u6578\u751f\u6210\u7684\u689d\u4ef6\u3002DI-PCG \u662f\u6709\u6548\u7387\u4e14\u6709\u6548\u7684\u3002\u5b83\u53ea\u6709 7.6M \u7db2\u8def\u53c3\u6578\u548c 30 \u500b GPU \u5c0f\u6642\u9032\u884c\u8a13\u7df4\uff0c\u5728\u6e96\u78ba\u6062\u5fa9\u53c3\u6578\u65b9\u9762\u8868\u73fe\u51fa\u512a\u7570\u7684\u6027\u80fd\uff0c\u4e26\u4e14\u80fd\u5f88\u597d\u5730\u63a8\u5ee3\u5230\u91ce\u5916\u5f71\u50cf\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u5be6\u9a57\u7d50\u679c\u9a57\u8b49\u4e86 DI-PCG \u5728\u53cd\u5411 PCG \u548c\u5f71\u50cf\u5230 3D \u751f\u6210\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\u3002DI-PCG \u70ba\u6709\u6548\u7387\u7684\u53cd\u5411 PCG \u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u4e26\u4ee3\u8868\u4e86\u4e00\u500b\u6709\u50f9\u503c\u7684\u63a2\u7d22\u6b65\u9a5f\uff0c\u671d\u8457\u4e00\u500b\u4f7f\u7528\u53c3\u6578\u6a21\u578b\u4f86\u5efa\u69cb 3D \u8cc7\u7522\u7684 3D \u751f\u6210\u8def\u5f91\u9081\u9032\u3002", "author": "Wang Zhao et.al.", "authors": "Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan", "id": "2412.15200v1", "paper_url": "http://arxiv.org/abs/2412.15200v1", "repo": "null"}}