{"2501.12619": {"publish_time": "2025-01-22", "title": "Distillation Quantification for Large Language Models", "paper_summary": "Model distillation is a technique for transferring knowledge from large\nlanguage models (LLMs) to smaller ones, aiming to create resource-efficient yet\nhigh-performing models. However, excessive distillation can lead to\nhomogenization, reducing diversity among models and impairing their ability to\nrobustly handle complex or novel tasks. These limitations underscore the need\nto systematically quantify the distillation process and its impact. In this\nwork, we propose a framework to evaluate and quantify model distillation. Our\nmethod addresses two key aspects: (1) Identifying identity cognition\ncontradictions to assess discrepancies in how models perceive and represent\nidentity-related information, and (2) Analyzing multi-granularity response\nsimilarities across models to measure the extent of homogenization.\nExperimental results demonstrate two key insights: (1) Well-known closed-source\nand open-source LLMs usually exhibit high distillation degrees, except for\nClaude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees\ncompared to aligned LLMs. By offering a systematic approach to improve the\ntransparency of LLM data distillation, we call for LLMs with more independent\ndevelopment and more transparent technical reports to improve LLMs' robustness\nand safety. The code and data are available under\nhttps://github.com/Aegis1863/LLMs-Distillation-Quantification.", "paper_summary_zh": "\u6a21\u578b\u84b8\u993e\u662f\u4e00\u7a2e\u5c07\u77e5\u8b58\u5f9e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8f49\u79fb\u5230\u8f03\u5c0f\u6a21\u578b\u7684\u6280\u8853\uff0c\u76ee\u7684\u662f\u5efa\u7acb\u8cc7\u6e90\u6548\u7387\u9ad8\u4f46\u6548\u80fd\u826f\u597d\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u904e\u5ea6\u7684\u84b8\u993e\u53ef\u80fd\u5c0e\u81f4\u540c\u8cea\u5316\uff0c\u6e1b\u5c11\u6a21\u578b\u4e4b\u9593\u7684\u591a\u6a23\u6027\uff0c\u4e26\u640d\u5bb3\u5b83\u5011\u7a69\u5065\u8655\u7406\u8907\u96dc\u6216\u65b0\u7a4e\u4efb\u52d9\u7684\u80fd\u529b\u3002\u9019\u4e9b\u9650\u5236\u5f37\u8abf\u4e86\u7cfb\u7d71\u5316\u91cf\u5316\u84b8\u993e\u904e\u7a0b\u53ca\u5176\u5f71\u97ff\u7684\u5fc5\u8981\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u8a55\u4f30\u548c\u91cf\u5316\u6a21\u578b\u84b8\u993e\u7684\u6846\u67b6\u3002\u6211\u5011\u7684\u6a21\u578b\u63a2\u8a0e\u4e86\u5169\u500b\u95dc\u9375\u9762\u5411\uff1a(1) \u8b58\u5225\u8eab\u5206\u8a8d\u77e5\u77db\u76fe\uff0c\u4ee5\u8a55\u4f30\u6a21\u578b\u5728\u611f\u77e5\u548c\u5448\u73fe\u8eab\u5206\u76f8\u95dc\u8cc7\u8a0a\u7684\u65b9\u5f0f\u4e0a\u7684\u5dee\u7570\uff0c\u4ee5\u53ca (2) \u5206\u6790\u6a21\u578b\u4e4b\u9593\u7684\u591a\u7c92\u5ea6\u56de\u61c9\u76f8\u4f3c\u6027\uff0c\u4ee5\u8861\u91cf\u540c\u8cea\u5316\u7684\u7a0b\u5ea6\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u5169\u500b\u95dc\u9375\u898b\u89e3\uff1a(1) \u9664\u4e86 Claude\u3001Doubao \u548c Gemini \u4e4b\u5916\uff0c\u773e\u6240\u5468\u77e5\u7684\u9589\u6e90\u548c\u958b\u6e90 LLM \u901a\u5e38\u8868\u73fe\u51fa\u9ad8\u5ea6\u7684\u84b8\u993e\u7a0b\u5ea6\u3002(2) \u8207\u5c0d\u9f4a\u7684 LLM \u76f8\u6bd4\uff0c\u57fa\u790e LLM \u986f\u793a\u51fa\u66f4\u9ad8\u7684\u84b8\u993e\u7a0b\u5ea6\u3002\u900f\u904e\u63d0\u4f9b\u4e00\u7a2e\u7cfb\u7d71\u5316\u7684\u65b9\u6cd5\u4f86\u63d0\u9ad8 LLM \u8cc7\u6599\u84b8\u993e\u7684\u900f\u660e\u5ea6\uff0c\u6211\u5011\u547c\u7c72\u958b\u767c\u51fa\u66f4\u7368\u7acb\u4e14\u6280\u8853\u5831\u544a\u66f4\u900f\u660e\u7684 LLM\uff0c\u4ee5\u63d0\u9ad8 LLM \u7684\u7a69\u5065\u6027\u548c\u5b89\u5168\u6027\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728 https://github.com/Aegis1863/LLMs-Distillation-Quantification \u4e0b\u53d6\u5f97\u3002", "author": "Sunbowen Lee et.al.", "authors": "Sunbowen Lee, Junting Zhou, Chang Ao, Kaige Li, Xinrun Du, Sirui He, Jiaheng Liu, Min Yang, Zhoufutu Wen, Shiwen Ni", "id": "2501.12619v1", "paper_url": "http://arxiv.org/abs/2501.12619v1", "repo": "https://github.com/aegis1863/llms-distillation-quantification"}}