{"2502.12064": {"publish_time": "2025-02-17", "title": "AI-generated Text Detection with a GLTR-based Approach", "paper_summary": "The rise of LLMs (Large Language Models) has contributed to the improved\nperformance and development of cutting-edge NLP applications. However, these\ncan also pose risks when used maliciously, such as spreading fake news, harmful\ncontent, impersonating individuals, or facilitating school plagiarism, among\nothers. This is because LLMs can generate high-quality texts, which are\nchallenging to differentiate from those written by humans. GLTR, which stands\nfor Giant Language Model Test Room and was developed jointly by the MIT-IBM\nWatson AI Lab and HarvardNLP, is a visual tool designed to help detect\nmachine-generated texts based on GPT-2, that highlights the words in text\ndepending on the probability that they were machine-generated. One limitation\nof GLTR is that the results it returns can sometimes be ambiguous and lead to\nconfusion. This study aims to explore various ways to improve GLTR's\neffectiveness for detecting AI-generated texts within the context of the\nIberLef-AuTexTification 2023 shared task, in both English and Spanish\nlanguages. Experiment results show that our GLTR-based GPT-2 model overcomes\nthe state-of-the-art models on the English dataset with a macro F1-score of\n80.19%, except for the first ranking model (80.91%). However, for the Spanish\ndataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%\ncompared to the top-performing model.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8208\u8d77\u6709\u52a9\u65bc\u6539\u9032\u5c16\u7aef NLP \u61c9\u7528\u7a0b\u5f0f\u7684\u6548\u80fd\u548c\u958b\u767c\u3002\u4e0d\u904e\uff0c\u9019\u4e9b\u61c9\u7528\u7a0b\u5f0f\u82e5\u906d\u60e1\u610f\u4f7f\u7528\uff0c\u4f8b\u5982\u6563\u5e03\u5047\u65b0\u805e\u3001\u6709\u5bb3\u5167\u5bb9\u3001\u5192\u5145\u500b\u4eba\u6216\u5354\u52a9\u5b78\u6821\u6284\u8972\u7b49\uff0c\u4e5f\u53ef\u80fd\u9020\u6210\u98a8\u96aa\u3002\u9019\u662f\u56e0\u70ba LLM \u53ef\u4ee5\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u6587\u5b57\uff0c\u800c\u9019\u4e9b\u6587\u5b57\u96e3\u4ee5\u8207\u4eba\u985e\u6240\u5beb\u7684\u6587\u5b57\u5340\u5206\u3002GLTR\uff08\u4ee3\u8868\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6e2c\u8a66\u5ba4\uff09\u662f\u7531\u9ebb\u7701\u7406\u5de5\u5b78\u9662-IBM Watson AI \u5be6\u9a57\u5ba4\u548c HarvardNLP \u5171\u540c\u958b\u767c\u7684\u8996\u89ba\u5de5\u5177\uff0c\u65e8\u5728\u5354\u52a9\u5075\u6e2c\u57fa\u65bc GPT-2 \u7684\u6a5f\u5668\u7522\u751f\u7684\u6587\u5b57\uff0c\u5b83\u6703\u6839\u64da\u6587\u5b57\u4e2d\u6bcf\u500b\u5b57\u8a5e\u6a5f\u5668\u7522\u751f\u7684\u6a5f\u7387\u4f86\u6a19\u793a\u3002GLTR \u7684\u4e00\u500b\u9650\u5236\u5728\u65bc\uff0c\u5b83\u56de\u50b3\u7684\u7d50\u679c\u6709\u6642\u53ef\u80fd\u6a21\u7a1c\u5169\u53ef\uff0c\u5bb9\u6613\u9020\u6210\u6df7\u6dc6\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8a0e\u5404\u7a2e\u65b9\u6cd5\u4f86\u6539\u5584 GLTR \u5728 IberLef-AuTexTification 2023 \u5171\u4eab\u4efb\u52d9\u4e2d\u5075\u6e2c AI \u751f\u6210\u7684\u6587\u5b57\u7684\u6548\u80fd\uff0c\u4efb\u52d9\u4e2d\u5305\u542b\u82f1\u6587\u548c\u897f\u73ed\u7259\u6587\u5169\u7a2e\u8a9e\u8a00\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u57fa\u65bc GLTR \u7684 GPT-2 \u6a21\u578b\u5728\u82f1\u6587\u8cc7\u6599\u96c6\u4e0a\u4ee5 80.19% \u7684\u5de8\u89c0 F1 \u5206\u6578\u8d85\u8d8a\u4e86\u6700\u5148\u9032\u7684\u6a21\u578b\uff0c\u50c5\u6b21\u65bc\u7b2c\u4e00\u540d\u6392\u540d\u6a21\u578b (80.91%)\u3002\u4e0d\u904e\uff0c\u5728\u897f\u73ed\u7259\u6587\u8cc7\u6599\u96c6\u4e0a\uff0c\u6211\u5011\u7372\u5f97\u7684\u5de8\u89c0 F1 \u5206\u6578\u70ba 66.20%\uff0c\u8207\u8868\u73fe\u6700\u4f73\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u76f8\u5dee 4.57%\u3002", "author": "Luc\u00eda Yan Wu et.al.", "authors": "Luc\u00eda Yan Wu, Isabel Segura-Bedmar", "id": "2502.12064v1", "paper_url": "http://arxiv.org/abs/2502.12064v1", "repo": "null"}}