{"2411.17304": {"publish_time": "2024-11-26", "title": "Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning", "paper_summary": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent.", "paper_summary_zh": "\u9019\u7bc7\u8ad6\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u7a31\u70ba\u300c\u96dc\u6e4a\u300d\uff0c\u5b83\u6d89\u53ca\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u4f7f\u7528\u96dc\u6e4a\u985e\u578b\u7684\u7121\u610f\u7fa9\u8b58\u5225\u78bc\u4f86\u906e\u853d\u6f5b\u5728\u6703\u5f15\u8d77\u504f\u898b\u7684\u5b57\u8a5e\uff0c\u4ee5\u6e1b\u5c11\u8a8d\u77e5\u504f\u898b\u548c\u5c0d\u5916\u90e8\u77e5\u8b58\u7684\u4f9d\u8cf4\u3002\u9019\u7a2e\u65b9\u6cd5\u5728\u4e09\u7d44\u5be6\u9a57\u4e2d\u9032\u884c\u4e86\u6e2c\u8a66\uff0c\u7e3d\u5171\u6d89\u53ca 490 \u500b\u63d0\u793a\u3002\u4f7f\u7528\u5361\u65b9\u6aa2\u5b9a\u7684\u7d71\u8a08\u5206\u6790\u986f\u793a\uff0c\u6240\u6709\u6e2c\u8a66\u60c5\u5883\u90fd\u6709\u986f\u8457\u7684\u6539\u5584\uff0c\u5176\u4e2d\u6db5\u84cb\u4e86 LLama\u3001ChatGPT\u3001Copilot\u3001Gemini \u548c Mixtral \u6a21\u578b\u3002\u5728\u7b2c\u4e00\u500b\u5be6\u9a57\u4e2d\uff0c\u96dc\u6e4a\u964d\u4f4e\u4e86\u300cLinda\u300d\u554f\u984c\u4fee\u6539\u7248\u672c\u4e2d\u7684\u8b2c\u8aa4\u7387\uff0c\u76ee\u7684\u662f\u8a55\u4f30\u5c0d\u8a8d\u77e5\u504f\u898b\u7684\u654f\u611f\u6027\u3002\u5728\u7b2c\u4e8c\u500b\u5be6\u9a57\u4e2d\uff0c\u5b83\u6539\u5584\u4e86 LLM \u5728\u983b\u7e41\u9805\u76ee\u96c6\u8403\u53d6\u4efb\u52d9\u4e2d\u7684\u7d50\u679c\u3002\u5728\u7b2c\u4e09\u500b\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u96dc\u6e4a\u5728\u4ee5\u8868\u683c\u683c\u5f0f\u800c\u975e\u6587\u5b57\u5448\u73fe Linda \u554f\u984c\u6642\u4e5f\u540c\u6a23\u6709\u6548\uff0c\u9019\u8868\u793a\u9019\u7a2e\u6280\u8853\u9069\u7528\u65bc\u5404\u7a2e\u8f38\u5165\u8868\u793a\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u9019\u7a2e\u65b9\u6cd5\u5df2\u88ab\u8b49\u660e\u53ef\u4ee5\u6539\u5584\u504f\u898b\u7684\u6e1b\u5c11\u548c\u5916\u90e8\u77e5\u8b58\u7684\u7d0d\u5165\u3002\u5118\u7ba1\u6e1b\u5c11\u4e86\u504f\u898b\uff0c\u4f46\u5e7b\u89ba\u7387\u5728\u4e0d\u540c\u985e\u578b\u7684 LLM \u6a21\u578b\u4e2d\u6e1b\u5c11\u7684\u60c5\u6cc1\u4e26\u4e0d\u4e00\u81f4\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\uff0c\u906e\u853d\u6703\u5f15\u8d77\u504f\u898b\u7684\u8853\u8a9e\u53ef\u4ee5\u6539\u5584 LLM \u7684\u6548\u80fd\uff0c\u5118\u7ba1\u5176\u6709\u6548\u6027\u53d6\u6c7a\u65bc\u6a21\u578b\u548c\u4efb\u52d9\u3002", "author": "Milena Chadimov\u00e1 et.al.", "authors": "Milena Chadimov\u00e1, Eduard Jur\u00e1\u0161ek, Tom\u00e1\u0161 Kliegr", "id": "2411.17304v1", "paper_url": "http://arxiv.org/abs/2411.17304v1", "repo": "null"}}