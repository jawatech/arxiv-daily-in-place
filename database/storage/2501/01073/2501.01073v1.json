{"2501.01073": {"publish_time": "2025-01-02", "title": "Graph Generative Pre-trained Transformer", "paper_summary": "Graph generation is a critical task in numerous domains, including molecular\ndesign and social network analysis, due to its ability to model complex\nrelationships and structured data. While most modern graph generative models\nutilize adjacency matrix representations, this work revisits an alternative\napproach that represents graphs as sequences of node set and edge set. We\nadvocate for this approach due to its efficient encoding of graphs and propose\na novel representation. Based on this representation, we introduce the Graph\nGenerative Pre-trained Transformer (G2PT), an auto-regressive model that learns\ngraph structures via next-token prediction. To further exploit G2PT's\ncapabilities as a general-purpose foundation model, we explore fine-tuning\nstrategies for two downstream applications: goal-oriented generation and graph\nproperty prediction. We conduct extensive experiments across multiple datasets.\nResults indicate that G2PT achieves superior generative performance on both\ngeneric graph and molecule datasets. Furthermore, G2PT exhibits strong\nadaptability and versatility in downstream tasks from molecular design to\nproperty prediction.", "paper_summary_zh": "\u5716\u5f62\u751f\u6210\u5728\u8a31\u591a\u9818\u57df\u4e2d\u662f\u4e00\u9805\u91cd\u8981\u7684\u4efb\u52d9\uff0c\u5305\u62ec\u5206\u5b50\u8a2d\u8a08\u548c\u793e\u4ea4\u7db2\u8def\u5206\u6790\uff0c\u56e0\u70ba\u5b83\u80fd\u5920\u5efa\u6a21\u8907\u96dc\u7684\u95dc\u4fc2\u548c\u7d50\u69cb\u5316\u8cc7\u6599\u3002\u96d6\u7136\u5927\u591a\u6578\u73fe\u4ee3\u5716\u5f62\u751f\u6210\u6a21\u578b\u4f7f\u7528\u9130\u63a5\u77e9\u9663\u8868\u793a\u6cd5\uff0c\u4f46\u672c\u7814\u7a76\u91cd\u65b0\u63a2\u8a0e\u4e86\u53e6\u4e00\u7a2e\u65b9\u6cd5\uff0c\u5c07\u5716\u5f62\u8868\u793a\u70ba\u7bc0\u9ede\u96c6\u548c\u908a\u7de3\u96c6\u7684\u5e8f\u5217\u3002\u6211\u5011\u63d0\u5021\u9019\u7a2e\u65b9\u6cd5\uff0c\u56e0\u70ba\u5b83\u80fd\u6709\u6548\u7de8\u78bc\u5716\u5f62\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u8868\u793a\u6cd5\u3002\u57fa\u65bc\u6b64\u8868\u793a\u6cd5\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5716\u5f62\u751f\u6210\u9810\u8a13\u7df4Transformer (G2PT)\uff0c\u9019\u662f\u4e00\u500b\u81ea\u8ff4\u6b78\u6a21\u578b\uff0c\u900f\u904e\u4e0b\u4e00\u500b\u6a19\u8a18\u9810\u6e2c\u4f86\u5b78\u7fd2\u5716\u5f62\u7d50\u69cb\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u5229\u7528 G2PT \u4f5c\u70ba\u901a\u7528\u57fa\u790e\u6a21\u578b\u7684\u80fd\u529b\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u91dd\u5c0d\u5169\u500b\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u9032\u884c\u5fae\u8abf\u7684\u7b56\u7565\uff1a\u76ee\u6a19\u5c0e\u5411\u751f\u6210\u548c\u5716\u5f62\u5c6c\u6027\u9810\u6e2c\u3002\u6211\u5011\u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u7d50\u679c\u8868\u660e\uff0cG2PT \u5728\u901a\u7528\u5716\u5f62\u548c\u5206\u5b50\u8cc7\u6599\u96c6\u4e0a\u90fd\u9054\u5230\u4e86\u512a\u7570\u7684\u751f\u6210\u6548\u80fd\u3002\u6b64\u5916\uff0cG2PT \u5728\u5f9e\u5206\u5b50\u8a2d\u8a08\u5230\u5c6c\u6027\u9810\u6e2c\u7684\u4e0b\u6e38\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u9069\u61c9\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "author": "Xiaohui Chen et.al.", "authors": "Xiaohui Chen, Yinkai Wang, Jiaxing He, Yuanqi Du, Soha Hassoun, Xiaolin Xu, Li-Ping Liu", "id": "2501.01073v1", "paper_url": "http://arxiv.org/abs/2501.01073v1", "repo": "null"}}