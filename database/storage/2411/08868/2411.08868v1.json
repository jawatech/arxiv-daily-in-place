{"2411.08868": {"publish_time": "2024-11-13", "title": "CamemBERT 2.0: A Smarter French Language Model Aged to Perfection", "paper_summary": "French language models, such as CamemBERT, have been widely adopted across\nindustries for natural language processing (NLP) tasks, with models like\nCamemBERT seeing over 4 million downloads per month. However, these models face\nchallenges due to temporal concept drift, where outdated training data leads to\na decline in performance, especially when encountering new topics and\nterminology. This issue emphasizes the need for updated models that reflect\ncurrent linguistic trends. In this paper, we introduce two new versions of the\nCamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these\nchallenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use\nof the Replaced Token Detection (RTD) objective for better contextual\nunderstanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked\nLanguage Modeling (MLM) objective. Both models are trained on a significantly\nlarger and more recent dataset with longer context length and an updated\ntokenizer that enhances tokenization performance for French. We evaluate the\nperformance of these models on both general-domain NLP tasks and\ndomain-specific applications, such as medical field tasks, demonstrating their\nversatility and effectiveness across a range of use cases. Our results show\nthat these updated models vastly outperform their predecessors, making them\nvaluable tools for modern NLP systems. All our new models, as well as\nintermediate checkpoints, are made openly available on Huggingface.", "paper_summary_zh": "\u6cd5\u8a9e\u8a9e\u8a00\u6a21\u578b\uff0c\u4f8b\u5982 CamemBERT\uff0c\u5df2\u5728\u5404\u7522\u696d\u5ee3\u6cdb\u63a1\u7528\u65bc\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\uff0c\u800c CamemBERT \u7b49\u6a21\u578b\u6bcf\u6708\u4e0b\u8f09\u91cf\u8d85\u904e 400 \u842c\u6b21\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u6703\u56e0\u6642\u9593\u6982\u5ff5\u6f02\u79fb\u800c\u9762\u81e8\u6311\u6230\uff0c\u904e\u6642\u7684\u8a13\u7df4\u8cc7\u6599\u6703\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\uff0c\u7279\u5225\u662f\u5728\u9047\u5230\u65b0\u4e3b\u984c\u548c\u8853\u8a9e\u6642\u3002\u6b64\u554f\u984c\u5f37\u8abf\u4e86\u9700\u8981\u66f4\u65b0\u7684\u6a21\u578b\u4f86\u53cd\u6620\u7576\u524d\u7684\u8a9e\u8a00\u8da8\u52e2\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 CamemBERT \u57fa\u790e\u6a21\u578b\u7684\u5169\u500b\u65b0\u7248\u672c\uff0cCamemBERTav2 \u548c CamemBERTv2\uff0c\u65e8\u5728\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\u3002CamemBERTav2 \u57fa\u65bc DeBERTaV3 \u67b6\u69cb\uff0c\u4e26\u5229\u7528\u66ff\u63db\u4ee3\u5e63\u5075\u6e2c (RTD) \u76ee\u6a19\uff0c\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u8108\u7d61\u7406\u89e3\uff0c\u800c CamemBERTv2 \u5247\u5efa\u69cb\u5728 RoBERTa \u4e0a\uff0c\u4f7f\u7528\u906e\u853d\u8a9e\u8a00\u6a21\u578b (MLM) \u76ee\u6a19\u3002\u9019\u5169\u500b\u6a21\u578b\u90fd\u8a13\u7df4\u65bc\u4e00\u500b\u986f\u8457\u66f4\u5927\u4e14\u66f4\u65b0\u7684\u8cc7\u6599\u96c6\uff0c\u5177\u6709\u8f03\u9577\u7684\u8108\u7d61\u9577\u5ea6\u548c\u4e00\u500b\u66f4\u65b0\u7684 tokenizer\uff0c\u53ef\u589e\u5f37\u6cd5\u8a9e\u7684 tokenization \u6548\u80fd\u3002\u6211\u5011\u8a55\u4f30\u4e86\u9019\u4e9b\u6a21\u578b\u5728\u4e00\u822c\u9818\u57df NLP \u4efb\u52d9\u548c\u7279\u5b9a\u9818\u57df\u61c9\u7528\u7a0b\u5f0f\uff08\u4f8b\u5982\u91ab\u5b78\u9818\u57df\u4efb\u52d9\uff09\u4e0a\u7684\u6548\u80fd\uff0c\u5c55\u793a\u4e86\u5b83\u5011\u5728\u5404\u7a2e\u4f7f\u7528\u6848\u4f8b\u4e2d\u7684\u591a\u529f\u80fd\u6027\u548c\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u9019\u4e9b\u66f4\u65b0\u7684\u6a21\u578b\u5927\u5e45\u512a\u65bc\u5b83\u5011\u7684\u524d\u8eab\uff0c\u4f7f\u5176\u6210\u70ba\u73fe\u4ee3 NLP \u7cfb\u7d71\u7684\u5bf6\u8cb4\u5de5\u5177\u3002\u6211\u5011\u6240\u6709\u7684\u65b0\u6a21\u578b\uff0c\u4ee5\u53ca\u4e2d\u9593\u6aa2\u67e5\u9ede\uff0c\u90fd\u516c\u958b\u5728 Huggingface \u4e0a\u3002", "author": "Wissam Antoun et.al.", "authors": "Wissam Antoun, Francis Kulumba, Rian Touchent, \u00c9ric de la Clergerie, Beno\u00eet Sagot, Djam\u00e9 Seddah", "id": "2411.08868v1", "paper_url": "http://arxiv.org/abs/2411.08868v1", "repo": "null"}}