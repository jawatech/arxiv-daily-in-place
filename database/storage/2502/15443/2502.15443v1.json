{"2502.15443": {"publish_time": "2025-02-21", "title": "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models", "paper_summary": "Large language models (LLMs) exhibit excellent performance in various tasks.\nHowever, the memory requirements of LLMs present a great challenge when\ndeploying on memory-limited devices, even for quantized LLMs. This paper\nintroduces a framework to compress LLM after quantization further, achieving\nabout 2.2x compression ratio. A compression-aware quantization is first\nproposed to enhance model weight compressibility by re-scaling the model\nparameters before quantization, followed by a pruning method to improve\nfurther. Upon this, we notice that decompression can be a bottleneck during\npractical scenarios. We then give a detailed analysis of the trade-off between\nmemory usage and latency brought by the proposed method. A speed-adaptive\nmethod is proposed to overcome it. The experimental results show inference with\nthe compressed model can achieve a 40% reduction in memory size with negligible\nloss in accuracy and inference speed.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\u3002\n\u7136\u800c\uff0cLLM \u7684\u8a18\u61b6\u9ad4\u9700\u6c42\u5728\u90e8\u7f72\u65bc\u8a18\u61b6\u9ad4\u53d7\u9650\u88dd\u7f6e\u6642\u69cb\u6210\u4e86\u4e00\u5927\u6311\u6230\uff0c\u5373\u4f7f\u662f\u7d93\u904e\u91cf\u5316\u7684 LLM \u4e5f\u662f\u5982\u6b64\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u500b\u67b6\u69cb\uff0c\u7528\u65bc\u9032\u4e00\u6b65\u58d3\u7e2e\u91cf\u5316\u5f8c\u7684 LLM\uff0c\u4e26\u9054\u5230\u4e86\u7d04 2.2 \u500d\u7684\u58d3\u7e2e\u6bd4\u3002\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u7a2e\u611f\u77e5\u58d3\u7e2e\u7684\u91cf\u5316\uff0c\u900f\u904e\u5728\u91cf\u5316\u524d\u91cd\u65b0\u8abf\u6574\u6a21\u578b\u53c3\u6578\u4f86\u589e\u5f37\u6a21\u578b\u6b0a\u91cd\u7684\u53ef\u58d3\u7e2e\u6027\uff0c\u7136\u5f8c\u518d\u63a1\u7528\u4e00\u7a2e\u526a\u679d\u65b9\u6cd5\u4f86\u9032\u4e00\u6b65\u6539\u5584\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u6ce8\u610f\u5230\u5728\u5be6\u969b\u5834\u666f\u4e2d\uff0c\u89e3\u58d3\u7e2e\u53ef\u80fd\u6703\u6210\u70ba\u74f6\u9838\u3002\u63a5\u8457\uff0c\u6211\u5011\u8a73\u7d30\u5206\u6790\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u548c\u5ef6\u9072\u4e4b\u9593\u7684\u6b0a\u8861\u3002\u63d0\u51fa\u4e86\u4e00\u7a2e\u901f\u5ea6\u81ea\u9069\u61c9\u65b9\u6cd5\u4f86\u514b\u670d\u9019\u500b\u554f\u984c\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u4f7f\u7528\u58d3\u7e2e\u6a21\u578b\u9032\u884c\u63a8\u8ad6\u53ef\u4ee5\u5c07\u8a18\u61b6\u9ad4\u5927\u5c0f\u6e1b\u5c11 40%\uff0c\u800c\u6e96\u78ba\u5ea6\u548c\u63a8\u8ad6\u901f\u5ea6\u7684\u640d\u5931\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\u3002", "author": "Weilan Wang et.al.", "authors": "Weilan Wang, Yu Mao, Dongdong Tang, Hongchao Du, Nan Guan, Chun Jason Xue", "id": "2502.15443v1", "paper_url": "http://arxiv.org/abs/2502.15443v1", "repo": "null"}}