{"2410.01504": {"publish_time": "2024-10-02", "title": "PersonaMath: Enhancing Math Reasoning through Persona-Driven Data Augmentation", "paper_summary": "While closed-source Large Language Models (LLMs) demonstrate strong\nmathematical problem-solving abilities, open-source models continue to struggle\nwith such tasks. To bridge this gap, we propose a data augmentation approach\nand introduce PersonaMathQA, a dataset derived from MATH and GSM8K, on which we\ntrain the PersonaMath models. Our approach consists of two stages: the first\nstage is learning from Persona Diversification, and the second stage is\nlearning from Reflection. In the first stage, we regenerate detailed\nchain-of-thought (CoT) solutions as instructions using a closed-source LLM and\nintroduce a novel persona-driven data augmentation technique to enhance the\ndataset's quantity and diversity. In the second stage, we incorporate\nreflection to fully leverage more challenging and valuable questions.\nEvaluation of our PersonaMath models on MATH and GSM8K reveals that the\nPersonaMath-7B model (based on LLaMA-2-7B) achieves an accuracy of 24.2% on\nMATH and 68.7% on GSM8K, surpassing all baseline methods and achieving\nstate-of-the-art performance. Notably, our dataset contains only 70.3K data\npoints-merely 17.8% of MetaMathQA and 27% of MathInstruct-yet our model\noutperforms these baselines, demonstrating the high quality and diversity of\nour dataset, which enables more efficient model training. We open-source the\nPersonaMathQA dataset, PersonaMath models, and our code for public usage.", "paper_summary_zh": "<paragraph>\u96d6\u7136\u5c01\u9589\u539f\u59cb\u78bc\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u4e86\u5f37\u5927\u7684\u6578\u5b78\u554f\u984c\u89e3\u6c7a\u80fd\u529b\uff0c\u4f46\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u5728\u8655\u7406\u6b64\u985e\u4efb\u52d9\u6642\u4ecd\u9762\u81e8\u6311\u6230\u3002\u70ba\u4e86\u5f4c\u5408\u6b64\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u8cc7\u6599\u64f4\u5145\u65b9\u6cd5\uff0c\u4e26\u5f15\u5165\u4e86 PersonaMathQA\uff0c\u9019\u662f\u4e00\u500b\u6e90\u81ea MATH \u548c GSM8K \u7684\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u5728\u4e0a\u9762\u8a13\u7df4 PersonaMath \u6a21\u578b\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u542b\u5169\u500b\u968e\u6bb5\uff1a\u7b2c\u4e00\u968e\u6bb5\u662f\u5f9e\u89d2\u8272\u591a\u6a23\u5316\u4e2d\u5b78\u7fd2\uff0c\u7b2c\u4e8c\u968e\u6bb5\u662f\u5f9e\u53cd\u601d\u4e2d\u5b78\u7fd2\u3002\u5728\u7b2c\u4e00\u968e\u6bb5\uff0c\u6211\u5011\u4f7f\u7528\u5c01\u9589\u539f\u59cb\u78bc LLM \u5c07\u8a73\u7d30\u7684\u601d\u8003\u93c8 (CoT) \u89e3\u6c7a\u65b9\u6848\u91cd\u65b0\u751f\u6210\u70ba\u6307\u4ee4\uff0c\u4e26\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u89d2\u8272\u9a45\u52d5\u8cc7\u6599\u64f4\u5145\u6280\u8853\u4f86\u63d0\u5347\u8cc7\u6599\u96c6\u7684\u6578\u91cf\u548c\u591a\u6a23\u6027\u3002\u5728\u7b2c\u4e8c\u968e\u6bb5\uff0c\u6211\u5011\u7d50\u5408\u4e86\u53cd\u601d\uff0c\u4ee5\u5145\u5206\u5229\u7528\u66f4\u5177\u6311\u6230\u6027\u548c\u50f9\u503c\u6027\u7684\u554f\u984c\u3002\u5728 MATH \u548c GSM8K \u4e0a\u5c0d\u6211\u5011\u7684 PersonaMath \u6a21\u578b\u9032\u884c\u8a55\u4f30\uff0c\u7d50\u679c\u986f\u793a PersonaMath-7B \u6a21\u578b\uff08\u57fa\u65bc LLaMA-2-7B\uff09\u5728 MATH \u4e0a\u9054\u5230\u4e86 24.2% \u7684\u6e96\u78ba\u5ea6\uff0c\u5728 GSM8K \u4e0a\u9054\u5230\u4e86 68.7%\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u57fa\u6e96\u65b9\u6cd5\uff0c\u4e26\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u8cc7\u6599\u96c6\u50c5\u5305\u542b 70.3K \u500b\u8cc7\u6599\u9ede\uff0c\u50c5\u70ba MetaMathQA \u7684 17.8% \u548c MathInstruct \u7684 27%\uff0c\u4f46\u6211\u5011\u7684\u6a21\u578b\u537b\u512a\u65bc\u9019\u4e9b\u57fa\u6e96\uff0c\u8b49\u660e\u4e86\u6211\u5011\u8cc7\u6599\u96c6\u7684\u9ad8\u54c1\u8cea\u548c\u591a\u6a23\u6027\uff0c\u9019\u4f7f\u5f97\u6a21\u578b\u8a13\u7df4\u66f4\u6709\u6548\u7387\u3002\u6211\u5011\u958b\u653e\u539f\u59cb\u78bc\u4e86 PersonaMathQA \u8cc7\u6599\u96c6\u3001PersonaMath \u6a21\u578b\u548c\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\uff0c\u4f9b\u516c\u773e\u4f7f\u7528\u3002</paragraph>", "author": "Jing Luo et.al.", "authors": "Jing Luo, Run Luo, Longze Chen, Liang Zhu, Chang Ao, Jiaming Li, Yukun Chen, Xin Cheng, Wen Yang, Jiayuan Su, Chengming Li, Min Yang", "id": "2410.01504v1", "paper_url": "http://arxiv.org/abs/2410.01504v1", "repo": "null"}}