{"2412.12089": {"publish_time": "2024-12-16", "title": "Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation", "paper_summary": "Recent advances in GPU-based parallel simulation have enabled practitioners\nto collect large amounts of data and train complex control policies using deep\nreinforcement learning (RL), on commodity GPUs. However, such successes for RL\nin robotics have been limited to tasks sufficiently simulated by fast\nrigid-body dynamics. Simulation techniques for soft bodies are comparatively\nseveral orders of magnitude slower, thereby limiting the use of RL due to\nsample complexity requirements. To address this challenge, this paper presents\nboth a novel RL algorithm and a simulation platform to enable scaling RL on\ntasks involving rigid bodies and deformables. We introduce Soft Analytic Policy\nOptimization (SAPO), a maximum entropy first-order model-based actor-critic RL\nalgorithm, which uses first-order analytic gradients from differentiable\nsimulation to train a stochastic actor to maximize expected return and entropy.\nAlongside our approach, we develop Rewarped, a parallel differentiable\nmultiphysics simulation platform that supports simulating various materials\nbeyond rigid bodies. We re-implement challenging manipulation and locomotion\ntasks in Rewarped, and show that SAPO outperforms baselines over a range of\ntasks that involve interaction between rigid bodies, articulations, and\ndeformables.", "paper_summary_zh": "\u6700\u8fd1\u5728\u57fa\u65bc GPU \u7684\u5e73\u884c\u6a21\u64ec\u65b9\u9762\u53d6\u5f97\u7684\u9032\u5c55\u4f7f\u5f9e\u696d\u8005\n\u80fd\u5920\u6536\u96c6\u5927\u91cf\u7684\u8cc7\u6599\uff0c\u4e26\u4f7f\u7528\u6df1\u5ea6\n\u5f37\u5316\u5b78\u7fd2 (RL) \u5728\u5546\u54c1 GPU \u4e0a\u8a13\u7df4\u8907\u96dc\u7684\u63a7\u5236\u7b56\u7565\u3002\u7136\u800c\uff0cRL\n\u5728\u6a5f\u5668\u4eba\u6280\u8853\u65b9\u9762\u7684\u6210\u529f\u50c5\u9650\u65bc\u7531\u5feb\u901f\n\u525b\u9ad4\u52d5\u529b\u5b78\u5145\u5206\u6a21\u64ec\u7684\u4efb\u52d9\u3002\u8edf\u9ad4\u7684\u6a21\u64ec\u6280\u8853\u6bd4\u8f03\n\u6162\u4e86\u5e7e\u500b\u6578\u91cf\u7d1a\uff0c\u5f9e\u800c\u7531\u65bc\u6a23\u672c\u8907\u96dc\u6027\u8981\u6c42\u800c\u9650\u5236\u4e86 RL \u7684\u4f7f\u7528\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u672c\u6587\u63d0\u51fa\n\u4e86\u4e00\u7a2e\u65b0\u7684 RL \u6f14\u7b97\u6cd5\u548c\u4e00\u500b\u6a21\u64ec\u5e73\u53f0\uff0c\u4ee5\u4fbf\u5728\u6d89\u53ca\u525b\u9ad4\u548c\u53ef\u8b8a\u5f62\u7684\u4efb\u52d9\u4e2d\u64f4\u5c55 RL\u3002\u6211\u5011\u4ecb\u7d39\u4e86\u8edf\u5206\u6790\u7b56\u7565\n\u512a\u5316 (SAPO)\uff0c\u9019\u662f\u4e00\u7a2e\u6700\u5927\u71b5\u4e00\u968e\u57fa\u65bc\u6a21\u578b\u7684\u52d5\u4f5c-\u8a55\u8ad6 RL\n\u6f14\u7b97\u6cd5\uff0c\u5b83\u4f7f\u7528\u53ef\u5fae\u5206\n\u6a21\u64ec\u7684\u4e00\u968e\u5206\u6790\u68af\u5ea6\u4f86\u8a13\u7df4\u96a8\u6a5f\u52d5\u4f5c\u4ee5\u6700\u5927\u5316\u9810\u671f\u56de\u5831\u548c\u71b5\u3002\u9664\u4e86\u6211\u5011\u7684\u505a\u6cd5\u4e4b\u5916\uff0c\u6211\u5011\u9084\u958b\u767c\u4e86 Rewarped\uff0c\u9019\u662f\u4e00\u500b\u4e26\u884c\u7684\u53ef\u5fae\u5206\n\u591a\u7269\u7406\u5834\u6a21\u64ec\u5e73\u53f0\uff0c\u5b83\u652f\u63f4\u6a21\u64ec\u5404\u7a2e\u8d85\u4e4e\u525b\u9ad4\u7684\u6750\u6599\u3002\u6211\u5011\u5728 Rewarped \u4e2d\u91cd\u65b0\u5be6\u4f5c\u4e86\u5177\u6709\u6311\u6230\u6027\u7684\u64cd\u4f5c\u548c\u904b\u52d5\u4efb\u52d9\uff0c\u4e26\u8868\u660e SAPO \u5728\u6d89\u53ca\u525b\u9ad4\u3001\u95dc\u7bc0\u548c\n\u53ef\u8b8a\u5f62\u4e4b\u9593\u4ea4\u4e92\u4f5c\u7528\u7684\u4e00\u7cfb\u5217\u4efb\u52d9\u4e2d\u512a\u65bc\u57fa\u6e96\u3002", "author": "Eliot Xing et.al.", "authors": "Eliot Xing, Vernon Luk, Jean Oh", "id": "2412.12089v1", "paper_url": "http://arxiv.org/abs/2412.12089v1", "repo": "null"}}