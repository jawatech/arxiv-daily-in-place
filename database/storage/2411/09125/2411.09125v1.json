{"2411.09125": {"publish_time": "2024-11-14", "title": "DROJ: A Prompt-Driven Attack against Large Language Models", "paper_summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Due to their training on\ninternet-sourced datasets, LLMs can sometimes generate objectionable content,\nnecessitating extensive alignment with human feedback to avoid such outputs.\nDespite massive alignment efforts, LLMs remain susceptible to adversarial\njailbreak attacks, which usually are manipulated prompts designed to circumvent\nsafety mechanisms and elicit harmful responses. Here, we introduce a novel\napproach, Directed Rrepresentation Optimization Jailbreak (DROJ), which\noptimizes jailbreak prompts at the embedding level to shift the hidden\nrepresentations of harmful queries towards directions that are more likely to\nelicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat\nmodel show that DROJ achieves a 100\\% keyword-based Attack Success Rate (ASR),\neffectively preventing direct refusals. However, the model occasionally\nproduces repetitive and non-informative responses. To mitigate this, we\nintroduce a helpfulness system prompt that enhances the utility of the model's\nresponses. Our code is available at\nhttps://github.com/Leon-Leyang/LLM-Safeguard.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7531\u65bc\u5728\u7db2\u969b\u7db2\u8def\u4f86\u6e90\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a13\u7df4\uff0cLLM \u6709\u6642\u6703\u7522\u751f\u4ee4\u4eba\u53cd\u611f\u7684\u5167\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u5927\u91cf\u6bd4\u5c0d\u4eba\u985e\u56de\u994b\uff0c\u4ee5\u907f\u514d\u6b64\u985e\u8f38\u51fa\u3002\u5118\u7ba1\u9032\u884c\u4e86\u5927\u91cf\u6bd4\u5c0d\u5de5\u4f5c\uff0cLLM \u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5c0d\u6297\u6027\u8d8a\u7344\u653b\u64ca\uff0c\u9019\u901a\u5e38\u662f\u70ba\u4e86\u898f\u907f\u5b89\u5168\u6a5f\u5236\u548c\u5f15\u767c\u6709\u5bb3\u56de\u61c9\u800c\u8a2d\u8a08\u7684\u64cd\u7e31\u63d0\u793a\u3002\u5728\u6b64\uff0c\u6211\u5011\u4ecb\u7d39\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u5373\u5b9a\u5411\u8868\u793a\u6700\u4f73\u5316\u8d8a\u7344 (DROJ)\uff0c\u5b83\u5728\u5d4c\u5165\u5c64\u6700\u4f73\u5316\u8d8a\u7344\u63d0\u793a\uff0c\u4ee5\u5c07\u6709\u5bb3\u67e5\u8a62\u7684\u96b1\u85cf\u8868\u793a\u8f49\u79fb\u5230\u66f4\u53ef\u80fd\u5f9e\u6a21\u578b\u5f15\u767c\u80af\u5b9a\u56de\u61c9\u7684\u65b9\u5411\u3002\u6211\u5011\u5c0d LLaMA-2-7b-chat \u6a21\u578b\u7684\u8a55\u4f30\u986f\u793a\uff0cDROJ \u9054\u5230\u4e86 100% \u57fa\u65bc\u95dc\u9375\u5b57\u7684\u653b\u64ca\u6210\u529f\u7387 (ASR)\uff0c\u6709\u6548\u9632\u6b62\u76f4\u63a5\u62d2\u7d55\u3002\u7136\u800c\uff0c\u8a72\u6a21\u578b\u5076\u723e\u6703\u7522\u751f\u91cd\u8907\u4e14\u7121\u610f\u7fa9\u7684\u56de\u61c9\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6709\u52a9\u76ca\u7684\u7cfb\u7d71\u63d0\u793a\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u56de\u61c9\u7684\u6548\u7528\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/Leon-Leyang/LLM-Safeguard \u53d6\u5f97\u3002", "author": "Leyang Hu et.al.", "authors": "Leyang Hu, Boran Wang", "id": "2411.09125v1", "paper_url": "http://arxiv.org/abs/2411.09125v1", "repo": "https://github.com/leon-leyang/llm-safeguard"}}