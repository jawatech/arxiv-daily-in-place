{"2407.07880": {"publish_time": "2024-07-10", "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization", "paper_summary": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.", "paper_summary_zh": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO) \u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u566a\u58f0\u6311\u6218\uff0cDPO \u662f\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e0e\u4eba\u7c7b\u504f\u597d\u76f8\u5339\u914d\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u5c06\u566a\u58f0\u5206\u4e3a\u9010\u70b9\u566a\u58f0\uff0c\u5176\u4e2d\u5305\u62ec\u4f4e\u8d28\u91cf\u6570\u636e\u70b9\uff0c\u4ee5\u53ca\u6210\u5bf9\u566a\u58f0\uff0c\u5176\u4e2d\u5305\u62ec\u5f71\u54cd\u504f\u597d\u6392\u540d\u7684\u9519\u8bef\u6570\u636e\u5bf9\u5173\u8054\u3002\u5229\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316 (DRO)\uff0c\u6211\u4eec\u589e\u5f3a\u4e86 DPO \u5bf9\u8fd9\u4e9b\u7c7b\u578b\u566a\u58f0\u7684\u5f39\u6027\u3002\u6211\u4eec\u7684\u7406\u8bba\u89c1\u89e3\u8868\u660e\uff0cDPO \u672c\u8d28\u4e0a\u5d4c\u5165\u4e86 DRO \u539f\u5219\uff0c\u8d4b\u4e88\u4e86\u5bf9\u9010\u70b9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5176\u4e2d\u6b63\u5219\u5316\u7cfb\u6570 $\\beta$ \u5728\u5176\u6297\u566a\u58f0\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u6269\u5c55\u6b64\u6846\u67b6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5206\u5e03\u9c81\u68d2\u5316 DPO (Dr. DPO)\uff0c\u5b83\u901a\u8fc7\u9488\u5bf9\u6700\u574f\u60c5\u51b5\u6210\u5bf9\u573a\u666f\u8fdb\u884c\u4f18\u5316\u6765\u96c6\u6210\u6210\u5bf9\u9c81\u68d2\u6027\u3002Dr. DPO \u4e2d\u7684\u65b0\u578b\u8d85\u53c2\u6570 $\\beta'$ \u5141\u8bb8\u5bf9\u6570\u636e\u5bf9\u53ef\u9760\u6027\u8fdb\u884c\u5fae\u8c03\u63a7\u5236\uff0c\u5728\u5608\u6742\u7684\u8bad\u7ec3\u73af\u5883\u4e2d\u63d0\u4f9b\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u95f4\u7684\u6218\u7565\u5e73\u8861\u3002\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cDr. DPO \u5728\u9996\u9009\u6570\u636e\u96c6\u4e2d\u7684\u751f\u6210\u6587\u672c\u8d28\u91cf\u548c\u54cd\u5e94\u51c6\u786e\u6027\u65b9\u9762\u6709\u4e86\u5b9e\u8d28\u6027\u7684\u63d0\u9ad8\uff0c\u5728\u6709\u566a\u58f0\u548c\u65e0\u566a\u58f0\u8bbe\u7f6e\u4e2d\u90fd\u5c55\u793a\u4e86\u589e\u5f3a\u7684\u6027\u80fd\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/junkangwu/Dr_DPO \u83b7\u5f97\u3002", "author": "Junkang Wu et.al.", "authors": "Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jiawei Chen, Jinyang Gao, Bolin Ding, Xiang Wang, Xiangnan He", "id": "2407.07880v1", "paper_url": "http://arxiv.org/abs/2407.07880v1", "repo": "https://github.com/junkangwu/dr_dpo"}}