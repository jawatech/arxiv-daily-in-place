{"2412.17395": {"publish_time": "2024-12-23", "title": "WarriorCoder: Learning from Expert Battles to Augment Code Large Language Models", "paper_summary": "Despite recent progress achieved by code large language models (LLMs), their\nremarkable abilities are largely dependent on fine-tuning on the high-quality\ndata, posing challenges for data collection and annotation. To address this,\ncurrent methods often design various data flywheels to gather complex code\ninstructions, enabling models to handle more intricate tasks. However, these\napproaches typically rely on off-the-shelf datasets and data augmentation from\nthe limited pool of proprietary LLMs (e.g., Claude, GPT4, and so on), which\nlimits the diversity of the constructed data and makes it prone to systemic\nbiases. In this paper, we propose WarriorCoder which learns from expert battles\nto address these limitations. Specifically, we create an arena for current\nexpert code LLMs, where each model challenges and responds to others'\nchallenges, with evaluations conducted by uninvolved judge models. This\ncompetitive framework generates novel training data constructed from scratch,\nharnessing the strengths of all participants. Experimental results demonstrate\nthat WarriorCoder achieves competitive performance compared to previous\nmethods, even without relying on proprietary LLMs.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7a0b\u5f0f\u78bc\u6700\u8fd1\u53d6\u5f97\u9032\u5c55\uff0c\u4f46\u5b83\u5011\u5353\u8d8a\u7684\u80fd\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8cf4\u65bc\u5c0d\u9ad8\u54c1\u8cea\u8cc7\u6599\u9032\u884c\u5fae\u8abf\uff0c\u9019\u5c0d\u8cc7\u6599\u6536\u96c6\u548c\u8a3b\u89e3\u69cb\u6210\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u76ee\u524d\u7684\u5404\u7a2e\u65b9\u6cd5\u901a\u5e38\u6703\u8a2d\u8a08\u5404\u7a2e\u8cc7\u6599\u98db\u8f2a\u4f86\u6536\u96c6\u8907\u96dc\u7684\u7a0b\u5f0f\u78bc\u6307\u4ee4\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u8655\u7406\u66f4\u8907\u96dc\u7684\u4efb\u52d9\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u65bc\u73fe\u6210\u7684\u8cc7\u6599\u96c6\u548c\u4f86\u81ea\u6709\u9650\u5c08\u6709 LLM\uff08\u4f8b\u5982 Claude\u3001GPT4 \u7b49\uff09\u7684\u8cc7\u6599\u64f4\u5145\uff0c\u9019\u9650\u5236\u4e86\u5efa\u69cb\u8cc7\u6599\u7684\u591a\u6a23\u6027\uff0c\u4e26\u4f7f\u5176\u5bb9\u6613\u7522\u751f\u7cfb\u7d71\u6027\u504f\u5dee\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa WarriorCoder\uff0c\u5b83\u5f9e\u5c08\u5bb6\u6230\u9b25\u4e2d\u5b78\u7fd2\u4ee5\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u70ba\u76ee\u524d\u7684\u5c08\u5bb6\u7a0b\u5f0f\u78bc LLM \u5efa\u7acb\u4e86\u4e00\u500b\u7af6\u6280\u5834\uff0c\u6bcf\u500b\u6a21\u578b\u90fd\u6703\u6311\u6230\u4e26\u56de\u61c9\u5176\u4ed6\u6a21\u578b\u7684\u6311\u6230\uff0c\u4e26\u7531\u672a\u53c3\u8207\u7684\u8a55\u5be9\u6a21\u578b\u9032\u884c\u8a55\u4f30\u3002\u9019\u500b\u7af6\u722d\u6846\u67b6\u6703\u7522\u751f\u5f9e\u982d\u5efa\u69cb\u7684\u65b0\u8a13\u7df4\u8cc7\u6599\uff0c\u5229\u7528\u6240\u6709\u53c3\u8207\u8005\u7684\u512a\u52e2\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5373\u4f7f\u4e0d\u4f9d\u8cf4\u5c08\u6709 LLM\uff0cWarriorCoder \u4e5f\u80fd\u9054\u5230\u8207\u5148\u524d\u65b9\u6cd5\u76f8\u7576\u7684\u7af6\u722d\u529b\u3002", "author": "Huawen Feng et.al.", "authors": "Huawen Feng, Pu Zhao, Qingfeng Sun, Can Xu, Fangkai Yang, Lu Wang, Qianli Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang", "id": "2412.17395v1", "paper_url": "http://arxiv.org/abs/2412.17395v1", "repo": "null"}}