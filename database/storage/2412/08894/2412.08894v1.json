{"2412.08894": {"publish_time": "2024-12-12", "title": "SMMF: Square-Matricized Momentum Factorization for Memory-Efficient Optimization", "paper_summary": "We propose SMMF (Square-Matricized Momentum Factorization), a\nmemory-efficient optimizer that reduces the memory requirement of the widely\nused adaptive learning rate optimizers, such as Adam, by up to 96%. SMMF\nenables flexible and efficient factorization of an arbitrary rank (shape) of\nthe first and second momentum tensors during optimization, based on the\nproposed square-matricization and one-time single matrix factorization. From\nthis, it becomes effectively applicable to any rank (shape) of momentum\ntensors, i.e., bias, matrix, and any rank-d tensors, prevalent in various deep\nmodel architectures, such as CNNs (high rank) and Transformers (low rank), in\ncontrast to existing memory-efficient optimizers that applies only to a\nparticular (rank-2) momentum tensor, e.g., linear layers. We conduct a regret\nbound analysis of SMMF, which shows that it converges similarly to\nnon-memory-efficient adaptive learning rate optimizers, such as AdamNC,\nproviding a theoretical basis for its competitive optimization capability. In\nour experiment, SMMF takes up to 96% less memory compared to state-of-the-art\nmemory efficient optimizers, e.g., Adafactor, CAME, and SM3, while achieving\ncomparable model performance on various CNN and Transformer tasks.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa SMMF\uff08\u65b9\u9663\u52d5\u91cf\u5206\u89e3\uff09\uff0c\u4e00\u7a2e\u8a18\u61b6\u9ad4\u6548\u7387\u6700\u4f73\u5316\u5668\uff0c\u5b83\u53ef\u5c07\u5ee3\u6cdb\u4f7f\u7528\u7684\u81ea\u9069\u61c9\u5b78\u7fd2\u7387\u6700\u4f73\u5316\u5668\uff08\u4f8b\u5982 Adam\uff09\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\u964d\u4f4e\u591a\u9054 96%\u3002SMMF \u80fd\u5920\u5728\u6700\u4f73\u5316\u904e\u7a0b\u4e2d\u9748\u6d3b\u4e14\u6709\u6548\u5730\u5c0d\u4efb\u610f\u79e9\uff08\u5f62\u72c0\uff09\u7684\u7b2c\u4e00\u548c\u7b2c\u4e8c\u52d5\u91cf\u5f35\u91cf\u9032\u884c\u5206\u89e3\uff0c\u9019\u57fa\u65bc\u6240\u63d0\u51fa\u7684\u65b9\u9663\u5316\u548c\u4e00\u6b21\u6027\u55ae\u77e9\u9663\u5206\u89e3\u3002\u56e0\u6b64\uff0c\u5b83\u53ef\u6709\u6548\u5730\u61c9\u7528\u65bc\u4efb\u4f55\u79e9\uff08\u5f62\u72c0\uff09\u7684\u52d5\u91cf\u5f35\u91cf\uff0c\u5373\u504f\u5dee\u3001\u77e9\u9663\u548c\u4efb\u4f55\u79e9 d \u5f35\u91cf\uff0c\u9019\u4e9b\u5f35\u91cf\u5728\u5404\u7a2e\u6df1\u5ea6\u6a21\u578b\u67b6\u69cb\u4e2d\u5f88\u666e\u904d\uff0c\u4f8b\u5982 CNN\uff08\u9ad8\u79e9\uff09\u548c Transformer\uff08\u4f4e\u79e9\uff09\uff0c\u9019\u8207\u73fe\u6709\u7684\u8a18\u61b6\u9ad4\u6548\u7387\u6700\u4f73\u5316\u5668\u5f62\u6210\u5c0d\u6bd4\uff0c\u5f8c\u8005\u50c5\u9069\u7528\u65bc\u7279\u5b9a\uff08\u79e9 2\uff09\u52d5\u91cf\u5f35\u91cf\uff0c\u4f8b\u5982\u7dda\u6027\u5c64\u3002\u6211\u5011\u5c0d SMMF \u9032\u884c\u4e86\u907a\u61be\u908a\u754c\u5206\u6790\uff0c\u9019\u8868\u660e\u5b83\u7684\u6536\u6582\u65b9\u5f0f\u8207\u975e\u8a18\u61b6\u9ad4\u6548\u7387\u81ea\u9069\u61c9\u5b78\u7fd2\u7387\u6700\u4f73\u5316\u5668\uff08\u4f8b\u5982 AdamNC\uff09\u985e\u4f3c\uff0c\u70ba\u5176\u7af6\u722d\u6027\u7684\u6700\u4f73\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8ad6\u57fa\u790e\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0c\u8207\u6700\u5148\u9032\u7684\u8a18\u61b6\u9ad4\u6548\u7387\u6700\u4f73\u5316\u5668\uff08\u4f8b\u5982 Adafactor\u3001CAME \u548c SM3\uff09\u76f8\u6bd4\uff0cSMMF \u4f54\u7528\u7684\u8a18\u61b6\u9ad4\u6700\u591a\u6e1b\u5c11 96%\uff0c\u540c\u6642\u5728\u5404\u7a2e CNN \u548c Transformer \u4efb\u52d9\u4e0a\u5be6\u73fe\u4e86\u76f8\u7576\u7684\u6a21\u578b\u6548\u80fd\u3002", "author": "Kwangryeol Park et.al.", "authors": "Kwangryeol Park, Seulki Lee", "id": "2412.08894v1", "paper_url": "http://arxiv.org/abs/2412.08894v1", "repo": "null"}}