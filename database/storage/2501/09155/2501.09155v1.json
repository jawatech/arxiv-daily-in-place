{"2501.09155": {"publish_time": "2025-01-15", "title": "VCRScore: Image captioning metric based on V\\&L Transformers, CLIP, and precision-recall", "paper_summary": "Image captioning has become an essential Vision & Language research task. It\nis about predicting the most accurate caption given a specific image or video.\nThe research community has achieved impressive results by continuously\nproposing new models and approaches to improve the overall model's performance.\nNevertheless, despite increasing proposals, the performance metrics used to\nmeasure their advances have remained practically untouched through the years. A\nprobe of that, nowadays metrics like BLEU, METEOR, CIDEr, and ROUGE are still\nvery used, aside from more sophisticated metrics such as BertScore and\nClipScore.\n  Hence, it is essential to adjust how are measure the advances, limitations,\nand scopes of the new image captioning proposals, as well as to adapt new\nmetrics to these new advanced image captioning approaches.\n  This work proposes a new evaluation metric for the image captioning problem.\nTo do that, first, it was generated a human-labeled dataset to assess to which\ndegree the captions correlate with the image's content. Taking these human\nscores as ground truth, we propose a new metric, and compare it with several\nwell-known metrics, from classical to newer ones. Outperformed results were\nalso found, and interesting insights were presented and discussed.", "paper_summary_zh": "\u5f71\u50cf\u6a19\u984c\u5df2\u6210\u70ba\u8996\u89ba\u8207\u8a9e\u8a00\u7814\u7a76\u7684\u91cd\u8981\u4efb\u52d9\u3002\u76ee\u7684\u662f\u6839\u64da\u7279\u5b9a\u5f71\u50cf\u6216\u5f71\u7247\u9810\u6e2c\u6700\u7cbe\u78ba\u7684\u6a19\u984c\u3002\u7814\u7a76\u793e\u7fa4\u6301\u7e8c\u63d0\u51fa\u65b0\u7684\u6a21\u578b\u548c\u65b9\u6cd5\u4f86\u63d0\u5347\u6574\u9ad4\u6a21\u578b\u7684\u6548\u80fd\uff0c\u5df2\u7372\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\u3002\u5118\u7ba1\u63d0\u51fa\u8d8a\u4f86\u8d8a\u591a\u7684\u5efa\u8b70\uff0c\u4f46\u591a\u5e74\u4f86\u7528\u65bc\u8861\u91cf\u5176\u9032\u5c55\u7684\u6548\u80fd\u6307\u6a19\u5be6\u969b\u4e0a\u4e26\u672a\u66f4\u52d5\u3002\u5176\u4e2d\u4e00\u500b\u63a2\u8a0e\u662f\uff0c\u5982\u4eca\u50cf BLEU\u3001METEOR\u3001CIDEr \u548c ROUGE \u7b49\u6307\u6a19\u4ecd\u5ee3\u6cdb\u4f7f\u7528\uff0c\u9664\u4e86 BertScore \u548c ClipScore \u7b49\u66f4\u7cbe\u5bc6\u7684\u6307\u6a19\u3002\u56e0\u6b64\uff0c\u5fc5\u9808\u8abf\u6574\u6211\u5011\u5982\u4f55\u8861\u91cf\u65b0\u5f71\u50cf\u6a19\u984c\u5efa\u8b70\u7684\u9032\u5c55\u3001\u9650\u5236\u548c\u7bc4\u570d\uff0c\u4ee5\u53ca\u8abf\u6574\u65b0\u7684\u6307\u6a19\u4ee5\u9069\u61c9\u9019\u4e9b\u65b0\u7684\u9032\u968e\u5f71\u50cf\u6a19\u984c\u65b9\u6cd5\u3002\u9019\u9805\u7814\u7a76\u91dd\u5c0d\u5f71\u50cf\u6a19\u984c\u554f\u984c\u63d0\u51fa\u65b0\u7684\u8a55\u91cf\u6307\u6a19\u3002\u9996\u5148\uff0c\u7522\u751f\u4e00\u500b\u7531\u4eba\u985e\u6a19\u8a18\u7684\u8cc7\u6599\u96c6\uff0c\u4ee5\u8a55\u4f30\u6a19\u984c\u8207\u5f71\u50cf\u5167\u5bb9\u76f8\u95dc\u7684\u7a0b\u5ea6\u3002\u5c07\u9019\u4e9b\u4eba\u985e\u8a55\u5206\u8996\u70ba\u771f\u5be6\u4f9d\u64da\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u6307\u6a19\uff0c\u4e26\u5c07\u5176\u8207\u5e7e\u500b\u8457\u540d\u7684\u6307\u6a19\u9032\u884c\u6bd4\u8f03\uff0c\u5f9e\u50b3\u7d71\u6307\u6a19\u5230\u8f03\u65b0\u7684\u6307\u6a19\u3002\u4e5f\u767c\u73fe\u8868\u73fe\u512a\u7570\u7684\u7d50\u679c\uff0c\u4e26\u63d0\u51fa\u548c\u8a0e\u8ad6\u4e86\u6709\u8da3\u7684\u898b\u89e3\u3002", "author": "Guillermo Ruiz et.al.", "authors": "Guillermo Ruiz, Tania Ram\u00edrez, Daniela Moctezuma", "id": "2501.09155v1", "paper_url": "http://arxiv.org/abs/2501.09155v1", "repo": "null"}}