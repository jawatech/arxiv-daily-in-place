{"2412.12888": {"publish_time": "2024-12-17", "title": "ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction", "paper_summary": "The emergence of diffusion models has significantly advanced image synthesis.\nThe recent studies of model interaction and self-corrective reasoning approach\nin large language models offer new insights for enhancing text-to-image models.\nInspired by these studies, we propose a novel method called ArtAug for\nenhancing text-to-image models in this paper. To the best of our knowledge,\nArtAug is the first one that improves image synthesis models via model\ninteractions with understanding models. In the interactions, we leverage human\npreferences implicitly learned by image understanding models to provide\nfine-grained suggestions for image synthesis models. The interactions can\nmodify the image content to make it aesthetically pleasing, such as adjusting\nexposure, changing shooting angles, and adding atmospheric effects. The\nenhancements brought by the interaction are iteratively fused into the\nsynthesis model itself through an additional enhancement module. This enables\nthe synthesis model to directly produce aesthetically pleasing images without\nany extra computational cost. In the experiments, we train the ArtAug\nenhancement module on existing text-to-image models. Various evaluation metrics\nconsistently demonstrate that ArtAug enhances the generative capabilities of\ntext-to-image models without incurring additional computational costs. The\nsource code and models will be released publicly.", "paper_summary_zh": "\u64f4\u6563\u6a21\u578b\u7684\u51fa\u73fe\u986f\u8457\u5730\u63a8\u52d5\u4e86\u5f71\u50cf\u5408\u6210\u3002\n\u6700\u8fd1\u5c0d\u65bc\u6a21\u578b\u4e92\u52d5\u8207\u81ea\u6211\u4fee\u6b63\u63a8\u7406\u65b9\u6cd5\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7814\u7a76\u70ba\u589e\u5f37\u6587\u5b57\u8f49\u5f71\u50cf\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\u3002\n\u53d7\u9019\u4e9b\u7814\u7a76\u555f\u767c\uff0c\u6211\u5011\u5728\u672c\u6587\u4e2d\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba ArtAug \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u65bc\u589e\u5f37\u6587\u5b57\u8f49\u5f71\u50cf\u6a21\u578b\u3002\n\u64da\u6211\u5011\u6240\u77e5\uff0cArtAug \u662f\u7b2c\u4e00\u500b\u900f\u904e\u6a21\u578b\u8207\u7406\u89e3\u6a21\u578b\u7684\u4e92\u52d5\u4f86\u6539\u5584\u5f71\u50cf\u5408\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u3002\n\u5728\u4e92\u52d5\u904e\u7a0b\u4e2d\uff0c\u6211\u5011\u5229\u7528\u5f71\u50cf\u7406\u89e3\u6a21\u578b\u96b1\u542b\u5b78\u7fd2\u5230\u7684\u4eba\u985e\u504f\u597d\uff0c\u70ba\u5f71\u50cf\u5408\u6210\u6a21\u578b\u63d0\u4f9b\u7d30\u7dfb\u7684\u5efa\u8b70\u3002\n\u4e92\u52d5\u53ef\u4ee5\u4fee\u6539\u5f71\u50cf\u5167\u5bb9\u4ee5\u4f7f\u5176\u7f8e\u89c0\uff0c\u4f8b\u5982\u8abf\u6574\u66dd\u5149\u3001\u6539\u8b8a\u62cd\u651d\u89d2\u5ea6\u548c\u589e\u52a0\u5927\u6c23\u6548\u679c\u3002\n\u4e92\u52d5\u5e36\u4f86\u7684\u589e\u5f37\u6703\u900f\u904e\u984d\u5916\u7684\u589e\u5f37\u6a21\u7d44\u53cd\u8986\u878d\u5408\u5230\u5408\u6210\u6a21\u578b\u672c\u8eab\u4e2d\u3002\n\u9019\u4f7f\u5408\u6210\u6a21\u578b\u80fd\u5920\u76f4\u63a5\u7522\u751f\u7f8e\u89c0\u7684\u5f71\u50cf\uff0c\u800c\u7121\u9700\u4efb\u4f55\u984d\u5916\u7684\u904b\u7b97\u6210\u672c\u3002\n\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5728\u73fe\u6709\u7684\u6587\u5b57\u8f49\u5f71\u50cf\u6a21\u578b\u4e0a\u8a13\u7df4 ArtAug \u589e\u5f37\u6a21\u7d44\u3002\n\u5404\u7a2e\u8a55\u4f30\u6307\u6a19\u4e00\u81f4\u5730\u8b49\u660e\uff0cArtAug \u589e\u5f37\u4e86\u6587\u5b57\u8f49\u5f71\u50cf\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u800c\u4e0d\u6703\u7522\u751f\u984d\u5916\u7684\u904b\u7b97\u6210\u672c\u3002\n\u539f\u59cb\u78bc\u548c\u6a21\u578b\u5c07\u516c\u958b\u767c\u5e03\u3002", "author": "Zhongjie Duan et.al.", "authors": "Zhongjie Duan, Qianyi Zhao, Cen Chen, Daoyuan Chen, Wenmeng Zhou, Yaliang Li, Yingda Chen", "id": "2412.12888v1", "paper_url": "http://arxiv.org/abs/2412.12888v1", "repo": "https://github.com/modelscope/DiffSynth-Studio"}}