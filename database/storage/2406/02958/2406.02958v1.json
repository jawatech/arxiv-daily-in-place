{"2406.02958": {"publish_time": "2024-06-05", "title": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs", "paper_summary": "On-device training is currently the most common approach for training machine\nlearning (ML) models on private, distributed user data. Despite this, on-device\ntraining has several drawbacks: (1) most user devices are too small to train\nlarge models on-device, (2) on-device training is communication- and\ncomputation-intensive, and (3) on-device training can be difficult to debug and\ndeploy. To address these problems, we propose Private Evolution-Text\n(PrE-Text), a method for generating differentially private (DP) synthetic\ntextual data. First, we show that across multiple datasets, training small\nmodels (models that fit on user devices) with PrE-Text synthetic data\noutperforms small models trained on-device under practical privacy regimes\n($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using\n9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and\n100$\\times$ less communication per round. Second, finetuning large models on\nPrE-Text's DP synthetic data improves large language model (LLM) performance on\nprivate data across the same range of privacy budgets. Altogether, these\nresults suggest that training on DP synthetic data can be a better option than\ntraining a model on-device on private distributed data. Code is available at\nhttps://github.com/houcharlie/PrE-Text.", "paper_summary_zh": "\u76ee\u524d\uff0c\u88dd\u7f6e\u4e0a\u8a13\u7df4\u662f\u91dd\u5c0d\u79c1\u4eba\u3001\u5206\u6563\u4f7f\u7528\u8005\u8cc7\u6599\u8a13\u7df4\u6a5f\u5668\u5b78\u7fd2 (ML) \u6a21\u578b\u6700\u5e38\u898b\u7684\u65b9\u6cd5\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u88dd\u7f6e\u4e0a\u8a13\u7df4\u6709\u5e7e\u500b\u7f3a\u9ede\uff1a(1) \u5927\u90e8\u5206\u4f7f\u7528\u8005\u88dd\u7f6e\u592a\u5c0f\uff0c\u7121\u6cd5\u5728\u88dd\u7f6e\u4e0a\u8a13\u7df4\u5927\u578b\u6a21\u578b\uff0c(2) \u88dd\u7f6e\u4e0a\u8a13\u7df4\u9700\u8981\u5927\u91cf\u901a\u8a0a\u548c\u904b\u7b97\uff0c\u4ee5\u53ca (3) \u88dd\u7f6e\u4e0a\u8a13\u7df4\u53ef\u80fd\u96e3\u4ee5\u9664\u932f\u548c\u90e8\u7f72\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa Private Evolution-Text (PrE-Text)\uff0c\u9019\u662f\u4e00\u7a2e\u7522\u751f\u5dee\u7570\u5316\u79c1\u4eba (DP) \u5408\u6210\u6587\u5b57\u8cc7\u6599\u7684\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u5011\u5c55\u793a\u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\uff0c\u4f7f\u7528 PrE-Text \u5408\u6210\u8cc7\u6599\u8a13\u7df4\u5c0f\u578b\u6a21\u578b\uff08\u9069\u7528\u65bc\u4f7f\u7528\u8005\u88dd\u7f6e\u7684\u6a21\u578b\uff09\u512a\u65bc\u5728\u5be6\u969b\u96b1\u79c1\u5236\u5ea6\u4e0b\u8a13\u7df4\u7684\u88dd\u7f6e\u4e0a\u5c0f\u578b\u6a21\u578b\uff08$\\epsilon=1.29$\u3001$\\epsilon=7.58$\uff09\u3002\u6211\u5011\u5728\u4f7f\u7528 9 \u500d\u8f03\u5c11\u56de\u5408\u3001\u6bcf\u4e00\u56de\u5408 6 \u500d\u8f03\u5c11\u7528\u6236\u7aef\u904b\u7b97\u4ee5\u53ca\u6bcf\u4e00\u56de\u5408 100 \u500d\u8f03\u5c11\u901a\u8a0a\u7684\u60c5\u6cc1\u4e0b\uff0c\u9054\u5230\u9019\u4e9b\u7d50\u679c\u3002\u5176\u6b21\uff0c\u5728 PrE-Text \u7684 DP \u5408\u6210\u8cc7\u6599\u4e0a\u5fae\u8abf\u5927\u578b\u6a21\u578b\uff0c\u6703\u6539\u5584\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u76f8\u540c\u96b1\u79c1\u9810\u7b97\u7bc4\u570d\u5167\u91dd\u5c0d\u79c1\u4eba\u8cc7\u6599\u7684\u6548\u80fd\u3002\u7e3d\u4e4b\uff0c\u9019\u4e9b\u7d50\u679c\u986f\u793a\u5728 DP \u5408\u6210\u8cc7\u6599\u4e0a\u8a13\u7df4\u6703\u6bd4\u5728\u79c1\u4eba\u5206\u6563\u8cc7\u6599\u4e0a\u8a13\u7df4\u6a21\u578b\u4f86\u5f97\u66f4\u597d\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/houcharlie/PrE-Text \u53d6\u5f97\u3002", "author": "Charlie Hou et.al.", "authors": "Charlie Hou, Akshat Shrivastava, Hongyuan Zhan, Rylan Conway, Trang Le, Adithya Sagar, Giulia Fanti, Daniel Lazar", "id": "2406.02958v1", "paper_url": "http://arxiv.org/abs/2406.02958v1", "repo": "https://github.com/houcharlie/pre-text"}}