{"2409.03735": {"publish_time": "2024-09-05", "title": "LLM-CI: Assessing Contextual Integrity Norms in Language Models", "paper_summary": "Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization).", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a18\u61b6\u5f9e\u7db2\u969b\u7db2\u8def\u64f7\u53d6\u7684\u90e8\u5206\u8a13\u7df4\u8cc7\u6599\u6642\uff0c\u4e5f\u53ef\u80fd\u7121\u610f\u9593\u7de8\u78bc\u793e\u6703\u504f\u597d\u548c\u898f\u7bc4\u3002\u96a8\u8457\u9019\u4e9b\u6a21\u578b\u6574\u5408\u5230\u793e\u6703\u6280\u8853\u7cfb\u7d71\u4e2d\uff0c\u5b83\u5011\u7de8\u78bc\u7684\u898f\u7bc4\u8207\u793e\u6703\u671f\u671b\u4e00\u81f4\u81f3\u95dc\u91cd\u8981\u3002\u9019\u4e9b\u898f\u7bc4\u53ef\u80fd\u6703\u56e0\u6a21\u578b\u3001\u8d85\u53c3\u6578\u3001\u6700\u4f73\u5316\u6280\u8853\u548c\u8cc7\u6599\u96c6\u800c\u7570\u3002\u7531\u65bc\u63d0\u793a\u654f\u611f\u6027\uff0c\u9019\u5c24\u5176\u5177\u6709\u6311\u6230\u6027$-$\u63d0\u793a\u7684\u5fae\u5c0f\u8b8a\u5316\u6703\u7522\u751f\u4e0d\u540c\u7684\u56de\u61c9\uff0c\u4f7f\u5f97\u73fe\u6709\u7684\u8a55\u4f30\u65b9\u6cd5\u8ad6\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u500b\u6db5\u84cb\u5404\u7a2e\u6a21\u578b\u3001\u6700\u4f73\u5316\u548c\u8cc7\u6599\u96c6\u7684\u7d9c\u5408\u67b6\u69cb\uff0c\u4ee5\u53ca\u4e00\u500b\u53ef\u9760\u7684\u65b9\u6cd5\u8ad6\u4f86\u8a55\u4f30\u7de8\u78bc\u7684\u898f\u7bc4\u3002\n\u6211\u5011\u63d0\u51fa LLM-CI\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u7528\u65bc\u8a55\u4f30 LLM \u4e2d\u7de8\u78bc\u7684\u96b1\u79c1\u898f\u7bc4\u7684\u958b\u6e90\u67b6\u69cb\u3002LLM-CI \u4f7f\u7528\u57fa\u65bc\u60c5\u5883\u5b8c\u6574\u6027\u7684\u968e\u4e58\u5c0f\u63d2\u66f2\u65b9\u6cd5\u8ad6\u4f86\u8a55\u4f30\u4e0d\u540c\u60c5\u5883\u548c LLM \u4e2d\u7de8\u78bc\u7684\u898f\u7bc4\u3002\u6211\u5011\u63d0\u51fa\u591a\u63d0\u793a\u8a55\u4f30\u65b9\u6cd5\u8ad6\u4f86\u89e3\u6c7a\u63d0\u793a\u654f\u611f\u6027\uff0c\u65b9\u6cd5\u662f\u50c5\u5f9e\u5728\u591a\u500b\u8b8a\u9ad4\u4e2d\u7522\u751f\u4e00\u81f4\u56de\u61c9\u7684\u63d0\u793a\u4e2d\u8a55\u4f30\u898f\u7bc4\u3002\u4f7f\u7528 LLM-CI \u548c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u8ad6\uff0c\u6211\u5011\u4f7f\u7528\u5148\u524d\u5de5\u4f5c\u4e2d\u7684 IoT \u548c COPPA \u5c0f\u63d2\u66f2\u8cc7\u6599\u96c6\u5168\u9762\u8a55\u4f30 LLM\uff0c\u6aa2\u67e5\u6a21\u578b\u5c6c\u6027\uff08\u4f8b\u5982\uff0c\u8d85\u53c3\u6578\u3001\u5bb9\u91cf\uff09\u548c\u6700\u4f73\u5316\u7b56\u7565\uff08\u4f8b\u5982\uff0c\u5c0d\u9f4a\u3001\u91cf\u5316\uff09\u7684\u5f71\u97ff\u3002", "author": "Yan Shvartzshnaider et.al.", "authors": "Yan Shvartzshnaider, Vasisht Duddu, John Lacalamita", "id": "2409.03735v1", "paper_url": "http://arxiv.org/abs/2409.03735v1", "repo": "null"}}