{"2405.19332": {"publish_time": "2024-05-29", "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment", "paper_summary": "Preference optimization, particularly through Reinforcement Learning from\nHuman Feedback (RLHF), has achieved significant success in aligning Large\nLanguage Models (LLMs) to adhere to human intentions. Unlike offline alignment\nwith a fixed dataset, online feedback collection from humans or AI on model\ngenerations typically leads to more capable reward models and better-aligned\nLLMs through an iterative process. However, achieving a globally accurate\nreward model requires systematic exploration to generate diverse responses that\nspan the vast space of natural language. Random sampling from standard\nreward-maximizing LLMs alone is insufficient to fulfill this requirement. To\naddress this issue, we propose a bilevel objective optimistically biased\ntowards potentially high-reward responses to actively explore\nout-of-distribution regions. By solving the inner-level problem with the\nreparameterized reward function, the resulting algorithm, named Self-Exploring\nLanguage Models (SELM), eliminates the need for a separate RM and iteratively\nupdates the LLM with a straightforward objective. Compared to Direct Preference\nOptimization (DPO), the SELM objective reduces indiscriminate favor of unseen\nextrapolations and enhances exploration efficiency. Our experimental results\ndemonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct\nmodels, SELM significantly boosts the performance on instruction-following\nbenchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard\nacademic benchmarks in different settings. Our code and models are available at\nhttps://github.com/shenao-zhang/SELM.", "paper_summary_zh": "\u504f\u597d\u6700\u4f73\u5316\uff0c\u5c24\u5176\u662f\u900f\u904e\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF)\uff0c\u5728\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7b26\u5408\u4eba\u985e\u610f\u5716\u65b9\u9762\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\u3002\u8207\u4f7f\u7528\u56fa\u5b9a\u8cc7\u6599\u96c6\u7684\u96e2\u7dda\u6821\u6e96\u4e0d\u540c\uff0c\u5f9e\u4eba\u985e\u6216\u4eba\u5de5\u667a\u6167\u5c0d\u6a21\u578b\u7522\u751f\u7684\u5728\u7dda\u56de\u994b\u901a\u5e38\u6703\u900f\u904e\u53cd\u8986\u7684\u904e\u7a0b\u7522\u751f\u66f4\u6709\u80fd\u529b\u7684\u734e\u52f5\u6a21\u578b\u548c\u66f4\u4f73\u6821\u6e96\u7684 LLM\u3002\u7136\u800c\uff0c\u8981\u9054\u6210\u5168\u7403\u7cbe\u6e96\u7684\u734e\u52f5\u6a21\u578b\u9700\u8981\u6709\u7cfb\u7d71\u7684\u63a2\u7d22\uff0c\u4ee5\u7522\u751f\u6a6b\u8de8\u81ea\u7136\u8a9e\u8a00\u5ee3\u5927\u7a7a\u9593\u7684\u591a\u5143\u56de\u61c9\u3002\u50c5\u5f9e\u6a19\u6e96\u734e\u52f5\u6700\u5927\u5316\u7684 LLM \u9032\u884c\u96a8\u6a5f\u53d6\u6a23\u4e0d\u8db3\u4ee5\u6eff\u8db3\u6b64\u9805\u9700\u6c42\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u96d9\u5c64\u6b21\u76ee\u6a19\uff0c\u6a02\u89c0\u504f\u5411\u65bc\u6f5b\u5728\u9ad8\u734e\u52f5\u56de\u61c9\uff0c\u4ee5\u7a4d\u6975\u63a2\u7d22\u5206\u4f48\u5916\u5340\u57df\u3002\u900f\u904e\u4f7f\u7528\u91cd\u65b0\u53c3\u6578\u5316\u7684\u734e\u52f5\u51fd\u6578\u89e3\u6c7a\u5167\u5c64\u6b21\u554f\u984c\uff0c\u6240\u7522\u751f\u7684\u6f14\u7b97\u6cd5\uff0c\u7a31\u70ba\u81ea\u6211\u63a2\u7d22\u8a9e\u8a00\u6a21\u578b (SELM)\uff0c\u6d88\u9664\u4e86\u5c0d\u55ae\u7368 RM \u7684\u9700\u6c42\uff0c\u4e26\u4f7f\u7528\u76f4\u63a5\u7684\u76ee\u6a19\u53cd\u8986\u66f4\u65b0 LLM\u3002\u8207\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u76f8\u6bd4\uff0cSELM \u76ee\u6a19\u6e1b\u5c11\u4e86\u5c0d\u672a\u898b\u5916\u63a8\u7684\u7121\u5dee\u5225\u504f\u597d\uff0c\u4e26\u63d0\u5347\u63a2\u7d22\u6548\u7387\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u7576\u5728 Zephyr-7B-SFT \u548c Llama-3-8B-Instruct \u6a21\u578b\u4e0a\u9032\u884c\u5fae\u8abf\u6642\uff0cSELM \u5728\u9075\u5faa\u6307\u4ee4\u7684\u57fa\u6e96\u6e2c\u8a66\uff08\u4f8b\u5982 MT-Bench \u548c AlpacaEval 2.0\uff09\u4ee5\u53ca\u4e0d\u540c\u8a2d\u5b9a\u4e2d\u7684\u5404\u7a2e\u6a19\u6e96\u5b78\u8853\u57fa\u6e96\u6e2c\u8a66\u4e0a\u5927\u5e45\u63d0\u5347\u6548\u80fd\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u5728 https://github.com/shenao-zhang/SELM \u53d6\u5f97\u3002", "author": "Shenao Zhang et.al.", "authors": "Shenao Zhang, Donghan Yu, Hiteshi Sharma, Ziyi Yang, Shuohang Wang, Hany Hassan, Zhaoran Wang", "id": "2405.19332v1", "paper_url": "http://arxiv.org/abs/2405.19332v1", "repo": "https://github.com/shenao-zhang/selm"}}