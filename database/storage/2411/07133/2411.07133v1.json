{"2411.07133": {"publish_time": "2024-11-11", "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning", "paper_summary": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines.", "paper_summary_zh": "\u6307\u4ee4\u5fae\u8c03\u5df2\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4ee5\u786e\u4fdd\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6709\u6548\u9075\u5faa\u7528\u6237\u6307\u4ee4\u3002LLM \u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u7528\u4e8e\u5fae\u8c03\u7684\u6307\u4ee4\u6570\u636e\u96c6\u3002\u6700\u8fd1\uff0c\u5408\u6210\u6307\u4ee4\u6570\u636e\u96c6\u5df2\u6210\u4e3a\u4e3a LLM \u63d0\u4f9b\u591a\u6837\u5316\u4e14\u9ad8\u8d28\u91cf\u6307\u4ee4\u7684\u4e00\u79cd\u7ecf\u6d4e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u66f4\u5927\u6216\u66f4\u5f3a\u7684\u6a21\u578b\u662f\u6307\u4ee4\u5fae\u8c03\u7684\u66f4\u5f3a\u6559\u5e08\uff0c\u56e0\u6b64\u53ea\u662f\u91c7\u7528\u8fd9\u4e9b\u6a21\u578b\u4f5c\u4e3a\u5408\u6210\u6307\u4ee4\u7684\u54cd\u5e94\u751f\u6210\u5668\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u8fd9\u4e00\u666e\u904d\u91c7\u7528\u7684\u5047\u8bbe\u63d0\u51fa\u8d28\u7591\u3002\u6211\u4eec\u5bf9\u4e94\u4e2a\u57fa\u7840\u6a21\u578b\u548c 20 \u4e2a\u54cd\u5e94\u751f\u6210\u5668\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u66f4\u5927\u3001\u66f4\u5f3a\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u662f\u5bf9\u8f83\u5c0f\u6a21\u578b\u7684\u66f4\u5f3a\u8001\u5e08\u3002\u6211\u4eec\u5c06\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u5927\u6a21\u578b\u6096\u8bba\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u73b0\u6709\u7684\u6307\u6807\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u54cd\u5e94\u751f\u6210\u5668\u7684\u6709\u6548\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86\u6b63\u5728\u5fae\u8c03\u7684\u6559\u5e08\u548c\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u7684\u517c\u5bb9\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3a\u517c\u5bb9\u6027\u8c03\u6574\u5956\u52b1 (CAR) \u7684\u65b0\u6307\u6807\u6765\u8861\u91cf\u54cd\u5e94\u751f\u6210\u5668\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5bf9\u4e94\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAR \u4f18\u4e8e\u51e0\u4e4e\u6240\u6709\u57fa\u7ebf\u3002", "author": "Zhangchen Xu et.al.", "authors": "Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran", "id": "2411.07133v1", "paper_url": "http://arxiv.org/abs/2411.07133v1", "repo": "null"}}