{"2411.14762": {"publish_time": "2024-11-22", "title": "Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction", "paper_summary": "Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.", "paper_summary_zh": "\u9577\u5f71\u7247\u7684\u6709\u6548\u6a19\u8a18\u5316\u5728\u8a13\u7df4\u80fd\u8655\u7406\u9577\u5f71\u7247\u7684\u8996\u89ba\u6a21\u578b\u6642\u4ecd\u7136\u662f\u4e00\u9805\u6311\u6230\u3002\u4e00\u500b\u6709\u524d\u666f\u7684\u65b9\u5411\u662f\u958b\u767c\u4e00\u500b\u80fd\u7de8\u78bc\u9577\u5f71\u7247\u7247\u6bb5\u7684\u6a19\u8a18\u5668\uff0c\u56e0\u70ba\u5b83\u80fd\u8b93\u6a19\u8a18\u5668\u66f4\u597d\u5730\u5229\u7528\u5f71\u7247\u7684\u6642\u9593\u9023\u8cab\u6027\u9032\u884c\u6a19\u8a18\u5316\u3002\u7136\u800c\uff0c\u5728\u9577\u5f71\u7247\u4e0a\u8a13\u7df4\u73fe\u6709\u7684\u6a19\u8a18\u5668\u901a\u5e38\u6703\u7522\u751f\u5de8\u5927\u7684\u8a13\u7df4\u6210\u672c\uff0c\u56e0\u70ba\u5b83\u5011\u88ab\u8a13\u7df4\u4e00\u6b21\u91cd\u5efa\u6240\u6709\u5f71\u683c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 CoordTok\uff0c\u4e00\u500b\u5f71\u7247\u6a19\u8a18\u5668\uff0c\u5b83\u5f9e\u57fa\u65bc\u5ea7\u6a19\u7684\u8868\u793a\u5b78\u7fd2\u5c0d\u61c9\u8f38\u5165\u5f71\u7247\u7684\u88dc\u4e01\u7684\u5c0d\u61c9\uff0c\u9748\u611f\u4f86\u81ea 3D \u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u9032\u5c55\u3002\u7279\u5225\u662f\uff0cCoordTok \u5c07\u5f71\u7247\u7de8\u78bc\u6210\u5206\u89e3\u7684\u4e09\u5e73\u9762\u8868\u793a\uff0c\u4e26\u91cd\u5efa\u5c0d\u61c9\u65bc\u96a8\u6a5f\u53d6\u6a23\u7684 $(x,y,t)$ \u5ea7\u6a19\u7684\u88dc\u4e01\u3002\u9019\u5141\u8a31\u5728\u9577\u5f71\u7247\u4e0a\u76f4\u63a5\u8a13\u7df4\u5927\u578b\u6a19\u8a18\u5668\u6a21\u578b\uff0c\u800c\u4e0d\u9700\u8981\u904e\u591a\u7684\u8a13\u7df4\u8cc7\u6e90\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cCoordTok \u53ef\u4ee5\u5927\u5e45\u6e1b\u5c11\u7de8\u78bc\u9577\u5f71\u7247\u7247\u6bb5\u7684\u6a19\u8a18\u6578\u91cf\u3002\u4f8b\u5982\uff0cCoordTok \u53ef\u4ee5\u5c07 128 \u5e40\u3001\u89e3\u6790\u5ea6\u70ba 128$\\times$128 \u7684\u5f71\u7247\u7de8\u78bc\u6210 1280 \u500b\u6a19\u8a18\uff0c\u800c\u57fa\u7dda\u9700\u8981 6144 \u6216 8192 \u500b\u6a19\u8a18\u624d\u80fd\u9054\u5230\u985e\u4f3c\u7684\u91cd\u5efa\u54c1\u8cea\u3002\u6211\u5011\u9032\u4e00\u6b65\u8868\u660e\uff0c\u9019\u7a2e\u6709\u6548\u7684\u5f71\u7247\u6a19\u8a18\u5316\u80fd\u8b93\u64f4\u6563\u8f49\u63db\u5668\u7684\u8a18\u61b6\u9ad4\u6709\u6548\u8a13\u7df4\uff0c\u4e00\u6b21\u53ef\u4ee5\u7522\u751f 128 \u5e40\u3002", "author": "Huiwon Jang et.al.", "authors": "Huiwon Jang, Sihyun Yu, Jinwoo Shin, Pieter Abbeel, Younggyo Seo", "id": "2411.14762v1", "paper_url": "http://arxiv.org/abs/2411.14762v1", "repo": "null"}}