{"2408.11745": {"publish_time": "2024-08-21", "title": "FocusLLM: Scaling LLM's Context by Parallel Decoding", "paper_summary": "Empowering LLMs with the ability to utilize useful information from a long\ncontext is crucial for many downstream applications. However, achieving long\ncontext lengths with the conventional transformer architecture requires\nsubstantial training and inference resources. In this paper, we present\nFocusLLM, a framework designed to extend the context length of any decoder-only\nLLM, enabling the model to focus on relevant information from very long\nsequences. FocusLLM processes long text inputs by dividing them into chunks\nbased on the model's original context length to alleviate the issue of\nattention distraction. Then, it appends the local context to each chunk as a\nprompt to extract essential information from each chunk based on a novel\nparallel decoding mechanism, and ultimately integrates the extracted\ninformation into the local context. FocusLLM stands out for great training\nefficiency and versatility: trained with an 8K input length with much less\ntraining cost than previous methods, FocusLLM exhibits superior performance\nacross downstream long-context tasks and maintains strong language modeling\nability when handling extensive long texts, even up to 400K tokens. Our code is\navailable at https://github.com/leezythu/FocusLLM.", "paper_summary_zh": "\u8ce6\u4e88 LLM \u5229\u7528\u9577\u8a9e\u5883\u4e2d\u6709\u7528\u8cc7\u8a0a\u7684\u80fd\u529b\u5c0d\u65bc\u8a31\u591a\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f7f\u7528\u50b3\u7d71\u7684 Transformer \u67b6\u69cb\u4f86\u5be6\u73fe\u9577\u8a9e\u5883\u9577\u5ea6\u9700\u8981\u5927\u91cf\u7684\u8a13\u7df4\u548c\u63a8\u8ad6\u8cc7\u6e90\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa FocusLLM\uff0c\u4e00\u500b\u65e8\u5728\u64f4\u5c55\u4efb\u4f55\u50c5\u89e3\u78bc\u5668 LLM \u7684\u8a9e\u5883\u9577\u5ea6\u7684\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u5c08\u6ce8\u65bc\u975e\u5e38\u9577\u7684\u5e8f\u5217\u4e2d\u7684\u76f8\u95dc\u8cc7\u8a0a\u3002FocusLLM \u900f\u904e\u5c07\u9577\u6587\u672c\u8f38\u5165\u5206\u5272\u6210\u584a\uff08\u57fa\u65bc\u6a21\u578b\u7684\u539f\u59cb\u8a9e\u5883\u9577\u5ea6\uff09\u4f86\u8655\u7406\uff0c\u4ee5\u6e1b\u8f15\u6ce8\u610f\u529b\u5206\u6563\u7684\u554f\u984c\u3002\u7136\u5f8c\uff0c\u5b83\u5c07\u5c40\u90e8\u8a9e\u5883\u9644\u52a0\u5230\u6bcf\u500b\u584a\u4f5c\u70ba\u63d0\u793a\uff0c\u4ee5\u57fa\u65bc\u65b0\u7a4e\u7684\u4e26\u884c\u89e3\u78bc\u6a5f\u5236\u5f9e\u6bcf\u500b\u584a\u4e2d\u63d0\u53d6\u5fc5\u8981\u8cc7\u8a0a\uff0c\u4e26\u6700\u7d42\u5c07\u63d0\u53d6\u7684\u8cc7\u8a0a\u6574\u5408\u5230\u5c40\u90e8\u8a9e\u5883\u4e2d\u3002FocusLLM \u4ee5\u5176\u51fa\u8272\u7684\u8a13\u7df4\u6548\u7387\u548c\u591a\u529f\u80fd\u6027\u800c\u8457\u7a31\uff1a\u4f7f\u7528 8K \u8f38\u5165\u9577\u5ea6\u9032\u884c\u8a13\u7df4\uff0c\u8a13\u7df4\u6210\u672c\u9060\u4f4e\u65bc\u4ee5\u524d\u7684\u65b9\u6cd5\uff0cFocusLLM \u5728\u4e0b\u6e38\u9577\u8a9e\u5883\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\uff0c\u4e26\u5728\u8655\u7406\u5ee3\u6cdb\u7684\u9577\u6587\u672c\uff08\u751a\u81f3\u9577\u9054 400K \u500b\u7b26\u865f\uff09\u6642\u4fdd\u6301\u5f37\u5927\u7684\u8a9e\u8a00\u5efa\u6a21\u80fd\u529b\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u4ee5\u5728 https://github.com/leezythu/FocusLLM \u53d6\u5f97\u3002", "author": "Zhenyu Li et.al.", "authors": "Zhenyu Li, Yike Zhang, Tengyu Pan, Yutao Sun, Zhichao Duan, Junjie Fang, Rong Han, Zixuan Wang, Jianyong Wang", "id": "2408.11745v1", "paper_url": "http://arxiv.org/abs/2408.11745v1", "repo": "null"}}