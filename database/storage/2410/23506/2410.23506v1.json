{"2410.23506": {"publish_time": "2024-10-30", "title": "Learning to Achieve Goals with Belief State Transformers", "paper_summary": "We introduce the \"Belief State Transformer\", a next-token predictor that\ntakes both a prefix and suffix as inputs, with a novel objective of predicting\nboth the next token for the prefix and the previous token for the suffix. The\nBelief State Transformer effectively learns to solve challenging problems that\nconventional forward-only transformers struggle with, in a domain-independent\nfashion. Key to this success is learning a compact belief state that captures\nall relevant information necessary for accurate predictions. Empirical\nablations show that each component of the model is essential in difficult\nscenarios where standard Transformers fall short. For the task of story writing\nwith known prefixes and suffixes, our approach outperforms the\nFill-in-the-Middle method for reaching known goals and demonstrates improved\nperformance even when the goals are unknown. Altogether, the Belief State\nTransformer enables more efficient goal-conditioned decoding, better test-time\ninference, and high-quality text representations on small scale problems.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4ecb\u7d39\u300c\u4fe1\u5ff5\u72c0\u614bTransformer\u300d\uff0c\u4e00\u7a2e\u4ee5\u8a5e\u9996\u548c\u8a5e\u5c3e\u4f5c\u70ba\u8f38\u5165\u7684\u4e0b\u4e00\u8a5e\u5143\u9810\u6e2c\u5668\uff0c\u4e26\u4ee5\u9810\u6e2c\u8a5e\u9996\u7684\u4e0b\u4e00\u8a5e\u5143\u548c\u8a5e\u5c3e\u7684\u524d\u4e00\u8a5e\u5143\u70ba\u65b0\u76ee\u6a19\u3002\u4fe1\u5ff5\u72c0\u614bTransformer\u6709\u6548\u5730\u5b78\u6703\u89e3\u6c7a\u50b3\u7d71\u50c5\u524d\u5411Transformer\u96e3\u4ee5\u61c9\u4ed8\u7684\u6311\u6230\u6027\u554f\u984c\uff0c\u800c\u4e14\u4e0d\u53d7\u7279\u5b9a\u9818\u57df\u9650\u5236\u3002\u6210\u529f\u7684\u95dc\u9375\u5728\u65bc\u5b78\u7fd2\u4e00\u500b\u7cbe\u7c21\u7684\u4fe1\u5ff5\u72c0\u614b\uff0c\u5b83\u64f7\u53d6\u4e86\u6240\u6709\u7cbe\u6e96\u9810\u6e2c\u6240\u9700\u7684\u76f8\u5173\u8cc7\u8a0a\u3002\u5be6\u8b49\u6d88\u878d\u7814\u7a76\u986f\u793a\uff0c\u6a21\u578b\u7684\u6bcf\u500b\u7d44\u6210\u90e8\u5206\u5728\u6a19\u6e96Transformer\u4e0d\u8db3\u4ee5\u61c9\u4ed8\u7684\u56f0\u96e3\u5834\u666f\u4e2d\u90fd\u81f3\u95dc\u91cd\u8981\u3002\u5c0d\u65bc\u64c1\u6709\u5df2\u77e5\u8a5e\u9996\u548c\u8a5e\u5c3e\u7684\u6545\u4e8b\u64b0\u5beb\u4efb\u52d9\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u9054\u6210\u5df2\u77e5\u76ee\u6a19\u65b9\u9762\u512a\u65bc\u586b\u5165\u4e2d\u9593\u6cd5\uff0c\u5373\u4f7f\u76ee\u6a19\u672a\u77e5\u6642\u4e5f\u5c55\u73fe\u51fa\u9032\u6b65\u7684\u6548\u80fd\u3002\u7e3d\u800c\u8a00\u4e4b\uff0c\u4fe1\u5ff5\u72c0\u614bTransformer\u80fd\u9032\u884c\u66f4\u6709\u6548\u7387\u7684\u76ee\u6a19\u689d\u4ef6\u89e3\u78bc\u3001\u66f4\u597d\u7684\u6e2c\u8a66\u6642\u9593\u63a8\u8ad6\uff0c\u4ee5\u53ca\u5728\u5c0f\u898f\u6a21\u554f\u984c\u4e2d\u63d0\u4f9b\u9ad8\u54c1\u8cea\u7684\u6587\u5b57\u8868\u5fb5\u3002</paragraph>", "author": "Edward S. Hu et.al.", "authors": "Edward S. Hu, Kwangjun Ahn, Qinghua Liu, Haoran Xu, Manan Tomar, Ada Langford, Dinesh Jayaraman, Alex Lamb, John Langford", "id": "2410.23506v1", "paper_url": "http://arxiv.org/abs/2410.23506v1", "repo": "null"}}