{"2501.05629": {"publish_time": "2025-01-10", "title": "The Impact of Model Scaling on Seen and Unseen Language Performance", "paper_summary": "The rapid advancement of Large Language Models (LLMs), particularly those\ntrained on multilingual corpora, has intensified the need for a deeper\nunderstanding of their performance across a diverse range of languages and\nmodel sizes. Our research addresses this critical need by studying the\nperformance and scaling behavior of multilingual LLMs in text classification\nand machine translation tasks across 204 languages. We systematically examine\nboth seen and unseen languages across three model families of varying sizes in\nzero-shot and few-shot settings. Our findings show significant differences in\nscaling behavior between zero-shot and two-shot scenarios, with striking\ndisparities in performance between seen and unseen languages. Model scale has\nlittle effect on zero-shot performance, which remains mostly flat. However, in\ntwo-shot settings, larger models show clear linear improvements in multilingual\ntext classification. For translation tasks, however, only the instruction-tuned\nmodel showed clear benefits from scaling. Our analysis also suggests that\noverall resource levels, not just the proportions of pretraining languages, are\nbetter predictors of model performance, shedding light on what drives\nmultilingual LLM effectiveness.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u9032\u5c55\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u5728\u591a\u8a9e\u8a00\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\uff0c\u52a0\u5287\u4e86\u5c0d\u5b83\u5011\u5728\u5404\u7a2e\u8a9e\u8a00\u548c\u6a21\u578b\u898f\u6a21\u4e2d\u8868\u73fe\u7684\u66f4\u6df1\u5165\u7406\u89e3\u7684\u9700\u6c42\u3002\u6211\u5011\u7684\u7814\u7a76\u901a\u904e\u7814\u7a76\u591a\u8a9e\u8a00 LLM \u5728 204 \u7a2e\u8a9e\u8a00\u7684\u6587\u672c\u5206\u985e\u548c\u6a5f\u5668\u7ffb\u8b6f\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u548c\u898f\u6a21\u5316\u884c\u70ba\u4f86\u6eff\u8db3\u9019\u4e00\u95dc\u9375\u9700\u6c42\u3002\u6211\u5011\u7cfb\u7d71\u5730\u6aa2\u67e5\u4e86\u5728\u96f6\u6b21\u5b78\u7fd2\u548c\u5c11\u6b21\u5b78\u7fd2\u8a2d\u7f6e\u4e2d\u4e09\u500b\u4e0d\u540c\u898f\u6a21\u7684\u6a21\u578b\u7cfb\u5217\u4e2d\u7684\u5df2\u898b\u548c\u672a\u898b\u8a9e\u8a00\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u96f6\u6b21\u5b78\u7fd2\u548c\u5169\u6b21\u5b78\u7fd2\u5834\u666f\u4e4b\u9593\u7684\u898f\u6a21\u5316\u884c\u70ba\u5b58\u5728\u986f\u8457\u5dee\u7570\uff0c\u5df2\u898b\u548c\u672a\u898b\u8a9e\u8a00\u4e4b\u9593\u7684\u8868\u73fe\u5b58\u5728\u986f\u8457\u5dee\u7570\u3002\u6a21\u578b\u898f\u6a21\u5c0d\u96f6\u6b21\u5b78\u7fd2\u8868\u73fe\u5f71\u97ff\u4e0d\u5927\uff0c\u8868\u73fe\u57fa\u672c\u6301\u5e73\u3002\u7136\u800c\uff0c\u5728\u5169\u6b21\u5b78\u7fd2\u8a2d\u7f6e\u4e2d\uff0c\u8f03\u5927\u7684\u6a21\u578b\u5728\u591a\u8a9e\u8a00\u6587\u672c\u5206\u985e\u4e2d\u986f\u793a\u51fa\u660e\u986f\u7684\u7dda\u6027\u6539\u9032\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u7ffb\u8b6f\u4efb\u52d9\uff0c\u53ea\u6709\u7d93\u904e\u6307\u4ee4\u8abf\u6574\u7684\u6a21\u578b\u624d\u986f\u793a\u51fa\u898f\u6a21\u5316\u7684\u660e\u986f\u597d\u8655\u3002\u6211\u5011\u7684\u5206\u6790\u9084\u8868\u660e\uff0c\u6574\u9ad4\u8cc7\u6e90\u6c34\u5e73\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u9810\u8a13\u7df4\u8a9e\u8a00\u7684\u6bd4\u4f8b\uff0c\u662f\u6a21\u578b\u8868\u73fe\u7684\u66f4\u597d\u9810\u6e2c\u6307\u6a19\uff0c\u9019\u63ed\u793a\u4e86\u591a\u8a9e\u8a00 LLM \u6548\u80fd\u7684\u9a45\u52d5\u529b\u3002", "author": "Rhitabrat Pokharel et.al.", "authors": "Rhitabrat Pokharel, Sina Bagheri Nezhad, Ameeta Agrawal, Suresh Singh", "id": "2501.05629v1", "paper_url": "http://arxiv.org/abs/2501.05629v1", "repo": "null"}}