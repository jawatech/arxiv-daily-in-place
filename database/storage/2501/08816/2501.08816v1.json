{"2501.08816": {"publish_time": "2025-01-15", "title": "IDEA: Image Description Enhanced CLIP-Adapter", "paper_summary": "CLIP (Contrastive Language-Image Pre-training) has attained great success in\npattern recognition and computer vision. Transferring CLIP to downstream tasks\n(e.g. zero- or few-shot classification) is a hot topic in multimodal learning.\nHowever, current studies primarily focus on either prompt learning for text or\nadapter tuning for vision, without fully exploiting the complementary\ninformation and correlations among image-text pairs. In this paper, we propose\nan Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to\nfew-shot image classification tasks. This method captures fine-grained features\nby leveraging both visual features and textual descriptions of images. IDEA is\na training-free method for CLIP, and it can be comparable to or even exceeds\nstate-of-the-art models on multiple tasks. Furthermore, we introduce\nTrainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable\ncomponents (i.e., a projector and a learnable latent space), further enhancing\nthe model's performance and achieving SOTA results on 11 datasets. As one\nimportant contribution, we employ the Llama model and design a comprehensive\npipeline to generate textual descriptions for images of 11 datasets, resulting\nin a total of 1,637,795 image-text pairs, named \"IMD-11\". Our code and data are\nreleased at https://github.com/FourierAI/IDEA.", "paper_summary_zh": "CLIP\uff08\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3\uff09\u5728\u6a21\u5f0f\u8bc6\u522b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u5c06 CLIP \u8f6c\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\uff08\u4f8b\u5982\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u5206\u7c7b\uff09\u662f\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u70ed\u95e8\u8bdd\u9898\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u7684\u63d0\u793a\u5b66\u4e60\u6216\u89c6\u89c9\u7684\u9002\u914d\u5668\u8c03\u6574\u4e0a\uff0c\u5e76\u672a\u5145\u5206\u5229\u7528\u56fe\u50cf\u6587\u672c\u5bf9\u4e4b\u95f4\u7684\u4e92\u8865\u4fe1\u606f\u548c\u76f8\u5173\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u50cf\u63cf\u8ff0\u589e\u5f3a CLIP \u9002\u914d\u5668 (IDEA) \u65b9\u6cd5\uff0c\u4ee5\u5c06 CLIP \u9002\u5e94\u4e8e\u5c11\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002\u6b64\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u7684\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u63cf\u8ff0\u6765\u6355\u6349\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002IDEA \u662f CLIP \u7684\u4e00\u79cd\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u53ef\u4ee5\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u5b83\u4eec\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u53ef\u8bad\u7ec3 IDEA (T-IDEA)\uff0c\u5b83\u901a\u8fc7\u6dfb\u52a0\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u7ec4\u4ef6\uff08\u5373\u6295\u5f71\u4eea\u548c\u53ef\u5b66\u4e60\u6f5c\u5728\u7a7a\u95f4\uff09\u6765\u6269\u5c55 IDEA\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728 11 \u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86 SOTA \u7ed3\u679c\u3002\u4f5c\u4e3a\u4e00\u9879\u91cd\u8981\u8d21\u732e\uff0c\u6211\u4eec\u91c7\u7528\u4e86 Llama \u6a21\u578b\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7efc\u5408\u7ba1\u9053\u6765\u4e3a 11 \u4e2a\u6570\u636e\u96c6\u7684\u56fe\u50cf\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff0c\u5171\u4ea7\u751f\u4e86 1,637,795 \u5bf9\u56fe\u50cf\u6587\u672c\uff0c\u540d\u4e3a\u201cIMD-11\u201d\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5728 https://github.com/FourierAI/IDEA \u4e0a\u53d1\u5e03\u3002", "author": "Zhipeng Ye et.al.", "authors": "Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang", "id": "2501.08816v1", "paper_url": "http://arxiv.org/abs/2501.08816v1", "repo": "null"}}