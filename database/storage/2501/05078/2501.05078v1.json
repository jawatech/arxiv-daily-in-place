{"2501.05078": {"publish_time": "2025-01-09", "title": "Analyzing Memorization in Large Language Models through the Lens of Model Attribution", "paper_summary": "Large Language Models (LLMs) are prevalent in modern applications but often\nmemorize training data, leading to privacy breaches and copyright issues.\nExisting research has mainly focused on posthoc analyses, such as extracting\nmemorized content or developing memorization metrics, without exploring the\nunderlying architectural factors that contribute to memorization. In this work,\nwe investigate memorization from an architectural lens by analyzing how\nattention modules at different layers impact its memorization and\ngeneralization performance. Using attribution techniques, we systematically\nintervene in the LLM architecture by bypassing attention modules at specific\nblocks while keeping other components like layer normalization and MLP\ntransformations intact. We provide theorems analyzing our intervention\nmechanism from a mathematical view, bounding the difference in layer outputs\nwith and without our attributions. Our theoretical and empirical analyses\nreveal that attention modules in deeper transformer blocks are primarily\nresponsible for memorization, whereas earlier blocks are crucial for the models\ngeneralization and reasoning capabilities. We validate our findings through\ncomprehensive experiments on different LLM families (Pythia and GPTNeo) and\nfive benchmark datasets. Our insights offer a practical approach to mitigate\nmemorization in LLMs while preserving their performance, contributing to safer\nand more ethical deployment in real world applications.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u73fe\u4ee3\u61c9\u7528\u4e2d\u5f88\u666e\u904d\uff0c\u4f46\u7d93\u5e38\u8a18\u4f4f\u8a13\u7df4\u8cc7\u6599\uff0c\u5c0e\u81f4\u96b1\u79c1\u6d29\u9732\u548c\u7248\u6b0a\u554f\u984c\u3002\u73fe\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4e8b\u5f8c\u5206\u6790\uff0c\u4f8b\u5982\u63d0\u53d6\u8a18\u61b6\u5167\u5bb9\u6216\u958b\u767c\u8a18\u61b6\u5ea6\u91cf\u6a19\u6e96\uff0c\u800c\u6c92\u6709\u63a2\u8a0e\u5c0e\u81f4\u8a18\u61b6\u7684\u5e95\u5c64\u67b6\u69cb\u56e0\u7d20\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5206\u6790\u4e0d\u540c\u5c64\u7d1a\u7684\u6ce8\u610f\u529b\u6a21\u7d44\u5982\u4f55\u5f71\u97ff\u5176\u8a18\u61b6\u548c\u6cdb\u5316\u6548\u80fd\uff0c\u5f9e\u67b6\u69cb\u89d2\u5ea6\u63a2\u8a0e\u8a18\u61b6\u3002\u4f7f\u7528\u6b78\u56e0\u6280\u8853\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u4ecb\u5165 LLM \u67b6\u69cb\uff0c\u65b9\u6cd5\u662f\u5728\u7279\u5b9a\u5340\u584a\u4e2d\u7e5e\u904e\u6ce8\u610f\u529b\u6a21\u7d44\uff0c\u540c\u6642\u4fdd\u6301\u5c64\u6b63\u898f\u5316\u548c MLP \u8f49\u63db\u7b49\u5176\u4ed6\u5143\u4ef6\u5b8c\u6574\u3002\u6211\u5011\u63d0\u4f9b\u5b9a\u7406\u5f9e\u6578\u5b78\u89d2\u5ea6\u5206\u6790\u6211\u5011\u7684\u4ecb\u5165\u6a5f\u5236\uff0c\u754c\u5b9a\u6709\u548c\u6c92\u6709\u6211\u5011\u7684\u6b78\u56e0\u6642\u5c64\u8f38\u51fa\u5dee\u7570\u3002\u6211\u5011\u7684\u7406\u8ad6\u548c\u5be6\u8b49\u5206\u6790\u986f\u793a\uff0c\u8f03\u6df1\u5c64Transformer\u5340\u584a\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u7d44\u4e3b\u8981\u8ca0\u8cac\u8a18\u61b6\uff0c\u800c\u8f03\u65e9\u671f\u7684\u5340\u584a\u5c0d\u65bc\u6a21\u578b\u7684\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u900f\u904e\u5c0d\u4e0d\u540c LLM \u5bb6\u65cf\uff08Pythia \u548c GPTNeo\uff09\u548c\u4e94\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u9032\u884c\u5168\u9762\u7684\u5be6\u9a57\u9a57\u8b49\u6211\u5011\u7684\u767c\u73fe\u3002\u6211\u5011\u7684\u898b\u89e3\u63d0\u4f9b\u4e86\u4e00\u7a2e\u5be6\u7528\u7684\u65b9\u6cd5\u4f86\u6e1b\u8f15 LLM \u4e2d\u7684\u8a18\u61b6\uff0c\u540c\u6642\u4fdd\u7559\u5176\u6548\u80fd\uff0c\u6709\u52a9\u65bc\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u66f4\u5b89\u5168\u3001\u66f4\u5408\u4e4e\u9053\u5fb7\u5730\u90e8\u7f72\u3002", "author": "Tarun Ram Menta et.al.", "authors": "Tarun Ram Menta, Susmit Agrawal, Chirag Agarwal", "id": "2501.05078v1", "paper_url": "http://arxiv.org/abs/2501.05078v1", "repo": "null"}}