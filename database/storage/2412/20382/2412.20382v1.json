{"2412.20382": {"publish_time": "2024-12-29", "title": "Natural Language Fine-Tuning", "paper_summary": "Large language model fine-tuning techniques typically depend on extensive\nlabeled data, external guidance, and feedback, such as human alignment, scalar\nrewards, and demonstration. However, in practical application, the scarcity of\nspecific knowledge poses unprecedented challenges to existing fine-tuning\ntechniques. In this paper, focusing on fine-tuning tasks in specific domains\nwith limited data, we introduce Natural Language Fine-Tuning (NLFT), which\nutilizes natural language for fine-tuning for the first time. By leveraging the\nstrong language comprehension capability of the target LM, NLFT attaches the\nguidance of natural language to the token-level outputs. Then, saliency tokens\nare identified with calculated probabilities. Since linguistic information is\neffectively utilized in NLFT, our proposed method significantly reduces\ntraining costs. It markedly enhances training efficiency, comprehensively\noutperforming reinforcement fine-tuning algorithms in accuracy, time-saving,\nand resource conservation. Additionally, on the macro level, NLFT can be viewed\nas a token-level fine-grained optimization of SFT, thereby efficiently\nreplacing the SFT process without the need for warm-up (as opposed to ReFT\nrequiring multiple rounds of warm-up with SFT). Compared to SFT, NLFT does not\nincrease the algorithmic complexity, maintaining O(n). Extensive experiments on\nthe GSM8K dataset demonstrate that NLFT, with only 50 data instances, achieves\nan accuracy increase that exceeds SFT by 219%. Compared to ReFT, the time\ncomplexity and space complexity of NLFT are reduced by 78.27% and 92.24%,\nrespectively. The superior technique of NLFT is paving the way for the\ndeployment of various innovative LLM fine-tuning applications when resources\nare limited at network edges.\n  Our code has been released at https://github.com/Julia-LiuJ/NLFT.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u6280\u672f\u901a\u5e38\u4f9d\u8d56\u4e8e\u5e7f\u6cdb\u7684\u6807\u8bb0\u6570\u636e\u3001\u5916\u90e8\u6307\u5bfc\u548c\u53cd\u9988\uff0c\u4f8b\u5982\u4eba\u5de5\u5bf9\u9f50\u3001\u6807\u91cf\u5956\u52b1\u548c\u6f14\u793a\u3002\u7136\u800c\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7279\u5b9a\u77e5\u8bc6\u7684\u7a00\u7f3a\u6027\u7ed9\u73b0\u6709\u7684\u5fae\u8c03\u6280\u672f\u5e26\u6765\u4e86\u524d\u6240\u672a\u6709\u7684\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u5229\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u5fae\u8c03\uff0c\u9488\u5bf9\u6570\u636e\u6709\u9650\u7684\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\u4efb\u52a1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u81ea\u7136\u8bed\u8a00\u5fae\u8c03 (NLFT)\uff0c\u8fd9\u662f\u9996\u6b21\u5229\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u5fae\u8c03\u3002\u901a\u8fc7\u5229\u7528\u76ee\u6807 LM \u5f3a\u5927\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0cNLFT \u5c06\u81ea\u7136\u8bed\u8a00\u7684\u6307\u5bfc\u9644\u52a0\u5230\u4ee4\u724c\u7ea7\u522b\u7684\u8f93\u51fa\u4e2d\u3002\u7136\u540e\uff0c\u4f7f\u7528\u8ba1\u7b97\u51fa\u7684\u6982\u7387\u8bc6\u522b\u663e\u7740\u4ee4\u724c\u3002\u7531\u4e8e\u8bed\u8a00\u4fe1\u606f\u5728 NLFT \u4e2d\u5f97\u5230\u4e86\u6709\u6548\u5229\u7528\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5927\u5927\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u3002\u5b83\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u51c6\u786e\u6027\u3001\u8282\u7701\u65f6\u95f4\u548c\u8d44\u6e90\u8282\u7ea6\u65b9\u9762\u5168\u9762\u4f18\u4e8e\u5f3a\u5316\u5fae\u8c03\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u5728\u5b8f\u89c2\u5c42\u9762\u4e0a\uff0cNLFT \u53ef\u4ee5\u88ab\u89c6\u4e3a SFT \u7684\u4ee4\u724c\u7ea7\u7ec6\u7c92\u5ea6\u4f18\u5316\uff0c\u4ece\u800c\u6709\u6548\u5730\u53d6\u4ee3 SFT \u8fc7\u7a0b\uff0c\u800c\u65e0\u9700\u9884\u70ed\uff08\u4e0e ReFT \u9700\u8981\u591a\u8f6e SFT \u9884\u70ed\u76f8\u53cd\uff09\u3002\u4e0e SFT \u76f8\u6bd4\uff0cNLFT \u4e0d\u4f1a\u589e\u52a0\u7b97\u6cd5\u590d\u6742\u5ea6\uff0c\u4fdd\u6301 O(n)\u3002\u5728 GSM8K \u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cNLFT \u4ec5\u4f7f\u7528 50 \u4e2a\u6570\u636e\u5b9e\u4f8b\uff0c\u5176\u51c6\u786e\u6027\u63d0\u5347\u8d85\u8fc7 SFT 219%\u3002\u4e0e ReFT \u76f8\u6bd4\uff0cNLFT \u7684\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u5206\u522b\u964d\u4f4e\u4e86 78.27% \u548c 92.24%\u3002\u5f53\u7f51\u7edc\u8fb9\u7f18\u7684\u8d44\u6e90\u6709\u9650\u65f6\uff0cNLFT \u7684\u5353\u8d8a\u6280\u672f\u4e3a\u5404\u79cd\u521b\u65b0 LLM \u5fae\u8c03\u5e94\u7528\u7a0b\u5e8f\u7684\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728 https://github.com/Julia-LiuJ/NLFT \u53d1\u5e03\u3002", "author": "Jia Liu et.al.", "authors": "Jia Liu, Yue Wang, Zhiqi Lin, Min Chen, Yixue Hao, Long Hu", "id": "2412.20382v1", "paper_url": "http://arxiv.org/abs/2412.20382v1", "repo": "https://github.com/julia-liuj/nlft"}}