{"2412.10151": {"publish_time": "2024-12-13", "title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation", "paper_summary": "We propose the VLR-Bench, a visual question answering (VQA) benchmark for\nevaluating vision language models (VLMs) based on retrieval augmented\ngeneration (RAG). Unlike existing evaluation datasets for external\nknowledge-based VQA, the proposed VLR-Bench includes five input passages. This\nallows testing of the ability to determine which passage is useful for\nanswering a given query, a capability lacking in previous research. In this\ncontext, we constructed a dataset of 32,000 automatically generated\ninstruction-following examples, which we denote as VLR-IF. This dataset is\nspecifically designed to enhance the RAG capabilities of VLMs by enabling them\nto learn how to generate appropriate answers based on input passages. We\nevaluated the validity of the proposed benchmark and training data and verified\nits performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3\nmodel. The proposed VLR-Bench and VLR-IF datasets are publicly available\nonline.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa VLR-Bench\uff0c\u4e00\u7a2e\u8996\u89ba\u554f\u7b54 (VQA) \u57fa\u6e96\uff0c\u7528\u65bc\u6839\u64da\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u8a55\u4f30\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\u3002\u8207\u73fe\u6709\u5916\u90e8\u77e5\u8b58\u5eab VQA \u8a55\u4f30\u8cc7\u6599\u96c6\u4e0d\u540c\uff0c\u6240\u63d0\u51fa\u7684 VLR-Bench \u5305\u542b\u4e94\u500b\u8f38\u5165\u6bb5\u843d\u3002\u9019\u5141\u8a31\u6e2c\u8a66\u78ba\u5b9a\u54ea\u500b\u6bb5\u843d\u6709\u52a9\u65bc\u56de\u7b54\u7279\u5b9a\u67e5\u8a62\u7684\u80fd\u529b\uff0c\u9019\u9805\u529f\u80fd\u5728\u5148\u524d\u7684\u7814\u7a76\u4e2d\u6709\u6240\u6b20\u7f3a\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u8cc7\u6599\u96c6\uff0c\u5305\u542b 32,000 \u500b\u81ea\u52d5\u7522\u751f\u7684\u9075\u5faa\u6307\u4ee4\u7bc4\u4f8b\uff0c\u6211\u5011\u5c07\u5176\u8868\u793a\u70ba VLR-IF\u3002\u6b64\u8cc7\u6599\u96c6\u7279\u5225\u8a2d\u8a08\u7528\u65bc\u589e\u5f37 VLM \u7684 RAG \u529f\u80fd\uff0c\u8b93\u5b83\u5011\u80fd\u5920\u5b78\u7fd2\u5982\u4f55\u6839\u64da\u8f38\u5165\u6bb5\u843d\u7522\u751f\u9069\u7576\u7684\u7b54\u6848\u3002\u6211\u5011\u8a55\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u57fa\u6e96\u548c\u8a13\u7df4\u8cc7\u6599\u7684\u6709\u6548\u6027\uff0c\u4e26\u4f7f\u7528\u6700\u5148\u9032\u7684\u57fa\u65bc Llama3 \u7684 VLM\uff0c\u5373 Llava-Llama-3 \u6a21\u578b\uff0c\u9a57\u8b49\u5176\u6548\u80fd\u3002\u6240\u63d0\u51fa\u7684 VLR-Bench \u548c VLR-IF \u8cc7\u6599\u96c6\u5df2\u516c\u958b\u5728\u7dda\u4e0a\u3002", "author": "Hyeonseok Lim et.al.", "authors": "Hyeonseok Lim, Dongjae Shin, Seohyun Song, Inho Won, Minjun Kim, Junghun Yuk, Haneol Jang, KyungTae Lim", "id": "2412.10151v1", "paper_url": "http://arxiv.org/abs/2412.10151v1", "repo": "null"}}