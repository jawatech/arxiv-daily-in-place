{"2412.07755": {"publish_time": "2024-12-10", "title": "SAT: Spatial Aptitude Training for Multimodal Language Models", "paper_summary": "Spatial perception is a fundamental component of intelligence. While many\nstudies highlight that large multimodal language models (MLMs) struggle to\nreason about space, they only test for static spatial reasoning, such as\ncategorizing the relative positions of objects. Meanwhile, real-world\ndeployment requires dynamic capabilities like perspective-taking and egocentric\naction recognition. As a roadmap to improving spatial intelligence, we\nintroduce SAT, Spatial Aptitude Training, which goes beyond static relative\nobject position questions to the more dynamic tasks. SAT contains 218K\nquestion-answer pairs for 22K synthetic scenes across a training and testing\nset. Generated using a photo-realistic physics engine, our dataset can be\narbitrarily scaled and easily extended to new actions, scenes, and 3D assets.\nWe find that even MLMs that perform relatively well on static questions\nstruggle to accurately answer dynamic spatial questions. Further, we show that\nSAT instruction-tuning data improves not only dynamic spatial reasoning on SAT,\nbut also zero-shot performance on existing real-image spatial benchmarks:\n$23\\%$ on CVBench, $8\\%$ on the harder BLINK benchmark, and $18\\%$ on VSR. When\ninstruction-tuned on SAT, our 13B model matches larger proprietary MLMs like\nGPT4-V and Gemini-3-1.0 in spatial reasoning. Our data/code is available at\nhttp://arijitray1993.github.io/SAT/ .", "paper_summary_zh": "\u7a7a\u9593\u611f\u77e5\u662f\u667a\u6167\u7684\u57fa\u672c\u7d44\u6210\u90e8\u5206\u3002\u96d6\u7136\u8a31\u591a\u7814\u7a76\u5f37\u8abf\u5927\u578b\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b (MLM) \u96e3\u4ee5\u63a8\u8ad6\u7a7a\u9593\uff0c\u4f46\u5b83\u5011\u50c5\u6e2c\u8a66\u975c\u614b\u7a7a\u9593\u63a8\u7406\uff0c\u4f8b\u5982\u5206\u985e\u7269\u9ad4\u7684\u76f8\u5c0d\u4f4d\u7f6e\u3002\u540c\u6642\uff0c\u73fe\u5be6\u4e16\u754c\u7684\u90e8\u7f72\u9700\u8981\u52d5\u614b\u80fd\u529b\uff0c\u4f8b\u5982\u89c0\u9ede\u63a1\u53d6\u548c\u81ea\u6211\u4e2d\u5fc3\u52d5\u4f5c\u8b58\u5225\u3002\u4f5c\u70ba\u6539\u5584\u7a7a\u9593\u667a\u6167\u7684\u8def\u7dda\u5716\uff0c\u6211\u5011\u5f15\u5165\u4e86 SAT\uff08\u7a7a\u9593\u80fd\u529b\u8a13\u7df4\uff09\uff0c\u5b83\u8d85\u8d8a\u4e86\u975c\u614b\u76f8\u5c0d\u7269\u9ad4\u4f4d\u7f6e\u554f\u984c\uff0c\u8f49\u5411\u66f4\u52d5\u614b\u7684\u4efb\u52d9\u3002SAT \u5305\u542b 218K \u500b\u554f\u984c\u89e3\u7b54\u5c0d\uff0c\u9069\u7528\u65bc\u8a13\u7df4\u548c\u6e2c\u8a66\u96c6\u4e2d 22K \u500b\u5408\u6210\u5834\u666f\u3002\u6211\u5011\u7684\u6578\u64da\u96c6\u4f7f\u7528\u903c\u771f\u7684\u7269\u7406\u5f15\u64ce\u751f\u6210\uff0c\u53ef\u4ee5\u4efb\u610f\u7e2e\u653e\uff0c\u4e26\u8f15\u9b06\u64f4\u5c55\u5230\u65b0\u7684\u52d5\u4f5c\u3001\u5834\u666f\u548c 3D \u8cc7\u7522\u3002\u6211\u5011\u767c\u73fe\uff0c\u5373\u4f7f\u5728\u975c\u614b\u554f\u984c\u4e0a\u8868\u73fe\u76f8\u5c0d\u826f\u597d\u7684 MLM \u4e5f\u96e3\u4ee5\u6e96\u78ba\u56de\u7b54\u52d5\u614b\u7a7a\u9593\u554f\u984c\u3002\u6b64\u5916\uff0c\u6211\u5011\u8868\u660e SAT \u6307\u4ee4\u8abf\u6574\u6578\u64da\u4e0d\u50c5\u6539\u5584\u4e86 SAT \u4e0a\u7684\u52d5\u614b\u7a7a\u9593\u63a8\u7406\uff0c\u9084\u6539\u5584\u4e86\u73fe\u6709\u771f\u5be6\u5716\u50cf\u7a7a\u9593\u57fa\u6e96\u4e0a\u7684\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\uff1aCVBench \u4e0a\u7684 23%\uff0c\u8f03\u56f0\u96e3\u7684 BLINK \u57fa\u6e96\u4e0a\u7684 8%\uff0c\u4ee5\u53ca VSR \u4e0a\u7684 18%\u3002\u7576\u5728 SAT \u4e0a\u9032\u884c\u6307\u4ee4\u8abf\u6574\u5f8c\uff0c\u6211\u5011\u7684 13B \u6a21\u578b\u5728\u7a7a\u9593\u63a8\u7406\u4e2d\u8207 GPT4-V \u548c Gemini-3-1.0 \u7b49\u8f03\u5927\u7684\u5c08\u6709 MLM \u76f8\u5339\u914d\u3002\u6211\u5011\u7684\u6578\u64da/\u7a0b\u5f0f\u78bc\u53ef\u5728 http://arijitray1993.github.io/SAT/ \u53d6\u5f97\u3002", "author": "Arijit Ray et.al.", "authors": "Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A. Plummer, Ranjay Krishna, Kuo-Hao Zeng, Kate Saenko", "id": "2412.07755v1", "paper_url": "http://arxiv.org/abs/2412.07755v1", "repo": "null"}}