{"2408.00244": {"publish_time": "2024-08-01", "title": "Enhanced Structured State Space Models via Grouped FIR Filtering and Attention Sink Mechanisms", "paper_summary": "Structured State Space Models (SSMs) have emerged as compelling alternatives\nto Transformer architectures, offering linear-time complexity and superior\nperformance in various sequence modeling tasks. Despite their advantages, SSMs\nlike the original Mamba-2 face training difficulties due to the sensitivities\nintroduced by the extended series of recurrent matrix multiplications. In this\npaper, we propose an advanced architecture that mitigates these challenges by\ndecomposing A-multiplications into multiple groups and optimizing positional\nencoding through Grouped Finite Impulse Response (FIR) filtering. This new\nstructure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable\nmatrices for efficient computation. Furthermore, inspired by the \"attention\nsink\" phenomenon identified in streaming language models, we incorporate a\nsimilar mechanism to enhance the stability and performance of our model over\nextended sequences. Our approach further bridges the gap between SSMs and\nTransformer architectures, offering a viable path forward for scalable and\nhigh-performing sequence modeling.", "paper_summary_zh": "\u7d50\u69cb\u5316\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM) \u5df2\u6210\u70ba Transformer \u67b6\u69cb\u7684\u5f15\u4eba\u6ce8\u76ee\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u5404\u7a2e\u5e8f\u5217\u5efa\u6a21\u4efb\u52d9\u4e2d\u63d0\u4f9b\u7dda\u6027\u6642\u9593\u8907\u96dc\u5ea6\u548c\u5353\u8d8a\u7684\u6548\u80fd\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u512a\u9ede\uff0c\u4f46\u50cf\u539f\u59cb Mamba-2 \u9019\u6a23\u7684 SSM \u6703\u56e0\u905e\u8ff4\u77e9\u9663\u4e58\u6cd5\u7684\u5ef6\u4f38\u5e8f\u5217\u6240\u5e36\u4f86\u7684\u654f\u611f\u6027\u800c\u9762\u81e8\u8a13\u7df4\u96e3\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5148\u9032\u7684\u67b6\u69cb\uff0c\u900f\u904e\u5c07 A \u4e58\u6cd5\u5206\u89e3\u6210\u591a\u500b\u7fa4\u7d44\u4e26\u900f\u904e\u7fa4\u7d44\u6709\u9650\u8108\u885d\u97ff\u61c9 (FIR) \u6ffe\u6ce2\u6700\u4f73\u5316\u4f4d\u7f6e\u7de8\u78bc\u4f86\u6e1b\u8f15\u9019\u4e9b\u6311\u6230\u3002\u9019\u500b\u65b0\u7684\u7d50\u69cb\u7a31\u70ba\u7fa4\u7d44 FIR \u589e\u5f37 SSM (GFSSM)\uff0c\u63a1\u7528\u534a\u53ef\u5206\u96e2\u77e9\u9663\u9032\u884c\u6709\u6548\u7387\u7684\u904b\u7b97\u3002\u6b64\u5916\uff0c\u53d7\u5230\u4e32\u6d41\u8a9e\u8a00\u6a21\u578b\u4e2d\u8b58\u5225\u51fa\u7684\u300c\u6ce8\u610f\u529b\u63a5\u6536\u5668\u300d\u73fe\u8c61\u7684\u555f\u767c\uff0c\u6211\u5011\u7d0d\u5165\u4e86\u4e00\u500b\u985e\u4f3c\u7684\u6a5f\u5236\u4f86\u589e\u5f37\u6211\u5011\u6a21\u578b\u5728\u5ef6\u4f38\u5e8f\u5217\u4e2d\u7684\u7a69\u5b9a\u6027\u548c\u6548\u80fd\u3002\u6211\u5011\u7684\u505a\u6cd5\u9032\u4e00\u6b65\u7e2e\u5c0f\u4e86 SSM \u8207 Transformer \u67b6\u69cb\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u70ba\u53ef\u64f4\u5145\u4e14\u9ad8\u57f7\u884c\u6548\u80fd\u7684\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u524d\u9032\u9053\u8def\u3002", "author": "Tian Meng et.al.", "authors": "Tian Meng, Yang Tao, Wuliang Yin", "id": "2408.00244v1", "paper_url": "http://arxiv.org/abs/2408.00244v1", "repo": "null"}}