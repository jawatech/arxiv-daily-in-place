{"2407.20557": {"publish_time": "2024-07-30", "title": "CELLM: An Efficient Communication in Large Language Models Training for Federated Learning", "paper_summary": "Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training.", "paper_summary_zh": "\u806f\u90a6\u5b78\u7fd2 (FL) \u662f\u4e00\u7a2e\u65b0\u8208\u7684\u6a21\u578b\u8a13\u7df4\u7bc4\u4f8b\uff0c\u5176\u4e2d\u7528\u6236\u7aef\u88dd\u7f6e\u5354\u4f5c\u8a13\u7df4\u6a21\u578b\uff0c\u537b\u5f9e\u4e0d\u5f59\u7e3d\u5176\u8cc7\u6599\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u6b64\u65b9\u6848\u50c5\u900f\u904e\u5c07\u6a21\u578b\u6b0a\u91cd\u66f4\u65b0\u50b3\u9001\u81f3\u4e2d\u592e\u4f3a\u670d\u5668\u7684\u65b9\u5f0f\uff0c\u63d0\u4f9b\u4f7f\u7528\u8005\u6f5b\u5728\u7684\u96b1\u79c1\u548c\u5b89\u5168\u6027\u512a\u52e2\uff0c\u9019\u8207\u76f4\u63a5\u50b3\u9001\u548c\u5f59\u7e3d\u8cc7\u6599\u7684\u50b3\u7d71\u6a5f\u5668\u5b78\u7fd2 (ML) \u8a13\u7df4\u76f8\u53cd\u3002\u7136\u800c\uff0cFL \u8a13\u7df4\u6703\u53d7\u5230\u7d71\u8a08\u7570\u8cea\u6027\u7684\u5f71\u97ff\uff0c\u56e0\u70ba\u7528\u6236\u7aef\u53ef\u80fd\u64c1\u6709\u4e0d\u540c\u7684\u672c\u5730\u8cc7\u6599\u5206\u4f48\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u70ba\u6b64\u7570\u8cea\u6027\u554f\u984c\u63d0\u4f9b\u4e86\u6f5b\u5728\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u56e0\u70ba\u5b83\u5011\u59cb\u7d42\u88ab\u8b49\u660e\u80fd\u5920\u5728\u5927\u91cf\u7684\u96dc\u8a0a\u8cc7\u6599\u4e2d\u5b78\u7fd2\u3002\u5118\u7ba1 LLM \u662f\u89e3\u6c7a\u975e I.I.D. \u7528\u6236\u7aef\u4e00\u81f4\u6027\u554f\u984c\u7684\u6709\u671b\u767c\u5c55\uff0c\u4f46\u806f\u90a6\u8a2d\u5b9a\u4e2d\u6703\u52a0\u5287 FL \u7684\u53e6\u5916\u5169\u500b\u74f6\u9838\uff1a\u6709\u9650\u7684\u672c\u5730\u904b\u7b97\u548c\u6602\u8cb4\u7684\u901a\u8a0a\u3002\u672c\u8ad6\u6587\u65e8\u5728\u70ba FL \u4e2d\u7684 LLM \u958b\u767c\u9ad8\u6548\u7684\u8a13\u7df4\u65b9\u6cd5\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63a1\u7528\u5169\u7a2e\u95dc\u9375\u6280\u8853\u4f86\u5be6\u73fe\u9ad8\u6548\u8a13\u7df4\u3002\u9996\u5148\uff0c\u6211\u5011\u4f7f\u7528\u4f4e\u79e9\u9069\u61c9 (LoRA) \u4f86\u964d\u4f4e\u672c\u5730\u6a21\u578b\u8a13\u7df4\u7684\u904b\u7b97\u8ca0\u8f09\u3002\u5176\u6b21\uff0c\u6211\u5011\u5728\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4e2d\u50b3\u9001\u7a00\u758f\u66f4\u65b0\uff0c\u4ee5\u5927\u5e45\u964d\u4f4e\u901a\u8a0a\u6210\u672c\u3002\u7d9c\u5408\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5c07\u901a\u8a0a\u6210\u672c\u964d\u4f4e\u4e86\u6700\u591a 10 \u500d\uff08\u76f8\u8f03\u65bc\u539f\u751f\u7684 LoRA\uff09\uff0c\u4e26\u6bd4\u66f4\u8907\u96dc\u7684\u7a00\u758f LoRA \u57fa\u6e96\u964d\u4f4e\u4e86\u6700\u591a 5 \u500d\uff0c\u540c\u6642\u7372\u5f97\u66f4\u9ad8\u7684\u6548\u7528\u3002\u6211\u5011\u5f37\u8abf\u4ed4\u7d30\u61c9\u7528\u7a00\u758f\u6027\u4ee5\u53ca\u6311\u9078\u6709\u6548\u7684\u79e9\u548c\u7a00\u758f\u6027\u914d\u7f6e\u5c0d\u65bc\u806f\u90a6 LLM \u8a13\u7df4\u7684\u91cd\u8981\u6027\u3002", "author": "Raja Vavekanand et.al.", "authors": "Raja Vavekanand, Kira Sam", "id": "2407.20557v1", "paper_url": "http://arxiv.org/abs/2407.20557v1", "repo": "null"}}