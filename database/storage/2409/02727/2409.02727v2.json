{"2409.02727": {"publish_time": "2024-09-04", "title": "Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?", "paper_summary": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u751f\u6210\u4efb\u52d9\u4e2d\u7684\u986f\u8457\u9032\u5c55\uff0c\u5c0e\u81f4\u4e86\u8d8a\u4f86\u8d8a\u591a\u7814\u7a76\u63a2\u7d22\u57fa\u65bc LLM \u7684\u5d4c\u5165\u6a21\u578b\u3002\n\u96d6\u7136\u9019\u4e9b\u6a21\u578b\u63a1\u7528\u4e0d\u540c\u7684\u532f\u7e3d\u548c\u6ce8\u610f\u529b\u7b56\u7565\uff0c\u5df2\u5728\u516c\u958b\u5d4c\u5165\u57fa\u6e96\u6e2c\u8a66\u4e2d\u9054\u5230\u6700\u5148\u9032\u7684\u6027\u80fd\uff0c\u4f46\u95dc\u65bc\u4ec0\u9ebc\u69cb\u6210\u4e86\u57fa\u65bc LLM \u7684\u5d4c\u5165\u6a21\u578b\u7684\u6709\u6548\u8a2d\u8a08\uff0c\u4ecd\u7136\u5b58\u5728\u7591\u554f\u3002\n\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u5728\u4e0d\u540c\u7684\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\uff0c\u4f7f\u7528\u4e0d\u540c\u7684 LLM \u57fa\u790e\u6a21\u578b\u6216\u8a13\u7df4\u8a2d\u5b9a\u3002\n\u6b64\u5916\uff0c\u5c0d\u516c\u958b\u5d4c\u5165\u57fa\u6e96\u6e2c\u8a66\u7684\u8a55\u4f30\u901a\u5e38\u672a\u80fd\u5831\u544a\u7d71\u8a08\u986f\u8457\u6027\uff0c\u9019\u4f7f\u5f97\u96e3\u4ee5\u78ba\u5b9a\u54ea\u4e9b\u8a2d\u8a08\u771f\u6b63\u6709\u52a9\u65bc\u6700\u7d42\u6027\u80fd\u3002\n\u9019\u4f7f\u5f97\u5f9e\u696d\u8005\u5c0b\u6c42\u57fa\u65bc LLM \u7684\u5d4c\u5165\u6a21\u578b\u7684\u6700\u4f73\u8a13\u7df4\u914d\u65b9\u8b8a\u5f97\u8907\u96dc\u3002\n\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u901a\u904e\u4f7f\u7528\u76f8\u540c\u7684\u8a13\u7df4\u8cc7\u6599\u548c\u57fa\u790e\u6a21\u578b\uff0c\u4f46\u532f\u7e3d\u548c\u6ce8\u610f\u529b\u7b56\u7565\u4e0d\u540c\u7684\u8a13\u7df4\u4e00\u7cfb\u5217\u57fa\u65bc LLM \u7684\u5d4c\u5165\u6a21\u578b\uff0c\u9032\u884c\u4e86\u5927\u898f\u6a21\u5be6\u9a57\u3002\n\u7d50\u679c\u8868\u660e\uff0c\u6c92\u6709\u901a\u7528\u7684\u89e3\u6c7a\u65b9\u6848\uff1a\u96d6\u7136\u96d9\u5411\u6ce8\u610f\u529b\u548c\u4e00\u500b\u984d\u5916\u7684\u53ef\u8a13\u7df4\u532f\u7e3d\u5c64\u5728\u6587\u672c\u76f8\u4f3c\u6027\u548c\u8cc7\u8a0a\u6aa2\u7d22\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5b83\u5011\u4e26\u6c92\u6709\u986f\u8457\u8d85\u904e\u5728\u805a\u985e\u548c\u5206\u985e\u4efb\u52d9\u4e2d\u8f03\u7c21\u55ae\u7684\u8a2d\u8a08\uff0c\u4f8b\u5982 EOS \u6700\u5f8c\u6a19\u8a18\u532f\u7e3d\u548c\u9810\u8a2d\u56e0\u679c\u6ce8\u610f\u529b\u3002\n\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u532f\u7e3d\u7b56\u7565\uff0c\u591a\u5c64\u53ef\u8a13\u7df4\u532f\u7e3d\uff0c\u5b83\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u7db2\u8def\u8f49\u63db\u6240\u6709\u96b1\u85cf\u5c64\u7684\u8f38\u51fa\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6700\u5f8c\u4e00\u5c64\u3002\n\u8207\u73fe\u6709\u7684\u532f\u7e3d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u9019\u7a2e\u65b9\u6cd5\u5728\u6587\u672c\u76f8\u4f3c\u6027\u548c\u6aa2\u7d22\u4efb\u52d9\u4e2d\u88ab\u8b49\u660e\u5177\u6709\u7d71\u8a08\u512a\u52e2\u3002\n\u7e3d\u7684\u4f86\u8aaa\uff0c\u672c\u6587\u95e1\u660e\u4e86\u57fa\u65bc LLM \u7684\u5d4c\u5165\u6a21\u578b\u7684\u6709\u6548\u8a13\u7df4\u7b56\u7565\u3002", "author": "Yixuan Tang et.al.", "authors": "Yixuan Tang, Yi Yang", "id": "2409.02727v2", "paper_url": "http://arxiv.org/abs/2409.02727v2", "repo": "https://github.com/yixuantt/poolingandattn"}}