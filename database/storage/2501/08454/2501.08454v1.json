{"2501.08454": {"publish_time": "2025-01-14", "title": "Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack", "paper_summary": "Large language models (LLMs) have become essential digital task assistance\ntools. Their training relies heavily on the collection of vast amounts of data,\nwhich may include copyright-protected or sensitive information. Recent studies\non the detection of pretraining data in LLMs have primarily focused on\nsentence-level or paragraph-level membership inference attacks (MIAs), usually\ninvolving probability analysis of the target model prediction tokens. However,\nthe proposed methods often demonstrate poor performance, specifically in terms\nof accuracy, failing to account for the semantic importance of textual content\nand word significance. To address these shortcomings, we propose Tag&Tab, a\nnovel approach for detecting data that has been used as part of the LLM\npretraining. Our method leverages advanced natural language processing (NLP)\ntechniques to tag keywords in the input text - a process we term Tagging. Then,\nthe LLM is used to obtain the probabilities of these keywords and calculate\ntheir average log-likelihood to determine input text membership, a process we\nrefer to as Tabbing. Our experiments on three benchmark datasets (BookMIA,\nMIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate\nan average increase in the AUC scores ranging from 4.1% to 12.1% over\nstate-of-the-art methods. Tag&Tab not only sets a new standard for data leakage\ndetection in LLMs, but its outstanding performance is a testament to the\nimportance of words in MIAs on LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u91cd\u8981\u7684\u6578\u4f4d\u5de5\u4f5c\u8f14\u52a9\u5de5\u5177\u3002\u5176\u8a13\u7df4\u4ef0\u8cf4\u5927\u91cf\u8cc7\u6599\u7684\u8490\u96c6\uff0c\u5176\u4e2d\u53ef\u80fd\u5305\u542b\u53d7\u8457\u4f5c\u6b0a\u4fdd\u8b77\u6216\u654f\u611f\u7684\u8cc7\u8a0a\u3002\u8fd1\u671f\u95dc\u65bc LLM \u4e2d\u9810\u8a13\u7df4\u8cc7\u6599\u5075\u6e2c\u7684\u7814\u7a76\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u53e5\u5b50\u5c64\u7d1a\u6216\u6bb5\u843d\u5c64\u7d1a\u7684\u6210\u54e1\u8eab\u5206\u63a8\u8ad6\u653b\u64ca (MIA)\uff0c\u901a\u5e38\u6d89\u53ca\u76ee\u6a19\u6a21\u578b\u9810\u6e2c\u6b0a\u6756\u7684\u6a5f\u7387\u5206\u6790\u3002\u7136\u800c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u7d93\u5e38\u5c55\u73fe\u4e0d\u4f73\u7684\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u6e96\u78ba\u5ea6\u65b9\u9762\uff0c\u7121\u6cd5\u8003\u91cf\u6587\u5b57\u5167\u5bb9\u7684\u8a9e\u610f\u91cd\u8981\u6027\u8207\u5b57\u8a5e\u610f\u7fa9\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9ede\uff0c\u6211\u5011\u63d0\u51fa Tag&Tab\uff0c\u4e00\u7a2e\u7528\u65bc\u5075\u6e2c\u8cc7\u6599\u662f\u5426\u66fe\u7528\u65bc LLM \u9810\u8a13\u7df4\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u9032\u968e\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u6280\u8853\uff0c\u6a19\u8a18\u8f38\u5165\u6587\u5b57\u4e2d\u7684\u95dc\u9375\u5b57\uff0c\u9019\u500b\u7a0b\u5e8f\u7a31\u4e4b\u70ba\u6a19\u8a18\u3002\u63a5\u8457\uff0c\u4f7f\u7528 LLM \u53d6\u5f97\u9019\u4e9b\u95dc\u9375\u5b57\u7684\u6a5f\u7387\uff0c\u4e26\u8a08\u7b97\u5176\u5e73\u5747\u5c0d\u6578\u4f3c\u7136\u503c\uff0c\u4ee5\u5224\u5b9a\u8f38\u5165\u6587\u5b57\u7684\u6210\u54e1\u8eab\u5206\uff0c\u9019\u500b\u7a0b\u5e8f\u6211\u5011\u7a31\u4e4b\u70ba\u6a19\u7c64\u5316\u3002\u6211\u5011\u91dd\u5c0d\u4e09\u500b\u57fa\u6e96\u8cc7\u6599\u96c6 (BookMIA\u3001MIMIR \u548c Pile) \u9032\u884c\u5be6\u9a57\uff0c\u4ee5\u53ca\u6578\u500b\u4e0d\u540c\u5927\u5c0f\u7684\u958b\u6e90 LLM\uff0c\u7d50\u679c\u986f\u793a\u8207\u73fe\u6709\u6280\u8853\u76f8\u6bd4\uff0cAUC \u5206\u6578\u5e73\u5747\u63d0\u5347 4.1% \u81f3 12.1%\u3002Tag&Tab \u4e0d\u50c5\u70ba LLM \u4e2d\u7684\u8cc7\u6599\u5916\u6d29\u5075\u6e2c\u6a39\u7acb\u65b0\u6a19\u6e96\uff0c\u5176\u5091\u51fa\u7684\u6548\u80fd\u4e5f\u8b49\u660e\u4e86\u5b57\u8a5e\u5728\u91dd\u5c0d LLM \u9032\u884c MIA \u7684\u91cd\u8981\u6027\u3002", "author": "Sagiv Antebi et.al.", "authors": "Sagiv Antebi, Edan Habler, Asaf Shabtai, Yuval Elovici", "id": "2501.08454v1", "paper_url": "http://arxiv.org/abs/2501.08454v1", "repo": "null"}}