{"2405.02933": {"publish_time": "2024-05-05", "title": "Relay Decoding: Concatenating Large Language Models for Machine Translation", "paper_summary": "Leveraging large language models for machine translation has demonstrated\npromising results. However, it does require the large language models to\npossess the capability of handling both the source and target languages in\nmachine translation. When it is challenging to find large models that support\nthe desired languages, resorting to continuous learning methods becomes a\ncostly endeavor. To mitigate these expenses, we propose an innovative approach\ncalled RD (Relay Decoding), which entails concatenating two distinct large\nmodels that individually support the source and target languages. By\nincorporating a simple mapping layer to facilitate the connection between these\ntwo models and utilizing a limited amount of parallel data for training, we\nsuccessfully achieve superior results in the machine translation task.\nExperimental results conducted on the Multi30k and WikiMatrix datasets validate\nthe effectiveness of our proposed method.", "paper_summary_zh": "", "author": "Chengpeng Fu et.al.", "authors": "Chengpeng Fu,Xiaocheng Feng,Yichong Huang,Wenshuai Huo,Baohang Li,Hui Wang,Bin Qin,Ting Liu", "id": "2405.02933v1", "paper_url": "http://arxiv.org/abs/2405.02933v1", "repo": "null"}}