{"2412.11919": {"publish_time": "2024-12-16", "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation", "paper_summary": "Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5e38\u5e38\u6703\u51fa\u73fe\u5e7b\u89ba\u3002\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u65b9\u6cd5\u662f\u7d0d\u5165\u5916\u90e8\u77e5\u8b58\uff0c\u4f46\u73fe\u6709\u65b9\u6cd5\u4ecd\u7136\u9762\u81e8\u5e7e\u500b\u9650\u5236\uff1a\u7368\u7acb\u6aa2\u7d22\u5668\u7684\u984d\u5916\u90e8\u7f72\u6210\u672c\u3001\u6aa2\u7d22\u7684\u6587\u5b57\u7247\u6bb5\u4e2d\u91cd\u8907\u7684\u8f38\u5165\u4ee3\u78bc\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6aa2\u7d22\u548c\u751f\u6210\u7684\u806f\u5408\u6700\u4f73\u5316\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa \\textbf{RetroLLM}\uff0c\u4e00\u500b\u7d71\u4e00\u7684\u67b6\u69cb\uff0c\u5c07\u6aa2\u7d22\u548c\u751f\u6210\u6574\u5408\u5230\u4e00\u500b\u55ae\u4e00\u7684\u3001\u6709\u51dd\u805a\u529b\u7684\u904e\u7a0b\u4e2d\uff0c\u4f7f LLM \u80fd\u5920\u76f4\u63a5\u5f9e\u8a9e\u6599\u5eab\u4e2d\u751f\u6210\u7d30\u7dfb\u7684\u8b49\u64da\uff0c\u4e26\u9032\u884c\u7d04\u675f\u89e3\u78bc\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u6e1b\u8f15\u53d7\u7d04\u675f\u8b49\u64da\u751f\u6210\u904e\u7a0b\u4e2d\u932f\u8aa4\u7684\u4fee\u526a\uff0c\u6211\u5011\u5f15\u5165\u4e86 (1) \u5c64\u7d1a\u5f0f FM-Index \u7d04\u675f\uff0c\u5728\u751f\u6210\u8b49\u64da\u4e4b\u524d\u7522\u751f\u8a9e\u6599\u5eab\u7d04\u675f\u7684\u7dda\u7d22\uff0c\u4ee5\u8b58\u5225\u76f8\u95dc\u6587\u4ef6\u5b50\u96c6\uff0c\u6e1b\u5c11\u4e0d\u76f8\u95dc\u7684\u89e3\u78bc\u7a7a\u9593\uff1b\u4ee5\u53ca (2) \u524d\u77bb\u6027\u7684\u7d04\u675f\u89e3\u78bc\u7b56\u7565\uff0c\u8003\u616e\u672a\u4f86\u5e8f\u5217\u7684\u76f8\u95dc\u6027\uff0c\u4ee5\u63d0\u9ad8\u8b49\u64da\u6e96\u78ba\u6027\u3002\u5728\u4e94\u500b\u958b\u653e\u9818\u57df QA \u8cc7\u6599\u96c6\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u8b49\u660e\u4e86 RetroLLM \u5728\u9818\u57df\u5167\u548c\u9818\u57df\u5916\u4efb\u52d9\u4e2d\u7684\u512a\u7570\u6027\u80fd\u3002\u7a0b\u5f0f\u78bc\u53ef\u4ee5\u5728 \\url{https://github.com/sunnynexus/RetroLLM} \u53d6\u5f97\u3002", "author": "Xiaoxi Li et.al.", "authors": "Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou", "id": "2412.11919v1", "paper_url": "http://arxiv.org/abs/2412.11919v1", "repo": "https://github.com/sunnynexus/retrollm"}}