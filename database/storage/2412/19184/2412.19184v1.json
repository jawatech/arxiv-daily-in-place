{"2412.19184": {"publish_time": "2024-12-26", "title": "Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for Enhanced Image-Text Matching", "paper_summary": "With the rapid development of multimodal learning, the image-text matching\ntask, as a bridge connecting vision and language, has become increasingly\nimportant. Based on existing research, this study proposes an innovative visual\nsemantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic\nEmbedding (MH-CVSE). This model introduces a multi-head self-attention\nmechanism based on the consensus-aware visual semantic embedding model (CVSE)\nto capture information in multiple subspaces in parallel, significantly\nenhancing the model's ability to understand and represent the complex\nrelationship between images and texts. In addition, we adopt a parameterized\nfeature fusion strategy to flexibly integrate feature information at different\nlevels, further improving the model's expressive power. In terms of loss\nfunction design, the MH-CVSE model adopts a dynamic weight adjustment strategy\nto dynamically adjust the weight according to the loss value itself, so that\nthe model can better balance the contribution of different loss terms during\ntraining. At the same time, we introduce a cosine annealing learning rate\nstrategy to help the model converge more stably in the later stages of\ntraining. Extensive experimental verification on the Flickr30k dataset shows\nthat the MH-CVSE model achieves better performance than previous methods in\nboth bidirectional image and text retrieval tasks, fully demonstrating its\neffectiveness and superiority.", "paper_summary_zh": "\u96a8\u8457\u591a\u6a21\u614b\u5b78\u7fd2\u7684\u5feb\u901f\u767c\u5c55\uff0c\u4f5c\u70ba\u9023\u63a5\u8996\u89ba\u8207\u8a9e\u8a00\u6a4b\u6a11\u7684\u5716\u50cf\u6587\u5b57\u5339\u914d\u4efb\u52d9\uff0c\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u672c\u7814\u7a76\u57fa\u65bc\u73fe\u6709\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u8996\u89ba\u8a9e\u7fa9\u5d4c\u5165\u6a21\u578b\uff0c\u591a\u982d\u5171\u8b58\u611f\u77e5\u8996\u89ba\u8a9e\u7fa9\u5d4c\u5165\uff08MH-CVSE\uff09\u3002\u8a72\u6a21\u578b\u5728\u5171\u8b58\u611f\u77e5\u8996\u89ba\u8a9e\u7fa9\u5d4c\u5165\u6a21\u578b\uff08CVSE\uff09\u7684\u57fa\u790e\u4e0a\uff0c\u5f15\u5165\u591a\u982d\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4e26\u884c\u6355\u6349\u591a\u500b\u5b50\u7a7a\u9593\u4e2d\u7684\u8cc7\u8a0a\uff0c\u5927\u5e45\u63d0\u5347\u6a21\u578b\u7406\u89e3\u548c\u8868\u5fb5\u5716\u50cf\u8207\u6587\u5b57\u9593\u8907\u96dc\u95dc\u4fc2\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a1\u7528\u53c3\u6578\u5316\u7684\u7279\u5fb5\u878d\u5408\u7b56\u7565\uff0c\u9748\u6d3b\u878d\u5408\u4e0d\u540c\u5c64\u7d1a\u7684\u7279\u5fb5\u8cc7\u8a0a\uff0c\u9032\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u7684\u8868\u73fe\u80fd\u529b\u3002\u5728\u640d\u5931\u51fd\u6578\u8a2d\u8a08\u65b9\u9762\uff0cMH-CVSE \u6a21\u578b\u63a1\u7528\u52d5\u614b\u6b0a\u91cd\u8abf\u6574\u7b56\u7565\uff0c\u6839\u64da\u640d\u5931\u503c\u672c\u8eab\u52d5\u614b\u8abf\u6574\u6b0a\u91cd\uff0c\u4f7f\u5f97\u6a21\u578b\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u80fd\u66f4\u597d\u5730\u5e73\u8861\u4e0d\u540c\u640d\u5931\u9805\u7684\u8ca2\u737b\u3002\u540c\u6642\uff0c\u6211\u5011\u5f15\u5165\u9918\u5f26\u9000\u706b\u5b78\u7fd2\u7387\u7b56\u7565\uff0c\u5e6b\u52a9\u6a21\u578b\u5728\u8a13\u7df4\u5f8c\u671f\u66f4\u7a69\u5b9a\u5730\u6536\u6582\u3002\u5728 Flickr30k \u8cc7\u6599\u96c6\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u9a57\u8b49\u8868\u660e\uff0cMH-CVSE \u6a21\u578b\u5728\u96d9\u5411\u5716\u50cf\u548c\u6587\u5b57\u6aa2\u7d22\u4efb\u52d9\u4e0a\u90fd\u53d6\u5f97\u4e86\u512a\u65bc\u4ee5\u5f80\u65b9\u6cd5\u7684\u6548\u80fd\uff0c\u5145\u5206\u5c55\u73fe\u5176\u6709\u6548\u6027\u548c\u512a\u8d8a\u6027\u3002", "author": "Wenjing Chen et.al.", "authors": "Wenjing Chen", "id": "2412.19184v1", "paper_url": "http://arxiv.org/abs/2412.19184v1", "repo": "null"}}