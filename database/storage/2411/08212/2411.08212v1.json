{"2411.08212": {"publish_time": "2024-11-12", "title": "PERFT: Parameter-Efficient Routed Fine-Tuning for Mixture-of-Expert Model", "paper_summary": "The Mixture-of-Experts (MoE) paradigm has emerged as a powerful approach for\nscaling transformers with improved resource utilization. However, efficiently\nfine-tuning MoE models remains largely underexplored. Inspired by recent works\non Parameter-Efficient Fine-Tuning (PEFT), we present a unified framework for\nintegrating PEFT modules directly into the MoE mechanism. Aligning with the\ncore principles and architecture of MoE, our framework encompasses a set of\ndesign dimensions including various functional and composition strategies. By\ncombining design choices within our framework, we introduce Parameter-Efficient\nRouted Fine-Tuning (PERFT) as a flexible and scalable family of PEFT strategies\ntailored for MoE models. Extensive experiments on adapting OLMoE-1B-7B and\nMixtral-8$\\times$7B for commonsense and arithmetic reasoning tasks demonstrate\nthe effectiveness, scalability, and intriguing dynamics of PERFT. Additionally,\nwe provide empirical findings for each specific design choice to facilitate\nbetter application of MoE and PEFT.", "paper_summary_zh": "\u6df7\u5408\u5c08\u5bb6 (MoE) \u5178\u7bc4\u5df2\u6210\u70ba\u4e00\u7a2e\u5f37\u5927\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u65bc\u7e2e\u653e\u5177\u6709\u6539\u5584\u8cc7\u6e90\u5229\u7528\u7387\u7684Transformer\u3002\u7136\u800c\uff0c\u6709\u6548\u5fae\u8abf MoE \u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u8a0e\u3002\u53d7\u5230\u6700\u8fd1\u95dc\u65bc\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u7684\u7814\u7a76\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7d71\u4e00\u6846\u67b6\uff0c\u7528\u65bc\u5c07 PEFT \u6a21\u7d44\u76f4\u63a5\u6574\u5408\u5230 MoE \u6a5f\u5236\u4e2d\u3002\u6211\u5011\u7684\u6846\u67b6\u8207 MoE \u7684\u6838\u5fc3\u539f\u5247\u548c\u67b6\u69cb\u4fdd\u6301\u4e00\u81f4\uff0c\u5305\u542b\u4e00\u7d44\u8a2d\u8a08\u7dad\u5ea6\uff0c\u5176\u4e2d\u5305\u62ec\u5404\u7a2e\u529f\u80fd\u548c\u7d44\u6210\u7b56\u7565\u3002\u900f\u904e\u7d50\u5408\u6211\u5011\u6846\u67b6\u5167\u7684\u8a2d\u8a08\u9078\u64c7\uff0c\u6211\u5011\u5f15\u5165\u53c3\u6578\u6709\u6548\u8def\u7531\u5fae\u8abf (PERFT) \u4f5c\u70ba\u5c08\u9580\u91dd\u5c0d MoE \u6a21\u578b\u8a2d\u8a08\u7684 PEFT \u7b56\u7565\u7684\u9748\u6d3b\u4e14\u53ef\u64f4\u5145\u7684\u7cfb\u5217\u3002\u91dd\u5c0d\u9069\u61c9 OLMoE-1B-7B \u548c Mixtral-8$\\times$7B \u4ee5\u9032\u884c\u5e38\u8b58\u548c\u7b97\u8853\u63a8\u7406\u4efb\u52d9\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 PERFT \u7684\u6709\u6548\u6027\u3001\u53ef\u64f4\u5145\u6027\u548c\u6709\u8da3\u7684\u52d5\u614b\u7279\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u6bcf\u500b\u7279\u5b9a\u8a2d\u8a08\u9078\u64c7\u7684\u7d93\u9a57\u767c\u73fe\uff0c\u4ee5\u4fc3\u9032 MoE \u548c PEFT \u7684\u66f4\u597d\u61c9\u7528\u3002", "author": "Yilun Liu et.al.", "authors": "Yilun Liu, Yunpu Ma, Shuo Chen, Zifeng Ding, Bailan He, Zhen Han, Volker Tresp", "id": "2411.08212v1", "paper_url": "http://arxiv.org/abs/2411.08212v1", "repo": "null"}}