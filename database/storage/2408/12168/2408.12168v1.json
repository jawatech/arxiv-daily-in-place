{"2408.12168": {"publish_time": "2024-08-22", "title": "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "paper_summary": "Large language models (LLMs) have become increasingly prevalent in our daily\nlives, leading to an expectation for LLMs to be trustworthy -- - both accurate\nand well-calibrated (the prediction confidence should align with its ground\ntruth correctness likelihood). Nowadays, fine-tuning has become the most\npopular method for adapting a model to practical usage by significantly\nincreasing accuracy on downstream tasks. Despite the great accuracy it\nachieves, we found fine-tuning is still far away from satisfactory\ntrustworthiness due to \"tuning-induced mis-calibration\". In this paper, we\ndelve deeply into why and how mis-calibration exists in fine-tuned models, and\nhow distillation can alleviate the issue. Then we further propose a brand new\nmethod named Efficient Trustworthy Distillation (FIRST), which utilizes a small\nportion of teacher's knowledge to obtain a reliable language model in a\ncost-efficient way. Specifically, we identify the \"concentrated knowledge\"\nphenomenon during distillation, which can significantly reduce the\ncomputational burden. Then we apply a \"trustworthy maximization\" process to\noptimize the utilization of this small portion of concentrated knowledge before\ntransferring it to the student. Experimental results demonstrate the\neffectiveness of our method, where better accuracy (+2.3%) and less\nmis-calibration (-10%) are achieved on average across both in-domain and\nout-of-domain scenarios, indicating better trustworthiness.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6211\u5011\u7684\u65e5\u5e38\u751f\u6d3b\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u666e\u904d\uff0c\u5c0e\u81f4\u4eba\u5011\u671f\u671b LLM \u503c\u5f97\u4fe1\u8cf4\u2014\u2014\u65e2\u6e96\u78ba\u53c8\u6821\u6e96\u826f\u597d\uff08\u9810\u6e2c\u4fe1\u5fc3\u61c9\u8207\u5176\u57fa\u672c\u6b63\u78ba\u6027\u6a5f\u7387\u4e00\u81f4\uff09\u3002\u5982\u4eca\uff0c\u5fae\u8abf\u5df2\u6210\u70ba\u900f\u904e\u986f\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52d9\u7684\u6e96\u78ba\u6027\u4f86\u8abf\u6574\u6a21\u578b\u4ee5\u9069\u61c9\u5be6\u969b\u7528\u9014\u6700\u53d7\u6b61\u8fce\u7684\u65b9\u6cd5\u3002\u5118\u7ba1\u5b83\u9054\u5230\u4e86\u6975\u9ad8\u7684\u6e96\u78ba\u6027\uff0c\u4f46\u6211\u5011\u767c\u73fe\u5fae\u8abf\u4ecd\u9060\u9060\u7121\u6cd5\u4ee4\u4eba\u6eff\u610f\uff0c\u56e0\u70ba\u5b83\u6703\u5c0e\u81f4\u300c\u5fae\u8abf\u8a98\u767c\u7684\u6821\u6e96\u4e0d\u4f73\u300d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u4e86\u5fae\u8abf\u6a21\u578b\u4e2d\u70ba\u4f55\u4e14\u5982\u4f55\u51fa\u73fe\u6821\u6e96\u4e0d\u4f73\uff0c\u4ee5\u53ca\u84b8\u993e\u5982\u4f55\u80fd\u7de9\u89e3\u9019\u500b\u554f\u984c\u3002\u7136\u5f8c\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba\u300c\u9ad8\u6548\u503c\u5f97\u4fe1\u8cf4\u84b8\u993e\u300d(FIRST) \u7684\u5168\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u8001\u5e2b\u7684\u4e00\u5c0f\u90e8\u5206\u77e5\u8b58\u4ee5\u7d93\u6fdf\u6709\u6548\u7684\u65b9\u5f0f\u7372\u5f97\u53ef\u9760\u7684\u8a9e\u8a00\u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728\u84b8\u993e\u904e\u7a0b\u4e2d\u8b58\u5225\u51fa\u300c\u96c6\u4e2d\u77e5\u8b58\u300d\u73fe\u8c61\uff0c\u9019\u53ef\u4ee5\u986f\u8457\u964d\u4f4e\u904b\u7b97\u8ca0\u64d4\u3002\u7136\u5f8c\uff0c\u6211\u5011\u904b\u7528\u300c\u503c\u5f97\u4fe1\u8cf4\u7684\u6700\u5927\u5316\u300d\u7a0b\u5e8f\u4f86\u6700\u4f73\u5316\u9019\u5c0f\u90e8\u5206\u96c6\u4e2d\u77e5\u8b58\u7684\u5229\u7528\uff0c\u518d\u5c07\u5176\u8f49\u79fb\u5230\u5b78\u751f\u8eab\u4e0a\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u9818\u57df\u5167\u548c\u9818\u57df\u5916\u60c5\u5883\u4e2d\u5e73\u5747\u7372\u5f97\u4e86\u66f4\u597d\u7684\u6e96\u78ba\u6027 (+2.3%) \u548c\u66f4\u5c11\u7684\u6821\u6e96\u4e0d\u4f73 (-10%)\uff0c\u9019\u8868\u793a\u5177\u6709\u66f4\u597d\u7684\u53ef\u4fe1\u5ea6\u3002", "author": "KaShun Shum et.al.", "authors": "KaShun Shum, Minrui Xu, Jianshu Zhang, Zixin Chen, Shizhe Diao, Hanze Dong, Jipeng Zhang, Muhammad Omer Raza", "id": "2408.12168v1", "paper_url": "http://arxiv.org/abs/2408.12168v1", "repo": "null"}}