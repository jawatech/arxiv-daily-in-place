{"2410.12360": {"publish_time": "2024-10-16", "title": "Towards Neural Scaling Laws for Time Series Foundation Models", "paper_summary": "Scaling laws offer valuable insights into the design of time series\nfoundation models (TSFMs). However, previous research has largely focused on\nthe scaling laws of TSFMs for in-distribution (ID) data, leaving their\nout-of-distribution (OOD) scaling behavior and the influence of model\narchitectures less explored. In this work, we examine two common TSFM\narchitectures, encoder-only and decoder-only Transformers, and investigate\ntheir scaling behavior on both ID and OOD data. These models are trained and\nevaluated across varying parameter counts, compute budgets, and dataset sizes.\nOur experiments reveal that the log-likelihood loss of TSFMs exhibits similar\nscaling behavior in both OOD and ID settings. We further compare the scaling\nproperties across different architectures, incorporating two state-of-the-art\nTSFMs as case studies, showing that model architecture plays a significant role\nin scaling. The encoder-only Transformers demonstrate better scalability than\nthe decoder-only Transformers, while the architectural enhancements in the two\nadvanced TSFMs primarily improve ID performance but reduce OOD scalability.\nWhile scaling up TSFMs is expected to drive performance breakthroughs, the lack\nof a comprehensive understanding of TSFM scaling laws has hindered the\ndevelopment of a robust framework to guide model scaling. We fill this gap in\nthis work by synthesizing our findings and providing practical guidelines for\ndesigning and scaling larger TSFMs with enhanced model capabilities.", "paper_summary_zh": "<paragraph>\u898f\u6a21\u5b9a\u5f8b\u70ba\u6642\u5e8f\u57fa\u790e\u6a21\u578b (TSFM) \u7684\u8a2d\u8a08\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728 TSFM \u7684\u5206\u4f48\u5167 (ID) \u8cc7\u6599\u7684\u898f\u6a21\u5b9a\u5f8b\uff0c\u8f03\u5c11\u63a2\u8a0e\u5b83\u5011\u7684\u5206\u4f48\u5916 (OOD) \u898f\u6a21\u884c\u70ba\u548c\u6a21\u578b\u67b6\u69cb\u7684\u5f71\u97ff\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u6aa2\u9a57\u4e86\u5169\u7a2e\u5e38\u898b\u7684 TSFM \u67b6\u69cb\uff0c\u50c5\u7de8\u78bc\u5668\u548c\u50c5\u89e3\u78bc\u5668 Transformer\uff0c\u4e26\u7814\u7a76\u5b83\u5011\u5728 ID \u548c OOD \u8cc7\u6599\u4e0a\u7684\u898f\u6a21\u884c\u70ba\u3002\u9019\u4e9b\u6a21\u578b\u7d93\u904e\u8a13\u7df4\u4e26\u5728\u4e0d\u540c\u7684\u53c3\u6578\u6578\u91cf\u3001\u904b\u7b97\u9810\u7b97\u548c\u8cc7\u6599\u96c6\u5927\u5c0f\u4e2d\u9032\u884c\u8a55\u4f30\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cTSFM \u7684\u5c0d\u6578\u4f3c\u7136\u640d\u5931\u5728 OOD \u548c ID \u8a2d\u5b9a\u4e2d\u90fd\u8868\u73fe\u51fa\u985e\u4f3c\u7684\u898f\u6a21\u884c\u70ba\u3002\u6211\u5011\u9032\u4e00\u6b65\u6bd4\u8f03\u4e86\u4e0d\u540c\u67b6\u69cb\u7684\u898f\u6a21\u5c6c\u6027\uff0c\u5c07\u5169\u500b\u6700\u5148\u9032\u7684 TSFM \u4f5c\u70ba\u6848\u4f8b\u7814\u7a76\uff0c\u8868\u660e\u6a21\u578b\u67b6\u69cb\u5728\u898f\u6a21\u4e2d\u626e\u6f14\u8457\u91cd\u8981\u7684\u89d2\u8272\u3002\u50c5\u7de8\u78bc\u5668 Transformer \u6bd4\u50c5\u89e3\u78bc\u5668 Transformer \u5177\u6709\u66f4\u597d\u7684\u53ef\u64f4\u5145\u6027\uff0c\u800c\u5169\u500b\u9032\u968e TSFM \u4e2d\u7684\u67b6\u69cb\u589e\u5f37\u4e3b\u8981\u6539\u5584\u4e86 ID \u6548\u80fd\uff0c\u4f46\u964d\u4f4e\u4e86 OOD \u53ef\u64f4\u5145\u6027\u3002\u5118\u7ba1\u64f4\u5145 TSFM \u9810\u8a08\u5c07\u63a8\u52d5\u6548\u80fd\u7a81\u7834\uff0c\u4f46\u7f3a\u4e4f\u5c0d TSFM \u898f\u6a21\u5b9a\u5f8b\u7684\u5168\u9762\u4e86\u89e3\u963b\u7919\u4e86\u5236\u5b9a\u4e00\u500b\u5065\u5168\u7684\u67b6\u69cb\u4f86\u6307\u5c0e\u6a21\u578b\u898f\u6a21\u3002\u6211\u5011\u900f\u904e\u7d9c\u5408\u6211\u5011\u7684\u767c\u73fe\u4e26\u63d0\u4f9b\u5be6\u7528\u7684\u6307\u5357\uff0c\u4f86\u586b\u88dc\u9019\u9805\u5de5\u4f5c\u4e2d\u7684\u7a7a\u767d\uff0c\u4ee5\u8a2d\u8a08\u548c\u64f4\u5145\u5177\u6709\u589e\u5f37\u6a21\u578b\u529f\u80fd\u7684\u66f4\u5927 TSFM\u3002</paragraph>", "author": "Qingren Yao et.al.", "authors": "Qingren Yao, Chao-Han Huck Yang, Renhe Jiang, Yuxuan Liang, Ming Jin, Shirui Pan", "id": "2410.12360v1", "paper_url": "http://arxiv.org/abs/2410.12360v1", "repo": "null"}}