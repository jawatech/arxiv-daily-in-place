{"2411.08666": {"publish_time": "2024-11-13", "title": "A Survey on Vision Autoregressive Model", "paper_summary": "Autoregressive models have demonstrated great performance in natural language\nprocessing (NLP) with impressive scalability, adaptability and\ngeneralizability. Inspired by their notable success in NLP field,\nautoregressive models have been intensively investigated recently for computer\nvision, which perform next-token predictions by representing visual data as\nvisual tokens and enables autoregressive modelling for a wide range of vision\ntasks, ranging from visual generation and visual understanding to the very\nrecent multimodal generation that unifies visual generation and understanding\nwith a single autoregressive model. This paper provides a systematic review of\nvision autoregressive models, including the development of a taxonomy of\nexisting methods and highlighting their major contributions, strengths, and\nlimitations, covering various vision tasks such as image generation, video\ngeneration, image editing, motion generation, medical image analysis, 3D\ngeneration, robotic manipulation, unified multimodal generation, etc. Besides,\nwe investigate and analyze the latest advancements in autoregressive models,\nincluding thorough benchmarking and discussion of existing methods across\nvarious evaluation datasets. Finally, we outline key challenges and promising\ndirections for future research, offering a roadmap to guide further\nadvancements in vision autoregressive models.", "paper_summary_zh": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u4e2d\u5c55\u73b0\u4e86\u6781\u4f73\u7684\u6027\u80fd\uff0c\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\u3002\u53d7\u5176\u5728 NLP \u9886\u57df\u7684\u663e\u8457\u6210\u529f\u542f\u53d1\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u6700\u8fd1\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5f97\u5230\u4e86\u6df1\u5165\u7814\u7a76\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u6570\u636e\u8868\u793a\u4e3a\u89c6\u89c9\u6807\u8bb0\u6765\u6267\u884c\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff0c\u5e76\u4e3a\u5e7f\u6cdb\u7684\u89c6\u89c9\u4efb\u52a1\u542f\u7528\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u4ece\u89c6\u89c9\u751f\u6210\u548c\u89c6\u89c9\u7406\u89e3\u5230\u6700\u8fd1\u5c06\u89c6\u89c9\u751f\u6210\u548c\u7406\u89e3\u7edf\u4e00\u5230\u4e00\u4e2a\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u591a\u6a21\u6001\u751f\u6210\u3002\u672c\u6587\u5bf9\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5305\u62ec\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5b83\u4eec\u7684\u4e3b\u8981\u8d21\u732e\u3001\u4f18\u70b9\u548c\u5c40\u9650\u6027\uff0c\u6db5\u76d6\u4e86\u56fe\u50cf\u751f\u6210\u3001\u89c6\u9891\u751f\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u3001\u52a8\u4f5c\u751f\u6210\u3001\u533b\u5b66\u56fe\u50cf\u5206\u6790\u30013D \u751f\u6210\u3001\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u7b49\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8c03\u67e5\u548c\u5206\u6790\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u5bf9\u73b0\u6709\u65b9\u6cd5\u5728\u5404\u79cd\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\u548c\u8ba8\u8bba\u3002\u6700\u540e\uff0c\u6211\u4eec\u6982\u8ff0\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u548c\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u4e3a\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "author": "Kai Jiang et.al.", "authors": "Kai Jiang, Jiaxing Huang", "id": "2411.08666v2", "paper_url": "http://arxiv.org/abs/2411.08666v2", "repo": "null"}}