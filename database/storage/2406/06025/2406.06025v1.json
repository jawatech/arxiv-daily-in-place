{"2406.06025": {"publish_time": "2024-06-10", "title": "RepoQA: Evaluating Long Context Code Understanding", "paper_summary": "Recent advances have been improving the context windows of Large Language\nModels (LLMs). To quantify the real long-context capabilities of LLMs,\nevaluators such as the popular Needle in a Haystack have been developed to test\nLLMs over a large chunk of raw texts. While effective, current evaluations\noverlook the insight of how LLMs work with long-context code, i.e.,\nrepositories. To this end, we initiate the RepoQA benchmark to evaluate LLMs on\nlong-context code understanding. Traditional needle testers ask LLMs to\ndirectly retrieve the answer from the context without necessary deep\nunderstanding. In RepoQA, we built our initial task, namely Searching Needle\nFunction (SNF), which exercises LLMs to search functions given their\nnatural-language description, i.e., LLMs cannot find the desired function if\nthey cannot understand the description and code. RepoQA is multilingual and\ncomprehensive: it includes 500 code search tasks gathered from 50 popular\nrepositories across 5 modern programming languages. By evaluating 26 general\nand code-specific LLMs on RepoQA, we show (i) there is still a small gap\nbetween the best open and proprietary models; (ii) different models are good at\ndifferent languages; and (iii) models may understand code better without\ncomments.", "paper_summary_zh": "\u6700\u8fd1\u7684\u8fdb\u6b65\u6539\u5584\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002\u4e3a\u4e86\u91cf\u5316 LLM \u7684\u771f\u6b63\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u5df2\u7ecf\u5f00\u53d1\u51fa\u8bf8\u5982\u6d41\u884c\u7684\u5927\u6d77\u635e\u9488\u4e4b\u7c7b\u7684\u8bc4\u4f30\u5668\uff0c\u4ee5\u5728\u5927\u91cf\u539f\u59cb\u6587\u672c\u4e0a\u6d4b\u8bd5 LLM\u3002\u867d\u7136\u6709\u6548\uff0c\u4f46\u5f53\u524d\u7684\u8bc4\u4f30\u5ffd\u7565\u4e86 LLM \u5982\u4f55\u4f7f\u7528\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\uff08\u5373\u5b58\u50a8\u5e93\uff09\u7684\u89c1\u89e3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u542f\u52a8\u4e86 RepoQA \u57fa\u51c6\u6765\u8bc4\u4f30 LLM \u5bf9\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u7406\u89e3\u3002\u4f20\u7edf\u7684\u9488\u6d4b\u8bd5\u8981\u6c42 LLM \u76f4\u63a5\u4ece\u4e0a\u4e0b\u6587\u4e2d\u68c0\u7d22\u7b54\u6848\uff0c\u800c\u65e0\u9700\u5fc5\u8981\u7684\u6df1\u5165\u7406\u89e3\u3002\u5728 RepoQA \u4e2d\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u6211\u4eec\u7684\u521d\u59cb\u4efb\u52a1\uff0c\u5373\u641c\u7d22\u9488\u51fd\u6570 (SNF)\uff0c\u5b83\u8ba9 LLM \u6839\u636e\u5176\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u641c\u7d22\u51fd\u6570\uff0c\u5373\u5982\u679c LLM \u65e0\u6cd5\u7406\u89e3\u63cf\u8ff0\u548c\u4ee3\u7801\uff0c\u5219\u65e0\u6cd5\u627e\u5230\u6240\u9700\u51fd\u6570\u3002RepoQA \u662f\u591a\u8bed\u8a00\u4e14\u5168\u9762\u7684\uff1a\u5b83\u5305\u542b\u4ece 5 \u79cd\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684 50 \u4e2a\u6d41\u884c\u5b58\u50a8\u5e93\u4e2d\u6536\u96c6\u7684 500 \u4e2a\u4ee3\u7801\u641c\u7d22\u4efb\u52a1\u3002\u901a\u8fc7\u8bc4\u4f30 RepoQA \u4e0a\u7684 26 \u4e2a\u901a\u7528\u548c\u7279\u5b9a\u4e8e\u4ee3\u7801\u7684 LLM\uff0c\u6211\u4eec\u5c55\u793a\u4e86 (i) \u6700\u597d\u7684\u5f00\u653e\u548c\u4e13\u6709\u6a21\u578b\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u5f88\u5c0f\u7684\u5dee\u8ddd\uff1b(ii) \u4e0d\u540c\u7684\u6a21\u578b\u64c5\u957f\u4e0d\u540c\u7684\u8bed\u8a00\uff1b(iii) \u6a21\u578b\u53ef\u80fd\u5728\u6ca1\u6709\u6ce8\u91ca\u7684\u60c5\u51b5\u4e0b\u66f4\u597d\u5730\u7406\u89e3\u4ee3\u7801\u3002", "author": "Jiawei Liu et.al.", "authors": "Jiawei Liu, Jia Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan Katherine Wang, Jun Yang, Lingming Zhang", "id": "2406.06025v1", "paper_url": "http://arxiv.org/abs/2406.06025v1", "repo": "null"}}