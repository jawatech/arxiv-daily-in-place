{"2406.04202": {"publish_time": "2024-06-06", "title": "Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model", "paper_summary": "With the development of large-scale Language Models (LLM), fine-tuning\npre-trained LLM has become a mainstream paradigm for solving downstream tasks\nof natural language processing. However, training a language model in the legal\nfield requires a large number of legal documents so that the language model can\nlearn legal terminology and the particularity of the format of legal documents.\nThe typical NLP approaches usually rely on many manually annotated data sets\nfor training. However, in the legal field application, it is difficult to\nobtain a large number of manually annotated data sets, which restricts the\ntypical method applied to the task of drafting legal documents. The\nexperimental results of this paper show that not only can we leverage a large\nnumber of annotation-free legal documents without Chinese word segmentation to\nfine-tune a large-scale language model, but more importantly, it can fine-tune\na pre-trained LLM on the local computer to achieve the generating legal\ndocument drafts task, and at the same time achieve the protection of\ninformation privacy and to improve information security issues.", "paper_summary_zh": "\u96a8\u8457\u5927\u898f\u6a21\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u767c\u5c55\uff0c\u5fae\u8abf\u9810\u5148\u8a13\u7df4\u7684 LLM \u5df2\u6210\u70ba\u89e3\u6c7a\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4e0b\u6e38\u4efb\u52d9\u7684\u4e3b\u6d41\u7bc4\u4f8b\u3002\u7136\u800c\uff0c\u5728\u6cd5\u5f8b\u9818\u57df\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u6cd5\u5f8b\u6587\u4ef6\uff0c\u4ee5\u4fbf\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u5b78\u7fd2\u6cd5\u5f8b\u8853\u8a9e\u548c\u6cd5\u5f8b\u6587\u4ef6\u683c\u5f0f\u7684\u7279\u6b8a\u6027\u3002\u5178\u578b\u7684 NLP \u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u65bc\u8a31\u591a\u624b\u52d5\u8a3b\u91cb\u7684\u8cc7\u6599\u96c6\u9032\u884c\u8a13\u7df4\u3002\u7136\u800c\uff0c\u5728\u6cd5\u5f8b\u9818\u57df\u61c9\u7528\u4e2d\uff0c\u5f88\u96e3\u7372\u5f97\u5927\u91cf\u7684\u3001\u624b\u52d5\u8a3b\u91cb\u7684\u8cc7\u6599\u96c6\uff0c\u9019\u9650\u5236\u4e86\u5178\u578b\u65b9\u6cd5\u61c9\u7528\u65bc\u6cd5\u5f8b\u6587\u4ef6\u8d77\u8349\u4efb\u52d9\u3002\u672c\u6587\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u4e0d\u50c5\u53ef\u4ee5\u5229\u7528\u5927\u91cf\u6c92\u6709\u4e2d\u6587\u5206\u8a5e\u7684\u7121\u8a3b\u91cb\u6cd5\u5f8b\u6587\u4ef6\u4f86\u5fae\u8abf\u5927\u898f\u6a21\u8a9e\u8a00\u6a21\u578b\uff0c\u66f4\u91cd\u8981\u7684\u662f\uff0c\u53ef\u4ee5\u5728\u672c\u5730\u96fb\u8166\u4e0a\u5fae\u8abf\u9810\u5148\u8a13\u7df4\u7684 LLM \u4ee5\u9054\u6210\u7522\u751f\u6cd5\u5f8b\u6587\u4ef6\u8349\u7a3f\u4efb\u52d9\uff0c\u540c\u6642\u9054\u6210\u8cc7\u8a0a\u96b1\u79c1\u4fdd\u8b77\u548c\u63d0\u5347\u8cc7\u8a0a\u5b89\u5168\u554f\u984c\u3002", "author": "Chun-Hsien Lin et.al.", "authors": "Chun-Hsien Lin, Pu-Jen Cheng", "id": "2406.04202v1", "paper_url": "http://arxiv.org/abs/2406.04202v1", "repo": "https://huggingface.co/jslin09/bloom-560m-finetuned-fraud"}}