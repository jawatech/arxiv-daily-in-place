{"2408.14016": {"publish_time": "2024-08-26", "title": "Pixel-Aligned Multi-View Generation with Depth Guided Decoder", "paper_summary": "The task of image-to-multi-view generation refers to generating novel views\nof an instance from a single image. Recent methods achieve this by extending\ntext-to-image latent diffusion models to multi-view version, which contains an\nVAE image encoder and a U-Net diffusion model. Specifically, these generation\nmethods usually fix VAE and finetune the U-Net only. However, the significant\ndownscaling of the latent vectors computed from the input images and\nindependent decoding leads to notable pixel-level misalignment across multiple\nviews. To address this, we propose a novel method for pixel-level\nimage-to-multi-view generation. Unlike prior work, we incorporate attention\nlayers across multi-view images in the VAE decoder of a latent video diffusion\nmodel. Specifically, we introduce a depth-truncated epipolar attention,\nenabling the model to focus on spatially adjacent regions while remaining\nmemory efficient. Applying depth-truncated attn is challenging during inference\nas the ground-truth depth is usually difficult to obtain and pre-trained depth\nestimation models is hard to provide accurate depth. Thus, to enhance the\ngeneralization to inaccurate depth when ground truth depth is missing, we\nperturb depth inputs during training. During inference, we employ a rapid\nmulti-view to 3D reconstruction approach, NeuS, to obtain coarse depth for the\ndepth-truncated epipolar attention. Our model enables better pixel alignment\nacross multi-view images. Moreover, we demonstrate the efficacy of our approach\nin improving downstream multi-view to 3D reconstruction tasks.", "paper_summary_zh": "\u5f71\u50cf\u8f49\u591a\u8996\u5716\u751f\u6210\u7684\u4efb\u52d9\uff0c\u662f\u6307\u5f9e\u55ae\u5f35\u5f71\u50cf\u751f\u6210\u4e00\u500b\u5be6\u4f8b\u7684\u65b0\u8996\u5716\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u900f\u904e\u5c07\u6587\u5b57\u8f49\u5f71\u50cf\u7684\u6f5b\u5728\u64f4\u6563\u6a21\u578b\u5ef6\u4f38\u81f3\u591a\u8996\u5716\u7248\u672c\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u500b VAE \u5f71\u50cf\u7de8\u78bc\u5668\u548c\u4e00\u500b U-Net \u64f4\u6563\u6a21\u578b\u4f86\u9054\u6210\u6b64\u76ee\u7684\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u9019\u4e9b\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u6703\u56fa\u5b9a VAE \u4e26\u50c5\u5fae\u8abf U-Net\u3002\u7136\u800c\uff0c\u5f9e\u8f38\u5165\u5f71\u50cf\u8a08\u7b97\u51fa\u7684\u6f5b\u5728\u5411\u91cf\u7684\u986f\u8457\u7e2e\u653e\u548c\u7368\u7acb\u89e3\u78bc\u6703\u5c0e\u81f4\u591a\u500b\u8996\u5716\u4e4b\u9593\u7522\u751f\u660e\u986f\u7684\u50cf\u7d20\u7d1a\u5931\u6e96\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7528\u65bc\u50cf\u7d20\u7d1a\u5f71\u50cf\u8f49\u591a\u8996\u5716\u751f\u6210\u7684\u65b0\u65b9\u6cd5\u3002\u8207\u5148\u524d\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u5011\u5728\u6f5b\u5728\u5f71\u7247\u64f4\u6563\u6a21\u578b\u7684 VAE \u89e3\u78bc\u5668\u4e2d\u52a0\u5165\u4e86\u8de8\u591a\u8996\u5716\u5f71\u50cf\u7684\u6ce8\u610f\u529b\u5c64\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6df1\u5ea6\u622a\u65b7\u7684\u6975\u7dda\u6ce8\u610f\u529b\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u5c08\u6ce8\u65bc\u7a7a\u9593\u76f8\u9130\u5340\u57df\uff0c\u540c\u6642\u4fdd\u6301\u8a18\u61b6\u9ad4\u6548\u7387\u3002\u5728\u63a8\u8ad6\u904e\u7a0b\u4e2d\u5957\u7528\u6df1\u5ea6\u622a\u65b7\u7684\u6ce8\u610f\u529b\u5177\u6709\u6311\u6230\u6027\uff0c\u56e0\u70ba\u901a\u5e38\u96e3\u4ee5\u53d6\u5f97\u771f\u5be6\u6df1\u5ea6\uff0c\u800c\u9810\u5148\u8a13\u7df4\u7684\u6df1\u5ea6\u4f30\u8a08\u6a21\u578b\u4e5f\u96e3\u4ee5\u63d0\u4f9b\u6e96\u78ba\u7684\u6df1\u5ea6\u3002\u56e0\u6b64\uff0c\u70ba\u4e86\u5728\u6c92\u6709\u771f\u5be6\u6df1\u5ea6\u7684\u60c5\u6cc1\u4e0b\u589e\u5f37\u5c0d\u4e0d\u6e96\u78ba\u6df1\u5ea6\u7684\u6cdb\u5316\uff0c\u6211\u5011\u5728\u8a13\u7df4\u671f\u9593\u64fe\u52d5\u6df1\u5ea6\u8f38\u5165\u3002\u5728\u63a8\u8ad6\u904e\u7a0b\u4e2d\uff0c\u6211\u5011\u63a1\u7528\u4e00\u7a2e\u5feb\u901f\u7684\u5f9e\u591a\u8996\u5716\u5230 3D \u91cd\u5efa\u65b9\u6cd5 NeuS \u4f86\u53d6\u5f97\u7528\u65bc\u6df1\u5ea6\u622a\u65b7\u7684\u6975\u7dda\u6ce8\u610f\u529b\u7684\u7c97\u7565\u6df1\u5ea6\u3002\u6211\u5011\u7684\u6a21\u578b\u80fd\u5920\u8b93\u591a\u8996\u5716\u5f71\u50cf\u4e4b\u9593\u7684\u50cf\u7d20\u5c0d\u9f4a\u66f4\u4f73\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u6539\u5584\u4e0b\u6e38\u591a\u8996\u5716\u5230 3D \u91cd\u5efa\u4efb\u52d9\u65b9\u9762\u7684\u6548\u529b\u3002", "author": "Zhenggang Tang et.al.", "authors": "Zhenggang Tang, Peiye Zhuang, Chaoyang Wang, Aliaksandr Siarohin, Yash Kant, Alexander Schwing, Sergey Tulyakov, Hsin-Ying Lee", "id": "2408.14016v1", "paper_url": "http://arxiv.org/abs/2408.14016v1", "repo": "null"}}