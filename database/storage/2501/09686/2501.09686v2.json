{"2501.09686": {"publish_time": "2025-01-16", "title": "Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning Capabilities", "paper_summary": "Language has long been conceived as an essential tool for human reasoning.\nThe breakthrough of Large Language Models (LLMs) has sparked significant\nresearch interest in leveraging these models to tackle complex reasoning tasks.\nResearchers have moved beyond simple autoregressive token generation by\nintroducing the concept of \"thought\" -- a sequence of tokens representing\nintermediate steps in the reasoning process. This innovative paradigm enables\nLLMs' to mimic complex human reasoning processes, such as tree search and\nreflective thinking. Recently, an emerging trend of learning to reason has\napplied reinforcement learning (RL) to train LLMs to master reasoning\nprocesses. This approach enables the automatic generation of high-quality\nreasoning trajectories through trial-and-error search algorithms, significantly\nexpanding LLMs' reasoning capacity by providing substantially more training\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\nwith more tokens during test-time inference can further significantly boost\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\nshow a new research frontier -- a path toward Large Reasoning Model. The\nintroduction of OpenAI's o1 series marks a significant milestone in this\nresearch direction. In this survey, we present a comprehensive review of recent\nprogress in LLM reasoning. We begin by introducing the foundational background\nof LLMs and then explore the key technical components driving the development\nof large reasoning models, with a focus on automated data construction,\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\nopen-source projects at building large reasoning models, and conclude with open\nchallenges and future research directions.", "paper_summary_zh": "\u8a9e\u8a00\u9577\u671f\u4ee5\u4f86\u88ab\u8996\u70ba\u4eba\u985e\u63a8\u7406\u7684\u5fc5\u8981\u5de5\u5177\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7a81\u7834\u6fc0\u767c\u4e86\u986f\u8457\u7684\u7814\u7a76\u8208\u8da3\uff0c\u4ee5\u5229\u7528\u9019\u4e9b\u6a21\u578b\u4f86\u89e3\u6c7a\u8907\u96dc\u7684\u63a8\u7406\u4efb\u52d9\u3002\u7814\u7a76\u4eba\u54e1\u5df2\u8d85\u8d8a\u7c21\u55ae\u7684\u81ea\u52d5\u8ff4\u6b78\u7b26\u865f\u751f\u6210\uff0c\u5f15\u5165\u4e86\u300c\u601d\u8003\u300d\u7684\u6982\u5ff5\u2014\u2014\u4e00\u500b\u7b26\u865f\u5e8f\u5217\uff0c\u4ee3\u8868\u63a8\u7406\u904e\u7a0b\u4e2d\u4e2d\u7684\u4e2d\u9593\u6b65\u9a5f\u3002\u9019\u7a2e\u5275\u65b0\u7684\u7bc4\u4f8b\u4f7f LLM \u80fd\u6a21\u64ec\u8907\u96dc\u7684\u4eba\u985e\u63a8\u7406\u904e\u7a0b\uff0c\u4f8b\u5982\u6a39\u72c0\u641c\u5c0b\u548c\u53cd\u601d\u6027\u601d\u8003\u3002\u6700\u8fd1\uff0c\u4e00\u7a2e\u65b0\u8208\u7684\u5b78\u7fd2\u63a8\u7406\u8da8\u52e2\u5c07\u5f37\u5316\u5b78\u7fd2 (RL) \u61c9\u7528\u65bc\u8a13\u7df4 LLM \u4ee5\u638c\u63e1\u63a8\u7406\u904e\u7a0b\u3002\u6b64\u65b9\u6cd5\u80fd\u900f\u904e\u8a66\u932f\u641c\u5c0b\u6f14\u7b97\u6cd5\u81ea\u52d5\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u63a8\u7406\u8ecc\u8de1\uff0c\u85c9\u7531\u63d0\u4f9b\u5927\u91cf\u8a13\u7df4\u8cc7\u6599\uff0c\u986f\u8457\u64f4\u5145 LLM \u7684\u63a8\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\uff0c\u9f13\u52f5 LLM \u5728\u6e2c\u8a66\u6642\u9593\u63a8\u8ad6\u671f\u9593\u4ee5\u66f4\u591a\u7b26\u865f\u300c\u601d\u8003\u300d\u80fd\u9032\u4e00\u6b65\u986f\u8457\u63d0\u5347\u63a8\u7406\u6e96\u78ba\u5ea6\u3002\u56e0\u6b64\uff0c\u8a13\u7df4\u6642\u9593\u548c\u6e2c\u8a66\u6642\u9593\u7684\u64f4\u5145\u7d50\u5408\u8d77\u4f86\uff0c\u5c55\u793a\u4e86\u4e00\u500b\u65b0\u7684\u7814\u7a76\u9818\u57df\u2014\u2014\u9081\u5411\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u9053\u8def\u3002OpenAI \u7684 o1 \u7cfb\u5217\u7684\u63a8\u51fa\u6a19\u8a8c\u8457\u9019\u500b\u7814\u7a76\u65b9\u5411\u7684\u91cd\u8981\u91cc\u7a0b\u7891\u3002\u5728\u9019\u9805\u8abf\u67e5\u4e2d\uff0c\u6211\u5011\u5c0d LLM \u63a8\u7406\u7684\u6700\u65b0\u9032\u5c55\u9032\u884c\u4e86\u5168\u9762\u7684\u56de\u9867\u3002\u6211\u5011\u5f9e\u4ecb\u7d39 LLM \u7684\u57fa\u790e\u80cc\u666f\u958b\u59cb\uff0c\u7136\u5f8c\u63a2\u8a0e\u63a8\u52d5\u5927\u578b\u63a8\u7406\u6a21\u578b\u767c\u5c55\u7684\u4e3b\u8981\u6280\u8853\u7d44\u6210\u90e8\u5206\uff0c\u91cd\u9ede\u653e\u5728\u81ea\u52d5\u5316\u8cc7\u6599\u5efa\u69cb\u3001\u5b78\u7fd2\u63a8\u7406\u6280\u8853\u548c\u6e2c\u8a66\u6642\u9593\u64f4\u5145\u3002\u6211\u5011\u9084\u5206\u6790\u4e86\u5efa\u7acb\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u71b1\u9580\u958b\u6e90\u5c08\u6848\uff0c\u4e26\u4ee5\u958b\u653e\u7684\u6311\u6230\u548c\u672a\u4f86\u7684\u7814\u7a76\u65b9\u5411\u4f5c\u7d50\u3002", "author": "Fengli Xu et.al.", "authors": "Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, Yong Li", "id": "2501.09686v2", "paper_url": "http://arxiv.org/abs/2501.09686v2", "repo": "null"}}