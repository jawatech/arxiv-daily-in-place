{"2410.01518": {"publish_time": "2024-10-02", "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "paper_summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.", "paper_summary_zh": "\u8655\u7406\u9577\u8f38\u5165\u8108\u7d61\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u8aaa\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u6311\u6230\uff0c\u5c24\u5176\u662f\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\uff0c\u4f8b\u5982\u884c\u52d5\u88dd\u7f6e\u3002\u6211\u5011\u7684\u7814\u7a76\u65e8\u5728\u900f\u904e\u5c0e\u5165 InfiniPot \u4f86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0cInfiniPot \u662f\u4e00\u500b\u65b0\u7a4e\u7684 KV \u5feb\u53d6\u63a7\u5236\u67b6\u69cb\uff0c\u65e8\u5728\u8b93\u9810\u5148\u8a13\u7df4\u7684 LLM \u80fd\u5920\u5728\u56fa\u5b9a\u7684\u8a18\u61b6\u9ad4\u9650\u5236\u5167\u6709\u6548\u7387\u5730\u7ba1\u7406\u5ee3\u6cdb\u7684\u5e8f\u5217\uff0c\u800c\u4e0d\u9700\u8981\u984d\u5916\u7684\u8a13\u7df4\u3002InfiniPot \u5229\u7528\u6301\u7e8c\u8108\u7d61\u8403\u53d6 (CCD)\uff0c\u9019\u662f\u4e00\u500b\u53cd\u8986\u7684\u7a0b\u5e8f\uff0c\u53ef\u900f\u904e\u65b0\u7a4e\u7684\u91cd\u8981\u6027\u6307\u6a19\u4f86\u58d3\u7e2e\u4e26\u4fdd\u7559\u5fc5\u8981\u7684\u8cc7\u8a0a\uff0c\u5373\u4f7f\u7121\u6cd5\u5b58\u53d6\u672a\u4f86\u7684\u8108\u7d61\uff0c\u4e5f\u80fd\u6709\u6548\u5730\u7dad\u8b77\u95dc\u9375\u8cc7\u6599\u3002\u6211\u5011\u7684\u5168\u9762\u8a55\u4f30\u6307\u51fa\uff0cInfiniPot \u5728\u5404\u7a2e NLP \u4efb\u52d9\u4e2d\u90fd\u660e\u986f\u512a\u65bc\u91dd\u5c0d\u9577\u8108\u7d61\u8a13\u7df4\u7684\u6a21\u578b\uff0c\u78ba\u7acb\u4e86\u5176\u6548\u80fd\u548c\u591a\u529f\u80fd\u6027\u3002\u9019\u9805\u7814\u7a76\u4ee3\u8868\u8457\u671d\u5411\u8b93 LLM \u9069\u7528\u65bc\u66f4\u5ee3\u6cdb\u7684\u771f\u5be6\u4e16\u754c\u60c5\u5883\u9081\u51fa\u4e86\u4e00\u5927\u6b65\u3002", "author": "Minsoo Kim et.al.", "authors": "Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang", "id": "2410.01518v1", "paper_url": "http://arxiv.org/abs/2410.01518v1", "repo": "null"}}