{"2406.04331": {"publish_time": "2024-06-06", "title": "PaCE: Parsimonious Concept Engineering for Large Language Models", "paper_summary": "Large Language Models (LLMs) are being used for a wide variety of tasks.\nWhile they are capable of generating human-like responses, they can also\nproduce undesirable output including potentially harmful information, racist or\nsexist language, and hallucinations. Alignment methods are designed to reduce\nsuch undesirable output, via techniques such as fine-tuning, prompt\nengineering, and representation engineering. However, existing methods face\nseveral challenges: some require costly fine-tuning for every alignment task;\nsome do not adequately remove undesirable concepts, failing alignment; some\nremove benign concepts, lowering the linguistic capabilities of LLMs. To\naddress these issues, we propose Parsimonious Concept Engineering (PaCE), a\nnovel activation engineering framework for alignment. First, to sufficiently\nmodel the concepts, we construct a large-scale concept dictionary in the\nactivation space, in which each atom corresponds to a semantic concept. Then,\ngiven any alignment task, we instruct a concept partitioner to efficiently\nannotate the concepts as benign or undesirable. Finally, at inference time, we\ndecompose the LLM activations along the concept dictionary via sparse coding,\nto accurately represent the activation as a linear combination of the benign\nand undesirable components. By removing the latter ones from the activation, we\nreorient the behavior of LLMs towards alignment goals. We conduct experiments\non tasks such as response detoxification, faithfulness enhancement, and\nsentiment revising, and show that PaCE achieves state-of-the-art alignment\nperformance while maintaining linguistic capabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u7528\u65bc\u5404\u7a2e\u4efb\u52d9\u3002\n\u96d6\u7136\u5b83\u5011\u80fd\u5920\u7522\u751f\u985e\u4f3c\u4eba\u985e\u7684\u56de\u61c9\uff0c\u4f46\u5b83\u5011\u4e5f\u53ef\u80fd\n\u7522\u751f\u4e0d\u826f\u7684\u8f38\u51fa\uff0c\u5305\u62ec\u6f5b\u5728\u6709\u5bb3\u8a0a\u606f\u3001\u7a2e\u65cf\u6216\n\u6027\u5225\u6b67\u8996\u8a9e\u8a00\u4ee5\u53ca\u5e7b\u89ba\u3002\u5c0d\u9f4a\u65b9\u6cd5\u65e8\u5728\u900f\u904e\u5fae\u8abf\u7b49\u6280\u8853\u6e1b\u5c11\n\u9019\u7a2e\u4e0d\u826f\u8f38\u51fa\uff0c\u63d0\u793a\u5de5\u7a0b\u548c\u8868\u793a\u5de5\u7a0b\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u9762\u81e8\n\u4e00\u4e9b\u6311\u6230\uff1a\u6709\u4e9b\u9700\u8981\u70ba\u6bcf\u500b\u5c0d\u9f4a\u4efb\u52d9\u9032\u884c\u6602\u8cb4\u7684\u5fae\u8abf\uff1b\n\u6709\u4e9b\u7121\u6cd5\u5145\u5206\u79fb\u9664\u4e0d\u826f\u6982\u5ff5\uff0c\u5c0e\u81f4\u5c0d\u9f4a\u5931\u6557\uff1b\u6709\u4e9b\n\u79fb\u9664\u826f\u6027\u6982\u5ff5\uff0c\u964d\u4f4e LLM \u7684\u8a9e\u8a00\u80fd\u529b\u3002\u70ba\u4e86\n\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u7c21\u7d04\u6982\u5ff5\u5de5\u7a0b (PaCE)\uff0c\u4e00\u7a2e\n\u7528\u65bc\u5c0d\u9f4a\u7684\u65b0\u578b\u6fc0\u6d3b\u5de5\u7a0b\u6846\u67b6\u3002\u9996\u5148\uff0c\u70ba\u4e86\u5145\u5206\n\u5efa\u69cb\u6982\u5ff5\u6a21\u578b\uff0c\u6211\u5011\u5728\u6fc0\u6d3b\u7a7a\u9593\u4e2d\u5efa\u69cb\u4e00\u500b\u5927\u578b\u6982\u5ff5\u5b57\u5178\uff0c\u5176\u4e2d\u6bcf\u500b\u539f\u5b50\u5c0d\u61c9\u4e00\u500b\u8a9e\u7fa9\u6982\u5ff5\u3002\u7136\u5f8c\uff0c\n\u91dd\u5c0d\u4efb\u4f55\u5c0d\u9f4a\u4efb\u52d9\uff0c\u6211\u5011\u6307\u793a\u6982\u5ff5\u5206\u5272\u5668\u6709\u6548\u5730\n\u5c07\u6982\u5ff5\u8a3b\u89e3\u70ba\u826f\u6027\u6216\u4e0d\u826f\u3002\u6700\u5f8c\uff0c\u5728\u63a8\u7406\u6642\u9593\uff0c\u6211\u5011\n\u900f\u904e\u7a00\u758f\u7de8\u78bc\u5206\u89e3 LLM \u6fc0\u6d3b\uff0c\u6cbf\u8457\u6982\u5ff5\u5b57\u5178\u9032\u884c\uff0c\n\u4ee5\u6e96\u78ba\u5730\u5c07\u6fc0\u6d3b\u8868\u793a\u70ba\u826f\u6027\u548c\u4e0d\u826f\u7d44\u6210\u7684\u7dda\u6027\u7d44\u5408\u3002\u900f\u904e\u79fb\u9664\u5f8c\u8005\u5f9e\u6fc0\u6d3b\u4e2d\uff0c\u6211\u5011\n\u91cd\u65b0\u5c0e\u5411 LLM \u7684\u884c\u70ba\u4ee5\u671d\u5411\u5c0d\u9f4a\u76ee\u6a19\u3002\u6211\u5011\u5728\u56de\u61c9\u89e3\u6bd2\u3001\u5fe0\u5be6\u5ea6\u589e\u5f37\u548c\n\u60c5\u7dd2\u4fee\u6b63\u7b49\u4efb\u52d9\u4e0a\u9032\u884c\u5be6\u9a57\uff0c\u4e26\u986f\u793a PaCE \u9054\u5230\u6700\u5148\u9032\u7684\u5c0d\u9f4a\n\u6548\u80fd\uff0c\u540c\u6642\u7dad\u6301\u8a9e\u8a00\u80fd\u529b\u3002", "author": "Jinqi Luo et.al.", "authors": "Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, Ren\u00e9 Vidal", "id": "2406.04331v1", "paper_url": "http://arxiv.org/abs/2406.04331v1", "repo": "https://github.com/peterljq/parsimonious-concept-engineering"}}