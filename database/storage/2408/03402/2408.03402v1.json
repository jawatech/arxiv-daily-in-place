{"2408.03402": {"publish_time": "2024-08-06", "title": "ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning", "paper_summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks, but leveraging them for dense passage embedding remains challenging.\nThis is due to their causal attention mechanism and the misalignment between\ntheir pre-training objectives and the text ranking tasks. Despite some recent\nefforts to address these issues, existing frameworks for LLM-based text\nembeddings have been limited by their support for only a limited range of LLM\narchitectures and fine-tuning strategies, limiting their practical application\nand versatility. In this work, we introduce the Unified framework for Large\nLanguage Model Embedding (ULLME), a flexible, plug-and-play implementation that\nenables bidirectional attention across various LLMs and supports a range of\nfine-tuning strategies. We also propose Generation-augmented Representation\nLearning (GRL), a novel fine-tuning method to boost LLMs for text embedding\ntasks. GRL enforces consistency between representation-based and\ngeneration-based relevance scores, leveraging LLMs' powerful generative\nabilities for learning passage embeddings. To showcase our framework's\nflexibility and effectiveness, we release three pre-trained models from ULLME\nwith different backbone architectures, ranging from 1.5B to 8B parameters, all\nof which demonstrate strong performance on the Massive Text Embedding\nBenchmark. Our framework is publicly available at:\nhttps://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found\nat https://rb.gy/ws1ile.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5229\u7528\u5b83\u5011\u9032\u884c\u5bc6\u96c6\u6bb5\u843d\u5d4c\u5165\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u9019\u662f\u56e0\u70ba\u5b83\u5011\u7684\u56e0\u679c\u6ce8\u610f\u6a5f\u5236\u4ee5\u53ca\u5b83\u5011\u7684\u9810\u8a13\u7df4\u76ee\u6a19\u8207\u6587\u672c\u6392\u5e8f\u4efb\u52d9\u4e4b\u9593\u7684\u932f\u4f4d\u3002\u5118\u7ba1\u6700\u8fd1\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u505a\u51fa\u4e86\u4e9b\u8a31\u52aa\u529b\uff0c\u4f46\u73fe\u6709\u7684\u57fa\u65bc LLM \u7684\u6587\u672c\u5d4c\u5165\u67b6\u69cb\u53d7\u5230\u5176\u50c5\u652f\u63f4\u6709\u9650\u7bc4\u570d\u7684 LLM \u67b6\u69cb\u548c\u5fae\u8abf\u7b56\u7565\u7684\u9650\u5236\uff0c\u5f9e\u800c\u9650\u5236\u4e86\u5b83\u5011\u7684\u5be6\u969b\u61c9\u7528\u548c\u591a\u529f\u80fd\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5d4c\u5165\u7684\u7d71\u4e00\u6846\u67b6 (ULLME)\uff0c\u9019\u662f\u4e00\u500b\u9748\u6d3b\u7684\u5373\u63d2\u5373\u7528\u5be6\u4f5c\uff0c\u53ef\u4ee5\u5728\u5404\u7a2e LLM \u4e2d\u555f\u7528\u96d9\u5411\u6ce8\u610f\uff0c\u4e26\u652f\u63f4\u4e00\u7cfb\u5217\u5fae\u8abf\u7b56\u7565\u3002\u6211\u5011\u9084\u63d0\u51fa\u4e86\u751f\u6210\u589e\u5f37\u8868\u793a\u5b78\u7fd2 (GRL)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u5fae\u8abf\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u5347 LLM \u4ee5\u9032\u884c\u6587\u672c\u5d4c\u5165\u4efb\u52d9\u3002GRL \u78ba\u4fdd\u57fa\u65bc\u8868\u793a\u548c\u57fa\u65bc\u751f\u6210\u7684\u76f8\u95dc\u6027\u5206\u6578\u4e4b\u9593\u7684\u4e00\u81f4\u6027\uff0c\u5229\u7528 LLM \u5f37\u5927\u7684\u751f\u6210\u80fd\u529b\u4f86\u5b78\u7fd2\u6bb5\u843d\u5d4c\u5165\u3002\u70ba\u4e86\u5c55\u793a\u6211\u5011\u6846\u67b6\u7684\u9748\u6d3b\u6027\u8207\u6709\u6548\u6027\uff0c\u6211\u5011\u5f9e ULLME \u767c\u5e03\u4e86\u4e09\u500b\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u5b83\u5011\u5177\u6709\u4e0d\u540c\u7684\u4e3b\u5e79\u67b6\u69cb\uff0c\u7bc4\u570d\u5f9e 1.5B \u5230 8B \u53c3\u6578\uff0c\u6240\u6709\u9019\u4e9b\u6a21\u578b\u5728 Massive Text Embedding Benchmark \u4e0a\u90fd\u8868\u73fe\u51fa\u5f37\u52c1\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u6846\u67b6\u516c\u958b\u65bc\uff1ahttps://github.com/nlp-uoregon/ullme\u3002ULLME \u7684\u793a\u7bc4\u5f71\u7247\u4e5f\u53ef\u4ee5\u5728 https://rb.gy/ws1ile \u627e\u5230\u3002", "author": "Hieu Man et.al.", "authors": "Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Thien Huu Nguyen", "id": "2408.03402v1", "paper_url": "http://arxiv.org/abs/2408.03402v1", "repo": "https://github.com/nlp-uoregon/ullme"}}