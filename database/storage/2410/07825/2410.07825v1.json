{"2410.07825": {"publish_time": "2024-10-10", "title": "Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models", "paper_summary": "Multi-lingual ability transfer has become increasingly important for the\nbroad application of large language models (LLMs). Existing work highly relies\non training with the multi-lingual ability-related data, which may be not\navailable for low-resource languages. To solve it, we propose a Multi-lingual\nAbility Extraction and Transfer approach, named as MAET. Our key idea is to\ndecompose and extract language-agnostic ability-related weights from LLMs, and\ntransfer them across different languages by simple addition and subtraction\noperations without training. Specially, our MAET consists of the extraction and\ntransfer stages. In the extraction stage, we firstly locate key neurons that\nare highly related to specific abilities, and then employ them to extract the\ntransferable ability-specific weights. In the transfer stage, we further select\nthe ability-related parameter tensors, and design the merging strategy based on\nthe linguistic and ability specific weights, to build the multi-lingual\nability-enhanced LLM. To demonstrate the effectiveness of our proposed\napproach, we conduct extensive experiments on mathematical and scientific tasks\nin both high-resource lingual and low-resource lingual scenarios. Experiment\nresults have shown that MAET can effectively and efficiently extract and\ntransfer the advanced abilities, and outperform training-based baseline\nmethods. Our code and data are available at\n\\url{https://github.com/RUCAIBox/MAET}.", "paper_summary_zh": "\u591a\u8bed\u8a00\u80fd\u529b\u8fc1\u79fb\u5bf9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5e7f\u6cdb\u5e94\u7528\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u73b0\u6709\u5de5\u4f5c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u4f7f\u7528\u4e0e\u591a\u8bed\u8a00\u80fd\u529b\u76f8\u5173\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u6765\u8bf4\u53ef\u80fd\u4e0d\u53ef\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MAET \u7684\u591a\u8bed\u8a00\u80fd\u529b\u63d0\u53d6\u548c\u8fc1\u79fb\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u5173\u952e\u601d\u60f3\u662f\u4ece LLM \u4e2d\u5206\u89e3\u548c\u63d0\u53d6\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u80fd\u529b\u76f8\u5173\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u7684\u52a0\u6cd5\u548c\u51cf\u6cd5\u8fd0\u7b97\u5728\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u8f6c\u79fb\u5b83\u4eec\uff0c\u800c\u65e0\u9700\u8bad\u7ec3\u3002\u7279\u522b\u5730\uff0c\u6211\u4eec\u7684 MAET \u5305\u542b\u63d0\u53d6\u548c\u8f6c\u79fb\u9636\u6bb5\u3002\u5728\u63d0\u53d6\u9636\u6bb5\uff0c\u6211\u4eec\u9996\u5148\u5b9a\u4f4d\u4e0e\u7279\u5b9a\u80fd\u529b\u9ad8\u5ea6\u76f8\u5173\u7684\u5173\u952e\u795e\u7ecf\u5143\uff0c\u7136\u540e\u4f7f\u7528\u5b83\u4eec\u6765\u63d0\u53d6\u53ef\u8f6c\u79fb\u7684\u80fd\u529b\u7279\u5b9a\u6743\u91cd\u3002\u5728\u8f6c\u79fb\u9636\u6bb5\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u9009\u62e9\u4e0e\u80fd\u529b\u76f8\u5173\u7684\u53c2\u6570\u5f20\u91cf\uff0c\u5e76\u6839\u636e\u8bed\u8a00\u548c\u80fd\u529b\u7279\u5b9a\u6743\u91cd\u8bbe\u8ba1\u5408\u5e76\u7b56\u7565\uff0c\u4ee5\u6784\u5efa\u591a\u8bed\u8a00\u80fd\u529b\u589e\u5f3a\u7684 LLM\u3002\u4e3a\u4e86\u8bc1\u660e\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e2d\u5bf9\u6570\u5b66\u548c\u79d1\u5b66\u4efb\u52a1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMAET \u53ef\u4ee5\u6709\u6548\u5730\u63d0\u53d6\u548c\u8f6c\u79fb\u9ad8\u7ea7\u80fd\u529b\uff0c\u5e76\u4e14\u4f18\u4e8e\u57fa\u4e8e\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728 \\url{https://github.com/RUCAIBox/MAET} \u83b7\u5f97\u3002", "author": "Zhipeng Chen et.al.", "authors": "Zhipeng Chen, Liang Song, Kun Zhou, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen", "id": "2410.07825v1", "paper_url": "http://arxiv.org/abs/2410.07825v1", "repo": "null"}}