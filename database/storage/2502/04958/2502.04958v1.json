{"2502.04958": {"publish_time": "2025-02-07", "title": "SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model", "paper_summary": "Fine-tuning is a key approach for adapting language models to specific\ndownstream tasks, but updating all model parameters becomes impractical as\nmodel sizes increase. Parameter-Efficient Fine-Tuning (PEFT) methods, such as\nLow-Rank Adaptation (LoRA), address this challenge by introducing additional\nadaptation parameters into pre-trained weight matrices. However, LoRA's\nperformance varies across different insertion points within the model,\nhighlighting potential parameter inefficiency due to unnecessary insertions. To\nthis end, we propose SSMLoRA (State Space Model Low-Rank Adaptation), an\nextension of LoRA that incorporates a State Space Model (SSM) to interconnect\nlow-rank matrices. SSMLoRA ensures that performance is maintained even with\nsparser insertions. SSMLoRA allows the model to not only map inputs to a\nlow-rank space for better feature extraction but also leverage the computations\nfrom the previous low-rank space. Our method achieves comparable performance to\nLoRA on the General Language Understanding Evaluation (GLUE) benchmark while\nusing only half the parameters. Additionally, due to its structure, SSMLoRA\nshows promise in handling tasks with longer input sequences. .You can find our\ncode here:https://github.com/yuhkalhic/SSMLoRA.", "paper_summary_zh": "\u5fae\u8abf\u662f\u4e00\u7a2e\u5c07\u8a9e\u8a00\u6a21\u578b\u9069\u61c9\u5230\u7279\u5b9a\u4e0b\u6e38\u4efb\u52d9\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u96a8\u8457\u6a21\u578b\u898f\u6a21\u7684\u589e\u52a0\uff0c\u66f4\u65b0\u6240\u6709\u6a21\u578b\u53c3\u6578\u8b8a\u5f97\u4e0d\u5207\u5be6\u969b\u3002\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\uff08\u4f8b\u5982\u4f4e\u79e9\u9069\u61c9 (LoRA)\uff09\u901a\u904e\u5728\u9810\u8a13\u7df4\u6b0a\u91cd\u77e9\u9663\u4e2d\u5f15\u5165\u984d\u5916\u7684\u9069\u61c9\u53c3\u6578\u4f86\u89e3\u6c7a\u9019\u4e00\u6311\u6230\u3002\u7136\u800c\uff0cLoRA \u7684\u6027\u80fd\u6703\u56e0\u6a21\u578b\u4e2d\u7684\u4e0d\u540c\u63d2\u5165\u9ede\u800c\u7570\uff0c\u9019\u7a81\u986f\u4e86\u7531\u65bc\u4e0d\u5fc5\u8981\u7684\u63d2\u5165\u800c\u5c0e\u81f4\u7684\u6f5b\u5728\u53c3\u6578\u6548\u7387\u4f4e\u4e0b\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SSMLoRA\uff08\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u4f4e\u79e9\u9069\u61c9\uff09\uff0c\u9019\u662f\u4e00\u7a2e LoRA \u7684\u64f4\u5c55\uff0c\u5b83\u7d50\u5408\u4e86\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM) \u4f86\u4e92\u9023\u4f4e\u79e9\u77e9\u9663\u3002SSMLoRA \u78ba\u4fdd\u5373\u4f7f\u5728\u63d2\u5165\u8f03\u7a00\u758f\u7684\u60c5\u6cc1\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6027\u80fd\u3002SSMLoRA \u4e0d\u50c5\u5141\u8a31\u6a21\u578b\u5c07\u8f38\u5165\u6620\u5c04\u5230\u4f4e\u79e9\u7a7a\u9593\u4ee5\u9032\u884c\u66f4\u597d\u7684\u7279\u5fb5\u63d0\u53d6\uff0c\u800c\u4e14\u9084\u5229\u7528\u4e86\u4f86\u81ea\u5148\u524d\u4f4e\u79e9\u7a7a\u9593\u7684\u8a08\u7b97\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u901a\u7528\u8a9e\u8a00\u7406\u89e3\u8a55\u4f30 (GLUE) \u57fa\u6e96\u4e0a\u5be6\u73fe\u4e86\u8207 LoRA \u76f8\u7576\u7684\u6027\u80fd\uff0c\u540c\u6642\u50c5\u4f7f\u7528\u4e86\u5f8c\u8005\u4e00\u534a\u7684\u53c3\u6578\u3002\u6b64\u5916\uff0c\u7531\u65bc\u5176\u7d50\u69cb\uff0cSSMLoRA \u5728\u8655\u7406\u5177\u6709\u8f03\u9577\u8f38\u5165\u5e8f\u5217\u7684\u4efb\u52d9\u65b9\u9762\u986f\u793a\u51fa\u524d\u666f\u3002\u60a8\u53ef\u4ee5\u5728\u9019\u88e1\u627e\u5230\u6211\u5011\u7684\u4ee3\u78bc\uff1ahttps://github.com/yuhkalhic/SSMLoRA\u3002", "author": "Jiayang Yu et.al.", "authors": "Jiayang Yu, Yihang Zhang, Bin Wang, Peiqin Lin, Yongkang Liu, Shi Feng", "id": "2502.04958v1", "paper_url": "http://arxiv.org/abs/2502.04958v1", "repo": "null"}}