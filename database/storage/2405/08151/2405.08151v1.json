{"2405.08151": {"publish_time": "2024-05-13", "title": "Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness", "paper_summary": "Large language models (LLM) have demonstrated remarkable capabilities in\nvarious biomedical natural language processing (NLP) tasks, leveraging the\ndemonstration within the input context to adapt to new tasks. However, LLM is\nsensitive to the selection of demonstrations. To address the hallucination\nissue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by\nretrieving pertinent information from an established database. Nonetheless,\nexisting research work lacks rigorous evaluation of the impact of\nretrieval-augmented large language models on different biomedical NLP tasks.\nThis deficiency makes it challenging to ascertain the capabilities of RAL\nwithin the biomedical domain. Moreover, the outputs from RAL are affected by\nretrieving the unlabeled, counterfactual, or diverse knowledge that is not well\nstudied in the biomedical domain. However, such knowledge is common in the real\nworld. Finally, exploring the self-awareness ability is also crucial for the\nRAL system. So, in this paper, we systematically investigate the impact of RALs\non 5 different biomedical tasks (triple extraction, link prediction,\nclassification, question answering, and natural language inference). We analyze\nthe performance of RALs in four fundamental abilities, including unlabeled\nrobustness, counterfactual robustness, diverse robustness, and negative\nawareness. To this end, we proposed an evaluation framework to assess the RALs'\nperformance on different biomedical NLP tasks and establish four different\ntestbeds based on the aforementioned fundamental abilities. Then, we evaluate 3\nrepresentative LLMs with 3 different retrievers on 5 tasks over 9 datasets.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u751f\u7269\u91ab\u5b78\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u5229\u7528\u8f38\u5165\u5167\u5bb9\u4e2d\u7684\u793a\u7bc4\u4f86\u9069\u61c9\u65b0\u4efb\u52d9\u3002\u7136\u800c\uff0cLLM \u5c0d\u793a\u7bc4\u7684\u9078\u64c7\u5f88\u654f\u611f\u3002\u70ba\u4e86\u89e3\u6c7a LLM \u4e2d\u56fa\u6709\u7684\u5e7b\u89ba\u554f\u984c\uff0c\u6aa2\u7d22\u589e\u5f37 LLM (RAL) \u63d0\u4f9b\u4e86\u4e00\u500b\u89e3\u6c7a\u65b9\u6848\uff0c\u5f9e\u65e2\u5b9a\u7684\u8cc7\u6599\u5eab\u4e2d\u6aa2\u7d22\u76f8\u95dc\u8cc7\u8a0a\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u73fe\u6709\u7684\u7814\u7a76\u5de5\u4f5c\u7f3a\u4e4f\u5c0d\u6aa2\u7d22\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5c0d\u4e0d\u540c\u751f\u7269\u91ab\u5b78 NLP \u4efb\u52d9\u7684\u5f71\u97ff\u9032\u884c\u56b4\u683c\u8a55\u4f30\u3002\u9019\u7a2e\u7f3a\u9677\u4f7f\u5f97\u96e3\u4ee5\u78ba\u5b9a RAL \u5728\u751f\u7269\u91ab\u5b78\u9818\u57df\u4e2d\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0cRAL \u7684\u8f38\u51fa\u53d7\u5230\u6aa2\u7d22\u672a\u6a19\u8a18\u3001\u53cd\u4e8b\u5be6\u6216\u591a\u6a23\u5316\u77e5\u8b58\u7684\u5f71\u97ff\uff0c\u800c\u9019\u4e9b\u77e5\u8b58\u5728\u751f\u7269\u91ab\u5b78\u9818\u57df\u4e2d\u4e26\u672a\u5f97\u5230\u5f88\u597d\u7684\u7814\u7a76\u3002\u7136\u800c\uff0c\u9019\u7a2e\u77e5\u8b58\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u5f88\u5e38\u898b\u3002\u6700\u5f8c\uff0c\u63a2\u7d22\u81ea\u6211\u610f\u8b58\u80fd\u529b\u5c0d\u65bc RAL \u7cfb\u7d71\u4f86\u8aaa\u4e5f\u81f3\u95dc\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u5730\u7814\u7a76\u4e86 RAL \u5c0d 5 \u500b\u4e0d\u540c\u751f\u7269\u91ab\u5b78\u4efb\u52d9\uff08\u4e09\u5143\u7d44\u63d0\u53d6\u3001\u9023\u7d50\u9810\u6e2c\u3001\u5206\u985e\u3001\u554f\u984c\u56de\u7b54\u548c\u81ea\u7136\u8a9e\u8a00\u63a8\u7406\uff09\u7684\u5f71\u97ff\u3002\u6211\u5011\u5206\u6790\u4e86 RAL \u5728\u56db\u9805\u57fa\u672c\u80fd\u529b\u4e2d\u7684\u8868\u73fe\uff0c\u5305\u62ec\u672a\u6a19\u8a18\u9b6f\u68d2\u6027\u3001\u53cd\u4e8b\u5be6\u9b6f\u68d2\u6027\u3001\u591a\u6a23\u6027\u9b6f\u68d2\u6027\u548c\u8ca0\u9762\u610f\u8b58\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u8a55\u4f30\u6846\u67b6\u4f86\u8a55\u4f30 RAL \u5728\u4e0d\u540c\u751f\u7269\u91ab\u5b78 NLP \u4efb\u52d9\u4e0a\u7684\u8868\u73fe\uff0c\u4e26\u6839\u64da\u4e0a\u8ff0\u57fa\u672c\u80fd\u529b\u5efa\u7acb\u4e86\u56db\u500b\u4e0d\u540c\u7684\u6e2c\u8a66\u5e73\u53f0\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5728 9 \u500b\u8cc7\u6599\u96c6\u4e0a\u7684 5 \u500b\u4efb\u52d9\u4e2d\u8a55\u4f30\u4e86 3 \u500b\u4e0d\u540c\u7684\u6aa2\u7d22\u5668\u548c 3 \u500b\u4ee3\u8868\u6027 LLM\u3002", "author": "Mingchen Li et.al.", "authors": "Mingchen Li, Zaifu Zhan, Han Yang, Yongkang Xiao, Jiatan Huang, Rui Zhang", "id": "2405.08151v1", "paper_url": "http://arxiv.org/abs/2405.08151v1", "repo": "null"}}