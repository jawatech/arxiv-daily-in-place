{"2405.08768": {"publish_time": "2024-05-14", "title": "EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training", "paper_summary": "The superior performance of modern visual backbones usually comes with a\ncostly training procedure. We contribute to this issue by generalizing the idea\nof curriculum learning beyond its original formulation, i.e., training models\nusing easier-to-harder data. Specifically, we reformulate the training\ncurriculum as a soft-selection function, which uncovers progressively more\ndifficult patterns within each example during training, instead of performing\neasier-to-harder sample selection. Our work is inspired by an intriguing\nobservation on the learning dynamics of visual backbones: during the earlier\nstages of training, the model predominantly learns to recognize some\n'easier-to-learn' discriminative patterns in the data. These patterns, when\nobserved through frequency and spatial domains, incorporate lower-frequency\ncomponents, and the natural image contents without distortion or data\naugmentation. Motivated by these findings, we propose a curriculum where the\nmodel always leverages all the training data at every learning stage, yet the\nexposure to the 'easier-to-learn' patterns of each example is initiated first,\nwith harder patterns gradually introduced as training progresses. To implement\nthis idea in a computationally efficient way, we introduce a cropping operation\nin the Fourier spectrum of the inputs, enabling the model to learn from only\nthe lower-frequency components. Then we show that exposing the contents of\nnatural images can be readily achieved by modulating the intensity of data\naugmentation. Finally, we integrate these aspects and design curriculum\nschedules with tailored search algorithms. The resulting method,\nEfficientTrain++, is simple, general, yet surprisingly effective. It reduces\nthe training time of a wide variety of popular models by 1.5-3.0x on\nImageNet-1K/22K without sacrificing accuracy. It also demonstrates efficacy in\nself-supervised learning (e.g., MAE).", "paper_summary_zh": "\u73fe\u4ee3\u8996\u89ba\u9aa8\u5e79\u7684\u512a\u7570\u6548\u80fd\u901a\u5e38\u4f34\u96a8\u8457\u6602\u8cb4\u7684\u8a13\u7df4\u7a0b\u5e8f\u3002\u6211\u5011\u900f\u904e\u5c07\u8ab2\u7a0b\u5b78\u7fd2\u7684\u6982\u5ff5\u63a8\u5ee3\u5230\u5176\u539f\u59cb\u516c\u5f0f\u4e4b\u5916\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u5373\u4f7f\u7528\u6613\u65bc\u96e3\u7684\u8cc7\u6599\u8a13\u7df4\u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u8a13\u7df4\u8ab2\u7a0b\u91cd\u65b0\u8868\u8ff0\u70ba\u8edf\u9078\u64c7\u51fd\u6578\uff0c\u5728\u8a13\u7df4\u671f\u9593\u9010\u6b65\u63ed\u793a\u6bcf\u500b\u7bc4\u4f8b\u4e2d\u66f4\u56f0\u96e3\u7684\u6a21\u5f0f\uff0c\u800c\u4e0d\u662f\u57f7\u884c\u6613\u65bc\u96e3\u7684\u6a23\u672c\u9078\u64c7\u3002\u6211\u5011\u7684\u7814\u7a76\u9748\u611f\u4f86\u81ea\u5c0d\u8996\u89ba\u9aa8\u5e79\u5b78\u7fd2\u52d5\u614b\u7684\u6709\u8da3\u89c0\u5bdf\uff1a\u5728\u8a13\u7df4\u7684\u65e9\u671f\u968e\u6bb5\uff0c\u6a21\u578b\u4e3b\u8981\u5b78\u7fd2\u8b58\u5225\u8cc7\u6599\u4e2d\u4e00\u4e9b\u300c\u8f03\u5bb9\u6613\u5b78\u7fd2\u300d\u7684\u8fa8\u5225\u6a21\u5f0f\u3002\u9019\u4e9b\u6a21\u5f0f\u5728\u900f\u904e\u983b\u7387\u548c\u7a7a\u9593\u57df\u89c0\u5bdf\u6642\uff0c\u6703\u7d0d\u5165\u8f03\u4f4e\u983b\u7387\u7684\u7d44\u6210\uff0c\u4ee5\u53ca\u6c92\u6709\u5931\u771f\u6216\u8cc7\u6599\u589e\u5f37\u7684\u81ea\u7136\u5f71\u50cf\u5167\u5bb9\u3002\u53d7\u5230\u9019\u4e9b\u767c\u73fe\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u8ab2\u7a0b\uff0c\u5176\u4e2d\u6a21\u578b\u5728\u6bcf\u500b\u5b78\u7fd2\u968e\u6bb5\u59cb\u7d42\u5229\u7528\u6240\u6709\u8a13\u7df4\u8cc7\u6599\uff0c\u4f46\u9996\u5148\u63a5\u89f8\u6bcf\u500b\u7bc4\u4f8b\u7684\u300c\u8f03\u5bb9\u6613\u5b78\u7fd2\u300d\u6a21\u5f0f\uff0c\u4e26\u96a8\u8457\u8a13\u7df4\u7684\u9032\u5c55\u9010\u6f38\u5f15\u5165\u66f4\u56f0\u96e3\u7684\u6a21\u5f0f\u3002\u70ba\u4e86\u4ee5\u8a08\u7b97\u6709\u6548\u7387\u7684\u65b9\u5f0f\u5be6\u4f5c\u9019\u500b\u60f3\u6cd5\uff0c\u6211\u5011\u5728\u8f38\u5165\u7684\u5085\u7acb\u8449\u983b\u8b5c\u4e2d\u5f15\u5165\u88c1\u5207\u64cd\u4f5c\uff0c\u8b93\u6a21\u578b\u53ea\u80fd\u5f9e\u8f03\u4f4e\u983b\u7387\u7684\u7d44\u6210\u5b78\u7fd2\u3002\u7136\u5f8c\u6211\u5011\u8868\u660e\uff0c\u900f\u904e\u8abf\u6574\u8cc7\u6599\u589e\u5f37\u7684\u5f37\u5ea6\uff0c\u53ef\u4ee5\u8f15\u6613\u5730\u63ed\u9732\u81ea\u7136\u5f71\u50cf\u7684\u5167\u5bb9\u3002\u6700\u5f8c\uff0c\u6211\u5011\u6574\u5408\u9019\u4e9b\u9762\u5411\uff0c\u4e26\u8a2d\u8a08\u51fa\u5177\u5099\u5ba2\u88fd\u5316\u641c\u5c0b\u6f14\u7b97\u6cd5\u7684\u8ab2\u7a0b\u6642\u7a0b\u3002\u7531\u6b64\u7522\u751f\u7684\u65b9\u6cd5 EfficientTrain++ \u7c21\u55ae\u3001\u901a\u7528\uff0c\u4f46\u6548\u679c\u9a5a\u4eba\u3002\u5b83\u5728\u4e0d\u72a7\u7272\u6e96\u78ba\u6027\u7684\u60c5\u6cc1\u4e0b\uff0c\u5c07\u5404\u7a2e\u71b1\u9580\u6a21\u578b\u5728 ImageNet-1K/22K \u4e0a\u7684\u8a13\u7df4\u6642\u9593\u6e1b\u5c11\u4e86 1.5-3.0 \u500d\u3002\u5b83\u4e5f\u8b49\u660e\u4e86\u5728\u81ea\u6211\u76e3\u7763\u5b78\u7fd2\uff08\u4f8b\u5982 MAE\uff09\u4e2d\u7684\u6548\u529b\u3002", "author": "Yulin Wang et.al.", "authors": "Yulin Wang, Yang Yue, Rui Lu, Yizeng Han, Shiji Song, Gao Huang", "id": "2405.08768v1", "paper_url": "http://arxiv.org/abs/2405.08768v1", "repo": "https://github.com/leaplabthu/efficienttrain"}}