{"2407.09111": {"publish_time": "2024-07-12", "title": "Inference Optimization of Foundation Models on AI Accelerators", "paper_summary": "Powerful foundation models, including large language models (LLMs), with\nTransformer architectures have ushered in a new era of Generative AI across\nvarious industries. Industry and research community have witnessed a large\nnumber of new applications, based on those foundation models. Such applications\ninclude question and answer, customer services, image and video generation, and\ncode completions, among others. However, as the number of model parameters\nreaches to hundreds of billions, their deployment incurs prohibitive inference\ncosts and high latency in real-world scenarios. As a result, the demand for\ncost-effective and fast inference using AI accelerators is ever more higher. To\nthis end, our tutorial offers a comprehensive discussion on complementary\ninference optimization techniques using AI accelerators. Beginning with an\noverview of basic Transformer architectures and deep learning system\nframeworks, we deep dive into system optimization techniques for fast and\nmemory-efficient attention computations and discuss how they can be implemented\nefficiently on AI accelerators. Next, we describe architectural elements that\nare key for fast transformer inference. Finally, we examine various model\ncompression and fast decoding strategies in the same context.", "paper_summary_zh": "\u5f37\u5927\u7684\u57fa\u790e\u6a21\u578b\uff0c\u5305\u62ec\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u914d\u5099 Transformer \u67b6\u69cb\uff0c\u5df2\u7d93\u5728\u5404\u7522\u696d\u5f15\u9032\u4e86\u751f\u6210\u5f0f AI \u7684\u65b0\u6642\u4ee3\u3002\u7522\u696d\u548c\u7814\u7a76\u793e\u7fa4\u898b\u8b49\u4e86\u5927\u91cf\u57fa\u65bc\u9019\u4e9b\u57fa\u790e\u6a21\u578b\u7684\u65b0\u61c9\u7528\u7a0b\u5f0f\u3002\u6b64\u985e\u61c9\u7528\u7a0b\u5f0f\u5305\u62ec\u554f\u7b54\u3001\u5ba2\u6236\u670d\u52d9\u3001\u5f71\u50cf\u548c\u5f71\u7247\u751f\u6210\uff0c\u4ee5\u53ca\u7a0b\u5f0f\u78bc\u5b8c\u6210\u7b49\u3002\u7136\u800c\uff0c\u7531\u65bc\u6a21\u578b\u53c3\u6578\u6578\u91cf\u9054\u5230\u6578\u767e\u5104\uff0c\u5176\u90e8\u7f72\u6703\u5728\u5be6\u969b\u60c5\u6cc1\u4e2d\u7522\u751f\u7981\u6b62\u7684\u63a8\u8ad6\u6210\u672c\u548c\u9ad8\u5ef6\u9072\u3002\u56e0\u6b64\uff0c\u4f7f\u7528 AI \u52a0\u901f\u5668\u9032\u884c\u7d93\u6fdf\u6709\u6548\u4e14\u5feb\u901f\u7684\u63a8\u8ad6\u7684\u9700\u6c42\u8d8a\u4f86\u8d8a\u9ad8\u3002\u70ba\u6b64\uff0c\u6211\u5011\u7684\u6559\u5b78\u63d0\u4f9b\u4e86\u4e00\u500b\u5168\u9762\u7684\u8a0e\u8ad6\uff0c\u8aaa\u660e\u4f7f\u7528 AI \u52a0\u901f\u5668\u7684\u4e92\u88dc\u63a8\u8ad6\u6700\u4f73\u5316\u6280\u8853\u3002\u5f9e\u57fa\u672c Transformer \u67b6\u69cb\u548c\u6df1\u5ea6\u5b78\u7fd2\u7cfb\u7d71\u67b6\u69cb\u7684\u6982\u89c0\u958b\u59cb\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u5feb\u901f\u4e14\u8a18\u61b6\u9ad4\u6709\u6548\u7387\u7684\u6ce8\u610f\u529b\u904b\u7b97\u7684\u7cfb\u7d71\u6700\u4f73\u5316\u6280\u8853\uff0c\u4e26\u8a0e\u8ad6\u5982\u4f55\u6709\u6548\u7387\u5730\u5728 AI \u52a0\u901f\u5668\u4e0a\u5be6\u4f5c\u5b83\u5011\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u5c0d\u65bc\u5feb\u901f Transformer \u63a8\u8ad6\u81f3\u95dc\u91cd\u8981\u7684\u67b6\u69cb\u5143\u7d20\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728\u76f8\u540c\u7684\u8108\u7d61\u4e2d\u6aa2\u8996\u5404\u7a2e\u6a21\u578b\u58d3\u7e2e\u548c\u5feb\u901f\u89e3\u78bc\u7b56\u7565\u3002", "author": "Youngsuk Park et.al.", "authors": "Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas K\u00fcbler, Jiaji Huang, Matth\u00e4us Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis", "id": "2407.09111v1", "paper_url": "http://arxiv.org/abs/2407.09111v1", "repo": "null"}}