{"2406.12182": {"publish_time": "2024-06-18", "title": "Aqulia-Med LLM: Pioneering Full-Process Open-Source Medical Language Models", "paper_summary": "Recently, both closed-source LLMs and open-source communities have made\nsignificant strides, outperforming humans in various general domains. However,\ntheir performance in specific professional fields such as medicine, especially\nwithin the open-source community, remains suboptimal due to the complexity of\nmedical knowledge. We propose Aquila-Med, a bilingual medical LLM based on\nAquila, addressing these challenges through continue pre-training, supervised\nfine-tuning (SFT), and reinforcement learning from human feedback (RLHF). We\nconstruct a large-scale Chinese and English medical dataset for continue\npre-training and a high-quality SFT dataset, covering extensive medical\nspecialties. Additionally, we develop a high-quality Direct Preference\nOptimization (DPO) dataset for further alignment. Aquila-Med achieves notable\nresults across single-turn, multi-turn dialogues, and medical multiple-choice\nquestions, demonstrating the effectiveness of our approach. We open-source the\ndatasets and the entire training process, contributing valuable resources to\nthe research community. Our models and datasets will released at\nhttps://huggingface.co/BAAI/AquilaMed-RL.", "paper_summary_zh": "\u8fd1\u6765\uff0c\u95ed\u6e90 LLM \u548c\u5f00\u6e90\u793e\u533a\u90fd\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5728\u5404\u79cd\u901a\u7528\u9886\u57df\u7684\u8868\u73b0\u90fd\u4f18\u4e8e\u4eba\u7c7b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u533b\u5b66\u7b49\u7279\u5b9a\u4e13\u4e1a\u9886\u57df\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5f00\u6e90\u793e\u533a\u4e2d\uff0c\u7531\u4e8e\u533b\u5b66\u77e5\u8bc6\u7684\u590d\u6742\u6027\uff0c\u4ecd\u7136\u4e0d\u591f\u7406\u60f3\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Aquila-Med\uff0c\u4e00\u4e2a\u57fa\u4e8e Aquila \u7684\u53cc\u8bed\u533b\u5b66 LLM\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03 (SFT) \u548c\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60 (RLHF) \u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4e2d\u82f1\u6587\u533b\u5b66\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u7684 SFT \u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u533b\u5b66\u4e13\u4e1a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO) \u6570\u636e\u96c6\uff0c\u4ee5\u8fdb\u4e00\u6b65\u5bf9\u9f50\u3002Aquila-Med \u5728\u5355\u8f6e\u3001\u591a\u8f6e\u5bf9\u8bdd\u548c\u533b\u5b66\u591a\u9879\u9009\u62e9\u9898\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u5f00\u6e90\u6570\u636e\u96c6\u548c\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u8d21\u732e\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002\u6211\u4eec\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u5c06\u5728 https://huggingface.co/BAAI/AquilaMed-RL \u53d1\u5e03\u3002", "author": "Lulu Zhao et.al.", "authors": "Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou, Donglin Hao, Yonghua Lin", "id": "2406.12182v1", "paper_url": "http://arxiv.org/abs/2406.12182v1", "repo": "null"}}