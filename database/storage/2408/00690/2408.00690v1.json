{"2408.00690": {"publish_time": "2024-08-01", "title": "Improving Text Embeddings for Smaller Language Models Using Contrastive Fine-tuning", "paper_summary": "While Large Language Models show remarkable performance in natural language\nunderstanding, their resource-intensive nature makes them less accessible. In\ncontrast, smaller language models such as MiniCPM offer more sustainable\nscalability, but often underperform without specialized optimization. In this\npaper, we explore the enhancement of smaller language models through the\nimprovement of their text embeddings. We select three language models, MiniCPM,\nPhi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our\nresults demonstrate that this fine-tuning method enhances the quality of text\nembeddings for all three models across various benchmarks, with MiniCPM showing\nthe most significant improvements of an average 56.33\\% performance gain. The\ncontrastive fine-tuning code is publicly available at\nhttps://github.com/trapoom555/Language-Model-STS-CFT.", "paper_summary_zh": "\u96d6\u7136\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u65b9\u9762\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u4f46\u5176\u8cc7\u6e90\u5bc6\u96c6\u7684\u672c\u8cea\u4f7f\u5176\u8f03\u96e3\u4ee5\u53d6\u5f97\u3002\u76f8\u8f03\u4e4b\u4e0b\uff0c\u50cf MiniCPM \u7b49\u8f03\u5c0f\u7684\u8a9e\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6c38\u7e8c\u7684\u53ef\u64f4\u5145\u6027\uff0c\u4f46\u901a\u5e38\u5728\u6c92\u6709\u7279\u6b8a\u6700\u4f73\u5316\u7684\u60c5\u6cc1\u4e0b\u8868\u73fe\u4e0d\u4f73\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u900f\u904e\u6539\u5584\u5176\u6587\u5b57\u5d4c\u5165\u4f86\u589e\u5f37\u8f03\u5c0f\u8a9e\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u6211\u5011\u9078\u53d6\u4e86\u4e09\u500b\u8a9e\u8a00\u6a21\u578b\uff0cMiniCPM\u3001Phi-2 \u548c Gemma\uff0c\u5728 NLI \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5c0d\u6bd4\u5fae\u8abf\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\uff0c\u9019\u7a2e\u5fae\u8abf\u65b9\u6cd5\u63d0\u5347\u4e86\u6240\u6709\u4e09\u500b\u6a21\u578b\u5728\u5404\u7a2e\u57fa\u6e96\u4e0a\u7684\u6587\u5b57\u5d4c\u5165\u54c1\u8cea\uff0c\u5176\u4e2d MiniCPM \u986f\u793a\u51fa\u6700\u986f\u8457\u7684\u6539\u9032\uff0c\u5e73\u5747\u6548\u80fd\u63d0\u5347\u4e86 56.33%\u3002\u5c0d\u6bd4\u5fae\u8abf\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u5728 https://github.com/trapoom555/Language-Model-STS-CFT\u3002", "author": "Trapoom Ukarapol et.al.", "authors": "Trapoom Ukarapol, Zhicheng Lee, Amy Xin", "id": "2408.00690v1", "paper_url": "http://arxiv.org/abs/2408.00690v1", "repo": "null"}}