{"2406.01314": {"publish_time": "2024-06-03", "title": "Compute-Efficient Medical Image Classification with Softmax-Free Transformers and Sequence Normalization", "paper_summary": "The Transformer model has been pivotal in advancing fields such as natural\nlanguage processing, speech recognition, and computer vision. However, a\ncritical limitation of this model is its quadratic computational and memory\ncomplexity relative to the sequence length, which constrains its application to\nlonger sequences. This is especially crucial in medical imaging where\nhigh-resolution images can reach gigapixel scale. Efforts to address this issue\nhave predominantely focused on complex techniques, such as decomposing the\nsoftmax operation integral to the Transformer's architecture. This paper\naddresses this quadratic computational complexity of Transformer models and\nintroduces a remarkably simple and effective method that circumvents this issue\nby eliminating the softmax function from the attention mechanism and adopting a\nsequence normalization technique for the key, query, and value tokens. Coupled\nwith a reordering of matrix multiplications this approach reduces the memory-\nand compute complexity to a linear scale. We evaluate this approach across\nvarious medical imaging datasets comprising fundoscopic, dermascopic,\nradiologic and histologic imaging data. Our findings highlight that these\nmodels exhibit a comparable performance to traditional transformer models,\nwhile efficiently handling longer sequences.", "paper_summary_zh": "Transformer \u6a21\u578b\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3001\u8a9e\u97f3\u8fa8\u8b58\u548c\u96fb\u8166\u8996\u89ba\u7b49\u9818\u57df\u7684\u9032\u5c55\u4e2d\u767c\u63ee\u4e86\u95dc\u9375\u4f5c\u7528\u3002\u7136\u800c\uff0c\u6b64\u6a21\u578b\u7684\u4e00\u500b\u91cd\u5927\u9650\u5236\u662f\u5176\u4e8c\u6b21\u8a08\u7b97\u548c\u8a18\u61b6\u9ad4\u8907\u96dc\u5ea6\u76f8\u5c0d\u65bc\u5e8f\u5217\u9577\u5ea6\uff0c\u9019\u9650\u5236\u4e86\u5176\u5728\u8f03\u9577\u5e8f\u5217\u4e2d\u7684\u61c9\u7528\u3002\u9019\u5728\u91ab\u7642\u5f71\u50cf\u4e2d\u5c24\u5176\u91cd\u8981\uff0c\u5176\u4e2d\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u53ef\u4ee5\u9054\u5230\u5409\u50cf\u7d20\u7b49\u7d1a\u3002\u89e3\u6c7a\u6b64\u554f\u984c\u7684\u52aa\u529b\u4e3b\u8981\u96c6\u4e2d\u5728\u8907\u96dc\u6280\u8853\u4e0a\uff0c\u4f8b\u5982\u5206\u89e3 Transformer \u67b6\u69cb\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684 softmax \u904b\u7b97\u3002\u672c\u6587\u63a2\u8a0e\u4e86 Transformer \u6a21\u578b\u7684\u4e8c\u6b21\u8a08\u7b97\u8907\u96dc\u5ea6\uff0c\u4e26\u4ecb\u7d39\u4e86\u4e00\u7a2e\u975e\u5e38\u7c21\u55ae\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u900f\u904e\u5f9e\u6ce8\u610f\u529b\u6a5f\u5236\u4e2d\u79fb\u9664 softmax \u51fd\u6578\u4e26\u63a1\u7528\u95dc\u9375\u3001\u67e5\u8a62\u548c\u503c\u7b26\u865f\u7684\u5e8f\u5217\u6b63\u898f\u5316\u6280\u8853\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\u3002\u7d50\u5408\u77e9\u9663\u4e58\u6cd5\u7684\u91cd\u65b0\u6392\u5e8f\uff0c\u6b64\u65b9\u6cd5\u5c07\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u8907\u96dc\u5ea6\u964d\u4f4e\u5230\u7dda\u6027\u7b49\u7d1a\u3002\u6211\u5011\u5728\u5305\u542b\u773c\u5e95\u93e1\u3001\u76ae\u819a\u93e1\u3001\u653e\u5c04\u5b78\u548c\u7d44\u7e54\u5b78\u5f71\u50cf\u8cc7\u6599\u7684\u5404\u7a2e\u91ab\u5b78\u5f71\u50cf\u8cc7\u6599\u96c6\u4e0a\u8a55\u4f30\u6b64\u65b9\u6cd5\u3002\u6211\u5011\u7684\u767c\u73fe\u5f37\u8abf\uff0c\u9019\u4e9b\u6a21\u578b\u8868\u73fe\u51fa\u8207\u50b3\u7d71 Transformer \u6a21\u578b\u76f8\u7576\u7684\u6548\u80fd\uff0c\u540c\u6642\u6709\u6548\u5730\u8655\u7406\u8f03\u9577\u7684\u5e8f\u5217\u3002", "author": "Firas Khader et.al.", "authors": "Firas Khader, Omar S. M. El Nahhas, Tianyu Han, Gustav M\u00fcller-Franzes, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn", "id": "2406.01314v1", "paper_url": "http://arxiv.org/abs/2406.01314v1", "repo": "null"}}