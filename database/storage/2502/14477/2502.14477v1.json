{"2502.14477": {"publish_time": "2025-02-20", "title": "Unshackling Context Length: An Efficient Selective Attention Approach through Query-Key Compression", "paper_summary": "Handling long-context sequences efficiently remains a significant challenge\nin large language models (LLMs). Existing methods for token selection in\nsequence extrapolation either employ a permanent eviction strategy or select\ntokens by chunk, which may lead to the loss of critical information. We propose\nEfficient Selective Attention (ESA), a novel approach that extends context\nlength by efficiently selecting the most critical tokens at the token level to\ncompute attention. ESA reduces the computational complexity of token selection\nby compressing query and key vectors into lower-dimensional representations. We\nevaluate ESA on long sequence benchmarks with maximum lengths up to 256k using\nopen-source LLMs with context lengths of 8k and 32k. ESA outperforms other\nselective attention methods, especially in tasks requiring the retrieval of\nmultiple pieces of information, achieving comparable performance to\nfull-attention extrapolation methods across various tasks, with superior\nresults in certain tasks.", "paper_summary_zh": "\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\uff0c\u6709\u6548\u8655\u7406\u9577\u8a9e\u5883\u5e8f\u5217\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u73fe\u6709\u7684\u5e8f\u5217\u5916\u63a8\u6a19\u8a18\u9078\u64c7\u65b9\u6cd5\u63a1\u7528\u6c38\u4e45\u9a45\u9010\u7b56\u7565\u6216\u6309\u584a\u9078\u64c7\u6a19\u8a18\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u95dc\u9375\u8cc7\u8a0a\u907a\u5931\u3002\u6211\u5011\u63d0\u51fa\u9ad8\u6548\u9078\u64c7\u6027\u6ce8\u610f (ESA)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u5b83\u900f\u904e\u5728\u6a19\u8a18\u5c64\u7d1a\u6709\u6548\u9078\u64c7\u6700\u95dc\u9375\u7684\u6a19\u8a18\u4f86\u8a08\u7b97\u6ce8\u610f\uff0c\u5f9e\u800c\u5ef6\u4f38\u8a9e\u5883\u9577\u5ea6\u3002ESA \u900f\u904e\u5c07\u67e5\u8a62\u548c\u95dc\u9375\u5411\u91cf\u58d3\u7e2e\u6210\u8f03\u4f4e\u7dad\u5ea6\u7684\u8868\u793a\uff0c\u4f86\u964d\u4f4e\u6a19\u8a18\u9078\u64c7\u7684\u904b\u7b97\u8907\u96dc\u5ea6\u3002\u6211\u5011\u4f7f\u7528\u958b\u653e\u539f\u59cb\u78bc LLM\uff0c\u5728\u8a9e\u5883\u9577\u5ea6\u70ba 8k \u548c 32k \u7684\u60c5\u6cc1\u4e0b\uff0c\u5c0d\u9577\u5e8f\u5217\u57fa\u6e96\u9032\u884c\u8a55\u4f30\uff0c\u6700\u5927\u9577\u5ea6\u9054 256k\u3002ESA \u7684\u8868\u73fe\u512a\u65bc\u5176\u4ed6\u9078\u64c7\u6027\u6ce8\u610f\u65b9\u6cd5\uff0c\u7279\u5225\u662f\u5728\u9700\u8981\u64f7\u53d6\u591a\u689d\u8cc7\u8a0a\u7684\u4efb\u52d9\u4e2d\uff0c\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u9054\u5230\u8207\u5168\u6ce8\u610f\u5916\u63a8\u65b9\u6cd5\u76f8\u7576\u7684\u6548\u80fd\uff0c\u4e26\u4e14\u5728\u67d0\u4e9b\u4efb\u52d9\u4e2d\u7372\u5f97\u66f4\u4f73\u7684\u7d50\u679c\u3002", "author": "Haoyu Wang et.al.", "authors": "Haoyu Wang, Tong Teng, Tianyu Guo, An Xiao, Duyu Tang, Hanting Chen, Yunhe Wang", "id": "2502.14477v1", "paper_url": "http://arxiv.org/abs/2502.14477v1", "repo": "null"}}