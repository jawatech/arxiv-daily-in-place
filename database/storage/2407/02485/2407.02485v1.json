{"2407.02485": {"publish_time": "2024-07-02", "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs", "paper_summary": "Large language models (LLMs) typically utilize the top-k contexts from a\nretriever in retrieval-augmented generation (RAG). In this work, we propose a\nnovel instruction fine-tuning framework RankRAG, which instruction-tunes a\nsingle LLM for the dual purpose of context ranking and answer generation in\nRAG. In particular, the instruction-tuned LLMs work surprisingly well by adding\na small fraction of ranking data into the training blend, and outperform\nexisting expert ranking models, including the same LLM exclusively fine-tuned\non a large amount of ranking data. For generation, we compare our model with\nmany strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and\nChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG\nbenchmarks. Specifically, our Llama3-RankRAG significantly outperforms\nLlama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In\naddition, it also performs comparably to GPT-4 on five RAG benchmarks in the\nbiomedical domain without instruction fine-tuning on biomedical data,\ndemonstrating its superb capability for generalization to new domains.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u5e38\u5728\u6aa2\u7d22\u589e\u5f37\u751f\u6210\uff08RAG\uff09\u4e2d\u5229\u7528\u6aa2\u7d22\u5668\u7684 top-k \u80cc\u666f\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u6307\u4ee4\u5fae\u8abf\u6846\u67b6 RankRAG\uff0c\u5b83\u70ba\u55ae\u4e00 LLM \u9032\u884c\u6307\u4ee4\u5fae\u8abf\uff0c\u4ee5\u9054\u5230 RAG \u4e2d\u80cc\u666f\u6392\u540d\u548c\u7b54\u6848\u751f\u6210\u7684\u96d9\u91cd\u76ee\u7684\u3002\u7279\u5225\u662f\uff0c\u6307\u4ee4\u5fae\u8abf\u7684 LLM \u900f\u904e\u5c07\u4e00\u5c0f\u90e8\u5206\u6392\u540d\u8cc7\u6599\u65b0\u589e\u5230\u8a13\u7df4\u6df7\u5408\u4e2d\u800c\u7522\u751f\u9a5a\u4eba\u7684\u6548\u679c\uff0c\u4e26\u512a\u65bc\u73fe\u6709\u7684\u5c08\u5bb6\u6392\u540d\u6a21\u578b\uff0c\u5305\u62ec\u5728\u5927\u91cf\u6392\u540d\u8cc7\u6599\u4e0a\u9032\u884c\u7368\u5bb6\u5fae\u8abf\u7684\u76f8\u540c LLM\u3002\u5c0d\u65bc\u751f\u6210\uff0c\u6211\u5011\u5c07\u6211\u5011\u7684\u6a21\u578b\u8207\u8a31\u591a\u5f37\u5927\u7684\u57fa\u7dda\u9032\u884c\u6bd4\u8f03\uff0c\u5305\u62ec GPT-4-0613\u3001GPT-4-turbo-2024-0409 \u548c ChatQA-1.5\uff0c\u9019\u662f\u4e00\u500b\u5728 RAG \u57fa\u6e96\u4e0a\u5177\u6709\u6700\u5148\u9032\u6548\u80fd\u7684\u958b\u6e90\u6a21\u578b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684 Llama3-RankRAG \u5728\u4e5d\u500b\u77e5\u8b58\u5bc6\u96c6\u578b\u57fa\u6e96\u4e0a\u986f\u8457\u512a\u65bc Llama3-ChatQA-1.5 \u548c GPT-4 \u6a21\u578b\u3002\u6b64\u5916\uff0c\u5b83\u5728\u751f\u7269\u91ab\u5b78\u9818\u57df\u7684\u4e94\u500b RAG \u57fa\u6e96\u4e0a\u4e5f\u8868\u73fe\u51fa\u8207 GPT-4 \u76f8\u7576\u7684\u6548\u80fd\uff0c\u800c\u7121\u9700\u5c0d\u751f\u7269\u91ab\u5b78\u8cc7\u6599\u9032\u884c\u6307\u4ee4\u5fae\u8abf\uff0c\u8b49\u660e\u4e86\u5b83\u5c0d\u65b0\u9818\u57df\u7684\u51fa\u8272\u6cdb\u5316\u80fd\u529b\u3002", "author": "Yue Yu et.al.", "authors": "Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, Bryan Catanzaro", "id": "2407.02485v1", "paper_url": "http://arxiv.org/abs/2407.02485v1", "repo": "null"}}