{"2502.06814": {"publish_time": "2025-02-04", "title": "Diffusion Instruction Tuning", "paper_summary": "We introduce Lavender, a simple supervised fine-tuning (SFT) method that\nboosts the performance of advanced vision-language models (VLMs) by leveraging\nstate-of-the-art image generation models such as Stable Diffusion.\nSpecifically, Lavender aligns the text-vision attention in the VLM transformer\nwith the equivalent used by Stable Diffusion during SFT, instead of adapting\nseparate encoders. This alignment enriches the model's visual understanding and\nsignificantly boosts performance across in- and out-of-distribution tasks.\nLavender requires just 0.13 million training examples, 2.5% of typical\nlarge-scale SFT datasets, and fine-tunes on standard hardware (8 GPUs) in a\nsingle day. It consistently improves state-of-the-art open-source multimodal\nLLMs (e.g., Llama-3.2-11B, MiniCPM-Llama3-v2.5), achieving up to 30% gains and\na 68% boost on challenging out-of-distribution medical QA tasks. By efficiently\ntransferring the visual expertise of image generators with minimal supervision,\nLavender offers a scalable solution for more accurate vision-language systems.\nAll code, training data, and models will be shared at\nhttps://astrazeneca.github.io/vlm/.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4ecb\u7d39 Lavender\uff0c\u4e00\u7a2e\u7c21\u55ae\u7684\u76e3\u7763\u5fae\u8abf (SFT) \u65b9\u6cd5\uff0c\u5b83\u900f\u904e\u5229\u7528 Stable Diffusion \u7b49\u6700\u5148\u9032\u7684\u5f71\u50cf\u751f\u6210\u6a21\u578b\u4f86\u63d0\u5347\u5148\u9032\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7684\u6548\u80fd\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0cLavender \u5728 SFT \u671f\u9593\u5c07 VLM \u8f49\u63db\u5668\u4e2d\u7684\u6587\u5b57\u8996\u89ba\u6ce8\u610f\u529b\u8207 Stable Diffusion \u4f7f\u7528\u7684\u7b49\u6548\u6ce8\u610f\u529b\u5c0d\u9f4a\uff0c\u800c\u4e0d\u662f\u8abf\u6574\u55ae\u7368\u7684\u7de8\u78bc\u5668\u3002\u6b64\u5c0d\u9f4a\u8c50\u5bcc\u4e86\u6a21\u578b\u7684\u8996\u89ba\u7406\u89e3\uff0c\u4e26\u986f\u8457\u63d0\u5347\u4e86\u5206\u4f48\u5167\u5916\u4efb\u52d9\u7684\u6548\u80fd\u3002\nLavender \u53ea\u9700\u8981 0.13 \u767e\u842c\u500b\u8a13\u7df4\u7bc4\u4f8b\uff0c\u76f8\u7576\u65bc\u5178\u578b\u5927\u578b SFT \u8cc7\u6599\u96c6\u7684 2.5%\uff0c\u4e26\u5728\u6a19\u6e96\u786c\u9ad4 (8 \u500b GPU) \u4e0a\u65bc\u4e00\u5929\u5167\u9032\u884c\u5fae\u8abf\u3002\u5b83\u6301\u7e8c\u6539\u5584\u6700\u5148\u9032\u7684\u958b\u653e\u539f\u59cb\u78bc\u591a\u6a21\u614b LLM\uff08\u4f8b\u5982 Llama-3.2-11B\u3001MiniCPM-Llama3-v2.5\uff09\uff0c\u5728\u5177\u6709\u6311\u6230\u6027\u7684\u5206\u4f48\u5916\u91ab\u7642 QA \u4efb\u52d9\u4e2d\u7372\u5f97\u9ad8\u9054 30% \u7684\u6536\u76ca\u548c 68% \u7684\u63d0\u5347\u3002\u900f\u904e\u6709\u6548\u8f49\u79fb\u5f71\u50cf\u751f\u6210\u5668\u7684\u8996\u89ba\u5c08\u696d\u77e5\u8b58\uff0c\u4e26\u50c5\u9700\u6700\u5c11\u7684\u76e3\u7763\uff0cLavender \u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4ee5\u5be6\u73fe\u66f4\u6e96\u78ba\u7684\u8996\u89ba\u8a9e\u8a00\u7cfb\u7d71\u3002\n\u6240\u6709\u7a0b\u5f0f\u78bc\u3001\u8a13\u7df4\u8cc7\u6599\u548c\u6a21\u578b\u5c07\u5728 https://astrazeneca.github.io/vlm/ \u5206\u4eab\u3002</paragraph>", "author": "Chen Jin et.al.", "authors": "Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, Philip Teare", "id": "2502.06814v1", "paper_url": "http://arxiv.org/abs/2502.06814v1", "repo": "null"}}