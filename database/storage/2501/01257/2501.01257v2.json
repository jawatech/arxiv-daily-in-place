{"2501.01257": {"publish_time": "2025-01-02", "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings", "paper_summary": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 25 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u73fe\u6709\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7a0b\u5f0f\u78bc\u63a8\u7406\u80fd\u529b\u4e0d\u65b7\u63d0\u5347\uff0c\u4ee5\u53ca OpenAI o1 \u548c o3 \u7b49\u63a8\u7406\u6a21\u578b\u7684\u7a81\u7834\uff0c\u8d8a\u4f86\u8d8a\u9700\u8981\u958b\u767c\u66f4\u5177\u6311\u6230\u6027\u548c\u5168\u9762\u6027\u7684\u57fa\u6e96\uff0c\u4ee5\u6709\u6548\u6e2c\u8a66\u5176\u8907\u96dc\u7684\u7af6\u8cfd\u7d1a\u7a0b\u5f0f\u7de8\u78bc\u80fd\u529b\u3002\u73fe\u6709\u7684\u57fa\u6e96\uff0c\u4f8b\u5982 LiveCodeBench \u548c USACO\uff0c\u7531\u65bc\u7f3a\u4e4f\u79c1\u4eba\u6e2c\u8a66\u6848\u4f8b\u3001\u7f3a\u4e4f\u5c0d\u7279\u6b8a\u8a55\u5be9\u7684\u652f\u6301\u4ee5\u53ca\u57f7\u884c\u74b0\u5883\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u7121\u6cd5\u9054\u5230\u8981\u6c42\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 CodeElo\uff0c\u9019\u662f\u4e00\u500b\u6a19\u6e96\u5316\u7684\u7af6\u8cfd\u7d1a\u7a0b\u5f0f\u78bc\u751f\u6210\u57fa\u6e96\uff0c\u9996\u6b21\u6709\u6548\u5730\u61c9\u5c0d\u4e86\u6240\u6709\u9019\u4e9b\u6311\u6230\u3002CodeElo \u57fa\u6e96\u4e3b\u8981\u57fa\u65bc\u5b98\u65b9\u7684 CodeForces \u5e73\u53f0\uff0c\u4e26\u76e1\u53ef\u80fd\u8207\u8a72\u5e73\u53f0\u4fdd\u6301\u4e00\u81f4\u3002\u6211\u5011\u7de8\u8b6f\u4e86 CodeForces \u8fd1\u516d\u500b\u6708\u7684\u7af6\u8cfd\u984c\u76ee\uff0c\u4e26\u9644\u6709\u8a73\u7d30\u8cc7\u8a0a\uff0c\u4f8b\u5982\u7af6\u8cfd\u7d44\u5225\u3001\u984c\u76ee\u96e3\u5ea6\u8a55\u7d1a\u548c\u984c\u76ee\u6f14\u7b97\u6cd5\u6a19\u7c64\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u7368\u7279\u7684\u8a55\u5be9\u65b9\u6cd5\uff0c\u5176\u4e2d\u984c\u76ee\u76f4\u63a5\u63d0\u4ea4\u5230\u5e73\u53f0\uff0c\u4e26\u958b\u767c\u4e86\u4e00\u500b\u53ef\u9760\u7684 Elo \u8a55\u5206\u8a08\u7b97\u7cfb\u7d71\uff0c\u8a72\u7cfb\u7d71\u8207\u5e73\u53f0\u4fdd\u6301\u4e00\u81f4\uff0c\u4e26\u4e14\u53ef\u4ee5\u8207\u4eba\u985e\u53c3\u8207\u8005\u76f8\u6bd4\u8f03\uff0c\u4f46\u8b8a\u7570\u6027\u8f03\u4f4e\u3002\u901a\u904e\u5728\u6211\u5011\u7684 CodeElo \u4e0a\u9032\u884c\u6e2c\u8a66\uff0c\u6211\u5011\u9996\u6b21\u63d0\u4f9b\u4e86 30 \u500b\u73fe\u6709\u6d41\u884c\u958b\u6e90\u548c 3 \u500b\u5c08\u6709 LLM \u7684 Elo \u8a55\u5206\u3002\u7d50\u679c\u8868\u660e\uff0co1-mini \u548c QwQ-32B-Preview \u986f\u8457\u812b\u7a4e\u800c\u51fa\uff0c\u5206\u5225\u7372\u5f97\u4e86 1578 \u548c 1261 \u7684 Elo \u8a55\u5206\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u751a\u81f3\u5728\u6700\u7c21\u55ae\u7684\u984c\u76ee\u4e0a\u4e5f\u96e3\u4ee5\u61c9\u5c0d\uff0c\u5728\u6240\u6709\u4eba\u985e\u53c3\u8207\u8005\u4e2d\u6392\u540d\u6700\u4f4e\u7684 25%\u3002\u9084\u9032\u884c\u4e86\u8a73\u7d30\u7684\u5206\u6790\u5be6\u9a57\uff0c\u4ee5\u6df1\u5165\u4e86\u89e3\u6f14\u7b97\u6cd5\u7684\u6548\u80fd\uff0c\u4ee5\u53ca\u4f7f\u7528 C++ \u548c Python \u4e4b\u9593\u7684\u6bd4\u8f03\uff0c\u9019\u53ef\u4ee5\u70ba\u672a\u4f86\u7684\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002</paragraph>", "author": "Shanghaoran Quan et.al.", "authors": "Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, Junyang Lin", "id": "2501.01257v2", "paper_url": "http://arxiv.org/abs/2501.01257v2", "repo": "null"}}