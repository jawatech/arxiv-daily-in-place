{"2408.04810": {"publish_time": "2024-08-09", "title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling", "paper_summary": "Significant research efforts have been made to scale and improve\nvision-language model (VLM) training approaches. Yet, with an ever-growing\nnumber of benchmarks, researchers are tasked with the heavy burden of\nimplementing each protocol, bearing a non-trivial computational cost, and\nmaking sense of how all these benchmarks translate into meaningful axes of\nprogress. To facilitate a systematic evaluation of VLM progress, we introduce\nUniBench: a unified implementation of 50+ VLM benchmarks spanning a\ncomprehensive range of carefully categorized capabilities from object\nrecognition to spatial awareness, counting, and much more. We showcase the\nutility of UniBench for measuring progress by evaluating nearly 60 publicly\navailable vision-language models, trained on scales of up to 12.8B samples. We\nfind that while scaling training data or model size can boost many\nvision-language model capabilities, scaling offers little benefit for reasoning\nor relations. Surprisingly, we also discover today's best VLMs struggle on\nsimple digit recognition and counting tasks, e.g. MNIST, which much simpler\nnetworks can solve. Where scale falls short, we find that more precise\ninterventions, such as data quality or tailored-learning objectives offer more\npromise. For practitioners, we also offer guidance on selecting a suitable VLM\nfor a given application. Finally, we release an easy-to-run UniBench code-base\nwith the full set of 50+ benchmarks and comparisons across 59 models as well as\na distilled, representative set of benchmarks that runs in 5 minutes on a\nsingle GPU.", "paper_summary_zh": "<paragraph>\u70ba\u4e86\u64f4\u5c55\u548c\u6539\u5584\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u8a13\u7df4\u65b9\u6cd5\uff0c\u5df2\u7d93\u9032\u884c\u4e86\u5927\u91cf\u7684\u7814\u7a76\u5de5\u4f5c\u3002\u7136\u800c\uff0c\u96a8\u8457\u57fa\u6e96\u6e2c\u8a66\u6578\u91cf\u4e0d\u65b7\u589e\u52a0\uff0c\u7814\u7a76\u4eba\u54e1\u9762\u81e8\u8457\u57f7\u884c\u6bcf\u500b\u5354\u5b9a\u7684\u6c89\u91cd\u8ca0\u64d4\uff0c\u627f\u53d7\u8457\u975e\u540c\u5c0f\u53ef\u7684\u8a08\u7b97\u6210\u672c\uff0c\u4e26\u4e14\u5fc5\u9808\u7406\u89e3\u6240\u6709\u9019\u4e9b\u57fa\u6e96\u6e2c\u8a66\u5982\u4f55\u8f49\u5316\u70ba\u6709\u610f\u7fa9\u7684\u9032\u5c55\u8ef8\u3002\u70ba\u4e86\u4fc3\u9032\u5c0d VLM \u9032\u5c55\u9032\u884c\u7cfb\u7d71\u6027\u8a55\u4f30\uff0c\u6211\u5011\u5f15\u5165\u4e86 UniBench\uff1a\u4e00\u500b\u7d71\u4e00\u5be6\u4f5c\uff0c\u6db5\u84cb\u5f9e\u7269\u4ef6\u8fa8\u8b58\u5230\u7a7a\u9593\u611f\u77e5\u3001\u8a08\u6578\u7b49\u7b49\uff0c\u8d85\u904e 50 \u500b\u7d93\u904e\u4ed4\u7d30\u5206\u985e\u529f\u80fd\u7684 VLM \u57fa\u6e96\u6e2c\u8a66\u3002\u6211\u5011\u5c55\u793a\u4e86 UniBench \u5728\u8a55\u4f30\u9032\u5ea6\u65b9\u9762\u7684\u6548\u7528\uff0c\u65b9\u6cd5\u662f\u8a55\u4f30\u8fd1 60 \u500b\u516c\u958b\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u91dd\u5c0d\u9ad8\u9054 12.8B \u500b\u6a23\u672c\u9032\u884c\u8a13\u7df4\u3002\u6211\u5011\u767c\u73fe\uff0c\u96d6\u7136\u64f4\u5c55\u8a13\u7df4\u8cc7\u6599\u6216\u6a21\u578b\u5927\u5c0f\u53ef\u4ee5\u63d0\u5347\u8a31\u591a\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u529f\u80fd\uff0c\u4f46\u64f4\u5c55\u5c0d\u65bc\u63a8\u7406\u6216\u95dc\u4fc2\u5e7e\u4e4e\u6c92\u6709\u597d\u8655\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u9084\u767c\u73fe\u7576\u4eca\u6700\u4f73\u7684 VLM \u5728\u7c21\u55ae\u7684\u6578\u5b57\u8fa8\u8b58\u548c\u8a08\u6578\u4efb\u52d9\uff08\u4f8b\u5982 MNIST\uff09\u4e0a\u8868\u73fe\u4e0d\u4f73\uff0c\u800c\u66f4\u7c21\u55ae\u7684\u7db2\u8def\u53ef\u4ee5\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002\u5728\u898f\u6a21\u4e0d\u8db3\u7684\u60c5\u6cc1\u4e0b\uff0c\u6211\u5011\u767c\u73fe\u66f4\u7cbe\u78ba\u7684\u5e72\u9810\u63aa\u65bd\uff0c\u4f8b\u5982\u8cc7\u6599\u54c1\u8cea\u6216\u91cf\u8eab\u6253\u9020\u7684\u5b78\u7fd2\u76ee\u6a19\uff0c\u63d0\u4f9b\u4e86\u66f4\u591a\u5e0c\u671b\u3002\u5c0d\u65bc\u5be6\u52d9\u5de5\u4f5c\u8005\uff0c\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u6709\u95dc\u70ba\u7279\u5b9a\u61c9\u7528\u7a0b\u5f0f\u9078\u64c7\u9069\u7576 VLM \u7684\u6307\u5357\u3002\u6700\u5f8c\uff0c\u6211\u5011\u767c\u5e03\u4e86\u4e00\u500b\u6613\u65bc\u57f7\u884c UniBench \u7a0b\u5f0f\u78bc\u5eab\uff0c\u5176\u4e2d\u5305\u542b 50 \u591a\u500b\u57fa\u6e96\u6e2c\u8a66\u548c 59 \u500b\u6a21\u578b\u7684\u5b8c\u6574\u6bd4\u8f03\uff0c\u4ee5\u53ca\u4e00\u500b\u7cbe\u7c21\u7684\u3001\u5177\u4ee3\u8868\u6027\u7684\u57fa\u6e96\u6e2c\u8a66\u96c6\uff0c\u53ef\u4ee5\u5728\u55ae\u4e00 GPU \u4e0a\u57f7\u884c 5 \u5206\u9418\u3002</paragraph>", "author": "Haider Al-Tahan et.al.", "authors": "Haider Al-Tahan, Quentin Garrido, Randall Balestriero, Diane Bouchacourt, Caner Hazirbas, Mark Ibrahim", "id": "2408.04810v1", "paper_url": "http://arxiv.org/abs/2408.04810v1", "repo": "https://github.com/facebookresearch/unibench"}}