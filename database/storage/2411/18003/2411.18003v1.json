{"2411.18003": {"publish_time": "2024-11-27", "title": "HAAT: Hybrid Attention Aggregation Transformer for Image Super-Resolution", "paper_summary": "In the research area of image super-resolution, Swin-transformer-based models\nare favored for their global spatial modeling and shifting window attention\nmechanism. However, existing methods often limit self-attention to non\noverlapping windows to cut costs and ignore the useful information that exists\nacross channels. To address this issue, this paper introduces a novel model,\nthe Hybrid Attention Aggregation Transformer (HAAT), designed to better\nleverage feature information. HAAT is constructed by integrating\nSwin-Dense-Residual-Connected Blocks (SDRCB) with Hybrid Grid Attention Blocks\n(HGAB). SDRCB expands the receptive field while maintaining a streamlined\narchitecture, resulting in enhanced performance. HGAB incorporates channel\nattention, sparse attention, and window attention to improve nonlocal feature\nfusion and achieve more visually compelling results. Experimental evaluations\ndemonstrate that HAAT surpasses state-of-the-art methods on benchmark datasets.\n  Keywords: Image super-resolution, Computer vision, Attention mechanism,\nTransformer", "paper_summary_zh": "\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u7814\u7a76\u9886\u57df\uff0c\u57fa\u4e8e Swin-transformer \u7684\u6a21\u578b\u56e0\u5176\u5168\u5c40\u7a7a\u95f4\u5efa\u6a21\u548c\u79fb\u7a97\u6ce8\u610f\u529b\u673a\u5236\u800c\u53d7\u5230\u9752\u7750\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u81ea\u6ce8\u610f\u529b\u9650\u5236\u5728\u975e\u91cd\u53e0\u7a97\u53e3\u4ee5\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u5ffd\u7565\u8de8\u901a\u9053\u5b58\u5728\u7684\u6709\u7528\u4fe1\u606f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u5373\u6df7\u5408\u6ce8\u610f\u529b\u805a\u5408\u53d8\u6362\u5668 (HAAT)\uff0c\u65e8\u5728\u66f4\u597d\u5730\u5229\u7528\u7279\u5f81\u4fe1\u606f\u3002HAAT \u662f\u901a\u8fc7\u5c06 Swin-Dense-Residual-Connected Blocks (SDRCB) \u4e0e\u6df7\u5408\u7f51\u683c\u6ce8\u610f\u529b\u5757 (HGAB) \u96c6\u6210\u6784\u5efa\u7684\u3002SDRCB \u5728\u4fdd\u6301\u6d41\u7ebf\u578b\u67b6\u6784\u7684\u540c\u65f6\u6269\u5c55\u611f\u53d7\u91ce\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u3002HGAB \u7ed3\u5408\u4e86\u901a\u9053\u6ce8\u610f\u529b\u3001\u7a00\u758f\u6ce8\u610f\u529b\u548c\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u4ee5\u6539\u8fdb\u975e\u5c40\u90e8\u7279\u5f81\u878d\u5408\u5e76\u5b9e\u73b0\u66f4\u5177\u89c6\u89c9\u5438\u5f15\u529b\u7684\u7ed3\u679c\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cHAAT \u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\n\u5173\u952e\u5b57\uff1a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001Transformer", "author": "Song-Jiang Lai et.al.", "authors": "Song-Jiang Lai, Tsun-Hin Cheung, Ka-Chun Fung, Kai-wen Xue, Kin-Man Lama", "id": "2411.18003v1", "paper_url": "http://arxiv.org/abs/2411.18003v1", "repo": "null"}}