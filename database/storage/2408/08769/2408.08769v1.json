{"2408.08769": {"publish_time": "2024-08-16", "title": "Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused", "paper_summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\nvarious natural language processing tasks, yet they occasionally tend to yield\ncontent that factually inaccurate or discordant with the expected output, a\nphenomenon empirically referred to as \"hallucination\". To tackle this issue,\nrecent works have investigated contrastive decoding between the original model\nand an amateur model with induced hallucination, which has shown promising\nresults. Nonetheless, this method may undermine the output distribution of the\noriginal LLM caused by its coarse contrast and simplistic subtraction\noperation, potentially leading to errors in certain cases. In this paper, we\nintroduce a novel contrastive decoding framework termed LOL (LOwer Layer\nMatters). Our approach involves concatenating the contrastive decoding of both\nthe final and lower layers between the original model and the amateur model,\nthereby achieving multi-layer fusion to aid in the mitigation of hallucination.\nAdditionally, we incorporate a truthfulness refocused module that leverages\ncontextual guidance to enhance factual encoding, further capturing truthfulness\nduring contrastive decoding. Extensive experiments conducted on two publicly\navailable datasets illustrate that our proposed LOL framework can substantially\nalleviate hallucination while surpassing existing baselines in most cases.\nCompared with the best baseline, we improve by average 4.5 points on all\nmetrics of TruthfulQA. The source code is coming soon.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u6548\u80fd\uff0c\u4f46\u5076\u723e\u6703\u7522\u751f\u4e8b\u5be6\u4e0d\u6b63\u78ba\u6216\u8207\u9810\u671f\u8f38\u51fa\u4e0d\u4e00\u81f4\u7684\u5167\u5bb9\uff0c\u9019\u7a2e\u73fe\u8c61\u5728\u7d93\u9a57\u4e0a\u7a31\u70ba\u300c\u5e7b\u89ba\u300d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u8a0e\u4e86\u539f\u59cb\u6a21\u578b\u8207\u8a98\u767c\u5e7b\u89ba\u7684\u696d\u9918\u6a21\u578b\u4e4b\u9593\u7684\u5c0d\u6bd4\u89e3\u78bc\uff0c\u4e26\u986f\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7d50\u679c\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u9019\u7a2e\u65b9\u6cd5\u53ef\u80fd\u6703\u7834\u58de\u539f\u59cb LLM \u7684\u8f38\u51fa\u5206\u4f48\uff0c\u9019\u662f\u56e0\u70ba\u5176\u5c0d\u6bd4\u7c97\u7cd9\u4e14\u6e1b\u6cd5\u904b\u7b97\u904e\u65bc\u7c21\u5316\uff0c\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\u53ef\u80fd\u6703\u5c0e\u81f4\u932f\u8aa4\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u5c0d\u6bd4\u89e3\u78bc\u67b6\u69cb\uff0c\u7a31\u70ba LOL\uff08\u8f03\u4f4e\u5c64\u7d1a\u5f88\u91cd\u8981\uff09\u3002\u6211\u5011\u7684\u505a\u6cd5\u6d89\u53ca\u4e32\u63a5\u539f\u59cb\u6a21\u578b\u548c\u696d\u9918\u6a21\u578b\u4e4b\u9593\u7684\u6700\u7d42\u5c64\u548c\u8f03\u4f4e\u5c64\u7684\u5c0d\u6bd4\u89e3\u78bc\uff0c\u5f9e\u800c\u5be6\u73fe\u591a\u5c64\u878d\u5408\u4ee5\u5e6b\u52a9\u6e1b\u8f15\u5e7b\u89ba\u3002\u6b64\u5916\uff0c\u6211\u5011\u6574\u5408\u4e86\u4e00\u500b\u771f\u5be6\u6027\u91cd\u65b0\u805a\u7126\u6a21\u7d44\uff0c\u8a72\u6a21\u7d44\u5229\u7528\u4e0a\u4e0b\u6587\u6307\u5c0e\u4f86\u589e\u5f37\u4e8b\u5be6\u7de8\u78bc\uff0c\u9032\u4e00\u6b65\u5728\u5c0d\u6bd4\u89e3\u78bc\u904e\u7a0b\u4e2d\u6355\u6349\u771f\u5be6\u6027\u3002\u5728\u5169\u500b\u516c\u958b\u53ef\u7528\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684 LOL \u67b6\u69cb\u53ef\u4ee5\u5728\u5927\u591a\u6578\u60c5\u6cc1\u4e0b\u5927\u5e45\u6e1b\u8f15\u5e7b\u89ba\uff0c\u540c\u6642\u8d85\u8d8a\u73fe\u6709\u7684\u57fa\u6e96\u3002\u8207\u6700\u4f73\u57fa\u6e96\u76f8\u6bd4\uff0c\u6211\u5011\u5728 TruthfulQA \u7684\u6240\u6709\u6307\u6a19\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e86 4.5 \u5206\u3002\u539f\u59cb\u78bc\u5373\u5c07\u63a8\u51fa\u3002", "author": "Dingwei Chen et.al.", "authors": "Dingwei Chen, Feiteng Fang, Shiwen Ni, Feng Liang, Ruifeng Xu, Min Yang, Chengming Li", "id": "2408.08769v1", "paper_url": "http://arxiv.org/abs/2408.08769v1", "repo": "null"}}