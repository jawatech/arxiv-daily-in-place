{"2405.05493": {"publish_time": "2024-05-09", "title": "Parameter-Efficient Fine-Tuning With Adapters", "paper_summary": "In the arena of language model fine-tuning, the traditional approaches, such\nas Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT),\nalthough effective, but computational intensive. This research introduces a\nnovel adaptation method utilizing the UniPELT framework as a base and added a\nPromptTuning Layer, which significantly reduces the number of trainable\nparameters while maintaining competitive performance across various benchmarks.\nOur method employs adapters, which enable efficient transfer of pretrained\nmodels to new tasks with minimal retraining of the base model parameters. We\nevaluate our approach using three diverse datasets: the GLUE benchmark, a\ndomain-specific dataset comprising four distinct areas, and the Stanford\nQuestion Answering Dataset 1.1 (SQuAD). Our results demonstrate that our\ncustomized adapter-based method achieves performance comparable to full model\nfine-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or\nequivalent amount of parameters. This parameter efficiency not only alleviates\nthe computational burden but also expedites the adaptation process. The study\nunderlines the potential of adapters in achieving high performance with\nsignificantly reduced resource consumption, suggesting a promising direction\nfor future research in parameter-efficient fine-tuning.", "paper_summary_zh": "\u5728\u8a9e\u8a00\u6a21\u578b\u5fae\u8abf\u9818\u57df\u4e2d\uff0c\u50b3\u7d71\u65b9\u6cd5\uff0c\u4f8b\u5982\u9818\u57df\u81ea\u9069\u61c9\u9810\u8a13\u7df4 (DAPT) \u548c\u4efb\u52d9\u81ea\u9069\u61c9\u9810\u8a13\u7df4 (TAPT)\uff0c\u96d6\u7136\u6709\u6548\uff0c\u4f46\u8a08\u7b97\u5bc6\u96c6\u3002\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u81ea\u9069\u61c9\u65b9\u6cd5\uff0c\u4ee5 UniPELT \u6846\u67b6\u70ba\u57fa\u790e\uff0c\u4e26\u6dfb\u52a0\u4e86 PromptTuning \u5c64\uff0c\u5728\u4fdd\u6301\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u7af6\u722d\u529b\u7684\u540c\u6642\uff0c\u986f\u8457\u6e1b\u5c11\u4e86\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u6578\u91cf\u3002\u6211\u5011\u7684\u6a21\u578b\u63a1\u7528\u9069\u914d\u5668\uff0c\u5b83\u80fd\u4ee5\u6700\u5c11\u7684\u57fa\u790e\u6a21\u578b\u53c3\u6578\u91cd\u65b0\u8a13\u7df4\uff0c\u5c07\u9810\u8a13\u7df4\u6a21\u578b\u6709\u6548\u5730\u8f49\u79fb\u5230\u65b0\u4efb\u52d9\u3002\u6211\u5011\u4f7f\u7528\u4e09\u500b\u4e0d\u540c\u7684\u6578\u64da\u96c6\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\uff1aGLUE \u57fa\u6e96\u6e2c\u8a66\u3001\u5305\u542b\u56db\u500b\u4e0d\u540c\u9818\u57df\u7684\u7279\u5b9a\u9818\u57df\u6578\u64da\u96c6\uff0c\u4ee5\u53ca\u53f2\u4e39\u4f5b\u554f\u7b54\u6578\u64da\u96c6 1.1 (SQuAD)\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u81ea\u8a02\u7684\u57fa\u65bc\u9069\u914d\u5668\u7684\u6a21\u578b\u5728\u57f7\u884c\u6548\u80fd\u4e0a\u53ef\u8207\u5b8c\u6574\u7684\u6a21\u578b\u5fae\u8abf\u3001DAPT+TAPT \u548c UniPELT \u7b56\u7565\u76f8\u5ab2\u7f8e\uff0c\u540c\u6642\u6240\u9700\u53c3\u6578\u66f4\u5c11\u6216\u76f8\u7b49\u3002\u9019\u7a2e\u53c3\u6578\u6548\u7387\u4e0d\u50c5\u6e1b\u8f15\u4e86\u8a08\u7b97\u8ca0\u64d4\uff0c\u4e5f\u52a0\u5feb\u4e86\u81ea\u9069\u61c9\u904e\u7a0b\u3002\u672c\u7814\u7a76\u5f37\u8abf\u4e86\u9069\u914d\u5668\u5728\u986f\u8457\u6e1b\u5c11\u8cc7\u6e90\u6d88\u8017\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u9ad8\u57f7\u884c\u6548\u80fd\u7684\u6f5b\u529b\uff0c\u70ba\u672a\u4f86\u5728\u53c3\u6578\u6709\u6548\u5fae\u8abf\u7684\u7814\u7a76\u4e2d\u63d0\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "author": "Keyu Chen et.al.", "authors": "Keyu Chen, Yuan Pang, Zi Yang", "id": "2405.05493v1", "paper_url": "http://arxiv.org/abs/2405.05493v1", "repo": "null"}}