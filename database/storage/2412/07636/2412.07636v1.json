{"2412.07636": {"publish_time": "2024-12-10", "title": "TrojanWhisper: Evaluating Pre-trained LLMs to Detect and Localize Hardware Trojans", "paper_summary": "Existing Hardware Trojans (HT) detection methods face several critical\nlimitations: logic testing struggles with scalability and coverage for large\ndesigns, side-channel analysis requires golden reference chips, and formal\nverification methods suffer from state-space explosion. The emergence of Large\nLanguage Models (LLMs) offers a promising new direction for HT detection by\nleveraging their natural language understanding and reasoning capabilities. For\nthe first time, this paper explores the potential of general-purpose LLMs in\ndetecting various HTs inserted in Register Transfer Level (RTL) designs,\nincluding SRAM, AES, and UART modules. We propose a novel tool for this goal\nthat systematically assesses state-of-the-art LLMs (GPT-4o, Gemini 1.5 pro, and\nLlama 3.1) in detecting HTs without prior fine-tuning. To address potential\ntraining data bias, the tool implements perturbation techniques, i.e., variable\nname obfuscation, and design restructuring, that make the cases more\nsophisticated for the used LLMs. Our experimental evaluation demonstrates\nperfect detection rates by GPT-4o and Gemini 1.5 pro in baseline scenarios\n(100%/100% precision/recall), with both models achieving better trigger line\ncoverage (TLC: 0.82-0.98) than payload line coverage (PLC: 0.32-0.46). Under\ncode perturbation, while Gemini 1.5 pro maintains perfect detection performance\n(100%/100%), GPT-4o (100%/85.7%) and Llama 3.1 (66.7%/85.7%) show some\ndegradation in detection rates, and all models experience decreased accuracy in\nlocalizing both triggers and payloads. This paper validates the potential of\nLLM approaches for hardware security applications, highlighting areas for\nfuture improvement.", "paper_summary_zh": "\u73fe\u6709\u7684\u786c\u9ad4\u6728\u99ac (HT) \u5075\u6e2c\u65b9\u6cd5\u9762\u81e8\u5e7e\u500b\u95dc\u9375\u7684\u9650\u5236\uff1a\u908f\u8f2f\u6e2c\u8a66\u5728\u53ef\u64f4\u5145\u6027\u548c\u5c0d\u5927\u578b\u8a2d\u8a08\u7684\u6db5\u84cb\u7bc4\u570d\u4e0a\u9047\u5230\u56f0\u96e3\uff0c\u5074\u901a\u9053\u5206\u6790\u9700\u8981\u9ec3\u91d1\u53c3\u8003\u6676\u7247\uff0c\u800c\u5f62\u5f0f\u9a57\u8b49\u65b9\u6cd5\u5247\u6703\u53d7\u5230\u72c0\u614b\u7a7a\u9593\u7206\u70b8\u7684\u5f71\u97ff\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\u70ba HT \u5075\u6e2c\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u666f\u7684\u65b0\u65b9\u5411\uff0c\u65b9\u6cd5\u662f\u5229\u7528\u5b83\u5011\u7684\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002\u9019\u7bc7\u8ad6\u6587\u9996\u6b21\u63a2\u8a0e\u4e86\u901a\u7528 LLM \u5728\u5075\u6e2c\u63d2\u5165\u66ab\u5b58\u5668\u50b3\u8f38\u5c64\u7d1a (RTL) \u8a2d\u8a08\u4e2d\u7684\u5404\u7a2e HT\uff08\u5305\u62ec SRAM\u3001AES \u548c UART \u6a21\u7d44\uff09\u7684\u6f5b\u529b\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u5de5\u5177\u4f86\u9054\u6210\u6b64\u76ee\u6a19\uff0c\u9019\u500b\u5de5\u5177\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u4e86\u6700\u5148\u9032\u7684 LLM\uff08GPT-4o\u3001Gemini 1.5 pro \u548c Llama 3.1\uff09\u5728\u672a\u7d93\u4e8b\u5148\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\u5075\u6e2c HT \u7684\u80fd\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u6f5b\u5728\u7684\u8a13\u7df4\u8cc7\u6599\u504f\u5dee\uff0c\u8a72\u5de5\u5177\u5be6\u4f5c\u4e86\u64fe\u52d5\u6280\u8853\uff0c\u4f8b\u5982\u8b8a\u6578\u540d\u7a31\u6df7\u6dc6\u548c\u8a2d\u8a08\u91cd\u7d44\uff0c\u8b93\u6848\u4f8b\u5c0d\u6240\u4f7f\u7528\u7684 LLM \u4f86\u8aaa\u66f4\u8907\u96dc\u3002\u6211\u5011\u7684\u5be6\u9a57\u8a55\u4f30\u986f\u793a\uff0c\u5728\u57fa\u6e96\u60c5\u5883\u4e2d\uff0cGPT-4o \u548c Gemini 1.5 pro \u7684\u5075\u6e2c\u7387\u5b8c\u7f8e\uff08100%/100% \u7cbe\u78ba\u5ea6/\u53ec\u56de\u7387\uff09\uff0c\u9019\u5169\u500b\u6a21\u578b\u9054\u5230\u7684\u89f8\u767c\u7dda\u6db5\u84cb\u7387 (TLC\uff1a0.82-0.98) \u90fd\u512a\u65bc\u916c\u8f09\u7dda\u6db5\u84cb\u7387 (PLC\uff1a0.32-0.46)\u3002\u5728\u7a0b\u5f0f\u78bc\u64fe\u52d5\u4e0b\uff0c\u96d6\u7136 Gemini 1.5 pro \u7dad\u6301\u4e86\u5b8c\u7f8e\u7684\u5075\u6e2c\u6548\u80fd (100%/100%)\uff0c\u4f46 GPT-4o (100%/85.7%) \u548c Llama 3.1 (66.7%/85.7%) \u7684\u5075\u6e2c\u7387\u51fa\u73fe\u4e86\u4e00\u4e9b\u4e0b\u964d\uff0c\u800c\u4e14\u6240\u6709\u6a21\u578b\u5728\u5b9a\u4f4d\u89f8\u767c\u5668\u548c\u916c\u8f09\u7684\u6e96\u78ba\u5ea6\u90fd\u4e0b\u964d\u4e86\u3002\u9019\u7bc7\u8ad6\u6587\u9a57\u8b49\u4e86 LLM \u65b9\u6cd5\u5728\u786c\u9ad4\u5b89\u5168\u61c9\u7528\u4e2d\u7684\u6f5b\u529b\uff0c\u4e26\u7a81\u51fa\u4e86\u672a\u4f86\u6539\u9032\u7684\u9818\u57df\u3002", "author": "Md Omar Faruque et.al.", "authors": "Md Omar Faruque, Peter Jamieson, Ahmad Patooghy, Abdel-Hameed A. Badawy", "id": "2412.07636v1", "paper_url": "http://arxiv.org/abs/2412.07636v1", "repo": "null"}}