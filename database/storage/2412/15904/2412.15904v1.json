{"2412.15904": {"publish_time": "2024-12-20", "title": "What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning", "paper_summary": "Step-level reward models (SRMs) can significantly enhance mathematical\nreasoning performance through process supervision or step-level preference\nalignment based on reinforcement learning. The performance of SRMs is pivotal,\nas they serve as critical guidelines, ensuring that each step in the reasoning\nprocess is aligned with desired outcomes. Recently, AlphaZero-like methods,\nwhere Monte Carlo Tree Search (MCTS) is employed for automatic step-level\npreference annotation, have proven particularly effective. However, the precise\nmechanisms behind the success of SRMs remain largely unexplored. To address\nthis gap, this study delves into the counterintuitive aspects of SRMs,\nparticularly focusing on MCTS-based approaches. Our findings reveal that the\nremoval of natural language descriptions of thought processes has minimal\nimpact on the efficacy of SRMs. Furthermore, we demonstrate that SRMs are adept\nat assessing the complex logical coherence present in mathematical language\nwhile having difficulty in natural language. These insights provide a nuanced\nunderstanding of the core elements that drive effective step-level reward\nmodeling in mathematical reasoning. By shedding light on these mechanisms, this\nstudy offers valuable guidance for developing more efficient and streamlined\nSRMs, which can be achieved by focusing on the crucial parts of mathematical\nreasoning.", "paper_summary_zh": "\u5206\u6b65\u734e\u52f5\u6a21\u578b (SRM) \u53ef\u900f\u904e\u904e\u7a0b\u76e3\u7763\u6216\u57fa\u65bc\u5f37\u5316\u5b78\u7fd2\u7684\u5206\u6b65\u504f\u597d\u8abf\u6574\uff0c\u986f\u8457\u63d0\u5347\u6578\u5b78\u63a8\u7406\u6548\u80fd\u3002SRM \u7684\u6548\u80fd\u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u5b83\u5011\u4f5c\u70ba\u95dc\u9375\u6307\u5357\uff0c\u78ba\u4fdd\u63a8\u7406\u904e\u7a0b\u4e2d\u6bcf\u4e00\u6b65\u90fd\u8207\u9810\u671f\u7d50\u679c\u4e00\u81f4\u3002\u8fd1\u4f86\uff0c\u63a1\u7528\u8499\u5730\u5361\u7f85\u6a39\u72c0\u641c\u5c0b (MCTS) \u9032\u884c\u81ea\u52d5\u5206\u6b65\u504f\u597d\u8a3b\u89e3\u7684 AlphaZero \u985e\u4f3c\u65b9\u6cd5\u5df2\u8b49\u660e\u7279\u5225\u6709\u6548\u3002\u7136\u800c\uff0cSRM \u6210\u529f\u80cc\u5f8c\u78ba\u5207\u7684\u6a5f\u5236\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u63a2\u8a0e\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u6df1\u5165\u63a2\u8a0e SRM \u7684\u53cd\u76f4\u89ba\u9762\u5411\uff0c\u7279\u5225\u95dc\u6ce8\u57fa\u65bc MCTS \u7684\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u79fb\u9664\u81ea\u7136\u8a9e\u8a00\u5c0d\u601d\u8003\u904e\u7a0b\u7684\u63cf\u8ff0\uff0c\u5c0d SRM \u7684\u6548\u80fd\u5f71\u97ff\u751a\u5fae\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e SRM \u80fd\u5920\u8a55\u4f30\u6578\u5b78\u8a9e\u8a00\u4e2d\u5b58\u5728\u7684\u8907\u96dc\u908f\u8f2f\u4e00\u81f4\u6027\uff0c\u4f46\u8655\u7406\u81ea\u7136\u8a9e\u8a00\u6642\u5247\u6709\u56f0\u96e3\u3002\u9019\u4e9b\u898b\u89e3\u63d0\u4f9b\u4e86\u5c0d\u9a45\u52d5\u6578\u5b78\u63a8\u7406\u4e2d\u6709\u6548\u5206\u6b65\u734e\u52f5\u5efa\u6a21\u7684\u6838\u5fc3\u5143\u7d20\u7684\u7d30\u7dfb\u7406\u89e3\u3002\u900f\u904e\u95e1\u660e\u9019\u4e9b\u6a5f\u5236\uff0c\u672c\u7814\u7a76\u70ba\u958b\u767c\u66f4\u6709\u6548\u7387\u4e14\u7c21\u5316\u7684 SRM \u63d0\u4f9b\u6709\u50f9\u503c\u7684\u6307\u5c0e\uff0c\u800c\u9019\u53ef\u900f\u904e\u5c08\u6ce8\u65bc\u6578\u5b78\u63a8\u7406\u7684\u95dc\u9375\u90e8\u5206\u4f86\u9054\u6210\u3002", "author": "Yiran Ma et.al.", "authors": "Yiran Ma, Zui Chen, Tianqiao Liu, Mi Tian, Zhuo Liu, Zitao Liu, Weiqi Luo", "id": "2412.15904v1", "paper_url": "http://arxiv.org/abs/2412.15904v1", "repo": "null"}}