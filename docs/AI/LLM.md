
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-31**|**Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey**|Atsuyuki Miyai et.al.|[2407.21794v1](http://arxiv.org/abs/2407.21794v1)|null|
|**2024-07-31**|**Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?**|Richard Ren et.al.|[2407.21792v1](http://arxiv.org/abs/2407.21792v1)|null|
|**2024-07-31**|**Vision-Language Model Based Handwriting Verification**|Mihir Chauhan et.al.|[2407.21788v1](http://arxiv.org/abs/2407.21788v1)|null|
|**2024-07-31**|**Large Language Monkeys: Scaling Inference Compute with Repeated Sampling**|Bradley Brown et.al.|[2407.21787v1](http://arxiv.org/abs/2407.21787v1)|null|
|**2024-07-31**|**The Llama 3 Herd of Models**|Abhimanyu Dubey et.al.|[2407.21783v1](http://arxiv.org/abs/2407.21783v1)|null|
|**2024-07-31**|**Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries**|Felix Ocker et.al.|[2407.21778v1](http://arxiv.org/abs/2407.21778v1)|null|
|**2024-07-31**|**ShieldGemma: Generative AI Content Moderation Based on Gemma**|Wenjun Zeng et.al.|[2407.21772v1](http://arxiv.org/abs/2407.21772v1)|null|
|**2024-07-31**|**MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts**|Xi Victoria Lin et.al.|[2407.21770v1](http://arxiv.org/abs/2407.21770v1)|null|
|**2024-07-31**|**HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection**|Junwei He et.al.|[2407.21742v1](http://arxiv.org/abs/2407.21742v1)|null|
|**2024-07-31**|**Contrastive Factor Analysis**|Zhibin Duan et.al.|[2407.21740v2](http://arxiv.org/abs/2407.21740v2)|null|
|**2024-07-31**|**A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation**|Mothilal Asokan et.al.|[2407.21739v1](http://arxiv.org/abs/2407.21739v1)|null|
|**2024-07-31**|**Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos**|Joseph Geo Benjamin et.al.|[2407.21738v1](http://arxiv.org/abs/2407.21738v1)|null|
|**2024-07-31**|**Open-Vocabulary Audio-Visual Semantic Segmentation**|Ruohao Guo et.al.|[2407.21721v1](http://arxiv.org/abs/2407.21721v1)|null|
|**2024-07-31**|**Social Learning through Interactions with Other Agents: A Survey**|Dylan hillier et.al.|[2407.21713v1](http://arxiv.org/abs/2407.21713v1)|null|
|**2024-07-31**|**Adaptive Retrieval-Augmented Generation for Conversational Systems**|Xi Wang et.al.|[2407.21712v1](http://arxiv.org/abs/2407.21712v1)|null|
|**2024-07-31**|**CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**|Stefan Langer et.al.|[2407.21708v1](http://arxiv.org/abs/2407.21708v1)|null|
|**2024-07-31**|**TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities**|Ming Zhang et.al.|[2407.21693v1](http://arxiv.org/abs/2407.21693v1)|null|
|**2024-07-31**|**Dynamic Object Queries for Transformer-based Incremental Object Detection**|Jichuan Zhang et.al.|[2407.21687v1](http://arxiv.org/abs/2407.21687v1)|null|
|**2024-07-31**|**Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation**|Krishan Agyakari Raja Babu et.al.|[2407.21674v1](http://arxiv.org/abs/2407.21674v1)|null|
|**2024-07-31**|**Universal Approximation Theory: Foundations for Parallelism in Neural Networks**|Wei Wang et.al.|[2407.21670v1](http://arxiv.org/abs/2407.21670v1)|null|
|**2024-07-31**|**Synth-Empathy: Towards High-Quality Synthetic Empathy Data**|Hao Liang et.al.|[2407.21669v1](http://arxiv.org/abs/2407.21669v1)|null|
|**2024-07-31**|**An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification**|Aswini Kumar Patra et.al.|[2407.21666v1](http://arxiv.org/abs/2407.21666v1)|null|
|**2024-07-31**|**Defending Jailbreak Attack in VLMs via Cross-modality Information Detector**|Yue Xu et.al.|[2407.21659v2](http://arxiv.org/abs/2407.21659v2)|null|
|**2024-07-31**|**Spatial Transformer Network YOLO Model for Agricultural Object Detection**|Yash Zambre et.al.|[2407.21652v1](http://arxiv.org/abs/2407.21652v1)|null|
|**2024-07-31**|**Human interaction classifier for LLM based chatbot**|Diego Mart√≠n et.al.|[2407.21647v1](http://arxiv.org/abs/2407.21647v1)|null|
|**2024-07-31**|**Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent**|Shanbo Cheng et.al.|[2407.21646v1](http://arxiv.org/abs/2407.21646v1)|null|
|**2024-07-31**|**Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components**|Hermione Warr et.al.|[2407.21638v1](http://arxiv.org/abs/2407.21638v1)|null|
|**2024-07-31**|**Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation**|Xiang Luo et.al.|[2407.21633v1](http://arxiv.org/abs/2407.21633v1)|null|
|**2024-07-31**|**TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods**|Gabriel Loiseau et.al.|[2407.21630v1](http://arxiv.org/abs/2407.21630v1)|null|
|**2024-07-31**|**Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music**|Pedro Sarmento et.al.|[2407.21615v1](http://arxiv.org/abs/2407.21615v1)|null|
|**2024-07-31**|**Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism**|Jiafeng Zhong et.al.|[2407.21611v1](http://arxiv.org/abs/2407.21611v1)|null|
|**2024-07-31**|**Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors**|Shoujin Huang et.al.|[2407.21600v1](http://arxiv.org/abs/2407.21600v1)|null|
|**2024-07-31**|**Voxel Scene Graph for Intracranial Hemorrhage**|Antoine P. Sanner et.al.|[2407.21580v1](http://arxiv.org/abs/2407.21580v1)|null|
|**2024-07-31**|**A Performance Study of LLM-Generated Code on Leetcode**|Tristan Coignion et.al.|[2407.21579v1](http://arxiv.org/abs/2407.21579v1)|null|
|**2024-07-31**|**Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography**|Kit M. Bransby et.al.|[2407.21577v1](http://arxiv.org/abs/2407.21577v1)|null|
|**2024-07-31**|**PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**|Min Jae Jung et.al.|[2407.21571v1](http://arxiv.org/abs/2407.21571v1)|null|
|**2024-07-31**|**Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding**|Jun Zhou et.al.|[2407.21560v1](http://arxiv.org/abs/2407.21560v1)|null|
|**2024-07-31**|**Operator-based semantics for choice programs: is choosing losing? (full version)**|Jesse Heyninck et.al.|[2407.21556v1](http://arxiv.org/abs/2407.21556v1)|null|
|**2024-07-31**|**Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition**|Jiang Li et.al.|[2407.21536v1](http://arxiv.org/abs/2407.21536v1)|null|
|**2024-07-31**|**Can LLMs "Reason" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation**|Ziya Zhou et.al.|[2407.21531v1](http://arxiv.org/abs/2407.21531v1)|null|
|**2024-07-31**|**Data Contamination Report from the 2024 CONDA Shared Task**|Oscar Sainz et.al.|[2407.21530v1](http://arxiv.org/abs/2407.21530v1)|null|
|**2024-07-31**|**Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI**|Lingxi Cui et.al.|[2407.21523v1](http://arxiv.org/abs/2407.21523v1)|null|
|**2024-07-31**|**Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images**|I. M. Chernenkiy et.al.|[2407.21516v1](http://arxiv.org/abs/2407.21516v1)|null|
|**2024-07-31**|**Interpreting and learning voice commands with a Large Language Model for a robot system**|Stanislau Stankevich et.al.|[2407.21512v1](http://arxiv.org/abs/2407.21512v1)|null|
|**2024-07-31**|**FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication**|Yuna Yan et.al.|[2407.21507v1](http://arxiv.org/abs/2407.21507v1)|null|
|**2024-07-31**|**MaskUno: Switch-Split Block For Enhancing Instance Segmentation**|Jawad Haidar et.al.|[2407.21498v1](http://arxiv.org/abs/2407.21498v1)|null|
|**2024-07-31**|**Generative Expressive Conversational Speech Synthesis**|Rui Liu et.al.|[2407.21491v2](http://arxiv.org/abs/2407.21491v2)|null|
|**2024-07-31**|**Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation**|Junxuan Yu et.al.|[2407.21490v1](http://arxiv.org/abs/2407.21490v1)|null|
|**2024-07-31**|**Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends**|Giuliano Martinelli et.al.|[2407.21489v1](http://arxiv.org/abs/2407.21489v1)|null|
|**2024-07-31**|**eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**|Xiny Pan et.al.|[2407.21483v2](http://arxiv.org/abs/2407.21483v2)|null|
|**2024-07-31**|**On the Problem of Text-To-Speech Model Selection for Synthetic Data Generation in Automatic Speech Recognition**|Nick Rossenbach et.al.|[2407.21476v1](http://arxiv.org/abs/2407.21476v1)|null|
|**2024-07-31**|**Fine-gained Zero-shot Video Sampling**|Dengsheng Chen et.al.|[2407.21475v1](http://arxiv.org/abs/2407.21475v1)|null|
|**2024-07-31**|**An Invertible State Space for Process Trees**|Gero Kolhof et.al.|[2407.21468v1](http://arxiv.org/abs/2407.21468v1)|null|
|**2024-07-31**|**Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data**|Mengtian Kang et.al.|[2407.21467v1](http://arxiv.org/abs/2407.21467v1)|null|
|**2024-07-31**|**KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making**|Gilang Fajar Febrian et.al.|[2407.21459v1](http://arxiv.org/abs/2407.21459v1)|null|
|**2024-07-31**|**TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors**|Zhaolan Huang et.al.|[2407.21453v1](http://arxiv.org/abs/2407.21453v1)|null|
|**2024-07-31**|**Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**|Haodong Hong et.al.|[2407.21452v1](http://arxiv.org/abs/2407.21452v1)|null|
|**2024-07-31**|**Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency**|Taiji Li et.al.|[2407.21443v1](http://arxiv.org/abs/2407.21443v1)|null|
|**2024-07-31**|**QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications**|Ritvik Setty et.al.|[2407.21441v2](http://arxiv.org/abs/2407.21441v2)|null|
|**2024-07-31**|**MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training**|Zhanpeng Chen et.al.|[2407.21439v1](http://arxiv.org/abs/2407.21439v1)|null|
|**2024-07-31**|**Deformable 3D Shape Diffusion Model**|Dengsheng Chen et.al.|[2407.21428v1](http://arxiv.org/abs/2407.21428v1)|null|
|**2024-07-31**|**Cost-Effective Hallucination Detection for LLMs**|Simon Valentin et.al.|[2407.21424v1](http://arxiv.org/abs/2407.21424v1)|null|
|**2024-07-31**|**Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models**|Zhengxuan Wu et.al.|[2407.21417v1](http://arxiv.org/abs/2407.21417v1)|null|
|**2024-07-31**|**Towards interfacing large language models with ASR systems using confidence measures and prompting**|Maryam Naderi et.al.|[2407.21414v1](http://arxiv.org/abs/2407.21414v1)|null|
|**2024-07-31**|**GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction**|Yanxu Mao et.al.|[2407.21384v1](http://arxiv.org/abs/2407.21384v1)|null|
|**2024-07-31**|**An Extended Kalman Filter Integrated Latent Feature Model on Dynamic Weighted Directed Graphs**|Hongxun Zhou et.al.|[2407.21376v1](http://arxiv.org/abs/2407.21376v1)|null|
|**2024-07-31**|**Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering**|Danfeng Guo et.al.|[2407.21368v1](http://arxiv.org/abs/2407.21368v1)|null|
|**2024-07-31**|**ProSpec RL: Plan Ahead, then Execute**|Liangliang Liu et.al.|[2407.21359v1](http://arxiv.org/abs/2407.21359v1)|null|
|**2024-07-31**|**Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**|Elan Markowitz et.al.|[2407.21358v1](http://arxiv.org/abs/2407.21358v1)|null|
|**2024-07-31**|**Small Object Few-shot Segmentation for Vision-based Industrial Inspection**|Zilong Zhang et.al.|[2407.21351v1](http://arxiv.org/abs/2407.21351v1)|null|
|**2024-07-31**|**Differentially Private Block-wise Gradient Shuffle for Deep Learning**|David Zagardo et.al.|[2407.21347v1](http://arxiv.org/abs/2407.21347v1)|null|
|**2024-07-31**|**Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction**|Jingyao Wu et.al.|[2407.21344v1](http://arxiv.org/abs/2407.21344v1)|null|
|**2024-07-31**|**Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks**|David Valencia et.al.|[2407.21338v1](http://arxiv.org/abs/2407.21338v1)|null|
|**2024-07-31**|**Performance of Recent Large Language Models for a Low-Resourced Language**|Ravindu Jayakody et.al.|[2407.21330v1](http://arxiv.org/abs/2407.21330v1)|null|
|**2024-07-31**|**MetaOpenFOAM: an LLM-based multi-agent framework for CFD**|Yuxuan Chena et.al.|[2407.21320v1](http://arxiv.org/abs/2407.21320v1)|null|
|**2024-07-31**|**Big Cooperative Learning**|Yulai Cong et.al.|[2407.21319v1](http://arxiv.org/abs/2407.21319v1)|null|
|**2024-07-31**|**Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances**|Zehui Wu et.al.|[2407.21315v2](http://arxiv.org/abs/2407.21315v2)|null|
|**2024-07-31**|**EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer**|Ali Abedi et.al.|[2407.21311v1](http://arxiv.org/abs/2407.21311v1)|null|
|**2024-07-31**|**Implementing Streaming algorithm and k-means clusters to RAG**|Haoyu Kang et.al.|[2407.21300v1](http://arxiv.org/abs/2407.21300v1)|null|
|**2024-07-31**|**Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models**|Kaustav Bhattacharjee et.al.|[2407.21299v1](http://arxiv.org/abs/2407.21299v1)|null|
|**2024-07-31**|**SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**|Peiru Zheng et.al.|[2407.21293v1](http://arxiv.org/abs/2407.21293v1)|null|
|**2024-07-31**|**Robust Box Prompt based SAM for Medical Image Segmentation**|Yuhao Huang et.al.|[2407.21284v1](http://arxiv.org/abs/2407.21284v1)|null|
|**2024-07-31**|**Multi-Level Querying using A Knowledge Pyramid**|Rubing Chen et.al.|[2407.21276v1](http://arxiv.org/abs/2407.21276v1)|null|
|**2024-07-31**|**Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net**|Rohini Banerjee et.al.|[2407.21273v1](http://arxiv.org/abs/2407.21273v1)|null|
|**2024-07-31**|**DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations**|Dongwon Son et.al.|[2407.21267v1](http://arxiv.org/abs/2407.21267v1)|null|
|**2024-07-31**|**Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning**|Alimohammad Beigi et.al.|[2407.21264v1](http://arxiv.org/abs/2407.21264v1)|null|
|**2024-07-31**|**Lifelong Person Search**|Jae-Won Yang et.al.|[2407.21252v1](http://arxiv.org/abs/2407.21252v1)|null|
|**2024-07-30**|**Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens**|Anqi Zhang et.al.|[2407.21248v1](http://arxiv.org/abs/2407.21248v1)|null|
|**2024-07-30**|**Informed Correctors for Discrete Diffusion Models**|Yixiu Zhao et.al.|[2407.21243v1](http://arxiv.org/abs/2407.21243v1)|null|
|**2024-07-30**|**Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration**|Ngoc Son Nguyen et.al.|[2407.21229v1](http://arxiv.org/abs/2407.21229v1)|null|
|**2024-07-30**|**Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models**|Florian Tambon et.al.|[2407.21227v1](http://arxiv.org/abs/2407.21227v1)|null|
|**2024-07-30**|**AI methods for approximate compiling of unitaries**|David Kremer et.al.|[2407.21225v1](http://arxiv.org/abs/2407.21225v1)|null|
|**2024-07-30**|**LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban Noise Enforcement**|H. Emre Erdem et.al.|[2407.21204v1](http://arxiv.org/abs/2407.21204v1)|null|
|**2024-07-30**|**GenRec: Generative Personalized Sequential Recommendation**|Panfeng Cao et.al.|[2407.21191v1](http://arxiv.org/abs/2407.21191v1)|null|
|**2024-07-30**|**AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning**|Maisha Binte Rashid et.al.|[2407.21174v1](http://arxiv.org/abs/2407.21174v1)|null|
|**2024-07-30**|**Decomposed Prompting to Answer Questions on a Course Discussion Board**|Brandon Jaipersaud et.al.|[2407.21170v1](http://arxiv.org/abs/2407.21170v1)|null|
|**2024-07-30**|**Understanding Public Safety Trends in Calgary through data mining**|Zack Dewis et.al.|[2407.21163v1](http://arxiv.org/abs/2407.21163v1)|null|
|**2024-07-30**|**Event-Arguments Extraction Corpus and Modeling using BERT for Arabic**|Alaa Aljabari et.al.|[2407.21153v1](http://arxiv.org/abs/2407.21153v1)|null|
|**2024-07-30**|**Private Collaborative Edge Inference via Over-the-Air Computation**|Selim F. Yilmaz et.al.|[2407.21151v1](http://arxiv.org/abs/2407.21151v1)|null|
|**2024-07-30**|**Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population**|Mayanka Chandrashekar et.al.|[2407.21149v1](http://arxiv.org/abs/2407.21149v1)|null|

#### Abstracts
##### **Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey**
2407.21794v1 by Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq Joty, Yixuan Li, Hai Li, Ziwei Liu, Toshihiko Yamasaki, Kiyoharu Aizawa

Detecting out-of-distribution (OOD) samples is crucial for ensuring the
safety of machine learning systems and has shaped the field of OOD detection.
Meanwhile, several other problems are closely related to OOD detection,
including anomaly detection (AD), novelty detection (ND), open set recognition
(OSR), and outlier detection (OD). To unify these problems, a generalized OOD
detection framework was proposed, taxonomically categorizing these five
problems. However, Vision Language Models (VLMs) such as CLIP have
significantly changed the paradigm and blurred the boundaries between these
fields, again confusing researchers. In this survey, we first present a
generalized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD
detection, and OD in the VLM era. Our framework reveals that, with some field
inactivity and integration, the demanding challenges have become OOD detection
and AD. In addition, we also highlight the significant shift in the definition,
problem settings, and benchmarks; we thus feature a comprehensive review of the
methodology for OOD detection, including the discussion over other related
tasks to clarify their relationship to OOD detection. Finally, we explore the
advancements in the emerging Large Vision Language Model (LVLM) era, such as
GPT-4V. We conclude this survey with open challenges and future directions.

ÊëòË¶ÅÔºöÂÅµÊ∏¨Áï∞Â∏∏Ê®£Êú¨ (OOD) Â∞çÊñºÁ¢∫‰øùÊ©üÂô®Â≠∏ÁøíÁ≥ªÁµ±ÁöÑÂÆâÂÖ®ÊÄßËá≥ÈóúÈáçË¶ÅÔºå‰∏¶ÂΩ¢Â°ë‰∫Ü OOD ÂÅµÊ∏¨È†òÂüü„ÄÇÂêåÊôÇÔºåÈÇÑÊúâË®±Â§öÂÖ∂‰ªñÂïèÈ°åËàá OOD ÂÅµÊ∏¨ÊÅØÊÅØÁõ∏ÈóúÔºåÂåÖÊã¨Áï∞Â∏∏ÂÅµÊ∏¨ (AD)„ÄÅÊñ∞Á©éÊÄßÂÅµÊ∏¨ (ND)„ÄÅÈñãÊîæÈõÜË≠òÂà• (OSR) ÂíåÈõ¢Áæ§ÂÄºÂÅµÊ∏¨ (OD)„ÄÇÁÇ∫‰∫ÜÁµ±‰∏ÄÈÄô‰∫õÂïèÈ°åÔºåÊèêÂá∫‰∫ÜÂª£Áæ©ÁöÑ OOD ÂÅµÊ∏¨Êû∂ÊßãÔºåÂ∞áÈÄô‰∫îÂÄãÂïèÈ°åÂàÜÈ°û„ÄÇÁÑ∂ËÄåÔºåÂÉè CLIP Á≠âË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Â∑≤Â§ßÂπÖÊîπËÆäÂÖ∏ÁØÑÔºå‰∏¶Ê®°Á≥ä‰∫ÜÈÄô‰∫õÈ†òÂüü‰πãÈñìÁöÑÁïåÁ∑öÔºåÂÜçÊ¨°ËÆìÁ†îÁ©∂‰∫∫Âì°ÊÑüÂà∞Âõ∞ÊÉë„ÄÇÂú®ÈÄôÈ†ÖË™øÊü•‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÊèêÂá∫Âª£Áæ©ÁöÑ OOD ÂÅµÊ∏¨ v2ÔºåÊ¶ÇÊã¨‰∫Ü AD„ÄÅND„ÄÅOSR„ÄÅOOD ÂÅµÊ∏¨Âíå OD Âú® VLM ÊôÇ‰ª£ÁöÑÊºîÈÄ≤„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊè≠Á§∫ÔºåÁî±ÊñºÊüê‰∫õÈ†òÂüüÁöÑ‰∏çÊ¥ªË∫çÂíåÊï¥ÂêàÔºåÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑÂïèÈ°åÂ∑≤ÊàêÁÇ∫ OOD ÂÅµÊ∏¨Âíå AD„ÄÇÊ≠§Â§ñÔºåÊàëÂÄë‰πüÈáçÈªûË™™ÊòéÂÆöÁæ©„ÄÅÂïèÈ°åË®≠ÂÆöÂíåÂü∫Ê∫ñÁöÑÈáçÂ§ßËΩâËÆäÔºõÂõ†Ê≠§ÔºåÊàëÂÄëÂ∞ç OOD ÂÅµÊ∏¨ÁöÑÊñπÊ≥ïË´ñÈÄ≤Ë°åÂÖ®Èù¢Ê™¢Ë¶ñÔºåÂåÖÊã¨Ë®éË´ñÂÖ∂‰ªñÁõ∏Èóú‰ªªÂãô‰ª•ÈáêÊ∏ÖÂÆÉÂÄëËàá OOD ÂÅµÊ∏¨ÁöÑÈóú‰øÇ„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊé¢Ë®éÊñ∞ËààÁöÑÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (LVLM) ÊôÇ‰ª£ÁöÑÈÄ≤Â±ïÔºå‰æãÂ¶Ç GPT-4V„ÄÇÊàëÂÄë‰ª•ÈñãÊîæÊåëÊà∞ÂíåÊú™‰æÜÊñπÂêë‰ΩúÁÇ∫ÈÄôÈ†ÖË™øÊü•ÁöÑÁµêË´ñ„ÄÇ

##### **Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?**
2407.21792v1 by Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks

As artificial intelligence systems grow more powerful, there has been
increasing interest in "AI safety" research to address emerging and future
risks. However, the field of AI safety remains poorly defined and
inconsistently measured, leading to confusion about how researchers can
contribute. This lack of clarity is compounded by the unclear relationship
between AI safety benchmarks and upstream general capabilities (e.g., general
knowledge and reasoning). To address these issues, we conduct a comprehensive
meta-analysis of AI safety benchmarks, empirically analyzing their correlation
with general capabilities across dozens of models and providing a survey of
existing directions in AI safety. Our findings reveal that many safety
benchmarks highly correlate with upstream model capabilities, potentially
enabling "safetywashing" -- where capability improvements are misrepresented as
safety advancements. Based on these findings, we propose an empirical
foundation for developing more meaningful safety metrics and define AI safety
in a machine learning research context as a set of clearly delineated research
goals that are empirically separable from generic capabilities advancements. In
doing so, we aim to provide a more rigorous framework for AI safety research,
advancing the science of safety evaluations and clarifying the path towards
measurable progress.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖßÁ≥ªÁµ±ËÆäÂæóË∂ä‰æÜË∂äÂº∑Â§ßÔºåÂ∞çÊñº„ÄåAI ÂÆâÂÖ®„ÄçÁöÑÁ†îÁ©∂ËààË∂£‰πüËàáÊó•‰ø±Â¢ûÔºå‰ª•Âõ†ÊáâÊñ∞ËààÂíåÊú™‰æÜÁöÑÈ¢®Èö™„ÄÇÁÑ∂ËÄåÔºåAI ÂÆâÂÖ®È†òÂüüÁöÑÂÆöÁæ©‰ªçÁÑ∂ÂæàÊ®°Á≥äÔºåË°°ÈáèÊ®ôÊ∫ñ‰πü‰∏ç‰∏ÄËá¥ÔºåÂ∞éËá¥Á†îÁ©∂‰∫∫Âì°Â¶Ç‰ΩïÂÅöÂá∫Ë≤¢ÁçªÊÑüÂà∞Âõ∞ÊÉë„ÄÇAI ÂÆâÂÖ®Âü∫Ê∫ñËàá‰∏äÊ∏∏‰∏ÄËà¨ËÉΩÂäõÔºà‰æãÂ¶Ç‰∏ÄËà¨Áü•Ë≠òÂíåÊé®ÁêÜÔºâ‰πãÈñìÈóú‰øÇ‰∏çÊòéÁ¢∫ÔºåÈÄ≤‰∏ÄÊ≠•Âä†Âäá‰∫ÜÈÄôÁ®Æ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂ∞ç AI ÂÆâÂÖ®Âü∫Ê∫ñÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂæåË®≠ÂàÜÊûêÔºåÊ†πÊìöÊï∏ÂçÅÂÄãÊ®°ÂûãÂØ¶Ë≠âÂàÜÊûêÂÆÉÂÄëËàá‰∏ÄËà¨ËÉΩÂäõÁöÑÁõ∏ÈóúÊÄßÔºå‰∏¶Â∞ç AI ÂÆâÂÖ®‰∏≠ÁöÑÁèæÊúâÊñπÂêëÈÄ≤Ë°åË™øÊü•„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåË®±Â§öÂÆâÂÖ®Âü∫Ê∫ñËàá‰∏äÊ∏∏Ê®°ÂûãËÉΩÂäõÈ´òÂ∫¶Áõ∏ÈóúÔºåÈÄôÂèØËÉΩÊúÉÂ∞éËá¥„ÄåÂÆâÂÖ®ÊºÇÁôΩ„Äç‚Äî‚ÄîÂ∞áËÉΩÂäõÁöÑÊèêÂçáË™§Ë™çÁÇ∫ÊòØÂÆâÂÖ®ÊÄßÁöÑÈÄ≤Ê≠•„ÄÇÊ†πÊìöÈÄô‰∫õÁ†îÁ©∂ÁµêÊûúÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂØ¶Ë≠âÂü∫Á§éÔºåÁî®ÊñºÈñãÁôºÊõ¥ÊúâÊÑèÁæ©ÁöÑÂÆâÂÖ®ÊåáÊ®ôÔºå‰∏¶Âú®Ê©üÂô®Â≠∏ÁøíÁ†îÁ©∂ËÉåÊôØ‰∏ãÂ∞á AI ÂÆâÂÖ®ÂÆöÁæ©ÁÇ∫‰∏ÄÁµÑÊòéÁ¢∫ÁïåÂÆöÁöÑÁ†îÁ©∂ÁõÆÊ®ôÔºåÈÄô‰∫õÁõÆÊ®ôÂú®ÂØ¶Ë≠â‰∏äÂèØ‰ª•Ëàá‰∏ÄËà¨ËÉΩÂäõÁöÑÈÄ≤Ê≠•ÂçÄÂàÜÈñã‰æÜ„ÄÇÈÄèÈÅéÈÄôÈ∫ºÂÅöÔºåÊàëÂÄëÊó®Âú®ÁÇ∫ AI ÂÆâÂÖ®Á†îÁ©∂Êèê‰æõ‰∏ÄÂÄãÊõ¥Âö¥Ë¨πÁöÑÊû∂ÊßãÔºåÊé®ÈÄ≤ÂÆâÂÖ®Ë©ï‰º∞ÁöÑÁßëÂ≠∏Ôºå‰∏¶ÈáêÊ∏ÖÈÇÅÂêëÂèØË°°ÈáèÈÄ≤Â±ïÁöÑÈÅìË∑Ø„ÄÇ

##### **Vision-Language Model Based Handwriting Verification**
2407.21788v1 by Mihir Chauhan, Abhishek Satbhai, Mohammad Abuzar Hashemi, Mir Basheer Ali, Bina Ramamurthy, Mingchen Gao, Siwei Lyu, Sargur Srihari

Handwriting Verification is a critical in document forensics. Deep learning
based approaches often face skepticism from forensic document examiners due to
their lack of explainability and reliance on extensive training data and
handcrafted features. This paper explores using Vision Language Models (VLMs),
such as OpenAI's GPT-4o and Google's PaliGemma, to address these challenges. By
leveraging their Visual Question Answering capabilities and 0-shot
Chain-of-Thought (CoT) reasoning, our goal is to provide clear,
human-understandable explanations for model decisions. Our experiments on the
CEDAR handwriting dataset demonstrate that VLMs offer enhanced
interpretability, reduce the need for large training datasets, and adapt better
to diverse handwriting styles. However, results show that the CNN-based
ResNet-18 architecture outperforms the 0-shot CoT prompt engineering approach
with GPT-4o (Accuracy: 70%) and supervised fine-tuned PaliGemma (Accuracy:
71%), achieving an accuracy of 84% on the CEDAR AND dataset. These findings
highlight the potential of VLMs in generating human-interpretable decisions
while underscoring the need for further advancements to match the performance
of specialized deep learning models.

ÊëòË¶ÅÔºöÊâãÂØ´È©óË≠âÂú®Êñá‰ª∂ÈëëË≠ò‰∏≠Ëá≥ÈóúÈáçË¶Å„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÂèóÂà∞Êñá‰ª∂ÈëëË≠òÂ∞àÂÆ∂ÁöÑÊá∑ÁñëÔºåÂéüÂõ†Âú®ÊñºÂÆÉÂÄëÁº∫‰πèÂèØËß£ÈáãÊÄßÔºå‰∏¶‰∏î‰æùË≥¥ÊñºÂ§ßÈáèÁöÑË®ìÁ∑¥Ë≥áÊñôÂíåÊâãÂ∑•ÁâπÂæµ„ÄÇÊú¨ÊñáÊé¢Ë®é‰ΩøÁî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)Ôºå‰æãÂ¶Ç OpenAI ÁöÑ GPT-4o Âíå Google ÁöÑ PaliGemmaÔºå‰æÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞„ÄÇÈÄöÈÅéÂà©Áî®ÂÆÉÂÄëÁöÑË¶ñË¶∫ÂïèÁ≠îËÉΩÂäõÂíå 0-shot ÊÄùÊÉ≥Èèà (CoT) Êé®ÁêÜÔºåÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÁÇ∫Ê®°ÂûãÊ±∫Á≠ñÊèê‰æõÊ∏ÖÊô∞„ÄÅ‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑËß£Èáã„ÄÇÊàëÂÄëÂú® CEDAR ÊâãÂØ´Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåVLM Êèê‰æõ‰∫ÜÂ¢ûÂº∑ÁöÑÂèØËß£ÈáãÊÄßÔºåÊ∏õÂ∞ë‰∫ÜÂ∞çÂ§ßÂûãË®ìÁ∑¥Ë≥áÊñôÈõÜÁöÑÈúÄÊ±ÇÔºå‰∏¶‰∏îÂèØ‰ª•Êõ¥Â•ΩÂú∞ÈÅ©Êáâ‰∏çÂêåÁöÑÊâãÂØ´È¢®Ê†º„ÄÇÁÑ∂ËÄåÔºåÁµêÊûúË°®ÊòéÔºåÂü∫Êñº CNN ÁöÑ ResNet-18 Êû∂ÊßãÂÑ™Êñº‰ΩøÁî® GPT-4oÔºàÊ∫ñÁ¢∫ÁéáÔºö70%ÔºâÂíåÁõ£Áù£ÂæÆË™ø PaliGemmaÔºàÊ∫ñÁ¢∫ÁéáÔºö71%ÔºâÁöÑ 0-shot CoT ÊèêÁ§∫Â∑•Á®ãÊñπÊ≥ïÔºåÂú® CEDAR AND Ë≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫Ü 84% ÁöÑÊ∫ñÁ¢∫Áéá„ÄÇÈÄô‰∫õÁôºÁèæÁ™ÅÈ°Ø‰∫Ü VLM Âú®Áî¢Áîü‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑÊ±∫Á≠ñÊñπÈù¢ÁöÑÊΩõÂäõÔºåÂêåÊôÇ‰πüÂº∑Ë™ø‰∫ÜÈÄ≤‰∏ÄÊ≠•ÊèêÂçáËàáÂ∞àÊ•≠Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁõ∏ÂåπÈÖçÁöÑÊïàËÉΩÁöÑÂøÖË¶ÅÊÄß„ÄÇ

##### **Large Language Monkeys: Scaling Inference Compute with Repeated Sampling**
2407.21787v1 by Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher R√©, Azalia Mirhoseini

Scaling the amount of compute used to train language models has dramatically
improved their capabilities. However, when it comes to inference, we often
limit the amount of compute to only one attempt per problem. Here, we explore
inference compute as another axis for scaling by increasing the number of
generated samples. Across multiple tasks and models, we observe that coverage -
the fraction of problems solved by any attempt - scales with the number of
samples over four orders of magnitude. In domains like coding and formal
proofs, where all answers can be automatically verified, these increases in
coverage directly translate into improved performance. When we apply repeated
sampling to SWE-bench Lite, the fraction of issues solved with
DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250
samples, outperforming the single-attempt state-of-the-art of 43% which uses
more capable frontier models. Moreover, using current API pricing, amplifying
the cheaper DeepSeek model with five samples is more cost-effective and solves
more issues than paying a premium for one sample from GPT-4o or Claude 3.5
Sonnet. Interestingly, the relationship between coverage and the number of
samples is often log-linear and can be modelled with an exponentiated power
law, suggesting the existence of inference-time scaling laws. Finally, we find
that identifying correct samples out of many generations remains an important
direction for future research in domains without automatic verifiers. When
solving math word problems from GSM8K and MATH, coverage with Llama-3 models
grows to over 95% with 10,000 samples. However, common methods to pick correct
solutions from a sample collection, such as majority voting or reward models,
plateau beyond several hundred samples and fail to fully scale with the sample
budget.

ÊëòË¶ÅÔºö<paragraph>Êì¥Â§ßÁî®ÊñºË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÁöÑÈÅãÁÆóÈáèÂ∑≤Â§ßÂπÖÊèêÂçáÂÖ∂ÂäüËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂú®ÈÄ≤Ë°åÊé®Ë´ñÊôÇÔºåÊàëÂÄëÈÄöÂ∏∏Â∞áÈÅãÁÆóÈáèÈôêÂà∂Âú®ÊØèÂÄãÂïèÈ°åÂÉÖÂòóË©¶‰∏ÄÊ¨°„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂ∞áÊé®Ë´ñÈÅãÁÆóË¶ñÁÇ∫Âè¶‰∏ÄÁ®ÆÊì¥Â±ïËª∏ÔºåËóâÁî±Â¢ûÂä†ÁîüÊàêÁØÑ‰æãÁöÑÊï∏Èáè‰æÜÈÄ≤Ë°åÊì¥Â±ï„ÄÇÂú®Â§öÂÄã‰ªªÂãôÂíåÊ®°Âûã‰∏≠ÔºåÊàëÂÄëËßÄÂØüÂà∞Ë¶ÜËìãÁéáÔºà‰ªª‰ΩïÂòóË©¶Ëß£Ê±∫ÂïèÈ°åÁöÑÂàÜÊï∏ÔºâÊúÉÈö®ËëóÁØÑ‰æãÊï∏ÈáèËÄåÊì¥Â±ïÔºåË∂ÖÈÅéÂõõÂÄãÊï∏ÈáèÁ¥ö„ÄÇÂú®Á∑®Á¢ºÂíåÂΩ¢ÂºèÂåñË≠âÊòéÁ≠âÈ†òÂüü‰∏≠ÔºåÊâÄÊúâÁ≠îÊ°àÈÉΩÂèØ‰ª•Ëá™ÂãïÈ©óË≠âÔºåÈÄô‰∫õË¶ÜËìãÁéáÁöÑÂ¢ûÂä†ÊúÉÁõ¥Êé•ËΩâÂåñÁÇ∫ÊïàËÉΩÁöÑÊèêÂçá„ÄÇÁï∂ÊàëÂÄëÂ∞áÈáçË§áÊäΩÊ®£Â•óÁî®Êñº SWE-bench Lite ÊôÇÔºå‰ΩøÁî® DeepSeek-V2-Coder-Instruct Ëß£Ê±∫ÂïèÈ°åÁöÑÂàÜÊï∏Âæû‰∏ÄÂÄãÁØÑ‰æãÁöÑ 15.9% ÊèêÂçáÂà∞ 250 ÂÄãÁØÑ‰æãÁöÑ 56%ÔºåÂÑ™Êñº‰ΩøÁî®ÂäüËÉΩÊõ¥Âº∑Â§ßÁöÑÂâçÊ≤øÊ®°ÂûãËÄåÈÅîÂà∞ÁöÑ 43% ÂñÆÊ¨°ÂòóË©¶ÊúÄÂÖàÈÄ≤Ê∞¥Ê∫ñ„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®ÁõÆÂâçÁöÑ API ÂÆöÂÉπÔºå‰ª•‰∫îÂÄãÁØÑ‰æãÊì¥ÂÖÖËºÉ‰æøÂÆúÁöÑ DeepSeek Ê®°ÂûãÊØîÊîØ‰ªòÊ∫¢ÂÉπÂèñÂæó GPT-4o Êàñ Claude 3.5 Sonnet ÁöÑ‰∏ÄÂÄãÁØÑ‰æãÊõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõäÔºå‰∏îËÉΩËß£Ê±∫Êõ¥Â§öÂïèÈ°å„ÄÇÊúâË∂£ÁöÑÊòØÔºåË¶ÜËìãÁéáËàáÁØÑ‰æãÊï∏Èáè‰πãÈñìÁöÑÈóú‰øÇÈÄöÂ∏∏ÊòØÂ∞çÊï∏Á∑öÊÄßÁöÑÔºå‰∏îÂèØÁî®ÊåáÊï∏ÂÜ™ÂæãÂª∫Ê®°ÔºåÈÄôË°®Á§∫Â≠òÂú®Êé®Ë´ñÊôÇÈñìÊì¥Â±ïÂæã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÁôºÁèæÂæûË®±Â§ö‰∏ñ‰ª£‰∏≠ÊâæÂá∫Ê≠£Á¢∫ÁØÑ‰æã‰ªçÁÑ∂ÊòØÊ≤íÊúâËá™ÂãïÈ©óË≠âÂô®ÁöÑÈ†òÂüü‰∏≠Êú™‰æÜÁ†îÁ©∂ÁöÑÈáçË¶ÅÊñπÂêë„ÄÇÂú®Ëß£Ê±∫ GSM8K Âíå MATH ÁöÑÊï∏Â≠∏ÊñáÂ≠óÈ°åÊôÇÔºå‰ΩøÁî® Llama-3 Ê®°ÂûãÁöÑË¶ÜËìãÁéáÊúÉÂú® 10,000 ÂÄãÁØÑ‰æã‰∏≠ÊàêÈï∑Âà∞Ë∂ÖÈÅé 95%„ÄÇÁÑ∂ËÄåÔºåÂæûÁØÑ‰æãÈõÜÂêà‰∏≠ÊåëÈÅ∏Ê≠£Á¢∫Ëß£Á≠îÁöÑÂ∏∏Ë¶ãÊñπÊ≥ïÔºà‰æãÂ¶ÇÂ§öÊï∏Ê±∫ÊàñÁçéÂãµÊ®°ÂûãÔºâÊúÉÂú®Êï∏ÁôæÂÄãÁØÑ‰æãÂæåÈÅîÂà∞Âπ≥Á©©ÊúüÔºå‰∏îÁÑ°Ê≥ïÂÆåÂÖ®Èö®ËëóÁØÑ‰æãÈ†êÁÆóËÄåÊì¥Â±ï„ÄÇ</paragraph>

##### **The Llama 3 Herd of Models**
2407.21783v1 by Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur √áelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzm√°n, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao

Modern artificial intelligence (AI) systems are powered by foundation models.
This paper presents a new set of foundation models, called Llama 3. It is a
herd of language models that natively support multilinguality, coding,
reasoning, and tool usage. Our largest model is a dense Transformer with 405B
parameters and a context window of up to 128K tokens. This paper presents an
extensive empirical evaluation of Llama 3. We find that Llama 3 delivers
comparable quality to leading language models such as GPT-4 on a plethora of
tasks. We publicly release Llama 3, including pre-trained and post-trained
versions of the 405B parameter language model and our Llama Guard 3 model for
input and output safety. The paper also presents the results of experiments in
which we integrate image, video, and speech capabilities into Llama 3 via a
compositional approach. We observe this approach performs competitively with
the state-of-the-art on image, video, and speech recognition tasks. The
resulting models are not yet being broadly released as they are still under
development.

ÊëòË¶ÅÔºöÁèæ‰ª£‰∫∫Â∑•Êô∫ÊÖß (AI) Á≥ªÁµ±Áî±Âü∫Á§éÊ®°ÂûãÊèê‰æõÂãïÂäõ„ÄÇ
Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁµÑÊñ∞ÁöÑÂü∫Á§éÊ®°ÂûãÔºåÁ®±ÁÇ∫ Llama 3„ÄÇÂÆÉÊòØ‰∏ÄÁæ§Ë™ûË®ÄÊ®°ÂûãÔºåÊú¨Ê©üÊîØÊè¥Â§öË™ûË®Ä„ÄÅÁ∑®Á¢º„ÄÅÊé®ÁêÜÂíåÂ∑•ÂÖ∑‰ΩøÁî®„ÄÇÊàëÂÄëÊúÄÂ§ßÁöÑÊ®°ÂûãÊòØ‰∏ÄÂÄãÂÖ∑Êúâ 405B ÂèÉÊï∏ÂíåÊúÄÂ§ö 128K ‰ª§ÁâåÁöÑ‰∏ä‰∏ãÊñáË¶ñÁ™óÁöÑÂØÜÈõÜTransformer„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜÂ∞ç Llama 3 ÁöÑÂª£Ê≥õÂØ¶Ë≠âË©ï‰º∞„ÄÇÊàëÂÄëÁôºÁèæ Llama 3 Âú®Â§ßÈáè‰ªªÂãô‰∏äÊèê‰æõ‰∫ÜËàá GPT-4 Á≠âÈ†òÂÖàË™ûË®ÄÊ®°ÂûãÁõ∏Áï∂ÁöÑÂìÅË≥™„ÄÇÊàëÂÄëÂÖ¨ÈñãÁôºÂ∏É Llama 3ÔºåÂåÖÊã¨ 405B ÂèÉÊï∏Ë™ûË®ÄÊ®°ÂûãÁöÑÈ†êË®ìÁ∑¥ÂíåÂæåË®ìÁ∑¥ÁâàÊú¨Ôºå‰ª•ÂèäÊàëÂÄëÁöÑ Llama Guard 3 Ê®°ÂûãÔºåÁî®ÊñºËº∏ÂÖ•ÂíåËº∏Âá∫ÂÆâÂÖ®ÊÄß„ÄÇÊú¨ÊñáÈÇÑÊèê‰æõ‰∫ÜÂ∞áÂΩ±ÂÉè„ÄÅÂΩ±ÁâáÂíåË™ûÈü≥ÂäüËÉΩÊï¥ÂêàÂà∞ Llama 3 ‰∏≠ÁöÑÂØ¶È©óÁµêÊûúÔºåÊñπÊ≥ïÊòØÊé°Áî®ÁµÑÂêàÂºèÊñπÊ≥ï„ÄÇÊàëÂÄëËßÄÂØüÂà∞ÈÄôÁ®ÆÊñπÊ≥ïÂú®ÂΩ±ÂÉè„ÄÅÂΩ±ÁâáÂíåË™ûÈü≥Ëæ®Ë≠ò‰ªªÂãô‰∏äË°®ÁèæÂá∫ËàáÊúÄÂÖàÈÄ≤ÊäÄË°ìÁõ∏Áï∂ÁöÑÁ´∂Áà≠Âäõ„ÄÇÁî±ÊñºÈÄô‰∫õÊ®°Âûã‰ªçËôïÊñºÈñãÁôºÈöéÊÆµÔºåÂõ†Ê≠§Â∞öÊú™Âª£Ê≥õÁôºÂ∏É„ÄÇ

##### **Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries**
2407.21778v1 by Felix Ocker, Daniel Tanneberg, Julian Eggert, Michael Gienger

We introduce tulip agent, an architecture for autonomous LLM-based agents
with Create, Read, Update, and Delete access to a tool library containing a
potentially large number of tools. In contrast to state-of-the-art
implementations, tulip agent does not encode the descriptions of all available
tools in the system prompt, which counts against the model's context window, or
embed the entire prompt for retrieving suitable tools. Instead, the tulip agent
can recursively search for suitable tools in its extensible tool library,
implemented exemplarily as a vector store. The tulip agent architecture
significantly reduces inference costs, allows using even large tool libraries,
and enables the agent to adapt and extend its set of tools. We evaluate the
architecture with several ablation studies in a mathematics context and
demonstrate its generalizability with an application to robotics. A reference
implementation and the benchmark are available at
github.com/HRI-EU/tulip_agent.

ÊëòË¶ÅÔºöÊàëÂÄë‰ªãÁ¥π‰∫Ü Tulip ‰ª£ÁêÜÔºå‰∏ÄÁ®ÆÂü∫ÊñºËá™‰∏ª LLM ÁöÑ‰ª£ÁêÜÊû∂ÊßãÔºåÂèØ‰ª•Â∞çÂåÖÂê´Â§ßÈáèÂ∑•ÂÖ∑ÁöÑÂ∑•ÂÖ∑Â∫´ÈÄ≤Ë°åÂª∫Á´ã„ÄÅËÆÄÂèñ„ÄÅÊõ¥Êñ∞ÂíåÂà™Èô§Â≠òÂèñ„ÄÇËàáÊúÄÂÖàÈÄ≤ÁöÑÂØ¶‰ΩúÁõ∏ÂèçÔºåTulip ‰ª£ÁêÜ‰∏çÊúÉÂ∞áÊâÄÊúâÂèØÁî®Â∑•ÂÖ∑ÁöÑÊèèËø∞Á∑®Á¢ºÂú®Á≥ªÁµ±ÊèêÁ§∫‰∏≠ÔºåÈÄôÊúÉË®àÂÖ•Ê®°ÂûãÁöÑÂÖßÂÆπË¶ñÁ™óÔºåÊàñÂµåÂÖ•Êï¥ÂÄãÊèêÁ§∫‰ª•Êì∑ÂèñÂêàÈÅ©ÁöÑÂ∑•ÂÖ∑„ÄÇÁõ∏ÂèçÂú∞ÔºåTulip ‰ª£ÁêÜÂèØ‰ª•Âú®ÂÖ∂ÂèØÂª∂‰º∏Â∑•ÂÖ∑Â∫´‰∏≠ÈÅûËø¥ÊêúÂ∞ãÂêàÈÅ©ÁöÑÂ∑•ÂÖ∑ÔºåÁØÑ‰æãÂØ¶‰ΩúÁÇ∫ÂêëÈáèÂÑ≤Â≠ò„ÄÇTulip ‰ª£ÁêÜÊû∂ÊßãÂ§ßÂπÖÈôç‰Ωé‰∫ÜÊé®Ë´ñÊàêÊú¨ÔºåÂÖÅË®±‰ΩøÁî®ÁîöËá≥Â§ßÂûãÂ∑•ÂÖ∑Â∫´Ôºå‰∏¶ËÆì‰ª£ÁêÜË™øÊï¥ÂíåÂª∂‰º∏ÂÖ∂Â∑•ÂÖ∑ÁµÑ„ÄÇÊàëÂÄëÂú®Êï∏Â≠∏ËÉåÊôØ‰∏≠‰ΩøÁî®Â§öÈ†ÖÊ∂àËûçÁ†îÁ©∂Ë©ï‰º∞Êû∂ÊßãÔºå‰∏¶ÈÄèÈÅéÊ©üÂô®‰∫∫ÊáâÁî®Á®ãÂºèÂ±ïÁ§∫ÂÖ∂Ê¶ÇÊã¨ÊÄß„ÄÇÂèØ‰ª•Âú® github.com/HRI-EU/tulip_agent ÂèñÂæóÂèÉËÄÉÂØ¶‰ΩúÂíåÂü∫Ê∫ñ„ÄÇ

##### **ShieldGemma: Generative AI Content Moderation Based on Gemma**
2407.21772v1 by Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, Olivia Sturman, Oscar Wahltinez

We present ShieldGemma, a comprehensive suite of LLM-based safety content
moderation models built upon Gemma2. These models provide robust,
state-of-the-art predictions of safety risks across key harm types (sexually
explicit, dangerous content, harassment, hate speech) in both user input and
LLM-generated output. By evaluating on both public and internal benchmarks, we
demonstrate superior performance compared to existing models, such as Llama
Guard (+10.8\% AU-PRC on public benchmarks) and WildCard (+4.3\%).
Additionally, we present a novel LLM-based data curation pipeline, adaptable to
a variety of safety-related tasks and beyond. We have shown strong
generalization performance for model trained mainly on synthetic data. By
releasing ShieldGemma, we provide a valuable resource to the research
community, advancing LLM safety and enabling the creation of more effective
content moderation solutions for developers.

ÊëòË¶ÅÔºöÊàëÂÄëÂ±ïÁ§∫ ShieldGemmaÔºåÈÄôÊòØ‰∏ÄÂ•óÂª∫ÊßãÊñº Gemma2 ÁöÑÂÖ®Èù¢ LLM ÂÆâÂÖ®ÂÖßÂÆπÂØ©Ê†∏Ê®°ÂûãÂ•ó‰ª∂„ÄÇÈÄô‰∫õÊ®°ÂûãÊèê‰æõÂÅ•ÂÖ®„ÄÅÊúÄÂÖàÈÄ≤ÁöÑÂÆâÂÖ®ÊÄßÈ¢®Èö™È†êÊ∏¨ÔºåÊ∂µËìã‰ΩøÁî®ËÄÖËº∏ÂÖ•Âíå LLM Áî¢ÁîüÁöÑËº∏Âá∫‰∏≠ÁöÑ‰∏ªË¶ÅÂç±ÂÆ≥È°ûÂûãÔºàÈú≤È™®ÊÄßÂÖßÂÆπ„ÄÅÂç±Èö™ÂÖßÂÆπ„ÄÅÈ®∑Êìæ„ÄÅ‰ªáÊÅ®Ë®ÄË´ñÔºâ„ÄÇÈÄèÈÅéÂú®ÂÖ¨ÈñãÂíåÂÖßÈÉ®Âü∫Ê∫ñ‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåÊàëÂÄëÂ±ïÁ§∫Âá∫ÂÑ™ÊñºÁèæÊúâÊ®°ÂûãÁöÑÂçìË∂äÊïàËÉΩÔºå‰æãÂ¶Ç Llama GuardÔºàÂú®ÂÖ¨ÈñãÂü∫Ê∫ñ‰∏ä +10.8% AU-PRCÔºâÂíå WildCardÔºà+4.3%Ôºâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑ LLM Ë≥áÊñôÁ≠ñÁÆ°ÊµÅÁ®ãÔºåÈÅ©Áî®ÊñºÂêÑÁ®ÆÂÆâÂÖ®Áõ∏Èóú‰ªªÂãôÂèäÂÖ∂‰ªñ‰ªªÂãô„ÄÇÊàëÂÄëÂ∑≤Â±ïÁ§∫Âá∫Â∞çÊñº‰∏ªË¶ÅË®ìÁ∑¥Âú®ÂêàÊàêË≥áÊñô‰∏äÁöÑÊ®°ÂûãÁöÑÂº∑Â§ßÊ≥õÂåñÊïàËÉΩ„ÄÇÈÄèÈÅéÈáãÂá∫ ShieldGemmaÔºåÊàëÂÄëÁÇ∫Á†îÁ©∂Á§æÁæ§Êèê‰æõ‰∫ÜÂØ∂Ë≤¥ÁöÑË≥áÊ∫êÔºåÊé®ÈÄ≤ LLM ÂÆâÂÖ®ÊÄß‰∏¶ÂçîÂä©ÈñãÁôº‰∫∫Âì°Âª∫Á´ãÊõ¥ÊúâÊïàÁöÑÂÖßÂÆπÂØ©Ê†∏Ëß£Ê±∫ÊñπÊ°à„ÄÇ

##### **MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts**
2407.21770v1 by Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, Armen Aghajanyan

We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)
architecture designed for pre-training mixed-modal, early-fusion language
models. MoMa processes images and text in arbitrary sequences by dividing
expert modules into modality-specific groups. These groups exclusively process
designated tokens while employing learned routing within each group to maintain
semantically informed adaptivity. Our empirical results reveal substantial
pre-training efficiency gains through this modality-specific parameter
allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,
featuring 4 text experts and 4 image experts, achieves impressive FLOPs
savings: 3.7x overall, with 2.6x for text and 5.2x for image processing
compared to a compute-equivalent dense baseline, measured by pre-training loss.
This outperforms the standard expert-choice MoE with 8 mixed-modal experts,
which achieves 3x overall FLOPs savings (3x for text, 2.8x for image).
Combining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs
savings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination
hurts performance in causal inference due to increased sensitivity to router
accuracy. These results demonstrate MoMa's potential to significantly advance
the efficiency of mixed-modal, early-fusion language model pre-training, paving
the way for more resource-efficient and capable multimodal AI systems.

ÊëòË¶ÅÔºö<paragraph>ÊàëÂÄë‰ªãÁ¥π MoMaÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ®°ÊÖãÊÑüÁü•Ê∑∑ÂêàÂ∞àÂÆ∂ (MoE) Êû∂ÊßãÔºåÂ∞àÁÇ∫Ê∑∑ÂêàÊ®°ÊÖã„ÄÅÊó©ÊúüËûçÂêàË™ûË®ÄÊ®°ÂûãÁöÑÈ†êË®ìÁ∑¥ËÄåË®≠Ë®à„ÄÇMoMa ÈÄèÈÅéÂ∞áÂ∞àÂÆ∂Ê®°ÁµÑÂàÜÁÇ∫Ê®°ÊÖãÁâπÂÆöÁæ§ÁµÑÔºå‰ª•‰ªªÊÑèÈ†ÜÂ∫èËôïÁêÜÂΩ±ÂÉèÂíåÊñáÂ≠ó„ÄÇÈÄô‰∫õÁæ§ÁµÑÊúÉÁç®Ëá™ËôïÁêÜÊåáÂÆöÁöÑ‰ª£Á¢ºÔºåÂêåÊôÇÂú®ÊØèÂÄãÁæ§ÁµÑÂÖß‰ΩøÁî®Â∑≤Â≠∏ÁøíÁöÑË∑ØÁî±Ôºå‰ª•Á∂≠ÊåÅË™ûÁæ©ÈÅ©ÊáâÊÄß„ÄÇÊàëÂÄëÁöÑÂØ¶Ë≠âÁµêÊûúÈ°ØÁ§∫ÔºåÈÄèÈÅéÈÄôÁ®ÆÊ®°ÊÖãÁâπÂÆöÂèÉÊï∏ÈÖçÁΩÆÔºåÂèØÂ§ßÂπÖÊèêÂçáÈ†êË®ìÁ∑¥ÊïàÁéá„ÄÇÂú® 1 ÂÖÜÂÄã‰ª£Á¢ºÁöÑË®ìÁ∑¥È†êÁÆó‰∏ãÔºåMoMa 1.4B Ê®°ÂûãÈÖçÂÇô 4 ÂÄãÊñáÂ≠óÂ∞àÂÆ∂Âíå 4 ÂÄãÂΩ±ÂÉèÂ∞àÂÆ∂ÔºåÂèØÁØÄÁúÅ‰ª§‰∫∫È©öË±îÁöÑ FLOPÔºöÊï¥È´îËÄåË®ÄÁÇ∫ 3.7 ÂÄçÔºåÊñáÂ≠óËôïÁêÜÁÇ∫ 2.6 ÂÄçÔºåÂΩ±ÂÉèËôïÁêÜÁÇ∫ 5.2 ÂÄçÔºåÈÄôÊòØ‰ª•È†êË®ìÁ∑¥ÊêçÂ§±Ê∏¨ÈáèÔºå‰∏¶ËàáÈÅãÁÆóÁ≠âÊïàÁöÑÂØÜÈõÜÂü∫Ê∫ñÁ∑öÁõ∏ÊØîËºÉ„ÄÇÈÄôÂÑ™ÊñºÊ®ôÊ∫ñÁöÑÂ∞àÂÆ∂ÈÅ∏Êìá MoEÔºåÂæåËÄÖÈÖçÂÇô 8 ÂÄãÊ∑∑ÂêàÊ®°ÊÖãÂ∞àÂÆ∂ÔºåÂèØÁØÄÁúÅÊï¥È´î FLOP 3 ÂÄçÔºàÊñáÂ≠óÁÇ∫ 3 ÂÄçÔºåÂΩ±ÂÉèÁÇ∫ 2.8 ÂÄçÔºâ„ÄÇÂ∞á MoMa ËàáÊ∑∑ÂêàÊ∑±Â∫¶ (MoD) ÁµêÂêàÔºåÂèØÈÄ≤‰∏ÄÊ≠•Â∞áÈ†êË®ìÁ∑¥ FLOP ÁØÄÁúÅÊèêÂçáËá≥Êï¥È´î 4.2 ÂÄçÔºàÊñáÂ≠óÔºö3.4 ÂÄçÔºåÂΩ±ÂÉèÔºö5.3 ÂÄçÔºâÔºåÂÑòÁÆ°ÈÄôÁ®ÆÁµÑÂêàÊúÉÂõ†Ë∑ØÁî±Âô®Á≤æÁ¢∫Â∫¶ÊïèÊÑüÂ∫¶Â¢ûÂä†ËÄåÊêçÂÆ≥Âõ†ÊûúÊé®ÁêÜÁöÑÊïàËÉΩ„ÄÇÈÄô‰∫õÁµêÊûúË≠âÊòé‰∫Ü MoMa Âú®ÊèêÂçáÊ∑∑ÂêàÊ®°ÊÖã„ÄÅÊó©ÊúüËûçÂêàË™ûË®ÄÊ®°ÂûãÈ†êË®ìÁ∑¥ÊïàÁéáÊñπÈù¢ÁöÑÊΩõÂäõÔºåÁÇ∫Êõ¥ÂÖ∑Ë≥áÊ∫êÊïàÁéá‰∏îÂäüËÉΩÂº∑Â§ßÁöÑÂ§öÊ®°ÊÖã AI Á≥ªÁµ±Èã™Ë∑Ø„ÄÇ</paragraph>

##### **HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection**
2407.21742v1 by Junwei He, Qianqian Xu, Yangbangyan Jiang, Zitai Wang, Yuchen Sun, Qingming Huang

With the progressive advancements in deep graph learning, out-of-distribution
(OOD) detection for graph data has emerged as a critical challenge. While the
efficacy of auxiliary datasets in enhancing OOD detection has been extensively
studied for image and text data, such approaches have not yet been explored for
graph data. Unlike Euclidean data, graph data exhibits greater diversity but
lower robustness to perturbations, complicating the integration of outliers. To
tackle these challenges, we propose the introduction of \textbf{H}ybrid
External and Internal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE)
to improve graph OOD detection performance. Our framework involves using
realistic external graph data from various domains and synthesizing internal
outliers within ID subgroups to address the poor robustness and presence of OOD
samples within the ID class. Furthermore, we develop a boundary-aware OE loss
that adaptively assigns weights to outliers, maximizing the use of high-quality
OOD samples while minimizing the impact of low-quality ones. Our proposed HGOE
framework is model-agnostic and designed to enhance the effectiveness of
existing graph OOD detection models. Experimental results demonstrate that our
HGOE framework can significantly improve the performance of existing OOD
detection models across all 8 real datasets.

ÊëòË¶ÅÔºöÈö®ËëóÊ∑±Â∫¶ÂúñÂΩ¢Â≠∏ÁøíÁöÑÈÄ≤Ê≠•ÔºåÂúñÂΩ¢Ë≥áÊñôÁöÑÂàÜÂ∏ÉÂ§ñ (OOD) ÂÅµÊ∏¨Â∑≤ÊàêÁÇ∫‰∏ÄÈ†ÖÈóúÈçµÊåëÊà∞„ÄÇÈõñÁÑ∂ËºîÂä©Ë≥áÊñôÈõÜÂú®Â¢ûÂº∑ÂΩ±ÂÉèÂíåÊñáÂ≠óË≥áÊñôÁöÑ OOD ÂÅµÊ∏¨ÊñπÈù¢Â∑≤Áç≤ÂæóÂª£Ê≥õÁ†îÁ©∂Ôºå‰ΩÜÊ≠§È°ûÊñπÊ≥ïÂ∞öÊú™Êé¢Á¥¢ÂúñÂΩ¢Ë≥áÊñô„ÄÇËàáÊ≠êÂπæÈáåÂæóË≥áÊñô‰∏çÂêåÔºåÂúñÂΩ¢Ë≥áÊñôÂ±ïÁèæÂá∫Êõ¥Â§ßÁöÑÂ§öÊ®£ÊÄßÔºå‰ΩÜÂ∞çÊìæÂãïÁöÑÈ≠ØÊ£íÊÄßËºÉ‰ΩéÔºå‰ΩøÈõ¢Áæ§ÂÄºÁöÑÊï¥ÂêàË§áÈõúÂåñ„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ÂºïÂÖ•\textbf{H}ybrid\textbf{E}xternal and \textbf{I}nternal \textbf{G}raph \textbf{O}utlier \textbf{E}xposure (HGOE) ‰ª•ÊèêÂçáÂúñÂΩ¢ OOD ÂÅµÊ∏¨ÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊ∂âÂèä‰ΩøÁî®‰æÜËá™‰∏çÂêåÈ†òÂüüÁöÑÁúüÂØ¶Â§ñÈÉ®ÂúñÂΩ¢Ë≥áÊñôÔºå‰∏¶Âú® ID Â≠êÁæ§‰∏≠ÂêàÊàêÂÖßÈÉ®Èõ¢Áæ§ÂÄºÔºå‰ª•Ëß£Ê±∫ ID È°ûÂà•‰∏≠ OOD Ê®£Êú¨ÁöÑÈ≠ØÊ£íÊÄßÂ∑ÆÂíåÂ≠òÂú®ÊÄßÂïèÈ°å„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆÈÇäÁïåÊÑüÁü• OE ÊêçÂ§±ÔºåÂèØÈÅ©ÊáâÊÄßÂú∞Â∞áÊ¨äÈáçÂàÜÈÖçÁµ¶Èõ¢Áæ§ÂÄºÔºåÊúÄÂ§ßÂåñÈ´òÂìÅË≥™ OOD Ê®£Êú¨ÁöÑ‰ΩøÁî®ÔºåÂêåÊôÇÊúÄÂ∞èÂåñ‰ΩéÂìÅË≥™Ê®£Êú¨ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑ HGOE Êû∂ÊßãËàáÊ®°ÂûãÁÑ°ÈóúÔºåÊó®Âú®ÊèêÂçáÁèæÊúâÂúñÂΩ¢ OOD ÂÅµÊ∏¨Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåÊàëÂÄëÁöÑ HGOE Êû∂ÊßãÂèØ‰ª•È°ØËëóÊèêÂçáÁèæÊúâ OOD ÂÅµÊ∏¨Ê®°ÂûãÂú®ÊâÄÊúâ 8 ÂÄãÁúüÂØ¶Ë≥áÊñôÈõÜ‰∏≠ÁöÑÊïàËÉΩ„ÄÇ

##### **Contrastive Factor Analysis**
2407.21740v2 by Zhibin Duan, Tiansheng Wen, Yifei Wang, Chen Zhu, Bo Chen, Mingyuan Zhou

Factor analysis, often regarded as a Bayesian variant of matrix
factorization, offers superior capabilities in capturing uncertainty, modeling
complex dependencies, and ensuring robustness. As the deep learning era
arrives, factor analysis is receiving less and less attention due to their
limited expressive ability. On the contrary, contrastive learning has emerged
as a potent technique with demonstrated efficacy in unsupervised
representational learning. While the two methods are different paradigms,
recent theoretical analysis has revealed the mathematical equivalence between
contrastive learning and matrix factorization, providing a potential
possibility for factor analysis combined with contrastive learning. Motivated
by the interconnectedness of contrastive learning, matrix factorization, and
factor analysis, this paper introduces a novel Contrastive Factor Analysis
framework, aiming to leverage factor analysis's advantageous properties within
the realm of contrastive learning. To further leverage the interpretability
properties of non-negative factor analysis, which can learn disentangled
representations, contrastive factor analysis is extended to a non-negative
version. Finally, extensive experimental validation showcases the efficacy of
the proposed contrastive (non-negative) factor analysis methodology across
multiple key properties, including expressiveness, robustness,
interpretability, and accurate uncertainty estimation.

ÊëòË¶ÅÔºöÂõ†Â≠êÂàÜÊûêÈÄöÂ∏∏Ë¢´ËßÜ‰∏∫Áü©ÈòµÂàÜËß£ÁöÑË¥ùÂè∂ÊñØÂèò‰ΩìÔºåÂú®ÊçïËé∑‰∏çÁ°ÆÂÆöÊÄß„ÄÅÂª∫Ê®°Â§çÊùÇ‰æùËµñÂÖ≥Á≥ªÂíåÁ°Æ‰øùÁ®≥ÂÅ•ÊÄßÊñπÈù¢ÂÖ∑ÊúâÂçìË∂äÁöÑËÉΩÂäõ„ÄÇÈöèÁùÄÊ∑±Â∫¶Â≠¶‰π†Êó∂‰ª£ÁöÑÂà∞Êù•ÔºåÂõ†Â≠êÂàÜÊûêÁî±‰∫éÂÖ∂Ë°®ËææËÉΩÂäõÊúâÈôêËÄåÂèóÂà∞Ë∂äÊù•Ë∂äÂ∞ëÁöÑÂÖ≥Ê≥®„ÄÇÁõ∏ÂèçÔºåÂØπÊØîÂ≠¶‰π†Â∑≤ÁªèÊàê‰∏∫‰∏ÄÁßçÊúâÊïàÁöÑÊäÄÊúØÔºåÂú®Êó†ÁõëÁù£Ë°®ÂæÅÂ≠¶‰π†‰∏≠ÊòæÁ§∫Âá∫ÂäüÊïà„ÄÇËôΩÁÑ∂Ëøô‰∏§ÁßçÊñπÊ≥ïÊòØ‰∏çÂêåÁöÑËåÉÂºèÔºå‰ΩÜÊúÄËøëÁöÑÁêÜËÆ∫ÂàÜÊûêÊè≠Á§∫‰∫ÜÂØπÊØîÂ≠¶‰π†ÂíåÁü©ÈòµÂàÜËß£‰πãÈó¥ÁöÑÊï∞Â≠¶Á≠â‰ª∑ÊÄßÔºå‰∏∫Âõ†Â≠êÂàÜÊûêÁªìÂêàÂØπÊØîÂ≠¶‰π†Êèê‰æõ‰∫ÜÊΩúÂú®ÁöÑÂèØËÉΩÊÄß„ÄÇÂèóÂØπÊØîÂ≠¶‰π†„ÄÅÁü©ÈòµÂàÜËß£ÂíåÂõ†Â≠êÂàÜÊûêÁöÑÁõ∏‰∫íËÅîÁ≥ªÁöÑÂêØÂèëÔºåÊú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂØπÊØîÂõ†Â≠êÂàÜÊûêÊ°ÜÊû∂ÔºåÊó®Âú®Âà©Áî®Âõ†Â≠êÂàÜÊûêÂú®ÂØπÊØîÂ≠¶‰π†È¢ÜÂüü‰∏≠ÁöÑ‰ºòÂäøÁâπÊÄß„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Âà©Áî®ÈùûË¥üÂõ†Â≠êÂàÜÊûêÁöÑÂèØËß£ÈáäÊÄßÔºåÂÆÉÂèØ‰ª•Â≠¶‰π†Ëß£ËÄ¶Ë°®ÂæÅÔºåÂ∞ÜÂØπÊØîÂõ†Â≠êÂàÜÊûêÊâ©Â±ïÂà∞ÈùûË¥üÁâàÊú¨„ÄÇÊúÄÂêéÔºåÂπøÊ≥õÁöÑÂÆûÈ™åÈ™åËØÅÂ±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÂØπÊØîÔºàÈùûË¥üÔºâÂõ†Â≠êÂàÜÊûêÊñπÊ≥ïÂú®Â§ö‰∏™ÂÖ≥ÈîÆÂ±ûÊÄßÔºàÂåÖÊã¨Ë°®ËææÊÄß„ÄÅÈ≤ÅÊ£íÊÄß„ÄÅÂèØËß£ÈáäÊÄßÂíåÂáÜÁ°ÆÁöÑ‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°Ôºâ‰∏äÁöÑÊúâÊïàÊÄß„ÄÇ

##### **A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation**
2407.21739v1 by Mothilal Asokan, Joseph Geo Benjamin, Mohammad Yaqub, Karthik Nandakumar

Adapting foundation models for medical image analysis requires finetuning
them on a considerable amount of data because of extreme distribution shifts
between natural (source) data used for pretraining and medical (target) data.
However, collecting task-specific medical data for such finetuning at a central
location raises many privacy concerns. Although Federated learning (FL)
provides an effective means for training on private decentralized data,
communication costs in federating large foundation models can quickly become a
significant bottleneck, impacting the solution's scalability. In this work, we
address this problem of efficient communication while ensuring effective
learning in FL by combining the strengths of Parameter-Efficient Fine-tuning
(PEFT) with FL. Specifically, we study plug-and-play Low-Rank Adapters (LoRA)
in a federated manner to adapt the Segment Anything Model (SAM) for 3D medical
image segmentation. Unlike prior works that utilize LoRA and finetune the
entire decoder, we critically analyze the contribution of each granular
component of SAM on finetuning performance. Thus, we identify specific layers
to be federated that are very efficient in terms of communication cost while
producing on-par accuracy. Our experiments show that retaining the parameters
of the SAM model (including most of the decoder) in their original state during
adaptation is beneficial because fine-tuning on small datasets tends to distort
the inherent capabilities of the underlying foundation model. On Fed-KiTS, our
approach decreases communication cost (~48x) compared to full fine-tuning while
increasing performance (~6% Dice score) in 3D segmentation tasks. Our approach
performs similar to SAMed while achieving ~2.8x reduction in communication and
parameters to be finetuned. We further validate our approach with experiments
on Fed-IXI and Prostate MRI datasets.

ÊëòË¶ÅÔºö<paragraph>Áî±ÊñºÈ†êË®ìÁ∑¥ÊâÄÁî®ÁöÑËá™ÁÑ∂Ôºà‰æÜÊ∫êÔºâË≥áÊñôÂíåÈÜ´ÁôÇÔºàÁõÆÊ®ôÔºâË≥áÊñô‰πãÈñìÁöÑÊ•µÁ´ØÂàÜ‰ΩàËΩâÁßªÔºåÂõ†Ê≠§Â∞áÂü∫Á§éÊ®°ÂûãË™øÊï¥Áî®ÊñºÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÊûêÈúÄË¶ÅÂú®Â§ßÈáèË≥áÊñô‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åÂæÆË™ø„ÄÇ
ÁÑ∂ËÄåÔºåÂú®‰∏≠ÂøÉ‰ΩçÁΩÆÊî∂ÈõÜÊ≠§È°ûÂæÆË™øÁöÑÁâπÂÆö‰ªªÂãôÈÜ´ÁôÇË≥áÊñôÊúÉÂºïÁôºË®±Â§öÈö±ÁßÅÂïèÈ°å„ÄÇÂÑòÁÆ°ËÅØÂêàÂ≠∏Áøí (FL) Êèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂú®ÁßÅÊúâÂàÜÊï£ÂºèË≥áÊñô‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÁöÑÊúâÊïàÊñπÊ≥ïÔºå‰ΩÜÂú®ËÅØÂêàÂ§ßÂûãÂü∫Á§éÊ®°ÂûãÊôÇÔºåÈÄöË®äÊàêÊú¨ÂèØËÉΩÊúÉËøÖÈÄüÊàêÁÇ∫‰∏ÄÂÄãÈáçÂ§ßÁì∂È†∏ÔºåÂΩ±ÈüøËß£Ê±∫ÊñπÊ°àÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄöÈÅéÁµêÂêàÂèÉÊï∏È´òÊïàÂæÆË™ø (PEFT) Âíå FL ÁöÑÂÑ™Âã¢ÔºåËß£Ê±∫‰∫ÜÂú®Á¢∫‰øù FL ‰∏≠ÊúâÊïàÂ≠∏ÁøíÁöÑÂêåÊôÇÈÄ≤Ë°åÈ´òÊïàÈÄöË®äÁöÑÂïèÈ°å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄë‰ª•ËÅØÂêàÁöÑÊñπÂºèÁ†îÁ©∂Âç≥ÊèíÂç≥Áî®‰ΩéÁß©ÈÅ©ÈÖçÂô® (LoRA)Ôºå‰ª•Ë™øÊï¥ÂçÄÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) ‰ª•ÈÄ≤Ë°å 3D ÈÜ´Â≠∏ÂΩ±ÂÉèÂàÜÂâ≤„ÄÇËàáÂà©Áî® LoRA ÂíåÂæÆË™øÊï¥ÂÄãËß£Á¢ºÂô®ÁöÑÂÖàÂâçÂ∑•‰Ωú‰∏çÂêåÔºåÊàëÂÄëÊâπÂà§ÊÄßÂú∞ÂàÜÊûê‰∫Ü SAM ÁöÑÊØèÂÄãÁ≤íÁãÄÁµÑÊàêÈÉ®ÂàÜÂ∞çÂæÆË™øÊïàËÉΩÁöÑË≤¢Áçª„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁ¢∫ÂÆö‰∫ÜÂú®ÈÄöË®äÊàêÊú¨ÊñπÈù¢ÈùûÂ∏∏È´òÊïàÁöÑÁâπÂÆöÂ±§ÔºåÂêåÊôÇÁî¢Áîü‰∫ÜÂêåÁ≠âÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåÂú®Ë™øÊï¥ÈÅéÁ®ã‰∏≠Â∞á SAM Ê®°ÂûãÁöÑÂèÉÊï∏ÔºàÂåÖÊã¨Â§ßÈÉ®ÂàÜËß£Á¢ºÂô®Ôºâ‰øùÁïôÂú®ÂÖ∂ÂéüÂßãÁãÄÊÖãÊòØÊúâÁõäÁöÑÔºåÂõ†ÁÇ∫Âú®Â∞èÂûãË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂæÆË™øÂæÄÂæÄÊúÉÊâ≠Êõ≤Âü∫Á§éÊ®°ÂûãÁöÑÂÖßÂú®ËÉΩÂäõ„ÄÇÂú® Fed-KiTS ‰∏äÔºåËàáÂÆåÂÖ®ÂæÆË™øÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÈôç‰Ωé‰∫ÜÈÄöË®äÊàêÊú¨ÔºàÁ¥Ñ 48 ÂÄçÔºâÔºåÂêåÊôÇÊèêÈ´ò‰∫Ü 3D ÂàÜÂâ≤‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩÔºàÁ¥Ñ 6% ÁöÑÈ™∞Â≠êÂàÜÊï∏Ôºâ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïËàá SAMed È°û‰ººÔºåÂêåÊôÇÂ∞áÈÄöË®äÂíåÂæÖÂæÆË™øÂèÉÊï∏Ê∏õÂ∞ë‰∫ÜÁ¥Ñ 2.8 ÂÄç„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄöÈÅéÂú® Fed-IXI Âíå Prostate MRI Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂØ¶È©óÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÅöÊ≥ï„ÄÇ</paragraph>

##### **Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos**
2407.21738v1 by Joseph Geo Benjamin, Mothilal Asokan, Amna Alhosani, Hussain Alasmawi, Werner Gerhard Diehl, Leanne Bricker, Karthik Nandakumar, Mohammad Yaqub

Self-supervised learning (SSL) methods are popular since they can address
situations with limited annotated data by directly utilising the underlying
data distribution. However, the adoption of such methods is not explored enough
in ultrasound (US) imaging, especially for fetal assessment. We investigate the
potential of dual-encoder SSL in utilizing unlabelled US video data to improve
the performance of challenging downstream Standard Fetal Cardiac Planes (SFCP)
classification using limited labelled 2D US images. We study 7 SSL approaches
based on reconstruction, contrastive loss, distillation, and information theory
and evaluate them extensively on a large private US dataset. Our observations
and findings are consolidated from more than 500 downstream training
experiments under different settings. Our primary observation shows that for
SSL training, the variance of the dataset is more crucial than its size because
it allows the model to learn generalisable representations, which improve the
performance of downstream tasks. Overall, the BarlowTwins method shows robust
performance, irrespective of the training settings and data variations, when
used as an initialisation for downstream tasks. Notably, full fine-tuning with
1% of labelled data outperforms ImageNet initialisation by 12% in F1-score and
outperforms other SSL initialisations by at least 4% in F1-score, thus making
it a promising candidate for transfer learning from US video to image data.

ÊëòË¶ÅÔºöËá™ÁõëÁù£Â≠¶‰π† (SSL) ÊñπÊ≥ïÂæàÂèóÊ¨¢ËøéÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÂèØ‰ª•ÈÄöËøáÁõ¥Êé•Âà©Áî®Â∫ïÂ±ÇÊï∞ÊçÆÂàÜÂ∏ÉÊù•Ëß£ÂÜ≥Â∏¶Ê≥®ÈáäÊï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ„ÄÇÁÑ∂ËÄåÔºåÂú®Ë∂ÖÂ£∞ (US) ÊàêÂÉè‰∏≠ÔºåÁâπÂà´ÊòØÂØπ‰∫éËÉéÂÑøËØÑ‰º∞ÔºåÂØπËøôÁßçÊñπÊ≥ïÁöÑÈááÁî®Â∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫ÜÂèåÁºñÁ†ÅÂô® SSL Âú®Âà©Áî®Êú™Ê†áËÆ∞ÁöÑ US ËßÜÈ¢ëÊï∞ÊçÆ‰ª•‰ΩøÁî®ÊúâÈôêÊ†áËÆ∞ÁöÑ 2D US ÂõæÂÉèÊèêÈ´òÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑ‰∏ãÊ∏∏Ê†áÂáÜËÉéÂÑøÂøÉËÑèÂπ≥Èù¢ (SFCP) ÂàÜÁ±ªÊÄßËÉΩÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫Ü 7 ÁßçÂü∫‰∫éÈáçÂª∫„ÄÅÂØπÊØîÊçüÂ§±„ÄÅËí∏È¶èÂíå‰ø°ÊÅØËÆ∫ÁöÑ SSL ÊñπÊ≥ïÔºåÂπ∂Âú®‰∏Ä‰∏™Â§ßÂûãÁßÅÊúâ US Êï∞ÊçÆÈõÜ‰∏äÂØπÂÆÉ‰ª¨ËøõË°å‰∫ÜÂπøÊ≥õÁöÑËØÑ‰º∞„ÄÇÊàë‰ª¨ÁöÑËßÇÂØüÂíåÂèëÁé∞Êù•Ëá™ 500 Â§ö‰∏™‰∏çÂêåËÆæÁΩÆ‰∏ãÁöÑ‰∏ãÊ∏∏ËÆ≠ÁªÉÂÆûÈ™å„ÄÇÊàë‰ª¨ÁöÑ‰∏ªË¶ÅËßÇÂØüÁªìÊûúË°®ÊòéÔºåÂØπ‰∫é SSL ËÆ≠ÁªÉÔºåÊï∞ÊçÆÈõÜÁöÑÊñπÂ∑ÆÊØîÂÖ∂Â§ßÂ∞èÊõ¥ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÆÉÂÖÅËÆ∏Ê®°ÂûãÂ≠¶‰π†ÂèØÊé®ÂπøÁöÑË°®Á§∫Ôºå‰ªéËÄåÊèêÈ´ò‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåBarlowTwins ÊñπÊ≥ïË°®Áé∞Âá∫Á®≥ÂÅ•ÁöÑÊÄßËÉΩÔºåÊó†ËÆ∫ËÆ≠ÁªÉËÆæÁΩÆÂíåÊï∞ÊçÆÂèòÂåñÂ¶Ç‰ΩïÔºåÂΩìÁî®‰Ωú‰∏ãÊ∏∏‰ªªÂä°ÁöÑÂàùÂßãÂåñÊó∂„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºå‰ΩøÁî® 1% ÁöÑÊ†áËÆ∞Êï∞ÊçÆËøõË°åÂÆåÂÖ®ÂæÆË∞ÉÂú® F1 ÂàÜÊï∞‰∏ä‰ºò‰∫é ImageNet ÂàùÂßãÂåñ 12%ÔºåÂπ∂‰∏îÂú® F1 ÂàÜÊï∞‰∏ä‰ºò‰∫éÂÖ∂‰ªñ SSL ÂàùÂßãÂåñËá≥Â∞ë 4%ÔºåÂõ†Ê≠§‰ΩøÂÖ∂Êàê‰∏∫‰ªé US ËßÜÈ¢ëÂà∞ÂõæÂÉèÊï∞ÊçÆÁöÑËøÅÁßªÂ≠¶‰π†ÁöÑÊúâÂ∏åÊúõÁöÑÂÄôÈÄâËÄÖ„ÄÇ

##### **Open-Vocabulary Audio-Visual Semantic Segmentation**
2407.21721v1 by Ruohao Guo, Liao Qu, Dantong Niu, Yanyu Qi, Wenzhen Yue, Ji Shi, Bowei Xing, Xianghua Ying

Audio-visual semantic segmentation (AVSS) aims to segment and classify
sounding objects in videos with acoustic cues. However, most approaches operate
on the close-set assumption and only identify pre-defined categories from
training data, lacking the generalization ability to detect novel categories in
practical applications. In this paper, we introduce a new task: open-vocabulary
audio-visual semantic segmentation, extending AVSS task to open-world scenarios
beyond the annotated label space. This is a more challenging task that requires
recognizing all categories, even those that have never been seen nor heard
during training. Moreover, we propose the first open-vocabulary AVSS framework,
OV-AVSS, which mainly consists of two parts: 1) a universal sound source
localization module to perform audio-visual fusion and locate all potential
sounding objects and 2) an open-vocabulary classification module to predict
categories with the help of the prior knowledge from large-scale pre-trained
vision-language models. To properly evaluate the open-vocabulary AVSS, we split
zero-shot training and testing subsets based on the AVSBench-semantic
benchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong
segmentation and zero-shot generalization ability of our model on all
categories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base
categories and 29.14% mIoU on novel categories, exceeding the state-of-the-art
zero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%.
The code is available at https://github.com/ruohaoguo/ovavss.

ÊëòË¶ÅÔºö<paragraph>Èü≥Ë¶ñË™ûÁæ©ÂàÜÂâ≤ÔºàAVSSÔºâÊó®Âú®‰ΩøÁî®ËÅΩË¶∫Á∑öÁ¥¢Â∞çÂΩ±Áâá‰∏≠ÁöÑÁôºËÅ≤Áâ©È´îÈÄ≤Ë°åÂàÜÂâ≤ÂíåÂàÜÈ°û„ÄÇÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏ÊñπÊ≥ïÈÉΩÈÅã‰ΩúÂú®Â∞ÅÈñâÈõÜÂêàÂÅáË®≠‰∏äÔºå‰∏îÂÉÖÂæûË®ìÁ∑¥Ë≥áÊñô‰∏≠Ë≠òÂà•È†êÂÖàÂÆöÁæ©ÁöÑÈ°ûÂà•ÔºåÁº∫‰πèÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂÅµÊ∏¨Êñ∞È°ûÂà•ÁöÑÊ¶ÇÂåñËÉΩÂäõ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÊñ∞‰ªªÂãôÔºöÈñãÊîæË©ûÂΩôÈü≥Ë¶ñË™ûÁæ©ÂàÜÂâ≤ÔºåÂ∞á AVSS ‰ªªÂãôÊì¥Â±ïÂà∞Ê®ôË®ªÊ®ôÁ±§Á©∫Èñì‰πãÂ§ñÁöÑÈñãÊîæ‰∏ñÁïåÂ†¥ÊôØ„ÄÇÈÄôÊòØ‰∏ÄÈ†ÖÊõ¥ÂÖ∑ÊåëÊà∞ÊÄßÁöÑ‰ªªÂãôÔºåÈúÄË¶ÅËæ®Ë≠òÊâÄÊúâÈ°ûÂà•ÔºåÂç≥‰ΩøÊòØÈÇ£‰∫õÂú®Ë®ìÁ∑¥ÊúüÈñìÂæûÊú™Ë¶ãÈÅéÊàñËÅΩÈÅéÁöÑÈ°ûÂà•„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁ¨¨‰∏ÄÂÄãÈñãÊîæË©ûÂΩô AVSS Êû∂Êßã OV-AVSSÔºåÂÆÉ‰∏ªË¶ÅÂåÖÂê´ÂÖ©ÂÄãÈÉ®ÂàÜÔºö1) ‰∏ÄÂÄãÈÄöÁî®Èü≥Ê∫êÂÆö‰ΩçÊ®°ÁµÑÔºåÁî®ÊñºÂü∑Ë°åÈü≥Ë¶ñËûçÂêà‰∏¶ÂÆö‰ΩçÊâÄÊúâÊΩõÂú®ÁôºËÅ≤Áâ©È´îÔºå‰ª•Âèä 2) ‰∏ÄÂÄãÈñãÊîæË©ûÂΩôÂàÜÈ°ûÊ®°ÁµÑÔºåÂú®‰æÜËá™Â§ßÂûãÈ†êË®ìÁ∑¥Ë¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÁöÑÂÖàÈ©óÁü•Ë≠òÁöÑÂπ´Âä©‰∏ãÈ†êÊ∏¨È°ûÂà•„ÄÇÁÇ∫‰∫ÜÈÅ©Áï∂Âú∞Ë©ï‰º∞ÈñãÊîæË©ûÂΩô AVSSÔºåÊàëÂÄëÊ†πÊìö AVSBench-semantic Âü∫Ê∫ñÔºàÂç≥ AVSBench-OVÔºâÊãÜÂàÜ‰∫ÜÈõ∂Ê¨°Â≠∏ÁøíË®ìÁ∑¥ÂíåÊ∏¨Ë©¶Â≠êÈõÜ„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊâÄÊúâÈ°ûÂà•‰∏äÂÖ∑ÊúâÂº∑Â§ßÁöÑÂàÜÂâ≤ÂíåÈõ∂Ê¨°Â≠∏ÁøíÊ¶ÇÂåñËÉΩÂäõ„ÄÇÂú® AVSBench-OV Ë≥áÊñôÈõÜ‰∏äÔºåOV-AVSS Âú®Âü∫Á§éÈ°ûÂà•‰∏äÈÅîÂà∞ 55.43% mIoUÔºåÂú®Êñ∞Á©éÈ°ûÂà•‰∏äÈÅîÂà∞ 29.14% mIoUÔºåÊØîÊúÄÂÖàÈÄ≤ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÊñπÊ≥ïÈ´òÂá∫ 41.88%/20.61%ÔºåÊØîÈñãÊîæË©ûÂΩôÊñπÊ≥ïÈ´òÂá∫ 10.2%/11.6%„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/ruohaoguo/ovavss ÂèñÂæó„ÄÇ</paragraph>

##### **Social Learning through Interactions with Other Agents: A Survey**
2407.21713v1 by Dylan hillier, Cheston Tan, Jing Jiang

Social learning plays an important role in the development of human
intelligence. As children, we imitate our parents' speech patterns until we are
able to produce sounds; we learn from them praising us and scolding us; and as
adults, we learn by working with others. In this work, we survey the degree to
which this paradigm -- social learning -- has been mirrored in machine
learning. In particular, since learning socially requires interacting with
others, we are interested in how embodied agents can and have utilised these
techniques. This is especially in light of the degree to which recent advances
in natural language processing (NLP) enable us to perform new forms of social
learning. We look at how behavioural cloning and next-token prediction mirror
human imitation, how learning from human feedback mirrors human education, and
how we can go further to enable fully communicative agents that learn from each
other. We find that while individual social learning techniques have been used
successfully, there has been little unifying work showing how to bring them
together into socially embodied agents.

ÊëòË¶ÅÔºöÁ§æÊúÉÂ≠∏ÁøíÂú®‰∫∫È°ûÊô∫ËÉΩÁôºÂ±ï‰∏≠ÊâÆÊºîËëóÈáçË¶ÅÁöÑËßíËâ≤„ÄÇË∫´ÁÇ∫Â≠©Á´•ÊôÇÔºåÊàëÂÄëÊ®°‰ªøÁà∂ÊØçÁöÑË™™Ë©±Ê®°ÂºèÔºåÁõ¥Âà∞ÊàëÂÄëËÉΩÂ§†ÁôºÂá∫ËÅ≤Èü≥ÔºõÊàëÂÄëÂæû‰ªñÂÄëÁöÑËÆöÁæéËàáË≤¨ÁΩµ‰∏≠Â≠∏ÁøíÔºõËÄåË∫´ÁÇ∫ÊàêÂπ¥‰∫∫ÊôÇÔºåÊàëÂÄëÂæûËàá‰ªñ‰∫∫Âêà‰Ωú‰∏≠Â≠∏Áøí„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË™øÊü•‰∫ÜÈÄôÂÄãÂÖ∏ÁØÑÔºàÁ§æÊúÉÂ≠∏ÁøíÔºâÂú®Ê©üÂô®Â≠∏Áøí‰∏≠Ë¢´ÂèçÊò†ÁöÑÁ®ãÂ∫¶„ÄÇÁâπÂà•Âú∞ÔºåÁî±ÊñºÁ§æÊúÉÂ≠∏ÁøíÈúÄË¶ÅËàá‰ªñ‰∫∫‰∫íÂãïÔºåÊàëÂÄëÊúâËààË∂£‰∫ÜËß£ÂÖ∑Ë∫´‰ª£ÁêÜÂ¶Ç‰ΩïËÉΩÂ§†‰∏¶Â∑≤Á∂ìÂà©Áî®ÈÄô‰∫õÊäÄË°ì„ÄÇÈÄôÁâπÂà•ÊòØÈëëÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÁöÑÊúÄÊñ∞ÈÄ≤Â±ï‰ΩøÊàëÂÄëËÉΩÂ§†Âü∑Ë°åÊñ∞ÁöÑÁ§æÊúÉÂ≠∏ÁøíÂΩ¢Âºè„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜË°åÁÇ∫Ë§áË£ΩÂíå‰∏ã‰∏ÄÂÄã‰ª£Âπ£È†êÊ∏¨Â¶Ç‰ΩïÂèçÊò†‰∫∫È°ûÊ®°‰ªø„ÄÅÂæû‰∫∫È°ûÂõûÈ•ã‰∏≠Â≠∏ÁøíÂ¶Ç‰ΩïÂèçÊò†‰∫∫È°ûÊïôËÇ≤Ôºå‰ª•ÂèäÊàëÂÄëÂ¶Ç‰ΩïÈÄ≤‰∏ÄÊ≠•ÂØ¶ÁèæËÉΩÂ§†ÂΩºÊ≠§Â≠∏ÁøíÁöÑÂÆåÂÖ®Ê∫ùÈÄö‰ª£ÁêÜ„ÄÇÊàëÂÄëÁôºÁèæÔºåÂÑòÁÆ°ÂÄãÂà•ÁöÑÁ§æÊúÉÂ≠∏ÁøíÊäÄË°ìÂ∑≤Ë¢´ÊàêÂäü‰ΩøÁî®Ôºå‰ΩÜÈÆÆÂ∞ëÊúâÁµ±‰∏ÄÁöÑÂ∑•‰ΩúÂ±ïÁ§∫Â¶Ç‰ΩïÂ∞áÂÆÉÂÄëÂ∏∂ÂÖ•Á§æÊúÉÂÖ∑Ë∫´‰ª£ÁêÜ‰∏≠„ÄÇ

##### **Adaptive Retrieval-Augmented Generation for Conversational Systems**
2407.21712v1 by Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz

Despite the success of integrating large language models into the development
of conversational systems, many studies have shown the effectiveness of
retrieving and augmenting external knowledge for informative responses. Hence,
many existing studies commonly assume the always need for Retrieval Augmented
Generation (RAG) in a conversational system without explicit control. This
raises a research question about such a necessity. In this study, we propose to
investigate the need for each turn of system response to be augmented with
external knowledge. In particular, by leveraging human judgements on the binary
choice of adaptive augmentation, we develop RAGate, a gating model, which
models conversation context and relevant inputs to predict if a conversational
system requires RAG for improved responses. We conduct extensive experiments on
devising and applying RAGate to conversational models and well-rounded analyses
of different conversational scenarios. Our experimental results and analysis
indicate the effective application of RAGate in RAG-based conversational
systems in identifying system responses for appropriate RAG with high-quality
responses and a high generation confidence. This study also identifies the
correlation between the generation's confidence level and the relevance of the
augmented knowledge.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊï¥ÂêàÂà∞Â∞çË©±Á≥ªÁµ±ÈñãÁôº‰∏≠Áç≤ÂæóÊàêÂäüÔºåË®±Â§öÁ†îÁ©∂Â∑≤È°ØÁ§∫Êì∑ÂèñÂíåÊì¥ÂÖÖÂ§ñÈÉ®Áü•Ë≠òÂ∞çÊñºÊèê‰æõË≥áË®äÊÄßÂõûÊáâÁöÑÊúâÊïàÊÄß„ÄÇÂõ†Ê≠§ÔºåË®±Â§öÁèæÊúâÁ†îÁ©∂ÈÄöÂ∏∏ÂÅáË®≠Â∞çË©±Á≥ªÁµ±‰∏≠Á∏ΩÊòØÈúÄË¶ÅÊ™¢Á¥¢Êì¥ÂÖÖÁîüÊàê (RAG)ÔºåËÄåÊ≤íÊúâÊòéÁ¢∫ÁöÑÊéßÂà∂„ÄÇÈÄôÂºïÁôº‰∫ÜÈóúÊñºÈÄôÁ®ÆÂøÖË¶ÅÊÄßÁöÑÁ†îÁ©∂ÂïèÈ°å„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂª∫Ë≠∞Êé¢Ë®éÁ≥ªÁµ±ÂõûÊáâÁöÑÊØèÂÄãËΩâÊäòÊòØÂê¶ÈÉΩÈúÄË¶ÅÊì¥ÂÖÖÂ§ñÈÉ®Áü•Ë≠ò„ÄÇÁâπÂà•ÊòØÔºåÈÄèÈÅéÂà©Áî®‰∫∫È°ûÂ∞çÈÅ©ÊáâÊÄßÊì¥ÂÖÖÁöÑ‰∫åÂÖÉÈÅ∏ÊìáÁöÑÂà§Êñ∑ÔºåÊàëÂÄëÈñãÁôº‰∫Ü RAGateÔºå‰∏ÄÁ®ÆÈñòÊéßÊ®°ÂûãÔºåË©≤Ê®°ÂûãÂ∞çË©±Ë™ûÂ¢ÉÂíåÁõ∏ÈóúËº∏ÂÖ•ÈÄ≤Ë°åÂª∫Ê®°Ôºå‰ª•È†êÊ∏¨Â∞çË©±Á≥ªÁµ±ÊòØÂê¶ÈúÄË¶Å RAG ‰æÜÊîπÂñÑÂõûÊáâ„ÄÇÊàëÂÄëÂ∞çË®≠Ë®àÂíåÂ∞á RAGate ÊáâÁî®ÊñºÂ∞çË©±Ê®°ÂûãÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰∏¶Â∞ç‰∏çÂêåÁöÑÂ∞çË©±Â†¥ÊôØÈÄ≤Ë°åÂÖ®Èù¢ÁöÑÂàÜÊûê„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÂíåÂàÜÊûêË°®Êòé RAGate Âú®Âü∫Êñº RAG ÁöÑÂ∞çË©±Á≥ªÁµ±‰∏≠ÊúâÊïàÊáâÁî®ÊñºË≠òÂà•Á≥ªÁµ±ÂõûÊáâÔºå‰ª•ÈÅ©Áï∂ÁöÑ RAG Áç≤ÂæóÈ´òÂìÅË≥™ÁöÑÂõûÊáâÂíåÈ´òÁîüÊàê‰ø°ÂøÉ„ÄÇÊú¨Á†îÁ©∂ÈÇÑÁ¢∫ÂÆö‰∫ÜÁîüÊàê‰ø°ÂøÉÊ∞¥Ê∫ñËàáÊì¥ÂÖÖÁü•Ë≠òÁõ∏ÈóúÊÄß‰πãÈñìÁöÑÈóúËÅØÊÄß„ÄÇ

##### **CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature**
2407.21708v1 by Stefan Langer, Fabian Neuhaus, Andreas N√ºrnberger

Ontologies are formal representations of knowledge in specific domains that
provide a structured framework for organizing and understanding complex
information. Creating ontologies, however, is a complex and time-consuming
endeavor. ChEBI is a well-known ontology in the field of chemistry, which
provides a comprehensive resource for defining chemical entities and their
properties. However, it covers only a small fraction of the rapidly growing
knowledge in chemistry and does not provide references to the scientific
literature. To address this, we propose a methodology that involves augmenting
existing annotated text corpora with knowledge from Chebi and fine-tuning a
large language model (LLM) to recognize chemical entities and their roles in
scientific text. Our experiments demonstrate the effectiveness of our approach.
By combining ontological knowledge and the language understanding capabilities
of LLMs, we achieve high precision and recall rates in identifying both the
chemical entities and roles in scientific literature. Furthermore, we extract
them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a
knowledge graph (KG) of chemical entities and roles (CEAR), which provides
complementary information to ChEBI, and can help to extend it.

ÊëòË¶ÅÔºöÊú¨‰ΩìÊòØÁâπÂÆöÈ†òÂüü‰∏≠Áü•Ë≠òÁöÑÂΩ¢ÂºèÂåñË°®Á§∫ÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁµêÊßãÂåñÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºÁµÑÁπîÂíåÁêÜËß£Ë§áÈõúÁöÑË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÂª∫Á´ãÊú¨‰ΩìÊòØ‰∏ÄÈ†ÖË§áÈõú‰∏îËÄóÊôÇÁöÑÂä™Âäõ„ÄÇChEBI ÊòØÂåñÂ≠∏È†òÂüü‰∏≠‰∏ÄÂÄãËëóÂêçÁöÑÊú¨‰ΩìÔºåÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑË≥áÊ∫êÔºåÁî®ÊñºÂÆöÁæ©ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Â±¨ÊÄß„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÉÖÊ∂µËìã‰∫ÜÂåñÂ≠∏È†òÂüüÂø´ÈÄüÂ¢ûÈï∑ÁöÑÁü•Ë≠ò‰∏≠ÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºå‰∏¶‰∏îÊ≤íÊúâÊèê‰æõÁßëÂ≠∏ÊñáÁçªÁöÑÂèÉËÄÉ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñπÊ≥ïÔºåÂÆÉÊ∂âÂèä‰ΩøÁî®‰æÜËá™ Chebi ÁöÑÁü•Ë≠òÊì¥ÂÖÖÁèæÊúâÁöÑË®ªÈáãÊñáÊú¨Ë™ûÊñôÂ∫´Ôºå‰∏¶ÂæÆË™øÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰ª•Ë≠òÂà•ÂåñÂ≠∏ÂØ¶È´îÂèäÂÖ∂Âú®ÁßëÂ≠∏ÊñáÊú¨‰∏≠ÁöÑ‰ΩúÁî®„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÈÄèÈÅéÁµêÂêàÊú¨‰ΩìÁü•Ë≠òÂíå LLM ÁöÑË™ûË®ÄÁêÜËß£ËÉΩÂäõÔºåÊàëÂÄëÂú®Ë≠òÂà•ÁßëÂ≠∏ÊñáÁçª‰∏≠ÁöÑÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî®ÊñπÈù¢ÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÊ∫ñÁ¢∫Â∫¶ÂíåÂè¨ÂõûÁéá„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæû‰∏ÄÁµÑ 8,000 ÁØá ChemRxiv ÊñáÁ´†‰∏≠ÊèêÂèñÂÆÉÂÄëÔºå‰∏¶ÊáâÁî®Á¨¨‰∫åÂÄã LLM ‰æÜÂª∫Á´ã‰∏ÄÂÄãÂåñÂ≠∏ÂØ¶È´îÂíå‰ΩúÁî® (CEAR) ÁöÑÁü•Ë≠òÂúñË≠ú (KG)ÔºåÂÆÉÊèê‰æõË£úÂÖÖ ChEBI ÁöÑË≥áË®äÔºå‰∏¶ÊúâÂä©ÊñºÊì¥ÂÖÖÂÆÉ„ÄÇ

##### **TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities**
2407.21693v1 by Ming Zhang, Caishuang Huang, Yilong Wu, Shichun Liu, Huiyuan Zheng, Yurui Dong, Yujiong Shen, Shihan Dou, Jun Zhao, Junjie Ye, Qi Zhang, Tao Gui, Xuanjing Huang

Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented
conversations, including information gathering. How to utilize ToD accurately,
efficiently and effectively for information gathering has always been a
critical and challenging task. Recent studies have demonstrated that Large
Language Models (LLMs) excel in dialogue, instruction generation, and
reasoning, and can significantly enhance the performance of TOD through
fine-tuning. However, current datasets primarily cater to user-led systems and
are limited to predefined specific scenarios and slots, thereby necessitating
improvements in the proactiveness, diversity, and capabilities of TOD. In this
study, we present a detailed multi-domain task-oriented data construction
process for conversations, and a Chinese dialogue dataset generated based on
this process, \textbf{TransferTOD}, which authentically simulates human-machine
dialogues in 30 popular life service scenarios. Leveraging this dataset, we
trained a \textbf{TransferTOD-7B} model using full-parameter fine-tuning,
showcasing notable abilities in slot filling and questioning. Our work has
demonstrated its strong generalization capabilities in various downstream
scenarios, significantly enhancing both data utilization efficiency and system
performance. The data is released in
https://github.com/KongLongGeFDU/TransferTOD.

ÊëòË¶ÅÔºö<paragraph>Èù¢Âêë‰ªªÂãôÁöÑÂ∞çË©± (TOD) Á≥ªÁµ±Êó®Âú®ÊúâÊïàÂú∞ËôïÁêÜÈù¢Âêë‰ªªÂãôÁöÑÂ∞çË©±ÔºåÂåÖÊã¨Ë≥áË®äÊî∂ÈõÜ„ÄÇÂ¶Ç‰ΩïÊ∫ñÁ¢∫„ÄÅÊúâÊïàÂú∞Âà©Áî® TOD ÈÄ≤Ë°åË≥áË®äÊî∂ÈõÜ‰∏ÄÁõ¥ÊòØ‰∏ÄÈ†ÖÈóúÈçµ‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®Â∞çË©±„ÄÅÊåá‰ª§ÁîüÊàêÂíåÊé®ÁêÜÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰∏¶‰∏îÂèØ‰ª•ÈÄèÈÅéÂæÆË™øÈ°ØËëóÊèêÂçá TOD ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑË≥áÊñôÈõÜ‰∏ªË¶ÅËøéÂêà‰ΩøÁî®ËÄÖ‰∏ªÂ∞éÁöÑÁ≥ªÁµ±Ôºå‰∏¶‰∏îÂÉÖÈôêÊñºÈ†êÂÆöÁæ©ÁöÑÁâπÂÆöÂ†¥ÊôØÂíåÊôÇÊÆµÔºåÂõ†Ê≠§ÊúâÂøÖË¶ÅÊèêÂçá TOD ÁöÑ‰∏ªÂãïÊÄß„ÄÅÂ§öÊ®£ÊÄßÂíåËÉΩÂäõ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãË©≥Á¥∞ÁöÑÂ§öÈ†òÂüüÈù¢Âêë‰ªªÂãôË≥áÊñôÂª∫ÊßãÊµÅÁ®ãÔºå‰ª•Âèä‰∏ÄÂÄãÂü∫ÊñºÊ≠§ÊµÅÁ®ãÁîüÊàêÁöÑ‰∏≠ÊñáÂ∞çË©±Ë≥áÊñôÈõÜÔºå\textbf{TransferTOD}ÔºåÂÆÉÁúüÂØ¶Âú∞Ê®°Êì¨‰∫Ü 30 ÂÄãÊµÅË°åÁîüÊ¥ªÊúçÂãôÂ†¥ÊôØ‰∏≠ÁöÑ‰∫∫Ê©üÂ∞çË©±„ÄÇÂà©Áî®Ê≠§Ë≥áÊñôÈõÜÔºåÊàëÂÄë‰ΩøÁî®ÂÖ®ÂèÉÊï∏ÂæÆË™øË®ìÁ∑¥‰∫Ü‰∏ÄÂÄã \textbf{TransferTOD-7B} Ê®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂú®ÊôÇÊÆµÂ°´Ë£úÂíåÊèêÂïèÊñπÈù¢ÁöÑÈ°ØËëóËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Â±ïÁ§∫‰∫ÜÂÖ∂Âú®ÂêÑÁ®Æ‰∏ãÊ∏∏Â†¥ÊôØ‰∏≠ÁöÑÂº∑Â§ßÊ≥õÂåñËÉΩÂäõÔºåÈ°ØËëóÊèêÂçá‰∫ÜË≥áÊñôÂà©Áî®ÁéáÂíåÁ≥ªÁµ±ÊïàËÉΩ„ÄÇË≥áÊñôÂ∑≤Áôº‰ΩàÂú® https://github.com/KongLongGeFDU/TransferTOD„ÄÇ</paragraph>

##### **Dynamic Object Queries for Transformer-based Incremental Object Detection**
2407.21687v1 by Jichuan Zhang, Wei Li, Shuang Cheng, Ya-Li Li, Shengjin Wang

Incremental object detection (IOD) aims to sequentially learn new classes,
while maintaining the capability to locate and identify old ones. As the
training data arrives with annotations only with new classes, IOD suffers from
catastrophic forgetting. Prior methodologies mainly tackle the forgetting issue
through knowledge distillation and exemplar replay, ignoring the conflict
between limited model capacity and increasing knowledge. In this paper, we
explore \textit{dynamic object queries} for incremental object detection built
on Transformer architecture. We propose the \textbf{Dy}namic object
\textbf{Q}uery-based \textbf{DE}tection \textbf{TR}ansformer (DyQ-DETR), which
incrementally expands the model representation ability to achieve
stability-plasticity tradeoff. First, a new set of learnable object queries are
fed into the decoder to represent new classes. These new object queries are
aggregated with those from previous phases to adapt both old and new knowledge
well. Second, we propose the isolated bipartite matching for object queries in
different phases, based on disentangled self-attention. The interaction among
the object queries at different phases is eliminated to reduce inter-class
confusion. Thanks to the separate supervision and computation over object
queries, we further present the risk-balanced partial calibration for effective
exemplar replay. Extensive experiments demonstrate that DyQ-DETR significantly
surpasses the state-of-the-art methods, with limited parameter overhead. Code
will be made publicly available.

ÊëòË¶ÅÔºöÂ¢ûÈáèÂºèÁâ©‰ª∂ÂÅµÊ∏¨ (IOD) Êó®Âú®Âæ™Â∫èÊº∏ÈÄ≤Âú∞Â≠∏ÁøíÊñ∞È°ûÂà•ÔºåÂêåÊôÇÁ∂≠ÊåÅÂÆö‰ΩçÂíåËæ®Ë≠òËàäÈ°ûÂà•ÁöÑËÉΩÂäõ„ÄÇÁî±ÊñºË®ìÁ∑¥Ë≥áÊñôÂÉÖÈôÑÊúâÊñ∞È°ûÂà•ÁöÑË®ªËß£ÔºåIOD ÊúÉÁôºÁîüÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÊñπÊ≥ï‰∏ªË¶ÅÈÄèÈÅéÁü•Ë≠òËêÉÂèñÂíåÁØÑ‰æãÈáçÊí≠‰æÜËß£Ê±∫ÈÅ∫ÂøòÂïèÈ°åÔºåÂçªÂøΩÁï•‰∫ÜÊ®°ÂûãÂÆπÈáèÊúâÈôêÂíåÁü•Ë≠òÂ¢ûÂä†‰πãÈñìÁöÑË°ùÁ™Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Á¥¢Âª∫Á´ãÂú® Transformer Êû∂Êßã‰∏äÁöÑÂ¢ûÈáèÂºèÁâ©‰ª∂ÂÅµÊ∏¨ÁöÑ„ÄåÂãïÊÖãÁâ©‰ª∂Êü•Ë©¢„Äç„ÄÇÊàëÂÄëÊèêÂá∫‰ª•ÂãïÊÖãÁâ©‰ª∂Êü•Ë©¢ÁÇ∫Âü∫Á§éÁöÑÂÅµÊ∏¨ËÆäÂΩ¢ÈáëÂâõ (DyQ-DETR)ÔºåÂÆÉÊúÉÈÄêÊ≠•Êì¥Â±ïÊ®°ÂûãË°®Á§∫ËÉΩÂäõÔºå‰ª•ÈÅîÊàêÁ©©ÂÆöÊÄßÂíåÂèØÂ°ëÊÄßÁöÑÊäòË°∑„ÄÇÈ¶ñÂÖàÔºåÂ∞á‰∏ÄÁµÑÊñ∞ÁöÑÂèØÂ≠∏ÁøíÁâ©‰ª∂Êü•Ë©¢Ëº∏ÂÖ•Ëß£Á¢ºÂô®Ôºå‰ª•Ë°®Á§∫Êñ∞È°ûÂà•„ÄÇÈÄô‰∫õÊñ∞ÁöÑÁâ©‰ª∂Êü•Ë©¢ÊúÉËàáÂâç‰∏ÄÈöéÊÆµÁöÑÊü•Ë©¢Âêà‰ΩµÔºå‰ª•ÈÅ©Áï∂Âú∞Ë™øÊï¥ËàäÊúâÂíåÊñ∞ÁöÑÁü•Ë≠ò„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫Âü∫ÊñºËß£ÈñãËá™ÊàëÊ≥®ÊÑèÂäõÁöÑ„ÄÅÁî®Êñº‰∏çÂêåÈöéÊÆµÁâ©‰ª∂Êü•Ë©¢ÁöÑÂ≠§Á´ã‰∫åÈÉ®ÂåπÈÖç„ÄÇÊ∂àÈô§‰∏çÂêåÈöéÊÆµÁâ©‰ª∂Êü•Ë©¢‰πãÈñìÁöÑ‰∫íÂãïÔºå‰ª•Ê∏õÂ∞ëÈ°ûÂà•ÈñìÁöÑÊ∑∑Ê∑Ü„ÄÇÁî±ÊñºÁâ©‰ª∂Êü•Ë©¢ÊúâÁç®Á´ãÁöÑÁõ£Áù£ÂíåÈÅãÁÆóÔºåÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÊèêÂá∫È¢®Èö™Âπ≥Ë°°ÁöÑÈÉ®ÂàÜÊ†°Ê≠£Ôºå‰ª•ÊúâÊïàÂú∞ÈáçÊí≠ÁØÑ‰æã„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË≠âÊòéÔºåDyQ-DETR Â§ßÂπÖË∂ÖË∂äÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÔºå‰∏îÂèÉÊï∏ÈñãÈä∑ÊúâÈôê„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂÖ¨ÈñãÊèê‰æõ„ÄÇ

##### **Synthetic Simplicity: Unveiling Bias in Medical Data Augmentation**
2407.21674v1 by Krishan Agyakari Raja Babu, Rachana Sathish, Mrunal Pattanaik, Rahul Venkataramani

Synthetic data is becoming increasingly integral in data-scarce fields such
as medical imaging, serving as a substitute for real data. However, its
inherent statistical characteristics can significantly impact downstream tasks,
potentially compromising deployment performance. In this study, we empirically
investigate this issue and uncover a critical phenomenon: downstream neural
networks often exploit spurious distinctions between real and synthetic data
when there is a strong correlation between the data source and the task label.
This exploitation manifests as \textit{simplicity bias}, where models overly
rely on superficial features rather than genuine task-related complexities.
Through principled experiments, we demonstrate that the source of data (real
vs.\ synthetic) can introduce spurious correlating factors leading to poor
performance during deployment when the correlation is absent. We first
demonstrate this vulnerability on a digit classification task, where the model
spuriously utilizes the source of data instead of the digit to provide an
inference. We provide further evidence of this phenomenon in a medical imaging
problem related to cardiac view classification in echocardiograms, particularly
distinguishing between 2-chamber and 4-chamber views. Given the increasing role
of utilizing synthetic datasets, we hope that our experiments serve as
effective guidelines for the utilization of synthetic datasets in model
training.

ÊëòË¶ÅÔºöÂêàÊàêË≥áÊñôÂú®Ë≥áÊñôÁ®ÄÂ∞ëÁöÑÈ†òÂüü‰∏≠ËÆäÂæóË∂ä‰æÜË∂ä‰∏çÂèØÊàñÁº∫Ôºå‰æãÂ¶ÇÈÜ´Â≠∏ÂΩ±ÂÉèÔºåÁî®‰ΩúÁúüÂØ¶Ë≥áÊñôÁöÑÊõø‰ª£ÂìÅ„ÄÇÁÑ∂ËÄåÔºåÂÖ∂ÂÖßÂú®ÁöÑÁµ±Ë®àÁâπÊÄßÊúÉÈ°ØËëóÂΩ±Èüø‰∏ãÊ∏∏‰ªªÂãôÔºåÂèØËÉΩÊêçÂÆ≥ÈÉ®ÁΩ≤ÊïàËÉΩ„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂØ¶Ë≠âË™øÊü•Ê≠§ÂïèÈ°åÔºå‰∏¶Êè≠Èú≤‰∏ÄÂÄãÈóúÈçµÁèæË±°ÔºöÁï∂Ë≥áÊñô‰æÜÊ∫êËàá‰ªªÂãôÊ®ôÁ±§‰πãÈñìÊúâÂæàÂº∑ÁöÑÁõ∏ÈóúÊÄßÊôÇÔºå‰∏ãÊ∏∏Á•ûÁ∂ìÁ∂≤Ë∑ØÈÄöÂ∏∏ÊúÉÂà©Áî®ÁúüÂØ¶Ë≥áÊñôËàáÂêàÊàêË≥áÊñô‰πãÈñìÁöÑËôõÂÅáÂçÄÂà•„ÄÇÈÄôÁ®ÆÂà©Áî®Ë°®ÁèæÁÇ∫„ÄåÁ∞°ÂåñÂÅèÂ∑Æ„ÄçÔºåÂÖ∂‰∏≠Ê®°ÂûãÈÅéÂ∫¶‰æùË≥¥Ë°®Èù¢ÁâπÂæµÔºåËÄå‰∏çÊòØÁúüÊ≠£ÁöÑËàá‰ªªÂãôÁõ∏ÈóúÁöÑË§áÈõúÊÄß„ÄÇÈÄèÈÅéÊúâÂéüÂâáÁöÑÂØ¶È©óÔºåÊàëÂÄëË≠âÊòéË≥áÊñô‰æÜÊ∫êÔºàÁúüÂØ¶Ë≥áÊñôËàáÂêàÊàêË≥áÊñôÔºâÂèØËÉΩÊúÉÂºïÂÖ•ËôõÂÅáÁöÑÁõ∏ÈóúÂõ†Á¥†ÔºåÂ∞éËá¥Âú®Áõ∏ÈóúÊÄß‰∏çÂ≠òÂú®ÊôÇÈÉ®ÁΩ≤ÊúüÈñìÊïàËÉΩ‰∏ç‰Ω≥„ÄÇÊàëÂÄëÈ¶ñÂÖàÂú®Êï∏Â≠óÂàÜÈ°û‰ªªÂãô‰∏≠Ë≠âÊòéÊ≠§ÊºèÊ¥ûÔºåÂÖ∂‰∏≠Ê®°ÂûãËôõÂÅáÂú∞Âà©Áî®Ë≥áÊñô‰æÜÊ∫êËÄåÈùûÊï∏Â≠ó‰æÜÊèê‰æõÊé®Ë´ñ„ÄÇÊàëÂÄëÂú®ËàáË∂ÖÈü≥Ê≥¢ÂøÉËáüË¶ñÈáéÂàÜÈ°ûÁõ∏ÈóúÁöÑÈÜ´Â≠∏ÂΩ±ÂÉèÂïèÈ°å‰∏≠ÈÄ≤‰∏ÄÊ≠•Êèê‰æõÊ≠§ÁèæË±°ÁöÑË≠âÊìöÔºåÁâπÂà•ÊòØÂçÄÂàÜ‰∫åËÖîÂíåÂõõËÖîË¶ñÈáé„ÄÇÈëëÊñºÂêàÊàêË≥áÊñôÈõÜÁöÑ‰ΩøÁî®ËßíËâ≤Êó•ÁõäÂ¢ûÂä†ÔºåÊàëÂÄëÂ∏åÊúõÊàëÂÄëÁöÑÂØ¶È©óËÉΩ‰ΩúÁÇ∫Âú®Ê®°ÂûãË®ìÁ∑¥‰∏≠Âà©Áî®ÂêàÊàêË≥áÊñôÈõÜÁöÑÊúâÊïàÊåáÂçó„ÄÇ

##### **Universal Approximation Theory: Foundations for Parallelism in Neural Networks**
2407.21670v1 by Wei Wang, Qing Li

Neural networks are increasingly evolving towards training large models with
big data, a method that has demonstrated superior performance across many
tasks. However, this approach introduces an urgent problem: current deep
learning models are predominantly serial, meaning that as the number of network
layers increases, so do the training and inference times. This is unacceptable
if deep learning is to continue advancing. Therefore, this paper proposes a
deep learning parallelization strategy based on the Universal Approximation
Theorem (UAT). From this foundation, we designed a parallel network called
Para-Former to test our theory. Unlike traditional serial models, the inference
time of Para-Former does not increase with the number of layers, significantly
accelerating the inference speed of multi-layer networks. Experimental results
validate the effectiveness of this network.

ÊëòË¶ÅÔºöÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ≠£ÊúùË®ìÁ∑¥Â§ßÂûãÊ®°ÂûãËàáÂ∑®ÈáèË≥áÊñôÁöÑÊñπÂêëÁôºÂ±ïÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂú®Ë®±Â§ö‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÂÑ™Áï∞ÊïàËÉΩÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÊñπÊ≥ïÂ∏∂‰æÜ‰∫Ü‰∏ÄÂÄãËø´ÂàáÁöÑÂïèÈ°åÔºöÁõÆÂâçÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰∏ªË¶ÅÊòØ‰∏≤ÂàóÁöÑÔºåÈÄôË°®Á§∫Á∂≤Ë∑ØÂ±§Êï∏Ë∂äÂ§öÔºåË®ìÁ∑¥ÂíåÊé®Ë´ñÊôÇÈñì‰πüÊúÉË∂äÈï∑„ÄÇÂ¶ÇÊûúÊ∑±Â∫¶Â≠∏ÁøíË¶ÅÁπºÁ∫åÈÄ≤Ê≠•ÔºåÈÄôÊòØÁÑ°Ê≥ïÊé•ÂèóÁöÑ„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÊ≥õÂáΩÈÄºËøëÂÆöÁêÜ (UAT) ÁöÑÊ∑±Â∫¶Â≠∏Áøí‰∏¶Ë°åÂåñÁ≠ñÁï•„ÄÇÂü∫ÊñºÊ≠§Âü∫Á§éÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ Para-Former ÁöÑÂπ≥Ë°åÁ∂≤Ë∑Ø‰æÜÊ∏¨Ë©¶ÊàëÂÄëÁöÑÁêÜË´ñ„ÄÇËàáÂÇ≥Áµ±‰∏≤ÂàóÊ®°Âûã‰∏çÂêåÔºåPara-Former ÁöÑÊé®Ë´ñÊôÇÈñì‰∏çÊúÉÈö®ËëóÂ±§Êï∏Â¢ûÂä†ËÄåÂ¢ûÂä†ÔºåÂ§ßÂπÖÂä†ÈÄüÂ§öÂ±§Á∂≤Ë∑ØÁöÑÊé®Ë´ñÈÄüÂ∫¶„ÄÇÂØ¶È©óÁµêÊûúÈ©óË≠â‰∫ÜÊ≠§Á∂≤Ë∑ØÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Synth-Empathy: Towards High-Quality Synthetic Empathy Data**
2407.21669v1 by Hao Liang, Linzhuang Sun, Jingxuan Wei, Xijie Huang, Linkun Sun, Bihui Yu, Conghui He, Wentao Zhang

In recent years, with the rapid advancements in large language models (LLMs),
achieving excellent empathetic response capabilities has become a crucial
prerequisite. Consequently, managing and understanding empathetic datasets have
gained increasing significance. However, empathetic data are typically
human-labeled, leading to insufficient datasets and wasted human labor. In this
work, we present Synth-Empathy, an LLM-based data generation and quality and
diversity selection pipeline that automatically generates high-quality
empathetic data while discarding low-quality data. With the data generated from
a low empathetic model, we are able to further improve empathetic response
performance and achieve state-of-the-art (SoTA) results across multiple
benchmarks. Moreover, our model achieves SoTA performance on various human
evaluation benchmarks, demonstrating its effectiveness and robustness in
real-world applications. Furthermore, we show the trade-off between data
quantity and quality, providing insights into empathetic data generation and
selection.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÈÄ≤Â±ïÔºåÂØ¶ÁèæÂÑ™Áï∞ÁöÑÂêåÁêÜÂøÉÂõûÊáâËÉΩÂäõÂ∑≤ÊàêÁÇ∫‰∏ÄÈ†ÖÈóúÈçµÂÖàÊ±∫Ê¢ù‰ª∂„ÄÇÂõ†Ê≠§ÔºåÁÆ°ÁêÜÂíåÁêÜËß£ÂêåÁêÜÂøÉË≥áÊñôÈõÜËÆäÂæóË∂ä‰æÜË∂äÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÂêåÁêÜÂøÉË≥áÊñôÈÄöÂ∏∏ÊòØÁî±‰∫∫È°ûÊ®ôË®òÔºåÂ∞éËá¥Ë≥áÊñôÈõÜ‰∏çË∂≥‰∏îÊµ™Ë≤ª‰∫∫Âäõ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü Synth-EmpathyÔºåÈÄôÊòØ‰∏ÄÂÄãÂü∫Êñº LLM ÁöÑË≥áÊñôÁîüÊàê„ÄÅÂìÅË≥™ÂíåÂ§öÊ®£ÊÄßÈÅ∏ÊìáÁÆ°ÈÅìÔºåÂèØ‰ª•Ëá™ÂãïÁîüÊàêÈ´òÂìÅË≥™ÁöÑÂêåÁêÜÂøÉË≥áÊñôÔºåÂêåÊôÇÊç®Ê£Ñ‰ΩéÂìÅË≥™Ë≥áÊñô„ÄÇÈÄèÈÅéÂæû‰ΩéÂêåÁêÜÂøÉÊ®°ÂûãÁîüÊàêÁöÑË≥áÊñôÔºåÊàëÂÄëËÉΩÂ§†ÈÄ≤‰∏ÄÊ≠•ÊîπÂñÑÂêåÁêÜÂøÉÂõûÊáâË°®ÁèæÔºå‰∏¶Âú®Â§öÂÄãÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ (SoTA) ÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÂêÑÁ®Æ‰∫∫È°ûË©ï‰º∞Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÈÉΩÈÅîÂà∞‰∫Ü SoTA ÁöÑË°®ÁèæÔºåË≠âÊòé‰∫ÜÂÖ∂Âú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÊúâÊïàÊÄßÂíåÁ©©ÂÅ•ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜË≥áÊñôÊï∏ÈáèÂíåÂìÅË≥™‰πãÈñìÁöÑÊ¨äË°°ÔºåÊèê‰æõ‰∫ÜÂ∞çÂêåÁêÜÂøÉË≥áÊñôÁîüÊàêÂíåÈÅ∏ÊìáÁöÑË¶ãËß£„ÄÇ

##### **An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification**
2407.21666v1 by Aswini Kumar Patra, Ankit Varshney, Lingaraj Sahoo

Early detection of drought stress is critical for taking timely measures for
reducing crop loss before the drought impact becomes irreversible. The subtle
phenotypical and physiological changes in response to drought stress are
captured by non-invasive imaging techniques and these imaging data serve as
valuable resource for machine learning methods to identify drought stress.
While convolutional neural networks (CNNs) are in wide use, vision transformers
(ViTs) present a promising alternative in capturing long-range dependencies and
intricate spatial relationships, thereby enhancing the detection of subtle
indicators of drought stress. We propose an explainable deep learning pipeline
that leverages the power of ViTs for drought stress detection in potato crops
using aerial imagery. We applied two distinct approaches: a synergistic
combination of ViT and support vector machine (SVM), where ViT extracts
intricate spatial features from aerial images, and SVM classifies the crops as
stressed or healthy and an end-to-end approach using a dedicated classification
layer within ViT to directly detect drought stress. Our key findings explain
the ViT model's decision-making process by visualizing attention maps. These
maps highlight the specific spatial features within the aerial images that the
ViT model focuses as the drought stress signature. Our findings demonstrate
that the proposed methods not only achieve high accuracy in drought stress
identification but also shedding light on the diverse subtle plant features
associated with drought stress. This offers a robust and interpretable solution
for drought stress monitoring for farmers to undertake informed decisions for
improved crop management.

ÊëòË¶ÅÔºöÂèäÊó©ÂÅµÊ∏¨‰πæÊó±Â£ìÂäõÔºåÂ∞çÊñºÂú®‰πæÊó±ÂΩ±ÈüøËÆäÂæó‰∏çÂèØÈÄÜËΩâ‰πãÂâçÊé°ÂèñÈÅ©ÊôÇÊé™ÊñΩ‰ª•Ê∏õÂ∞ë‰ΩúÁâ©ÊêçÂ§±Ëá≥ÈóúÈáçË¶Å„ÄÇÈùû‰æµÂÖ•ÂºèÂΩ±ÂÉèÊäÄË°ìÂèØÊçïÊçâÂà∞Â∞ç‰πæÊó±Â£ìÂäõÁî¢ÁîüÁöÑÁ¥∞ÂæÆË°®ÂûãÂíåÁîüÁêÜËÆäÂåñÔºåËÄåÈÄô‰∫õÂΩ±ÂÉèË≥áÊñôÂèØ‰ΩúÁÇ∫Ê©üÂô®Â≠∏ÁøíÊñπÊ≥ïË≠òÂà•‰πæÊó±Â£ìÂäõÁöÑÂØ∂Ë≤¥Ë≥áÊ∫ê„ÄÇÈõñÁÑ∂Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø (CNN) Ë¢´Âª£Ê≥õ‰ΩøÁî®Ôºå‰ΩÜË¶ñË¶∫Transformer (ViT) Âú®ÊçïÊçâÈï∑Á®ã‰æùË≥¥Èóú‰øÇÂíåË§áÈõúÁ©∫ÈñìÈóú‰øÇÊñπÈù¢Êèê‰æõ‰∫ÜÊúâÂâçÊôØÁöÑÊõø‰ª£ÊñπÊ°àÔºåÂæûËÄåÂ¢ûÂº∑‰∫ÜÂ∞ç‰πæÊó±Â£ìÂäõÁöÑÁ¥∞ÂæÆÊåáÊ®ôÁöÑÂÅµÊ∏¨„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂèØËß£ÈáãÁöÑÊ∑±Â∫¶Â≠∏ÁøíÁÆ°ÈÅìÔºåÂÆÉÂà©Áî® ViT ÁöÑÂäüËÉΩ‰æÜÂÅµÊ∏¨È¶¨Èà¥ËñØ‰ΩúÁâ©‰∏≠ÁöÑ‰πæÊó±Â£ìÂäõÔºå‰∏¶‰ΩøÁî®Ëà™ÊãçÂΩ±ÂÉè„ÄÇÊàëÂÄëÊáâÁî®ÂÖ©Á®Æ‰∏çÂêåÁöÑÊñπÊ≥ïÔºöViT ÂíåÊîØÊè¥ÂêëÈáèÊ©ü (SVM) ÁöÑÂçîÂêåÁµÑÂêàÔºåÂÖ∂‰∏≠ ViT ÂæûËà™ÊãçÂΩ±ÂÉè‰∏≠ËêÉÂèñË§áÈõúÁöÑÁ©∫ÈñìÁâπÂæµÔºåËÄå SVM Â∞á‰ΩúÁâ©ÂàÜÈ°ûÁÇ∫ÂèóÂ£ìÊàñÂÅ•Â∫∑Ôºå‰ª•Âèä‰ΩøÁî® ViT ÂÖßÁöÑÂ∞àÁî®ÂàÜÈ°ûÂ±§ÁöÑÁ´ØÂà∞Á´ØÊñπÊ≥ï‰æÜÁõ¥Êé•ÂÅµÊ∏¨‰πæÊó±Â£ìÂäõ„ÄÇÊàëÂÄëÁöÑÈóúÈçµÁôºÁèæÈÄèÈÅéË¶ñË¶∫ÂåñÊ≥®ÊÑèÂäõÂúñ‰æÜËß£Èáã ViT Ê®°ÂûãÁöÑÊ±∫Á≠ñÈÅéÁ®ã„ÄÇÈÄô‰∫õÂúñÁ™ÅÈ°Ø‰∫Ü ViT Ê®°ÂûãÈóúÊ≥®ÁöÑËà™ÊãçÂΩ±ÂÉè‰∏≠ÁöÑÁâπÂÆöÁ©∫ÈñìÁâπÂæµÔºå‰ΩúÁÇ∫‰πæÊó±Â£ìÂäõÁöÑÁâπÂæµ„ÄÇÊàëÂÄëÁöÑÁôºÁèæË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ï‰∏çÂÉÖÂú®‰πæÊó±Â£ìÂäõË≠òÂà•‰∏≠ÂØ¶Áèæ‰∫ÜÈ´òÊ∫ñÁ¢∫Â∫¶ÔºåËÄå‰∏îÈÇÑÈó°Êòé‰∫ÜËàá‰πæÊó±Â£ìÂäõÁõ∏ÈóúÁöÑÂêÑÁ®ÆÁ¥∞ÂæÆÊ§çÁâ©ÁâπÂæµ„ÄÇÈÄôÁÇ∫‰πæÊó±Â£ìÂäõÁõ£ÊéßÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂº∑Â§ß‰∏îÂèØËß£ÈáãÁöÑËß£Ê±∫ÊñπÊ°àÔºåËÆìËæ≤Ê∞ëÂèØ‰ª•ÂÅöÂá∫ÊòéÊô∫ÁöÑÊ±∫Á≠ñÔºå‰ª•ÊîπÂñÑ‰ΩúÁâ©ÁÆ°ÁêÜ„ÄÇ

##### **Defending Jailbreak Attack in VLMs via Cross-modality Information Detector**
2407.21659v2 by Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang

Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively
understand vision information, achieving remarkable performance in many
vision-centric tasks. Despite that, recent studies have shown that these models
are susceptible to jailbreak attacks, which refer to an exploitative technique
where malicious users can break the safety alignment of the target model and
generate misleading and harmful answers. This potential threat is caused by
both the inherent vulnerabilities of LLM and the larger attack scope introduced
by vision input. To enhance the security of VLMs against jailbreak attacks,
researchers have developed various defense techniques. However, these methods
either require modifications to the model's internal structure or demand
significant computational resources during the inference phase. Multimodal
information is a double-edged sword. While it increases the risk of attacks, it
also provides additional data that can enhance safeguards. Inspired by this, we
propose $\underline{\textbf{C}}$ross-modality
$\underline{\textbf{I}}$nformation
$\underline{\textbf{DE}}$tecto$\underline{\textbf{R}}$ ($\textit{CIDER})$, a
plug-and-play jailbreaking detector designed to identify maliciously perturbed
image inputs, utilizing the cross-modal similarity between harmful queries and
adversarial images. This simple yet effective cross-modality information
detector, $\textit{CIDER}$, is independent of the target VLMs and requires less
computation cost. Extensive experimental results demonstrate the effectiveness
and efficiency of $\textit{CIDER}$, as well as its transferability to both
white-box and black-box VLMs.

ÊëòË¶ÅÔºö<paragraph>Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM) Êì¥Â±ï‰∫Ü LLM ÂÖ®Èù¢ÁêÜËß£Ë¶ñË¶∫Ë≥áË®äÁöÑËÉΩÂäõÔºåÂú®Ë®±Â§ö‰ª•Ë¶ñË¶∫ÁÇ∫‰∏≠ÂøÉÁöÑ‰ªªÂä°‰∏≠ÂèñÂæóÈ°ØËëóÁöÑË°®Áèæ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊúÄËøëÁöÑÁ†îÁ©∂È°ØÁ§∫ÔºåÈÄô‰∫õÊ®°ÂûãÂÆπÊòìÂèóÂà∞Ë∂äÁçÑÊîªÊìäÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂâùÂâäÊäÄË°ìÔºåÊÉ°ÊÑè‰ΩøÁî®ËÄÖÂèØ‰ª•Á†¥Â£ûÁõÆÊ®ôÊ®°ÂûãÁöÑÂÆâÂÖ®Â∞çÈΩäÔºå‰∏¶Áî¢ÁîüÂÖ∑ÊúâË™§Â∞éÊÄßÂíåÂç±ÂÆ≥ÊÄßÁöÑÁ≠îÊ°à„ÄÇÈÄôÁ®ÆÊΩõÂú®Â®ÅËÑÖÊòØÁî± LLM ÁöÑÂõ∫ÊúâÊºèÊ¥ûÂíåË¶ñË¶∫Ëº∏ÂÖ•ÂºïÂÖ•ÁöÑÊõ¥Â§ßÊîªÊìäÁØÑÂúçÊâÄÈÄ†ÊàêÁöÑ„ÄÇÁÇ∫‰∫ÜÂ¢ûÂº∑ VLM Â∞çÊäóË∂äÁçÑÊîªÊìäÁöÑÂÆâÂÖ®ÊÄßÔºåÁ†îÁ©∂‰∫∫Âì°ÈñãÁôº‰∫ÜÂêÑÁ®ÆÈò≤Á¶¶ÊäÄË°ì„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈúÄË¶Å‰øÆÊîπÊ®°ÂûãÁöÑÂÖßÈÉ®ÁµêÊßãÔºåÊàñÂú®Êé®ÁêÜÈöéÊÆµÈúÄË¶ÅÂ§ßÈáèÁöÑË®àÁÆóË≥áÊ∫ê„ÄÇÂ§öÊ®°ÊÖãË≥áË®äÊòØ‰∏ÄÊääÈõôÈù¢ÂàÉ„ÄÇÈõñÁÑ∂ÂÆÉÂ¢ûÂä†‰∫ÜÊîªÊìäÈ¢®Èö™Ôºå‰ΩÜÂÆÉ‰πüÊèê‰æõ‰∫ÜÂèØ‰ª•Â¢ûÂº∑Èò≤Ë≠∑Êé™ÊñΩÁöÑÈ°çÂ§ñË≥áÊñô„ÄÇÂèóÊ≠§ÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫Ë∑®Ê®°ÊÖãË≥áË®äÂÅµÊ∏¨Âô® ($\textit{CIDER}$)ÔºåÈÄôÊòØ‰∏ÄÂÄãÂç≥ÊèíÂç≥Áî®ÁöÑË∂äÁçÑÂÅµÊ∏¨Âô®ÔºåÊó®Âú®Ë≠òÂà•ÊÉ°ÊÑèÊìæÂãïÁöÑÂΩ±ÂÉèËº∏ÂÖ•ÔºåÂà©Áî®ÊúâÂÆ≥Êü•Ë©¢ÂíåÂ∞çÊäóÊÄßÂΩ±ÂÉè‰πãÈñìÁöÑË∑®Ê®°ÊÖãÁõ∏‰ººÊÄß„ÄÇÈÄôÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑË∑®Ê®°ÊÖãË≥áË®äÂÅµÊ∏¨Âô® $\textit{CIDER}$ Áç®Á´ãÊñºÁõÆÊ®ô VLMÔºå‰∏¶‰∏îÈúÄË¶ÅËºÉÂ∞ëÁöÑË®àÁÆóÊàêÊú¨„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫Ü $\textit{CIDER}$ ÁöÑÊúâÊïàÊÄßÂíåÊïàÁéáÔºå‰ª•ÂèäÂÆÉÂ∞çÁôΩÁõíÂíåÈªëÁõí VLM ÁöÑÂèØÁßªÊ§çÊÄß„ÄÇ</paragraph>

##### **Spatial Transformer Network YOLO Model for Agricultural Object Detection**
2407.21652v1 by Yash Zambre, Ekdev Rajkitkul, Akshatha Mohan, Joshua Peeples

Object detection plays a crucial role in the field of computer vision by
autonomously identifying and locating objects of interest. The You Only Look
Once (YOLO) model is an effective single-shot detector. However, YOLO faces
challenges in cluttered or partially occluded scenes and can struggle with
small, low-contrast objects. We propose a new method that integrates spatial
transformer networks (STNs) into YOLO to improve performance. The proposed
STN-YOLO aims to enhance the model's effectiveness by focusing on important
areas of the image and improving the spatial invariance of the model before the
detection process. Our proposed method improved object detection performance
both qualitatively and quantitatively. We explore the impact of different
localization networks within the STN module as well as the robustness of the
model across different spatial transformations. We apply the STN-YOLO on
benchmark datasets for Agricultural object detection as well as a new dataset
from a state-of-the-art plant phenotyping greenhouse facility. Our code and
dataset are publicly available.

ÊëòË¶ÅÔºöÁõÆÊ®ôÂÅµÊ∏¨Âú®ÈõªËÖ¶Ë¶ñË¶∫È†òÂüü‰∏≠ÊâÆÊºîËëóÈóúÈçµËßíËâ≤ÔºåÂÆÉËÉΩËá™‰∏ªËæ®Ë≠ò‰∏¶ÂÆö‰ΩçÊÑüËààË∂£ÁöÑÁõÆÊ®ô„ÄÇYou Only Look Once (YOLO) Ê®°ÂûãÊòØ‰∏ÄÂÄãÊúâÊïàÁöÑÂñÆÊ¨°ÂÅµÊ∏¨Âô®„ÄÇÁÑ∂ËÄåÔºåYOLO Âú®Èõú‰∫ÇÊàñÈÉ®ÂàÜÈÅÆÊìãÁöÑÂ†¥ÊôØ‰∏≠ÊúÉÈù¢Ëá®ÊåëÊà∞Ôºå‰∏¶‰∏îÂú®ËôïÁêÜÂ∞èËÄåÂ∞çÊØîÂ∫¶‰ΩéÁöÑÁõÆÊ®ôÊôÇÊúÉÈÅáÂà∞Âõ∞Èõ£„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ïÔºåÂ∞áÁ©∫ÈñìËΩâÊèõÁ∂≤Ë∑Ø (STN) Êï¥ÂêàÂà∞ YOLO ‰∏≠‰ª•ÊèêÂçáÊïàËÉΩ„ÄÇÊâÄÊèêÂá∫ÁöÑ STN-YOLO Êó®Âú®ÈÄèÈÅéÂú®ÂÅµÊ∏¨ÈÅéÁ®ãÂâçÂ∞àÊ≥®ÊñºÂΩ±ÂÉèÁöÑÈáçË¶ÅÂçÄÂüü‰∏¶ÊîπÂñÑÊ®°ÂûãÁöÑÁ©∫Èñì‰∏çËÆäÊÄßÔºå‰æÜÊèêÂçáÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®Ë≥™ÈáèÂíåÊï∏Èáè‰∏äÈÉΩÊîπÂñÑ‰∫ÜÁõÆÊ®ôÂÅµÊ∏¨ÊïàËÉΩ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫Ü STN Ê®°ÁµÑ‰∏≠‰∏çÂêåÂÆö‰ΩçÁ∂≤Ë∑ØÁöÑÂΩ±ÈüøÔºå‰ª•ÂèäÊ®°ÂûãÂú®‰∏çÂêåÁ©∫ÈñìËΩâÊèõ‰∏≠ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÂ∞á STN-YOLO ÊáâÁî®ÊñºËæ≤Ê•≠ÁõÆÊ®ôÂÅµÊ∏¨ÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜÔºå‰ª•Âèä‰æÜËá™ÊúÄÂÖàÈÄ≤ÁöÑÊ§çÁâ©Ë°®ÂûãÊ∫´ÂÆ§Ë®≠ÊñΩÁöÑÊñ∞Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂ∑≤ÂÖ¨Èñã„ÄÇ

##### **Human interaction classifier for LLM based chatbot**
2407.21647v1 by Diego Mart√≠n, Jordi Sanchez, Xavier Vizca√≠no

This study investigates different approaches to classify human interactions
in an artificial intelligence-based environment, specifically for Applus+
IDIADA's intelligent agent AIDA. The main objective is to develop a classifier
that accurately identifies the type of interaction received (Conversation,
Services, or Document Translation) to direct requests to the appropriate
channel and provide a more specialized and efficient service. Various models
are compared, including LLM-based classifiers, KNN using Titan and Cohere
embeddings, SVM, and artificial neural networks. Results show that SVM and ANN
models with Cohere embeddings achieve the best overall performance, with
superior F1 scores and faster execution times compared to LLM-based approaches.
The study concludes that the SVM model with Cohere embeddings is the most
suitable option for classifying human interactions in the AIDA environment,
offering an optimal balance between accuracy and computational efficiency.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂú®Âü∫Êñº‰∫∫Â∑•Êô∫ÊÖßÁöÑÁí∞Â¢É‰∏≠Â∞ç‰∫∫È°û‰∫íÂãïÈÄ≤Ë°åÂàÜÈ°ûÁöÑ‰∏çÂêåÊñπÊ≥ïÔºåÁâπÂà•ÊòØÈáùÂ∞ç Applus+ IDIADA ÁöÑÊô∫ÊÖß‰ª£ÁêÜ‰∫∫ AIDA„ÄÇ‰∏ªË¶ÅÁõÆÊ®ôÊòØÈñãÁôº‰∏ÄÂÄãÂàÜÈ°ûÂô®ÔºåËÉΩÁ≤æÁ¢∫Ë≠òÂà•Êé•Êî∂Âà∞ÁöÑ‰∫íÂãïÈ°ûÂûãÔºàÂ∞çË©±„ÄÅÊúçÂãôÊàñÊñá‰ª∂ÁøªË≠ØÔºâÔºå‰ª•Â∞áË´ãÊ±ÇÂ∞éÂêëÈÅ©Áï∂ÁöÑÁÆ°ÈÅìÔºå‰∏¶Êèê‰æõÊõ¥Â∞àÊ•≠‰∏îÊúâÊïàÁéáÁöÑÊúçÂãô„ÄÇÊØîËºÉ‰∫ÜÂêÑÁ®ÆÊ®°ÂûãÔºåÂåÖÊã¨Âü∫Êñº LLM ÁöÑÂàÜÈ°ûÂô®„ÄÅ‰ΩøÁî® Titan Âíå Cohere ÂÖßÂµåÁöÑ KNN„ÄÅSVM Âíå‰∫∫Â∑•Á•ûÁ∂ìÁ∂≤Ë∑Ø„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊé°Áî® Cohere ÂÖßÂµåÁöÑ SVM Âíå ANN Ê®°ÂûãÂèØÈÅîÊàêÊúÄ‰Ω≥Êï¥È´îÊïàËÉΩÔºåËàáÂü∫Êñº LLM ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∑ÊúâÂÑ™Áï∞ÁöÑ F1 ÂàÜÊï∏ÂíåÊõ¥Âø´ÁöÑÂü∑Ë°åÊôÇÈñì„ÄÇÁ†îÁ©∂ÁµêË´ñÊòØÔºåÊé°Áî® Cohere ÂÖßÂµåÁöÑ SVM Ê®°ÂûãÊúÄÈÅ©ÂêàÂú® AIDA Áí∞Â¢É‰∏≠Â∞ç‰∫∫È°û‰∫íÂãïÈÄ≤Ë°åÂàÜÈ°ûÔºåÂú®Ê∫ñÁ¢∫Â∫¶ÂíåÈÅãÁÆóÊïàÁéá‰πãÈñìÂèñÂæóÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇ

##### **Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent**
2407.21646v1 by Shanbo Cheng, Zhichao Huang, Tom Ko, Hang Li, Ningxin Peng, Lu Xu, Qini Zhang

In this paper, we present Cross Language Agent -- Simultaneous
Interpretation, CLASI, a high-quality and human-like Simultaneous Speech
Translation (SiST) System. Inspired by professional human interpreters, we
utilize a novel data-driven read-write strategy to balance the translation
quality and latency. To address the challenge of translating in-domain
terminologies, CLASI employs a multi-modal retrieving module to obtain relevant
information to augment the translation. Supported by LLMs, our approach can
generate error-tolerated translation by considering the input audio, historical
context, and retrieved information. Experimental results show that our system
outperforms other systems by significant margins. Aligned with professional
human interpreters, we evaluate CLASI with a better human evaluation metric,
valid information proportion (VIP), which measures the amount of information
that can be successfully conveyed to the listeners. In the real-world
scenarios, where the speeches are often disfluent, informal, and unclear, CLASI
achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese
translation directions, respectively. In contrast, state-of-the-art commercial
or open-source systems only achieve 35.4% and 41.6%. On the extremely hard
dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70%
VIP.

ÊëòË¶ÅÔºö<paragraph>Âú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Ë∑®Ë™ûË®Ä‰ª£ÁêÜ‚Äî‚ÄîÂêåËÅ≤ÂÇ≥Ë≠Ø (CLAS)Ôºå‰∏ÄÂÄãÈ´òÂìÅË≥™‰∏îÈ°û‰ºº‰∫∫È°ûÁöÑÂêåËÅ≤ÂÇ≥Ë≠Ø (SiST) Á≥ªÁµ±„ÄÇÂèóÂà∞Â∞àÊ•≠‰∫∫È°ûÂè£Ë≠ØÂì°ÁöÑÂïüÁôºÔºåÊàëÂÄëÂà©Áî®‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊï∏ÊìöÈ©ÖÂãïËÆÄÂØ´Á≠ñÁï•‰æÜÂπ≥Ë°°ÁøªË≠ØÂìÅË≥™ÂíåÂª∂ÈÅ≤„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÁøªË≠ØÈ†òÂüüÂÖßË°ìË™ûÁöÑÊåëÊà∞ÔºåCLAS Êé°Áî®Â§öÊ®°ÂºèÊ™¢Á¥¢Ê®°ÁµÑ‰æÜÁç≤ÂèñÁõ∏ÈóúË≥áË®ä‰ª•Êì¥ÂÖÖÁøªË≠Ø„ÄÇÂú®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊîØÊè¥‰∏ãÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂèØ‰ª•ÈÄèÈÅéËÄÉÈáèËº∏ÂÖ•Èü≥Ë®ä„ÄÅÊ≠∑Âè≤ËÑàÁµ°ÂíåÊ™¢Á¥¢Ë≥áË®ä‰æÜÁî¢ÁîüÂÆπÈåØÁøªË≠Ø„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑÁ≥ªÁµ±‰ª•È°ØËëóÁöÑÂπÖÂ∫¶ÂÑ™ÊñºÂÖ∂‰ªñÁ≥ªÁµ±„ÄÇËàáÂ∞àÊ•≠‰∫∫È°ûÂè£Ë≠ØÂì°‰øùÊåÅ‰∏ÄËá¥ÔºåÊàëÂÄë‰ΩøÁî®Êõ¥Â•ΩÁöÑ‰∫∫È°ûË©ïÈáèÊåáÊ®ô‚Äî‚ÄîÊúâÊïàË≥áË®äÊØî‰æã (VIP) ‰æÜË©ïÈáè CLASÔºåË©≤ÊåáÊ®ôË°°ÈáèÂèØ‰ª•ÊàêÂäüÂÇ≥ÈÅîÁµ¶ËÅΩÁúæÁöÑË≥áË®äÈáè„ÄÇÂú®ÁèæÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÔºåÊºîË¨õÈÄöÂ∏∏‰∏çÊµÅÊö¢„ÄÅÈùûÊ≠£Âºè‰∏î‰∏çÊ∏Ö‰∏çÊ•öÔºåCLAS Âú®‰∏≠Ë≠ØËã±ÂíåËã±Ë≠Ø‰∏≠ÁøªË≠ØÊñπÂêëÂàÜÂà•ÈÅîÂà∞ 81.3% Âíå 78.0% ÁöÑ VIP„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÊúÄÂÖàÈÄ≤ÁöÑÂïÜÁî®ÊàñÈñãÊ∫êÁ≥ªÁµ±ÂÉÖÈÅîÂà∞ 35.4% Âíå 41.6%„ÄÇÂú®Ê•µÂõ∞Èõ£ÁöÑË≥áÊñôÈõÜ‰∏äÔºåÂÖ∂‰ªñÁ≥ªÁµ±ÁöÑ VIP ‰ΩéÊñº 13%ÔºåCLAS ‰ªçËÉΩÈÅîÂà∞ 70% ÁöÑ VIP„ÄÇ</paragraph>

##### **Quality Control for Radiology Report Generation Models via Auxiliary Auditing Components**
2407.21638v1 by Hermione Warr, Yasin Ibrahim, Daniel R. McGowan, Konstantinos Kamnitsas

Automation of medical image interpretation could alleviate bottlenecks in
diagnostic workflows, and has become of particular interest in recent years due
to advancements in natural language processing. Great strides have been made
towards automated radiology report generation via AI, yet ensuring clinical
accuracy in generated reports is a significant challenge, hindering deployment
of such methods in clinical practice. In this work we propose a quality control
framework for assessing the reliability of AI-generated radiology reports with
respect to semantics of diagnostic importance using modular auxiliary auditing
components (AC). Evaluating our pipeline on the MIMIC-CXR dataset, our findings
show that incorporating ACs in the form of disease-classifiers can enable
auditing that identifies more reliable reports, resulting in higher F1 scores
compared to unfiltered generated reports. Additionally, leveraging the
confidence of the AC labels further improves the audit's effectiveness.

ÊëòË¶ÅÔºöÈÜ´ÁôÇÂΩ±ÂÉèÂà§ËÆÄÁöÑËá™ÂãïÂåñÂèØ‰ª•Ê∏õËºïË®∫Êñ∑Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÁöÑÁì∂È†∏Ôºå‰∏¶‰∏îÁî±ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈÄ≤Ê≠•ÔºåÂú®ËøëÂπ¥‰æÜÁâπÂà•ÂèóÂà∞ÈáçË¶ñ„ÄÇÂú®ÈÄèÈÅé AI Ëá™ÂãïÁîüÊàêÊîæÂ∞ÑÁ∑öÂ†±ÂëäÊñπÈù¢Â∑≤Á∂ìÂèñÂæó‰∫ÜÈï∑Ë∂≥ÁöÑÈÄ≤Â±ïÔºåÁÑ∂ËÄåÁ¢∫‰øùÁîüÊàêÂ†±ÂëäÁöÑËá®Â∫äÊ∫ñÁ¢∫ÊÄßÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÈòªÁ§ô‰∫ÜÊ≠§È°ûÊñπÊ≥ïÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂìÅË≥™ÊéßÂà∂Êû∂ÊßãÔºåÁî®ÊñºË©ï‰º∞ AI ÁîüÊàêÁöÑÊîæÂ∞ÑÁ∑öÂ†±ÂëäÁöÑÂèØÈù†ÊÄßÔºå‰∏¶‰ΩøÁî®Ê®°ÁµÑÂåñËºîÂä©Á®ΩÊ†∏ÂÖÉ‰ª∂ (AC) ÈáùÂ∞çË®∫Êñ∑ÈáçË¶ÅÊÄßÁöÑË™ûÁæ©ÈÄ≤Ë°åË©ï‰º∞„ÄÇÂú® MIMIC-CXR Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÁÆ°ÈÅìÔºåÊàëÂÄëÁöÑÁôºÁèæÈ°ØÁ§∫Ôºå‰ª•ÁñæÁóÖÂàÜÈ°ûÂô®ÁöÑÂΩ¢ÂºèÁ¥çÂÖ• AC ÂèØ‰ª•ÂïüÁî®Á®ΩÊ†∏Ôºå‰ª•Ë≠òÂà•Êõ¥ÂèØÈù†ÁöÑÂ†±ÂëäÔºåËàáÊú™Á∂ìÁØ©ÈÅ∏ÁöÑÁîüÊàêÂ†±ÂëäÁõ∏ÊØîÔºåÊúÉÁî¢ÁîüÊõ¥È´òÁöÑ F1 ÂàÜÊï∏„ÄÇÊ≠§Â§ñÔºåÈÄ≤‰∏ÄÊ≠•Âà©Áî® AC Ê®ôÁ±§ÁöÑ‰ø°ÂøÉÂèØ‰ª•ÊèêÈ´òÁ®ΩÊ†∏ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation**
2407.21633v1 by Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang

Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to
transition to unfamiliar domains without manual annotation or extensive
retraining. Prior research has approached this objective by embedding prompts
into language models (LMs). Common methodologies include integrating prompts at
the input layer or introducing learnable variables at each transformer layer.
Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at
the input layer risk underutilization, with their impact potentially
diminishing across successive transformer layers. Conversely, the addition of
learnable variables to each layer can complicate the training process and
increase inference latency. To tackle the issues mentioned above, this paper
proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture
designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank
Adaptation (LoRA) components, targeting both dialogue context processing and
prompt optimization, to ensure the comprehensive influence of prompts
throughout the transformer model layers. This is achieved without incurring
additional inference latency, showcasing an efficient integration into existing
architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets,
DualLoRA demonstrates notable improvements across multiple domains,
outperforming traditional baseline methods in zero-shot settings. Our code is
accessible at: \url{https://github.com/suntea233/DualLoRA}.

ÊëòË¶ÅÔºöÈõ∂Ê¨°ÁôºË©±ÁãÄÊÖãËøΩËπ§ (DST) Êó®Âú®ËÆìÂ∞çË©±Á≥ªÁµ±ËÉΩÂ§†Âú®Ê≤íÊúâÊâãÂãïË®ªËß£ÊàñÂª£Ê≥õÈáçÊñ∞Ë®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãËΩâÊèõÂà∞‰∏çÁÜüÊÇâÁöÑÈ†òÂüü„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂ÈÄèÈÅéÂ∞áÊèêÁ§∫ÂµåÂÖ•Ë™ûË®ÄÊ®°Âûã (LM) ‰æÜÈÅîÊàêÊ≠§ÁõÆÊ®ô„ÄÇÂ∏∏Ë¶ãÁöÑÊñπÊ≥ïÂåÖÊã¨Âú®Ëº∏ÂÖ•Â±§Êï¥ÂêàÊèêÁ§∫ÊàñÂú®ÊØèÂÄãTransformerÂ±§‰∏≠ÂºïÂÖ•ÂèØÂ≠∏ÁøíËÆäÊï∏„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊØèÁ®ÆÁ≠ñÁï•ÈÉΩÂ±ïÁèæÂá∫Âõ∫ÊúâÁöÑÈôêÂà∂„ÄÇÊï¥ÂêàÂú®Ëº∏ÂÖ•Â±§ÁöÑÊèêÁ§∫ÊúâÊú™ÂÖÖÂàÜÂà©Áî®ÁöÑÈ¢®Èö™ÔºåÂÆÉÂÄëÁöÑÂΩ±ÈüøÂèØËÉΩÊúÉÈö®ËëóÂæåÁ∫åÁöÑTransformerÂ±§ËÄåÈÅûÊ∏õ„ÄÇÁõ∏ÂèçÂú∞ÔºåÂú®ÊØèÂÄãÂ±§‰∏≠Âä†ÂÖ•ÂèØÂ≠∏ÁøíËÆäÊï∏ÊúÉ‰ΩøË®ìÁ∑¥ÈÅéÁ®ãË§áÈõúÂåñ‰∏¶Â¢ûÂä†Êé®Ë´ñÂª∂ÈÅ≤„ÄÇÁÇ∫‰∫ÜËß£Ê±∫‰∏äËø∞ÂïèÈ°åÔºåÊú¨ÊñáÊèêÂá∫ÈõôÈáç‰ΩéÁß©ÈÅ©Êáâ (DualLoRA)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞àÁÇ∫Èõ∂Ê¨° DST Ë®≠Ë®àÁöÑÂç≥ÊèíÂç≥Áî®Êû∂Êßã„ÄÇDualLoRA ÁµêÂêà‰∫ÜÂÖ©ÂÄã‰∏çÂêåÁöÑ‰ΩéÁß©ÈÅ©Êáâ (LoRA) ÂÖÉ‰ª∂ÔºåÈáùÂ∞çÂ∞çË©±ÂÖßÂÆπËôïÁêÜÂíåÊèêÁ§∫ÊúÄ‰Ω≥ÂåñÔºå‰ª•Á¢∫‰øùÊèêÁ§∫Âú®TransformerÊ®°ÂûãÂ±§‰∏≠ÂÖ®Èù¢ÁôºÊèÆÂΩ±ÈüøÂäõ„ÄÇÈÄôÊòØÂú®‰∏çÁî¢ÁîüÈ°çÂ§ñÊé®Ë´ñÂª∂ÈÅ≤ÁöÑÊÉÖÊ≥Å‰∏ãÂØ¶ÁèæÁöÑÔºåÂ±ïÁ§∫‰∫ÜËàáÁèæÊúâÊû∂ÊßãÁöÑÊúâÊïàÊï¥Âêà„ÄÇÈÄèÈÅéÂú® MultiWOZ Âíå SGD Ë≥áÊñôÈõÜ‰∏äÁöÑÂö¥Ê†ºË©ï‰º∞ÔºåDualLoRA Âú®Â§öÂÄãÈ†òÂüüÂ±ïÁèæÂá∫È°ØËëóÁöÑÊîπÈÄ≤ÔºåÂú®Èõ∂Ê¨°Ë®≠ÂÆö‰∏≠ÂÑ™ÊñºÂÇ≥Áµ±ÁöÑÂü∫Ê∫ñÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÊñºÊ≠§ËôïÂèñÂæóÔºö\url{https://github.com/suntea233/DualLoRA}„ÄÇ

##### **TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization Methods**
2407.21630v1 by Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi

Authorship obfuscation aims to disguise the identity of an author within a
text by altering the writing style, vocabulary, syntax, and other linguistic
features associated with the text author. This alteration needs to balance
privacy and utility. While strong obfuscation techniques can effectively hide
the author's identity, they often degrade the quality and usefulness of the
text for its intended purpose. Conversely, maintaining high utility tends to
provide insufficient privacy, making it easier for an adversary to de-anonymize
the author. Thus, achieving an optimal trade-off between these two conflicting
objectives is crucial. In this paper, we propose TAROT: Task-Oriented
Authorship Obfuscation Using Policy Optimization, a new unsupervised authorship
obfuscation method whose goal is to optimize the privacy-utility trade-off by
regenerating the entire text considering its downstream utility. Our approach
leverages policy optimization as a fine-tuning paradigm over small language
models in order to rewrite texts by preserving author identity and downstream
task utility. We show that our approach largely reduce the accuracy of
attackers while preserving utility. We make our code and models publicly
available.

ÊëòË¶ÅÔºö‰ΩúËÄÖÊ∑∑Ê∑ÜÊó®Âú®ÈÄöËøáÊîπÂèò‰∏éÊñáÊú¨‰ΩúËÄÖÁõ∏ÂÖ≥ÁöÑÂÜô‰ΩúÈ£éÊ†º„ÄÅËØçÊ±á„ÄÅËØ≠Ê≥ïÂíåÂÖ∂‰ªñËØ≠Ë®ÄÁâπÂæÅÊù•ÈöêËóè‰ΩúËÄÖÂú®ÊñáÊú¨‰∏≠ÁöÑË∫´‰ªΩ„ÄÇËøôÁßçÊîπÂèòÈúÄË¶ÅÂπ≥Ë°°ÈöêÁßÅÂíåÂÆûÁî®ÊÄß„ÄÇËôΩÁÑ∂Âº∫Â§ßÁöÑÊ∑∑Ê∑ÜÊäÄÊúØÂèØ‰ª•ÊúâÊïàÂú∞ÈöêËóè‰ΩúËÄÖÁöÑË∫´‰ªΩÔºå‰ΩÜÂÆÉ‰ª¨ÈÄöÂ∏∏‰ºöÈôç‰ΩéÊñáÊú¨ÁöÑË¥®ÈáèÂíåÂÆûÁî®ÊÄßÔºå‰ΩøÂÖ∂Êó†Ê≥ïËææÂà∞È¢ÑÊúüÁõÆÁöÑ„ÄÇÁõ∏ÂèçÔºåÁª¥ÊåÅÈ´òÂÆûÁî®ÊÄßÂæÄÂæÄ‰ºöÊèê‰æõ‰∏çË∂≥ÁöÑÈöêÁßÅÔºå‰ΩøÂæóÂØπÊâãÊõ¥ÂÆπÊòìÂØπ‰ΩúËÄÖËøõË°åÂéªÂåøÂêçÂåñ„ÄÇÂõ†Ê≠§ÔºåÂú®‰∏§‰∏™Áõ∏‰∫íÂÜ≤Á™ÅÁöÑÁõÆÊ†á‰πãÈó¥ÂèñÂæóÊúÄ‰Ω≥Âπ≥Ë°°Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü TAROTÔºöÈù¢Âêë‰ªªÂä°ÁöÑ‰ΩúËÄÖÊ∑∑Ê∑ÜÔºå‰ΩøÁî®Á≠ñÁï•‰ºòÂåñÔºåËøôÊòØ‰∏ÄÁßçÊñ∞ÁöÑÊó†ÁõëÁù£‰ΩúËÄÖÊ∑∑Ê∑ÜÊñπÊ≥ïÔºåÂÖ∂ÁõÆÊ†áÊòØÈÄöËøáÈáçÊñ∞ÁîüÊàêËÄÉËôëÂÖ∂‰∏ãÊ∏∏ÂÆûÁî®ÊÄßÁöÑÊï¥‰∏™ÊñáÊú¨Êù•‰ºòÂåñÈöêÁßÅÂÆûÁî®ÊÄßÊùÉË°°„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®Á≠ñÁï•‰ºòÂåñ‰Ωú‰∏∫Â∞èÂûãËØ≠Ë®ÄÊ®°Âûã‰∏äÁöÑÂæÆË∞ÉËåÉÂºèÔºå‰ª•‰æøÈÄöËøá‰øùÁïô‰ΩúËÄÖË∫´‰ªΩÂíå‰∏ãÊ∏∏‰ªªÂä°ÂÆûÁî®ÊÄßÊù•ÈáçÂÜôÊñáÊú¨„ÄÇÊàë‰ª¨Ë°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®‰øùÁïôÂÆûÁî®ÊÄßÁöÑÂêåÊó∂Â§ßÂ§ßÈôç‰Ωé‰∫ÜÊîªÂáªËÄÖÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Â∞ÜÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂíåÊ®°ÂûãÂÖ¨ÂºÄÊèê‰æõ„ÄÇ

##### **Between the AI and Me: Analysing Listeners' Perspectives on AI- and Human-Composed Progressive Metal Music**
2407.21615v1 by Pedro Sarmento, Jackson Loth, Mathieu Barthet

Generative AI models have recently blossomed, significantly impacting
artistic and musical traditions. Research investigating how humans interact
with and deem these models is therefore crucial. Through a listening and
reflection study, we explore participants' perspectives on AI- vs
human-generated progressive metal, in symbolic format, using rock music as a
control group. AI-generated examples were produced by ProgGP, a
Transformer-based model. We propose a mixed methods approach to assess the
effects of generation type (human vs. AI), genre (progressive metal vs. rock),
and curation process (random vs. cherry-picked). This combines quantitative
feedback on genre congruence, preference, creativity, consistency, playability,
humanness, and repeatability, and qualitative feedback to provide insights into
listeners' experiences. A total of 32 progressive metal fans completed the
study. Our findings validate the use of fine-tuning to achieve genre-specific
specialization in AI music generation, as listeners could distinguish between
AI-generated rock and progressive metal. Despite some AI-generated excerpts
receiving similar ratings to human music, listeners exhibited a preference for
human compositions. Thematic analysis identified key features for genre and AI
vs. human distinctions. Finally, we consider the ethical implications of our
work in promoting musical data diversity within MIR research by focusing on an
under-explored genre.

ÊëòË¶ÅÔºöÁîüÊàêÂºè AI Ê®°ÂûãËøëÊúüËì¨ÂãÉÁôºÂ±ïÔºåÂ∞çËóùË°ìÂíåÈü≥Ê®ÇÂÇ≥Áµ±Áî¢ÁîüÈáçÂ§ßÂΩ±Èüø„ÄÇÂõ†Ê≠§ÔºåÊé¢Ë®é‰∫∫È°ûÂ¶Ç‰ΩïËàáÈÄô‰∫õÊ®°Âûã‰∫íÂãï‰∏¶Ë©ïÊñ∑ÂÆÉÂÄëÁöÑÁ†îÁ©∂Ëá≥ÈóúÈáçË¶Å„ÄÇÈÄèÈÅéËÅÜËÅΩËàáÂèçÊÄùÁ†îÁ©∂ÔºåÊàëÂÄë‰ª•Á¨¶ËôüÊ†ºÂºèÊé¢Ë®éÂèÉËàáËÄÖÂ∞ç AI Ëàá‰∫∫È°ûÁî¢ÁîüÁöÑÂâçË°õÈáëÂ±¨Èü≥Ê®ÇÁöÑËßÄÈªûÔºå‰∏¶‰ª•ÊêñÊªæÈü≥Ê®Ç‰ΩúÁÇ∫Â∞çÁÖßÁµÑ„ÄÇAI ÁîüÊàêÁöÑÁØÑ‰æãÁî± ProgGPÔºå‰∏ÄÂÄãÂü∫Êñº Transformer ÁöÑÊ®°ÂûãË£Ω‰Ωú„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ∑∑ÂêàÊñπÊ≥ï‰æÜË©ï‰º∞ÁîüÊàêÈ°ûÂûãÔºà‰∫∫È°ûËàá AIÔºâ„ÄÅÈ°ûÂûãÔºàÂâçË°õÈáëÂ±¨ËàáÊêñÊªæÔºâÂíåÁ≠ñÂ±ïÊµÅÁ®ãÔºàÈö®Ê©üËàáÁ≤æÈÅ∏ÔºâÁöÑÂΩ±Èüø„ÄÇÈÄôÁµêÂêà‰∫ÜÂ∞çÈ°ûÂûã‰∏ÄËá¥ÊÄß„ÄÅÂÅèÂ•Ω„ÄÅÂâµÈÄ†Âäõ„ÄÅ‰∏ÄËá¥ÊÄß„ÄÅÂèØÊºîÂ•èÊÄß„ÄÅ‰∫∫ÊÄßÂåñÂíåÂèØÈáçË§áÊÄßÁöÑÈáèÂåñÂõûÈ•ãÔºå‰ª•ÂèäÂÆöÊÄßÂõûÈ•ãÔºå‰ª•Êèê‰æõÂ∞çËÅΩÁúæÈ´îÈ©óÁöÑË¶ãËß£„ÄÇÂÖ±Êúâ 32 ‰ΩçÂâçË°õÈáëÂ±¨Ê®ÇËø∑ÂÆåÊàêÈÄôÈ†ÖÁ†îÁ©∂„ÄÇÊàëÂÄëÁöÑÁôºÁèæÈ©óË≠â‰∫ÜÂæÆË™øÂú® AI Èü≥Ê®ÇÁîüÊàê‰∏≠ÂØ¶ÁèæÁâπÂÆöÈ°ûÂûãÂ∞àÊ•≠ÂåñÁöÑÁî®ÈÄîÔºåÂõ†ÁÇ∫ËÅΩÁúæÂèØ‰ª•ÂçÄÂàÜ AI ÁîüÊàêÁöÑÊêñÊªæÂíåÂâçË°õÈáëÂ±¨„ÄÇÂÑòÁÆ°‰∏Ä‰∫õ AI ÁîüÊàêÁöÑÁâáÊÆµÁç≤ÂæóËàá‰∫∫È°ûÈü≥Ê®ÇÈ°û‰ººÁöÑË©ïÂàÜÔºå‰ΩÜËÅΩÁúæË°®ÁèæÂá∫Â∞ç‰∫∫È°û‰ΩúÂìÅÁöÑÂÅèÂ•Ω„ÄÇ‰∏ªÈ°åÂàÜÊûêË≠òÂà•Âá∫È°ûÂûãÂíå AI Ëàá‰∫∫È°ûÂçÄÂà•ÁöÑ‰∏ªË¶ÅÁâπÂæµ„ÄÇÊúÄÂæåÔºåÊàëÂÄëËÄÉÊÖÆ‰∫ÜÊàëÂÄëÁöÑÂ∑•‰ΩúÂú®‰øÉÈÄ≤ MIR Á†îÁ©∂‰∏≠ÁöÑÈü≥Ê®ÇË≥áÊñôÂ§öÊ®£ÊÄßÊñπÈù¢ÁöÑÂÄ´ÁêÜÂΩ±ÈüøÔºåÈáçÈªûÊîæÂú®‰∏ÄÂÄãÊé¢Á¥¢‰∏çË∂≥ÁöÑÈ°ûÂûã‰∏ä„ÄÇ

##### **Enhancing Partially Spoofed Audio Localization with Boundary-aware Attention Mechanism**
2407.21611v1 by Jiafeng Zhong, Bin Li, Jiangyan Yi

The task of partially spoofed audio localization aims to accurately determine
audio authenticity at a frame level. Although some works have achieved
encouraging results, utilizing boundary information within a single model
remains an unexplored research topic. In this work, we propose a novel method
called Boundary-aware Attention Mechanism (BAM). Specifically, it consists of
two core modules: Boundary Enhancement and Boundary Frame-wise Attention. The
former assembles the intra-frame and inter-frame information to extract
discriminative boundary features that are subsequently used for boundary
position detection and authenticity decision, while the latter leverages
boundary prediction results to explicitly control the feature interaction
between frames, which achieves effective discrimination between real and fake
frames. Experimental results on PartialSpoof database demonstrate our proposed
method achieves the best performance. The code is available at
https://github.com/media-sec-lab/BAM.

ÊëòË¶ÅÔºöÈÉ®ÂàÜÊ¨∫È™óÈü≥È¢ëÂÆö‰ΩçÁöÑ‰ªªÂä°Êó®Âú®ÂáÜÁ°ÆÁ°ÆÂÆöÂ∏ßÁ∫ßÂà´ÁöÑÈü≥È¢ëÁúüÂÆûÊÄß„ÄÇÂ∞ΩÁÆ°‰∏Ä‰∫õ‰ΩúÂìÅÂèñÂæó‰∫Ü‰ª§‰∫∫ÈºìËàûÁöÑÁªìÊûúÔºå‰ΩÜÂú®Âçï‰∏™Ê®°Âûã‰∏≠Âà©Áî®ËæπÁïå‰ø°ÊÅØ‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™Â∞öÊú™Êé¢Á¥¢ÁöÑÁ†îÁ©∂ËØæÈ¢ò„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ËæπÁïåÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂ (BAM) ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂÆÉÂåÖÂê´‰∏§‰∏™Ê†∏ÂøÉÊ®°ÂùóÔºöËæπÁïåÂ¢ûÂº∫ÂíåËæπÁïåÈÄêÂ∏ßÊ≥®ÊÑèÂäõ„ÄÇÂâçËÄÖÊ±áÈõÜÂ∏ßÂÜÖÂíåÂ∏ßÈó¥‰ø°ÊÅØ‰ª•ÊèêÂèñÂà§Âà´ËæπÁïåÁâπÂæÅÔºåÈöèÂêéÁî®‰∫éËæπÁïå‰ΩçÁΩÆÊ£ÄÊµãÂíåÁúüÂÆûÊÄßÂÜ≥Á≠ñÔºåËÄåÂêéËÄÖÂà©Áî®ËæπÁïåÈ¢ÑÊµãÁªìÊûúÊòæÂºèÊéßÂà∂Â∏ß‰πãÈó¥ÁöÑÁâπÂæÅ‰∫§‰∫íÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÂØπÁúüÂÆûÂ∏ßÂíåËôöÂÅáÂ∏ßÁöÑÊúâÊïàÂå∫ÂàÜ„ÄÇPartialSpoof Êï∞ÊçÆÂ∫ì‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïÂèñÂæó‰∫ÜÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/media-sec-lab/BAM Ëé∑Âæó„ÄÇ

##### **Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative Priors**
2407.21600v1 by Shoujin Huang, Guanxiong Luo, Yuwan Wang, Kexin Yang, Lingyan Zhang, Jingzhe Liu, Hua Guo, Min Wang, Mengye Lyu

Simultaneous multislice (SMS) imaging is a powerful technique for
accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS
reconstruction remains challenging due to the complex signal interactions
between and within the excited slices. This study presents a robust SMS MRI
reconstruction method using deep generative priors. Starting from Gaussian
noise, we leverage denoising diffusion probabilistic models (DDPM) to gradually
recover the individual slices through reverse diffusion iterations while
imposing data consistency from the measured k-space under readout concatenation
framework. The posterior sampling procedure is designed such that the DDPM
training can be performed on single-slice images without special adjustments
for SMS tasks. Additionally, our method integrates a low-frequency enhancement
(LFE) module to address a practical issue that SMS-accelerated fast spin echo
(FSE) and echo-planar imaging (EPI) sequences cannot easily embed
autocalibration signals. Extensive experiments demonstrate that our approach
consistently outperforms existing methods and generalizes well to unseen
datasets. The code is available at https://github.com/Solor-pikachu/ROGER after
the review process.

ÊëòË¶ÅÔºöÂêåÊôÇÂ§öÂàáÁâá (SMS) ÂΩ±ÂÉèÊòØ‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÊäÄË°ìÔºåÁî®ÊñºÂä†ÈÄüÁ£ÅÊåØÈÄ†ÂΩ± (MRI) ÁöÑÊì∑Âèñ„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊøÄÁôºÂàáÁâá‰πãÈñìÂíå‰πãÂÖßÁöÑË§áÈõúË®äËôü‰∫§‰∫í‰ΩúÁî®ÔºåSMS ÈáçÂª∫‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Ê∑±Â∫¶ÁîüÊàêÂÖàÈ©óÁöÑÂº∑ÂÅ• SMS MRI ÈáçÂª∫ÊñπÊ≥ï„ÄÇÂæûÈ´òÊñØÈõúË®äÈñãÂßãÔºåÊàëÂÄëÂà©Áî®ÂéªÂô™Êì¥Êï£Ê©üÁéáÊ®°Âûã (DDPM) ÈÄèÈÅéÂèçÂêëÊì¥Êï£ÂèçË¶ÜÈÅãÁÆó‰æÜÈÄêÊº∏ÊÅ¢Âæ©ÂÄãÂà•ÂàáÁâáÔºåÂêåÊôÇÂú®ËÆÄÂèñ‰∏≤Êé•Êû∂Êßã‰∏ãÊñΩÂä†Ê∏¨Èáè k Á©∫ÈñìÁöÑË≥áÊñô‰∏ÄËá¥ÊÄß„ÄÇÂæåÈ©óÊäΩÊ®£Á®ãÂ∫èÁöÑË®≠Ë®à‰ΩøÂæó DDPM Ë®ìÁ∑¥ÂèØ‰ª•Âú®ÂñÆÂàáÁâáÂΩ±ÂÉè‰∏äÂü∑Ë°åÔºåÁÑ°ÈúÄÈáùÂ∞ç SMS ‰ªªÂãôÈÄ≤Ë°åÁâπÊÆäË™øÊï¥„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊï¥Âêà‰∫Ü‰∏ÄÂÄã‰ΩéÈ†ªÂ¢ûÂº∑ (LFE) Ê®°ÁµÑÔºå‰ª•Ëß£Ê±∫‰∏ÄÂÄãÂØ¶ÈöõÂïèÈ°åÔºåÂç≥ SMS Âä†ÈÄüÁöÑÂø´ÈÄüËá™ÊóãÂõûÊ≥¢ (FSE) ÂíåÂõûÊ≥¢Âπ≥Èù¢ÂΩ±ÂÉè (EPI) Â∫èÂàóÁÑ°Ê≥ïËºïÈ¨ÜÂµåÂÖ•Ëá™ÂãïÊ†°Ê≠£Ë®äËôü„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂßãÁµÇÂÑ™ÊñºÁèæÊúâÊñπÊ≥ïÔºå‰∏¶‰∏îÂèØ‰ª•ÂæàÂ•ΩÂú∞Êé®Âª£Âà∞Êú™Ë¶ãÁöÑË≥áÊñôÈõÜ„ÄÇÁ®ãÂºèÁ¢ºÂú®ÂØ©Êü•Á®ãÂ∫èÂæåÂèØÊñº https://github.com/Solor-pikachu/ROGER ÂèñÂæó„ÄÇ

##### **Voxel Scene Graph for Intracranial Hemorrhage**
2407.21580v1 by Antoine P. Sanner, Nils F. Grauhan, Marc A. Brockmann, Ahmed E. Othman, Anirban Mukhopadhyay

Patients with Intracranial Hemorrhage (ICH) face a potentially
life-threatening condition, and patient-centered individualized treatment
remains challenging due to possible clinical complications. Deep-Learning-based
methods can efficiently analyze the routinely acquired head CTs to support the
clinical decision-making. The majority of early work focuses on the detection
and segmentation of ICH, but do not model the complex relations between ICH and
adjacent brain structures. In this work, we design a tailored object detection
method for ICH, which we unite with segmentation-grounded Scene Graph
Generation (SGG) methods to learn a holistic representation of the clinical
cerebral scene. To the best of our knowledge, this is the first application of
SGG for 3D voxel images. We evaluate our method on two head-CT datasets and
demonstrate that our model can recall up to 74% of clinically relevant
relations. This work lays the foundation towards SGG for 3D voxel data. The
generated Scene Graphs can already provide insights for the clinician, but are
also valuable for all downstream tasks as a compact and interpretable
representation.

ÊëòË¶ÅÔºöËÖ¶Âá∫Ë°Ä (ICH) ÊÇ£ËÄÖÈù¢Ëá®ÂèØËÉΩÂç±ÂèäÁîüÂëΩÁöÑÁãÄÊ≥ÅÔºåÁî±ÊñºÂèØËÉΩÁöÑËá®Â∫ä‰ΩµÁôºÁóáÔºå‰ª•ÊÇ£ËÄÖÁÇ∫‰∏≠ÂøÉÁöÑÂÄã‰∫∫ÂåñÊ≤ªÁôÇ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂèØ‰ª•ÊúâÊïàÂàÜÊûêÂ∏∏Ë¶èÁç≤ÂæóÁöÑÈ†≠ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÔºå‰ª•ÊîØÊåÅËá®Â∫äÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÂ§ßÂ§öÊï∏Êó©ÊúüÂ∑•‰ΩúÈÉΩÈõÜ‰∏≠Âú® ICH ÁöÑÊ™¢Ê∏¨ÂíåÂàÜÂâ≤Ôºå‰ΩÜÊ≤íÊúâÂ∞ç ICH ÂíåÁõ∏ÈÑ∞Â§ßËÖ¶ÁµêÊßã‰πãÈñìÁöÑË§áÈõúÈóú‰øÇÈÄ≤Ë°åÂª∫Ê®°„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÈáùÂ∞ç ICH ÁöÑÂÆ¢Ë£ΩÂåñÁõÆÊ®ôÊ™¢Ê∏¨ÊñπÊ≥ïÔºåÊàëÂÄëÂ∞áÂÖ∂ËàáÂü∫ÊñºÂàÜÂâ≤ÁöÑÂ†¥ÊôØÂúñÁîüÊàê (SGG) ÊñπÊ≥ïÁµêÂêàÔºå‰ª•Â≠∏ÁøíËá®Â∫äËÖ¶ÈÉ®Â†¥ÊôØÁöÑÊï¥È´îË°®Âæµ„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØ SGG Á¨¨‰∏ÄÊ¨°ÊáâÁî®Êñº 3D È´îÁ¥†ÂΩ±ÂÉè„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÈ†≠ÈÉ®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÊï∏ÊìöÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÊ®°ÂûãÔºå‰∏¶Ë≠âÊòéÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Âè¨ÂõûÈ´òÈÅî 74% ÁöÑËá®Â∫äÁõ∏ÈóúÈóú‰øÇ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁÇ∫ 3D È´îÁ¥†Êï∏ÊìöÁöÑ SGG Â•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇÁîüÊàêÁöÑÂ†¥ÊôØÂúñÂ∑≤Á∂ìÂèØ‰ª•ÁÇ∫Ëá®Â∫äÈÜ´ÁîüÊèê‰æõË¶ãËß£Ôºå‰ΩÜÂ∞çÊñºÊâÄÊúâ‰∏ãÊ∏∏‰ªªÂãôËÄåË®ÄÔºåÂÆÉ‰πüÊòØ‰∏ÄÁ®ÆÁ≤æÁ∞°‰∏îÂèØËß£ÈáãÁöÑË°®ÂæµÔºåÂõ†Ê≠§ÈùûÂ∏∏ÊúâÂÉπÂÄº„ÄÇ

##### **A Performance Study of LLM-Generated Code on Leetcode**
2407.21579v1 by Tristan Coignion, Cl√©ment Quinton, Romain Rouvoy

This study evaluates the efficiency of code generation by Large Language
Models (LLMs) and measures their performance against human-crafted solutions
using a dataset from Leetcode. We compare 18 LLMs, considering factors such as
model temperature and success rate, and their impact on code performance. This
research introduces a novel method for measuring and comparing the speed of
LLM-generated code, revealing that LLMs produce code with comparable
performance, irrespective of the adopted LLM. We also find that LLMs are
capable of generating code that is, on average, more efficient than the code
written by humans. The paper further discusses the use of Leetcode as a
benchmarking dataset, the limitations imposed by potential data contamination,
and the platform's measurement reliability. We believe that our findings
contribute to a better understanding of LLM capabilities in code generation and
set the stage for future optimizations in the field.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Ë©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁ®ãÂºèÁ¢ºÁî¢ÁîüÊïàÁéáÔºå‰∏¶‰ΩøÁî®‰æÜËá™ Leetcode ÁöÑË≥áÊñôÈõÜË°°ÈáèÂÆÉÂÄëËàá‰∫∫Â∑•Ë£Ω‰ΩúÁöÑËß£Ê±∫ÊñπÊ°àÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÊØîËºÉ‰∫Ü 18 ÂÄã LLMÔºåËÄÉÈáè‰∫ÜÊ®°ÂûãÊ∫´Â∫¶ÂíåÊàêÂäüÁéáÁ≠âÂõ†Á¥†Ôºå‰ª•ÂèäÂÆÉÂÄëÂ∞çÁ®ãÂºèÁ¢ºÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ï‰æÜË°°ÈáèÂíåÊØîËºÉ LLM ÁîüÊàêÁöÑÁ®ãÂºèÁ¢ºÁöÑÈÄüÂ∫¶ÔºåÊè≠Á§∫‰∫Ü LLM Áî¢ÁîüÁöÑÁ®ãÂºèÁ¢ºÂÖ∑ÊúâÂèØÊØîËºÉÁöÑÊïàËÉΩÔºåËÄåËàáÊé°Áî®ÁöÑ LLM ÁÑ°Èóú„ÄÇÊàëÂÄëÈÇÑÁôºÁèæÔºåLLM ËÉΩÂ§†Áî¢ÁîüÁöÑÁ®ãÂºèÁ¢ºÂπ≥ÂùáËÄåË®ÄÊØî‰∫∫Â∑•Á∑®ÂØ´ÁöÑÁ®ãÂºèÁ¢ºÊõ¥ÊúâÊïàÁéá„ÄÇÊú¨ÊñáÈÄ≤‰∏ÄÊ≠•Ë®éË´ñ‰∫Ü‰ΩøÁî® Leetcode ‰ΩúÁÇ∫Âü∫Ê∫ñË≥áÊñôÈõÜ„ÄÅÊΩõÂú®Ë≥áÊñôÊ±°ÊüìÊâÄÈÄ†ÊàêÁöÑÈôêÂà∂Ôºå‰ª•ÂèäË©≤Âπ≥Âè∞ÁöÑÊ∏¨ÈáèÂèØÈù†ÊÄß„ÄÇÊàëÂÄëÁõ∏‰ø°ÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÊúâÂä©ÊñºÊõ¥‰∫ÜËß£ LLM Âú®Á®ãÂºèÁ¢ºÁî¢ÁîüÊñπÈù¢ÁöÑËÉΩÂäõÔºå‰∏¶ÁÇ∫Ë©≤È†òÂüüÁöÑÊú™‰æÜÊúÄ‰Ω≥ÂåñÂ•†ÂÆöÂü∫Á§é„ÄÇ

##### **Multi-Site Class-Incremental Learning with Weighted Experts in Echocardiography**
2407.21577v1 by Kit M. Bransby, Woo-jin Cho Kim, Jorge Oliveira, Alex Thorley, Arian Beqiri, Alberto Gomez, Agisilaos Chartsias

Building an echocardiography view classifier that maintains performance in
real-life cases requires diverse multi-site data, and frequent updates with
newly available data to mitigate model drift. Simply fine-tuning on new
datasets results in "catastrophic forgetting", and cannot adapt to variations
of view labels between sites. Alternatively, collecting all data on a single
server and re-training may not be feasible as data sharing agreements may
restrict image transfer, or datasets may only become available at different
times. Furthermore, time and cost associated with re-training grows with every
new dataset. We propose a class-incremental learning method which learns an
expert network for each dataset, and combines all expert networks with a score
fusion model. The influence of ``unqualified experts'' is minimised by
weighting each contribution with a learnt in-distribution score. These weights
promote transparency as the contribution of each expert is known during
inference. Instead of using the original images, we use learned features from
each dataset, which are easier to share and raise fewer licensing and privacy
concerns. We validate our work on six datasets from multiple sites,
demonstrating significant reductions in training time while improving view
classification performance.

ÊëòË¶ÅÔºöÂª∫Á´ã‰∏ÄÂÄãÂú®ÂØ¶ÈöõÊÉÖÊ≥Å‰∏≠ËÉΩÁ∂≠ÊåÅÊïàËÉΩÁöÑÂøÉËáüË∂ÖÈü≥Ê≥¢Ê™¢Ë¶ñÂàÜÈ°ûÂô®ÔºåÈúÄË¶ÅÂ§öÊ®£ÂåñÁöÑÂ§öÁ´ôÈªûË≥áÊñôÔºå‰ª•ÂèäÈ†ªÁπÅ‰ΩøÁî®Êñ∞ÂèñÂæóÁöÑË≥áÊñôÈÄ≤Ë°åÊõ¥Êñ∞Ôºå‰ª•Ê∏õËºïÊ®°ÂûãÊºÇÁßª„ÄÇÂñÆÁ¥îÈáùÂ∞çÊñ∞ÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÂæÆË™øÊúÉÂ∞éËá¥„ÄåÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄçÔºå‰∏îÁÑ°Ê≥ïÈÅ©ÊáâÁ´ôÈªû‰πãÈñìË¶ñÂúñÊ®ôÁ±§ÁöÑÂ∑ÆÁï∞„ÄÇÊàñËÄÖÔºåÂú®ÂñÆ‰∏Ä‰º∫ÊúçÂô®‰∏äÊî∂ÈõÜÊâÄÊúâË≥áÊñô‰∏¶ÈáçÊñ∞Ë®ìÁ∑¥ÂèØËÉΩ‰∏çÂèØË°åÔºåÂõ†ÁÇ∫Ë≥áÊñôÂàÜ‰∫´ÂçîË≠∞ÂèØËÉΩÊúÉÈôêÂà∂ÂΩ±ÂÉèÂÇ≥Ëº∏ÔºåÊàñË≥áÊñôÈõÜÂèØËÉΩÂè™Âú®‰∏çÂêåÁöÑÊôÇÈñìÈªûÂèñÂæó„ÄÇÊ≠§Â§ñÔºåÊØèÊ¨°‰ΩøÁî®Êñ∞ÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åÈáçÊñ∞Ë®ìÁ∑¥ÊâÄËä±Ë≤ªÁöÑÊôÇÈñìÂíåÊàêÊú¨ÈÉΩÊúÉÂ¢ûÂä†„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÈ°ûÂà•ÈÅûÂ¢ûÂ≠∏ÁøíÊñπÊ≥ïÔºåÊúÉÁÇ∫ÊØèÂÄãË≥áÊñôÈõÜÂ≠∏Áøí‰∏ÄÂÄãÂ∞àÂÆ∂Á∂≤Ë∑ØÔºå‰∏¶Â∞áÊâÄÊúâÂ∞àÂÆ∂Á∂≤Ë∑ØËàá‰∏ÄÂÄãÂàÜÊï∏ËûçÂêàÊ®°ÂûãÁµêÂêà„ÄÇÈÄèÈÅé‰ΩøÁî®Â≠∏ÁøíÂà∞ÁöÑÂàÜ‰ΩàÂÖßÂàÜÊï∏Â∞çÊØèÂÄãË≤¢ÁçªÈÄ≤Ë°åÂä†Ê¨äÔºåÂèØ‰ª•Â∞á„Äå‰∏çÂêàÊ†ºÂ∞àÂÆ∂„ÄçÁöÑÂΩ±ÈüøÈôçÂà∞ÊúÄ‰Ωé„ÄÇÈÄô‰∫õÊ¨äÈáçÊúÉÊèêÂçáÈÄèÊòéÂ∫¶ÔºåÂõ†ÁÇ∫Âú®Êé®Ë´ñÈÅéÁ®ã‰∏≠ÊúÉÁü•ÈÅìÊØèÂÄãÂ∞àÂÆ∂ÁöÑË≤¢Áçª„ÄÇÊàëÂÄë‰∏ç‰ΩøÁî®ÂéüÂßãÂΩ±ÂÉèÔºåËÄåÊòØ‰ΩøÁî®ÂæûÊØèÂÄãË≥áÊñôÈõÜ‰∏≠Â≠∏ÁøíÂà∞ÁöÑÁâπÂæµÔºåÈÄô‰∫õÁâπÂæµÊõ¥ÂÆπÊòìÂàÜ‰∫´Ôºå‰∏îÊúÉÂºïÁôºËºÉÂ∞ëÁöÑÊéàÊ¨äÂíåÈö±ÁßÅÂïèÈ°å„ÄÇÊàëÂÄëÂú®Â§öÂÄãÁ´ôÈªûÁöÑÂÖ≠ÂÄãË≥áÊñôÈõÜ‰∏äÈ©óË≠âÊàëÂÄëÁöÑÊàêÊûúÔºåË≠âÊòéÂú®ÊîπÂñÑË¶ñÂúñÂàÜÈ°ûÊïàËÉΩÁöÑÂêåÊôÇÔºåÂ§ßÂπÖÊ∏õÂ∞ë‰∫ÜË®ìÁ∑¥ÊôÇÈñì„ÄÇ

##### **PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning**
2407.21571v1 by Min Jae Jung, JooHee Kim

Large Language Models (LLMs) encounter significant challenges in continual
learning due to catastrophic forgetting, where new information overwrites
previously acquired knowledge. This limitation leads to substantial
environmental and economic waste. In this study, we introduce the PMoE,
Progressive Mixture of Experts with Asymmetric Transformer, which aims to
minimize forgetting by utilizing an asymmetric design with shallow layers
dedicated to general knowledge and deep layers for new knowledge. PMoE
incorporates progressively added experts in deep layers and a router that
allocates new knowledge to the appropriate experts efficiently. The router,
positioned adjacent to the deep layers, utilizes deep features aggregating
consolidated information. This enables the router to perform efficiently,
allocating new knowledge to the appropriate experts, which progressively
increase in the deep layers. Extensive experiments on TRACE datasets and
general language understanding datasets demonstrate that the proposed PMoE
outperforms previous state-of-the-art approaches.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÊåÅÁ∫åÂ≠∏Áøí‰∏≠ÊúÉÈÅ≠ÈÅáÈáçÂ§ßÊåëÊà∞ÔºåÂõ†ÁÇ∫ÁÅΩÈõ£ÊÄßÈÅ∫ÂøòÊúÉÂ∞éËá¥Êñ∞Ë≥áË®äË¶ÜËìãÂÖàÂâçÁç≤ÂæóÁöÑÁü•Ë≠ò„ÄÇÊ≠§ÈôêÂà∂ÊúÉÈÄ†ÊàêÂ§ßÈáèÁöÑÁí∞Â¢ÉÂíåÁ∂ìÊøüÊµ™Ë≤ª„ÄÇÊú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü PMoEÔºå‰∏ÄÁ®ÆÈùûÂ∞çÁ®±TransformerÁöÑÂ∞àÂÆ∂Êº∏ÈÄ≤Ê∑∑ÂêàÔºåÂÖ∂ÁõÆÊ®ôÊòØÈÄèÈÅé‰ΩøÁî®ÈùûÂ∞çÁ®±Ë®≠Ë®à‰æÜÊúÄÂ∞èÂåñÈÅ∫ÂøòÔºåÂÖ∂‰∏≠Ê∑∫Â±§Â∞àÈñÄÁî®Êñº‰∏ÄËà¨Áü•Ë≠òÔºåËÄåÊ∑±Â±§ÂâáÁî®ÊñºÊñ∞Áü•Ë≠ò„ÄÇPMoE Âú®Ê∑±Â±§‰∏≠Âä†ÂÖ•Êº∏ÈÄ≤Êñ∞Â¢ûÁöÑÂ∞àÂÆ∂Ôºå‰ª•Âèä‰∏ÄÂÄãË∑ØÁî±Âô®ÔºåÂèØÊúâÊïàÂú∞Â∞áÊñ∞Áü•Ë≠òÂàÜÈÖçÁµ¶ÈÅ©Áï∂ÁöÑÂ∞àÂÆ∂„ÄÇË∑ØÁî±Âô®ËàáÊ∑±Â±§Áõ∏ÈÑ∞Ôºå‰ΩøÁî®Êï¥ÂêàÂæåÁöÑË≥áË®äÂΩôÁ∏ΩÊ∑±Â∫¶ÁâπÂæµ„ÄÇÈÄôËÆìË∑ØÁî±Âô®Âæó‰ª•ÊúâÊïàÂü∑Ë°åÔºåÂ∞áÊñ∞Áü•Ë≠òÂàÜÈÖçÁµ¶ÈÅ©Áï∂ÁöÑÂ∞àÂÆ∂ÔºåËÄåÈÄô‰∫õÂ∞àÂÆ∂ÊúÉÂú®Ê∑±Â±§‰∏≠ÈÄêÊº∏Â¢ûÂä†„ÄÇÂú® TRACE Ë≥áÊñôÈõÜÂíå‰∏ÄËà¨Ë™ûË®ÄÁêÜËß£Ë≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòéÔºåÊâÄÊèêÂá∫ÁöÑ PMoE ÂÑ™ÊñºÂÖàÂâçÁöÑÊúÄÂÖàÈÄ≤ÊñπÊ≥ï„ÄÇ

##### **Generative Sentiment Analysis via Latent Category Distribution and Constrained Decoding**
2407.21560v1 by Jun Zhou, Dongyang Yu, Kamran Aziz, Fangfang Su, Qing Zhang, Fei Li, Donghong Ji

Fine-grained sentiment analysis involves extracting and organizing sentiment
elements from textual data. However, existing approaches often overlook issues
of category semantic inclusion and overlap, as well as inherent structural
patterns within the target sequence. This study introduces a generative
sentiment analysis model. To address the challenges related to category
semantic inclusion and overlap, a latent category distribution variable is
introduced. By reconstructing the input of a variational autoencoder, the model
learns the intensity of the relationship between categories and text, thereby
improving sequence generation. Additionally, a trie data structure and
constrained decoding strategy are utilized to exploit structural patterns,
which in turn reduces the search space and regularizes the generation process.
Experimental results on the Restaurant-ACOS and Laptop-ACOS datasets
demonstrate a significant performance improvement compared to baseline models.
Ablation experiments further confirm the effectiveness of latent category
distribution and constrained decoding strategy.

ÊëòË¶ÅÔºöÁ¥∞Á≤íÂ∫¶ÊÉÖÁ∑íÂàÜÊûêÊ∂âÂèäÂæûÊñáÊú¨Ë≥áÊñô‰∏≠ËêÉÂèñÂíåÁµÑÁπîÊÉÖÁ∑íÂÖÉÁ¥†„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÊñπÊ≥ïÁ∂ìÂ∏∏ÂøΩÁï•È°ûÂà•Ë™ûÁæ©ÂåÖÂê´ÂíåÈáçÁñäÁöÑÂïèÈ°åÔºå‰ª•ÂèäÁõÆÊ®ôÂ∫èÂàóÂÖßÂú®ÁöÑÁµêÊßãÊ®°Âºè„ÄÇÊú¨Á†îÁ©∂ÂºïÂÖ•‰∏ÄÂÄãÁîüÊàêÂºèÊÉÖÁ∑íÂàÜÊûêÊ®°Âûã„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çËàáÈ°ûÂà•Ë™ûÁæ©ÂåÖÂê´ÂíåÈáçÁñäÁõ∏ÈóúÁöÑÊåëÊà∞ÔºåÂºïÂÖ•‰∫ÜÊΩõÂú®È°ûÂà•ÂàÜ‰ΩàËÆäÊï∏„ÄÇÈÄèÈÅéÈáçÂª∫ËÆäÁï∞Ëá™ÂãïÁ∑®Á¢ºÂô®ÁöÑËº∏ÂÖ•ÔºåÊ®°ÂûãÂ≠∏ÁøíÈ°ûÂà•ËàáÊñáÂ≠ó‰πãÈñìÈóú‰øÇÁöÑÂº∑Â∫¶ÔºåÂæûËÄåÊîπÂñÑÂ∫èÂàóÁîüÊàê„ÄÇÊ≠§Â§ñÔºåÂà©Áî®Ê®πÁãÄÁµêÊßãÂíåÁ¥ÑÊùüÂºèËß£Á¢ºÁ≠ñÁï•‰æÜÂà©Áî®ÁµêÊßãÊ®°ÂºèÔºåÈÄ≤ËÄåÊ∏õÂ∞ëÊêúÂ∞ãÁ©∫Èñì‰∏¶Ë¶èÁØÑÁîüÊàêÈÅéÁ®ã„ÄÇÂú® Restaurant-ACOS Âíå Laptop-ACOS Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéËàáÂü∫Á∑öÊ®°ÂûãÁõ∏ÊØîÊúâÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçá„ÄÇÊ∂àËûçÂØ¶È©óÈÄ≤‰∏ÄÊ≠•Á¢∫Ë™ç‰∫ÜÊΩõÂú®È°ûÂà•ÂàÜ‰ΩàÂíåÁ¥ÑÊùüÂºèËß£Á¢ºÁ≠ñÁï•ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Operator-based semantics for choice programs: is choosing losing? (full version)**
2407.21556v1 by Jesse Heyninck

Choice constructs are an important part of the language of logic programming,
yet the study of their semantics has been a challenging task. So far, only
two-valued semantics have been studied, and the different proposals for such
semantics have not been compared in a principled way. In this paper, an
operator-based framework allow for the definition and comparison of different
semantics in a principled way is proposed.

ÊëòË¶ÅÔºöÈÅ∏ÊìáÂª∫ÊßãÊòØÈÇèËºØÁ®ãÂºèË®≠Ë®àË™ûË®Ä‰∏≠ÈáçË¶ÅÁöÑÈÉ®ÂàÜÔºå
ÁÑ∂ËÄåÁ†îÁ©∂ÂÖ∂Ë™ûÊÑèÂ≠∏‰∏ÄÁõ¥ÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÁõÆÂâçÁÇ∫Ê≠¢Ôºå
ÂÉÖÁ†îÁ©∂‰∫Ü‰∫åÂÄºË™ûÊÑèÂ≠∏Ôºå‰∏îÂ∞çÊñºÊ≠§È°ûË™ûÊÑèÂ≠∏ÁöÑ‰∏çÂêåÊèêÊ°àÂ∞öÊú™‰ª•ÊúâÂéüÂâáÁöÑÊñπÂºèÈÄ≤Ë°åÊØîËºÉ„ÄÇÊú¨Êñá‰∏≠Ôºå
ÊèêÂá∫‰∫ÜÂü∫ÊñºÈÅãÁÆóÂ≠êÁöÑÊû∂ÊßãÔºåÂÖÅË®±‰ª•ÊúâÂéüÂâáÁöÑÊñπÂºèÂÆöÁæ©ÂíåÊØîËºÉ‰∏çÂêåÁöÑË™ûÊÑèÂ≠∏„ÄÇ

##### **Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition**
2407.21536v1 by Jiang Li, Xiaoping Wang, Zhigang Zeng

Multimodal emotion recognition in conversation (MERC) has garnered
substantial research attention recently. Existing MERC methods face several
challenges: (1) they fail to fully harness direct inter-modal cues, possibly
leading to less-than-thorough cross-modal modeling; (2) they concurrently
extract information from the same and different modalities at each network
layer, potentially triggering conflicts from the fusion of multi-source data;
(3) they lack the agility required to detect dynamic sentimental changes,
perhaps resulting in inaccurate classification of utterances with abrupt
sentiment shifts. To address these issues, a novel approach named GraphSmile is
proposed for tracking intricate emotional cues in multimodal dialogues.
GraphSmile comprises two key components, i.e., GSF and SDP modules. GSF
ingeniously leverages graph structures to alternately assimilate inter-modal
and intra-modal emotional dependencies layer by layer, adequately capturing
cross-modal cues while effectively circumventing fusion conflicts. SDP is an
auxiliary task to explicitly delineate the sentiment dynamics between
utterances, promoting the model's ability to distinguish sentimental
discrepancies. Furthermore, GraphSmile is effortlessly applied to multimodal
sentiment analysis in conversation (MSAC), forging a unified multimodal
affective model capable of executing MERC and MSAC tasks. Empirical results on
multiple benchmarks demonstrate that GraphSmile can handle complex emotional
and sentimental patterns, significantly outperforming baseline models.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂØπËØùÊÉÖÊÑüËØÜÂà´ÔºàMERCÔºâÊúÄËøëÂºïËµ∑‰∫ÜÂ§ßÈáèÁ†îÁ©∂ÂÖ≥Ê≥®„ÄÇÁé∞ÊúâÁöÑ MERC ÊñπÊ≥ïÈù¢‰∏¥ÁùÄ‰∏Ä‰∫õÊåëÊàòÔºöÔºà1ÔºâÂÆÉ‰ª¨Êú™ËÉΩÂÖÖÂàÜÂà©Áî®Áõ¥Êé•ÁöÑÊ®°ÊÄÅÈó¥Á∫øÁ¥¢ÔºåÂèØËÉΩÂØºËá¥‰∏çÂ§™ÂΩªÂ∫ïÁöÑË∑®Ê®°ÊÄÅÂª∫Ê®°ÔºõÔºà2ÔºâÂÆÉ‰ª¨Âú®ÊØè‰∏™ÁΩëÁªúÂ±ÇÂêåÊó∂‰ªéÁõ∏ÂêåÂíå‰∏çÂêåÁöÑÊ®°ÊÄÅ‰∏≠ÊèêÂèñ‰ø°ÊÅØÔºåÂèØËÉΩÂºïÂèëÊù•Ëá™Â§öÊ∫êÊï∞ÊçÆËûçÂêàÁöÑÂÜ≤Á™ÅÔºõÔºà3ÔºâÂÆÉ‰ª¨Áº∫‰πèÊ£ÄÊµãÂä®ÊÄÅÊÉÖÊÑüÂèòÂåñÊâÄÈúÄÁöÑÊïèÊç∑ÊÄßÔºåÂèØËÉΩÂØºËá¥ÂØπÊÉÖÊÑüÊÄ•ÂâßËΩ¨ÂèòÁöÑËØùËØ≠ËøõË°å‰∏çÂáÜÁ°ÆÁöÑÂàÜÁ±ª„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ GraphSmile ÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éËøΩË∏™Â§öÊ®°ÊÄÅÂØπËØù‰∏≠ÁöÑÂ§çÊùÇÊÉÖÊÑüÁ∫øÁ¥¢„ÄÇGraphSmile ÂåÖÂê´‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºåÂç≥ GSF Âíå SDP Ê®°Âùó„ÄÇGSF Â∑ßÂ¶ôÂú∞Âà©Áî®ÂõæÁªìÊûÑÈÄêÂ±Ç‰∫§ÊõøÂêåÂåñÊ®°ÊÄÅÈó¥ÂíåÊ®°ÊÄÅÂÜÖÊÉÖÊÑü‰æùËµñÂÖ≥Á≥ªÔºåÂÖÖÂàÜÊçïÊçâË∑®Ê®°ÊÄÅÁ∫øÁ¥¢ÔºåÂêåÊó∂ÊúâÊïàËßÑÈÅøËûçÂêàÂÜ≤Á™Å„ÄÇSDP ÊòØ‰∏ÄÈ°πËæÖÂä©‰ªªÂä°ÔºåÁî®‰∫éÊòéÁ°ÆÊèèÁªòËØùËØ≠‰πãÈó¥ÁöÑÊÉÖÊÑüÂä®ÊÄÅÔºåÊèêÂçáÊ®°ÂûãÂå∫ÂàÜÊÉÖÊÑüÂ∑ÆÂºÇÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåGraphSmile ÂèØ‰ª•ÊØ´‰∏çË¥πÂäõÂú∞Â∫îÁî®‰∫éÂØπËØù‰∏≠ÁöÑÂ§öÊ®°ÊÄÅÊÉÖÊÑüÂàÜÊûêÔºàMSACÔºâÔºåÊâìÈÄ†‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÊÉÖÊÑüÊ®°ÂûãÔºåËÉΩÂ§üÊâßË°å MERC Âíå MSAC ‰ªªÂä°„ÄÇÂú®Â§ö‰∏™Âü∫ÂáÜ‰∏äÁöÑÂÆûËØÅÁªìÊûúË°®ÊòéÔºåGraphSmile ÂèØ‰ª•Â§ÑÁêÜÂ§çÊùÇÁöÑÊÉÖÊÑüÂíåÊÉÖÊÑüÊ®°ÂºèÔºåÊòéÊòæ‰ºò‰∫éÂü∫Á∫øÊ®°Âûã„ÄÇ

##### **Can LLMs "Reason" in Music? An Evaluation of LLMs' Capability of Music Understanding and Generation**
2407.21531v1 by Ziya Zhou, Yuhang Wu, Zhiyue Wu, Xinyue Zhang, Ruibin Yuan, Yinghao Ma, Lu Wang, Emmanouil Benetos, Wei Xue, Yike Guo

Symbolic Music, akin to language, can be encoded in discrete symbols. Recent
research has extended the application of large language models (LLMs) such as
GPT-4 and Llama2 to the symbolic music domain including understanding and
generation. Yet scant research explores the details of how these LLMs perform
on advanced music understanding and conditioned generation, especially from the
multi-step reasoning perspective, which is a critical aspect in the
conditioned, editable, and interactive human-computer co-creation process. This
study conducts a thorough investigation of LLMs' capability and limitations in
symbolic music processing. We identify that current LLMs exhibit poor
performance in song-level multi-step music reasoning, and typically fail to
leverage learned music knowledge when addressing complex musical tasks. An
analysis of LLMs' responses highlights distinctly their pros and cons. Our
findings suggest achieving advanced musical capability is not intrinsically
obtained by LLMs, and future research should focus more on bridging the gap
between music knowledge and reasoning, to improve the co-creation experience
for musicians.

ÊëòË¶ÅÔºöÁ¨¶ËôüÈü≥Ê®ÇÈ°û‰ººÊñºË™ûË®ÄÔºåÂèØ‰ª•Áî®Èõ¢Êï£Á¨¶ËôüÁ∑®Á¢º„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Â∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æãÂ¶Ç GPT-4 Âíå Llama2ÔºåÁöÑÊáâÁî®Êì¥Â±ïÂà∞Á¨¶ËôüÈü≥Ê®ÇÈ†òÂüüÔºåÂåÖÊã¨ÁêÜËß£ÂíåÁîüÊàê„ÄÇÁÑ∂ËÄåÔºåÂæàÂ∞ëÊúâÁ†îÁ©∂Êé¢Ë®éÈÄô‰∫õ LLM Â¶Ç‰ΩïÂü∑Ë°åÈÄ≤ÈöéÈü≥Ê®ÇÁêÜËß£ÂíåÊ¢ù‰ª∂ÁîüÊàêÔºåÁâπÂà•ÊòØÂæûÂ§öÊ≠•È©üÊé®ÁêÜÁöÑËßíÂ∫¶‰æÜÁúãÔºåÈÄôÊòØÊ¢ù‰ª∂Âºè„ÄÅÂèØÁ∑®ËºØÂíå‰∫íÂãïÂºè‰∫∫Ê©üÂÖ±ÂêåÂâµ‰ΩúÈÅéÁ®ã‰∏≠ÁöÑ‰∏ÄÂÄãÈóúÈçµÈù¢Âêë„ÄÇÊú¨Á†îÁ©∂Â∞ç LLM Âú®Á¨¶ËôüÈü≥Ê®ÇËôïÁêÜ‰∏≠ÁöÑËÉΩÂäõÂíåÈôêÂà∂ÈÄ≤Ë°å‰∫ÜÂæπÂ∫ïÁöÑË™øÊü•„ÄÇÊàëÂÄëÁôºÁèæÔºåÁõÆÂâçÁöÑ LLM Âú®Ê≠åÊõ≤Â±§Á¥öÁöÑÂ§öÊ≠•È©üÈü≥Ê®ÇÊé®ÁêÜ‰∏≠Ë°®Áèæ‰∏ç‰Ω≥Ôºå‰∏¶‰∏îÂú®ËôïÁêÜË§áÈõúÁöÑÈü≥Ê®Ç‰ªªÂãôÊôÇÈÄöÂ∏∏ÁÑ°Ê≥ïÂà©Áî®ÊâÄÂ≠∏ÁöÑÈü≥Ê®ÇÁü•Ë≠ò„ÄÇÂ∞ç LLM ÂõûÊáâÁöÑÂàÜÊûêÁ™ÅÂá∫‰∫ÜÂÆÉÂÄëÁöÑÂÑ™Áº∫Èªû„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåLLM Êú¨Ë≥™‰∏ä‰∏¶Êú™Áç≤ÂæóÈÄ≤ÈöéÁöÑÈü≥Ê®ÇËÉΩÂäõÔºåÊú™‰æÜÁöÑÁ†îÁ©∂ÊáâÊõ¥Â∞àÊ≥®ÊñºÂΩåÂêàÈü≥Ê®ÇÁü•Ë≠òÂíåÊé®ÁêÜ‰πãÈñìÁöÑÂ∑ÆË∑ùÔºå‰ª•ÊîπÂñÑÈü≥Ê®ÇÂÆ∂ÁöÑÂÖ±ÂêåÂâµ‰ΩúÈ´îÈ©ó„ÄÇ

##### **Data Contamination Report from the 2024 CONDA Shared Task**
2407.21530v1 by Oscar Sainz, Iker Garc√≠a-Ferrero, Alon Jacovi, Jon Ander Campos, Yanai Elazar, Eneko Agirre, Yoav Goldberg, Wei-Lin Chen, Jenny Chim, Leshem Choshen, Luca D'Amico-Wong, Melissa Dell, Run-Ze Fan, Shahriar Golchin, Yucheng Li, Pengfei Liu, Bhavish Pahwa, Ameya Prabhu, Suryansh Sharma, Emily Silcock, Kateryna Solonko, David Stap, Mihai Surdeanu, Yu-Min Tseng, Vishaal Udandarao, Zengzhi Wang, Ruijie Xu, Jinglin Yang

The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant
aspects of data contamination in natural language processing, where data
contamination is understood as situations where evaluation data is included in
pre-training corpora used to train large scale models, compromising evaluation
results. The workshop fostered a shared task to collect evidence on data
contamination in current available datasets and models. The goal of the shared
task and associated database is to assist the community in understanding the
extent of the problem and to assist researchers in avoiding reporting
evaluation results on known contaminated resources. The shared task provides a
structured, centralized public database for the collection of contamination
evidence, open to contributions from the community via GitHub pool requests.
This first compilation paper is based on 566 reported entries over 91
contaminated sources from a total of 23 contributors. The details of the
individual contamination events are available in the platform. The platform
continues to be online, open to contributions from the community.

ÊëòË¶ÅÔºöÁ¨¨‰∏ÄÂ±ÜË≥áÊñôÊ±°ÊüìÂ∑•‰ΩúÂùäÔºàCONDA 2024ÔºâÂ∞àÊ≥®ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠Ë≥áÊñôÊ±°ÊüìÁöÑÊâÄÊúâÁõ∏ÈóúÈù¢ÂêëÔºåÂÖ∂‰∏≠Ë≥áÊñôÊ±°ÊüìË¢´ÁêÜËß£ÁÇ∫Ë©ïÈáèË≥áÊñôÂåÖÂê´Âú®Áî®ÊñºË®ìÁ∑¥Â§ßÂûãÊ®°ÂûãÁöÑÈ†êË®ìÁ∑¥Ë™ûÊñôÂ∫´‰∏≠ÔºåÈÄ≤ËÄåÂΩ±ÈüøË©ïÈáèÁµêÊûúÁöÑÊÉÖÊ≥Å„ÄÇÂ∑•‰ΩúÂùä‰øÉÈÄ≤‰∫Ü‰∏ÄÈ†ÖÂÖ±Âêå‰ªªÂãôÔºå‰ª•Êî∂ÈõÜË≠âÊìöÔºå‰∫ÜËß£Áï∂ÂâçÂèØÁî®Ë≥áÊñôÈõÜÂíåÊ®°Âûã‰∏≠ÁöÑË≥áÊñôÊ±°Êüì„ÄÇÂÖ±Âêå‰ªªÂãôÂíåÁõ∏ÈóúË≥áÊñôÂ∫´ÁöÑÁõÆÊ®ôÊòØÂçîÂä©Á§æÁæ§‰∫ÜËß£ÂïèÈ°åÁöÑÁ®ãÂ∫¶Ôºå‰∏¶ÂçîÂä©Á†îÁ©∂‰∫∫Âì°ÈÅøÂÖçÂõûÂ†±Â∑≤Áü•ÂèóÊ±°ÊüìË≥áÊ∫êÁöÑË©ïÈáèÁµêÊûú„ÄÇÂÖ±Âêå‰ªªÂãôÊèê‰æõ‰∏ÄÂÄãÁµêÊßãÂåñ„ÄÅÈõÜ‰∏≠ÁöÑÂÖ¨ÈñãË≥áÊñôÂ∫´ÔºåÁî®ÊñºÊî∂ÈõÜÊ±°ÊüìË≠âÊìöÔºå‰∏¶ÈñãÊîæÁ§æÁæ§ÈÄèÈÅé GitHub Ê±†Ë´ãÊ±ÇÊèê‰æõË≤¢Áçª„ÄÇÈÄô‰ªΩÁ¨¨‰∏Ä‰ªΩÂΩôÁ∑®Ë´ñÊñáÊòØÊ†πÊìö‰æÜËá™ 23 ‰ΩçË≤¢ÁçªËÄÖÁöÑ 91 ÂÄãÂèóÊ±°Êüì‰æÜÊ∫êÊâÄÂõûÂ†±ÁöÑ 566 ÂÄãÊ¢ùÁõÆ„ÄÇÂÄãÂà•Ê±°Êüì‰∫ã‰ª∂ÁöÑË©≥Á¥∞Ë≥áË®äÂèØÂú®Âπ≥Âè∞‰∏äÂèñÂæó„ÄÇÂπ≥Âè∞ÊåÅÁ∫å‰∏äÁ∑öÔºåÈñãÊîæÁ§æÁæ§Êèê‰æõË≤¢Áçª„ÄÇ

##### **Tabular Data Augmentation for Machine Learning: Progress and Prospects of Embracing Generative AI**
2407.21523v1 by Lingxi Cui, Huan Li, Ke Chen, Lidan Shou, Gang Chen

Machine learning (ML) on tabular data is ubiquitous, yet obtaining abundant
high-quality tabular data for model training remains a significant obstacle.
Numerous works have focused on tabular data augmentation (TDA) to enhance the
original table with additional data, thereby improving downstream ML tasks.
Recently, there has been a growing interest in leveraging the capabilities of
generative AI for TDA. Therefore, we believe it is time to provide a
comprehensive review of the progress and future prospects of TDA, with a
particular emphasis on the trending generative AI. Specifically, we present an
architectural view of the TDA pipeline, comprising three main procedures:
pre-augmentation, augmentation, and post-augmentation. Pre-augmentation
encompasses preparation tasks that facilitate subsequent TDA, including error
handling, table annotation, table simplification, table representation, table
indexing, table navigation, schema matching, and entity matching. Augmentation
systematically analyzes current TDA methods, categorized into retrieval-based
methods, which retrieve external data, and generation-based methods, which
generate synthetic data. We further subdivide these methods based on the
granularity of the augmentation process at the row, column, cell, and table
levels. Post-augmentation focuses on the datasets, evaluation and optimization
aspects of TDA. We also summarize current trends and future directions for TDA,
highlighting promising opportunities in the era of generative AI. In addition,
the accompanying papers and related resources are continuously updated and
maintained in the GitHub repository at
https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentation to reflect
ongoing advancements in the field.

ÊëòË¶ÅÔºöË°®Ê†ºË≥áÊñôÁöÑÊ©üÂô®Â≠∏Áøí (ML) ÁÑ°ÊâÄ‰∏çÂú®Ôºå‰ΩÜÂèñÂæóÂ§ßÈáèÁöÑÂÑ™Ë≥™Ë°®Ê†ºË≥áÊñô‰ª•ÈÄ≤Ë°åÊ®°ÂûãË®ìÁ∑¥‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÈöúÁ§ô„ÄÇË®±Â§öÁ†îÁ©∂Â∞àÊ≥®ÊñºË°®Ê†ºË≥áÊñôÊì¥ÂÖÖ (TDA)Ôºå‰ª•‰ΩøÁî®È°çÂ§ñÁöÑË≥áÊñô‰æÜÂ¢ûÂº∑ÂéüÂßãË°®Ê†ºÔºåÂæûËÄåÊîπÂñÑ‰∏ãÊ∏∏ ML ‰ªªÂãô„ÄÇÊúÄËøëÔºå‰∫∫ÂÄëÂ∞çÂà©Áî®ÁîüÊàêÂºè AI ÁöÑÂäüËÉΩÈÄ≤Ë°å TDA Áî¢ÁîüË∂ä‰æÜË∂äÂ§ßÁöÑËààË∂£„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÁõ∏‰ø°ÁèæÂú®ÊòØÊôÇÂÄôÂ∞ç TDA ÁöÑÈÄ≤Â±ïÂíåÊú™‰æÜÂâçÊôØÈÄ≤Ë°åÂÖ®Èù¢ÂõûÈ°ßÔºåÁâπÂà•Âº∑Ë™øË∂®Âã¢ÊÄßÁöÑÁîüÊàêÂºè AI„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü TDA ÁÆ°Á∑öÁöÑÊû∂ÊßãË¶ñÂúñÔºåÂåÖÂê´‰∏âÂÄã‰∏ªË¶ÅÁ®ãÂ∫èÔºöÈ†êÊì¥ÂÖÖ„ÄÅÊì¥ÂÖÖÂíåÂæåÊì¥ÂÖÖ„ÄÇÈ†êÊì¥ÂÖÖÂåÖÂê´ÊúâÂä©ÊñºÂæåÁ∫å TDA ÁöÑÊ∫ñÂÇô‰ªªÂãôÔºåÂåÖÊã¨ÈåØË™§ËôïÁêÜ„ÄÅË°®Ê†ºË®ªËß£„ÄÅË°®Ê†ºÁ∞°Âåñ„ÄÅË°®Ê†ºË°®Á§∫„ÄÅË°®Ê†ºÁ¥¢Âºï„ÄÅË°®Ê†ºÂ∞éË¶Ω„ÄÅÊû∂ÊßãÊØîÂ∞çÂíåÂØ¶È´îÊØîÂ∞ç„ÄÇÊì¥ÂÖÖÁ≥ªÁµ±ÊÄßÂú∞ÂàÜÊûêÁõÆÂâçÁöÑ TDA ÊñπÊ≥ïÔºåÂàÜÈ°ûÁÇ∫Âü∫ÊñºÊ™¢Á¥¢ÁöÑÊñπÊ≥ïÔºàÊ™¢Á¥¢Â§ñÈÉ®Ë≥áÊñôÔºâÂíåÂü∫ÊñºÁîüÊàêÁöÑÁöÑÊñπÊ≥ïÔºàÁîüÊàêÂêàÊàêË≥áÊñôÔºâ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ê†πÊìöÂàó„ÄÅÊ¨Ñ„ÄÅÂÑ≤Â≠òÊ†ºÂíåË°®Ê†ºÂ±§Á¥öÁöÑÊì¥ÂÖÖÁ®ãÂ∫èÁ≤íÂ∫¶Á¥∞ÂàÜÈÄô‰∫õÊñπÊ≥ï„ÄÇÂæåÊì¥ÂÖÖÂ∞àÊ≥®Êñº TDA ÁöÑË≥áÊñôÈõÜ„ÄÅË©ï‰º∞ÂíåÊúÄ‰Ω≥ÂåñÈù¢Âêë„ÄÇÊàëÂÄë‰πüÁ∏ΩÁµê‰∫Ü TDA ÁõÆÂâçÁöÑË∂®Âã¢ÂíåÊú™‰æÜÊñπÂêëÔºåÂº∑Ë™ø‰∫ÜÁîüÊàêÂºè AI ÊôÇ‰ª£ÂÖÖÊªøÂ∏åÊúõÁöÑÊ©üÊúÉ„ÄÇÊ≠§Â§ñÔºåÈö®ÈôÑÁöÑË´ñÊñáÂíåÁõ∏ÈóúË≥áÊ∫êÊúÉÊåÅÁ∫åÊõ¥Êñ∞‰∏¶‰øùÂ≠òÂú® GitHub ÂÑ≤Â≠òÂ∫´‰∏≠ÔºåÁ∂≤ÂùÄÁÇ∫ https://github.com/SuDIS-ZJU/awesome-tabular-data-augmentationÔºå‰ª•ÂèçÊò†Ë©≤È†òÂüüÁöÑÊåÅÁ∫åÈÄ≤Â±ï„ÄÇ

##### **Expanding the Medical Decathlon dataset: segmentation of colon and colorectal cancer from computed tomography images**
2407.21516v1 by I. M. Chernenkiy, Y. A. Drach, S. R. Mustakimova, V. V. Kazantseva, N. A. Ushakov, S. K. Efetov, M. V. Feldsherov

Colorectal cancer is the third-most common cancer in the Western Hemisphere.
The segmentation of colorectal and colorectal cancer by computed tomography is
an urgent problem in medicine. Indeed, a system capable of solving this problem
will enable the detection of colorectal cancer at early stages of the disease,
facilitate the search for pathology by the radiologist, and significantly
accelerate the process of diagnosing the disease. However, scientific
publications on medical image processing mostly use closed, non-public data.
This paper presents an extension of the Medical Decathlon dataset with
colorectal markups in order to improve the quality of segmentation algorithms.
An experienced radiologist validated the data, categorized it into subsets by
quality, and published it in the public domain. Based on the obtained results,
we trained neural network models of the UNet architecture with 5-part
cross-validation and achieved a Dice metric quality of $0.6988 \pm 0.3$. The
published markups will improve the quality of colorectal cancer detection and
simplify the radiologist's job for study description.

ÊëòË¶ÅÔºöÂ§ßËÖ∏ÁôåÊòØË•øÂçäÁêÉÁ¨¨‰∏âÂ∏∏Ë¶ãÁöÑÁôåÁóá„ÄÇ
Âà©Áî®ÈõªËÖ¶Êñ∑Â±§ÊéÉÊèèÂ∞çÂ§ßËÖ∏ÁôåËàáÂ§ßËÖ∏ÁôåÈÄ≤Ë°åÂàÜÊÆµÊòØÈÜ´Â≠∏‰∏äÁöÑÁ∑äÊÄ•ÂïèÈ°å„ÄÇ‰∫ãÂØ¶‰∏äÔºå‰∏ÄÂÄãËÉΩÂ§†Ëß£Ê±∫ÈÄôÂÄãÂïèÈ°åÁöÑÁ≥ªÁµ±Â∞áËÉΩÂ§†Âú®ÁñæÁóÖÁöÑÊó©ÊúüÈöéÊÆµÂÅµÊ∏¨Â§ßËÖ∏ÁôåÔºåÂçîÂä©ÊîæÂ∞ÑÁßëÈÜ´Â∏´Â∞ãÊâæÁóÖÁêÜÔºå‰∏¶È°ØËëóÂä†ÈÄüË®∫Êñ∑ÁñæÁóÖÁöÑÈÅéÁ®ã„ÄÇÁÑ∂ËÄåÔºåÈóúÊñºÈÜ´Â≠∏ÂΩ±ÂÉèËôïÁêÜÁöÑÁßëÂ≠∏ÂàäÁâ©Â§ßÂ§ö‰ΩøÁî®Â∞ÅÈñâ„ÄÅÈùûÂÖ¨ÈñãÁöÑË≥áÊñô„ÄÇÈÄôÁØáË´ñÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ∏∂ÊúâÂ§ßËÖ∏Ê®ôË®òÁöÑÈÜ´Â≠∏ÂçÅÈ†ÖÂÖ®ËÉΩË≥áÊñôÈõÜÁöÑÂª∂‰º∏Ôºå‰ª•ÊèêÈ´òÂàÜÊÆµÊºîÁÆóÊ≥ïÁöÑÂìÅË≥™„ÄÇ‰∏Ä‰ΩçÁ∂ìÈ©óË±êÂØåÁöÑÊîæÂ∞ÑÁßëÈÜ´Â∏´È©óË≠â‰∫ÜË≥áÊñôÔºåÂ∞áÂÖ∂‰æùÂìÅË≥™ÂàÜÈ°ûÊàêÂ≠êÈõÜÔºå‰∏¶Â∞áÂÖ∂ÁôºÂ∏ÉÂú®ÂÖ¨ÂÖ±È†òÂüü„ÄÇÊ†πÊìöÁç≤ÂæóÁöÑÁµêÊûúÔºåÊàëÂÄëË®ìÁ∑¥‰∫ÜÂÖ∑Êúâ 5 ÈÉ®ÂàÜ‰∫§ÂèâÈ©óË≠âÁöÑ UNet Êû∂ÊßãÁöÑÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÔºå‰∏¶ÈÅîÂà∞‰∫Ü $0.6988 \pm 0.3$ ÁöÑ Dice ÊåáÊ®ôÂìÅË≥™„ÄÇÁôºÂ∏ÉÁöÑÊ®ôË®òÂ∞áÊèêÈ´òÂ§ßËÖ∏ÁôåÂÅµÊ∏¨ÁöÑÂìÅË≥™Ôºå‰∏¶Á∞°ÂåñÊîæÂ∞ÑÁßëÈÜ´Â∏´Á†îÁ©∂ÊèèËø∞ÁöÑÂ∑•‰Ωú„ÄÇ

##### **Interpreting and learning voice commands with a Large Language Model for a robot system**
2407.21512v1 by Stanislau Stankevich, Wojciech Dudek

Robots are increasingly common in industry and daily life, such as in nursing
homes where they can assist staff. A key challenge is developing intuitive
interfaces for easy communication. The use of Large Language Models (LLMs) like
GPT-4 has enhanced robot capabilities, allowing for real-time interaction and
decision-making. This integration improves robots' adaptability and
functionality. This project focuses on merging LLMs with databases to improve
decision-making and enable knowledge acquisition for request interpretation
problems.

ÊëòË¶ÅÔºöÊ©üÂô®‰∫∫Ë∂ä‰æÜË∂äÊôÆÈÅçÊñºÂ∑•Ê•≠ÂíåÊó•Â∏∏ÁîüÊ¥ª‰∏≠Ôºå‰æãÂ¶ÇÂú®Ë≠∑ÁêÜ‰πãÂÆ∂ÔºåÂÆÉÂÄëÂèØ‰ª•ÂçîÂä©Âì°Â∑•„ÄÇ‰∏ÄÈ†ÖÈóúÈçµÊåëÊà∞ÊòØÈñãÁôºÁõ¥Ë¶∫Âºè‰ªãÈù¢Ôºå‰ª•‰æøËºïÈ¨ÜÊ∫ùÈÄö„ÄÇ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)Ôºå‰æãÂ¶Ç GPT-4ÔºåÂ¢ûÂº∑‰∫ÜÊ©üÂô®‰∫∫ÁöÑËÉΩÂäõÔºåÂÖÅË®±Âç≥ÊôÇ‰∫íÂãïÂíåÊ±∫Á≠ñÂà∂ÂÆö„ÄÇÊ≠§Êï¥ÂêàÊîπÂñÑ‰∫ÜÊ©üÂô®‰∫∫ÁöÑÈÅ©ÊáâÊÄßÂíåÂäüËÉΩÊÄß„ÄÇÊ≠§Â∞àÊ°àÂ∞àÊ≥®ÊñºÂ∞á LLM ËàáË≥áÊñôÂ∫´Âêà‰ΩµÔºå‰ª•ÊîπÂñÑÊ±∫Á≠ñÂà∂ÂÆöÔºå‰∏¶ËÆìÁü•Ë≠òÊì∑ÂèñÂèØÁî®ÊñºË´ãÊ±ÇË©ÆÈáãÂïèÈ°å„ÄÇ

##### **FSSC: Federated Learning of Transformer Neural Networks for Semantic Image Communication**
2407.21507v1 by Yuna Yan, Xin Zhang, Lixin Li, Wensheng Lin, Rui Li, Wenchi Cheng, Zhu Han

In this paper, we address the problem of image semantic communication in a
multi-user deployment scenario and propose a federated learning (FL) strategy
for a Swin Transformer-based semantic communication system (FSSC). Firstly, we
demonstrate that the adoption of a Swin Transformer for joint source-channel
coding (JSCC) effectively extracts semantic information in the communication
system. Next, the FL framework is introduced to collaboratively learn a global
model by aggregating local model parameters, rather than directly sharing
clients' data. This approach enhances user privacy protection and reduces the
workload on the server or mobile edge. Simulation evaluations indicate that our
method outperforms the typical JSCC algorithm and traditional separate-based
communication algorithms. Particularly after integrating local semantics, the
global aggregation model has further increased the Peak Signal-to-Noise Ratio
(PSNR) by more than 2dB, thoroughly proving the effectiveness of our algorithm.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Êé¢ËÆ®‰∫ÜÂ§öÁî®Êà∑ÈÉ®ÁΩ≤Âú∫ÊôØ‰∏≠ÁöÑÂõæÂÉèËØ≠‰πâÈÄö‰ø°ÈóÆÈ¢òÔºåÂπ∂ÈíàÂØπÂü∫‰∫é Swin Transformer ÁöÑËØ≠‰πâÈÄö‰ø°Á≥ªÁªü (FSSC) ÊèêÂá∫‰∫Ü‰∏ÄÁßçËÅîÈÇ¶Â≠¶‰π† (FL) Á≠ñÁï•„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÈááÁî® Swin Transformer ËøõË°åËÅîÂêàÊ∫ê‰ø°ÈÅìÁºñÁ†Å (JSCC) ÂèØ‰ª•ÊúâÊïàÂú∞ÊèêÂèñÈÄö‰ø°Á≥ªÁªü‰∏≠ÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇÊé•‰∏ãÊù•ÔºåÂºïÂÖ• FL Ê°ÜÊû∂ÔºåÈÄöËøáËÅöÂêàÊú¨Âú∞Ê®°ÂûãÂèÇÊï∞Êù•Âçè‰ΩúÂ≠¶‰π†ÂÖ®Â±ÄÊ®°ÂûãÔºåËÄå‰∏çÊòØÁõ¥Êé•ÂÖ±‰∫´ÂÆ¢Êà∑Á´ØÊï∞ÊçÆ„ÄÇËøôÁßçÊñπÊ≥ïÂ¢ûÂº∫‰∫ÜÁî®Êà∑ÈöêÁßÅ‰øùÊä§ÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÊúçÂä°Âô®ÊàñÁßªÂä®ËæπÁºòÁöÑÂ∑•‰ΩúË¥üËΩΩ„ÄÇ‰ªøÁúüËØÑ‰º∞Ë°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ºò‰∫éÂÖ∏ÂûãÁöÑ JSCC ÁÆóÊ≥ïÂíå‰º†ÁªüÁöÑÂü∫‰∫éÂàÜÁ¶ªÁöÑÈÄö‰ø°ÁÆóÊ≥ï„ÄÇÁâπÂà´ÊòØÂú®Êï¥Âêà‰∫ÜÂ±ÄÈÉ®ËØ≠‰πâ‰πãÂêéÔºåÂÖ®Â±ÄËÅöÂêàÊ®°ÂûãËøõ‰∏ÄÊ≠•Â∞ÜÂ≥∞ÂÄº‰ø°Âô™ÊØî (PSNR) ÊèêÈ´ò‰∫Ü 2dB ‰ª•‰∏äÔºåÂÖÖÂàÜËØÅÊòé‰∫ÜÊàë‰ª¨ÁÆóÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **MaskUno: Switch-Split Block For Enhancing Instance Segmentation**
2407.21498v1 by Jawad Haidar, Marc Mouawad, Imad Elhajj, Daniel Asmar

Instance segmentation is an advanced form of image segmentation which, beyond
traditional segmentation, requires identifying individual instances of
repeating objects in a scene. Mask R-CNN is the most common architecture for
instance segmentation, and improvements to this architecture include steps such
as benefiting from bounding box refinements, adding semantics, or backbone
enhancements. In all the proposed variations to date, the problem of competing
kernels (each class aims to maximize its own accuracy) persists when models try
to synchronously learn numerous classes. In this paper, we propose mitigating
this problem by replacing mask prediction with a Switch-Split block that
processes refined ROIs, classifies them, and assigns them to specialized mask
predictors. We name the method MaskUno and test it on various models from the
literature, which are then trained on multiple classes using the benchmark COCO
dataset. An increase in the mean Average Precision (mAP) of 2.03% was observed
for the high-performing DetectoRS when trained on 80 classes. MaskUno proved to
enhance the mAP of instance segmentation models regardless of the number and
typ

ÊëòË¶ÅÔºöÂØ¶‰æãÂàÜÂâ≤ÊòØÂΩ±ÂÉèÂàÜÂâ≤ÁöÑÈÄ≤ÈöéÂΩ¢ÂºèÔºåÈô§‰∫ÜÂÇ≥Áµ±ÂàÜÂâ≤‰πãÂ§ñÔºåÈÇÑÈúÄË¶ÅË≠òÂà•Â†¥ÊôØ‰∏≠ÈáçË§áÁâ©È´îÁöÑÂÄãÂà•ÂØ¶‰æã„ÄÇMask R-CNN ÊòØÂØ¶‰æãÂàÜÂâ≤ÊúÄÂ∏∏Ë¶ãÁöÑÊû∂ÊßãÔºåËÄåÂ∞çÊ≠§Êû∂ÊßãÁöÑÊîπÈÄ≤ÂåÖÊã¨ÂæûÈÇäÁïåÊ°ÜÂÑ™Âåñ„ÄÅÂä†ÂÖ•Ë™ûÊÑèÊàñ‰∏ªÂππÂ¢ûÂº∑Á≠âÊ≠•È©ü„ÄÇÂú®ËøÑ‰ªäÁÇ∫Ê≠¢ÊèêÂá∫ÁöÑÊâÄÊúâËÆäÈ´î‰∏≠ÔºåÁï∂Ê®°ÂûãÂòóË©¶ÂêåÊ≠•Â≠∏ÁøíÂ§öÂÄãÈ°ûÂà•ÊôÇÔºåÁ´∂Áà≠Ê†∏ÂøÉÁöÑÂïèÈ°åÔºàÊØèÂÄãÈ°ûÂà•ÈÉΩÊó®Âú®ÊúÄÂ§ßÂåñÂÖ∂Ëá™Ë∫´Ê∫ñÁ¢∫Â∫¶Ôºâ‰ªçÁÑ∂Â≠òÂú®„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ÈÄèÈÅéÂ∞áÈÅÆÁΩ©È†êÊ∏¨ÊõøÊèõÁÇ∫ËôïÁêÜÁ≤æÁ∑ª ROI„ÄÅÂ∞çÂÖ∂ÂàÜÈ°û‰∏¶Â∞áÂÖ∂ÂàÜÈÖçÁµ¶Â∞àÈñÄÈÅÆÁΩ©È†êÊ∏¨Âô®ÁöÑÈñãÈóúÂàÜÂâ≤ÂçÄÂ°ä‰æÜÊ∏õËºïÊ≠§ÂïèÈ°å„ÄÇÊàëÂÄëÂ∞áÊ≠§ÊñπÊ≥ïÂëΩÂêçÁÇ∫ MaskUnoÔºå‰∏¶Âú®‰æÜËá™ÊñáÁçªÁöÑÂêÑÁ®ÆÊ®°Âûã‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åÊ∏¨Ë©¶ÔºåÁÑ∂Âæå‰ΩøÁî®Âü∫Ê∫ñ COCO Ë≥áÊñôÈõÜÂ∞çÂÖ∂ÈÄ≤Ë°åÂ§öÂÄãÈ°ûÂà•ÁöÑË®ìÁ∑¥„ÄÇÂú® 80 ÂÄãÈ°ûÂà•‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÊôÇÔºåÈ´òÊÄßËÉΩ DetectoRS ÁöÑÂπ≥ÂùáÊ∫ñÁ¢∫Â∫¶ (mAP) ËßÄÂØüÂà∞Â¢ûÂä†‰∫Ü 2.03%„ÄÇÁÑ°Ë´ñÈ°ûÂà•Êï∏ÈáèÂíå

##### **Generative Expressive Conversational Speech Synthesis**
2407.21491v2 by Rui Liu, Yifan Hu, Yi Ren, Xiang Yin, Haizhou Li

Conversational Speech Synthesis (CSS) aims to express a target utterance with
the proper speaking style in a user-agent conversation setting. Existing CSS
methods employ effective multi-modal context modeling techniques to achieve
empathy understanding and expression. However, they often need to design
complex network architectures and meticulously optimize the modules within
them. In addition, due to the limitations of small-scale datasets containing
scripted recording styles, they often fail to simulate real natural
conversational styles. To address the above issues, we propose a novel
generative expressive CSS system, termed GPT-Talker.We transform the multimodal
information of the multi-turn dialogue history into discrete token sequences
and seamlessly integrate them to form a comprehensive user-agent dialogue
context. Leveraging the power of GPT, we predict the token sequence, that
includes both semantic and style knowledge, of response for the agent. After
that, the expressive conversational speech is synthesized by the
conversation-enriched VITS to deliver feedback to the user.Furthermore, we
propose a large-scale Natural CSS Dataset called NCSSD, that includes both
naturally recorded conversational speech in improvised styles and dialogues
extracted from TV shows. It encompasses both Chinese and English languages,
with a total duration of 236 hours.We conducted comprehensive experiments on
the reliability of the NCSSD and the effectiveness of our GPT-Talker. Both
subjective and objective evaluations demonstrate that our model outperforms
other state-of-the-art CSS systems significantly in terms of naturalness and
expressiveness. The Code, Dataset, and Pre-trained Model are available at:
https://github.com/AI-S2-Lab/GPT-Talker.

ÊëòË¶ÅÔºöÂ∞çË©±ÂºèË™ûÈü≥ÂêàÊàê (CSS) Êó®Âú®‰ª•ÈÅ©Áï∂ÁöÑË™™Ë©±È¢®Ê†ºÂú®‰ΩøÁî®ËÄÖ‰ª£ÁêÜÂ∞çË©±Ë®≠ÂÆö‰∏≠Ë°®ÈÅîÁõÆÊ®ôË©±Ë™û„ÄÇÁèæÊúâÁöÑ CSS ÊñπÊ≥ïÊé°Áî®ÊúâÊïàÁöÑÂ§öÊ®°ÂºèË™ûÂ¢ÉÂª∫Ê®°ÊäÄË°ì‰æÜÈÅîÊàêÂêåÁêÜÁêÜËß£ÂíåË°®ÈÅî„ÄÇÁÑ∂ËÄåÔºå‰ªñÂÄëÁ∂ìÂ∏∏ÈúÄË¶ÅË®≠Ë®àË§áÈõúÁöÑÁ∂≤Ë∑ØÊû∂Êßã‰∏¶Á≤æÂøÉÊúÄ‰Ω≥ÂåñÂÖ∂‰∏≠ÁöÑÊ®°ÁµÑ„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºÂåÖÂê´ËÖ≥Êú¨Ë®òÈåÑÈ¢®Ê†ºÁöÑÂ∞èË¶èÊ®°Ë≥áÊñôÈõÜÁöÑÈôêÂà∂Ôºå‰ªñÂÄëÁ∂ìÂ∏∏ÁÑ°Ê≥ïÊ®°Êì¨ÁúüÂØ¶Ëá™ÁÑ∂ÁöÑÂ∞çË©±È¢®Ê†º„ÄÇÁÇ∫‰∫ÜËß£Ê±∫‰∏äËø∞ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑÁîüÊàêÂºèË°®ÈÅîÊÄß CSS Á≥ªÁµ±ÔºåÁ®±ÁÇ∫ GPT-Talker„ÄÇÊàëÂÄëÂ∞áÂ§öËº™Â∞çË©±Ê≠∑Âè≤ÁöÑÂ§öÊ®°ÂºèË≥áË®äËΩâÊèõÁÇ∫Èõ¢Êï£ÁöÑÁ¨¶ËôüÂ∫èÂàóÔºå‰∏¶Â∞áÂÆÉÂÄëÁÑ°Á∏´Êï¥Âêà‰ª•ÂΩ¢ÊàêÂÖ®Èù¢ÁöÑ‰ΩøÁî®ËÄÖ‰ª£ÁêÜÂ∞çË©±Ë™ûÂ¢É„ÄÇÂà©Áî® GPT ÁöÑÂäüËÉΩÔºåÊàëÂÄëÈ†êÊ∏¨ÂõûÊáâÁöÑÁ¨¶ËôüÂ∫èÂàóÔºåÂÖ∂‰∏≠ÂåÖÂê´Ë™ûÁæ©ÂíåÈ¢®Ê†ºÁü•Ë≠òÔºå‰ª•‰æõ‰ª£ÁêÜ‰ΩøÁî®„ÄÇÂú®ÈÇ£‰πãÂæåÔºåÁî±Â∞çË©±Ë±êÂØåÁöÑ VITS ÂêàÊàêÂØåÊúâË°®ÁèæÂäõÁöÑÂ∞çË©±ÂºèË™ûÈü≥Ôºå‰ª•Âêë‰ΩøÁî®ËÄÖÊèê‰æõÂõûÈ•ã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ NCSSD ÁöÑÂ§ßÂûãËá™ÁÑ∂ CSS Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´‰ª•Âç≥ËààÈ¢®Ê†ºËá™ÁÑ∂Ë®òÈåÑÁöÑÂ∞çË©±ÂºèË™ûÈü≥ÂíåÂæûÈõªË¶ñÁØÄÁõÆ‰∏≠Êì∑ÂèñÁöÑÂ∞çË©±„ÄÇÂÆÉÂåÖÂê´‰∏≠ÊñáÂíåËã±ÊñáÔºåÁ∏ΩÊôÇÈï∑ÁÇ∫ 236 Â∞èÊôÇ„ÄÇÊàëÂÄëÂ∞ç NCSSD ÁöÑÂèØÈù†ÊÄßÂíåÊàëÂÄë GPT-Talker ÁöÑÊúâÊïàÊÄßÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©ó„ÄÇ‰∏ªËßÄÂíåÂÆ¢ËßÄË©ï‰º∞ÈÉΩË≠âÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂú®Ëá™ÁÑ∂ÊÄßÂíåË°®ÁèæÂäõÊñπÈù¢È°ØËëóÂÑ™ÊñºÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑ CSS Á≥ªÁµ±„ÄÇÁ®ãÂºèÁ¢º„ÄÅË≥áÊñôÈõÜÂíåÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºöhttps://github.com/AI-S2-Lab/GPT-Talker„ÄÇ

##### **Explainable and Controllable Motion Curve Guided Cardiac Ultrasound Video Generation**
2407.21490v1 by Junxuan Yu, Rusi Chen, Yongsong Zhou, Yanlin Chen, Yaofei Duan, Yuhao Huang, Han Zhou, Tan Tao, Xin Yang, Dong Ni

Echocardiography video is a primary modality for diagnosing heart diseases,
but the limited data poses challenges for both clinical teaching and machine
learning training. Recently, video generative models have emerged as a
promising strategy to alleviate this issue. However, previous methods often
relied on holistic conditions during generation, hindering the flexible
movement control over specific cardiac structures. In this context, we propose
an explainable and controllable method for echocardiography video generation,
taking an initial frame and a motion curve as guidance. Our contributions are
three-fold. First, we extract motion information from each heart substructure
to construct motion curves, enabling the diffusion model to synthesize
customized echocardiography videos by modifying these curves. Second, we
propose the structure-to-motion alignment module, which can map semantic
features onto motion curves across cardiac structures. Third, The
position-aware attention mechanism is designed to enhance video consistency
utilizing Gaussian masks with structural position information. Extensive
experiments on three echocardiography datasets show that our method outperforms
others regarding fidelity and consistency. The full code will be released at
https://github.com/mlmi-2024-72/ECM.

ÊëòË¶ÅÔºöË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±ÁâáÊòØË®∫Êñ∑ÂøÉËáüÁñæÁóÖÁöÑ‰∏ªË¶ÅÊñπÂºèÔºå
‰ΩÜÊúâÈôêÁöÑÊï∏ÊìöÂ∞çËá®Â∫äÊïôÂ≠∏ÂíåÊ©üÂô®Â≠∏ÁøíË®ìÁ∑¥ÈÉΩÊßãÊàêÊåëÊà∞„ÄÇÊúÄËøëÔºåÂΩ±ÁâáÁîüÊàêÊ®°ÂûãÂ∑≤ÊàêÁÇ∫Á∑©Ëß£Ê≠§ÂïèÈ°åÁöÑ‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÁ≠ñÁï•„ÄÇÁÑ∂ËÄåÔºåÂÖàÂâçÁöÑËæ¶Ê≥ïÂú®ÁîüÊàêÈÅéÁ®ã‰∏≠ÈÄöÂ∏∏‰æùË≥¥Êï¥È´îÊ¢ù‰ª∂ÔºåÈòªÁ§ô‰∫ÜÂ∞çÁâπÂÆöÂøÉËáüÁµêÊßãÁöÑÈùàÊ¥ªÈÅãÂãïÊéßÂà∂„ÄÇÂú®Ê≠§ËÉåÊôØ‰∏ãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂèØËß£Èáã‰∏îÂèØÊéßÁöÑË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±ÁâáÁîüÊàêÊñπÊ≥ïÔºå‰ª•ÂàùÂßãÂπÄÂíåÈÅãÂãïÊõ≤Á∑ö‰ΩúÁÇ∫ÊåáÂ∞é„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÊúâ‰∏âÊñπÈù¢„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂæûÊØèÂÄãÂøÉËáüÂ≠êÁµêÊßã‰∏≠ÊèêÂèñÈÅãÂãïË≥áË®ä‰ª•Âª∫ÊßãÈÅãÂãïÊõ≤Á∑öÔºåËÆìÊì¥Êï£Ê®°ÂûãËÉΩÂ§†ÈÄèÈÅé‰øÆÊîπÈÄô‰∫õÊõ≤Á∑ö‰æÜÂêàÊàêÂÆ¢Ë£ΩÂåñÁöÑË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñÂΩ±Áâá„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁµêÊßãÂà∞ÈÅãÂãïÂ∞çÈΩäÊ®°ÁµÑÔºåÂÆÉÂèØ‰ª•Â∞áË™ûÁæ©ÁâπÂæµÂ∞çÊáâÂà∞ÂøÉËáüÁµêÊßã‰∏≠ÁöÑÈÅãÂãïÊõ≤Á∑ö„ÄÇÁ¨¨‰∏âÔºå‰ΩçÁΩÆÊÑüÁü•Ê≥®ÊÑèÂäõÊ©üÂà∂Êó®Âú®Âà©Áî®ÂÖ∑ÊúâÁµêÊßã‰ΩçÁΩÆË≥áË®äÁöÑÈ´òÊñØÈÅÆÁΩ©‰æÜÂ¢ûÂº∑ÂΩ±ÁâáÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂú®‰∏âÂÄãË∂ÖÈü≥Ê≥¢ÂøÉÂãïÂúñË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óÈ°ØÁ§∫ÔºåÊàëÂÄëÁöÑËæ¶Ê≥ïÂú®‰øùÁúüÂ∫¶Âíå‰∏ÄËá¥ÊÄßÊñπÈù¢ÂÑ™ÊñºÂÖ∂‰ªñËæ¶Ê≥ï„ÄÇÂÆåÊï¥Á®ãÂºèÁ¢ºÂ∞áÂú® https://github.com/mlmi-2024-72/ECM ‰∏äÈáãÂá∫„ÄÇ

##### **Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends**
2407.21489v1 by Giuliano Martinelli, Edoardo Barba, Roberto Navigli

Large autoregressive generative models have emerged as the cornerstone for
achieving the highest performance across several Natural Language Processing
tasks. However, the urge to attain superior results has, at times, led to the
premature replacement of carefully designed task-specific approaches without
exhaustive experimentation. The Coreference Resolution task is no exception;
all recent state-of-the-art solutions adopt large generative autoregressive
models that outperform encoder-based discriminative systems. In this work,we
challenge this recent trend by introducing Maverick, a carefully designed - yet
simple - pipeline, which enables running a state-of-the-art Coreference
Resolution system within the constraints of an academic budget, outperforming
models with up to 13 billion parameters with as few as 500 million parameters.
Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,
training with up to 0.006x the memory resources and obtaining a 170x faster
inference compared to previous state-of-the-art systems. We extensively
validate the robustness of the Maverick framework with an array of diverse
experiments, reporting improvements over prior systems in data-scarce,
long-document, and out-of-domain settings. We release our code and models for
research purposes at https://github.com/SapienzaNLP/maverick-coref.

ÊëòË¶ÅÔºöÂ§ßÂûãËá™ÂõûÂΩíÁîüÊàêÊ®°ÂûãÂ∑≤Êàê‰∏∫Âú®Â§öÈ°πËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÂÆûÁé∞ÊúÄÈ´òÊÄßËÉΩÁöÑÂü∫Áü≥„ÄÇÁÑ∂ËÄåÔºåËøΩÊ±ÇÂçìË∂äÊàêÊûúÁöÑÂÜ≤Âä®ÊúâÊó∂‰ºöÂØºËá¥Âú®Ê≤°ÊúâËØ¶Â∞ΩÂÆûÈ™åÁöÑÊÉÖÂÜµ‰∏ãËøáÊó©ÊõøÊç¢Á≤æÂøÉËÆæËÆ°ÁöÑÁâπÂÆö‰ªªÂä°ÊñπÊ≥ï„ÄÇÂÖ±ÊåáÊ∂àËß£‰ªªÂä°‰πü‰∏ç‰æãÂ§ñÔºõÊâÄÊúâÊúÄËøëÁöÑÊúÄÊñ∞Ëß£ÂÜ≥ÊñπÊ°àÈÉΩÈááÁî®Â§ßÂûãÁîüÊàêËá™ÂõûÂΩíÊ®°ÂûãÔºåÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÂü∫‰∫éÁºñÁ†ÅÂô®ÁöÑÂà§Âà´Á≥ªÁªü„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÈÄöËøáÂºïÂÖ• Maverick Êù•ÊåëÊàòËøô‰∏ÄÊúÄÊñ∞Ë∂ãÂäøÔºåMaverick ÊòØ‰∏ÄÊ¨æÁ≤æÂøÉËÆæËÆ°‰ΩÜÁÆÄÂçïÁöÑÁÆ°ÈÅìÔºåÂÆÉÂèØ‰ª•Âú®Â≠¶ÊúØÈ¢ÑÁÆóÁöÑÈôêÂà∂ÂÜÖËøêË°åÊúÄÂÖàËøõÁöÑÂÖ±ÊåáÊ∂àËß£Á≥ªÁªüÔºåÂÖ∂ÊÄßËÉΩ‰ºò‰∫éÂ§öËææ 130 ‰∫øÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåËÄåÂèÇÊï∞Âç¥Â∞ëËá≥ 5 ‰∫ø„ÄÇMaverick Âú® CoNLL-2012 Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåËÆ≠ÁªÉÊó∂‰ΩøÁî®ÁöÑÂÜÖÂ≠òËµÑÊ∫êÊúÄÂ§ö‰∏∫ 0.006 ÂÄçÔºåÂπ∂‰∏î‰∏é‰πãÂâçÁöÑÊúÄÂÖàËøõÁ≥ªÁªüÁõ∏ÊØîÔºåÊé®ÁêÜÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü 170 ÂÄç„ÄÇÊàë‰ª¨ÈÄöËøá‰∏ÄÁ≥ªÂàó‰∏çÂêåÁöÑÂÆûÈ™åÂπøÊ≥õÈ™åËØÅ‰∫Ü Maverick Ê°ÜÊû∂ÁöÑÈ≤ÅÊ£íÊÄßÔºåÊä•Âëä‰∫ÜÂú®Êï∞ÊçÆÁ®ÄÁº∫„ÄÅÈïøÊñáÊ°£ÂíåÂüüÂ§ñËÆæÁΩÆ‰∏≠ÂØπÂÖàÂâçÁ≥ªÁªüÁöÑÊîπËøõ„ÄÇÊàë‰ª¨Âú® https://github.com/SapienzaNLP/maverick-coref ‰∏äÂèëÂ∏ÉÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂíåÊ®°Âûã‰ª•Áî®‰∫éÁ†îÁ©∂ÁõÆÁöÑ„ÄÇ

##### **eSPARQL: Representing and Reconciling Agnostic and Atheistic Beliefs in RDF-star Knowledge Graphs**
2407.21483v2 by Xiny Pan, Daniel Hern√°ndez, Philipp Seifer, Ralf L√§mmel, Steffen Staab

Over the past few years, we have seen the emergence of large knowledge graphs
combining information from multiple sources. Sometimes, this information is
provided in the form of assertions about other assertions, defining contexts
where assertions are valid. A recent extension to RDF which admits statements
over statements, called RDF-star, is in revision to become a W3C standard.
However, there is no proposal for a semantics of these RDF-star statements nor
a built-in facility to operate over them. In this paper, we propose a query
language for epistemic RDF-star metadata based on a four-valued logic, called
eSPARQL. Our proposed query language extends SPARQL-star, the query language
for RDF-star, with a new type of FROM clause to facilitate operating with
multiple and sometimes conflicting beliefs. We show that the proposed query
language can express four use case queries, including the following features:
(i) querying the belief of an individual, (ii) the aggregating of beliefs,
(iii) querying who is conflicting with somebody, and (iv) beliefs about beliefs
(i.e., nesting of beliefs).

ÊëòË¶ÅÔºöÂú®ÈÅéÂéªÂπæÂπ¥ÔºåÊàëÂÄëÂ∑≤Á∂ìÁúãÂà∞Â§ßÂûãÁü•Ë≠òÂúñË≠úÁöÑÂá∫ÁèæÔºåÁµêÂêà‰æÜËá™Â§öÂÄã‰æÜÊ∫êÁöÑË≥áË®ä„ÄÇÊúâÊôÇÔºåÊ≠§Ë≥áË®ä‰ª•Â∞çÂÖ∂‰ªñÊñ∑Ë®ÄÁöÑÊñ∑Ë®ÄÂΩ¢ÂºèÊèê‰æõÔºåÂÆöÁæ©Êñ∑Ë®ÄÊúâÊïàÁöÑËÑàÁµ°„ÄÇRDF ÁöÑÊúÄÊñ∞Êì¥ÂÖÖÂÖÅË®±Â∞çÊñ∑Ë®ÄÈÄ≤Ë°åÈô≥Ëø∞ÔºåÁ®±ÁÇ∫ RDF-starÔºåÁõÆÂâçÊ≠£Âú®‰øÆË®ÇÁÇ∫ W3C Ê®ôÊ∫ñ„ÄÇÁÑ∂ËÄåÔºåÊ≤íÊúâÈáùÂ∞çÈÄô‰∫õ RDF-star Èô≥Ëø∞ÁöÑË™ûÁæ©ÊèêÂá∫Âª∫Ë≠∞Ôºå‰πüÊ≤íÊúâÂÖßÂª∫ÁöÑÈÅã‰ΩúÂ∑•ÂÖ∑„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂü∫ÊñºÂõõÂÄºÈÇèËºØÁöÑÁü•Ë≠ò RDF-star ÂÖÉË≥áÊñôÊü•Ë©¢Ë™ûË®ÄÔºåÁ®±ÁÇ∫ eSPARQL„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÊì¥ÂÖÖ‰∫Ü RDF-star ÁöÑÊü•Ë©¢Ë™ûË®Ä SPARQL-starÔºå‰∏¶‰ΩøÁî®Êñ∞ÁöÑ FROM Â≠êÂè•È°ûÂûã‰æÜ‰øÉÈÄ≤ÈÅã‰ΩúÔºåÂåÖÊã¨Â§öÈáç‰∏îÊúâÊôÇË°ùÁ™ÅÁöÑ‰ø°Âøµ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫ÁöÑÊü•Ë©¢Ë™ûË®ÄÂèØ‰ª•Ë°®ÈÅîÂõõÂÄãÁî®‰æãÊü•Ë©¢ÔºåÂåÖÊã¨‰∏ãÂàóÂäüËÉΩÔºö(i) Êü•Ë©¢ÂÄã‰∫∫ÁöÑ‰ø°ÂøµÔºå(ii) ‰ø°ÂøµÁöÑÂΩôÁ∏ΩÔºå(iii) Êü•Ë©¢Ë™∞ËàáÊüê‰∫∫Ë°ùÁ™ÅÔºå‰ª•Âèä (iv) ÈóúÊñº‰ø°ÂøµÁöÑ‰ø°ÂøµÔºàÂç≥‰ø°ÂøµÁöÑÂ∑¢ÁãÄÔºâ„ÄÇ

##### **On the Problem of Text-To-Speech Model Selection for Synthetic Data Generation in Automatic Speech Recognition**
2407.21476v1 by Nick Rossenbach, Ralf Schl√ºter, Sakriani Sakti

The rapid development of neural text-to-speech (TTS) systems enabled its
usage in other areas of natural language processing such as automatic speech
recognition (ASR) or spoken language translation (SLT). Due to the large number
of different TTS architectures and their extensions, selecting which TTS
systems to use for synthetic data creation is not an easy task. We use the
comparison of five different TTS decoder architectures in the scope of
synthetic data generation to show the impact on CTC-based speech recognition
training. We compare the recognition results to computable metrics like NISQA
MOS and intelligibility, finding that there are no clear relations to the ASR
performance. We also observe that for data generation auto-regressive decoding
performs better than non-autoregressive decoding, and propose an approach to
quantify TTS generalization capabilities.

ÊëòË¶ÅÔºöÁ•ûÁ∂ìÊñáÂ≠óËΩâË™ûÈü≥ (TTS) Á≥ªÁµ±ÁöÑÂø´ÈÄüÁôºÂ±ïÔºå‰ΩøÂÖ∂Âæó‰ª•ÊáâÁî®ÊñºËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÂÖ∂‰ªñÈ†òÂüüÔºå‰æãÂ¶ÇËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) ÊàñÂè£Ë™ûÁøªË≠Ø (SLT)„ÄÇÁî±Êñº TTS Êû∂ÊßãÂèäÂÖ∂Êì¥ÂÖÖÂäüËÉΩÁ®ÆÈ°ûÁπÅÂ§öÔºåÂõ†Ê≠§ÈÅ∏ÊìáÂì™ÂÄã TTS Á≥ªÁµ±‰æÜÈÄ≤Ë°åÂêàÊàêË≥áÊñôÂª∫Á´ã‰∏¶ÈùûÊòì‰∫ã„ÄÇÊàëÂÄëÂú®ÂêàÊàêË≥áÊñôÁî¢ÁîüÁØÑÂúçÂÖßÊØîËºÉ‰∫îÁ®Æ‰∏çÂêåÁöÑ TTS Ëß£Á¢ºÂô®Êû∂ÊßãÔºå‰ª•È°ØÁ§∫Â∞çÂü∫Êñº CTC ÁöÑË™ûÈü≥Ëæ®Ë≠òË®ìÁ∑¥ÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÂ∞áËæ®Ë≠òÁµêÊûúËàáÂèØË®àÁÆóÁöÑÊåáÊ®ôÔºà‰æãÂ¶Ç NISQA MOS ÂíåÂèØÁêÜËß£Â∫¶ÔºâÈÄ≤Ë°åÊØîËºÉÔºåÁôºÁèæËàá ASR ÊïàËÉΩ‰∏¶ÁÑ°ÊòéÁ¢∫ÁöÑÈóú‰øÇ„ÄÇÊàëÂÄëÈÇÑËßÄÂØüÂà∞ÔºåÂ∞çÊñºË≥áÊñôÁî¢ÁîüÔºåËá™Ëø¥Ê≠∏Ëß£Á¢ºÁöÑË°®ÁèæÂÑ™ÊñºÈùûËá™Ëø¥Ê≠∏Ëß£Á¢ºÔºå‰∏¶ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÈáèÂåñ TTS Ê≥õÂåñËÉΩÂäõÁöÑÊñπÊ≥ï„ÄÇ

##### **Fine-gained Zero-shot Video Sampling**
2407.21475v1 by Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu

Incorporating a temporal dimension into pretrained image diffusion models for
video generation is a prevalent approach. However, this method is
computationally demanding and necessitates large-scale video datasets. More
critically, the heterogeneity between image and video datasets often results in
catastrophic forgetting of the image expertise. Recent attempts to directly
extract video snippets from image diffusion models have somewhat mitigated
these problems. Nevertheless, these methods can only generate brief video clips
with simple movements and fail to capture fine-grained motion or non-grid
deformation. In this paper, we propose a novel Zero-Shot video Sampling
algorithm, denoted as $\mathcal{ZS}^2$, capable of directly sampling
high-quality video clips from existing image synthesis methods, such as Stable
Diffusion, without any training or optimization. Specifically, $\mathcal{ZS}^2$
utilizes the dependency noise model and temporal momentum attention to ensure
content consistency and animation coherence, respectively. This ability enables
it to excel in related tasks, such as conditional and context-specialized video
generation and instruction-guided video editing. Experimental results
demonstrate that $\mathcal{ZS}^2$ achieves state-of-the-art performance in
zero-shot video generation, occasionally outperforming recent supervised
methods.
  Homepage: \url{https://densechen.github.io/zss/}.

ÊëòË¶ÅÔºöÂ∞áÊôÇÈñìÁ∂≠Â∫¶Êï¥ÂêàÂà∞È†êË®ìÁ∑¥ÂΩ±ÂÉèÊì¥Êï£Ê®°Âûã‰∏≠Ôºå‰ª•ÈÄ≤Ë°åÂΩ±ÁâáÁîüÊàêÔºåÊòØ‰∏ÄÁ®ÆÊôÆÈÅçÁöÑÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÊñπÊ≥ïÂú®ÈÅãÁÆó‰∏äË¶ÅÊ±ÇÂæàÈ´òÔºå‰∏îÈúÄË¶ÅÂ§ßË¶èÊ®°ÁöÑÂΩ±ÁâáË≥áÊñôÈõÜ„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºåÂΩ±ÂÉèË≥áÊñôÈõÜÂíåÂΩ±ÁâáË≥áÊñôÈõÜ‰πãÈñìÁöÑÁï∞Ë≥™ÊÄßÔºåÈÄöÂ∏∏ÊúÉÂ∞éËá¥ÂΩ±ÂÉèÂ∞àÊ•≠Áü•Ë≠òÁöÑÁÅΩÈõ£ÊÄßÈÅ∫Âøò„ÄÇËøëÊúüÂòóË©¶Áõ¥Êé•ÂæûÂΩ±ÂÉèÊì¥Êï£Ê®°Âûã‰∏≠Êì∑ÂèñÂΩ±ÁâáÁâáÊÆµÔºåÂú®ÊüêÁ®ÆÁ®ãÂ∫¶‰∏äÁ∑©Ëß£‰∫ÜÈÄô‰∫õÂïèÈ°å„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÈÄô‰∫õÊñπÊ≥ïÂè™ËÉΩÁîüÊàêÂÖ∑ÊúâÁ∞°ÂñÆÂãï‰ΩúÁöÑÁ∞°Áü≠ÂΩ±ÁâáÁâáÊÆµÔºåËÄå‰∏îÁÑ°Ê≥ïÊçïÊçâÁ¥∞Á≤íÂ∫¶ÁöÑÂãï‰ΩúÊàñÈùûÁ∂≤Ê†ºËÆäÂΩ¢„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈõ∂Ê¨°Â≠∏ÁøíÂΩ±ÁâáÊé°Ê®£ÊºîÁÆóÊ≥ïÔºåË°®Á§∫ÁÇ∫ $\mathcal{ZS}^2$ÔºåÂÆÉËÉΩÂ§†Áõ¥Êé•ÂæûÁèæÊúâÁöÑÂΩ±ÂÉèÂêàÊàêÊñπÊ≥ïÔºà‰æãÂ¶Ç Stable DiffusionÔºâ‰∏≠Êé°Ê®£È´òÂìÅË≥™ÁöÑÂΩ±ÁâáÁâáÊÆµÔºåËÄåÁÑ°ÈúÄ‰ªª‰ΩïË®ìÁ∑¥ÊàñÊúÄ‰Ω≥Âåñ„ÄÇÂÖ∑È´î‰æÜË™™Ôºå$\mathcal{ZS}^2$ ÂàÜÂà•Âà©Áî®‰æùË≥¥ÈõúË®äÊ®°ÂûãÂíåÊôÇÈñìÂãïÈáèÊ≥®ÊÑèÂäõÔºå‰ª•Á¢∫‰øùÂÖßÂÆπ‰∏ÄËá¥ÊÄßÂíåÂãïÁï´ÈÄ£Ë≤´ÊÄß„ÄÇÈÄôÁ®ÆËÉΩÂäõ‰ΩøÂÆÉËÉΩÂ§†Âú®Áõ∏Èóú‰ªªÂãô‰∏≠Ë°®ÁèæÂá∫Ëâ≤Ôºå‰æãÂ¶ÇÊ¢ù‰ª∂ÂºèÂíåÁâπÂÆöÊñºÂÖßÂÆπÁöÑÂΩ±ÁâáÁîüÊàêÔºå‰ª•ÂèäÊåá‰ª§ÂºïÂ∞éÁöÑÂΩ±ÁâáÁ∑®ËºØ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºå$\mathcal{ZS}^2$ Âú®Èõ∂Ê¨°Â≠∏ÁøíÂΩ±ÁâáÁîüÊàê‰∏≠ÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºåÂÅ∂ÁàæÂÑ™ÊñºÊúÄËøëÁöÑÁõ£Áù£ÂºèÊñπÊ≥ï„ÄÇ‰∏ªÈ†ÅÔºö\url{https://densechen.github.io/zss/}„ÄÇ

##### **An Invertible State Space for Process Trees**
2407.21468v1 by Gero Kolhof, Sebastiaan J. van Zelst

Process models are, like event data, first-class citizens in most process
mining approaches. Several process modeling formalisms have been proposed and
used, e.g., Petri nets, BPMN, and process trees. Despite their frequent use,
little research addresses the formal properties of process trees and the
corresponding potential to improve the efficiency of solving common
computational problems. Therefore, in this paper, we propose an invertible
state space definition for process trees and demonstrate that the corresponding
state space graph is isomorphic to the state space graph of the tree's inverse.
Our result supports the development of novel, time-efficient, decomposition
strategies for applications of process trees. Our experiments confirm that our
state space definition allows for the adoption of bidirectional state space
search, which significantly improves the overall performance of state space
searches.

ÊëòË¶ÅÔºöÂú®Â§ßÂ§öÊï∞ÊµÅÁ®ãÊåñÊéòÊñπÊ≥ï‰∏≠ÔºåÊµÅÁ®ãÊ®°Âûã‰∏é‰∫ã‰ª∂Êï∞ÊçÆÁ±ª‰ººÔºåÈÉΩÊòØ‰∏ÄÁ≠âÂÖ¨Ê∞ë„ÄÇÂ∑≤ÁªèÊèêÂá∫Âπ∂‰ΩøÁî®‰∫ÜÂ§öÁßçÊµÅÁ®ãÂª∫Ê®°ÂΩ¢Âºè‰∏ª‰πâÔºå‰æãÂ¶ÇÔºåPetri ÁΩë„ÄÅBPMN ÂíåÊµÅÁ®ãÊ†ë„ÄÇÂ∞ΩÁÆ°ÂÆÉ‰ª¨ÁªèÂ∏∏‰ΩøÁî®Ôºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂Ê∂âÂèäÊµÅÁ®ãÊ†ëÁöÑÂΩ¢ÂºèÂ±ûÊÄß‰ª•ÂèäÊèêÈ´òËß£ÂÜ≥Â∏∏ËßÅËÆ°ÁÆóÈóÆÈ¢òÁöÑÊïàÁéáÁöÑÁõ∏Â∫îÊΩúÂäõ„ÄÇÂõ†Ê≠§ÔºåÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÊµÅÁ®ãÊ†ëÁöÑÂèØÈÄÜÁä∂ÊÄÅÁ©∫Èó¥ÂÆö‰πâÔºåÂπ∂ËØÅÊòé‰∫ÜÁõ∏Â∫îÁöÑÁä∂ÊÄÅÁ©∫Èó¥Âõæ‰∏éÊ†ëÁöÑÈÄÜÁä∂ÊÄÅÁ©∫Èó¥ÂõæÂêåÊûÑ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúÊîØÊåÅ‰∏∫ÊµÅÁ®ãÊ†ëÁöÑÂ∫îÁî®Á®ãÂ∫èÂºÄÂèëÊñ∞È¢ñ„ÄÅÁúÅÊó∂ÁöÑÂàÜËß£Á≠ñÁï•„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åËØÅÂÆûÔºåÊàë‰ª¨ÁöÑÁä∂ÊÄÅÁ©∫Èó¥ÂÆö‰πâÂÖÅËÆ∏ÈááÁî®ÂèåÂêëÁä∂ÊÄÅÁ©∫Èó¥ÊêúÁ¥¢ÔºåËøôÊòæËëóÊèêÈ´ò‰∫ÜÁä∂ÊÄÅÁ©∫Èó¥ÊêúÁ¥¢ÁöÑÊï¥‰ΩìÊÄßËÉΩ„ÄÇ

##### **Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data**
2407.21467v1 by Mengtian Kang, Yansong Hu, Shuo Gao, Yuanyuan Liu, Hongbei Meng, Xuemeng Li, Xuhang Chen, Hubin Zhao, Jing Fu, Guohua Hu, Wei Wang, Yanning Dai, Arokia Nathan, Peter Smielewski, Ningli Wang, Shiming Li

Childhood myopia constitutes a significant global health concern. It exhibits
an escalating prevalence and has the potential to evolve into severe,
irreversible conditions that detrimentally impact familial well-being and
create substantial economic costs. Contemporary research underscores the
importance of precisely predicting myopia progression to enable timely and
effective interventions, thereby averting severe visual impairment in children.
Such predictions predominantly rely on subjective clinical assessments, which
are inherently biased and resource-intensive, thus hindering their widespread
application. In this study, we introduce a novel, high-accuracy method for
quantitatively predicting the myopic trajectory and myopia risk in children
using only fundus images and baseline refraction data. This approach was
validated through a six-year longitudinal study of 3,408 children in Henan,
utilizing 16,211 fundus images and corresponding refractive data. Our method
based on deep learning demonstrated predictive accuracy with an error margin of
0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of
developing myopia and high myopia, respectively. These findings confirm the
utility of our model in supporting early intervention strategies and in
significantly reducing healthcare costs, particularly by obviating the need for
additional metadata and repeated consultations. Furthermore, our method was
designed to rely only on fundus images and refractive error data, without the
need for meta data or multiple inquiries from doctors, strongly reducing the
associated medical costs and facilitating large-scale screening. Our model can
even provide good predictions based on only a single time measurement.
Consequently, the proposed method is an important means to reduce medical
inequities caused by economic disparities.

ÊëòË¶ÅÔºöÂÖíÁ´•ËøëË¶ñÊßãÊàêÂÖ®ÁêÉÈáçË¶ÅÁöÑÂÅ•Â∫∑ÂïèÈ°å„ÄÇÂÆÉÈ°ØÁ§∫Âá∫Êó•ÁõäÂ¢ûÂä†ÁöÑÁõõË°åÁéáÔºå‰∏¶ÂèØËÉΩÊºîËÆäÊàêÂö¥Èáç„ÄÅ‰∏çÂèØÈÄÜËΩâÁöÑÁãÄÊ≥ÅÔºåÂ∞çÂÆ∂Â∫≠Á¶èÁ•âÈÄ†Êàê‰∏çÂà©ÂΩ±ÈüøÔºå‰∏¶Áî¢ÁîüÂ§ßÈáèÁöÑÁ∂ìÊøüÊàêÊú¨„ÄÇÁèæ‰ª£Á†îÁ©∂Âº∑Ë™øÁ≤æÊ∫ñÈ†êÊ∏¨ËøëË¶ñÈÄ≤Â±ïÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÂØ¶ÁèæÂèäÊôÇÊúâÊïàÁöÑÂπ≤È†êÔºåÂæûËÄåÈÅøÂÖçÂÖíÁ´•Âá∫ÁèæÂö¥ÈáçÁöÑË¶ñÂäõÊêçÂÆ≥„ÄÇÊ≠§È°ûÈ†êÊ∏¨‰∏ªË¶Å‰æùË≥¥‰∏ªËßÄÁöÑËá®Â∫äË©ï‰º∞ÔºåÂÖ∂Êú¨Ë∫´ÂÖ∑ÊúâÂÅèË¶ã‰∏îË≥áÊ∫êÂØÜÈõÜÔºåÂæûËÄåÈòªÁ§ô‰∫ÜÂÆÉÂÄëÁöÑÂª£Ê≥õÊáâÁî®„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©é„ÄÅÈ´òÁ≤æÁ¢∫Â∫¶ÁöÑÊñπÊ≥ïÔºåÂÉÖ‰ΩøÁî®ÁúºÂ∫ïÂúñÂÉèÂíåÂü∫Á∑öÂ±àÂÖâÊï∏ÊìöÔºåÂ∞±ËÉΩÂÆöÈáèÈ†êÊ∏¨ÂÖíÁ´•ÁöÑËøëË¶ñËªåË∑°ÂíåËøëË¶ñÈ¢®Èö™„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈÄöÈÅéÂ∞çÊ≤≥ÂçóÁúÅ 3,408 ÂêçÂÖíÁ´•ÈÄ≤Ë°åÁÇ∫ÊúüÂÖ≠Âπ¥ÁöÑÁ∏±ÂêëÁ†îÁ©∂ÔºåÂà©Áî® 16,211 ÂºµÁúºÂ∫ïÂúñÂÉèÂíåÁõ∏ÊáâÁöÑÂ±àÂÖâÊï∏ÊìöÈÄ≤Ë°å‰∫ÜÈ©óË≠â„ÄÇÊàëÂÄëÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÈ†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÔºåÂπ¥Ë™§Â∑ÆÁØÑÂúçÁÇ∫ 0.311DÔºåÈ†êÊ∏¨ÁôºÁîüËøëË¶ñÂíåÈ´òÂ∫¶ËøëË¶ñÁöÑÈ¢®Èö™ÁöÑ AUC ÂàÜÊï∏ÂàÜÂà•ÁÇ∫ 0.944 Âíå 0.995„ÄÇÈÄô‰∫õÁôºÁèæË≠âÂØ¶‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÊîØÊåÅÊó©ÊúüÂπ≤È†êÁ≠ñÁï•ÂíåÈ°ØËëóÈôç‰ΩéÈÜ´ÁôÇ‰øùÂÅ•ÊàêÊú¨ÊñπÈù¢ÁöÑÊïàÁî®ÔºåÁâπÂà•ÊòØÈÄöÈÅéÊ∂àÈô§Â∞çÈ°çÂ§ñÂÖÉÊï∏ÊìöÂíåÈáçË§áË´ÆË©¢ÁöÑÈúÄË¶Å„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÊñπÊ≥ïË¢´Ë®≠Ë®àÁÇ∫ÂÉÖ‰æùË≥¥ÁúºÂ∫ïÂúñÂÉèÂíåÂ±àÂÖâ‰∏çÊ≠£Êï∏ÊìöÔºåËÄåÁÑ°ÈúÄÂÖÉÊï∏ÊìöÊàñÈÜ´ÁîüÁöÑÂ§öÊ¨°Ë©¢ÂïèÔºåÂæûËÄåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜÁõ∏ÈóúÁöÑÈÜ´ÁôÇÊàêÊú¨Ôºå‰∏¶‰øÉÈÄ≤‰∫ÜÂ§ßË¶èÊ®°ÁØ©Êü•„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÁîöËá≥ÂèØ‰ª•ÂÉÖÊ†πÊìöÂñÆÊ¨°ÊôÇÈñìÊ∏¨ÈáèÊèê‰æõËâØÂ•ΩÁöÑÈ†êÊ∏¨„ÄÇÂõ†Ê≠§ÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊòØÊ∏õÂ∞ëÁî±Á∂ìÊøüÂ∑ÆË∑ùÈÄ†ÊàêÁöÑÈÜ´ÁôÇ‰∏çÂπ≥Á≠âÁöÑÈáçË¶ÅÊâãÊÆµ„ÄÇ

##### **KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making**
2407.21459v1 by Gilang Fajar Febrian, Grazziela Figueredo

Data is crucial for evidence-based policymaking and enhancing public
services, including those at the Ministry of Finance of the Republic of
Indonesia. However, the complexity and dynamic nature of governmental financial
data and regulations can hinder decision-making. This study investigates the
potential of Large Language Models (LLMs) to address these challenges, focusing
on Indonesia's financial data and regulations. While LLMs are effective in the
financial sector, their use in the public sector in Indonesia is unexplored.
This study undertakes an iterative process to develop KemenkeuGPT using the
LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and
fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of
Finance, Statistics Indonesia and the International Monetary Fund (IMF).
Surveys and interviews with Ministry officials informed, enhanced and
fine-tuned the model. We evaluated the model using human feedback, LLM-based
evaluation and benchmarking. The model's accuracy improved from 35% to 61%,
with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation
Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness
with 73% faithfulness, 40% precision and 60% recall, outperforming several
other base models. An interview with an expert from the Ministry of Finance
indicated that KemenkeuGPT has the potential to become an essential tool for
decision-making. These results are expected to improve with continuous human
feedback.

ÊëòË¶ÅÔºö<paragraph>Ë≥áÊñôÂ∞çÊñºË≠âÊìöÂü∫Á§éÁöÑÊîøÁ≠ñÂà∂ÂÆöÂíåÊîπÂñÑÂÖ¨ÂÖ±ÊúçÂãôËá≥ÈóúÈáçË¶ÅÔºåÂåÖÊã¨Âç∞Â∞ºÂÖ±ÂíåÂúãË≤°ÊîøÈÉ®„ÄÇÁÑ∂ËÄåÔºåÊîøÂ∫úË≤°ÂãôË≥áÊñôÂíåÊ≥ïË¶èÁöÑË§áÈõúÊÄßÂíåÂãïÊÖãÊÄßÂèØËÉΩÊúÉÈòªÁ§ôÊ±∫Á≠ñ„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫ÜÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÁöÑÊΩõÂäõÔºåÈáçÈªûÈóúÊ≥®Âç∞Â∞ºÁöÑË≤°ÂãôË≥áÊñôÂíåÊ≥ïË¶è„ÄÇÈõñÁÑ∂ LLM Âú®ÈáëËûçÈ†òÂüüÂæàÊúâÊïàÔºå‰ΩÜÂÆÉÂÄëÂú®Âç∞Â∞ºÂÖ¨ÂÖ±ÈÉ®ÈñÄÁöÑ‰ΩøÁî®Â∞öÊú™ÂæóÂà∞Êé¢Á¥¢„ÄÇÊú¨Á†îÁ©∂Êé°Áî®Ëø≠‰ª£ÊµÅÁ®ãÔºå‰ΩøÁî®Â∏∂ÊúâÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG)„ÄÅÊèêÁ§∫Â∑•Á®ãÂíåÂæÆË™øÁöÑ LangChain ‰æÜÈñãÁôº KemenkeuGPT„ÄÇË≥áÊñôÈõÜÂæû 2003 Âπ¥Âà∞ 2023 Âπ¥Êî∂ÈõÜËá™Âç∞Â∞ºË≤°ÊîøÈÉ®„ÄÅÂç∞Â∞ºÁµ±Ë®àÂ±ÄÂíåÂúãÈöõË≤®Âπ£Âü∫ÈáëÁµÑÁπî (IMF)„ÄÇÂ∞çË≤°ÊîøÈÉ®ÂÆòÂì°ÁöÑË™øÊü•ÂíåË®™Ë´áÂëäÁü•„ÄÅÂ¢ûÂº∑ÂíåÂæÆË™ø‰∫ÜÊ®°Âûã„ÄÇÊàëÂÄë‰ΩøÁî®‰∫∫È°ûÂõûÈ•ã„ÄÅÂü∫Êñº LLM ÁöÑË©ï‰º∞ÂíåÂü∫Ê∫ñÂ∞çÊ®°ÂûãÈÄ≤Ë°å‰∫ÜË©ï‰º∞„ÄÇË©≤Ê®°ÂûãÁöÑÊ∫ñÁ¢∫ÁéáÂæû 35% ÊèêÈ´òÂà∞ 61%ÔºåÊ≠£Á¢∫ÁéáÂæû 48% ÊèêÈ´òÂà∞ 64%„ÄÇÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêË©ï‰º∞ (RAGAS) Ê°ÜÊû∂Ë°®ÊòéÔºåKemenkeuGPT ÈÅîÂà∞‰∫Ü 44% ÁöÑÊ≠£Á¢∫ÁéáÔºå73% ÁöÑÂø†ÂØ¶Â∫¶Ôºå40% ÁöÑÁ≤æÁ¢∫Â∫¶Âíå 60% ÁöÑÂè¨ÂõûÁéáÔºåÂÑ™ÊñºÂÖ∂‰ªñÂπæÂÄãÂü∫Á§éÊ®°Âûã„ÄÇÂ∞çË≤°ÊîøÈÉ®Â∞àÂÆ∂ÁöÑÊé°Ë®™Ë°®ÊòéÔºåKemenkeuGPT ÊúâÂèØËÉΩÊàêÁÇ∫Ê±∫Á≠ñÁöÑÈáçË¶ÅÂ∑•ÂÖ∑„ÄÇÈ†êË®àÈÄô‰∫õÁµêÊûúÊúÉÈö®ËëóÊåÅÁ∫åÁöÑ‰∫∫È°ûÂõûÈ•ãËÄåÂæóÂà∞ÊîπÂñÑ„ÄÇ</paragraph>

##### **TinyChirp: Bird Song Recognition Using TinyML Models on Low-power Wireless Acoustic Sensors**
2407.21453v1 by Zhaolan Huang, Adrien Tousnakhoff, Polina Kozyr, Roman Rehausen, Felix Bie√ümann, Robert Lachlan, Cedric Adjih, Emmanuel Baccelli

Monitoring biodiversity at scale is challenging. Detecting and identifying
species in fine grained taxonomies requires highly accurate machine learning
(ML) methods. Training such models requires large high quality data sets. And
deploying these models to low power devices requires novel compression
techniques and model architectures. While species classification methods have
profited from novel data sets and advances in ML methods, in particular neural
networks, deploying these state of the art models to low power devices remains
difficult. Here we present a comprehensive empirical comparison of various
tinyML neural network architectures and compression techniques for species
classification. We focus on the example of bird song detection, more concretely
a data set curated for studying the corn bunting bird species. The data set is
released along with all code and experiments of this study. In our experiments
we compare predictive performance, memory and time complexity of classical
spectrogram based methods and recent approaches operating on raw audio signal.
Our results indicate that individual bird species can be robustly detected with
relatively simple architectures that can be readily deployed to low power
devices.

ÊëòË¶ÅÔºöÂ§ßË¶èÊ®°Áõ£ÊéßÁîüÁâ©Â§öÊ®£ÊÄßÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÂú®Á¥∞Á≤íÂ∫¶ÂàÜÈ°ûÊ≥ï‰∏≠Ê™¢Ê∏¨ÂíåË≠òÂà•Áâ©Á®ÆÈúÄË¶ÅÈ´òÂ∫¶Ê∫ñÁ¢∫ÁöÑÊ©üÂô®Â≠∏Áøí (ML) ÊñπÊ≥ï„ÄÇË®ìÁ∑¥Ê≠§È°ûÊ®°ÂûãÈúÄË¶ÅÂ§ßÈáèÈ´òÂìÅË≥™ÁöÑË≥áÊñôÈõÜ„ÄÇËÄåÂ∞áÈÄô‰∫õÊ®°ÂûãÈÉ®ÁΩ≤Âà∞‰ΩéÂäüËÄóË£ùÁΩÆÈúÄË¶ÅÂâµÊñ∞ÁöÑÂ£ìÁ∏ÆÊäÄË°ìÂíåÊ®°ÂûãÊû∂Êßã„ÄÇÈõñÁÑ∂Áâ©Á®ÆÂàÜÈ°ûÊñπÊ≥ïÂèóÁõäÊñºÊñ∞Á©éÁöÑË≥áÊñôÈõÜÂíåÊ©üÂô®Â≠∏ÁøíÊñπÊ≥ïÁöÑÈÄ≤Ê≠•ÔºåÁâπÂà•ÊòØÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºå‰ΩÜÂ∞áÈÄô‰∫õÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÈÉ®ÁΩ≤Âà∞‰ΩéÂäüËÄóË£ùÁΩÆ‰ªçÁÑ∂ÂæàÂõ∞Èõ£„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂêÑÁ®Æ tinyML Á•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÂíåÁâ©Á®ÆÂàÜÈ°ûÂ£ìÁ∏ÆÊäÄË°ìÁöÑÂÖ®Èù¢ÂØ¶Ë≠âÊØîËºÉ„ÄÇÊàëÂÄëÂ∞àÊ≥®ÊñºÈ≥•È≥¥ËÅ≤Ê™¢Ê∏¨ÁöÑÁØÑ‰æãÔºåÊõ¥ÂÖ∑È´îÂú∞Ë™™ÔºåÊòØÁÇ∫Á†îÁ©∂ÁéâÁ±≥ÈµêÈ≥•È°ûÁâ©Á®ÆËÄåÁ≠ñÂ±ïÁöÑË≥áÊñôÈõÜ„ÄÇË©≤Ë≥áÊñôÈõÜËàáÊú¨Á†îÁ©∂ÁöÑÊâÄÊúâÁ®ãÂºèÁ¢ºÂíåÂØ¶È©ó‰∏ÄËµ∑ÁôºÂ∏É„ÄÇÂú®ÊàëÂÄëÁöÑÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÊØîËºÉ‰∫ÜÂü∫ÊñºÁ∂ìÂÖ∏ËÅ≤Ë≠úÂúñÁöÑÊñπÊ≥ïÂíåËôïÁêÜÂéüÂßãÈü≥Ë®äË®äËôüÁöÑÊúÄÊñ∞ÊñπÊ≥ïÁöÑÈ†êÊ∏¨ÊïàËÉΩ„ÄÅË®òÊÜ∂È´îÂíåÊôÇÈñìË§áÈõúÂ∫¶„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÂèØ‰ª•ÈÄèÈÅéÁõ∏Â∞çÁ∞°ÂñÆÁöÑÊû∂ÊßãÁ©©ÂÅ•Âú∞Ê™¢Ê∏¨ÂÄãÂà•È≥•È°ûÁâ©Á®ÆÔºåÈÄô‰∫õÊû∂ÊßãÂèØ‰ª•ËºïÊòìÂú∞ÈÉ®ÁΩ≤Âà∞‰ΩéÂäüËÄóË£ùÁΩÆ„ÄÇ

##### **Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments**
2407.21452v1 by Haodong Hong, Sen Wang, Zi Huang, Qi Wu, Jiajun Liu

Real-world navigation often involves dealing with unexpected obstructions
such as closed doors, moved objects, and unpredictable entities. However,
mainstream Vision-and-Language Navigation (VLN) tasks typically assume
instructions perfectly align with the fixed and predefined navigation graphs
without any obstructions. This assumption overlooks potential discrepancies in
actual navigation graphs and given instructions, which can cause major failures
for both indoor and outdoor agents. To address this issue, we integrate diverse
obstructions into the R2R dataset by modifying both the navigation graphs and
visual observations, introducing an innovative dataset and task, R2R with
UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers
of path obstructions to generate instruction-reality mismatches for VLN
research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods
inevitably encounter significant challenges when facing such mismatches,
indicating that they rigidly follow instructions rather than navigate
adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN),
which includes a curriculum training strategy and virtual graph construction to
help agents effectively adapt to obstructed environments. Empirical results
show that ObVLN not only maintains robust performance in unobstructed scenarios
but also achieves a substantial performance advantage with unexpected
obstructions.

ÊëòË¶ÅÔºöÁé∞ÂÆû‰∏ñÁïåÁöÑÂØºËà™ÈÄöÂ∏∏Ê∂âÂèäÂ§ÑÁêÜÊÑèÂ§ñÁöÑÈöúÁ¢çÔºå‰æãÂ¶ÇÂÖ≥ÁùÄÁöÑÈó®„ÄÅÁßªÂä®ÁöÑÁâ©‰ΩìÂíå‰∏çÂèØÈ¢ÑÊµãÁöÑÂÆû‰Ωì„ÄÇÁÑ∂ËÄåÔºå‰∏ªÊµÅÁöÑËßÜËßâÂíåËØ≠Ë®ÄÂØºËà™ (VLN) ‰ªªÂä°ÈÄöÂ∏∏ÂÅáËÆæÊåá‰ª§‰∏éÂõ∫ÂÆöÁöÑÂíåÈ¢ÑÂÆö‰πâÁöÑÂØºËà™ÂõæÂÆåÂÖ®‰∏ÄËá¥ÔºåÊ≤°Êúâ‰ªª‰ΩïÈöúÁ¢ç„ÄÇËøôÁßçÂÅáËÆæÂøΩÁï•‰∫ÜÂÆûÈôÖÂØºËà™ÂõæÂíåÁªôÂÆöÊåá‰ª§‰∏≠ÊΩúÂú®ÁöÑÂ∑ÆÂºÇÔºåËøôÂèØËÉΩ‰ºöÂØºËá¥ÂÆ§ÂÜÖÂíåÂÆ§Â§ñ‰ª£ÁêÜÂá∫Áé∞ÈáçÂ§ßÊïÖÈöú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÈÄöËøá‰øÆÊîπÂØºËà™ÂõæÂíåËßÜËßâËßÇÂØüÔºåÂ∞ÜÂêÑÁßçÈöúÁ¢çÊï¥ÂêàÂà∞ R2R Êï∞ÊçÆÈõÜ‰∏≠ÔºåÂºïÂÖ•‰∫ÜÂàõÊñ∞Êï∞ÊçÆÈõÜÂíå‰ªªÂä°ÔºåÂç≥Â∏¶ÊúâÊÑèÂ§ñÈöúÁ¢çÁöÑ R2R (R2R-UNO)„ÄÇR2R-UNO ÂåÖÂê´ÂêÑÁßçÁ±ªÂûãÂíåÊï∞ÈáèÁöÑË∑ØÂæÑÈöúÁ¢çÔºå‰ª•ÁîüÊàê VLN Á†îÁ©∂ÁöÑÊåá‰ª§-Áé∞ÂÆû‰∏çÂåπÈÖç„ÄÇÂú® R2R-UNO ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåÊúÄÂÖàËøõÁöÑ VLN ÊñπÊ≥ïÂú®Èù¢ÂØπÊ≠§Á±ª‰∏çÂåπÈÖçÊó∂‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÈÅáÂà∞ÈáçÂ§ßÊåëÊàòÔºåËøôË°®ÊòéÂÆÉ‰ª¨‰∏•Ê†ºÈÅµÂæ™Êåá‰ª§ÔºåËÄå‰∏çÊòØËá™ÈÄÇÂ∫îÂú∞ÂØºËà™„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫ ObVLNÔºàÂèóÈòª VLNÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÊã¨ËØæÁ®ãËÆ≠ÁªÉÁ≠ñÁï•ÂíåËôöÊãüÂõæÊûÑÂª∫Ôºå‰ª•Â∏ÆÂä©‰ª£ÁêÜÊúâÊïàÂú∞ÈÄÇÂ∫îÂèóÈòªÁéØÂ¢É„ÄÇÁªèÈ™åÁªìÊûúË°®ÊòéÔºåObVLN ‰∏ç‰ªÖÂú®Êó†ÈöúÁ¢çÂú∫ÊôØ‰∏≠‰øùÊåÅ‰∫ÜÁ®≥ÂÅ•ÁöÑÊÄßËÉΩÔºåËÄå‰∏îÂú®ÊÑèÂ§ñÈöúÁ¢ç‰∏≠‰πüËé∑Âæó‰∫ÜÂÆûË¥®ÊÄßÁöÑÊÄßËÉΩ‰ºòÂäø„ÄÇ

##### **Improving Faithfulness of Large Language Models in Summarization via Sliding Generation and Self-Consistency**
2407.21443v1 by Taiji Li, Zhi Li, Yin Zhang

Despite large language models (LLMs) have demonstrated impressive performance
in various tasks, they are still suffering from the factual inconsistency
problem called hallucinations. For instance, LLMs occasionally generate content
that diverges from source article, and prefer to extract information that
appears at the beginning and end of the context, especially in long document
summarization. Inspired by these findings, we propose to improve the
faithfulness of LLMs in summarization by impelling them to process the entire
article more fairly and faithfully. We present a novel summary generation
strategy, namely SliSum, which exploits the ideas of sliding windows and
self-consistency. Specifically, SliSum divides the source article into
overlapping windows, and utilizes LLM to generate local summaries for the
content in the windows. Finally, SliSum aggregates all local summaries using
clustering and majority voting algorithm to produce more faithful summary of
entire article. Extensive experiments demonstrate that SliSum significantly
improves the faithfulness of diverse LLMs including LLaMA-2, Claude-2 and
GPT-3.5 in both short and long text summarization, while maintaining their
fluency and informativeness and without additional fine-tuning and resources.
We further conduct qualitative and quantitative studies to investigate why
SliSum works and impacts of hyperparameters in SliSum on performance.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÊïàËÉΩÔºå‰ΩÜÂÆÉÂÄë‰ªçÈ£ΩÂèóÁ®±ÁÇ∫ÂπªË¶∫ÁöÑËôõÂÅá‰∏ç‰∏ÄËá¥ÂïèÈ°åÊâÄËã¶„ÄÇ‰æãÂ¶ÇÔºåLLM ÊúâÊôÇÊúÉÁî¢ÁîüËàáÂéüÂßãÊñáÁ´†‰∏çÂêåÁöÑÂÖßÂÆπÔºå‰∏îÂÅèÂ•ΩÊì∑ÂèñÂá∫ÁèæÂú®ÂÖßÂÆπÈñãÈ†≠ÂíåÁµêÂ∞æÁöÑË≥áË®äÔºåÁâπÂà•ÊòØÂú®Èï∑ÁØáÊñá‰ª∂ÊëòË¶Å‰∏≠„ÄÇÂèóÂà∞ÈÄô‰∫õÁôºÁèæÁöÑÂïüÁôºÔºåÊàëÂÄëÂª∫Ë≠∞ÈÄèÈÅé‰øÉ‰Ωø LLM Êõ¥ÂÖ¨Âπ≥‰∏îÂø†ÂØ¶Âú∞ËôïÁêÜÊï¥ÁØáÊñáÁ´†Ôºå‰æÜÊîπÂñÑÂÖ∂Âú®ÊëòË¶Å‰∏≠ÁöÑÁúüÂØ¶ÊÄß„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊëòË¶ÅÁî¢ÁîüÁ≠ñÁï•ÔºåÁ®±ÁÇ∫ SliSumÔºåÂÆÉÂà©Áî®ÊªëÂãïË¶ñÁ™óÂíåËá™Êàë‰∏ÄËá¥ÊÄßÁöÑÊ¶ÇÂøµ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåSliSum Â∞áÂéüÂßãÊñáÁ´†ÂàÜÊàêÈáçÁñäÁöÑË¶ñÁ™óÔºå‰∏¶Âà©Áî® LLM ÁÇ∫Ë¶ñÁ™ó‰∏≠ÁöÑÂÖßÂÆπÁî¢ÁîüÂ±ÄÈÉ®ÊëòË¶Å„ÄÇÊúÄÂæåÔºåSliSum ‰ΩøÁî®Áæ§ÈõÜÂíåÂ§öÊï∏Ê±∫ÊºîÁÆóÊ≥ïÂΩôÁ∏ΩÊâÄÊúâÂ±ÄÈÉ®ÊëòË¶ÅÔºå‰ª•Áî¢ÁîüÊõ¥Âø†ÂØ¶ÁöÑÊï¥ÁØáÊñáÁ´†ÊëòË¶Å„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË≠âÊòéÔºåSliSum Âú®Á∞°Áü≠ÂíåÈï∑ÁØáÊñáÂ≠óÊëòË¶Å‰∏≠ÔºåÈ°ØËëóÊèêÂçá‰∫Ü LLaMA-2„ÄÅClaude-2 Âíå GPT-3.5 Á≠â‰∏çÂêå LLM ÁöÑÁúüÂØ¶ÊÄßÔºåÂêåÊôÇÁ∂≠ÊåÅÂÖ∂ÊµÅÊö¢Â∫¶ÂíåË≥áË®äÈáèÔºå‰∏îÁÑ°ÈúÄÈ°çÂ§ñÁöÑÂæÆË™øÂíåË≥áÊ∫ê„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÈÄ≤Ë°åÂÆöÊÄßÂíåÂÆöÈáèÁ†îÁ©∂Ôºå‰ª•Êé¢Ë®é SliSum ÁöÑÈÅã‰ΩúÂéüÁêÜÔºå‰ª•Âèä SliSum ‰∏≠ÁöÑË∂ÖÂèÉÊï∏Â∞çÊïàËÉΩÁöÑÂΩ±Èüø„ÄÇ

##### **QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications**
2407.21441v2 by Ritvik Setty, Vinay Setty

Verifying fact-checking claims poses a significant challenge, even for
humans. Recent approaches have demonstrated that decomposing claims into
relevant questions to gather evidence enhances the efficiency of the
fact-checking process. In this paper, we provide empirical evidence showing
that this question decomposition can be effectively automated. We demonstrate
that smaller generative models, fine-tuned for the question generation task
using data augmentation from various datasets, outperform large language models
by up to 8%. Surprisingly, in some cases, the evidence retrieved using
machine-generated questions proves to be significantly more effective for
fact-checking than that obtained from human-written questions. We also perform
manual evaluation of the decomposed questions to assess the quality of the
questions generated.

ÊëòË¶ÅÔºöÈ©óË≠âÊü•Ê†∏‰∫ãÂØ¶ÁöÑËÅ≤ÊòéÔºåÂç≥‰ΩøÂ∞ç‰∫∫È°û‰æÜË™™Ôºå‰πüÊòØ‰∏ÄÈ†ÖÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇÊúÄËøëÁöÑÊñπÊ≥ïÂ∑≤Ë≠âÊòéÔºåÂ∞áËÅ≤ÊòéÂàÜËß£ÊàêÁõ∏ÈóúÂïèÈ°å‰ª•Êî∂ÈõÜË≠âÊìöÔºåÂèØ‰ª•ÊèêÈ´òÊü•Ê†∏‰∫ãÂØ¶Á®ãÂ∫èÁöÑÊïàÁéá„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÁ∂ìÈ©óË≠âÊìöÔºåË™™ÊòéÈÄôÁ®ÆÂïèÈ°åÂàÜËß£ÂèØ‰ª•ÊúâÊïàËá™ÂãïÂåñ„ÄÇÊàëÂÄëË≠âÊòé‰∫ÜËºÉÂ∞èÁöÑÁîüÊàêÊ®°ÂûãÔºåÈáùÂ∞çÂïèÈ°åÁîüÊàê‰ªªÂãôÈÄ≤Ë°åÂæÆË™øÔºå‰ΩøÁî®‰æÜËá™ÂêÑÁ®ÆË≥áÊñôÈõÜÁöÑË≥áÊñôÊì¥ÂÖÖÔºåÊØîÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑË°®ÁèæÈ´òÂá∫ 8%„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºå‰ΩøÁî®Ê©üÂô®Áî¢ÁîüÁöÑÂïèÈ°åÊâÄÊ™¢Á¥¢Âà∞ÁöÑË≠âÊìöÔºåË¢´Ë≠âÊòéÊØîÂæû‰∫∫Â∑•Êí∞ÂØ´ÁöÑÂïèÈ°å‰∏≠Áç≤ÂæóÁöÑË≠âÊìöÔºåÂ∞çÊü•Ê†∏‰∫ãÂØ¶Êõ¥ÊúâÊïà„ÄÇÊàëÂÄë‰πüÂ∞çÂàÜËß£ÁöÑÂïèÈ°åÈÄ≤Ë°åÊâãÂãïË©ï‰º∞Ôºå‰ª•Ë©ï‰º∞ÊâÄÁî¢ÁîüÂïèÈ°åÁöÑÂìÅË≥™„ÄÇ

##### **MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training**
2407.21439v1 by Zhanpeng Chen, Chengjin Xu, Yiyan Qi, Jian Guo

Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in processing and generating content across multiple data
modalities, including text, images, audio, and video. However, a significant
drawback of MLLMs is their reliance on static training data, leading to
outdated information and limited contextual awareness. This static nature
hampers their ability to provide accurate, up-to-date responses, particularly
in dynamic or rapidly evolving contexts. Integrating Multimodal
Retrieval-augmented Generation (Multimodal RAG) offers a promising solution,
but the system would inevitably encounter the multi-granularity noisy
correspondence (MNC) problem, which involves two types of noise: coarse-grained
(query-caption) and fine-grained (query-image). This noise hinders accurate
retrieval and generation. In this work, we propose \textbf{RagLLaVA}, a novel
framework with knowledge-enhanced reranking and noise-injected training, to
address these limitations. We instruction-tune the MLLM with a simple yet
effective instruction template to induce its ranking ability and serve it as a
reranker to precisely filter the top-k retrieved images. For generation, we
inject visual noise during training at the data and token levels to enhance the
generator's robustness. Extensive experiments are conducted on the subsets of
two datasets that require retrieving and reasoning over images to answer a
given query. Our results demonstrate the superiority of RagLLaVA in retrieving
accurately and generating robustly. Code and models are available at
https://github.com/IDEA-FinAI/RagLLaVA.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã (MLLM) Â∑≤Âú®Â§ÑÁêÜÂíåÁîüÊàêË∑®Â§ö‰∏™Êï∞ÊçÆÊ®°ÊÄÅÔºàÂåÖÊã¨ÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÈü≥È¢ëÂíåËßÜÈ¢ëÔºâÁöÑÂÜÖÂÆπÊñπÈù¢Â±ïÁ§∫Âá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåMLLM ÁöÑ‰∏Ä‰∏™ÈáçÂ§ßÁº∫ÁÇπÊòØÂÆÉ‰ª¨‰æùËµñ‰∫éÈùôÊÄÅËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥‰ø°ÊÅØËøáÊó∂Âíå‰∏ä‰∏ãÊñáÊÑüÁü•ÊúâÈôê„ÄÇËøôÁßçÈùôÊÄÅÁâπÊÄßÈòªÁ¢ç‰∫ÜÂÆÉ‰ª¨Êèê‰æõÂáÜÁ°Æ„ÄÅÊúÄÊñ∞ÁöÑÂìçÂ∫îÁöÑËÉΩÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Âä®ÊÄÅÊàñÂø´ÈÄüÂèòÂåñÁöÑ‰∏ä‰∏ãÊñá‰∏≠„ÄÇÈõÜÊàêÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (Multimodal RAG) Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÊúâÂ∏åÊúõÁöÑËß£ÂÜ≥ÊñπÊ°àÔºå‰ΩÜËØ•Á≥ªÁªü‰∏çÂèØÈÅøÂÖçÂú∞‰ºöÈÅáÂà∞Â§öÁ≤íÂ∫¶Âô™Â£∞ÂØπÂ∫î (MNC) ÈóÆÈ¢òÔºåÂÖ∂‰∏≠Ê∂âÂèä‰∏§ÁßçÁ±ªÂûãÁöÑÂô™Â£∞ÔºöÁ≤óÁ≤íÂ∫¶ÔºàÊü•ËØ¢Ê†áÈ¢òÔºâÂíåÁªÜÁ≤íÂ∫¶ÔºàÊü•ËØ¢ÂõæÂÉèÔºâ„ÄÇËøôÁßçÂô™Â£∞ÈòªÁ¢ç‰∫ÜÂáÜÁ°ÆÁöÑÊ£ÄÁ¥¢ÂíåÁîüÊàê„ÄÇÂú®ËøôÈ°πÂ∑•‰Ωú‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü RagLLaVAÔºåËøôÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂÖ∑ÊúâÁü•ËØÜÂ¢ûÂº∫ÈáçÊñ∞ÊéíÂ∫èÂíåÂô™Â£∞Ê≥®ÂÖ•ËÆ≠ÁªÉÔºå‰ª•Ëß£ÂÜ≥Ëøô‰∫õÈôêÂà∂„ÄÇÊàë‰ª¨‰ΩøÁî®‰∏Ä‰∏™ÁÆÄÂçï‰ΩÜÊúâÊïàÁöÑÊåá‰ª§Ê®°ÊùøÂØπ MLLM ËøõË°åÊåá‰ª§ÂæÆË∞ÉÔºå‰ª•ËØ±ÂØºÂÖ∂ÊéíÂ∫èËÉΩÂäõÔºåÂπ∂Â∞ÜÂÖ∂Áî®‰ΩúÈáçÊñ∞ÊéíÂ∫èÂô®Êù•Á≤æÁ°ÆËøáÊª§Ââç k ‰∏™Ê£ÄÁ¥¢Âà∞ÁöÑÂõæÂÉè„ÄÇÂØπ‰∫éÁîüÊàêÔºåÊàë‰ª¨Âú®Êï∞ÊçÆÂíåÊ†áËÆ∞Á∫ßÂà´Âú®ËÆ≠ÁªÉÊúüÈó¥Ê≥®ÂÖ•ËßÜËßâÂô™Â£∞Ôºå‰ª•Â¢ûÂº∫ÁîüÊàêÂô®ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂú®‰∏§‰∏™Êï∞ÊçÆÈõÜÁöÑÂ≠êÈõÜ‰∏äËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÔºåËøô‰∫õÊï∞ÊçÆÈõÜÈúÄË¶ÅÊ£ÄÁ¥¢ÂíåÊé®ÁêÜÂõæÂÉèÊâçËÉΩÂõûÁ≠îÁªôÂÆöÁöÑÊü•ËØ¢„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúËØÅÊòé‰∫Ü RagLLaVA Âú®ÂáÜÁ°ÆÊ£ÄÁ¥¢ÂíåÁ®≥ÂÅ•ÁîüÊàêÊñπÈù¢ÁöÑ‰ºòË∂äÊÄß„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂèØÂú® https://github.com/IDEA-FinAI/RagLLaVA Ëé∑Âæó„ÄÇ

##### **Deformable 3D Shape Diffusion Model**
2407.21428v1 by Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu

The Gaussian diffusion model, initially designed for image generation, has
recently been adapted for 3D point cloud generation. However, these adaptations
have not fully considered the intrinsic geometric characteristics of 3D shapes,
thereby constraining the diffusion model's potential for 3D shape manipulation.
To address this limitation, we introduce a novel deformable 3D shape diffusion
model that facilitates comprehensive 3D shape manipulation, including point
cloud generation, mesh deformation, and facial animation. Our approach
innovatively incorporates a differential deformation kernel, which deconstructs
the generation of geometric structures into successive non-rigid deformation
stages. By leveraging a probabilistic diffusion model to simulate this
step-by-step process, our method provides a versatile and efficient solution
for a wide range of applications, spanning from graphics rendering to facial
expression animation. Empirical evidence highlights the effectiveness of our
approach, demonstrating state-of-the-art performance in point cloud generation
and competitive results in mesh deformation. Additionally, extensive visual
demonstrations reveal the significant potential of our approach for practical
applications. Our method presents a unique pathway for advancing 3D shape
manipulation and unlocking new opportunities in the realm of virtual reality.

ÊëòË¶ÅÔºöÈ´òÊñØÊì¥Êï£Ê®°ÂûãÊúÄÂàùÊòØÁÇ∫ÂΩ±ÂÉèÁîüÊàêËÄåË®≠Ë®àÔºåÊúÄËøëÂ∑≤ÊîπÁî®Êñº 3D ÈªûÈõ≤ÁîüÊàê„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊîπÁ∑®‰∏¶Êú™ÂÖÖÂàÜËÄÉÊÖÆ 3D ÂΩ¢ÁãÄÁöÑÂÖßÂú®Âπæ‰ΩïÁâπÂæµÔºåÂæûËÄåÈôêÂà∂‰∫ÜÊì¥Êï£Ê®°ÂûãÂú® 3D ÂΩ¢ÁãÄÊìç‰ΩúÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÈôêÂà∂ÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂèØËÆäÂΩ¢ 3D ÂΩ¢ÁãÄÊì¥Êï£Ê®°ÂûãÔºåË©≤Ê®°ÂûãÊúâÂä©ÊñºÈÄ≤Ë°åÂÖ®Èù¢ÁöÑ 3D ÂΩ¢ÁãÄÊìç‰ΩúÔºåÂåÖÊã¨ÈªûÈõ≤ÁîüÊàê„ÄÅÁ∂≤Ê†ºËÆäÂΩ¢ÂíåÈù¢ÈÉ®ÂãïÁï´„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂâµÊñ∞Âú∞Á¥çÂÖ•‰∫ÜÂæÆÂàÜËÆäÂΩ¢Ê†∏ÔºåÂ∞áÂπæ‰ΩïÁµêÊßãÁöÑÁîüÊàêËß£ÊßãÁÇ∫ÈÄ£Á∫åÁöÑÈùûÂâõÊÄßËÆäÂΩ¢ÈöéÊÆµ„ÄÇÈÄèÈÅéÂà©Áî®Ê©üÁéáÊì¥Êï£Ê®°Âûã‰æÜÊ®°Êì¨Ê≠§ÈÄêÊ≠•Á®ãÂ∫èÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂ§öÂäüËÉΩ‰∏îÊúâÊïàÁéáÁöÑËß£Ê±∫ÊñπÊ°àÔºåÈÅ©Áî®ÊñºÂæûÂúñÂΩ¢Ê∏≤ÊüìÂà∞Èù¢ÈÉ®Ë°®ÊÉÖÂãïÁï´ÁöÑÂª£Ê≥õÊáâÁî®„ÄÇÁ∂ìÈ©óË≠âÊìöÁ™ÅÈ°Ø‰∫ÜÊàëÂÄëÂÅöÊ≥ïÁöÑÊúâÊïàÊÄßÔºåË≠âÊòé‰∫ÜÂú®ÈªûÈõ≤ÁîüÊàê‰∏≠ÈÅîÂà∞ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰ª•ÂèäÂú®Á∂≤Ê†ºËÆäÂΩ¢‰∏≠Áç≤ÂæóÁ´∂Áà≠ÂäõÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåÂª£Ê≥õÁöÑË¶ñË¶∫Á§∫ÁØÑÊè≠Á§∫‰∫ÜÊàëÂÄëÂÅöÊ≥ïÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÂÖ∑ÊúâÈ°ØËëóÁöÑÊΩõÂäõ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁÇ∫Êé®ÈÄ≤ 3D ÂΩ¢ÁãÄÊìç‰ΩúÂíåÂú®ËôõÊì¨ÂØ¶Â¢ÉÈ†òÂüüÈñãÂïüÊñ∞Ê©üÊúÉÊèê‰æõ‰∫ÜÁç®ÁâπÁöÑÈÄîÂæë„ÄÇ

##### **Cost-Effective Hallucination Detection for LLMs**
2407.21424v1 by Simon Valentin, Jinmiao Fu, Gianluca Detommaso, Shaoyuan Xu, Giovanni Zappella, Bryan Wang

Large language models (LLMs) can be prone to hallucinations - generating
unreliable outputs that are unfaithful to their inputs, external facts or
internally inconsistent. In this work, we address several challenges for
post-hoc hallucination detection in production settings. Our pipeline for
hallucination detection entails: first, producing a confidence score
representing the likelihood that a generated answer is a hallucination; second,
calibrating the score conditional on attributes of the inputs and candidate
response; finally, performing detection by thresholding the calibrated score.
We benchmark a variety of state-of-the-art scoring methods on different
datasets, encompassing question answering, fact checking, and summarization
tasks. We employ diverse LLMs to ensure a comprehensive assessment of
performance. We show that calibrating individual scoring methods is critical
for ensuring risk-aware downstream decision making. Based on findings that no
individual score performs best in all situations, we propose a multi-scoring
framework, which combines different scores and achieves top performance across
all datasets. We further introduce cost-effective multi-scoring, which can
match or even outperform more expensive detection methods, while significantly
reducing computational overhead.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂÆπÊòìÂá∫ÁèæÂπªË¶∫ - ÁîüÊàê‰∏çÂø†ÊñºÂÖ∂Ëº∏ÂÖ•„ÄÅÂ§ñÈÉ®‰∫ãÂØ¶ÊàñÂÖßÈÉ®‰∏ç‰∏ÄËá¥ÁöÑ‰∏çÂèØÈù†Ëº∏Âá∫„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÁîüÁî¢Áí∞Â¢É‰∏≠‰∫ãÂæåÂπªË¶∫Ê™¢Ê∏¨ÁöÑÂπæÂÄãÊåëÊà∞„ÄÇÊàëÂÄëÁöÑÂπªË¶∫Ê™¢Ê∏¨ÁÆ°ÈÅìÂåÖÊã¨ÔºöÈ¶ñÂÖàÔºåÁî¢Áîü‰∏ÄÂÄãÁΩÆ‰ø°Â∫¶ÂàÜÊï∏ÔºåË°®Á§∫ÁîüÊàêÁöÑÁ≠îÊ°àÊòØÂπªË¶∫ÁöÑÂèØËÉΩÊÄßÔºõÂÖ∂Ê¨°ÔºåÊ†πÊìöËº∏ÂÖ•ÂíåÂÄôÈÅ∏ÈüøÊáâÁöÑÂ±¨ÊÄßÊ†°Ê∫ñÂàÜÊï∏ÔºõÊúÄÂæåÔºåÈÄöÈÅéÂ∞çÊ†°Ê∫ñÂàÜÊï∏ÈÄ≤Ë°åÈñæÂÄºËôïÁêÜ‰æÜÂü∑Ë°åÊ™¢Ê∏¨„ÄÇÊàëÂÄëÂ∞ç‰∏çÂêåÁöÑË≥áÊñôÈõÜÈÄ≤Ë°å‰∫ÜÂêÑÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑË©ïÂàÜÊñπÊ≥ïÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÔºåÂåÖÊã¨ÂïèÁ≠î„ÄÅ‰∫ãÂØ¶Êü•Ê†∏ÂíåÊëòË¶Å‰ªªÂãô„ÄÇÊàëÂÄëÊé°Áî®‰∏çÂêåÁöÑ LLM ‰æÜÁ¢∫‰øùÂ∞çÊÄßËÉΩÈÄ≤Ë°åÂÖ®Èù¢Ë©ï‰º∞„ÄÇÊàëÂÄëË°®ÊòéÔºåÊ†°Ê∫ñÂÄãÂà•Ë©ïÂàÜÊñπÊ≥ïÂ∞çÊñºÁ¢∫‰øùÈ¢®Èö™ÊÑüÁü•ÁöÑ‰∏ãÊ∏∏Ê±∫Á≠ñÂà∂ÂÆöËá≥ÈóúÈáçË¶Å„ÄÇÂü∫ÊñºÊ≤íÊúâ‰ªª‰ΩïÂÄãÂà•ÂàÜÊï∏Âú®ÊâÄÊúâÊÉÖÊ≥Å‰∏ãË°®ÁèæÊúÄ‰Ω≥ÁöÑÁôºÁèæÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ§öË©ïÂàÜÊ°ÜÊû∂ÔºåÂÆÉÁµêÂêà‰∫Ü‰∏çÂêåÁöÑÂàÜÊï∏Ôºå‰∏¶Âú®ÊâÄÊúâË≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÂºïÂÖ•‰∫ÜÂÖ∑ÊúâÊàêÊú¨ÊïàÁõäÁöÑÂ§öË©ïÂàÜÔºåÂÆÉÂèØ‰ª•ÂåπÈÖçÁîöËá≥ÂÑ™ÊñºÊõ¥ÊòÇË≤¥ÁöÑÊ™¢Ê∏¨ÊñπÊ≥ïÔºåÂêåÊôÇÈ°ØËëóÈôç‰ΩéË®àÁÆóÈñãÈä∑„ÄÇ

##### **Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models**
2407.21417v1 by Zhengxuan Wu, Yuhao Zhang, Peng Qi, Yumo Xu, Rujun Han, Yian Zhang, Jifan Chen, Bonan Min, Zhiheng Huang

Modern language models (LMs) need to follow human instructions while being
faithful; yet, they often fail to achieve both. Here, we provide concrete
evidence of a trade-off between instruction following (i.e., follow open-ended
instructions) and faithfulness (i.e., ground responses in given context) when
training LMs with these objectives. For instance, fine-tuning LLaMA-7B on
instruction following datasets renders it less faithful. Conversely,
instruction-tuned Vicuna-7B shows degraded performance at following
instructions when further optimized on tasks that require contextual grounding.
One common remedy is multi-task learning (MTL) with data mixing, yet it remains
far from achieving a synergic outcome. We propose a simple yet effective method
that relies on Rejection Sampling for Continued Self-instruction Tuning
(ReSet), which significantly outperforms vanilla MTL. Surprisingly, we find
that less is more, as training ReSet with high-quality, yet substantially
smaller data (three-fold less) yields superior results. Our findings offer a
better understanding of objective discrepancies in alignment training of LMs.

ÊëòË¶ÅÔºöÁèæ‰ª£Ë™ûË®ÄÊ®°Âûã (LM) Âú®Âø†ÂØ¶ÁöÑÂêåÊôÇÈúÄË¶ÅÈÅµÂæ™‰∫∫È°ûÁöÑÊåáÁ§∫ÔºõÁÑ∂ËÄåÔºåÂÆÉÂÄëÂ∏∏Â∏∏ÁÑ°Ê≥ïÂêåÊôÇÂØ¶ÁèæÈÄôÂÖ©Èªû„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂÖ∑È´îË≠âÊìöÔºåË™™ÊòéÂú®‰ΩøÁî®ÈÄô‰∫õÁõÆÊ®ôË®ìÁ∑¥ LM ÊôÇÔºåÂú®ÈÅµÂæ™ÊåáÁ§∫ÔºàÂç≥ÈÅµÂæ™ÈñãÊîæÂºèÊåáÁ§∫ÔºâÂíåÂø†ÂØ¶Â∫¶ÔºàÂç≥Âú®Áµ¶ÂÆöÁöÑËÉåÊôØ‰∏≠Âª∫Á´ãÂõûÊáâÔºâ‰πãÈñìÂ≠òÂú®Ê¨äË°°„ÄÇ‰æãÂ¶ÇÔºåÂú®ÈÅµÂæ™ÊåáÁ§∫ÁöÑÊï∏ÊìöÈõÜ‰∏äÂæÆË™ø LLaMA-7B ÊúÉÈôç‰ΩéÂÖ∂Âø†ÂØ¶Â∫¶„ÄÇÁõ∏ÂèçÔºåÂú®ÈÄ≤‰∏ÄÊ≠•ÈáùÂ∞çÈúÄË¶Å‰∏ä‰∏ãÊñá‰æùÊìöÁöÑ‰ªªÂãôÈÄ≤Ë°åÂÑ™ÂåñÊôÇÔºåÁ∂ìÈÅéÊåáÁ§∫Ë™øÊï¥ÁöÑ Vicuna-7B Âú®ÈÅµÂæ™ÊåáÁ§∫ÊñπÈù¢ÁöÑË°®ÁèæÊúÉ‰∏ãÈôç„ÄÇ‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑË£úÊïëÊé™ÊñΩÊòØ‰ΩøÁî®Êï∏ÊìöÊ∑∑ÂêàÁöÑÂ§ö‰ªªÂãôÂ≠∏Áøí (MTL)Ôºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÈÅ†Êú™ÈÅîÂà∞ÂçîÂêåÊïàÊáâ„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂÆÉ‰æùË≥¥ÊñºÊãíÁµïÊé°Ê®£ÔºåÁî®ÊñºÊåÅÁ∫åËá™ÊåáÂ∞éË™øÊï¥ (ReSet)ÔºåÂÆÉÊòéÈ°ØÂÑ™ÊñºÈ¶ôËçâ MTL„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåÊàëÂÄëÁôºÁèæÂ∞ëÂç≥ÊòØÂ§öÔºåÂõ†ÁÇ∫‰ΩøÁî®È´òÂìÅË≥™‰ΩÜÊï∏ÈáèÈ°ØËëóËºÉÂ∞ëÔºàÂ∞ë‰∏âÂÄçÔºâÁöÑÊï∏ÊìöË®ìÁ∑¥ ReSet ÊúÉÁî¢ÁîüÊõ¥Â•ΩÁöÑÁµêÊûú„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊúâÂä©ÊñºÊõ¥Â•ΩÂú∞ÁêÜËß£ LM Â∞çÈΩäË®ìÁ∑¥‰∏≠ÁöÑÁõÆÊ®ôÂ∑ÆÁï∞„ÄÇ

##### **Towards interfacing large language models with ASR systems using confidence measures and prompting**
2407.21414v1 by Maryam Naderi, Enno Hermann, Alexandre Nanchen, Sevada Hovsepyan, Mathew Magimai. -Doss

As large language models (LLMs) grow in parameter size and capabilities, such
as interaction through prompting, they open up new ways of interfacing with
automatic speech recognition (ASR) systems beyond rescoring n-best lists. This
work investigates post-hoc correction of ASR transcripts with LLMs. To avoid
introducing errors into likely accurate transcripts, we propose a range of
confidence-based filtering methods. Our results indicate that this can improve
the performance of less competitive ASR systems.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂèÉÊï∏Ë¶èÊ®°ÂíåÂäüËÉΩ‰∏ä‰∏çÊñ∑Â¢ûÈï∑Ôºå‰æãÂ¶ÇÈÄèÈÅéÊèêÁ§∫ÈÄ≤Ë°å‰∫íÂãïÔºåÂÆÉÂÄëÈñãÂïü‰∫ÜËàáËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) Á≥ªÁµ±‰∫íÂãïÁöÑÊñ∞ÊñπÂºèÔºåËÄå‰∏çÂÜçÂÉÖÈôêÊñºÈáçÊñ∞Ë©ïÂàÜ n ÂÄãÊúÄ‰Ω≥Ê∏ÖÂñÆ„ÄÇÊú¨Á†îÁ©∂Êé¢Ë®é‰∫Ü‰ΩøÁî® LLM Â∞ç ASR ËΩâÈåÑÈÄ≤Ë°å‰∫ãÂæåÊ†°Ê≠£„ÄÇÁÇ∫‰∫ÜÈÅøÂÖçÂú®ÂèØËÉΩÊ∫ñÁ¢∫ÁöÑËΩâÈåÑ‰∏≠ÂºïÂÖ•ÈåØË™§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÂü∫Êñº‰ø°ÂøÉÁöÑÈÅéÊøæÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÈÄôÂèØ‰ª•ÊèêÂçáÁ´∂Áà≠ÂäõËºÉ‰ΩéÁöÑ ASR Á≥ªÁµ±ÁöÑÊïàËÉΩ„ÄÇ

##### **GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction**
2407.21384v1 by Yanxu Mao, Peipei Liu, Tiehan Cui

Document-level relation extraction (DocRE) aims to extract relations between
entities from unstructured document text. Compared to sentence-level relation
extraction, it requires more complex semantic understanding from a broader text
context. Currently, some studies are utilizing logical rules within evidence
sentences to enhance the performance of DocRE. However, in the data without
provided evidence sentences, researchers often obtain a list of evidence
sentences for the entire document through evidence retrieval (ER). Therefore,
DocRE suffers from two challenges: firstly, the relevance between evidence and
entity pairs is weak; secondly, there is insufficient extraction of complex
cross-relations between long-distance multi-entities. To overcome these
challenges, we propose GEGA, a novel model for DocRE. The model leverages graph
neural networks to construct multiple weight matrices, guiding attention
allocation to evidence sentences. It also employs multi-scale representation
aggregation to enhance ER. Subsequently, we integrate the most efficient
evidence information to implement both fully supervised and weakly supervised
training processes for the model. We evaluate the GEGA model on three widely
used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The
experimental results indicate that our model has achieved comprehensive
improvements compared to the existing SOTA model.

ÊëòË¶ÅÔºö<paragraph>Êñá‰ª∂Á¥öÈóú‰øÇËêÉÂèñ (DocRE) Êó®Âú®ÂæûÈùûÁµêÊßãÊñá‰ª∂ÊñáÂ≠ó‰∏≠ËêÉÂèñÂØ¶È´îÈñìÁöÑÈóú‰øÇ„ÄÇËàáÂè•Â≠êÁ¥öÈóú‰øÇËêÉÂèñÁõ∏ÊØîÔºåÂÆÉÈúÄË¶ÅÂæûÊõ¥Âª£Ê≥õÁöÑÊñáÂ≠óËÑàÁµ°‰∏≠ÈÄ≤Ë°åÊõ¥Ë§áÈõúÁöÑË™ûÁæ©ÁêÜËß£„ÄÇÁõÆÂâçÔºå‰∏Ä‰∫õÁ†îÁ©∂Âà©Áî®Ë≠âÊìöÂè•Â≠ê‰∏≠ÁöÑÈÇèËºØË¶èÂâá‰æÜÂ¢ûÂº∑ DocRE ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂú®Ê≤íÊúâÊèê‰æõË≠âÊìöÂè•Â≠êÁöÑË≥áÊñô‰∏≠ÔºåÁ†îÁ©∂‰∫∫Âì°ÈÄöÂ∏∏ÊúÉÈÄèÈÅéË≠âÊìöÊì∑Âèñ (ER) ÁÇ∫Êï¥ÂÄãÊñá‰ª∂ÂèñÂæó‰∏Ä‰ªΩË≠âÊìöÂè•Â≠êÊ∏ÖÂñÆ„ÄÇÂõ†Ê≠§ÔºåDocRE Èù¢Ëá®ÂÖ©È†ÖÊåëÊà∞ÔºöÈ¶ñÂÖàÔºåË≠âÊìöÂíåÂØ¶È´îÂ∞ç‰πãÈñìÁöÑÈóúËÅØÊÄßËºÉÂº±ÔºõÂÖ∂Ê¨°ÔºåÁÑ°Ê≥ïÂÖÖÂàÜËêÉÂèñÈÅ†Ë∑ùÈõ¢Â§öÂØ¶È´î‰πãÈñìÁöÑË§áÈõú‰∫§ÂèâÈóú‰øÇ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫ GEGAÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®Êñº DocRE ÁöÑÊñ∞ÂûãÊ®°Âûã„ÄÇË©≤Ê®°ÂûãÂà©Áî®ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰æÜÂª∫ÊßãÂ§öÂÄãÊ¨äÈáçÁü©Èô£ÔºåÂºïÂ∞éÊ≥®ÊÑèÂäõÂàÜÈÖçÂà∞Ë≠âÊìöÂè•Â≠ê„ÄÇÂÆÉÈÇÑÊé°Áî®Â§öÂ∞∫Â∫¶Ë°®Á§∫ËÅöÂêà‰æÜÂ¢ûÂº∑ ER„ÄÇÈö®ÂæåÔºåÊàëÂÄëÊï¥ÂêàÊúÄÊúâÊïàÁöÑË≠âÊìöË≥áË®äÔºåÁÇ∫Ê®°ÂûãÂØ¶‰ΩúÂÆåÂÖ®Áõ£Áù£ÂºèÂíåÂº±Áõ£Áù£ÂºèË®ìÁ∑¥ÊµÅÁ®ã„ÄÇÊàëÂÄëÂú®‰∏âÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜÔºöDocRED„ÄÅRe-DocRED Âíå Revisit-DocRED ‰∏äË©ï‰º∞ GEGA Ê®°Âûã„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàáÁèæÊúâÁöÑ SOTA Ê®°ÂûãÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂ∑≤Áç≤ÂæóÂÖ®Èù¢ÁöÑÊîπÈÄ≤„ÄÇ</paragraph>

##### **An Extended Kalman Filter Integrated Latent Feature Model on Dynamic Weighted Directed Graphs**
2407.21376v1 by Hongxun Zhou, Xiangyu Chen, Ye Yuan

A dynamic weighted directed graph (DWDG) is commonly encountered in various
application scenarios. It involves extensive dynamic interactions among
numerous nodes. Most existing approaches explore the intricate temporal
patterns hidden in a DWDG from the purely data-driven perspective, which
suffers from accuracy loss when a DWDG exhibits strong fluctuations over time.
To address this issue, this study proposes a novel
Extended-Kalman-Filter-Incorporated Latent Feature (EKLF) model to represent a
DWDG from the model-driven perspective. Its main idea is divided into the
following two-fold ideas: a) adopting a control model, i.e., the Extended
Kalman Filter (EKF), to track the complex temporal patterns precisely with its
nonlinear state-transition and observation functions; and b) introducing an
alternating least squares (ALS) algorithm to train the latent features (LFs)
alternatively for precisely representing a DWDG. Empirical studies on DWDG
datasets demonstrate that the proposed EKLF model outperforms state-of-the-art
models in prediction accuracy and computational efficiency for missing edge
weights of a DWDG. It unveils the potential for precisely representing a DWDG
by incorporating a control model.

ÊëòË¶ÅÔºöÂãïÊÖãÂä†Ê¨äÊúâÂêëÂúñ (DWDG) Â∏∏Ë¶ãÊñºÂêÑÁ®ÆÊáâÁî®ÊÉÖÂ¢É‰∏≠„ÄÇÂÆÉÊ∂âÂèäÁúæÂ§öÁØÄÈªû‰πãÈñìÂª£Ê≥õÁöÑÂãïÊÖã‰∫§‰∫í„ÄÇÁèæÊúâÊñπÊ≥ïÂ§ßÂ§öÂæûÁ¥îÁ≤πÁöÑÊï∏ÊìöÈ©ÖÂãïËßÄÈªûÊé¢Á¥¢Èö±ËóèÂú® DWDG ‰∏≠ÁöÑË§áÈõúÊôÇÈñìÊ®°ÂºèÔºåÁï∂ DWDG Èö®ÊôÇÈñìÊé®ÁßªÂá∫ÁèæÂäáÁÉàÊ≥¢ÂãïÊôÇÔºåÈÄôÁ®ÆÊñπÊ≥ïÊúÉÂ∞éËá¥Ê∫ñÁ¢∫Â∫¶‰∏ãÈôç„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÊì¥Â±ïÂç°ÁàæÊõºÊøæÊ≥¢Âô®ÁµêÂêàÊΩõÂú®ÁâπÂæµ (EKLF) Ê®°ÂûãÔºåÂæûÊ®°ÂûãÈ©ÖÂãïÁöÑËßíÂ∫¶Ë°®Á§∫ DWDG„ÄÇÂÖ∂‰∏ªË¶ÅÊÄùÊÉ≥ÂèØÂàÜÁÇ∫‰ª•‰∏ãÂÖ©ÂÄãÊñπÈù¢Ôºöa) Êé°Áî®ÊéßÂà∂Ê®°ÂûãÔºåÂç≥Êì¥Â±ïÂç°ÁàæÊõºÊøæÊ≥¢Âô® (EKF)Ôºå‰ª•ÂÖ∂ÈùûÁ∑öÊÄßÁãÄÊÖãËΩâÁßªÂíåËßÄÊ∏¨ÂáΩÊï∏Á≤æÁ¢∫ËøΩËπ§Ë§áÈõúÁöÑÊôÇÈñìÊ®°ÂºèÔºõb) Â∞éÂÖ•‰∫§ÊõøÊúÄÂ∞è‰∫å‰πò (ALS) ÊºîÁÆóÊ≥ïÔºå‰∫§ÊõøË®ìÁ∑¥ÊΩõÂú®ÁâπÂæµ (LF)Ôºå‰ª•Á≤æÁ¢∫Ë°®Á§∫ DWDG„ÄÇÈáùÂ∞ç DWDG Ë≥áÊñôÈõÜÁöÑÂØ¶Ë≠âÁ†îÁ©∂Ë°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑ EKLF Ê®°ÂûãÂú®È†êÊ∏¨Ê∫ñÁ¢∫Â∫¶ÂíåË®àÁÆóÊïàÁéáÊñπÈù¢ÂÑ™ÊñºÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÔºåÁî®ÊñºÈ†êÊ∏¨ DWDG ÁöÑÁº∫Â§±ÈÇäÊ¨äÈáç„ÄÇÂÆÉÊè≠Á§∫‰∫ÜÈÄèÈÅéÁµêÂêàÊéßÂà∂Ê®°ÂûãÁ≤æÁ¢∫Ë°®Á§∫ DWDG ÁöÑÊΩõÂäõ„ÄÇ

##### **Prompting Medical Large Vision-Language Models to Diagnose Pathologies by Visual Question Answering**
2407.21368v1 by Danfeng Guo, Demetri Terzopoulos

Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.

ÊëòË¶ÅÔºöÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (LVLMs) Âú®ËøëÂπ¥‰æÜÂèñÂæóÈ°ØËëóÁöÑÊàêÂäüÔºå‰∏¶Â∑≤Êì¥Â±ïÂà∞ÈÜ´ÁôÇÈ†òÂüü„ÄÇÂÑòÁÆ°Âú®ÈÜ´Â≠∏Ë¶ñË¶∫ÂïèÁ≠î (VQA) ‰ªªÂãô‰∏≠Ë°®Áèæ‰ª§‰∫∫ÊªøÊÑèÔºå‰ΩÜÈÜ´Â≠∏ LVLMs (MLVLMs) ‰ªçÂ≠òÂú®ÂπªË¶∫ÂïèÈ°åÔºåÂ∞éËá¥ÂÆÉÂÄëÁÑ°Ê≥ïË®∫Êñ∑Âá∫Ë§áÈõúÁöÑÁóÖÁêÜ„ÄÇÊ≠§Â§ñÔºåÁî±ÊñºË®ìÁ∑¥Ë≥áÊñô‰∏çÂπ≥Ë°°ÔºåÂÆÉÂÄëÂæàÂÆπÊòìÁÑ°Ê≥ïÂ≠∏ÁøíÂ∞ëÊï∏ÁóÖÁêÜ„ÄÇÊàëÂÄëÊèêÂá∫ÂÖ©Á®ÆÈáùÂ∞ç MLVLMs ÁöÑÊèêÁ§∫Á≠ñÁï•Ôºå‰ª•Ê∏õÂ∞ëÂπªË¶∫‰∏¶ÊîπÂñÑ VQA ÊïàËÉΩ„ÄÇÂú®Á¨¨‰∏ÄÂÄãÁ≠ñÁï•‰∏≠ÔºåÊàëÂÄëÊèê‰æõÊü•Ë©¢ÁóÖÁêÜÁöÑË©≥Á¥∞Ë™™Êòé„ÄÇÂú®Á¨¨‰∫åÂÄãÁ≠ñÁï•‰∏≠ÔºåÊàëÂÄëÂæÆË™ø‰∏ÄÂÄã‰æøÂÆú„ÄÅÊïàËÉΩ‰∏ç‰Ω≥ÁöÑÂ≠∏ÁøíÂô®Ôºå‰ª•Âú®ÁâπÂÆöÊåáÊ®ô‰∏äÁç≤ÂæóÈ´òÊïàËÉΩÔºå‰∏¶‰ª•ÊñáÂ≠óÊñπÂºèÂêë MLVLM Êèê‰æõÂÖ∂Âà§Êñ∑„ÄÇÂú® MIMIC-CXR-JPG Âíå Chexpert Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÊ∏¨Ë©¶ÂæåÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈ°ØËëóÊîπÂñÑ‰∫ÜË®∫Êñ∑ F1 ÂàÜÊï∏ÔºåÊúÄÈ´òÊèêÂçáÂπÖÂ∫¶ÁÇ∫ 0.27„ÄÇÊàëÂÄëÈÇÑÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊèêÁ§∫Á≠ñÁï•ÂèØ‰ª•Êì¥Â±ïÂà∞‰∏ÄËà¨ÁöÑ LVLM È†òÂüü„ÄÇÊ†πÊìö POPE ÊåáÊ®ôÔºåÂÆÉÊúâÊïàÂú∞ÊäëÂà∂‰∫ÜÁèæÊúâ LVLMs ÁöÑÂÅáÈô∞ÊÄßÈ†êÊ∏¨Ôºå‰∏¶Â∞áÂè¨ÂõûÁéáÊèêÈ´ò‰∫ÜÁ¥Ñ 0.07„ÄÇ

##### **ProSpec RL: Plan Ahead, then Execute**
2407.21359v1 by Liangliang Liu, Yi Guan, BoRan Wang, Rujia Shen, Yi Lin, Chaoran Kong, Lian Yan, Jingchi Jiang

Imagining potential outcomes of actions before execution helps agents make
more informed decisions, a prospective thinking ability fundamental to human
cognition. However, mainstream model-free Reinforcement Learning (RL) methods
lack the ability to proactively envision future scenarios, plan, and guide
strategies. These methods typically rely on trial and error to adjust policy
functions, aiming to maximize cumulative rewards or long-term value, even if
such high-reward decisions place the environment in extremely dangerous states.
To address this, we propose the Prospective (ProSpec) RL method, which makes
higher-value, lower-risk optimal decisions by imagining future n-stream
trajectories. Specifically, ProSpec employs a dynamic model to predict future
states (termed "imagined states") based on the current state and a series of
sampled actions. Furthermore, we integrate the concept of Model Predictive
Control and introduce a cycle consistency constraint that allows the agent to
evaluate and select the optimal actions from these trajectories. Moreover,
ProSpec employs cycle consistency to mitigate two fundamental issues in RL:
augmenting state reversibility to avoid irreversible events (low risk) and
augmenting actions to generate numerous virtual trajectories, thereby improving
data efficiency. We validated the effectiveness of our method on the DMControl
benchmarks, where our approach achieved significant performance improvements.
Code will be open-sourced upon acceptance.

ÊëòË¶ÅÔºöÂú®Âü∑Ë°åÂãï‰ΩúÂâçÊÉ≥ÂÉèÂÖ∂ÊΩõÂú®ÁµêÊûúÔºåÊúâÂä©Êñº‰ª£ÁêÜ‰∫∫ÂÅöÂá∫Êõ¥ÊòéÊô∫ÁöÑÊ±∫Á≠ñÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂ∞ç‰∫∫È°ûË™çÁü•Ëá≥ÈóúÈáçË¶ÅÁöÑÂâçÁûªÊÄßÊÄùËÄÉËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºå‰∏ªÊµÅÁöÑÁÑ°Ê®°ÂûãÂº∑ÂåñÂ≠∏Áøí (RL) ÊñπÊ≥ïÁº∫‰πè‰∏ªÂãïË®≠ÊÉ≥Êú™‰æÜÂ†¥ÊôØ„ÄÅË¶èÂäÉÂíåÊåáÂ∞éÁ≠ñÁï•ÁöÑËÉΩÂäõ„ÄÇÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏‰æùË≥¥ÊñºË©¶ÈåØ‰æÜË™øÊï¥Á≠ñÁï•ÂáΩÊï∏ÔºåÊó®Âú®ÊúÄÂ§ßÂåñÁ¥ØÁ©çÁçéÂãµÊàñÈï∑ÊúüÂÉπÂÄºÔºåÂç≥‰ΩøÈÄôÊ®£ÁöÑÁç≤ÂèñÈ´òÁçéÂãµÊ±∫Á≠ñÊúÉËÆìÁí∞Â¢ÉËôïÊñºÊ•µÂ∫¶Âç±Èö™ÁöÑÁãÄÊÖã„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂâçÁûªÊÄß (ProSpec) RL ÊñπÊ≥ïÔºåÂÆÉÈÄöÈÅéÊÉ≥ÂÉèÊú™‰æÜÁöÑ n ‰∏≤ÊµÅËªåË∑°‰æÜÂÅöÂá∫ÂÉπÂÄºÊõ¥È´ò„ÄÅÈ¢®Èö™Êõ¥‰ΩéÁöÑÊúÄ‰Ω≥Ê±∫Á≠ñ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåProSpec ‰ΩøÁî®ÂãïÊÖãÊ®°ÂûãÊ†πÊìöÁï∂ÂâçÁãÄÊÖãÂíå‰∏ÄÁ≥ªÂàóÊé°Ê®£Âãï‰Ωú‰æÜÈ†êÊ∏¨Êú™‰æÜÁöÑÁãÄÊÖãÔºàÁ®±ÁÇ∫„ÄåÊÉ≥ÂÉèÁãÄÊÖã„ÄçÔºâ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊï¥Âêà‰∫ÜÊ®°ÂûãÈ†êÊ∏¨ÊéßÂà∂ÁöÑÊ¶ÇÂøµÔºå‰∏¶ÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÈÄ±Êúü‰∏ÄËá¥ÊÄßÁ¥ÑÊùüÔºåÂÖÅË®±‰ª£ÁêÜ‰∫∫ÂæûÈÄô‰∫õËªåË∑°‰∏≠Ë©ï‰º∞ÂíåÈÅ∏ÊìáÊúÄ‰Ω≥Âãï‰Ωú„ÄÇÊ≠§Â§ñÔºåProSpec ‰ΩøÁî®ÈÄ±Êúü‰∏ÄËá¥ÊÄß‰æÜÁ∑©Ëß£ RL ‰∏≠ÁöÑÂÖ©ÂÄãÂü∫Êú¨ÂïèÈ°åÔºöÂ¢ûÂä†ÁãÄÊÖãÂèØÈÄÜÊÄß‰ª•ÈÅøÂÖç‰∏çÂèØÈÄÜ‰∫ã‰ª∂Ôºà‰ΩéÈ¢®Èö™ÔºâÂíåÂ¢ûÂä†Âãï‰Ωú‰ª•Áî¢ÁîüÂ§ßÈáèÁöÑËôõÊì¨ËªåË∑°ÔºåÂæûËÄåÊèêÈ´òË≥áÊñôÊïàÁéá„ÄÇÊàëÂÄëÂú® DMControl Âü∫Ê∫ñ‰∏äÈ©óË≠â‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊïàËÉΩÊèêÂçá„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂú®Ë¢´Êé•ÂèóÂæåÈñãÊ∫ê„ÄÇ

##### **Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs**
2407.21358v1 by Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan

Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing
reliable, structured, domain-specific, and up-to-date external knowledge.
However, KGs and LLMs are often developed separately and must be integrated
after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning
algorithm that enables augmentation of black-box LLMs with one or more KGs. The
algorithm equips a LLM with actions for interfacing a KG and enables the LLM to
perform tree search over possible thoughts and actions to find high confidence
reasoning paths. We evaluate on two popular benchmark datasets. Our results
show that Tree-of-Traversals significantly improves performance on question
answering and KG question answering tasks. Code is available at
\url{https://github.com/amazon-science/tree-of-traversals}

ÊëòË¶ÅÔºöÁü•Ë≠òÂúñË≠ú (KG) ÈÄèÈÅéÊèê‰æõÂèØÈù†„ÄÅÁµêÊßãÂåñ„ÄÅÁâπÂÆöÊñºÈ†òÂüü‰∏îÊúÄÊñ∞ÁöÑÂ§ñÈÉ®Áü•Ë≠òÔºå‰æÜË£úÂÖÖÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)„ÄÇ
ÁÑ∂ËÄåÔºåKG Âíå LLM ÈÄöÂ∏∏ÊòØÂàÜÈñãÈñãÁôºÔºå‰∏¶‰∏îÂøÖÈ†àÂú®Ë®ìÁ∑¥ÂæåÊï¥Âêà„ÄÇÊàëÂÄë‰ªãÁ¥π‰∫Ü Tree-of-TraversalsÔºå‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÈõ∂Ê¨°Êé®ÁêÜÊºîÁÆóÊ≥ïÔºåÂÆÉËÉΩËÆìÈªëÁõí LLM ‰ΩøÁî®‰∏ÄÂÄãÊàñÂ§öÂÄã KG„ÄÇË©≤ÊºîÁÆóÊ≥ïÁÇ∫ LLM Êèê‰æõËàá KG ‰ªãÈù¢ÁöÑÂãï‰ΩúÔºå‰∏¶ËÆì LLM ËÉΩÂú®ÂèØËÉΩÁöÑÊÄùËÄÉÂíåÂãï‰Ωú‰∏äÂü∑Ë°åÊ®πÁãÄÊêúÂ∞ãÔºå‰ª•ÊâæÂá∫È´òÂ∫¶‰ø°ÂøÉÁöÑÊé®ÁêÜË∑ØÂæë„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãÁÜ±ÈñÄÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåTree-of-Traversals Â§ßÂπÖÊèêÂçá‰∫ÜÂïèÈ°åËß£Á≠îÂíå KG ÂïèÈ°åËß£Á≠î‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® \url{https://github.com/amazon-science/tree-of-traversals} ÂèñÂæó

##### **Small Object Few-shot Segmentation for Vision-based Industrial Inspection**
2407.21351v1 by Zilong Zhang, Chang Niu, Zhibin Zhao, Xingwu Zhang, Xuefeng Chen

Vision-based industrial inspection (VII) aims to locate defects quickly and
accurately. Supervised learning under a close-set setting and industrial
anomaly detection, as two common paradigms in VII, face different problems in
practical applications. The former is that various and sufficient defects are
difficult to obtain, while the latter is that specific defects cannot be
located. To solve these problems, in this paper, we focus on the few-shot
semantic segmentation (FSS) method, which can locate unseen defects conditioned
on a few annotations without retraining. Compared to common objects in natural
images, the defects in VII are small. This brings two problems to current FSS
methods: 1 distortion of target semantics and 2 many false positives for
backgrounds. To alleviate these problems, we propose a small object few-shot
segmentation (SOFS) model. The key idea for alleviating 1 is to avoid the
resizing of the original image and correctly indicate the intensity of target
semantics. SOFS achieves this idea via the non-resizing procedure and the
prototype intensity downsampling of support annotations. To alleviate 2, we
design an abnormal prior map in SOFS to guide the model to reduce false
positives and propose a mixed normal Dice loss to preferentially prevent the
model from predicting false positives. SOFS can achieve FSS and few-shot
anomaly detection determined by support masks. Diverse experiments substantiate
the superior performance of SOFS. Code is available at
https://github.com/zhangzilongc/SOFS.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºË¶ñË¶∫ÁöÑÂ∑•Ê•≠Ê™¢Ê∏¨ (VII) Êó®Âú®Âø´ÈÄü‰∏îÊ∫ñÁ¢∫Âú∞ÊâæÂá∫Áº∫Èô∑„ÄÇÊúâÁõ£Áù£Â≠∏ÁøíÂú®Â∞ÅÈñâÂºèË®≠ÂÆö‰∏ãÂíåÂ∑•Ê•≠Áï∞Â∏∏ÂÅµÊ∏¨Ôºå‰ΩúÁÇ∫ VII ‰∏≠ÂÖ©ÂÄãÂ∏∏Ë¶ãÁöÑÁØÑ‰æãÔºåÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠Èù¢Ëá®‰∏çÂêåÁöÑÂïèÈ°å„ÄÇÂâçËÄÖÊòØÈõ£‰ª•ÂèñÂæóÂêÑÁ®Æ‰∏îË∂≥Â§†ÁöÑÁº∫Èô∑ÔºåËÄåÂæåËÄÖÂâáÊòØÁÑ°Ê≥ïÊâæÂá∫ÁâπÂÆöÁöÑÁº∫Èô∑„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞àÊ≥®ÊñºÂ∞ëÊ®£Êú¨Ë™ûÊÑèÂàÜÂâ≤ (FSS) ÊñπÊ≥ïÔºåÂÆÉÂèØ‰ª•Âú®‰∏çÈáçÊñ∞Ë®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊ†πÊìöÂ∞ëÊï∏Ë®ªËß£ÊâæÂá∫Êú™Ë¶ãÈÅéÁöÑÁº∫Èô∑„ÄÇËàáËá™ÁÑ∂ÂΩ±ÂÉè‰∏≠ÁöÑÂ∏∏Ë¶ãÁâ©‰ª∂Áõ∏ÊØîÔºåVII ‰∏≠ÁöÑÁº∫Èô∑ÂæàÂ∞è„ÄÇÈÄôÁÇ∫ÁõÆÂâçÁöÑ FSS ÊñπÊ≥ïÂ∏∂‰æÜÂÖ©ÂÄãÂïèÈ°åÔºö1 ÁõÆÊ®ôË™ûÊÑèÁöÑÂ§±ÁúüÂíå 2 ËÉåÊôØÁöÑË®±Â§öÂÅáÈôΩÊÄß„ÄÇÁÇ∫‰∫ÜÊ∏õËºïÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ∞èÂûãÁâ©‰ª∂Â∞ëÊ®£Êú¨ÂàÜÂâ≤ (SOFS) Ê®°Âûã„ÄÇÊ∏õËºï 1 ÁöÑÈóúÈçµÊÉ≥Ê≥ïÊòØÈÅøÂÖçË™øÊï¥ÂéüÂßãÂΩ±ÂÉèÁöÑÂ§ßÂ∞èÔºå‰∏¶Ê≠£Á¢∫ÊåáÂá∫ÁõÆÊ®ôË™ûÊÑèÁöÑÂº∑Â∫¶„ÄÇSOFS ÈÄèÈÅéÈùûË™øÊï¥Â§ßÂ∞èÁöÑÁ®ãÂ∫èÂíåÊîØÊè¥Ë®ªËß£ÁöÑÂéüÂûãÂº∑Â∫¶‰∏ãÊé°Ê®£‰æÜÈÅîÊàêÈÄôÂÄãÊÉ≥Ê≥ï„ÄÇÁÇ∫‰∫ÜÊ∏õËºï 2ÔºåÊàëÂÄëÂú® SOFS ‰∏≠Ë®≠Ë®à‰∏ÄÂÄãÁï∞Â∏∏ÂÖàÈ©óÂú∞ÂúñÔºåÂºïÂ∞éÊ®°ÂûãÊ∏õÂ∞ëÂÅáÈôΩÊÄßÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÊ∑∑ÂêàÊ≠£Ë¶è Dice ÊêçÂ§±Ôºå‰ª•ÂÑ™ÂÖàÈò≤Ê≠¢Ê®°ÂûãÈ†êÊ∏¨ÂÅáÈôΩÊÄß„ÄÇSOFS ÂèØ‰ª•ÈÅîÊàêÁî±ÊîØÊè¥ÈÅÆÁΩ©Ê±∫ÂÆöÁöÑ FSS ÂíåÂ∞ëÊ®£Êú¨Áï∞Â∏∏ÂÅµÊ∏¨„ÄÇÂêÑÁ®ÆÂØ¶È©óË≠âÂØ¶‰∫Ü SOFS ÁöÑÂÑ™Áï∞ÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/zhangzilongc/SOFS ÂèñÂæó„ÄÇ</paragraph>

##### **Differentially Private Block-wise Gradient Shuffle for Deep Learning**
2407.21347v1 by David Zagardo

Traditional Differentially Private Stochastic Gradient Descent (DP-SGD)
introduces statistical noise on top of gradients drawn from a Gaussian
distribution to ensure privacy. This paper introduces the novel Differentially
Private Block-wise Gradient Shuffle (DP-BloGS) algorithm for deep learning.
BloGS builds off of existing private deep learning literature, but makes a
definitive shift by taking a probabilistic approach to gradient noise
introduction through shuffling modeled after information theoretic privacy
analyses. The theoretical results presented in this paper show that the
combination of shuffling, parameter-specific block size selection, batch layer
clipping, and gradient accumulation allows DP-BloGS to achieve training times
close to that of non-private training while maintaining similar privacy and
utility guarantees to DP-SGD. DP-BloGS is found to be significantly more
resistant to data extraction attempts than DP-SGD. The theoretical results are
validated by the experimental findings.

ÊëòË¶ÅÔºöÂÇ≥Áµ±Â∑ÆÂàÜÁßÅ‰∫∫Èö®Ê©üÊ¢ØÂ∫¶‰∏ãÈôçÔºàDP-SGDÔºâ
Âú®ÂæûÈ´òÊñØÂàÜ‰Ωà‰∏≠Áπ™Ë£ΩÁöÑÊ¢ØÂ∫¶‰∏äÂºïÂÖ•Áµ±Ë®àÂô™ËÅ≤‰ª•Á¢∫‰øùÈö±ÁßÅ„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫ÜÁî®ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂÖ®Êñ∞Â∑ÆÂàÜÁßÅ‰∫∫ÂçÄÂ°äÊ¢ØÂ∫¶Ê∑∑Ê¥óÔºàDP-BloGSÔºâÊºîÁÆóÊ≥ï„ÄÇ
BloGS Âª∫ÊßãÊñºÁèæÊúâÁöÑÁßÅ‰∫∫Ê∑±Â∫¶Â≠∏ÁøíÊñáÁçª‰πã‰∏äÔºå‰ΩÜÈÄèÈÅéÊé°Áî®Ê©üÁéáÊñπÊ≥ï‰æÜÂºïÂÖ•Ê¢ØÂ∫¶Âô™ËÅ≤Ôºå‰∏¶ÈÄèÈÅéÊ®°Êì¨Ë≥áË®äÁêÜË´ñÈö±ÁßÅÂàÜÊûê‰∏≠ÁöÑÊ¥óÁâåÈÄ≤Ë°åÂª∫Ê®°ÔºåÂæûËÄåÂÅöÂá∫ÊòéÁ¢∫ÁöÑËΩâËÆä„ÄÇÊú¨Êñá‰∏≠ÊèêÂá∫ÁöÑÁêÜË´ñÁµêÊûúË°®ÊòéÔºåÊ¥óÁâå„ÄÅÁâπÂÆöÊñºÂèÉÊï∏ÁöÑÂçÄÂ°äÂ§ßÂ∞èÈÅ∏Êìá„ÄÅÊâπÊ¨°Â±§Ë£ÅÂâ™ÂíåÊ¢ØÂ∫¶Á¥ØÁ©çÁöÑÁµÑÂêà‰Ωø DP-BloGS ËÉΩÂ§†ÂØ¶ÁèæÊé•ËøëÊñºÈùûÁßÅ‰∫∫Ë®ìÁ∑¥ÁöÑË®ìÁ∑¥ÊôÇÈñìÔºåÂêåÊôÇ‰øùÊåÅËàá DP-SGD È°û‰ººÁöÑÈö±ÁßÅÂíåÊïàÁî®‰øùË≠â„ÄÇÁôºÁèæ DP-BloGS ÊØî DP-SGD Â∞çË≥áÊñôÊèêÂèñÂòóË©¶ÂÖ∑ÊúâÈ°ØËëóÊõ¥È´òÁöÑÊäµÊäóÂäõ„ÄÇÂØ¶È©óÁµêÊûúÈ©óË≠â‰∫ÜÁêÜË´ñÁµêÊûú„ÄÇ

##### **Dual-Constrained Dynamical Neural ODEs for Ambiguity-aware Continuous Emotion Prediction**
2407.21344v1 by Jingyao Wu, Ting Dang, Vidhyasaharan Sethu, Eliathamby Ambikairajah

There has been a significant focus on modelling emotion ambiguity in recent
years, with advancements made in representing emotions as distributions to
capture ambiguity. However, there has been comparatively less effort devoted to
the consideration of temporal dependencies in emotion distributions which
encodes ambiguity in perceived emotions that evolve smoothly over time.
Recognizing the benefits of using constrained dynamical neural ordinary
differential equations (CD-NODE) to model time series as dynamic processes, we
propose an ambiguity-aware dual-constrained Neural ODE approach to model the
dynamics of emotion distributions on arousal and valence. In our approach, we
utilize ODEs parameterised by neural networks to estimate the distribution
parameters, and we integrate additional constraints to restrict the range of
the system outputs to ensure the validity of predicted distributions. We
evaluated our proposed system on the publicly available RECOLA dataset and
observed very promising performance across a range of evaluation metrics.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÊÉÖÁ∑íÊ®°Á≥äÂª∫Ê®°ÂÇôÂèóÈóúÊ≥®ÔºåÂú®Ë°®Á§∫ÊÉÖÁ∑íÁÇ∫ÂàÜ‰Ωà‰ª•ÊçïÊçâÊ®°Á≥äÊÄßÊñπÈù¢ÂèñÂæóÈÄ≤Â±ï„ÄÇÁÑ∂ËÄåÔºåÁõ∏Â∞çËÄåË®ÄÔºåËºÉÂ∞ëÈóúÊ≥®ÊÉÖÁ∑íÂàÜ‰Ωà‰∏≠ÁöÑÊôÇÈñì‰æùË≥¥ÊÄßÔºåËÄåÊôÇÈñì‰æùË≥¥ÊÄßÁ∑®Á¢º‰∫ÜÈö®ËëóÊôÇÈñìÊé®ÁßªËÄåÂπ≥Á©©ÊºîËÆäÁöÑÊÑüÁü•ÊÉÖÁ∑í‰∏≠ÁöÑÊ®°Á≥äÊÄß„ÄÇË™çË≠òÂà∞‰ΩøÁî®Á¥ÑÊùüÂãïÊÖãÁ•ûÁ∂ìÂ∏∏ÂæÆÂàÜÊñπÁ®ã (CD-NODE) Â∞áÊôÇÈñìÂ∫èÂàóÂª∫Ê®°ÁÇ∫ÂãïÊÖãÈÅéÁ®ãÁöÑÂ•ΩËôïÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°Á≥äÊÑüÁü•ÈõôÁ¥ÑÊùüÁ•ûÁ∂ì ODE ÊñπÊ≥ïÔºå‰ª•Âª∫Ê®°ÂñöÈÜíÂíåÊïàÂÉπÁöÑÊÉÖÁ∑íÂàÜ‰ΩàÂãïÊÖã„ÄÇÂú®ÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏≠ÔºåÊàëÂÄëÂà©Áî®Á•ûÁ∂ìÁ∂≤Ë∑ØÂèÉÊï∏ÂåñÁöÑ ODE ‰æÜ‰º∞Ë®àÂàÜ‰ΩàÂèÉÊï∏Ôºå‰∏¶Êï¥ÂêàÈ°çÂ§ñÁöÑÁ¥ÑÊùüÊ¢ù‰ª∂‰æÜÈôêÂà∂Á≥ªÁµ±Ëº∏Âá∫ÁöÑÁØÑÂúçÔºå‰ª•Á¢∫‰øùÈ†êÊ∏¨ÂàÜ‰ΩàÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÂú®ÂÖ¨ÈñãÁöÑ RECOLA Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞‰∫ÜÊàëÂÄëÊèêÂá∫ÁöÑÁ≥ªÁµ±Ôºå‰∏¶Âú®ÂêÑÁ®ÆË©ï‰º∞ÊåáÊ®ô‰∏≠ËßÄÂØüÂà∞ÈùûÂ∏∏ÊúâÂ∏åÊúõÁöÑË°®Áèæ„ÄÇ

##### **Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks**
2407.21338v1 by David Valencia, Henry Williams, Yuning Xing, Trevor Gee, Minas Liarokapis, Bruce A. MacDonald

Reinforcement Learning (RL) has been widely used to solve tasks where the
environment consistently provides a dense reward value. However, in real-world
scenarios, rewards can often be poorly defined or sparse. Auxiliary signals are
indispensable for discovering efficient exploration strategies and aiding the
learning process. In this work, inspired by intrinsic motivation theory, we
postulate that the intrinsic stimuli of novelty and surprise can assist in
improving exploration in complex, sparsely rewarded environments. We introduce
a novel sample-efficient method able to learn directly from pixels, an
image-based extension of TD3 with an autoencoder called \textit{NaSA-TD3}. The
experiments demonstrate that NaSA-TD3 is easy to train and an efficient method
for tackling complex continuous-control robotic tasks, both in simulated
environments and real-world settings. NaSA-TD3 outperforms existing
state-of-the-art RL image-based methods in terms of final performance without
requiring pre-trained models or human demonstrations.

ÊëòË¶ÅÔºöÂº∑ÂåñÂ≠∏Áøí (RL) Â∑≤Âª£Ê≥õÁî®ÊñºËß£Ê±∫Áí∞Â¢ÉÊåÅÁ∫åÊèê‰æõÂØÜÈõÜÁçéÂãµÂÄºÁöÑ‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºåÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ‰∏≠ÔºåÁçéÂãµÈÄöÂ∏∏ÂèØËÉΩÂÆöÁæ©‰∏ç‰Ω≥ÊàñÁ®ÄÁñè„ÄÇËºîÂä©Ë®äËôüÂ∞çÊñºÁôºÁèæÊúâÊïàÁöÑÊé¢Á¥¢Á≠ñÁï•ÂíåÂçîÂä©Â≠∏ÁøíÈÅéÁ®ãÊòØ‰∏çÂèØÊàñÁº∫ÁöÑ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÂèóÂà∞ÂÖßÂú®ÂãïÊ©üÁêÜË´ñÁöÑÂïüÁôºÔºåÊàëÂÄëÂÅáË®≠Êñ∞Â•áÂíåÈ©öÂ•áÁöÑÂÖßÂú®Âà∫ÊøÄÊúâÂä©ÊñºÊîπÂñÑÂú®Ë§áÈõú„ÄÅÁçéÂãµÁ®ÄÁñèÁöÑÁí∞Â¢É‰∏≠Êé¢Á¥¢„ÄÇÊàëÂÄë‰ªãÁ¥π‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ®£Êú¨ÊúâÊïàÁéáÊñπÊ≥ïÔºåËÉΩÂ§†Áõ¥Êé•ÂæûÂÉèÁ¥†Â≠∏ÁøíÔºå‰∏ÄÁ®Æ TD3 ÁöÑÂü∫ÊñºÂΩ±ÂÉèÁöÑÂª∂‰º∏ÔºåÂ∏∂ÊúâÁ®±ÁÇ∫ \textit{NaSA-TD3} ÁöÑËá™ÂãïÁ∑®Á¢ºÂô®„ÄÇÂØ¶È©óË≠âÊòéÔºåNaSA-TD3 ÊòìÊñºË®ìÁ∑¥Ôºå‰∏îÊòØ‰∏ÄÁ®ÆÊúâÊïàÁöÑÊñπÊ≥ïÔºåÁî®ÊñºËß£Ê±∫Ë§áÈõúÁöÑÈÄ£Á∫åÊéßÂà∂Ê©üÂô®‰∫∫‰ªªÂãôÔºåÁÑ°Ë´ñÊòØÂú®Ê®°Êì¨Áí∞Â¢ÉÊàñÁúüÂØ¶‰∏ñÁïåË®≠ÂÆö‰∏≠„ÄÇNaSA-TD3 Âú®ÊúÄÁµÇÊïàËÉΩÊñπÈù¢ÂÑ™ÊñºÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤ RL Âü∫ÊñºÂΩ±ÂÉèÁöÑÊñπÊ≥ïÔºåËÄå‰∏çÈúÄË¶ÅÈ†êÂÖàË®ìÁ∑¥ÁöÑÊ®°ÂûãÊàñ‰∫∫È°ûÁ§∫ÁØÑ„ÄÇ

##### **Performance of Recent Large Language Models for a Low-Resourced Language**
2407.21330v1 by Ravindu Jayakody, Gihan Dias

Large Language Models (LLMs) have shown significant advances in the past
year. In addition to new versions of GPT and Llama, several other LLMs have
been introduced recently. Some of these are open models available for download
and modification.
  Although multilingual large language models have been available for some
time, their performance on low-resourced languages such as Sinhala has been
poor. We evaluated four recent LLMs on their performance directly in the
Sinhala language, and by translation to and from English. We also evaluated
their fine-tunability with a small amount of fine-tuning data. Claude and GPT
4o perform well out-of-the-box and do significantly better than previous
versions. Llama and Mistral perform poorly but show some promise of improvement
with fine tuning.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈÅéÂéª‰∏ÄÂπ¥‰∏≠Â±ïÁèæÂá∫È°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÈô§‰∫Ü GPT Âíå Llama ÁöÑÊñ∞ÁâàÊú¨‰πãÂ§ñÔºåÊúÄËøëÈÇÑÊé®Âá∫‰∫ÜÂÖ∂‰ªñÂπæÁ®Æ LLM„ÄÇÂÖ∂‰∏≠‰∏Ä‰∫õÊòØÈñãÊîæÊ®°ÂûãÔºåÂèØ‰æõ‰∏ãËºâÂíå‰øÆÊîπ„ÄÇ
ÂÑòÁÆ°Â§öË™ûË®ÄÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂ∑≤Á∂ìÂ≠òÂú®‰∏ÄÊÆµÊôÇÈñìÔºå‰ΩÜÂÆÉÂÄëÂú®ÂÉß‰ºΩÁæÖË™ûÁ≠â‰ΩéË≥áÊ∫êË™ûË®Ä‰∏äÁöÑË°®Áèæ‰∏ÄÁõ¥ÂæàÂ∑Æ„ÄÇÊàëÂÄëÁõ¥Êé•Âú®ÂÉß‰ºΩÁæÖË™ûÂíåÈÄöÈÅéÁøªË≠ØÊàêËã±Ë™ûÂíåÂæûËã±Ë™ûÁøªË≠ØÁöÑÊñπÂºèË©ï‰º∞‰∫ÜÂõõÁ®ÆÊúÄÊñ∞ÁöÑ LLM ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÈÇÑË©ï‰º∞‰∫ÜÂÆÉÂÄëÂú®Â∞ëÈáèÂæÆË™øÊï∏Êìö‰∏ãÁöÑÂæÆË™øËÉΩÂäõ„ÄÇClaude Âíå GPT 4o ÈñãÁÆ±Âç≥Áî®Ë°®ÁèæËâØÂ•ΩÔºå‰∏¶‰∏îÊØî‰πãÂâçÁöÑÁâàÊú¨ÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇLlama Âíå Mistral Ë°®Áèæ‰∏ç‰Ω≥Ôºå‰ΩÜÂæÆË™øÂæåË°®ÁèæÂá∫‰∫Ü‰∏Ä‰∫õÊîπÈÄ≤ÁöÑÂ∏åÊúõ„ÄÇ

##### **MetaOpenFOAM: an LLM-based multi-agent framework for CFD**
2407.21320v1 by Yuxuan Chena, Xu Zhua, Hua Zhoua, Zhuyin Rena

Remarkable progress has been made in automated problem solving through
societies of agents based on large language models (LLMs). Computational fluid
dynamics (CFD), as a complex problem, presents unique challenges in automated
simulations that require sophisticated solutions. MetaOpenFOAM, as a novel
multi-agent collaborations framework, aims to complete CFD simulation tasks
with only natural language as input. These simulation tasks include mesh
pre-processing, simulation and post-processing, etc. MetaOpenFOAM harnesses the
power of MetaGPT's assembly line paradigm, which assigns diverse roles to
various agents, efficiently breaking down complex CFD tasks into manageable
subtasks. Langchain further complements MetaOpenFOAM by integrating
Retrieval-Augmented Generation (RAG) technology, which enhances the framework's
ability by integrating a searchable database of OpenFOAM tutorials for LLMs.
Tests on a benchmark for natural language-based CFD solver, consisting of 8 CFD
simulation tasks, have shown that MetaOpenFOAM achieved a high pass rate per
test (85%), with each test case costing only $0.22 on average. The 8 CFD
simulation tasks include compressible and incompressible flows, 2D and 3D
flows, heat transfer, and combustion, demonstrating the ability to automate CFD
simulations using only natural language input and iteratively correct errors to
achieve the desired simulation at a low cost. An ablation study was conducted
to verify the necessity of each component in the multi-agent system and the RAG
technology. A sensitivity study on the randomness of LLM showed that LLM with
low randomness can obtain more stable and accurate results. Additionally,
MetaOpenFOAM own the ability to identify and modify key parameters in user
requirements and excels in correcting bugs when failures occur, with or without
human participation, which demonstrates the generalization of MetaOpenFOAM.

ÊëòË¶ÅÔºö<paragraph>Âà©Áî®Âü∫ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑ‰ª£ÁêÜÁ§æÁæ§ÔºåÂú®Ëá™ÂãïÂåñÂïèÈ°åËß£Ê±∫ÊñπÈù¢ÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÈÄ≤Â±ï„ÄÇË®àÁÆóÊµÅÈ´îÂäõÂ≠∏ (CFD) ‰ΩúÁÇ∫‰∏ÄÂÄãË§áÈõúÁöÑÂïèÈ°åÔºåÂú®ÈúÄË¶ÅÁ≤æÂØÜËß£Ê±∫ÊñπÊ°àÁöÑËá™ÂãïÂåñÊ®°Êì¨‰∏≠ÊèêÂá∫‰∫ÜÁç®ÁâπÁöÑÊåëÊà∞„ÄÇMetaOpenFOAM ‰ΩúÁÇ∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ§öÈáç‰ª£ÁêÜÂçî‰ΩúÊ°ÜÊû∂ÔºåÊó®Âú®ÂÉÖ‰ª•Ëá™ÁÑ∂Ë™ûË®Ä‰ΩúÁÇ∫Ëº∏ÂÖ•‰æÜÂÆåÊàê CFD Ê®°Êì¨‰ªªÂãô„ÄÇÈÄô‰∫õÊ®°Êì¨‰ªªÂãôÂåÖÊã¨Á∂≤Ê†ºÂâçËôïÁêÜ„ÄÅÊ®°Êì¨ÂíåÂæåËôïÁêÜÁ≠â„ÄÇMetaOpenFOAM Âà©Áî®‰∫Ü MetaGPT ÁµÑË£ùÁ∑öÁØÑ‰æãÁöÑÂäõÈáèÔºåÂ∞á‰∏çÂêåÁöÑËßíËâ≤ÂàÜÈÖçÁµ¶‰∏çÂêåÁöÑ‰ª£ÁêÜÔºåÊúâÊïàÂú∞Â∞áË§áÈõúÁöÑ CFD ‰ªªÂãôÂàÜËß£ÁÇ∫ÂèØÁÆ°ÁêÜÁöÑÂ≠ê‰ªªÂãô„ÄÇLangchain ÈÄ≤‰∏ÄÊ≠•ÈÄèÈÅéÊï¥ÂêàÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊäÄË°ì‰æÜË£úÂÖÖ MetaOpenFOAMÔºåÈÄôÈÄèÈÅéÊï¥ÂêàÂèØÊêúÂ∞ãÁöÑ OpenFOAM ÊïôÂ≠∏Ë≥áÊñôÂ∫´‰æÜÂ¢ûÂº∑ LLM ÁöÑÊ°ÜÊû∂ËÉΩÂäõ„ÄÇÂú®‰∏ÄÂÄãÂü∫ÊñºËá™ÁÑ∂Ë™ûË®ÄÁöÑ CFD Ê±ÇËß£Âô®ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåÂåÖÂê´ 8 ÂÄã CFD Ê®°Êì¨‰ªªÂãôÔºåÁµêÊûúÈ°ØÁ§∫ MetaOpenFOAM Âú®ÊØèÂÄãÊ∏¨Ë©¶‰∏≠ÈÉΩÈÅîÂà∞‰∫ÜÂæàÈ´òÁöÑÈÄöÈÅéÁéá (85%)ÔºåÊØèÂÄãÊ∏¨Ë©¶Ê°à‰æãÂπ≥ÂùáÂè™Ëä±Ë≤ª 0.22 ÁæéÂÖÉ„ÄÇÈÄô 8 ÂÄã CFD Ê®°Êì¨‰ªªÂãôÂåÖÊã¨ÂèØÂ£ìÁ∏ÆÂíå‰∏çÂèØÂ£ìÁ∏ÆÊµÅ„ÄÅ2D Âíå 3D ÊµÅ„ÄÅÁÜ±ÂÇ≥ÈÅûÂíåÁáÉÁáíÔºåÂ±ïÁ§∫‰∫ÜÂÉÖ‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄËº∏ÂÖ•ÂíåÂèçË¶ÜÊõ¥Ê≠£ÈåØË™§Â∞±ËÉΩËá™ÂãïÂü∑Ë°å CFD Ê®°Êì¨ÁöÑËÉΩÂäõÔºå‰ª•‰ΩéÊàêÊú¨ÂØ¶ÁèæÊâÄÈúÄÁöÑÊ®°Êì¨„ÄÇÈÄ≤Ë°å‰∫ÜÊ∂àËûçÁ†îÁ©∂‰ª•È©óË≠âÂ§öÈáç‰ª£ÁêÜÁ≥ªÁµ±Âíå RAG ÊäÄË°ì‰∏≠ÊØèÂÄãÁµÑÊàêÁöÑÂøÖË¶ÅÊÄß„ÄÇÂ∞ç LLM Èö®Ê©üÊÄßÁöÑÊïèÊÑüÊÄßÁ†îÁ©∂Ë°®ÊòéÔºåÈö®Ê©üÊÄß‰ΩéÁöÑ LLM ÂèØ‰ª•Áç≤ÂæóÊõ¥Á©©ÂÆöÂíåÊ∫ñÁ¢∫ÁöÑÁµêÊûú„ÄÇÊ≠§Â§ñÔºåMetaOpenFOAM ÊìÅÊúâË≠òÂà•Âíå‰øÆÊîπ‰ΩøÁî®ËÄÖÈúÄÊ±Ç‰∏≠ÈóúÈçµÂèÉÊï∏ÁöÑËÉΩÂäõÔºå‰∏¶‰∏îÂú®ÁôºÁîüÊïÖÈöúÊôÇÊìÖÈï∑‰øÆÊ≠£ÈåØË™§ÔºåÁÑ°Ë´ñÊòØÂê¶Êúâ‰∫∫ÂèÉËàáÔºåÈÄôË≠âÊòé‰∫Ü MetaOpenFOAM ÁöÑÊ¶ÇÊã¨ÊÄß„ÄÇ</paragraph>

##### **Big Cooperative Learning**
2407.21319v1 by Yulai Cong

Cooperation plays a pivotal role in the evolution of human intelligence;
moreover, it also underlies the recent revolutionary advancement of artificial
intelligence (AI) that is driven by foundation models. Specifically, we reveal
that the training of foundation models can be interpreted as a form of big
cooperative learning (\textit{abbr.} big learning), where massive learning
individuals/tasks \emph{cooperate} to approach the unique essence of data from
diverse perspectives of data prediction, leveraging a universal model. The
presented big learning therefore unifies most training objectives of foundation
models within a consistent framework, where their underlying assumptions are
exposed simultaneously. We design tailored simulations to demonstrate the
principle of big learning, based on which we provide learning-perspective
justifications for the successes of foundation models, with interesting
side-products. Furthermore, we reveal that big learning is a new dimension for
upgrading conventional machine learning paradigms, valuable for endowing
reinvigorations to associated applications; as an illustrative example, we
propose the BigLearn-GAN, which is a novel adversarially-trained foundation
model with versatile data sampling capabilities. Code is available at
\texttt{https://github.com/YulaiCong/BigCooperativeLearning}.

ÊëòË¶ÅÔºöÂêà‰ΩúÂú®‰∫∫È°ûÊô∫ËÉΩÁöÑÊºîÂåñ‰∏≠ÊâÆÊºîËëóËàâË∂≥ËºïÈáçÁöÑËßíËâ≤Ôºõ
Ê≠§Â§ñÔºåÂÆÉ‰πüÊîØÊíêËëóËøëÊúüÁî±Âü∫Á§éÊ®°ÂûãÊé®ÂãïÁöÑ‰∫∫Â∑•Êô∫ÊÖß (AI) ÁöÑÈù©ÂëΩÊÄßÈÄ≤Â±ï„ÄÇÂÖ∑È´îËÄåË®ÄÔºåÊàëÂÄëÊè≠Á§∫‰∫ÜÂü∫Á§éÊ®°ÂûãÁöÑË®ìÁ∑¥ÂèØ‰ª•Ë¢´Ëß£ÈáãÁÇ∫‰∏ÄÁ®ÆÂ§ßÂûãÂêà‰ΩúÂ≠∏ÁøíÔºàÁ∞°Á®±Â§ßÂ≠∏ÁøíÔºâÁöÑÂΩ¢ÂºèÔºåÂÖ∂‰∏≠Â§ßÈáèÁöÑÂ≠∏ÁøíÂÄãÈ´î/‰ªªÂãô„ÄåÂêà‰Ωú„ÄçÂæûË≥áÊñôÈ†êÊ∏¨ÁöÑ‰∏çÂêåËßÄÈªû‰æÜÊé•ËøëË≥áÊñôÁöÑÁç®ÁâπÊú¨Ë≥™ÔºåÂà©Áî®‰∏ÄÂÄãÈÄöÁî®Ê®°Âûã„ÄÇÂõ†Ê≠§ÔºåÊâÄÊèêÂá∫ÁöÑ„ÄåÂ§ßÂ≠∏Áøí„ÄçÂú®‰∏ÄÂÄã‰∏ÄËá¥ÁöÑÊû∂Êßã‰∏≠Áµ±‰∏Ä‰∫ÜÂü∫Á§éÊ®°ÂûãÁöÑÂ§ßÈÉ®ÂàÜË®ìÁ∑¥ÁõÆÊ®ôÔºåÂêåÊôÇÊè≠Èú≤‰∫ÜÂÖ∂ËÉåÂæåÁöÑÂÅáË®≠„ÄÇÊàëÂÄëË®≠Ë®à‰∫ÜÂÆ¢Ë£ΩÂåñÁöÑÊ®°Êì¨‰æÜÂ±ïÁ§∫Â§ßÂ≠∏ÁøíÁöÑÂéüÁêÜÔºå‰∏¶Ê†πÊìöÊ≠§ÂéüÁêÜÔºåÁÇ∫Âü∫Á§éÊ®°ÂûãÁöÑÊàêÂäüÊèê‰æõÂ≠∏ÁøíËßÄÈªûÁöÑÂêàÁêÜÂåñÔºå‰∏¶Áî¢ÁîüÊúâË∂£ÁöÑÂâØÁî¢ÂìÅ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊè≠Á§∫‰∫ÜÂ§ßÂ≠∏ÁøíÊòØÂçáÁ¥öÂÇ≥Áµ±Ê©üÂô®Â≠∏ÁøíÁØÑ‰æãÁöÑÊñ∞Èù¢ÂêëÔºåÂ∞çÊñºË≥¶‰∫àÁõ∏ÈóúÊáâÁî®Á®ãÂºèÊñ∞ÁöÑÊ¥ªÂäõÈùûÂ∏∏ÊúâÂÉπÂÄºÔºõ‰ΩúÁÇ∫‰∏ÄÂÄãË™™ÊòéÊÄßÁöÑÁØÑ‰æãÔºåÊàëÂÄëÊèêÂá∫‰∫Ü BigLearn-GANÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÂ∞çÊäóÂºèË®ìÁ∑¥Âü∫Á§éÊ®°ÂûãÔºåÂÖ∑ÂÇôÂ§öÂäüËÉΩÁöÑË≥áÊñôÂèñÊ®£ËÉΩÂäõ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® \texttt{https://github.com/YulaiCong/BigCooperativeLearning} ÂèñÂæó„ÄÇ

##### **Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal Nuances**
2407.21315v2 by Zehui Wu, Ziwei Gong, Lin Ai, Pengyuan Shi, Kaan Donbekci, Julia Hirschberg

This paper introduces a novel approach to emotion detection in speech using
Large Language Models (LLMs). We address the limitation of LLMs in processing
audio inputs by translating speech characteristics into natural language
descriptions. Our method integrates these descriptions into text prompts,
enabling LLMs to perform multimodal emotion analysis without architectural
modifications. We evaluate our approach on two datasets: IEMOCAP and MELD,
demonstrating significant improvements in emotion recognition accuracy,
particularly for high-quality audio data. Our experiments show that
incorporating speech descriptions yields a 2 percentage point increase in
weighted F1 score on IEMOCAP (from 70.111\% to 72.596\%). We also compare
various LLM architectures and explore the effectiveness of different feature
representations. Our findings highlight the potential of this approach in
enhancing emotion detection capabilities of LLMs and underscore the importance
of audio quality in speech-based emotion recognition tasks. We'll release the
source code on Github.

ÊëòË¶ÅÔºöÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑÊñπÊ≥ïÔºå‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄ≤Ë°åË™ûÈü≥ÁöÑÊÉÖÁ∑íÂÅµÊ∏¨„ÄÇÊàëÂÄëÈÄèÈÅéÂ∞áË™ûÈü≥ÁâπÂæµËΩâÊèõÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄÊèèËø∞Ôºå‰æÜËß£Ê±∫ LLM Âú®ËôïÁêÜÈü≥Ë®äËº∏ÂÖ•ÊôÇÁöÑÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂ∞áÈÄô‰∫õÊèèËø∞Êï¥ÂêàÂà∞ÊñáÂ≠óÊèêÁ§∫‰∏≠ÔºåËÆì LLM ËÉΩÂ§†Âú®‰∏ç‰øÆÊîπÊû∂ÊßãÁöÑÊÉÖÊ≥Å‰∏ãÂü∑Ë°åÂ§öÊ®°ÊÖãÁöÑÊÉÖÁ∑íÂàÜÊûê„ÄÇÊàëÂÄëÂú®ÂÖ©ÂÄãË≥áÊñôÈõÜÔºöIEMOCAP Âíå MELD ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÊäÄË°ìÔºåÈ°ØÁ§∫Âá∫Âú®ÊÉÖÁ∑íËæ®Ë≠òÊ∫ñÁ¢∫Â∫¶‰∏äÂ§ßÂπÖÊèêÂçáÔºåÁâπÂà•ÊòØÂ∞çÊñºÈ´òÂìÅË≥™ÁöÑÈü≥Ë®äË≥áÊñô„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÂä†ÂÖ•Ë™ûÈü≥ÊèèËø∞ËÆì IEMOCAP ‰∏äÁöÑÂä†Ê¨ä F1 ÂàÜÊï∏ÊèêÂçá‰∫Ü 2 ÂÄãÁôæÂàÜÈªûÔºàÂæû 70.111% Âà∞ 72.596%Ôºâ„ÄÇÊàëÂÄë‰πüÊØîËºÉ‰∫ÜÂêÑÁ®Æ LLM Êû∂ÊßãÔºå‰∏¶Êé¢Á¥¢‰∫Ü‰∏çÂêåÁâπÂæµË°®ÂæµÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÁöÑÁôºÁèæÁ™ÅÈ°Ø‰∫ÜÈÄôÁ®ÆÊäÄË°ìÂú®ÊèêÂçá LLM ÊÉÖÁ∑íÂÅµÊ∏¨ËÉΩÂäõÁöÑÊΩõÂäõÔºå‰∏¶Âº∑Ë™ø‰∫ÜÈü≥Ë®äÂìÅË≥™Âú®Âü∫ÊñºË™ûÈü≥ÁöÑÊÉÖÁ∑íËæ®Ë≠ò‰ªªÂãô‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÊàëÂÄëÂ∞áÂú® Github ‰∏äÈáãÂá∫ÂéüÂßãÁ¢º„ÄÇ

##### **EUDA: An Efficient Unsupervised Domain Adaptation via Self-Supervised Vision Transformer**
2407.21311v1 by Ali Abedi, Q. M. Jonathan Wu, Ning Zhang, Farhad Pourpanah

Unsupervised domain adaptation (UDA) aims to mitigate the domain shift issue,
where the distribution of training (source) data differs from that of testing
(target) data. Many models have been developed to tackle this problem, and
recently vision transformers (ViTs) have shown promising results. However, the
complexity and large number of trainable parameters of ViTs restrict their
deployment in practical applications. This underscores the need for an
efficient model that not only reduces trainable parameters but also allows for
adjustable complexity based on specific needs while delivering comparable
performance. To achieve this, in this paper we introduce an Efficient
Unsupervised Domain Adaptation (EUDA) framework. EUDA employs the DINOv2, which
is a self-supervised ViT, as a feature extractor followed by a simplified
bottleneck of fully connected layers to refine features for enhanced domain
adaptation. Additionally, EUDA employs the synergistic domain alignment loss
(SDAL), which integrates cross-entropy (CE) and maximum mean discrepancy (MMD)
losses, to balance adaptation by minimizing classification errors in the source
domain while aligning the source and target domain distributions. The
experimental results indicate the effectiveness of EUDA in producing comparable
results as compared with other state-of-the-art methods in domain adaptation
with significantly fewer trainable parameters, between 42% to 99.7% fewer. This
showcases the ability to train the model in a resource-limited environment. The
code of the model is available at: https://github.com/A-Abedi/EUDA.

ÊëòË¶ÅÔºöÁÑ°Áõ£Áù£ÂüüÈÅ©Êáâ (UDA) Êó®Âú®Ê∏õËºïÂüüÂÅèÁßªÂïèÈ°åÔºåÂÖ∂‰∏≠Ë®ìÁ∑¥ (‰æÜÊ∫ê) Ë≥áÊñôÁöÑÂàÜÈÖçËàáÊ∏¨Ë©¶ (ÁõÆÊ®ô) Ë≥áÊñôÁöÑÂàÜÈÖç‰∏çÂêå„ÄÇÂ∑≤ÈñãÁôºË®±Â§öÊ®°Âûã‰æÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåËÄåÊúÄËøëÁöÑË¶ñË¶∫ËΩâÊèõÂô® (ViT) Â∑≤È°ØÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåViT ÁöÑË§áÈõúÊÄßÂíåÂ§ßÈáèÁöÑÂèØË®ìÁ∑¥ÂèÉÊï∏ÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®ÂØ¶ÈöõÊáâÁî®‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇÈÄôÂº∑Ë™ø‰∫ÜÂ∞ç‰∏ÄÁ®ÆÊúâÊïàÊ®°ÂûãÁöÑÈúÄÊ±ÇÔºåË©≤Ê®°Âûã‰∏çÂÉÖÂèØ‰ª•Ê∏õÂ∞ëÂèØË®ìÁ∑¥ÂèÉÊï∏ÔºåÈÇÑÂèØ‰ª•Ê†πÊìöÁâπÂÆöÈúÄÊ±ÇË™øÊï¥Ë§áÈõúÊÄßÔºåÂêåÊôÇÊèê‰æõÂèØÊØîËºÉÁöÑÊïàËÉΩ„ÄÇÁÇ∫‰∫ÜÂØ¶ÁèæÈÄô‰∏ÄÈªûÔºåÊàëÂÄëÂú®Êú¨Êñá‰∏≠‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊúâÊïàÁöÑÁÑ°Áõ£Áù£ÂüüÈÅ©Êáâ (EUDA) Ê°ÜÊû∂„ÄÇEUDA ‰ΩøÁî® DINOv2ÔºåÈÄôÊòØ‰∏ÄÂÄãËá™Áõ£Áù£ÁöÑ ViTÔºå‰ΩúÁÇ∫‰∏ÄÂÄãÁâπÂæµÊèêÂèñÂô®ÔºåÂæåÈù¢ÊòØ‰∏ÄÂÄãÁ∞°ÂåñÁöÑÂÖ®ÈÄ£Êé•Â±§Áì∂È†∏Ôºå‰ª•ÂÑ™ÂåñÁâπÂæµ‰ª•Â¢ûÂº∑ÂüüÈÅ©Êáâ„ÄÇÊ≠§Â§ñÔºåEUDA Êé°Áî®ÂçîÂêåÂüüÂ∞çÈΩäÊêçÂ§± (SDAL)ÔºåÂÆÉÊï¥Âêà‰∫Ü‰∫§ÂèâÁÜµ (CE) ÂíåÊúÄÂ§ßÂπ≥ÂùáÂ∑ÆÁï∞ (MMD) ÊêçÂ§±ÔºåÈÄöÈÅéÊúÄÂ∞èÂåñ‰æÜÊ∫êÂüü‰∏≠ÁöÑÂàÜÈ°ûÈåØË™§ÂêåÊôÇÂ∞çÈΩä‰æÜÊ∫êÂíåÁõÆÊ®ôÂüüÂàÜÈÖç‰æÜÂπ≥Ë°°ÈÅ©Êáâ„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàáÂÖ∂‰ªñÊúÄÂÖàÈÄ≤ÁöÑÂüüÈÅ©ÊáâÊñπÊ≥ïÁõ∏ÊØîÔºåEUDA Âú®Áî¢ÁîüÂèØÊØîËºÉÁöÑÁµêÊûúÊñπÈù¢ÊòØÊúâÊïàÁöÑÔºåÂèØË®ìÁ∑¥ÂèÉÊï∏È°ØËëóÊ∏õÂ∞ëÔºåÊ∏õÂ∞ë‰∫Ü 42% Âà∞ 99.7%„ÄÇÈÄôÂ±ïÁ§∫‰∫ÜÂú®Ë≥áÊ∫êÂèóÈôêÁöÑÁí∞Â¢É‰∏≠Ë®ìÁ∑¥Ê®°ÂûãÁöÑËÉΩÂäõ„ÄÇÊ®°ÂûãÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/A-Abedi/EUDA ‰∏≠ÂèñÂæó„ÄÇ

##### **Implementing Streaming algorithm and k-means clusters to RAG**
2407.21300v1 by Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang

Retrieval-augmented generation (RAG) has achieved great success in
information retrieval to assist large models because it builds an external
knowledge database. However, it also has many problems: it consumes a lot of
memory because of the huge database. When faced with massive streaming data, it
is unable to update the established index database in time. To save the memory
of building the database and maintain accuracy simultaneously, we proposed a
new approach combining a streaming algorithm and k-means cluster with RAG. Our
approach applies a streaming algorithm to update the index and reduce memory
consumption. Then use the k-means algorithm to cluster documents with high
similarities together, the query time will be shortened by doing this. We
conducted comparative experiments on four methods, and the results show that
RAG with streaming algorithm and k-means cluster performs well in accuracy and
memory. For massive streaming data, we find that our method behaves better than
traditional RAG

ÊëòË¶ÅÔºöÊ™¢Á¥¢Â¢ûÂº∑ÂºèÁîüÊàêÔºàRAGÔºâÂú®Ë≥áË®äÊ™¢Á¥¢ÊñπÈù¢Â∑≤ÂèñÂæóÂ∑®Â§ßÊàêÂäüÔºåÂèØÂçîÂä©Â§ßÂûãÊ®°ÂûãÂª∫ÁΩÆÂ§ñÈÉ®Áü•Ë≠òË≥áÊñôÂ∫´„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰πüÂ≠òÂú®Ë®±Â§öÂïèÈ°åÔºöÁî±ÊñºË≥áÊñôÂ∫´ÈæêÂ§ßÔºåÂÆÉÊúÉÊ∂àËÄóÂ§ßÈáèË®òÊÜ∂È´î„ÄÇÁï∂Èù¢Â∞çÂ§ßÈáè‰∏≤ÊµÅË≥áÊñôÊôÇÔºåÂÆÉÁÑ°Ê≥ïÂç≥ÊôÇÊõ¥Êñ∞Â∑≤Âª∫Á´ãÁöÑÁ¥¢ÂºïË≥áÊñôÂ∫´„ÄÇÁÇ∫‰∫ÜÂêåÊôÇÁØÄÁúÅÂª∫ÁΩÆË≥áÊñôÂ∫´ÁöÑË®òÊÜ∂È´î‰∏¶Á∂≠ÊåÅÊ∫ñÁ¢∫ÊÄßÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁµêÂêà‰∏≤ÊµÅÊºîÁÆóÊ≥ïÂíå k Âπ≥ÂùáÂÄºÁæ§ÈõÜËàá RAG ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØÊáâÁî®‰∏≤ÊµÅÊºîÁÆóÊ≥ï‰æÜÊõ¥Êñ∞Á¥¢Âºï‰∏¶Ê∏õÂ∞ëË®òÊÜ∂È´îÊ∂àËÄó„ÄÇÁÑ∂Âæå‰ΩøÁî® k Âπ≥ÂùáÂÄºÊºîÁÆóÊ≥ïÂ∞áÁõ∏‰ººÂ∫¶È´òÁöÑÊñá‰ª∂ÂàÜÁæ§ÔºåÈÄôÊ®£ÂèØ‰ª•Á∏ÆÁü≠Êü•Ë©¢ÊôÇÈñì„ÄÇÊàëÂÄëÂ∞çÂõõÁ®ÆÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÊØîËºÉÂØ¶È©óÔºåÁµêÊûúÈ°ØÁ§∫ÔºåÊé°Áî®‰∏≤ÊµÅÊºîÁÆóÊ≥ïÂíå k Âπ≥ÂùáÂÄºÁæ§ÈõÜÁöÑ RAG Âú®Ê∫ñÁ¢∫ÊÄßÂíåË®òÊÜ∂È´îÊñπÈù¢Ë°®ÁèæËâØÂ•Ω„ÄÇÂ∞çÊñºÂ§ßÈáè‰∏≤ÊµÅË≥áÊñôÔºåÊàëÂÄëÁôºÁèæÊàëÂÄëÁöÑÂÅöÊ≥ïÂÑ™ÊñºÂÇ≥Áµ± RAG

##### **Who should I trust? A Visual Analytics Approach for Comparing Net Load Forecasting Models**
2407.21299v1 by Kaustav Bhattacharjee, Soumya Kundu, Indrasis Chakraborty, Aritra Dasgupta

Net load forecasting is crucial for energy planning and facilitating informed
decision-making regarding trade and load distributions. However, evaluating
forecasting models' performance against benchmark models remains challenging,
thereby impeding experts' trust in the model's performance. In this context,
there is a demand for technological interventions that allow scientists to
compare models across various timeframes and solar penetration levels. This
paper introduces a visual analytics-based application designed to compare the
performance of deep-learning-based net load forecasting models with other
models for probabilistic net load forecasting. This application employs
carefully selected visual analytic interventions, enabling users to discern
differences in model performance across different solar penetration levels,
dataset resolutions, and hours of the day over multiple months. We also present
observations made using our application through a case study, demonstrating the
effectiveness of visualizations in aiding scientists in making informed
decisions and enhancing trust in net load forecasting models.

ÊëòË¶ÅÔºöÊ∑®ËºâËç∑È†êÊ∏¨Â∞çÊñºËÉΩÊ∫êË¶èÂäÉÂíå‰øÉÈÄ≤ÊúâÈóúË≤øÊòìÂíåË≤†ËºâÂàÜÈÖçÁöÑÊòéÊô∫Ê±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåË©ï‰º∞È†êÊ∏¨Ê®°ÂûãÁõ∏Â∞çÊñºÂü∫Ê∫ñÊ®°ÂûãÁöÑÊïàËÉΩ‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂæûËÄåÈòªÁ§ôÂ∞àÂÆ∂Â∞çÊ®°ÂûãÊïàËÉΩÁöÑ‰ø°‰ªª„ÄÇÂú®Ê≠§ËÉåÊôØ‰∏ãÔºåÈúÄË¶ÅÊäÄË°ìÂπ≤È†êÔºåÂÖÅË®±ÁßëÂ≠∏ÂÆ∂Âú®ÂêÑÁ®ÆÊôÇÈñìÁØÑÂúçÂíåÂ§™ÈôΩËÉΩÊª≤ÈÄèÁéáÂ±§Á¥ö‰∏≠ÊØîËºÉÊ®°Âûã„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÂü∫ÊñºË¶ñË¶∫ÂàÜÊûêÁöÑÊáâÁî®Á®ãÂºèÔºåÊó®Âú®ÊØîËºÉÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊ∑®ËºâËç∑È†êÊ∏¨Ê®°ÂûãËàáÂÖ∂‰ªñÊ©üÁéáÊ∑®ËºâËç∑È†êÊ∏¨Ê®°ÂûãÁöÑÊïàËÉΩ„ÄÇÊ≠§ÊáâÁî®Á®ãÂºèÊé°Áî®‰ªîÁ¥∞ÊåëÈÅ∏ÁöÑË¶ñË¶∫ÂàÜÊûêÂπ≤È†êÊé™ÊñΩÔºå‰ΩøÁî®Êà∂ËÉΩÂ§†Ëæ®Âà•Ê®°ÂûãÊïàËÉΩÁöÑÂ∑ÆÁï∞ÔºåÈÄô‰∫õÂ∑ÆÁï∞Â≠òÂú®Êñº‰∏çÂêåÁöÑÂ§™ÈôΩËÉΩÊª≤ÈÄèÁéáÂ±§Á¥ö„ÄÅË≥áÊñôÈõÜËß£ÊûêÂ∫¶ÂíåÂ§öÂÄãÊúàÁöÑÁôΩÂ§©ÊôÇÊÆµ‰∏≠„ÄÇÊàëÂÄë‰πüÈÄèÈÅéÊ°à‰æãÁ†îÁ©∂ÊèêÂá∫‰ΩøÁî®ÊàëÂÄëÁöÑÊáâÁî®Á®ãÂºèÊâÄÂÅöÁöÑËßÄÂØüÔºåË≠âÊòéË¶ñË¶∫ÂåñÂú®ÂçîÂä©ÁßëÂ≠∏ÂÆ∂ÂÅöÂá∫ÊòéÊô∫Ê±∫Á≠ñÂíåÂ¢ûÂº∑Â∞çÊ∑®ËºâËç∑È†êÊ∏¨Ê®°ÂûãÁöÑ‰ø°‰ªªÊñπÈù¢ÊòØÊúâÊïàÁöÑ„ÄÇ

##### **SimpleLLM4AD: An End-to-End Vision-Language Model with Graph Visual Question Answering for Autonomous Driving**
2407.21293v1 by Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, Shaohua Wu

Many fields could benefit from the rapid development of the large language
models (LLMs). The end-to-end autonomous driving (e2eAD) is one of the
typically fields facing new opportunities as the LLMs have supported more and
more modalities. Here, by utilizing vision-language model (VLM), we proposed an
e2eAD method called SimpleLLM4AD. In our method, the e2eAD task are divided
into four stages, which are perception, prediction, planning, and behavior.
Each stage consists of several visual question answering (VQA) pairs and VQA
pairs interconnect with each other constructing a graph called Graph VQA
(GVQA). By reasoning each VQA pair in the GVQA through VLM stage by stage, our
method could achieve e2e driving with language. In our method, vision
transformers (ViT) models are employed to process nuScenes visual data, while
VLM are utilized to interpret and reason about the information extracted from
the visual inputs. In the perception stage, the system identifies and
classifies objects from the driving environment. The prediction stage involves
forecasting the potential movements of these objects. The planning stage
utilizes the gathered information to develop a driving strategy, ensuring the
safety and efficiency of the autonomous vehicle. Finally, the behavior stage
translates the planned actions into executable commands for the vehicle. Our
experiments demonstrate that SimpleLLM4AD achieves competitive performance in
complex driving scenarios.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂø´ÈÄüÁôºÂ±ïÂèØËÉΩ‰ΩøË®±Â§öÈ†òÂüüÂèóÁõä„ÄÇÁ´ØÂà∞Á´ØËá™ÂãïÈßïÈßõ (e2eAD) ÊòØÂÖ∏ÂûãÈ†òÂüü‰πã‰∏ÄÔºåÂõ†ÁÇ∫ LLM ÊîØÊè¥Ë∂ä‰æÜË∂äÂ§öÁöÑÊ®°ÂºèÔºåÂõ†Ê≠§Èù¢Ëá®Êñ∞ÁöÑÊ©üÊúÉ„ÄÇÂú®Ê≠§ÔºåÈÄèÈÅéÂà©Áî®Ë¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (VLM)ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ SimpleLLM4AD ÁöÑ e2eAD ÊñπÊ≥ï„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠Ôºåe2eAD ‰ªªÂãôÂàÜÁÇ∫ÂõõÂÄãÈöéÊÆµÔºåÂàÜÂà•ÊòØÊÑüÁü•„ÄÅÈ†êÊ∏¨„ÄÅË¶èÂäÉÂíåË°åÁÇ∫„ÄÇÊØèÂÄãÈöéÊÆµÂåÖÂê´Â§öÂÄãË¶ñË¶∫ÂïèÁ≠î (VQA) ÈÖçÂ∞çÔºå‰∏î VQA ÈÖçÂ∞çÁõ∏‰∫íÈÄ£Êé•ÔºåÊßãÂª∫‰∏ÄÂÄãÁ®±ÁÇ∫ÂúñÂΩ¢ VQA (GVQA) ÁöÑÂúñÂΩ¢„ÄÇÈÄèÈÅé VLM ÂàÜÈöéÊÆµÊé®ÁêÜ GVQA ‰∏≠ÁöÑÊØèÂÄã VQA ÈÖçÂ∞çÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÈÄèÈÅéË™ûË®ÄÂØ¶ÁèæÁ´ØÂà∞Á´ØÈßïÈßõ„ÄÇÂú®ÊàëÂÄëÁöÑÊ®°Âûã‰∏≠ÔºåÊé°Áî®Ë¶ñË¶∫Transformer (ViT) Ê®°Âûã‰æÜËôïÁêÜ nuScenes Ë¶ñË¶∫Ë≥áÊñôÔºåÂêåÊôÇÂà©Áî® VLM ‰æÜË©ÆÈáãÂíåÊé®ÁêÜÂæûË¶ñË¶∫Ëº∏ÂÖ•‰∏≠ÊèêÂèñÁöÑË≥áË®ä„ÄÇÂú®ÊÑüÁü•ÈöéÊÆµÔºåÁ≥ªÁµ±Ë≠òÂà•ÂíåÂàÜÈ°ûÈßïÈßõÁí∞Â¢É‰∏≠ÁöÑÁâ©‰ª∂„ÄÇÈ†êÊ∏¨ÈöéÊÆµÊ∂âÂèäÈ†êÊ∏¨ÈÄô‰∫õÁâ©‰ª∂ÁöÑÊΩõÂú®ÁßªÂãï„ÄÇË¶èÂäÉÈöéÊÆµÂà©Áî®Êî∂ÈõÜÁöÑË≥áË®ä‰æÜÂà∂ÂÆöÈßïÈßõÁ≠ñÁï•ÔºåÁ¢∫‰øùËá™ÂãïÈßïÈßõÊ±ΩËªäÁöÑÂÆâÂÖ®ÊÄßÂíåÊïàÁéá„ÄÇÊúÄÂæåÔºåË°åÁÇ∫ÈöéÊÆµÂ∞áË¶èÂäÉÁöÑÂãï‰ΩúËΩâÊèõÁÇ∫ËªäËºõÂèØÂü∑Ë°åÁöÑÂëΩ‰ª§„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºåSimpleLLM4AD Âú®Ë§áÈõúÁöÑÈßïÈßõÂ†¥ÊôØ‰∏≠ÂØ¶Áèæ‰∫ÜÁ´∂Áà≠Âäõ„ÄÇ

##### **Robust Box Prompt based SAM for Medical Image Segmentation**
2407.21284v1 by Yuhao Huang, Xin Yang, Han Zhou, Yan Cao, Haoran Dou, Fajin Dong, Dong Ni

The Segment Anything Model (SAM) can achieve satisfactory segmentation
performance under high-quality box prompts. However, SAM's robustness is
compromised by the decline in box quality, limiting its practicality in
clinical reality. In this study, we propose a novel Robust Box prompt based SAM
(\textbf{RoBox-SAM}) to ensure SAM's segmentation performance under prompts
with different qualities. Our contribution is three-fold. First, we propose a
prompt refinement module to implicitly perceive the potential targets, and
output the offsets to directly transform the low-quality box prompt into a
high-quality one. We then provide an online iterative strategy for further
prompt refinement. Second, we introduce a prompt enhancement module to
automatically generate point prompts to assist the box-promptable segmentation
effectively. Last, we build a self-information extractor to encode the prior
information from the input image. These features can optimize the image
embeddings and attention calculation, thus, the robustness of SAM can be
further enhanced. Extensive experiments on the large medical segmentation
dataset including 99,299 images, 5 modalities, and 25 organs/targets validated
the efficacy of our proposed RoBox-SAM.

ÊëòË¶ÅÔºöÂàÜÊÆµ‰ªª‰ΩïÊ®°Âûã (SAM) ÂèØ‰ª•Âú®È´òË¥®ÈáèÊ°ÜÊèêÁ§∫‰∏ãÂÆûÁé∞‰ª§‰∫∫Êª°ÊÑèÁöÑÂàÜÊÆµÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåSAM ÁöÑÈ≤ÅÊ£íÊÄßÂõ†Ê°ÜË¥®ÈáèÁöÑ‰∏ãÈôçËÄåÂèóÂà∞ÊçüÂÆ≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂Âú®‰∏¥Â∫äÁé∞ÂÆû‰∏≠ÁöÑÂÆûÁî®ÊÄß„ÄÇÂú®ËøôÈ°πÁ†îÁ©∂‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Âü∫‰∫é SAM ÁöÑÊñ∞ÂûãÈ≤ÅÊ£íÊ°ÜÊèêÁ§∫Ôºà**RoBox-SAM**ÔºâÔºå‰ª•Á°Æ‰øù SAM Âú®ÂÖ∑Êúâ‰∏çÂêåË¥®ÈáèÁöÑÊèêÁ§∫‰∏ãÁöÑÂàÜÊÆµÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑË¥°ÁåÆÊòØ‰∏âÊñπÈù¢ÁöÑ„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨ÊèêÂá∫‰∏Ä‰∏™ÊèêÁ§∫‰ºòÂåñÊ®°ÂùóÔºå‰ª•ÈöêÂºèÊÑüÁü•ÊΩúÂú®ÁõÆÊ†áÔºåÂπ∂ËæìÂá∫ÂÅèÁßªÈáèÔºå‰ª•Áõ¥Êé•Â∞Ü‰ΩéË¥®ÈáèÊ°ÜÊèêÁ§∫ËΩ¨Êç¢‰∏∫È´òË¥®ÈáèÊèêÁ§∫„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™Âú®Á∫øËø≠‰ª£Á≠ñÁï•Ôºå‰ª•‰æøËøõ‰∏ÄÊ≠•‰ºòÂåñÊèêÁ§∫„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÊèêÁ§∫Â¢ûÂº∫Ê®°ÂùóÔºå‰ª•Ëá™Âä®ÁîüÊàêÁÇπÊèêÁ§∫Ôºå‰ª•ÊúâÊïàÂú∞ËæÖÂä©Ê°ÜÊèêÁ§∫ÂàÜÊÆµ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Ëá™‰ø°ÊÅØÊèêÂèñÂô®Ôºå‰ª•ÂØπÊù•Ëá™ËæìÂÖ•ÂõæÂÉèÁöÑÂÖàÈ™å‰ø°ÊÅØËøõË°åÁºñÁ†Å„ÄÇËøô‰∫õÁâπÂæÅÂèØ‰ª•‰ºòÂåñÂõæÂÉèÂµåÂÖ•ÂíåÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÂõ†Ê≠§ÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Â¢ûÂº∫ SAM ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÂú®ÂåÖÊã¨ 99,299 Âº†ÂõæÂÉè„ÄÅ5 ÁßçÊñπÂºèÂíå 25 ‰∏™Âô®ÂÆò/ÁõÆÊ†áÁöÑÂ§ßÂûãÂåªÂ≠¶ÂàÜÊÆµÊï∞ÊçÆÈõÜ‰∏äËøõË°åÁöÑÂπøÊ≥õÂÆûÈ™åÈ™åËØÅ‰∫ÜÊàë‰ª¨ÊèêÂá∫ÁöÑ RoBox-SAM ÁöÑÂäüÊïà„ÄÇ

##### **Multi-Level Querying using A Knowledge Pyramid**
2407.21276v1 by Rubing Chen, Xulu Zhang, Jiaxin Wu, Wenqi Fan, Xiao-Yong Wei, Qing Li

This paper addresses the need for improved precision in existing
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395\% F1 gain by improving its performance from 0.1636 to 0.8109.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÁèæÊúâÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) ÊñπÊ≥ï‰∏≠ÔºåÂ∞çÊñºÁ≤æÊ∫ñÂ∫¶ÁöÑÊèêÂçáÈúÄÊ±ÇÔºåÈÄô‰∫õÊñπÊ≥ï‰∏ªË¶ÅÂ∞àÊ≥®ÊñºÂ¢ûÂº∑Âè¨ÂõûÁéá„ÄÇÊàëÂÄëÂú® RAG Êû∂Êßã‰∏≠ÊèêÂá∫‰∏ÄÂÄãÂ§öÂ±§Áü•Ë≠òÈáëÂ≠óÂ°îÊñπÊ≥ïÔºå‰ª•Âú®Á≤æÊ∫ñÂ∫¶ÂíåÂè¨ÂõûÁéá‰πãÈñìÂèñÂæóÊõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇÁü•Ë≠òÈáëÂ≠óÂ°îÂåÖÂê´‰∏âÂÄãÂ±§Á¥öÔºöÊú¨‰Ωì„ÄÅÁü•Ë≠òÂúñË≠ú (KG) ÂíåÂü∫ÊñºÂçÄÂ°äÁöÑÂéüÂßãÊñáÂ≠ó„ÄÇÊàëÂÄëÊé°Áî®Ë∑®Â±§Â¢ûÂº∑ÊäÄË°ìÔºå‰ª•ÂØ¶ÁèæÂÖ®Èù¢ÁöÑÁü•Ë≠òÊ∂µËìãÁØÑÂúçÂíåÊú¨‰ΩìÊû∂ÊßãÂèäÂØ¶‰æãÁöÑÂãïÊÖãÊõ¥Êñ∞„ÄÇÁÇ∫‰∫ÜÁ¢∫‰øùÁ∑äÊπäÊÄßÔºåÊàëÂÄëÂà©Áî®Ë∑®Â±§ÈÅéÊøæÊñπÊ≥ïÔºå‰ª•ÈÄ≤Ë°å KG ‰∏≠ÁöÑÁü•Ë≠òÊøÉÁ∏Æ„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÁ®±ÁÇ∫ PolyRAGÔºåÂÆÉÈÅµÂæ™ÁÄëÂ∏ÉÊ®°ÂûãÈÄ≤Ë°åÊ™¢Á¥¢ÔºåÂæûÈáëÂ≠óÂ°îÈ†ÇÁ´ØÈñãÂßãÔºå‰∏¶Âêë‰∏ãÈÄ≤Ë°åÔºåÁõ¥Âà∞Áç≤ÂæóÁ¢∫ÂÆöÁöÑÁ≠îÊ°à„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜÂÖ©ÂÄãÁâπÂÆöÈ†òÂüüÁü•Ë≠òÊ™¢Á¥¢Âü∫Ê∫ñÔºå‰∏ÄÂÄãÂú®Â≠∏Ë°ìÈ†òÂüüÔºåÂè¶‰∏ÄÂÄãÂú®ÈáëËûçÈ†òÂüü„ÄÇÈÄô‰∫õÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂ∑≤ÈÄöÈÅéÂÖ®Èù¢ÂØ¶È©óÈ©óË≠âÔºåÂÖ∂Ë°®ÁèæÂÑ™Êñº 19 Á®Æ SOTA ÊñπÊ≥ï„ÄÇ‰∏ÄÂÄã‰ª§‰∫∫ÊåØÂ•ÆÁöÑËßÄÂØüÁµêÊûúÊòØÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂ∑≤Á∂ìÂ¢ûÂº∑‰∫Ü GPT-4ÔºåÈÄöÈÅéÂ∞áÂÖ∂ÊïàËÉΩÂæû 0.1636 ÊèêÂçáËá≥ 0.8109ÔºåÊèê‰æõ‰∫Ü 395% ÁöÑ F1 Â¢ûÁõä„ÄÇ

##### **Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net**
2407.21273v1 by Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski

Efficient intravascular access in trauma and critical care significantly
impacts patient outcomes. However, the availability of skilled medical
personnel in austere environments is often limited. Autonomous robotic
ultrasound systems can aid in needle insertion for medication delivery and
support non-experts in such tasks. Despite advances in autonomous needle
insertion, inaccuracies in vessel segmentation predictions pose risks.
Understanding the uncertainty of predictive models in ultrasound imaging is
crucial for assessing their reliability. We introduce MSU-Net, a novel
multistage approach for training an ensemble of U-Nets to yield accurate
ultrasound image segmentation maps. We demonstrate substantial improvements,
18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model
transparency, and trustworthiness. By highlighting areas of model certainty,
MSU-Net can guide safe needle insertions, empowering non-experts to accomplish
such tasks.

ÊëòË¶ÅÔºöÂú®ÂâµÂÇ∑ÂíåÈáçÁóáÁÖßË≠∑‰∏≠ÔºåÊúâÊïàÁöÑË°ÄÁÆ°ÂÖßÈÄöË∑ØÊúÉÈ°ØËëóÂΩ±ÈüøÁóÖÊÇ£ÁöÑÊ≤ªÁôÇÁµêÊûú„ÄÇÁÑ∂ËÄåÔºåÂú®ÊÉ°Âä£ÁöÑÁí∞Â¢É‰∏≠ÔºåÁÜüÁ∑¥ÁöÑÈÜ´ÁôÇ‰∫∫Âì°ÂæÄÂæÄ‰∏çË∂≥„ÄÇËá™‰∏ªÊ©üÂô®‰∫∫Ë∂ÖÈü≥Ê≥¢Á≥ªÁµ±ÂèØ‰ª•ÂçîÂä©ÈáùÈ†≠ÊèíÂÖ•Ôºå‰ª•Êèê‰æõËó•Áâ©‰∏¶ÊîØÊè¥ÈùûÂ∞àÂÆ∂Âü∑Ë°åÊ≠§È°û‰ªªÂãô„ÄÇÂÑòÁÆ°Ëá™‰∏ªÈáùÈ†≠ÊèíÂÖ•ÊäÄË°ìÈÄ≤Ê≠•Ôºå‰ΩÜË°ÄÁÆ°ÂàÜÂâ≤È†êÊ∏¨ÁöÑ‰∏çÊ∫ñÁ¢∫ÊÄßÊúÉÈÄ†ÊàêÈ¢®Èö™„ÄÇ‰∫ÜËß£Ë∂ÖÈü≥Ê≥¢ÂΩ±ÂÉè‰∏≠È†êÊ∏¨Ê®°ÂûãÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÔºåÂ∞çÊñºË©ï‰º∞ÂÖ∂ÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÂºïÈÄ≤ MSU-NetÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂ§öÈöéÊÆµÊñπÊ≥ïÔºåÁî®ÊñºË®ìÁ∑¥‰∏ÄÁµÑ U-Net ‰ª•Áî¢ÁîüÊ∫ñÁ¢∫ÁöÑË∂ÖÈü≥Ê≥¢ÂΩ±ÂÉèÂàÜÂâ≤Âúñ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ§ßÂπÖÊîπÂñÑÔºåÊØîÂñÆ‰∏ÄÁöÑËíôÂú∞Âç°ÁæÖ U-Net ÊîπÂñÑ‰∫Ü 18.1%ÔºåÂ¢ûÂº∑‰∫Ü‰∏çÁ¢∫ÂÆöÊÄßË©ï‰º∞„ÄÅÊ®°ÂûãÈÄèÊòéÂ∫¶ÂíåÂèØ‰ø°Â∫¶„ÄÇÈÄèÈÅéÂº∑Ë™øÊ®°ÂûãÁ¢∫ÂÆöÊÄßÁöÑÂçÄÂüüÔºåMSU-Net ÂèØ‰ª•ÂºïÂ∞éÂÆâÂÖ®ÁöÑÈáùÈ†≠ÊèíÂÖ•ÔºåËÆìÈùûÂ∞àÂÆ∂‰πüËÉΩÂü∑Ë°åÊ≠§È°û‰ªªÂãô„ÄÇ

##### **DEF-oriCORN: efficient 3D scene understanding for robust language-directed manipulation without demonstrations**
2407.21267v1 by Dongwon Son, Sanghyeon Son, Jaehyung Kim, Beomjoon Kim

We present DEF-oriCORN, a framework for language-directed manipulation tasks.
By leveraging a novel object-based scene representation and
diffusion-model-based state estimation algorithm, our framework enables
efficient and robust manipulation planning in response to verbal commands, even
in tightly packed environments with sparse camera views without any
demonstrations. Unlike traditional representations, our representation affords
efficient collision checking and language grounding. Compared to
state-of-the-art baselines, our framework achieves superior estimation and
motion planning performance from sparse RGB images and zero-shot generalizes to
real-world scenarios with diverse materials, including transparent and
reflective objects, despite being trained exclusively in simulation. Our code
for data generation, training, inference, and pre-trained weights are publicly
available at: https://sites.google.com/view/def-oricorn/home.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫ DEF-oriCORNÔºå‰∏ÄÂÄãÁî®ÊñºË™ûË®ÄÂ∞éÂêëÊìç‰Ωú‰ªªÂãôÁöÑÊ°ÜÊû∂„ÄÇ
ÈÄèÈÅéÂà©Áî®Êñ∞Á©éÁöÑÂü∫ÊñºÁâ©‰ª∂ÁöÑÂ†¥ÊôØË°®Á§∫Âíå
Âü∫ÊñºÊì¥Êï£Ê®°ÂûãÁöÑÁãÄÊÖã‰º∞Ë®àÊºîÁÆóÊ≥ïÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂ËÉΩÂ§†
ÊúâÊïà‰∏îÁ©©ÂÅ•Âú∞ÈÄ≤Ë°åÊìç‰ΩúË¶èÂäÉÔºå‰ª•ÂõûÊáâÂè£È†≠Êåá‰ª§ÔºåÂç≥‰Ωø
Âú®Á∑äÂØÜÂ∞ÅÈñâÁöÑÁí∞Â¢É‰∏≠Ôºå‰ΩøÁî®Á®ÄÁñèÁõ∏Ê©üË¶ñËßíÔºå‰∏îÁÑ°‰ªª‰Ωï
Á§∫ÁØÑ„ÄÇËàáÂÇ≥Áµ±Ë°®Á§∫Ê≥ï‰∏çÂêåÔºåÊàëÂÄëÁöÑË°®Á§∫Ê≥ïÊèê‰æõ
ÊúâÊïàÁöÑÁ¢∞ÊíûÊ™¢Êü•ÂíåË™ûË®ÄÂü∫Á§é„ÄÇËàá
ÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂ÂæûÁ®ÄÁñè RGB ÂΩ±ÂÉè‰∏≠Áç≤ÂæóÂÑ™Áï∞ÁöÑ‰º∞Ë®àÂíå
Âãï‰ΩúË¶èÂäÉÊïàËÉΩÔºå‰∏¶Âú®Èõ∂Ê¨°Â≠∏ÁøíÁöÑÊÉÖÊ≥Å‰∏ãÊ¶ÇÊã¨Âà∞
ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂêÑÁ®ÆÊùêÊñôÔºåÂåÖÊã¨ÈÄèÊòéÂíå
ÂèçÂ∞ÑÁâ©È´îÔºåÂÑòÁÆ°ÂÉÖÂú®Ê®°Êì¨‰∏≠ÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑË≥áÊñôÁî¢Áîü„ÄÅË®ìÁ∑¥„ÄÅÊé®Ë´ñÂíåÈ†êÂÖàË®ìÁ∑¥Ê¨äÈáçÁöÑÁ®ãÂºèÁ¢ºÂ∑≤ÂÖ¨ÈñãÊñºÔºöhttps://sites.google.com/view/def-oricorn/home„ÄÇ

##### **Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning**
2407.21264v1 by Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, Huan Liu

Model attribution for machine-generated disinformation poses a significant
challenge in understanding its origins and mitigating its spread. This task is
especially challenging because modern large language models (LLMs) produce
disinformation with human-like quality. Additionally, the diversity in
prompting methods used to generate disinformation complicates accurate source
attribution. These methods introduce domain-specific features that can mask the
fundamental characteristics of the models. In this paper, we introduce the
concept of model attribution as a domain generalization problem, where each
prompting method represents a unique domain. We argue that an effective
attribution model must be invariant to these domain-specific features. It
should also be proficient in identifying the originating models across all
scenarios, reflecting real-world detection challenges. To address this, we
introduce a novel approach based on Supervised Contrastive Learning. This
method is designed to enhance the model's robustness to variations in prompts
and focuses on distinguishing between different source LLMs. We evaluate our
model through rigorous experiments involving three common prompting methods:
``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:
``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the
effectiveness of our approach in model attribution tasks, achieving
state-of-the-art performance across diverse and unseen datasets.

ÊëòË¶ÅÔºöÊ©üÂô®Áî¢ÁîüÁöÑÈåØË™§Ë®äÊÅØÁöÑÊ®°ÂûãÊ≠∏Âõ†Â∞ç‰∫ÜËß£ÂÖ∂Ëµ∑Ê∫êÂíåÊ∏õËºïÂÖ∂ÂÇ≥Êí≠ÊßãÊàêÈáçÂ§ßÊåëÊà∞„ÄÇÈÄôÈ†Ö‰ªªÂãôÁâπÂà•ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫Áèæ‰ª£Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊúÉÁî¢ÁîüÂÖ∑ÊúâÈ°û‰∫∫ÂìÅË≥™ÁöÑÈåØË™§Ë®äÊÅØ„ÄÇÊ≠§Â§ñÔºåÁî®ÊñºÁî¢ÁîüÈåØË™§Ë®äÊÅØÁöÑÊèêÁ§∫ÊñπÊ≥ïÁöÑÂ§öÊ®£ÊÄß‰ΩøÂæóÊ∫ñÁ¢∫ÁöÑ‰æÜÊ∫êÊ≠∏Âõ†ËÆäÂæóË§áÈõú„ÄÇÈÄô‰∫õÊñπÊ≥ïÂºïÂÖ•‰∫ÜÁâπÂÆöÊñºÈ†òÂüüÁöÑÂäüËÉΩÔºåÈÄô‰∫õÂäüËÉΩÂèØËÉΩÊúÉÊé©ËìãÊ®°ÂûãÁöÑÂü∫Êú¨ÁâπÂæµ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞áÊ®°ÂûãÊ≠∏Âõ†ÁöÑÊ¶ÇÂøµÂºïÂÖ•ÁÇ∫È†òÂüüÊ¶ÇÂåñÂïèÈ°åÔºåÂÖ∂‰∏≠ÊØèÂÄãÊèêÁ§∫ÊñπÊ≥ï‰ª£Ë°®‰∏ÄÂÄãÁç®ÁâπÁöÑÈ†òÂüü„ÄÇÊàëÂÄëË™çÁÇ∫Ôºå‰∏ÄÂÄãÊúâÊïàÁöÑÊ≠∏Âõ†Ê®°ÂûãÂøÖÈ†à‰∏çËÆäÊñºÈÄô‰∫õÁâπÂÆöÊñºÈ†òÂüüÁöÑÂäüËÉΩ„ÄÇÂÆÉÈÇÑÊáâË©≤ËÉΩÂ§†Ë≠òÂà•ÊâÄÊúâÂ†¥ÊôØ‰∏≠ÁöÑÂéüÂßãÊ®°ÂûãÔºåÂèçÊò†ÁèæÂØ¶‰∏ñÁïåÁöÑÂÅµÊ∏¨ÊåëÊà∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÁõ£Áù£Â∞çÊØîÂ≠∏ÁøíÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÊ≠§ÊñπÊ≥ïÊó®Âú®Â¢ûÂº∑Ê®°ÂûãÂ∞çÊèêÁ§∫ËÆäÂåñÁöÑÈ≠ØÊ£íÊÄßÔºå‰∏¶Â∞àÊ≥®ÊñºÂçÄÂàÜ‰∏çÂêåÁöÑ‰æÜÊ∫ê LLM„ÄÇÊàëÂÄëÈÄèÈÅéÊ∂âÂèä‰∏âÁ®ÆÂ∏∏Ë¶ãÊèêÁ§∫ÊñπÊ≥ïÁöÑÂö¥Ê†ºÂØ¶È©ó‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÊ®°ÂûãÔºö``ÈñãÊîæÂºè''„ÄÅ``ÈáçÂØ´''Âíå``ÊîπÂØ´''Ôºå‰ª•Âèä‰∏âÁ®ÆÈÄ≤Èöé LLMÔºö``llama 2''„ÄÅ``chatgpt''Âíå``vicuna''„ÄÇÊàëÂÄëÁöÑÁµêÊûúË≠âÊòé‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Ê®°ÂûãÊ≠∏Âõ†‰ªªÂãô‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÂú®Â§öÊ®£Âåñ‰∏îÊú™Ë¶ãÁöÑË≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ

##### **Lifelong Person Search**
2407.21252v1 by Jae-Won Yang, Seungbin Hong, Jae-Young Sim

Person search is the task to localize a query person in gallery datasets of
scene images. Existing methods have been mainly developed to handle a single
target dataset only, however diverse datasets are continuously given in
practical applications of person search. In such cases, they suffer from the
catastrophic knowledge forgetting in the old datasets when trained on new
datasets. In this paper, we first introduce a novel problem of lifelong person
search (LPS) where the model is incrementally trained on the new datasets while
preserving the knowledge learned in the old datasets. We propose an end-to-end
LPS framework that facilitates the knowledge distillation to enforce the
consistency learning between the old and new models by utilizing the prototype
features of the foreground persons as well as the hard background proposals in
the old domains. Moreover, we also devise the rehearsal-based instance matching
to further improve the discrimination ability in the old domains by using the
unlabeled person instances additionally. Experimental results demonstrate that
the proposed method achieves significantly superior performance of both the
detection and re-identification to preserve the knowledge learned in the old
domains compared with the existing methods.

ÊëòË¶ÅÔºö‰∫∫Áâ©ÊêúÂ∞ãÊòØÂ∞áÊü•Ë©¢‰∫∫Áâ©ÂÆö‰ΩçÂú®Â†¥ÊôØÂΩ±ÂÉèÁöÑË≥áÊñôÂ∫´Ë≥áÊñôÈõÜ‰∏≠ÁöÑ‰ªªÂãô„ÄÇÁèæÊúâÊñπÊ≥ï‰∏ªË¶ÅÈñãÁôºÁÇ∫ÂÉÖËôïÁêÜÂñÆ‰∏ÄÁõÆÊ®ôË≥áÊñôÈõÜÔºåÁÑ∂ËÄåÂú®‰∫∫Áâ©ÊêúÂ∞ãÁöÑÂØ¶ÈöõÊáâÁî®‰∏≠ÔºåÊúÉÊåÅÁ∫åÊèê‰æõÂ§öÊ®£ÂåñÁöÑË≥áÊñôÈõÜ„ÄÇÂú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÁï∂Âú®Êñ∞ÁöÑË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÊôÇÔºåËàäË≥áÊñôÈõÜÊúÉÈÅ≠ÂèóÁÅΩÈõ£ÊÄßÁöÑÁü•Ë≠òÈÅ∫Âøò„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖà‰ªãÁ¥πÁµÇË∫´‰∫∫Áâ©ÊêúÂ∞ã (LPS) ÁöÑÊñ∞ÂïèÈ°åÔºåÂÖ∂‰∏≠Ê®°ÂûãÊúÉÈÄêÊ≠•Âú®Êñ∞ÁöÑË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÔºåÂêåÊôÇ‰øùÁïôÂú®ËàäË≥áÊñôÈõÜ‰∏≠Â≠¶Âà∞ÁöÑÁü•Ë≠ò„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ´ØÂ∞çÁ´ØÁöÑ LPS Êû∂ÊßãÔºåÈÄèÈÅéÂà©Áî®ÂâçÊôØ‰∫∫Áâ©ÁöÑÂéüÂûãÁâπÂæµÂíåËàäÁ∂≤Âüü‰∏≠ÁöÑÁ°¨ËÉåÊôØÊèêÊ°àÔºå‰øÉÈÄ≤Áü•Ë≠òËêÉÂèñÔºå‰ª•Âº∑Âà∂ËàäÊ®°ÂûãÂíåÊñ∞Ê®°Âûã‰πãÈñìÁöÑ‰∏ÄËá¥ÊÄßÂ≠∏Áøí„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑË®≠Ë®à‰∫ÜÂü∫ÊñºÂΩ©ÊéíÁöÑÂØ¶‰æãÂåπÈÖçÔºå‰ª•ÈÄèÈÅéÂè¶Â§ñ‰ΩøÁî®Êú™Ê®ôË®òÁöÑ‰∫∫Áâ©ÂØ¶‰æãÔºåÈÄ≤‰∏ÄÊ≠•ÊèêÂçáËàäÁ∂≤Âüü‰∏≠ÁöÑËæ®Âà•ËÉΩÂäõ„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòéÔºåËàáÁèæÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÂÅµÊ∏¨ÂíåÂÜçËæ®Ë≠òÊñπÈù¢ÈÉΩÁç≤ÂæóÈ°ØËëóÁöÑÂÑ™Áï∞ÊïàËÉΩÔºå‰ª•‰øùÁïôÂú®ËàäÁ∂≤Âüü‰∏≠Â≠∏Âà∞ÁöÑÁü•Ë≠ò„ÄÇ

##### **Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens**
2407.21248v1 by Anqi Zhang, Chaofeng Wu

While large language models (LLMs) are extensively used, there are raising
concerns regarding privacy, security, and copyright due to their opaque
training data, which brings the problem of detecting pre-training data on the
table. Current solutions to this problem leverage techniques explored in
machine learning privacy such as Membership Inference Attacks (MIAs), which
heavily depend on LLMs' capability of verbatim memorization. However, this
reliance presents challenges, especially given the vast amount of training data
and the restricted number of effective training epochs. In this paper, we
propose an adaptive pre-training data detection method which alleviates this
reliance and effectively amplify the identification. Our method adaptively
locates \textit{surprising tokens} of the input. A token is surprising to a LLM
if the prediction on the token is "certain but wrong", which refers to low
Shannon entropy of the probability distribution and low probability of the
ground truth token at the same time. By using the prediction probability of
surprising tokens to measure \textit{surprising}, the detection method is
achieved based on the simple hypothesis that seeing seen data is less
surprising for the model compared with seeing unseen data. The method can be
applied without any access to the the pre-training data corpus or additional
training like reference models. Our approach exhibits a consistent enhancement
compared to existing methods in diverse experiments conducted on various
benchmarks and models, achieving a maximum improvement of 29.5\%. We also
introduce a new benchmark Dolma-Book developed upon a novel framework, which
employs book data collected both before and after model training to provide
further evaluation.

ÊëòË¶ÅÔºö<paragraph>ÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë¢´Âª£Ê≥õ‰ΩøÁî®Ôºå‰ΩÜÁî±ÊñºÂÆÉÂÄë‰∏çÈÄèÊòéÁöÑË®ìÁ∑¥Ë≥áÊñôÔºåÂ∞çÊñºÈö±ÁßÅ„ÄÅÂÆâÂÖ®ÊÄßÔºå‰ª•ÂèäÁâàÊ¨äÁöÑÂïèÈ°åÊ≠£ÈÄêÊº∏ÊµÆÁèæÔºåÈÄô‰πüÂ∏∂‰æÜ‰∫ÜÂú®Ë°®Ê†º‰∏≠ÂÅµÊ∏¨È†êË®ìÁ∑¥Ë≥áÊñôÁöÑÂïèÈ°å„ÄÇÁõÆÂâçËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÁöÑÊñπÊ≥ïÔºåÂà©Áî®‰∫ÜÊ©üÂô®Â≠∏ÁøíÈö±ÁßÅ‰∏≠ÊâÄÊé¢Ë®éÁöÑÊäÄË°ìÔºå‰æãÂ¶ÇÊàêÂì°Ë∫´ÂàÜÊé®Ë´ñÊîªÊìä (MIA)ÔºåÈÄôÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰æùË≥¥Êñº LLM ÁöÑÈÄêÂ≠óË®òÊÜ∂ËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®Æ‰æùË≥¥ÊÄßÂ∏∂‰æÜ‰∫ÜÊåëÊà∞ÔºåÁâπÂà•ÊòØËÄÉÈáèÂà∞Â§ßÈáèÁöÑË®ìÁ∑¥Ë≥áÊñôÂíåÂèóÈôêÁöÑÊúâÊïàË®ìÁ∑¥Ëº™Ê¨°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÈÅ©ÊáâÊÄßÈ†êË®ìÁ∑¥Ë≥áÊñôÂÅµÊ∏¨ÊñπÊ≥ïÔºåÂèØ‰ª•Ê∏õËºïÈÄôÁ®Æ‰æùË≥¥ÊÄßÔºå‰∏¶ÊúâÊïàÂú∞Êì¥Â§ßË≠òÂà•ÁØÑÂúç„ÄÇÊàëÂÄëÁöÑÈÅ©ÊáâÊÄßÊñπÊ≥ïÂÆö‰ΩçËº∏ÂÖ•ÁöÑ„Äå‰ª§‰∫∫È©öË®ùÁöÑÁ¨¶Ëôü„Äç„ÄÇÂ¶ÇÊûúÁ¨¶ËôüÁöÑÈ†êÊ∏¨ÊòØ„ÄåÁ¢∫ÂÆö‰ΩÜÈåØË™§„ÄçÔºåÂâáË©≤Á¨¶ËôüÂ∞ç LLM ‰æÜË™™ÊòØ‰ª§‰∫∫È©öË®ùÁöÑÔºåÈÄôÊåáÁöÑÊòØÊ©üÁéáÂàÜ‰ΩàÁöÑÈ¶ôËæ≤ÁÜµ‰ΩéÔºå‰∏îÂêåÊôÇÂÖ∑Êúâ‰ΩéÊñºÁúüÂØ¶Á¨¶ËôüÁöÑÊ©üÁéá„ÄÇÈÄèÈÅé‰ΩøÁî®‰ª§‰∫∫È©öË®ùÁöÑÁ¨¶ËôüÁöÑÈ†êÊ∏¨Ê©üÁéá‰æÜË°°Èáè„Äå‰ª§‰∫∫È©öË®ù„ÄçÔºåÂÅµÊ∏¨ÊñπÊ≥ïÊòØÂü∫Êñº‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÂÅáË®≠ÔºåËàáÁúãÂà∞Êú™Ë¶ãÈÅéÁöÑË≥áÊñôÁõ∏ÊØîÔºåÁúãÂà∞Â∑≤Ë¶ãÈÅéÁöÑË≥áÊñôÂ∞çÊñºÊ®°Âûã‰æÜË™™ËºÉ‰∏ç‰ª§‰∫∫È©öË®ù„ÄÇÊ≠§ÊñπÊ≥ïÂèØ‰ª•ÊáâÁî®ÔºåËÄåÁÑ°ÈúÄÂ≠òÂèñÈ†êË®ìÁ∑¥Ë≥áÊñôË™ûÊñôÂ∫´ÊàñÈ°çÂ§ñÁöÑË®ìÁ∑¥Ôºå‰æãÂ¶ÇÂèÉËÄÉÊ®°Âûã„ÄÇËàáÂú®ÂêÑÁ®ÆÂü∫Ê∫ñÂíåÊ®°Âûã‰∏äÈÄ≤Ë°åÁöÑ‰∏çÂêåÂØ¶È©ó‰∏≠ÁèæÊúâÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ±ïÁèæ‰∫Ü‰∏ÄËá¥ÁöÑÈÄ≤Ê≠•ÔºåÈÅîÂà∞‰∫Ü 29.5% ÁöÑÊúÄÂ§ßÊîπÈÄ≤„ÄÇÊàëÂÄëÈÇÑÂºïÂÖ•‰∫ÜÂü∫ÊñºÊñ∞Êû∂ÊßãÈñãÁôºÁöÑÊñ∞Âü∫Ê∫ñ Dolma-BookÔºåÂÆÉÊé°Áî®Âú®Ê®°ÂûãË®ìÁ∑¥ÂâçÂæåÊî∂ÈõÜÁöÑÊõ∏Á±çË≥áÊñô‰æÜÊèê‰æõÈÄ≤‰∏ÄÊ≠•ÁöÑË©ï‰º∞„ÄÇ</paragraph>

##### **Informed Correctors for Discrete Diffusion Models**
2407.21243v1 by Yixiu Zhao, Jiaxin Shi, Lester Mackey, Scott Linderman

Discrete diffusion modeling is a promising framework for modeling and
generating data in discrete spaces. To sample from these models, different
strategies present trade-offs between computation and sample quality. A
predominant sampling strategy is predictor-corrector $\tau$-leaping, which
simulates the continuous time generative process with discretized predictor
steps and counteracts the accumulation of discretization error via corrector
steps. However, for absorbing state diffusion, an important class of discrete
diffusion models, the standard forward-backward corrector can be ineffective in
fixing such errors, resulting in subpar sample quality. To remedy this problem,
we propose a family of informed correctors that more reliably counteracts
discretization error by leveraging information learned by the model. For
further efficiency gains, we also propose $k$-Gillespie's, a sampling algorithm
that better utilizes each model evaluation, while still enjoying the speed and
flexibility of $\tau$-leaping. Across several real and synthetic datasets, we
show that $k$-Gillespie's with informed correctors reliably produces higher
quality samples at lower computational cost.

ÊëòË¶ÅÔºöÈõ¢Êï£Êì¥Êï£Ê®°ÂûãÊòØ‰∏ÄÁ®ÆÁî®ÊñºÂ∞çÈõ¢Êï£Á©∫Èñì‰∏≠ÁöÑË≥áÊñôÈÄ≤Ë°åÂª∫Ê®°ÂíåÁîüÊàêÁöÑÂæàÊúâÂâçÊôØÁöÑÊ°ÜÊû∂„ÄÇÁÇ∫‰∫ÜÂæûÈÄô‰∫õÊ®°Âûã‰∏≠ÂèñÊ®£Ôºå‰∏çÂêåÁöÑÁ≠ñÁï•Âú®ÈÅãÁÆóÂíåÊ®£Êú¨ÂìÅË≥™‰πãÈñìÈÄ≤Ë°åÊ¨äË°°„ÄÇ‰∏ÄÁ®Æ‰∏ªË¶ÅÁöÑÂèñÊ®£Á≠ñÁï•ÊòØÈ†êÊ∏¨Ê†°Ê≠£ $\tau$-Ë∑≥Ë∫çÔºåÂÆÉ‰ΩøÁî®Èõ¢Êï£ÂåñÈ†êÊ∏¨Ê≠•È©üÊ®°Êì¨ÈÄ£Á∫åÊôÇÈñìÁîüÊàêÈÅéÁ®ãÔºå‰∏¶ÈÄèÈÅéÊ†°Ê≠£Ê≠•È©üÊäµÊ∂àÈõ¢Êï£ÂåñË™§Â∑ÆÁöÑÁ¥ØÁ©ç„ÄÇÁÑ∂ËÄåÔºåÂ∞çÊñºÂê∏Êî∂ÁãÄÊÖãÊì¥Êï£ÔºàÈõ¢Êï£Êì¥Êï£Ê®°Âûã‰∏≠ÁöÑ‰∏ÄÂÄãÈáçË¶ÅÈ°ûÂà•ÔºâÔºåÊ®ôÊ∫ñÁöÑÂâçÂêëÂæåÂêëÊ†°Ê≠£Âô®Âú®‰øÆÊ≠£Ê≠§È°ûË™§Â∑ÆÊôÇÂèØËÉΩÁÑ°ÊïàÔºåÂ∞éËá¥Ê¨°‰Ω≥ÁöÑÊ®£Êú¨ÂìÅË≥™„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ≥ªÂàóÁöÑÁü•ÊÉÖÊ†°Ê≠£Âô®ÔºåÈÄô‰∫õÊ†°Ê≠£Âô®ÈÄèÈÅéÂà©Áî®Ê®°ÂûãÂ≠∏ÁøíÂà∞ÁöÑË≥áË®äÔºåÊõ¥ÂèØÈù†Âú∞ÊäµÊ∂àÈõ¢Êï£ÂåñË™§Â∑Æ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÊïàÁéáÔºåÊàëÂÄëÈÇÑÊèêÂá∫‰∫Ü $k$-GillespieÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂèñÊ®£ÊºîÁÆóÊ≥ïÔºåÂÆÉËÉΩÊõ¥Â•ΩÂú∞Âà©Áî®ÊØèÂÄãÊ®°ÂûãË©ï‰º∞ÔºåÂêåÊôÇ‰ªç‰∫´Êúâ $\tau$-Ë∑≥Ë∫çÁöÑÈÄüÂ∫¶ÂíåÈùàÊ¥ªÊÄß„ÄÇÂú®ÂπæÂÄãÁúüÂØ¶ÂíåÂêàÊàêË≥áÊñôÈõÜ‰∏äÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰ΩøÁî®Áü•ÊÉÖÊ†°Ê≠£Âô®ÁöÑ $k$-Gillespie ËÉΩÂèØÈù†Âú∞‰ª•ËºÉ‰ΩéÁöÑÈÅãÁÆóÊàêÊú¨Áî¢ÁîüËºÉÈ´òÂìÅË≥™ÁöÑÊ®£Êú¨„ÄÇ

##### **Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration**
2407.21229v1 by Ngoc Son Nguyen, Van Son Nguyen, Tung Le

Visual Question Answering (VQA) has recently emerged as a potential research
domain, captivating the interest of many in the field of artificial
intelligence and computer vision. Despite the prevalence of approaches in
English, there is a notable lack of systems specifically developed for certain
languages, particularly Vietnamese. This study aims to bridge this gap by
conducting comprehensive experiments on the Vietnamese Visual Question
Answering (ViVQA) dataset, demonstrating the effectiveness of our proposed
model. In response to community interest, we have developed a model that
enhances image representation capabilities, thereby improving overall
performance in the ViVQA system. Specifically, our model integrates the
Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2)
and the convolutional neural network EfficientNet to extract and process both
local and global features from images. This integration leverages the strengths
of transformer-based architectures for capturing comprehensive contextual
information and convolutional networks for detailed local features. By freezing
the parameters of these pre-trained models, we significantly reduce the
computational cost and training time, while maintaining high performance. This
approach significantly improves image representation and enhances the
performance of existing VQA systems. We then leverage a multi-modal fusion
module based on a general-purpose multi-modal foundation model (BEiT-3) to fuse
the information between visual and textual features. Our experimental findings
demonstrate that our model surpasses competing baselines, achieving promising
performance. This is particularly evident in its accuracy of $71.04\%$ on the
test set of the ViVQA dataset, marking a significant advancement in our
research area. The code is available at https://github.com/nngocson2002/ViVQA.

ÊëòË¶ÅÔºöË¶ñË¶∫ÂïèÁ≠î (VQA) ÊúÄËøëÂ∑≤ÊàêÁÇ∫ÊΩõÂú®ÁöÑÁ†îÁ©∂È†òÂüüÔºåÂê∏Âºï‰∫ÜË®±Â§ö‰∫∫Â∑•Êô∫ÊÖßÂíåÈõªËÖ¶Ë¶ñË¶∫È†òÂüüÁöÑÁ†îÁ©∂‰∫∫Âì°„ÄÇÂÑòÁÆ°Ëã±Ë™û‰∏≠ÊúâË®±Â§öÊñπÊ≥ïÔºå‰ΩÜÈ°ØËëóÁº∫‰πèÂ∞àÈñÄÁÇ∫ÁâπÂÆöË™ûË®ÄÔºàÂ∞§ÂÖ∂ÊòØË∂äÂçóË™ûÔºâÈñãÁôºÁöÑÁ≥ªÁµ±„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÈÄèÈÅéÂ∞çË∂äÂçóË¶ñË¶∫ÂïèÁ≠î (ViVQA) Ë≥áÊñôÈõÜÈÄ≤Ë°åÂÖ®Èù¢ÂØ¶È©óÔºåÂ±ïÁ§∫ÊàëÂÄëÊèêÂá∫ÁöÑÊ®°ÂûãÁöÑÊúâÊïàÊÄßÔºå‰ª•ÂΩåË£úÈÄô‰∏ÄÂ∑ÆË∑ù„ÄÇÁÇ∫‰∫ÜÂõûÊáâÁ§æÁæ§ÁöÑËààË∂£ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãÊ®°ÂûãÔºåÂ¢ûÂº∑‰∫ÜÂΩ±ÂÉèË°®ÂæµËÉΩÂäõÔºåÂæûËÄåÊîπÂñÑ‰∫Ü ViVQA Á≥ªÁµ±ÁöÑÊï¥È´îÊïàËÉΩ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊï¥Âêà‰∫ÜÂáçÁµêÂñÆÊ®°ÊÖãÊ®°Âûã (BLIP-2) ÁöÑÂºïÂ∞éË™ûË®ÄÂΩ±ÂÉèÈ†êË®ìÁ∑¥ÂíåÂç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø EfficientNetÔºå‰ª•ÂæûÂΩ±ÂÉè‰∏≠ÊèêÂèñÂíåËôïÁêÜÂ±ÄÈÉ®ÂíåÂÖ®ÂüüÁâπÂæµ„ÄÇÈÄôÁ®ÆÊï¥ÂêàÂà©Áî®‰∫ÜÂü∫Êñº Transformer ÁöÑÊû∂Êßã‰æÜÊì∑ÂèñÂÖ®Èù¢ÁöÑËÑàÁµ°Ë≥áË®äÔºå‰ª•ÂèäÂç∑Á©çÁ∂≤Ë∑Ø‰æÜÊì∑ÂèñË©≥Á¥∞ÁöÑÂ±ÄÈÉ®ÁâπÂæµ„ÄÇÈÄèÈÅéÂáçÁµêÈÄô‰∫õÈ†êË®ìÁ∑¥Ê®°ÂûãÁöÑÂèÉÊï∏ÔºåÊàëÂÄëÂ§ßÂπÖÈôç‰Ωé‰∫ÜÈÅãÁÆóÊàêÊú¨ÂíåË®ìÁ∑¥ÊôÇÈñìÔºåÂêåÊôÇÁ∂≠ÊåÅÈ´òÊïàËÉΩ„ÄÇÈÄôÁ®ÆÊñπÊ≥ïÈ°ØËëóÊîπÂñÑ‰∫ÜÂΩ±ÂÉèË°®ÂæµÔºå‰∏¶Â¢ûÂº∑‰∫ÜÁèæÊúâ VQA Á≥ªÁµ±ÁöÑÊïàËÉΩ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂà©Áî®Âü∫ÊñºÈÄöÁî®Â§öÊ®°ÊÖãÂü∫Á§éÊ®°Âûã (BEiT-3) ÁöÑÂ§öÊ®°ÊÖãËûçÂêàÊ®°ÁµÑÔºåËûçÂêàË¶ñË¶∫ÂíåÊñáÂ≠óÁâπÂæµ‰πãÈñìÁöÑË≥áË®ä„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãË∂ÖË∂ä‰∫ÜÁ´∂Áà≠Âü∫Ê∫ñÔºåÈÅîÂà∞‰∫Ü‰ª§‰∫∫ÊªøÊÑèÁöÑÊïàËÉΩ„ÄÇÈÄôÂú® ViVQA Ë≥áÊñôÈõÜÁöÑÊ∏¨Ë©¶ÈõÜ‰∏≠Ê∫ñÁ¢∫ÁéáÈÅî 71.04% ‰∏≠ÁâπÂà•ÊòéÈ°ØÔºåÊ®ôË™åËëóÊàëÂÄëÁ†îÁ©∂È†òÂüüÁöÑÈáçÂ§ßÈÄ≤Â±ï„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/nngocson2002/ViVQA ÂèñÂæó„ÄÇ

##### **Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models**
2407.21227v1 by Florian Tambon, Amin Nikanjam, Foutse Khomh, Giuliano Antoniol

Large Language Models (LLMs) show promising potential in Software
Engineering, especially for code-related tasks like code completion and code
generation. LLMs' evaluation is generally centred around general metrics
computed over benchmarks. While painting a macroscopic view of the benchmarks
and of the LLMs' capacity, it is unclear how each programming task in these
benchmarks assesses the capabilities of the LLMs. In particular, the difficulty
level of the tasks in the benchmarks is not reflected in the score used to
report the performance of the model. Yet, a model achieving a 90% score on a
benchmark of predominantly easy tasks is likely less capable than a model
achieving a 90% score on a benchmark containing predominantly difficult tasks.
This paper devises a framework, HardEval, for assessing task difficulty for
LLMs and crafting new tasks based on identified hard tasks. The framework uses
a diverse array of prompts for a single task across multiple LLMs to obtain a
difficulty score for each task of a benchmark. Using two code generation
benchmarks, HumanEval+ and ClassEval, we show that HardEval can reliably
identify the hard tasks within those benchmarks, highlighting that only 21% of
HumanEval+ and 27% of ClassEval tasks are hard for LLMs. Through our analysis
of task difficulty, we also characterize 6 practical hard task topics which we
used to generate new hard tasks. Orthogonal to current benchmarking evaluation
efforts, HardEval can assist researchers and practitioners in fostering better
assessments of LLMs. The difficulty score can be used to identify hard tasks
within existing benchmarks. This, in turn, can be leveraged to generate more
hard tasks centred around specific topics either for evaluation or improvement
of LLMs. HardEval generalistic approach can be applied to other domains such as
code completion or Q/A.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ËªüÈ´îÂ∑•Á®ã‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫ÊúüÂæÖÁöÑÊΩõÂäõÔºåÁâπÂà•ÊòØÂú®ËàáÁ®ãÂºèÁ¢ºÁõ∏ÈóúÁöÑ‰ªªÂãô‰∏≠Ôºå‰æãÂ¶ÇÁ®ãÂºèÁ¢ºÂÆåÊàêÂíåÁ®ãÂºèÁ¢ºÁîüÊàê„ÄÇLLM ÁöÑË©ï‰º∞ÈÄöÂ∏∏ÈõÜ‰∏≠Âú®Âü∫Ê∫ñ‰∏äË®àÁÆóÂá∫ÁöÑÈÄöÁî®ÊåáÊ®ô„ÄÇÈõñÁÑ∂ÊèèÁπ™Âá∫Âü∫Ê∫ñÂíå LLM ËÉΩÂäõÁöÑÂ∑®ËßÄËßÄÈªûÔºå‰ΩÜÁõÆÂâçÂ∞ö‰∏çÊ∏ÖÊ•öÈÄô‰∫õÂü∫Ê∫ñ‰∏≠ÁöÑÊØèÂÄãÁ®ãÂºèË®≠Ë®à‰ªªÂãôÂ¶Ç‰ΩïË©ï‰º∞ LLM ÁöÑËÉΩÂäõ„ÄÇÁâπÂà•ÊòØÔºåÂü∫Ê∫ñ‰∏≠‰ªªÂãôÁöÑÈõ£Â∫¶Á≠âÁ¥ö‰∏¶Êú™ÂèçÊò†Âú®Áî®ÊñºÂ†±ÂëäÊ®°ÂûãÊïàËÉΩÁöÑÂàÜÊï∏‰∏≠„ÄÇÁÑ∂ËÄåÔºåÂú®‰∏ªË¶ÅÁî±ÂÆπÊòì‰ªªÂãôÁµÑÊàêÁöÑÂü∫Ê∫ñ‰∏äÈÅîÂà∞ 90% ÂàÜÊï∏ÁöÑÊ®°ÂûãÔºåÂÖ∂ËÉΩÂäõÂèØËÉΩ‰ΩéÊñºÂú®‰∏ªË¶ÅÁî±Âõ∞Èõ£‰ªªÂãôÁµÑÊàêÁöÑÂü∫Ê∫ñ‰∏äÈÅîÂà∞ 90% ÂàÜÊï∏ÁöÑÊ®°Âûã„ÄÇÊú¨ÊñáË®≠Ë®à‰∫Ü‰∏ÄÂÄãÊû∂Êßã HardEvalÔºåÁî®ÊñºË©ï‰º∞ LLM ÁöÑ‰ªªÂãôÈõ£Â∫¶Ôºå‰∏¶Ê†πÊìöË≠òÂà•Âá∫ÁöÑÂõ∞Èõ£‰ªªÂãôÂª∫Á´ãÊñ∞‰ªªÂãô„ÄÇË©≤Êû∂ÊßãÈáùÂ∞çÂñÆ‰∏Ä‰ªªÂãô‰ΩøÁî®Â§öÂÄã LLM ÁöÑÂêÑÁ®ÆÊèêÁ§∫Ôºå‰ª•ÂèñÂæóÂü∫Ê∫ñ‰∏≠ÊØèÂÄã‰ªªÂãôÁöÑÈõ£Â∫¶ÂàÜÊï∏„ÄÇ‰ΩøÁî®ÂÖ©ÂÄãÁ®ãÂºèÁ¢ºÁîüÊàêÂü∫Ê∫ñ HumanEval+ Âíå ClassEvalÔºåÊàëÂÄëÂ±ïÁ§∫ HardEval ÂèØ‰ª•ÂèØÈù†Âú∞Ë≠òÂà•ÈÄô‰∫õÂü∫Ê∫ñ‰∏≠ÁöÑÂõ∞Èõ£‰ªªÂãôÔºå‰∏¶Âº∑Ë™øÂè™Êúâ 21% ÁöÑ HumanEval+ Âíå 27% ÁöÑ ClassEval ‰ªªÂãôÂ∞ç LLM ‰æÜË™™ÊòØÂõ∞Èõ£ÁöÑ„ÄÇÈÄèÈÅéÂ∞ç‰ªªÂãôÈõ£Â∫¶ÁöÑÂàÜÊûêÔºåÊàëÂÄëÈÇÑÊèèËø∞‰∫Ü 6 ÂÄãÂØ¶ÈöõÁöÑÂõ∞Èõ£‰ªªÂãô‰∏ªÈ°åÔºåÊàëÂÄëÁî®ÈÄô‰∫õ‰∏ªÈ°å‰æÜÁî¢ÁîüÊñ∞ÁöÑÂõ∞Èõ£‰ªªÂãô„ÄÇËàáÁõÆÂâçÁöÑÂü∫Ê∫ñË©ï‰º∞Â∑•‰ΩúÊ≠£‰∫§ÔºåHardEval ÂèØ‰ª•ÂçîÂä©Á†îÁ©∂‰∫∫Âì°ÂíåÂæûÊ•≠‰∫∫Âì°‰øÉÈÄ≤Â∞ç LLM ÁöÑÊõ¥‰Ω≥Ë©ï‰º∞„ÄÇÈõ£Â∫¶ÂàÜÊï∏ÂèØÁî®ÊñºË≠òÂà•ÁèæÊúâÂü∫Ê∫ñ‰∏≠ÁöÑÂõ∞Èõ£‰ªªÂãô„ÄÇÂèçÈÅé‰æÜÔºåÈÄôÂèØ‰ª•Áî®ÊñºÁî¢ÁîüÊõ¥Â§öÂúçÁπûÁâπÂÆö‰∏ªÈ°åÁöÑÂõ∞Èõ£‰ªªÂãôÔºå‰ª•Áî®ÊñºË©ï‰º∞ÊàñÊîπÈÄ≤ LLM„ÄÇHardEval ÁöÑÈÄöÁî®ÊñπÊ≥ïÂèØ‰ª•ÊáâÁî®ÊñºÂÖ∂‰ªñÈ†òÂüüÔºå‰æãÂ¶ÇÁ®ãÂºèÁ¢ºÂÆåÊàêÊàñÂïèÁ≠î„ÄÇ

##### **AI methods for approximate compiling of unitaries**
2407.21225v1 by David Kremer, Victor Villar, Sanjay Vishwakarma, Ismael Faro, Juan Cruz-Benito

This paper explores artificial intelligence (AI) methods for the approximate
compiling of unitaries, focusing on the use of fixed two-qubit gates and
arbitrary single-qubit rotations typical in superconducting hardware. Our
approach involves three main stages: identifying an initial template that
approximates the target unitary, predicting initial parameters for this
template, and refining these parameters to maximize the fidelity of the
circuit. We propose AI-driven approaches for the first two stages, with a deep
learning model that suggests initial templates and an autoencoder-like model
that suggests parameter values, which are refined through gradient descent to
achieve the desired fidelity. We demonstrate the method on 2 and 3-qubit
unitaries, showcasing promising improvements over exhaustive search and random
parameter initialization. The results highlight the potential of AI to enhance
the transpiling process, supporting more efficient quantum computations on
current and future quantum hardware.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫Ü‰∫∫Â∑•Êô∫ÊÖß (AI) ÊñπÊ≥ïÔºåÁî®ÊñºËøë‰ººÁ∑®Ë≠ØÈÖâÁÆóÂ≠êÔºåÈáçÈªûÂú®Êñº‰ΩøÁî®Ë∂ÖÂ∞éÁ°¨È´î‰∏≠Â∏∏Ë¶ãÁöÑÂõ∫ÂÆöÈõô‰ΩçÂÖÉÈñòÂíå‰ªªÊÑèÂñÆ‰ΩçÂÖÉÊóãËΩâ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊ∂âÂèä‰∏âÂÄã‰∏ªË¶ÅÈöéÊÆµÔºöÊâæÂá∫Ëøë‰ººÁõÆÊ®ôÈÖâÁÆóÂ≠êÁöÑÂàùÂßãÁØÑÊú¨„ÄÅÈ†êÊ∏¨Ê≠§ÁØÑÊú¨ÁöÑÂàùÂßãÂèÉÊï∏Ôºå‰ª•ÂèäÂæÆË™øÈÄô‰∫õÂèÉÊï∏‰ª•ÊúÄÂ§ßÂåñÈõªË∑ØÁöÑ‰øùÁúüÂ∫¶„ÄÇÊàëÂÄëÊèêÂá∫ÈáùÂ∞çÂâçÂÖ©ÂÄãÈöéÊÆµÁöÑ AI È©ÖÂãïÊñπÊ≥ïÔºå‰ΩøÁî®Ê∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÂª∫Ë≠∞ÂàùÂßãÁØÑÊú¨Ôºå‰ª•ÂèäÈ°û‰ººËá™ÂãïÁ∑®Á¢ºÂô®ÁöÑÊ®°ÂûãÂª∫Ë≠∞ÂèÉÊï∏ÂÄºÔºåÈÄèÈÅéÊ¢ØÂ∫¶‰∏ãÈôçÂæÆË™øÈÄô‰∫õÂèÉÊï∏‰ª•ÈÅîÂà∞ÊâÄÈúÄÁöÑ‰øùÁúüÂ∫¶„ÄÇÊàëÂÄëÂú® 2 Âíå 3 ‰ΩçÂÖÉÈÖâÁÆóÂ≠ê‰∏äÂ±ïÁ§∫Ê≠§ÊñπÊ≥ïÔºåÂ±ïÁ§∫Âá∫ÊØîÁ™ÆËàâÊêúÂ∞ãÂíåÈö®Ê©üÂèÉÊï∏ÂàùÂßãÂåñÊõ¥Â•ΩÁöÑÊîπÈÄ≤„ÄÇÁµêÊûúÁ™ÅÈ°Ø‰∫Ü AI Â¢ûÂº∑ËΩâË≠ØÈÅéÁ®ãÁöÑÊΩõÂäõÔºåÊîØÊè¥Âú®ÁèæÊúâÂíåÊú™‰æÜÁöÑÈáèÂ≠êÁ°¨È´î‰∏äÈÄ≤Ë°åÊõ¥ÊúâÊïàÁéáÁöÑÈáèÂ≠êÈÅãÁÆó„ÄÇ

##### **LoRaWAN Based Dynamic Noise Mapping with Machine Learning for Urban Noise Enforcement**
2407.21204v1 by H. Emre Erdem, Henry Leung

Static noise maps depicting long-term noise levels over wide areas are
valuable urban planning assets for municipalities in decreasing noise exposure
of residents. However, non-traffic noise sources with transient behavior, which
people complain frequently, are usually ignored by static maps. We propose here
a dynamic noise mapping approach using the data collected via low-power
wide-area network (LPWAN, specifically LoRaWAN) based internet of things (IoT)
infrastructure, which is one of the most common communication backbones for
smart cities. Noise mapping based on LPWAN is challenging due to the low data
rates of these protocols. The proposed dynamic noise mapping approach
diminishes the negative implications of data rate limitations using machine
learning (ML) for event and location prediction of non-traffic sources based on
the scarce data. The strength of these models lies in their consideration of
the spatial variance in acoustic behavior caused by the buildings in urban
settings. The effectiveness of the proposed method and the accuracy of the
resulting dynamic maps are evaluated in field tests. The results show that the
proposed system can decrease the map error caused by non-traffic sources up to
51% and can stay effective under significant packet losses.

ÊëòË¶ÅÔºöÈùúÊÖãÂô™Èü≥Âú∞ÂúñÊèèÁπ™‰∫ÜÂª£ÈóäÂçÄÂüüÁöÑÈï∑ÊúüÂô™Èü≥Ê∞¥Âπ≥ÔºåÂ∞çÊñºÈôç‰ΩéÂ±ÖÊ∞ëÁöÑÂô™Èü≥ÊõùÈú≤ÔºåÊòØÂ∏ÇÊîøÂñÆ‰ΩçÊúâÂÉπÂÄºÁöÑÈÉΩÂ∏ÇË¶èÂäÉË≥áÁî¢„ÄÇÁÑ∂ËÄåÔºå‰∫∫ÂÄëÁ∂ìÂ∏∏Êä±ÊÄ®ÁöÑÈùû‰∫§ÈÄöÂô™Èü≥‰æÜÊ∫êÂÖ∑ÊúâÊö´ÊÖãË°åÁÇ∫ÔºåÈÄöÂ∏∏ÊúÉË¢´ÈùúÊÖãÂú∞ÂúñÂøΩÁï•„ÄÇÊàëÂÄëÂú®Ê≠§ÊèêÂá∫‰ΩøÁî®ÈÄèÈÅé‰ΩéÂäüÁéáÂª£ÂüüÁ∂≤Ë∑ØÔºàLPWANÔºåÁâπÂà•ÊòØ LoRaWANÔºâÊâÄÊî∂ÈõÜÁöÑË≥áÊñôÔºå‰ª•ÂèäÂü∫ÊñºÁâ©ËÅØÁ∂≤ÔºàIoTÔºâÂü∫Á§éË®≠ÊñΩÁöÑÂãïÊÖãÂô™Èü≥Áπ™Ë£ΩÊñπÊ≥ïÔºåÈÄôÊòØÊô∫ÊÖßÂüéÂ∏ÇÊúÄÂ∏∏Ë¶ãÁöÑÈÄöË®ä‰∏ªÂππ‰πã‰∏Ä„ÄÇÁî±ÊñºÈÄô‰∫õÂçîÂÆöÁöÑË≥áÊñôÈÄüÁéá‰ΩéÔºåÂõ†Ê≠§Âü∫Êñº LPWAN ÁöÑÂô™Èü≥Áπ™Ë£ΩÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÊâÄÊèêÂá∫ÁöÑÂãïÊÖãÂô™Èü≥Áπ™Ë£ΩÊñπÊ≥ïÈÄèÈÅéÊ©üÂô®Â≠∏ÁøíÔºàMLÔºâ‰æÜÈ†êÊ∏¨Èùû‰∫§ÈÄö‰æÜÊ∫êÁöÑ‰∫ã‰ª∂Âíå‰ΩçÁΩÆÔºå‰ª•Ê∏õÂ∞ëË≥áÊñôÈÄüÁéáÈôêÂà∂ÁöÑË≤†Èù¢ÂΩ±ÈüøÔºåËÄåË≥áÊñôÈùûÂ∏∏Á®ÄÂ∞ë„ÄÇÈÄô‰∫õÊ®°ÂûãÁöÑÂÑ™ÈªûÂú®ÊñºËÄÉÈáè‰∫ÜÈÉΩÂ∏ÇÁí∞Â¢É‰∏≠Âª∫ÁØâÁâ©ÊâÄÈÄ†ÊàêÁöÑËÅ≤Â≠∏Ë°åÁÇ∫ÁöÑÁ©∫ÈñìËÆäÁï∞„ÄÇÂú®ÂØ¶Âú∞Ê∏¨Ë©¶‰∏≠Ë©ï‰º∞‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß‰ª•ÂèäÁî¢ÁîüÁöÑÂãïÊÖãÂú∞ÂúñÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÁ≥ªÁµ±ÂèØ‰ª•Â∞áÈùû‰∫§ÈÄö‰æÜÊ∫êÈÄ†ÊàêÁöÑÊò†Â∞ÑË™§Â∑ÆÈôç‰ΩéÂ§öÈÅî 51%Ôºå‰∏îÂú®Â∞ÅÂåÖÈÅ∫Â§±ÁéáÈ°ØËëóÁöÑÊÉÖÊ≥Å‰∏ã‰ªçËÉΩ‰øùÊåÅÊúâÊïàÊÄß„ÄÇ

##### **GenRec: Generative Personalized Sequential Recommendation**
2407.21191v1 by Panfeng Cao, Pietro Lio

Sequential recommendation is a task to capture hidden user preferences from
historical user item interaction data. Significant progress has been made in
this domain by leveraging classification based learning methods. Inspired by
the recent paradigm of 'pretrain, prompt and predict' in NLP, we consider
sequential recommendation as a sequence to sequence generation task and propose
a novel model named Generative Recommendation (GenRec). Unlike classification
based models that learn explicit user and item representations, GenRec utilizes
the sequence modeling capability of Transformer and adopts the masked item
prediction objective to effectively learn the hidden bidirectional sequential
patterns. Different from existing generative sequential recommendation models,
GenRec does not rely on manually designed hard prompts. The input to GenRec is
textual user item sequence and the output is top ranked next items. Moreover,
GenRec is lightweight and requires only a few hours to train effectively in
low-resource settings, making it highly applicable to real-world scenarios and
helping to democratize large language models in the sequential recommendation
domain. Our extensive experiments have demonstrated that GenRec generalizes on
various public real-world datasets and achieves state-of-the-art results. Our
experiments also validate the effectiveness of the the proposed masked item
prediction objective that improves the model performance by a large margin.

ÊëòË¶ÅÔºöÂ∫èÂàóÊé®Ëñ¶‰ªªÂãôÊòØÂæûÊ≠∑Âè≤‰ΩøÁî®ËÄÖÈ†ÖÁõÆ‰∫íÂãïË≥áÊñô‰∏≠Êì∑ÂèñÈö±Ëóè‰ΩøÁî®ËÄÖÂÅèÂ•Ω„ÄÇÈÄèÈÅéÂà©Áî®Âü∫ÊñºÂàÜÈ°ûÁöÑÂ≠∏ÁøíÊñπÊ≥ïÔºåÂ∑≤Âú®ÈÄôÂÄãÈ†òÂüüÂèñÂæóÈ°ØËëóÈÄ≤Â±ï„ÄÇÂèóÂà∞Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠„ÄåÈ†êË®ìÁ∑¥„ÄÅÊèêÁ§∫ÂíåÈ†êÊ∏¨„ÄçÁöÑÊñ∞ËààÂÖ∏ÁØÑÂïüÁôºÔºåÊàëÂÄëÂ∞áÂ∫èÂàóÊé®Ëñ¶Ë¶ñÁÇ∫Â∫èÂàóÂà∞Â∫èÂàóÁî¢Áîü‰ªªÂãôÔºå‰∏¶ÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ÁîüÊàêÂºèÊé®Ëñ¶ (GenRec) ÁöÑÊñ∞Á©éÊ®°Âûã„ÄÇËàáÂ≠∏ÁøíÊòéÁ¢∫‰ΩøÁî®ËÄÖÂíåÈ†ÖÁõÆË°®ÂæµÁöÑÂü∫ÊñºÂàÜÈ°ûÁöÑÊ®°Âûã‰∏çÂêåÔºåGenRec Âà©Áî® Transformer ÁöÑÂ∫èÂàóÂª∫Ê®°ËÉΩÂäõÔºå‰∏¶Êé°Áî®ÈÅÆËîΩÈ†ÖÁõÆÈ†êÊ∏¨ÁõÆÊ®ôÔºå‰ª•ÊúâÊïàÂ≠∏ÁøíÈö±ËóèÁöÑÈõôÂêëÂ∫èÂàóÊ®°Âºè„ÄÇËàáÁèæÊúâÁöÑÁîüÊàêÂºèÂ∫èÂàóÊé®Ëñ¶Ê®°Âûã‰∏çÂêåÔºåGenRec ‰∏ç‰æùË≥¥ÊâãÂãïË®≠Ë®àÁöÑÁ°¨ÊèêÁ§∫„ÄÇGenRec ÁöÑËº∏ÂÖ•ÊòØÊñáÂ≠ó‰ΩøÁî®ËÄÖÈ†ÖÁõÆÂ∫èÂàóÔºåËÄåËº∏Âá∫ÊòØÊéíÂêçÊúÄÂâçÈù¢ÁöÑ‰∏ã‰∏ÄÂÄãÈ†ÖÁõÆ„ÄÇÊ≠§Â§ñÔºåGenRec ÈùûÂ∏∏Á≤æÁ∞°ÔºåÂè™ÈúÄÂπæÂÄãÂ∞èÊôÇÂç≥ÂèØÂú®‰ΩéË≥áÊ∫êË®≠ÂÆö‰∏≠ÊúâÊïàË®ìÁ∑¥Ôºå‰ΩøÂÖ∂ÈùûÂ∏∏ÈÅ©Áî®ÊñºÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØÔºå‰∏¶ÊúâÂä©ÊñºÂú®Â∫èÂàóÊé®Ëñ¶È†òÂüüÊ∞ë‰∏ªÂåñÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇÊàëÂÄëÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé GenRec ÂèØ‰ª•Ê¶ÇÊã¨Âà∞ÂêÑÁ®ÆÂÖ¨ÈñãÁöÑÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜÔºå‰∏¶ÂèñÂæóÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÁöÑÂØ¶È©ó‰πüÈ©óË≠â‰∫ÜÊâÄÊèêÂá∫ÁöÑÈÅÆËîΩÈ†ÖÁõÆÈ†êÊ∏¨ÁõÆÊ®ôÁöÑÊúâÊïàÊÄßÔºåÂÆÉÂ∞áÊ®°ÂûãÊïàËÉΩÂ§ßÂπÖÊèêÂçá„ÄÇ

##### **AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning**
2407.21174v1 by Maisha Binte Rashid, Pablo Rivas

Multimodal machine learning models that combine visual and textual data are
increasingly being deployed in critical applications, raising significant
safety and security concerns due to their vulnerability to adversarial attacks.
This paper presents an effective strategy to enhance the robustness of
multimodal image captioning models against such attacks. By leveraging the Fast
Gradient Sign Method (FGSM) to generate adversarial examples and incorporating
adversarial training techniques, we demonstrate improved model robustness on
two benchmark datasets: Flickr8k and COCO. Our findings indicate that
selectively training only the text decoder of the multimodal architecture shows
performance comparable to full adversarial training while offering increased
computational efficiency. This targeted approach suggests a balance between
robustness and training costs, facilitating the ethical deployment of
multimodal AI systems across various domains.

ÊëòË¶ÅÔºöÂ§öÊ®°ÊÄÅÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁªìÂêà‰∫ÜËßÜËßâÂíåÊñáÊú¨Êï∞ÊçÆÔºå
Ë∂äÊù•Ë∂äÂ§öÂú∞Áî®‰∫éÂÖ≥ÈîÆÂ∫îÁî®Á®ãÂ∫è‰∏≠ÔºåÁî±‰∫éÂÖ∂ÂÆπÊòìÂèóÂà∞ÂØπÊäóÊÄßÊîªÂáªÔºåÂõ†Ê≠§ÂºïÂèë‰∫ÜÈáçÂ§ßÁöÑ
ÂÆâÂÖ®ÈóÆÈ¢ò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑÁ≠ñÁï•Êù•Â¢ûÂº∫
Â§öÊ®°ÊÄÅÂõæÂÉèÂ≠óÂπïÊ®°ÂûãÂØπËøôÁßçÊîªÂáªÁöÑÈ≤ÅÊ£íÊÄß„ÄÇÈÄöËøáÂà©Áî®Âø´ÈÄü
Ê¢ØÂ∫¶Á¨¶Âè∑ÊñπÊ≥ï (FGSM) ÁîüÊàêÂØπÊäóÊÄßÁ§∫‰æãÂπ∂ÁªìÂêà
ÂØπÊäóÊÄßËÆ≠ÁªÉÊäÄÊúØÔºåÊàë‰ª¨Âú®‰∏§‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºöFlickr8k Âíå COCO ‰∏äÂ±ïÁ§∫‰∫ÜÊîπËøõÁöÑÊ®°ÂûãÈ≤ÅÊ£íÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÁªìÊûúË°®Êòé
ÈÄâÊã©ÊÄßÂú∞‰ªÖËÆ≠ÁªÉÂ§öÊ®°ÊÄÅÊû∂ÊûÑÁöÑÊñáÊú¨Ëß£Á†ÅÂô®ÊòæÁ§∫
ÊÄßËÉΩ‰∏éÂÆåÂÖ®ÂØπÊäóÊÄßËÆ≠ÁªÉÁõ∏ÂΩìÔºåÂêåÊó∂Êèê‰æõÊõ¥È´òÁöÑ
ËÆ°ÁÆóÊïàÁéá„ÄÇËøôÁßçÊúâÈíàÂØπÊÄßÁöÑÊñπÊ≥ïË°®Êòé‰∫Ü
È≤ÅÊ£íÊÄßÂíåËÆ≠ÁªÉÊàêÊú¨‰πãÈó¥ÁöÑÂπ≥Ë°°Ôºå‰øÉËøõ‰∫Ü
Â§öÊ®°ÊÄÅ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂú®ÂêÑ‰∏™È¢ÜÂüüÁöÑÈÅìÂæ∑ÈÉ®ÁΩ≤„ÄÇ

##### **Decomposed Prompting to Answer Questions on a Course Discussion Board**
2407.21170v1 by Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang

We propose and evaluate a question-answering system that uses decomposed
prompting to classify and answer student questions on a course discussion
board. Our system uses a large language model (LLM) to classify questions into
one of four types: conceptual, homework, logistics, and not answerable. This
enables us to employ a different strategy for answering questions that fall
under different types. Using a variant of GPT-3, we achieve $81\%$
classification accuracy. We discuss our system's performance on answering
conceptual questions from a machine learning course and various failure modes.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏¶Ë©ï‰º∞‰∏ÄÂÄãÂïèÈ°åÂõûÁ≠îÁ≥ªÁµ±ÔºåË©≤Á≥ªÁµ±‰ΩøÁî®ÂàÜËß£ÊèêÁ§∫‰æÜÂàÜÈ°ûÂíåÂõûÁ≠îÂ≠∏ÁîüÂú®Ë™≤Á®ãË®éË´ñÂçÄ‰∏äÁöÑÂïèÈ°å„ÄÇÊàëÂÄëÁöÑÁ≥ªÁµ±‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∞áÂïèÈ°åÂàÜÈ°ûÁÇ∫ÂõõÁ®ÆÈ°ûÂûã‰πã‰∏ÄÔºöÊ¶ÇÂøµ„ÄÅ‰ΩúÊ•≠„ÄÅÂæåÂã§ÂíåÁÑ°Ê≥ïÂõûÁ≠î„ÄÇÈÄô‰ΩøÊàëÂÄëËÉΩÂ§†ÈáùÂ∞ç‰∏çÂêåÈ°ûÂûãÁöÑÂïèÈ°åÊé°Áî®‰∏çÂêåÁöÑÂõûÁ≠îÁ≠ñÁï•„ÄÇ‰ΩøÁî® GPT-3 ÁöÑËÆäÈ´îÔºåÊàëÂÄëÈÅîÂà∞‰∫Ü 81% ÁöÑÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÇÊàëÂÄëË®éË´ñ‰∫ÜÊàëÂÄëÁöÑÁ≥ªÁµ±Âú®ÂõûÁ≠îÊ©üÂô®Â≠∏ÁøíË™≤Á®ã‰∏≠ÁöÑÊ¶ÇÂøµÊÄßÂïèÈ°åÊôÇÁöÑË°®ÁèæÂíåÂêÑÁ®ÆÂ§±ÊïóÊ®°Âºè„ÄÇ

##### **Understanding Public Safety Trends in Calgary through data mining**
2407.21163v1 by Zack Dewis, Apratim Sen, Jeffrey Wong, Yujia Zhang

This paper utilizes statistical data from various open datasets in Calgary to
to uncover patterns and insights for community crimes, disorders, and traffic
incidents. Community attributes like demographics, housing, and pet
registration were collected and analyzed through geospatial visualization and
correlation analysis. Strongly correlated features were identified using the
chi-square test, and predictive models were built using association rule mining
and machine learning algorithms. The findings suggest that crime rates are
closely linked to factors such as population density, while pet registration
has a smaller impact. This study offers valuable insights for city managers to
enhance community safety strategies.

ÊëòË¶ÅÔºöÊú¨Ë´ñÊñáÂà©Áî®Âç°Âä†Âà©ÂêÑÁ®ÆÈñãÊîæË≥áÊñôÈõÜÁöÑÁµ±Ë®àÊï∏ÊìöÔºåÊè≠Á§∫Á§æÂçÄÁäØÁΩ™„ÄÅÂ§±Â∫èÂíå‰∫§ÈÄö‰∫ãÊïÖÁöÑÊ®°ÂºèÂíåË¶ãËß£„ÄÇÈÄöÈÅéÂú∞ÁêÜÁ©∫ÈñìÂèØË¶ñÂåñÂíåÁõ∏ÈóúÊÄßÂàÜÊûêÊî∂ÈõÜÂíåÂàÜÊûê‰∫ÜÁ§æÂçÄÂ±¨ÊÄßÔºå‰æãÂ¶Ç‰∫∫Âè£Áµ±Ë®à„ÄÅ‰ΩèÊàøÂíåÂØµÁâ©Ë®ªÂÜä„ÄÇ‰ΩøÁî®Âç°ÊñπÊ™¢È©óË≠òÂà•Âá∫Áõ∏ÈóúÊÄßÂæàÂº∑ÁöÑÁâπÂæµÔºå‰∏¶‰ΩøÁî®ÈóúËÅØË¶èÂâáÊåñÊéòÂíåÊ©üÂô®Â≠∏ÁøíÁÆóÊ≥ïÊßãÂª∫È†êÊ∏¨Ê®°Âûã„ÄÇÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÁäØÁΩ™ÁéáËàá‰∫∫Âè£ÂØÜÂ∫¶Á≠âÂõ†Á¥†ÂØÜÂàáÁõ∏ÈóúÔºåËÄåÂØµÁâ©Ë®ªÂÜäÁöÑÂΩ±ÈüøËºÉÂ∞è„ÄÇÊú¨Á†îÁ©∂ÁÇ∫ÂüéÂ∏ÇÁÆ°ÁêÜËÄÖÊèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰ª•Â¢ûÂº∑Á§æÂçÄÂÆâÂÖ®Á≠ñÁï•„ÄÇ

##### **Event-Arguments Extraction Corpus and Modeling using BERT for Arabic**
2407.21153v1 by Alaa Aljabari, Lina Duaibes, Mustafa Jarrar, Mohammed Khalilia

Event-argument extraction is a challenging task, particularly in Arabic due
to sparse linguistic resources. To fill this gap, we introduce the \hadath
corpus ($550$k tokens) as an extension of Wojood, enriched with event-argument
annotations. We used three types of event arguments: $agent$, $location$, and
$date$, which we annotated as relation types. Our inter-annotator agreement
evaluation resulted in $82.23\%$ $Kappa$ score and $87.2\%$ $F_1$-score.
Additionally, we propose a novel method for event relation extraction using
BERT, in which we treat the task as text entailment. This method achieves an
$F_1$-score of $94.01\%$. To further evaluate the generalization of our
proposed method, we collected and annotated another out-of-domain corpus (about
$80$k tokens) called \testNLI and used it as a second test set, on which our
approach achieved promising results ($83.59\%$ $F_1$-score). Last but not
least, we propose an end-to-end system for event-arguments extraction. This
system is implemented as part of SinaTools, and both corpora are publicly
available at {\small \url{https://sina.birzeit.edu/wojood}}

ÊëòË¶ÅÔºö‰∫ã‰ª∂Ë´ñÂÖÉËêÉÂèñÊòØ‰∏ÄÈ†ÖÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãôÔºåÁâπÂà•ÊòØÂ∞çÊñºÈòøÊãâ‰ºØË™ûÔºåÂõ†ÁÇ∫Ë™ûË®ÄË≥áÊ∫êÁ®ÄÂ∞ë„ÄÇÁÇ∫‰∫ÜÂ°´Ë£úÈÄôÂÄãÁ©∫ÁôΩÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü\hadathË™ûÊñôÂ∫´Ôºà55 Ëê¨ÂÄãË©ûÂΩôÔºâÔºå‰ΩúÁÇ∫WojoodÁöÑÂª∂‰º∏Ôºå‰∏¶Âä†ÂÖ•‰∫Ü‰∫ã‰ª∂Ë´ñÂÖÉË®ªËß£„ÄÇÊàëÂÄë‰ΩøÁî®‰∫Ü‰∏âÁ®ÆÈ°ûÂûãÁöÑ‰∫ã‰ª∂Ë´ñÂÖÉÔºö$agent$„ÄÅ$location$Âíå$date$ÔºåÊàëÂÄëÂ∞áÂÆÉÂÄëË®ªËß£ÁÇ∫Èóú‰øÇÈ°ûÂûã„ÄÇÊàëÂÄëÁöÑÊ®ôË®ªËÄÖÈñì‰∏ÄËá¥ÊÄßË©ï‰º∞ÂæóÂá∫ÁöÑKappaË©ïÂàÜÁÇ∫82.23%ÔºåF1Ë©ïÂàÜÁÇ∫87.2%„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®BERTÈÄ≤Ë°å‰∫ã‰ª∂Èóú‰øÇËêÉÂèñÁöÑÊñ∞ÊñπÊ≥ïÔºåÂú®Ë©≤ÊñπÊ≥ï‰∏≠ÔºåÊàëÂÄëÂ∞á‰ªªÂãôË¶ñÁÇ∫ÊñáÊú¨ËòäÊ∂µ„ÄÇÊ≠§ÊñπÊ≥ïÈÅîÂà∞‰∫Ü94.01%ÁöÑF1Ë©ïÂàÜ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Ë©ï‰º∞ÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊ≥õÂåñÊÄßÔºåÊàëÂÄëÊî∂ÈõÜ‰∏¶Ë®ªËß£‰∫ÜÂè¶‰∏ÄÂÄãÈ†òÂüüÂ§ñÁöÑË™ûÊñôÂ∫´ÔºàÁ¥Ñ8Ëê¨ÂÄãË©ûÂΩôÔºâÔºåÁ®±ÁÇ∫\testNLIÔºå‰∏¶Â∞áÂÖ∂Áî®‰ΩúÁ¨¨‰∫åÂÄãÊ∏¨Ë©¶ÈõÜÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂú®Ë©≤Ê∏¨Ë©¶ÈõÜ‰∏äÂèñÂæó‰∫ÜÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºàF1Ë©ïÂàÜÁÇ∫83.59%Ôºâ„ÄÇÊúÄÂæå‰ΩÜ‰∏¶ÈùûÊúÄ‰∏çÈáçË¶ÅÁöÑ‰∏ÄÈªûÊòØÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰∫ã‰ª∂Ë´ñÂÖÉËêÉÂèñÁöÑÁ´ØÂà∞Á´ØÁ≥ªÁµ±„ÄÇÊ≠§Á≥ªÁµ±‰ΩúÁÇ∫SinaToolsÁöÑ‰∏ÄÈÉ®ÂàÜÂØ¶‰ΩúÔºå‰∏¶‰∏îÂÖ©ÂÄãË™ûÊñôÂ∫´ÈÉΩÂèØ‰ª•Âú®{\small \url{https://sina.birzeit.edu/wojood}}ÂÖ¨ÈñãÂèñÂæó„ÄÇ

##### **Private Collaborative Edge Inference via Over-the-Air Computation**
2407.21151v1 by Selim F. Yilmaz, Burak Hasircioglu, Li Qiao, Deniz Gunduz

We consider collaborative inference at the wireless edge, where each client's
model is trained independently on their local datasets. Clients are queried in
parallel to make an accurate decision collaboratively. In addition to
maximizing the inference accuracy, we also want to ensure the privacy of local
models. To this end, we leverage the superposition property of the multiple
access channel to implement bandwidth-efficient multi-user inference methods.
Specifically, we propose different methods for ensemble and multi-view
classification that exploit over-the-air computation. We show that these
schemes perform better than their orthogonal counterparts with statistically
significant differences while using fewer resources and providing privacy
guarantees. We also provide experimental results verifying the benefits of the
proposed over-the-air multi-user inference approach and perform an ablation
study to demonstrate the effectiveness of our design choices. We share the
source code of the framework publicly on Github to facilitate further research
and reproducibility.

ÊëòË¶ÅÔºöÊàëÂÄëÂú®ÁÑ°Á∑öÈÇäÁ∑£ËÄÉÊÖÆÂçî‰ΩúÊé®Ë´ñÔºåÂÖ∂‰∏≠ÊØèÂÄãÂÆ¢Êà∂Á´ØÁöÑÊ®°ÂûãÈÉΩÊòØÁç®Á´ãÂú®‰ªñÂÄëÁöÑÊú¨Âú∞Ë≥áÊñôÈõÜ‰∏äË®ìÁ∑¥ÁöÑ„ÄÇÂÆ¢Êà∂Á´ØÊúÉË¢´Âπ≥Ë°åÊü•Ë©¢‰ª•Âçî‰ΩúÂÅöÂá∫Ê∫ñÁ¢∫ÁöÑÊ±∫Á≠ñ„ÄÇÈô§‰∫ÜÊúÄÂ§ßÂåñÊé®Ë´ñÊ∫ñÁ¢∫ÊÄß‰πãÂ§ñÔºåÊàëÂÄë‰πüÊÉ≥Ë¶ÅÁ¢∫‰øùÊú¨Âú∞Ê®°ÂûãÁöÑÈö±ÁßÅÊÄß„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂà©Áî®Â§öÈáçÂ≠òÂèñÈÄöÈÅìÁöÑÁñäÂä†Â±¨ÊÄß‰æÜÂØ¶‰ΩúÈ†ªÂØ¨ÊïàÁéáÈ´òÁöÑÂ§ö‰ΩøÁî®ËÄÖÊé®Ë´ñÊñπÊ≥ï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∏çÂêåÁöÑÊñπÊ≥ïÁî®ÊñºÊï¥È´îÂíåÂ§öË¶ñËßíÂàÜÈ°ûÔºåÂà©Áî®Á©∫‰∏≠ÈÅãÁÆó„ÄÇÊàëÂÄëÂ±ïÁ§∫ÈÄô‰∫õÊñπÊ°àÊØîÂÆÉÂÄëÁöÑÊ≠£‰∫§Â∞çÊáâÊñπÊ°àË°®ÁèæÂæóÊõ¥Â•ΩÔºåÂÖ∑ÊúâÁµ±Ë®à‰∏äÁöÑÈ°ØËëóÂ∑ÆÁï∞ÔºåÂêåÊôÇ‰ΩøÁî®ËºÉÂ∞ëÁöÑË≥áÊ∫ê‰∏¶Êèê‰æõÈö±ÁßÅ‰øùË≠â„ÄÇÊàëÂÄëÈÇÑÊèê‰æõÂØ¶È©óÁµêÊûúÔºåÈ©óË≠âÊâÄÊèêÂá∫ÁöÑÁ©∫‰∏≠Â§ö‰ΩøÁî®ËÄÖÊé®Ë´ñÊñπÊ≥ïÁöÑÂÑ™ÈªûÔºå‰∏¶ÈÄ≤Ë°åÊ∂àËûçÁ†îÁ©∂‰ª•Ë≠âÊòéÊàëÂÄëÁöÑË®≠Ë®àÈÅ∏ÊìáÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÂú® Github ‰∏äÂÖ¨ÈñãÂàÜ‰∫´Ê°ÜÊû∂ÁöÑÂéüÂßãÁ¢ºÔºå‰ª•Âà©ÊñºÈÄ≤‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂ÂíåÈáçÁèæÊÄß„ÄÇ

##### **Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population**
2407.21149v1 by Mayanka Chandrashekar, Ian Goethert, Md Inzamam Ul Haque, Benjamin McMahon, Sayera Dhaubhadel, Kathryn Knight, Joseph Erdos, Donna Reagan, Caroline Taylor, Peter Kuzmak, John Michael Gaziano, Eileen McAllister, Lauren Costa, Yuk-Lam Ho, Kelly Cho, Suzanne Tamang, Samah Fodeh-Jarad, Olga S. Ovchinnikova, Amy C. Justice, Jacob Hinkle, Ioana Danciu

Objectives: This study aims to assess the impact of domain shift on chest
X-ray classification accuracy and to analyze the influence of ground truth
label quality and demographic factors such as age group, sex, and study year.
Materials and Methods: We used a DenseNet121 model pretrained MIMIC-CXR dataset
for deep learning-based multilabel classification using ground truth labels
from radiology reports extracted using the CheXpert and CheXbert Labeler. We
compared the performance of the 14 chest X-ray labels on the MIMIC-CXR and
Veterans Healthcare Administration chest X-ray dataset (VA-CXR). The VA-CXR
dataset comprises over 259k chest X-ray images spanning between the years 2010
and 2022. Results: The validation of ground truth and the assessment of
multi-label classification performance across various NLP extraction tools
revealed that the VA-CXR dataset exhibited lower disagreement rates than the
MIMIC-CXR datasets. Additionally, there were notable differences in AUC scores
between models utilizing CheXpert and CheXbert. When evaluating multi-label
classification performance across different datasets, minimal domain shift was
observed in unseen datasets, except for the label "Enlarged Cardiomediastinum."
The study year's subgroup analyses exhibited the most significant variations in
multi-label classification model performance. These findings underscore the
importance of considering domain shifts in chest X-ray classification tasks,
particularly concerning study years. Conclusion: Our study reveals the
significant impact of domain shift and demographic factors on chest X-ray
classification, emphasizing the need for improved transfer learning and
equitable model development. Addressing these challenges is crucial for
advancing medical imaging and enhancing patient care.

ÊëòË¶ÅÔºö<paragraph>ÁõÆÊ®ôÔºöÊú¨Á†îÁ©∂Êó®Âú®Ë©ï‰º∞È†òÂüüËΩâÁßªÂ∞çËÉ∏ÈÉ® X ÂÖâÂàÜÈ°ûÁ≤æÂ∫¶ÁöÑÂΩ±ÈüøÔºå‰∏¶ÂàÜÊûêÂü∫Êú¨‰∫ãÂØ¶Ê®ôÁ±§ÂìÅË≥™ÂíåÂπ¥ÈΩ°ÁµÑ„ÄÅÊÄßÂà•ÂíåÁ†îÁ©∂Âπ¥‰ªΩÁ≠â‰∫∫Âè£Âõ†Á¥†ÁöÑÂΩ±Èüø„ÄÇ
ÊùêÊñôÂíåÊñπÊ≥ïÔºöÊàëÂÄë‰ΩøÁî® DenseNet121 Ê®°ÂûãÈ†êË®ìÁ∑¥ MIMIC-CXR Ë≥áÊñôÈõÜÔºå‰ΩøÁî®Âæû‰ΩøÁî® CheXpert Âíå CheXbert Ê®ôÁ±§Âô®ÂæûÊîæÂ∞ÑÁßëÂ†±Âëä‰∏≠ÊèêÂèñÁöÑÂü∫Êú¨‰∫ãÂØ¶Ê®ôÁ±§ÈÄ≤Ë°åÂü∫ÊñºÊ∑±Â∫¶Â≠∏ÁøíÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°û„ÄÇÊàëÂÄëÊØîËºÉ‰∫Ü MIMIC-CXR ÂíåÈÄÄ‰ºçËªç‰∫∫ÂÅ•Â∫∑ÁÆ°ÁêÜÂ±ÄËÉ∏ÈÉ® X ÂÖâË≥áÊñôÈõÜ (VA-CXR) ‰∏ä 14 ÂÄãËÉ∏ÈÉ® X ÂÖâÊ®ôÁ±§ÁöÑÊÄßËÉΩ„ÄÇVA-CXR Ë≥áÊñôÈõÜÂåÖÂê´Ë∂ÖÈÅé 259k ÂºµËÉ∏ÈÉ® X ÂÖâÂΩ±ÂÉèÔºåÊôÇÈñìË∑®Â∫¶ÁÇ∫ 2010 Âπ¥Ëá≥ 2022 Âπ¥„ÄÇÁµêÊûúÔºöÂü∫Êú¨‰∫ãÂØ¶ÁöÑÈ©óË≠âÂíåÂ∞çÂêÑÁ®Æ NLP ÊèêÂèñÂ∑•ÂÖ∑ÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊÄßËÉΩÁöÑË©ï‰º∞È°ØÁ§∫ÔºåVA-CXR Ë≥áÊñôÈõÜË°®ÁèæÂá∫ÁöÑÂàÜÊ≠ßÁéá‰ΩéÊñº MIMIC-CXR Ë≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî® CheXpert Âíå CheXbert ÁöÑÊ®°Âûã‰πãÈñìÁöÑ AUC ÂæóÂàÜÂ≠òÂú®È°ØËëóÂ∑ÆÁï∞„ÄÇÂú®Ë©ï‰º∞‰∏çÂêåË≥áÊñôÈõÜ‰∏äÁöÑÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊÄßËÉΩÊôÇÔºåÈô§‰∫ÜÊ®ôÁ±§„ÄåÂøÉÁ∏±ÈöîÂ¢ûÂ§ß„Äç‰πãÂ§ñÔºåÂú®Êú™Ë¶ãË≥áÊñôÈõÜ‰∏≠ËßÄÂØüÂà∞ÁöÑÈ†òÂüüËΩâÁßªÂæàÂ∞è„ÄÇÁ†îÁ©∂Âπ¥‰ªΩÁöÑÂ≠êÁæ§ÂàÜÊûêÈ°ØÁ§∫ÔºåÂ§öÊ®ôÁ±§ÂàÜÈ°ûÊ®°ÂûãÊÄßËÉΩËÆäÂåñÊúÄÂ§ß„ÄÇÈÄô‰∫õÁôºÁèæÂº∑Ë™ø‰∫ÜÂú®ËÉ∏ÈÉ® X ÂÖâÂàÜÈ°û‰ªªÂãô‰∏≠ËÄÉÊÖÆÈ†òÂüüËΩâÁßªÁöÑÈáçË¶ÅÊÄßÔºåÁâπÂà•ÊòØÈóúÊñºÁ†îÁ©∂Âπ¥‰ªΩ„ÄÇÁµêË´ñÔºöÊàëÂÄëÁöÑÁ†îÁ©∂Êè≠Á§∫‰∫ÜÈ†òÂüüËΩâÁßªÂíå‰∫∫Âè£Âõ†Á¥†Â∞çËÉ∏ÈÉ® X ÂÖâÂàÜÈ°ûÁöÑÈ°ØËëóÂΩ±ÈüøÔºåÂº∑Ë™ø‰∫ÜÊîπÈÄ≤ÈÅ∑ÁßªÂ≠∏ÁøíÂíåÂÖ¨Âπ≥Ê®°ÂûãÈñãÁôºÁöÑÂøÖË¶ÅÊÄß„ÄÇÊáâÂ∞çÈÄô‰∫õÊåëÊà∞Â∞çÊñºÊé®ÈÄ≤ÈÜ´Â≠∏ÂΩ±ÂÉèÂíåÂä†Âº∑ÊÇ£ËÄÖË≠∑ÁêÜËá≥ÈóúÈáçË¶Å„ÄÇ</paragraph>

