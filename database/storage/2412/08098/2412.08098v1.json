{"2412.08098": {"publish_time": "2024-12-11", "title": "What You See Is Not Always What You Get: An Empirical Study of Code Comprehension by Large Language Models", "paper_summary": "Recent studies have demonstrated outstanding capabilities of large language\nmodels (LLMs) in software engineering domain, covering numerous tasks such as\ncode generation and comprehension. While the benefit of LLMs for coding task is\nwell noted, it is perceived that LLMs are vulnerable to adversarial attacks. In\nthis paper, we study the specific LLM vulnerability to imperceptible character\nattacks, a type of prompt-injection attack that uses special characters to\nbefuddle an LLM whilst keeping the attack hidden to human eyes. We devise four\ncategories of attacks and investigate their effects on the performance outcomes\nof tasks relating to code analysis and code comprehension. Two generations of\nChatGPT are included to evaluate the impact of advancements made to\ncontemporary models. Our experimental design consisted of comparing perturbed\nand unperturbed code snippets and evaluating two performance outcomes, which\nare model confidence using log probabilities of response, and correctness of\nresponse. We conclude that earlier version of ChatGPT exhibits a strong\nnegative linear correlation between the amount of perturbation and the\nperformance outcomes, while the recent ChatGPT presents a strong negative\ncorrelation between the presence of perturbation and performance outcomes, but\nno valid correlational relationship between perturbation budget and performance\noutcomes. We anticipate this work contributes to an in-depth understanding of\nleveraging LLMs for coding tasks. It is suggested future research should delve\ninto how to create LLMs that can return a correct response even if the prompt\nexhibits perturbations.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u8b49\u660e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8edf\u9ad4\u5de5\u7a0b\u9818\u57df\u5177\u6709\u5091\u51fa\u7684\u80fd\u529b\uff0c\u6db5\u84cb\u4e86\u8a31\u591a\u4efb\u52d9\uff0c\u4f8b\u5982\u7a0b\u5f0f\u78bc\u7522\u751f\u548c\u7406\u89e3\u3002\u96d6\u7136 LLM \u5c0d\u7de8\u78bc\u4efb\u52d9\u7684\u597d\u8655\u5df2\u5ee3\u70ba\u4eba\u77e5\uff0c\u4f46 LLM \u88ab\u8a8d\u70ba\u5bb9\u6613\u53d7\u5230\u5c0d\u6297\u6027\u653b\u64ca\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86 LLM \u5c0d\u96e3\u4ee5\u5bdf\u89ba\u7684\u5b57\u5143\u653b\u64ca\u7684\u5177\u9ad4\u6f0f\u6d1e\uff0c\u9019\u662f\u4e00\u7a2e\u63d0\u793a\u6ce8\u5165\u653b\u64ca\uff0c\u5b83\u4f7f\u7528\u7279\u6b8a\u5b57\u5143\u4f86\u6df7\u6dc6 LLM\uff0c\u540c\u6642\u8b93\u653b\u64ca\u5c0d\u4eba\u773c\u96b1\u85cf\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u56db\u7a2e\u985e\u578b\u7684\u653b\u64ca\uff0c\u4e26\u7814\u7a76\u5b83\u5011\u5c0d\u8207\u7a0b\u5f0f\u78bc\u5206\u6790\u548c\u7a0b\u5f0f\u78bc\u7406\u89e3\u76f8\u95dc\u4efb\u52d9\u7684\u6548\u80fd\u7d50\u679c\u7684\u5f71\u97ff\u3002\u5305\u542b\u5169\u4ee3 ChatGPT \u4ee5\u8a55\u4f30\u5c0d\u7576\u4ee3\u6a21\u578b\u6240\u505a\u9032\u5c55\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u5be6\u9a57\u8a2d\u8a08\u5305\u62ec\u6bd4\u8f03\u64fe\u52d5\u548c\u672a\u64fe\u52d5\u7684\u7a0b\u5f0f\u78bc\u7247\u6bb5\uff0c\u4e26\u8a55\u4f30\u5169\u500b\u6548\u80fd\u7d50\u679c\uff0c\u5206\u5225\u662f\u4f7f\u7528\u56de\u61c9\u7684\u5c0d\u6578\u6a5f\u7387\u7684\u6a21\u578b\u4fe1\u5fc3\uff0c\u4ee5\u53ca\u56de\u61c9\u7684\u6b63\u78ba\u6027\u3002\u6211\u5011\u5f97\u51fa\u7684\u7d50\u8ad6\u662f\uff0c\u8f03\u65e9\u7248\u672c\u7684 ChatGPT \u5728\u64fe\u52d5\u91cf\u548c\u6548\u80fd\u7d50\u679c\u4e4b\u9593\u8868\u73fe\u51fa\u5f37\u70c8\u7684\u8ca0\u7dda\u6027\u76f8\u95dc\u6027\uff0c\u800c\u6700\u8fd1\u7684 ChatGPT \u5728\u64fe\u52d5\u7684\u5b58\u5728\u548c\u6548\u80fd\u7d50\u679c\u4e4b\u9593\u8868\u73fe\u51fa\u5f37\u70c8\u7684\u8ca0\u76f8\u95dc\u6027\uff0c\u4f46\u64fe\u52d5\u9810\u7b97\u548c\u6548\u80fd\u7d50\u679c\u4e4b\u9593\u6c92\u6709\u6709\u6548\u7684\u76f8\u95dc\u95dc\u4fc2\u3002\u6211\u5011\u9810\u671f\u9019\u9805\u5de5\u4f5c\u6709\u52a9\u65bc\u6df1\u5165\u4e86\u89e3\u5982\u4f55\u5229\u7528 LLM \u9032\u884c\u7de8\u78bc\u4efb\u52d9\u3002\u5efa\u8b70\u672a\u4f86\u7684\u7814\u7a76\u61c9\u6df1\u5165\u63a2\u8a0e\u5982\u4f55\u5efa\u7acb\u5373\u4f7f\u63d0\u793a\u51fa\u73fe\u64fe\u52d5\u4e5f\u80fd\u56de\u50b3\u6b63\u78ba\u56de\u61c9\u7684 LLM\u3002</paragraph>", "author": "Bangshuo Zhu et.al.", "authors": "Bangshuo Zhu, Jiawen Wen, Huaming Chen", "id": "2412.08098v1", "paper_url": "http://arxiv.org/abs/2412.08098v1", "repo": "null"}}