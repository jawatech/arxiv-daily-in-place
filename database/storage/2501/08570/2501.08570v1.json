{"2501.08570": {"publish_time": "2025-01-15", "title": "Information Entropy Invariance: Enhancing Length Extrapolation in Attention Mechanisms", "paper_summary": "Improving the length extrapolation capabilities of Large Language Models\n(LLMs) remains a critical challenge in natural language processing. Many recent\nefforts have focused on modifying the scaled dot-product attention mechanism,\nand often introduce scaled temperatures without rigorous theoretical\njustification. To fill this gap, we introduce a novel approach based on\ninformation entropy invariance. We propose two new scaled temperatures to\nenhance length extrapolation. First, a training-free method InfoScale is\ndesigned for dot-product attention, and preserves focus on original tokens\nduring length extrapolation by ensuring information entropy remains consistent.\nSecond, we theoretically analyze the impact of scaling (CosScale) on cosine\nattention. Experimental data demonstrates that combining InfoScale and CosScale\nachieves state-of-the-art performance on the GAU-{\\alpha} model with a context\nwindow extended to 64 times the training length, and outperforms seven existing\nmethods. Our analysis reveals that significantly increasing CosScale\napproximates windowed attention, and highlights the significance of attention\nscore dilution as a key challenge in long-range context handling. The code and\ndata are available at https://github.com/HT-NEKO/InfoScale.", "paper_summary_zh": "\u6539\u5584\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9577\u5ea6\u5916\u63a8\u80fd\u529b\u4ecd\u7136\u662f\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4e2d\u7684\u4e00\u9805\u95dc\u9375\u6311\u6230\u3002\u8a31\u591a\u8fd1\u671f\u7684\u52aa\u529b\u90fd\u5c08\u6ce8\u65bc\u4fee\u6539\u7e2e\u653e\u9ede\u7a4d\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4e26\u4e14\u7d93\u5e38\u5728\u6c92\u6709\u56b4\u8b39\u7406\u8ad6\u4f9d\u64da\u7684\u60c5\u6cc1\u4e0b\u5f15\u5165\u7e2e\u653e\u6eab\u5ea6\u3002\u70ba\u4e86\u586b\u88dc\u9019\u500b\u7f3a\u53e3\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u57fa\u65bc\u8cc7\u8a0a\u71b5\u4e0d\u8b8a\u6027\u7684\u65b0\u65b9\u6cd5\u3002\u6211\u5011\u63d0\u51fa\u4e86\u5169\u7a2e\u65b0\u7684\u7e2e\u653e\u6eab\u5ea6\u4f86\u589e\u5f37\u9577\u5ea6\u5916\u63a8\u3002\u9996\u5148\uff0c\u4e00\u7a2e\u514d\u8a13\u7df4\u7684\u65b9\u6cd5 InfoScale \u662f\u70ba\u9ede\u7a4d\u6ce8\u610f\u529b\u8a2d\u8a08\u7684\uff0c\u4e26\u4e14\u5728\u9577\u5ea6\u5916\u63a8\u671f\u9593\u4fdd\u6301\u5c0d\u539f\u59cb\u6a19\u8a18\u7684\u95dc\u6ce8\uff0c\u65b9\u6cd5\u662f\u78ba\u4fdd\u8cc7\u8a0a\u71b5\u4fdd\u6301\u4e00\u81f4\u3002\u5176\u6b21\uff0c\u6211\u5011\u5f9e\u7406\u8ad6\u4e0a\u5206\u6790\u4e86\u7e2e\u653e (CosScale) \u5c0d\u9918\u5f26\u6ce8\u610f\u529b\u7684\u5f71\u97ff\u3002\u5be6\u9a57\u6578\u64da\u8868\u660e\uff0c\u5c07 InfoScale \u548c CosScale \u7d50\u5408\u4f7f\u7528\uff0c\u5728 GAU-{\\alpha} \u6a21\u578b\u4e0a\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684\u6027\u80fd\uff0c\u4e0a\u4e0b\u6587\u7a97\u53e3\u64f4\u5c55\u5230\u8a13\u7df4\u9577\u5ea6\u7684 64 \u500d\uff0c\u4e26\u4e14\u512a\u65bc\u4e03\u7a2e\u73fe\u6709\u65b9\u6cd5\u3002\u6211\u5011\u7684\u5206\u6790\u8868\u660e\uff0c\u986f\u8457\u589e\u52a0 CosScale \u8fd1\u4f3c\u65bc\u8996\u7a97\u6ce8\u610f\u529b\uff0c\u4e26\u5f37\u8abf\u4e86\u6ce8\u610f\u529b\u5206\u6578\u7a00\u91cb\u4f5c\u70ba\u9577\u7a0b\u4e0a\u4e0b\u6587\u8655\u7406\u4e2d\u7684\u95dc\u9375\u6311\u6230\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728 https://github.com/HT-NEKO/InfoScale \u53d6\u5f97\u3002", "author": "Kewei Li et.al.", "authors": "Kewei Li, Yanwen Kong, Yiping Xu, Lan Huang, Ruochi Zhang, Fengfeng Zhou", "id": "2501.08570v1", "paper_url": "http://arxiv.org/abs/2501.08570v1", "repo": "null"}}