{"2409.17819": {"publish_time": "2024-09-26", "title": "Inference-Time Language Model Alignment via Integrated Value Guidance", "paper_summary": "Large language models are typically fine-tuned to align with human\npreferences, but tuning large models is computationally intensive and complex.\nIn this work, we introduce $\\textit{Integrated Value Guidance}$ (IVG), a method\nthat uses implicit and explicit value functions to guide language model\ndecoding at token and chunk-level respectively, efficiently aligning large\nlanguage models purely at inference time. This approach circumvents the\ncomplexities of direct fine-tuning and outperforms traditional methods.\nEmpirically, we demonstrate the versatility of IVG across various tasks. In\ncontrolled sentiment generation and summarization tasks, our method\nsignificantly improves the alignment of large models using inference-time\nguidance from $\\texttt{gpt2}$-based value functions. Moreover, in a more\nchallenging instruction-following benchmark AlpacaEval 2.0, we show that both\nspecifically tuned and off-the-shelf value functions greatly improve the\nlength-controlled win rates of large models against $\\texttt{gpt-4-turbo}$\n(e.g., $19.51\\% \\rightarrow 26.51\\%$ for $\\texttt{Mistral-7B-Instruct-v0.2}$\nand $25.58\\% \\rightarrow 33.75\\%$ for $\\texttt{Mixtral-8x7B-Instruct-v0.1}$\nwith Tulu guidance).", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u901a\u5e38\u7d93\u904e\u5fae\u8abf\u4ee5\u7b26\u5408\u4eba\u985e\u504f\u597d\uff0c\u4f46\u8abf\u6574\u5927\u578b\u6a21\u578b\u5728\u8a08\u7b97\u4e0a\u5f88\u5bc6\u96c6\u4e14\u8907\u96dc\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u300c\u6574\u5408\u50f9\u503c\u5f15\u5c0e\u300d(IVG)\uff0c\u4e00\u7a2e\u4f7f\u7528\u96b1\u5f0f\u548c\u986f\u5f0f\u50f9\u503c\u51fd\u6578\u4f86\u5206\u5225\u5728\u7b26\u865f\u548c\u5340\u584a\u5c64\u7d1a\u5f15\u5c0e\u8a9e\u8a00\u6a21\u578b\u89e3\u78bc\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u5730\u5728\u7d14\u7cb9\u7684\u63a8\u8ad6\u6642\u9593\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u3002\u9019\u7a2e\u65b9\u6cd5\u907f\u958b\u4e86\u76f4\u63a5\u5fae\u8abf\u7684\u8907\u96dc\u6027\uff0c\u4e26\u4e14\u512a\u65bc\u50b3\u7d71\u65b9\u6cd5\u3002\u6839\u64da\u7d93\u9a57\uff0c\u6211\u5011\u5c55\u793a\u4e86 IVG \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u591a\u529f\u80fd\u6027\u3002\u5728\u53d7\u63a7\u60c5\u7dd2\u751f\u6210\u548c\u6458\u8981\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u4f7f\u7528\u57fa\u65bc gpt2 \u7684\u50f9\u503c\u51fd\u6578\u7684\u63a8\u8ad6\u6642\u9593\u5f15\u5c0e\uff0c\u986f\u8457\u6539\u5584\u4e86\u5927\u578b\u6a21\u578b\u7684\u5c0d\u9f4a\u3002\u6b64\u5916\uff0c\u5728\u66f4\u5177\u6311\u6230\u6027\u7684\u6307\u4ee4\u9075\u5faa\u57fa\u6e96 AlpacaEval 2.0 \u4e2d\uff0c\u6211\u5011\u8868\u660e\u5c08\u9580\u8abf\u6574\u548c\u73fe\u6210\u7684\u50f9\u503c\u51fd\u6578\u90fd\u5927\u5927\u63d0\u9ad8\u4e86\u5927\u578b\u6a21\u578b\u5c0d gpt-4-turbo \u7684\u9577\u5ea6\u63a7\u5236\u52dd\u7387\uff08\u4f8b\u5982\uff0cMistral-7B-Instruct-v0.2 \u70ba 19.51% \u2192 26.51%\uff0cMixtral-8x7B-Instruct-v0.1 \u4f7f\u7528 Tulu \u6307\u5c0e\u70ba 25.58% \u2192 33.75%\uff09\u3002", "author": "Zhixuan Liu et.al.", "authors": "Zhixuan Liu, Zhanhui Zhou, Yuanfu Wang, Chao Yang, Yu Qiao", "id": "2409.17819v1", "paper_url": "http://arxiv.org/abs/2409.17819v1", "repo": "null"}}