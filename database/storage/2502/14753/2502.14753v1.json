{"2502.14753": {"publish_time": "2025-02-20", "title": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders", "paper_summary": "Medical images are acquired at high resolutions with large fields of view in\norder to capture fine-grained features necessary for clinical decision-making.\nConsequently, training deep learning models on medical images can incur large\ncomputational costs. In this work, we address the challenge of downsizing\nmedical images in order to improve downstream computational efficiency while\npreserving clinically-relevant features. We introduce MedVAE, a family of six\nlarge-scale 2D and 3D autoencoders capable of encoding medical images as\ndownsized latent representations and decoding latent representations back to\nhigh-resolution images. We train MedVAE autoencoders using a novel two-stage\ntraining approach with 1,052,730 medical images. Across diverse tasks obtained\nfrom 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent\nrepresentations in place of high-resolution images when training downstream\nmodels can lead to efficiency benefits (up to 70x improvement in throughput)\nwhile simultaneously preserving clinically-relevant features and (2) MedVAE can\ndecode latent representations back to high-resolution images with high\nfidelity. Our work demonstrates that large-scale, generalizable autoencoders\ncan help address critical efficiency challenges in the medical domain. Our code\nis available at https://github.com/StanfordMIMI/MedVAE.", "paper_summary_zh": "\u533b\u5b66\u5f71\u50cf\u4ee5\u9ad8\u89e3\u6790\u5ea6\u548c\u5e7f\u9614\u7684\u89c6\u91ce\u83b7\u53d6\uff0c\u4ee5\u4fbf\u6355\u6349\u4e34\u5e8a\u51b3\u7b56\u6240\u9700\u7684\u7ec6\u5fae\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u4e0a\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u80fd\u4f1a\u4ea7\u751f\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u89e3\u51b3\u4e86\u7f29\u5c0f\u533b\u5b66\u5f71\u50cf\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u8ba1\u7b97\u6548\u7387\u540c\u65f6\u4fdd\u7559\u4e34\u5e8a\u76f8\u5173\u7279\u5f81\u7684\u6311\u6218\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86 MedVAE\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u516d\u4e2a\u5927\u578b 2D \u548c 3D \u81ea\u52a8\u7f16\u7801\u5668\u7ec4\u6210\u7684\u7cfb\u5217\uff0c\u80fd\u591f\u5c06\u533b\u5b66\u5f71\u50cf\u7f16\u7801\u4e3a\u7f29\u5c0f\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u5c06\u6f5c\u5728\u8868\u793a\u89e3\u7801\u56de\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528 1,052,730 \u5f20\u533b\u5b66\u5f71\u50cf\u6765\u8bad\u7ec3 MedVAE \u81ea\u52a8\u7f16\u7801\u5668\u3002\u5728\u4ece 20 \u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u83b7\u5f97\u7684\u4e0d\u540c\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 (1) \u5728\u8bad\u7ec3\u4e0b\u6e38\u6a21\u578b\u65f6\uff0c\u5229\u7528 MedVAE \u6f5c\u5728\u8868\u793a\u4ee3\u66ff\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\u53ef\u4ee5\u5e26\u6765\u6548\u7387\u4f18\u52bf\uff08\u541e\u5410\u91cf\u63d0\u9ad8\u9ad8\u8fbe 70 \u500d\uff09\uff0c\u540c\u65f6\u4fdd\u7559\u4e34\u5e8a\u76f8\u5173\u7279\u5f81\uff1b(2) MedVAE \u53ef\u4ee5\u5c06\u6f5c\u5728\u8868\u793a\u89e3\u7801\u56de\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\uff0c\u4e14\u4fdd\u771f\u5ea6\u9ad8\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u5927\u89c4\u6a21\u3001\u53ef\u63a8\u5e7f\u7684\u81ea\u52a8\u7f16\u7801\u5668\u53ef\u4ee5\u5e2e\u52a9\u89e3\u51b3\u533b\u5b66\u9886\u57df\u7684\u91cd\u5927\u6548\u7387\u6311\u6218\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/StanfordMIMI/MedVAE \u83b7\u5f97\u3002", "author": "Maya Varma et.al.", "authors": "Maya Varma, Ashwin Kumar, Rogier van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay Chaudhari", "id": "2502.14753v1", "paper_url": "http://arxiv.org/abs/2502.14753v1", "repo": "null"}}