{"2406.18043": {"publish_time": "2024-06-26", "title": "Multimodal foundation world models for generalist embodied agents", "paper_summary": "Learning generalist embodied agents, able to solve multitudes of tasks in\ndifferent domains is a long-standing problem. Reinforcement learning (RL) is\nhard to scale up as it requires a complex reward design for each task. In\ncontrast, language can specify tasks in a more natural way. Current foundation\nvision-language models (VLMs) generally require fine-tuning or other\nadaptations to be functional, due to the significant domain gap. However, the\nlack of multimodal data in such domains represents an obstacle toward\ndeveloping foundation models for embodied applications. In this work, we\novercome these problems by presenting multimodal foundation world models, able\nto connect and align the representation of foundation VLMs with the latent\nspace of generative world models for RL, without any language annotations. The\nresulting agent learning framework, GenRL, allows one to specify tasks through\nvision and/or language prompts, ground them in the embodied domain's dynamics,\nand learns the corresponding behaviors in imagination. As assessed through\nlarge-scale multi-task benchmarking, GenRL exhibits strong multi-task\ngeneralization performance in several locomotion and manipulation domains.\nFurthermore, by introducing a data-free RL strategy, it lays the groundwork for\nfoundation model-based RL for generalist embodied agents.", "paper_summary_zh": "\u5b78\u7fd2\u901a\u624d\u5177\u8c61\u4ee3\u7406\uff0c\u80fd\u5920\u89e3\u6c7a\u4e0d\u540c\u9818\u57df\u7684\u773e\u591a\u4efb\u52d9\u662f\u4e00\u500b\u9577\u4e45\u5b58\u5728\u7684\u554f\u984c\u3002\u5f37\u5316\u5b78\u7fd2 (RL) \u96e3\u4ee5\u64f4\u5c55\uff0c\u56e0\u70ba\u5b83\u9700\u8981\u91dd\u5c0d\u6bcf\u9805\u4efb\u52d9\u8a2d\u8a08\u8907\u96dc\u7684\u734e\u52f5\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u8a9e\u8a00\u53ef\u4ee5\u7528\u66f4\u81ea\u7136\u7684\u65b9\u5f0f\u6307\u5b9a\u4efb\u52d9\u3002\u7531\u65bc\u5b58\u5728\u986f\u8457\u7684\u9818\u57df\u5dee\u8ddd\uff0c\u76ee\u524d\u7684\u57fa\u790e\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u901a\u5e38\u9700\u8981\u5fae\u8abf\u6216\u5176\u4ed6\u9069\u61c9\u624d\u80fd\u767c\u63ee\u529f\u80fd\u3002\u7136\u800c\uff0c\u6b64\u985e\u9818\u57df\u4e2d\u7f3a\u4e4f\u591a\u6a21\u614b\u6578\u64da\u4ee3\u8868\u4e86\u958b\u767c\u5177\u8c61\u61c9\u7528\u57fa\u790e\u6a21\u578b\u7684\u969c\u7919\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u901a\u904e\u5c55\u793a\u591a\u6a21\u614b\u57fa\u790e\u4e16\u754c\u6a21\u578b\u4f86\u514b\u670d\u9019\u4e9b\u554f\u984c\uff0c\u80fd\u5920\u5c07\u57fa\u790e VLM \u7684\u8868\u793a\u8207 RL \u751f\u6210\u4e16\u754c\u6a21\u578b\u7684\u6f5b\u5728\u7a7a\u9593\u9023\u63a5\u4e26\u5c0d\u9f4a\uff0c\u800c\u7121\u9700\u4efb\u4f55\u8a9e\u8a00\u8a3b\u89e3\u3002\u7531\u6b64\u7522\u751f\u7684\u4ee3\u7406\u5b78\u7fd2\u6846\u67b6 GenRL \u5141\u8a31\u4eba\u5011\u901a\u904e\u8996\u89ba\u548c/\u6216\u8a9e\u8a00\u63d0\u793a\u6307\u5b9a\u4efb\u52d9\uff0c\u5c07\u5b83\u5011\u7f6e\u65bc\u5177\u8c61\u9818\u57df\u7684\u52d5\u614b\u4e2d\uff0c\u4e26\u5728\u60f3\u50cf\u4e2d\u5b78\u7fd2\u76f8\u61c9\u7684\u884c\u70ba\u3002\u6b63\u5982\u901a\u904e\u5927\u898f\u6a21\u591a\u4efb\u52d9\u57fa\u6e96\u6e2c\u8a66\u6240\u8a55\u4f30\u7684\u90a3\u6a23\uff0cGenRL \u5728\u5e7e\u500b\u904b\u52d5\u548c\u64cd\u4f5c\u9818\u57df\u8868\u73fe\u51fa\u5f37\u52c1\u7684\u591a\u4efb\u52d9\u6cdb\u5316\u6027\u80fd\u3002\u6b64\u5916\uff0c\u901a\u904e\u5f15\u5165\u7121\u6578\u64da RL \u7b56\u7565\uff0c\u5b83\u70ba\u901a\u624d\u5177\u8c61\u4ee3\u7406\u7684\u57fa\u65bc\u57fa\u790e\u6a21\u578b\u7684 RL \u5960\u5b9a\u4e86\u57fa\u790e\u3002", "author": "Pietro Mazzaglia et.al.", "authors": "Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, Sai Rajeswar", "id": "2406.18043v1", "paper_url": "http://arxiv.org/abs/2406.18043v1", "repo": "null"}}