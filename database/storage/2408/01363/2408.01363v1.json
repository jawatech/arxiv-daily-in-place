{"2408.01363": {"publish_time": "2024-08-02", "title": "Toward Automatic Relevance Judgment using Vision--Language Models for Image--Text Retrieval Evaluation", "paper_summary": "Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5df2\u5728\u5404\u7a2e\u61c9\u7528\u4e2d\u5c55\u73fe\u6210\u529f\uff0c\u4f46\u5b83\u5011\u5728\u5354\u52a9\u76f8\u95dc\u6027\u5224\u65b7\u65b9\u9762\u7684\u6f5b\u529b\u4ecd\u4e0d\u78ba\u5b9a\u3002\u672c\u6587\u8a55\u4f30\u4e86 VLM \u7684\u76f8\u95dc\u6027\u4f30\u8a08\u80fd\u529b\uff0c\u5305\u62ec CLIP\u3001LLaVA \u548c GPT-4V\uff0c\u5728\u5c08\u70ba\u96f6\u6b21\u5b78\u7fd2\u6a21\u5f0f\u7684\u591a\u5a92\u9ad4\u5167\u5bb9\u5275\u4f5c\u91cf\u8eab\u6253\u9020\u7684\u5927\u898f\u6a21\u300c\u81e8\u6642\u300d\u6aa2\u7d22\u4efb\u52d9\u4e2d\u3002\u521d\u6b65\u5be6\u9a57\u63ed\u793a\u4e86\u4ee5\u4e0b\u5167\u5bb9\uff1a(1) \u5305\u542b\u958b\u6e90\u548c\u9589\u6e90\u8996\u89ba\u6307\u4ee4\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684 LLaVA \u548c GPT-4V\uff0c\u8207\u4eba\u985e\u76f8\u95dc\u6027\u5224\u65b7\u76f8\u6bd4\uff0c\u9054\u5230\u4e86\u986f\u8457\u7684 Kendall's $\\tau \\sim 0.4$\uff0c\u8d85\u8d8a\u4e86 CLIPScore \u6307\u6a19\u3002(2) \u96d6\u7136\u5f37\u70c8\u504f\u597d CLIPScore\uff0c\u4f46 LLM \u5c0d\u57fa\u65bc CLIP \u7684\u6aa2\u7d22\u7cfb\u7d71\u7684\u504f\u898b\u8f03\u5c0f\u3002(3) GPT-4V \u7684\u5206\u6578\u5206\u4f48\u8207\u4eba\u985e\u5224\u65b7\u66f4\u70ba\u4e00\u81f4\uff0c\u9054\u5230\u7d04 0.08 \u7684 Cohen's $\\kappa$ \u503c\uff0c\u512a\u65bc CLIPScore \u7684\u7d04 -0.096\u3002\u9019\u4e9b\u767c\u73fe\u5f37\u8abf\u4e86 LLM \u9a45\u52d5\u7684 VLM \u5728\u589e\u5f37\u76f8\u95dc\u6027\u5224\u65b7\u65b9\u9762\u7684\u6f5b\u529b\u3002", "author": "Jheng-Hong Yang et.al.", "authors": "Jheng-Hong Yang, Jimmy Lin", "id": "2408.01363v1", "paper_url": "http://arxiv.org/abs/2408.01363v1", "repo": "null"}}