{"2409.16040": {"publish_time": "2024-09-24", "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts", "paper_summary": "Deep learning for time series forecasting has seen significant advancements\nover the past decades. However, despite the success of large-scale pre-training\nin language and vision domains, pre-trained time series models remain limited\nin scale and operate at a high cost, hindering the development of larger\ncapable forecasting models in real-world applications. In response, we\nintroduce Time-MoE, a scalable and unified architecture designed to pre-train\nlarger, more capable forecasting foundation models while reducing inference\ncosts. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE\nenhances computational efficiency by activating only a subset of networks for\neach prediction, reducing computational load while maintaining high model\ncapacity. This allows Time-MoE to scale effectively without a corresponding\nincrease in inference costs. Time-MoE comprises a family of decoder-only\ntransformer models that operate in an auto-regressive manner and support\nflexible forecasting horizons with varying input context lengths. We\npre-trained these models on our newly introduced large-scale data Time-300B,\nwhich spans over 9 domains and encompassing over 300 billion time points. For\nthe first time, we scaled a time series foundation model up to 2.4 billion\nparameters, achieving significantly improved forecasting precision. Our results\nvalidate the applicability of scaling laws for training tokens and model size\nin the context of time series forecasting. Compared to dense models with the\nsame number of activated parameters or equivalent computation budgets, our\nmodels consistently outperform them by large margin. These advancements\nposition Time-MoE as a state-of-the-art solution for tackling real-world time\nseries forecasting challenges with superior capability, efficiency, and\nflexibility.", "paper_summary_zh": "<paragraph>\u5728\u904e\u53bb\u7684\u5e7e\u5341\u5e74\u4e2d\uff0c\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u7684\u6df1\u5ea6\u5b78\u7fd2\u6709\u4e86\u986f\u8457\u7684\u9032\u5c55\u3002\u7136\u800c\uff0c\u5118\u7ba1\u5728\u8a9e\u8a00\u548c\u8996\u89ba\u9818\u57df\u53d6\u5f97\u4e86\u5927\u898f\u6a21\u9810\u8a13\u7df4\u7684\u6210\u529f\uff0c\u4f46\u9810\u8a13\u7df4\u7684\u6642\u9593\u5e8f\u5217\u6a21\u578b\u5728\u898f\u6a21\u4e0a\u4ecd\u7136\u6709\u9650\uff0c\u4e26\u4e14\u904b\u4f5c\u6210\u672c\u5f88\u9ad8\uff0c\u963b\u7919\u4e86\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u958b\u767c\u66f4\u5927\u80fd\u529b\u7684\u9810\u6e2c\u6a21\u578b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 Time-MoE\uff0c\u9019\u662f\u4e00\u500b\u53ef\u64f4\u5145\u4e14\u7d71\u4e00\u7684\u67b6\u69cb\uff0c\u65e8\u5728\u9810\u8a13\u7df4\u66f4\u5927\u3001\u66f4\u6709\u80fd\u529b\u7684\u9810\u6e2c\u57fa\u790e\u6a21\u578b\uff0c\u540c\u6642\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002\u900f\u904e\u5229\u7528\u7a00\u758f\u7684\u6df7\u5408\u5c08\u5bb6 (MoE) \u8a2d\u8a08\uff0cTime-MoE \u50c5\u91dd\u5c0d\u6bcf\u500b\u9810\u6e2c\u555f\u7528\u4e00\u500b\u5b50\u7db2\u8def\uff0c\u9032\u800c\u63d0\u9ad8\u904b\u7b97\u6548\u7387\uff0c\u5728\u7dad\u6301\u9ad8\u6a21\u578b\u5bb9\u91cf\u7684\u540c\u6642\u964d\u4f4e\u904b\u7b97\u8ca0\u8f09\u3002\u9019\u8b93 Time-MoE \u80fd\u5920\u6709\u6548\u64f4\u5145\uff0c\u800c\u63a8\u7406\u6210\u672c\u4e0d\u6703\u96a8\u4e4b\u589e\u52a0\u3002Time-MoE \u5305\u542b\u4e00\u7cfb\u5217\u50c5\u89e3\u78bc\u5668\u7684Transformer\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u4ee5\u81ea\u8ff4\u6b78\u7684\u65b9\u5f0f\u904b\u4f5c\uff0c\u4e26\u652f\u63f4\u5177\u6709\u4e0d\u540c\u8f38\u5165\u5167\u5bb9\u9577\u5ea6\u7684\u5f48\u6027\u9810\u6e2c\u7bc4\u570d\u3002\u6211\u5011\u5728\u6211\u5011\u65b0\u63a8\u51fa\u7684 Time-300B \u5927\u578b\u8cc7\u6599\u96c6\u4e0a\u9810\u8a13\u7df4\u4e86\u9019\u4e9b\u6a21\u578b\uff0c\u8a72\u8cc7\u6599\u96c6\u6db5\u84cb\u4e86 9 \u500b\u9818\u57df\uff0c\u5305\u542b\u8d85\u904e 3000 \u5104\u500b\u6642\u9593\u9ede\u3002\u6211\u5011\u9996\u6b21\u5c07\u6642\u9593\u5e8f\u5217\u57fa\u790e\u6a21\u578b\u64f4\u5145\u5230 24 \u5104\u500b\u53c3\u6578\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u9810\u6e2c\u7cbe\u5ea6\u3002\u6211\u5011\u7684\u7d50\u679c\u9a57\u8b49\u4e86\u5728\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u4e2d\uff0c\u8a13\u7df4 token \u548c\u6a21\u578b\u5927\u5c0f\u7684\u898f\u6a21\u5b9a\u5f8b\u7684\u9069\u7528\u6027\u3002\u8207\u5177\u6709\u76f8\u540c\u6578\u91cf\u5df2\u555f\u7528\u53c3\u6578\u6216\u7b49\u6548\u904b\u7b97\u9810\u7b97\u7684\u7a20\u5bc6\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u59cb\u7d42\u4ee5\u5f88\u5927\u7684\u5e45\u5ea6\u512a\u65bc\u5b83\u5011\u3002\u9019\u4e9b\u9032\u5c55\u8b93 Time-MoE \u6210\u70ba\u89e3\u6c7a\u5be6\u969b\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u6311\u6230\u7684\u6700\u65b0\u89e3\u6c7a\u65b9\u6848\uff0c\u5177\u5099\u5353\u8d8a\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u9748\u6d3b\u6027\u3002</paragraph>", "author": "Xiaoming Shi et.al.", "authors": "Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin", "id": "2409.16040v1", "paper_url": "http://arxiv.org/abs/2409.16040v1", "repo": "https://github.com/time-moe/time-moe"}}