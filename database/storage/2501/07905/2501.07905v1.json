{"2501.07905": {"publish_time": "2025-01-14", "title": "Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence Modeling for Resource-Constrained Environments", "paper_summary": "Long-range sequence modeling is a crucial aspect of natural language\nprocessing and time series analysis. However, traditional models like Recurrent\nNeural Networks (RNNs) and Transformers suffer from computational and memory\ninefficiencies, especially when dealing with long sequences. This paper\nintroduces Logarithmic Memory Networks (LMNs), a novel architecture that\nleverages a hierarchical logarithmic tree structure to efficiently store and\nretrieve past information. LMNs dynamically summarize historical context,\nsignificantly reducing the memory footprint and computational complexity of\nattention mechanisms from O(n2) to O(log(n)). The model employs a\nsingle-vector, targeted attention mechanism to access stored information, and\nthe memory block construction worker (summarizer) layer operates in two modes:\na parallel execution mode during training for efficient processing of\nhierarchical tree structures and a sequential execution mode during inference,\nwhich acts as a memory management system. It also implicitly encodes positional\ninformation, eliminating the need for explicit positional encodings. These\nfeatures make LMNs a robust and scalable solution for processing long-range\nsequences in resource-constrained environments, offering practical improvements\nin efficiency and scalability. The code is publicly available under the MIT\nLicense on GitHub: https://github.com/AhmedBoin/LogarithmicMemory.", "paper_summary_zh": "\u9577\u7a0b\u5e8f\u5217\u5efa\u6a21\u662f\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u548c\u6642\u9593\u5e8f\u5217\u5206\u6790\u4e2d\u81f3\u95dc\u91cd\u8981\u7684\u4e00\u9762\u3002\u7136\u800c\uff0c\u50b3\u7d71\u6a21\u578b\uff0c\u4f8b\u5982\u5faa\u74b0\u795e\u7d93\u7db2\u8def (RNN) \u548c Transformer\uff0c\u5728\u8a08\u7b97\u548c\u8a18\u61b6\u65b9\u9762\u6548\u7387\u4f4e\u843d\uff0c\u7279\u5225\u662f\u5728\u8655\u7406\u9577\u5e8f\u5217\u6642\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u5c0d\u6578\u8a18\u61b6\u7db2\u8def (LMN)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u5206\u5c64\u5c0d\u6578\u6a39\u7d50\u69cb\u6709\u6548\u5730\u5132\u5b58\u548c\u64f7\u53d6\u904e\u53bb\u7684\u8cc7\u8a0a\u3002LMN \u52d5\u614b\u5730\u7e3d\u7d50\u6b77\u53f2\u8108\u7d61\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u8a18\u61b6\u9ad4\u4f54\u7528\u7a7a\u9593\u548c\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u8a08\u7b97\u8907\u96dc\u5ea6\uff0c\u5f9e O(n2) \u964d\u81f3 O(log(n))\u3002\u8a72\u6a21\u578b\u63a1\u7528\u55ae\u4e00\u5411\u91cf\u3001\u6709\u91dd\u5c0d\u6027\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u4f86\u5b58\u53d6\u5132\u5b58\u7684\u8cc7\u8a0a\uff0c\u800c\u8a18\u61b6\u9ad4\u5340\u584a\u5efa\u69cb\u5668 (\u6458\u8981\u5668) \u5c64\u5728\u5169\u7a2e\u6a21\u5f0f\u4e0b\u904b\u4f5c\uff1a\u8a13\u7df4\u671f\u9593\u7684\u4e26\u884c\u57f7\u884c\u6a21\u5f0f\uff0c\u7528\u65bc\u6709\u6548\u8655\u7406\u5206\u5c64\u6a39\u72c0\u7d50\u69cb\uff0c\u4ee5\u53ca\u63a8\u8ad6\u671f\u9593\u7684\u9806\u5e8f\u57f7\u884c\u6a21\u5f0f\uff0c\u4f5c\u70ba\u8a18\u61b6\u9ad4\u7ba1\u7406\u7cfb\u7d71\u3002\u5b83\u4e5f\u96b1\u542b\u5730\u7de8\u78bc\u4f4d\u7f6e\u8cc7\u8a0a\uff0c\u6d88\u9664\u4e86\u5c0d\u660e\u78ba\u4f4d\u7f6e\u7de8\u78bc\u7684\u9700\u6c42\u3002\u9019\u4e9b\u7279\u9ede\u4f7f LMN \u6210\u70ba\u5728\u8cc7\u6e90\u53d7\u9650\u74b0\u5883\u4e2d\u8655\u7406\u9577\u7a0b\u5e8f\u5217\u7684\u5f37\u5927\u4e14\u53ef\u64f4\u5145\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5728\u6548\u7387\u548c\u53ef\u64f4\u5145\u6027\u65b9\u9762\u63d0\u4f9b\u4e86\u5be6\u969b\u7684\u6539\u9032\u3002\u7a0b\u5f0f\u78bc\u5728 GitHub \u4e0a\u4ee5 MIT \u6388\u6b0a\u516c\u958b\uff1ahttps://github.com/AhmedBoin/LogarithmicMemory\u3002", "author": "Mohamed A. Taha et.al.", "authors": "Mohamed A. Taha", "id": "2501.07905v1", "paper_url": "http://arxiv.org/abs/2501.07905v1", "repo": "https://github.com/ahmedboin/logarithmicmemory"}}