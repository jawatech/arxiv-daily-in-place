{"2405.20180": {"publish_time": "2024-05-30", "title": "Transformers and Slot Encoding for Sample Efficient Physical World Modelling", "paper_summary": "World modelling, i.e. building a representation of the rules that govern the\nworld so as to predict its evolution, is an essential ability for any agent\ninteracting with the physical world. Recent applications of the Transformer\narchitecture to the problem of world modelling from video input show notable\nimprovements in sample efficiency. However, existing approaches tend to work\nonly at the image level thus disregarding that the environment is composed of\nobjects interacting with each other. In this paper, we propose an architecture\ncombining Transformers for world modelling with the slot-attention paradigm, an\napproach for learning representations of objects appearing in a scene. We\ndescribe the resulting neural architecture and report experimental results\nshowing an improvement over the existing solutions in terms of sample\nefficiency and a reduction of the variation of the performance over the\ntraining examples. The code for our architecture and experiments is available\nat https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm", "paper_summary_zh": "\u4e16\u754c\u5efa\u6a21\uff0c\u5373\u6784\u5efa\u652f\u914d\u4e16\u754c\u7684\u89c4\u5219\u7684\u8868\u793a\uff0c\u4ee5\u4fbf\u9884\u6d4b\u5176\u6f14\u53d8\uff0c\u662f\u4efb\u4f55\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u4ee3\u7406\u7684\u57fa\u672c\u80fd\u529b\u3002Transformer \u67b6\u6784\u6700\u8fd1\u5e94\u7528\u4e8e\u4ece\u89c6\u9891\u8f93\u5165\u4e2d\u8fdb\u884c\u4e16\u754c\u5efa\u6a21\u7684\u95ee\u9898\uff0c\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u663e\u793a\u51fa\u663e\u7740\u6539\u8fdb\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u53ea\u5728\u56fe\u50cf\u7ea7\u522b\u5de5\u4f5c\uff0c\u56e0\u6b64\u5ffd\u7565\u4e86\u73af\u5883\u662f\u7531\u76f8\u4e92\u4ea4\u4e92\u7684\u5bf9\u8c61\u7ec4\u6210\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\uff0c\u5c06\u7528\u4e8e\u4e16\u754c\u5efa\u6a21\u7684 Transformer \u4e0e\u63d2\u69fd\u6ce8\u610f\u529b\u8303\u4f8b\u76f8\u7ed3\u5408\uff0c\u8fd9\u662f\u4e00\u79cd\u5b66\u4e60\u573a\u666f\u4e2d\u51fa\u73b0\u7684\u5bf9\u8c61\u8868\u793a\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u63cf\u8ff0\u4e86\u7531\u6b64\u4ea7\u751f\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u5e76\u62a5\u544a\u4e86\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8868\u660e\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u51cf\u5c11\u4e86\u8bad\u7ec3\u793a\u4f8b\u4e2d\u6027\u80fd\u7684\u53d8\u5316\u3002\u6211\u4eec\u67b6\u6784\u548c\u5b9e\u9a8c\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm \u83b7\u5f97", "author": "Francesco Petri et.al.", "authors": "Francesco Petri, Luigi Asprino, Aldo Gangemi", "id": "2405.20180v1", "paper_url": "http://arxiv.org/abs/2405.20180v1", "repo": "https://github.com/torchipeppo/transformers-and-slot-encoding-for-wm"}}