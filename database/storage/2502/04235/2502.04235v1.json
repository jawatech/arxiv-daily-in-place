{"2502.04235": {"publish_time": "2025-02-06", "title": "MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion", "paper_summary": "Despite the remarkable capabilities of large language models across various\ntasks, their continued scaling faces a critical challenge: the scarcity of\nhigh-quality pretraining data. While model architectures continue to evolve,\nthe natural language data struggles to scale up. To tackle this bottleneck, we\npropose \\textbf{MA}ssive \\textbf{G}enre-\\textbf{A}udience~(MAGA) reformulation\nmethod, which systematic synthesizes diverse, contextually-rich pretraining\ndata from existing corpus. This work makes three main contributions: (1) We\npropose MAGA reformulation method, a lightweight and scalable approach for\npretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We\nevaluate MAGACorpus with different data budget scaling strategies,\ndemonstrating consistent improvements across various model sizes (134M-13B),\nestablishing the necessity for next-generation large-scale synthetic\npretraining language models. (3) Through comprehensive analysis, we investigate\nprompt engineering's impact on synthetic training collapse and reveal\nlimitations in conventional collapse detection metrics using validation losses.\nOur work shows that MAGA can substantially expand training datasets while\nmaintaining quality, offering a reliably pathway for scaling models beyond data\nlimitations.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4f46\u6301\u7e8c\u64f4\u5145\u537b\u9762\u81e8\u4e00\u9805\u56b4\u5cfb\u7684\u6311\u6230\uff1a\u7f3a\u4e4f\u9ad8\u54c1\u8cea\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u3002\u96d6\u7136\u6a21\u578b\u67b6\u69cb\u6301\u7e8c\u6f14\u9032\uff0c\u4f46\u81ea\u7136\u8a9e\u8a00\u8cc7\u6599\u537b\u96e3\u4ee5\u64f4\u5145\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u74f6\u9838\uff0c\u6211\u5011\u63d0\u51fa**M**assive **G**enre-**A**udience~(MAGA) \u6539\u5beb\u65b9\u6cd5\uff0c\u6709\u7cfb\u7d71\u5730\u7d9c\u5408\u4f86\u81ea\u73fe\u6709\u8a9e\u6599\u5eab\u7684\u591a\u5143\u4e14\u8108\u7d61\u8c50\u5bcc\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u3002\u9019\u9805\u5de5\u4f5c\u6709\u4e09\u500b\u4e3b\u8981\u8ca2\u737b\uff1a(1) \u6211\u5011\u63d0\u51fa MAGA \u6539\u5beb\u65b9\u6cd5\uff0c\u9019\u662f\u4e00\u7a2e\u8f15\u91cf\u4e14\u53ef\u64f4\u5145\u7684\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u64f4\u5145\u65b9\u6cd5\uff0c\u4e26\u5efa\u7acb\u4e00\u500b 770B \u500b token \u7684 MAGACorpus\u3002(2) \u6211\u5011\u4f7f\u7528\u4e0d\u540c\u7684\u8cc7\u6599\u9810\u7b97\u64f4\u5145\u7b56\u7565\u8a55\u4f30 MAGACorpus\uff0c\u8b49\u660e\u5404\u7a2e\u6a21\u578b\u5927\u5c0f(134M-13B) \u90fd\u6301\u7e8c\u63d0\u5347\uff0c\u78ba\u7acb\u4e86\u4e0b\u4e00\u4ee3\u5927\u898f\u6a21\u5408\u6210\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u7684\u5fc5\u8981\u6027\u3002(3) \u900f\u904e\u5168\u9762\u7684\u5206\u6790\uff0c\u6211\u5011\u63a2\u8a0e\u63d0\u793a\u5de5\u7a0b\u5c0d\u5408\u6210\u8a13\u7df4\u5d29\u6f70\u7684\u5f71\u97ff\uff0c\u4e26\u63ed\u9732\u4f7f\u7528\u9a57\u8b49\u640d\u5931\u9032\u884c\u50b3\u7d71\u5d29\u6f70\u6aa2\u6e2c\u6307\u6a19\u7684\u9650\u5236\u3002\u6211\u5011\u7684\u7814\u7a76\u986f\u793a\uff0cMAGA \u80fd\u5920\u5927\u5e45\u64f4\u5145\u8a13\u7df4\u8cc7\u6599\u96c6\uff0c\u540c\u6642\u7dad\u6301\u54c1\u8cea\uff0c\u63d0\u4f9b\u4e00\u7a2e\u53ef\u9760\u7684\u8def\u5f91\uff0c\u8b93\u6a21\u578b\u5728\u8d85\u8d8a\u8cc7\u6599\u9650\u5236\u7684\u540c\u6642\u9032\u884c\u64f4\u5145\u3002", "author": "Xintong Hao et.al.", "authors": "Xintong Hao, Ke Shen, Chenggang Li", "id": "2502.04235v1", "paper_url": "http://arxiv.org/abs/2502.04235v1", "repo": "null"}}