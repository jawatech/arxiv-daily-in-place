{"2408.07055": {"publish_time": "2024-08-13", "title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs", "paper_summary": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.", "paper_summary_zh": "<paragraph>\u73fe\u4eca\u9577\u8a9e\u5883\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u8655\u7406\u591a\u9054 100,000 \u500b\u8a5e\u5f59\u7684\u8f38\u5165\uff0c\u4f46\u4ecd\u96e3\u4ee5\u7522\u751f\u8d85\u904e 2,000 \u5b57\u7684\u9069\u5ea6\u9577\u5ea6\u8f38\u51fa\u3002\u900f\u904e\u53d7\u63a7\u5be6\u9a57\uff0c\u6211\u5011\u767c\u73fe\u6a21\u578b\u7684\u6709\u6548\u7522\u751f\u9577\u5ea6\u672c\u8cea\u4e0a\u53d7\u5230\u5728\u76e3\u7763\u5fae\u8abf (SFT) \u671f\u9593\u6240\u898b\u7bc4\u4f8b\u7684\u9650\u5236\u3002\u63db\u53e5\u8a71\u8aaa\uff0c\u4ed6\u5011\u7684\u8f38\u51fa\u9650\u5236\u662f\u56e0\u73fe\u6709 SFT \u8cc7\u6599\u96c6\u4e2d\u7f3a\u4e4f\u9577\u8f38\u51fa\u7bc4\u4f8b\u6240\u81f4\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 AgentWrite\uff0c\u4e00\u500b\u57fa\u65bc\u4ee3\u7406\u7684\u7ba1\u9053\uff0c\u5c07\u8d85\u9577\u7522\u751f\u4efb\u52d9\u5206\u89e3\u6210\u5b50\u4efb\u52d9\uff0c\u4f7f\u73fe\u6210\u7684 LLM \u80fd\u5920\u7522\u751f\u8d85\u904e 20,000 \u5b57\u7684\u76f8\u5e72\u8f38\u51fa\u3002\u5229\u7528 AgentWrite\uff0c\u6211\u5011\u69cb\u5efa\u4e86 LongWriter-6k\uff0c\u4e00\u500b\u5305\u542b 6,000 \u500b SFT \u8cc7\u6599\u7684\u8cc7\u6599\u96c6\uff0c\u8f38\u51fa\u9577\u5ea6\u5f9e 2k \u5230 32k \u5b57\u4e0d\u7b49\u3002\u900f\u904e\u5c07\u6b64\u8cc7\u6599\u96c6\u7d0d\u5165\u6a21\u578b\u8a13\u7df4\u4e2d\uff0c\u6211\u5011\u6210\u529f\u5730\u5c07\u73fe\u6709\u6a21\u578b\u7684\u8f38\u51fa\u9577\u5ea6\u64f4\u5c55\u5230\u8d85\u904e 10,000 \u5b57\uff0c\u540c\u6642\u7dad\u6301\u8f38\u51fa\u54c1\u8cea\u3002\u6211\u5011\u4e5f\u958b\u767c\u4e86 LongBench-Write\uff0c\u4e00\u500b\u8a55\u4f30\u8d85\u9577\u7522\u751f\u80fd\u529b\u7684\u7d9c\u5408\u57fa\u6e96\u3002\u6211\u5011\u900f\u904e DPO \u9032\u4e00\u6b65\u6539\u9032\u7684 9B \u53c3\u6578\u6a21\u578b\uff0c\u5728\u6b64\u57fa\u6e96\u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u66f4\u5927\u898f\u6a21\u7684\u5c08\u6709\u6a21\u578b\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u7814\u7a76\u8b49\u660e\u73fe\u6709\u7684\u9577\u8a9e\u5883 LLM \u5df2\u5177\u5099\u66f4\u5927\u8f38\u51fa\u8996\u7a97\u7684\u6f5b\u529b\u2014\u2014\u53ea\u8981\u5728\u6a21\u578b\u6bd4\u5c0d\u671f\u9593\u63d0\u4f9b\u5177\u6709\u5ef6\u4f38\u8f38\u51fa\u7684\u8cc7\u6599\uff0c\u5c31\u80fd\u89e3\u9396\u6b64\u80fd\u529b\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u4f4d\u65bc\uff1ahttps://github.com/THUDM/LongWriter\u3002</paragraph>", "author": "Yushi Bai et.al.", "authors": "Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li", "id": "2408.07055v1", "paper_url": "http://arxiv.org/abs/2408.07055v1", "repo": "https://github.com/thudm/longwriter"}}