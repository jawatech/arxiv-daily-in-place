{"2409.17958": {"publish_time": "2024-09-26", "title": "The Hard Positive Truth about Vision-Language Compositionality", "paper_summary": "Several benchmarks have concluded that our best vision-language models (e.g.,\nCLIP) are lacking in compositionality. Given an image, these benchmarks probe a\nmodel's ability to identify its associated caption amongst a set of\ncompositional distractors. In response, a surge of recent proposals show\nimprovements by finetuning CLIP with distractors as hard negatives. Our\ninvestigations reveal that these improvements have, in fact, been significantly\noverstated -- because existing benchmarks do not probe whether finetuned\nvision-language models remain invariant to hard positives. By curating an\nevaluation dataset with 112,382 hard negatives and hard positives, we uncover\nthat including hard positives decreases CLIP's performance by 12.9%, while\nhumans perform effortlessly at 99%. CLIP finetuned with hard negatives results\nin an even larger decrease, up to 38.7%. With this finding, we then produce a\n1,775,259 image-text training set with both hard negative and hard positive\ncaptions. By training with both, we see improvements on existing benchmarks\nwhile simultaneously improving performance on hard positives, indicating a more\nrobust improvement in compositionality. Our work suggests the need for future\nresearch to rigorously test and improve CLIP's understanding of semantic\nrelationships between related \"positive\" concepts.", "paper_summary_zh": "\u591a\u9805\u57fa\u6e96\u5df2\u5f97\u51fa\u7d50\u8ad6\uff0c\u6211\u5011\u7684\u6700\u4f73\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (\u4f8b\u5982 CLIP) \u7f3a\u4e4f\u7d44\u6210\u6027\u3002\u7d66\u5b9a\u4e00\u5f35\u5716\u50cf\uff0c\u9019\u4e9b\u57fa\u6e96\u6703\u63a2\u8a0e\u6a21\u578b\u5728\u7d44\u6210\u5f0f\u5e72\u64fe\u9805\u4e2d\u8b58\u5225\u5176\u95dc\u806f\u6a19\u984c\u7684\u80fd\u529b\u3002\u4f5c\u70ba\u56de\u61c9\uff0c\u6700\u8fd1\u7684\u4e00\u7cfb\u5217\u63d0\u6848\u986f\u793a\uff0c\u901a\u904e\u4f7f\u7528\u5e72\u64fe\u9805\u4f5c\u70ba\u786c\u8ca0\u4f8b\u5c0d CLIP \u9032\u884c\u5fae\u8abf\u53ef\u4ee5\u6539\u5584\u6a21\u578b\u3002\u6211\u5011\u7684\u8abf\u67e5\u986f\u793a\uff0c\u9019\u4e9b\u6539\u9032\u5be6\u969b\u4e0a\u5df2\u88ab\u5927\u5e45\u8a87\u5927\u2014\u2014\u56e0\u70ba\u73fe\u6709\u7684\u57fa\u6e96\u4e26\u672a\u63a2\u8a0e\u7d93\u904e\u5fae\u8abf\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u662f\u5426\u5c0d\u786c\u6b63\u4f8b\u4fdd\u6301\u4e0d\u8b8a\u3002\u901a\u904e\u7b56\u5283\u4e00\u500b\u5305\u542b 112,382 \u500b\u786c\u8ca0\u4f8b\u548c\u786c\u6b63\u4f8b\u7684\u8a55\u4f30\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u767c\u73fe\u5305\u542b\u786c\u6b63\u4f8b\u6703\u4f7f CLIP \u7684\u6548\u80fd\u964d\u4f4e 12.9%\uff0c\u800c\u4eba\u985e\u5247\u6beb\u4e0d\u8cbb\u529b\u5730\u9054\u5230\u4e86 99%\u3002\u4f7f\u7528\u786c\u8ca0\u4f8b\u5fae\u8abf\u7684 CLIP \u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\u5e45\u5ea6\u66f4\u5927\uff0c\u6700\u9ad8\u9054 38.7%\u3002\u6709\u4e86\u9019\u500b\u767c\u73fe\uff0c\u6211\u5011\u63a5\u8457\u88fd\u4f5c\u4e86\u4e00\u500b\u5305\u542b\u786c\u8ca0\u4f8b\u548c\u786c\u6b63\u4f8b\u6a19\u984c\u7684 1,775,259 \u5f35\u5716\u50cf\u6587\u5b57\u8a13\u7df4\u96c6\u3002\u901a\u904e\u540c\u6642\u4f7f\u7528\u9019\u5169\u500b\u6a19\u984c\u9032\u884c\u8a13\u7df4\uff0c\u6211\u5011\u770b\u5230\u73fe\u6709\u57fa\u6e96\u6709\u6240\u6539\u5584\uff0c\u540c\u6642\u4e5f\u6539\u5584\u4e86\u5c0d\u786c\u6b63\u4f8b\u7684\u6548\u80fd\uff0c\u9019\u8868\u793a\u7d44\u6210\u6027\u6709\u4e86\u66f4\u986f\u8457\u7684\u6539\u5584\u3002\u6211\u5011\u7684\u7814\u7a76\u8868\u660e\uff0c\u672a\u4f86\u9700\u8981\u9032\u884c\u56b4\u8b39\u7684\u6e2c\u8a66\u548c\u6539\u9032\uff0c\u4ee5\u4e86\u89e3 CLIP \u5c0d\u76f8\u95dc\u300c\u6b63\u5411\u300d\u6982\u5ff5\u4e4b\u9593\u8a9e\u7fa9\u95dc\u4fc2\u7684\u7406\u89e3\u3002", "author": "Amita Kamath et.al.", "authors": "Amita Kamath, Cheng-Yu Hsieh, Kai-Wei Chang, Ranjay Krishna", "id": "2409.17958v1", "paper_url": "http://arxiv.org/abs/2409.17958v1", "repo": "null"}}