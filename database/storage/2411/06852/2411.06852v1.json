{"2411.06852": {"publish_time": "2024-11-11", "title": "Evaluating Large Language Models on Financial Report Summarization: An Empirical Study", "paper_summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nversatility across various applications, including natural language\nunderstanding, domain-specific knowledge tasks, etc. However, applying LLMs to\ncomplex, high-stakes domains like finance requires rigorous evaluation to\nensure reliability, accuracy, and compliance with industry standards. To\naddress this need, we conduct a comprehensive and comparative study on three\nstate-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their\neffectiveness in generating automated financial reports. Our primary motivation\nis to explore how these models can be harnessed within finance, a field\ndemanding precision, contextual relevance, and robustness against erroneous or\nmisleading information. By examining each model's capabilities, we aim to\nprovide an insightful assessment of their strengths and limitations. Our paper\noffers benchmarks for financial report analysis, encompassing proposed metrics\nsuch as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative\nevaluation framework that integrates both quantitative metrics (e.g.,\nprecision, recall) and qualitative analyses (e.g., contextual fit, consistency)\nto provide a holistic view of each model's output quality. Additionally, we\nmake our financial dataset publicly available, inviting researchers and\npractitioners to leverage, scrutinize, and enhance our findings through broader\ncommunity engagement and collaborative improvement. Our dataset is available on\nhuggingface.", "paper_summary_zh": "<paragraph>\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u61c9\u7528\u4e2d\u5c55\u73fe\u4e86\u975e\u51e1\u7684\u591a\u6a23\u6027\uff0c\u5305\u62ec\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u3001\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\u4efb\u52d9\u7b49\u3002\u7136\u800c\uff0c\u5c07 LLM \u61c9\u7528\u65bc\u91d1\u878d\u7b49\u8907\u96dc\u4e14\u9ad8\u98a8\u96aa\u7684\u9818\u57df\u9700\u8981\u56b4\u683c\u7684\u8a55\u4f30\uff0c\u4ee5\u78ba\u4fdd\u53ef\u9760\u6027\u3001\u6e96\u78ba\u6027\u548c\u7b26\u5408\u7522\u696d\u6a19\u6e96\u3002\u70ba\u4e86\u6eff\u8db3\u9019\u500b\u9700\u6c42\uff0c\u6211\u5011\u5c0d\u4e09\u7a2e\u6700\u5148\u9032\u7684 LLM \u9032\u884c\u4e86\u5168\u9762\u4e14\u6bd4\u8f03\u6027\u7684\u7814\u7a76\uff0c\u5305\u62ec GLM-4\u3001Mistral-NeMo \u548c LLaMA3.1\uff0c\u91cd\u9ede\u5728\u65bc\u5b83\u5011\u751f\u6210\u81ea\u52d5\u5316\u8ca1\u52d9\u5831\u8868\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u9996\u8981\u52d5\u6a5f\u662f\u63a2\u7d22\u5982\u4f55\u5c07\u9019\u4e9b\u6a21\u578b\u61c9\u7528\u65bc\u91d1\u878d\u9818\u57df\uff0c\u9019\u662f\u4e00\u500b\u9700\u8981\u7cbe\u6e96\u6027\u3001\u8108\u7d61\u76f8\u95dc\u6027\u548c\u5c0d\u932f\u8aa4\u6216\u8aa4\u5c0e\u8cc7\u8a0a\u5177\u6709\u7a69\u5065\u6027\u7684\u9818\u57df\u3002\u900f\u904e\u6aa2\u8996\u6bcf\u500b\u6a21\u578b\u7684\u80fd\u529b\uff0c\u6211\u5011\u65e8\u5728\u5c0d\u5b83\u5011\u7684\u512a\u7f3a\u9ede\u63d0\u4f9b\u6df1\u5165\u7684\u8a55\u4f30\u3002\u6211\u5011\u7684\u8ad6\u6587\u63d0\u4f9b\u4e86\u8ca1\u52d9\u5831\u8868\u5206\u6790\u7684\u57fa\u6e96\uff0c\u5305\u542b\u63d0\u8b70\u7684\u6307\u6a19\uff0c\u4f8b\u5982 ROUGE-1\u3001BERT \u5206\u6578\u548c LLM \u5206\u6578\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u5275\u65b0\u7684\u8a55\u4f30\u67b6\u69cb\uff0c\u5b83\u6574\u5408\u4e86\u91cf\u5316\u6307\u6a19\uff08\u4f8b\u5982\uff0c\u7cbe\u6e96\u5ea6\u3001\u53ec\u56de\u7387\uff09\u548c\u5b9a\u6027\u5206\u6790\uff08\u4f8b\u5982\uff0c\u8108\u7d61\u8cbc\u5408\u5ea6\u3001\u4e00\u81f4\u6027\uff09\uff0c\u4ee5\u63d0\u4f9b\u6bcf\u500b\u6a21\u578b\u8f38\u51fa\u54c1\u8cea\u7684\u5168\u8c8c\u3002\u6b64\u5916\uff0c\u6211\u5011\u516c\u958b\u4e86\u6211\u5011\u7684\u8ca1\u52d9\u8cc7\u6599\u96c6\uff0c\u9080\u8acb\u7814\u7a76\u4eba\u54e1\u548c\u5be6\u52d9\u5de5\u4f5c\u8005\u900f\u904e\u66f4\u5ee3\u6cdb\u7684\u793e\u7fa4\u53c3\u8207\u548c\u5354\u4f5c\u6539\u9032\uff0c\u4f86\u5229\u7528\u3001\u5be9\u67e5\u548c\u589e\u5f37\u6211\u5011\u7684\u767c\u73fe\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u53ef\u5728 huggingface \u4e0a\u53d6\u5f97\u3002</paragraph>", "author": "Xinqi Yang et.al.", "authors": "Xinqi Yang, Scott Zang, Yong Ren, Dingjie Peng, Zheng Wen", "id": "2411.06852v1", "paper_url": "http://arxiv.org/abs/2411.06852v1", "repo": "null"}}