{"2409.11650": {"publish_time": "2024-09-18", "title": "Art and Science of Quantizing Large-Scale Models: A Comprehensive Overview", "paper_summary": "This paper provides a comprehensive overview of the principles, challenges,\nand methodologies associated with quantizing large-scale neural network models.\nAs neural networks have evolved towards larger and more complex architectures\nto address increasingly sophisticated tasks, the computational and energy costs\nhave escalated significantly. We explore the necessity and impact of model size\ngrowth, highlighting the performance benefits as well as the computational\nchallenges and environmental considerations. The core focus is on model\nquantization as a fundamental approach to mitigate these challenges by reducing\nmodel size and improving efficiency without substantially compromising\naccuracy. We delve into various quantization techniques, including both\npost-training quantization (PTQ) and quantization-aware training (QAT), and\nanalyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),\nZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine\nhow these methods address issues like outliers, importance weighting, and\nactivation quantization, ultimately contributing to more sustainable and\naccessible deployment of large-scale models.", "paper_summary_zh": "\u672c\u6587\u5168\u9762\u6982\u8ff0\u4e86\u4e0e\u91cf\u5316\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u76f8\u5173\u7684\u539f\u5219\u3001\u6311\u6218\u548c\u65b9\u6cd5\u3002\n\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u5411\u66f4\u5927\u3001\u66f4\u590d\u6742\u7684\u67b6\u6784\u53d1\u5c55\u4ee5\u89e3\u51b3\u65e5\u76ca\u590d\u6742\u7684\u4efb\u52a1\uff0c\u8ba1\u7b97\u548c\u80fd\u6e90\u6210\u672c\u5927\u5e45\u589e\u52a0\u3002\u6211\u4eec\u63a2\u8ba8\u4e86\u6a21\u578b\u89c4\u6a21\u589e\u957f\u7684\u5fc5\u8981\u6027\u548c\u5f71\u54cd\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6027\u80fd\u4f18\u52bf\u4ee5\u53ca\u8ba1\u7b97\u6311\u6218\u548c\u73af\u5883\u8003\u91cf\u3002\u6838\u5fc3\u91cd\u70b9\u662f\u6a21\u578b\u91cf\u5316\uff0c\u4f5c\u4e3a\u4e00\u79cd\u901a\u8fc7\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u548c\u63d0\u9ad8\u6548\u7387\u6765\u7f13\u89e3\u8fd9\u4e9b\u6311\u6218\u7684\u57fa\u672c\u65b9\u6cd5\uff0c\u800c\u4e0d\u4f1a\u5927\u5e45\u964d\u4f4e\u51c6\u786e\u6027\u3002\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u5404\u79cd\u91cf\u5316\u6280\u672f\uff0c\u5305\u62ec\u8bad\u7ec3\u540e\u91cf\u5316 (PTQ) \u548c\u611f\u77e5\u91cf\u5316\u8bad\u7ec3 (QAT)\uff0c\u5e76\u5206\u6790\u4e86\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\uff0c\u4f8b\u5982 LLM-QAT\u3001PEQA(L4Q)\u3001ZeroQuant\u3001SmoothQuant \u7b49\u3002\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5982\u4f55\u89e3\u51b3\u5f02\u5e38\u503c\u3001\u91cd\u8981\u6027\u52a0\u6743\u548c\u6fc0\u6d3b\u91cf\u5316\u7b49\u95ee\u9898\uff0c\u6700\u7ec8\u6709\u52a9\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u66f4\u53ef\u6301\u7eed\u3001\u66f4\u6613\u4e8e\u90e8\u7f72\u3002", "author": "Yanshu Wang et.al.", "authors": "Yanshu Wang, Tong Yang, Xiyan Liang, Guoan Wang, Hanning Lu, Xu Zhe, Yaoming Li, Li Weitao", "id": "2409.11650v1", "paper_url": "http://arxiv.org/abs/2409.11650v1", "repo": "null"}}