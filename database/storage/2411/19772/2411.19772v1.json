{"2411.19772": {"publish_time": "2024-11-29", "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos", "paper_summary": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.", "paper_summary_zh": "\u5118\u7ba1\u5728\u5f71\u7247\u7406\u89e3\u4e0a\u5df2\u6709\u4ee4\u4eba\u9a5a\u8c54\u7684\u9032\u5c55\uff0c\u4f46\u5927\u591a\u6578\u7684\u52aa\u529b\u4ecd\u4fb7\u9650\u65bc\u7c97\u7565\u6216\u50c5\u8996\u89ba\u7684\u5f71\u7247\u4efb\u52d9\u3002\u7136\u800c\uff0c\u771f\u5be6\u4e16\u754c\u7684\u5f71\u7247\u5305\u542b\u5168\u6a21\u614b\u8cc7\u8a0a\uff08\u8996\u89ba\u3001\u97f3\u8a0a\u548c\u8a9e\u97f3\uff09\uff0c\u4e00\u7cfb\u5217\u4e8b\u4ef6\u5f62\u6210\u4e00\u500b\u6709\u51dd\u805a\u529b\u7684\u6545\u4e8b\u7dda\u3002\u7f3a\u4e4f\u5177\u5099\u7d30\u7dfb\u4e8b\u4ef6\u8a3b\u89e3\u7684\u591a\u6a21\u614b\u5f71\u7247\u8cc7\u6599\uff0c\u4ee5\u53ca\u4eba\u5de5\u6a19\u8a3b\u7684\u9ad8\u6602\u6210\u672c\uff0c\u662f\u5168\u9762\u5168\u6a21\u614b\u5f71\u7247\u611f\u77e5\u7684\u4e3b\u8981\u969c\u7919\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u81ea\u52d5\u5316\u6d41\u7a0b\uff0c\u5305\u542b\u9ad8\u54c1\u8cea\u5168\u6a21\u614b\u5f71\u7247\u904e\u6ffe\u3001\u8a9e\u610f\u9023\u8cab\u7684\u5168\u6a21\u614b\u4e8b\u4ef6\u908a\u754c\u5075\u6e2c\uff0c\u4ee5\u53ca\u8de8\u6a21\u614b\u95dc\u806f\u611f\u77e5\u4e8b\u4ef6\u6a19\u984c\u3002\u900f\u904e\u9019\u7a2e\u65b9\u5f0f\uff0c\u6211\u5011\u5c55\u793a\u4e86 LongVALE\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u8996\u89ba-\u97f3\u8a0a-\u8a9e\u8a00\u4e8b\u4ef6\u7406\u89e3\u57fa\u6e96\uff0c\u5305\u542b 105K \u500b\u5168\u6a21\u614b\u4e8b\u4ef6\uff0c\u5177\u5099\u7cbe\u78ba\u7684\u6642\u9593\u908a\u754c\u548c\u8a73\u7d30\u7684\u95dc\u4fc2\u611f\u77e5\u6a19\u984c\uff0c\u5305\u542b\u5728 8.4K \u500b\u9ad8\u54c1\u8cea\u9577\u5f71\u7247\u4e2d\u3002\u6b64\u5916\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u57fa\u6e96\uff0c\u5229\u7528 LongVALE \u8b93\u5f71\u7247\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u5920\u9032\u884c\u5168\u6a21\u614b\u7d30\u7dfb\u6642\u9593\u5f71\u7247\u7406\u89e3\uff0c\u9019\u662f\u7b2c\u4e00\u6b21\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8b49\u660e\u4e86 LongVALE \u5728\u63a8\u9032\u5168\u9762\u5168\u6a21\u614b\u5f71\u7247\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u5de8\u5927\u6f5b\u529b\u3002", "author": "Tiantian Geng et.al.", "authors": "Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng", "id": "2411.19772v1", "paper_url": "http://arxiv.org/abs/2411.19772v1", "repo": "null"}}