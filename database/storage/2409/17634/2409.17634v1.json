{"2409.17634": {"publish_time": "2024-09-26", "title": "P4Q: Learning to Prompt for Quantization in Visual-language Models", "paper_summary": "Large-scale pre-trained Vision-Language Models (VLMs) have gained prominence\nin various visual and multimodal tasks, yet the deployment of VLMs on\ndownstream application platforms remains challenging due to their prohibitive\nrequirements of training samples and computing resources. Fine-tuning and\nquantization of VLMs can substantially reduce the sample and computation costs,\nwhich are in urgent need. There are two prevailing paradigms in quantization,\nQuantization-Aware Training (QAT) can effectively quantize large-scale VLMs but\nincur a huge training cost, while low-bit Post-Training Quantization (PTQ)\nsuffers from a notable performance drop. We propose a method that balances\nfine-tuning and quantization named ``Prompt for Quantization'' (P4Q), in which\nwe design a lightweight architecture to leverage contrastive loss supervision\nto enhance the recognition performance of a PTQ model. Our method can\neffectively reduce the gap between image features and text features caused by\nlow-bit quantization, based on learnable prompts to reorganize textual\nrepresentations and a low-bit adapter to realign the distributions of image and\ntext features. We also introduce a distillation loss based on cosine similarity\npredictions to distill the quantized model using a full-precision teacher.\nExtensive experimental results demonstrate that our P4Q method outperforms\nprior arts, even achieving comparable results to its full-precision\ncounterparts. For instance, our 8-bit P4Q can theoretically compress the\nCLIP-ViT/B-32 by 4 $\\times$ while achieving 66.94\\% Top-1 accuracy,\noutperforming the learnable prompt fine-tuned full-precision model by 2.24\\%\nwith negligible additional parameters on the ImageNet dataset.", "paper_summary_zh": "<paragraph>\u5927\u898f\u6a21\u9810\u8a13\u7df4\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u5404\u7a2e\u8996\u89ba\u548c\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u7372\u5f97\u986f\u8457\u5730\u4f4d\uff0c\u4f46\u7531\u65bc\u8a13\u7df4\u6a23\u672c\u548c\u904b\u7b97\u8cc7\u6e90\u7684\u9650\u5236\u8981\u6c42\uff0c\u5c07 VLM \u90e8\u7f72\u5728\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u5e73\u53f0\u4e0a\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002VLM \u7684\u5fae\u8abf\u548c\u91cf\u5316\u53ef\u4ee5\u5927\u5e45\u6e1b\u5c11\u6a23\u672c\u548c\u904b\u7b97\u6210\u672c\uff0c\u9019\u662f\u4e00\u500b\u8feb\u5207\u9700\u6c42\u3002\u91cf\u5316\u6709\u5169\u7a2e\u76db\u884c\u7684\u5178\u7bc4\uff0c\u91cf\u5316\u611f\u77e5\u8a13\u7df4 (QAT) \u53ef\u4ee5\u6709\u6548\u91cf\u5316\u5927\u898f\u6a21 VLM\uff0c\u4f46\u6703\u7522\u751f\u5de8\u5927\u7684\u8a13\u7df4\u6210\u672c\uff0c\u800c\u4f4e\u4f4d\u5143\u5f8c\u8a13\u7df4\u91cf\u5316 (PTQ) \u5247\u6703\u5c0e\u81f4\u986f\u8457\u7684\u6548\u80fd\u4e0b\u964d\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5e73\u8861\u5fae\u8abf\u548c\u91cf\u5316\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba\u300c\u91cf\u5316\u63d0\u793a\u300d(P4Q)\uff0c\u6211\u5011\u5728\u5176\u4e2d\u8a2d\u8a08\u4e86\u4e00\u500b\u8f15\u91cf\u7d1a\u67b6\u69cb\uff0c\u5229\u7528\u5c0d\u6bd4\u640d\u5931\u76e3\u7763\u4f86\u589e\u5f37 PTQ \u6a21\u578b\u7684\u8fa8\u8b58\u6548\u80fd\u3002\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u6e1b\u5c11\u4f4e\u4f4d\u5143\u91cf\u5316\u9020\u6210\u7684\u5f71\u50cf\u7279\u5fb5\u548c\u6587\u5b57\u7279\u5fb5\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u57fa\u65bc\u53ef\u5b78\u7fd2\u63d0\u793a\u4f86\u91cd\u65b0\u7d44\u7e54\u6587\u5b57\u8868\u5fb5\uff0c\u4e26\u4f7f\u7528\u4f4e\u4f4d\u5143\u9069\u914d\u5668\u91cd\u65b0\u8abf\u6574\u5f71\u50cf\u548c\u6587\u5b57\u7279\u5fb5\u7684\u5206\u5e03\u3002\u6211\u5011\u9084\u5f15\u5165\u4e86\u4e00\u500b\u57fa\u65bc\u9918\u5f26\u76f8\u4f3c\u6027\u9810\u6e2c\u7684\u84b8\u993e\u640d\u5931\uff0c\u4ee5\u4f7f\u7528\u5168\u7cbe\u5ea6\u6559\u5e2b\u4f86\u84b8\u993e\u91cf\u5316\u6a21\u578b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u6211\u5011\u7684 P4Q \u65b9\u6cd5\u512a\u65bc\u5148\u524d\u7684\u6280\u8853\uff0c\u751a\u81f3\u53ef\u4ee5\u9054\u5230\u8207\u5176\u5168\u7cbe\u5ea6\u5c0d\u61c9\u9805\u76f8\u7576\u7684\u7d50\u679c\u3002\u4f8b\u5982\uff0c\u6211\u5011\u7684 8 \u4f4d\u5143 P4Q \u7406\u8ad6\u4e0a\u53ef\u4ee5\u5c07 CLIP-ViT/B-32 \u58d3\u7e2e 4 \u500d\uff0c\u540c\u6642\u9054\u5230 66.94% \u7684 Top-1 \u6e96\u78ba\u7387\uff0c\u5728 ImageNet \u8cc7\u6599\u96c6\u4e0a\u4ee5\u6975\u5c11\u7684\u984d\u5916\u53c3\u6578\u512a\u65bc\u53ef\u5b78\u7fd2\u63d0\u793a\u5fae\u8abf\u7684\u5168\u7cbe\u5ea6\u6a21\u578b 2.24%\u3002</paragraph>", "author": "Huixin Sun et.al.", "authors": "Huixin Sun, Runqi Wang, Yanjing Li, Xianbin Cao, Xiaolong Jiang, Yao Hu, Baochang Zhang", "id": "2409.17634v1", "paper_url": "http://arxiv.org/abs/2409.17634v1", "repo": "null"}}