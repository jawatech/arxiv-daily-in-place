{"2406.03136": {"publish_time": "2024-06-05", "title": "Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models", "paper_summary": "We study the computational limits of Low-Rank Adaptation (LoRA) update for\nfinetuning transformer-based models using fine-grained complexity theory. Our\nkey observation is that the existence of low-rank decompositions within the\ngradient computation of LoRA adaptation leads to possible algorithmic speedup.\nThis allows us to (i) identify a phase transition behavior and (ii) prove the\nexistence of nearly linear algorithms by controlling the LoRA update\ncomputation term by term, assuming the Strong Exponential Time Hypothesis\n(SETH). For the former, we identify a sharp transition in the efficiency of all\npossible rank-$r$ LoRA update algorithms for transformers, based on specific\nnorms resulting from the multiplications of the input sequence $\\mathbf{X}$,\npretrained weights $\\mathbf{W^\\star}$, and adapter matrices $\\alpha \\mathbf{B}\n\\mathbf{A} / r$. Specifically, we derive a shared upper bound threshold for\nsuch norms and show that efficient (sub-quadratic) approximation algorithms of\nLoRA exist only below this threshold. For the latter, we prove the existence of\nnearly linear approximation algorithms for LoRA adaptation by utilizing the\nhierarchical low-rank structures of LoRA gradients and approximating the\ngradients with a series of chained low-rank approximations. To showcase our\ntheory, we consider two practical scenarios: partial (e.g., only $\\mathbf{W}_V$\nand $\\mathbf{W}_Q$) and full adaptations (e.g., $\\mathbf{W}_Q$, $\\mathbf{W}_V$,\nand $\\mathbf{W}_K$) of weights in attention heads.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4f7f\u7528\u7cbe\u7d30\u7684\u8907\u96dc\u6027\u7406\u8ad6\u7814\u7a76\u4f4e\u79e9\u9069\u61c9 (LoRA) \u66f4\u65b0\u7684\u8a08\u7b97\u9650\u5236\uff0c\u4ee5\u5fae\u8abf\u57fa\u65bc\u8f49\u63db\u5668\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u95dc\u9375\u89c0\u5bdf\u662f\uff0c\u5728 LoRA \u9069\u61c9\u7684\u68af\u5ea6\u8a08\u7b97\u4e2d\u5b58\u5728\u4f4e\u79e9\u5206\u89e3\uff0c\u9019\u6703\u5c0e\u81f4\u53ef\u80fd\u7684\u6f14\u7b97\u6cd5\u52a0\u901f\u3002\u9019\u8b93\u6211\u5011\u80fd\u5920 (i) \u8b58\u5225\u76f8\u8b8a\u884c\u70ba\uff0c\u4ee5\u53ca (ii) \u8b49\u660e\u8fd1\u4f3c\u7dda\u6027\u6f14\u7b97\u6cd5\u7684\u5b58\u5728\uff0c\u65b9\u6cd5\u662f\u9010\u9805\u63a7\u5236 LoRA \u66f4\u65b0\u8a08\u7b97\u9805\uff0c\u5047\u8a2d\u5f37\u6307\u6578\u6642\u9593\u5047\u8a2d (SETH)\u3002\u5c0d\u65bc\u524d\u8005\uff0c\u6211\u5011\u6839\u64da\u8f38\u5165\u5e8f\u5217 $\\mathbf{X}$\u3001\u9810\u8a13\u7df4\u6b0a\u91cd $\\mathbf{W^\\star}$ \u548c\u9069\u914d\u5668\u77e9\u9663 $\\alpha \\mathbf{B} \\mathbf{A} / r$ \u7684\u4e58\u7a4d\u7522\u751f\u7684\u7279\u5b9a\u7bc4\u6578\uff0c\u8b58\u5225\u51fa\u6240\u6709\u53ef\u80fd\u7684\u79e9-$r$ LoRA \u66f4\u65b0\u6f14\u7b97\u6cd5\u6548\u7387\u7684\u6025\u5287\u8f49\u8b8a\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u70ba\u6b64\u985e\u7bc4\u6578\u63a8\u5c0e\u51fa\u4e00\u500b\u5171\u7528\u4e0a\u9650\u95be\u503c\uff0c\u4e26\u986f\u793a LoRA \u7684\u6709\u6548\uff08\u6b21\u4e8c\u6b21\uff09\u8fd1\u4f3c\u6f14\u7b97\u6cd5\u50c5\u5b58\u5728\u65bc\u6b64\u95be\u503c\u4ee5\u4e0b\u3002\u5c0d\u65bc\u5f8c\u8005\uff0c\u6211\u5011\u8b49\u660e\u4e86 LoRA \u9069\u61c9\u7684\u8fd1\u4f3c\u7dda\u6027\u6f14\u7b97\u6cd5\u7684\u5b58\u5728\uff0c\u65b9\u6cd5\u662f\u5229\u7528 LoRA \u68af\u5ea6\u7684\u968e\u5c64\u5f0f\u4f4e\u79e9\u7d50\u69cb\uff0c\u4e26\u4f7f\u7528\u4e00\u7cfb\u5217\u93c8\u5f0f\u4f4e\u79e9\u8fd1\u4f3c\u4f86\u8fd1\u4f3c\u68af\u5ea6\u3002\u70ba\u4e86\u5c55\u793a\u6211\u5011\u7684\u7406\u8ad6\uff0c\u6211\u5011\u8003\u616e\u4e86\u5169\u500b\u5be6\u969b\u5834\u666f\uff1a\u6b0a\u91cd\u5728\u6ce8\u610f\u529b\u982d\u4e2d\u7684\u90e8\u5206\uff08\u4f8b\u5982\uff0c\u50c5 $\\mathbf{W}_V$ \u548c $\\mathbf{W}_Q$\uff09\u548c\u5b8c\u5168\u9069\u61c9\uff08\u4f8b\u5982\uff0c$\\mathbf{W}_Q$\u3001$\\mathbf{W}_V$ \u548c $\\mathbf{W}_K$\uff09\u3002</paragraph>", "author": "Jerry Yao-Chieh Hu et.al.", "authors": "Jerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, Han Liu", "id": "2406.03136v1", "paper_url": "http://arxiv.org/abs/2406.03136v1", "repo": "null"}}