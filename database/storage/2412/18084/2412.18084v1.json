{"2412.18084": {"publish_time": "2024-12-24", "title": "Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models", "paper_summary": "Large language models (LLMs) are widely applied in various natural language\nprocessing tasks such as question answering and machine translation. However,\ndue to the lack of labeled data and the difficulty of manual annotation for\nbiochemical properties, the performance for molecule generation tasks is still\nlimited, especially for tasks involving multi-properties constraints. In this\nwork, we present a two-step framework PEIT (Property Enhanced Instruction\nTuning) to improve LLMs for molecular-related tasks. In the first step, we use\ntextual descriptions, SMILES, and biochemical properties as multimodal inputs\nto pre-train a model called PEIT-GEN, by aligning multi-modal representations\nto synthesize instruction data. In the second step, we fine-tune existing\nopen-source LLMs with the synthesized data, the resulting PEIT-LLM can handle\nmolecule captioning, text-based molecule generation, molecular property\nprediction, and our newly proposed multi-constraint molecule generation tasks.\nExperimental results show that our pre-trained PEIT-GEN outperforms MolT5 and\nBioT5 in molecule captioning, demonstrating modalities align well between\ntextual descriptions, structures, and biochemical properties. Furthermore,\nPEIT-LLM shows promising improvements in multi-task molecule generation,\nproving the scalability of the PEIT framework for various molecular tasks. We\nrelease the code, constructed instruction data, and model checkpoints in\nhttps://github.com/chenlong164/PEIT.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5ee3\u6cdb\u61c9\u7528\u65bc\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\uff0c\u4f8b\u5982\u554f\u7b54\u548c\u6a5f\u5668\u7ffb\u8b6f\u3002\u7136\u800c\uff0c\u7531\u65bc\u7f3a\u4e4f\u6a19\u8a18\u6578\u64da\u548c\u4eba\u5de5\u6a19\u8a3b\u751f\u5316\u7279\u6027\u7684\u96e3\u5ea6\uff0c\u5206\u5b50\u751f\u6210\u4efb\u52d9\u7684\u6027\u80fd\u4ecd\u7136\u6709\u9650\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u6d89\u53ca\u591a\u91cd\u5c6c\u6027\u7d04\u675f\u7684\u4efb\u52d9\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5169\u6b65\u6846\u67b6 PEIT\uff08\u5c6c\u6027\u589e\u5f37\u6307\u4ee4\u8abf\u6574\uff09\uff0c\u4ee5\u6539\u9032 LLM \u4ee5\u9032\u884c\u8207\u5206\u5b50\u76f8\u95dc\u7684\u4efb\u52d9\u3002\u5728\u7b2c\u4e00\u6b65\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u6587\u672c\u63cf\u8ff0\u3001SMILES \u548c\u751f\u5316\u7279\u6027\u4f5c\u70ba\u591a\u6a21\u614b\u8f38\u5165\uff0c\u901a\u904e\u5c0d\u9f4a\u591a\u6a21\u614b\u8868\u793a\u4f86\u9810\u8a13\u7df4\u4e00\u500b\u540d\u70ba PEIT-GEN \u7684\u6a21\u578b\uff0c\u4ee5\u5408\u6210\u6307\u4ee4\u6578\u64da\u3002\u5728\u7b2c\u4e8c\u6b65\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u5408\u6210\u7684\u6578\u64da\u5c0d\u73fe\u6709\u7684\u958b\u6e90 LLM \u9032\u884c\u5fae\u8abf\uff0c\u751f\u6210\u7684 PEIT-LLM \u53ef\u4ee5\u8655\u7406\u5206\u5b50\u6a19\u984c\u3001\u57fa\u65bc\u6587\u672c\u7684\u5206\u5b50\u751f\u6210\u3001\u5206\u5b50\u5c6c\u6027\u9810\u6e2c\u4ee5\u53ca\u6211\u5011\u65b0\u63d0\u51fa\u7684\u591a\u7d04\u675f\u5206\u5b50\u751f\u6210\u4efb\u52d9\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u9810\u5148\u8a13\u7df4\u7684 PEIT-GEN \u5728\u5206\u5b50\u6a19\u984c\u4e2d\u512a\u65bc MolT5 \u548c BioT5\uff0c\u8b49\u660e\u4e86\u6587\u672c\u63cf\u8ff0\u3001\u7d50\u69cb\u548c\u751f\u5316\u7279\u6027\u4e4b\u9593\u7684\u6a21\u614b\u5f88\u597d\u5730\u5c0d\u9f4a\u3002\u6b64\u5916\uff0cPEIT-LLM \u5728\u591a\u4efb\u52d9\u5206\u5b50\u751f\u6210\u4e2d\u986f\u793a\u51fa\u6709\u5e0c\u671b\u7684\u6539\u9032\uff0c\u8b49\u660e\u4e86 PEIT \u6846\u67b6\u5c0d\u5404\u7a2e\u5206\u5b50\u4efb\u52d9\u7684\u53ef\u64f4\u5c55\u6027\u3002\u6211\u5011\u5728 https://github.com/chenlong164/PEIT \u4e2d\u767c\u5e03\u4e86\u4ee3\u78bc\u3001\u69cb\u5efa\u7684\u6307\u4ee4\u6578\u64da\u548c\u6a21\u578b\u6aa2\u67e5\u9ede\u3002", "author": "Xuan Lin et.al.", "authors": "Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu", "id": "2412.18084v1", "paper_url": "http://arxiv.org/abs/2412.18084v1", "repo": "https://github.com/chenlong164/peit"}}