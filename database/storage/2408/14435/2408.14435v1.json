{"2408.14435": {"publish_time": "2024-08-26", "title": "Social perception of faces in a vision-language model", "paper_summary": "We explore social perception of human faces in CLIP, a widely used\nopen-source vision-language model. To this end, we compare the similarity in\nCLIP embeddings between different textual prompts and a set of face images. Our\ntextual prompts are constructed from well-validated social psychology terms\ndenoting social perception. The face images are synthetic and are\nsystematically and independently varied along six dimensions: the legally\nprotected attributes of age, gender, and race, as well as facial expression,\nlighting, and pose. Independently and systematically manipulating face\nattributes allows us to study the effect of each on social perception and\navoids confounds that can occur in wild-collected data due to uncontrolled\nsystematic correlations between attributes. Thus, our findings are experimental\nrather than observational. Our main findings are three. First, while CLIP is\ntrained on the widest variety of images and texts, it is able to make\nfine-grained human-like social judgments on face images. Second, age, gender,\nand race do systematically impact CLIP's social perception of faces, suggesting\nan undesirable bias in CLIP vis-a-vis legally protected attributes. Most\nstrikingly, we find a strong pattern of bias concerning the faces of Black\nwomen, where CLIP produces extreme values of social perception across different\nages and facial expressions. Third, facial expression impacts social perception\nmore than age and lighting as much as age. The last finding predicts that\nstudies that do not control for unprotected visual attributes may reach the\nwrong conclusions on bias. Our novel method of investigation, which is founded\non the social psychology literature and on the experiments involving the\nmanipulation of individual attributes, yields sharper and more reliable\nobservations than previous observational methods and may be applied to study\nbiases in any vision-language model.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5728 CLIP \u4e2d\u63a2\u7d22\u4eba\u985e\u81c9\u90e8\u7684\u793e\u6703\u611f\u77e5\uff0cCLIP \u662f\u4e00\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684\u958b\u6e90\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u3002\u70ba\u6b64\uff0c\u6211\u5011\u6bd4\u8f03\u4e86\u4e0d\u540c\u6587\u5b57\u63d0\u793a\u548c\u4e00\u7cfb\u5217\u81c9\u90e8\u5f71\u50cf\u5728 CLIP \u5d4c\u5165\u4e2d\u7684\u76f8\u4f3c\u6027\u3002\u6211\u5011\u7684\u6587\u5b57\u63d0\u793a\u662f\u7531\u7d93\u904e\u5145\u5206\u9a57\u8b49\u7684\u793e\u6703\u5fc3\u7406\u5b78\u8853\u8a9e\u5efa\u69cb\u800c\u6210\uff0c\u7528\u65bc\u8868\u793a\u793e\u6703\u611f\u77e5\u3002\u81c9\u90e8\u5f71\u50cf\u70ba\u5408\u6210\u5f71\u50cf\uff0c\u4e26\u6cbf\u8457\u516d\u500b\u9762\u5411\u9032\u884c\u7cfb\u7d71\u4e14\u7368\u7acb\u7684\u8b8a\u5316\uff1a\u53d7\u6cd5\u5f8b\u4fdd\u8b77\u7684\u5e74\u9f61\u3001\u6027\u5225\u548c\u7a2e\u65cf\u5c6c\u6027\uff0c\u4ee5\u53ca\u9762\u90e8\u8868\u60c5\u3001\u5149\u7dda\u548c\u59ff\u52e2\u3002\u7368\u7acb\u4e14\u7cfb\u7d71\u5730\u64cd\u4f5c\u81c9\u90e8\u5c6c\u6027\uff0c\u8b93\u6211\u5011\u5f97\u4ee5\u7814\u7a76\u6bcf\u500b\u5c6c\u6027\u5c0d\u793e\u6703\u611f\u77e5\u7684\u5f71\u97ff\uff0c\u4e26\u907f\u514d\u5728\u91ce\u5916\u6536\u96c6\u7684\u8cc7\u6599\u4e2d\u6703\u51fa\u73fe\u7684\u6df7\u6dc6\u56e0\u7d20\uff0c\u539f\u56e0\u662f\u5c6c\u6027\u4e4b\u9593\u6703\u51fa\u73fe\u4e0d\u53d7\u63a7\u7684\u7cfb\u7d71\u6027\u95dc\u806f\u3002\u56e0\u6b64\uff0c\u6211\u5011\u7684\u767c\u73fe\u662f\u5be6\u9a57\u6027\u7684\uff0c\u800c\u975e\u89c0\u5bdf\u6027\u7684\u3002\u6211\u5011\u7684\u767c\u73fe\u4e3b\u8981\u6709\u4e09\u500b\u3002\u9996\u5148\uff0c\u5118\u7ba1 CLIP \u662f\u5728\u5404\u7a2e\u5f71\u50cf\u548c\u6587\u5b57\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u4f46\u5b83\u80fd\u5920\u5c0d\u81c9\u90e8\u5f71\u50cf\u505a\u51fa\u7d30\u5fae\u7684\u4eba\u985e\u793e\u6703\u5224\u65b7\u3002\u5176\u6b21\uff0c\u5e74\u9f61\u3001\u6027\u5225\u548c\u7a2e\u65cf\u78ba\u5be6\u7cfb\u7d71\u6027\u5730\u5f71\u97ff CLIP \u5c0d\u81c9\u90e8\u7684\u793e\u6703\u611f\u77e5\uff0c\u9019\u8868\u660e CLIP \u5728\u53d7\u6cd5\u5f8b\u4fdd\u8b77\u7684\u5c6c\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u826f\u504f\u898b\u3002\u6700\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\u4e86\u4e00\u500b\u95dc\u65bc\u9ed1\u4eba\u5973\u6027\u81c9\u90e8\u7684\u5f37\u70c8\u504f\u898b\u6a21\u5f0f\uff0c\u5176\u4e2d CLIP \u5728\u4e0d\u540c\u7684\u5e74\u9f61\u548c\u9762\u90e8\u8868\u60c5\u4e2d\u7522\u751f\u4e86\u6975\u7aef\u7684\u793e\u6703\u611f\u77e5\u503c\u3002\u7b2c\u4e09\uff0c\u9762\u90e8\u8868\u60c5\u5c0d\u793e\u6703\u611f\u77e5\u7684\u5f71\u97ff\u5927\u65bc\u5e74\u9f61\uff0c\u5c0d\u5149\u7dda\u7684\u5f71\u97ff\u5247\u8207\u5e74\u9f61\u76f8\u7576\u3002\u6700\u5f8c\u4e00\u9805\u767c\u73fe\u9810\u6e2c\uff0c\u672a\u63a7\u5236\u4e0d\u53d7\u4fdd\u8b77\u8996\u89ba\u5c6c\u6027\u7684\u7814\u7a76\u53ef\u80fd\u6703\u5c0d\u504f\u898b\u5f97\u51fa\u932f\u8aa4\u7684\u7d50\u8ad6\u3002\u6211\u5011\u65b0\u7a4e\u7684\u7814\u7a76\u65b9\u6cd5\u5efa\u7acb\u5728\u793e\u6703\u5fc3\u7406\u5b78\u6587\u737b\u548c\u6d89\u53ca\u64cd\u4f5c\u500b\u5225\u5c6c\u6027\u7684\u5be6\u9a57\u4e4b\u4e0a\uff0c\u6bd4\u5148\u524d\u7684\u89c0\u5bdf\u65b9\u6cd5\u7522\u751f\u66f4\u6e05\u6670\u4e14\u66f4\u53ef\u9760\u7684\u89c0\u5bdf\u7d50\u679c\uff0c\u4e26\u53ef\u61c9\u7528\u65bc\u7814\u7a76\u4efb\u4f55\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u898b\u3002</paragraph>", "author": "Carina I. Hausladen et.al.", "authors": "Carina I. Hausladen, Manuel Knott, Colin F. Camerer, Pietro Perona", "id": "2408.14435v1", "paper_url": "http://arxiv.org/abs/2408.14435v1", "repo": "https://github.com/carinahausladen/clip-face-bias"}}