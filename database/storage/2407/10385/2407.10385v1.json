{"2407.10385": {"publish_time": "2024-07-15", "title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting", "paper_summary": "Large language models (LLMs) have demonstrated exceptional abilities across\nvarious domains. However, utilizing LLMs for ubiquitous sensing applications\nremains challenging as existing text-prompt methods show significant\nperformance degradation when handling long sensor data sequences. We propose a\nvisual prompting approach for sensor data using multimodal LLMs (MLLMs). We\ndesign a visual prompt that directs MLLMs to utilize visualized sensor data\nalongside the target sensory task descriptions. Additionally, we introduce a\nvisualization generator that automates the creation of optimal visualizations\ntailored to a given sensory task, eliminating the need for prior task-specific\nknowledge. We evaluated our approach on nine sensory tasks involving four\nsensing modalities, achieving an average of 10% higher accuracy than text-based\nprompts and reducing token costs by 15.8x. Our findings highlight the\neffectiveness and cost-efficiency of visual prompts with MLLMs for various\nsensory tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u500b\u9818\u57df\u5c55\u793a\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5c07 LLM \u7528\u65bc\u666e\u904d\u611f\u6e2c\u61c9\u7528\u7a0b\u5f0f\u4ecd\u5177\u6709\u6311\u6230\u6027\uff0c\u56e0\u70ba\u73fe\u6709\u7684\u6587\u5b57\u63d0\u793a\u65b9\u6cd5\u5728\u8655\u7406\u9577\u611f\u6e2c\u5668\u8cc7\u6599\u5e8f\u5217\u6642\u6703\u986f\u8457\u964d\u4f4e\u6548\u80fd\u3002\u6211\u5011\u63d0\u51fa\u4f7f\u7528\u591a\u6a21\u614b LLM (MLLM) \u7684\u611f\u6e2c\u5668\u8cc7\u6599\u8996\u89ba\u63d0\u793a\u65b9\u6cd5\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u8996\u89ba\u63d0\u793a\uff0c\u5f15\u5c0e MLLM \u5229\u7528\u8996\u89ba\u5316\u7684\u611f\u6e2c\u5668\u8cc7\u6599\u4ee5\u53ca\u76ee\u6a19\u611f\u6e2c\u4efb\u52d9\u8aaa\u660e\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e00\u500b\u8996\u89ba\u5316\u7522\u751f\u5668\uff0c\u7528\u65bc\u81ea\u52d5\u5efa\u7acb\u91dd\u5c0d\u7279\u5b9a\u611f\u6e2c\u4efb\u52d9\u91cf\u8eab\u6253\u9020\u7684\u6700\u4f73\u8996\u89ba\u5316\uff0c\u7121\u9700\u5177\u5099\u5148\u524d\u7684\u4efb\u52d9\u7279\u5b9a\u77e5\u8b58\u3002\u6211\u5011\u5728\u6d89\u53ca\u56db\u7a2e\u611f\u6e2c\u6a21\u5f0f\u7684\u4e5d\u9805\u611f\u6e2c\u4efb\u52d9\u4e0a\u8a55\u4f30\u4e86\u6211\u5011\u7684\u65b9\u6cd5\uff0c\u6e96\u78ba\u5ea6\u5e73\u5747\u6bd4\u57fa\u65bc\u6587\u5b57\u7684\u63d0\u793a\u9ad8\u51fa 10%\uff0c\u4e26\u5c07\u4ee3\u5e63\u6210\u672c\u964d\u4f4e\u4e86 15.8 \u500d\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u7a81\u986f\u4e86\u8996\u89ba\u63d0\u793a\u8207 MLLM \u5728\u5404\u7a2e\u611f\u6e2c\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "author": "Hyungjun Yoon et.al.", "authors": "Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee, Sung-Ju Lee", "id": "2407.10385v1", "paper_url": "http://arxiv.org/abs/2407.10385v1", "repo": "null"}}