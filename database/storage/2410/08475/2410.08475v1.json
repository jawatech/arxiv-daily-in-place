{"2410.08475": {"publish_time": "2024-10-11", "title": "GIVE: Structured Reasoning with Knowledge Graph Inspired Veracity Extrapolation", "paper_summary": "Existing retrieval-based reasoning approaches for large language models\n(LLMs) heavily rely on the density and quality of the non-parametric knowledge\nsource to provide domain knowledge and explicit reasoning chain. However,\ninclusive knowledge sources are expensive and sometimes infeasible to build for\nscientific or corner domains. To tackle the challenges, we introduce Graph\nInspired Veracity Extrapolation (GIVE), a novel reasoning framework that\nintegrates the parametric and non-parametric memories to enhance both knowledge\nretrieval and faithful reasoning processes on very sparse knowledge graphs. By\nleveraging the external structured knowledge to inspire LLM to model the\ninterconnections among relevant concepts, our method facilitates a more logical\nand step-wise reasoning approach akin to experts' problem-solving, rather than\ngold answer retrieval. Specifically, the framework prompts LLMs to decompose\nthe query into crucial concepts and attributes, construct entity groups with\nrelevant entities, and build an augmented reasoning chain by probing potential\nrelationships among node pairs across these entity groups. Our method\nincorporates both factual and extrapolated linkages to enable comprehensive\nunderstanding and response generation. Extensive experiments on\nreasoning-intense benchmarks on biomedical and commonsense QA demonstrate the\neffectiveness of our proposed method. Specifically, GIVE enables GPT3.5-turbo\nto outperform advanced models like GPT4 without any additional training cost,\nthereby underscoring the efficacy of integrating structured information and\ninternal reasoning ability of LLMs for tackling specialized tasks with limited\nexternal resources.", "paper_summary_zh": "\u73fe\u6709\u7684\u57fa\u65bc\u6aa2\u7d22\u7684\u63a8\u7406\u65b9\u6cd5\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u56b4\u91cd\u4f9d\u8cf4\u975e\u53c3\u6578\u77e5\u8b58\u4f86\u6e90\u7684\u5bc6\u5ea6\u548c\u54c1\u8cea\uff0c\u4ee5\u63d0\u4f9b\u9818\u57df\u77e5\u8b58\u548c\u660e\u78ba\u7684\u63a8\u7406\u93c8\u3002\u7136\u800c\uff0c\u5305\u5bb9\u6027\u7684\u77e5\u8b58\u4f86\u6e90\u5f88\u6602\u8cb4\uff0c\u6709\u6642\u5c0d\u65bc\u79d1\u5b78\u6216\u89d2\u843d\u9818\u57df\u4f86\u8aaa\uff0c\u5efa\u7acb\u8d77\u4f86\u4e5f\u4e0d\u53ef\u884c\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5716\u5f62\u555f\u767c\u771f\u5be6\u63a8\u65b7 (GIVE)\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u63a8\u7406\u67b6\u69cb\uff0c\u5b83\u6574\u5408\u4e86\u53c3\u6578\u548c\u975e\u53c3\u6578\u8a18\u61b6\u9ad4\uff0c\u4ee5\u589e\u5f37\u5728\u975e\u5e38\u7a00\u758f\u7684\u77e5\u8b58\u5716\u8b5c\u4e0a\u9032\u884c\u77e5\u8b58\u6aa2\u7d22\u548c\u5fe0\u5be6\u63a8\u7406\u904e\u7a0b\u3002\u900f\u904e\u5229\u7528\u5916\u90e8\u7d50\u69cb\u5316\u77e5\u8b58\u4f86\u6fc0\u52f5 LLM \u6a21\u64ec\u76f8\u95dc\u6982\u5ff5\u4e4b\u9593\u7684\u76f8\u4e92\u95dc\u806f\uff0c\u6211\u5011\u7684\u6280\u8853\u4fc3\u9032\u4e86\u4e00\u7a2e\u66f4\u5408\u4e4e\u908f\u8f2f\u4e14\u5faa\u5e8f\u6f38\u9032\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u985e\u4f3c\u65bc\u5c08\u5bb6\u7684\u554f\u984c\u89e3\u6c7a\uff0c\u800c\u4e0d\u662f\u9ec3\u91d1\u7b54\u6848\u6aa2\u7d22\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u8a72\u67b6\u69cb\u63d0\u793a LLM \u5c07\u67e5\u8a62\u5206\u89e3\u70ba\u95dc\u9375\u6982\u5ff5\u548c\u5c6c\u6027\uff0c\u4f7f\u7528\u76f8\u95dc\u5be6\u9ad4\u5efa\u69cb\u5be6\u9ad4\u7fa4\u7d44\uff0c\u4e26\u900f\u904e\u63a2\u67e5\u9019\u4e9b\u5be6\u9ad4\u7fa4\u7d44\u4e2d\u7bc0\u9ede\u5c0d\u4e4b\u9593\u7684\u6f5b\u5728\u95dc\u4fc2\u4f86\u5efa\u7acb\u589e\u5f37\u7684\u63a8\u7406\u93c8\u3002\u6211\u5011\u7684\u6280\u8853\u7d50\u5408\u4e86\u4e8b\u5be6\u548c\u5916\u63a8\u95dc\u806f\uff0c\u4ee5\u5be6\u73fe\u5168\u9762\u7684\u7406\u89e3\u548c\u56de\u61c9\u7522\u751f\u3002\u5728\u751f\u7269\u91ab\u5b78\u548c\u5e38\u8b58\u554f\u7b54\u4e0a\u5c0d\u63a8\u7406\u5bc6\u96c6\u57fa\u6e96\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cGIVE \u4f7f GPT3.5-turbo \u80fd\u5920\u5728\u6c92\u6709\u4efb\u4f55\u984d\u5916\u8a13\u7df4\u6210\u672c\u7684\u60c5\u6cc1\u4e0b\u512a\u65bc GPT4 \u7b49\u9032\u968e\u6a21\u578b\uff0c\u5f9e\u800c\u5f37\u8abf\u4e86\u6574\u5408\u7d50\u69cb\u5316\u8cc7\u8a0a\u548c LLM \u5167\u90e8\u63a8\u7406\u80fd\u529b\u5c0d\u65bc\u8655\u7406\u5177\u6709\u6709\u9650\u5916\u90e8\u8cc7\u6e90\u7684\u5c08\u696d\u4efb\u52d9\u7684\u6548\u80fd\u3002", "author": "Jiashu He et.al.", "authors": "Jiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, Alejandro Ribeiro", "id": "2410.08475v1", "paper_url": "http://arxiv.org/abs/2410.08475v1", "repo": "null"}}