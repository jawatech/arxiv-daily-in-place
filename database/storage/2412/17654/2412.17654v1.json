{"2412.17654": {"publish_time": "2024-12-23", "title": "Enhanced Temporal Processing in Spiking Neural Networks for Static Object Detection Using 3D Convolutions", "paper_summary": "Spiking Neural Networks (SNNs) are a class of network models capable of\nprocessing spatiotemporal information, with event-driven characteristics and\nenergy efficiency advantages. Recently, directly trained SNNs have shown\npotential to match or surpass the performance of traditional Artificial Neural\nNetworks (ANNs) in classification tasks. However, in object detection tasks,\ndirectly trained SNNs still exhibit a significant performance gap compared to\nANNs when tested on frame-based static object datasets (such as COCO2017).\nTherefore, bridging this performance gap and enabling directly trained SNNs to\nachieve performance comparable to ANNs on these static datasets has become one\nof the key challenges in the development of SNNs.To address this challenge,\nthis paper focuses on enhancing the SNN's unique ability to process\nspatiotemporal information. Spiking neurons, as the core components of SNNs,\nfacilitate the exchange of information between different temporal channels\nduring the process of converting input floating-point data into binary spike\nsignals. However, existing neuron models still have certain limitations in the\ncommunication of temporal information. Some studies have even suggested that\ndisabling the backpropagation in the time dimension during SNN training can\nstill yield good training results. To improve the SNN handling of temporal\ninformation, this paper proposes replacing traditional 2D convolutions with 3D\nconvolutions, thus directly incorporating temporal information into the\nconvolutional process. Additionally, temporal information recurrence mechanism\nis introduced within the neurons to further enhance the neurons' efficiency in\nutilizing temporal information.Experimental results show that the proposed\nmethod enables directly trained SNNs to achieve performance levels comparable\nto ANNs on the COCO2017 and VOC datasets.", "paper_summary_zh": "\u5c16\u5cf0\u795e\u7ecf\u7db2\u8def (SNN) \u662f\u4e00\u7a2e\u7db2\u8def\u6a21\u578b\uff0c\u5177\u6709\u8655\u7406\u6642\u7a7a\u8cc7\u8a0a\u7684\u80fd\u529b\uff0c\u5177\u5099\u4e8b\u4ef6\u9a45\u52d5\u7279\u6027\u548c\u80fd\u6e90\u6548\u7387\u512a\u52e2\u3002\u6700\u8fd1\uff0c\u76f4\u63a5\u8a13\u7df4\u7684 SNN \u5df2\u5c55\u73fe\u51fa\u5728\u5206\u985e\u4efb\u52d9\u4e2d\u8207\u50b3\u7d71\u4eba\u5de5\u795e\u7d93\u7db2\u8def (ANN) \u76f8\u5339\u914d\u6216\u8d85\u8d8a\u5176\u6548\u80fd\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u5728\u76ee\u6a19\u5075\u6e2c\u4efb\u52d9\u4e2d\uff0c\u76f4\u63a5\u8a13\u7df4\u7684 SNN \u5728\u57fa\u65bc\u5e40\u7684\u975c\u614b\u76ee\u6a19\u8cc7\u6599\u96c6\uff08\u4f8b\u5982 COCO2017\uff09\u4e0a\u6e2c\u8a66\u6642\uff0c\u8207 ANN \u76f8\u6bd4\u4ecd\u5b58\u5728\u986f\u8457\u7684\u6548\u80fd\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u5f4c\u5408\u6b64\u6548\u80fd\u5dee\u8ddd\u4e26\u4f7f\u76f4\u63a5\u8a13\u7df4\u7684 SNN \u80fd\u5728\u9019\u4e9b\u975c\u614b\u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u8207 ANN \u76f8\u7576\u7684\u6548\u80fd\uff0c\u5df2\u6210\u70ba SNN \u767c\u5c55\u4e2d\u7684\u95dc\u9375\u6311\u6230\u4e4b\u4e00\u3002\u70ba\u4e86\u61c9\u5c0d\u6b64\u6311\u6230\uff0c\u672c\u6587\u91cd\u9ede\u5728\u65bc\u589e\u5f37 SNN \u8655\u7406\u6642\u7a7a\u8cc7\u8a0a\u7684\u7368\u7279\u80fd\u529b\u3002\u5c16\u5cf0\u795e\u7d93\u5143\u4f5c\u70ba SNN \u7684\u6838\u5fc3\u5143\u4ef6\uff0c\u5728\u5c07\u8f38\u5165\u6d6e\u9ede\u8cc7\u6599\u8f49\u63db\u70ba\u4e8c\u9032\u5236\u5c16\u5cf0\u8a0a\u865f\u7684\u904e\u7a0b\u4e2d\uff0c\u4fc3\u9032\u4e0d\u540c\u6642\u9593\u901a\u9053\u4e4b\u9593\u7684\u8cc7\u8a0a\u4ea4\u63db\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u795e\u7d93\u5143\u6a21\u578b\u5728\u6642\u9593\u8cc7\u8a0a\u7684\u50b3\u905e\u4e0a\u4ecd\u6709\u67d0\u4e9b\u9650\u5236\u3002\u4e00\u4e9b\u7814\u7a76\u751a\u81f3\u8868\u660e\uff0c\u5728 SNN \u8a13\u7df4\u671f\u9593\u505c\u7528\u6642\u9593\u7dad\u5ea6\u7684\u53cd\u5411\u50b3\u64ad\u4ecd\u7136\u53ef\u4ee5\u7522\u751f\u826f\u597d\u7684\u8a13\u7df4\u7d50\u679c\u3002\u70ba\u4e86\u6539\u5584 SNN \u5c0d\u6642\u9593\u8cc7\u8a0a\u7684\u8655\u7406\uff0c\u672c\u6587\u5efa\u8b70\u4ee5 3D \u6372\u7a4d\u53d6\u4ee3\u50b3\u7d71\u7684 2D \u6372\u7a4d\uff0c\u5f9e\u800c\u5c07\u6642\u9593\u8cc7\u8a0a\u76f4\u63a5\u7d0d\u5165\u6372\u7a4d\u904e\u7a0b\u4e2d\u3002\u6b64\u5916\uff0c\u5728\u795e\u7d93\u5143\u4e2d\u5f15\u5165\u6642\u9593\u8cc7\u8a0a\u905e\u8ff4\u6a5f\u5236\uff0c\u4ee5\u9032\u4e00\u6b65\u589e\u5f37\u795e\u7d93\u5143\u5229\u7528\u6642\u9593\u8cc7\u8a0a\u7684\u6548\u7387\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u76f4\u63a5\u8a13\u7df4\u7684 SNN \u80fd\u5728 COCO2017 \u548c VOC \u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u8207 ANN \u76f8\u7576\u7684\u6548\u80fd\u6c34\u6e96\u3002", "author": "Huaxu He et.al.", "authors": "Huaxu He", "id": "2412.17654v1", "paper_url": "http://arxiv.org/abs/2412.17654v1", "repo": "null"}}