{"2406.02350": {"publish_time": "2024-06-04", "title": "LlamaCare: A Large Medical Language Model for Enhancing Healthcare Knowledge Sharing", "paper_summary": "Large language models (LLMs) have shown amazing capabilities in knowledge\nmemorization and the present. However, when it comes to domain-specific\nknowledge and downstream tasks like medical, general LLMs are often unable to\ngive precise answers. In addition, when people want LLMs to answer\nclassification questions, they usually go through instruction tuning first.\nHowever, LLMs do not always give a direct index of the categorization after\ninstruction tuning. In this paper, we proposed LlamaCare, a fine-tuned medical\nlanguage model, and Extended Classification Integration(ECI), a module to\nhandle classification problems of LLMs. Our contributions are : (i) We\nfine-tuned a large language model of medical knowledge with very low carbon\nemissions and achieved similar performance with ChatGPT by a 24G GPU. (ii) We\nsolved the problem of redundant categorical answers and improved the\nperformance of LLMs by proposing a new module called Extended Classification\nIntegration. (iii) We released our processed data for one-shot and few-shot\ntraining for some benchmarks such as PubMedQA and USMLE 1-3 step. Our method\nachieves a close performance comparable to some state-of-the-art models with\nthe same quantity of parameters on benchmarks, while being more environmentally\nfriendly by using less GPU computation time. Our models, codes, and datasets\ncan be found at \\url{https://github.com/Stephen-SMJ/LLamaCare}.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u77e5\u8b58\u8a18\u61b6\u548c\u73fe\u5728\u5c55\u73fe\u51fa\u9a5a\u4eba\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u7576\u6d89\u53ca\u5230\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\u548c\u4e0b\u6e38\u4efb\u52d9\uff08\u5982\u91ab\u7642\uff09\uff0c\u4e00\u822c\u7684 LLM \u901a\u5e38\u7121\u6cd5\u7d66\u51fa\u7cbe\u78ba\u7684\u7b54\u6848\u3002\u6b64\u5916\uff0c\u7576\u4eba\u5011\u5e0c\u671b LLM \u56de\u7b54\u5206\u985e\u554f\u984c\u6642\uff0c\u4ed6\u5011\u901a\u5e38\u6703\u5148\u9032\u884c\u6307\u4ee4\u8abf\u6574\u3002\u7136\u800c\uff0cLLM \u5728\u6307\u4ee4\u8abf\u6574\u5f8c\u4e26\u975e\u7e3d\u662f\u7d66\u51fa\u5206\u985e\u7684\u76f4\u63a5\u6307\u6a19\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LlamaCare\uff0c\u4e00\u7a2e\u7d93\u904e\u5fae\u8abf\u7684\u91ab\u5b78\u8a9e\u8a00\u6a21\u578b\uff0c\u4ee5\u53ca\u64f4\u5c55\u5206\u985e\u6574\u5408 (ECI)\uff0c\u4e00\u500b\u7528\u65bc\u8655\u7406 LLM \u5206\u985e\u554f\u984c\u7684\u6a21\u7d44\u3002\u6211\u5011\u7684\u8ca2\u737b\u662f\uff1a(i) \u6211\u5011\u5fae\u8abf\u4e86\u4e00\u500b\u91ab\u7642\u77e5\u8b58\u7684\u5927\u8a9e\u8a00\u6a21\u578b\uff0c\u78b3\u6392\u653e\u975e\u5e38\u4f4e\uff0c\u4e26\u901a\u904e 24G GPU \u9054\u5230\u4e86\u8207 ChatGPT \u76f8\u4f3c\u7684\u6548\u80fd\u3002(ii) \u6211\u5011\u89e3\u6c7a\u4e86\u5197\u9918\u985e\u5225\u7b54\u6848\u7684\u554f\u984c\uff0c\u4e26\u901a\u904e\u63d0\u51fa\u4e00\u500b\u7a31\u70ba\u64f4\u5c55\u5206\u985e\u6574\u5408\u7684\u65b0\u6a21\u7d44\u4f86\u63d0\u5347 LLM \u7684\u6548\u80fd\u3002(iii) \u6211\u5011\u91cb\u51fa\u4e86\u6211\u5011\u8655\u7406\u904e\u7684\u8cc7\u6599\uff0c\u7528\u65bc PubMedQA \u548c USMLE 1-3 \u6b65\u9a5f\u7b49\u4e00\u4e9b\u57fa\u6e96\u7684\u55ae\u6b21\u548c\u5c11\u6b21\u8a13\u7df4\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u57fa\u6e96\u4e0a\u9054\u5230\u4e86\u8207\u4e00\u4e9b\u6700\u5148\u9032\u7684\u6a21\u578b\u76f8\u7576\u7684\u6548\u80fd\uff0c\u540c\u6642\u900f\u904e\u6e1b\u5c11 GPU \u8a08\u7b97\u6642\u9593\uff0c\u5c0d\u74b0\u5883\u66f4\u53cb\u5584\u3002\u6211\u5011\u7684\u6a21\u578b\u3001\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u96c6\u53ef\u4ee5\u5728 \\url{https://github.com/Stephen-SMJ/LLamaCare} \u627e\u5230\u3002", "author": "Maojun Sun et.al.", "authors": "Maojun Sun", "id": "2406.02350v2", "paper_url": "http://arxiv.org/abs/2406.02350v2", "repo": "https://github.com/stephen-smj/llamacare"}}