{"2410.23749": {"publish_time": "2024-10-31", "title": "LSEAttention is All You Need for Time Series Forecasting", "paper_summary": "Transformer-based architectures have achieved remarkable success in natural\nlanguage processing and computer vision. However, their performance in\nmultivariate long-term forecasting often lags behind simpler linear baselines.\nPrevious studies have identified the traditional attention mechanism as a\nsignificant factor contributing to this limitation. To unlock the full\npotential of transformers for multivariate time series forecasting, I introduce\n\\textbf{LSEAttention}, an approach designed to address entropy collapse and\ntraining instability commonly observed in transformer models. I validate the\neffectiveness of LSEAttention across various real-world multivariate time\nseries datasets, demonstrating that it not only outperforms existing time\nseries transformer models but also exceeds the performance of some\nstate-of-the-art models on specific datasets.", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684\u67b6\u69cb\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u548c\u96fb\u8166\u8996\u89ba\u65b9\u9762\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u591a\u8b8a\u6578\u9577\u671f\u9810\u6e2c\u4e2d\u7684\u8868\u73fe\u901a\u5e38\u843d\u5f8c\u65bc\u8f03\u7c21\u55ae\u7684\u7dda\u6027\u57fa\u7dda\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u5c07\u50b3\u7d71\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u78ba\u5b9a\u70ba\u5c0e\u81f4\u6b64\u9650\u5236\u7684\u4e00\u500b\u91cd\u8981\u56e0\u7d20\u3002\u70ba\u4e86\u767c\u63ee Transformer \u5728\u591a\u8b8a\u6578\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u65b9\u9762\u7684\u5168\u90e8\u6f5b\u529b\uff0c\u6211\u5f15\u5165\u4e86 \\textbf{LSEAttention}\uff0c\u4e00\u7a2e\u65e8\u5728\u89e3\u6c7a\u5728 Transformer \u6a21\u578b\u4e2d\u666e\u904d\u89c0\u5bdf\u5230\u7684\u71b5\u5d29\u6f70\u548c\u8a13\u7df4\u4e0d\u7a69\u5b9a\u7684\u65b9\u6cd5\u3002\u6211\u9a57\u8b49\u4e86 LSEAttention \u5728\u5404\u7a2e\u771f\u5be6\u4e16\u754c\u591a\u8b8a\u6578\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u96c6\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8b49\u660e\u5b83\u4e0d\u50c5\u512a\u65bc\u73fe\u6709\u7684\u6642\u9593\u5e8f\u5217 Transformer \u6a21\u578b\uff0c\u800c\u4e14\u5728\u7279\u5b9a\u8cc7\u6599\u96c6\u4e0a\u4e5f\u8d85\u904e\u4e86\u67d0\u4e9b\u6700\u5148\u9032\u6a21\u578b\u7684\u8868\u73fe\u3002", "author": "Dizhen Liang et.al.", "authors": "Dizhen Liang", "id": "2410.23749v1", "paper_url": "http://arxiv.org/abs/2410.23749v1", "repo": "null"}}