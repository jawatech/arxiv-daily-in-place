{"2405.20612": {"publish_time": "2024-05-31", "title": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation", "paper_summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious tasks using the in-context learning (ICL) paradigm. However, their\neffectiveness is often compromised by inherent bias, leading to prompt\nbrittleness, i.e., sensitivity to design settings such as example selection,\norder, and prompt formatting. Previous studies have addressed LLM bias through\nexternal adjustment of model outputs, but the internal mechanisms that lead to\nsuch bias remain unexplored. Our work delves into these mechanisms,\nparticularly investigating how feedforward neural networks (FFNs) and attention\nheads result in the bias of LLMs. By Interpreting the contribution of\nindividual FFN vectors and attention heads, we identify the biased LLM\ncomponents that skew LLMs' prediction toward specific labels. To mitigate these\nbiases, we introduce UniBias, an inference-only method that effectively\nidentifies and eliminates biased FFN vectors and attention heads. Extensive\nexperiments across 12 NLP datasets demonstrate that UniBias significantly\nenhances ICL performance and alleviates prompt brittleness of LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u4f7f\u7528\u60c5\u5883\u5b78\u7fd2 (ICL) \u5178\u7bc4\u7684\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u6709\u6548\u6027\u5e38\u5e38\u53d7\u5230\u5167\u5728\u504f\u5dee\u7684\u5f71\u97ff\uff0c\u5c0e\u81f4\u63d0\u793a\u8106\u5f31\u6027\uff0c\u5373\u5c0d\u7bc4\u4f8b\u9078\u64c7\u3001\u9806\u5e8f\u548c\u63d0\u793a\u683c\u5f0f\u7b49\u8a2d\u8a08\u8a2d\u5b9a\u7684\u654f\u611f\u6027\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u900f\u904e\u6a21\u578b\u8f38\u51fa\u7684\u5916\u90e8\u8abf\u6574\u4f86\u89e3\u6c7a LLM \u504f\u5dee\uff0c\u4f46\u5c0e\u81f4\u6b64\u985e\u504f\u5dee\u7684\u5167\u90e8\u6a5f\u5236\u4ecd\u672a\u63a2\u8a0e\u3002\u6211\u5011\u7684\u7814\u7a76\u6df1\u5165\u63a2\u8a0e\u9019\u4e9b\u6a5f\u5236\uff0c\u7279\u5225\u662f\u8abf\u67e5\u524d\u994b\u795e\u7d93\u7db2\u8def (FFN) \u548c\u6ce8\u610f\u529b\u982d\u5982\u4f55\u5c0e\u81f4 LLM \u7684\u504f\u5dee\u3002\u900f\u904e\u8a6e\u91cb\u500b\u5225 FFN \u5411\u91cf\u548c\u6ce8\u610f\u529b\u982d\u7684\u8ca2\u737b\uff0c\u6211\u5011\u627e\u51fa\u504f\u5dee\u7684 LLM \u7d44\u4ef6\uff0c\u4f7f LLM \u7684\u9810\u6e2c\u504f\u5411\u7279\u5b9a\u6a19\u7c64\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u4e9b\u504f\u5dee\uff0c\u6211\u5011\u5f15\u5165\u4e86 UniBias\uff0c\u9019\u662f\u4e00\u7a2e\u50c5\u9650\u63a8\u8ad6\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u627e\u51fa\u4e26\u6d88\u9664\u6709\u504f\u5dee\u7684 FFN \u5411\u91cf\u548c\u6ce8\u610f\u529b\u982d\u3002\u5728 12 \u500b NLP \u8cc7\u6599\u96c6\u4e2d\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0cUniBias \u5927\u5e45\u63d0\u5347\u4e86 ICL \u6548\u80fd\uff0c\u4e26\u6e1b\u8f15\u4e86 LLM \u7684\u63d0\u793a\u8106\u5f31\u6027\u3002", "author": "Hanzhang Zhou et.al.", "authors": "Hanzhang Zhou, Zijian Feng, Zixiao Zhu, Junlang Qian, Kezhi Mao", "id": "2405.20612v1", "paper_url": "http://arxiv.org/abs/2405.20612v1", "repo": "null"}}