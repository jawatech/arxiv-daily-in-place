{"2411.05232": {"publish_time": "2024-11-07", "title": "Abstract2Appendix: Academic Reviews Enhance LLM Long-Context Capabilities", "paper_summary": "Large language models (LLMs) have shown remarkable performance across various\ntasks, yet their ability to handle long-context reading remains challenging.\nThis study explores the effectiveness of leveraging high-quality academic peer\nreview data for fine-tuning LLMs to enhance their long-context capabilities. We\ncompare the Direct Preference Optimization (DPO) method with the Supervised\nFine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency.\nOur experiments show that the fine-tuned model achieves a 4.04-point\nimprovement over phi-3 and a 2.6\\% increase on the Qasper benchmark using only\n2000 samples. Despite facing limitations in data scale and processing costs,\nthis study underscores the potential of DPO and high-quality data in advancing\nLLM performance.\n  Additionally, the zero-shot benchmark results indicate that aggregated\nhigh-quality human reviews are overwhelmingly preferred over LLM-generated\nresponses, even for the most capable models like GPT-4o. This suggests that\nhigh-quality human reviews are extremely rich in information, reasoning, and\nlong-context retrieval, capabilities that even the most advanced models have\nnot fully captured. These findings highlight the high utility of leveraging\nhuman reviews to further advance the field.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u986f\u8457\u7684\u6548\u80fd\uff0c\u4f46\u5b83\u5011\u8655\u7406\u9577\u8a9e\u5883\u95b1\u8b80\u7684\u80fd\u529b\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u5229\u7528\u9ad8\u54c1\u8cea\u7684\u5b78\u8853\u540c\u884c\u8a55\u5be9\u8cc7\u6599\u5fae\u8abf LLM\uff0c\u4ee5\u589e\u5f37\u5176\u9577\u8a9e\u5883\u80fd\u529b\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u5c07\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff08DPO\uff09\u65b9\u6cd5\u8207\u76e3\u7763\u5fae\u8abf\uff08SFT\uff09\u65b9\u6cd5\u9032\u884c\u6bd4\u8f03\uff0c\u8b49\u660e\u4e86 DPO \u7684\u512a\u8d8a\u6027\u548c\u8cc7\u6599\u6548\u7387\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u5fae\u8abf\u5f8c\u7684\u6a21\u578b\u5728 phi-3 \u4e0a\u53d6\u5f97\u4e86 4.04 \u5206\u7684\u9032\u6b65\uff0c\u5728 Qasper \u57fa\u6e96\u4e0a\u50c5\u4f7f\u7528 2000 \u500b\u6a23\u672c\u5c31\u589e\u52a0\u4e86 2.6%\u3002\u5118\u7ba1\u5728\u8cc7\u6599\u898f\u6a21\u548c\u8655\u7406\u6210\u672c\u65b9\u9762\u9762\u81e8\u9650\u5236\uff0c\u4f46\u672c\u7814\u7a76\u5f37\u8abf\u4e86 DPO \u548c\u9ad8\u54c1\u8cea\u8cc7\u6599\u5728\u63d0\u5347 LLM \u6548\u80fd\u65b9\u9762\u7684\u6f5b\u529b\u3002\u6b64\u5916\uff0c\u96f6\u6b21\u5b78\u7fd2\u57fa\u6e96\u7d50\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5c0d\u65bc\u50cf GPT-4o \u9019\u6a23\u6700\u5f37\u5927\u7684\u6a21\u578b\uff0c\u5f59\u7e3d\u7684\u9ad8\u54c1\u8cea\u4eba\u985e\u8a55\u8ad6\u4e5f\u6bd4 LLM \u751f\u6210\u7684\u56de\u61c9\u66f4\u53d7\u6b61\u8fce\u3002\u9019\u8868\u660e\u9ad8\u54c1\u8cea\u7684\u4eba\u985e\u8a55\u8ad6\u6975\u5176\u8c50\u5bcc\uff0c\u5305\u542b\u8cc7\u8a0a\u3001\u63a8\u7406\u548c\u9577\u8a9e\u5883\u6aa2\u7d22\uff0c\u9019\u662f\u5373\u4f7f\u662f\u6700\u5148\u9032\u7684\u6a21\u578b\u4e5f\u5c1a\u672a\u5b8c\u5168\u638c\u63e1\u7684\u80fd\u529b\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u51fa\u4e86\u5229\u7528\u4eba\u985e\u8a55\u8ad6\u9032\u4e00\u6b65\u63a8\u52d5\u8a72\u9818\u57df\u767c\u5c55\u7684\u9ad8\u6548\u7528\u6027\u3002", "author": "Shengzhi Li et.al.", "authors": "Shengzhi Li, Kittipat Kampa, Rongyu Lin, Bohang Li, Shichao Pei", "id": "2411.05232v1", "paper_url": "http://arxiv.org/abs/2411.05232v1", "repo": "null"}}