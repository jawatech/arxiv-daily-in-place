{"2405.05329": {"publish_time": "2024-05-08", "title": "KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation", "paper_summary": "Large Language Model or LLM inference has two phases, the prompt (or prefill)\nphase to output the first token and the extension (or decoding) phase to the\ngenerate subsequent tokens. In this work, we propose an efficient\nparallelization scheme, KV-Runahead to accelerate the prompt phase. The key\nobservation is that the extension phase generates tokens faster than the prompt\nphase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes\nthe prompt phase by orchestrating multiple processes to populate the KV-cache\nand minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache\nscheme has two main benefits. Fist, since KV-cache is designed to leverage the\ncausal attention map, we minimize computation and computation automatically.\nSecond, since it already exists for the exten- sion phase, KV-Runahead is easy\nto implement. We further propose context-level load-balancing to handle uneven\nKV-cache generation (due to the causal attention) and to optimize TTFT.\nCompared with an existing parallelization scheme such as tensor or sequential\nparallelization where keys and values are locally generated and exchanged via\nall-gather collectives, our experimental results demonstrate that KV-Runahead\ncan offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6216 LLM \u63a8\u8ad6\u6709\u5169\u500b\u968e\u6bb5\uff0c\u63d0\u793a\uff08\u6216\u9810\u586b\uff09\u968e\u6bb5\u4ee5\u8f38\u51fa\u7b2c\u4e00\u500b\u7b26\u865f\u548c\u5ef6\u4f38\uff08\u6216\u89e3\u78bc\uff09\u968e\u6bb5\u4ee5\u7522\u751f\u5f8c\u7e8c\u7b26\u865f\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u9ad8\u6548\u7684\u4e26\u884c\u5316\u65b9\u6848\uff0cKV-Runahead \u4f86\u52a0\u901f\u63d0\u793a\u968e\u6bb5\u3002\u4e3b\u8981\u7684\u89c0\u5bdf\u7d50\u679c\u662f\uff0c\u7531\u65bc\u5feb\u53d6\u9375\u503c\uff08KV \u5feb\u53d6\uff09\uff0c\u5ef6\u4f38\u968e\u6bb5\u6bd4\u63d0\u793a\u968e\u6bb5\u66f4\u5feb\u5730\u7522\u751f\u7b26\u865f\u3002\u56e0\u6b64\uff0cKV-Runahead \u901a\u904e\u7de8\u6392\u591a\u500b\u8655\u7406\u7a0b\u5e8f\u4f86\u586b\u5145 KV \u5feb\u53d6\u4e26\u5c07\u9996\u6b21\u7b26\u865f\u6642\u9593\uff08TTFT\uff09\u6700\u5c0f\u5316\uff0c\u5f9e\u800c\u4e26\u884c\u5316\u63d0\u793a\u968e\u6bb5\u3002KV \u5feb\u53d6\u65b9\u6848\u7684\u96d9\u91cd\u7528\u9014\u6709\u5169\u500b\u4e3b\u8981\u597d\u8655\u3002\u9996\u5148\uff0c\u7531\u65bc KV \u5feb\u53d6\u88ab\u8a2d\u8a08\u70ba\u5229\u7528\u56e0\u679c\u95dc\u4fc2\u6ce8\u610f\u529b\u5716\uff0c\u6211\u5011\u5c07\u8a08\u7b97\u548c\u8a08\u7b97\u81ea\u52d5\u5316\u5230\u6700\u5c0f\u3002\u5176\u6b21\uff0c\u7531\u65bc\u5b83\u5df2\u7d93\u5b58\u5728\u65bc\u5ef6\u4f38\u968e\u6bb5\uff0cKV-Runahead \u5f88\u5bb9\u6613\u5be6\u73fe\u3002\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e0a\u4e0b\u6587\u7d1a\u8ca0\u8f09\u5e73\u8861\u4f86\u8655\u7406\u4e0d\u5747\u52fb\u7684 KV \u5feb\u53d6\u7522\u751f\uff08\u7531\u65bc\u56e0\u679c\u95dc\u4fc2\u6ce8\u610f\u529b\uff09\u4e26\u512a\u5316 TTFT\u3002\u8207\u73fe\u6709\u7684\u4e26\u884c\u5316\u65b9\u6848\uff08\u4f8b\u5982\u5f35\u91cf\u6216\u9806\u5e8f\u4e26\u884c\u5316\uff0c\u5176\u4e2d\u9375\u548c\u503c\u662f\u5c40\u90e8\u751f\u6210\u7684\uff0c\u4e26\u901a\u904e\u5168\u6536\u96c6\u4ea4\u63db\uff09\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cKV-Runahead \u53ef\u4ee5\u5206\u5225\u70ba Llama 7B \u548c Falcon 7B \u63d0\u4f9b\u8d85\u904e 1.4 \u500d\u548c 1.6 \u500d\u7684\u52a0\u901f\u3002", "author": "Minsik Cho et.al.", "authors": "Minsik Cho, Mohammad Rastegari, Devang Naik", "id": "2405.05329v1", "paper_url": "http://arxiv.org/abs/2405.05329v1", "repo": "null"}}