{"2501.01243": {"publish_time": "2025-01-02", "title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants", "paper_summary": "Faces and humans are crucial elements in social interaction and are widely\nincluded in everyday photos and videos. Therefore, a deep understanding of\nfaces and humans will enable multi-modal assistants to achieve improved\nresponse quality and broadened application scope. Currently, the multi-modal\nassistant community lacks a comprehensive and scientific evaluation of face and\nhuman understanding abilities. In this paper, we first propose a hierarchical\nability taxonomy that includes three levels of abilities. Then, based on this\ntaxonomy, we collect images and annotations from publicly available datasets in\nthe face and human community and build a semi-automatic data pipeline to\nproduce problems for the new benchmark. Finally, the obtained Face-Human-Bench\ncomprises a development set with 900 problems and a test set with 1800\nproblems, supporting both English and Chinese. We conduct evaluations over 25\nmainstream multi-modal large language models (MLLMs) with our Face-Human-Bench,\nfocusing on the correlation between abilities, the impact of the relative\nposition of targets on performance, and the impact of Chain of Thought (CoT)\nprompting on performance. Moreover, inspired by multi-modal agents, we also\nexplore which abilities of MLLMs need to be supplemented by specialist models.", "paper_summary_zh": "\u81c9\u90e8\u548c\u4eba\u985e\u662f\u793e\u4ea4\u4e92\u52d5\u4e2d\u81f3\u95dc\u91cd\u8981\u7684\u5143\u7d20\uff0c\u4e26\u5ee3\u6cdb\u5305\u542b\u5728\u65e5\u5e38\u7167\u7247\u548c\u5f71\u7247\u4e2d\u3002\u56e0\u6b64\uff0c\u5c0d\u81c9\u90e8\u548c\u4eba\u985e\u7684\u6df1\u5165\u4e86\u89e3\u5c07\u4f7f\u591a\u6a21\u614b\u52a9\u7406\u80fd\u5920\u7372\u5f97\u66f4\u597d\u7684\u56de\u61c9\u54c1\u8cea\u548c\u66f4\u5ee3\u6cdb\u7684\u61c9\u7528\u7bc4\u570d\u3002\u76ee\u524d\uff0c\u591a\u6a21\u614b\u52a9\u7406\u793e\u7fa4\u7f3a\u4e4f\u5c0d\u81c9\u90e8\u548c\u4eba\u985e\u7406\u89e3\u80fd\u529b\u7684\u5168\u9762\u4e14\u79d1\u5b78\u7684\u8a55\u4f30\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u4e00\u500b\u5206\u5c64\u80fd\u529b\u5206\u985e\u6cd5\uff0c\u5176\u4e2d\u5305\u542b\u4e09\u7d1a\u80fd\u529b\u3002\u7136\u5f8c\uff0c\u57fa\u65bc\u6b64\u5206\u985e\u6cd5\uff0c\u6211\u5011\u5f9e\u4eba\u81c9\u548c\u4eba\u985e\u793e\u7fa4\u4e2d\u516c\u958b\u53ef\u7528\u7684\u8cc7\u6599\u96c6\u6536\u96c6\u5f71\u50cf\u548c\u8a3b\u89e3\uff0c\u4e26\u5efa\u7acb\u4e00\u500b\u534a\u81ea\u52d5\u5316\u8cc7\u6599\u7ba1\u9053\uff0c\u4ee5\u7522\u751f\u65b0\u57fa\u6e96\u7684\u554f\u984c\u3002\u6700\u5f8c\uff0c\u7372\u5f97\u7684 Face-Human-Bench \u5305\u542b\u4e00\u500b\u6709 900 \u500b\u554f\u984c\u7684\u958b\u767c\u96c6\u548c\u4e00\u500b\u6709 1800 \u500b\u554f\u984c\u7684\u6e2c\u8a66\u96c6\uff0c\u540c\u6642\u652f\u63f4\u82f1\u6587\u548c\u4e2d\u6587\u3002\u6211\u5011\u4f7f\u7528\u6211\u5011\u7684 Face-Human-Bench \u5c0d 25 \u500b\u4e3b\u6d41\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u9032\u884c\u8a55\u4f30\uff0c\u91cd\u9ede\u95dc\u6ce8\u80fd\u529b\u4e4b\u9593\u7684\u95dc\u806f\u6027\u3001\u76ee\u6a19\u7684\u76f8\u5c0d\u4f4d\u7f6e\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u4ee5\u53ca\u601d\u8003\u93c8 (CoT) \u63d0\u793a\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\u3002\u6b64\u5916\uff0c\u53d7\u5230\u591a\u6a21\u614b\u4ee3\u7406\u7684\u555f\u767c\uff0c\u6211\u5011\u4e5f\u63a2\u8a0e\u4e86\u54ea\u4e9b MLLM \u7684\u80fd\u529b\u9700\u8981\u7531\u5c08\u5bb6\u6a21\u578b\u88dc\u5145\u3002", "author": "Lixiong Qin et.al.", "authors": "Lixiong Qin, Shilong Ou, Miaoxuan Zhang, Jiangning Wei, Yuhang Zhang, Xiaoshuai Song, Yuchen Liu, Mei Wang, Weiran Xu", "id": "2501.01243v1", "paper_url": "http://arxiv.org/abs/2501.01243v1", "repo": "null"}}