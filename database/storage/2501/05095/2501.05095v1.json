{"2501.05095": {"publish_time": "2025-01-09", "title": "Advancing ALS Applications with Large-Scale Pre-training: Dataset Development and Downstream Assessment", "paper_summary": "The pre-training and fine-tuning paradigm has revolutionized satellite remote\nsensing applications. However, this approach remains largely underexplored for\nairborne laser scanning (ALS), an important technology for applications such as\nforest management and urban planning. In this study, we address this gap by\nconstructing a large-scale ALS point cloud dataset and evaluating its impact on\ndownstream applications. Our dataset comprises ALS point clouds collected\nacross the contiguous United States, provided by the United States Geological\nSurvey's 3D Elevation Program. To ensure efficient data collection while\ncapturing diverse land cover and terrain types, we introduce a geospatial\nsampling method that selects point cloud tiles based on land cover maps and\ndigital elevation models. As a baseline self-supervised learning model, we\nadopt BEV-MAE, a state-of-the-art masked autoencoder for 3D outdoor point\nclouds, and pre-train it on the constructed dataset. The pre-trained models are\nsubsequently fine-tuned for downstream tasks, including tree species\nclassification, terrain scene recognition, and point cloud semantic\nsegmentation. Our results show that the pre-trained models significantly\noutperform their scratch counterparts across all downstream tasks,\ndemonstrating the transferability of the representations learned from the\nproposed dataset. Furthermore, we observe that scaling the dataset using our\ngeospatial sampling method consistently enhances performance, whereas\npre-training on datasets constructed with random sampling fails to achieve\nsimilar improvements. These findings highlight the utility of the constructed\ndataset and the effectiveness of our sampling strategy in the pre-training and\nfine-tuning paradigm. The source code and pre-trained models will be made\npublicly available at \\url{https://github.com/martianxiu/ALS_pretraining}.", "paper_summary_zh": "\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u7bc4\u4f8b\u5df2\u5fb9\u5e95\u6539\u8b8a\u885b\u661f\u9059\u6e2c\u61c9\u7528\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u5728\u6a5f\u8f09\u96f7\u5c04\u6383\u63cf (ALS) \u4e0a\u4ecd\u672a\u88ab\u5ee3\u6cdb\u63a2\u7d22\uff0c\u800c ALS \u662f\u4e00\u9805\u61c9\u7528\u65bc\u68ee\u6797\u7ba1\u7406\u548c\u90fd\u5e02\u898f\u5283\u7b49\u9818\u57df\u7684\u91cd\u8981\u6280\u8853\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5efa\u69cb\u5927\u898f\u6a21 ALS \u9ede\u96f2\u8cc7\u6599\u96c6\u4e26\u8a55\u4f30\u5176\u5c0d\u4e0b\u6e38\u61c9\u7528\u7684\u5f71\u97ff\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u5305\u542b\u7531\u7f8e\u570b\u5730\u8cea\u8abf\u67e5\u5c40\u7684 3D \u9ad8\u7a0b\u8a08\u756b\u6240\u63d0\u4f9b\u7684\uff0c\u5728\u7f8e\u570b\u672c\u571f\u6536\u96c6\u7684 ALS \u9ede\u96f2\u3002\u70ba\u4e86\u78ba\u4fdd\u6709\u6548\u7387\u7684\u8cc7\u6599\u6536\u96c6\uff0c\u540c\u6642\u64f7\u53d6\u591a\u6a23\u7684\u571f\u5730\u8986\u84cb\u548c\u5730\u5f62\u985e\u578b\uff0c\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u5730\u7406\u7a7a\u9593\u53d6\u6a23\u65b9\u6cd5\uff0c\u6839\u64da\u571f\u5730\u8986\u84cb\u5730\u5716\u548c\u6578\u4f4d\u9ad8\u7a0b\u6a21\u578b\u4f86\u9078\u64c7\u9ede\u96f2\u78da\u584a\u3002\u8eab\u70ba\u57fa\u6e96\u7684\u81ea\u76e3\u7763\u5f0f\u5b78\u7fd2\u6a21\u578b\uff0c\u6211\u5011\u63a1\u7528 BEV-MAE\uff0c\u9019\u662f\u4e00\u7a2e\u6700\u5148\u9032\u7684 3D \u6236\u5916\u9ede\u96f2\u906e\u7f69\u81ea\u52d5\u7de8\u78bc\u5668\uff0c\u4e26\u5728\u5efa\u69cb\u7684\u8cc7\u6599\u96c6\u4e0a\u5c0d\u5176\u9032\u884c\u9810\u8a13\u7df4\u3002\u9810\u8a13\u7df4\u6a21\u578b\u96a8\u5f8c\u91dd\u5c0d\u4e0b\u6e38\u4efb\u52d9\u9032\u884c\u5fae\u8abf\uff0c\u5305\u62ec\u6a39\u7a2e\u5206\u985e\u3001\u5730\u5f62\u5834\u666f\u8fa8\u8b58\u548c\u9ede\u96f2\u8a9e\u610f\u5206\u5272\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u9810\u8a13\u7df4\u6a21\u578b\u5728\u6240\u6709\u4e0b\u6e38\u4efb\u52d9\u4e2d\u90fd\u986f\u8457\u512a\u65bc\u5f9e\u982d\u8a13\u7df4\u7684\u6a21\u578b\uff0c\u8b49\u660e\u4e86\u5f9e\u5efa\u8b70\u8cc7\u6599\u96c6\u4e2d\u5b78\u7fd2\u5230\u7684\u8868\u5fb5\u7684\u53ef\u8f49\u79fb\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u4f7f\u7528\u6211\u5011\u7684\u5730\u7406\u7a7a\u9593\u53d6\u6a23\u65b9\u6cd5\u64f4\u5145\u8cc7\u6599\u96c6\u6703\u6301\u7e8c\u63d0\u5347\u6548\u80fd\uff0c\u800c\u4f7f\u7528\u96a8\u6a5f\u53d6\u6a23\u5efa\u69cb\u7684\u8cc7\u6599\u96c6\u9032\u884c\u9810\u8a13\u7df4\u5247\u7121\u6cd5\u7372\u5f97\u985e\u4f3c\u7684\u9032\u6b65\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u5efa\u69cb\u8cc7\u6599\u96c6\u7684\u6548\u7528\uff0c\u4ee5\u53ca\u6211\u5011\u7684\u53d6\u6a23\u7b56\u7565\u5728\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u7bc4\u4f8b\u4e2d\u7684\u6709\u6548\u6027\u3002\u539f\u59cb\u78bc\u548c\u9810\u8a13\u7df4\u6a21\u578b\u5c07\u5728 \\url{https://github.com/martianxiu/ALS_pretraining} \u516c\u958b\u3002", "author": "Haoyi Xiu et.al.", "authors": "Haoyi Xiu, Xin Liu, Taehoon Kim, Kyoung-Sook Kim", "id": "2501.05095v1", "paper_url": "http://arxiv.org/abs/2501.05095v1", "repo": "null"}}