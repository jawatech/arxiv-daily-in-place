
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-26**|**Towards Compositionality in Concept Learning**|Adam Stein et.al.|[2406.18534v1](http://arxiv.org/abs/2406.18534v1)|[link](https://github.com/adaminsky/compositional_concepts)|
|**2024-06-26**|**Symbolic Learning Enables Self-Evolving Agents**|Wangchunshu Zhou et.al.|[2406.18532v1](http://arxiv.org/abs/2406.18532v1)|[link](https://github.com/aiwaves-cn/agents)|
|**2024-06-26**|**PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation**|Christoph Leiter et.al.|[2406.18528v1](http://arxiv.org/abs/2406.18528v1)|null|
|**2024-06-26**|**ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation**|Shenghai Yuan et.al.|[2406.18522v1](http://arxiv.org/abs/2406.18522v1)|null|
|**2024-06-26**|**CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs**|Zirui Wang et.al.|[2406.18521v1](http://arxiv.org/abs/2406.18521v1)|null|
|**2024-06-26**|**APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets**|Zuxin Liu et.al.|[2406.18518v1](http://arxiv.org/abs/2406.18518v1)|null|
|**2024-06-26**|**"Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline**|Grace Li et.al.|[2406.18512v1](http://arxiv.org/abs/2406.18512v1)|null|
|**2024-06-26**|**WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models**|Liwei Jiang et.al.|[2406.18510v1](http://arxiv.org/abs/2406.18510v1)|null|
|**2024-06-26**|**Mental Modeling of Reinforcement Learning Agents by Language Models**|Wenhao Lu et.al.|[2406.18505v1](http://arxiv.org/abs/2406.18505v1)|null|
|**2024-06-26**|**Is In-Context Learning a Type of Gradient-Based Learning? Evidence from the Inverse Frequency Effect in Structural Priming**|Zhenghao Zhou et.al.|[2406.18501v1](http://arxiv.org/abs/2406.18501v1)|null|
|**2024-06-26**|**WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs**|Seungju Han et.al.|[2406.18495v1](http://arxiv.org/abs/2406.18495v1)|null|
|**2024-06-26**|**Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation**|Ahmed Njifenjou et.al.|[2406.18460v1](http://arxiv.org/abs/2406.18460v1)|null|
|**2024-06-26**|**Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers**|Jonas Ngnawé et.al.|[2406.18451v1](http://arxiv.org/abs/2406.18451v1)|[link](https://github.com/ngnawejonas/margin-consistency)|
|**2024-06-26**|**Preference Elicitation for Offline Reinforcement Learning**|Alizée Pace et.al.|[2406.18450v1](http://arxiv.org/abs/2406.18450v1)|null|
|**2024-06-26**|**Cascading Large Language Models for Salient Event Graph Generation**|Xingwei Tan et.al.|[2406.18449v1](http://arxiv.org/abs/2406.18449v1)|null|
|**2024-06-26**|**Graph Neural Networks for Emulation of Finite-Element Ice Dynamics in Greenland and Antarctic Ice Sheets**|Younghyun Koo et.al.|[2406.18423v1](http://arxiv.org/abs/2406.18423v1)|null|
|**2024-06-26**|**Mixture of Experts in a Mixture of RL settings**|Timon Willi et.al.|[2406.18420v1](http://arxiv.org/abs/2406.18420v1)|null|
|**2024-06-26**|**IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons**|Dan Shi et.al.|[2406.18406v1](http://arxiv.org/abs/2406.18406v1)|null|
|**2024-06-26**|**LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks**|Anna Bavaresco et.al.|[2406.18403v1](http://arxiv.org/abs/2406.18403v1)|null|
|**2024-06-26**|**Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers**|Yibo Jiang et.al.|[2406.18400v1](http://arxiv.org/abs/2406.18400v1)|null|
|**2024-06-26**|**AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha Factors**|Hao Shi et.al.|[2406.18394v1](http://arxiv.org/abs/2406.18394v1)|null|
|**2024-06-26**|**MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for Iterative Binary Malware Summarization**|Haolang Lu et.al.|[2406.18379v1](http://arxiv.org/abs/2406.18379v1)|null|
|**2024-06-26**|**Dynamic Data Pruning for Automatic Speech Recognition**|Qiao Xiao et.al.|[2406.18373v1](http://arxiv.org/abs/2406.18373v1)|null|
|**2024-06-26**|**Themis: Towards Flexible and Interpretable NLG Evaluation**|Xinyu Hu et.al.|[2406.18365v1](http://arxiv.org/abs/2406.18365v1)|null|
|**2024-06-26**|**Research on Information Extraction of LCSTS Dataset Based on an Improved BERTSum-LSTM Model**|Yiming Chen et.al.|[2406.18364v1](http://arxiv.org/abs/2406.18364v1)|null|
|**2024-06-26**|**Stable Diffusion Segmentation for Biomedical Images with Single-step Reverse Process**|Tianyu Lin et.al.|[2406.18361v1](http://arxiv.org/abs/2406.18361v1)|[link](https://github.com/lin-tianyu/stable-diffusion-seg)|
|**2024-06-26**|**Kolmogorov-Arnold Graph Neural Networks**|Gianluca De Carlo et.al.|[2406.18354v1](http://arxiv.org/abs/2406.18354v1)|null|
|**2024-06-26**|**AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations**|Adam Dahlgren Lindström et.al.|[2406.18346v1](http://arxiv.org/abs/2406.18346v1)|null|
|**2024-06-26**|**Grammar Assistance Using Syntactic Structures (GAUSS)**|Olga Zamaraeva et.al.|[2406.18340v1](http://arxiv.org/abs/2406.18340v1)|null|
|**2024-06-26**|**Continuous Sign Language Recognition Using Intra-inter Gloss Attention**|Hossein Ranjbar et.al.|[2406.18333v1](http://arxiv.org/abs/2406.18333v1)|null|
|**2024-06-26**|**PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models**|Huixuan Zhang et.al.|[2406.18326v1](http://arxiv.org/abs/2406.18326v1)|null|
|**2024-06-26**|**MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large Language Models Using Odyssey Math Data**|Meng Fang et.al.|[2406.18321v1](http://arxiv.org/abs/2406.18321v1)|null|
|**2024-06-26**|**Advancing Airport Tower Command Recognition: Integrating Squeeze-and-Excitation and Broadcasted Residual Learning**|Yuanxi Lin et.al.|[2406.18313v1](http://arxiv.org/abs/2406.18313v1)|null|
|**2024-06-26**|**AI-native Memory: A Pathway from LLMs Towards AGI**|Jingbo Shang et.al.|[2406.18312v1](http://arxiv.org/abs/2406.18312v1)|null|
|**2024-06-26**|**S3: A Simple Strong Sample-effective Multimodal Dialog System**|Elisei Rykov et.al.|[2406.18305v1](http://arxiv.org/abs/2406.18305v1)|[link](https://github.com/s-nlp/s3)|
|**2024-06-26**|**MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research**|Song Li et.al.|[2406.18301v1](http://arxiv.org/abs/2406.18301v1)|null|
|**2024-06-26**|**FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning**|Yufeng Li et.al.|[2406.18297v1](http://arxiv.org/abs/2406.18297v1)|null|
|**2024-06-26**|**Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs**|Lei Zhang et.al.|[2406.18294v1](http://arxiv.org/abs/2406.18294v1)|[link](https://github.com/hambaobao/hcp-coder)|
|**2024-06-26**|**Sanskrit Knowledge-based Systems: Annotation and Computational Tools**|Hrishikesh Terdalkar et.al.|[2406.18276v1](http://arxiv.org/abs/2406.18276v1)|null|
|**2024-06-26**|**"Vorbeşti Româneşte?" A Recipe to Train Powerful Romanian LLMs with English Instructions**|Mihai Masala et.al.|[2406.18266v1](http://arxiv.org/abs/2406.18266v1)|null|
|**2024-06-26**|**Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability is Complicated**|Jiazhou Ji et.al.|[2406.18259v1](http://arxiv.org/abs/2406.18259v1)|null|
|**2024-06-26**|**LLaMIPa: An Incremental Discourse Parser**|Kate Thompson et.al.|[2406.18256v1](http://arxiv.org/abs/2406.18256v1)|null|
|**2024-06-26**|**Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning**|Zhijie Nie et.al.|[2406.18254v1](http://arxiv.org/abs/2406.18254v1)|null|
|**2024-06-26**|**Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems**|Italo Luis da Silva et.al.|[2406.18245v1](http://arxiv.org/abs/2406.18245v1)|[link](https://github.com/oyarsa/event_extraction)|
|**2024-06-26**|**Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets**|Simon Münker et.al.|[2406.18239v1](http://arxiv.org/abs/2406.18239v1)|null|
|**2024-06-26**|**GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension**|Jiafeng Liang et.al.|[2406.18227v1](http://arxiv.org/abs/2406.18227v1)|null|
|**2024-06-26**|**Enhancing Data Privacy in Large Language Models through Private Association Editing**|Davide Venditti et.al.|[2406.18221v1](http://arxiv.org/abs/2406.18221v1)|null|
|**2024-06-26**|**Guiding Video Prediction with Explicit Procedural Knowledge**|Patrick Takenaka et.al.|[2406.18220v1](http://arxiv.org/abs/2406.18220v1)|null|
|**2024-06-26**|**A Closer Look into Mixture-of-Experts in Large Language Models**|Ka Man Lo et.al.|[2406.18219v1](http://arxiv.org/abs/2406.18219v1)|[link](https://github.com/kamanphoebe/look-into-moes)|
|**2024-06-26**|**SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding**|Zhenglin Wang et.al.|[2406.18200v1](http://arxiv.org/abs/2406.18200v1)|null|
|**2024-06-26**|**MammothModa: Multi-Modal Large Language Model**|Qi She et.al.|[2406.18193v1](http://arxiv.org/abs/2406.18193v1)|null|
|**2024-06-26**|**Methodology of Adapting Large English Language Models for Specific Cultural Contexts**|Wenjing Zhang et.al.|[2406.18192v1](http://arxiv.org/abs/2406.18192v1)|null|
|**2024-06-26**|**Selective Prompting Tuning for Personalized Conversations with LLMs**|Qiushi Huang et.al.|[2406.18187v1](http://arxiv.org/abs/2406.18187v1)|[link](https://github.com/hqsiswiliam/SPT)|
|**2024-06-26**|**Games of Knightian Uncertainty**|Spyridon Samothrakis et.al.|[2406.18178v1](http://arxiv.org/abs/2406.18178v1)|null|
|**2024-06-26**|**Galaxy spectroscopy without spectra: Galaxy properties from photometric images with conditional diffusion models**|Lars Doorenbos et.al.|[2406.18175v1](http://arxiv.org/abs/2406.18175v1)|[link](https://github.com/larsdoorenbos/generate-spectra)|
|**2024-06-26**|**UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs**|Wenhao Li et.al.|[2406.18173v1](http://arxiv.org/abs/2406.18173v1)|null|
|**2024-06-26**|**NeBuLa: A discourse aware Minecraft Builder**|Akshay Chaturvedi et.al.|[2406.18164v1](http://arxiv.org/abs/2406.18164v1)|null|
|**2024-06-26**|**Innovating for Tomorrow: The Convergence of SE and Green AI**|Luís Cruz et.al.|[2406.18142v1](http://arxiv.org/abs/2406.18142v1)|null|
|**2024-06-26**|**LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference**|Zhongwei Wan et.al.|[2406.18139v1](http://arxiv.org/abs/2406.18139v1)|null|
|**2024-06-26**|**Automatic Speech Recognition for Hindi**|Anish Saha et.al.|[2406.18135v1](http://arxiv.org/abs/2406.18135v1)|null|
|**2024-06-26**|**Assessing "Implicit" Retrieval Robustness of Large Language Models**|Xiaoyu Shen et.al.|[2406.18134v1](http://arxiv.org/abs/2406.18134v1)|null|
|**2024-06-26**|**ConvoCache: Smart Re-Use of Chatbot Responses**|Conor Atkins et.al.|[2406.18133v1](http://arxiv.org/abs/2406.18133v1)|null|
|**2024-06-26**|**ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets and Large Language Models**|Ahmed Heakl et.al.|[2406.18125v1](http://arxiv.org/abs/2406.18125v1)|null|
|**2024-06-26**|**Poisoned LangChain: Jailbreak LLMs by LangChain**|Ziqiu Wang et.al.|[2406.18122v1](http://arxiv.org/abs/2406.18122v1)|null|
|**2024-06-26**|**ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech Recognition Using LLMs**|Ahmed Heakl et.al.|[2406.18120v1](http://arxiv.org/abs/2406.18120v1)|[link](https://github.com/ahmedheakl/arazn-llm)|
|**2024-06-26**|**SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance**|Caishuang Huang et.al.|[2406.18118v1](http://arxiv.org/abs/2406.18118v1)|null|
|**2024-06-26**|**BADGE: BADminton report Generation and Evaluation with LLM**|Shang-Hsuan Chiang et.al.|[2406.18116v1](http://arxiv.org/abs/2406.18116v1)|[link](https://github.com/andychiangsh/badge)|
|**2024-06-26**|**Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps**|Dicong Qiu et.al.|[2406.18115v1](http://arxiv.org/abs/2406.18115v1)|null|
|**2024-06-26**|**Token-Weighted RNN-T for Learning from Flawed Data**|Gil Keren et.al.|[2406.18108v1](http://arxiv.org/abs/2406.18108v1)|null|
|**2024-06-26**|**Shimo Lab at "Discharge Me!": Discharge Summarization by Prompt-Driven Concatenation of Electronic Health Record Sections**|Yunzhen He et.al.|[2406.18094v1](http://arxiv.org/abs/2406.18094v1)|null|
|**2024-06-26**|**LLM-Driven Multimodal Opinion Expression Identification**|Bonian Jia et.al.|[2406.18088v1](http://arxiv.org/abs/2406.18088v1)|null|
|**2024-06-26**|**EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction Using Large Language Multimodal Models**|Chun-Chieh Liao et.al.|[2406.18087v1](http://arxiv.org/abs/2406.18087v1)|null|
|**2024-06-26**|**Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints**|Ran Song et.al.|[2406.18085v1](http://arxiv.org/abs/2406.18085v1)|null|
|**2024-06-26**|**Octo-planner: On-device Language Model for Planner-Action Agents**|Wei Chen et.al.|[2406.18082v1](http://arxiv.org/abs/2406.18082v1)|null|
|**2024-06-26**|**Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction**|Yice Zhang et.al.|[2406.18078v1](http://arxiv.org/abs/2406.18078v1)|[link](https://github.com/hitsz-hlt/st-w-scorer-absa)|
|**2024-06-26**|**Few-Shot Medical Image Segmentation with High-Fidelity Prototypes**|Song Tang et.al.|[2406.18074v1](http://arxiv.org/abs/2406.18074v1)|null|
|**2024-06-26**|**Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals**|Zengding Liu et.al.|[2406.18069v1](http://arxiv.org/abs/2406.18069v1)|null|
|**2024-06-26**|**Exploring Energy-Based Models for Out-of-Distribution Detection in Dialect Identification**|Yaqian Hao et.al.|[2406.18067v1](http://arxiv.org/abs/2406.18067v1)|null|
|**2024-06-26**|**Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need**|Yang Wang et.al.|[2406.18064v1](http://arxiv.org/abs/2406.18064v1)|null|
|**2024-06-26**|**AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning**|Yifan Yang et.al.|[2406.18060v1](http://arxiv.org/abs/2406.18060v1)|[link](https://github.com/yifanycc/adazeta)|
|**2024-06-26**|**Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources**|Yiming Li et.al.|[2406.18049v1](http://arxiv.org/abs/2406.18049v1)|null|
|**2024-06-26**|**PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry**|Linqing Chen et.al.|[2406.18045v1](http://arxiv.org/abs/2406.18045v1)|null|
|**2024-06-26**|**Multimodal foundation world models for generalist embodied agents**|Pietro Mazzaglia et.al.|[2406.18043v1](http://arxiv.org/abs/2406.18043v1)|null|
|**2024-06-26**|**LLMs for Doctors: Leveraging Medical LLMs to Assist Doctors, Not Replace Them**|Wenya Xie et.al.|[2406.18034v1](http://arxiv.org/abs/2406.18034v1)|null|
|**2024-06-26**|**Automated Clinical Data Extraction with Knowledge Conditioned LLMs**|Diya Li et.al.|[2406.18027v1](http://arxiv.org/abs/2406.18027v1)|null|
|**2024-06-26**|**AutoOPE: Automated Off-Policy Estimator Selection**|Nicolò Felicioni et.al.|[2406.18022v1](http://arxiv.org/abs/2406.18022v1)|null|
|**2024-06-26**|**Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher**|Hyunjong Ok et.al.|[2406.18002v1](http://arxiv.org/abs/2406.18002v1)|null|
|**2024-06-26**|**Catching Chameleons: Detecting Evolving Disinformation Generated using Large Language Models**|Bohan Jiang et.al.|[2406.17992v1](http://arxiv.org/abs/2406.17992v1)|null|
|**2024-06-26**|**Explicit Diversity Conditions for Effective Question Answer Generation with Large Language Models**|Vikas Yadav et.al.|[2406.17990v1](http://arxiv.org/abs/2406.17990v1)|null|
|**2024-06-26**|**Multi-step Knowledge Retrieval and Inference over Unstructured Data**|Aditya Kalyanpur et.al.|[2406.17987v1](http://arxiv.org/abs/2406.17987v1)|null|
|**2024-06-25**|**EDEN: Empathetic Dialogues for English learning**|Li Siyan et.al.|[2406.17982v1](http://arxiv.org/abs/2406.17982v1)|null|
|**2024-06-25**|**Inherent Challenges of Post-Hoc Membership Inference for Large Language Models**|Matthieu Meeus et.al.|[2406.17975v1](http://arxiv.org/abs/2406.17975v1)|null|
|**2024-06-25**|**Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts**|Xuyang Wu et.al.|[2406.17974v1](http://arxiv.org/abs/2406.17974v1)|null|
|**2024-06-25**|**LABOR-LLM: Language-Based Occupational Representations with Large Language Models**|Tianyu Du et.al.|[2406.17972v1](http://arxiv.org/abs/2406.17972v1)|null|
|**2024-06-25**|**Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective**|Hanqi Yan et.al.|[2406.17969v1](http://arxiv.org/abs/2406.17969v1)|null|
|**2024-06-25**|**Efficient Document Ranking with Learnable Late Interactions**|Ziwei Ji et.al.|[2406.17968v1](http://arxiv.org/abs/2406.17968v1)|null|
|**2024-06-25**|**Unmasking the Imposters: In-Domain Detection of Human vs. Machine-Generated Tweets**|Bryan E. Tuck et.al.|[2406.17967v1](http://arxiv.org/abs/2406.17967v1)|null|
|**2024-06-25**|**SimsChat: A Customisable Persona-Driven Role-Playing Agent**|Bohao Yang et.al.|[2406.17962v1](http://arxiv.org/abs/2406.17962v1)|[link](https://github.com/bernard-yang/simschat)|
|**2024-06-25**|**NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization**|Md Mahadi Hasan Nahid et.al.|[2406.17961v1](http://arxiv.org/abs/2406.17961v1)|null|
|**2024-06-25**|**MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation**|Liuyi Wang et.al.|[2406.17960v1](http://arxiv.org/abs/2406.17960v1)|[link](https://github.com/crystalsixone/vln-magic)|

#### Abstracts
##### **Towards Compositionality in Concept Learning**
2406.18534v1 by Adam Stein, Aaditya Naik, Yinjun Wu, Mayur Naik, Eric Wong

Concept-based interpretability methods offer a lens into the internals of
foundation models by decomposing their embeddings into high-level concepts.
These concept representations are most useful when they are compositional,
meaning that the individual concepts compose to explain the full sample. We
show that existing unsupervised concept extraction methods find concepts which
are not compositional. To automatically discover compositional concept
representations, we identify two salient properties of such representations,
and propose Compositional Concept Extraction (CCE) for finding concepts which
obey these properties. We evaluate CCE on five different datasets over image
and text data. Our evaluation shows that CCE finds more compositional concept
representations than baselines and yields better accuracy on four downstream
classification tasks. Code and data are available at
https://github.com/adaminsky/compositional_concepts .

摘要：基於概念的可解釋性方法提供了一個透鏡，可以透過將嵌入分解成高階概念來了解基礎模型的內部結構。
這些概念表示法在具有組合性的時候最有用，這表示個別概念可以組合起來解釋完整的樣本。我們證明現有的非監督式概念萃取方法會找到非組合性的概念。為了自動發現組合性的概念表示法，我們找出這種表示法的兩個顯著屬性，並提出組合性概念萃取 (CCE) 來尋找符合這些屬性的概念。我們在五個不同的資料集上評估 CCE，涵蓋影像和文字資料。我們的評估顯示，與基準線相比，CCE 找到了更多組合性的概念表示法，並且在四個下游分類任務中產生了更好的準確度。程式碼和資料可在 https://github.com/adaminsky/compositional_concepts 取得。

##### **Symbolic Learning Enables Self-Evolving Agents**
2406.18532v1 by Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang

The AI community has been exploring a pathway to artificial general
intelligence (AGI) by developing "language agents", which are complex large
language models (LLMs) pipelines involving both prompting techniques and tool
usage methods. While language agents have demonstrated impressive capabilities
for many real-world tasks, a fundamental limitation of current language agents
research is that they are model-centric, or engineering-centric. That's to say,
the progress on prompts, tools, and pipelines of language agents requires
substantial manual engineering efforts from human experts rather than
automatically learning from data. We believe the transition from model-centric,
or engineering-centric, to data-centric, i.e., the ability of language agents
to autonomously learn and evolve in environments, is the key for them to
possibly achieve AGI.
  In this work, we introduce agent symbolic learning, a systematic framework
that enables language agents to optimize themselves on their own in a
data-centric way using symbolic optimizers. Specifically, we consider agents as
symbolic networks where learnable weights are defined by prompts, tools, and
the way they are stacked together. Agent symbolic learning is designed to
optimize the symbolic network within language agents by mimicking two
fundamental algorithms in connectionist learning: back-propagation and gradient
descent. Instead of dealing with numeric weights, agent symbolic learning works
with natural language simulacrums of weights, loss, and gradients. We conduct
proof-of-concept experiments on both standard benchmarks and complex real-world
tasks and show that agent symbolic learning enables language agents to update
themselves after being created and deployed in the wild, resulting in
"self-evolving agents".

摘要：人工智慧社群已經透過開發「語言代理」，也就是包含提示技術和工具使用方式的複雜大型語言模型 (LLM) 管線，探索通往人工通用智慧 (AGI) 的途徑。儘管語言代理在許多實際任務中展現出令人印象深刻的能力，但目前語言代理研究的基本限制在於它們以模型為中心，或以工程為中心。也就是說，語言代理的提示、工具和管線的進度需要人類專家的大量手動工程，而不是自動從資料中學習。我們相信從以模型為中心或以工程為中心轉變為以資料為中心，也就是語言代理在環境中自主學習和演化的能力，是它們可能實現 AGI 的關鍵。
在這項工作中，我們引入了代理符號學習，這是一個系統性的架構，讓語言代理能夠使用符號最佳化器以資料為中心的方式自行最佳化。具體來說，我們將代理視為符號網路，其中可學習的權重由提示、工具和堆疊在一起的方式定義。代理符號學習旨在透過模仿連接主義學習中的兩個基本演算法：反向傳播和梯度下降，來最佳化語言代理中的符號網路。代理符號學習並非處理數值權重，而是處理權重、損失和梯度的自然語言模擬。我們在標準基準和複雜的實際任務上進行了概念驗證實驗，並展示代理符號學習讓語言代理能夠在被建立和部署到實際環境後更新自己，進而產生「自我演化代理」。

##### **PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation**
2406.18528v1 by Christoph Leiter, Steffen Eger

Large language models (LLMs) have revolutionized the field of NLP. Notably,
their in-context learning capabilities also enable their use as evaluation
metrics for natural language generation, making them particularly advantageous
in low-resource scenarios and time-restricted applications. In this work, we
introduce PrExMe, a large-scale prompt exploration for metrics, where we
evaluate more than 720 prompt templates for open-source LLM-based metrics on
machine translation (MT) and summarization datasets, totalling over 6.6M
evaluations. This extensive comparison (1) serves as a benchmark of the
performance of recent open-source LLMs as metrics and (2) explores the
stability and variability of different prompting strategies. We discover that,
on the one hand, there are scenarios for which prompts are stable. For
instance, some LLMs show idiosyncratic preferences and favor to grade generated
texts with textual labels while others prefer to return numeric scores. On the
other hand, the stability of prompts and model rankings can be susceptible to
seemingly innocuous changes. For example, changing the requested output format
from "0 to 100" to "-1 to +1" can strongly affect the rankings in our
evaluation. Our study contributes to understanding the impact of different
prompting approaches on LLM-based metrics for MT and summarization evaluation,
highlighting the most stable prompting patterns and potential limitations.

摘要：大型語言模型 (LLM) 已徹底改變自然語言處理 (NLP) 領域。值得注意的是，其情境學習能力也讓它們可用作自然語言生成的評估指標，這使得它們在低資源場景和時間受限的應用中特別有利。在這項工作中，我們介紹 PrExMe，一種用於指標的大規模提示探索，我們在機器翻譯 (MT) 和摘要資料集上針對超過 720 個基於開放原始碼 LLM 的指標評估提示範本，總計超過 660 萬次評估。這種廣泛的比較 (1) 作為指標最近開放原始碼 LLM 效能的基準，並 (2) 探索不同提示策略的穩定性和可變性。我們發現，一方面，有些場景提示是穩定的。例如，某些 LLM 顯示出獨特的偏好，並傾向於使用文字標籤對產生的文字進行評分，而另一些則偏好回傳數字分數。另一方面，提示和模型排名的穩定性可能容易受到看似無害的變更影響。例如，將要求的輸出格式從「0 到 100」變更為「-1 到 +1」會對我們的評估排名產生重大的影響。我們的研究有助於了解不同提示方法對用於機器翻譯和摘要評估的 LLM 指標的影響，強調最穩定的提示模式和潛在限制。

##### **ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation**
2406.18522v1 by Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, Li Yuan

We propose a novel text-to-video (T2V) generation benchmark,
ChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the
T2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast
to existing benchmarks that focus on the visual quality and textual relevance
of generated videos, ChronoMagic-Bench focuses on the model's ability to
generate time-lapse videos with significant metamorphic amplitude and temporal
coherence. The benchmark probes T2V models for their physics, biology, and
chemistry capabilities, in a free-form text query. For these purposes,
ChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,
categorized into four major types of time-lapse videos: biological,
human-created, meteorological, and physical phenomena, which are further
divided into 75 subcategories. This categorization comprehensively evaluates
the model's capacity to handle diverse and complex transformations. To
accurately align human preference with the benchmark, we introduce two new
automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic
attributes and temporal coherence. MTScore measures the metamorphic amplitude,
reflecting the degree of change over time, while CHScore assesses the temporal
coherence, ensuring the generated videos maintain logical progression and
continuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual
evaluations of ten representative T2V models, revealing their strengths and
weaknesses across different categories of prompts, and providing a thorough
evaluation framework that addresses current gaps in video generation research.
Moreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k
high-quality pairs of 720p time-lapse videos and detailed captions ensuring
high physical pertinence and large metamorphic amplitude.

摘要：<paragraph>我們提出一個創新的文字轉影片 (T2V) 生成基準，ChronoMagic-Bench，用於評估時序影片生成中 T2V 模型（例如 Sora 和 Lumiere）的時間和變形能力。與現有專注於生成影片的視覺品質和文字相關性的基準不同，ChronoMagic-Bench 專注於模型生成具有顯著變形幅度和時間相干性的時序影片的能力。此基準使用自由形式文字查詢來探測 T2V 模型的物理、生物和化學能力。為此，ChronoMagic-Bench 引入了 1,649 個提示和真實影片作為參考，分為四種類型的時序影片：生物、人造、氣象和物理現象，進一步分為 75 個子類別。此分類全面評估模型處理多樣且複雜轉換的能力。為了準確地將人類偏好與基準對齊，我們引入了兩個新的自動量測標準，MTScore 和 CHScore，以評估影片的變形屬性和時間相干性。MTScore 量測變形幅度，反映時間變化的程度，而 CHScore 評估時間相干性，確保生成的影片維持邏輯進程和連續性。根據 ChronoMagic-Bench，我們對十個具代表性的 T2V 模型進行全面的手動評估，揭示它們在不同類別提示中的優缺點，並提供一個徹底的評估架構來解決影片生成研究中的現有差距。此外，我們建立了一個大型 ChronoMagic-Pro 資料集，其中包含 46 萬對高品質 720p 時序影片和詳細字幕，確保高度物理相關性和大的變形幅度。</paragraph>

##### **CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs**
2406.18521v1 by Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen

Chart understanding plays a pivotal role when applying Multimodal Large
Language Models (MLLMs) to real-world tasks such as analyzing scientific papers
or financial reports. However, existing datasets often focus on oversimplified
and homogeneous charts with template-based questions, leading to an
over-optimistic measure of progress. We demonstrate that although open-source
models can appear to outperform strong proprietary models on these benchmarks,
a simple stress test with slightly different charts or questions can
deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a
comprehensive evaluation suite involving 2,323 natural, challenging, and
diverse charts from arXiv papers. CharXiv includes two types of questions: 1)
descriptive questions about examining basic chart elements and 2) reasoning
questions that require synthesizing information across complex visual elements
in the chart. To ensure quality, all charts and questions are handpicked,
curated, and verified by human experts. Our results reveal a substantial,
previously underestimated gap between the reasoning skills of the strongest
proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the
strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.
All models lag far behind human performance of 80.5%, underscoring weaknesses
in the chart understanding capabilities of existing MLLMs. We hope CharXiv
facilitates future research on MLLM chart understanding by providing a more
realistic and faithful measure of progress. Project page and leaderboard:
https://charxiv.github.io/

摘要：<paragraph>在將多模態大型語言模型 (MLLM) 應用於分析科學論文或財務報告等實際任務時，圖表理解扮演著至關重要的角色。然而，現有的資料集通常著重於過於簡化且同質的圖表，並搭配基於範本的問題，導致進度的衡量過於樂觀。我們證明，儘管開源模型在這些基準測試中似乎優於強大的專有模型，但使用略有不同的圖表或問題進行簡單的壓力測試，會使效能下降多達 34.5%。在這項工作中，我們提出 CharXiv，這是一個全面的評估套件，包含來自 arXiv 論文的 2,323 個自然、具挑戰性和多樣化的圖表。CharXiv 包含兩種類型的問題：1) 關於檢查基本圖表元素的描述性問題，以及 2) 推理問題，需要綜合圖表中複雜視覺元素中的資訊。為了確保品質，所有圖表和問題都由人類專家親自挑選、策劃和驗證。我們的結果揭示了一個實質性的、先前低估的差距，存在於最強大的專有模型（即 GPT-4o）的推理技能（達到 47.1% 的準確度）和最強大的開源模型（即 InternVL Chat V1.5）的推理技能（達到 29.2%）之間。所有模型都遠遠落後於人類的 80.5% 效能，突顯了現有 MLLM 在圖表理解能力方面的弱點。我們希望 CharXiv 能夠透過提供更實際且忠實的進度衡量，促進未來關於 MLLM 圖表理解的研究。專案頁面和排行榜：https://charxiv.github.io/</paragraph>

##### **APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets**
2406.18518v1 by Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, Caiming Xiong

The advancement of function-calling agent models requires diverse, reliable,
and high-quality datasets. This paper presents APIGen, an automated data
generation pipeline designed to synthesize verifiable high-quality datasets for
function-calling applications. We leverage APIGen and collect 3,673 executable
APIs across 21 different categories to generate diverse function-calling
datasets in a scalable and structured manner. Each data in our dataset is
verified through three hierarchical stages: format checking, actual function
executions, and semantic verification, ensuring its reliability and
correctness. We demonstrate that models trained with our curated datasets, even
with only 7B parameters, can achieve state-of-the-art performance on the
Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.
Moreover, our 1B model achieves exceptional performance, surpassing
GPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000
high-quality entries, aiming to advance the field of function-calling agent
domains. The dataset is available on Huggingface:
https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the
project homepage: https://apigen-pipeline.github.io/

摘要：函數呼叫代理模型的進步需要多樣化、可靠且高品質的資料集。本文提出了 APIGen，這是一個自動化資料生成管線，旨在為函數呼叫應用程式合成可驗證的高品質資料集。我們利用 APIGen 並收集 21 個不同類別中的 3,673 個可執行 API，以可擴充且結構化的方式產生多樣化的函數呼叫資料集。我們資料集中的每個資料都透過三個階層階段進行驗證：格式檢查、實際函數執行和語義驗證，確保其可靠性和正確性。我們證明使用我們整理的資料集訓練的模型，即使只有 7B 個參數，也能在 Berkeley Function-Calling Benchmark 上達到最先進的效能，優於多個 GPT-4 模型。此外，我們的 1B 模型達到了非凡的效能，超越了 GPT-3.5-Turbo 和 Claude-3 Haiku。我們發布了一個包含 60,000 個高品質條目的資料集，旨在推進函數呼叫代理領域。該資料集可在 Huggingface 上取得：https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k，專案首頁為：https://apigen-pipeline.github.io/

##### **"Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline**
2406.18512v1 by Grace Li, Milad Alshomary, Smaranda Muresan

Explanations form the foundation of knowledge sharing and build upon
communication principles, social dynamics, and learning theories. We focus
specifically on conversational approaches for explanations because the context
is highly adaptive and interactive. Our research leverages previous work on
explanatory acts, a framework for understanding the different strategies that
explainers and explainees employ in a conversation to both explain, understand,
and engage with the other party. We use the 5-Levels dataset was constructed
from the WIRED YouTube series by Wachsmuth et al., and later annotated by
Booshehri et al. with explanatory acts. These annotations provide a framework
for understanding how explainers and explainees structure their response when
crafting a response.
  With the rise of generative AI in the past year, we hope to better understand
the capabilities of Large Language Models (LLMs) and how they can augment
expert explainer's capabilities in conversational settings. To achieve this
goal, the 5-Levels dataset (We use Booshehri et al.'s 2023 annotated dataset
with explanatory acts.) allows us to audit the ability of LLMs in engaging in
explanation dialogues. To evaluate the effectiveness of LLMs in generating
explainer responses, we compared 3 different strategies, we asked human
annotators to evaluate 3 different strategies: human explainer response, GPT4
standard response, GPT4 response with Explanation Moves.

摘要：解釋是知識分享的基礎，建立在溝通原則、社會動態和學習理論之上。我們特別專注於解釋的對話方法，因為其情境具有高度適應性和互動性。我們的研究利用了先前關於解釋行為的工作，這是一個用於理解解釋者和被解釋者在對話中用於解釋、理解和與對方互動的不同策略的框架。我們使用 5-Levels 資料集，該資料集是由 Wachsmuth 等人根據 WIRED YouTube 系列構建的，後由 Booshehri 等人使用解釋行為進行註解。這些註解提供了一個框架，用於了解解釋者和被解釋者在撰寫回應時如何構建他們的回應。隨著生成式 AI 在過去一年的興起，我們希望更好地了解大型語言模型 (LLM) 的能力，以及它們如何增強專家解釋者在對話環境中的能力。為了實現這一目標，5-Levels 資料集（我們使用 Booshehri 等人 2023 年帶有解釋行為的註解資料集）使我們能夠審核 LLM 在參與解釋對話中的能力。為了評估 LLM 在生成解釋者回應方面的有效性，我們比較了 3 種不同的策略，我們請人類註解者評估 3 種不同的策略：人類解釋者的回應、GPT4 標準回應、帶有解釋動作的 GPT4 回應。

##### **WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models**
2406.18510v1 by Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, Nouha Dziri

We introduce WildTeaming, an automatic LLM safety red-teaming framework that
mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of
novel jailbreak tactics, and then composes multiple tactics for systematic
exploration of novel jailbreaks. Compared to prior work that performed
red-teaming via recruited human workers, gradient-based optimization, or
iterative revision with LLMs, our work investigates jailbreaks from chatbot
users who were not specifically instructed to break the system. WildTeaming
reveals previously unidentified vulnerabilities of frontier LLMs, resulting in
up to 4.6x more diverse and successful adversarial attacks compared to
state-of-the-art jailbreak methods.
  While many datasets exist for jailbreak evaluation, very few open-source
datasets exist for jailbreak training, as safety training data has been closed
even when model weights are open. With WildTeaming we create WildJailbreak, a
large-scale open-source synthetic safety dataset with 262K vanilla (direct
request) and adversarial (complex jailbreak) prompt-response pairs. To mitigate
exaggerated safety behaviors, WildJailbreak provides two contrastive types of
queries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that
resemble harmful queries in form but contain no harm. As WildJailbreak
considerably upgrades the quality and scale of existing safety resources, it
uniquely enables us to examine the scaling effects of data and the interplay of
data properties and model capabilities during safety training. Through
extensive experiments, we identify the training properties that enable an ideal
balance of safety behaviors: appropriate safeguarding without over-refusal,
effective handling of vanilla and adversarial queries, and minimal, if any,
decrease in general capabilities. All components of WildJailbeak contribute to
achieving balanced safety behaviors of models.

摘要：<paragraph>我們引進 WildTeaming，一個自動 LLM 安全紅隊框架，它
挖掘野外使用者聊天機器人互動，以發現 5.7K 個獨特的
越獄策略群集，然後組合多種策略以系統性探索新的越獄。與先前透過招募人類工作者、基於梯度的最佳化或
與 LLM 進行反覆修改來執行紅隊測試的工作相比，我們的研究調查了聊天機器人使用者的越獄，這些使用者並未特別被指示破壞系統。WildTeaming
揭露了前沿 LLM 以前未發現的漏洞，導致多達 4.6 倍更多樣化且成功的對抗攻擊，與
最先進的越獄方法相比。
雖然存在許多用於越獄評估的資料集，但很少有開放原始碼
資料集可用於越獄訓練，因為即使模型權重是開放的，安全訓練資料也已關閉。透過 WildTeaming，我們建立了 WildJailbreak，一個
大型開放原始碼合成安全資料集，其中包含 262K 個香草（直接
請求）和對抗（複雜越獄）提示回應對。為了減輕誇大的安全行為，WildJailbreak 提供了兩種對比類型的
查詢：1) 有害查詢（香草和對抗）和 2) 良性查詢，其形式類似於有害查詢，但沒有危害。由於 WildJailbreak
大幅提升了現有安全資源的品質和規模，它讓我們能夠獨特地檢查資料的擴展效果，以及在安全訓練期間資料屬性和模型功能的交互作用。透過
廣泛的實驗，我們找出能夠實現安全行為理想平衡的訓練屬性：適當的保護措施，不會過度拒絕、有效處理香草和對抗查詢，以及一般功能的減少（如果有）。WildJailbeak 的所有組成部分都有助於實現模型的平衡安全行為。</paragraph>

##### **Mental Modeling of Reinforcement Learning Agents by Language Models**
2406.18505v1 by Wenhao Lu, Xufeng Zhao, Josua Spisak, Jae Hee Lee, Stefan Wermter

Can emergent language models faithfully model the intelligence of
decision-making agents? Though modern language models exhibit already some
reasoning ability, and theoretically can potentially express any probable
distribution over tokens, it remains underexplored how the world knowledge
these pretrained models have memorized can be utilized to comprehend an agent's
behaviour in the physical world. This study empirically examines, for the first
time, how well large language models (LLMs) can build a mental model of agents,
termed agent mental modelling, by reasoning about an agent's behaviour and its
effect on states from agent interaction history. This research may unveil the
potential of leveraging LLMs for elucidating RL agent behaviour, addressing a
key challenge in eXplainable reinforcement learning (XRL). To this end, we
propose specific evaluation metrics and test them on selected RL task datasets
of varying complexity, reporting findings on agent mental model establishment.
Our results disclose that LLMs are not yet capable of fully mental modelling
agents through inference alone without further innovations. This work thus
provides new insights into the capabilities and limitations of modern LLMs.

摘要：新興語言模型能否忠實模擬決策制定代理的智慧？儘管現代語言模型已經展現出一些推理能力，理論上也能表達任何可能的符號分佈，但這些預先訓練模型所記憶的世界知識如何用於理解代理在物理世界中的行為，這方面仍未得到充分探討。本研究首次實證檢驗大型語言模型 (LLM) 在推理代理行為及其對代理互動歷史狀態的影響後，建立代理心智模型（稱為代理心智建模）的能力。這項研究可能揭示利用 LLM 來闡明 RL 代理行為的潛力，以應對可解釋強化學習 (XRL) 中的一項關鍵挑戰。為此，我們提出了具體的評估指標，並在複雜度不同的選定 RL 任務資料集上對其進行測試，報告代理心智模型建立的發現。我們的結果揭示，LLM 尚未能夠僅透過推理，在沒有進一步創新的情況下，完全對代理進行心智建模。因此，這項工作對現代 LLM 的能力和限制提供了新的見解。

##### **Is In-Context Learning a Type of Gradient-Based Learning? Evidence from the Inverse Frequency Effect in Structural Priming**
2406.18501v1 by Zhenghao Zhou, Robert Frank, R. Thomas McCoy

Large language models (LLMs) have shown the emergent capability of in-context
learning (ICL). One line of research has explained ICL as functionally
performing gradient descent. In this paper, we introduce a new way of
diagnosing whether ICL is functionally equivalent to gradient-based learning.
Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in
which an error-driven learner is expected to show larger updates when trained
on infrequent examples than frequent ones. The IFE has previously been studied
in psycholinguistics because humans show this effect in the context of
structural priming (the tendency for people to produce sentence structures they
have encountered recently); the IFE has been used as evidence that human
structural priming must involve error-driven learning mechanisms. In our
experiments, we simulated structural priming within ICL and found that LLMs
display the IFE, with the effect being stronger in larger models. We conclude
that ICL is indeed a type of gradient-based learning, supporting the hypothesis
that a gradient component is implicitly computed in the forward pass during
ICL. Our results suggest that both humans and LLMs make use of gradient-based,
error-driven processing mechanisms.

摘要：大型語言模型 (LLM) 已展示出情境學習 (ICL) 的新興能力。一項研究將 ICL 解釋為功能性地執行梯度下降。在本文中，我們介紹了一種新的方式來診斷 ICL 是否在功能上等同於基於梯度的學習。我們的做法基於逆頻率效應 (IFE)——一種現象，其中預期錯誤驅動的學習者在不頻繁的範例上訓練時會顯示出比頻繁的範例更大的更新。IFE 之前已在心理語言學中進行過研究，因為人類在結構性啟動 (人們傾向於產生他們最近遇到的句子結構) 的背景下顯示出這種效應；IFE 已被用作證據，證明人類的結構性啟動必須涉及錯誤驅動的學習機制。在我們的實驗中，我們在 ICL 內模擬了結構性啟動，發現 LLM 顯示出 IFE，並且在較大的模型中效果更強。我們得出結論，ICL 確實是一種基於梯度的學習，支持在 ICL 期間的前向傳遞中隱式計算梯度組成的假設。我們的結果表明，人類和 LLM 都利用基於梯度的、錯誤驅動的處理機制。

##### **WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs**
2406.18495v1 by Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, Nouha Dziri

We introduce WildGuard -- an open, light-weight moderation tool for LLM
safety that achieves three goals: (1) identifying malicious intent in user
prompts, (2) detecting safety risks of model responses, and (3) determining
model refusal rate. Together, WildGuard serves the increasing needs for
automatic safety moderation and evaluation of LLM interactions, providing a
one-stop tool with enhanced accuracy and broad coverage across 13 risk
categories. While existing open moderation tools such as Llama-Guard2 score
reasonably well in classifying straightforward model interactions, they lag far
behind a prompted GPT-4, especially in identifying adversarial jailbreaks and
in evaluating models' refusals, a key measure for evaluating safety behaviors
in model responses.
  To address these challenges, we construct WildGuardMix, a large-scale and
carefully balanced multi-task safety moderation dataset with 92K labeled
examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired
with various refusal and compliance responses. WildGuardMix is a combination of
WildGuardTrain, the training data of WildGuard, and WildGuardTest, a
high-quality human-annotated moderation test set with 5K labeled items covering
broad risk scenarios. Through extensive evaluations on WildGuardTest and ten
existing public benchmarks, we show that WildGuard establishes state-of-the-art
performance in open-source safety moderation across all the three tasks
compared to ten strong existing open-source moderation models (e.g., up to
26.4% improvement on refusal detection). Importantly, WildGuard matches and
sometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt
harmfulness identification). WildGuard serves as a highly effective safety
moderator in an LLM interface, reducing the success rate of jailbreak attacks
from 79.8% to 2.4%.

摘要：<paragraph>我們推出 WildGuard——一種開放、輕量級的 LLM 安全性審核工具，可達成三個目標：(1) 識別使用者提示中的惡意意圖，(2) 偵測模型回應的安全性風險，以及 (3) 決定模型拒絕率。WildGuard 結合了對 LLM 互動進行自動安全性審核和評估日益增長的需求，提供了一種一站式工具，在 13 個風險類別中具備更高的準確度和廣泛的涵蓋範圍。儘管現有的開放式審核工具（例如 Llama-Guard2）在對直接的模型互動進行分類方面表現得相當不錯，但它們遠遠落後於提示式 GPT-4，特別是在識別對抗性越獄和評估模型拒絕方面，而拒絕是評估模型回應中安全性行為的一項關鍵指標。
為了應對這些挑戰，我們構建了 WildGuardMix，這是一個大型且經過仔細平衡的多任務安全性審核資料集，其中包含 92K 個標記範例，涵蓋了原始 (直接) 提示和對抗性越獄，並配對了各種拒絕和服從回應。WildGuardMix 結合了 WildGuard 的訓練資料 WildGuardTrain，以及 WildGuardTest，一個高品質的人工標記審核測試集，其中包含涵蓋廣泛風險情境的 5K 個標記項目。透過對 WildGuardTest 和十個現有公開基準進行廣泛的評估，我們表明 WildGuard 在開放原始碼安全性審核方面建立了最先進的效能，在所有三項任務上與十個強大的現有開放原始碼審核模型相比（例如，拒絕偵測的改進幅度高達 26.4%）。重要的是，WildGuard 匹配並有時超過 GPT-4 的效能（例如，提示危害識別的改進幅度高達 3.9%）。WildGuard 在 LLM 介面中作為一個高效能的安全性審核器，將越獄攻擊的成功率從 79.8% 降低到 2.4%。</paragraph>

##### **Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation**
2406.18460v1 by Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, Fabrice Lefèvre

Recently, various methods have been proposed to create open-domain
conversational agents with Large Language Models (LLMs). These models are able
to answer user queries, but in a one-way Q&A format rather than a true
conversation. Fine-tuning on particular datasets is the usual way to modify
their style to increase conversational ability, but this is expensive and
usually only available in a few languages. In this study, we explore role-play
zero-shot prompting as an efficient and cost-effective solution for open-domain
conversation, using capable multilingual LLMs (Beeching et al., 2023) trained
to obey instructions. We design a prompting system that, when combined with an
instruction-following model - here Vicuna (Chiang et al., 2023) - produces
conversational agents that match and even surpass fine-tuned models in human
evaluation in French in two different tasks.

摘要：最近，已經提出各種方法來使用大型語言模型 (LLM) 建立開放領域對話代理。這些模型能夠回答使用者的查詢，但採用單向問答格式，而非真正的對話。針對特定資料集進行微調是修改其風格以增加對話能力的常規方法，但這很昂貴，而且通常僅在少數語言中可用。在這項研究中，我們探討角色扮演零次提示，作為開放領域對話的高效且具有成本效益的解決方案，使用訓練有素的多語言 LLM（Beeching et al., 2023）來遵循指令。我們設計了一個提示系統，當與遵循指令的模型（此處為 Vicuna（Chiang et al., 2023））結合使用時，會產生對話代理，在法語中的兩項不同任務中，這些代理在人類評估中與微調模型相匹配，甚至超越微調模型。

##### **Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers**
2406.18451v1 by Jonas Ngnawé, Sabyasachi Sahoo, Yann Pequignot, Frédéric Precioso, Christian Gagné

Despite extensive research on adversarial training strategies to improve
robustness, the decisions of even the most robust deep learning models can
still be quite sensitive to imperceptible perturbations, creating serious risks
when deploying them for high-stakes real-world applications. While detecting
such cases may be critical, evaluating a model's vulnerability at a
per-instance level using adversarial attacks is computationally too intensive
and unsuitable for real-time deployment scenarios. The input space margin is
the exact score to detect non-robust samples and is intractable for deep neural
networks. This paper introduces the concept of margin consistency -- a property
that links the input space margins and the logit margins in robust models --
for efficient detection of vulnerable samples. First, we establish that margin
consistency is a necessary and sufficient condition to use a model's logit
margin as a score for identifying non-robust samples. Next, through
comprehensive empirical analysis of various robustly trained models on CIFAR10
and CIFAR100 datasets, we show that they indicate strong margin consistency
with a strong correlation between their input space margins and the logit
margins. Then, we show that we can effectively use the logit margin to
confidently detect brittle decisions with such models and accurately estimate
robust accuracy on an arbitrarily large test set by estimating the input
margins only on a small subset. Finally, we address cases where the model is
not sufficiently margin-consistent by learning a pseudo-margin from the feature
representation. Our findings highlight the potential of leveraging deep
representations to efficiently assess adversarial vulnerability in deployment
scenarios.

摘要：儘管對對抗訓練策略進行了廣泛的研究以提高穩健性，但即使是最穩健的深度學習模型的決策仍可能對難以察覺的擾動相當敏感，在將其部署到高風險的真實世界應用時會產生嚴重的風險。雖然檢測此類案例可能至關重要，但使用對抗攻擊在每個實例級別評估模型的漏洞在計算上過於密集，不適合於實時部署場景。輸入空間裕度是檢測非穩健樣本的確切分數，對於深度神經網路來說是難以處理的。本文引入了裕度一致性的概念——一種將輸入空間裕度和穩健模型中的 logit 裕度聯繫起來的屬性——用於有效檢測易受攻擊的樣本。首先，我們確定裕度一致性是使用模型的 logit 裕度作為識別非穩健樣本的分數的必要且充分條件。接下來，通過對 CIFAR10 和 CIFAR100 資料集上各種穩健訓練模型的全面實證分析，我們表明它們表明強烈的裕度一致性，其輸入空間裕度和 logit 裕度之間具有很強的相關性。然後，我們表明我們可以有效地使用 logit 裕度來自信地檢測具有此類模型的脆弱決策，並通過僅在一個小子集中估計輸入裕度來準確估計任意大的測試集上的穩健準確度。最後，我們通過從特徵表示中學習偽裕度來解決模型裕度一致性不足的情況。我們的研究結果突出了利用深度表示來有效評估部署場景中對抗漏洞的潛力。

##### **Preference Elicitation for Offline Reinforcement Learning**
2406.18450v1 by Alizée Pace, Bernhard Schölkopf, Gunnar Rätsch, Giorgia Ramponi

Applying reinforcement learning (RL) to real-world problems is often made
challenging by the inability to interact with the environment and the
difficulty of designing reward functions. Offline RL addresses the first
challenge by considering access to an offline dataset of environment
interactions labeled by the reward function. In contrast, Preference-based RL
does not assume access to the reward function and learns it from preferences,
but typically requires an online interaction with the environment. We bridge
the gap between these frameworks by exploring efficient methods for acquiring
preference feedback in a fully offline setup. We propose Sim-OPRL, an offline
preference-based reinforcement learning algorithm, which leverages a learned
environment model to elicit preference feedback on simulated rollouts. Drawing
on insights from both the offline RL and the preference-based RL literature,
our algorithm employs a pessimistic approach for out-of-distribution data, and
an optimistic approach for acquiring informative preferences about the optimal
policy. We provide theoretical guarantees regarding the sample complexity of
our approach, dependent on how well the offline data covers the optimal policy.
Finally, we demonstrate the empirical performance of Sim-OPRL in different
environments.

摘要：將強化學習 (RL) 運用於實際問題時，經常會遇到無法與環境互動，以及設計獎勵函數的困難，而這會造成挑戰。離線 RL 透過考慮存取標記有獎勵函數的環境互動離線資料集來解決第一個挑戰。相對地，基於偏好的 RL 沒有假設可以存取獎勵函數，而是從偏好中學習獎勵函數，但通常需要與環境線上互動。我們透過探索在完全離線設定中取得偏好回饋的有效方法來彌補這些框架之間的差距。我們提出 Sim-OPRL，一種離線基於偏好的強化學習演算法，它利用已學習的環境模型在模擬的滾動中引發偏好回饋。我們的演算法根據離線 RL 和基於偏好的 RL 文獻中的見解，採用悲觀的方式處理分佈外資料，並採用樂觀的方式取得關於最佳政策的資訊性偏好。我們提供關於我們方法的範例複雜性的理論保證，這取決於離線資料涵蓋最佳政策的程度。最後，我們在不同的環境中展示 Sim-OPRL 的經驗效能。

##### **Cascading Large Language Models for Salient Event Graph Generation**
2406.18449v1 by Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He

Generating event graphs from long documents is challenging due to the
inherent complexity of multiple tasks involved such as detecting events,
identifying their relationships, and reconciling unstructured input with
structured graphs. Recent studies typically consider all events with equal
importance, failing to distinguish salient events crucial for understanding
narratives. This paper presents CALLMSAE, a CAscading Large Language Model
framework for SAlient Event graph generation, which leverages the capabilities
of LLMs and eliminates the need for costly human annotations. We first identify
salient events by prompting LLMs to generate summaries, from which salient
events are identified. Next, we develop an iterative code refinement prompting
strategy to generate event relation graphs, removing hallucinated relations and
recovering missing edges. Fine-tuning contextualised graph generation models on
the LLM-generated graphs outperforms the models trained on CAEVO-generated
data. Experimental results on a human-annotated test set show that the proposed
method generates salient and more accurate graphs, outperforming competitive
baselines.

摘要：由於涉及多項任務的內在複雜性，例如偵測事件、識別其關係，以及調和非結構化輸入與結構化圖表，因此從長篇文件產生事件圖表是一項挑戰。最近的研究通常將所有事件視為同等重要，未能區分對理解敘事至關重要的顯著事件。本文提出了 CALLMSAE，一個用於生成顯著事件圖表的層疊式大型語言模型框架，它利用了 LLM 的功能，並消除了對昂貴的人工標註的需求。我們首先透過提示 LLM 產生摘要來識別顯著事件，從中識別出顯著事件。接下來，我們開發了一種反覆的程式碼精煉提示策略來產生事件關係圖表，移除幻覺關係並恢復遺失的邊緣。在 LLM 生成的圖表上微調情境化圖表生成模型，其表現優於在 CAEVO 生成的資料上訓練的模型。在人工標註的測試集上的實驗結果顯示，所提出的方法產生了顯著且更準確的圖表，優於競爭性的基準。

##### **Graph Neural Networks for Emulation of Finite-Element Ice Dynamics in Greenland and Antarctic Ice Sheets**
2406.18423v1 by Younghyun Koo, Maryam Rahnemoonfar

Although numerical models provide accurate solutions for ice sheet dynamics
based on physics laws, they accompany intensified computational demands to
solve partial differential equations. In recent years, convolutional neural
networks (CNNs) have been widely used as statistical emulators for those
numerical models. However, since CNNs operate on regular grids, they cannot
represent the refined meshes and computational efficiency of finite-element
numerical models. Therefore, instead of CNNs, this study adopts an equivariant
graph convolutional network (EGCN) as an emulator for the ice sheet dynamics
modeling. EGCN reproduces ice thickness and velocity changes in the Helheim
Glacier, Greenland, and Pine Island Glacier, Antarctica, with 260 times and 44
times faster computation time, respectively. Compared to the traditional CNN
and graph convolutional network, EGCN shows outstanding accuracy in thickness
prediction near fast ice streams by preserving the equivariance to the
translation and rotation of graphs.

摘要：雖然數值模型根據物理定律為冰蓋動力學提供準確的解，但它們伴隨著強化運算需求來求解偏微分方程。近年來，卷積神經網路 (CNN) 已廣泛用作這些數值模型的統計模擬器。然而，由於 CNN 在規則網格上運作，它們無法表示有限元素數值模型的精緻網格和運算效率。因此，本研究採用等變圖形卷積網路 (EGCN) 作為冰蓋動力學建模的模擬器，而不是 CNN。EGCN 再現格陵蘭海爾海姆冰川和南極洲松島冰川的冰層厚度和速度變化，運算時間分別快了 260 倍和 44 倍。與傳統 CNN 和圖形卷積網路相比，EGCN 在快速冰流附近厚度預測中展現出傑出的準確性，方法是保持圖形的平移和旋轉等變性。

##### **Mixture of Experts in a Mixture of RL settings**
2406.18420v1 by Timon Willi, Johan Obando-Ceron, Jakob Foerster, Karolina Dziugaite, Pablo Samuel Castro

Mixtures of Experts (MoEs) have gained prominence in (self-)supervised
learning due to their enhanced inference efficiency, adaptability to
distributed training, and modularity. Previous research has illustrated that
MoEs can significantly boost Deep Reinforcement Learning (DRL) performance by
expanding the network's parameter count while reducing dormant neurons, thereby
enhancing the model's learning capacity and ability to deal with
non-stationarity. In this work, we shed more light on MoEs' ability to deal
with non-stationarity and investigate MoEs in DRL settings with "amplified"
non-stationarity via multi-task training, providing further evidence that MoEs
improve learning capacity. In contrast to previous work, our multi-task results
allow us to better understand the underlying causes for the beneficial effect
of MoE in DRL training, the impact of the various MoE components, and insights
into how best to incorporate them in actor-critic-based DRL networks. Finally,
we also confirm results from previous work.

摘要：专家混合體 (MoE) 因其增強的推論效率、對分布式訓練的適應性，以及模組化而顯著提升了（自我）監督式學習。先前的研究表明，MoE 可以透過擴充網路的參數數量，同時減少休眠神經元，從而顯著提升深度強化學習 (DRL) 的效能，進而增強模型的學習能力和處理非平穩性的能力。在這項工作中，我們更深入地探討 MoE 處理非平穩性的能力，並透過多任務訓練調查具有「擴增」非平穩性的 DRL 設定中的 MoE，進一步提供證據證明 MoE 能提升學習能力。與先前的研究相比，我們的多任務結果讓我們能更深入地了解 MoE 在 DRL 訓練中產生有益效果的潛在原因、各種 MoE 組件的影響，以及如何將它們整合到基於動作-評論家的 DRL 網路中的見解。最後，我們也確認了先前研究的結果。

##### **IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons**
2406.18406v1 by Dan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xinwei Wu, Deyi Xiong

It is widely acknowledged that large language models (LLMs) encode a vast
reservoir of knowledge after being trained on mass data. Recent studies
disclose knowledge conflicts in LLM generation, wherein outdated or incorrect
parametric knowledge (i.e., encoded knowledge) contradicts new knowledge
provided in the context. To mitigate such knowledge conflicts, we propose a
novel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to
capitalize on neurons that are crucial in processing contextual cues.
Specifically, IRCAN first identifies neurons that significantly contribute to
context processing, utilizing a context-aware attribution score derived from
integrated gradients. Subsequently, the identified context-aware neurons are
strengthened via reweighting. In doing so, we steer LLMs to generate
context-sensitive outputs with respect to the new knowledge provided in the
context. Extensive experiments conducted across a variety of models and tasks
demonstrate that IRCAN not only achieves remarkable improvements in handling
knowledge conflicts but also offers a scalable, plug-andplay solution that can
be integrated seamlessly with existing models.

摘要：眾所周知，大型語言模型 (LLM) 在接受大量數據訓練後，會編碼大量的知識庫。最近的研究揭露了 LLM 生成中的知識衝突，其中過時的或不正確的參數知識（即編碼知識）與上下文中提供的新的知識相矛盾。為了緩解這種知識衝突，我們提出了一個新的框架，IRCAN（識別和重新加權上下文感知神經元），以利用在處理上下文線索中至關重要的神經元。具體來說，IRCAN 首先識別對上下文處理有顯著貢獻的神經元，利用從整合梯度中導出的上下文感知歸因分數。隨後，通過重新加權來增強已識別的上下文感知神經元。這樣一來，我們引導 LLM 針對上下文中提供的新知識生成上下文敏感的輸出。在各種模型和任務中進行的廣泛實驗表明，IRCAN 不僅在處理知識衝突方面取得了顯著的改進，而且還提供了一個可擴展的即插即用解決方案，可以與現有模型無縫集成。

##### **LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks**
2406.18403v1 by Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni

There is an increasing trend towards evaluating NLP models with LLM-generated
judgments instead of human judgments. In the absence of a comparison against
human data, this raises concerns about the validity of these evaluations; in
case they are conducted with proprietary models, this also raises concerns over
reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with
human annotations, and comprehensively evaluate 11 current LLMs, covering both
open-weight and proprietary models, for their ability to replicate the
annotations. Our evaluations show that each LLM exhibits a large variance
across datasets in its correlation to human judgments. We conclude that LLMs
are not yet ready to systematically replace human judges in NLP.

摘要：NLP 模型評估的趨勢正朝著使用 LLM 生成的判斷，而非人類判斷的方向前進。在沒有與人類資料進行比較的情況下，這引發了對這些評估的有效性之疑慮；在使用專有模型進行評估時，這也引發了對可複製性的疑慮。我們提供了 JUDGE-BENCH，這是一個包含 20 個帶有人類註解的 NLP 資料集的集合，並全面評估了 11 個現有的 LLM，涵蓋開放權重和專有模型，以了解它們複製註解的能力。我們的評估顯示，每個 LLM 在與人類判斷的相關性上，在不同資料集之間都展現出很大的差異。我們得出的結論是，LLM 尚未準備好系統性地取代 NLP 中的人類評審。

##### **Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers**
2406.18400v1 by Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam

Large Language Models (LLMs) have the capacity to store and recall facts.
Through experimentation with open-source models, we observe that this ability
to retrieve facts can be easily manipulated by changing contexts, even without
altering their factual meanings. These findings highlight that LLMs might
behave like an associative memory model where certain tokens in the contexts
serve as clues to retrieving facts. We mathematically explore this property by
studying how transformers, the building blocks of LLMs, can complete such
memory tasks. We study a simple latent concept association problem with a
one-layer transformer and we show theoretically and empirically that the
transformer gathers information using self-attention and uses the value matrix
for associative memory.

摘要：大型語言模型 (LLM) 具備儲存和提取事實的能力。
透過對開源模型的實驗，我們觀察到這種提取事實的能力可以透過改變語境輕鬆地進行操控，即使不改變它們的事實意義。這些發現強調 LLM 可能表現得像一個聯想記憶模型，其中語境中的某些代幣用作提取事實的線索。我們透過研究 LLM 的構建模組轉換器如何完成此類記憶任務，在數學上探討此特性。我們研究了一個使用單層轉換器的簡單潛在概念關聯問題，並在理論和經驗上證明轉換器使用自注意力收集資訊，並使用值矩陣進行聯想記憶。

##### **AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha Factors**
2406.18394v1 by Hao Shi, Cuicui Luo, Weili Song, Xinting Zhang, Xiang Ao

The variability and low signal-to-noise ratio in financial data, combined
with the necessity for interpretability, make the alpha factor mining workflow
a crucial component of quantitative investment. Transitioning from early manual
extraction to genetic programming, the most advanced approach in this domain
currently employs reinforcement learning to mine a set of combination factors
with fixed weights. However, the performance of resultant alpha factors
exhibits inconsistency, and the inflexibility of fixed factor weights proves
insufficient in adapting to the dynamic nature of financial markets. To address
this issue, this paper proposes a two-stage formulaic alpha generating
framework AlphaForge, for alpha factor mining and factor combination. This
framework employs a generative-predictive neural network to generate factors,
leveraging the robust spatial exploration capabilities inherent in deep
learning while concurrently preserving diversity. The combination model within
the framework incorporates the temporal performance of factors for selection
and dynamically adjusts the weights assigned to each component alpha factor.
Experiments conducted on real-world datasets demonstrate that our proposed
model outperforms contemporary benchmarks in formulaic alpha factor mining.
Furthermore, our model exhibits a notable enhancement in portfolio returns
within the realm of quantitative investment.

摘要：金融資料中的變異性和低信號雜訊比，加上可解釋性的必要性，使 Alpha 因子挖掘工作流程成為量化投資的關鍵組成部分。從早期的人工提取轉變為遺傳程式設計，目前此領域最先進的方法採用強化學習來挖掘一組具有固定權重的組合因子。然而，產生的 Alpha 因子的表現不一致，而固定因子權重的缺乏彈性證明不足以適應金融市場的動態特性。為了解決這個問題，本文提出了一個兩階段公式化 Alpha 生成架構 AlphaForge，用於 Alpha 因子挖掘和因子組合。此架構採用生成預測神經網路來產生因子，同時利用深度學習中固有的強大空間探索能力，並同時保持多樣性。架構中的組合模型結合了因子的時間效能以進行選擇，並動態調整分配給每個組成 Alpha 因子的權重。在真實世界資料集上進行的實驗證明，我們提出的模型在公式化 Alpha 因子挖掘中優於當代基準。此外，我們的模型在量化投資領域的投資組合報酬率方面表現出顯著的提升。

##### **MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for Iterative Binary Malware Summarization**
2406.18379v1 by Haolang Lu, Hongrui Peng, Guoshun Nan, Jiaoyang Cui, Cheng Wang, Weifei Jin

Binary malware summarization aims to automatically generate human-readable
descriptions of malware behaviors from executable files, facilitating tasks
like malware cracking and detection. Previous methods based on Large Language
Models (LLMs) have shown great promise. However, they still face significant
issues, including poor usability, inaccurate explanations, and incomplete
summaries, primarily due to the obscure pseudocode structure and the lack of
malware training summaries. Further, calling relationships between functions,
which involve the rich interactions within a binary malware, remain largely
underexplored. To this end, we propose MALSIGHT, a novel code summarization
framework that can iteratively generate descriptions of binary malware by
exploring malicious source code and benign pseudocode. Specifically, we
construct the first malware summaries, MalS and MalP, using an LLM and manually
refine this dataset with human effort. At the training stage, we tune our
proposed MalT5, a novel LLM-based code model, on the MalS dataset and a benign
pseudocode dataset. Then, at the test stage, we iteratively feed the pseudocode
functions into MalT5 to obtain the summary. Such a procedure facilitates the
understanding of pseudocode structure and captures the intricate interactions
between functions, thereby benefiting the usability, accuracy, and completeness
of summaries. Additionally, we propose a novel evaluation benchmark,
BLEURT-sum, to measure the quality of summaries. Experiments on three datasets
show the effectiveness of the proposed MALSIGHT. Notably, our proposed MalT5,
with only 0.77B parameters, delivers comparable performance to much larger
ChatGPT3.5.

摘要：二進制惡意軟體摘要旨在自動產生可讀取的人類惡意軟體行為描述從可執行檔中，促進惡意軟體破解和檢測等任務。以前基於大型語言模型 (LLM) 的方法已展現極大的希望。然而，他們仍然面臨重大問題，包括可用性差、解釋不準確和摘要不完整，這主要是由於模糊的偽代碼結構和缺乏惡意軟體訓練摘要。此外，函數之間的呼叫關係，涉及二進制惡意軟體中的豐富互動，在很大程度上仍未得到充分探索。為此，我們提出了 MALSIGHT，一個新穎的代碼摘要框架，它可以透過探索惡意原始碼和良性偽代碼，反覆產生二進制惡意軟體的描述。具體來說，我們使用 LLM 建構第一個惡意軟體摘要 MalS 和 MalP，並透過人工努力手動優化此資料集。在訓練階段，我們在 MalS 資料集和良性偽代碼資料集上調整我們提出的 MalT5，一個新穎的基於 LLM 的代碼模型。然後，在測試階段，我們反覆將偽代碼函數饋入 MalT5 以取得摘要。這樣的程序促進了對偽代碼結構的理解，並捕捉函數之間的複雜交互，從而有利於摘要的可用性、準確性和完整性。此外，我們提出一個新穎的評估基準，BLEURT-sum，以衡量摘要的品質。在三個資料集上的實驗顯示了所提出的 MALSIGHT 的有效性。值得注意的是，我們提出的 MalT5，只有 0.77B 參數，提供了與大得多的 ChatGPT3.5 相當的效能。

##### **Dynamic Data Pruning for Automatic Speech Recognition**
2406.18373v1 by Qiao Xiao, Pingchuan Ma, Adriana Fernandez-Lopez, Boqian Wu, Lu Yin, Stavros Petridis, Mykola Pechenizkiy, Maja Pantic, Decebal Constantin Mocanu, Shiwei Liu

The recent success of Automatic Speech Recognition (ASR) is largely
attributed to the ever-growing amount of training data. However, this trend has
made model training prohibitively costly and imposed computational demands.
While data pruning has been proposed to mitigate this issue by identifying a
small subset of relevant data, its application in ASR has been barely explored,
and existing works often entail significant overhead to achieve meaningful
results. To fill this gap, this paper presents the first investigation of
dynamic data pruning for ASR, finding that we can reach the full-data
performance by dynamically selecting 70% of data. Furthermore, we introduce
Dynamic Data Pruning for ASR (DDP-ASR), which offers several fine-grained
pruning granularities specifically tailored for speech-related datasets, going
beyond the conventional pruning of entire time sequences. Our intensive
experiments show that DDP-ASR can save up to 1.6x training time with negligible
performance loss.

摘要：自動語音辨識 (ASR) 近期的成功，在很大程度上歸因於訓練資料數量不斷增加。然而，這種趨勢使得模型訓練成本高得令人望而卻步，並造成運算需求。雖然資料剪枝已被提出用於緩解這個問題，方法是找出一個相關資料的小子集，但它在 ASR 中的應用鮮少被探討，而且現有的作品通常需要大量的額外開銷才能達到有意義的結果。為了填補這個缺口，本文首次探討了 ASR 的動態資料剪枝，發現我們可以透過動態選取 70% 的資料來達到全資料的效能。此外，我們引入了 ASR 的動態資料剪枝 (DDP-ASR)，它提供了專門針對語音相關資料集調整的多種細緻剪枝粒度，超越了傳統的整個時間序列剪枝。我們的密集實驗顯示，DDP-ASR 可以節省高達 1.6 倍的訓練時間，而效能損失微乎其微。

##### **Themis: Towards Flexible and Interpretable NLG Evaluation**
2406.18365v1 by Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, Xiaojun Wan

The evaluation of natural language generation (NLG) tasks is a significant
and longstanding research issue. With the recent emergence of powerful large
language models (LLMs), some studies have turned to LLM-based automatic
evaluation methods, which demonstrate great potential to become a new
evaluation paradigm following traditional string-based and model-based metrics.
However, despite the improved performance of existing methods, they still
possess some deficiencies, such as dependency on references and limited
evaluation flexibility. Therefore, in this paper, we meticulously construct a
large-scale NLG evaluation corpus NLG-Eval with human and GPT-4 annotations to
alleviate the lack of relevant data in this field. Furthermore, we propose
Themis, an LLM dedicated to NLG evaluation, which has been trained with our
designed multi-perspective consistency and rating-oriented preference alignment
methods. Themis can conduct flexible and interpretable evaluations without
references, and it exhibits superior evaluation performance on various NLG
tasks, simultaneously generalizing well to unseen tasks and surpassing other
evaluation models, including GPT-4.

摘要：自然語言生成 (NLG) 任務的評估是一項重要且長期的研究議題。隨著強大的大型語言模型 (LLM) 的近期出現，一些研究已轉向基於 LLM 的自動評估方法，這顯示出成為傳統基於字串和基於模型的指標之後的新評估範例的巨大潛力。然而，儘管現有方法的效能有所提升，它們仍然存在一些缺陷，例如依賴參考和評估彈性有限。因此，在本文中，我們精心建構了一個大型 NLG 評估語料庫 NLG-Eval，其中包含人類和 GPT-4 標註，以減輕該領域中相關資料的缺乏。此外，我們提出 Themis，一種專門用於 NLG 評估的 LLM，它已使用我們設計的多視角一致性和面向評分的偏好對齊方法進行訓練。Themis 能夠在沒有參考的情況下進行靈活且可解釋的評估，並且在各種 NLG 任務上展現出優異的評估效能，同時能很好地概括到未見過的任務，並超越其他評估模型，包括 GPT-4。

##### **Research on Information Extraction of LCSTS Dataset Based on an Improved BERTSum-LSTM Model**
2406.18364v1 by Yiming Chen, Haobin Chen, Simin Liu, Yunyun Liu, Fanhao Zhou, Bing Wei

With the continuous advancement of artificial intelligence, natural language
processing technology has become widely utilized in various fields. At the same
time, there are many challenges in creating Chinese news summaries. First of
all, the semantics of Chinese news is complex, and the amount of information is
enormous. Extracting critical information from Chinese news presents a
significant challenge. Second, the news summary should be concise and clear,
focusing on the main content and avoiding redundancy. In addition, the
particularity of the Chinese language, such as polysemy, word segmentation,
etc., makes it challenging to generate Chinese news summaries. Based on the
above, this paper studies the information extraction method of the LCSTS
dataset based on an improved BERTSum-LSTM model. We improve the BERTSum-LSTM
model to make it perform better in generating Chinese news summaries. The
experimental results show that the proposed method has a good effect on
creating news summaries, which is of great importance to the construction of
news summaries.

摘要：隨著人工智慧的持續進步，自然語言處理技術已廣泛應用於各個領域。同時，中文新聞摘要的生成也面臨著諸多挑戰。首先，中文新聞語義複雜，資訊量龐大，從中文新聞中抽取關鍵資訊是一項艱鉅的挑戰。其次，新聞摘要應簡潔明瞭，著重於主要內容，避免冗餘。此外，中文語言的特殊性，如多義性、詞語切分等，也為中文新聞摘要的生成帶來挑戰。基於以上，本文研究了基於改進的 BERTSum-LSTM 模型的 LCSTS 資料集的資訊抽取方法。我們改進了 BERTSum-LSTM 模型，使其在生成中文新聞摘要方面有更好的表現。實驗結果表明，所提出的方法對新聞摘要的生成有良好的效果，對新聞摘要的構建具有重要的意義。

##### **Stable Diffusion Segmentation for Biomedical Images with Single-step Reverse Process**
2406.18361v1 by Tianyu Lin, Zhiguang Chen, Zhonghao Yan, Fudan Zheng, Weijiang Yu

Diffusion models have demonstrated their effectiveness across various
generative tasks. However, when applied to medical image segmentation, these
models encounter several challenges, including significant resource and time
requirements. They also necessitate a multi-step reverse process and multiple
samples to produce reliable predictions. To address these challenges, we
introduce the first latent diffusion segmentation model, named SDSeg, built
upon stable diffusion (SD). SDSeg incorporates a straightforward latent
estimation strategy to facilitate a single-step reverse process and utilizes
latent fusion concatenation to remove the necessity for multiple samples.
Extensive experiments indicate that SDSeg surpasses existing state-of-the-art
methods on five benchmark datasets featuring diverse imaging modalities.
Remarkably, SDSeg is capable of generating stable predictions with a solitary
reverse step and sample, epitomizing the model's stability as implied by its
name. The code is available at
https://github.com/lin-tianyu/Stable-Diffusion-Seg

摘要：擴散模型已證明其在各種生成任務中的有效性。然而，當應用於醫學影像分割時，這些模型會遇到一些挑戰，包括顯著的資源和時間需求。它們還需要多步驟的反向處理和多個樣本來產生可靠的預測。為了應對這些挑戰，我們引入了第一個潛在擴散分割模型，名為 SDSeg，它建立在穩定擴散 (SD) 之上。SDSeg 結合了一個直接的潛在估計策略，以促進單步反向處理，並利用潛在融合串接來消除對多個樣本的必要性。廣泛的實驗表明，SDSeg 在具有不同影像模式的五個基準資料集上超越了現有的最先進方法。值得注意的是，SDSeg 能夠通過單一的反向步驟和樣本來產生穩定的預測，體現了模型的穩定性，正如其名稱所暗示的那樣。程式碼可在 https://github.com/lin-tianyu/Stable-Diffusion-Seg 取得

##### **Kolmogorov-Arnold Graph Neural Networks**
2406.18354v1 by Gianluca De Carlo, Andrea Mastropietro, Aris Anagnostopoulos

Graph neural networks (GNNs) excel in learning from network-like data but
often lack interpretability, making their application challenging in domains
requiring transparent decision-making. We propose the Graph Kolmogorov-Arnold
Network (GKAN), a novel GNN model leveraging spline-based activation functions
on edges to enhance both accuracy and interpretability. Our experiments on five
benchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN
models in node classification, link prediction, and graph classification tasks.
In addition to the improved accuracy, GKAN's design inherently provides clear
insights into the model's decision-making process, eliminating the need for
post-hoc explainability techniques. This paper discusses the methodology,
performance, and interpretability of GKAN, highlighting its potential for
applications in domains where interpretability is crucial.

摘要：圖形神經網路 (GNN) 擅長從網路狀資料學習，但通常缺乏可解釋性，這使得它們在需要透明決策的領域中應用具有挑戰性。我們提出圖形 Kolmogorov-Arnold 網路 (GKAN)，這是一種新穎的 GNN 模型，在邊緣上利用基於樣條的激活函數來增強準確性和可解釋性。我們在五個基準資料集上的實驗表明，GKAN 在節點分類、連結預測和圖形分類任務中優於最先進的 GNN 模型。除了提高準確性之外，GKAN 的設計本身就對模型的決策過程提供了清晰的見解，消除了對事後可解釋性技術的需求。本文討論了 GKAN 的方法、性能和可解釋性，強調了其在可解釋性至關重要的領域中的應用潛力。

##### **AI Alignment through Reinforcement Learning from Human Feedback? Contradictions and Limitations**
2406.18346v1 by Adam Dahlgren Lindström, Leila Methnani, Lea Krause, Petter Ericson, Íñigo Martínez de Rituerto de Troya, Dimitri Coelho Mollo, Roel Dobbe

This paper critically evaluates the attempts to align Artificial Intelligence
(AI) systems, especially Large Language Models (LLMs), with human values and
intentions through Reinforcement Learning from Feedback (RLxF) methods,
involving either human feedback (RLHF) or AI feedback (RLAIF). Specifically, we
show the shortcomings of the broadly pursued alignment goals of honesty,
harmlessness, and helpfulness. Through a multidisciplinary sociotechnical
critique, we examine both the theoretical underpinnings and practical
implementations of RLxF techniques, revealing significant limitations in their
approach to capturing the complexities of human ethics and contributing to AI
safety. We highlight tensions and contradictions inherent in the goals of RLxF.
In addition, we discuss ethically-relevant issues that tend to be neglected in
discussions about alignment and RLxF, among which the trade-offs between
user-friendliness and deception, flexibility and interpretability, and system
safety. We conclude by urging researchers and practitioners alike to critically
assess the sociotechnical ramifications of RLxF, advocating for a more nuanced
and reflective approach to its application in AI development.

摘要：這篇論文批判性地評估了將人工智慧 (AI) 系統，特別是大型語言模型 (LLM)，與人類價值觀和意圖相符的嘗試，這些嘗試透過回饋強化學習 (RLxF) 方法進行，包括人類回饋 (RLHF) 或 AI 回饋 (RLAIF)。具體來說，我們展示了誠實、無害和有幫助等廣泛追求的對齊目標的缺點。透過多學科社會技術批判，我們檢視了 RLxF 技術的理論基礎和實際實作，揭示了它們在捕捉人類倫理複雜性和促成 AI 安全方面的重大限制。我們強調了 RLxF 目標中固有的緊張和矛盾。此外，我們討論了在關於對齊和 RLxF 的討論中往往被忽略的與倫理相關的問題，其中包括使用者友善性和欺騙性、靈活性與可解釋性，以及系統安全性之間的權衡。我們最後敦促研究人員和從業人員批判性地評估 RLxF 的社會技術影響，並倡導在 AI 開發中採用更細緻且具反省性的應用方法。

##### **Grammar Assistance Using Syntactic Structures (GAUSS)**
2406.18340v1 by Olga Zamaraeva, Lorena S. Allegue, Carlos Gómez-Rodríguez, Anastasiia Ogneva, Margarita Alonso-Ramos

Automatic grammar coaching serves an important purpose of advising on
standard grammar varieties while not imposing social pressures or reinforcing
established social roles. Such systems already exist but most of them are for
English and few of them offer meaningful feedback. Furthermore, they typically
rely completely on neural methods and require huge computational resources
which most of the world cannot afford. We propose a grammar coaching system for
Spanish that relies on (i) a rich linguistic formalism capable of giving
informative feedback; and (ii) a faster parsing algorithm which makes using
this formalism practical in a real-world application. The approach is feasible
for any language for which there is a computerized grammar and is less reliant
on expensive and environmentally costly neural methods. We seek to contribute
to Greener AI and to address global education challenges by raising the
standards of inclusivity and engagement in grammar coaching.

摘要：自動文法指導提供了一個重要的目的，就是建議標準文法種類，同時不施加社會壓力或加強既定的社會角色。此類系統已經存在，但大多數都是針對英語，而且很少提供有意義的回饋。此外，它們通常完全依賴神經方法，並且需要大量的計算資源，而世界上大多數人負擔不起。我們提出一個西班牙語文法指導系統，它依賴於 (i) 能夠提供有益回饋的豐富語言形式主義；以及 (ii) 一個更快的解析演算法，這使得在現實世界應用中使用此形式主義具有實用性。此方法對任何有電腦化文法且較不依賴昂貴且對環境有負擔的神經方法的語言都可行。我們尋求為更環保的人工智慧做出貢獻，並透過提升文法指導中的包容性和參與度標準來解決全球教育挑戰。

##### **Continuous Sign Language Recognition Using Intra-inter Gloss Attention**
2406.18333v1 by Hossein Ranjbar, Alireza Taheri

Many continuous sign language recognition (CSLR) studies adopt
transformer-based architectures for sequence modeling due to their powerful
capacity for capturing global contexts. Nevertheless, vanilla self-attention,
which serves as the core module of the transformer, calculates a weighted
average over all time steps; therefore, the local temporal semantics of sign
videos may not be fully exploited. In this study, we introduce a novel module
in sign language recognition studies, called intra-inter gloss attention
module, to leverage the relationships among frames within glosses and the
semantic and grammatical dependencies between glosses in the video. In the
intra-gloss attention module, the video is divided into equally sized chunks
and a self-attention mechanism is applied within each chunk. This localized
self-attention significantly reduces complexity and eliminates noise introduced
by considering non-relative frames. In the inter-gloss attention module, we
first aggregate the chunk-level features within each gloss chunk by average
pooling along the temporal dimension. Subsequently, multi-head self-attention
is applied to all chunk-level features. Given the non-significance of the
signer-environment interaction, we utilize segmentation to remove the
background of the videos. This enables the proposed model to direct its focus
toward the signer. Experimental results on the PHOENIX-2014 benchmark dataset
demonstrate that our method can effectively extract sign language features in
an end-to-end manner without any prior knowledge, improve the accuracy of CSLR,
and achieve the word error rate (WER) of 20.4 on the test set which is a
competitive result compare to the state-of-the-art which uses additional
supervisions.

摘要：許多連續手語識別 (CSLR) 研究採用基於轉換器的架構進行序列建模，因為它們具有捕捉全局脈絡的強大能力。儘管如此，作為轉換器核心模組的香草自注意力會計算所有時間步長的加權平均值；因此，手語影片的局部時間語義可能無法被充分利用。在本研究中，我們在手語識別研究中引入了一個新模組，稱為句內句間光照注意力模組，以利用手勢內的幀之間的關係以及影片中手勢之間的語義和語法依賴性。在句內注意力模組中，影片被分成大小相等的塊，並在每個塊內應用自注意力機制。這種局部自注意力顯著降低了複雜性，並消除了考慮非相關幀所產生的雜訊。在句間注意力模組中，我們首先透過沿時間維度進行平均池化，彙總每個手勢塊內的塊級特徵。隨後，將多頭自注意力應用於所有塊級特徵。鑑於手勢者環境互動的不顯著性，我們利用分割來移除影片的背景。這使提出的模型能夠將其焦點導向手勢者。PHOENIX-2014 基準資料集上的實驗結果表明，我們的方法能夠在端到端的方式中有效地提取手語特徵，而無需任何先驗知識，提高 CSLR 的準確性，並在測試集上實現 20.4 的字元錯誤率 (WER)，這是一個與使用額外監督的最新技術相比具有競爭力的結果。

##### **PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models**
2406.18326v1 by Huixuan Zhang, Yun Lin, Xiaojun Wan

Large language models (LLMs) are known to be trained on vast amounts of data,
which may unintentionally or intentionally include data from commonly used
benchmarks. This inclusion can lead to cheatingly high scores on model
leaderboards, yet result in disappointing performance in real-world
applications. To address this benchmark contamination problem, we first propose
a set of requirements that practical contamination detection methods should
follow. Following these proposed requirements, we introduce PaCoST, a Paired
Confidence Significance Testing to effectively detect benchmark contamination
in LLMs. Our method constructs a counterpart for each piece of data with the
same distribution, and performs statistical analysis of the corresponding
confidence to test whether the model is significantly more confident under the
original benchmark. We validate the effectiveness of PaCoST and apply it on
popular open-source models and benchmarks. We find that almost all models and
benchmarks we tested are suspected contaminated more or less. We finally call
for new LLM evaluation methods.

摘要：大型語言模型 (LLM) 被認為是經過大量數據訓練，
其中可能無意或有意地包含來自常用
基準的數據。這種包含可能會導致模型
排行榜上的作弊高分，但在現實世界中卻表現令人失望
應用。為了解決這個基準污染問題，我們首先提出
一系列實用的污染檢測方法應遵循的要求。遵循這些建議的要求，我們介紹 PaCoST，一種配對
置信度顯著性測試，可有效檢測 LLM 中的基準污染
在 LLM 中。我們的模型為每個數據構建一個具有相同分佈的對應物，並執行統計分析
對應的置信度以測試模型在
原始基準。我們驗證了 PaCoST 的有效性並將其應用於
流行的開源模型和基準。我們發現我們測試的幾乎所有模型和
基準都被懷疑或多或少受到污染。我們最終呼籲新的 LLM 評估方法。

##### **MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large Language Models Using Odyssey Math Data**
2406.18321v1 by Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, Kai Zou

Large language models (LLMs) have significantly advanced natural language
understanding and demonstrated strong problem-solving abilities. Despite these
successes, most LLMs still struggle with solving mathematical problems due to
the intricate reasoning required. This paper investigates the mathematical
problem-solving capabilities of LLMs using the newly developed "MathOdyssey"
dataset. The dataset includes diverse mathematical problems at high school and
university levels, created by experts from notable institutions to rigorously
test LLMs in advanced problem-solving scenarios and cover a wider range of
subject areas. By providing the MathOdyssey dataset as a resource to the AI
community, we aim to contribute to the understanding and improvement of AI
capabilities in complex mathematical problem-solving. We conduct benchmarking
on open-source models, such as Llama-3 and DBRX-Instruct, and closed-source
models from the GPT series and Gemini models. Our results indicate that while
LLMs perform well on routine and moderately difficult tasks, they face
significant challenges with Olympiad-level problems and complex
university-level questions. Our analysis shows a narrowing performance gap
between open-source and closed-source models, yet substantial challenges
remain, particularly with the most demanding problems. This study highlights
the ongoing need for research to enhance the mathematical reasoning of LLMs.
The dataset, results, and code are publicly available.

摘要：大型語言模型（LLM）顯著提升了自然語言理解，並展示了強大的問題解決能力。儘管有這些成功，但由於所需的複雜推理，大多數 LLM 仍難以解決數學問題。本文使用新開發的「MathOdyssey」資料集探討 LLM 的數學問題解決能力。該資料集包含高中和大學程度的多元數學問題，由著名機構的專家建立，以嚴格測試 LLM 在進階問題解決情境中的表現，並涵蓋更廣泛的科目領域。透過提供 MathOdyssey 資料集作為 AI 社群的資源，我們旨在促進理解和提升 AI 在複雜數學問題解決中的能力。我們對開放原始碼模型（例如 Llama-3 和 DBRX-Instruct）以及 GPT 系列和 Gemini 模型中的閉源模型進行基準測試。我們的結果表明，儘管 LLM 在例行和中等難度的任務中表現良好，但它們在奧林匹克級別的問題和複雜的大學程度問題中面臨重大挑戰。我們的分析顯示，開放原始碼和閉源模型之間的效能差距正在縮小，但仍有實質性的挑戰，特別是在最困難的問題上。這項研究強調了持續研究以增強 LLM 數學推理能力的必要性。資料集、結果和程式碼公開提供。

##### **Advancing Airport Tower Command Recognition: Integrating Squeeze-and-Excitation and Broadcasted Residual Learning**
2406.18313v1 by Yuanxi Lin, Tonglin Zhou, Yang Xiao

Accurate recognition of aviation commands is vital for flight safety and
efficiency, as pilots must follow air traffic control instructions precisely.
This paper addresses challenges in speech command recognition, such as noisy
environments and limited computational resources, by advancing keyword spotting
technology. We create a dataset of standardized airport tower commands,
including routine and emergency instructions. We enhance broadcasted residual
learning with squeeze-and-excitation and time-frame frequency-wise
squeeze-and-excitation techniques, resulting in our BC-SENet model. This model
focuses on crucial information with fewer parameters. Our tests on five keyword
spotting models, including BC-SENet, demonstrate superior accuracy and
efficiency. These findings highlight the effectiveness of our model
advancements in improving speech command recognition for aviation safety and
efficiency in noisy, high-stakes environments. Additionally, BC-SENet shows
comparable performance on the common Google Speech Command dataset.

摘要：準確辨識航空指令對於飛行安全和效率至關重要，因為飛行員必須精確遵循空中交通管制指示。本文透過推進關鍵字偵測技術，探討語音指令辨識的挑戰，例如有噪音的環境和受限的運算資源。我們建立一個標準化機場塔台指令的資料集，包括例行和緊急指示。我們使用擠壓和激勵以及時頻擠壓和激勵技術來增強廣播殘差學習，進而產生我們的 BC-SENet 模型。此模型以較少的參數專注於關鍵資訊。我們對包括 BC-SENet 在內的五個關鍵字偵測模型進行測試，證明了其優異的準確度和效率。這些發現突顯了我們模型進展在改善語音指令辨識的成效，以提升在有噪音、高風險環境中的航空安全和效率。此外，BC-SENet 在常見的 Google 語音指令資料集上也展現了相當的效能。

##### **AI-native Memory: A Pathway from LLMs Towards AGI**
2406.18312v1 by Jingbo Shang, Zai Zheng, Xiang Ying, Felix Tao, Mindverse Team

Large language models (LLMs) have demonstrated the world with the sparks of
artificial general intelligence (AGI). One opinion, especially from some
startups working on LLMs, argues that an LLM with nearly unlimited context
length can realize AGI. However, they might be too optimistic about the
long-context capability of (existing) LLMs -- (1) Recent literature has shown
that their effective context length is significantly smaller than their claimed
context length; and (2) Our reasoning-in-a-haystack experiments further
demonstrate that simultaneously finding the relevant information from a long
context and conducting (simple) reasoning is nearly impossible. In this paper,
we envision a pathway from LLMs to AGI through the integration of
\emph{memory}. We believe that AGI should be a system where LLMs serve as core
processors. In addition to raw data, the memory in this system would store a
large number of important conclusions derived from reasoning processes.
Compared with retrieval-augmented generation (RAG) that merely processing raw
data, this approach not only connects semantically related information closer,
but also simplifies complex inferences at the time of querying. As an
intermediate stage, the memory will likely be in the form of natural language
descriptions, which can be directly consumed by users too. Ultimately, every
agent/person should have its own large personal model, a deep neural network
model (thus \emph{AI-native}) that parameterizes and compresses all types of
memory, even the ones cannot be described by natural languages. Finally, we
discuss the significant potential of AI-native memory as the transformative
infrastructure for (proactive) engagement, personalization, distribution, and
social in the AGI era, as well as the incurred privacy and security challenges
with preliminary solutions.

摘要：大型語言模型 (LLM) 向世界展示了人工通用智慧 (AGI) 的火花。一種意見，特別是來自一些致力於 LLM 的新創公司，認為具有幾乎無限的背景長度的 LLM 可以實現 AGI。然而，他們可能對（現有）LLM 的長背景能力過於樂觀——（1）最近的文獻表明，它們的有效背景長度顯著小於其聲稱的背景長度；（2）我們的乾草堆推理實驗進一步證明，同時從長背景中找到相關資訊並進行（簡單）推理幾乎是不可能的。在本文中，我們設想通過整合「記憶」從 LLM 到 AGI 的途徑。我們相信 AGI 應該是一個系統，其中 LLM 作為核心處理器。除了原始資料外，此系統中的記憶體將儲存大量從推理過程中得出的重要結論。與僅處理原始資料的檢索增強生成 (RAG) 相比，這種方法不僅將語義相關資訊更緊密地連結，而且在查詢時簡化了複雜的推論。作為一個中間階段，記憶體很可能以自然語言描述的形式出現，使用者也可以直接使用。最終，每個代理人／人都應該有自己的大型個人模型，一個深度神經網路模型（因此是「AI 原生的」），它參數化並壓縮所有類型的記憶體，即使是無法用自然語言描述的記憶體。最後，我們討論了 AI 原生記憶體作為 AGI 時代（主動）參與、個人化、分發和社交的轉型基礎設施的巨大潛力，以及初步解決方案帶來的隱私和安全挑戰。

##### **S3: A Simple Strong Sample-effective Multimodal Dialog System**
2406.18305v1 by Elisei Rykov, Egor Malkershin, Alexander Panchenko

In this work, we present a conceptually simple yet powerful baseline for the
multimodal dialog task, an S3 model, that achieves near state-of-the-art
results on two compelling leaderboards: MMMU and AI Journey Contest 2023. The
system is based on a pre-trained large language model, pre-trained modality
encoders for image and audio, and a trainable modality projector. The proposed
effective data mixture for training such an architecture demonstrates that a
multimodal model based on a strong language model and trained on a small amount
of multimodal data can perform efficiently in the task of multimodal dialog.

摘要：在這項工作中，我們為多模態對話任務提供了一個概念簡單但功能強大的基線，一個 S3 模型，在兩個引人注目的排行榜：MMMU 和 AI Journey Contest 2023 上達到了接近最先進的結果。該系統基於預先訓練的大語言模型、預先訓練的影像和音訊模態編碼器，以及一個可訓練的模態投影器。建議用於訓練此架構的有效資料混合證明了一個基於強大語言模型且在少量的多模態資料上訓練的多模態模型可以在多模態對話的任務中有效執行。

##### **MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research**
2406.18301v1 by Song Li, Yongbin You, Xuezhi Wang, Zhengkun Tian, Ke Ding, Guanglu Wan

Recently, multilingual artificial intelligence assistants, exemplified by
ChatGPT, have gained immense popularity. As a crucial gateway to human-computer
interaction, multilingual automatic speech recognition (ASR) has also garnered
significant attention, as evidenced by systems like Whisper. However, the
proprietary nature of the training data has impeded researchers' efforts to
study multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale
multilingual corpus for speech recognition research. The corpus is derived from
publicly accessible videos on YouTube, comprising 15 languages and a total of
86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K
corpus and other open-source corpora to train a robust multilingual ASR model
that is competitive with Whisper. MSR-86K will be publicly released on
HuggingFace, and we believe that such a large corpus will pave new avenues for
research in multilingual ASR.

摘要：最近，以 ChatGPT 為例的多語言人工智慧助理獲得極大的歡迎。作為人機互動的重要管道，多語言自動語音辨識 (ASR) 也備受關注，例如 Whisper 等系統。然而，訓練資料的專有性質阻礙了研究人員研究多語言 ASR 的努力。本文介紹 MSR-86K，這是一個不斷演進的大規模多語言語料庫，用於語音辨識研究。語料庫來自 YouTube 上公開的影片，包含 15 種語言，總共 86,300 小時的轉錄 ASR 資料。我們也介紹如何使用 MSR-86K 語料庫和其他開源語料庫來訓練一個健全的多語言 ASR 模型，與 Whisper 相當。MSR-86K 將在 HuggingFace 上公開發布，我們相信如此龐大的語料庫將為多語言 ASR 研究開闢新的途徑。

##### **FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning**
2406.18297v1 by Yufeng Li, Rrubaa Panchendrarajan, Arkaitz Zubiaga

The rapid dissemination of information through social media and the Internet
has posed a significant challenge for fact-checking, among others in
identifying check-worthy claims that fact-checkers should pay attention to,
i.e. filtering claims needing fact-checking from a large pool of sentences.
This challenge has stressed the need to focus on determining the priority of
claims, specifically which claims are worth to be fact-checked. Despite
advancements in this area in recent years, the application of large language
models (LLMs), such as GPT, has only recently drawn attention in studies.
However, many open-source LLMs remain underexplored. Therefore, this study
investigates the application of eight prominent open-source LLMs with
fine-tuning and prompt engineering to identify check-worthy statements from
political transcriptions. Further, we propose a two-step data pruning approach
to automatically identify high-quality training data instances for effective
learning. The efficiency of our approach is demonstrated through evaluations on
the English language dataset as part of the check-worthiness estimation task of
CheckThat! 2024. Further, the experiments conducted with data pruning
demonstrate that competitive performance can be achieved with only about 44\%
of the training data. Our team ranked first in the check-worthiness estimation
task in the English language.

摘要：社群媒體與網際網路的資訊快速傳播，對事實查核造成重大挑戰，包括識別事實查核人員應注意的值得查核的說法，亦即從大量的句子中過濾出需要事實查核的說法。這個挑戰強調了必須專注於確定說法的優先順序，特別是哪些說法值得進行事實查核。儘管近年來在這個領域已有進展，但大型語言模型（LLM）的應用，例如 GPT，直到最近才引起研究的注意。然而，許多開源 LLM 仍未被充分探索。因此，本研究探討了八種傑出的開源 LLM 的應用，並透過微調和提示工程來識別政治謄本中的值得查核的陳述。此外，我們提出了一種兩步驟資料修剪方法，用於自動識別高品質的訓練資料實例，以進行有效的學習。我們的方法的效率透過在 CheckThat! 2024 的值得查核性評估任務中，對英語語言資料集的評估來證明。此外，使用資料修剪進行的實驗證明，僅使用約 44% 的訓練資料即可達成競爭性的表現。我們的團隊在英語語言的值得查核性評估任務中排名第一。

##### **Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs**
2406.18294v1 by Lei Zhang, Yunshui Li, Jiaming Li, Xiaobo Xia, Jiaxi Yang, Run Luo, Minzheng Wang, Longze Chen, Junhao Liu, Min Yang

Some recently developed code large language models (Code LLMs) have been
pre-trained on repository-level code data (Repo-Code LLMs), enabling these
models to recognize repository structures and utilize cross-file information
for code completion. However, in real-world development scenarios, simply
concatenating the entire code repository often exceeds the context window
limits of these Repo-Code LLMs, leading to significant performance degradation.
In this study, we conducted extensive preliminary experiments and analyses on
six Repo-Code LLMs. The results indicate that maintaining the topological
dependencies of files and increasing the code file content in the completion
prompts can improve completion accuracy; pruning the specific implementations
of functions in all dependent files does not significantly reduce the accuracy
of completions. Based on these findings, we proposed a strategy named
Hierarchical Context Pruning (HCP) to construct completion prompts with high
informational code content. The HCP models the code repository at the function
level, maintaining the topological dependencies between code files while
removing a large amount of irrelevant code content, significantly reduces the
input length for repository-level code completion. We applied the HCP strategy
in experiments with six Repo-Code LLMs, and the results demonstrate that our
proposed method can significantly enhance completion accuracy while
substantially reducing the length of input. Our code and data are available at
https://github.com/Hambaobao/HCP-Coder.

摘要：一些最近開發的程式碼大型語言模型 (Code LLM) 已在儲存庫層級程式碼資料 (Repo-Code LLM) 上進行預先訓練，讓這些模型能夠辨識儲存庫結構並利用跨檔案資訊進行程式碼完成。然而，在真實世界的開發情境中，單純串接整個程式碼儲存庫通常會超過這些 Repo-Code LLM 的內容視窗限制，導致效能大幅下降。在本研究中，我們對六個 Repo-Code LLM 進行了廣泛的初步實驗和分析。結果顯示，維護檔案的拓撲相依性並增加完成提示中的程式碼檔案內容，可以提升完成準確度；移除所有相依檔案中函式的特定實作並不會顯著降低完成的準確度。基於這些發現，我們提出了一種名為階層式內容移除 (HCP) 的策略，以建構具有高資訊程式碼內容的完成提示。HCP 在函式層級對程式碼儲存庫進行建模，維護程式碼檔案之間的拓撲相依性，同時移除大量的無關程式碼內容，大幅減少儲存庫層級程式碼完成的輸入長度。我們在六個 Repo-Code LLM 的實驗中應用 HCP 策略，結果證明我們提出的方法可以在大幅減少輸入長度的同時，顯著提升完成準確度。我們的程式碼和資料可在 https://github.com/Hambaobao/HCP-Coder 取得。

##### **Sanskrit Knowledge-based Systems: Annotation and Computational Tools**
2406.18276v1 by Hrishikesh Terdalkar

We address the challenges and opportunities in the development of knowledge
systems for Sanskrit, with a focus on question answering. By proposing a
framework for the automated construction of knowledge graphs, introducing
annotation tools for ontology-driven and general-purpose tasks, and offering a
diverse collection of web-interfaces, tools, and software libraries, we have
made significant contributions to the field of computational Sanskrit. These
contributions not only enhance the accessibility and accuracy of Sanskrit text
analysis but also pave the way for further advancements in knowledge
representation and language processing. Ultimately, this research contributes
to the preservation, understanding, and utilization of the rich linguistic
information embodied in Sanskrit texts.

摘要：我們著手解決梵語知識系統開發中的挑戰和機會，重點在於問題解答。透過提出一個用於自動建構知識圖譜的架構，導入用於本體驅動和一般用途任務的註解工具，並提供多樣化的網路介面、工具和軟體函式庫，我們對計算梵語領域做出了重大貢獻。這些貢獻不僅增強了梵語文本分析的可存取性和準確性，也為知識表徵和語言處理的進一步進展鋪平了道路。最終，這項研究有助於保存、理解和利用梵語文本中蘊含的豐富語言資訊。

##### **"Vorbeşti Româneşte?" A Recipe to Train Powerful Romanian LLMs with English Instructions**
2406.18266v1 by Mihai Masala, Denis C. Ilie-Ablachim, Alexandru Dima, Dragos Corlatescu, Miruna Zavelca, Ovio Olaru, Simina Terian-Dan, Andrei Terian-Dan, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea

In recent years, Large Language Models (LLMs) have achieved almost human-like
performance on various tasks. While some LLMs have been trained on multilingual
data, most of the training data is in English; hence, their performance in
English greatly exceeds other languages. To our knowledge, we are the first to
collect and translate a large collection of texts, instructions, and benchmarks
and train, evaluate, and release open-source LLMs tailored for Romanian. We
evaluate our methods on four different categories, including academic
benchmarks, MT-Bench (manually translated), and a professionally built
historical, cultural, and social benchmark adapted to Romanian. We argue for
the usefulness and high performance of RoLLMs by obtaining state-of-the-art
results across the board. We publicly release all resources (i.e., data,
training and evaluation code, models) to support and encourage research on
Romanian LLMs while concurrently creating a generalizable recipe, adequate for
other low or less-resourced languages.

摘要：近年來，大型語言模型 (LLM) 在各種任務上達到了近乎人類般的表現。儘管某些 LLM 已針對多語言資料進行訓練，但大部分訓練資料仍為英文；因此，它們在英文中的表現遠遠超過其他語言。據我們所知，我們是第一個收集並翻譯大量文本、說明和基準，並訓練、評估和釋出針對羅馬尼亞語量身打造的開源 LLM。我們在四個不同類別上評估我們的模型，包括學術基準、MT-Bench（人工翻譯），以及針對羅馬尼亞語調整的專業建置歷史、文化和社會基準。我們透過獲得全面的最新結果來主張 RoLLM 的實用性和高性能。我們公開釋出所有資源（即資料、訓練和評估程式碼、模型），以支援和鼓勵針對羅馬尼亞語 LLM 的研究，同時建立適於其他低資源或較少資源語言的通用配方。

##### **Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability is Complicated**
2406.18259v1 by Jiazhou Ji, Ruizhe Li, Shujun Li, Jie Guo, Weidong Qiu, Zheng Huang, Chiyu Chen, Xiaoyu Jiang, Xinru Lu

As LLMs rapidly advance, increasing concerns arise regarding risks about
actual authorship of texts we see online and in real world. The task of
distinguishing LLM-authored texts is complicated by the nuanced and overlapping
behaviors of both machines and humans. In this paper, we challenge the current
practice of considering LLM-generated text detection a binary classification
task of differentiating human from AI. Instead, we introduce a novel ternary
text classification scheme, adding an "undecided" category for texts that could
be attributed to either source, and we show that this new category is crucial
to understand how to make the detection result more explainable to lay users.
This research shifts the paradigm from merely classifying to explaining
machine-generated texts, emphasizing need for detectors to provide clear and
understandable explanations to users. Our study involves creating four new
datasets comprised of texts from various LLMs and human authors. Based on new
datasets, we performed binary classification tests to ascertain the most
effective SOTA detection methods and identified SOTA LLMs capable of producing
harder-to-detect texts. We constructed a new dataset of texts generated by two
top-performing LLMs and human authors, and asked three human annotators to
produce ternary labels with explanation notes. This dataset was used to
investigate how three top-performing SOTA detectors behave in new ternary
classification context. Our results highlight why "undecided" category is much
needed from the viewpoint of explainability. Additionally, we conducted an
analysis of explainability of the three best-performing detectors and the
explanation notes of the human annotators, revealing insights about the
complexity of explainable detection of machine-generated texts. Finally, we
propose guidelines for developing future detection systems with improved
explanatory power.

摘要：<paragraph>隨著大型語言模型 (LLM) 快速進步，對於我們在網路上和現實世界中所見文字的實際作者身分，也引發了越來越多的風險疑慮。區分 LLM 撰寫文字的任務，因為機器和人類細微且重疊的行為而變得複雜。在本文中，我們挑戰了將 LLM 生成的文字偵測視為區分人類與 AI 的二元分類任務的現行做法。相反地，我們引進了一種新穎的三元文字分類架構，為可能歸因於任一來源的文字新增了一個「未決定」類別，並且我們證明了這個新類別對於了解如何讓偵測結果對一般使用者更具解釋性至關重要。這項研究將典範從單純分類轉變為解釋機器產生的文字，強調偵測器需要向使用者提供明確且易懂的解釋。我們的研究包括建立四個由來自各種 LLM 和人類作者的文字組成的全新資料集。根據新的資料集，我們執行了二元分類測試，以確定最有效的 SOTA 偵測方法，並找出能夠產生難以偵測文字的 SOTA LLM。我們建構了一個由兩個效能最佳的 LLM 和人類作者產生的文字組成的全新資料集，並請三位人類註解員提供包含解釋註解的三元標籤。這個資料集被用於調查三種效能最佳的 SOTA 偵測器在新三元分類脈絡中的表現。我們的結果強調了為什麼從可解釋性的觀點來看，「未決定」類別非常必要。此外，我們對效能最佳的三種偵測器的可解釋性以及人類註解員的解釋註解進行了分析，揭示了機器產生的文字的可解釋性偵測的複雜性。最後，我們提出了開發具有改進解釋力的未來偵測系統的準則。</paragraph>

##### **LLaMIPa: An Incremental Discourse Parser**
2406.18256v1 by Kate Thompson, Akshay Chaturvedi, Julie Hunter, Nicholas Asher

This paper provides the first discourse parsing experiments with a large
language model (LLM) finetuned on corpora annotated in the style of SDRT
(Asher, 1993; Asher and Lascarides, 2003). The result is a discourse parser,
LLaMIPa (LLaMA Incremental Parser), which is able to more fully exploit
discourse context, leading to substantial performance gains over approaches
that use encoder-only models to provide local, context-sensitive
representations of discourse units. Furthermore, it is able to process
discourse data incrementally, which is essential for the eventual use of
discourse information in downstream tasks.

摘要：本文提供了第一個語篇解析實驗，其中包含一個大型語言模型 (LLM)，該模型針對 SDRT 風格的語料庫進行微調（Asher，1993；Asher 和 Lascarides，2003）。結果是一個語篇解析器，LLaMIPa（LLaMA 增量解析器），它能夠更充分地利用語篇上下文，從而大幅提高了僅使用編碼器模型來提供語篇單元的局部上下文敏感表示的方法的效能。此外，它能夠增量處理語篇數據，這對於在後續任務中最終使用語篇信息至關重要。

##### **Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning**
2406.18254v1 by Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, Xudong Liu

Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search,
which aims to break the barriers between modality and language simultaneously
and achieves image-text retrieval in the multi-lingual scenario with a single
model. In recent years, excellent progress has been made based on cross-lingual
cross-modal pre-training; particularly, the methods based on contrastive
learning on large-scale data have significantly improved retrieval tasks.
However, these methods directly follow the existing pre-training methods in the
cross-lingual or cross-modal domain, leading to two problems of inconsistency
in CCR: The methods with cross-lingual style suffer from the intra-modal error
propagation, resulting in inconsistent recall performance across languages in
the whole dataset. The methods with cross-modal style suffer from the
inter-modal optimization direction bias, resulting in inconsistent rank across
languages within each instance, which cannot be reflected by Recall@K. To solve
these problems, we propose a simple but effective 1-to-K contrastive learning
method, which treats each language equally and eliminates error propagation and
optimization bias. In addition, we propose a new evaluation metric, Mean Rank
Variance (MRV), to reflect the rank inconsistency across languages within each
instance. Extensive experiments on four CCR datasets show that our method
improves both recall rates and MRV with smaller-scale pre-trained data,
achieving the new state-of-art.

摘要：跨語言跨模態檢索 (CCR) 是網路搜尋中的一項重要任務，其目標是同時打破模態和語言之間的障礙，並透過單一模型在多語言場景中達成影像文字檢索。近年來，基於跨語言跨模態預訓練已取得顯著進展；特別是，基於大型資料對比學習的方法已大幅改善檢索任務。然而，這些方法直接遵循跨語言或跨模態領域中現有的預訓練方法，導致 CCR 中出現兩個不一致的問題：具有跨語言風格的方法會受到模態內部錯誤傳播的影響，導致整個資料集中各語言之間的召回率表現不一致。具有跨模態風格的方法會受到模態間最佳化方向偏差的影響，導致每個實例中各語言之間的排名不一致，這無法透過 Recall@K 反映出來。為了解決這些問題，我們提出一個簡單但有效的 1 對 K 對比學習方法，該方法一視同仁地對待每種語言，並消除了錯誤傳播和最佳化偏差。此外，我們提出一個新的評估指標，平均排名差異 (MRV)，以反映每個實例中各語言之間的排名不一致性。在四個 CCR 資料集上進行的廣泛實驗顯示，我們的模型透過較小規模的預訓練資料改善了召回率和 MRV，達到了新的技術水準。

##### **Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems**
2406.18245v1 by Italo Luis da Silva, Hanqi Yan, Lin Gui, Yulan He

The inherent ambiguity of cause and effect boundaries poses a challenge in
evaluating causal event extraction tasks. Traditional metrics like Exact Match
and BertScore poorly reflect model performance, so we trained evaluation models
to approximate human evaluation, achieving high agreement. We used them to
perform Reinforcement Learning with extraction models to align them with human
preference, prioritising semantic understanding. We successfully explored our
approach through multiple datasets, including transferring an evaluator trained
on one dataset to another as a way to decrease the reliance on human-annotated
data. In that vein, we also propose a weak-to-strong supervision method that
uses a fraction of the annotated data to train an evaluation model while still
achieving high performance in training an RL model. Our code is available at
\url{https://github.com/oyarsa/event_extraction/tree/causal-event-extraction}.

摘要：因果關係邊界的內在模糊性對評估因果事件抽取任務構成挑戰。傳統指標，例如完全匹配和 BertScore，無法充分反映模型效能，因此我們訓練評估模型以近似人類評估，並達成高度一致性。我們使用它們與抽取模型執行強化學習，以使其與人類偏好一致，並優先考慮語義理解。我們透過多個資料集成功地探索了我們的做法，包括將在一個資料集上訓練的評估員轉移到另一個資料集，作為減少對人工標註資料依賴的一種方式。在那個脈絡中，我們還提出了一種弱到強的監督方法，它使用一小部分標註資料來訓練評估模型，同時仍在訓練 RL 模型時達到高性能。我們的程式碼可在網址取得：\url{https://github.com/oyarsa/event_extraction/tree/causal-event-extraction}。

##### **Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets**
2406.18239v1 by Simon Münker, Kai Kugler, Achim Rettinger

Filtering and annotating textual data are routine tasks in many areas, like
social media or news analytics. Automating these tasks allows to scale the
analyses wrt. speed and breadth of content covered and decreases the manual
effort required. Due to technical advancements in Natural Language Processing,
specifically the success of large foundation models, a new tool for automating
such annotation processes by using a text-to-text interface given written
guidelines without providing training samples has become available.
  In this work, we assess these advancements in-the-wild by empirically testing
them in an annotation task on German Twitter data about social and political
European crises. We compare the prompt-based results with our human annotation
and preceding classification approaches, including Naive Bayes and a BERT-based
fine-tuning/domain adaptation pipeline. Our results show that the prompt-based
approach - despite being limited by local computation resources during the
model selection - is comparable with the fine-tuned BERT but without any
annotated training data. Our findings emphasize the ongoing paradigm shift in
the NLP landscape, i.e., the unification of downstream tasks and elimination of
the need for pre-labeled training data.

摘要：過濾和註解文本資料在許多領域中都是例行公事，例如社群媒體或新聞分析。自動化這些任務可以擴展分析的範圍，包括涵蓋內容的速度和廣度，並減少所需的手動工作。由於自然語言處理技術的進步，特別是大規模基礎模型的成功，使用文字轉文字介面，在沒有提供訓練範例的情況下，自動化此類註解程序的新工具已經問世。
在這項工作中，我們透過在關於社會和政治歐洲危機的德文 Twitter 資料中，對註解任務進行經驗測試，來評估這些進步在實際情況中的表現。我們將提示式結果與我們的人工註解和先前的分類方法進行比較，包括朴素貝氏和基於 BERT 的微調/領域適應管道。我們的結果顯示，提示式方法儘管在模型選擇期間受到本地運算資源的限制，但與微調過的 BERT 相當，但沒有任何註解過的訓練資料。我們的發現強調了 NLP 領域中持續進行的典範轉移，即整合下游任務並消除對預先標記訓練資料的需求。

##### **GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension**
2406.18227v1 by Jiafeng Liang, Shixin Jiang, Zekun Wang, Haojie Pan, Zerui Chen, Zheng Chu, Ming Liu, Ruiji Fu, Zhongyuan Wang, Bing Qin

There are substantial instructional videos on the Internet, which provide us
tutorials for completing various tasks. Existing instructional video datasets
only focus on specific steps at the video level, lacking experiential
guidelines at the task level, which can lead to beginners struggling to learn
new tasks due to the lack of relevant experience. Moreover, the specific steps
without guidelines are trivial and unsystematic, making it difficult to provide
a clear tutorial. To address these problems, we present the GUIDE
(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructional
tasks in 8 domains related to our daily life. Specifically, we annotate each
instructional task with a guideline, representing a common pattern shared by
all task-related videos. On this basis, we annotate systematic specific steps,
including their associated guideline steps, specific step descriptions and
timestamps. Our proposed benchmark consists of three sub-tasks to evaluate
comprehension ability of models: (1) Step Captioning: models have to generate
captions for specific steps from videos. (2) Guideline Summarization: models
have to mine the common pattern in task-related videos and summarize a
guideline from them. (3) Guideline-Guided Captioning: models have to generate
captions for specific steps under the guide of guideline. We evaluate plenty of
foundation models with GUIDE and perform in-depth analysis. Given the diversity
and practicality of GUIDE, we believe that it can be used as a better benchmark
for instructional video comprehension.

摘要：<paragraph>網路上有大量的教學影片，提供我們完成各種任務的教學。現有的教學影片資料集僅專注於影片層面的特定步驟，缺乏任務層面的體驗指南，這可能導致初學者因缺乏相關經驗而難以學習新任務。此外，沒有指南的特定步驟很瑣碎且不系統化，這使得很難提供明確的教學。為了解決這些問題，我們提出了 GUIDE（指南引導）資料集，其中包含 8 個與我們日常生活相關的領域中 560 個教學任務的 3.5K 個影片。具體來說，我們使用指南註解每個教學任務，代表所有與任務相關影片共有的常見模式。在此基礎上，我們註解了系統化的具體步驟，包括它們相關的指南步驟、具體步驟說明和時間戳記。我們提出的基準測試包含三個子任務，用於評估模型的理解能力：(1) 步驟標題：模型必須從影片中為特定步驟生成標題。(2) 指南摘要：模型必須挖掘與任務相關影片中的常見模式，並從中總結出指南。(3) 指南引導標題：模型必須在指南的指導下為特定步驟生成標題。我們使用 GUIDE 評估了大量的基礎模型，並進行了深入的分析。鑑於 GUIDE 的多樣性和實用性，我們相信它可以用作教學影片理解的更好基準測試。</paragraph>

##### **Enhancing Data Privacy in Large Language Models through Private Association Editing**
2406.18221v1 by Davide Venditti, Elena Sofia Ruzzetti, Giancarlo A. Xompero, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto

Large Language Models (LLMs) are powerful tools with extensive applications,
but their tendency to memorize private information raises significant concerns
as private data leakage can easily happen. In this paper, we introduce Private
Association Editing (PAE), a novel defense approach for private data leakage.
PAE is designed to effectively remove Personally Identifiable Information (PII)
without retraining the model. Our approach consists of a four-step procedure:
detecting memorized PII, applying PAE cards to mitigate memorization of private
data, verifying resilience to targeted data extraction (TDE) attacks, and
ensuring consistency in the post-edit LLMs. The versatility and efficiency of
PAE, which allows for batch modifications, significantly enhance data privacy
in LLMs. Experimental results demonstrate the effectiveness of PAE in
mitigating private data leakage. We believe PAE will serve as a critical tool
in the ongoing effort to protect data privacy in LLMs, encouraging the
development of safer models for real-world applications.

摘要：大型語言模型 (LLM) 是功能強大的工具，具有廣泛的應用，
但它們記住私人資訊的傾向引起了重大疑慮，
因為私人資料外洩很容易發生。在本文中，我們介紹了私人關聯編輯 (PAE)，
這是一種針對私人資料外洩的新穎防禦方法。
PAE 被設計為有效移除個人身分資訊 (PII)，
而無需重新訓練模型。我們的做法包含四個步驟程序：
偵測記住的 PII，應用 PAE 卡來減輕記住私人資料，
驗證對目標資料擷取 (TDE) 攻擊的復原力，以及
確保後編輯 LLM 的一致性。
PAE 的多功能性和效率，允許批次修改，大幅增強了 LLM 中的資料隱私。
實驗結果證明了 PAE 在減輕私人資料外洩方面的有效性。
我們相信 PAE 將成為保護 LLM 中資料隱私的持續努力中一個重要的工具，
鼓勵開發更安全的模型以供現實世界應用。

##### **Guiding Video Prediction with Explicit Procedural Knowledge**
2406.18220v1 by Patrick Takenaka, Johannes Maucher, Marco F. Huber

We propose a general way to integrate procedural knowledge of a domain into
deep learning models. We apply it to the case of video prediction, building on
top of object-centric deep models and show that this leads to a better
performance than using data-driven models alone. We develop an architecture
that facilitates latent space disentanglement in order to use the integrated
procedural knowledge, and establish a setup that allows the model to learn the
procedural interface in the latent space using the downstream task of video
prediction. We contrast the performance to a state-of-the-art data-driven
approach and show that problems where purely data-driven approaches struggle
can be handled by using knowledge about the domain, providing an alternative to
simply collecting more data.

摘要：我們提出了一種將領域程序知識整合到深度學習模型中的通用方法。我們將其應用於影片預測的案例中，建立在以物件為中心的深度模型之上，並顯示出這樣做會比單獨使用資料驅動模型帶來更好的效能。我們開發了一種架構，以利於潛在空間解開糾纏，以便使用整合的程序知識，並建立一個設定，讓模型能夠使用影片預測的下游任務在潛在空間中學習程序介面。我們將效能與最先進的資料驅動方法進行對比，並顯示出純粹資料驅動方法難以處理的問題，可以使用關於領域的知識來處理，提供一種替代方案，而不用只是收集更多資料。

##### **A Closer Look into Mixture-of-Experts in Large Language Models**
2406.18219v1 by Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu

Mixture-of-experts (MoE) is gaining increasing attention due to its unique
properties and remarkable performance, especially for language tasks. By
sparsely activating a subset of parameters for each token, MoE architecture
could increase the model size without sacrificing computational efficiency,
achieving a better trade-off between performance and training costs. However,
the underlying mechanism of MoE still lacks further exploration, and its
modularization degree remains questionable. In this paper, we make an initial
attempt to understand the inner workings of MoE-based large language models.
Concretely, we comprehensively study the parametric and behavioral features of
three recent MoE-based models and reveal some intriguing observations,
including (1) Neurons act like fine-grained experts. (2) The router of MoE
usually selects experts with larger output norms. (3) The expert diversity
increases as the layer increases, while the last layer is an outlier. Based on
the observations, we also provide suggestions for a broad spectrum of MoE
practitioners, such as router design and expert allocation. We hope this work
could shed light on future research on the MoE framework and other modular
architectures. Code is available at
https://github.com/kamanphoebe/Look-into-MoEs.

摘要：混合专家 (MoE) 因其独特的特性和卓越的性能而备受关注，尤其是在语言任务中。通过稀疏激活每个标记的子集参数，MoE 架构可以在不牺牲计算效率的情况下增加模型大小，从而在性能和训练成本之间取得更好的平衡。然而，MoE 的底层机制仍然缺乏进一步的探索，其模块化程度也值得商榷。在本文中，我们尝试初步了解基于 MoE 的大型语言模型的内部运作方式。具体来说，我们全面研究了三个近期基于 MoE 的模型的参数和行为特征，并揭示了一些有趣的观察结果，包括 (1) 神经元充当细粒度专家。 (2) MoE 的路由器通常选择具有较大输出范数的专家。 (3) 专家多样性随着层数的增加而增加，而最后一层是一个异常值。基于这些观察结果，我们还为广泛的 MoE 实践者提供了建议，例如路由器设计和专家分配。我们希望这项工作可以为未来对 MoE 框架和其他模块化架构的研究提供启示。代码可在 https://github.com/kamanphoebe/Look-into-MoEs 获得。

##### **SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding**
2406.18200v1 by Zhenglin Wang, Jialong Wu, Yilong Lai, Congzhi Zhang, Deyu Zhou

Large Language Models (LLMs) demonstrate remarkable emergent abilities across
various tasks, yet fall short of complex reasoning and planning tasks. The
tree-search-based reasoning methods address this by surpassing the capabilities
of chain-of-thought prompting, encouraging exploration of intermediate steps.
However, such methods introduce significant inference latency due to the
systematic exploration and evaluation of multiple thought paths. This paper
introduces SeeD, a novel and efficient inference framework to optimize runtime
speed and GPU memory management concurrently. By employing a scheduled
speculative execution, SeeD efficiently handles multiple iterations for the
thought generation and the state evaluation, leveraging a rounds-scheduled
strategy to manage draft model dispatching. Extensive experimental evaluations
on three reasoning datasets demonstrate superior speedup performance of SeeD,
providing a viable path for batched inference in training-free speculative
decoding.

摘要：大型語言模型 (LLM) 在各種任務中展現出非凡的新興能力，但仍無法進行複雜的推理和規劃任務。基於樹狀搜尋的推理方法通過超越思想提示鏈的能力來解決這個問題，鼓勵探索中間步驟。然而，由於系統性地探索和評估多個思考路徑，這種方法會引入顯著的推理延遲。本文介紹了 SeeD，這是一個新穎且高效的推理框架，用於同時最佳化執行時間速度和 GPU 記憶體管理。通過採用預定的推測性執行，SeeD 可以有效地處理思想生成和狀態評估的多次反覆運算，利用回合預定策略來管理草稿模型的調度。在三個推理資料集上的廣泛實驗評估證明了 SeeD 的優異加速效能，為訓練免費推測解碼中的批次推理提供了可行的路徑。

##### **MammothModa: Multi-Modal Large Language Model**
2406.18193v1 by Qi She, Junwen Pan, Xin Wan, Rui Zhang, Dawei Lu, Kai Huang

In this report, we introduce MammothModa, yet another multi-modal large
language model (MLLM) designed to achieve state-of-the-art performance starting
from an elementary baseline. We focus on three key design insights: (i)
Integrating Visual Capabilities while Maintaining Complex Language
Understanding: In addition to the vision encoder, we incorporated the Visual
Attention Experts into the LLM to enhance its visual capabilities. (ii)
Extending Context Window for High-Resolution and Long-Duration Visual Feature:
We explore the Visual Merger Module to effectively reduce the token number of
high-resolution images and incorporated frame position ids to avoid position
interpolation. (iii) High-Quality Bilingual Datasets: We meticulously curated
and filtered a high-quality bilingual multimodal dataset to reduce visual
hallucinations. With above recipe we build MammothModa that consistently
outperforms the state-of-the-art models, e.g., LLaVA-series, across main
real-world visual language benchmarks without bells and whistles.

摘要：在本文中，我們介紹 MammothModa，另一個多模態大型語言模型 (MLLM)，旨在從基礎基準開始實現最先進的效能。我們專注於三個關鍵設計見解：(i) 整合視覺功能，同時維持複雜語言理解：除了視覺編碼器之外，我們將視覺注意力專家納入 LLM，以增強其視覺功能。(ii) 擴展內容視窗以獲得高解析度和長時程視覺特徵：我們探討視覺合併模組，以有效減少高解析度影像的代幣數，並納入影像位置 ID，以避免位置內插。(iii) 高品質雙語資料集：我們仔細策劃並過濾高品質雙語多模態資料集，以減少視覺幻覺。透過上述方法，我們建構出 MammothModa，它在主要的真實世界視覺語言基準中，始終優於最先進的模型，例如 LLaVA 系列，而且沒有花俏的技巧。

##### **Methodology of Adapting Large English Language Models for Specific Cultural Contexts**
2406.18192v1 by Wenjing Zhang, Siqi Xiao, Xuejiao Lei, Ning Wang, Huazheng Zhang, Meijuan An, Bikun Yang, Zhaoxiang Liu, Kai Wang, Shiguo Lian

The rapid growth of large language models(LLMs) has emerged as a prominent
trend in the field of artificial intelligence. However, current
state-of-the-art LLMs are predominantly based on English. They encounter
limitations when directly applied to tasks in specific cultural domains, due to
deficiencies in domain-specific knowledge and misunderstandings caused by
differences in cultural values. To address this challenge, our paper proposes a
rapid adaptation method for large models in specific cultural contexts, which
leverages instruction-tuning based on specific cultural knowledge and safety
values data. Taking Chinese as the specific cultural context and utilizing the
LLaMA3-8B as the experimental English LLM, the evaluation results demonstrate
that the adapted LLM significantly enhances its capabilities in domain-specific
knowledge and adaptability to safety values, while maintaining its original
expertise advantages.

摘要：大型語言模型 (LLM) 的快速發展已成為人工智慧領域的顯著趨勢。然而，目前最先進的 LLM 主要基於英語。由於缺乏特定文化領域的知識，以及文化價值差異造成的誤解，當直接應用於特定文化領域的任務時，它們會遇到限制。為了應對這一挑戰，我們的論文提出了一種在特定文化背景下快速調整大型模型的方法，該方法利用基於特定文化知識和安全價值觀數據的指令調整。以中文為特定文化背景，並使用 LLaMA3-8B 作為實驗性的英語 LLM，評估結果表明，調整後的 LLM 大幅提升了其在特定領域知識和對安全價值觀的適應能力，同時保持其原有的專業優勢。

##### **Selective Prompting Tuning for Personalized Conversations with LLMs**
2406.18187v1 by Qiushi Huang, Xubo Liu, Tom Ko, Bo Wu, Wenwu Wang, Yu Zhang, Lilian Tang

In conversational AI, personalizing dialogues with persona profiles and
contextual understanding is essential. Despite large language models' (LLMs)
improved response coherence, effective persona integration remains a challenge.
In this work, we first study two common approaches for personalizing LLMs:
textual prompting and direct fine-tuning. We observed that textual prompting
often struggles to yield responses that are similar to the ground truths in
datasets, while direct fine-tuning tends to produce repetitive or overly
generic replies. To alleviate those issues, we propose \textbf{S}elective
\textbf{P}rompt \textbf{T}uning (SPT), which softly prompts LLMs for
personalized conversations in a selective way. Concretely, SPT initializes a
set of soft prompts and uses a trainable dense retriever to adaptively select
suitable soft prompts for LLMs according to different input contexts, where the
prompt retriever is dynamically updated through feedback from the LLMs.
Additionally, we propose context-prompt contrastive learning and prompt fusion
learning to encourage the SPT to enhance the diversity of personalized
conversations. Experiments on the CONVAI2 dataset demonstrate that SPT
significantly enhances response diversity by up to 90\%, along with
improvements in other critical performance indicators. Those results highlight
the efficacy of SPT in fostering engaging and personalized dialogue generation.
The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available
for further exploration.

摘要：在對話式 AI 中，使用角色設定檔和情境理解來個人化對話至關重要。儘管大型語言模型 (LLM) 改善了回應的連貫性，但有效整合角色設定檔仍然是一個挑戰。在這項工作中，我們首先研究了兩種用於個人化 LLM 的常見方法：文字提示和直接微調。我們觀察到，文字提示通常難以產生與資料集中的基本事實相似的回應，而直接微調則傾向於產生重複或過於通用的回覆。為了緩解這些問題，我們提出了**選**擇性**提**示**調**校 (SPT)，它以選擇性的方式柔和地提示 LLM 進行個人化對話。具體來說，SPT 初始化一組軟提示，並使用可訓練的密集檢索器根據不同的輸入情境自適應地為 LLM 選擇合適的軟提示，其中提示檢索器會透過 LLM 的回饋進行動態更新。此外，我們提出了情境提示對比學習和提示融合學習，以鼓勵 SPT 提升個人化對話的多樣性。在 CONVAI2 資料集上的實驗證明，SPT 將回應多樣性顯著提升了 90%，同時也改善了其他重要的效能指標。這些結果突顯了 SPT 在促進引人入勝且個人化的對話產生方面的效力。SPT 模型程式碼 (https://github.com/hqsiswiliam/SPT) 已公開，供進一步探索。

##### **Games of Knightian Uncertainty**
2406.18178v1 by Spyridon Samothrakis, Dennis J. N. J. Soemers, Damian Machlanski

Arguably, for the latter part of the late 20th and early 21st centuries,
games have been seen as the drosophila of AI. Games are a set of exciting
testbeds, whose solutions (in terms of identifying optimal players) would lead
to machines that would possess some form of general intelligence, or at the
very least help us gain insights toward building intelligent machines.
Following impressive successes in traditional board games like Go, Chess, and
Poker, but also video games like the Atari 2600 collection, it is clear that
this is not the case. Games have been attacked successfully, but we are nowhere
near AGI developments (or, as harsher critics might say, useful AI
developments!). In this short vision paper, we argue that for game research to
become again relevant to the AGI pathway, we need to be able to address
\textit{Knightian uncertainty} in the context of games, i.e. agents need to be
able to adapt to rapid changes in game rules on the fly with no warning, no
previous data, and no model access.

摘要：可以說，在 20 世紀末和 21 世紀初的後半段，遊戲被視為 AI 的果蠅。遊戲是一組令人興奮的測試平台，其解決方案（在識別最佳玩家方面）將導致擁有某種形式的通用智能的機器，或至少幫助我們獲得對構建智能機器的見解。在圍棋、國際象棋和撲克等傳統棋盤遊戲以及 Atari 2600 收藏等電子遊戲中取得令人印象深刻的成功之後，很明顯情況並非如此。遊戲已經成功受到攻擊，但我們遠遠沒有達到 AGI 發展（或者，正如嚴厲的批評者所說，有用的 AI 發展！）。在這篇簡短的願景論文中，我們認為遊戲研究要再次與 AGI 路徑相關，我們需要能夠在遊戲的背景下解決「奈特不確定性」，即代理必須能夠在沒有警告、沒有先前數據和沒有模型訪問的情況下適應遊戲規則的快速變化。

##### **Galaxy spectroscopy without spectra: Galaxy properties from photometric images with conditional diffusion models**
2406.18175v1 by Lars Doorenbos, Eva Sextl, Kevin Heng, Stefano Cavuoti, Massimo Brescia, Olena Torbaniuk, Giuseppe Longo, Raphael Sznitman, Pablo Márquez-Neila

Modern spectroscopic surveys can only target a small fraction of the vast
amount of photometrically cataloged sources in wide-field surveys. Here, we
report the development of a generative AI method capable of predicting optical
galaxy spectra from photometric broad-band images alone. This method draws from
the latest advances in diffusion models in combination with contrastive
networks. We pass multi-band galaxy images into the architecture to obtain
optical spectra. From these, robust values for galaxy properties can be derived
with any methods in the spectroscopic toolbox, such as standard population
synthesis techniques and Lick indices. When trained and tested on 64x64-pixel
images from the Sloan Digital Sky Survey, the global bimodality of star-forming
and quiescent galaxies in photometric space is recovered, as well as a
mass-metallicity relation of star-forming galaxies. The comparison between the
observed and the artificially created spectra shows good agreement in overall
metallicity, age, Dn4000, stellar velocity dispersion, and E(B-V) values.
Photometric redshift estimates of our generative algorithm can compete with
other current, specialized deep-learning techniques. Moreover, this work is the
first attempt in the literature to infer velocity dispersion from photometric
images. Additionally, we can predict the presence of an active galactic nucleus
up to an accuracy of 82%. With our method, scientifically interesting galaxy
properties, normally requiring spectroscopic inputs, can be obtained in future
data sets from large-scale photometric surveys alone. The spectra prediction
via AI can further assist in creating realistic mock catalogs.

摘要：現代光譜調查只能針對廣域調查中大量光度編目的來源的一小部分。在這裡，我們報告了一種生成式 AI 方法的開發，該方法能夠僅從光度寬頻影像預測光學星系光譜。此方法汲取了擴散模型與對比網路相結合的最新進展。我們將多波段星系影像傳遞到架構中以取得光學光譜。從這些光譜中，可以透過光譜工具箱中的任何方法導出星系屬性的穩健值，例如標準族群合成技術和 Lick 指數。當使用史隆數位天空調查的 64x64 像素影像進行訓練和測試時，光度空間中恆星形成和靜止星系的整體雙峰性得以恢復，以及恆星形成星系的質量金屬關係。觀測光譜和人工建立的光譜之間的比較顯示在整體金屬量、年齡、Dn4000、恆星速度離散度和 E(B-V) 值方面有良好的一致性。我們生成式演算法的光度紅移估計可以與其他當前、專門的深度學習技術競爭。此外，這項工作是文獻中首次嘗試從光度影像推斷速度離散度。此外，我們可以預測活躍星系核的存在，準確度高達 82%。透過我們的方法，通常需要光譜輸入的科學上有趣的星系特性，可以在未來的資料集中僅從大規模光度調查中獲得。透過 AI 進行光譜預測可以進一步協助建立逼真的模擬目錄。

##### **UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs**
2406.18173v1 by Wenhao Li, Mingbao Lin, Yunshan Zhong, Shuicheng Yan, Rongrong Ji

Managing long texts is challenging for large language models (LLMs) due to
limited context window sizes. This study introduces UIO-LLMs, an unbiased
incremental optimization approach for memory-enhanced transformers under
long-context settings. We initially conceptualize the process as a streamlined
encoder-decoder framework where the weights-shared encoder and decoder
respectively encapsulate a context segment into memories and leverage these
memories to predict outputs of the subsequent segment. Subsequently, by
treating our memory-enhanced transformers as fully-connected recurrent neural
networks (RNNs), we refine the training process using the Truncated
Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative
incremental optimization techniques. These techniques not only diminish time
complexity but also address the bias in gradient computation through an
unbiased optimization process. UIO-LLMs successfully handle long context, such
as extending the context window of Llama2-7b-chat from 4K to 100K tokens with
minimal 2% additional parameters, while keeping the inference cost nearly
linear as context length increases.

摘要：管理長文本對於大型語言模型 (LLM) 來說具有挑戰性，因為其內容視窗大小有限。本研究引入了 UIO-LLM，這是一種無偏的遞增最佳化方法，用於在長內容設定下記憶體增強的Transformer。我們最初將此過程概念化為簡化的編碼器-解碼器框架，其中權重共享編碼器和解碼器分別將內容區段封裝到記憶體中，並利用這些記憶體來預測後續區段的輸出。隨後，通過將我們記憶體增強的Transformer視為全連接遞迴神經網路 (RNN)，我們使用截斷時間反向傳播 (TBPTT) 演算法改進訓練過程，該演算法結合了創新的遞增最佳化技術。這些技術不僅降低了時間複雜度，還通過無偏最佳化過程解決了梯度計算中的偏差。UIO-LLM 成功處理長內容，例如將 Llama2-7b-chat 的內容視窗從 4K 延伸到 100K 個符號，額外參數最少增加 2%，同時隨著內容長度的增加，推理成本幾乎保持線性。

##### **NeBuLa: A discourse aware Minecraft Builder**
2406.18164v1 by Akshay Chaturvedi, Kate Thompson, Nicholas Asher

When engaging in collaborative tasks, humans efficiently exploit the semantic
structure of a conversation to optimize verbal and nonverbal interactions. But
in recent "language to code" or "language to action" models, this information
is lacking. We show how incorporating the prior discourse and nonlinguistic
context of a conversation situated in a nonlinguistic environment can improve
the "language to action" component of such interactions. We fine tune an LLM to
predict actions based on prior context; our model, NeBuLa, doubles the
net-action F1 score over the baseline on this task of Jayannavar et al.(2020).
We also investigate our model's ability to construct shapes and understand
location descriptions using a synthetic dataset.

摘要：在參與協作任務時，人類有效利用對話的語義結構來優化口頭和非口頭互動。但在最近的「語言到程式碼」或「語言到動作」模型中，卻缺乏此資訊。我們展示如何將置於非語言環境中的對話的先前對話和非語言脈絡納入，可以改善此類互動的「語言到動作」組成部分。我們微調 LLM 以根據先前脈絡預測動作；我們的模型 NeBuLa 在 Jayannavar 等人 (2020) 的此任務中，將網路動作 F1 分數加倍。我們也探討我們的模型建構形狀和使用合成資料集瞭解位置描述的能力。

##### **Innovating for Tomorrow: The Convergence of SE and Green AI**
2406.18142v1 by Luís Cruz, Xavier Franch Gutierrez, Silverio Martínez-Fernández

The latest advancements in machine learning, specifically in foundation
models, are revolutionizing the frontiers of existing software engineering (SE)
processes. This is a bi-directional phenomona, where 1) software systems are
now challenged to provide AI-enabled features to their users, and 2) AI is used
to automate tasks within the software development lifecycle. In an era where
sustainability is a pressing societal concern, our community needs to adopt a
long-term plan enabling a conscious transformation that aligns with
environmental sustainability values. In this paper, we reflect on the impact of
adopting environmentally friendly practices to create AI-enabled software
systems and make considerations on the environmental impact of using foundation
models for software development.

摘要：機器學習的最新進展，特別是基礎模型，正在革新現有軟體工程 (SE) 流程的疆界。這是一個雙向現象，其中 1) 軟體系統現在面臨為其使用者提供 AI 啟用功能的挑戰，以及 2) AI 用於自動化軟體開發生命週期中的任務。在永續性是迫切社會關注的時代，我們的社群需要採用長期計畫，讓有意識的轉型與環境永續價值觀保持一致。在本文中，我們反思採用對環境友善的實務來建立 AI 啟用軟體系統的影響，並考量使用基礎模型進行軟體開發對環境的影響。

##### **LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference**
2406.18139v1 by Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan

Long-context Multimodal Large Language Models (MLLMs) demand substantial
computational resources for inference as the growth of their multimodal
Key-Value (KV) cache, in response to increasing input lengths, challenges
memory and time efficiency. Unlike single-modality LLMs that manage only
textual contexts, the KV cache of long-context MLLMs includes representations
from multiple images with temporal and spatial relationships and related
textual contexts. The predominance of image tokens means traditional
optimizations for LLMs' KV caches are unsuitable for multimodal long-context
settings, and no prior works have addressed this challenge. In this work, we
introduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently
reduces the multimodal KV cache size while maintaining performance comparable
to a full cache. We observe that during prompt prefill, the model prioritizes
more textual attention over image features, and based on the multimodal
interaction observation, a new proposed text-prior method is explored to
compress the KV cache. Furthermore, to mitigate the degradation of image
contextual information, we propose several compensatory strategies using KV
pairs merging. LOOK-M demonstrates that with a significant reduction in KV
Cache memory usage, such as reducing it by 80% in some cases, it not only
achieves up to 1.5x faster decoding but also maintains or even enhances
performance across a variety of long context multimodal tasks.

摘要：長語境多模態大型語言模型 (MLLM) 對推理需要大量的計算資源，因為其多模態鍵值 (KV) 快取隨著輸入長度的增加而增長，對記憶體和時間效率構成挑戰。與僅管理文字語境的單模態 LLM 不同，長語境 MLLM 的 KV 快取包含具有時間和空間關係的多個影像表徵和相關文字語境。影像代碼的優勢表示傳統 LLM KV 快取的最佳化不適用於多模態長語境設定，而且沒有先前的研究解決這個挑戰。在這項工作中，我們引入了 LOOK-M，一種先驅性的微調方法，它有效地減少了多模態 KV 快取大小，同時維持與完整快取相當的效能。我們觀察到在提示預填期間，模型優先於影像特徵對文字注意力，並且基於多模態互動觀察，探索了一個新的建議文字優先方法來壓縮 KV 快取。此外，為了減輕影像語境資訊的劣化，我們提出使用 KV 成對合併的幾個補償策略。LOOK-M 證明了透過大幅減少 KV 快取記憶體使用量，例如在某些情況下減少 80%，它不僅實現了快達 1.5 倍的解碼速度，而且在各種長語境多模態任務中維持甚至提升了效能。

##### **Automatic Speech Recognition for Hindi**
2406.18135v1 by Anish Saha, A. G. Ramakrishnan

Automatic speech recognition (ASR) is a key area in computational
linguistics, focusing on developing technologies that enable computers to
convert spoken language into text. This field combines linguistics and machine
learning. ASR models, which map speech audio to transcripts through supervised
learning, require handling real and unrestricted text. Text-to-speech systems
directly work with real text, while ASR systems rely on language models trained
on large text corpora. High-quality transcribed data is essential for training
predictive models. The research involved two main components: developing a web
application and designing a web interface for speech recognition. The web
application, created with JavaScript and Node.js, manages large volumes of
audio files and their transcriptions, facilitating collaborative human
correction of ASR transcripts. It operates in real-time using a client-server
architecture. The web interface for speech recognition records 16 kHz mono
audio from any device running the web app, performs voice activity detection
(VAD), and sends the audio to the recognition engine. VAD detects human speech
presence, aiding efficient speech processing and reducing unnecessary
processing during non-speech intervals, thus saving computation and network
bandwidth in VoIP applications. The final phase of the research tested a neural
network for accurately aligning the speech signal to hidden Markov model (HMM)
states. This included implementing a novel backpropagation method that utilizes
prior statistics of node co-activations.

摘要：自動語音辨識 (ASR) 是計算語言學中的關鍵領域，專注於開發技術，讓電腦能將口語轉換成文字。此領域結合了語言學和機器學習。ASR 模型透過監督式學習將語音音訊對應到文字檔，需要處理真實且不受限制的文字。文字轉語音系統直接處理真實文字，而 ASR 系統則仰賴訓練於大量文字語料庫的語言模型。高品質的轉錄資料對於訓練預測模型至關重要。此研究包含兩個主要組成部分：開發網路應用程式和設計語音辨識的網路介面。使用 JavaScript 和 Node.js 建立的網路應用程式管理大量音訊檔案及其轉錄，協助人類協作修正 ASR 轉錄。它使用客戶端伺服器架構進行即時運作。語音辨識的網路介面會從執行網路應用程式的任何裝置錄製 16 kHz 單聲道音訊，執行語音活動偵測 (VAD)，並將音訊傳送至辨識引擎。VAD 偵測人類語音的存在，協助有效率的語音處理，並減少非語音區間內不必要的處理，進而節省 VoIP 應用程式中的運算和網路頻寬。研究的最後階段測試了神經網路，以準確地將語音訊號與隱藏馬可夫模型 (HMM) 狀態對齊。這包括實作一種新穎的反向傳播方法，利用節點共激活的先驗統計資料。

##### **Assessing "Implicit" Retrieval Robustness of Large Language Models**
2406.18134v1 by Xiaoyu Shen, Rexhina Blloshmi, Dawei Zhu, Jiahuan Pei, Wei Zhang

Retrieval-augmented generation has gained popularity as a framework to
enhance large language models with external knowledge. However, its
effectiveness hinges on the retrieval robustness of the model. If the model
lacks retrieval robustness, its performance is constrained by the accuracy of
the retriever, resulting in significant compromises when the retrieved context
is irrelevant. In this paper, we evaluate the "implicit" retrieval robustness
of various large language models, instructing them to directly output the final
answer without explicitly judging the relevance of the retrieved context. Our
findings reveal that fine-tuning on a mix of gold and distracting context
significantly enhances the model's robustness to retrieval inaccuracies, while
still maintaining its ability to extract correct answers when retrieval is
accurate. This suggests that large language models can implicitly handle
relevant or irrelevant retrieved context by learning solely from the
supervision of the final answer in an end-to-end manner. Introducing an
additional process for explicit relevance judgment can be unnecessary and
disrupts the end-to-end approach.

摘要：檢索增強生成作為一種框架，已廣受歡迎，用於增強外部知識的大型語言模型。然而，它的有效性取決於模型的檢索穩健性。如果模型缺乏檢索穩健性，其效能會受到檢索器的準確性限制，當檢索的內容無關時，會造成顯著的折衷。在本文中，我們評估各種大型語言模型的「隱含」檢索穩健性，指示它們直接輸出最終答案，而不明確判斷檢索內容的相關性。我們的發現表明，在黃金和分散內容的組合上進行微調，可以顯著增強模型對檢索不準確性的穩健性，同時仍保持其在檢索準確時提取正確答案的能力。這表明大型語言模型可以通過僅從最終答案的監督中以端到端的方式學習，來隱含處理相關或無關的檢索內容。引入一個額外的顯式相關性判斷過程是不必要的，而且會破壞端到端方法。

##### **ConvoCache: Smart Re-Use of Chatbot Responses**
2406.18133v1 by Conor Atkins, Ian Wood, Mohamed Ali Kaafar, Hassan Asghar, Nardine Basta, Michal Kepkowski

We present ConvoCache, a conversational caching system that solves the
problem of slow and expensive generative AI models in spoken chatbots.
ConvoCache finds a semantically similar prompt in the past and reuses the
response. In this paper we evaluate ConvoCache on the DailyDialog dataset. We
find that ConvoCache can apply a UniEval coherence threshold of 90% and respond
to 89% of prompts using the cache with an average latency of 214ms, replacing
LLM and voice synthesis that can take over 1s. To further reduce latency we
test prefetching and find limited usefulness. Prefetching with 80% of a request
leads to a 63% hit rate, and a drop in overall coherence. ConvoCache can be
used with any chatbot to reduce costs by reducing usage of generative AI by up
to 89%.

摘要：我們提出 ConvoCache，一種對話式快取系統，可解決對話式聊天機器人中生成式 AI 模型緩慢且昂貴的問題。
ConvoCache 在過去找到語義上類似的提示，並重複使用回應。在本文中，我們在 DailyDialog 資料集上評估 ConvoCache。我們發現 ConvoCache 可以應用 90% 的 UniEval 相干性閾值，並使用快取回應 89% 的提示，平均延遲時間為 214 毫秒，取代可能需要 1 秒以上的 LLM 和語音合成。為了進一步降低延遲時間，我們測試了預讀取，並發現其用途有限。使用 80% 的請求進行預讀取會導致 63% 的命中率，以及整體相干性的下降。ConvoCache 可與任何聊天機器人一起使用，透過將生成式 AI 的使用量減少多達 89% 來降低成本。

##### **ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets and Large Language Models**
2406.18125v1 by Ahmed Heakl, Youssef Mohamed, Noran Mohamed, Ali Sharkaway, Ahmed Zaky

The increasing reliance on online recruitment platforms coupled with the
adoption of AI technologies has highlighted the critical need for efficient
resume classification methods. However, challenges such as small datasets, lack
of standardized resume templates, and privacy concerns hinder the accuracy and
effectiveness of existing classification models. In this work, we address these
challenges by presenting a comprehensive approach to resume classification. We
curated a large-scale dataset of 13,389 resumes from diverse sources and
employed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for
classification. Our results demonstrate significant improvements over
traditional machine learning approaches, with our best model achieving a top-1
accuracy of 92\% and a top-5 accuracy of 97.5\%. These findings underscore the
importance of dataset quality and advanced model architectures in enhancing the
accuracy and robustness of resume classification systems, thus advancing the
field of online recruitment practices.

摘要：隨著線上徵才平台的倚賴度日益增加，加上人工智慧技術的採用，凸顯了對於有效履歷分類方法的迫切需求。然而，諸如資料集小、缺乏標準化的履歷範本，以及隱私問題等挑戰，阻礙了現有分類模型的準確性和有效性。在這項工作中，我們透過提出履歷分類的全面性方法來因應這些挑戰。我們從不同的來源整理出一個包含 13,389 份履歷的大規模資料集，並採用大型語言模型 (LLM)，例如 BERT 和 Gemma1.1 2B，進行分類。我們的結果顯示，與傳統機器學習方法相比，有顯著的進步，我們的最佳模型在 top-1 準確率達到 92%，top-5 準確率達到 97.5%。這些發現強調了資料集品質和進階模型架構在提升履歷分類系統的準確性和穩健性方面的重要性，進而促進線上徵才實務領域的發展。

##### **Poisoned LangChain: Jailbreak LLMs by LangChain**
2406.18122v1 by Ziqiu Wang, Jun Liu, Shengkai Zhang, Yang Yang

With the development of natural language processing (NLP), large language
models (LLMs) are becoming increasingly popular. LLMs are integrating more into
everyday life, raising public concerns about their security vulnerabilities.
Consequently, the security of large language models is becoming critically
important. Currently, the techniques for attacking and defending against LLMs
are continuously evolving. One significant method type of attack is the
jailbreak attack, which designed to evade model safety mechanisms and induce
the generation of inappropriate content. Existing jailbreak attacks primarily
rely on crafting inducement prompts for direct jailbreaks, which are less
effective against large models with robust filtering and high comprehension
abilities. Given the increasing demand for real-time capabilities in large
language models, real-time updates and iterations of new knowledge have become
essential. Retrieval-Augmented Generation (RAG), an advanced technique to
compensate for the model's lack of new knowledge, is gradually becoming
mainstream. As RAG enables the model to utilize external knowledge bases, it
provides a new avenue for jailbreak attacks.
  In this paper, we conduct the first work to propose the concept of indirect
jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on
this, we further design a novel method of indirect jailbreak attack, termed
Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to
interact with large language models, thereby causing the large models to
generate malicious non-compliant dialogues.We tested this method on six
different large language models across three major categories of jailbreak
issues. The experiments demonstrate that PLC successfully implemented indirect
jailbreak attacks under three different scenarios, achieving success rates of
88.56%, 79.04%, and 82.69% respectively.

摘要：<paragraph>隨著自然語言處理 (NLP) 的發展，大型語言模型 (LLM) 變得越來越流行。LLM 正逐漸融入日常生活，引發公眾對其安全漏洞的擔憂。因此，大型語言模型的安全性變得至關重要。目前，攻擊和防禦 LLM 的技術不斷發展。一種重要的攻擊類型是越獄攻擊，旨在規避模型安全機制並誘發不當內容的產生。現有的越獄攻擊主要依賴於為直接越獄製作誘導提示，對於具有強大過濾和高理解能力的大型模型，其效果較差。由於對大型語言模型中實時功能的需求不斷增加，實時更新和新知識的迭代已變得至關重要。檢索增強生成 (RAG) 是一種先進的技術，用於彌補模型缺乏新知識的缺陷，逐漸成為主流。由於 RAG 使模型能夠利用外部知識庫，因此為越獄攻擊提供了新途徑。在本文中，我們進行了首次工作，提出了間接越獄的概念，並通過 LangChain 实现了檢索增強生成。在此基礎上，我們進一步設計了一種新穎的間接越獄攻擊方法，稱為中毒 LangChain (PLC)，它利用中毒的外部知識庫與大型語言模型進行交互，從而導致大型模型生成惡意的不合規對話。我們在六種不同的、跨越三大類越獄問題的大型語言模型上對此方法進行了測試。實驗表明，PLC 在三種不同的場景下成功實現了間接越獄攻擊，分別達到了 88.56%、79.04% 和 82.69% 的成功率。</paragraph>

##### **ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech Recognition Using LLMs**
2406.18120v1 by Ahmed Heakl, Youssef Zaghloul, Mennatullah Ali, Rania Hossam, Walid Gomaa

Motivated by the widespread increase in the phenomenon of code-switching
between Egyptian Arabic and English in recent times, this paper explores the
intricacies of machine translation (MT) and automatic speech recognition (ASR)
systems, focusing on translating code-switched Egyptian Arabic-English to
either English or Egyptian Arabic. Our goal is to present the methodologies
employed in developing these systems, utilizing large language models such as
LLama and Gemma. In the field of ASR, we explore the utilization of the Whisper
model for code-switched Egyptian Arabic recognition, detailing our experimental
procedures including data preprocessing and training techniques. Through the
implementation of a consecutive speech-to-text translation system that
integrates ASR with MT, we aim to overcome challenges posed by limited
resources and the unique characteristics of the Egyptian Arabic dialect.
Evaluation against established metrics showcases promising results, with our
methodologies yielding a significant improvement of $56\%$ in English
translation over the state-of-the-art and $9.3\%$ in Arabic translation. Since
code-switching is deeply inherent in spoken languages, it is crucial that ASR
systems can effectively handle this phenomenon. This capability is crucial for
enabling seamless interaction in various domains, including business
negotiations, cultural exchanges, and academic discourse. Our models and code
are available as open-source resources. Code:
\url{http://github.com/ahmedheakl/arazn-llm}}, Models:
\url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}.

摘要：在埃及阿拉伯语和英语中代码切换现象最近广泛增加的推动下，本文探讨了机器翻译 (MT) 和自动语音识别 (ASR) 系统的复杂性，重点翻译代码切换的埃及阿拉伯语-英语到英语或埃及阿拉伯语。我们的目标是展示用于开发这些系统的，利用大型语言模型（例如 LLama 和 Gemma）的方法。在 ASR 领域，我们探索了 Whisper 模型在代码切换的埃及阿拉伯语识别中的利用，详细介绍了我们的实验程序，包括数据预处理和训练技术。通过实施将 ASR 与 MT 集成的连续语音到文本翻译系统，我们旨在克服有限资源和埃及阿拉伯语方言的独特特征带来的挑战。针对既定指标的评估展示了有希望的结果，我们的方法在英语翻译中比最先进的方法提高了 56%，在阿拉伯语翻译中提高了 9.3%。由于代码切换深深植根于口语中，因此 ASR 系统能够有效处理此现象至关重要。此功能对于在包括商务谈判、文化交流和学术讨论在内的各个领域实现无缝交互至关重要。我们的模型和代码可用作开源资源。代码：\url{http://github.com/ahmedheakl/arazn-llm}，模型：\url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}。

##### **SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance**
2406.18118v1 by Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang

As the development of large language models (LLMs) rapidly advances, securing
these models effectively without compromising their utility has become a
pivotal area of research. However, current defense strategies against jailbreak
attacks (i.e., efforts to bypass security protocols) often suffer from limited
adaptability, restricted general capability, and high cost. To address these
challenges, we introduce SafeAligner, a methodology implemented at the decoding
stage to fortify defenses against jailbreak attacks. We begin by developing two
specialized models: the Sentinel Model, which is trained to foster safety, and
the Intruder Model, designed to generate riskier responses. SafeAligner
leverages the disparity in security levels between the responses from these
models to differentiate between harmful and beneficial tokens, effectively
guiding the safety alignment by altering the output token distribution of the
target model. Extensive experiments show that SafeAligner can increase the
likelihood of beneficial tokens, while reducing the occurrence of harmful ones,
thereby ensuring secure alignment with minimal loss to generality.

摘要：隨著大型語言模型 (LLM) 的發展迅速進展，在不損害其效用的情況下有效保護這些模型已成為研究的關鍵領域。然而，目前針對越獄攻擊（即繞過安全協定的嘗試）的防禦策略通常會受到適應性有限、一般能力受限和成本高等問題的影響。為了應對這些挑戰，我們引入了 SafeAligner，這是一種在解碼階段實施的方法，用於加強對越獄攻擊的防禦。我們首先開發了兩個專用模型：Sentinel 模型（訓練用於促進安全性）和 Intruder 模型（設計用於產生較高風險的回應）。SafeAligner 利用這些模型的回應之間的安全性差異來區分有害和有益的代碼，有效地通過改變目標模型的輸出代碼分佈來引導安全性對齊。大量的實驗表明，SafeAligner 可以增加有益代碼的可能性，同時減少有害代碼的出現，從而確保安全的對齊，同時將對一般性的損失降至最低。

##### **BADGE: BADminton report Generation and Evaluation with LLM**
2406.18116v1 by Shang-Hsuan Chiang, Lin-Wei Chao, Kuang-Da Wang, Chih-Chuan Wang, Wen-Chih Peng

Badminton enjoys widespread popularity, and reports on matches generally
include details such as player names, game scores, and ball types, providing
audiences with a comprehensive view of the games. However, writing these
reports can be a time-consuming task. This challenge led us to explore whether
a Large Language Model (LLM) could automate the generation and evaluation of
badminton reports. We introduce a novel framework named BADGE, designed for
this purpose using LLM. Our method consists of two main phases: Report
Generation and Report Evaluation. Initially, badminton-related data is
processed by the LLM, which then generates a detailed report of the match. We
tested different Input Data Types, In-Context Learning (ICL), and LLM, finding
that GPT-4 performs best when using CSV data type and the Chain of Thought
prompting. Following report generation, the LLM evaluates and scores the
reports to assess their quality. Our comparisons between the scores evaluated
by GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.
Since the application of LLM in badminton reporting remains largely unexplored,
our research serves as a foundational step for future advancements in this
area. Moreover, our method can be extended to other sports games, thereby
enhancing sports promotion. For more details, please refer to
https://github.com/AndyChiangSH/BADGE.

摘要：羽球運動廣受歡迎，而比賽報導通常會包含球員姓名、比賽比分和球類等細節，為觀眾提供比賽的全面視角。然而，撰寫這些報導可能是一項耗時的任務。這個挑戰讓我們開始探討大型語言模型 (LLM) 是否可以自動生成和評估羽球比賽報導。我們介紹了一個名為 BADGE 的創新框架，專為使用 LLM 的這個目的而設計。我們的做法包含兩個主要階段：比賽報導生成與比賽報導評估。最初，羽球相關數據由 LLM 處理，然後生成比賽的詳細報導。我們測試了不同的輸入資料類型、情境內學習 (ICL) 和 LLM，發現 GPT-4 在使用 CSV 資料類型和思考鏈提示時表現最佳。在比賽報導生成後，LLM 會評估並評分比賽報導以評估其品質。我們在 GPT-4 和人類評審評估的分數之間的比較顯示出偏好 GPT-4 所生成比賽報導的趨勢。由於 LLM 在羽球報導中的應用仍然很大程度上未被探索，我們的研究作為這個領域未來進展的基礎步驟。此外，我們的方法可以延伸到其他運動比賽，從而加強體育推廣。更多詳情，請參閱 https://github.com/AndyChiangSH/BADGE。

##### **Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps**
2406.18115v1 by Dicong Qiu, Wenzong Ma, Zhenfu Pan, Hui Xiong, Junwei Liang

Open-Vocabulary Mobile Manipulation (OVMM) is a crucial capability for
autonomous robots, especially when faced with the challenges posed by unknown
and dynamic environments. This task requires robots to explore and build a
semantic understanding of their surroundings, generate feasible plans to
achieve manipulation goals, adapt to environmental changes, and comprehend
natural language instructions from humans. To address these challenges, we
propose a novel framework that leverages the zero-shot detection and grounded
recognition capabilities of pretraining visual-language models (VLMs) combined
with dense 3D entity reconstruction to build 3D semantic maps. Additionally, we
utilize large language models (LLMs) for spatial region abstraction and online
planning, incorporating human instructions and spatial semantic context. We
have built a 10-DoF mobile manipulation robotic platform JSR-1 and demonstrated
in real-world robot experiments that our proposed framework can effectively
capture spatial semantics and process natural language user instructions for
zero-shot OVMM tasks under dynamic environment settings, with an overall
navigation and task success rate of 80.95% and 73.33% over 105 episodes, and
better SFT and SPL by 157.18% and 19.53% respectively compared to the baseline.
Furthermore, the framework is capable of replanning towards the next most
probable candidate location based on the spatial semantic context derived from
the 3D semantic map when initial plans fail, keeping an average success rate of
76.67%.

摘要：開放詞彙行動操作 (OVMM) 是自主機器人的關鍵能力，尤其是在面對未知和動態環境所帶來的挑戰時。此任務需要機器人探索並建立其周圍環境的語義理解、產生可行的計畫以達成操作目標、適應環境變化，以及理解人類的自然語言指令。為了應對這些挑戰，我們提出一個新穎的架構，該架構利用預訓練視覺語言模型 (VLM) 的零次方偵測和紮根識別能力，結合密集的 3D 實體重建，以建立 3D 語義地圖。此外，我們利用大型語言模型 (LLM) 進行空間區域抽象化和線上規劃，並納入人類指令和空間語義背景。我們已經建立了一個 10-DoF 行動操作機器人平台 JSR-1，並在真實世界的機器人實驗中證明，我們提出的架構可以在動態環境設定下有效擷取空間語義和處理自然語言使用者指令，以進行零次方 OVMM 任務，在 105 個回合中的整體導航和任務成功率分別為 80.95% 和 73.33%，與基線相比，SFT 和 SPL 分別提高了 157.18% 和 19.53%。此外，當初始計畫失敗時，該架構能夠根據從 3D 語義地圖衍生的空間語義背景，重新計畫到下一個最可能的候選位置，保持平均成功率為 76.67%。

##### **Token-Weighted RNN-T for Learning from Flawed Data**
2406.18108v1 by Gil Keren, Wei Zhou, Ozlem Kalinli

ASR models are commonly trained with the cross-entropy criterion to increase
the probability of a target token sequence. While optimizing the probability of
all tokens in the target sequence is sensible, one may want to de-emphasize
tokens that reflect transcription errors. In this work, we propose a novel
token-weighted RNN-T criterion that augments the RNN-T objective with
token-specific weights. The new objective is used for mitigating accuracy loss
from transcriptions errors in the training data, which naturally appear in two
settings: pseudo-labeling and human annotation errors. Experiments results show
that using our method for semi-supervised learning with pseudo-labels leads to
a consistent accuracy improvement, up to 38% relative. We also analyze the
accuracy degradation resulting from different levels of WER in the reference
transcription, and show that token-weighted RNN-T is suitable for overcoming
this degradation, recovering 64%-99% of the accuracy loss.

摘要：ASR 模型通常使用交叉熵准则进行训练，以增加目标标记序列的概率。虽然优化目标序列中所有标记的概率是合理的，但人们可能希望弱化反映转录错误的标记。在这项工作中，我们提出了一种新颖的标记加权 RNN-T 准则，该准则使用标记特定权重来扩充 RNN-T 目标。新的目标用于缓解训练数据中转录错误造成的准确性损失，这自然出现在两种设置中：伪标签和人工注释错误。实验结果表明，使用我们的方法进行伪标签的半监督学习可带来一致的准确性提升，相对提升高达 38%。我们还分析了参考转录中不同 WER 级别导致的准确性下降，并表明标记加权 RNN-T 适用于克服这种下降，恢复 64%-99% 的准确性损失。

##### **Shimo Lab at "Discharge Me!": Discharge Summarization by Prompt-Driven Concatenation of Electronic Health Record Sections**
2406.18094v1 by Yunzhen He, Hiroaki Yamagiwa, Hidetoshi Shimodaira

In this paper, we present our approach to the shared task "Discharge Me!" at
the BioNLP Workshop 2024. The primary goal of this task is to reduce the time
and effort clinicians spend on writing detailed notes in the electronic health
record (EHR). Participants develop a pipeline to generate the "Brief Hospital
Course" and "Discharge Instructions" sections from the EHR. Our approach
involves a first step of extracting the relevant sections from the EHR. We then
add explanatory prompts to these sections and concatenate them with separate
tokens to create the input text. To train a text generation model, we perform
LoRA fine-tuning on the ClinicalT5-large model. On the final test data, our
approach achieved a ROUGE-1 score of $0.394$, which is comparable to the top
solutions.

摘要：在本文中，我們提出我們對 BioNLP 研討會 2024 中共享任務「讓我出院！」的方法。此任務的主要目標是減少臨床醫生在電子健康記錄 (EHR) 中撰寫詳細筆記所花費的時間和精力。參與者開發一條管道，從 EHR 中產生「簡短住院病程」和「出院指示」部分。我們的方法涉及從 EHR 中提取相關部分的第一個步驟。然後，我們向這些部分添加說明性提示，並將它們與單獨的代幣連接起來以創建輸入文本。為了訓練文本生成模型，我們對 ClinicalT5-large 模型執行 LoRA 微調。在最終測試數據中，我們的方法達到了 0.394 的 ROUGE-1 分數，這與頂尖的解決方案相當。

##### **LLM-Driven Multimodal Opinion Expression Identification**
2406.18088v1 by Bonian Jia, Huiyao Chen, Yueheng Sun, Meishan Zhang, Min Zhang

Opinion Expression Identification (OEI) is essential in NLP for applications
ranging from voice assistants to depression diagnosis. This study extends OEI
to encompass multimodal inputs, underlining the significance of auditory cues
in delivering emotional subtleties beyond the capabilities of text. We
introduce a novel multimodal OEI (MOEI) task, integrating text and speech to
mirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we
construct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is
applied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template
for the OEI task to take full advantage of the generative power of large
language models (LLMs). Advancing further, we propose an LLM-driven method
STOEI, which combines speech and text modal to identify opinion expressions.
Our experiments demonstrate that MOEI significantly improves the performance
while our method outperforms existing methods by 9.20\% and obtains SOTA
results.

摘要：意見表達識別 (OEI) 在自然語言處理 (NLP) 中對於從語音助理到憂鬱症診斷等應用至關重要。本研究將 OEI 擴展到涵蓋多模態輸入，強調聽覺線索在傳達超出文本能力的情緒細微差別方面的重要性。我們引入了一項新穎的多模態 OEI (MOEI) 任務，整合文本和語音以反映真實世界的場景。利用 CMU MOSEI 和 IEMOCAP 資料集，我們建構了 CI-MOEI 資料集。此外，將文字轉語音 (TTS) 技術應用於 MPQA 資料集以取得 CIM-OEI 資料集。我們設計了 OEI 任務的範本，以充分利用大型語言模型 (LLM) 的生成能力。更進一步，我們提出了一種 LLM 驅動的方法 STOEI，它結合語音和文本模式來識別意見表達。我們的實驗證明，MOEI 大幅提升了效能，而我們的模型則比現有方法高出 9.20%，並取得 SOTA 結果。

##### **EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction Using Large Language Multimodal Models**
2406.18087v1 by Chun-Chieh Liao, Wei-Ting Kuo, I-Hsuan Hu, Yen-Chen Shih, Jun-En Ding, Feng Liu, Fang-Ming Hung

Traditional diagnosis of chronic diseases involves in-person consultations
with physicians to identify the disease. However, there is a lack of research
focused on predicting and developing application systems using clinical notes
and blood test values. We collected five years of Electronic Health Records
(EHRs) from Taiwan's hospital database between 2017 and 2021 as an AI database.
Furthermore, we developed an EHR-based chronic disease prediction platform
utilizing Large Language Multimodal Models (LLMMs), successfully integrating
with frontend web and mobile applications for prediction. This prediction
platform can also connect to the hospital's backend database, providing
physicians with real-time risk assessment diagnostics. The demonstration link
can be found at https://www.youtube.com/watch?v=oqmL9DEDFgA.

摘要：傳統慢性病的診斷涉及親自諮詢醫師以找出疾病。然而，缺乏針對使用臨床筆記和血液檢驗值來預測和開發應用系統的研究。我們從2017年到2021年間收集了台灣醫院資料庫中五年的電子健康記錄（EHR）作為AI資料庫。此外，我們開發了一個基於EHR的慢性病預測平台，利用大型語言多模態模型（LLMM），成功整合前端網路和行動應用程式進行預測。這個預測平台也可以連接到醫院的後端資料庫，為醫師提供即時風險評估診斷。示範連結可以在https://www.youtube.com/watch?v=oqmL9DEDFgA找到。

##### **Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints**
2406.18085v1 by Ran Song, Shizhu He, Shengxiang Gao, Li Cai, Kang Liu, Zhengtao Yu, Jun Zhao

Multilingual Knowledge Graph Completion (mKGC) aim at solving queries like
(h, r, ?) in different languages by reasoning a tail entity t thus improving
multilingual knowledge graphs. Previous studies leverage multilingual
pretrained language models (PLMs) and the generative paradigm to achieve mKGC.
Although multilingual pretrained language models contain extensive knowledge of
different languages, its pretraining tasks cannot be directly aligned with the
mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit
a pronounced English-centric bias. This makes it difficult for mKGC to achieve
good results, particularly in the context of low-resource languages. To
overcome previous problems, this paper introduces global and local knowledge
constraints for mKGC. The former is used to constrain the reasoning of answer
entities, while the latter is used to enhance the representation of query
contexts. The proposed method makes the pretrained model better adapt to the
mKGC task. Experimental results on public datasets demonstrate that our method
outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32% and
16.03%, which indicates that our proposed method has significant enhancement on
mKGC.

摘要：多語言知識圖譜完成 (mKGC) 旨在透過推理尾部實體 t 來解決不同語言中的查詢，例如 (h, r, ?)，進而改善多語言知識圖譜。先前的研究利用多語言預訓練語言模型 (PLM) 和生成範例來達成 mKGC。儘管多語言預訓練語言模型包含不同語言的廣泛知識，但其預訓練任務無法直接與 mKGC 任務對齊。此外，目前大多數的知識圖譜和 PLM 都展現出明顯的英語中心偏誤。這使得 mKGC 難以達成良好的結果，特別是在低資源語言的脈絡中。為了克服先前的問題，本文針對 mKGC 引入了全域與局部知識限制。前者用於限制答案實體的推理，而後者用於加強查詢脈絡的表示。所提出的方法使得預訓練模型能更好地適應 mKGC 任務。公開資料集上的實驗結果顯示，我們的模型在 Hits@1 和 Hits@10 上平均優於先前的 SOTA 12.32% 和 16.03%，這表示我們提出的方法顯著地增強了 mKGC。

##### **Octo-planner: On-device Language Model for Planner-Action Agents**
2406.18082v1 by Wei Chen, Zhiyuan Li, Zhen Guo, Yikang Shen

AI agents have become increasingly significant in various domains, enabling
autonomous decision-making and problem-solving. To function effectively, these
agents require a planning process that determines the best course of action and
then executes the planned actions. In this paper, we present an efficient
on-device Planner-Action framework that separates planning and action execution
into two distinct components: a planner agent based on Phi-3 Mini, a 3.8
billion parameter LLM optimized for edge devices, and an action agent using the
Octopus model for function execution. The planner agent first responds to user
queries by decomposing tasks into a sequence of sub-steps, which are then
executed by the action agent. To optimize performance on resource-constrained
devices, we employ model fine-tuning instead of in-context learning, reducing
computational costs and energy consumption while improving response times. Our
approach involves using GPT-4 to generate diverse planning queries and
responses based on available functions, with subsequent validations to ensure
data quality. We fine-tune the Phi-3 Mini model on this curated dataset,
achieving a 97\% success rate in our in-domain test environment. To address
multi-domain planning challenges, we developed a multi-LoRA training method
that merges weights from LoRAs trained on distinct function subsets. This
approach enables flexible handling of complex, multi-domain queries while
maintaining computational efficiency on resource-constrained devices. To
support further research, we have open-sourced our model weights at
\url{https://huggingface.co/NexaAIDev/octopus-planning}. For the demo, please
refer to \url{https://www.nexa4ai.com/octo-planner}.

摘要：<paragraph>AI 代理在各種領域中變得越來越重要，能進行自主決策和問題解決。為了有效運作，這些代理需要一個規劃流程，用來決定最佳行動方案，然後執行已規劃的行動。在本文中，我們提出了一個高效的裝置內規劃器行動架構，將規劃和行動執行分為兩個不同的組成部分：一個基於 Phi-3 Mini 的規劃器代理，一個針對邊緣裝置最佳化的 38 億個參數 LLM，以及一個使用 Octopus 模型進行功能執行的動作代理。規劃器代理會先回應使用者的查詢，將任務分解成一系列子步驟，然後再由行動代理執行。為了在資源受限的裝置上最佳化效能，我們採用模型微調，而不是情境內學習，這樣可以減少運算成本和能源消耗，同時改善回應時間。我們的做法包括使用 GPT-4 根據可用的功能產生多樣的規劃查詢和回應，並進行後續驗證以確保資料品質。我們針對這個精選的資料集微調 Phi-3 Mini 模型，在我們的領域內測試環境中達到了 97% 的成功率。為了應對多領域規劃挑戰，我們開發了一種多 LoRA 訓練方法，可以合併在不同功能子集上訓練的 LoRA 的權重。這種方法可以在資源受限的裝置上維持運算效率的同時，靈活處理複雜的多領域查詢。為了支持進一步的研究，我們已在 \url{https://huggingface.co/NexaAIDev/octopus-planning} 開源了我們的模型權重。有關示範，請參閱 \url{https://www.nexa4ai.com/octo-planner}。</paragraph>

##### **Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction**
2406.18078v1 by Yice Zhang, Jie Zeng, Weiming Hu, Ziyi Wang, Shiwei Chen, Ruifeng Xu

Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect
term, aspect category, opinion term, sentiment polarity) for a given review,
which is the most representative and challenging task in aspect-based sentiment
analysis. A key challenge in the ASQP task is the scarcity of labeled data,
which limits the performance of existing methods. To tackle this issue, we
propose a self-training framework with a pseudo-label scorer, wherein a scorer
assesses the match between reviews and their pseudo-labels, aiming to filter
out mismatches and thereby enhance the effectiveness of self-training. We
highlight two critical aspects to ensure the scorer's effectiveness and
reliability: the quality of the training dataset and its model architecture. To
this end, we create a human-annotated comparison dataset and train a generative
model on it using ranking-based objectives. Extensive experiments on public
ASQP datasets reveal that using our scorer can greatly and consistently improve
the effectiveness of self-training. Moreover, we explore the possibility of
replacing humans with large language models for comparison dataset annotation,
and experiments demonstrate its feasibility. We release our code and data at
https://github.com/HITSZ-HLT/ST-w-Scorer-ABSA .

摘要：面向方面的情感四元組預測 (ASQP) 旨在預測給定評論的所有四元組（面向術語、面向類別、觀點術語、情感極性），這是面向方面的情感分析中最具代表性和挑戰性的任務。ASQP 任務中的關鍵挑戰是標籤數據的稀缺性，這限制了現有方法的性能。為了解決這個問題，我們提出了一個帶有偽標籤評分器的自訓練框架，其中評分器評估評論及其偽標籤之間的匹配，旨在過濾不匹配，從而增強自訓練的有效性。我們強調了兩個關鍵方面，以確保評分器的有效性和可靠性：訓練數據集的質量及其模型架構。為此，我們創建了一個人工標註的比較數據集，並使用基於排名的目標在其上訓練一個生成模型。在公共 ASQP 數據集上的大量實驗表明，使用我們的評分器可以極大地且持續地提高自訓練的有效性。此外，我們探索了用於比較數據集標註的大型語言模型替換人工的可能性，實驗證明了其可行性。我們在 https://github.com/HITSZ-HLT/ST-w-Scorer-ABSA 上發布我們的代碼和數據。

##### **Few-Shot Medical Image Segmentation with High-Fidelity Prototypes**
2406.18074v1 by Song Tang, Shaxu Yan, Xiaozhi Qi, Jianxin Gao, Mao Ye, Jianwei Zhang, Xiatian Zhu

Few-shot Semantic Segmentation (FSS) aims to adapt a pretrained model to new
classes with as few as a single labelled training sample per class. Despite the
prototype based approaches have achieved substantial success, existing models
are limited to the imaging scenarios with considerably distinct objects and not
highly complex background, e.g., natural images. This makes such models
suboptimal for medical imaging with both conditions invalid. To address this
problem, we propose a novel Detail Self-refined Prototype Network (DSPNet) to
constructing high-fidelity prototypes representing the object foreground and
the background more comprehensively. Specifically, to construct global
semantics while maintaining the captured detail semantics, we learn the
foreground prototypes by modelling the multi-modal structures with clustering
and then fusing each in a channel-wise manner. Considering that the background
often has no apparent semantic relation in the spatial dimensions, we integrate
channel-specific structural information under sparse channel-aware regulation.
Extensive experiments on three challenging medical image benchmarks show the
superiority of DSPNet over previous state-of-the-art methods.

摘要：少样本语义分割 (FSS) 旨在以每类仅一个标记训练样本的方式将预训练模型调整到新类。尽管基于原型的办法已取得重大成功，但现有模型仅限于对象明显不同且背景不太复杂的成像场景，例如自然图像。这使得此类模型不适用于同时不满足这两个条件的医学影像。为了解决这个问题，我们提出了一种新颖的细节自精炼原型网络 (DSPNet)，以构建高保真原型，更全面地表示对象前景和背景。具体来说，为了在保持捕获的细节语义的同时构建全局语义，我们通过使用聚类对多模态结构进行建模，然后以逐通道的方式融合每个结构，从而学习前景原型。考虑到背景在空间维度上通常没有明显的语义关系，我们在稀疏通道感知调节下整合特定于通道的结构信息。在三个极具挑战性的医学影像基准上进行的广泛实验表明，DSPNet 优于以前最先进的方法。

##### **Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals**
2406.18069v1 by Zengding Liu, Chen Chen, Jiannong Cao, Minglei Pan, Jikui Liu, Nan Li, Fen Miao, Ye Li

Large language models (LLMs) have captured significant interest from both
academia and industry due to their impressive performance across various
textual tasks. However, the potential of LLMs to analyze physiological
time-series data remains an emerging research field. Particularly, there is a
notable gap in the utilization of LLMs for analyzing wearable biosignals to
achieve cuffless blood pressure (BP) measurement, which is critical for the
management of cardiovascular diseases. This paper presents the first work to
explore the capacity of LLMs to perform cuffless BP estimation based on
wearable biosignals. We extracted physiological features from electrocardiogram
(ECG) and photoplethysmogram (PPG) signals and designed context-enhanced
prompts by combining these features with BP domain knowledge and user
information. Subsequently, we adapted LLMs to BP estimation tasks through
instruction tuning. To evaluate the proposed approach, we conducted assessments
of ten advanced LLMs using a comprehensive public dataset of wearable
biosignals from 1,272 participants. The experimental results demonstrate that
the optimally fine-tuned LLM significantly surpasses conventional task-specific
baselines, achieving an estimation error of 0.00 $\pm$ 9.25 mmHg for systolic
BP and 1.29 $\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies
highlight the benefits of our context enhancement strategy, leading to an 8.9%
reduction in mean absolute error for systolic BP estimation. This paper
pioneers the exploration of LLMs for cuffless BP measurement, providing a
potential solution to enhance the accuracy of cuffless BP measurement.

摘要：大型語言模型 (LLM) 由於其在各種文本任務中的出色表現，引起了學術界和產業界的濃厚興趣。然而，LLM 分析生理時序數據的潛力仍然是一個新興的研究領域。特別是，在利用 LLM 分析可穿戴生物信號以實現無袖套血壓 (BP) 測量方面存在顯著差距，這對於心血管疾病的管理至關重要。本文提出了第一項工作，探討 LLM 基於可穿戴生物信號執行無袖套血壓估計的能力。我們從心電圖 (ECG) 和光電容積描記法 (PPG) 信號中提取生理特徵，並通過將這些特徵與血壓領域知識和用戶信息相結合來設計上下文增強提示。隨後，我們通過指令調整將 LLM 適應於血壓估計任務。為了評估所提出的方法，我們使用來自 1,272 名參與者的可穿戴生物信號的綜合公共數據集，對十個先進的 LLM 進行了評估。實驗結果表明，經過最佳微調的 LLM 明顯優於傳統的特定任務基準，收縮壓的估計誤差為 0.00 ± 9.25 mmHg，舒張壓的估計誤差為 1.29 ± 6.37 mmHg。值得注意的是，消融研究突出了我們上下文增強策略的好處，使收縮壓估計的平均絕對誤差降低了 8.9%。本文開創了探索 LLM 進行無袖套血壓測量的先河，為提高無袖套血壓測量的準確性提供了一個潛在的解決方案。

##### **Exploring Energy-Based Models for Out-of-Distribution Detection in Dialect Identification**
2406.18067v1 by Yaqian Hao, Chenguang Hu, Yingying Gao, Shilei Zhang, Junlan Feng

The diverse nature of dialects presents challenges for models trained on
specific linguistic patterns, rendering them susceptible to errors when
confronted with unseen or out-of-distribution (OOD) data. This study introduces
a novel margin-enhanced joint energy model (MEJEM) tailored specifically for
OOD detection in dialects. By integrating a generative model and the energy
margin loss, our approach aims to enhance the robustness of dialect
identification systems. Furthermore, we explore two OOD scores for OOD dialect
detection, and our findings conclusively demonstrate that the energy score
outperforms the softmax score. Leveraging Sharpness-Aware Minimization to
optimize the training process of the joint model, we enhance model
generalization by minimizing both loss and sharpness. Experiments conducted on
dialect identification tasks validate the efficacy of Energy-Based Models and
provide valuable insights into their performance.

摘要：方言的多樣性對訓練特定語言模式的模型構成挑戰，使它們在面對未見或分佈外 (OOD) 資料時容易出現錯誤。本研究介紹了一種新的邊界增強聯合能量模型 (MEJEM)，專門針對方言中的 OOD 偵測而設計。透過整合生成模型和能量邊界損失，我們的做法旨在增強方言辨識系統的穩健性。此外，我們探討了兩個用於 OOD 方言偵測的 OOD 分數，我們的研究結果明確證明能量分數優於 softmax 分數。利用銳利度感知最小化來最佳化聯合模型的訓練過程，我們透過最小化損失和銳利度來增強模型的泛化能力。在方言辨識任務上進行的實驗驗證了基於能量的模型的效能，並提供了對其效能的寶貴見解。

##### **Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need**
2406.18064v1 by Yang Wang, Alberto Garcia Hernandez, Roman Kyslyi, Nicholas Kersting

We present a comprehensive evaluation of answer quality in
Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel
grading system that is designed to assess correctness, completeness, and
honesty. We further map the grading of quality aspects aforementioned into a
binary score, indicating an accept or reject decision, mirroring the intuitive
"thumbs-up" or "thumbs-down" gesture commonly used in chat applications. This
approach suits factual business settings where a clear decision opinion is
essential. Our assessment applies vRAG-Eval to two Large Language Models
(LLMs), evaluating the quality of answers generated by a vanilla RAG
application. We compare these evaluations with human expert judgments and find
a substantial alignment between GPT-4's assessments and those of human experts,
reaching 83% agreement on accept or reject decisions. This study highlights the
potential of LLMs as reliable evaluators in closed-domain, closed-ended
settings, particularly when human evaluations require significant resources.

摘要：我們提供了檢索增強生成 (RAG) 應用程式中答案品質的全面評估，使用 vRAG-Eval，這是一個新穎的評分系統，旨在評估正確性、完整性和誠實性。我們進一步將上述品質面向的評分映射到二元分數，表示接受或拒絕的決定，反映了聊天應用程式中常用的直觀「讚」或「倒讚」手勢。此方法適用於明確的決策意見至關重要的實際業務設定。我們的評估將 vRAG-Eval 應用於兩個大型語言模型 (LLM)，評估由香草 RAG 應用程式產生的答案品質。我們將這些評估與人類專家判斷進行比較，發現 GPT-4 的評估與人類專家之間有實質性的對齊，在接受或拒絕的決定上達到 83% 的一致性。這項研究突顯了 LLM 在封閉領域、封閉式設定中作為可靠評估者的潛力，特別是在人類評估需要大量資源時。

##### **AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning**
2406.18060v1 by Yifan Yang, Kai Zhen, Ershad Banijamal, Athanasios Mouchtaris, Zheng Zhang

Fine-tuning large language models (LLMs) has achieved remarkable performance
across various natural language processing tasks, yet it demands more and more
memory as model sizes keep growing. To address this issue, the recently
proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs
using only forward passes, thereby avoiding the need for a backpropagation
graph. However, significant performance drops and a high risk of divergence
have limited their widespread adoption. In this paper, we propose the Adaptive
Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed
to improve the performance and convergence of the ZO methods. To enhance
dimension-dependent ZO estimation accuracy, we introduce a fast-forward,
low-parameter tensorized adapter. To tackle the frequently observed divergence
issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number
schedule that guarantees convergence. Detailed theoretical analysis and
extensive experimental results on Roberta-Large and Llama-2-7B models
substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory
efficiency, and convergence speed.

摘要：微调大型语言模型 (LLM) 在各种自然语言处理任务中取得了显著的性能，但随着模型规模的不断扩大，它对内存的需求也越来越大。为了解决这个问题，最近提出的内存高效零阶 (MeZO) 方法试图仅使用前向传递来微调 LLM，从而避免了对反向传播图的需求。然而，严重的性能下降和发散的高风险限制了它们的广泛采用。在本文中，我们提出了自适应零阶张量训练自适应 (AdaZeta) 框架，专门设计用于提高 ZO 方法的性能和收敛性。为了增强维度相关的 ZO 估计精度，我们引入了一个快速前向、低参数张量化适配器。为了解决在大规模 ZO 微调任务中经常观察到的发散问题，我们提出了一个自适应查询数量计划，以保证收敛性。对 Roberta-Large 和 Llama-2-7B 模型的详细理论分析和广泛的实验结果证明了我们的 AdaZeta 框架在准确性、内存效率和收敛速度方面的有效性。

##### **Improving Entity Recognition Using Ensembles of Deep Learning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple Sources**
2406.18049v1 by Yiming Li, Deepthi Viswaroopan, William He, Jianfu Li, Xu Zuo, Hua Xu, Cui Tao

Adverse event (AE) extraction following COVID-19 vaccines from text data is
crucial for monitoring and analyzing the safety profiles of immunizations.
Traditional deep learning models are adept at learning intricate feature
representations and dependencies in sequential data, but often require
extensive labeled data. In contrast, large language models (LLMs) excel in
understanding contextual information, but exhibit unstable performance on named
entity recognition tasks, possibly due to their broad but unspecific training.
This study aims to evaluate the effectiveness of LLMs and traditional deep
learning models in AE extraction, and to assess the impact of ensembling these
models on performance. In this study, we utilized reports and posts from the
VAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal
was to extract three types of entities: "vaccine", "shot", and "ae". We
explored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5,
GPT-4, and Llama-2, as well as traditional deep learning models like RNN and
BioBERT. To enhance performance, we created ensembles of the three models with
the best performance. For evaluation, we used strict and relaxed F1 scores to
evaluate the performance for each entity type, and micro-average F1 was used to
assess the overall performance. The ensemble model achieved the highest
performance in "vaccine", "shot", and "ae" with strict F1-scores of 0.878,
0.930, and 0.925, respectively, along with a micro-average score of 0.903. In
conclusion, this study demonstrates the effectiveness and robustness of
ensembling fine-tuned traditional deep learning models and LLMs, for extracting
AE-related information. This study contributes to the advancement of biomedical
natural language processing, providing valuable insights into improving AE
extraction from text data for pharmacovigilance and public health surveillance.

摘要：從文本資料中擷取 COVID-19 疫苗的不良事件 (AE) 對於監控和分析免疫的安全性非常重要。傳統深度學習模型擅長學習序列資料中的複雜特徵表示和依賴關係，但通常需要大量的標籤資料。相比之下，大型語言模型 (LLM) 擅長理解上下文資訊，但在命名實體識別任務上的表現不穩定，這可能是因為它們的訓練範圍廣泛但缺乏針對性。本研究旨在評估 LLM 和傳統深度學習模型在 AE 擷取中的有效性，並評估將這些模型組成的影響。在本研究中，我們利用 VAERS (n=621)、Twitter (n=9,133) 和 Reddit (n=131) 的報告和文章作為語料庫。我們的目標是擷取三種類型的實體：「疫苗」、「注射」和「不良事件」。我們探索並微調了多個 LLM，包括 GPT-2、GPT-3.5、GPT-4 和 Llama-2，以及傳統深度學習模型，例如 RNN 和 BioBERT（GPT-4 除外）。為了提升效能，我們建立了表現最佳的三個模型的集合。在評估方面，我們使用嚴格和放寬的 F1 分數來評估每個實體類型的效能，並使用微平均 F1 來評估整體效能。組合模型在「疫苗」、「注射」和「不良事件」中分別以 0.878、0.930 和 0.925 的嚴格 F1 分數獲得最高效能，微平均分數為 0.903。結論是，本研究證明了微調後的傳統深度學習模型和 LLM 的集合在擷取與 AE 相關的資訊方面的有效性和穩健性。本研究有助於促進生物醫學自然語言處理，並提供有價值的見解，以改善藥物警戒和公共衛生監測中從文本資料中擷取 AE 的方式。

##### **PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical and Chemistry**
2406.18045v1 by Linqing Chen, Weilei Wang, Zilong Bai, Peng Xu, Yan Fang, Jie Fang, Wentao Wu, Lizhi Zhou, Ruiji Zhang, Yubin Xia, Chaobo Xu, Ran Hu, Licong Xu, Qijun Cai, Haoran Hua, Jing Sun, Jin Liu, Tian Qiu, Haowen Liu, Meng Hu, Xiuwen Li, Fei Gao, Yufu Wang, Lin Tie, Chaochao Wang, Jianping Lu, Cheng Sun, Yixin Wang, Shengjie Yang, Yuancheng Li, Lu Jin, Lisha Zhang, Fu Bian, Changyang Tu

Large language models (LLMs) have revolutionized Natural Language Processing
(NLP) by by minimizing the need for complex feature engineering. However, the
application of LLMs in specialized domains like biopharmaceuticals and
chemistry remains largely unexplored. These fields are characterized by
intricate terminologies, specialized knowledge, and a high demand for precision
areas where general purpose LLMs often fall short. In this study, we introduce
PharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion
parameters, specifically trained on a comprehensive corpus of hundreds of
billions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our
evaluation shows that PharmGPT matches or surpasses existing general models on
key benchmarks, such as NAPLEX, demonstrating its exceptional capability in
domain-specific tasks. This advancement establishes a new benchmark for LLMs in
the Bio-Pharmaceutical and Chemical fields, addressing the existing gap in
specialized language modeling. Furthermore, this suggests a promising path for
enhanced research and development in these specialized areas, paving the way
for more precise and effective applications of NLP in specialized domains.

摘要：大型語言模型 (LLM) 透過減少複雜特徵工程的需求，徹底改變了自然語言處理 (NLP)。然而，LLM 在生物製藥和化學等專業領域的應用仍未廣泛探討。這些領域的特點是術語繁雜、知識專業，且對精準度有很高的需求，而這正是通用 LLM 常無法達到的。在此研究中，我們推出了 PharmGPT，這是一種多語言 LLM 套件，具有 130 億和 700 億個參數，特別針對數百億個專門針對生物製藥和化學領域的語料庫進行訓練。我們的評估顯示，PharmGPT 在關鍵基準上（例如 NAPLEX）與現有的通用模型相匹配或超越它們，證明了其在特定領域任務中的出色能力。這項進展為生物製藥和化學領域的 LLM 建立了一個新的基準，解決了專業語言建模中現有的差距。此外，這也為這些專業領域的加強研究和開發指出一條有前景的道路，為 NLP 在專業領域中更精準、更有效的應用鋪路。

##### **Multimodal foundation world models for generalist embodied agents**
2406.18043v1 by Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, Sai Rajeswar

Learning generalist embodied agents, able to solve multitudes of tasks in
different domains is a long-standing problem. Reinforcement learning (RL) is
hard to scale up as it requires a complex reward design for each task. In
contrast, language can specify tasks in a more natural way. Current foundation
vision-language models (VLMs) generally require fine-tuning or other
adaptations to be functional, due to the significant domain gap. However, the
lack of multimodal data in such domains represents an obstacle toward
developing foundation models for embodied applications. In this work, we
overcome these problems by presenting multimodal foundation world models, able
to connect and align the representation of foundation VLMs with the latent
space of generative world models for RL, without any language annotations. The
resulting agent learning framework, GenRL, allows one to specify tasks through
vision and/or language prompts, ground them in the embodied domain's dynamics,
and learns the corresponding behaviors in imagination. As assessed through
large-scale multi-task benchmarking, GenRL exhibits strong multi-task
generalization performance in several locomotion and manipulation domains.
Furthermore, by introducing a data-free RL strategy, it lays the groundwork for
foundation model-based RL for generalist embodied agents.

摘要：學習通才具象代理，能夠解決不同領域的眾多任務是一個長久存在的問題。強化學習 (RL) 難以擴展，因為它需要針對每項任務設計複雜的獎勵。相比之下，語言可以用更自然的方式指定任務。由於存在顯著的領域差距，目前的基礎視覺語言模型 (VLM) 通常需要微調或其他適應才能發揮功能。然而，此類領域中缺乏多模態數據代表了開發具象應用基礎模型的障礙。在這項工作中，我們通過展示多模態基礎世界模型來克服這些問題，能夠將基礎 VLM 的表示與 RL 生成世界模型的潛在空間連接並對齊，而無需任何語言註解。由此產生的代理學習框架 GenRL 允許人們通過視覺和/或語言提示指定任務，將它們置於具象領域的動態中，並在想像中學習相應的行為。正如通過大規模多任務基準測試所評估的那樣，GenRL 在幾個運動和操作領域表現出強勁的多任務泛化性能。此外，通過引入無數據 RL 策略，它為通才具象代理的基於基礎模型的 RL 奠定了基礎。

##### **LLMs for Doctors: Leveraging Medical LLMs to Assist Doctors, Not Replace Them**
2406.18034v1 by Wenya Xie, Qingying Xiao, Yu Zheng, Xidong Wang, Junying Chen, Ke Ji, Anningzhe Gao, Xiang Wan, Feng Jiang, Benyou Wang

The recent success of Large Language Models (LLMs) has had a significant
impact on the healthcare field, providing patients with medical advice,
diagnostic information, and more. However, due to a lack of professional
medical knowledge, patients are easily misled by generated erroneous
information from LLMs, which may result in serious medical problems. To address
this issue, we focus on tuning the LLMs to be medical assistants who
collaborate with more experienced doctors. We first conduct a two-stage survey
by inspiration-feedback to gain a broad understanding of the real needs of
doctors for medical assistants. Based on this, we construct a Chinese medical
dataset called DoctorFLAN to support the entire workflow of doctors, which
includes 92K Q\&A samples from 22 tasks and 27 specialists. Moreover, we
evaluate LLMs in doctor-oriented scenarios by constructing the
DoctorFLAN-\textit{test} containing 550 single-turn Q\&A and DotaBench
containing 74 multi-turn conversations. The evaluation results indicate that
being a medical assistant still poses challenges for existing open-source
models, but DoctorFLAN can help them significantly. It demonstrates that the
doctor-oriented dataset and benchmarks we construct can complement existing
patient-oriented work and better promote medical LLMs research.

摘要：大型語言模型 (LLM) 近期的成功對醫療領域產生了重大影響，為患者提供醫療建議、診斷資訊等。然而，由於缺乏專業的醫療知識，患者很容易被 LLM 生成的錯誤資訊誤導，這可能會導致嚴重的醫療問題。為了解決這個問題，我們專注於調整 LLM，使其成為與經驗豐富的醫生合作的醫療助理。我們首先進行一項兩階段的調查，透過靈感回饋來廣泛了解醫生對醫療助理的實際需求。基於此，我們構建了一個名為 DoctorFLAN 的中文醫療資料集，以支援醫生的整個工作流程，其中包括來自 22 個任務和 27 位專家的 92K 問答範例。此外，我們透過構建包含 550 個單回合問答的 DoctorFLAN-\textit{test} 和包含 74 個多回合對話的 DotaBench，在面向醫生的場景中評估 LLM。評估結果表明，成為一名醫療助理對現有的開源模型來說仍然是一個挑戰，但 DoctorFLAN 可以為它們提供顯著的幫助。這證明了我們構建的面向醫生的資料集和基準可以補充現有的面向患者的工作，並更好地促進醫療 LLM 研究。

##### **Automated Clinical Data Extraction with Knowledge Conditioned LLMs**
2406.18027v1 by Diya Li, Asim Kadav, Aijing Gao, Rui Li, Richard Bourgon

The extraction of lung lesion information from clinical and medical imaging
reports is crucial for research on and clinical care of lung-related diseases.
Large language models (LLMs) can be effective at interpreting unstructured text
in reports, but they often hallucinate due to a lack of domain-specific
knowledge, leading to reduced accuracy and posing challenges for use in
clinical settings. To address this, we propose a novel framework that aligns
generated internal knowledge with external knowledge through in-context
learning (ICL). Our framework employs a retriever to identify relevant units of
internal or external knowledge and a grader to evaluate the truthfulness and
helpfulness of the retrieved internal-knowledge rules, to align and update the
knowledge bases. Our knowledge-conditioned approach also improves the accuracy
and reliability of LLM outputs by addressing the extraction task in two stages:
(i) lung lesion finding detection and primary structured field parsing,
followed by (ii) further parsing of lesion description text into additional
structured fields. Experiments with expert-curated test datasets demonstrate
that this ICL approach can increase the F1 score for key fields (lesion size,
margin and solidity) by an average of 12.9% over existing ICL methods.

摘要：從臨床和醫學影像報告中萃取肺部病灶資訊對於肺部相關疾病的研究和臨床照護至關重要。大型語言模型 (LLM) 可以有效解讀報告中的非結構化文字，但由於缺乏特定領域知識，它們經常會出現幻覺，導致準確度降低，並對在臨床環境中使用帶來挑戰。為了解決這個問題，我們提出了一個創新的框架，透過脈絡中學習 (ICL) 將產生的內部知識與外部知識對齊。我們的框架採用檢索器來識別內部或外部知識的相关單元，並採用評分器來評估檢索到的內部知識規則的真實性和有益性，以對齊和更新知識庫。我們以知識為條件的方法也透過以下兩個階段來處理萃取任務，進而提高 LLM 輸出的準確性和可靠性：(i) 肺部病灶發現偵測和主要結構化欄位分析，接著是 (ii) 進一步將病灶描述文字分析為其他結構化欄位。使用專家策展的測試資料集進行的實驗證明，這個 ICL 方法可以將關鍵欄位 (病灶大小、邊緣和實心度) 的 F1 分數平均提高 12.9%，優於現有的 ICL 方法。

##### **AutoOPE: Automated Off-Policy Estimator Selection**
2406.18022v1 by Nicolò Felicioni, Michael Benigni, Maurizio Ferrari Dacrema

The Off-Policy Evaluation (OPE) problem consists of evaluating the
performance of counterfactual policies with data collected by another one. This
problem is of utmost importance for various application domains, e.g.,
recommendation systems, medical treatments, and many others. To solve the OPE
problem, we resort to estimators, which aim to estimate in the most accurate
way possible the performance that the counterfactual policies would have had if
they were deployed in place of the logging policy. In the literature, several
estimators have been developed, all with different characteristics and
theoretical guarantees. Therefore, there is no dominant estimator, and each
estimator may be the best one for different OPE problems, depending on the
characteristics of the dataset at hand. While the selection of the estimator is
a crucial choice for an accurate OPE, this problem has been widely overlooked
in the literature. We propose an automated data-driven OPE estimator selection
method based on machine learning. In particular, the core idea we propose in
this paper is to create several synthetic OPE tasks and use a machine learning
model trained to predict the best estimator for those synthetic tasks. We
empirically show how our method is able to generalize to unseen tasks and make
a better estimator selection compared to a baseline method on several
real-world datasets, with a computational cost significantly lower than the one
of the baseline.

摘要：離線策略評估 (OPE) 問題包含使用由其他政策收集的資料評估反事實政策的效能。此問題對於各種應用領域至關重要，例如推薦系統、醫療治療等。為了解決 OPE 問題，我們求助於估計器，其目標是以最精確的方式估計反事實政策在部署於記錄政策時所具備的效能。在文獻中，已經開發出多個估計器，每個估計器都具有不同的特性和理論保證。因此，沒有主導估計器，每個估計器可能是不同 OPE 問題的最佳估計器，具體取決於手邊資料集的特徵。雖然估計器的選擇對於準確的 OPE 至關重要，但這個問題在文獻中已被廣泛忽視。我們提出一個基於機器學習的自動化資料驅動 OPE 估計器選擇方法。特別是，我們在這篇論文中提出的核心概念是建立幾個合成 OPE 任務，並使用機器學習模型訓練來預測這些合成任務的最佳估計器。我們以實證方式說明我們的模型如何能夠概化為未見任務，並在幾個真實世界資料集上與基線方法相比，做出更好的估計器選擇，且運算成本遠低於基線方法。

##### **Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher**
2406.18002v1 by Hyunjong Ok, Jegwang Ryu, Jaeho Lee

How can sLLMs efficiently utilize the supervision of LLMs to improve their
generative quality? This question has been well studied in scenarios where
there is no restriction on the number of LLM supervisions one can use, giving
birth to many decoding algorithms that utilize supervision without further
training. However, it is still unclear what is an effective strategy under the
limited supervision scenario, where we assume that no more than a few tokens
can be generated by LLMs. To this end, we develop an algorithm to effectively
aggregate the sLLM and LLM predictions on initial tokens so that the generated
tokens can more accurately condition the subsequent token generation by sLLM
only. Critically, we find that it is essential to adaptively overtrust or
disregard the LLM prediction based on the confidence of the sLLM. Through our
experiments on a wide range of models and datasets, we demonstrate that our
method provides a consistent improvement over conventional decoding strategies.

摘要：sLLM 如何有效利用 LLM 的監督，以提升其生成品質？這個問題在沒有 LLM 監督次數限制的情況下已被廣泛探討，並衍生出許多利用監督而無需進一步訓練的解碼演算法。然而，在有限監督的情況下，我們假設 LLM 最多只能產生幾個符號，而有效策略為何仍不清楚。為此，我們開發了一種演算法，可有效彙整 sLLM 和 LLM 對初始符號的預測，以便產生的符號能更準確地調整 sLLM 後續的符號產生。至關重要的是，我們發現基於 sLLM 的信心，適應性地過度信任或忽略 LLM 預測至關重要。透過在各種模型和資料集上進行實驗，我們證明了我們的方法相較於傳統的解碼策略，提供了穩定的改進。

##### **Catching Chameleons: Detecting Evolving Disinformation Generated using Large Language Models**
2406.17992v1 by Bohan Jiang, Chengshuai Zhao, Zhen Tan, Huan Liu

Despite recent advancements in detecting disinformation generated by large
language models (LLMs), current efforts overlook the ever-evolving nature of
this disinformation. In this work, we investigate a challenging yet practical
research problem of detecting evolving LLM-generated disinformation.
Disinformation evolves constantly through the rapid development of LLMs and
their variants. As a consequence, the detection model faces significant
challenges. First, it is inefficient to train separate models for each
disinformation generator. Second, the performance decreases in scenarios when
evolving LLM-generated disinformation is encountered in sequential order. To
address this problem, we propose DELD (Detecting Evolving LLM-generated
Disinformation), a parameter-efficient approach that jointly leverages the
general fact-checking capabilities of pre-trained language models (PLM) and the
independent disinformation generation characteristics of various LLMs. In
particular, the learned characteristics are concatenated sequentially to
facilitate knowledge accumulation and transformation. DELD addresses the issue
of label scarcity by integrating the semantic embeddings of disinformation with
trainable soft prompts to elicit model-specific knowledge. Our experiments show
that \textit{DELD} significantly outperforms state-of-the-art methods.
Moreover, our method provides critical insights into the unique patterns of
disinformation generation across different LLMs, offering valuable perspectives
in this line of research.

摘要：儘管在偵測大型語言模型 (LLM) 所產生的錯誤資訊方面有近期進展，但目前的努力忽視了此類錯誤資訊不斷演變的本質。在這項研究中，我們探討了偵測不斷演變的 LLM 所產生錯誤資訊的具挑戰性但實用的研究問題。錯誤資訊透過 LLM 及其變體的快速發展而持續演變。因此，偵測模型面臨了重大挑戰。首先，針對每個錯誤資訊產生器訓練個別模型效率不彰。其次，當按順序遇到不斷演變的 LLM 所產生的錯誤資訊時，效能會下降。為了解決此問題，我們提出了 DELD（偵測不斷演變的 LLM 所產生的錯誤資訊），這是一種參數效率方法，它結合了預先訓練的語言模型 (PLM) 的一般事實查核功能，以及各種 LLM 的獨立錯誤資訊產生特性。特別是，學習到的特性會依序串聯，以利於知識的累積與轉化。DELD 透過將錯誤資訊的語意嵌入與可訓練的軟提示整合，來解決標籤稀少的議題，以引發特定於模型的知識。我們的實驗顯示，\textit{DELD} 的表現顯著優於現有技術。此外，我們的技術提供了對不同 LLM 中錯誤資訊產生模式的關鍵見解，為此研究領域提供了寶貴的觀點。

##### **Explicit Diversity Conditions for Effective Question Answer Generation with Large Language Models**
2406.17990v1 by Vikas Yadav, Hyuk Joon Kwon, Vijay Srinivasan, Hongxia Jin

Question Answer Generation (QAG) is an effective data augmentation technique
to improve the accuracy of question answering systems, especially in
low-resource domains. While recent pretrained and large language model-based
QAG methods have made substantial progress, they face the critical issue of
redundant QA pair generation, affecting downstream QA systems. Implicit
diversity techniques such as sampling and diverse beam search are proven
effective solutions but often yield smaller diversity. We present explicit
diversity conditions for QAG, focusing on spatial aspects, question types, and
entities, substantially increasing diversity in QA generation. Our work
emphasizes the need of explicit diversity conditions for generating diverse
question-answer synthetic data by showing significant improvements in
downstream QA task over existing widely adopted implicit diversity techniques.
In particular, generated QA pairs from explicit diversity conditions when used
to train the downstream QA model results in an average 4.1% exact match and
4.5% F1 improvement over QAG from implicit sampling techniques on SQuADDU. Our
work emphasizes the need for explicit diversity conditions even more in
low-resource datasets (SubjQA), where average downstream QA performance
improvements are around 12% EM.

摘要：問題回答生成 (QAG) 是一種有效的資料擴充技術，用於提升問題回答系統的準確度，特別是在低資源領域。雖然最近預先訓練和基於大型語言模型的 QAG 方法已經取得了重大進展，但它們面臨著冗餘 QA 配對生成的重要問題，影響了下游 QA 系統。隱式多樣性技術（例如抽樣和多樣化波束搜尋）已被證明是有效的解決方案，但通常會產生較小的多樣性。我們針對 QAG 提出明確的多樣性條件，重點關注空間方面、問題類型和實體，大幅提升 QA 生成中的多樣性。我們的研究強調了為生成多樣化的問題回答合成資料而制定明確多樣性條件的必要性，方法是展示在現有廣泛採用的隱式多樣性技術上，下游 QA 任務有顯著的改進。特別是，當用於訓練下游 QA 模型時，來自明確多樣性條件的 QA 配對生成會產生平均 4.1% 的完全匹配和 4.5% 的 F1，優於 SQuADDU 上來自隱式抽樣技術的 QAG。我們的研究更強調了在低資源資料集 (SubjQA) 中對明確多樣性條件的需求，其中平均下游 QA 效能改善約為 12% EM。

##### **Multi-step Knowledge Retrieval and Inference over Unstructured Data**
2406.17987v1 by Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, CJ McFate, Lori Moon, Nati Seifu, Maksim Eremeev, Jose Barrera, Eric Brown, David Ferrucci

The advent of Large Language Models (LLMs) and Generative AI has
revolutionized natural language applications across various domains. However,
high-stakes decision-making tasks in fields such as medical, legal and finance
require a level of precision, comprehensiveness, and logical consistency that
pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to
deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI
platform to tackle these problems. The platform integrates fine-tuned LLMs for
knowledge extraction and alignment with a robust symbolic reasoning engine for
logical inference, planning and interactive constraint solving. We describe
Cora, a Collaborative Research Assistant built on this platform, that is
designed to perform complex research and discovery tasks in high-stakes
domains. This paper discusses the multi-step inference challenges inherent in
such domains, critiques the limitations of existing LLM-based methods, and
demonstrates how Cora's neuro-symbolic approach effectively addresses these
issues. We provide an overview of the system architecture, key algorithms for
knowledge extraction and formal reasoning, and present preliminary evaluation
results that highlight Cora's superior performance compared to well-known LLM
and RAG baselines.

摘要：大型語言模型 (LLM) 和生成式 AI 的出現徹底改變了各個領域的自然語言應用。然而，醫療、法律和金融等領域的高風險決策制定任務需要精確度、全面性和邏輯一致性，而純粹的 LLM 或檢索增強生成 (RAG) 方法通常無法提供。在 Elemental Cognition (EC)，我們開發了一個神經符號 AI 平台來解決這些問題。該平台整合了經過微調的 LLM，用於知識提取和與強大的符號推理引擎對齊，用於邏輯推理、規劃和互動約束求解。我們描述了 Cora，一個建立在這個平台上的協作研究助理，它被設計用於在高風險領域執行複雜的研究和發現任務。本文討論了此類領域中固有的多步驟推理挑戰，批評了現有基於 LLM 的方法的局限性，並展示了 Cora 的神經符號方法如何有效地解決這些問題。我們概述了系統架構、知識提取和形式推理的關鍵演算法，並提供了初步評估結果，突出了 Cora 與眾所周知的 LLM 和 RAG 基準相比的優異性能。

##### **EDEN: Empathetic Dialogues for English learning**
2406.17982v1 by Li Siyan, Teresa Shao, Zhou Yu, Julia Hirschberg

Dialogue systems have been used as conversation partners in English learning,
but few have studied whether these systems improve learning outcomes. Student
passion and perseverance, or grit, has been associated with language learning
success. Recent work establishes that as students perceive their English
teachers to be more supportive, their grit improves. Hypothesizing that the
same pattern applies to English-teaching chatbots, we create EDEN, a robust
open-domain chatbot for spoken conversation practice that provides empathetic
feedback. To construct EDEN, we first train a specialized spoken utterance
grammar correction model and a high-quality social chit-chat conversation
model. We then conduct a preliminary user study with a variety of strategies
for empathetic feedback. Our experiment suggests that using adaptive empathetic
feedback leads to higher perceived affective support, which, in turn, predicts
increased student grit.

摘要：對話系統已被用作英語學習的對話夥伴，但很少有人研究這些系統是否能改善學習成果。學生的熱情和毅力，或堅韌不拔的精神，與語言學習的成功有關。最近的研究表明，當學生認為他們的英語老師更支持時，他們的毅力就會得到提升。我們假設同樣的模式也適用於英語教學聊天機器人，因此我們創造了 EDEN，一個強大的開放領域聊天機器人，用於口語對話練習，並提供同理心的回饋。為了構建 EDEN，我們首先訓練一個專門的口語表達語法校正模型和一個高品質的社交閒聊對話模型。然後，我們對各種同理心回饋策略進行了初步的使用者研究。我們的實驗表明，使用適應性同理心回饋會導致更高的感知情感支持，而這反過來又預測了學生的毅力會增加。

##### **Inherent Challenges of Post-Hoc Membership Inference for Large Language Models**
2406.17975v1 by Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye

Large Language Models (LLMs) are often trained on vast amounts of undisclosed
data, motivating the development of post-hoc Membership Inference Attacks
(MIAs) to gain insight into their training data composition. However, in this
paper, we identify inherent challenges in post-hoc MIA evaluation due to
potential distribution shifts between collected member and non-member datasets.
Using a simple bag-of-words classifier, we demonstrate that datasets used in
recent post-hoc MIAs suffer from significant distribution shifts, in some cases
achieving near-perfect distinction between members and non-members. This
implies that previously reported high MIA performance may be largely
attributable to these shifts rather than model memorization. We confirm that
randomized, controlled setups eliminate such shifts and thus enable the
development and fair evaluation of new MIAs. However, we note that such
randomized setups are rarely available for the latest LLMs, making post-hoc
data collection still required to infer membership for real-world LLMs. As a
potential solution, we propose a Regression Discontinuity Design (RDD) approach
for post-hoc data collection, which substantially mitigates distribution
shifts. Evaluating various MIA methods on this RDD setup yields performance
barely above random guessing, in stark contrast to previously reported results.
Overall, our findings highlight the challenges in accurately measuring LLM
memorization and the need for careful experimental design in (post-hoc)
membership inference tasks.

摘要：大型語言模型 (LLM) 通常在大量未公開的數據上進行訓練，這促使開發事後成員推論攻擊 (MIA) 以深入了解其訓練數據組成。然而，在本文中，我們發現由於收集的成員和非成員數據集之間潛在的分布轉移，事後 MIA 評估存在固有的挑戰。使用一個簡單的詞袋分類器，我們證明了最近的事後 MIA 中使用的數據集存在顯著的分布轉移，在某些情況下，在成員和非成員之間實現了近乎完美的區別。這意味著先前報告的高 MIA 性能可能在很大程度上歸因於這些轉移，而不是模型記憶。我們確認隨機控制的設置消除了這種轉移，從而能夠開發和公平評估新的 MIA。然而，我們注意到這種隨機設置對於最新的 LLM 來說很少見，這使得事後數據收集仍然需要推斷現實世界 LLM 的成員資格。作為一個潛在的解決方案，我們提出了一個回歸不連續性設計 (RDD) 方法，用於事後數據收集，這大大減輕了分佈轉移。在這個 RDD 設置上評估各種 MIA 方法產生的性能僅略高於隨機猜測，這與先前報告的結果形成鮮明對比。總的來說，我們的發現突出了準確測量 LLM 記憶的挑戰，以及在（事後）成員推論任務中仔細進行實驗設計的必要性。

##### **Evaluating Fairness in Large Vision-Language Models Across Diverse Demographic Attributes and Prompts**
2406.17974v1 by Xuyang Wu, Yuan Wang, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang

Large vision-language models (LVLMs) have recently achieved significant
progress, demonstrating strong capabilities in open-world visual understanding.
However, it is not yet clear how LVLMs address demographic biases in real life,
especially the disparities across attributes such as gender, skin tone, and
age. In this paper, we empirically investigate \emph{visual fairness} in
several mainstream LVLMs and audit their performance disparities across
sensitive demographic attributes, based on public fairness benchmark datasets
(e.g., FACET). To disclose the visual bias in LVLMs, we design a fairness
evaluation framework with direct questions and single-choice
question-instructed prompts on visual question-answering/classification tasks.
The zero-shot prompting results indicate that, despite enhancements in visual
understanding, both open-source and closed-source LVLMs exhibit prevalent
fairness issues across different instruct prompts and demographic attributes.

摘要：大型視覺語言模型 (LVLMs) 近期已取得顯著進展，在開放世界視覺理解方面展現強大能力。然而，目前尚不清楚 LVLMs 如何解決現實生活中的人口統計偏差，特別是性別、膚色和年齡等屬性上的差異。在本文中，我們針對多個主流 LVLMs 經驗性地探討「視覺公平性」，並根據公開公平性基準資料集（例如 FACET）審查其在敏感人口統計屬性上的效能差異。為了揭露 LVLMs 中的視覺偏差，我們設計了一個公平性評估架構，其中包含直接問題和單選題指示提示，用於視覺問答/分類任務。零次學習提示結果表明，儘管視覺理解能力有所提升，但開源和閉源 LVLMs 在不同的指示提示和人口統計屬性上都表現出普遍的公平性問題。

##### **LABOR-LLM: Language-Based Occupational Representations with Large Language Models**
2406.17972v1 by Tianyu Du, Ayush Kanodia, Herman Brunborg, Keyon Vafa, Susan Athey

Many empirical studies of labor market questions rely on estimating
relatively simple predictive models using small, carefully constructed
longitudinal survey datasets based on hand-engineered features. Large Language
Models (LLMs), trained on massive datasets, encode vast quantities of world
knowledge and can be used for the next job prediction problem. However, while
an off-the-shelf LLM produces plausible career trajectories when prompted, the
probability with which an LLM predicts a particular job transition conditional
on career history will not, in general, align with the true conditional
probability in a given population. Recently, Vafa et al. (2024) introduced a
transformer-based "foundation model", CAREER, trained using a large,
unrepresentative resume dataset, that predicts transitions between jobs; it
further demonstrated how transfer learning techniques can be used to leverage
the foundation model to build better predictive models of both transitions and
wages that reflect conditional transition probabilities found in nationally
representative survey datasets. This paper considers an alternative where the
fine-tuning of the CAREER foundation model is replaced by fine-tuning LLMs. For
the task of next job prediction, we demonstrate that models trained with our
approach outperform several alternatives in terms of predictive performance on
the survey data, including traditional econometric models, CAREER, and LLMs
with in-context learning, even though the LLM can in principle predict job
titles that are not allowed in the survey data. Further, we show that our
fine-tuned LLM-based models' predictions are more representative of the career
trajectories of various workforce subpopulations than off-the-shelf LLM models
and CAREER. We conduct experiments and analyses that highlight the sources of
the gains in the performance of our models for representative predictions.

摘要：許多勞動市場問題的實證研究依賴於估計相對簡單的預測模型，這些模型使用基於手工特徵的小型、仔細建構的縱向調查資料集。大型語言模型 (LLM) 在大量資料集上訓練，編碼了大量的世界知識，可用於下一個工作預測問題。然而，雖然現成的 LLM 在提示下會產生合理的職業軌跡，但 LLM 預測特定工作轉換的機率，以職業歷史為條件，通常不會與特定人口中的真實條件機率一致。最近，Vafa 等人 (2024) 介紹了一個基於Transformer的「基礎模型」CAREER，使用大型、非代表性履歷資料集進行訓練，預測工作之間的轉換；它進一步展示了如何使用遷移學習技術來利用基礎模型建立更好的轉換和工資預測模型，以反映在全國代表性調查資料集中發現的條件轉換機率。本文考慮了一個替代方案，其中 CAREER 基礎模型的微調被微調 LLM 取代。對於下一個工作預測任務，我們證明使用我們的方法訓練的模型在預測效能方面優於幾種替代方案，包括傳統計量經濟模型、CAREER 和具有情境學習的 LLM，即使 LLM 原則上可以預測調查資料中不允許的職稱。此外，我們表明，我們微調後的基於 LLM 的模型的預測比現成的 LLM 模型和 CAREER 更能代表各種勞動力子群體的職業軌跡。我們進行實驗和分析，重點說明我們模型在代表性預測效能方面獲得提升的來源。

##### **Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective**
2406.17969v1 by Hanqi Yan, Yanzheng Xiang, Guangyi Chen, Yifei Wang, Lin Gui, Yulan He

To better interpret the intrinsic mechanism of large language models (LLMs),
recent studies focus on monosemanticity on its basic units. A monosemantic
neuron is dedicated to a single and specific concept, which forms a one-to-one
correlation between neurons and concepts. Despite extensive research in
monosemanticity probing, it remains unclear whether monosemanticity is
beneficial or harmful to model capacity. To explore this question, we revisit
monosemanticity from the feature decorrelation perspective and advocate for its
encouragement. We experimentally observe that the current conclusion by
wang2024learning, which suggests that decreasing monosemanticity enhances model
performance, does not hold when the model changes. Instead, we demonstrate that
monosemanticity consistently exhibits a positive correlation with model
capacity, in the preference alignment process. Consequently, we apply feature
correlation as a proxy for monosemanticity and incorporate a feature
decorrelation regularizer into the dynamic preference optimization process. The
experiments show that our method not only enhances representation diversity and
activation sparsity but also improves preference alignment performance.

摘要：為了更好地詮釋大型語言模型 (LLM) 的內在機制，
近期研究專注於其基本單元的單一語義。單一語義
神經元專注於單一且特定的概念，形成神經元與概念之間的一對一
對應關係。儘管在單一語義探測方面進行了廣泛的研究，
但單一語義是否有益或有害於模型容量仍不清楚。為了探討這個問題，我們從特徵去相關的角度重新審視單一語義並提倡其
鼓勵。我們透過實驗觀察到 wang2024learning 目前的結論，
這表明降低單一語義會增強模型效能，在模型改變時不成立。相反，我們證明單一語義在偏好對齊過程中始終與模型
容量呈正相關。因此，我們將特徵相關性作為單一語義的代理，並將特徵
去相關正則化項納入動態偏好最佳化過程中。實驗表明，我們的
方法不僅增強了表示多樣性和激活稀疏性，而且還改善了偏好對齊效能。

##### **Efficient Document Ranking with Learnable Late Interactions**
2406.17968v1 by Ziwei Ji, Himanshu Jain, Andreas Veit, Sashank J. Reddi, Sadeep Jayasumana, Ankit Singh Rawat, Aditya Krishna Menon, Felix Yu, Sanjiv Kumar

Cross-Encoder (CE) and Dual-Encoder (DE) models are two fundamental
approaches for query-document relevance in information retrieval. To predict
relevance, CE models use joint query-document embeddings, while DE models
maintain factorized query and document embeddings; usually, the former has
higher quality while the latter benefits from lower latency. Recently,
late-interaction models have been proposed to realize more favorable
latency-quality tradeoffs, by using a DE structure followed by a lightweight
scorer based on query and document token embeddings. However, these lightweight
scorers are often hand-crafted, and there is no understanding of their
approximation power; further, such scorers require access to individual
document token embeddings, which imposes an increased latency and storage
burden. In this paper, we propose novel learnable late-interaction models
(LITE) that resolve these issues. Theoretically, we prove that LITE is a
universal approximator of continuous scoring functions, even for relatively
small embedding dimension. Empirically, LITE outperforms previous
late-interaction models such as ColBERT on both in-domain and zero-shot
re-ranking tasks. For instance, experiments on MS MARCO passage re-ranking show
that LITE not only yields a model with better generalization, but also lowers
latency and requires 0.25x storage compared to ColBERT.

摘要：交叉編碼器 (CE) 和雙編碼器 (DE) 模型是資訊檢索中用於查詢文件相關性的兩種基本方法。為了預測相關性，CE 模型使用聯合查詢文件嵌入，而 DE 模型則維護分解的查詢和文件嵌入；通常，前者品質較高，而後者則受益於較低的延遲。最近，已經提出後互動模型，透過使用 DE 結構，然後使用基於查詢和文件標記嵌入的輕量級評分器，來實現更佳的延遲品質權衡。然而，這些輕量級評分器通常是手工製作的，而且無法理解它們的近似能力；此外，此類評分器需要存取個別文件標記嵌入，這會增加延遲和儲存負擔。在本文中，我們提出新穎的可學習後互動模型 (LITE)，以解決這些問題。在理論上，我們證明 LITE 是連續評分函數的通用逼近器，即使對於相對較小的嵌入維度也是如此。在經驗上，LITE 在領域內和零次重新排序任務上都優於先前的後互動模型，例如 ColBERT。例如，在 MS MARCO 段落重新排序上的實驗顯示，LITE 不僅產生具有更好泛化的模型，而且還降低延遲，並與 ColBERT 相比，需要 0.25 倍的儲存空間。

##### **Unmasking the Imposters: In-Domain Detection of Human vs. Machine-Generated Tweets**
2406.17967v1 by Bryan E. Tuck, Rakesh M. Verma

The rapid development of large language models (LLMs) has significantly
improved the generation of fluent and convincing text, raising concerns about
their misuse on social media platforms. We present a methodology using Twitter
datasets to examine the generative capabilities of four LLMs: Llama 3, Mistral,
Qwen2, and GPT4o. We evaluate 7B and 8B parameter base-instruction models of
the three open-source LLMs and validate the impact of further fine-tuning and
"uncensored" versions. Our findings show that "uncensored" models with
additional in-domain fine-tuning dramatically reduce the effectiveness of
automated detection methods. This study addresses a gap by exploring smaller
open-source models and the effects of "uncensoring," providing insights into
how fine-tuning and content moderation influence machine-generated text
detection.

摘要：大型語言模型 (LLM) 的快速發展已大幅提升流暢且有說服力的文字生成，引發了人們對其在社群媒體平台上遭濫用的擔憂。我們提出一個方法，使用 Twitter 資料集來檢視四個 LLM 的生成能力：Llama 3、Mistral、Qwen2 和 GPT4o。我們評估三個開源 LLM 的 7B 和 8B 參數基本指令模型，並驗證進一步微調和「無審查」版本的影響。我們的研究結果顯示，經過額外領域內微調的「無審查」模型大幅降低了自動化偵測方法的效能。本研究透過探索較小的開源模型和「無審查」的效應來解決一個缺口，提供微調和內容審核如何影響機器生成的文字偵測的見解。

##### **SimsChat: A Customisable Persona-Driven Role-Playing Agent**
2406.17962v1 by Bohao Yang, Dong Liu, Chen Tang, Chenghao Xiao, Kun Zhao, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin

Large Language Models (LLMs) possess the remarkable capability to understand
human instructions and generate high-quality text, enabling them to act as
agents that simulate human behaviours. This capability allows LLMs to emulate
human beings in a more advanced manner, beyond merely replicating simple human
behaviours. However, there is a lack of exploring into leveraging LLMs to craft
characters from several aspects. In this work, we introduce the Customisable
Conversation Agent Framework, which employs LLMs to simulate real-world
characters that can be freely customised according to different user
preferences. The customisable framework is helpful for designing customisable
characters and role-playing agents according to human's preferences. We first
propose the SimsConv dataset, which comprises 68 different customised
characters, 1,360 multi-turn role-playing dialogues, and encompasses 13,971
interaction dialogues in total. The characters are created from several
real-world elements, such as career, aspiration, trait, and skill. Building on
these foundations, we present SimsChat, a freely customisable role-playing
agent. It incorporates different real-world scenes and topic-specific character
interaction dialogues, simulating characters' life experiences in various
scenarios and topic-specific interactions with specific emotions. Experimental
results show that our proposed framework achieves desirable performance and
provides helpful guideline for building better simulacra of human beings in the
future. Our data and code are available at
https://github.com/Bernard-Yang/SimsChat.

摘要：大型語言模型 (LLM) 擁有理解人類指令並產生高品質文字的卓越能力，讓它們能夠充當模擬人類行為的代理人。此能力讓 LLM 能夠以更進階的方式模擬人類，而不仅仅是複製簡單的人類行為。然而，目前缺乏探索利用 LLM 從多個方面打造角色。在這項工作中，我們介紹了可自訂對話代理程式架構，它採用 LLM 來模擬真實世界角色，這些角色可以根據不同的使用者偏好自由自訂。此可自訂架構有助於根據人類的偏好設計可自訂角色和角色扮演代理程式。我們首先提出 SimsConv 資料集，其中包含 68 個不同的自訂角色、1,360 個多輪角色扮演對話，並總共包含 13,971 個互動對話。這些角色是根據多種真實世界元素建立的，例如職業、抱負、特質和技能。在這些基礎上，我們展示了 SimsChat，一個自由可自訂的角色扮演代理程式。它結合了不同的真實世界場景和特定主題的角色互動對話，模擬角色在各種場景中的生活經驗和與特定情緒的特定主題互動。實驗結果顯示，我們提出的架構達到了理想的效能，並為在未來建立更佳的人類模擬提供有用的準則。我們的資料和程式碼可以在 https://github.com/Bernard-Yang/SimsChat 取得。

##### **NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization**
2406.17961v1 by Md Mahadi Hasan Nahid, Davood Rafiei

In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities in parsing textual data and generating code. However, their
performance in tasks involving tabular data, especially those requiring
symbolic reasoning, faces challenges due to the structural variance and
inconsistency in table cell values often found in web tables. In this paper, we
introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning
performance of LLMs by normalizing web tables. We study table normalization as
a stand-alone, one-time preprocessing step using LLMs to support symbolic
reasoning on tabular data. Our experimental evaluation, conducted on
challenging web table datasets such as WikiTableQuestion and TabFact,
demonstrates that leveraging NormTab significantly improves symbolic reasoning
performance, showcasing the importance and effectiveness of web table
normalization for enhancing LLM-based symbolic reasoning tasks.

摘要：近年来，大型语言模型 (LLM) 已在解析文本数据和生成代码方面展示出卓越的能力。然而，它们在涉及表格数据（特别是需要符号推理的任务）时的表现面临着挑战，这是因为网络表格中的表格单元格值通常存在结构差异和不一致性。在本文中，我们介绍了 NormTab，这是一个旨在通过规范化网络表格来增强 LLM 符号推理性能的新框架。我们研究了表格规范化，将其作为使用 LLM 支持表格数据符号推理的独立的一次性预处理步骤。我们在 WikiTableQuestion 和 TabFact 等具有挑战性的网络表格数据集上进行的实验评估表明，利用 NormTab 可以显著提高符号推理性能，展示了网络表格规范化对于增强基于 LLM 的符号推理任务的重要性及有效性。

##### **MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation**
2406.17960v1 by Liuyi Wang, Zongtao He, Mengjiao Shen, Jingwei Yang, Chengju Liu, Qijun Chen

Despite the remarkable developments of recent large models in Embodied
Artificial Intelligence (E-AI), their integration into robotics is hampered by
their excessive parameter sizes and computational demands. Towards the
Vision-and-Language Navigation (VLN) task, a core task in E-AI, this paper
reveals the great potential of using knowledge distillation for obtaining
lightweight student models by proposing a Meta-Ability Guided Interactive
Chain-of-distillation (MAGIC) method. Specifically, a Meta-Ability Knowledge
Distillation (MAKD) framework is proposed for decoupling and refining the
necessary meta-abilities of VLN agents. A Meta-Knowledge Randomization
Weighting (MKRW) and a Meta-Knowledge Transferable Determination (MKTD) module
are incorporated to dynamically adjust aggregation weights at the meta-ability
and sample levels, respectively. Move beyond the traditional one-step
unidirectional distillation, an Interactive Chain-of-Distillation (ICoD)
learning strategy is proposed to allow students to give feedback to teachers,
forming a new multi-step teacher-student co-evolution pipeline. Remarkably, on
the R2R test unseen public leaderboard, our smallest model, MAGIC-S, with only
5% (11M) of the teacher's size, outperforms all previous methods under the same
training data. Additionally, our largest model, MAGIC-L, surpasses the previous
state-of-the-art by 5.84% in SPL and 3.18% in SR. Furthermore, a new dataset
was collected and annotated from our living environments, where MAGIC-S
demonstrated superior performance and real-time efficiency. Our code is
publicly available on https://github.com/CrystalSixone/VLN-MAGIC.

摘要：儘管最近具身人工智慧 (E-AI) 中大型模型有顯著的發展，但其過於龐大的參數規模和運算需求阻礙了它們與機器人的整合。針對 E-AI 中的核心任務，即視覺語言導航 (VLN) 任務，本文揭示了利用知識蒸餾來取得輕量級學生模型的巨大潛力，並提出了一個元能力引導互動式蒸餾鏈 (MAGIC) 方法。具體來說，提出了一個元能力知識蒸餾 (MAKD) 架構，用於解耦和精煉 VLN 代理所需的元能力。並整合了一個元知識隨機加權 (MKRW) 和一個元知識可轉移確定 (MKTD) 模組，分別用於動態調整元能力和樣本層級的聚合權重。超越傳統的單步驟單向蒸餾，提出了一個互動式蒸餾鏈 (ICoD) 學習策略，讓學生可以向老師提供回饋，形成一個新的多步驟師生共同演化管道。值得注意的是，在 R2R 測試未公開排行榜上，我們最小的模型 MAGIC-S，只有老師規模的 5% (11M)，在相同的訓練資料下表現優於所有先前的模型。此外，我們最大的模型 MAGIC-L 在 SPL 中超越了先前的最佳表現 5.84%，在 SR 中超越了 3.18%。此外，從我們的居住環境中收集並標註了一個新的資料集，其中 MAGIC-S 展示了卓越的效能和即時效率。我們的程式碼已公開於 https://github.com/CrystalSixone/VLN-MAGIC。

