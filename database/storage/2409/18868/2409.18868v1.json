{"2409.18868": {"publish_time": "2024-09-27", "title": "Individuation in Neural Models with and without Visual Grounding", "paper_summary": "We show differences between a language-and-vision model CLIP and two\ntext-only models - FastText and SBERT - when it comes to the encoding of\nindividuation information. We study latent representations that CLIP provides\nfor substrates, granular aggregates, and various numbers of objects. We\ndemonstrate that CLIP embeddings capture quantitative differences in\nindividuation better than models trained on text-only data. Moreover, the\nindividuation hierarchy we deduce from the CLIP embeddings agrees with the\nhierarchies proposed in linguistics and cognitive science.", "paper_summary_zh": "\u6211\u5011\u5c55\u793a\u4e86\u5728\u7de8\u78bc\u500b\u5225\u5316\u8cc7\u8a0a\u6642\uff0c\u8a9e\u8a00\u548c\u8996\u89ba\u6a21\u578b CLIP \u8207\u5169\u500b\u7d14\u6587\u5b57\u6a21\u578b - FastText \u548c SBERT - \u4e4b\u9593\u7684\u5dee\u7570\u3002\u6211\u5011\u7814\u7a76 CLIP \u63d0\u4f9b\u7d66\u57fa\u8cea\u3001\u9846\u7c92\u805a\u96c6\u9ad4\u548c\u5404\u7a2e\u7269\u4ef6\u6578\u91cf\u7684\u6f5b\u5728\u8868\u5fb5\u3002\u6211\u5011\u8b49\u660e CLIP \u5167\u5d4c\u6bd4\u50c5\u91dd\u5c0d\u7d14\u6587\u5b57\u8cc7\u6599\u8a13\u7df4\u7684\u6a21\u578b\u66f4\u80fd\u6355\u6349\u500b\u5225\u5316\u7684\u91cf\u5316\u5dee\u7570\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f9e CLIP \u5167\u5d4c\u63a8\u8ad6\u51fa\u7684\u500b\u5225\u5316\u968e\u5c64\u8207\u8a9e\u8a00\u5b78\u548c\u8a8d\u77e5\u79d1\u5b78\u4e2d\u63d0\u51fa\u7684\u968e\u5c64\u4e00\u81f4\u3002", "author": "Alexey Tikhonov et.al.", "authors": "Alexey Tikhonov, Lisa Bylinina, Ivan P. Yamshchikov", "id": "2409.18868v1", "paper_url": "http://arxiv.org/abs/2409.18868v1", "repo": "null"}}