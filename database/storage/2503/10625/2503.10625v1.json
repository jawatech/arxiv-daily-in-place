{"2503.10625": {"publish_time": "2025-03-13", "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds", "paper_summary": "Animatable 3D human reconstruction from a single image is a challenging\nproblem due to the ambiguity in decoupling geometry, appearance, and\ndeformation. Recent advances in 3D human reconstruction mainly focus on static\nhuman modeling, and the reliance of using synthetic 3D scans for training\nlimits their generalization ability. Conversely, optimization-based video\nmethods achieve higher fidelity but demand controlled capture conditions and\ncomputationally intensive refinement processes. Motivated by the emergence of\nlarge reconstruction models for efficient static reconstruction, we propose LHM\n(Large Animatable Human Reconstruction Model) to infer high-fidelity avatars\nrepresented as 3D Gaussian splatting in a feed-forward pass. Our model\nleverages a multimodal transformer architecture to effectively encode the human\nbody positional features and image features with attention mechanism, enabling\ndetailed preservation of clothing geometry and texture. To further boost the\nface identity preservation and fine detail recovery, we propose a head feature\npyramid encoding scheme to aggregate multi-scale features of the head regions.\nExtensive experiments demonstrate that our LHM generates plausible animatable\nhuman in seconds without post-processing for face and hands, outperforming\nexisting methods in both reconstruction accuracy and generalization ability.", "paper_summary_zh": "<paragraph>\u5f9e\u55ae\u5f35\u5716\u50cf\u91cd\u5efa\u53ef\u52d5\u756b\u7684 3D \u4eba\u9ad4\u6a21\u578b\u662f\u4e00\u500b\u6975\u5177\u6311\u6230\u6027\u7684\u554f\u984c\uff0c\u56e0\u70ba\u96e3\u4ee5\u5c07\u5e7e\u4f55\u5f62\u72c0\u3001\u5916\u89c0\u548c\u8b8a\u5f62\u5206\u96e2\u3002\u8fd1\u5e74\u4f86\uff0c3D \u4eba\u9ad4\u91cd\u5efa\u7684\u9032\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u975c\u614b\u4eba\u9ad4\u5efa\u6a21\u4e0a\uff0c\u800c\u4e14\u4f9d\u8cf4\u4f7f\u7528\u5408\u6210\u7684 3D \u6383\u63cf\u6578\u64da\u9032\u884c\u8a13\u7df4\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u76f8\u53cd\u5730\uff0c\u57fa\u65bc\u512a\u5316\u7684\u8996\u983b\u65b9\u6cd5\u96d6\u7136\u80fd\u5be6\u73fe\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\uff0c\u4f46\u9700\u8981\u53d7\u63a7\u7684\u62cd\u651d\u689d\u4ef6\u548c\u8a08\u7b97\u5bc6\u96c6\u578b\u7684\u7d30\u5316\u904e\u7a0b\u3002\u53d7\u9ad8\u6548\u975c\u614b\u91cd\u5efa\u5927\u578b\u91cd\u5efa\u6a21\u578b\u51fa\u73fe\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LHM\uff08\u5927\u578b\u53ef\u52d5\u756b\u4eba\u9ad4\u91cd\u5efa\u6a21\u578b\uff09\uff0c\u4ee5\u5728\u524d\u994b\u904e\u7a0b\u4e2d\u63a8\u65b7\u4ee5 3D \u9ad8\u65af\u6a23\u689d\u8868\u793a\u7684\u9ad8\u4fdd\u771f\u865b\u64ec\u5316\u8eab\u3002\u6211\u5011\u7684\u6a21\u578b\u5229\u7528\u591a\u6a21\u614b Transformer \u67b6\u69cb\uff0c\u901a\u904e\u6ce8\u610f\u529b\u6a5f\u5236\u6709\u6548\u5730\u7de8\u78bc\u4eba\u9ad4\u4f4d\u7f6e\u7279\u5fb5\u548c\u5716\u50cf\u7279\u5fb5\uff0c\u5f9e\u800c\u80fd\u5920\u8a73\u7d30\u4fdd\u7559\u670d\u88dd\u7684\u5e7e\u4f55\u5f62\u72c0\u548c\u7d0b\u7406\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63d0\u9ad8\u9762\u90e8\u8b58\u5225\u7684\u4fdd\u7559\u548c\u7d30\u7bc0\u7684\u6062\u5fa9\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u982d\u90e8\u7279\u5fb5\u91d1\u5b57\u5854\u7de8\u78bc\u65b9\u6848\uff0c\u7528\u65bc\u805a\u5408\u982d\u90e8\u5340\u57df\u7684\u591a\u5c3a\u5ea6\u7279\u5fb5\u3002\u5927\u91cf\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684 LHM \u53ef\u4ee5\u5728\u5e7e\u79d2\u9418\u5167\u751f\u6210\u903c\u771f\u7684\u53ef\u52d5\u756b\u4eba\u9ad4\u6a21\u578b\uff0c\u7121\u9700\u5c0d\u81c9\u90e8\u548c\u624b\u90e8\u9032\u884c\u5f8c\u8655\u7406\uff0c\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002</paragraph>\n", "author": "Lingteng Qiu et.al.", "authors": "Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo", "id": "2503.10625v1", "paper_url": "http://arxiv.org/abs/2503.10625v1", "repo": "null"}}