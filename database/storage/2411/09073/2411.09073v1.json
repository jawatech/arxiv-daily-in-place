{"2411.09073": {"publish_time": "2024-11-13", "title": "Code-mixed LLM: Improve Large Language Models' Capability to Handle Code-Mixing through Reinforcement Learning from AI Feedback", "paper_summary": "Code-mixing(CM) or code-switching(CSW) refers to the juxtaposition of\nlinguistic units from two or more languages during the conversation or\nsometimes even a single utterance. Code-mixing introduces unique challenges in\ndaily life, such as syntactic mismatches and semantic blending, that are rarely\nencountered in monolingual settings. Large language models (LLMs) have\nrevolutionized the field of natural language processing (NLP) by offering\nunprecedented capabilities in understanding human languages. However, the\neffectiveness of current state-of-the-art multilingual LLMs has not yet been\nfully explored in the CM scenario. To fill this gap, we first benchmark the\nperformance of multilingual LLMs on various code-mixing NLP tasks. Then we\npropose to improve the multilingual LLMs' ability to understand code-mixing\nthrough reinforcement learning from human feedback (RLHF) and code-mixed\nmachine translation tasks. Given the high-cost and time-consuming preference\nlabeling procedure, we improve this by utilizing LLMs as annotators to perform\nthe reinforcement learning from AI feedback (RLAIF). The experiments show the\neffectiveness of the proposed method.", "paper_summary_zh": "\u4ee3\u78bc\u6df7\u5408\uff08CM\uff09\u6216\u4ee3\u78bc\u8f49\u63db\uff08CSW\uff09\u662f\u6307\u5728\u5c0d\u8a71\u4e2d\u6216\u6709\u6642\u751a\u81f3\u55ae\u4e00\u8a9e\u53e5\u4e2d\u4e26\u7f6e\u4f86\u81ea\u5169\u7a2e\u6216\u66f4\u591a\u8a9e\u8a00\u7684\u8a9e\u8a00\u55ae\u4f4d\u3002\u4ee3\u78bc\u6df7\u5408\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u5f15\u5165\u4e86\u7368\u7279\u7684\u6311\u6230\uff0c\u4f8b\u5982\u8a9e\u6cd5\u4e0d\u5339\u914d\u548c\u8a9e\u7fa9\u6df7\u5408\uff0c\u9019\u5728\u55ae\u8a9e\u74b0\u5883\u4e2d\u5f88\u5c11\u9047\u5230\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u901a\u904e\u63d0\u4f9b\u7406\u89e3\u4eba\u985e\u8a9e\u8a00\u7684\u7a7a\u524d\u80fd\u529b\uff0c\u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u9818\u57df\u3002\u7136\u800c\uff0c\u7576\u524d\u6700\u5148\u9032\u7684\u591a\u8a9e\u7a2e LLM \u7684\u6709\u6548\u6027\u5c1a\u672a\u5728 CM \u5834\u666f\u4e2d\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u70ba\u4e86\u586b\u88dc\u9019\u4e00\u7a7a\u767d\uff0c\u6211\u5011\u9996\u5148\u5c0d\u591a\u8a9e\u7a2e LLM \u5728\u5404\u7a2e\u4ee3\u78bc\u6df7\u5408 NLP \u4efb\u52d9\u4e0a\u7684\u6027\u80fd\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5efa\u8b70\u901a\u904e\u4eba\u985e\u53cd\u994b\uff08RLHF\uff09\u548c\u4ee3\u78bc\u6df7\u5408\u6a5f\u5668\u7ffb\u8b6f\u4efb\u52d9\u7684\u5f37\u5316\u5b78\u7fd2\u4f86\u63d0\u9ad8\u591a\u8a9e\u7a2e LLM \u7406\u89e3\u4ee3\u78bc\u6df7\u5408\u7684\u80fd\u529b\u3002\u9451\u65bc\u9ad8\u6210\u672c\u548c\u8017\u6642\u7684\u504f\u597d\u6a19\u7c64\u7a0b\u5e8f\uff0c\u6211\u5011\u901a\u904e\u5229\u7528 LLM \u4f5c\u70ba\u8a3b\u91cb\u8005\u4f86\u57f7\u884c AI \u53cd\u994b\uff08RLAIF\uff09\u7684\u5f37\u5316\u5b78\u7fd2\u4f86\u6539\u9032\u9019\u4e00\u9ede\u3002\u5be6\u9a57\u8868\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "author": "Wenbo Zhang et.al.", "authors": "Wenbo Zhang, Aditya Majumdar, Amulya Yadav", "id": "2411.09073v1", "paper_url": "http://arxiv.org/abs/2411.09073v1", "repo": "null"}}