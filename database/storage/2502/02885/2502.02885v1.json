{"2502.02885": {"publish_time": "2025-02-05", "title": "Expertized Caption Auto-Enhancement for Video-Text Retrieval", "paper_summary": "The burgeoning field of video-text retrieval has witnessed significant\nadvancements with the advent of deep learning. However, the challenge of\nmatching text and video persists due to inadequate textual descriptions of\nvideos. The substantial information gap between the two modalities hinders a\ncomprehensive understanding of videos, resulting in ambiguous retrieval\nresults. While rewriting methods based on large language models have been\nproposed to broaden text expressions, carefully crafted prompts are essential\nto ensure the reasonableness and completeness of the rewritten texts. This\npaper proposes an automatic caption enhancement method that enhances expression\nquality and mitigates empiricism in augmented captions through self-learning.\nAdditionally, an expertized caption selection mechanism is designed and\nintroduced to customize augmented captions for each video, facilitating\nvideo-text matching. Our method is entirely data-driven, which not only\ndispenses with heavy data collection and computation workload but also improves\nself-adaptability by circumventing lexicon dependence and introducing\npersonalized matching. The superiority of our method is validated by\nstate-of-the-art results on various benchmarks, specifically achieving Top-1\nrecall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo.", "paper_summary_zh": "\u84ec\u52c3\u767c\u5c55\u7684\u5f71\u7247\u6587\u5b57\u6aa2\u7d22\u9818\u57df\uff0c\u96a8\u8457\u6df1\u5ea6\u5b78\u7fd2\u7684\u51fa\u73fe\uff0c\u898b\u8b49\u4e86\u986f\u8457\u7684\u9032\u5c55\u3002\u7136\u800c\uff0c\u7531\u65bc\u5f71\u7247\u7684\u6587\u5b57\u63cf\u8ff0\u4e0d\u8db3\uff0c\u6587\u5b57\u548c\u5f71\u7247\u914d\u5c0d\u7684\u6311\u6230\u4f9d\u7136\u5b58\u5728\u3002\u5169\u7a2e\u6a21\u5f0f\u4e4b\u9593\u7684\u5be6\u8cea\u6027\u8cc7\u8a0a\u5dee\u8ddd\u963b\u7919\u4e86\u5c0d\u5f71\u7247\u7684\u5168\u9762\u7406\u89e3\uff0c\u5c0e\u81f4\u6a21\u7cca\u7684\u6aa2\u7d22\u7d50\u679c\u3002\u96d6\u7136\u5df2\u7d93\u63d0\u51fa\u57fa\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u91cd\u5beb\u65b9\u6cd5\u4f86\u64f4\u5c55\u6587\u5b57\u8868\u9054\uff0c\u4f46\u7cbe\u5fc3\u8a2d\u8a08\u7684\u63d0\u793a\u5c0d\u65bc\u78ba\u4fdd\u91cd\u5beb\u6587\u5b57\u7684\u5408\u7406\u6027\u548c\u5b8c\u6574\u6027\u81f3\u95dc\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u81ea\u52d5\u5b57\u5e55\u589e\u5f37\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u901a\u904e\u81ea\u5b78\u7fd2\u589e\u5f37\u8868\u9054\u54c1\u8cea\uff0c\u4e26\u6e1b\u8f15\u64f4\u589e\u5b57\u5e55\u4e2d\u7684\u7d93\u9a57\u4e3b\u7fa9\u3002\u6b64\u5916\uff0c\u8a2d\u8a08\u4e26\u5f15\u9032\u4e86\u4e00\u7a2e\u5c08\u5bb6\u5b57\u5e55\u9078\u64c7\u6a5f\u5236\uff0c\u4ee5\u91dd\u5c0d\u6bcf\u500b\u5f71\u7247\u81ea\u8a02\u64f4\u589e\u5b57\u5e55\uff0c\u4fc3\u9032\u5f71\u7247\u6587\u5b57\u914d\u5c0d\u3002\u6211\u5011\u7684\u505a\u6cd5\u5b8c\u5168\u7531\u8cc7\u6599\u9a45\u52d5\uff0c\u4e0d\u50c5\u7701\u53bb\u4e86\u7e41\u91cd\u7684\u8cc7\u6599\u6536\u96c6\u548c\u904b\u7b97\u5de5\u4f5c\u8ca0\u8f09\uff0c\u9084\u901a\u904e\u898f\u907f\u8a5e\u5f59\u4f9d\u8cf4\u4e26\u5f15\u5165\u500b\u6027\u5316\u914d\u5c0d\uff0c\u63d0\u9ad8\u4e86\u81ea\u9069\u61c9\u6027\u3002\u6211\u5011\u7684\u505a\u6cd5\u7684\u512a\u8d8a\u6027\u901a\u904e\u5404\u7a2e\u57fa\u6e96\u4e0a\u7684\u6700\u65b0\u7d50\u679c\u5f97\u5230\u9a57\u8b49\uff0c\u7279\u5225\u662f\u5728 MSR-VTT \u4e0a\u5be6\u73fe\u4e86 68.5% \u7684 Top-1 \u53ec\u56de\u7387\u6e96\u78ba\u5ea6\uff0c\u5728 MSVD \u4e0a\u5be6\u73fe\u4e86 68.1%\uff0c\u5728 DiDeMo \u4e0a\u5be6\u73fe\u4e86 62.0%\u3002", "author": "Junxiang Chen et.al.", "authors": "Junxiang Chen, Baoyao yang, Wenbin Yao", "id": "2502.02885v1", "paper_url": "http://arxiv.org/abs/2502.02885v1", "repo": "null"}}