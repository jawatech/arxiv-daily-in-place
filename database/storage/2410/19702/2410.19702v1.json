{"2410.19702": {"publish_time": "2024-10-25", "title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning", "paper_summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5df2\u5728\u77ed\u89c6\u9891\u7406\u89e3\u4e2d\u5c55\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u7406\u89e3\u957f\u7bc7\u89c6\u9891\u5bf9\u4e8e MLLM \u6765\u8bf4\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86 TimeSuite\uff0c\u8fd9\u662f\u4e00\u7ec4\u65b0\u8bbe\u8ba1\uff0c\u7528\u4e8e\u8c03\u6574\u73b0\u6709\u7684\u77ed\u7bc7\u89c6\u9891 MLLM \u4ee5\u7406\u89e3\u957f\u89c6\u9891\uff0c\u5305\u62ec\u4e00\u4e2a\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u6846\u67b6\u6765\u5904\u7406\u957f\u89c6\u9891\u5e8f\u5217\u3001\u4e00\u4e2a\u7528\u4e8e MLLM \u7684\u57fa\u7840\u5fae\u8c03\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\u5fae\u8c03\u4efb\u52a1\uff0c\u4ee5\u660e\u786e\u5730\u5c06\u57fa\u7840\u76d1\u7763\u7eb3\u5165\u4f20\u7edf\u7684\u95ee\u7b54\u683c\u5f0f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u57fa\u4e8e VideoChat\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6211\u4eec\u7684\u957f\u89c6\u9891 MLLM\uff0c\u79f0\u4e3a VideoChat-T\uff0c\u901a\u8fc7\u5b9e\u73b0\u4ee4\u724c\u6539\u7ec4\u6765\u538b\u7f29\u957f\u89c6\u9891\u4ee4\u724c\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u81ea\u9002\u5e94\u4f4d\u7f6e\u7f16\u7801 (TAPE) \u6765\u589e\u5f3a\u89c6\u89c9\u8868\u793a\u7684\u65f6\u95f4\u611f\u77e5\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5f15\u5165\u4e86 TimePro\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u4ee5\u57fa\u7840\u4e3a\u4e2d\u5fc3\u7684\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u7531 9 \u4e2a\u4efb\u52a1\u548c 349k \u4e2a\u9ad8\u8d28\u91cf\u7684\u57fa\u7840\u6ce8\u91ca\u7ec4\u6210\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u4ee4\u5fae\u8c03\u4efb\u52a1\u7c7b\u578b\uff0c\u79f0\u4e3a\u65f6\u95f4\u57fa\u7840\u5b57\u5e55\uff0c\u7528\u4e8e\u6267\u884c\u8be6\u7ec6\u7684\u89c6\u9891\u63cf\u8ff0\uff0c\u5e76\u9884\u6d4b\u76f8\u5e94\u7684\u65f6\u95f4\u6233\u3002\u8fd9\u79cd\u660e\u786e\u7684\u65f6\u95f4\u4f4d\u7f6e\u9884\u6d4b\u5c06\u6307\u5bfc MLLM \u5728\u751f\u6210\u63cf\u8ff0\u65f6\u6b63\u786e\u5173\u6ce8\u89c6\u89c9\u5185\u5bb9\uff0c\u4ece\u800c\u964d\u4f4e\u7531 LLM \u5f15\u8d77\u7684\u5e7b\u89c9\u98ce\u9669\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684 TimeSuite \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6210\u529f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u589e\u5f3a\u77ed\u7bc7 MLLM \u7684\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5206\u522b\u5728 Egoschema \u548c VideoMME \u7684\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86 5.6% \u548c 6.8% \u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0cVideoChat-T \u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u65f6\u95f4\u57fa\u7840\u80fd\u529b\uff0c\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684 MLLM\u3002\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u5b83\u7684\u6027\u80fd\u4e0e\u4f20\u7edf\u7684\u76d1\u7763\u4e13\u5bb6\u6a21\u578b\u76f8\u5f53\u3002", "author": "Xiangyu Zeng et.al.", "authors": "Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang", "id": "2410.19702v1", "paper_url": "http://arxiv.org/abs/2410.19702v1", "repo": "null"}}