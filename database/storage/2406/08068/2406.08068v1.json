{"2406.08068": {"publish_time": "2024-06-12", "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey", "paper_summary": "Compared to traditional sentiment analysis, which only considers text,\nmultimodal sentiment analysis needs to consider emotional signals from\nmultimodal sources simultaneously and is therefore more consistent with the way\nhow humans process sentiment in real-world scenarios. It involves processing\nemotional information from various sources such as natural language, images,\nvideos, audio, physiological signals, etc. However, although other modalities\nalso contain diverse emotional cues, natural language usually contains richer\ncontextual information and therefore always occupies a crucial position in\nmultimodal sentiment analysis. The emergence of ChatGPT has opened up immense\npotential for applying large language models (LLMs) to text-centric multimodal\ntasks. However, it is still unclear how existing LLMs can adapt better to\ntext-centric multimodal sentiment analysis tasks. This survey aims to (1)\npresent a comprehensive review of recent research in text-centric multimodal\nsentiment analysis tasks, (2) examine the potential of LLMs for text-centric\nmultimodal sentiment analysis, outlining their approaches, advantages, and\nlimitations, (3) summarize the application scenarios of LLM-based multimodal\nsentiment analysis technology, and (4) explore the challenges and potential\nresearch directions for multimodal sentiment analysis in the future.", "paper_summary_zh": "\u76f8\u8f03\u65bc\u50b3\u7d71\u53ea\u8003\u616e\u6587\u5b57\u7684\u60c5\u611f\u5206\u6790\uff0c\u591a\u6a21\u614b\u60c5\u611f\u5206\u6790\u9700\u8981\u540c\u6642\u8003\u616e\u4f86\u81ea\u591a\u6a21\u614b\u4f86\u6e90\u7684\u60c5\u7dd2\u4fe1\u865f\uff0c\u56e0\u6b64\u66f4\u7b26\u5408\u4eba\u985e\u5728\u771f\u5be6\u4e16\u754c\u5834\u666f\u4e2d\u8655\u7406\u60c5\u7dd2\u7684\u65b9\u5f0f\u3002\u5b83\u6d89\u53ca\u8655\u7406\u4f86\u81ea\u81ea\u7136\u8a9e\u8a00\u3001\u5f71\u50cf\u3001\u5f71\u7247\u3001\u97f3\u8a0a\u3001\u751f\u7406\u8a0a\u865f\u7b49\u5404\u7a2e\u4f86\u6e90\u7684\u60c5\u7dd2\u8cc7\u8a0a\u3002\u7136\u800c\uff0c\u5118\u7ba1\u5176\u4ed6\u6a21\u614b\u4e5f\u5305\u542b\u591a\u5143\u7684\u60c5\u7dd2\u7dda\u7d22\uff0c\u4f46\u81ea\u7136\u8a9e\u8a00\u901a\u5e38\u5305\u542b\u66f4\u8c50\u5bcc\u7684\u8108\u7d61\u8cc7\u8a0a\uff0c\u56e0\u6b64\u5728\u591a\u6a21\u614b\u60c5\u611f\u5206\u6790\u4e2d\u59cb\u7d42\u4f54\u6709\u95dc\u9375\u5730\u4f4d\u3002ChatGPT \u7684\u51fa\u73fe\u70ba\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u61c9\u7528\u65bc\u4ee5\u6587\u5b57\u70ba\u4e2d\u5fc3\u7684\u591a\u6a21\u614b\u4efb\u52d9\u958b\u555f\u4e86\u5de8\u5927\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u73fe\u6709 LLM \u5982\u4f55\u80fd\u66f4\u597d\u5730\u9069\u61c9\u4ee5\u6587\u5b57\u70ba\u4e2d\u5fc3\u7684\u591a\u6a21\u614b\u60c5\u611f\u5206\u6790\u4efb\u52d9\u4ecd\u4e0d\u6e05\u695a\u3002\u672c\u7d9c\u8ff0\u65e8\u5728\uff1a(1) \u5168\u9762\u56de\u9867\u8fd1\u671f\u4ee5\u6587\u5b57\u70ba\u4e2d\u5fc3\u7684\u591a\u6a21\u614b\u60c5\u611f\u5206\u6790\u4efb\u52d9\u7684\u7814\u7a76\uff0c(2) \u63a2\u8a0e LLM \u5728\u4ee5\u6587\u5b57\u70ba\u4e2d\u5fc3\u7684\u591a\u6a21\u614b\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6f5b\u529b\uff0c\u6982\u8ff0\u5176\u65b9\u6cd5\u3001\u512a\u9ede\u548c\u9650\u5236\uff0c(3) \u7e3d\u7d50\u57fa\u65bc LLM \u7684\u591a\u6a21\u614b\u60c5\u611f\u5206\u6790\u6280\u8853\u7684\u61c9\u7528\u60c5\u5883\uff0c\u4ee5\u53ca (4) \u63a2\u8a0e\u672a\u4f86\u591a\u6a21\u614b\u60c5\u611f\u5206\u6790\u7684\u6311\u6230\u548c\u6f5b\u5728\u7814\u7a76\u65b9\u5411\u3002", "author": "Hao Yang et.al.", "authors": "Hao Yang, Yanyan Zhao, Yang Wu, Shilong Wang, Tian Zheng, Hongbo Zhang, Wanxiang Che, Bing Qin", "id": "2406.08068v1", "paper_url": "http://arxiv.org/abs/2406.08068v1", "repo": "null"}}