{"2406.18740": {"publish_time": "2024-06-26", "title": "Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models", "paper_summary": "Large Language Models (LLMs) have been revolutionizing a myriad of natural\nlanguage processing tasks with their diverse zero-shot capabilities. Indeed,\nexisting work has shown that LLMs can be used to great effect for many tasks,\nsuch as information retrieval (IR), and passage ranking. However, current\nstate-of-the-art results heavily lean on the capabilities of the LLM being\nused. Currently, proprietary, and very large LLMs such as GPT-4 are the highest\nperforming passage re-rankers. Hence, users without the resources to leverage\ntop of the line LLMs, or ones that are closed source, are at a disadvantage. In\nthis paper, we investigate the use of a pre-filtering step before passage\nre-ranking in IR. Our experiments show that by using a small number of human\ngenerated relevance scores, coupled with LLM relevance scoring, it is\neffectively possible to filter out irrelevant passages before re-ranking. Our\nexperiments also show that this pre-filtering then allows the LLM to perform\nsignificantly better at the re-ranking task. Indeed, our results show that\nsmaller models such as Mixtral can become competitive with much larger\nproprietary models (e.g., ChatGPT and GPT-4).", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6191\u85c9\u5176\u591a\u6a23\u5316\u7684\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\uff0c\u5fb9\u5e95\u6539\u8b8a\u4e86\u7121\u6578\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u3002\u4e8b\u5be6\u4e0a\uff0c\u73fe\u6709\u7814\u7a76\u5df2\u986f\u793a\uff0cLLM \u53ef\u7528\u65bc\u8a31\u591a\u4efb\u52d9\uff0c\u4f8b\u5982\u8cc7\u8a0a\u6aa2\u7d22 (IR) \u548c\u6bb5\u843d\u6392\u5e8f\uff0c\u4e26\u80fd\u767c\u63ee\u6975\u4f73\u7684\u6548\u679c\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u6700\u65b0\u6280\u8853\u6210\u679c\u6975\u5ea6\u4ef0\u8cf4\u6240\u4f7f\u7528\u7684 LLM \u7684\u80fd\u529b\u3002\u76ee\u524d\uff0c\u5c01\u9589\u539f\u59cb\u78bc\u4e14\u898f\u6a21\u6975\u5927\u7684 LLM\uff0c\u4f8b\u5982 GPT-4\uff0c\u662f\u6548\u80fd\u6700\u9ad8\u7684\u6bb5\u843d\u91cd\u65b0\u6392\u5e8f\u5668\u3002\u56e0\u6b64\uff0c\u6c92\u6709\u8cc7\u6e90\u4f7f\u7528\u9802\u7d1a LLM \u6216\u5c01\u9589\u539f\u59cb\u78bc LLM \u7684\u4f7f\u7528\u8005\u8655\u65bc\u52a3\u52e2\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5728 IR \u4e2d\u4f7f\u7528\u6bb5\u843d\u91cd\u65b0\u6392\u5e8f\u524d\u7684\u9810\u5148\u7be9\u9078\u6b65\u9a5f\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0c\u900f\u904e\u4f7f\u7528\u5c11\u91cf\u4eba\u5de5\u7522\u751f\u7684\u76f8\u95dc\u6027\u8a55\u5206\uff0c\u4e26\u7d50\u5408 LLM \u76f8\u95dc\u6027\u8a55\u5206\uff0c\u5728\u91cd\u65b0\u6392\u5e8f\u524d\u6709\u6548\u904e\u6ffe\u6389\u4e0d\u76f8\u95dc\u7684\u6bb5\u843d\u3002\u6211\u5011\u7684\u5be6\u9a57\u4e5f\u986f\u793a\uff0c\u6b64\u9810\u5148\u7be9\u9078\u8b93 LLM \u5728\u91cd\u65b0\u6392\u5e8f\u4efb\u52d9\u4e2d\u8868\u73fe\u5927\u5e45\u63d0\u5347\u3002\u4e8b\u5be6\u4e0a\uff0c\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u8f03\u5c0f\u7684\u6a21\u578b\uff0c\u4f8b\u5982 Mixtral\uff0c\u53ef\u8207\u898f\u6a21\u66f4\u5927\u7684\u5c01\u9589\u539f\u59cb\u78bc\u6a21\u578b\uff08\u4f8b\u5982 ChatGPT \u548c GPT-4\uff09\u7af6\u722d\u3002", "author": "Baharan Nouriinanloo et.al.", "authors": "Baharan Nouriinanloo, Maxime Lamothe", "id": "2406.18740v1", "paper_url": "http://arxiv.org/abs/2406.18740v1", "repo": "null"}}