{"2409.11365": {"publish_time": "2024-09-17", "title": "CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration", "paper_summary": "The deployment of multimodal large language models (MLLMs) has demonstrated\nremarkable success in engaging in conversations involving visual inputs, thanks\nto the superior power of large language models (LLMs). Those MLLMs are\ntypically built based on the LLMs, with an image encoder to process images into\nthe token embedding space of the LLMs. However, the integration of visual\nmodality has introduced a unique vulnerability: the MLLM becomes susceptible to\nmalicious visual inputs and prone to generating sensitive or harmful responses,\neven though the LLM has been trained on textual dataset to align with human\nvalue. In this paper, we first raise the question: ``Do the MLLMs possess\nsafety-awareness against malicious image inputs?\". We find that after adding a\nprinciple that specifies the safety requirement into the input of the MLLM, the\nmodel's safety awareness becomes boosted. This phenomenon verifies the\nexistence of MLLM's safety-awareness against image inputs, it is only weakened\nby the modality gap. We then introduce a simple yet effective technique termed\nCoCA, which amplifies the safety-awareness of the MLLM by calibrating its\noutput distribution. Our proposed strategy helps the model reclaim its original\nsafety awareness without losing its original capabilities. We verify the\neffectiveness of our approach on both multimodal safety and understanding\nbenchmarks.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u90e8\u7f72\u5df2\u5c55\u793a\u51fa\u5728\u6d89\u53ca\u89c6\u89c9\u8f93\u5165\u7684\u5bf9\u8bdd\u4e2d\u8fdb\u884c\u4ea4\u4e92\u7684\u663e\u7740\u6210\u529f\uff0c\u8fd9\u8981\u5f52\u529f\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5f3a\u5927\u529f\u80fd\u3002\u8fd9\u4e9b MLLM \u901a\u5e38\u57fa\u4e8e LLM \u6784\u5efa\uff0c\u5e76\u4f7f\u7528\u56fe\u50cf\u7f16\u7801\u5668\u5c06\u56fe\u50cf\u5904\u7406\u5230 LLM \u7684\u6807\u8bb0\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\u7136\u800c\uff0c\u89c6\u89c9\u6a21\u6001\u7684\u96c6\u6210\u5f15\u5165\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u6f0f\u6d1e\uff1aMLLM \u53d8\u5f97\u5bb9\u6613\u53d7\u5230\u6076\u610f\u89c6\u89c9\u8f93\u5165\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u5bb9\u6613\u4ea7\u751f\u654f\u611f\u6216\u6709\u5bb3\u7684\u53cd\u5e94\uff0c\u5373\u4f7f LLM \u5df2\u5728\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4ee5\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e86\u8fd9\u4e2a\u95ee\u9898\uff1a\u201cMLLM \u662f\u5426\u5bf9\u6076\u610f\u56fe\u50cf\u8f93\u5165\u5177\u6709\u5b89\u5168\u610f\u8bc6\uff1f\u201d\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u5c06\u6307\u5b9a\u5b89\u5168\u8981\u6c42\u7684\u539f\u5219\u6dfb\u52a0\u5230 MLLM \u7684\u8f93\u5165\u4e2d\u540e\uff0c\u6a21\u578b\u7684\u5b89\u5168\u610f\u8bc6\u5f97\u5230\u4e86\u63d0\u5347\u3002\u8fd9\u79cd\u73b0\u8c61\u9a8c\u8bc1\u4e86 MLLM \u5bf9\u56fe\u50cf\u8f93\u5165\u7684\u5b89\u5168\u610f\u8bc6\u7684\u5b58\u5728\uff0c\u5b83\u53ea\u662f\u88ab\u6a21\u6001\u5dee\u8ddd\u524a\u5f31\u4e86\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u6280\u672f\uff0c\u79f0\u4e3a CoCA\uff0c\u5b83\u901a\u8fc7\u6821\u51c6\u5176\u8f93\u51fa\u5206\u5e03\u6765\u653e\u5927 MLLM \u7684\u5b89\u5168\u610f\u8bc6\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7b56\u7565\u5e2e\u52a9\u6a21\u578b\u5728\u4e0d\u4e27\u5931\u5176\u539f\u59cb\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u5176\u539f\u59cb\u5b89\u5168\u610f\u8bc6\u3002\u6211\u4eec\u5728\u591a\u6a21\u6001\u5b89\u5168\u548c\u7406\u89e3\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "author": "Jiahui Gao et.al.", "authors": "Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li", "id": "2409.11365v1", "paper_url": "http://arxiv.org/abs/2409.11365v1", "repo": "null"}}