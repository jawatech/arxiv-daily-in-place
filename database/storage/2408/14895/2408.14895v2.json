{"2408.14895": {"publish_time": "2024-08-27", "title": "VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities", "paper_summary": "Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data\n(e.g., images and videos) into symbols, have attracted attention as resources\nenabling knowledge processing and machine learning across modalities. However,\nthe construction of MMKGs for videos consisting of multiple events, such as\ndaily activities, is still in the early stages. In this paper, we construct an\nMMKG based on synchronized multi-view simulated videos of daily activities.\nBesides representing the content of daily life videos as event-centric\nknowledge, our MMKG also includes frame-by-frame fine-grained changes, such as\nbounding boxes within video frames. In addition, we provide support tools for\nquerying our MMKG. As an application example, we demonstrate that our MMKG\nfacilitates benchmarking vision-language models by providing the necessary\nvision-language datasets for a tailored task.", "paper_summary_zh": "\u591a\u6a21\u614b\u77e5\u8b58\u5716\uff08MMKG\uff09\u5c07\u5404\u7a2e\u975e\u7b26\u865f\u6578\u64da\uff08\u4f8b\u5982\uff0c\u5f71\u50cf\u548c\u5f71\u7247\uff09\u8f49\u63db\u70ba\u7b26\u865f\uff0c\u6210\u70ba\u4e00\u7a2e\u8cc7\u6e90\uff0c\u80fd\u8b93\u8de8\u6a21\u614b\u7684\u77e5\u8b58\u8655\u7406\u548c\u6a5f\u5668\u5b78\u7fd2\u6210\u70ba\u53ef\u80fd\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u5305\u542b\u591a\u500b\u4e8b\u4ef6\uff08\u4f8b\u5982\u65e5\u5e38\u751f\u6d3b\u6d3b\u52d5\uff09\u7684\u5f71\u7247\uff0c\u5176 MMKG \u7684\u5efa\u69cb\u4ecd\u8655\u65bc\u65e9\u671f\u968e\u6bb5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u57fa\u65bc\u6bcf\u65e5\u6d3b\u52d5\u7684\u540c\u6b65\u591a\u8996\u89d2\u6a21\u64ec\u5f71\u7247\uff0c\u5efa\u69cb\u4e86\u4e00\u500b MMKG\u3002\u9664\u4e86\u5c07\u65e5\u5e38\u751f\u6d3b\u5f71\u7247\u7684\u5167\u5bb9\u8868\u793a\u70ba\u4ee5\u4e8b\u4ef6\u70ba\u4e2d\u5fc3\u7684\u77e5\u8b58\u5916\uff0c\u6211\u5011\u7684 MMKG \u4e5f\u5305\u542b\u9010\u5e40\u7684\u7d30\u5fae\u8b8a\u5316\uff0c\u4f8b\u5982\u5f71\u7247\u5e40\u4e2d\u7684\u908a\u754c\u6846\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u7528\u65bc\u67e5\u8a62 MMKG \u7684\u652f\u63f4\u5de5\u5177\u3002\u4f5c\u70ba\u61c9\u7528\u7bc4\u4f8b\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u7684 MMKG \u5982\u4f55\u900f\u904e\u63d0\u4f9b\u7279\u5b9a\u4efb\u52d9\u6240\u9700\u7684\u8996\u89ba\u8a9e\u8a00\u8cc7\u6599\u96c6\uff0c\u4f86\u4fc3\u9032\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u57fa\u6e96\u6e2c\u8a66\u3002", "author": "Shusaku Egami et.al.", "authors": "Shusaku Egami, Takahiro Ugai, Swe Nwe Nwe Htun, Ken Fukuda", "id": "2408.14895v2", "paper_url": "http://arxiv.org/abs/2408.14895v2", "repo": "null"}}