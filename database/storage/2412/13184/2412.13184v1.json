{"2412.13184": {"publish_time": "2024-12-17", "title": "Tilted Quantile Gradient Updates for Quantile-Constrained Reinforcement Learning", "paper_summary": "Safe reinforcement learning (RL) is a popular and versatile paradigm to learn\nreward-maximizing policies with safety guarantees. Previous works tend to\nexpress the safety constraints in an expectation form due to the ease of\nimplementation, but this turns out to be ineffective in maintaining safety\nconstraints with high probability. To this end, we move to the\nquantile-constrained RL that enables a higher level of safety without any\nexpectation-form approximations. We directly estimate the quantile gradients\nthrough sampling and provide the theoretical proofs of convergence. Then a\ntilted update strategy for quantile gradients is implemented to compensate the\nasymmetric distributional density, with a direct benefit of return performance.\nExperiments demonstrate that the proposed model fully meets safety requirements\n(quantile constraints) while outperforming the state-of-the-art benchmarks with\nhigher return.", "paper_summary_zh": "\u5b89\u5168\u5f37\u5316\u5b78\u7fd2 (RL) \u662f\u4e00\u7a2e\u6d41\u884c\u4e14\u901a\u7528\u7684\u7bc4\u4f8b\uff0c\u7528\u65bc\u5b78\u7fd2\u5177\u6709\u5b89\u5168\u4fdd\u8b49\u7684\u734e\u52f5\u6700\u5927\u5316\u653f\u7b56\u3002\u7531\u65bc\u6613\u65bc\u5be6\u4f5c\uff0c\u5148\u524d\u7684\u7814\u7a76\u50be\u5411\u65bc\u4ee5\u671f\u671b\u5f62\u5f0f\u8868\u9054\u5b89\u5168\u7d04\u675f\uff0c\u4f46\u9019\u5728\u7dad\u6301\u9ad8\u6a5f\u7387\u7684\u5b89\u5168\u7d04\u675f\u65b9\u9762\u88ab\u8b49\u660e\u662f\u7121\u6548\u7684\u3002\u70ba\u6b64\uff0c\u6211\u5011\u8f49\u5411\u5206\u4f4d\u6578\u7d04\u675f RL\uff0c\u5b83\u53ef\u4ee5\u5728\u6c92\u6709\u4efb\u4f55\u671f\u671b\u5f62\u5f0f\u8fd1\u4f3c\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u66f4\u9ad8\u7684\u5b89\u5168\u7d1a\u5225\u3002\u6211\u5011\u76f4\u63a5\u900f\u904e\u53d6\u6a23\u4f30\u8a08\u5206\u4f4d\u6578\u68af\u5ea6\uff0c\u4e26\u63d0\u4f9b\u6536\u6582\u7684\u7406\u8ad6\u8b49\u660e\u3002\u7136\u5f8c\u5be6\u4f5c\u5206\u4f4d\u6578\u68af\u5ea6\u7684\u50be\u659c\u66f4\u65b0\u7b56\u7565\uff0c\u4ee5\u88dc\u511f\u975e\u5c0d\u7a31\u5206\u4f48\u5bc6\u5ea6\uff0c\u4e26\u76f4\u63a5\u63d0\u5347\u56de\u5831\u6548\u80fd\u3002\u5be6\u9a57\u8b49\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5b8c\u5168\u7b26\u5408\u5b89\u5168\u8981\u6c42\uff08\u5206\u4f4d\u6578\u7d04\u675f\uff09\uff0c\u540c\u6642\u5728\u66f4\u9ad8\u7684\u56de\u5831\u4e0b\u512a\u65bc\u6700\u5148\u9032\u7684\u57fa\u6e96\u3002", "author": "Chenglin Li et.al.", "authors": "Chenglin Li, Guangchun Ruan, Hua Geng", "id": "2412.13184v1", "paper_url": "http://arxiv.org/abs/2412.13184v1", "repo": "https://github.com/CharlieLeeeee/TQPO"}}