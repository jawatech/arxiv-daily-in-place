{"2411.10979": {"publish_time": "2024-11-17", "title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?", "paper_summary": "The advancement of Multimodal Large Language Models (MLLMs) has enabled\nsignificant progress in multimodal understanding, expanding their capacity to\nanalyze video content. However, existing evaluation benchmarks for MLLMs\nprimarily focus on abstract video comprehension, lacking a detailed assessment\nof their ability to understand video compositions, the nuanced interpretation\nof how visual elements combine and interact within highly compiled video\ncontexts. We introduce VidComposition, a new benchmark specifically designed to\nevaluate the video composition understanding capabilities of MLLMs using\ncarefully curated compiled videos and cinematic-level annotations.\nVidComposition includes 982 videos with 1706 multiple-choice questions,\ncovering various compositional aspects such as camera movement, angle, shot\nsize, narrative structure, character actions and emotions, etc. Our\ncomprehensive evaluation of 33 open-source and proprietary MLLMs reveals a\nsignificant performance gap between human and model capabilities. This\nhighlights the limitations of current MLLMs in understanding complex, compiled\nvideo compositions and offers insights into areas for further improvement. The\nleaderboard and evaluation code are available at\nhttps://yunlong10.github.io/VidComposition/.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u8fdb\u6b65\u4fc3\u8fdb\u4e86\u591a\u6a21\u6001\u7406\u89e3\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u6269\u5927\u4e86\u5176\u5206\u6790\u89c6\u9891\u5185\u5bb9\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684 MLLM \u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u4fa7\u91cd\u4e8e\u62bd\u8c61\u89c6\u9891\u7406\u89e3\uff0c\u7f3a\u4e4f\u5bf9\u5176\u7406\u89e3\u89c6\u9891\u6784\u56fe\u7684\u80fd\u529b\u7684\u8be6\u7ec6\u8bc4\u4f30\uff0c\u5373\u89c6\u89c9\u5143\u7d20\u5728\u9ad8\u5ea6\u7f16\u8bd1\u7684\u89c6\u9891\u4e0a\u4e0b\u6587\u4e2d\u5982\u4f55\u7ec4\u5408\u548c\u4ea4\u4e92\u7684\u7ec6\u5fae\u89e3\u91ca\u3002\u6211\u4eec\u5f15\u5165\u4e86 VidComposition\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u4f7f\u7528\u7cbe\u5fc3\u7b56\u5212\u7684\u7f16\u8bd1\u89c6\u9891\u548c\u7535\u5f71\u7ea7\u6ce8\u91ca\u6765\u8bc4\u4f30 MLLM \u7684\u89c6\u9891\u6784\u56fe\u7406\u89e3\u80fd\u529b\u3002VidComposition \u5305\u542b 982 \u4e2a\u89c6\u9891\u548c 1706 \u4e2a\u591a\u9879\u9009\u62e9\u9898\uff0c\u6db5\u76d6\u4e86\u5404\u79cd\u6784\u56fe\u65b9\u9762\uff0c\u5982\u6444\u50cf\u673a\u8fd0\u52a8\u3001\u89d2\u5ea6\u3001\u955c\u5934\u5927\u5c0f\u3001\u53d9\u4e8b\u7ed3\u6784\u3001\u4eba\u7269\u52a8\u4f5c\u548c\u60c5\u7eea\u7b49\u3002\u6211\u4eec\u5bf9 33 \u4e2a\u5f00\u6e90\u548c\u4e13\u6709 MLLM \u7684\u7efc\u5408\u8bc4\u4f30\u63ed\u793a\u4e86\u4eba\u7c7b\u548c\u6a21\u578b\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u8fd9\u7a81\u51fa\u4e86\u5f53\u524d MLLM \u5728\u7406\u89e3\u590d\u6742\u3001\u7f16\u8bd1\u7684\u89c6\u9891\u6784\u56fe\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u9886\u57df\u7684\u89c1\u89e3\u3002\u6392\u884c\u699c\u548c\u8bc4\u4f30\u4ee3\u7801\u53ef\u5728 https://yunlong10.github.io/VidComposition/ \u83b7\u5f97\u3002", "author": "Yunlong Tang et.al.", "authors": "Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, Pooyan Fazli, Chenliang Xu", "id": "2411.10979v1", "paper_url": "http://arxiv.org/abs/2411.10979v1", "repo": "null"}}