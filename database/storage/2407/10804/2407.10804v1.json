{"2407.10804": {"publish_time": "2024-07-15", "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment", "paper_summary": "Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods.", "paper_summary_zh": "\u5c07\u901a\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9069\u61c9\u5230\u7279\u5b9a\u9818\u57df\u6703\u56e0\u70ba\u8cc7\u6599\u5206\u4f48\u4e0d\u540c\u800c\u9762\u81e8\u6975\u5927\u7684\u6311\u6230\u3002\u9019\u7a2e\u9069\u61c9\u901a\u5e38\u9700\u8981\u6301\u7e8c\u5728\u5927\u91cf\u7684\u7279\u5b9a\u9818\u57df\u8a9e\u6599\u5eab\u4e0a\u9032\u884c\u9810\u8a13\u7df4\uff0c\u4ee5\u5229\u65bc\u77e5\u8b58\u8a18\u61b6\uff0c\u63a5\u8457\u518d\u9032\u884c\u8a13\u7df4\uff0c\u4ee5\u61c9\u7528\u9019\u4e9b\u77e5\u8b58\u4f86\u9075\u5faa\u4eba\u985e\u7684\u6307\u793a\u548c\u504f\u597d\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u53ef\u80fd\u6703\u56e0\u70ba\u7f3a\u4e4f\u77e5\u8b58\u5229\u7528\u7684\u610f\u8b58\u800c\u5c0e\u81f4\u77e5\u8b58\u8a18\u61b6\u6548\u7387\u4e0d\u5f70\uff0c\u4e26\u5c0d LLM \u65bd\u52a0\u6975\u5927\u7684\u9700\u6c42\uff0c\u4ee5\u540c\u6642\u5b78\u7fd2\u77e5\u8b58\u5229\u7528\u548c\u683c\u5f0f\u5c0d\u9f4a\uff0c\u800c\u8a13\u7df4\u6a23\u672c\u6709\u9650\u3002\u70ba\u4e86\u4fc3\u9032 LLM \u7684\u9818\u57df\u9069\u61c9\uff0c\u6211\u5011\u4fee\u6539\u4e86\u9019\u500b\u6d41\u7a0b\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u9818\u57df\u9069\u61c9\u67b6\u69cb\uff0c\u5305\u62ec\u9818\u57df\u77e5\u8b58\u5b78\u7fd2\u548c\u4e00\u822c\u683c\u5f0f\u5c0d\u9f4a\uff0c\u7a31\u70ba Mix-CPT\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u9032\u884c\u77e5\u8b58\u6df7\u5408\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u540c\u6642\u5c08\u6ce8\u65bc\u77e5\u8b58\u8a18\u61b6\u548c\u5229\u7528\uff0c\u4ee5\u5229\u65bc\u76f8\u4e92\u5f37\u5316\u3002\u70ba\u4e86\u907f\u514d\u5728\u6301\u7e8c\u9810\u8a13\u7df4\u904e\u7a0b\u4e2d\u767c\u751f\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u6211\u5011\u9032\u4e00\u6b65\u7d0d\u5165 logit \u4ea4\u63db\u81ea\u6211\u84b8\u993e\u7d04\u675f\u3002\u96a8\u5f8c\uff0c\u5229\u7528\u5728\u6301\u7e8c\u9810\u8a13\u7df4\u671f\u9593\u7372\u5f97\u7684\u77e5\u8b58\u548c\u80fd\u529b\uff0c\u6211\u5011\u6709\u6548\u5730\u57f7\u884c\u6307\u4ee4\u8abf\u6574\u548c\u5c0d\u9f4a\uff0c\u4e26\u4f7f\u7528\u5c11\u91cf\u7684\u901a\u7528\u8a13\u7df4\u6a23\u672c\u4f86\u9054\u6210\u683c\u5f0f\u5c0d\u9f4a\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u50b3\u7d71\u7684\u9069\u61c9\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u63d0\u51fa\u7684 Mix-CPT \u67b6\u69cb\u53ef\u4ee5\u540c\u6642\u6539\u5584 LLM \u5728\u76ee\u6a19\u548c\u4e00\u822c\u9818\u57df\u7684\u4efb\u52d9\u89e3\u6c7a\u80fd\u529b\u3002", "author": "Jinhao Jiang et.al.", "authors": "Jinhao Jiang, Junyi Li, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen", "id": "2407.10804v1", "paper_url": "http://arxiv.org/abs/2407.10804v1", "repo": "null"}}