{"2411.14797": {"publish_time": "2024-11-22", "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision", "paper_summary": "Multimodal RLHF usually happens after supervised finetuning (SFT) stage to\ncontinually improve vision-language models' (VLMs) comprehension. Conventional\nwisdom holds its superiority over continual SFT during this preference\nalignment stage. In this paper, we observe that the inherent value of\nmultimodal RLHF lies in its negative supervision, the logit of the rejected\nresponses. We thus propose a novel negative supervised finetuning (nSFT)\napproach that fully excavates these information resided. Our nSFT disentangles\nthis negative supervision in RLHF paradigm, and continually aligns VLMs with a\nsimple SFT loss. This is more memory efficient than multimodal RLHF where 2\n(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The\neffectiveness of nSFT is rigorously proved by comparing it with various\nmultimodal RLHF approaches, across different dataset sources, base VLMs and\nevaluation metrics. Besides, fruitful of ablations are provided to support our\nhypothesis. We hope this paper will stimulate further research to properly\nalign large vision language models.", "paper_summary_zh": "\u591a\u6a21\u6001 RLHF \u901a\u5e38\u5728\u76d1\u7763\u5fae\u8c03 (SFT) \u9636\u6bb5\u4e4b\u540e\u53d1\u751f\uff0c\u4ee5\u6301\u7eed\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u7406\u89e3\u529b\u3002\u4f20\u7edf\u667a\u6167\u8ba4\u4e3a\u5728\u8fd9\u4e00\u504f\u597d\u5bf9\u9f50\u9636\u6bb5\uff0c\u5b83\u4f18\u4e8e\u6301\u7eed\u7684 SFT\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u591a\u6a21\u6001 RLHF \u7684\u5185\u5728\u4ef7\u503c\u5728\u4e8e\u5176\u8d1f\u76d1\u7763\uff0c\u5373\u88ab\u62d2\u7edd\u7684\u54cd\u5e94\u7684\u5bf9\u6570\u51e0\u7387\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d1f\u76d1\u7763\u5fae\u8c03 (nSFT) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5145\u5206\u6316\u6398\u4e86\u8fd9\u4e9b\u9a7b\u7559\u7684\u4fe1\u606f\u3002\u6211\u4eec\u7684 nSFT \u89e3\u5f00\u4e86 RLHF \u8303\u5f0f\u4e2d\u7684\u8d1f\u76d1\u7763\uff0c\u5e76\u6301\u7eed\u4f7f\u7528\u7b80\u5355\u7684 SFT \u635f\u5931\u5bf9\u9f50 VLM\u3002\u8fd9\u6bd4\u591a\u6a21\u6001 RLHF \u66f4\u8282\u7701\u5185\u5b58\uff0c\u5728\u591a\u6a21\u6001 RLHF \u4e2d\u4e25\u683c\u9700\u8981 2\uff08\u4f8b\u5982\uff0cDPO\uff09\u6216 4\uff08\u4f8b\u5982\uff0cPPO\uff09\u4e2a\u5927\u578b VLM\u3002\u901a\u8fc7\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u6e90\u3001\u57fa\u672c VLM \u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u5c06 nSFT \u4e0e\u5404\u79cd\u591a\u6a21\u6001 RLHF \u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u4e25\u683c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6d88\u878d\u5b9e\u9a8c\u6765\u652f\u6301\u6211\u4eec\u7684\u5047\u8bbe\u3002\u6211\u4eec\u5e0c\u671b\u672c\u6587\u5c06\u6fc0\u53d1\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u4ee5\u6b63\u786e\u5bf9\u9f50\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "author": "Ke Zhu et.al.", "authors": "Ke Zhu, Yu Wang, Yanpeng Sun, Qiang Chen, Jiangjiang Liu, Gang Zhang, Jingdong Wang", "id": "2411.14797v1", "paper_url": "http://arxiv.org/abs/2411.14797v1", "repo": "null"}}