{"2412.18091": {"publish_time": "2024-12-24", "title": "AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning", "paper_summary": "As deep neural networks (DNNs) are increasingly deployed on edge devices,\noptimizing models for constrained computational resources is critical. Existing\nauto-pruning methods face challenges due to the diversity of DNN models,\nvarious operators (e.g., filters), and the difficulty in balancing pruning\ngranularity with model accuracy. To address these limitations, we introduce\nAutoSculpt, a pattern-based automated pruning framework designed to enhance\nefficiency and accuracy by leveraging graph learning and deep reinforcement\nlearning (DRL). AutoSculpt automatically identifies and prunes regular patterns\nwithin DNN architectures that can be recognized by existing inference engines,\nenabling runtime acceleration. Three key steps in AutoSculpt include: (1)\nConstructing DNNs as graphs to encode their topology and parameter\ndependencies, (2) embedding computationally efficient pruning patterns, and (3)\nutilizing DRL to iteratively refine auto-pruning strategies until the optimal\nbalance between compression and accuracy is achieved. Experimental results\ndemonstrate the effectiveness of AutoSculpt across various architectures,\nincluding ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning\nrates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming\nall baselines. The codes can be available at\nhttps://anonymous.4open.science/r/AutoSculpt-DDA0", "paper_summary_zh": "\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u9488\u5bf9\u53d7\u9650\u8ba1\u7b97\u8d44\u6e90\u4f18\u5316\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u81ea\u52a8\u526a\u679d\u65b9\u6cd5\u7531\u4e8e DNN \u6a21\u578b\u7684\u591a\u6837\u6027\u3001\u5404\u79cd\u8fd0\u7b97\u7b26\uff08\u4f8b\u5982\u8fc7\u6ee4\u5668\uff09\u4ee5\u53ca\u5e73\u8861\u526a\u679d\u7c92\u5ea6\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u7684\u96be\u5ea6\u800c\u9762\u4e34\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 AutoSculpt\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u5f0f\u7684\u81ea\u52a8\u526a\u679d\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5229\u7528\u56fe\u5b66\u4e60\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60 (DRL) \u6765\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002AutoSculpt \u81ea\u52a8\u8bc6\u522b\u548c\u526a\u679d DNN \u67b6\u6784\u4e2d\u7684\u89c4\u5219\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u53ef\u4ee5\u88ab\u73b0\u6709\u7684\u63a8\u7406\u5f15\u64ce\u8bc6\u522b\uff0c\u4ece\u800c\u5b9e\u73b0\u8fd0\u884c\u65f6\u52a0\u901f\u3002AutoSculpt \u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\u5305\u62ec\uff1a(1) \u5c06 DNN \u6784\u5efa\u4e3a\u56fe\u4ee5\u7f16\u7801\u5176\u62d3\u6251\u548c\u53c2\u6570\u4f9d\u8d56\u5173\u7cfb\uff0c(2) \u5d4c\u5165\u8ba1\u7b97\u9ad8\u6548\u7684\u526a\u679d\u6a21\u5f0f\uff0c\u4ee5\u53ca (3) \u5229\u7528 DRL \u8fed\u4ee3\u4f18\u5316\u81ea\u52a8\u526a\u679d\u7b56\u7565\uff0c\u76f4\u5230\u5b9e\u73b0\u538b\u7f29\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86 AutoSculpt \u5728\u5404\u79cd\u67b6\u6784\uff08\u5305\u62ec ResNet\u3001MobileNet\u3001VGG \u548c Vision Transformer\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 90% \u7684\u526a\u679d\u7387\u548c\u8fd1 18% \u7684 FLOP \u51cf\u5c11\u6539\u8fdb\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u3002\u4ee3\u7801\u53ef\u5728 https://anonymous.4open.science/r/AutoSculpt-DDA0 \u83b7\u5f97", "author": "Lixian Jing et.al.", "authors": "Lixian Jing, Jianpeng Qi, Junyu Dong, Yanwei Yu", "id": "2412.18091v1", "paper_url": "http://arxiv.org/abs/2412.18091v1", "repo": "null"}}