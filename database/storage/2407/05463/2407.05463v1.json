{"2407.05463": {"publish_time": "2024-07-07", "title": "Training Task Experts through Retrieval Based Distillation", "paper_summary": "One of the most reliable ways to create deployable models for specialized\ntasks is to obtain an adequate amount of high-quality task-specific data.\nHowever, for specialized tasks, often such datasets do not exist. Existing\nmethods address this by creating such data from large language models (LLMs)\nand then distilling such knowledge into smaller models. However, these methods\nare limited by the quality of the LLMs output, and tend to generate repetitive\nor incorrect data. In this work, we present Retrieval Based Distillation\n(ReBase), a method that first retrieves data from rich online sources and then\ntransforms them into domain-specific data. This method greatly enhances data\ndiversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills\nthe reasoning capacity of LLMs. We test our method on 4 benchmarks and results\nshow that our method significantly improves performance by up to 7.8% on SQuAD,\n1.37% on MNLI, and 1.94% on BigBench-Hard.", "paper_summary_zh": "\u5efa\u7acb\u53ef\u90e8\u7f72\u6a21\u578b\u4ee5\u57f7\u884c\u5c08\u696d\u4efb\u52d9\u6700\u53ef\u9760\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u5c31\u662f\u53d6\u5f97\u8db3\u5920\u6578\u91cf\u7684\u7279\u5b9a\u4efb\u52d9\u9ad8\u54c1\u8cea\u8cc7\u6599\u3002\u4e0d\u904e\uff0c\u5c0d\u65bc\u5c08\u696d\u4efb\u52d9\u800c\u8a00\uff0c\u901a\u5e38\u4e26\u4e0d\u5b58\u5728\u6b64\u985e\u8cc7\u6599\u96c6\u3002\u73fe\u6709\u65b9\u6cd5\u6703\u900f\u904e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5efa\u7acb\u6b64\u985e\u8cc7\u6599\uff0c\u7136\u5f8c\u5c07\u6b64\u985e\u77e5\u8b58\u63d0\u7149\u81f3\u8f03\u5c0f\u7684\u6a21\u578b\u4e2d\uff0c\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u53d7\u5230 LLM \u8f38\u51fa\u54c1\u8cea\u7684\u9650\u5236\uff0c\u800c\u4e14\u50be\u5411\u7522\u751f\u91cd\u8907\u6216\u4e0d\u6b63\u78ba\u7684\u8cc7\u6599\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u57fa\u65bc\u6aa2\u7d22\u7684\u63d0\u7149 (ReBase)\uff0c\u9019\u662f\u4e00\u500b\u65b9\u6cd5\uff0c\u5b83\u6703\u5148\u5f9e\u8c50\u5bcc\u7684\u7dda\u4e0a\u4f86\u6e90\u4e2d\u6aa2\u7d22\u8cc7\u6599\uff0c\u7136\u5f8c\u5c07\u5b83\u5011\u8f49\u63db\u6210\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\u3002\u6b64\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u4e86\u8cc7\u6599\u7684\u591a\u6a23\u6027\u3002\u6b64\u5916\uff0cReBase \u6703\u7522\u751f\u601d\u8003\u93c8\u63a8\u7406\uff0c\u4e26\u63d0\u7149 LLM \u7684\u63a8\u7406\u80fd\u529b\u3002\u6211\u5011\u5728 4 \u500b\u57fa\u6e96\u4e0a\u6e2c\u8a66\u4e86\u6211\u5011\u7684\u9019\u9805\u65b9\u6cd5\uff0c\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684\u9019\u9805\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u4e86\u6548\u80fd\uff0c\u5728 SQuAD \u4e0a\u63d0\u5347\u4e86 7.8%\u3001\u5728 MNLI \u4e0a\u63d0\u5347\u4e86 1.37%\u3001\u5728 BigBench-Hard \u4e0a\u63d0\u5347\u4e86 1.94%\u3002", "author": "Jiaxin Ge et.al.", "authors": "Jiaxin Ge, Xueying Jia, Vijay Viswanathan, Hongyin Luo, Graham Neubig", "id": "2407.05463v1", "paper_url": "http://arxiv.org/abs/2407.05463v1", "repo": "null"}}