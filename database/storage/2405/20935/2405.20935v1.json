{"2405.20935": {"publish_time": "2024-05-31", "title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice", "paper_summary": "The increasing size of deep neural networks necessitates effective model\ncompression to improve computational efficiency and reduce their memory\nfootprint. Sparsity and quantization are two prominent compression methods that\nhave individually demonstrated significant reduction in computational and\nmemory footprints while preserving model accuracy. While effective, the\ninterplay between these two methods remains an open question. In this paper, we\ninvestigate the interaction between these two methods and assess whether their\ncombination impacts final model accuracy. We mathematically prove that applying\nsparsity before quantization is the optimal sequence for these operations,\nminimizing error in computation. Our empirical studies across a wide range of\nmodels, including OPT and Llama model families (125M-8B) and ViT corroborate\nthese theoretical findings. In addition, through rigorous analysis, we\ndemonstrate that sparsity and quantization are not orthogonal; their\ninteraction can significantly harm model accuracy, with quantization error\nplaying a dominant role in this degradation. Our findings extend to the\nefficient deployment of large models in resource-limited compute platforms and\nreduce serving cost, offering insights into best practices for applying these\ncompression methods to maximize efficacy without compromising accuracy.", "paper_summary_zh": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u89c4\u6a21\u65e5\u76ca\u6269\u5927\uff0c\u9700\u8981\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u5e76\u51cf\u5c11\u5176\u5185\u5b58\u5360\u7528\u3002\u7a00\u758f\u6027\u548c\u91cf\u5316\u662f\u4e24\u79cd\u7a81\u51fa\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u5b83\u4eec\u5206\u522b\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u5360\u7528\u3002\u867d\u7136\u6709\u6548\uff0c\u4f46\u8fd9\u4e24\u4e2a\u65b9\u6cd5\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u4ecd\u7136\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u8bc4\u4f30\u5b83\u4eec\u7684\u7ec4\u5408\u662f\u5426\u4f1a\u5f71\u54cd\u6700\u7ec8\u7684\u6a21\u578b\u51c6\u786e\u6027\u3002\u6211\u4eec\u4ece\u6570\u5b66\u4e0a\u8bc1\u660e\uff0c\u5728\u91cf\u5316\u4e4b\u524d\u5e94\u7528\u7a00\u758f\u6027\u662f\u8fd9\u4e9b\u64cd\u4f5c\u7684\u6700\u4f73\u5e8f\u5217\uff0c\u4ece\u800c\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u8ba1\u7b97\u8bef\u5dee\u3002\u6211\u4eec\u5bf9\u5305\u62ec OPT \u548c Llama \u6a21\u578b\u65cf\uff08125M-8B\uff09\u548c ViT \u5728\u5185\u7684\u5404\u79cd\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u8bc1\u7814\u7a76\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u7406\u8bba\u53d1\u73b0\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u5206\u6790\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u7a00\u758f\u6027\u548c\u91cf\u5316\u4e0d\u662f\u6b63\u4ea4\u7684\uff1b\u5b83\u4eec\u7684\u76f8\u4e92\u4f5c\u7528\u4f1a\u4e25\u91cd\u635f\u5bb3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u800c\u91cf\u5316\u8bef\u5dee\u5728\u8fd9\u4e00\u9000\u5316\u4e2d\u8d77\u4e3b\u5bfc\u4f5c\u7528\u3002\u6211\u4eec\u7684\u53d1\u73b0\u6269\u5c55\u5230\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8ba1\u7b97\u5e73\u53f0\u4e2d\u6709\u6548\u90e8\u7f72\u5927\u578b\u6a21\u578b\uff0c\u5e76\u964d\u4f4e\u670d\u52a1\u6210\u672c\uff0c\u4e3a\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u5e94\u7528\u8fd9\u4e9b\u538b\u7f29\u65b9\u6cd5\u4ee5\u6700\u5927\u5316\u529f\u6548\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "author": "Simla Burcu Harma et.al.", "authors": "Simla Burcu Harma, Ayan Chakraborty, Elizaveta Kostenok, Danila Mishin, Dongho Ha, Babak Falsafi, Martin Jaggi, Ming Liu, Yunho Oh, Suvinay Subramanian, Amir Yazdanbakhsh", "id": "2405.20935v1", "paper_url": "http://arxiv.org/abs/2405.20935v1", "repo": "null"}}