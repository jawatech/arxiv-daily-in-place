{"2405.10936": {"publish_time": "2024-05-17", "title": "A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers", "paper_summary": "The rapid development of Large Language Models (LLMs) demonstrates remarkable\nmultilingual capabilities in natural language processing, attracting global\nattention in both academia and industry. To mitigate potential discrimination\nand enhance the overall usability and accessibility for diverse language user\ngroups, it is important for the development of language-fair technology.\nDespite the breakthroughs of LLMs, the investigation into the multilingual\nscenario remains insufficient, where a comprehensive survey to summarize recent\napproaches, developments, limitations, and potential solutions is desirable. To\nthis end, we provide a survey with multiple perspectives on the utilization of\nLLMs in the multilingual scenario. We first rethink the transitions between\nprevious and current research on pre-trained language models. Then we introduce\nseveral perspectives on the multilingualism of LLMs, including training and\ninference methods, model security, multi-domain with language culture, and\nusage of datasets. We also discuss the major challenges that arise in these\naspects, along with possible solutions. Besides, we highlight future research\ndirections that aim at further enhancing LLMs with multilingualism. The survey\naims to help the research community address multilingual problems and provide a\ncomprehensive understanding of the core concepts, key techniques, and latest\ndevelopments in multilingual natural language processing based on LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u767c\u5c55\u5c55\u793a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4e2d\u986f\u8457\u7684\u591a\u8a9e\u8a00\u80fd\u529b\uff0c\u5438\u5f15\u4e86\u5b78\u8853\u754c\u548c\u7522\u696d\u754c\u7684\u5168\u7403\u95dc\u6ce8\u3002\u70ba\u4e86\u6e1b\u8f15\u6f5b\u5728\u7684\u6b67\u8996\u4e26\u589e\u5f37\u4e0d\u540c\u8a9e\u8a00\u4f7f\u7528\u8005\u7fa4\u9ad4\u7684\u6574\u9ad4\u53ef\u7528\u6027\u548c\u53ef\u53ca\u6027\uff0c\u8a9e\u8a00\u516c\u5e73\u6280\u8853\u7684\u767c\u5c55\u975e\u5e38\u91cd\u8981\u3002\u5118\u7ba1 LLM \u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u5c0d\u591a\u8a9e\u8a00\u5834\u666f\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u9032\u884c\u5168\u9762\u7684\u8abf\u67e5\u4f86\u7e3d\u7d50\u6700\u8fd1\u7684\u65b9\u6cd5\u3001\u767c\u5c55\u3001\u9650\u5236\u548c\u6f5b\u5728\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u9805\u8abf\u67e5\uff0c\u5c0d LLM \u5728\u591a\u8a9e\u8a00\u5834\u666f\u4e2d\u7684\u5229\u7528\u63d0\u51fa\u4e86\u591a\u7a2e\u89c0\u9ede\u3002\u6211\u5011\u9996\u5148\u91cd\u65b0\u601d\u8003\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u4e4b\u524d\u548c\u7576\u524d\u7814\u7a76\u4e4b\u9593\u7684\u8f49\u8b8a\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 LLM \u591a\u8a9e\u8a00\u7684\u5e7e\u500b\u89c0\u9ede\uff0c\u5305\u62ec\u8a13\u7df4\u548c\u63a8\u7406\u65b9\u6cd5\u3001\u6a21\u578b\u5b89\u5168\u6027\u3001\u5177\u6709\u8a9e\u8a00\u6587\u5316\u7684\u591a\u9818\u57df\u4ee5\u53ca\u8cc7\u6599\u96c6\u7684\u4f7f\u7528\u3002\u6211\u5011\u9084\u8a0e\u8ad6\u4e86\u9019\u4e9b\u65b9\u9762\u51fa\u73fe\u7684\u4e3b\u8981\u6311\u6230\u4ee5\u53ca\u53ef\u80fd\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u6b64\u5916\uff0c\u6211\u5011\u91cd\u9ede\u4ecb\u7d39\u4e86\u65e8\u5728\u9032\u4e00\u6b65\u589e\u5f37\u5177\u6709\u591a\u8a9e\u8a00\u80fd\u529b\u7684 LLM \u7684\u672a\u4f86\u7814\u7a76\u65b9\u5411\u3002\u8a72\u8abf\u67e5\u65e8\u5728\u5e6b\u52a9\u7814\u7a76\u793e\u7fa4\u89e3\u6c7a\u591a\u8a9e\u8a00\u554f\u984c\uff0c\u4e26\u63d0\u4f9b\u57fa\u65bc LLM \u7684\u591a\u8a9e\u8a00\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u6838\u5fc3\u6982\u5ff5\u3001\u95dc\u9375\u6280\u8853\u548c\u6700\u65b0\u767c\u5c55\u7684\u5168\u9762\u7406\u89e3\u3002", "author": "Kaiyu Huang et.al.", "authors": "Kaiyu Huang, Fengran Mo, Hongliang Li, You Li, Yuanchi Zhang, Weijian Yi, Yulong Mao, Jinchen Liu, Yuzhuang Xu, Jinan Xu, Jian-Yun Nie, Yang Liu", "id": "2405.10936v1", "paper_url": "http://arxiv.org/abs/2405.10936v1", "repo": "https://github.com/kaiyuhwang/mllm-survey"}}