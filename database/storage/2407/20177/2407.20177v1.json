{"2407.20177": {"publish_time": "2024-07-29", "title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs", "paper_summary": "To ensure performance on a diverse set of downstream tasks, LLMs are\npretrained via data mixtures over different domains. In this work, we\ndemonstrate that the optimal data composition for a fixed compute budget varies\ndepending on the scale of the training data, suggesting that the common\npractice of empirically determining an optimal composition using small-scale\nexperiments will not yield the optimal data mixtures when scaling up to the\nfinal model. To address this challenge, we propose *AutoScale*, an automated\ntool that finds a compute-optimal data composition for training at any desired\ntarget scale. AutoScale first determines the optimal composition at a small\nscale using a novel bilevel optimization framework, Direct Data Optimization\n(*DDO*), and then fits a predictor to estimate the optimal composition at\nlarger scales. The predictor's design is inspired by our theoretical analysis\nof scaling laws related to data composition, which could be of independent\ninterest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2\nLarge) on RedPajama dataset, AutoScale decreases validation perplexity at least\n25% faster than any baseline with up to 38% speed up compared to without\nreweighting, achieving the best overall performance across downstream tasks. On\npre-training Encoder-only LMs (BERT) with masked language modeling, DDO is\nshown to decrease loss on all domains while visibly improving average task\nperformance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by\n5.9% compared with without reweighting. AutoScale speeds up training by up to\n28%. Our codes are open-sourced.", "paper_summary_zh": "<paragraph>\u70ba\u4e86\u78ba\u4fdd\u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\uff0cLLM \u6703\u900f\u904e\u4e0d\u540c\u9818\u57df\u7684\u8cc7\u6599\u6df7\u5408\u9032\u884c\u9810\u5148\u8a13\u7df4\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86\u5728\u56fa\u5b9a\u7684\u904b\u7b97\u9810\u7b97\u4e0b\uff0c\u6700\u4f73\u7684\u8cc7\u6599\u7d44\u6210\u6703\u4f9d\u8a13\u7df4\u8cc7\u6599\u7684\u898f\u6a21\u800c\u6709\u6240\u4e0d\u540c\uff0c\u9019\u8868\u793a\u5728\u64f4\u5145\u5230\u6700\u7d42\u6a21\u578b\u6642\uff0c\u4f7f\u7528\u5c0f\u898f\u6a21\u5be6\u9a57\u4f86\u7d93\u9a57\u6027\u5730\u6c7a\u5b9a\u6700\u4f73\u7d44\u6210\u7684\u5e38\u898b\u505a\u6cd5\uff0c\u5c07\u7121\u6cd5\u7522\u751f\u6700\u4f73\u7684\u8cc7\u6599\u6df7\u5408\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u9805\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u300cAutoScale\u300d\uff0c\u4e00\u500b\u81ea\u52d5\u5316\u5de5\u5177\uff0c\u53ef\u4ee5\u70ba\u4efb\u4f55\u6240\u9700\u7684\u76ee\u6a19\u898f\u6a21\u7684\u8a13\u7df4\u627e\u5230\u4e00\u500b\u904b\u7b97\u6700\u4f73\u7684\u8cc7\u6599\u7d44\u6210\u3002AutoScale \u9996\u5148\u4f7f\u7528\u4e00\u7a2e\u65b0\u7a4e\u7684\u96d9\u5c64\u6b21\u6700\u4f73\u5316\u67b6\u69cb\uff0c\u76f4\u63a5\u8cc7\u6599\u6700\u4f73\u5316\uff08DDO\uff09\uff0c\u4f86\u6c7a\u5b9a\u5c0f\u898f\u6a21\u7684\u6700\u4f73\u7d44\u6210\uff0c\u7136\u5f8c\u64ec\u5408\u4e00\u500b\u9810\u6e2c\u5668\u4f86\u4f30\u8a08\u8f03\u5927\u898f\u6a21\u7684\u6700\u4f73\u7d44\u6210\u3002\u9810\u6e2c\u5668\u7684\u8a2d\u8a08\u9748\u611f\u4f86\u81ea\u6211\u5011\u5c0d\u8207\u8cc7\u6599\u7d44\u6210\u76f8\u95dc\u7684\u898f\u6a21\u5b9a\u5f8b\u7684\u7406\u8ad6\u5206\u6790\uff0c\u9019\u53ef\u80fd\u662f\u7368\u7acb\u7684\u8208\u8da3\u3002\u5728\u4f7f\u7528\u9810\u8a13\u7df4 774M \u50c5\u89e3\u78bc\u5668 LMs\uff08GPT-2 Large\uff09\u65bc RedPajama \u8cc7\u6599\u96c6\u9032\u884c\u7684\u5be6\u8b49\u7814\u7a76\u4e2d\uff0cAutoScale \u9a57\u8b49\u56f0\u60d1\u5ea6\u964d\u4f4e\u7684\u901f\u5ea6\u6bd4\u4efb\u4f55\u57fa\u7dda\u5feb\u81f3\u5c11 25%\uff0c\u8207\u4e0d\u91cd\u65b0\u52a0\u6b0a\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u5347\u591a\u9054 38%\uff0c\u5728\u6240\u6709\u4e0b\u6e38\u4efb\u52d9\u4e2d\u90fd\u53d6\u5f97\u6700\u4f73\u7684\u6574\u9ad4\u6548\u80fd\u3002\u5728\u4f7f\u7528\u906e\u7f69\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u50c5\u7de8\u78bc\u5668 LMs\uff08BERT\uff09\u6642\uff0cDDO \u5df2\u88ab\u8b49\u660e\u53ef\u4ee5\u6e1b\u5c11\u6240\u6709\u9818\u57df\u7684\u640d\u5931\uff0c\u540c\u6642\u5728 GLUE \u57fa\u6e96\u4e0a\u5c07\u5e73\u5747\u4efb\u52d9\u6548\u80fd\u986f\u8457\u63d0\u5347 8.7%\uff0c\u5728\u5927\u578b QA \u8cc7\u6599\u96c6\uff08SQuAD\uff09\u4e0a\u63d0\u5347 5.9%\uff0c\u8207\u4e0d\u91cd\u65b0\u52a0\u6b0a\u76f8\u6bd4\u3002AutoScale \u5c07\u8a13\u7df4\u901f\u5ea6\u63d0\u5347\u591a\u9054 28%\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u662f\u958b\u6e90\u7684\u3002</paragraph>", "author": "Feiyang Kang et.al.", "authors": "Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, Ruoxi Jia", "id": "2407.20177v1", "paper_url": "http://arxiv.org/abs/2407.20177v1", "repo": "null"}}