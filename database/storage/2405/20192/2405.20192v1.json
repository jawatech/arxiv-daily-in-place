{"2405.20192": {"publish_time": "2024-05-30", "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners", "paper_summary": "Fine-tuning on task-specific question-answer pairs is a predominant method\nfor enhancing the performance of instruction-tuned large language models (LLMs)\non downstream tasks. However, in certain specialized domains, such as\nhealthcare or harmless content generation, it is nearly impossible to obtain a\nlarge volume of high-quality data that matches the downstream distribution. To\nimprove the performance of LLMs in data-scarce domains with domain-mismatched\ndata, we re-evaluated the Transformer architecture and discovered that not all\nparameter updates during fine-tuning contribute positively to downstream\nperformance. Our analysis reveals that within the self-attention and\nfeed-forward networks, only the fine-tuned attention parameters are\nparticularly beneficial when the training set's distribution does not fully\nalign with the test set. Based on this insight, we propose an effective\ninference-time intervention method: \\uline{T}raining \\uline{A}ll parameters but\n\\uline{I}nferring with only \\uline{A}ttention (\\trainallInfAttn). We\nempirically validate \\trainallInfAttn using two general instruction-tuning\ndatasets and evaluate it on seven downstream tasks involving math, reasoning,\nand knowledge understanding across LLMs of different parameter sizes and\nfine-tuning techniques. Our comprehensive experiments demonstrate that\n\\trainallInfAttn achieves superior improvements compared to both the fully\nfine-tuned model and the base model in most scenarios, with significant\nperformance gains. The high tolerance of \\trainallInfAttn to data mismatches\nmakes it resistant to jailbreaking tuning and enhances specialized tasks using\ngeneral data.", "paper_summary_zh": "<paragraph>\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u95ee\u7b54\u5bf9\u8fdb\u884c\u5fae\u8c03\u662f\u4e00\u79cd\u589e\u5f3a\u6307\u4ee4\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u6027\u80fd\u7684\u4e3b\u8981\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5728\u67d0\u4e9b\u4e13\u4e1a\u9886\u57df\uff08\u5982\u533b\u7597\u4fdd\u5065\u6216\u65e0\u5bb3\u5185\u5bb9\u751f\u6210\uff09\u4e2d\uff0c\u51e0\u4e4e\u4e0d\u53ef\u80fd\u83b7\u5f97\u4e0e\u4e0b\u6e38\u5206\u5e03\u5339\u914d\u7684\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u3002\u4e3a\u4e86\u63d0\u9ad8 LLM \u5728\u6570\u636e\u7a00\u7f3a\u4e14\u6570\u636e\u4e0e\u9886\u57df\u4e0d\u5339\u914d\u7684\u9886\u57df\u4e2d\u7684\u6027\u80fd\uff0c\u6211\u4eec\u91cd\u65b0\u8bc4\u4f30\u4e86 Transformer \u67b6\u6784\uff0c\u5e76\u53d1\u73b0\u5fae\u8c03\u671f\u95f4\u5e76\u975e\u6240\u6709\u53c2\u6570\u66f4\u65b0\u90fd\u80fd\u5bf9\u4e0b\u6e38\u6027\u80fd\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5728\u81ea\u6ce8\u610f\u529b\u548c\u524d\u9988\u7f51\u7edc\u4e2d\uff0c\u53ea\u6709\u7ecf\u8fc7\u5fae\u8c03\u7684\u6ce8\u610f\u529b\u53c2\u6570\u5728\u8bad\u7ec3\u96c6\u5206\u5e03\u4e0e\u6d4b\u8bd5\u96c6\u4e0d\u5b8c\u5168\u4e00\u81f4\u65f6\u624d\u7279\u522b\u6709\u76ca\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u95f4\u5e72\u9884\u65b9\u6cd5\uff1a\\uline{T}raining \\uline{A}ll parameters but \\uline{I}nferring with only \\uline{A}ttention (\\trainallInfAttn)\u3002\u6211\u4eec\u4f7f\u7528\u4e24\u4e2a\u901a\u7528\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u5bf9 \\trainallInfAttn \u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5e76\u5728\u6d89\u53ca\u4e0d\u540c\u53c2\u6570\u5927\u5c0f\u548c\u5fae\u8c03\u6280\u672f\u7684 LLM \u4e2d\u5bf9\u4e03\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u5305\u62ec\u6570\u5b66\u3001\u63a8\u7406\u548c\u77e5\u8bc6\u7406\u89e3\uff09\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u4e0e\u7ecf\u8fc7\u5b8c\u5168\u5fae\u8c03\u7684\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u76f8\u6bd4\uff0c\\trainallInfAttn \u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6539\u8fdb\uff0c\u5e76\u83b7\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\\trainallInfAttn \u5bf9\u6570\u636e\u4e0d\u5339\u914d\u7684\u9ad8\u5ea6\u5bb9\u5fcd\u6027\u4f7f\u5176\u80fd\u591f\u62b5\u6297\u8d8a\u72f1\u8c03\u4f18\uff0c\u5e76\u4f7f\u7528\u901a\u7528\u6570\u636e\u589e\u5f3a\u4e13\u4e1a\u4efb\u52a1\u3002</paragraph>", "author": "Shuyang Jiang et.al.", "authors": "Shuyang Jiang, Yusheng Liao, Ya Zhang, Yu Wang, Yanfeng Wang", "id": "2405.20192v1", "paper_url": "http://arxiv.org/abs/2405.20192v1", "repo": "null"}}