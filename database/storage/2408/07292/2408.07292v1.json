{"2408.07292": {"publish_time": "2024-08-14", "title": "LiPCoT: Linear Predictive Coding based Tokenizer for Self-supervised Learning of Time Series Data via Language Models", "paper_summary": "Language models have achieved remarkable success in various natural language\nprocessing tasks. However, their application to time series data, a crucial\ncomponent in many domains, remains limited. This paper proposes LiPCoT (Linear\nPredictive Coding based Tokenizer for time series), a novel tokenizer that\nencodes time series data into a sequence of tokens, enabling self-supervised\nlearning of time series using existing Language model architectures such as\nBERT. Unlike traditional time series tokenizers that rely heavily on CNN\nencoder for time series feature generation, LiPCoT employs stochastic modeling\nthrough linear predictive coding to create a latent space for time series\nproviding a compact yet rich representation of the inherent stochastic nature\nof the data. Furthermore, LiPCoT is computationally efficient and can\neffectively handle time series data with varying sampling rates and lengths,\novercoming common limitations of existing time series tokenizers. In this\nproof-of-concept work, we present the effectiveness of LiPCoT in classifying\nParkinson's disease (PD) using an EEG dataset from 46 participants. In\nparticular, we utilize LiPCoT to encode EEG data into a small vocabulary of\ntokens and then use BERT for self-supervised learning and the downstream task\nof PD classification. We benchmark our approach against several\nstate-of-the-art CNN-based deep learning architectures for PD detection. Our\nresults reveal that BERT models utilizing self-supervised learning outperformed\nthe best-performing existing method by 7.1% in precision, 2.3% in recall, 5.5%\nin accuracy, 4% in AUC, and 5% in F1-score highlighting the potential for\nself-supervised learning even on small datasets. Our work will inform future\nfoundational models for time series, particularly for self-supervised learning.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5df2\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u4e0a\u7684\u61c9\u7528\uff0c\u5728\u8a31\u591a\u9818\u57df\u4e2d\u662f\u4e00\u500b\u95dc\u9375\u7684\u7d44\u6210\u90e8\u5206\uff0c\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86 LiPCoT\uff08\u57fa\u65bc\u7dda\u6027\u9810\u6e2c\u7de8\u78bc\u7684\u6642\u9593\u5e8f\u5217\u6a19\u8a18\u5668\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u6a19\u8a18\u5668\uff0c\u5b83\u5c07\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u7de8\u78bc\u6210\u4e00\u7cfb\u5217\u6a19\u8a18\uff0c\u4f7f\u7528\u73fe\u6709\u7684\u8a9e\u8a00\u6a21\u578b\u67b6\u69cb\uff08\u4f8b\u5982 BERT\uff09\u9032\u884c\u6642\u9593\u5e8f\u5217\u7684\u81ea\u76e3\u7763\u5b78\u7fd2\u3002\u8207\u4f9d\u8cf4 CNN \u7de8\u78bc\u5668\u9032\u884c\u6642\u9593\u5e8f\u5217\u7279\u5fb5\u751f\u6210\u7684\u50b3\u7d71\u6642\u9593\u5e8f\u5217\u6a19\u8a18\u5668\u4e0d\u540c\uff0cLiPCoT \u63a1\u7528\u900f\u904e\u7dda\u6027\u9810\u6e2c\u7de8\u78bc\u9032\u884c\u7684\u96a8\u6a5f\u5efa\u6a21\uff0c\u70ba\u6642\u9593\u5e8f\u5217\u5efa\u7acb\u4e00\u500b\u6f5b\u5728\u7a7a\u9593\uff0c\u63d0\u4f9b\u5c0d\u8cc7\u6599\u5167\u5728\u96a8\u6a5f\u6027\u8cea\u7684\u7dca\u6e4a\u4e14\u8c50\u5bcc\u7684\u8868\u793a\u3002\u6b64\u5916\uff0cLiPCoT \u5728\u8a08\u7b97\u4e0a\u5f88\u6709\u6548\u7387\uff0c\u4e26\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u8655\u7406\u5177\u6709\u4e0d\u540c\u63a1\u6a23\u7387\u548c\u9577\u5ea6\u7684\u6642\u9593\u5e8f\u5217\u8cc7\u6599\uff0c\u514b\u670d\u4e86\u73fe\u6709\u6642\u9593\u5e8f\u5217\u6a19\u8a18\u5668\u7684\u5e38\u898b\u9650\u5236\u3002\u5728\u9019\u9805\u6982\u5ff5\u9a57\u8b49\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 LiPCoT \u5728\u4f7f\u7528\u4f86\u81ea 46 \u4f4d\u53c3\u8207\u8005\u7684 EEG \u8cc7\u6599\u96c6\u5c0d\u5e15\u91d1\u68ee\u6c0f\u75c7 (PD) \u9032\u884c\u5206\u985e\u4e2d\u7684\u6709\u6548\u6027\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u5229\u7528 LiPCoT \u5c07 EEG \u8cc7\u6599\u7de8\u78bc\u6210\u4e00\u500b\u5c0f\u7684\u6a19\u8a18\u8a5e\u5f59\uff0c\u7136\u5f8c\u4f7f\u7528 BERT \u9032\u884c\u81ea\u76e3\u7763\u5b78\u7fd2\u548c PD \u5206\u985e\u7684\u4e0b\u6e38\u4efb\u52d9\u3002\u6211\u5011\u5c07\u6211\u5011\u7684\u65b9\u6cd5\u8207\u5e7e\u7a2e\u7528\u65bc PD \u6aa2\u6e2c\u7684\u5148\u9032 CNN \u57fa\u65bc\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u5229\u7528\u81ea\u76e3\u7763\u5b78\u7fd2\u7684 BERT \u6a21\u578b\u5728\u7cbe\u78ba\u5ea6\u4e0a\u512a\u65bc\u73fe\u6709\u6700\u4f73\u57f7\u884c\u65b9\u6cd5 7.1%\uff0c\u5728\u53ec\u56de\u7387\u4e0a\u512a\u65bc 2.3%\uff0c\u5728\u6e96\u78ba\u5ea6\u4e0a\u512a\u65bc 5.5%\uff0c\u5728 AUC \u4e0a\u512a\u65bc 4%\uff0c\u5728 F1 \u5206\u6578\u4e0a\u512a\u65bc 5%\uff0c\u7a81\u986f\u4e86\u81ea\u76e3\u7763\u5b78\u7fd2\u7684\u6f5b\u529b\uff0c\u5373\u4f7f\u5728\u5c0f\u578b\u8cc7\u6599\u96c6\u4e0a\u4e5f\u662f\u5982\u6b64\u3002\u6211\u5011\u7684\u5de5\u4f5c\u5c07\u70ba\u6642\u9593\u5e8f\u5217\u7684\u672a\u4f86\u57fa\u790e\u6a21\u578b\u63d0\u4f9b\u8cc7\u8a0a\uff0c\u7279\u5225\u662f\u81ea\u76e3\u7763\u5b78\u7fd2\u3002", "author": "Md Fahim Anjum et.al.", "authors": "Md Fahim Anjum", "id": "2408.07292v1", "paper_url": "http://arxiv.org/abs/2408.07292v1", "repo": "https://github.com/mdfahimanjum/lipcot"}}