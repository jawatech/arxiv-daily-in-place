{"2410.21271": {"publish_time": "2024-10-28", "title": "EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation", "paper_summary": "In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements.", "paper_summary_zh": "<paragraph>\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c07\u6a21\u578b\u58d3\u7e2e\u554f\u984c\u91cd\u65b0\u8868\u8ff0\u70ba\u81ea\u8a02\u88dc\u511f\u554f\u984c\uff1a\u7d66\u5b9a\u4e00\u500b\u58d3\u7e2e\u6a21\u578b\uff0c\u6211\u5011\u65e8\u5728\u5f15\u5165\u6b98\u5dee\u4f4e\u79e9\u8def\u5f91\uff0c\u4ee5\u88dc\u511f\u4f86\u81ea\u4f7f\u7528\u8005\u81ea\u8a02\u9700\u6c42\uff08\u4f8b\u5982\uff0c\u4efb\u52d9\u3001\u58d3\u7e2e\u7387\uff09\u7684\u58d3\u7e2e\u932f\u8aa4\uff0c\u5f9e\u800c\u63d0\u9ad8\u8abf\u6574\u6574\u9ad4\u5bb9\u91cf\u7684\u9748\u6d3b\u6027\uff0c\u800c\u4e0d\u6703\u53d7\u5230\u7279\u5b9a\u58d3\u7e2e\u683c\u5f0f\u7684\u7d04\u675f\u3002\u7136\u800c\uff0c\u5929\u771f\u5730\u61c9\u7528 SVD \u4f86\u63a8\u5c0e\u6b98\u5dee\u8def\u5f91\u6703\u5c0e\u81f4\u4f4e\u79e9\u8868\u793a\u5bb9\u91cf\u7684\u6b21\u512a\u5229\u7528\u3002\u76f8\u53cd\uff0c\u6211\u5011\u63d0\u51fa\u7121\u9700\u8a13\u7df4\u7684\u7279\u5fb5\u7a7a\u9593\u4f4e\u79e9\u8fd1\u4f3c (EoRA)\uff0c\u9019\u662f\u4e00\u7a2e\u76f4\u63a5\u6700\u5c0f\u5316\u58d3\u7e2e\u5f15\u8d77\u7684\u932f\u8aa4\u7684\u65b9\u6cd5\uff0c\u7121\u9700\u57fa\u65bc\u68af\u5ea6\u7684\u8a13\u7df4\uff0c\u4f7f\u7528\u5c11\u91cf\u6821\u6e96\u8cc7\u6599\u5728\u5e7e\u5206\u9418\u5167\u5373\u53ef\u5be6\u73fe\u5feb\u901f\u6700\u4f73\u5316\u3002EoRA \u5c07\u58d3\u7e2e\u932f\u8aa4\u6295\u5c04\u5230\u8f38\u5165\u6fc0\u6d3b\u7684\u7279\u5fb5\u7a7a\u9593\u4e2d\uff0c\u5229\u7528\u7279\u5fb5\u503c\u6709\u6548\u5730\u512a\u5148\u91cd\u5efa\u9ad8\u91cd\u8981\u6027\u932f\u8aa4\u7d44\u6210\u3002\u6b64\u5916\uff0cEoRA \u53ef\u4ee5\u8207\u5fae\u8abf\u548c\u91cf\u5316\u7121\u7e2b\u6574\u5408\uff0c\u9032\u4e00\u6b65\u63d0\u9ad8\u6709\u6548\u6027\u548c\u6548\u7387\u3002EoRA \u5728\u88dc\u511f\u58d3\u7e2e\u7684 LLaMA2/3 \u6a21\u578b\u5728\u5404\u7a2e\u4efb\u52d9\u4e0a\u7684\u932f\u8aa4\u65b9\u9762\u59cb\u7d42\u512a\u65bc\u5148\u524d\u7684\u5404\u7a2e\u65b9\u6cd5\uff0c\u4f8b\u5982\u8a9e\u8a00\u751f\u6210\u3001\u5e38\u8b58\u63a8\u7406\u548c\u6578\u5b78\u63a8\u7406\u4efb\u52d9\uff08\u4f8b\u5982\uff0c\u5728\u88dc\u511f\u91cf\u5316\u70ba 4 \u4f4d\u5143\u4e14\u4fee\u526a\u70ba 2:4 \u7a00\u758f\u5ea6\u7684 LLaMA3-8B \u6642\uff0cARC-Easy/ARC-Challenge \u548c MathQA \u5206\u5225\u63d0\u9ad8\u4e86 31.31% / 12.88% \u548c 9.69%\uff09\u3002EoRA \u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u64f4\u5145\u3001\u7121\u9700\u8a13\u7df4\u7684\u89e3\u6c7a\u65b9\u6848\u4f86\u88dc\u511f\u58d3\u7e2e\u932f\u8aa4\uff0c\u4f7f\u5176\u6210\u70ba\u5728\u5404\u7a2e\u5bb9\u91cf\u548c\u6548\u7387\u9700\u6c42\u4e2d\u90e8\u7f72 LLM \u7684\u5f37\u5927\u5de5\u5177\u3002</paragraph>", "author": "Shih-Yang Liu et.al.", "authors": "Shih-Yang Liu, Huck Yang, Chein-Yi Wang, Nai Chit Fung, Hongxu Yin, Charbel Sakr, Saurav Muralidharan, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen", "id": "2410.21271v1", "paper_url": "http://arxiv.org/abs/2410.21271v1", "repo": "null"}}