{"2411.01819": {"publish_time": "2024-11-04", "title": "DiffuMask-Editor: A Novel Paradigm of Integration Between the Segmentation Diffusion Model and Image Editing to Improve Segmentation Ability", "paper_summary": "Semantic segmentation models, like mask2former, often demand a substantial\namount of manually annotated data, which is time-consuming and inefficient to\nacquire. Leveraging state-of-the-art text-to-image models like Midjourney and\nStable Diffusion has emerged as an effective strategy for automatically\ngenerating synthetic data instead of human annotations. However, prior\napproaches have been constrained to synthesizing single-instance images due to\nthe instability inherent in generating multiple instances with Stable\nDiffusion. To expand the domains and diversity of synthetic datasets, this\npaper introduces a novel paradigm named DiffuMask-Editor, which combines the\nDiffusion Model for Segmentation with Image Editing. By integrating multiple\nobjects into images using Text2Image models, our method facilitates the\ncreation of more realistic datasets that closely resemble open-world settings\nwhile simultaneously generating accurate masks. Our approach significantly\nreduces the laborious effort associated with manual annotation while ensuring\nprecise mask generation. Experimental results demonstrate that synthetic data\ngenerated by DiffuMask-Editor enable segmentation methods to achieve superior\nperformance compared to real data. Particularly in zero-shot backgrounds,\nDiffuMask-Editor achieves new state-of-the-art results on Unseen classes of VOC\n2012. The code and models will be publicly available soon.", "paper_summary_zh": "\u8a9e\u7fa9\u5206\u5272\u6a21\u578b\uff08\u5982 mask2former\uff09\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6a19\u8a3b\u8cc7\u6599\uff0c\u800c\u53d6\u5f97\u9019\u4e9b\u8cc7\u6599\u8017\u6642\u4e14\u4f4e\u6548\u7387\u3002\u5584\u7528\u6700\u5148\u9032\u7684\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\uff08\u5982 Midjourney \u548c Stable Diffusion\uff09\u5df2\u6210\u70ba\u81ea\u52d5\u7522\u751f\u5408\u6210\u8cc7\u6599\uff08\u800c\u975e\u4eba\u5de5\u6a19\u8a3b\uff09\u7684\u6709\u6548\u7b56\u7565\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u505a\u6cd5\u53d7\u9650\u65bc\u5408\u6210\u55ae\u4e00\u5be6\u4f8b\u5716\u50cf\uff0c\u56e0\u70ba\u5728 Stable Diffusion \u4e2d\u7522\u751f\u591a\u500b\u5be6\u4f8b\u6642\u6703\u7522\u751f\u4e0d\u7a69\u5b9a\u6027\u3002\u70ba\u4e86\u64f4\u5c55\u5408\u6210\u8cc7\u6599\u96c6\u7684\u9818\u57df\u548c\u591a\u6a23\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u500b\u540d\u70ba DiffuMask-Editor \u7684\u65b0\u7bc4\u4f8b\uff0c\u5b83\u7d50\u5408\u4e86\u7528\u65bc\u5206\u5272\u7684\u64f4\u6563\u6a21\u578b\u8207\u5f71\u50cf\u7de8\u8f2f\u3002\u6211\u5011\u7684\u505a\u6cd5\u900f\u904e\u4f7f\u7528 Text2Image \u6a21\u578b\u5c07\u591a\u500b\u7269\u4ef6\u6574\u5408\u5230\u5716\u50cf\u4e2d\uff0c\u6709\u52a9\u65bc\u5efa\u7acb\u66f4\u903c\u771f\u7684\u8cc7\u6599\u96c6\uff0c\u9019\u4e9b\u8cc7\u6599\u96c6\u8207\u958b\u653e\u4e16\u754c\u8a2d\u5b9a\u975e\u5e38\u76f8\u4f3c\uff0c\u540c\u6642\u7522\u751f\u7cbe\u78ba\u7684\u906e\u7f69\u3002\u6211\u5011\u7684\u505a\u6cd5\u5927\u5e45\u6e1b\u5c11\u4e86\u8207\u4eba\u5de5\u6a19\u8a3b\u76f8\u95dc\u7684\u7e41\u7463\u5de5\u4f5c\uff0c\u540c\u6642\u78ba\u4fdd\u7522\u751f\u7cbe\u78ba\u7684\u906e\u7f69\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u7531 DiffuMask-Editor \u7522\u751f\u7684\u5408\u6210\u8cc7\u6599\u8b93\u5206\u5272\u65b9\u6cd5\u80fd\u9054\u6210\u6bd4\u771f\u5be6\u8cc7\u6599\u66f4\u597d\u7684\u6548\u80fd\u3002\u7279\u5225\u662f\u5728\u96f6\u6b21\u80cc\u666f\u4e2d\uff0cDiffuMask-Editor \u5728 VOC 2012 \u7684\u672a\u898b\u985e\u5225\u4e2d\u9054\u6210\u65b0\u7684\u6700\u5148\u9032\u7d50\u679c\u3002\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u5c07\u5f88\u5feb\u516c\u958b\u3002", "author": "Bo Gao et.al.", "authors": "Bo Gao, Fangxu Xing, Daniel Tang", "id": "2411.01819v1", "paper_url": "http://arxiv.org/abs/2411.01819v1", "repo": "null"}}