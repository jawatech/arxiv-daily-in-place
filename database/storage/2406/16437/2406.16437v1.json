{"2406.16437": {"publish_time": "2024-06-24", "title": "Theory on Mixture-of-Experts in Continual Learning", "paper_summary": "Continual learning (CL) has garnered significant attention because of its\nability to adapt to new tasks that arrive over time. Catastrophic forgetting\n(of old tasks) has been identified as a major issue in CL, as the model adapts\nto new tasks. The Mixture-of-Experts (MoE) model has recently been shown to\neffectively mitigate catastrophic forgetting in CL, by employing a gating\nnetwork to sparsify and distribute diverse tasks among multiple experts.\nHowever, there is a lack of theoretical analysis of MoE and its impact on the\nlearning performance in CL. This paper provides the first theoretical results\nto characterize the impact of MoE in CL via the lens of overparameterized\nlinear regression tasks. We establish the benefit of MoE over a single expert\nby proving that the MoE model can diversify its experts to specialize in\ndifferent tasks, while its router learns to select the right expert for each\ntask and balance the loads across all experts. Our study further suggests an\nintriguing fact that the MoE in CL needs to terminate the update of the gating\nnetwork after sufficient training rounds to attain system convergence, which is\nnot needed in the existing MoE studies that do not consider the continual task\narrival. Furthermore, we provide explicit expressions for the expected\nforgetting and overall generalization error to characterize the benefit of MoE\nin the learning performance in CL. Interestingly, adding more experts requires\nadditional rounds before convergence, which may not enhance the learning\nperformance. Finally, we conduct experiments on both synthetic and real\ndatasets to extend these insights from linear models to deep neural networks\n(DNNs), which also shed light on the practical algorithm design for MoE in CL.", "paper_summary_zh": "\u6301\u7e8c\u5b78\u7fd2 (CL) \u56e0\u5176\u9069\u61c9\u96a8\u8457\u6642\u9593\u63a8\u79fb\u800c\u51fa\u73fe\u7684\u65b0\u4efb\u52d9\u7684\u80fd\u529b\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u707d\u96e3\u6027\u907a\u5fd8 (\u820a\u4efb\u52d9) \u5df2\u88ab\u78ba\u5b9a\u70ba CL \u4e2d\u7684\u4e00\u500b\u4e3b\u8981\u554f\u984c\uff0c\u56e0\u70ba\u6a21\u578b\u9069\u61c9\u65b0\u4efb\u52d9\u3002\u6df7\u5408\u5c08\u5bb6 (MoE) \u6a21\u578b\u6700\u8fd1\u88ab\u8b49\u660e\u53ef\u4ee5\u6709\u6548\u6e1b\u8f15 CL \u4e2d\u7684\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u65b9\u6cd5\u662f\u63a1\u7528\u9598\u63a7\u7db2\u8def\u5728\u591a\u500b\u5c08\u5bb6\u4e4b\u9593\u7a00\u758f\u5316\u548c\u5206\u914d\u4e0d\u540c\u7684\u4efb\u52d9\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5c0d MoE \u53ca\u5176\u5c0d CL \u4e2d\u5b78\u7fd2\u6548\u80fd\u5f71\u97ff\u7684\u7406\u8ad6\u5206\u6790\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u7b2c\u4e00\u500b\u7406\u8ad6\u7d50\u679c\uff0c\u900f\u904e\u904e\u5ea6\u53c3\u6578\u5316\u7dda\u6027\u56de\u6b78\u4efb\u52d9\u7684\u8996\u89d2\u4f86\u63cf\u8ff0 MoE \u5728 CL \u4e2d\u7684\u5f71\u97ff\u3002\u6211\u5011\u900f\u904e\u8b49\u660e MoE \u6a21\u578b\u53ef\u4ee5\u4f7f\u5176\u5c08\u5bb6\u591a\u6a23\u5316\u4ee5\u5c08\u7cbe\u65bc\u4e0d\u540c\u7684\u4efb\u52d9\uff0c\u540c\u6642\u5176\u8def\u7531\u5668\u5b78\u6703\u70ba\u6bcf\u500b\u4efb\u52d9\u9078\u64c7\u6b63\u78ba\u7684\u5c08\u5bb6\u4e26\u5e73\u8861\u6240\u6709\u5c08\u5bb6\u7684\u8ca0\u8f09\uff0c\u4f86\u78ba\u7acb MoE \u512a\u65bc\u55ae\u4e00\u5c08\u5bb6\u7684\u512a\u9ede\u3002\u6211\u5011\u7684\u7814\u7a76\u9032\u4e00\u6b65\u8868\u660e\u4e86\u4e00\u500b\u6709\u8da3\u7684\u4e8b\u5be6\uff0cCL \u4e2d\u7684 MoE \u9700\u8981\u5728\u8db3\u5920\u7684\u8a13\u7df4\u56de\u5408\u5f8c\u7d42\u6b62\u9598\u63a7\u7db2\u8def\u7684\u66f4\u65b0\uff0c\u4ee5\u9054\u5230\u7cfb\u7d71\u6536\u6582\uff0c\u9019\u5728\u4e0d\u8003\u616e\u6301\u7e8c\u4efb\u52d9\u5230\u4f86\u7684\u73fe\u6709 MoE \u7814\u7a76\u4e2d\u662f\u4e0d\u9700\u8981\u7684\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u9810\u671f\u907a\u5fd8\u548c\u6574\u9ad4\u6cdb\u5316\u8aa4\u5dee\u7684\u660e\u78ba\u8868\u9054\u5f0f\uff0c\u4ee5\u63cf\u8ff0 MoE \u5728 CL \u4e2d\u5b78\u7fd2\u6548\u80fd\u7684\u512a\u9ede\u3002\u6709\u8da3\u7684\u662f\uff0c\u589e\u52a0\u66f4\u591a\u5c08\u5bb6\u9700\u8981\u5728\u6536\u6582\u524d\u9032\u884c\u984d\u5916\u7684\u56de\u5408\uff0c\u9019\u53ef\u80fd\u4e0d\u6703\u589e\u5f37\u5b78\u7fd2\u6548\u80fd\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728\u5408\u6210\u548c\u771f\u5be6\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5be6\u9a57\uff0c\u5c07\u9019\u4e9b\u898b\u89e3\u5f9e\u7dda\u6027\u6a21\u578b\u5ef6\u4f38\u5230\u6df1\u5ea6\u795e\u7d93\u7db2\u8def (DNN)\uff0c\u9019\u4e5f\u70ba MoE \u5728 CL \u4e2d\u7684\u5be6\u52d9\u6f14\u7b97\u6cd5\u8a2d\u8a08\u63d0\u4f9b\u4e86\u555f\u793a\u3002", "author": "Hongbo Li et.al.", "authors": "Hongbo Li, Sen Lin, Lingjie Duan, Yingbin Liang, Ness B. Shroff", "id": "2406.16437v1", "paper_url": "http://arxiv.org/abs/2406.16437v1", "repo": "null"}}