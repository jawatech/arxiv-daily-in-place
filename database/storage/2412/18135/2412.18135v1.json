{"2412.18135": {"publish_time": "2024-12-24", "title": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment", "paper_summary": "As large language models (LLMs) demonstrate exceptional performance across\nvarious domains, the deployment of these models on edge devices has emerged as\na new trend. Quantization techniques, which reduce the size and memory\nfootprint of LLMs, are effective for enabling deployment on\nresource-constrained edge devices. However, existing one-size-fits-all\nquantization methods often fail to dynamically adjust the memory consumption of\nLLMs based on specific hardware characteristics and usage scenarios. To address\nthis limitation, we propose LSAQ (Layer-Specific Adaptive Quantization), a\nsystem for adaptive quantization and dynamic deployment of LLMs based on layer\nimportance. LSAQ evaluates layer importance by constructing top-k token sets\nfrom the inputs and outputs of each layer and calculating their Jaccard\ncoefficient. Using this evaluation, the system adaptively adjusts quantization\nstrategies in real time according to the resource availability of edge devices,\nassigning different precision levels to layers of varying importance. This\napproach significantly reduces the storage requirements of LLMs while\nmaintaining model performance, enabling efficient deployment across diverse\nhardware platforms and usage scenarios.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u9818\u57df\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u5728\u908a\u7de3\u88dd\u7f6e\u90e8\u7f72\u9019\u4e9b\u6a21\u578b\u5df2\u6210\u70ba\u4e00\u7a2e\u65b0\u8da8\u52e2\u3002\u91cf\u5316\u6280\u8853\u53ef\u7e2e\u5c0f LLM \u7684\u5927\u5c0f\u548c\u8a18\u61b6\u9ad4\u4f54\u7528\u7a7a\u9593\uff0c\u5c0d\u65bc\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u908a\u7de3\u88dd\u7f6e\u4e0a\u90e8\u7f72 LLM \u4f86\u8aaa\u5341\u5206\u6709\u6548\u3002\u4e0d\u904e\uff0c\u73fe\u6709\u7684\u7d71\u4e00\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u7121\u6cd5\u6839\u64da\u7279\u5b9a\u7684\u786c\u9ad4\u7279\u6027\u548c\u4f7f\u7528\u60c5\u5883\uff0c\u52d5\u614b\u8abf\u6574 LLM \u7684\u8a18\u61b6\u9ad4\u6d88\u8017\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa LSAQ\uff08\u7279\u5b9a\u5c64\u81ea\u9069\u61c9\u91cf\u5316\uff09\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc\u5c64\u91cd\u8981\u6027\u7684 LLM \u81ea\u9069\u61c9\u91cf\u5316\u548c\u52d5\u614b\u90e8\u7f72\u7cfb\u7d71\u3002LSAQ \u900f\u904e\u5f9e\u6bcf\u500b\u5c64\u7684\u8f38\u5165\u548c\u8f38\u51fa\u5efa\u69cb\u524d K \u500b\u6b0a\u6756\u7d44\uff0c\u4e26\u8a08\u7b97\u5176 Jaccard \u4fc2\u6578\uff0c\u4f86\u8a55\u4f30\u5c64\u7684\u91cd\u8981\u6027\u3002\u7cfb\u7d71\u4f7f\u7528\u6b64\u8a55\u4f30\uff0c\u6839\u64da\u908a\u7de3\u88dd\u7f6e\u7684\u8cc7\u6e90\u53ef\u7528\u6027\uff0c\u5373\u6642\u81ea\u9069\u61c9\u8abf\u6574\u91cf\u5316\u7b56\u7565\uff0c\u4e26\u5c07\u4e0d\u540c\u7684\u7cbe\u78ba\u5ea6\u7b49\u7d1a\u6307\u5b9a\u7d66\u4e0d\u540c\u91cd\u8981\u6027\u7684\u5c64\u3002\u9019\u7a2e\u65b9\u6cd5\u5927\u5e45\u964d\u4f4e\u4e86 LLM \u7684\u5132\u5b58\u9700\u6c42\uff0c\u540c\u6642\u7dad\u6301\u6a21\u578b\u6548\u80fd\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u786c\u9ad4\u5e73\u53f0\u548c\u4f7f\u7528\u60c5\u5883\u4e2d\u9032\u884c\u6709\u6548\u7387\u7684\u90e8\u7f72\u3002", "author": "Binrui Zeng et.al.", "authors": "Binrui Zeng, Bin Ji, Xiaodong Liu, Jie Yu, Shasha Li, Jun Ma, Xiaopeng Li, Shangwen Wang, Xinran Hong", "id": "2412.18135v1", "paper_url": "http://arxiv.org/abs/2412.18135v1", "repo": "null"}}