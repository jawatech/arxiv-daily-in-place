{"2412.07585": {"publish_time": "2024-12-10", "title": "Scaling Sequential Recommendation Models with Transformers", "paper_summary": "Modeling user preferences has been mainly addressed by looking at users'\ninteraction history with the different elements available in the system.\nTailoring content to individual preferences based on historical data is the\nmain goal of sequential recommendation.\n  The nature of the problem, as well as the good performance observed across\nvarious domains, has motivated the use of the transformer architecture, which\nhas proven effective in leveraging increasingly larger amounts of training data\nwhen accompanied by an increase in the number of model parameters. This scaling\nbehavior has brought a great deal of attention, as it provides valuable\nguidance in the design and training of even larger models.\n  Taking inspiration from the scaling laws observed in training large language\nmodels, we explore similar principles for sequential recommendation.\n  We use the full Amazon Product Data dataset, which has only been partially\nexplored in other studies, and reveal scaling behaviors similar to those found\nin language models. Compute-optimal training is possible but requires a careful\nanalysis of the compute-performance trade-offs specific to the application.\n  We also show that performance scaling translates to downstream tasks by\nfine-tuning larger pre-trained models on smaller task-specific domains. Our\napproach and findings provide a strategic roadmap for model training and\ndeployment in real high-dimensional preference spaces, facilitating better\ntraining and inference efficiency.\n  We hope this paper bridges the gap between the potential of transformers and\nthe intrinsic complexities of high-dimensional sequential recommendation in\nreal-world recommender systems.\n  Code and models can be found at https://github.com/mercadolibre/srt", "paper_summary_zh": "<paragraph>\u5efa\u6a21\u4f7f\u7528\u8005\u504f\u597d\u4e3b\u8981\u900f\u904e\u89c0\u5bdf\u4f7f\u7528\u8005\u8207\u7cfb\u7d71\u4e2d\u4e0d\u540c\u5143\u7d20\u7684\u4e92\u52d5\u8a18\u9304\u3002\n\u6839\u64da\u6b77\u53f2\u8cc7\u6599\u8abf\u6574\u500b\u4eba\u504f\u597d\u7684\u5167\u5bb9\u662f\u9023\u7e8c\u63a8\u85a6\u7684\u4e3b\u8981\u76ee\u6a19\u3002\n\u554f\u984c\u7684\u672c\u8cea\uff0c\u4ee5\u53ca\u5728\u5404\u500b\u9818\u57df\u89c0\u5bdf\u5230\u7684\u826f\u597d\u6548\u80fd\uff0c\u6fc0\u52f5\u4e86Transformer\u67b6\u69cb\u7684\u4f7f\u7528\uff0c\u5728\u589e\u52a0\u6a21\u578b\u53c3\u6578\u6578\u91cf\u6642\uff0c\u5df2\u8b49\u660e\u80fd\u6709\u6548\u5229\u7528\u8d8a\u4f86\u8d8a\u591a\u8a13\u7df4\u8cc7\u6599\u3002\u9019\u7a2e\u898f\u6a21\u884c\u70ba\u5f15\u8d77\u4e86\u6975\u5927\u7684\u95dc\u6ce8\uff0c\u56e0\u70ba\u5b83\u5728\u8a2d\u8a08\u548c\u8a13\u7df4\u66f4\u5927\u6a21\u578b\u6642\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u6307\u5c0e\u3002\n\u5f9e\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\u89c0\u5bdf\u5230\u7684\u898f\u6a21\u6cd5\u5247\u4e2d\u6c72\u53d6\u9748\u611f\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u9023\u7e8c\u63a8\u85a6\u7684\u985e\u4f3c\u539f\u5247\u3002\n\u6211\u5011\u4f7f\u7528\u4e86\u5b8c\u6574\u7684 Amazon \u7522\u54c1\u8cc7\u6599\u96c6\uff0c\u5176\u4ed6\u7814\u7a76\u50c5\u90e8\u5206\u63a2\u8a0e\u904e\uff0c\u4e26\u63ed\u793a\u4e86\u8207\u5728\u8a9e\u8a00\u6a21\u578b\u4e2d\u767c\u73fe\u7684\u985e\u4f3c\u7684\u898f\u6a21\u884c\u70ba\u3002\u8a08\u7b97\u6700\u4f73\u8a13\u7df4\u662f\u53ef\u80fd\u7684\uff0c\u4f46\u9700\u8981\u4ed4\u7d30\u5206\u6790\u7279\u5b9a\u65bc\u61c9\u7528\u7a0b\u5f0f\u7684\u8a08\u7b97\u6548\u80fd\u6298\u8877\u3002\n\u6211\u5011\u9084\u5c55\u793a\u4e86\u6548\u80fd\u898f\u6a21\u8f49\u5316\u70ba\u4e0b\u6e38\u4efb\u52d9\uff0c\u900f\u904e\u5c0d\u8f03\u5c0f\u7684\u7279\u5b9a\u4efb\u52d9\u9818\u57df\u5fae\u8abf\u8f03\u5927\u7684\u9810\u8a13\u7df4\u6a21\u578b\u3002\u6211\u5011\u7684\u505a\u6cd5\u548c\u767c\u73fe\u70ba\u6a21\u578b\u8a13\u7df4\u548c\u5728\u5be6\u969b\u9ad8\u7dad\u5ea6\u504f\u597d\u7a7a\u9593\u4e2d\u90e8\u7f72\u63d0\u4f9b\u4e86\u7b56\u7565\u6027\u8def\u7dda\u5716\uff0c\u4fc3\u9032\u66f4\u597d\u7684\u8a13\u7df4\u548c\u63a8\u7406\u6548\u7387\u3002\n\u6211\u5011\u5e0c\u671b\u9019\u7bc7\u8ad6\u6587\u80fd\u5f4c\u5408Transformer\u6f5b\u529b\u8207\u5be6\u969b\u63a8\u85a6\u7cfb\u7d71\u4e2d\u9ad8\u7dad\u5ea6\u9023\u7e8c\u63a8\u85a6\u7684\u5167\u5728\u8907\u96dc\u6027\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\n\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u53ef\u4ee5\u5728 https://github.com/mercadolibre/srt \u627e\u5230</paragraph>", "author": "Pablo Zivic et.al.", "authors": "Pablo Zivic, Hernan Vazquez, Jorge Sanchez", "id": "2412.07585v1", "paper_url": "http://arxiv.org/abs/2412.07585v1", "repo": "https://github.com/mercadolibre/srt"}}