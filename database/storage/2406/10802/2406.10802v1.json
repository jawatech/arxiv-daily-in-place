{"2406.10802": {"publish_time": "2024-06-16", "title": "KGPA: Robustness Evaluation for Large Language Models via Cross-Domain Knowledge Graphs", "paper_summary": "Existing frameworks for assessing robustness of large language models (LLMs)\noverly depend on specific benchmarks, increasing costs and failing to evaluate\nperformance of LLMs in professional domains due to dataset limitations. This\npaper proposes a framework that systematically evaluates the robustness of LLMs\nunder adversarial attack scenarios by leveraging knowledge graphs (KGs). Our\nframework generates original prompts from the triplets of knowledge graphs and\ncreates adversarial prompts by poisoning, assessing the robustness of LLMs\nthrough the results of these adversarial attacks. We systematically evaluate\nthe effectiveness of this framework and its modules. Experiments show that\nadversarial robustness of the ChatGPT family ranks as GPT-4-turbo > GPT-4o >\nGPT-3.5-turbo, and the robustness of large language models is influenced by the\nprofessional domains in which they operate.", "paper_summary_zh": "\u73fe\u6709\u7684\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7a69\u5065\u6027\u7684\u6846\u67b6\u904e\u65bc\u4f9d\u8cf4\u7279\u5b9a\u57fa\u6e96\uff0c\u9019\u6703\u589e\u52a0\u6210\u672c\uff0c\u800c\u4e14\u7531\u65bc\u8cc7\u6599\u96c6\u7684\u9650\u5236\uff0c\u7121\u6cd5\u8a55\u4f30 LLM \u5728\u5c08\u696d\u9818\u57df\u7684\u6548\u80fd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u6846\u67b6\uff0c\u5229\u7528\u77e5\u8b58\u5716\u8b5c (KG) \u7cfb\u7d71\u6027\u5730\u8a55\u4f30 LLM \u5728\u5c0d\u6297\u653b\u64ca\u5834\u666f\u4e0b\u7684\u7a69\u5065\u6027\u3002\u6211\u5011\u7684\u6846\u67b6\u5f9e\u77e5\u8b58\u5716\u8b5c\u7684\u4e09\u5143\u7d44\u4e2d\u7522\u751f\u539f\u59cb\u63d0\u793a\uff0c\u4e26\u900f\u904e\u6295\u6bd2\u5efa\u7acb\u5c0d\u6297\u63d0\u793a\uff0c\u900f\u904e\u9019\u4e9b\u5c0d\u6297\u653b\u64ca\u7684\u7d50\u679c\u4f86\u8a55\u4f30 LLM \u7684\u7a69\u5065\u6027\u3002\u6211\u5011\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u4e86\u6b64\u6846\u67b6\u53ca\u5176\u6a21\u7d44\u7684\u6709\u6548\u6027\u3002\u5be6\u9a57\u986f\u793a\uff0cChatGPT \u5bb6\u65cf\u7684\u5c0d\u6297\u7a69\u5065\u6027\u6392\u540d\u70ba GPT-4-turbo > GPT-4o > GPT-3.5-turbo\uff0c\u800c\u4e14\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u7a69\u5065\u6027\u6703\u53d7\u5230\u5176\u904b\u4f5c\u7684\u5c08\u696d\u9818\u57df\u5f71\u97ff\u3002", "author": "Aihua Pei et.al.", "authors": "Aihua Pei, Zehua Yang, Shunan Zhu, Ruoxi Cheng, Ju Jia, Lina Wang", "id": "2406.10802v1", "paper_url": "http://arxiv.org/abs/2406.10802v1", "repo": "null"}}