{"2410.02605": {"publish_time": "2024-10-03", "title": "Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative Prospect Theoretic Reinforcement Learning", "paper_summary": "The widely used expected utility theory has been shown to be empirically\ninconsistent with human preferences in the psychology and behavioral economy\nliteratures. Cumulative Prospect Theory (CPT) has been developed to fill in\nthis gap and provide a better model for human-based decision-making supported\nby empirical evidence. It allows to express a wide range of attitudes and\nperceptions towards risk, gains and losses. A few years ago, CPT has been\ncombined with Reinforcement Learning (RL) to formulate a CPT policy\noptimization problem where the goal of the agent is to search for a policy\ngenerating long-term returns which are aligned with their preferences. In this\nwork, we revisit this policy optimization problem and provide new insights on\noptimal policies and their nature depending on the utility function under\nconsideration. We further derive a novel policy gradient theorem for the CPT\npolicy optimization objective generalizing the seminal corresponding result in\nstandard RL. This result enables us to design a model-free policy gradient\nalgorithm to solve the CPT-RL problem. We illustrate the performance of our\nalgorithm in simple examples motivated by traffic control and electricity\nmanagement applications. We also demonstrate that our policy gradient algorithm\nscales better to larger state spaces compared to the existing zeroth order\nalgorithm for solving the same problem.", "paper_summary_zh": "\u5ee3\u6cdb\u4f7f\u7528\u7684\u9810\u671f\u6548\u7528\u7406\u8ad6\u5df2\u88ab\u8b49\u5be6\u8207\u5fc3\u7406\u5b78\u548c\u884c\u70ba\u7d93\u6fdf\u6587\u737b\u4e2d\u7684\u4eba\u985e\u504f\u597d\u7d93\u9a57\u4e0d\u7b26\u3002\u7d2f\u7a4d\u5c55\u671b\u7406\u8ad6 (CPT) \u5df2\u88ab\u767c\u5c55\u51fa\u4f86\u4ee5\u586b\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u4e26\u70ba\u57fa\u65bc\u4eba\u985e\u7684\u6c7a\u7b56\u5236\u5b9a\u63d0\u4f9b\u66f4\u597d\u7684\u6a21\u578b\uff0c\u4e26\u5f97\u5230\u7d93\u9a57\u8b49\u64da\u7684\u652f\u6301\u3002\u5b83\u5141\u8a31\u8868\u9054\u5c0d\u98a8\u96aa\u3001\u6536\u76ca\u548c\u640d\u5931\u7684\u5ee3\u6cdb\u614b\u5ea6\u548c\u770b\u6cd5\u3002\u5e7e\u5e74\u524d\uff0cCPT \u5df2\u8207\u5f37\u5316\u5b78\u7fd2 (RL) \u7d50\u5408\uff0c\u4ee5\u5236\u5b9a CPT \u653f\u7b56\u512a\u5316\u554f\u984c\uff0c\u5176\u4e2d\u4ee3\u7406\u7684\u76ee\u6a19\u662f\u641c\u5c0b\u4e00\u500b\u7522\u751f\u8207\u5176\u504f\u597d\u4e00\u81f4\u7684\u9577\u671f\u56de\u5831\u7684\u653f\u7b56\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u91cd\u65b0\u5be9\u8996\u9019\u500b\u653f\u7b56\u512a\u5316\u554f\u984c\uff0c\u4e26\u63d0\u4f9b\u95dc\u65bc\u6700\u4f73\u653f\u7b56\u53ca\u5176\u6027\u8cea\u7684\u65b0\u898b\u89e3\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u8003\u616e\u4e2d\u7684\u6548\u7528\u51fd\u6578\u3002\u6211\u5011\u9032\u4e00\u6b65\u70ba CPT \u653f\u7b56\u512a\u5316\u76ee\u6a19\u5c0e\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u653f\u7b56\u68af\u5ea6\u5b9a\u7406\uff0c\u6982\u62ec\u4e86\u6a19\u6e96 RL \u4e2d\u958b\u5275\u6027\u7684\u5c0d\u61c9\u7d50\u679c\u3002\u9019\u500b\u7d50\u679c\u4f7f\u6211\u5011\u80fd\u5920\u8a2d\u8a08\u4e00\u500b\u7121\u6a21\u578b\u653f\u7b56\u68af\u5ea6\u6f14\u7b97\u6cd5\u4f86\u89e3\u6c7a CPT-RL \u554f\u984c\u3002\u6211\u5011\u5728\u7531\u4ea4\u901a\u63a7\u5236\u548c\u96fb\u529b\u7ba1\u7406\u61c9\u7528\u6fc0\u52f5\u7684\u7c21\u55ae\u7bc4\u4f8b\u4e2d\u8aaa\u660e\u4e86\u6211\u5011\u6f14\u7b97\u6cd5\u7684\u6548\u80fd\u3002\u6211\u5011\u9084\u8b49\u660e\uff0c\u8207\u7528\u65bc\u89e3\u6c7a\u76f8\u540c\u554f\u984c\u7684\u73fe\u6709\u96f6\u968e\u6f14\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u653f\u7b56\u68af\u5ea6\u6f14\u7b97\u6cd5\u53ef\u4ee5\u66f4\u597d\u5730\u64f4\u5c55\u5230\u66f4\u5927\u7684\u72c0\u614b\u7a7a\u9593\u3002", "author": "Olivier Lepel et.al.", "authors": "Olivier Lepel, Anas Barakat", "id": "2410.02605v1", "paper_url": "http://arxiv.org/abs/2410.02605v1", "repo": "null"}}