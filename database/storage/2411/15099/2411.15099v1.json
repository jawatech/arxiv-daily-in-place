{"2411.15099": {"publish_time": "2024-11-22", "title": "Context-Aware Multimodal Pretraining", "paper_summary": "Large-scale multimodal representation learning successfully optimizes for\nzero-shot transfer at test time. Yet the standard pretraining paradigm\n(contrastive learning on large amounts of image-text data) does not explicitly\nencourage representations to support few-shot adaptation. In this work, we\npropose a simple, but carefully designed extension to multimodal pretraining\nwhich enables representations to accommodate additional context. Using this\nobjective, we show that vision-language models can be trained to exhibit\nsignificantly increased few-shot adaptation: across 21 downstream tasks, we\nfind up to four-fold improvements in test-time sample efficiency, and average\nfew-shot adaptation gains of over 5%, while retaining zero-shot generalization\nperformance across model scales and training durations. In particular, equipped\nwith simple, training-free, metric-based adaptation mechanisms, our\nrepresentations easily surpass more complex and expensive optimization-based\nschemes, vastly simplifying generalization to new domains.", "paper_summary_zh": "\u5927\u898f\u6a21\u591a\u6a21\u614b\u8868\u5fb5\u5b78\u7fd2\u6210\u529f\u91dd\u5c0d\u6e2c\u8a66\u6642\u9593\u7684\u96f6\u6b21\u5b78\u7fd2\u50b3\u8f38\u9032\u884c\u6700\u4f73\u5316\u3002\u7136\u800c\uff0c\u6a19\u6e96\u9810\u8a13\u7df4\u7bc4\u4f8b\uff08\u5c0d\u5927\u91cf\u5f71\u50cf\u6587\u5b57\u8cc7\u6599\u9032\u884c\u5c0d\u6bd4\u5b78\u7fd2\uff09\u4e26\u672a\u660e\u78ba\u9f13\u52f5\u8868\u5fb5\u652f\u63f4\u5c11\u6b21\u5b78\u7fd2\u9069\u61c9\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u4f46\u7d93\u904e\u4ed4\u7d30\u8a2d\u8a08\u7684\u591a\u6a21\u614b\u9810\u8a13\u7df4\u5ef6\u4f38\uff0c\u8b93\u8868\u5fb5\u80fd\u5920\u5bb9\u7d0d\u984d\u5916\u7684\u5167\u5bb9\u3002\u4f7f\u7528\u6b64\u76ee\u6a19\uff0c\u6211\u5011\u5c55\u793a\u4e86\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u8a13\u7df4\u6210\u5c55\u73fe\u5927\u5e45\u589e\u52a0\u7684\u5c11\u6b21\u5b78\u7fd2\u9069\u61c9\uff1a\u5728 21 \u500b\u4e0b\u6e38\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u6e2c\u8a66\u6642\u9593\u6a23\u672c\u6548\u7387\u63d0\u5347\u591a\u9054\u56db\u500d\uff0c\u4e14\u5e73\u5747\u5c11\u6b21\u5b78\u7fd2\u9069\u61c9\u589e\u76ca\u8d85\u904e 5%\uff0c\u540c\u6642\u5728\u6a21\u578b\u898f\u6a21\u548c\u8a13\u7df4\u6301\u7e8c\u6642\u9593\u4e2d\u4fdd\u7559\u96f6\u6b21\u5b78\u7fd2\u6982\u5316\u6548\u80fd\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u7684\u8868\u5fb5\u914d\u5099\u7c21\u55ae\u3001\u7121\u9700\u8a13\u7df4\u7684\u57fa\u65bc\u6307\u6a19\u7684\u9069\u61c9\u6a5f\u5236\uff0c\u8f15\u6613\u8d85\u8d8a\u66f4\u8907\u96dc\u4e14\u6602\u8cb4\u7684\u57fa\u65bc\u6700\u4f73\u5316\u7684\u65b9\u6848\uff0c\u5927\u5e45\u7c21\u5316\u5c0d\u65b0\u9818\u57df\u7684\u6982\u5316\u3002", "author": "Karsten Roth et.al.", "authors": "Karsten Roth, Zeynep Akata, Dima Damen, Ivana Bala\u017eevi\u0107, Olivier J. H\u00e9naff", "id": "2411.15099v1", "paper_url": "http://arxiv.org/abs/2411.15099v1", "repo": "null"}}