{"2502.00094": {"publish_time": "2025-01-31", "title": "AIN: The Arabic INclusive Large Multimodal Model", "paper_summary": "Amid the swift progress of large language models (LLMs) and their evolution\ninto large multimodal models (LMMs), significant strides have been made in\nhigh-resource languages such as English and Chinese. While Arabic LLMs have\nseen notable progress, Arabic LMMs remain largely unexplored, often narrowly\nfocusing on a few specific aspects of the language and visual understanding. To\nbridge this gap, we introduce AIN-the Arabic Inclusive Multimodal\nModel-designed to excel across diverse domains. AIN is an English-Arabic\nbilingual LMM designed to excel in English and Arabic, leveraging carefully\nconstructed 3.6 million high-quality Arabic-English multimodal data samples.\nAIN demonstrates state-of-the-art Arabic performance, while also possessing\nstrong English-language visual capabilities. On the recent CAMEL-Bench\nbenchmark comprising 38 sub-domains including, multi-image understanding,\ncomplex visual perception, handwritten document understanding, video\nunderstanding, medical imaging, plant diseases, and remote sensing-based land\nuse understanding, our AIN demonstrates strong performance with the 7B model\noutperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains\nand 38 sub-domains. AIN's superior capabilities position it as a significant\nstep toward empowering Arabic speakers with advanced multimodal generative AI\ntools across diverse applications.", "paper_summary_zh": "\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5feb\u901f\u9032\u5c55\u53ca\u5176\u6f14\u8b8a\u6210\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u7684\u904e\u7a0b\u4e2d\uff0c\u82f1\u8a9e\u548c\u4e2d\u6587\u7b49\u9ad8\u8cc7\u6e90\u8a9e\u8a00\u53d6\u5f97\u4e86\u986f\u8457\u9032\u5c55\u3002\u5118\u7ba1\u963f\u62c9\u4f2f\u8a9e LLM \u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f46\u963f\u62c9\u4f2f\u8a9e LMM \u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u901a\u5e38\u50c5\u72f9\u9698\u5730\u95dc\u6ce8\u8a9e\u8a00\u548c\u8996\u89ba\u7406\u89e3\u7684\u5e7e\u500b\u7279\u5b9a\u65b9\u9762\u3002\u70ba\u4e86\u5f4c\u5408\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 AIN\uff08\u963f\u62c9\u4f2f\u8a9e\u5305\u5bb9\u6027\u591a\u6a21\u614b\u6a21\u578b\uff09\uff0c\u65e8\u5728\u5728\u4e0d\u540c\u7684\u9818\u57df\u4e2d\u8868\u73fe\u51fa\u8272\u3002AIN \u662f\u4e00\u7a2e\u82f1\u8a9e-\u963f\u62c9\u4f2f\u8a9e\u96d9\u8a9e LMM\uff0c\u65e8\u5728\u5728\u82f1\u8a9e\u548c\u963f\u62c9\u4f2f\u8a9e\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u5229\u7528\u7cbe\u5fc3\u69cb\u5efa\u7684 360 \u842c\u500b\u9ad8\u54c1\u8cea\u963f\u62c9\u4f2f\u8a9e-\u82f1\u8a9e\u591a\u6a21\u614b\u6578\u64da\u6a23\u672c\u3002AIN \u5c55\u793a\u4e86\u6700\u5148\u9032\u7684\u963f\u62c9\u4f2f\u8a9e\u6027\u80fd\uff0c\u540c\u6642\u9084\u5177\u5099\u5f37\u5927\u7684\u82f1\u8a9e\u8996\u89ba\u80fd\u529b\u3002\u5728\u6700\u8fd1\u7684 CAMEL-Bench \u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u5305\u62ec 38 \u500b\u5b50\u9818\u57df\uff0c\u5305\u62ec\u591a\u5716\u50cf\u7406\u89e3\u3001\u8907\u96dc\u8996\u89ba\u611f\u77e5\u3001\u624b\u5beb\u6587\u4ef6\u7406\u89e3\u3001\u8996\u983b\u7406\u89e3\u3001\u91ab\u5b78\u6210\u50cf\u3001\u690d\u7269\u75c5\u5bb3\u548c\u57fa\u65bc\u9059\u611f\u7684\u571f\u5730\u5229\u7528\u7406\u89e3\uff0c\u6211\u5011\u7684 AIN \u8868\u73fe\u51fa\u8272\uff0c\u5176\u4e2d 7B \u6a21\u578b\u6bd4 GPT-4o \u9ad8\u51fa 3.4%\uff0c\u5e73\u5747\u512a\u65bc\u516b\u500b\u9818\u57df\u548c 38 \u500b\u5b50\u9818\u57df\u3002AIN \u7684\u5353\u8d8a\u80fd\u529b\u4f7f\u5176\u6210\u70ba\u8ce6\u4e88\u963f\u62c9\u4f2f\u8a9e\u4f7f\u7528\u8005\u8de8\u4e0d\u540c\u61c9\u7528\u7a0b\u5f0f\u5148\u9032\u591a\u6a21\u614b\u751f\u6210\u5f0f AI \u5de5\u5177\u7684\u91cd\u8981\u4e00\u6b65\u3002", "author": "Ahmed Heakl et.al.", "authors": "Ahmed Heakl, Sara Ghaboura, Omkar Thawkar, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan", "id": "2502.00094v1", "paper_url": "http://arxiv.org/abs/2502.00094v1", "repo": "null"}}