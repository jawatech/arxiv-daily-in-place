{"2406.11813": {"publish_time": "2024-06-17", "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?", "paper_summary": "Despite the recent observation that large language models (LLMs) can store\nsubstantial factual knowledge, there is a limited understanding of the\nmechanisms of how they acquire factual knowledge through pretraining. This work\naddresses this gap by studying how LLMs acquire factual knowledge during\npretraining. The findings reveal several important insights into the dynamics\nof factual knowledge acquisition during pretraining. First, counterintuitively,\nwe observe that pretraining on more data shows no significant improvement in\nthe model's capability to acquire and maintain factual knowledge. Next, there\nis a power-law relationship between training steps and forgetting of\nmemorization and generalization of factual knowledge, and LLMs trained with\nduplicated training data exhibit faster forgetting. Third, training LLMs with\nlarger batch sizes can enhance the models' robustness to forgetting. Overall,\nour observations suggest that factual knowledge acquisition in LLM pretraining\noccurs by progressively increasing the probability of factual knowledge\npresented in the pretraining data at each step. However, this increase is\ndiluted by subsequent forgetting. Based on this interpretation, we demonstrate\nthat we can provide plausible explanations for recently observed behaviors of\nLLMs, such as the poor performance of LLMs on long-tail knowledge and the\nbenefits of deduplicating the pretraining corpus.", "paper_summary_zh": "\u5118\u7ba1\u6700\u8fd1\u89c0\u5bdf\u5230\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u5132\u5b58\u5927\u91cf\u7684\u5be6\u969b\u77e5\u8b58\uff0c\u4f46\u5c0d\u65bc LLM \u5728\u9810\u8a13\u7df4\u904e\u7a0b\u4e2d\u5982\u4f55\u7372\u53d6\u5be6\u969b\u77e5\u8b58\u7684\u6a5f\u5236\uff0c\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002\u9019\u9805\u7814\u7a76\u900f\u904e\u63a2\u8a0e LLM \u5728\u9810\u8a13\u7df4\u671f\u9593\u5982\u4f55\u7372\u53d6\u5be6\u969b\u77e5\u8b58\uff0c\u4f86\u63a2\u8a0e\u9019\u500b\u77e5\u8b58\u7f3a\u53e3\u3002\u7814\u7a76\u7d50\u679c\u63ed\u9732\u4e86\u5e7e\u500b\u91cd\u8981\u7684\u898b\u89e3\uff0c\u8aaa\u660e\u5728\u9810\u8a13\u7df4\u671f\u9593\u7372\u53d6\u5be6\u969b\u77e5\u8b58\u7684\u52d5\u614b\u3002\u9996\u5148\uff0c\u8207\u76f4\u89ba\u76f8\u53cd\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u4f7f\u7528\u66f4\u591a\u8cc7\u6599\u9032\u884c\u9810\u8a13\u7df4\u4e26\u672a\u986f\u8457\u63d0\u5347\u6a21\u578b\u7372\u53d6\u548c\u7dad\u8b77\u5be6\u969b\u77e5\u8b58\u7684\u80fd\u529b\u3002\u5176\u6b21\uff0c\u8a13\u7df4\u6b65\u9a5f\u8207\u907a\u5fd8\u3001\u8a18\u61b6\u548c\u5be6\u969b\u77e5\u8b58\u7684\u6982\u5316\u4e4b\u9593\u5b58\u5728\u51aa\u5f8b\u95dc\u4fc2\uff0c\u800c\u4f7f\u7528\u91cd\u8907\u8a13\u7df4\u8cc7\u6599\u8a13\u7df4\u7684 LLM \u907a\u5fd8\u5f97\u66f4\u5feb\u3002\u7b2c\u4e09\uff0c\u4f7f\u7528\u8f03\u5927\u7684\u6279\u6b21\u5927\u5c0f\u8a13\u7df4 LLM \u80fd\u5920\u63d0\u5347\u6a21\u578b\u5c0d\u907a\u5fd8\u7684\u7a69\u5065\u6027\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u89c0\u5bdf\u7d50\u679c\u986f\u793a\uff0cLLM \u9810\u8a13\u7df4\u4e2d\u7684\u5be6\u969b\u77e5\u8b58\u7372\u53d6\u662f\u900f\u904e\u5728\u6bcf\u500b\u6b65\u9a5f\u9010\u6b65\u589e\u52a0\u9810\u8a13\u7df4\u8cc7\u6599\u4e2d\u5448\u73fe\u7684\u5be6\u969b\u77e5\u8b58\u6a5f\u7387\u800c\u767c\u751f\u7684\u3002\u7136\u800c\uff0c\u9019\u7a2e\u589e\u52a0\u6703\u88ab\u96a8\u5f8c\u7684\u907a\u5fd8\u6240\u7a00\u91cb\u3002\u6839\u64da\u9019\u500b\u89e3\u91cb\uff0c\u6211\u5011\u8b49\u660e\u6211\u5011\u53ef\u4ee5\u5c0d\u6700\u8fd1\u89c0\u5bdf\u5230\u7684 LLM \u884c\u70ba\u63d0\u4f9b\u5408\u7406\u7684\u89e3\u91cb\uff0c\u4f8b\u5982 LLM \u5728\u9577\u5c3e\u77e5\u8b58\u4e0a\u7684\u8868\u73fe\u4e0d\u4f73\uff0c\u4ee5\u53ca\u5c0d\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u9032\u884c\u91cd\u8907\u8cc7\u6599\u522a\u9664\u7684\u597d\u8655\u3002", "author": "Hoyeon Chang et.al.", "authors": "Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo", "id": "2406.11813v1", "paper_url": "http://arxiv.org/abs/2406.11813v1", "repo": "null"}}