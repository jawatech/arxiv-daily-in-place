{"2501.04070": {"publish_time": "2025-01-07", "title": "More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives", "paper_summary": "Large language models (LLMs) excel at few-shot in-context learning (ICL)\nwithout requiring parameter updates. However, as the number of ICL\ndemonstrations increases from a few to many, performance tends to plateau and\neventually decline. We identify two primary causes for this trend: the\nsuboptimal negative log-likelihood (NLL) optimization objective and the\nincremental data noise. To address these issues, we introduce DR-ICL, a novel\noptimization method that enhances model performance through Differentiated\nLearning and advantage-based Reweighting objectives. Globally, DR-ICL utilizes\ndifferentiated learning to optimize the NLL objective, ensuring that many-shot\nperformance surpasses zero-shot levels. Locally, it dynamically adjusts the\nweighting of many-shot demonstrations by leveraging cumulative advantages\ninspired by reinforcement learning, thereby improving generalization. This\napproach allows the model to handle varying numbers of shots effectively,\nmitigating the impact of noisy data. Recognizing the lack of multi-task\ndatasets with diverse many-shot distributions, we develop the Many-Shot ICL\nBenchmark (MICLB)-a large-scale benchmark covering shot numbers from 1 to 350\nwithin sequences of up to 8,000 tokens-for fine-tuning purposes. MICLB\nfacilitates the evaluation of many-shot ICL strategies across seven prominent\nNLP tasks and 50 distinct datasets. Experimental results demonstrate that LLMs\nenhanced with DR-ICL achieve significant improvements in many-shot setups\nacross various tasks, including both in-domain and out-of-domain scenarios. We\nrelease the code and benchmark dataset hoping to facilitate further research in\nmany-shot ICL.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u4e0d\u9700\u8981\u53c3\u6578\u66f4\u65b0\u7684\u60c5\u6cc1\u4e0b\uff0c\u64c5\u9577\u65bc\u5c11\u6b21\u6578\u7684\u8108\u7d61\u5b78\u7fd2 (ICL)\u3002\u7136\u800c\uff0c\u96a8\u8457 ICL \u793a\u7bc4\u7684\u6578\u91cf\u5f9e\u5c11\u6578\u589e\u52a0\u5230\u591a\u6578\uff0c\u6548\u80fd\u5f80\u5f80\u6703\u9054\u5230\u505c\u6eef\u72c0\u614b\uff0c\u751a\u81f3\u6700\u7d42\u4e0b\u964d\u3002\u6211\u5011\u627e\u51fa\u6b64\u8da8\u52e2\u7684\u5169\u500b\u4e3b\u8981\u539f\u56e0\uff1a\u6b21\u4f73\u8ca0\u5c0d\u6578\u4f3c\u7136 (NLL) \u6700\u4f73\u5316\u76ee\u6a19\u4ee5\u53ca\u6f38\u589e\u8cc7\u6599\u96dc\u8a0a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 DR-ICL\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u53ef\u900f\u904e\u5dee\u7570\u5316\u5b78\u7fd2\u548c\u57fa\u65bc\u512a\u52e2\u7684\u91cd\u65b0\u52a0\u6b0a\u76ee\u6a19\u4f86\u589e\u5f37\u6a21\u578b\u6548\u80fd\u3002\u5728\u6574\u9ad4\u4e0a\uff0cDR-ICL \u5229\u7528\u5dee\u7570\u5316\u5b78\u7fd2\u4f86\u6700\u4f73\u5316 NLL \u76ee\u6a19\uff0c\u78ba\u4fdd\u591a\u6a23\u672c\u6548\u80fd\u8d85\u8d8a\u96f6\u6a23\u672c\u5c64\u7d1a\u3002\u5728\u5c40\u90e8\u4e0a\uff0c\u5b83\u900f\u904e\u904b\u7528\u5f37\u5316\u5b78\u7fd2\u6240\u555f\u767c\u7684\u7d2f\u7a4d\u512a\u52e2\uff0c\u52d5\u614b\u8abf\u6574\u591a\u6a23\u672c\u793a\u7bc4\u7684\u52a0\u6b0a\uff0c\u9032\u800c\u6539\u5584\u6982\u5316\u80fd\u529b\u3002\u6b64\u65b9\u6cd5\u8b93\u6a21\u578b\u80fd\u5920\u6709\u6548\u8655\u7406\u4e0d\u540c\u6578\u91cf\u7684\u6a23\u672c\uff0c\u4e26\u6e1b\u8f15\u96dc\u8a0a\u8cc7\u6599\u7684\u5f71\u97ff\u3002\u9451\u65bc\u7f3a\u4e4f\u5177\u6709\u591a\u6a23\u5316\u591a\u6a23\u672c\u5206\u4f48\u7684\u591a\u4efb\u52d9\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u958b\u767c\u4e86\u591a\u6a23\u672c ICL \u57fa\u6e96 (MICLB)\uff0c\u9019\u662f\u4e00\u500b\u5927\u898f\u6a21\u57fa\u6e96\uff0c\u6db5\u84cb 1 \u5230 350 \u500b\u6a23\u672c\u7684\u6578\u76ee\uff0c\u5e8f\u5217\u9577\u5ea6\u9054 8,000 \u500b\u7b26\u865f\uff0c\u7528\u65bc\u5fae\u8abf\u76ee\u7684\u3002MICLB \u4fbf\u65bc\u8a55\u4f30\u4e03\u9805\u91cd\u8981\u7684 NLP \u4efb\u52d9\u548c 50 \u500b\u4e0d\u540c\u8cc7\u6599\u96c6\u7684\u591a\u6a23\u672c ICL \u7b56\u7565\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u900f\u904e DR-ICL \u589e\u5f37\u7684 LLM \u5728\u591a\u6a23\u672c\u8a2d\u5b9a\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6539\u5584\uff0c\u6db5\u84cb\u5404\u7a2e\u4efb\u52d9\uff0c\u5305\u62ec\u57df\u5167\u548c\u57df\u5916\u5834\u666f\u3002\u6211\u5011\u91cb\u51fa\u7a0b\u5f0f\u78bc\u548c\u57fa\u6e96\u8cc7\u6599\u96c6\uff0c\u5e0c\u671b\u80fd\u4fc3\u9032\u591a\u6a23\u672c ICL \u7684\u9032\u4e00\u6b65\u7814\u7a76\u3002", "author": "Xiaoqing Zhang et.al.", "authors": "Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Shuo Shang, Xiuying Chen, Rui Yan", "id": "2501.04070v1", "paper_url": "http://arxiv.org/abs/2501.04070v1", "repo": "null"}}