{"2410.03234": {"publish_time": "2024-10-04", "title": "Showing LLM-Generated Code Selectively Based on Confidence of LLMs", "paper_summary": "Large Language Models (LLMs) have shown impressive abilities in code\ngeneration, but they may generate erroneous programs. Reading a program takes\nten times longer than writing it. Showing these erroneous programs to\ndevelopers will waste developers' energies and introduce security risks to\nsoftware.\n  To address the above limitations, we propose HonestCoder, a novel LLM-based\ncode generation approach. HonestCoder selectively shows the generated programs\nto developers based on LLMs' confidence. The confidence provides valuable\ninsights into the correctness of generated programs. To achieve this goal, we\npropose a novel approach to estimate LLMs' confidence in code generation. It\nestimates confidence by measuring the multi-modal similarity between\nLLMs-generated programs.\n  We collect and release a multilingual benchmark named TruthCodeBench, which\nconsists of 2,265 samples and covers two popular programming languages (i.e.,\nPython and Java). We apply HonestCoder to four popular LLMs (e.g.,\nDeepSeek-Coder and Code Llama) and evaluate it on TruthCodeBench. Based on the\nexperiments, we obtain the following insights. (1) HonestCoder can effectively\nestimate LLMs' confidence and accurately determine the correctness of generated\nprograms. For example, HonestCoder outperforms the state-of-the-art baseline by\n27.79% in AUROC and 63.74% in AUCPR. (2) HonestCoder can decrease the number of\nerroneous programs shown to developers. Compared to eight baselines, it can\nshow more correct programs and fewer erroneous programs to developers. (3)\nCompared to showing code indiscriminately, HonestCoder only adds slight time\noverhead (approximately 0.4 seconds per requirement). (4) We discuss future\ndirections to facilitate the application of LLMs in software development. We\nhope this work can motivate broad discussions about measuring the reliability\nof LLMs' outputs in performing code-related tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u65b9\u9762\u5c55\u73fe\u9a5a\u4eba\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u53ef\u80fd\u6703\u7522\u751f\u932f\u8aa4\u7684\u7a0b\u5f0f\u3002\u95b1\u8b80\u7a0b\u5f0f\u78bc\u7684\u6642\u9593\u662f\u64b0\u5beb\u7a0b\u5f0f\u78bc\u7684\u5341\u500d\u3002\u5c07\u9019\u4e9b\u6709\u932f\u8aa4\u7684\u7a0b\u5f0f\u78bc\u986f\u793a\u7d66\u958b\u767c\u4eba\u54e1\u6703\u6d6a\u8cbb\u958b\u767c\u4eba\u54e1\u7684\u7cbe\u529b\uff0c\u4e26\u5c0d\u8edf\u9ad4\u5f15\u5165\u5b89\u5168\u98a8\u96aa\u3002\n\u70ba\u4e86\u89e3\u6c7a\u4e0a\u8ff0\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa HonestCoder\uff0c\u4e00\u7a2e\u65b0\u7684\u57fa\u65bc LLM \u7684\u7a0b\u5f0f\u78bc\u751f\u6210\u65b9\u6cd5\u3002HonestCoder \u6839\u64da LLM \u7684\u4fe1\u5fc3\u6709\u9078\u64c7\u5730\u5411\u958b\u767c\u4eba\u54e1\u986f\u793a\u751f\u6210\u7684\u7a0b\u5f0f\u78bc\u3002\u4fe1\u5fc3\u63d0\u4f9b\u5c0d\u751f\u6210\u7a0b\u5f0f\u78bc\u6b63\u78ba\u6027\u7684\u5bf6\u8cb4\u898b\u89e3\u3002\u70ba\u4e86\u5be6\u73fe\u9019\u500b\u76ee\u6a19\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\u4f86\u4f30\u8a08 LLM \u5728\u7a0b\u5f0f\u78bc\u751f\u6210\u4e2d\u7684\u4fe1\u5fc3\u3002\u5b83\u900f\u904e\u6e2c\u91cf LLM \u751f\u6210\u7684\u7a0b\u5f0f\u78bc\u4e4b\u9593\u7684\u591a\u6a21\u5f0f\u76f8\u4f3c\u6027\u4f86\u4f30\u8a08\u4fe1\u5fc3\u3002\n\u6211\u5011\u6536\u96c6\u4e26\u767c\u5e03\u4e86\u4e00\u500b\u540d\u70ba TruthCodeBench \u7684\u591a\u8a9e\u8a00\u57fa\u6e96\uff0c\u5176\u4e2d\u5305\u542b 2,265 \u500b\u7bc4\u4f8b\uff0c\u6db5\u84cb\u5169\u7a2e\u6d41\u884c\u7684\u7a0b\u5f0f\u8a9e\u8a00\uff08\u5373 Python \u548c Java\uff09\u3002\u6211\u5011\u5c07 HonestCoder \u61c9\u7528\u65bc\u56db\u500b\u6d41\u884c\u7684 LLM\uff08\u4f8b\u5982 DeepSeek-Coder \u548c Code Llama\uff09\uff0c\u4e26\u5728 TruthCodeBench \u4e0a\u5c0d\u5176\u9032\u884c\u8a55\u4f30\u3002\u6839\u64da\u5be6\u9a57\uff0c\u6211\u5011\u7372\u5f97\u4ee5\u4e0b\u898b\u89e3\u3002\uff081\uff09HonestCoder \u53ef\u4ee5\u6709\u6548\u4f30\u8a08 LLM \u7684\u4fe1\u5fc3\uff0c\u4e26\u6e96\u78ba\u78ba\u5b9a\u751f\u6210\u7a0b\u5f0f\u78bc\u7684\u6b63\u78ba\u6027\u3002\u4f8b\u5982\uff0cHonestCoder \u5728 AUROC \u4e2d\u6bd4\u6700\u5148\u9032\u7684\u57fa\u6e96\u9ad8\u51fa 27.79%\uff0c\u5728 AUCPR \u4e2d\u9ad8\u51fa 63.74%\u3002\uff082\uff09HonestCoder \u53ef\u4ee5\u6e1b\u5c11\u986f\u793a\u7d66\u958b\u767c\u4eba\u54e1\u7684\u932f\u8aa4\u7a0b\u5f0f\u78bc\u6578\u91cf\u3002\u8207\u516b\u500b\u57fa\u6e96\u76f8\u6bd4\uff0c\u5b83\u53ef\u4ee5\u5411\u958b\u767c\u4eba\u54e1\u986f\u793a\u66f4\u591a\u6b63\u78ba\u7684\u7a0b\u5f0f\u78bc\u548c\u66f4\u5c11\u7684\u932f\u8aa4\u7a0b\u5f0f\u78bc\u3002\uff083\uff09\u8207\u4e0d\u52a0\u9078\u64c7\u5730\u986f\u793a\u7a0b\u5f0f\u78bc\u76f8\u6bd4\uff0cHonestCoder \u53ea\u6703\u589e\u52a0\u8f15\u5fae\u7684\u6642\u9593\u958b\u92b7\uff08\u6bcf\u500b\u9700\u6c42\u5927\u7d04 0.4 \u79d2\uff09\u3002\uff084\uff09\u6211\u5011\u8a0e\u8ad6\u4e86\u4fc3\u9032 LLM \u5728\u8edf\u9ad4\u958b\u767c\u4e2d\u61c9\u7528\u7684\u672a\u4f86\u65b9\u5411\u3002\u6211\u5011\u5e0c\u671b\u9019\u9805\u5de5\u4f5c\u53ef\u4ee5\u6fc0\u52f5\u5ee3\u6cdb\u8a0e\u8ad6\uff0c\u4ee5\u8861\u91cf LLM \u5728\u57f7\u884c\u8207\u7a0b\u5f0f\u78bc\u76f8\u95dc\u4efb\u52d9\u6642\u7684\u8f38\u51fa\u53ef\u9760\u6027\u3002", "author": "Jia Li et.al.", "authors": "Jia Li, Yuqi Zhu, Yongmin Li, Ge Li, Zhi Jin", "id": "2410.03234v1", "paper_url": "http://arxiv.org/abs/2410.03234v1", "repo": "null"}}