{"2412.04445": {"publish_time": "2024-12-05", "title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation", "paper_summary": "Recent developments in Large Language Models pre-trained on extensive corpora\nhave shown significant success in various natural language processing tasks\nwith minimal fine-tuning. This success offers new promise for robotics, which\nhas long been constrained by the high cost of action-labeled data. We ask:\ngiven the abundant video data containing interaction-related knowledge\navailable as a rich \"corpus\", can a similar generative pre-training approach be\neffectively applied to enhance robot learning? The key challenge is to identify\nan effective representation for autoregressive pre-training that benefits robot\nmanipulation tasks. Inspired by the way humans learn new skills through\nobserving dynamic environments, we propose that effective robotic learning\nshould emphasize motion-related knowledge, which is closely tied to low-level\nactions and is hardware-agnostic, facilitating the transfer of learned motions\nto actual robot actions. To this end, we introduce Moto, which converts video\ncontent into latent Motion Token sequences by a Latent Motion Tokenizer,\nlearning a bridging \"language\" of motion from videos in an unsupervised manner.\nWe pre-train Moto-GPT through motion token autoregression, enabling it to\ncapture diverse visual motion knowledge. After pre-training, Moto-GPT\ndemonstrates the promising ability to produce semantically interpretable motion\ntokens, predict plausible motion trajectories, and assess trajectory\nrationality through output likelihood. To transfer learned motion priors to\nreal robot actions, we implement a co-fine-tuning strategy that seamlessly\nbridges latent motion token prediction and real robot control. Extensive\nexperiments show that the fine-tuned Moto-GPT exhibits superior robustness and\nefficiency on robot manipulation benchmarks, underscoring its effectiveness in\ntransferring knowledge from video data to downstream visual manipulation tasks.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5927\u91cf\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u6700\u65b0\u8fdb\u5c55\n\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6210\u529f\uff0c\n\u53ea\u9700\u8fdb\u884c\u6700\u5c11\u7684\u5fae\u8c03\u3002\u8fd9\u4e00\u6210\u529f\u4e3a\u673a\u5668\u4eba\u6280\u672f\u5e26\u6765\u4e86\u65b0\u7684\u5e0c\u671b\uff0c\n\u673a\u5668\u4eba\u6280\u672f\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u53d7\u5230\u52a8\u4f5c\u6807\u8bb0\u6570\u636e\u7684\u9ad8\u6210\u672c\u7684\u5236\u7ea6\u3002\u6211\u4eec\u95ee\uff1a\n\u9274\u4e8e\u5305\u542b\u4ea4\u4e92\u76f8\u5173\u77e5\u8bc6\u7684\u4e30\u5bcc\u89c6\u9891\u6570\u636e\n\u53ef\u7528\u4f5c\u4e30\u5bcc\u7684\u201c\u8bed\u6599\u5e93\u201d\uff0c\u7c7b\u4f3c\u7684\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\n\u6709\u6548\u5730\u5e94\u7528\u4e8e\u589e\u5f3a\u673a\u5668\u4eba\u5b66\u4e60\u5417\uff1f\u5173\u952e\u7684\u6311\u6218\u662f\u8bc6\u522b\n\u4e00\u4e2a\u6709\u6548\u7684\u8868\u793a\uff0c\u7528\u4e8e\u81ea\u56de\u5f52\u9884\u8bad\u7ec3\uff0c\u8be5\u8868\u793a\u6709\u5229\u4e8e\u673a\u5668\u4eba\n\u64cd\u4f5c\u4efb\u52a1\u3002\u53d7\u4eba\u7c7b\u901a\u8fc7\n\u89c2\u5bdf\u52a8\u6001\u73af\u5883\u5b66\u4e60\u65b0\u6280\u80fd\u7684\u65b9\u5f0f\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u6709\u6548\u7684\u673a\u5668\u4eba\u5b66\u4e60\n\u5e94\u8be5\u5f3a\u8c03\u4e0e\u8fd0\u52a8\u76f8\u5173\u7684\u77e5\u8bc6\uff0c\u8fd9\u4e0e\u4f4e\u7ea7\n\u52a8\u4f5c\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u4e0e\u786c\u4ef6\u65e0\u5173\uff0c\u4ece\u800c\u4fc3\u8fdb\u5b66\u4e60\u52a8\u4f5c\u7684\u8f6c\u79fb\n\u5230\u5b9e\u9645\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Moto\uff0c\u5b83\u5c06\u89c6\u9891\n\u5185\u5bb9\u901a\u8fc7\u6f5c\u5728\u8fd0\u52a8\u6807\u8bb0\u5316\u5668\u8f6c\u6362\u4e3a\u6f5c\u5728\u8fd0\u52a8\u6807\u8bb0\u5e8f\u5217\uff0c\n\u4ee5\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u4ece\u89c6\u9891\u4e2d\u5b66\u4e60\u8fd0\u52a8\u7684\u6865\u63a5\u201c\u8bed\u8a00\u201d\u3002\n\u6211\u4eec\u901a\u8fc7\u8fd0\u52a8\u6807\u8bb0\u81ea\u56de\u5f52\u5bf9 Moto-GPT \u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u5176\u80fd\u591f\n\u6355\u6349\u4e0d\u540c\u7684\u89c6\u89c9\u8fd0\u52a8\u77e5\u8bc6\u3002\u9884\u8bad\u7ec3\u540e\uff0cMoto-GPT\n\u5c55\u793a\u4e86\u751f\u6210\u8bed\u4e49\u53ef\u89e3\u91ca\u8fd0\u52a8\u6807\u8bb0\u3001\u9884\u6d4b\u5408\u7406\u7684\u8fd0\u52a8\u8f68\u8ff9\uff0c\u4ee5\u53ca\u8bc4\u4f30\u8f68\u8ff9\n\u901a\u8fc7\u8f93\u51fa\u53ef\u80fd\u6027\u8fdb\u884c\u5408\u7406\u6027\u3002\u4e3a\u4e86\u5c06\u5b66\u4e60\u5230\u7684\u8fd0\u52a8\u5148\u9a8c\u8f6c\u79fb\u5230\n\u771f\u6b63\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u6211\u4eec\u5b9e\u65bd\u4e86\u534f\u540c\u5fae\u8c03\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u65e0\u7f1d\n\u6865\u63a5\u6f5c\u5728\u8fd0\u52a8\u6807\u8bb0\u9884\u6d4b\u548c\u771f\u6b63\u7684\u673a\u5668\u4eba\u63a7\u5236\u3002\u5e7f\u6cdb\u7684\n\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u540e\u7684 Moto-GPT \u5728\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\n\u6548\u7387\uff0c\u5f3a\u8c03\u4e86\u5176\u4ece\u89c6\u9891\u6570\u636e\u5230\u4e0b\u6e38\u89c6\u89c9\u64cd\u7eb5\u4efb\u52a1\u8f6c\u79fb\u77e5\u8bc6\u7684\u6709\u6548\u6027\u3002</paragraph>", "author": "Yi Chen et.al.", "authors": "Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, Xihui Liu", "id": "2412.04445v1", "paper_url": "http://arxiv.org/abs/2412.04445v1", "repo": "null"}}