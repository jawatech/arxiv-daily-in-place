{"2406.20030": {"publish_time": "2024-06-28", "title": "LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models", "paper_summary": "Large language models (LLMs) require continual knowledge updates to stay\nabreast of the ever-changing world facts, prompting the formulation of lifelong\nmodel editing task. While recent years have witnessed the development of\nvarious techniques for single and batch editing, these methods either fail to\napply or perform sub-optimally when faced with lifelong editing. In this paper,\nwe introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong\nmodel editing. We first analyze the factors influencing the effectiveness of\nconventional MoE adaptor in lifelong editing, including catastrophic\nforgetting, inconsistent routing and order sensitivity. Based on these\ninsights, we propose a tailored module insertion method to achieve lifelong\nediting, incorporating a novel KV anchor routing to enhance routing consistency\nbetween training and inference stage, along with a concise yet effective\nclustering-based editing order planning. Experimental results demonstrate the\neffectiveness of our method in lifelong editing, surpassing previous model\nediting techniques while maintaining outstanding performance in batch editing\ntask. Our code will be available.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u6301\u7e8c\u66f4\u65b0\u77e5\u8b58\uff0c\u4ee5\u638c\u63e1\u77ac\u606f\u842c\u8b8a\u7684\u4e16\u754c\u4e8b\u5be6\uff0c\u4fc3\u4f7f\u5236\u5b9a\u7d42\u8eab\u6a21\u578b\u7de8\u8f2f\u4efb\u52d9\u3002\u5118\u7ba1\u8fd1\u5e74\u4f86\u898b\u8b49\u4e86\u5404\u7a2e\u55ae\u6b21\u548c\u6279\u6b21\u7de8\u8f2f\u6280\u8853\u7684\u767c\u5c55\uff0c\u4f46\u9019\u4e9b\u65b9\u6cd5\u5728\u9762\u5c0d\u7d42\u8eab\u7de8\u8f2f\u6642\uff0c\u8981\u4e0d\u7121\u6cd5\u61c9\u7528\uff0c\u8981\u4e0d\u5c31\u662f\u8868\u73fe\u4e0d\u4f73\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 LEMoE\uff0c\u4e00\u7a2e\u5148\u9032\u7684\u5c08\u5bb6\u6df7\u5408 (MoE) \u9069\u914d\u5668\uff0c\u7528\u65bc\u7d42\u8eab\u6a21\u578b\u7de8\u8f2f\u3002\u6211\u5011\u9996\u5148\u5206\u6790\u4e86\u5f71\u97ff\u50b3\u7d71 MoE \u9069\u914d\u5668\u5728\u7d42\u8eab\u7de8\u8f2f\u4e2d\u6709\u6548\u6027\u7684\u56e0\u7d20\uff0c\u5305\u62ec\u707d\u96e3\u6027\u907a\u5fd8\u3001\u4e0d\u4e00\u81f4\u7684\u8def\u7531\u548c\u9806\u5e8f\u654f\u611f\u6027\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u91cf\u8eab\u6253\u9020\u7684\u6a21\u7d44\u63d2\u5165\u65b9\u6cd5\u4f86\u5be6\u73fe\u7d42\u8eab\u7de8\u8f2f\uff0c\u7d50\u5408\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684 KV\u9328\u5b9a\u8def\u7531\uff0c\u4ee5\u589e\u5f37\u8a13\u7df4\u548c\u63a8\u7406\u968e\u6bb5\u4e4b\u9593\u7684\u8def\u7531\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u4e00\u7a2e\u7c21\u6f54\u4f46\u6709\u6548\u7684\u57fa\u65bc\u7fa4\u96c6\u7684\u7de8\u8f2f\u9806\u5e8f\u898f\u5283\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u7d42\u8eab\u7de8\u8f2f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u6a21\u578b\u7de8\u8f2f\u6280\u8853\uff0c\u540c\u6642\u5728\u6279\u6b21\u7de8\u8f2f\u4efb\u52d9\u4e2d\u4fdd\u6301\u51fa\u8272\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5c07\u6703\u63d0\u4f9b\u3002", "author": "Renzhi Wang et.al.", "authors": "Renzhi Wang, Piji Li", "id": "2406.20030v1", "paper_url": "http://arxiv.org/abs/2406.20030v1", "repo": "null"}}