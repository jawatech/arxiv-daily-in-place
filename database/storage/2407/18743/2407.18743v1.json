{"2407.18743": {"publish_time": "2024-07-26", "title": "Towards Effective and Efficient Continual Pre-training of Large Language Models", "paper_summary": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.", "paper_summary_zh": "\u6301\u7e8c\u9810\u8a13\u7df4 (CPT) \u4e00\u76f4\u662f\u5c07\u8a9e\u8a00\u6a21\u578b\u8abf\u6574\u5230\u7279\u5b9a\u9818\u57df\u6216\u4efb\u52d9\u7684\u91cd\u8981\u65b9\u6cd5\u3002\u70ba\u4e86\u8b93 CPT \u65b9\u6cd5\u66f4\u5177\u53ef\u8ffd\u6eaf\u6027\uff0c\u672c\u6587\u91dd\u5c0d Llama-3 (8B) \u7684\u6301\u7e8c\u9810\u8a13\u7df4\u63d0\u51fa\u6280\u8853\u5831\u544a\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u4e3b\u5e79\u6a21\u578b\u7684\u4e2d\u6587\u80fd\u529b\u548c\u79d1\u5b78\u63a8\u7406\u80fd\u529b\u3002\u70ba\u4e86\u5728\u4fdd\u7559\u539f\u672c\u80fd\u529b\u7684\u540c\u6642\u63d0\u5347\u65b0\u80fd\u529b\uff0c\u6211\u5011\u5229\u7528\u65e2\u6709\u8cc7\u6599\u96c6\u4e26\u5408\u6210\u9ad8\u54c1\u8cea\u8cc7\u6599\u96c6\uff0c\u8a2d\u8a08\u51fa\u7279\u5b9a\u7684\u8cc7\u6599\u6df7\u5408\u548c\u8ab2\u7a0b\u7b56\u7565\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u6839\u64da\u76f8\u95dc\u7db2\u9801\u5408\u6210\u8de8\u5b78\u79d1\u79d1\u5b78\u554f\u7b54 (QA) \u914d\u5c0d\uff0c\u4e26\u9032\u4e00\u6b65\u7d0d\u5165\u9019\u4e9b\u5408\u6210\u8cc7\u6599\uff0c\u4ee5\u63d0\u5347 Llama-3 \u7684\u79d1\u5b78\u63a8\u7406\u80fd\u529b\u3002\u6211\u5011\u5c07 CPT \u5f8c\u7684\u6a21\u578b\u7a31\u70ba Llama-3-SynE (\u5408\u6210\u8cc7\u6599\u589e\u5f37 Llama-3)\u3002\u6211\u5011\u4e5f\u63d0\u51fa\u4e00\u500b\u4f7f\u7528\u76f8\u5c0d\u8f03\u5c0f\u6a21\u578b TinyLlama \u7684\u5fae\u8abf\u5be6\u9a57\uff0c\u4e26\u904b\u7528\u5f97\u51fa\u7684\u767c\u73fe\u4f86\u8a13\u7df4\u4e3b\u5e79\u6a21\u578b\u3002\u5728\u591a\u500b\u8a55\u91cf\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5927\u5e45\u63d0\u5347\u4e86\u4e3b\u5e79\u6a21\u578b\u7684\u6548\u80fd\uff0c\u5305\u62ec\u4e00\u822c\u80fd\u529b\uff08\u5728 C-Eval \u4e0a\u63d0\u5347 +8.81\uff0c\u5728 CMMLU \u4e0a\u63d0\u5347 +6.31\uff09\u548c\u79d1\u5b78\u63a8\u7406\u80fd\u529b\uff08\u5728 MATH \u4e0a\u63d0\u5347 +12.00\uff0c\u5728 SciEval \u4e0a\u63d0\u5347 +4.13\uff09\uff0c\u4e14\u4e0d\u640d\u53ca\u539f\u672c\u7684\u80fd\u529b\u3002\u6211\u5011\u7684\u6a21\u578b\u3001\u8cc7\u6599\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/RUC-GSAI/Llama-3-SynE \u53d6\u5f97\u3002", "author": "Jie Chen et.al.", "authors": "Jie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yutao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang, Ji-Rong Wen", "id": "2407.18743v1", "paper_url": "http://arxiv.org/abs/2407.18743v1", "repo": "null"}}