{"2410.11325": {"publish_time": "2024-10-15", "title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling", "paper_summary": "Recent advances in knowledge distillation (KD) have enabled smaller student\nmodels to approach the performance of larger teacher models. However, popular\nmethods such as supervised KD and on-policy KD, are adversely impacted by the\nknowledge gaps between teacher-student in practical scenarios. Supervised KD\nsuffers from a distribution mismatch between training with a static dataset and\ninference over final student-generated outputs. Conversely, on-policy KD, which\nuses student-generated samples for training, can suffer from low-quality\ntraining examples with which teacher models are not familiar, resulting in\ninaccurate teacher feedback. To address these limitations, we introduce\nSpeculative Knowledge Distillation (SKD), a novel approach that leverages\ncooperation between student and teacher models to generate high-quality\ntraining data on-the-fly while aligning with the student's inference-time\ndistribution. In SKD, the student proposes tokens, and the teacher replaces\npoorly ranked ones based on its own distribution, transferring high-quality\nknowledge adaptively. We evaluate SKD on various text generation tasks,\nincluding translation, summarization, math, and instruction following, and show\nthat SKD consistently outperforms existing KD methods across different domains,\ndata sizes, and model initialization strategies.", "paper_summary_zh": "\u6700\u8fd1\u77e5\u8bc6\u63d0\u70bc (KD) \u7684\u8fdb\u6b65\u4f7f\u5f97\u8f83\u5c0f\u7684\u5b66\u751f\u6a21\u578b\u80fd\u591f\u63a5\u8fd1\u8f83\u5927\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u6d41\u884c\u7684\u65b9\u6cd5\uff0c\u5982\u76d1\u7763 KD \u548c\u57fa\u4e8e\u7b56\u7565 KD\uff0c\u4f1a\u53d7\u5230\u6559\u5e08\u548c\u5b66\u751f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u77e5\u8bc6\u5dee\u8ddd\u7684\u4e0d\u5229\u5f71\u54cd\u3002\u76d1\u7763 KD \u5b58\u5728\u8bad\u7ec3\u65f6\u9759\u6001\u6570\u636e\u96c6\u548c\u6700\u7ec8\u5b66\u751f\u751f\u6210\u8f93\u51fa\u7684\u63a8\u7406\u4e4b\u95f4\u7684\u5206\u5e03\u5931\u914d\u3002\u76f8\u53cd\uff0c\u57fa\u4e8e\u7b56\u7565 KD \u4f7f\u7528\u5b66\u751f\u751f\u6210\u7684\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u80fd\u4f1a\u53d7\u5230\u6559\u5e08\u6a21\u578b\u4e0d\u719f\u6089\u7684\u4f4e\u8d28\u91cf\u8bad\u7ec3\u793a\u4f8b\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5bfc\u81f4\u6559\u5e08\u53cd\u9988\u4e0d\u51c6\u786e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u63a8\u6d4b\u77e5\u8bc6\u63d0\u70bc (SKD)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5b66\u751f\u548c\u6559\u5e08\u6a21\u578b\u4e4b\u95f4\u7684\u5408\u4f5c\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u540c\u65f6\u4e0e\u5b66\u751f\u7684\u63a8\u7406\u65f6\u95f4\u5206\u5e03\u4fdd\u6301\u4e00\u81f4\u3002\u5728 SKD \u4e2d\uff0c\u5b66\u751f\u63d0\u51fa\u6807\u8bb0\uff0c\u6559\u5e08\u6839\u636e\u81ea\u5df1\u7684\u5206\u5e03\u66ff\u6362\u6392\u540d\u8f83\u5dee\u7684\u6807\u8bb0\uff0c\u81ea\u9002\u5e94\u5730\u8f6c\u79fb\u9ad8\u8d28\u91cf\u7684\u77e5\u8bc6\u3002\u6211\u4eec\u5728\u5404\u79cd\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86 SKD\uff0c\u5305\u62ec\u7ffb\u8bd1\u3001\u6458\u8981\u3001\u6570\u5b66\u548c\u6307\u4ee4\u9075\u5faa\uff0c\u5e76\u8868\u660e SKD \u5728\u4e0d\u540c\u7684\u9886\u57df\u3001\u6570\u636e\u5927\u5c0f\u548c\u6a21\u578b\u521d\u59cb\u5316\u7b56\u7565\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684 KD \u65b9\u6cd5\u3002", "author": "Wenda Xu et.al.", "authors": "Wenda Xu, Rujun Han, Zifeng Wang, Long T. Le, Dhruv Madeka, Lei Li, William Yang Wang, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister", "id": "2410.11325v1", "paper_url": "http://arxiv.org/abs/2410.11325v1", "repo": "null"}}