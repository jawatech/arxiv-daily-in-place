{"2407.04681": {"publish_time": "2024-07-05", "title": "Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge", "paper_summary": "In recent years, multimodal large language models (MLLMs) have made\nsignificant strides by training on vast high-quality image-text datasets,\nenabling them to generally understand images well. However, the inherent\ndifficulty in explicitly conveying fine-grained or spatially dense information\nin text, such as masks, poses a challenge for MLLMs, limiting their ability to\nanswer questions requiring an understanding of detailed or localized visual\nelements. Drawing inspiration from the Retrieval-Augmented Generation (RAG)\nconcept, this paper proposes a new visual prompt approach to integrate\nfine-grained external knowledge, gleaned from specialized vision models (e.g.,\ninstance segmentation/OCR models), into MLLMs. This is a promising yet\nunderexplored direction for enhancing MLLMs' performance. Our approach diverges\nfrom concurrent works, which transform external knowledge into additional text\nprompts, necessitating the model to indirectly learn the correspondence between\nvisual content and text coordinates. Instead, we propose embedding fine-grained\nknowledge information directly into a spatial embedding map as a visual prompt.\nThis design can be effortlessly incorporated into various MLLMs, such as LLaVA\nand Mipha, considerably improving their visual understanding performance.\nThrough rigorous experiments, we demonstrate that our method can enhance MLLM\nperformance across nine benchmarks, amplifying their fine-grained context-aware\ncapabilities.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5728\u5927\u91cf\u9ad8\u54c1\u8cea\u5f71\u50cf\u6587\u5b57\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u5f8c\uff0c\u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u8b93\u5b83\u5011\u80fd\u666e\u904d\u7406\u89e3\u5f71\u50cf\u3002\u7136\u800c\uff0c\u5728\u6587\u5b57\u4e2d\u660e\u78ba\u50b3\u9054\u7cbe\u7d30\u6216\u7a7a\u9593\u5bc6\u96c6\u7684\u8cc7\u8a0a\uff08\u4f8b\u5982\u906e\u7f69\uff09\u7684\u56fa\u6709\u56f0\u96e3\uff0c\u5c0d MLLM \u69cb\u6210\u6311\u6230\uff0c\u9650\u5236\u4e86\u5b83\u5011\u56de\u7b54\u9700\u8981\u4e86\u89e3\u8a73\u7d30\u6216\u5c40\u90e8\u8996\u89ba\u5143\u7d20\u7684\u554f\u984c\u7684\u80fd\u529b\u3002\u672c\u6587\u5f9e\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u6982\u5ff5\u4e2d\u6c72\u53d6\u9748\u611f\uff0c\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u8996\u89ba\u63d0\u793a\u65b9\u6cd5\uff0c\u4ee5\u6574\u5408\u5f9e\u5c08\u696d\u8996\u89ba\u6a21\u578b\uff08\u4f8b\u5982\u5be6\u4f8b\u5206\u5272/OCR \u6a21\u578b\uff09\u6536\u96c6\u5230\u7684\u7cbe\u7d30\u5916\u90e8\u77e5\u8b58\uff0c\u878d\u5165 MLLM \u4e2d\u3002\u9019\u662f\u589e\u5f37 MLLM \u6548\u80fd\u7684\u4e00\u500b\u6709\u524d\u666f\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u65b9\u5411\u3002\u6211\u5011\u7684\u505a\u6cd5\u8207\u540c\u6642\u9032\u884c\u7684\u5de5\u4f5c\u4e0d\u540c\uff0c\u9019\u4e9b\u5de5\u4f5c\u5c07\u5916\u90e8\u77e5\u8b58\u8f49\u63db\u70ba\u984d\u5916\u7684\u6587\u5b57\u63d0\u793a\uff0c\u9700\u8981\u6a21\u578b\u9593\u63a5\u5b78\u7fd2\u8996\u89ba\u5167\u5bb9\u548c\u6587\u5b57\u5ea7\u6a19\u4e4b\u9593\u7684\u5c0d\u61c9\u95dc\u4fc2\u3002\u76f8\u53cd\u5730\uff0c\u6211\u5011\u5efa\u8b70\u5c07\u7cbe\u7d30\u7684\u77e5\u8b58\u8cc7\u8a0a\u76f4\u63a5\u5d4c\u5165\u7a7a\u9593\u5d4c\u5165\u6620\u5c04\u4e2d\uff0c\u4f5c\u70ba\u8996\u89ba\u63d0\u793a\u3002\u6b64\u8a2d\u8a08\u53ef\u4ee5\u6beb\u4e0d\u8cbb\u529b\u5730\u6574\u5408\u5230\u5404\u7a2e MLLM \u4e2d\uff0c\u4f8b\u5982 LLaVA \u548c Mipha\uff0c\u5927\u5e45\u63d0\u5347\u5b83\u5011\u7684\u8996\u89ba\u7406\u89e3\u6548\u80fd\u3002\u900f\u904e\u56b4\u8b39\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f37 MLLM \u5728\u4e5d\u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7684\u6548\u80fd\uff0c\u64f4\u5927\u5b83\u5011\u7684\u7cbe\u7d30\u8108\u7d61\u611f\u77e5\u80fd\u529b\u3002", "author": "Yuanze Lin et.al.", "authors": "Yuanze Lin, Yunsheng Li, Dongdong Chen, Weijian Xu, Ronald Clark, Philip Torr, Lu Yuan", "id": "2407.04681v1", "paper_url": "http://arxiv.org/abs/2407.04681v1", "repo": "null"}}