{"2412.18273": {"publish_time": "2024-12-24", "title": "Sampling Bag of Views for Open-Vocabulary Object Detection", "paper_summary": "Existing open-vocabulary object detection (OVD) develops methods for testing\nunseen categories by aligning object region embeddings with corresponding VLM\nfeatures. A recent study leverages the idea that VLMs implicitly learn\ncompositional structures of semantic concepts within the image. Instead of\nusing an individual region embedding, it utilizes a bag of region embeddings as\na new representation to incorporate compositional structures into the OVD task.\nHowever, this approach often fails to capture the contextual concepts of each\nregion, leading to noisy compositional structures. This results in only\nmarginal performance improvements and reduced efficiency. To address this, we\npropose a novel concept-based alignment method that samples a more powerful and\nefficient compositional structure. Our approach groups contextually related\n``concepts'' into a bag and adjusts the scale of concepts within the bag for\nmore effective embedding alignment. Combined with Faster R-CNN, our method\nachieves improvements of 2.6 box AP50 and 0.5 mask AP over prior work on novel\ncategories in the open-vocabulary COCO and LVIS benchmarks. Furthermore, our\nmethod reduces CLIP computation in FLOPs by 80.3% compared to previous\nresearch, significantly enhancing efficiency. Experimental results demonstrate\nthat the proposed method outperforms previous state-of-the-art models on the\nOVD datasets.", "paper_summary_zh": "\u73fe\u6709\u7684\u958b\u653e\u5f0f\u8a5e\u5f59\u76ee\u6a19\u5075\u6e2c (OVD) \u767c\u5c55\u51fa\u900f\u904e\u5c07\u76ee\u6a19\u5340\u57df\u5d4c\u5165\u8207\u5c0d\u61c9\u7684 VLM \u7279\u5fb5\u5c0d\u9f4a\uff0c\u4f86\u6e2c\u8a66\u672a\u898b\u985e\u5225\u7684\u65b9\u6cd5\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5229\u7528 VLM \u96b1\u542b\u5730\u5b78\u7fd2\u5f71\u50cf\u4e2d\u8a9e\u610f\u6982\u5ff5\u7684\u7d44\u5408\u7d50\u69cb\u9019\u500b\u60f3\u6cd5\u3002\u5b83\u4f7f\u7528\u5340\u57df\u5d4c\u5165\u7684\u888b\u5b50\u4f5c\u70ba\u65b0\u7684\u8868\u793a\uff0c\u4f86\u5c07\u7d44\u5408\u7d50\u69cb\u7d0d\u5165 OVD \u4efb\u52d9\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u500b\u5225\u5340\u57df\u5d4c\u5165\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u901a\u5e38\u7121\u6cd5\u6355\u6349\u6bcf\u500b\u5340\u57df\u7684\u8108\u7d61\u6982\u5ff5\uff0c\u5c0e\u81f4\u7d44\u5408\u7d50\u69cb\u6709\u96dc\u8a0a\u3002\u9019\u53ea\u6703\u5e36\u4f86\u908a\u969b\u6548\u80fd\u6539\u5584\u548c\u964d\u4f4e\u6548\u7387\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u57fa\u65bc\u6982\u5ff5\u7684\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u4f86\u53d6\u6a23\u66f4\u5f37\u5927\u4e14\u6709\u6548\u7684\u7d44\u5408\u7d50\u69cb\u3002\u6211\u5011\u7684\u505a\u6cd5\u5c07\u8108\u7d61\u76f8\u95dc\u7684\u300c\u6982\u5ff5\u300d\u5206\u7d44\u6210\u4e00\u500b\u888b\u5b50\uff0c\u4e26\u8abf\u6574\u888b\u5b50\u4e2d\u6982\u5ff5\u7684\u898f\u6a21\uff0c\u4ee5\u9032\u884c\u66f4\u6709\u6548\u7684\u5d4c\u5165\u5c0d\u9f4a\u3002\u6211\u5011\u7684\u505a\u6cd5\u7d50\u5408\u4e86 Faster R-CNN\uff0c\u5728\u958b\u653e\u5f0f\u8a5e\u5f59 COCO \u548c LVIS \u57fa\u6e96\u4e2d\uff0c\u91dd\u5c0d\u65b0\u985e\u5225\u53d6\u5f97\u4e86 2.6 \u500b\u6846 AP50 \u548c 0.5 \u500b\u906e\u7f69 AP \u7684\u6539\u9032\u3002\u6b64\u5916\uff0c\u8207\u5148\u524d\u7684\u7814\u7a76\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5c07 FLOP \u4e2d\u7684 CLIP \u8a08\u7b97\u6e1b\u5c11\u4e86 80.3%\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6548\u7387\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 OVD \u8cc7\u6599\u96c6\u4e0a\u512a\u65bc\u5148\u524d\u7684\u6700\u5148\u9032\u6a21\u578b\u3002", "author": "Hojun Choi et.al.", "authors": "Hojun Choi, Junsuk Choe, Hyunjung Shim", "id": "2412.18273v1", "paper_url": "http://arxiv.org/abs/2412.18273v1", "repo": "null"}}