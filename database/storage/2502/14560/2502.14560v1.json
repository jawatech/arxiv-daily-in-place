{"2502.14560": {"publish_time": "2025-02-20", "title": "Less is More: Improving LLM Alignment via Preference Data Selection", "paper_summary": "Direct Preference Optimization (DPO) has emerged as a promising approach for\naligning large language models with human preferences. While prior work mainly\nextends DPO from the aspect of the objective function, we instead improve DPO\nfrom the largely overlooked but critical aspect of data selection.\nSpecifically, we address the issue of parameter shrinkage caused by noisy data\nby proposing a novel margin-maximization principle for dataset curation in DPO\ntraining. To accurately estimate margins for data selection, we propose a\ndual-margin guided approach that considers both external reward margins and\nimplicit DPO reward margins. Extensive experiments demonstrate that our method\nreduces computational cost dramatically while improving performance.\nRemarkably, by using just 10\\% of the Ultrafeedback dataset, our approach\nachieves 3\\% to 8\\% improvements across various Llama and Mistral series models\non the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends\nto iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data,\nwhile further reducing training time. These results highlight the potential of\ndata selection strategies for advancing preference optimization.", "paper_summary_zh": "\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\uff0c\u53ef\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u5f9e\u76ee\u6a19\u51fd\u6578\u7684\u89d2\u5ea6\u5ef6\u4f38 DPO\uff0c\u4f46\u6211\u5011\u53cd\u800c\u5f9e\u8cc7\u6599\u9078\u64c7\u9019\u500b\u6975\u6613\u88ab\u5ffd\u7565\u4f46\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u5ea6\u6539\u9032 DPO\u3002\n\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u900f\u904e\u63d0\u51fa\u4e00\u500b\u7528\u65bc DPO \u8a13\u7df4\u4e2d\u8cc7\u6599\u96c6\u6574\u7406\u7684\u65b0\u908a\u969b\u6700\u5927\u5316\u539f\u5247\uff0c\u4f86\u89e3\u6c7a\u7531\u96dc\u8a0a\u8cc7\u6599\u9020\u6210\u7684\u53c3\u6578\u6536\u7e2e\u554f\u984c\u3002\u70ba\u4e86\u6e96\u78ba\u4f30\u8a08\u8cc7\u6599\u9078\u64c7\u7684\u908a\u969b\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u96d9\u908a\u969b\u5f15\u5c0e\u65b9\u6cd5\uff0c\u5b83\u540c\u6642\u8003\u616e\u5916\u90e8\u734e\u52f5\u908a\u969b\u548c\u96b1\u542b DPO \u734e\u52f5\u908a\u969b\u3002\u5927\u898f\u6a21\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u9019\u7a2e\u65b9\u6cd5\u5927\u5e45\u964d\u4f4e\u4e86\u904b\u7b97\u6210\u672c\uff0c\u540c\u6642\u6539\u5584\u4e86\u6548\u80fd\u3002\n\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u9019\u7a2e\u65b9\u6cd5\u50c5\u4f7f\u7528 Ultrafeedback \u8cc7\u6599\u96c6\u7684 10%\uff0c\u4fbf\u5728 AlpacaEval 2.0 \u57fa\u6e96\u4e0a\uff0c\u5728\u5404\u7a2e Llama \u548c Mistral \u7cfb\u5217\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86 3% \u5230 8% \u7684\u6539\u9032\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u9019\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u7121\u7e2b\u5730\u5ef6\u4f38\u5230\u8fed\u4ee3 DPO\uff0c\u5728\u4f7f\u7528 25% \u7dda\u4e0a\u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\u7522\u751f\u4e86\u5927\u7d04 3% \u7684\u6539\u9032\uff0c\u540c\u6642\u9032\u4e00\u6b65\u6e1b\u5c11\u4e86\u8a13\u7df4\u6642\u9593\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86\u8cc7\u6599\u9078\u64c7\u7b56\u7565\u5728\u63a8\u9032\u504f\u597d\u6700\u4f73\u5316\u65b9\u9762\u7684\u6f5b\u529b\u3002", "author": "Xun Deng et.al.", "authors": "Xun Deng, Han Zhong, Rui Ai, Fuli Feng, Zheng Wang, Xiangnan He", "id": "2502.14560v1", "paper_url": "http://arxiv.org/abs/2502.14560v1", "repo": "null"}}