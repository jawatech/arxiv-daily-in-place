{"2412.00765": {"publish_time": "2024-12-01", "title": "SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts", "paper_summary": "Traditional methods for evaluating the robustness of large language models\n(LLMs) often rely on standardized benchmarks, which can escalate costs and\nlimit evaluations across varied domains. This paper introduces a novel\nframework designed to autonomously evaluate the robustness of LLMs by\nincorporating refined adversarial prompts and domain-constrained knowledge\nguidelines in the form of knowledge graphs. Our method systematically generates\ndescriptive sentences from domain-constrained knowledge graph triplets to\nformulate adversarial prompts, enhancing the relevance and challenge of the\nevaluation. These prompts, generated by the LLM itself and tailored to evaluate\nits own robustness, undergo a rigorous filtering and refinement process,\nensuring that only those with high textual fluency and semantic fidelity are\nused. This self-evaluation mechanism allows the LLM to evaluate its robustness\nwithout the need for external benchmarks. We assess the effectiveness of our\nframework through extensive testing on both proprietary models like ChatGPT and\nopen-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that\nour approach not only reduces dependency on conventional data but also provides\na targeted and efficient means of evaluating LLM robustness in constrained\ndomains.", "paper_summary_zh": "\u50b3\u7d71\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7a69\u5065\u6027\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u6a19\u6e96\u5316\u57fa\u6e96\uff0c\u9019\u53ef\u80fd\u6703\u589e\u52a0\u6210\u672c\u4e26\u9650\u5236\u8de8\u4e0d\u540c\u9818\u57df\u7684\u8a55\u4f30\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u65e8\u5728\u900f\u904e\u5728\u77e5\u8b58\u5716\u8b5c\u7684\u5f62\u5f0f\u4e2d\u7d0d\u5165\u7cbe\u7dfb\u7684\u5c0d\u6297\u63d0\u793a\u548c\u9818\u57df\u7d04\u675f\u77e5\u8b58\u6e96\u5247\uff0c\u4f86\u81ea\u4e3b\u8a55\u4f30 LLM \u7684\u7a69\u5065\u6027\u3002\u6211\u5011\u7684\u505a\u6cd5\u662f\u7cfb\u7d71\u6027\u5730\u5f9e\u9818\u57df\u7d04\u675f\u77e5\u8b58\u5716\u8b5c\u4e09\u5143\u7d44\u4e2d\u7522\u751f\u63cf\u8ff0\u6027\u53e5\u5b50\uff0c\u4ee5\u5236\u5b9a\u5c0d\u6297\u63d0\u793a\uff0c\u589e\u5f37\u8a55\u4f30\u7684\u76f8\u95dc\u6027\u548c\u6311\u6230\u6027\u3002\u9019\u4e9b\u63d0\u793a\u662f\u7531 LLM \u672c\u8eab\u7522\u751f\uff0c\u4e26\u91dd\u5c0d\u8a55\u4f30\u5176\u81ea\u8eab\u7684\u7a69\u5065\u6027\u800c\u91cf\u8eab\u6253\u9020\uff0c\u5b83\u5011\u6703\u7d93\u6b77\u56b4\u683c\u7684\u904e\u6ffe\u548c\u7cbe\u7149\u904e\u7a0b\uff0c\u78ba\u4fdd\u53ea\u6709\u90a3\u4e9b\u5177\u6709\u9ad8\u5ea6\u6587\u672c\u6d41\u66a2\u6027\u548c\u8a9e\u7fa9\u4fdd\u771f\u6027\u7684\u63d0\u793a\u624d\u6703\u88ab\u4f7f\u7528\u3002\u9019\u7a2e\u81ea\u6211\u8a55\u4f30\u6a5f\u5236\u5141\u8a31 LLM \u5728\u4e0d\u9700\u8981\u5916\u90e8\u57fa\u6e96\u7684\u60c5\u6cc1\u4e0b\u8a55\u4f30\u5176\u7a69\u5065\u6027\u3002\u6211\u5011\u900f\u904e\u5c0d\u5c08\u6709\u6a21\u578b\uff08\u4f8b\u5982 ChatGPT\uff09\u548c\u958b\u6e90\u6a21\u578b\uff08\u4f8b\u5982 Llama-3.1\u3001Phi-3 \u548c Mistral\uff09\u9032\u884c\u5ee3\u6cdb\u6e2c\u8a66\uff0c\u8a55\u4f30\u6211\u5011\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u7d50\u679c\u8b49\u5be6\uff0c\u6211\u5011\u7684\u505a\u6cd5\u4e0d\u50c5\u6e1b\u5c11\u4e86\u5c0d\u50b3\u7d71\u8cc7\u6599\u7684\u4f9d\u8cf4\u6027\uff0c\u9084\u63d0\u4f9b\u4e86\u4e00\u7a2e\u6709\u91dd\u5c0d\u6027\u548c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u53d7\u9650\u9818\u57df\u4e2d\u8a55\u4f30 LLM \u7684\u7a69\u5065\u6027\u3002", "author": "Aihua Pei et.al.", "authors": "Aihua Pei, Zehua Yang, Shunan Zhu, Ruoxi Cheng, Ju Jia", "id": "2412.00765v1", "paper_url": "http://arxiv.org/abs/2412.00765v1", "repo": "null"}}