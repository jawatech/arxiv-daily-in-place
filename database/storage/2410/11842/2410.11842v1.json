{"2410.11842": {"publish_time": "2024-10-15", "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention", "paper_summary": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.", "paper_summary_zh": "<paragraph>\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5347\u7d1a\u4e86\u591a\u982d\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4e5f\u5c31\u662f Transformer \u6a21\u578b\u7684\u6838\u5fc3\uff0c\u4ee5\u63d0\u5347\u6548\u7387\uff0c\u540c\u6642\u7dad\u6301\u6216\u8d85\u8d8a\u4e4b\u524d\u7684\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u5c55\u793a\u4e86\u591a\u982d\u6ce8\u610f\u529b\u53ef\u4ee5\u7528\u7e3d\u548c\u5f62\u5f0f\u8868\u793a\u3002\u6839\u64da\u9019\u6a23\u7684\u898b\u89e3\uff0c\u5373\u4e26\u975e\u6240\u6709\u6ce8\u610f\u529b\u982d\u90fd\u5177\u6709\u540c\u7b49\u7684\u91cd\u8981\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u6df7\u5408\u982d\u6ce8\u610f\u529b (MoH)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u67b6\u69cb\uff0c\u5c07\u6ce8\u610f\u529b\u982d\u8996\u70ba\u6df7\u5408\u5c08\u5bb6 (MoE) \u6a5f\u5236\u4e2d\u7684\u5c08\u5bb6\u3002MoH \u6709\u5169\u500b\u986f\u8457\u512a\u9ede\uff1a\u9996\u5148\uff0cMoH \u80fd\u8b93\u6bcf\u500b\u7b26\u865f\u9078\u64c7\u9069\u7576\u7684\u6ce8\u610f\u529b\u982d\uff0c\u589e\u5f37\u63a8\u8ad6\u6548\u7387\uff0c\u540c\u6642\u4e0d\u5f71\u97ff\u6e96\u78ba\u5ea6\u6216\u589e\u52a0\u53c3\u6578\u6578\u91cf\u3002\u5176\u6b21\uff0cMoH \u4ee5\u52a0\u6b0a\u7e3d\u548c\u53d6\u4ee3\u591a\u982d\u6ce8\u610f\u529b\u4e2d\u7684\u6a19\u6e96\u7e3d\u548c\uff0c\u70ba\u6ce8\u610f\u529b\u6a5f\u5236\u5f15\u5165\u9748\u6d3b\u6027\uff0c\u4e26\u91cb\u653e\u984d\u5916\u7684\u6548\u80fd\u6f5b\u529b\u3002\u5728 ViT\u3001DiT \u548c LLM \u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 MoH \u53ea\u4f7f\u7528 50%-90% \u7684\u6ce8\u610f\u529b\u982d\u5c31\u512a\u65bc\u591a\u982d\u6ce8\u610f\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e\u4e86\u9810\u5148\u8a13\u7df4\u7684\u591a\u982d\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u4f8b\u5982 LLaMA3-8B\uff0c\u53ef\u4ee5\u9032\u4e00\u6b65\u6301\u7e8c\u8abf\u6574\u5230\u6211\u5011\u7684 MoH \u6a21\u578b\u4e2d\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMoH-LLaMA3-8B \u5728 14 \u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u7684\u5e73\u5747\u6e96\u78ba\u5ea6\u9054\u5230 64.0%\uff0c\u50c5\u4f7f\u7528 75% \u7684\u6ce8\u610f\u529b\u982d\u5c31\u6bd4 LLaMA3-8B \u9ad8\u51fa 2.4%\u3002\u6211\u5011\u76f8\u4fe1\u6240\u63d0\u51fa\u7684 MoH \u662f\u591a\u982d\u6ce8\u610f\u529b\u7684\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e26\u70ba\u958b\u767c\u5148\u9032\u4e14\u9ad8\u6548\u7684\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5805\u5be6\u7684\u57fa\u790e\u3002</paragraph>", "author": "Peng Jin et.al.", "authors": "Peng Jin, Bo Zhu, Li Yuan, Shuicheng Yan", "id": "2410.11842v1", "paper_url": "http://arxiv.org/abs/2410.11842v1", "repo": "https://github.com/skyworkai/moh"}}