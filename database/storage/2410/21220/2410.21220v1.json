{"2410.21220": {"publish_time": "2024-10-28", "title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines", "paper_summary": "Search engines enable the retrieval of unknown information with texts.\nHowever, traditional methods fall short when it comes to understanding\nunfamiliar visual content, such as identifying an object that the model has\nnever seen before. This challenge is particularly pronounced for large\nvision-language models (VLMs): if the model has not been exposed to the object\ndepicted in an image, it struggles to generate reliable answers to the user's\nquestion regarding that image. Moreover, as new objects and events continuously\nemerge, frequently updating VLMs is impractical due to heavy computational\nburdens. To address this limitation, we propose Vision Search Assistant, a\nnovel framework that facilitates collaboration between VLMs and web agents.\nThis approach leverages VLMs' visual understanding capabilities and web agents'\nreal-time information access to perform open-world Retrieval-Augmented\nGeneration via the web. By integrating visual and textual representations\nthrough this collaboration, the model can provide informed responses even when\nthe image is novel to the system. Extensive experiments conducted on both\nopen-set and closed-set QA benchmarks demonstrate that the Vision Search\nAssistant significantly outperforms the other models and can be widely applied\nto existing VLMs.", "paper_summary_zh": "\u641c\u5c0b\u5f15\u64ce\u80fd\u7528\u6587\u5b57\u641c\u5c0b\u5230\u672a\u77e5\u7684\u8cc7\u8a0a\u3002\n\u7136\u800c\uff0c\u50b3\u7d71\u7684\u65b9\u6cd5\u5728\u7406\u89e3\u964c\u751f\u7684\u8996\u89ba\u5167\u5bb9\u6642\uff0c\u4f8b\u5982\u8b58\u5225\u6a21\u578b\u5f9e\u672a\u770b\u904e\u7684\u7269\u4ef6\u6642\uff0c\u5c31\u6703\u6709\u6240\u4e0d\u8db3\u3002\u9019\u500b\u6311\u6230\u5c0d\u65bc\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4f86\u8aaa\u5c24\u5176\u660e\u986f\uff1a\u5982\u679c\u6a21\u578b\u6c92\u6709\u63a5\u89f8\u904e\u5716\u7247\u4e2d\u63cf\u7e6a\u7684\u7269\u4ef6\uff0c\u5b83\u5c31\u6703\u96e3\u4ee5\u5c0d\u4f7f\u7528\u8005\u7684\u554f\u984c\u7522\u751f\u53ef\u9760\u7684\u7b54\u6848\u3002\u6b64\u5916\uff0c\u7531\u65bc\u65b0\u7684\u7269\u4ef6\u548c\u4e8b\u4ef6\u4e0d\u65b7\u51fa\u73fe\uff0c\u983b\u7e41\u66f4\u65b0 VLM \u56e0\u70ba\u6c89\u91cd\u7684\u904b\u7b97\u8ca0\u64d4\u800c\u4e0d\u53ef\u884c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u8996\u89ba\u641c\u5c0b\u52a9\u7406\uff0c\u4e00\u500b\u4fc3\u9032 VLM \u548c\u7db2\u8def\u4ee3\u7406\u4e4b\u9593\u5354\u4f5c\u7684\u65b0\u7a4e\u67b6\u69cb\u3002\u9019\u7a2e\u65b9\u6cd5\u5229\u7528 VLM \u7684\u8996\u89ba\u7406\u89e3\u80fd\u529b\u548c\u7db2\u8def\u4ee3\u7406\u7684\u5373\u6642\u8cc7\u8a0a\u5b58\u53d6\uff0c\u900f\u904e\u7db2\u8def\u57f7\u884c\u958b\u653e\u4e16\u754c\u7684\u6aa2\u7d22\u589e\u5f37\u751f\u6210\u3002\u85c9\u7531\u900f\u904e\u9019\u7a2e\u5354\u4f5c\u6574\u5408\u8996\u89ba\u548c\u6587\u5b57\u8868\u5fb5\uff0c\u5373\u4f7f\u7cfb\u7d71\u5c0d\u5716\u7247\u5f88\u964c\u751f\uff0c\u6a21\u578b\u4e5f\u80fd\u63d0\u4f9b\u6709\u6839\u64da\u7684\u56de\u61c9\u3002\u5728\u958b\u653e\u5f0f\u548c\u5c01\u9589\u5f0f\u554f\u7b54\u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u8996\u89ba\u641c\u5c0b\u52a9\u7406\u660e\u986f\u512a\u65bc\u5176\u4ed6\u6a21\u578b\uff0c\u4e26\u4e14\u53ef\u4ee5\u5ee3\u6cdb\u61c9\u7528\u65bc\u73fe\u6709\u7684 VLM\u3002", "author": "Zhixin Zhang et.al.", "authors": "Zhixin Zhang, Yiyuan Zhang, Xiaohan Ding, Xiangyu Yue", "id": "2410.21220v1", "paper_url": "http://arxiv.org/abs/2410.21220v1", "repo": "https://github.com/cnzzx/vsa"}}