{"2405.15232": {"publish_time": "2024-05-24", "title": "DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception", "paper_summary": "The development of large language models (LLMs) has significantly advanced\nthe emergence of large multimodal models (LMMs). While LMMs have achieved\ntremendous success by promoting the synergy between multimodal comprehension\nand creation, they often face challenges when confronted with\nout-of-distribution data. This is primarily due to their reliance on image\nencoders trained to encode images into task-relevant features, which may lead\nthem to disregard irrelevant details. Delving into the modeling capabilities of\ndiffusion models for images naturally prompts the question: Can diffusion\nmodels serve as the eyes of large language models for image perception? In this\npaper, we propose DEEM, a simple and effective approach that utilizes the\ngenerative feedback of diffusion models to align the semantic distributions of\nthe image encoder. This addresses the drawbacks of previous methods that solely\nrelied on image encoders like ViT, thereby enhancing the model's resilience\nagainst out-of-distribution samples and reducing visual hallucinations.\nImportantly, this is achieved without requiring additional training modules and\nwith fewer training parameters. We extensively evaluated DEEM on both our newly\nconstructed RobustVQA benchmark and another well-known benchmark, POPE, for\nobject hallucination. Compared to the state-of-the-art interleaved content\ngeneration models, DEEM exhibits enhanced robustness and a superior capacity to\nalleviate model hallucinations while utilizing fewer trainable parameters, less\npre-training data (10%), and a smaller base model size.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u767c\u5c55\u986f\u8457\u4fc3\u9032\u4e86\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\uff08LMM\uff09\u7684\u51fa\u73fe\u3002\u5118\u7ba1 LMM \u900f\u904e\u4fc3\u9032\u591a\u6a21\u614b\u7406\u89e3\u548c\u5275\u4f5c\u4e4b\u9593\u7684\u5354\u540c\u6548\u61c9\u800c\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u6210\u529f\uff0c\u4f46\u5b83\u5011\u5728\u9762\u5c0d\u5206\u5e03\u5916\u6578\u64da\u6642\u5e38\u5e38\u6703\u9047\u5230\u6311\u6230\u3002\u9019\u4e3b\u8981\u662f\u56e0\u70ba\u5b83\u5011\u4f9d\u8cf4\u65bc\u5716\u50cf\u7de8\u78bc\u5668\uff0c\u800c\u5716\u50cf\u7de8\u78bc\u5668\u7d93\u904e\u8a13\u7df4\uff0c\u53ef\u4ee5\u5c07\u5716\u50cf\u7de8\u78bc\u6210\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u7279\u5fb5\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u5b83\u5011\u5ffd\u7565\u7121\u95dc\u7684\u7d30\u7bc0\u3002\u6df1\u5165\u63a2\u8a0e\u7528\u65bc\u5716\u50cf\u7684\u64f4\u6563\u6a21\u578b\u7684\u5efa\u6a21\u80fd\u529b\u81ea\u7136\u6703\u5f15\u767c\u4e00\u500b\u554f\u984c\uff1a\u64f4\u6563\u6a21\u578b\u53ef\u4ee5\u4f5c\u70ba\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u300c\u773c\u775b\u300d\u7528\u65bc\u5716\u50cf\u611f\u77e5\u55ce\uff1f\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 DEEM\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u64f4\u6563\u6a21\u578b\u7684\u751f\u6210\u53cd\u994b\u4f86\u6821\u6e96\u5716\u50cf\u7de8\u78bc\u5668\u7684\u8a9e\u7fa9\u5206\u5e03\u3002\u9019\u89e3\u6c7a\u4e86\u4ee5\u524d\u50c5\u4f9d\u8cf4\u65bc ViT \u7b49\u5716\u50cf\u7de8\u78bc\u5668\u7684\u7f3a\u9ede\uff0c\u5f9e\u800c\u589e\u5f37\u4e86\u6a21\u578b\u5c0d\u5206\u5e03\u5916\u6a23\u672c\u7684\u9069\u61c9\u529b\u4e26\u6e1b\u5c11\u4e86\u8996\u89ba\u5e7b\u89ba\u3002\u91cd\u8981\u7684\u662f\uff0c\u9019\u662f\u5728\u4e0d\u9700\u8981\u984d\u5916\u8a13\u7df4\u6a21\u7d44\u548c\u4f7f\u7528\u8f03\u5c11\u8a13\u7df4\u53c3\u6578\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u7684\u3002\u6211\u5011\u5728\u6211\u5011\u65b0\u69cb\u5efa\u7684 RobustVQA \u8a55\u91cf\u6a19\u6e96\u548c\u53e6\u4e00\u500b\u8457\u540d\u7684\u8a55\u91cf\u6a19\u6e96 POPE \u4e0a\u5c0d DEEM \u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u8a55\u4f30\uff0c\u4ee5\u9032\u884c\u7269\u4ef6\u5e7b\u89ba\u3002\u8207\u6700\u5148\u9032\u7684\u4ea4\u932f\u5167\u5bb9\u751f\u6210\u6a21\u578b\u76f8\u6bd4\uff0cDEEM \u8868\u73fe\u51fa\u589e\u5f37\u7684\u9069\u61c9\u529b\u548c\u512a\u8d8a\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u5728\u4f7f\u7528\u8f03\u5c11\u7684\u53ef\u8a13\u7df4\u53c3\u6578\u3001\u8f03\u5c11\u7684\u9810\u8a13\u7df4\u6578\u64da\uff0810%\uff09\u548c\u8f03\u5c0f\u7684\u57fa\u790e\u6a21\u578b\u5927\u5c0f\u7684\u60c5\u6cc1\u4e0b\u6e1b\u8f15\u6a21\u578b\u5e7b\u89ba\u3002", "author": "Run Luo et.al.", "authors": "Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui", "id": "2405.15232v1", "paper_url": "http://arxiv.org/abs/2405.15232v1", "repo": "null"}}