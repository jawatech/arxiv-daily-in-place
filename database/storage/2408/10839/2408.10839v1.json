{"2408.10839": {"publish_time": "2024-08-20", "title": "Benchmarking Large Language Models for Math Reasoning Tasks", "paper_summary": "The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.", "paper_summary_zh": "\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u4e3a\u76f8\u5173\u7814\u7a76\u7684\u57fa\u77f3\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u667a\u80fd\uff0c\u5e76\u901a\u8fc7\u5176\u5148\u8fdb\u7684\u6027\u80fd\uff08\u4f8b\u5982\u5728\u6559\u80b2\u73af\u5883\u4e2d\uff09\u5b9e\u73b0\u4e86\u6f5c\u5728\u7684\u5b9e\u9645\u5e94\u7528\u3002\u5c3d\u7ba1\u6709\u5404\u79cd\u6570\u636e\u96c6\u548c\u65e8\u5728\u63d0\u9ad8 LLM \u81ea\u52a8\u5316\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u7684\u80fd\u529b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u8de8\u4e0d\u540c\u6570\u636e\u96c6\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4f7f\u5f97\u4e3a\u7279\u5b9a\u4efb\u52a1\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u53d8\u5f97\u590d\u6742\u3002\u5728\u8fd9\u4e2a\u9879\u76ee\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u516c\u5e73\u5730\u6bd4\u8f83\u4e86\u4e03\u79cd\u6700\u5148\u8fdb\u7684\u7528\u4e8e\u6570\u5b66\u95ee\u9898\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u95ee\u9898\u5206\u5e03\u5728\u56db\u4e2a\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u4e0a\u7684\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86 LLM \u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u50cf GPT-4o \u548c LLaMA 3-70B \u8fd9\u6837\u8f83\u5927\u7684\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u72ec\u7acb\u4e8e\u5177\u4f53\u7684\u63d0\u793a\u7b56\u7565\u89e3\u51b3\u6570\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u800c\u5bf9\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u4f1a\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6700\u4f73\u63d0\u793a\u53d6\u51b3\u4e8e\u6240\u9009\u7684\u57fa\u7840\u6a21\u578b\u3002\u6211\u4eec\u5f00\u6e90\u6211\u4eec\u7684\u57fa\u51c6\u4ee3\u7801\u4ee5\u652f\u6301\u5728\u672a\u6765\u7684\u7814\u7a76\u4e2d\u96c6\u6210\u5176\u4ed6\u6a21\u578b\u3002", "author": "Kathrin Se\u00dfler et.al.", "authors": "Kathrin Se\u00dfler, Yao Rong, Emek G\u00f6zl\u00fckl\u00fc, Enkelejda Kasneci", "id": "2408.10839v1", "paper_url": "http://arxiv.org/abs/2408.10839v1", "repo": "null"}}