{"2410.23933": {"publish_time": "2024-10-31", "title": "Language Models can Self-Lengthen to Generate Long Texts", "paper_summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to process long contexts, yet a notable gap remains in\ngenerating long, aligned outputs. This limitation stems from a training gap\nwhere pre-training lacks effective instructions for long-text generation, and\npost-training data primarily consists of short query-response pairs. Current\napproaches, such as instruction backtranslation and behavior imitation, face\nchallenges including data quality, copyright issues, and constraints on\nproprietary model usage. In this paper, we introduce an innovative iterative\ntraining framework called Self-Lengthen that leverages only the intrinsic\nknowledge and skills of LLMs without the need for auxiliary data or proprietary\nmodels. The framework consists of two roles: the Generator and the Extender.\nThe Generator produces the initial response, which is then split and expanded\nby the Extender. This process results in a new, longer response, which is used\nto train both the Generator and the Extender iteratively. Through this process,\nthe models are progressively trained to handle increasingly longer responses.\nExperiments on benchmarks and human evaluations show that Self-Lengthen\noutperforms existing methods in long-text generation, when applied to top\nopen-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at\nhttps://github.com/QwenLM/Self-Lengthen.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u986f\u8457\u589e\u5f37\u4e86\u5b83\u5011\u8655\u7406\u9577\u8a9e\u5883\u7684\u80fd \u529b\uff0c\u7136\u800c\u5728\u7522\u751f\u9577\u800c\u5c0d\u9f4a\u7684\u8f38\u51fa\u65b9\u9762\u4ecd\u5b58\u5728\u986f\u8457\u7684\u5dee\u8ddd\u3002\u9019\u7a2e\u9650\u5236 \u4f86\u81ea\u65bc\u8a13\u7df4\u5dee\u8ddd\uff0c\u5176\u4e2d\u9810\u8a13\u7df4\u7f3a\u4e4f\u9577\u6587\u672c\u751f\u6210\u7684\u6709\u6548\u6307\u4ee4\uff0c\u4e14\u8a13\u7df4 \u5f8c\u7684\u8cc7\u6599\u4e3b\u8981\u5305\u542b\u7c21\u77ed\u7684\u67e5\u8a62\u56de\u61c9\u914d\u5c0d\u3002\u76ee\u524d\u7684\u505a\u6cd5\uff0c\u4f8b\u5982\u6307\u4ee4\u56de\u8b6f \u548c\u884c\u70ba\u6a21\u4eff\uff0c\u9762\u81e8\u8457\u8cc7\u6599\u54c1\u8cea\u3001\u7248\u6b0a\u554f\u984c\uff0c\u4ee5\u53ca\u5c0d\u5c08\u6709\u6a21\u578b\u4f7f\u7528\u7684 \u9650\u5236\u7b49\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u5275\u65b0\u7684\u53cd\u8986\u8a13\u7df4\u67b6\u69cb\uff0c\u7a31 \u70ba Self-Lengthen\uff0c\u5b83\u50c5\u5229\u7528 LLM \u7684\u5167\u5728\u77e5\u8b58\u548c\u6280\u80fd\uff0c\u800c\u4e0d\u9700\u8981\u8f14\u52a9 \u8cc7\u6599\u6216\u5c08\u6709\u6a21\u578b\u3002\u8a72\u67b6\u69cb\u5305\u542b\u5169\u500b\u89d2\u8272\uff1a\u7522\u751f\u5668\u548c\u5ef6\u4f38\u5668\u3002\u7522\u751f\u5668 \u7522\u751f\u521d\u59cb\u56de\u61c9\uff0c\u7136\u5f8c\u7531\u5ef6\u4f38\u5668\u5206\u5272\u548c\u64f4\u5145\u3002\u9019\u500b\u904e\u7a0b\u7522\u751f\u4e00\u500b\u65b0\u7684\u3001 \u66f4\u9577\u7684\u56de\u61c9\uff0c\u7528\u65bc\u53cd\u8986\u8a13\u7df4\u7522\u751f\u5668\u548c\u5ef6\u4f38\u5668\u3002\u900f\u904e\u9019\u500b\u904e\u7a0b\uff0c\u9019\u4e9b \u6a21\u578b\u9010\u6f38\u63a5\u53d7\u8a13\u7df4\u4ee5\u8655\u7406\u8d8a\u4f86\u8d8a\u9577\u7684\u56de\u61c9\u3002\u5728\u57fa\u6e96\u548c\u4eba\u985e\u8a55\u4f30\u4e0a\u7684 \u5be6\u9a57\u8868\u660e\uff0c\u7576\u61c9\u7528\u65bc\u9802\u5c16\u7684\u958b\u6e90 LLM\uff0c\u4f8b\u5982 Qwen2 \u548c LLaMA3 \u6642\uff0c Self-Lengthen \u5728\u9577\u6587\u672c\u751f\u6210\u65b9\u9762\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u516c\u958b\u65bc https://github.com/QwenLM/Self-Lengthen\u3002", "author": "Shanghaoran Quan et.al.", "authors": "Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, Junyang Lin", "id": "2410.23933v1", "paper_url": "http://arxiv.org/abs/2410.23933v1", "repo": "null"}}