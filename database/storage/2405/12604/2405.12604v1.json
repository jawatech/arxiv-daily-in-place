{"2405.12604": {"publish_time": "2024-05-21", "title": "Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming", "paper_summary": "With the proliferation of red-teaming strategies for Large Language Models\n(LLMs), the deficiency in the literature about improving the safety and\nrobustness of LLM defense strategies is becoming increasingly pronounced. This\npaper introduces the LLM-based \\textbf{sentinel} model as a plug-and-play\nprefix module designed to reconstruct the input prompt with just a few ($<30$)\nadditional tokens, effectively reducing toxicity in responses from target LLMs.\nThe sentinel model naturally overcomes the \\textit{parameter inefficiency} and\n\\textit{limited model accessibility} for fine-tuning large target models. We\nemploy an interleaved training regimen using Proximal Policy Optimization (PPO)\nto optimize both red team and sentinel models dynamically, incorporating a\nvalue head-sharing mechanism inspired by the multi-agent centralized critic to\nmanage the complex interplay between agents. Our extensive experiments across\ntext-to-text and text-to-image demonstrate the effectiveness of our approach in\nmitigating toxic outputs, even when dealing with larger models like\n\\texttt{Llama-2}, \\texttt{GPT-3.5} and \\texttt{Stable-Diffusion}, highlighting\nthe potential of our framework in enhancing safety and robustness in various\napplications.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7d05\u968a\u7b56\u7565\u64f4\u6563\uff0c\u95dc\u65bc\u6539\u5584 LLM \u9632\u79a6\u7b56\u7565\u5b89\u5168\u6027\u8207\u7a69\u5065\u6027\u7684\u6587\u737b\u4e0d\u8db3\u4e4b\u8655\u65e5\u76ca\u660e\u986f\u3002\u672c\u6587\u4ecb\u7d39\u57fa\u65bc LLM \u7684**\u54e8\u5175**\u6a21\u578b\uff0c\u4f5c\u70ba\u4e00\u500b\u5373\u63d2\u5373\u7528\u7684\u524d\u7db4\u6a21\u7d44\uff0c\u65e8\u5728\u50c5\u4f7f\u7528\u5c11\u6578\uff08$<30$\uff09\u984d\u5916\u4ee3\u5e63\u91cd\u5efa\u8f38\u5165\u63d0\u793a\uff0c\u6709\u6548\u964d\u4f4e\u76ee\u6a19 LLM \u56de\u61c9\u4e2d\u7684\u6bd2\u6027\u3002\u54e8\u5175\u6a21\u578b\u81ea\u7136\u514b\u670d\u4e86\u5fae\u8abf\u5927\u578b\u76ee\u6a19\u6a21\u578b\u7684**\u53c3\u6578\u6548\u7387\u4f4e\u4e0b**\u548c**\u6a21\u578b\u53ef\u5b58\u53d6\u6027\u6709\u9650**\u3002\u6211\u5011\u63a1\u7528\u7a7f\u63d2\u8a13\u7df4\u65b9\u5f0f\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u6700\u4f73\u5316\uff08PPO\uff09\u52d5\u614b\u6700\u4f73\u5316\u7d05\u968a\u548c\u54e8\u5175\u6a21\u578b\uff0c\u4e26\u7d50\u5408\u53d7\u591a\u91cd\u4ee3\u7406\u96c6\u4e2d\u5f0f\u6279\u8a55\u8005\u555f\u767c\u7684\u50f9\u503c\u982d\u90e8\u5171\u4eab\u6a5f\u5236\uff0c\u4f86\u7ba1\u7406\u4ee3\u7406\u4e4b\u9593\u7684\u8907\u96dc\u4ea4\u4e92\u3002\u6211\u5011\u5728\u6587\u5b57\u8f49\u6587\u5b57\u548c\u6587\u5b57\u8f49\u5f71\u50cf\u7684\u5ee3\u6cdb\u5be6\u9a57\u4e2d\uff0c\u5c55\u793a\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u6e1b\u8f15\u6709\u6bd2\u8f38\u51fa\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u8655\u7406\u50cf \\texttt{Llama-2}\u3001\\texttt{GPT-3.5} \u548c \\texttt{Stable-Diffusion} \u7b49\u8f03\u5927\u578b\u6a21\u578b\u6642\u4e5f\u662f\u5982\u6b64\uff0c\u7a81\u986f\u4e86\u6211\u5011\u7684\u67b6\u69cb\u5728\u589e\u5f37\u5404\u7a2e\u61c9\u7528\u4e2d\u5b89\u5168\u6027\u8207\u7a69\u5065\u6027\u7684\u6f5b\u529b\u3002", "author": "Jiaxu Liu et.al.", "authors": "Jiaxu Liu, Xiangyu Yin, Sihao Wu, Jianhong Wang, Meng Fang, Xinping Yi, Xiaowei Huang", "id": "2405.12604v1", "paper_url": "http://arxiv.org/abs/2405.12604v1", "repo": "null"}}