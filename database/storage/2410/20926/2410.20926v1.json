{"2410.20926": {"publish_time": "2024-10-28", "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning", "paper_summary": "As the demand for processing extended textual data grows, the ability to\nhandle long-range dependencies and maintain computational efficiency is more\ncritical than ever. One of the key issues for long-sequence modeling using\nattention-based model is the mismatch between the limited-range modeling power\nof full attention and the long-range token dependency in the input sequence. In\nthis work, we propose to scale up the attention receptive field by tensorizing\nlong input sequences into compact tensor representations followed by attention\non each transformed dimension. The resulting Tensorized Attention can be\nadopted as efficient transformer backbones to extend input context length with\nimproved memory and time efficiency. We show that the proposed attention\ntensorization encodes token dependencies as a multi-hop attention process, and\nis equivalent to Kronecker decomposition of full attention. Extensive\nexperiments show that tensorized attention can be used to adapt pretrained LLMs\nwith improved efficiency. Notably, Llama-8B with tensorization is trained under\n32,768 context length and can steadily extrapolate to 128k length during\ninference with $11\\times$ speedup, compared to full attention with\nFlashAttention-2.", "paper_summary_zh": "\u96a8\u8457\u8655\u7406\u5ef6\u4f38\u6587\u672c\u8cc7\u6599\u7684\u9700\u6c42\u589e\u52a0\uff0c\u8655\u7406\u9577\u8ddd\u96e2\u4f9d\u8cf4\u95dc\u4fc2\u4e26\u7dad\u6301\u904b\u7b97\u6548\u7387\u7684\u80fd\u529b\u6bd4\u4ee5\u5f80\u4efb\u4f55\u6642\u5019\u90fd\u66f4\u70ba\u95dc\u9375\u3002\u4f7f\u7528\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u6a21\u578b\u9032\u884c\u9577\u5e8f\u5217\u5efa\u6a21\u7684\u4e3b\u8981\u554f\u984c\u4e4b\u4e00\uff0c\u5728\u65bc\u5168\u6ce8\u610f\u529b\u7684\u6709\u9650\u7bc4\u570d\u5efa\u6a21\u80fd\u529b\u8207\u8f38\u5165\u5e8f\u5217\u4e2d\u7684\u9577\u8ddd\u96e2 token \u4f9d\u8cf4\u95dc\u4fc2\u4e4b\u9593\u7684\u4e0d\u5339\u914d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5efa\u8b70\u900f\u904e\u5c07\u9577\u8f38\u5165\u5e8f\u5217\u5f35\u91cf\u5316\u70ba\u7dca\u6e4a\u7684\u5f35\u91cf\u8868\u793a\uff0c\u7136\u5f8c\u5c0d\u6bcf\u500b\u8f49\u63db\u7dad\u5ea6\u9032\u884c\u6ce8\u610f\u529b\uff0c\u4f86\u64f4\u5145\u6ce8\u610f\u529b\u611f\u53d7\u91ce\u3002\u7531\u6b64\u7522\u751f\u7684\u5f35\u91cf\u5316\u6ce8\u610f\u529b\u53ef\u4ee5\u63a1\u7528\u70ba\u9ad8\u6548\u7684 Transformer \u4e3b\u5e79\uff0c\u4ee5\u64f4\u5145\u8f38\u5165\u4e0a\u4e0b\u6587\u9577\u5ea6\uff0c\u4e26\u63d0\u9ad8\u8a18\u61b6\u9ad4\u548c\u6642\u9593\u6548\u7387\u3002\u6211\u5011\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u5f35\u91cf\u5316\u5c07 token \u4f9d\u8cf4\u95dc\u4fc2\u7de8\u78bc\u70ba\u591a\u8df3\u6ce8\u610f\u529b\u8655\u7406\uff0c\u4e26\u4e14\u7b49\u65bc\u5168\u6ce8\u610f\u529b\u7684\u514b\u7f85\u5167\u514b\u5206\u89e3\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0c\u5f35\u91cf\u5316\u6ce8\u610f\u529b\u53ef\u7528\u65bc\u8abf\u6574\u9810\u8a13\u7df4\u7684 LLM\uff0c\u4e26\u63d0\u9ad8\u6548\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u63a1\u7528\u5f35\u91cf\u5316\u7684 Llama-8B \u5728 32,768 \u7684\u4e0a\u4e0b\u6587\u9577\u5ea6\u4e0b\u8a13\u7df4\uff0c\u4e26\u4e14\u5728\u63a8\u8ad6\u671f\u9593\u53ef\u4ee5\u7a69\u5b9a\u5730\u5916\u63a8\u5230 128k \u7684\u9577\u5ea6\uff0c\u8207\u63a1\u7528 FlashAttention-2 \u7684\u5168\u6ce8\u610f\u529b\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u5347\u4e86 $11\\times$\u3002", "author": "Aosong Feng et.al.", "authors": "Aosong Feng, Rex Ying, Leandros Tassiulas", "id": "2410.20926v1", "paper_url": "http://arxiv.org/abs/2410.20926v1", "repo": "null"}}