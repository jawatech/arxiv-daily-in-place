{"2410.23956": {"publish_time": "2024-10-31", "title": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a Single Source Language", "paper_summary": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM.", "paper_summary_zh": "<paragraph>\u82f1\u8a9e\u4f5c\u70ba\u4e00\u7a2e\u8cc7\u6e90\u975e\u5e38\u8c50\u5bcc\u7684\u8a9e\u8a00\uff0c\u80fd\u5920\u9810\u8a13\u7df4\u51fa\u9ad8\u54c1\u8cea\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u5c0d\u65bc\u5927\u591a\u6578\u5176\u4ed6\u8a9e\u8a00\u800c\u8a00\uff0c\u60c5\u6cc1\u4e26\u975e\u5982\u6b64\uff0c\u56e0\u70ba\u9818\u5148\u7684 LLM \u5c0d\u65bc\u975e\u82f1\u8a9e\u8a9e\u8a00\u7684\u8868\u73fe\u4ecd\u7136\u4e0d\u4f73\uff0c\u9019\u53ef\u80fd\u662f\u7531\u65bc\u53ef\u7528\u591a\u8a9e\u8a00\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u7684\u54c1\u8cea\u548c\u591a\u6a23\u6027\u5b58\u5728\u5dee\u8ddd\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u4f86\u81ea\u55ae\u4e00\u9ad8\u54c1\u8cea\u539f\u59cb\u8a9e\u8a00\u7684\u6a5f\u5668\u7ffb\u8b6f\u6587\u672c\uff0c\u53ef\u4ee5\u70ba\u591a\u8a9e\u8a00 LLM \u7684\u9810\u8a13\u7df4\u505a\u51fa\u91cd\u5927\u8ca2\u737b\u3002\u6211\u5011\u5c07\u9ad8\u54c1\u8cea\u82f1\u8a9e\u7db2\u8def\u8cc7\u6599\u96c6 FineWeb-Edu \u7ffb\u8b6f\u6210\u6cd5\u8a9e\u3001\u5fb7\u8a9e\u548c\u897f\u73ed\u7259\u8a9e\uff0c\u6700\u7d42\u7522\u751f\u4e00\u500b 300B-token \u7684\u8cc7\u6599\u96c6\uff0c\u6211\u5011\u7a31\u4e4b\u70ba TransWeb-Edu\uff0c\u4e26\u5f9e\u982d\u958b\u59cb\u5728\u9019\u500b\u8cc7\u6599\u96c6\u4e0a\u9810\u8a13\u7df4\u4e00\u500b 1.3B-\u53c3\u6578\u6a21\u578b CuatroLLM\u3002\u5728\u4e94\u9805\u975e\u82f1\u8a9e\u63a8\u7406\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 CuatroLLM \u5373\u4f7f\u4f7f\u7528\u6bd4 Llama3.2 \u5c11\u4e00\u500b\u6578\u91cf\u7d1a\u7684\u8cc7\u6599\uff08\u4f8b\u5982 Llama3.2 \u8a13\u7df4\u4e2d\u4f7f\u7528\u7684 token \u6578\u91cf\u7684\u7d04 6%\uff09\uff0c\u4e5f\u80fd\u8207\u4f7f\u7528\u5c01\u9589\u8cc7\u6599\u8a13\u7df4\u7684\u6700\u65b0\u591a\u8a9e\u8a00\u6a21\u578b\uff08\u4f8b\u5982 Llama3.2 \u548c Gemma2\uff09\u76f8\u5339\u914d\u6216\u8868\u73fe\u5f97\u66f4\u597d\u3002\u6211\u5011\u9032\u4e00\u6b65\u8b49\u660e\uff0c\u900f\u904e\u984d\u5916\u7684\u7279\u5b9a\u9818\u57df\u9810\u8a13\u7df4\uff08\u5c11\u65bc TransWeb-Edu \u7684 1%\uff09\uff0cCuatroLLM \u8d85\u8d8a\u4e86\u591a\u8a9e\u8a00\u63a8\u7406\u7684\u6700\u65b0\u6280\u8853\u3002\u70ba\u4e86\u4fc3\u9032\u53ef\u8907\u88fd\u6027\uff0c\u6211\u5011\u5728 hf.co/britllm/CuatroLLM \u4e0b\u958b\u653e\u6388\u6b0a\u91cb\u51fa\u6211\u5011\u7684\u8a9e\u6599\u5eab\u3001\u6a21\u578b\u548c\u8a13\u7df4\u7ba1\u9053\u3002</paragraph>", "author": "Jiayi Wang et.al.", "authors": "Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, Yihong Chen, Raphael Tang, Pontus Stenetorp", "id": "2410.23956v1", "paper_url": "http://arxiv.org/abs/2410.23956v1", "repo": "null"}}