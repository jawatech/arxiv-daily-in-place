{"2408.17431": {"publish_time": "2024-08-30", "title": "Advancing Multi-talker ASR Performance with Large Language Models", "paper_summary": "Recognizing overlapping speech from multiple speakers in conversational\nscenarios is one of the most challenging problem for automatic speech\nrecognition (ASR). Serialized output training (SOT) is a classic method to\naddress multi-talker ASR, with the idea of concatenating transcriptions from\nmultiple speakers according to the emission times of their speech for training.\nHowever, SOT-style transcriptions, derived from concatenating multiple related\nutterances in a conversation, depend significantly on modeling long contexts.\nTherefore, compared to traditional methods that primarily emphasize encoder\nperformance in attention-based encoder-decoder (AED) architectures, a novel\napproach utilizing large language models (LLMs) that leverages the capabilities\nof pre-trained decoders may be better suited for such complex and challenging\nscenarios. In this paper, we propose an LLM-based SOT approach for multi-talker\nASR, leveraging pre-trained speech encoder and LLM, fine-tuning them on\nmulti-talker dataset using appropriate strategies. Experimental results\ndemonstrate that our approach surpasses traditional AED-based methods on the\nsimulated dataset LibriMix and achieves state-of-the-art performance on the\nevaluation set of the real-world dataset AMI, outperforming the AED model\ntrained with 1000 times more supervised data in previous works.", "paper_summary_zh": "\u8b58\u5225\u5c0d\u8a71\u5834\u666f\u4e2d\u591a\u4f4d\u8aaa\u8a71\u8005\u7684\u91cd\u758a\u8a9e\u97f3\u662f\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u6700\u5177\u6311\u6230\u6027\u7684\u554f\u984c\u4e4b\u4e00\u3002\u5e8f\u5217\u5316\u8f38\u51fa\u8a13\u7df4 (SOT) \u662f\u4e00\u7a2e\u7528\u65bc\u8655\u7406\u591a\u8aaa\u8a71\u8005 ASR \u7684\u7d93\u5178\u65b9\u6cd5\uff0c\u5176\u6982\u5ff5\u662f\u6839\u64da\u8aaa\u8a71\u8005\u7684\u767c\u8a71\u6642\u9593\uff0c\u5c07\u591a\u4f4d\u8aaa\u8a71\u8005\u7684\u8f49\u9304\u5167\u5bb9\u4e32\u63a5\u8d77\u4f86\u9032\u884c\u8a13\u7df4\u3002\u7136\u800c\uff0c\u5f9e\u5c0d\u8a71\u4e2d\u4e32\u63a5\u591a\u500b\u76f8\u95dc\u8a9e\u53e5\u800c\u884d\u751f\u7684 SOT \u98a8\u683c\u8f49\u9304\u5167\u5bb9\uff0c\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u5c0d\u9577\u8a9e\u5883\u7684\u5efa\u6a21\u3002\u56e0\u6b64\uff0c\u8207\u50b3\u7d71\u65b9\u6cd5\uff08\u4e3b\u8981\u5f37\u8abf\u6ce8\u610f\u529b\u5f0f\u7de8\u78bc\u5668-\u89e3\u78bc\u5668 (AED) \u67b6\u69cb\u4e2d\u7de8\u78bc\u5668\u7684\u6548\u80fd\uff09\u76f8\u6bd4\uff0c\u4e00\u7a2e\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u65b0\u7a4e\u65b9\u6cd5\uff08\u8a72\u65b9\u6cd5\u904b\u7528\u9810\u5148\u8a13\u7df4\u89e3\u78bc\u5668\u7684\u529f\u80fd\uff09\uff0c\u53ef\u80fd\u66f4\u9069\u5408\u9019\u7a2e\u8907\u96dc\u4e14\u5177\u6311\u6230\u6027\u7684\u5834\u666f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc LLM \u7684 SOT \u65b9\u6cd5\uff0c\u7528\u65bc\u591a\u8aaa\u8a71\u8005 ASR\uff0c\u8a72\u65b9\u6cd5\u904b\u7528\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u97f3\u7de8\u78bc\u5668\u548c LLM\uff0c\u4e26\u4f7f\u7528\u9069\u7576\u7684\u7b56\u7565\u5c0d\u591a\u8aaa\u8a71\u8005\u8cc7\u6599\u96c6\u9032\u884c\u5fae\u8abf\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728\u6a21\u64ec\u8cc7\u6599\u96c6 LibriMix \u4e0a\u8d85\u8d8a\u4e86\u50b3\u7d71\u7684\u57fa\u65bc AED \u7684\u65b9\u6cd5\uff0c\u4e26\u5728\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6 AMI \u7684\u8a55\u4f30\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u512a\u65bc\u5148\u524d\u5de5\u4f5c\u4e2d\u4f7f\u7528\u591a 1000 \u500d\u76e3\u7763\u8cc7\u6599\u8a13\u7df4\u7684 AED \u6a21\u578b\u3002", "author": "Mohan Shi et.al.", "authors": "Mohan Shi, Zengrui Jin, Yaoxun Xu, Yong Xu, Shi-Xiong Zhang, Kun Wei, Yiwen Shao, Chunlei Zhang, Dong Yu", "id": "2408.17431v1", "paper_url": "http://arxiv.org/abs/2408.17431v1", "repo": "null"}}