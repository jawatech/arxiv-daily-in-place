{"2412.04110": {"publish_time": "2024-12-05", "title": "Enhancing Mathematical Reasoning in LLMs with Background Operators", "paper_summary": "We propose utilizing background operators for mathematical reasoning in large\nlanguage models (LLMs). To achieve this, we define a set of fundamental\nmathematical predicates as the basic building blocks. For each mathematical\nproblem, we develop a Prolog solution that includes problem-specific predicates\nand intermediate predicates derived from these background operators, ensuring\nthat each solution adheres to the defined operator set. We introduce the\nMATH-Prolog corpus, which is derived from the counting and probability\ncategories of the MATH corpus. For efficient data augmentation, we apply K-fold\ncross-validated self-training. This method incrementally generates new Prolog\nsolutions for each fold, incorporating those verified as correct into the\ntraining set throughout the model training process. Our experimental results\ndemonstrate that 5-fold crossvalidated self-training effectively identifies\nnew, accurate Prolog solutions, achieving an accuracy of 84.6% on the\ncross-validated set, and 84.8% on the test set during fine-tuning the\nMeta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new\nsolutions with fully computable inference steps for previously unseen problems.\nAdditionally, incorporating the background mathematical predicates into the\nprompt enhances solution coverage.", "paper_summary_zh": "\u6211\u5011\u5efa\u8b70\u5229\u7528\u80cc\u666f\u904b\u7b97\u5b50\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u9032\u884c\u6578\u5b78\u63a8\u7406\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5c07\u4e00\u7d44\u57fa\u672c\u6578\u5b78\u8b02\u8a5e\u5b9a\u7fa9\u70ba\u57fa\u672c\u69cb\u5efa\u5340\u584a\u3002\u5c0d\u65bc\u6bcf\u500b\u6578\u5b78\u554f\u984c\uff0c\u6211\u5011\u958b\u767c\u4e00\u500b Prolog \u89e3\u6c7a\u65b9\u6848\uff0c\u5176\u4e2d\u5305\u542b\u7279\u5b9a\u65bc\u554f\u984c\u7684\u8b02\u8a5e\u548c\u5f9e\u9019\u4e9b\u80cc\u666f\u904b\u7b97\u5b50\u884d\u751f\u7684\u4e2d\u9593\u8b02\u8a5e\uff0c\u78ba\u4fdd\u6bcf\u500b\u89e3\u6c7a\u65b9\u6848\u90fd\u9075\u5b88\u5b9a\u7fa9\u7684\u904b\u7b97\u5b50\u96c6\u3002\u6211\u5011\u5f15\u5165\u4e86 MATH-Prolog \u8a9e\u6599\u5eab\uff0c\u5b83\u4f86\u81ea MATH \u8a9e\u6599\u5eab\u7684\u8a08\u6578\u548c\u6a5f\u7387\u985e\u5225\u3002\u5c0d\u65bc\u6709\u6548\u8cc7\u6599\u64f4\u5145\uff0c\u6211\u5011\u61c9\u7528 K \u6298\u4ea4\u53c9\u9a57\u8b49\u81ea\u6211\u8a13\u7df4\u3002\u6b64\u65b9\u6cd5\u70ba\u6bcf\u500b\u6298\u758a\u905e\u589e\u7522\u751f\u65b0\u7684 Prolog \u89e3\u6c7a\u65b9\u6848\uff0c\u5c07\u9a57\u8b49\u70ba\u6b63\u78ba\u7684\u89e3\u6c7a\u65b9\u6848\u7d0d\u5165\u6574\u500b\u6a21\u578b\u8a13\u7df4\u904e\u7a0b\u4e2d\u7684\u8a13\u7df4\u96c6\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c5 \u6298\u4ea4\u53c9\u9a57\u8b49\u81ea\u6211\u8a13\u7df4\u6709\u6548\u5730\u8b58\u5225\u51fa\u65b0\u7684\u3001\u6e96\u78ba\u7684 Prolog \u89e3\u6c7a\u65b9\u6848\uff0c\u5728\u5fae\u8abf Meta-Llama-3.1-8B-Instruct \u6a21\u578b\u6642\uff0c\u5728\u4ea4\u53c9\u9a57\u8b49\u96c6\u4e0a\u9054\u5230 84.6% \u7684\u6e96\u78ba\u7387\uff0c\u5728\u6e2c\u8a66\u96c6\u4e0a\u9054\u5230 84.8%\u3002\u9019\u7a2e\u65b9\u6cd5\u6210\u529f\u5730\u70ba\u4ee5\u524d\u672a\u898b\u7684\u554f\u984c\u63ed\u793a\u4e86\u5177\u6709\u5b8c\u5168\u53ef\u8a08\u7b97\u63a8\u7406\u6b65\u9a5f\u7684\u65b0\u89e3\u6c7a\u65b9\u6848\u3002\u6b64\u5916\uff0c\u5c07\u80cc\u666f\u6578\u5b78\u8b02\u8a5e\u7d0d\u5165\u63d0\u793a\u4e2d\u6703\u589e\u5f37\u89e3\u6c7a\u65b9\u6848\u6db5\u84cb\u7bc4\u570d\u3002", "author": "Jiajun Chen et.al.", "authors": "Jiajun Chen, Yik-Cheung Tam", "id": "2412.04110v1", "paper_url": "http://arxiv.org/abs/2412.04110v1", "repo": "null"}}