{"2409.11143": {"publish_time": "2024-09-17", "title": "Semformer: Transformer Language Models with Semantic Planning", "paper_summary": "Next-token prediction serves as the dominant component in current neural\nlanguage models. During the training phase, the model employs teacher forcing,\nwhich predicts tokens based on all preceding ground truth tokens. However, this\napproach has been found to create shortcuts, utilizing the revealed prefix to\nspuriously fit future tokens, potentially compromising the accuracy of the\nnext-token predictor. In this paper, we introduce Semformer, a novel method of\ntraining a Transformer language model that explicitly models the semantic\nplanning of response. Specifically, we incorporate a sequence of planning\ntokens into the prefix, guiding the planning token representations to predict\nthe latent semantic representations of the response, which are induced by an\nautoencoder. In a minimal planning task (i.e., graph path-finding), our model\nexhibits near-perfect performance and effectively mitigates shortcut learning,\na feat that standard training methods and baseline models have been unable to\naccomplish. Furthermore, we pretrain Semformer from scratch with 125M\nparameters, demonstrating its efficacy through measures of perplexity,\nin-context learning, and fine-tuning on summarization tasks.", "paper_summary_zh": "\u5728\u7576\u524d\u7684\u8a9e\u8a00\u6a21\u578b\u4e2d\uff0c\u4e0b\u4e00\u500b\u8a5e\u5f59\u9810\u6e2c\u662f\u4e3b\u5c0e\u7d44\u6210\u90e8\u5206\u3002\u5728\u8a13\u7df4\u968e\u6bb5\uff0c\u6a21\u578b\u63a1\u7528\u6559\u5e2b\u5f37\u5236\u6cd5\uff0c\u6839\u64da\u6240\u6709\u524d\u4e00\u500b\u7684\u771f\u5be6\u8a5e\u5f59\u4f86\u9810\u6e2c\u8a5e\u5f59\u3002\u7136\u800c\uff0c\u767c\u73fe\u9019\u7a2e\u65b9\u6cd5\u6703\u7522\u751f\u6377\u5f91\uff0c\u5229\u7528\u5df2\u63ed\u9732\u7684\u524d\u7db4\u4f86\u865b\u5047\u5730\u7b26\u5408\u5f8c\u7e8c\u7684\u8a5e\u5f59\uff0c\u6f5b\u5728\u6703\u5371\u5bb3\u4e0b\u4e00\u500b\u8a5e\u5f59\u9810\u6e2c\u5668\u7684\u6e96\u78ba\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 Semformer\uff0c\u4e00\u7a2e\u8a13\u7df4 Transformer \u8a9e\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u660e\u78ba\u5730\u5efa\u69cb\u56de\u61c9\u7684\u8a9e\u610f\u898f\u5283\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u4e00\u7cfb\u5217\u898f\u5283\u8a5e\u5f59\u7d0d\u5165\u524d\u7db4\uff0c\u5f15\u5c0e\u898f\u5283\u8a5e\u5f59\u7684\u8868\u5fb5\u53bb\u9810\u6e2c\u56de\u61c9\u7684\u6f5b\u5728\u8a9e\u610f\u8868\u5fb5\uff0c\u9019\u4e9b\u8868\u5fb5\u662f\u7531\u81ea\u52d5\u7de8\u78bc\u5668\u8a98\u5c0e\u7684\u3002\u5728\u4e00\u500b\u6700\u5c0f\u7684\u898f\u5283\u4efb\u52d9\uff08\u5373\u5716\u5f62\u8def\u5f91\u5c0b\u627e\uff09\u4e2d\uff0c\u6211\u5011\u7684\u6a21\u578b\u8868\u73fe\u51fa\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6548\u80fd\uff0c\u4e26\u6709\u6548\u5730\u6e1b\u8f15\u6377\u5f91\u5b78\u7fd2\uff0c\u9019\u662f\u6a19\u6e96\u8a13\u7df4\u65b9\u6cd5\u548c\u57fa\u7dda\u6a21\u578b\u7121\u6cd5\u9054\u6210\u7684\u58ef\u8209\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f9e\u982d\u958b\u59cb\u4f7f\u7528 1.25 \u5104\u500b\u53c3\u6578\u9810\u8a13\u7df4 Semformer\uff0c\u900f\u904e\u56f0\u60d1\u5ea6\u3001\u8a9e\u5883\u5b78\u7fd2\u548c\u5728\u6458\u8981\u4efb\u52d9\u4e0a\u7684\u5fae\u8abf\u4f86\u8b49\u660e\u5176\u529f\u6548\u3002", "author": "Yongjing Yin et.al.", "authors": "Yongjing Yin, Junran Ding, Kai Song, Yue Zhang", "id": "2409.11143v1", "paper_url": "http://arxiv.org/abs/2409.11143v1", "repo": "null"}}