{"2405.09220": {"publish_time": "2024-05-15", "title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models", "paper_summary": "In this paper, we present the findings of our Project ALPINE which stands for\n``Autoregressive Learning for Planning In NEtworks.\" Project ALPINE initiates a\ntheoretical investigation into the development of planning capabilities in\nTransformer-based language models through their autoregressive learning\nmechanisms, aiming to identify any potential limitations in their planning\nabilities. We abstract planning as a network path-finding task where the\nobjective is to generate a valid path from a specified source node to a\ndesignated target node. In terms of expressiveness, we show that the\nTransformer is capable of executing path-finding by embedding the adjacency and\nreachability matrices within its weights. Our theoretical analysis of the\ngradient-based learning dynamic of the Transformer reveals that the Transformer\nis capable of learning both the adjacency matrix and a limited form of the\nreachability matrix. These theoretical insights are then validated through\nexperiments, which demonstrate that the Transformer indeed learns the adjacency\nmatrix and an incomplete reachability matrix, which aligns with the predictions\nmade in our theoretical analysis. Additionally, when applying our methodology\nto a real-world planning benchmark, called Blocksworld, our observations remain\nconsistent. Our theoretical and empirical analyses further unveil a potential\nlimitation of Transformer in path-finding: it cannot identify reachability\nrelationships through transitivity, and thus would fail when path concatenation\nis needed to generate a path. In summary, our findings shed new light on how\nthe internal mechanisms of autoregressive learning enable planning in networks.\nThis study may contribute to our understanding of the general planning\ncapabilities in other related domains.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07\u5c55\u793a\u6211\u5011\u7684 Project ALPINE \u7684\u7814\u7a76\u7d50\u679c\uff0c\u5176\u4e2d ALPINE \u4ee3\u8868\u300c\u795e\u7d93\u7db2\u8def\u898f\u5283\u7684\u81ea\u52d5\u56de\u6b78\u5b78\u7fd2\u300d\u3002Project ALPINE \u555f\u52d5\u4e86\u4e00\u9805\u7406\u8ad6\u7814\u7a76\uff0c\u63a2\u8a0e\u900f\u904e\u81ea\u52d5\u56de\u6b78\u5b78\u7fd2\u6a5f\u5236\uff0c\u5728\u57fa\u65bc Transformer \u7684\u8a9e\u8a00\u6a21\u578b\u4e2d\u958b\u767c\u898f\u5283\u80fd\u529b\uff0c\u76ee\u6a19\u662f\u627e\u51fa\u5176\u898f\u5283\u80fd\u529b\u7684\u4efb\u4f55\u6f5b\u5728\u9650\u5236\u3002\u6211\u5011\u5c07\u898f\u5283\u62bd\u8c61\u70ba\u7db2\u8def\u8def\u5f91\u5c0b\u627e\u4efb\u52d9\uff0c\u5176\u76ee\u6a19\u662f\u5f9e\u6307\u5b9a\u7684\u4f86\u6e90\u7bc0\u9ede\u7522\u751f\u4e00\u689d\u6709\u6548\u7684\u8def\u5f91\u5230\u6307\u5b9a\u7684\u76ee\u6a19\u7bc0\u9ede\u3002\u5728\u8868\u9054\u80fd\u529b\u65b9\u9762\uff0c\u6211\u5011\u5c55\u793a\u4e86 Transformer \u80fd\u5920\u900f\u904e\u5728\u5176\u6b0a\u91cd\u4e2d\u5d4c\u5165\u9130\u63a5\u77e9\u9663\u548c\u53ef\u53ca\u77e9\u9663\u4f86\u57f7\u884c\u8def\u5f91\u5c0b\u627e\u3002\u6211\u5011\u5c0d Transformer \u7684\u57fa\u65bc\u68af\u5ea6\u7684\u5b78\u7fd2\u52d5\u614b\u7684\u7406\u8ad6\u5206\u6790\u63ed\u793a\u4e86 Transformer \u80fd\u5920\u5b78\u7fd2\u9130\u63a5\u77e9\u9663\u548c\u53ef\u53ca\u77e9\u9663\u7684\u6709\u9650\u5f62\u5f0f\u3002\u9019\u4e9b\u7406\u8ad6\u898b\u89e3\u96a8\u5f8c\u900f\u904e\u5be6\u9a57\u5f97\u5230\u9a57\u8b49\uff0c\u5be6\u9a57\u8868\u660e Transformer \u78ba\u5be6\u5b78\u7fd2\u4e86\u9130\u63a5\u77e9\u9663\u548c\u4e0d\u5b8c\u6574\u7684\u53ef\u53ca\u77e9\u9663\uff0c\u9019\u8207\u6211\u5011\u5728\u7406\u8ad6\u5206\u6790\u4e2d\u505a\u51fa\u7684\u9810\u6e2c\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u7576\u5c07\u6211\u5011\u7684 methodology \u61c9\u7528\u65bc\u7a31\u70ba Blocksworld \u7684\u771f\u5be6\u898f\u5283\u57fa\u6e96\u6642\uff0c\u6211\u5011\u7684\u89c0\u5bdf\u7d50\u679c\u4ecd\u7136\u4e00\u81f4\u3002\u6211\u5011\u7684\u7406\u8ad6\u548c\u5be6\u8b49\u5206\u6790\u9032\u4e00\u6b65\u63ed\u793a\u4e86 Transformer \u5728\u8def\u5f91\u5c0b\u627e\u4e2d\u7684\u4e00\u500b\u6f5b\u5728\u9650\u5236\uff1a\u5b83\u7121\u6cd5\u900f\u904e\u905e\u79fb\u6027\u8b58\u5225\u53ef\u53ca\u6027\u95dc\u4fc2\uff0c\u56e0\u6b64\u5728\u9700\u8981\u8def\u5f91\u4e32\u63a5\u4f86\u7522\u751f\u8def\u5f91\u6642\u6703\u5931\u6557\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u70ba\u81ea\u52d5\u56de\u6b78\u5b78\u7fd2\u7684\u5167\u90e8\u6a5f\u5236\u5982\u4f55\u4f7f\u7db2\u8def\u898f\u5283\u6210\u70ba\u53ef\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\u3002\u9019\u9805\u7814\u7a76\u53ef\u80fd\u6709\u52a9\u65bc\u6211\u5011\u4e86\u89e3\u5176\u4ed6\u76f8\u95dc\u9818\u57df\u4e2d\u7684\u4e00\u822c\u898f\u5283\u80fd\u529b\u3002</paragraph>", "author": "Siwei Wang et.al.", "authors": "Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-Hua Teng, Wei Chen", "id": "2405.09220v1", "paper_url": "http://arxiv.org/abs/2405.09220v1", "repo": "null"}}