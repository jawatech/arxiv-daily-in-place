{"2409.17750": {"publish_time": "2024-09-26", "title": "Are Transformers in Pre-trained LM A Good ASR Encoder? An Empirical Study", "paper_summary": "In this study, we delve into the efficacy of transformers within pre-trained\nlanguage models (PLMs) when repurposed as encoders for Automatic Speech\nRecognition (ASR). Our underlying hypothesis posits that, despite being\ninitially trained on text-based corpora, these transformers possess a\nremarkable capacity to extract effective features from the input sequence. This\ninherent capability, we argue, is transferrable to speech data, thereby\naugmenting the acoustic modeling ability of ASR. Through rigorous empirical\nanalysis, our findings reveal a notable improvement in Character Error Rate\n(CER) and Word Error Rate (WER) across diverse ASR tasks when transformers from\npre-trained LMs are incorporated. Particularly, they serve as an advantageous\nstarting point for initializing ASR encoders. Furthermore, we uncover that\nthese transformers, when integrated into a well-established ASR encoder, can\nsignificantly boost performance, especially in scenarios where profound\nsemantic comprehension is pivotal. This underscores the potential of leveraging\nthe semantic prowess embedded within pre-trained transformers to advance ASR\nsystems' capabilities.", "paper_summary_zh": "\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u4e86\u5728\u5c07\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u4e2d\u7684Transformer\u91cd\u65b0\u7528\u4f5c\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u7de8\u78bc\u5668\u6642\uff0c\u9019\u4e9bTransformer\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u57fa\u672c\u5047\u8a2d\u5047\u8a2d\uff0c\u5118\u7ba1\u6700\u521d\u662f\u5728\u57fa\u65bc\u6587\u5b57\u7684\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\uff0c\u4f46\u9019\u4e9bTransformer\u5177\u5099\u5f9e\u8f38\u5165\u5e8f\u5217\u4e2d\u63d0\u53d6\u6709\u6548\u7279\u5fb5\u7684\u975e\u51e1\u80fd\u529b\u3002\u6211\u5011\u8a8d\u70ba\uff0c\u9019\u7a2e\u5167\u5728\u80fd\u529b\u53ef\u4ee5\u8f49\u79fb\u5230\u8a9e\u97f3\u8cc7\u6599\uff0c\u5f9e\u800c\u589e\u5f37 ASR \u7684\u8072\u5b78\u5efa\u6a21\u80fd\u529b\u3002\u900f\u904e\u56b4\u8b39\u7684\u5be6\u8b49\u5206\u6790\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u7576\u5c07\u4f86\u81ea\u9810\u8a13\u7df4 LM \u7684Transformer\u7d0d\u5165\u6642\uff0c\u5404\u7a2e ASR \u4efb\u52d9\u7684\u5b57\u5143\u932f\u8aa4\u7387 (CER) \u548c\u5b57\u8a5e\u932f\u8aa4\u7387 (WER) \u90fd\u986f\u8457\u6539\u5584\u3002\u7279\u5225\u662f\uff0c\u5b83\u5011\u4f5c\u70ba\u521d\u59cb\u5316 ASR \u7de8\u78bc\u5668\u7684\u6709\u5229\u8d77\u9ede\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\uff0c\u7576\u9019\u4e9bTransformer\u6574\u5408\u5230\u4e00\u500b\u5b8c\u5584\u7684 ASR \u7de8\u78bc\u5668\u4e2d\u6642\uff0c\u53ef\u4ee5\u986f\u8457\u63d0\u5347\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u6df1\u523b\u8a9e\u7fa9\u7406\u89e3\u81f3\u95dc\u91cd\u8981\u7684\u5834\u666f\u4e2d\u3002\u9019\u5f37\u8abf\u4e86\u5229\u7528\u9810\u8a13\u7df4Transformer\u4e2d\u5d4c\u5165\u7684\u8a9e\u7fa9\u80fd\u529b\u4f86\u63d0\u5347 ASR \u7cfb\u7d71\u529f\u80fd\u7684\u6f5b\u529b\u3002", "author": "Keyu An et.al.", "authors": "Keyu An, Shiliang Zhang, Zhijie Yan", "id": "2409.17750v1", "paper_url": "http://arxiv.org/abs/2409.17750v1", "repo": "null"}}