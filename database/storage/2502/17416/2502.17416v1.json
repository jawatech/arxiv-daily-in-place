{"2502.17416": {"publish_time": "2025-02-24", "title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers", "paper_summary": "Large language models have shown remarkable reasoning abilities and scaling\nlaws suggest that large parameter count, especially along the depth axis, is\nthe primary driver. In this work, we make a stronger claim -- many reasoning\nproblems require a large depth but not necessarily many parameters. This\nunlocks a novel application of looped models for reasoning. Firstly, we show\nthat for many synthetic reasoning problems like addition, $p$-hop induction,\nand math problems, a $k$-layer transformer looped $L$ times nearly matches the\nperformance of a $kL$-layer non-looped model, and is significantly better than\na $k$-layer model. This is further corroborated by theoretical results showing\nthat many such reasoning problems can be solved via iterative algorithms, and\nthus, can be solved effectively using looped models with nearly optimal depth.\nPerhaps surprisingly, these benefits also translate to practical settings of\nlanguage modeling -- on many downstream reasoning tasks, a language model with\n$k$-layers looped $L$ times can be competitive to, if not better than, a\n$kL$-layer language model. In fact, our empirical analysis reveals an\nintriguing phenomenon: looped and non-looped models exhibit scaling behavior\nthat depends on their effective depth, akin to the inference-time scaling of\nchain-of-thought (CoT) reasoning. We further elucidate the connection to CoT\nreasoning by proving that looped models implicitly generate latent thoughts and\ncan simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we\nalso present an interesting dichotomy between reasoning and memorization, and\ndesign a looping-based regularization that is effective on both fronts.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5c55\u73fe\u4e86\u975e\u51e1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u898f\u6a21\u5b9a\u5f8b\u8868\u660e\uff0c\u5927\u91cf\u7684\u53c3\u6578\u6578\u91cf\uff0c\u7279\u5225\u662f\u5728\u6df1\u5ea6\u8ef8\u4e0a\uff0c\u662f\u4e3b\u8981\u9a45\u52d5\u529b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u66f4\u5f37\u6709\u529b\u7684\u4e3b\u5f35\u2014\u2014\u8a31\u591a\u63a8\u7406\u554f\u984c\u9700\u8981\u5927\u91cf\u7684\u6df1\u5ea6\uff0c\u4f46\u4e26\u4e0d\u4e00\u5b9a\u9700\u8981\u5927\u91cf\u7684\u53c3\u6578\u3002\u9019\u70ba\u5faa\u74b0\u6a21\u578b\u5728\u63a8\u7406\u4e2d\u7684\u65b0\u61c9\u7528\u958b\u555f\u4e86\u5927\u9580\u3002\u9996\u5148\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5c0d\u65bc\u8a31\u591a\u5408\u6210\u63a8\u7406\u554f\u984c\uff0c\u4f8b\u5982\u52a0\u6cd5\u3001p \u8df3\u8e8d\u6b78\u7d0d\u548c\u6578\u5b78\u554f\u984c\uff0c\u4e00\u500b\u5faa\u74b0 L \u6b21\u7684 k \u5c64Transformer\u5e7e\u4e4e\u53ef\u4ee5\u5339\u914d kL \u5c64\u975e\u5faa\u74b0\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e26\u4e14\u986f\u8457\u512a\u65bc k \u5c64\u6a21\u578b\u3002\u9019\u9032\u4e00\u6b65\u5f97\u5230\u4e86\u7406\u8ad6\u7d50\u679c\u7684\u8b49\u5be6\uff0c\u8a72\u7d50\u679c\u8868\u660e\u8a31\u591a\u9019\u6a23\u7684\u63a8\u7406\u554f\u984c\u90fd\u53ef\u4ee5\u901a\u904e\u8fed\u4ee3\u7b97\u6cd5\u4f86\u89e3\u6c7a\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f7f\u7528\u6df1\u5ea6\u63a5\u8fd1\u6700\u4f73\u7684\u5faa\u74b0\u6a21\u578b\u6709\u6548\u5730\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u9019\u4e9b\u597d\u8655\u4e5f\u8f49\u5316\u70ba\u8a9e\u8a00\u5efa\u6a21\u7684\u5be6\u969b\u8a2d\u7f6e\u2014\u2014\u5728\u8a31\u591a\u4e0b\u6e38\u63a8\u7406\u4efb\u52d9\u4e0a\uff0c\u4e00\u500b\u5faa\u74b0 L \u6b21\u7684 k \u5c64\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u8207 kL \u5c64\u8a9e\u8a00\u6a21\u578b\u7af6\u722d\uff0c\u751a\u81f3\u512a\u65bc kL \u5c64\u8a9e\u8a00\u6a21\u578b\u3002\u4e8b\u5be6\u4e0a\uff0c\u6211\u5011\u7684\u5be6\u8b49\u5206\u6790\u63ed\u793a\u4e86\u4e00\u500b\u6709\u8da3\u7684\u73fe\u8c61\uff1a\u5faa\u74b0\u548c\u975e\u5faa\u74b0\u6a21\u578b\u8868\u73fe\u51fa\u53d6\u6c7a\u65bc\u5b83\u5011\u6709\u6548\u6df1\u5ea6\u7684\u898f\u6a21\u884c\u70ba\uff0c\u985e\u4f3c\u65bc\u601d\u60f3\u93c8 (CoT) \u63a8\u7406\u7684\u63a8\u7406\u6642\u9593\u898f\u6a21\u3002\u6211\u5011\u9032\u4e00\u6b65\u901a\u904e\u8b49\u660e\u5faa\u74b0\u6a21\u578b\u6703\u96b1\u5f0f\u751f\u6210\u6f5b\u5728\u601d\u60f3\uff0c\u4e26\u4e14\u53ef\u4ee5\u7528 T \u500b\u5faa\u74b0\u6a21\u64ec T \u6b65\u7684 CoT\uff0c\u95e1\u660e\u4e86\u8207 CoT \u63a8\u7406\u7684\u806f\u7e6b\u3002\u53d7\u9019\u4e9b\u767c\u73fe\u7684\u555f\u767c\uff0c\u6211\u5011\u9084\u63d0\u51fa\u4e86\u63a8\u7406\u548c\u8a18\u61b6\u4e4b\u9593\u7684\u6709\u8da3\u4e8c\u5206\u6cd5\uff0c\u4e26\u8a2d\u8a08\u4e86\u4e00\u500b\u5728\u5169\u500b\u65b9\u9762\u90fd\u6709\u6548\u7684\u57fa\u65bc\u5faa\u74b0\u7684\u6b63\u5247\u5316\u3002", "author": "Nikunj Saunshi et.al.", "authors": "Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J. Reddi", "id": "2502.17416v1", "paper_url": "http://arxiv.org/abs/2502.17416v1", "repo": "null"}}