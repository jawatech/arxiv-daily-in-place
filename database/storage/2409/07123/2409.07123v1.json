{"2409.07123": {"publish_time": "2024-09-11", "title": "Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem", "paper_summary": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.", "paper_summary_zh": "\u81ea\u7136\u8a9e\u8a00\u89e3\u91cb (NLE) \u5c0d\u65bc\u95e1\u660e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6c7a\u7b56\u80cc\u5f8c\u7684\u63a8\u7406\u81f3\u95dc\u91cd\u8981\u3002\u5df2\u7d93\u958b\u767c\u51fa\u8a31\u591a\u6280\u8853\u4f86\u4f7f\u7528 LLM \u751f\u6210 NLE\u3002\u7136\u800c\uff0c\u8207\u4eba\u985e\u4e00\u6a23\uff0cLLM \u53ef\u80fd\u4e26\u975e\u7e3d\u662f\u80fd\u5728\u9996\u6b21\u5617\u8a66\u6642\u7522\u751f\u6700\u4f73\u7684 NLE\u3002\u53d7\u4eba\u985e\u5b78\u7fd2\u904e\u7a0b\u7684\u555f\u767c\uff0c\u6211\u5011\u5f15\u5165\u4e86 Cross-Refine\uff0c\u5b83\u901a\u904e\u90e8\u7f72\u5169\u500b LLM \u5206\u5225\u4f5c\u70ba\u751f\u6210\u5668\u548c\u6279\u8a55\u8005\u4f86\u63a1\u7528\u89d2\u8272\u5efa\u6a21\u3002\u751f\u6210\u5668\u8f38\u51fa\u7b2c\u4e00\u500b NLE\uff0c\u7136\u5f8c\u4f7f\u7528\u6279\u8a55\u8005\u63d0\u4f9b\u7684\u53cd\u994b\u548c\u5efa\u8b70\u4f86\u5b8c\u5584\u9019\u500b\u521d\u59cb\u89e3\u91cb\u3002Cross-Refine \u4e0d\u9700\u8981\u4efb\u4f55\u76e3\u7763\u5f0f\u8a13\u7df4\u8cc7\u6599\u6216\u984d\u5916\u8a13\u7df4\u3002\u6211\u5011\u901a\u904e\u81ea\u52d5\u548c\u4eba\u5de5\u8a55\u4f30\uff0c\u4f7f\u7528\u4e09\u500b\u6700\u5148\u9032\u7684\u958b\u6e90 LLM\uff0c\u5728\u4e09\u500b NLP \u4efb\u52d9\u4e2d\u9a57\u8b49\u4e86 Cross-Refine\u3002\u6211\u5011\u9078\u64c7 Self-Refine (Madaan \u7b49\u4eba\uff0c2023 \u5e74) \u4f5c\u70ba\u57fa\u7dda\uff0c\u5b83\u50c5\u5229\u7528\u81ea\u6211\u53cd\u994b\u4f86\u5b8c\u5584\u89e3\u91cb\u3002\u6211\u5011\u5f9e\u81ea\u52d5\u8a55\u4f30\u548c\u7528\u6236\u7814\u7a76\u4e2d\u5f97\u51fa\u7684\u7d50\u679c\u8868\u660e\uff0cCross-Refine \u512a\u65bc Self-Refine\u3002\u540c\u6642\uff0cCross-Refine \u53ef\u4ee5\u6709\u6548\u5730\u4f7f\u7528\u529f\u80fd\u8f03\u5f31\u7684 LLM\uff0c\u800c Self-Refine \u50c5\u5728 ChatGPT \u4e2d\u7522\u751f\u5f37\u52c1\u7684\u7d50\u679c\u3002\u6b64\u5916\uff0c\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u6d88\u878d\u7814\u7a76\u4f86\u8a55\u4f30\u53cd\u994b\u548c\u5efa\u8b70\u7684\u91cd\u8981\u6027\u3002\u5b83\u5011\u90fd\u5c0d\u5b8c\u5584\u89e3\u91cb\u8d77\u8457\u91cd\u8981\u4f5c\u7528\u3002\u6211\u5011\u9032\u4e00\u6b65\u5728\u82f1\u8a9e\u548c\u5fb7\u8a9e\u7684\u96d9\u8a9e\u6578\u64da\u96c6\u4e0a\u8a55\u4f30\u4e86 Cross-Refine\u3002", "author": "Qianli Wang et.al.", "authors": "Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian M\u00f6ller, Vera Schmitt", "id": "2409.07123v1", "paper_url": "http://arxiv.org/abs/2409.07123v1", "repo": "null"}}