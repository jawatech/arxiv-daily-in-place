{"2407.17447": {"publish_time": "2024-07-24", "title": "Fluent Student-Teacher Redteaming", "paper_summary": "Many publicly available language models have been safety tuned to reduce the\nlikelihood of toxic or liability-inducing text. Users or security analysts\nattempt to jailbreak or redteam these models with adversarial prompts which\ncause compliance with requests. One attack method is to apply discrete\noptimization techniques to the prompt. However, the resulting attack strings\nare often gibberish text, easily filtered by defenders due to high measured\nperplexity, and may fail for unseen tasks and/or well-tuned models. In this\nwork, we improve existing algorithms (primarily GCG and BEAST) to develop\npowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our\ntechnique centers around a new distillation-based approach that encourages the\nvictim model to emulate a toxified finetune, either in terms of output\nprobabilities or internal activations. To encourage human-fluent attacks, we\nadd a multi-model perplexity penalty and a repetition penalty to the objective.\nWe also enhance optimizer strength by allowing token insertions, token swaps,\nand token deletions and by using longer attack sequences. The resulting process\nis able to reliably jailbreak the most difficult target models with prompts\nthat appear similar to human-written prompts. On Advbench we achieve attack\nsuccess rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while\nmaintaining model-measured perplexity $<33$; we achieve $95$% attack success\nfor Phi-3, though with higher perplexity. We also find a universally-optimized\nsingle fluent prompt that induces $>88$% compliance on previously unseen tasks\nacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box\nmodels.", "paper_summary_zh": "\u8a31\u591a\u516c\u958b\u53ef\u7528\u7684\u8a9e\u8a00\u6a21\u578b\u90fd\u7d93\u904e\u5b89\u5168\u6027\u8abf\u6574\uff0c\u4ee5\u964d\u4f4e\u7522\u751f\u6709\u6bd2\u6216\u5f15\u767c\u8cac\u4efb\u7684\u6587\u5b57\u7684\u53ef\u80fd\u6027\u3002\u4f7f\u7528\u8005\u6216\u5b89\u5168\u5206\u6790\u5e2b\u5617\u8a66\u4f7f\u7528\u5c0d\u6297\u6027\u63d0\u793a\u4f86\u7834\u89e3\u6216\u5c0d\u9019\u4e9b\u6a21\u578b\u9032\u884c\u7d05\u968a\u6e2c\u8a66\uff0c\u9019\u6703\u5c0e\u81f4\u7b26\u5408\u8acb\u6c42\u3002\u4e00\u7a2e\u653b\u64ca\u65b9\u6cd5\u662f\u5c0d\u63d0\u793a\u5957\u7528\u96e2\u6563\u6700\u4f73\u5316\u6280\u8853\u3002\u7136\u800c\uff0c\u7522\u751f\u7684\u653b\u64ca\u5b57\u4e32\u901a\u5e38\u662f\u7121\u610f\u7fa9\u7684\u6587\u5b57\uff0c\u7531\u65bc\u6e2c\u91cf\u5230\u7684\u56f0\u60d1\u5ea6\u9ad8\uff0c\u5f88\u5bb9\u6613\u88ab\u9632\u79a6\u8005\u904e\u6ffe\u6389\uff0c\u4e26\u4e14\u53ef\u80fd\u7121\u6cd5\u57f7\u884c\u672a\u898b\u904e\u7684\u5de5\u4f5c\u548c/\u6216\u8abf\u6574\u826f\u597d\u7684\u6a21\u578b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u6539\u9032\u4e86\u73fe\u6709\u6f14\u7b97\u6cd5\uff08\u4e3b\u8981\u662f GCG \u548c BEAST\uff09\uff0c\u4ee5\u5c0d Llama-2 \u548c Phi-3 \u7b49\u5b89\u5168\u6027\u8abf\u6574\u6a21\u578b\u767c\u52d5\u5f37\u5927\u4e14\u6d41\u66a2\u7684\u653b\u64ca\u3002\u6211\u5011\u7684\u6280\u8853\u570d\u7e5e\u4e00\u7a2e\u65b0\u7684\u57fa\u65bc\u84b8\u993e\u7684\u65b9\u6cd5\uff0c\u5b83\u9f13\u52f5\u53d7\u5bb3\u8005\u6a21\u578b\u6a21\u64ec\u4e2d\u6bd2\u7684\u5fae\u8abf\uff0c\u7121\u8ad6\u662f\u5728\u8f38\u51fa\u6a5f\u7387\u6216\u5167\u90e8\u6fc0\u6d3b\u65b9\u9762\u3002\u70ba\u4e86\u9f13\u52f5\u4eba\u985e\u6d41\u66a2\u7684\u653b\u64ca\uff0c\u6211\u5011\u5728\u76ee\u6a19\u4e2d\u52a0\u5165\u591a\u6a21\u578b\u56f0\u60d1\u5ea6\u61f2\u7f70\u548c\u91cd\u8907\u61f2\u7f70\u3002\u6211\u5011\u9084\u900f\u904e\u5141\u8a31\u63d2\u5165\u4ee3\u78bc\u3001\u4ea4\u63db\u4ee3\u78bc\u548c\u522a\u9664\u4ee3\u78bc\uff0c\u4ee5\u53ca\u4f7f\u7528\u8f03\u9577\u7684\u653b\u64ca\u5e8f\u5217\u4f86\u589e\u5f37\u6700\u4f73\u5316\u5668\u7684\u5f37\u5ea6\u3002\u7531\u6b64\u7522\u751f\u7684\u7a0b\u5e8f\u80fd\u5920\u53ef\u9760\u5730\u7834\u89e3\u6700\u56f0\u96e3\u7684\u76ee\u6a19\u6a21\u578b\uff0c\u5176\u63d0\u793a\u770b\u8d77\u4f86\u985e\u4f3c\u65bc\u4eba\u985e\u7de8\u5beb\u7684\u63d0\u793a\u3002\u5728 Advbench \u4e0a\uff0c\u6211\u5011\u5c0d Llama-2-7B\u3001Llama-3-8B \u548c Vicuna-7B \u9054\u5230\u4e86 $>93$% \u7684\u653b\u64ca\u6210\u529f\u7387\uff0c\u540c\u6642\u7dad\u6301\u6a21\u578b\u6e2c\u91cf\u7684\u56f0\u60d1\u5ea6 $<33$\uff1b\u6211\u5011\u5c0d Phi-3 \u9054\u5230\u4e86 $95$% \u7684\u653b\u64ca\u6210\u529f\u7387\uff0c\u5118\u7ba1\u56f0\u60d1\u5ea6\u8f03\u9ad8\u3002\u6211\u5011\u9084\u627e\u5230\u4e86\u7d93\u904e\u901a\u7528\u6700\u4f73\u5316\u7684\u55ae\u4e00\u6d41\u66a2\u63d0\u793a\uff0c\u5b83\u53ef\u4ee5\u5728 Llama-2-7B\u3001Phi-3-mini \u548c Vicuna-7B \u4e2d\u4ee5\u524d\u6240\u672a\u898b\u7684\u5de5\u4f5c\u4e0a\u5f15\u767c $>88$% \u7684\u5408\u898f\u6027\uff0c\u4e26\u50b3\u8f38\u5230\u5176\u4ed6\u9ed1\u76d2\u6a21\u578b\u3002", "author": "T. Ben Thompson et.al.", "authors": "T. Ben Thompson, Michael Sklar", "id": "2407.17447v1", "paper_url": "http://arxiv.org/abs/2407.17447v1", "repo": "null"}}