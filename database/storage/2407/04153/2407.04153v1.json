{"2407.04153": {"publish_time": "2024-07-04", "title": "Mixture of A Million Experts", "paper_summary": "The feedforward (FFW) layers in standard transformer architectures incur a\nlinear increase in computational costs and activation memory as the hidden\nlayer width grows. Sparse mixture-of-experts (MoE) architectures have emerged\nas a viable approach to address this issue by decoupling model size from\ncomputational cost. The recent discovery of the fine-grained MoE scaling law\nshows that higher granularity leads to better performance. However, existing\nMoE models are limited to a small number of experts due to computational and\noptimization challenges. This paper introduces PEER (parameter efficient expert\nretrieval), a novel layer design that utilizes the product key technique for\nsparse retrieval from a vast pool of tiny experts (over a million). Experiments\non language modeling tasks demonstrate that PEER layers outperform dense FFWs\nand coarse-grained MoEs in terms of performance-compute trade-off. By enabling\nefficient utilization of a massive number of experts, PEER unlocks the\npotential for further scaling of transformer models while maintaining\ncomputational efficiency.", "paper_summary_zh": "\u6a19\u6e96Transformer\u67b6\u69cb\u4e2d\u7684\u524d\u994b (FFW) \u5c64\u6703\u96a8\u8457\u96b1\u85cf\u5c64\u5bec\u5ea6\u7684\u589e\u52a0\u800c\u5c0e\u81f4\u8a08\u7b97\u6210\u672c\u548c\u6fc0\u6d3b\u8a18\u61b6\u9ad4\u5448\u7dda\u6027\u589e\u52a0\u3002\u7a00\u758f\u6df7\u5408\u5c08\u5bb6 (MoE) \u67b6\u69cb\u5df2\u6210\u70ba\u4e00\u7a2e\u53ef\u884c\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u900f\u904e\u89e3\u8026\u6a21\u578b\u5927\u5c0f\u548c\u8a08\u7b97\u6210\u672c\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\u3002\u6700\u8fd1\u767c\u73fe\u7684\u7d30\u7c92\u5ea6 MoE \u64f4\u5145\u6cd5\u5247\u986f\u793a\uff0c\u66f4\u9ad8\u7684\u7c92\u5ea6\u6703\u5e36\u4f86\u66f4\u597d\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 MoE \u6a21\u578b\u7531\u65bc\u8a08\u7b97\u548c\u6700\u4f73\u5316\u6311\u6230\u800c\u50c5\u9650\u65bc\u5c11\u6578\u5c08\u5bb6\u3002\u672c\u6587\u4ecb\u7d39\u4e86 PEER (\u53c3\u6578\u9ad8\u6548\u5c08\u5bb6\u6aa2\u7d22)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u5c64\u7d1a\u8a2d\u8a08\uff0c\u5b83\u5229\u7528\u7522\u54c1\u91d1\u9470\u6280\u8853\u5f9e\u9f90\u5927\u7684\u5fae\u5c0f\u5c08\u5bb6\u6c60 (\u8d85\u904e\u4e00\u767e\u842c) \u4e2d\u9032\u884c\u7a00\u758f\u6aa2\u7d22\u3002\u5728\u8a9e\u8a00\u6a21\u578b\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\uff0cPEER \u5c64\u5728\u6548\u80fd\u8a08\u7b97\u6298\u8877\u65b9\u9762\u512a\u65bc\u5bc6\u96c6 FFW \u548c\u7c97\u7c92\u5ea6 MoE\u3002\u900f\u904e\u6709\u6548\u5229\u7528\u5927\u91cf\u5c08\u5bb6\uff0cPEER \u91cb\u653e\u4e86Transformer\u6a21\u578b\u9032\u4e00\u6b65\u64f4\u5145\u7684\u6f5b\u529b\uff0c\u540c\u6642\u7dad\u6301\u8a08\u7b97\u6548\u7387\u3002", "author": "Xu Owen He et.al.", "authors": "Xu Owen He", "id": "2407.04153v1", "paper_url": "http://arxiv.org/abs/2407.04153v1", "repo": "null"}}