{"2411.08733": {"publish_time": "2024-11-13", "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "paper_summary": "Aligning Large Language Models (LLMs) traditionally relies on costly training\nand human preference annotations. Self-alignment seeks to reduce these expenses\nby enabling models to align themselves. To further lower costs and achieve\nalignment without any expensive tuning or annotations, we introduce a new\ntuning-free approach for self-alignment, Dynamic Rewarding with Prompt\nOptimization (\\ours). Our approach leverages a search-based optimization\nframework that allows LLMs to iteratively self-improve and craft the optimal\nalignment instructions, all without additional training or human intervention.\nThe core of \\ours is a dynamic rewarding mechanism, which identifies and\nrectifies model-specific alignment weaknesses, allowing LLMs to adapt\nefficiently to diverse alignment challenges. Empirical evaluations on eight\nrecent LLMs, both open- and closed-sourced, demonstrate that \\ours\nsignificantly enhances alignment performance, with base models outperforming\ntheir SFT/RLHF-tuned counterparts. Moreover, the prompts automatically\noptimized by \\ours surpass those curated by human experts, further validating\nthe effectiveness of our approach. Our findings highlight the great potential\nof current LLMs to achieve adaptive self-alignment through inference-time\noptimization, complementing tuning-based alignment methods.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5c0d\u9f4a\u50b3\u7d71\u4e0a\u4f9d\u8cf4\u65bc\u6602\u8cb4\u7684\u8a13\u7df4\u548c\u4eba\u5de5\u504f\u597d\u8a3b\u89e3\u3002\u81ea\u6211\u5c0d\u9f4a\u8a66\u5716\u901a\u904e\u4f7f\u6a21\u578b\u80fd\u5920\u81ea\u6211\u5c0d\u9f4a\u4f86\u964d\u4f4e\u9019\u4e9b\u8cbb\u7528\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u964d\u4f4e\u6210\u672c\u4e26\u5728\u6c92\u6709\u4efb\u4f55\u6602\u8cb4\u7684\u8abf\u6574\u6216\u8a3b\u89e3\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u5c0d\u9f4a\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u7121\u8abf\u6574\u81ea\u6211\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u5373\u5e36\u63d0\u793a\u512a\u5316\u7684\u52d5\u614b\u734e\u52f5\uff08\\ours\uff09\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u4e86\u4e00\u500b\u57fa\u65bc\u641c\u7d22\u7684\u512a\u5316\u6846\u67b6\uff0c\u5141\u8a31 LLM \u8fed\u4ee3\u5f0f\u5730\u81ea\u6211\u6539\u9032\u4e26\u5236\u5b9a\u6700\u4f73\u5c0d\u9f4a\u6307\u4ee4\uff0c\u6240\u6709\u9019\u4e9b\u90fd\u4e0d\u9700\u8981\u984d\u5916\u7684\u8a13\u7df4\u6216\u4eba\u5de5\u5e72\u9810\u3002\\ours \u7684\u6838\u5fc3\u662f\u4e00\u500b\u52d5\u614b\u734e\u52f5\u6a5f\u5236\uff0c\u5b83\u8b58\u5225\u548c\u7cfe\u6b63\u7279\u5b9a\u65bc\u6a21\u578b\u7684\u5c0d\u9f4a\u5f31\u9ede\uff0c\u5141\u8a31 LLM \u6709\u6548\u5730\u9069\u61c9\u4e0d\u540c\u7684\u5c0d\u9f4a\u6311\u6230\u3002\u5c0d\u516b\u7a2e\u6700\u8fd1\u7684 LLM\uff08\u5305\u62ec\u958b\u6e90\u548c\u9589\u6e90\uff09\u9032\u884c\u7684\u7d93\u9a57\u8a55\u4f30\u8868\u660e\uff0c\\ours\u986f\u8457\u63d0\u9ad8\u4e86\u5c0d\u9f4a\u6027\u80fd\uff0c\u57fa\u672c\u6a21\u578b\u512a\u65bc\u7d93\u904e SFT/RLHF \u8abf\u6574\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u7531 \\ours \u81ea\u52d5\u512a\u5316\u7684\u63d0\u793a\u512a\u65bc\u7531\u4eba\u985e\u5c08\u5bb6\u7b56\u5283\u7684\u63d0\u793a\uff0c\u9032\u4e00\u6b65\u9a57\u8b49\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u767c\u73fe\u5f37\u8abf\u4e86\u7576\u524d LLM \u901a\u904e\u63a8\u7406\u6642\u9593\u512a\u5316\u5be6\u73fe\u81ea\u9069\u61c9\u81ea\u6211\u5c0d\u9f4a\u7684\u5de8\u5927\u6f5b\u529b\uff0c\u88dc\u5145\u4e86\u57fa\u65bc\u8abf\u6574\u7684\u5c0d\u9f4a\u65b9\u6cd5\u3002", "author": "Somanshu Singla et.al.", "authors": "Somanshu Singla, Zhen Wang, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing", "id": "2411.08733v1", "paper_url": "http://arxiv.org/abs/2411.08733v1", "repo": "null"}}