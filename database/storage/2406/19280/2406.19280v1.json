{"2406.19280": {"publish_time": "2024-06-27", "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "paper_summary": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f8b\u5982 GPT-4V\uff0c\u5e26\u6765\u4e86\u91cd\u5927\u7684\u8fdb\u6b65\u3002\u7136\u800c\uff0c\u7531\u4e8e\u533b\u7597\u89c6\u89c9\u6587\u672c\u6570\u636e\u7684\u6570\u91cf\u548c\u8d28\u91cf\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u533b\u7597\u591a\u6a21\u6001\u80fd\u529b\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u6e90\u4e8e\u6570\u636e\u9690\u79c1\u95ee\u9898\u548c\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\u3002\u867d\u7136\u5f00\u521b\u6027\u7684\u65b9\u6cd5\u5229\u7528 PubMed \u7684\u5927\u89c4\u6a21\u3001\u53bb\u6807\u8bc6\u5316\u7684\u533b\u5b66\u56fe\u50cf\u6587\u672c\u5bf9\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u7531\u4e8e\u56fa\u6709\u7684\u6570\u636e\u566a\u58f0\uff0c\u5b83\u4eec\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4ece PubMed \u4e2d\u4f18\u5316\u4e86\u533b\u5b66\u56fe\u50cf\u6587\u672c\u5bf9\uff0c\u5e76\u4ee5\u201c\u975e\u76f2\u201d\u7684\u65b9\u5f0f\u91c7\u7528\u4e86 MLLM\uff08GPT-4V\uff09\u6765\u5bf9\u6570\u636e\u8fdb\u884c\u53bb\u566a\u548c\u91cd\u65b0\u683c\u5f0f\u5316\uff0c\u4ece\u800c\u521b\u5efa\u4e86\u5305\u542b 130 \u4e07\u4e2a\u533b\u5b66 VQA \u6837\u672c\u7684 PubMedVision \u6570\u636e\u96c6\u3002\u6211\u4eec\u7684\u9a8c\u8bc1\u8868\u660e\uff1a(1) PubMedVision \u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u5f53\u524d MLLM \u7684\u533b\u7597\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5728\u5305\u62ec MMMU \u5065\u5eb7\u4e0e\u533b\u5b66\u8f68\u9053\u5728\u5185\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\uff1b(2) \u533b\u5b66\u4e13\u5bb6\u7684\u624b\u52a8\u68c0\u67e5\u548c\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u6570\u636e\u96c6\u4e0e\u5176\u4ed6\u6570\u636e\u6784\u5efa\u65b9\u6cd5\u76f8\u6bd4\u7684\u5353\u8d8a\u6570\u636e\u8d28\u91cf\u3002\u4f7f\u7528 PubMedVision\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a 34B \u533b\u5b66 MLLM HuatuoGPT-Vision\uff0c\u5b83\u5728\u5f00\u6e90 MLLM \u4e2d\u7684\u533b\u5b66\u591a\u6a21\u6001\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "author": "Junying Chen et.al.", "authors": "Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang", "id": "2406.19280v1", "paper_url": "http://arxiv.org/abs/2406.19280v1", "repo": "null"}}