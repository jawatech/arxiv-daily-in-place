{"2410.19730": {"publish_time": "2024-10-25", "title": "Counting Ability of Large Language Models and Impact of Tokenization", "paper_summary": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs.", "paper_summary_zh": "<paragraph>\u4f5c\u70ba\u73fe\u4ee3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9aa8\u5e79\uff0cTransformer \u9762\u81e8\u56fa\u6709\u7684\u67b6\u69cb\u9650\u5236\uff0c\u9019\u6703\u963b\u7919\u5176\u63a8\u7406\u80fd\u529b\u3002\u8207\u905e\u8ff4\u7db2\u8def\u4e0d\u540c\uff0cTransformer \u7f3a\u5c11\u905e\u8ff4\u9023\u63a5\uff0c\u5c07\u5176\u9650\u5236\u5728\u6046\u5b9a\u6df1\u5ea6\u8a08\u7b97\u4e2d\u3002\u9019\u7a2e\u9650\u5236\u5c07\u5b83\u5011\u7f6e\u65bc\u8907\u96dc\u5ea6\u985e\u5225 TC$^0$ \u4e2d\uff0c\u5f9e\u7406\u8ad6\u4e0a\u4f86\u8aaa\uff0c\u5b83\u5011\u7121\u6cd5\u89e3\u6c7a\u96a8\u8457\u8f38\u5165\u9577\u5ea6\u589e\u9577\u800c\u9700\u8981\u8d8a\u4f86\u8d8a\u6df1\u5165\u63a8\u7406\u7684\u4efb\u52d9\u3002\u4f5c\u70ba\u8a31\u591a\u63a8\u7406\u4efb\u52d9\u7684\u57fa\u672c\u7d44\u6210\u90e8\u5206\uff0c\u8a08\u6578\u4e5f\u9700\u8981\u63a8\u7406\u6df1\u5ea6\u4ee5\u7dda\u6027\u589e\u9577\u624d\u80fd\u6b78\u7d0d\u57f7\u884c\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u5df2\u7d93\u78ba\u5b9a\u4e86\u57fa\u65bc Transformer \u7684\u5c08\u5bb6\u6a21\u578b\uff08\u5373\u5c08\u9580\u91dd\u5c0d\u8a08\u6578\u4efb\u52d9\u8a13\u7df4\u7684\u6a21\u578b\uff09\u4e2d\u7684\u8a08\u6578\u80fd\u529b\u7684\u4e0a\u9650\uff0c\u4f46\u7531\u65bc\u63a8\u7406\u6a5f\u5236\u7684\u5dee\u7570\uff0c\u9019\u4e9b\u767c\u73fe\u4e26\u4e0d\u80fd\u76f4\u63a5\u5ef6\u4f38\u5230\u901a\u7528 LLM\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5f37\u8abf\u4e86\u601d\u60f3\u93c8 (CoT) \u63a8\u7406\u5982\u4f55\u6709\u52a9\u65bc\u7de9\u89e3 Transformer \u5728\u8a08\u6578\u4efb\u52d9\u4e2d\u7684\u67d0\u4e9b\u67b6\u69cb\u9650\u5236\u3002\u7136\u800c\uff0c\u5f88\u5c11\u6709\u4eba\u95dc\u6ce8\u9019\u4e9b\u6a21\u578b\u4e2d\u6a19\u8a18\u5316\u7684\u4f5c\u7528\u3002\u8207\u901a\u5e38\u4f7f\u7528\u5b57\u5143\u7d1a\u6a19\u8a18\u5316\u7684\u5c08\u5bb6\u6a21\u578b\u4e0d\u540c\uff0cLLM \u901a\u5e38\u4f9d\u8cf4\u65bc\u4f4d\u5143\u7d44\u7d1a (BPE) \u6a19\u8a18\u5316\u5668\uff0c\u9019\u5f9e\u6839\u672c\u4e0a\u6539\u8b8a\u4e86\u63a8\u7406\u8655\u7406\u7684\u65b9\u5f0f\u3002\u6211\u5011\u7684\u7814\u7a76\u8abf\u67e5\u4e86\u6a19\u8a18\u5316\u5c0d LLM \u8a08\u6578\u80fd\u529b\u7684\u5f71\u97ff\uff0c\u6839\u64da\u8f38\u5165\u6a19\u8a18\u5316\u7684\u5dee\u7570\u63ed\u793a\u4e86\u5927\u91cf\u7684\u6548\u80fd\u8b8a\u5316\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u7406\u8ad6\u548c\u5be6\u9a57\u5206\u6790\uff0c\u6df1\u5165\u4e86\u89e3\u6a19\u8a18\u5316\u9078\u64c7\u5982\u4f55\u640d\u5bb3\u6a21\u578b\u7684\u7406\u8ad6\u53ef\u8a08\u7b97\u6027\uff0c\u5f9e\u800c\u6fc0\u52f5\u8a2d\u8a08\u65b0\u7684\u6a19\u8a18\u5316\u65b9\u6cd5\u4ee5\u589e\u5f37 LLM \u4e2d\u7684\u63a8\u7406\u3002</paragraph>", "author": "Xiang Zhang et.al.", "authors": "Xiang Zhang, Juntai Cao, Chenyu You", "id": "2410.19730v1", "paper_url": "http://arxiv.org/abs/2410.19730v1", "repo": "null"}}