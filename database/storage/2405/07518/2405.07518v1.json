{"2405.07518": {"publish_time": "2024-05-13", "title": "SambaNova SN40L: Scaling the AI Memory Wall with Dataflow and Composition of Experts", "paper_summary": "Monolithic large language models (LLMs) like GPT-4 have paved the way for\nmodern generative AI applications. Training, serving, and maintaining\nmonolithic LLMs at scale, however, remains prohibitively expensive and\nchallenging. The disproportionate increase in compute-to-memory ratio of modern\nAI accelerators have created a memory wall, necessitating new methods to deploy\nAI. Composition of Experts (CoE) is an alternative modular approach that lowers\nthe cost and complexity of training and serving. However, this approach\npresents two key challenges when using conventional hardware: (1) without fused\noperations, smaller models have lower operational intensity, which makes high\nutilization more challenging to achieve; and (2) hosting a large number of\nmodels can be either prohibitively expensive or slow when dynamically switching\nbetween them.\n  In this paper, we describe how combining CoE, streaming dataflow, and a\nthree-tier memory system scales the AI memory wall. We describe Samba-CoE, a\nCoE system with 150 experts and a trillion total parameters. We deploy\nSamba-CoE on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU) - a\ncommercial dataflow accelerator architecture that has been co-designed for\nenterprise inference and training applications. The chip introduces a new\nthree-tier memory system with on-chip distributed SRAM, on-package HBM, and\noff-package DDR DRAM. A dedicated inter-RDU network enables scaling up and out\nover multiple sockets. We demonstrate speedups ranging from 2x to 13x on\nvarious benchmarks running on eight RDU sockets compared with an unfused\nbaseline. We show that for CoE inference deployments, the 8-socket RDU Node\nreduces machine footprint by up to 19x, speeds up model switching time by 15x\nto 31x, and achieves an overall speedup of 3.7x over a DGX H100 and 6.6x over a\nDGX A100.", "paper_summary_zh": "\u5927\u578b\u55ae\u9ad4\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4f8b\u5982 GPT-4\uff0c\u70ba\u73fe\u4ee3\u751f\u6210\u5f0f AI \u61c9\u7528\u7a0b\u5f0f\u92ea\u5e73\u4e86\u9053\u8def\u3002\u7136\u800c\uff0c\u5927\u898f\u6a21\u8a13\u7df4\u3001\u670d\u52d9\u548c\u7dad\u8b77\u55ae\u9ad4 LLM \u4ecd\u7136\u6975\u5ea6\u6602\u8cb4\u4e14\u5177\u6709\u6311\u6230\u6027\u3002\u73fe\u4ee3 AI \u52a0\u901f\u5668\u7684\u904b\u7b97\u8207\u8a18\u61b6\u9ad4\u6bd4\u4f8b\u4e0d\u6210\u6bd4\u4f8b\u5730\u589e\u52a0\uff0c\u9020\u6210\u4e86\u8a18\u61b6\u9ad4\u7246\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u4f86\u90e8\u7f72 AI\u3002\u5c08\u5bb6\u7d44\u6210 (CoE) \u662f\u4e00\u7a2e\u66ff\u4ee3\u6027\u7684\u6a21\u7d44\u5316\u65b9\u6cd5\uff0c\u53ef\u964d\u4f4e\u8a13\u7df4\u548c\u670d\u52d9\u7684\u6210\u672c\u548c\u8907\u96dc\u6027\u3002\u7136\u800c\uff0c\u6b64\u65b9\u6cd5\u5728\u4f7f\u7528\u50b3\u7d71\u786c\u9ad4\u6642\u6703\u7522\u751f\u5169\u500b\u4e3b\u8981\u6311\u6230\uff1a(1) \u6c92\u6709\u878d\u5408\u904b\u7b97\uff0c\u8f03\u5c0f\u7684\u6a21\u578b\u5177\u6709\u8f03\u4f4e\u7684\u904b\u7b97\u5f37\u5ea6\uff0c\u9019\u4f7f\u5f97\u96e3\u4ee5\u9054\u6210\u9ad8\u5229\u7528\u7387\uff1b(2) \u5728\u52d5\u614b\u5730\u5728\u5927\u91cf\u6a21\u578b\u4e4b\u9593\u5207\u63db\u6642\uff0c\u4e3b\u6a5f\u5927\u91cf\u7684\u6a21\u578b\u53ef\u80fd\u6703\u975e\u5e38\u6602\u8cb4\u6216\u7de9\u6162\u3002\n\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u5982\u4f55\u7d50\u5408 CoE\u3001\u4e32\u6d41\u8cc7\u6599\u6d41\u548c\u4e09\u5c64\u8a18\u61b6\u9ad4\u7cfb\u7d71\u4f86\u64f4\u5145 AI \u8a18\u61b6\u9ad4\u7246\u3002\u6211\u5011\u63cf\u8ff0\u4e86 Samba-CoE\uff0c\u4e00\u500b\u64c1\u6709 150 \u500b\u5c08\u5bb6\u548c\u4e00\u5146\u5104\u500b\u7e3d\u53c3\u6578\u7684 CoE \u7cfb\u7d71\u3002\u6211\u5011\u5728 SambaNova SN40L \u53ef\u91cd\u65b0\u7d44\u614b\u8cc7\u6599\u6d41\u55ae\u5143 (RDU) \u4e0a\u90e8\u7f72 Samba-CoE\uff0c\u9019\u662f\u4e00\u500b\u5546\u696d\u8cc7\u6599\u6d41\u52a0\u901f\u5668\u67b6\u69cb\uff0c\u5c08\u70ba\u4f01\u696d\u63a8\u8ad6\u548c\u8a13\u7df4\u61c9\u7528\u7a0b\u5f0f\u800c\u5171\u540c\u8a2d\u8a08\u3002\u6b64\u6676\u7247\u5f15\u5165\u4e86\u65b0\u7684\u4e09\u5c64\u8a18\u61b6\u9ad4\u7cfb\u7d71\uff0c\u5305\u542b\u6676\u7247\u4e0a\u5206\u5e03\u5f0f SRAM\u3001\u5c01\u88dd\u4e0a HBM \u548c\u5c01\u88dd\u5916 DDR DRAM\u3002\u5c08\u7528\u7684 RDU \u9593\u7db2\u8def\u53ef\u4ee5\u5728\u591a\u500b\u63d2\u69fd\u4e0a\u64f4\u5145\u548c\u64f4\u5145\u3002\u6211\u5011\u5c55\u793a\u4e86\u5728\u516b\u500b RDU \u63d2\u69fd\u4e0a\u57f7\u884c\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u7684\u52a0\u901f\uff0c\u7bc4\u570d\u5f9e 2 \u500d\u5230 13 \u500d\uff0c\u8207\u672a\u878d\u5408\u7684\u57fa\u6e96\u76f8\u6bd4\u3002\u6211\u5011\u5c55\u793a\u4e86\u5c0d\u65bc CoE \u63a8\u8ad6\u90e8\u7f72\uff0c8 \u63d2\u69fd RDU \u7bc0\u9ede\u5c07\u6a5f\u5668\u4f54\u7528\u7a7a\u9593\u6e1b\u5c11\u4e86 19 \u500d\uff0c\u5c07\u6a21\u578b\u5207\u63db\u6642\u9593\u52a0\u901f\u4e86 15 \u500d\u5230 31 \u500d\uff0c\u4e26\u4e14\u6574\u9ad4\u52a0\u901f\u6bd4 DGX H100 \u5feb\u4e86 3.7 \u500d\uff0c\u6bd4 DGX A100 \u5feb\u4e86 6.6 \u500d\u3002", "author": "Raghu Prabhakar et.al.", "authors": "Raghu Prabhakar, Ram Sivaramakrishnan, Darshan Gandhi, Yun Du, Mingran Wang, Xiangyu Song, Kejie Zhang, Tianren Gao, Angela Wang, Karen Li, Yongning Sheng, Joshua Brot, Denis Sokolov, Apurv Vivek, Calvin Leung, Arjun Sabnis, Jiayu Bai, Tuowen Zhao, Mark Gottscho, David Jackson, Mark Luttrell, Manish K. Shah, Edison Chen, Kaizhao Liang, Swayambhoo Jain, Urmish Thakker, Dawei Huang, Sumti Jairath, Kevin J. Brown, Kunle Olukotun", "id": "2405.07518v1", "paper_url": "http://arxiv.org/abs/2405.07518v1", "repo": "null"}}