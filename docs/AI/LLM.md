
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-12**|**LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for Humor Classification**|Tanisha Khurana et.al.|[2408.06335v1](http://arxiv.org/abs/2408.06335v1)|null|
|**2024-08-12**|**FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection**|Yufei Huang et.al.|[2408.06333v1](http://arxiv.org/abs/2408.06333v1)|[link](https://github.com/thunlp/fastfid)|
|**2024-08-12**|**Animate, or Inanimate, That is the Question for Large Language Models**|Leonardo Ranaldi et.al.|[2408.06332v1](http://arxiv.org/abs/2408.06332v1)|null|
|**2024-08-12**|**VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents**|Xiao Liu et.al.|[2408.06327v1](http://arxiv.org/abs/2408.06327v1)|[link](https://github.com/thudm/visualagentbench)|
|**2024-08-12**|**Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example**|Yanan Chen et.al.|[2408.06318v1](http://arxiv.org/abs/2408.06318v1)|null|
|**2024-08-12**|**Body Transformer: Leveraging Robot Embodiment for Policy Learning**|Carmelo Sferrazza et.al.|[2408.06316v1](http://arxiv.org/abs/2408.06316v1)|null|
|**2024-08-12**|**Long-Form Answers to Visual Questions from Blind and Low Vision People**|Mina Huh et.al.|[2408.06303v1](http://arxiv.org/abs/2408.06303v1)|null|
|**2024-08-12**|**The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery**|Chris Lu et.al.|[2408.06292v1](http://arxiv.org/abs/2408.06292v1)|[link](https://github.com/sakanaai/ai-scientist)|
|**2024-08-12**|**Synthetic Patient-Physician Dialogue Generation from Clinical Notes Using LLM**|Trisha Das et.al.|[2408.06285v1](http://arxiv.org/abs/2408.06285v1)|null|
|**2024-08-12**|**MovieSum: An Abstractive Summarization Dataset for Movie Screenplays**|Rohit Saxena et.al.|[2408.06281v1](http://arxiv.org/abs/2408.06281v1)|[link](https://github.com/saxenarohit/moviesum)|
|**2024-08-12**|**Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation**|Jieyong Kim et.al.|[2408.06276v2](http://arxiv.org/abs/2408.06276v2)|null|
|**2024-08-12**|**FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data**|Haoran Sun et.al.|[2408.06273v2](http://arxiv.org/abs/2408.06273v2)|null|
|**2024-08-12**|**Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment**|Karel D'Oosterlinck et.al.|[2408.06266v1](http://arxiv.org/abs/2408.06266v1)|null|
|**2024-08-12**|**Audio Enhancement for Computer Audition -- An Iterative Training Paradigm Using Sample Importance**|Manuel Milling et.al.|[2408.06264v1](http://arxiv.org/abs/2408.06264v1)|null|
|**2024-08-12**|**Open-Source Molecular Processing Pipeline for Generating Molecules**|Shreyas V et.al.|[2408.06261v1](http://arxiv.org/abs/2408.06261v1)|null|
|**2024-08-12**|**Context-aware Visual Storytelling with Visual Prefix Tuning and Contrastive Learning**|Yingjin Song et.al.|[2408.06259v1](http://arxiv.org/abs/2408.06259v1)|null|
|**2024-08-12**|**Decentralized Intelligence Health Network (DIHN)**|Abraham Nash et.al.|[2408.06240v2](http://arxiv.org/abs/2408.06240v2)|null|
|**2024-08-12**|**FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks**|Min Ma et.al.|[2408.06227v1](http://arxiv.org/abs/2408.06227v1)|null|
|**2024-08-12**|**A Large-Scale Study of Model Integration in ML-Enabled Software Systems**|Yorick Sens et.al.|[2408.06226v1](http://arxiv.org/abs/2408.06226v1)|null|
|**2024-08-12**|**On Effects of Steering Latent Representation for Large Language Model Unlearning**|Dang Huu-Tien et.al.|[2408.06223v1](http://arxiv.org/abs/2408.06223v1)|null|
|**2024-08-12**|**Dynamic Blocked Clause Elimination for Projected Model Counting**|Jean-Marie Lagniez et.al.|[2408.06199v1](http://arxiv.org/abs/2408.06199v1)|null|
|**2024-08-12**|**Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers**|Zhenting Qi et.al.|[2408.06195v1](http://arxiv.org/abs/2408.06195v1)|null|
|**2024-08-12**|**Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting**|Halley Young et.al.|[2408.06186v1](http://arxiv.org/abs/2408.06186v1)|null|
|**2024-08-12**|**LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library**|Tianhao Yu et.al.|[2408.06150v1](http://arxiv.org/abs/2408.06150v1)|null|
|**2024-08-12**|**Med42-v2: A Suite of Clinical LLMs**|Clément Christophe et.al.|[2408.06142v1](http://arxiv.org/abs/2408.06142v1)|null|
|**2024-08-12**|**Utilize Transformers for translating Wikipedia category names**|Hoang-Thang Ta et.al.|[2408.06124v1](http://arxiv.org/abs/2408.06124v1)|null|
|**2024-08-12**|**A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs**|Xiaohua Lu et.al.|[2408.06121v1](http://arxiv.org/abs/2408.06121v1)|null|
|**2024-08-12**|**How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated Narrative Analysis Through Frame Semantics**|Igor Ryazanov et.al.|[2408.06120v1](http://arxiv.org/abs/2408.06120v1)|null|
|**2024-08-12**|**Building Decision Making Models Through Language Model Regime**|Yu Zhang et.al.|[2408.06087v1](http://arxiv.org/abs/2408.06087v1)|null|
|**2024-08-12**|**Fully Bayesian Differential Gaussian Processes through Stochastic Differential Equations**|Jian Xu et.al.|[2408.06069v1](http://arxiv.org/abs/2408.06069v1)|null|
|**2024-08-12**|**An Investigation Into Explainable Audio Hate Speech Detection**|Jinmyeong An et.al.|[2408.06065v1](http://arxiv.org/abs/2408.06065v1)|null|
|**2024-08-12**|**Quantum Algorithms for Compositional Text Processing**|Tuomas Laakkonen et.al.|[2408.06061v1](http://arxiv.org/abs/2408.06061v1)|null|
|**2024-08-12**|**Enhancing Dialogue Speech Recognition with Robust Contextual Awareness via Noise Representation Learning**|Wonjun Lee et.al.|[2408.06043v1](http://arxiv.org/abs/2408.06043v1)|null|
|**2024-08-12**|**Understanding Byzantine Robustness in Federated Learning with A Black-box Server**|Fangyuan Zhao et.al.|[2408.06042v1](http://arxiv.org/abs/2408.06042v1)|[link](https://github.com/alibaba/federatedscope)|
|**2024-08-12**|**ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation Using Large Language Models and Transformers**|Aristi Papastavrou et.al.|[2408.06040v1](http://arxiv.org/abs/2408.06040v1)|null|
|**2024-08-12**|**Spacetime $E(n)$-Transformer: Equivariant Attention for Spatio-temporal Graphs**|Sergio G. Charles et.al.|[2408.06039v1](http://arxiv.org/abs/2408.06039v1)|null|
|**2024-08-12**|**Peaking into the Black-box: Prediction Intervals Give Insight into Data-driven Quadrotor Model Reliability**|Jasper van Beers et.al.|[2408.06036v1](http://arxiv.org/abs/2408.06036v1)|null|
|**2024-08-12**|**Controlling Surprisal in Music Generation via Information Content Curve Matching**|Mathias Rose Bjare et.al.|[2408.06022v1](http://arxiv.org/abs/2408.06022v1)|[link](https://github.com/muthissar/iic)|
|**2024-08-12**|**Uncertainty-Informed Volume Visualization using Implicit Neural Representation**|Shanu Saklani et.al.|[2408.06018v1](http://arxiv.org/abs/2408.06018v1)|null|
|**2024-08-12**|**Exploring and Learning Structure: Active Inference Approach in Navigational Agents**|Daria de Tinguy et.al.|[2408.05982v1](http://arxiv.org/abs/2408.05982v1)|null|
|**2024-08-12**|**The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI**|Miriam Schirmer et.al.|[2408.05977v1](http://arxiv.org/abs/2408.05977v1)|null|
|**2024-08-12**|**Freehand Sketch Generation from Mechanical Components**|Zhichao Liao et.al.|[2408.05966v1](http://arxiv.org/abs/2408.05966v1)|null|
|**2024-08-12**|**Markov Senior -- Learning Markov Junior Grammars to Generate User-specified Content**|Mehmet Kayra Oğuz et.al.|[2408.05959v1](http://arxiv.org/abs/2408.05959v1)|[link](https://github.com/adockhorn/markovsenior)|
|**2024-08-12**|**ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models**|Ronak Pradeep et.al.|[2408.05948v1](http://arxiv.org/abs/2408.05948v1)|null|
|**2024-08-12**|**Multimodal Large Language Models for Phishing Webpage Detection and Identification**|Jehyun Lee et.al.|[2408.05941v1](http://arxiv.org/abs/2408.05941v1)|[link](https://github.com/jehleekr/multimodal_llm_phishing_detection)|
|**2024-08-12**|**Spb3DTracker: A Robust LiDAR-Based Person Tracker for Noisy Environment**|Eunsoo Im et.al.|[2408.05940v2](http://arxiv.org/abs/2408.05940v2)|null|
|**2024-08-12**|**Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models**|Fei Liu et.al.|[2408.05933v1](http://arxiv.org/abs/2408.05933v1)|null|
|**2024-08-12**|**BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation**|Hee Suk Yoon et.al.|[2408.05926v1](http://arxiv.org/abs/2408.05926v1)|[link](https://github.com/hee-suk-yoon/bi-mdrg)|
|**2024-08-12**|**Adapting a Foundation Model for Space-based Tasks**|Matthew Foutter et.al.|[2408.05924v1](http://arxiv.org/abs/2408.05924v1)|null|
|**2024-08-12**|**Urban Region Pre-training and Prompting: A Graph-based Approach**|Jiahui Jin et.al.|[2408.05920v1](http://arxiv.org/abs/2408.05920v1)|null|
|**2024-08-12**|**Inverse design of Non-parameterized Ventilated Acoustic Resonator via Variational Autoencoder with Acoustic Response-encoded Latent Space**|Min Woo Cho et.al.|[2408.05917v1](http://arxiv.org/abs/2408.05917v1)|null|
|**2024-08-12**|**A New Pipeline For Generating Instruction Dataset via RAG and Self Fine-Tuning**|Chih-Wei Song et.al.|[2408.05911v1](http://arxiv.org/abs/2408.05911v1)|null|
|**2024-08-12**|**AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising**|Peinan Zhang et.al.|[2408.05906v1](http://arxiv.org/abs/2408.05906v1)|[link](https://github.com/cyberagentailab/adtec)|
|**2024-08-12**|**Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts**|Peng Wu et.al.|[2408.05905v2](http://arxiv.org/abs/2408.05905v2)|null|
|**2024-08-12**|**Quantum Gradient Class Activation Map for Model Interpretability**|Hsin-Yi Lin et.al.|[2408.05899v1](http://arxiv.org/abs/2408.05899v1)|null|
|**2024-08-12**|**GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models**|Zixuan Wu et.al.|[2408.05894v1](http://arxiv.org/abs/2408.05894v1)|null|
|**2024-08-12**|**Polyp SAM 2: Advancing Zero shot Polyp Segmentation in Colorectal Cancer Detection**|Mobina Mansoori et.al.|[2408.05892v1](http://arxiv.org/abs/2408.05892v1)|[link](https://github.com/sajjad-sh33/polyp-sam-2)|
|**2024-08-12**|**Creating Arabic LLM Prompts at Scale**|Abdelrahman El-Sheikh et.al.|[2408.05882v1](http://arxiv.org/abs/2408.05882v1)|null|
|**2024-08-11**|**LLM-Based Robust Product Classification in Commerce and Compliance**|Sina Gholamian et.al.|[2408.05874v1](http://arxiv.org/abs/2408.05874v1)|null|
|**2024-08-11**|**Defining Boundaries: A Spectrum of Task Feasibility for Large Language Models**|Wenbo Zhang et.al.|[2408.05873v1](http://arxiv.org/abs/2408.05873v1)|null|
|**2024-08-11**|**The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms**|Adam Davies et.al.|[2408.05859v1](http://arxiv.org/abs/2408.05859v1)|null|
|**2024-08-11**|**Scaling Virtual World with Delta-Engine**|Hongqiu Wu et.al.|[2408.05842v1](http://arxiv.org/abs/2408.05842v1)|[link](https://github.com/gingasan/delta-engine)|
|**2024-08-11**|**Iterative Improvement of an Additively Regularized Topic Model**|Alex Gorbulev et.al.|[2408.05840v1](http://arxiv.org/abs/2408.05840v1)|null|
|**2024-08-11**|**Real-Time Drowsiness Detection Using Eye Aspect Ratio and Facial Landmark Detection**|Varun Shiva Krishna Rupani et.al.|[2408.05836v1](http://arxiv.org/abs/2408.05836v1)|null|
|**2024-08-11**|**Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm**|Eli Sennesh et.al.|[2408.05834v1](http://arxiv.org/abs/2408.05834v1)|null|
|**2024-08-11**|**Robust Domain Generalization for Multi-modal Object Recognition**|Yuxin Qiao et.al.|[2408.05831v1](http://arxiv.org/abs/2408.05831v1)|null|
|**2024-08-11**|**Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences**|Zhaoze Wang et.al.|[2408.05798v1](http://arxiv.org/abs/2408.05798v1)|null|
|**2024-08-11**|**A Meta-Engine Framework for Interleaved Task and Motion Planning using Topological Refinements**|Elisa Tosello et.al.|[2408.05795v1](http://arxiv.org/abs/2408.05795v1)|null|
|**2024-08-11**|**HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes**|Xuanyu Su et.al.|[2408.05794v1](http://arxiv.org/abs/2408.05794v1)|null|
|**2024-08-11**|**SAGA: A Participant-specific Examination of Story Alternatives and Goal Applicability for a Deeper Understanding of Complex Events**|Sai Vallurupalli et.al.|[2408.05793v1](http://arxiv.org/abs/2408.05793v1)|[link](https://github.com/saiumbc/saga)|
|**2024-08-11**|**Continual Learning of Nonlinear Independent Representations**|Boyang Sun et.al.|[2408.05788v1](http://arxiv.org/abs/2408.05788v1)|null|
|**2024-08-11**|**HiLight: A Hierarchy-aware Light Global Model with Hierarchical Local ConTrastive Learning**|Zhijian Chen et.al.|[2408.05786v1](http://arxiv.org/abs/2408.05786v1)|null|
|**2024-08-11**|**CURLing the Dream: Contrastive Representations for World Modeling in Reinforcement Learning**|Victor Augusto Kich et.al.|[2408.05781v1](http://arxiv.org/abs/2408.05781v1)|null|
|**2024-08-11**|**Seg-CycleGAN : SAR-to-optical image translation guided by a downstream task**|Hannuo Zhang et.al.|[2408.05777v1](http://arxiv.org/abs/2408.05777v1)|null|
|**2024-08-11**|**Neurosymbolic Methods for Rule Mining**|Agnieszka Lawrynowicz et.al.|[2408.05773v1](http://arxiv.org/abs/2408.05773v1)|null|
|**2024-08-11**|**An analysis of HOI: using a training-free method with multimodal visual foundation models when only the test set is available, without the training set**|Chaoyi Ai et.al.|[2408.05772v1](http://arxiv.org/abs/2408.05772v1)|null|
|**2024-08-11**|**LI-TTA: Language Informed Test-Time Adaptation for Automatic Speech Recognition**|Eunseop Yoon et.al.|[2408.05769v1](http://arxiv.org/abs/2408.05769v1)|null|
|**2024-08-11**|**Reference-free Hallucination Detection for Large Vision-Language Models**|Qing Li et.al.|[2408.05767v1](http://arxiv.org/abs/2408.05767v1)|null|
|**2024-08-11**|**VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing**|Chunyu Qiang et.al.|[2408.05758v1](http://arxiv.org/abs/2408.05758v1)|null|
|**2024-08-11**|**Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation**|Xiaoxiong Zhang et.al.|[2408.05748v1](http://arxiv.org/abs/2408.05748v1)|null|
|**2024-08-11**|**MTSCI: A Conditional Diffusion Model for Multivariate Time Series Consistent Imputation**|Jianping Zhou et.al.|[2408.05740v1](http://arxiv.org/abs/2408.05740v1)|[link](https://github.com/jeremychou28/mtsci)|
|**2024-08-11**|**Language-Informed Beam Search Decoding for Multilingual Machine Translation**|Yilin Yang et.al.|[2408.05738v1](http://arxiv.org/abs/2408.05738v1)|[link](https://github.com/yilinyang7/fairseq_multi_fix)|
|**2024-08-11**|**Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking**|Zhi-Cun Lyu et.al.|[2408.05715v1](http://arxiv.org/abs/2408.05715v1)|null|
|**2024-08-11**|**TC-KANRecon: High-Quality and Accelerated MRI Reconstruction via Adaptive KAN Mechanisms and Intelligent Feature Scaling**|Ruiquan Ge et.al.|[2408.05705v1](http://arxiv.org/abs/2408.05705v1)|[link](https://github.com/lcbkmm/tc-kanrecon)|
|**2024-08-11**|**A Novel Momentum-Based Deep Learning Techniques for Medical Image Classification and Segmentation**|Koushik Biswas et.al.|[2408.05692v1](http://arxiv.org/abs/2408.05692v1)|null|
|**2024-08-11**|**SRTFD: Scalable Real-Time Fault Diagnosis through Online Continual Learning**|Dandan Zhao et.al.|[2408.05681v1](http://arxiv.org/abs/2408.05681v1)|null|
|**2024-08-11**|**Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data**|Ji Liu et.al.|[2408.05678v1](http://arxiv.org/abs/2408.05678v1)|null|
|**2024-08-11**|**StealthDiffusion: Towards Evading Diffusion Forensic Detection through Diffusion Model**|Ziyin Zhou et.al.|[2408.05669v1](http://arxiv.org/abs/2408.05669v1)|[link](https://github.com/wyczzy/stealthdiffusion)|
|**2024-08-11**|**Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites**|Sayak Saha Roy et.al.|[2408.05667v1](http://arxiv.org/abs/2408.05667v1)|null|
|**2024-08-11**|**Training an NLP Scholar at a Small Liberal Arts College: A Backwards Designed Course Proposal**|Grusha Prasad et.al.|[2408.05664v1](http://arxiv.org/abs/2408.05664v1)|null|
|**2024-08-10**|**WiDe-analysis: Enabling One-click Content Moderation Analysis on Wikipedia's Articles for Deletion**|Hsuvas Borkakoty et.al.|[2408.05655v1](http://arxiv.org/abs/2408.05655v1)|null|
|**2024-08-10**|**Eigen Attention: Attention in Low-Rank Space for KV Cache Compression**|Utkarsh Saxena et.al.|[2408.05646v1](http://arxiv.org/abs/2408.05646v1)|null|
|**2024-08-10**|**Federated Smoothing Proximal Gradient for Quantile Regression with Non-Convex Penalties**|Reza Mirzaeifard et.al.|[2408.05640v2](http://arxiv.org/abs/2408.05640v2)|null|
|**2024-08-10**|**Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion**|Jacob K Christopher et.al.|[2408.05636v1](http://arxiv.org/abs/2408.05636v1)|null|
|**2024-08-10**|**Forecasting Day-Ahead Electricity Prices in the Integrated Single Electricity Market: Addressing Volatility with Comparative Machine Learning Methods**|Ben Harkin et.al.|[2408.05628v1](http://arxiv.org/abs/2408.05628v1)|null|
|**2024-08-10**|**UrFound: Towards Universal Retinal Foundation Models via Knowledge-Guided Masked Modeling**|Kai Yu et.al.|[2408.05618v1](http://arxiv.org/abs/2408.05618v1)|[link](https://github.com/yukkai/urfound)|
|**2024-08-10**|**Mitigating Metropolitan Carbon Emissions with Dynamic Eco-driving at Scale**|Vindula Jayawardana et.al.|[2408.05609v1](http://arxiv.org/abs/2408.05609v1)|null|
|**2024-08-10**|**Exploring Applications of State Space Models and Advanced Training Techniques in Sequential Recommendations: A Comparative Study on Efficiency and Performance**|Mark Obozov et.al.|[2408.05606v1](http://arxiv.org/abs/2408.05606v1)|null|
|**2024-08-10**|**Sequential Representation Learning via Static-Dynamic Conditional Disentanglement**|Mathieu Cyrille Simon et.al.|[2408.05599v1](http://arxiv.org/abs/2408.05599v1)|null|
|**2024-08-10**|**In-Context Exploiter for Extensive-Form Games**|Shuxin Li et.al.|[2408.05575v1](http://arxiv.org/abs/2408.05575v1)|null|

#### Abstracts
##### **LOLgorithm: Integrating Semantic,Syntactic and Contextual Elements for Humor Classification**
2408.06335v1 by Tanisha Khurana, Kaushik Pillalamarri, Vikram Pande, Munindar Singh

This paper explores humor detection through a linguistic lens, prioritizing
syntactic, semantic, and contextual features over computational methods in
Natural Language Processing. We categorize features into syntactic, semantic,
and contextual dimensions, including lexicons, structural statistics, Word2Vec,
WordNet, and phonetic style. Our proposed model, Colbert, utilizes BERT
embeddings and parallel hidden layers to capture sentence congruity. By
combining syntactic, semantic, and contextual features, we train Colbert for
humor detection. Feature engineering examines essential syntactic and semantic
features alongside BERT embeddings. SHAP interpretations and decision trees
identify influential features, revealing that a holistic approach improves
humor detection accuracy on unseen data. Integrating linguistic cues from
different dimensions enhances the model's ability to understand humor
complexity beyond traditional computational methods.

摘要：本文透過語言學角度探討幽默偵測，在自然語言處理中優先考量句法、語意和語境特徵，而非計算方法。我們將特徵分類為句法、語意和語境面向，包括詞彙、結構統計、Word2Vec、WordNet 和語音風格。我們提出的 Colbert 模型利用 BERT 嵌入和並行隱藏層來擷取句子的相符性。透過結合句法、語意和語境特徵，我們訓練 Colbert 進行幽默偵測。特徵工程會檢視基本的句法和語意特徵，以及 BERT 嵌入。SHAP 解釋和決策樹識別出有影響力的特徵，顯示出整體方法能提升未見資料的幽默偵測準確度。整合不同面向的語言線索，增強模型理解幽默複雜性的能力，超越傳統的計算方法。

##### **FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection**
2408.06333v1 by Yufei Huang, Xu Han, Maosong Sun

Open Domain Question Answering (ODQA) has been advancing rapidly in recent
times, driven by significant developments in dense passage retrieval and
pretrained language models. Current models typically incorporate the FiD
framework, which is composed by a neural retriever alongside an encoder-decoder
neural reader. In the answer generation process, the retriever will retrieve
numerous passages (around 100 for instance), each of which is then individually
encoded by the encoder. Subsequently, the decoder makes predictions based on
these encoded passages. Nevertheless, this framework can be relatively
time-consuming, particularly due to the extensive length of the gathered
passages. To address this, we introduce FastFiD in this paper, a novel approach
that executes sentence selection on the encoded passages. This aids in
retaining valuable sentences while reducing the context length required for
generating answers. Experiments on three commonly used datasets (Natural
Questions, TriviaQA and ASQA) demonstrate that our method can enhance the
inference speed by 2.3X-5.7X, while simultaneously maintaining the model's
performance. Moreover, an in-depth analysis of the model's attention reveals
that the selected sentences indeed hold a substantial contribution towards the
final answer. The codes are publicly available at
https://github.com/thunlp/FastFiD.

摘要：開放領域問答 (ODQA) 最近進展快速，由密集段落檢索和預訓練語言模型的重大發展所推動。目前的模型通常結合 FiD 框架，它由神經檢索器和編碼器-解碼器神經閱讀器組成。在答案生成過程中，檢索器會檢索大量段落（例如約 100 個），然後編碼器會個別編碼每個段落。隨後，解碼器根據這些編碼段落進行預測。然而，這個框架可能相對耗時，特別是因為收集的段落長度很長。為了解決這個問題，我們在這篇論文中介紹 FastFiD，這是一種創新的方法，可以在編碼段落上執行句子選取。這有助於保留有價值的句子，同時減少生成答案所需的上下文長度。在三個常用的資料集（自然問題、TriviaQA 和 ASQA）上進行的實驗表明，我們的模型可以將推論速度提高 2.3X-5.7X，同時保持模型的效能。此外，對模型注意力的深入分析顯示，選取的句子確實對最終答案有很大的貢獻。程式碼可在 https://github.com/thunlp/FastFiD 公開取得。

##### **Animate, or Inanimate, That is the Question for Large Language Models**
2408.06332v1 by Leonardo Ranaldi, Giulia Pucci, Fabio Massimo Zanzotto

The cognitive essence of humans is deeply intertwined with the concept of
animacy, which plays an essential role in shaping their memory, vision, and
multi-layered language understanding. Although animacy appears in language via
nuanced constraints on verbs and adjectives, it is also learned and refined
through extralinguistic information. Similarly, we assume that the LLMs'
limited abilities to understand natural language when processing animacy are
motivated by the fact that these models are trained exclusively on text.
  Hence, the question this paper aims to answer arises: can LLMs, in their
digital wisdom, process animacy in a similar way to what humans would do? We
then propose a systematic analysis via prompting approaches. In particular, we
probe different LLMs by prompting them using animate, inanimate, usual, and
stranger contexts. Results reveal that, although LLMs have been trained
predominantly on textual data, they exhibit human-like behavior when faced with
typical animate and inanimate entities in alignment with earlier studies.
Hence, LLMs can adapt to understand unconventional situations by recognizing
oddities as animated without needing to interface with unspoken cognitive
triggers humans rely on to break down animations.

摘要：人類的認知本質與有生命概念緊密相連，而有生命概念在塑造人類的記憶、視覺和多層次語言理解中扮演著至關重要的角色。儘管有生命概念透過動詞和形容詞的微妙約束出現在語言中，但它也是透過語言外的資訊學習和提煉的。同樣地，我們假設大型語言模型（LLM）在處理有生命概念時理解自然語言的能力有限，其原因在於這些模型僅接受過文字訓練。因此，本文旨在回答的問題是：LLM 能否以類似於人類的方式處理有生命概念？我們接著透過提示方法提出一個系統性的分析。具體來說，我們透過使用有生命、無生命、常見和陌生語境提示不同的 LLM 來探測它們。結果顯示，儘管 LLM 主要接受過文字資料訓練，但它們在面對典型的有生命和無生命實體時表現出類似人類的行為，這與先前的研究一致。因此，LLM 可以適應理解非常規情況，將奇異事物視為有生命的，而無需與人類用來分解動畫的未言明的認知觸發器介面。

##### **VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents**
2408.06327v1 by Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, Jie Tang

Large Multimodal Models (LMMs) have ushered in a new era in artificial
intelligence, merging capabilities in both language and vision to form highly
capable Visual Foundation Agents. These agents are postulated to excel across a
myriad of tasks, potentially approaching general artificial intelligence.
However, existing benchmarks fail to sufficiently challenge or showcase the
full potential of LMMs in complex, real-world environments. To address this
gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering
benchmark specifically designed to train and evaluate LMMs as visual foundation
agents across diverse scenarios, including Embodied, Graphical User Interface,
and Visual Design, with tasks formulated to probe the depth of LMMs'
understanding and interaction capabilities. Through rigorous testing across
nine proprietary LMM APIs and eight open models, we demonstrate the
considerable yet still developing agent capabilities of these models.
Additionally, VAB constructs a trajectory training set constructed through
hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and
Human Demonstrations, promoting substantial performance improvements in LMMs
through behavior cloning. Our work not only aims to benchmark existing models
but also provides a solid foundation for future development into visual
foundation agents. Code, train \& test data, and part of fine-tuned open LMMs
are available at \url{https://github.com/THUDM/VisualAgentBench}.

摘要：大型多模態模型 (LMM) 開啟了人工智慧的新紀元，結合語言和視覺能力，形成功能強大的視覺基礎代理。這些代理被假設可以在無數任務中表現出色，潛在地接近一般人工智慧。然而，現有的基準未能充分挑戰或展示 LMM 在複雜的現實環境中的全部潛力。為了解決這個差距，我們引入了 VisualAgentBench (VAB)，一個全面且開創性的基準，專門設計用於訓練和評估 LMM 作為視覺基礎代理，涵蓋各種場景，包括具身、圖形使用者介面和視覺設計，任務旨在探討 LMM 的理解和互動能力的深度。透過對九個專有 LMM API 和八個開放模型進行嚴格測試，我們展示了這些模型相當大但仍在發展的代理能力。此外，VAB 構建了一個通過混合方法構建的軌跡訓練集，包括基於程式的求解器、LMM 代理自舉和人類示範，透過行為複製提升 LMM 的顯著效能。我們的研究不僅旨在評量現有模型，也為未來發展成視覺基礎代理奠定了穩固的基礎。程式碼、訓練和測試資料，以及部分微調的開放 LMM 可在 \url{https://github.com/THUDM/VisualAgentBench} 取得。

##### **Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example**
2408.06318v1 by Yanan Chen, Ali Pesaranghader, Tanmana Sadhu, Dong Hoon Yi

Large language models (LLMs) have brought autonomous agents closer to
artificial general intelligence (AGI) due to their promising generalization and
emergent capabilities. There is, however, a lack of studies on how LLM-based
agents behave, why they could potentially fail, and how to improve them,
particularly in demanding real-world planning tasks. In this paper, as an
effort to fill the gap, we present our study using a realistic benchmark,
TravelPlanner, where an agent must meet multiple constraints to generate
accurate plans. We leverage this benchmark to address four key research
questions: (1) are LLM agents robust enough to lengthy and noisy contexts when
it comes to reasoning and planning? (2) can few-shot prompting adversely impact
the performance of LLM agents in scenarios with long context? (3) can we rely
on refinement to improve plans, and (4) can fine-tuning LLMs with both positive
and negative feedback lead to further improvement? Our comprehensive
experiments indicate that, firstly, LLMs often fail to attend to crucial parts
of a long context, despite their ability to handle extensive reference
information and few-shot examples; secondly, they still struggle with analyzing
the long plans and cannot provide accurate feedback for refinement; thirdly, we
propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and
negative feedback, resulting in substantial gains over Supervised Fine-Tuning
(SFT). Our findings offer in-depth insights to the community on various aspects
related to real-world planning applications.

摘要：大型語言模型 (LLM) 由於其令人期待的概括和新興能力，讓自主代理更接近於人工通用智能 (AGI)。然而，對於 LLM 基礎代理的行為方式、潛在失敗原因以及如何改進它們的研究卻有所欠缺，尤其是在要求嚴格的真實世界規劃任務中。在本文中，我們提出使用現實基準 TravelPlanner 的研究，作為填補空白的努力，其中代理必須滿足多項約束才能產生準確的計畫。我們利用此基準來解決四個關鍵研究問題：(1) 在推理和規劃方面，LLM 代理是否足夠健全，足以應對冗長且嘈雜的背景？(2) 在具有長背景的場景中，少次提示是否會對 LLM 代理的效能產生負面影響？(3) 我們是否可以依賴精進來改善計畫？(4) 使用正面和負面回饋微調 LLM 是否能進一步改善？我們的綜合實驗表明，首先，儘管 LLM 能夠處理廣泛的參考資訊和少次範例，但它們經常無法關注長背景的關鍵部分；其次，它們仍然難以分析長計畫，無法提供準確的回饋以供精進；第三，我們提出回饋感知微調 (FAFT)，它利用正面和負面回饋，產生優於監督式微調 (SFT) 的實質收益。我們的發現為社群提供深入的見解，涵蓋與現實世界規劃應用相關的各個方面。

##### **Body Transformer: Leveraging Robot Embodiment for Policy Learning**
2408.06316v1 by Carmelo Sferrazza, Dun-Ming Huang, Fangchen Liu, Jongmin Lee, Pieter Abbeel

In recent years, the transformer architecture has become the de facto
standard for machine learning algorithms applied to natural language processing
and computer vision. Despite notable evidence of successful deployment of this
architecture in the context of robot learning, we claim that vanilla
transformers do not fully exploit the structure of the robot learning problem.
Therefore, we propose Body Transformer (BoT), an architecture that leverages
the robot embodiment by providing an inductive bias that guides the learning
process. We represent the robot body as a graph of sensors and actuators, and
rely on masked attention to pool information throughout the architecture. The
resulting architecture outperforms the vanilla transformer, as well as the
classical multilayer perceptron, in terms of task completion, scaling
properties, and computational efficiency when representing either imitation or
reinforcement learning policies. Additional material including the open-source
code is available at https://sferrazza.cc/bot_site.

摘要：近年来，变压器架构已成为应用于自然语言处理和计算机视觉的机器学习算法的实际标准。尽管有显着证据表明在机器人学习的背景下成功部署了此架构，但我们声称原始变压器并未充分利用机器人学习问题的结构。因此，我们提出了 Body Transformer (BoT)，一种通过提供指导学习过程的归纳偏差来利用机器人体现的架构。我们将机器人主体表示为传感器和执行器的图形，并依靠掩码注意力来汇集整个架构中的信息。在完成任务、缩放属性和计算效率方面，无论是表示模仿还是强化学习策略，由此产生的架构都优于原始变压器以及经典的多层感知器。包括开源代码在内的其他材料可从 https://sferrazza.cc/bot_site 获得。

##### **Long-Form Answers to Visual Questions from Blind and Low Vision People**
2408.06303v1 by Mina Huh, Fangyuan Xu, Yi-Hao Peng, Chongyan Chen, Hansika Murugu, Danna Gurari, Eunsol Choi, Amy Pavel

Vision language models can now generate long-form answers to questions about
images - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a
dataset of long-form answers to visual questions posed by blind and low vision
(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,
collected from human expert describers and six VQA models. We develop and
annotate functional roles of sentences of LFVQA and demonstrate that long-form
answers contain information beyond the question answer such as explanations and
suggestions. We further conduct automatic and human evaluations with BLV and
sighted people to evaluate long-form answers. BLV people perceive both
human-written and generated long-form answers to be plausible, but generated
answers often hallucinate incorrect visual details, especially for unanswerable
visual questions (e.g., blurry or irrelevant images). To reduce hallucinations,
we evaluate the ability of VQA models to abstain from answering unanswerable
questions across multiple prompting strategies.

摘要：視覺語言模型現在可以針對圖片問題產生長篇答案，也就是長篇視覺問題解答 (LFVQA)。我們貢獻了 VizWiz-LF，這是一個長篇答案的資料集，由失明和視力低 (BLV) 的使用者提出的視覺問題。VizWiz-LF 包含 4.2k 個長篇答案，針對 600 個視覺問題，由人類專家描述者和六個 VQA 模型收集。我們開發並註解 LFVQA 句子的功能角色，並證明長篇答案包含了問題答案以外的資訊，例如解釋和建議。我們進一步與 BLV 和視力正常的人進行自動和人工評估，以評估長篇答案。BLV 人士認為，人類撰寫和產生的長篇答案都合理，但產生的答案通常會產生不正確的視覺細節，特別是針對無法回答的視覺問題（例如模糊或不相關的圖片）。為了減少幻覺，我們評估 VQA 模型在多種提示策略下，避免回答無法回答的問題的能力。

##### **The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery**
2408.06292v1 by Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha

One of the grand challenges of artificial general intelligence is developing
agents capable of conducting scientific research and discovering new knowledge.
While frontier models have already been used as aids to human scientists, e.g.
for brainstorming ideas, writing code, or prediction tasks, they still conduct
only a small part of the scientific process. This paper presents the first
comprehensive framework for fully automatic scientific discovery, enabling
frontier large language models to perform research independently and
communicate their findings. We introduce The AI Scientist, which generates
novel research ideas, writes code, executes experiments, visualizes results,
describes its findings by writing a full scientific paper, and then runs a
simulated review process for evaluation. In principle, this process can be
repeated to iteratively develop ideas in an open-ended fashion, acting like the
human scientific community. We demonstrate its versatility by applying it to
three distinct subfields of machine learning: diffusion modeling,
transformer-based language modeling, and learning dynamics. Each idea is
implemented and developed into a full paper at a cost of less than $15 per
paper. To evaluate the generated papers, we design and validate an automated
reviewer, which we show achieves near-human performance in evaluating paper
scores. The AI Scientist can produce papers that exceed the acceptance
threshold at a top machine learning conference as judged by our automated
reviewer. This approach signifies the beginning of a new era in scientific
discovery in machine learning: bringing the transformative benefits of AI
agents to the entire research process of AI itself, and taking us closer to a
world where endless affordable creativity and innovation can be unleashed on
the world's most challenging problems. Our code is open-sourced at
https://github.com/SakanaAI/AI-Scientist

摘要：人工智能通才的一大難題，在於開發出有能力進行科學研究和發現新知識的代理人。儘管前沿模型已用於協助人類科學家，例如集思廣益、撰寫程式碼或預測任務，但它們仍只進行科學程序的一小部分。本文提出了第一個全自動科學發現的綜合架構，讓前沿大型語言模型能夠獨立進行研究並傳達其發現。我們介紹了「AI 科學家」，它會產生新的研究想法、撰寫程式碼、執行實驗、視覺化結果，並透過撰寫完整的科學論文來描述其發現，然後執行模擬審查程序進行評估。原則上，這個程序可以重複進行，以開放式的方式反覆發展想法，就像人類科學社群一樣。我們透過將其應用於機器學習的三個不同子領域來展示其多樣性：擴散模型、基於Transformer的語言模型和學習動態。每個想法都以每篇論文不到 15 美元的成本實作並發展成一篇完整的論文。為了評估產生的論文，我們設計並驗證了一個自動審查員，我們展示其在評估論文分數時達到了接近人類的表現。AI 科學家可以產生論文，其根據我們的自動審查員的判斷，超過了頂尖機器學習會議的接受門檻。這種方法標誌著機器學習科學發現新時代的開始：將 AI 代理人的轉型優勢帶入 AI 本身的整個研究程序，並讓我們更接近一個無盡且負擔得起的創意和創新能被釋放到世界上最具挑戰性問題的世界。我們的程式碼在 https://github.com/SakanaAI/AI-Scientist 開源。

##### **Synthetic Patient-Physician Dialogue Generation from Clinical Notes Using LLM**
2408.06285v1 by Trisha Das, Dina Albassam, Jimeng Sun

Medical dialogue systems (MDS) enhance patient-physician communication,
improve healthcare accessibility, and reduce costs. However, acquiring suitable
data to train these systems poses significant challenges. Privacy concerns
prevent the use of real conversations, necessitating synthetic alternatives.
Synthetic dialogue generation from publicly available clinical notes offers a
promising solution to this issue, providing realistic data while safeguarding
privacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot
prompting and a feedback loop to generate and refine high-quality synthetic
dialogues. The feedback consists of weighted evaluation scores for similarity
and extractiveness. The iterative process ensures dialogues meet predefined
thresholds, achieving superior extractiveness as a result of the feedback loop.
Additionally, evaluation shows that the generated dialogues excel in factuality
metric compared to the baselines and has comparable diversity scores with GPT4.

摘要：醫療對話系統 (MDS) 可增強病患與醫師的溝通、改善醫療保健的可近性，並降低成本。然而，取得適當的資料來訓練這些系統會造成重大的挑戰。隱私問題會妨礙真實對話的使用，因此需要合成替代方案。從公開可取得的臨床筆記生成合成對話提供了這個問題一個有前景的解決方案，在保護隱私的同時提供真實的資料。我們的 SynDial 方法使用單一 LLM 透過零次提示和回饋迴路，反覆生成和改善高品質的合成對話。回饋包含相似性和抽取性的加權評分。反覆的程序可確保對話符合預先定義的閾值，並因回饋迴路而達成優異的抽取性。此外，評估顯示生成的對話在事實性指標上優於基準，且與 GPT4 具有相當的多樣性評分。

##### **MovieSum: An Abstractive Summarization Dataset for Movie Screenplays**
2408.06281v1 by Rohit Saxena, Frank Keller

Movie screenplay summarization is challenging, as it requires an
understanding of long input contexts and various elements unique to movies.
Large language models have shown significant advancements in document
summarization, but they often struggle with processing long input contexts.
Furthermore, while television transcripts have received attention in recent
studies, movie screenplay summarization remains underexplored. To stimulate
research in this area, we present a new dataset, MovieSum, for abstractive
summarization of movie screenplays. This dataset comprises 2200 movie
screenplays accompanied by their Wikipedia plot summaries. We manually
formatted the movie screenplays to represent their structural elements.
Compared to existing datasets, MovieSum possesses several distinctive features:
(1) It includes movie screenplays, which are longer than scripts of TV
episodes. (2) It is twice the size of previous movie screenplay datasets. (3)
It provides metadata with IMDb IDs to facilitate access to additional external
knowledge. We also show the results of recently released large language models
applied to summarization on our dataset to provide a detailed baseline.

摘要：電影劇本摘要具有挑戰性，因為它需要了解長的輸入背景和電影獨有的各種元素。大型語言模型在文件摘要中顯示出顯著的進步，但它們通常難以處理長的輸入背景。此外，儘管電視轉錄在最近的研究中受到關注，但電影劇本摘要仍未得到充分探討。為了激發這方面的研究，我們提出了新的資料集 MovieSum，用於電影劇本的抽象摘要。此資料集包含 2200 部電影劇本及其維基百科情節摘要。我們手動格式化電影劇本以表示其結構元素。與現有資料集相比，MovieSum 具有幾個顯著特徵：(1) 它包括電影劇本，其長度超過電視劇集的腳本。(2) 它比以前的電影劇本資料集大兩倍。(3) 它提供帶有 IMDb ID 的元資料，以方便存取其他外部知識。我們還展示了最近發布的大型語言模型應用於我們資料集上的摘要結果，以提供詳細的基準。

##### **Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation**
2408.06276v2 by Jieyong Kim, Hyunseo Kim, Hyunjin Cho, SeongKu Kang, Buru Chang, Jinyoung Yeo, Dongha Lee

Recent advancements in Large Language Models (LLMs) have demonstrated
exceptional performance across a wide range of tasks, generating significant
interest in their application to recommendation systems. However, existing
methods have not fully capitalized on the potential of LLMs, often constrained
by limited input information or failing to fully utilize their advanced
reasoning capabilities. To address these limitations, we introduce EXP3RT, a
novel LLM-based recommender designed to leverage rich preference information
contained in user and item reviews. EXP3RT is basically fine-tuned through
distillation from a teacher LLM to perform three key tasks in order: EXP3RT
first extracts and encapsulates essential subjective preferences from raw
reviews, aggregates and summarizes them according to specific criteria to
create user and item profiles. It then generates detailed step-by-step
reasoning followed by predicted rating, i.e., reasoning-enhanced rating
prediction, by considering both subjective and objective information from
user/item profiles and item descriptions. This personalized preference
reasoning from EXP3RT enhances rating prediction accuracy and also provides
faithful and reasonable explanations for recommendation. Extensive experiments
show that EXP3RT outperforms existing methods on both rating prediction and
candidate item reranking for top-k recommendation, while significantly
enhancing the explainability of recommendation systems.

摘要：大型語言模型 (LLM) 最近的進展已展現出在各種任務上的卓越表現，並對其在推薦系統中的應用產生極大的興趣。然而，現有方法並未充分發揮 LLM 的潛力，通常受到輸入資訊有限或未能充分利用其先進推理能力的限制。為了解決這些限制，我們引入了 EXP3RT，一種新穎的基於 LLM 的推薦器，旨在利用使用者和商品評論中包含的豐富偏好資訊。EXP3RT 基本上透過從教師 LLM 中進行蒸餾來微調，以便按順序執行三項關鍵任務：EXP3RT 首先從原始評論中提取並封裝必要的個人主觀偏好，根據特定標準彙總並總結它們以建立使用者和商品檔案。然後，它會考慮使用者/商品檔案和商品說明中的主觀和客觀資訊，產生詳細的逐步推理，然後進行預測評分，即增強推理的評分預測。EXP3RT 的這種個人化偏好推理增強了評分預測的準確性，也為推薦提供了忠實且合理的解釋。大量的實驗表明，EXP3RT 在評分預測和候選項目重新排序方面都優於現有方法，同時顯著提高了推薦系統的可解釋性。

##### **FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data**
2408.06273v2 by Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Du, Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong

Large language models (LLMs) have demonstrated prowess in a wide range of
tasks. However, many LLMs exhibit significant performance discrepancies between
high- and low-resource languages. To mitigate this challenge, we present
FuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the
need of the research community for balanced and high-performing multilingual
capabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is
trained from scratch on a meticulously balanced multilingual data repository
that contains 600 billion tokens covering 43 natural languages and 16
programming languages. In addition to the base model, we also develop two
instruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse
multilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined
with DPO on a preference dataset for enhanced alignment ability. Extensive
experiments on a wide range of multilingual benchmarks demonstrate the
competitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,
BLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability
analyses at both the neuron and representation level suggest that FuxiTranyu is
able to learn consistent multilingual representations across different
languages. To promote further research into multilingual LLMs and their working
mechanisms, we release both the base and instruction-tuned FuxiTranyu models
together with 58 pretraining checkpoints at HuggingFace and Github.

摘要：大型語言模型 (LLM) 已在各種任務中展現其優異的表現。然而，許多 LLM 在高資源語言和低資源語言之間表現出顯著的差異。為了緩解此挑戰，我們提出 FuxiTranyu，這是一個開源的多語言 LLM，旨在滿足研究社群對於平衡且高性能的多語言功能的需求。FuxiTranyu-8B，一個擁有 80 億個參數的基本模型，從一個精心平衡的多語言資料庫中從頭訓練，其中包含 6000 億個符號，涵蓋 43 種自然語言和 16 種程式語言。除了基本模型之外，我們還開發了兩個指令調整模型：FuxiTranyu-8B-SFT，在一個多元的多語言指令資料集上進行微調，以及 FuxiTranyu-8B-DPO，在一個偏好資料集上進一步使用 DPO 進行調整，以增強對齊能力。在各種多語言基準上的廣泛實驗證明了 FuxiTranyu 相對於現有的多語言 LLM（例如 BLOOM-7B、PolyLM-13B、Llama-2-Chat-7B 和 Mistral-7B-Instruct）具有競爭力。在神經元和表徵層級的詮釋性分析表明，FuxiTranyu 能夠學習跨不同語言的一致多語言表徵。為了促進對多語言 LLM 及其工作機制的進一步研究，我們在 HuggingFace 和 Github 上同時釋出基本和指令調整的 FuxiTranyu 模型，以及 58 個預訓練檢查點。

##### **Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment**
2408.06266v1 by Karel D'Oosterlinck, Winnie Xu, Chris Develder, Thomas Demeester, Amanpreet Singh, Christopher Potts, Douwe Kiela, Shikib Mehri

Large Language Models (LLMs) are often aligned using contrastive alignment
objectives and preference pair datasets. The interaction between model, paired
data, and objective makes alignment a complicated procedure, sometimes
producing subpar results. We study this and find that (i) preference data gives
a better learning signal when the underlying responses are contrastive, and
(ii) alignment objectives lead to better performance when they specify more
control over the model during training. Based on these insights, we introduce
Contrastive Learning from AI Revisions (CLAIR), a data-creation method which
leads to more contrastive preference pairs, and Anchored Preference
Optimization (APO), a controllable and more stable alignment objective. We
align Llama-3-8B-Instruct using various comparable datasets and alignment
objectives and measure MixEval-Hard scores, which correlate highly with human
judgments. The CLAIR preferences lead to the strongest performance out of all
datasets, and APO consistently outperforms less controllable objectives. Our
best model, trained on 32K CLAIR preferences with APO, improves
Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code
is available at https://github.com/ContextualAI/CLAIR_and_APO.

摘要：大型語言模型 (LLM) 通常使用對比式比對目標和偏好配對資料集來對齊。模型、配對資料和目標之間的互動使對齊成為一個複雜的程序，有時會產生次佳結果。我們研究了這一點，發現 (i) 當基礎回應具有對比性時，偏好資料會提供更好的學習訊號，以及 (ii) 對齊目標會在訓練期間對模型有更多控制時帶來更好的效能。根據這些見解，我們引入了 AI 修訂對比學習 (CLAIR)，這是一種資料建立方法，可產生更多對比偏好配對，以及錨定偏好最佳化 (APO)，這是一種可控且更穩定的對齊目標。我們使用各種可比較的資料集和對齊目標來對齊 Llama-3-8B-Instruct，並測量與人類判斷高度相關的 MixEval-Hard 分數。在所有資料集中，CLAIR 偏好帶來最強的效能，而 APO 則始終優於可控性較低的目標。我們在 32K CLAIR 偏好上訓練的最佳模型，使用 APO，將 Llama-3-8B-Instruct 提升了 7.65%，將與 GPT4-turbo 的差距縮小了 45%。我們的程式碼可在 https://github.com/ContextualAI/CLAIR_and_APO 取得。

##### **Audio Enhancement for Computer Audition -- An Iterative Training Paradigm Using Sample Importance**
2408.06264v1 by Manuel Milling, Shuo Liu, Andreas Triantafyllopoulos, Ilhan Aslan, Björn W. Schuller

Neural network models for audio tasks, such as automatic speech recognition
(ASR) and acoustic scene classification (ASC), are susceptible to noise
contamination for real-life applications. To improve audio quality, an
enhancement module, which can be developed independently, is explicitly used at
the front-end of the target audio applications. In this paper, we present an
end-to-end learning solution to jointly optimise the models for audio
enhancement (AE) and the subsequent applications. To guide the optimisation of
the AE module towards a target application, and especially to overcome
difficult samples, we make use of the sample-wise performance measure as an
indication of sample importance. In experiments, we consider four
representative applications to evaluate our training paradigm, i.e., ASR,
speech command recognition (SCR), speech emotion recognition (SER), and ASC.
These applications are associated with speech and non-speech tasks concerning
semantic and non-semantic features, transient and global information, and the
experimental results indicate that our proposed approach can considerably boost
the noise robustness of the models, especially at low signal-to-noise ratios
(SNRs), for a wide range of computer audition tasks in everyday-life noisy
environments.

摘要：針對音訊任務的神經網路模型，例如自動語音辨識 (ASR) 和音景分類 (ASC)，容易受到真實應用中的噪音污染。為了提升音訊品質，一個可以獨立開發的增強模組會明確用於目標音訊應用的前端。在本文中，我們提出一個端對端的學習解決方案，以共同最佳化音訊增強 (AE) 模型和後續應用。為了引導 AE 模組針對目標應用進行最佳化，特別是克服困難的樣本，我們利用樣本層級的效能測量作為樣本重要性的指標。在實驗中，我們考慮了四個代表性應用來評估我們的訓練範例，即 ASR、語音指令辨識 (SCR)、語音情緒辨識 (SER) 和 ASC。這些應用與語音和非語音任務相關，涉及語意和非語意特徵、瞬態和全域性資訊，而實驗結果表明，我們提出的方法可以大幅提升模型的抗噪性，特別是在日常生活中嘈雜環境中低訊噪比 (SNR) 的情況下，適用於廣泛的電腦聽覺任務。

##### **Open-Source Molecular Processing Pipeline for Generating Molecules**
2408.06261v1 by Shreyas V, Jose Siguenza, Karan Bania, Bharath Ramsundar

Generative models for molecules have shown considerable promise for use in
computational chemistry, but remain difficult to use for non-experts. For this
reason, we introduce open-source infrastructure for easily building generative
molecular models into the widely used DeepChem [Ramsundar et al., 2019] library
with the aim of creating a robust and reusable molecular generation pipeline.
In particular, we add high quality PyTorch [Paszke et al., 2019]
implementations of the Molecular Generative Adversarial Networks (MolGAN) [Cao
and Kipf, 2022] and Normalizing Flows [Papamakarios et al., 2021]. Our
implementations show strong performance comparable with past work [Kuznetsov
and Polykovskiy, 2021, Cao and Kipf, 2022].

摘要：分子生成模型在計算化學中展現出極大的應用前景，但對於非專家來說，仍然難以使用。因此，我們引入了開源基礎架構，可以輕鬆地將生成式分子模型建置到廣泛使用的 DeepChem [Ramsundar 等人，2019] 函式庫中，目的是建立一個強健且可重複使用的分子生成管道。特別是，我們加入了高品質的 PyTorch [Paszke 等人，2019] 分子生成對抗網路 (MolGAN) [Cao 和 Kipf，2022] 和正規化流 [Papamakarios 等人，2021] 的實作。我們的實作展現出強勁的效能，與過去的研究 [Kuznetsov 和 Polykovskiy，2021，Cao 和 Kipf，2022] 相當。

##### **Context-aware Visual Storytelling with Visual Prefix Tuning and Contrastive Learning**
2408.06259v1 by Yingjin Song, Denis Paperno, Albert Gatt

Visual storytelling systems generate multi-sentence stories from image
sequences. In this task, capturing contextual information and bridging visual
variation bring additional challenges. We propose a simple yet effective
framework that leverages the generalization capabilities of pretrained
foundation models, only training a lightweight vision-language mapping network
to connect modalities, while incorporating context to enhance coherence. We
introduce a multimodal contrastive objective that also improves visual
relevance and story informativeness. Extensive experimental results, across
both automatic metrics and human evaluations, demonstrate that the stories
generated by our framework are diverse, coherent, informative, and interesting.

摘要：視覺敘事系統從影像序列產生多句式故事。在此任務中，擷取上下文資訊和彌合理視覺變化帶來額外的挑戰。我們提出一個簡單但有效的架構，利用預訓練基礎模型的概化能力，僅訓練一個輕量級的視覺語言對應網路來連接模態，同時納入上下文以增強連貫性。我們引入一個多模態對比目標，它也改善了視覺相關性和故事的資訊性。在自動指標和人工評估中，廣泛的實驗結果證明，我們的架構產生的故事多樣、連貫、資訊豐富且有趣。

##### **Decentralized Intelligence Health Network (DIHN)**
2408.06240v2 by Abraham Nash

Decentralized Health Intelligence Network (DHIN) is a theoretical framework
addressing significant challenges of health data sovereignty and AI utilization
in healthcare caused by data fragmentation across providers and institutions.
It establishes a sovereign architecture for healthcare provision as a
prerequisite to a sovereign health network, then facilitates effective AI
utilization by overcoming barriers to accessing diverse medical data sources.
This comprehensive framework leverages: 1) self-sovereign identity architecture
coupled with a personal health record (PHR) as a prerequisite for health data
sovereignty; 2) a scalable federated learning (FL) protocol implemented on a
public blockchain for decentralized AI training in healthcare, where health
data remains with participants and only model parameter updates are shared; and
3) a scalable, trustless rewards mechanism to incentivize participation and
ensure fair reward distribution. This framework ensures that no entity can
prevent or control access to training on health data offered by participants or
determine financial benefits, as these processes operate on a public blockchain
with an immutable record and without a third party. It supports effective AI
training in healthcare, allowing patients to maintain control over their health
data, benefit financially, and contribute to a decentralized, scalable
ecosystem that leverages collective AI to develop beneficial healthcare
algorithms. Patients receive rewards into their digital wallets as an incentive
to opt-in to the FL protocol, with a long-term roadmap to funding decentralized
insurance solutions. This approach introduces a novel, self-financed healthcare
model that adapts to individual needs, complements existing systems, and
redefines universal coverage. It highlights the potential to transform
healthcare data management and AI utilization while empowering patients.

摘要：<paragraph>分散式健康情報網路 (DHIN) 是個理論架構，
用於解決醫療保健中因資料在各供應商和機構間分散所造成的健康資料主權和 AI 使用的重大挑戰。
它為醫療保健提供建立一個主權架構，作為一個主權健康網路的前提，然後透過克服取得多元醫療資料來源的障礙來促進有效的 AI 使用。
這個全面的架構利用：1) 自主權身分架構，結合個人健康記錄 (PHR) 作為健康資料主權的前提；2) 一個可擴充的聯盟式學習 (FL) 協定，實作於一個公共區塊鏈上，用於醫療保健中的分散式 AI 訓練，其中健康資料仍屬於參與者，且僅共享模型參數更新；3) 一個可擴充的、無信任的獎勵機制，用於激勵參與並確保公平的獎勵分配。這個架構確保沒有任何實體可以阻止或控制參與者提供的健康資料的訓練存取，或決定財務利益，因為這些流程在一個具有不可變記錄且沒有第三方的公共區塊鏈上運作。它支援醫療保健中的有效 AI 訓練，讓患者可以保有對其健康資料的控制權，從中獲取財務利益，並貢獻到一個分散式、可擴充的生態系統，該生態系統利用集體 AI 開發有益的醫療保健演算法。患者會收到獎勵到他們的數位錢包中，作為選擇加入 FL 協定的誘因，並有一個長期路線圖來資助分散式保險解決方案。這種方法引入了一個新穎的、自我資助的醫療保健模式，可以適應個別需求、補充現有系統，並重新定義全民覆蓋。它強調了轉型醫療保健資料管理和 AI 使用的潛力，同時賦能患者。</paragraph>

##### **FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks**
2408.06227v1 by Min Ma, Yuma Koizumi, Shigeki Karita, Heiga Zen, Jason Riesa, Haruko Ishikawa, Michiel Bacchiani

This paper introduces FLEURS-R, a speech restoration applied version of the
Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS)
corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as
FLEURS, with improved audio quality and fidelity by applying the speech
restoration model Miipher. The aim of FLEURS-R is to advance speech technology
in more languages and catalyze research including text-to-speech (TTS) and
other speech generation tasks in low-resource languages. Comprehensive
evaluations with the restored speech and TTS baseline models trained from the
new corpus show that the new corpus obtained significantly improved speech
quality while maintaining the semantic contents of the speech. The corpus is
publicly released via Hugging Face.

摘要：本文介紹 FLEURS-R，這是應用於語音還原的版本，來自於語音通用表示的少量學習評估 (FLEURS) 語料庫。FLEURS-R 在 102 種語言中維護一個 N 路徑並列語料庫，如同 FLEURS，並透過應用語音還原模型 Miipher 來改善音訊品質和保真度。FLEURS-R 的目標是提升更多語言的語音技術，並促進研究，包括低資源語言中的文字轉語音 (TTS) 和其他語音產生任務。使用從新語料庫訓練的還原語音和 TTS 基準模型進行的全面評估顯示，新語料庫顯著改善了語音品質，同時維持語音的語義內容。該語料庫透過 Hugging Face 公開發布。

##### **A Large-Scale Study of Model Integration in ML-Enabled Software Systems**
2408.06226v1 by Yorick Sens, Henriette Knopp, Sven Peldszus, Thorsten Berger

The rise of machine learning (ML) and its embedding in systems has
drastically changed the engineering of software-intensive systems.
Traditionally, software engineering focuses on manually created artifacts such
as source code and the process of creating them, as well as best practices for
integrating them, i.e., software architectures. In contrast, the development of
ML artifacts, i.e. ML models, comes from data science and focuses on the ML
models and their training data. However, to deliver value to end users, these
ML models must be embedded in traditional software, often forming complex
topologies. In fact, ML-enabled software can easily incorporate many different
ML models. While the challenges and practices of building ML-enabled systems
have been studied to some extent, beyond isolated examples, little is known
about the characteristics of real-world ML-enabled systems. Properly embedding
ML models in systems so that they can be easily maintained or reused is far
from trivial. We need to improve our empirical understanding of such systems,
which we address by presenting the first large-scale study of real ML-enabled
software systems, covering over 2,928 open source systems on GitHub. We
classified and analyzed them to determine their characteristics, as well as
their practices for reusing ML models and related code, and the architecture of
these systems. Our findings provide practitioners and researchers with insight
into practices for embedding and integrating ML models, bringing data science
and software engineering closer together.

摘要：機器學習 (ML) 的崛起及其在系統中的嵌入，徹底改變了軟體密集型系統的工程。
傳統上，軟體工程專注於人工建立的成品，例如原始碼和建立它們的流程，以及整合它們的最佳實務，也就是軟體架構。
相反地，ML 成品的開發，也就是 ML 模型，來自於資料科學，並專注於 ML 模型及其訓練資料。
然而，為了提供價值給最終使用者，這些 ML 模型必須嵌入在傳統的軟體中，通常形成複雜的拓撲。
事實上，啟用 ML 的軟體可以輕易地整合許多不同的 ML 模型。
雖然建構啟用 ML 的系統的挑戰和實務已經在某種程度上被研究，但除了孤立的範例之外，對於真實世界中啟用 ML 的系統的特性所知甚少。
適當地將 ML 模型嵌入系統中，以便它們可以被輕易地維護或重複使用，遠非易事。
我們需要改善對此類系統的經驗理解，我們透過提出第一個針對真實啟用 ML 的軟體系統的大規模研究來解決，涵蓋了 GitHub 上超過 2,928 個開源系統。
我們對它們進行分類和分析，以確定它們的特性，以及它們重複使用 ML 模型和相關程式碼的實務，以及這些系統的架構。
我們的發現為實務工作者和研究人員提供了對嵌入和整合 ML 模型實務的見解，讓資料科學和軟體工程更接近。

##### **On Effects of Steering Latent Representation for Large Language Model Unlearning**
2408.06223v1 by Dang Huu-Tien, Trung-Tin Pham, Hoang Thanh-Tung, Naoya Inoue

Representation Misdirection for Unlearning (RMU), which steers model
representation in the intermediate layer to a target random representation, is
an effective method for large language model (LLM) unlearning. Despite its high
performance, the underlying cause and explanation remain underexplored. In this
paper, we first theoretically demonstrate that steering forget representations
in the intermediate layer reduces token confidence, causing LLMs to generate
wrong or nonsense responses. Second, we investigate how the coefficient
influences the alignment of forget-sample representations with the random
direction and hint at the optimal coefficient values for effective unlearning
across different network layers. Third, we show that RMU unlearned models are
robust against adversarial jailbreak attacks. Last, our empirical analysis
shows that RMU is less effective when applied to the middle and later layers in
LLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet
effective alternative method that makes unlearning effective with most layers.
Extensive experiments demonstrate that Adaptive RMU significantly improves the
unlearning performance compared to prior art while incurring no additional
computational cost.

摘要：表示失導用於去學習（RMU），它將中間層的模型表示導向目標隨機表示，是一種用於大型語言模型（LLM）去學習的有效方法。儘管其效能很高，但其根本原因和解釋仍未充分探討。在本文中，我們首先從理論上證明，在中間層導向遺忘表示會降低標記信心，導致 LLM 產生錯誤或無意義的回應。其次，我們探討係數如何影響遺忘範例表示與隨機方向的一致性，並暗示了在不同網路層中有效去學習的最佳係數值。第三，我們表明，RMU 去學習的模型對於對抗性越獄攻擊具有魯棒性。最後，我們的經驗分析表明，當應用於 LLM 中間層和後續層時，RMU 的效果較差。為了解決這個缺點，我們提出了自適應 RMU——一種簡單但有效的替代方法，它使去學習在大部分層中都有效。大量的實驗表明，與先前的技術相比，自適應 RMU 大幅改善了去學習效能，同時不會產生額外的運算成本。

##### **Dynamic Blocked Clause Elimination for Projected Model Counting**
2408.06199v1 by Jean-Marie Lagniez, Pierre Marquis, Armin Biere

In this paper, we explore the application of blocked clause elimination for
projected model counting. This is the problem of determining the number of
models ||\exists X.{\Sigma}|| of a propositional formula {\Sigma} after
eliminating a given set X of variables existentially. Although blocked clause
elimination is a well-known technique for SAT solving, its direct application
to model counting is challenging as in general it changes the number of models.
However, we demonstrate, by focusing on projected variables during the blocked
clause search, that blocked clause elimination can be leveraged while
preserving the correct model count. To take advantage of blocked clause
elimination in an efficient way during model counting, a novel data structure
and associated algorithms are introduced. Our proposed approach is implemented
in the model counter d4. Our experiments demonstrate the computational benefits
of our new method of blocked clause elimination for projected model counting.

摘要：在本文中，我們探討了封鎖子句消除在預測模型計數中的應用。這是確定在存在地消除給定變數集 X 後，命題公式 {\Sigma} 的模型數量 ||\exists X.{\Sigma}|| 的問題。儘管封鎖子句消除是 SAT 求解的眾所周知技術，但其直接應用於模型計數具有挑戰性，因為它通常會改變模型數量。然而，我們透過在封鎖子句搜尋期間專注於預測變數，證明封鎖子句消除可以在保留正確模型計數的同時被利用。為了在模型計數期間有效利用封鎖子句消除，引入了新穎的資料結構和相關演算法。我們提出的方法在模型計數器 d4 中實作。我們的實驗證明了我們新的封鎖子句消除方法在預測模型計數中帶來的計算優點。

##### **Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers**
2408.06195v1 by Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang

This paper introduces rStar, a self-play mutual reasoning approach that
significantly improves reasoning capabilities of small language models (SLMs)
without fine-tuning or superior models. rStar decouples reasoning into a
self-play mutual generation-discrimination process. First, a target SLM
augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like
reasoning actions to construct higher quality reasoning trajectories. Next,
another SLM, with capabilities similar to the target SLM, acts as a
discriminator to verify each trajectory generated by the target SLM. The
mutually agreed reasoning trajectories are considered mutual consistent, thus
are more likely to be correct. Extensive experiments across five SLMs
demonstrate rStar can effectively solve diverse reasoning problems, including
GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K
accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for
Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be
available at https://github.com/zhentingqi/rStar.

摘要：本文介绍 rStar，一种自博弈相互推理方法，它显著提高了小型语言模型 (SLM) 的推理能力，无需微调或更高级的模型。rStar 将推理分解为一个自博弈相互生成-判别过程。首先，目标 SLM 使用一组丰富的人类推理动作增强蒙特卡罗树搜索 (MCTS)，以构建更高质量的推理轨迹。接下来，另一个具有与目标 SLM 类似能力的 SLM 充当判别器，以验证目标 SLM 生成的每个轨迹。相互商定的推理轨迹被认为是相互一致的，因此更有可能是正确的。跨越五个 SLM 的广泛实验表明，rStar 可以有效解决各种推理问题，包括 GSM8K、GSM-Hard、MATH、SVAMP 和 StrategyQA。值得注意的是，rStar 将 LLaMA2-7B 的 GSM8K 准确率从 12.51% 提高到 63.91%，Mistral-7B 的准确率从 36.46% 提高到 81.88%，LLaMA3-8B-Instruct 的准确率从 74.53% 提高到 91.13%。代码可在 https://github.com/zhentingqi/rStar 获得。

##### **Improving Structural Diversity of Blackbox LLMs via Chain-of-Specification Prompting**
2408.06186v1 by Halley Young, Yimeng Zeng, Jacob Gardner, Osbert Bastani

The capability to generate diverse text is a key challenge facing large
language models (LLMs). Thus far, diversity has been studied via metrics such
as $n$-gram diversity or diversity of BERT embeddings. However, for these kinds
of diversity, the user has little control over the dimensions along which
diversity is considered. For example, in the poetry domain, one might desire
diversity in terms of rhyme and meter, whereas in the code domain, one might
desire diversity in terms of the kinds of expressions used to solve a problem.
We propose a diversity metric called structural diversity, where the user
provides a mapping from generated text to features capturing the kinds of
diversity that they care about. In addition, we propose a novel strategy called
chain-of-specification (CoS) prompting for improving diversity by first having
the LLM generate a specification encoding one instance of structural features,
and then prompting the LLM to generate text that satisfies these features;
notably, our strategy works with blackbox LLMs. In our experiments, we show
that for structural diversity in the poetry and code domains, CoS significantly
improves diversity compared to several baselines.

摘要：生成多樣化的文本的能力是大語言模型 (LLM) 面臨的一項關鍵挑戰。到目前為止，多樣性已透過諸如 $n$-gram 多樣性或 BERT 嵌入多樣性等指標進行研究。然而，對於這些種類的多樣性，使用者對於多樣性被考慮的維度幾乎沒有控制權。例如，在詩歌領域中，人們可能希望在押韻和格律方面具有多樣性，而在程式碼領域中，人們可能希望在用於解決問題的表達式種類方面具有多樣性。我們提出了一種稱為結構多樣性的多樣性指標，其中使用者提供從生成文本到特徵的對應，以擷取他們關心的多樣性種類。此外，我們提出了一種稱為規格鏈 (CoS) 提示的新策略，透過先讓 LLM 產生編碼結構特徵的一個實例的規格，然後提示 LLM 產生滿足這些特徵的文本，進而改善多樣性；值得注意的是，我們的策略適用於黑盒 LLM。在我們的實驗中，我們展示了對於詩歌和程式碼領域中的結構多樣性，與幾個基線相比，CoS 大幅改善了多樣性。

##### **LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library**
2408.06150v1 by Tianhao Yu, Cai Yao, Zhuorui Sun, Feng Shi, Lin Zhang, Kangjie Lyu, Xuan Bai, Andong Liu, Xicheng Zhang, Jiali Zou, Wenshou Wang, Chris Lai, Kai Wang

In this study, we generate and maintain a database of 10 million virtual
lipids through METiS's in-house de novo lipid generation algorithms and lipid
virtual screening techniques. These virtual lipids serve as a corpus for
pre-training, lipid representation learning, and downstream task knowledge
transfer, culminating in state-of-the-art LNP property prediction performance.
We propose LipidBERT, a BERT-like model pre-trained with the Masked Language
Model (MLM) and various secondary tasks. Additionally, we compare the
performance of embeddings generated by LipidBERT and PhatGPT, our GPT-like
lipid generation model, on downstream tasks. The proposed bilingual LipidBERT
model operates in two languages: the language of ionizable lipid pre-training,
using in-house dry-lab lipid structures, and the language of LNP fine-tuning,
utilizing in-house LNP wet-lab data. This dual capability positions LipidBERT
as a key AI-based filter for future screening tasks, including new versions of
METiS de novo lipid libraries and, more importantly, candidates for in vivo
testing for orgran-targeting LNPs. To the best of our knowledge, this is the
first successful demonstration of the capability of a pre-trained language
model on virtual lipids and its effectiveness in downstream tasks using web-lab
data. This work showcases the clever utilization of METiS's in-house de novo
lipid library as well as the power of dry-wet lab integration.

摘要：在這項研究中，我們透過 METiS 內部的新生脂質生成演算法和脂質虛擬篩選技術，產生並維護一個包含 1000 萬種虛擬脂質的資料庫。這些虛擬脂質作為預先訓練、脂質表徵學習和下游任務知識轉移的語料庫，最終實現了最先進的 LNP 屬性預測效能。我們提出了 LipidBERT，這是一個使用遮罩語言模型 (MLM) 和各種次要任務預先訓練的類 BERT 模型。此外，我們比較了 LipidBERT 和 PhatGPT（我們的類 GPT 脂質生成模型）產生的嵌入在下游任務上的效能。所提出的雙語 LipidBERT 模型使用兩種語言：可電離脂質預先訓練的語言（使用內部乾式實驗室脂質結構）和 LNP 微調的語言（使用內部 LNP 濕式實驗室資料）。這種雙重能力將 LipidBERT 定位為未來篩選任務（包括 METiS 新生脂質庫的新版本，更重要的是，針對器官靶向 LNP 的體內測試候選藥物）的關鍵 AI 篩選器。據我們所知，這是預先訓練語言模型在虛擬脂質上的能力首次成功展示，以及它在使用網路實驗室資料執行下游任務的有效性。這項工作展示了巧妙利用 METiS 內部的新生脂質庫，以及乾濕實驗室整合的力量。

##### **Med42-v2: A Suite of Clinical LLMs**
2408.06142v1 by Clément Christophe, Praveen K Kanithi, Tathagata Raha, Shadab Khan, Marco AF Pimentel

Med42-v2 introduces a suite of clinical large language models (LLMs) designed
to address the limitations of generic models in healthcare settings. These
models are built on Llama3 architecture and fine-tuned using specialized
clinical data. They underwent multi-stage preference alignment to effectively
respond to natural prompts. While generic models are often preference-aligned
to avoid answering clinical queries as a precaution, Med42-v2 is specifically
trained to overcome this limitation, enabling its use in clinical settings.
Med42-v2 models demonstrate superior performance compared to the original
Llama3 models in both 8B and 70B parameter configurations and GPT-4 across
various medical benchmarks. These LLMs are developed to understand clinical
queries, perform reasoning tasks, and provide valuable assistance in clinical
environments. The models are now publicly available at
\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.

摘要：Med42-v2 引進了一套臨床大型語言模型 (LLM)，旨在解決醫療保健環境中通用模型的限制。這些模型建立在 Llama3 架構上，並使用專業臨床資料進行微調。它們經歷了多階段偏好調整，以有效回應自然提示。雖然通用模型通常偏好調整為避免預防性回答臨床查詢，但 Med42-v2 經過特別訓練以克服此限制，使其能夠在臨床環境中使用。與原始 Llama3 模型相比，Med42-v2 模型在 8B 和 70B 參數配置以及 GPT-4 中表現出優異的效能，橫跨各種醫療基準。這些 LLM 被開發用於理解臨床查詢、執行推理任務，並在臨床環境中提供有價值的協助。這些模型現在已於 \href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health} 公開提供。

##### **Utilize Transformers for translating Wikipedia category names**
2408.06124v1 by Hoang-Thang Ta, Quoc Thang La

On Wikipedia, articles are categorized to aid readers in navigating content
efficiently. The manual creation of new categories can be laborious and
time-intensive. To tackle this issue, we built language models to translate
Wikipedia categories from English to Vietnamese with a dataset containing
15,000 English-Vietnamese category pairs. Subsequently, small to medium-scale
Transformer pre-trained models with a sequence-to-sequence architecture were
fine-tuned for category translation. The experiments revealed that
OPUS-MT-en-vi surpassed other models, attaining the highest performance with a
BLEU score of 0.73, despite its smaller model storage. We expect our paper to
be an alternative solution for translation tasks with limited computer
resources.

摘要：在維基百科中，文章被分類，以幫助讀者有效率地瀏覽內容。手動建立新分類可能很費力且耗時。為了解決這個問題，我們建立語言模型，將維基百科的分類從英文翻譯成越南文，並使用一個包含 15,000 個英文-越南文分類對應的資料集。隨後，將具有序列到序列架構的小規模到中規模 Transformer 預訓練模型微調用於分類翻譯。實驗表明，儘管 OPUS-MT-en-vi 的模型儲存較小，但它超越了其他模型，以 0.73 的 BLEU 分數獲得了最高的效能。我們預計我們的論文將成為電腦資源有限的翻譯任務的替代方案。

##### **A Methodological Report on Anomaly Detection on Dynamic Knowledge Graphs**
2408.06121v1 by Xiaohua Lu, Leshanshui Yang

In this paper, we explore different approaches to anomaly detection on
dynamic knowledge graphs, specifically in a microservices environment for
Kubernetes applications. Our approach explores three dynamic knowledge graph
representations: sequential data, one-hop graph structure, and two-hop graph
structure, with each representation incorporating increasingly complex
structural information. Each phase includes different machine learning and deep
learning models. We empirically analyse their performance and propose an
approach based on ensemble learning of these models. Our approach significantly
outperforms the baseline on the ISWC 2024 Dynamic Knowledge Graph Anomaly
Detection dataset, providing a robust solution for anomaly detection in dynamic
complex data.

摘要：在本文中，我們探討了在動態知識圖譜上異常偵測的不同方法，特別是在 Kubernetes 應用程式的微服務環境中。我們的做法探討了三種動態知識圖譜表示：順序資料、一跳圖結構和兩跳圖結構，每個表示都包含越來越複雜的結構資訊。每個階段都包含不同的機器學習和深度學習模型。我們實證分析它們的效能，並提出一個基於這些模型的整體學習方法。我們的做法在 ISWC 2024 動態知識圖譜異常偵測資料集上大幅優於基準，為動態複雜資料中的異常偵測提供了一個強大的解決方案。

##### **How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated Narrative Analysis Through Frame Semantics**
2408.06120v1 by Igor Ryazanov, Carl Öhman, Johanna Björklund

The recent explosion of attention to AI is arguably one of the biggest in the
technology's media coverage. To investigate the effects it has on the
discourse, we perform a mixed-method frame semantics-based analysis on a
dataset of more than 49,000 sentences collected from 5846 news articles that
mention AI. The dataset covers the twelve-month period centred around the
launch of OpenAI's chatbot ChatGPT and is collected from the most visited
open-access English-language news publishers. Our findings indicate that during
the half year succeeding the launch, media attention rose
tenfold$\unicode{x2014}$from already historically high levels. During this
period, discourse has become increasingly centred around experts and political
leaders, and AI has become more closely associated with dangers and risks. A
deeper review of the data also suggests a qualitative shift in the types of
threat AI is thought to represent, as well as the anthropomorphic qualities
ascribed to it.

摘要：最近對 AI 的關注爆炸性增長，這無疑是科技媒體報導中最大的事件之一。為了探討它對話語的影響，我們對一個從 5846 篇提到 AI 的新聞文章中收集的超過 49,000 個句子的資料集，執行了一個基於混合方法框架語義的分析。該資料集涵蓋了圍繞 OpenAI 的聊天機器人 ChatGPT 推出前後的 12 個月期間，並且從最多人瀏覽的開放取用英文新聞出版商中收集。我們的研究結果表明，在推出後半年內，媒體關注度上升了十倍，從歷史高位進一步上升。在此期間，話語越來越集中在專家和政治領袖身上，而 AI 與危險和風險的關聯也越來越密切。對資料的更深入檢視也表明，人們認為 AI 所代表的威脅類型以及歸因於它的擬人化特質，都發生了質的轉變。

##### **Building Decision Making Models Through Language Model Regime**
2408.06087v1 by Yu Zhang, Haoxiang Liu, Feijun Jiang, Weihua Luo, Kaifu Zhang

We propose a novel approach for decision making problems leveraging the
generalization capabilities of large language models (LLMs). Traditional
methods such as expert systems, planning algorithms, and reinforcement learning
often exhibit limited generalization, typically requiring the training of new
models for each unique task. In contrast, LLMs demonstrate remarkable success
in generalizing across varied language tasks, inspiring a new strategy for
training decision making models. Our approach, referred to as "Learning then
Using" (LTU), entails a two-stage process. Initially, the \textit{learning}
phase develops a robust foundational decision making model by integrating
diverse knowledge from various domains and decision making contexts. The
subsequent \textit{using} phase refines this foundation model for specific
decision making scenarios. Distinct from other studies that employ LLMs for
decision making through supervised learning, our LTU method embraces a
versatile training methodology that combines broad pre-training with targeted
fine-tuning. Experiments in e-commerce domains such as advertising and search
optimization have shown that LTU approach outperforms traditional supervised
learning regimes in decision making capabilities and generalization. The LTU
approach is the first practical training architecture for both single-step and
multi-step decision making tasks combined with LLMs, which can be applied
beyond game and robot domains. It provides a robust and adaptable framework for
decision making, enhances the effectiveness and flexibility of various systems
in tackling various challenges.

摘要：我們提出了一個創新的決策制定問題方法，利用大型語言模型 (LLM) 的概化能力。傳統方法，例如專家系統、規劃演算法和強化學習，通常表現出有限的概化能力，通常需要為每個獨特任務訓練新的模型。相反，LLM 在各種語言任務中展現出顯著的概化成功，激發了訓練決策制定模型的新策略。我們的方法稱為「先學習再使用」(LTU)，需要一個兩階段的過程。最初，\textit{學習}階段透過整合來自各種領域和決策制定情境的知識，開發一個強大的基礎決策制定模型。後續的\textit{使用}階段為特定的決策制定情境調整這個基礎模型。與其他使用 LLM 透過監督式學習進行決策制定的研究不同，我們的 LTU 方法採用了多功能的訓練方法，結合廣泛的預訓練和目標微調。在電子商務領域（例如廣告和搜尋最佳化）的實驗顯示，LTU 方法在決策制定能力和概化能力方面優於傳統的監督式學習方法。LTU 方法是第一個結合 LLM 的單步和多步決策制定任務的實用訓練架構，它可以應用於遊戲和機器人領域之外。它提供了一個強大且適應性強的決策制定框架，增強了各種系統應對各種挑戰的有效性和靈活性。

##### **Fully Bayesian Differential Gaussian Processes through Stochastic Differential Equations**
2408.06069v1 by Jian Xu, Zhiqi Lin, Min Chen, Junmei Yang, Delu Zeng, John Paisley

Traditional deep Gaussian processes model the data evolution using a discrete
hierarchy, whereas differential Gaussian processes (DIFFGPs) represent the
evolution as an infinitely deep Gaussian process. However, prior DIFFGP methods
often overlook the uncertainty of kernel hyperparameters and assume them to be
fixed and time-invariant, failing to leverage the unique synergy between
continuous-time models and approximate inference. In this work, we propose a
fully Bayesian approach that treats the kernel hyperparameters as random
variables and constructs coupled stochastic differential equations (SDEs) to
learn their posterior distribution and that of inducing points. By
incorporating estimation uncertainty on hyperparameters, our method enhances
the model's flexibility and adaptability to complex dynamics. Additionally, our
approach provides a time-varying, comprehensive, and realistic posterior
approximation through coupling variables using SDE methods. Experimental
results demonstrate the advantages of our method over traditional approaches,
showcasing its superior performance in terms of flexibility, accuracy, and
other metrics. Our work opens up exciting research avenues for advancing
Bayesian inference and offers a powerful modeling tool for continuous-time
Gaussian processes.

摘要：傳統深度高斯程序使用離散層級來建模資料演進，而差分高斯程序 (DIFFGP) 則將演進表示為無限深度的 Gaussian 程序。然而，先前的 DIFFGP 方法經常忽略核仁超參數的不確定性，並假設它們是固定的和時不變的，未能利用連續時間模型和近似推論之間的獨特協同作用。在這項工作中，我們提出了一種完全貝氏方法，將核仁超參數視為隨機變數，並建構耦合隨機微分方程式 (SDE) 來學習它們的後驗分佈和誘導點的後驗分佈。透過納入超參數的估計不確定性，我們的模型增強了模型對複雜動態的靈活性與適應性。此外，我們的模型透過使用 SDE 方法耦合變數，提供了一個時變、全面且真實的後驗近似。實驗結果證明了我們的方法優於傳統方法，展示了它在靈活性、準確性和其他指標方面的優異效能。我們的模型開啟了推進貝氏推論的令人興奮的研究途徑，並為連續時間高斯程序提供了一個強大的建模工具。

##### **An Investigation Into Explainable Audio Hate Speech Detection**
2408.06065v1 by Jinmyeong An, Wonjun Lee, Yejin Jeon, Jungseul Ok, Yunsu Kim, Gary Geunbae Lee

Research on hate speech has predominantly revolved around detection and
interpretation from textual inputs, leaving verbal content largely unexplored.
While there has been limited exploration into hate speech detection within
verbal acoustic speech inputs, the aspect of interpretability has been
overlooked. Therefore, we introduce a new task of explainable audio hate speech
detection. Specifically, we aim to identify the precise time intervals,
referred to as audio frame-level rationales, which serve as evidence for hate
speech classification. Towards this end, we propose two different approaches:
cascading and End-to-End (E2E). The cascading approach initially converts audio
to transcripts, identifies hate speech within these transcripts, and
subsequently locates the corresponding audio time frames. Conversely, the E2E
approach processes audio utterances directly, which allows it to pinpoint hate
speech within specific time frames. Additionally, due to the lack of
explainable audio hate speech datasets that include audio frame-level
rationales, we curated a synthetic audio dataset to train our models. We
further validated these models on actual human speech utterances and found that
the E2E approach outperforms the cascading method in terms of the audio frame
Intersection over Union (IoU) metric. Furthermore, we observed that including
frame-level rationales significantly enhances hate speech detection accuracy
for the E2E approach.
  \textbf{Disclaimer} The reader may encounter content of an offensive or
hateful nature. However, given the nature of the work, this cannot be avoided.

摘要：仇恨言論的研究主要圍繞著文本輸入的偵測和解讀，而口語內容則鮮少被探討。雖然對於口語聲學語音輸入中的仇恨言論偵測已有有限的探討，但可解釋性的面向卻被忽略了。因此，我們提出了一個新的任務，即可解釋的音訊仇恨言論偵測。具體來說，我們的目標是找出精確的時間區間，稱為音訊幀級依據，作為仇恨言論分類的證據。為此，我們提出了兩種不同的方法：串聯和端到端 (E2E)。串聯方法最初將音訊轉換為文字稿，在這些文字稿中辨識仇恨言論，然後找出對應的音訊時間幀。相反地，E2E 方法直接處理音訊語句，這讓它能夠精確找出特定時間幀內的仇恨言論。此外，由於缺乏包含音訊幀級依據的可解釋音訊仇恨言論資料集，我們策劃了一個合成音訊資料集來訓練我們的模型。我們進一步在實際的人類語音語句上驗證這些模型，發現 E2E 方法在音訊幀交集比聯合 (IoU) 指標方面優於串聯方法。此外，我們觀察到，加入幀級依據可以顯著提升 E2E 方法的仇恨言論偵測準確度。
**免責聲明** 讀者可能會遇到具有冒犯性或仇恨性質的內容。然而，基於這項工作的性質，這是無法避免的。

##### **Quantum Algorithms for Compositional Text Processing**
2408.06061v1 by Tuomas Laakkonen, Konstantinos Meichanetzidis, Bob Coecke

Quantum computing and AI have found a fruitful intersection in the field of
natural language processing. We focus on the recently proposed DisCoCirc
framework for natural language, and propose a quantum adaptation, QDisCoCirc.
This is motivated by a compositional approach to rendering AI interpretable:
the behavior of the whole can be understood in terms of the behavior of parts,
and the way they are put together. For the model-native primitive operation of
text similarity, we derive quantum algorithms for fault-tolerant quantum
computers to solve the task of question-answering within QDisCoCirc, and show
that this is BQP-hard; note that we do not consider the complexity of
question-answering in other natural language processing models. Assuming
widely-held conjectures, implementing the proposed model classically would
require super-polynomial resources. Therefore, it could provide a meaningful
demonstration of the power of practical quantum processors. The model
construction builds on previous work in compositional quantum natural language
processing. Word embeddings are encoded as parameterized quantum circuits, and
compositionality here means that the quantum circuits compose according to the
linguistic structure of the text. We outline a method for evaluating the model
on near-term quantum processors, and elsewhere we report on a recent
implementation of this on quantum hardware. In addition, we adapt a quantum
algorithm for the closest vector problem to obtain a Grover-like speedup in the
fault-tolerant regime for our model. This provides an unconditional quadratic
speedup over any classical algorithm in certain circumstances, which we will
verify empirically in future work.

摘要：<paragraph>量子运算和 AI 在自然語言處理領域中找到了富有成效的交集。我們專注於最近提出的自然語言 DisCoCirc 框架，並提出一個量子適應 QDisCoCirc。這是一個基於組合式方法來讓 AI 可解讀的動機：整體的行為可以用部分的行為來理解，以及它們組合在一起的方式。對於文本相似性的模型原生原始操作，我們推導出容錯量子電腦的量子演算法，以解決 QDisCoCirc 中的問答任務，並表明這是一個 BQP-hard；請注意，我們不考慮其他自然語言處理模型中問答的複雜性。假設廣泛存在的猜想，以傳統方式實作建議的模型將需要超多項式資源。因此，它可以提供實際量子處理器功能的有意義示範。模型建構建立在先前的組合式量子自然語言處理工作上。字詞嵌入被編碼為參數化量子電路，而組合性在此表示量子電路根據文本的語言結構進行組合。我們概述了一種在近期量子處理器上評估模型的方法，而在其他地方我們報告了最近在量子硬體上的實作。此外，我們調整了最近向量問題的量子演算法，以在我們模型的容錯機制中獲得類似 Grover 的加速。在某些情況下，這提供了對任何傳統演算法的無條件二次加速，我們將在未來的研究中以經驗方式驗證這一點。</paragraph>

##### **Enhancing Dialogue Speech Recognition with Robust Contextual Awareness via Noise Representation Learning**
2408.06043v1 by Wonjun Lee, San Kim, Gary Geunbae Lee

Recent dialogue systems rely on turn-based spoken interactions, requiring
accurate Automatic Speech Recognition (ASR). Errors in ASR can significantly
impact downstream dialogue tasks. To address this, using dialogue context from
user and agent interactions for transcribing subsequent utterances has been
proposed. This method incorporates the transcription of the user's speech and
the agent's response as model input, using the accumulated context generated by
each turn. However, this context is susceptible to ASR errors because it is
generated by the ASR model in an auto-regressive fashion. Such noisy context
can further degrade the benefits of context input, resulting in suboptimal ASR
performance. In this paper, we introduce Context Noise Representation Learning
(CNRL) to enhance robustness against noisy context, ultimately improving
dialogue speech recognition accuracy. To maximize the advantage of context
awareness, our approach includes decoder pre-training using text-based dialogue
data and noise representation learning for a context encoder. Based on the
evaluation of speech dialogues, our method shows superior results compared to
baselines. Furthermore, the strength of our approach is highlighted in noisy
environments where user speech is barely audible due to real-world noise,
relying on contextual information to transcribe the input accurately.

摘要：<paragraph>最近的對話系統依賴於基於回合的口語互動，需要準確的自動語音辨識 (ASR)。ASR 中的錯誤會對下游對話任務產生重大影響。為了解決這個問題，有人提議使用使用者和代理互動的對話脈絡來轉錄後續的語句。此方法將使用者的語音轉錄和代理的回應作為模型輸入，並使用每回合產生的累積脈絡。然而，此脈絡容易受到 ASR 錯誤的影響，因為它是由 ASR 模型以自迴歸方式產生的。這種有雜訊的脈絡會進一步降低脈絡輸入的優點，導致次佳的 ASR 效能。在本文中，我們介紹了脈絡雜訊表徵學習 (CNRL) 來增強對抗有雜訊脈絡的穩健性，最終改善對話語音辨識的準確性。為了最大化脈絡感知的優點，我們的做法包括使用基於文字的對話資料進行解碼器預訓練，以及針對脈絡編碼器進行雜訊表徵學習。根據語音對話的評估，我們的做法與基準相比顯示出優異的結果。此外，我們做法的優點在有雜訊的環境中很明顯，在這種環境中，由於現實世界的雜訊，使用者的語音幾乎聽不見，必須依賴脈絡資訊來準確轉錄輸入。</paragraph>

##### **Understanding Byzantine Robustness in Federated Learning with A Black-box Server**
2408.06042v1 by Fangyuan Zhao, Yuexiang Xie, Xuebin Ren, Bolin Ding, Shusen Yang, Yaliang Li

Federated learning (FL) becomes vulnerable to Byzantine attacks where some of
participators tend to damage the utility or discourage the convergence of the
learned model via sending their malicious model updates. Previous works propose
to apply robust rules to aggregate updates from participators against different
types of Byzantine attacks, while at the same time, attackers can further
design advanced Byzantine attack algorithms targeting specific aggregation rule
when it is known. In practice, FL systems can involve a black-box server that
makes the adopted aggregation rule inaccessible to participants, which can
naturally defend or weaken some Byzantine attacks. In this paper, we provide an
in-depth understanding on the Byzantine robustness of the FL system with a
black-box server. Our investigation demonstrates the improved Byzantine
robustness of a black-box server employing a dynamic defense strategy. We
provide both empirical evidence and theoretical analysis to reveal that the
black-box server can mitigate the worst-case attack impact from a maximum level
to an expectation level, which is attributed to the inherent inaccessibility
and randomness offered by a black-box server.The source code is available at
https://github.com/alibaba/FederatedScope/tree/Byzantine_attack_defense to
promote further research in the community.

摘要：联邦学习 (FL) 容易遭受拜占庭攻击，在拜占庭攻击中，一些参与者倾向于通过发送其恶意模型更新来破坏效用或阻止学习模型的收敛。以前的工作提出应用稳健规则来聚合参与者针对不同类型的拜占庭攻击的更新，同时，攻击者可以进一步设计高级拜占庭攻击算法，在已知的情况下针对特定的聚合规则。在实践中，FL 系统可以涉及一个黑盒服务器，该服务器使参与者无法访问所采用的聚合规则，这可以自然地防御或削弱一些拜占庭攻击。在本文中，我们对具有黑盒服务器的 FL 系统的拜占庭鲁棒性提供了深入的理解。我们的调查证明了采用动态防御策略的黑盒服务器的拜占庭鲁棒性得到了提高。我们提供了经验证据和理论分析来揭示黑盒服务器可以将最坏情况下的攻击影响从最大级别降低到期望级别，这归因于黑盒服务器固有的不可访问性和随机性。源代码可在 https://github.com/alibaba/FederatedScope/tree/Byzantine_attack_defense 获得，以促进社区的进一步研究。

##### **ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation Using Large Language Models and Transformers**
2408.06040v1 by Aristi Papastavrou, Maria Lymperaiou, Giorgos Stamou

In the rapidly evolving fields of natural language processing and computer
vision, Visual Word Sense Disambiguation (VWSD) stands as a critical, yet
challenging task. The quest for models that can seamlessly integrate and
interpret multimodal data is more pressing than ever. Imagine a system that can
understand language with the depth and nuance of human cognition, while
simultaneously interpreting the rich visual context of the world around it.
  We present ARPA, an architecture that fuses the unparalleled contextual
understanding of large language models with the advanced feature extraction
capabilities of transformers, which then pass through a custom Graph Neural
Network (GNN) layer to learn intricate relationships and subtle nuances within
the data. This innovative architecture not only sets a new benchmark in visual
word disambiguation but also introduces a versatile framework poised to
transform how linguistic and visual data interact by harnessing the synergistic
strengths of its components, ensuring robust performance even in the most
complex disambiguation scenarios. Through a series of experiments and
comparative analysis, we reveal the substantial advantages of our model,
underscoring its potential to redefine standards in the field. Beyond its
architectural prowess, our architecture excels through experimental
enrichments, including sophisticated data augmentation and multi-modal training
techniques.
  ARPA's introduction marks a significant milestone in visual word
disambiguation, offering a compelling solution that bridges the gap between
linguistic and visual modalities. We invite researchers and practitioners to
explore the capabilities of our model, envisioning a future where such hybrid
models drive unprecedented advancements in artificial intelligence.

摘要：在自然語言處理和電腦視覺快速演進的領域中，視覺詞彙消歧 (VWSD) 是一個關鍵且具有挑戰性的任務。尋找能夠無縫整合和詮釋多模態資料的模型比以往任何時候都更加迫切。想像一個系統，它可以像人類認知一樣深入且細緻地理解語言，同時還能詮釋周圍世界的豐富視覺脈絡。
我們提出 ARPA，一種架構，它融合了大型語言模型無與倫比的脈絡理解能力和 Transformer 的進階特徵萃取能力，然後通過一個自訂圖形神經網路 (GNN) 層來學習資料中的複雜關係和細微差異。這種創新的架構不僅在視覺詞彙消歧中設定了新的基準，還引入了一個多功能的框架，準備通過利用其組成部分的協同優勢來轉變語言和視覺資料的互動方式，確保即使在最複雜的消歧場景中也能有強健的效能。透過一系列的實驗和比較分析，我們揭示了我們模型的顯著優勢，強調了它在重新定義該領域標準的潛力。除了其架構優勢之外，我們的架構還通過實驗豐富化而表現出色，包括精密的資料擴充和多模態訓練技術。
ARPA 的推出標誌著視覺詞彙消歧的一個重要里程碑，提供了一個引人注目的解決方案，彌合了語言和視覺模態之間的差距。我們邀請研究人員和從業人員探索我們模型的能力，展望一個由這種混合模型推動人工智慧前所未有的進步的未來。

##### **Spacetime $E(n)$-Transformer: Equivariant Attention for Spatio-temporal Graphs**
2408.06039v1 by Sergio G. Charles

We introduce an $E(n)$-equivariant Transformer architecture for
spatio-temporal graph data. By imposing rotation, translation, and permutation
equivariance inductive biases in both space and time, we show that the
Spacetime $E(n)$-Transformer (SET) outperforms purely spatial and temporal
models without symmetry-preserving properties. We benchmark SET against said
models on the charged $N$-body problem, a simple physical system with complex
dynamics. While existing spatio-temporal graph neural networks focus on
sequential modeling, we empirically demonstrate that leveraging underlying
domain symmetries yields considerable improvements for modeling dynamical
systems on graphs.

摘要：我們引入一個 $E(n)$-等變換器架構，用於時空圖形資料。透過施加旋轉、平移和排列等變異歸納偏差於時空，我們顯示時空 $E(n)$-Transformer (SET) 優於純時空模型，而後者沒有對稱性保留屬性。我們針對帶電的 $N$-體問題（一個具有複雜動態的簡單物理系統）對 SET 與上述模型進行基準測試。儘管現有的時空圖形神經網路著重於序列建模，我們實證地證明，利用基礎領域對稱性可為圖形上的動態系統建模帶來顯著的改進。

##### **Peaking into the Black-box: Prediction Intervals Give Insight into Data-driven Quadrotor Model Reliability**
2408.06036v1 by Jasper van Beers, Coen de Visser

Ensuring the reliability and validity of data-driven quadrotor model
predictions is essential for their accepted and practical use. This is
especially true for grey- and black-box models wherein the mapping of inputs to
predictions is not transparent and subsequent reliability notoriously difficult
to ascertain. Nonetheless, such techniques are frequently and successfully used
to identify quadrotor models. Prediction intervals (PIs) may be employed to
provide insight into the consistency and accuracy of model predictions. This
paper estimates such PIs for polynomial and Artificial Neural Network (ANN)
quadrotor aerodynamic models. Two existing ANN PI estimation techniques - the
bootstrap method and the quality driven method - are validated numerically for
quadrotor aerodynamic models using an existing high-fidelity quadrotor
simulation. Quadrotor aerodynamic models are then identified on real quadrotor
flight data to demonstrate their utility and explore their sensitivity to model
interpolation and extrapolation. It is found that the ANN-based PIs widen
considerably when extrapolating and remain constant, or shrink, when
interpolating. While this behaviour also occurs for the polynomial PIs, it is
of lower magnitude. The estimated PIs establish probabilistic bounds within
which the quadrotor model outputs will likely lie, subject to modelling and
measurement uncertainties that are reflected through the PI widths.

摘要：確保資料驅動四旋翼模型預測的可靠性和有效性，對於它們的接受和實際使用至關重要。這對於灰盒和黑盒模型來說尤其如此，其中輸入到預測的映射是不透明的，後續的可靠性難以確定。儘管如此，此類技術還是經常成功地用於識別四旋翼模型。預測區間 (PI) 可用於深入了解模型預測的一致性和準確性。本文估計了多項式和人工神經網路 (ANN) 四旋翼空氣動力模型的此類 PI。兩種現有的 ANN PI 估計技術——自舉法和品質驅動法——使用現有的高保真四旋翼模擬，對四旋翼空氣動力模型進行了數值驗證。然後在真實四旋翼飛行數據上識別四旋翼空氣動力模型，以展示它們的效用並探索它們對模型插值和外推的敏感性。發現基於 ANN 的 PI 在外推時會顯著擴大，而在插值時保持恆定或縮小。雖然多項式 PI 也會出現這種行為，但幅度較小。估計的 PI 建立了概率界限，四旋翼模型輸出很可能位於其中，具體取決於通過 PI 寬度反映的建模和測量不確定性。

##### **Controlling Surprisal in Music Generation via Information Content Curve Matching**
2408.06022v1 by Mathias Rose Bjare, Stefan Lattner, Gerhard Widmer

In recent years, the quality and public interest in music generation systems
have grown, encouraging research into various ways to control these systems. We
propose a novel method for controlling surprisal in music generation using
sequence models. To achieve this goal, we define a metric called Instantaneous
Information Content (IIC). The IIC serves as a proxy function for the perceived
musical surprisal (as estimated from a probabilistic model) and can be
calculated at any point within a music piece. This enables the comparison of
surprisal across different musical content even if the musical events occur in
irregular time intervals. We use beam search to generate musical material whose
IIC curve closely approximates a given target IIC. We experimentally show that
the IIC correlates with harmonic and rhythmic complexity and note density. The
correlation decreases with the length of the musical context used for
estimating the IIC. Finally, we conduct a qualitative user study to test if
human listeners can identify the IIC curves that have been used as targets when
generating the respective musical material. We provide code for creating IIC
interpolations and IIC visualizations on https://github.com/muthissar/iic.

摘要：近年来，人们对音乐生成系统的品质和公众兴趣与日俱增，这鼓励了人们对控制这些系统各种方式的研究。我们提出一种使用序列模型控制音乐生成中惊奇度的新方法。为了实现这一目标，我们定义了一个称为瞬时信息内容（IIC）的指标。IIC 作为感知到的音乐惊奇度（根据概率模型估计）的代理函数，可以在音乐作品中的任何点计算。即使音乐事件发生在不规则的时间间隔内，这也使得能够比较不同音乐内容中的惊奇度。我们使用波束搜索来生成音乐素材，其 IIC 曲线与给定的目标 IIC 非常接近。我们通过实验表明，IIC 与和声和节奏复杂度以及音符密度相关。相关性随着用于估计 IIC 的音乐背景长度而降低。最后，我们进行了一项定性的用户研究，以测试人类听众是否能够识别在生成各自音乐素材时用作目标的 IIC 曲线。我们提供了在 https://github.com/muthissar/iic 上创建 IIC 插值和 IIC 可视化的代码。

##### **Uncertainty-Informed Volume Visualization using Implicit Neural Representation**
2408.06018v1 by Shanu Saklani, Chitwan Goel, Shrey Bansal, Zhe Wang, Soumya Dutta, Tushar M. Athawale, David Pugmire, Christopher R. Johnson

The increasing adoption of Deep Neural Networks (DNNs) has led to their
application in many challenging scientific visualization tasks. While advanced
DNNs offer impressive generalization capabilities, understanding factors such
as model prediction quality, robustness, and uncertainty is crucial. These
insights can enable domain scientists to make informed decisions about their
data. However, DNNs inherently lack ability to estimate prediction uncertainty,
necessitating new research to construct robust uncertainty-aware visualization
techniques tailored for various visualization tasks. In this work, we propose
uncertainty-aware implicit neural representations to model scalar field data
sets effectively and comprehensively study the efficacy and benefits of
estimated uncertainty information for volume visualization tasks. We evaluate
the effectiveness of two principled deep uncertainty estimation techniques: (1)
Deep Ensemble and (2) Monte Carlo Dropout (MCDropout). These techniques enable
uncertainty-informed volume visualization in scalar field data sets. Our
extensive exploration across multiple data sets demonstrates that
uncertainty-aware models produce informative volume visualization results.
Moreover, integrating prediction uncertainty enhances the trustworthiness of
our DNN model, making it suitable for robustly analyzing and visualizing
real-world scientific volumetric data sets.

摘要：深度神经网络 (DNN) 的采用率不断提高，已将其应用于许多具有挑战性的科学可视化任务中。虽然先进的 DNN 提供了令人印象深刻的泛化能力，但了解模型预测质量、鲁棒性和不确定性等因素至关重要。这些见解使领域科学家能够对其数据做出明智的决策。然而，DNN 本质上缺乏估计预测不确定性的能力，因此需要新的研究来构建针对各种可视化任务量身定制的鲁棒不确定性感知可视化技术。在这项工作中，我们提出了不确定性感知隐式神经表征，以有效地对标量场数据集进行建模，并全面研究估计不确定性信息对体积可视化任务的功效和益处。我们评估了两种基于原理的深度不确定性估计技术的有效性：(1) 深度集成和 (2) 蒙特卡罗丢弃法 (MCDropout)。这些技术支持标量场数据集中的不确定性感知体积可视化。我们对多个数据集的广泛探索表明，不确定性感知模型产生了信息丰富的体积可视化结果。此外，整合预测不确定性增强了我们 DNN 模型的可信度，使其适用于对现实世界的科学体积数据集进行稳健分析和可视化。

##### **Exploring and Learning Structure: Active Inference Approach in Navigational Agents**
2408.05982v1 by Daria de Tinguy, Tim Verbelen, Bart Dhoedt

Drawing inspiration from animal navigation strategies, we introduce a novel
computational model for navigation and mapping, rooted in biologically inspired
principles. Animals exhibit remarkable navigation abilities by efficiently
using memory, imagination, and strategic decision-making to navigate complex
and aliased environments. Building on these insights, we integrate traditional
cognitive mapping approaches with an Active Inference Framework (AIF) to learn
an environment structure in a few steps. Through the incorporation of
topological mapping for long-term memory and AIF for navigation planning and
structure learning, our model can dynamically apprehend environmental
structures and expand its internal map with predicted beliefs during
exploration. Comparative experiments with the Clone-Structured Graph (CSCG)
model highlight our model's ability to rapidly learn environmental structures
in a single episode, with minimal navigation overlap. this is achieved without
prior knowledge of the dimensions of the environment or the type of
observations, showcasing its robustness and effectiveness in navigating
ambiguous environments.

摘要：從動物導航策略中汲取靈感，我們引入了一個新的導航和繪製地圖的計算模型，其根植於生物啟發原則。動物通過有效利用記憶、想像力和策略決策來導航複雜的別名環境，從而表現出非凡的導航能力。基於這些見解，我們將傳統的認知繪製地圖方法與主動推理框架 (AIF) 相結合，以便在幾個步驟中學習環境結構。通過將拓撲映射用於長期記憶，以及將 AIF 用於導航規劃和結構學習，我們的模型可以在探索過程中動態地理解環境結構，並通過預測信念擴展其內部地圖。與克隆結構圖 (CSCG) 模型的比較實驗突出了我們模型在單個情節中快速學習環境結構的能力，且導航重疊最少。這是沒有環境維度或觀察類型先驗知識就能實現的，展示了其在導航模棱兩可的環境中的穩健性和有效性。

##### **The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI**
2408.05977v1 by Miriam Schirmer, Tobias Leemann, Gjergji Kasneci, Jürgen Pfeffer, David Jurgens

Psychological trauma can manifest following various distressing events and is
captured in diverse online contexts. However, studies traditionally focus on a
single aspect of trauma, often neglecting the transferability of findings
across different scenarios. We address this gap by training language models
with progressing complexity on trauma-related datasets, including
genocide-related court data, a Reddit dataset on post-traumatic stress disorder
(PTSD), counseling conversations, and Incel forum posts. Our results show that
the fine-tuned RoBERTa model excels in predicting traumatic events across
domains, slightly outperforming large language models like GPT-4. Additionally,
SLALOM-feature scores and conceptual explanations effectively differentiate and
cluster trauma-related language, highlighting different trauma aspects and
identifying sexual abuse and experiences related to death as a common traumatic
event across all datasets. This transferability is crucial as it allows for the
development of tools to enhance trauma detection and intervention in diverse
populations and settings.

摘要：心理創傷可能在各種令人痛苦的事件後顯現，並在不同的網路情境中被捕捉到。然而，研究傳統上只專注於創傷的單一面向，常常忽略發現跨不同情境的可轉移性。我們透過在創傷相關的資料集上訓練進階複雜度的語言模型來解決這個差距，包括與種族滅絕相關的法院資料、關於創傷後壓力症候群 (PTSD) 的 Reddit 資料集、諮商對話以及 Incel 論壇文章。我們的結果顯示，微調後的 RoBERTa 模型在預測跨領域的創傷事件方面表現出色，略勝過 GPT-4 等大型語言模型。此外，SLALOM 特徵評分和概念解釋有效區分和群集創傷相關的語言，突顯不同的創傷面向，並將性虐待和與死亡相關的經驗識別為所有資料集中常見的創傷事件。這種可轉移性至關重要，因為它允許開發工具來增強創傷偵測和介入，適用於不同的族群和環境。

##### **Freehand Sketch Generation from Mechanical Components**
2408.05966v1 by Zhichao Liao, Di Huang, Heming Fang, Yue Ma, Fengyuan Piao, Xinghui Li, Long Zeng, Pingfa Feng

Drawing freehand sketches of mechanical components on multimedia devices for
AI-based engineering modeling has become a new trend. However, its development
is being impeded because existing works cannot produce suitable sketches for
data-driven research. These works either generate sketches lacking a freehand
style or utilize generative models not originally designed for this task
resulting in poor effectiveness. To address this issue, we design a two-stage
generative framework mimicking the human sketching behavior pattern, called
MSFormer, which is the first time to produce humanoid freehand sketches
tailored for mechanical components. The first stage employs Open CASCADE
technology to obtain multi-view contour sketches from mechanical components,
filtering perturbing signals for the ensuing generation process. Meanwhile, we
design a view selector to simulate viewpoint selection tasks during human
sketching for picking out information-rich sketches. The second stage
translates contour sketches into freehand sketches by a transformer-based
generator. To retain essential modeling features as much as possible and
rationalize stroke distribution, we introduce a novel edge-constraint stroke
initialization. Furthermore, we utilize a CLIP vision encoder and a new loss
function incorporating the Hausdorff distance to enhance the generalizability
and robustness of the model. Extensive experiments demonstrate that our
approach achieves state-of-the-art performance for generating freehand sketches
in the mechanical domain. Project page: https://mcfreeskegen.github.io .

摘要：<paragraph>在多媒體裝置上徒手繪製機械元件的素描，用於基於 AI 的工程建模，已成為一種新趨勢。然而，其發展受到阻礙，因為現有作品無法產生適合資料驅動研究的素描。這些作品要么產生缺乏徒手風格的素描，要么使用最初未針對此任務設計的生成模型，導致效果不佳。為了解決這個問題，我們設計了一個兩階段生成框架，模擬人類素描行為模式，稱為 MSFomer，這是首次產生針對機械元件量身定制的人形徒手素描。第一階段採用 Open CASCADE 技術從機械元件中獲取多視圖輪廓素描，過濾後續生成過程中的擾動信號。同時，我們設計了一個視圖選擇器來模擬人類素描期間的視點選擇任務，以挑選出資訊豐富的素描。第二階段通過基於Transformer的生成器將輪廓素描轉換為徒手素描。為了盡可能保留必要的建模特徵並合理化筆觸分佈，我們引入了一種新的邊緣約束筆觸初始化。此外，我們利用 CLIP 視覺編碼器和一個新的損失函數，結合豪斯多夫距離來增強模型的泛化性和魯棒性。大量的實驗證明，我們的模型在機械域中產生徒手素描方面取得了最先進的效能。專案頁面：https://mcfreeskegen.github.io。</paragraph>

##### **Markov Senior -- Learning Markov Junior Grammars to Generate User-specified Content**
2408.05959v1 by Mehmet Kayra Oğuz, Alexander Dockhorn

Markov Junior is a probabilistic programming language used for procedural
content generation across various domains. However, its reliance on manually
crafted and tuned probabilistic rule sets, also called grammars, presents a
significant bottleneck, diverging from approaches that allow rule learning from
examples. In this paper, we propose a novel solution to this challenge by
introducing a genetic programming-based optimization framework for learning
hierarchical rule sets automatically. Our proposed method ``Markov Senior''
focuses on extracting positional and distance relations from single input
samples to construct probabilistic rules to be used by Markov Junior. Using a
Kullback-Leibler divergence-based fitness measure, we search for grammars to
generate content that is coherent with the given sample. To enhance
scalability, we introduce a divide-and-conquer strategy that enables the
efficient generation of large-scale content. We validate our approach through
experiments in generating image-based content and Super Mario levels,
demonstrating its flexibility and effectiveness. In this way, ``Markov Senior''
allows for the wider application of Markov Junior for tasks in which an example
may be available, but the design of a generative rule set is infeasible.

摘要：馬可夫小輩是一種機率程式語言，用於在各種領域中產生程序化內容。然而，它依賴於手動製作和調整的機率規則集（也稱為文法），這是一個重大的瓶頸，偏離了允許從範例中學習規則的方法。在本文中，我們提出了一個創新的解決方案來應對這個挑戰，方法是引入一個基於遺傳程式設計的最佳化架構，用於自動學習階層式規則集。我們提出的方法「馬可夫長輩」專注於從單一輸入範例中提取位置和距離關係，以建構馬可夫小輩將使用的機率規則。使用一個基於庫爾貝克-李布勒散度的適應度量度，我們搜尋文法，以產生與給定範例相符的內容。為了增強可擴充性，我們引入了一個分而治之策略，使大規模內容的有效產生成為可能。我們透過在產生基於影像的內容和超級瑪利歐關卡的實驗中驗證我們的做法，證明了它的靈活性與有效性。透過這種方式，「馬可夫長輩」允許將馬可夫小輩更廣泛地應用於可能提供範例，但產生式規則集的設計卻不可行的任務中。

##### **ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge Graph QA datasets with Large Language Models**
2408.05948v1 by Ronak Pradeep, Daniel Lee, Ali Mousavi, Jeff Pound, Yisi Sang, Jimmy Lin, Ihab Ilyas, Saloni Potdar, Mostafa Arefiyan, Yunyao Li

The rapid advancement of Large Language Models (LLMs) and conversational
assistants necessitates dynamic, scalable, and configurable conversational
datasets for training and evaluation. These datasets must accommodate diverse
user interaction modes, including text and voice, each presenting unique
modeling challenges. Knowledge Graphs (KGs), with their structured and evolving
nature, offer an ideal foundation for current and precise knowledge. Although
human-curated KG-based conversational datasets exist, they struggle to keep
pace with the rapidly changing user information needs. We present ConvKGYarn, a
scalable method for generating up-to-date and configurable conversational KGQA
datasets. Qualitative psychometric analyses confirm our method can generate
high-quality datasets rivaling a popular conversational KGQA dataset while
offering it at scale and covering a wide range of human-interaction
configurations. We showcase its utility by testing LLMs on diverse
conversations - exploring model behavior on conversational KGQA sets with
different configurations grounded in the same KG fact set. Our results
highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate
parametric knowledge of LLMs, thus offering a robust solution to the constantly
evolving landscape of conversational assistants.

摘要：隨著大型語言模型 (LLM) 和對話式助理的快速進步，需要動態、可擴充且可設定的對話式資料集來進行訓練和評估。這些資料集必須容納不同的使用者互動模式，包括文字和語音，每種模式都呈現獨特的建模挑戰。知識圖譜 (KG) 具有結構化且不斷演進的特性，為當前和精確的知識提供了理想的基礎。儘管存在人工策展的基於知識圖譜的對話式資料集，但它們難以跟上快速變化的使用者資訊需求。我們提出 ConvKGYarn，這是一種可擴充的方法，用於產生最新的且可設定的對話式 KGQA 資料集。定性的心理測量分析證實，我們的模型可以產生與流行的對話式 KGQA 資料集相媲美的優質資料集，同時大規模提供資料集，並涵蓋廣泛的人機互動設定。我們透過在不同的對話中測試 LLM 來展示其效用，探索模型在對話式 KGQA 設定上的行為，這些設定基於相同的知識圖譜事實集。我們的結果突顯了 ConvKGYarn 改善 KGQA 基礎和評估 LLM 參數化知識的能力，從而為不斷演進的對話式助理領域提供了一個強大的解決方案。

##### **Multimodal Large Language Models for Phishing Webpage Detection and Identification**
2408.05941v1 by Jehyun Lee, Peiyuan Lim, Bryan Hooi, Dinil Mon Divakaran

To address the challenging problem of detecting phishing webpages,
researchers have developed numerous solutions, in particular those based on
machine learning (ML) algorithms. Among these, brand-based phishing detection
that uses models from Computer Vision to detect if a given webpage is imitating
a well-known brand has received widespread attention. However, such models are
costly and difficult to maintain, as they need to be retrained with labeled
dataset that has to be regularly and continuously collected. Besides, they also
need to maintain a good reference list of well-known websites and related
meta-data for effective performance.
  In this work, we take steps to study the efficacy of large language models
(LLMs), in particular the multimodal LLMs, in detecting phishing webpages.
Given that the LLMs are pretrained on a large corpus of data, we aim to make
use of their understanding of different aspects of a webpage (logo, theme,
favicon, etc.) to identify the brand of a given webpage and compare the
identified brand with the domain name in the URL to detect a phishing attack.
We propose a two-phase system employing LLMs in both phases: the first phase
focuses on brand identification, while the second verifies the domain. We carry
out comprehensive evaluations on a newly collected dataset. Our experiments
show that the LLM-based system achieves a high detection rate at high
precision; importantly, it also provides interpretable evidence for the
decisions. Our system also performs significantly better than a
state-of-the-art brand-based phishing detection system while demonstrating
robustness against two known adversarial attacks.

摘要：<paragraph>為了解決偵測網路釣魚網頁的難題，研究人員已開發出許多解決方案，特別是那些基於機器學習 (ML) 演算法的解決方案。在這些解決方案中，基於品牌的網路釣魚偵測使用電腦視覺模型來偵測給定的網頁是否模仿知名品牌，已受到廣泛關注。然而，此類模型的維護成本高且困難，因為它們需要使用定期且持續收集的標記資料集進行重新訓練。此外，它們還需要維護一個良好的知名網站參考清單和相關元資料，才能發揮良好的效能。
在這項工作中，我們採取步驟來研究大型語言模型 (LLM) 的效能，特別是多模態 LLM，在偵測網路釣魚網頁方面的效能。由於 LLM 是在大型資料語料庫上進行預訓練，因此我們旨在利用它們對網頁不同面向（例如標誌、主題、網站小圖示等）的理解，來識別給定網頁的品牌，並將識別出的品牌與 URL 中的網域名稱進行比較，以偵測網路釣魚攻擊。我們提出一個在兩個階段都使用 LLM 的兩階段系統：第一階段專注於品牌識別，而第二階段則驗證網域名稱。我們對新收集的資料集進行全面評估。我們的實驗顯示，基於 LLM 的系統在高準確度下達到了很高的偵測率；重要的是，它還為決策提供了可解釋的證據。我們的系統也比現有的基於品牌的網路釣魚偵測系統執行得更好，同時證明了對兩種已知的對抗性攻擊具有穩健性。</paragraph>

##### **Spb3DTracker: A Robust LiDAR-Based Person Tracker for Noisy Environment**
2408.05940v2 by Eunsoo Im, Changhyun Jee, Jung Kwon Lee

Person detection and tracking (PDT) has seen significant advancements with 2D
camera-based systems in the autonomous vehicle field, leading to widespread
adoption of these algorithms. However, growing privacy concerns have recently
emerged as a major issue, prompting a shift towards LiDAR-based PDT as a viable
alternative. Within this domain, "Tracking-by-Detection" (TBD) has become a
prominent methodology. Despite its effectiveness, LiDAR-based PDT has not yet
achieved the same level of performance as camera-based PDT. This paper examines
key components of the LiDAR-based PDT framework, including detection
post-processing, data association, motion modeling, and lifecycle management.
Building upon these insights, we introduce SpbTrack, a robust person tracker
designed for diverse environments. Our method achieves superior performance on
noisy datasets and state-of-the-art results on KITTI Dataset benchmarks and
custom office indoor dataset among LiDAR-based trackers.

摘要：行人偵測與追蹤 (PDT) 已在自駕車領域中，透過 2D 攝影機為基礎的系統取得顯著的進展，並廣泛採用這些演算法。然而，日益增長的隱私問題最近已成為一個主要議題，促使轉向以 LiDAR 為基礎的 PDT 作為可行的替代方案。在這個領域中，「追蹤偵測」(TBD) 已成為一個顯著的方法。儘管其有效性，但以 LiDAR 為基礎的 PDT 尚未達到與以攝影機為基礎的 PDT 相同的效能。本文探討以 LiDAR 為基礎的 PDT 架構之關鍵組成，包括偵測後處理、資料關聯、動作建模和生命週期管理。根據這些見解，我們引進 SpbTrack，一個強健的行人追蹤器，專為多樣化的環境而設計。我們的技術在有雜訊的資料集上達成卓越的效能，並在 LiDAR 為基礎的追蹤器中，於 KITTI 資料集基準和自訂辦公室室內資料集上達成最先進的結果。

##### **Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models**
2408.05933v1 by Fei Liu, Zejun Kang, Xing Han

With the growing demand for offline PDF chatbots in automotive industrial
production environments, optimizing the deployment of large language models
(LLMs) in local, low-performance settings has become increasingly important.
This study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques
for processing complex automotive industry documents using locally deployed
Ollama models. Based on the Langchain framework, we propose a multi-dimensional
optimization approach for Ollama's local RAG implementation. Our method
addresses key challenges in automotive document processing, including
multi-column layouts and technical specifications. We introduce improvements in
PDF processing, retrieval mechanisms, and context compression, tailored to the
unique characteristics of automotive industry documents. Additionally, we
design custom classes supporting embedding pipelines and an agent supporting
self-RAG based on LangGraph best practices. To evaluate our approach, we
constructed a proprietary dataset comprising typical automotive industry
documents, including technical reports and corporate regulations. We compared
our optimized RAG model and self-RAG agent against a naive RAG baseline across
three datasets: our automotive industry dataset, QReCC, and CoQA. Results
demonstrate significant improvements in context precision, context recall,
answer relevancy, and faithfulness, with particularly notable performance on
the automotive industry dataset. Our optimization scheme provides an effective
solution for deploying local RAG systems in the automotive sector, addressing
the specific needs of PDF chatbots in industrial production environments. This
research has important implications for advancing information processing and
intelligent production in the automotive industry.

摘要：<paragraph>隨著汽車產業生產環境中對離線 PDF 聊天機器人的需求不斷增長，在當地低效能設定中最佳化大型語言模型 (LLM) 的部署變得越來越重要。本研究專注於增強檢索增強生成 (RAG) 技術，以使用當地部署的 Ollama 模型處理複雜的汽車產業文件。我們根據 Langchain 架構，提出 Ollama 的當地 RAG 實作的多維度最佳化方法。我們的技術解決了汽車文件處理中的主要挑戰，包括多欄位配置和技術規格。我們針對汽車產業文件的獨特特性，引進了 PDF 處理、檢索機制和脈絡壓縮的改良。此外，我們設計了支援嵌入管線和支援基於 LangGraph 最佳實務的自 RAG 的自訂類別。為了評估我們的技術，我們建構了一個專有的資料集，其中包含典型的汽車產業文件，包括技術報告和公司法規。我們在三個資料集上比較了我們最佳化的 RAG 模型和自 RAG 代理與樸素的 RAG 基準：我們的汽車產業資料集、QReCC 和 CoQA。結果顯示在脈絡精確度、脈絡召回率、答案相關性和忠實度上都有顯著的改善，尤其在汽車產業資料集上表現出色。我們的最佳化架構提供了一個在汽車產業中部署當地 RAG 系統的有效解決方案，滿足了工業生產環境中 PDF 聊天機器人的特定需求。本研究對於推進汽車產業中的資訊處理和智慧生產具有重要的意義。</paragraph>

##### **BI-MDRG: Bridging Image History in Multimodal Dialogue Response Generation**
2408.05926v1 by Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Kang Zhang, Yu-Jung Heo, Du-Seong Chang, Chang D. Yoo

Multimodal Dialogue Response Generation (MDRG) is a recently proposed task
where the model needs to generate responses in texts, images, or a blend of
both based on the dialogue context. Due to the lack of a large-scale dataset
specifically for this task and the benefits of leveraging powerful pre-trained
models, previous work relies on the text modality as an intermediary step for
both the image input and output of the model rather than adopting an end-to-end
approach. However, this approach can overlook crucial information about the
image, hindering 1) image-grounded text response and 2) consistency of objects
in the image response. In this paper, we propose BI-MDRG that bridges the
response generation path such that the image history information is utilized
for enhanced relevance of text responses to the image content and the
consistency of objects in sequential image responses. Through extensive
experiments on the multimodal dialogue benchmark dataset, we show that BI-MDRG
can effectively increase the quality of multimodal dialogue. Additionally,
recognizing the gap in benchmark datasets for evaluating the image consistency
in multimodal dialogue, we have created a curated set of 300 dialogues
annotated to track object consistency across conversations.

摘要：多模态对话响应生成 (MDRG) 是一项最近提出的任务，其中模型需要根据对话语境生成文本、图像或两者的混合形式的响应。由于缺乏专门针对此任务的大规模数据集以及利用强大的预训练模型的好处，以前的工作依赖于文本模态作为模型的图像输入和输出的中间步骤，而不是采用端到端的方法。然而，这种方法可能会忽略有关图像的关键信息，从而阻碍 1) 以图像为基础的文本响应和 2) 图像响应中对象的一致性。在本文中，我们提出了 BI-MDRG，它弥合了响应生成路径，以便图像历史信息被用于增强文本响应与图像内容的相关性以及顺序图像响应中对象的一致性。通过对多模态对话基准数据集进行广泛的实验，我们表明 BI-MDRG 可以有效地提高多模态对话的质量。此外，认识到在用于评估多模态对话中图像一致性的基准数据集中的差距，我们创建了一个精选的 300 个对话集，这些对话集经过注释以跟踪对话中的对象一致性。

##### **Adapting a Foundation Model for Space-based Tasks**
2408.05924v1 by Matthew Foutter, Praneet Bhoj, Rohan Sinha, Amine Elhafsi, Somrita Banerjee, Christopher Agia, Justin Kruger, Tommaso Guffanti, Daniele Gammelli, Simone D'Amico, Marco Pavone

Foundation models, e.g., large language models, possess attributes of
intelligence which offer promise to endow a robot with the contextual
understanding necessary to navigate complex, unstructured tasks in the wild. In
the future of space robotics, we see three core challenges which motivate the
use of a foundation model adapted to space-based applications: 1) Scalability
of ground-in-the-loop operations; 2) Generalizing prior knowledge to novel
environments; and 3) Multi-modality in tasks and sensor data. Therefore, as a
first-step towards building a foundation model for space-based applications, we
automatically label the AI4Mars dataset to curate a language annotated dataset
of visual-question-answer tuples. We fine-tune a pretrained LLaVA checkpoint on
this dataset to endow a vision-language model with the ability to perform
spatial reasoning and navigation on Mars' surface. In this work, we demonstrate
that 1) existing vision-language models are deficient visual reasoners in
space-based applications, and 2) fine-tuning a vision-language model on
extraterrestrial data significantly improves the quality of responses even with
a limited training dataset of only a few thousand samples.

摘要：基礎模型（例如大型語言模型）具備智慧屬性，有望賦予機器人在野外執行複雜、非結構化任務所需的脈絡理解力。在太空機器人的未來，我們看到三個核心挑戰，促使我們使用適用於太空應用的基礎模型：1) 閉環地面操作的可擴充性；2) 將先驗知識概括到新環境；3) 任務和感測器資料的多模態。因此，作為建立太空應用基礎模型的第一步，我們自動標記 AI4Mars 資料集，以整理一個帶有視覺問題解答元組的語言註解資料集。我們微調 LLaVA 預訓練檢查點，使用此資料集賦予視覺語言模型在火星表面執行空間推理和導航的能力。在這項工作中，我們證明 1) 現有的視覺語言模型在太空應用中是缺乏視覺推理能力的，以及 2) 在異星資料上微調視覺語言模型，即使只有數千個樣本的有限訓練資料集，也能顯著提升回應品質。

##### **Urban Region Pre-training and Prompting: A Graph-based Approach**
2408.05920v1 by Jiahui Jin, Yifan Song, Dong Kan, Haojia Zhu, Xiangguo Sun, Zhicheng Li, Xigang Sun, Jinghui Zhang

Urban region representation is crucial for various urban downstream tasks.
However, despite the proliferation of methods and their success, acquiring
general urban region knowledge and adapting to different tasks remains
challenging. Previous work often neglects the spatial structures and functional
layouts between entities, limiting their ability to capture transferable
knowledge across regions. Further, these methods struggle to adapt effectively
to specific downstream tasks, as they do not adequately address the unique
features and relationships required for different downstream tasks. In this
paper, we propose a $\textbf{G}$raph-based $\textbf{U}$rban $\textbf{R}$egion
$\textbf{P}$re-training and $\textbf{P}$rompting framework ($\textbf{GURPP}$)
for region representation learning. Specifically, we first construct an urban
region graph that integrates detailed spatial entity data for more effective
urban region representation. Then, we develop a subgraph-centric urban region
pre-training model to capture the heterogeneous and transferable patterns of
interactions among entities. To further enhance the adaptability of these
embeddings to different tasks, we design two graph-based prompting methods to
incorporate explicit/hidden task knowledge. Extensive experiments on various
urban region prediction tasks and different cities demonstrate the superior
performance of our GURPP framework. The implementation is available at this
repository: https://anonymous.4open.science/r/GURPP.

摘要：城市區域表徵對於各種城市下游任務至關重要。
然而，儘管方法激增且成功，但獲取一般城市區域知識並適應不同任務仍然具有挑戰性。先前的研究常常忽略實體之間的空間結構和功能佈局，這限制了它們跨區域擷取可轉移知識的能力。此外，這些方法難以有效適應特定的下游任務，因為它們沒有充分考量不同下游任務所需的獨特特徵和關係。在本文中，我們提出一個基於圖形的城市區域預訓練和提示框架（GURPP），用於區域表徵學習。具體來說，我們首先構建一個城市區域圖形，整合詳細的空間實體資料，以獲得更有效的城市區域表徵。然後，我們開發一個以子圖為中心的城市區域預訓練模型，以擷取實體之間的異質且可轉移的互動模式。為了進一步增強這些嵌入對不同任務的適應性，我們設計了兩種基於圖形的提示方法，以納入明確/隱藏的任務知識。在各種城市區域預測任務和不同城市上的廣泛實驗證明了我們 GURPP 框架的優異效能。實作可在此儲存庫取得：https://anonymous.4open.science/r/GURPP。

##### **Inverse design of Non-parameterized Ventilated Acoustic Resonator via Variational Autoencoder with Acoustic Response-encoded Latent Space**
2408.05917v1 by Min Woo Cho, Seok Hyeon Hwang, Jun-Young Jang, Jin Yeong Song, Sun-kwang Hwang, Kyoung Je Cha, Dong Yong Park, Kyungjun Song, Sang Min Park

Ventilated acoustic resonator(VAR), a type of acoustic metamaterial, emerge
as an alternative for sound attenuation in environments that require
ventilation, owing to its excellent low-frequency attenuation performance and
flexible shape adaptability. However, due to the non-linear acoustic responses
of VARs, the VAR designs are generally obtained within a limited parametrized
design space, and the design relies on the iteration of the numerical
simulation which consumes a considerable amount of computational time and
resources. This paper proposes an acoustic response-encoded variational
autoencoder (AR-VAE), a novel variational autoencoder-based generative design
model for the efficient and accurate inverse design of VAR even with
non-parametrized designs. The AR-VAE matches the high-dimensional acoustic
response with the VAR cross-section image in the dimension-reduced latent
space, which enables the AR-VAE to generate various non-parametrized VAR
cross-section images with the target acoustic response. AR-VAE generates
non-parameterized VARs from target acoustic responses, which show a 25-fold
reduction in mean squared error compared to conventional deep learning-based
parameter searching methods while exhibiting lower average mean squared error
and peak frequency variance. By combining the inverse-designed VARs by AR-VAE,
multi-cavity VAR was devised for broadband and multitarget peak frequency
attenuation. The proposed design method presents a new approach for structural
inverse-design with a high-dimensional non-linear physical response.

摘要：通風式聲學諧振器 (VAR) 是一種聲學超材料，由於其低頻衰減性能優異且形狀適應性強，因此成為需要通風的環境中聲衰減的替代方案。然而，由於 VAR 的非線性聲學響應，VAR 設計通常在有限的參數化設計空間內獲得，且設計依賴於消耗大量計算時間和資源的數值模擬迭代。本文提出了一種聲學響應編碼變分自編碼器 (AR-VAE)，這是一種新穎的基於變分自編碼器的生成設計模型，即使對於非參數化設計，也能高效且準確地進行 VAR 的逆向設計。AR-VAE 在降維潛在空間中將高維聲學響應與 VAR 橫截面圖像匹配，這使得 AR-VAE 能夠生成具有目標聲學響應的各種非參數化 VAR 橫截面圖像。AR-VAE 從目標聲學響應中生成非參數化 VAR，與基於深度學習的傳統參數搜索方法相比，均方誤差降低了 25 倍，同時展現出更低的平均均方誤差和峰值頻率方差。通過結合 AR-VAE 逆向設計的 VAR，設計出多腔體 VAR，用於寬頻和多目標峰值頻率衰減。所提出的設計方法為具有高維非線性物理響應的結構逆向設計提供了一種新方法。

##### **A New Pipeline For Generating Instruction Dataset via RAG and Self Fine-Tuning**
2408.05911v1 by Chih-Wei Song, Yu-Kai Lee, Yin-Te Tsai

With the rapid development of large language models in recent years, there
has been an increasing demand for domain-specific Agents that can cater to the
unique needs of enterprises and organizations. Unlike general models, which
strive for broad coverage, these specialized Agents rely on focused datasets
tailored to their intended applications. This research proposes a pipeline that
leverages the power of LLMs and the Retrieval-Augmented Generation related
framework to construct high-quality instruction datasets for fine-tuning on
specific domains using custom document collections. By ingesting
domain-specific documents, the pipeline generates relevant and contextually
appropriate instructions, thus effectively creating a comprehensive dataset for
fine-tuning LLMs on the target domain. This approach overcomes the limitations
of traditional dataset creation methods, which often rely on manual curation or
web-scraping techniques that may introduce noise and irrelevant data. Notably,
our pipeline offers a dynamic solution that can quickly adapt to updates or
modifications in the domain-specific document collection, eliminating the need
for complete retraining. Additionally, it addresses the challenge of data
scarcity by enabling the generation of instruction datasets from a limited set
of initial documents, rendering it suitable for unpopular or specialized
domains where comprehensive datasets are scarce. As a case study, we apply this
approach to the domain of psychiatry, a field requiring specialized knowledge
and sensitive handling of patient information. The resulting fine-tuned LLM
demonstrates showcases the viability of the proposed approach and underscores
its potential for widespread adoption across various industries and domains
where tailored, accurate, and contextually relevant language models are
indispensable.

摘要：<paragraph>隨著近幾年大型語言模型的快速發展，對能夠滿足企業和組織獨特需求的特定領域代理的需求與日俱增。這些專用代理與追求廣泛涵蓋範圍的通用模型不同，它們依賴於針對其預期應用程式量身打造的聚焦式資料集。本研究提出了一個管道，它利用 LLM 的強大功能和檢索增強生成相關框架，使用自訂文件集合為特定領域的微調構建高品質的指令資料集。透過擷取特定領域的文件，管道會產生相關且在脈絡上適當的指令，從而有效地為目標領域上的 LLM 微調建立一個全面的資料集。這種方法克服了傳統資料集建立方法的限制，這些方法通常依賴於人工整理或網路擷取技術，而這些技術可能會引入雜訊和不相關的資料。值得注意的是，我們的管道提供了一個動態的解決方案，可以快速適應特定領域文件集合中的更新或修改，從而無需進行完整的重新訓練。此外，它透過從一組有限的初始文件產生指令資料集來解決資料稀缺的挑戰，使其適用於全面的資料集稀缺的非熱門或專業領域。作為一個案例研究，我們將此方法應用於精神病學領域，這是一個需要專業知識和敏感處理患者資訊的領域。微調後的 LLM 證明了所提出方法的可行性，並強調了其在各種產業和領域中廣泛採用的潛力，在這些領域中，量身打造、準確且與脈絡相關的語言模型是不可或缺的。</paragraph>

##### **AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising**
2408.05906v1 by Peinan Zhang, Yusuke Sakai, Masato Mita, Hiroki Ouchi, Taro Watanabe

With the increase in the more fluent ad texts automatically created by
natural language generation technology, it is in the high demand to verify the
quality of these creatives in a real-world setting. We propose AdTEC, the first
public benchmark to evaluate ad texts in multiple aspects from the perspective
of practical advertising operations. Our contributions are: (i) Defining five
tasks for evaluating the quality of ad texts and building a dataset based on
the actual operational experience of advertising agencies, which is typically
kept in-house. (ii) Validating the performance of existing pre-trained language
models (PLMs) and human evaluators on the dataset. (iii) Analyzing the
characteristics and providing challenges of the benchmark. The results show
that while PLMs have already reached the practical usage level in several
tasks, human still outperforms in certain domains, implying that there is
significant room for improvement in such area.

摘要：隨著自然語言生成技術自動建立的廣告文字越來越流暢，在現實環境中驗證這些素材的品質變得極為重要。我們提出 AdTEC，這是第一個從實務廣告操作觀點評估廣告文字多面向品質的公開基準。我們的貢獻包括：(i) 定義評估廣告文字品質的五項任務，並根據廣告代理商的實際操作經驗建立資料集，這些經驗通常只限內部使用。(ii) 驗證現有預先訓練語言模型 (PLM) 和人類評估者在資料集上的表現。(iii) 分析基準的特徵並提出挑戰。結果顯示，儘管 PLM 已在數項任務中達到實務使用等級，人類在特定領域仍表現得較好，這表示在這些領域仍有顯著的改進空間。

##### **Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts**
2408.05905v2 by Peng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Peng Wang, Yanning Zhang

Current weakly supervised video anomaly detection (WSVAD) task aims to
achieve frame-level anomalous event detection with only coarse video-level
annotations available. Existing works typically involve extracting global
features from full-resolution video frames and training frame-level classifiers
to detect anomalies in the temporal dimension. However, most anomalous events
tend to occur in localized spatial regions rather than the entire video frames,
which implies existing frame-level feature based works may be misled by the
dominant background information and lack the interpretation of the detected
anomalies. To address this dilemma, this paper introduces a novel method called
STPrompt that learns spatio-temporal prompt embeddings for weakly supervised
video anomaly detection and localization (WSVADL) based on pre-trained
vision-language models (VLMs). Our proposed method employs a two-stream network
structure, with one stream focusing on the temporal dimension and the other
primarily on the spatial dimension. By leveraging the learned knowledge from
pre-trained VLMs and incorporating natural motion priors from raw videos, our
model learns prompt embeddings that are aligned with spatio-temporal regions of
videos (e.g., patches of individual frames) for identify specific local regions
of anomalies, enabling accurate video anomaly detection while mitigating the
influence of background information. Without relying on detailed
spatio-temporal annotations or auxiliary object detection/tracking, our method
achieves state-of-the-art performance on three public benchmarks for the WSVADL
task.

摘要：當前的弱監督影片異常偵測 (WSVAD) 任務旨在僅使用粗略的影片層級註解來達成影格層級異常事件偵測。現有的作品通常涉及從全解析度影片影格中萃取全域特徵，並訓練影格層級分類器來偵測時間維度中的異常。然而，大多數異常事件往往發生在局部空間區域，而非整個影片影格，這表示現有的基於影格層級特徵的作品可能會受到主要的背景資訊誤導，且缺乏對偵測到的異常進行詮釋。為了解決這個困境，本文介紹一種稱為 STPrompt 的新方法，它會學習時空提示嵌入，以進行基於預訓練視覺語言模型 (VLM) 的弱監督影片異常偵測與定位 (WSVADL)。我們提出的方法採用雙串流網路結構，其中一個串流專注於時間維度，而另一個主要專注於空間維度。透過利用預訓練 VLM 中學習到的知識，並將原始影片中的自然動作先驗納入，我們的模型會學習與影片的時空區域（例如，個別影格的區塊）對齊的提示嵌入，以識別異常的特定區域，進而進行精確的影片異常偵測，同時減輕背景資訊的影響。在不依賴詳細的時空註解或輔助物件偵測/追蹤的情況下，我們的模型在三個 WSVADL 任務的公開基準上達到了最先進的效能。

##### **Quantum Gradient Class Activation Map for Model Interpretability**
2408.05899v1 by Hsin-Yi Lin, Huan-Hsin Tseng, Samuel Yen-Chi Chen, Shinjae Yoo

Quantum machine learning (QML) has recently made significant advancements in
various topics. Despite the successes, the safety and interpretability of QML
applications have not been thoroughly investigated. This work proposes using
Variational Quantum Circuits (VQCs) for activation mapping to enhance model
transparency, introducing the Quantum Gradient Class Activation Map
(QGrad-CAM). This hybrid quantum-classical computing framework leverages both
quantum and classical strengths and gives access to the derivation of an
explicit formula of feature map importance. Experimental results demonstrate
significant, fine-grained, class-discriminative visual explanations generated
across both image and speech datasets.

摘要：量子機器學習（QML）最近在各種主題上取得了顯著進展。儘管取得了成功，但 QML 應用程式的安全性與可解釋性尚未經過徹底調查。本研究建議使用變分量子電路（VQC）進行激活對應，以增強模型透明度，並引入了量子梯度類激活圖（QGrad-CAM）。這種混合量子經典計算架構同時利用了量子和經典的優勢，並提供了特徵映射重要性的明確公式推導。實驗結果證明了在影像和語音資料集上產生的顯著、細緻、具有類別區別性的視覺說明。

##### **GlyphPattern: An Abstract Pattern Recognition for Vision-Language Models**
2408.05894v1 by Zixuan Wu, Yoolim Kim, Carolyn Jane Anderson

Vision-Language Models (VLMs) building upon the foundation of powerful large
language models have made rapid progress in reasoning across visual and textual
data. While VLMs perform well on vision tasks that they are trained on, our
results highlight key challenges in abstract pattern recognition. We present
GlyphPattern, a 954 item dataset that pairs 318 human-written descriptions of
visual patterns from 40 writing systems with three visual presentation styles.
  GlyphPattern evaluates abstract pattern recognition in VLMs, requiring models
to understand and judge natural language descriptions of visual patterns.
GlyphPattern patterns are drawn from a large-scale cognitive science
investigation of human writing systems; as a result, they are rich in spatial
reference and compositionality. Our experiments show that GlyphPattern is
challenging for state-of-the-art VLMs (GPT-4o achieves only 55% accuracy), with
marginal gains from few-shot prompting. Our detailed error analysis reveals
challenges at multiple levels, including visual processing, natural language
understanding, and pattern generalization.

摘要：視覺語言模型 (VLM) 建構於強大的大型語言模型的基礎之上，在視覺和文字資料的推理方面取得了快速的進展。儘管 VLM 在訓練的視覺任務上表現良好，但我們的結果突顯了抽象模式識別中的關鍵挑戰。我們提出了 GlyphPattern，一個 954 項的資料集，將 40 種書寫系統中 318 個人類編寫的視覺模式描述與三種視覺呈現樣式配對。GlyphPattern 評估 VLM 中的抽象模式識別，要求模型理解和判斷視覺模式的自然語言描述。GlyphPattern 模式取自於大規模認知科學對人類書寫系統的研究；因此，它們富含空間參考和組合性。我們的實驗表明，GlyphPattern 對最先進的 VLM 來說具有挑戰性（GPT-4o 只達到了 55% 的準確度），從少量提示中獲得的收益很小。我們詳細的錯誤分析揭示了多個層面的挑戰，包括視覺處理、自然語言理解和模式概括。

##### **Polyp SAM 2: Advancing Zero shot Polyp Segmentation in Colorectal Cancer Detection**
2408.05892v1 by Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi

Polyp segmentation plays a crucial role in the early detection and diagnosis
of colorectal cancer. However, obtaining accurate segmentations often requires
labor-intensive annotations and specialized models. Recently, Meta AI Research
released a general Segment Anything Model 2 (SAM 2), which has demonstrated
promising performance in several segmentation tasks. In this work, we evaluate
the performance of SAM 2 in segmenting polyps under various prompted settings.
We hope this report will provide insights to advance the field of polyp
segmentation and promote more interesting work in the future. This project is
publicly available at https://github.com/ sajjad-sh33/Polyp-SAM-2.

摘要：息肉分割在早期檢測和診斷大腸癌中扮演至關重要的角色。然而，取得精準的分割通常需要大量標註和專業模型。最近，Meta AI Research 發布了一款通用的 Segment Anything Model 2 (SAM 2)，在多項分割任務中展現出令人滿意的表現。在本文中，我們評估了 SAM 2 在不同提示設定下分割息肉的表現。我們希望這份報告能提供見解，以促進息肉分割領域的進步，並在未來推動更多有趣的研究。此專案已公開發布於 https://github.com/sajjad-sh33/Polyp-SAM-2。

##### **Creating Arabic LLM Prompts at Scale**
2408.05882v1 by Abdelrahman El-Sheikh, Ahmed Elmogtaba, Kareem Darwish, Muhammad Elmallah, Ashraf Elneima, Hassan Sawaf

The debut of chatGPT and BARD has popularized instruction following text
generation using LLMs, where a user can interrogate an LLM using natural
language requests and obtain natural language answers that matches their
requests. Training LLMs to respond in this manner requires a large number of
worked out examples of user requests (aka prompts) with corresponding gold
responses. In this paper, we introduce two methods for creating such prompts
for Arabic cheaply and quickly. The first methods entails automatically
translating existing prompt datasets from English, such as PromptSource and
Super-NaturalInstructions, and then using machine translation quality
estimation to retain high quality translations only. The second method involves
creating natural language prompts on top of existing Arabic NLP datasets. Using
these two methods we were able to create more than 67.4 million Arabic prompts
that cover a variety of tasks including summarization, headline generation,
grammar checking, open/closed question answering, creative writing, etc. We
show that fine tuning an open 7 billion parameter large language model, namely
base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter
instruction tuned model, namely Llama3 70B, in handling Arabic prompts.

摘要：ChatGPT 和 BARD 的首次亮相普及了使用 LLM 的指令遵循文本生成，其中用户可以使用自然语言请求询问 LLM，并获得与他们的请求相匹配的自然语言答案。训练 LLM 以这种方式进行响应需要大量已解决的用户请求（又称提示）示例，并带有相应的黄金响应。在本文中，我们介绍了两种方法，可用于快速且廉价地为阿拉伯语创建此类提示。第一个方法需要自动翻译现有的提示数据集（例如 PromptSource 和 Super-NaturalInstructions）从英语，然后使用机器翻译质量评估仅保留高质量的翻译。第二种方法涉及在现有的阿拉伯语 NLP 数据集之上创建自然语言提示。使用这两种方法，我们能够创建超过 6740 万个阿拉伯语提示，涵盖各种任务，包括摘要、标题生成、语法检查、开放/封闭式问题解答、创意写作等。我们表明，对开放的 70 亿参数大语言模型（即基础 Qwen2 7B）进行微调，使其能够在处理阿拉伯语提示方面优于最先进的 700 亿参数指令调整模型（即 Llama3 70B）。

##### **LLM-Based Robust Product Classification in Commerce and Compliance**
2408.05874v1 by Sina Gholamian, Gianfranco Romani, Bartosz Rudnikowicz, Laura Skylaki

Product classification is a crucial task in international trade, as
compliance regulations are verified and taxes and duties are applied based on
product categories. Manual classification of products is time-consuming and
error-prone, and the sheer volume of products imported and exported renders the
manual process infeasible. Consequently, e-commerce platforms and enterprises
involved in international trade have turned to automatic product classification
using machine learning. However, current approaches do not consider the
real-world challenges associated with product classification, such as very
abbreviated and incomplete product descriptions. In addition, recent
advancements in generative Large Language Models (LLMs) and their reasoning
capabilities are mainly untapped in product classification and e-commerce. In
this research, we explore the real-life challenges of industrial classification
and we propose data perturbations that allow for realistic data simulation.
Furthermore, we employ LLM-based product classification to improve the
robustness of the prediction in presence of incomplete data. Our research shows
that LLMs with in-context learning outperform the supervised approaches in the
clean-data scenario. Additionally, we illustrate that LLMs are significantly
more robust than the supervised approaches when data attacks are present.

摘要：產品分類在國際貿易中是一項重要的任務，因為依據產品類別驗證法規遵循情況並課徵稅金和關稅。手動分類產品耗時且容易出錯，而進出口的產品數量龐大，使得手動處理程序不可行。因此，參與國際貿易的電子商務平台和企業已轉向使用機器學習進行自動產品分類。然而，目前的做法並未考量產品分類相關的現實世界挑戰，例如非常簡短且不完整的產品說明。此外，生成式大型語言模型 (LLM) 及其推理能力的最新進展在產品分類和電子商務中仍未獲得充分利用。在這項研究中，我們探討產業分類的實際挑戰，並提出可進行實際資料模擬的資料擾動。此外，我們採用基於 LLM 的產品分類來提升預測在資料不完整情況下的穩健性。我們的研究顯示，具備情境學習功能的 LLM 在乾淨資料情境中優於監督式方法。此外，我們說明當資料遭到攻擊時，LLM 明顯比監督式方法更穩健。

##### **Defining Boundaries: A Spectrum of Task Feasibility for Large Language Models**
2408.05873v1 by Wenbo Zhang, Zihang Xu, Hengrui Cai

Large language models (LLMs) have shown remarkable performance in various
tasks but often fail to handle queries that exceed their knowledge and
capabilities, leading to incorrect or fabricated responses. This paper
addresses the need for LLMs to recognize and refuse infeasible tasks due to the
required skills surpassing their capabilities. We first systematically
conceptualize infeasible tasks for LLMs, providing formal definitions and
categorizations that cover a spectrum of related hallucinations. We develop and
benchmark a new dataset comprising diverse infeasible and feasible tasks to
test multiple LLMs' abilities on task feasibility. Furthermore, we explore the
potential of training enhancements to increase LLMs' refusal capabilities with
fine-tuning. Experiments validate the effectiveness of our methods, offering
promising directions for refining the operational boundaries of LLMs in real
applications.

摘要：大型語言模型（LLM）在各種任務中展現出卓越的表現，但常常無法處理超出其知識和能力的查詢，導致回應不正確或虛構。本文探討 LLM 必須認知並拒絕無法執行之任務，因為所需的技能超出其能力。我們首先系統性地概念化 LLM 的不可行任務，提供涵蓋一系列相關幻覺的正式定義和分類。我們開發並基準測試一個新的資料集，包含各種不可行和可行的任務，以測試多個 LLM 在任務可行性上的能力。此外，我們探索訓練增強的潛力，以透過微調來提升 LLM 的拒絕能力。實驗驗證了我們方法的有效性，為在實際應用中精進 LLM 的運作界限提供了有希望的方向。

##### **The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms**
2408.05859v1 by Adam Davies, Ashkan Khakzar

Artificial neural networks have long been understood as "black boxes": though
we know their computation graphs and learned parameters, the knowledge encoded
by these weights and functions they perform are not inherently interpretable.
As such, from the early days of deep learning, there have been efforts to
explain these models' behavior and understand them internally; and recently,
mechanistic interpretability (MI) has emerged as a distinct research area
studying the features and implicit algorithms learned by foundation models such
as large language models. In this work, we aim to ground MI in the context of
cognitive science, which has long struggled with analogous questions in
studying and explaining the behavior of "black box" intelligent systems like
the human brain. We leverage several important ideas and developments in the
history of cognitive science to disentangle divergent objectives in MI and
indicate a clear path forward. First, we argue that current methods are ripe to
facilitate a transition in deep learning interpretation echoing the "cognitive
revolution" in 20th-century psychology that shifted the study of human
psychology from pure behaviorism toward mental representations and processing.
Second, we propose a taxonomy mirroring key parallels in computational
neuroscience to describe two broad categories of MI research, semantic
interpretation (what latent representations are learned and used) and
algorithmic interpretation (what operations are performed over representations)
to elucidate their divergent goals and objects of study. Finally, we elaborate
the parallels and distinctions between various approaches in both categories,
analyze the respective strengths and weaknesses of representative works,
clarify underlying assumptions, outline key challenges, and discuss the
possibility of unifying these modes of interpretation under a common framework.

摘要：人工神經網路長期以來都被視為「黑盒子」：儘管我們知道它們的運算圖表和學習參數，但這些權重和它們執行的函數所編碼的知識並非天生就可解釋。因此，從深度學習的早期開始，就有許多人致力於解釋這些模型的行為並在內部理解它們；最近，機制可解釋性 (MI) 已成為一個獨特的的研究領域，探討基礎模型（例如大型語言模型）學習到的特徵和隱式演算法。在這項工作中，我們旨在將 MI 基於認知科學的背景，認知科學長期以來一直在研究和解釋「黑盒子」智能系統（例如人腦）的行為時，努力解決類似的問題。我們利用認知科學史上幾個重要的想法和發展，來解開 MI 中不同的目標，並指出明確的前進道路。首先，我們認為當前的各種方法已準備好促進深度學習解釋的轉變，這呼應了 20 世紀心理學中的「認知革命」，將人類心理學的研究從純粹的行為主義轉向心智表徵和處理。其次，我們提出一個分類法，反映計算神經科學中的關鍵相似之處，以描述 MI 研究的兩個廣泛類別，語義解釋（學習和使用的潛在表徵是什麼）和演算法解釋（在表徵上執行的運算是什麼），以闡明它們不同的目標和研究對象。最後，我們闡述了這兩個類別中各種方法之間的相似之處和區別，分析代表性作品的優缺點，釐清基本假設，概述關鍵挑戰，並討論在一個共同架構下統一這些解釋模式的可能性。

##### **Scaling Virtual World with Delta-Engine**
2408.05842v1 by Hongqiu Wu, Zekai Xu, Tianyang Xu, Jiale Hong, Weiqi Wu, Hai Zhao, Min Zhang, Zhezhi He

In this paper, we focus on \emph{virtual world}, a cyberspace where people
can live in. An ideal virtual world shares great similarity with our real
world. One of the crucial aspects is its evolving nature, reflected by the
individuals' capacity to grow and thereby influence the objective world. Such
dynamics is unpredictable and beyond the reach of existing systems. For this,
we propose a special engine called \emph{Delta-Engine} to drive this virtual
world. $\Delta$ associates the world's evolution to the engine's expansion. A
delta-engine consists of a base engine and a neural proxy. Given an
observation, the proxy generates new code based on the base engine through the
process of \emph{incremental prediction}.
  This paper presents a full-stack introduction to the delta-engine. The key
feature of the delta-engine is its scalability to unknown elements within the
world, Technically, it derives from the prefect co-work of the neural proxy and
the base engine, and the alignment with high-quality data. We an
engine-oriented fine-tuning method that embeds the base engine into the proxy.
We then discuss a human-AI collaborative design process to produce novel and
interesting data efficiently. Eventually, we propose three evaluation
principles to comprehensively assess the performance of a delta engine: naive
evaluation, incremental evaluation, and adversarial evaluation. Our code, data,
and models are open-sourced at \url{https://github.com/gingasan/delta-engine}.

摘要：<paragraph>在本文中，我們專注於「虛擬世界」，一個人們可以居住的網路空間。一個理想的虛擬世界與我們的現實世界有很大的相似性。其中一個關鍵面向是其演化性質，反映在個人成長並因此影響客觀世界的能力。這種動態是不可預測的，而且超出了現有系統的範圍。為此，我們提出一個稱為「Delta-Engine」的特殊引擎來驅動這個虛擬世界。$\Delta$ 將世界的演化與引擎的擴展聯繫起來。Delta-引擎包含一個基礎引擎和一個神經代理。給定一個觀察結果，代理會透過「增量預測」的過程，根據基礎引擎產生新的程式碼。
本文介紹了 delta-engine 的全堆疊介紹。Delta-engine 的關鍵特徵是它對世界中未知元素的可擴充性，技術上來說，它源自神經代理和基礎引擎的完美協作，以及與高品質資料的一致性。我們可以將基礎引擎嵌入到代理中的引擎導向微調方法。然後，我們討論一個人類與 AI 協作的設計過程，以有效產生新穎且有趣的資料。最後，我們提出了三個評估原則，以全面評估 delta 引擎的效能：樸素評估、增量評估和對抗性評估。我們的程式碼、資料和模型在 \url{https://github.com/gingasan/delta-engine} 開源。</paragraph>

##### **Iterative Improvement of an Additively Regularized Topic Model**
2408.05840v1 by Alex Gorbulev, Vasiliy Alekseev, Konstantin Vorontsov

Topic modelling is fundamentally a soft clustering problem (of known objects
-- documents, over unknown clusters -- topics). That is, the task is
incorrectly posed. In particular, the topic models are unstable and incomplete.
All this leads to the fact that the process of finding a good topic model
(repeated hyperparameter selection, model training, and topic quality
assessment) can be particularly long and labor-intensive. We aim to simplify
the process, to make it more deterministic and provable. To this end, we
present a method for iterative training of a topic model. The essence of the
method is that a series of related topic models are trained so that each
subsequent model is at least as good as the previous one, i.e., that it retains
all the good topics found earlier. The connection between the models is
achieved by additive regularization. The result of this iterative training is
the last topic model in the series, which we call the iteratively updated
additively regularized topic model (ITAR). Experiments conducted on several
collections of natural language texts show that the proposed ITAR model
performs better than other popular topic models (LDA, ARTM, BERTopic), its
topics are diverse, and its perplexity (ability to "explain" the underlying
data) is moderate.

摘要：主题模型从根本上来说是一个软聚类问题（已知对象——文档，未知聚类——主题）。也就是说，任务的提出是不正确的。特别是，主题模型不稳定且不完整。所有这些都导致了寻找一个好的主题模型的过程（重复的超参数选择、模型训练和主题质量评估）可能会特别漫长且费力。我们的目标是简化这一过程，使其更具确定性和可证明性。为此，我们提出了一种主题模型迭代训练的方法。该方法的本质是训练一系列相关的主题模型，以便每个后续模型至少与前一个模型一样好，即它保留了之前找到的所有好主题。模型之间的联系是通过附加正则化实现的。这种迭代训练的结果是该系列中的最后一个主题模型，我们称之为迭代更新的附加正则化主题模型（ITAR）。在几个自然语言文本集合上进行的实验表明，所提出的 ITAR 模型比其他流行的主题模型（LDA、ARTM、BERTopic）表现得更好，其主题是多样的，其困惑度（“解释”底层数据的 ability）是适中的。

##### **Real-Time Drowsiness Detection Using Eye Aspect Ratio and Facial Landmark Detection**
2408.05836v1 by Varun Shiva Krishna Rupani, Velpooru Venkata Sai Thushar, Kondadi Tejith

Drowsiness detection is essential for improving safety in areas such as
transportation and workplace health. This study presents a real-time system
designed to detect drowsiness using the Eye Aspect Ratio (EAR) and facial
landmark detection techniques. The system leverages Dlibs pre-trained shape
predictor model to accurately detect and monitor 68 facial landmarks, which are
used to compute the EAR. By establishing a threshold for the EAR, the system
identifies when eyes are closed, indicating potential drowsiness. The process
involves capturing a live video stream, detecting faces in each frame,
extracting eye landmarks, and calculating the EAR to assess alertness. Our
experiments show that the system reliably detects drowsiness with high accuracy
while maintaining low computational demands. This study offers a strong
solution for real-time drowsiness detection, with promising applications in
driver monitoring and workplace safety. Future research will investigate
incorporating additional physiological and contextual data to further enhance
detection accuracy and reliability.

摘要：瞌睡檢測對於改善運輸和職場健康等領域的安全至關重要。本研究提出一個即時系統，旨在使用眼睛長寬比 (EAR) 和面部特徵檢測技術來檢測瞌睡。該系統利用 Dlibs 預先訓練的形狀預測模型來準確檢測和監控 68 個面部特徵，這些特徵用於計算 EAR。通過為 EAR 設定一個閾值，該系統可以識別眼睛閉上的時間，表明可能有瞌睡。這個過程包括擷取即時視訊串流、檢測每一幀中的臉部、提取眼睛特徵，並計算 EAR 來評估警覺性。我們的實驗表明，該系統可以可靠地檢測瞌睡，準確度高，同時保持低運算需求。本研究為即時瞌睡檢測提供了一個強大的解決方案，在駕駛員監控和職場安全方面有廣泛的應用前景。未來的研究將探討整合額外的生理和環境資料，以進一步提高檢測準確度和可靠性。

##### **Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm**
2408.05834v1 by Eli Sennesh, Hao Wu, Tommaso Salvatori

Unexpected stimuli induce "error" or "surprise" signals in the brain. The
theory of predictive coding promises to explain these observations in terms of
Bayesian inference by suggesting that the cortex implements variational
inference in a probabilistic graphical model. However, when applied to machine
learning tasks, this family of algorithms has yet to perform on par with other
variational approaches in high-dimensional, structured inference problems. To
address this, we introduce a novel predictive coding algorithm for structured
generative models, that we call divide-and-conquer predictive coding (DCPC).
DCPC differs from other formulations of predictive coding, as it respects the
correlation structure of the generative model and provably performs
maximum-likelihood updates of model parameters, all without sacrificing
biological plausibility. Empirically, DCPC achieves better numerical
performance than competing algorithms and provides accurate inference in a
number of problems not previously addressed with predictive coding. We provide
an open implementation of DCPC in Pyro on Github.

摘要：意外刺激會誘發大腦中的「錯誤」或「驚喜」信號。預測編碼理論承諾根據貝氏推論解釋這些觀察，建議皮質在大機率圖形模型中實施變異推論。然而，當應用於機器學習任務時，這類演算法在高維度、結構化推論問題中尚未與其他變異方法表現得一樣好。為了解決這個問題，我們引入了一種針對結構化生成模型的新穎預測編碼演算法，我們稱之為分而治之預測編碼 (DCPC)。DCPC 與其他預測編碼公式不同，因為它尊重生成模型的關聯結構，並可證明執行模型參數的最大似然更新，同時不犧牲生物學上的合理性。根據經驗，DCPC 比競爭演算法實現更好的數值效能，並在許多先前未透過預測編碼處理的問題中提供準確的推論。我們在 Github 上的 Pyro 中提供了 DCPC 的開放實作。

##### **Robust Domain Generalization for Multi-modal Object Recognition**
2408.05831v1 by Yuxin Qiao, Keqin Li, Junhong Lin, Rong Wei, Chufeng Jiang, Yang Luo, Haoyu Yang

In multi-label classification, machine learning encounters the challenge of
domain generalization when handling tasks with distributions differing from the
training data. Existing approaches primarily focus on vision object recognition
and neglect the integration of natural language. Recent advancements in
vision-language pre-training leverage supervision from extensive
visual-language pairs, enabling learning across diverse domains and enhancing
recognition in multi-modal scenarios. However, these approaches face
limitations in loss function utilization, generality across backbones, and
class-aware visual fusion. This paper proposes solutions to these limitations
by inferring the actual loss, broadening evaluations to larger vision-language
backbones, and introducing Mixup-CLIPood, which incorporates a novel mix-up
loss for enhanced class-aware visual fusion. Our method demonstrates superior
performance in domain generalization across multiple datasets.

摘要：在多標籤分類中，機器學習在處理與訓練資料分佈不同的任務時，會遇到領域泛化的挑戰。現有的方法主要專注於視覺物件辨識，而忽略了自然語言的整合。最近視覺語言預訓練的進展，利用了大量的視覺語言對的監督，可以在不同的領域中進行學習，並增強多模式場景中的辨識。然而，這些方法在損失函數的利用、骨幹網路的通用性以及類別感知的視覺融合方面面臨限制。本文透過推論實際損失、將評估擴展到更大的視覺語言骨幹網路，以及引入 Mixup-CLIPood（結合了一種新的混合損失，以增強類別感知的視覺融合），提出了解決這些限制的方案。我們的模型在多個資料集的領域泛化中展現出優異的效能。

##### **Time Makes Space: Emergence of Place Fields in Networks Encoding Temporally Continuous Sensory Experiences**
2408.05798v1 by Zhaoze Wang, Ronald W. Di Tullio, Spencer Rooke, Vijay Balasubramanian

The vertebrate hippocampus is believed to use recurrent connectivity in area
CA3 to support episodic memory recall from partial cues. This brain area also
contains place cells, whose location-selective firing fields implement maps
supporting spatial memory. Here we show that place cells emerge in networks
trained to remember temporally continuous sensory episodes. We model CA3 as a
recurrent autoencoder that recalls and reconstructs sensory experiences from
noisy and partially occluded observations by agents traversing simulated rooms.
The agents move in realistic trajectories modeled from rodents and environments
are modeled as high-dimensional sensory experience maps. Training our
autoencoder to pattern-complete and reconstruct experiences with a constraint
on total activity causes spatially localized firing fields, i.e., place cells,
to emerge in the encoding layer. The emergent place fields reproduce key
aspects of hippocampal phenomenology: a) remapping (maintenance of and
reversion to distinct learned maps in different environments), implemented via
repositioning of experience manifolds in the network's hidden layer, b)
orthogonality of spatial representations in different arenas, c) robust place
field emergence in differently shaped rooms, with single units showing multiple
place fields in large or complex spaces, and d) slow representational drift of
place fields. We argue that these results arise because continuous traversal of
space makes sensory experience temporally continuous. We make testable
predictions: a) rapidly changing sensory context will disrupt place fields, b)
place fields will form even if recurrent connections are blocked, but reversion
to previously learned representations upon remapping will be abolished, c) the
dimension of temporally smooth experience sets the dimensionality of place
fields, including during virtual navigation of abstract spaces.

摘要：脊椎動物的海馬迴被認為會使用 CA3 區域中的迴路連接來支援根據部分線索回憶情節記憶。這個腦區也包含位置細胞，其位置選擇性放電場會實作支援空間記憶的地圖。我們在此顯示，位置細胞會在經過訓練以記住時間連續感官情節的網路中出現。我們將 CA3 建模為一個迴路自動編碼器，它可以根據在模擬房間中穿梭的代理所做的有雜訊且部分遮蔽的觀察，回憶和重建感官體驗。代理會在根據齧齒動物建模的真實軌跡中移動，而環境則被建模為高維度感官體驗地圖。訓練我們的自動編碼器以模式完成並在總活動量受限的情況下重建體驗，會導致在編碼層中出現空間局部放電場，即位置細胞。出現的位置細胞會複製海馬現象學的主要面向：a) 重新對應（在不同環境中維護和恢復不同的學習地圖），透過在網路的隱藏層中重新定位體驗流形來實作，b) 在不同場景中空間表示的正交性，c) 在不同形狀的房間中穩健的位置場出現，其中單元會在大型或複雜空間中顯示多個位置場，以及 d) 位置場的緩慢表示漂移。我們認為這些結果會出現，是因為空間的連續橫越會讓感官體驗在時間上連續。我們做出可測試的預測：a) 快速變化的感官環境會破壞位置場，b) 即使阻斷迴路連接，位置場仍會形成，但重新對應時恢復到先前學習的表示會被廢除，c) 時間平滑體驗的維度會設定位置場的維度，包括在抽象空間的虛擬導航期間。

##### **A Meta-Engine Framework for Interleaved Task and Motion Planning using Topological Refinements**
2408.05795v1 by Elisa Tosello, Alessandro Valentini, Andrea Micheli

Task And Motion Planning (TAMP) is the problem of finding a solution to an
automated planning problem that includes discrete actions executable by
low-level continuous motions. This field is gaining increasing interest within
the robotics community, as it significantly enhances robot's autonomy in
real-world applications. Many solutions and formulations exist, but no clear
standard representation has emerged. In this paper, we propose a general and
open-source framework for modeling and benchmarking TAMP problems. Moreover, we
introduce an innovative meta-technique to solve TAMP problems involving moving
agents and multiple task-state-dependent obstacles. This approach enables using
any off-the-shelf task planner and motion planner while leveraging a geometric
analysis of the motion planner's search space to prune the task planner's
exploration, enhancing its efficiency. We also show how to specialize this
meta-engine for the case of an incremental SMT-based planner. We demonstrate
the effectiveness of our approach across benchmark problems of increasing
complexity, where robots must navigate environments with movable obstacles.
Finally, we integrate state-of-the-art TAMP algorithms into our framework and
compare their performance with our achievements.

摘要：任務和動作規劃 (TAMP) 是尋找自動化規劃問題的解決方案的問題，其中包括低階連續動作可執行的離散動作。由於這顯著增強了機器人在現實世界應用中的自主性，因此這個領域在機器人社群中越來越受到重視。存在許多解決方案和公式，但尚未出現明確的標準表示。在本文中，我們提出了一個用於建模和基準測試 TAMP 問題的通用且開放原始碼框架。此外，我們引入了一種創新的元技術來解決涉及移動代理和多個任務狀態依賴障礙的 TAMP 問題。這種方法能夠使用任何現成的任務規劃器和動作規劃器，同時利用動作規劃器搜尋空間的幾何分析來修剪任務規劃器的探索，從而提高其效率。我們還展示了如何針對增量式 SMT-based 規劃器的情況對此元引擎進行專業化。我們在越來越複雜的基準問題中展示了我們方法的有效性，在這些問題中，機器人必須在有可移動障礙物的環境中導航。最後，我們將最先進的 TAMP 演算法整合到我們的框架中，並將其效能與我們的成就進行比較。

##### **HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes**
2408.05794v1 by Xuanyu Su, Yansong Li, Diana Inkpen, Nathalie Japkowicz

Amidst the rise of Large Multimodal Models (LMMs) and their widespread
application in generating and interpreting complex content, the risk of
propagating biased and harmful memes remains significant. Current safety
measures often fail to detect subtly integrated hateful content within
``Confounder Memes''. To address this, we introduce \textsc{HateSieve}, a new
framework designed to enhance the detection and segmentation of hateful
elements in memes. \textsc{HateSieve} features a novel Contrastive Meme
Generator that creates semantically paired memes, a customized triplet dataset
for contrastive learning, and an Image-Text Alignment module that produces
context-aware embeddings for accurate meme segmentation. Empirical experiments
on the Hateful Meme Dataset show that \textsc{HateSieve} not only surpasses
existing LMMs in performance with fewer trainable parameters but also offers a
robust mechanism for precisely identifying and isolating hateful content.
\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer
discretion advised.}

摘要：隨著大型多模態模型 (LMM) 的興起及其在生成和詮釋複雜內容中的廣泛應用，傳播有偏見和有害迷因的風險仍然很大。目前的安全性措施通常無法偵測到巧妙整合在「混淆迷因」中的仇恨內容。為了解決這個問題，我們引入了 \textsc{HateSieve}，一個新的架構，旨在加強偵測和區隔迷因中具有仇恨性的元素。\textsc{HateSieve} 具備一個新穎的對比迷因生成器，可建立語義配對的迷因、一個自訂的三元組資料集，用於對比學習，以及一個影像文字對齊模組，可產生具有情境感知的嵌入，以進行精確的迷因區隔。在 Hateful Meme Dataset 上的實證實驗顯示，\textsc{HateSieve} 不僅以較少的可訓練參數超越現有的 LMM，還提供了一個強健的機制來精確識別和隔離仇恨內容。
\textcolor{red}{注意：包含仇恨言論的學術討論；建議觀眾自行斟酌。}

##### **SAGA: A Participant-specific Examination of Story Alternatives and Goal Applicability for a Deeper Understanding of Complex Events**
2408.05793v1 by Sai Vallurupalli, Katrin Erk, Francis Ferraro

Interpreting and assessing goal driven actions is vital to understanding and
reasoning over complex events. It is important to be able to acquire the
knowledge needed for this understanding, though doing so is challenging. We
argue that such knowledge can be elicited through a participant achievement
lens. We analyze a complex event in a narrative according to the intended
achievements of the participants in that narrative, the likely future actions
of the participants, and the likelihood of goal success. We collect 6.3K high
quality goal and action annotations reflecting our proposed participant
achievement lens, with an average weighted Fleiss-Kappa IAA of 80%. Our
collection contains annotated alternate versions of each narrative. These
alternate versions vary minimally from the "original" story, but can license
drastically different inferences. Our findings suggest that while modern large
language models can reflect some of the goal-based knowledge we study, they
find it challenging to fully capture the design and intent behind concerted
actions, even when the model pretraining included the data from which we
extracted the goal knowledge. We show that smaller models fine-tuned on our
dataset can achieve performance surpassing larger models.

摘要：解釋和評估目標驅動行為對於理解和推理複雜事件至關重要。能夠獲得此理解所需的知識很重要，儘管這樣做具有挑戰性。我們認為，此類知識可以透過參與者成就觀點引發出來。我們根據敘事中參與者的預期成就、參與者的未來可能行動和目標成功的可能性，分析敘事中的複雜事件。我們收集了 6.3K 個高品質目標和動作註解，反映了我們提出的參與者成就觀點，平均加權 Fleiss-Kappa IAA 為 80%。我們的集合包含每個敘事的註解備用版本。這些備用版本與「原始」故事的差異很小，但可以授權截然不同的推論。我們的研究結果表明，儘管現代大型語言模型可以反映我們研究的一些基於目標的知識，但它們發現難以完全捕捉協同動作背後的设计和意圖，即使模型預訓練包含我們从中提取目標知識的數據。我們表明，針對我們的數據集進行微調的較小模型可以實現超越較大模型的效能。

##### **Continual Learning of Nonlinear Independent Representations**
2408.05788v1 by Boyang Sun, Ignavier Ng, Guangyi Chen, Yifan Shen, Qirong Ho, Kun Zhang

Identifying the causal relations between interested variables plays a pivotal
role in representation learning as it provides deep insights into the dataset.
Identifiability, as the central theme of this approach, normally hinges on
leveraging data from multiple distributions (intervention, distribution shift,
time series, etc.). Despite the exciting development in this field, a practical
but often overlooked problem is: what if those distribution shifts happen
sequentially? In contrast, any intelligence possesses the capacity to abstract
and refine learned knowledge sequentially -- lifelong learning. In this paper,
with a particular focus on the nonlinear independent component analysis (ICA)
framework, we move one step forward toward the question of enabling models to
learn meaningful (identifiable) representations in a sequential manner, termed
continual causal representation learning. We theoretically demonstrate that
model identifiability progresses from a subspace level to a component-wise
level as the number of distributions increases. Empirically, we show that our
method achieves performance comparable to nonlinear ICA methods trained jointly
on multiple offline distributions and, surprisingly, the incoming new
distribution does not necessarily benefit the identification of all latent
variables.

摘要：辨識感興趣變數之間的因果關係在表徵學習中扮演著關鍵角色，因為它提供了對資料集的深入見解。可辨識性，作為此方法的核心主題，通常取決於利用來自多重分配（介入、分配轉移、時間序列等）的資料。儘管此領域有令人興奮的發展，一個實用但經常被忽視的問題是：如果這些分配轉移依序發生會怎麼樣？相反地，任何智慧都具備抽象和依序精煉學習知識的能力——終身學習。在本文中，我們特別關注非線性獨立成分分析（ICA）架構，朝著讓模型能夠以依序的方式學習有意義（可辨識）表徵的問題邁進一步，稱為連續因果表徵學習。我們在理論上證明了模型可辨識性從子空間層級進展到組件層級，因為分配的數量會增加。根據經驗，我們顯示我們的模型達到了與非線性 ICA 模型在多個離線分配上聯合訓練相當的效能，令人驚訝的是，新進的分配不一定有利於所有潛在變數的辨識。

##### **HiLight: A Hierarchy-aware Light Global Model with Hierarchical Local ConTrastive Learning**
2408.05786v1 by Zhijian Chen, Zhonghua Li, Jianxin Yang, Ye Qi

Hierarchical text classification (HTC) is a special sub-task of multi-label
classification (MLC) whose taxonomy is constructed as a tree and each sample is
assigned with at least one path in the tree. Latest HTC models contain three
modules: a text encoder, a structure encoder and a multi-label classification
head. Specially, the structure encoder is designed to encode the hierarchy of
taxonomy. However, the structure encoder has scale problem. As the taxonomy
size increases, the learnable parameters of recent HTC works grow rapidly.
Recursive regularization is another widely-used method to introduce
hierarchical information but it has collapse problem and generally relaxed by
assigning with a small weight (ie. 1e-6). In this paper, we propose a
Hierarchy-aware Light Global model with Hierarchical local conTrastive learning
(HiLight), a lightweight and efficient global model only consisting of a text
encoder and a multi-label classification head. We propose a new learning task
to introduce the hierarchical information, called Hierarchical Local
Contrastive Learning (HiLCL). Extensive experiments are conducted on two
benchmark datasets to demonstrate the effectiveness of our model.

摘要：階層式文字分類 (HTC) 是多標籤分類 (MLC) 的一個特殊子任務，其分類法建構為一棵樹，且每個範例都至少分配到樹中的某一條路徑。最新的 HTC 模型包含三個模組：文字編碼器、結構編碼器和多標籤分類頭部。特別地，結構編碼器被設計用來編碼分類法的階層。然而，結構編碼器有規模問題。隨著分類法大小的增加，最近 HTC 作品的可學習參數快速增長。遞迴正則化是另一個廣泛使用的引入階層資訊的方法，但它有崩潰問題，且通常透過分配一個小權重 (例如 1e-6) 來放寬。在本文中，我們提出了一個具有階層局部對比學習 (HiLight) 的階層感知輕量級全局模型，這是一個僅由文字編碼器和多標籤分類頭部組成的輕量級且高效的全局模型。我們提出了一個新的學習任務來引入階層資訊，稱為階層局部對比學習 (HiLCL)。我們在兩個基準資料集上進行了廣泛的實驗，以證明我們模型的有效性。

##### **CURLing the Dream: Contrastive Representations for World Modeling in Reinforcement Learning**
2408.05781v1 by Victor Augusto Kich, Jair Augusto Bottega, Raul Steinmetz, Ricardo Bedin Grando, Ayano Yorozu, Akihisa Ohya

In this work, we present Curled-Dreamer, a novel reinforcement learning
algorithm that integrates contrastive learning into the DreamerV3 framework to
enhance performance in visual reinforcement learning tasks. By incorporating
the contrastive loss from the CURL algorithm and a reconstruction loss from
autoencoder, Curled-Dreamer achieves significant improvements in various
DeepMind Control Suite tasks. Our extensive experiments demonstrate that
Curled-Dreamer consistently outperforms state-of-the-art algorithms, achieving
higher mean and median scores across a diverse set of tasks. The results
indicate that the proposed approach not only accelerates learning but also
enhances the robustness of the learned policies. This work highlights the
potential of combining different learning paradigms to achieve superior
performance in reinforcement learning applications.

摘要：在這項工作中，我們提出 Curled-Dreamer，一種新的強化學習演算法，它將對比學習整合到 DreamerV3 架構中，以增強視覺強化學習任務的效能。透過結合 CURL 演算法的對比損失和自動編碼器的重建損失，Curled-Dreamer 在各種 DeepMind Control Suite 任務中取得顯著進步。我們廣泛的實驗證明，Curled-Dreamer 持續優於最先進的演算法，在各種任務中達成更高的平均分和中位數分數。結果表明，所提出的方法不僅加速學習，還增強了學習策略的穩健性。這項工作突顯了結合不同學習範例以在強化學習應用中達成卓越效能的潛力。

##### **Seg-CycleGAN : SAR-to-optical image translation guided by a downstream task**
2408.05777v1 by Hannuo Zhang, Huihui Li, Jiarui Lin, Yujie Zhang, Jianghua Fan, Hang Liu

Optical remote sensing and Synthetic Aperture Radar(SAR) remote sensing are
crucial for earth observation, offering complementary capabilities. While
optical sensors provide high-quality images, they are limited by weather and
lighting conditions. In contrast, SAR sensors can operate effectively under
adverse conditions. This letter proposes a GAN-based SAR-to-optical image
translation method named Seg-CycleGAN, designed to enhance the accuracy of ship
target translation by leveraging semantic information from a pre-trained
semantic segmentation model. Our method utilizes the downstream task of ship
target semantic segmentation to guide the training of image translation
network, improving the quality of output Optical-styled images. The potential
of foundation-model-annotated datasets in SAR-to-optical translation tasks is
revealed. This work suggests broader research and applications for
downstream-task-guided frameworks. The code will be available at
https://github.com/NPULHH/

摘要：光學遙測與合成孔徑雷達(SAR)遙測對於地球觀測至關重要，提供互補的能力。儘管光學感測器提供高品質影像，但受到天氣和光照條件的限制。相比之下，SAR感測器可以在惡劣條件下有效運作。這封信提出一個基於GAN的SAR到光學影像轉換方法，稱為Seg-CycleGAN，旨在透過利用預先訓練的語意分割模型的語意資訊來增強船舶目標轉換的準確性。我們的模型利用船舶目標語意分割的下游任務來引導影像轉換網路的訓練，改善輸出光學樣式影像的品質。揭示了基礎模型註解資料集在SAR到光學轉換任務中的潛力。這項工作建議更廣泛的研究和應用於下游任務引導的框架。程式碼將在https://github.com/NPULHH/提供。

##### **Neurosymbolic Methods for Rule Mining**
2408.05773v1 by Agnieszka Lawrynowicz, Luis Galarraga, Mehwish Alam, Berenice Jaulmes, Vaclav Zeman, Tomas Kliegr

In this chapter, we address the problem of rule mining, beginning with
essential background information, including measures of rule quality. We then
explore various rule mining methodologies, categorized into three groups:
inductive logic programming, path sampling and generalization, and linear
programming. Following this, we delve into neurosymbolic methods, covering
topics such as the integration of deep learning with rules, the use of
embeddings for rule learning, and the application of large language models in
rule learning.

摘要：在本章中，我們將探討規則挖掘的問題，從包括規則品質測量在內的必要背景資訊開始。接著，我們將探討各種規則挖掘方法，將其分類為三組：歸納邏輯程式設計、路徑取樣和概化，以及線性程式設計。在此之後，我們將深入探討神經符號方法，涵蓋主題，例如深度學習與規則的整合、嵌入式規則學習的使用，以及大型語言模型在規則學習中的應用。

##### **An analysis of HOI: using a training-free method with multimodal visual foundation models when only the test set is available, without the training set**
2408.05772v1 by Chaoyi Ai

Human-Object Interaction (HOI) aims to identify the pairs of humans and
objects in images and to recognize their relationships, ultimately forming
$\langle human, object, verb \rangle$ triplets. Under default settings, HOI
performance is nearly saturated, with many studies focusing on long-tail
distribution and zero-shot/few-shot scenarios. Let us consider an intriguing
problem:``What if there is only test dataset without training dataset, using
multimodal visual foundation model in a training-free manner? '' This study
uses two experimental settings: grounding truth and random arbitrary
combinations. We get some interesting conclusion and find that the open
vocabulary capabilities of the multimodal visual foundation model are not yet
fully realized. Additionally, replacing the feature extraction with grounding
DINO further confirms these findings.

摘要：人類-物體互動 (HOI) 旨在識別影像中人類和物體的配對，並辨識它們的關係，最終形成 $\langle 人類，物體，動詞 \rangle$ 三元組。在預設設定下，HOI 的效能已接近飽和，許多研究專注於長尾分佈和零次學習/少次學習場景。讓我們考慮一個有趣的題目：``如果只有測試資料集而沒有訓練資料集，使用多模態視覺基礎模型進行無訓練的訓練，會怎麼樣？'' 本研究使用兩種實驗設定：接地實況和隨機任意組合。我們得到一些有趣的結論，並發現多模態視覺基礎模型的開放式詞彙能力尚未完全實現。此外，用接地 DINO 取代特徵萃取進一步證實了這些發現。

##### **LI-TTA: Language Informed Test-Time Adaptation for Automatic Speech Recognition**
2408.05769v1 by Eunseop Yoon, Hee Suk Yoon, John Harvill, Mark Hasegawa-Johnson, Chang D. Yoo

Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain
shift challenge, wherein the target environment diverges from the original
training environment. A prime exemplification is TTA for Automatic Speech
Recognition (ASR), which enhances model performance by leveraging output
prediction entropy minimization as a self-supervision signal. However, a key
limitation of this self-supervision lies in its primary focus on acoustic
features, with minimal attention to the linguistic properties of the input. To
address this gap, we propose Language Informed Test-Time Adaptation (LI-TTA),
which incorporates linguistic insights during TTA for ASR. LI-TTA integrates
corrections from an external language model to merge linguistic with acoustic
information by minimizing the CTC loss from the correction alongside the
standard TTA loss. With extensive experiments, we show that LI-TTA effectively
improves the performance of TTA for ASR in various distribution shift
situations.

摘要：測試時間適應 (TTA) 已成為領域轉移挑戰的一項關鍵解決方案，其中目標環境偏離了原始訓練環境。一個主要的範例是自動語音辨識 (ASR) 的 TTA，它透過利用輸出預測熵最小化作為自我監督訊號來提升模型效能。然而，這種自我監督的一個主要限制在於它主要關注聲學特徵，而較少關注輸入的語言特性。為了解決這個差距，我們提出語言訊息測試時間適應 (LI-TTA)，它在 ASR 的 TTA 過程中納入了語言洞察。LI-TTA 整合了來自外部語言模型的修正，以最小化修正的 CTC 損失以及標準 TTA 損失，從而將語言訊息與聲學訊息合併。透過廣泛的實驗，我們展示了 LI-TTA 在各種分佈轉移情況下有效提升了 ASR 的 TTA 效能。

##### **Reference-free Hallucination Detection for Large Vision-Language Models**
2408.05767v1 by Qing Li, Chenyang Lyu, Jiahui Geng, Derui Zhu, Maxim Panov, Fakhri Karray

Large vision-language models (LVLMs) have made significant progress in recent
years. While LVLMs exhibit excellent ability in language understanding,
question answering, and conversations of visual inputs, they are prone to
producing hallucinations. While several methods are proposed to evaluate the
hallucinations in LVLMs, most are reference-based and depend on external tools,
which complicates their practical application. To assess the viability of
alternative methods, it is critical to understand whether the reference-free
approaches, which do not rely on any external tools, can efficiently detect
hallucinations. Therefore, we initiate an exploratory study to demonstrate the
effectiveness of different reference-free solutions in detecting hallucinations
in LVLMs. In particular, we conduct an extensive study on three kinds of
techniques: uncertainty-based, consistency-based, and supervised uncertainty
quantification methods on four representative LVLMs across two different tasks.
The empirical results show that the reference-free approaches are capable of
effectively detecting non-factual responses in LVLMs, with the supervised
uncertainty quantification method outperforming the others, achieving the best
performance across different settings.

摘要：大型視覺語言模型 (LVLMs) 近年來取得顯著進展。雖然 LVLMs 在語言理解、問題解答和視覺輸入對話方面表現出色的能力，但它們容易產生幻覺。雖然已經提出多種方法來評估 LVLMs 中的幻覺，但大多數都是基於參考且依賴於外部工具，這使得它們的實際應用變得複雜。為了評估替代方法的可行性，了解不依賴任何外部工具的無參考方法是否能有效檢測幻覺至關重要。因此，我們啟動了一項探索性研究，以證明不同的無參考解決方案在檢測 LVLMs 中的幻覺方面的有效性。特別是，我們對三種類型的技術進行了廣泛的研究：不確定性基準、一致性基準和監督不確定性量化方法，針對四個代表性的 LVLMs 進行了兩個不同的任務。實證結果表明，無參考方法能夠有效檢測 LVLMs 中的非事實回應，其中監督不確定性量化方法優於其他方法，在不同的設置中實現了最佳性能。

##### **VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing**
2408.05758v1 by Chunyu Qiang, Wang Geng, Yi Zhao, Ruibo Fu, Tao Wang, Cheng Gong, Tianrui Wang, Qiuyu Liu, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Hao Che, Longbiao Wang, Jianwu Dang, Jianhua Tao

Deep learning has brought significant improvements to the field of
cross-modal representation learning. For tasks such as text-to-speech (TTS),
voice conversion (VC), and automatic speech recognition (ASR), a cross-modal
fine-grained (frame-level) sequence representation is desired, emphasizing the
semantic content of the text modality while de-emphasizing the paralinguistic
information of the speech modality. We propose a method called "Vector
Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)", which uses the
cross-modal aligned sequence transcoder to bring text and speech into a joint
multimodal space, learning how to connect text and speech at the frame level.
The proposed VQ-CTAP is a paradigm for cross-modal sequence representation
learning, offering a promising solution for fine-grained generation and
recognition tasks in speech processing. The VQ-CTAP can be directly applied to
VC and ASR tasks without fine-tuning or additional structures. We propose a
sequence-aware semantic connector, which connects multiple frozen pre-trained
modules for the TTS task, exhibiting a plug-and-play capability. We design a
stepping optimization strategy to ensure effective model convergence by
gradually injecting and adjusting the influence of various loss components.
Furthermore, we propose a semantic-transfer-wise paralinguistic consistency
loss to enhance representational capabilities, allowing the model to better
generalize to unseen data and capture the nuances of paralinguistic
information. In addition, VQ-CTAP achieves high-compression speech coding at a
rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the
sampling rate. The audio demo is available at
https://qiangchunyu.github.io/VQCTAP/

摘要：深度學習為跨模態表徵學習領域帶來了顯著的進步。對於諸如文字轉語音 (TTS)、語音轉換 (VC) 和自動語音辨識 (ASR) 等任務，需要跨模態細粒度（幀級別）序列表徵，強調文字模態的語義內容，同時淡化語音模態的副語言資訊。我們提出了一種名為「向量量化對比標記-聲學預訓練 (VQ-CTAP)」的方法，它使用跨模態對齊序列轉碼器將文字和語音帶入一個聯合多模態空間，學習如何在幀級別連接文字和語音。所提出的 VQ-CTAP 是跨模態序列表徵學習的典範，為語音處理中的細粒度生成和辨識任務提供了有希望的解決方案。VQ-CTAP 可以直接應用於 VC 和 ASR 任務，而無需微調或額外的結構。我們提出了一個序列感知語義連接器，它連接了多個凍結的預訓練模組以進行 TTS 任務，展現了即插即用的能力。我們設計了一個逐步優化策略，藉由逐漸注入和調整各種損失組成的影響，以確保模型有效收斂。此外，我們提出了一個語義轉移式的副語言一致性損失，以增強表徵能力，讓模型能更好地概括到未見過的資料，並捕捉副語言資訊的細微差別。此外，VQ-CTAP 以 25Hz 的速率從 24kHz 輸入波形中實現高壓縮語音編碼，這是採樣率的 960 倍縮減。音訊示範可在 https://qiangchunyu.github.io/VQCTAP/ 取得

##### **Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation**
2408.05748v1 by Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, Zhiqi Shen

Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative
learning of entity and relation embeddings from distributed Knowledge Graphs
(KGs) across multiple clients, while preserving data privacy. Training FKGE
models with higher dimensions is typically favored due to their potential for
achieving superior performance. However, high-dimensional embeddings present
significant challenges in terms of storage resource and inference speed. Unlike
traditional KG embedding methods, FKGE involves multiple client-server
communication rounds, where communication efficiency is critical. Existing
embedding compression methods for traditional KGs may not be directly
applicable to FKGE as they often require multiple model trainings which
potentially incur substantial communication costs. In this paper, we propose a
light-weight component based on Knowledge Distillation (KD) which is titled
FedKD and tailored specifically for FKGE methods. During client-side local
training, FedKD facilitates the low-dimensional student model to mimic the
score distribution of triples from the high-dimensional teacher model using KL
divergence loss. Unlike traditional KD way, FedKD adaptively learns a
temperature to scale the score of positive triples and separately adjusts the
scores of corresponding negative triples using a predefined temperature,
thereby mitigating teacher over-confidence issue. Furthermore, we dynamically
adjust the weight of KD loss to optimize the training process. Extensive
experiments on three datasets support the effectiveness of FedKD.

摘要：联邦知识图嵌入（FKGE）旨在促进来自多个客户端的分布式知识图（KG）中实体和关系嵌入的协作学习，同时保护数据隐私。通常青睐训练具有更高维度的 FKGE 模型，因为它们具有实现卓越性能的潜力。然而，高维嵌入在存储资源和推理速度方面提出了重大挑战。与传统的 KG 嵌入方法不同，FKGE 涉及多个客户端-服务器通信回合，其中通信效率至关重要。传统的 KG 的现有嵌入压缩方法可能不直接适用于 FKGE，因为它们通常需要多次模型训练，这可能会产生大量的通信成本。在本文中，我们提出了一种基于知识蒸馏（KD）的轻量级组件，称为 FedKD，并专门针对 FKGE 方法量身定制。在客户端本地训练期间，FedKD 促进低维学生模型使用 KL 散度损失来模仿高维教师模型中三元组的分数分布。与传统的 KD 方式不同，FedKD 自适应地学习温度以缩放正三元组的分数，并使用预定义的温度分别调整相应负三元组的分数，从而减轻教师过度自信的问题。此外，我们动态调整 KD 损失的权重以优化训练过程。在三个数据集上的大量实验支持了 FedKD 的有效性。

##### **MTSCI: A Conditional Diffusion Model for Multivariate Time Series Consistent Imputation**
2408.05740v1 by Jianping Zhou, Junhao Li, Guanjie Zheng, Xinbing Wang, Chenghu Zhou

Missing values are prevalent in multivariate time series, compromising the
integrity of analyses and degrading the performance of downstream tasks.
Consequently, research has focused on multivariate time series imputation,
aiming to accurately impute the missing values based on available observations.
A key research question is how to ensure imputation consistency, i.e.,
intra-consistency between observed and imputed values, and inter-consistency
between adjacent windows after imputation. However, previous methods rely
solely on the inductive bias of the imputation targets to guide the learning
process, ignoring imputation consistency and ultimately resulting in poor
performance. Diffusion models, known for their powerful generative abilities,
prefer to generate consistent results based on available observations.
Therefore, we propose a conditional diffusion model for Multivariate Time
Series Consistent Imputation (MTSCI). Specifically, MTSCI employs a contrastive
complementary mask to generate dual views during the forward noising process.
Then, the intra contrastive loss is calculated to ensure intra-consistency
between the imputed and observed values. Meanwhile, MTSCI utilizes a mixup
mechanism to incorporate conditional information from adjacent windows during
the denoising process, facilitating the inter-consistency between imputed
samples. Extensive experiments on multiple real-world datasets demonstrate that
our method achieves the state-of-the-art performance on multivariate time
series imputation task under different missing scenarios. Code is available at
https://github.com/JeremyChou28/MTSCI.

摘要：缺失值在多變量時間序列中很常見，會損害分析的完整性並降低下游任務的效能。因此，研究集中在多變量時間序列插補上，旨在根據可用的觀測值準確插補缺失值。一個關鍵的研究問題是如何確保插補一致性，即觀測值和插補值之間的內部一致性，以及插補後相鄰視窗之間的內部一致性。然而，先前的做法僅依賴插補目標的歸納偏誤來指導學習過程，忽略插補一致性，最終導致效能不佳。擴散模型以其強大的生成能力而聞名，傾向於根據可用的觀測值生成一致的結果。因此，我們提出了一個條件擴散模型，用於多變量時間序列一致插補 (MTSCI)。具體來說，MTSCI 在正向雜訊處理過程中採用對比互補遮罩來生成雙重視圖。然後，計算內部對比損失以確保插補值和觀測值之間的內部一致性。同時，MTSCI 在去雜訊過程中利用混合機制來納入來自相鄰視窗的條件資訊，促進插補樣本之間的內部一致性。在多個真實世界資料集上的大量實驗表明，我們的模型在不同的缺失情境下，在多變量時間序列插補任務上達到了最先進的效能。程式碼可在 https://github.com/JeremyChou28/MTSCI 取得。

##### **Language-Informed Beam Search Decoding for Multilingual Machine Translation**
2408.05738v1 by Yilin Yang, Stefan Lee, Prasad Tadepalli

Beam search decoding is the de-facto method for decoding auto-regressive
Neural Machine Translation (NMT) models, including multilingual NMT where the
target language is specified as an input. However, decoding multilingual NMT
models commonly produces ``off-target'' translations -- yielding translation
outputs not in the intended language. In this paper, we first conduct an error
analysis of off-target translations for a strong multilingual NMT model and
identify how these decodings are produced during beam search. We then propose
Language-informed Beam Search (LiBS), a general decoding algorithm
incorporating an off-the-shelf Language Identification (LiD) model into beam
search decoding to reduce off-target translations. LiBS is an inference-time
procedure that is NMT-model agnostic and does not require any additional
parallel data. Results show that our proposed LiBS algorithm on average
improves +1.1 BLEU and +0.9 BLEU on WMT and OPUS datasets, and reduces
off-target rates from 22.9\% to 7.7\% and 65.8\% to 25.3\% respectively.

摘要：光束搜尋解碼是解碼自迴歸神經機器翻譯 (NMT) 模型的事實方法，包括多語言 NMT，其中目標語言指定為輸入。然而，解碼多語言 NMT 模型通常會產生「離題」翻譯，產生非預期語言的翻譯輸出。在本文中，我們首先對強大的多語言 NMT 模型的離題翻譯進行錯誤分析，並找出這些解碼是如何在光束搜尋期間產生的。然後，我們提出語言引導光束搜尋 (LiBS)，這是一種通用解碼演算法，將現成的語言識別 (LiD) 模型納入光束搜尋解碼，以減少離題翻譯。LiBS 是一種推論時間程序，與 NMT 模型無關，且不需要任何額外的平行資料。結果顯示，我們提出的 LiBS 演算法平均在 WMT 和 OPUS 資料集上分別提升 +1.1 BLEU 和 +0.9 BLEU，並將離題率從 22.9% 降低到 7.7%，以及從 65.8% 降低到 25.3%。

##### **Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking**
2408.05715v1 by Zhi-Cun Lyu, Xin-Ye Li, Zheng Xie, Ming Li

Code generation has been greatly enhanced by the profound advancements in
Large Language Models (LLMs) recently. Nevertheless, such LLM-based code
generation approaches still struggle to generate error-free code in a few tries
when faced with complex problems. To address this, the prevailing strategy is
to sample a huge number of candidate programs, with the hope of any one in them
could work. However, users of code generation systems usually expect to find a
correct program by reviewing or testing only a small number of code candidates.
Otherwise, the system would be unhelpful. In this paper, we propose Top Pass, a
code ranking approach that identifies potential correct solutions from a large
number of candidates. Top Pass directly optimizes the pass@k loss function,
enhancing the quality at the top of the candidate list. This enables the user
to find the correct solution within as few tries as possible. Experimental
results on four benchmarks indicate that our Top Pass method enhances the
usability of code generation models by producing better ranking results,
particularly achieving a 32.9\% relative improvement in pass@1 on CodeContests
when compared to the state-of-the-art ranking method.

摘要：大型語言模型 (LLM) 近期的重大進展大幅提升了程式碼產生。儘管如此，此類基於 LLM 的程式碼產生方法在面對複雜問題時，仍難以在幾次嘗試中產生無錯誤程式碼。為了解決此問題，目前普遍的策略是取樣大量候選程式，期望其中一個能成功。然而，程式碼產生系統的使用者通常預期僅檢閱或測試少數候選程式碼就能找到正確的程式。否則，系統將毫無幫助。在本文中，我們提出 Top Pass，一種從大量候選程式中找出潛在正確解的程式碼排名方法。Top Pass 直接最佳化 pass@k 損失函數，提升候選清單頂端的品質。這使用戶能盡可能在最少嘗試次數內找到正確解。在四個基準上的實驗結果顯示，我們的 Top Pass 方法透過產生更好的排名結果，提升了程式碼產生模型的可用性，特別是在 CodeContests 上，與最先進的排名方法相比，pass@1 相對提升了 32.9%。

##### **TC-KANRecon: High-Quality and Accelerated MRI Reconstruction via Adaptive KAN Mechanisms and Intelligent Feature Scaling**
2408.05705v1 by Ruiquan Ge, Xiao Yu, Yifei Chen, Fan Jia, Shenghao Zhu, Guanyu Zhou, Yiyu Huang, Chenyan Zhang, Dong Zeng, Changmiao Wang, Qiegen Liu, Shanzhou Niu

Magnetic Resonance Imaging (MRI) has become essential in clinical diagnosis
due to its high resolution and multiple contrast mechanisms. However, the
relatively long acquisition time limits its broader application. To address
this issue, this study presents an innovative conditional guided diffusion
model, named as TC-KANRecon, which incorporates the Multi-Free U-KAN (MF-UKAN)
module and a dynamic clipping strategy. TC-KANRecon model aims to accelerate
the MRI reconstruction process through deep learning methods while maintaining
the quality of the reconstructed images. The MF-UKAN module can effectively
balance the tradeoff between image denoising and structure preservation.
Specifically, it presents the multi-head attention mechanisms and scalar
modulation factors, which significantly enhances the model's robustness and
structure preservation capabilities in complex noise environments. Moreover,
the dynamic clipping strategy in TC-KANRecon adjusts the cropping interval
according to the sampling steps, thereby mitigating image detail loss typically
caused by traditional cropping methods and enriching the visual features of the
images. Furthermore, the MC-Model module incorporates full-sampling k-space
information, realizing efficient fusion of conditional information, enhancing
the model's ability to process complex data, and improving the realism and
detail richness of reconstructed images. Experimental results demonstrate that
the proposed method outperforms other MRI reconstruction methods in both
qualitative and quantitative evaluations. Notably, TC-KANRecon method exhibits
excellent reconstruction results when processing high-noise, low-sampling-rate
MRI data. Our source code is available at
https://github.com/lcbkmm/TC-KANRecon.

摘要：磁振造影（MRI）由於其高解析度和多重對比機制，已成為臨床診斷中不可或缺的技術。然而，相對較長的擷取時間限制了其更廣泛的應用。為了解決這個問題，本研究提出了一個創新的條件引導擴散模型，稱為 TC-KANRecon，它結合了多自由 U-KAN（MF-UKAN）模組和一個動態裁剪策略。TC-KANRecon 模型旨在透過深度學習方法加速 MRI 重建過程，同時保持重建影像的品質。MF-UKAN 模組可以有效平衡影像去噪和結構保留之間的取捨。具體來說，它呈現多頭注意力機制和標量調製因子，這顯著增強了模型在複雜噪聲環境中的穩健性和結構保留能力。此外，TC-KANRecon 中的動態裁剪策略根據取樣步驟調整裁剪間隔，從而減輕傳統裁剪方法通常造成的影像細節損失，並豐富影像的視覺特徵。此外，MC-Model 模組結合了全取樣 k 空間資訊，實現條件資訊的有效融合，增強了模型處理複雜資料的能力，並改善了重建影像的真實感和細節豐富度。實驗結果表明，所提出的方法在定性和定量評估中都優於其他 MRI 重建方法。值得注意的是，TC-KANRecon 方法在處理高雜訊、低取樣率 MRI 資料時表現出優異的重建結果。我們的原始程式碼可在 https://github.com/lcbkmm/TC-KANRecon 取得。

##### **A Novel Momentum-Based Deep Learning Techniques for Medical Image Classification and Segmentation**
2408.05692v1 by Koushik Biswas, Ridal Pal, Shaswat Patel, Debesh Jha, Meghana Karri, Amit Reza, Gorkem Durak, Alpay Medetalibeyoglu, Matthew Antalek, Yury Velichko, Daniela Ladner, Amir Borhani, Ulas Bagci

Accurately segmenting different organs from medical images is a critical
prerequisite for computer-assisted diagnosis and intervention planning. This
study proposes a deep learning-based approach for segmenting various organs
from CT and MRI scans and classifying diseases. Our study introduces a novel
technique integrating momentum within residual blocks for enhanced training
dynamics in medical image analysis. We applied our method in two distinct
tasks: segmenting liver, lung, & colon data and classifying abdominal pelvic CT
and MRI scans. The proposed approach has shown promising results, outperforming
state-of-the-art methods on publicly available benchmarking datasets. For
instance, in the lung segmentation dataset, our approach yielded significant
enhancements over the TransNetR model, including a 5.72% increase in dice
score, a 5.04% improvement in mean Intersection over Union (mIoU), an 8.02%
improvement in recall, and a 4.42% improvement in precision. Hence,
incorporating momentum led to state-of-the-art performance in both segmentation
and classification tasks, representing a significant advancement in the field
of medical imaging.

摘要：準確地從醫療影像中分割出不同的器官，是電腦輔助診斷和介入規劃的關鍵先決條件。本研究提出了一種基於深度學習的方法，用於分割 CT 和 MRI 掃描中的各種器官並對疾病進行分類。我們的研究引入了一種新技術，將動量整合到殘差塊中，以增強醫療影像分析中的訓練動態。我們將方法應用於兩個不同的任務：分割肝臟、肺臟和結腸資料，以及對腹部骨盆 CT 和 MRI 掃描進行分類。所提出的方法已顯示出有希望的結果，在公開的基準資料集上優於最先進的方法。例如，在肺部分割資料集中，我們的模型比 TransNetR 模型產生了顯著的提升，包括骰子係數增加了 5.72%，平均聯合交集 (mIoU) 提高了 5.04%，召回率提高了 8.02%，精度提高了 4.42%。因此，結合動量在分割和分類任務中都帶來了最先進的效能，代表了醫療影像領域的重大進展。

##### **SRTFD: Scalable Real-Time Fault Diagnosis through Online Continual Learning**
2408.05681v1 by Dandan Zhao, Karthick Sharma, Hongpeng Yin, Yuxin Qi, Shuhao Zhang

Fault diagnosis (FD) is essential for maintaining operational safety and
minimizing economic losses by detecting system abnormalities. Recently, deep
learning (DL)-driven FD methods have gained prominence, offering significant
improvements in precision and adaptability through the utilization of extensive
datasets and advanced DL models. Modern industrial environments, however,
demand FD methods that can handle new fault types, dynamic conditions,
large-scale data, and provide real-time responses with minimal prior
information. Although online continual learning (OCL) demonstrates potential in
addressing these requirements by enabling DL models to continuously learn from
streaming data, it faces challenges such as data redundancy, imbalance, and
limited labeled data. To overcome these limitations, we propose SRTFD, a
scalable real-time fault diagnosis framework that enhances OCL with three
critical methods: Retrospect Coreset Selection (RCS), which selects the most
relevant data to reduce redundant training and improve efficiency; Global
Balance Technique (GBT), which ensures balanced coreset selection and robust
model performance; and Confidence and Uncertainty-driven Pseudo-label Learning
(CUPL), which updates the model using unlabeled data for continuous adaptation.
Extensive experiments on a real-world dataset and two public simulated datasets
demonstrate SRTFD's effectiveness and potential for providing advanced,
scalable, and precise fault diagnosis in modern industrial systems.

摘要：故障診斷 (FD) 對於維持運作安全和透過偵測系統異常狀況來將經濟損失降至最低至關重要。最近，深度學習 (DL) 驅動的 FD 方法已獲得顯著地位，透過利用廣泛的資料集和先進的 DL 模型，在精準度和適應性方面提供顯著的進步。然而，現代工業環境需要 FD 方法，這些方法能夠處理新的故障類型、動態條件、大規模資料，並在事前資訊最少的情況下提供即時回應。雖然線上持續學習 (OCL) 透過讓 DL 模型能夠持續從串流資料中學習，證明了在滿足這些需求方面具有潛力，但它面臨資料冗餘、失衡和標籤資料有限等挑戰。為了克服這些限制，我們提出了 SRTFD，一個可擴充的即時故障診斷架構，它透過三種關鍵方法增強了 OCL：回顧核心集選擇 (RCS)，它選擇最相關的資料以減少冗餘訓練並提高效率；全域平衡技術 (GBT)，它確保平衡的核心集選擇和強健的模型效能；以及信心和不確定性驅動的偽標籤學習 (CUPL)，它使用未標籤的資料更新模型以進行持續適應。在真實世界資料集和兩個公開模擬資料集上的廣泛實驗證明了 SRTFD 在現代工業系統中提供先進、可擴充且精確故障診斷的有效性和潛力。

##### **Efficient Federated Learning Using Dynamic Update and Adaptive Pruning with Momentum on Shared Server Data**
2408.05678v1 by Ji Liu, Juncheng Jia, Hong Zhang, Yuhui Yun, Leye Wang, Yang Zhou, Huaiyu Dai, Dejing Dou

Despite achieving remarkable performance, Federated Learning (FL) encounters
two important problems, i.e., low training efficiency and limited computational
resources. In this paper, we propose a new FL framework, i.e., FedDUMAP, with
three original contributions, to leverage the shared insensitive data on the
server in addition to the distributed data in edge devices so as to efficiently
train a global model. First, we propose a simple dynamic server update
algorithm, which takes advantage of the shared insensitive data on the server
while dynamically adjusting the update steps on the server in order to speed up
the convergence and improve the accuracy. Second, we propose an adaptive
optimization method with the dynamic server update algorithm to exploit the
global momentum on the server and each local device for superior accuracy.
Third, we develop a layer-adaptive model pruning method to carry out specific
pruning operations, which is adapted to the diverse features of each layer so
as to attain an excellent trade-off between effectiveness and efficiency. Our
proposed FL model, FedDUMAP, combines the three original techniques and has a
significantly better performance compared with baseline approaches in terms of
efficiency (up to 16.9 times faster), accuracy (up to 20.4% higher), and
computational cost (up to 62.6% smaller).

摘要：儘管聯邦學習 (FL) 達到了顯著的效能，但它會遇到兩個重要的問題，即訓練效率低和運算資源有限。在本文中，我們提出了一個新的 FL 架構，即 FedDUMAP，它有三個原創貢獻，除了邊緣裝置中的分佈式資料外，還利用伺服器上的共享不敏感資料，以便有效率地訓練一個全球模型。首先，我們提出了一個簡單的動態伺服器更新演算法，它利用了伺服器上的共享不敏感資料，同時動態調整伺服器上的更新步驟，以加快收斂速度並提高準確度。其次，我們提出了一個具有動態伺服器更新演算法的自適應最佳化方法，以利用伺服器和每個本地裝置上的全局動能，以獲得更高的準確度。第三，我們開發了一種層自適應模型修剪方法來執行具體的修剪操作，這種方法根據每層的不同特徵進行調整，以便在有效性和效率之間取得極佳的平衡。我們提出的 FL 模型 FedDUMAP 結合了這三種原創技術，在效率（快達 16.9 倍）、準確度（高達 20.4%）和運算成本（小達 62.6%）方面，與基準方法相比具有顯著更好的效能。

##### **StealthDiffusion: Towards Evading Diffusion Forensic Detection through Diffusion Model**
2408.05669v1 by Ziyin Zhou, Ke Sun, Zhongxi Chen, Huafeng Kuang, Xiaoshuai Sun, Rongrong Ji

The rapid progress in generative models has given rise to the critical task
of AI-Generated Content Stealth (AIGC-S), which aims to create AI-generated
images that can evade both forensic detectors and human inspection. This task
is crucial for understanding the vulnerabilities of existing detection methods
and developing more robust techniques. However, current adversarial attacks
often introduce visible noise, have poor transferability, and fail to address
spectral differences between AI-generated and genuine images. To address this,
we propose StealthDiffusion, a framework based on stable diffusion that
modifies AI-generated images into high-quality, imperceptible adversarial
examples capable of evading state-of-the-art forensic detectors.
StealthDiffusion comprises two main components: Latent Adversarial
Optimization, which generates adversarial perturbations in the latent space of
stable diffusion, and Control-VAE, a module that reduces spectral differences
between the generated adversarial images and genuine images without affecting
the original diffusion model's generation process. Extensive experiments show
that StealthDiffusion is effective in both white-box and black-box settings,
transforming AI-generated images into high-quality adversarial forgeries with
frequency spectra similar to genuine images. These forgeries are classified as
genuine by advanced forensic classifiers and are difficult for humans to
distinguish.

摘要：生成模型的快速進展催生了 AI 生成的內容隱匿（AIGC-S）這項重要的任務，其目標是創造出既能規避法醫偵測器又能躲過人類檢驗的 AI 生成的圖像。這項任務對於理解現有偵測方法的漏洞並開發更強健的技術至關重要。然而，目前的對抗攻擊經常會引入可見的雜訊、具有差勁的可傳遞性，並且無法處理 AI 生成的圖像與真實圖像之間的頻譜差異。為了解決這個問題，我們提出了 StealthDiffusion，一個基於穩定擴散的框架，它能將 AI 生成的圖像修改成高品質、難以察覺的對抗範例，足以規避最先進的法醫偵測器。StealthDiffusion 包含兩個主要元件：潛在對抗最佳化，它在穩定擴散的潛在空間中產生對抗擾動，以及 Control-VAE，一個模組，它能減少生成的對抗圖像與真實圖像之間的頻譜差異，而不會影響原始擴散模型的生成程序。廣泛的實驗顯示，StealthDiffusion 在白盒和黑盒設定中都非常有效，將 AI 生成的圖像轉換成高品質的對抗偽造品，其頻率譜與真實圖像類似。這些偽造品被先進的法醫分類器歸類為真實的，而且人類也很難區分。

##### **Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites**
2408.05667v1 by Sayak Saha Roy, Shirin Nilizadeh

In this paper, we introduce PhishLang, an open-source, lightweight Large
Language Model (LLM) specifically designed for phishing website detection
through contextual analysis of the website. Unlike traditional heuristic or
machine learning models that rely on static features and struggle to adapt to
new threats and deep learning models that are computationally intensive, our
model utilizes the advanced language processing capabilities of LLMs to learn
granular features that are characteristic of phishing attacks. Furthermore,
PhishLang operates with minimal data preprocessing and offers performance
comparable to leading deep learning tools, while being significantly faster and
less resource-intensive. Over a 3.5-month testing period, PhishLang
successfully identified approximately 26K phishing URLs, many of which were
undetected by popular antiphishing blocklists, thus demonstrating its potential
to aid current detection measures. We also evaluate PhishLang against several
realistic adversarial attacks and develop six patches that make it very robust
against such threats. Furthermore, we integrate PhishLang with GPT-3.5 Turbo to
create \textit{explainable blocklisting} - warnings that provide users with
contextual information about different features that led to a website being
marked as phishing. Finally, we have open-sourced the PhishLang framework and
developed a Chromium-based browser extension and URL scanner website, which
implement explainable warnings for end-users.

摘要：在本文中，我們介紹了 PhishLang，這是一個開源、輕量級的大型語言模型 (LLM)，專門設計用於通過網站的上下文分析來檢測網路釣魚網站。與依賴於靜態特徵且難以適應新威脅的傳統啟發式或機器學習模型，以及計算密集型深度學習模型不同，我們的模型利用了 LLM 的先進語言處理能力來學習網路釣魚攻擊特有的細微特徵。此外，PhishLang 以最少的資料預處理運作，並提供與領先的深度學習工具相當的效能，同時速度顯著加快且資源消耗更少。在 3.5 個月的測試期中，PhishLang 成功識別出大約 26K 個網路釣魚網址，其中許多網址未被熱門的反網路釣魚封鎖清單所偵測到，由此證明了其有助於現有偵測措施的潛力。我們還針對 PhishLang 評估了多種實際的對抗性攻擊，並開發了六個修補程式，使其對此類威脅非常強大。此外，我們將 PhishLang 與 GPT-3.5 Turbo 整合，以建立「可解釋的封鎖清單」——警告，向使用者提供有關導致網站被標記為網路釣魚的不同特徵的上下文資訊。最後，我們開源了 PhishLang 框架，並開發了一個基於 Chromium 的瀏覽器擴充功能和網址掃描器網站，為最終使用者實作可解釋的警告。

##### **Training an NLP Scholar at a Small Liberal Arts College: A Backwards Designed Course Proposal**
2408.05664v1 by Grusha Prasad, Forrest Davis

The rapid growth in natural language processing (NLP) over the last couple
years has generated student interest and excitement in learning more about the
field. In this paper, we present two types of students that NLP courses might
want to train. First, an "NLP engineer" who is able to flexibly design, build
and apply new technologies in NLP for a wide range of tasks. Second, an "NLP
scholar" who is able to pose, refine and answer questions in NLP and how it
relates to the society, while also learning to effectively communicate these
answers to a broader audience. While these two types of skills are not mutually
exclusive -- NLP engineers should be able to think critically, and NLP scholars
should be able to build systems -- we think that courses can differ in the
balance of these skills. As educators at Small Liberal Arts Colleges, the
strengths of our students and our institution favors an approach that is better
suited to train NLP scholars. In this paper we articulate what kinds of skills
an NLP scholar should have, and then adopt a backwards design to propose course
components that can aid the acquisition of these skills.

摘要：在過去幾年中，自然語言處理 (NLP) 的快速發展激發了學生對深入學習該領域的興趣和熱情。在本文中，我們提出 NLP 課程可能希望培養的兩種學生類型。首先，一名「NLP 工程師」能夠靈活地設計、建構和應用 NLP 中的新技術來執行廣泛的任務。其次，一名「NLP 學者」能夠提出、精煉和回答 NLP 中的問題，以及它與社會的關聯，同時也學習有效地將這些答案傳達給更廣泛的受眾。雖然這兩種技能類型並非互斥的——NLP 工程師應該具備批判性思考能力，而 NLP 學者應該能夠建構系統——我們認為課程可以在這些技能的平衡中有所不同。作為小型文理學院的教育工作者，我們學生和機構的優勢有利於採用更適合培訓 NLP 學者的方法。在本文中，我們闡明 NLP 學者應該具備哪些技能，然後採用逆向設計來提出有助於習得這些技能的課程組成部分。

##### **WiDe-analysis: Enabling One-click Content Moderation Analysis on Wikipedia's Articles for Deletion**
2408.05655v1 by Hsuvas Borkakoty, Luis Espinosa-Anke

Content moderation in online platforms is crucial for ensuring activity
therein adheres to existing policies, especially as these platforms grow. NLP
research in this area has typically focused on automating some part of it given
that it is not feasible to monitor all active discussions effectively. Past
works have focused on revealing deletion patterns with like sentiment analysis,
or on developing platform-specific models such as Wikipedia policy or stance
detectors. Unsurprisingly, however, this valuable body of work is rather
scattered, with little to no agreement with regards to e.g., the deletion
discussions corpora used for training or the number of stance labels. Moreover,
while efforts have been made to connect stance with rationales (e.g., to ground
a deletion decision on the relevant policy), there is little explanability work
beyond that. In this paper, we introduce a suite of experiments on Wikipedia
deletion discussions and wide-analyis (Wikipedia Deletion Analysis), a Python
package aimed at providing one click analysis to content moderation
discussions. We release all assets associated with wide-analysis, including
data, models and the Python package, and a HuggingFace space with the goal to
accelerate research on automating content moderation in Wikipedia and beyond.

摘要：線上平台的內容審核對於確保平台上的活動遵守現有政策至關重要，尤其是在這些平台不斷發展的過程中。這方面的自然語言處理研究通常專注於自動化其中一部分，因為有效監控所有活躍討論並不可行。過去的研究集中在揭露刪除模式，例如情緒分析，或開發特定於平台的模型，例如維基百科政策或立場偵測器。然而，毫不奇怪的是，這項有價值的研究成果相當分散，對於例如用於訓練的刪除討論語料庫或立場標籤數量等方面幾乎沒有達成共識。此外，儘管已努力將立場與依據聯繫起來（例如，根據相關政策為刪除決策提供依據），但這方面的解釋性工作很少。在本文中，我們介紹了一系列針對維基百科刪除討論和廣泛分析（維基百科刪除分析）的實驗，這是一個 Python 套件，旨在提供一鍵式分析內容審核討論。我們發布與廣泛分析相關的所有資產，包括資料、模型和 Python 套件，以及一個 HuggingFace 空間，目標是加速自動化維基百科及其他平台內容審核的研究。

##### **Eigen Attention: Attention in Low-Rank Space for KV Cache Compression**
2408.05646v1 by Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy

Large language models (LLMs) represent a groundbreaking advancement in the
domain of natural language processing due to their impressive reasoning
abilities. Recently, there has been considerable interest in increasing the
context lengths for these models to enhance their applicability to complex
tasks. However, at long context lengths and large batch sizes, the key-value
(KV) cache, which stores the attention keys and values, emerges as the new
bottleneck in memory usage during inference. To address this, we propose Eigen
Attention, which performs the attention operation in a low-rank space, thereby
reducing the KV cache memory overhead. Our proposed approach is orthogonal to
existing KV cache compression techniques and can be used synergistically with
them. Through extensive experiments over OPT, MPT, and Llama model families, we
demonstrate that Eigen Attention results in up to 40% reduction in KV cache
sizes and up to 60% reduction in attention operation latency with minimal drop
in performance.

摘要：大型語言模型 (LLM) 由於其令人印象深刻的推理能力，代表了自然語言處理領域的突破性進展。最近，人們相當有興趣增加這些模型的上下文長度，以增強其對複雜任務的適用性。然而，在較長的上下文長度和較大的批次大小下，儲存注意力鍵和值的快取記憶體 (KV) 成為推論期間記憶體使用量的新瓶頸。為了解決這個問題，我們提出了特徵值注意力，它在低秩空間中執行注意力操作，從而減少了 KV 快取記憶體的開銷。我們提出的方法與現有的 KV 快取壓縮技術正交，並且可以與它們協同使用。通過對 OPT、MPT 和 Llama 模型系列進行廣泛的實驗，我們證明特徵值注意力可使 KV 快取大小減少多達 40%，注意力操作延遲減少多達 60%，而效能下降幅度很小。

##### **Federated Smoothing Proximal Gradient for Quantile Regression with Non-Convex Penalties**
2408.05640v2 by Reza Mirzaeifard, Diyako Ghaderyan, Stefan Werner

Distributed sensors in the internet-of-things (IoT) generate vast amounts of
sparse data. Analyzing this high-dimensional data and identifying relevant
predictors pose substantial challenges, especially when data is preferred to
remain on the device where it was collected for reasons such as data integrity,
communication bandwidth, and privacy. This paper introduces a federated
quantile regression algorithm to address these challenges. Quantile regression
provides a more comprehensive view of the relationship between variables than
mean regression models. However, traditional approaches face difficulties when
dealing with nonconvex sparse penalties and the inherent non-smoothness of the
loss function. For this purpose, we propose a federated smoothing proximal
gradient (FSPG) algorithm that integrates a smoothing mechanism with the
proximal gradient framework, thereby enhancing both precision and computational
speed. This integration adeptly handles optimization over a network of devices,
each holding local data samples, making it particularly effective in federated
learning scenarios. The FSPG algorithm ensures steady progress and reliable
convergence in each iteration by maintaining or reducing the value of the
objective function. By leveraging nonconvex penalties, such as the minimax
concave penalty (MCP) and smoothly clipped absolute deviation (SCAD), the
proposed method can identify and preserve key predictors within sparse models.
Comprehensive simulations validate the robust theoretical foundations of the
proposed algorithm and demonstrate improved estimation precision and reliable
convergence.

摘要：物联网 (IoT) 中的分布式传感器会产生大量稀疏数据。分析这些高维数据并识别相关预测变量会带来重大挑战，尤其是在出于数据完整性、通信带宽和隐私等原因，数据更适合保留在收集数据的设备上时。本文介绍了一种联邦分位数回归算法来应对这些挑战。分位数回归比均值回归模型提供了变量之间关系的更全面视图。然而，在处理非凸稀疏惩罚和损失函数的固有非光滑性时，传统方法会遇到困难。为此，我们提出了一种联邦平滑近端梯度 (FSPG) 算法，该算法将平滑机制与近端梯度框架相集成，从而提高了精度和计算速度。这种集成巧妙地处理了网络中设备上的优化，每个设备都持有本地数据样本，使其在联邦学习场景中特别有效。FSPG 算法通过维持或降低目标函数的值，确保在每次迭代中取得稳定的进展和可靠的收敛。通过利用非凸惩罚，例如极小极凹惩罚 (MCP) 和平滑截断绝对偏差 (SCAD)，所提出的方法可以在稀疏模型中识别和保留关键预测变量。综合仿真验证了所提出算法的稳健理论基础，并展示了改进的估计精度和可靠的收敛。

##### **Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion**
2408.05636v1 by Jacob K Christopher, Brian R Bartoldson, Bhavya Kailkhura, Ferdinando Fioretto

Speculative decoding has emerged as a widely adopted method to accelerate
large language model inference without sacrificing the quality of the model
outputs. While this technique has facilitated notable speed improvements by
enabling parallel sequence verification, its efficiency remains inherently
limited by the reliance on incremental token generation in existing draft
models. To overcome this limitation, this paper proposes an adaptation of
speculative decoding which uses discrete diffusion models to generate draft
sequences. This allows parallelization of both the drafting and verification
steps, providing significant speed-ups to the inference process. Our proposed
approach, \textit{Speculative Diffusion Decoding (SpecDiff)}, is validated on
standard language generation benchmarks and empirically demonstrated to provide
a \textbf{up to 8.7x speed-up over standard generation processes and up to 2.5x
speed-up over existing speculative decoding approaches.}

摘要：推論性解碼已成為一種廣泛採用的方法，用於加速大型語言模型推論，同時不犧牲模型輸出的品質。雖然此技術透過啟用並行序列驗證促進了顯著的速度提升，但其效率本質上仍受到現有草稿模型中依賴漸進式代碼產生所限制。為了克服此限制，本文提出推論性解碼的改編，該改編使用離散擴散模型來產生草稿序列。這允許起草和驗證步驟並行化，大幅提升推論程序的速度。我們提出的方法「推論性擴散解碼 (SpecDiff)」已在標準語言產生基準上驗證，並經實證證明可提供「比標準產生程序快上 8.7 倍，比現有推論性解碼方法快上 2.5 倍」的加速效果。

##### **Forecasting Day-Ahead Electricity Prices in the Integrated Single Electricity Market: Addressing Volatility with Comparative Machine Learning Methods**
2408.05628v1 by Ben Harkin, Xueqin Liu

This paper undertakes a comprehensive investigation of electricity price
forecasting methods, focused on the Irish Integrated Single Electricity Market,
particularly on changes during recent periods of high volatility. The primary
objective of this research is to evaluate and compare the performance of
various forecasting models, ranging from traditional machine learning models to
more complex neural networks, as well as the impact of different lengths of
training periods. The performance metrics, mean absolute error, root mean
square error, and relative mean absolute error, are utilized to assess and
compare the accuracy of each model. A comprehensive set of input features was
investigated and selected from data recorded between October 2018 and September
2022. The paper demonstrates that the daily EU Natural Gas price is a more
useful feature for electricity price forecasting in Ireland than the daily
Henry Hub Natural Gas price. This study also shows that the correlation of
features to the day-ahead market price has changed in recent years. The price
of natural gas on the day and the amount of wind energy on the grid that hour
are significantly more important than any other features. More specifically
speaking, the input fuel for electricity has become a more important driver of
the price of it, than the total generation or demand. In addition, it can be
seen that System Non-Synchronous Penetration (SNSP) is highly correlated with
the day-ahead market price, and that renewables are pushing down the price of
electricity.

摘要：本文對電力價格預測方法進行全面調查，重點關注愛爾蘭綜合單一電力市場，特別是近期高波動時期的變化。本研究的主要目的是評估和比較各種預測模型的性能，從傳統機器學習模型到更複雜的神經網路，以及不同長度的培訓週期的影響。性能指標、平均絕對誤差、均方根誤差和相對平均絕對誤差用於評估和比較每個模型的準確性。從 2018 年 10 月至 2022 年 9 月期間記錄的數據中調查並選擇了一組全面的輸入特徵。本文證明，與每日亨利樞紐天然氣價格相比，每日歐盟天然氣價格是愛爾蘭電力價格預測更有用的特徵。這項研究還表明，特徵與隔夜市場價格的相關性近年來發生了變化。當天的天然氣價格和那一小時電網上的風能比任何其他特徵都重要得多。更具體地說，電力的輸入燃料已成為其價格更重要的驅動力，而不是總發電量或需求。此外，可以看出系統非同步滲透 (SNSP) 與隔夜市場價格高度相關，並且可再生能源正在推動電力價格下跌。

##### **UrFound: Towards Universal Retinal Foundation Models via Knowledge-Guided Masked Modeling**
2408.05618v1 by Kai Yu, Yang Zhou, Yang Bai, Zhi Da Soh, Xinxing Xu, Rick Siow Mong Goh, Ching-Yu Cheng, Yong Liu

Retinal foundation models aim to learn generalizable representations from
diverse retinal images, facilitating label-efficient model adaptation across
various ophthalmic tasks. Despite their success, current retinal foundation
models are generally restricted to a single imaging modality, such as Color
Fundus Photography (CFP) or Optical Coherence Tomography (OCT), limiting their
versatility. Moreover, these models may struggle to fully leverage expert
annotations and overlook the valuable domain knowledge essential for
domain-specific representation learning. To overcome these limitations, we
introduce UrFound, a retinal foundation model designed to learn universal
representations from both multimodal retinal images and domain knowledge.
UrFound is equipped with a modality-agnostic image encoder and accepts either
CFP or OCT images as inputs. To integrate domain knowledge into representation
learning, we encode expert annotation in text supervision and propose a
knowledge-guided masked modeling strategy for model pre-training. It involves
reconstructing randomly masked patches of retinal images while predicting
masked text tokens conditioned on the corresponding retinal image. This
approach aligns multimodal images and textual expert annotations within a
unified latent space, facilitating generalizable and domain-specific
representation learning. Experimental results demonstrate that UrFound exhibits
strong generalization ability and data efficiency when adapting to various
tasks in retinal image analysis. By training on ~180k retinal images, UrFound
significantly outperforms the state-of-the-art retinal foundation model trained
on up to 1.6 million unlabelled images across 8 public retinal datasets. Our
code and data are available at https://github.com/yukkai/UrFound.

摘要：視網膜基礎模型旨在從多種視網膜影像中學習可概括的表徵，促進各種眼科任務中標籤有效率的模型適應。儘管它們成功，目前的視網膜基礎模型通常僅限於單一影像型態，例如彩色眼底攝影 (CFP) 或光學相干斷層掃描 (OCT)，限制了它們的多功能性。此外，這些模型可能難以充分利用專家註解，並忽略了對特定領域表徵學習至關重要的寶貴領域知識。為了克服這些限制，我們引入了 UrFound，這是一個視網膜基礎模型，旨在從多模式視網膜影像和領域知識中學習通用表徵。UrFound 配備了與型態無關的影像編碼器，並接受 CFP 或 OCT 影像作為輸入。為了將領域知識整合到表徵學習中，我們在文字監督中編碼專家註解，並提出了一種知識引導的遮罩建模策略，用於模型預訓練。它涉及在預測遮罩文字符號（以對應的視網膜影像為條件）的同時，重建視網膜影像的隨機遮罩區塊。此方法在統一的潛在空間中對齊了多模式影像和文字專家註解，促進了可概括且特定於領域的表徵學習。實驗結果表明，UrFound 在適應視網膜影像分析中的各種任務時，展現出強大的概括能力和資料效率。透過訓練約 18 萬張視網膜影像，UrFound 在 8 個公開視網膜資料集上顯著優於訓練超過 160 萬張未標籤影像的最新視網膜基礎模型。我們的程式碼和資料可在 https://github.com/yukkai/UrFound 取得。

##### **Mitigating Metropolitan Carbon Emissions with Dynamic Eco-driving at Scale**
2408.05609v1 by Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Edgar Sanchez, Catherine Tang, Mark Taylor, Blaine Leonard, Cathy Wu

The sheer scale and diversity of transportation make it a formidable sector
to decarbonize. Here, we consider an emerging opportunity to reduce carbon
emissions: the growing adoption of semi-autonomous vehicles, which can be
programmed to mitigate stop-and-go traffic through intelligent speed commands
and, thus, reduce emissions. But would such dynamic eco-driving move the needle
on climate change? A comprehensive impact analysis has been out of reach due to
the vast array of traffic scenarios and the complexity of vehicle emissions. We
address this challenge with large-scale scenario modeling efforts and by using
multi-task deep reinforcement learning with a carefully designed network
decomposition strategy. We perform an in-depth prospective impact assessment of
dynamic eco-driving at 6,011 signalized intersections across three major US
metropolitan cities, simulating a million traffic scenarios. Overall, we find
that vehicle trajectories optimized for emissions can cut city-wide
intersection carbon emissions by 11-22%, without harming throughput or safety,
and with reasonable assumptions, equivalent to the national emissions of Israel
and Nigeria, respectively. We find that 10% eco-driving adoption yields 25%-50%
of the total reduction, and nearly 70% of the benefits come from 20% of
intersections, suggesting near-term implementation pathways. However, the
composition of this high-impact subset of intersections varies considerably
across different adoption levels, with minimal overlap, calling for careful
strategic planning for eco-driving deployments. Moreover, the impact of
eco-driving, when considered jointly with projections of vehicle
electrification and hybrid vehicle adoption remains significant. More broadly,
this work paves the way for large-scale analysis of traffic externalities, such
as time, safety, and air quality, and the potential impact of solution
strategies.

摘要：<paragraph>運輸業的規模和多樣性使其成為一個難以脫碳的產業。在此，我們考慮一個新興的機會來減少碳排放：半自動車輛的採用日益增加，這些車輛可透過智慧型速度指令來編程以減少走走停停的交通，從而減少排放。但這種動態生態駕駛是否會對氣候變遷產生影響？由於交通狀況眾多且車輛排放複雜，因此無法進行全面的影響分析。我們透過大規模情境建模工作和使用多任務深度強化學習，以及精心設計的網路分解策略來應對這項挑戰。我們對美國三大都會城市 6,011 個有信號的交叉路口進行深入的前瞻性影響評估，模擬一百萬個交通狀況。總體而言，我們發現針對排放量最佳化的車輛軌跡可以減少 11-22% 的城市交叉路口碳排放，而不會損害吞吐量或安全性，且根據合理的假設，分別等於以色列和奈及利亞的國家排放量。我們發現 10% 的生態駕駛採用率會產生總減量的 25%-50%，而近 70% 的好處來自 20% 的交叉路口，這表明了近期的實施途徑。然而，這個高影響交叉路口子集的組成在不同的採用率之間變化很大，重疊性很小，這需要仔細的策略性規劃來進行生態駕駛部署。此外，生態駕駛的影響，在與車輛電氣化和混合動力車輛採用率的預測共同考量時，仍然顯著。更廣泛而言，這項工作為大規模分析交通外部性（例如時間、安全性和空氣品質）以及解決策略的潛在影響鋪平了道路。</paragraph>

##### **Exploring Applications of State Space Models and Advanced Training Techniques in Sequential Recommendations: A Comparative Study on Efficiency and Performance**
2408.05606v1 by Mark Obozov, Makar Baderko, Stepan Kulibaba, Nikolay Kutuzov, Alexander Gasnikov

Recommender systems aim to estimate the dynamically changing user preferences
and sequential dependencies between historical user behaviour and metadata.
Although transformer-based models have proven to be effective in sequential
recommendations, their state growth is proportional to the length of the
sequence that is being processed, which makes them expensive in terms of memory
and inference costs. Our research focused on three promising directions in
sequential recommendations: enhancing speed through the use of State Space
Models (SSM), as they can achieve SOTA results in the sequential
recommendations domain with lower latency, memory, and inference costs, as
proposed by arXiv:2403.03900 improving the quality of recommendations with
Large Language Models (LLMs) via Monolithic Preference Optimization without
Reference Model (ORPO); and implementing adaptive batch- and step-size
algorithms to reduce costs and accelerate training processes.

摘要：推薦系統旨在估計動態變化的使用者偏好，以及歷史使用者行為和元資料之間的順序相關性。
儘管基於 Transformer 的模型已證明在順序推薦中很有效，但它們的狀態增長與正在處理的序列長度成正比，這使得它們在記憶體和推論成本方面非常昂貴。我們的研究專注於順序推薦中的三個有希望的方向：通過使用狀態空間模型 (SSM) 提升速度，因為它們可以以較低的延遲、記憶體和推論成本在順序推薦領域實現 SOTA 結果，如 arXiv:2403.03900 所建議的；透過無參考模型的單一偏好最佳化 (ORPO) 使用大型語言模型 (LLM) 改善推薦品質；以及實作自適應批次和步長演算法，以降低成本並加速訓練流程。

##### **Sequential Representation Learning via Static-Dynamic Conditional Disentanglement**
2408.05599v1 by Mathieu Cyrille Simon, Pascal Frossard, Christophe De Vleeschouwer

This paper explores self-supervised disentangled representation learning
within sequential data, focusing on separating time-independent and
time-varying factors in videos. We propose a new model that breaks the usual
independence assumption between those factors by explicitly accounting for the
causal relationship between the static/dynamic variables and that improves the
model expressivity through additional Normalizing Flows. A formal definition of
the factors is proposed. This formalism leads to the derivation of sufficient
conditions for the ground truth factors to be identifiable, and to the
introduction of a novel theoretically grounded disentanglement constraint that
can be directly and efficiently incorporated into our new framework. The
experiments show that the proposed approach outperforms previous complex
state-of-the-art techniques in scenarios where the dynamics of a scene are
influenced by its content.

摘要：本文探討了序列資料中的自我監督解糾纏表徵學習，專注於分離影片中的時間無關和時間變異因子。我們提出一個新的模型，透過明確考量靜態/動態變數之間的因果關係，打破了這些因子之間通常的獨立假設，並透過額外的正規化流改善模型表達能力。本文提出了這些因子的正式定義。這種形式主義導致推導出基本真相因子可識別的充分條件，並引入一個新的理論基礎解糾纏約束，可以直接有效地整合到我們的新架構中。實驗顯示，在場景動態受其內容影響的情況下，所提出的方法優於先前的複雜最先進技術。

##### **In-Context Exploiter for Extensive-Form Games**
2408.05575v1 by Shuxin Li, Chang Yang, Youzhi Zhang, Pengdeng Li, Xinrun Wang, Xiao Huang, Hau Chan, Bo An

Nash equilibrium (NE) is a widely adopted solution concept in game theory due
to its stability property. However, we observe that the NE strategy might not
always yield the best results, especially against opponents who do not adhere
to NE strategies. Based on this observation, we pose a new game-solving
question: Can we learn a model that can exploit any, even NE, opponent to
maximize their own utility? In this work, we make the first attempt to
investigate this problem through in-context learning. Specifically, we
introduce a novel method, In-Context Exploiter (ICE), to train a single model
that can act as any player in the game and adaptively exploit opponents
entirely by in-context learning. Our ICE algorithm involves generating diverse
opponent strategies, collecting interactive history training data by a
reinforcement learning algorithm, and training a transformer-based agent within
a well-designed curriculum learning framework. Finally, comprehensive
experimental results validate the effectiveness of our ICE algorithm,
showcasing its in-context learning ability to exploit any unknown opponent,
thereby positively answering our initial game-solving question.

摘要：Nash 均衡 (NE) 是博弈论中广泛采用的解概念，因为它具有稳定性。然而，我们观察到 NE 策略可能并不总是产生最佳结果，特别是对于不遵守 NE 策略的对手。基于这一观察，我们提出了一个新的博弈求解问题：我们能否学习一个可以利用任何对手（甚至是 NE 对手）来最大化自身效用的模型？在这项工作中，我们首次尝试通过上下文学习来研究这个问题。具体来说，我们引入了一种新颖的方法，即上下文利用器 (ICE)，以训练一个可以作为游戏中任何玩家的单一模型，并通过上下文学习自适应地利用对手。我们的 ICE 算法涉及生成不同的对手策略，通过强化学习算法收集交互式历史训练数据，并在精心设计的课程学习框架内训练基于 Transformer 的代理。最后，全面的实验结果验证了我们 ICE 算法的有效性，展示了它利用任何未知对手的上下文学习能力，从而对我们最初的博弈求解问题做出了肯定的回答。

