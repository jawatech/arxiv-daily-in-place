{"2410.12513": {"publish_time": "2024-10-16", "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction", "paper_summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.", "paper_summary_zh": "\u81ea\u52d5\u56de\u6b78\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8996\u89ba\u548c\u8a9e\u8a00\u8655\u7406\u7b49\u9818\u57df\u8868\u73fe\u51fa\u8272\u3002\u7136\u800c\uff0c\u7531\u65bc\u901a\u904e\u5806\u758a\u8f49\u63db\u5668\u5c64\u9032\u884c\u9806\u5e8f\u8655\u7406\uff0c\u81ea\u52d5\u56de\u6b78\u89e3\u78bc\u9762\u81e8\u8457\u986f\u8457\u7684\u8a08\u7b97/\u5ef6\u9072\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u79fb\u52d5\u548c\u908a\u7de3\u8a2d\u5099\u7b49\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u3002\u73fe\u6709\u6587\u737b\u4e2d\u65e8\u5728\u901a\u904e\u8df3\u904e\u5c64\u4f86\u6539\u5584\u5ef6\u9072\u7684\u65b9\u6cd5\u6709\u5169\u7a2e\u4e0d\u540c\u7684\u98a8\u683c - 1) \u65e9\u671f\u9000\u51fa 2) \u8207\u8f38\u5165\u7121\u95dc\u7684\u555f\u767c\u5f0f\u65b9\u6cd5\uff0c\u5176\u4e2d\u4ee4\u724c\u5728\u9810\u5148\u78ba\u5b9a\u7684\u5c64\u9000\u51fa\uff0c\u800c\u8207\u8f38\u5165\u5e8f\u5217\u7121\u95dc\u3002\u4e0a\u8ff0\u5169\u7a2e\u7b56\u7565\u90fd\u6709\u5c40\u9650\u6027 - \u524d\u8005\u4e0d\u80fd\u7528\u65bc\u8655\u7406\u73fe\u4ee3\u6846\u67b6\u4e2d\u52a0\u901f\u6240\u9700\u7684 KV \u5feb\u53d6\uff0c\u800c\u5f8c\u8005\u6c92\u6709\u6355\u6349\u5230\u8de8\u4efb\u52d9\u6216\u66f4\u4e00\u822c\u5730\u8de8\u8f38\u5165\u5e8f\u5217\u7684\u5c64\u91cd\u8981\u6027\u7684\u8b8a\u5316\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u5169\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86 FIRST\uff0c\u9019\u662f\u4e00\u7a2e\u901a\u904e\u4f7f\u7528\u7279\u5b9a\u65bc\u5c64\u7684\u8def\u7531\u5668\u81ea\u9069\u61c9\u5730\u70ba\u6bcf\u500b\u8f38\u5165\u5e8f\u5217\u9078\u64c7\u8f49\u63db\u5668\u5c64\u5b50\u96c6\u4f86\u6e1b\u5c11\u63a8\u8ad6\u5ef6\u9072\u7684\u6f14\u7b97\u6cd5 - \u63d0\u793a (\u5728\u9810\u586b\u5145\u968e\u6bb5) \u6c7a\u5b9a\u5728\u89e3\u78bc\u904e\u7a0b\u4e2d\u5c07\u8df3\u904e\u54ea\u4e9b\u5c64\u3002FIRST \u4fdd\u6301\u8207 KV \u5feb\u53d6\u7684\u76f8\u5bb9\u6027\uff0c\u5f9e\u800c\u5728\u4fdd\u8b49\u54c1\u8cea\u7684\u540c\u6642\u5be6\u73fe\u66f4\u5feb\u7684\u63a8\u8ad6\u3002FIRST \u8207\u6a21\u578b\u7121\u95dc\uff0c\u4e26\u4e14\u53ef\u4ee5\u8f15\u9b06\u5730\u5728\u4efb\u4f55\u9810\u5148\u8a13\u7df4\u7684 LLM \u4e0a\u555f\u7528\u3002\u6211\u5011\u9032\u4e00\u6b65\u901a\u904e\u6574\u5408 LoRA \u9069\u914d\u5668\u4f86\u6539\u9032\u5728\u5916\u90e8\u8cc7\u6599\u96c6\u4e0a\u7684\u5fae\u8abf\u6548\u80fd\uff0c\u5728\u4fdd\u6301\u5ef6\u9072\u512a\u52e2\u7684\u540c\u6642\u63d0\u9ad8\u7279\u5b9a\u4efb\u52d9\u7684\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u7684\u505a\u6cd5\u8868\u660e\uff0c\u8f38\u5165\u81ea\u9069\u61c9\u6027\u81f3\u95dc\u91cd\u8981 - \u4e8b\u5be6\u4e0a\uff0c\u4e0d\u540c\u7684\u7279\u5b9a\u4efb\u52d9\u4e2d\u9593\u5c64\u5728\u6839\u64da\u4efb\u52d9\u6f14\u5316\u96b1\u85cf\u8868\u793a\u65b9\u9762\u767c\u63ee\u8457\u81f3\u95dc\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8868\u660e\uff0cFIRST \u5728\u4fdd\u6301\u7af6\u722d\u6548\u80fd\u7684\u540c\u6642\u986f\u8457\u964d\u4f4e\u4e86\u5ef6\u9072 (\u8207\u57fa\u7dda\u76f8\u6bd4)\uff0c\u4f7f\u6211\u5011\u7684\u505a\u6cd5\u6210\u70ba\u5728\u4f4e\u8cc7\u6e90\u74b0\u5883\u4e2d\u90e8\u7f72 LLM \u7684\u6709\u6548\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Akriti Jain et.al.", "authors": "Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal", "id": "2410.12513v1", "paper_url": "http://arxiv.org/abs/2410.12513v1", "repo": "null"}}