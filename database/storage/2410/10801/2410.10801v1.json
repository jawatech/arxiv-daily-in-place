{"2410.10801": {"publish_time": "2024-10-14", "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning", "paper_summary": "Large Language Models (LLMs) have been adopted and deployed worldwide for a\nbroad variety of applications. However, ensuring their safe use remains a\nsignificant challenge. Preference training and safety measures often overfit to\nharms prevalent in Western-centric datasets, and safety protocols frequently\nfail to extend to multilingual settings. In this work, we explore model merging\nin a diverse multi-task setting, combining safety and general-purpose tasks\nwithin a multilingual context. Each language introduces unique and varied\nlearning challenges across tasks. We find that objective-based merging is more\neffective than mixing data, with improvements of up to 8% and 10% in general\nperformance and safety respectively. We also find that language-based merging\nis highly effective -- by merging monolingually fine-tuned models, we achieve a\n4% increase in general performance and 7% reduction in harm across all\nlanguages on top of the data mixtures method using the same available data.\nOverall, our comprehensive study of merging approaches provides a useful\nframework for building strong and safe multilingual models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5168\u7403\u7bc4\u570d\u5167\u5ee3\u6cdb\u63a1\u7528\u548c\u90e8\u7f72\uff0c\u9069\u7528\u65bc\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u3002\u7136\u800c\uff0c\u78ba\u4fdd\u5176\u5b89\u5168\u4f7f\u7528\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u504f\u597d\u8a13\u7df4\u548c\u5b89\u5168\u63aa\u65bd\u7d93\u5e38\u904e\u5ea6\u7b26\u5408\u4ee5\u897f\u65b9\u70ba\u4e2d\u5fc3\u7684\u8cc7\u6599\u96c6\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5371\u5bb3\uff0c\u800c\u5b89\u5168\u5354\u8b70\u901a\u5e38\u7121\u6cd5\u64f4\u5c55\u5230\u591a\u8a9e\u8a00\u8a2d\u5b9a\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5728\u591a\u5143\u5316\u7684\u591a\u4efb\u52d9\u8a2d\u5b9a\u4e2d\u63a2\u7d22\u6a21\u578b\u5408\u4f75\uff0c\u5728\u591a\u8a9e\u8a00\u74b0\u5883\u4e2d\u7d50\u5408\u5b89\u5168\u6027\u548c\u901a\u7528\u4efb\u52d9\u3002\u6bcf\u7a2e\u8a9e\u8a00\u5728\u5404\u9805\u4efb\u52d9\u4e2d\u90fd\u6703\u5e36\u4f86\u7368\u7279\u4e14\u591a\u6a23\u7684\u5b78\u7fd2\u6311\u6230\u3002\u6211\u5011\u767c\u73fe\u57fa\u65bc\u76ee\u6a19\u7684\u5408\u4f75\u6bd4\u6df7\u5408\u8cc7\u6599\u66f4\u6709\u6548\uff0c\u5728\u4e00\u822c\u6548\u80fd\u548c\u5b89\u5168\u6027\u7684\u6539\u9032\u5206\u5225\u9ad8\u9054 8% \u548c 10%\u3002\u6211\u5011\u9084\u767c\u73fe\u57fa\u65bc\u8a9e\u8a00\u7684\u5408\u4f75\u975e\u5e38\u6709\u6548\u2014\u2014\u900f\u904e\u5408\u4f75\u55ae\u8a9e\u8a00\u5fae\u8abf\u6a21\u578b\uff0c\u6211\u5011\u5728\u4e00\u822c\u6548\u80fd\u4e0a\u63d0\u9ad8\u4e86 4%\uff0c\u5728\u6240\u6709\u8a9e\u8a00\u4e2d\u6e1b\u5c11\u4e86 7% \u7684\u5371\u5bb3\uff0c\u9ad8\u65bc\u4f7f\u7528\u76f8\u540c\u53ef\u7528\u8cc7\u6599\u7684\u8cc7\u6599\u6df7\u5408\u65b9\u6cd5\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u5c0d\u5408\u4f75\u65b9\u6cd5\u7684\u5168\u9762\u7814\u7a76\u70ba\u5efa\u69cb\u5f37\u5927\u4e14\u5b89\u5168\u7684\u7684\u591a\u8a9e\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u67b6\u69cb\u3002", "author": "Aakanksha et.al.", "authors": "Aakanksha, Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, Marzieh Fadaee, Sara Hooker", "id": "2410.10801v1", "paper_url": "http://arxiv.org/abs/2410.10801v1", "repo": "null"}}