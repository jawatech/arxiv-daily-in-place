{"2501.12956": {"publish_time": "2025-01-22", "title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models", "paper_summary": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u56e0\u5176\u9f90\u5927\u7684\u8cc7\u6e90\u9700\u6c42\u800c\u9762\u81e8\u91cd\u5927\u7684\u90e8\u7f72\u6311\u6230\u3002\u5118\u7ba1\u4f4e\u4f4d\u5143\u91cf\u5316\u6b0a\u91cd\u53ef\u4ee5\u6e1b\u5c11\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u4e26\u63d0\u5347\u63a8\u8ad6\u6548\u7387\uff0c\u4f46\u76ee\u524d\u7684\u786c\u9ad4\u7f3a\u4e4f\u5c0d\u6df7\u5408\u7cbe\u5ea6\u4e00\u822c\u77e9\u9663\u4e58\u6cd5 (mpGEMM) \u7684\u539f\u751f\u652f\u63f4\uff0c\u5c0e\u81f4\u975e\u91cf\u5316\u70ba\u57fa\u790e\u7684\u5be6\u4f5c\u6548\u7387\u4e0d\u5f70\u3002\u6b64\u5916\uff0c\u5747\u52fb\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u7121\u6cd5\u5145\u5206\u64f7\u53d6\u6b0a\u91cd\u5206\u4f48\uff0c\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\u3002\u6211\u5011\u63d0\u51fa GANQ (GPU \u81ea\u9069\u61c9\u975e\u5747\u52fb\u91cf\u5316)\uff0c\u4e00\u7a2e\u91dd\u5c0d\u786c\u9ad4\u6709\u6548\u67e5\u627e\u8868\u683c\u578b mpGEMM \u6700\u4f73\u5316\u7684\u5c64\u7d1a\u5f8c\u8a13\u7df4\u975e\u5747\u52fb\u91cf\u5316\u67b6\u69cb\u3002GANQ \u900f\u904e\u5229\u7528\u7121\u9700\u8a13\u7df4\u7684 GPU \u81ea\u9069\u61c9\u6700\u4f73\u5316\u6f14\u7b97\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u5c64\u7d1a\u91cf\u5316\u8aa4\u5dee\uff0c\u9054\u6210\u512a\u7570\u7684\u91cf\u5316\u6548\u80fd\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e GANQ \u964d\u4f4e\u56f0\u60d1\u5ea6\u9593\u9694\u7684\u80fd\u529b\uff0c\u8207 3 \u4f4d\u5143\u548c 4 \u4f4d\u5143\u91cf\u5316\u7684\u6700\u5148\u9032\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5f9e FP16 \u57fa\u6e96\u7dda\u964d\u4f4e\u56f0\u60d1\u5ea6\u9593\u9694\u3002\u6b64\u5916\uff0c\u7576\u90e8\u7f72\u5728\u55ae\u4e00 NVIDIA RTX 4090 GPU \u4e0a\u6642\uff0cGANQ \u7684\u91cf\u5316\u6a21\u578b\u53ef\u9054\u5230\u6bd4\u57fa\u6e96\u7dda\u5feb 2.57 \u500d\u7684\u901f\u5ea6\uff0c\u63d0\u5347 LLM \u90e8\u7f72\u4e2d\u7684\u8a18\u61b6\u9ad4\u548c\u63a8\u8ad6\u6548\u7387\u3002", "author": "Pengxiang Zhao et.al.", "authors": "Pengxiang Zhao, Xiaoming Yuan", "id": "2501.12956v1", "paper_url": "http://arxiv.org/abs/2501.12956v1", "repo": "null"}}