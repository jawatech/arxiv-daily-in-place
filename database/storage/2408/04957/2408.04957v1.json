{"2408.04957": {"publish_time": "2024-08-09", "title": "LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description", "paper_summary": "Visual Spatial Description (VSD) aims to generate texts that describe the\nspatial relationships between objects within images. Traditional visual spatial\nrelationship classification (VSRC) methods typically output the spatial\nrelationship between two objects in an image, often neglecting world knowledge\nand lacking general language capabilities. In this paper, we propose a Large\nLanguage-and-Vision Assistant for Visual Spatial Description, named LLaVA-VSD,\nwhich is designed for the classification, description, and open-ended\ndescription of visual spatial relationships. Specifically, the model first\nconstructs a VSD instruction-following dataset using given figure-caption pairs\nfor the three tasks. It then employs LoRA to fine-tune a Large Language and\nVision Assistant for VSD, which has 13 billion parameters and supports\nhigh-resolution images. Finally, a large language model (Qwen-2) is used to\nrefine the generated sentences, enhancing their diversity and accuracy.\nLLaVA-VSD demonstrates excellent multimodal conversational capabilities and can\nfollow open-ended instructions to assist with inquiries about object\nrelationships in images.", "paper_summary_zh": "\u8996\u89ba\u7a7a\u9593\u63cf\u8ff0 (VSD) \u65e8\u5728\u7522\u751f\u63cf\u8ff0\u5f71\u50cf\u4e2d\u7269\u4ef6\u4e4b\u9593\u7a7a\u9593\u95dc\u4fc2\u7684\u6587\u5b57\u3002\u50b3\u7d71\u7684\u8996\u89ba\u7a7a\u9593\u95dc\u4fc2\u5206\u985e (VSRC) \u65b9\u6cd5\u901a\u5e38\u6703\u8f38\u51fa\u5f71\u50cf\u4e2d\u5169\u500b\u7269\u4ef6\u4e4b\u9593\u7684\u7a7a\u9593\u95dc\u4fc2\uff0c\u4f46\u901a\u5e38\u6703\u5ffd\u7565\u4e16\u754c\u77e5\u8b58\uff0c\u4e14\u7f3a\u4e4f\u4e00\u822c\u7684\u8a9e\u8a00\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5927\u578b\u8a9e\u8a00\u548c\u8996\u89ba\u52a9\u7406\uff0c\u7528\u65bc\u8996\u89ba\u7a7a\u9593\u63cf\u8ff0\uff0c\u7a31\u70ba LLaVA-VSD\uff0c\u5b83\u88ab\u8a2d\u8a08\u7528\u65bc\u5206\u985e\u3001\u63cf\u8ff0\u548c\u8996\u89ba\u7a7a\u9593\u95dc\u4fc2\u7684\u958b\u653e\u5f0f\u63cf\u8ff0\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u8a72\u6a21\u578b\u9996\u5148\u4f7f\u7528\u7d66\u5b9a\u7684\u5716\u5f62\u6a19\u984c\u5c0d\uff0c\u70ba\u9019\u4e09\u500b\u4efb\u52d9\u5efa\u69cb\u4e00\u500b VSD \u6307\u4ee4\u9075\u5faa\u8cc7\u6599\u96c6\u3002\u7136\u5f8c\uff0c\u5b83\u63a1\u7528 LoRA \u5fae\u8abf\u4e00\u500b\u5927\u578b\u8a9e\u8a00\u548c\u8996\u89ba\u52a9\u7406\uff0c\u7528\u65bc VSD\uff0c\u5b83\u6709 130 \u5104\u500b\u53c3\u6578\uff0c\u4e26\u652f\u63f4\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u3002\u6700\u5f8c\uff0c\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (Qwen-2) \u4f86\u6539\u5584\u7522\u751f\u7684\u53e5\u5b50\uff0c\u589e\u5f37\u5b83\u5011\u7684\u591a\u6a23\u6027\u548c\u6e96\u78ba\u6027\u3002LLaVA-VSD \u5c55\u793a\u4e86\u51fa\u8272\u7684\u591a\u6a21\u5f0f\u5c0d\u8a71\u80fd\u529b\uff0c\u4e26\u4e14\u53ef\u4ee5\u9075\u5faa\u958b\u653e\u5f0f\u6307\u4ee4\uff0c\u5354\u52a9\u8a62\u554f\u5f71\u50cf\u4e2d\u7269\u4ef6\u7684\u95dc\u4fc2\u3002", "author": "Yizhang Jin et.al.", "authors": "Yizhang Jin, Jian Li, Jiangning Zhang, Jianlong Hu, Zhenye Gan, Xin Tan, Yong Liu, Yabiao Wang, Chengjie Wang, Lizhuang Ma", "id": "2408.04957v1", "paper_url": "http://arxiv.org/abs/2408.04957v1", "repo": "null"}}