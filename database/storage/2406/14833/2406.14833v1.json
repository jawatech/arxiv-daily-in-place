{"2406.14833": {"publish_time": "2024-06-21", "title": "Efficient Continual Pre-training by Mitigating the Stability Gap", "paper_summary": "Continual pre-training has increasingly become the predominant approach for\nadapting Large Language Models (LLMs) to new domains. This process involves\nupdating the pre-trained LLM with a corpus from a new domain, resulting in a\nshift in the training distribution. To study the behavior of LLMs during this\nshift, we measured the model's performance throughout the continual\npre-training process. we observed a temporary performance drop at the\nbeginning, followed by a recovery phase, a phenomenon known as the \"stability\ngap,\" previously noted in vision models classifying new classes. To address\nthis issue and enhance LLM performance within a fixed compute budget, we\npropose three effective strategies: (1) Continually pre-training the LLM on a\nsubset with a proper size for multiple epochs, resulting in faster performance\nrecovery than pre-training the LLM on a large corpus in a single epoch; (2)\nPre-training the LLM only on high-quality sub-corpus, which rapidly boosts\ndomain performance; and (3) Using a data mixture similar to the pre-training\ndata to reduce distribution gap. We conduct various experiments on Llama-family\nmodels to validate the effectiveness of our strategies in both medical\ncontinual pre-training and instruction tuning. For example, our strategies\nimprove the average medical task performance of the OpenLlama-3B model from\n36.2% to 40.7% with only 40% of the original training budget and enhance the\naverage general task performance without causing forgetting. Furthermore, we\napply our strategies to the Llama-3-8B model. The resulting model,\nLlama-3-Physician, achieves the best medical performance among current\nopen-source models, and performs comparably to or even better than GPT-4 on\nseveral medical benchmarks. We release our models at\n\\url{https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct}.", "paper_summary_zh": "\u6301\u7e8c\u9810\u8a13\u7df4\u5df2\u9010\u6f38\u6210\u70ba\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9069\u61c9\u5230\u65b0\u9818\u57df\u7684\u4e3b\u8981\u65b9\u6cd5\u3002\u6b64\u7a0b\u5e8f\u6d89\u53ca\u4f7f\u7528\u65b0\u9818\u57df\u7684\u8a9e\u6599\u5eab\u66f4\u65b0\u9810\u5148\u8a13\u7df4\u7684 LLM\uff0c\u5c0e\u81f4\u8a13\u7df4\u5206\u4f48\u767c\u751f\u8f49\u8b8a\u3002\u70ba\u4e86\u7814\u7a76 LLM \u5728\u6b64\u8f49\u8b8a\u671f\u9593\u7684\u884c\u70ba\uff0c\u6211\u5011\u5728\u6574\u500b\u6301\u7e8c\u9810\u8a13\u7df4\u904e\u7a0b\u4e2d\u6e2c\u91cf\u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u89c0\u5bdf\u5230\u4e00\u958b\u59cb\u6548\u80fd\u66ab\u6642\u4e0b\u964d\uff0c\u63a5\u8457\u662f\u5fa9\u539f\u968e\u6bb5\uff0c\u9019\u7a2e\u73fe\u8c61\u7a31\u70ba\u300c\u7a69\u5b9a\u6027\u5dee\u8ddd\u300d\uff0c\u5148\u524d\u5728\u5206\u985e\u65b0\u985e\u5225\u7684\u8996\u89ba\u6a21\u578b\u4e2d\u767c\u73fe\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\u4e26\u5728\u56fa\u5b9a\u7684\u904b\u7b97\u9810\u7b97\u5167\u63d0\u5347 LLM \u6548\u80fd\uff0c\u6211\u5011\u63d0\u51fa\u4e09\u7a2e\u6709\u6548\u7684\u7b56\u7565\uff1a(1) \u6301\u7e8c\u5728\u9069\u7576\u5927\u5c0f\u7684\u5b50\u96c6\u4e0a\u5c0d LLM \u9032\u884c\u591a\u500b\u6642\u671f\u7684\u9810\u8a13\u7df4\uff0c\u6bd4\u5728\u5927\u578b\u8a9e\u6599\u5eab\u4e0a\u5c0d LLM \u9032\u884c\u55ae\u4e00\u6642\u671f\u7684\u9810\u8a13\u7df4\u80fd\u66f4\u5feb\u5fa9\u539f\u6548\u80fd\uff1b(2) \u50c5\u5728\u9ad8\u54c1\u8cea\u7684\u5b50\u8a9e\u6599\u5eab\u4e0a\u5c0d LLM \u9032\u884c\u9810\u8a13\u7df4\uff0c\u9019\u80fd\u5feb\u901f\u63d0\u5347\u9818\u57df\u6548\u80fd\uff1b(3) \u4f7f\u7528\u985e\u4f3c\u9810\u8a13\u7df4\u8cc7\u6599\u7684\u8cc7\u6599\u6df7\u5408\uff0c\u4ee5\u7e2e\u5c0f\u5206\u4f48\u5dee\u8ddd\u3002\u6211\u5011\u5728 Llama \u5bb6\u65cf\u6a21\u578b\u4e0a\u9032\u884c\u5404\u7a2e\u5be6\u9a57\uff0c\u4ee5\u9a57\u8b49\u6211\u5011\u7684\u7b56\u7565\u5728\u91ab\u5b78\u6301\u7e8c\u9810\u8a13\u7df4\u548c\u6307\u4ee4\u8abf\u6574\u4e2d\u7684\u6709\u6548\u6027\u3002\u4f8b\u5982\uff0c\u6211\u5011\u7684\u7b56\u7565\u5c07 OpenLlama-3B \u6a21\u578b\u7684\u5e73\u5747\u91ab\u5b78\u4efb\u52d9\u6548\u80fd\u5f9e 36.2% \u63d0\u5347\u81f3 40.7%\uff0c\u50c5\u4f7f\u7528\u539f\u5148\u8a13\u7df4\u9810\u7b97\u7684 40%\uff0c\u4e26\u63d0\u5347\u5e73\u5747\u4e00\u822c\u4efb\u52d9\u6548\u80fd\uff0c\u4e14\u4e0d\u6703\u9020\u6210\u907a\u5fd8\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07\u6211\u5011\u7684\u7b56\u7565\u61c9\u7528\u65bc Llama-3-8B \u6a21\u578b\u3002\u7522\u751f\u7684\u6a21\u578b Llama-3-Physician \u5728\u76ee\u524d\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u4e2d\u7372\u5f97\u6700\u4f73\u91ab\u5b78\u6548\u80fd\uff0c\u4e14\u5728\u5e7e\u500b\u91ab\u5b78\u57fa\u6e96\u4e0a\u8207 GPT-4 \u76f8\u7576\u751a\u81f3\u66f4\u597d\u3002\u6211\u5011\u5728 \\url{https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct} \u767c\u5e03\u6211\u5011\u7684\u6a21\u578b\u3002", "author": "Yiduo Guo et.al.", "authors": "Yiduo Guo, Jie Fu, Huishuai Zhang, Dongyan Zhao, Yikang Shen", "id": "2406.14833v1", "paper_url": "http://arxiv.org/abs/2406.14833v1", "repo": "null"}}