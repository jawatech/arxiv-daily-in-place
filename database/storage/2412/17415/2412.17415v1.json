{"2412.17415": {"publish_time": "2024-12-23", "title": "VidCtx: Context-aware Video Question Answering with Image Models", "paper_summary": "To address computational and memory limitations of Large Multimodal Models in\nthe Video Question-Answering task, several recent methods extract textual\nrepresentations per frame (e.g., by captioning) and feed them to a Large\nLanguage Model (LLM) that processes them to produce the final response.\nHowever, in this way, the LLM does not have access to visual information and\noften has to process repetitive textual descriptions of nearby frames. To\naddress those shortcomings, in this paper, we introduce VidCtx, a novel\ntraining-free VideoQA framework which integrates both modalities, i.e. both\nvisual information from input frames and textual descriptions of others frames\nthat give the appropriate context. More specifically, in the proposed framework\na pre-trained Large Multimodal Model (LMM) is prompted to extract at regular\nintervals, question-aware textual descriptions (captions) of video frames.\nThose will be used as context when the same LMM will be prompted to answer the\nquestion at hand given as input a) a certain frame, b) the question and c) the\ncontext/caption of an appropriate frame. To avoid redundant information, we\nchose as context the descriptions of distant frames. Finally, a simple yet\neffective max pooling mechanism is used to aggregate the frame-level decisions.\nThis methodology enables the model to focus on the relevant segments of the\nvideo and scale to a high number of frames. Experiments show that VidCtx\nachieves competitive performance among approaches that rely on open models on\nthree public Video QA benchmarks, NExT-QA, IntentQA and STAR.", "paper_summary_zh": "<paragraph>\u70ba\u4e86\u89e3\u6c7a\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\u5728\u5f71\u7247\u554f\u7b54\u4efb\u52d9\u4e2d\u7684\u904b\u7b97\u548c\u8a18\u61b6\u9650\u5236\uff0c\u6700\u8fd1\u6709\u5e7e\u7a2e\u65b9\u6cd5\u6bcf\u5e40\u63d0\u53d6\u6587\u5b57\u8868\u5fb5\uff08\u4f8b\u5982\uff0c\u900f\u904e\u6a19\u984c\uff09\uff0c\u4e26\u5c07\u5176\u63d0\u4f9b\u7d66\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u7531\u5176\u8655\u7406\u4ee5\u7522\u751f\u6700\u5f8c\u7684\u56de\u61c9\u3002\n\u7136\u800c\uff0c\u9019\u6a23\u4e00\u4f86\uff0cLLM \u7121\u6cd5\u5b58\u53d6\u8996\u89ba\u8cc7\u8a0a\uff0c\u800c\u4e14\u5e38\u5e38\u5fc5\u9808\u8655\u7406\u9644\u8fd1\u5e40\u7684\u91cd\u8907\u6587\u5b57\u63cf\u8ff0\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u7f3a\u9ede\uff0c\u6211\u5011\u5728\u9019\u7bc7\u8ad6\u6587\u4e2d\u4ecb\u7d39 VidCtx\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u514d\u8a13\u7df4 VideoQA \u67b6\u69cb\uff0c\u5b83\u6574\u5408\u4e86\u5169\u7a2e\u6a21\u5f0f\uff0c\u4e5f\u5c31\u662f\u8f38\u5165\u5e40\u7684\u8996\u89ba\u8cc7\u8a0a\u548c\u63d0\u4f9b\u9069\u7576\u8108\u7d61\u7684\u5176\u4ed6\u5e40\u7684\u6587\u5b57\u63cf\u8ff0\u3002\u66f4\u5177\u9ad4\u5730\u8aaa\uff0c\u5728\u5efa\u8b70\u7684\u67b6\u69cb\u4e2d\uff0c\u6703\u63d0\u793a\u9810\u5148\u8a13\u7df4\u597d\u7684\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\uff08LMM\uff09\u5b9a\u671f\u63d0\u53d6\u5f71\u7247\u5e40\u7684\u8207\u554f\u984c\u76f8\u95dc\u7684\u6587\u5b57\u63cf\u8ff0\uff08\u6a19\u984c\uff09\u3002\n\u7576\u63d0\u793a\u76f8\u540c\u7684 LMM \u56de\u7b54\u624b\u908a\u7684\u554f\u984c\u6642\uff0c\u9019\u4e9b\u63cf\u8ff0\u5c07\u7528\u4f5c\u8108\u7d61\uff0c\u8f38\u5165\u70ba a\uff09\u67d0\u500b\u5e40\u3001b\uff09\u554f\u984c\uff0c\u4ee5\u53ca c\uff09\u9069\u7576\u5e40\u7684\u8108\u7d61/\u6a19\u984c\u3002\u70ba\u4e86\u907f\u514d\u5197\u9918\u8cc7\u8a0a\uff0c\u6211\u5011\u9078\u64c7\u9060\u8ddd\u96e2\u5e40\u7684\u63cf\u8ff0\u4f5c\u70ba\u8108\u7d61\u3002\u6700\u5f8c\uff0c\u4f7f\u7528\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u6700\u5927\u6c60\u5316\u6a5f\u5236\u4f86\u5f59\u7e3d\u5e40\u5c64\u7d1a\u7684\u6c7a\u7b56\u3002\u9019\u7a2e\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u5920\u5c08\u6ce8\u65bc\u5f71\u7247\u4e2d\u7684\u76f8\u95dc\u7247\u6bb5\uff0c\u4e26\u64f4\u5c55\u5230\u5927\u91cf\u7684\u5e40\u3002\u5be6\u9a57\u986f\u793a\uff0cVidCtx \u5728\u4f9d\u8cf4\u958b\u653e\u6a21\u578b\u7684\u65b9\u6cd5\u4e2d\uff0c\u65bc\u4e09\u500b\u516c\u958b\u7684\u5f71\u7247\u554f\u7b54\u57fa\u6e96\uff0cNExT-QA\u3001IntentQA \u548c STAR\uff0c\u53d6\u5f97\u4e86\u6709\u7af6\u722d\u529b\u7684\u8868\u73fe\u3002</paragraph>", "author": "Andreas Goulas et.al.", "authors": "Andreas Goulas, Vasileios Mezaris, Ioannis Patras", "id": "2412.17415v1", "paper_url": "http://arxiv.org/abs/2412.17415v1", "repo": "null"}}