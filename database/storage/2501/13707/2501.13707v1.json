{"2501.13707": {"publish_time": "2025-01-23", "title": "EventVL: Understand Event Streams via Multimodal Large Language Model", "paper_summary": "The event-based Vision-Language Model (VLM) recently has made good progress\nfor practical vision tasks. However, most of these works just utilize CLIP for\nfocusing on traditional perception tasks, which obstruct model understanding\nexplicitly the sufficient semantics and context from event streams. To address\nthe deficiency, we propose EventVL, the first generative event-based MLLM\n(Multimodal Large Language Model) framework for explicit semantic\nunderstanding. Specifically, to bridge the data gap for connecting different\nmodalities semantics, we first annotate a large event-image/video-text dataset,\ncontaining almost 1.4 million high-quality pairs of data, which enables\neffective learning across various scenes, e.g., drive scene or human motion.\nAfter that, we design Event Spatiotemporal Representation to fully explore the\ncomprehensive information by diversely aggregating and segmenting the event\nstream. To further promote a compact semantic space, Dynamic Semantic Alignment\nis introduced to improve and complete sparse semantic spaces of events.\nExtensive experiments show that our EventVL can significantly surpass existing\nMLLM baselines in event captioning and scene description generation tasks. We\nhope our research could contribute to the development of the event vision\ncommunity.", "paper_summary_zh": "<paragraph>\u57fa\u65bc\u4e8b\u4ef6\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u6700\u8fd1\u5728\u5be6\u52d9\u8996\u89ba\u4efb\u52d9\u4e0a\u53d6\u5f97\u826f\u597d\u7684\u9032\u5c55\u3002\u7136\u800c\uff0c\u9019\u4e9b\u4f5c\u54c1\u5927\u591a\u50c5\u5229\u7528 CLIP \u5c08\u6ce8\u65bc\u50b3\u7d71\u7684\u611f\u77e5\u4efb\u52d9\uff0c\u9019\u963b\u7919\u4e86\u6a21\u578b\u660e\u78ba\u7406\u89e3\u4e8b\u4ef6\u4e32\u6d41\u4e2d\u7684\u8db3\u5920\u8a9e\u610f\u548c\u80cc\u666f\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u7f3a\u9677\uff0c\u6211\u5011\u63d0\u51fa\u4e86 EventVL\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u7528\u65bc\u660e\u78ba\u8a9e\u610f\u7406\u89e3\u7684\u751f\u6210\u5f0f\u57fa\u65bc\u4e8b\u4ef6\u7684 MLLM\uff08\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff09\u67b6\u69cb\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u70ba\u4e86\u5f4c\u5408\u9023\u63a5\u4e0d\u540c\u6a21\u614b\u8a9e\u610f\u7684\u8cc7\u6599\u5dee\u8ddd\uff0c\u6211\u5011\u9996\u5148\u6a19\u8a3b\u4e00\u500b\u5927\u578b\u4e8b\u4ef6\u5f71\u50cf/\u5f71\u7247\u6587\u5b57\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5c07\u8fd1 140 \u842c\u5c0d\u9ad8\u54c1\u8cea\u8cc7\u6599\uff0c\u9019\u4f7f\u5f97\u8de8\u5404\u7a2e\u5834\u666f\uff08\u4f8b\u5982\u99d5\u99db\u5834\u666f\u6216\u4eba\u985e\u52d5\u4f5c\uff09\u7684\u6709\u6548\u5b78\u7fd2\u6210\u70ba\u53ef\u80fd\u3002\u5728\u90a3\u4e4b\u5f8c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e8b\u4ef6\u6642\u7a7a\u8868\u793a\uff0c\u85c9\u7531\u591a\u6a23\u5316\u5730\u5f59\u7e3d\u548c\u5206\u5272\u4e8b\u4ef6\u4e32\u6d41\uff0c\u4f86\u5145\u5206\u63a2\u7d22\u7d9c\u5408\u8cc7\u8a0a\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u4fc3\u9032\u4e00\u500b\u7dca\u6e4a\u7684\u8a9e\u610f\u7a7a\u9593\uff0c\u5f15\u5165\u4e86\u52d5\u614b\u8a9e\u610f\u5c0d\u9f4a\uff0c\u4ee5\u6539\u5584\u548c\u5b8c\u6210\u4e8b\u4ef6\u7684\u7a00\u758f\u8a9e\u610f\u7a7a\u9593\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684 EventVL \u80fd\u5728\u4e8b\u4ef6\u5b57\u5e55\u548c\u5834\u666f\u63cf\u8ff0\u7522\u751f\u4efb\u52d9\u4e2d\uff0c\u986f\u8457\u8d85\u8d8a\u73fe\u6709\u7684 MLLM \u57fa\u6e96\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684\u7814\u7a76\u80fd\u70ba\u4e8b\u4ef6\u8996\u89ba\u793e\u7fa4\u7684\u767c\u5c55\u505a\u51fa\u8ca2\u737b\u3002</paragraph>", "author": "Pengteng Li et.al.", "authors": "Pengteng Li, Yunfan Lu, Pinghao Song, Wuyang Li, Huizai Yao, Hui Xiong", "id": "2501.13707v1", "paper_url": "http://arxiv.org/abs/2501.13707v1", "repo": "null"}}