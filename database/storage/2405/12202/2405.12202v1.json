{"2405.12202": {"publish_time": "2024-05-20", "title": "Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution", "paper_summary": "In this work, we present an arbitrary-scale super-resolution (SR) method to\nenhance the resolution of scientific data, which often involves complex\nchallenges such as continuity, multi-scale physics, and the intricacies of\nhigh-frequency signals. Grounded in operator learning, the proposed method is\nresolution-invariant. The core of our model is a hierarchical neural operator\nthat leverages a Galerkin-type self-attention mechanism, enabling efficient\nlearning of mappings between function spaces. Sinc filters are used to\nfacilitate the information transfer across different levels in the hierarchy,\nthereby ensuring representation equivalence in the proposed neural operator.\nAdditionally, we introduce a learnable prior structure that is derived from the\nspectral resizing of the input data. This loss prior is model-agnostic and is\ndesigned to dynamically adjust the weighting of pixel contributions, thereby\nbalancing gradients effectively across the model. We conduct extensive\nexperiments on diverse datasets from different domains and demonstrate\nconsistent improvements compared to strong baselines, which consist of various\nstate-of-the-art SR methods.", "paper_summary_zh": "\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u4efb\u610f\u5c3a\u5ea6\u7684\u8d85\u89e3\u6790\u5ea6 (SR) \u65b9\u6cd5\uff0c\u4ee5\u589e\u5f37\u79d1\u5b78\u6578\u64da\u7684\u5206\u8fa8\u7387\uff0c\u9019\u901a\u5e38\u6d89\u53ca\u9023\u7e8c\u6027\u3001\u591a\u5c3a\u5ea6\u7269\u7406\u4ee5\u53ca\u9ad8\u983b\u7387\u8a0a\u865f\u7684\u8907\u96dc\u6027\u7b49\u6311\u6230\u3002\u8a72\u65b9\u6cd5\u4ee5\u7b97\u5b50\u5b78\u7fd2\u70ba\u57fa\u790e\uff0c\u5177\u6709\u89e3\u6790\u5ea6\u4e0d\u8b8a\u6027\u3002\u6211\u5011\u6a21\u578b\u7684\u6838\u5fc3\u662f\u4e00\u500b\u5206\u5c64\u795e\u7d93\u7b97\u5b50\uff0c\u5b83\u5229\u7528\u4e86\u4f3d\u907c\u91d1\u985e\u578b\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u5be6\u73fe\u4e86\u51fd\u6578\u7a7a\u9593\u4e4b\u9593\u5c0d\u61c9\u95dc\u4fc2\u7684\u6709\u6548\u5b78\u7fd2\u3002Sinc \u6ffe\u6ce2\u5668\u7528\u65bc\u4fc3\u9032\u5c64\u6b21\u7d50\u69cb\u4e2d\u4e0d\u540c\u5c64\u7d1a\u4e4b\u9593\u7684\u8cc7\u8a0a\u50b3\u905e\uff0c\u5f9e\u800c\u78ba\u4fdd\u4e86\u6240\u63d0\u51fa\u7684\u795e\u7d93\u7b97\u5b50\u4e2d\u7684\u8868\u793a\u7b49\u50f9\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86\u4e00\u500b\u53ef\u5b78\u7fd2\u7684\u5148\u9a57\u7d50\u69cb\uff0c\u5b83\u4f86\u81ea\u8f38\u5165\u6578\u64da\u7684\u983b\u8b5c\u8abf\u6574\u3002\u6b64\u640d\u5931\u5148\u9a57\u8207\u6a21\u578b\u7121\u95dc\uff0c\u65e8\u5728\u52d5\u614b\u8abf\u6574\u50cf\u7d20\u8ca2\u737b\u7684\u6b0a\u91cd\uff0c\u5f9e\u800c\u6709\u6548\u5730\u5e73\u8861\u6574\u500b\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u3002\u6211\u5011\u5c0d\u4f86\u81ea\u4e0d\u540c\u9818\u57df\u7684\u4e0d\u540c\u6578\u64da\u96c6\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4e26\u5c55\u793a\u4e86\u8207\u5f37\u57fa\u7dda\u76f8\u6bd4\u7684\u4e00\u81f4\u6539\u9032\uff0c\u9019\u4e9b\u57fa\u7dda\u5305\u62ec\u5404\u7a2e\u6700\u5148\u9032\u7684 SR \u65b9\u6cd5\u3002", "author": "Xihaier Luo et.al.", "authors": "Xihaier Luo, Xiaoning Qian, Byung-Jun Yoon", "id": "2405.12202v1", "paper_url": "http://arxiv.org/abs/2405.12202v1", "repo": "null"}}