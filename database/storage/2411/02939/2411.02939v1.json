{"2411.02939": {"publish_time": "2024-11-05", "title": "A Post-Training Enhanced Optimization Approach for Small Language Models", "paper_summary": "This paper delves into the continuous post-training optimization methods for\nsmall language models, and proposes a continuous post-training alignment data\nconstruction method for small language models. The core of this method is based\non the data guidance of large models, optimizing the diversity and accuracy of\nalignment data. In addition, to verify the effectiveness of the methods in this\npaper, we used Qwen2-0.5B-Instruct model as the baseline model for small\nlanguage models, using the alignment dataset constructed by our proposed\nmethod, we trained and compared several groups of experiments, including SFT\n(Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky\noptimization) post-training experiment, as well as SFT-KTO two-stage\npost-training experiment and model weight fusion experiment. Finally, we\nevaluated and analyzed the performance of post-training models, and confirmed\nthat the continuous post-training optimization method proposed by us can\nsignificantly improve the performance of small language models.", "paper_summary_zh": "\u672c\u6587\u6df1\u5165\u63a2\u8ba8\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u540e\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u540e\u8bad\u7ec3\u5bf9\u9f50\u6570\u636e\u6784\u5efa\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u6570\u636e\u6307\u5bfc\uff0c\u4f18\u5316\u5bf9\u9f50\u6570\u636e\u7684 diversity \u548c\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u9a8c\u8bc1\u672c\u6587\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6211\u4eec\u4f7f\u7528 Qwen2-0.5B-Instruct \u6a21\u578b\u4f5c\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u7684 baseline \u6a21\u578b\uff0c\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u6784\u5efa\u7684\u5bf9\u9f50\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5e76\u6bd4\u8f83\u4e86\u51e0\u7ec4\u5b9e\u9a8c\uff0c\u5305\u62ec SFT\uff08\u6709\u76d1\u7763\u5fae\u8c03\uff09\u540e\u8bad\u7ec3\u5b9e\u9a8c\u548c KTO\uff08Kahneman Tversky \u4f18\u5316\uff09\u540e\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u4ee5\u53ca SFT-KTO \u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u5b9e\u9a8c\u548c\u6a21\u578b\u6743\u91cd\u878d\u5408\u5b9e\u9a8c\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u540e\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u8bc4\u4f30\u548c\u5206\u6790\uff0c\u5e76\u786e\u8ba4\u6211\u4eec\u63d0\u51fa\u7684\u6301\u7eed\u540e\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "author": "Keke Zhai et.al.", "authors": "Keke Zhai", "id": "2411.02939v1", "paper_url": "http://arxiv.org/abs/2411.02939v1", "repo": "null"}}