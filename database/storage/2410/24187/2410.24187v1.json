{"2410.24187": {"publish_time": "2024-10-31", "title": "Chasing Better Deep Image Priors between Over- and Under-parameterization", "paper_summary": "Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep\nimage priors (DIP) that regularize various image inverse problems. Meanwhile,\nresearchers also proposed extremely compact, under-parameterized image priors\n(e.g., deep decoder) that are strikingly competent for image restoration too,\ndespite a loss of accuracy. These two extremes push us to think whether there\nexists a better solution in the middle: between over- and under-parameterized\nimage priors, can one identify \"intermediate\" parameterized image priors that\nachieve better trade-offs between performance, efficiency, and even preserving\nstrong transferability? Drawing inspirations from the lottery ticket hypothesis\n(LTH), we conjecture and study a novel \"lottery image prior\" (LIP) by\nexploiting DNN inherent sparsity, stated as: given an over-parameterized\nDNN-based image prior, it will contain a sparse subnetwork that can be trained\nin isolation, to match the original DNN's performance when being applied as a\nprior to various image inverse problems. Our results validate the superiority\nof LIPs: we can successfully locate the LIP subnetworks from over-parameterized\nDIPs at substantial sparsity ranges. Those LIP subnetworks significantly\noutperform deep decoders under comparably compact model sizes (by often fully\npreserving the effectiveness of their over-parameterized counterparts), and\nthey also possess high transferability across different images as well as\nrestoration task types. Besides, we also extend LIP to compressive sensing\nimage reconstruction, where a pre-trained GAN generator is used as the prior\n(in contrast to untrained DIP or deep decoder), and confirm its validity in\nthis setting too. To our best knowledge, this is the first time that LTH is\ndemonstrated to be relevant in the context of inverse problems or image priors.", "paper_summary_zh": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u4ee5\u4f5c\u4e3a\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c (DIP) \u800c\u95fb\u540d\uff0c\u53ef\u5bf9\u5404\u79cd\u56fe\u50cf\u9006\u95ee\u9898\u8fdb\u884c\u6b63\u5219\u5316\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4eba\u5458\u8fd8\u63d0\u51fa\u4e86\u6781\u5ea6\u7d27\u51d1\u7684\u3001\u53c2\u6570\u5316\u4e0d\u8db3\u7684\u56fe\u50cf\u5148\u9a8c\uff08\u4f8b\u5982\uff0c\u6df1\u5ea6\u89e3\u7801\u5668\uff09\uff0c\u5c3d\u7ba1\u51c6\u786e\u6027\u6709\u6240\u4e0b\u964d\uff0c\u4f46\u5bf9\u4e8e\u56fe\u50cf\u4fee\u590d\u4e5f\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u80fd\u529b\u3002\u8fd9\u4e24\u4e2a\u6781\u7aef\u4fc3\u4f7f\u6211\u4eec\u601d\u8003\u662f\u5426\u5b58\u5728\u4e00\u4e2a\u66f4\u597d\u7684\u4e2d\u95f4\u89e3\u51b3\u65b9\u6848\uff1a\u5728\u8fc7\u5ea6\u53c2\u6570\u5316\u548c\u53c2\u6570\u5316\u4e0d\u8db3\u7684\u56fe\u50cf\u5148\u9a8c\u4e4b\u95f4\uff0c\u80fd\u5426\u627e\u5230\u201c\u4e2d\u95f4\u201d\u53c2\u6570\u5316\u56fe\u50cf\u5148\u9a8c\uff0c\u5728\u6027\u80fd\u3001\u6548\u7387\uff0c\u751a\u81f3\u4fdd\u7559\u5f3a\u5927\u7684\u53ef\u8fc1\u79fb\u6027\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u7684\u6743\u8861\uff1f\u4ece\u5f69\u7968\u5047\u8bbe (LTH) \u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u6211\u4eec\u901a\u8fc7\u5229\u7528 DNN \u56fa\u6709\u7684\u7a00\u758f\u6027\u6765\u63a8\u6d4b\u548c\u7814\u7a76\u4e00\u79cd\u65b0\u9896\u7684\u201c\u5f69\u7968\u56fe\u50cf\u5148\u9a8c\u201d(LIP)\uff0c\u8868\u8ff0\u5982\u4e0b\uff1a\u7ed9\u5b9a\u4e00\u4e2a\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u57fa\u4e8e DNN \u7684\u56fe\u50cf\u5148\u9a8c\uff0c\u5b83\u5c06\u5305\u542b\u4e00\u4e2a\u7a00\u758f\u5b50\u7f51\u7edc\uff0c\u8be5\u5b50\u7f51\u7edc\u53ef\u4ee5\u72ec\u7acb\u8bad\u7ec3\uff0c\u4ee5\u5339\u914d\u539f\u59cb DNN \u7684\u6027\u80fd\uff0c\u5f53\u4f5c\u4e3a\u5148\u9a8c\u5e94\u7528\u4e8e\u5404\u79cd\u56fe\u50cf\u9006\u95ee\u9898\u65f6\u3002\u6211\u4eec\u7684\u7ed3\u679c\u9a8c\u8bc1\u4e86 LIP \u7684\u4f18\u8d8a\u6027\uff1a\u6211\u4eec\u53ef\u4ee5\u5728\u5927\u91cf\u7684\u7a00\u758f\u6027\u8303\u56f4\u5185\u6210\u529f\u5730\u4ece\u8fc7\u5ea6\u53c2\u6570\u5316\u7684 DIP \u4e2d\u627e\u5230 LIP \u5b50\u7f51\u7edc\u3002\u8fd9\u4e9b LIP \u5b50\u7f51\u7edc\u5728\u76f8\u5f53\u7d27\u51d1\u7684\u6a21\u578b\u5927\u5c0f\u4e0b\u660e\u663e\u4f18\u4e8e\u6df1\u5ea6\u89e3\u7801\u5668\uff08\u901a\u5e38\u5b8c\u5168\u4fdd\u7559\u5176\u8fc7\u5ea6\u53c2\u6570\u5316\u5bf9\u5e94\u6a21\u578b\u7684\u6709\u6548\u6027\uff09\uff0c\u5e76\u4e14\u5b83\u4eec\u8fd8\u5bf9\u4e0d\u540c\u7684\u56fe\u50cf\u4ee5\u53ca\u4fee\u590d\u4efb\u52a1\u7c7b\u578b\u5177\u6709\u5f88\u9ad8\u7684\u53ef\u8fc1\u79fb\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c06 LIP \u62d3\u5c55\u5230\u4e86\u538b\u7f29\u611f\u77e5\u56fe\u50cf\u91cd\u5efa\uff0c\u5176\u4e2d\u7ecf\u8fc7\u9884\u8bad\u7ec3\u7684 GAN \u751f\u6210\u5668\u88ab\u7528\u4f5c\u5148\u9a8c\uff08\u4e0e\u672a\u7ecf\u8bad\u7ec3\u7684 DIP \u6216\u6df1\u5ea6\u89e3\u7801\u5668\u76f8\u53cd\uff09\uff0c\u5e76\u5728\u6b64\u8bbe\u7f6e\u4e2d\u786e\u8ba4\u4e86\u5176\u6709\u6548\u6027\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f LTH \u9996\u6b21\u88ab\u8bc1\u660e\u4e0e\u9006\u95ee\u9898\u6216\u56fe\u50cf\u5148\u9a8c\u76f8\u5173\u3002", "author": "Qiming Wu et.al.", "authors": "Qiming Wu, Xiaohan Chen, Yifan Jiang, Zhangyang Wang", "id": "2410.24187v1", "paper_url": "http://arxiv.org/abs/2410.24187v1", "repo": "https://github.com/vita-group/chasing-better-dips"}}