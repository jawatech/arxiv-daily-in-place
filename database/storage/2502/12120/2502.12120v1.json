{"2502.12120": {"publish_time": "2025-02-17", "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws", "paper_summary": "Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.", "paper_summary_zh": "\u898f\u6a21\u5316\u5b9a\u5f8b\u900f\u904e\u63d0\u4f9b\u6a21\u578b\u5927\u5c0f\u3001\u7b26\u865f\u548c\u904b\u7b97\u7684\u6700\u4f73\u5e73\u8861\u4f30\u8a08\uff0c\u5f15\u5c0e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u958b\u767c\u3002\u6700\u8fd1\uff0c\u8207\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u548c\u4e0b\u6e38\u4efb\u52d9\u76f8\u95dc\u7684\u640d\u5931\u5230\u640d\u5931\u7e2e\u653e\u5b9a\u5f8b\u5df2\u6210\u70ba\u4e86\u89e3\u548c\u6539\u5584 LLM \u6548\u80fd\u7684\u5f37\u5927\u5de5\u5177\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u54ea\u4e9b\u56e0\u7d20\u6700\u80fd\u5f71\u97ff\u640d\u5931\u5230\u640d\u5931\u7e2e\u653e\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0c\u9810\u8a13\u7df4\u8cc7\u6599\u548c\u5206\u8a5e\u5668\u6703\u6c7a\u5b9a\u7e2e\u653e\u8da8\u52e2\u3002\u76f8\u53cd\u5730\uff0c\u6a21\u578b\u5927\u5c0f\u3001\u6700\u4f73\u5316\u8d85\u53c3\u6578\uff0c\u751a\u81f3\u91cd\u5927\u7684\u67b6\u69cb\u5dee\u7570\uff08\u4f8b\u5982\u57fa\u65bcTransformer\u7684\u6a21\u578b\uff0c\u5982 Llama\uff0c\u548c\u72c0\u614b\u7a7a\u9593\u6a21\u578b\uff0c\u5982 Mamba \u4e4b\u9593\u7684\u5dee\u7570\uff09\u5f71\u97ff\u6709\u9650\u3002\u56e0\u6b64\uff0c\u5f9e\u696d\u4eba\u54e1\u61c9\u4ed4\u7d30\u7b56\u5283\u9069\u7576\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u4ee5\u7372\u5f97\u6700\u4f73\u7684\u4e0b\u6e38\u6548\u80fd\uff0c\u800c\u67b6\u69cb\u548c\u5176\u4ed6\u8a2d\u5b9a\u53ef\u4ee5\u81ea\u7531\u6700\u4f73\u5316\u4ee5\u63d0\u5347\u8a13\u7df4\u6548\u7387\u3002", "author": "Prasanna Mayilvahanan et.al.", "authors": "Prasanna Mayilvahanan, Thadd\u00e4us Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel", "id": "2502.12120v1", "paper_url": "http://arxiv.org/abs/2502.12120v1", "repo": "null"}}