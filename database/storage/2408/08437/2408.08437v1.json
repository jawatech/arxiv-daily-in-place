{"2408.08437": {"publish_time": "2024-08-15", "title": "PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications", "paper_summary": "While Vision Transformers (ViTs) are extremely effective at computer vision\ntasks and are replacing convolutional neural networks as the new\nstate-of-the-art, they are complex and memory-intensive models. In order to\neffectively run these models on resource-constrained mobile/edge systems, there\nis a need to not only compress these models but also to optimize them and\nconvert them into deployment-friendly formats. To this end, this paper presents\na combined pruning and quantization tool, called PQV-Mobile, to optimize vision\ntransformers for mobile applications. The tool is able to support different\ntypes of structured pruning based on magnitude importance, Taylor importance,\nand Hessian importance. It also supports quantization from FP32 to FP16 and\nint8, targeting different mobile hardware backends. We demonstrate the\ncapabilities of our tool and show important latency-memory-accuracy trade-offs\nfor different amounts of pruning and int8 quantization with Facebook Data\nEfficient Image Transformer (DeiT) models. Our results show that even pruning a\nDeiT model by 9.375% and quantizing it to int8 from FP32 followed by optimizing\nfor mobile applications, we find a latency reduction by 7.18X with a small\naccuracy loss of 2.24%. The tool is open source.", "paper_summary_zh": "\u5118\u7ba1\u8996\u89baTransformer (ViT) \u5728\u96fb\u8166\u8996\u89ba\u4efb\u52d9\u4e2d\u6975\u70ba\u6709\u6548\uff0c\u4e14\u53d6\u4ee3\u4e86\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u6210\u70ba\u65b0\u7684\u9032\u6b65\u6307\u6a19\uff0c\u4f46\u5b83\u5011\u662f\u8907\u96dc\u4e14\u9700\u8981\u5927\u91cf\u8a18\u61b6\u9ad4\u7684\u6a21\u578b\u3002\u70ba\u4e86\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u884c\u52d5/\u908a\u7de3\u7cfb\u7d71\u4e0a\u6709\u6548\u57f7\u884c\u9019\u4e9b\u6a21\u578b\uff0c\u4e0d\u50c5\u9700\u8981\u58d3\u7e2e\u9019\u4e9b\u6a21\u578b\uff0c\u9084\u9700\u8981\u6700\u4f73\u5316\u5b83\u5011\uff0c\u4e26\u5c07\u5b83\u5011\u8f49\u63db\u6210\u90e8\u7f72\u53cb\u5584\u7684\u683c\u5f0f\u3002\u70ba\u6b64\uff0c\u672c\u8ad6\u6587\u63d0\u51fa\u4e00\u500b\u540d\u70ba PQV-Mobile \u7684\u7d50\u5408\u526a\u679d\u8207\u91cf\u5316\u5de5\u5177\uff0c\u4ee5\u6700\u4f73\u5316\u884c\u52d5\u61c9\u7528\u7a0b\u5f0f\u7684\u8996\u89baTransformer\u3002\u6b64\u5de5\u5177\u80fd\u5920\u652f\u63f4\u57fa\u65bc\u5e45\u5ea6\u91cd\u8981\u6027\u3001Taylor \u91cd\u8981\u6027\u4ee5\u53ca Hessian \u91cd\u8981\u6027\u7684\u4e0d\u540c\u7d50\u69cb\u5316\u526a\u679d\u985e\u578b\u3002\u5b83\u4e5f\u652f\u63f4\u5f9e FP32 \u5230 FP16 \u548c int8 \u7684\u91cf\u5316\uff0c\u9396\u5b9a\u4e0d\u540c\u7684\u884c\u52d5\u786c\u9ad4\u5f8c\u7aef\u3002\u6211\u5011\u793a\u7bc4\u4e86\u6211\u5011\u5de5\u5177\u7684\u529f\u80fd\uff0c\u4e26\u5c55\u793a\u4e86 Facebook \u8cc7\u6599\u9ad8\u6548\u80fd\u5f71\u50cfTransformer (DeiT) \u6a21\u578b\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u526a\u679d\u8207 int8 \u91cf\u5316\u4e2d\u91cd\u8981\u7684\u5ef6\u9072-\u8a18\u61b6\u9ad4-\u7cbe\u78ba\u5ea6\u6b0a\u8861\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u5373\u4f7f\u5c07 DeiT \u6a21\u578b\u526a\u679d 9.375%\uff0c\u4e26\u5f9e FP32 \u91cf\u5316\u70ba int8\uff0c\u7136\u5f8c\u6700\u4f73\u5316\u884c\u52d5\u61c9\u7528\u7a0b\u5f0f\uff0c\u6211\u5011\u767c\u73fe\u5ef6\u9072\u6e1b\u5c11\u4e86 7.18 \u500d\uff0c\u7cbe\u78ba\u5ea6\u50c5\u640d\u5931 2.24%\u3002\u6b64\u5de5\u5177\u662f\u958b\u6e90\u7684\u3002", "author": "Kshitij Bhardwaj et.al.", "authors": "Kshitij Bhardwaj", "id": "2408.08437v1", "paper_url": "http://arxiv.org/abs/2408.08437v1", "repo": "null"}}