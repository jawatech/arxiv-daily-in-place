{"2410.06916": {"publish_time": "2024-10-09", "title": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration", "paper_summary": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by first employing a compact model to draft multiple tokens\nefficiently and then using the target LLM to verify them in parallel. While\nthis technique has achieved notable speedups, most existing approaches\nnecessitate either additional parameters or extensive training to construct\neffective draft models, thereby restricting their applicability across\ndifferent LLMs and tasks. To address this limitation, we explore a novel\nplug-and-play SD solution with layer-skipping, which skips intermediate layers\nof the target LLM as the compact draft model. Our analysis reveals that LLMs\nexhibit great potential for self-acceleration through layer sparsity and the\ntask-specific nature of this sparsity. Building on these insights, we introduce\nSWIFT, an on-the-fly self-speculative decoding algorithm that adaptively\nselects intermediate layers of LLMs to skip during inference. SWIFT does not\nrequire auxiliary models or additional training, making it a plug-and-play\nsolution for accelerating LLM inference across diverse input data streams. Our\nextensive experiments across a wide range of models and downstream tasks\ndemonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving\nthe original distribution of the generated text.", "paper_summary_zh": "\u63a8\u6e2c\u6027\u89e3\u78bc (SD) \u5df2\u6210\u70ba\u5ee3\u6cdb\u4f7f\u7528\u7684\u7bc4\u4f8b\uff0c\u7528\u65bc\u52a0\u901f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u8ad6\uff0c\u540c\u6642\u4e0d\u640d\u5bb3\u751f\u6210\u54c1\u8cea\u3002\u5b83\u7684\u904b\u4f5c\u65b9\u5f0f\u662f\u5148\u63a1\u7528\u7cbe\u7c21\u6a21\u578b\u6709\u6548\u7387\u5730\u8d77\u8349\u591a\u500b\u7b26\u865f\uff0c\u7136\u5f8c\u4f7f\u7528\u76ee\u6a19 LLM \u5e73\u884c\u9a57\u8b49\u5b83\u5011\u3002\u96d6\u7136\u6b64\u6280\u8853\u5df2\u9054\u5230\u986f\u8457\u7684\u52a0\u901f\uff0c\u4f46\u73fe\u6709\u7684\u65b9\u6cd5\u5927\u591a\u9700\u8981\u984d\u5916\u7684\u53c3\u6578\u6216\u5ee3\u6cdb\u7684\u8a13\u7df4\uff0c\u624d\u80fd\u5efa\u69cb\u6709\u6548\u7684\u8d77\u8349\u6a21\u578b\uff0c\u56e0\u6b64\u9650\u5236\u4e86\u5b83\u5011\u5728\u4e0d\u540c LLM \u548c\u4efb\u52d9\u4e2d\u7684\u9069\u7528\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u9650\u5236\uff0c\u6211\u5011\u63a2\u7d22\u4e00\u7a2e\u5275\u65b0\u7684\u5373\u63d2\u5373\u7528 SD \u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8df3\u5c64\u529f\u80fd\uff0c\u5b83\u6703\u7565\u904e\u76ee\u6a19 LLM \u7684\u4e2d\u9593\u5c64\uff0c\u4f5c\u70ba\u7cbe\u7c21\u7684\u8d77\u8349\u6a21\u578b\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0cLLM \u901a\u904e\u5c64\u7a00\u758f\u6027\u4ee5\u53ca\u6b64\u7a00\u758f\u6027\u7684\u4efb\u52d9\u7279\u5b9a\u6027\u8cea\uff0c\u5c55\u73fe\u51fa\u6975\u5927\u7684\u81ea\u6211\u52a0\u901f\u6f5b\u529b\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86 SWIFT\uff0c\u9019\u662f\u4e00\u7a2e\u5373\u6642\u81ea\u6211\u63a8\u6e2c\u89e3\u78bc\u6f14\u7b97\u6cd5\uff0c\u53ef\u81ea\u9069\u61c9\u5730\u9078\u53d6 LLM \u7684\u4e2d\u9593\u5c64\uff0c\u5728\u63a8\u8ad6\u671f\u9593\u7565\u904e\u3002SWIFT \u4e0d\u9700\u8981\u8f14\u52a9\u6a21\u578b\u6216\u984d\u5916\u8a13\u7df4\uff0c\u4f7f\u5176\u6210\u70ba\u5373\u63d2\u5373\u7528\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u52a0\u901f LLM \u63a8\u8ad6\uff0c\u4ee5\u6db5\u84cb\u4e0d\u540c\u7684\u8f38\u5165\u8cc7\u6599\u4e32\u6d41\u3002\u6211\u5011\u5728\u5404\u7a2e\u6a21\u578b\u548c\u4e0b\u6e38\u4efb\u52d9\u4e2d\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u8b49\u660e SWIFT \u53ef\u4ee5\u9054\u5230 1.3 \u500d\u81f3 1.6 \u500d\u7684\u52a0\u901f\uff0c\u540c\u6642\u4fdd\u7559\u751f\u6210\u7684\u6587\u5b57\u7684\u539f\u59cb\u5206\u4f48\u3002", "author": "Heming Xia et.al.", "authors": "Heming Xia, Yongqi Li, Jun Zhang, Cunxiao Du, Wenjie Li", "id": "2410.06916v1", "paper_url": "http://arxiv.org/abs/2410.06916v1", "repo": "null"}}