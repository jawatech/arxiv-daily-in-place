{"2405.02509": {"publish_time": "2024-05-03", "title": "Implicit Neural Representations for Robust Joint Sparse-View CT Reconstruction", "paper_summary": "Computed Tomography (CT) is pivotal in industrial quality control and medical\ndiagnostics. Sparse-view CT, offering reduced ionizing radiation, faces\nchallenges due to its under-sampled nature, leading to ill-posed reconstruction\nproblems. Recent advancements in Implicit Neural Representations (INRs) have\nshown promise in addressing sparse-view CT reconstruction. Recognizing that CT\noften involves scanning similar subjects, we propose a novel approach to\nimprove reconstruction quality through joint reconstruction of multiple objects\nusing INRs. This approach can potentially leverage both the strengths of INRs\nand the statistical regularities across multiple objects. While current INR\njoint reconstruction techniques primarily focus on accelerating convergence via\nmeta-initialization, they are not specifically tailored to enhance\nreconstruction quality. To address this gap, we introduce a novel INR-based\nBayesian framework integrating latent variables to capture the inter-object\nrelationships. These variables serve as a dynamic reference throughout the\noptimization, thereby enhancing individual reconstruction fidelity. Our\nextensive experiments, which assess various key factors such as reconstruction\nquality, resistance to overfitting, and generalizability, demonstrate\nsignificant improvements over baselines in common numerical metrics. This\nunderscores a notable advancement in CT reconstruction methods.", "paper_summary_zh": "\u96fb\u8166\u65b7\u5c64\u6383\u63cf (CT) \u5728\u5de5\u696d\u54c1\u8cea\u7ba1\u63a7\u548c\u91ab\u7642\u8a3a\u65b7\u4e2d\u626e\u6f14\u8457\u95dc\u9375\u89d2\u8272\u3002\u7a00\u8996\u89d2 CT \u80fd\u964d\u4f4e\u6e38\u96e2\u8f3b\u5c04\uff0c\u4f46\u7531\u65bc\u5176\u6b20\u63a1\u6a23\u7684\u7279\u6027\uff0c\u5728\u91cd\u5efa\u904e\u7a0b\u4e2d\u6703\u9762\u81e8\u554f\u984c\uff0c\u5c0e\u81f4\u91cd\u5efa\u7d50\u679c\u4e0d\u4f73\u3002\u96b1\u5f0f\u795e\u7d93\u8868\u793a (INR) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u986f\u793a\u51fa\u5728\u89e3\u6c7a\u7a00\u8996\u89d2 CT \u91cd\u5efa\u65b9\u9762\u7684\u524d\u666f\u3002\u6211\u5011\u8a8d\u77e5\u5230 CT \u901a\u5e38\u6d89\u53ca\u6383\u63cf\u985e\u4f3c\u7684\u53d7\u8a66\u8005\uff0c\u56e0\u6b64\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u900f\u904e\u4f7f\u7528 INR \u806f\u5408\u91cd\u5efa\u591a\u500b\u7269\u4ef6\u4f86\u63d0\u5347\u91cd\u5efa\u54c1\u8cea\u3002\u6b64\u65b9\u6cd5\u6709\u6f5b\u529b\u540c\u6642\u5229\u7528 INR \u7684\u512a\u9ede\u548c\u591a\u500b\u7269\u4ef6\u4e4b\u9593\u7684\u7d71\u8a08\u898f\u5f8b\u6027\u3002\u96d6\u7136\u76ee\u524d\u7684 INR \u806f\u5408\u91cd\u5efa\u6280\u8853\u4e3b\u8981\u8457\u91cd\u65bc\u900f\u904e\u5143\u521d\u59cb\u5316\u52a0\u901f\u6536\u6582\uff0c\u4f46\u5b83\u5011\u4e26\u672a\u7279\u5225\u91dd\u5c0d\u63d0\u5347\u91cd\u5efa\u54c1\u8cea\u800c\u8a2d\u8a08\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7684\u57fa\u65bc INR \u7684\u8c9d\u6c0f\u67b6\u69cb\uff0c\u6574\u5408\u6f5b\u5728\u8b8a\u6578\u4ee5\u6355\u6349\u7269\u4ef6\u9593\u7684\u95dc\u4fc2\u3002\u9019\u4e9b\u8b8a\u6578\u5728\u6574\u500b\u6700\u4f73\u5316\u904e\u7a0b\u4e2d\u4f5c\u70ba\u52d5\u614b\u53c3\u8003\uff0c\u5f9e\u800c\u63d0\u5347\u500b\u5225\u91cd\u5efa\u7684\u4fdd\u771f\u5ea6\u3002\u6211\u5011\u7684\u5ee3\u6cdb\u5be6\u9a57\u8a55\u4f30\u4e86\u91cd\u5efa\u54c1\u8cea\u3001\u6297\u904e\u64ec\u5408\u80fd\u529b\u548c\u6cdb\u5316\u6027\u7b49\u5404\u7a2e\u95dc\u9375\u56e0\u7d20\uff0c\u7d50\u679c\u986f\u793a\u5728\u5e38\u898b\u7684\u6578\u503c\u6307\u6a19\u4e0a\u5927\u5e45\u512a\u65bc\u57fa\u6e96\u3002\u9019\u7a81\u986f\u4e86 CT \u91cd\u5efa\u65b9\u6cd5\u7684\u986f\u8457\u9032\u6b65\u3002", "author": "Jiayang Shi et.al.", "authors": "Jiayang Shi, Junyi Zhu, Daniel M. Pelt, K. Joost Batenburg, Matthew B. Blaschko", "id": "2405.02509v1", "paper_url": "http://arxiv.org/abs/2405.02509v1", "repo": "null"}}