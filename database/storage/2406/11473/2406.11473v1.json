{"2406.11473": {"publish_time": "2024-06-17", "title": "Promises, Outlooks and Challenges of Diffusion Language Modeling", "paper_summary": "The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.", "paper_summary_zh": "<paragraph>\u73fe\u4ee3\u7684\u81ea\u8ff4\u6b78\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728 NLP \u57fa\u6e96\u4e0a\u53d6\u5f97\u4e86\u5091\u51fa\u7684\u8868\u73fe\uff0c\u4e26\u5df2\u90e8\u7f72\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u3002\u7136\u800c\uff0c\u5b83\u5011\u4ecd\u7136\u53d7\u5230\u81ea\u8ff4\u6b78\u8a13\u7df4\u7bc4\u4f8b\u7684\u9650\u5236\u3002\u4f8b\u5982\uff0c\u81ea\u8ff4\u6b78\u6b0a\u6756\u751f\u6210\u660e\u986f\u7de9\u6162\uff0c\u4e14\u53ef\u80fd\u5bb9\u6613\u51fa\u73fe\u300c\u66dd\u5149\u504f\u5dee\u300d\u3002\u57fa\u65bc\u64f4\u6563\u7684\u8a9e\u8a00\u6a21\u578b\u88ab\u63d0\u51fa\u4f5c\u70ba\u81ea\u8ff4\u6b78\u751f\u6210\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u89e3\u6c7a\u5176\u4e2d\u4e00\u4e9b\u9650\u5236\u3002\u6211\u5011\u8a55\u4f30\u4e86\u6700\u8fd1\u63d0\u51fa\u7684\u5206\u6578\u71b5\u96e2\u6563\u64f4\u6563 (SEDD) \u65b9\u6cd5\uff0c\u4e26\u5c55\u793a\u5b83\u662f\u4e00\u7a2e\u5f88\u6709\u524d\u9014\u7684\u81ea\u8ff4\u6b78\u751f\u6210\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5b83\u4e5f\u6709\u4e00\u4e9b\u7f3a\u9ede\u3002\u6211\u5011\u4ee5\u7d93\u9a57\u65b9\u5f0f\u8b49\u660e\u4e86 SEDD \u7684\u512a\u9ede\u548c\u6311\u6230\uff0c\u4e26\u89c0\u5bdf\u5230 SEDD \u901a\u5e38\u5728\u56f0\u60d1\u5ea6\u548c HellaSwag\u3001Arc \u6216 WinoGrande \u7b49\u57fa\u6e96\u4e0a\u8207\u81ea\u8ff4\u6b78\u6a21\u578b\u76f8\u5339\u914d\u3002\u6b64\u5916\uff0c\u6211\u5011\u8868\u660e\u5728\u63a8\u7406\u5ef6\u9072\u65b9\u9762\uff0cSEDD \u7684\u6548\u7387\u53ef\u4ee5\u6bd4 GPT-2 \u9ad8\u9054 4.5 \u500d\u3002\u96d6\u7136 SEDD \u5141\u8a31\u5728\u4efb\u610f\u4f4d\u7f6e\u5c0d\u6b0a\u6756\u9032\u884c\u689d\u4ef6\u8a2d\u5b9a\uff0c\u4f46\u5c0d\u65bc\u7d66\u5b9a\u7c21\u77ed\u63d0\u793a\u7684\u689d\u4ef6\u751f\u6210\uff0cSEDD \u4f3c\u4e4e\u6bd4 GPT-2 \u7a0d\u5f31\u3002\u6700\u5f8c\uff0c\u6211\u5011\u91cd\u73fe\u4e86\u539f\u59cb SEDD \u8ad6\u6587\u4e2d\u7684\u4e3b\u8981\u7d50\u679c\u3002</paragraph>", "author": "Justin Deschenaux et.al.", "authors": "Justin Deschenaux, Caglar Gulcehre", "id": "2406.11473v1", "paper_url": "http://arxiv.org/abs/2406.11473v1", "repo": "null"}}