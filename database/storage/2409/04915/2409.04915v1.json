{"2409.04915": {"publish_time": "2024-09-07", "title": "Activation Function Optimization Scheme for Image Classification", "paper_summary": "Activation function has a significant impact on the dynamics, convergence,\nand performance of deep neural networks. The search for a consistent and\nhigh-performing activation function has always been a pursuit during deep\nlearning model development. Existing state-of-the-art activation functions are\nmanually designed with human expertise except for Swish. Swish was developed\nusing a reinforcement learning-based search strategy. In this study, we propose\nan evolutionary approach for optimizing activation functions specifically for\nimage classification tasks, aiming to discover functions that outperform\ncurrent state-of-the-art options. Through this optimization framework, we\nobtain a series of high-performing activation functions denoted as Exponential\nError Linear Unit (EELU). The developed activation functions are evaluated for\nimage classification tasks from two perspectives: (1) five state-of-the-art\nneural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and\nCompact Convolutional Transformer which cover computationally heavy to light\nneural networks, and (2) eight standard datasets, including CIFAR10,\nImagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15,\nand TinyImageNet which cover from typical machine vision benchmark,\nagricultural image applications to medical image applications. Finally, we\nstatistically investigate the generalization of the resultant activation\nfunctions developed through the optimization scheme. With a Friedman test, we\nconclude that the optimization scheme is able to generate activation functions\nthat outperform the existing standard ones in 92.8% cases among 28 different\ncases studied, and $-x\\cdot erf(e^{-x})$ is found to be the best activation\nfunction for image classification generated by the optimization scheme.", "paper_summary_zh": "<paragraph>\u6fc0\u6d3b\u51fd\u6578\u5c0d\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u7684\u52d5\u614b\u3001\u6536\u6582\u548c\u6548\u80fd\u6709\u986f\u8457\u7684\u5f71\u97ff\u3002\u5728\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u958b\u767c\u904e\u7a0b\u4e2d\uff0c\u4e00\u76f4\u81f4\u529b\u65bc\u5c0b\u627e\u4e00\u81f4\u4e14\u6548\u80fd\u9ad8\u7684\u6fc0\u6d3b\u51fd\u6578\u3002\u73fe\u6709\u7684\u6700\u5148\u9032\u6fc0\u6d3b\u51fd\u6578\uff0c\u9664\u4e86 Swish \u4e4b\u5916\uff0c\u90fd\u662f\u7531\u4eba\u985e\u5c08\u5bb6\u624b\u52d5\u8a2d\u8a08\u7684\u3002Swish \u662f\u4f7f\u7528\u57fa\u65bc\u5f37\u5316\u5b78\u7fd2\u7684\u641c\u5c0b\u7b56\u7565\u958b\u767c\u7684\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u6f14\u5316\u65b9\u6cd5\uff0c\u5c08\u9580\u91dd\u5c0d\u5716\u50cf\u5206\u985e\u4efb\u52d9\u6700\u4f73\u5316\u6fc0\u6d3b\u51fd\u6578\uff0c\u65e8\u5728\u767c\u73fe\u6548\u80fd\u512a\u65bc\u73fe\u6709\u6700\u5148\u9032\u9078\u9805\u7684\u51fd\u6578\u3002\u900f\u904e\u9019\u500b\u6700\u4f73\u5316\u67b6\u69cb\uff0c\u6211\u5011\u7372\u5f97\u4e86\u4e00\u7cfb\u5217\u6548\u80fd\u9ad8\u7684\u6fc0\u6d3b\u51fd\u6578\uff0c\u8868\u793a\u70ba\u6307\u6578\u8aa4\u5dee\u7dda\u6027\u55ae\u5143 (EELU)\u3002\u5df2\u91dd\u5c0d\u5169\u500b\u89c0\u9ede\u8a55\u4f30\u5df2\u958b\u767c\u7684\u6fc0\u6d3b\u51fd\u6578\uff0c\u7528\u65bc\u5716\u50cf\u5206\u985e\u4efb\u52d9\uff1a(1) \u4e94\u7a2e\u6700\u5148\u9032\u7684\u795e\u7d93\u7db2\u8def\u67b6\u69cb\uff0c\u4f8b\u5982 ResNet50\u3001AlexNet\u3001VGG16\u3001MobileNet \u548c Compact Convolutional Transformer\uff0c\u6db5\u84cb\u5f9e\u8a08\u7b97\u91cf\u91cd\u7684\u5230\u8f15\u91cf\u7684\u7db2\u8def\uff0c(2) \u516b\u500b\u6a19\u6e96\u8cc7\u6599\u96c6\uff0c\u5305\u62ec CIFAR10\u3001Imagenette\u3001MNIST\u3001Fashion MNIST\u3001Beans\u3001Colorectal Histology\u3001CottonWeedID15 \u548c TinyImageNet\uff0c\u6db5\u84cb\u5f9e\u5178\u578b\u7684\u6a5f\u5668\u8996\u89ba\u57fa\u6e96\u3001\u8fb2\u696d\u5f71\u50cf\u61c9\u7528\u5230\u91ab\u5b78\u5f71\u50cf\u61c9\u7528\u3002\u6700\u5f8c\uff0c\u6211\u5011\u7d71\u8a08\u8abf\u67e5\u4e86\u900f\u904e\u6700\u4f73\u5316\u65b9\u6848\u958b\u767c\u7684\u7d50\u679c\u6fc0\u6d3b\u51fd\u6578\u7684\u6982\u5316\u3002\u900f\u904e Friedman \u6aa2\u5b9a\uff0c\u6211\u5011\u5f97\u51fa\u7d50\u8ad6\uff0c\u6700\u4f73\u5316\u65b9\u6848\u80fd\u5920\u7522\u751f\u5728 28 \u500b\u4e0d\u540c\u7684\u7814\u7a76\u6848\u4f8b\u4e2d\uff0c\u6709 92.8% \u7684\u6848\u4f8b\u6548\u80fd\u512a\u65bc\u73fe\u6709\u6a19\u6e96\u51fd\u6578\uff0c\u4e26\u4e14\u767c\u73fe $-x\\cdot erf(e^{-x})$ \u662f\u6700\u4f73\u5316\u65b9\u6848\u7522\u751f\u7684\u6700\u4f73\u5f71\u50cf\u5206\u985e\u6fc0\u6d3b\u51fd\u6578\u3002</paragraph>", "author": "Abdur Rahman et.al.", "authors": "Abdur Rahman, Lu He, Haifeng Wang", "id": "2409.04915v1", "paper_url": "http://arxiv.org/abs/2409.04915v1", "repo": "null"}}