{"2405.17052": {"publish_time": "2024-05-27", "title": "SelfCP: Compressing Long Prompt to 1/12 Using the Frozen Large Language Model Itself", "paper_summary": "Long prompt leads to huge hardware costs when using Large Language Models\n(LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce\nlong task-inputs, and the wide application of in-context learning easily makes\nthe prompt length explode. Inspired by the language understanding ability of\nLLMs, this paper proposes SelfCP, which uses the LLM \\textbf{itself} to\n\\textbf{C}ompress long \\textbf{P}rompt into compact virtual tokens. SelfCP\napplies a general frozen LLM twice, first as an encoder to compress the prompt\nand then as a decoder to generate responses. Specifically, given a long prompt,\nwe place special tokens within the lengthy segment for compression and signal\nthe LLM to generate $k$ virtual tokens. Afterward, the virtual tokens\nconcatenate with the uncompressed prompt and are fed into the same LLM to\ngenerate the response. In general, SelfCP facilitates the unconditional and\nconditional compression of prompts, fitting both standard tasks and those with\nspecific objectives. Since the encoder and decoder are frozen, SelfCP only\ncontains 17M trainable parameters and allows for convenient adaptation across\nvarious backbones. We implement SelfCP with two LLM backbones and evaluate it\nin both in- and out-domain tasks. Results show that the compressed virtual\ntokens can substitute $12 \\times$ larger original prompts effectively", "paper_summary_zh": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u65f6\uff0c\u957f\u63d0\u793a\u4f1a\u5bfc\u81f4\u5de8\u5927\u7684\u786c\u4ef6\u6210\u672c\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u8bb8\u591a\u4efb\u52a1\uff08\u4f8b\u5982\u6458\u8981\uff09\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5f15\u5165\u957f\u4efb\u52a1\u8f93\u5165\uff0c\u5e76\u4e14\u4e0a\u4e0b\u6587\u4e2d\u5b66\u4e60\u7684\u5e7f\u6cdb\u5e94\u7528\u5f88\u5bb9\u6613\u4f7f\u63d0\u793a\u957f\u5ea6\u7206\u70b8\u3002\u53d7 LLM \u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86 SelfCP\uff0c\u5b83\u4f7f\u7528 LLM \u672c\u8eab\u5c06\u957f\u63d0\u793a\u538b\u7f29\u6210\u7d27\u51d1\u7684\u865a\u62df\u6807\u8bb0\u3002SelfCP \u4e24\u6b21\u5e94\u7528\u901a\u7528\u7684\u51bb\u7ed3 LLM\uff0c\u9996\u5148\u4f5c\u4e3a\u7f16\u7801\u5668\u6765\u538b\u7f29\u63d0\u793a\uff0c\u7136\u540e\u4f5c\u4e3a\u89e3\u7801\u5668\u6765\u751f\u6210\u54cd\u5e94\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u957f\u63d0\u793a\uff0c\u6211\u4eec\u5728\u5197\u957f\u7684\u538b\u7f29\u6bb5\u4e2d\u653e\u7f6e\u7279\u6b8a\u6807\u8bb0\uff0c\u5e76\u53d1\u51fa\u4fe1\u53f7\u8ba9 LLM \u751f\u6210 k \u4e2a\u865a\u62df\u6807\u8bb0\u3002\u4e4b\u540e\uff0c\u865a\u62df\u6807\u8bb0\u4e0e\u672a\u538b\u7f29\u7684\u63d0\u793a\u8fde\u63a5\uff0c\u5e76\u9988\u9001\u5230\u76f8\u540c\u7684 LLM \u4ee5\u751f\u6210\u54cd\u5e94\u3002\u901a\u5e38\uff0cSelfCP \u4fc3\u8fdb\u4e86\u63d0\u793a\u7684\u65e0\u6761\u4ef6\u548c\u6761\u4ef6\u538b\u7f29\uff0c\u65e2\u9002\u7528\u4e8e\u6807\u51c6\u4efb\u52a1\uff0c\u4e5f\u9002\u7528\u4e8e\u5177\u6709\u7279\u5b9a\u76ee\u6807\u7684\u4efb\u52a1\u3002\u7531\u4e8e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u88ab\u51bb\u7ed3\uff0cSelfCP \u4ec5\u5305\u542b 17M \u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5e76\u5141\u8bb8\u8de8\u5404\u79cd\u4e3b\u5e72\u8fdb\u884c\u65b9\u4fbf\u7684\u9002\u5e94\u3002\u6211\u4eec\u4f7f\u7528\u4e24\u4e2a LLM \u4e3b\u5e72\u5b9e\u73b0\u4e86 SelfCP\uff0c\u5e76\u5728\u57df\u5185\u548c\u57df\u5916\u4efb\u52a1\u4e2d\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u538b\u7f29\u7684\u865a\u62df\u6807\u8bb0\u53ef\u4ee5\u6709\u6548\u5730\u66ff\u4ee3\u5927 12 \u500d\u7684\u539f\u59cb\u63d0\u793a", "author": "Jun Gao et.al.", "authors": "Jun Gao", "id": "2405.17052v1", "paper_url": "http://arxiv.org/abs/2405.17052v1", "repo": "null"}}