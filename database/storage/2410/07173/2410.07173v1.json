{"2410.07173": {"publish_time": "2024-10-09", "title": "Do better language models have crisper vision?", "paper_summary": "How well do text-only Large Language Models (LLMs) grasp the visual world? As\nLLMs are increasingly used in computer vision, addressing this question becomes\nboth fundamental and pertinent. However, existing studies have primarily\nfocused on limited scenarios, such as their ability to generate visual content\nor cluster multimodal data. To this end, we propose the Visual Text\nRepresentation Benchmark (ViTeRB) to isolate key properties that make language\nmodels well-aligned with the visual world. With this, we identify large-scale\ndecoder-based LLMs as ideal candidates for representing text in vision-centric\ncontexts, counter to the current practice of utilizing text encoders. Building\non these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.\nBy leveraging precomputable frozen features from strong vision and language\nmodels, ShareLock achieves an impressive 51% accuracy on ImageNet despite\nutilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU\nhour (or 10 hours including the precomputation of features) - orders of\nmagnitude less than prior methods. Code will be released.", "paper_summary_zh": "\u7d14\u6587\u5b57\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0d\u8996\u89ba\u4e16\u754c\u7684\u638c\u63e1\u7a0b\u5ea6\u5982\u4f55\uff1f\u7531\u65bc LLM \u5728\u96fb\u8166\u8996\u89ba\u9818\u57df\u7684\u61c9\u7528\u65e5\u76ca\u5ee3\u6cdb\uff0c\u63a2\u8a0e\u9019\u500b\u554f\u984c\u65e2\u57fa\u672c\u53c8\u5207\u5408\u6642\u5b9c\u3002\u7136\u800c\uff0c\u73fe\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6709\u9650\u7684\u5834\u666f\u4e2d\uff0c\u4f8b\u5982\u5b83\u5011\u751f\u6210\u8996\u89ba\u5167\u5bb9\u6216\u805a\u985e\u591a\u6a21\u614b\u6578\u64da\u7684\u80fd\u529b\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u8996\u89ba\u6587\u672c\u8868\u793a\u57fa\u6e96 (ViTeRB)\uff0c\u4ee5\u5206\u96e2\u51fa\u8b93\u8a9e\u8a00\u6a21\u578b\u8207\u8996\u89ba\u4e16\u754c\u7dca\u5bc6\u8cbc\u5408\u7684\u4e3b\u8981\u7279\u6027\u3002\u85c9\u6b64\uff0c\u6211\u5011\u5c07\u5927\u578b\u57fa\u65bc\u89e3\u78bc\u5668\u7684 LLM \u78ba\u5b9a\u70ba\u5728\u4ee5\u8996\u89ba\u70ba\u4e2d\u5fc3\u7684\u4e0a\u4e0b\u6587\u4e2d\u8868\u793a\u6587\u672c\u7684\u7406\u60f3\u5019\u9078\u8005\uff0c\u9019\u8207\u7576\u524d\u4f7f\u7528\u6587\u672c\u7de8\u78bc\u5668\u7684\u505a\u6cd5\u76f8\u53cd\u3002\u6839\u64da\u9019\u4e9b\u767c\u73fe\uff0c\u6211\u5011\u63d0\u51fa\u4e86 ShareLock\uff0c\u4e00\u500b\u8d85\u8f15\u91cf\u7d1a\u7684 CLIP \u985e\u4f3c\u6a21\u578b\u3002\u901a\u904e\u5229\u7528\u4f86\u81ea\u5f37\u5927\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b\u7684\u9810\u5148\u53ef\u8a08\u7b97\u51cd\u7d50\u7279\u5fb5\uff0cShareLock \u5728 ImageNet \u4e0a\u9054\u5230\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 51% \u6e96\u78ba\u7387\uff0c\u5118\u7ba1\u50c5\u4f7f\u7528\u4e86 563k \u5716\u50cf\u6a19\u984c\u5c0d\u3002\u6b64\u5916\uff0c\u8a13\u7df4\u50c5\u9700\u8981 1 \u500b GPU \u5c0f\u6642\uff08\u6216\u5305\u62ec\u7279\u5fb5\u9810\u8a08\u7b97\u5728\u5167\u7684 10 \u5c0f\u6642\uff09\u2014\u2014\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u5c11\u5e7e\u500b\u6578\u91cf\u7d1a\u3002\u7a0b\u5f0f\u78bc\u5c07\u6703\u767c\u5e03\u3002", "author": "Jona Ruthardt et.al.", "authors": "Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano", "id": "2410.07173v1", "paper_url": "http://arxiv.org/abs/2410.07173v1", "repo": "null"}}