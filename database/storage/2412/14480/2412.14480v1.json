{"2412.14480": {"publish_time": "2024-12-19", "title": "GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering", "paper_summary": "In Embodied Question Answering (EQA), agents must explore and develop a\nsemantic understanding of an unseen environment in order to answer a situated\nquestion with confidence. This remains a challenging problem in robotics, due\nto the difficulties in obtaining useful semantic representations, updating\nthese representations online, and leveraging prior world knowledge for\nefficient exploration and planning. Aiming to address these limitations, we\npropose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic\nscene graphs (3DSGs) and task relevant images as multi-modal memory for\ngrounding Vision-Language Models (VLMs) to perform EQA tasks in unseen\nenvironments. We employ a hierarchical planning approach that exploits the\nhierarchical nature of 3DSGs for structured planning and semantic-guided\nexploration. Through experiments in simulation on the HM-EQA dataset and in the\nreal world in home and office environments, we demonstrate that our method\noutperforms key baselines by completing EQA tasks with higher success rates and\nfewer planning steps.", "paper_summary_zh": "\u5728\u5177\u8eab\u554f\u7b54 (EQA) \u4e2d\uff0c\u4ee3\u7406\u5fc5\u9808\u63a2\u7d22\u4e26\u767c\u5c55\u5c0d\u672a\u898b\u904e\u74b0\u5883\u7684\u8a9e\u7fa9\u7406\u89e3\uff0c\u624d\u80fd\u6709\u4fe1\u5fc3\u5730\u56de\u7b54\u60c5\u5883\u554f\u984c\u3002\u7531\u65bc\u96e3\u4ee5\u53d6\u5f97\u6709\u7528\u7684\u8a9e\u7fa9\u8868\u793a\u3001\u7dda\u4e0a\u66f4\u65b0\u9019\u4e9b\u8868\u793a\uff0c\u4ee5\u53ca\u5229\u7528\u5148\u524d\u7684\u4e16\u754c\u77e5\u8b58\u9032\u884c\u6709\u6548\u7387\u7684\u63a2\u7d22\u548c\u898f\u5283\uff0c\u9019\u5728\u6a5f\u5668\u4eba\u5b78\u4e2d\u4ecd\u7136\u662f\u4e00\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u554f\u984c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa GraphEQA\uff0c\u4e00\u7a2e\u5229\u7528\u5373\u6642 3D \u5ea6\u91cf\u8a9e\u7fa9\u5834\u666f\u5716 (3DSG) \u548c\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u5f71\u50cf\u4f5c\u70ba\u591a\u6a21\u5f0f\u8a18\u61b6\u9ad4\u7684\u65b0\u7a4e\u65b9\u6cd5\uff0c\u4ee5\u63a5\u5730\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4f86\u57f7\u884c\u672a\u898b\u904e\u74b0\u5883\u4e2d\u7684 EQA \u4efb\u52d9\u3002\u6211\u5011\u63a1\u7528\u5206\u5c64\u898f\u5283\u65b9\u6cd5\uff0c\u5229\u7528 3DSG \u7684\u5206\u5c64\u6027\u8cea\u9032\u884c\u7d50\u69cb\u5316\u898f\u5283\u548c\u8a9e\u7fa9\u5f15\u5c0e\u63a2\u7d22\u3002\u900f\u904e\u5728 HM-EQA \u8cc7\u6599\u96c6\u4e0a\u7684\u6a21\u64ec\u5be6\u9a57\uff0c\u4ee5\u53ca\u5728\u5bb6\u5ead\u548c\u8fa6\u516c\u5ba4\u74b0\u5883\u4e2d\u7684\u771f\u5be6\u4e16\u754c\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u6211\u5011\u7684\u6a21\u578b\u900f\u904e\u4ee5\u8f03\u9ad8\u7684\u6210\u529f\u7387\u548c\u8f03\u5c11\u7684\u898f\u5283\u6b65\u9a5f\u5b8c\u6210 EQA \u4efb\u52d9\uff0c\u512a\u65bc\u4e3b\u8981\u7684\u57fa\u7dda\u3002", "author": "Saumya Saxena et.al.", "authors": "Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer", "id": "2412.14480v1", "paper_url": "http://arxiv.org/abs/2412.14480v1", "repo": "null"}}