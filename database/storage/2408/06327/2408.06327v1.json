{"2408.06327": {"publish_time": "2024-08-12", "title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents", "paper_summary": "Large Multimodal Models (LMMs) have ushered in a new era in artificial\nintelligence, merging capabilities in both language and vision to form highly\ncapable Visual Foundation Agents. These agents are postulated to excel across a\nmyriad of tasks, potentially approaching general artificial intelligence.\nHowever, existing benchmarks fail to sufficiently challenge or showcase the\nfull potential of LMMs in complex, real-world environments. To address this\ngap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering\nbenchmark specifically designed to train and evaluate LMMs as visual foundation\nagents across diverse scenarios, including Embodied, Graphical User Interface,\nand Visual Design, with tasks formulated to probe the depth of LMMs'\nunderstanding and interaction capabilities. Through rigorous testing across\nnine proprietary LMM APIs and eight open models, we demonstrate the\nconsiderable yet still developing agent capabilities of these models.\nAdditionally, VAB constructs a trajectory training set constructed through\nhybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and\nHuman Demonstrations, promoting substantial performance improvements in LMMs\nthrough behavior cloning. Our work not only aims to benchmark existing models\nbut also provides a solid foundation for future development into visual\nfoundation agents. Code, train \\& test data, and part of fine-tuned open LMMs\nare available at \\url{https://github.com/THUDM/VisualAgentBench}.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u958b\u555f\u4e86\u4eba\u5de5\u667a\u6167\u7684\u65b0\u7d00\u5143\uff0c\u7d50\u5408\u8a9e\u8a00\u548c\u8996\u89ba\u80fd\u529b\uff0c\u5f62\u6210\u529f\u80fd\u5f37\u5927\u7684\u8996\u89ba\u57fa\u790e\u4ee3\u7406\u3002\u9019\u4e9b\u4ee3\u7406\u88ab\u5047\u8a2d\u53ef\u4ee5\u5728\u7121\u6578\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u6f5b\u5728\u5730\u63a5\u8fd1\u4e00\u822c\u4eba\u5de5\u667a\u6167\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u6e96\u672a\u80fd\u5145\u5206\u6311\u6230\u6216\u5c55\u793a LMM \u5728\u8907\u96dc\u7684\u73fe\u5be6\u74b0\u5883\u4e2d\u7684\u5168\u90e8\u6f5b\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 VisualAgentBench (VAB)\uff0c\u4e00\u500b\u5168\u9762\u4e14\u958b\u5275\u6027\u7684\u57fa\u6e96\uff0c\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u8a13\u7df4\u548c\u8a55\u4f30 LMM \u4f5c\u70ba\u8996\u89ba\u57fa\u790e\u4ee3\u7406\uff0c\u6db5\u84cb\u5404\u7a2e\u5834\u666f\uff0c\u5305\u62ec\u5177\u8eab\u3001\u5716\u5f62\u4f7f\u7528\u8005\u4ecb\u9762\u548c\u8996\u89ba\u8a2d\u8a08\uff0c\u4efb\u52d9\u65e8\u5728\u63a2\u8a0e LMM \u7684\u7406\u89e3\u548c\u4e92\u52d5\u80fd\u529b\u7684\u6df1\u5ea6\u3002\u900f\u904e\u5c0d\u4e5d\u500b\u5c08\u6709 LMM API \u548c\u516b\u500b\u958b\u653e\u6a21\u578b\u9032\u884c\u56b4\u683c\u6e2c\u8a66\uff0c\u6211\u5011\u5c55\u793a\u4e86\u9019\u4e9b\u6a21\u578b\u76f8\u7576\u5927\u4f46\u4ecd\u5728\u767c\u5c55\u7684\u4ee3\u7406\u80fd\u529b\u3002\u6b64\u5916\uff0cVAB \u69cb\u5efa\u4e86\u4e00\u500b\u901a\u904e\u6df7\u5408\u65b9\u6cd5\u69cb\u5efa\u7684\u8ecc\u8de1\u8a13\u7df4\u96c6\uff0c\u5305\u62ec\u57fa\u65bc\u7a0b\u5f0f\u7684\u6c42\u89e3\u5668\u3001LMM \u4ee3\u7406\u81ea\u8209\u548c\u4eba\u985e\u793a\u7bc4\uff0c\u900f\u904e\u884c\u70ba\u8907\u88fd\u63d0\u5347 LMM \u7684\u986f\u8457\u6548\u80fd\u3002\u6211\u5011\u7684\u7814\u7a76\u4e0d\u50c5\u65e8\u5728\u8a55\u91cf\u73fe\u6709\u6a21\u578b\uff0c\u4e5f\u70ba\u672a\u4f86\u767c\u5c55\u6210\u8996\u89ba\u57fa\u790e\u4ee3\u7406\u5960\u5b9a\u4e86\u7a69\u56fa\u7684\u57fa\u790e\u3002\u7a0b\u5f0f\u78bc\u3001\u8a13\u7df4\u548c\u6e2c\u8a66\u8cc7\u6599\uff0c\u4ee5\u53ca\u90e8\u5206\u5fae\u8abf\u7684\u958b\u653e LMM \u53ef\u5728 \\url{https://github.com/THUDM/VisualAgentBench} \u53d6\u5f97\u3002", "author": "Xiao Liu et.al.", "authors": "Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, Jie Tang", "id": "2408.06327v1", "paper_url": "http://arxiv.org/abs/2408.06327v1", "repo": "https://github.com/thudm/visualagentbench"}}