{"2412.18279": {"publish_time": "2024-12-24", "title": "Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization", "paper_summary": "The role of reinforcement learning (RL) in enhancing the reasoning of large\nlanguage models (LLMs) is becoming increasingly significant. Despite the\nsuccess of RL in many scenarios, there are still many challenges in improving\nthe reasoning of LLMs. One challenge is the sparse reward, which makes\noptimization difficult for RL and necessitates a large amount of data samples.\nAnother challenge stems from the inherent instability of RL, particularly when\nusing Actor-Critic (AC) methods to derive optimal policies, which often leads\nto unstable training processes. To address these issues, we introduce Direct\nAdvantage Policy Optimization (DAPO), an novel step-level offline RL algorithm.\nUnlike standard alignment that rely solely outcome rewards to optimize policies\n(such as DPO), DAPO employs a critic function to predict the reasoning accuracy\nat each step, thereby generating dense signals to refine the generation\nstrategy. Additionally, the Actor and Critic components in DAPO are trained\nindependently, avoiding the co-training instability observed in standard AC\nalgorithms like PPO. We train DAPO on mathematical and code query datasets and\nthen evaluate its performance on multiple benchmarks. Our results show that\nDAPO can effectively enhance the mathematical and code capabilities on both SFT\nmodels and RL models, demonstrating the effectiveness of DAPO.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u5728\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u63a8\u8ad6\u4e2d\u7684\u89d2\u8272\u6b63\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u5118\u7ba1 RL \u5728\u8a31\u591a\u5834\u666f\u4e2d\u90fd\u7372\u5f97\u6210\u529f\uff0c\u4f46\u5728\u6539\u5584 LLM \u63a8\u8ad6\u65b9\u9762\u4ecd\u6709\u8a31\u591a\u6311\u6230\u3002\u5176\u4e2d\u4e00\u9805\u6311\u6230\u662f\u7a00\u758f\u734e\u52f5\uff0c\u9019\u4f7f\u5f97 RL \u7684\u6700\u4f73\u5316\u8b8a\u5f97\u56f0\u96e3\uff0c\u4e26\u4e14\u9700\u8981\u5927\u91cf\u7684\u8cc7\u6599\u6a23\u672c\u3002\u53e6\u4e00\u9805\u6311\u6230\u6e90\u65bc RL \u7684\u5167\u5728\u4e0d\u7a69\u5b9a\u6027\uff0c\u7279\u5225\u662f\u5728\u4f7f\u7528 Actor-Critic (AC) \u65b9\u6cd5\u4f86\u63a8\u5c0e\u6700\u4f73\u7b56\u7565\u6642\uff0c\u9019\u901a\u5e38\u6703\u5c0e\u81f4\u4e0d\u7a69\u5b9a\u7684\u8a13\u7df4\u904e\u7a0b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u76f4\u63a5\u512a\u52e2\u7b56\u7565\u6700\u4f73\u5316 (DAPO)\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u6b65\u9a5f\u7d1a\u96e2\u7dda RL \u6f14\u7b97\u6cd5\u3002\u8207\u50c5\u4f9d\u8cf4\u7d50\u679c\u734e\u52f5\u4f86\u6700\u4f73\u5316\u7b56\u7565\u7684\u6a19\u6e96\u5c0d\u9f4a\u4e0d\u540c\uff08\u4f8b\u5982 DPO\uff09\uff0cDAPO \u4f7f\u7528\u4e00\u500b\u8a55\u8ad6\u51fd\u6578\u4f86\u9810\u6e2c\u6bcf\u500b\u6b65\u9a5f\u7684\u63a8\u8ad6\u6e96\u78ba\u5ea6\uff0c\u5f9e\u800c\u7522\u751f\u5bc6\u96c6\u7684\u8a0a\u865f\u4f86\u512a\u5316\u751f\u6210\u7b56\u7565\u3002\u6b64\u5916\uff0cDAPO \u4e2d\u7684 Actor \u548c Critic \u7d44\u4ef6\u662f\u7368\u7acb\u8a13\u7df4\u7684\uff0c\u907f\u514d\u4e86\u5728\u6a19\u6e96 AC \u6f14\u7b97\u6cd5\u4e2d\u89c0\u5bdf\u5230\u7684\u5171\u540c\u8a13\u7df4\u4e0d\u7a69\u5b9a\u6027\uff0c\u4f8b\u5982 PPO\u3002\u6211\u5011\u5728\u6578\u5b78\u548c\u7a0b\u5f0f\u78bc\u67e5\u8a62\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4 DAPO\uff0c\u7136\u5f8c\u8a55\u4f30\u5176\u5728\u591a\u500b\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0cDAPO \u53ef\u4ee5\u6709\u6548\u63d0\u5347 SFT \u6a21\u578b\u548c RL \u6a21\u578b\u4e0a\u7684\u6578\u5b78\u548c\u7a0b\u5f0f\u78bc\u80fd\u529b\uff0c\u8b49\u660e\u4e86 DAPO \u7684\u6709\u6548\u6027\u3002", "author": "Jiacai Liu et.al.", "authors": "Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, Yahui Zhou", "id": "2412.18279v1", "paper_url": "http://arxiv.org/abs/2412.18279v1", "repo": "null"}}