{"2412.16120": {"publish_time": "2024-12-20", "title": "PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics", "paper_summary": "Evaluating the quality of machine-generated natural language content is a\nchallenging task in Natural Language Processing (NLP). Recently, large language\nmodels (LLMs) like GPT-4 have been employed for this purpose, but they are\ncomputationally expensive due to the extensive token usage required by complex\nevaluation prompts. In this paper, we propose a prompt optimization approach\nthat uses a smaller, fine-tuned language model to compress input data for\nevaluation prompt, thus reducing token usage and computational cost when using\nlarger LLMs for downstream evaluation. Our method involves a two-stage\nfine-tuning process: supervised fine-tuning followed by preference optimization\nto refine the model's outputs based on human preferences. We focus on Machine\nTranslation (MT) evaluation and utilize the GEMBA-MQM metric as a starting\npoint. Our results show a $2.37\\times$ reduction in token usage without any\nloss in evaluation quality. This work makes state-of-the-art LLM-based metrics\nlike GEMBA-MQM more cost-effective and efficient, enhancing their accessibility\nfor broader use.", "paper_summary_zh": "\u8a55\u4f30\u6a5f\u5668\u7522\u751f\u7684\u81ea\u7136\u8a9e\u8a00\u5167\u5bb9\u54c1\u8cea\u662f\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u7684\u4e00\u9805\u6311\u6230\u6027\u4efb\u52d9\u3002\u6700\u8fd1\uff0cGPT-4 \u7b49\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u88ab\u7528\u65bc\u6b64\u76ee\u7684\uff0c\u4f46\u7531\u65bc\u8907\u96dc\u7684\u8a55\u4f30\u63d0\u793a\u9700\u8981\u5927\u91cf\u4f7f\u7528\u7b26\u865f\uff0c\u56e0\u6b64\u5b83\u5011\u5728\u8a08\u7b97\u4e0a\u5f88\u6602\u8cb4\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u63d0\u793a\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u8f03\u5c0f\u7684\u5fae\u8abf\u8a9e\u8a00\u6a21\u578b\u4f86\u58d3\u7e2e\u8f38\u5165\u8cc7\u6599\u4ee5\u9032\u884c\u8a55\u4f30\u63d0\u793a\uff0c\u5f9e\u800c\u6e1b\u5c11\u4f7f\u7528\u7b26\u865f\u548c\u5728\u4f7f\u7528\u8f03\u5927\u7684 LLM \u9032\u884c\u4e0b\u6e38\u8a55\u4f30\u6642\u7684\u8a08\u7b97\u6210\u672c\u3002\u6211\u5011\u7684\u505a\u6cd5\u5305\u62ec\u4e00\u500b\u5169\u968e\u6bb5\u5fae\u8abf\u904e\u7a0b\uff1a\u76e3\u7763\u5fae\u8abf\uff0c\u7136\u5f8c\u662f\u504f\u597d\u6700\u4f73\u5316\uff0c\u4ee5\u6839\u64da\u4eba\u985e\u504f\u597d\u5fae\u8abf\u6a21\u578b\u7684\u8f38\u51fa\u3002\u6211\u5011\u5c08\u6ce8\u65bc\u6a5f\u5668\u7ffb\u8b6f (MT) \u8a55\u4f30\uff0c\u4e26\u5229\u7528 GEMBA-MQM \u6307\u6a19\u4f5c\u70ba\u8d77\u9ede\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\u7b26\u865f\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 2.37 \u500d\uff0c\u800c\u8a55\u4f30\u54c1\u8cea\u6c92\u6709\u4efb\u4f55\u640d\u5931\u3002\u9019\u9805\u5de5\u4f5c\u4f7f\u57fa\u65bc LLM \u7684\u6700\u5148\u9032\u6307\u6a19\uff08\u4f8b\u5982 GEMBA-MQM\uff09\u66f4\u5177\u6210\u672c\u6548\u76ca\u548c\u6548\u7387\uff0c\u4e26\u63d0\u9ad8\u4e86\u5b83\u5011\u5728\u66f4\u5ee3\u6cdb\u4f7f\u7528\u4e2d\u7684\u53ef\u53ca\u6027\u3002", "author": "Daniil Larionov et.al.", "authors": "Daniil Larionov, Steffen Eger", "id": "2412.16120v1", "paper_url": "http://arxiv.org/abs/2412.16120v1", "repo": "null"}}