{"2410.05080": {"publish_time": "2024-10-07", "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery", "paper_summary": "The advancements of language language models (LLMs) have piqued growing\ninterest in developing LLM-based language agents to automate scientific\ndiscovery end-to-end, which has sparked both excitement and skepticism about\nthe true capabilities of such agents. In this work, we argue that for an agent\nto fully automate scientific discovery, it must be able to complete all\nessential tasks in the workflow. Thus, we call for rigorous assessment of\nagents on individual tasks in a scientific workflow before making bold claims\non end-to-end automation. To this end, we present ScienceAgentBench, a new\nbenchmark for evaluating language agents for data-driven scientific discovery.\nTo ensure the scientific authenticity and real-world relevance of our\nbenchmark, we extract 102 tasks from 44 peer-reviewed publications in four\ndisciplines and engage nine subject matter experts to validate them. We unify\nthe target output for every task to a self-contained Python program file and\nemploy an array of evaluation metrics to examine the generated programs,\nexecution results, and costs. Each task goes through multiple rounds of manual\nvalidation by annotators and subject matter experts to ensure its annotation\nquality and scientific plausibility. We also propose two effective strategies\nto mitigate data contamination concerns. Using our benchmark, we evaluate five\nopen-weight and proprietary LLMs, each with three frameworks: direct prompting,\nOpenHands, and self-debug. Given three attempts for each task, the\nbest-performing agent can only solve 32.4% of the tasks independently and 34.3%\nwith expert-provided knowledge. These results underscore the limited capacities\nof current language agents in generating code for data-driven discovery, let\nalone end-to-end automation for scientific research.", "paper_summary_zh": "<paragraph>\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9032\u6b65\u5f15\u8d77\u4eba\u5011\u5c0d\u958b\u767c\u57fa\u65bc LLM \u7684\u8a9e\u8a00\u4ee3\u7406\u4ee5\u81ea\u52d5\u5316\u79d1\u5b78\u767c\u73fe\u7684\u8208\u8da3\uff0c\u9019\u5f15\u8d77\u4e86\u4eba\u5011\u5c0d\u6b64\u985e\u4ee3\u7406\u7684\u771f\u6b63\u80fd\u529b\u7684\u8208\u596e\u548c\u61f7\u7591\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a8d\u70ba\uff0c\u8981\u8b93\u4ee3\u7406\u5b8c\u5168\u81ea\u52d5\u5316\u79d1\u5b78\u767c\u73fe\uff0c\u5b83\u5fc5\u9808\u80fd\u5920\u5b8c\u6210\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6240\u6709\u57fa\u672c\u4efb\u52d9\u3002\u56e0\u6b64\uff0c\u6211\u5011\u547c\u7c72\u5728\u5c0d\u79d1\u5b78\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u500b\u5225\u4efb\u52d9\u9032\u884c\u56b4\u683c\u8a55\u4f30\u5f8c\uff0c\u518d\u5c0d\u7aef\u5230\u7aef\u81ea\u52d5\u5316\u63d0\u51fa\u5927\u81bd\u4e3b\u5f35\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86 ScienceAgentBench\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7684\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30\u6578\u64da\u9a45\u52d5\u79d1\u5b78\u767c\u73fe\u7684\u8a9e\u8a00\u4ee3\u7406\u3002\u70ba\u4e86\u78ba\u4fdd\u6211\u5011\u57fa\u6e96\u7684\u79d1\u5b78\u771f\u5be6\u6027\u548c\u73fe\u5be6\u4e16\u754c\u76f8\u95dc\u6027\uff0c\u6211\u5011\u5f9e\u56db\u500b\u5b78\u79d1\u7684 44 \u7bc7\u540c\u884c\u8a55\u5be9\u51fa\u7248\u7269\u4e2d\u63d0\u53d6\u4e86 102 \u9805\u4efb\u52d9\uff0c\u4e26\u8058\u8acb\u4e86\u4e5d\u4f4d\u4e3b\u984c\u5c08\u5bb6\u5c0d\u5176\u9032\u884c\u9a57\u8b49\u3002\u6211\u5011\u5c07\u6bcf\u500b\u4efb\u52d9\u7684\u76ee\u6a19\u8f38\u51fa\u7d71\u4e00\u70ba\u4e00\u500b\u7368\u7acb\u7684 Python \u7a0b\u5e8f\u6587\u4ef6\uff0c\u4e26\u63a1\u7528\u4e00\u7cfb\u5217\u8a55\u4f30\u6307\u6a19\u4f86\u6aa2\u67e5\u751f\u6210\u7684\u7a0b\u5e8f\u3001\u57f7\u884c\u7d50\u679c\u548c\u6210\u672c\u3002\u6bcf\u500b\u4efb\u52d9\u90fd\u7d93\u904e\u8a3b\u91cb\u8005\u548c\u4e3b\u984c\u5c08\u5bb6\u591a\u8f2a\u624b\u52d5\u9a57\u8b49\uff0c\u4ee5\u78ba\u4fdd\u5176\u8a3b\u91cb\u8cea\u91cf\u548c\u79d1\u5b78\u5408\u7406\u6027\u3002\u6211\u5011\u9084\u63d0\u51fa\u4e86\u5169\u7a2e\u6709\u6548\u7684\u7b56\u7565\u4f86\u6e1b\u8f15\u6578\u64da\u6c61\u67d3\u554f\u984c\u3002\u4f7f\u7528\u6211\u5011\u7684\u57fa\u6e96\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u4e94\u500b\u958b\u653e\u6b0a\u91cd\u548c\u5c08\u6709 LLM\uff0c\u6bcf\u500b LLM \u6709\u4e09\u500b\u6846\u67b6\uff1a\u76f4\u63a5\u63d0\u793a\u3001OpenHands \u548c\u81ea\u8abf\u8a66\u3002\u7d66\u5b9a\u6bcf\u500b\u4efb\u52d9\u4e09\u6b21\u5617\u8a66\uff0c\u8868\u73fe\u6700\u597d\u7684\u4ee3\u7406\u53ea\u80fd\u7368\u7acb\u89e3\u6c7a 32.4% \u7684\u4efb\u52d9\uff0c\u5728\u5c08\u5bb6\u63d0\u4f9b\u7684\u77e5\u8b58\u4e0b\u53ef\u4ee5\u89e3\u6c7a 34.3% \u7684\u4efb\u52d9\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u8abf\u4e86\u7576\u524d\u8a9e\u8a00\u4ee3\u7406\u5728\u70ba\u6578\u64da\u9a45\u52d5\u767c\u73fe\u751f\u6210\u4ee3\u78bc\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\uff0c\u66f4\u4e0d\u7528\u8aaa\u79d1\u5b78\u7814\u7a76\u7684\u7aef\u5230\u7aef\u81ea\u52d5\u5316\u4e86\u3002</paragraph>", "author": "Ziru Chen et.al.", "authors": "Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun", "id": "2410.05080v1", "paper_url": "http://arxiv.org/abs/2410.05080v1", "repo": "null"}}