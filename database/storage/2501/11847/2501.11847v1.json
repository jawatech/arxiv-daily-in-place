{"2501.11847": {"publish_time": "2025-01-21", "title": "A Survey on Memory-Efficient Large-Scale Model Training in AI for Science", "paper_summary": "Scientific research faces high costs and inefficiencies with traditional\nmethods, but the rise of deep learning and large language models (LLMs) offers\ninnovative solutions. This survey reviews LLM applications across scientific\nfields such as biology, medicine, chemistry, and meteorology, underscoring\ntheir role in advancing research. However, the continuous expansion of model\nsize has led to significant memory demands, hindering further development and\napplication of LLMs for science. To address this, we review memory-efficient\ntraining techniques for LLMs based on the transformer architecture, including\ndistributed training, mixed precision training, and gradient checkpointing.\nUsing AlphaFold 2 as an example, we demonstrate how tailored memory\noptimization methods can reduce storage needs while preserving prediction\naccuracy. We also discuss the challenges of memory optimization in practice and\npotential future directions, hoping to provide valuable insights for\nresearchers and engineers.", "paper_summary_zh": "\u79d1\u5b78\u7814\u7a76\u9762\u81e8\u50b3\u7d71\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u548c\u4f4e\u6548\u7387\uff0c\u4f46\u6df1\u5ea6\u5b78\u7fd2\u548c\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8208\u8d77\u63d0\u4f9b\u4e86\u5275\u65b0\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u6b64\u8abf\u67e5\u56de\u9867\u4e86 LLM \u5728\u751f\u7269\u5b78\u3001\u91ab\u5b78\u3001\u5316\u5b78\u548c\u6c23\u8c61\u5b78\u7b49\u79d1\u5b78\u9818\u57df\u7684\u61c9\u7528\uff0c\u5f37\u8abf\u5b83\u5011\u5728\u63a8\u9032\u7814\u7a76\u4e2d\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u6a21\u578b\u898f\u6a21\u7684\u6301\u7e8c\u64f4\u5c55\u5c0e\u81f4\u4e86\u986f\u8457\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u963b\u7919\u4e86 LLM \u5728\u79d1\u5b78\u9818\u57df\u7684\u9032\u4e00\u6b65\u958b\u767c\u548c\u61c9\u7528\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u56de\u9867\u4e86\u57fa\u65bcTransformer\u67b6\u69cb\u7684 LLM \u7684\u8a18\u61b6\u9ad4\u9ad8\u6548\u8a13\u7df4\u6280\u8853\uff0c\u5305\u62ec\u5206\u4f48\u5f0f\u8a13\u7df4\u3001\u6df7\u5408\u7cbe\u5ea6\u8a13\u7df4\u548c\u68af\u5ea6\u6aa2\u67e5\u9ede\u3002\u4ee5 AlphaFold 2 \u70ba\u4f8b\uff0c\u6211\u5011\u5c55\u793a\u4e86\u91cf\u8eab\u5b9a\u5236\u7684\u8a18\u61b6\u9ad4\u512a\u5316\u65b9\u6cd5\u5982\u4f55\u5728\u4fdd\u6301\u9810\u6e2c\u6e96\u78ba\u6027\u7684\u540c\u6642\u6e1b\u5c11\u5132\u5b58\u9700\u6c42\u3002\u6211\u5011\u9084\u8a0e\u8ad6\u4e86\u8a18\u61b6\u9ad4\u512a\u5316\u5728\u5be6\u52d9\u4e2d\u7684\u6311\u6230\u548c\u6f5b\u5728\u7684\u672a\u4f86\u65b9\u5411\uff0c\u5e0c\u671b\u80fd\u70ba\u7814\u7a76\u4eba\u54e1\u548c\u5de5\u7a0b\u5e2b\u63d0\u4f9b\u6709\u50f9\u503c\u7684\u898b\u89e3\u3002", "author": "Kaiyuan Tian et.al.", "authors": "Kaiyuan Tian, Linbo Qiao, Baihui Liu, Gongqingjian Jiang, Dongsheng Li", "id": "2501.11847v1", "paper_url": "http://arxiv.org/abs/2501.11847v1", "repo": "null"}}