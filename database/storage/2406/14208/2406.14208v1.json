{"2406.14208": {"publish_time": "2024-06-20", "title": "SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots", "paper_summary": "Previous studies have shown that demonstrations can significantly help Large\nLanguage Models (LLMs ) perform better on the given tasks. However, this\nso-called In-Context Learning ( ICL ) ability is very sensitive to the\npresenting context, and often dozens of demonstrations are needed. In this\nwork, we investigate if we can reduce the shot number while still maintaining a\ncompetitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD\n) training framework that aligns the student model with a heavily prompted\nvariation, thereby increasing the utilization of a single demonstration. We\nexperiment with the SeCoKD across three LLMs and six benchmarks focusing mainly\non reasoning tasks. Results show that our method outperforms the base model and\nSupervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings\nby 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts\nwhen evaluated on new tasks, which is more robust than Supervised Fine-tuning.", "paper_summary_zh": "\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u793a\u7bc4\u53ef\u4ee5\u986f\u8457\u5e6b\u52a9\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u65e2\u5b9a\u4efb\u52d9\u4e0a\u8868\u73fe\u5f97\u66f4\u597d\u3002\u7136\u800c\uff0c\u9019\u7a2e\u6240\u8b02\u7684\u300c\u60c5\u5883\u4e2d\u5b78\u7fd2\u300d(ICL) \u80fd\u529b\u5c0d\u5448\u73fe\u60c5\u5883\u975e\u5e38\u654f\u611f\uff0c\u800c\u4e14\u901a\u5e38\u9700\u8981\u6578\u5341\u500b\u793a\u7bc4\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u662f\u5426\u53ef\u4ee5\u5728\u7dad\u6301\u7af6\u722d\u529b\u8868\u73fe\u7684\u540c\u6642\u6e1b\u5c11\u793a\u7bc4\u6b21\u6578\u3002\u6211\u5011\u63d0\u51fa SeCoKD\uff0c\u4e00\u7a2e\u81ea\u6211\u77e5\u8b58\u84b8\u993e (KD) \u8a13\u7df4\u67b6\u69cb\uff0c\u5b83\u5c07\u5b78\u751f\u6a21\u578b\u8207\u5927\u91cf\u63d0\u793a\u7684\u8b8a\u9ad4\u5c0d\u9f4a\uff0c\u5f9e\u800c\u589e\u52a0\u55ae\u4e00\u793a\u7bc4\u7684\u5229\u7528\u7387\u3002\u6211\u5011\u4f7f\u7528 SeCoKD \u91dd\u5c0d\u4e09\u500b LLM \u548c\u516d\u500b\u57fa\u6e96\u9032\u884c\u5be6\u9a57\uff0c\u4e3b\u8981\u5c08\u6ce8\u65bc\u63a8\u7406\u4efb\u52d9\u3002\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728\u96f6\u6b21\u793a\u7bc4\u548c\u4e00\u6b21\u793a\u7bc4\u8a2d\u7f6e\u4e2d\u5206\u5225\u512a\u65bc\u57fa\u790e\u6a21\u578b\u548c\u76e3\u7763\u5fae\u8abf (SFT) 30% \u548c 10%\u3002\u6b64\u5916\uff0cSeCoKD \u5728\u8a55\u4f30\u65b0\u4efb\u52d9\u6642\u5e7e\u4e4e\u6c92\u6709\u5e36\u4f86\u8ca0\u9762\u5f71\u97ff\uff0c\u9019\u6bd4\u76e3\u7763\u5fae\u8abf\u66f4\u5f37\u5927\u3002", "author": "Weixing Wang et.al.", "authors": "Weixing Wang, Haojin Yang, Christoph Meinel", "id": "2406.14208v1", "paper_url": "http://arxiv.org/abs/2406.14208v1", "repo": "null"}}