{"2407.19825": {"publish_time": "2024-07-29", "title": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost", "paper_summary": "Today's large language models (LLMs) can solve challenging question-answering\ntasks, and prompt engineering techniques, such as chain-of-thought (CoT), have\ngained attention for enhancing the explanation and correctness of outputs.\nNevertheless, models require significant time to generate answers augmented\nwith lengthy reasoning details. To address this issue, this paper analyzes the\nimpact of output lengths on LLM inference pipelines and proposes novel metrics\nto evaluate them in terms of \\textit{correct conciseness}. It also examines the\nimpact of controlling output length through a refined prompt engineering\nstrategy, Constrained-CoT (CCoT), which encourages the model to limit output\nlength. Experiments on pre-trained LLMs demonstrated the benefit of the\nproposed metrics and the effectiveness of CCoT across different models. For\ninstance, constraining the reasoning of LLaMA2-70b to 100 words improves the\naccuracy from 36.01\\% (CoT) to 41.07\\% (CCoT) on the GSM8K dataset, while\nreducing the average output length by 28 words.", "paper_summary_zh": "\u4eca\u65e5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u53ef\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u800c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u4f8b\u5982\u601d\u60f3\u94fe (CoT)\uff0c\u56e0\u589e\u5f3a\u8f93\u51fa\u7684\u89e3\u91ca\u548c\u6b63\u786e\u6027\u800c\u53d7\u5230\u5173\u6ce8\u3002\u7136\u800c\uff0c\u6a21\u578b\u9700\u8981\u5927\u91cf\u65f6\u95f4\u6765\u751f\u6210\u5e26\u6709\u5197\u957f\u63a8\u7406\u7ec6\u8282\u7684\u7b54\u6848\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u5206\u6790\u4e86\u8f93\u51fa\u957f\u5ea6\u5bf9 LLM \u63a8\u7406\u7ba1\u9053\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6307\u6807\u6765\u8bc4\u4f30\u5b83\u4eec\u5728\u201c\u6b63\u786e\u7b80\u6d01\u6027\u201d\u65b9\u9762\u7684\u8868\u73b0\u3002\u5b83\u8fd8\u63a2\u8ba8\u4e86\u901a\u8fc7\u6539\u8fdb\u7684\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u7ea6\u675f CoT (CCoT) \u6765\u63a7\u5236\u8f93\u51fa\u957f\u5ea6\u7684\u5f71\u54cd\uff0c\u8be5\u7b56\u7565\u9f13\u52b1\u6a21\u578b\u9650\u5236\u8f93\u51fa\u957f\u5ea6\u3002\u5bf9\u9884\u8bad\u7ec3 LLM \u7684\u5b9e\u9a8c\u8868\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6307\u6807\u7684\u597d\u5904\u548c CCoT \u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002\u4f8b\u5982\uff0c\u5c06 LLaMA2-70b \u7684\u63a8\u7406\u9650\u5236\u5728 100 \u4e2a\u5355\u8bcd\u5185\uff0c\u5c06 GSM8K \u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u5ea6\u4ece 36.01%\uff08CoT\uff09\u63d0\u9ad8\u5230 41.07%\uff08CCoT\uff09\uff0c\u540c\u65f6\u5c06\u5e73\u5747\u8f93\u51fa\u957f\u5ea6\u51cf\u5c11\u4e86 28 \u4e2a\u5355\u8bcd\u3002", "author": "Sania Nayab et.al.", "authors": "Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, Fabrizio Giacomelli", "id": "2407.19825v1", "paper_url": "http://arxiv.org/abs/2407.19825v1", "repo": "null"}}