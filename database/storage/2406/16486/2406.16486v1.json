{"2406.16486": {"publish_time": "2024-06-24", "title": "Towards Comprehensive Preference Data Collection for Reward Modeling", "paper_summary": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment\nof large language models (LLMs) with human preferences, thereby enhancing the\nquality of responses generated. A critical component of RLHF is the reward\nmodel, which is trained on preference data and outputs a scalar reward during\nthe inference stage. However, the collection of preference data still lacks\nthorough investigation. Recent studies indicate that preference data is\ncollected either by AI or humans, where chosen and rejected instances are\nidentified among pairwise responses. We question whether this process\neffectively filters out noise and ensures sufficient diversity in collected\ndata. To address these concerns, for the first time, we propose a comprehensive\nframework for preference data collection, decomposing the process into four\nincremental steps: Prompt Generation, Response Generation, Response Filtering,\nand Human Labeling. This structured approach ensures the collection of\nhigh-quality preferences while reducing reliance on human labor. We conducted\ncomprehensive experiments based on the data collected at different stages,\ndemonstrating the effectiveness of the proposed data collection method.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2\u4f86\u81ea\u4eba\u985e\u56de\u994b\uff08RLHF\uff09\u4fc3\u9032\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u8207\u4eba\u985e\u504f\u597d\u7684\u5c0d\u9f4a\uff0c\u5f9e\u800c\u63d0\u5347\u7522\u751f\u7684\u56de\u61c9\u54c1\u8cea\u3002RLHF \u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\u662f\u734e\u52f5\u6a21\u578b\uff0c\u8a72\u6a21\u578b\u6703\u91dd\u5c0d\u504f\u597d\u8cc7\u6599\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u5728\u63a8\u8ad6\u968e\u6bb5\u8f38\u51fa\u4e00\u500b\u6a19\u91cf\u734e\u52f5\u3002\u7136\u800c\uff0c\u504f\u597d\u8cc7\u6599\u7684\u6536\u96c6\u4ecd\u7136\u7f3a\u4e4f\u5fb9\u5e95\u7684\u8abf\u67e5\u3002\u6700\u8fd1\u7684\u7814\u7a76\u6307\u51fa\uff0c\u504f\u597d\u8cc7\u6599\u662f\u7531 AI \u6216\u4eba\u985e\u6536\u96c6\uff0c\u5176\u4e2d\u6703\u5f9e\u6210\u5c0d\u7684\u56de\u61c9\u4e2d\u627e\u51fa\u88ab\u9078\u64c7\u548c\u88ab\u62d2\u7d55\u7684\u5be6\u4f8b\u3002\u6211\u5011\u8cea\u7591\u9019\u500b\u904e\u7a0b\u662f\u5426\u80fd\u6709\u6548\u6ffe\u9664\u96dc\u8a0a\u4e26\u78ba\u4fdd\u6536\u96c6\u5230\u7684\u8cc7\u6599\u6709\u8db3\u5920\u7684\u591a\u6a23\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u9996\u6b21\u63d0\u51fa\u4e00\u500b\u504f\u597d\u8cc7\u6599\u6536\u96c6\u7684\u5168\u9762\u67b6\u69cb\uff0c\u5c07\u9019\u500b\u904e\u7a0b\u5206\u89e3\u6210\u56db\u500b\u905e\u589e\u7684\u6b65\u9a5f\uff1a\u63d0\u793a\u7522\u751f\u3001\u56de\u61c9\u7522\u751f\u3001\u56de\u61c9\u904e\u6ffe\u548c\u4eba\u985e\u6a19\u8a18\u3002\u9019\u7a2e\u7d50\u69cb\u5316\u7684\u65b9\u6cd5\u78ba\u4fdd\u6536\u96c6\u5230\u9ad8\u54c1\u8cea\u7684\u504f\u597d\uff0c\u540c\u6642\u6e1b\u5c11\u5c0d\u4eba\u529b\u7684\u4f9d\u8cf4\u3002\u6211\u5011\u6839\u64da\u5728\u4e0d\u540c\u968e\u6bb5\u6536\u96c6\u5230\u7684\u8cc7\u6599\u9032\u884c\u4e86\u5168\u9762\u7684\u5be6\u9a57\uff0c\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u7684\u8cc7\u6599\u6536\u96c6\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "author": "Yulan Hu et.al.", "authors": "Yulan Hu, Qingyang Li, Sheng Ouyang, Ge Chen, Kaihui Chen, Lijun Mei, Xucheng Ye, Fuzheng Zhang, Yong Liu", "id": "2406.16486v1", "paper_url": "http://arxiv.org/abs/2406.16486v1", "repo": "null"}}