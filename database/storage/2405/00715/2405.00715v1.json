{"2405.00715": {"publish_time": "2024-04-25", "title": "Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation", "paper_summary": "Large Language Models (LLMs) have shown promising capabilities in handling\nclinical text summarization tasks. In this study, we demonstrate that a small\nopen-source LLM can be effectively trained to generate high-quality clinical\nnotes from outpatient patient-doctor dialogues. We achieve this through a\ncomprehensive domain- and task-specific adaptation process for the LLaMA-2 13\nbillion parameter model. This process incorporates continued pre-training,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced an enhanced approach, termed DistillDirect, for\nperforming on-policy reinforcement learning with Gemini Pro serving as the\nteacher model. Our resulting model, LLaMA-Clinic, is capable of generating\nclinical notes that are comparable in quality to those authored by physicians.\nIn a blinded physician reader study, the majority (90.4%) of individual\nevaluations rated the notes generated by LLaMA-Clinic as \"acceptable\" or higher\nacross all three criteria: real-world readiness, completeness, and accuracy.\nNotably, in the more challenging \"Assessment and Plan\" section, LLaMA-Clinic\nscored higher (4.2/5) in real-world readiness compared to physician-authored\nnotes (4.1/5). Additionally, we identified caveats in public clinical note\ndatasets, such as ACI-BENCH. We highlight key considerations for future\nclinical note-generation tasks, emphasizing the importance of pre-defining a\nbest-practice note format. Overall, our research demonstrates the potential and\nfeasibility of training smaller, open-source LLMs to assist with clinical\ndocumentation, capitalizing on healthcare institutions' access to patient\nrecords and domain expertise. We have made our newly created synthetic clinic\ndialogue-note dataset and the physician feedback dataset publicly available to\nfoster future research in this field.", "paper_summary_zh": "", "author": "Hanyin Wang et.al.", "authors": "Hanyin Wang,Chufan Gao,Bolun Liu,Qiping Xu,Guleid Hussein,Mohamad El Labban,Kingsley Iheasirim,Hariprasad Korsapati,Jimeng Sun", "id": "2405.00715v1", "paper_url": "http://arxiv.org/abs/2405.00715v1", "repo": "null"}}