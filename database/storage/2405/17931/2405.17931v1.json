{"2405.17931": {"publish_time": "2024-05-28", "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment", "paper_summary": "Effectively aligning Large Language Models (LLMs) with human-centric values\nwhile preventing the degradation of abilities acquired through Pre-training and\nSupervised Fine-tuning (SFT) poses a central challenge in Reinforcement\nLearning from Human Feedback (RLHF). In this paper, we first discover that\ninterpolating RLHF and SFT model parameters can adjust the trade-off between\nhuman preference and basic capabilities, thereby reducing the alignment tax at\nthe cost of alignment reward. Inspired by this, we propose integrating the RL\npolicy and SFT models at each optimization step in RLHF to continuously\nregulate the training direction, introducing the Online Merging Optimizer.\nSpecifically, we merge gradients with the parameter differences between SFT and\npretrained models, effectively steering the gradient towards maximizing rewards\nin the direction of SFT optimization. We demonstrate that our optimizer works\nwell with different LLM families, such as Qwen and LLaMA, across various model\nsizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and\nexisting model merging methods. It significantly enhances alignment reward\nwhile mitigating alignment tax, achieving higher overall performance across 14\nbenchmarks.", "paper_summary_zh": "\u6709\u6548\u5730\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u4ee5\u4eba\u70ba\u4e2d\u5fc3\u7684\u50f9\u503c\u89c0\u4fdd\u6301\u4e00\u81f4\uff0c\u540c\u6642\u9632\u6b62\u901a\u904e\u9810\u8a13\u7df4\u548c\u76e3\u7763\u5fae\u8abf (SFT) \u7372\u5f97\u7684\u80fd\u529b\u4e0b\u964d\uff0c\u9019\u5c0d\u5f37\u5316\u5b78\u7fd2\u5f9e\u4eba\u985e\u56de\u994b (RLHF) \u4e2d\u63d0\u51fa\u4e86\u6838\u5fc3\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u5148\u767c\u73fe\u63d2\u88dc RLHF \u548c SFT \u6a21\u578b\u53c3\u6578\u53ef\u4ee5\u8abf\u6574\u4eba\u985e\u504f\u597d\u548c\u57fa\u672c\u80fd\u529b\u4e4b\u9593\u7684\u6b0a\u8861\uff0c\u5f9e\u800c\u4ee5\u5c0d\u9f4a\u734e\u52f5\u70ba\u4ee3\u50f9\u964d\u4f4e\u5c0d\u9f4a\u6210\u672c\u3002\u53d7\u6b64\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u5728 RLHF \u4e2d\u7684\u6bcf\u500b\u512a\u5316\u6b65\u9a5f\u4e2d\u6574\u5408 RL \u7b56\u7565\u548c SFT \u6a21\u578b\uff0c\u4ee5\u6301\u7e8c\u8abf\u7bc0\u8a13\u7df4\u65b9\u5411\uff0c\u5f15\u5165\u7dda\u4e0a\u5408\u4f75\u512a\u5316\u5668\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u68af\u5ea6\u8207 SFT \u548c\u9810\u8a13\u7df4\u6a21\u578b\u4e4b\u9593\u7684\u53c3\u6578\u5dee\u7570\u5408\u4f75\uff0c\u6709\u6548\u5730\u5f15\u5c0e\u68af\u5ea6\u671d\u8457 SFT \u512a\u5316\u7684\u65b9\u5411\u6700\u5927\u5316\u734e\u52f5\u3002\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u512a\u5316\u5668\u9069\u7528\u65bc\u4e0d\u540c\u7684 LLM \u5bb6\u65cf\uff0c\u4f8b\u5982 Qwen \u548c LLaMA\uff0c\u8de8\u8d8a\u5f9e 1.8B \u5230 8B \u7684\u5404\u7a2e\u6a21\u578b\u5927\u5c0f\u3001\u5404\u7a2e RLHF \u6f14\u7b97\u6cd5\uff0c\u4f8b\u5982 DPO \u548c KTO\uff0c\u4ee5\u53ca\u73fe\u6709\u7684\u6a21\u578b\u5408\u4f75\u65b9\u6cd5\u3002\u5b83\u986f\u8457\u63d0\u9ad8\u4e86\u5c0d\u9f4a\u734e\u52f5\uff0c\u540c\u6642\u6e1b\u8f15\u4e86\u5c0d\u9f4a\u6210\u672c\uff0c\u5728 14 \u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5be6\u73fe\u4e86\u66f4\u9ad8\u7684\u6574\u9ad4\u6027\u80fd\u3002", "author": "Keming Lu et.al.", "authors": "Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, Chang Zhou", "id": "2405.17931v1", "paper_url": "http://arxiv.org/abs/2405.17931v1", "repo": "null"}}