{"2411.12925": {"publish_time": "2024-11-19", "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets", "paper_summary": "While scaling laws provide a reliable methodology for predicting train loss\nacross compute scales for a single data distribution, less is known about how\nthese predictions should change as we change the distribution. In this paper,\nwe derive a strategy for predicting one loss from another and apply it to\npredict across different pre-training datasets and from pre-training data to\ndownstream task data. Our predictions extrapolate well even at 20x the largest\nFLOP budget used to fit the curves. More precisely, we find that there are\nsimple shifted power law relationships between (1) the train losses of two\nmodels trained on two separate datasets when the models are paired by training\ncompute (train-to-train), (2) the train loss and the test loss on any\ndownstream distribution for a single model (train-to-test), and (3) the test\nlosses of two models trained on two separate train datasets (test-to-test). The\nresults hold up for pre-training datasets that differ substantially (some are\nentirely code and others have no code at all) and across a variety of\ndownstream tasks. Finally, we find that in some settings these shifted power\nlaw relationships can yield more accurate predictions than extrapolating\nsingle-dataset scaling laws.", "paper_summary_zh": "\u96d6\u7136\u898f\u6a21\u5b9a\u5f8b\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u9760\u7684\u65b9\u6cd5\u4f86\u9810\u6e2c\u55ae\u4e00\u8cc7\u6599\u5206\u4f48\u4e2d\uff0c\u8de8\u904b\u7b97\u898f\u6a21\u7684\u8a13\u7df4\u640d\u5931\uff0c\u4f46\u5c0d\u65bc\u9019\u4e9b\u9810\u6e2c\u5728\u6211\u5011\u6539\u8b8a\u5206\u4f48\u6642\u61c9\u5982\u4f55\u6539\u8b8a\uff0c\u6211\u5011\u6240\u77e5\u751a\u5c11\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a8\u5c0e\u51fa\u5f9e\u4e00\u500b\u640d\u5931\u9810\u6e2c\u53e6\u4e00\u500b\u640d\u5931\u7684\u7b56\u7565\uff0c\u4e26\u5c07\u5176\u61c9\u7528\u65bc\u9810\u6e2c\u4e0d\u540c\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\uff0c\u4ee5\u53ca\u5f9e\u9810\u8a13\u7df4\u8cc7\u6599\u5230\u4e0b\u6e38\u4efb\u52d9\u8cc7\u6599\u3002\u5373\u4f7f\u5728\u7528\u65bc\u64ec\u5408\u66f2\u7dda\u7684\u6700\u5927 FLOP \u9810\u7b97\u7684 20 \u500d\u4e0b\uff0c\u6211\u5011\u7684\u9810\u6e2c\u4e5f\u80fd\u5f88\u597d\u5730\u5916\u63a8\u3002\u66f4\u7cbe\u78ba\u5730\u8aaa\uff0c\u6211\u5011\u767c\u73fe\uff081\uff09\u5728\u5169\u500b\u6a21\u578b\u5728\u8a13\u7df4\u904b\u7b97\uff08\u8a13\u7df4\u5230\u8a13\u7df4\uff09\u4e2d\u914d\u5c0d\u6642\uff0c\u5728\u5169\u500b\u55ae\u7368\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u5169\u500b\u6a21\u578b\u7684\u8a13\u7df4\u640d\u5931\u4e4b\u9593\u5b58\u5728\u7c21\u55ae\u7684\u79fb\u4f4d\u51aa\u5f8b\u95dc\u4fc2\uff0c\uff082\uff09\u55ae\u4e00\u6a21\u578b\u5728\u4efb\u4f55\u4e0b\u6e38\u5206\u4f48\u4e0a\u7684\u8a13\u7df4\u640d\u5931\u548c\u6e2c\u8a66\u640d\u5931\uff08\u8a13\u7df4\u5230\u6e2c\u8a66\uff09\uff0c\u4ee5\u53ca\uff083\uff09\u5728\u5169\u500b\u55ae\u7368\u8a13\u7df4\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u5169\u500b\u6a21\u578b\u7684\u6e2c\u8a66\u640d\u5931\uff08\u6e2c\u8a66\u5230\u6e2c\u8a66\uff09\u3002\u7d50\u679c\u9069\u7528\u65bc\u5dee\u7570\u5f88\u5927\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\uff08\u6709\u4e9b\u5b8c\u5168\u662f\u7a0b\u5f0f\u78bc\uff0c\u800c\u6709\u4e9b\u6839\u672c\u6c92\u6709\u7a0b\u5f0f\u78bc\uff09\uff0c\u4ee5\u53ca\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u3002\u6700\u5f8c\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5728\u67d0\u4e9b\u8a2d\u5b9a\u4e2d\uff0c\u9019\u4e9b\u79fb\u4f4d\u51aa\u5f8b\u95dc\u4fc2\u53ef\u4ee5\u7522\u751f\u6bd4\u5916\u63a8\u55ae\u4e00\u8cc7\u6599\u96c6\u898f\u6a21\u5b9a\u5f8b\u66f4\u6e96\u78ba\u7684\u9810\u6e2c\u3002", "author": "David Brandfonbrener et.al.", "authors": "David Brandfonbrener, Nikhil Anand, Nikhil Vyas, Eran Malach, Sham Kakade", "id": "2411.12925v1", "paper_url": "http://arxiv.org/abs/2411.12925v1", "repo": "https://github.com/kempnerinstitute/loss-to-loss-olmo"}}