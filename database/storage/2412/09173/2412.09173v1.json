{"2412.09173": {"publish_time": "2024-12-12", "title": "ReFF: Reinforcing Format Faithfulness in Language Models across Varied Tasks", "paper_summary": "Following formatting instructions to generate well-structured content is a\nfundamental yet often unmet capability for large language models (LLMs). To\nstudy this capability, which we refer to as format faithfulness, we present\nFormatBench, a comprehensive format-related benchmark. Compared to previous\nformat-related benchmarks, FormatBench involves a greater variety of tasks in\nterms of application scenes (traditional NLP tasks, creative works, autonomous\nagency tasks), human-LLM interaction styles (single-turn instruction,\nmulti-turn chat), and format types (inclusion, wrapping, length, coding).\nMoreover, each task in FormatBench is attached with a format checker program.\nExtensive experiments on the benchmark reveal that state-of-the-art open- and\nclosed-source LLMs still suffer from severe deficiency in format faithfulness.\nBy virtue of the decidable nature of formats, we propose to Reinforce Format\nFaithfulness (ReFF) to help LLMs generate formatted output as instructed\nwithout compromising general quality. Without any annotated data, ReFF can\nsubstantially improve the format faithfulness rate (e.g., from 21.6% in\noriginal LLaMA3 to 95.0% on caption segmentation task), while keep the general\nquality comparable (e.g., from 47.3 to 46.4 in F1 scores). Combined with\nlabeled training data, ReFF can simultaneously improve both format faithfulness\n(e.g., from 21.6% in original LLaMA3 to 75.5%) and general quality (e.g., from\n47.3 to 61.6 in F1 scores). We further offer an interpretability analysis to\nexplain how ReFF improves both format faithfulness and general quality.", "paper_summary_zh": "<paragraph>\u9075\u5faa\u683c\u5f0f\u5316\u6307\u4ee4\u4f86\u7522\u751f\u7d50\u69cb\u826f\u597d\u7684\u5167\u5bb9\uff0c\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u57fa\u672c\u529f\u80fd\uff0c\u4f46\u901a\u5e38\u7121\u6cd5\u6eff\u8db3\u3002\u70ba\u4e86\u7814\u7a76\u9019\u7a2e\u6211\u5011\u7a31\u70ba\u683c\u5f0f\u5fe0\u5be6\u5ea6\u7684\u529f\u80fd\uff0c\u6211\u5011\u63d0\u51fa\u4e86 FormatBench\uff0c\u4e00\u500b\u5168\u9762\u7684\u683c\u5f0f\u76f8\u95dc\u57fa\u6e96\u6e2c\u8a66\u3002\u8207\u4e4b\u524d\u7684\u683c\u5f0f\u76f8\u95dc\u57fa\u6e96\u6e2c\u8a66\u76f8\u6bd4\uff0cFormatBench \u6d89\u53ca\u66f4\u591a\u6a23\u5316\u7684\u4efb\u52d9\uff0c\u5305\u62ec\u61c9\u7528\u5834\u666f\uff08\u50b3\u7d71 NLP \u4efb\u52d9\u3001\u5275\u610f\u4f5c\u54c1\u3001\u81ea\u4e3b\u4ee3\u7406\u4efb\u52d9\uff09\u3001\u4eba\u6a5f\u4e92\u52d5\u98a8\u683c\uff08\u55ae\u56de\u5408\u6307\u4ee4\u3001\u591a\u56de\u5408\u804a\u5929\uff09\u548c\u683c\u5f0f\u985e\u578b\uff08\u5305\u542b\u3001\u63db\u884c\u3001\u9577\u5ea6\u3001\u7de8\u78bc\uff09\u3002\u6b64\u5916\uff0cFormatBench \u4e2d\u7684\u6bcf\u500b\u4efb\u52d9\u90fd\u9644\u5e36\u4e00\u500b\u683c\u5f0f\u6aa2\u67e5\u5668\u7a0b\u5f0f\u3002\u5728\u57fa\u6e96\u6e2c\u8a66\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0c\u6700\u5148\u9032\u7684\u958b\u6e90\u548c\u9589\u6e90 LLM \u4ecd\u7136\u5b58\u5728\u56b4\u91cd\u7684\u683c\u5f0f\u5fe0\u5be6\u5ea6\u7f3a\u9677\u3002\u7531\u65bc\u683c\u5f0f\u7684\u53ef\u6c7a\u5b9a\u6027\uff0c\u6211\u5011\u63d0\u51fa\u52a0\u5f37\u683c\u5f0f\u5fe0\u5be6\u5ea6 (ReFF) \u4f86\u5e6b\u52a9 LLM \u6309\u7167\u6307\u793a\u7522\u751f\u683c\u5f0f\u5316\u7684\u8f38\u51fa\uff0c\u800c\u4e0d\u6703\u640d\u5bb3\u4e00\u822c\u54c1\u8cea\u3002\u5728\u6c92\u6709\u4efb\u4f55\u8a3b\u89e3\u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\uff0cReFF \u53ef\u4ee5\u5927\u5e45\u63d0\u9ad8\u683c\u5f0f\u5fe0\u5be6\u5ea6\uff08\u4f8b\u5982\uff0c\u5f9e LLaMA3 \u4e2d\u7684 21.6% \u63d0\u9ad8\u5230\u6a19\u984c\u5206\u6bb5\u4efb\u52d9\u4e2d\u7684 95.0%\uff09\uff0c\u540c\u6642\u4fdd\u6301\u4e00\u822c\u54c1\u8cea\u76f8\u7576\uff08\u4f8b\u5982\uff0cF1 \u5206\u6578\u5f9e 47.3 \u63d0\u9ad8\u5230 46.4\uff09\u3002\u7d50\u5408\u6a19\u7c64\u8a13\u7df4\u8cc7\u6599\uff0cReFF \u53ef\u4ee5\u540c\u6642\u63d0\u9ad8\u683c\u5f0f\u5fe0\u5be6\u5ea6\uff08\u4f8b\u5982\uff0c\u5f9e LLaMA3 \u4e2d\u7684 21.6% \u63d0\u9ad8\u5230 75.5%\uff09\u548c\u4e00\u822c\u54c1\u8cea\uff08\u4f8b\u5982\uff0cF1 \u5206\u6578\u5f9e 47.3 \u63d0\u9ad8\u5230 61.6\uff09\u3002\u6211\u5011\u9032\u4e00\u6b65\u63d0\u4f9b\u4e86\u53ef\u89e3\u91cb\u6027\u5206\u6790\uff0c\u8aaa\u660e ReFF \u5982\u4f55\u540c\u6642\u63d0\u9ad8\u683c\u5f0f\u5fe0\u5be6\u5ea6\u548c\u4e00\u822c\u54c1\u8cea\u3002</paragraph>", "author": "Jiashu Yao et.al.", "authors": "Jiashu Yao, Heyan Huang, Zeming Liu, Haoyu Wen, Wei Su, Boao Qian, Yuhang Guo", "id": "2412.09173v1", "paper_url": "http://arxiv.org/abs/2412.09173v1", "repo": "null"}}