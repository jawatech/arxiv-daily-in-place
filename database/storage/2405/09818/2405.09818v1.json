{"2405.09818": {"publish_time": "2024-05-16", "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models", "paper_summary": "We present Chameleon, a family of early-fusion token-based mixed-modal models\ncapable of understanding and generating images and text in any arbitrary\nsequence. We outline a stable training approach from inception, an alignment\nrecipe, and an architectural parameterization tailored for the early-fusion,\ntoken-based, mixed-modal setting. The models are evaluated on a comprehensive\nrange of tasks, including visual question answering, image captioning, text\ngeneration, image generation, and long-form mixed modal generation. Chameleon\ndemonstrates broad and general capabilities, including state-of-the-art\nperformance in image captioning tasks, outperforms Llama-2 in text-only tasks\nwhile being competitive with models such as Mixtral 8x7B and Gemini-Pro, and\nperforms non-trivial image generation, all in a single model. It also matches\nor exceeds the performance of much larger models, including Gemini Pro and\nGPT-4V, according to human judgments on a new long-form mixed-modal generation\nevaluation, where either the prompt or outputs contain mixed sequences of both\nimages and text. Chameleon marks a significant step forward in a unified\nmodeling of full multimodal documents.", "paper_summary_zh": "\u6211\u5011\u5c55\u793a Chameleon\uff0c\u4e00\u7a2e\u65e9\u671f\u878d\u5408\u7684\u57fa\u65bc token \u7684\u6df7\u5408\u6a21\u5f0f\u6a21\u578b\u5bb6\u65cf\uff0c\n\u80fd\u5920\u7406\u89e3\u548c\u751f\u6210\u4efb\u610f\u9806\u5e8f\u7684\u5716\u50cf\u548c\u6587\u5b57\u3002\u6211\u5011\u6982\u8ff0\u4e86\u5f9e\u958b\u59cb\u6642\u7684\u7a69\u5b9a\u8a13\u7df4\u65b9\u6cd5\u3001\u5c0d\u9f4a\u914d\u65b9\u548c\u5c08\u70ba\u65e9\u671f\u878d\u5408\u3001\u57fa\u65bc token \u7684\u6df7\u5408\u6a21\u5f0f\u8a2d\u7f6e\u91cf\u8eab\u6253\u9020\u7684\u67b6\u69cb\u53c3\u6578\u5316\u3002\u9019\u4e9b\u6a21\u578b\u5728\u5ee3\u6cdb\u7684\u4efb\u52d9\u4e0a\u9032\u884c\u4e86\u8a55\u4f30\uff0c\u5305\u62ec\u8996\u89ba\u554f\u7b54\u3001\u5716\u50cf\u6a19\u984c\u3001\u6587\u5b57\u751f\u6210\u3001\u5716\u50cf\u751f\u6210\u548c\u9577\u7bc7\u6df7\u5408\u6a21\u5f0f\u751f\u6210\u3002Chameleon \u5c55\u793a\u4e86\u5ee3\u6cdb\u4e14\u901a\u7528\u7684\u80fd\u529b\uff0c\u5305\u62ec\u5728\u5716\u50cf\u6a19\u984c\u4efb\u52d9\u4e2d\u9054\u5230\u6700\u5148\u9032\u7684\u6027\u80fd\uff0c\u5728\u50c5\u6587\u5b57\u4efb\u52d9\u4e2d\u512a\u65bc Llama-2\uff0c\u540c\u6642\u8207 Mixtral 8x7B \u548c Gemini-Pro \u7b49\u6a21\u578b\u7af6\u722d\uff0c\u4e26\u57f7\u884c\u975e\u5e73\u51e1\u7684\u5716\u50cf\u751f\u6210\uff0c\u6240\u6709\u9019\u4e9b\u90fd\u5728\u4e00\u500b\u6a21\u578b\u4e2d\u3002\u6839\u64da\u4eba\u985e\u5c0d\u65b0\u7684\u9577\u7bc7\u6df7\u5408\u6a21\u5f0f\u751f\u6210\u8a55\u4f30\u7684\u5224\u65b7\uff0c\u5b83\u4e5f\u5339\u914d\u6216\u8d85\u904e\u4e86\u66f4\u5927\u7684\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5305\u62ec Gemini Pro \u548c GPT-4V\uff0c\u5176\u4e2d\u63d0\u793a\u6216\u8f38\u51fa\u5305\u542b\u5716\u50cf\u548c\u6587\u5b57\u7684\u6df7\u5408\u5e8f\u5217\u3002Chameleon \u6a19\u8a8c\u8457\u5728\u7d71\u4e00\u5efa\u6a21\u5b8c\u6574\u7684\u591a\u6a21\u614b\u6587\u4ef6\u4e2d\u9081\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002", "author": "Chameleon Team et.al.", "authors": "Chameleon Team", "id": "2405.09818v1", "paper_url": "http://arxiv.org/abs/2405.09818v1", "repo": "null"}}