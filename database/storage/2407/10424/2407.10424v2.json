{"2407.10424": {"publish_time": "2024-07-15", "title": "CodeV: Empowering LLMs for Verilog Generation through Multi-Level Summarization", "paper_summary": "The increasing complexity and high costs associated with modern processor\ndesign have led to a surge in demand for processor design automation.\nInstruction-tuned large language models (LLMs) have demonstrated remarkable\nperformance in automatically generating code for general-purpose programming\nlanguages like Python. However, these methods fail on hardware description\nlanguages (HDLs) like Verilog due to the scarcity of high-quality instruction\ntuning data, as even advanced LLMs like GPT-3.5 exhibit limited performance on\nVerilog generation. Regarding this issue, we observe that (1) Verilog code\ncollected from the real world has higher quality than those generated by LLMs.\n(2) LLMs like GPT-3.5 excel in summarizing Verilog code rather than generating\nit. Based on these observations, this paper introduces CodeV, a series of\nopen-source instruction-tuned Verilog generation LLMs. Instead of generating\ndescriptions first and then getting the corresponding code from advanced LLMs,\nwe prompt the LLM with Verilog code and let the LLM generate the corresponding\nnatural language description by multi-level summarization. Experimental results\nshow that CodeV relatively surpasses the previous open-source SOTA by 14.4%\n(BetterV in VerilogEval) and 11.3% (RTLCoder in RTLLM) respectively, and also\nrelatively outperforms previous commercial SOTA GPT-4 by 22.1% in VerilogEval.", "paper_summary_zh": "\u96a8\u8457\u73fe\u4ee3\u8655\u7406\u5668\u8a2d\u8a08\u7684\u8907\u96dc\u6027\u8207\u9ad8\u6210\u672c\u65e5\u76ca\u589e\u52a0\uff0c\u5c0d\u65bc\u8655\u7406\u5668\u8a2d\u8a08\u81ea\u52d5\u5316\u7684\u9700\u6c42\u4e5f\u96a8\u4e4b\u6fc0\u589e\u3002\u6307\u4ee4\u8abf\u6574\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u9a5a\u4eba\u7684\u6548\u80fd\uff0c\u80fd\u81ea\u52d5\u70ba Python \u7b49\u901a\u7528\u7a0b\u5f0f\u8a9e\u8a00\u7522\u751f\u7a0b\u5f0f\u78bc\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u7121\u6cd5\u7528\u65bc\u50cf Verilog \u7b49\u786c\u9ad4\u63cf\u8ff0\u8a9e\u8a00 (HDL)\uff0c\u56e0\u70ba\u7f3a\u4e4f\u9ad8\u54c1\u8cea\u7684\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\uff0c\u5373\u4f7f\u662f\u50cf GPT-3.5 \u7b49\u9032\u968e LLM \u5728\u7522\u751f Verilog \u4e0a\u4e5f\u8868\u73fe\u6709\u9650\u3002\u91dd\u5c0d\u6b64\u554f\u984c\uff0c\u6211\u5011\u89c0\u5bdf\u5230 (1) \u5f9e\u771f\u5be6\u4e16\u754c\u6536\u96c6\u7684 Verilog \u7a0b\u5f0f\u78bc\u54c1\u8cea\u9ad8\u65bc LLM \u6240\u7522\u751f\u7684\u7a0b\u5f0f\u78bc\u3002(2) \u50cf GPT-3.5 \u7b49 LLM \u64c5\u9577\u7e3d\u7d50 Verilog \u7a0b\u5f0f\u78bc\uff0c\u800c\u975e\u7522\u751f\u7a0b\u5f0f\u78bc\u3002\u57fa\u65bc\u9019\u4e9b\u89c0\u5bdf\uff0c\u672c\u6587\u4ecb\u7d39 CodeV\uff0c\u4e00\u7cfb\u5217\u958b\u6e90\u7684\u6307\u4ee4\u8abf\u6574 Verilog \u7522\u751f LLM\u3002\u6211\u5011\u4e26\u975e\u5148\u7522\u751f\u63cf\u8ff0\uff0c\u518d\u5f9e\u9032\u968e LLM \u53d6\u5f97\u5c0d\u61c9\u7684\u7a0b\u5f0f\u78bc\uff0c\u800c\u662f\u63d0\u793a LLM \u4f7f\u7528 Verilog \u7a0b\u5f0f\u78bc\uff0c\u4e26\u8b93 LLM \u900f\u904e\u591a\u5c64\u6b21\u6458\u8981\u7522\u751f\u5c0d\u61c9\u7684\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cCodeV \u5206\u5225\u76f8\u5c0d\u8d85\u8d8a\u5148\u524d\u7684\u958b\u6e90 SOTA 14.4%\uff08VerilogEval \u4e2d\u7684 BetterV\uff09\u548c 11.3%\uff08RTLLM \u4e2d\u7684 RTLCoder\uff09\uff0c\u5728 VerilogEval \u4e2d\u4e5f\u76f8\u5c0d\u512a\u65bc\u5148\u524d\u7684\u5546\u696d SOTA GPT-4 22.1%\u3002", "author": "Yang Zhao et.al.", "authors": "Yang Zhao, Di Huang, Chongxiao Li, Pengwei Jin, Ziyuan Nan, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, Xishan Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen", "id": "2407.10424v2", "paper_url": "http://arxiv.org/abs/2407.10424v2", "repo": "null"}}