{"2412.19423": {"publish_time": "2024-12-27", "title": "Revisiting PCA for time series reduction in temporal dimension", "paper_summary": "Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,\nWenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series\nanalysis (TSA), enabling the extraction of complex patterns for tasks like\nclassification, forecasting, and regression. Although dimensionality reduction\nhas traditionally focused on the variable space-achieving notable success in\nminimizing data redundancy and computational complexity-less attention has been\npaid to reducing the temporal dimension. In this study, we revisit Principal\nComponent Analysis (PCA), a classical dimensionality reduction technique, to\nexplore its utility in temporal dimension reduction for time series data. It is\ngenerally thought that applying PCA to the temporal dimension would disrupt\ntemporal dependencies, leading to limited exploration in this area. However,\nour theoretical analysis and extensive experiments demonstrate that applying\nPCA to sliding series windows not only maintains model performance, but also\nenhances computational efficiency. In auto-regressive forecasting, the temporal\nstructure is partially preserved through windowing, and PCA is applied within\nthese windows to denoise the time series while retaining their statistical\ninformation. By preprocessing time-series data with PCA, we reduce the temporal\ndimensionality before feeding it into TSA models such as Linear, Transformer,\nCNN, and RNN architectures. This approach accelerates training and inference\nand reduces resource consumption. Notably, PCA improves Informer training and\ninference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,\nwithout sacrificing model accuracy. Comparative analysis against other\nreduction methods further highlights the effectiveness of PCA in improving the\nefficiency of TSA models.", "paper_summary_zh": "<paragraph>\u91cd\u8a2a\u6642\u9593\u7dad\u5ea6\u4e2d PCA \u6642\u5e8f\u7c21\u7d04\uff1b\u8cc8\u946b\u9ad8\u3001\u80e1\u6587\u6ce2\u3001\u9673\u96f2\u5929\uff1b\u6df1\u5ea6\u5b78\u7fd2\u986f\u8457\u5730\u63d0\u5347\u4e86\u6642\u5e8f\u5206\u6790 (TSA)\uff0c\u80fd\u70ba\u5206\u985e\u3001\u9810\u6e2c\u548c\u8ff4\u6b78\u7b49\u4efb\u52d9\u8403\u53d6\u8907\u96dc\u6a21\u5f0f\u3002\u5118\u7ba1\u964d\u7dad\u50b3\u7d71\u4e0a\u5c08\u6ce8\u65bc\u8b8a\u6578\u7a7a\u9593\uff0c\u5728\u6700\u5c0f\u5316\u8cc7\u6599\u5197\u9918\u548c\u904b\u7b97\u8907\u96dc\u5ea6\u4e0a\u53d6\u5f97\u986f\u8457\u6210\u529f\uff0c\u4f46\u8f03\u5c11\u95dc\u6ce8\u65bc\u6e1b\u5c11\u6642\u9593\u7dad\u5ea6\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u91cd\u65b0\u63a2\u8a0e\u4e3b\u6210\u5206\u5206\u6790 (PCA)\uff0c\u4e00\u7a2e\u7d93\u5178\u7684\u964d\u7dad\u6280\u8853\uff0c\u4ee5\u63a2\u7d22\u5176\u5728\u6642\u5e8f\u8cc7\u6599\u6642\u9593\u7dad\u5ea6\u7c21\u7d04\u4e2d\u7684\u6548\u7528\u3002\u4e00\u822c\u8a8d\u70ba\u5c07 PCA \u61c9\u7528\u65bc\u6642\u9593\u7dad\u5ea6\u6703\u7834\u58de\u6642\u9593\u4f9d\u8cf4\u6027\uff0c\u5c0e\u81f4\u5728\u9019\u500b\u9818\u57df\u7684\u63a2\u7d22\u6709\u9650\u3002\u7136\u800c\uff0c\u6211\u5011\u7684\u7406\u8ad6\u5206\u6790\u548c\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u5c07 PCA \u61c9\u7528\u65bc\u6ed1\u52d5\u5e8f\u5217\u8996\u7a97\u4e0d\u50c5\u80fd\u7dad\u6301\u6a21\u578b\u6548\u80fd\uff0c\u9084\u80fd\u63d0\u5347\u904b\u7b97\u6548\u7387\u3002\u5728\u81ea\u8ff4\u6b78\u9810\u6e2c\u4e2d\uff0c\u6642\u9593\u7d50\u69cb\u90e8\u5206\u5730\u900f\u904e\u8996\u7a97\u5316\u4fdd\u7559\uff0c\u800c PCA \u5728\u9019\u4e9b\u8996\u7a97\u4e2d\u61c9\u7528\u4ee5\u53bb\u9664\u6642\u5e8f\u96dc\u8a0a\uff0c\u540c\u6642\u4fdd\u7559\u5176\u7d71\u8a08\u8cc7\u8a0a\u3002\u900f\u904e PCA \u9810\u8655\u7406\u6642\u5e8f\u8cc7\u6599\uff0c\u6211\u5011\u5728\u5c07\u5176\u8f38\u5165 TSA \u6a21\u578b\uff08\u4f8b\u5982\u7dda\u6027\u3001Transformer\u3001CNN \u548c RNN \u67b6\u69cb\uff09\u4e4b\u524d\u964d\u4f4e\u6642\u9593\u7dad\u5ea6\u3002\u9019\u7a2e\u65b9\u6cd5\u52a0\u901f\u8a13\u7df4\u548c\u63a8\u8ad6\uff0c\u4e26\u6e1b\u5c11\u8cc7\u6e90\u6d88\u8017\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cPCA \u5c07 Informer \u8a13\u7df4\u548c\u63a8\u8ad6\u901f\u5ea6\u63d0\u5347\u4e86 40%\uff0c\u4e26\u5c07 TimesNet \u7684 GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u4e86 30%\uff0c\u4e14\u4e0d\u72a7\u7272\u6a21\u578b\u6e96\u78ba\u5ea6\u3002\u8207\u5176\u4ed6\u7c21\u7d04\u65b9\u6cd5\u7684\u6bd4\u8f03\u5206\u6790\u9032\u4e00\u6b65\u7a81\u986f\u4e86 PCA \u5728\u63d0\u5347 TSA \u6a21\u578b\u6548\u7387\u65b9\u9762\u7684\u6548\u7528\u3002</paragraph>", "author": "Jiaxin Gao et.al.", "authors": "Jiaxin Gao, Wenbo Hu, Yuntian Chen", "id": "2412.19423v1", "paper_url": "http://arxiv.org/abs/2412.19423v1", "repo": "null"}}