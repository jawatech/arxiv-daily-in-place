{"2406.12416": {"publish_time": "2024-06-18", "title": "Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models", "paper_summary": "Large language models (LLMs) have achieved remarkable success but still tend\nto generate factually erroneous responses, a phenomenon known as hallucination.\nA recent trend is to use preference learning to fine-tune models to align with\nfactuality. However, existing work primarily evaluates fine-tuned models on\nin-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets\nremains underexplored. In this paper, we conduct a comprehensive evaluation of\nthe factuality of different models tuned by various preference learning\nalgorithms and demonstrate that their performance on OOD datasets either\nincreases minimally or decreases. Subsequently, we reveal that the main cause\nof model's failure to uphold factuality under a distribution shift is\n\\textbf{under-alignment}, rather than \\textbf{over-alignment}, by analyzing the\ntoken distribution shift of the models before and after tuning. Finally, we\npropose \\textbf{APEFT} (\\textbf{A}tomic \\textbf{P}reference \\textbf{E}nhanced\n\\textbf{F}actuality \\textbf{T}uning), a framework that enhances model's\nawareness of factuality at the granularity of individual facts. Extensive\nexperiments demonstrate that APEFT improves model performance by an average of\n$\\boldsymbol{3.45\\%}$ on both ID and OOD datasets, which is highly effective.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u7d93\u53d6\u5f97\u986f\u8457\u7684\u6210\u529f\uff0c\u4f46\u4ecd\u50be\u5411\u65bc\u7522\u751f\u4e8b\u5be6\u932f\u8aa4\u7684\u56de\u61c9\uff0c\u9019\u662f\u4e00\u7a2e\u7a31\u70ba\u5e7b\u89ba\u7684\u73fe\u8c61\u3002\u6700\u8fd1\u7684\u8da8\u52e2\u662f\u4f7f\u7528\u504f\u597d\u5b78\u7fd2\u4f86\u5fae\u8abf\u6a21\u578b\u4ee5\u7b26\u5408\u4e8b\u5be6\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u5de5\u4f5c\u4e3b\u8981\u5728\u57df\u5167 (ID) \u8cc7\u6599\u96c6\u4e0a\u8a55\u4f30\u5fae\u8abf\u6a21\u578b\uff0c\u800c\u57df\u5916 (OOD) \u8cc7\u6599\u96c6\u4e0a\u7684\u4e8b\u5be6\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c0d\u7531\u5404\u7a2e\u504f\u597d\u5b78\u7fd2\u6f14\u7b97\u6cd5\u8abf\u6574\u7684\u4e0d\u540c\u6a21\u578b\u7684\u4e8b\u5be6\u6027\u9032\u884c\u4e86\u5168\u9762\u7684\u8a55\u4f30\uff0c\u4e26\u8b49\u660e\u5b83\u5011\u5728 OOD \u8cc7\u6599\u96c6\u4e0a\u7684\u6548\u80fd\u4e0d\u662f\u6700\u4f4e\u9650\u5ea6\u5730\u589e\u52a0\u5c31\u662f\u6e1b\u5c11\u3002\u96a8\u5f8c\uff0c\u6211\u5011\u63ed\u793a\u4e86\u6a21\u578b\u7121\u6cd5\u5728\u5206\u4f48\u8f49\u79fb\u4e0b\u7dad\u6301\u4e8b\u5be6\u6027\u7684\u4e3b\u8981\u539f\u56e0\u662f\u300c\u5c0d\u9f4a\u4e0d\u8db3\u300d\uff0c\u800c\u4e0d\u662f\u300c\u5c0d\u9f4a\u904e\u5ea6\u300d\uff0c\u65b9\u6cd5\u662f\u5206\u6790\u6a21\u578b\u5728\u8abf\u6574\u524d\u5f8c\u7684\u7b26\u865f\u5206\u4f48\u8f49\u79fb\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u300cAPEFT\u300d\uff08\u539f\u5b50\u504f\u597d\u589e\u5f37\u4e8b\u5be6\u8abf\u6574\uff09\uff0c\u9019\u662f\u4e00\u500b\u6846\u67b6\uff0c\u5b83\u589e\u5f37\u4e86\u6a21\u578b\u5c0d\u500b\u5225\u4e8b\u5be6\u7c92\u5ea6\u7684\u4e8b\u5be6\u8a8d\u8b58\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8b49\u660e\uff0cAPEFT \u5728 ID \u548c OOD \u8cc7\u6599\u96c6\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e86\u6a21\u578b\u6548\u80fd $\\boldsymbol{3.45\\%}$\uff0c\u9019\u975e\u5e38\u6709\u6548\u3002", "author": "Hongbang Yuan et.al.", "authors": "Hongbang Yuan, Yubo Chen, Pengfei Cao, Zhuoran Jin, Kang Liu, Jun Zhao", "id": "2406.12416v1", "paper_url": "http://arxiv.org/abs/2406.12416v1", "repo": "null"}}