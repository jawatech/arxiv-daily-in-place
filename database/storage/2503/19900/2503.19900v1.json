{"2503.19900": {"publish_time": "2025-03-25", "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning", "paper_summary": "The rapid advancement of large vision-language models (LVLMs) has driven\nsignificant progress in multimodal tasks, enabling models to interpret, reason,\nand generate outputs across both visual and textual domains. While excelling in\ngenerative tasks, existing LVLMs often face limitations in tasks requiring\nhigh-fidelity representation learning, such as generating image or text\nembeddings for retrieval. Recent work has proposed finetuning LVLMs for\nrepresentational learning, but the fine-tuned model often loses its generative\ncapabilities due to the representational learning training paradigm. To address\nthis trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning\nframework that enhances LVLMs for both representation and generative tasks. By\nintegrating a contrastive objective with autoregressive language modeling, our\napproach unifies these traditionally separate tasks, achieving state-of-the-art\nresults in both multimodal retrieval and multimodal generative benchmarks,\nincluding object hallucination (OH) mitigation. CAFe establishes a novel\nframework that synergizes embedding and generative functionalities in a single\nmodel, setting a foundation for future multimodal models that excel in both\nretrieval precision and coherent output generation.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u7684\u5feb\u901f\u767c\u5c55\u63a8\u52d5\u4e86\u591a\u6a21\u614b\u4efb\u52d9\u7684\u986f\u8457\u9032\u6b65\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u5728\u8996\u89ba\u548c\u6587\u672c\u9818\u57df\u9032\u884c\u89e3\u8b80\u3001\u63a8\u7406\u548c\u751f\u6210\u8f38\u51fa\u3002\u96d6\u7136\u73fe\u6709\u7684 LVLMs \u5728\u751f\u6210\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u9ad8\u4fdd\u771f\u8868\u5fb5\u5b78\u7fd2\u7684\u4efb\u52d9\uff08\u4f8b\u5982\u751f\u6210\u7528\u65bc\u6aa2\u7d22\u7684\u5716\u50cf\u6216\u6587\u672c\u5d4c\u5165\uff09\u4e2d\u537b\u5e38\u5e38\u9762\u81e8\u9650\u5236\u3002\u6700\u8fd1\u7684\u7814\u7a76\u63d0\u51fa\u5c0d LVLMs \u9032\u884c\u5fae\u8abf\u4ee5\u9032\u884c\u8868\u5fb5\u5b78\u7fd2\uff0c\u4f46\u5fae\u8abf\u5f8c\u7684\u6a21\u578b\u901a\u5e38\u6703\u56e0\u70ba\u8868\u5fb5\u5b78\u7fd2\u7684\u8a13\u7df4\u7bc4\u5f0f\u800c\u5931\u53bb\u5176\u751f\u6210\u80fd\u529b\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u7a2e\u6b0a\u8861\uff0c\u6211\u5011\u5f15\u5165\u4e86 CAFe\uff0c\u9019\u662f\u4e00\u500b\u5c0d\u6bd4\u5f0f\u81ea\u56de\u6b78\u5fae\u8abf\u6846\u67b6\uff0c\u53ef\u4ee5\u589e\u5f37 LVLMs \u7684\u8868\u5fb5\u548c\u751f\u6210\u4efb\u52d9\u80fd\u529b\u3002\u901a\u904e\u5c07\u5c0d\u6bd4\u76ee\u6a19\u8207\u81ea\u56de\u6b78\u8a9e\u8a00\u5efa\u6a21\u76f8\u7d50\u5408\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5c07\u9019\u4e9b\u50b3\u7d71\u4e0a\u5206\u96e2\u7684\u4efb\u52d9\u7d71\u4e00\u4e86\u8d77\u4f86\uff0c\u5728\u591a\u6a21\u614b\u6aa2\u7d22\u548c\u591a\u6a21\u614b\u751f\u6210\u57fa\u6e96\u6e2c\u8a66\uff08\u5305\u62ec\u7269\u4ef6\u5e7b\u89ba (OH) \u7de9\u89e3\uff09\u4e2d\u90fd\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u7d50\u679c\u3002CAFe \u5efa\u7acb\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u5c07\u5d4c\u5165\u548c\u751f\u6210\u529f\u80fd\u5354\u540c\u6574\u5408\u5230\u55ae\u500b\u6a21\u578b\u4e2d\uff0c\u70ba\u672a\u4f86\u5728\u6aa2\u7d22\u7cbe\u5ea6\u548c\u9023\u8cab\u8f38\u51fa\u751f\u6210\u65b9\u9762\u90fd\u8868\u73fe\u51fa\u8272\u7684\u591a\u6a21\u614b\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u790e\u3002\n", "author": "Hao Yu et.al.", "authors": "Hao Yu, Zhuokai Zhao, Shen Yan, Lukasz Korycki, Jianyu Wang, Baosheng He, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hanchao Yu", "id": "2503.19900v1", "paper_url": "http://arxiv.org/abs/2503.19900v1", "repo": "null"}}