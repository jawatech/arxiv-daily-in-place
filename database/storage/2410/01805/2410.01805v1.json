{"2410.01805": {"publish_time": "2024-10-02", "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads", "paper_summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u652f\u63f4\u9577\u8a9e\u5883\u7406\u89e3\u548c\u8655\u7406\u4efb\u52d9\u65b9\u9762\u5df2\u5c55\u73fe\u51fa\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0c\u5c07 LLM \u7684\u7522\u751f\u63a8\u8ad6\u64f4\u5c55\u5230\u5982\u6b64\u9577\u7684\u8a9e\u5883\u6703\u9020\u6210\u986f\u8457\u7684\u984d\u5916\u904b\u7b97\u8ca0\u8f09\uff0c\u4e26\u9700\u8981\u5927\u91cf\u7684 GPU \u8a18\u61b6\u9ad4\u7a7a\u9593\u4f86\u7dad\u8b77\u57fa\u65bc\u8f49\u63db\u5668\u7684 LLM \u7684\u9375\u503c (KV) \u5feb\u53d6\u3002\u73fe\u6709\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u65b9\u6cd5\uff08\u4f8b\u5982\u91cf\u5316\uff09\u6703\u5728\u8a9e\u5883\u9577\u5ea6\u589e\u52a0\u6642\u9762\u81e8\u8a18\u61b6\u9ad4\u74f6\u9838\uff0c\u800c\u975c\u614b\u5927\u5c0f\u7684\u5feb\u53d6\uff08\u4f8b\u5982\u9a45\u9010\uff09\u5247\u6709\u653f\u7b56\u6548\u7387\u4e0d\u4f73\u7684\u554f\u984c\u3002\u9019\u4e9b\u9650\u5236\u6703\u9650\u5236\u5728\u6d88\u8cbb\u8005\u7b49\u7d1a\u7684\u88dd\u7f6e\uff08\u4f8b\u5982\u55ae\u4e00\u7684 Nvidia 4090 GPU\uff09\u4e0a\u90e8\u7f72\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa Locret\uff0c\u4e00\u500b\u9577\u8a9e\u5883 LLM \u63a8\u8ad6\u7684\u67b6\u69cb\uff0c\u5b83\u5f15\u5165\u4e86\u4fdd\u7559\u982d\u90e8\u4f86\u8a55\u4f30 KV \u5feb\u53d6\u55ae\u5143\u7684\u56e0\u679c\u91cd\u8981\u6027\uff0c\u5141\u8a31\u5728\u56fa\u5b9a\u7684\u5feb\u53d6\u5927\u5c0f\u5167\u9032\u884c\u66f4\u7cbe\u78ba\u7684\u9a45\u9010\u3002Locret \u5728\u51cd\u7d50\u7684\u9aa8\u5e79 LLM \u4e0a\u9032\u884c\u5fae\u8abf\uff0c\u4f7f\u7528\u4f86\u81ea\u6a19\u6e96\u9577\u8a9e\u5883 SFT \u8cc7\u6599\u96c6\u7684\u5c11\u91cf\u8cc7\u6599\u3002\u5728\u63a8\u8ad6\u671f\u9593\uff0c\u6211\u5011\u6703\u9a45\u9010\u4f4e\u91cd\u8981\u6027\u7684\u5feb\u53d6\u55ae\u5143\u4ee5\u53ca\u5206\u584a\u9810\u5148\u586b\u5165\u6a21\u5f0f\uff0c\u5927\u5e45\u964d\u4f4e GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u7387\u7684\u5cf0\u503c\u3002\u6211\u5011\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u8b49\u7814\u7a76\u4f86\u8a55\u4f30 Locret\uff0c\u5be6\u9a57\u7d50\u679c\u986f\u793a Locret \u5728\u8a18\u61b6\u9ad4\u6548\u7387\u548c\u7522\u751f\u5167\u5bb9\u7684\u54c1\u8cea\u65b9\u9762\u512a\u65bc\u6700\u8fd1\u7684\u7af6\u722d\u65b9\u6cd5\uff0c\u5305\u62ec InfLLM\u3001\u91cf\u5316\u3001SirLLM \u548c MInference -- \u8207 Phi-3-mini-128K \u548c Llama-3.1-8B-instruct \u7684\u5b8c\u6574 KV \u5feb\u53d6\u76f8\u6bd4\uff0cLocret \u9054\u5230\u4e86\u8d85\u904e 20 \u500d\u548c 8 \u500d\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u7387\u3002\u6b64\u5916\uff0cLocret \u53ef\u4ee5\u8207\u5176\u4ed6\u65b9\u6cd5\uff08\u4f8b\u5982\u91cf\u5316\u548c\u6a19\u8a18\u5408\u4f75\uff09\u7d50\u5408\u4f7f\u7528\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cLocret \u662f\u7b2c\u4e00\u500b\u80fd\u5920\u5728\u55ae\u4e00\u7684 Nvidia 4090 GPU \u4e0a\u90e8\u7f72 Llama-3.1-8B \u6216\u985e\u4f3c\u6a21\u578b\u7684\u67b6\u69cb\uff0c\u80fd\u5920\u9032\u884c 128K \u9577\u8a9e\u5883\u63a8\u8ad6\u800c\u4e0d\u5f71\u97ff\u7522\u751f\u54c1\u8cea\uff0c\u800c\u4e14\u53ea\u9700\u8981\u5f88\u5c11\u7684\u984d\u5916\u7cfb\u7d71\u6700\u4f73\u5316\u3002", "author": "Yuxiang Huang et.al.", "authors": "Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu", "id": "2410.01805v1", "paper_url": "http://arxiv.org/abs/2410.01805v1", "repo": "https://github.com/huangyuxiang03/Locret"}}