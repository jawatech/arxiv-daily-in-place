{"2407.20395": {"publish_time": "2024-07-29", "title": "Dense Self-Supervised Learning for Medical Image Segmentation", "paper_summary": "Deep learning has revolutionized medical image segmentation, but it relies\nheavily on high-quality annotations. The time, cost and expertise required to\nlabel images at the pixel-level for each new task has slowed down widespread\nadoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL)\napproach for few-shot segmentation, that reduces the manual annotation burden\nby learning powerful pixel-level representations directly from unlabeled\nimages. Pix2Rep is a novel pixel-level loss and pre-training paradigm for\ncontrastive SSL on whole images. It is applied to generic encoder-decoder deep\nlearning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance\nof the learned image-level representations under intensity and spatial image\naugmentations, Pix2Rep enforces equivariance of the pixel-level\nrepresentations. We demonstrate the framework on a task of cardiac MRI\nsegmentation. Results show improved performance compared to existing semi- and\nself-supervised approaches; and a 5-fold reduction in the annotation burden for\nequivalent performance versus a fully supervised U-Net baseline. This includes\na 30% (resp. 31%) DICE improvement for one-shot segmentation under\nlinear-probing (resp. fine-tuning). Finally, we also integrate the novel\nPix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even\nbetter segmentation performance.", "paper_summary_zh": "\u6df1\u5ea6\u5b78\u7fd2\u5fb9\u5e95\u6539\u8b8a\u4e86\u91ab\u5b78\u5f71\u50cf\u5206\u5272\uff0c\u4f46\u5b83\u6975\u5ea6\u4f9d\u8cf4\u65bc\u9ad8\u54c1\u8cea\u7684\u8a3b\u89e3\u3002\u70ba\u6bcf\u500b\u65b0\u4efb\u52d9\u6a19\u8a18\u50cf\u7d20\u5c64\u7d1a\u7684\u5f71\u50cf\u6240\u9700\u7684\u6642\u9593\u3001\u6210\u672c\u548c\u5c08\u696d\u77e5\u8b58\uff0c\u5df2\u6e1b\u7de9\u4e86\u7bc4\u4f8b\u7684\u5ee3\u6cdb\u63a1\u7528\u3002\u6211\u5011\u63d0\u51fa Pix2Rep\uff0c\u4e00\u7a2e\u91dd\u5c0d\u5c11\u6b21\u5206\u5272\u7684\u81ea\u76e3\u7763\u5f0f\u5b78\u7fd2 (SSL) \u65b9\u6cd5\uff0c\u53ef\u900f\u904e\u76f4\u63a5\u5f9e\u672a\u6a19\u8a18\u7684\u5f71\u50cf\u4e2d\u5b78\u7fd2\u5f37\u5927\u7684\u50cf\u7d20\u5c64\u7d1a\u8868\u793a\uff0c\u4f86\u6e1b\u8f15\u624b\u52d5\u8a3b\u89e3\u8ca0\u64d4\u3002Pix2Rep \u662f\u4e00\u7a2e\u91dd\u5c0d\u5b8c\u6574\u5f71\u50cf\u5c0d\u6bd4\u5f0f SSL \u7684\u65b0\u7a4e\u50cf\u7d20\u5c64\u7d1a\u640d\u5931\u548c\u9810\u8a13\u7df4\u7bc4\u4f8b\u3002\u5b83\u88ab\u61c9\u7528\u65bc\u901a\u7528\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6df1\u5ea6\u5b78\u7fd2\u4e3b\u5e79 (\u4f8b\u5982 U-Net)\u3002\u5927\u591a\u6578 SSL \u65b9\u6cd5\u5f37\u5236\u5b78\u7fd2\u7684\u5f71\u50cf\u5c64\u7d1a\u8868\u793a\u5728\u5f37\u5ea6\u548c\u7a7a\u9593\u5f71\u50cf\u64f4\u5145\u4e0b\u5177\u6709\u4e0d\u8b8a\u6027\uff0c\u800c Pix2Rep \u5247\u5f37\u5236\u50cf\u7d20\u5c64\u7d1a\u8868\u793a\u5177\u6709\u7b49\u8b8a\u6027\u3002\u6211\u5011\u5728\u5fc3\u81df MRI \u5206\u5272\u4efb\u52d9\u4e2d\u5c55\u793a\u4e86\u9019\u500b\u67b6\u69cb\u3002\u7d50\u679c\u986f\u793a\u8207\u73fe\u6709\u7684\u534a\u76e3\u7763\u5f0f\u548c\u81ea\u76e3\u7763\u5f0f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6548\u80fd\u6709\u6240\u63d0\u5347\uff1b\u4e14\u5728\u8207\u5b8c\u5168\u76e3\u7763\u5f0f U-Net \u57fa\u6e96\u5177\u6709\u76f8\u540c\u6548\u80fd\u7684\u60c5\u6cc1\u4e0b\uff0c\u8a3b\u89e3\u8ca0\u64d4\u6e1b\u5c11\u4e86 5 \u500d\u3002\u9019\u5305\u62ec\u5728\u7dda\u6027\u63a2\u6e2c (resp. \u5fae\u8abf) \u4e0b\uff0c\u55ae\u6b21\u5206\u5272\u7684 DICE \u63d0\u5347\u4e86 30% (resp. 31%)\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4e5f\u5c07\u65b0\u7a4e\u7684 Pix2Rep \u6982\u5ff5\u8207 Barlow Twins \u975e\u5c0d\u6bd4\u5f0f SSL \u6574\u5408\uff0c\u9019\u5c0e\u81f4\u4e86\u66f4\u597d\u7684\u5206\u5272\u6548\u80fd\u3002", "author": "Maxime Seince et.al.", "authors": "Maxime Seince, Loic Le Folgoc, Luiz Augusto Facury de Souza, Elsa Angelini", "id": "2407.20395v1", "paper_url": "http://arxiv.org/abs/2407.20395v1", "repo": "null"}}