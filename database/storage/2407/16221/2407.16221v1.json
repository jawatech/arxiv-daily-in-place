{"2407.16221": {"publish_time": "2024-07-23", "title": "Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large Language Models", "paper_summary": "As Large Language Models (LLMs) achieve remarkable performance across various\nNLP tasks, their reliability becomes essential for widespread adoption. This\npaper focuses on Abstention Ability (AA), a critical yet under explored aspect\nof reliability - the ability of LLMs to refrain from answering questions when\nthey are uncertain or when definitive answer is not possible, while maintaining\nquestion-answering (QA) task performance. While previous works have focused on\nunderstanding the recollection abilities of LLMs or their ability to identify\nimponderable/unanswerable questions, we believe there is a need for an\neffective AA evaluation method. Therefore, we propose a black-box evaluation\nmethodology to examine and understand the AA of LLMs across a variety of\nmultiple-choice QA tasks. We measure AA by rewarding models for abstaining from\nanswering when their predictions are incorrect or when the questions are\ninherently unanswerable. We investigate three strategies, Strict Prompting,\nVerbal Confidence Thresholding, and Chain-of-Thought (CoT), to understand their\nimpact on abstention across different LLMs. Our findings reveal that while even\nstate-of-the-art LLMs like GPT-4 struggle with abstention, strategic prompting\nsuch as CoT, can significantly enhance this ability. Furthermore, we\ndemonstrate that improving AA also leads to better overall QA task performance,\nunderscoring the importance of evaluating AA in LLMs.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u5176\u53ef\u9760\u6027\u5c0d\u65bc\u5ee3\u6cdb\u63a1\u7528\u81f3\u95dc\u91cd\u8981\u3002\u672c\u6587\u91cd\u9ede\u63a2\u8a0e\u6212\u9664\u80fd\u529b (AA)\uff0c\u9019\u662f\u53ef\u9760\u6027\u4e2d\u4e00\u500b\u95dc\u9375\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u8a0e\u7684\u9762\u5411 - LLM \u5728\u4e0d\u78ba\u5b9a\u6216\u7121\u6cd5\u63d0\u4f9b\u660e\u78ba\u7b54\u6848\u6642\uff0c\u5728\u7dad\u6301\u554f\u7b54 (QA) \u4efb\u52d9\u8868\u73fe\u7684\u540c\u6642\uff0c\u62d2\u7d55\u56de\u7b54\u554f\u984c\u7684\u80fd\u529b\u3002\u5118\u7ba1\u5148\u524d\u7684\u7814\u7a76\u91cd\u9ede\u5728\u65bc\u4e86\u89e3 LLM \u7684\u56de\u61b6\u80fd\u529b\u6216\u5176\u8b58\u5225\u4e0d\u53ef\u601d\u8b70/\u7121\u89e3\u554f\u984c\u7684\u80fd\u529b\uff0c\u4f46\u6211\u5011\u76f8\u4fe1\u9700\u8981\u4e00\u7a2e\u6709\u6548\u7684 AA \u8a55\u4f30\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u9ed1\u76d2\u8a55\u4f30\u65b9\u6cd5\uff0c\u7528\u65bc\u6aa2\u67e5\u548c\u4e86\u89e3 LLM \u5728\u5404\u7a2e\u591a\u9078\u984c QA \u4efb\u52d9\u4e2d\u7684 AA\u3002\u6211\u5011\u901a\u904e\u734e\u52f5\u6a21\u578b\u5728\u9810\u6e2c\u4e0d\u6b63\u78ba\u6216\u554f\u984c\u672c\u8cea\u4e0a\u7121\u6cd5\u56de\u7b54\u6642\u62d2\u7d55\u56de\u7b54\u4f86\u8861\u91cf AA\u3002\u6211\u5011\u7814\u7a76\u4e86\u56b4\u683c\u63d0\u793a\u3001\u8a00\u8a9e\u4fe1\u5fc3\u95be\u503c\u548c\u601d\u7dad\u93c8 (CoT) \u4e09\u7a2e\u7b56\u7565\uff0c\u4ee5\u4e86\u89e3\u5b83\u5011\u5c0d\u4e0d\u540c LLM \u4e2d\u6212\u9664\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u5118\u7ba1\u50cf GPT-4 \u9019\u6a23\u7684\u6700\u5148\u9032 LLM \u4e5f\u5728\u6212\u9664\u65b9\u9762\u82e6\u82e6\u6399\u624e\uff0c\u4f46 CoT \u7b49\u7b56\u7565\u63d0\u793a\u53ef\u4ee5\u986f\u8457\u589e\u5f37\u9019\u7a2e\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e\u6539\u9032 AA \u4e5f\u6709\u52a9\u65bc\u63d0\u9ad8\u6574\u9ad4 QA \u4efb\u52d9\u8868\u73fe\uff0c\u9019\u5f37\u8abf\u4e86\u8a55\u4f30 LLM \u4e2d AA \u7684\u91cd\u8981\u6027\u3002", "author": "Nishanth Madhusudhan et.al.", "authors": "Nishanth Madhusudhan, Sathwik Tejaswi Madhusudhan, Vikas Yadav, Masoud Hashemi", "id": "2407.16221v1", "paper_url": "http://arxiv.org/abs/2407.16221v1", "repo": "null"}}