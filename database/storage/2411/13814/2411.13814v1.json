{"2411.13814": {"publish_time": "2024-11-21", "title": "AutoMixQ: Self-Adjusting Quantization for High Performance Memory-Efficient Fine-Tuning", "paper_summary": "Fine-tuning large language models (LLMs) under resource constraints is a\nsignificant challenge in deep learning. Low-Rank Adaptation (LoRA), pruning,\nand quantization are all effective methods for improving resource efficiency.\nHowever, combining them directly often results in suboptimal performance,\nespecially with uniform quantization across all model layers. This is due to\nthe complex, uneven interlayer relationships introduced by pruning,\nnecessitating more refined quantization strategies. To address this, we propose\nAutoMixQ, an end-to-end optimization framework that selects optimal\nquantization configurations for each LLM layer. AutoMixQ leverages lightweight\nperformance models to guide the selection process, significantly reducing time\nand computational resources compared to exhaustive search methods. By\nincorporating Pareto optimality, AutoMixQ balances memory usage and\nperformance, approaching the upper bounds of model capability under strict\nresource constraints. Our experiments on widely used benchmarks show that\nAutoMixQ reduces memory consumption while achieving superior performance. For\nexample, at a 30\\% pruning rate in LLaMA-7B, AutoMixQ achieved 66.21\\% on BoolQ\ncompared to 62.45\\% for LoRA and 58.96\\% for LoftQ, while reducing memory\nconsumption by 35.5\\% compared to LoRA and 27.5\\% compared to LoftQ.", "paper_summary_zh": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u4f4e\u79e9\u81ea\u9002\u5e94 (LoRA)\u3001\u526a\u679d\u548c\u91cf\u5316\u90fd\u662f\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u76f4\u63a5\u7ec4\u5408\u5b83\u4eec\u901a\u5e38\u4f1a\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6240\u6709\u6a21\u578b\u5c42\u4e0a\u8fdb\u884c\u5747\u5300\u91cf\u5316\u65f6\u3002\u8fd9\u662f\u7531\u4e8e\u526a\u679d\u5f15\u5165\u7684\u590d\u6742\u3001\u4e0d\u5747\u5300\u7684\u5c42\u95f4\u5173\u7cfb\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u91cf\u5316\u7b56\u7565\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AutoMixQ\uff0c\u8fd9\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u6bcf\u4e2a LLM \u5c42\u9009\u62e9\u6700\u4f73\u91cf\u5316\u914d\u7f6e\u3002AutoMixQ \u5229\u7528\u8f7b\u91cf\u7ea7\u6027\u80fd\u6a21\u578b\u6765\u6307\u5bfc\u9009\u62e9\u8fc7\u7a0b\uff0c\u4e0e\u7a77\u4e3e\u641c\u7d22\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u901a\u8fc7\u7ed3\u5408\u5e15\u7d2f\u6258\u6700\u4f18\u6027\uff0cAutoMixQ \u5e73\u8861\u4e86\u5185\u5b58\u4f7f\u7528\u548c\u6027\u80fd\uff0c\u5728\u4e25\u683c\u7684\u8d44\u6e90\u9650\u5236\u4e0b\u63a5\u8fd1\u6a21\u578b\u80fd\u529b\u7684\u4e0a\u9650\u3002\u6211\u4eec\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAutoMixQ \u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5728 LLaMA-7B \u4e2d\u4ee5 30% \u7684\u526a\u679d\u7387\uff0cAutoMixQ \u5728 BoolQ \u4e0a\u8fbe\u5230 66.21%\uff0c\u800c LoRA \u4e3a 62.45%\uff0cLoftQ \u4e3a 58.96%\uff0c\u540c\u65f6\u4e0e LoRA \u76f8\u6bd4\u51cf\u5c11\u4e86 35.5% \u7684\u5185\u5b58\u6d88\u8017\uff0c\u4e0e LoftQ \u76f8\u6bd4\u51cf\u5c11\u4e86 27.5%\u3002", "author": "Changhai Zhou et.al.", "authors": "Changhai Zhou, Shiyang Zhang, Yuhua Zhou, Zekai Liu, Shichao Weng", "id": "2411.13814v1", "paper_url": "http://arxiv.org/abs/2411.13814v1", "repo": "null"}}