{"2406.14137": {"publish_time": "2024-06-20", "title": "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "paper_summary": "Large vision-language models (LVLMs), while proficient in following\ninstructions and responding to diverse questions, invariably generate detailed\nresponses even when questions are ambiguous or unanswerable, leading to\nhallucinations and bias issues. Thus, it is essential for LVLMs to proactively\nengage with humans to ask for clarifications or additional information for\nbetter responses. In this study, we aim to shift LVLMs from passive answer\nproviders to proactive engaged partners. We begin by establishing a\nthree-tiered hierarchy for questions of invalid, ambiguous, and personalizable\nnature to measure the proactive engagement capabilities of LVLMs. Utilizing\nthis hierarchy, we create PIE, (ProactIve Engagement Evaluation) through GPT-4o\nand human annotators, consisting of 853 questions across six distinct,\nfine-grained question types that are verified by human annotators and\naccompanied with well-defined metrics. Our evaluations on \\benchmark indicate\npoor performance of existing LVLMs, with the best-performing open-weights model\nonly achieving an Aggregate Align Rate (AAR) of 0.28. In response, we introduce\nMACAROON, self-iMaginAtion for ContrAstive pReference OptimizatiON, which\ninstructs LVLMs to autonomously generate contrastive response pairs for\nunlabeled questions given the task description and human-crafted criteria.\nThen, the self-imagined data is formatted for conditional reinforcement\nlearning. Experimental results show MACAROON effectively improves LVLMs'\ncapabilities to be proactively engaged (0.84 AAR) while maintaining comparable\nperformance on general tasks.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u96d6\u7136\u64c5\u9577\u9075\u5faa\u6307\u793a\u4e26\u56de\u7b54\u5404\u7a2e\u554f\u984c\uff0c\u4f46\u5373\u4f7f\u5728\u554f\u984c\u6a21\u7a1c\u5169\u53ef\u6216\u7121\u6cd5\u56de\u7b54\u6642\uff0c\u5b83\u5011\u4e5f\u6703\u7522\u751f\u8a73\u7d30\u7684\u56de\u61c9\uff0c\u5c0e\u81f4\u51fa\u73fe\u5e7b\u89ba\u548c\u504f\u898b\u554f\u984c\u3002\u56e0\u6b64\uff0cLVLMs \u4e3b\u52d5\u8207\u4eba\u985e\u4e92\u52d5\u4ee5\u5c0b\u6c42\u6f84\u6e05\u6216\u984d\u5916\u8cc7\u8a0a\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u56de\u61c9\u81f3\u95dc\u91cd\u8981\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u5c07 LVLMs \u5f9e\u88ab\u52d5\u7684\u7b54\u6848\u63d0\u4f9b\u8005\u8f49\u8b8a\u70ba\u4e3b\u52d5\u53c3\u8207\u7684\u5408\u4f5c\u5925\u4f34\u3002\u6211\u5011\u9996\u5148\u70ba\u7121\u6548\u3001\u6a21\u7a1c\u5169\u53ef\u548c\u53ef\u500b\u4eba\u5316\u7684\u554f\u984c\u5efa\u7acb\u4e00\u500b\u4e09\u5c64\u7b49\u7d1a\uff0c\u4ee5\u8861\u91cf LVLMs \u7684\u4e3b\u52d5\u53c3\u8207\u80fd\u529b\u3002\u5229\u7528\u9019\u500b\u5c64\u7d1a\uff0c\u6211\u5011\u900f\u904e GPT-4o \u548c\u4eba\u5de5\u6a19\u8a18\u54e1\u5efa\u7acb PIE\uff08\u4e3b\u52d5\u53c3\u8207\u8a55\u4f30\uff09\uff0c\u5305\u542b\u516d\u7a2e\u4e0d\u540c\u3001\u7d30\u7dfb\u7684\u554f\u984c\u985e\u578b\u4e2d\u7684 853 \u500b\u554f\u984c\uff0c\u9019\u4e9b\u554f\u984c\u5df2\u7531\u4eba\u5de5\u6a19\u8a18\u54e1\u9a57\u8b49\u4e26\u9644\u4e0a\u660e\u78ba\u7684\u6307\u6a19\u3002\u6211\u5011\u5c0d \\benchmark \u7684\u8a55\u4f30\u8868\u660e\u73fe\u6709 LVLMs \u7684\u6548\u80fd\u4e0d\u4f73\uff0c\u6548\u80fd\u6700\u4f73\u7684\u958b\u653e\u6b0a\u91cd\u6a21\u578b\u50c5\u9054\u5230 0.28 \u7684\u7e3d\u9ad4\u5c0d\u9f4a\u7387 (AAR)\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 MACAROON\uff0c\u4e00\u7a2e\u7528\u65bc\u5c0d\u6bd4\u6027\u504f\u597d\u6700\u4f73\u5316\u7684\u81ea\u6211\u60f3\u50cf\uff0c\u5b83\u6307\u793a LVLMs \u5728\u7d66\u5b9a\u4efb\u52d9\u63cf\u8ff0\u548c\u4eba\u5de5\u5efa\u7acb\u7684\u6a19\u6e96\u4e0b\uff0c\u81ea\u4e3b\u7522\u751f\u672a\u6a19\u8a18\u554f\u984c\u7684\u5c0d\u6bd4\u56de\u61c9\u5c0d\u3002\u7136\u5f8c\uff0c\u5c07\u81ea\u6211\u60f3\u50cf\u7684\u8cc7\u6599\u683c\u5f0f\u5316\u70ba\u689d\u4ef6\u589e\u5f37\u5b78\u7fd2\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cMACAROON \u6709\u6548\u5730\u63d0\u5347\u4e86 LVLMs \u4e3b\u52d5\u53c3\u8207\u7684\u80fd\u529b\uff080.84 AAR\uff09\uff0c\u540c\u6642\u5728\u4e00\u822c\u4efb\u52d9\u4e0a\u7dad\u6301\u4e86\u76f8\u7576\u7684\u6548\u80fd\u3002", "author": "Shujin Wu et.al.", "authors": "Shujin Wu, Yi R. Fung, Sha Li, Yixin Wan, Kai-Wei Chang, Heng Ji", "id": "2406.14137v1", "paper_url": "http://arxiv.org/abs/2406.14137v1", "repo": "null"}}