{"2410.10553": {"publish_time": "2024-10-14", "title": "SLaNC: Static LayerNorm Calibration", "paper_summary": "The ever increasing sizes of Large Language Models (LLMs) beyond hundreds of\nbillions of parameters have generated enormous pressure on the manufacturers of\ndedicated hardware accelerators and made the innovative design of the latter\none of the most rapidly expanding fields of the AI industry. Various approaches\nhave been explored to enable efficient and accurate processing of LLMs on the\navailable accelerators given their computational and storage limitations. Among\nthese, various quantization techniques have become the main focus of the\ncommunity as a means of reducing the compute, communication and storage\nrequirements. Quantization to lower precision formats naturally poses a number\nof challenges caused by the limited range of the available value\nrepresentations. When it comes to processing the popular Transformer models on\nhardware, one of the main issues becomes calculation of the LayerNorm simply\nbecause accumulation of the variance requires a much wider dynamic range than\nthe hardware enables. In this article, we address this matter and propose a\ncomputationally-efficient scaling technique that can be easily applied to\nTransformer models during inference. Our method suggests a straightforward way\nof scaling the LayerNorm inputs based on the static weights of the immediately\npreceding linear layers. The scaling factors are computed offline, based solely\non the linear layer weights, hence no latency or computational overhead is\nadded during inference. Most importantly, our technique ensures that no\nnumerical issues such as overflow or underflow could happen during the compute.\nThis approach offers smooth, accurate and resource-effective inference across a\nwide range of hardware architectures. The article provides theoretical\njustification as well as supporting numerical simulations.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u898f\u6a21\u4e0d\u65b7\u589e\u52a0\uff0c\u5df2\u8d85\u904e\u6578\u5343\u5104\u500b\u53c3\u6578\uff0c\u5c0d\u5c08\u7528\u786c\u9ad4\u52a0\u901f\u5668\u7684\u88fd\u9020\u5546\u9020\u6210\u5de8\u5927\u58d3\u529b\uff0c\u4e26\u4f7f\u5f8c\u8005\u7684\u5275\u65b0\u8a2d\u8a08\u6210\u70ba AI \u7522\u696d\u767c\u5c55\u6700\u8fc5\u901f\u7684\u9818\u57df\u4e4b\u4e00\u3002\u70ba\u4e86\u5728\u8a08\u7b97\u548c\u5132\u5b58\u9650\u5236\u4e0b\uff0c\u5728\u73fe\u6709\u7684\u52a0\u901f\u5668\u4e0a\u80fd\u6709\u6548\u4e14\u6e96\u78ba\u5730\u8655\u7406 LLM\uff0c\u5df2\u63a2\u8a0e\u5404\u7a2e\u65b9\u6cd5\u3002\u5176\u4e2d\uff0c\u5404\u7a2e\u91cf\u5316\u6280\u8853\u5df2\u6210\u70ba\u793e\u7fa4\u95dc\u6ce8\u7684\u91cd\u9ede\uff0c\u4f5c\u70ba\u964d\u4f4e\u904b\u7b97\u3001\u901a\u8a0a\u548c\u5132\u5b58\u9700\u6c42\u7684\u65b9\u6cd5\u3002\u91cf\u5316\u70ba\u8f03\u4f4e\u7cbe\u5ea6\u7684\u683c\u5f0f\u81ea\u7136\u6703\u7522\u751f\u8a31\u591a\u6311\u6230\uff0c\u539f\u56e0\u662f\u53ef\u7528\u503c\u8868\u793a\u7684\u7bc4\u570d\u6709\u9650\u3002\u5728\u786c\u9ad4\u4e0a\u8655\u7406\u6d41\u884c\u7684 Transformer \u6a21\u578b\u6642\uff0c\u4e3b\u8981\u554f\u984c\u4e4b\u4e00\u5728\u65bc LayerNorm \u7684\u8a08\u7b97\uff0c\u539f\u56e0\u662f\u7d2f\u7a4d\u8b8a\u7570\u9700\u8981\u6bd4\u786c\u9ad4\u652f\u63f4\u7684\u52d5\u614b\u7bc4\u570d\u66f4\u5ee3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u6b64\u554f\u984c\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u8a08\u7b97\u6548\u7387\u9ad8\u7684\u7e2e\u653e\u6280\u8853\uff0c\u53ef\u8f15\u9b06\u61c9\u7528\u65bc\u63a8\u7406\u671f\u9593\u7684 Transformer \u6a21\u578b\u3002\u6211\u5011\u7684\u5efa\u8b70\u65b9\u6cd5\u662f\u6839\u64da\u7dca\u63a5\u5728\u5f8c\u7684\u7dda\u6027\u5c64\u7684\u975c\u614b\u6b0a\u91cd\uff0c\u7e2e\u653e LayerNorm \u8f38\u5165\u3002\u7e2e\u653e\u56e0\u5b50\u662f\u6839\u64da\u7dda\u6027\u5c64\u6b0a\u91cd\u96e2\u7dda\u8a08\u7b97\u7684\uff0c\u56e0\u6b64\u5728\u63a8\u7406\u671f\u9593\u4e0d\u6703\u589e\u52a0\u5ef6\u9072\u6216\u8a08\u7b97\u8ca0\u64d4\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u7684\u6280\u8853\u53ef\u78ba\u4fdd\u5728\u8a08\u7b97\u671f\u9593\u4e0d\u6703\u767c\u751f\u6ea2\u4f4d\u6216\u4e0b\u6ea2\u7b49\u6578\u503c\u554f\u984c\u3002\u6b64\u65b9\u6cd5\u53ef\u5728\u5404\u7a2e\u786c\u9ad4\u67b6\u69cb\u4e2d\u63d0\u4f9b\u6d41\u66a2\u3001\u6e96\u78ba\u4e14\u8cc7\u6e90\u6709\u6548\u7684\u63a8\u7406\u3002\u672c\u6587\u63d0\u4f9b\u4e86\u7406\u8ad6\u4f9d\u64da\u4ee5\u53ca\u652f\u63f4\u6578\u503c\u6a21\u64ec\u3002</paragraph>", "author": "Mahsa Salmani et.al.", "authors": "Mahsa Salmani, Nikita Trukhanov, Ilya Soloveychik", "id": "2410.10553v1", "paper_url": "http://arxiv.org/abs/2410.10553v1", "repo": "null"}}