{"2411.16618": {"publish_time": "2024-11-25", "title": "StructFormer: Document Structure-based Masked Attention and its Impact on Language Model Pre-Training", "paper_summary": "Most state-of-the-art techniques for Language Models (LMs) today rely on\ntransformer-based architectures and their ubiquitous attention mechanism.\nHowever, the exponential growth in computational requirements with longer input\nsequences confines Transformers to handling short passages. Recent efforts have\naimed to address this limitation by introducing selective attention mechanisms,\nnotably local and global attention. While sparse attention mechanisms, akin to\nfull attention in being Turing-complete, have been theoretically established,\ntheir practical impact on pre-training remains unexplored. This study focuses\non empirically assessing the influence of global attention on BERT\npre-training. The primary steps involve creating an extensive corpus of\nstructure-aware text through arXiv data, alongside a text-only counterpart. We\ncarry out pre-training on these two datasets, investigate shifts in attention\npatterns, and assess their implications for downstream tasks. Our analysis\nunderscores the significance of incorporating document structure into LM\nmodels, demonstrating their capacity to excel in more abstract tasks, such as\ndocument understanding.", "paper_summary_zh": "\u73fe\u4eca\u591a\u6578\u8a9e\u8a00\u6a21\u578b (LM) \u7684\u6700\u5148\u9032\u6280\u8853\u4ef0\u8cf4\u65bc\u57fa\u65bc\u8f49\u63db\u5668\u7684\u67b6\u69cb\u53ca\u5176\u666e\u904d\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u3002\n\u7136\u800c\uff0c\u96a8\u8457\u8f38\u5165\u5e8f\u5217\u8b8a\u9577\uff0c\u904b\u7b97\u9700\u6c42\u5448\u6307\u6578\u6210\u9577\uff0c\u5c07\u8f49\u63db\u5668\u9650\u5236\u5728\u8655\u7406\u77ed\u7bc7\u7ae0\u7bc0\u3002\u6700\u8fd1\u7684\u52aa\u529b\u65e8\u5728\u900f\u904e\u5f15\u5165\u9078\u64c7\u6027\u6ce8\u610f\u529b\u6a5f\u5236\u4f86\u89e3\u6c7a\u6b64\u9650\u5236\uff0c\u7279\u5225\u662f\u5c40\u90e8\u548c\u5168\u5c40\u6ce8\u610f\u529b\u3002\u96d6\u7136\u7a00\u758f\u6ce8\u610f\u529b\u6a5f\u5236\u985e\u4f3c\u65bc\u5728\u5716\u9748\u5b8c\u5099\u6027\u4e2d\u7684\u5b8c\u6574\u6ce8\u610f\u529b\uff0c\u5df2\u5728\u7406\u8ad6\u4e0a\u5efa\u7acb\uff0c\u4f46\u5176\u5c0d\u9810\u8a13\u7df4\u7684\u5be6\u969b\u5f71\u97ff\u4ecd\u672a\u63a2\u8a0e\u3002\u672c\u7814\u7a76\u5c08\u6ce8\u65bc\u7d93\u9a57\u8a55\u4f30\u5168\u5c40\u6ce8\u610f\u529b\u5c0d BERT \u9810\u8a13\u7df4\u7684\u5f71\u97ff\u3002\u4e3b\u8981\u6b65\u9a5f\u5305\u62ec\u900f\u904e arXiv \u8cc7\u6599\u5efa\u7acb\u5ee3\u6cdb\u7684\u7d50\u69cb\u611f\u77e5\u6587\u5b57\u8a9e\u6599\u5eab\uff0c\u4ee5\u53ca\u7d14\u6587\u5b57\u5c0d\u61c9\u7248\u672c\u3002\u6211\u5011\u5c0d\u9019\u5169\u500b\u8cc7\u6599\u96c6\u9032\u884c\u9810\u8a13\u7df4\uff0c\u63a2\u8a0e\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u8f49\u8b8a\uff0c\u4e26\u8a55\u4f30\u5176\u5c0d\u4e0b\u6e38\u4efb\u52d9\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u5206\u6790\u5f37\u8abf\u5c07\u6587\u4ef6\u7d50\u69cb\u7d0d\u5165 LM \u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u8b49\u660e\u5176\u5728\u66f4\u62bd\u8c61\u4efb\u52d9\uff08\u4f8b\u5982\u6587\u4ef6\u7406\u89e3\uff09\u4e2d\u8868\u73fe\u512a\u7570\u7684\u80fd\u529b\u3002", "author": "Kaustubh Ponkshe et.al.", "authors": "Kaustubh Ponkshe, Venkatapathy Subramanian, Natwar Modani, Ganesh Ramakrishnan", "id": "2411.16618v1", "paper_url": "http://arxiv.org/abs/2411.16618v1", "repo": "null"}}