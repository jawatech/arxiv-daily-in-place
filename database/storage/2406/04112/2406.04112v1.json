{"2406.04112": {"publish_time": "2024-06-06", "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation", "paper_summary": "While overparameterization in machine learning models offers great benefits\nin terms of optimization and generalization, it also leads to increased\ncomputational requirements as model sizes grow. In this work, we show that by\nleveraging the inherent low-dimensional structures of data and compressible\ndynamics within the model parameters, we can reap the benefits of\noverparameterization without the computational burdens. In practice, we\ndemonstrate the effectiveness of this approach for deep low-rank matrix\ncompletion as well as fine-tuning language models. Our approach is grounded in\ntheoretical findings for deep overparameterized low-rank matrix recovery, where\nwe show that the learning dynamics of each weight matrix are confined to an\ninvariant low-dimensional subspace. Consequently, we can construct and train\ncompact, highly compressed factorizations possessing the same benefits as their\noverparameterized counterparts. In the context of deep matrix completion, our\ntechnique substantially improves training efficiency while retaining the\nadvantages of overparameterization. For language model fine-tuning, we propose\na method called \"Deep LoRA\", which improves the existing low-rank adaptation\n(LoRA) technique, leading to reduced overfitting and a simplified\nhyperparameter setup, while maintaining comparable efficiency. We validate the\neffectiveness of Deep LoRA on natural language tasks, particularly when\nfine-tuning with limited data.", "paper_summary_zh": "\u96d6\u7136\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u4e2d\u7684\u904e\u5ea6\u53c3\u6578\u5316\u5728\u6700\u4f73\u5316\u548c\u6cdb\u5316\u65b9\u9762\u63d0\u4f9b\u4e86\u6975\u5927\u7684\u597d\u8655\uff0c\u4f46\u96a8\u8457\u6a21\u578b\u5927\u5c0f\u7684\u589e\u52a0\uff0c\u5b83\u4e5f\u5c0e\u81f4\u904b\u7b97\u9700\u6c42\u589e\u52a0\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u900f\u904e\u5229\u7528\u8cc7\u6599\u7684\u5167\u5728\u4f4e\u7dad\u5ea6\u7d50\u69cb\u548c\u6a21\u578b\u53c3\u6578\u4e2d\u7684\u53ef\u58d3\u7e2e\u52d5\u614b\uff0c\u6211\u5011\u53ef\u4ee5\u5728\u6c92\u6709\u904b\u7b97\u8ca0\u64d4\u7684\u60c5\u6cc1\u4e0b\u7372\u5f97\u904e\u5ea6\u53c3\u6578\u5316\u7684\u512a\u9ede\u3002\u5728\u5be6\u52d9\u4e0a\uff0c\u6211\u5011\u5c55\u793a\u4e86\u9019\u7a2e\u65b9\u6cd5\u5c0d\u65bc\u6df1\u5ea6\u4f4e\u79e9\u77e9\u9663\u5b8c\u6210\u529f\u80fd\u4ee5\u53ca\u5fae\u8abf\u8a9e\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u7684\u505a\u6cd5\u5960\u57fa\u65bc\u6df1\u5ea6\u904e\u5ea6\u53c3\u6578\u5316\u4f4e\u79e9\u77e9\u9663\u5fa9\u539f\u7684\u7406\u8ad6\u767c\u73fe\uff0c\u6211\u5011\u5728\u5176\u4e2d\u5c55\u793a\u4e86\u6bcf\u500b\u6b0a\u91cd\u77e9\u9663\u7684\u5b78\u7fd2\u52d5\u614b\u90fd\u4fb7\u9650\u65bc\u4e0d\u8b8a\u7684\u4f4e\u7dad\u5ea6\u5b50\u7a7a\u9593\u3002\u56e0\u6b64\uff0c\u6211\u5011\u53ef\u4ee5\u5efa\u69cb\u4e26\u8a13\u7df4\u51fa\u7dca\u6e4a\u3001\u9ad8\u5ea6\u58d3\u7e2e\u7684\u5206\u89e3\uff0c\u5b83\u5011\u5177\u6709\u8207\u904e\u5ea6\u53c3\u6578\u5316\u5c0d\u61c9\u9805\u76f8\u540c\u7684\u597d\u8655\u3002\u5728\u6df1\u5ea6\u77e9\u9663\u5b8c\u6210\u529f\u80fd\u7684\u80cc\u666f\u4e0b\uff0c\u6211\u5011\u7684\u6280\u8853\u5927\u5e45\u6539\u5584\u4e86\u8a13\u7df4\u6548\u7387\uff0c\u540c\u6642\u4fdd\u7559\u4e86\u904e\u5ea6\u53c3\u6578\u5316\u7684\u512a\u9ede\u3002\u5c0d\u65bc\u8a9e\u8a00\u6a21\u578b\u5fae\u8abf\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba\u300c\u6df1\u5ea6 LoRA\u300d\u7684\u65b9\u6cd5\uff0c\u5b83\u6539\u9032\u4e86\u73fe\u6709\u7684\u4f4e\u79e9\u9069\u61c9 (LoRA) \u6280\u8853\uff0c\u5c0e\u81f4\u904e\u5ea6\u64ec\u5408\u6e1b\u5c11\u548c\u7c21\u5316\u7684\u8d85\u53c3\u6578\u8a2d\u5b9a\uff0c\u540c\u6642\u7dad\u6301\u76f8\u7576\u7684\u6548\u7387\u3002\u6211\u5011\u9a57\u8b49\u4e86\u6df1\u5ea6 LoRA \u5728\u81ea\u7136\u8a9e\u8a00\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u5225\u662f\u5728\u4f7f\u7528\u6709\u9650\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u6642\u3002", "author": "Can Yaras et.al.", "authors": "Can Yaras, Peng Wang, Laura Balzano, Qing Qu", "id": "2406.04112v1", "paper_url": "http://arxiv.org/abs/2406.04112v1", "repo": "null"}}