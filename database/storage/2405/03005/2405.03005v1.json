{"2405.03005": {"publish_time": "2024-05-05", "title": "Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints", "paper_summary": "In safe Reinforcement Learning (RL), safety cost is typically defined as a\nfunction dependent on the immediate state and actions. In practice, safety\nconstraints can often be non-Markovian due to the insufficient fidelity of\nstate representation, and safety cost may not be known. We therefore address a\ngeneral setting where safety labels (e.g., safe or unsafe) are associated with\nstate-action trajectories. Our key contributions are: first, we design a safety\nmodel that specifically performs credit assignment to assess contributions of\npartial state-action trajectories on safety. This safety model is trained using\na labeled safety dataset. Second, using RL-as-inference strategy we derive an\neffective algorithm for optimizing a safe policy using the learned safety\nmodel. Finally, we devise a method to dynamically adapt the tradeoff\ncoefficient between reward maximization and safety compliance. We rewrite the\nconstrained optimization problem into its dual problem and derive a\ngradient-based method to dynamically adjust the tradeoff coefficient during\ntraining. Our empirical results demonstrate that this approach is highly\nscalable and able to satisfy sophisticated non-Markovian safety constraints.", "paper_summary_zh": "", "author": "Siow Meng Low et.al.", "authors": "Siow Meng Low,Akshat Kumar", "id": "2405.03005v1", "paper_url": "http://arxiv.org/abs/2405.03005v1", "repo": "null"}}