{"2504.16020": {"publish_time": "2025-04-22", "title": "AlphaGrad: Non-Linear Gradient Normalization Optimizer", "paper_summary": "We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer\naddressing the memory overhead and hyperparameter complexity of adaptive\nmethods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2\ngradient normalization followed by a smooth hyperbolic tangent transformation,\n$g' = \\tanh(\\alpha \\cdot \\tilde{g})$, controlled by a single steepness\nparameter $\\alpha$. Our contributions include: (1) the AlphaGrad algorithm\nformulation; (2) a formal non-convex convergence analysis guaranteeing\nstationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,\nTD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent\nperformance profile. While exhibiting instability in off-policy DQN, it\nprovides enhanced training stability with competitive results in TD3 (requiring\ncareful $\\alpha$ tuning) and achieves substantially superior performance in\non-policy PPO. These results underscore the critical importance of empirical\n$\\alpha$ selection, revealing strong interactions between the optimizer's\ndynamics and the underlying RL algorithm. AlphaGrad presents a compelling\nalternative optimizer for memory-constrained scenarios and shows significant\npromise for on-policy learning regimes where its stability and efficiency\nadvantages can be particularly impactful.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 AlphaGrad\uff0c\u9019\u662f\u4e00\u500b\u8a18\u61b6\u9ad4\u6548\u7387\u9ad8\u3001\u689d\u4ef6\u7121\u72c0\u614b\u7684\u512a\u5316\u5668\uff0c\u7528\u65bc\u89e3\u6c7a Adam \u7b49\u81ea\u9069\u61c9\u65b9\u6cd5\u7684\u8a18\u61b6\u9ad4\u958b\u92b7\u548c\u8d85\u53c3\u6578\u8907\u96dc\u6027\u554f\u984c\u3002AlphaGrad \u901a\u904e\u5f35\u91cf\u5f0f L2 \u68af\u5ea6\u6b78\u4e00\u5316\uff0c\u518d\u63a5\u8457\u9032\u884c\u5e73\u6ed1\u7684\u96d9\u66f2\u6b63\u5207\u8b8a\u63db\u4f86\u5be6\u73fe\u5c3a\u5ea6\u4e0d\u8b8a\u6027\uff0c$g' = \\tanh(\\alpha \\cdot \\tilde{g})$\uff0c\u7531\u55ae\u500b\u9661\u5ea6\u53c3\u6578 $\\alpha$ \u63a7\u5236\u3002\u6211\u5011\u7684\u8ca2\u737b\u5305\u62ec\uff1a(1) AlphaGrad \u6f14\u7b97\u6cd5\u7684\u516c\u5f0f\uff1b(2) \u4fdd\u8b49\u7a69\u614b\u6027\u7684\u6b63\u5f0f\u975e\u51f8\u6536\u6582\u5206\u6790\uff1b(3) \u5728\u5404\u7a2e\u5f37\u5316\u5b78\u7fd2\u57fa\u6e96\uff08DQN\u3001TD3\u3001PPO\uff09\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8a55\u4f30\u3002\u8207 Adam \u76f8\u6bd4\uff0cAlphaGrad \u5c55\u73fe\u51fa\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u74b0\u5883\u7684\u6027\u80fd\u8868\u73fe\u3002\u96d6\u7136\u5b83\u5728\u96e2\u7dda\u7b56\u7565 DQN \u4e2d\u8868\u73fe\u51fa\u4e0d\u7a69\u5b9a\u6027\uff0c\u4f46\u5728 TD3 \u4e2d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u8a13\u7df4\u7a69\u5b9a\u6027\u548c\u5177\u6709\u7af6\u722d\u529b\u7684\u7d50\u679c\uff08\u9700\u8981\u4ed4\u7d30\u8abf\u6574 $\\alpha$\uff09\uff0c\u4e26\u4e14\u5728\u7dda\u4e0a\u7b56\u7565 PPO \u4e2d\u5be6\u73fe\u4e86\u986f\u8457\u512a\u8d8a\u7684\u6027\u80fd\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u8abf\u4e86 $\\alpha$ \u7684\u7d93\u9a57\u9078\u64c7\u81f3\u95dc\u91cd\u8981\uff0c\u63ed\u793a\u4e86\u512a\u5316\u5668\u52d5\u614b\u8207\u5e95\u5c64\u5f37\u5316\u5b78\u7fd2\u6f14\u7b97\u6cd5\u4e4b\u9593\u7684\u5f37\u70c8\u4ea4\u4e92\u4f5c\u7528\u3002\u5c0d\u65bc\u8a18\u61b6\u9ad4\u53d7\u9650\u7684\u5834\u666f\uff0cAlphaGrad \u63d0\u4f9b\u4e86\u4e00\u500b\u5f15\u4eba\u6ce8\u76ee\u7684\u66ff\u4ee3\u512a\u5316\u5668\uff0c\u4e26\u5728\u7dda\u4e0a\u7b56\u7565\u5b78\u7fd2\u6a5f\u5236\u4e2d\u5c55\u73fe\u51fa\u5de8\u5927\u7684\u6f5b\u529b\uff0c\u56e0\u70ba\u5728\u9019\u4e9b\u6a5f\u5236\u4e2d\uff0c\u5b83\u7684\u7a69\u5b9a\u6027\u548c\u6548\u7387\u512a\u52e2\u6703\u8b8a\u5f97\u5c24\u5176\u91cd\u8981\u3002\n", "author": "Soham Sane et.al.", "authors": "Soham Sane", "id": "2504.16020v1", "paper_url": "http://arxiv.org/abs/2504.16020v1", "repo": "null"}}