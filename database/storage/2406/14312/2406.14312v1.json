{"2406.14312": {"publish_time": "2024-06-20", "title": "Infusing clinical knowledge into tokenisers for language models", "paper_summary": "This study introduces a novel knowledge enhanced tokenisation mechanism,\nK-Tokeniser, for clinical text processing. Technically, at initialisation\nstage, K-Tokeniser populates global representations of tokens based on semantic\ntypes of domain concepts (such as drugs or diseases) from either a domain\nontology like Unified Medical Language System or the training data of the task\nrelated corpus. At training or inference stage, sentence level localised\ncontext will be utilised for choosing the optimal global token representation\nto realise the semantic-based tokenisation. To avoid pretraining using the new\ntokeniser, an embedding initialisation approach is proposed to generate\nrepresentations for new tokens. Using three transformer-based language models,\na comprehensive set of experiments are conducted on four real-world datasets\nfor evaluating K-Tokeniser in a wide range of clinical text analytics tasks\nincluding clinical concept and relation extraction, automated clinical coding,\nclinical phenotype identification, and clinical research article\nclassification. Overall, our models demonstrate consistent improvements over\ntheir counterparts in all tasks. In particular, substantial improvements are\nobserved in the automated clinical coding task with 13\\% increase on Micro\n$F_1$ score. Furthermore, K-Tokeniser also shows significant capacities in\nfacilitating quicker converge of language models. Specifically, using\nK-Tokeniser, the language models would only require 50\\% of the training data\nto achieve the best performance of the baseline tokeniser using all training\ndata in the concept extraction task and less than 20\\% of the data for the\nautomated coding task. It is worth mentioning that all these improvements\nrequire no pre-training process, making the approach generalisable.", "paper_summary_zh": "\u672c\u7814\u7a76\u5f15\u5165\u4e00\u7a2e\u65b0\u7a4e\u7684\u77e5\u8b58\u589e\u5f37\u6a19\u8a18\u5316\u6a5f\u5236\uff0cK-Tokeniser\uff0c\u7528\u65bc\u81e8\u5e8a\u6587\u672c\u8655\u7406\u3002\u6280\u8853\u4e0a\uff0c\u5728\u521d\u59cb\u5316\u968e\u6bb5\uff0cK-Tokeniser \u6703\u6839\u64da\u4f86\u81ea\u9818\u57df\u6982\u5ff5\uff08\u4f8b\u5982\u85e5\u7269\u6216\u75be\u75c5\uff09\u7684\u8a9e\u7fa9\u985e\u578b\uff0c\u5f9e\u7d71\u4e00\u91ab\u5b78\u8a9e\u8a00\u7cfb\u7d71\u6216\u4efb\u52d9\u76f8\u95dc\u8a9e\u6599\u5eab\u7684\u8a13\u7df4\u8cc7\u6599\u4e2d\uff0c\u586b\u5145\u6a19\u8a18\u7684\u5168\u5c40\u8868\u793a\u3002\u5728\u8a13\u7df4\u6216\u63a8\u8ad6\u968e\u6bb5\uff0c\u53e5\u5b50\u7d1a\u5225\u7684\u5c40\u90e8\u5316\u4e0a\u4e0b\u6587\u5c07\u88ab\u7528\u65bc\u9078\u64c7\u6700\u4f73\u7684\u5168\u5c40\u6a19\u8a18\u8868\u793a\uff0c\u4ee5\u5be6\u73fe\u57fa\u65bc\u8a9e\u7fa9\u7684\u6a19\u8a18\u5316\u3002\u70ba\u4e86\u907f\u514d\u4f7f\u7528\u65b0\u7684\u6a19\u8a18\u5316\u5668\u9032\u884c\u9810\u8a13\u7df4\uff0c\u63d0\u51fa\u4e86\u4e00\u7a2e\u5d4c\u5165\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u4ee5\u7522\u751f\u65b0\u6a19\u8a18\u7684\u8868\u793a\u3002\u4f7f\u7528\u4e09\u7a2e\u57fa\u65bcTransformer\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u5c0d\u56db\u500b\u771f\u5be6\u4e16\u754c\u6578\u64da\u96c6\u9032\u884c\u4e86\u4e00\u7d44\u5168\u9762\u7684\u5be6\u9a57\uff0c\u4ee5\u8a55\u4f30 K-Tokeniser \u5728\u5ee3\u6cdb\u7684\u81e8\u5e8a\u6587\u672c\u5206\u6790\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\uff0c\u5305\u62ec\u81e8\u5e8a\u6982\u5ff5\u548c\u95dc\u4fc2\u63d0\u53d6\u3001\u81ea\u52d5\u81e8\u5e8a\u7de8\u78bc\u3001\u81e8\u5e8a\u8868\u578b\u8b58\u5225\u548c\u81e8\u5e8a\u7814\u7a76\u6587\u7ae0\u5206\u985e\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u6240\u6709\u4efb\u52d9\u4e2d\u90fd\u5c55\u793a\u51fa\u6bd4\u5176\u5c0d\u61c9\u6a21\u578b\u66f4\u4e00\u81f4\u7684\u6539\u9032\u3002\u7279\u5225\u662f\uff0c\u5728\u81ea\u52d5\u81e8\u5e8a\u7de8\u78bc\u4efb\u52d9\u4e2d\u89c0\u5bdf\u5230\u4e86\u986f\u8457\u7684\u6539\u9032\uff0cMicro $F_1$ \u5f97\u5206\u63d0\u9ad8\u4e86 13%\u3002\u6b64\u5916\uff0cK-Tokeniser \u9084\u986f\u793a\u51fa\u986f\u8457\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u4fc3\u9032\u8a9e\u8a00\u6a21\u578b\u66f4\u5feb\u7684\u6536\u6582\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u4f7f\u7528 K-Tokeniser\uff0c\u8a9e\u8a00\u6a21\u578b\u53ea\u9700\u8981 50% \u7684\u8a13\u7df4\u6578\u64da\u5373\u53ef\u5728\u6982\u5ff5\u63d0\u53d6\u4efb\u52d9\u4e2d\u9054\u5230\u57fa\u7dda\u6a19\u8a18\u5316\u5668\u4f7f\u7528\u6240\u6709\u8a13\u7df4\u6578\u64da\u7684\u6700\u4f73\u6027\u80fd\uff0c\u800c\u81ea\u52d5\u7de8\u78bc\u4efb\u52d9\u5247\u4e0d\u5230 20% \u7684\u6578\u64da\u3002\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u6240\u6709\u9019\u4e9b\u6539\u9032\u90fd\u4e0d\u9700\u8981\u9810\u8a13\u7df4\u904e\u7a0b\uff0c\u9019\u4f7f\u5f97\u8a72\u65b9\u6cd5\u5177\u6709\u666e\u904d\u6027\u3002", "author": "Abul Hasan et.al.", "authors": "Abul Hasan, Jinge Wu, Quang Ngoc Nguyen, Salom\u00e9 Andres, Imane Guellil, Huayu Zhang, Arlene Casey, Beatrice Alex, Bruce Guthrie, Honghan Wu", "id": "2406.14312v1", "paper_url": "http://arxiv.org/abs/2406.14312v1", "repo": "null"}}