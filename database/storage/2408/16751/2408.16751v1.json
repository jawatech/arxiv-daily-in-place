{"2408.16751": {"publish_time": "2024-08-29", "title": "A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models", "paper_summary": "Beyond maximum likelihood estimation (MLE), the standard objective of a\nlanguage model (LM) that optimizes good examples probabilities, many studies\nhave explored ways that also penalize bad examples for enhancing the quality of\noutput distribution, including unlikelihood training, exponential maximizing\naverage treatment effect (ExMATE), and direct preference optimization (DPO). To\nsystematically compare these methods and further provide a unified recipe for\nLM optimization, in this paper, we present a unique angle of gradient analysis\nof loss functions that simultaneously reward good examples and penalize bad\nones in LMs. Through both mathematical results and experiments on\nCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional\ncharacteristics among these methods. We find that ExMATE serves as a superior\nsurrogate for MLE, and that combining DPO with ExMATE instead of MLE further\nenhances both the statistical (5-7%) and generative (+18% win rate)\nperformance.", "paper_summary_zh": "\u9664\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1 (MLE) \u4e4b\u5916\uff0c\u8bed\u8a00\u6a21\u578b (LM) \u7684\u6807\u51c6\u76ee\u6807\u662f\u4f18\u5316\u597d\u4f8b\u5b50\u7684\u6982\u7387\uff0c\u8bb8\u591a\u7814\u7a76\u63a2\u7d22\u4e86\u540c\u65f6\u60e9\u7f5a\u574f\u4f8b\u5b50\u4ee5\u63d0\u9ad8\u8f93\u51fa\u5206\u5e03\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u975e\u4f3c\u7136\u8bad\u7ec3\u3001\u6307\u6570\u6700\u5927\u5316\u5e73\u5747\u5904\u7406\u6548\u679c (ExMATE) \u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO)\u3002\u4e3a\u4e86\u7cfb\u7edf\u5730\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u8fdb\u4e00\u6b65\u4e3a LM \u4f18\u5316\u63d0\u4f9b\u7edf\u4e00\u7684\u914d\u65b9\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5bf9\u635f\u5931\u51fd\u6570\u8fdb\u884c\u68af\u5ea6\u5206\u6790\u7684\u72ec\u7279\u89d2\u5ea6\uff0c\u8be5\u635f\u5931\u51fd\u6570\u540c\u65f6\u5956\u52b1\u597d\u4f8b\u5b50\u5e76\u60e9\u7f5a LM \u4e2d\u7684\u574f\u4f8b\u5b50\u3002\u901a\u8fc7\u5bf9 CausalDialogue \u548c Anthropic HH-RLHF \u6570\u636e\u96c6\u7684\u6570\u5b66\u7ed3\u679c\u548c\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u4e4b\u95f4\u4e0d\u540c\u7684\u529f\u80fd\u7279\u5f81\u3002\u6211\u4eec\u53d1\u73b0 ExMATE \u4f5c\u4e3a MLE \u7684\u4f18\u79c0\u66ff\u4ee3\u54c1\uff0c\u5e76\u4e14\u5c06 DPO \u4e0e ExMATE\uff08\u800c\u4e0d\u662f MLE\uff09\u7ed3\u5408\u4f7f\u7528\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7edf\u8ba1\uff085-7%\uff09\u548c\u751f\u6210\uff08+18% \u83b7\u80dc\u7387\uff09\u6027\u80fd\u3002", "author": "Yi-Lin Tuan et.al.", "authors": "Yi-Lin Tuan, William Yang Wang", "id": "2408.16751v1", "paper_url": "http://arxiv.org/abs/2408.16751v1", "repo": "null"}}