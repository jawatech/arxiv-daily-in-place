{"2405.20830": {"publish_time": "2024-05-31", "title": "Self-Augmented Preference Optimization: Off-Policy Paradigms for Language Model Alignment", "paper_summary": "Traditional language model alignment methods, such as Direct Preference\nOptimization (DPO), are limited by their dependence on static, pre-collected\npaired preference data, which hampers their adaptability and practical\napplicability. To overcome this limitation, we introduce Self-Augmented\nPreference Optimization (SAPO), an effective and scalable training paradigm\nthat does not require existing paired data. Building on the self-play concept,\nwhich autonomously generates negative responses, we further incorporate an\noff-policy learning pipeline to enhance data exploration and exploitation.\nSpecifically, we employ an Exponential Moving Average (EMA) model in\nconjunction with a replay buffer to enable dynamic updates of response\nsegments, effectively integrating real-time feedback with insights from\nhistorical data. Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B\nmodels across benchmarks, including the Open LLM Leaderboard, IFEval,\nAlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses\nestablished offline contrastive baselines, such as DPO and Odds Ratio\nPreference Optimization, and outperforms offline self-play methods like SPIN.\nOur code is available at https://github.com/yinyueqin/SAPO", "paper_summary_zh": "\u50b3\u7d71\u8a9e\u8a00\u6a21\u578b\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u4f8b\u5982\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\uff0c\u53d7\u5230\u4f9d\u8cf4\u975c\u614b\u3001\u9810\u5148\u6536\u96c6\u7684\u914d\u5c0d\u504f\u597d\u8cc7\u6599\u7684\u9650\u5236\uff0c\u9019\u6703\u59a8\u7919\u5176\u9069\u61c9\u6027\u548c\u5be6\u7528\u6027\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86\u81ea\u6211\u64f4\u5145\u504f\u597d\u6700\u4f73\u5316 (SAPO)\uff0c\u9019\u662f\u4e00\u7a2e\u6709\u6548\u4e14\u53ef\u64f4\u5145\u7684\u8a13\u7df4\u7bc4\u4f8b\uff0c\u4e0d\u9700\u8981\u73fe\u6709\u7684\u914d\u5c0d\u8cc7\u6599\u3002\u5efa\u7acb\u5728\u81ea\u6211\u5c0d\u5f08\u6982\u5ff5\u4e4b\u4e0a\uff0c\u5b83\u6703\u81ea\u52d5\u7522\u751f\u8ca0\u9762\u56de\u61c9\uff0c\u6211\u5011\u9032\u4e00\u6b65\u6574\u5408\u4e86\u4e00\u500b\u975e\u7b56\u7565\u5b78\u7fd2\u7ba1\u9053\uff0c\u4ee5\u589e\u5f37\u8cc7\u6599\u63a2\u7d22\u548c\u5229\u7528\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63a1\u7528\u6307\u6578\u79fb\u52d5\u5e73\u5747 (EMA) \u6a21\u578b\uff0c\u4e26\u7d50\u5408\u91cd\u64ad\u7de9\u885d\u5340\uff0c\u4ee5\u555f\u7528\u56de\u61c9\u7247\u6bb5\u7684\u52d5\u614b\u66f4\u65b0\uff0c\u6709\u6548\u5730\u5c07\u5373\u6642\u56de\u994b\u8207\u6b77\u53f2\u8cc7\u6599\u7684\u898b\u89e3\u6574\u5408\u5728\u4e00\u8d77\u3002\u6211\u5011\u5c0d LLaMA3-8B \u548c Mistral-7B \u6a21\u578b\u5728\u57fa\u6e96\u4e0a\u7684\u5168\u9762\u8a55\u4f30\uff0c\u5305\u62ec Open LLM Leaderboard\u3001IFEval\u3001AlpacaEval 2.0 \u548c MT-Bench\uff0c\u8b49\u660e SAPO \u5339\u914d\u6216\u8d85\u8d8a\u5df2\u5efa\u7acb\u7684\u96e2\u7dda\u5c0d\u6bd4\u57fa\u6e96\uff0c\u4f8b\u5982 DPO \u548c\u6a5f\u7387\u6bd4\u504f\u597d\u6700\u4f73\u5316\uff0c\u4e26\u4e14\u512a\u65bc\u96e2\u7dda\u81ea\u6211\u5c0d\u5f08\u65b9\u6cd5\uff0c\u4f8b\u5982 SPIN\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/yinyueqin/SAPO \u53d6\u5f97", "author": "Yueqin Yin et.al.", "authors": "Yueqin Yin, Zhendong Wang, Yujia Xie, Weizhu Chen, Mingyuan Zhou", "id": "2405.20830v1", "paper_url": "http://arxiv.org/abs/2405.20830v1", "repo": "null"}}