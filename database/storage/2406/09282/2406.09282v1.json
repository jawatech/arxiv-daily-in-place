{"2406.09282": {"publish_time": "2024-06-13", "title": "On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models", "paper_summary": "The Open Whisper-style Speech Model (OWSM) series was introduced to achieve\nfull transparency in building advanced speech-to-text (S2T) foundation models.\nTo this end, OWSM models are trained on 25 public speech datasets, which are\nheterogeneous in multiple ways. In this study, we advance the OWSM series by\nintroducing OWSM v3.2, which improves on prior models by investigating and\naddressing the impacts of this data heterogeneity. Our study begins with a\ndetailed analysis of each dataset, from which we derive two key strategies:\ndata filtering with proxy task to enhance data quality, and the incorporation\nof punctuation and true-casing using an open large language model (LLM). With\nall other configurations staying the same, OWSM v3.2 improves performance over\nthe OWSM v3.1 baseline while using 15% less training data.", "paper_summary_zh": "Open Whisper \u98a8\u683c\u8a9e\u97f3\u6a21\u578b (OWSM) \u7cfb\u5217\u7684\u63a8\u51fa\uff0c\u65e8\u5728\u5efa\u69cb\u9032\u968e\u8a9e\u97f3\u8f49\u6587\u5b57 (S2T) \u57fa\u790e\u6a21\u578b\u6642\uff0c\u9054\u6210\u5b8c\u5168\u900f\u660e\u3002\u70ba\u6b64\uff0cOWSM \u6a21\u578b\u6703\u5728 25 \u500b\u516c\u958b\u8a9e\u97f3\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u9019\u4e9b\u8cc7\u6599\u96c6\u5728\u591a\u65b9\u9762\u90fd\u662f\u7570\u8cea\u7684\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e\u63a8\u51fa OWSM v3.2 \u4f86\u63a8\u9032 OWSM \u7cfb\u5217\uff0c\u85c9\u7531\u8abf\u67e5\u548c\u8655\u7406\u8cc7\u6599\u7570\u8cea\u6027\u7684\u5f71\u97ff\uff0c\u4f86\u6539\u5584\u5148\u524d\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u7814\u7a76\u5f9e\u5c0d\u6bcf\u500b\u8cc7\u6599\u96c6\u7684\u8a73\u7d30\u5206\u6790\u958b\u59cb\uff0c\u7531\u6b64\u6211\u5011\u884d\u751f\u51fa\u5169\u500b\u95dc\u9375\u7b56\u7565\uff1a\u4f7f\u7528\u4ee3\u7406\u4efb\u52d9\u9032\u884c\u8cc7\u6599\u904e\u6ffe\u4ee5\u63d0\u5347\u8cc7\u6599\u54c1\u8cea\uff0c\u4ee5\u53ca\u4f7f\u7528\u958b\u653e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u52a0\u5165\u6a19\u9ede\u7b26\u865f\u548c\u771f\u5be6\u5927\u5c0f\u5beb\u3002\u5728\u6240\u6709\u5176\u4ed6\u8a2d\u5b9a\u90fd\u4fdd\u6301\u4e0d\u8b8a\u7684\u60c5\u6cc1\u4e0b\uff0cOWSM v3.2 \u5728\u4f7f\u7528\u5c11 15% \u8a13\u7df4\u8cc7\u6599\u7684\u540c\u6642\uff0c\u6539\u5584\u4e86 OWSM v3.1 \u57fa\u6e96\u7684\u6548\u80fd\u3002", "author": "Jinchuan Tian et.al.", "authors": "Jinchuan Tian, Yifan Peng, William Chen, Kwanghee Choi, Karen Livescu, Shinji Watanabe", "id": "2406.09282v1", "paper_url": "http://arxiv.org/abs/2406.09282v1", "repo": "null"}}