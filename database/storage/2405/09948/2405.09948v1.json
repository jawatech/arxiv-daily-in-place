{"2405.09948": {"publish_time": "2024-05-16", "title": "Mitigating Text Toxicity with Counterfactual Generation", "paper_summary": "Toxicity mitigation consists in rephrasing text in order to remove offensive\nor harmful meaning. Neural natural language processing (NLP) models have been\nwidely used to target and mitigate textual toxicity. However, existing methods\nfail to detoxify text while preserving the initial non-toxic meaning at the\nsame time. In this work, we propose to apply counterfactual generation methods\nfrom the eXplainable AI (XAI) field to target and mitigate textual toxicity. In\nparticular, we perform text detoxification by applying local feature importance\nand counterfactual generation methods to a toxicity classifier distinguishing\nbetween toxic and non-toxic texts. We carry out text detoxification through\ncounterfactual generation on three datasets and compare our approach to three\ncompetitors. Automatic and human evaluations show that recently developed NLP\ncounterfactual generators can mitigate toxicity accurately while better\npreserving the meaning of the initial text as compared to classical\ndetoxification methods. Finally, we take a step back from using automated\ndetoxification tools, and discuss how to manage the polysemous nature of\ntoxicity and the risk of malicious use of detoxification tools. This work is\nthe first to bridge the gap between counterfactual generation and text\ndetoxification and paves the way towards more practical application of XAI\nmethods.", "paper_summary_zh": "\u6bd2\u6027\u7de9\u89e3\u5305\u542b\u6539\u5beb\u6587\u5b57\u4ee5\u79fb\u9664\u5192\u72af\u6216\u6709\u5bb3\u7684\u610f\u7fa9\u3002\u795e\u7d93\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u6a21\u578b\u5df2\u88ab\u5ee3\u6cdb\u7528\u65bc\u9396\u5b9a\u548c\u7de9\u89e3\u6587\u5b57\u6bd2\u6027\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u7121\u6cd5\u5728\u540c\u6642\u4fdd\u7559\u6700\u521d\u975e\u6bd2\u6027\u610f\u7fa9\u7684\u60c5\u6cc1\u4e0b\u5c0d\u6587\u5b57\u9032\u884c\u89e3\u6bd2\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u8b70\u61c9\u7528\u4f86\u81ea\u53ef\u89e3\u91cb AI (XAI) \u9818\u57df\u7684\u53cd\u4e8b\u5be6\u751f\u6210\u65b9\u6cd5\u4f86\u9396\u5b9a\u548c\u7de9\u89e3\u6587\u5b57\u6bd2\u6027\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u900f\u904e\u5c07\u5c40\u90e8\u7279\u5fb5\u91cd\u8981\u6027\u548c\u53cd\u4e8b\u5be6\u751f\u6210\u65b9\u6cd5\u61c9\u7528\u65bc\u5340\u5206\u6709\u6bd2\u548c\u7121\u6bd2\u6587\u5b57\u7684\u6bd2\u6027\u5206\u985e\u5668\u4f86\u57f7\u884c\u6587\u5b57\u89e3\u6bd2\u3002\u6211\u5011\u900f\u904e\u5728\u4e09\u500b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u53cd\u4e8b\u5be6\u751f\u6210\u4f86\u57f7\u884c\u6587\u5b57\u89e3\u6bd2\uff0c\u4e26\u5c07\u6211\u5011\u7684\u65b9\u6cd5\u8207\u4e09\u500b\u7af6\u722d\u8005\u9032\u884c\u6bd4\u8f03\u3002\u81ea\u52d5\u548c\u4eba\u5de5\u8a55\u4f30\u986f\u793a\uff0c\u6700\u65b0\u958b\u767c\u7684 NLP \u53cd\u4e8b\u5be6\u751f\u6210\u5668\u53ef\u4ee5\u6e96\u78ba\u5730\u7de9\u89e3\u6bd2\u6027\uff0c\u540c\u6642\u6bd4\u50b3\u7d71\u7684\u89e3\u6bd2\u65b9\u6cd5\u66f4\u597d\u5730\u4fdd\u7559\u539f\u59cb\u6587\u5b57\u7684\u610f\u7fa9\u3002\u6700\u5f8c\uff0c\u6211\u5011\u4e0d\u518d\u4f7f\u7528\u81ea\u52d5\u89e3\u6bd2\u5de5\u5177\uff0c\u4e26\u8a0e\u8ad6\u5982\u4f55\u7ba1\u7406\u6bd2\u6027\u7684\u591a\u7fa9\u6027\u4ee5\u53ca\u89e3\u6bd2\u5de5\u5177\u88ab\u60e1\u610f\u4f7f\u7528\u7684\u98a8\u96aa\u3002\u9019\u9805\u5de5\u4f5c\u9996\u6b21\u6a4b\u63a5\u4e86\u53cd\u4e8b\u5be6\u751f\u6210\u548c\u6587\u5b57\u89e3\u6bd2\u4e4b\u9593\u7684\u5dee\u8ddd\uff0c\u4e26\u70ba XAI \u65b9\u6cd5\u7684\u66f4\u5be6\u969b\u61c9\u7528\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Milan Bhan et.al.", "authors": "Milan Bhan, Jean-Noel Vittaut, Nina Achache, Victor Legrand, Nicolas Chesneau, Annabelle Blangero, Juliette Murris, Marie-Jeanne Lesot", "id": "2405.09948v1", "paper_url": "http://arxiv.org/abs/2405.09948v1", "repo": "null"}}