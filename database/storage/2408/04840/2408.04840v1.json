{"2408.04840": {"publish_time": "2024-08-09", "title": "mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models", "paper_summary": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in executing instructions for a variety of single-image tasks.\nDespite this progress, significant challenges remain in modeling long image\nsequences. In this work, we introduce the versatile multi-modal large language\nmodel, mPLUG-Owl3, which enhances the capability for long image-sequence\nunderstanding in scenarios that incorporate retrieved image-text knowledge,\ninterleaved image-text, and lengthy videos. Specifically, we propose novel\nhyper attention blocks to efficiently integrate vision and language into a\ncommon language-guided semantic space, thereby facilitating the processing of\nextended multi-image scenarios. Extensive experimental results suggest that\nmPLUG-Owl3 achieves state-of-the-art performance among models with a similar\nsize on single-image, multi-image, and video benchmarks. Moreover, we propose a\nchallenging long visual sequence evaluation named Distractor Resistance to\nassess the ability of models to maintain focus amidst distractions. Finally,\nwith the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance\non ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to\nthe development of more efficient and powerful multimodal large language\nmodels.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5df2\u5c55\u793a\u51fa\u6267\u884c\u5404\u79cd\u5355\u56fe\u50cf\u4efb\u52a1\u6307\u4ee4\u7684\u975e\u51e1\u80fd\u529b\u3002\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fd9\u4e9b\u8fdb\u5c55\uff0c\u5728\u5bf9\u957f\u56fe\u50cf\u5e8f\u5217\u8fdb\u884c\u5efa\u6a21\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u591a\u529f\u80fd\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b mPLUG-Owl3\uff0c\u5b83\u589e\u5f3a\u4e86\u5728\u7ed3\u5408\u4e86\u68c0\u7d22\u5230\u7684\u56fe\u50cf\u6587\u672c\u77e5\u8bc6\u3001\u4ea4\u9519\u56fe\u50cf\u6587\u672c\u548c\u5197\u957f\u89c6\u9891\u7684\u60c5\u51b5\u4e0b\u5bf9\u957f\u56fe\u50cf\u5e8f\u5217\u8fdb\u884c\u7406\u89e3\u7684\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u8d85\u6ce8\u610f\u529b\u5757\uff0c\u4ee5\u5c06\u89c6\u89c9\u548c\u8bed\u8a00\u6709\u6548\u5730\u6574\u5408\u5230\u4e00\u4e2a\u5171\u540c\u7684\u8bed\u8a00\u5f15\u5bfc\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u6269\u5c55\u591a\u56fe\u50cf\u573a\u666f\u7684\u5904\u7406\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cmPLUG-Owl3 \u5728\u5177\u6709\u76f8\u4f3c\u5927\u5c0f\u7684\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u5355\u56fe\u50cf\u3001\u591a\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u7684\u6700\u65b0\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u89c6\u89c9\u5e8f\u5217\u8bc4\u4f30\uff0c\u540d\u4e3a\u5e72\u6270\u62b5\u6297\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u5728\u5e72\u6270\u4e2d\u4fdd\u6301\u4e13\u6ce8\u7684\u80fd\u529b\u3002\u6700\u540e\uff0c\u51ed\u501f\u6240\u63d0\u51fa\u7684\u67b6\u6784\uff0cmPLUG-Owl3 \u5728\u8d85\u957f\u89c6\u89c9\u5e8f\u5217\u8f93\u5165\u4e0a\u5c55\u793a\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002\u6211\u4eec\u5e0c\u671b mPLUG-Owl3 \u80fd\u591f\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u66f4\u5f3a\u5927\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u505a\u51fa\u8d21\u732e\u3002", "author": "Jiabo Ye et.al.", "authors": "Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou", "id": "2408.04840v1", "paper_url": "http://arxiv.org/abs/2408.04840v1", "repo": "null"}}