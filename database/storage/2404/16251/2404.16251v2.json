{"2404.16251": {"publish_time": "2024-04-24", "title": "Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions", "paper_summary": "Prompt leakage in large language models (LLMs) poses a significant security\nand privacy threat, particularly in retrieval-augmented generation (RAG)\nsystems. However, leakage in multi-turn LLM interactions along with mitigation\nstrategies has not been studied in a standardized manner. This paper\ninvestigates LLM vulnerabilities against prompt leakage across 4 diverse\ndomains and 10 closed- and open-source LLMs. Our unique multi-turn threat model\nleverages the LLM's sycophancy effect and our analysis dissects task\ninstruction and knowledge leakage in the LLM response. In a multi-turn setting,\nour threat model elevates the average attack success rate (ASR) to 86.2%,\nincluding a 99% leakage with GPT-4 and claude-1.3. We find that some black-box\nLLMs like Gemini show variable susceptibility to leakage across domains - they\nare more likely to leak contextual knowledge in the news domain compared to the\nmedical domain. Our experiments measure specific effects of 6 black-box defense\nstrategies, including a query-rewriter in the RAG scenario. Our proposed\nmulti-tier combination of defenses still has an ASR of 5.3% for black-box LLMs,\nindicating room for enhancement and future direction for LLM security research.", "paper_summary_zh": "", "author": "Divyansh Agarwal et.al.", "authors": "Divyansh Agarwal,Alexander R. Fabbri,Philippe Laban,Ben Risher,Shafiq Joty,Caiming Xiong,Chien-Sheng Wu", "id": "2404.16251v2", "paper_url": "http://arxiv.org/abs/2404.16251v2", "repo": "null"}}