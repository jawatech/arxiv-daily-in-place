{"2502.11008": {"publish_time": "2025-02-16", "title": "CounterBench: A Benchmark for Counterfactuals Reasoning in Large Language Models", "paper_summary": "Counterfactual reasoning is widely recognized as one of the most challenging\nand intricate aspects of causality in artificial intelligence. In this paper,\nwe evaluate the performance of large language models (LLMs) in counterfactual\nreasoning. In contrast to previous studies that primarily focus on commonsense\ncausal reasoning, where LLMs often rely on prior knowledge for inference, we\nspecifically assess their ability to perform counterfactual inference using a\nset of formal rules. To support this evaluation, we introduce a new benchmark\ndataset, CounterBench, comprising 1K counterfactual reasoning questions. The\ndataset is designed with varying levels of difficulty, diverse causal graph\nstructures, distinct types of counterfactual questions, and multiple\nnonsensical name variants. Our experiments demonstrate that counterfactual\nreasoning poses a significant challenge for LLMs, with most models performing\nat levels comparable to random guessing. To enhance LLM's counterfactual\nreasoning ability, we propose a novel reasoning paradigm, CoIn, which guides\nLLMs through iterative reasoning and backtracking to systematically explore\ncounterfactual solutions. Experimental results show that our method\nsignificantly improves LLM performance on counterfactual reasoning tasks and\nconsistently enhances performance across different LLMs.Our dataset is\navailable at https://huggingface.co/datasets/CounterBench/CounterBench.", "paper_summary_zh": "\u53cd\u4e8b\u5be6\u63a8\u7406\u88ab\u5ee3\u6cdb\u8a8d\u70ba\u662f\u4eba\u5de5\u667a\u6167\u4e2d\u56e0\u679c\u95dc\u4fc2\u6700\u5177\u6311\u6230\u6027\u548c\u8907\u96dc\u7684\u9762\u5411\u4e4b\u4e00\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u53cd\u4e8b\u5be6\u63a8\u7406\u4e2d\u7684\u8868\u73fe\u3002\u8207\u4e3b\u8981\u95dc\u6ce8\u5e38\u8b58\u56e0\u679c\u63a8\u7406\uff0c\u5176\u4e2d LLM \u7d93\u5e38\u4f9d\u8cf4\u5148\u9a57\u77e5\u8b58\u4f86\u9032\u884c\u63a8\u7406\u7684\u5148\u524d\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u5011\u7279\u5225\u8a55\u4f30\u5b83\u5011\u4f7f\u7528\u4e00\u7d44\u5f62\u5f0f\u898f\u5247\u57f7\u884c\u53cd\u4e8b\u5be6\u63a8\u7406\u7684\u80fd\u529b\u3002\u70ba\u4e86\u652f\u6301\u6b64\u8a55\u4f30\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7684\u57fa\u6e96\u8cc7\u6599\u96c6 CounterBench\uff0c\u5176\u4e2d\u5305\u542b 1K \u500b\u53cd\u4e8b\u5be6\u63a8\u7406\u554f\u984c\u3002\u8cc7\u6599\u96c6\u7684\u8a2d\u8a08\u5177\u6709\u4e0d\u540c\u7684\u96e3\u5ea6\u7b49\u7d1a\u3001\u591a\u6a23\u5316\u7684\u56e0\u679c\u5716\u7d50\u69cb\u3001\u4e0d\u540c\u985e\u578b\u7684\u53cd\u4e8b\u5be6\u554f\u984c\u548c\u591a\u7a2e\u7121\u610f\u7fa9\u7684\u540d\u7a31\u8b8a\u9ad4\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u53cd\u4e8b\u5be6\u63a8\u7406\u5c0d LLM \u69cb\u6210\u91cd\u5927\u6311\u6230\uff0c\u5927\u591a\u6578\u6a21\u578b\u7684\u8868\u73fe\u8207\u96a8\u6a5f\u731c\u6e2c\u76f8\u7576\u3002\u70ba\u4e86\u589e\u5f37 LLM \u7684\u53cd\u4e8b\u5be6\u63a8\u7406\u80fd\u529b\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u63a8\u7406\u7bc4\u4f8b CoIn\uff0c\u5b83\u5f15\u5c0e LLM \u900f\u904e\u53cd\u8986\u63a8\u7406\u548c\u56de\u6eaf\u7cfb\u7d71\u6027\u5730\u63a2\u7d22\u53cd\u4e8b\u5be6\u89e3\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u986f\u8457\u63d0\u5347 LLM \u5728\u53cd\u4e8b\u5be6\u63a8\u7406\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\uff0c\u4e26\u6301\u7e8c\u589e\u5f37\u4e0d\u540c LLM \u7684\u8868\u73fe\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u53ef\u5728 https://huggingface.co/datasets/CounterBench/CounterBench \u53d6\u5f97\u3002", "author": "Yuefei Chen et.al.", "authors": "Yuefei Chen, Vivek K. Singh, Jing Ma, Ruxiang Tang", "id": "2502.11008v1", "paper_url": "http://arxiv.org/abs/2502.11008v1", "repo": "null"}}