
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-03**|**Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos**|Jianrui Zhang et.al.|[2410.02763v1](http://arxiv.org/abs/2410.02763v1)|null|
|**2024-10-03**|**FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models**|Zhipei Xu et.al.|[2410.02761v1](http://arxiv.org/abs/2410.02761v1)|null|
|**2024-10-03**|**Erasing Conceptual Knowledge from Language Models**|Rohit Gandikota et.al.|[2410.02760v1](http://arxiv.org/abs/2410.02760v1)|[link](https://github.com/rohitgandikota/erasing-llm)|
|**2024-10-03**|**CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text**|Milan Straka et.al.|[2410.02756v1](http://arxiv.org/abs/2410.02756v1)|null|
|**2024-10-03**|**SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at 1% the Cost**|Jifan Zhang et.al.|[2410.02755v1](http://arxiv.org/abs/2410.02755v1)|null|
|**2024-10-03**|**Training Language Models on Synthetic Edit Sequences Improves Code Synthesis**|Ulyana Piterbarg et.al.|[2410.02749v1](http://arxiv.org/abs/2410.02749v1)|null|
|**2024-10-03**|**CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation**|Han He et.al.|[2410.02748v1](http://arxiv.org/abs/2410.02748v1)|null|
|**2024-10-03**|**Neutral residues: revisiting adapters for model extension**|Franck Signe Talla et.al.|[2410.02744v1](http://arxiv.org/abs/2410.02744v1)|null|
|**2024-10-03**|**MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions**|Yekun Chai et.al.|[2410.02743v1](http://arxiv.org/abs/2410.02743v1)|null|
|**2024-10-03**|**Grounding Large Language Models In Embodied Environment With Imperfect World Models**|Haolan Liu et.al.|[2410.02742v1](http://arxiv.org/abs/2410.02742v1)|null|
|**2024-10-03**|**Salient Information Prompting to Steer Content in Prompt-based Abstractive Summarization**|Lei Xu et.al.|[2410.02741v1](http://arxiv.org/abs/2410.02741v1)|null|
|**2024-10-03**|**Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models**|Zhengfeng Lai et.al.|[2410.02740v1](http://arxiv.org/abs/2410.02740v1)|null|
|**2024-10-03**|**Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge**|Jiayi Ye et.al.|[2410.02736v2](http://arxiv.org/abs/2410.02736v2)|null|
|**2024-10-03**|**Custom Non-Linear Model Predictive Control for Obstacle Avoidance in Indoor and Outdoor Environments**|Lara Laban et.al.|[2410.02732v1](http://arxiv.org/abs/2410.02732v1)|[link](https://github.com/larasupernovae/nmpc_flash_multi_obstacle)|
|**2024-10-03**|**DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects**|Zhaowei Wang et.al.|[2410.02730v1](http://arxiv.org/abs/2410.02730v1)|null|
|**2024-10-03**|**Unified Multi-Modal Interleaved Document Representation for Information Retrieval**|Jaewoo Lee et.al.|[2410.02729v1](http://arxiv.org/abs/2410.02729v1)|null|
|**2024-10-03**|**Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation**|Rohin Manvi et.al.|[2410.02725v1](http://arxiv.org/abs/2410.02725v1)|null|
|**2024-10-03**|**Large Language Models as Markov Chains**|Oussama Zekri et.al.|[2410.02724v1](http://arxiv.org/abs/2410.02724v1)|null|
|**2024-10-03**|**Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization**|Ryan C. Barron et.al.|[2410.02721v1](http://arxiv.org/abs/2410.02721v1)|null|
|**2024-10-03**|**Curvature Diversity-Driven Deformation and Domain Alignment for Point Cloud**|Mengxi Wu et.al.|[2410.02720v1](http://arxiv.org/abs/2410.02720v1)|[link](https://github.com/IdanAchituve/DefRec_and_PCM)|
|**2024-10-03**|**UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling for Retrieval-Augmented Generation**|Zixuan Li et.al.|[2410.02719v1](http://arxiv.org/abs/2410.02719v1)|null|
|**2024-10-03**|**Video Instruction Tuning With Synthetic Data**|Yuanhan Zhang et.al.|[2410.02713v2](http://arxiv.org/abs/2410.02713v2)|null|
|**2024-10-03**|**LLaVA-Critic: Learning to Evaluate Multimodal Models**|Tianyi Xiong et.al.|[2410.02712v1](http://arxiv.org/abs/2410.02712v1)|null|
|**2024-10-03**|**SteerDiff: Steering towards Safe Text-to-Image Diffusion Models**|Hongxiang Zhang et.al.|[2410.02710v1](http://arxiv.org/abs/2410.02710v1)|null|
|**2024-10-03**|**LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations**|Hadas Orgad et.al.|[2410.02707v1](http://arxiv.org/abs/2410.02707v1)|[link](https://github.com/technion-cs-nlp/llmsknow)|
|**2024-10-03**|**Selective Attention Improves Transformer**|Yaniv Leviathan et.al.|[2410.02703v1](http://arxiv.org/abs/2410.02703v1)|null|
|**2024-10-03**|**HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly**|Howard Yen et.al.|[2410.02694v1](http://arxiv.org/abs/2410.02694v1)|null|
|**2024-10-03**|**Discovering Clues of Spoofed LM Watermarks**|Thibaud Gloaguen et.al.|[2410.02693v1](http://arxiv.org/abs/2410.02693v1)|null|
|**2024-10-03**|**On the Proper Treatment of Tokenization in Psycholinguistics**|Mario Giulianelli et.al.|[2410.02691v1](http://arxiv.org/abs/2410.02691v1)|null|
|**2024-10-03**|**User-centric Immersive Communications in 6G: A Data-oriented Approach via Digital Twin**|Conghao Zhou et.al.|[2410.02688v1](http://arxiv.org/abs/2410.02688v1)|null|
|**2024-10-03**|**HiddenGuard: Fine-Grained Safe Generation with Specialized Representation Router**|Lingrui Mei et.al.|[2410.02684v1](http://arxiv.org/abs/2410.02684v1)|[link](https://github.com/Meirtz/HiddenGuard)|
|**2024-10-03**|**DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life**|Yu Ying Chiu et.al.|[2410.02683v1](http://arxiv.org/abs/2410.02683v1)|null|
|**2024-10-03**|**Distilling an End-to-End Voice Assistant Without Instruction Training Data**|William Held et.al.|[2410.02678v1](http://arxiv.org/abs/2410.02678v1)|null|
|**2024-10-03**|**CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of LLMs**|Yu Ying Chiu et.al.|[2410.02677v1](http://arxiv.org/abs/2410.02677v1)|null|
|**2024-10-03**|**FAN: Fourier Analysis Networks**|Yihong Dong et.al.|[2410.02675v1](http://arxiv.org/abs/2410.02675v1)|null|
|**2024-10-03**|**Examining Language Modeling Assumptions Using an Annotated Literary Dialect Corpus**|Craig Messner et.al.|[2410.02674v1](http://arxiv.org/abs/2410.02674v1)|[link](https://github.com/comp-int-hum/orthography-embedding-clustering)|
|**2024-10-03**|**Unsupervised Point Cloud Completion through Unbalanced Optimal Transport**|Taekyung Lee et.al.|[2410.02671v1](http://arxiv.org/abs/2410.02671v1)|null|
|**2024-10-03**|**AlphaIntegrator: Transformer Action Search for Symbolic Integration Proofs**|Mert Ünsal et.al.|[2410.02666v1](http://arxiv.org/abs/2410.02666v1)|null|
|**2024-10-03**|**Grounded Answers for Multi-agent Decision-making Problem through Generative World Model**|Zeyang Liu et.al.|[2410.02664v1](http://arxiv.org/abs/2410.02664v1)|null|
|**2024-10-03**|**How to Train Long-Context Language Models (Effectively)**|Tianyu Gao et.al.|[2410.02660v1](http://arxiv.org/abs/2410.02660v1)|[link](https://github.com/princeton-nlp/prolong)|
|**2024-10-03**|**Hate Personified: Investigating the role of LLMs in content moderation**|Sarah Masud et.al.|[2410.02657v1](http://arxiv.org/abs/2410.02657v1)|null|
|**2024-10-03**|**Scalable Simulation-free Entropic Unbalanced Optimal Transport**|Jaemoo Choi et.al.|[2410.02656v1](http://arxiv.org/abs/2410.02656v1)|null|
|**2024-10-03**|**Measuring and Improving Persuasiveness of Generative Models**|Somesh Singh et.al.|[2410.02653v1](http://arxiv.org/abs/2410.02653v1)|null|
|**2024-10-03**|**CAX: Cellular Automata Accelerated in JAX**|Maxence Faldor et.al.|[2410.02651v1](http://arxiv.org/abs/2410.02651v1)|null|
|**2024-10-03**|**Undesirable Memorization in Large Language Models: A Survey**|Ali Satvaty et.al.|[2410.02650v1](http://arxiv.org/abs/2410.02650v1)|null|
|**2024-10-03**|**Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection**|Song Li et.al.|[2410.02647v1](http://arxiv.org/abs/2410.02647v1)|null|
|**2024-10-03**|**Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents**|Hanrong Zhang et.al.|[2410.02644v1](http://arxiv.org/abs/2410.02644v1)|[link](https://github.com/agiresearch/asb)|
|**2024-10-03**|**Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers**|Shijie Chen et.al.|[2410.02642v1](http://arxiv.org/abs/2410.02642v1)|null|
|**2024-10-03**|**Plots Unlock Time-Series Understanding in Multimodal Models**|Mayank Daswani et.al.|[2410.02637v1](http://arxiv.org/abs/2410.02637v1)|null|
|**2024-10-03**|**Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning**|Tianxiang Hu et.al.|[2410.02631v1](http://arxiv.org/abs/2410.02631v1)|null|
|**2024-10-03**|**Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization**|Mikhail Persiianov et.al.|[2410.02628v1](http://arxiv.org/abs/2410.02628v1)|null|
|**2024-10-03**|**NL-Eye: Abductive NLI for Images**|Mor Ventura et.al.|[2410.02613v1](http://arxiv.org/abs/2410.02613v1)|null|
|**2024-10-03**|**IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?**|Akhilesh Aravapalli et.al.|[2410.02611v1](http://arxiv.org/abs/2410.02611v1)|null|
|**2024-10-03**|**Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in Under-Resourced Languages Using Explainable AI**|Mesay Gemeda Yigezu et.al.|[2410.02609v1](http://arxiv.org/abs/2410.02609v1)|null|
|**2024-10-03**|**Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative Prospect Theoretic Reinforcement Learning**|Olivier Lepel et.al.|[2410.02605v1](http://arxiv.org/abs/2410.02605v1)|null|
|**2024-10-03**|**Agents' Room: Narrative Generation through Multi-step Collaboration**|Fantine Huot et.al.|[2410.02603v1](http://arxiv.org/abs/2410.02603v1)|[link](https://github.com/google-deepmind/tell-me-a-story)|
|**2024-10-03**|**Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks**|Rui Hu et.al.|[2410.02596v1](http://arxiv.org/abs/2410.02596v1)|null|
|**2024-10-03**|**IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of Both Driver and Passengers**|Zihan Fang et.al.|[2410.02592v1](http://arxiv.org/abs/2410.02592v1)|null|
|**2024-10-03**|**Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions**|Angana Borah et.al.|[2410.02584v1](http://arxiv.org/abs/2410.02584v1)|[link](https://github.com/MichiganNLP/MultiAgent_ImplicitBias)|
|**2024-10-03**|**Deep Regression 2D-3D Ultrasound Registration for Liver Motion Correction in Focal Tumor Thermal Ablation**|Shuwei Xing et.al.|[2410.02579v1](http://arxiv.org/abs/2410.02579v1)|[link](https://github.com/xingorno/deepregs2v)|
|**2024-10-03**|**Convolutional Variational Autoencoders for Spectrogram Compression in Automatic Speech Recognition**|Olga Iakovenko et.al.|[2410.02560v2](http://arxiv.org/abs/2410.02560v2)|null|
|**2024-10-03**|**Improving Unsupervised Constituency Parsing via Maximizing Semantic Information**|Junjie Chen et.al.|[2410.02558v1](http://arxiv.org/abs/2410.02558v1)|null|
|**2024-10-03**|**ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration**|Zixiang Wang et.al.|[2410.02551v1](http://arxiv.org/abs/2410.02551v1)|null|
|**2024-10-03**|**Personalized Quantum Federated Learning for Privacy Image Classification**|Jinjing Shi et.al.|[2410.02547v1](http://arxiv.org/abs/2410.02547v1)|null|
|**2024-10-03**|**MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation**|Gurucharan Marthi Krishna Kumar et.al.|[2410.02458v1](http://arxiv.org/abs/2410.02458v1)|null|
|**2024-10-03**|**Algorithms For Automatic Accentuation And Transcription Of Russian Texts In Speech Recognition Systems**|Olga Iakovenko et.al.|[2410.02538v1](http://arxiv.org/abs/2410.02538v1)|null|
|**2024-10-03**|**Intelligence at the Edge of Chaos**|Shiyang Zhang et.al.|[2410.02536v1](http://arxiv.org/abs/2410.02536v1)|null|
|**2024-10-03**|**A Schema-aware Logic Reformulation for Graph Reachability**|Davide Di Pierro et.al.|[2410.02533v1](http://arxiv.org/abs/2410.02533v1)|null|
|**2024-10-03**|**Methods for Automatic Matrix Language Determination of Code-Switched Speech**|Olga Iakovenko et.al.|[2410.02521v1](http://arxiv.org/abs/2410.02521v1)|null|
|**2024-10-03**|**SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation**|Mucong Ding et.al.|[2410.02512v1](http://arxiv.org/abs/2410.02512v1)|null|
|**2024-10-03**|**Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration**|Yun Qu et.al.|[2410.02511v1](http://arxiv.org/abs/2410.02511v1)|[link](https://github.com/hijkzzz/pymarl2)|
|**2024-10-03**|**Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration**|Weikang Yuan et.al.|[2410.02507v1](http://arxiv.org/abs/2410.02507v1)|[link](https://github.com/yuanwk99/malr)|
|**2024-10-03**|**Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment**|Kai Liu et.al.|[2410.02505v1](http://arxiv.org/abs/2410.02505v1)|[link](https://github.com/kai-liu001/dog-iqa)|
|**2024-10-03**|**Mixed-Session Conversation with Egocentric Memory**|Jihyoung Jang et.al.|[2410.02503v1](http://arxiv.org/abs/2410.02503v1)|null|
|**2024-10-03**|**Defining Knowledge: Bridging Epistemology and Large Language Models**|Constanza Fierro et.al.|[2410.02499v1](http://arxiv.org/abs/2410.02499v1)|null|
|**2024-10-03**|**Dynamic Gradient Alignment for Online Data Mixing**|Simin Fan et.al.|[2410.02498v1](http://arxiv.org/abs/2410.02498v1)|null|
|**2024-10-03**|**DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM**|Xuchen Li et.al.|[2410.02492v1](http://arxiv.org/abs/2410.02492v1)|null|
|**2024-10-03**|**Meta-Models: An Architecture for Decoding LLM Behaviors Through Interpreted Embeddings and Natural Language**|Anthony Costarelli et.al.|[2410.02472v1](http://arxiv.org/abs/2410.02472v1)|null|
|**2024-10-03**|**Response Tuning: Aligning Large Language Models without Instruction**|Seokhyun An et.al.|[2410.02465v1](http://arxiv.org/abs/2410.02465v1)|null|
|**2024-10-03**|**Recurrent Few-Shot model for Document Verification**|Maxime Talarmain et.al.|[2410.02456v1](http://arxiv.org/abs/2410.02456v1)|null|
|**2024-10-03**|**Strong Preferences Affect the Robustness of Value Alignment**|Ziwei Xu et.al.|[2410.02451v1](http://arxiv.org/abs/2410.02451v1)|null|
|**2024-10-03**|**Clinnova Federated Learning Proof of Concept: Key Takeaways from a Cross-border Collaboration**|Julia Alekseenko et.al.|[2410.02443v1](http://arxiv.org/abs/2410.02443v1)|null|
|**2024-10-03**|**Embedded Topic Models Enhanced by Wikification**|Takashi Shibuya et.al.|[2410.02441v1](http://arxiv.org/abs/2410.02441v1)|null|
|**2024-10-03**|**Optimizing Adaptive Attacks against Content Watermarks for Language Models**|Abdulrahman Diaa et.al.|[2410.02440v1](http://arxiv.org/abs/2410.02440v1)|null|
|**2024-10-03**|**Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization**|Mingyang Wang et.al.|[2410.02433v1](http://arxiv.org/abs/2410.02433v1)|null|
|**2024-10-03**|**Predictive Attractor Models**|Ramy Mounir et.al.|[2410.02430v1](http://arxiv.org/abs/2410.02430v1)|null|
|**2024-10-03**|**IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models**|Tuo An et.al.|[2410.02429v2](http://arxiv.org/abs/2410.02429v2)|null|
|**2024-10-03**|**Collective Critics for Creative Story Generation**|Minwook Bae et.al.|[2410.02428v1](http://arxiv.org/abs/2410.02428v1)|[link](https://github.com/emnlp-2024-critics/collective-critics-for-creative-story-generation)|
|**2024-10-03**|**Learning the Latent Rules of a Game from Data: A Chess Story**|Ben Fauber et.al.|[2410.02426v1](http://arxiv.org/abs/2410.02426v1)|null|
|**2024-10-03**|**LLM-Pilot: Characterize and Optimize Performance of your LLM Inference Services**|Małgorzata Łazuka et.al.|[2410.02425v1](http://arxiv.org/abs/2410.02425v1)|null|
|**2024-10-03**|**MenakBERT -- Hebrew Diacriticizer**|Ido Cohen et.al.|[2410.02417v1](http://arxiv.org/abs/2410.02417v1)|null|
|**2024-10-03**|**SynCo: Synthetic Hard Negatives in Contrastive Learning for Better Unsupervised Visual Representations**|Nikolaos Giakoumoglou et.al.|[2410.02401v1](http://arxiv.org/abs/2410.02401v1)|[link](https://github.com/giakoumoglou/synco)|
|**2024-10-03**|**Parameter Competition Balancing for Model Merging**|Guodong Du et.al.|[2410.02396v1](http://arxiv.org/abs/2410.02396v1)|[link](https://github.com/duguodong7/pcb-merging)|
|**2024-10-03**|**Online Multi-Label Classification under Noisy and Changing Label Distribution**|Yizhang Zou et.al.|[2410.02394v1](http://arxiv.org/abs/2410.02394v1)|null|
|**2024-10-03**|**Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks**|Zeyu Feng et.al.|[2410.02389v1](http://arxiv.org/abs/2410.02389v1)|null|
|**2024-10-03**|**BiSSL: Bilevel Optimization for Self-Supervised Pre-Training and Fine-Tuning**|Gustav Wagner Zakarias et.al.|[2410.02387v1](http://arxiv.org/abs/2410.02387v1)|null|
|**2024-10-03**|**MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences**|Genta Indra Winata et.al.|[2410.02381v1](http://arxiv.org/abs/2410.02381v1)|[link](https://github.com/meta-metrics/metametrics)|
|**2024-10-03**|**Towards Comprehensive Detection of Chinese Harmful Memes**|Junyu Lu et.al.|[2410.02378v1](http://arxiv.org/abs/2410.02378v1)|[link](https://github.com/dut-lujunyu/toxicn_mm)|
|**2024-10-03**|**NTU-NPU System for Voice Privacy 2024 Challenge**|Nikita Kuzmin et.al.|[2410.02371v1](http://arxiv.org/abs/2410.02371v1)|null|
|**2024-10-03**|**From Concrete to Abstract: A Multimodal Generative Approach to Abstract Concept Learning**|Haodong Xie et.al.|[2410.02365v1](http://arxiv.org/abs/2410.02365v1)|null|

#### Abstracts
##### **Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos**
2410.02763v1 by Jianrui Zhang, Mu Cai, Yong Jae Lee

There has been growing sentiment recently that modern large multimodal models
(LMMs) have addressed most of the key challenges related to short video
comprehension. As a result, both academia and industry are gradually shifting
their attention towards the more complex challenges posed by understanding
long-form videos. However, is this really the case? Our studies indicate that
LMMs still lack many fundamental reasoning capabilities even when dealing with
short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation
benchmark encompassing 1000 short and natural video-caption pairs. We
demonstrate that existing LMMs severely struggle to distinguish temporal
differences between different actions and object transformations. For example,
the best model GPT-4o only obtains ~50% on our text and video scores, showing a
large gap compared to the human baseline of ~90%. All open-source multimodal
models and CLIP-based models perform much worse, producing mostly random chance
performance. Through this work, we shed light onto the fact that temporal
reasoning in short videos is a problem yet to be fully solved. The dataset and
evaluation code are available at https://vinoground.github.io.

摘要：近来有越来越多的观点认为，现代大型多模态模型 (LMM) 已解决了与短视频理解相关的大部分关键挑战。因此，学术界和工业界正逐渐将注意力转向理解长格式视频所带来的更复杂的挑战。然而，事实真的如此吗？我们的研究表明，即使在处理短视频时，LMM 仍然缺乏许多基本的推理能力。我们引入了 Vinoground，这是一个时间反事实 LMM 评估基准，包含 1000 个短而自然的视频字幕对。我们证明，现有的 LMM 在区分不同动作和对象转换之间的时间差异时遇到了严重困难。例如，最好的模型 GPT-4o 在我们的文本和视频得分上仅获得约 50%，与约 90% 的人类基准相比差距很大。所有开源多模态模型和基于 CLIP 的模型表现都更差，产生的主要是随机机会表现。通过这项工作，我们揭示了短视频中的时间推理是一个尚未完全解决的问题。数据集和评估代码可在 https://vinoground.github.io/ 获得。

##### **FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models**
2410.02761v1 by Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, Jian Zhang

The rapid development of generative AI is a double-edged sword, which not
only facilitates content creation but also makes image manipulation easier and
more difficult to detect. Although current image forgery detection and
localization (IFDL) methods are generally effective, they tend to face two
challenges: \textbf{1)} black-box nature with unknown detection principle,
\textbf{2)} limited generalization across diverse tampering methods (e.g.,
Photoshop, DeepFake, AIGC-Editing). To address these issues, we propose the
explainable IFDL task and design FakeShield, a multi-modal framework capable of
evaluating image authenticity, generating tampered region masks, and providing
a judgment basis based on pixel-level and image-level tampering clues.
Additionally, we leverage GPT-4o to enhance existing IFDL datasets, creating
the Multi-Modal Tamper Description dataSet (MMTD-Set) for training FakeShield's
tampering analysis capabilities. Meanwhile, we incorporate a Domain Tag-guided
Explainable Forgery Detection Module (DTE-FDM) and a Multi-modal Forgery
Localization Module (MFLM) to address various types of tamper detection
interpretation and achieve forgery localization guided by detailed textual
descriptions. Extensive experiments demonstrate that FakeShield effectively
detects and localizes various tampering techniques, offering an explainable and
superior solution compared to previous IFDL methods.

摘要：生成式 AI 的快速發展是一把雙面刃，它不僅促進了內容創作，還使影像處理更容易，且更難以偵測。儘管目前的影像偽造偵測與定位 (IFDL) 方法通常有效，但它們往往面臨兩項挑戰：\textbf{1)} 具有未知偵測原理的黑盒子性質，\textbf{2)} 在不同的竄改方法（例如 Photoshop、DeepFake、AIGC 編輯）中，有限的概化能力。為了解決這些問題，我們提出了可解釋的 IFDL 任務，並設計了 FakeShield，這是一個多模式架構，能夠評估影像真實性、產生竄改區域遮罩，並根據像素級別和影像級別竄改線索提供判斷依據。此外，我們利用 GPT-4o 來增強現有的 IFDL 資料集，建立多模式竄改描述資料集 (MMTD-Set)，以訓練 FakeShield 的竄改分析能力。同時，我們整合了網域標籤引導的可解釋偽造偵測模組 (DTE-FDM) 和多模式偽造定位模組 (MFLM)，以解決各種類型的竄改偵測詮釋，並根據詳細的文字描述，實現偽造定位。廣泛的實驗證明，與先前的 IFDL 方法相比，FakeShield 有效地偵測和定位各種竄改技術，提供了一個可解釋且優異的解決方案。

##### **Erasing Conceptual Knowledge from Language Models**
2410.02760v1 by Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau

Concept erasure in language models has traditionally lacked a comprehensive
evaluation framework, leading to incomplete assessments of effectiveness of
erasure methods. We propose an evaluation paradigm centered on three critical
criteria: innocence (complete knowledge removal), seamlessness (maintaining
conditional fluent generation), and specificity (preserving unrelated task
performance). Our evaluation metrics naturally motivate the development of
Erasure of Language Memory (ELM), a new method designed to address all three
dimensions. ELM employs targeted low-rank updates to alter output distributions
for erased concepts while preserving overall model capabilities including
fluency when prompted for an erased concept. We demonstrate ELM's efficacy on
biosecurity, cybersecurity, and literary domain erasure tasks. Comparative
analysis shows that ELM achieves superior performance across our proposed
metrics, including near-random scores on erased topic assessments, generation
fluency, maintained accuracy on unrelated benchmarks, and robustness under
adversarial attacks. Our code, data, and trained models are available at
https://elm.baulab.info

摘要：語言模型中的概念抹除傳統上缺乏一個全面的評估架構，導致對抹除方法的有效性進行不完整的評估。我們提出了一個評估範例，以三個關鍵標準為中心：無辜（完全知識移除）、無縫（維持條件流暢生成）和具體性（保留不相關任務的執行）。我們的評估指標自然地激勵了語言記憶抹除 (ELM) 的發展，這是一種旨在解決所有三個維度的新方法。ELM 使用目標低秩更新來改變抹除概念的輸出分佈，同時保留整體模型功能，包括在提示抹除概念時保持流暢性。我們展示了 ELM 在生物安全、網路安全和文學領域抹除任務上的功效。比較分析表明，ELM 在我們提出的指標上取得了優異的效能，包括在抹除主題評估中接近隨機的得分、生成流暢性、在不相關基準上保持準確性以及在對抗性攻擊下的穩健性。我們的程式碼、資料和訓練過的模型可在 https://elm.baulab.info 取得

##### **CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text**
2410.02756v1 by Milan Straka

We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on
Multilingual Coreference Resolution. In this third iteration of the shared
task, a novel objective is to also predict empty nodes needed for zero
coreference mentions (while the empty nodes were given on input in previous
years). This way, coreference resolution can be performed on raw text. We
evaluate two model variants: a~two-stage approach (where the empty nodes are
predicted first using a pretrained encoder model and then processed together
with sentence words by another pretrained model) and a single-stage approach
(where a single pretrained encoder model generates empty nodes, coreference
mentions, and coreference links jointly). In both settings, CorPipe surpasses
other participants by a large margin of 3.9 and 2.8 percent points,
respectively. The source code and the trained model are available at
https://github.com/ufal/crac2024-corpipe .

摘要：我們展示 CorPipe 24，這是 CRAC 2024 共享任務中多語言核心指稱解析的獲獎作品。在共享任務的第三次迭代中，一項新目標是預測零核心指稱提及所需的空節點（而空節點在過去幾年中是在輸入中給定的）。這樣，核心指稱解析就可以在原始文字上執行。我們評估了兩種模型變體：a~兩階段方法（其中首先使用預訓練編碼器模型預測空節點，然後由另一個預訓練模型與句子單詞一起處理）和單階段方法（其中單個預訓練編碼器模型共同生成空節點、核心指稱提及和核心指稱連結）。在這兩種設定中，CorPipe 分別以 3.9 和 2.8 個百分點的巨大差距超越了其他參與者。原始碼和訓練好的模型可在 https://github.com/ufal/crac2024-corpipe 取得。

##### **SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at 1% the Cost**
2410.02755v1 by Jifan Zhang, Robert Nowak

Creating specialized large language models requires vast amounts of clean,
special purpose data for training and fine-tuning. With only a handful of
existing large-scale, domain-specific datasets, creation of new datasets is
required in most applications. This requires the development of new
application-specific filtering of web-scale data. Filtering with a
high-performance, general-purpose LLM such as GPT-4o can be highly effective,
but this is extremely expensive at web-scale. This paper proposes SIEVE, a
lightweight alternative that matches GPT-4o accuracy at a fraction of the cost.
SIEVE can perform up to 500 filtering operations for the cost of one GPT-4o
filtering call. The key to SIEVE is a seamless integration of GPT-4o and
lightweight T5 models, using active learning to fine-tune T5 in the background
with a small number of calls to GPT-4o. Once trained, it performs as well as
GPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the
OpenWebText dataset, using five highly customized filter tasks targeting high
quality and domain-specific content. Our results demonstrate the effectiveness
and efficiency of our method in curating large, high-quality datasets for
language model training at a substantially lower cost (1%) than existing
techniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o
achieve similar accuracy, with human evaluators preferring SIEVE's filtering
results to those of GPT-4o.

摘要：<paragraph>建立專門的大語言模型需要大量乾淨且具有特定用途的資料，以進行訓練和微調。由於現有的大規模特定領域資料集只有少數幾個，因此在大部分應用程式中都必須建立新的資料集。這需要開發新的特定應用程式過濾網路規模資料的方式。使用高性能、通用 LLM（例如 GPT-4o）進行過濾可能會非常有效，但這在網路規模下極為昂貴。本文提出 SIEVE，這是一種輕量級的替代方案，其準確度與 GPT-4o 相同，但成本僅為其一小部分。SIEVE 可以執行多達 500 次過濾操作，其成本只相當於一次 GPT-4o 過濾呼叫。SIEVE 的關鍵是無縫整合 GPT-4o 和輕量級 T5 模型，使用主動學習在背景中微調 T5，並呼叫 GPT-4o 的次數很少。一旦完成訓練，其執行效能與 GPT-4o 一樣好，但成本卻只有後者的極小一部分。我們在 OpenWebText 資料集上對 SIEVE 進行實驗驗證，使用五項高度自訂的過濾任務，目標是高品質且特定於領域的內容。我們的結果證明了我們的方法在策展大型、高品質資料集以進行語言模型訓練的有效性和效率，其成本遠低於現有技術（1%）。為了進一步驗證 SIEVE，實驗顯示 SIEVE 和 GPT-4o 達到了類似的準確度，而人類評估者更偏好 SIEVE 的過濾結果，而非 GPT-4o 的過濾結果。</paragraph>

##### **Training Language Models on Synthetic Edit Sequences Improves Code Synthesis**
2410.02749v1 by Ulyana Piterbarg, Lerrel Pinto, Rob Fergus

Software engineers mainly write code by editing existing programs. In
contrast, large language models (LLMs) autoregressively synthesize programs in
a single pass. One explanation for this is the scarcity of open-sourced edit
data. While high-quality instruction data for code synthesis is already scarce,
high-quality edit data is even scarcer. To fill this gap, we develop a
synthetic data generation algorithm called LintSeq. This algorithm refactors
existing code into a sequence of code edits by using a linter to procedurally
sample across the error-free insertions that can be used to sequentially write
programs. It outputs edit sequences as text strings consisting of consecutive
program diffs. To test LintSeq, we use it to refactor a dataset of instruction
+ program pairs into instruction + program-diff-sequence tuples. Then, we
instruction finetune a series of smaller LLMs ranging from 2.6B to 14B
parameters on both the re-factored and original versions of this dataset,
comparing zero-shot performance on code synthesis benchmarks. We show that
during repeated sampling, edit sequence finetuned models produce more diverse
programs than baselines. This results in better inference-time scaling for
benchmark coverage as a function of samples, i.e. the fraction of problems
"pass@k" solved by any attempt given "k" tries. For example, on HumanEval
pass@50, small LLMs finetuned on synthetic edit sequences are competitive with
GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)
in absolute score. Finally, we also pretrain our own tiny LMs for code
understanding. We show that finetuning tiny models on synthetic code edits
results in state-of-the-art code synthesis for the on-device model class. Our
150M parameter edit sequence LM matches or outperforms code models with twice
as many parameters, both with and without repeated sampling, including Codex
and AlphaCode.

摘要：軟體工程師主要透過編輯現有程式來撰寫程式碼。相對而言，大型語言模型 (LLM) 會在單次傳遞中自動回歸地合成程式碼。其中一個解釋是開放原始碼編輯資料的稀少性。雖然用於程式碼合成的優質指令資料已經很稀少，但優質的編輯資料更為稀少。為了填補這個差距，我們開發了一種名為 LintSeq 的合成資料產生演算法。此演算法會透過使用 linter 循序漸進地對無錯誤插入進行抽樣，將現有程式碼重構為一系列程式碼編輯。它會以由連續程式碼差異組成的文字字串作為輸出編輯序列。為了測試 LintSeq，我們使用它將指令 + 程式碼配對資料集重構為指令 + 程式碼差異序列組。然後，我們對從 2.6B 到 14B 參數範圍內的一系列較小的 LLM 進行指令微調，同時針對此資料集的重構版本和原始版本進行微調，並比較程式碼合成基準上的零次學習效能。我們展示了在重複抽樣期間，編輯序列微調模型會產生比基準更為多樣化的程式碼。這會導致基準涵蓋範圍在樣本函數中的推論時間縮放效果更好，亦即在給定「k」次嘗試的任何嘗試中解決「pass@k」問題的比例。例如，在 HumanEval pass@50 中，針對合成編輯序列進行微調的小型 LLM 與 GPT-4 具有競爭力，且在絕對分數上比針對基準資料集進行微調的模型高出 +20% (+/-3%)。最後，我們也針對程式碼理解預訓練我們自己的小型 LM。我們展示了針對合成程式碼編輯進行微調的小型模型，會產生裝置上模型類別的最新程式碼合成。我們的 150M 參數編輯序列 LM 匹配或超越參數多一倍的程式碼模型，無論是否重複抽樣，其中包括 Codex 和 AlphaCode。

##### **CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation**
2410.02748v1 by Han He, Qianchu Liu, Lei Xu, Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff

Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned
to extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.

摘要：大型語言模型 (LLM) 可以使用提示技術跨領域生成流利的摘要，減少訓練模型以進行摘要應用程式的需求。然而，製作出有效的提示以引導 LLM 生成具有適當詳細程度和寫作風格的摘要仍然是一項挑戰。在本文中，我們探討了使用從原始文件提取的顯著資訊來增強摘要提示的用途。我們表明，在提示中加入關鍵字組可以改善 ROUGE F1 和召回率，使生成的摘要更類似於參考摘要且更完整。關鍵字組的數量可以控制精確度召回率權衡。此外，我們的分析表明，納入片語層級的顯著資訊優於詞彙或句子層級。然而，對幻覺的影響並非普遍對所有 LLM 都是正面的。為了進行此分析，我們引入了關鍵字組訊號提取器 (CriSPO)，這是一個可以微調以提取顯著關鍵字組的輕量級模型。透過使用 CriSPO，我們在沒有任何 LLM 自訂化的情況下，在資料集和開放權重與專有 LLM 中實現了一致的 ROUGE 改進。我們的研究結果提供了見解，說明如何利用顯著資訊來建構基於提示的摘要系統。

##### **Neutral residues: revisiting adapters for model extension**
2410.02744v1 by Franck Signe Talla, Herve Jegou, Edouard Grave

We address the problem of extending a pretrained large language model to a
new domain that was not seen at training time, like adding a language for which
the original model has seen no or little training data. Popular solutions like
fine-tuning or low-rank adaptation are successful at domain adaptation, but
formally they do not add any extra capacity and degrade the performance in the
original domain.
  Our paper analyzes this extension problem under three angles: data,
architecture and training procedure, which are advantageously considered
jointly. In particular, we improve adapters and make it possible to learn an
entire new language while ensuring that the output of the neural network is
almost unchanged in the original domain. For this purpose, we modify the new
residual blocks in a way that leads each new residual block to output
near-zeros in the original domain.
  This solution of neutral residues, which borrows architectural components
from mixture of experts, is effective: with only 20% extra learnable weights
compared to an original model trained on English, we get results that are
significantly better than concurrent approaches (fine-tuning, low-rank or
vanilla adapters) in terms of the trade-off between learning a new language and
not forgetting English.

摘要：<paragraph>我們解決了將預訓練的大語言模型擴充到訓練時未見過的新領域的問題，例如增加原始模型未見過或訓練資料很少的語言。微調或低階適應等熱門解決方案在領域適應方面很成功，但形式上它們不會增加任何額外容量，並會降低原始領域的效能。
  我們的論文從資料、架構和訓練程序三個角度分析這個擴充問題，這些角度有利於共同考量。特別是，我們改進了適配器，並讓學習一種全新語言成為可能，同時確保神經網路的輸出在原始領域幾乎保持不變。為此，我們修改了新的殘差區塊，讓每個新的殘差區塊在原始領域輸出接近零。
  這種中性殘差的解決方案借用了專家混合的架構元件，非常有效：與在英語上訓練的原始模型相比，僅增加 20% 可學習權重，我們獲得的結果在學習新語言和不忘英語之間的權衡方面顯著優於並行方法（微調、低階或香草適配器）。</paragraph>

##### **MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions**
2410.02743v1 by Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu

Reinforcement learning from human feedback (RLHF) has demonstrated
effectiveness in aligning large language models (LLMs) with human preferences.
However, token-level RLHF suffers from the credit assignment problem over long
sequences, where delayed rewards make it challenging for the model to discern
which actions contributed to successful outcomes. This hinders learning
efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple
yet effective RLHF framework that incorporates macro actions -- sequences of
tokens or higher-level language constructs -- into the learning process. By
operating at this higher level of abstraction, our approach reduces the
temporal distance between actions and rewards, facilitating faster and more
accurate credit assignment. This results in more stable policy gradient
estimates and enhances learning efficiency within each episode, all without
increasing computational complexity during training or inference. We validate
our approach through extensive experiments across various model sizes and
tasks, including text summarization, dialogue generation, question answering,
and program synthesis. Our method achieves substantial performance improvements
over standard RLHF, with performance gains of up to 30% in text summarization
and code generation, 18% in dialogue, and 8% in question answering tasks.
Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in
terms of training time and continues to outperform it with further training. We
will make our code and data publicly available at
https://github.com/ernie-research/MA-RLHF .

摘要：人類回饋強化學習 (RLHF) 已證明有效能使大型語言模型 (LLM) 與人類偏好保持一致。
然而，代幣級 RLHF 在長序列中會遇到信用分配問題，延遲的獎勵使得模型難以辨別哪些動作促成了成功的結果。這會阻礙學習效率並減緩收斂。在本文中，我們提出 MA-RLHF，一個簡單但有效的 RLHF 框架，它將巨動作序（代幣序列或更高級的語言結構）納入學習過程中。透過在這個更高層次的抽象中運作，我們的做法縮短了動作和獎勵之間的時間距離，促進了更快速且更準確的信用分配。這會產生更穩定的策略梯度估計，並在每個情節中增強學習效率，而無需增加訓練或推論期間的運算複雜性。我們透過各種模型大小和任務的廣泛實驗驗證了我們的做法，包括文本摘要、對話產生、問題解答和程式合成。我們的做法在標準 RLHF 上實現了顯著的效能提升，在文本摘要和程式產生中效能提升高達 30%，在對話中提升 18%，在問題解答任務中提升 8%。值得注意的是，我們的做法在訓練時間方面比傳統 RLHF 快 1.7 倍到 2 倍，並持續在進一步的訓練中表現優於它。我們將在 https://github.com/ernie-research/MA-RLHF 公開我們的程式碼和資料。

##### **Grounding Large Language Models In Embodied Environment With Imperfect World Models**
2410.02742v1 by Haolan Liu, Jishen Zhao

Despite a widespread success in various applications, large language models
(LLMs) often stumble when tackling basic physical reasoning or executing
robotics tasks, due to a lack of direct experience with the physical nuances of
the real world. To address these issues, we propose a Grounding Large language
model with Imperfect world MOdel (GLIMO), which utilizes proxy world models
such as simulators to collect and synthesize trining data. GLIMO incorporates
an LLM agent-based data generator to automatically create high-quality and
diverse instruction datasets. The generator includes an iterative self-refining
module for temporally consistent experience sampling, a diverse set of
question-answering instruction seeds, and a retrieval-augmented generation
module for reflecting on prior experiences. Comprehensive experiments show that
our approach improve the performance of strong open-source LLMs like LLaMA-3
with a performance boost of 2.04 $\times$, 1.54 $\times$, and 1.82 $\times$
across three different benchmarks, respectively. The performance is able to
compete with or surpass their larger counterparts such as GPT-4.

摘要：儘管大型語言模型 (LLM) 在各種應用中獲得廣泛成功，但由於缺乏對現實世界物理細微差別的直接經驗，它們在處理基本物理推理或執行機器人任務時經常會遇到困難。為了解決這些問題，我們提出了一個具有不完美世界模型 (GLIMO) 的基礎大型語言模型，它利用代理世界模型（例如模擬器）來收集和綜合訓練資料。GLIMO 結合了一個基於 LLM 代理的資料生成器，以自動建立高品質且多樣化的指令資料集。該生成器包含一個反覆自我精煉模組，用於時間一致的經驗取樣、一套多樣化的問答指令種子，以及一個檢索增強生成模組，用於反思先前的經驗。全面的實驗表明，我們的做法改善了強大的開源 LLM（例如 LLaMA-3）的效能，分別在三個不同的基準中提升了 2.04 倍、1.54 倍和 1.82 倍。該效能能夠與 GPT-4 等規模更大的同類產品競爭或超越它們。

##### **Salient Information Prompting to Steer Content in Prompt-based Abstractive Summarization**
2410.02741v1 by Lei Xu, Mohammed Asad Karim, Saket Dingliwal, Aparna Elangovan

Large language models (LLMs) can generate fluent summaries across domains
using prompting techniques, reducing the need to train models for summarization
applications. However, crafting effective prompts that guide LLMs to generate
summaries with the appropriate level of detail and writing style remains a
challenge. In this paper, we explore the use of salient information extracted
from the source document to enhance summarization prompts. We show that adding
keyphrases in prompts can improve ROUGE F1 and recall, making the generated
summaries more similar to the reference and more complete. The number of
keyphrases can control the precision-recall trade-off. Furthermore, our
analysis reveals that incorporating phrase-level salient information is
superior to word- or sentence-level. However, the impact on hallucination is
not universally positive across LLMs. To conduct this analysis, we introduce
Keyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned
to extract salient keyphrases. By using SigExt, we achieve consistent ROUGE
improvements across datasets and open-weight and proprietary LLMs without any
LLM customization. Our findings provide insights into leveraging salient
information in building prompt-based summarization systems.

摘要：大型語言模型 (LLM) 可以使用提示技術跨領域生成流利的摘要，減少了為摘要應用程式訓練模型的需要。然而，製作有效的提示以引導 LLM 生成具有適當詳細程度和寫作風格的摘要仍然是一個挑戰。在本文中，我們探討了從原始文件中提取顯著資訊以增強摘要提示的使用。我們展示了在提示中加入關鍵字組可以改善 ROUGE F1 和召回率，使生成的摘要更類似於參考摘要且更完整。關鍵字組的數量可以控制精確度召回率權衡。此外，我們的分析表明，納入短語層面的顯著資訊優於字詞或句子層面。然而，對幻覺的影響並非普遍存在於所有 LLM 中。為了進行此分析，我們引入了關鍵字組訊號萃取器 (SigExt)，這是一個輕量級模型，可以微調以萃取顯著的關鍵字組。透過使用 SigExt，我們在沒有任何 LLM 客製化的情況下，在資料集、開放權重和專有 LLM 中實現了一致的 ROUGE 改進。我們的發現提供了在建構基於提示的摘要系統中利用顯著資訊的見解。

##### **Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models**
2410.02740v1 by Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, Meng Cao, Yinfei Yang

Recent advancements in multimodal models highlight the value of rewritten
captions for improving performance, yet key challenges remain. For example,
while synthetic captions often provide superior quality and image-text
alignment, it is not clear whether they can fully replace AltTexts: the role of
synthetic captions and their interaction with original web-crawled AltTexts in
pre-training is still not well understood. Moreover, different multimodal
foundation models may have unique preferences for specific caption formats, but
efforts to identify the optimal captions for each model remain limited. In this
work, we propose a novel, controllable, and scalable captioning pipeline
designed to generate diverse caption formats tailored to various multimodal
models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic
Captions (DSC+) as case studies, we systematically explore their effects and
interactions with AltTexts across models such as CLIP, multimodal LLMs, and
diffusion models. Our findings reveal that a hybrid approach that keeps both
synthetic captions and AltTexts can outperform the use of synthetic captions
alone, improving both alignment and performance, with each model demonstrating
preferences for particular caption formats. This comprehensive analysis
provides valuable insights into optimizing captioning strategies, thereby
advancing the pre-training of multimodal foundation models.

摘要：多模態模型的最新進展突顯了改寫字幕在提升效能方面的價值，但仍有重要的挑戰存在。例如，儘管合成字幕通常提供優異的品質和影像文字對齊，但目前尚不清楚它們是否能完全取代 AltText：合成字幕的角色及其與原始網路爬取的 AltText 在預訓練中的互動仍未被充分理解。此外，不同的多模態基礎模型可能對特定的字幕格式有獨特的偏好，但找出每個模型最佳字幕的努力仍然有限。在這項工作中，我們提出一個新穎、可控且可擴充的字幕處理程序，旨在產生針對各種多模態模型量身打造的不同字幕格式。透過檢視簡短合成字幕 (SSC) 到密集合成字幕 (DSC+) 作為案例研究，我們系統性地探討它們對 CLIP、多模態 LLM 和擴散模型等模型中 AltText 的影響和互動。我們的研究結果顯示，保留合成字幕和 AltText 的混合方法可以優於僅使用合成字幕，同時改善對齊和效能，每個模型都展現出對特定字幕格式的偏好。這項全面的分析提供了優化字幕策略的寶貴見解，從而推動多模態基礎模型的預訓練。

##### **Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge**
2410.02736v2 by Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh V Chawla, Xiangliang Zhang

LLM-as-a-Judge has been widely utilized as an evaluation method in various
benchmarks and served as supervised rewards in model training. However, despite
their excellence in many domains, potential issues are under-explored,
undermining their reliability and the scope of their utility. Therefore, we
identify 12 key potential biases and propose a new automated bias
quantification framework-CALM-which systematically quantifies and analyzes each
type of bias in LLM-as-a-Judge by using automated and principle-guided
modification. Our experiments cover multiple popular language models, and the
results indicate that while advanced models have achieved commendable overall
performance, significant biases persist in certain specific tasks. Empirical
results suggest that there remains room for improvement in the reliability of
LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence
of these biases and give some suggestions for the reliable application of
LLM-as-a-Judge. Our work highlights the need for stakeholders to address these
issues and remind users to exercise caution in LLM-as-a-Judge applications.

摘要：LLM-as-a-Judge 已被广泛用作各種基準中的評估方法，並作為模型訓練中的監督獎勵。然而，儘管它們在許多領域表現出色，但潛在的問題卻未被充分探討，這損害了它們的可靠性和實用範圍。因此，我們識別出 12 個關鍵潛在偏差，並提出一個新的自動化偏差量化架構 CALM，它通過使用自動化和原則引導的修改來系統地量化和分析 LLM-as-a-Judge 中的每種類型偏差。我們的實驗涵蓋了多種流行的語言模型，結果表明，儘管先進的模型已經取得了令人稱道的整體表現，但在某些特定任務中仍然存在顯著的偏差。經驗結果表明，LLM-as-a-Judge 的可靠性仍有改進空間。此外，我們還討論了這些偏差的顯性和隱性影響，並對 LLM-as-a-Judge 的可靠應用提出了一些建議。我們的研究強調了利益相關者需要解決這些問題，並提醒用戶在 LLM-as-a-Judge 應用中保持謹慎。

##### **Custom Non-Linear Model Predictive Control for Obstacle Avoidance in Indoor and Outdoor Environments**
2410.02732v1 by Lara Laban, Mariusz Wzorek, Piotr Rudol, Tommy Persson

Navigating complex environments requires Unmanned Aerial Vehicles (UAVs) and
autonomous systems to perform trajectory tracking and obstacle avoidance in
real-time. While many control strategies have effectively utilized linear
approximations, addressing the non-linear dynamics of UAV, especially in
obstacle-dense environments, remains a key challenge that requires further
research. This paper introduces a Non-linear Model Predictive Control (NMPC)
framework for the DJI Matrice 100, addressing these challenges by using a
dynamic model and B-spline interpolation for smooth reference trajectories,
ensuring minimal deviation while respecting safety constraints. The framework
supports various trajectory types and employs a penalty-based cost function for
control accuracy in tight maneuvers. The framework utilizes CasADi for
efficient real-time optimization, enabling the UAV to maintain robust operation
even under tight computational constraints. Simulation and real-world indoor
and outdoor experiments demonstrated the NMPC ability to adapt to disturbances,
resulting in smooth, collision-free navigation.

摘要：複雜環境的導航需要無人機 (UAV) 和自主系統，以執行軌跡追蹤和障礙物迴避
在實時。雖然許多控制策略有效地利用線性近似，但解決無人機的非線性動態，特別是在
障礙物密集的環境中，仍然是一個關鍵挑戰，需要進一步的研究。本文介紹了一種非線性模型預測控制 (NMPC)
DJI Matrice 100 的框架，通過使用動態模型和 B 樣條插值來解決這些挑戰以獲得平滑的參考軌跡，
確保最小偏差，同時遵守安全約束。該框架支持各種軌跡類型，並採用基於懲罰的成本函數來
控制精確度在緊密的機動中。該框架利用 CasADi 進行高效的實時優化，使無人機能夠保持穩健的運行
即使在嚴格的計算約束下。模擬和現實世界室內和室外實驗證明了 NMPC 適應擾動的能力，
導致平穩、無碰撞的導航。

##### **DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects**
2410.02730v1 by Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu

Object navigation in unknown environments is crucial for deploying embodied
agents in real-world applications. While we have witnessed huge progress due to
large-scale scene datasets, faster simulators, and stronger models, previous
studies mainly focus on limited scene types and target objects. In this paper,
we study a new task of navigating to diverse target objects in a large number
of scene types. To benchmark the problem, we present a large-scale scene
dataset, DivScene, which contains 4,614 scenes across 81 different types. With
the dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a
Large Vision Language Model (LVLM) through imitation learning. The LVLM is
trained to take previous observations from the environment and generate the
next actions. We also introduce CoT explanation traces of the action prediction
for better performance when tuning LVLMs. Our extensive experiments find that
we can build a performant LVLM-based agent through imitation learning on the
shortest paths constructed by a BFS planner without any human supervision. Our
agent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we
carry out various analyses showing the generalization ability of our agent.

摘要：在未知環境中進行物體導航對於在真實世界應用中部署具身代理至關重要。儘管我們見證了由於大規模場景數據集、更快的模擬器和更強大的模型而取得的巨大進展，但先前的研究主要集中在有限的場景類型和目標物體上。在本文中，我們研究了一項新任務，即在大量場景類型中導航到不同的目標物體。為了對問題進行基準測試，我們提出了大型場景數據集 DivScene，其中包含 81 種不同類型的 4,614 個場景。利用該數據集，我們通過模仿學習微調大型視覺語言模型 (LVLM) 來構建端到端的具身代理 NatVLM。LVLM 經過訓練，可以從環境中獲取先前的觀察結果並生成後續動作。我們還引入了動作預測的 CoT 解釋軌跡，以便在調整 LVLMs 時獲得更好的性能。我們的廣泛實驗發現，我們可以在沒有任何人工監督的情況下，通過對 BFS 規劃器構造的最短路徑進行模仿學習，構建一個基於 LVLM 的高性能代理。我們的代理實現的成功率比 GPT-4o 高出 20% 以上。同時，我們進行了各種分析，展示了我們代理的泛化能力。

##### **Unified Multi-Modal Interleaved Document Representation for Information Retrieval**
2410.02729v1 by Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, Sung Ju Hwang

Information Retrieval (IR) methods aim to identify relevant documents in
response to a given query, which have gained remarkable attention due to their
successful application in various natural language tasks. However, existing
approaches typically consider only the textual information within the
documents, which overlooks the fact that documents can contain multiple
modalities, including texts, images, and tables. Further, they often segment
each long document into multiple discrete passages for embedding, preventing
them from capturing the overall document context and interactions between
paragraphs. We argue that these two limitations lead to suboptimal document
representations for retrieval. In this work, to address them, we aim to produce
more comprehensive and nuanced document representations by holistically
embedding documents interleaved with different modalities. Specifically, we
achieve this by leveraging the capability of recent vision-language models that
enable the processing and integration of text, images, and tables into a
unified format and representation. Moreover, to mitigate the information loss
from segmenting documents into passages, instead of representing and retrieving
passages individually, we further merge the representations of segmented
passages into one single document representation, while we additionally
introduce a reranking strategy to decouple and identify the relevant passage
within the document if necessary. Then, through extensive experiments on
diverse information retrieval scenarios considering both the textual and
multimodal queries, we show that our approach substantially outperforms
relevant baselines, thanks to the consideration of the multimodal information
interleaved within the documents in a unified way.

摘要：資訊檢索 (IR) 方法旨在找出與特定查詢相關的文件，由於它們在各種自然語言任務中獲得成功應用，因此備受關注。然而，現有方法通常僅考慮文件中的文字資訊，這忽略了文件可能包含多種形式的事實，包括文字、圖片和表格。此外，它們通常將每個長文件分割成多個離散段落進行嵌入，無法擷取整體文件脈絡和段落之間的互動。我們認為這兩個限制導致次佳文件表示無法進行檢索。在這項工作中，為了解決這些問題，我們旨在透過整體嵌入穿插不同形式的文件來產生更全面且細緻的文件表示。具體來說，我們透過利用近期視覺語言模型的能力來達成此目標，這些模型能夠處理和整合文字、圖片和表格，形成統一的格式和表示。此外，為了減輕將文件分割成段落造成的資訊遺失，我們進一步將分割段落的表示合併成單一文件表示，而不是個別表示和檢索段落，同時我們還引入重新排序策略，在必要時解耦和找出文件中的相關段落。然後，透過在考量文字和多模態查詢的各種資訊檢索情境中進行廣泛實驗，我們證明了我們的方法大幅優於相關基準，這歸功於統一考量文件中的多模態資訊。

##### **Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation**
2410.02725v1 by Rohin Manvi, Anikait Singh, Stefano Ermon

Inference-time computation is a powerful paradigm to enhance the performance
of large language models (LLMs), with Best-of-N sampling being a widely used
technique. However, this method is computationally expensive, requiring both
(1) an external reward model and (2) the generation of multiple samples. In
this work, we introduce a new generative self-evaluation scheme designed to
adaptively reduce the number of generated samples while maintaining or even
improving performance. We use a generative reward model formulation, allowing
the LLM to predict mid-generation the probability that restarting the
generation will yield a better response. These predictions are obtained without
an external reward model and can be used to decide whether or not to generate
more samples, prune unpromising samples early on, or to pick the best sample.
This capability is very inexpensive as it involves generating a single
predefined token. Trained using a dataset constructed with real unfiltered
LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval
increases from 21% to 34% with 16 samples and math performance on GSM8K
improves from 84% to 91%. By sampling only when the LLM determines that it is
beneficial to do so and adaptively adjusting temperature annealing, we
demonstrate that 74% of the improvement from using 16 samples can be achieved
with only 1.2 samples on average. We further demonstrate that 50-75% of samples
can be pruned early in generation with minimal degradation in performance.
Overall, our methods enable more efficient and scalable compute utilization
during inference for LLMs.

摘要：推理時間計算是一種強大的範例，可以增強大型語言模型 (LLM) 的效能，其中最佳 N 取樣是一種廣泛使用的技術。然而，這種方法在計算上很昂貴，需要 (1) 外部獎勵模型和 (2) 產生多個樣本。在這項工作中，我們引入了一種新的生成式自我評估方案，旨在自適應地減少生成的樣本數量，同時維持甚至改善效能。我們使用生成式獎勵模型公式，允許 LLM 在生成過程中預測重新啟動生成將產生更好回應的機率。這些預測是在沒有外部獎勵模型的情況下獲得的，可用於決定是否產生更多樣本、及早移除不佳的樣本，或選擇最佳樣本。這種能力非常便宜，因為它涉及生成單一的預定義權杖。使用由真實未過濾的 LMSYS 使用者提示所建構的資料集進行訓練，Llama 3.1 8B 在 AlpacaEval 上對抗 GPT-4 的獲勝率從 21% 增加到 34%，且在 GSM8K 上的數學效能從 84% 提升到 91%。透過僅在 LLM 確定這樣做有利時進行取樣，並自適應地調整溫度退火，我們證明了使用 16 個樣本中 74% 的改進，平均只需 1.2 個樣本即可實現。我們進一步證明，50-75% 的樣本可以在生成早期進行移除，而效能下降幅度很小。總的來說，我們的技術可以在 LLM 的推理過程中實現更有效率且可擴充的運算使用。

##### **Large Language Models as Markov Chains**
2410.02724v1 by Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boullé, Ievgen Redko

Large language models (LLMs) have proven to be remarkably efficient, both
across a wide range of natural language processing tasks and well beyond them.
However, a comprehensive theoretical analysis of the origins of their
impressive performance remains elusive. In this paper, we approach this
challenging task by drawing an equivalence between generic autoregressive
language models with vocabulary of size $T$ and context window of size $K$ and
Markov chains defined on a finite state space of size $\mathcal{O}(T^K)$. We
derive several surprising findings related to the existence of a stationary
distribution of Markov chains that capture the inference power of LLMs, their
speed of convergence to it, and the influence of the temperature on the latter.
We then prove pre-training and in-context generalization bounds and show how
the drawn equivalence allows us to enrich their interpretation. Finally, we
illustrate our theoretical guarantees with experiments on several recent LLMs
to highlight how they capture the behavior observed in practice.

摘要：大型語言模型 (LLM) 已被證明非常有效率，無論是在廣泛的自然語言處理任務中，還是遠遠超出這些任務。然而，對其令人印象深刻的效能來源進行全面的理論分析仍然難以捉摸。在本文中，我們通過將具有大小為 $T$ 的詞彙和大小為 $K$ 的上下文窗口的通用自迴歸語言模型與定義在大小為 $\mathcal{O}(T^K)$ 的有限狀態空間上的馬可夫鏈等同起來，來應對這項艱鉅的任務。我們得出了幾個令人驚訝的發現，這些發現與捕獲 LLM 推論能力的馬可夫鏈的平穩分佈的存在、它們收斂到它的速度以及溫度對後者的影響有關。然後，我們證明了預訓練和情境內概括界限，並展示了所畫的等價關係如何讓我們豐富它們的解釋。最後，我們用最近幾個 LLM 的實驗來說明我們的理論保證，以強調它們如何捕捉到在實務中觀察到的行為。

##### **Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization**
2410.02721v1 by Ryan C. Barron, Ves Grantcharov, Selma Wanna, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim Ø. Rasmussen, Cynthia Matuszek, Boian S. Alexandrov

Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.

摘要：大型語言模型 (LLM) 經過大量語料庫的預先訓練，在許多一般自然語言處理 (NLP) 任務中表現出色，例如問題解答 (QA)。儘管它們具有先進的語言能力，但 LLM 在特定領域和知識密集型任務方面會出現幻覺、知識斷層和缺乏知識歸因。此外，微調 LLM 的內在知識以適應高度特定領域是一個昂貴且耗時的過程。檢索增強生成 (RAG) 流程最近已成為一種優化 LLM 回應的方法，方法是將它們參照預先確定的本体。研究表明，將知識圖譜 (KG) 本体用於 RAG 可透過考慮以結構化方式保留資訊的相關子圖，來提高 QA 的準確性。在本文中，我們介紹了 SMART-SLIC，這是一個高度特定領域的 LLM 框架，它將 RAG 與 KG 和一個儲存事實特定領域資訊的向量儲存 (VS) 整合在一起。重要的是，為了避免 KG 中的幻覺，我們在不使用 LLM 的情況下建立了這些高度特定領域的 KG 和 VS，而是透過 NLP、資料探勘和具有自動模型選擇的非負張量分解。將我們的 RAG 與特定領域配對：(i) KG（包含結構化資訊），和 (ii) VS（包含非結構化資訊）能夠開發特定領域的聊天機器人，這些聊天機器人會歸因於資訊來源、減輕幻覺、減少微調的需要，並在高度特定領域的問題解答任務中表現出色。我們將 SMART-SLIC 與思考鏈提示代理配對。該框架被設計成可概括以適應任何特定或專業領域。在本文中，我們在惡意軟體分析和異常偵測的科學出版物語料庫上展示了我們框架的問題解答能力。

##### **Curvature Diversity-Driven Deformation and Domain Alignment for Point Cloud**
2410.02720v1 by Mengxi Wu, Hao Huang, Yi Fang, Mohammad Rostami

Unsupervised Domain Adaptation (UDA) is crucial for reducing the need for
extensive manual data annotation when training deep networks on point cloud
data. A significant challenge of UDA lies in effectively bridging the domain
gap. To tackle this challenge, we propose \textbf{C}urvature
\textbf{D}iversity-Driven \textbf{N}uclear-Norm Wasserstein \textbf{D}omain
Alignment (CDND). Our approach first introduces a \textit{\textbf{Curv}ature
Diversity-driven Deformation \textbf{Rec}onstruction (CurvRec)} task, which
effectively mitigates the gap between the source and target domains by enabling
the model to extract salient features from semantically rich regions of a given
point cloud. We then propose \textit{\textbf{D}eformation-based
\textbf{N}uclear-norm \textbf{W}asserstein \textbf{D}iscrepancy (D-NWD)}, which
applies the Nuclear-norm Wasserstein Discrepancy to both \textit{deformed and
original} data samples to align the source and target domains. Furthermore, we
contribute a theoretical justification for the effectiveness of D-NWD in
distribution alignment and demonstrate that it is \textit{generic} enough to be
applied to \textbf{any} deformations. To validate our method, we conduct
extensive experiments on two public domain adaptation datasets for point cloud
classification and segmentation tasks. Empirical experiment results show that
our CDND achieves state-of-the-art performance by a noticeable margin over
existing approaches.

摘要：無監督域適應 (UDA) 在訓練點雲數據上的深度網路時，對於減少廣泛手動數據標註的需求至關重要。UDA 的一大挑戰在於有效地彌合域差距。為了應對這一挑戰，我們提出了**C**urvature **D**iversity-Driven **N**uclear-Norm Wasserstein **D**omain Alignment (CDND)。我們的做法首先引入了一個**Curv**ature Diversity-driven Deformation **Rec**onstruction (CurvRec) 任務，它通過使模型能夠從給定點雲的語義豐富區域中提取顯著特徵，有效地減輕了源域和目標域之間的差距。然後我們提出了**D**eformation-based **N**uclear-norm **W**asserstein **D**iscrepancy (D-NWD)，它將核範 Wasserstein 差異應用於**變形和原始**數據樣本，以對齊源域和目標域。此外，我們為 D-NWD 在分佈對齊中的有效性提供了理論依據，並證明它足夠**通用**，可以應用於**任何**變形。為了驗證我們的模型，我們對點雲分類和分割任務的兩個公共域適應數據集進行了廣泛的實驗。實證實驗結果表明，我們的 CDND 以明顯的優勢優於現有方法，達到了最先進的性能。

##### **UncertaintyRAG: Span-Level Uncertainty Enhanced Long-Context Modeling for Retrieval-Augmented Generation**
2410.02719v1 by Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, Zhongwei Wan, Xiaodan Liang, Chengming Li, Zhenan Sun, Lingpeng Kong, Ngai Wong

We present UncertaintyRAG, a novel approach for long-context
Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio
(SNR)-based span uncertainty to estimate similarity between text chunks. This
span uncertainty enhances model calibration, improving robustness and
mitigating semantic inconsistencies introduced by random chunking. Leveraging
this insight, we propose an efficient unsupervised learning technique to train
the retrieval model, alongside an effective data sampling and scaling strategy.
UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving
state-of-the-art results while using only 4% of the training data compared to
other advanced open-source retrieval models under distribution shift settings.
Our method demonstrates strong calibration through span uncertainty, leading to
improved generalization and robustness in long-context RAG tasks. Additionally,
UncertaintyRAG provides a lightweight retrieval model that can be integrated
into any large language model with varying context window lengths, without the
need for fine-tuning, showcasing the flexibility of our approach.

摘要：我們提出 UncertaintyRAG，一種針對長語境檢索增強生成 (RAG) 的新穎方法，它利用基於訊號雜訊比 (SNR) 的區間不確定性來估計文字區塊之間的相似性。此區間不確定性增強了模型校準，進而提升了穩健性並減輕了隨機區塊化所造成的語義不一致性。利用此見解，我們提出了一種有效的無監督學習技術來訓練檢索模型，並搭配有效的資料抽樣和擴充策略。UncertaintyRAG 在 LLaMA-2-7B 上優於基準 2.03%，在分佈轉移設定下僅使用 4% 的訓練資料便達成最先進的結果，與其他進階的開源檢索模型相比。我們的模型透過區間不確定性展現出強大的校準，進而提升長語境 RAG 任務中的概化能力和穩健性。此外，UncertaintyRAG 提供了一個輕量級的檢索模型，可以整合到任何具有不同語境視窗長度的大語言模型中，而無需微調，展現了我們方法的靈活性。

##### **Video Instruction Tuning With Synthetic Data**
2410.02713v2 by Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, Chunyuan Li

The development of video large multimodal models (LMMs) has been hindered by
the difficulty of curating large amounts of high-quality raw data from the web.
To address this, we propose an alternative approach by creating a high-quality
synthetic dataset specifically for video instruction-following, namely
LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,
open-ended question-answering (QA), and multiple-choice QA. By training on this
dataset, in combination with existing visual instruction tuning data, we
introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that
LLaVA-Video achieves strong performance across various video benchmarks,
highlighting the effectiveness of our dataset. We plan to release the dataset,
its generation pipeline, and the model checkpoints.

摘要：影片大型多模态模型 (LMM) 的发展受到从网络中整理大量高质量原始数据的困难所阻碍。为了解决这个问题，我们提出一种替代方法，即创建高质量的合成数据集，专门用于影片指令遵循，即 LLaVA-Video-178K。此数据集包括关键任务，例如详细字幕、开放式问答 (QA) 和多项选择 QA。通过在此数据集上进行训练，结合现有的视觉指令调整数据，我们介绍了 LLaVA-Video，一种新的影片 LMM。我们的实验表明，LLaVA-Video 在各种影片基准测试中都取得了强劲的性能，突显了我们数据集的有效性。我们计划发布数据集、其生成管道和模型检查点。

##### **LLaVA-Critic: Learning to Evaluate Multimodal Models**
2410.02712v1 by Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, Chunyuan Li

We introduce LLaVA-Critic, the first open-source large multimodal model (LMM)
designed as a generalist evaluator to assess performance across a wide range of
multimodal tasks. LLaVA-Critic is trained using a high-quality critic
instruction-following dataset that incorporates diverse evaluation criteria and
scenarios. Our experiments demonstrate the model's effectiveness in two key
areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation
scores, performing on par with or surpassing GPT models on multiple evaluation
benchmarks; and (2) Preference Learning, where it generates reward signals for
preference learning, enhancing model alignment capabilities. This work
underscores the potential of open-source LMMs in self-critique and evaluation,
setting the stage for future research into scalable, superhuman alignment
feedback mechanisms for LMMs.

摘要：我們引入了 LLaVA-Critic，第一個開放原始碼的大型多模態模型 (LMM)，
設計為一個通才評估器，用於評估廣泛多模態任務的效能。LLaVA-Critic 使用高品質的批評者
指令遵循資料集進行訓練，該資料集包含多樣化的評估標準和
情境。我們的實驗證明了該模型在兩個關鍵領域的效能：(1) LMM 作為評審，LLaVA-Critic 提供可靠的評估
分數，在多個評估基準上與 GPT 模型表現相當或超越；以及 (2) 偏好學習，它為
偏好學習產生獎勵訊號，增強模型對齊能力。這項工作
強調了開放原始碼 LMM 在自我批評和評估中的潛力，為未來研究可擴充、超人類對齊設定了舞台
LMM 的回饋機制。

##### **SteerDiff: Steering towards Safe Text-to-Image Diffusion Models**
2410.02710v1 by Hongxiang Zhang, Yifeng He, Hao Chen

Text-to-image (T2I) diffusion models have drawn attention for their ability
to generate high-quality images with precise text alignment. However, these
models can also be misused to produce inappropriate content. Existing safety
measures, which typically rely on text classifiers or ControlNet-like
approaches, are often insufficient. Traditional text classifiers rely on
large-scale labeled datasets and can be easily bypassed by rephrasing. As
diffusion models continue to scale, fine-tuning these safeguards becomes
increasingly challenging and lacks flexibility. Recent red-teaming attack
researches further underscore the need for a new paradigm to prevent the
generation of inappropriate content. In this paper, we introduce SteerDiff, a
lightweight adaptor module designed to act as an intermediary between user
input and the diffusion model, ensuring that generated images adhere to ethical
and safety standards with little to no impact on usability. SteerDiff
identifies and manipulates inappropriate concepts within the text embedding
space to guide the model away from harmful outputs. We conduct extensive
experiments across various concept unlearning tasks to evaluate the
effectiveness of our approach. Furthermore, we benchmark SteerDiff against
multiple red-teaming strategies to assess its robustness. Finally, we explore
the potential of SteerDiff for concept forgetting tasks, demonstrating its
versatility in text-conditioned image generation.

摘要：文本到图像 (T2I) 扩散模型因其生成具有精确文本对齐的高质量图像的能力而备受关注。然而，这些模型也可能被滥用以产生不当内容。现有的安全措施通常依赖于文本分类器或 ControlNet 等方法，往往不足以应对。传统的文本分类器依赖于大规模标记数据集，并且可以通过改写轻松绕过。随着扩散模型的不断扩展，对这些保障措施进行微调变得越来越具有挑战性，并且缺乏灵活性。最近的红队攻击研究进一步强调了需要一种新的范例来防止生成不当内容。在本文中，我们介绍了 SteerDiff，这是一个轻量级的适配器模块，旨在充当用户输入和扩散模型之间的中介，确保生成的图像符合道德和安全标准，同时几乎不会影响可用性。SteerDiff 在文本嵌入空间中识别和操作不当概念，以引导模型远离有害输出。我们对各种概念取消学习任务进行了广泛的实验，以评估我们方法的有效性。此外，我们对 SteerDiff 进行了基准测试，以针对多个红队策略评估其稳健性。最后，我们探讨了 SteerDiff 在概念遗忘任务中的潜力，展示了它在文本条件图像生成中的多功能性。

##### **LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations**
2410.02707v1 by Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov

Large language models (LLMs) often produce errors, including factual
inaccuracies, biases, and reasoning failures, collectively referred to as
"hallucinations". Recent studies have demonstrated that LLMs' internal states
encode information regarding the truthfulness of their outputs, and that this
information can be utilized to detect errors. In this work, we show that the
internal representations of LLMs encode much more information about
truthfulness than previously recognized. We first discover that the
truthfulness information is concentrated in specific tokens, and leveraging
this property significantly enhances error detection performance. Yet, we show
that such error detectors fail to generalize across datasets, implying that --
contrary to prior claims -- truthfulness encoding is not universal but rather
multifaceted. Next, we show that internal representations can also be used for
predicting the types of errors the model is likely to make, facilitating the
development of tailored mitigation strategies. Lastly, we reveal a discrepancy
between LLMs' internal encoding and external behavior: they may encode the
correct answer, yet consistently generate an incorrect one. Taken together,
these insights deepen our understanding of LLM errors from the model's internal
perspective, which can guide future research on enhancing error analysis and
mitigation.

摘要：大型語言模型 (LLM) 經常會產生錯誤，包括事實不正確、偏見和推理失敗，這些錯誤統稱為「幻覺」。最近的研究表明，LLM 的內部狀態會編碼有關其輸出真實性的資訊，而且這些資訊可用於偵測錯誤。在這項工作中，我們展示了 LLM 的內部表示編碼了比先前認知還要多得多的真實性資訊。我們首先發現真實性資訊集中在特定代碼中，而且利用此特性可以大幅提升錯誤偵測效能。然而，我們發現此類錯誤偵測器無法在資料集之間廣泛應用，這表示與先前的說法相反，真實性編碼並非普遍存在，而是多面向的。接下來，我們展示了內部表示也可以用於預測模型可能產生的錯誤類型，有助於制定客製化的緩解策略。最後，我們揭露了 LLM 內部編碼與外部行為之間的差異：它們可能會編碼正確答案，但卻持續產生不正確的答案。綜合而言，這些見解加深了我們從模型內部觀點了解 LLM 錯誤的方式，這有助於引導未來增強錯誤分析和緩解措施的研究。

##### **Selective Attention Improves Transformer**
2410.02703v1 by Yaniv Leviathan, Matan Kalman, Yossi Matias

Unneeded elements in the attention's context degrade performance. We
introduce Selective Attention, a simple parameter-free change to the standard
attention mechanism which reduces attention to unneeded elements. Selective
attention improves language modeling performance in a variety of model sizes
and context lengths. For example, a range of transformers trained with the
language modeling objective on C4 with selective attention perform equivalently
to standard transformers with ~2X more heads and parameters in their attention
modules. Selective attention also allows decreasing the size of the attention's
context buffer, leading to meaningful reductions in the memory and compute
requirements during inference. For example, transformers with 100M parameters
trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and
47X less memory for their attention module, respectively, when equipped with
selective attention, as those without selective attention, with the same
validation perplexity.

摘要：注意力脈絡中的非必要元素會降低效能。我們引進選擇性注意力，這是標準注意力機制的簡單無參數變更，可減少對非必要元素的注意力。選擇性注意力可提升語言模型效能，適用於各種模型大小和脈絡長度。例如，使用選擇性注意力訓練的各種Transformer在 C4 上以語言模型目標訓練，其效能等同於標準Transformer，而後者的注意力模組具有約 2 倍的注意力頭和參數。選擇性注意力還能縮小注意力脈絡緩衝區的大小，進而大幅減少推論期間的記憶體和運算需求。例如，使用選擇性注意力訓練的 100M 參數Transformer在 C4 上的脈絡大小為 512、1,024 和 2,048，其注意力模組分別比沒有選擇性注意力的Transformer減少 16 倍、25 倍和 47 倍的記憶體，且驗證困惑度相同。

##### **HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly**
2410.02694v1 by Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izasak, Moshe Wasserblat, Danqi Chen

There have been many benchmarks for evaluating long-context language models
(LCLMs), but developers often rely on synthetic tasks like needle-in-a-haystack
(NIAH) or arbitrary subsets of tasks. It remains unclear whether they translate
to the diverse downstream applications of LCLMs, and the inconsistency further
complicates model comparison. We investigate the underlying reasons behind
current practices and find that existing benchmarks often provide noisy signals
due to low coverage of applications, insufficient lengths, unreliable metrics,
and incompatibility with base models. In this work, we present HELMET (How to
Evaluate Long-context Models Effectively and Thoroughly), a comprehensive
benchmark encompassing seven diverse, application-centric categories. We also
address many issues in previous benchmarks by adding controllable lengths up to
128k tokens, model-based evaluation for reliable metrics, and few-shot
prompting for robustly evaluating base models. Consequently, we demonstrate
that HELMET offers more reliable and consistent rankings of frontier LCLMs.
Through a comprehensive study of 51 LCLMs, we find that (1) synthetic tasks
like NIAH are not good predictors of downstream performance; (2) the diverse
categories in HELMET exhibit distinct trends and low correlation with each
other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models
significantly lag behind closed ones when the task requires full-context
reasoning or following complex instructions -- the gap widens with increased
lengths. Finally, we recommend using our RAG tasks for fast model development,
as they are easy to run and more predictive of other downstream performance;
ultimately, we advocate for a holistic evaluation across diverse tasks.

摘要：<paragraph>已經有許多用於評估長語境語言模型 (LCLM) 的基準，但開發人員經常依賴於合成任務，例如大海撈針 (NIAH) 或任務的任意子集。目前尚不清楚它們是否轉換為 LCLM 的各種下游應用，而這種不一致性進一步複雜化了模型比較。我們探討了當前做法背後的基本原因，並發現現有的基準經常由於應用覆蓋率低、長度不足、指標不可靠以及與基礎模型不相容而提供雜訊訊號。在這項工作中，我們提出了 HELMET（如何有效且徹底地評估長語境模型），這是一個包含七個不同的以應用為中心的類別的綜合基準。我們還通過增加長度可控性（最多 128k 個符號）、基於模型的評估以獲得可靠的指標，以及少量提示以穩健地評估基礎模型來解決先前基準中的許多問題。因此，我們證明 HELMET 為前沿 LCLM 提供了更可靠且一致的排名。通過對 51 個 LCLM 的全面研究，我們發現 (1) NIAH 等合成任務並不能很好地預測下游性能；(2) HELMET 中不同的類別表現出不同的趨勢，並且彼此之間的相關性很低；以及 (3) 儘管大多數 LCLM 都達到了完美的 NIAH 分數，但當任務需要全語境推理或遵循複雜指令時，開源模型顯著落後於封閉模型 - 差距隨著長度的增加而擴大。最後，我們建議使用我們的 RAG 任務進行快速模型開發，因為它們易於執行且更能預測其他下游性能；最終，我們提倡對各種任務進行整體評估。</paragraph>

##### **Discovering Clues of Spoofed LM Watermarks**
2410.02693v1 by Thibaud Gloaguen, Nikola Jovanović, Robin Staab, Martin Vechev

LLM watermarks stand out as a promising way to attribute ownership of
LLM-generated text. One threat to watermark credibility comes from spoofing
attacks, where an unauthorized third party forges the watermark, enabling it to
falsely attribute arbitrary texts to a particular LLM. While recent works have
demonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,
they lack deeper qualitative analysis of the texts produced by spoofing
methods. In this work, we for the first time reveal that there are observable
differences between genuine and spoofed watermark texts. Namely, we show that
regardless of their underlying approach, all current spoofing methods
consistently leave observable artifacts in spoofed texts, indicative of
watermark forgery. We build upon these findings to propose rigorous statistical
tests that reliably reveal the presence of such artifacts, effectively
discovering that a watermark was spoofed. Our experimental evaluation shows
high test power across all current spoofing methods, providing insights into
their fundamental limitations, and suggesting a way to mitigate this threat.

摘要：LLM 水印作為一種有前景的方式，用於標示 LLM 生成的文字所有權。水印的可信度會受到欺騙攻擊的威脅，未經授權的第三方會偽造水印，使其能將任意文字虛假標示為特定 LLM 所生成。儘管最近的研究顯示，最先進的方案實際上容易受到欺騙，但它們缺乏對欺騙方法所產生文字的更深入定性分析。在這項研究中，我們首次揭露真實水印文字和欺騙水印文字之間存在可觀察的差異。具體來說，我們表明，無論其基本方法為何，所有當前的欺騙方法都會一貫地在欺騙文字中留下可觀察的人工製品，表明水印是偽造的。我們根據這些發現提出嚴謹的統計檢定，可靠地揭露此類人工製品的存在，有效地發現水印是否遭到欺騙。我們的實驗評估顯示，在所有當前欺騙方法中，檢定力都非常高，洞察其基本限制，並提出減輕此威脅的方法。

##### **On the Proper Treatment of Tokenization in Psycholinguistics**
2410.02691v1 by Mario Giulianelli, Luca Malagutti, Juan Luis Gastaldi, Brian DuSell, Tim Vieira, Ryan Cotterell

Language models are widely used in computational psycholinguistics to test
theories that relate the negative log probability (the surprisal) of a region
of interest (a substring of characters) under a language model to its cognitive
cost experienced by readers, as operationalized, for example, by gaze duration
on the region. However, the application of modern language models to
psycholinguistic studies is complicated by the practice of using tokenization
as an intermediate step in training a model. Doing so results in a language
model over token strings rather than one over character strings. Vexingly,
regions of interest are generally misaligned with these token strings. The
paper argues that token-level language models should be (approximately)
marginalized into character-level language models before they are used in
psycholinguistic studies to compute the surprisal of a region of interest;
then, the marginalized character-level language model can be used to compute
the surprisal of an arbitrary character substring, which we term a focal area,
that the experimenter may wish to use as a predictor. Our proposal of
marginalizing a token-level model into a character-level one solves this
misalignment issue independently of the tokenization scheme. Empirically, we
discover various focal areas whose surprisal is a better psychometric predictor
than the surprisal of the region of interest itself.

摘要：語言模型廣泛用於計算心理語言學中，用來測試理論，將語言模型中感興趣區域（字元子串）的負對數機率（驚訝度）與讀者所經歷的認知成本相關聯，例如，透過注視區域的持續時間來操作。然而，將現代語言模型應用於心理語言學研究，會因為在訓練模型時使用標記化作為中間步驟的實務而變得複雜。這麼做會產生一個針對標記字串的語言模型，而不是針對字元字串的語言模型。令人困擾的是，感興趣區域通常與這些標記字串不對齊。本文主張，在心理語言學研究中使用標記層級語言模型來計算感興趣區域的驚訝度之前，應該將其（近似地）邊緣化為字元層級語言模型；然後，可以將邊緣化的字元層級語言模型用於計算任意字元子串的驚訝度，我們稱之為焦點區域，實驗者可能希望將其用作預測因子。我們將標記層級模型邊緣化為字元層級模型的提議，獨立於標記化方案解決了這個不對齊問題。憑經驗，我們發現各種焦點區域，其驚訝度比感興趣區域本身的驚訝度更佳的心理測量預測因子。

##### **User-centric Immersive Communications in 6G: A Data-oriented Approach via Digital Twin**
2410.02688v1 by Conghao Zhou, Shisheng Hu, Jie Gao, Xinyu Huang, Weihua Zhuang, Xuemin Shen

In this article, we present a novel user-centric service provision for
immersive communications (IC) in 6G to deal with the uncertainty of individual
user behaviors while satisfying unique requirements on the quality of
multi-sensory experience. To this end, we propose a data-oriented approach for
network resource management, featuring personalized data management that can
support network modeling tailored to different user demands. Our approach
leverages the digital twin (DT) technique as a key enabler. Particularly, a DT
is established for each user, and the data attributes in the DT are customized
based on the characteristics of the user. The DT functions, corresponding to
various data operations, are customized in the development, evaluation, and
update of network models to meet unique user demands. A trace-driven case study
demonstrates the effectiveness of our approach in achieving user-centric IC and
the significance of personalized data management in 6G.

摘要：在本文中，我們提出了一種創新的以使用者為中心的服務提供方式，用於 6G 中的身歷其境通訊 (IC)，以應對個人使用者行為的不確定性，同時滿足多感官體驗品質的獨特需求。為此，我們提出了一種以資料為導向的網路資源管理方法，其特點是個人化資料管理，可以支援根據不同使用者需求量身打造的網路建模。我們的做法以數位雙胞胎 (DT) 技術為關鍵推手。特別是，為每個使用者建立一個 DT，並且根據使用者的特徵自訂 DT 中的資料屬性。DT 功能，對應於各種資料操作，在開發、評估和更新網路模型時進行自訂，以滿足獨特的使用者需求。一個追蹤驅動的案例研究證明了我們的方法在實現以使用者為中心的 IC 和 6G 中個人化資料管理的重要性方面的有效性。

##### **HiddenGuard: Fine-Grained Safe Generation with Specialized Representation Router**
2410.02684v1 by Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, Xueqi Cheng

As Large Language Models (LLMs) grow increasingly powerful, ensuring their
safety and alignment with human values remains a critical challenge. Ideally,
LLMs should provide informative responses while avoiding the disclosure of
harmful or sensitive information. However, current alignment approaches, which
rely heavily on refusal strategies, such as training models to completely
reject harmful prompts or applying coarse filters are limited by their binary
nature. These methods either fully deny access to information or grant it
without sufficient nuance, leading to overly cautious responses or failures to
detect subtle harmful content. For example, LLMs may refuse to provide basic,
public information about medication due to misuse concerns. Moreover, these
refusal-based methods struggle to handle mixed-content scenarios and lack the
ability to adapt to context-dependent sensitivities, which can result in
over-censorship of benign content. To overcome these challenges, we introduce
HiddenGuard, a novel framework for fine-grained, safe generation in LLMs.
HiddenGuard incorporates Prism (rePresentation Router for In-Stream
Moderation), which operates alongside the LLM to enable real-time, token-level
detection and redaction of harmful content by leveraging intermediate hidden
states. This fine-grained approach allows for more nuanced, context-aware
moderation, enabling the model to generate informative responses while
selectively redacting or replacing sensitive information, rather than outright
refusal. We also contribute a comprehensive dataset with token-level
fine-grained annotations of potentially harmful information across diverse
contexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1
score for detecting and redacting harmful content while preserving the overall
utility and informativeness of the model's responses.

摘要：隨著大型語言模型 (LLM) 變得越來越強大，確保其安全性和與人類價值觀的一致性仍然是一項嚴峻的挑戰。理想情況下，LLM 應提供有意義的回應，同時避免揭露有害或敏感資訊。然而，當前的一致性方法過於依賴拒絕策略，例如訓練模型完全拒絕有害提示或套用粗略的過濾器，而受到其二進位制的限制。這些方法不是完全拒絕存取資訊，就是沒有足夠的細微差別就授予存取，導致過於謹慎的回應或無法偵測到微妙的有害內容。例如，LLM 可能拒絕提供有關藥物的基本公開資訊，因為擔心會被濫用。此外，這些基於拒絕的方法難以處理混合內容的情境，並且缺乏適應依賴於脈絡的敏感性的能力，這可能導致過度審查良性的內容。為了克服這些挑戰，我們引入了 HiddenGuard，一個適用於 LLM 的創新且細緻且安全的生成架構。HiddenGuard 結合了 Prism（串流內審核的表示路由器），與 LLM 並行運作，透過利用中間的隱藏狀態，實現即時的、標記層級的偵測和刪除有害內容。這種細緻的方法允許更細緻、有脈絡意識的審核，使模型能夠產生有意義的回應，同時有選擇地刪除或替換敏感資訊，而不是直接拒絕。我們還貢獻了一個全面的資料集，其中包含在不同脈絡中，標記層級的細緻註解，說明潛在有害的資訊。我們的實驗表明，HiddenGuard 在 F1 得分上達到了 90% 以上，用於偵測和刪除有害內容，同時保留模型回應的整體效用和資訊性。

##### **DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life**
2410.02683v1 by Yu Ying Chiu, Liwei Jiang, Yejin Choi

As we increasingly seek guidance from LLMs for decision-making in daily life,
many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of the users. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
includes two possible actions and with each action, the affected parties and
human values invoked. Based on these dilemmas, we consolidated a set of human
values across everyday topics e.g., interpersonal relationships, workplace, and
environmental issues. We evaluated LLMs on these dilemmas to determine what
action they will take and the values represented by these actions. Then, we
analyzed these values through the lens of five popular theories inspired by
sociology, psychology and philosophy. These theories are: World Value Survey,
Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and
Plutchik Wheel of Emotion. We find that LLMs are most aligned with the
self-expression over survival values in terms of World Value Survey, care over
loyalty in Moral Foundation Theory. Interestingly, we find large preferences
differences in models for some core values such as truthfulness e.g.,
Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to
select it by 9.4%. We also study the recent guidance released by OpenAI
(ModelSpec), and Anthropic (Constitutional AI) to understand how their released
principles reflect their actual value prioritization when facing nuanced moral
reasoning in daily-life settings. We find that end users cannot effectively
steer such prioritization using system prompts.

摘要：<paragraph>隨著我們在日常生活中越來越依賴 LLM 來進行決策，
其中許多決策並非明確，而且在很大程度上取決於
使用者的個人價值觀和道德標準。我們提出了日常困境，一個
在日常生活中遇到的 1,360 個道德困境的數據集。每個困境
包含兩個可能的行動，並針對每個行動，受到影響的各方和
引發的人類價值觀。根據這些困境，我們彙整了一組人類
價值觀，涵蓋日常主題，例如人際關係、職場和
環境問題。我們針對這些困境評估 LLM，以確定它們將採取什麼
行動以及這些行動所代表的價值觀。接著，我們
透過社會學、心理學和哲學啟發的五種流行理論來分析這些價值觀。這些理論為：世界價值觀調查、
道德基礎理論、馬斯洛需求層次理論、亞里斯多德美德，以及
普拉契克情緒之輪。我們發現 LLM 在世界價值觀調查中與
生存價值觀相比，更傾向於自我表達；在道德基礎理論中，更傾向於關懷而非忠誠。有趣的是，我們發現對於某些核心價值觀，例如誠實，不同模型之間存在很大的偏好差異，例如，
Mixtral-8x7B 模型傾向於忽略它 9.7%，而 GPT-4-turbo 模型傾向於選擇它 9.4%。我們還研究了 OpenAI
(ModelSpec) 和 Anthropic (Constitutional AI) 最近發布的指導方針，以了解他們發布的原則如何在面對日常生活中微妙的道德推理時反映他們的實際價值優先順序。我們發現，最終使用者無法有效地
使用系統提示來引導此類優先順序。</paragraph>

##### **Distilling an End-to-End Voice Assistant Without Instruction Training Data**
2410.02678v1 by William Held, Ella Li, Michael Ryan, Weiyan Shi, Yanzhe Zhang, Diyi Yang

Voice assistants, such as Siri and Google Assistant, typically model audio
and text separately, resulting in lost speech information and increased
complexity. Recent efforts to address this with end-to-end Speech Large
Language Models (LLMs) trained with supervised finetuning (SFT)
  have led to models ``forgetting" capabilities from text-only LLMs. Our work
proposes an alternative paradigm for training Speech LLMs without instruction
data, using the response of a text-only LLM to transcripts as self-supervision.
Importantly, this process can be performed without annotated responses. We show
that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question
Answering, Classification, and Translation. Furthermore, we show that DiVA
better meets user preferences, achieving a 72\% win rate compared with
state-of-the-art models like Qwen 2 Audio, despite using $>$100x less training
compute.

摘要：語音助理，例如 Siri 和 Google Assistant，通常會分別建模音訊和文字，導致語音資訊遺失和複雜性增加。最近為了解決這個問題，使用監督微調 (SFT) 訓練的端對端語音大型語言模型 (LLM) 導致模型「忘記」純文字 LLM 的功能。我們的研究提出了一種替代範例，用於在沒有指令資料的情況下訓練語音 LLM，使用純文字 LLM 對謄本的回應作為自我監督。重要的是，這個過程可以在沒有註解回應的情況下執行。我們展示了我們的 Distilled Voice Assistant (DiVA) 可以概括為口說問題解答、分類和翻譯。此外，我們展示了 DiVA 更能滿足使用者的偏好，與 Qwen 2 Audio 等最先進的模型相比，達到了 72% 的獲勝率，儘管訓練運算量減少了 >100 倍。

##### **CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of LLMs**
2410.02677v1 by Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi

To make large language models (LLMs) more helpful across diverse cultures, it
is essential to have effective cultural knowledge benchmarks to measure and
track our progress. Effective benchmarks need to be robust, diverse, and
challenging. We introduce CulturalBench: a set of 1,227 human-written and
human-verified questions for effectively assessing LLMs' cultural knowledge,
covering 45 global regions including the underrepresented ones like Bangladesh,
Zimbabwe, and Peru. Questions - each verified by five independent annotators -
span 17 diverse topics ranging from food preferences to greeting etiquettes. We
evaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which
share the same questions but asked differently. We find that LLMs are sensitive
to such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to
human performance (92.6% accuracy), CulturalBench-Hard is more challenging for
frontier LLMs with the best performing model (GPT-4o) at only 61.5% and the
worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with
tricky questions that have multiple correct answers (e.g., What utensils do the
Chinese usually use?), revealing a tendency to converge to a single answer. Our
results also indicate that OpenAI GPT-4o substantially outperform other
proprietary and open source models in questions related to all but one region
(Oceania). Nonetheless, all models consistently underperform on questions
related to South America and the Middle East.

摘要：<paragraph>為了讓大型語言模型 (LLM) 在不同的文化中提供更多幫助，擁有有效的文化知識基準來衡量和追蹤我們的進度至關重要。有效的基準需要強健、多元且具有挑戰性。我們推出 CulturalBench：一組 1,227 個由人類撰寫並經過人類驗證的問題，用於有效評估 LLM 的文化知識，涵蓋 45 個全球區域，包括孟加拉、辛巴威和秘魯等代表性不足的區域。問題——每個問題都經過五位獨立註解者驗證——涵蓋從食物偏好到問候禮儀等 17 個不同的主題。我們在兩種設置上評估模型：CulturalBench-Easy 和 CulturalBench-Hard，它們的問題相同，但詢問方式不同。我們發現 LLM 對此類設置差異很敏感（例如，GPT-4o 的差異為 27.3%）。與人類表現（準確率 92.6%）相比，CulturalBench-Hard 對前沿 LLM 來說更具挑戰性，表現最佳的模型 (GPT-4o) 僅為 61.5%，表現最差的模型 (Llama3-8b) 為 21.4%。此外，我們發現 LLM 經常難以應對具有多個正確答案的棘手問題（例如，中國人通常使用什麼餐具？），這表明它們傾向於收斂到一個答案。我們的結果還表明，OpenAI GPT-4o 在與所有區域（大洋洲除外）相關的問題上都明顯優於其他專有和開放原始碼模型。儘管如此，所有模型在與南美和中東相關的問題上都持續表現不佳。</paragraph>

##### **FAN: Fourier Analysis Networks**
2410.02675v1 by Yihong Dong, Ge Li, Yongding Tao, Xue Jiang, Kechi Zhang, Jia Li, Jing Su, Jun Zhang, Jingjing Xu

Despite the remarkable success achieved by neural networks, particularly
those represented by MLP and Transformer, we reveal that they exhibit potential
flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize
the periodic data rather than genuinely understanding the underlying principles
of periodicity. However, periodicity is a crucial trait in various forms of
reasoning and generalization, underpinning predictability across natural and
engineered systems through recurring patterns in observations. In this paper,
we propose FAN, a novel network architecture based on Fourier Analysis, which
empowers the ability to efficiently model and reason about periodic phenomena.
By introducing Fourier Series, the periodicity is naturally integrated into the
structure and computational processes of the neural network, thus achieving a
more accurate expression and prediction of periodic patterns. As a promising
substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in
various models with fewer parameters and FLOPs. Through extensive experiments,
we demonstrate the effectiveness of FAN in modeling and reasoning about
periodic functions, and the superiority and generalizability of FAN across a
range of real-world tasks, including symbolic formula representation, time
series forecasting, and language modeling.

摘要：儘管神經網路取得了顯著的成功，尤其是 MLP 和 Transformer 所代表的神經網路，我們發現它們在週期性的建模和推理中表現出潛在的缺陷，也就是說，它們傾向於記憶週期性資料，而不是真正理解週期性的基本原理。然而，週期性是各種推理和概括中的一項關鍵特徵，它通過觀察中的重複模式支撐著自然和工程系統中的可預測性。在本文中，我們提出了 FAN，一種基於傅立葉分析的新網路架構，它賦予了有效建模和推理週期現象的能力。透過引入傅立葉級數，週期性自然地整合到神經網路的結構和計算過程中，從而更準確地表達和預測週期性模式。作為多層感知器 (MLP) 的有希望的替代品，FAN 可以無縫地取代各種模型中的 MLP，而參數和 FLOP 更少。透過廣泛的實驗，我們證明了 FAN 在週期性函數建模和推理方面的有效性，以及 FAN 在各種實際任務中的優越性和概括性，包括符號公式表示、時間序列預測和語言建模。

##### **Examining Language Modeling Assumptions Using an Annotated Literary Dialect Corpus**
2410.02674v1 by Craig Messner, Tom Lippincott

We present a dataset of 19th century American literary orthovariant tokens
with a novel layer of human-annotated dialect group tags designed to serve as
the basis for computational experiments exploring literarily meaningful
orthographic variation. We perform an initial broad set of experiments over
this dataset using both token (BERT) and character (CANINE)-level contextual
language models. We find indications that the "dialect effect" produced by
intentional orthographic variation employs multiple linguistic channels, and
that these channels are able to be surfaced to varied degrees given particular
language modelling assumptions. Specifically, we find evidence showing that
choice of tokenization scheme meaningfully impact the type of orthographic
information a model is able to surface.

摘要：我們提供一個 19 世紀美國文學正字變體詞彙的資料集，其中包含一層由人工標記的方言群組標籤，旨在作為探索具有文學意義的正字法變化的計算實驗的基礎。我們使用詞彙（BERT）和字元（CANINE）層級的脈絡語言模型，針對此資料集執行一組初步的廣泛實驗。我們發現由刻意正字法變異產生的「方言效應」運用多種語言管道，並且這些管道能夠在給定特定語言建模假設的情況下以不同程度浮現。具體來說，我們找到證據顯示，標記化方案的選擇會對模型能夠浮現的正字法資訊類型產生有意義的影響。

##### **Unsupervised Point Cloud Completion through Unbalanced Optimal Transport**
2410.02671v1 by Taekyung Lee, Jaemoo Choi, Jaewoong Choi

Unpaired point cloud completion explores methods for learning a completion
map from unpaired incomplete and complete point cloud data. In this paper, we
propose a novel approach for unpaired point cloud completion using the
unbalanced optimal transport map, called Unbalanced Optimal Transport Map for
Unpaired Point Cloud Completion (UOT-UPC). We demonstrate that the unpaired
point cloud completion can be naturally interpreted as the Optimal Transport
(OT) problem and introduce the Unbalanced Optimal Transport (UOT) approach to
address the class imbalance problem, which is prevalent in unpaired point cloud
completion datasets. Moreover, we analyze the appropriate cost function for
unpaired completion tasks. This analysis shows that the InfoCD cost function is
particularly well-suited for this task. Our model is the first attempt to
leverage UOT for unpaired point cloud completion, achieving competitive or
superior results on both single-category and multi-category datasets. In
particular, our model is especially effective in scenarios with class
imbalance, where the proportions of categories are different between the
incomplete and complete point cloud datasets.

摘要：未配对点云补全探索了从未配对的不完整和完整点云数据中学习补全映射的方法。在本文中，我们提出了一种使用不平衡最优传输映射的新颖方法来进行未配对点云补全，称为用于未配对点云补全的不平衡最优传输映射 (UOT-UPC)。我们证明了未配对点云补全可以自然地解释为最优传输 (OT) 问题，并引入了不平衡最优传输 (UOT) 方法来解决在未配对点云补全数据集中普遍存在的类别不平衡问题。此外，我们分析了未配对补全任务的适当成本函数。该分析表明 InfoCD 成本函数特别适合此任务。我们的模型是首次尝试利用 UOT 进行未配对点云补全，在单类别和多类别数据集上均取得了有竞争力或更优的结果。特别是，我们的模型在类别不平衡的情况下特别有效，其中不完整和完整点云数据集之间的类别比例不同。

##### **AlphaIntegrator: Transformer Action Search for Symbolic Integration Proofs**
2410.02666v1 by Mert Ünsal, Timon Gehr, Martin Vechev

We present the first correct-by-construction learning-based system for
step-by-step mathematical integration. The key idea is to learn a policy,
represented by a GPT transformer model, which guides the search for the right
mathematical integration rule, to be carried out by a symbolic solver.
Concretely, we introduce a symbolic engine with axiomatically correct actions
on mathematical expressions, as well as the first dataset for step-by-step
integration. Our GPT-style transformer model, trained on this synthetic data,
demonstrates strong generalization by surpassing its own data generator in
accuracy and efficiency, using 50% fewer search steps. Our experimental results
with SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs
on a set of question-answer pairs is insufficient for solving this mathematical
task. This motivates the importance of discovering creative methods for
combining LLMs with symbolic reasoning engines, of which our work is an
instance.

摘要：我們提出了一個基於學習的第一個正確建構系統，用於逐步的數學積分。關鍵想法是學習一個策略，由 GPT 轉換器模型表示，它引導尋找正確的數學積分規則，由符號求解器執行。具體來說，我們引入了一個符號引擎，它對數學表達式具有公理上正確的動作，以及用於逐步積分的首個資料集。我們的 GPT 風格轉換器模型在這個合成資料上訓練，展示了強大的泛化能力，在準確度和效率上超越了自己的資料生成器，使用少 50% 的搜尋步驟。我們使用 SoTA LLM 的實驗結果也表明，在問題答案對上微調 LLM 的標準方法不足以解決這個數學任務。這激勵了發現將 LLM 與符號推理引擎相結合的創造性方法的重要性，其中我們的作品是一個實例。

##### **Grounded Answers for Multi-agent Decision-making Problem through Generative World Model**
2410.02664v1 by Zeyang Liu, Xinrui Yang, Shiguang Sun, Long Qian, Lipeng Wan, Xingyu Chen, Xuguang Lan

Recent progress in generative models has stimulated significant innovations
in many fields, such as image generation and chatbots. Despite their success,
these models often produce sketchy and misleading solutions for complex
multi-agent decision-making problems because they miss the trial-and-error
experience and reasoning as humans. To address this limitation, we explore a
paradigm that integrates a language-guided simulator into the multi-agent
reinforcement learning pipeline to enhance the generated answer. The simulator
is a world model that separately learns dynamics and reward, where the dynamics
model comprises an image tokenizer as well as a causal transformer to generate
interaction transitions autoregressively, and the reward model is a
bidirectional transformer learned by maximizing the likelihood of trajectories
in the expert demonstrations under language guidance. Given an image of the
current state and the task description, we use the world model to train the
joint policy and produce the image sequence as the answer by running the
converged policy on the dynamics model. The empirical results demonstrate that
this framework can improve the answers for multi-agent decision-making problems
by showing superior performance on the training and unseen tasks of the
StarCraft Multi-Agent Challenge benchmark. In particular, it can generate
consistent interaction sequences and explainable reward functions at
interaction states, opening the path for training generative models of the
future.

摘要：生成模型的最新进展刺激了許多領域的重大創新，例如圖像生成和聊天機器人。儘管這些模型很成功，但它們通常會為複雜的多主體決策問題產生粗略且誤導的解決方案，因為它們錯過了人類的試錯經驗和推理。為了解決這個限制，我們探索了一種範例，將語言引導模擬器整合到多主體強化學習管道中，以增強產生的答案。模擬器是一個世界模型，分別學習動態和獎勵，其中動態模型包含一個影像標記器以及一個因果變換器，以自迴歸的方式產生交互轉換，而獎勵模型是一個雙向變換器，透過最大化專家示範中軌跡的可能性在語言指導下學習。給定當前狀態的影像和任務描述，我們使用世界模型來訓練聯合策略，並透過在動態模型上執行收斂策略來產生影像序列作為答案。實證結果表明，這個框架可以透過在 StarCraft 多主體挑戰基準的訓練和未見任務上展現優異的效能，來改善多主體決策問題的答案。特別是，它可以在互動狀態中產生一致的互動序列和可解釋的獎勵函數，為訓練未來的生成模型開啟道路。

##### **How to Train Long-Context Language Models (Effectively)**
2410.02660v1 by Tianyu Gao, Alexander Wettig, Howard Yen, Danqi Chen

We study continued training and supervised fine-tuning (SFT) of a language
model (LM) to make effective use of long-context information. We first
establish a reliable evaluation protocol to guide model development -- Instead
of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set
of long-context tasks, and we evaluate models after SFT with instruction data
as this better reveals long-context abilities. Supported by our robust
evaluations, we run thorough experiments to decide the data mix for continued
pre-training, the instruction tuning dataset, and many other design choices. We
find that (1) code repositories and books are excellent sources of long data,
but it is crucial to combine them with high-quality short data; (2) training
with a sequence length beyond the evaluation length boosts long-context
performance; (3) for SFT, using only short instruction datasets yields strong
performance on long-context tasks. Our final model, ProLong-8B, which is
initialized from Llama-3 and trained on 40B tokens, demonstrates
state-of-the-art long-context performance among similarly sized models at a
length of 128K. ProLong outperforms Llama-3.18B-Instruct on the majority of
long-context tasks despite having seen only 5% as many tokens during
long-context training. Additionally, ProLong can effectively process up to 512K
tokens, one of the longest context windows of publicly available LMs.

摘要：<paragraph>我們研究持續訓練和監督微調 (SFT) 語言模型 (LM)，以有效利用長語境資訊。我們首先建立一個可靠的評估協定，以指導模型開發——我們使用廣泛的長語境任務，而不是困惑度或簡單的大海撈針 (NIAH) 測試，並在 SFT 後使用說明資料評估模型，因為這能更好地揭示長語境能力。在我們穩健的評估支持下，我們進行了徹底的實驗，以決定持續預訓練的資料組合、說明微調資料集和許多其他設計選擇。我們發現 (1) 程式碼儲存庫和書籍是長資料的絕佳來源，但將它們與高品質的短資料結合起來至關重要；(2) 使用超過評估長度的序列長度進行訓練會提升長語境效能；(3) 對於 SFT，僅使用短說明資料集會在長語境任務中產生強勁的效能。我們的最終模型 ProLong-8B，由 Llama-3 初始化並在 40B 個代幣上訓練，在長度為 128K 時展現出與類似規模模型相比最先進的長語境效能。儘管在長語境訓練期間僅看到 5% 的代幣，但 ProLong 在大多數長語境任務上都優於 Llama-3.18B-Instruct。此外，ProLong 可以有效處理多達 512K 個代幣，這是公開 LM 中最長的上下文視窗之一。</paragraph>

##### **Hate Personified: Investigating the role of LLMs in content moderation**
2410.02657v1 by Sarah Masud, Sahajpreet Singh, Viktor Hangya, Alexander Fraser, Tanmoy Chakraborty

For subjective tasks such as hate detection, where people perceive hate
differently, the Large Language Model's (LLM) ability to represent diverse
groups is unclear. By including additional context in prompts, we
comprehensively analyze LLM's sensitivity to geographical priming, persona
attributes, and numerical information to assess how well the needs of various
groups are reflected. Our findings on two LLMs, five languages, and six
datasets reveal that mimicking persona-based attributes leads to annotation
variability. Meanwhile, incorporating geographical signals leads to better
regional alignment. We also find that the LLMs are sensitive to numerical
anchors, indicating the ability to leverage community-based flagging efforts
and exposure to adversaries. Our work provides preliminary guidelines and
highlights the nuances of applying LLMs in culturally sensitive cases.

摘要：對於主觀任務，例如仇恨偵測，人們對仇恨的感知不同，大型語言模型 (LLM) 代表不同群體的能力尚不清楚。透過在提示中加入額外的背景，我們全面分析 LLM 對地理提示、角色屬性和數字資訊的敏感度，以評估不同群體的需求得到反映的程度。我們對兩個 LLM、五種語言和六個資料集的研究結果顯示，模仿基於角色的屬性會導致註解變異性。同時，納入地理訊號會導致更好的區域對齊。我們還發現，LLM 對數字錨點很敏感，這表示有能力利用基於社群的標記工作和對對手的曝光。我們的研究提供了初步指南，並強調了在文化敏感案例中應用 LLM 的細微差別。

##### **Scalable Simulation-free Entropic Unbalanced Optimal Transport**
2410.02656v1 by Jaemoo Choi, Jaewoong Choi

The Optimal Transport (OT) problem investigates a transport map that connects
two distributions while minimizing a given cost function. Finding such a
transport map has diverse applications in machine learning, such as generative
modeling and image-to-image translation. In this paper, we introduce a scalable
and simulation-free approach for solving the Entropic Unbalanced Optimal
Transport (EUOT) problem. We derive the dynamical form of this EUOT problem,
which is a generalization of the Schr\"odinger bridges (SB) problem. Based on
this, we derive dual formulation and optimality conditions of the EUOT problem
from the stochastic optimal control interpretation. By leveraging these
properties, we propose a simulation-free algorithm to solve EUOT, called
Simulation-free EUOT (SF-EUOT). While existing SB models require expensive
simulation costs during training and evaluation, our model achieves
simulation-free training and one-step generation by utilizing the reciprocal
property. Our model demonstrates significantly improved scalability in
generative modeling and image-to-image translation tasks compared to previous
SB methods.

摘要：最佳運輸 (OT) 問題研究了一種運輸映射，它連接了兩個分佈，同時最小化給定的成本函數。找到這樣的運輸映射在機器學習中具有廣泛的應用，例如生成模型和圖像到圖像的翻譯。在本文中，我們介紹了一種可擴充且無需模擬的方法來解決熵不平衡最佳運輸 (EUOT) 問題。我們推導了這個 EUOT 問題的動態形式，它是薛丁格橋 (SB) 問題的推廣。基於此，我們從隨機最優控制解釋中推導出 EUOT 問題的對偶公式和最優條件。通過利用這些屬性，我們提出了一個無需模擬的演算法來解決 EUOT，稱為無需模擬 EUOT (SF-EUOT)。雖然現有的 SB 模型在訓練和評估過程中需要昂貴的模擬成本，但我們的模型通過利用互惠屬性實現了無需模擬的訓練和一步生成。與先前的 SB 方法相比，我們的模型在生成模型和圖像到圖像翻譯任務中展示了顯著提高的可擴充性。

##### **Measuring and Improving Persuasiveness of Generative Models**
2410.02653v1 by Somesh Singh, Yaman K Singla, Harini SI, Balaji Krishnamurthy

LLMs are increasingly being used in workflows involving generating content to
be consumed by humans (e.g., marketing) and also in directly interacting with
humans (e.g., through chatbots). The development of such systems that are
capable of generating verifiably persuasive messages presents both
opportunities and challenges for society. On the one hand, such systems could
positively impact domains like advertising and social good, such as addressing
drug addiction, and on the other, they could be misused for spreading
misinformation and shaping political opinions. To channel LLMs' impact on
society, we need to develop systems to measure and benchmark their
persuasiveness. With this motivation, we introduce PersuasionBench and
PersuasionArena, the first large-scale benchmark and arena containing a battery
of tasks to measure the persuasion ability of generative models automatically.
We investigate to what extent LLMs know and leverage linguistic patterns that
can help them generate more persuasive language. Our findings indicate that the
persuasiveness of LLMs correlates positively with model size, but smaller
models can also be made to have a higher persuasiveness than much larger
models. Notably, targeted training using synthetic and natural datasets
significantly enhances smaller models' persuasive capabilities, challenging
scale-dependent assumptions. Our findings carry key implications for both model
developers and policymakers. For instance, while the EU AI Act and California's
SB-1047 aim to regulate AI models based on the number of floating point
operations, we demonstrate that simple metrics like this alone fail to capture
the full scope of AI's societal impact. We invite the community to explore and
contribute to PersuasionArena and PersuasionBench, available at
https://bit.ly/measure-persuasion, to advance our understanding of AI-driven
persuasion and its societal implications.

摘要：大型語言模型（LLM）正日益用於涉及產生人類可消費內容（例如行銷）的工作流程中，也用於直接與人類互動（例如透過聊天機器人）。開發出能夠產生可驗證說服性訊息的此類系統，對社會既帶來機遇，也帶來挑戰。一方面，此類系統可以對廣告和社會公益等領域產生積極影響，例如解決藥物成癮問題；另一方面，它們可能會被濫用於散布錯誤訊息和塑造政治觀點。為了引導 LLM 對社會的影響，我們需要開發系統來衡量和評估它們的說服力。基於這一動機，我們介紹了 PersuasionBench 和 PersuasionArena，這是第一個包含一系列任務的大規模基準測試和競技場，用於自動衡量生成模型的說服能力。我們探討了 LLM 在多大程度上了解並利用了可以幫助它們產生更具說服力的語言的語言模式。我們的研究結果表明，LLM 的說服力與模型大小呈正相關，但較小的模型也可以比大得多的模型具有更高的說服力。值得注意的是，使用合成和自然資料集進行的目標訓練顯著增強了較小模型的說服能力，從而挑戰了依賴規模的假設。我們的研究結果對模型開發人員和政策制定者都具有重要意義。例如，儘管歐盟人工智能法案和加州的 SB-1047 旨在根據浮點運算的數量對人工智能模型進行監管，但我們證明了僅憑藉這樣的簡單指標無法捕捉人工智能對社會影響的全部範圍。我們邀請社群探索和貢獻 PersuasionArena 和 PersuasionBench，網址為 https://bit.ly/measure-persuasion，以增進我們對人工智能驅動的說服及其社會影響的理解。

##### **CAX: Cellular Automata Accelerated in JAX**
2410.02651v1 by Maxence Faldor, Antoine Cully

Cellular automata have become a cornerstone for investigating emergence and
self-organization across diverse scientific disciplines, spanning neuroscience,
artificial life, and theoretical physics. However, the absence of a
hardware-accelerated cellular automata library limits the exploration of new
research directions, hinders collaboration, and impedes reproducibility. In
this work, we introduce CAX (Cellular Automata Accelerated in JAX), a
high-performance and flexible open-source library designed to accelerate
cellular automata research. CAX offers cutting-edge performance and a modular
design through a user-friendly interface, and can support both discrete and
continuous cellular automata with any number of dimensions. We demonstrate
CAX's performance and flexibility through a wide range of benchmarks and
applications. From classic models like elementary cellular automata and
Conway's Game of Life to advanced applications such as growing neural cellular
automata and self-classifying MNIST digits, CAX speeds up simulations up to
2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate
research by presenting a collection of three novel cellular automata
experiments, each implemented in just a few lines of code thanks to the
library's modular architecture. Notably, we show that a simple one-dimensional
cellular automaton can outperform GPT-4 on the 1D-ARC challenge.

摘要：細胞自動機已成為跨越神經科學、人工生命和理論物理等不同科學領域中研究出現和自組織的基石。然而，缺乏硬體加速的細胞自動機程式庫限制了新研究方向的探索，阻礙了合作，並妨礙了再現性。在這項工作中，我們介紹了 CAX（JAX 中加速的細胞自動機），這是一個高性能且靈活的開源程式庫，旨在加速細胞自動機研究。CAX 透過使用者友善的介面提供尖端的效能和模組化設計，並且可以支援任何維度的離散和連續細胞自動機。我們透過廣泛的基準和應用程式展示了 CAX 的效能和靈活性。從基本細胞自動機和康威生命遊戲等經典模型到先進應用程式，例如生長神經細胞自動機和自分類 MNIST 數位，CAX 將模擬速度提高了 2,000 倍。此外，我們展示了 CAX 加速研究的潛力，方法是展示三項新穎的細胞自動機實驗，由於程式庫的模組化架構，每個實驗僅使用幾行程式碼就能實作。值得注意的是，我們展示了一個簡單的一維細胞自動機可以在 1D-ARC 挑戰中優於 GPT-4。

##### **Undesirable Memorization in Large Language Models: A Survey**
2410.02650v1 by Ali Satvaty, Suzan Verberne, Fatih Turkmen

While recent research increasingly showcases the remarkable capabilities of
Large Language Models (LLMs), it's vital to confront their hidden pitfalls.
Among these challenges, the issue of memorization stands out, posing
significant ethical and legal risks. In this paper, we presents a
Systematization of Knowledge (SoK) on the topic of memorization in LLMs.
Memorization is the effect that a model tends to store and reproduce phrases or
passages from the training data and has been shown to be the fundamental issue
to various privacy and security attacks against LLMs.
  We begin by providing an overview of the literature on the memorization,
exploring it across five key dimensions: intentionality, degree,
retrievability, abstraction, and transparency. Next, we discuss the metrics and
methods used to measure memorization, followed by an analysis of the factors
that contribute to memorization phenomenon. We then examine how memorization
manifests itself in specific model architectures and explore strategies for
mitigating these effects. We conclude our overview by identifying potential
research topics for the near future: to develop methods for balancing
performance and privacy in LLMs, and the analysis of memorization in specific
contexts, including conversational agents, retrieval-augmented generation,
multilingual language models, and diffusion language models.

摘要：儘管最近的研究越來越展示出大型語言模型 (LLM) 的非凡能力，但面對其隱藏的陷阱至關重要。在這些挑戰中，記憶化問題脫穎而出，構成顯著的道德和法律風險。在本文中，我們針對 LLM 中記憶化的主題提出知識系統化 (SoK)。記憶化是一種模型傾向於儲存和複製訓練資料中的片語或段落的效果，並且已被證明是針對 LLM 的各種隱私和安全攻擊的基本問題。我們首先提供關於記憶化的文獻概述，並從五個關鍵面向探討它：意圖性、程度、可檢索性、抽象性和透明性。接下來，我們討論用於衡量記憶化的指標和方法，接著分析導致記憶化現象的因素。然後我們探討記憶化如何在特定模型架構中表現出來，並探討減輕這些影響的策略。我們透過找出近期潛在的研究主題來結束我們的概述：開發在 LLM 中平衡效能和隱私的方法，以及在特定脈絡中分析記憶化，包括對話代理、檢索增強生成、多語言語言模型和擴散語言模型。

##### **Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection**
2410.02647v1 by Song Li, Yang Tan, Song Ke, Liang Hong, Bingxin Zhou

Immunogenicity prediction is a central topic in reverse vaccinology for
finding candidate vaccines that can trigger protective immune responses.
Existing approaches typically rely on highly compressed features and simple
model architectures, leading to limited prediction accuracy and poor
generalizability. To address these challenges, we introduce ProVaccine, a novel
deep learning solution with a dual attention mechanism that integrates
pre-trained latent vector representations of protein sequences and structures.
We also compile the most comprehensive immunogenicity dataset to date,
encompassing over 9,500 antigen sequences, structures, and immunogenicity
labels from bacteria, viruses, and tumors. Extensive experiments demonstrate
that ProVaccine outperforms existing methods across a wide range of evaluation
metrics. Furthermore, we establish a post-hoc validation protocol to assess the
practical significance of deep learning models in tackling vaccine design
challenges. Our work provides an effective tool for vaccine design and sets
valuable benchmarks for future research.

摘要：免疫原性預測是反向疫苗學中的一個核心主題，用於尋找能夠引發保護性免疫反應的候選疫苗。現有的方法通常依賴於高度壓縮的特徵和簡單的模型架構，導致預測準確度有限且概括性較差。為了應對這些挑戰，我們引入了 ProVaccine，這是一種具有雙重注意力機制的深度學習解決方案，它整合了蛋白質序列和結構的預訓練潛在向量表示。我們還編制了迄今為止最全面的免疫原性數據集，其中包含來自細菌、病毒和腫瘤的 9,500 多個抗原序列、結構和免疫原性標籤。大量的實驗表明，ProVaccine 在廣泛的評估指標中優於現有方法。此外，我們建立了一個事後驗證協議，以評估深度學習模型在解決疫苗設計挑戰中的實際意義。我們的研究為疫苗設計提供了一個有效的工具，並為未來的研究設定了有價值的基準。

##### **Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents**
2410.02644v1 by Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, Yongfeng Zhang

Although LLM-based agents, powered by Large Language Models (LLMs), can use
external tools and memory mechanisms to solve complex real-world tasks, they
may also introduce critical security vulnerabilities. However, the existing
literature does not comprehensively evaluate attacks and defenses against
LLM-based agents. To address this, we introduce Agent Security Bench (ASB), a
comprehensive framework designed to formalize, benchmark, and evaluate the
attacks and defenses of LLM-based agents, including 10 scenarios (e.g.,
e-commerce, autonomous driving, finance), 10 agents targeting the scenarios,
over 400 tools, 23 different types of attack/defense methods, and 8 evaluation
metrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory
poisoning attack, a novel Plan-of-Thought backdoor attack, a mixed attack, and
10 corresponding defenses across 13 LLM backbones with nearly 90,000 testing
cases in total. Our benchmark results reveal critical vulnerabilities in
different stages of agent operation, including system prompt, user prompt
handling, tool usage, and memory retrieval, with the highest average attack
success rate of 84.30\%, but limited effectiveness shown in current defenses,
unveiling important works to be done in terms of agent security for the
community. Our code can be found at https://github.com/agiresearch/ASB.

摘要：儘管由大型語言模型（LLM）驅動的 LLM 型代理可以使用外部工具和記憶體機制來解決複雜的真實世界任務，但它們也可能引入嚴重的安全性漏洞。然而，現有的文獻並未全面評估針對 LLM 型代理的攻擊和防禦。為了解決這個問題，我們引入了代理安全基準（ASB），這是一個全面的框架，旨在形式化、評比和評估 LLM 型代理的攻擊和防禦，包括 10 個場景（例如電子商務、自動駕駛、金融）、10 個針對這些場景的代理、超過 400 個工具、23 種不同類型的攻擊/防禦方法和 8 個評估指標。根據 ASB，我們評比了 10 個提示注入攻擊、一個記憶體中毒攻擊、一個新穎的思考計劃後門攻擊、一個混合攻擊和 10 個相應的防禦措施，涵蓋 13 個 LLM 主幹，總共將近 90,000 個測試案例。我們的基準結果揭露了代理操作的不同階段中的嚴重漏洞，包括系統提示、使用者提示處理、工具使用和記憶體擷取，其中最高平均攻擊成功率為 84.30%，但目前的防禦措施顯示效果有限，揭示了社區在代理安全方面有重要的工作要做。我們的程式碼可以在 https://github.com/agiresearch/ASB 找到。

##### **Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers**
2410.02642v1 by Shijie Chen, Bernal Jiménez Gutiérrez, Yu Su

Information retrieval (IR) systems have played a vital role in modern digital
life and have cemented their continued usefulness in this new era of generative
AI via retrieval-augmented generation. With strong language processing
capabilities and remarkable versatility, large language models (LLMs) have
become popular choices for zero-shot re-ranking in IR systems. So far,
LLM-based re-ranking methods rely on strong generative capabilities, which
restricts their use to either specialized or powerful proprietary models. Given
these restrictions, we ask: is autoregressive generation necessary and optimal
for LLMs to perform re-ranking? We hypothesize that there are abundant signals
relevant to re-ranking within LLMs that might not be used to their full
potential via generation. To more directly leverage such signals, we propose
in-context re-ranking (ICR), a novel method that leverages the change in
attention pattern caused by the search query for accurate and efficient
re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration
method using a content-free query. Due to the absence of generation, ICR only
requires two ($O(1)$) forward passes to re-rank $N$ documents, making it
substantially more efficient than generative re-ranking methods that require at
least $O(N)$ forward passes. Our novel design also enables ICR to be applied to
any LLM without specialized training while guaranteeing a well-formed ranking.
Extensive experiments with two popular open-weight LLMs on standard single-hop
and multi-hop information retrieval benchmarks show that ICR outperforms
RankGPT while cutting the latency by more than 60% in practice. Through
detailed analyses, we show that ICR's performance is specially strong on tasks
that require more complex re-ranking signals. Our findings call for further
exploration on novel ways of utilizing open-weight LLMs beyond text generation.

摘要：資訊檢索 (IR) 系統在現代數位生活中扮演了至關重要的角色，並透過檢索增強生成，在生成式 AI 的新時代中鞏固了其持續的實用性。具備強大的語言處理能力和非凡的多功能性，大型語言模型 (LLM) 已成為 IR 系統中零次學習重新排序的熱門選擇。到目前為止，基於 LLM 的重新排序方法依賴於強大的生成能力，這將其使用限制在專門的或強大的專有模型中。有鑑於這些限制，我們提出疑問：自迴歸生成對於 LLM 執行重新排序來說是否必要且最佳？我們假設 LLM 中有大量與重新排序相關的訊號，而這些訊號可能無法透過生成發揮其全部潛力。為了更直接地利用這些訊號，我們提出情境內重新排序 (ICR)，這是一種新穎的方法，它利用搜尋查詢所造成的注意力模式變化來進行準確且有效的重新排序。為了減輕 LLM 中的內在偏差，我們提出使用無內容查詢的校正方法。由於沒有生成，ICR 只需要兩次 ($O(1)$) 前向傳遞來重新排序 $N$ 個文件，使其比需要至少 $O(N)$ 次前向傳遞的生成式重新排序方法大幅提高效率。我們的新穎設計還使 ICR 能夠應用於任何 LLM，而無需專門訓練，同時保證形成良好的排名。在標準單次跳躍和多跳躍資訊檢索基準上使用兩個流行的開放權重 LLM 進行的廣泛實驗表明，ICR 在實務上優於 RankGPT，同時將延遲時間減少了 60% 以上。透過詳細分析，我們表明 ICR 的效能特別強於需要更複雜重新排序訊號的任務。我們的發現呼籲進一步探索在文字生成之外利用開放權重 LLM 的新方法。

##### **Plots Unlock Time-Series Understanding in Multimodal Models**
2410.02637v1 by Mayank Daswani, Mathias M. J. Bellaiche, Marc Wilson, Desislav Ivanov, Mikhail Papkov, Eva Schnider, Jing Tang, Kay Lamerigts, Gabriela Botea, Michael A. Sanchez, Yojan Patel, Shruthi Prabhakara, Shravya Shetty, Umesh Telang

While multimodal foundation models can now natively work with data beyond
text, they remain underutilized in analyzing the considerable amounts of
multi-dimensional time-series data in fields like healthcare, finance, and
social sciences, representing a missed opportunity for richer, data-driven
insights. This paper proposes a simple but effective method that leverages the
existing vision encoders of these models to "see" time-series data via plots,
avoiding the need for additional, potentially costly, model training. Our
empirical evaluations show that this approach outperforms providing the raw
time-series data as text, with the additional benefit that visual time-series
representations demonstrate up to a 90% reduction in model API costs. We
validate our hypothesis through synthetic data tasks of increasing complexity,
progressing from simple functional form identification on clean data, to
extracting trends from noisy scatter plots. To demonstrate generalizability
from synthetic tasks with clear reasoning steps to more complex, real-world
scenarios, we apply our approach to consumer health tasks - specifically fall
detection, activity recognition, and readiness assessment - which involve
heterogeneous, noisy data and multi-step reasoning. The overall success in plot
performance over text performance (up to an 120% performance increase on
zero-shot synthetic tasks, and up to 150% performance increase on real-world
tasks), across both GPT and Gemini model families, highlights our approach's
potential for making the best use of the native capabilities of foundation
models.

摘要：儘管多模態基礎模型現在可以原生處理文字以外的資料，但它們在分析醫療保健、金融和社會科學等領域中大量多維時間序列資料時仍未被充分利用，這代表錯失了獲得更豐富資料驅動見解的機會。本文提出了一種簡單但有效的方法，利用這些模型現有的視覺編碼器透過圖表「查看」時間序列資料，避免需要額外且可能昂貴的模型訓練。我們的經驗評估顯示，這種方法優於提供原始時間序列資料作為文字，額外的好處是視覺時間序列表示可以減少高達 90% 的模型 API 成本。我們透過日益複雜的合成資料任務驗證我們的假設，從乾淨資料上的簡單函數形式識別，到從雜訊散佈圖中萃取趨勢。為了證明從具有明確推理步驟的合成任務到更複雜的真實世界場景的概括性，我們將我們的做法應用於消費者健康任務，特別是跌倒偵測、活動識別和準備評估，這些任務涉及異質、雜訊資料和多步驟推理。在 GPT 和 Gemini 模型系列中，圖表表現優於文字表現的整體成功（在零次學習合成任務中表現提升高達 120%，在真實世界任務中表現提升高達 150%），突顯了我們的方法在最佳利用基礎模型原生功能方面的潛力。

##### **Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning**
2410.02631v1 by Tianxiang Hu, Pei Zhang, Baosong Yang, Jun Xie, Derek F. Wong, Rui Wang

Achieving consistent high-quality machine translation (MT) across diverse
domains remains a significant challenge, primarily due to the limited and
imbalanced parallel training data available in various domains. While large
language models (LLMs) have demonstrated impressive general understanding and
generation abilities, their potential in multi-domain MT is under-explored. We
establish a comprehensive benchmark for multi-domain translation, featuring 25
German$\Leftrightarrow$English and 22 Chinese$\Leftrightarrow$English test sets
respectively covering 15 domains. Our evaluation of prominent LLMs reveals a
discernible performance gap against traditional MT systems, highlighting domain
overfitting and catastrophic forgetting issues after fine-tuning on
domain-limited corpora. To mitigate this, we propose a domain Chain of Thought
(CoT) fine-tuning technique that utilizes the intrinsic multi-domain
intelligence of LLMs to improve translation performance. This method inspires
the LLM to perceive domain information from the source text, which then serves
as a helpful hint to guide the translation process. Despite being trained on a
small dataset of four domains, our CoT fine-tune approach achieves notable
enhancements in translation accuracy and domain robustness than traditional
fine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20
German$\rightarrow$English distinct out-of-domain tests.

摘要：在不同的領域中持續獲得高品質的機器翻譯 (MT) 仍是一項重大的挑戰，這主要是因為在不同的領域中，可用的平行訓練資料有限且不平衡。儘管大型語言模型 (LLM) 已展現出令人印象深刻的理解力和生成能力，但其在多領域 MT 中的潛力尚未被充分探索。我們建立了一個全面的多領域翻譯基準，分別包含 25 個德語$\Leftrightarrow$英語和 22 個中文$\Leftrightarrow$英語的測試集，涵蓋 15 個領域。我們對著名的 LLM 進行評估，發現與傳統 MT 系統相比，其性能存在明顯差距，這突顯了在特定領域語料庫上微調後，會出現領域過度擬合和災難性遺忘問題。為了減輕這種情況，我們提出了一種基於思考鏈 (CoT) 的領域微調技術，該技術利用了 LLM 內在的多領域智能來改善翻譯性能。此方法啟發 LLM 從原始文本中感知領域信息，然後作為有用的提示來指導翻譯過程。儘管在四個領域的小型數據集上進行訓練，但我們的 CoT 微調方法在翻譯準確性和領域魯棒性方面取得了顯著的提升，這一點在超過 20 個德語$\rightarrow$英語的不同領域外測試中，平均 BLEU 分數提升了 1.53 個點即可證明。

##### **Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization**
2410.02628v1 by Mikhail Persiianov, Arip Asadulaev, Nikita Andreev, Nikita Starodubcev, Dmitry Baranchuk, Anastasis Kratsios, Evgeny Burnaev, Alexander Korotin

Learning conditional distributions $\pi^*(\cdot|x)$ is a central problem in
machine learning, which is typically approached via supervised methods with
paired data $(x,y) \sim \pi^*$. However, acquiring paired data samples is often
challenging, especially in problems such as domain translation. This
necessitates the development of $\textit{semi-supervised}$ models that utilize
both limited paired data and additional unpaired i.i.d. samples $x \sim
\pi^*_x$ and $y \sim \pi^*_y$ from the marginal distributions. The usage of
such combined data is complex and often relies on heuristic approaches. To
tackle this issue, we propose a new learning paradigm that integrates both
paired and unpaired data $\textbf{seamlessly}$ through the data likelihood
maximization techniques. We demonstrate that our approach also connects
intriguingly with inverse entropic optimal transport (OT). This finding allows
us to apply recent advances in computational OT to establish a $\textbf{light}$
learning algorithm to get $\pi^*(\cdot|x)$. Furthermore, we demonstrate through
empirical tests that our method effectively learns conditional distributions
using paired and unpaired data simultaneously.

摘要：學習條件分配 $\pi^*(\cdot|x)$ 是機器學習中的一個核心問題，通常透過監督式方法搭配成對資料 $(x,y) \sim \pi^*$ 來處理。然而，取得成對資料樣本通常具有挑戰性，尤其是在網域轉換等問題中。這需要開發 $\textit{半監督式}$ 模型，利用有限的成對資料和來自邊際分配的額外未成對 i.i.d. 樣本 $x \sim \pi^*_x$ 和 $y \sim \pi^*_y$。這種組合資料的使用很複雜，而且經常依賴於啟發式方法。為了解決這個問題，我們提出一個新的學習範例，透過資料似然最大化技術將成對和未成對資料 $\textbf{無縫}$ 整合。我們證明了我們的做法也與反熵最優傳輸 (OT) 有著有趣的關聯。這個發現讓我們得以應用計算 OT 的最新進展，建立一個 $\textbf{輕量級}$ 的學習演算法來取得 $\pi^*(\cdot|x)$。此外，我們透過實證測試證明了我們的模型能有效地同時使用成對和未成對資料來學習條件分配。

##### **NL-Eye: Abductive NLI for Images**
2410.02613v1 by Mor Ventura, Michael Toker, Nitay Calderon, Zorik Gekhman, Yonatan Bitton, Roi Reichart

Will a Visual Language Model (VLM)-based bot warn us about slipping if it
detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet
their ability to infer outcomes and causes remains underexplored. To address
this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual
abductive reasoning skills. NL-Eye adapts the abductive Natural Language
Inference (NLI) task to the visual domain, requiring models to evaluate the
plausibility of hypothesis images based on a premise image and explain their
decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050
images) spanning diverse reasoning categories: physical, functional, logical,
emotional, cultural, and social. The data curation process involved two steps -
writing textual descriptions and generating images using text-to-image models,
both requiring substantial human involvement to ensure high-quality and
challenging scenes. Our experiments show that VLMs struggle significantly on
NL-Eye, often performing at random baseline levels, while humans excel in both
plausibility prediction and explanation quality. This demonstrates a deficiency
in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a
crucial step toward developing VLMs capable of robust multimodal reasoning for
real-world applications, including accident-prevention bots and generated video
verification.

摘要：基於視覺語言模型 (VLM) 的機器人如果偵測到濕滑的地板，會警告我們滑倒嗎？最近的 VLM 已展現令人印象深刻的能力，但它們推論結果和原因的能力仍未被充分探索。為了解決這個問題，我們引進 NL-Eye，這是一個基準測試，旨在評估 VLM 的視覺演繹推理技能。NL-Eye 將演繹自然語言推理 (NLI) 任務調整到視覺領域，要求模型根據前提影像評估假設影像的可能性，並說明它們的決定。NL-Eye 包含 350 個經過仔細策展的三元組範例（1,050 張影像），涵蓋各種推理類別：物理、功能、邏輯、情緒、文化和社會。資料策展過程包含兩個步驟 - 撰寫文字描述和使用文字轉影像模型產生影像，這兩個步驟都需要大量人為參與，以確保場景的高品質和挑戰性。我們的實驗顯示，VLM 在 NL-Eye 上顯得相當吃力，通常表現得像隨機基線層級，而人類則在可能性預測和解釋品質方面都很出色。這顯示出現代 VLM 在演繹推理能力上的不足。NL-Eye 代表著朝向開發具備強健多模態推理能力的 VLM 邁出的關鍵一步，適用於預防事故機器人和產生的影片驗證等實際應用。

##### **IndicSentEval: How Effectively do Multilingual Transformer Models encode Linguistic Properties for Indic Languages?**
2410.02611v1 by Akhilesh Aravapalli, Mounika Marreddy, Subba Reddy Oota, Radhika Mamidi, Manish Gupta

Transformer-based models have revolutionized the field of natural language
processing. To understand why they perform so well and to assess their
reliability, several studies have focused on questions such as: Which
linguistic properties are encoded by these models, and to what extent? How
robust are these models in encoding linguistic properties when faced with
perturbations in the input text? However, these studies have mainly focused on
BERT and the English language. In this paper, we investigate similar questions
regarding encoding capability and robustness for 8 linguistic properties across
13 different perturbations in 6 Indic languages, using 9 multilingual
Transformer models (7 universal and 2 Indic-specific). To conduct this study,
we introduce a novel multilingual benchmark dataset, IndicSentEval, containing
approximately $\sim$47K sentences. Surprisingly, our probing analysis of
surface, syntactic, and semantic properties reveals that while almost all
multilingual models demonstrate consistent encoding performance for English,
they show mixed results for Indic languages. As expected, Indic-specific
multilingual models capture linguistic properties in Indic languages better
than universal models. Intriguingly, universal models broadly exhibit better
robustness compared to Indic-specific models, particularly under perturbations
such as dropping both nouns and verbs, dropping only verbs, or keeping only
nouns. Overall, this study provides valuable insights into probing and
perturbation-specific strengths and weaknesses of popular multilingual
Transformer-based models for different Indic languages. We make our code and
dataset publicly available [https://tinyurl.com/IndicSentEval}].

摘要：<paragraph>基於 Transformer 的模型已經徹底改變了自然語言處理領域。為了了解它們為何表現如此出色，並評估它們的可靠性，多項研究專注於以下問題：這些模型編碼了哪些語言特性，編碼程度如何？這些模型在面對輸入文字中的擾動時，編碼語言特性的穩健性如何？然而，這些研究主要集中於 BERT 和英文。在本文中，我們研究了 6 種印度語言中 13 種不同擾動的 8 種語言特性的編碼能力和穩健性，並使用了 9 種多語言 Transformer 模型（7 種通用模型和 2 種印度特定模型）。為了進行這項研究，我們引入了新的多語言基準數據集 IndicSentEval，其中包含大約 47K 個句子。令人驚訝的是，我們對表面、語法和語義特性的探測分析顯示，雖然幾乎所有多語言模型都展現出一致的英文編碼效能，但它們在印度語言方面卻呈現出好壞參半的結果。正如預期，印度特定多語言模型比通用模型更能捕捉印度語言中的語言特性。有趣的是，與印度特定模型相比，通用模型普遍表現出更好的穩健性，特別是在丟棄名詞和動詞、僅丟棄動詞或僅保留名詞等擾動下。總體而言，這項研究提供了有價值的見解，用於探測和擾動特定優勢和劣勢，以了解不同印度語言的多語言基於 Transformer 的熱門模型。我們公開了我們的程式碼和數據集 [https://tinyurl.com/IndicSentEval}].</paragraph>

##### **Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in Under-Resourced Languages Using Explainable AI**
2410.02609v1 by Mesay Gemeda Yigezu, Melkamu Abay Mersha, Girma Yohannis Bade, Jugal Kalita, Olga Kolesnikova, Alexander Gelbukh

The proliferation of fake news has emerged as a significant threat to the
integrity of information dissemination, particularly on social media platforms.
Misinformation can spread quickly due to the ease of creating and disseminating
content, affecting public opinion and sociopolitical events. Identifying false
information is therefore essential to reducing its negative consequences and
maintaining the reliability of online news sources. Traditional approaches to
fake news detection often rely solely on content-based features, overlooking
the crucial role of social context in shaping the perception and propagation of
news articles. In this paper, we propose a comprehensive approach that
integrates social context-based features with news content features to enhance
the accuracy of fake news detection in under-resourced languages. We perform
several experiments utilizing a variety of methodologies, including traditional
machine learning, neural networks, ensemble learning, and transfer learning.
Assessment of the outcomes of the experiments shows that the ensemble learning
approach has the highest accuracy, achieving a 0.99 F1 score. Additionally,
when compared with monolingual models, the fine-tuned model with the target
language outperformed others, achieving a 0.94 F1 score. We analyze the
functioning of the models, considering the important features that contribute
to model performance, using explainable AI techniques.

摘要：假新聞的激增已成為資訊傳播的完整性一大威脅，特別是在社群媒體平台上。由於內容的建立和傳播十分容易，錯誤資訊能快速散布，影響公眾輿論和社會政治事件。因此，辨識假資訊對於減少其負面影響和維持線上新聞來源的可靠性至關重要。傳統的假新聞偵測方法通常僅依賴於基於內容的特徵，忽略了社會脈絡在塑造新聞文章的認知和傳播過程中所扮演的關鍵角色。在本文中，我們提出了一個綜合性的方法，將基於社會脈絡的特徵與新聞內容特徵整合，以提升在資源不足的語言中偵測假新聞的準確性。我們執行多項實驗，利用各種方法，包括傳統機器學習、神經網路、集成學習和遷移學習。實驗結果的評估顯示，集成學習方法具有最高的準確性，達到 0.99 的 F1 分數。此外，與單語模型相比，使用目標語言微調的模型表現優於其他模型，達到 0.94 的 F1 分數。我們分析模型的功能，考量有助於模型效能的重要特徵，並使用可解釋的 AI 技術。

##### **Beyond Expected Returns: A Policy Gradient Algorithm for Cumulative Prospect Theoretic Reinforcement Learning**
2410.02605v1 by Olivier Lepel, Anas Barakat

The widely used expected utility theory has been shown to be empirically
inconsistent with human preferences in the psychology and behavioral economy
literatures. Cumulative Prospect Theory (CPT) has been developed to fill in
this gap and provide a better model for human-based decision-making supported
by empirical evidence. It allows to express a wide range of attitudes and
perceptions towards risk, gains and losses. A few years ago, CPT has been
combined with Reinforcement Learning (RL) to formulate a CPT policy
optimization problem where the goal of the agent is to search for a policy
generating long-term returns which are aligned with their preferences. In this
work, we revisit this policy optimization problem and provide new insights on
optimal policies and their nature depending on the utility function under
consideration. We further derive a novel policy gradient theorem for the CPT
policy optimization objective generalizing the seminal corresponding result in
standard RL. This result enables us to design a model-free policy gradient
algorithm to solve the CPT-RL problem. We illustrate the performance of our
algorithm in simple examples motivated by traffic control and electricity
management applications. We also demonstrate that our policy gradient algorithm
scales better to larger state spaces compared to the existing zeroth order
algorithm for solving the same problem.

摘要：廣泛使用的預期效用理論已被證實與心理學和行為經濟文獻中的人類偏好經驗不符。累積展望理論 (CPT) 已被發展出來以填補這個差距，並為基於人類的決策制定提供更好的模型，並得到經驗證據的支持。它允許表達對風險、收益和損失的廣泛態度和看法。幾年前，CPT 已與強化學習 (RL) 結合，以制定 CPT 政策優化問題，其中代理的目標是搜尋一個產生與其偏好一致的長期回報的政策。在這項工作中，我們重新審視這個政策優化問題，並提供關於最佳政策及其性質的新見解，具體取決於考慮中的效用函數。我們進一步為 CPT 政策優化目標導出一個新穎的政策梯度定理，概括了標準 RL 中開創性的對應結果。這個結果使我們能夠設計一個無模型政策梯度演算法來解決 CPT-RL 問題。我們在由交通控制和電力管理應用激勵的簡單範例中說明了我們演算法的效能。我們還證明，與用於解決相同問題的現有零階演算法相比，我們的政策梯度演算法可以更好地擴展到更大的狀態空間。

##### **Agents' Room: Narrative Generation through Multi-step Collaboration**
2410.02603v1 by Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, Mirella Lapata

Writing compelling fiction is a multifaceted process combining elements such
as crafting a plot, developing interesting characters, and using evocative
language. While large language models (LLMs) show promise for story writing,
they currently rely heavily on intricate prompting, which limits their use. We
propose Agents' Room, a generation framework inspired by narrative theory, that
decomposes narrative writing into subtasks tackled by specialized agents. To
illustrate our method, we introduce Tell Me A Story, a high-quality dataset of
complex writing prompts and human-written stories, and a novel evaluation
framework designed specifically for assessing long narratives. We show that
Agents' Room generates stories that are preferred by expert evaluators over
those produced by baseline systems by leveraging collaboration and
specialization to decompose the complex story writing task into tractable
components. We provide extensive analysis with automated and human-based
metrics of the generated output.

摘要：撰寫引人入勝的小說是一個多方面的過程，結合了諸如構思情節、塑造有趣角色和使用引人注目的語言等元素。儘管大型語言模型 (LLM) 在故事寫作方面顯示出前景，但它們目前嚴重依賴於複雜的提示，這限制了它們的使用。我們提出受敘事理論啟發的生成框架「Agents' Room」，它將敘事寫作分解為由專業代理人處理的子任務。為了說明我們的做法，我們引入了 Tell Me A Story，這是一個由複雜寫作提示和人工撰寫的故事組成的優質資料集，以及一個專門用於評估長篇敘事的創新評估框架。我們展示了「Agents' Room」產生的故事比基線系統產生的故事更受專家評估員的青睞，因為它利用協作和專業化將複雜的故事寫作任務分解為易於處理的組成部分。我們提供了對生成輸出的自動化和基於人類的指標的廣泛分析。

##### **Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks**
2410.02596v1 by Rui Hu, Yifan Zhang, Zhuoran Li, Longbo Huang

Generative Flow Networks (GFlowNets) are a novel class of generative models
designed to sample from unnormalized distributions and have found applications
in various important tasks, attracting great research interest in their
training algorithms. In general, GFlowNets are trained by fitting the forward
flow to the backward flow on sampled training objects. Prior work focused on
the choice of training objects, parameterizations, sampling and resampling
strategies, and backward policies, aiming to enhance credit assignment,
exploration, or exploitation of the training process. However, the choice of
regression loss, which can highly influence the exploration and exploitation
behavior of the under-training policy, has been overlooked. Due to the lack of
theoretical understanding for choosing an appropriate regression loss, most
existing algorithms train the flow network by minimizing the squared error of
the forward and backward flows in log-space, i.e., using the quadratic
regression loss. In this work, we rigorously prove that distinct regression
losses correspond to specific divergence measures, enabling us to design and
analyze regression losses according to the desired properties of the
corresponding divergence measures. Specifically, we examine two key properties:
zero-forcing and zero-avoiding, where the former promotes exploitation and
higher rewards, and the latter encourages exploration and enhances diversity.
Based on our theoretical framework, we propose three novel regression losses,
namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three
benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our
proposed losses are compatible with most existing training algorithms, and
significantly improve the performances of the algorithms concerning convergence
speed, sample diversity, and robustness.

摘要：生成流網路 (GFlowNets) 是一種新穎的生成模型類別，
旨在從未正規化的分佈中進行取樣，並已在各種重要任務中找到應用，
在他們的訓練演算法中吸引了極大的研究興趣。一般來說，GFlowNets 是透過將正向流與取樣訓練物件上的反向流相擬合來訓練的。先前的研究專注於訓練物件、參數化、取樣和再取樣策略以及反向策略的選擇，旨在增強訓練過程的信用分配、探索或利用。然而，回歸損失的選擇可能會高度影響受訓練策略的探索和利用行為，而這點已被忽略。由於缺乏選擇適當回歸損失的理論理解，大多數現有演算法透過最小化對數空間中正向和反向流的平方誤差來訓練流網路，即使用二次回歸損失。在這項工作中，我們嚴謹地證明了不同的回歸損失對應於特定的距離測度，使我們能夠根據對應距離測度的所需特性來設計和分析回歸損失。具體來說，我們檢查了兩個關鍵特性：零強制和零避免，其中前者促進利用和更高的回報，而後者鼓勵探索並增強多樣性。基於我們的理論框架，我們提出了三種新穎的回歸損失，即 Shifted-Cosh、Linex(1/2) 和 Linex(1)。我們在三個基準上對它們進行評估：超網格、位元序列生成和分子生成。我們提出的損失與大多數現有的訓練演算法相容，並且顯著改善了演算法在收斂速度、樣本多樣性和穩健性方面的效能。

##### **IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of Both Driver and Passengers**
2410.02592v1 by Zihan Fang, Zheng Lin, Senkang Hu, Hangcheng Cao, Yiqin Deng, Xianhao Chen, Yuguang Fang

Recently, in-car monitoring has emerged as a promising technology for
detecting early-stage abnormal status of the driver and providing timely alerts
to prevent traffic accidents. Although training models with multimodal data
enhances the reliability of abnormal status detection, the scarcity of labeled
data and the imbalance of class distribution impede the extraction of critical
abnormal state features, significantly deteriorating training performance.
Furthermore, missing modalities due to environment and hardware limitations
further exacerbate the challenge of abnormal status identification. More
importantly, monitoring abnormal health conditions of passengers, particularly
in elderly care, is of paramount importance but remains underexplored. To
address these challenges, we introduce our IC3M, an efficient
camera-rotation-based multimodal framework for monitoring both driver and
passengers in a car. Our IC3M comprises two key modules: an adaptive threshold
pseudo-labeling strategy and a missing modality reconstruction. The former
customizes pseudo-labeling thresholds for different classes based on the class
distribution, generating class-balanced pseudo labels to guide model training
effectively, while the latter leverages crossmodality relationships learned
from limited labels to accurately recover missing modalities by distribution
transferring from available modalities. Extensive experimental results
demonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,
precision, and recall while exhibiting superior robustness under limited
labeled data and severe missing modality.

摘要：<paragraph>近來，車載監控已成為一種很有前景的技術，可用於偵測駕駛的異常狀態，並提供及時的警示，以預防交通事故。儘管使用多模式資料訓練模型可提升異常狀態偵測的可靠性，但標籤資料的稀少和類別分佈的不平衡會阻礙關鍵異常狀態特徵的萃取，大幅降低訓練效能。此外，由於環境和硬體限制而遺失的模式會進一步加劇異常狀態識別的挑戰。更重要的是，監控乘客的異常健康狀況，特別是在老年照護方面，至關重要，但仍未受到充分的探討。為了應對這些挑戰，我們引入了 IC3M，這是一個用於監控車輛中駕駛和乘客的有效攝影機旋轉式多模式架構。我們的 IC3M 包含兩個關鍵模組：自適應閾值偽標籤策略和遺失模式重建。前者會根據類別分佈為不同的類別自訂偽標籤閾值，產生類別平衡的偽標籤以有效引導模型訓練，而後者則利用從有限標籤中學習到的跨模式關係，透過從可用模式傳輸分佈來準確還原遺失的模式。大量的實驗結果證明，在標籤資料有限且遺失模式嚴重的情況下，IC3M 在準確度、精確度和召回率方面都優於最先進的基準。
</paragraph>

##### **Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions**
2410.02584v1 by Angana Borah, Rada Mihalcea

As Large Language Models (LLMs) continue to evolve, they are increasingly
being employed in numerous studies to simulate societies and execute diverse
social tasks. However, LLMs are susceptible to societal biases due to their
exposure to human-generated data. Given that LLMs are being used to gain
insights into various societal aspects, it is essential to mitigate these
biases. To that end, our study investigates the presence of implicit gender
biases in multi-agent LLM interactions and proposes two strategies to mitigate
these biases. We begin by creating a dataset of scenarios where implicit gender
biases might arise, and subsequently develop a metric to assess the presence of
biases. Our empirical analysis reveals that LLMs generate outputs characterized
by strong implicit bias associations (>= 50\% of the time). Furthermore, these
biases tend to escalate following multi-agent interactions. To mitigate them,
we propose two strategies: self-reflection with in-context examples (ICE); and
supervised fine-tuning. Our research demonstrates that both methods effectively
mitigate implicit biases, with the ensemble of fine-tuning and self-reflection
proving to be the most successful.

摘要：隨著大型語言模型 (LLM) 的持續發展，它們正越來越多地用於模擬社會並執行各種社會任務的研究中。然而，由於 LLM 接觸了人為生成的資料，因此容易受到社會偏見的影響。鑑於 LLM 被用於深入了解各種社會層面，因此減輕這些偏見至關重要。為此，我們的研究調查了多主體 LLM 互動中隱含性別偏見的存在，並提出了減輕這些偏見的兩個策略。我們首先建立了一個場景資料集，其中可能會出現隱含的性別偏見，然後開發一個指標來評估偏見的存在。我們的實證分析表明，LLM 生成的輸出具有強烈的隱含偏見關聯（>= 50% 的時間）。此外，這些偏見往往會在多主體互動後升級。為了減輕它們，我們提出了兩種策略：帶有情境範例 (ICE) 的自我反省；以及監督微調。我們的研究表明，這兩種方法都能有效減輕隱含偏見，其中微調和自我反省的組合被證明是最成功的。

##### **Deep Regression 2D-3D Ultrasound Registration for Liver Motion Correction in Focal Tumor Thermal Ablation**
2410.02579v1 by Shuwei Xing, Derek W. Cool, David Tessier, Elvis C. S. Chen, Terry M. Peters, Aaron Fenster

Liver tumor ablation procedures require accurate placement of the needle
applicator at the tumor centroid. The lower-cost and real-time nature of
ultrasound (US) has advantages over computed tomography (CT) for applicator
guidance, however, in some patients, liver tumors may be occult on US and tumor
mimics can make lesion identification challenging. Image registration
techniques can aid in interpreting anatomical details and identifying tumors,
but their clinical application has been hindered by the tradeoff between
alignment accuracy and runtime performance, particularly when compensating for
liver motion due to patient breathing or movement. Therefore, we propose a
2D-3D US registration approach to enable intra-procedural alignment that
mitigates errors caused by liver motion. Specifically, our approach can
correlate imbalanced 2D and 3D US image features and use continuous 6D rotation
representations to enhance the model's training stability. The dataset was
divided into 2388, 196 and 193 image pairs for training, validation and
testing, respectively. Our approach achieved a mean Euclidean distance error of
2.28 mm $\pm$ 1.81 mm and a mean geodesic angular error of 2.99$^{\circ}$ $\pm$
1.95$^{\circ}$, with a runtime of 0.22 seconds per 2D-3D US image pair. These
results demonstrate that our approach can achieve accurate alignment and
clinically acceptable runtime, indicating potential for clinical translation.

摘要：肝臟腫瘤消融手術需要將針頭施用器精準置於腫瘤中心點。超音波 (US) 成本較低且為即時影像，相較於電腦斷層掃描 (CT)，在施用器導引方面有優勢，然而，對於某些患者，肝臟腫瘤在超音波影像上可能不明顯，而腫瘤模擬會讓病灶辨識更具挑戰性。影像配準技術有助於解讀解剖細節和辨識腫瘤，但由於平衡校準準確度和執行效能，其臨床應用受到阻礙，特別是在補償因患者呼吸或移動而產生的肝臟運動時。因此，我們提出 2D-3D 超音波配準方法，以進行術中校準，減輕因肝臟運動而產生的誤差。具體來說，我們的做法可以關聯失衡的 2D 和 3D 超音波影像特徵，並使用連續的 6D 旋轉表示來增強模型的訓練穩定度。資料集分為 2388、196 和 193 影像對，分別用於訓練、驗證和測試。我們的做法達成平均歐氏距離誤差 2.28 mm $\pm$ 1.81 mm，以及平均測地角誤差 2.99$^{\circ}$ $\pm$ 1.95$^{\circ}$，執行時間為每 2D-3D 超音波影像對 0.22 秒。這些結果證明我們的做法可以達成精準校準和臨床上可接受的執行時間，顯示出臨床轉譯的潛力。

##### **Convolutional Variational Autoencoders for Spectrogram Compression in Automatic Speech Recognition**
2410.02560v2 by Olga Iakovenko, Ivan Bondarenko

For many Automatic Speech Recognition (ASR) tasks audio features as
spectrograms show better results than Mel-frequency Cepstral Coefficients
(MFCC), but in practice they are hard to use due to a complex dimensionality of
a feature space. The following paper presents an alternative approach towards
generating compressed spectrogram representation, based on Convolutional
Variational Autoencoders (VAE). A Convolutional VAE model was trained on a
subsample of the LibriSpeech dataset to reconstruct short fragments of audio
spectrograms (25 ms) from a 13-dimensional embedding. The trained model for a
40-dimensional (300 ms) embedding was used to generate features for corpus of
spoken commands on the GoogleSpeechCommands dataset. Using the generated
features an ASR system was built and compared to the model with MFCC features.

摘要：對於許多自動語音辨識 (ASR) 任務，音訊特徵如語譜圖顯示的結果優於梅爾頻率倒頻譜係數 (MFCC)，但實際上由於特徵空間的複雜維度，它們很難使用。以下論文提出了一種基於卷積變異自動編碼器 (VAE) 的生成壓縮語譜圖表示的替代方法。在 LibriSpeech 資料集的子樣本上訓練了卷積 VAE 模型，以從 13 維嵌入中重建音訊語譜圖的短片段 (25 毫秒)。40 維 (300 毫秒) 嵌入的訓練模型用於為 GoogleSpeechCommands 資料集上的口說命令語料庫生成特徵。使用生成的特性建立了一個 ASR 系統，並與具有 MFCC 特性的模型進行比較。

##### **Improving Unsupervised Constituency Parsing via Maximizing Semantic Information**
2410.02558v1 by Junjie Chen, Xiangheng He, Yusuke Miyao, Danushka Bollegala

Unsupervised constituency parsers organize phrases within a sentence into a
tree-shaped syntactic constituent structure that reflects the organization of
sentence semantics. However, the traditional objective of maximizing sentence
log-likelihood (LL) does not explicitly account for the close relationship
between the constituent structure and the semantics, resulting in a weak
correlation between LL values and parsing accuracy. In this paper, we introduce
a novel objective for training unsupervised parsers: maximizing the information
between constituent structures and sentence semantics (SemInfo). We introduce a
bag-of-substrings model to represent the semantics and apply the
probability-weighted information metric to estimate the SemInfo. Additionally,
we develop a Tree Conditional Random Field (TreeCRF)-based model to apply the
SemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG)
induction, the state-of-the-art method for unsupervised constituency parsing.
Experiments demonstrate that SemInfo correlates more strongly with parsing
accuracy than LL. Our algorithm significantly enhances parsing accuracy by an
average of 7.85 points across five PCFG variants and in four languages,
achieving new state-of-the-art results in three of the four languages.

摘要：無監督成分句法剖析器將句子中的片語組織成一個樹形句法成分結構，反映出句子語義的組織。然而，最大化句子對數似然 (LL) 的傳統目標並未明確說明成分結構和語義之間的密切關係，導致 LL 值和剖析準確度之間的相關性較弱。在本文中，我們介紹了一個訓練無監督剖析器的目標：最大化成分結構和句子語義 (SemInfo) 之間的資訊。我們引入一個子字串袋模型來表示語義，並應用機率加權資訊指標來估計 SemInfo。此外，我們開發了一個基於樹條件隨機場 (TreeCRF) 的模型，將 SemInfo 最大化目標應用於機率無上下文文法 (PCFG) 歸納，這是無監督成分剖析的最新方法。實驗證明，SemInfo 與剖析準確度比 LL 的相關性更強。我們的演算法顯著提高了剖析準確度，在五個 PCFG 變體和四種語言中平均提高了 7.85 分，在四種語言中的三種語言中取得了新的最新結果。

##### **ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration**
2410.02551v1 by Zixiang Wang, Yinghao Zhu, Huiya Zhao, Xiaochen Zheng, Tianlong Wang, Wen Tang, Yasha Wang, Chengwei Pan, Ewen M. Harrison, Junyi Gao, Liantao Ma

We introduce ColaCare, a framework that enhances Electronic Health Record
(EHR) modeling through multi-agent collaboration driven by Large Language
Models (LLMs). Our approach seamlessly integrates domain-specific expert models
with LLMs to bridge the gap between structured EHR data and text-based
reasoning. Inspired by clinical consultations, ColaCare employs two types of
agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.
Expert models process and generate predictions from numerical EHR data, while
LLM agents produce reasoning references and decision-making reports within the
collaborative consultation framework. We additionally incorporate the Merck
Manual of Diagnosis and Therapy (MSD) medical guideline within a
retrieval-augmented generation (RAG) module for authoritative evidence support.
Extensive experiments conducted on four distinct EHR datasets demonstrate
ColaCare's superior performance in mortality prediction tasks, underscoring its
potential to revolutionize clinical decision support systems and advance
personalized precision medicine. The code, complete prompt templates, more case
studies, etc. are publicly available at the anonymous link:
https://colacare.netlify.app.

摘要：我們推出 ColaCare，一個透過大型語言模型 (LLM) 驅動的多重代理協作，來增強電子健康紀錄 (EHR) 建模的框架。我們的做法將領域特定專家模型與 LLM 無縫整合，以彌合結構化 EHR 資料與基於文字的推理之間的差距。ColaCare 受到臨床諮詢的啟發，採用了兩種代理：DoctorAgent 和 MetaAgent，它們協作分析患者資料。專家模型處理並從數值 EHR 資料產生預測，而 LLM 代理則在協作諮詢框架內產生推理參考和決策報告。我們另外在一個檢索增強生成 (RAG) 模組中納入了默克診斷與治療手冊 (MSD) 醫療指南，以獲得權威證據支持。在四個不同的 EHR 資料集上進行的廣泛實驗證明了 ColaCare 在死亡率預測任務中的優異效能，強調了其革新臨床決策支援系統和推進個人化精準醫療的潛力。程式碼、完整的提示範本、更多案例研究等，都可以在匿名連結中公開取得：https://colacare.netlify.app。

##### **Personalized Quantum Federated Learning for Privacy Image Classification**
2410.02547v1 by Jinjing Shi, Tian Chen, Shichao Zhang, Xuelong Li

Quantum federated learning has brought about the improvement of privacy image
classification, while the lack of personality of the client model may
contribute to the suboptimal of quantum federated learning. A personalized
quantum federated learning algorithm for privacy image classification is
proposed to enhance the personality of the client model in the case of an
imbalanced distribution of images. First, a personalized quantum federated
learning model is constructed, in which a personalized layer is set for the
client model to maintain the personalized parameters. Second, a personalized
quantum federated learning algorithm is introduced to secure the information
exchanged between the client and server.Third, the personalized federated
learning is applied to image classification on the FashionMNIST dataset, and
the experimental results indicate that the personalized quantum federated
learning algorithm can obtain global and local models with excellent
performance, even in situations where local training samples are imbalanced.
The server's accuracy is 100% with 8 clients and a distribution parameter of
100, outperforming the non-personalized model by 7%. The average client
accuracy is 2.9% higher than that of the non-personalized model with 2 clients
and a distribution parameter of 1. Compared to previous quantum federated
learning algorithms, the proposed personalized quantum federated learning
algorithm eliminates the need for additional local training while safeguarding
both model and data privacy.It may facilitate broader adoption and application
of quantum technologies, and pave the way for more secure, scalable, and
efficient quantum distribute machine learning solutions.

摘要：<paragraph>量子联邦学习带来了隐私图像分类的改进，而客户端模型缺乏个性化可能会导致量子联邦学习次优。提出了一种针对隐私图像分类的个性化量子联邦学习算法，以增强图像分布不平衡情况下客户端模型的个性化。首先，构建了一个个性化的量子联邦学习模型，其中为客户端模型设置了一个个性化层来维护个性化参数。其次，引入了一种个性化的量子联邦学习算法来保护客户端和服务器之间交换的信息。第三，将个性化的联邦学习应用于 FashionMNIST 数据集上的图像分类，实验结果表明，个性化的量子联邦学习算法可以在局部训练样本不平衡的情况下，获得具有出色性能的全局和局部模型。在 8 个客户端和分布参数为 100 的情况下，服务器的准确率为 100%，比非个性化模型高出 7%。在 2 个客户端和分布参数为 1 的情况下，平均客户端准确率比非个性化模型高 2.9%。与之前的量子联邦学习算法相比，提出的个性化量子联邦学习算法无需额外的本地训练，同时保护模型和数据隐私。它可能促进量子技术的更广泛采用和应用，并为更安全、可扩展且高效的量子分布式机器学习解决方案铺平道路。</paragraph>

##### **MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation**
2410.02458v1 by Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel

Large Language Models (LLMs), known for their versatility in textual data,
are increasingly being explored for their potential to enhance medical image
segmentation, a crucial task for accurate diagnostic imaging. This study
explores enhancing Vision Transformers (ViTs) for medical image segmentation by
integrating pre-trained LLM transformer blocks. Our approach, which
incorporates a frozen LLM transformer block into the encoder of a ViT-based
model, leads to substantial improvements in segmentation performance across
various medical imaging modalities. We propose a Hybrid Attention Mechanism
that combines global and local feature learning with a Multi-Scale Fusion Block
for aggregating features across different scales. The enhanced model shows
significant performance gains, including an average Dice score increase from
0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.
These results demonstrate the effectiveness of LLM-based transformers in
refining medical image segmentation, highlighting their potential to
significantly boost model accuracy and robustness. The source code and our
implementation are available at: https://bit.ly/3zf2CVs

摘要：大型語言模型 (LLM) 以其在文本數據中的多功能性而聞名，
正越來越多地被探索其增強醫學影像分割的潛力，這對於準確的診斷影像至關重要。這項研究
探討了通過整合預先訓練的 LLM transformer 塊來增強用於醫學影像分割的 Vision Transformers (ViT)。我們的做法是，
將凍結的 LLM transformer 塊整合到基於 ViT 的模型的編碼器中，從而大幅提升了各種醫學影像模式的分割效能。我們提出了一種混合注意力機制，
結合全局和局部特徵學習，並使用多尺度融合塊來彙總不同尺度的特徵。增強後的模型顯示出顯著的效能提升，包括 Dice 評分平均從
0.74 提升至 0.79，以及準確度、精確度和 Jaccard 指數的提升。
這些結果證明了基於 LLM 的 transformer 在優化醫學影像分割方面的有效性，突顯了其大幅提升模型準確度和穩健性的潛力。
原始碼和我們的實作可於以下網址取得：https://bit.ly/3zf2CVs

##### **Algorithms For Automatic Accentuation And Transcription Of Russian Texts In Speech Recognition Systems**
2410.02538v1 by Olga Iakovenko, Ivan Bondarenko, Mariya Borovikova, Daniil Vodolazsky

This paper presents an overview of rule-based system for automatic
accentuation and phonemic transcription of Russian texts for speech connected
tasks, such as Automatic Speech Recognition (ASR). Two parts of the developed
system, accentuation and transcription, use different approaches to achieve
correct phonemic representations of input phrases. Accentuation is based on
"Grammatical dictionary of the Russian language" of A.A. Zaliznyak and
wiktionary corpus. To distinguish homographs, the accentuation system also
utilises morphological information of the sentences based on Recurrent Neural
Networks (RNN). Transcription algorithms apply the rules presented in the
monograph of B.M. Lobanov and L.I. Tsirulnik "Computer Synthesis and Voice
Cloning". The rules described in the present paper are implemented in an
open-source module, which can be of use to any scientific study connected to
ASR or Speech To Text (STT) tasks. Automatically marked up text annotations of
the Russian Voxforge database were used as training data for an acoustic model
in CMU Sphinx. The resulting acoustic model was evaluated on cross-validation,
mean Word Accuracy being 71.2%. The developed toolkit is written in the Python
language and is accessible on GitHub for any researcher interested.

摘要：這篇論文概述了基於規則的系統，用於俄語文本的自動重音和音位轉錄，以進行語音連接任務，例如自動語音辨識 (ASR)。已開發系統的兩個部分，重音和轉錄，使用不同的方法來達成輸入語句的正確音位表示。重音基於 A.A. Zaliznyak 的「俄語語法詞典」和維基詞典語料庫。為了區分同形異義詞，重音系統也利用基於遞迴神經網路 (RNN) 的句子的形態資訊。轉錄演算法套用 B.M. Lobanov 和 L.I. Tsirulnik 的專著「電腦合成和語音複製」中提出的規則。本文中描述的規則實作在一個開源模組中，可用於任何與 ASR 或語音轉文字 (STT) 任務相關的科學研究。俄語 Voxforge 資料庫中自動標記的文字註解被用作 CMU Sphinx 中一個聲學模型的訓練資料。結果的聲學模型在交叉驗證中進行評估，平均字詞準確度為 71.2%。已開發的工具包使用 Python 語言撰寫，任何有興趣的研究人員都可以在 GitHub 上取得。

##### **Intelligence at the Edge of Chaos**
2410.02536v1 by Shiyang Zhang, Aakash Patel, Syed A Rizvi, Nianchen Liu, Sizhuang He, Amin Karbasi, Emanuele Zappala, David van Dijk

We explore the emergence of intelligent behavior in artificial systems by
investigating how the complexity of rule-based systems influences the
capabilities of models trained to predict these rules. Our study focuses on
elementary cellular automata (ECA), simple yet powerful one-dimensional systems
that generate behaviors ranging from trivial to highly complex. By training
distinct Large Language Models (LLMs) on different ECAs, we evaluated the
relationship between the complexity of the rules' behavior and the intelligence
exhibited by the LLMs, as reflected in their performance on downstream tasks.
Our findings reveal that rules with higher complexity lead to models exhibiting
greater intelligence, as demonstrated by their performance on reasoning and
chess move prediction tasks. Both uniform and periodic systems, and often also
highly chaotic systems, resulted in poorer downstream performance, highlighting
a sweet spot of complexity conducive to intelligence. We conjecture that
intelligence arises from the ability to predict complexity and that creating
intelligence may require only exposure to complexity.

摘要：我們透過探討基於規則系統的複雜度如何影響訓練來預測這些規則的模型的能力，來探討人工系統中智能行為的出現。我們的研究重點在於基本細胞自動機 (ECA)，這是簡單但強大的單維系統，可以產生從平凡到高度複雜的行為。透過在不同的 ECA 上訓練不同的大型語言模型 (LLM)，我們評估了規則行為的複雜度與 LLM 表現出的智能之間的關係，這反映在它們在下游任務上的表現。我們的發現顯示，複雜度較高的規則會導致模型表現出更高的智能，這從它們在推理和西洋棋走法預測任務上的表現就可以看出來。均勻和週期性系統，以及通常也高度混亂的系統，都導致較差的下游表現，突出了有利於智能的複雜度甜美點。我們推測智能來自於預測複雜性的能力，而創造智能可能只需要接觸複雜性。

##### **A Schema-aware Logic Reformulation for Graph Reachability**
2410.02533v1 by Davide Di Pierro, Stefano Ferilli

Graph reachability is the task of understanding whether two distinct points
in a graph are interconnected by arcs to which in general a semantic is
attached. Reachability has plenty of applications, ranging from motion planning
to routing. Improving reachability requires structural knowledge of relations
so as to avoid the complexity of traditional depth-first and breadth-first
strategies, implemented in logic languages. In some contexts, graphs are
enriched with their schema definitions establishing domain and range for every
arc. The introduction of a schema-aware formalization for guiding the search
may result in a sensitive improvement by cutting out unuseful paths and
prioritising those that, in principle, reach the target earlier. In this work,
we propose a strategy to automatically exclude and sort certain graph paths by
exploiting the higher-level conceptualization of instances. The aim is to
obtain a new first-order logic reformulation of the graph reachability
scenario, capable of improving the traditional algorithms in terms of time,
space requirements, and number of backtracks. The experiments exhibit the
expected advantages of the approach in reducing the number of backtracks during
the search strategy, resulting in saving time and space as well.

摘要：圖形可達性是了解圖形中兩個不同點是否由弧線相互連接的任務，這些弧線通常附帶語義。可達性有很多應用，從運動規劃到路由。提高可達性需要結構關係知識，以避免邏輯語言中實現的傳統深度優先和廣度優先策略的複雜性。在某些情況下，圖形會通過其架構定義得到豐富，為每個弧線建立域和範圍。引入架構感知形式化以指導搜尋可能會通過切斷無用的路徑和優先考慮原則上較早到達目標的路徑而產生顯著的改進。在這項工作中，我們提出了一種策略，通過利用實例的高階概念化來自動排除和排序某些圖形路徑。目的是獲得圖形可達性場景的新一階邏輯重新表述，能夠在時間、空間需求和回溯次數方面改進傳統演算法。實驗展示了該方法在減少搜尋策略期間回溯次數方面的預期優點，從而節省了時間和空間。

##### **Methods for Automatic Matrix Language Determination of Code-Switched Speech**
2410.02521v1 by Olga Iakovenko, Thomas Hain

Code-switching (CS) is the process of speakers interchanging between two or
more languages which in the modern world becomes increasingly common. In order
to better describe CS speech the Matrix Language Frame (MLF) theory introduces
the concept of a Matrix Language, which is the language that provides the
grammatical structure for a CS utterance. In this work the MLF theory was used
to develop systems for Matrix Language Identity (MLID) determination. The MLID
of English/Mandarin and English/Spanish CS text and speech was compared to
acoustic language identity (LID), which is a typical way to identify a language
in monolingual utterances. MLID predictors from audio show higher correlation
with the textual principles than LID in all cases while also outperforming LID
in an MLID recognition task based on F1 macro (60\%) and correlation score
(0.38). This novel approach has identified that non-English languages (Mandarin
and Spanish) are preferred over the English language as the ML contrary to the
monolingual choice of LID.

摘要：語碼轉換 (CS) 是講者在兩種或更多語言之間相互轉換的過程，在現代世界中變得越來越普遍。為了更好地描述 CS 語言，矩陣語言框架 (MLF) 理論引入了矩陣語言的概念，這是為 CS 語句提供語法結構的語言。在這項工作中，MLF 理論被用於開發矩陣語言識別 (MLID) 確定系統。將英語/普通話和英語/西班牙語 CS 文本和語音的 MLID 與聲學語言識別 (LID) 進行了比較，聲學語言識別是識別單語語句中語言的典型方法。在所有情況下，音訊中的 MLID 預測因子與文本文本原則的相關性都比 LID 高，同時在基於 F1 巨集 (60%) 和相關性分數 (0.38) 的 MLID 識別任務中也優於 LID。這種新方法已經確定，與 LID 的單語選擇相反，非英語語言（普通話和西班牙語）被優先於英語作為 ML。

##### **SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation**
2410.02512v1 by Mucong Ding, Bang An, Yuancheng Xu, Anirudh Satheesh, Furong Huang

Data augmentation, a cornerstone technique in deep learning, is crucial in
enhancing model performance, especially with scarce labeled data. While
traditional techniques are effective, their reliance on hand-crafted methods
limits their applicability across diverse data types and tasks. Although modern
learnable augmentation methods offer increased adaptability, they are
computationally expensive and challenging to incorporate within prevalent
augmentation workflows. In this work, we present a novel, efficient method for
data augmentation, effectively bridging the gap between existing augmentation
strategies and emerging datasets and learning tasks. We introduce SAFLEX
(Self-Adaptive Augmentation via Feature Label EXtrapolation), which learns the
sample weights and soft labels of augmented samples provided by any given
upstream augmentation pipeline, using a specifically designed efficient bilevel
optimization algorithm. Remarkably, SAFLEX effectively reduces the noise and
label errors of the upstream augmentation pipeline with a marginal
computational cost. As a versatile module, SAFLEX excels across diverse
datasets, including natural and medical images and tabular data, showcasing its
prowess in few-shot learning and out-of-distribution generalization. SAFLEX
seamlessly integrates with common augmentation strategies like RandAug, CutMix,
and those from large pre-trained generative models like stable diffusion and is
also compatible with frameworks such as CLIP's fine-tuning. Our findings
highlight the potential to adapt existing augmentation pipelines for new data
types and tasks, signaling a move towards more adaptable and resilient training
frameworks.

摘要：資料擴充是深度學習的基石技術，在提升模型效能方面至關重要，特別是在標籤資料稀少的情況下。雖然傳統技術很有效，但它們依賴於手工製作的方法，限制了它們在不同資料類型和任務中的適用性。儘管現代可學習的擴充方法提供了更高的適應性，但它們在計算上很昂貴，並且難以整合到普遍的擴充工作流程中。在這項工作中，我們提出了一種新穎、有效率的資料擴充方法，有效地彌合了現有擴充策略與新興資料集和學習任務之間的差距。我們引入了 SAFLEX（透過特徵標籤外推進行自適應擴充），它使用專門設計的有效雙層次最佳化演算法，學習由任何給定的上游擴充管道提供的擴充樣本的樣本權重和軟標籤。值得注意的是，SAFLEX 有效地降低了上游擴充管道的雜訊和標籤錯誤，且計算成本很低。作為一個多功能模組，SAFLEX 在各種資料集中表現出色，包括自然和醫學影像以及表格資料，展示了它在少樣本學習和分布外概括中的優異能力。SAFLEX 與常見的擴充策略（如 RandAug、CutMix，以及來自大型預訓練生成模型（如穩定擴散）的擴充策略）無縫整合，並且也相容於 CLIP 的微調等框架。我們的研究結果突顯了調整現有擴充管道以適應新資料類型和任務的潛力，標誌著朝向更具適應性和韌性的訓練框架邁進。

##### **Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration**
2410.02511v1 by Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhun Shao, Yixiu Mao, Cheems Wang, Chang Liu, Xiangyang Ji

With expansive state-action spaces, efficient multi-agent exploration remains
a longstanding challenge in reinforcement learning. Although pursuing novelty,
diversity, or uncertainty attracts increasing attention, redundant efforts
brought by exploration without proper guidance choices poses a practical issue
for the community. This paper introduces a systematic approach, termed LEMAE,
choosing to channel informative task-relevant guidance from a knowledgeable
Large Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,
we ground linguistic knowledge from LLM into symbolic key states, that are
critical for task fulfillment, in a discriminative manner at low LLM inference
costs. To unleash the power of key states, we design Subspace-based Hindsight
Intrinsic Reward (SHIR) to guide agents toward key states by increasing reward
density. Additionally, we build the Key State Memory Tree (KSMT) to track
transitions between key states in a specific task for organized exploration.
Benefiting from diminishing redundant explorations, LEMAE outperforms existing
SOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large
margin, achieving a 10x acceleration in certain scenarios.

摘要：在廣泛的狀態動作空間中，有效的多智能體探索仍然是強化學習中一個長期的挑戰。儘管追求新奇性、多樣性或不確定性吸引了越來越多的關注，但探索帶來的重複工作，而沒有適當的指導選擇，對社群來說是一個實際問題。本文介紹了一種系統化方法，稱為 LEMAE，選擇從知識淵博的大語言模型 (LLM) 中引導有用的與任務相關的指導，以進行高效的多智能體探索。具體來說，我們以一種區別性的方式，以低 LLM 推論成本，將來自 LLM 的語言知識基礎化為符號關鍵狀態，這對於任務的完成至關重要。為了釋放關鍵狀態的力量，我們設計了基於子空間的回顧內在獎勵 (SHIR)，通過增加獎勵密度來引導智能體走向關鍵狀態。此外，我們建立了關鍵狀態記憶樹 (KSMT) 來追蹤特定任務中關鍵狀態之間的轉換，以進行有組織的探索。受益於減少重複探索，LEMAE 在具有挑戰性的基準測試（例如 SMAC 和 MPE）上優於現有的 SOTA 方法，在某些場景中實現了 10 倍的加速。

##### **Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration**
2410.02507v1 by Weikang Yuan, Junjie Cao, Zhuoren Jiang, Yangyang Kang, Jun Lin, Kaisong Song, tianqianjin lin, Pengwei Yan, Changlong Sun, Xiaozhong Liu

Large Language Models (LLMs) could struggle to fully understand legal
theories and perform complex legal reasoning tasks. In this study, we introduce
a challenging task (confusing charge prediction) to better evaluate LLMs'
understanding of legal theories and reasoning capabilities. We also propose a
novel framework: Multi-Agent framework for improving complex Legal Reasoning
capability (MALR). MALR employs non-parametric learning, encouraging LLMs to
automatically decompose complex legal tasks and mimic human learning process to
extract insights from legal rules, helping LLMs better understand legal
theories and enhance their legal reasoning abilities. Extensive experiments on
multiple real-world datasets demonstrate that the proposed framework
effectively addresses complex reasoning issues in practical scenarios, paving
the way for more reliable applications in the legal domain.

摘要：大型語言模型 (LLM) 可能難以完全理解法律理論並執行複雜的法律推理任務。在本研究中，我們引入了一項具有挑戰性的任務（混淆指控預測），以更好地評估 LLM 對法律理論和推理能力的理解。我們還提出了一個新的框架：用於改進複雜法律推理能力的多代理框架 (MALR)。MALR 採用非參數學習，鼓勵 LLM 自動分解複雜的法律任務並模擬人類學習過程以從法律規則中提取見解，幫助 LLM 更深入地理解法律理論並增強其法律推理能力。在多個真實世界數據集上進行的廣泛實驗表明，所提出的框架有效地解決了實際場景中的複雜推理問題，為在法律領域中更可靠的應用鋪平了道路。

##### **Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment**
2410.02505v1 by Kai Liu, Ziqing Zhang, Wenbo Li, Renjing Pei, Fenglong Song, Xiaohong Liu, Linghe Kong, Yulun Zhang

Image quality assessment (IQA) serves as the golden standard for all models'
performance in nearly all computer vision fields. However, it still suffers
from poor out-of-distribution generalization ability and expensive training
costs. To address these problems, we propose Dog-IQA, a standard-guided
zero-shot mix-grained IQA method, which is training-free and utilizes the
exceptional prior knowledge of multimodal large language models (MLLMs). To
obtain accurate IQA scores, namely scores consistent with humans, we design an
MLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA
applies two techniques. First, Dog-IQA objectively scores with specific
standards that utilize MLLM's behavior pattern and minimize the influence of
subjective factors. Second, Dog-IQA comprehensively takes local semantic
objects and the whole image as input and aggregates their scores, leveraging
local and global information. Our proposed Dog-IQA achieves state-of-the-art
(SOTA) performance compared with training-free methods, and competitive
performance compared with training-based methods in cross-dataset scenarios.
Our code and models will be available at https://github.com/Kai-Liu001/Dog-IQA.

摘要：影像品質評估 (IQA) 是幾乎所有電腦視覺領域中所有模型效能的黃金標準。然而，它仍然有分佈外概化能力差和訓練成本高的問題。為了解決這些問題，我們提出 Dog-IQA，一種標準引導的零次學習混合粒度 IQA 方法，它無需訓練，並利用多模態大型語言模型 (MLLM) 的特殊先驗知識。為了獲得準確的 IQA 分數，也就是與人類一致的分數，我們設計了一個基於 MLLM 的推論管線，模擬人類專家。詳細來說，Dog-IQA 應用兩種技術。首先，Dog-IQA 利用 MLLM 的行為模式，並將主觀因素的影響降至最低，以特定標準進行客觀評分。其次，Dog-IQA 全面地將局部語義物件和整張影像作為輸入，並匯總其分數，利用局部和全域資訊。我們提出的 Dog-IQA 在與無訓練方法相比時，達到了最先進 (SOTA) 的效能，而在跨資料集場景中，與基於訓練的方法相比，也具有競爭力的效能。我們的程式碼和模型將在 https://github.com/Kai-Liu001/Dog-IQA 中提供。

##### **Mixed-Session Conversation with Egocentric Memory**
2410.02503v1 by Jihyoung Jang, Taeyoung Kim, Hyounghun Kim

Recently introduced dialogue systems have demonstrated high usability.
However, they still fall short of reflecting real-world conversation scenarios.
Current dialogue systems exhibit an inability to replicate the dynamic,
continuous, long-term interactions involving multiple partners. This shortfall
arises because there have been limited efforts to account for both aspects of
real-world dialogues: deeply layered interactions over the long-term dialogue
and widely expanded conversation networks involving multiple participants. As
the effort to incorporate these aspects combined, we introduce Mixed-Session
Conversation, a dialogue system designed to construct conversations with
various partners in a multi-session dialogue setup. We propose a new dataset
called MiSC to implement this system. The dialogue episodes of MiSC consist of
6 consecutive sessions, with four speakers (one main speaker and three
partners) appearing in each episode. Also, we propose a new dialogue model with
a novel memory management mechanism, called Egocentric Memory Enhanced
Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories
from the main speaker's perspective during conversations with partners,
enabling seamless continuity in subsequent interactions. Extensive human
evaluations validate that the dialogues in MiSC demonstrate a seamless
conversational flow, even when conversation partners change in each session.
EMMA trained with MiSC is also evaluated to maintain high memorability without
contradiction throughout the entire conversation.

摘要：最近推出的对话系统已展示出极高的可用性。
然而，它们仍无法反映现实世界的对话场景。
当前的对话系统无法复制涉及多个参与者的动态、连续、长期的交互。这种不足的原因是，人们在考虑现实世界对话的两个方面时付出的努力有限：长期对话中的深度分层交互和涉及多个参与者的广泛扩展的对话网络。随着将这些方面结合起来的努力，我们引入了混合会话对话，这是一种对话系统，旨在在多会话对话设置中与各种参与者构建对话。我们提出了一个名为 MiSC 的新数据集来实现此系统。MiSC 的对话片段由 6 个连续会话组成，每个片段中出现四位发言者（一位主发言者和三位参与者）。此外，我们提出了一个具有新颖内存管理机制的新对话模型，称为自我中心记忆增强混合会话对话代理（EMMA）。EMMA 在与参与者的对话中收集并保留来自主发言者视角的记忆，从而在后续交互中实现无缝连续性。广泛的人类评估验证了 MiSC 中的对话展示了无缝的对话流程，即使每个会话中的对话参与者发生了变化。使用 MiSC 训练的 EMMA 也被评估为在整个对话中保持较高的记忆力，且没有矛盾。

##### **Defining Knowledge: Bridging Epistemology and Large Language Models**
2410.02499v1 by Constanza Fierro, Ruchira Dhar, Filippos Stamatiou, Nicolas Garneau, Anders Søgaard

Knowledge claims are abundant in the literature on large language models
(LLMs); but can we say that GPT-4 truly "knows" the Earth is round? To address
this question, we review standard definitions of knowledge in epistemology and
we formalize interpretations applicable to LLMs. In doing so, we identify
inconsistencies and gaps in how current NLP research conceptualizes knowledge
with respect to epistemological frameworks. Additionally, we conduct a survey
of 100 professional philosophers and computer scientists to compare their
preferences in knowledge definitions and their views on whether LLMs can really
be said to know. Finally, we suggest evaluation protocols for testing knowledge
in accordance to the most relevant definitions.

摘要：在大型語言模型 (LLM) 的文獻中，知識主張很豐富；但我們能說 GPT-4 真正「知道」地球是圓的嗎？為了回答這個問題，我們回顧了認識論中知識的標準定義，並將適用於 LLM 的詮釋形式化。在這樣做的過程中，我們辨識出當前 NLP 研究在如何概念化知識方面與認識論架構之間的不一致和差距。此外，我們對 100 位專業哲學家和電腦科學家進行了一項調查，以比較他們在知識定義上的偏好，以及他們對 LLM 是否真的可以說是知道的想法。最後，我們建議根據最相關的定義來測試知識的評估協定。

##### **Dynamic Gradient Alignment for Online Data Mixing**
2410.02498v1 by Simin Fan, David Grangier, Pierre Ablin

The composition of training data mixtures is critical for effectively
training large language models (LLMs), as it directly impacts their performance
on downstream tasks. Our goal is to identify an optimal data mixture to
specialize an LLM for a specific task with access to only a few examples.
Traditional approaches to this problem include ad-hoc reweighting methods,
importance sampling, and gradient alignment techniques. This paper focuses on
gradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable
online gradient alignment algorithm. DGA dynamically estimates the pre-training
data mixture on which the models' gradients align as well as possible with
those of the model on the specific task. DGA is the first gradient alignment
approach that incurs minimal overhead compared to standard pre-training and
outputs a competitive model, eliminating the need for retraining the model.
Experimentally, we demonstrate significant improvements over importance
sampling in two key scenarios: (i) when the pre-training set is small and
importance sampling overfits due to limited data; and (ii) when there is
insufficient specialized data, trapping importance sampling on narrow pockets
of data. Our findings underscore the effectiveness of gradient alignment
methods in optimizing training data mixtures, particularly in data-constrained
environments, and offer a practical solution for enhancing LLM performance on
specific tasks with limited data availability.

摘要：訓練資料混合物的組成對於有效訓練大型語言模型 (LLM) 至關重要，因為它直接影響其在下游任務的表現。我們的目標是找出最佳資料混合物，以使用特定任務的少數範例，針對特定任務專門化 LLM。解決此問題的傳統方法包括臨時重新加權方法、重要性取樣和梯度對齊技術。本文重點在梯度對齊，並介紹動態梯度對齊 (DGA)，一種可擴充的線上梯度對齊演算法。DGA 動態估計預訓練資料混合物，模型的梯度與特定任務模型的梯度在此混合物上對齊得最好。DGA 是第一個梯度對齊方法，與標準預訓練相比，產生的開銷最小，且輸出具競爭力的模型，消除了重新訓練模型的需要。透過實驗，我們展示了在兩個關鍵情境中，相較於重要性取樣，有顯著的進步：(i) 當預訓練集很小，且重要性取樣因資料有限而過度擬合時；(ii) 當沒有足夠的專業化資料時，將重要性取樣限制在狹窄的資料區塊中。我們的研究結果強調了梯度對齊方法在最佳化訓練資料混合物上的有效性，特別是在資料受限的環境中，並為在資料可用性有限的情況下，提升特定任務的 LLM 表現，提供了實用的解決方案。

##### **DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM**
2410.02492v1 by Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang

Visual language tracking (VLT) has emerged as a cutting-edge research area,
harnessing linguistic data to enhance algorithms with multi-modal inputs and
broadening the scope of traditional single object tracking (SOT) to encompass
video understanding applications. Despite this, most VLT benchmarks still
depend on succinct, human-annotated text descriptions for each video. These
descriptions often fall short in capturing the nuances of video content
dynamics and lack stylistic variety in language, constrained by their uniform
level of detail and a fixed annotation frequency. As a result, algorithms tend
to default to a "memorize the answer" strategy, diverging from the core
objective of achieving a deeper understanding of video content. Fortunately,
the emergence of large language models (LLMs) has enabled the generation of
diverse text. This work utilizes LLMs to generate varied semantic annotations
(in terms of text lengths and granularities) for representative SOT benchmarks,
thereby establishing a novel multi-modal benchmark. Specifically, we (1)
propose a new visual language tracking benchmark with diverse texts, named
DTVLT, based on five prominent VLT and SOT benchmarks, including three
sub-tasks: short-term tracking, long-term tracking, and global instance
tracking. (2) We offer four granularity texts in our benchmark, considering the
extent and density of semantic information. We expect this multi-granular
generation strategy to foster a favorable environment for VLT and video
understanding research. (3) We conduct comprehensive experimental analyses on
DTVLT, evaluating the impact of diverse text on tracking performance and hope
the identified performance bottlenecks of existing algorithms can support
further research in VLT and video understanding. The proposed benchmark,
experimental results and toolkit will be released gradually on
http://videocube.aitestunion.com/.

摘要：視覺語言追蹤 (VLT) 已成為尖端的的研究領域，
利用語言資料來增強帶有多模式輸入的演算法，並擴展傳統單一物件追蹤 (SOT) 的範圍，以涵蓋影片理解應用。儘管如此，大多數 VLT 評量基準仍然依賴每個影片的簡潔、人工標註的文字描述。這些描述通常無法捕捉影片內容動態的細微差別，且缺乏語言風格的多樣性，受到其均勻的詳細程度和固定的標註頻率所限制。因此，演算法傾向於預設為「記住答案」策略，偏離了深入理解影片內容的核心目標。幸運的是，大型語言模型 (LLM) 的出現使得能夠產生多樣化的文字。這項工作利用 LLM 為具代表性的 SOT 評量基準產生多樣的語義標註（就文字長度和粒度而言），從而建立一個新穎的多模式評量基準。具體來說，我們 (1) 提出一個新的具有多樣化文字的視覺語言追蹤評量基準，稱為 DTVLT，基於五個著名的 VLT 和 SOT 評量基準，包括三個子任務：短期追蹤、長期追蹤和全局實例追蹤。(2) 我們在評量基準中提供了四個粒度文字，考慮了語義資訊的範圍和密度。我們預期這種多粒度生成策略將為 VLT 和影片理解研究營造一個有利的環境。(3) 我們對 DTVLT 進行了全面的實驗分析，評估了多樣化文字對追蹤效能的影響，並希望現有演算法識別出的效能瓶頸能夠支持 VLT 和影片理解的進一步研究。建議的評量基準、實驗結果和工具包將逐步在 http://videocube.aitestunion.com/ 上發布。

##### **Meta-Models: An Architecture for Decoding LLM Behaviors Through Interpreted Embeddings and Natural Language**
2410.02472v1 by Anthony Costarelli, Mat Allen, Severin Field, Joshua Clymer

As Large Language Models (LLMs) become increasingly integrated into our daily
lives, the potential harms from deceptive behavior underlie the need for
faithfully interpreting their decision-making. While traditional probing
methods have shown some effectiveness, they remain best for narrowly scoped
tasks while more comprehensive explanations are still necessary. To this end,
we investigate meta-models-an architecture using a "meta-model" that takes
activations from an "input-model" and answers natural language questions about
the input-model's behaviors. We evaluate the meta-model's ability to generalize
by training them on selected task types and assessing their out-of-distribution
performance in deceptive scenarios. Our findings show that meta-models
generalize well to out-of-distribution tasks and point towards opportunities
for future research in this area.

摘要：隨著大型語言模型 (LLM) 愈來愈融入我們的日常生活，欺騙行為的潛在危害突顯了忠實詮釋其決策的必要性。雖然傳統的探測方法已展現一定成效，但它們仍然最適合範圍狹窄的任務，而更全面的解釋仍然有必要。為此，我們研究了元模型——一種使用「元模型」的架構，它擷取「輸入模型」的活化，並回答有關輸入模型行為的自然語言問題。我們評估元模型在不同任務類型上訓練後，其概化能力，並評估其在欺騙情境中超出分佈的表現。我們的研究結果顯示，元模型能很好地概化到超出分佈的任務，並指出未來在這個領域進行研究的機會。

##### **Response Tuning: Aligning Large Language Models without Instruction**
2410.02465v1 by Seokhyun An, Hyounghun Kim

Instruction tuning-supervised fine-tuning using instruction-response pairs-is
a foundational step in transitioning pre-trained Large Language Models (LLMs)
into helpful and safe chat assistants. Our hypothesis is that establishing an
adequate output space can enable such a transition given the capabilities
inherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),
which eliminates the instruction-conditioning step in instruction tuning and
solely focuses on response space supervision. Our experiments demonstrate that
RT models, trained only using responses, can effectively respond to a wide
range of instructions and exhibit helpfulness comparable to that of their
instruction-tuned counterparts. Furthermore, we observe that controlling the
training response distribution can significantly improve their user preference
or elicit target behaviors such as refusing assistance for unsafe queries. Our
findings illuminate the role of establishing an adequate output space in
alignment, highlighting the potential of the extensive inherent capabilities of
pre-trained LLMs.

摘要：指令微調——使用指令-回應配對進行監督式微調——是將預先訓練好的大型語言模型 (LLM) 轉變為有用且安全的聊天助理的基本步驟。我們的假設是，建立一個足夠的輸出空間可以讓這樣的轉換成為可能，因為預先訓練好的 LLM 具有內在的能力。為了驗證這一點，我們提出了回應微調 (RT)，它消除了指令微調中的指令制約步驟，並僅專注於回應空間監督。我們的實驗表明，僅使用回應訓練的 RT 模型可以有效地回應各種指令，並表現出與指令微調對應模型相當的樂於助人。此外，我們觀察到控制訓練回應分佈可以顯著提高使用者的偏好，或引發目標行為，例如拒絕對不安全的查詢提供協助。我們的發現闡明了在對齊中建立足夠輸出空間的作用，突出了預先訓練好的 LLM 廣泛內在能力的潛力。

##### **Recurrent Few-Shot model for Document Verification**
2410.02456v1 by Maxime Talarmain, Carlos Boned, Sanket Biswas, Oriol Ramos

General-purpose ID, or travel, document image- and video-based verification
systems have yet to achieve good enough performance to be considered a solved
problem. There are several factors that negatively impact their performance,
including low-resolution images and videos and a lack of sufficient data to
train the models. This task is particularly challenging when dealing with
unseen class of ID, or travel, documents. In this paper we address this task by
proposing a recurrent-based model able to detect forged documents in a few-shot
scenario. The recurrent architecture makes the model robust to document
resolution variability. Moreover, the few-shot approach allow the model to
perform well even for unseen class of documents. Preliminary results on the
SIDTD and Findit datasets show good performance of this model for this task.

摘要：通用身分證或旅遊證件影像及影片驗證系統尚未達到足夠好的效能，以被視為已解決的問題。有幾個因素對其效能造成負面影響，包括低解析度的影像和影片，以及用於訓練模型的資料不足。在處理未見類別的身分證或旅遊證件時，此任務特別具有挑戰性。在本文中，我們提出一個基於遞迴的模型來解決這項任務，此模型能夠在少量樣本的場景中偵測偽造文件。遞迴架構使模型對文件解析度變異具有穩健性。此外，少量樣本的方法讓模型即使在未見類別的文件中也能表現良好。SIDTD 和 Findit 資料集的初步結果顯示此模型在此任務中具有良好的效能。

##### **Strong Preferences Affect the Robustness of Value Alignment**
2410.02451v1 by Ziwei Xu, Mohan Kankanhalli

Value alignment, which aims to ensure that large language models (LLMs) and
other AI agents behave in accordance with human values, is critical for
ensuring safety and trustworthiness of these systems. A key component of value
alignment is the modeling of human preferences as a representation of human
values. In this paper, we investigate the robustness of value alignment by
examining the sensitivity of preference models. Specifically, we ask: how do
changes in the probabilities of some preferences affect the predictions of
these models for other preferences? To answer this question, we theoretically
analyze the robustness of widely used preference models by examining their
sensitivities to minor changes in preferences they model. Our findings reveal
that, in the Bradley-Terry and the Placket-Luce model, the probability of a
preference can change significantly as other preferences change, especially
when these preferences are dominant (i.e., with probabilities near 0 or 1). We
identify specific conditions where this sensitivity becomes significant for
these models and discuss the practical implications for the robustness and
safety of value alignment in AI systems.

摘要：價值對齊旨在確保大型語言模型 (LLM) 和其他 AI 代理根據人類價值觀行事，這對於確保這些系統的安全性和可信度至關重要。價值對齊的一個關鍵組成部分是將人類偏好建模為人類價值觀的表現。在本文中，我們透過檢驗偏好模型的敏感性來探討價值對齊的穩健性。具體來說，我們詢問：某些偏好機率的變化如何影響這些模型對其他偏好的預測？為了回答這個問題，我們透過檢驗廣泛使用的偏好模型對其所建模偏好中的微小變化的敏感性，從理論上分析其穩健性。我們的研究結果揭示，在 Bradley-Terry 和 Placket-Luce 模型中，隨著其他偏好的改變，偏好的機率可能會發生顯著變化，特別是在這些偏好佔優勢時（即機率接近 0 或 1）。我們找出這些模型的敏感性在哪些特定條件下變得顯著，並討論對 AI 系統中價值對齊的穩健性和安全性造成的實際影響。

##### **Clinnova Federated Learning Proof of Concept: Key Takeaways from a Cross-border Collaboration**
2410.02443v1 by Julia Alekseenko, Bram Stieltjes, Michael Bach, Melanie Boerries, Oliver Opitz, Alexandros Karargyris, Nicolas Padoy

Clinnova, a collaborative initiative involving France, Germany, Switzerland,
and Luxembourg, is dedicated to unlocking the power of precision medicine
through data federation, standardization, and interoperability. This European
Greater Region initiative seeks to create an interoperable European standard
using artificial intelligence (AI) and data science to enhance healthcare
outcomes and efficiency. Key components include multidisciplinary research
centers, a federated biobanking strategy, a digital health innovation platform,
and a federated AI strategy. It targets inflammatory bowel disease, rheumatoid
diseases, and multiple sclerosis (MS), emphasizing data quality to develop AI
algorithms for personalized treatment and translational research.
  The IHU Strasbourg (Institute of Minimal-invasive Surgery) has the lead in
this initiative to develop the federated learning (FL) proof of concept (POC)
that will serve as a foundation for advancing AI in healthcare. At its core,
Clinnova-MS aims to enhance MS patient care by using FL to develop more
accurate models that detect disease progression, guide interventions, and
validate digital biomarkers across multiple sites. This technical report
presents insights and key takeaways from the first cross-border federated POC
on MS segmentation of MRI images within the Clinnova framework. While our work
marks a significant milestone in advancing MS segmentation through cross-border
collaboration, it also underscores the importance of addressing technical,
logistical, and ethical considerations to realize the full potential of FL in
healthcare settings.

摘要：Clinnova 是一項由法國、德國、瑞士和盧森堡合作發起的計畫，致力於透過資料聯合、標準化和互通性來釋放精準醫療的力量。這個歐洲大區計畫旨在使用人工智慧 (AI) 和資料科學建立一個可互通的歐洲標準，以提升醫療保健成果和效率。主要組成部分包括跨領域研究中心、聯合生物銀行策略、數位健康創新平台和聯合 AI 策略。它針對發炎性腸道疾病、類風濕性疾病和多發性硬化症 (MS) 進行研究，強調資料品質以開發 AI 演算法，用於個人化治療和轉譯研究。
史特拉斯堡 IHU（微創手術研究所）在這個計畫中領先開發聯合學習 (FL) 概念驗證 (POC)，這將作為在醫療保健中推進 AI 的基礎。Clinnova-MS 的核心目標是透過使用 FL 來提升 MS 患者照護，以開發更精確的模型來偵測疾病進程、引導介入措施，並驗證多個地點的數位生物標記。這份技術報告提供了 Clinnova 架構內 MS 磁振造影影像分割首次跨境聯合 POC 的見解和主要結論。雖然我們的成果在透過跨境合作推進 MS 分割方面是一個重要的里程碑，但也強調了在醫療保健環境中實現 FL 的全部潛力時，解決技術、後勤和倫理考量的必要性。

##### **Embedded Topic Models Enhanced by Wikification**
2410.02441v1 by Takashi Shibuya, Takehito Utsuro

Topic modeling analyzes a collection of documents to learn meaningful
patterns of words. However, previous topic models consider only the spelling of
words and do not take into consideration the homography of words. In this
study, we incorporate the Wikipedia knowledge into a neural topic model to make
it aware of named entities. We evaluate our method on two datasets, 1) news
articles of \textit{New York Times} and 2) the AIDA-CoNLL dataset. Our
experiments show that our method improves the performance of neural topic
models in generalizability. Moreover, we analyze frequent terms in each topic
and the temporal dependencies between topics to demonstrate that our
entity-aware topic models can capture the time-series development of topics
well.

摘要：主題模型分析文件集合以學習有意義的詞彙模式。然而，先前的主題模型只考慮詞彙的拼寫，而沒有考慮詞彙的同形異義。在本研究中，我們將維基百科知識納入神經主題模型，使其認識命名實體。我們在兩個資料集上評估我們的模型，1) \textit{紐約時報} 的新聞文章和 2) AIDA-CoNLL 資料集。我們的實驗表明，我們的模型改善了神經主題模型在概括性方面的效能。此外，我們分析每個主題中的頻繁詞彙和主題之間的時間依賴性，以證明我們的實體感知主題模型可以很好地捕捉主題的時間序列發展。

##### **Optimizing Adaptive Attacks against Content Watermarks for Language Models**
2410.02440v1 by Abdulrahman Diaa, Toluwani Aremu, Nils Lukas

Large Language Models (LLMs) can be \emph{misused} to spread online spam and
misinformation. Content watermarking deters misuse by hiding a message in
model-generated outputs, enabling their detection using a secret watermarking
key. Robustness is a core security property, stating that evading detection
requires (significant) degradation of the content's quality. Many LLM
watermarking methods have been proposed, but robustness is tested only against
\emph{non-adaptive} attackers who lack knowledge of the watermarking method and
can find only suboptimal attacks. We formulate the robustness of LLM
watermarking as an objective function and propose preference-based optimization
to tune \emph{adaptive} attacks against the specific watermarking method. Our
evaluation shows that (i) adaptive attacks substantially outperform
non-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks
optimized against a few known watermarks remain highly effective when tested
against other unseen watermarks, and (iii) optimization-based attacks are
practical and require less than seven GPU hours. Our findings underscore the
need to test robustness against adaptive attackers.

摘要：大型語言模型 (LLM) 可能被惡意利用來散播線上垃圾郵件和錯誤訊息。內容浮水印透過在模型產生的輸出中隱藏訊息來阻止惡意使用，並使用秘密浮水印金鑰來偵測它們。穩健性是核心安全屬性，表示規避偵測需要（大幅）降低內容品質。已經提出許多 LLM 浮水印方法，但穩健性僅針對缺乏浮水印方法知識且只能找到次佳攻擊的非適應性攻擊者進行測試。我們將 LLM 浮水印的穩健性制定為目標函數，並提出基於偏好的最佳化來調整針對特定浮水印方法的適應性攻擊。我們的評估顯示：(i) 適應性攻擊大幅優於非適應性基準。(ii) 即使在非適應性設定中，針對已知浮水印進行最佳化的適應性攻擊在針對其他未見浮水印時仍然非常有效，而且 (iii) 基於最佳化的攻擊具有實用性，且所需 GPU 小時數不到 7 小時。我們的研究結果強調需要針對適應性攻擊者測試穩健性。

##### **Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization**
2410.02433v1 by Mingyang Wang, Lukas Lange, Heike Adel, Jannik Strötgen, Hinrich Schütze

To ensure large language models contain up-to-date knowledge, they need to be
updated regularly. However, model editing is challenging as it might also
affect knowledge that is unrelated to the new data. State-of-the-art methods
identify parameters associated with specific knowledge and then modify them via
direct weight updates. However, these locate-and-edit methods suffer from heavy
computational overhead and lack theoretical validation. In contrast, directly
fine-tuning the model on requested edits affects the model's behavior on
unrelated knowledge, and significantly damages the model's generation fluency
and consistency. To address these challenges, we propose SAUL, a streamlined
model editing method that uses sentence concatenation with augmented random
facts for generation regularization. Evaluations on three model editing
benchmarks show that SAUL is a practical and reliable solution for model
editing outperforming state-of-the-art methods while maintaining generation
quality and reducing computational overhead.

摘要：為了確保大型語言模型包含最新的知識，它們需要定期更新。然而，模型編輯具有挑戰性，因為它也可能影響與新數據無關的知識。最先進的方法識別與特定知識相關的參數，然後通過直接權重更新對其進行修改。然而，這些定位和編輯方法存在計算開銷大且缺乏理論驗證的問題。相比之下，直接微調請求編輯的模型會影響模型在不相關知識上的行為，並顯著損害模型的生成流暢性和一致性。為了應對這些挑戰，我們提出了 SAUL，這是一種簡化的模型編輯方法，它使用句子連接和增強的隨機事實進行生成正則化。在三個模型編輯基準上的評估表明，SAUL 是一種實用且可靠的模型編輯解決方案，在保持生成質量的同時優於最先進的方法，並降低了計算開銷。

##### **Predictive Attractor Models**
2410.02430v1 by Ramy Mounir, Sudeep Sarkar

Sequential memory, the ability to form and accurately recall a sequence of
events or stimuli in the correct order, is a fundamental prerequisite for
biological and artificial intelligence as it underpins numerous cognitive
functions (e.g., language comprehension, planning, episodic memory formation,
etc.) However, existing methods of sequential memory suffer from catastrophic
forgetting, limited capacity, slow iterative learning procedures, low-order
Markov memory, and, most importantly, the inability to represent and generate
multiple valid future possibilities stemming from the same context. Inspired by
biologically plausible neuroscience theories of cognition, we propose
\textit{Predictive Attractor Models (PAM)}, a novel sequence memory
architecture with desirable generative properties. PAM is a streaming model
that learns a sequence in an online, continuous manner by observing each input
\textit{only once}. Additionally, we find that PAM avoids catastrophic
forgetting by uniquely representing past context through lateral inhibition in
cortical minicolumns, which prevents new memories from overwriting previously
learned knowledge. PAM generates future predictions by sampling from a union
set of predicted possibilities; this generative ability is realized through an
attractor model trained alongside the predictor. We show that PAM is trained
with local computations through Hebbian plasticity rules in a biologically
plausible framework. Other desirable traits (e.g., noise tolerance, CPU-based
learning, capacity scaling) are discussed throughout the paper. Our findings
suggest that PAM represents a significant step forward in the pursuit of
biologically plausible and computationally efficient sequential memory models,
with broad implications for cognitive science and artificial intelligence
research.

摘要：序列記憶，即形成並準確回憶事件或刺激序列的正確順序的能力，是生物和人工智能的基本先決條件，因為它支撐了許多認知功能（例如，語言理解、規劃、情節記憶形成等）。然而，現有的序列記憶方法存在災難性遺忘、容量有限、緩慢的迭代學習程序、低階馬可夫記憶，最重要的是，無法表示和生成源自同一背景的未來多種有效可能性。受生物學上合理的認知神經科學理論啟發，我們提出了預測吸引子模型 (PAM)，一種具有理想生成特性的新序列記憶架構。PAM 是一個串流模型，它通過以線上、連續的方式觀察每個輸入來學習序列，而且僅觀察「一次」。此外，我們發現 PAM 透過皮質微柱中的側向抑制來唯一表示過去的背景，從而避免了災難性遺忘，這可以防止新的記憶覆蓋先前學習的知識。PAM 透過從預測可能性的聯集進行抽樣來生成未來的預測；這種生成能力是透過與預測器一起訓練的吸引子模型來實現的。我們展示了 PAM 是透過生物學上合理的框架中的赫布塑性規則，以局部運算進行訓練的。在本文中討論了其他理想的特質（例如，耐噪性、基於 CPU 的學習、容量擴充）。我們的研究結果表明，PAM 在追求生物學上合理且計算效率高的序列記憶模型方面邁出了一大步，對認知科學和人工智能研究具有廣泛的影響。

##### **IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models**
2410.02429v2 by Tuo An, Yunjiao Zhou, Han Zou, Jianfei Yang

Large Language Models (LLMs) have demonstrated remarkable capabilities across
textual and visual domains but often generate outputs that violate physical
laws, revealing a gap in their understanding of the physical world. Inspired by
human cognition, where perception is fundamental to reasoning, we explore
augmenting LLMs with enhanced perception abilities using Internet of Things
(IoT) sensor data and pertinent knowledge for IoT task reasoning in the
physical world. In this work, we systematically study LLMs capability to
address real-world IoT tasks by augmenting their perception and knowledge base,
and then propose a unified framework, IoT-LLM, to enhance such capability. In
IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats
amenable to LLMs, activating their commonsense knowledge through
chain-of-thought prompting and specialized role definitions, and expanding
their understanding via IoT-oriented retrieval-augmented generation based on
in-context learning. To evaluate the performance, We design a new benchmark
with five real-world IoT tasks with different data types and reasoning
difficulties and provide the benchmarking results on six open-source and
close-source LLMs. Experimental results demonstrate the limitations of existing
LLMs with naive textual inputs that cannot perform these tasks effectively. We
show that IoT-LLM significantly enhances the performance of IoT tasks reasoning
of LLM, such as GPT-4, achieving an average improvement of 65% across various
tasks against previous methods. The results also showcase LLMs ability to
comprehend IoT data and the physical law behind data by providing a reasoning
process. Limitations of our work are claimed to inspire future research in this
new era.

摘要：大型語言模型 (LLM) 已在文本和視覺領域展示出非凡的能力，但經常會產生違反物理定律的輸出，這揭示了它們對物理世界的理解存在差距。受人類認知的啟發，感知是推理的基礎，我們探討利用物聯網 (IoT) 感測器資料和與 IoT 任務推理相關的知識，來增強 LLM 的感知能力，以應對物理世界。在這項工作中，我們系統性地研究 LLM 的能力，透過增強它們的感知和知識庫來解決真實世界的 IoT 任務，並提出一個統一的架構 IoT-LLM 來增強這種能力。在 IoT-LLM 中，我們為 LLM 客製化三個步驟：將 IoT 資料預處理成 LLM 適用的格式、透過思考鏈提示和專門的角色定義來啟動它們的常識知識，以及透過基於情境學習的 IoT 導向檢索增強生成來擴展它們的理解。為了評估效能，我們設計了一個新的基準，包含五個具有不同資料類型和推理難度的真實世界 IoT 任務，並在六個開源和閉源 LLM 上提供基準測試結果。實驗結果證明了現有 LLM 使用天真的文字輸入的限制，無法有效執行這些任務。我們展示了 IoT-LLM 大幅提升了 LLM 對 IoT 任務推理的效能，例如 GPT-4，在各種任務中相較於先前的技術，平均提升了 65%。結果也展示了 LLM 透過提供推理過程來理解 IoT 資料和資料背後的物理定律的能力。我們工作的限制被認為能激勵這個新時代的未來研究。

##### **Collective Critics for Creative Story Generation**
2410.02428v1 by Minwook Bae, Hyounghun Kim

Generating a long story of several thousand words with narrative coherence
using Large Language Models (LLMs) has been a challenging task. Previous
research has addressed this challenge by proposing different frameworks that
create a story plan and generate a long story based on that plan. However,
these frameworks have been mainly focusing on maintaining narrative coherence
in stories, often overlooking creativity in story planning and the
expressiveness of the stories generated from those plans, which are desirable
properties to captivate readers' interest. In this paper, we propose Collective
Critics for Creative Story Generation framework (CritiCS), which is composed of
plan refining stage (CrPlan) and story generation stage (CrText), to integrate
a collective revision mechanism that promotes those properties into long-form
story generation process. Specifically, in each stage, a group of LLM critics
and one leader collaborate to incrementally refine drafts of plan and story
throughout multiple rounds. Extensive human evaluation shows that the CritiCS
can significantly enhance story creativity and reader engagement, while also
maintaining narrative coherence. Furthermore, the design of the framework
allows active participation from human writers in any role within the critique
process, enabling interactive human-machine collaboration in story writing.

摘要：使用大型語言模型 (LLM) 生成具有敘事連貫性的數千字長篇故事一直是一項具有挑戰性的任務。先前的研究通過提出不同的框架來解決這個挑戰，這些框架會建立故事大綱並根據該大綱生成長篇故事。然而，這些框架主要專注於維持故事中的敘事連貫性，常常忽略故事規劃中的創造力和從這些大綱中生成的故事的表現力，而這些都是吸引讀者興趣的理想特質。在本文中，我們提出用於創意故事生成框架的集體評論 (CritiCS)，它由大綱精煉階段 (CrPlan) 和故事生成階段 (CrText) 組成，以整合一種集體修改機制，將這些特質融入長篇故事生成過程中。具體來說，在每個階段，一群 LLM 評論家和一位領導者合作，在多輪過程中逐步精煉大綱和故事草稿。廣泛的人類評估表明，CritiCS 可以顯著提升故事的創造力和讀者參與度，同時還能維持敘事連貫性。此外，該框架的設計允許人類作家積極參與評論過程中的任何角色，從而實現故事寫作中的人機互動協作。

##### **Learning the Latent Rules of a Game from Data: A Chess Story**
2410.02426v1 by Ben Fauber

We demonstrate that small pretrained foundational generative language models
with millions of parameters can learn the latent rules of a process from data
associated with the process. Inspired by Stefan Zweig's novella
"Schachnovelle," also known as "The Royal Game" in English, we show that 28M
and 125M parameter pretrained foundational small language models (SLMs) can be
instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of
chess, propose legal moves, and accurately solve chess problems. We also
explore the impact of successive language model fine-tuning epochs on improved
outcomes and demonstrate reductions in model hallucinations by increasing the
number of instruction fine-tuning examples.

摘要：我們展示了數百萬個參數的小型預訓練基礎生成語言模型可以從與流程相關的數據中學習流程的潛在規則。受 Stefan Zweig 的中篇小說「象棋小說」啟發，在英文中也稱為「皇家遊戲」，我們展示了 28M 和 125M 參數預訓練基礎小型語言模型 (SLM) 可以用 1,000 到 1,000,000 個範例進行微調，以學習西洋棋規則、建議合法移動，並準確解決西洋棋問題。我們還探討了連續語言模型微調時期對改善結果的影響，並展示了通過增加指令微調範例數量來減少模型幻覺。

##### **LLM-Pilot: Characterize and Optimize Performance of your LLM Inference Services**
2410.02425v1 by Małgorzata Łazuka, Andreea Anghel, Thomas Parnell

As Large Language Models (LLMs) are rapidly growing in popularity, LLM
inference services must be able to serve requests from thousands of users while
satisfying performance requirements. The performance of an LLM inference
service is largely determined by the hardware onto which it is deployed, but
understanding of which hardware will deliver on performance requirements
remains challenging. In this work we present LLM-Pilot - a first-of-its-kind
system for characterizing and predicting performance of LLM inference services.
LLM-Pilot performs benchmarking of LLM inference services, under a realistic
workload, across a variety of GPUs, and optimizes the service configuration for
each considered GPU to maximize performance. Finally, using this
characterization data, LLM-Pilot learns a predictive model, which can be used
to recommend the most cost-effective hardware for a previously unseen LLM.
Compared to existing methods, LLM-Pilot can deliver on performance requirements
33% more frequently, whilst reducing costs by 60% on average.

摘要：隨著大型語言模型 (LLM) 迅速普及，LLM 推論服務必須能夠滿足效能需求，同時處理來自數千名使用者的請求。LLM 推論服務的效能很大程度上取決於其所部署的硬體，但要了解哪種硬體能夠滿足效能需求仍然是一項挑戰。在這項工作中，我們提出了 LLM-Pilot，這是一個首創的系統，用於描述和預測 LLM 推論服務的效能。LLM-Pilot 在實際的工作負載下對 LLM 推論服務進行基準測試，並針對每種考量的 GPU 最佳化服務設定，以最大化效能。最後，LLM-Pilot 使用此描述資料，學習一個預測模型，可用於建議最具成本效益的硬體，以供先前未見的 LLM 使用。與現有方法相比，LLM-Pilot 能夠更頻繁地滿足效能需求，同時平均降低 60% 的成本。

##### **MenakBERT -- Hebrew Diacriticizer**
2410.02417v1 by Ido Cohen, Jacob Gidron, Idan Pinto

Diacritical marks in the Hebrew language give words their vocalized form. The
task of adding diacritical marks to plain Hebrew text is still dominated by a
system that relies heavily on human-curated resources. Recent models trained on
diacritized Hebrew texts still present a gap in performance. We use a recently
developed char-based PLM to narrowly bridge this gap. Presenting MenakBERT, a
character level transformer pretrained on Hebrew text and fine-tuned to produce
diacritical marks for Hebrew sentences. We continue to show how finetuning a
model for diacritizing transfers to a task such as part of speech tagging.

摘要：希伯來語中的附加符號賦予單字發音形式。
將附加符號加入純希伯來語文字的工作，目前仍由高度依賴人工整理資源的系統主導。
最近在希伯來語附加符號文字上訓練的模型，在效能上仍有進步空間。
我們使用最近開發的基於字元的 PLM，來縮小這個差距。
提出 MenakBERT，一個預先訓練於希伯來語文字的字元級轉換器，並微調用於產生希伯來語句子的附加符號。
我們繼續展示如何微調用於附加符號化的模型，轉移到詞性標記等任務。

##### **SynCo: Synthetic Hard Negatives in Contrastive Learning for Better Unsupervised Visual Representations**
2410.02401v1 by Nikolaos Giakoumoglou, Tania Stathaki

Contrastive learning has become a dominant approach in self-supervised visual
representation learning, with hard negatives-samples that closely resemble the
anchor-being key to enhancing the discriminative power of learned
representations. However, efficiently leveraging hard negatives remains a
challenge due to the difficulty in identifying and incorporating them without
significantly increasing computational costs. To address this, we introduce
SynCo (Synthetic Negatives in Contrastive learning), a novel contrastive
learning approach that improves model performance by generating synthetic hard
negatives. Built on the MoCo framework, SynCo introduces six novel strategies
for creating diverse synthetic hard negatives that can be generated on-the-fly
with minimal computational overhead. SynCo achieves faster training and better
representation learning, achieving a top-1 accuracy of 68.1% in ImageNet linear
evaluation after only 200 epochs on pretraining, surpassing MoCo's 67.5% with
the same ResNet-50 encoder. Additionally, it transfers more effectively to
detection tasks: on the PASCAL VOC, it outperforms both the supervised baseline
and MoCo, achieving an AP of 82.5%; on the COCO dataset, it sets a new
benchmark with 40.4% AP for bounding box detection and 35.4% AP for instance
segmentation. Our synthetic hard negative generation procedure significantly
enhances the quality of visual representations learned through self-supervised
contrastive learning. Code is available at
https://github.com/giakoumoglou/synco.

摘要：對比學習已成為自監督視覺表示學習中的主要方法，其中與錨點非常相似的困難負樣本是增強學習表示的判別力的關鍵。然而，由於難以在不顯著增加計算成本的情況下識別和整合困難負樣本，因此有效利用困難負樣本仍然是一個挑戰。為了解決這個問題，我們引入了 SynCo（對比學習中的合成負樣本），這是一種通過生成合成困難負樣本來提高模型效能的新穎對比學習方法。SynCo 建立在 MoCo 框架之上，引入了六種創新的策略來創建多樣化的合成困難負樣本，這些負樣本可以在執行期間生成，且計算負擔極小。SynCo 實現了更快的訓練和更好的表示學習，在預訓練後僅 200 個世代就達到了 ImageNet 線性評估中 68.1% 的 top-1 準確度，超越了使用相同 ResNet-50 編碼器的 MoCo 的 67.5%。此外，它更有效地轉移到檢測任務：在 PASCAL VOC 上，它優於監督基線和 MoCo，達到了 82.5% 的 AP；在 COCO 資料集上，它為邊界框檢測設定了新的基準，AP 為 40.4%，而實例分割的 AP 為 35.4%。我們的合成困難負樣本生成程序顯著提高了通過自監督對比學習學習的視覺表示的品質。程式碼可在 https://github.com/giakoumoglou/synco 取得。

##### **Parameter Competition Balancing for Model Merging**
2410.02396v1 by Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Min Zhang

While fine-tuning pretrained models has become common practice, these models
often underperform outside their specific domains. Recently developed model
merging techniques enable the direct integration of multiple models, each
fine-tuned for distinct tasks, into a single model. This strategy promotes
multitasking capabilities without requiring retraining on the original
datasets. However, existing methods fall short in addressing potential
conflicts and complex correlations between tasks, especially in parameter-level
adjustments, posing a challenge in effectively balancing parameter competition
across various tasks. This paper introduces an innovative technique named
PCB-Merging (Parameter Competition Balancing), a lightweight and training-free
technique that adjusts the coefficients of each parameter for effective model
merging. PCB-Merging employs intra-balancing to gauge parameter significance
within individual tasks and inter-balancing to assess parameter similarities
across different tasks. Parameters with low importance scores are dropped, and
the remaining ones are rescaled to form the final merged model. We assessed our
approach in diverse merging scenarios, including cross-task, cross-domain, and
cross-training configurations, as well as out-of-domain generalization. The
experimental results reveal that our approach achieves substantial performance
enhancements across multiple modalities, domains, model sizes, number of tasks,
fine-tuning forms, and large language models, outperforming existing model
merging methods. The code is publicly available at:
\url{https://github.com/duguodong7/pcb-merging}.

摘要：<paragraph>儘管微調預訓練模型已成為常見做法，但這些模型通常在其特定領域外表現不佳。最近開發的模型合併技術能直接整合多個模型，每個模型都針對不同的任務進行微調，並整合到單一模型中。此策略可促進多工能力，而無需針對原始資料集進行重新訓練。然而，現有方法無法解決潛在衝突和任務之間的複雜關聯，特別是在參數層級調整中，對跨各種任務有效平衡參數競爭構成挑戰。本文介紹了一種名為 PCB-Merging（參數競爭平衡）的創新技術，這是一種輕量級且無需訓練的技術，可調整每個參數的係數以進行有效的模型合併。PCB-Merging 使用內部平衡來衡量個別任務中的參數重要性，並使用外部平衡來評估不同任務之間的參數相似性。會捨棄重要性評分低的參數，並重新調整其餘參數的比例以形成最終合併的模型。我們在各種合併情境中評估了我們的做法，包括跨任務、跨領域和跨訓練組態，以及領域外概化。實驗結果顯示，我們的做法在多種模式、領域、模型大小、任務數量、微調形式和大語言模型中都獲得了顯著的效能提升，優於現有的模型合併方法。程式碼已公開於：\url{https://github.com/duguodong7/pcb-merging}。</paragraph>

##### **Online Multi-Label Classification under Noisy and Changing Label Distribution**
2410.02394v1 by Yizhang Zou, Xuegang Hu, Peipei Li, Jun Hu, You Wu

Multi-label data stream usually contains noisy labels in the real-world
applications, namely occuring in both relevant and irrelevant labels. However,
existing online multi-label classification methods are mostly limited in terms
of label quality and fail to deal with the case of noisy labels. On the other
hand, the ground-truth label distribution may vary with the time changing,
which is hidden in the observed noisy label distribution and difficult to
track, posing a major challenge for concept drift adaptation. Motivated by
this, we propose an online multi-label classification algorithm under Noisy and
Changing Label Distribution (NCLD). The convex objective is designed to
simultaneously model the label scoring and the label ranking for high accuracy,
whose robustness to NCLD benefits from three novel works: 1) The local feature
graph is used to reconstruct the label scores jointly with the observed labels,
and an unbiased ranking loss is derived and applied to learn reliable ranking
information. 2) By detecting the difference between two adjacent chunks with
the unbiased label cardinality, we identify the change in the ground-truth
label distribution and reset the ranking or all information learned from the
past to match the new distribution. 3) Efficient and accurate updating is
achieved based on the updating rule derived from the closed-form optimal model
solution. Finally, empirical experimental results validate the effectiveness of
our method in classifying instances under NCLD.

摘要：多標籤資料串流通常在真實世界的應用中包含雜訊標籤，也就是同時出現在相關和不相關的標籤中。然而，現有的線上多標籤分類方法在標籤品質方面大多受到限制，無法處理雜訊標籤的情況。另一方面，真實標籤分佈可能會隨著時間而改變，這隱藏在觀察到的雜訊標籤分佈中，難以追蹤，對概念漂移適應構成重大挑戰。有鑑於此，我們提出一個在雜訊和變動標籤分佈 (NCLD) 下的線上多標籤分類演算法。凸目標旨在同時建模標籤評分和標籤排名以獲得高準確度，其對 NCLD 的穩健性受益於三項新穎的工作：1) 局部特徵圖用於與觀察到的標籤一起重建標籤分數，並推導出無偏的排名損失並應用於學習可靠的排名資訊。2) 透過使用無偏標籤基數偵測兩個相鄰塊之間的差異，我們識別真實標籤分佈的變化，並重設從過去學習到的排名或所有資訊以符合新的分佈。3) 基於從閉合形式最佳模型解推導出的更新規則，實現高效且準確的更新。最後，實證實驗結果驗證了我們的方法在 NCLD 下對實例進行分類的有效性。

##### **Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks**
2410.02389v1 by Zeyu Feng, Hao Luan, Kevin Yuchen Ma, Harold Soh

Safe and successful deployment of robots requires not only the ability to
generate complex plans but also the capacity to frequently replan and correct
execution errors. This paper addresses the challenge of long-horizon trajectory
planning under temporally extended objectives in a receding horizon manner. To
this end, we propose DOPPLER, a data-driven hierarchical framework that
generates and updates plans based on instruction specified by linear temporal
logic (LTL). Our method decomposes temporal tasks into chain of options with
hierarchical reinforcement learning from offline non-expert datasets. It
leverages diffusion models to generate options with low-level actions. We
devise a determinantal-guided posterior sampling technique during batch
generation, which improves the speed and diversity of diffusion generated
options, leading to more efficient querying. Experiments on robot navigation
and manipulation tasks demonstrate that DOPPLER can generate sequences of
trajectories that progressively satisfy the specified formulae for obstacle
avoidance and sequential visitation. Demonstration videos are available online
at: https://philiptheother.github.io/doppler/.

摘要：機器人的安全且成功的部署不僅需要生成複雜計畫的能力，還需要頻繁重新計畫和修正執行錯誤的能力。本文以遞迴視界的方式探討在時間延伸目標下進行長時程軌跡規劃的挑戰。為此，我們提出了 DOPPLER，一個資料驅動的分層架構，它根據線性時序邏輯 (LTL) 指定的指令來生成和更新計畫。我們的技術將時間任務分解為選項鏈，並從離線非專家資料集中進行分層強化學習。它利用擴散模型來生成具有低階動作的選項。我們在批次生成期間設計了一種行列式引導後驗抽樣技術，它提高了擴散生成選項的速度和多樣性，從而提高了查詢效率。機器人導航和操作任務的實驗表明，DOPPLER 可以生成一系列軌跡，逐漸滿足指定公式以避免障礙物和順序拜訪。示範影片可在線上取得：https://philiptheother.github.io/doppler/。

##### **BiSSL: Bilevel Optimization for Self-Supervised Pre-Training and Fine-Tuning**
2410.02387v1 by Gustav Wagner Zakarias, Lars Kai Hansen, Zheng-Hua Tan

In this work, we present BiSSL, a first-of-its-kind training framework that
introduces bilevel optimization to enhance the alignment between the pretext
pre-training and downstream fine-tuning stages in self-supervised learning.
BiSSL formulates the pretext and downstream task objectives as the lower- and
upper-level objectives in a bilevel optimization problem and serves as an
intermediate training stage within the self-supervised learning pipeline. By
more explicitly modeling the interdependence of these training stages, BiSSL
facilitates enhanced information sharing between them, ultimately leading to a
backbone parameter initialization that is better suited for the downstream
task. We propose a training algorithm that alternates between optimizing the
two objectives defined in BiSSL. Using a ResNet-18 backbone pre-trained with
SimCLR on the STL10 dataset, we demonstrate that our proposed framework
consistently achieves improved or competitive classification accuracies across
various downstream image classification datasets compared to the conventional
self-supervised learning pipeline. Qualitative analyses of the backbone
features further suggest that BiSSL enhances the alignment of downstream
features in the backbone prior to fine-tuning.

摘要：在這項工作中，我們提出了 BiSSL，這是一個首創的訓練架構，它引入了雙層最佳化，以增強自我監督學習中預訓練和下游微調階段之間的對齊。BiSSL 將預訓練和下游任務目標公式化為雙層最佳化問題中的下層和上層目標，並作為自我監督學習流程中的中間訓練階段。通過更明確地建模這些訓練階段的相互依賴性，BiSSL 促進了它們之間增強的信息共享，最終導致更適合下游任務的主幹參數初始化。我們提出了一種訓練演算法，在 BiSSL 中定義的兩個目標之間交替最佳化。使用在 STL10 資料集上使用 SimCLR 預訓練的 ResNet-18 主幹，我們證明了我們提出的架構與傳統的自我監督學習流程相比，在各種下游影像分類資料集上持續獲得改善或具有競爭力的分類準確度。主幹特徵的定性分析進一步表明，BiSSL 增強了微調之前主幹中下游特徵的對齊。

##### **MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences**
2410.02381v1 by Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya

Understanding the quality of a performance evaluation metric is crucial for
ensuring that model outputs align with human preferences. However, it remains
unclear how well each metric captures the diverse aspects of these preferences,
as metrics often excel in one particular area but not across all dimensions. To
address this, it is essential to systematically calibrate metrics to specific
aspects of human preference, catering to the unique characteristics of each
aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate
generation tasks across different modalities in a supervised manner.
MetaMetrics optimizes the combination of existing metrics to enhance their
alignment with human preferences. Our metric demonstrates flexibility and
effectiveness in both language and vision downstream tasks, showing significant
benefits across various multilingual and multi-domain scenarios. MetaMetrics
aligns closely with human preferences and is highly extendable and easily
integrable into any application. This makes MetaMetrics a powerful tool for
improving the evaluation of generation tasks, ensuring that metrics are more
representative of human judgment across diverse contexts.

摘要：了解績效評估指標的品質對於確保模型輸出與人類偏好一致至關重要。然而，目前仍不清楚每個指標如何捕捉這些偏好的不同面向，因為指標通常只在特定領域表現出色，而無法涵蓋所有面向。為了解決這個問題，系統性地校準指標以符合人類偏好的特定面向至關重要，並迎合每個面向的獨特特徵。我們引入了 MetaMetrics，這是一個校準的元指標，旨在以監督的方式評估不同模式的產生任務。MetaMetrics 最佳化現有指標的組合，以增強它們與人類偏好的對齊。我們的指標展現了在語言和視覺下游任務中的靈活性與有效性，在各種多語言和多領域場景中展現顯著的優點。MetaMetrics 與人類偏好緊密對齊，且高度可擴充且易於整合到任何應用程式中。這使得 MetaMetrics 成為改善產生任務評估的強大工具，確保指標更能代表不同脈絡下的人類判斷。

##### **Towards Comprehensive Detection of Chinese Harmful Memes**
2410.02378v1 by Junyu Lu, Bo Xu, Xiaokun Zhang, Hongbo Wang, Haohao Zhu, Dongyu Zhang, Liang Yang, Hongfei Lin

This paper has been accepted in the NeurIPS 2024 D & B Track. Harmful memes
have proliferated on the Chinese Internet, while research on detecting Chinese
harmful memes significantly lags behind due to the absence of reliable datasets
and effective detectors. To this end, we focus on the comprehensive detection
of Chinese harmful memes. We construct ToxiCN MM, the first Chinese harmful
meme dataset, which consists of 12,000 samples with fine-grained annotations
for various meme types. Additionally, we propose a baseline detector,
Multimodal Knowledge Enhancement (MKE), incorporating contextual information of
meme content generated by the LLM to enhance the understanding of Chinese
memes. During the evaluation phase, we conduct extensive quantitative
experiments and qualitative analyses on multiple baselines, including LLMs and
our MKE. The experimental results indicate that detecting Chinese harmful memes
is challenging for existing models while demonstrating the effectiveness of
MKE. The resources for this paper are available at
https://github.com/DUT-lujunyu/ToxiCN_MM.

摘要：這篇論文已被 NeurIPS 2024 D & B Track 接受。有害迷因在中國網際網路上大量散布，而由於缺乏可靠的資料集和有效的偵測器，偵測中文有害迷因的研究顯著落後。為此，我們專注於全面偵測中文有害迷因。我們建構了 ToxiCN MM，這是第一個中文有害迷因資料集，其中包含 12,000 個範例，並針對各種迷因類型進行細緻的註解。此外，我們提出了一個基準偵測器，多模式知識增強 (MKE)，它結合了 LLM 生成的迷因內容的背景資訊，以增強對中文迷因的理解。在評估階段，我們對多個基準進行廣泛的量化實驗和定性分析，包括 LLM 和我們的 MKE。實驗結果表明，偵測中文有害迷因對現有模型來說具有挑戰性，同時也證明了 MKE 的有效性。這篇論文的資源可在 https://github.com/DUT-lujunyu/ToxiCN_MM 取得。

##### **NTU-NPU System for Voice Privacy 2024 Challenge**
2410.02371v1 by Nikita Kuzmin, Hieu-Thi Luong, Jixun Yao, Lei Xie, Kong Aik Lee, Eng Siong Chng

In this work, we describe our submissions for the Voice Privacy Challenge
2024. Rather than proposing a novel speech anonymization system, we enhance the
provided baselines to meet all required conditions and improve evaluated
metrics. Specifically, we implement emotion embedding and experiment with WavLM
and ECAPA2 speaker embedders for the B3 baseline. Additionally, we compare
different speaker and prosody anonymization techniques. Furthermore, we
introduce Mean Reversion F0 for B5, which helps to enhance privacy without a
loss in utility. Finally, we explore disentanglement models, namely $\beta$-VAE
and NaturalSpeech3 FACodec.

摘要：在這項工作中，我們描述了我們對 2024 年語音隱私挑戰的提交。我們沒有提出新的語音匿名化系統，而是增強了提供的基準，以滿足所有必需條件並改進評估指標。具體來說，我們實作了情緒嵌入，並針對 B3 基準測試了 WavLM 和 ECAPA2 揚聲器嵌入器。此外，我們比較了不同的揚聲器和韻律匿名化技術。此外，我們為 B5 引入了均值回歸 F0，這有助於在不損失效用的情況下增強隱私。最後，我們探討了 disentanglement 模型，即 $\beta$-VAE 和 NaturalSpeech3 FACodec。

##### **From Concrete to Abstract: A Multimodal Generative Approach to Abstract Concept Learning**
2410.02365v1 by Haodong Xie, Rahul Singh Maharjan, Federico Tavella, Angelo Cangelosi

Understanding and manipulating concrete and abstract concepts is fundamental
to human intelligence. Yet, they remain challenging for artificial agents. This
paper introduces a multimodal generative approach to high order abstract
concept learning, which integrates visual and categorical linguistic
information from concrete ones. Our model initially grounds subordinate level
concrete concepts, combines them to form basic level concepts, and finally
abstracts to superordinate level concepts via the grounding of basic-level
concepts. We evaluate the model language learning ability through
language-to-visual and visual-to-language tests with high order abstract
concepts. Experimental results demonstrate the proficiency of the model in both
language understanding and language naming tasks.

摘要：理解和操作具體和抽象概念是人類智能的基礎。然而，它們對人工智慧來說仍然具有挑戰性。本文介紹了一種多模態生成方法，用於高階抽象概念學習，它整合了來自具體概念的視覺和分類語言信息。我們的模型最初建立了從屬級別的具體概念，將它們組合起來形成基本級別的概念，最後通過基本級別概念的基礎，抽象到超類別級別的概念。我們通過高階抽象概念的語言到視覺和視覺到語言測試來評估模型的語言學習能力。實驗結果證明了該模型在語言理解和語言命名任務中的熟練程度。

