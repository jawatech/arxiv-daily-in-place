{"2502.14739": {"publish_time": "2025-02-20", "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines", "paper_summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities of LLMs in\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we present SuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novel Human-LLM collaborative filtering mechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-art\nLLMs across diverse knowledge domains (e.g., the reasoning-focused model\nDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive Human-LLM collaborative system, offering valuable\nmethodological guidance for future research initiatives of comparable scope.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u5728\u4e3b\u6d41\u5b78\u8853\u9818\u57df\uff08\u5982\u6578\u5b78\u3001\u7269\u7406\u548c\u96fb\u8166\u79d1\u5b78\uff09\u7684\u5353\u8d8a\u80fd\u529b\u3002\u7136\u800c\uff0c\u4eba\u985e\u77e5\u8b58\u5305\u542b\u8d85\u904e 200 \u500b\u5c08\u696d\u9818\u57df\uff0c\u9060\u9060\u8d85\u904e\u73fe\u6709\u57fa\u6e96\u7684\u7bc4\u570d\u3002LLM \u5728\u8a31\u591a\u9019\u4e9b\u5c08\u696d\u9818\u57df\uff08\u7279\u5225\u662f\u5728\u8f15\u5de5\u696d\u3001\u8fb2\u696d\u548c\u670d\u52d9\u5c0e\u5411\u9818\u57df\uff09\u7684\u80fd\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u8a55\u4f30\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SuperGPQA\uff0c\u9019\u662f\u4e00\u500b\u7d9c\u5408\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30 285 \u500b\u9818\u57df\u7684\u7814\u7a76\u751f\u7d1a\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\u3002\u6211\u5011\u7684\u57fa\u6e96\u63a1\u7528\u65b0\u7a4e\u7684\u4eba\u985e-LLM \u5354\u540c\u904e\u6ffe\u6a5f\u5236\uff0c\u900f\u904e\u57fa\u65bc LLM \u56de\u61c9\u548c\u5c08\u5bb6\u56de\u994b\u7684\u8fed\u4ee3\u6539\u9032\uff0c\u4f86\u6d88\u9664\u7463\u788e\u6216\u6a21\u7a1c\u5169\u53ef\u7684\u554f\u984c\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u7576\u524d\u6700\u5148\u9032\u7684 LLM \u5728\u4e0d\u540c\u77e5\u8b58\u9818\u57df\u7684\u8868\u73fe\u4ecd\u6709\u5f88\u5927\u7684\u6539\u9032\u7a7a\u9593\uff08\u4f8b\u5982\uff0c\u4ee5\u63a8\u7406\u70ba\u91cd\u9ede\u7684\u6a21\u578b DeepSeek-R1 \u5728 SuperGPQA \u4e0a\u9054\u5230\u4e86 61.82% \u7684\u6700\u9ad8\u6e96\u78ba\u5ea6\uff09\uff0c\u7a81\u986f\u4e86\u7576\u524d\u6a21\u578b\u80fd\u529b\u8207\u4eba\u5de5\u901a\u7528\u667a\u6167\u4e4b\u9593\u7684\u5de8\u5927\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f9e\u7ba1\u7406\u5927\u578b\u8a3b\u91cb\u904e\u7a0b\uff08\u6d89\u53ca 80 \u591a\u4f4d\u5c08\u5bb6\u8a3b\u91cb\u8005\u548c\u4e00\u500b\u4e92\u52d5\u5f0f\u4eba\u985e-LLM \u5354\u4f5c\u7cfb\u7d71\uff09\u4e2d\u63d0\u51fa\u4e86\u5168\u9762\u7684\u898b\u89e3\uff0c\u70ba\u672a\u4f86\u5177\u6709\u53ef\u6bd4\u898f\u6a21\u7684\u7814\u7a76\u8a08\u756b\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u65b9\u6cd5\u8ad6\u6307\u5c0e\u3002", "author": "M-A-P Team et.al.", "authors": "M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixing Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jingyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, Ge Zhang", "id": "2502.14739v1", "paper_url": "http://arxiv.org/abs/2502.14739v1", "repo": "null"}}