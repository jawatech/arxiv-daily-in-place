{"2411.19626": {"publish_time": "2024-11-29", "title": "GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding", "paper_summary": "Open-Vocabulary 3D object affordance grounding aims to anticipate ``action\npossibilities'' regions on 3D objects with arbitrary instructions, which is\ncrucial for robots to generically perceive real scenarios and respond to\noperational changes. Existing methods focus on combining images or languages\nthat depict interactions with 3D geometries to introduce external interaction\npriors. However, they are still vulnerable to a limited semantic space by\nfailing to leverage implied invariant geometries and potential interaction\nintentions. Normally, humans address complex tasks through multi-step reasoning\nand respond to diverse situations by leveraging associative and analogical\nthinking. In light of this, we propose GREAT (GeometRy-intEntion collAboraTive\ninference) for Open-Vocabulary 3D Object Affordance Grounding, a novel\nframework that mines the object invariant geometry attributes and performs\nanalogically reason in potential interaction scenarios to form affordance\nknowledge, fully combining the knowledge with both geometries and visual\ncontents to ground 3D object affordance. Besides, we introduce the Point Image\nAffordance Dataset v2 (PIADv2), the largest 3D object affordance dataset at\npresent to support the task. Extensive experiments demonstrate the\neffectiveness and superiority of GREAT. Code and dataset are available at\nproject.", "paper_summary_zh": "\u958b\u653e\u8a5e\u5f59 3D \u7269\u4ef6\u53ef\u8ca0\u64d4\u57fa\u790e\u65e8\u5728\u9810\u6e2c 3D \u7269\u4ef6\u4e0a\u5177\u6709\u4efb\u610f\u6307\u793a\u7684\u300c\u52d5\u4f5c\u53ef\u80fd\u6027\u300d\u5340\u57df\uff0c\u5c0d\u65bc\u6a5f\u5668\u4eba\u4ee5\u901a\u7528\u65b9\u5f0f\u611f\u77e5\u771f\u5be6\u5834\u666f\u548c\u56de\u61c9\u64cd\u4f5c\u8b8a\u66f4\u81f3\u95dc\u91cd\u8981\u3002\u73fe\u6709\u65b9\u6cd5\u5c08\u6ce8\u65bc\u7d50\u5408\u63cf\u7e6a\u8207 3D \u5e7e\u4f55\u5f62\u72c0\u4e92\u52d5\u7684\u5f71\u50cf\u6216\u8a9e\u8a00\uff0c\u4ee5\u5f15\u5165\u5916\u90e8\u4e92\u52d5\u5148\u9a57\u3002\u7136\u800c\uff0c\u5b83\u5011\u4ecd\u5bb9\u6613\u53d7\u5230\u8a9e\u7fa9\u7a7a\u9593\u6709\u9650\u7684\u5f71\u97ff\uff0c\u56e0\u70ba\u7121\u6cd5\u5229\u7528\u96b1\u542b\u7684\u4e0d\u8b8a\u5e7e\u4f55\u5f62\u72c0\u548c\u6f5b\u5728\u7684\u4e92\u52d5\u610f\u5716\u3002\u901a\u5e38\uff0c\u4eba\u985e\u900f\u904e\u591a\u6b65\u9a5f\u63a8\u7406\u4f86\u8655\u7406\u8907\u96dc\u4efb\u52d9\uff0c\u4e26\u900f\u904e\u806f\u60f3\u548c\u985e\u6bd4\u601d\u8003\u4f86\u56de\u61c9\u4e0d\u540c\u7684\u60c5\u6cc1\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6211\u5011\u63d0\u51fa GREAT\uff08GeometRy-intEntion collAboraTive inference\uff09\uff0c\u7528\u65bc\u958b\u653e\u8a5e\u5f59 3D \u7269\u4ef6\u53ef\u8ca0\u64d4\u57fa\u790e\uff0c\u4e00\u500b\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u7528\u65bc\u6316\u6398\u7269\u4ef6\u4e0d\u8b8a\u5e7e\u4f55\u5c6c\u6027\uff0c\u4e26\u5728\u6f5b\u5728\u4e92\u52d5\u5834\u666f\u4e2d\u9032\u884c\u985e\u6bd4\u63a8\u7406\uff0c\u4ee5\u5f62\u6210\u53ef\u8ca0\u64d4\u77e5\u8b58\uff0c\u5c07\u77e5\u8b58\u8207\u5e7e\u4f55\u5f62\u72c0\u548c\u8996\u89ba\u5167\u5bb9\u5b8c\u5168\u7d50\u5408\uff0c\u4ee5\u5960\u5b9a 3D \u7269\u4ef6\u53ef\u8ca0\u64d4\u57fa\u790e\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86 Point Image Affordance Dataset v2 (PIADv2)\uff0c\u76ee\u524d\u6700\u5927\u7684 3D \u7269\u4ef6\u53ef\u8ca0\u64d4\u8cc7\u6599\u96c6\uff0c\u4ee5\u652f\u63f4\u9019\u9805\u4efb\u52d9\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86 GREAT \u7684\u6709\u6548\u6027\u548c\u512a\u8d8a\u6027\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u96c6\u53ef\u5728\u5c08\u6848\u4e2d\u53d6\u5f97\u3002", "author": "Yawen Shao et.al.", "authors": "Yawen Shao, Wei Zhai, Yuhang Yang, Hongchen Luo, Yang Cao, Zheng-Jun Zha", "id": "2411.19626v1", "paper_url": "http://arxiv.org/abs/2411.19626v1", "repo": "null"}}