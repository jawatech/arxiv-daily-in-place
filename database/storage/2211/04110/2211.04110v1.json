{"2211.04110": {"publish_time": "2022-11-08", "title": "Privacy Meets Explainability: A Comprehensive Impact Benchmark", "paper_summary": "Since the mid-10s, the era of Deep Learning (DL) has continued to this day,\nbringing forth new superlatives and innovations each year. Nevertheless, the\nspeed with which these innovations translate into real applications lags behind\nthis fast pace. Safety-critical applications, in particular, underlie strict\nregulatory and ethical requirements which need to be taken care of and are\nstill active areas of debate. eXplainable AI (XAI) and privacy-preserving\nmachine learning (PPML) are both crucial research fields, aiming at mitigating\nsome of the drawbacks of prevailing data-hungry black-box models in DL. Despite\nbrisk research activity in the respective fields, no attention has yet been\npaid to their interaction. This work is the first to investigate the impact of\nprivate learning techniques on generated explanations for DL-based models. In\nan extensive experimental analysis covering various image and time series\ndatasets from multiple domains, as well as varying privacy techniques, XAI\nmethods, and model architectures, the effects of private training on generated\nexplanations are studied. The findings suggest non-negligible changes in\nexplanations through the introduction of privacy. Apart from reporting\nindividual effects of PPML on XAI, the paper gives clear recommendations for\nthe choice of techniques in real applications. By unveiling the\ninterdependencies of these pivotal technologies, this work is a first step\ntowards overcoming the remaining hurdles for practically applicable AI in\nsafety-critical domains.", "paper_summary_zh": "", "author": "Saifullah Saifullah et.al.", "authors": "Saifullah Saifullah,Dominique Mercier,Adriano Lucieri,Andreas Dengel,Sheraz Ahmed", "id": "2211.04110v1", "paper_url": "http://arxiv.org/abs/2211.04110v1", "repo": "null"}}