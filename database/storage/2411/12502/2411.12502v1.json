{"2411.12502": {"publish_time": "2024-11-19", "title": "Transformer Neural Processes -- Kernel Regression", "paper_summary": "Stochastic processes model various natural phenomena from disease\ntransmission to stock prices, but simulating and quantifying their uncertainty\ncan be computationally challenging. For example, modeling a Gaussian Process\nwith standard statistical methods incurs an $\\mathcal{O}(n^3)$ penalty, and\neven using state-of-the-art Neural Processes (NPs) incurs an $\\mathcal{O}(n^2)$\npenalty due to the attention mechanism. We introduce the Transformer Neural\nProcess - Kernel Regression (TNP-KR), a new architecture that incorporates a\nnovel transformer block we call a Kernel Regression Block (KRBlock), which\nreduces the computational complexity of attention in transformer-based Neural\nProcesses (TNPs) from $\\mathcal{O}((n_C+n_T)^2)$ to $O(n_C^2+n_Cn_T)$ by\neliminating masked computations, where $n_C$ is the number of context, and\n$n_T$ is the number of test points, respectively, and a fast attention variant\nthat further reduces all attention calculations to $\\mathcal{O}(n_C)$ in space\nand time complexity. In benchmarks spanning such tasks as meta-regression,\nBayesian optimization, and image completion, we demonstrate that the full\nvariant matches the performance of state-of-the-art methods while training\nfaster and scaling two orders of magnitude higher in number of test points, and\nthe fast variant nearly matches that performance while scaling to millions of\nboth test and context points on consumer hardware.", "paper_summary_zh": "\u96a8\u6a5f\u904e\u7a0b\u6a21\u64ec\u4e86\u5f9e\u75be\u75c5\u50b3\u64ad\u5230\u80a1\u7968\u50f9\u683c\u7684\u5404\u7a2e\u81ea\u7136\u73fe\u8c61\uff0c\u4f46\u6a21\u64ec\u548c\u91cf\u5316\u5b83\u5011\u7684\u4e0d\u78ba\u5b9a\u6027\u5728\u8a08\u7b97\u4e0a\u53ef\u80fd\u5177\u6709\u6311\u6230\u6027\u3002\u4f8b\u5982\uff0c\u4f7f\u7528\u6a19\u6e96\u7d71\u8a08\u65b9\u6cd5\u5c0d\u9ad8\u65af\u904e\u7a0b\u9032\u884c\u5efa\u6a21\u6703\u7522\u751f $\\mathcal{O}(n^3)$ \u61f2\u7f70\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u5148\u9032\u7684\u795e\u7d93\u904e\u7a0b (NP) \u7531\u65bc\u6ce8\u610f\u529b\u6a5f\u5236\u4e5f\u6703\u7522\u751f $\\mathcal{O}(n^2)$ \u61f2\u7f70\u3002\u6211\u5011\u4ecb\u7d39\u4e86 Transformer \u795e\u7d93\u904e\u7a0b - \u6838\u8ff4\u6b78 (TNP-KR)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u67b6\u69cb\uff0c\u5b83\u5305\u542b\u4e86\u4e00\u500b\u6211\u5011\u7a31\u4e4b\u70ba\u6838\u8ff4\u6b78\u584a (KRBlock) \u7684\u65b0Transformer\u584a\uff0c\u5b83\u901a\u904e\u6d88\u9664Transformer\u4e2d\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u795e\u7d93\u904e\u7a0b (TNP) \u4e2d\u7684\u63a9\u78bc\u8a08\u7b97\u5c07\u8a08\u7b97\u8907\u96dc\u5ea6\u5f9e $\\mathcal{O}((n_C+n_T)^2)$ \u964d\u4f4e\u5230 $O(n_C^2+n_Cn_T)$\uff0c\u5176\u4e2d $n_C$ \u5206\u5225\u662f\u4e0a\u4e0b\u6587\u6578\u91cf\uff0c\u800c $n_T$ \u662f\u6e2c\u8a66\u9ede\u6578\u91cf\uff0c\u4ee5\u53ca\u4e00\u7a2e\u5feb\u901f\u6ce8\u610f\u8b8a\u9ad4\uff0c\u5b83\u9032\u4e00\u6b65\u5c07\u6240\u6709\u6ce8\u610f\u529b\u8a08\u7b97\u6e1b\u5c11\u5230 $\\mathcal{O}(n_C)$ \u7684\u7a7a\u9593\u548c\u6642\u9593\u8907\u96dc\u5ea6\u3002\u5728\u6db5\u84cb\u5143\u8ff4\u6b78\u3001\u8c9d\u8449\u65af\u512a\u5316\u548c\u5716\u50cf\u5b8c\u6210\u7b49\u4efb\u52d9\u7684\u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86\u5b8c\u6574\u8b8a\u9ad4\u5728\u8a13\u7df4\u901f\u5ea6\u66f4\u5feb\u4e14\u6e2c\u8a66\u9ede\u6578\u91cf\u64f4\u5c55\u5169\u500b\u6578\u91cf\u7d1a\u7684\u540c\u6642\uff0c\u8207\u6700\u5148\u9032\u65b9\u6cd5\u7684\u6027\u80fd\u76f8\u5339\u914d\uff0c\u800c\u5feb\u901f\u8b8a\u9ad4\u5728\u64f4\u5c55\u5230\u6578\u767e\u842c\u500b\u6e2c\u8a66\u548c\u4e0a\u4e0b\u6587\u9ede\u6642\u5e7e\u4e4e\u8207\u8a72\u6027\u80fd\u76f8\u5339\u914d\u6d88\u8cbb\u8005\u786c\u9ad4\u3002", "author": "Daniel Jenson et.al.", "authors": "Daniel Jenson, Jhonathan Navott, Mengyan Zhang, Makkunda Sharma, Elizaveta Semenova, Seth Flaxman", "id": "2411.12502v1", "paper_url": "http://arxiv.org/abs/2411.12502v1", "repo": "null"}}