{"2412.04234": {"publish_time": "2024-12-05", "title": "DEIM: DETR with Improved Matching for Fast Convergence", "paper_summary": "We introduce DEIM, an innovative and efficient training framework designed to\naccelerate convergence in real-time object detection with Transformer-based\narchitectures (DETR). To mitigate the sparse supervision inherent in one-to-one\n(O2O) matching in DETR models, DEIM employs a Dense O2O matching strategy. This\napproach increases the number of positive samples per image by incorporating\nadditional targets, using standard data augmentation techniques. While Dense\nO2O matching speeds up convergence, it also introduces numerous low-quality\nmatches that could affect performance. To address this, we propose the\nMatchability-Aware Loss (MAL), a novel loss function that optimizes matches\nacross various quality levels, enhancing the effectiveness of Dense O2O.\nExtensive experiments on the COCO dataset validate the efficacy of DEIM. When\nintegrated with RT-DETR and D-FINE, it consistently boosts performance while\nreducing training time by 50%. Notably, paired with RT-DETRv2, DEIM achieves\n53.2% AP in a single day of training on an NVIDIA 4090 GPU. Additionally,\nDEIM-trained real-time models outperform leading real-time object detectors,\nwith DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7% and 56.5% AP at 124 and 78\nFPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We\nbelieve DEIM sets a new baseline for advancements in real-time object\ndetection. Our code and pre-trained models are available at\nhttps://github.com/ShihuaHuang95/DEIM.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa DEIM\uff0c\u4e00\u500b\u5275\u65b0\u4e14\u9ad8\u6548\u7684\u8a13\u7df4\u67b6\u69cb\uff0c\u65e8\u5728\u52a0\u901f\u57fa\u65bc Transformer \u7684\u67b6\u69cb\uff08DETR\uff09\u7684\u5373\u6642\u7269\u4ef6\u5075\u6e2c\u4e2d\u7684\u6536\u6582\u3002\u70ba\u4e86\u6e1b\u8f15 DETR \u6a21\u578b\u4e2d\u7684\u4e00\u5c0d\u4e00 (O2O) \u5339\u914d\u4e2d\u56fa\u6709\u7684\u7a00\u758f\u76e3\u7763\uff0cDEIM \u63a1\u7528\u5bc6\u96c6 O2O \u5339\u914d\u7b56\u7565\u3002\u6b64\u65b9\u6cd5\u900f\u904e\u4f7f\u7528\u6a19\u6e96\u8cc7\u6599\u64f4\u5145\u6280\u8853\uff0c\u7d0d\u5165\u984d\u5916\u7684\u76ee\u6a19\uff0c\u4f86\u589e\u52a0\u6bcf\u5f35\u5f71\u50cf\u4e2d\u7684\u6b63\u6a23\u672c\u6578\u91cf\u3002\u96d6\u7136\u5bc6\u96c6 O2O \u5339\u914d\u52a0\u901f\u4e86\u6536\u6582\uff0c\u4f46\u5b83\u4e5f\u5f15\u5165\u4e86\u8a31\u591a\u53ef\u80fd\u5f71\u97ff\u6548\u80fd\u7684\u4f4e\u54c1\u8cea\u5339\u914d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5339\u914d\u611f\u77e5\u640d\u5931 (MAL)\uff0c\u4e00\u7a2e\u91dd\u5c0d\u5404\u7a2e\u54c1\u8cea\u5c64\u7d1a\u6700\u4f73\u5316\u5339\u914d\u7684\u65b0\u640d\u5931\u51fd\u6578\uff0c\u4ee5\u589e\u5f37\u5bc6\u96c6 O2O \u7684\u6548\u80fd\u3002\u5728 COCO \u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u9a57\u8b49\u4e86 DEIM \u7684\u6548\u80fd\u3002\u7576\u8207 RT-DETR \u548c D-FINE \u6574\u5408\u6642\uff0c\u5b83\u6301\u7e8c\u63d0\u5347\u6548\u80fd\uff0c\u540c\u6642\u5c07\u8a13\u7df4\u6642\u9593\u7e2e\u77ed 50%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u642d\u914d RT-DETRv2\uff0cDEIM \u5728 NVIDIA 4090 GPU \u4e0a\u8a13\u7df4\u4e00\u5929\u5373\u53ef\u9054\u5230 53.2% \u7684 AP\u3002\u6b64\u5916\uff0cDEIM \u8a13\u7df4\u7684\u5373\u6642\u6a21\u578b\u512a\u65bc\u9818\u5148\u7684\u5373\u6642\u7269\u4ef6\u5075\u6e2c\u5668\uff0c\u5176\u4e2d DEIM-D-FINE-L \u548c DEIM-D-FINE-X \u5728 NVIDIA T4 GPU \u4e0a\u5206\u5225\u4ee5 124 \u548c 78 FPS \u9054\u5230 54.7% \u548c 56.5% \u7684 AP\uff0c\u800c\u7121\u9700\u984d\u5916\u8cc7\u6599\u3002\u6211\u5011\u76f8\u4fe1 DEIM \u70ba\u5373\u6642\u7269\u4ef6\u5075\u6e2c\u7684\u9032\u5c55\u8a2d\u5b9a\u4e86\u65b0\u7684\u57fa\u6e96\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u9810\u8a13\u7df4\u6a21\u578b\u53ef\u5728 https://github.com/ShihuaHuang95/DEIM \u53d6\u5f97\u3002</paragraph>", "author": "Shihua Huang et.al.", "authors": "Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, Xi Shen", "id": "2412.04234v1", "paper_url": "http://arxiv.org/abs/2412.04234v1", "repo": "null"}}