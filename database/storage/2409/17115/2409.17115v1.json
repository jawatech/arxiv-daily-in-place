{"2409.17115": {"publish_time": "2024-09-25", "title": "Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale", "paper_summary": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u50b3\u7d71\u4e0a\u4f9d\u8cf4\u4eba\u985e\u5c08\u5bb6\n\u5236\u5b9a\u555f\u767c\u5f0f\u65b9\u6cd5\u4f86\u6539\u5584\u8a9e\u6599\u5eab\u54c1\u8cea\uff0c\u5c0e\u81f4\u8fc4\u4eca\u5df2\u958b\u767c\u51fa\u8a31\u591a\u898f\u5247\u3002\u7136\u800c\uff0c\u9019\u4e9b\u898f\u5247\u7f3a\u4e4f\u9748\u6d3b\u6027\uff0c\u7121\u6cd5\u6709\u6548\u89e3\u6c7a\u500b\u5225\u7bc4\u4f8b\u7684\u7368\u7279\u7279\u6027\u3002\u540c\u6642\uff0c\u5c0d\u6bcf\u500b\u7bc4\u4f8b\u61c9\u7528\u91cf\u8eab\u6253\u9020\u7684\u898f\u5247\u5c0d\u4eba\u985e\u5c08\u5bb6\u4f86\u8aaa\u4e26\u4e0d\u5be6\u969b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u5373\u4f7f\u662f\u53ea\u6709 0.3B \u53c3\u6578\u7684\u5c0f\u8a9e\u8a00\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u5c55\u73fe\u51fa\u8207\u4eba\u985e\u5c08\u5bb6\u76f8\u7576\u7684\u5be6\u8cea\u6027\u8cc7\u6599\u7cbe\u7149\u80fd\u529b\u3002\u6211\u5011\u5f15\u5165\u4e86\u300c\u70ba\u6bcf\u500b\u7bc4\u4f8b\u7de8\u7a0b\u300d\uff08ProX\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u5c07\u8cc7\u6599\u7cbe\u7149\u8996\u70ba\u4e00\u9805\u7de8\u7a0b\u4efb\u52d9\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u900f\u904e\u70ba\u6bcf\u500b\u500b\u5225\u7bc4\u4f8b\u7522\u751f\u4e26\u57f7\u884c\u7d30\u5fae\u7684\u904b\u7b97\uff08\u4f8b\u5982\u5b57\u4e32\u6b63\u898f\u5316\uff09\u4f86\u7cbe\u7149\u8a9e\u6599\u5eab\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u4f7f\u7528 ProX \u6574\u7406\u8cc7\u6599\u5f8c\u9810\u5148\u8a13\u7df4\u7684\u6a21\u578b\uff0c\u5728\u5404\u7a2e\u4e0b\u6e38\u57fa\u6e96\u4e2d\u90fd\u6bd4\u539f\u59cb\u8cc7\u6599\u6216\u7531\u5176\u4ed6\u9078\u64c7\u65b9\u6cd5\u7be9\u9078\u7684\u8cc7\u6599\u9ad8\u51fa 2% \u4ee5\u4e0a\u3002\u5176\u6548\u80fd\u6db5\u84cb\u5404\u7a2e\u6a21\u578b\u5927\u5c0f\u548c\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\uff0c\u5305\u62ec C4\u3001RedPajama-V2 \u548c FineWeb\u3002\u6b64\u5916\uff0cProX \u5728\u7279\u5b9a\u9818\u57df\u7684\u6301\u7e8c\u9810\u8a13\u7df4\u4e2d\u5c55\u73fe\u51fa\u986f\u8457\u7684\u6f5b\u529b\uff1a\u5728\u6c92\u6709\u7279\u5b9a\u9818\u57df\u8a2d\u8a08\u7684\u60c5\u6cc1\u4e0b\uff0c\u4f7f\u7528 ProX \u7cbe\u7149\u7684 OpenWebMath \u8a13\u7df4\u6a21\u578b\u512a\u65bc\u4eba\u985e\u7de8\u5beb\u7684\u57fa\u65bc\u898f\u5247\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u6e96\u78ba\u5ea6\u6bd4 Mistral-7B \u63d0\u9ad8 7.6%\uff0c\u6bd4 Llama-2-7B \u63d0\u9ad8 14.6%\uff0c\u6bd4 CodeLlama-7B \u63d0\u9ad8 20.3%\uff0c\u6240\u6709\u90fd\u5728 10B \u500b\u4ee3\u5e63\u5167\uff0c\u8207\u8a13\u7df4 200B \u500b\u4ee3\u5e63\u7684 Llemma-7B \u7b49\u6a21\u578b\u76f8\u7576\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u91cd\u9ede\u6307\u51fa\uff0cProX \u5927\u5e45\u7bc0\u7701\u4e86\u8a13\u7df4 FLOP\uff0c\u70ba\u9ad8\u6548 LLM \u9810\u8a13\u7df4\u63d0\u4f9b\u4e86\u4e00\u689d\u6709\u5e0c\u671b\u7684\u8def\u5f91\u3002\u6211\u5011\u958b\u653e\u539f\u59cb\u78bc ProX\uff0c\u5176\u4e2d\u5305\u542b >100B \u8a9e\u6599\u5eab\u3001\u6a21\u578b\uff0c\u4e26\u5206\u4eab\u6240\u6709\u8a13\u7df4\u548c\u5be6\u4f5c\u7d30\u7bc0\uff0c\u4ee5\u5229\u65bc\u53ef\u91cd\u88fd\u7684\u7814\u7a76\u548c\u672a\u4f86\u7684\u5275\u65b0\u3002\u7a0b\u5f0f\u78bc\uff1ahttps://github.com/GAIR-NLP/ProX", "author": "Fan Zhou et.al.", "authors": "Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu", "id": "2409.17115v1", "paper_url": "http://arxiv.org/abs/2409.17115v1", "repo": "https://github.com/gair-nlp/prox"}}