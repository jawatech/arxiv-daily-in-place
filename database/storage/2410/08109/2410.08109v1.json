{"2410.08109": {"publish_time": "2024-10-10", "title": "A Closer Look at Machine Unlearning for Large Language Models", "paper_summary": "Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u80fd\u6703\u8a18\u61b6\u654f\u611f\u6216\u53d7\u7248\u6b0a\u4fdd\u8b77\u7684\u5167\u5bb9\uff0c\u5f15\u767c\u96b1\u79c1\u548c\u6cd5\u5f8b\u554f\u984c\u3002\u7531\u65bc\u5f9e\u982d\u958b\u59cb\u91cd\u65b0\u8a13\u7df4\u7684\u6210\u672c\u5f88\u9ad8\uff0c\u7814\u7a76\u4eba\u54e1\u5617\u8a66\u63a1\u7528\u6a5f\u5668\u907a\u5fd8\u4f86\u79fb\u9664 LLM \u4e2d\u7684\u7279\u5b9a\u5167\u5bb9\uff0c\u540c\u6642\u4fdd\u7559\u6574\u9ad4\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86 LLM \u6a5f\u5668\u907a\u5fd8\u7684\u5e7e\u500b\u554f\u984c\uff0c\u4e26\u63d0\u4f9b\u6211\u5011\u5c0d\u53ef\u80fd\u65b9\u6cd5\u7684\u898b\u89e3\u3002\u70ba\u4e86\u89e3\u6c7a\u907a\u5fd8\u5f8c\u6a21\u578b\u8f38\u51fa\u7684\u8a55\u4f30\u4e0d\u8db3\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e09\u500b\u984d\u5916\u7684\u6307\u6a19\u4f86\u8a55\u4f30\u4ee3\u5e63\u591a\u6a23\u6027\u3001\u53e5\u5b50\u8a9e\u7fa9\u548c\u4e8b\u5be6\u6b63\u78ba\u6027\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07\u907a\u5fd8\u65b9\u6cd5\u5206\u985e\u70ba\u975e\u76ee\u6a19\u548c\u76ee\u6a19\uff0c\u4e26\u5206\u5225\u8a0e\u8ad6\u5b83\u5011\u7684\u554f\u984c\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u975e\u76ee\u6a19\u907a\u5fd8\u5617\u8a66\u8fd1\u4f3c\u7684\u884c\u70ba\u662f\u4e0d\u53ef\u9810\u6e2c\u7684\uff0c\u53ef\u80fd\u6d89\u53ca\u5e7b\u89ba\uff0c\u800c\u73fe\u6709\u7684\u6b63\u5247\u5316\u4e0d\u8db3\u4ee5\u9032\u884c\u76ee\u6a19\u907a\u5fd8\u3002\u70ba\u4e86\u7de9\u89e3\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u6700\u5927\u5316\u71b5 (ME) \u7684\u76ee\u6a19\u9032\u884c\u975e\u76ee\u6a19\u907a\u5fd8\uff0c\u4e26\u5c07\u7b54\u6848\u4fdd\u7559 (AP) \u640d\u5931\u7d0d\u5165\u76ee\u6a19\u907a\u5fd8\u7684\u6b63\u5247\u5316\u3002\u5728\u865b\u69cb\u907a\u5fd8\u3001\u6301\u7e8c\u907a\u5fd8\u548c\u771f\u5be6\u4e16\u754c\u907a\u5fd8\u9019\u4e09\u7a2e\u5834\u666f\u4e2d\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/sail-sg/closer-look-LLM-unlearning \u53d6\u5f97\u3002", "author": "Xiaojian Yuan et.al.", "authors": "Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin", "id": "2410.08109v1", "paper_url": "http://arxiv.org/abs/2410.08109v1", "repo": "https://github.com/sail-sg/closer-look-llm-unlearning"}}