{"2410.17519": {"publish_time": "2024-10-23", "title": "Large Language Models Still Exhibit Bias in Long Text", "paper_summary": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks.", "paper_summary_zh": "\u73fe\u6709\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u516c\u5e73\u6027\u57fa\u6e96\u4e3b\u8981\u95dc\u6ce8\u65bc\u7c21\u55ae\u7684\u4efb\u52d9\uff0c\u4f8b\u5982\u9078\u64c7\u984c\uff0c\u800c\u5ffd\u7565\u4e86\u5728\u9577\u6587\u672c\u751f\u6210\u7b49\u66f4\u8907\u96dc\u7684\u5834\u666f\u4e2d\u53ef\u80fd\u51fa\u73fe\u7684\u504f\u898b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86\u9577\u6587\u672c\u516c\u5e73\u6027\u6e2c\u8a66 (LTF-TEST)\uff0c\u4e00\u500b\u901a\u904e\u8ad6\u6587\u5f0f\u63d0\u793a\u8a55\u4f30 LLM \u4e2d\u504f\u898b\u7684\u6846\u67b6\u3002LTF-TEST \u6db5\u84cb 14 \u500b\u4e3b\u984c\u548c 10 \u500b\u4eba\u53e3\u7d71\u8a08\u8ef8\uff0c\u5305\u62ec\u6027\u5225\u548c\u7a2e\u65cf\uff0c\u7522\u751f 11,948 \u500b\u6a23\u672c\u3002\u901a\u904e\u8a55\u4f30\u6a21\u578b\u7684\u56de\u61c9\u53ca\u5176\u80cc\u5f8c\u7684\u63a8\u7406\uff0cLTF-TEST \u63ed\u793a\u4e86\u96e3\u4ee5\u5728\u7c21\u55ae\u7684\u56de\u61c9\u4e2d\u6aa2\u6e2c\u5230\u7684\u5fae\u5999\u504f\u898b\u3002\u5728\u6211\u5011\u5c0d\u5305\u62ec GPT-4o \u548c LLaMa3 \u5728\u5167\u7684\u4e94\u500b\u6700\u65b0 LLM \u7684\u8a55\u4f30\u4e2d\uff0c\u6211\u5011\u78ba\u5b9a\u4e86\u5169\u7a2e\u95dc\u9375\u7684\u504f\u898b\u6a21\u5f0f\u3002\u9996\u5148\uff0c\u9019\u4e9b\u6a21\u578b\u7d93\u5e38\u5728\u5176\u56de\u61c9\u4e2d\u504f\u611b\u67d0\u4e9b\u4eba\u53e3\u7d71\u8a08\u7fa4\u9ad4\u3002\u5176\u6b21\uff0c\u5b83\u5011\u5c0d\u50b3\u7d71\u4e0a\u8655\u65bc\u5f31\u52e2\u7fa4\u9ad4\u8868\u73fe\u51fa\u904e\u5ea6\u7684\u654f\u611f\u6027\uff0c\u7d93\u5e38\u63d0\u4f9b\u904e\u5ea6\u4fdd\u8b77\u6027\u7684\u56de\u61c9\uff0c\u800c\u5ffd\u8996\u5176\u4ed6\u7fa4\u9ad4\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u4e9b\u504f\u898b\uff0c\u6211\u5011\u63d0\u51fa\u4e86 FT-REGARD\uff0c\u4e00\u7a2e\u5c07\u6709\u504f\u898b\u7684\u63d0\u793a\u8207\u4e2d\u7acb\u56de\u61c9\u914d\u5c0d\u7684\u5fae\u8abf\u65b9\u6cd5\u3002FT-REGARD \u5c07\u6027\u5225\u504f\u898b\u964d\u4f4e\u4e86 34.6%\uff0c\u4e26\u5728 BBQ \u57fa\u6e96\u4e0a\u5c07\u6027\u80fd\u63d0\u9ad8\u4e86 1.4 \u500b\u767e\u5206\u9ede\uff0c\u70ba\u89e3\u6c7a\u9577\u6587\u672c\u751f\u6210\u4efb\u52d9\u4e2d\u7684\u504f\u898b\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "author": "Wonje Jeung et.al.", "authors": "Wonje Jeung, Dongjae Jeon, Ashkan Yousefpour, Jonghyun Choi", "id": "2410.17519v1", "paper_url": "http://arxiv.org/abs/2410.17519v1", "repo": "null"}}