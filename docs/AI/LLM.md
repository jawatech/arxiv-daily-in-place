
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-11-29**|**T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs**|Shukang Yin et.al.|[2411.19951v1](http://arxiv.org/abs/2411.19951v1)|[link](https://github.com/xjtupanda/t2vid)|
|**2024-11-29**|**DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation**|Zhiqiang Shen et.al.|[2411.19946v1](http://arxiv.org/abs/2411.19946v1)|[link](https://github.com/vila-lab/delt)|
|**2024-11-29**|**Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability**|Zicheng Lin et.al.|[2411.19943v1](http://arxiv.org/abs/2411.19943v1)|null|
|**2024-11-29**|**Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark**|Joseph Heyward et.al.|[2411.19941v1](http://arxiv.org/abs/2411.19941v1)|null|
|**2024-11-29**|**VLSBench: Unveiling Visual Leakage in Multimodal Safety**|Xuhao Hu et.al.|[2411.19939v1](http://arxiv.org/abs/2411.19939v1)|null|
|**2024-11-29**|**On Domain-Specific Post-Training for Multimodal Large Language Models**|Daixuan Cheng et.al.|[2411.19930v1](http://arxiv.org/abs/2411.19930v1)|null|
|**2024-11-29**|**SIMS: Simulating Human-Scene Interactions with Real World Script Planning**|Wenjia Wang et.al.|[2411.19921v1](http://arxiv.org/abs/2411.19921v1)|null|
|**2024-11-29**|**Handling irresolvable conflicts in the Semantic Web: an RDF-based conflict-tolerant version of the Deontic Traditional Scheme**|Livio Robaldo et.al.|[2411.19918v1](http://arxiv.org/abs/2411.19918v1)|[link](https://github.com/liviorobaldo/conflict-tolerantdeontictraditionalscheme)|
|**2024-11-29**|**Quantifying the synthetic and real domain gap in aerial scene understanding**|Alina Marcu et.al.|[2411.19913v1](http://arxiv.org/abs/2411.19913v1)|null|
|**2024-11-29**|**Classical and Quantum Algorithms for the Deterministic L-system Inductive Inference Problem**|Ali Lotfi et.al.|[2411.19906v1](http://arxiv.org/abs/2411.19906v1)|null|
|**2024-11-29**|**PDDLFuse: A Tool for Generating Diverse Planning Domains**|Vedant Khandelwal et.al.|[2411.19886v1](http://arxiv.org/abs/2411.19886v1)|null|
|**2024-11-29**|**LUMIA: Linear probing for Unimodal and MultiModal Membership Inference A!acks leveraging internal LLM states**|Luis Ibanez-Lissen et.al.|[2411.19876v1](http://arxiv.org/abs/2411.19876v1)|null|
|**2024-11-29**|**Enhanced anomaly detection in well log data through the application of ensemble GANs**|Abdulrahman Al-Fakih et.al.|[2411.19875v1](http://arxiv.org/abs/2411.19875v1)|[link](https://github.com/ARhaman/EGANs-vs.GMM)|
|**2024-11-29**|**DeMo: Decoupled Momentum Optimization**|Bowen Peng et.al.|[2411.19870v1](http://arxiv.org/abs/2411.19870v1)|[link](https://github.com/bloc97/demo)|
|**2024-11-29**|**AIDetx: a compression-based method for identification of machine-learning generated text**|Leonardo Almeida et.al.|[2411.19869v1](http://arxiv.org/abs/2411.19869v1)|[link](https://github.com/aidetx/aidetx)|
|**2024-11-29**|**Reverse Thinking Makes LLMs Stronger Reasoners**|Justin Chih-Yao Chen et.al.|[2411.19865v1](http://arxiv.org/abs/2411.19865v1)|null|
|**2024-11-29**|**What fifty-one years of Linguistics and Artificial Intelligence research tell us about their correlation: A scientometric review**|Mohammed Q. Shormani et.al.|[2411.19858v1](http://arxiv.org/abs/2411.19858v1)|null|
|**2024-11-29**|**Artificial intelligence contribution to translation industry: looking back and forward**|Mohammed Q. Shormani et.al.|[2411.19855v1](http://arxiv.org/abs/2411.19855v1)|null|
|**2024-11-29**|**Scaling Transformers for Low-Bitrate High-Quality Speech Coding**|Julian D Parker et.al.|[2411.19842v1](http://arxiv.org/abs/2411.19842v1)|null|
|**2024-11-29**|**Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation**|Dimosthenis Antypas et.al.|[2411.19832v1](http://arxiv.org/abs/2411.19832v1)|null|
|**2024-11-29**|**SDR-GNN: Spectral Domain Reconstruction Graph Neural Network for Incomplete Multimodal Learning in Conversational Emotion Recognition**|Fangze Fu et.al.|[2411.19822v1](http://arxiv.org/abs/2411.19822v1)|null|
|**2024-11-29**|**Q-learning-based Model-free Safety Filter**|Guo Ning Sue et.al.|[2411.19809v1](http://arxiv.org/abs/2411.19809v1)|null|
|**2024-11-29**|**Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive Architectures**|Alain Riou et.al.|[2411.19806v1](http://arxiv.org/abs/2411.19806v1)|null|
|**2024-11-29**|**Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation**|Robin D. Pesl et.al.|[2411.19804v1](http://arxiv.org/abs/2411.19804v1)|null|
|**2024-11-29**|**INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge**|Angelika Romanou et.al.|[2411.19799v1](http://arxiv.org/abs/2411.19799v1)|null|
|**2024-11-29**|**CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives**|Armin Saghafian et.al.|[2411.19787v1](http://arxiv.org/abs/2411.19787v1)|null|
|**2024-11-29**|**MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks**|Yiming Wu et.al.|[2411.19786v1](http://arxiv.org/abs/2411.19786v1)|null|
|**2024-11-29**|**PerLA: Perceptive 3D Language Assistant**|Guofeng Mei et.al.|[2411.19774v1](http://arxiv.org/abs/2411.19774v1)|null|
|**2024-11-29**|**LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos**|Tiantian Geng et.al.|[2411.19772v1](http://arxiv.org/abs/2411.19772v1)|null|
|**2024-11-29**|**Noro: A Noise-Robust One-shot Voice Conversion System with Hidden Speaker Representation Capabilities**|Haorui He et.al.|[2411.19770v1](http://arxiv.org/abs/2411.19770v1)|null|
|**2024-11-29**|**Stock Price Prediction using Multi-Faceted Information based on Deep Recurrent Neural Networks**|Lida Shahbandari et.al.|[2411.19766v1](http://arxiv.org/abs/2411.19766v1)|null|
|**2024-11-29**|**Forecasting Foreign Exchange Market Prices Using Technical Indicators with Deep Learning and Attention Mechanism**|Sahabeh Saadati et.al.|[2411.19763v1](http://arxiv.org/abs/2411.19763v1)|null|
|**2024-11-29**|**LaVIDE: A Language-Vision Discriminator for Detecting Changes in Satellite Image with Map References**|Shuguo Jiang et.al.|[2411.19758v1](http://arxiv.org/abs/2411.19758v1)|null|
|**2024-11-29**|**A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining Off-Road, Diversity, and Directional Consistency Losses**|Ahmad Rahimi et.al.|[2411.19747v1](http://arxiv.org/abs/2411.19747v1)|[link](https://github.com/vita-epfl/stay-on-track)|
|**2024-11-29**|**Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph**|Heloisa Oss Boll et.al.|[2411.19742v1](http://arxiv.org/abs/2411.19742v1)|null|
|**2024-11-29**|**A Deep Learning Approach to Language-independent Gender Prediction on Twitter**|Reyhaneh Hashempour et.al.|[2411.19733v1](http://arxiv.org/abs/2411.19733v1)|null|
|**2024-11-29**|**Towards Santali Linguistic Inclusion: Building the First Santali-to-English Translation Model using mT5 Transformer and Data Augmentation**|Syed Mohammed Mostaque Billah et.al.|[2411.19726v1](http://arxiv.org/abs/2411.19726v1)|null|
|**2024-11-29**|**JetFormer: An Autoregressive Generative Model of Raw Images and Text**|Michael Tschannen et.al.|[2411.19722v1](http://arxiv.org/abs/2411.19722v1)|null|
|**2024-11-29**|**TakeLab Retriever: AI-Driven Search Engine for Articles from Croatian News Outlets**|David Dukić et.al.|[2411.19718v1](http://arxiv.org/abs/2411.19718v1)|null|
|**2024-11-29**|**MonoPP: Metric-Scaled Self-Supervised Monocular Depth Estimation by Planar-Parallax Geometry in Automotive Applications**|Gasser Elazab et.al.|[2411.19717v1](http://arxiv.org/abs/2411.19717v1)|null|
|**2024-11-29**|**MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating Multi-Insight Multi-Document Extraction Tasks**|John Francis et.al.|[2411.19689v1](http://arxiv.org/abs/2411.19689v1)|null|
|**2024-11-29**|**ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information**|Wanyue Zhang et.al.|[2411.19668v1](http://arxiv.org/abs/2411.19668v1)|[link](https://github.com/casia-lm/chinesewebtext-2.0)|
|**2024-11-29**|**Multimodal Whole Slide Foundation Model for Pathology**|Tong Ding et.al.|[2411.19666v1](http://arxiv.org/abs/2411.19666v1)|null|
|**2024-11-29**|**Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS**|Alessandro Scirè et.al.|[2411.19655v1](http://arxiv.org/abs/2411.19655v1)|null|
|**2024-11-29**|**Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing**|Wenyi Mo et.al.|[2411.19652v1](http://arxiv.org/abs/2411.19652v1)|[link](https://github.com/mowenyii/uniform-attention-maps)|
|**2024-11-29**|**CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation**|Qixiu Li et.al.|[2411.19650v1](http://arxiv.org/abs/2411.19650v1)|null|
|**2024-11-29**|**CAdam: Confidence-Based Optimization for Online Learning**|Shaowen Wang et.al.|[2411.19647v1](http://arxiv.org/abs/2411.19647v1)|null|
|**2024-11-29**|**LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification**|Taja Kuzman et.al.|[2411.19638v1](http://arxiv.org/abs/2411.19638v1)|null|
|**2024-11-29**|**Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings**|Qiong Wu et.al.|[2411.19628v1](http://arxiv.org/abs/2411.19628v1)|[link](https://github.com/doubtedsteam/dyvte)|
|**2024-11-29**|**GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding**|Yawen Shao et.al.|[2411.19626v1](http://arxiv.org/abs/2411.19626v1)|null|
|**2024-11-29**|**FairDD: Fair Dataset Distillation via Synchronized Matching**|Qihang Zhou et.al.|[2411.19623v1](http://arxiv.org/abs/2411.19623v1)|null|
|**2024-11-29**|**Can Large Language Models Reason about the Region Connection Calculus?**|Anthony G Cohn et.al.|[2411.19589v1](http://arxiv.org/abs/2411.19589v1)|null|
|**2024-11-29**|**Solving Rubik's Cube Without Tricky Sampling**|Yicheng Lin et.al.|[2411.19583v1](http://arxiv.org/abs/2411.19583v1)|null|
|**2024-11-29**|**In-Context Learning with Noisy Labels**|Junyong Kang et.al.|[2411.19581v1](http://arxiv.org/abs/2411.19581v1)|null|
|**2024-11-29**|**ICPR 2024 Competition on Multilingual Claim-Span Identification**|Soham Poddar et.al.|[2411.19579v1](http://arxiv.org/abs/2411.19579v1)|null|
|**2024-11-29**|**KV Shifting Attention Enhances Language Modeling**|Mingyu Xu et.al.|[2411.19574v1](http://arxiv.org/abs/2411.19574v1)|null|
|**2024-11-29**|**Ensemble Watermarks for Large Language Models**|Georg Niess et.al.|[2411.19563v1](http://arxiv.org/abs/2411.19563v1)|[link](https://github.com/commodoreeu/master-generation)|
|**2024-11-29**|**Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning**|Kaustubh Ponkshe et.al.|[2411.19557v1](http://arxiv.org/abs/2411.19557v1)|[link](https://github.com/raghavsinghal10/lora-sb)|
|**2024-11-29**|**Unimib Assistant: designing a student-friendly RAG-based chatbot for all their needs**|Chiara Antico et.al.|[2411.19554v1](http://arxiv.org/abs/2411.19554v1)|null|
|**2024-11-29**|**ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration**|Chaojun Ni et.al.|[2411.19548v1](http://arxiv.org/abs/2411.19548v1)|null|
|**2024-11-29**|**Training Agents with Weakly Supervised Feedback from Large Language Models**|Dihong Gong et.al.|[2411.19547v1](http://arxiv.org/abs/2411.19547v1)|null|
|**2024-11-29**|**SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders**|Niki Martinel et.al.|[2411.19544v1](http://arxiv.org/abs/2411.19544v1)|null|
|**2024-11-29**|**Knowledge Management for Automobile Failure Analysis Using Graph RAG**|Yuta Ojima et.al.|[2411.19539v1](http://arxiv.org/abs/2411.19539v1)|null|
|**2024-11-29**|**Deepfake Media Generation and Detection in the Generative AI Era: A Survey and Outlook**|Florinel-Alin Croitoru et.al.|[2411.19537v1](http://arxiv.org/abs/2411.19537v1)|[link](https://github.com/croitorualin/biodeep)|
|**2024-11-29**|**Quantized Delta Weight Is Safety Keeper**|Yule Liu et.al.|[2411.19530v1](http://arxiv.org/abs/2411.19530v1)|null|
|**2024-11-29**|**RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation**|Xianfeng Tan et.al.|[2411.19528v1](http://arxiv.org/abs/2411.19528v1)|null|
|**2024-11-29**|**DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding**|Jungbin Cho et.al.|[2411.19527v1](http://arxiv.org/abs/2411.19527v1)|null|
|**2024-11-29**|**RL-MILP Solver: A Reinforcement Learning Approach for Solving Mixed-Integer Linear Programs with Graph Neural Networks**|Tae-Hoon Lee et.al.|[2411.19517v1](http://arxiv.org/abs/2411.19517v1)|null|
|**2024-11-29**|**TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with Scalable Context and Symbolic Extension**|Zipeng Qiu et.al.|[2411.19504v1](http://arxiv.org/abs/2411.19504v1)|null|
|**2024-11-29**|**Knowledge-Data Fusion Based Source-Free Semi-Supervised Domain Adaptation for Seizure Subtype Classification**|Ruimin Peng et.al.|[2411.19502v1](http://arxiv.org/abs/2411.19502v1)|null|
|**2024-11-29**|**COLD: Causal reasOning in cLosed Daily activities**|Abhinav Joshi et.al.|[2411.19500v1](http://arxiv.org/abs/2411.19500v1)|null|
|**2024-11-29**|**Interleaved-Modal Chain-of-Thought**|Jun Gao et.al.|[2411.19488v1](http://arxiv.org/abs/2411.19488v1)|null|
|**2024-11-29**|**Action Engine: An LLM-based Framework for Automatic FaaS Workflow Generation**|Akiharu Esashi et.al.|[2411.19485v1](http://arxiv.org/abs/2411.19485v1)|null|
|**2024-11-29**|**FLARE: Towards Universal Dataset Purification against Backdoor Attacks**|Linshan Hou et.al.|[2411.19479v1](http://arxiv.org/abs/2411.19479v1)|null|
|**2024-11-29**|**A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models**|Yanxi Chen et.al.|[2411.19477v1](http://arxiv.org/abs/2411.19477v1)|null|
|**2024-11-29**|**Effective Fine-Tuning of Vision-Language Models for Accurate Galaxy Morphology Analysis**|Ruoqi Wang et.al.|[2411.19475v1](http://arxiv.org/abs/2411.19475v1)|null|
|**2024-11-29**|**Towards Understanding Retrieval Accuracy and Prompt Quality in RAG Systems**|Shengming Zhao et.al.|[2411.19463v1](http://arxiv.org/abs/2411.19463v1)|null|
|**2024-11-29**|**Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing**|Hosu Lee et.al.|[2411.19460v1](http://arxiv.org/abs/2411.19460v1)|null|
|**2024-11-29**|**Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension Ability**|Yujin Han et.al.|[2411.19456v1](http://arxiv.org/abs/2411.19456v1)|null|
|**2024-11-29**|**Learning Visual Abstract Reasoning through Dual-Stream Networks**|Kai Zhao et.al.|[2411.19451v1](http://arxiv.org/abs/2411.19451v1)|null|
|**2024-11-29**|**Adaptive Interactive Segmentation for Multimodal Medical Imaging via Selection Engine**|Zhi Li et.al.|[2411.19447v1](http://arxiv.org/abs/2411.19447v1)|null|
|**2024-11-29**|**Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models**|Tian Yu et.al.|[2411.19443v1](http://arxiv.org/abs/2411.19443v1)|[link](https://github.com/ictnlp/auto-rag)|
|**2024-11-29**|**Gradient Inversion Attack on Graph Neural Networks**|Divya Anand Sinha et.al.|[2411.19440v1](http://arxiv.org/abs/2411.19440v1)|null|
|**2024-11-29**|**Actions and Objects Pathways for Domain Adaptation in Video Question Answering**|Safaa Abdullahi Moallim Mohamud et.al.|[2411.19434v1](http://arxiv.org/abs/2411.19434v1)|null|
|**2024-11-28**|**AMO Sampler: Enhancing Text Rendering with Overshooting**|Xixi Hu et.al.|[2411.19415v1](http://arxiv.org/abs/2411.19415v1)|null|
|**2024-11-28**|**Concept-driven Off Policy Evaluation**|Ritam Majumdar et.al.|[2411.19395v1](http://arxiv.org/abs/2411.19395v1)|null|
|**2024-11-28**|**Marconi: Prefix Caching for the Era of Hybrid LLMs**|Rui Pan et.al.|[2411.19379v1](http://arxiv.org/abs/2411.19379v1)|null|
|**2024-11-28**|**Libra: Leveraging Temporal Images for Biomedical Radiology Analysis**|Xi Zhang et.al.|[2411.19378v1](http://arxiv.org/abs/2411.19378v1)|null|
|**2024-11-28**|**DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack Abilities**|Hui Dai et.al.|[2411.19360v1](http://arxiv.org/abs/2411.19360v1)|null|
|**2024-11-28**|**Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance**|Philipp Brauner et.al.|[2411.19356v1](http://arxiv.org/abs/2411.19356v1)|null|
|**2024-11-28**|**OMuleT: Orchestrating Multiple Tools for Practicable Conversational Recommendation**|Se-eun Yoon et.al.|[2411.19352v1](http://arxiv.org/abs/2411.19352v1)|null|
|**2024-11-28**|**CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections**|Mohamed Fazli Imam et.al.|[2411.19346v1](http://arxiv.org/abs/2411.19346v1)|null|
|**2024-11-28**|**An Adversarial Learning Approach to Irregular Time-Series Forecasting**|Heejeong Nam et.al.|[2411.19341v1](http://arxiv.org/abs/2411.19341v1)|null|
|**2024-11-28**|**Towards a Mechanistic Explanation of Diffusion Model Generalization**|Matthew Niedoba et.al.|[2411.19339v1](http://arxiv.org/abs/2411.19339v1)|null|
|**2024-11-28**|**PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning**|Shenghui Li et.al.|[2411.19335v1](http://arxiv.org/abs/2411.19335v1)|null|
|**2024-11-28**|**Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation**|Luca Barsellotti et.al.|[2411.19331v1](http://arxiv.org/abs/2411.19331v1)|null|
|**2024-11-28**|**Structured Object Language Modeling (SoLM): Native Structured Objects Generation Conforming to Complex Schemas with Self-Supervised Denoising**|Amir Tavanaei et.al.|[2411.19301v1](http://arxiv.org/abs/2411.19301v1)|null|
|**2024-11-28**|**Extracting Information in a Low-resource Setting: Case Study on Bioinformatics Workflows**|Clémence Sebe et.al.|[2411.19295v1](http://arxiv.org/abs/2411.19295v1)|null|
|**2024-11-28**|**On-chip Hyperspectral Image Segmentation with Fully Convolutional Networks for Scene Understanding in Autonomous Driving**|Jon Gutiérrez-Zaballa et.al.|[2411.19274v1](http://arxiv.org/abs/2411.19274v1)|null|
|**2024-11-28**|**Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks**|Jinu Nyachhyon et.al.|[2411.19244v1](http://arxiv.org/abs/2411.19244v1)|null|

#### Abstracts
##### **T2Vid: Translating Long Text into Multi-Image is the Catalyst for Video-LLMs**
2411.19951v1 by Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Tong Xu, Xing Sun, Ran He, Caifeng Shan, Enhong Chen

The success of Multimodal Large Language Models (MLLMs) in the image domain
has garnered wide attention from the research community. Drawing on previous
successful experiences, researchers have recently explored extending the
success to the video understanding realms. Apart from training from scratch, an
efficient way is to utilize the pre-trained image-LLMs, leading to two
mainstream approaches, i.e. zero-shot inference and further fine-tuning with
video data. In this work, our study of these approaches harvests an effective
data augmentation method. We first make a deeper inspection of the zero-shot
inference way and identify two limitations, i.e. limited generalization and
lack of temporal understanding capabilities. Thus, we further investigate the
fine-tuning approach and find a low learning efficiency when simply using all
the video data samples, which can be attributed to a lack of instruction
diversity. Aiming at this issue, we develop a method called T2Vid to synthesize
video-like samples to enrich the instruction diversity in the training corpus.
Integrating these data enables a simple and efficient training scheme, which
achieves performance comparable to or even superior to using full video
datasets by training with just 15% the sample size. Meanwhile, we find that the
proposed scheme can boost the performance of long video understanding without
training with long video samples. We hope our study will spark more thinking
about using MLLMs for video understanding and curation of high-quality data.
The code is released at https://github.com/xjtupanda/T2Vid.

摘要：多模态大语言模型 (MLLM) 在图像领域的成功引起了研究界的广泛关注。借鉴以往的成功经验，研究人员最近探索将成功扩展到视频理解领域。除了从头开始训练外，一种有效的方法是利用预训练的图像 LLM，从而产生两种主流方法，即零样本推理和使用视频数据进行进一步微调。在这项工作中，我们对这些方法的研究收获了一种有效的数据增强方法。我们首先对零样本推理方式进行更深入的检查，并找出两个限制，即有限的泛化能力和缺乏时间理解能力。因此，我们进一步研究了微调方法，并发现仅使用所有视频数据样本时的学习效率较低，这可归因于缺乏指令多样性。针对此问题，我们开发了一种名为 T2Vid 的方法来合成类视频样本，以丰富训练语料库中的指令多样性。集成这些数据可以实现简单高效的训练方案，仅通过训练 15% 的样本量，即可实现与使用完整视频数据集相当甚至更好的性能。同时，我们发现所提出的方案可以在不使用长视频样本进行训练的情况下提高长视频理解的性能。我们希望我们的研究将激发更多关于使用 MLLM 进行视频理解和高质量数据整理的思考。代码已发布在 https://github.com/xjtupanda/T2Vid。

##### **DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation**
2411.19946v1 by Zhiqiang Shen, Ammar Sherif, Zeyuan Yin, Shitong Shao

Recent advances in dataset distillation have led to solutions in two main
directions. The conventional batch-to-batch matching mechanism is ideal for
small-scale datasets and includes bi-level optimization methods on models and
syntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like
distribution matching, gradient matching, and weight trajectory matching.
Conversely, batch-to-global matching typifies decoupled methods, which are
particularly advantageous for large-scale datasets. This approach has garnered
substantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD,
and CDA. A primary challenge with the second approach is the lack of diversity
among syntheses within each class since samples are optimized independently and
the same global supervision signals are reused across different synthetic
images. In this study, we propose a new Diversity-driven EarlyLate Training
(DELT) scheme to enhance the diversity of images in batch-to-global matching
with less computation. Our approach is conceptually simple yet effective, it
partitions predefined IPC samples into smaller subtasks and employs local
optimizations to distill each subset into distributions from distinct phases,
reducing the uniformity induced by the unified optimization process. These
distilled images from the subtasks demonstrate effective generalization when
applied to the entire task. We conduct extensive experiments on CIFAR,
Tiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the
previous state-of-the-art by 2$\sim$5% on average across different datasets and
IPCs (images per class), increasing diversity per class by more than 5% while
reducing synthesis time by up to 39.3% for enhancing the training efficiency.
Code is available at: https://github.com/VILA-Lab/DELT.

摘要：<paragraph>資料集萃取的最新進展已導致兩個主要方向的解決方案。傳統的批次對批次匹配機制非常適合小規模資料集，包括模型和合成上的雙層最佳化方法，例如 FRePo、RCIG 和 RaT-BPTT，以及分佈匹配、梯度匹配和權重軌跡匹配等其他方法。相反，批次對全局匹配是解耦方法的典型，這對於大規模資料集特別有利。這種方法在社群中引起了極大的興趣，如在 SRe$^2$L、G-VBSM、WMDD 和 CDA 中所見。第二種方法的主要挑戰是每個類別內的合成缺乏多樣性，因為範例是獨立最佳化的，並且相同的全局監督訊號在不同的合成影像中重複使用。在本研究中，我們提出了一種新的多樣性驅動的早期後期訓練 (DELT) 方案，以增強批次對全局匹配中影像的多樣性，並減少運算。我們的做法在概念上既簡單又有效，它將預定義的 IPC 範例分割成更小的子任務，並採用局部最佳化將每個子集萃取成不同階段的分布，減少統一最佳化流程所導致的一致性。這些來自子任務的萃取影像在應用於整個任務時展現了有效的概化。我們對 CIFAR、Tiny-ImageNet、ImageNet-1K 及其子資料集進行了廣泛的實驗。我們的做法在不同的資料集和 IPC（每個類別的影像）中平均比先前的最新技術高出 2$\sim$5%，每個類別的多樣性增加了 5% 以上，同時將合成時間減少了 39.3%，以提高訓練效率。程式碼可在 https://github.com/VILA-Lab/DELT 取得。</paragraph>

##### **Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability**
2411.19943v1 by Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, Zhaopeng Tu

Large Language Models (LLMs) have exhibited remarkable performance on
reasoning tasks. They utilize autoregressive token generation to construct
reasoning trajectories, enabling the development of a coherent chain of
thought. In this work, we explore the impact of individual tokens on the final
outcomes of reasoning tasks. We identify the existence of ``critical tokens''
that lead to incorrect reasoning trajectories in LLMs. Specifically, we find
that LLMs tend to produce positive outcomes when forced to decode other tokens
instead of critical tokens. Motivated by this observation, we propose a novel
approach - cDPO - designed to automatically recognize and conduct token-level
rewards for the critical tokens during the alignment process. Specifically, we
develop a contrastive estimation approach to automatically identify critical
tokens. It is achieved by comparing the generation likelihood of positive and
negative models. To achieve this, we separately fine-tune the positive and
negative models on various reasoning trajectories, consequently, they are
capable of identifying identify critical tokens within incorrect trajectories
that contribute to erroneous outcomes. Moreover, to further align the model
with the critical token information during the alignment process, we extend the
conventional DPO algorithms to token-level DPO and utilize the differential
likelihood from the aforementioned positive and negative model as important
weight for token-level DPO learning.Experimental results on GSM8K and MATH500
benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math
(7B) demonstrate the effectiveness of the propsoed approach cDPO.

摘要：大型語言模型 (LLM) 在推理任務上展現出顯著的表現。它們利用自迴歸符號產生來構建推理軌跡，從而能夠發展出一條連貫的思維鏈。在這項工作中，我們探討了個別符號對推理任務最終結果的影響。我們發現 LLM 中存在導致推理軌跡不正確的「關鍵符號」。具體來說，我們發現 LLM 傾向於在被迫解碼其他符號而不是關鍵符號時產生正向結果。受此觀察結果啟發，我們提出了一種新穎的方法 - cDPO - 旨在自動識別並在對齊過程中對關鍵符號進行符號層級的獎勵。具體來說，我們開發了一種對比估計方法來自動識別關鍵符號。這是通過比較正向和負向模型的產生可能性來實現的。為此，我們分別對正向和負向模型進行微調，針對各種推理軌跡，因此，它們能夠識別出導致錯誤結果的不正確軌跡中的關鍵符號。此外，為了進一步在對齊過程中將模型與關鍵符號資訊對齊，我們將傳統的 DPO 演算法擴展到符號層級的 DPO，並利用上述正向和負向模型的差異可能性作為符號層級 DPO 學習的重要權重。在 GSM8K 和 MATH500 基準上使用兩個廣泛使用的模型 Llama-3（8B 和 70B）和 deepseek-math（7B）進行的實驗結果證明了所提出的方法 cDPO 的有效性。

##### **Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark**
2411.19941v1 by Joseph Heyward, João Carreira, Dima Damen, Andrew Zisserman, Viorica Pătrăucean

Following the successful 2023 edition, we organised the Second Perception
Test challenge as a half-day workshop alongside the IEEE/CVF European
Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking
state-of-the-art video models and measuring the progress since last year using
the Perception Test benchmark. This year, the challenge had seven tracks (up
from six last year) and covered low-level and high-level tasks, with language
and non-language interfaces, across video, audio, and text modalities; the
additional track covered hour-long video understanding and introduced a novel
video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks
were: object tracking, point tracking, temporal action localisation, temporal
sound localisation, multiple-choice video question-answering, grounded video
question-answering, and hour-long video question-answering. We summarise in
this report the challenge tasks and results, and introduce in detail the novel
hour-long video QA benchmark 1h-walk VQA.

摘要：繼 2023 年成功舉辦後，我們在 IEEE/CVF 歐洲電腦視覺會議 (ECCV) 2024 期間舉辦了為期半天的工作坊，作為第二屆感知測試挑戰賽，目標是對現有最先進的影片模型進行基準測試，並使用感知測試基準衡量自去年以來的進度。今年，挑戰賽有七個軌道（比去年增加一個），涵蓋低階和高階任務，並跨越影片、音訊和文字模式，使用語言和非語言介面；新增的軌道涵蓋長達一小時的影片理解，並引入了新穎的影片問答基準 1h-walk VQA。整體而言，不同軌道中的任務為：物件追蹤、點追蹤、時序動作定位、時序聲音定位、多選影片問答、基礎影片問答和長達一小時的影片問答。我們在這份報告中總結了挑戰任務和結果，並詳細介紹新穎的長達一小時影片問答基準 1h-walk VQA。

##### **VLSBench: Unveiling Visual Leakage in Multimodal Safety**
2411.19939v1 by Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao

Safety concerns of Multimodal large language models (MLLMs) have gradually
become an important problem in various applications. Surprisingly, previous
works indicate a counter-intuitive phenomenon that using textual unlearning to
align MLLMs achieves comparable safety performances with MLLMs trained with
image-text pairs. To explain such a counter-intuitive phenomenon, we discover a
visual safety information leakage (VSIL) problem in existing multimodal safety
benchmarks, i.e., the potentially risky and sensitive content in the image has
been revealed in the textual query. In this way, MLLMs can easily refuse these
sensitive text-image queries according to textual queries. However, image-text
pairs without VSIL are common in real-world scenarios and are overlooked by
existing multimodal safety benchmarks. To this end, we construct multimodal
visual leakless safety benchmark (VLSBench) preventing visual safety leakage
from image to textual query with 2.4k image-text pairs. Experimental results
indicate that VLSBench poses a significant challenge to both open-source and
close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.
This study demonstrates that textual alignment is enough for multimodal safety
scenarios with VSIL, while multimodal alignment is a more promising solution
for multimodal safety scenarios without VSIL. Please see our code and data at:
http://hxhcreate.github.io/VLSBench

摘要：多模态大型语言模型 (MLLM) 的安全性问题已逐渐成为各种应用中的重要问题。令人惊讶的是，先前研究表明，使用文本式反学习来调整 MLLM，可达到与使用图像文本对训练的 MLLM 相当的安全性表现。为了解释这种反直觉现象，我们发现现有多模态安全性基准中存在视觉安全性信息泄漏 (VSIL) 问题，即图像中潜在的危险和敏感内容已在文本查询中揭露。通过这种方式，MLLM 可以根据文本查询轻松拒绝这些敏感的文本图像查询。然而，在现实世界场景中，没有 VSIL 的图像文本对很常见，并且被现有的多模态安全性基准所忽略。为此，我们构建了多模态视觉无泄漏安全性基准 (VLSBench)，防止视觉安全性从图像泄漏到文本查询，其中包含 2.4k 图像文本对。实验结果表明，VLSBench 对开源和闭源 MLLM 构成了重大挑战，包括 LLaVA、Qwen2-VL、Llama3.2-Vision 和 GPT-4o。这项研究表明，文本调整对于具有 VSIL 的多模态安全性场景来说已经足够，而多模态调整对于没有 VSIL 的多模态安全性场景来说是一个更有前途的解决方案。请参阅我们的代码和数据：http://hxhcreate.github.io/VLSBench

##### **On Domain-Specific Post-Training for Multimodal Large Language Models**
2411.19930v1 by Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang

Recent years have witnessed the rapid development of general multimodal large
language models (MLLMs). However, adapting general MLLMs to specific domains,
such as scientific fields and industrial applications, remains less explored.
This paper systematically investigates domain adaptation of MLLMs through
post-training, focusing on data synthesis, training pipelines, and task
evaluation. (1) Data Synthesis: Using open-source models, we develop a visual
instruction synthesizer that effectively generates diverse visual instruction
tasks from domain-specific image-caption pairs. Our synthetic tasks surpass
those generated by manual rules, GPT-4, and GPT-4V in enhancing the
domain-specific performance of MLLMs. (2) Training Pipeline: While the
two-stage training--initially on image-caption pairs followed by visual
instruction tasks--is commonly adopted for developing general MLLMs, we apply a
single-stage training pipeline to enhance task diversity for domain-specific
post-training. (3) Task Evaluation: We conduct experiments in two domains,
biomedicine and food, by post-training MLLMs of different sources and scales
(e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM
performance on various domain-specific tasks. To support further research in
MLLM domain adaptation, we will open-source our implementations.

摘要：近年來，一般多模態大型語言模型 (MLLM) 的發展十分迅速。然而，將一般 MLLM 適應到特定領域（例如科學領域和產業應用）的研究仍較少。本文透過後訓練系統性地探討 MLLM 的領域適應，重點放在資料合成、訓練管線和任務評估上。(1) 資料合成：我們使用開源模型，開發出一個視覺指令合成器，可有效地從特定領域的影像標題對中產生多樣的視覺指令任務。我們的合成任務超越了手動規則、GPT-4 和 GPT-4V 所產生的任務，以增強 MLLM 的特定領域效能。(2) 訓練管線：雖然兩階段訓練（最初在影像標題對上進行，然後在視覺指令任務上進行）通常用於開發一般 MLLM，但我們採用單階段訓練管線來增強特定領域後訓練的任務多樣性。(3) 任務評估：我們在生物醫學和食品這兩個領域進行實驗，透過後訓練不同來源和規模的 MLLM（例如 Qwen2-VL-2B、LLaVA-v1.6-8B、Llama-3.2-11B），然後評估 MLLM 在各種特定領域任務上的效能。為了支持 MLLM 領域適應的後續研究，我們將開放我們的實作原始碼。

##### **SIMS: Simulating Human-Scene Interactions with Real World Script Planning**
2411.19921v1 by Wenjia Wang, Liang Pan, Zhiyang Dou, Zhouyingcheng Liao, Yuke Lou, Lei Yang, Jingbo Wang, Taku Komura

Simulating long-term human-scene interaction is a challenging yet fascinating
task. Previous works have not effectively addressed the generation of long-term
human scene interactions with detailed narratives for physics-based animation.
This paper introduces a novel framework for the planning and controlling of
long-horizon physical plausible human-scene interaction. On the one hand, films
and shows with stylish human locomotions or interactions with scenes are
abundantly available on the internet, providing a rich source of data for
script planning. On the other hand, Large Language Models (LLMs) can understand
and generate logical storylines.
  This motivates us to marry the two by using an LLM-based pipeline to extract
scripts from videos, and then employ LLMs to imitate and create new scripts,
capturing complex, time-series human behaviors and interactions with
environments. By leveraging this, we utilize a dual-aware policy that achieves
both language comprehension and scene understanding to guide character motions
within contextual and spatial constraints. To facilitate training and
evaluation, we contribute a comprehensive planning dataset containing diverse
motion sequences extracted from real-world videos and expand them with large
language models. We also collect and re-annotate motion clips from existing
kinematic datasets to enable our policy learn diverse skills. Extensive
experiments demonstrate the effectiveness of our framework in versatile task
execution and its generalization ability to various scenarios, showing
remarkably enhanced performance compared with existing methods. Our code and
data will be publicly available soon.

摘要：模擬長期的人類場景互動是一項具有挑戰性但迷人的任務。先前的作品並未有效解決基於物理動畫的長期人類場景互動的生成問題。本文介紹了一個用於規劃和控制長期物理可信人類場景互動的新框架。一方面，網路上充斥著具有時尚人類運動或場景互動的電影和節目，為腳本規劃提供了豐富的數據來源。另一方面，大型語言模型 (LLM) 可以理解和生成邏輯故事情節。這促使我們透過使用基於 LLM 的管道從影片中提取腳本，然後使用 LLM 來模仿和建立新的腳本，捕捉複雜的時間序列人類行為和與環境的互動。透過利用這一點，我們利用一個雙重感知策略，它實現了語言理解和場景理解，以在語境和空間限制內引導角色動作。為了促進訓練和評估，我們提供了一個全面的規劃資料集，其中包含從真實影片中提取的多樣化動作序列，並使用大型語言模型對其進行擴充。我們還會從現有的運動資料集中收集和重新註解動作片段，以使我們的策略學習多樣化的技能。廣泛的實驗證明了我們的框架在多功能任務執行中的有效性，以及其對各種場景的泛化能力，與現有方法相比，表現出顯著的提升。我們的程式碼和資料將很快公開。

##### **Handling irresolvable conflicts in the Semantic Web: an RDF-based conflict-tolerant version of the Deontic Traditional Scheme**
2411.19918v1 by Livio Robaldo, Gianluca Pozzato

This paper presents a new ontology that implements the well-known Deontic
Traditional Scheme in RDFs and SPARQL, fit to handle irresolvable conflicts,
i.e., situations in which two or more statements prescribe conflicting
obligations, prohibitions, or permissions, with none of them being "stronger"
than the other one(s). In our view, this paper marks a significant advancement
in standard theoretical research in formal Deontic Logic. Most contemporary
approaches in this field are confined to the propositional level, mainly focus
on the notion of obligation, and lack implementations. The proposed framework
is encoded in RDF, which is not only a first-order language but also the most
widely used knowledge representation language, as it forms the foundation of
the Semantic Web. Moreover, the proposed computational ontology formalizes all
deontic modalities defined in the Deontic Traditional Scheme, without
specifically focusing on obligations, and offers constructs to model and reason
with various types of irresolvable conflicts, violations, and the interaction
between deontic modalities and contextual constraints in a given state of
affairs. To the best of our knowledge, no existing approach in the literature
addresses all these aspects within a unified integrated framework. All examples
presented and discussed in this paper, together with Java code and clear
instructions to re-execute them locally, are available at
https://github.com/liviorobaldo/conflict-tolerantDeonticTraditionalScheme

摘要：本文提出了一個新的本体，它在 RDF 和 SPARQL 中實作了著名的義務傳統方案，適合處理無法解決的衝突，也就是，兩個或多個陳述規定了衝突的義務、禁止或許可，而沒有任何一個比其他一個「更強」。依據我們的觀點，本文標誌著形式義務邏輯中標準理論研究的重大進展。這個領域中大多數當代方法都侷限於命題層次，主要關注義務的概念，而且缺乏實作。所提出的架構編碼在 RDF 中，它不僅是一階語言，也是使用最廣泛的知識表示語言，因為它構成了語意網路的基礎。此外，所提出的計算本体形式化了義務傳統方案中定義的所有義務形式，而沒有特別關注義務，並提供建構來建模和推理各種類型的無法解決的衝突、違規，以及在特定事務狀態中義務形式和脈絡約束之間的互動。據我們所知，文獻中沒有現有的方法在一個統一的整合架構中處理所有這些面向。本文中提供和討論的所有範例，連同 Java 程式碼和重新在本地執行它們的明確說明，都可以在 https://github.com/liviorobaldo/conflict-tolerantDeonticTraditionalScheme 取得

##### **Quantifying the synthetic and real domain gap in aerial scene understanding**
2411.19913v1 by Alina Marcu

Quantifying the gap between synthetic and real-world imagery is essential for
improving both transformer-based models - that rely on large volumes of data -
and datasets, especially in underexplored domains like aerial scene
understanding where the potential impact is significant. This paper introduces
a novel methodology for scene complexity assessment using Multi-Model Consensus
Metric (MMCM) and depth-based structural metrics, enabling a robust evaluation
of perceptual and structural disparities between domains. Our experimental
analysis, utilizing real-world (Dronescapes) and synthetic (Skyscenes)
datasets, demonstrates that real-world scenes generally exhibit higher
consensus among state-of-the-art vision transformers, while synthetic scenes
show greater variability and challenge model adaptability. The results
underline the inherent complexities and domain gaps, emphasizing the need for
enhanced simulation fidelity and model generalization. This work provides
critical insights into the interplay between domain characteristics and model
performance, offering a pathway for improved domain adaptation strategies in
aerial scene understanding.

摘要：量化合成影像與真實世界影像之間的差距對於改進依賴大量資料的Transformer模型和資料集至關重要，特別是在潛在影響重大的未探索領域中，例如空中場景理解。本文介紹了一種使用多模型共識指標 (MMCM) 和基於深度的結構指標對場景複雜性進行評估的新方法，從而能夠對不同領域之間的感知和結構差異進行穩健評估。我們的實驗分析利用真實世界 (Dronescapes) 和合成 (Skyscenes) 資料集，證明真實世界的場景通常在最先進的視覺Transformer之間表現出更高的共識，而合成場景則表現出更大的變異性和挑戰模型適應性。結果強調了固有的複雜性和領域差距，強調了增強模擬保真度和模型泛化的必要性。這項工作提供了對領域特徵和模型效能之間相互作用的重要見解，為空中場景理解中改進的領域適應策略提供了途徑。

##### **Classical and Quantum Algorithms for the Deterministic L-system Inductive Inference Problem**
2411.19906v1 by Ali Lotfi, Ian McQuillan, Steven Rayan

L-systems can be made to model and create simulations of many biological
processes, such as plant development. Finding an L-system for a given process
is typically solved by hand, by experts, in a hugely time-consuming process. It
would be significant if this could be done automatically from data, such as
from sequences of images. In this paper, we are interested in inferring a
particular type of L-system, deterministic context-free L-system (D0L-system)
from a sequence of strings. We introduce the characteristic graph of a sequence
of strings, which we then utilize to translate our problem (inferring
D0L-system) in polynomial time into the maximum independent set problem (MIS)
and the SAT problem. After that, we offer a classical exact algorithm and an
approximate quantum algorithm for the problem.

摘要：L 系統可以模擬和建立許多生物過程，例如植物生長。找到特定過程的 L 系統通常是由專家手動解決，這個過程非常耗時。如果這項工作可以從資料中自動完成，例如從影像序列中，將會是一項重大突破。在本文中，我們有興趣從一連串字串推論出特定類型的 L 系統，即確定性無文法 L 系統 (D0L 系統)。我們引入了字串序列的特徵圖，然後利用它在多項式時間內將我們的問題 (推論 D0L 系統) 轉換為最大獨立集合問題 (MIS) 和 SAT 問題。之後，我們提供了一個經典的精確演算法和一個近似的量子演算法來解決這個問題。

##### **PDDLFuse: A Tool for Generating Diverse Planning Domains**
2411.19886v1 by Vedant Khandelwal, Amit Sheth, Forest Agostinelli

Various real-world challenges require planning algorithms that can adapt to a
broad range of domains. Traditionally, the creation of planning domains has
relied heavily on human implementation, which limits the scale and diversity of
available domains. While recent advancements have leveraged generative AI
technologies such as large language models (LLMs) for domain creation, these
efforts have predominantly focused on translating existing domains from natural
language descriptions rather than generating novel ones. In contrast, the
concept of domain randomization, which has been highly effective in
reinforcement learning, enhances performance and generalizability by training
on a diverse array of randomized new domains. Inspired by this success, our
tool, PDDLFuse, aims to bridge this gap in Planning Domain Definition Language
(PDDL). PDDLFuse is designed to generate new, diverse planning domains that can
be used to validate new planners or test foundational planning models. We have
developed methods to adjust the domain generators parameters to modulate the
difficulty of the domains it generates. This adaptability is crucial as
existing domain-independent planners often struggle with more complex problems.
Initial tests indicate that PDDLFuse efficiently creates intricate and varied
domains, representing a significant advancement over traditional domain
generation methods and making a contribution towards planning research.

摘要：各種現實世界的挑戰需要能夠適應廣泛領域的規劃演算法。傳統上，規劃領域的建立高度依賴於人工實作，這限制了可用領域的規模和多樣性。雖然最近的進展已利用生成式 AI 技術，例如大型語言模型 (LLM) 來進行領域建立，但這些努力主要集中於從自然語言描述中翻譯現有領域，而不是生成新的領域。相比之下，在強化學習中非常有效的領域隨機化的概念，透過在各種隨機新領域上進行訓練，增強了效能和泛化能力。受到這種成功的啟發，我們的工具 PDDLFuse 旨在彌補規劃領域定義語言 (PDDL) 中的這一差距。PDDLFuse 旨在產生新的、多樣化的規劃領域，可用於驗證新的規劃器或測試基礎規劃模型。我們已開發出調整領域產生器參數的方法，以調整其所產生領域的難度。這種適應能力至關重要，因為現有的與領域無關的規劃器通常難以處理更複雜的問題。初步測試表明，PDDLFuse 有效地建立了複雜且多樣的領域，代表了傳統領域產生方法的重大進步，並為規劃研究做出了貢獻。

##### **LUMIA: Linear probing for Unimodal and MultiModal Membership Inference A!acks leveraging internal LLM states**
2411.19876v1 by Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux, Joaquin Garcia-Alfaro

Large Language Models (LLMs) are increasingly used in a variety of
applications, but concerns around membership inference have grown in parallel.
Previous efforts focus on black-to-grey-box models, thus neglecting the
potential benefit from internal LLM information. To address this, we propose
the use of Linear Probes (LPs) as a method to detect Membership Inference
Attacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed
LUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner
workings. We test this method across several model architectures, sizes and
datasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA
achieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous
techniques. Remarkably, LUMIA reaches AUC>60% in 65.33% of cases -- an
increment of 46.80% against the state of the art. Furthermore, our approach
reveals key insights, such as the model layers where MIAs are most detectable.
In multimodal models, LPs indicate that visual inputs can significantly
contribute to detect MIAs -- AUC>60% is reached in 85.90% of experiments.

摘要：大型語言模型 (LLM) 在各種應用中越來越廣泛地使用，但對成員推論的擔憂也與日俱增。先前的努力主要集中在黑箱到灰箱模型，因此忽視了 LLM 內部資訊的潛在好處。為了解決這個問題，我們建議使用線性探測 (LP) 作為一種方法，透過檢查 LLM 的內部激活來偵測成員推論攻擊 (MIA)。我們的方法稱為 LUMIA，它逐層應用 LP 以取得關於模型內部運作的細粒度資料。我們在幾個模型架構、大小和資料集上測試此方法，包括單模態和多模態任務。在單模態 MIA 中，LUMIA 在曲線下面積 (AUC) 上比先前的技術平均增加了 15.71%。值得注意的是，LUMIA 在 65.33% 的案例中達到 AUC>60%，與目前技術相比增加了 46.80%。此外，我們的做法揭示了關鍵見解，例如 MIA 最容易被偵測到的模型層。在多模態模型中，LP 指出視覺輸入可以顯著有助於偵測 MIA，在 85.90% 的實驗中達到 AUC>60%。

##### **Enhanced anomaly detection in well log data through the application of ensemble GANs**
2411.19875v1 by Abdulrahman Al-Fakih, A. Koeshidayatullah, Tapan Mukerji, SanLinn I. Kaka

Although generative adversarial networks (GANs) have shown significant
success in modeling data distributions for image datasets, their application to
structured or tabular data, such as well logs, remains relatively
underexplored. This study extends the ensemble GANs (EGANs) framework to
capture the distribution of well log data and detect anomalies that fall
outside of these distributions. The proposed approach compares the performance
of traditional methods, such as Gaussian mixture models (GMMs), with EGANs in
detecting anomalies outside the expected data distributions. For the gamma ray
(GR) dataset, EGANs achieved a precision of 0.62 and F1 score of 0.76,
outperforming GMM's precision of 0.38 and F1 score of 0.54. Similarly, for
travel time (DT), EGANs achieved a precision of 0.70 and F1 score of 0.79,
surpassing GMM 0.56 and 0.71. In the neutron porosity (NPHI) dataset, EGANs
recorded a precision of 0.53 and F1 score of 0.68, outshining GMM 0.47 and
0.61. For the bulk density (RHOB) dataset, EGANs achieved a precision of 0.52
and an F1 score of 0.67, slightly outperforming GMM, which yielded a precision
of 0.50 and an F1 score of 0.65. This work's novelty lies in applying EGANs for
well log data analysis, showcasing their ability to learn data patterns and
identify anomalies that deviate from them. This approach offers more reliable
anomaly detection compared to traditional methods like GMM. The findings
highlight the potential of EGANs in enhancing anomaly detection for well log
data, delivering significant implications for optimizing drilling strategies
and reservoir management through more accurate, data-driven insights into
subsurface characterization.

摘要：<paragraph>儘管生成對抗網路 (GAN) 在建模影像資料集的資料分佈方面已展現顯著的成功，但其在結構化或表格資料（例如測井資料）的應用仍相對未被充分探討。本研究延伸了整體 GAN (EGAN) 架構，以擷取測井資料的分布並偵測落在這些分布之外的異常值。所提出的方法比較了傳統方法（例如高斯混合模型 (GMM)）與 EGAN 在偵測預期資料分布之外的異常值方面的效能。對於伽馬射線 (GR) 資料集，EGAN 達到了 0.62 的精準度和 0.76 的 F1 分數，優於 GMM 的 0.38 精準度和 0.54 F1 分數。類似地，對於行進時間 (DT)，EGAN 達到了 0.70 的精準度和 0.79 的 F1 分數，超越了 GMM 的 0.56 和 0.71。在中子孔隙度 (NPHI) 資料集，EGAN 記錄了 0.53 的精準度和 0.68 的 F1 分數，勝過 GMM 的 0.47 和 0.61。對於體積密度 (RHOB) 資料集，EGAN 達到了 0.52 的精準度和 0.67 的 F1 分數，略優於 GMM，後者產生了 0.50 的精準度和 0.65 的 F1 分數。這項工作的創新之處在於應用 EGAN 進行測井資料分析，展示其學習資料模式和識別偏離這些模式的異常值的能力。與 GMM 等傳統方法相比，此方法提供了更可靠的異常值偵測。這些發現突顯了 EGAN 在加強測井資料異常值偵測方面的潛力，透過更準確、資料驅動的地下特徵洞察，為最佳化鑽探策略和儲層管理帶來重大影響。</paragraph>

##### **DeMo: Decoupled Momentum Optimization**
2411.19870v1 by Bowen Peng, Jeffrey Quesnelle, Diederik P. Kingma

Training large neural networks typically requires sharing gradients between
accelerators through specialized high-speed interconnects. Drawing from the
signal processing principles of frequency decomposition and energy compaction,
we demonstrate that synchronizing full optimizer states and model parameters
during training is unnecessary. By decoupling momentum updates and allowing
controlled divergence in optimizer states across accelerators, we achieve
improved convergence compared to state-of-the-art optimizers. We introduce
{\textbf{De}}coupled {\textbf{Mo}}mentum (DeMo), a fused optimizer and data
parallel algorithm that reduces inter-accelerator communication requirements by
several orders of magnitude. This enables training of large neural networks
even with limited network bandwidth and heterogeneous hardware. Our method is
topology-agnostic and architecture-independent and supports scalable
clock-synchronous distributed training with negligible compute and memory
overhead. Empirical results show that models trained with DeMo match or exceed
the performance of equivalent models trained with AdamW, while eliminating the
need for high-speed interconnects when pre-training large scale foundation
models. An open source reference PyTorch implementation is published on GitHub
at https://github.com/bloc97/DeMo

摘要：訓練大型神經網路通常需要透過特殊的高速互連來分享加速器之間的梯度。我們從頻率分解和能量壓縮的訊號處理原理中擷取靈感，證明在訓練期間同步完整的最佳化器狀態和模型參數是不必要的。透過解耦動量更新並允許加速器之間的最佳化器狀態受控分歧，我們與最先進的最佳化器相比，達到了更佳的收斂性。我們引入了{\textbf{De}}coupled {\textbf{Mo}}mentum (DeMo)，一個融合的最佳化器和資料平行演算法，它將加速器間的通訊需求降低了幾個數量級。這使得即使在網路頻寬有限和硬體異質的情況下，也能訓練大型神經網路。我們的模型與拓撲無關且與架構無關，並支援可擴充的時脈同步分散式訓練，且運算和記憶體負擔可以忽略不計。經驗結果顯示，使用 DeMo 訓練的模型與使用 AdamW 訓練的等效模型相匹配或優於後者，同時在預訓練大型基礎模型時，消除了對高速互連的需求。一個開放原始碼的 PyTorch 參考實作已發佈在 GitHub 上，網址為 https://github.com/bloc97/DeMo

##### **AIDetx: a compression-based method for identification of machine-learning generated text**
2411.19869v1 by Leonardo Almeida, Pedro Rodrigues, Diogo Magalhães, Armando J. Pinho, Diogo Pratas

This paper introduces AIDetx, a novel method for detecting machine-generated
text using data compression techniques. Traditional approaches, such as deep
learning classifiers, often suffer from high computational costs and limited
interpretability. To address these limitations, we propose a compression-based
classification framework that leverages finite-context models (FCMs). AIDetx
constructs distinct compression models for human-written and AI-generated text,
classifying new inputs based on which model achieves a higher compression
ratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores
exceeding 97% and 99%, respectively, highlighting its high accuracy. Compared
to current methods, such as large language models (LLMs), AIDetx offers a more
interpretable and computationally efficient solution, significantly reducing
both training time and hardware requirements (e.g., no GPUs needed). The full
implementation is publicly available at https://github.com/AIDetx/AIDetx.

摘要：本文介紹 AIDetx，這是一種使用資料壓縮技術來偵測機器產生的文字的新方法。傳統方法，例如深度學習分類器，通常會產生高運算成本和有限的可解釋性。為了解決這些限制，我們提出一個基於壓縮的分類架構，利用有限語境模型 (FCM)。AIDetx 為人工撰寫和 AI 生成的文字建構不同的壓縮模型，根據哪個模型達到較高的壓縮率來對新輸入進行分類。我們在兩個基準資料集上評估 AIDetx，分別達到 F1 分數超過 97% 和 99%，突顯其高準確度。與目前的方法（例如大型語言模型 (LLM)）相比，AIDetx 提供更具可解釋性且運算效率更高的解決方案，大幅減少訓練時間和硬體需求（例如，不需要 GPU）。完整實作已公開於 https://github.com/AIDetx/AIDetx。

##### **Reverse Thinking Makes LLMs Stronger Reasoners**
2411.19865v1 by Justin Chih-Yao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, Tomas Pfister

Reverse thinking plays a crucial role in human reasoning. Humans can reason
not only from a problem to a solution but also in reverse, i.e., start from the
solution and reason towards the problem. This often enhances overall reasoning
performance as it enables consistency checks between their forward and backward
thinking. To enable Large Language Models (LLMs) to perform reverse thinking,
we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data
augmentation and learning objectives. In RevThink, we augment the dataset by
collecting structured forward-backward reasoning from a teacher model,
consisting of: (1) the original question, (2) forward reasoning, (3) backward
question, and (4) backward reasoning. We then employ three objectives to train
a smaller student model in a multi-task learning fashion: (a) generate forward
reasoning from a question, (b) generate a backward question from a question,
and (c) generate backward reasoning from the backward question. Experiments
across 12 datasets covering commonsense, math, and logical reasoning show an
average 13.53% improvement over the student model's zero-shot performance and a
6.84% improvement over the strongest knowledge distillation baselines.
Moreover, our method demonstrates sample efficiency -- using only 10% of the
correct forward reasoning from the training data, it outperforms a standard
fine-tuning method trained on 10x more forward reasoning. RevThink also
exhibits strong generalization to out-of-distribution held-out datasets.

摘要：逆向思考在人類推理中扮演著至關重要的角色。人類不僅能從問題推論出解答，也能反向推理，即從解答出發，推論出問題。這常常能提升整體推理表現，因為它能讓正向與反向思考之間進行一致性檢查。為了讓大型語言模型（LLM）能進行反向思考，我們引入了反向增強思考（RevThink），一個由資料擴充與學習目標組成的架構。在 RevThink 中，我們透過從教師模型收集結構化的正向反向推理來擴充資料集，包含：(1) 原始問題，(2) 正向推理，(3) 反向問題，以及 (4) 反向推理。接著我們採用三個目標，以多任務學習的方式訓練較小的學生模型：(a) 從問題產生正向推理，(b) 從問題產生反向問題，以及 (c) 從反向問題產生反向推理。涵蓋常識、數學和邏輯推理的 12 個資料集的實驗顯示，與學生模型的零次學習表現相比，平均提升了 13.53%，與最強的知識提煉基準相比，提升了 6.84%。此外，我們的模型展現出樣本效率——僅使用訓練資料中 10% 的正確正向推理，就優於在多 10 倍正向推理上訓練的標準微調方法。RevThink 也展現出對分布外保留資料集的強大泛化能力。

##### **What fifty-one years of Linguistics and Artificial Intelligence research tell us about their correlation: A scientometric review**
2411.19858v1 by Mohammed Q. Shormani

There is a strong correlation between linguistics and artificial intelligence
(AI), best manifested by deep learning language models. This study provides a
thorough scientometric analysis of this correlation, synthesizing the
intellectual production during 51 years, from 1974 to 2024. It involves 5750
Web of Science-indexed articles published in 2124 journals, which are written
by 20835 authors belonging to 13773 research centers in 794 countries. Two
powerful software, viz., CiteSpace and VOSviewer, were used to generate mapping
visualizations of the intellectual landscape, trending issues and (re)emerging
hotspots. The results indicate that in the 1980s and 1990s, linguistics and AI
research was not robust, characterized by unstable publication over time. It
has, however, witnessed a remarkable increase of publication since then,
reaching 1478 articles in 2023, and 546 articles in January-March timespan in
2024, involving emerging issues and hotspots, addressing new horizons, new
topics, and launching new applications and powerful deep learning language
models including ChatGPT.

摘要：語言學與人工智慧 (AI) 之間有密切的關聯，而深度學習語言模型最能體現這一點。本研究提供對此關聯的全面科學計量分析，綜合了 1974 年至 2024 年 51 年來的知識產出。這涉及 2124 份期刊發表的 5750 篇 Web of Science 所索引的文章，這些文章是由 794 個國家/地區 13773 個研究中心中的 20835 位作者所撰寫。兩個強大的軟體，即 CiteSpace 和 VOSviewer，用於產生知識領域、趨勢議題和（重新）浮現熱點的映射視覺化。結果表明，在 1980 年代和 1990 年代，語言學和 AI 研究並不強勁，其特點是隨著時間推移而發表的文章不穩定。然而，從那時起，它見證了出版物的顯著增加，在 2023 年達到 1478 篇文章，在 2024 年的 1 月至 3 月期間達到 546 篇文章，涉及新興議題和熱點，探討新的視野、新的主題，並推出新的應用程式和強大的深度學習語言模型，包括 ChatGPT。

##### **Artificial intelligence contribution to translation industry: looking back and forward**
2411.19855v1 by Mohammed Q. Shormani

This study provides a comprehensive analysis of artificial intelligence (AI)
contribution to translation industry (ACTI) research, synthesizing it over
forty-one years from 1980-2024. 13220 articles were retrieved from three
sources, namely WoS, Scopus, and Lens. We provided two types of analysis, viz.,
scientometric and thematic, focusing on cluster, subject categories, keywords,
burstness, centrality and research centers as for the former. For the latter,
we thematically review 18 articles, selected purposefully from the articles
involved, centering on purpose, approach, findings, and contribution to ACTI
future directions. The findings reveal that in the past AI contribution to
translation industry was not rigorous, resulting in rule-based machine
translation and statistical machine translation whose output was not
satisfactory. However, the more AI develops, the more machine translation
develops, incorporating Neural Networking Algorithms and (Deep) Language
Learning Models like ChatGPT whose translation output has developed
considerably. However, much rigorous research is still needed to overcome
several problems encountering translation industry, specifically concerning
low-source languages, multi-dialectical and free word order languages, and
cultural and religious registers.

摘要：本研究提供了對人工智慧 (AI) 對翻譯產業 (ACTI) 研究的全面分析，綜合了 1980-2024 年 41 年來的研究。從 WoS、Scopus 和 Lens 三個來源中檢索了 13220 篇文章。我們提供了兩種分析，即科學計量和主題分析，重點關注群集、主題類別、關鍵字、爆發性、中心性和研究中心。對於後者，我們有目的地從所涉及的文章中選擇了 18 篇文章進行主題回顧，重點關注目的、方法、發現和對 ACTI 未來方向的貢獻。研究結果表明，過去 AI 對翻譯產業的貢獻並不嚴謹，導致基於規則的機器翻譯和統計機器翻譯的輸出並不令人滿意。然而，隨著 AI 的發展，機器翻譯也得到了發展，結合了神經網路演算法和像 ChatGPT 這樣的（深度）語言學習模型，其翻譯輸出有了顯著的發展。然而，仍然需要大量的嚴謹研究來克服翻譯產業遇到的幾個問題，特別是關於低資源語言、多方言和自由語序語言以及文化和宗教註冊。

##### **Scaling Transformers for Low-Bitrate High-Quality Speech Coding**
2411.19842v1 by Julian D Parker, Anton Smirnov, Jordi Pons, CJ Carr, Zack Zukowski, Zach Evans, Xubo Liu

The tokenization of speech with neural audio codec models is a vital part of
modern AI pipelines for the generation or understanding of speech, alone or in
a multimodal context. Traditionally such tokenization models have concentrated
on low parameter-count architectures using only components with strong
inductive biases. In this work we show that by scaling a transformer
architecture with large parameter count to this problem, and applying a
flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to
reach state-of-the-art speech quality at extremely low bit-rates of $400$ or
$700$ bits-per-second. The trained models strongly out-perform existing
baselines in both objective and subjective tests.

摘要：神經音訊編解碼器模型的語音標記化是現代 AI 管道中用於產生或理解語音（單獨或在多模態上下文中）的重要部分。傳統上，此類標記化模型專注於僅使用具有強感應偏差的組件的低參數計數架構。在這項工作中，我們展示了通過將具有大量參數計數的轉換器架構擴展到這個問題，並應用基於靈活有限標量量化 (FSQ) 的瓶頸，可以在極低的 $400$ 或 $700$ 位元率達到最先進的語音品質。訓練後的模型在客觀和主觀測試中都明顯優於現有的基準。

##### **Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation**
2411.19832v1 by Dimosthenis Antypas, Indira Sen, Carla Perez-Almendros, Jose Camacho-Collados, Francesco Barbieri

The detection of sensitive content in large datasets is crucial for ensuring
that shared and analysed data is free from harmful material. However, current
moderation tools, such as external APIs, suffer from limitations in
customisation, accuracy across diverse sensitive categories, and privacy
concerns. Additionally, existing datasets and open-source models focus
predominantly on toxic language, leaving gaps in detecting other sensitive
categories such as substance abuse or self-harm. In this paper, we put forward
a unified dataset tailored for social media content moderation across six
sensitive categories: conflictual language, profanity, sexually explicit
material, drug-related content, self-harm, and spam. By collecting and
annotating data with consistent retrieval strategies and guidelines, we address
the shortcomings of previous focalised research. Our analysis demonstrates that
fine-tuning large language models (LLMs) on this novel dataset yields
significant improvements in detection performance compared to open
off-the-shelf models such as LLaMA, and even proprietary OpenAI models, which
underperform by 10-15% overall. This limitation is even more pronounced on
popular moderation APIs, which cannot be easily tailored to specific sensitive
content categories, among others.

摘要：偵測大型資料集中的敏感內容對於確保共享和分析的資料沒有有害內容至關重要。然而，目前的審核工具（例如外部 API）在自訂、不同敏感類別的準確性以及隱私疑慮方面有其限制。此外，現有的資料集和開源模型主要關注有毒語言，在偵測其他敏感類別（例如藥物濫用或自殘）方面存在缺口。在本文中，我們提出了一個統一的資料集，專門針對社交媒體內容審核，涵蓋六個敏感類別：衝突性語言、褻瀆、色情內容、與藥物相關的內容、自殘和垃圾訊息。透過收集和註解具有一致性擷取策略和準則的資料，我們解決了先前焦點研究的缺點。我們的分析表明，針對這個新穎的資料集微調大型語言模型 (LLM) 與 LLaMA 等現成的公開模型相比，偵測效能有顯著的提升，甚至連專有的 OpenAI 模型也低於整體 10-15%。這種限制在流行的審核 API 上更加明顯，其中無法輕鬆地針對特定敏感內容類別進行調整。

##### **SDR-GNN: Spectral Domain Reconstruction Graph Neural Network for Incomplete Multimodal Learning in Conversational Emotion Recognition**
2411.19822v1 by Fangze Fu, Wei Ai, Fan Yang, Yuntao Shou, Tao Meng, Keqin Li

Multimodal Emotion Recognition in Conversations (MERC) aims to classify
utterance emotions using textual, auditory, and visual modal features. Most
existing MERC methods assume each utterance has complete modalities,
overlooking the common issue of incomplete modalities in real-world scenarios.
Recently, graph neural networks (GNNs) have achieved notable results in
Incomplete Multimodal Emotion Recognition in Conversations (IMERC). However,
traditional GNNs focus on binary relationships between nodes, limiting their
ability to capture more complex, higher-order information. Moreover, repeated
message passing can cause over-smoothing, reducing their capacity to preserve
essential high-frequency details. To address these issues, we propose a
Spectral Domain Reconstruction Graph Neural Network (SDR-GNN) for incomplete
multimodal learning in conversational emotion recognition. SDR-GNN constructs
an utterance semantic interaction graph using a sliding window based on both
speaker and context relationships to model emotional dependencies. To capture
higher-order and high-frequency information, SDR-GNN utilizes weighted
relationship aggregation, ensuring consistent semantic feature extraction
across utterances. Additionally, it performs multi-frequency aggregation in the
spectral domain, enabling efficient recovery of incomplete modalities by
extracting both high- and low-frequency information. Finally, multi-head
attention is applied to fuse and optimize features for emotion recognition.
Extensive experiments on various real-world datasets demonstrate that our
approach is effective in incomplete multimodal learning and outperforms current
state-of-the-art methods.

摘要：多模態對話情感辨識 (MERC) 旨在使用文本、聽覺和視覺模態特徵來分類話語情感。大多數現有的 MERC 方法假設每個話語都有完整的模態，忽略了在現實世界場景中不完整模態的常見問題。最近，圖神經網路 (GNN) 在對話中的不完整多模態情感辨識 (IMERC) 中取得了顯著的成果。然而，傳統的 GNN 專注於節點之間的二元關係，限制了它們捕捉更複雜、更高階資訊的能力。此外，重複的訊息傳遞可能會導致過度平滑，降低它們保留必要的 high-frequency 細節的能力。為了解決這些問題，我們提出了一個用於對話情感辨識的不完整多模態學習的頻譜域重建圖神經網路 (SDR-GNN)。SDR-GNN 使用基於說話者和上下文關係的滑動視窗構建一個話語語義互動圖，以建模情緒依賴性。為了捕捉更高階和 high-frequency 資訊，SDR-GNN 利用加權關係聚合，確保在話語中進行一致的語義特徵提取。此外，它在頻譜域中執行多頻率聚合，透過提取 high-frequency 和 low-frequency 資訊，有效地恢復不完整模態。最後，應用多頭注意力來融合和最佳化情感辨識特徵。在各種真實世界資料集上的大量實驗表明，我們的做法在不完整的多模態學習中是有效的，並且優於當前最先進的方法。

##### **Q-learning-based Model-free Safety Filter**
2411.19809v1 by Guo Ning Sue, Yogita Choudhary, Richard Desatnik, Carmel Majidi, John Dolan, Guanya Shi

Ensuring safety via safety filters in real-world robotics presents
significant challenges, particularly when the system dynamics is complex or
unavailable. To handle this issue, learning-based safety filters recently
gained popularity, which can be classified as model-based and model-free
methods. Existing model-based approaches requires various assumptions on system
model (e.g., control-affine), which limits their application in complex
systems, and existing model-free approaches need substantial modifications to
standard RL algorithms and lack versatility. This paper proposes a simple,
plugin-and-play, and effective model-free safety filter learning framework. We
introduce a novel reward formulation and use Q-learning to learn Q-value
functions to safeguard arbitrary task specific nominal policies via filtering
out their potentially unsafe actions. The threshold used in the filtering
process is supported by our theoretical analysis. Due to its model-free nature
and simplicity, our framework can be seamlessly integrated with various RL
algorithms. We validate the proposed approach through simulations on double
integrator and Dubin's car systems and demonstrate its effectiveness in
real-world experiments with a soft robotic limb.

摘要：透過實際機器人技術中的安全過濾器確保安全會面臨重大挑戰，特別是在系統動態複雜或不可用時。為了應對這個問題，基於學習的安全過濾器最近獲得普及，可分類為基於模型和無模型方法。現有的基於模型的方法需要對系統模型（例如控制仿射）進行各種假設，這會限制它們在複雜系統中的應用，而現有的無模型方法則需要對標準 RL 演算法進行大幅修改，而且缺乏通用性。本文提出了一個簡單、即插即用且有效的無模型安全過濾器學習架構。我們引入一種新穎的獎勵公式，並使用 Q 學習來學習 Q 值函數，以透過過濾掉潛在不安全的動作來保護任意任務特定標稱策略。過濾過程中使用的閾值受到我們的理論分析支持。由於其無模型的性質和簡潔性，我們的框架可以與各種 RL 演算法無縫整合。我們透過在雙積分器和杜賓汽車系統上進行模擬來驗證所提出的方法，並展示其在使用軟機器人肢體進行實際實驗中的有效性。

##### **Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive Architectures**
2411.19806v1 by Alain Riou, Antonin Gagneré, Gaëtan Hadjeres, Stefan Lattner, Geoffroy Peeters

In this paper, we tackle the task of musical stem retrieval. Given a musical
mix, it consists in retrieving a stem that would fit with it, i.e., that would
sound pleasant if played together. To do so, we introduce a new method based on
Joint-Embedding Predictive Architectures, where an encoder and a predictor are
jointly trained to produce latent representations of a context and predict
latent representations of a target. In particular, we design our predictor to
be conditioned on arbitrary instruments, enabling our model to perform
zero-shot stem retrieval. In addition, we discover that pretraining the encoder
using contrastive learning drastically improves the model's performance.
  We validate the retrieval performances of our model using the MUSDB18 and
MoisesDB datasets. We show that it significantly outperforms previous baselines
on both datasets, showcasing its ability to support more or less precise (and
possibly unseen) conditioning. We also evaluate the learned embeddings on a
beat tracking task, demonstrating that they retain temporal structure and local
information.

摘要：在本文中，我們處理音樂音符檢索任務。給定一個音樂混音，它包含檢索一個與其匹配的音符，即如果一起播放，聽起來會令人愉快的音符。為此，我們引入一種基於聯合嵌入預測架構的新方法，其中編碼器和預測器經過聯合訓練，以產生上下文的潛在表示並預測目標的潛在表示。特別是，我們設計我們的預測器以任意樂器為條件，使我們的模型能夠執行零次檢索音符。此外，我們發現使用對比學習對編碼器進行預訓練會大幅提升模型的效能。
我們使用 MUSDB18 和 MoisesDB 資料集驗證模型的檢索效能。我們表明，它在兩個資料集上都明顯優於先前的基準，展示了它支援或多或少精確（且可能未見）條件化的能力。我們還評估了在節拍追蹤任務中學習的嵌入，證明它們保留了時間結構和局部資訊。

##### **Advanced System Integration: Analyzing OpenAPI Chunking for Retrieval-Augmented Generation**
2411.19804v1 by Robin D. Pesl, Jerin G. Mathew, Massimo Mecella, Marco Aiello

Integrating multiple (sub-)systems is essential to create advanced
Information Systems (ISs). Difficulties mainly arise when integrating dynamic
environments across the IS lifecycle. A traditional approach is a registry that
provides the API documentation of the systems' endpoints. Large Language Models
(LLMs) have shown to be capable of automatically creating system integrations
(e.g., as service composition) based on this documentation but require concise
input due to input token limitations, especially regarding comprehensive API
descriptions. Currently, it is unknown how best to preprocess these API
descriptions. Within this work, we (i) analyze the usage of Retrieval Augmented
Generation (RAG) for endpoint discovery and the chunking, i.e., preprocessing,
of OpenAPIs to reduce the input token length while preserving the most relevant
information. To further reduce the input token length for the composition
prompt and improve endpoint retrieval, we propose (ii) a Discovery Agent that
only receives a summary of the most relevant endpoints and retrieves details on
demand. We evaluate RAG for endpoint discovery using the RestBench benchmark,
first, for the different chunking possibilities and parameters measuring the
endpoint retrieval recall, precision, and F1 score. Then, we assess the
Discovery Agent using the same test set. With our prototype, we demonstrate how
to successfully employ RAG for endpoint discovery to reduce the token count.
While revealing high values for recall, precision, and F1, further research is
necessary to retrieve all requisite endpoints. Our experiments show that for
preprocessing, LLM-based and format-specific approaches outperform na\"ive
chunking methods. Relying on an agent further enhances these results as the
agent splits the tasks into multiple fine granular subtasks, improving the
overall RAG performance in the token count, precision, and F1 score.

摘要：整合多個（子）系統對於建立進階資訊系統（IS）至關重要。困難主要出現在整合 IS 生命週期中的動態環境時。傳統方法是登錄檔，它提供系統端點的 API 文件。大型語言模型（LLM）已證明有能力根據此文件自動建立系統整合（例如，作為服務組合），但由於輸入令牌限制，特別是關於全面的 API 描述，因此需要簡潔的輸入。目前，尚不知道如何最好地預處理這些 API 描述。在這項工作中，我們（i）分析了檢索增強生成（RAG）在端點發現中的用法，以及 OpenAPI 的分塊，即預處理，以減少輸入令牌長度，同時保留最相關的資訊。為了進一步減少組合提示的輸入令牌長度並改善端點檢索，我們提出（ii）一個發現代理，它只接收最相關端點的摘要，並根據需要檢索詳細資訊。我們使用 RestBench 基準評估 RAG 的端點發現，首先，對於不同的分塊可能性和參數測量端點檢索召回率、準確度和 F1 分數。然後，我們使用相同的測試集評估發現代理。透過我們的原型，我們展示了如何成功使用 RAG 進行端點發現以減少令牌計數。雖然顯示出召回率、準確度和 F1 的高值，但仍需要進一步的研究來檢索所有必要的端點。我們的實驗表明，對於預處理，基於 LLM 和特定格式的方法優於天真的分塊方法。依賴代理進一步增強了這些結果，因為代理將任務分為多個細緻的子任務，改善了令牌計數、準確度和 F1 分數中的整體 RAG 效能。

##### **INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge**
2411.19799v1 by Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, Antoine Bosselut

The performance differential of large language models (LLM) between languages
hinders their effective deployment in many regions, inhibiting the potential
economic and societal value of generative AI tools in many communities.
However, the development of functional LLMs in many languages (\ie,
multilingual LLMs) is bottlenecked by the lack of high-quality evaluation
resources in languages other than English. Moreover, current practices in
multilingual benchmark construction often translate English resources, ignoring
the regional and cultural knowledge of the environments in which multilingual
systems would be used. In this work, we construct an evaluation suite of
197,243 QA pairs from local exam sources to measure the capabilities of
multilingual LLMs in a variety of regional contexts. Our novel resource,
INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across
44 written languages that evaluates multilingual LLMs for performance in the
actual language environments where they would be deployed.

摘要：大型語言模型 (LLM) 在不同語言之間的效能差異，阻礙了它們在許多地區的有效部署，抑制了生成式 AI 工具在許多社群中潛在的經濟和社會價值。然而，多語言 LLM 的發展受到缺乏英語以外語言的高品質評估資源的瓶頸。此外，多語言基準建構的現行做法通常是翻譯英語資源，忽略了多語言系統將被使用的環境中的區域和文化知識。在這項工作中，我們從地方考試來源建構了一個包含 197,243 個問答對的評估套件，以衡量多語言 LLM 在各種區域脈絡中的能力。我們的創新資源 INCLUDE 是一個跨越 44 種書面語言的綜合知識和推理為中心的基準，用於評估多語言 LLM 在實際部署語言環境中的效能。

##### **CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives**
2411.19787v1 by Armin Saghafian, Amirmohammad Izadi, Negin Hashemi Dijujin, Mahdieh Soleymani Baghshah

Grounding the instruction in the environment is a key step in solving
language-guided goal-reaching reinforcement learning problems. In automated
reinforcement learning, a key concern is to enhance the model's ability to
generalize across various tasks and environments. In goal-reaching scenarios,
the agent must comprehend the different parts of the instructions within the
environmental context in order to complete the overall task successfully. In
this work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a
new framework to solve this problem using auxiliary loss functions inspired by
video-text retrieval literature and a novel method called instruction tracking,
which automatically keeps track of progress in an environment. The results of
our experiments suggest superior sample efficiency and systematic
generalization for this framework in multi-modal reinforcement learning
problems. Our code base is available here.

摘要：在環境中奠定指令是解決語言引導目標達成強化學習問題的關鍵步驟。在自動化強化學習中，一個關鍵問題是提升模型在各種任務和環境中泛化的能力。在目標達成場景中，代理程式必須在環境脈絡中理解指令的不同部分，才能成功完成整體任務。在這項工作中，我們提出 CAREL（跨模態輔助強化學習），作為一個新的架構，使用受影片文字檢索文獻啟發的輔助損失函數，以及一種稱為指令追蹤的新方法來解決這個問題，該方法會自動追蹤環境中的進度。我們的實驗結果顯示，此架構在多模態強化學習問題中具有優異的樣本效率和系統性泛化能力。我們的程式碼庫在此處提供。

##### **MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks**
2411.19786v1 by Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, Dong Xu

Recently, human motion analysis has experienced great improvement due to
inspiring generative models such as the denoising diffusion model and large
language model. While the existing approaches mainly focus on generating
motions with textual descriptions and overlook the reciprocal task. In this
paper, we present~\textbf{MoTe}, a unified multi-modal model that could handle
diverse tasks by learning the marginal, conditional, and joint distributions of
motion and text simultaneously. MoTe enables us to handle the paired
text-motion generation, motion captioning, and text-driven motion generation by
simply modifying the input context. Specifically, MoTe is composed of three
components: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and
Moti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for
extracting latent embeddings, and subsequently reconstructing the motion
sequences and textual descriptions from the extracted embeddings, respectively.
MTDM, on the other hand, performs an iterative denoising process on the input
context to handle diverse tasks. Experimental results on the benchmark datasets
demonstrate the superior performance of our proposed method on text-to-motion
generation and competitive performance on motion captioning.

摘要：近年來，由於去噪擴散模型和大語言模型等啟發式生成模型，人類動作分析有了很大的進展。雖然現有方法主要集中於生成具有文本描述的動作，卻忽略了倒置任務。在本文中，我們提出了 MoTe，這是一個統一的多模態模型，它可以通過同時學習動作和文本的邊際、條件和聯合分佈來處理各種任務。MoTe 使我們能夠通過簡單地修改輸入上下文來處理配對的文本運動生成、運動字幕和文本驅動運動生成。具體來說，MoTe 由三個組成部分組成：運動編碼器-解碼器 (MED)、文本編碼器-解碼器 (TED) 和文本運動擴散模型 (MTDM)。特別是，MED 和 TED 被訓練用於提取潛在嵌入，然後分別從提取的嵌入中重建運動序列和文本描述。另一方面，MTDM 對輸入上下文執行迭代去噪處理以處理各種任務。基準數據集上的實驗結果證明了我們提出的方法在文本到動作生成方面的優異性能和在運動字幕方面的競爭性能。

##### **PerLA: Perceptive 3D Language Assistant**
2411.19774v1 by Guofeng Mei, Wei Lin, Luigi Riz, Yujiao Wu, Fabio Poiesi, Yiming Wang

Enabling Large Language Models (LLMs) to understand the 3D physical world is
an emerging yet challenging research direction. Current strategies for
processing point clouds typically downsample the scene or divide it into
smaller parts for separate analysis. However, both approaches risk losing key
local details or global contextual information. In this paper, we introduce
PerLA, a 3D language assistant designed to be more perceptive to both details
and context, making visual representations more informative for the LLM. PerLA
captures high-resolution (local) details in parallel from different point cloud
areas and integrates them with (global) context obtained from a
lower-resolution whole point cloud. We present a novel algorithm that preserves
point cloud locality through the Hilbert curve and effectively aggregates
local-to-global information via cross-attention and a graph neural network.
Lastly, we introduce a novel loss for local representation consensus to promote
training stability. PerLA outperforms state-of-the-art 3D language assistants,
with gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on
ScanRefer and +3.88 on Nr3D for dense
captioning.\url{https://gfmei.github.io/PerLA/}

摘要：讓大型語言模型 (LLM) 理解 3D 物理世界是一個新興但具有挑戰性的研究方向。當前處理點雲的策略通常會對場景進行降採樣或將其分為更小的部分以進行單獨分析。然而，這兩種方法都有可能遺失關鍵的局部細節或全局背景資訊。在本文中，我們介紹了 PerLA，這是一個 3D 語言助理，旨在更敏銳地感知細節和背景，讓視覺表現對 LLM 更有資訊性。PerLA 從不同的點雲區域並行擷取高解析度（局部）細節，並將其與從低解析度全點雲中獲得的（全局）背景整合在一起。我們提出了一種新演算法，透過希爾伯特曲線保留點雲局部性，並透過交叉注意力和圖形神經網路有效地匯總局部到全局資訊。最後，我們引入了一個新的損失函數，用於局部表示共識，以促進訓練穩定性。PerLA 優於最先進的 3D 語言助理，在 ScanQA 上問答獲得高達 +1.34 CiDEr 的增益，在 ScanRefer 上獲得 +4.22，在 Nr3D 上獲得 +3.88 的密集標題。\url{https://gfmei.github.io/PerLA/}

##### **LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos**
2411.19772v1 by Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng

Despite impressive advancements in video understanding, most efforts remain
limited to coarse-grained or visual-only video tasks. However, real-world
videos encompass omni-modal information (vision, audio, and speech) with a
series of events forming a cohesive storyline. The lack of multi-modal video
data with fine-grained event annotations and the high cost of manual labeling
are major obstacles to comprehensive omni-modality video perception. To address
this gap, we propose an automatic pipeline consisting of high-quality
multi-modal video filtering, semantically coherent omni-modal event boundary
detection, and cross-modal correlation-aware event captioning. In this way, we
present LongVALE, the first-ever Vision-Audio-Language Event understanding
benchmark comprising 105K omni-modal events with precise temporal boundaries
and detailed relation-aware captions within 8.4K high-quality long videos.
Further, we build a baseline that leverages LongVALE to enable video large
language models (LLMs) for omni-modality fine-grained temporal video
understanding for the first time. Extensive experiments demonstrate the
effectiveness and great potential of LongVALE in advancing comprehensive
multi-modal video understanding.

摘要：儘管在影片理解上已有令人驚豔的進展，但大多數的努力仍侷限於粗略或僅視覺的影片任務。然而，真實世界的影片包含全模態資訊（視覺、音訊和語音），一系列事件形成一個有凝聚力的故事線。缺乏具備細緻事件註解的多模態影片資料，以及人工標註的高昂成本，是全面全模態影片感知的主要障礙。為了解決這個差距，我們提出一個自動化流程，包含高品質全模態影片過濾、語意連貫的全模態事件邊界偵測，以及跨模態關聯感知事件標題。透過這種方式，我們展示了 LongVALE，這是第一個視覺-音訊-語言事件理解基準，包含 105K 個全模態事件，具備精確的時間邊界和詳細的關係感知標題，包含在 8.4K 個高品質長影片中。此外，我們建立了一個基準，利用 LongVALE 讓影片大型語言模型 (LLM) 能夠進行全模態細緻時間影片理解，這是第一次。大量的實驗證明了 LongVALE 在推進全面全模態影片理解方面的有效性和巨大潛力。

##### **Noro: A Noise-Robust One-shot Voice Conversion System with Hidden Speaker Representation Capabilities**
2411.19770v1 by Haorui He, Yuchen Song, Yuancheng Wang, Haoyang Li, Xueyao Zhang, Li Wang, Gongping Huang, Eng Siong Chng, Zhizheng Wu

One-shot voice conversion (VC) aims to alter the timbre of speech from a
source speaker to match that of a target speaker using just a single reference
speech from the target, while preserving the semantic content of the original
source speech. Despite advancements in one-shot VC, its effectiveness decreases
in real-world scenarios where reference speeches, often sourced from the
internet, contain various disturbances like background noise. To address this
issue, we introduce Noro, a Noise Robust One-shot VC system. Noro features
innovative components tailored for VC using noisy reference speeches, including
a dual-branch reference encoding module and a noise-agnostic contrastive
speaker loss. Experimental results demonstrate that Noro outperforms our
baseline system in both clean and noisy scenarios, highlighting its efficacy
for real-world applications. Additionally, we investigate the hidden speaker
representation capabilities of our baseline system by repurposing its reference
encoder as a speaker encoder. The results shows that it is competitive with
several advanced self-supervised learning models for speaker representation
under the SUPERB settings, highlighting the potential for advancing speaker
representation learning through one-shot VC task.

摘要：單次語音轉換 (VC) 旨在僅使用目標對象的單一參考語音，改變來源說話者的語音音色，以匹配目標說話者的語音音色，同時保留原始來源語音的語義內容。儘管單次 VC 技術進步，但在現實世界場景中，其有效性會降低，因為參考語音通常來自網路，其中包含各種干擾，例如背景噪音。為了解決這個問題，我們引入了 Noro，一個對噪音穩健的單次 VC 系統。Noro 具有創新的元件，專門用於使用有噪音參考語音的 VC，包括雙分支參考編碼模組和與噪音無關的對比揚聲器損失。實驗結果表明，在乾淨和有噪音的場景中，Noro 都優於我們的基準系統，突顯了其在現實世界應用中的功效。此外，我們透過將其參考編碼器重新用作揚聲器編碼器，來研究我們基準系統的隱藏揚聲器表示功能。結果表明，在 SUPERB 設定下，它與幾個進階的自我監督學習模型具有競爭力，用於揚聲器表示，突顯了透過單次 VC 任務推進揚聲器表示學習的潛力。

##### **Stock Price Prediction using Multi-Faceted Information based on Deep Recurrent Neural Networks**
2411.19766v1 by Lida Shahbandari, Elahe Moradi, Mohammad Manthouri

Accurate prediction of stock market trends is crucial for informed investment
decisions and effective portfolio management, ultimately leading to enhanced
wealth creation and risk mitigation. This study proposes a novel approach for
predicting stock prices in the stock market by integrating Convolutional Neural
Networks (CNN) and Long Short-Term Memory (LSTM) networks, using sentiment
analysis of social network data and candlestick data (price). The proposed
methodology consists of two primary components: sentiment analysis of social
network and candlestick data. By amalgamating candlestick data with insights
gleaned from Twitter, this approach facilitates a more detailed and accurate
examination of market trends and patterns, ultimately leading to more effective
stock price predictions. Additionally, a Random Forest algorithm is used to
classify tweets as either positive or negative, allowing for a more subtle and
informed assessment of market sentiment. This study uses CNN and LSTM networks
to predict stock prices. The CNN extracts short-term features, while the LSTM
models long-term dependencies. The integration of both networks enables a more
comprehensive analysis of market trends and patterns, leading to more accurate
stock price predictions.

摘要：準確預測股市趨勢對明智的投資決策和有效的投資組合管理至關重要，最終能提升財富創造並降低風險。本研究提出了一種新穎的方法，透過整合卷積神經網路 (CNN) 和長短期記憶網路 (LSTM) 網路，使用社群網路資料和蠟燭圖資料 (價格) 的情緒分析，來預測股市中的股票價格。所提出的方法論包含兩個主要組成部分：社群網路情緒分析和蠟燭圖資料。透過將蠟燭圖資料與從 Twitter 中收集的見解結合，此方法有助於更詳細且準確地檢視市場趨勢和模式，最終能更有效地預測股票價格。此外，隨機森林演算法用於將推文分類為正面或負面，以便更細緻且明智地評估市場情緒。本研究使用 CNN 和 LSTM 網路來預測股票價格。CNN 會擷取短期特徵，而 LSTM 則會建模長期依賴關係。整合這兩個網路能更全面地分析市場趨勢和模式，進而更準確地預測股票價格。

##### **Forecasting Foreign Exchange Market Prices Using Technical Indicators with Deep Learning and Attention Mechanism**
2411.19763v1 by Sahabeh Saadati, Mohammad Manthouri

Accurate prediction of price behavior in the foreign exchange market is
crucial. This paper proposes a novel approach that leverages technical
indicators and deep neural networks. The proposed architecture consists of a
Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN), and
attention mechanism. Initially, trend and oscillation technical indicators are
employed to extract statistical features from Forex currency pair data,
providing insights into price trends, market volatility, relative price
strength, and overbought and oversold conditions. Subsequently, the LSTM and
CNN networks are utilized in parallel to predict future price movements,
leveraging the strengths of both recurrent and convolutional architectures. The
LSTM network captures long-term dependencies and temporal patterns in the data,
while the CNN network extracts local patterns. The outputs of the parallel LSTM
and CNN networks are then fed into an attention mechanism, which learns to
weigh the importance of each feature and temporal dependency, generating a
context-aware representation of the input data. The attention-weighted output
is then used to predict future price movements, enabling the model to focus on
the most relevant features and temporal dependencies. Through a comprehensive
evaluation of the proposed approach on multiple Forex currency pairs, we
demonstrate its effectiveness in predicting price behavior and outperforming
benchmark models.

摘要：準確預測外匯市場的價格行為至關重要。本文提出了一種利用技術指標和深度神經網路的新方法。所提出的架構包含長短期記憶 (LSTM) 和卷積神經網路 (CNN)，以及注意力機制。最初，趨勢和振盪技術指標用於從外匯貨幣對數據中提取統計特徵，提供對價格趨勢、市場波動、相對價格強度以及超買和超賣狀況的見解。隨後，LSTM 和 CNN 網路並行使用來預測未來的價格變動，利用遞迴和卷積架構的優勢。LSTM 網路捕捉數據中的長期依賴性和時間模式，而 CNN 網路提取局部模式。並行 LSTM 和 CNN 網路的輸出隨後被饋入注意力機制，該機制學習權衡每個特徵和時間依賴性的重要性，生成輸入數據的上下文感知表示。然後使用注意力加權輸出預測未來的價格變動，使模型能夠專注於最相關的特徵和時間依賴性。通過對多個外匯貨幣對的所提出方法進行全面評估，我們證明了其在預測價格行為和優於基準模型方面的有效性。

##### **LaVIDE: A Language-Vision Discriminator for Detecting Changes in Satellite Image with Map References**
2411.19758v1 by Shuguo Jiang, Fang Xu, Sen Jia, Gui-Song Xia

Change detection, which typically relies on the comparison of bi-temporal
images, is significantly hindered when only a single image is available.
Comparing a single image with an existing map, such as OpenStreetMap, which is
continuously updated through crowd-sourcing, offers a viable solution to this
challenge. Unlike images that carry low-level visual details of ground objects,
maps convey high-level categorical information. This discrepancy in abstraction
levels complicates the alignment and comparison of the two data types. In this
paper, we propose a \textbf{La}nguage-\textbf{VI}sion \textbf{D}iscriminator
for d\textbf{E}tecting changes in satellite image with map references, namely
\ours{}, which leverages language to bridge the information gap between maps
and images. Specifically, \ours{} formulates change detection as the problem of
``{\textit Does the pixel belong to [class]?}'', aligning maps and images
within the feature space of the language-vision model to associate high-level
map categories with low-level image details. Moreover, we build a
mixture-of-experts discriminative module, which compares linguistic features
from maps with visual features from images across various semantic
perspectives, achieving comprehensive semantic comparison for change detection.
Extensive evaluation on four benchmark datasets demonstrates that \ours{} can
effectively detect changes in satellite image with map references,
outperforming state-of-the-art change detection algorithms, e.g., with gains of
about $13.8$\% on the DynamicEarthNet dataset and $4.3$\% on the SECOND
dataset.

摘要：<paragraph>變異偵測通常依賴於雙時態影像的比較，但當只有一張影像可用時，會受到顯著的阻礙。將單一影像與現有地圖（例如透過群眾外包持續更新的 OpenStreetMap）進行比較，為此挑戰提供了可行的解決方案。與承載地面物件低階視覺細節的影像不同，地圖傳達的是高階類別資訊。抽象層級的這種差異，使得兩種資料類型的對齊和比較變得複雜。在本文中，我們提出一個用於偵測衛星影像中變異的語言視覺辨別器，並使用地圖參考，即 \ours{}，它利用語言來彌合地圖與影像之間的資訊差距。具體來說，\ours{} 將變異偵測制定為「像素是否屬於 [類別]？」的問題，在語言視覺模型的特徵空間內對齊地圖和影像，以將高階地圖類別與低階影像細節關聯起來。此外，我們建立了一個專家混合辨別模組，它比較來自地圖的語言特徵與來自影像的視覺特徵，涵蓋各種語義觀點，實現全面的語義比較以進行變異偵測。在四個基準資料集上的廣泛評估表明，\ours{} 可以有效偵測衛星影像中具有地圖參考的變異，優於最先進的變異偵測演算法，例如在 DynamicEarthNet 資料集上獲得約 13.8% 的增益，在 SECOND 資料集上獲得 4.3% 的增益。</paragraph>

##### **A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining Off-Road, Diversity, and Directional Consistency Losses**
2411.19747v1 by Ahmad Rahimi, Alexandre Alahi

Trajectory prediction is essential for the safety and efficiency of planning
in autonomous vehicles. However, current models often fail to fully capture
complex traffic rules and the complete range of potential vehicle movements.
Addressing these limitations, this study introduces three novel loss functions:
Offroad Loss, Direction Consistency Error, and Diversity Loss. These functions
are designed to keep predicted paths within driving area boundaries, aligned
with traffic directions, and cover a wider variety of plausible driving
scenarios. As all prediction modes should adhere to road rules and conditions,
this work overcomes the shortcomings of traditional "winner takes all" training
methods by applying the loss functions to all prediction modes. These loss
functions not only improve model training but can also serve as metrics for
evaluating the realism and diversity of trajectory predictions. Extensive
validation on the nuScenes and Argoverse 2 datasets with leading baseline
models demonstrates that our approach not only maintains accuracy but
significantly improves safety and robustness, reducing offroad errors on
average by 47% on original and by 37% on attacked scenes. This work sets a new
benchmark for trajectory prediction in autonomous driving, offering substantial
improvements in navigating complex environments. Our code is available at
https://github.com/vita-epfl/stay-on-track .

摘要：軌跡預測對於自動駕駛車輛的規劃安全與效率至關重要。然而，目前的模型往往無法完全掌握複雜的交通規則和車輛移動的完整範圍。針對這些限制，本研究引入了三種新穎的損失函數：越野損失、方向一致性誤差和多樣性損失。這些函數旨在將預測路徑保持在駕駛區域邊界內，與交通方向保持一致，並涵蓋更廣泛的合理駕駛場景。由於所有預測模式都應遵守道路規則和條件，因此這項工作通過將損失函數應用於所有預測模式來克服傳統「贏家全拿」訓練方法的缺點。這些損失函數不僅改善了模型訓練，還可以作為評估軌跡預測的真實性和多樣性的指標。在具有領先基準模型的 nuScenes 和 Argoverse 2 資料集上進行廣泛驗證表明，我們的做法不僅保持了準確性，而且顯著提高了安全性與穩健性，平均將越野誤差降低了 47%，在原始場景中降低了 37%。這項工作為自動駕駛中的軌跡預測設定了一個新的基準，在複雜環境中的導航方面提供了顯著的改進。我們的程式碼可以在 https://github.com/vita-epfl/stay-on-track 取得。

##### **Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph**
2411.19742v1 by Heloisa Oss Boll, Ali Amirahmadi, Amira Soliman, Stefan Byttner, Mariana Recamonde-Mendoza

Objective: In modern healthcare, accurately predicting diseases is a crucial
matter. This study introduces a novel approach using graph neural networks
(GNNs) and a Graph Transformer (GT) to predict the incidence of heart failure
(HF) on a patient similarity graph at the next hospital visit. Materials and
Methods: We used electronic health records (EHR) from the MIMIC-III dataset and
applied the K-Nearest Neighbors (KNN) algorithm to create a patient similarity
graph using embeddings from diagnoses, procedures, and medications. Three
models - GraphSAGE, Graph Attention Network (GAT), and Graph Transformer (GT) -
were implemented to predict HF incidence. Model performance was evaluated using
F1 score, AUROC, and AUPRC metrics, and results were compared against baseline
algorithms. An interpretability analysis was performed to understand the
model's decision-making process. Results: The GT model demonstrated the best
performance (F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168). Although the
Random Forest (RF) baseline achieved a similar AUPRC value, the GT model
offered enhanced interpretability due to the use of patient relationships in
the graph structure. A joint analysis of attention weights, graph connectivity,
and clinical features provided insight into model predictions across different
classification groups. Discussion and Conclusion: Graph-based approaches such
as GNNs provide an effective framework for predicting HF. By leveraging a
patient similarity graph, GNNs can capture complex relationships in EHR data,
potentially improving prediction accuracy and clinical interpretability.

摘要：<paragraph>目標：在現代醫療保健中，準確預測疾病是一項至關重要的問題。本研究介紹了一種使用圖神經網絡 (GNN) 和圖形轉換器 (GT) 的新方法，用於預測下次醫院就診時患者相似圖表上的心臟衰竭 (HF) 發生率。材料和方法：我們使用了 MIMIC-III 資料集中的電子健康記錄 (EHR)，並應用 K-最近鄰 (KNN) 演算法，使用來自診斷、程序和藥物的嵌入來建立患者相似圖表。實作了三個模型 - GraphSAGE、圖形注意力網路 (GAT) 和圖形轉換器 (GT) - 來預測 HF 發生率。使用 F1 分數、AUROC 和 AUPRC 指標評估模型效能，並將結果與基準演算法進行比較。執行了解釋性分析以了解模型的決策過程。結果：GT 模型表現出最佳效能 (F1 分數：0.5361，AUROC：0.7925，AUPRC：0.5168)。儘管隨機森林 (RF) 基準達到了類似的 AUPRC 值，但由於在圖形結構中使用了患者關係，因此 GT 模型提供了增強的解釋性。對注意力權重、圖形連通性和臨床特徵的聯合分析提供了對不同分類群組中模型預測的見解。討論和結論：基於圖形的方法（例如 GNN）提供了預測 HF 的有效框架。透過利用患者相似圖形，GNN 可以擷取 EHR 資料中的複雜關係，進而可能提高預測準確度和臨床解釋性。</paragraph>

##### **A Deep Learning Approach to Language-independent Gender Prediction on Twitter**
2411.19733v1 by Reyhaneh Hashempour, Barbara Plank, Aline Villavicencio, Renato Cordeiro de Amorim

This work presents a set of experiments conducted to predict the gender of
Twitter users based on language-independent features extracted from the text of
the users' tweets. The experiments were performed on a version of TwiSty
dataset including tweets written by the users of six different languages:
Portuguese, French, Dutch, English, German, and Italian. Logistic regression
(LR), and feed-forward neural networks (FFNN) with back-propagation were used
to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual
(CL). In the IL setting, the training and testing were performed on the same
language whereas in the CL, Italian and German datasets were set aside and only
used as test sets and the rest were combined to compose training and
development sets. In the IL, the highest accuracy score belongs to LR whereas
in the CL, FFNN with three hidden layers yields the highest score. The results
show that neural network based models underperform traditional models when the
size of the training set is small; however, they beat traditional models by a
non-trivial margin, when they are fed with large enough data. Finally, the
feature analysis confirms that men and women have different writing styles
independent of their language.

摘要：本研究提出了一組實驗，用來預測 Twitter 使用者的性別，基礎是從使用者推文文字中萃取出的與語言無關的特徵。這些實驗是在 TwiSty 資料集的版本上執行，其中包含六種不同語言的使用者所寫的推文：葡萄牙語、法語、荷蘭語、英語、德語和義大利語。邏輯迴歸 (LR) 和帶反向傳播的饋送前向神經網路 (FFNN) 用於在兩種不同的設定中建立模型：語言間 (IL) 和跨語言 (CL)。在 IL 設定中，訓練和測試是在同一種語言上執行，而在 CL 中，義大利語和德語資料集被保留，僅用作測試集，而其餘的則合併用於組成訓練和開發集。在 IL 中，最高的準確度分數屬於 LR，而在 CL 中，具有三個隱藏層的 FFNN 產生最高分數。結果顯示，當訓練集較小時，基於神經網路的模型表現不如傳統模型；然而，當它們被輸入足夠大的資料時，它們會以非顯著的幅度超越傳統模型。最後，特徵分析證實，男性和女性有不同的寫作風格，與他們的語言無關。

##### **Towards Santali Linguistic Inclusion: Building the First Santali-to-English Translation Model using mT5 Transformer and Data Augmentation**
2411.19726v1 by Syed Mohammed Mostaque Billah, Ateya Ahmed Subarna, Sudipta Nandi Sarna, Ahmad Shawkat Wasit, Anika Fariha, Asif Sushmit, Arig Yousuf Sadeque

Around seven million individuals in India, Bangladesh, Bhutan, and Nepal
speak Santali, positioning it as nearly the third most commonly used
Austroasiatic language. Despite its prominence among the Austroasiatic language
family's Munda subfamily, Santali lacks global recognition. Currently, no
translation models exist for the Santali language. Our paper aims to include
Santali to the NPL spectrum. We aim to examine the feasibility of building
Santali translation models based on available Santali corpora. The paper
successfully addressed the low-resource problem and, with promising results,
examined the possibility of creating a functional Santali machine translation
model in a low-resource setup. Our study shows that Santali-English parallel
corpus performs better when in transformers like mt5 as opposed to untrained
transformers, proving that transfer learning can be a viable technique that
works with Santali language. Besides the mT5 transformer, Santali-English
performs better than Santali-Bangla parallel corpus as the mT5 has been trained
in way more English data than Bangla data. Lastly, our study shows that with
data augmentation, our model performs better.

摘要：在印度、孟加拉、不丹和尼泊爾，約有七百萬人說桑塔利語，這使它成為使用人數第三多的南亞語系語言。儘管桑塔利語在南亞語系中的蒙達語族中很突出，但它缺乏全球認可度。目前，桑塔利語沒有翻譯模型。我們的論文旨在將桑塔利語納入 NPL 範疇。我們旨在探討在現有的桑塔利語語料庫基礎上構建桑塔利語翻譯模型的可行性。本文成功解決了低資源問題，並以有希望的結果探討了在低資源設置中創建功能性桑塔利語機器翻譯模型的可能性。我們的研究表明，與未訓練的Transformer相比，當桑塔利語-英語並行語料庫在像 mt5 這樣的Transformer中使用時表現得更好，證明遷移學習可以是一種可行的技術，適用於桑塔利語。除了 mT5 Transformer之外，桑塔利語-英語的表現優於桑塔利語-孟加拉語並行語料庫，因為 mT5 在比孟加拉語數據多得多的英語數據中進行了訓練。最後，我們的研究表明，通過數據擴充，我們的模型表現得更好。

##### **JetFormer: An Autoregressive Generative Model of Raw Images and Text**
2411.19722v1 by Michael Tschannen, André Susano Pinto, Alexander Kolesnikov

Removing modeling constraints and unifying architectures across domains has
been a key driver of the recent progress in training large multimodal models.
However, most of these models still rely on many separately trained components
such as modality-specific encoders and decoders. In this work, we further
streamline joint generative modeling of images and text. We propose an
autoregressive decoder-only transformer - JetFormer - which is trained to
directly maximize the likelihood of raw data, without relying on any separately
pretrained components, and can understand and generate both text and images.
Specifically, we leverage a normalizing flow model to obtain a soft-token image
representation that is jointly trained with an autoregressive multimodal
transformer. The normalizing flow model serves as both an image encoder for
perception tasks and an image decoder for image generation tasks during
inference. JetFormer achieves text-to-image generation quality competitive with
recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained
image autoencoders, which are trained with a complex mixture of losses,
including perceptual ones. At the same time, JetFormer demonstrates robust
image understanding capabilities. To the best of our knowledge, JetFormer is
the first model that is capable of generating high-fidelity images and
producing strong log-likelihood bounds.

摘要：移除模型限制和統一跨領域架構一直是最近訓練大型多模態模型進展的關鍵驅動力。
然而，這些模型大多仍依賴許多個別訓練的組成部分，例如特定於模態的編碼器和解碼器。在這項工作中，我們進一步簡化了影像和文字的聯合生成式建模。我們提出一個自迴歸僅解碼器轉換器 - JetFormer - 訓練直接最大化原始資料的可能性，而無需依賴任何個別預訓練組成部分，並且可以理解並生成文字和影像。
具體來說，我們利用正規化流模型來取得與自迴歸多模態轉換器聯合訓練的軟代幣影像表示。正規化流模型用作感知任務的影像編碼器和推論期間影像生成任務的影像解碼器。JetFormer 達到與最近基於 VQ-VAE 和 VAE 的基準相當的文字到影像生成品質。這些基準依賴於預訓練的影像自動編碼器，這些編碼器使用複雜的損失混合訓練，包括感知損失。同時，JetFormer 展示了強大的影像理解能力。據我們所知，JetFormer 是第一個能夠生成高保真影像並產生強勁對數似然界限的模型。

##### **TakeLab Retriever: AI-Driven Search Engine for Articles from Croatian News Outlets**
2411.19718v1 by David Dukić, Marin Petričević, Sven Ćurković, Jan Šnajder

TakeLab Retriever is an AI-driven search engine designed to discover,
collect, and semantically analyze news articles from Croatian news outlets. It
offers a unique perspective on the history and current landscape of Croatian
online news media, making it an essential tool for researchers seeking to
uncover trends, patterns, and correlations that general-purpose search engines
cannot provide. TakeLab retriever utilizes cutting-edge natural language
processing (NLP) methods, enabling users to sift through articles using named
entities, phrases, and topics through the web application. This technical
report is divided into two parts: the first explains how TakeLab Retriever is
utilized, while the second provides a detailed account of its design. In the
second part, we also address the software engineering challenges involved and
propose solutions for developing a microservice-based semantic search engine
capable of handling over ten million news articles published over the past two
decades.

摘要：TakeLab Retriever 是一個以人工智慧為基礎的搜尋引擎，旨在探索、收集和語意分析來自克羅埃西亞新聞媒體的新聞文章。它提供了一個獨特的觀點，探討克羅埃西亞線上新聞媒體的歷史和現況，成為研究人員發掘趨勢、模式和關聯性的重要工具，而這些是通用搜尋引擎無法提供的。TakeLab Retriever 運用尖端的自然語言處理 (NLP) 方法，讓使用者能透過網路應用程式，使用命名實體、詞組和主題來篩選文章。這份技術報告分為兩部分：第一部分說明如何使用 TakeLab Retriever，而第二部分則詳細說明其設計。在第二部分中，我們也處理了相關的軟體工程挑戰，並提出解決方案，以開發一個微服務基礎的語意搜尋引擎，能夠處理過去二十年發布的超過一千萬篇新聞文章。

##### **MonoPP: Metric-Scaled Self-Supervised Monocular Depth Estimation by Planar-Parallax Geometry in Automotive Applications**
2411.19717v1 by Gasser Elazab, Torben Gräber, Michael Unterreiner, Olaf Hellwich

Self-supervised monocular depth estimation (MDE) has gained popularity for
obtaining depth predictions directly from videos. However, these methods often
produce scale invariant results, unless additional training signals are
provided. Addressing this challenge, we introduce a novel self-supervised
metric-scaled MDE model that requires only monocular video data and the
camera's mounting position, both of which are readily available in modern
vehicles. Our approach leverages planar-parallax geometry to reconstruct scene
structure. The full pipeline consists of three main networks, a multi-frame
network, a singleframe network, and a pose network. The multi-frame network
processes sequential frames to estimate the structure of the static scene using
planar-parallax geometry and the camera mounting position. Based on this
reconstruction, it acts as a teacher, distilling knowledge such as scale
information, masked drivable area, metric-scale depth for the static scene, and
dynamic object mask to the singleframe network. It also aids the pose network
in predicting a metric-scaled relative pose between two subsequent images. Our
method achieved state-of-the-art results for the driving benchmark KITTI for
metric-scaled depth prediction. Notably, it is one of the first methods to
produce self-supervised metric-scaled depth prediction for the challenging
Cityscapes dataset, demonstrating its effectiveness and versatility.

摘要：自监督单目深度估计 (MDE) 因可直接从视频获取深度预测而广受欢迎。然而，这些方法通常会产生尺度不变的结果，除非提供了额外的训练信号。为了应对这一挑战，我们引入了一个新颖的自监督度量缩放 MDE 模型，该模型仅需要单目视频数据和摄像头的安装位置，这两者在现代车辆中都很容易获得。我们的方法利用平面视差几何来重建场景结构。完整的管道由三个主要网络组成，一个多帧网络、一个单帧网络和一个姿态网络。多帧网络处理顺序帧，以使用平面视差几何和相机安装位置估计静态场景的结构。基于此重建，它充当教师，将诸如比例信息、蒙版可驾驶区域、静态场景的度量比例深度和动态对象蒙版之类的知识提炼到单帧网络中。它还帮助姿态网络预测两个后续图像之间的度量比例相对姿态。我们的方法为驾驶基准 KITTI 的度量比例深度预测取得了最先进的结果。值得注意的是，它是第一个为具有挑战性的 Cityscapes 数据集生成自监督度量比例深度预测的方法之一，证明了它的有效性和多功能性。

##### **MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating Multi-Insight Multi-Document Extraction Tasks**
2411.19689v1 by John Francis, Saba Esnaashari, Anton Poletaev, Sukankana Chakraborty, Youmna Hashem, Jonathan Bright

Large language models (LLMs) have demonstrated remarkable capabilities in
text analysis tasks, yet their evaluation on complex, real-world applications
remains challenging. We define a set of tasks, Multi-Insight Multi-Document
Extraction (MIMDE) tasks, which involves extracting an optimal set of insights
from a document corpus and mapping these insights back to their source
documents. This task is fundamental to many practical applications, from
analyzing survey responses to processing medical records, where identifying and
tracing key insights across documents is crucial. We develop an evaluation
framework for MIMDE and introduce a novel set of complementary human and
synthetic datasets to examine the potential of synthetic data for LLM
evaluation. After establishing optimal metrics for comparing extracted
insights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis
reveals a strong correlation (0.71) between the ability of LLMs to extracts
insights on our two datasets but synthetic data fails to capture the complexity
of document-level analysis. These findings offer crucial guidance for the use
of synthetic data in evaluating text analysis systems, highlighting both its
potential and limitations.

摘要：大型語言模型 (LLM) 在文本分析任務中展現出非凡的能力，然而，它們在複雜的現實世界應用中仍難以評估。我們定義了一組任務，多見解多文件萃取 (MIMDE) 任務，它涉及從文件語料庫中萃取一組最佳見解，並將這些見解映射回其原始文件。這項任務對於許多實際應用至關重要，從分析調查回應到處理醫療記錄，其中識別和追蹤文件中的關鍵見解至關重要。我們開發了一個 MIMDE 評估架構，並引入了一組新穎的互補人類和合成資料集，以檢視合成資料在 LLM 評估中的潛力。在建立用於比較萃取見解的最佳指標後，我們在兩個資料集上對 20 個最先進的 LLM 進行基準測試。我們的分析顯示，LLM 在我們的兩個資料集上萃取見解的能力之間存在強相關性 (0.71)，但合成資料無法捕捉文件層級分析的複雜性。這些發現為在評估文本分析系統時使用合成資料提供了至關重要的指導，突出了其潛力和局限性。

##### **ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information**
2411.19668v1 by Wanyue Zhang, Ziyong Li, Wen Yang, Chunlin Leng, Yinan Bai, Qianlong Du, Chengqing Zong, Jiajun Zhang

During the development of large language models (LLMs), pre-training data
play a critical role in shaping LLMs' capabilities. In recent years several
large-scale and high-quality pre-training datasets have been released to
accelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,
WanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has
increasingly shifted to domain-specific capabilities and safety concerns,
making those previous coarse-grained texts insufficient for meeting training
requirements. Furthermore, fine-grained information, such as quality, domain
and toxicity, is becoming increasingly important in building powerful and
reliable LLMs for various scenarios. To address these challenges, in this paper
we propose a new tool-chain called MDFG-tool for constructing large-scale and
high-quality Chinese datasets with multi-dimensional and fine-grained
information. First, we employ manually crafted rules to discard explicit noisy
texts from raw contents. Second, the quality evaluation model, domain
classifier, and toxicity evaluation model are well-designed to assess the
remaining cleaned data respectively. Finally, we integrate these three types of
fine-grained information for each text. With this approach, we release the
largest, high-quality and fine-grained Chinese text ChineseWebText2.0, which
consists of 3.8TB and each text is associated with a quality score, domain
labels, a toxicity label and a toxicity score, facilitating the LLM researchers
to select data based on various types of fine-grained information. The data,
codes and the tool-chain are available on this website
https://github.com/CASIA-LM/ChineseWebText-2.0

摘要：<paragraph>在大语言模型（LLM）开发中，预训练数据在塑造 LLM 能力方面起着至关重要的作用。近年来，已经发布了几个大规模且高质量的预训练数据集，以加速 LLM 的研究，包括 ChineseWebText1.0、C4、Pile、WanJuan、MAPCC 等。然而，随着 LLM 的不断发展，重点已逐渐转向特定领域的语言能力和安全性问题，这使得以前那些粒度较粗的文本不足以满足训练要求。此外，精细的信息（如质量、领域和毒性）在为各种场景构建强大且可靠的 LLM 中变得越来越重要。为了应对这些挑战，在本文中，我们提出了一种名为 MDFG-tool 的新工具链，用于构建具有多维和细粒度信息的大规模和高质量的中文数据集。首先，我们采用人工制作的规则从原始内容中丢弃明显的噪声文本。其次，质量评估模型、领域分类器和毒性评估模型经过精心设计，分别评估剩余的已清理数据。最后，我们将这三种类型的细粒度信息集成到每段文本中。通过这种方法，我们发布了最大、高质量和细粒度的中文文本 ChineseWebText2.0，它包含 3.8TB，并且每段文本都与质量分数、领域标签、毒性标签和毒性分数相关联，从而方便 LLM 研究人员根据各种类型的细粒度信息选择数据。数据、代码和工具链可在以下网站上获得 https://github.com/CASIA-LM/ChineseWebText-2.0</paragraph>

##### **Multimodal Whole Slide Foundation Model for Pathology**
2411.19666v1 by Tong Ding, Sophia J. Wagner, Andrew H. Song, Richard J. Chen, Ming Y. Lu, Andrew Zhang, Anurag J. Vaidya, Guillaume Jaume, Muhammad Shaban, Ahrong Kim, Drew F. K. Williamson, Bowen Chen, Cristina Almagro-Perez, Paul Doucet, Sharifa Sahai, Chengkuan Chen, Daisuke Komura, Akihiro Kawabe, Shumpei Ishikawa, Georg Gerber, Tingying Peng, Long Phi Le, Faisal Mahmood

The field of computational pathology has been transformed with recent
advances in foundation models that encode histopathology region-of-interests
(ROIs) into versatile and transferable feature representations via
self-supervised learning (SSL). However, translating these advancements to
address complex clinical challenges at the patient and slide level remains
constrained by limited clinical data in disease-specific cohorts, especially
for rare clinical conditions. We propose TITAN, a multimodal whole slide
foundation model pretrained using 335,645 WSIs via visual self-supervised
learning and vision-language alignment with corresponding pathology reports and
423,122 synthetic captions generated from a multimodal generative AI copilot
for pathology. Without any finetuning or requiring clinical labels, TITAN can
extract general-purpose slide representations and generate pathology reports
that generalize to resource-limited clinical scenarios such as rare disease
retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and
find that TITAN outperforms both ROI and slide foundation models across machine
learning settings such as linear probing, few-shot and zero-shot
classification, rare cancer retrieval and cross-modal retrieval, and pathology
report generation.

摘要：計算病理學領域已因基礎模型的最新進展而轉型，這些模型透過自監督學習 (SSL) 將組織病理學感興趣區域 (ROI) 編碼成多功能且可轉移的特徵表示。然而，要解決患者和切片層面的複雜臨床挑戰，將這些進展轉化為解決方案仍受限於特定疾病群體中有限的臨床資料，尤其是罕見的臨床情況。我們提出 TITAN，這是一個多模態全切片基礎模型，使用 335,645 個 WSI 透過視覺自監督學習和與對應病理報告的視覺語言對齊，以及由多模態生成式 AI 輔助員為病理學生成的 423,122 個合成標題進行預訓練。在沒有任何微調或需要臨床標籤的情況下，TITAN 可以提取通用切片表示，並生成病理報告，以概括到資源有限的臨床場景，例如罕見疾病檢索和癌症預後。我們在不同的臨床任務上評估 TITAN，發現 TITAN 在機器學習設定中優於 ROI 和切片基礎模型，例如線性探查、少次學習和零次學習分類、罕見癌症檢索和跨模態檢索，以及病理報告生成。

##### **Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-OASIS**
2411.19655v1 by Alessandro Scirè, Andrei Stefan Bejgu, Simone Tedeschi, Karim Ghonim, Federico Martelli, Roberto Navigli

After the introduction of Large Language Models (LLMs), there have been
substantial improvements in the performance of Natural Language Generation
(NLG) tasks, including Text Summarization and Machine Translation. However,
LLMs still produce outputs containing hallucinations, that is, content not
grounded in factual information. Therefore, developing methods to assess the
factuality of LLMs has become urgent.
  Indeed, resources for factuality evaluation have recently emerged. Although
challenging, these resources face one or more of the following limitations: (i)
they are tailored to a specific task or domain; (ii) they are limited in size,
thereby preventing the training of new factuality evaluators; (iii) they are
designed for simpler verification tasks, such as claim verification.
  To address these issues, we introduce LLM-Oasis, to the best of our knowledge
the largest resource for training end-to-end factuality evaluators. LLM-Oasis
is constructed by extracting claims from Wikipedia, falsifying a subset of
these claims, and generating pairs of factual and unfactual texts. We then rely
on human annotators to both validate the quality of our dataset and to create a
gold standard test set for benchmarking factuality evaluation systems.
  Our experiments demonstrate that LLM-Oasis presents a significant challenge
for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our
proposed end-to-end factuality evaluation task, highlighting its potential to
drive future research in the field.

摘要：在大語言模型 (LLM) 問世後，自然語言生成 (NLG) 任務的表現已大幅提升，包括文字摘要和機器翻譯。然而，LLM 仍會產生包含幻覺的輸出，也就是沒有根據事實資訊的內容。因此，開發評估 LLM 事實性的方法已刻不容緩。
事實上，事實性評估資源最近已浮現。儘管面臨挑戰，這些資源仍面臨以下一個或多個限制：(i) 它們專門針對特定任務或領域；(ii) 它們的規模有限，因此無法訓練新的事實性評估器；(iii) 它們設計用於較簡單的驗證任務，例如聲明驗證。
為了解決這些問題，我們引入了 LLM-Oasis，據我們所知，這是用於訓練端對端事實性評估器最大的資源。LLM-Oasis 是透過從維基百科中萃取聲明、偽造其中一部分聲明，以及產生事實和非事實文字對來建構的。然後我們依賴人工標記員來驗證我們資料集的品質，並為事實性評估系統的基準測試建立黃金標準測試集。
我們的實驗證明，LLM-Oasis 對現有最先進的 LLM 構成重大挑戰，GPT-4o 在我們提出的端對端事實性評估任務中達到 60% 的準確率，突顯其推動該領域未來研究的潛力。

##### **Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing**
2411.19652v1 by Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen

Text-guided image generation and editing using diffusion models have achieved
remarkable advancements. Among these, tuning-free methods have gained attention
for their ability to perform edits without extensive model adjustments,
offering simplicity and efficiency. However, existing tuning-free approaches
often struggle with balancing fidelity and editing precision. Reconstruction
errors in DDIM Inversion are partly attributed to the cross-attention mechanism
in U-Net, which introduces misalignments during the inversion and
reconstruction process. To address this, we analyze reconstruction from a
structural perspective and propose a novel approach that replaces traditional
cross-attention with uniform attention maps, significantly enhancing image
reconstruction fidelity. Our method effectively minimizes distortions caused by
varying text conditions during noise prediction. To complement this
improvement, we introduce an adaptive mask-guided editing technique that
integrates seamlessly with our reconstruction approach, ensuring consistency
and accuracy in editing tasks. Experimental results demonstrate that our
approach not only excels in achieving high-fidelity image reconstruction but
also performs robustly in real image composition and editing scenarios. This
study underscores the potential of uniform attention maps to enhance the
fidelity and versatility of diffusion-based image processing methods. Code is
available at https://github.com/Mowenyii/Uniform-Attention-Maps.

摘要：文本引导的图像生成和编辑使用扩散模型已取得显著进展。其中，无调优方法因其无需大量模型调整即可执行编辑的能力而备受关注，提供了简单性和效率。然而，现有的无调优方法通常难以平衡保真度和编辑精度。DDIM 反演中的重建误差部分归因于 U-Net 中的交叉注意力机制，该机制在反演和重建过程中引入了错位。为了解决这个问题，我们从结构的角度分析重建，并提出了一种新颖的方法，用统一的注意力图替换传统的交叉注意力，显著提高了图像重建保真度。我们的方法有效地最小化了噪声预测过程中不同文本条件引起的失真。为了补充这一改进，我们引入了一种自适应遮罩引导编辑技术，该技术与我们的重建方法无缝集成，确保了编辑任务的一致性和准确性。实验结果表明，我们的方法不仅在实现高保真图像重建方面表现出色，而且在真实图像合成和编辑场景中也表现得很稳健。本研究强调了统一注意力图在增强基于扩散的图像处理方法的保真度和多功能性方面的潜力。代码可在 https://github.com/Mowenyii/Uniform-Attention-Maps 获得。

##### **CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation**
2411.19650v1 by Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo

The advancement of large Vision-Language-Action (VLA) models has
significantly improved robotic manipulation in terms of language-guided task
execution and generalization to unseen scenarios. While existing VLAs adapted
from pretrained large Vision-Language-Models (VLM) have demonstrated promising
generalizability, their task performance is still unsatisfactory as indicated
by the low tasks success rates in different environments. In this paper, we
present a new advanced VLA architecture derived from VLM. Unlike previous works
that directly repurpose VLM for action prediction by simple action
quantization, we propose a omponentized VLA architecture that has a specialized
action module conditioned on VLM output. We systematically study the design of
the action module and demonstrates the strong performance enhancement with
diffusion action transformers for action sequence modeling, as well as their
favorable scaling behaviors. We also conduct comprehensive experiments and
ablation studies to evaluate the efficacy of our models with varied designs.
The evaluation on 5 robot embodiments in simulation and real work shows that
our model not only significantly surpasses existing VLAs in task performance
and but also exhibits remarkable adaptation to new robots and generalization to
unseen objects and backgrounds. It exceeds the average success rates of OpenVLA
which has similar model size (7B) with ours by over 35% in simulated evaluation
and 55% in real robot experiments. It also outperforms the large RT-2-X model
(55B) by 18% absolute success rates in simulation. Code and models can be found
on our project page (https://cogact.github.io/).

摘要：大型視覺語言動作 (VLA) 模型的進步在語言引導任務執行和廣泛化到未見場景方面顯著改善了機器人操作。儘管從預訓練大型視覺語言模型 (VLM) 改編的現有 VLA 已展現出有希望的廣泛化能力，但根據不同環境中的低任務成功率顯示，其任務執行能力仍不令人滿意。在本文中，我們提出了一個源自 VLM 的新進階 VLA 架構。與先前直接重新利用 VLM 進行動作預測的著作不同，我們提出了一個組件化的 VLA 架構，其具有基於 VLM 輸出的專用動作模組。我們系統性地研究動作模組的設計，並展示了擴散動作Transformer在動作序列建模方面的強勁效能提升，以及它們有利的擴充行為。我們還進行了全面的實驗和消融研究，以評估我們設計多變的模型的功效。在模擬和實際工作中對 5 個機器人實體的評估顯示，我們的模型不僅在任務執行方面顯著超越現有的 VLA，而且還展現出對新機器人的顯著適應性和對未見物體和背景的廣泛化能力。它超過了 OpenVLA 的平均成功率，而 OpenVLA 具有與我們相似的模型大小 (7B)，在模擬評估中高出 35%，在真實機器人實驗中高出 55%。它還在模擬中以 18% 的絕對成功率優於大型 RT-2-X 模型 (55B)。程式碼和模型可以在我們的專案頁面 (https://cogact.github.io/) 中找到。

##### **CAdam: Confidence-Based Optimization for Online Learning**
2411.19647v1 by Shaowen Wang, Anan Liu, Jian Xiao, Huan Liu, Yuekui Yang, Cong Xu, Qianqian Pu, Suncong Zheng, Wei Zhang, Jian Li

Modern recommendation systems frequently employ online learning to
dynamically update their models with freshly collected data. The most commonly
used optimizer for updating neural networks in these contexts is the Adam
optimizer, which integrates momentum ($m_t$) and adaptive learning rate
($v_t$). However, the volatile nature of online learning data, characterized by
its frequent distribution shifts and presence of noises, poses significant
challenges to Adam's standard optimization process: (1) Adam may use outdated
momentum and the average of squared gradients, resulting in slower adaptation
to distribution changes, and (2) Adam's performance is adversely affected by
data noise. To mitigate these issues, we introduce CAdam, a confidence-based
optimization strategy that assesses the consistence between the momentum and
the gradient for each parameter dimension before deciding on updates. If
momentum and gradient are in sync, CAdam proceeds with parameter updates
according to Adam's original formulation; if not, it temporarily withholds
updates and monitors potential shifts in data distribution in subsequent
iterations. This method allows CAdam to distinguish between the true
distributional shifts and mere noise, and adapt more quickly to new data
distributions. Our experiments with both synthetic and real-world datasets
demonstrate that CAdam surpasses other well-known optimizers, including the
original Adam, in efficiency and noise robustness. Furthermore, in large-scale
A/B testing within a live recommendation system, CAdam significantly enhances
model performance compared to Adam, leading to substantial increases in the
system's gross merchandise volume (GMV).

摘要：<paragraph>現代推薦系統經常使用線上學習來動態地更新模型，使用新收集的資料。在這些情況下，用於更新神經網路最常見的最佳化器是 Adam 最佳化器，它整合了動能 ($m_t$) 和自適應學習率 ($v_t$)。然而，線上學習資料的波動性質，其特徵是頻繁的分配轉移和雜訊的存在，對 Adam 的標準最佳化過程構成重大挑戰：(1) Adam 可能使用過時的動能和平方梯度的平均值，導致適應分配變化的速度較慢，以及 (2) Adam 的效能受到資料雜訊的不利影響。為了減輕這些問題，我們引入了 CAdam，一種基於信心的最佳化策略，它在決定更新之前評估動能和每個參數維度的梯度之間的一致性。如果動能和梯度同步，CAdam 會根據 Adam 的原始公式進行參數更新；如果不是，它會暫時保留更新並監控後續反覆運算中資料分配的潛在轉移。這種方法允許 CAdam 區分真正的分配轉移和單純的雜訊，並更快地適應新的資料分配。我們對合成和真實世界資料集的實驗證明，CAdam 在效率和抗雜訊性方面優於其他眾所周知的最佳化器，包括原始的 Adam。此外，在一個即時推薦系統內進行大規模 A/B 測試時，與 Adam 相比，CAdam 顯著地提升了模型效能，導致系統的商品交易總額 (GMV) 大幅增加。</paragraph>

##### **LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification**
2411.19638v1 by Taja Kuzman, Nikola Ljubešić

With the ever-increasing number of news stories available online, classifying
them by topic, regardless of the language they are written in, has become
crucial for enhancing readers' access to relevant content. To address this
challenge, we propose a teacher-student framework based on large language
models (LLMs) for developing multilingual news classification models of
reasonable size with no need for manual data annotation. The framework employs
a Generative Pretrained Transformer (GPT) model as the teacher model to develop
an IPTC Media Topic training dataset through automatic annotation of news
articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits
a high zero-shot performance on all four languages. Its agreement with human
annotators is comparable to that between the human annotators themselves. To
mitigate the computational limitations associated with the requirement of
processing millions of texts daily, smaller BERT-like student models are
fine-tuned on the GPT-annotated dataset. These student models achieve high
performance comparable to the teacher model. Furthermore, we explore the impact
of the training data size on the performance of the student models and
investigate their monolingual, multilingual and zero-shot cross-lingual
capabilities. The findings indicate that student models can achieve high
performance with a relatively small number of training instances, and
demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the
best-performing news topic classifier, enabling multilingual classification
with the top-level categories of the IPTC Media Topic schema.

摘要：<paragraph>隨著網路上可取得的新聞故事數量持續增加，不論其撰寫語言為何，依主題對其進行分類已成為提升讀者取得相關內容管道的重要關鍵。為了解決此項挑戰，我們提出一個基於大型語言模型 (LLM) 的師生架構，用於開發合理大小的多語言新聞分類模型，且無需手動資料標註。此架構採用生成式預訓練轉換器 (GPT) 模型作為教師模型，透過自動標註斯洛維尼亞文、克羅埃西亞文、希臘文和加泰隆尼亞文的新聞文章，來開發 IPTC 媒體主題訓練資料集。教師模型在所有四種語言上皆展現出極高的零次學習效能。其與人類標註員的一致性，與人類標註員之間的一致性相當。為減輕與每日處理數百萬筆文字需求相關的運算限制，較小的類 BERT 學生模型在 GPT 標註的資料集上進行微調。這些學生模型可達成與教師模型相當的高效能。此外，我們探討訓練資料大小對學生模型效能的影響，並調查其單語、多語和零次學習跨語言能力。研究結果顯示，學生模型即使在訓練案例數量相對較少的情況下，也能達成高效率能，並展現出強大的零次學習跨語言能力。最後，我們發布效能最佳的新聞主題分類器，並使用 IPTC 媒體主題架構的頂層類別進行多語言分類。</paragraph>

##### **Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings**
2411.19628v1 by Qiong Wu, Wenhao Lin, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji

The excessive use of visual tokens in existing Multimoal Large Language
Models (MLLMs) often exhibits obvious redundancy and brings in prohibitively
expensive computation. To gain insights into this problem, we first conduct
extensive empirical studies on the attention behaviors of MLLMs, and summarize
three main inference stages in MLLMs: (i) Early fusion between tokens is first
accomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)
Multimodal reasoning} resumes and lasts until the end of inference. In
particular, we reveal that visual tokens will stop contributing to reasoning
when the text tokens receive enough image information, yielding obvious visual
redundancy. Based on these generalized observations, we propose a simple yet
effective method to improve the efficiency of MLLMs, termed dynamic
visual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive
the text token status and decide the removal of all visual tokens after a
certain layer, thereby addressing the observed visual redundancy. To validate
VTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,
and conduct extensive experiments on a bunch of benchmarks. The experiment
results not only show the effectiveness of our VTE in improving MLLMs'
efficiency, but also yield the general modeling patterns of MLLMs, well
facilitating the in-depth understanding of MLLMs. Our code is anonymously
released at https://github.com/DoubtedSteam/DyVTE.

摘要：在現有的多模態大型語言模型 (MLLM) 中過度使用視覺符號通常會表現出明顯的冗餘，並帶來極其昂貴的計算成本。為了深入了解這個問題，我們首先對 MLLM 的注意力行為進行了廣泛的實證研究，並總結了 MLLM 中的三個主要推理階段：(i) 符號之間的早期融合首先快速完成。(ii) 然後進行模態內建模。(iii) 多模態推理} 恢復並持續到推理結束。特別是，我們揭示了當文本符號接收足夠的圖像信息時，視覺符號將停止對推理做出貢獻，從而產生明顯的視覺冗餘。基於這些概括的觀察，我們提出了一種簡單但有效的方法來提高 MLLM 的效率，稱為動態視覺符號退出 (DyVTE)。DyVTE 使用輕量級超網路感知文本符號狀態，並決定在某一層之後刪除所有視覺符號，從而解決觀察到的視覺冗餘。為了驗證 VTE，我們將其應用於一組 MLLM，包括 LLaVA、VILA、Eagle 和 InternVL，並對一堆基準進行了廣泛的實驗。實驗結果不僅證明了我們的 VTE 在提高 MLLM 效率方面的有效性，而且還產生了 MLLM 的一般建模模式，很好地促進了對 MLLM 的深入理解。我們的代碼已匿名發布在 https://github.com/DoubtedSteam/DyVTE。

##### **GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding**
2411.19626v1 by Yawen Shao, Wei Zhai, Yuhang Yang, Hongchen Luo, Yang Cao, Zheng-Jun Zha

Open-Vocabulary 3D object affordance grounding aims to anticipate ``action
possibilities'' regions on 3D objects with arbitrary instructions, which is
crucial for robots to generically perceive real scenarios and respond to
operational changes. Existing methods focus on combining images or languages
that depict interactions with 3D geometries to introduce external interaction
priors. However, they are still vulnerable to a limited semantic space by
failing to leverage implied invariant geometries and potential interaction
intentions. Normally, humans address complex tasks through multi-step reasoning
and respond to diverse situations by leveraging associative and analogical
thinking. In light of this, we propose GREAT (GeometRy-intEntion collAboraTive
inference) for Open-Vocabulary 3D Object Affordance Grounding, a novel
framework that mines the object invariant geometry attributes and performs
analogically reason in potential interaction scenarios to form affordance
knowledge, fully combining the knowledge with both geometries and visual
contents to ground 3D object affordance. Besides, we introduce the Point Image
Affordance Dataset v2 (PIADv2), the largest 3D object affordance dataset at
present to support the task. Extensive experiments demonstrate the
effectiveness and superiority of GREAT. Code and dataset are available at
project.

摘要：開放詞彙 3D 物件可負擔基礎旨在預測 3D 物件上具有任意指示的「動作可能性」區域，對於機器人以通用方式感知真實場景和回應操作變更至關重要。現有方法專注於結合描繪與 3D 幾何形狀互動的影像或語言，以引入外部互動先驗。然而，它們仍容易受到語義空間有限的影響，因為無法利用隱含的不變幾何形狀和潛在的互動意圖。通常，人類透過多步驟推理來處理複雜任務，並透過聯想和類比思考來回應不同的情況。有鑑於此，我們提出 GREAT（GeometRy-intEntion collAboraTive inference），用於開放詞彙 3D 物件可負擔基礎，一個新穎的架構，用於挖掘物件不變幾何屬性，並在潛在互動場景中進行類比推理，以形成可負擔知識，將知識與幾何形狀和視覺內容完全結合，以奠定 3D 物件可負擔基礎。此外，我們引入了 Point Image Affordance Dataset v2 (PIADv2)，目前最大的 3D 物件可負擔資料集，以支援這項任務。廣泛的實驗證明了 GREAT 的有效性和優越性。程式碼和資料集可在專案中取得。

##### **FairDD: Fair Dataset Distillation via Synchronized Matching**
2411.19623v1 by Qihang Zhou, Shenhao Fang, Shibo He, Wenchao Meng, Jiming Chen

Condensing large datasets into smaller synthetic counterparts has
demonstrated its promise for image classification. However, previous research
has overlooked a crucial concern in image recognition: ensuring that models
trained on condensed datasets are unbiased towards protected attributes (PA),
such as gender and race. Our investigation reveals that dataset distillation
(DD) fails to alleviate the unfairness towards minority groups within original
datasets. Moreover, this bias typically worsens in the condensed datasets due
to their smaller size. To bridge the research gap, we propose a novel fair
dataset distillation (FDD) framework, namely FairDD, which can be seamlessly
applied to diverse matching-based DD approaches, requiring no modifications to
their original architectures. The key innovation of FairDD lies in
synchronously matching synthetic datasets to PA-wise groups of original
datasets, rather than indiscriminate alignment to the whole distributions in
vanilla DDs, dominated by majority groups. This synchronized matching allows
synthetic datasets to avoid collapsing into majority groups and bootstrap their
balanced generation to all PA groups. Consequently, FairDD could effectively
regularize vanilla DDs to favor biased generation toward minority groups while
maintaining the accuracy of target attributes. Theoretical analyses and
extensive experimental evaluations demonstrate that FairDD significantly
improves fairness compared to vanilla DD methods, without sacrificing
classification accuracy. Its consistent superiority across diverse DDs,
spanning Distribution and Gradient Matching, establishes it as a versatile FDD
approach.

摘要：將大型資料集濃縮成較小的合成對應資料集已證明其對影像分類很有幫助。然而，先前的研究忽略了影像辨識中的關鍵問題：確保在濃縮資料集上訓練的模型對受保護屬性 (PA)（例如性別和種族）沒有偏見。我們的調查顯示，資料集蒸餾 (DD) 無法減輕對原始資料集中少數群體的不公平性。此外，由於濃縮資料集的規模較小，因此這種偏見通常會惡化。為了彌補研究差距，我們提出了一個新的公平資料集蒸餾 (FDD) 架構，即 FairDD，它可以無縫地應用於各種基於匹配的 DD 方法，而不需要修改其原始架構。FairDD 的關鍵創新在於同步匹配合成資料集與原始資料集的 PA 智慧群組，而不是像傳統 DD 中由多數群組主導的無差別對齊到整個分布。這種同步匹配允許合成資料集避免崩潰到多數群組，並引導它們平衡生成到所有 PA 群組。因此，FairDD 可以有效地規範傳統 DD，以利於對少數群體的偏見生成，同時保持目標屬性的準確性。理論分析和廣泛的實驗評估表明，與傳統 DD 方法相比，FairDD 大幅提升了公平性，同時不犧牲分類準確性。它在各種 DD 中的一致優異表現，包括分佈和梯度匹配，使其成為一種通用的 FDD 方法。

##### **Can Large Language Models Reason about the Region Connection Calculus?**
2411.19589v1 by Anthony G Cohn, Robert E Blackwell

Qualitative Spatial Reasoning is a well explored area of Knowledge
Representation and Reasoning and has multiple applications ranging from
Geographical Information Systems to Robotics and Computer Vision. Recently,
many claims have been made for the reasoning capabilities of Large Language
Models (LLMs). Here, we investigate the extent to which a set of representative
LLMs can perform classical qualitative spatial reasoning tasks on the
mereotopological Region Connection Calculus, RCC-8. We conduct three pairs of
experiments (reconstruction of composition tables, alignment to human
composition preferences, conceptual neighbourhood reconstruction) using
state-of-the-art LLMs; in each pair one experiment uses eponymous relations and
one, anonymous relations (to test the extent to which the LLM relies on
knowledge about the relation names obtained during training). All instances are
repeated 30 times to measure the stochasticity of the LLMs.

摘要：定性的空間推理是知識表徵與推理中廣泛探索的領域，且有從地理資訊系統到機器人和電腦視覺等多種應用。最近，對於大型語言模型 (LLM) 的推理能力提出了許多主張。在此，我們探討一組具代表性的 LLM 在僅具備拓撲區域連接演算 (RCC-8) 的古典定性空間推理任務中所能執行的程度。我們使用最先進的 LLM 進行三組實驗（重構組合表、對齊人類組合偏好、概念鄰域重構）；在每組實驗中，一個實驗使用同名的關係，另一個使用匿名的關係（用於測試 LLM 在訓練期間獲得的關係名稱知識的依賴程度）。所有範例重複 30 次以測量 LLM 的隨機性。

##### **Solving Rubik's Cube Without Tricky Sampling**
2411.19583v1 by Yicheng Lin, Siyu Liang

The Rubiks Cube, with its vast state space and sparse reward structure,
presents a significant challenge for reinforcement learning (RL) due to the
difficulty of reaching rewarded states. Previous research addressed this by
propagating cost-to-go estimates from the solved state and incorporating search
techniques. These approaches differ from human strategies that start from fully
scrambled cubes, which can be tricky for solving a general sparse-reward
problem. In this paper, we introduce a novel RL algorithm using policy gradient
methods to solve the Rubiks Cube without relying on near solved-state sampling.
Our approach employs a neural network to predict cost patterns between states,
allowing the agent to learn directly from scrambled states. Our method was
tested on the 2x2x2 Rubiks Cube, where the cube was scrambled 50,000 times, and
the model successfully solved it in over 99.4% of cases. Notably, this result
was achieved using only the policy network without relying on tree search as in
previous methods, demonstrating its effectiveness and potential for broader
applications in sparse-reward problems.

摘要：魔術方塊擁有廣大的狀態空間和稀疏的獎勵結構，
由於難以達到獎勵狀態，因此對強化學習 (RL) 來說是一個重大的挑戰。先前的研究透過從已解決的狀態傳播成本估計值並納入搜尋技術來解決這個問題。這些方法不同於從完全打亂的方塊開始的人類策略，這對於解決一般的稀疏獎勵問題來說可能是棘手的。在本文中，我們介紹了一種新穎的 RL 演算法，使用策略梯度方法來解決魔術方塊，而無需依賴接近已解決狀態的取樣。我們的做法採用神經網路來預測狀態之間的成本模式，讓代理人可以直接從打亂的狀態中學習。我們的演算法在 2x2x2 魔術方塊上進行測試，其中方塊被打亂了 50,000 次，而模型在超過 99.4% 的情況下成功地解決了它。值得注意的是，這個結果僅使用策略網路達成，而未依賴於先前的樹狀搜尋方法，證明了其在稀疏獎勵問題中更廣泛的應用上的有效性和潛力。

##### **In-Context Learning with Noisy Labels**
2411.19581v1 by Junyong Kang, Donghyun Son, Hwanjun Song, Buru Chang

In-context learning refers to the emerging ability of large language models
(LLMs) to perform a target task without additional training, utilizing
demonstrations of the task. Recent studies aim to enhance in-context learning
performance by selecting more useful demonstrations. However, they overlook the
presence of inevitable noisy labels in task demonstrations that arise during
the labeling process in the real-world. In this paper, we propose a new task,
in-context learning with noisy labels, which aims to solve real-world problems
for in-context learning where labels in task demonstrations would be corrupted.
Moreover, we propose a new method and baseline methods for the new task,
inspired by studies in learning with noisy labels. Through experiments, we
demonstrate that our proposed method can serve as a safeguard against
performance degradation in in-context learning caused by noisy labels.

摘要：情境學習指的是大型語言模型 (LLM) 在沒有額外訓練的情況下，利用任務示範來執行目標任務的新興能力。最近的研究旨在透過選擇更有用的示範來增強情境學習表現。然而，它們忽略了在標記過程中產生的任務示範中不可避免的雜訊標籤。在本文中，我們提出了一個新的任務，即帶有雜訊標籤的情境學習，旨在解決情境學習的現實問題，其中任務示範中的標籤會被破壞。此外，我們提出了一種新方法和基準方法，靈感來自帶有雜訊標籤的學習研究。透過實驗，我們證明了我們提出的方法可以作為一種保障措施，防止情境學習中因雜訊標籤而導致的效能下降。

##### **ICPR 2024 Competition on Multilingual Claim-Span Identification**
2411.19579v1 by Soham Poddar, Biswajit Paul, Moumita Basu, Saptarshi Ghosh

A lot of claims are made in social media posts, which may contain
misinformation or fake news. Hence, it is crucial to identify claims as a first
step towards claim verification. Given the huge number of social media posts,
the task of identifying claims needs to be automated. This competition deals
with the task of 'Claim Span Identification' in which, given a text, parts /
spans that correspond to claims are to be identified. This task is more
challenging than the traditional binary classification of text into claim or
not-claim, and requires state-of-the-art methods in Pattern Recognition,
Natural Language Processing and Machine Learning. For this competition, we used
a newly developed dataset called HECSI containing about 8K posts in English and
about 8K posts in Hindi with claim-spans marked by human annotators. This paper
gives an overview of the competition, and the solutions developed by the
participating teams.

摘要：在社群媒體貼文中提出了許多主張，這些主張可能包含錯誤資訊或假新聞。因此，將主張視為主張驗證的第一步至關重要。鑑於社群媒體貼文的數量龐大，識別主張的任務需要自動化。這項競賽處理「主張跨度識別」的任務，其中在給定的文字中，識別與主張相符的部分/跨度。這項任務比傳統的二元分類（將文字分類為主張或非主張）更具挑戰性，需要模式識別、自然語言處理和機器學習的最新方法。對於這項競賽，我們使用了新開發的名為 HECSI 的資料集，其中包含約 8K 篇英文貼文和約 8K 篇印地語貼文，並由人類註解者標記主張跨度。本文概述了競賽和參與團隊開發的解決方案。

##### **KV Shifting Attention Enhances Language Modeling**
2411.19574v1 by Mingyu Xu, Wei Cheng, Bingning Wang, Weipeng Chen

The current large language models are mainly based on decode-only structure
transformers, which have great in-context learning (ICL) capabilities. It is
generally believed that the important foundation of its ICL capability is the
induction heads mechanism, which requires at least two layers attention. In
order to more efficiently implement the ability of the model's induction, we
revisit the induction heads mechanism and proposed a KV shifting attention. We
theoretically prove that the KV shifting attention reducing the model's
requirements for the depth and width of the induction heads mechanism. Our
experimental results demonstrate that KV shifting attention is beneficial to
learning induction heads and language modeling, which lead to better
performance or faster convergence from toy models to the pre-training models
with more than 10 B parameters.

摘要：目前的大語言模型主要基於僅解碼結構的 Transformer，它具有強大的情境內學習 (ICL) 能力。一般認為其 ICL 能力的重要基礎是歸納頭機制，這需要至少兩層注意力。為了更有效地實現模型歸納的能力，我們重新審視了歸納頭機制，並提出了一種 KV 轉移注意力。我們從理論上證明了 KV 轉移注意力降低了模型對歸納頭機制的深度和寬度的要求。我們的實驗結果表明，KV 轉移注意力有利於學習歸納頭和語言建模，這導致了從玩具模型到擁有超過 10B 參數的預訓練模型的更好的性能或更快的收斂。

##### **Ensemble Watermarks for Large Language Models**
2411.19563v1 by Georg Niess, Roman Kern

The rapid advancement of large language models (LLMs) has made it
increasingly difficult to distinguish between text written by humans and
machines. While watermarks already exist for LLMs, they often lack flexibility,
and struggle with attacks such as paraphrasing. To address these issues, we
propose a multi-feature method for generating watermarks that combines multiple
distinct watermark features into an ensemble watermark. Concretely, we combine
acrostica and sensorimotor norms with the established red-green watermark to
achieve a 98% detection rate. After a paraphrasing attack the performance
remains high with 95% detection rate. The red-green feature alone as baseline
achieves a detection rate of 49%. The evaluation of all feature combinations
reveals that the ensemble of all three consistently has the highest detection
rate across several LLMs and watermark strength settings. Due to the
flexibility of combining features in the ensemble, various requirements and
trade-offs can be addressed. Additionally, for all ensemble configurations the
same detection function can be used without adaptations. This method is
particularly of interest to facilitate accountability and prevent societal
harm.

摘要：大型語言模型 (LLM) 的快速進展使得區分人類和機器編寫的文字變得越來越困難。雖然 LLM 已經有浮水印，但它們通常缺乏靈活性，並且難以應對諸如改寫等攻擊。為了解決這些問題，我們提出了一種多功能方法來生成浮水印，該方法將多個不同的浮水印特徵組合成一個整體浮水印。具體來說，我們將首字母縮寫和感覺運動規範與已建立的紅綠浮水印結合起來，以實現 98% 的檢測率。在改寫攻擊後，性能仍然很高，檢測率為 95%。僅紅綠功能作為基準，檢測率為 49%。對所有特徵組合的評估表明，所有三者的整體始終在多個 LLM 和浮水印強度設置中具有最高的檢測率。由於組合整體中特徵的靈活性，可以解決各種需求和權衡。此外，對於所有整體配置，都可以使用相同的檢測功能而無需調整。這種方法特別有益於促進問責制和防止社會危害。

##### **Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning**
2411.19557v1 by Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horvath, Praneeth Vepakomma

Low-rank adapters have become a standard approach for efficiently fine-tuning
large language models (LLMs), but they often fall short of achieving the
performance of full fine-tuning. We propose a method, LoRA Silver Bullet or
LoRA-SB, that approximates full fine-tuning within low-rank subspaces using a
carefully designed initialization strategy. We theoretically demonstrate that
the architecture of LoRA-XS, which inserts a trainable (r x r) matrix between B
and A while keeping other matrices fixed, provides the precise conditions
needed for this approximation. We leverage its constrained update space to
achieve optimal scaling for high-rank gradient updates while removing the need
for hyperparameter tuning. We prove that our initialization offers an optimal
low-rank approximation of the initial gradient and preserves update directions
throughout training. Extensive experiments across mathematical reasoning,
commonsense reasoning, and language understanding tasks demonstrate that our
approach exceeds the performance of standard LoRA while using 27-90x fewer
parameters, and comprehensively outperforms LoRA-XS. Our findings establish
that it is possible to simulate full fine-tuning in low-rank subspaces, and
achieve significant efficiency gains without sacrificing performance. Our code
is publicly available at https://github.com/RaghavSinghal10/lora-sb.

摘要：低秩適配器已成為有效微調大型語言模型 (LLM) 的標準方法，但它們通常無法達到完全微調的效能。我們提出了一種方法，稱為 LoRA 銀彈或 LoRA-SB，它使用精心設計的初始化策略，在低秩子空間中近似完全微調。我們在理論上證明了 LoRA-XS 的架構，它在 B 和 A 之間插入一個可訓練的 (r x r) 矩陣，同時保持其他矩陣固定，提供了此近似所需的精確條件。我們利用其受限的更新空間，在移除超參數調整需求的同時，實現高秩梯度更新的最佳縮放。我們證明我們的初始化提供了初始梯度的最佳低秩近似，並在整個訓練過程中保留更新方向。跨越數學推理、常識推理和語言理解任務的廣泛實驗證明，我們的做法超過了標準 LoRA 的效能，同時使用少 27-90 倍的參數，並全面優於 LoRA-XS。我們的發現證實了在低秩子空間中模擬完全微調是可行的，而且可以在不犧牲效能的情況下實現顯著的效率提升。我們的程式碼可在 https://github.com/RaghavSinghal10/lora-sb 公開取得。

##### **Unimib Assistant: designing a student-friendly RAG-based chatbot for all their needs**
2411.19554v1 by Chiara Antico, Stefano Giordano, Cansu Koyuturk, Dimitri Ognibene

Natural language processing skills of Large Language Models (LLMs) are
unprecedented, having wide diffusion and application in different tasks. This
pilot study focuses on specializing ChatGPT behavior through a
Retrieval-Augmented Generation (RAG) system using the OpenAI custom GPTs
feature. The purpose of our chatbot, called Unimib Assistant, is to provide
information and solutions to the specific needs of University of Milano-Bicocca
(Unimib) students through a question-answering approach. We provided the system
with a prompt highlighting its specific purpose and behavior, as well as
university-related documents and links obtained from an initial need-finding
phase, interviewing six students. After a preliminary customization phase, a
qualitative usability test was conducted with six other students to identify
the strengths and weaknesses of the chatbot, with the goal of improving it in a
subsequent redesign phase. While the chatbot was appreciated for its
user-friendly experience, perceived general reliability, well-structured
responses, and conversational tone, several significant technical and
functional limitations emerged. In particular, the satisfaction and overall
experience of the users was impaired by the system's inability to always
provide fully accurate information. Moreover, it would often neglect to report
relevant information even if present in the materials uploaded and prompt
given. Furthermore, it sometimes generated unclickable links, undermining its
trustworthiness, since providing the source of information was an important
aspect for our users. Further in-depth studies and feedback from other users as
well as implementation iterations are planned to refine our Unimib Assistant.

摘要：大型语言模型 (LLM) 的自然语言处理技能前所未有，在不同的任务中具有广泛的传播和应用。这项试点研究重点通过使用 OpenAI 自定义 GPT 功能的检索增强生成 (RAG) 系统，专门化 ChatGPT 行为。我们名为 Unimib Assistant 的聊天的目的是通过问答方式为米兰比可卡大学 (Unimib) 学生的特定需求提供信息和解决方案。我们通过一个提示为系统提供了其特定目的和行为，以及从最初的需求发现阶段获得的与大学相关的文档和链接，采访了六名学生。在初步定制阶段后，对另外六名学生进行了定性的可用性测试，以识别聊天的优点和缺点，目的是在随后的重新设计阶段对其进行改进。虽然聊天因其用户友好的体验、感知到的总体可靠性、结构良好的响应和对话语调而受到赞赏，但出现了一些重大的技术和功能限制。特别是，由于系统无法始终提供完全准确的信息，用户的满意度和整体体验受到了损害。此外，即使在上传的材料和给定的提示中存在相关信息，它也经常忽略报告相关信息。此外，它有时会生成不可点击的链接，从而破坏其可信度，因为为用户提供信息来源是我们用户的一个重要方面。计划进一步深入研究和收集其他用户的反馈以及实施迭代，以完善我们的 Unimib Assistant。

##### **ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration**
2411.19548v1 by Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, Yifei Zhan, Kun Zhan, Peng Jia, Xianpeng Lang, Xingang Wang, Wenjun Mei

Closed-loop simulation is crucial for end-to-end autonomous driving. Existing
sensor simulation methods (e.g., NeRF and 3DGS) reconstruct driving scenes
based on conditions that closely mirror training data distributions. However,
these methods struggle with rendering novel trajectories, such as lane changes.
Recent works have demonstrated that integrating world model knowledge
alleviates these issues. Despite their efficiency, these approaches still
encounter difficulties in the accurate representation of more complex
maneuvers, with multi-lane shifts being a notable example. Therefore, we
introduce ReconDreamer, which enhances driving scene reconstruction through
incremental integration of world model knowledge. Specifically, DriveRestorer
is proposed to mitigate artifacts via online restoration. This is complemented
by a progressive data update strategy designed to ensure high-quality rendering
for more complex maneuvers. To the best of our knowledge, ReconDreamer is the
first method to effectively render in large maneuvers. Experimental results
demonstrate that ReconDreamer outperforms Street Gaussians in the NTA-IoU,
NTL-IoU, and FID, with relative improvements by 24.87%, 6.72%, and 29.97%.
Furthermore, ReconDreamer surpasses DriveDreamer4D with PVG during large
maneuver rendering, as verified by a relative improvement of 195.87% in the
NTA-IoU metric and a comprehensive user study.

摘要：封閉迴路模擬對於端到端的自動駕駛至關重要。現有的感測器模擬方法（例如 NeRF 和 3DGS）根據與訓練資料分佈高度相似的條件重建駕駛場景。然而，這些方法難以呈現新的軌跡，例如變換車道。最近的研究表明，整合世界模型知識可以緩解這些問題。儘管這些方法很有效率，但在準確表示更複雜的機動方面仍然遇到困難，其中多車道變換是一個顯著的例子。因此，我們引入了 ReconDreamer，它透過逐步整合世界模型知識來增強駕駛場景重建。具體來說，提出了 DriveRestorer 以透過線上修復來減輕人工製品。這由進步的資料更新策略所補充，旨在確保更複雜機動的高品質渲染。據我們所知，ReconDreamer 是第一個有效呈現大型機動的方法。實驗結果表明，ReconDreamer 在 NTA-IoU、NTL-IoU 和 FID 中優於 Street Gaussians，相對改進了 24.87%、6.72% 和 29.97%。此外，ReconDreamer 在大型機動渲染期間使用 PVG 超越了 DriveDreamer4D，這由 NTA-IoU 指標中 195.87% 的相對改進和全面的使用者研究所驗證。

##### **Training Agents with Weakly Supervised Feedback from Large Language Models**
2411.19547v1 by Dihong Gong, Pu Lu, Zelong Wang, Meng Zhou, Xiuqiang He

Large Language Models (LLMs) offer a promising basis for creating agents that
can tackle complex tasks through iterative environmental interaction. Existing
methods either require these agents to mimic expert-provided trajectories or
rely on definitive environmental feedback for reinforcement learning which
limits their application to specific scenarios like gaming or code generation.
This paper introduces a novel training method for LLM-based agents using weakly
supervised signals from a critic LLM, bypassing the need for expert
trajectories or definitive feedback. Our agents are trained in iterative
manner, where they initially generate trajectories through environmental
interaction. Subsequently, a critic LLM selects a subset of good trajectories,
which are then used to update the agents, enabling them to generate improved
trajectories in the next iteration. Extensive tests on the API-bank dataset
show consistent improvement in our agents' capabilities and comparable
performance to GPT-4, despite using open-source models with much fewer
parameters.

摘要：大型語言模型 (LLM) 提供了一個有希望的基礎，用於建立代理，這些代理可以通過反覆的環境互動來處理複雜任務。現有方法要求這些代理模擬專家提供的軌跡，或者依賴於確定性的環境反饋來進行強化學習，這會將它們的應用限制在特定場景中，例如遊戲或程式碼生成。本文介紹了一種使用來自評論 LLM 的弱監督信號來訓練 LLM 基礎代理的新穎訓練方法，繞過了對專家軌跡或確定性反饋的需求。我們的代理以反覆的方式進行訓練，它們最初通過環境互動來生成軌跡。隨後，評論 LLM 選擇了一組好的軌跡，然後用於更新代理，使它們能夠在下一次迭代中生成改進的軌跡。在 API-bank 資料集上進行的廣泛測試表明，我們的代理的能力持續提高，並且儘管使用開源模型的參數少得多，但性能與 GPT-4 相當。

##### **SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders**
2411.19544v1 by Niki Martinel, Mariano Serrao, Christian Micheloni

We introduce a novel state-space model (SSM)-based framework for
skeleton-based human action recognition, with an anatomically-guided
architecture that improves state-of-the-art performance in both clinical
diagnostics and general action recognition tasks. Our approach decomposes
skeletal motion analysis into spatial, temporal, and spatio-temporal streams,
using channel partitioning to capture distinct movement characteristics
efficiently. By implementing a structured, multi-directional scanning strategy
within SSMs, our model captures local joint interactions and global motion
patterns across multiple anatomical body parts. This anatomically-aware
decomposition enhances the ability to identify subtle motion patterns critical
in medical diagnosis, such as gait anomalies associated with neurological
conditions. On public action recognition benchmarks, i.e., NTU RGB+D, NTU RGB+D
120, and NW-UCLA, our model outperforms current state-of-the-art methods,
achieving accuracy improvements up to $3.2\%$ with lower computational
complexity than previous leading transformer-based models. We also introduce a
novel medical dataset for motion-based patient neurological disorder analysis
to validate our method's potential in automated disease diagnosis.

摘要：<paragraph>我們提出一個新穎的基於狀態空間模型 (SSM) 的框架，用於基於骨架的人類動作識別，它具有解剖學指導架構，可改善臨床診斷和一般動作識別任務的最新技術性能。我們的做法將骨骼運動分析分解為空間、時間和時空流，使用通道分割來有效捕捉不同的運動特徵。通過在 SSM 中實施結構化、多向掃描策略，我們的模型捕捉到多個解剖身體部位的局部關節交互和整體運動模式。這種解剖學感知分解增強了識別微妙運動模式的能力，這些模式在醫學診斷中至關重要，例如與神經系統疾病相關的步態異常。在公共動作識別基準上，即 NTU RGB+D、NTU RGB+D 120 和 NW-UCLA，我們的模型優於當前最先進的方法，與以前領先的基於Transformer的模型相比，在較低的計算複雜度下實現了高達 3.2% 的準確度改進。我們還引入了一個新的醫學數據集，用於基於運動的患者神經系統疾病分析，以驗證我們的方法在自動疾病診斷中的潛力。</paragraph>

##### **Knowledge Management for Automobile Failure Analysis Using Graph RAG**
2411.19539v1 by Yuta Ojima, Hiroki Sakaji, Tadashi Nakamura, Hiroaki Sakata, Kazuya Seki, Yuu Teshigawara, Masami Yamashita, Kazuhiro Aoyama

This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.

摘要：本文提出了一個使用檢索增強生成（RAG）和大型語言模型（LLM）和知識圖譜（KG）的汽車故障分析知識管理系統。在汽車產業中，有越來越多的需求，將故障分析知識從經驗豐富的工程師傳授給年輕的工程師。然而，故障事件是一種連鎖反應中發生的現象，這使得初學者難以分析它們。儘管知識圖譜可以描述語義關係和結構化資訊，並有效地表示故障事件，由於它們有表示元件之間關係的能力，KG 中有許多資訊，因此年輕的工程師很難從 KG 中提取和理解子圖。另一方面，人們越來越有興趣使用 Graph RAG，這是一種結合 LLM 和 KG 進行知識管理的 RAG。然而，當將目前的 Graph RAG 框架與現有的汽車故障知識圖譜一起使用時，會出現幾個問題，因為難以生成針對非 LLM 構建的知識圖譜資料庫的可執行查詢。為了解決這個問題，我們專注於針對現有知識圖譜最佳化 Graph RAG 管道。使用原始問答資料集，所提出方法生成的句子的 ROUGE F1 分數與目前方法相比，平均提升了 157.6%。這突顯了所提出方法對於汽車故障分析的有效性。

##### **Deepfake Media Generation and Detection in the Generative AI Era: A Survey and Outlook**
2411.19537v1 by Florinel-Alin Croitoru, Andrei-Iulian Hiji, Vlad Hondru, Nicolae Catalin Ristea, Paul Irofti, Marius Popescu, Cristian Rusu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Mubarak Shah

With the recent advancements in generative modeling, the realism of deepfake
content has been increasing at a steady pace, even reaching the point where
people often fail to detect manipulated media content online, thus being
deceived into various kinds of scams. In this paper, we survey deepfake
generation and detection techniques, including the most recent developments in
the field, such as diffusion models and Neural Radiance Fields. Our literature
review covers all deepfake media types, comprising image, video, audio and
multimodal (audio-visual) content. We identify various kinds of deepfakes,
according to the procedure used to alter or generate the fake content. We
further construct a taxonomy of deepfake generation and detection methods,
illustrating the important groups of methods and the domains where these
methods are applied. Next, we gather datasets used for deepfake detection and
provide updated rankings of the best performing deepfake detectors on the most
popular datasets. In addition, we develop a novel multimodal benchmark to
evaluate deepfake detectors on out-of-distribution content. The results
indicate that state-of-the-art detectors fail to generalize to deepfake content
generated by unseen deepfake generators. Finally, we propose future directions
to obtain robust and powerful deepfake detectors. Our project page and new
benchmark are available at https://github.com/CroitoruAlin/biodeep.

摘要：隨著生成式模型的最新進展，深度偽造內容的逼真度正穩定地提升，甚至達到人們經常無法在網路上偵測出被竄改的媒體內容的程度，因此被各種詐騙手法所欺騙。在本文中，我們調查深度偽造生成和偵測技術，包括該領域最新的發展，例如擴散模型和神經輻射場。我們的文獻回顧涵蓋所有深度偽造媒體類型，包括影像、影片、音訊和多模式（音訊視覺）內容。我們根據用於變更或生成偽造內容的程序，識別出各種深度偽造。我們進一步建構深度偽造生成和偵測方法的分類法，說明方法的重要群組和這些方法的應用領域。接下來，我們收集用於深度偽造偵測的資料集，並提供在最熱門資料集上效能最佳的深度偽造偵測器的最新排名。此外，我們開發一個新穎的多模式基準，以評估深度偽造偵測器在分佈外內容上的表現。結果顯示，最先進的偵測器無法概化到由未見過的深度偽造生成器生成的深度偽造內容。最後，我們提出未來的方向，以獲得強健且強大的深度偽造偵測器。我們的專案頁面和新基準可在 https://github.com/CroitoruAlin/biodeep 取得。

##### **Quantized Delta Weight Is Safety Keeper**
2411.19530v1 by Yule Liu, Zhen Sun, Xinlei He, Xinyi Huang

Recent advancements in fine-tuning proprietary language models enable
customized applications across various domains but also introduce two major
challenges: high resource demands and security risks. Regarding resource
demands, recent work proposes novel partial compression, such as BitDelta, to
quantize the delta weights between the fine-tuned model and base model.
Regarding the security risks, user-defined fine-tuning can introduce security
vulnerabilities, such as alignment issues, backdoor attacks, and
hallucinations. However, most of the current efforts in security assessment
focus on the full-precision or full-compression models, it is not
well-discussed how the partial compression methods affect security concerns. To
bridge this gap, we evaluate the robustness of delta-weight quantization
against these security threats. In this paper, we uncover a "free lunch"
phenomenon: partial compression can enhance model security against
fine-tuning-based attacks with bearable utility loss. Using Llama-2-7b-chat as
a case study, we show that, with under 10% utility degradation, the partial
compression mitigates alignment-breaking risks by up to 66.17%, harmful
backdoor vulnerabilities by 64.46%, and targeted output manipulation risks by
up to 90.53%. We further apply LogitLens to visualize internal state
transformations during forward passes, suggesting mechanisms for both security
failure and recovery in standard versus compressed fine-tuning. This work
offers new insights into selecting effective delta compression methods for
secure, resource-efficient multi-tenant services.

摘要：<paragraph>微調專有語言模型的最新進展讓各種領域都能客製化應用，但也帶來了兩個主要挑戰：高資源需求和安全性風險。關於資源需求，近期研究提出創新的部分壓縮技術，例如 BitDelta，用於量化微調模型和基礎模型之間的 delta 權重。關於安全性風險，使用者定義的微調可能會引發安全漏洞，例如對齊問題、後門攻擊和幻覺。然而，目前在安全評估方面的大部分工作都著重於全精度或全壓縮模型，對於部分壓縮方法如何影響安全問題，討論得並不多。為了彌補這個差距，我們評估 delta 權重量化對於這些安全威脅的穩健性。在本文中，我們揭露了一個「免費午餐」現象：部分壓縮可以增強模型對於微調式攻擊的安全性，同時損失可承受的效用。以 Llama-2-7b-chat 為例，我們展示出，在效用降低不到 10% 的情況下，部分壓縮可將對齊破壞風險降低多達 66.17%，有害後門漏洞降低 64.46%，目標輸出操作風險降低多達 90.53%。我們進一步應用 LogitLens 來視覺化正向傳遞期間的內部狀態轉換，提出標準微調與壓縮微調中安全失敗和復原的機制。這項工作為安全、資源有效的多租戶服務選擇有效的 delta 壓縮方法提供了新的見解。</paragraph>

##### **RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation**
2411.19528v1 by Xianfeng Tan, Yuhan Li, Wenxiang Shang, Yubo Wu, Jian Wang, Xuanhong Chen, Yi Zhang, Ran Lin, Bingbing Ni

Standard clothing asset generation involves creating forward-facing flat-lay
garment images displayed on a clear background by extracting clothing
information from diverse real-world contexts, which presents significant
challenges due to highly standardized sampling distributions and precise
structural requirements in the generated images. Existing models have limited
spatial perception and often exhibit structural hallucinations in this
high-specification generative task. To address this issue, we propose a novel
Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance
structure determinacy and mitigate hallucinations by assimilating external
knowledge from LLM and databases. RAGDiffusion consists of two core processes:
(1) Retrieval-based structure aggregation, which employs contrastive learning
and a Structure Locally Linear Embedding (SLLE) to derive global structure and
spatial landmarks, providing both soft and hard guidance to counteract
structural ambiguities; and (2) Omni-level faithful garment generation, which
introduces a three-level alignment that ensures fidelity in structural,
pattern, and decoding components within the diffusing. Extensive experiments on
challenging real-world datasets demonstrate that RAGDiffusion synthesizes
structurally and detail-faithful clothing assets with significant performance
improvements, representing a pioneering effort in high-specification faithful
generation with RAG to confront intrinsic hallucinations and enhance fidelity.

摘要：標準服裝資產生成涉及建立在明確背景上顯示的正面平鋪服裝影像，透過從多樣化的真實世界情境中擷取服裝資訊，這會造成顯著的挑戰，因為在生成的影像中，取樣分配高度標準化，且結構需求精確。現有的模型具有有限的空間感知，且在這個高規格的生成任務中，通常會展現結構性幻覺。為了解決這個問題，我們提出一個創新的檢索增強生成 (RAG) 架構，稱為 RAGDiffusion，透過同化來自 LLM 和資料庫的外部知識來增強結構確定性並減輕幻覺。RAGDiffusion 包含兩個核心流程：(1) 基於檢索的結構聚合，採用對比學習和結構局部線性嵌入 (SLLE) 來推導整體結構和空間地標，提供軟性和硬性指導來對抗結構模糊性；以及 (2) 全方位忠實服裝生成，在擴散中引入三層對齊，確保結構、圖案和解碼元件的保真度。在具有挑戰性的真實世界資料集上進行的廣泛實驗證明，RAGDiffusion 合成了結構和細節都忠實的服裝資產，效能顯著提升，代表了在高規格忠實生成中使用 RAG 來對抗內在幻覺並提升保真度的先驅性努力。

##### **DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding**
2411.19527v1 by Jungbin Cho, Junwan Kim, Jisoo Kim, Minseo Kim, Mingu Kang, Sungeun Hong, Tae-Hyun Oh, Youngjae Yu

Human motion, inherently continuous and dynamic, presents significant
challenges for generative models. Despite their dominance, discrete
quantization methods, such as VQ-VAEs, suffer from inherent limitations,
including restricted expressiveness and frame-wise noise artifacts. Continuous
approaches, while producing smoother and more natural motions, often falter due
to high-dimensional complexity and limited training data. To resolve this
"discord" between discrete and continuous representations, we introduce
DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a
novel method that decodes discrete motion tokens into continuous motion through
rectified flow. By employing an iterative refinement process in the continuous
space, DisCoRD captures fine-grained dynamics and ensures smoother and more
natural motions. Compatible with any discrete-based framework, our method
enhances naturalness without compromising faithfulness to the conditioning
signals. Extensive evaluations demonstrate that DisCoRD achieves
state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on
KIT-ML. These results solidify DisCoRD as a robust solution for bridging the
divide between discrete efficiency and continuous realism. Our project page is
available at: https://whwjdqls.github.io/discord.github.io/.

摘要：人類動作本質上是連續且動態的，對生成模型而言是一項重大的挑戰。儘管離散量化方法（例如 VQ-VAE）佔據主導地位，但它們卻有固有的限制，包括表達力受限和逐幀雜訊偽影。連續方法雖然產生了更平滑、更自然的動作，但由於高維複雜性和有限的訓練資料，常常會失敗。為了解決離散和連續表示之間的「不協調」，我們引入了 DisCoRD：透過整流流解碼將離散動作代碼解碼為連續動作，這是一種透過整流流將離散動作代碼解碼為連續動作的新方法。透過在連續空間中採用迭代優化程序，DisCoRD 捕捉了細微的動態，並確保了更平滑、更自然的動作。我們的這種方法與任何基於離散的方法相容，它增強了自然度，同時不影響對條件訊號的忠實度。廣泛的評估證明，DisCoRD 達到了最先進的效能，在 HumanML3D 上的 FID 為 0.032，在 KIT-ML 上為 0.169。這些結果穩固了 DisCoRD 作為在離散效率和連續真實感之間搭起橋樑的強大解決方案。我們的專案頁面可於下列網址取得：https://whwjdqls.github.io/discord.github.io/。

##### **RL-MILP Solver: A Reinforcement Learning Approach for Solving Mixed-Integer Linear Programs with Graph Neural Networks**
2411.19517v1 by Tae-Hoon Lee, Min-Soo Kim

Mixed-Integer Linear Programming (MILP) is an optimization technique widely
used in various fields. Primal heuristics, which reduce the search space of
MILP, have enabled traditional solvers (e.g., Gurobi) to efficiently find
high-quality solutions. However, traditional primal heuristics rely on expert
knowledge, motivating the advent of machine learning (ML)-based primal
heuristics that learn repetitive patterns in MILP. Nonetheless, existing
ML-based primal heuristics do not guarantee solution feasibility (i.e.,
satisfying all constraints) and primarily focus on prediction for binary
decision variables. When addressing MILP involving non-binary integer variables
using ML-based approaches, feasibility issues can become even more pronounced.
Since finding an optimal solution requires satisfying all constraints,
addressing feasibility is critical. To overcome these limitations, we propose a
novel reinforcement learning (RL)-based solver that interacts with MILP to find
feasible solutions, rather than delegating sub-problems to traditional solvers.
We design reward functions tailored for MILP, which enables the RL agent to
learn relationships between decision variables and constraints. Additionally,
to effectively model complex relationships among decision variables, we
leverage a Transformer encoder-based graph neural network (GNN). Our
experimental results demonstrate that the proposed method can solve MILP
problems and find near-optimal solutions without delegating the remainder to
traditional solvers. The proposed method provides a meaningful step forward as
an initial study in solving MILP problems end-to-end based solely on ML.

摘要：混合整數線性規劃 (MILP) 是一種廣泛用於各個領域的最佳化技術。簡化 MILP 搜尋空間的原始啟發法讓傳統求解器 (例如 Gurobi) 能夠有效率地找到高品質的解法。然而，傳統的原始啟發法仰賴專家知識，這促使基於機器學習 (ML) 的原始啟發法出現，這種啟發法會學習 MILP 中的重複模式。儘管如此，現有的基於 ML 的原始啟發法無法保證解法的可行性 (亦即滿足所有約束條件)，而且主要專注於預測二元決策變數。當使用基於 ML 的方法處理包含非二元整數變數的 MILP 時，可行性問題可能會變得更加明顯。由於找到最佳解法需要滿足所有約束條件，因此處理可行性至關重要。為了克服這些限制，我們提出了一種新的基於強化學習 (RL) 的求解器，它會與 MILP 互動以找到可行的解法，而不是將子問題委派給傳統求解器。我們設計了專門針對 MILP 的獎勵函數，讓 RL 代理程式能夠學習決策變數與約束條件之間的關係。此外，為了有效地模擬決策變數之間的複雜關係，我們利用了基於 Transformer 編碼器的圖神經網路 (GNN)。我們的實驗結果證明，所提出的方法可以解決 MILP 問題，並在不將剩餘問題委派給傳統求解器的狀況下找到近乎最佳的解法。所提出的方法作為一個純粹基於 ML 的端對端解決 MILP 問題的初步研究，提供了有意義的進展。

##### **TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with Scalable Context and Symbolic Extension**
2411.19504v1 by Zipeng Qiu, You Peng, Guangxin He, Binhang Yuan, Chen Wang

The advent of large language models (LLMs) has unlocked great opportunities
in complex data management tasks, particularly in question answering (QA) over
complicated multi-table relational data. Despite significant progress,
systematically evaluating LLMs on multi-table QA remains a critical challenge
due to the inherent complexity of analyzing heterogeneous table structures and
potential large scale of serialized relational data. Existing benchmarks
primarily focus on single-table QA, failing to capture the intricacies of
reasoning across multiple relational tables, as required in real-world domains
such as finance, healthcare, and e-commerce. To address this gap, we present
TQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities
of LLMs in tackling complex QA tasks over relational data. Our benchmark
incorporates diverse relational database instances sourced from real-world
public datasets and introduces a flexible sampling mechanism to create tasks
with varying multi-table context lengths, ranging from 8K to 64K tokens. To
ensure robustness and reliability, we integrate symbolic extensions into the
evaluation framework, enabling the assessment of LLM reasoning capabilities
beyond simple data retrieval or probabilistic pattern matching. We
systematically evaluate a range of LLMs, both open-source and closed-source,
spanning model scales from 7 billion to 70 billion parameters. Our extensive
experiments reveal critical insights into the performance of LLMs in
multi-table QA, highlighting both challenges and opportunities for advancing
their application in complex, data-driven environments. Our benchmark
implementation and results are available at
https://github.com/Relaxed-System-Lab/TQA-Bench.

摘要：大型語言模型（LLM）的出現為複雜資料管理任務開啟了絕佳的機遇，特別是在複雜的多表關聯資料中進行問答（QA）。儘管進展顯著，但由於分析異質表結構和序列化關聯資料的潛在規模的複雜性，在多表 QA 上系統性地評估 LLM 仍然是一項重大的挑戰。現有的基準主要關注單表 QA，未能捕捉到跨多個關聯表的推理複雜性，這在金融、醫療保健和電子商務等現實世界領域中是必需的。為了解決這個差距，我們提出了 TQA-Bench，這是一個新的多表 QA 基準，旨在評估 LLM 在處理關聯資料上的複雜 QA 任務的能力。我們的基準納入了從現實世界公共資料集採集的不同關聯資料庫實例，並引入了一個靈活的抽樣機制來建立具有不同多表內容長度的任務，範圍從 8K 到 64K 個符號。為了確保穩健性和可靠性，我們將符號擴充整合到評估框架中，使 LLM 推理能力的評估不僅限於簡單的資料檢索或機率模式匹配。我們系統性地評估了一系列 LLM，包括開源和閉源，模型規模從 70 億到 700 億個參數。我們廣泛的實驗揭示了 LLM 在多表 QA 中效能的重要見解，突出了在複雜的資料驅動環境中推進其應用所面臨的挑戰和機遇。我們的基準實作和結果可在 https://github.com/Relaxed-System-Lab/TQA-Bench 取得。

##### **Knowledge-Data Fusion Based Source-Free Semi-Supervised Domain Adaptation for Seizure Subtype Classification**
2411.19502v1 by Ruimin Peng, Jiayu An, Dongrui Wu

Electroencephalogram (EEG)-based seizure subtype classification enhances
clinical diagnosis efficiency. Source-free semi-supervised domain adaptation
(SF-SSDA), which transfers a pre-trained model to a new dataset with no source
data and limited labeled target data, can be used for privacy-preserving
seizure subtype classification. This paper considers two challenges in SF-SSDA
for EEG-based seizure subtype classification: 1) How to effectively fuse both
raw EEG data and expert knowledge in classifier design? 2) How to align the
source and target domain distributions for SF-SSDA? We propose a Knowledge-Data
Fusion based SF-SSDA approach, KDF-MutualSHOT, for EEG-based seizure subtype
classification. In source model training, KDF uses Jensen-Shannon Divergence to
facilitate mutual learning between a feature-driven Decision Tree-based model
and a data-driven Transformer-based model. To adapt KDF to a new target
dataset, an SF-SSDA algorithm, MutualSHOT, is developed, which features a
consistency-based pseudo-label selection strategy. Experiments on the public
TUSZ and CHSZ datasets demonstrated that KDF-MutualSHOT outperformed other
supervised and source-free domain adaptation approaches in cross-subject
seizure subtype classification.

摘要：基於腦電圖 (EEG) 的癲癇亞型分類可提升臨床診斷效率。無來源半監督領域適應 (SF-SSDA) 可將預先訓練的模型轉移至沒有來源資料且標籤目標資料有限的新資料集，可用於隱私保護的癲癇亞型分類。本文探討 SF-SSDA 在基於 EEG 的癲癇亞型分類中的兩個挑戰：1) 如何有效融合原始 EEG 資料和分類器設計中的專家知識？2) 如何調整 SF-SSDA 的來源和目標網域分佈？我們提出一個基於知識資料融合的 SF-SSDA 方法，KDF-MutualSHOT，用於基於 EEG 的癲癇亞型分類。在來源模型訓練中，KDF 使用 Jensen-Shannon 距離促進特徵驅動的決策樹模型和資料驅動的 Transformer 模型之間的相互學習。為了將 KDF 調整至新的目標資料集，開發了一個 SF-SSDA 演算法，MutualSHOT，其特點是基於一致性的偽標籤選擇策略。在公開的 TUSZ 和 CHSZ 資料集上的實驗表明，KDF-MutualSHOT 在跨受試者癲癇亞型分類中優於其他監督式和無來源領域適應方法。

##### **COLD: Causal reasOning in cLosed Daily activities**
2411.19500v1 by Abhinav Joshi, Areeb Ahmad, Ashutosh Modi

Large Language Models (LLMs) have shown state-of-the-art performance in a
variety of tasks, including arithmetic and reasoning; however, to gauge the
intellectual capabilities of LLMs, causal reasoning has become a reliable proxy
for validating a general understanding of the mechanics and intricacies of the
world similar to humans. Previous works in natural language processing (NLP)
have either focused on open-ended causal reasoning via causal commonsense
reasoning (CCR) or framed a symbolic representation-based question answering
for theoretically backed-up analysis via a causal inference engine. The former
adds an advantage of real-world grounding but lacks theoretically backed-up
analysis/validation, whereas the latter is far from real-world grounding. In
this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed
Daily activities) framework, which is built upon human understanding of daily
real-world activities to reason about the causal nature of events. We show that
the proposed framework facilitates the creation of enormous causal queries (~ 9
million) and comes close to the mini-turing test, simulating causal reasoning
to evaluate the understanding of a daily real-world task. We evaluate multiple
LLMs on the created causal queries and find that causal reasoning is
challenging even for activities trivial to humans. We further explore (the
causal reasoning abilities of LLMs) using the backdoor criterion to determine
the causal strength between events.

摘要：大型語言模型 (LLM) 在各種任務中展現出最先進的效能，包括算術和推理；然而，為了評估 LLM 的智力能力，因果推理已成為驗證人類對世界機制和複雜性的一般理解的可靠指標。自然語言處理 (NLP) 中先前的研究，不是專注於透過因果常識推理 (CCR) 進行開放式因果推理，就是建構一個基於符號表徵的問題回答，以透過因果推論引擎進行理論支持的分析。前者增加了現實世界的基礎優勢，但缺乏理論支持的分析/驗證，而後者則遠離現實世界的基礎。在這項研究中，我們透過提出 COLD（封閉日常活動中的因果推理）架構來彌合這個差距，該架構建立在人類對日常現實世界活動的理解之上，以推論事件的因果性質。我們證明，所提出的架構促進了大量因果查詢的建立（約 900 萬），並且接近迷你圖靈測試，模擬因果推理以評估對日常現實世界任務的理解。我們針對建立的因果查詢評估多個 LLM，並發現即使對人類來說微不足道的活動，因果推理仍具有挑戰性。我們進一步探討（LLM 的因果推理能力），使用後門標準來確定事件之間的因果強度。

##### **Interleaved-Modal Chain-of-Thought**
2411.19488v1 by Jun Gao, Yongqi Li, Ziqiang Cao, Wenjie Li

Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to
produce a series of intermediate reasoning steps before arriving at the final
answer. However, when transitioning to vision-language models (VLMs), their
text-only rationales struggle to express the fine-grained associations with the
original image. In this paper, we propose an image-incorporated multimodal
Chain-of-Thought, named \textbf{Interleaved-modal Chain-of-Thought (ICoT)},
which generates sequential reasoning steps consisting of paired visual and
textual rationales to infer the final answer. Intuitively, the novel ICoT
requires VLMs to enable the generation of fine-grained interleaved-modal
content, which is hard for current VLMs to fulfill. Considering that the
required visual information is usually part of the input image, we propose
\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.
ADS intelligently inserts regions of the input image to generate the
interleaved-modal reasoning steps with ignorable additional latency. ADS relies
solely on the attention map of VLMs without the need for parameterization, and
therefore it is a plug-and-play strategy that can be generalized to a spectrum
of VLMs. We apply ADS to realize ICoT on two popular VLMs of different
architectures. Extensive evaluations of three benchmarks have shown that ICoT
prompting achieves substantial performance (up to 14\%) and interpretability
improvements compared to existing multimodal CoT prompting methods.

摘要：鏈式思考 (CoT) 提示引發大型語言模型 (LLM) 在得出最終答案之前產生一系列中間推理步驟。然而，在轉換到視覺語言模型 (VLM) 時，它們僅限文字的依據難以表達與原始影像的細微關聯。在本文中，我們提出了一種結合影像的多模態鏈式思考，稱為「穿插模態鏈式思考 (ICoT)」，它會產生由成對視覺和文字依據組成的順序推理步驟來推論最終答案。直觀來說，新穎的 ICoT 要求 VLM 能夠產生細微的穿插模態內容，這對於目前的 VLM 來說很難達成。考量到所需的視覺資訊通常是輸入影像的一部分，我們提出「注意力驅動選擇 (ADS)」來在現有的 VLM 上實現 ICoT。ADS 智慧地插入輸入影像的區域，以產生穿插模態推理步驟，且不會造成額外的延遲。ADS 僅依賴 VLM 的注意力圖，無需參數化，因此它是一種即插即用的策略，可以推廣到各種 VLM。我們將 ADS 應用於兩種不同架構的流行 VLM 上，以實現 ICoT。對三個基準的廣泛評估顯示，與現有的多模態 CoT 提示方法相比，ICoT 提示可實現顯著的效能（最高達 14%）和可解釋性改進。

##### **Action Engine: An LLM-based Framework for Automatic FaaS Workflow Generation**
2411.19485v1 by Akiharu Esashi, Pawissanutt Lertpongrujikorn, Mohsen Amini Salehi

Function as a Service (FaaS) is poised to become the foundation of the next
generation of cloud systems due to its inherent advantages in scalability,
cost-efficiency, and ease of use. However, challenges such as the need for
specialized knowledge and difficulties in building function workflows persist
for cloud-native application developers. To overcome these challenges and
mitigate the burden of developing FaaS-based applications, in this paper, we
propose a mechanism called Action Engine, that makes use of Tool-Augmented
Large Language Models (LLMs) at its kernel to interpret human language queries
and automates FaaS workflow generation, thereby, reducing the need for
specialized expertise and manual design. Action Engine includes modules to
identify relevant functions from the FaaS repository and seamlessly manage the
data dependency between them, ensuring that the developer's query is processed
and resolved. Beyond that, Action Engine can execute the generated workflow by
feeding the user-provided parameters. Our evaluations show that Action Engine
can generate workflows with up to 20\% higher correctness without developer
involvement. We notice that Action Engine can unlock FaaS workflow generation
for non-cloud-savvy developers and expedite the development cycles of
cloud-native applications.

摘要：服務即功能 (FaaS) 因其在可擴充性、成本效益和易用性方面的內在優勢，而有望成為下一代雲端系統的基礎。然而，對於雲端原生應用程式開發人員來說，諸如需要專業知識和建構功能工作流程的困難等挑戰依然存在。為了克服這些挑戰並減輕開發基於 FaaS 的應用程式的負擔，在本文中，我們提出了一種名為 Action Engine 的機制，它利用其核心中的工具增強大型語言模型 (LLM) 來詮釋人類語言查詢並自動化 FaaS 工作流程產生，從而減少對專業知識和手動設計的需求。Action Engine 包含從 FaaS 儲存庫中識別相關功能並無縫管理它們之間資料依賴性的模組，確保處理和解決開發人員的查詢。除此之外，Action Engine 可以透過提供使用者提供的參數來執行產生的工作流程。我們的評估顯示，Action Engine 可以產生正確率高達 20% 的工作流程，而不需要開發人員參與。我們注意到，Action Engine 可以為非雲端專家開發人員解鎖 FaaS 工作流程產生，並加快雲端原生應用程式的開發週期。

##### **FLARE: Towards Universal Dataset Purification against Backdoor Attacks**
2411.19479v1 by Linshan Hou, Wei Luo, Zhongyun Hua, Songhua Chen, Leo Yu Zhang, Yiming Li

Deep neural networks (DNNs) are susceptible to backdoor attacks, where
adversaries poison datasets with adversary-specified triggers to implant hidden
backdoors, enabling malicious manipulation of model predictions. Dataset
purification serves as a proactive defense by removing malicious training
samples to prevent backdoor injection at its source. We first reveal that the
current advanced purification methods rely on a latent assumption that the
backdoor connections between triggers and target labels in backdoor attacks are
simpler to learn than the benign features. We demonstrate that this assumption,
however, does not always hold, especially in all-to-all (A2A) and untargeted
(UT) attacks. As a result, purification methods that analyze the separation
between the poisoned and benign samples in the input-output space or the final
hidden layer space are less effective. We observe that this separability is not
confined to a single layer but varies across different hidden layers. Motivated
by this understanding, we propose FLARE, a universal purification method to
counter various backdoor attacks. FLARE aggregates abnormal activations from
all hidden layers to construct representations for clustering. To enhance
separation, FLARE develops an adaptive subspace selection algorithm to isolate
the optimal space for dividing an entire dataset into two clusters. FLARE
assesses the stability of each cluster and identifies the cluster with higher
stability as poisoned. Extensive evaluations on benchmark datasets demonstrate
the effectiveness of FLARE against 22 representative backdoor attacks,
including all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and
its robustness to adaptive attacks.

摘要：深度神经網路 (DNN) 容易遭受後門攻擊，其中對手會使用對手指定的觸發器來污染資料集，以植入隱藏的後門，進而惡意操縱模型預測。資料集淨化作為一項主動防禦措施，透過移除惡意的訓練樣本來防止後門注入。我們首先揭露，目前進階的淨化方法依賴於一個潛在假設，即後門攻擊中觸發器與目標標籤之間的後門連接比良性特徵更容易學習。然而，我們證明此假設並非總是成立，特別是在全對全 (A2A) 和非目標 (UT) 攻擊中。因此，分析輸入輸出空間或最終隱藏層空間中受污染和良性樣本之間分離度的淨化方法效果較差。我們觀察到，這種可分離性不限於單一層，而會在不同的隱藏層之間變化。基於這種理解，我們提出 FLARE，一種用於對抗各種後門攻擊的通用淨化方法。FLARE 從所有隱藏層聚合異常激活，以建構用於聚類的表示。為了增強分離，FLARE 開發了一種自適應子空間選擇演算法，用於隔離將整個資料集劃分為兩個群集的最佳空間。FLARE 評估每個群集的穩定性，並將穩定性較高的群集識別為受污染的群集。在基準資料集上的廣泛評估證明了 FLARE 對抗 22 種代表性後門攻擊的有效性，包括全對一 (A2O)、全對全 (A2A) 和非目標 (UT) 攻擊，以及其對自適應攻擊的魯棒性。

##### **A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models**
2411.19477v1 by Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou

We propose a general two-stage algorithm that enjoys a provable scaling law
for the test-time compute of large language models (LLMs). Given an input
problem, the proposed algorithm first generates $N$ candidate solutions, and
then chooses the best one via a multiple-round knockout tournament where each
pair of candidates are compared for $K$ times and only the winners move on to
the next round. In a minimalistic implementation, both stages can be executed
with a black-box LLM alone and nothing else (e.g., no external verifier or
reward model), and a total of $N \times (K + 1)$ highly parallelizable LLM
calls are needed for solving an input problem. Assuming that a generated
candidate solution is correct with probability $p_{\text{gen}} > 0$ and a
comparison between a pair of correct and incorrect solutions identifies the
right winner with probability $p_{\text{comp}} > 0.5$ (i.e., better than a
random guess), we prove theoretically that the failure probability of the
proposed algorithm decays to zero exponentially with respect to $N$ and $K$:
$$\mathbb{P}(\text{final output is incorrect}) \le (1 - p_{\text{gen}})^N +
\lceil \log_2 N \rceil e^{-2 K (p_{\text{comp}} - 0.5)^2}.$$ Our empirical
results with the challenging MMLU-Pro benchmark validate the technical
assumptions, as well as the efficacy of the proposed algorithm and the gains
from scaling up its test-time compute.

摘要：<paragraph>我們提出了一種通用的兩階段演算法，該演算法享有可證明的大語言模型 (LLM) 測試時計算的可擴充性定律。對於一個輸入問題，所提出的演算法首先產生 $N$ 個候選解，然後透過多輪淘汰錦標賽選擇最佳解，其中每一對候選解會比較 $K$ 次，只有獲勝者才能晉級下一輪。在最簡化的實作中，兩個階段都可以只用一個黑盒 LLM 執行，不需要其他任何東西（例如，沒有外部驗證器或獎勵模型），而且要解決一個輸入問題，總共需要 $N \times (K + 1)$ 個高度並行化的 LLM 呼叫。假設一個產生的候選解是正確的，其機率為 $p_{\text{gen}} > 0$，而且比較一對正確和不正確的解會以機率 $p_{\text{comp}} > 0.5$（即比隨機猜測好）找出正確的獲勝者，我們在理論上證明，所提出的演算法的失敗機率會隨著 $N$ 和 $K$ 指數遞減：$$\mathbb{P}(\text{最終輸出不正確}) \le (1 - p_{\text{gen}})^N + \lceil \log_2 N \rceil e^{-2 K (p_{\text{comp}} - 0.5)^2}.$$ 我們使用具有挑戰性的 MMLU-Pro 基準所做的實證結果驗證了技術假設，以及所提出的演算法的功效，以及擴大其測試時計算所獲得的增益。</paragraph>

##### **Effective Fine-Tuning of Vision-Language Models for Accurate Galaxy Morphology Analysis**
2411.19475v1 by Ruoqi Wang, Haitao Wang, Qiong Luo

Galaxy morphology analysis involves classifying galaxies by their shapes and
structures. For this task, directly training domain-specific models on large,
annotated astronomical datasets is effective but costly. In contrast,
fine-tuning vision foundation models on a smaller set of astronomical images is
more resource-efficient but generally results in lower accuracy. To harness the
benefits of both approaches and address their shortcomings, we propose
GalaxAlign, a novel method that fine-tunes pre-trained foundation models to
achieve high accuracy on astronomical tasks. Specifically, our method extends a
contrastive learning architecture to align three types of data in fine-tuning:
(1) a set of schematic symbols representing galaxy shapes and structures, (2)
textual labels of these symbols, and (3) galaxy images. This way, GalaxAlign
not only eliminates the need for expensive pretraining but also enhances the
effectiveness of fine-tuning. Extensive experiments on galaxy classification
and similarity search demonstrate that our method effectively fine-tunes
general pre-trained models for astronomical tasks by incorporating
domain-specific multi-modal knowledge.

摘要：星系形態分析涉及根據星系的形狀和結構對星系進行分類。對於這項任務，直接在大型標註的天文資料集上訓練特定領域模型是有效但代價高昂的。相比之下，在較小的天文圖像集上微調視覺基礎模型在資源上更有效率，但通常會導致較低的準確度。為了利用兩種方法的優點並解決其缺點，我們提出了 GalaxAlign，這是一種新穎的方法，它對預訓練的基礎模型進行微調，以在天文任務上實現高準確度。具體來說，我們的模型擴充了一個對比學習架構，以在微調中對齊三種類型的資料：(1) 一組代表星系形狀和結構的示意符號，(2) 這些符號的文字標籤，以及 (3) 星系圖像。這樣，GalaxAlign 不僅消除了昂貴的預訓練的需要，而且還增強了微調的有效性。在星系分類和相似性搜尋上的大量實驗表明，我們的模型通過整合特定領域的多模態知識，有效地微調了用於天文任務的一般預訓練模型。

##### **Towards Understanding Retrieval Accuracy and Prompt Quality in RAG Systems**
2411.19463v1 by Shengming Zhao, Yuheng Huang, Jiayang Song, Zhijie Wang, Chengcheng Wan, Lei Ma

Retrieval-Augmented Generation (RAG) is a pivotal technique for enhancing the
capability of large language models (LLMs) and has demonstrated promising
efficacy across a diverse spectrum of tasks. While LLM-driven RAG systems show
superior performance, they face unique challenges in stability and reliability.
Their complexity hinders developers' efforts to design, maintain, and optimize
effective RAG systems. Therefore, it is crucial to understand how RAG's
performance is impacted by its design. In this work, we conduct an early
exploratory study toward a better understanding of the mechanism of RAG
systems, covering three code datasets, three QA datasets, and two LLMs. We
focus on four design factors: retrieval document type, retrieval recall,
document selection, and prompt techniques. Our study uncovers how each factor
impacts system correctness and confidence, providing valuable insights for
developing an accurate and reliable RAG system. Based on these findings, we
present nine actionable guidelines for detecting defects and optimizing the
performance of RAG systems. We hope our early exploration can inspire further
advancements in engineering, improving and maintaining LLM-driven intelligent
software systems for greater efficiency and reliability.

摘要：檢索增強生成（RAG）是一種增強大型語言模型（LLM）能力的關鍵技術，並已在各種任務中展示出良好的效果。儘管 LLM 驅動的 RAG 系統表現出色的性能，但它們在穩定性和可靠性方面面臨著獨特的挑戰。它們的複雜性阻礙了開發人員設計、維護和優化有效 RAG 系統的努力。因此，了解 RAG 的性能如何受到其設計的影響至關重要。在這項工作中，我們進行了一項早期的探索性研究，以期更好地理解 RAG 系統的機制，涵蓋了三個程式碼資料集、三個 QA 資料集和兩個 LLM。我們專注於四個設計因素：檢索文件類型、檢索召回、文件選擇和提示技術。我們的研究揭示了每個因素如何影響系統的正確性和可信度，為開發準確且可靠的 RAG 系統提供了寶貴的見解。根據這些發現，我們提出了九項可操作的準則，用於檢測缺陷和優化 RAG 系統的性能。我們希望我們的早期探索能夠激發工程方面的進一步進展，改進和維護 LLM 驅動的智慧軟體系統，以提高效率和可靠性。

##### **Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing**
2411.19460v1 by Hosu Lee, Junho Kim, Hyunjun Kim, Yong Man Ro

With the growing scale and complexity of video data, efficiently processing
long video sequences poses significant challenges due to the quadratic increase
in memory and computational demands associated with existing transformer-based
Large Multi-modal Models (LMMs). To address these issues, we introduce
Video-Ma$^2$mba, a novel architecture that incorporates State Space Models
(SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This
allows the LMMs to scale linearly in terms of time and memory requirements,
making it feasible to handle long-duration video content. Furthermore, we
enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing
(MA-GC) method, which strategically manages memory by retaining only essential
activations across multiple computational axes. Our approach significantly
reduces the memory footprint compared to standard gradient checkpointing.
Empirical analyses show that Video-Ma$^2$mba can process extensive video
sequences-equivalent to millions of tokens or over two hours of continuous
sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of
temporal dynamics, our model improves the accuracy and relevance of responses
in long video understanding tasks, demonstrating substantial advantages over
existing frameworks.

摘要：隨著影片資料規模和複雜度的增加，有效處理長影片序列會帶來重大挑戰，因為現有的基於 Transformer 的大型多模態模型 (LMM) 在記憶體和運算需求上會呈二次方增加。為了解決這些問題，我們引入了 Video-Ma$^2$mba，這是一種新的架構，它在 Mamba-2 框架中整合了狀態空間模型 (SSM)，取代了注意力機制。這讓 LMM 在時間和記憶體需求方面可以線性擴充，使得處理長時程影片內容成為可能。此外，我們透過引入多軸梯度檢查點 (MA-GC) 方法來提升記憶體效率，此方法透過只保留多個運算軸之間的必要活化，來策略性地管理記憶體。與標準梯度檢查點相比，我們的做法大幅減少了記憶體使用量。經驗分析顯示，Video-Ma$^2$mba 可以處理大量的影片序列，相當於數百萬個符號或在單一 GPU 上以 1 FPS 連續播放超過兩小時的序列。透過詳細捕捉時間動態，我們的模型提升了在長影片理解任務中回應的準確性和相關性，證明了其相較於現有框架的顯著優勢。

##### **Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension Ability**
2411.19456v1 by Yujin Han, Lei Xu, Sirui Chen, Difan Zou, Chaochao Lu

Large language models (LLMs) have shown remarkable capability in natural
language tasks, yet debate persists on whether they truly comprehend deep
structure (i.e., core semantics) or merely rely on surface structure (e.g.,
presentation format). Prior studies observe that LLMs' performance declines
when intervening on surface structure, arguing their success relies on surface
structure recognition. However, surface structure sensitivity does not prevent
deep structure comprehension. Rigorously evaluating LLMs' capability requires
analyzing both, yet deep structure is often overlooked. To this end, we assess
LLMs' comprehension ability using causal mediation analysis, aiming to fully
discover the capability of using both deep and surface structures.
Specifically, we formulate the comprehension of deep structure as direct causal
effect (DCE) and that of surface structure as indirect causal effect (ICE),
respectively. To address the non-estimability of original DCE and ICE --
stemming from the infeasibility of isolating mutual influences of deep and
surface structures, we develop the corresponding quantifiable surrogates,
including approximated DCE (ADCE) and approximated ICE (AICE). We further apply
the ADCE to evaluate a series of mainstream LLMs, showing that most of them
exhibit deep structure comprehension ability, which grows along with the
prediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs
rely more on deep structure, while open-source LLMs are more surface-sensitive,
which decreases with model scale. Theoretically, ADCE is a bidirectional
evaluation, which measures both the sufficiency and necessity of deep structure
changes in causing output variations, thus offering a more comprehensive
assessment than accuracy, a common evaluation in LLMs. Our work provides new
insights into LLMs' deep structure comprehension and offers novel methods for
LLMs evaluation.

摘要：大型語言模型 (LLM) 在自然語言任務中展現了卓越的能力，但關於它們是否真正理解深層結構（即核心語義）或僅依賴表面結構（例如呈現格式）的爭論仍持續不斷。先前的研究觀察到，當干預表面結構時，LLM 的性能會下降，並認為它們的成功依賴於表面結構識別。然而，對表面結構的敏感性並不妨礙深層結構的理解。嚴格評估 LLM 的能力需要分析兩者，但深層結構常常被忽視。為此，我們使用因果中介分析評估 LLM 的理解能力，旨在充分發現使用深層結構和表面結構的能力。具體來說，我們將對深層結構的理解表述為直接因果效應 (DCE)，而對表面結構的理解表述為間接因果效應 (ICE)。為了解決原始 DCE 和 ICE 的不可估計性——源於無法分離深層結構和表面結構的相互影響，我們開發了相應的可量化替代，包括近似 DCE (ADCE) 和近似 ICE (AICE)。我們進一步應用 ADCE 來評估一系列主流 LLM，結果表明它們大多數都表現出深層結構理解能力，並且隨著預測準確度而增長。比較 ADCE 和 AICE 表明，閉源 LLM 更依賴於深層結構，而開源 LLM 對表面更敏感，並且隨著模型規模而降低。理論上，ADCE 是一種雙向評估，它測量了深層結構變化在引起輸出變化中既是充分條件又是必要條件的程度，從而提供了比 LLM 中常見的評估準確度更全面的評估。我們的研究為 LLM 的深層結構理解提供了新的見解，並為 LLM 評估提供了新的方法。

##### **Learning Visual Abstract Reasoning through Dual-Stream Networks**
2411.19451v1 by Kai Zhao, Chang Xu, Bailu Si

Visual abstract reasoning tasks present challenges for deep neural networks,
exposing limitations in their capabilities. In this work, we present a neural
network model that addresses the challenges posed by Raven's Progressive
Matrices (RPM). Inspired by the two-stream hypothesis of visual processing, we
introduce the Dual-stream Reasoning Network (DRNet), which utilizes two
parallel branches to capture image features. On top of the two streams, a
reasoning module first learns to merge the high-level features of the same
image. Then, it employs a rule extractor to handle combinations involving the
eight context images and each candidate image, extracting discrete abstract
rules and utilizing an multilayer perceptron (MLP) to make predictions.
Empirical results demonstrate that the proposed DRNet achieves state-of-the-art
average performance across multiple RPM benchmarks. Furthermore, DRNet
demonstrates robust generalization capabilities, even extending to various
out-of-distribution scenarios. The dual streams within DRNet serve distinct
functions by addressing local or spatial information. They are then integrated
into the reasoning module, leveraging abstract rules to facilitate the
execution of visual reasoning tasks. These findings indicate that the
dual-stream architecture could play a crucial role in visual abstract
reasoning.

摘要：視覺抽象推理任務對深度神經網路提出挑戰，
暴露其能力的限制。在這項工作中，我們提出一個神經
網路模型，解決 Raven's Progressive Matrices (RPM) 所提出的挑戰。受到視覺處理的雙流假說啟發，我們
引進雙流推理網路 (DRNet)，它利用兩個平行分支來擷取影像特徵。在兩個串流之上，一個
推理模組首先學會合併同一影像的高階特徵。然後，它採用一個規則萃取器來處理涉及
八個背景影像和每個候選影像的組合，萃取出離散的抽象規則，並利用多層感知器 (MLP) 來做出預測。
實證結果證明，所提出的 DRNet 在多個 RPM 基準上達成最先進的平均效能。此外，DRNet
展現出強健的概化能力，甚至延伸到各種非分布情況。DRNet 內部的雙串流透過處理局部或空間資訊，提供不同的功能。然後將它們整合
到推理模組中，利用抽象規則來促進視覺推理任務的執行。這些發現指出，
雙流架構可以在視覺抽象推理中扮演關鍵角色。

##### **Adaptive Interactive Segmentation for Multimodal Medical Imaging via Selection Engine**
2411.19447v1 by Zhi Li, Kai Zhao, Yaqi Wang, Shuai Wang

In medical image analysis, achieving fast, efficient, and accurate
segmentation is essential for automated diagnosis and treatment. Although
recent advancements in deep learning have significantly improved segmentation
accuracy, current models often face challenges in adaptability and
generalization, particularly when processing multi-modal medical imaging data.
These limitations stem from the substantial variations between imaging
modalities and the inherent complexity of medical data. To address these
challenges, we propose the Strategy-driven Interactive Segmentation Model
(SISeg), built on SAM2, which enhances segmentation performance across various
medical imaging modalities by integrating a selection engine. To mitigate
memory bottlenecks and optimize prompt frame selection during the inference of
2D image sequences, we developed an automated system, the Adaptive Frame
Selection Engine (AFSE). This system dynamically selects the optimal prompt
frames without requiring extensive prior medical knowledge and enhances the
interpretability of the model's inference process through an interactive
feedback mechanism. We conducted extensive experiments on 10 datasets covering
7 representative medical imaging modalities, demonstrating the SISeg model's
robust adaptability and generalization in multi-modal tasks. The project page
and code will be available at: [URL].

摘要：在医学影像分析中，实现快速、高效和准确的分割对于自动化诊断和治疗至关重要。尽管深度学习的最新进展显著提高了分割准确性，但当前模型在适应性和泛化性方面常常面临挑战，尤其是在处理多模态医学影像数据时。这些限制源于影像方式之间的巨大差异和医学数据的固有复杂性。为了应对这些挑战，我们提出了基于 SAM2 的策略驱动交互式分割模型 (SISeg)，它通过集成选择引擎来增强各种医学影像方式的分割性能。为了缓解内存瓶颈并优化 2D 图像序列推理期间的提示帧选择，我们开发了一个自动化系统，即自适应帧选择引擎 (AFSE)。该系统在无需广泛的先前医学知识的情况下动态选择最佳提示帧，并通过交互式反馈机制增强模型推理过程的可解释性。我们在涵盖 7 种代表性医学影像方式的 10 个数据集上进行了广泛的实验，展示了 SISeg 模型在多模态任务中的鲁棒适应性和泛化性。项目页面和代码将提供在：[URL]。

##### **Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models**
2411.19443v1 by Tian Yu, Shaolei Zhang, Yang Feng

Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.

摘要：反覆擷取是指模型在生成過程中持續向擷取器發出查詢，以提升擷取知識的相關性，進而提升擷取增強生成 (RAG) 的效能。現有研究通常採用少次提示或人工建構的規則來實作反覆擷取。這引入了額外的推論負擔，且忽略了大型語言模型 (LLM) 卓越的推理能力。在本文中，我們引入了 Auto-RAG，一種以 LLM 強大的決策能力為中心的自主反覆擷取模型。Auto-RAG 與擷取器進行多回合對話，系統性地規劃擷取並精煉查詢以獲取有價值的知識。此程序會持續進行，直到收集到足夠的外部資訊為止，此時會將結果呈現給使用者。為此，我們開發了一種方法，可自主綜合基於推理的決策制定指示到反覆擷取中，並微調最新的開源 LLM。實驗結果顯示，Auto-RAG 能夠自主與擷取器進行反覆互動，有效利用 LLM 卓越的推理和決策能力，進而提升六個基準測試的傑出效能。進一步的分析顯示，Auto-RAG 能夠根據問題的難度和擷取知識的效用，自主調整反覆次數，而不需要任何人工介入。此外，Auto-RAG 以自然語言表達反覆擷取程序，在提供使用者更直覺的體驗的同時，也提升了可解釋性\footnote{程式碼可於\url{https://github.com/ictnlp/Auto-RAG}取得。}。

##### **Gradient Inversion Attack on Graph Neural Networks**
2411.19440v1 by Divya Anand Sinha, Yezi Liu, Ruijie Du, Yanning Shen

Graph federated learning is of essential importance for training over large
graph datasets while protecting data privacy, where each client stores a subset
of local graph data, while the server collects the local gradients and
broadcasts only the aggregated gradients. Recent studies reveal that a
malicious attacker can steal private image data from gradient exchanging of
neural networks during federated learning. However, none of the existing works
have studied the vulnerability of graph data and graph neural networks under
such attack. To answer this question, the present paper studies the problem of
whether private data can be recovered from leaked gradients in both node
classification and graph classification tasks and { proposes a novel attack
named Graph Leakage from Gradients (GLG)}. Two widely-used GNN frameworks are
analyzed, namely GCN and GraphSAGE. The effects of different model settings on
recovery are extensively discussed. Through theoretical analysis and empirical
validation, it is shown that parts of the graph data can be leaked from the
gradients.

摘要：圖表聯邦學習對於在大型圖表資料集上進行訓練同時保護資料隱私至關重要，其中每個用戶端儲存本地圖表資料的子集，而伺服器收集本地梯度並僅廣播聚合梯度。最近的研究顯示，惡意攻擊者可以在聯邦學習期間從神經網路的梯度交換中竊取私人影像資料。然而，現有研究並未探討圖表資料和圖表神經網路在這種攻擊下的脆弱性。為了回答這個問題，本文探討了在節點分類和圖表分類任務中是否可以從洩露的梯度中恢復私人資料，並提出了一種名為梯度圖表洩漏 (GLG) 的新型攻擊。分析了兩個廣泛使用的 GNN 框架，即 GCN 和 GraphSAGE。深入探討了不同模型設定對復原的影響。透過理論分析和實證驗證，表明圖表資料的一部分可以從梯度中洩漏。

##### **Actions and Objects Pathways for Domain Adaptation in Video Question Answering**
2411.19434v1 by Safaa Abdullahi Moallim Mohamud, Ho-Young Jung

In this paper, we introduce the Actions and Objects Pathways (AOPath) for
out-of-domain generalization in video question answering tasks. AOPath
leverages features from a large pretrained model to enhance generalizability
without the need for explicit training on the unseen domains. Inspired by human
brain, AOPath dissociates the pretrained features into action and object
features, and subsequently processes them through separate reasoning pathways.
It utilizes a novel module which converts out-of-domain features into
domain-agnostic features without introducing any trainable weights. We validate
the proposed approach on the TVQA dataset, which is partitioned into multiple
subsets based on genre to facilitate the assessment of generalizability. The
proposed approach demonstrates 5% and 4% superior performance over conventional
classifiers on out-of-domain and in-domain datasets, respectively. It also
outperforms prior methods that involve training millions of parameters, whereas
the proposed approach trains very few parameters.

摘要：在本文中，我們介紹了動作和對象路徑 (AOPath)，用於影片問答任務中的領域外概化。AOPath 利用大型預訓練模型中的特徵來增強概化能力，而無需針對未見過的領域進行明確的訓練。受人腦啟發，AOPath 將預訓練特徵分解為動作和對象特徵，然後透過獨立的推理路徑進行處理。它利用一個新穎的模組，將領域外特徵轉換為領域不可知的特徵，而不會引入任何可訓練權重。我們在 TVQA 資料集上驗證所提出的方法，該資料集根據類型劃分為多個子集，以利於評估概化能力。所提出的方法在領域外和領域內資料集上分別展示出比傳統分類器高出 5% 和 4% 的優異效能。它也優於先前涉及訓練數百萬個參數的方法，而所提出的方法訓練的參數非常少。

##### **AMO Sampler: Enhancing Text Rendering with Overshooting**
2411.19415v1 by Xixi Hu, Keyang Xu, Bo Liu, Qiang Liu, Hongliang Fei

Achieving precise alignment between textual instructions and generated images
in text-to-image generation is a significant challenge, particularly in
rendering written text within images. Sate-of-the-art models like Stable
Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text
depiction, resulting in misspelled or inconsistent text. We introduce a
training-free method with minimal computational overhead that significantly
enhances text rendering quality. Specifically, we introduce an overshooting
sampler for pretrained rectified flow (RF) models, by alternating between
over-simulating the learned ordinary differential equation (ODE) and
reintroducing noise. Compared to the Euler sampler, the overshooting sampler
effectively introduces an extra Langevin dynamics term that can help correct
the compounding error from successive Euler steps and therefore improve the
text rendering. However, when the overshooting strength is high, we observe
over-smoothing artifacts on the generated images. To address this issue, we
propose an Attention Modulated Overshooting sampler (AMO), which adaptively
controls the strength of overshooting for each image patch according to their
attention score with the text content. AMO demonstrates a 32.3% and 35.9%
improvement in text rendering accuracy on SD3 and Flux without compromising
overall image quality or increasing inference cost.

摘要：在文本到图像的生成中，实现文本说明与生成的图像之间的精确对齐是一项重大挑战，尤其是在图像中呈现书面文本时。最先进的模型，如 Stable Diffusion 3 (SD3)、Flux 和 AuraFlow 仍然难以准确描绘文本，导致文本拼写错误或不一致。我们引入了一种训练自由的方法，计算开销最小，可以显著提高文本渲染质量。具体来说，我们为预训练的整流流 (RF) 模型引入了一个过冲采样器，通过在过度模拟已学习的常微分方程 (ODE) 和重新引入噪声之间交替进行。与欧拉采样器相比，过冲采样器有效地引入了一个额外的朗之万动力学项，可以帮助修正连续欧拉步骤的复合误差，从而改善文本渲染。但是，当过冲强度较高时，我们观察到生成图像上存在过度平滑的伪影。为了解决这个问题，我们提出了一种注意力调制过冲采样器 (AMO)，它根据图像块与其文本内容的注意力分数自适应地控制每个图像块的过冲强度。AMO 在 SD3 和 Flux 上展示了文本渲染准确度提高了 32.3% 和 35.9%，而不会损害整体图像质量或增加推理成本。

##### **Concept-driven Off Policy Evaluation**
2411.19395v1 by Ritam Majumdar, Jack Teversham, Sonali Parbhoo

Evaluating off-policy decisions using batch data poses significant challenges
due to limited sample sizes leading to high variance. To improve Off-Policy
Evaluation (OPE), we must identify and address the sources of this variance.
Recent research on Concept Bottleneck Models (CBMs) shows that using
human-explainable concepts can improve predictions and provide better
understanding. We propose incorporating concepts into OPE to reduce variance.
Our work introduces a family of concept-based OPE estimators, proving that they
remain unbiased and reduce variance when concepts are known and predefined.
Since real-world applications often lack predefined concepts, we further
develop an end-to-end algorithm to learn interpretable, concise, and diverse
parameterized concepts optimized for variance reduction. Our experiments with
synthetic and real-world datasets show that both known and learned
concept-based estimators significantly improve OPE performance. Crucially, we
show that, unlike other OPE methods, concept-based estimators are easily
interpretable and allow for targeted interventions on specific concepts,
further enhancing the quality of these estimators.

摘要：由於樣本量有限導致變異性高，使用批次資料評估非策略決策會造成重大的挑戰。為了改善非策略評估 (OPE)，我們必須找出並解決此變異性的來源。最近對概念瓶頸模型 (CBM) 的研究顯示，使用人類可解釋的概念可以改善預測並提供更佳的理解。我們建議將概念納入 OPE 以減少變異性。我們的研究引入了概念基礎 OPE 估計量家族，證明它們在概念已知且預先定義的情況下保持無偏且減少變異性。由於現實世界應用通常缺乏預先定義的概念，我們進一步開發一個端到端演算法，以學習可解釋、簡潔且多樣化的參數化概念，針對變異性降低進行最佳化。我們對合成和現實世界資料集的實驗顯示，已知和學習的概念基礎估計量都顯著改善了 OPE 效能。至關重要的是，我們表明，與其他 OPE 方法不同，概念基礎估計量容易解釋，並允許針對特定概念進行有針對性的干預，進一步提升這些估計量的品質。

##### **Marconi: Prefix Caching for the Era of Hybrid LLMs**
2411.19379v1 by Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Ravi Netravali, Yida Wang

Hybrid models that combine the language modeling capabilities of Attention
layers with the efficiency of Recurrent layers (e.g., State Space Models) have
gained traction in practically supporting long contexts in Large Language Model
serving. Yet, the unique properties of these models complicate the usage of
complementary efficiency optimizations such as prefix caching that skip
redundant computations across requests. Most notably, their use of in-place
state updates for recurrent layers precludes rolling back cache entries for
partial sequence overlaps, and instead mandates only exact-match cache hits;
the effect is a deluge of (large) cache entries per sequence, most of which
yield minimal reuse opportunities. We present Marconi, the first system that
supports efficient prefix caching with Hybrid LLMs. Key to Marconi are its
novel admission and eviction policies that more judiciously assess potential
cache entries based not only on recency, but also on (1) forecasts of their
reuse likelihood across a taxonomy of different hit scenarios, and (2) the
compute savings that hits deliver relative to memory footprints. Across diverse
workloads and Hybrid models, Marconi achieves up to 34.4$\times$ higher token
hit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix
caching systems.

摘要：結合了注意力層的語言建模功能與遞迴層（例如狀態空間模型）的效率的混合模型在大型語言模型服務中實務上支援長脈絡方面獲得了進展。然而，這些模型的獨特屬性讓使用補充效率最佳化變得複雜，例如跳過請求間重複運算的前綴快取。最值得注意的是，它們對遞迴層使用就地狀態更新，排除回滾部分序列重疊的快取條目，而只強制完全比對的快取命中；其結果是每個序列產生大量（大型）快取條目，其中大部分產生的重複使用機會最少。我們提出 Marconi，這是第一個支援使用混合 LLM 的有效前綴快取的系統。Marconi 的關鍵在於其新穎的接納和驅逐政策，這些政策不僅根據最近使用情況，還根據 (1) 不同命中情況分類中它們的重複使用可能性的預測，以及 (2) 相對於記憶體使用量，命中提供的運算節省，更審慎地評估潛在快取條目。在不同的工作負載和混合模型中，與最先進的前綴快取系統相比，Marconi 達到高達 34.4 倍的 token 命中率（71.1% 或低 617 毫秒的 TTFT）。

##### **Libra: Leveraging Temporal Images for Biomedical Radiology Analysis**
2411.19378v1 by Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho

Radiology report generation (RRG) is a challenging task, as it requires a
thorough understanding of medical images, integration of multiple temporal
inputs, and accurate report generation. Effective interpretation of medical
images, such as chest X-rays (CXRs), demands sophisticated visual-language
reasoning to map visual findings to structured reports. Recent studies have
shown that multimodal large language models (MLLMs) can acquire multimodal
capabilities by aligning with pre-trained vision encoders. However, current
approaches predominantly focus on single-image analysis or utilise rule-based
symbolic processing to handle multiple images, thereby overlooking the
essential temporal information derived from comparing current images with prior
ones. To overcome this critical limitation, we introduce Libra, a
temporal-aware MLLM tailored for CXR report generation using temporal images.
Libra integrates a radiology-specific image encoder with a MLLM and utilises a
novel Temporal Alignment Connector to capture and synthesise temporal
information of images across different time points with unprecedented
precision. Extensive experiments show that Libra achieves new state-of-the-art
performance among the same parameter scale MLLMs for RRG tasks on the
MIMIC-CXR. Specifically, Libra improves the RadCliQ metric by 12.9% and makes
substantial gains across all lexical metrics compared to previous models.

摘要：放射學報告生成 (RRG) 是一項具有挑戰性的任務，因為它需要透徹了解醫學影像、整合多個時間輸入以及準確的報告生成。有效解讀醫學影像，例如胸部 X 光 (CXR)，需要複雜的視覺語言推理才能將視覺發現對應到結構化的報告中。最近的研究表明，多模態大型語言模型 (MLLM) 可以透過與預先訓練的視覺編碼器對齊來獲得多模態能力。然而，目前的方法主要專注於單一影像分析或利用基於規則的符號處理來處理多個影像，從而忽略了從比較當前影像與先前影像中得出的基本時間資訊。為了克服這個關鍵限制，我們引入了 Libra，一個專為使用時間影像進行 CXR 報告生成的時態感知 MLLM。Libra 將放射學專用影像編碼器與 MLLM 整合在一起，並利用一個新穎的時間對齊連接器來擷取和合成不同時間點影像的時間資訊，並具有前所未有的精確度。廣泛的實驗表明，Libra 在 MIMIC-CXR 的 RRG 任務中，在同參數規模的 MLLM 中取得了新的最先進效能。具體來說，Libra 將 RadCliQ 指標提升了 12.9%，並在所有詞彙指標方面都比以前的模型取得了顯著進步。

##### **DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack Abilities**
2411.19360v1 by Hui Dai, Dan Pechi, Xinyi Yang, Garvit Banga, Raghav Mantri

The Needle-in-a-haystack (NIAH) test is a general task used to assess
language models' (LMs') abilities to recall particular information from long
input context. This framework however does not provide a means of analyzing
what factors, beyond context length, contribute to LMs' abilities or
inabilities to separate and recall needles from their haystacks. To provide a
systematic means of assessing what features contribute to LMs' NIAH
capabilities, we developed a synthetic benchmark called DENIAHL (Data-oriented
Evaluation of NIAH for LLM's). Our work expands on previous NIAH studies by
ablating NIAH features beyond typical context length including data type, size,
and patterns. We find stark differences between GPT-3.5 and LLaMA 2-7B's
performance on DENIAHL, and drops in recall performance when features like item
size are increased, and to some degree when data type is changed from numbers
to letters. This has implications for increasingly large context models,
demonstrating factors beyond item-number impact NIAH capabilities.

摘要：針插草堆 (NIAH) 測試是一種一般任務，用於評估語言模型 (LM) 從長輸入語境中提取特定資訊的能力。然而，此架構並未提供分析因素的方法，除了語境長度之外，這些因素有助於 LM 從其草堆中分離和提取針頭或無法做到這一點。為了提供評估哪些特徵有助於 LM 的 NIAH 能力的系統性方法，我們開發了一個名為 DENIAHL（針對 LLM 的 NIAH 的資料導向評估）的合成基準。我們的工作透過消融 NIAH 特徵擴展了先前的 NIAH 研究，這些特徵超出了典型的語境長度，包括資料類型、大小和模式。我們發現 GPT-3.5 和 LLaMA 2-7B 在 DENIAHL 上的效能有顯著差異，並且在特徵（例如項目大小）增加時召回效能下降，在一定程度上在資料類型從數字變更為字母時也會下降。這對越來越大的語境模型有影響，證明了除了項目數量之外的因素會影響 NIAH 能力。

##### **Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance**
2411.19356v1 by Philipp Brauner, Felix Glawe, Gian Luca Liehner, Luisa Vervier, Martina Ziefle

Understanding public perception of artificial intelligence (AI) and the
tradeoffs between potential risks and benefits is crucial, as these perceptions
might shape policy decisions, influence innovation trajectories for successful
market strategies, and determine individual and societal acceptance of AI
technologies. Using a representative sample of 1100 participants from Germany,
this study examines mental models of AI. Participants quantitatively evaluated
71 statements about AI's future capabilities (e.g., autonomous driving, medical
care, art, politics, warfare, and societal divides), assessing the expected
likelihood of occurrence, perceived risks, benefits, and overall value. We
present rankings of these projections alongside visual mappings illustrating
public risk-benefit tradeoffs. While many scenarios were deemed likely,
participants often associated them with high risks, limited benefits, and low
overall value. Across all scenarios, 96.4% ($r^2=96.4\%$) of the variance in
value assessment can be explained by perceived risks ($\beta=-.504$) and
perceived benefits ($\beta=+.710$), with no significant relation to expected
likelihood. Demographics and personality traits influenced perceptions of
risks, benefits, and overall evaluations, underscoring the importance of
increasing AI literacy and tailoring public information to diverse user needs.
These findings provide actionable insights for researchers, developers, and
policymakers by highlighting critical public concerns and individual factors
essential to align AI development with individual values.

摘要：<paragraph>了解公眾對人工智慧 (AI) 的認知以及潛在風險與好處之間的權衡至關重要，因為這些認知可能會影響政策決策、影響成功市場策略的創新軌跡，並決定個人和社會對 AI 技術的接受度。本研究使用來自德國的 1100 名參與者的代表性樣本，探討了 AI 的心智模型。參與者對 71 項關於 AI 未來能力的陳述（例如，自動駕駛、醫療保健、藝術、政治、戰爭和社會分歧）進行了定量評估，評估預期的發生可能性、感知風險、好處和整體價值。我們展示了這些預測的排名，並附上視覺化映射，說明了公眾的風險收益權衡。儘管許多場景被認為是可能的，但參與者通常將它們與高風險、有限的好處和低整體價值聯繫起來。在所有場景中，96.4% ($r^2=96.4\%$) 的價值評估差異可以用感知風險 ($\beta=-.504$) 和感知好處 ($\beta=+.710$) 來解釋，與預期的可能性沒有顯著關係。人口統計和人格特質影響了對風險、好處和整體評估的看法，這凸顯了提高 AI 素養和根據不同的使用者需求調整公共資訊的重要性。這些發現通過強調關鍵的公共關注和與個人價值觀一致的 AI 開發必不可少的個人因素，為研究人員、開發人員和政策制定者提供了可行的見解。</paragraph>

##### **OMuleT: Orchestrating Multiple Tools for Practicable Conversational Recommendation**
2411.19352v1 by Se-eun Yoon, Xiaokai Wei, Yexi Jiang, Rachit Pareek, Frank Ong, Kevin Gao, Julian McAuley, Michelle Gong

In this paper, we present a systematic effort to design, evaluate, and
implement a realistic conversational recommender system (CRS). The objective of
our system is to allow users to input free-form text to request
recommendations, and then receive a list of relevant and diverse items. While
previous work on synthetic queries augments large language models (LLMs) with
1-3 tools, we argue that a more extensive toolbox is necessary to effectively
handle real user requests. As such, we propose a novel approach that equips
LLMs with over 10 tools, providing them access to the internal knowledge base
and API calls used in production. We evaluate our model on a dataset of real
users and show that it generates relevant, novel, and diverse recommendations
compared to vanilla LLMs. Furthermore, we conduct ablation studies to
demonstrate the effectiveness of using the full range of tools in our toolbox.
We share our designs and lessons learned from deploying the system for internal
alpha release. Our contribution is the addressing of all four key aspects of a
practicable CRS: (1) real user requests, (2) augmenting LLMs with a wide
variety of tools, (3) extensive evaluation, and (4) deployment insights.

摘要：<paragraph>在本文中，我们提出了設計、評估和實作一個實際的對話式推薦系統 (CRS) 的系統性努力。我們系統的目標是讓使用者輸入自由格式文字來請求推薦，然後收到一個相關且多樣化的項目清單。雖然先前針對合成查詢的工作會使用 1-3 個工具來擴充大型語言模型 (LLM)，但我們認為需要一個更廣泛的工具箱才能有效地處理真實使用者的要求。因此，我們提出了一種創新的方法，為 LLM 提供超過 10 種工具，讓它們可以存取製作中使用的內部知識庫和 API 呼叫。我們在真實使用者的資料集上評估我們的模型，並顯示與香草 LLM 相比，它會產生相關、新穎且多樣化的推薦。此外，我們進行消融研究以證明使用我們工具箱中所有工具的有效性。我們分享我們的設計和從部署系統進行內部 alpha 釋出的經驗教訓。我們的貢獻是解決了實用 CRS 的所有四個關鍵面向：(1) 真實使用者的要求、(2) 使用各種工具擴充 LLM、(3) 廣泛的評估，以及 (4) 部署見解。</paragraph>

##### **CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections**
2411.19346v1 by Mohamed Fazli Imam, Rufael Fedaku Marew, Jameel Hassan, Mustansar Fiaz, Alham Fikri Aji, Hisham Cholakkal

In the era of foundation models, CLIP has emerged as a powerful tool for
aligning text and visual modalities into a common embedding space. However, the
alignment objective used to train CLIP often results in subpar visual features
for fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at
extracting rich visual features due to their specialized training paradigm.
Yet, these SSL models require an additional supervised linear probing step,
which relies on fully labeled data which is often expensive and difficult to
obtain at scale. In this paper, we propose a label-free prompt-tuning method
that leverages the rich visual features of self-supervised learning models
(DINO) and the broad textual knowledge of large language models (LLMs) to
largely enhance CLIP-based image classification performance using unlabeled
images. Our approach unfolds in three key steps: (1) We generate robust textual
feature embeddings that more accurately represent object classes by leveraging
class-specific descriptions from LLMs, enabling more effective zero-shot
classification compared to CLIP's default name-specific prompts. (2) These
textual embeddings are then used to produce pseudo-labels to train an alignment
module that integrates the complementary strengths of LLM description-based
textual embeddings and DINO's visual features. (3) Finally, we prompt-tune
CLIP's vision encoder through DINO-assisted supervision using the trained
alignment module. This three-step process allows us to harness the best of
visual and textual foundation models, resulting in a powerful and efficient
approach that surpasses state-of-the-art label-free classification methods.
Notably, our framework, NoLA (No Labels Attached), achieves an average absolute
gain of 3.6% over the state-of-the-art LaFter across 11 diverse image
classification datasets.

摘要：在基礎模型時代，CLIP 已成為一種強大的工具，用於將文字和視覺模態對齊到一個通用的嵌入空間中。然而，用於訓練 CLIP 的對齊目標通常會導致精細任務的視覺特徵次佳。相比之下，由於其專門的訓練範例，像 DINO 這樣的 SSL 預訓練模型擅長提取豐富的視覺特徵。然而，這些 SSL 模型需要一個額外的監督線性探測步驟，這依賴於完全標記的數據，而這些數據通常很昂貴且難以大規模獲取。在本文中，我們提出了一種無標籤提示調整方法，該方法利用自監督學習模型（DINO）的豐富視覺特徵和大語言模型（LLM）的廣泛文本知識，使用未標記的圖像極大地增強了基於 CLIP 的圖像分類性能。我們的做法分為三個關鍵步驟：(1) 我們生成強大的文本特徵嵌入，這些嵌入通過利用來自 LLM 的類特定描述更準確地表示對象類別，與 CLIP 的默認名稱特定提示相比，實現了更有效的零次分類。(2) 然後使用這些文本嵌入來產生偽標籤，以訓練一個對齊模組，該模組整合了基於 LLM 描述的文本嵌入和 DINO 的視覺特徵的互補優勢。(3) 最後，我們通過使用訓練好的對齊模組，通過 DINO 輔助監督提示調整 CLIP 的視覺編碼器。這個三步驟流程使我們能夠利用視覺和文本基礎模型的優點，從而產生一種強大且有效的方法，超越了最先進的無標籤分類方法。值得注意的是，我們的框架 NoLA（無標籤附加）在 11 個不同的圖像分類數據集上實現了比最先進的 LaFter 高出 3.6% 的平均絕對增益。

##### **An Adversarial Learning Approach to Irregular Time-Series Forecasting**
2411.19341v1 by Heejeong Nam, Jihyun Kim, Jimin Yeom

Forecasting irregular time series presents significant challenges due to two
key issues: the vulnerability of models to mean regression, driven by the noisy
and complex nature of the data, and the limitations of traditional error-based
evaluation metrics, which fail to capture meaningful patterns and penalize
unrealistic forecasts. These problems result in forecasts that often misalign
with human intuition. To tackle these challenges, we propose an adversarial
learning framework with a deep analysis of adversarial components.
Specifically, we emphasize the importance of balancing the modeling of global
distribution (overall patterns) and transition dynamics (localized temporal
changes) to better capture the nuances of irregular time series. Overall, this
research provides practical insights for improving models and evaluation
metrics, and pioneers the application of adversarial learning in the domian of
irregular time-series forecasting.

摘要：預測不規則時間序列會因為兩個關鍵問題而產生重大挑戰：模型容易受到均值迴歸的影響，這是因為資料的雜訊和複雜性質所致，以及傳統基於誤差的評估指標的限制，這些指標無法捕捉有意義的模式並懲罰不切實際的預測。這些問題導致預測經常與人類直覺不一致。為了應對這些挑戰，我們提出了一個對抗性學習架構，並深入分析了對抗性組成。具體來說，我們強調平衡全局分佈（整體模式）和轉換動態（局部時間變化）建模的重要性，以更好地捕捉不規則時間序列的細微差別。總的來說，這項研究為改進模型和評估指標提供了實用的見解，並開創了對抗性學習在不規則時間序列預測領域的應用。

##### **Towards a Mechanistic Explanation of Diffusion Model Generalization**
2411.19339v1 by Matthew Niedoba, Berend Zwartsenberg, Kevin Murphy, Frank Wood

We propose a mechanism for diffusion generalization based on local denoising
operations. Through analysis of network and empirical denoisers, we identify
local inductive biases in diffusion models. We demonstrate that local denoising
operations can be used to approximate the optimal diffusion denoiser. Using a
collection of patch-based, local empirical denoisers, we construct a denoiser
which approximates the generalization behaviour of diffusion model denoisers
over forward and reverse diffusion processes.

摘要：我們提出了一種基於局部去噪運算的擴散概化機制。透過對網路和經驗去噪器的分析，我們找出擴散模型中的局部歸納偏差。我們證明了局部去噪運算可用於逼近最佳擴散去噪器。使用一系列基於區塊的局部經驗去噪器，我們建構了一個去噪器，它逼近了擴散模型去噪器在正向和反向擴散過程中的一般化行為。

##### **PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning**
2411.19335v1 by Shenghui Li, Edith C. -H. Ngai, Fanghua Ye, Thiemo Voigt

Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a
promising paradigm for privacy-preserving and efficient adaptation of
Pre-trained Language Models (PLMs) in Federated Learning (FL) settings. It
preserves data privacy by keeping the data decentralized and training the model
on local devices, ensuring that raw data never leaves the user's device.
Moreover, the integration of PEFT methods such as LoRA significantly reduces
the number of trainable parameters compared to fine-tuning the entire model,
thereby minimizing communication costs and computational overhead. Despite its
potential, the security implications of FedPEFT remain underexplored. This
paper introduces a novel security threat to FedPEFT, termed PEFT-as-an-Attack
(PaaA), which exposes how PEFT can be exploited as an attack vector to
circumvent PLMs' safety alignment and generate harmful content in response to
malicious prompts. Our evaluation of PaaA reveals that with less than 1% of the
model's parameters set as trainable, and a small subset of clients acting
maliciously, the attack achieves an approximate 80% attack success rate using
representative PEFT methods such as LoRA. To mitigate this threat, we further
investigate potential defense strategies, including Robust Aggregation Schemes
(RASs) and Post-PEFT Safety Alignment (PPSA). However, our empirical analysis
highlights the limitations of these defenses, i.e., even the most advanced
RASs, such as DnC and ClippedClustering, struggle to defend against PaaA in
scenarios with highly heterogeneous data distributions. Similarly, while PPSA
can reduce attack success rates to below 10%, it severely degrades the model's
accuracy on the target task. Our results underscore the urgent need for more
effective defense mechanisms that simultaneously ensure security and maintain
the performance of the FedPEFT paradigm.

摘要：聯邦參數高效微調 (FedPEFT) 已成為聯邦學習 (FL) 設定中一種有前途的範例，用於隱私保護和預訓練語言模型 (PLM) 的高效適應。它透過維持資料分散化和在本地裝置上訓練模型來維護資料隱私，確保原始資料絕不會離開使用者的裝置。此外，整合 LoRA 等 PEFT 方法可大幅減少可訓練參數的數量，與微調整個模型相比，從而將通訊成本和運算負擔降至最低。儘管有潛力，FedPEFT 的安全性影響仍未得到充分探討。本文介紹了 FedPEFT 的一種新型安全威脅，稱為 PEFT 作為攻擊 (PaaA)，它揭露了 PEFT 如何被當作攻擊媒介，用以規避 PLM 的安全比對，並根據惡意提示產生有害內容。我們對 PaaA 的評估顯示，使用 LoRA 等具代表性的 PEFT 方法，即使只有不到 1% 的模型參數設定為可訓練參數，且只有少數用戶惡意行事，攻擊仍可達到約 80% 的攻擊成功率。為了減輕此威脅，我們進一步探討潛在的防禦策略，包括穩健聚合方案 (RAS) 和 PEFT 後安全比對 (PPSA)。然而，我們的實證分析突顯了這些防禦的限制，亦即，即使是最先進的 RAS，例如 DnC 和 ClippedClustering，在資料分佈高度異質化的情況下，也很難抵禦 PaaA。同樣地，雖然 PPSA 可以將攻擊成功率降低到 10% 以下，但它會嚴重降低模型在目標任務上的準確度。我們的結果強調了迫切需要更有效的防禦機制，同時確保安全性並維持 FedPEFT 範例的效能。

##### **Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation**
2411.19331v1 by Luca Barsellotti, Lorenzo Bianchi, Nicola Messina, Fabio Carrara, Marcella Cornia, Lorenzo Baraldi, Fabrizio Falchi, Rita Cucchiara

Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form
textual concepts without predefined training classes. While existing
vision-language models such as CLIP can generate segmentation masks by
leveraging coarse spatial information from Vision Transformers, they face
challenges in spatial localization due to their global alignment of image and
text features. Conversely, self-supervised visual models like DINO excel in
fine-grained visual encoding but lack integration with language. To bridge this
gap, we present Talk2DINO, a novel hybrid approach that combines the spatial
accuracy of DINOv2 with the language understanding of CLIP. Our approach aligns
the textual embeddings of CLIP to the patch-level features of DINOv2 through a
learned mapping function without the need to fine-tune the underlying
backbones. At training time, we exploit the attention maps of DINOv2 to
selectively align local visual patches with textual embeddings. We show that
the powerful semantic and localization abilities of Talk2DINO can enhance the
segmentation process, resulting in more natural and less noisy segmentations,
and that our approach can also effectively distinguish foreground objects from
the background. Experimental results demonstrate that Talk2DINO achieves
state-of-the-art performance across several unsupervised OVS benchmarks. Source
code and models are publicly available at:
https://lorebianchi98.github.io/Talk2DINO/.

摘要：開放式詞彙分割 (OVS) 旨在從自由形式文本概念中分割影像，而無需預先定義的訓練類別。雖然現有的視覺語言模型（例如 CLIP）可以透過利用 Vision Transformers 中的粗略空間資訊來產生分割遮罩，但由於其影像和文字特徵的整體對齊，它們在空間定位方面面臨挑戰。相反地，自監督視覺模型（例如 DINO）擅長精細的視覺編碼，但缺乏與語言的整合。為了彌合這一差距，我們提出了 Talk2DINO，這是一種新穎的混合方法，結合了 DINOv2 的空間準確性和 CLIP 的語言理解。我們的做法透過一個學習的對應函數，將 CLIP 的文本嵌入與 DINOv2 的區塊層級特徵對齊，而無需微調底層主幹。在訓練期間，我們利用 DINOv2 的注意力圖有選擇性地將局部視覺區塊與文本嵌入對齊。我們展示了 Talk2DINO 強大的語義和定位能力可以增強分割過程，產生更自然、雜訊更少的分割，並且我們的做法也可以有效地將前景物體與背景區分開來。實驗結果表明，Talk2DINO 在多個非監督 OVS 基準測試中實現了最先進的效能。原始程式碼和模型已公開在以下網址取得：
https://lorebianchi98.github.io/Talk2DINO/。

##### **Structured Object Language Modeling (SoLM): Native Structured Objects Generation Conforming to Complex Schemas with Self-Supervised Denoising**
2411.19301v1 by Amir Tavanaei, Kee Kiat Koo, Hayreddin Ceker, Shaobai Jiang, Qi Li, Julien Han, Karim Bouyarmane

In this paper, we study the problem of generating structured objects that
conform to a complex schema, with intricate dependencies between the different
components (facets) of the object. The facets of the object (attributes,
fields, columns, properties) can be a mix of short, structured,
type-constrained facts, or long natural-language descriptions. The object has
to be self-consistent between the different facets in the redundant information
it carries (relative consistency), while being grounded with respect to world
knowledge (absolute consistency). We frame the problem as a Language Modeling
problem (Structured Object Language Modeling) and train an LLM to perform the
task natively, without requiring instructions or prompt-engineering. We propose
a self-supervised denoising method to train the model from an existing dataset
of such objects. The input query can be the existing object itself, in which
case the model acts as a regenerator, completing, correcting, normalizing the
input, or any unstructured blurb to be structured. We show that the
self-supervised denoising training provides a strong baseline, and that
additional supervised fine-tuning with small amount of human demonstrations
leads to further improvement. Experimental results show that the proposed
method matches or outperforms prompt-engineered general-purpose
state-of-the-art LLMs (Claude 3, Mixtral-8x7B), while being order-of-magnitude
more cost-efficient.

摘要：<paragraph>在本文中，我们研究了生成符合复杂模式的结构化对象的问题，该模式在对象的各个组件（方面）之间存在错综复杂的依赖关系。对象的方面（属性、字段、列、属性）可以是简短的、结构化的、受类型约束的事实，或者冗长的自然语言描述。对象必须在其携带的冗余信息（相对一致性）中在不同方面之间保持自洽，同时在世界知识（绝对一致性）方面保持基础。我们将问题构建为语言建模问题（结构化对象语言建模），并训练 LLM 以本机方式执行任务，而无需指令或提示工程。我们提出了一种自监督去噪方法，以使用此类对象的现有数据集训练模型。输入查询可以是现有对象本身，在这种情况下，模型充当再生器，完成、更正、规范输入或任何非结构化短文以进行结构化。我们表明，自监督去噪训练提供了强大的基线，并且少量人工演示的额外监督微调会进一步改进。实验结果表明，所提出的方法与提示工程通用最先进的 LLM（Claude 3、Mixtral-8x7B）相匹配或优于它们，同时具有数量级更高的成本效益。</paragraph>

##### **Extracting Information in a Low-resource Setting: Case Study on Bioinformatics Workflows**
2411.19295v1 by Clémence Sebe, Sarah Cohen-Boulakia, Olivier Ferret, Aurélie Névéol

Bioinformatics workflows are essential for complex biological data analyses
and are often described in scientific articles with source code in public
repositories. Extracting detailed workflow information from articles can
improve accessibility and reusability but is hindered by limited annotated
corpora. To address this, we framed the problem as a low-resource extraction
task and tested four strategies: 1) creating a tailored annotated corpus, 2)
few-shot named-entity recognition (NER) with an autoregressive language model,
3) NER using masked language models with existing and new corpora, and 4)
integrating workflow knowledge into NER models. Using BioToFlow, a new corpus
of 52 articles annotated with 16 entities, a SciBERT-based NER model achieved a
70.4 F-measure, comparable to inter-annotator agreement. While knowledge
integration improved performance for specific entities, it was less effective
across the entire information schema. Our results demonstrate that
high-performance information extraction for bioinformatics workflows is
achievable.

摘要：生物資訊工作流程對於複雜的生物資料分析至關重要，且通常以公開儲存庫中的原始碼在科學文章中描述。從文章中萃取出詳細的工作流程資訊可以提升可近性與可重複使用性，但受到標註語料庫有限的阻礙。為了解決這個問題，我們將問題設定為低資源萃取任務，並測試了四種策略：1) 建立一個量身打造的標註語料庫，2) 使用自迴歸語言模型進行少量樣本命名實體辨識 (NER)，3) 使用遮蔽語言模型與現有和新的語料庫進行 NER，以及 4) 將工作流程知識整合到 NER 模型中。使用 BioToFlow，一個標註有 16 個實體的 52 篇文章的新語料庫，基於 SciBERT 的 NER 模型達到了 70.4 的 F 值，與標註者間的一致性相當。雖然知識整合改善了特定實體的效能，但對於整個資訊架構來說，它的效果較差。我們的結果證明，對於生物資訊工作流程的高效能資訊萃取是可以達成的。

##### **On-chip Hyperspectral Image Segmentation with Fully Convolutional Networks for Scene Understanding in Autonomous Driving**
2411.19274v1 by Jon Gutiérrez-Zaballa, Koldo Basterretxea, Javier Echanobe, M. Victoria Martínez, Unai Martínez-Corral, Óscar Mata Carballeira, Inés del Campo

Most of current computer vision-based advanced driver assistance systems
(ADAS) perform detection and tracking of objects quite successfully under
regular conditions. However, under adverse weather and changing lighting
conditions, and in complex situations with many overlapping objects, these
systems are not completely reliable. The spectral reflectance of the different
objects in a driving scene beyond the visible spectrum can offer additional
information to increase the reliability of these systems, especially under
challenging driving conditions. Furthermore, this information may be
significant enough to develop vision systems that allow for a better
understanding and interpretation of the whole driving scene. In this work we
explore the use of snapshot, video-rate hyperspectral imaging (HSI) cameras in
ADAS on the assumption that the near infrared (NIR) spectral reflectance of
different materials can help to better segment the objects in real driving
scenarios. To do this, we have used the HSI-Drive 1.1 dataset to perform
various experiments on spectral classification algorithms. However, the
information retrieval of hyperspectral recordings in natural outdoor scenarios
is challenging, mainly because of deficient colour constancy and other inherent
shortcomings of current snapshot HSI technology, which poses some limitations
to the development of pure spectral classifiers. In consequence, in this work
we analyze to what extent the spatial features codified by standard, tiny fully
convolutional network (FCN) models can improve the performance of HSI
segmentation systems for ADAS applications.
  The abstract above is truncated due to submission limits. For the full
abstract, please refer to the published article.

摘要：當前基於電腦視覺的高階駕駛輔助系統 (ADAS) 大多在一般情況下相當成功地執行物件偵測和追蹤。然而，在惡劣天氣和變化的光照條件下，以及許多重疊物件的複雜情況中，這些系統並非完全可靠。駕駛場景中不同物件在可見光譜之外的不同光譜反射率可提供額外資訊，以提高這些系統的可靠性，特別是在具有挑戰性的駕駛條件下。此外，此資訊可能足夠重要，足以開發出能讓系統更佳理解和詮釋整個駕駛場景的視覺系統。在這項工作中，我們探討在 ADAS 中使用快照、影片速率高光譜影像 (HSI) 相機，假設不同材質的近紅外線 (NIR) 光譜反射率有助於在真實駕駛場景中更佳地分割物件。為此，我們使用 HSI-Drive 1.1 資料集對光譜分類演算法執行各種實驗。然而，在自然戶外場景中擷取高光譜記錄的資訊具有挑戰性，這主要是由於當前快照 HSI 技術的色彩恆常性不足和其他固有缺點，這對純光譜分類器的開發造成了一些限制。因此，在這項工作中，我們分析標準、小型全卷積網路 (FCN) 模型所編碼的空間特徵在多大程度上能改善 ADAS 應用中 HSI 分割系統的效能。
由於提交限制，以上摘要已被截斷。如需完整摘要，請參閱已發表的文章。

##### **Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks**
2411.19244v1 by Jinu Nyachhyon, Mridul Sharma, Prajwal Thapa, Bal Krishna Bal

The Nepali language has distinct linguistic features, especially its complex
script (Devanagari script), morphology, and various dialects, which pose a
unique challenge for natural language processing (NLP) evaluation. While the
Nepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a
foundation for evaluating models, it remains limited in scope, covering four
tasks. This restricts their utility for comprehensive assessments of NLP
models. To address this limitation, we introduce eight new datasets, creating a
new benchmark, the Nepali Language Understanding Evaluation (NLUE) benchmark,
which covers a total of 12 tasks for evaluating the performance of models
across a diverse set of Natural Language Understanding (NLU) tasks. The added
tasks include single-sentence classification, similarity and paraphrase tasks,
and Natural Language Inference (NLI) tasks. On evaluating the models using
added tasks, we observe that the existing models fall short in handling complex
NLU tasks effectively. This expanded benchmark sets a new standard for
evaluating, comparing, and advancing models, contributing significantly to the
broader goal of advancing NLP research for low-resource languages.

摘要：尼泊爾語具有獨特的語言特徵，特別是其複雜的文字（天城文）、形態學和各種方言，對自然語言處理 (NLP) 評估構成獨特的挑戰。儘管尼泊爾語理解評估 (Nep-gLUE) 基準為評估模型提供了基礎，但其範圍仍然有限，僅涵蓋四項任務。這限制了它們對 NLP 模型進行全面評估的效用。為了解決這個限制，我們引入了八個新資料集，建立了一個新的基準，即尼泊爾語理解評估 (NLUE) 基準，它涵蓋了總共 12 項任務，用於評估模型在各種自然語言理解 (NLU) 任務中的表現。新增的任務包括單句分類、相似性和同義詞改寫任務，以及自然語言推理 (NLI) 任務。在使用新增任務評估模型時，我們觀察到現有模型在有效處理複雜的 NLU 任務方面存在不足。這個擴展的基準為評估、比較和推進模型設定了一個新的標準，為推進低資源語言的 NLP 研究的更廣泛目標做出了重大貢獻。

