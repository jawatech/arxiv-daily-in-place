{"2407.02483": {"publish_time": "2024-07-02", "title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "paper_summary": "Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit\nlimited generality and often fall short when compared to specialized models.\nRecently, LLM-based agents have been developed to address these challenges by\nselecting appropriate specialized models as tools based on user inputs.\nHowever, such advancements have not been extensively explored within the\nmedical domain. To bridge this gap, this paper introduces the first agent\nexplicitly designed for the medical field, named \\textbf{M}ulti-modal\n\\textbf{Med}ical \\textbf{Agent} (MMedAgent). We curate an instruction-tuning\ndataset comprising six medical tools solving seven tasks, enabling the agent to\nchoose the most suitable tools for a given task. Comprehensive experiments\ndemonstrate that MMedAgent achieves superior performance across a variety of\nmedical tasks compared to state-of-the-art open-source methods and even the\nclosed-source model, GPT-4o. Furthermore, MMedAgent exhibits efficiency in\nupdating and integrating new medical tools.", "paper_summary_zh": "\u5118\u7ba1\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u6210\u529f\uff0c\u4f46\u5176\u666e\u904d\u6027\u6709\u9650\uff0c\u8207\u5c08\u7528\u6a21\u578b\u76f8\u6bd4\u6642\u5e38\u6709\u6240\u4e0d\u8db3\u3002\n\u6700\u8fd1\uff0c\u57fa\u65bc LLM \u7684\u4ee3\u7406\u5df2\u88ab\u958b\u767c\u51fa\u4f86\uff0c\u4ee5\u900f\u904e\u6839\u64da\u4f7f\u7528\u8005\u8f38\u5165\u9078\u64c7\u9069\u7576\u7684\u5c08\u7528\u6a21\u578b\u4f5c\u70ba\u5de5\u5177\u4f86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\u3002\n\u7136\u800c\uff0c\u6b64\u985e\u9032\u5c55\u5c1a\u672a\u5728\u91ab\u7642\u9818\u57df\u4e2d\u5ee3\u6cdb\u63a2\u8a0e\u3002\u70ba\u4e86\u5f4c\u88dc\u6b64\u5dee\u8ddd\uff0c\u672c\u6587\u4ecb\u7d39\u4e86\u7b2c\u4e00\u500b\u5c08\u9580\u70ba\u91ab\u7642\u9818\u57df\u8a2d\u8a08\u7684\u4ee3\u7406\uff0c\u540d\u70ba**M**ulti-modal **Med**ical **Agent** (MMedAgent)\u3002\u6211\u5011\u6574\u7406\u4e86\u4e00\u500b\u7531\u516d\u7a2e\u89e3\u6c7a\u4e03\u9805\u4efb\u52d9\u7684\u91ab\u7642\u5de5\u5177\u7d44\u6210\u7684\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u96c6\uff0c\u8b93\u4ee3\u7406\u80fd\u5920\u70ba\u7279\u5b9a\u4efb\u52d9\u9078\u64c7\u6700\u5408\u9069\u7684\u5de5\u5177\u3002\u5168\u9762\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u6700\u5148\u9032\u7684\u958b\u6e90\u65b9\u6cd5\uff0c\u751a\u81f3\u9589\u6e90\u6a21\u578b GPT-4o \u76f8\u6bd4\uff0cMMedAgent \u5728\u5404\u7a2e\u91ab\u7642\u4efb\u52d9\u4e2d\u90fd\u53d6\u5f97\u4e86\u512a\u7570\u7684\u8868\u73fe\u3002\u6b64\u5916\uff0cMMedAgent \u5728\u66f4\u65b0\u548c\u6574\u5408\u65b0\u7684\u91ab\u7642\u5de5\u5177\u65b9\u9762\u5c55\u73fe\u51fa\u6548\u7387\u3002", "author": "Binxu Li et.al.", "authors": "Binxu Li, Tiankai Yan, Yuanting Pan, Zhe Xu, Jie Luo, Ruiyang Ji, Shilong Liu, Haoyu Dong, Zihao Lin, Yixin Wang", "id": "2407.02483v1", "paper_url": "http://arxiv.org/abs/2407.02483v1", "repo": "null"}}