{"2410.11302": {"publish_time": "2024-10-15", "title": "Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs", "paper_summary": "In the study of LLMs, sycophancy represents a prevalent hallucination that\nposes significant challenges to these models. Specifically, LLMs often fail to\nadhere to original correct responses, instead blindly agreeing with users'\nopinions, even when those opinions are incorrect or malicious. However,\nresearch on sycophancy in visual language models (VLMs) has been scarce. In\nthis work, we extend the exploration of sycophancy from LLMs to VLMs,\nintroducing the MM-SY benchmark to evaluate this phenomenon. We present\nevaluation results from multiple representative models, addressing the gap in\nsycophancy research for VLMs. To mitigate sycophancy, we propose a synthetic\ndataset for training and employ methods based on prompts, supervised\nfine-tuning, and DPO. Our experiments demonstrate that these methods\neffectively alleviate sycophancy in VLMs. Additionally, we probe VLMs to assess\nthe semantic impact of sycophancy and analyze the attention distribution of\nvisual tokens. Our findings indicate that the ability to prevent sycophancy is\npredominantly observed in higher layers of the model. The lack of attention to\nimage knowledge in these higher layers may contribute to sycophancy, and\nenhancing image attention at high layers proves beneficial in mitigating this\nissue.", "paper_summary_zh": "\u5728 LLM \u7684\u7814\u7a76\u4e2d\uff0c\u963f\u8c00\u5949\u627f\u4ee3\u8868\u4e86\u4e00\u79cd\u666e\u904d\u5b58\u5728\u7684\u5e7b\u89c9\uff0c\u5bf9\u8fd9\u4e9b\u6a21\u578b\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u5177\u4f53\u800c\u8a00\uff0cLLM \u7ecf\u5e38\u65e0\u6cd5\u575a\u6301\u6700\u521d\u7684\u6b63\u786e\u53cd\u5e94\uff0c\u800c\u662f\u76f2\u76ee\u5730\u540c\u610f\u7528\u6237\u7684\u610f\u89c1\uff0c\u5373\u4f7f\u8fd9\u4e9b\u610f\u89c1\u4e0d\u6b63\u786e\u6216\u5e26\u6709\u6076\u610f\u3002\u7136\u800c\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4e2d\u7684\u963f\u8c00\u5949\u627f\u7814\u7a76\u5374\u5f88\u5c11\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u5bf9\u963f\u8c00\u5949\u627f\u7684\u63a2\u7d22\u4ece LLM \u6269\u5c55\u5230 VLM\uff0c\u5f15\u5165\u4e86 MM-SY \u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u79cd\u73b0\u8c61\u3002\u6211\u4eec\u5c55\u793a\u4e86\u591a\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u7684\u8bc4\u4f30\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86 VLM \u4e2d\u963f\u8c00\u5949\u627f\u7814\u7a76\u7684\u5dee\u8ddd\u3002\u4e3a\u4e86\u51cf\u8f7b\u963f\u8c00\u5949\u627f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u4e86\u57fa\u4e8e\u63d0\u793a\u3001\u76d1\u7763\u5fae\u8c03\u548c DPO \u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u8f7b\u4e86 VLM \u4e2d\u7684\u963f\u8c00\u5949\u627f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63a2\u6d4b VLM \u4ee5\u8bc4\u4f30\u963f\u8c00\u5949\u627f\u7684\u8bed\u4e49\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u5e03\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9632\u6b62\u963f\u8c00\u5949\u627f\u7684\u80fd\u529b\u4e3b\u8981\u5b58\u5728\u4e8e\u6a21\u578b\u7684\u8f83\u9ad8\u5c42\u3002\u5728\u8fd9\u4e9b\u8f83\u9ad8\u5c42\u4e2d\u7f3a\u4e4f\u5bf9\u56fe\u50cf\u77e5\u8bc6\u7684\u5173\u6ce8\u53ef\u80fd\u4f1a\u5bfc\u81f4\u963f\u8c00\u5949\u627f\uff0c\u800c\u5728\u9ad8\u5c42\u589e\u5f3a\u56fe\u50cf\u6ce8\u610f\u529b\u88ab\u8bc1\u660e\u6709\u52a9\u4e8e\u51cf\u8f7b\u8fd9\u4e2a\u95ee\u9898\u3002", "author": "Shuo Li et.al.", "authors": "Shuo Li, Tao Ji, Xiaoran Fan, Linsheng Lu, Leyi Yang, Yuming Yang, Zhiheng Xi, Rui Zheng, Yuran Wang, Xiaohui Zhao, Tao Gui, Qi Zhang, Xuanjing Huang", "id": "2410.11302v1", "paper_url": "http://arxiv.org/abs/2410.11302v1", "repo": "null"}}