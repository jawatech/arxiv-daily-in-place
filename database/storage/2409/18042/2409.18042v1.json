{"2409.18042": {"publish_time": "2024-09-26", "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions", "paper_summary": "GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nin the open-source community. Existing vision-language models rely on external\ntools for the speech processing, while speech-language models still suffer from\nlimited or even without vision-understanding abilities. To address this gap, we\npropose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large\nLanguage Models with end-to-end speech capabilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we notice surprisingly that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the corresponding\nbi-modal aligned counterparts. Moreover, a lightweight style module is proposed\nfor flexible speech style controls (e.g., emotions and pitches). For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.", "paper_summary_zh": "GPT-4o \u662f\u4e00\u500b\u5168\u6a21\u614b\u6a21\u578b\uff0c\u80fd\u4ee5\u4e0d\u540c\u7684\u60c5\u7dd2\u548c\u8a9e\u8abf\u9032\u884c\u8a9e\u97f3\u5c0d\u8a71\uff0c\u6a19\u8a8c\u8457\u5168\u6a21\u614b\u57fa\u790e\u6a21\u578b\u7684\u91cc\u7a0b\u7891\u3002\n\u7136\u800c\uff0c\u5728\u958b\u653e\u539f\u59cb\u78bc\u793e\u7fa4\u4e2d\uff0c\u8ce6\u4e88\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u611f\u77e5\u548c\u751f\u6210\u5716\u50cf\u3001\u6587\u5b57\u548c\u8a9e\u97f3\u7684\u80fd\u529b\uff0c\u4e26\u4f7f\u7528\u516c\u958b\u8cc7\u6599\u9032\u884c\u7aef\u5c0d\u7aef\u8655\u7406\uff0c\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u73fe\u6709\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4f9d\u8cf4\u65bc\u5916\u90e8\u5de5\u5177\u9032\u884c\u8a9e\u97f3\u8655\u7406\uff0c\u800c\u8a9e\u97f3\u8a9e\u8a00\u6a21\u578b\u4ecd\u7136\u7f3a\u4e4f\u6216\u751a\u81f3\u6c92\u6709\u8996\u89ba\u7406\u89e3\u80fd\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86 EMOVA\uff08\u60c5\u611f\u5168\u65b9\u4f4d\u8a9e\u97f3\u52a9\u7406\uff09\uff0c\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5177\u5099\u7aef\u5c0d\u7aef\u8a9e\u97f3\u529f\u80fd\uff0c\u540c\u6642\u4fdd\u6301\u9818\u5148\u7684\u8996\u89ba\u8a9e\u8a00\u6548\u80fd\u3002\u900f\u904e\u8a9e\u7fa9\u8072\u5b78\u5206\u96e2\u7684\u8a9e\u97f3\u5206\u8a5e\u5668\uff0c\u6211\u5011\u9a5a\u8a1d\u5730\u767c\u73fe\uff0c\u8207\u5c0d\u61c9\u7684\u96d9\u6a21\u614b\u5c0d\u61c9\u9805\u76ee\u76f8\u6bd4\uff0c\u5168\u6a21\u614b\u5c0d\u9f4a\u53ef\u4ee5\u9032\u4e00\u6b65\u589e\u5f37\u8996\u89ba\u8a9e\u8a00\u548c\u8a9e\u97f3\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u63d0\u51fa\u4e86\u4e00\u500b\u8f15\u91cf\u7d1a\u6a23\u5f0f\u6a21\u7d44\uff0c\u7528\u65bc\u9748\u6d3b\u7684\u8a9e\u97f3\u6a23\u5f0f\u63a7\u5236\uff08\u4f8b\u5982\uff0c\u60c5\u7dd2\u548c\u97f3\u9ad8\uff09\u3002EMOVA \u9996\u6b21\u5728\u8996\u89ba\u8a9e\u8a00\u548c\u8a9e\u97f3\u57fa\u6e96\u6e2c\u8a66\u4e2d\u90fd\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u540c\u6642\u652f\u63f4\u5177\u6709\u751f\u52d5\u60c5\u7dd2\u7684\u5168\u6a21\u614b\u53e3\u8a9e\u5c0d\u8a71\u3002", "author": "Kai Chen et.al.", "authors": "Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Lanqing Hong, Lu Hou, Hang Xu", "id": "2409.18042v1", "paper_url": "http://arxiv.org/abs/2409.18042v1", "repo": "null"}}