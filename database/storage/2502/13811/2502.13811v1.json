{"2502.13811": {"publish_time": "2025-02-19", "title": "On the Duality between Gradient Transformations and Adapters", "paper_summary": "We study memory-efficient optimization of neural networks with linear\ngradient transformations, where the gradients are linearly mapped to a lower\ndimensional space than the full parameter space, thus saving memory required\nfor gradient accumulation and optimizer state persistence. The model parameters\nare updated by first performing an optimization step in the lower dimensional\nspace and then going back into the original parameter space via the linear\nmap's transpose. We show that optimizing the model in this transformed space is\nequivalent to reparameterizing the original model through a linear adapter that\nadditively modifies the model parameters, and then only optimizing the\nadapter's parameters. When the transformation is Kronecker-factored, this\nestablishes an equivalence between GaLore and one-sided LoRA. We show that this\nduality between gradient transformations and adapter-based reparameterizations\nunifies existing approaches to memory-efficient training and suggests new\ntechniques for improving training efficiency and memory use.", "paper_summary_zh": "\u6211\u5011\u7814\u7a76\u5177\u6709\u7dda\u6027\u68af\u5ea6\u8f49\u63db\u7684\u795e\u7d93\u7db2\u8def\u7684\u8a18\u61b6\u9ad4\u9ad8\u6548\u6700\u4f73\u5316\uff0c\u5176\u4e2d\u68af\u5ea6\u88ab\u7dda\u6027\u6620\u5c04\u5230\u6bd4\u5b8c\u6574\u53c3\u6578\u7a7a\u9593\u66f4\u4f4e\u7dad\u5ea6\u7684\u7a7a\u9593\uff0c\u5f9e\u800c\u7bc0\u7701\u4e86\u68af\u5ea6\u7d2f\u7a4d\u548c\u6700\u4f73\u5316\u5668\u72c0\u614b\u6301\u7e8c\u6027\u6240\u9700\u7684\u8a18\u61b6\u9ad4\u3002\u6a21\u578b\u53c3\u6578\u6703\u5148\u5728\u4f4e\u7dad\u5ea6\u7a7a\u9593\u57f7\u884c\u6700\u4f73\u5316\u6b65\u9a5f\uff0c\u518d\u900f\u904e\u7dda\u6027\u6620\u5c04\u7684\u8f49\u7f6e\u8fd4\u56de\u5230\u539f\u59cb\u53c3\u6578\u7a7a\u9593\u4f86\u9032\u884c\u66f4\u65b0\u3002\u6211\u5011\u8aaa\u660e\u5728\u9019\u500b\u8f49\u63db\u7a7a\u9593\u6700\u4f73\u5316\u6a21\u578b\u7b49\u65bc\u900f\u904e\u7dda\u6027\u8f49\u63a5\u5668\u91cd\u65b0\u53c3\u6578\u5316\u539f\u59cb\u6a21\u578b\uff0c\u9019\u500b\u8f49\u63a5\u5668\u6703\u7d2f\u52a0\u4fee\u6539\u6a21\u578b\u53c3\u6578\uff0c\u7136\u5f8c\u50c5\u6700\u4f73\u5316\u8f49\u63a5\u5668\u7684\u53c3\u6578\u3002\u7576\u8f49\u63db\u662f Kronecker \u5206\u89e3\u6642\uff0c\u9019\u6703\u5728 GaLore \u548c\u55ae\u908a LoRA \u4e4b\u9593\u5efa\u7acb\u7b49\u6548\u6027\u3002\u6211\u5011\u8aaa\u660e\u68af\u5ea6\u8f49\u63db\u548c\u57fa\u65bc\u8f49\u63a5\u5668\u7684\u91cd\u65b0\u53c3\u6578\u5316\u4e4b\u9593\u7684\u9019\u7a2e\u5c0d\u5076\u6027\u7d71\u4e00\u4e86\u73fe\u6709\u7684\u8a18\u61b6\u9ad4\u9ad8\u6548\u8a13\u7df4\u65b9\u6cd5\uff0c\u4e26\u5efa\u8b70\u65b0\u7684\u6280\u8853\u4f86\u6539\u5584\u8a13\u7df4\u6548\u7387\u548c\u8a18\u61b6\u9ad4\u4f7f\u7528\u3002", "author": "Lucas Torroba-Hennigen et.al.", "authors": "Lucas Torroba-Hennigen, Hunter Lang, Han Guo, Yoon Kim", "id": "2502.13811v1", "paper_url": "http://arxiv.org/abs/2502.13811v1", "repo": "null"}}