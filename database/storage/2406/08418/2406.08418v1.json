{"2406.08418": {"publish_time": "2024-06-12", "title": "OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text", "paper_summary": "Image-text interleaved data, consisting of multiple images and texts arranged\nin a natural document format, aligns with the presentation paradigm of internet\ndata and closely resembles human reading habits. Recent studies have shown that\nsuch data aids multimodal in-context learning and maintains the capabilities of\nlarge language models during multimodal fine-tuning. However, the limited scale\nand diversity of current image-text interleaved data restrict the development\nof multimodal large language models. In this paper, we introduce OmniCorpus, a\n10 billion-scale image-text interleaved dataset. Using an efficient data\nengine, we filter and extract large-scale high-quality documents, which contain\n8.6 billion images and 1,696 billion text tokens. Compared to counterparts\n(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while\nmaintaining good data quality; 2) features more diverse sources, including both\nEnglish and non-English websites as well as video-centric websites; 3) is more\nflexible, easily degradable from an image-text interleaved format to pure text\ncorpus and image-text pairs. Through comprehensive analysis and experiments, we\nvalidate the quality, usability, and effectiveness of the proposed dataset. We\nhope this could provide a solid data foundation for future multimodal model\nresearch. Code and data are released at\nhttps://github.com/OpenGVLab/OmniCorpus.", "paper_summary_zh": "\u5f71\u50cf\u6587\u5b57\u7a7f\u63d2\u8cc7\u6599\uff0c\u7531\u591a\u500b\u5f71\u50cf\u548c\u6587\u5b57\u7d44\u6210\uff0c\u4ee5\u81ea\u7136\u7684\u6587\u4ef6\u683c\u5f0f\u6392\u5217\uff0c\u8207\u7db2\u969b\u7db2\u8def\u8cc7\u6599\u7684\u5448\u73fe\u7bc4\u4f8b\u76f8\u7b26\uff0c\u4e14\u8207\u4eba\u985e\u7684\u95b1\u8b80\u7fd2\u6163\u5341\u5206\u985e\u4f3c\u3002\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\uff0c\u6b64\u985e\u8cc7\u6599\u6709\u52a9\u65bc\u591a\u6a21\u614b\u8108\u7d61\u5b78\u7fd2\uff0c\u4e26\u5728\u591a\u6a21\u614b\u5fae\u8abf\u904e\u7a0b\u4e2d\u7dad\u6301\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u5f71\u50cf\u6587\u5b57\u7a7f\u63d2\u8cc7\u6599\u7684\u898f\u6a21\u548c\u591a\u6a23\u6027\u6709\u9650\uff0c\u9650\u5236\u4e86\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u767c\u5c55\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 OmniCorpus\uff0c\u4e00\u500b 100 \u5104\u898f\u6a21\u7684\u5f71\u50cf\u6587\u5b57\u7a7f\u63d2\u8cc7\u6599\u96c6\u3002\u6211\u5011\u4f7f\u7528\u9ad8\u6548\u80fd\u7684\u8cc7\u6599\u5f15\u64ce\uff0c\u7be9\u9078\u4e26\u64f7\u53d6\u5927\u91cf\u9ad8\u54c1\u8cea\u7684\u6587\u4ef6\uff0c\u5176\u4e2d\u5305\u542b 86 \u5104\u5f35\u5f71\u50cf\u548c 1,6960 \u5104\u500b\u6587\u5b57\u7b26\u865f\u3002\u8207\u540c\u985e\u578b\u8cc7\u6599\u96c6\uff08\u4f8b\u5982 MMC4\u3001OBELICS\uff09\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u8cc7\u6599\u96c6 1) \u898f\u6a21\u5927 15 \u500d\uff0c\u540c\u6642\u7dad\u6301\u826f\u597d\u7684\u8cc7\u6599\u54c1\u8cea\uff1b2) \u4f86\u6e90\u66f4\u591a\u5143\uff0c\u5305\u542b\u82f1\u6587\u548c\u975e\u82f1\u6587\u7db2\u7ad9\uff0c\u4ee5\u53ca\u4ee5\u5f71\u7247\u70ba\u4e2d\u5fc3\u7684\u7db2\u7ad9\uff1b3) \u66f4\u5177\u5f48\u6027\uff0c\u53ef\u8f15\u6613\u5f9e\u5f71\u50cf\u6587\u5b57\u7a7f\u63d2\u683c\u5f0f\u8f49\u63db\u70ba\u7d14\u6587\u5b57\u8a9e\u6599\u5eab\u548c\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\u3002\u900f\u904e\u5168\u9762\u7684\u5206\u6790\u548c\u5be6\u9a57\uff0c\u6211\u5011\u9a57\u8b49\u4e86\u6240\u63d0\u51fa\u8cc7\u6599\u96c6\u7684\u54c1\u8cea\u3001\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\u3002\u6211\u5011\u5e0c\u671b\u9019\u80fd\u70ba\u672a\u4f86\u7684\u591a\u6a21\u614b\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u7a69\u56fa\u7684\u8cc7\u6599\u57fa\u790e\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u5df2\u65bc https://github.com/OpenGVLab/OmniCorpus \u767c\u5e03\u3002", "author": "Qingyun Li et.al.", "authors": "Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Pei Chu, Yi Wang, Min Dou, Changyao Tian, Xizhou Zhu, Lewei Lu, Yushi Chen, Junjun He, Tong Lu, Yali Wang, Limin Wang, Dahua Lin, Yu Qiao, Botian Shi, Conghui He, Jifeng Dai", "id": "2406.08418v1", "paper_url": "http://arxiv.org/abs/2406.08418v1", "repo": "https://github.com/opengvlab/omnicorpus"}}