{"2410.19290": {"publish_time": "2024-10-25", "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning", "paper_summary": "Recent studies have identified one aggravating factor of LLM hallucinations\nas the knowledge inconsistency between pre-training and fine-tuning, where\nunfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong\noutputs. In this paper, we propose a novel fine-tuning strategy called\nPrereq-Tune to address this knowledge inconsistency and reduce hallucinations.\nFundamentally, Prereq-Tune disentangles the learning of skills and knowledge,\nso the model learns only the task skills without being impacted by the\nknowledge inconsistency. To achieve this, Prereq-Tune introduces an additional\nprerequisite learning stage to learn the necessary knowledge for SFT, allowing\nsubsequent SFT to focus only on task skills. Prereq-Tune can also be combined\nwith fictitious synthetic data to enhance the grounding of LLM outputs to their\ninternal knowledge. Experiments show that Prereq-Tune outperforms existing\nbaselines in improving LLM's factuality across short QA and long-form\ngeneration tasks. It also opens new possibilities for knowledge-controlled\ngeneration in LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/Prereq_tune.git.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u627e\u51fa LLM \u5e7b\u89c9\u7684\u4e00\u4e2a\u52a0\u91cd\u56e0\u7d20\uff0c\u5373\u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03\u4e4b\u95f4\u7684\u77e5\u8bc6\u4e0d\u4e00\u81f4\uff0c\u5176\u4e2d\u4e0d\u719f\u6089\u7684\u5fae\u8c03\u6570\u636e\u4f1a\u8bef\u5bfc LLM \u634f\u9020\u4f3c\u662f\u800c\u975e\u4f46\u9519\u8bef\u7684\u8f93\u51fa\u3002\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a Prereq-Tune \u7684\u65b0\u5fae\u8c03\u7b56\u7565\u6765\u89e3\u51b3\u6b64\u77e5\u8bc6\u4e0d\u4e00\u81f4\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002\u4ece\u6839\u672c\u4e0a\uff0cPrereq-Tune \u89e3\u5f00\u6280\u80fd\u548c\u77e5\u8bc6\u7684\u5b66\u4e60\uff0c\u56e0\u6b64\u6a21\u578b\u4ec5\u5b66\u4e60\u4efb\u52a1\u6280\u80fd\uff0c\u800c\u4e0d\u4f1a\u53d7\u5230\u77e5\u8bc6\u4e0d\u4e00\u81f4\u7684\u5f71\u54cd\u3002\u4e3a\u5b9e\u73b0\u6b64\u76ee\u7684\uff0cPrereq-Tune \u5f15\u5165\u4e00\u4e2a\u989d\u5916\u7684\u5148\u51b3\u6761\u4ef6\u5b66\u4e60\u9636\u6bb5\u6765\u5b66\u4e60 SFT \u6240\u9700\u7684\u77e5\u8bc6\uff0c\u4ece\u800c\u5141\u8bb8\u540e\u7eed SFT \u4ec5\u4e13\u6ce8\u4e8e\u4efb\u52a1\u6280\u80fd\u3002Prereq-Tune \u8fd8\u53ef\u4ee5\u4e0e\u865a\u6784\u7684\u5408\u6210\u6570\u636e\u7ed3\u5408\u4f7f\u7528\uff0c\u4ee5\u589e\u5f3a LLM \u8f93\u51fa\u4e0e\u5176\u5185\u90e8\u77e5\u8bc6\u7684\u57fa\u7840\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPrereq-Tune \u5728\u6539\u5584 LLM \u5728\u77ed\u95ee\u7b54\u548c\u957f\u7bc7\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4e8b\u5b9e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u51c6\u3002\u5b83\u8fd8\u4e3a LLM \u4e2d\u7684\u77e5\u8bc6\u63a7\u5236\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/UCSB-NLP-Chang/Prereq_tune.git \u83b7\u5f97\u3002", "author": "Yujian Liu et.al.", "authors": "Yujian Liu, Shiyu Chang, Tommi Jaakkola, Yang Zhang", "id": "2410.19290v1", "paper_url": "http://arxiv.org/abs/2410.19290v1", "repo": "https://github.com/ucsb-nlp-chang/prereq_tune"}}