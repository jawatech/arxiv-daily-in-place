{"2501.12895": {"publish_time": "2025-01-22", "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback", "paper_summary": "Large language models (LLMs) demonstrate impressive performance but lack the\nflexibility to adapt to human preferences quickly without retraining. In this\nwork, we introduce Test-time Preference Optimization (TPO), a framework that\naligns LLM outputs with human preferences during inference, removing the need\nto update model parameters. Rather than relying on purely numerical rewards,\nTPO translates reward signals into textual critiques and uses them as textual\nrewards to iteratively refine its response. Evaluations on benchmarks covering\ninstruction following, preference alignment, safety, and mathematics reveal\nthat TPO progressively improves alignment with human preferences. Notably,\nafter only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can\nsurpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO\nscales efficiently with both the search width and depth during inference.\nThrough case studies, we illustrate how TPO exploits the innate capacity of LLM\nto interpret and act upon reward signals. Our findings establish TPO as a\npractical, lightweight alternative for test-time preference optimization,\nachieving alignment on the fly. Our code is publicly available at\nhttps://github.com/yafuly/TPO.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8868\u73fe\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u9748\u6d3b\u6027\uff0c\u7121\u6cd5\u5728\u4e0d\u91cd\u65b0\u8a13\u7df4\u7684\u60c5\u6cc1\u4e0b\u5feb\u901f\u9069\u61c9\u4eba\u985e\u504f\u597d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u6e2c\u8a66\u6642\u504f\u597d\u6700\u4f73\u5316 (TPO)\uff0c\u9019\u662f\u4e00\u500b\u5728\u63a8\u7406\u671f\u9593\u5c07 LLM \u8f38\u51fa\u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u7684\u6846\u67b6\uff0c\u6d88\u9664\u4e86\u66f4\u65b0\u6a21\u578b\u53c3\u6578\u7684\u9700\u8981\u3002TPO \u6c92\u6709\u4f9d\u8cf4\u7d14\u7cb9\u7684\u6578\u5b57\u734e\u52f5\uff0c\u800c\u662f\u5c07\u734e\u52f5\u8a0a\u865f\u8f49\u63db\u70ba\u6587\u5b57\u6279\u8a55\uff0c\u4e26\u5c07\u5b83\u5011\u4f5c\u70ba\u6587\u5b57\u734e\u52f5\u4f86\u53cd\u8986\u6539\u5584\u5176\u56de\u61c9\u3002\u5728\u6db5\u84cb\u6307\u4ee4\u9075\u5faa\u3001\u504f\u597d\u5c0d\u9f4a\u3001\u5b89\u5168\u6027\u4ee5\u53ca\u6578\u5b78\u7684\u57fa\u6e96\u6e2c\u8a66\u8a55\u4f30\u4e2d\u767c\u73fe\uff0cTPO \u9010\u6f38\u6539\u5584\u4e86\u8207\u4eba\u985e\u504f\u597d\u7684\u5c0d\u9f4a\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u50c5\u7d93\u904e\u5e7e\u500b TPO \u6b65\u9a5f\u5f8c\uff0c\u6700\u521d\u672a\u5c0d\u9f4a\u7684 Llama-3.1-70B-SFT \u6a21\u578b\u5c31\u53ef\u4ee5\u8d85\u8d8a\u5c0d\u9f4a\u7684\u5c0d\u61c9\u6a21\u578b Llama-3.1-70B-Instruct\u3002\u6b64\u5916\uff0cTPO \u5728\u63a8\u7406\u671f\u9593\u53ef\u4ee5\u6709\u6548\u5730\u64f4\u5145\u641c\u5c0b\u5ee3\u5ea6\u548c\u6df1\u5ea6\u3002\u900f\u904e\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u5011\u8aaa\u660e\u4e86 TPO \u5982\u4f55\u5229\u7528 LLM \u89e3\u91cb\u548c\u6839\u64da\u734e\u52f5\u8a0a\u865f\u63a1\u53d6\u884c\u52d5\u7684\u5167\u5728\u80fd\u529b\u3002\u6211\u5011\u7684\u767c\u73fe\u5c07 TPO \u5b9a\u4f4d\u70ba\u4e00\u7a2e\u5be6\u7528\u7684\u3001\u8f15\u91cf\u7d1a\u7684\u6e2c\u8a66\u6642\u504f\u597d\u6700\u4f73\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u5373\u6642\u5be6\u73fe\u5c0d\u9f4a\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u516c\u958b\u65bc https://github.com/yafuly/TPO\u3002", "author": "Yafu Li et.al.", "authors": "Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, Yu Cheng", "id": "2501.12895v1", "paper_url": "http://arxiv.org/abs/2501.12895v1", "repo": "https://github.com/yafuly/tpo"}}