{"2412.17383": {"publish_time": "2024-12-23", "title": "Interweaving Memories of a Siamese Large Language Model", "paper_summary": "Parameter-efficient fine-tuning (PEFT) methods optimize large language models\n(LLMs) by modifying or introducing a small number of parameters to enhance\nalignment with downstream tasks. However, they can result in catastrophic\nforgetting, where LLMs prioritize new knowledge at the expense of comprehensive\nworld knowledge. A promising approach to mitigate this issue is to recall prior\nmemories based on the original knowledge. To this end, we propose a\nmodel-agnostic PEFT framework, IMSM, which Interweaves Memories of a Siamese\nLarge Language Model. Specifically, our siamese LLM is equipped with an\nexisting PEFT method. Given an incoming query, it generates two distinct\nmemories based on the pre-trained and fine-tuned parameters. IMSM then\nincorporates an interweaving mechanism that regulates the contributions of both\noriginal and enhanced memories when generating the next token. This framework\nis theoretically applicable to all open-source LLMs and existing PEFT methods.\nWe conduct extensive experiments across various benchmark datasets, evaluating\nthe performance of popular open-source LLMs using the proposed IMSM, in\ncomparison to both classical and leading PEFT methods. Our findings indicate\nthat IMSM maintains comparable time and space efficiency to backbone PEFT\nmethods while significantly improving performance and effectively mitigating\ncatastrophic forgetting.", "paper_summary_zh": "\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\u900f\u904e\u4fee\u6539\u6216\u5f15\u5165\u5c11\u6578\u53c3\u6578\u4f86\u6700\u4f73\u5316\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5\u589e\u5f37\u8207\u4e0b\u6e38\u4efb\u52d9\u7684\u5c0d\u9f4a\u3002\u7136\u800c\uff0c\u5b83\u5011\u53ef\u80fd\u6703\u5c0e\u81f4\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u5176\u4e2d LLM \u4ee5\u72a7\u7272\u5168\u9762\u4e16\u754c\u77e5\u8b58\u70ba\u4ee3\u50f9\u512a\u5148\u8003\u616e\u65b0\u77e5\u8b58\u3002\u6e1b\u8f15\u6b64\u554f\u984c\u7684\u4e00\u500b\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\u662f\u6839\u64da\u539f\u59cb\u77e5\u8b58\u56de\u60f3\u8d77\u5148\u524d\u7684\u8a18\u61b6\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u8207\u6a21\u578b\u7121\u95dc\u7684 PEFT \u6846\u67b6 IMSM\uff0c\u5b83\u4ea4\u7e54\u4e86\u9023\u9ad4\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u8a18\u61b6\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u9023\u9ad4 LLM \u914d\u5099\u4e86\u73fe\u6709\u7684 PEFT \u65b9\u6cd5\u3002\u7d66\u5b9a\u4e00\u500b\u8f38\u5165\u67e5\u8a62\uff0c\u5b83\u6703\u6839\u64da\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u53c3\u6578\u7522\u751f\u5169\u500b\u4e0d\u540c\u7684\u8a18\u61b6\u3002\u7136\u5f8c\uff0cIMSM \u6703\u7d0d\u5165\u4e00\u500b\u4ea4\u7e54\u6a5f\u5236\uff0c\u5728\u7522\u751f\u4e0b\u4e00\u500b\u6a19\u8a18\u6642\u8abf\u7bc0\u539f\u59cb\u8a18\u61b6\u548c\u589e\u5f37\u8a18\u61b6\u7684\u8ca2\u737b\u3002\u6b64\u6846\u67b6\u5728\u7406\u8ad6\u4e0a\u9069\u7528\u65bc\u6240\u6709\u958b\u6e90 LLM \u548c\u73fe\u6709\u7684 PEFT \u65b9\u6cd5\u3002\u6211\u5011\u5c0d\u5404\u7a2e\u57fa\u6e96\u6578\u64da\u96c6\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4f7f\u7528\u63d0\u8b70\u7684 IMSM \u8a55\u4f30\u4e86\u4f7f\u7528\u6d41\u884c\u958b\u6e90 LLM \u7684\u6548\u80fd\uff0c\u4e26\u8207\u50b3\u7d71\u548c\u9818\u5148\u7684 PEFT \u65b9\u6cd5\u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0cIMSM \u5728\u4fdd\u6301\u8207\u4e3b\u5e79 PEFT \u65b9\u6cd5\u76f8\u7576\u7684\u6642\u9593\u548c\u7a7a\u9593\u6548\u7387\u7684\u540c\u6642\uff0c\u986f\u8457\u63d0\u5347\u4e86\u6548\u80fd\uff0c\u4e26\u6709\u6548\u6e1b\u8f15\u4e86\u707d\u96e3\u6027\u907a\u5fd8\u3002", "author": "Xin Song et.al.", "authors": "Xin Song, Zhikai Xue, Guoxiu He, Jiawei Liu, Wei Lu", "id": "2412.17383v1", "paper_url": "http://arxiv.org/abs/2412.17383v1", "repo": "https://github.com/ecnu-text-computing/imsm"}}