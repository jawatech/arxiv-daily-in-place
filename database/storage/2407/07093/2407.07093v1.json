{"2407.07093": {"publish_time": "2024-07-09", "title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation", "paper_summary": "This work presents a Fully BInarized Large Language Model (FBI-LLM),\ndemonstrating for the first time how to train a large-scale binary language\nmodel from scratch (not the partial binary or ternary LLM like BitNet b1.58) to\nmatch the performance of its full-precision counterparts (e.g., FP16 or BF16)\nin transformer-based LLMs. It achieves this by employing an autoregressive\ndistillation (AD) loss with maintaining equivalent model dimensions (130M,\n1.3B, 7B) and training data volume as regular LLM pretraining, while delivering\ncompetitive results in terms of perplexity and task-specific effectiveness.\nIntriguingly, by analyzing the training trajectory, we find that the pretrained\nweight is not necessary for training binarized LLMs from scratch. This research\nencourages a new computational framework and may facilitate the future design\nof specialized hardware tailored for fully 1-bit LLMs. We make all models,\ncode, and training dataset fully accessible and transparent to support further\nresearch (Code: https://github.com/LiqunMa/FBI-LLM. Model:\nhttps://huggingface.co/LiqunMa/).", "paper_summary_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u500b\u5168\u4e8c\u503c\u5316\u5927\u578b\u8a9e\u8a00\u6a21\u578b (FBI-LLM)\uff0c\u9996\u6b21\u5c55\u793a\u5982\u4f55\u5f9e\u982d\u8a13\u7df4\u4e00\u500b\u5927\u578b\u4e8c\u503c\u8a9e\u8a00\u6a21\u578b\uff08\u4e0d\u662f\u50cf BitNet b1.58 \u90a3\u6a23\u7684\u5c40\u90e8\u4e8c\u503c\u6216\u4e09\u503c LLM\uff09\uff0c\u4ee5\u5339\u914d\u5176\u5168\u7cbe\u5ea6\u5c0d\u61c9\u9805\uff08\u4f8b\u5982\uff0cFP16 \u6216 BF16\uff09\u5728\u57fa\u65bcTransformer\u7684 LLM \u4e2d\u7684\u6027\u80fd\u3002\u5b83\u901a\u904e\u63a1\u7528\u81ea\u8ff4\u6b78\u84b8\u993e (AD) \u640d\u5931\u4f86\u5be6\u73fe\u9019\u4e00\u9ede\uff0c\u540c\u6642\u4fdd\u6301\u7b49\u6548\u7684\u6a21\u578b\u7dad\u5ea6\uff08130M\u30011.3B\u30017B\uff09\u548c\u8a13\u7df4\u6578\u64da\u91cf\u4f5c\u70ba\u5e38\u898f LLM \u9810\u8a13\u7df4\uff0c\u540c\u6642\u5728\u56f0\u60d1\u5ea6\u548c\u7279\u5b9a\u4efb\u52d9\u7684\u6709\u6548\u6027\u65b9\u9762\u63d0\u4f9b\u6709\u7af6\u722d\u529b\u7684\u7d50\u679c\u3002\u6709\u8da3\u7684\u662f\uff0c\u901a\u904e\u5206\u6790\u8a13\u7df4\u8ecc\u8de1\uff0c\u6211\u5011\u767c\u73fe\u9810\u8a13\u7df4\u6b0a\u91cd\u5c0d\u65bc\u5f9e\u982d\u8a13\u7df4\u4e8c\u503c\u5316 LLM \u4e26\u975e\u5fc5\u8981\u3002\u9019\u9805\u7814\u7a76\u9f13\u52f5\u65b0\u7684\u8a08\u7b97\u6846\u67b6\uff0c\u4e26\u53ef\u80fd\u4fc3\u9032\u5c08\u9580\u91dd\u5c0d\u5168 1 \u4f4d\u5143 LLM \u91cf\u8eab\u6253\u9020\u7684\u786c\u9ad4\u7684\u672a\u4f86\u8a2d\u8a08\u3002\u6211\u5011\u8b93\u6240\u6709\u6a21\u578b\u3001\u7a0b\u5f0f\u78bc\u548c\u8a13\u7df4\u8cc7\u6599\u96c6\u5b8c\u5168\u516c\u958b\u4e14\u900f\u660e\uff0c\u4ee5\u652f\u6301\u9032\u4e00\u6b65\u7684\u7814\u7a76\uff08\u7a0b\u5f0f\u78bc\uff1ahttps://github.com/LiqunMa/FBI-LLM\u3002\u6a21\u578b\uff1ahttps://huggingface.co/LiqunMa/\uff09\u3002", "author": "Liqun Ma et.al.", "authors": "Liqun Ma, Mingjie Sun, Zhiqiang Shen", "id": "2407.07093v1", "paper_url": "http://arxiv.org/abs/2407.07093v1", "repo": "https://github.com/liqunma/fbi-llm"}}