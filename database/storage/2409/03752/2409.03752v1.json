{"2409.03752": {"publish_time": "2024-09-05", "title": "Attention Heads of Large Language Models: A Survey", "paper_summary": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain largely as black-box systems. Consequently, their\ndevelopment relies heavily on data-driven approaches, limiting performance\nenhancement through changes in internal architecture and reasoning pathways. As\na result, many researchers have begun exploring the potential internal\nmechanisms of LLMs, aiming to identify the essence of their reasoning\nbottlenecks, with most studies focusing on attention heads. Our survey aims to\nshed light on the internal reasoning processes of LLMs by concentrating on the\ninterpretability and underlying mechanisms of attention heads. We first distill\nthe human thought process into a four-stage framework: Knowledge Recalling,\nIn-Context Identification, Latent Reasoning, and Expression Preparation. Using\nthis framework, we systematically review existing research to identify and\ncategorize the functions of specific attention heads. Furthermore, we summarize\nthe experimental methodologies used to discover these special heads, dividing\nthem into two categories: Modeling-Free methods and Modeling-Required methods.\nAlso, we outline relevant evaluation methods and benchmarks. Finally, we\ndiscuss the limitations of current research and propose several potential\nfuture directions. Our reference list is open-sourced at\n\\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}.", "paper_summary_zh": "\u81ea ChatGPT \u554f\u4e16\u4ee5\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u4ecd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u9ed1\u7bb1\u7cfb\u7d71\u3002\u56e0\u6b64\uff0c\u5b83\u5011\u7684\u767c\u5c55\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8cf4\u65bc\u6578\u64da\u9a45\u52d5\u7684\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u901a\u904e\u5167\u90e8\u67b6\u69cb\u548c\u63a8\u7406\u8def\u5f91\u7684\u6539\u8b8a\u4f86\u63d0\u5347\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u8a31\u591a\u7814\u7a76\u4eba\u54e1\u958b\u59cb\u63a2\u7d22 LLM \u7684\u6f5b\u5728\u5167\u90e8\u6a5f\u5236\uff0c\u65e8\u5728\u627e\u51fa\u5176\u63a8\u7406\u74f6\u9838\u7684\u672c\u8cea\uff0c\u5927\u591a\u6578\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u6ce8\u610f\u529b\u982d\u90e8\u3002\u6211\u5011\u7684\u8abf\u67e5\u65e8\u5728\u901a\u904e\u5c08\u6ce8\u65bc\u6ce8\u610f\u529b\u982d\u90e8\u7684\u53ef\u89e3\u91cb\u6027\u548c\u5e95\u5c64\u6a5f\u5236\uff0c\u95e1\u660e LLM \u7684\u5167\u90e8\u63a8\u7406\u904e\u7a0b\u3002\u6211\u5011\u9996\u5148\u5c07\u4eba\u985e\u7684\u601d\u7dad\u904e\u7a0b\u63d0\u7149\u6210\u4e00\u500b\u56db\u968e\u6bb5\u6846\u67b6\uff1a\u77e5\u8b58\u56de\u61b6\u3001\u8a9e\u5883\u8b58\u5225\u3001\u6f5b\u5728\u63a8\u7406\u548c\u8868\u9054\u6e96\u5099\u3002\u4f7f\u7528\u9019\u500b\u6846\u67b6\uff0c\u6211\u5011\u7cfb\u7d71\u5730\u56de\u9867\u73fe\u6709\u7814\u7a76\uff0c\u4ee5\u8b58\u5225\u548c\u5206\u985e\u7279\u5b9a\u6ce8\u610f\u529b\u982d\u90e8\u7684\u529f\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u7e3d\u7d50\u4e86\u7528\u65bc\u767c\u73fe\u9019\u4e9b\u7279\u6b8a\u982d\u90e8\u7684\u5be6\u9a57\u65b9\u6cd5\uff0c\u5c07\u5b83\u5011\u5206\u70ba\u5169\u985e\uff1a\u7121\u5efa\u6a21\u65b9\u6cd5\u548c\u9700\u8981\u5efa\u6a21\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u5011\u6982\u8ff0\u4e86\u76f8\u95dc\u7684\u8a55\u4f30\u65b9\u6cd5\u548c\u57fa\u6e96\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u7576\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u4e26\u63d0\u51fa\u4e86\u5e7e\u500b\u6f5b\u5728\u7684\u672a\u4f86\u65b9\u5411\u3002\u6211\u5011\u7684\u53c3\u8003\u6e05\u55ae\u5728 \\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads} \u958b\u6e90\u3002", "author": "Zifan Zheng et.al.", "authors": "Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Bo Tang, Feiyu Xiong, Zhiyu Li", "id": "2409.03752v1", "paper_url": "http://arxiv.org/abs/2409.03752v1", "repo": "https://github.com/iaar-shanghai/awesome-attention-heads"}}