{"2409.07966": {"publish_time": "2024-09-12", "title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE", "paper_summary": "Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).", "paper_summary_zh": "<paragraph>\u97f3\u9891\u9a71\u52a8\u7684 3D \u9762\u90e8\u52a8\u753b\u5408\u6210\u4e00\u76f4\u662f\u5b66\u672f\u754c\u548c\u4ea7\u4e1a\u754c\u5173\u6ce8\u7684\u4e00\u4e2a\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\u3002\u867d\u7136\u8be5\u9886\u57df\u6709\u4ee4\u4eba\u632f\u594b\u7684\u7814\u7a76\u6210\u679c\uff0c\u4f46\u6700\u8fd1\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5507\u5f62\u540c\u6b65\u548c\u8eab\u4efd\u63a7\u5236\u4e0a\uff0c\u5ffd\u7565\u4e86\u60c5\u611f\u548c\u60c5\u611f\u63a7\u5236\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u7f3a\u4e4f\u60c5\u611f\u4e30\u5bcc\u7684\u9762\u90e8\u52a8\u753b\u6570\u636e\u548c\u80fd\u591f\u540c\u65f6\u5408\u6210\u5177\u6709\u60c5\u611f\u8868\u8fbe\u7684\u8bed\u97f3\u52a8\u753b\u7684\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u6a21\u578b\u90fd\u662f\u786e\u5b9a\u6027\u7684\uff0c\u8fd9\u610f\u5473\u7740\u7ed9\u5b9a\u76f8\u540c\u7684\u97f3\u9891\u8f93\u5165\uff0c\u5b83\u4eec\u4f1a\u4ea7\u751f\u76f8\u540c\u7684\u8f93\u51fa\u52a8\u4f5c\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u60c5\u611f\u548c\u975e\u786e\u5b9a\u6027\u5bf9\u4e8e\u751f\u6210\u591a\u6837\u5316\u4e14\u60c5\u611f\u4e30\u5bcc\u7684\u9762\u90e8\u52a8\u753b\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ProbTalk3D\uff0c\u8fd9\u662f\u4e00\u79cd\u975e\u786e\u5b9a\u6027\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f7f\u7528\u4e24\u9636\u6bb5 VQ-VAE \u6a21\u578b\u548c\u60c5\u611f\u4e30\u5bcc\u7684\u9762\u90e8\u52a8\u753b\u6570\u636e\u96c6 3DMEAD \u8fdb\u884c\u60c5\u611f\u53ef\u63a7\u7684\u8bed\u97f3\u9a71\u52a8\u7684 3D \u9762\u90e8\u52a8\u753b\u5408\u6210\u3002\u6211\u4eec\u901a\u8fc7\u5ba2\u89c2\u3001\u5b9a\u6027\u548c\u611f\u77e5\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u7ed3\u679c\uff0c\u5bf9\u6211\u4eec\u7684\u6a21\u578b\u4e0e\u6700\u8fd1\u7684 3D \u9762\u90e8\u52a8\u753b\u5408\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6bd4\u8f83\u5206\u6790\u3002\u6211\u4eec\u91cd\u70b9\u4ecb\u7ecd\u4e86\u51e0\u4e2a\u66f4\u9002\u5408\u8bc4\u4f30\u968f\u673a\u8f93\u51fa\u7684\u76ee\u6807\u6307\u6807\uff0c\u5e76\u5728\u4e3b\u89c2\u8bc4\u4f30\u4e2d\u4f7f\u7528\u4e86\u91ce\u5916\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u79cd\u975e\u786e\u5b9a\u6027 3D \u9762\u90e8\u52a8\u753b\u5408\u6210\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u4e30\u5bcc\u7684\u8868\u60c5\u6570\u636e\u96c6\u548c\u5e26\u6709\u8868\u60c5\u6807\u7b7e\u548c\u5f3a\u5ea6\u7b49\u7ea7\u7684\u8868\u60c5\u63a7\u5236\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u60c5\u611f\u63a7\u5236\u3001\u786e\u5b9a\u6027\u548c\u975e\u786e\u5b9a\u6027\u6a21\u578b\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6211\u4eec\u5efa\u8bae\u89c2\u770b\u8865\u5145\u89c6\u9891\u4ee5\u8fdb\u884c\u8d28\u91cf\u5224\u65ad\u3002\u6574\u4e2a\u4ee3\u7801\u5e93\u5df2\u516c\u5f00\uff08https://github.com/uuembodiedsocialai/ProbTalk3D/\uff09\u3002</paragraph>", "author": "Sichun Wu et.al.", "authors": "Sichun Wu, Kazi Injamamul Haque, Zerrin Yumak", "id": "2409.07966v1", "paper_url": "http://arxiv.org/abs/2409.07966v1", "repo": "https://github.com/uuembodiedsocialai/probtalk3d"}}