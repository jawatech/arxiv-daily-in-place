{"2405.05615": {"publish_time": "2024-05-09", "title": "Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning", "paper_summary": "Current solutions for efficiently constructing large vision-language (VL)\nmodels follow a two-step paradigm: projecting the output of pre-trained vision\nencoders to the input space of pre-trained language models as visual prompts;\nand then transferring the models to downstream VL tasks via end-to-end\nparameter-efficient fine-tuning (PEFT). However, this paradigm still exhibits\ninefficiency since it significantly increases the input length of the language\nmodels. In this paper, in contrast to integrating visual prompts into inputs,\nwe regard visual prompts as additional knowledge that facilitates language\nmodels in addressing tasks associated with visual information. Motivated by the\nfinding that Feed-Forward Network (FFN) of language models acts as \"key-value\nmemory\", we introduce a novel approach termed memory-space visual prompting\n(MemVP), wherein visual prompts are concatenated with the weights of FFN for\nvisual knowledge injection. Experimental results across various VL tasks and\nlanguage models reveal that MemVP significantly reduces the training time and\ninference latency of the finetuned VL models and surpasses the performance of\nprevious PEFT methods. Code: https://github.com/JieShibo/MemVP", "paper_summary_zh": "\u73fe\u4eca\u6709\u6548\u5efa\u69cb\u5927\u578b\u8996\u89ba\u8a9e\u8a00 (VL) \u6a21\u578b\u7684\u89e3\u6c7a\u65b9\u6848\u9075\u5faa\u5169\u968e\u6bb5\u7bc4\u4f8b\uff1a\u5c07\u9810\u5148\u8a13\u7df4\u7684\u8996\u89ba\u7de8\u78bc\u5668\u8f38\u51fa\u6295\u5f71\u5230\u9810\u5148\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u7684\u8f38\u5165\u7a7a\u9593\uff0c\u4f5c\u70ba\u8996\u89ba\u63d0\u793a\uff1b\u7136\u5f8c\u900f\u904e\u7aef\u5c0d\u7aef\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u5c07\u6a21\u578b\u8f49\u79fb\u5230\u4e0b\u6e38 VL \u4efb\u52d9\u3002\u7136\u800c\uff0c\u6b64\u7bc4\u4f8b\u4ecd\u5c55\u73fe\u51fa\u4f4e\u6548\u7387\uff0c\u56e0\u70ba\u5b83\u986f\u8457\u589e\u52a0\u4e86\u8a9e\u8a00\u6a21\u578b\u7684\u8f38\u5165\u9577\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u8207\u5c07\u8996\u89ba\u63d0\u793a\u6574\u5408\u5230\u8f38\u5165\u4e2d\u76f8\u53cd\uff0c\u6211\u5011\u5c07\u8996\u89ba\u63d0\u793a\u8996\u70ba\u984d\u5916\u7684\u77e5\u8b58\uff0c\u6709\u52a9\u65bc\u8a9e\u8a00\u6a21\u578b\u8655\u7406\u8207\u8996\u89ba\u8cc7\u8a0a\u76f8\u95dc\u7684\u4efb\u52d9\u3002\u5728 Feed-Forward \u7db2\u8def (FFN) \u7684\u8a9e\u8a00\u6a21\u578b\u5145\u7576\u300c\u9375\u503c\u8a18\u61b6\u9ad4\u300d\u7684\u767c\u73fe\u6fc0\u52f5\u4e0b\uff0c\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u7a31\u70ba\u8a18\u61b6\u7a7a\u9593\u8996\u89ba\u63d0\u793a (MemVP) \u7684\u65b0\u65b9\u6cd5\uff0c\u5176\u4e2d\u8996\u89ba\u63d0\u793a\u8207 FFN \u7684\u6b0a\u91cd\u4e32\u63a5\uff0c\u4ee5\u9032\u884c\u8996\u89ba\u77e5\u8b58\u6ce8\u5165\u3002\u8de8\u5404\u7a2e VL \u4efb\u52d9\u548c\u8a9e\u8a00\u6a21\u578b\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cMemVP \u5927\u5e45\u6e1b\u5c11\u5fae\u8abf VL \u6a21\u578b\u7684\u8a13\u7df4\u6642\u9593\u548c\u63a8\u8ad6\u5ef6\u9072\uff0c\u4e26\u8d85\u8d8a\u5148\u524d\u7684 PEFT \u65b9\u6cd5\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\uff1ahttps://github.com/JieShibo/MemVP", "author": "Shibo Jie et.al.", "authors": "Shibo Jie, Yehui Tang, Ning Ding, Zhi-Hong Deng, Kai Han, Yunhe Wang", "id": "2405.05615v1", "paper_url": "http://arxiv.org/abs/2405.05615v1", "repo": "null"}}