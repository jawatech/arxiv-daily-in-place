{"2409.15790": {"publish_time": "2024-09-24", "title": "Small Language Models: Survey, Measurements, and Insights", "paper_summary": "Small language models (SLMs), despite their widespread adoption in modern\nsmart devices, have received significantly less academic attention compared to\ntheir large language model (LLM) counterparts, which are predominantly deployed\nin data centers and cloud environments. While researchers continue to improve\nthe capabilities of LLMs in the pursuit of artificial general intelligence, SLM\nresearch aims to make machine intelligence more accessible, affordable, and\nefficient for everyday tasks. Focusing on transformer-based, decoder-only\nlanguage models with 100M-5B parameters, we survey 59 state-of-the-art\nopen-source SLMs, analyzing their technical innovations across three axes:\narchitectures, training datasets, and training algorithms. In addition, we\nevaluate their capabilities in various domains, including commonsense\nreasoning, in-context learning, mathematics, and coding. To gain further\ninsight into their on-device runtime costs, we benchmark their inference\nlatency and memory footprints. Through in-depth analysis of our benchmarking\ndata, we offer valuable insights to advance research in this field.", "paper_summary_zh": "\u5118\u7ba1\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM) \u5df2\u5ee3\u6cdb\u904b\u7528\u65bc\u73fe\u4ee3\u667a\u6167\u88dd\u7f6e\uff0c\u4f46\u8207\u4e3b\u8981\u90e8\u7f72\u65bc\u8cc7\u6599\u4e2d\u5fc3\u548c\u96f2\u7aef\u74b0\u5883\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u76f8\u6bd4\uff0c\u5b78\u8853\u754c\u5c0d\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u95dc\u6ce8\u537b\u5c11\u5f97\u591a\u3002\u5118\u7ba1\u7814\u7a76\u4eba\u54e1\u6301\u7e8c\u63d0\u5347 LLM \u7684\u529f\u80fd\uff0c\u4ee5\u8ffd\u6c42\u4eba\u5de5\u901a\u7528\u667a\u6167\uff0c\u4f46 SLM \u7814\u7a76\u7684\u76ee\u6a19\u662f\u8b93\u6a5f\u5668\u667a\u6167\u66f4\u5e73\u6613\u8fd1\u4eba\u3001\u66f4\u5be6\u60e0\uff0c\u4e14\u66f4\u6709\u6548\u7387\u5730\u57f7\u884c\u65e5\u5e38\u4efb\u52d9\u3002\u6211\u5011\u91dd\u5c0d\u5177\u5099 100M-5B \u53c3\u6578\u7684\u57fa\u65bc\u8f49\u63db\u5668\u3001\u50c5\u89e3\u78bc\u5668\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u8abf\u67e5\u4e86 59 \u500b\u6700\u5148\u9032\u7684\u958b\u6e90 SLM\uff0c\u5206\u6790\u5b83\u5011\u5728\u67b6\u69cb\u3001\u8a13\u7df4\u8cc7\u6599\u96c6\u548c\u8a13\u7df4\u6f14\u7b97\u6cd5\u9019\u4e09\u500b\u9762\u5411\u4e0a\u7684\u6280\u8853\u5275\u65b0\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u5b83\u5011\u5728\u5404\u7a2e\u9818\u57df\u7684\u80fd\u529b\uff0c\u5305\u62ec\u5e38\u8b58\u63a8\u7406\u3001\u60c5\u5883\u5b78\u7fd2\u3001\u6578\u5b78\u548c\u7de8\u78bc\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u4e86\u89e3\u5b83\u5011\u5728\u88dd\u7f6e\u4e0a\u7684\u57f7\u884c\u6642\u9593\u6210\u672c\uff0c\u6211\u5011\u5c0d\u5b83\u5011\u7684\u63a8\u8ad6\u5ef6\u9072\u548c\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u9032\u884c\u4e86\u57fa\u6e96\u6e2c\u8a66\u3002\u900f\u904e\u5c0d\u57fa\u6e96\u6e2c\u8a66\u8cc7\u6599\u7684\u6df1\u5165\u5206\u6790\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\uff0c\u4ee5\u63a8\u52d5\u6b64\u9818\u57df\u7684\u7814\u7a76\u3002", "author": "Zhenyan Lu et.al.", "authors": "Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu", "id": "2409.15790v1", "paper_url": "http://arxiv.org/abs/2409.15790v1", "repo": "null"}}