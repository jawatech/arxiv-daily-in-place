{"2412.03187": {"publish_time": "2024-12-04", "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion", "paper_summary": "While fusing heterogeneous open-source LLMs with varying architectures and\nsizes can potentially integrate the strengths of different models, existing\nfusion methods face significant challenges, such as vocabulary alignment and\nmerging distribution matrices. These procedures are not only complex but also\nprone to introducing noise and errors. In this paper, we propose an implicit\nfusion method, Weighted-Reward Preference Optimization (WRPO), which leverages\npreference optimization between the source LLMs and the target LLM to transfer\ntheir capabilities effectively. WRPO eliminates the need for vocabulary\nalignment and matrix fusion and can be efficiently scaled to accommodate\nvarious LLMs. To address distributional deviations between the source and\ntarget LLMs, WRPO introduces a progressive adaptation strategy that gradually\nshifts reliance on preferred examples from the target LLM to the source LLMs.\nExtensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks\ndemonstrate that WRPO consistently outperforms existing knowledge fusion\nmethods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct\nas the target model, WRPO achieves a length-controlled win rate of 55.9%\nagainst GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against\nGPT-4-0314 on Arena-Hard. Our code is available at\n\\url{https://github.com/SLIT-AI/WRPO}.", "paper_summary_zh": "<paragraph>\u96d6\u7136\u878d\u5408\u7570\u8cea\u958b\u653e\u539f\u59cb\u78bc LLM\uff0c\u5176\u67b6\u69cb\u548c\u898f\u6a21\u5404\u7570\uff0c\u6709\u6574\u5408\u4e0d\u540c\u6a21\u578b\u512a\u52e2\u7684\u6f5b\u529b\uff0c\u73fe\u6709\u7684\u878d\u5408\u65b9\u6cd5\u537b\u9762\u81e8\u8af8\u591a\u6311\u6230\uff0c\u4f8b\u5982\u8a5e\u5f59\u6bd4\u5c0d\u548c\u5408\u4f75\u5206\u4f48\u77e9\u9663\u3002\u9019\u4e9b\u7a0b\u5e8f\u4e0d\u50c5\u8907\u96dc\uff0c\u9084\u5bb9\u6613\u5f15\u5165\u96dc\u8a0a\u548c\u932f\u8aa4\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u96b1\u5f0f\u878d\u5408\u65b9\u6cd5\uff0c\u5373\u52a0\u6b0a\u734e\u52f5\u504f\u597d\u6700\u4f73\u5316 (WRPO)\uff0c\u5b83\u5229\u7528\u539f\u59cb LLM \u548c\u76ee\u6a19 LLM \u4e4b\u9593\u7684\u504f\u597d\u6700\u4f73\u5316\u4f86\u6709\u6548\u8f49\u79fb\u5b83\u5011\u7684\u80fd\u529b\u3002WRPO \u6d88\u9664\u4e86\u8a5e\u5f59\u6bd4\u5c0d\u548c\u77e9\u9663\u878d\u5408\u7684\u9700\u8981\uff0c\u4e26\u4e14\u53ef\u4ee5\u6709\u6548\u64f4\u5c55\u4ee5\u5bb9\u7d0d\u5404\u7a2e LLM\u3002\u70ba\u4e86\u89e3\u6c7a\u539f\u59cb\u548c\u76ee\u6a19 LLM \u4e4b\u9593\u7684\u5206\u914d\u504f\u5dee\uff0cWRPO \u5f15\u5165\u4e86\u4e00\u7a2e\u6f38\u9032\u9069\u61c9\u7b56\u7565\uff0c\u9010\u6f38\u5c07\u5c0d\u76ee\u6a19 LLM \u7684\u504f\u597d\u7bc4\u4f8b\u7684\u4f9d\u8cf4\u8f49\u79fb\u5230\u539f\u59cb LLM\u3002\u5728 MT-Bench\u3001AlpacaEval-2 \u548c Arena-Hard \u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cWRPO \u6301\u7e8c\u512a\u65bc\u73fe\u6709\u7684\u77e5\u8b58\u878d\u5408\u65b9\u6cd5\u548c\u5404\u7a2e\u5fae\u8abf\u57fa\u6e96\u3002\u7576\u61c9\u7528\u65bc LLaMA3-8B-Instruct \u4f5c\u70ba\u76ee\u6a19\u6a21\u578b\u6642\uff0cWRPO \u5728 AlpacaEval-2 \u4e0a\u5c0d GPT-4-Preview-1106 \u9054\u5230\u4e86 55.9% \u7684\u9577\u5ea6\u63a7\u5236\u7372\u52dd\u7387\uff0c\u5728 Arena-Hard \u4e0a\u5c0d GPT-4-0314 \u9054\u5230\u4e86 46.2% \u7684\u7372\u52dd\u7387\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1a\\url{https://github.com/SLIT-AI/WRPO}\u3002</paragraph>", "author": "Ziyi Yang et.al.", "authors": "Ziyi Yang, Fanqi Wan, Longguang Zhong, Tianyuan Shi, Xiaojun Quan", "id": "2412.03187v1", "paper_url": "http://arxiv.org/abs/2412.03187v1", "repo": "https://github.com/SLIT-AI/WRPO"}}