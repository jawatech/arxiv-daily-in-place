{"2501.05147": {"publish_time": "2025-01-09", "title": "A Systematic Literature Review on Deep Learning-based Depth Estimation in Computer Vision", "paper_summary": "Depth estimation (DE) provides spatial information about a scene and enables\ntasks such as 3D reconstruction, object detection, and scene understanding.\nRecently, there has been an increasing interest in using deep learning\n(DL)-based methods for DE. Traditional techniques rely on handcrafted features\nthat often struggle to generalise to diverse scenes and require extensive\nmanual tuning. However, DL models for DE can automatically extract relevant\nfeatures from input data, adapt to various scene conditions, and generalise\nwell to unseen environments. Numerous DL-based methods have been developed,\nmaking it necessary to survey and synthesize the state-of-the-art (SOTA).\nPrevious reviews on DE have mainly focused on either monocular or stereo-based\ntechniques, rather than comprehensively reviewing DE. Furthermore, to the best\nof our knowledge, there is no systematic literature review (SLR) that\ncomprehensively focuses on DE. Therefore, this SLR study is being conducted.\nInitially, electronic databases were searched for relevant publications,\nresulting in 1284 publications. Using defined exclusion and quality criteria,\n128 publications were shortlisted and further filtered to select 59\nhigh-quality primary studies. These studies were analysed to extract data and\nanswer defined research questions. Based on the results, DL methods were\ndeveloped for mainly three different types of DE: monocular, stereo, and\nmulti-view. 20 publicly available datasets were used to train, test, and\nevaluate DL models for DE, with KITTI, NYU Depth V2, and Make 3D being the most\nused datasets. 29 evaluation metrics were used to assess the performance of DE.\n35 base models were reported in the primary studies, and the top five most-used\nbase models were ResNet-50, ResNet-18, ResNet-101, U-Net, and VGG-16. Finally,\nthe lack of ground truth data was among the most significant challenges\nreported by primary studies.", "paper_summary_zh": "\u6df1\u5ea6\u4f30\u8a08 (DE) \u63d0\u4f9b\u5834\u666f\u7684\u7a7a\u9593\u8cc7\u8a0a\uff0c\u4e26\u80fd\u57f7\u884c 3D \u91cd\u5efa\u3001\u7269\u9ad4\u5075\u6e2c\u548c\u5834\u666f\u7406\u89e3\u7b49\u4efb\u52d9\u3002\u6700\u8fd1\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b78\u7fd2 (DL) \u7684\u65b9\u6cd5\u9032\u884c DE \u9010\u6f38\u53d7\u5230\u91cd\u8996\u3002\u50b3\u7d71\u6280\u8853\u4ef0\u8cf4\u624b\u5de5\u7279\u5fb5\uff0c\u800c\u9019\u4e9b\u7279\u5fb5\u901a\u5e38\u96e3\u4ee5\u63a8\u5ee3\u5230\u4e0d\u540c\u7684\u5834\u666f\uff0c\u4e26\u4e14\u9700\u8981\u5ee3\u6cdb\u7684\u624b\u52d5\u8abf\u6574\u3002\u7136\u800c\uff0cDE \u7684 DL \u6a21\u578b\u53ef\u4ee5\u81ea\u52d5\u5f9e\u8f38\u5165\u8cc7\u6599\u4e2d\u8403\u53d6\u76f8\u95dc\u7279\u5fb5\u3001\u9069\u61c9\u5404\u7a2e\u5834\u666f\u689d\u4ef6\uff0c\u4e26\u80fd\u63a8\u5ee3\u5230\u672a\u77e5\u7684\u74b0\u5883\u3002\u5df2\u7d93\u958b\u767c\u51fa\u8a31\u591a\u57fa\u65bc DL \u7684\u65b9\u6cd5\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u8abf\u67e5\u548c\u7d9c\u5408\u73fe\u6709\u6280\u8853 (SOTA)\u3002\u5148\u524d\u95dc\u65bc DE \u7684\u56de\u9867\u4e3b\u8981\u5c08\u6ce8\u65bc\u55ae\u773c\u6216\u7acb\u9ad4\u6280\u8853\uff0c\u800c\u4e0d\u662f\u5168\u9762\u56de\u9867 DE\u3002\u6b64\u5916\uff0c\u64da\u6211\u5011\u6240\u77e5\uff0c\u6c92\u6709\u7cfb\u7d71\u6027\u7684\u6587\u737b\u56de\u9867 (SLR) \u5168\u9762\u95dc\u6ce8 DE\u3002\u56e0\u6b64\uff0c\u6b63\u5728\u9032\u884c\u9019\u9805 SLR \u7814\u7a76\u3002\u6700\u521d\uff0c\u5728\u96fb\u5b50\u8cc7\u6599\u5eab\u4e2d\u641c\u5c0b\u76f8\u95dc\u51fa\u7248\u54c1\uff0c\u5171\u5f97\u5230 1284 \u7bc7\u51fa\u7248\u54c1\u3002\u4f7f\u7528\u5b9a\u7fa9\u7684\u6392\u9664\u548c\u54c1\u8cea\u6a19\u6e96\uff0c\u5c07 128 \u7bc7\u51fa\u7248\u54c1\u5217\u5165\u5019\u9078\u540d\u55ae\uff0c\u4e26\u9032\u4e00\u6b65\u7be9\u9078\u51fa 59 \u9805\u9ad8\u54c1\u8cea\u7684\u4e3b\u8981\u7814\u7a76\u3002\u5206\u6790\u9019\u4e9b\u7814\u7a76\u4ee5\u8403\u53d6\u8cc7\u6599\u4e26\u56de\u7b54\u5b9a\u7fa9\u7684\u7814\u7a76\u554f\u984c\u3002\u6839\u64da\u7d50\u679c\uff0cDL \u65b9\u6cd5\u4e3b\u8981\u91dd\u5c0d\u4e09\u7a2e\u985e\u578b\u7684 DE \u9032\u884c\u958b\u767c\uff1a\u55ae\u773c\u3001\u7acb\u9ad4\u548c\u591a\u8996\u5716\u300220 \u500b\u516c\u958b\u53ef\u7528\u7684\u8cc7\u6599\u96c6\u7528\u65bc\u8a13\u7df4\u3001\u6e2c\u8a66\u548c\u8a55\u4f30 DE \u7684 DL \u6a21\u578b\uff0c\u5176\u4e2d KITTI\u3001NYU Depth V2 \u548c Make 3D \u662f\u6700\u5e38\u7528\u7684\u8cc7\u6599\u96c6\u300229 \u500b\u8a55\u4f30\u6307\u6a19\u7528\u65bc\u8a55\u4f30 DE \u7684\u6548\u80fd\u300235 \u500b\u57fa\u790e\u6a21\u578b\u5728\u4e3b\u8981\u7814\u7a76\u4e2d\u88ab\u5831\u5c0e\uff0c\u524d\u4e94\u500b\u6700\u5e38\u4f7f\u7528\u7684\u57fa\u790e\u6a21\u578b\u662f ResNet-50\u3001ResNet-18\u3001ResNet-101\u3001U-Net \u548c VGG-16\u3002\u6700\u5f8c\uff0c\u4e3b\u8981\u7814\u7a76\u5831\u544a\u7684\u91cd\u5927\u6311\u6230\u4e4b\u4e00\u662f\u7f3a\u4e4f\u771f\u5be6\u8cc7\u6599\u3002", "author": "Ali Rohan et.al.", "authors": "Ali Rohan, Md Junayed Hasan, Andrei Petrovski", "id": "2501.05147v1", "paper_url": "http://arxiv.org/abs/2501.05147v1", "repo": "null"}}