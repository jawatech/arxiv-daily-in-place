{"2501.12380": {"publish_time": "2025-01-21", "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding", "paper_summary": "We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark\nfor evaluating foundation models in video understanding. MMVU includes 3,000\nexpert-annotated questions spanning 27 subjects across four core disciplines:\nScience, Healthcare, Humanities & Social Sciences, and Engineering. Compared to\nprior benchmarks, MMVU features three key advancements. First, it challenges\nmodels to apply domain-specific knowledge and perform expert-level reasoning to\nanalyze specialized-domain videos, moving beyond the basic visual perception\ntypically assessed in current video benchmarks. Second, each example is\nannotated by human experts from scratch. We implement strict data quality\ncontrols to ensure the high quality of the dataset. Finally, each example is\nenriched with expert-annotated reasoning rationals and relevant domain\nknowledge, facilitating in-depth analysis. We conduct an extensive evaluation\nof 32 frontier multimodal foundation models on MMVU. The latest\nSystem-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest\nperformance among the tested models. However, they still fall short of matching\nhuman expertise. Through in-depth error analyses and case studies, we offer\nactionable insights for future advancements in expert-level,\nknowledge-intensive video understanding for specialized domains.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 MMVU\uff0c\u4e00\u500b\u5168\u9762\u7684\u5c08\u5bb6\u7d1a\u3001\u591a\u9818\u57df\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30\u5f71\u7247\u7406\u89e3\u4e2d\u7684\u57fa\u790e\u6a21\u578b\u3002MMVU \u5305\u542b 3,000 \u500b\u5c08\u5bb6\u8a3b\u89e3\u554f\u984c\uff0c\u6db5\u84cb\u56db\u500b\u6838\u5fc3\u9818\u57df\u7684 27 \u500b\u79d1\u76ee\uff1a\u79d1\u5b78\u3001\u91ab\u7642\u4fdd\u5065\u3001\u4eba\u6587\u8207\u793e\u6703\u79d1\u5b78\u4ee5\u53ca\u5de5\u7a0b\u3002\u8207\u5148\u524d\u7684\u57fa\u6e96\u76f8\u6bd4\uff0cMMVU \u5177\u5099\u4e09\u5927\u9032\u5c55\u3002\u9996\u5148\uff0c\u5b83\u6311\u6230\u6a21\u578b\u61c9\u7528\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\uff0c\u4e26\u57f7\u884c\u5c08\u5bb6\u7d1a\u63a8\u7406\uff0c\u4ee5\u5206\u6790\u7279\u5b9a\u9818\u57df\u7684\u5f71\u7247\uff0c\u8d85\u8d8a\u7576\u524d\u5f71\u7247\u57fa\u6e96\u4e2d\u901a\u5e38\u8a55\u4f30\u7684\u57fa\u672c\u8996\u89ba\u611f\u77e5\u3002\u5176\u6b21\uff0c\u6bcf\u500b\u7bc4\u4f8b\u90fd\u662f\u7531\u4eba\u985e\u5c08\u5bb6\u5f9e\u982d\u958b\u59cb\u8a3b\u89e3\u3002\u6211\u5011\u5be6\u65bd\u56b4\u683c\u7684\u8cc7\u6599\u54c1\u8cea\u63a7\u7ba1\uff0c\u4ee5\u78ba\u4fdd\u8cc7\u6599\u96c6\u7684\u9ad8\u54c1\u8cea\u3002\u6700\u5f8c\uff0c\u6bcf\u500b\u7bc4\u4f8b\u90fd\u8c50\u5bcc\u4e86\u5c08\u5bb6\u8a3b\u89e3\u7684\u63a8\u7406\u539f\u7406\u548c\u76f8\u95dc\u9818\u57df\u77e5\u8b58\uff0c\u4fc3\u9032\u6df1\u5165\u5206\u6790\u3002\u6211\u5011\u5c0d 32 \u500b\u524d\u6cbf\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u9032\u884c\u4e86\u5ee3\u6cdb\u7684 MMVU \u8a55\u4f30\u3002\u6700\u65b0\u7684 System-2 \u80fd\u529b\u6a21\u578b o1 \u548c Gemini 2.0 Flash Thinking \u5728\u6e2c\u8a66\u6a21\u578b\u4e2d\u7372\u5f97\u6700\u9ad8\u6548\u80fd\u3002\u7136\u800c\uff0c\u5b83\u5011\u4ecd\u7136\u7121\u6cd5\u8207\u4eba\u985e\u5c08\u696d\u77e5\u8b58\u76f8\u5339\u914d\u3002\u900f\u904e\u6df1\u5165\u7684\u932f\u8aa4\u5206\u6790\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u5011\u70ba\u7279\u5b9a\u9818\u57df\u7684\u5c08\u5bb6\u7d1a\u3001\u77e5\u8b58\u5bc6\u96c6\u578b\u5f71\u7247\u7406\u89e3\u7684\u672a\u4f86\u9032\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u898b\u89e3\u3002", "author": "Yilun Zhao et.al.", "authors": "Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, Arman Cohan", "id": "2501.12380v1", "paper_url": "http://arxiv.org/abs/2501.12380v1", "repo": "https://github.com/yale-nlp/mmvu"}}