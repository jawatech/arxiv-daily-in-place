{"2406.05963": {"publish_time": "2024-06-10", "title": "Solution for SMART-101 Challenge of CVPR Multi-modal Algorithmic Reasoning Task 2024", "paper_summary": "In this paper, the solution of HYU MLLAB KT Team to the Multimodal\nAlgorithmic Reasoning Task: SMART-101 CVPR 2024 Challenge is presented. Beyond\nconventional visual question-answering problems, the SMART-101 challenge aims\nto achieve human-level multimodal understanding by tackling complex\nvisio-linguistic puzzles designed for children in the 6-8 age group. To solve\nthis problem, we suggest two main ideas. First, to utilize the reasoning\nability of a large-scale language model (LLM), the given visual cues (images)\nare grounded in the text modality. For this purpose, we generate highly\ndetailed text captions that describe the context of the image and use these\ncaptions as input for the LLM. Second, due to the nature of puzzle images,\nwhich often contain various geometric visual patterns, we utilize an object\ndetection algorithm to ensure these patterns are not overlooked in the\ncaptioning process. We employed the SAM algorithm, which can detect\nvarious-size objects, to capture the visual features of these geometric\npatterns and used this information as input for the LLM. Under the puzzle split\nconfiguration, we achieved an option selection accuracy Oacc of 29.5 on the\ntest set and a weighted option selection accuracy (WOSA) of 27.1 on the\nchallenge set.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u4ecb\u7d39\u4e86 HYU MLLAB KT \u5718\u968a\u91dd\u5c0d\u591a\u6a21\u614b\u6f14\u7b97\u6cd5\u63a8\u7406\u4efb\u52d9\uff1aSMART-101 CVPR 2024 \u6311\u6230\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u9664\u4e86\u50b3\u7d71\u7684\u8996\u89ba\u554f\u7b54\u554f\u984c\u4e4b\u5916\uff0cSMART-101 \u6311\u6230\u65e8\u5728\u900f\u904e\u89e3\u6c7a\u5c08\u70ba 6-8 \u6b72\u5152\u7ae5\u8a2d\u8a08\u7684\u8907\u96dc\u8996\u89ba\u8a9e\u8a00\u8b0e\u984c\uff0c\u4f86\u9054\u6210\u4eba\u985e\u7b49\u7d1a\u7684\u591a\u6a21\u614b\u7406\u89e3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5169\u500b\u4e3b\u8981\u60f3\u6cd5\u3002\u9996\u5148\uff0c\u70ba\u4e86\u5229\u7528\u5927\u898f\u6a21\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\uff0c\u7d66\u5b9a\u7684\u8996\u89ba\u7dda\u7d22\uff08\u5f71\u50cf\uff09\u88ab\u57fa\u790e\u5316\u65bc\u6587\u5b57\u6a21\u5f0f\u4e2d\u3002\u70ba\u4e86\u9019\u500b\u76ee\u7684\uff0c\u6211\u5011\u7522\u751f\u4e86\u9ad8\u5ea6\u8a73\u7d30\u7684\u6587\u5b57\u6a19\u984c\uff0c\u4f86\u63cf\u8ff0\u5f71\u50cf\u7684\u5167\u5bb9\uff0c\u4e26\u5c07\u9019\u4e9b\u6a19\u984c\u7528\u4f5c LLM \u7684\u8f38\u5165\u3002\u5176\u6b21\uff0c\u7531\u65bc\u8b0e\u984c\u5f71\u50cf\u7684\u672c\u8cea\uff0c\u901a\u5e38\u5305\u542b\u5404\u7a2e\u5e7e\u4f55\u8996\u89ba\u6a21\u5f0f\uff0c\u6211\u5011\u5229\u7528\u7269\u4ef6\u5075\u6e2c\u6f14\u7b97\u6cd5\u4f86\u78ba\u4fdd\u9019\u4e9b\u6a21\u5f0f\u4e0d\u6703\u5728\u6a19\u984c\u8655\u7406\u904e\u7a0b\u4e2d\u88ab\u5ffd\u7565\u3002\u6211\u5011\u63a1\u7528\u4e86 SAM \u6f14\u7b97\u6cd5\uff0c\u5b83\u53ef\u4ee5\u5075\u6e2c\u5404\u7a2e\u5927\u5c0f\u7684\u7269\u4ef6\uff0c\u4f86\u64f7\u53d6\u9019\u4e9b\u5e7e\u4f55\u6a21\u5f0f\u7684\u8996\u89ba\u7279\u5fb5\uff0c\u4e26\u5c07\u6b64\u8cc7\u8a0a\u7528\u4f5c LLM \u7684\u8f38\u5165\u3002\u5728\u8b0e\u984c\u5206\u5272\u914d\u7f6e\u4e0b\uff0c\u6211\u5011\u5728\u6e2c\u8a66\u96c6\u4e0a\u9054\u5230\u4e86\u9078\u9805\u9078\u64c7\u6e96\u78ba\u5ea6 Oacc \u70ba 29.5\uff0c\u5728\u6311\u6230\u96c6\u4e0a\u9054\u5230\u4e86\u52a0\u6b0a\u9078\u9805\u9078\u64c7\u6e96\u78ba\u5ea6 (WOSA) \u70ba 27.1\u3002", "author": "Jinwoo Ahn et.al.", "authors": "Jinwoo Ahn, Junhyeok Park, Min-Jun Kim, Kang-Hyeon Kim, So-Yeong Sohn, Yun-Ji Lee, Du-Seong Chang, Yu-Jung Heo, Eun-Sol Kim", "id": "2406.05963v1", "paper_url": "http://arxiv.org/abs/2406.05963v1", "repo": "null"}}