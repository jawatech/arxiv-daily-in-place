{"2409.15825": {"publish_time": "2024-09-24", "title": "Empirical Insights on Fine-Tuning Large Language Models for Question-Answering", "paper_summary": "Large language models (LLMs) encode extensive world knowledge through\npre-training on massive datasets, which can then be fine-tuned for the\nquestion-answering (QA) task. However, effective strategies for fine-tuning\nLLMs for the QA task remain largely unexplored. To address this gap, we\ncategorize supervised fine-tuning (SFT) data based on the extent of knowledge\nmemorized by the pretrained LLMs and conduct a series of empirical analyses.\nOur experiments, involving four LLMs from three different model families, focus\non three key factors: the amount of data required for SFT, the impact of\ndifferent SFT datasets on model performance, and how data requirements vary\nacross LLMs. The results show that as few as 60 data points during the SFT\nstage can activate the knowledge encoded during pre-training, enabling LLMs to\nperform the QA task. Additionally, SFT with data of varying memory levels has a\nsignificant impact on LLM performance, with the optimal dataset differing based\non the specific model being fine-tuned. Future research will delve deeper into\nthe mechanisms underlying these phenomena.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u900f\u904e\u5927\u91cf\u8cc7\u6599\u96c6\u7684\u9810\u8a13\u7df4\u7de8\u78bc\u5ee3\u6cdb\u7684\u4e16\u754c\u77e5\u8b58\uff0c\u7136\u5f8c\u53ef\u4ee5\u91dd\u5c0d\u554f\u7b54\uff08QA\uff09\u4efb\u52d9\u9032\u884c\u5fae\u8abf\u3002\u7136\u800c\uff0c\u91dd\u5c0d QA \u4efb\u52d9\u5fae\u8abf LLM \u7684\u6709\u6548\u7b56\u7565\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u63a2\u7d22\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u6839\u64da\u9810\u8a13\u7df4 LLM \u8a18\u61b6\u7684\u77e5\u8b58\u7a0b\u5ea6\u5c0d\u76e3\u7763\u5f0f\u5fae\u8abf\uff08SFT\uff09\u8cc7\u6599\u9032\u884c\u5206\u985e\uff0c\u4e26\u9032\u884c\u4e00\u7cfb\u5217\u5be6\u8b49\u5206\u6790\u3002\u6211\u5011\u7684\u5be6\u9a57\u6d89\u53ca\u4f86\u81ea\u4e09\u500b\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u7684\u56db\u500b LLM\uff0c\u91cd\u9ede\u95dc\u6ce8\u4e09\u500b\u95dc\u9375\u56e0\u7d20\uff1aSFT \u6240\u9700\u7684\u8cc7\u6599\u91cf\u3001\u4e0d\u540c SFT \u8cc7\u6599\u96c6\u5c0d\u6a21\u578b\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u4ee5\u53ca\u8cc7\u6599\u9700\u6c42\u5982\u4f55\u56e0 LLM \u800c\u7570\u3002\u7d50\u679c\u986f\u793a\uff0c\u5728 SFT \u968e\u6bb5\u4e2d\uff0c\u53ea\u8981 60 \u500b\u8cc7\u6599\u9ede\u5c31\u80fd\u555f\u52d5\u9810\u8a13\u7df4\u671f\u9593\u7de8\u78bc\u7684\u77e5\u8b58\uff0c\u8b93 LLM \u80fd\u57f7\u884c QA \u4efb\u52d9\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u5177\u6709\u4e0d\u540c\u8a18\u61b6\u5c64\u7d1a\u8cc7\u6599\u7684 SFT \u5c0d LLM \u6548\u80fd\u6709\u986f\u8457\u5f71\u97ff\uff0c\u6700\u4f73\u8cc7\u6599\u96c6\u6703\u6839\u64da\u8981\u5fae\u8abf\u7684\u7279\u5b9a\u6a21\u578b\u800c\u6709\u6240\u4e0d\u540c\u3002\u672a\u4f86\u7684\u7814\u7a76\u5c07\u6df1\u5165\u63a2\u8a0e\u9019\u4e9b\u73fe\u8c61\u80cc\u5f8c\u7684\u6a5f\u5236\u3002", "author": "Junjie Ye et.al.", "authors": "Junjie Ye, Yuming Yang, Qi Zhang, Tao Gui, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan", "id": "2409.15825v1", "paper_url": "http://arxiv.org/abs/2409.15825v1", "repo": "null"}}