{"2410.14211": {"publish_time": "2024-10-18", "title": "Paths-over-Graph: Knowledge Graph Enpowered Large Language Model Reasoning", "paper_summary": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\uff0c\u4f46\u537b\u96e3\u4ee5\u514b\u670d\u5e7b\u89ba\u554f\u984c\uff0c\u4e14\u7f3a\u4e4f\u76f8\u95dc\u77e5\u8b58\uff0c\u5c24\u5176\u662f\u5728\u6df1\u5165\u8907\u96dc\u7684\u63a8\u7406\u548c\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u4e2d\u3002\u77e5\u8b58\u5716\u8b5c (KG) \u4ee5\u7d50\u69cb\u5316\u683c\u5f0f\u64f7\u53d6\u5927\u91cf\u4e8b\u5be6\uff0c\u70ba\u63a8\u7406\u63d0\u4f9b\u53ef\u9760\u7684\u77e5\u8b58\u4f86\u6e90\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u65bc KG \u7684 LLM \u63a8\u7406\u65b9\u6cd5\u9762\u81e8\u8655\u7406\u591a\u8df3\u63a8\u7406\u3001\u591a\u5be6\u9ad4\u554f\u984c\u548c\u6709\u6548\u5229\u7528\u5716\u5f62\u7d50\u69cb\u7b49\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u5716\u5f62\u8def\u5f91 (PoG)\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u900f\u904e\u6574\u5408\u4f86\u81ea KG \u7684\u77e5\u8b58\u63a8\u7406\u8def\u5f91\u4f86\u589e\u5f37 LLM \u63a8\u7406\uff0c\u9032\u800c\u63d0\u5347 LLM \u8f38\u51fa\u7684\u53ef\u89e3\u91cb\u6027\u548c\u771f\u5be6\u6027\u3002PoG \u900f\u904e\u4e09\u968e\u6bb5\u52d5\u614b\u591a\u8df3\u8def\u5f91\u63a2\u7d22\u4f86\u8655\u7406\u591a\u8df3\u548c\u591a\u5be6\u9ad4\u554f\u984c\uff0c\u5c07 LLM \u7684\u5167\u5728\u77e5\u8b58\u8207\u4f86\u81ea KG \u7684\u4e8b\u5be6\u77e5\u8b58\u7d50\u5408\u8d77\u4f86\u3002\u70ba\u4e86\u63d0\u9ad8\u6548\u7387\uff0cPoG \u9996\u5148\u5f9e\u5716\u5f62\u63a2\u7d22\u4e2d\u4fee\u526a\u4e0d\u76f8\u95dc\u7684\u8cc7\u8a0a\uff0c\u4e26\u5f15\u5165\u6709\u6548\u7684\u4e09\u6b65\u9a5f\u4fee\u526a\u6280\u8853\uff0c\u7d50\u5408\u5716\u5f62\u7d50\u69cb\u3001LLM \u63d0\u793a\u548c\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b (\u4f8b\u5982 SBERT)\uff0c\u4ee5\u6709\u6548\u7e2e\u5c0f\u63a2\u7d22\u7684\u5019\u9078\u8def\u5f91\u3002\u9019\u78ba\u4fdd\u6240\u6709\u63a8\u7406\u8def\u5f91\u90fd\u5305\u542b\u5f9e KG \u4e2d\u64f7\u53d6\u7684\u9ad8\u5ea6\u76f8\u95dc\u8cc7\u8a0a\uff0c\u4f7f\u63a8\u7406\u5728\u554f\u984c\u89e3\u6c7a\u4e2d\u4fdd\u6301\u771f\u5be6\u4e14\u53ef\u89e3\u91cb\u3002PoG \u5275\u65b0\u5730\u5229\u7528\u5716\u5f62\u7d50\u69cb\u4f86\u4fee\u526a\u7121\u95dc\u7684\u96dc\u8a0a\uff0c\u4e26\u9996\u6b21\u5be6\u4f5c\u5728 KG \u4e0a\u91dd\u5c0d LLM \u63a8\u7406\u4efb\u52d9\u9032\u884c\u591a\u5be6\u9ad4\u6df1\u5ea6\u8def\u5f91\u5075\u6e2c\u7684\u65b9\u6cd5\u3002\u5728\u4e94\u500b\u57fa\u6e96 KGQA \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5168\u9762\u5be6\u9a57\u8b49\u660e\uff0cPoG \u5728 GPT-3.5-Turbo \u548c GPT-4 \u4e0a\u512a\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5 ToG\uff0c\u5e73\u5747\u6e96\u78ba\u5ea6\u63d0\u5347 18.9%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u642d\u8f09 GPT-3.5-Turbo \u7684 PoG \u6bd4\u642d\u8f09 GPT-4 \u7684 ToG \u9ad8\u51fa 23.9%\u3002", "author": "Xingyu Tan et.al.", "authors": "Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang", "id": "2410.14211v1", "paper_url": "http://arxiv.org/abs/2410.14211v1", "repo": "null"}}