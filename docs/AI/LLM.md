
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-07-16**|**Does Refusal Training in LLMs Generalize to the Past Tense?**|Maksym Andriushchenko et.al.|[2407.11969v1](http://arxiv.org/abs/2407.11969v1)|[link](https://github.com/tml-epfl/llm-past-tense)|
|**2024-07-16**|**Efficient Training with Denoised Neural Weights**|Yifan Gong et.al.|[2407.11966v1](http://arxiv.org/abs/2407.11966v1)|null|
|**2024-07-16**|**NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?**|Mo Li et.al.|[2407.11963v1](http://arxiv.org/abs/2407.11963v1)|[link](https://github.com/open-compass/opencompass)|
|**2024-07-16**|**Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling**|Jaehyeok Kim et.al.|[2407.11962v1](http://arxiv.org/abs/2407.11962v1)|null|
|**2024-07-16**|**Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation**|Congbo Ma et.al.|[2407.11948v1](http://arxiv.org/abs/2407.11948v1)|null|
|**2024-07-16**|**Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering**|Rachneet Sachdeva et.al.|[2407.11930v1](http://arxiv.org/abs/2407.11930v1)|[link](https://github.com/ukplab/arxiv2024-lfqa-hallucination)|
|**2024-07-16**|**Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based Approach**|Tanvir Hossain et.al.|[2407.11928v1](http://arxiv.org/abs/2407.11928v1)|null|
|**2024-07-16**|**What's Wrong? Refining Meeting Summaries with LLM Feedback**|Frederic Kirstein et.al.|[2407.11919v1](http://arxiv.org/abs/2407.11919v1)|null|
|**2024-07-16**|**Imitation of human motion achieves natural head movements for humanoid robots in an active-speaker detection task**|Bosong Ding et.al.|[2407.11915v1](http://arxiv.org/abs/2407.11915v1)|[link](https://github.com/dingdingding60/humanoids2024hri)|
|**2024-07-16**|**Bridging Weighted First Order Model Counting and Graph Polynomials**|Qipeng Kuang et.al.|[2407.11877v1](http://arxiv.org/abs/2407.11877v1)|[link](https://github.com/l2l7l9p/polynomials-for-wfomc)|
|**2024-07-16**|**Evaluating Task-Oriented Dialogue Consistency through Constraint Satisfaction**|Tiziano Labruna et.al.|[2407.11857v1](http://arxiv.org/abs/2407.11857v1)|null|
|**2024-07-16**|**Scaling Sign Language Translation**|Biao Zhang et.al.|[2407.11855v1](http://arxiv.org/abs/2407.11855v1)|null|
|**2024-07-16**|**Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection**|Gaetan Lopez Latouche et.al.|[2407.11854v1](http://arxiv.org/abs/2407.11854v1)|null|
|**2024-07-16**|**Schema Matching with Large Language Models: an Experimental Study**|Marcel Parciak et.al.|[2407.11852v1](http://arxiv.org/abs/2407.11852v1)|[link](https://github.com/uhasselt-dsi-data-systems-lab/code-schema-matching-llms-artefacs)|
|**2024-07-16**|**Variational Randomized Smoothing for Sample-Wise Adversarial Robustness**|Ryo Hase et.al.|[2407.11844v1](http://arxiv.org/abs/2407.11844v1)|null|
|**2024-07-16**|**InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback**|Haishuo Fang et.al.|[2407.11843v1](http://arxiv.org/abs/2407.11843v1)|null|
|**2024-07-16**|**LoFTI: Localization and Factuality Transfer to Indian Locales**|Sona Elza Simon et.al.|[2407.11833v1](http://arxiv.org/abs/2407.11833v1)|[link](https://github.com/csalt-research/lofti)|
|**2024-07-16**|**Personalized Conversational Travel Assistant powered by Generative AI**|Alexio Cassani et.al.|[2407.11830v1](http://arxiv.org/abs/2407.11830v1)|null|
|**2024-07-16**|**GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text**|Kyle Hamilton et.al.|[2407.11827v1](http://arxiv.org/abs/2407.11827v1)|null|
|**2024-07-16**|**The Future of Data Science Education**|Brian Wright et.al.|[2407.11824v1](http://arxiv.org/abs/2407.11824v1)|null|
|**2024-07-16**|**Invariant Consistency for Knowledge Distillation**|Nikolaos Giakoumoglou et.al.|[2407.11802v1](http://arxiv.org/abs/2407.11802v1)|null|
|**2024-07-16**|**PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation**|Branden Butler et.al.|[2407.11798v1](http://arxiv.org/abs/2407.11798v1)|null|
|**2024-07-16**|**Characterizing and Understanding HGNN Training on GPUs**|Dengke Han et.al.|[2407.11790v1](http://arxiv.org/abs/2407.11790v1)|null|
|**2024-07-16**|**Large Language Models as Misleading Assistants in Conversation**|Betty Li Hou et.al.|[2407.11789v1](http://arxiv.org/abs/2407.11789v1)|null|
|**2024-07-16**|**Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development**|Daoyuan Chen et.al.|[2407.11784v1](http://arxiv.org/abs/2407.11784v1)|[link](https://github.com/modelscope/data-juicer)|
|**2024-07-16**|**SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models**|Xinbo Wu et.al.|[2407.11780v1](http://arxiv.org/abs/2407.11780v1)|null|
|**2024-07-16**|**Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to Detect Machine Generated Text**|Seyedeh Fatemeh Ebrahimi et.al.|[2407.11774v1](http://arxiv.org/abs/2407.11774v1)|null|
|**2024-07-16**|**Educational Personalized Learning Path Planning with Large Language Models**|Chee Ng et.al.|[2407.11773v1](http://arxiv.org/abs/2407.11773v1)|null|
|**2024-07-16**|**XEdgeAI: A Human-centered Industrial Inspection Framework with Data-centric Explainable Edge AI Approach**|Truong Thanh Hung Nguyen et.al.|[2407.11771v1](http://arxiv.org/abs/2407.11771v1)|null|
|**2024-07-16**|**Robust Utility-Preserving Text Anonymization Based on Large Language Models**|Tianyu Yang et.al.|[2407.11770v1](http://arxiv.org/abs/2407.11770v1)|[link](https://github.com/ukplab/arxiv2024-rupta)|
|**2024-07-16**|**Vectoring Languages**|Joseph Chen et.al.|[2407.11766v1](http://arxiv.org/abs/2407.11766v1)|null|
|**2024-07-16**|**A Channel Attention-Driven Hybrid CNN Framework for Paddy Leaf Disease Detection**|Pandiyaraju V et.al.|[2407.11753v1](http://arxiv.org/abs/2407.11753v1)|null|
|**2024-07-16**|**Universal Sound Separation with Self-Supervised Audio Masked Autoencoder**|Junqi Zhao et.al.|[2407.11745v1](http://arxiv.org/abs/2407.11745v1)|null|
|**2024-07-16**|**How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies**|Alina Leidinger et.al.|[2407.11733v1](http://arxiv.org/abs/2407.11733v1)|null|
|**2024-07-16**|**NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks**|Alberto Pirillo et.al.|[2407.11698v1](http://arxiv.org/abs/2407.11698v1)|null|
|**2024-07-16**|**CCoE: A Compact LLM with Collaboration of Experts**|Shaomang Huang et.al.|[2407.11686v2](http://arxiv.org/abs/2407.11686v2)|null|
|**2024-07-16**|**MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models**|Hongrong Cheng et.al.|[2407.11681v1](http://arxiv.org/abs/2407.11681v1)|null|
|**2024-07-16**|**SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation**|Yanis Lalou et.al.|[2407.11676v1](http://arxiv.org/abs/2407.11676v1)|[link](https://github.com/scikit-adaptation/skada-bench)|
|**2024-07-16**|**ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues**|John Mendon√ßa et.al.|[2407.11660v1](http://arxiv.org/abs/2407.11660v1)|null|
|**2024-07-16**|**R-SFLLM: Jamming Resilient Framework for Split Federated Learning with Large Language Models**|Aladin Djuhera et.al.|[2407.11654v1](http://arxiv.org/abs/2407.11654v1)|null|
|**2024-07-16**|**CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging**|Sunny Gupta et.al.|[2407.11652v1](http://arxiv.org/abs/2407.11652v1)|null|
|**2024-07-16**|**A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting**|He Chang et.al.|[2407.11638v1](http://arxiv.org/abs/2407.11638v1)|null|
|**2024-07-16**|**Rethinking Fair Graph Neural Networks from Re-balancing**|Zhixun Li et.al.|[2407.11624v1](http://arxiv.org/abs/2407.11624v1)|[link](https://github.com/zhixunlee/fairgb)|
|**2024-07-16**|**Graph Dimension Attention Networks for Enterprise Credit Assessment**|Shaopeng Wei et.al.|[2407.11615v1](http://arxiv.org/abs/2407.11615v1)|null|
|**2024-07-16**|**Bringing AI Participation Down to Scale: A Comment on Open AIs Democratic Inputs to AI Project**|David Moats et.al.|[2407.11613v1](http://arxiv.org/abs/2407.11613v1)|null|
|**2024-07-16**|**Statistical Reachability Analysis of Stochastic Cyber-Physical Systems under Distribution Shift**|Navid Hashemi et.al.|[2407.11609v1](http://arxiv.org/abs/2407.11609v1)|null|
|**2024-07-16**|**The Foundations of Tokenization: Statistical and Computational Concerns**|Juan Luis Gastaldi et.al.|[2407.11606v1](http://arxiv.org/abs/2407.11606v1)|null|
|**2024-07-16**|**Enhancing TinyML Security: Study of Adversarial Attack Transferability**|Parin Shah et.al.|[2407.11599v1](http://arxiv.org/abs/2407.11599v1)|null|
|**2024-07-16**|**DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised Pre-Training**|Guillermo Jimenez-Perez et.al.|[2407.11594v1](http://arxiv.org/abs/2407.11594v1)|null|
|**2024-07-16**|**AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization**|Anum Afzal et.al.|[2407.11591v1](http://arxiv.org/abs/2407.11591v1)|null|
|**2024-07-16**|**QVD: Post-training Quantization for Video Diffusion Models**|Shilong Tian et.al.|[2407.11585v2](http://arxiv.org/abs/2407.11585v2)|null|
|**2024-07-16**|**Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification**|Naif Alkhunaizi et.al.|[2407.11573v1](http://arxiv.org/abs/2407.11573v1)|null|
|**2024-07-16**|**TGIF: Text-Guided Inpainting Forgery Dataset**|Hannes Mareen et.al.|[2407.11566v1](http://arxiv.org/abs/2407.11566v1)|[link](https://github.com/idlabmedia/tgif-dataset)|
|**2024-07-16**|**Self-Guided Generation of Minority Samples Using Diffusion Models**|Soobin Um et.al.|[2407.11555v1](http://arxiv.org/abs/2407.11555v1)|[link](https://github.com/soobin-um/sg-minority)|
|**2024-07-16**|**Learning Global and Local Features of Power Load Series Through Transformer and 2D-CNN: An image-based Multi-step Forecasting Approach Incorporating Phase Space Reconstruction**|Zihan Tang et.al.|[2407.11553v1](http://arxiv.org/abs/2407.11553v1)|null|
|**2024-07-16**|**Optimizing KV Cache Eviction in LLMs: Adaptive Allocation for Enhanced Budget Utilization**|Yuan Feng et.al.|[2407.11550v1](http://arxiv.org/abs/2407.11550v1)|null|
|**2024-07-16**|**How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models**|Yin Jou Huang et.al.|[2407.11549v1](http://arxiv.org/abs/2407.11549v1)|null|
|**2024-07-16**|**AEMIM: Adversarial Examples Meet Masked Image Modeling**|Wenzhao Xiang et.al.|[2407.11537v1](http://arxiv.org/abs/2407.11537v1)|null|
|**2024-07-16**|**Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise**|Qimin Yang et.al.|[2407.11536v1](http://arxiv.org/abs/2407.11536v1)|null|
|**2024-07-16**|**LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices**|Jung Hyun Lee et.al.|[2407.11534v1](http://arxiv.org/abs/2407.11534v1)|[link](https://github.com/onliwad101/flexround_lrq)|
|**2024-07-16**|**Reasoning with Large Language Models, a Survey**|Aske Plaat et.al.|[2407.11511v1](http://arxiv.org/abs/2407.11511v1)|null|
|**2024-07-16**|**Diff-MTS: Temporal-Augmented Conditional Diffusion-based AIGC for Industrial Time Series Towards the Large Model Era**|Lei Ren et.al.|[2407.11501v1](http://arxiv.org/abs/2407.11501v1)|null|
|**2024-07-16**|**An AI System for Continuous Knee Osteoarthritis Severity Grading Using Self-Supervised Anomaly Detection with Limited Data**|Niamh Belton et.al.|[2407.11500v1](http://arxiv.org/abs/2407.11500v1)|[link](https://github.com/niamhbelton/ss-fewsome_disease_severity_knee_osteoarthritis)|
|**2024-07-16**|**MMSD-Net: Towards Multi-modal Stuttering Detection**|Liangyu Nie et.al.|[2407.11492v1](http://arxiv.org/abs/2407.11492v1)|null|
|**2024-07-16**|**A Meta-Learning Approach for Multi-Objective Reinforcement Learning in Sustainable Home Environments**|Junlin Lu et.al.|[2407.11489v1](http://arxiv.org/abs/2407.11489v1)|null|
|**2024-07-16**|**Scientific QA System with Verifiable Answers**|Adela Ljajiƒá et.al.|[2407.11485v1](http://arxiv.org/abs/2407.11485v1)|null|
|**2024-07-16**|**The Oscars of AI Theater: A Survey on Role-Playing with Language Models**|Nuo Chen et.al.|[2407.11484v2](http://arxiv.org/abs/2407.11484v2)|[link](https://github.com/nuochenpku/awesome-role-play-papers)|
|**2024-07-16**|**Multi-Channel Masked Autoencoder and Comprehensive Evaluations for Reconstructing 12-Lead ECG from Arbitrary Single-Lead ECG**|Jiarong Chen et.al.|[2407.11481v1](http://arxiv.org/abs/2407.11481v1)|[link](https://github.com/chenjiar3/mcma)|
|**2024-07-16**|**AIGC for Industrial Time Series: From Deep Generative Models to Large Generative Models**|Lei Ren et.al.|[2407.11480v1](http://arxiv.org/abs/2407.11480v1)|null|
|**2024-07-16**|**XTraffic: A Dataset Where Traffic Meets Incidents with Explainability and More**|Xiaochuan Gou et.al.|[2407.11477v1](http://arxiv.org/abs/2407.11477v1)|null|
|**2024-07-16**|**DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems**|Kaibo He et.al.|[2407.11472v1](http://arxiv.org/abs/2407.11472v1)|null|
|**2024-07-16**|**Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models**|Jiasheng Zheng et.al.|[2407.11470v1](http://arxiv.org/abs/2407.11470v1)|[link](https://github.com/jszheng21/race)|
|**2024-07-16**|**Investigating Imperceptibility of Adversarial Attacks on Tabular Data: An Empirical Analysis**|Zhipeng He et.al.|[2407.11463v1](http://arxiv.org/abs/2407.11463v1)|[link](https://github.com/zhipenghe/imperceptibility-of-tabular-adversarial-attack)|
|**2024-07-16**|**Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights**|Shunqi Mao et.al.|[2407.11449v1](http://arxiv.org/abs/2407.11449v1)|null|
|**2024-07-16**|**EARN Fairness: Explaining, Asking, Reviewing and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders**|Lin Luo et.al.|[2407.11442v1](http://arxiv.org/abs/2407.11442v1)|null|
|**2024-07-16**|**Repurformer: Transformers for Repurposing-Aware Molecule Generation**|Changhun Lee et.al.|[2407.11439v1](http://arxiv.org/abs/2407.11439v1)|null|
|**2024-07-16**|**Trust No Bot: Discovering Personal Disclosures in Human-LLM Conversations in the Wild**|Niloofar Mireshghallah et.al.|[2407.11438v1](http://arxiv.org/abs/2407.11438v1)|null|
|**2024-07-16**|**Generally-Occurring Model Change for Robust Counterfactual Explanations**|Ao Xu et.al.|[2407.11426v1](http://arxiv.org/abs/2407.11426v1)|null|
|**2024-07-16**|**States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly**|Junhao Chen et.al.|[2407.11421v1](http://arxiv.org/abs/2407.11421v1)|null|
|**2024-07-16**|**LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data**|Liana Patel et.al.|[2407.11418v1](http://arxiv.org/abs/2407.11418v1)|[link](https://github.com/stanford-futuredata/lotus)|
|**2024-07-16**|**SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions**|Shicheng Liu et.al.|[2407.11417v1](http://arxiv.org/abs/2407.11417v1)|null|
|**2024-07-16**|**Representation Bias in Political Sample Simulations with Large Language Models**|Weihong Qi et.al.|[2407.11409v1](http://arxiv.org/abs/2407.11409v1)|null|
|**2024-07-16**|**Revisiting the Impact of Pursuing Modularity for Code Generation**|Deokyeong Kang et.al.|[2407.11406v1](http://arxiv.org/abs/2407.11406v1)|null|
|**2024-07-16**|**DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation**|Jiwook Kim et.al.|[2407.11394v1](http://arxiv.org/abs/2407.11394v1)|[link](https://github.com/kaist-cvml-lab/DreamCatalyst)|
|**2024-07-16**|**CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**|Kalliopi Basioti et.al.|[2407.11393v1](http://arxiv.org/abs/2407.11393v1)|[link](https://github.com/SamsungLabs/CIC-BART-SSA)|
|**2024-07-16**|**InvAgent: A Large Language Model based Multi-Agent System for Inventory Management in Supply Chains**|Yinzhu Quan et.al.|[2407.11384v1](http://arxiv.org/abs/2407.11384v1)|null|
|**2024-07-16**|**Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts**|Jianhao Li et.al.|[2407.11382v2](http://arxiv.org/abs/2407.11382v2)|null|
|**2024-07-16**|**Reliable Reasoning Beyond Natural Language**|Nasim Borazjanizadeh et.al.|[2407.11373v1](http://arxiv.org/abs/2407.11373v1)|null|
|**2024-07-16**|**Estimating Agreement by Chance for Sequence Annotation**|Diya Li et.al.|[2407.11371v1](http://arxiv.org/abs/2407.11371v1)|null|
|**2024-07-16**|**A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora**|Kentaro Onda et.al.|[2407.11370v1](http://arxiv.org/abs/2407.11370v1)|null|
|**2024-07-16**|**Ancient Korean Archive Translation: Comparison Analysis on Statistical phrase alignment, LLM in-context learning, and inter-methodological approach**|Sojung Lucia Kim et.al.|[2407.11368v1](http://arxiv.org/abs/2407.11368v1)|null|
|**2024-07-16**|**Feature Inference Attack on Shapley Values**|Xinjian Luo et.al.|[2407.11359v1](http://arxiv.org/abs/2407.11359v1)|null|
|**2024-07-16**|**Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models**|Matthew Perez et.al.|[2407.11345v1](http://arxiv.org/abs/2407.11345v1)|null|
|**2024-07-16**|**COMET: "Cone of experience" enhanced large multimodal model for mathematical problem generation**|Sannyuya Liu et.al.|[2407.11315v1](http://arxiv.org/abs/2407.11315v1)|null|
|**2024-07-16**|**Large Vision-Language Models as Emotion Recognizers in Context Awareness**|Yuxuan Lei et.al.|[2407.11300v1](http://arxiv.org/abs/2407.11300v1)|null|
|**2024-07-16**|**Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems**|Ya≈üar Utku Al√ßalar et.al.|[2407.11288v1](http://arxiv.org/abs/2407.11288v1)|null|
|**2024-07-15**|**CLAMS: A System for Zero-Shot Model Selection for Clustering**|Prabhant Singh et.al.|[2407.11286v1](http://arxiv.org/abs/2407.11286v1)|null|
|**2024-07-15**|**Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models**|Qingcheng Zeng et.al.|[2407.11282v2](http://arxiv.org/abs/2407.11282v2)|[link](https://github.com/qcznlp/uncertainty_attack)|
|**2024-07-15**|**Quality Scalable Quantization Methodology for Deep Learning on Edge**|Salman Abdul Khaliq et.al.|[2407.11260v1](http://arxiv.org/abs/2407.11260v1)|null|
|**2024-07-15**|**Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation**|Chung Park et.al.|[2407.11245v1](http://arxiv.org/abs/2407.11245v1)|[link](https://github.com/cpark88/syncrec)|

#### Abstracts
##### **Does Refusal Training in LLMs Generalize to the Past Tense?**
2407.11969v1 by Maksym Andriushchenko, Nicolas Flammarion

Refusal training is widely used to prevent LLMs from generating harmful,
undesirable, or illegal outputs. We reveal a curious generalization gap in the
current refusal training approaches: simply reformulating a harmful request in
the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make
a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art
LLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo,
Gemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2 models using GPT-3.5 Turbo as a
reformulation model. For example, the success rate of this simple attack on
GPT-4o increases from 1% using direct requests to 88% using 20 past tense
reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a
jailbreak judge. Interestingly, we also find that reformulations in the future
tense are less effective, suggesting that refusal guardrails tend to consider
past historical questions more benign than hypothetical future questions.
Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending
against past reformulations is feasible when past tense examples are explicitly
included in the fine-tuning data. Overall, our findings highlight that the
widely used alignment techniques -- such as SFT, RLHF, and adversarial training
-- employed to align the studied models can be brittle and do not always
generalize as intended. We provide code and jailbreak artifacts at
https://github.com/tml-epfl/llm-past-tense.

ÊëòË¶ÅÔºöÊãíÁµïË®ìÁ∑¥Ë¢´Âª£Ê≥õÁî®ÊñºÈò≤Ê≠¢ LLM Áî¢ÁîüÊúâÂÆ≥„ÄÅ‰∏çÂèóÊ≠°ËøéÊàñÈùûÊ≥ïÁöÑËº∏Âá∫„ÄÇÊàëÂÄëÊè≠Á§∫‰∫ÜÁï∂ÂâçÊãíÁµïË®ìÁ∑¥ÊñπÊ≥ï‰∏≠‰∏ÄÂÄãÂ•áÊÄ™ÁöÑÊ¶ÇÊã¨Â∑ÆË∑ùÔºöÂÉÖÂÉÖÁî®ÈÅéÂéªÂºèÈáçÊñ∞Ë°®Ëø∞‰∏ÄÂÄãÊúâÂÆ≥ÁöÑË´ãÊ±ÇÔºà‰æãÂ¶ÇÔºå„ÄåÂ¶Ç‰ΩïË£Ω‰ΩúËé´Ê¥õÊâòÂ§´ÈõûÂ∞æÈÖíÔºü„ÄçÊîπÁÇ∫„Äå‰∫∫ÂÄëÊòØÂ¶Ç‰ΩïË£Ω‰ΩúËé´Ê¥õÊâòÂ§´ÈõûÂ∞æÈÖíÁöÑÔºü„ÄçÔºâÈÄöÂ∏∏Ë∂≥‰ª•ËÆìË®±Â§öÊúÄÂÖàÈÄ≤ÁöÑ LLM Ë∂äÁçÑ„ÄÇÊàëÂÄë‰ΩøÁî® GPT-3.5 Turbo ‰ΩúÁÇ∫ÈáçÊñ∞Ë°®Ëø∞Ê®°ÂûãÔºåÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫ÜÈÄôÁ®ÆÊñπÊ≥ïÂú® Llama-3 8B„ÄÅGPT-3.5 Turbo„ÄÅGemma-2 9B„ÄÅPhi-3-Mini„ÄÅGPT-4o Âíå R2D2 Ê®°Âûã‰∏äÁöÑÊïàÊûú„ÄÇ‰æãÂ¶ÇÔºåÈÄôÁ®ÆÁ∞°ÂñÆÊîªÊìäÂú® GPT-4o ‰∏äÁöÑÊàêÂäüÁéáÂæû‰ΩøÁî®Áõ¥Êé•Ë´ãÊ±ÇÊôÇÁöÑ 1% Â¢ûÂä†Âà∞‰ΩøÁî® 20 Ê¨°ÈÅéÂéªÂºèÈáçÊñ∞Ë°®Ëø∞ÂòóË©¶Â∞ç‰æÜËá™ JailbreakBench ÁöÑÊúâÂÆ≥Ë´ãÊ±ÇÊôÇÁÇ∫ 88%ÔºåËÄå GPT-4 Ââá‰ΩúÁÇ∫Ë∂äÁçÑË©ïÂà§„ÄÇÊúâË∂£ÁöÑÊòØÔºåÊàëÂÄëÈÇÑÁôºÁèæÔºåÊú™‰æÜÊôÇÊÖãÁöÑÈáçÊñ∞Ë°®Ëø∞ÊïàÊûúËºÉÂ∑ÆÔºåÈÄôË°®ÊòéÊãíÁµïÈò≤Ë≠∑Êé™ÊñΩÂÇæÂêëÊñºÂ∞áÈÅéÂéªÁöÑÊ≠∑Âè≤ÂïèÈ°åË¶ñÁÇ∫ÊØîÂÅáË®≠ÁöÑÊú™‰æÜÂïèÈ°åÊõ¥ËâØÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞çÂæÆË™ø GPT-3.5 Turbo ÁöÑÂØ¶È©óË°®ÊòéÔºåÂú®ÂæÆË™øÊï∏Êìö‰∏≠ÊòéÁ¢∫ÂåÖÂê´ÈÅéÂéªÊôÇÊÖãÁöÑÁ§∫‰æãÊôÇÔºåÂèØ‰ª•Èò≤Á¶¶ÈÅéÂéªÁöÑÈáçÊñ∞Ë°®Ëø∞„ÄÇÁ∏ΩÁöÑ‰æÜË™™ÔºåÊàëÂÄëÁöÑÁôºÁèæÂº∑Ë™ø‰∫ÜÂª£Ê≥õ‰ΩøÁî®ÁöÑÂ∞çÈΩäÊäÄË°ì‚Äî‚Äî‰æãÂ¶Ç SFT„ÄÅRLHF ÂíåÂ∞çÊäóË®ìÁ∑¥‚Äî‚ÄîÁî®ÊñºÂ∞çÈΩäÊâÄÁ†îÁ©∂ÁöÑÊ®°ÂûãÂèØËÉΩÊòØËÑÜÂº±ÁöÑÔºå‰∏¶‰∏î‰∏¶‰∏çÁ∏ΩÊòØÊåâÈ†êÊúüÁöÑÈÇ£Ê®£Ê¶ÇÊã¨„ÄÇÊàëÂÄëÂú® https://github.com/tml-epfl/llm-past-tense Êèê‰æõ‰ª£Á¢ºÂíåË∂äÁçÑÂ∑•‰ª∂„ÄÇ

##### **Efficient Training with Denoised Neural Weights**
2407.11966v1 by Yifan Gong, Zheng Zhan, Yanyu Li, Yerlan Idelbayev, Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, Jian Ren

Good weight initialization serves as an effective measure to reduce the
training cost of a deep neural network (DNN) model. The choice of how to
initialize parameters is challenging and may require manual tuning, which can
be time-consuming and prone to human error. To overcome such limitations, this
work takes a novel step towards building a weight generator to synthesize the
neural weights for initialization. We use the image-to-image translation task
with generative adversarial networks (GANs) as an example due to the ease of
collecting model weights spanning a wide range. Specifically, we first collect
a dataset with various image editing concepts and their corresponding trained
weights, which are later used for the training of the weight generator. To
address the different characteristics among layers and the substantial number
of weights to be predicted, we divide the weights into equal-sized blocks and
assign each block an index. Subsequently, a diffusion model is trained with
such a dataset using both text conditions of the concept and the block indexes.
By initializing the image translation model with the denoised weights predicted
by our diffusion model, the training requires only 43.3 seconds. Compared to
training from scratch (i.e., Pix2pix), we achieve a 15x training time
acceleration for a new concept while obtaining even better image generation
quality.

ÊëòË¶ÅÔºöËâØÂ•ΩÁöÑÊ¨äÈáçÂàùÂßãÂåñÊòØÊ∏õÂ∞ëÊ∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (DNN) Ê®°ÂûãË®ìÁ∑¥ÊàêÊú¨ÁöÑÊúâÊïàÊé™ÊñΩ„ÄÇÂ¶Ç‰ΩïÂàùÂßãÂåñÂèÉÊï∏ÁöÑÈÅ∏ÊìáÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏îÂèØËÉΩÈúÄË¶ÅÊâãÂãïË™øÊï¥ÔºåÈÄôÂèØËÉΩÊúÉËä±Ë≤ªÂ§ßÈáèÊôÇÈñì‰∏îÂÆπÊòìÂá∫ÈåØ„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄô‰∫õÈôêÂà∂ÔºåÈÄôÈ†ÖÂ∑•‰ΩúÊé°Âèñ‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑÊ≠•È©üÔºåÊúùËëóÂª∫Á´ã‰∏ÄÂÄãÊ¨äÈáçÁîüÊàêÂô®‰æÜÂêàÊàêÂàùÂßãÂåñÁöÑÁ•ûÁ∂ìÊ¨äÈáçÈÇÅÈÄ≤„ÄÇÊàëÂÄë‰ª•‰ΩøÁî®ÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑Ø (GAN) ÁöÑÂΩ±ÂÉèËΩâÂΩ±ÂÉèËΩâÊèõ‰ªªÂãôÁÇ∫‰æãÔºåÂõ†ÁÇ∫Êî∂ÈõÜÊ∂µËìãÂª£Ê≥õÁØÑÂúçÁöÑÊ®°ÂûãÊ¨äÈáçÂæàÁ∞°ÂñÆ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÊî∂ÈõÜ‰∏ÄÂÄãÂåÖÂê´ÂêÑÁ®ÆÂΩ±ÂÉèÁ∑®ËºØÊ¶ÇÂøµÂèäÂÖ∂Â∞çÊáâË®ìÁ∑¥Ê¨äÈáçÁöÑË≥áÊñôÈõÜÔºåÈÄô‰∫õÊ¨äÈáçÁ®çÂæåÁî®ÊñºË®ìÁ∑¥Ê¨äÈáçÁîüÊàêÂô®„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂ±§‰πãÈñìÁöÑ‰∏çÂêåÁâπÊÄßÂíåÂ§ßÈáèÁöÑÂæÖÈ†êÊ∏¨Ê¨äÈáçÔºåÊàëÂÄëÂ∞áÊ¨äÈáçÂàÜÊàêÂ§ßÂ∞èÁõ∏Á≠âÁöÑÂçÄÂ°äÔºå‰∏¶ÁÇ∫ÊØèÂÄãÂçÄÂ°äÊåáÂÆö‰∏ÄÂÄãÁ¥¢Âºï„ÄÇÈö®ÂæåÔºå‰ΩøÁî®ÂåÖÂê´Ê¶ÇÂøµÊñáÂ≠óÊ¢ù‰ª∂ÂíåÂçÄÂ°äÁ¥¢ÂºïÁöÑË≥áÊñôÈõÜÔºåË®ìÁ∑¥‰∏ÄÂÄãÊì¥Êï£Ê®°Âûã„ÄÇÈÄèÈÅé‰ΩøÁî®ÊàëÂÄëÁöÑÊì¥Êï£Ê®°ÂûãÈ†êÊ∏¨ÁöÑÂéªÂô™Ê¨äÈáçÂàùÂßãÂåñÂΩ±ÂÉèËΩâÊèõÊ®°ÂûãÔºåË®ìÁ∑¥ÂÉÖÈúÄ 43.3 Áßí„ÄÇËàáÂæûÈ†≠ÈñãÂßãË®ìÁ∑¥ (Âç≥ Pix2pix) Áõ∏ÊØîÔºåÊàëÂÄëÁÇ∫‰∏ÄÂÄãÊñ∞Ê¶ÇÂøµÂØ¶Áèæ‰∫Ü 15 ÂÄçÁöÑË®ìÁ∑¥ÊôÇÈñìÂä†ÈÄüÔºåÂêåÊôÇÁç≤ÂæóÊõ¥Â•ΩÁöÑÂΩ±ÂÉèÁîüÊàêÂìÅË≥™„ÄÇ

##### **NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?**
2407.11963v1 by Mo Li, Songyang Zhang, Yunxin Liu, Kai Chen

In evaluating the long-context capabilities of large language models (LLMs),
identifying content relevant to a user's query from original long documents is
a crucial prerequisite for any LLM to answer questions based on long text. We
present NeedleBench, a framework consisting of a series of progressively more
challenging tasks for assessing bilingual long-context capabilities, spanning
multiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and
different depth ranges, allowing the strategic insertion of critical data
points in different text depth zones to rigorously test the retrieval and
reasoning capabilities of models in diverse contexts. We use the NeedleBench
framework to assess how well the leading open-source models can identify key
information relevant to the question and apply that information to reasoning in
bilingual long texts. Furthermore, we propose the Ancestral Trace Challenge
(ATC) to mimic the complexity of logical reasoning challenges that are likely
to be present in real-world long-context tasks, providing a simple method for
evaluating LLMs in dealing with complex long-context situations. Our results
suggest that current LLMs have significant room for improvement in practical
long-context applications, as they struggle with the complexity of logical
reasoning challenges that are likely to be present in real-world long-context
tasks. All codes and resources are available at OpenCompass:
https://github.com/open-compass/opencompass.

ÊëòË¶ÅÔºö<paragraph>Âú®Ë©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈï∑Ë™ûÂ¢ÉËÉΩÂäõÊôÇÔºåÂæûÂéüÂßãÈï∑ÁØáÊñá‰ª∂‰∏≠Ëæ®Ë≠òËàá‰ΩøÁî®ËÄÖÊü•Ë©¢Áõ∏ÈóúÁöÑÂÖßÂÆπÊòØ‰ªª‰Ωï LLM Ê†πÊìöÈï∑ÊñáÂõûÁ≠îÂïèÈ°åÁöÑÂøÖË¶ÅÂÖàÊ±∫Ê¢ù‰ª∂„ÄÇÊàëÂÄëÊèêÂá∫ NeedleBenchÔºå‰∏ÄÂÄãÁî±‰∏ÄÁ≥ªÂàóÈõ£Â∫¶ÈÄêÊº∏Â¢ûÂä†ÁöÑ‰ªªÂãôÁµÑÊàêÁöÑÊû∂ÊßãÔºåÁî®ÊñºË©ï‰º∞ÈõôË™ûÈï∑Ë™ûÂ¢ÉËÉΩÂäõÔºåÊ∂µËìãÂ§öÂÄãÈï∑Â∫¶ÂçÄÈñìÔºà4k„ÄÅ8k„ÄÅ32k„ÄÅ128k„ÄÅ200k„ÄÅ1000kÔºå‰ª•ÂèäÊõ¥Â§öÔºâÂíå‰∏çÂêåÁöÑÊ∑±Â∫¶ÁØÑÂúçÔºåÂÖÅË®±Âú®‰∏çÂêåÁöÑÊñáÂ≠óÊ∑±Â∫¶ÂçÄÂüüÁ≠ñÁï•ÊÄßÂú∞ÊèíÂÖ•ÈóúÈçµË≥áÊñôÈªûÔºå‰ª•Âö¥Ê†ºÊ∏¨Ë©¶Ê®°ÂûãÂú®‰∏çÂêåË™ûÂ¢É‰∏≠ÁöÑÊ™¢Á¥¢ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÊàëÂÄë‰ΩøÁî® NeedleBench Êû∂Êßã‰æÜË©ï‰º∞È†òÂÖàÁöÑÈñãÊ∫êÊ®°ÂûãÂú®Ëæ®Ë≠òËàáÂïèÈ°åÁõ∏ÈóúÁöÑÈóúÈçµË≥áË®äÔºå‰ª•ÂèäÂ∞áË©≤Ë≥áË®äÊáâÁî®ÊñºÈõôË™ûÈï∑Êñá‰∏≠Êé®ÁêÜÁöÑËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫Á•ñÂÖàËøΩËπ§ÊåëÊà∞ (ATC)ÔºåÊ®°Êì¨Âú®ÁèæÂØ¶‰∏ñÁïåÈï∑Ë™ûÂ¢É‰ªªÂãô‰∏≠ÂèØËÉΩÂ≠òÂú®ÁöÑÈÇèËºØÊé®ÁêÜÊåëÊà∞ÁöÑË§áÈõúÊÄßÔºåÊèê‰æõ‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÊñπÊ≥ï‰æÜË©ï‰º∞ LLM Âú®ËôïÁêÜË§áÈõúÈï∑Ë™ûÂ¢ÉÊÉÖÊ≥ÅÊôÇÁöÑË°®Áèæ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÁõÆÂâçÁöÑ LLM Âú®ÂØ¶ÈöõÈï∑Ë™ûÂ¢ÉÊáâÁî®‰∏≠‰ªçÊúâÂæàÂ§ßÁöÑÊîπÈÄ≤Á©∫ÈñìÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÈõ£‰ª•Êáâ‰ªòÁèæÂØ¶‰∏ñÁïåÈï∑Ë™ûÂ¢É‰ªªÂãô‰∏≠ÂèØËÉΩÂ≠òÂú®ÁöÑÈÇèËºØÊé®ÁêÜÊåëÊà∞ÁöÑË§áÈõúÊÄß„ÄÇÊâÄÊúâÁ®ãÂºèÁ¢ºÂíåË≥áÊ∫êÈÉΩÂèØ‰ª•Âú® OpenCompass ÂèñÂæóÔºöhttps://github.com/open-compass/opencompass„ÄÇ</paragraph>

##### **Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling**
2407.11962v1 by Jaehyeok Kim, Dongyoon Wee, Dan Xu

This paper introduces Motion-oriented Compositional Neural Radiance Fields
(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of
monocular human videos via novel non-rigid motion modeling approach. In the
context of dynamic clothed humans, complex cloth dynamics generate non-rigid
motions that are intrinsically distinct from skeletal articulations and
critically important for the rendering quality. The conventional approach
models non-rigid motions as spatial (3D) deviations in addition to skeletal
transformations. However, it is either time-consuming or challenging to achieve
optimal quality due to its high learning complexity without a direct
supervision. To target this problem, we propose a novel approach of modeling
non-rigid motions as radiance residual fields to benefit from more direct color
supervision in the rendering and utilize the rigid radiance fields as a prior
to reduce the complexity of the learning process. Our approach utilizes a
single multiresolution hash encoding (MHE) to concurrently learn the canonical
T-pose representation from rigid skeletal motions and the radiance residual
field for non-rigid motions. Additionally, to further improve both training
efficiency and usability, we extend MoCo-NeRF to support simultaneous training
of multiple subjects within a single framework, thanks to our effective design
for modeling non-rigid motions. This scalability is achieved through the
integration of a global MHE and learnable identity codes in addition to
multiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,
clearly demonstrating state-of-the-art performance in both single- and
multi-subject settings. The code and model will be made publicly available at
the project page: https://stevejaehyeok.github.io/publications/moco-nerf.

ÊëòË¶ÅÔºö<paragraph>Êú¨Êñá‰ªãÁªç‰∫ÜÈù¢ÂêëËøêÂä®ÁöÑÂêàÊàêÁ•ûÁªèËæêÂ∞ÑÂú∫ (MoCo-NeRF)ÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ÈÄöËøáÊñ∞È¢ñÁöÑÈùûÂàöÊÄßËøêÂä®Âª∫Ê®°ÊñπÊ≥ïÊâßË°åÂçïÁúº‰∫∫Á±ªËßÜÈ¢ëÁöÑËá™Áî±ËßÜÁÇπÊ∏≤ÊüìÁöÑÊ°ÜÊû∂„ÄÇÂú®Âä®ÊÄÅÁùÄË£ÖÁöÑ‰∫∫Á±ªÁöÑËÉåÊôØ‰∏ãÔºåÂ§çÊùÇÁöÑÂ∏ÉÊñôÂä®ÊÄÅ‰ºö‰∫ßÁîüÈùûÂàöÊÄßËøêÂä®ÔºåËøô‰∫õËøêÂä®Êú¨Ë¥®‰∏ä‰∏çÂêå‰∫éÈ™®È™ºÂÖ≥ËäÇÔºåÂπ∂‰∏îÂØπÊ∏≤ÊüìË¥®ÈáèËá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰º†ÁªüÊñπÊ≥ïÂ∞ÜÈùûÂàöÊÄßËøêÂä®Âª∫Ê®°‰∏∫Á©∫Èó¥ (3D) ÂÅèÂ∑Æ‰ª•ÂèäÈ™®È™ºÂèòÊç¢„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫éÂÖ∂Â≠¶‰π†Â§çÊùÇÂ∫¶È´ò‰∏îÊ≤°ÊúâÁõ¥Êé•ÁõëÁù£ÔºåÂõ†Ê≠§Ë¶ÅËææÂà∞ÊúÄ‰Ω≥Ë¥®ÈáèÊó¢ËÄóÊó∂ÂèàÂÖ∑ÊúâÊåëÊàòÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÂ∞ÜÈùûÂàöÊÄßËøêÂä®Âª∫Ê®°‰∏∫ËæêÂ∞ÑÊÆãÂ∑ÆÂú∫Ôºå‰ª•ÂèóÁõä‰∫éÊ∏≤Êüì‰∏≠Êõ¥Áõ¥Êé•ÁöÑÈ¢úËâ≤ÁõëÁù£ÔºåÂπ∂Â∞ÜÂàöÊÄßËæêÂ∞ÑÂú∫Áî®‰ΩúÂÖàÈ™åÊù•Èôç‰ΩéÂ≠¶‰π†ËøáÁ®ãÁöÑÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂà©Áî®Âçï‰∏ÄÁöÑÂ§öÂàÜËæ®ÁéáÂìàÂ∏åÁºñÁ†Å (MHE) Êù•ÂêåÊó∂‰ªéÂàöÊÄßÈ™®È™ºËøêÂä®‰∏≠Â≠¶‰π†ËßÑËåÉÁöÑ T ÂßøÂäøË°®Á§∫Ôºå‰ª•ÂèäÁî®‰∫éÈùûÂàöÊÄßËøêÂä®ÁöÑËæêÂ∞ÑÊÆãÂ∑ÆÂú∫„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•ÊèêÈ´òËÆ≠ÁªÉÊïàÁéáÂíåÂèØÁî®ÊÄßÔºåÊàë‰ª¨Êâ©Â±ï‰∫Ü MoCo-NeRF ‰ª•ÊîØÊåÅÂú®Âçï‰∏™Ê°ÜÊû∂ÂÜÖÂêåÊó∂ËÆ≠ÁªÉÂ§ö‰∏™‰∏ª‰ΩìÔºåËøôË¶ÅÂΩíÂäü‰∫éÊàë‰ª¨Áî®‰∫éÂª∫Ê®°ÈùûÂàöÊÄßËøêÂä®ÁöÑÊúâÊïàËÆæËÆ°„ÄÇÈô§‰∫ÜÂ§ö‰∏™Â±ÄÈÉ® MHE ‰πãÂ§ñÔºåËøôÁßçÂèØÊâ©Â±ïÊÄßÊòØÈÄöËøáÈõÜÊàêÂÖ®Â±Ä MHE ÂíåÂèØÂ≠¶‰π†ÁöÑË∫´‰ªΩ‰ª£Á†ÅÂÆûÁé∞ÁöÑ„ÄÇÊàë‰ª¨Âú® ZJU-MoCap Âíå MonoCap ‰∏äÂ±ïÁ§∫‰∫ÜÂπøÊ≥õÁöÑÁªìÊûúÔºåÊ∏ÖÊ•öÂú∞Â±ïÁ§∫‰∫ÜÂú®Âçï‰∏ª‰ΩìÂíåÂ§ö‰∏ª‰ΩìËÆæÁΩÆ‰∏≠ÈÉΩËææÂà∞ÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ‰ª£Á†ÅÂíåÊ®°ÂûãÂ∞ÜÂú®È°πÁõÆÈ°µÈù¢ÂÖ¨ÂºÄÔºöhttps://stevejaehyeok.github.io/publications/moco-nerf„ÄÇ</paragraph>

##### **Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation**
2407.11948v1 by Congbo Ma, Wei Emma Zhang, Dileepa Pitawela, Haojie Zhuang, Yanfeng Shu

The utilization of Transformer-based models prospers the growth of
multi-document summarization (MDS). Given the huge impact and widespread
adoption of Transformer-based models in various natural language processing
tasks, investigating their performance and behaviors in the context of MDS
becomes crucial for advancing the field and enhancing the quality of summary.
To thoroughly examine the behaviours of Transformer-based MDS models, this
paper presents five empirical studies on (1) measuring the impact of document
boundary separators quantitatively; (2) exploring the effectiveness of
different mainstream Transformer structures; (3) examining the sensitivity of
the encoder and decoder; (4) discussing different training strategies; and (5)
discovering the repetition in a summary generation. The experimental results on
prevalent MDS datasets and eleven evaluation metrics show the influence of
document boundary separators, the granularity of different level features and
different model training strategies. The results also reveal that the decoder
exhibits greater sensitivity to noises compared to the encoder. This
underscores the important role played by the decoder, suggesting a potential
direction for future research in MDS. Furthermore, the experimental results
indicate that the repetition problem in the generated summaries has
correlations with the high uncertainty scores.

ÊëòË¶ÅÔºö<paragraph>Âü∫Êñº Transformer ÁöÑÊ®°ÂûãÈÅãÁî®Ëì¨ÂãÉÁôºÂ±ï‰∫ÜÂ§öÊñá‰ª∂ÊëòË¶Å (MDS) ÁöÑÊàêÈï∑„ÄÇÁî±ÊñºÂü∫Êñº Transformer ÁöÑÊ®°ÂûãÂú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãô‰∏≠ÂÖ∑ÊúâÂ∑®Â§ßÁöÑÂΩ±ÈüøÂäõÂíåÂª£Ê≥õÁöÑÊé°Áî®ÔºåÂõ†Ê≠§Á†îÁ©∂ÂÆÉÂÄëÂú® MDS ËÉåÊôØ‰∏ãÁöÑÊïàËÉΩÂíåË°åÁÇ∫Â∞çÊñºÊé®ÂãïË©≤È†òÂüüÂíåÊèêÂçáÊëòË¶ÅÂìÅË≥™Ëá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫‰∫ÜÂæπÂ∫ïÊ™¢È©óÂü∫Êñº Transformer ÁöÑ MDS Ê®°ÂûãÁöÑË°åÁÇ∫ÔºåÊú¨ÊñáÈáùÂ∞ç (1) ÂÆöÈáèÊ∏¨ÈáèÊñá‰ª∂ÈÇäÁïåÂàÜÈöîÁ¨¶ÁöÑÂΩ±ÈüøÔºõ(2) Êé¢Ë®é‰∏çÂêå‰∏ªÊµÅ Transformer ÁµêÊßãÁöÑÊúâÊïàÊÄßÔºõ(3) Ê™¢È©óÁ∑®Á¢ºÂô®ÂíåËß£Á¢ºÂô®ÁöÑÊïèÊÑüÊÄßÔºõ(4) Ë®éË´ñ‰∏çÂêåÁöÑË®ìÁ∑¥Á≠ñÁï•Ôºõ‰ª•Âèä (5) ÁôºÁèæÊëòË¶ÅÁîüÊàê‰∏≠ÁöÑÈáçË§áÊÄßÔºåÊèêÂá∫‰∫Ü‰∫îÈ†ÖÂØ¶Ë≠âÁ†îÁ©∂„ÄÇÂú®ÊµÅË°åÁöÑ MDS Ë≥áÊñôÈõÜÂíå 11 È†ÖË©ï‰º∞ÊåáÊ®ô‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫‰∫ÜÊñá‰ª∂ÈÇäÁïåÂàÜÈöîÁ¨¶„ÄÅ‰∏çÂêåÂ±§Á¥öÁâπÂæµÁöÑÁ≤íÂ∫¶Âíå‰∏çÂêåÊ®°ÂûãË®ìÁ∑¥Á≠ñÁï•ÁöÑÂΩ±Èüø„ÄÇÁµêÊûúÈÇÑÊè≠Á§∫ÔºåËàáÁ∑®Á¢ºÂô®Áõ∏ÊØîÔºåËß£Á¢ºÂô®Â∞çÈõúË®äË°®ÁèæÂá∫Êõ¥Â§ßÁöÑÊïèÊÑüÊÄß„ÄÇÈÄôÂº∑Ë™ø‰∫ÜËß£Á¢ºÂô®ÊâÄÊâÆÊºîÁöÑÈáçË¶ÅËßíËâ≤ÔºåÁÇ∫ MDS Êú™‰æÜÁöÑÁ†îÁ©∂ÊåáÂá∫‰∫ÜÊΩõÂú®ÁöÑÊñπÂêë„ÄÇÊ≠§Â§ñÔºåÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÁîüÊàêÁöÑÊëòË¶Å‰∏≠ÈáçË§áÂá∫ÁèæÁöÑÂïèÈ°åËàáÈ´ò‰∏çÁ¢∫ÂÆöÊÄßÂàÜÊï∏ÊúâÈóú„ÄÇ</paragraph>

##### **Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering**
2407.11930v1 by Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych

Long-form question answering (LFQA) aims to provide thorough and in-depth
answers to complex questions, enhancing comprehension. However, such detailed
responses are prone to hallucinations and factual inconsistencies, challenging
their faithful evaluation. This work introduces HaluQuestQA, the first
hallucination dataset with localized error annotations for human-written and
model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k
span-level error annotations for five different error types by expert
annotators, along with preference judgments. Using our collected data, we
thoroughly analyze the shortcomings of long-form answers and find that they
lack comprehensiveness and provide unhelpful references. We train an automatic
feedback model on this dataset that predicts error spans with incomplete
information and provides associated explanations. Finally, we propose a
prompt-based approach, Error-informed refinement, that uses signals from the
learned feedback model to refine generated answers, which we show reduces
hallucination and improves answer quality. Furthermore, humans find answers
generated by our approach comprehensive and highly prefer them (84%) over the
baseline answers.

ÊëòË¶ÅÔºöÈï∑ÁØáÂïèÁ≠î (LFQA) Êó®Âú®Êèê‰æõÂ∞çË§áÈõúÂïèÈ°åÁöÑÂÖ®Èù¢Ê∑±ÂÖ•ÁöÑÁ≠îÊ°àÔºå‰ª•Â¢ûÂº∑ÁêÜËß£Âäõ„ÄÇÁÑ∂ËÄåÔºåÂ¶ÇÊ≠§Ë©≥Á¥∞ÁöÑÂõûÊáâÂÆπÊòìÂá∫ÁèæÂπªË¶∫Âíå‰∫ãÂØ¶‰∏çÁ¨¶ÔºåÂ∞çÂÖ∂Âø†ÂØ¶ÁöÑË©ï‰º∞ÊßãÊàêÊåëÊà∞„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂºïÂÖ•‰∫Ü HaluQuestQAÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÂÖ∑ÊúâÈáùÂ∞ç‰∫∫È°ûÊí∞ÂØ´ÂíåÊ®°ÂûãÁîüÊàêÁöÑ LFQA Á≠îÊ°àÁöÑÂ±ÄÈÉ®ÈåØË™§Ë®ªËß£ÁöÑÂπªË¶∫Êï∏ÊìöÈõÜ„ÄÇHaluQuestQA ÂåÖÂê´ 698 ÂÄã QA Â∞çÔºåÂÖ∂‰∏≠ÂåÖÂê´Áî±Â∞àÂÆ∂Ë®ªËß£ËÄÖÈáùÂ∞ç‰∫îÁ®Æ‰∏çÂêåÈåØË™§È°ûÂûãÈÄ≤Ë°åÁöÑ 4.7k ÂÄãË∑®Â∫¶Á¥öÂà•ÈåØË™§Ë®ªËß£Ôºå‰ª•ÂèäÂÅèÂ•ΩÂà§Êñ∑„ÄÇÂà©Áî®ÊàëÂÄëÊî∂ÈõÜÁöÑÊï∏ÊìöÔºåÊàëÂÄëÂæπÂ∫ïÂàÜÊûê‰∫ÜÈï∑ÁØáÁ≠îÊ°àÁöÑÁº∫ÈªûÔºåÁôºÁèæÂÆÉÂÄëÁº∫‰πèÂÖ®Èù¢ÊÄßÔºå‰∏¶‰∏îÊèê‰æõÁöÑÂèÉËÄÉÊ≤íÊúâÂπ´Âä©„ÄÇÊàëÂÄëÂú®ÈÄôÂÄãÊï∏ÊìöÈõÜ‰∏äË®ìÁ∑¥‰∫Ü‰∏ÄÂÄãËá™ÂãïÂõûÈ•ãÊ®°ÂûãÔºåÂÆÉÂèØ‰ª•È†êÊ∏¨ÂÖ∑Êúâ‰∏çÂÆåÊï¥‰ø°ÊÅØÁöÑÈåØË™§Ë∑®Â∫¶Ôºå‰∏¶Êèê‰æõÁõ∏ÈóúËß£Èáã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÊèêÁ§∫ÁöÑÊñπÊ≥ïÔºåÂç≥ÈåØË™§Áü•ÊÉÖÂÑ™ÂåñÔºåÂÆÉ‰ΩøÁî®ÂæûÂ≠∏ÁøíÁöÑÂõûÈ•ãÊ®°Âûã‰∏≠Áç≤ÂæóÁöÑ‰ø°Ëôü‰æÜÂÑ™ÂåñÁîüÊàêÁöÑÁ≠îÊ°àÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÈÄôÊ∏õÂ∞ë‰∫ÜÂπªË¶∫‰∏¶ÊèêÈ´ò‰∫ÜÁ≠îÊ°àË≥™Èáè„ÄÇÊ≠§Â§ñÔºå‰∫∫È°ûÁôºÁèæÁî±ÊàëÂÄëÁöÑÊñπÊ≥ïÁîüÊàêÁöÑÁ≠îÊ°àÂÖ®Èù¢Ôºå‰∏¶‰∏îÈùûÂ∏∏ÂñúÊ≠°ÂÆÉÂÄëÔºà84%ÔºâËÄå‰∏çÊòØÂü∫Ê∫ñÁ≠îÊ°à„ÄÇ

##### **Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based Approach**
2407.11928v1 by Tanvir Hossain, Khaled Mohammed Saifuddin, Muhammad Ifte Khairul Islam, Farhan Tanvir, Esra Akbas

Graph Neural Network (GNN) achieves great success for node-level and
graph-level tasks via encoding meaningful topological structures of networks in
various domains, ranging from social to biological networks. However, repeated
aggregation operations lead to excessive mixing of node representations,
particularly in dense regions with multiple GNN layers, resulting in nearly
indistinguishable embeddings. This phenomenon leads to the oversmoothing
problem that hampers downstream graph analytics tasks. To overcome this issue,
we propose a novel and flexible truss-based graph sparsification model that
prunes edges from dense regions of the graph. Pruning redundant edges in dense
regions helps to prevent the aggregation of excessive neighborhood information
during hierarchical message passing and pooling in GNN models. We then utilize
our sparsification model in the state-of-the-art baseline GNNs and pooling
models, such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL, DMonPool, and
AdamGNN. Extensive experiments on different real-world datasets show that our
model significantly improves the performance of the baseline GNN models in the
graph classification task.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÈÄèÈÅéÁ∑®Á¢ºÂêÑÁ®ÆÈ†òÂüü‰∏≠Á∂≤Ë∑ØÁöÑÊÑèÁæ©ÊãìÊí≤ÁµêÊßãÔºåÂú®ÁØÄÈªûÂ±§Á¥öÂíåÂúñÂΩ¢Â±§Á¥ö‰ªªÂãô‰∏≠ÂèñÂæóÊ•µ‰Ω≥ÁöÑÊàêÂäüÔºåÂæûÁ§æ‰∫§Á∂≤Ë∑ØÂà∞ÁîüÁâ©Á∂≤Ë∑ØÁöÜÊúâÊ∂âÁçµ„ÄÇÁÑ∂ËÄåÔºåÈáçË§áÁöÑËÅöÈõÜÈÅãÁÆóÂ∞éËá¥ÁØÄÈªûË°®Á§∫ÈÅéÂ∫¶Ê∑∑ÂêàÔºåÁâπÂà•ÊòØÂú®ÂÖ∑ÊúâÂ§öÂÄã GNN Â±§ÁöÑÂØÜÈõÜÂçÄÂüüÔºåÂ∞éËá¥ÂµåÂÖ•ÂºèÂπæ‰πéÁÑ°Ê≥ïÂçÄÂàÜ„ÄÇÈÄôÁ®ÆÁèæË±°Â∞éËá¥ÈÅéÂ∫¶Âπ≥ÊªëÂïèÈ°åÔºåÈòªÁ§ô‰∏ãÊ∏∏ÂúñÂΩ¢ÂàÜÊûê‰ªªÂãô„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©é‰∏îÈùàÊ¥ªÁöÑÂü∫ÊñºÊ°ÅÊû∂ÁöÑÂúñÂΩ¢Á®ÄÁñèÂåñÊ®°ÂûãÔºåÂèØÂæûÂúñÂΩ¢ÁöÑÂØÜÈõÜÂçÄÂüü‰øÆÂâ™ÈÇäÁ∑£„ÄÇ‰øÆÂâ™ÂØÜÈõÜÂçÄÂüü‰∏≠Â§öÈ§òÁöÑÈÇäÁ∑£ÊúâÂä©ÊñºÈò≤Ê≠¢Âú® GNN Ê®°Âûã‰∏≠ÈÄ≤Ë°åÈöéÂ±§ÂºèË®äÊÅØÂÇ≥ÈÅûÂíåÂåØÁ∏ΩÊúüÈñìÈÅéÂ∫¶ÈÑ∞ÂüüË≥áË®äÁöÑËÅöÈõÜ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂú®ÊúÄÂÖàÈÄ≤ÁöÑÂü∫Á∑ö GNN ÂíåÂåØÁ∏ΩÊ®°Âûã‰∏≠‰ΩøÁî®ÊàëÂÄëÁöÑÁ®ÄÁñèÂåñÊ®°ÂûãÔºå‰æãÂ¶Ç GIN„ÄÅSAGPool„ÄÅGMT„ÄÅDiffPool„ÄÅMinCutPool„ÄÅHGP-SL„ÄÅDMonPool Âíå AdamGNN„ÄÇÂú®‰∏çÂêåÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂ§ßÈáèÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈ°ØËëóÊèêÈ´ò‰∫ÜÂü∫Á∑ö GNN Ê®°ÂûãÂú®ÂúñÂΩ¢ÂàÜÈ°û‰ªªÂãô‰∏≠ÁöÑÊïàËÉΩ„ÄÇ

##### **What's Wrong? Refining Meeting Summaries with LLM Feedback**
2407.11919v1 by Frederic Kirstein, Terry Ruas, Bela Gipp

Meeting summarization has become a critical task since digital encounters
have become a common practice. Large language models (LLMs) show great
potential in summarization, offering enhanced coherence and context
understanding compared to traditional methods. However, they still struggle to
maintain relevance and avoid hallucination. We introduce a multi-LLM correction
approach for meeting summarization using a two-phase process that mimics the
human review process: mistake identification and summary refinement. We release
QMSum Mistake, a dataset of 200 automatically generated meeting summaries
annotated by humans on nine error types, including structural, omission, and
irrelevance errors. Our experiments show that these errors can be identified
with high accuracy by an LLM. We transform identified mistakes into actionable
feedback to improve the quality of a given summary measured by relevance,
informativeness, conciseness, and coherence. This post-hoc refinement
effectively improves summary quality by leveraging multiple LLMs to validate
output quality. Our multi-LLM approach for meeting summarization shows
potential for similar complex text generation tasks requiring robustness,
action planning, and discussion towards a goal.

ÊëòË¶ÅÔºöÊúÉË≠∞ÊëòË¶ÅÂ∑≤ÊàêÁÇ∫‰∏ÄÈ†ÖÈóúÈçµ‰ªªÂãôÔºåÂõ†ÁÇ∫Êï∏‰ΩçÊúÉË≠∞Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÂ∏∏Ë¶ãÁöÑÂØ¶Âãô„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÊëòË¶ÅÊñπÈù¢Â±ïÁèæ‰∫ÜÊ•µÂ§ßÁöÑÊΩõÂäõÔºåËàáÂÇ≥Áµ±ÊñπÊ≥ïÁõ∏ÊØîÔºåÂÆÉÊèê‰æõ‰∫ÜÂ¢ûÂº∑ÁöÑ‰∏ÄËá¥ÊÄßÂíåËÑàÁµ°ÁêÜËß£„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰ªçÁÑ∂Èõ£‰ª•Á∂≠ÊåÅÁõ∏ÈóúÊÄß‰∏¶ÈÅøÂÖçÁî¢ÁîüÂπªË¶∫„ÄÇÊàëÂÄëÈáùÂ∞çÊúÉË≠∞ÊëòË¶ÅÊé®Âá∫‰∫Ü‰∏ÄÁ®ÆÂ§ö LLM ‰øÆÊ≠£ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ï‰ΩøÁî®‰∏ÄÂÄãÂÖ©ÈöéÊÆµÊµÅÁ®ã‰æÜÊ®°Êì¨‰∫∫È°ûÂØ©Êü•ÊµÅÁ®ãÔºöÈåØË™§Ëæ®Ë≠òÂíåÊëòË¶ÅÁ≤æÁÖâ„ÄÇÊàëÂÄëÁôºÂ∏É‰∫Ü QMSum MistakeÔºåÈÄôÊòØ‰∏ÄÂÄãÂåÖÂê´ 200 ÂÄãËá™ÂãïÁî¢ÁîüÁöÑÊúÉË≠∞ÊëòË¶ÅÁöÑË≥áÊñôÈõÜÔºåÁî±‰∫∫È°ûÈáùÂ∞ç‰πùÁ®ÆÈ°ûÂûãÁöÑÈåØË™§ÈÄ≤Ë°åË®ªËß£ÔºåÂåÖÊã¨ÁµêÊßã„ÄÅÈÅ∫ÊºèÂíå‰∏çÁõ∏ÈóúÁöÑÈåØË™§„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÈÄô‰∫õÈåØË™§ÂèØ‰ª•Áî± LLM ‰ª•È´òÊ∫ñÁ¢∫Â∫¶Ëæ®Ë≠òÂá∫‰æÜ„ÄÇÊàëÂÄëÂ∞áËæ®Ë≠òÂá∫ÁöÑÈåØË™§ËΩâÊèõÁÇ∫ÂèØË°åÁöÑÂõûÈ•ãÔºå‰ª•ÊîπÂñÑÊëòË¶ÅÁöÑÂìÅË≥™ÔºåÈÄô‰∫õÂìÅË≥™Áî±Áõ∏ÈóúÊÄß„ÄÅË≥áË®äÊÄß„ÄÅÁ∞°ÊΩîÊÄßÂíå‰∏ÄËá¥ÊÄß‰æÜË°°Èáè„ÄÇÈÄôÁ®Æ‰∫ãÂæåÁ≤æÁÖâÊúâÊïàÂú∞ÊîπÂñÑ‰∫ÜÊëòË¶ÅÂìÅË≥™ÔºåÊñπÊ≥ïÊòØÂà©Áî®Â§öÂÄã LLM ‰æÜÈ©óË≠âËº∏Âá∫ÂìÅË≥™„ÄÇÊàëÂÄëÂú®ÊúÉË≠∞ÊëòË¶ÅÊñπÈù¢Êé°Áî®ÁöÑÂ§ö LLM ÊñπÊ≥ïÔºåÈ°ØÁ§∫‰∫ÜÂú®ÈúÄË¶ÅÁ©©ÂÅ•ÊÄß„ÄÅË°åÂãïË¶èÂäÉÂíåÊúùÂêëÁõÆÊ®ôË®éË´ñÁöÑÈ°û‰ººË§áÈõúÊñáÂ≠óÁîüÊàê‰ªªÂãô‰∏≠ÂÖ∑ÊúâÊΩõÂäõ„ÄÇ

##### **Imitation of human motion achieves natural head movements for humanoid robots in an active-speaker detection task**
2407.11915v1 by Bosong Ding, Murat Kirtay, Giacomo Spigler

Head movements are crucial for social human-human interaction. They can
transmit important cues (e.g., joint attention, speaker detection) that cannot
be achieved with verbal interaction alone. This advantage also holds for
human-robot interaction. Even though modeling human motions through generative
AI models has become an active research area within robotics in recent years,
the use of these methods for producing head movements in human-robot
interaction remains underexplored. In this work, we employed a generative AI
pipeline to produce human-like head movements for a Nao humanoid robot. In
addition, we tested the system on a real-time active-speaker tracking task in a
group conversation setting. Overall, the results show that the Nao robot
successfully imitates human head movements in a natural manner while actively
tracking the speakers during the conversation. Code and data from this study
are available at https://github.com/dingdingding60/Humanoids2024HRI

ÊëòË¶ÅÔºöÈ†≠ÈÉ®Âãï‰ΩúÂ∞çÊñº‰∫∫È°û‰πãÈñìÁöÑÁ§æ‰∫§‰∫íÂãïËá≥ÈóúÈáçË¶Å„ÄÇÂÆÉÂÄëÂèØ‰ª•ÂÇ≥ÈÅîÈáçË¶ÅÁöÑÁ∑öÁ¥¢Ôºà‰æãÂ¶ÇÔºåÂÖ±ÂêåÈóúÊ≥®„ÄÅË™™Ë©±ËÄÖÂÅµÊ∏¨ÔºâÔºåËÄåÂÉÖÈù†Âè£Ë™û‰∫íÂãïÁÑ°Ê≥ïÈÅîÊàê„ÄÇÈÄôÁ®ÆÂÑ™Âã¢‰πüÈÅ©Áî®Êñº‰∫∫Ê©ü‰∫íÂãï„ÄÇÂÑòÁÆ°ËøëÂπ¥‰æÜÈÄèÈÅéÁîüÊàêÂºè AI Ê®°ÂûãÊ®°Êì¨‰∫∫È°ûÂãï‰ΩúÂ∑≤ÊàêÁÇ∫Ê©üÂô®‰∫∫È†òÂüü‰∏≠ÁöÑÊ¥ªË∫çÁ†îÁ©∂È†òÂüüÔºå‰ΩÜÈÄô‰∫õÊñπÊ≥ïÁî®ÊñºÁî¢Áîü‰∫∫Ê©ü‰∫íÂãï‰∏≠ÁöÑÈ†≠ÈÉ®Âãï‰Ωú‰ªçÊú™ÂÖÖÂàÜÊé¢Ë®é„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé°Áî®ÁîüÊàêÂºè AI ÁÆ°Á∑öÁÇ∫ Nao È°û‰∫∫Ê©üÂô®‰∫∫Áî¢ÁîüÈ°û‰ºº‰∫∫È°ûÁöÑÈ†≠ÈÉ®Âãï‰Ωú„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®Áæ§ÁµÑÂ∞çË©±Ë®≠ÂÆö‰∏≠Â∞ç‰∏ÄÂÄãÂç≥ÊôÇ‰∏ªÂãïË™™Ë©±ËÄÖËøΩËπ§‰ªªÂãôÊ∏¨Ë©¶‰∫ÜÈÄôÂÄãÁ≥ªÁµ±„ÄÇÊï¥È´îËÄåË®ÄÔºåÁµêÊûúÈ°ØÁ§∫ Nao Ê©üÂô®‰∫∫Âú®Â∞çË©±ÊúüÈñì‰∏ªÂãïËøΩËπ§Ë™™Ë©±ËÄÖÁöÑÂêåÊôÇÔºåÊàêÂäü‰ª•Ëá™ÁÑ∂ÁöÑÊñπÂºèÊ®°‰ªø‰∫∫È°ûÁöÑÈ†≠ÈÉ®Âãï‰Ωú„ÄÇÊú¨Á†îÁ©∂ÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú® https://github.com/dingdingding60/Humanoids2024HRI ÂèñÂæó

##### **Bridging Weighted First Order Model Counting and Graph Polynomials**
2407.11877v1 by Qipeng Kuang, Ond≈ôej Ku≈æelka, Yuanhong Wang, Yuyi Wang

The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the
weighted sum of models of a given first-order logic sentence over a given
domain. It can be solved in time polynomial in the domain size for sentences
from the two-variable fragment with counting quantifiers, known as $C^2$. This
polynomial-time complexity is also retained when extending $C^2$ by one of the
following axioms: linear order axiom, tree axiom, forest axiom, directed
acyclic graph axiom or connectedness axiom. An interesting question remains as
to which other axioms can be added to the first-order sentences in this way. We
provide a new perspective on this problem by associating WFOMC with graph
polynomials. Using WFOMC, we define Weak Connectedness Polynomial and Strong
Connectedness Polynomials for first-order logic sentences. It turns out that
these polynomials have the following interesting properties. First, they can be
computed in polynomial time in the domain size for sentences from $C^2$.
Second, we can use them to solve WFOMC with all of the existing axioms known to
be tractable as well as with new ones such as bipartiteness, strong
connectedness, being a spanning subgraph, having $k$ connected components, etc.
Third, the well-known Tutte polynomial can be recovered as a special case of
the Weak Connectedness Polynomial, and the Strict and Non-Strict Directed
Chromatic Polynomials can be recovered from the Strong Connectedness
Polynomials, which allows us to show that these important graph polynomials can
be computed in time polynomial in the number of vertices for any graph that can
be encoded by a fixed $C^2$ sentence and a conjunction of an arbitrary number
of ground unary literals.

ÊëòË¶ÅÔºöÂä†Ê¨ä‰∏ÄÈöéÊ®°ÂûãË®àÁÆóÂïèÈ°å (WFOMC) Ë¶ÅÊ±ÇË®àÁÆóÁµ¶ÂÆö‰∏ÄÈöéÈÇèËºØÂè•Â≠êÂú®Áµ¶ÂÆöÁ∂≤Âüü‰∏äÁöÑÊ®°ÂûãÁöÑÂä†Ê¨äÁ∏ΩÂíå„ÄÇÂ∞çÊñºÂÖ∑ÊúâË®àÊï∏ÈáèË©ûÁöÑ‰∫åËÆäÊï∏ÁâáÊÆµÔºàÁ®±ÁÇ∫ $C^2$Ôºâ‰∏≠ÁöÑÂè•Â≠êÔºåÂèØ‰ª•Âú®Â§öÈ†ÖÂºèÊôÇÈñìÂÖßËß£Ê±∫Ê≠§ÂïèÈ°å„ÄÇÁï∂ÈÄöÈÅé‰ª•‰∏ãÂÖ¨ÁêÜ‰πã‰∏Ä‰æÜÊì¥ÂÖÖ $C^2$ ÊôÇÔºåÊ≠§Â§öÈ†ÖÂºèÊôÇÈñìË§áÈõúÂ∫¶‰πüÊúÉ‰øùÁïôÔºöÁ∑öÊÄßÂ∫èÂÖ¨ÁêÜ„ÄÅÊ®πÂÖ¨ÁêÜ„ÄÅÊ£ÆÊûóÂÖ¨ÁêÜ„ÄÅÊúâÂêëÁÑ°Áí∞ÂúñÂÖ¨ÁêÜÊàñÈÄ£ÈÄöÂÖ¨ÁêÜ„ÄÇ‰∏ÄÂÄãÊúâË∂£ÁöÑÂïèÈ°å‰ªçÁÑ∂ÊòØÂì™‰∫õÂÖ∂‰ªñÂÖ¨ÁêÜÂèØ‰ª•ÈÄôÊ®£Ê∑ªÂä†Âà∞‰∏ÄÈöéÂè•Â≠ê‰∏≠„ÄÇÊàëÂÄëÈÄöÈÅéÂ∞á WFOMC ËàáÂúñÂ§öÈ†ÖÂºèÈóúËÅØËµ∑‰æÜÔºåÂ∞çÈÄôÂÄãÂïèÈ°åÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÄÈªû„ÄÇ‰ΩøÁî® WFOMCÔºåÊàëÂÄëÂÆöÁæ©‰∫Ü‰∏ÄÈöéÈÇèËºØÂè•Â≠êÁöÑÂº±ÈÄ£ÈÄöÂ§öÈ†ÖÂºèÂíåÂº∑ÈÄ£ÈÄöÂ§öÈ†ÖÂºè„ÄÇÁµêÊûúË≠âÊòéÔºåÈÄô‰∫õÂ§öÈ†ÖÂºèÂÖ∑Êúâ‰ª•‰∏ãÊúâË∂£ÁöÑÊÄßË≥™„ÄÇÈ¶ñÂÖàÔºåÂÆÉÂÄëÂèØ‰ª•Âú® $C^2$ Âè•Â≠ê‰∏≠Â§öÈ†ÖÂºèÊôÇÈñìÂÖßË®àÁÆóÁ∂≤ÂüüÂ§ßÂ∞è„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÂèØ‰ª•‰ΩøÁî®ÂÆÉÂÄë‰æÜËß£Ê±∫ WFOMCÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÊâÄÊúâÂ∑≤Áü•ÊòìÊñºËôïÁêÜÁöÑÂÖ¨ÁêÜ‰ª•ÂèäÊñ∞ÁöÑÂÖ¨ÁêÜÔºå‰æãÂ¶Ç‰∫åÂàÜÊÄß„ÄÅÂº∑ÈÄ£ÈÄöÊÄß„ÄÅ‰ΩúÁÇ∫ÁîüÊàêÂ≠êÂúñ„ÄÅÂÖ∑Êúâ $k$ ÂÄãÈÄ£ÈÄöÁµÑ‰ª∂Á≠â„ÄÇÁ¨¨‰∏âÔºåÁúæÊâÄÂë®Áü•ÁöÑ Tutte Â§öÈ†ÖÂºèÂèØ‰ª•‰ΩúÁÇ∫Âº±ÈÄ£ÈÄöÂ§öÈ†ÖÂºèÁöÑÁâπ‰æãÊÅ¢Âæ©ÔºåËÄåÂö¥Ê†ºÂíåÈùûÂö¥Ê†ºÊúâÂêëËâ≤Â§öÈ†ÖÂºèÂèØ‰ª•ÂæûÂº∑ÈÄ£ÈÄöÂ§öÈ†ÖÂºè‰∏≠ÊÅ¢Âæ©ÔºåÈÄôËÆìÊàëÂÄëÂèØ‰ª•Ë≠âÊòéÈÄô‰∫õÈáçË¶ÅÁöÑÂúñÂ§öÈ†ÖÂºèÂèØ‰ª•Âú®Â§öÈ†ÖÂºèÊôÇÈñìÂÖßË®àÁÆó‰ªª‰ΩïÂúñÁöÑÈ†ÇÈªûÊï∏ÔºåË©≤ÂúñÂèØ‰ª•Áî®Âõ∫ÂÆöÁöÑ $C^2$ Âè•Â≠êÂíå‰ªªÊÑèÊï∏ÈáèÁöÑÂü∫Êú¨‰∏ÄÂÖÉÊñáÂ≠óÁöÑÂêàÂèñ‰æÜÁ∑®Á¢º„ÄÇ

##### **Evaluating Task-Oriented Dialogue Consistency through Constraint Satisfaction**
2407.11857v1 by Tiziano Labruna, Bernardo Magnini

Task-oriented dialogues must maintain consistency both within the dialogue
itself, ensuring logical coherence across turns, and with the conversational
domain, accurately reflecting external knowledge. We propose to conceptualize
dialogue consistency as a Constraint Satisfaction Problem (CSP), wherein
variables represent segments of the dialogue referencing the conversational
domain, and constraints among variables reflect dialogue properties, including
linguistic, conversational, and domain-based aspects. To demonstrate the
feasibility of the approach, we utilize a CSP solver to detect inconsistencies
in dialogues re-lexicalized by an LLM. Our findings indicate that: (i) CSP is
effective to detect dialogue inconsistencies; and (ii) consistent dialogue
re-lexicalization is challenging for state-of-the-art LLMs, achieving only a
0.15 accuracy rate when compared to a CSP solver. Furthermore, through an
ablation study, we reveal that constraints derived from domain knowledge pose
the greatest difficulty in being respected. We argue that CSP captures core
properties of dialogue consistency that have been poorly considered by
approaches based on component pipelines.

ÊëòË¶ÅÔºö‰ªªÂãôÂ∞éÂêëÂ∞çË©±ÂøÖÈ†àÂú®Â∞çË©±Êú¨Ë∫´‰∏≠‰øùÊåÅ‰∏ÄËá¥ÊÄßÔºåÁ¢∫‰øùËº™ÊµÅÁöÑÈÇèËºØÈÄ£Ë≤´ÊÄßÔºå‰∏¶ËàáÂ∞çË©±È†òÂüü‰∏ÄËá¥ÔºåÊ∫ñÁ¢∫ÂèçÊò†Â§ñÈÉ®Áü•Ë≠ò„ÄÇÊàëÂÄëÂª∫Ë≠∞Â∞áÂ∞çË©±‰∏ÄËá¥ÊÄßÊ¶ÇÂøµÂåñÁÇ∫Á¥ÑÊùüÊªøË∂≥ÂïèÈ°å (CSP)ÔºåÂÖ∂‰∏≠ËÆäÊï∏‰ª£Ë°®Â∞çË©±‰∏≠ÂèÉËÄÉÂ∞çË©±È†òÂüüÁöÑÂçÄÊÆµÔºåËÄåËÆäÊï∏‰πãÈñìÁöÑÁ¥ÑÊùüÂèçÊò†Â∞çË©±Â±¨ÊÄßÔºåÂåÖÊã¨Ë™ûË®Ä„ÄÅÂ∞çË©±ÂíåÂü∫ÊñºÈ†òÂüüÁöÑÊñπÈù¢„ÄÇÁÇ∫‰∫ÜË≠âÊòéÈÄôÁ®ÆÊñπÊ≥ïÁöÑÂèØË°åÊÄßÔºåÊàëÂÄëÂà©Áî® CSP Ê±ÇËß£Âô®‰æÜÊ™¢Ê∏¨ LLM ÈáçÊñ∞Ë©ûÂΩôÂåñÁöÑÂ∞çË©±‰∏≠ÁöÑ‰∏ç‰∏ÄËá¥ÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºö(i) CSP ÂèØÊúâÊïàÊ™¢Ê∏¨Â∞çË©±‰∏ç‰∏ÄËá¥ÊÄßÔºõ(ii) Â∞çÊñºÊúÄÂÖàÈÄ≤ÁöÑ LLM ‰æÜË™™Ôºå‰∏ÄËá¥ÁöÑÂ∞çË©±ÈáçÊñ∞Ë©ûÂΩôÂåñÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåËàá CSP Ê±ÇËß£Âô®Áõ∏ÊØîÔºåÂÉÖÈÅîÂà∞ 0.15 ÁöÑÊ∫ñÁ¢∫Áéá„ÄÇÊ≠§Â§ñÔºåÈÄöÈÅéÊ∂àËûçÁ†îÁ©∂ÔºåÊàëÂÄëÁôºÁèæÊ∫êËá™È†òÂüüÁü•Ë≠òÁöÑÁ¥ÑÊùüÊúÄÈõ£Ë¢´ÈÅµÂÆà„ÄÇÊàëÂÄëË™çÁÇ∫ÔºåCSP ÊçïÊçâ‰∫ÜÂ∞çË©±‰∏ÄËá¥ÊÄßÁöÑÊ†∏ÂøÉÂ±¨ÊÄßÔºåËÄåÂü∫ÊñºÁµÑ‰ª∂ÁÆ°Á∑öÁöÑÊñπÊ≥ïÂ∞çÊ≠§ËÄÉÊÖÆ‰∏çÂë®„ÄÇ

##### **Scaling Sign Language Translation**
2407.11855v1 by Biao Zhang, Garrett Tanzer, Orhan Firat

Sign language translation (SLT) addresses the problem of translating
information from a sign language in video to a spoken language in text.
Existing studies, while showing progress, are often limited to narrow domains
and/or few sign languages and struggle with open-domain tasks. In this paper,
we push forward the frontier of SLT by scaling pretraining data, model size,
and number of translation directions. We perform large-scale SLT pretraining on
different data including 1) noisy multilingual YouTube SLT data, 2) parallel
text corpora, and 3) SLT data augmented by translating video captions to other
languages with off-the-shelf machine translation models. We unify different
pretraining tasks with task-specific prompts under the encoder-decoder
architecture, and initialize the SLT model with pretrained (m/By)T5 models
across model sizes. SLT pretraining results on How2Sign and FLEURS-ASL#0 (ASL
to 42 spoken languages) demonstrate the significance of data/model scaling and
cross-lingual cross-modal transfer, as well as the feasibility of zero-shot
SLT. We finetune the pretrained SLT models on 5 downstream open-domain SLT
benchmarks covering 5 sign languages. Experiments show substantial quality
improvements over the vanilla baselines, surpassing the previous
state-of-the-art (SOTA) by wide margins.

ÊëòË¶ÅÔºöÊâãË™ûÁøªË≠Ø (SLT) Ëß£Ê±∫‰∫ÜÂ∞áÂΩ±Áâá‰∏≠ÁöÑÊâãË™ûË≥áË®äÁøªË≠ØÊàêÊñáÂ≠ó‰∏≠ÁöÑÂè£Ë™ûÂïèÈ°å„ÄÇÁèæÊúâÁ†îÁ©∂ÈõñÁÑ∂È°ØÁ§∫ÈÄ≤Â±ïÔºå‰ΩÜÈÄöÂ∏∏ÂÉÖÈôêÊñºÁãπÁ™ÑÁöÑÈ†òÂüüÂíå/ÊàñÂ∞ëÊï∏ÊâãË™ûÔºå‰∏îÈõ£‰ª•Êáâ‰ªòÈñãÊîæÈ†òÂüü‰ªªÂãô„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÊì¥ÂÖÖÈ†êË®ìÁ∑¥Ë≥áÊñô„ÄÅÊ®°ÂûãÂ§ßÂ∞èÂíåÁøªË≠ØÊñπÂêëÊï∏ÈáèÔºåÊé®Âãï SLT ÁöÑÂâçÊ≤ø„ÄÇÊàëÂÄëÂ∞ç‰∏çÂêåË≥áÊñôÂü∑Ë°åÂ§ßË¶èÊ®° SLT È†êË®ìÁ∑¥ÔºåÂÖ∂‰∏≠ÂåÖÊã¨ 1) ÂòàÈõúÁöÑÂ§öË™ûË®Ä YouTube SLT Ë≥áÊñô„ÄÅ2) Âπ≥Ë°åÊñáÂ≠óË™ûÊñôÂ∫´Ôºå‰ª•Âèä 3) ÈÄèÈÅé‰ΩøÁî®ÁèæÊàêÁöÑÊ©üÂô®ÁøªË≠ØÊ®°ÂûãÂ∞áÂΩ±ÁâáÂ≠óÂπïÁøªË≠ØÊàêÂÖ∂‰ªñË™ûË®ÄËÄåÊì¥ÂÖÖÁöÑ SLT Ë≥áÊñô„ÄÇÊàëÂÄëÂú®Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Êû∂Êßã‰∏ãÔºå‰ΩøÁî®ÁâπÂÆöÊñº‰ªªÂãôÁöÑÊèêÁ§∫Áµ±‰∏Ä‰∏çÂêåÁöÑÈ†êË®ìÁ∑¥‰ªªÂãôÔºå‰∏¶‰ΩøÁî®Ë∑®Ê®°ÂûãÂ§ßÂ∞èÁöÑÈ†êË®ìÁ∑¥ (m/By)T5 Ê®°ÂûãÂàùÂßãÂåñ SLT Ê®°Âûã„ÄÇHow2Sign Âíå FLEURS-ASL#0 (ASL Âà∞ 42 Á®ÆÂè£Ë™û) ‰∏äÁöÑ SLT È†êË®ìÁ∑¥ÁµêÊûúË≠âÊòé‰∫ÜË≥áÊñô/Ê®°ÂûãÊì¥ÂÖÖÂíåË∑®Ë™ûË®ÄË∑®Ê®°ÂºèËΩâÁßªÁöÑÈáçË¶ÅÊÄßÔºå‰ª•ÂèäÈõ∂Ê¨°Â≠∏Áøí SLT ÁöÑÂèØË°åÊÄß„ÄÇÊàëÂÄëÂæÆË™øÈ†êË®ìÁ∑¥ÁöÑ SLT Ê®°ÂûãÔºåÈáùÂ∞çÊ∂µËìã 5 Á®ÆÊâãË™ûÁöÑ 5 ÂÄã‰∏ãÊ∏∏ÈñãÊîæÈ†òÂüü SLT Ë©ïÈáèÊ®ôÊ∫ñÈÄ≤Ë°åÂæÆË™ø„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåËàáÈ¶ôËçâÂü∫Á∑öÁõ∏ÊØîÊúâÈ°ØËëóÁöÑÂìÅË≥™ÊèêÂçáÔºåÂ§ßÂπÖË∂ÖË∂äÂÖàÂâçÁöÑÊäÄË°ìÊ∞¥Ê∫ñ (SOTA)„ÄÇ

##### **Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection**
2407.11854v1 by Gaetan Lopez Latouche, Marc-Andr√© Carbonneau, Ben Swanson

Grammatical Error Detection (GED) methods rely heavily on human annotated
error corpora. However, these annotations are unavailable in many low-resource
languages. In this paper, we investigate GED in this context. Leveraging the
zero-shot cross-lingual transfer capabilities of multilingual pre-trained
language models, we train a model using data from a diverse set of languages to
generate synthetic errors in other languages. These synthetic error corpora are
then used to train a GED model. Specifically we propose a two-stage fine-tuning
pipeline where the GED model is first fine-tuned on multilingual synthetic data
from target languages followed by fine-tuning on human-annotated GED corpora
from source languages. This approach outperforms current state-of-the-art
annotation-free GED methods. We also analyse the errors produced by our method
and other strong baselines, finding that our approach produces errors that are
more diverse and more similar to human errors.

ÊëòË¶ÅÔºöË™ûÊ≥ïÈåØË™§ÂÅµÊ∏¨ (GED) ÊñπÊ≥ïÊ•µÂ∫¶‰æùË≥¥‰∫∫Â∑•Ê®ôË®ªÁöÑÈåØË™§Ë™ûÊñôÂ∫´„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®ôË®ªÂú®Ë®±Â§ö‰ΩéË≥áÊ∫êË™ûË®Ä‰∏≠‰∏¶‰∏çÂèØÁî®„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÁ†îÁ©∂Ê≠§ËÑàÁµ°‰∏≠ÁöÑ GED„ÄÇÂà©Áî®Â§öË™ûË®ÄÈ†êË®ìÁ∑¥Ë™ûË®ÄÊ®°ÂûãÁöÑÈõ∂Ê¨°Â≠∏ÁøíË∑®Ë™ûË®ÄËΩâÁßªËÉΩÂäõÔºåÊàëÂÄë‰ΩøÁî®‰æÜËá™ÂêÑÁ®ÆË™ûË®ÄÁöÑË≥áÊñôË®ìÁ∑¥‰∏ÄÂÄãÊ®°ÂûãÔºå‰ª•Áî¢ÁîüÂÖ∂‰ªñË™ûË®Ä‰∏≠ÁöÑÂêàÊàêÈåØË™§„ÄÇÈÄô‰∫õÂêàÊàêÈåØË™§Ë™ûÊñôÂ∫´Êé•ËëóÁî®ÊñºË®ìÁ∑¥ GED Ê®°Âûã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂÖ©ÈöéÊÆµÂæÆË™øÁÆ°ÈÅìÔºåÂÖ∂‰∏≠ GED Ê®°ÂûãÈ¶ñÂÖàÈáùÂ∞çÁõÆÊ®ôË™ûË®ÄÁöÑÂ§öË™ûË®ÄÂêàÊàêË≥áÊñôÈÄ≤Ë°åÂæÆË™øÔºåÁÑ∂ÂæåÈáùÂ∞ç‰æÜÊ∫êË™ûË®ÄÁöÑ‰∫∫Â∑•Ê®ôË®ª GED Ë™ûÊñôÂ∫´ÈÄ≤Ë°åÂæÆË™ø„ÄÇÊ≠§ÊñπÊ≥ïÂÑ™ÊñºÁõÆÂâçÊúÄÂÖàÈÄ≤ÁöÑÁÑ°Ê®ôË®ª GED ÊñπÊ≥ï„ÄÇÊàëÂÄë‰πüÂàÜÊûêÊàëÂÄëÁöÑÊ®°ÂûãÂíåÂÖ∂‰ªñÁöÑÂº∑Â§ßÂü∫Ê∫ñÊâÄÁî¢ÁîüÁöÑÈåØË™§ÔºåÁôºÁèæÊàëÂÄëÁöÑÊ®°ÂûãÊâÄÁî¢ÁîüÁöÑÈåØË™§Êõ¥Â§öÂÖÉ‰∏îÊõ¥È°û‰ººÊñº‰∫∫È°ûÁöÑÈåØË™§„ÄÇ

##### **Schema Matching with Large Language Models: an Experimental Study**
2407.11852v1 by Marcel Parciak, Brecht Vandevoort, Frank Neven, Liesbet M. Peeters, Stijn Vansummeren

Large Language Models (LLMs) have shown useful applications in a variety of
tasks, including data wrangling. In this paper, we investigate the use of an
off-the-shelf LLM for schema matching. Our objective is to identify semantic
correspondences between elements of two relational schemas using only names and
descriptions. Using a newly created benchmark from the health domain, we
propose different so-called task scopes. These are methods for prompting the
LLM to do schema matching, which vary in the amount of context information
contained in the prompt. Using these task scopes we compare LLM-based schema
matching against a string similarity baseline, investigating matching quality,
verification effort, decisiveness, and complementarity of the approaches. We
find that matching quality suffers from a lack of context information, but also
from providing too much context information. In general, using newer LLM
versions increases decisiveness. We identify task scopes that have acceptable
verification effort and succeed in identifying a significant number of true
semantic matches. Our study shows that LLMs have potential in bootstrapping the
schema matching process and are able to assist data engineers in speeding up
this task solely based on schema element names and descriptions without the
need for data instances.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÊúâÁî®ÁöÑÊáâÁî®ÔºåÂåÖÊã¨Ë≥áÊñôÊï¥ÁêÜ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÁèæÊàê LLM Âú®Êû∂ÊßãÊØîÂ∞ç‰∏≠ÁöÑÁî®ÈÄî„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÂÉÖ‰ΩøÁî®ÂêçÁ®±ÂíåÊèèËø∞ÔºåÊâæÂá∫ÂÖ©ÂÄãÈóúËÅØÂºèÊû∂ÊßãÁöÑÂÖÉÁ¥†‰πãÈñìÁöÑË™ûÊÑèÂ∞çÊáâ„ÄÇ‰ΩøÁî®ÂæûÂÅ•Â∫∑È†òÂüüÊñ∞Âª∫Á´ãÁöÑÂü∫Ê∫ñÔºåÊàëÂÄëÊèêÂá∫‰∏çÂêåÁöÑÊâÄË¨Ç‰ªªÂãôÁØÑÂúç„ÄÇÈÄô‰∫õÊñπÊ≥ïÊòØÁî®ÊñºÊèêÁ§∫ LLM ÈÄ≤Ë°åÊû∂ÊßãÊØîÂ∞çÔºåÂÖ∂ÂåÖÂê´Âú®ÊèêÁ§∫‰∏≠ÁöÑËÑàÁµ°Ë≥áË®äÈáèÊúâÊâÄ‰∏çÂêå„ÄÇ‰ΩøÁî®ÈÄô‰∫õ‰ªªÂãôÁØÑÂúçÔºåÊàëÂÄëÂ∞áÂü∫Êñº LLM ÁöÑÊû∂ÊßãÊØîÂ∞çËàáÂ≠ó‰∏≤Áõ∏‰ººÊÄßÂü∫Ê∫ñÈÄ≤Ë°åÊØîËºÉÔºåÊé¢Ë®éÊØîÂ∞çÂìÅË≥™„ÄÅÈ©óË≠âÂ∑•‰Ωú„ÄÅÊûúÊñ∑ÊÄßÔºå‰ª•ÂèäÊñπÊ≥ïÁöÑ‰∫íË£úÊÄß„ÄÇÊàëÂÄëÁôºÁèæÊØîÂ∞çÂìÅË≥™ÊúÉÂèóÂà∞ËÑàÁµ°Ë≥áË®ä‰∏çË∂≥‰ª•ÂèäÊèê‰æõÈÅéÂ§öËÑàÁµ°Ë≥áË®äÁöÑÂΩ±Èüø„ÄÇ‰∏ÄËà¨‰æÜË™™Ôºå‰ΩøÁî®ËºÉÊñ∞ÁöÑ LLM ÁâàÊú¨ÊúÉÂ¢ûÂä†ÊûúÊñ∑ÊÄß„ÄÇÊàëÂÄëÊâæÂá∫ÂÖ∑ÊúâÂèØÊé•ÂèóÈ©óË≠âÂ∑•‰ΩúÔºå‰∏¶ÊàêÂäüÊâæÂá∫Â§ßÈáèÁúüÂØ¶Ë™ûÊÑèÊØîÂ∞çÁöÑ‰ªªÂãôÁØÑÂúç„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂È°ØÁ§∫ÔºåLLM ÊúâÂä©ÊñºÂºïÂ∞éÊû∂ÊßãÊØîÂ∞çÊµÅÁ®ãÔºå‰∏¶‰∏îËÉΩÂ§†ÂçîÂä©Ë≥áÊñôÂ∑•Á®ãÂ∏´ÂÉÖÊ†πÊìöÊû∂ÊßãÂÖÉÁ¥†ÂêçÁ®±ÂíåÊèèËø∞Âä†ÈÄüÊ≠§‰ªªÂãôÔºåËÄå‰∏çÈúÄË¶ÅË≥áÊñôÂØ¶‰æã„ÄÇ

##### **Variational Randomized Smoothing for Sample-Wise Adversarial Robustness**
2407.11844v1 by Ryo Hase, Ye Wang, Toshiaki Koike-Akino, Jing Liu, Kieran Parsons

Randomized smoothing is a defensive technique to achieve enhanced robustness
against adversarial examples which are small input perturbations that degrade
the performance of neural network models. Conventional randomized smoothing
adds random noise with a fixed noise level for every input sample to smooth out
adversarial perturbations. This paper proposes a new variational framework that
uses a per-sample noise level suitable for each input by introducing a noise
level selector. Our experimental results demonstrate enhancement of empirical
robustness against adversarial attacks. We also provide and analyze the
certified robustness for our sample-wise smoothing method.

ÊëòË¶ÅÔºöÈö®Ê©üÂπ≥ÊªëÊòØ‰∏ÄÁ®ÆÈò≤Á¶¶ÊäÄË°ìÔºåÁî®ÊñºÂ¢ûÂº∑Â∞çÊäóÁØÑ‰æãÁöÑÈ≠ØÊ£íÊÄßÔºåËÄåÂ∞çÊäóÁØÑ‰æãÊòØÂ∞çÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÁöÑÊïàËÉΩÈÄ†ÊàêÊêçÂÆ≥ÁöÑÂ∞èÂûãËº∏ÂÖ•ÊìæÂãï„ÄÇÂÇ≥Áµ±Èö®Ê©üÂπ≥ÊªëÊúÉÈáùÂ∞çÊØèÂÄãËº∏ÂÖ•Ê®£Êú¨Âä†ÂÖ•Âõ∫ÂÆöÈõúË®äÂ±§Á¥öÁöÑÈö®Ê©üÈõúË®äÔºå‰ª•Âπ≥ÊªëÂ∞çÊäóÊìæÂãï„ÄÇÊú¨ÊñáÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑËÆäÂàÜÊû∂ÊßãÔºåÈÄèÈÅéÂºïÂÖ•ÈõúË®äÂ±§Á¥öÈÅ∏ÊìáÂô®Ôºå‰ΩøÁî®ÈÅ©ÂêàÊØèÂÄãËº∏ÂÖ•ÁöÑÊØèÂÄãÊ®£Êú¨ÈõúË®äÂ±§Á¥ö„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÂ∞çÊäóÊîªÊìäÁöÑÁ∂ìÈ©óÈ≠ØÊ£íÊÄßÂæóÂà∞Â¢ûÂº∑„ÄÇÊàëÂÄë‰πüÊèê‰æõ‰∏¶ÂàÜÊûê‰∫ÜÊàëÂÄëÊ®£Êú¨Âπ≥ÊªëÊñπÊ≥ïÁöÑË™çË≠âÈ≠ØÊ£íÊÄß„ÄÇ

##### **InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback**
2407.11843v1 by Haishuo Fang, Xiaodan Zhu, Iryna Gurevych

A crucial requirement for deploying LLM-based agents in real-life
applications is robustness against risky or irreversible mistakes. However,
existing research lacks a focus on the preemptive evaluation of reasoning
trajectories performed by LLM agents, leading to a gap in ensuring safe and
reliable operations. To explore better solutions, this paper introduces
InferAct, a novel approach that leverages the Theory-of-Mind capability of LLMs
to proactively detect potential errors before critical actions are executed
(e.g., "buy-now" in automatic online trading or web shopping). InferAct is also
capable of integrating human feedback to prevent irreversible risks and enhance
the actor agent's decision-making process. Experiments on three widely used
tasks demonstrate the effectiveness of InferAct. The proposed solution presents
a novel approach and concrete contributions toward developing LLM agents that
can be safely deployed in different environments involving critical
decision-making.

ÊëòË¶ÅÔºöÈÉ®ÁΩ≤Âü∫Êñº LLM ÁöÑ‰ª£ÁêÜËá≥ÂØ¶ÈöõÊáâÁî®ÊôÇÔºåÈóúÈçµÈúÄÊ±Ç‰πã‰∏ÄÊòØËÉΩÊäµÁ¶¶È¢®Èö™Êàñ‰∏çÂèØÈÄÜËΩâÁöÑÈåØË™§„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁ†îÁ©∂Áº∫‰πèÂ∞ç LLM ‰ª£ÁêÜÂü∑Ë°åÊé®ÁêÜËªåË∑°ÁöÑÂÖàÂà∂Ë©ï‰º∞ÁöÑÈáçÈªûÔºåÂ∞éËá¥Á¢∫‰øùÂÆâÂÖ®ÂíåÂèØÈù†ÈÅã‰ΩúÂ≠òÂú®Â∑ÆË∑ù„ÄÇÁÇ∫‰∫ÜÊé¢Á¥¢Êõ¥Â•ΩÁöÑËß£Ê±∫ÊñπÊ°àÔºåÊú¨Êñá‰ªãÁ¥π InferActÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÂÆÉÂà©Áî® LLM ÁöÑÂøÉÊô∫ÁêÜË´ñËÉΩÂäõÂú®Âü∑Ë°åÈóúÈçµÂãï‰Ωú‰πãÂâç‰∏ªÂãïÂÅµÊ∏¨ÊΩõÂú®ÈåØË™§Ôºà‰æãÂ¶ÇÔºåËá™ÂãïÁ∑ö‰∏ä‰∫§ÊòìÊàñÁ∂≤Ë∑ØË≥ºÁâ©ÁöÑ„ÄåÁ´ãÂç≥Ë≥ºË≤∑„ÄçÔºâ„ÄÇInferAct ‰πüËÉΩÊï¥Âêà‰∫∫È°ûÂõûÈ•ã‰ª•Èò≤Ê≠¢‰∏çÂèØÈÄÜËΩâÁöÑÈ¢®Èö™Ôºå‰∏¶Â¢ûÂº∑Ë°åÁÇ∫‰ª£ÁêÜÁöÑÊ±∫Á≠ñÈÅéÁ®ã„ÄÇÂú®‰∏âÂÄãÂª£Ê≥õ‰ΩøÁî®ÁöÑ‰ªªÂãô‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óË≠âÊòé‰∫Ü InferAct ÁöÑÊúâÊïàÊÄß„ÄÇÊâÄÊèêÂá∫ÁöÑËß£Ê±∫ÊñπÊ°àÂëàÁèæÂá∫‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºå‰∏¶ÁÇ∫ÈñãÁôº LLM ‰ª£ÁêÜÂÅöÂá∫ÂÖ∑È´îË≤¢ÁçªÔºåÈÄô‰∫õ‰ª£ÁêÜËÉΩÂÆâÂÖ®Âú∞ÈÉ®ÁΩ≤Âú®Ê∂âÂèäÈóúÈçµÊ±∫Á≠ñÁöÑ‰∏çÂêåÁí∞Â¢É‰∏≠„ÄÇ

##### **LoFTI: Localization and Factuality Transfer to Indian Locales**
2407.11833v1 by Sona Elza Simon, Soumen Kumar Mondal, Abhishek Singhania, Sayambhu Sen, Preethi Jyothi

Large language models (LLMs) encode vast amounts of world knowledge acquired
via training on large web-scale datasets crawled from the internet. However,
these datasets typically exhibit a geographical bias towards English-speaking
Western countries. This results in LLMs producing biased or hallucinated
responses to queries that require answers localized to other geographical
regions. In this work, we introduce a new benchmark named LoFTI (Localization
and Factuality Transfer to Indian Locales) that can be used to evaluate an
LLM's localization and factual text transfer capabilities. LoFTI consists of
factual statements about entities in source and target locations; the source
locations are spread across the globe and the target locations are all within
India with varying degrees of hyperlocality (country, states, cities). The
entities span a wide variety of categories. We use LoFTI to evaluate Mixtral,
GPT-4 and two other Mixtral-based approaches well-suited to the task of
localized factual transfer. We demonstrate that LoFTI is a high-quality
evaluation benchmark and all the models, including GPT-4, produce skewed
results across varying levels of hyperlocality.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á∑®Á¢º‰∫ÜÂ§ßÈáè‰∏ñÁïåÁü•Ë≠òÔºåÈÄô‰∫õÁü•Ë≠òÊòØÈÄèÈÅéË®ìÁ∑¥Â§ßÂûãÁ∂≤Ë∑ØË¶èÊ®°Ë≥áÊñôÈõÜÔºàÂæûÁ∂≤ÈöõÁ∂≤Ë∑ØÁà¨ÂèñÔºâËÄåÁç≤ÂæóÁöÑ„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õË≥áÊñôÈõÜÈÄöÂ∏∏ÊúÉÂ∞çËã±Ë™ûÁ≥ªË•øÊñπÂúãÂÆ∂Ë°®ÁèæÂá∫Âú∞ÁêÜÂÅèË¶ã„ÄÇÈÄôÂ∞éËá¥ LLM Â∞çÈúÄË¶ÅÂ∞áÁ≠îÊ°àÂú®Âú∞ÁêÜÂçÄÂüüÊú¨Âú∞ÂåñÁöÑÊü•Ë©¢Áî¢ÁîüÊúâÂÅèË¶ãÊàñËôõÊßãÁöÑÂõûÊáâ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂêçÁÇ∫ LoFTIÔºàÊú¨Âú∞ÂåñÂíå‰∫ãÂØ¶ÂÇ≥Ëº∏Âà∞Âç∞Â∫¶Êú¨Âú∞ÔºâÁöÑÊñ∞Âü∫Ê∫ñÔºåÂèØÁî®ÊñºË©ï‰º∞ LLM ÁöÑÊú¨Âú∞ÂåñÂíå‰∫ãÂØ¶ÊñáÊú¨ÂÇ≥Ëº∏ËÉΩÂäõ„ÄÇLoFTI ÂåÖÂê´ÈóúÊñº‰æÜÊ∫êÂíåÁõÆÊ®ô‰ΩçÁΩÆÂØ¶È´îÁöÑ‰∫ãÂØ¶Èô≥Ëø∞Ôºõ‰æÜÊ∫ê‰ΩçÁΩÆÈÅçÂ∏ÉÂÖ®ÁêÉÔºåËÄåÁõÆÊ®ô‰ΩçÁΩÆÂùáÂú®Âç∞Â∫¶Â¢ÉÂÖßÔºåÂÖ∑Êúâ‰∏çÂêåÁ®ãÂ∫¶ÁöÑË∂ÖÂú∞ÊñπÊÄßÔºàÂúãÂÆ∂„ÄÅÂ∑û„ÄÅÂüéÂ∏ÇÔºâ„ÄÇÂØ¶È´îÊ∂µËìã‰∫ÜÂª£Ê≥õÁöÑÈ°ûÂà•„ÄÇÊàëÂÄë‰ΩøÁî® LoFTI ‰æÜË©ï‰º∞ Mixtral„ÄÅGPT-4 ÂíåÂÖ∂‰ªñÂÖ©Á®ÆÈùûÂ∏∏ÈÅ©ÂêàÊñºÊú¨Âú∞Âåñ‰∫ãÂØ¶ÂÇ≥Ëº∏‰ªªÂãôÁöÑÂü∫Êñº Mixtral ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëË≠âÊòé LoFTI ÊòØÂÄãÈ´òÂìÅË≥™ÁöÑË©ï‰º∞Âü∫Ê∫ñÔºåÂåÖÊã¨ GPT-4 Âú®ÂÖßÁöÑÊâÄÊúâÊ®°ÂûãÂú®‰∏çÂêåÂ±§Á¥öÁöÑË∂ÖÂú∞ÊñπÊÄß‰∏≠ÈÉΩÁî¢Áîü‰∫ÜÂÅèÊñúÁöÑÁµêÊûú„ÄÇ

##### **Personalized Conversational Travel Assistant powered by Generative AI**
2407.11830v1 by Alexio Cassani, Michele Ruberl, Antonio Salis, Giacomo Giannese, Gianluca Boanelli

The Tourism and Destination Management Organization (DMO) industry is rapidly
evolving to adapt to new technologies and traveler expectations. Generative
Artificial Intelligence (AI) offers an astonishing and innovative opportunity
to enhance the tourism experience by providing personalized, interactive and
engaging assistance. In this article, we propose a generative AI-based chatbot
for tourism assistance. The chatbot leverages AI ability to generate realistic
and creative texts, adopting the friendly persona of the well-known Italian
all-knowledgeable aunties, to provide tourists with personalized information,
tailored and dynamic pre, during and post recommendations and trip plans and
personalized itineraries, using both text and voice commands, and supporting
different languages to satisfy Italian and foreign tourists expectations. This
work is under development in the Molise CTE research project, funded by the
Italian Minister of the Economic Growth (MIMIT), with the aim to leverage the
best emerging technologies available, such as Cloud and AI to produce state of
the art solutions in the Smart City environment.

ÊëòË¶ÅÔºöÊóÖÈÅäÂíåÁõÆÁöÑÂú∞ÁÆ°ÁêÜÁµÑÁπî (DMO) Áî¢Ê•≠Âø´ÈÄüÊºîÈÄ≤Ôºå‰ª•ÈÅ©ÊáâÊñ∞ÁßëÊäÄÂíåÊóÖÂÆ¢ÊúüÊúõ„ÄÇÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖß (AI) Êèê‰æõÈ©ö‰∫∫ÁöÑÂâµÊñ∞Ê©üÊúÉÔºåÈÄèÈÅéÊèê‰æõÂÄã‰∫∫Âåñ„ÄÅ‰∫íÂãï‰∏îÂºï‰∫∫ÂÖ•ÂãùÁöÑÂçîÂä©Ôºå‰æÜÊèêÂçáÊóÖÈÅäÈ´îÈ©ó„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁî®ÊñºÊóÖÈÅäÂçîÂä©ÁöÑÁîüÊàêÂºè AI ËÅäÂ§©Ê©üÂô®‰∫∫„ÄÇËÅäÂ§©Ê©üÂô®‰∫∫Âà©Áî® AI Áî¢ÁîüÈÄºÁúü‰∏îÂÖ∑ÂâµÊÑèÁöÑÊñáÂ≠óÔºåÊé°Áî®ËëóÂêçÁöÑÁæ©Â§ßÂà©Ëê¨‰∫ãÈÄöÈòøÂß®ÁöÑÂèãÂñÑËßíËâ≤Ôºå‰ΩøÁî®ÊñáÂ≠óÂíåË™ûÈü≥Êåá‰ª§ÁÇ∫ÊóÖÂÆ¢Êèê‰æõÂÄã‰∫∫ÂåñË≥áË®ä„ÄÅÈáèË∫´ÊâìÈÄ†‰∏îÂãïÊÖãÁöÑÊóÖÈÅäÂª∫Ë≠∞ÂíåË°åÁ®ãË¶èÂäÉÔºå‰ª•ÂèäÂÄã‰∫∫ÂåñË°åÁ®ãÔºå‰∏¶ÊîØÊè¥Â§öÂúãË™ûË®Ä‰ª•ÊªøË∂≥Áæ©Â§ßÂà©ÂíåÂ§ñÂúãÊóÖÂÆ¢ÁöÑÊúüÊúõ„ÄÇÊ≠§È†ÖÂ∑•‰ΩúÁõÆÂâçÂú®Áî±Áæ©Â§ßÂà©Á∂ìÊøüÊàêÈï∑ÈÉ® (MIMIT) Ë≥áÂä©ÁöÑ Molise CTE Á†îÁ©∂Ë®àÁï´‰∏≠ÈÄ≤Ë°åÔºåÁõÆÊ®ôÊòØÂà©Áî®Èõ≤Á´ØÂíå AI Á≠âÊúÄ‰Ω≥Êñ∞ËààÁßëÊäÄÔºåÂú®Êô∫ÊÖßÂüéÂ∏ÇÁí∞Â¢É‰∏≠Áî¢ÁîüÊúÄÂÖàÈÄ≤ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇ

##### **GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text**
2407.11827v1 by Kyle Hamilton, Luca Longo, Bojan Bozic

While the use of machine learning for the detection of propaganda techniques
in text has garnered considerable attention, most approaches focus on
"black-box" solutions with opaque inner workings. Interpretable approaches
provide a solution, however, they depend on careful feature engineering and
costly expert annotated data. Additionally, language features specific to
propagandistic text are generally the focus of rhetoricians or linguists, and
there is no data set labeled with such features suitable for machine learning.
This study codifies 22 rhetorical and linguistic features identified in
literature related to the language of persuasion for the purpose of annotating
an existing data set labeled with propaganda techniques. To help human experts
annotate natural language sentences with these features, RhetAnn, a web
application, was specifically designed to minimize an otherwise considerable
mental effort. Finally, a small set of annotated data was used to fine-tune
GPT-3.5, a generative large language model (LLM), to annotate the remaining
data while optimizing for financial cost and classification accuracy. This
study demonstrates how combining a small number of human annotated examples
with GPT can be an effective strategy for scaling the annotation process at a
fraction of the cost of traditional annotation relying solely on human experts.
The results are on par with the best performing model at the time of writing,
namely GPT-4, at 10x less the cost. Our contribution is a set of features,
their properties, definitions, and examples in a machine-readable format, along
with the code for RhetAnn and the GPT prompts and fine-tuning procedures for
advancing state-of-the-art interpretable propaganda technique detection.

ÊëòË¶ÅÔºö<paragraph>ÂÑòÁÆ°‰ΩøÁî®Ê©üÂô®Â≠∏Áøí‰æÜÂÅµÊ∏¨ÂÆ£ÂÇ≥ÊäÄÂ∑ßÂú®ÊñáÊú¨‰∏≠Â∑≤Áç≤ÂæóÁõ∏Áï∂ÁöÑÈóúÊ≥®Ôºå‰ΩÜÂ§ßÂ§öÊï∏ÊñπÊ≥ïÈÉΩÂ∞àÊ≥®ÊñºÂÖ∑Êúâ‰∏çÈÄèÊòéÂÖßÈÉ®ÈÅã‰ΩúÁöÑ„ÄåÈªëÁõíÂ≠ê„ÄçËß£Ê±∫ÊñπÊ°à„ÄÇÂèØËß£ÈáãÁöÑÊñπÊ≥ïÊèê‰æõ‰∫ÜËß£Ê±∫ÊñπÊ°àÔºåÁÑ∂ËÄåÔºåÂÆÉÂÄë‰æùË≥¥Êñº‰ªîÁ¥∞ÁöÑÁâπÂæµÂ∑•Á®ãÂíåÊòÇË≤¥ÁöÑÂ∞àÂÆ∂Ë®ªÈáãË≥áÊñô„ÄÇÊ≠§Â§ñÔºåÂÆ£ÂÇ≥ÊÄßÊñáÊú¨ÁöÑÁâπÂÆöË™ûË®ÄÁâπÂæµÈÄöÂ∏∏ÊòØ‰øÆËæ≠Â≠∏ÂÆ∂ÊàñË™ûË®ÄÂ≠∏ÂÆ∂ÁöÑÈóúÊ≥®ÁÑ¶ÈªûÔºå‰∏¶‰∏îÊ≤íÊúâÊ®ôË®òÊúâÊ≠§È°ûÁâπÂæµÁöÑË≥áÊñôÈõÜÈÅ©ÂêàÊ©üÂô®Â≠∏Áøí„ÄÇÊú¨Á†îÁ©∂Â∞áÂá∫ÁèæÂú®ËàáË™™ÊúçË™ûË®ÄÁõ∏ÈóúÁöÑÊñáÁçª‰∏≠Ë≠òÂà•Âá∫ÁöÑ 22 ÂÄã‰øÆËæ≠ÂíåË™ûË®ÄÁâπÂæµÁ∑®Á∫ÇÊàêÊ≥ïÂÖ∏ÔºåÁõÆÁöÑÊòØÁÇ∫Ê®ôË®òÊúâÂÆ£ÂÇ≥ÊäÄÂ∑ßÁöÑÁèæÊúâË≥áÊñôÈõÜ„ÄÇÁÇ∫‰∫ÜÂπ´Âä©‰∫∫È°ûÂ∞àÂÆ∂‰ΩøÁî®ÈÄô‰∫õÁâπÂæµË®ªÈáãËá™ÁÑ∂Ë™ûË®ÄÂè•Â≠êÔºåÂ∞àÈñÄË®≠Ë®à‰∫ÜÁ∂≤Ë∑ØÊáâÁî®Á®ãÂºè RhetAnnÔºå‰ª•ÊúÄÂ§ßÁ®ãÂ∫¶Âú∞Ê∏õÂ∞ëÂéüÊú¨Áõ∏Áï∂Â§ßÁöÑÂøÉÊô∫Ë≤†Êìî„ÄÇÊúÄÂæåÔºå‰ΩøÁî®‰∏ÄÂ∞èÁµÑË®ªÈáãË≥áÊñôÂæÆË™ø‰∫ÜÁîüÊàêÂºèÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) GPT-3.5Ôºå‰ª•Ë®ªÈáãÂâ©È§òË≥áÊñôÔºåÂêåÊôÇÈáùÂ∞çË≤°ÂãôÊàêÊú¨ÂíåÂàÜÈ°ûÊ∫ñÁ¢∫Â∫¶ÈÄ≤Ë°åÊúÄ‰Ω≥Âåñ„ÄÇÊú¨Á†îÁ©∂Â±ïÁ§∫‰∫ÜÂ∞áÂ∞ëÊï∏‰∫∫È°ûË®ªÈáãÁØÑ‰æãËàá GPT ÁµêÂêàÂ¶Ç‰ΩïÊàêÁÇ∫‰ª•ÂÇ≥Áµ±ÂÉÖ‰æùË≥¥‰∫∫È°ûÂ∞àÂÆ∂ÁöÑË®ªÈáãÊàêÊú¨ÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜ‰æÜÊì¥Â±ïË®ªÈáãÁ®ãÂ∫èÁöÑÊúâÊïàÁ≠ñÁï•„ÄÇÂú®Êí∞ÂØ´Êú¨ÊñáÊôÇÔºåÁµêÊûúËàáÁï∂ÊôÇË°®ÁèæÊúÄ‰Ω≥ÁöÑÊ®°Âûã GPT-4 Áõ∏Áï∂ÔºåÊàêÊú¨Âçª‰Ωé‰∫Ü 10 ÂÄç„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÊòØ‰∏ÄÁµÑÁâπÂæµ„ÄÅÂÆÉÂÄëÁöÑÂ±¨ÊÄß„ÄÅÂÆöÁæ©ÂíåÁØÑ‰æãÔºåÊé°Áî®Ê©üÂô®ÂèØËÆÄÊ†ºÂºèÔºå‰ª•Âèä RhetAnn ÁöÑÁ®ãÂºèÁ¢ºÂíå GPT ÊèêÁ§∫ÂíåÂæÆË™øÁ®ãÂ∫èÔºåÁî®ÊñºÊé®ÈÄ≤ÊúÄÂÖàÈÄ≤ÁöÑÂèØËß£ÈáãÂÆ£ÂÇ≥ÊäÄÂ∑ßÂÅµÊ∏¨„ÄÇ</paragraph>

##### **The Future of Data Science Education**
2407.11824v1 by Brian Wright, Peter Alonzi, Ali Riveria

The definition of Data Science is a hotly debated topic. For many, the
definition is a simple shortcut to Artificial Intelligence or Machine Learning.
However, there is far more depth and nuance to the field of Data Science than a
simple shortcut can provide. The School of Data Science at the University of
Virginia has developed a novel model for the definition of Data Science. This
model is based on identifying a unified understanding of the data work done
across all areas of Data Science. It represents a generational leap forward in
how we understand and teach Data Science. In this paper we will present the
core features of the model and explain how it unifies various concepts going
far beyond the analytics component of AI. From this foundation we will present
our Undergraduate Major curriculum in Data Science and demonstrate how it
prepares students to be well-rounded Data Science team members and leaders. The
paper will conclude with an in-depth overview of the Foundations of Data
Science course designed to introduce students to the field while also
implementing proven STEM oriented pedagogical methods. These include, for
example, specifications grading, active learning lectures, guest lectures from
industry experts and weekly gamification labs.

ÊëòË¶ÅÔºöÊï∏ÊìöÁßëÂ≠∏ÁöÑÂÆöÁæ©ÊòØ‰∏ÄÂÄãÁÜ±ÈñÄÁöÑÁà≠Ë´ñË©±È°å„ÄÇÂ∞çË®±Â§ö‰∫∫‰æÜË™™ÔºåÈÄôÂÄãÂÆöÁæ©ÊòØÈÄöÂæÄ‰∫∫Â∑•Êô∫ÊÖßÊàñÊ©üÂô®Â≠∏ÁøíÁöÑÊç∑Âæë„ÄÇÁÑ∂ËÄåÔºåÊï∏ÊìöÁßëÂ≠∏È†òÂüüÁöÑÊ∑±Â∫¶ÂíåÁ¥∞ÂæÆÂ∑ÆÂà•ÈÅ†ÈÅ†Ë∂ÖÈÅé‰∏ÄÂÄãÁ∞°ÂñÆÁöÑÊç∑ÂæëÊâÄËÉΩÊèê‰æõÁöÑ„ÄÇÁ∂≠ÂêâÂ∞º‰∫ûÂ§ßÂ≠∏ÁöÑÊï∏ÊìöÁßëÂ≠∏Â≠∏Èô¢ÁÇ∫Êï∏ÊìöÁßëÂ≠∏ÁöÑÂÆöÁæ©ÈñãÁôº‰∫Ü‰∏ÄÂÄãÊñ∞Ê®°Âûã„ÄÇÈÄôÂÄãÊ®°ÂûãÂü∫ÊñºÂ∞çÊï∏ÊìöÁßëÂ≠∏ÊâÄÊúâÈ†òÂüüÊâÄÂÅöÁöÑÊï∏ÊìöÂ∑•‰ΩúÁöÑÁµ±‰∏ÄÁêÜËß£„ÄÇÂÆÉ‰ª£Ë°®‰∫ÜÊàëÂÄëÁêÜËß£ÂíåÊïôÊéàÊï∏ÊìöÁßëÂ≠∏ÊñπÂºèÁöÑ‰∏Ä‰ª£È£õË∫ç„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞á‰ªãÁ¥πË©≤Ê®°ÂûãÁöÑÊ†∏ÂøÉÁâπÂæµÔºå‰∏¶Ëß£ÈáãÂÆÉÂ¶Ç‰ΩïÁµ±‰∏ÄÂêÑÁ®ÆÊ¶ÇÂøµÔºåÈÅ†ÈÅ†Ë∂ÖÂá∫‰∫Ü AI ÁöÑÂàÜÊûêÁµÑÊàêÈÉ®ÂàÜ„ÄÇÂæûÈÄôÂÄãÂü∫Á§é‰∏äÔºåÊàëÂÄëÂ∞á‰ªãÁ¥πÊàëÂÄëÁöÑÊï∏ÊìöÁßëÂ≠∏Êú¨ÁßëÂ∞àÊ•≠Ë™≤Á®ãÔºå‰∏¶Â±ïÁ§∫ÂÆÉÂ¶Ç‰ΩïËÆìÂ≠∏ÁîüÂÅöÂ•ΩÊ∫ñÂÇôÔºåÊàêÁÇ∫ÂÖ®Èù¢ÁôºÂ±ïÁöÑÊï∏ÊìöÁßëÂ≠∏ÂúòÈöäÊàêÂì°ÂíåÈ†òÂ∞éËÄÖ„ÄÇÊú¨ÊñáÂ∞á‰ª•Â∞çÊï∏ÊìöÁßëÂ≠∏Âü∫Á§éË™≤Á®ãÁöÑÊ∑±ÂÖ•Ê¶ÇËø∞‰ΩúÁÇ∫ÁµêÂ∞æÔºåË©≤Ë™≤Á®ãÊó®Âú®ÂêëÂ≠∏Áîü‰ªãÁ¥πË©≤È†òÂüüÔºåÂêåÊôÇ‰πüÂØ¶ÊñΩÁ∂ìÈÅéÈ©óË≠âÁöÑ STEM Â∞éÂêëÊïôÂ≠∏ÊñπÊ≥ï„ÄÇ‰æãÂ¶ÇÔºåÈÄô‰∫õÊñπÊ≥ïÂåÖÊã¨Ë¶èÁØÑË©ïÂàÜ„ÄÅ‰∏ªÂãïÂ≠∏ÁøíË¨õÂ∫ß„ÄÅ‰æÜËá™Ë°åÊ•≠Â∞àÂÆ∂ÁöÑÂÆ¢Â∫ßË¨õÂ∫ßÂíåÊØèÈÄ±ÁöÑÈÅäÊà≤ÂåñÂØ¶È©óÂÆ§„ÄÇ

##### **Invariant Consistency for Knowledge Distillation**
2407.11802v1 by Nikolaos Giakoumoglou, Tania Stathaki

Knowledge distillation (KD) involves transferring the knowledge from one
neural network to another, often from a larger, well-trained model (teacher) to
a smaller, more efficient model (student). Traditional KD methods minimize the
Kullback-Leibler (KL) divergence between the probabilistic outputs of the
teacher and student networks. However, this approach often overlooks crucial
structural knowledge embedded within the teacher's network. In this paper, we
introduce Invariant Consistency Distillation (ICD), a novel methodology
designed to enhance KD by ensuring that the student model's representations are
consistent with those of the teacher. Our approach combines contrastive
learning with an explicit invariance penalty, capturing significantly more
information from the teacher's representation of the data. Our results on
CIFAR-100 demonstrate that ICD outperforms traditional KD techniques and
surpasses 13 state-of-the-art methods. In some cases, the student model even
exceeds the teacher model in terms of accuracy. Furthermore, we successfully
transfer our method to other datasets, including Tiny ImageNet and STL-10. The
code will be made public soon.

ÊëòË¶ÅÔºöÁü•Ë≠òËí∏È§æ (KD) Ê∂âÂèäÂ∞áÁü•Ë≠òÂæû‰∏ÄÂÄãÁ•ûÁ∂ìÁ∂≤Ë∑ØËΩâÁßªÂà∞Âè¶‰∏ÄÂÄãÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåÈÄöÂ∏∏Âæû‰∏ÄÂÄãËºÉÂ§ß„ÄÅË®ìÁ∑¥ËâØÂ•ΩÁöÑÊ®°ÂûãÔºàÊïôÂ∏´ÔºâËΩâÁßªÂà∞‰∏ÄÂÄãËºÉÂ∞è„ÄÅÊõ¥ÊúâÊïàÁéáÁöÑÊ®°ÂûãÔºàÂ≠∏ÁîüÔºâ„ÄÇÂÇ≥Áµ±ÁöÑ KD ÊñπÊ≥ïÂ∞áÊïôÂ∏´ÂíåÂ≠∏ÁîüÁ∂≤Ë∑ØÁöÑÊ©üÁéáËº∏Âá∫‰πãÈñìÁöÑ Kullback-Leibler (KL) Â∑ÆÁï∞ÊúÄÂ∞èÂåñ„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÊñπÊ≥ïÈÄöÂ∏∏ÂøΩÁï•‰∫ÜÊïôÂ∏´Á∂≤Ë∑Ø‰∏≠ÂµåÂÖ•ÁöÑÈóúÈçµÁµêÊßãÁü•Ë≠ò„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏çËÆä‰∏ÄËá¥ÊÄßËí∏È§æ (ICD)ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ïÔºåÊó®Âú®ÈÄèÈÅéÁ¢∫‰øùÂ≠∏ÁîüÊ®°ÂûãÁöÑË°®ÂæµËàáÊïôÂ∏´ÁöÑË°®Âæµ‰∏ÄËá¥‰æÜÂ¢ûÂº∑ KD„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÁµêÂêà‰∫ÜÂ∞çÊØîÂ≠∏ÁøíËàáÊòéÁ¢∫ÁöÑ‰∏çËÆäÁΩ∞ÂâáÔºåÂæûÊïôÂ∏´Â∞çË≥áÊñôÁöÑË°®Âæµ‰∏≠Êì∑ÂèñÊõ¥Â§öË≥áË®ä„ÄÇÊàëÂÄëÂú® CIFAR-100 ‰∏äÁöÑÁµêÊûúË≠âÊòéÔºåICD ÂÑ™ÊñºÂÇ≥Áµ±ÁöÑ KD ÊäÄË°ìÔºå‰∏¶Ë∂ÖË∂ä‰∫Ü 13 Á®ÆÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ï„ÄÇÂú®Êüê‰∫õÊÉÖÊ≥Å‰∏ãÔºåÂ≠∏ÁîüÊ®°ÂûãÁîöËá≥Âú®Ê∫ñÁ¢∫Â∫¶ÊñπÈù¢Ë∂ÖÈÅé‰∫ÜÊïôÂ∏´Ê®°Âûã„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊàêÂäüÂú∞Â∞áÊàëÂÄëÁöÑÊ®°ÂûãËΩâÁßªÂà∞ÂÖ∂‰ªñË≥áÊñôÈõÜÔºåÂåÖÊã¨ Tiny ImageNet Âíå STL-10„ÄÇÁ®ãÂºèÁ¢ºÂ∞áÂæàÂø´ÂÖ¨Èñã„ÄÇ

##### **PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation**
2407.11798v1 by Branden Butler, Sixing Yu, Arya Mazaheri, Ali Jannesari

Inference of Large Language Models (LLMs) across computer clusters has become
a focal point of research in recent times, with many acceleration techniques
taking inspiration from CPU speculative execution. These techniques reduce
bottlenecks associated with memory bandwidth, but also increase end-to-end
latency per inference run, requiring high speculation acceptance rates to
improve performance. Combined with a variable rate of acceptance across tasks,
speculative inference techniques can result in reduced performance.
Additionally, pipeline-parallel designs require many user requests to maintain
maximum utilization. As a remedy, we propose PipeInfer, a pipelined speculative
acceleration technique to reduce inter-token latency and improve system
utilization for single-request scenarios while also improving tolerance to low
speculation acceptance rates and low-bandwidth interconnects. PipeInfer
exhibits up to a 2.15$\times$ improvement in generation speed over standard
speculative inference. PipeInfer achieves its improvement through Continuous
Asynchronous Speculation and Early Inference Cancellation, the former improving
latency and generation speed by running single-token inference simultaneously
with several speculative runs, while the latter improves speed and latency by
skipping the computation of invalidated runs, even in the middle of inference.

ÊëòË¶ÅÔºöËøë‰æÜÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÈõªËÖ¶Âè¢ÈõÜ‰∏≠ÁöÑÊé®Ë´ñÂ∑≤ÊàêÁÇ∫Á†îÁ©∂ÁöÑÈáçÈªûÔºåË®±Â§öÂä†ÈÄüÊäÄË°ìÂæû CPU Êé®Ê∏¨Âü∑Ë°å‰∏≠Ê±≤ÂèñÈùàÊÑü„ÄÇÈÄô‰∫õÊäÄË°ìÊ∏õÂ∞ë‰∫ÜËàáË®òÊÜ∂È´îÈ†ªÂØ¨Áõ∏ÈóúÁöÑÁì∂È†∏Ôºå‰ΩÜ‰πüÂ¢ûÂä†‰∫ÜÊØèÂÄãÊé®Ë´ñÈÅãË°åÁöÑÁ´ØÂà∞Á´ØÂª∂ÈÅ≤ÔºåÈúÄË¶ÅÂæàÈ´òÁöÑÊé®Ê∏¨Êé•ÂèóÁéáÊâçËÉΩÊèêÂçáÊïàËÉΩ„ÄÇÁµêÂêàË∑®‰ªªÂãôÁöÑÂèØËÆäÊé•ÂèóÁéáÔºåÊé®Ê∏¨Êé®Ë´ñÊäÄË°ìÂèØËÉΩÊúÉÂ∞éËá¥ÊïàËÉΩ‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÁÆ°Á∑öÂπ≥Ë°åË®≠Ë®àÈúÄË¶ÅË®±Â§ö‰ΩøÁî®ËÄÖË¶ÅÊ±ÇÊâçËÉΩÁ∂≠ÊåÅÊúÄÂ§ßÁöÑ‰ΩøÁî®Áéá„ÄÇ‰ΩúÁÇ∫Ë£úÊïëÊé™ÊñΩÔºåÊàëÂÄëÊèêÂá∫ PipeInferÔºå‰∏ÄÁ®ÆÁÆ°Á∑öÊé®Ê∏¨Âä†ÈÄüÊäÄË°ìÔºåÁî®ÊñºÊ∏õÂ∞ë‰ª£Âπ£ÈñìÂª∂ÈÅ≤‰∏¶ÊîπÂñÑÂñÆ‰∏ÄË¶ÅÊ±ÇÂ†¥ÊôØÁöÑÁ≥ªÁµ±‰ΩøÁî®ÁéáÔºåÂêåÊôÇ‰πüÊèêÈ´òÂ∞ç‰ΩéÊé®Ê∏¨Êé•ÂèóÁéáÂíå‰ΩéÈ†ªÂØ¨‰∫íÈÄ£ÁöÑÂÆπÂøçÂ∫¶„ÄÇËàáÊ®ôÊ∫ñÊé®Ê∏¨Êé®Ë´ñÁõ∏ÊØîÔºåPipeInfer Âú®Áî¢ÁîüÈÄüÂ∫¶‰∏äÂ±ïÁèæÂá∫È´òÈÅî 2.15 ÂÄçÁöÑÈÄ≤Ê≠•„ÄÇPipeInfer ÈÄèÈÅéÈÄ£Á∫åÈùûÂêåÊ≠•Êé®Ê∏¨ÂíåÊó©ÊúüÊé®Ë´ñÂèñÊ∂à‰æÜÈÅîÊàêÈÄ≤Ê≠•ÔºåÂâçËÄÖÈÄèÈÅéÂêåÊôÇÂü∑Ë°åÂñÆ‰∏Ä‰ª£Âπ£Êé®Ë´ñÂíåÂ§öÂÄãÊé®Ê∏¨ÈÅãË°å‰æÜÊîπÂñÑÂª∂ÈÅ≤ÂíåÁî¢ÁîüÈÄüÂ∫¶ÔºåËÄåÂæåËÄÖÈÄèÈÅéÁï•ÈÅéÁÑ°ÊïàÈÅãË°åÁöÑË®àÁÆóÔºàÂç≥‰ΩøÊòØÂú®Êé®Ë´ñÈÅéÁ®ã‰∏≠Ôºâ‰æÜÊîπÂñÑÈÄüÂ∫¶ÂíåÂª∂ÈÅ≤„ÄÇ

##### **Characterizing and Understanding HGNN Training on GPUs**
2407.11790v1 by Dengke Han, Mingyu Yan, Xiaochun Ye, Dongrui Fan, Ninghui Sun

Owing to their remarkable representation capabilities for heterogeneous graph
data, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in
many critical real-world domains such as recommendation systems and medical
analysis. Prior to their practical application, identifying the optimal HGNN
model parameters tailored to specific tasks through extensive training is a
time-consuming and costly process. To enhance the efficiency of HGNN training,
it is essential to characterize and analyze the execution semantics and
patterns within the training process to identify performance bottlenecks. In
this study, we conduct an in-depth quantification and analysis of two
mainstream HGNN training scenarios, including single-GPU and multi-GPU
distributed training. Based on the characterization results, we disclose the
performance bottlenecks and their underlying causes in different HGNN training
scenarios and provide optimization guidelines from both software and hardware
perspectives.

ÊëòË¶ÅÔºöÁî±ÊñºÁï∞Ë≥™ÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (HGNN) ÂÖ∑ÊúâÂçìË∂äÁöÑÁï∞Ë≥™ÂúñÂΩ¢Êï∏ÊìöË°®Á§∫ËÉΩÂäõÔºåÂõ†Ê≠§Â∑≤Âª£Ê≥õÊáâÁî®ÊñºË®±Â§öÈáçË¶ÅÁöÑÁúüÂØ¶‰∏ñÁïåÈ†òÂüüÔºå‰æãÂ¶ÇÊé®Ëñ¶Á≥ªÁµ±ÂíåÈÜ´ÁôÇÂàÜÊûê„ÄÇÂú®ÂØ¶ÈöõÊáâÁî®‰πãÂâçÔºåÈÄöÈÅéÂª£Ê≥õÁöÑË®ìÁ∑¥‰æÜË≠òÂà•ÈáùÂ∞çÁâπÂÆö‰ªªÂãôË™øÊï¥ÁöÑÊúÄ‰Ω≥ HGNN Ê®°ÂûãÂèÉÊï∏ÊòØ‰∏ÄÂÄãËÄóÊôÇ‰∏îÊòÇË≤¥ÁöÑÈÅéÁ®ã„ÄÇÁÇ∫‰∫ÜÊèêÈ´ò HGNN Ë®ìÁ∑¥ÁöÑÊïàÁéáÔºåÂøÖÈ†àÊèèËø∞ÂíåÂàÜÊûêË®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÁöÑÂü∑Ë°åË™ûÁæ©ÂíåÊ®°ÂºèÔºå‰ª•Ë≠òÂà•ÊïàËÉΩÁì∂È†∏„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ∞çÂÖ©ÂÄã‰∏ªÊµÅÁöÑ HGNN Ë®ìÁ∑¥Â†¥ÊôØÔºàÂåÖÊã¨ÂñÆ GPU ÂíåÂ§ö GPU ÂàÜÊï£ÂºèË®ìÁ∑¥ÔºâÈÄ≤Ë°åÊ∑±ÂÖ•ÁöÑÈáèÂåñÂíåÂàÜÊûê„ÄÇÊ†πÊìöÊèèËø∞ÁµêÊûúÔºåÊàëÂÄëÊè≠Á§∫‰∫Ü‰∏çÂêå HGNN Ë®ìÁ∑¥Â†¥ÊôØ‰∏≠ÁöÑÊïàËÉΩÁì∂È†∏ÂèäÂÖ∂Ê†πÊú¨ÂéüÂõ†Ôºå‰∏¶ÂæûËªüÈ´îÂíåÁ°¨È´îÁöÑËßíÂ∫¶Êèê‰æõ‰∫ÜÊúÄ‰Ω≥ÂåñÊåáÂçó„ÄÇ

##### **Large Language Models as Misleading Assistants in Conversation**
2407.11789v1 by Betty Li Hou, Kejian Shi, Jason Phang, James Aung, Steven Adler, Rosie Campbell

Large Language Models (LLMs) are able to provide assistance on a wide range
of information-seeking tasks. However, model outputs may be misleading, whether
unintentionally or in cases of intentional deception. We investigate the
ability of LLMs to be deceptive in the context of providing assistance on a
reading comprehension task, using LLMs as proxies for human users. We compare
outcomes of (1) when the model is prompted to provide truthful assistance, (2)
when it is prompted to be subtly misleading, and (3) when it is prompted to
argue for an incorrect answer. Our experiments show that GPT-4 can effectively
mislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up
to a 23% drop in accuracy on the task compared to when a truthful assistant is
used. We also find that providing the user model with additional context from
the passage partially mitigates the influence of the deceptive model. This work
highlights the ability of LLMs to produce misleading information and the
effects this may have in real-world situations.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËÉΩÂ§†Âú®ÂêÑÁ®ÆË≥áË®äÊêúÂ∞ã‰ªªÂãô‰∏≠Êèê‰æõÂçîÂä©„ÄÇÁÑ∂ËÄåÔºåÊ®°ÂûãÁöÑËº∏Âá∫ÂèØËÉΩÂÖ∑ÊúâË™§Â∞éÊÄßÔºåÁÑ°Ë´ñÊòØÁÑ°ÊÑèÁöÑÊàñÊòØÊúâÊÑèÊ¨∫È®ô„ÄÇÊàëÂÄëË™øÊü•‰∫Ü LLM Âú®Èñ±ËÆÄÁêÜËß£‰ªªÂãô‰∏≠Êèê‰æõÂçîÂä©ÊôÇÂÖ∑ÊúâÊ¨∫È®ôÊÄßÁöÑËÉΩÂäõÔºå‰ΩøÁî® LLM ‰ΩúÁÇ∫‰∫∫È°û‰ΩøÁî®ËÄÖÁöÑ‰ª£ÁêÜ„ÄÇÊàëÂÄëÊØîËºÉ‰∫Ü (1) Áï∂Ê®°ÂûãË¢´ÊèêÁ§∫Êèê‰æõÁúüÂØ¶ÂçîÂä©„ÄÅ(2) Áï∂ÂÆÉË¢´ÊèêÁ§∫ÈÄ≤Ë°åÂæÆÂ¶ôÁöÑË™§Â∞é„ÄÅ‰ª•Âèä (3) Áï∂ÂÆÉË¢´ÊèêÁ§∫ÁÇ∫ÈåØË™§Á≠îÊ°àËæØË≠∑ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåGPT-4 ÂèØ‰ª•ÊúâÊïàÂú∞Ë™§Â∞é GPT-3.5-Turbo Âíå GPT-4ÔºåËàá‰ΩøÁî®ÁúüÂØ¶Âä©ÁêÜÁõ∏ÊØîÔºåÂÖ∑ÊúâÊ¨∫È®ôÊÄßÁöÑÂä©ÁêÜÂ∞éËá¥‰ªªÂãôÊ∫ñÁ¢∫Â∫¶‰∏ãÈôçÂ§öÈÅî 23%„ÄÇÊàëÂÄëÈÇÑÁôºÁèæÔºåÁÇ∫‰ΩøÁî®ËÄÖÊ®°ÂûãÊèê‰æõ‰æÜËá™ÊÆµËêΩÁöÑÈ°çÂ§ñËÉåÊôØÔºåÂèØ‰ª•ÈÉ®ÂàÜÊ∏õËºïÊ¨∫È®ôÊÄßÊ®°ÂûãÁöÑÂΩ±Èüø„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁ™ÅÈ°Ø‰∫Ü LLM Áî¢ÁîüË™§Â∞éÊÄßË≥áË®äÁöÑËÉΩÂäõÔºå‰ª•ÂèäÈÄôÂèØËÉΩÂú®ÁèæÂØ¶‰∏ñÁïå‰∏≠Áî¢ÁîüÁöÑÂΩ±Èüø„ÄÇ

##### **Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development**
2407.11784v1 by Daoyuan Chen, Haibin Wang, Yilun Huang, Ce Ge, Yaliang Li, Bolin Ding, Jingren Zhou

The emergence of large-scale multi-modal generative models has drastically
advanced artificial intelligence, introducing unprecedented levels of
performance and functionality. However, optimizing these models remains
challenging due to historically isolated paths of model-centric and
data-centric developments, leading to suboptimal outcomes and inefficient
resource utilization. In response, we present a novel sandbox suite tailored
for integrated data-model co-development. This sandbox provides a comprehensive
experimental platform, enabling rapid iteration and insight-driven refinement
of both data and models. Our proposed "Probe-Analyze-Refine" workflow,
validated through applications on state-of-the-art LLaVA-like and DiT based
models, yields significant performance boosts, such as topping the VBench
leaderboard. We also uncover fruitful insights gleaned from exhaustive
benchmarks, shedding light on the critical interplay between data quality,
diversity, and model behavior. With the hope of fostering deeper understanding
and future progress in multi-modal data and generative modeling, our codes,
datasets, and models are maintained and accessible at
https://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßË¶èÊ®°Â§öÊ®°ÊÖãÁîüÊàêÊ®°ÂûãÁöÑÂá∫ÁèæÔºåÂ§ßÂπÖÊèêÂçá‰∫Ü‰∫∫Â∑•Êô∫ÊÖßÔºå‰∏¶Â∞éÂÖ•‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑÊïàËÉΩÂíåÂäüËÉΩÂ±§Á¥ö„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊ®°Âûã‰∏≠ÂøÉÂíåË≥áÊñô‰∏≠ÂøÉÁôºÂ±ïË∑ØÂæëÈï∑Êúü‰ª•‰æÜÂêÑËá™Áç®Á´ãÔºåÂ∞éËá¥Ê¨°‰Ω≥ÁµêÊûúÂíåË≥áÊ∫êÂà©Áî®Áéá‰ΩéËêΩÔºåÂõ†Ê≠§ÊúÄ‰Ω≥ÂåñÈÄô‰∫õÊ®°Âûã‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÈáùÂ∞çÊï¥ÂêàË≥áÊñôÊ®°ÂûãÂÖ±ÂêåÈñãÁôºÈáèË∫´ÊâìÈÄ†ÁöÑÊñ∞Ê≤ôÁõíÂ•ó‰ª∂„ÄÇÈÄôÂÄãÊ≤ôÁõíÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂØ¶È©óÂπ≥Âè∞ÔºåËÉΩÂø´ÈÄüÂèçË¶ÜÈÅãÁÆóÔºå‰∏¶Ê†πÊìöÊ∑±ÂÖ•Ê¥ûÂØüÊîπÈÄ≤Ë≥áÊñôÂíåÊ®°Âûã„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑ„ÄåÊé¢Ê∏¨ÂàÜÊûêÁ≤æÈÄ≤„ÄçÂ∑•‰ΩúÊµÅÁ®ãÔºåÂ∑≤ÈÄöÈÅéÊáâÁî®ÊñºÊúÄÂÖàÈÄ≤ÁöÑ LLaVA È°û‰ººÊ®°ÂûãÂíåÂü∫Êñº DiT ÁöÑÊ®°ÂûãÁç≤ÂæóÈ©óË≠âÔºåÂèØÂ§ßÂπÖÊèêÂçáÊïàËÉΩÔºå‰æãÂ¶ÇÁôª‰∏ä VBench ÊéíË°åÊ¶úÈ¶ñ‰Ωç„ÄÇÊàëÂÄë‰πüÂæûË©≥Áõ°ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÁôºÁèæÊúâÁî®ÁöÑÊ∑±ÂÖ•Ê¥ûÂØüÔºåÈó°Êòé‰∫ÜË≥áÊñôÂìÅË≥™„ÄÅÂ§öÊ®£ÊÄßÂíåÊ®°ÂûãË°åÁÇ∫‰πãÈñìÁöÑÈóúÈçµ‰∫§‰∫í‰ΩúÁî®„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Â∞çÂ§öÊ®°ÊÖãË≥áÊñôÂíåÁîüÊàêÊ®°ÂûãÁöÑÊõ¥Ê∑±ÂÖ•ÁêÜËß£ÂíåÊú™‰æÜÈÄ≤Â±ïÔºåÊàëÂÄëÁöÑÁ®ãÂºèÁ¢º„ÄÅË≥áÊñôÈõÜÂíåÊ®°ÂûãÂùáÂ∑≤Á∂≠Ë≠∑‰∏¶ÂèØÊñº https://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md ÂèñÂæó„ÄÇ

##### **SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models**
2407.11780v1 by Xinbo Wu, Max Hartman, Vidhata Arjun Jayaraman, Lav R. Varshney

Large language models (LLMs) have exhibited impressive capabilities in
various domains, particularly in general language understanding. However these
models, trained on massive text data, may not be finely optimized for specific
tasks triggered by instructions. Continual instruction tuning is crucial to
adapt LLMs to evolving tasks and domains, ensuring their effectiveness and
relevance across a wide range of applications. In the context of continual
instruction tuning, where models are sequentially trained on different tasks,
catastrophic forgetting can occur, leading to performance degradation on
previously learned tasks. This work addresses the catastrophic forgetting in
continual instruction learning for LLMs through a switching mechanism for
routing computations to parameter-efficient tuned models. We demonstrate the
effectiveness of our method through experiments on continual instruction tuning
of different natural language generation tasks.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆÈ†òÂüü‰∏≠Â±ïÁ§∫‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÁâπÂà•ÊòØÂú®‰∏ÄËà¨Ë™ûË®ÄÁêÜËß£ÊñπÈù¢„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÂú®Â§ßÈáèÊñáÂ≠óË≥áÊñô‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÂèØËÉΩÁÑ°Ê≥ïÈáùÂ∞çÁî±Êåá‰ª§Ëß∏ÁôºÁöÑÁâπÂÆö‰ªªÂãôÈÄ≤Ë°åÁ≤æÁ¥∞ÊúÄ‰Ω≥Âåñ„ÄÇÊåÅÁ∫åÁöÑÊåá‰ª§Ë™øÊï¥Â∞çÊñºËÆì LLM ÈÅ©Êáâ‰∏çÊñ∑ËÆäÂåñÁöÑ‰ªªÂãôÂíåÈ†òÂüüËá≥ÈóúÈáçË¶ÅÔºåÁ¢∫‰øùÂÆÉÂÄëÂú®Âª£Ê≥õÁöÑÊáâÁî®‰∏≠ÈÉΩËÉΩÁôºÊèÆÊïàÁî®ÂíåÁõ∏ÈóúÊÄß„ÄÇÂú®ÊåÅÁ∫åÊåá‰ª§Ë™øÊï¥ÁöÑËÉåÊôØ‰∏ãÔºåÊ®°ÂûãÊúÉ‰æùÂ∫èÂú®‰∏çÂêåÁöÑ‰ªªÂãô‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÔºåÊ≠§ÊôÇÂèØËÉΩÊúÉÁôºÁîüÁÅΩÈõ£ÊÄßÈÅ∫ÂøòÔºåÂ∞éËá¥ÂÖàÂâçÂ≠∏ÁøíÁöÑ‰ªªÂãôÊïàËÉΩ‰∏ãÈôç„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÈÄèÈÅéÂàáÊèõÊ©üÂà∂‰æÜËß£Ê±∫ LLM ÊåÅÁ∫åÊåá‰ª§Â≠∏Áøí‰∏≠ÁöÑÁÅΩÈõ£ÊÄßÈÅ∫ÂøòÔºåÂ∞áÈÅãÁÆóË∑ØÁî±Âà∞ÂèÉÊï∏ÊïàÁéáÊúÄ‰Ω≥ÂåñÁöÑË™øÊï¥Ê®°Âûã„ÄÇÊàëÂÄëÈÄèÈÅéÈáùÂ∞ç‰∏çÂêåËá™ÁÑ∂Ë™ûË®ÄÁî¢Áîü‰ªªÂãôÁöÑÊåÅÁ∫åÊåá‰ª§Ë™øÊï¥ÈÄ≤Ë°åÂØ¶È©óÔºå‰æÜË≠âÊòéÊàëÂÄëÁöÑÊñπÊ≥ïÊúâÊïà„ÄÇ

##### **Sharif-MGTD at SemEval-2024 Task 8: A Transformer-Based Approach to Detect Machine Generated Text**
2407.11774v1 by Seyedeh Fatemeh Ebrahimi, Karim Akhavan Azari, Amirmasoud Iravani, Arian Qazvini, Pouya Sadeghi, Zeinab Sadat Taghavi, Hossein Sameti

Detecting Machine-Generated Text (MGT) has emerged as a significant area of
study within Natural Language Processing. While language models generate text,
they often leave discernible traces, which can be scrutinized using either
traditional feature-based methods or more advanced neural language models. In
this research, we explore the effectiveness of fine-tuning a RoBERTa-base
transformer, a powerful neural architecture, to address MGT detection as a
binary classification task. Focusing specifically on Subtask A
(Monolingual-English) within the SemEval-2024 competition framework, our
proposed system achieves an accuracy of 78.9% on the test dataset, positioning
us at 57th among participants. Our study addresses this challenge while
considering the limited hardware resources, resulting in a system that excels
at identifying human-written texts but encounters challenges in accurately
discerning MGTs.

ÊëòË¶ÅÔºöÊ©üÂô®Áî¢ÁîüÁöÑÊñáÂ≠óÔºàMGTÔºâÂÅµÊ∏¨Â∑≤ÊàêÁÇ∫Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠‰∏ÄÂÄãÈáçË¶ÅÁöÑÁ†îÁ©∂È†òÂüü„ÄÇÈõñÁÑ∂Ë™ûË®ÄÊ®°ÂûãÊúÉÁî¢ÁîüÊñáÂ≠óÔºå‰ΩÜÂÆÉÂÄëÈÄöÂ∏∏ÊúÉÁïô‰∏ãÂèØËæ®Ë≠òÁöÑÁóïË∑°ÔºåÂèØ‰ª•‰ΩøÁî®ÂÇ≥Áµ±ÁöÑÂü∫ÊñºÁâπÂæµÁöÑÊñπÊ≥ïÊàñÊõ¥ÂÖàÈÄ≤ÁöÑÁ•ûÁ∂ìË™ûË®ÄÊ®°Âûã‰æÜÊ™¢Ë¶ñÈÄô‰∫õÁóïË∑°„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂæÆË™ø RoBERTa-base ËΩâÊèõÂô®Ôºà‰∏ÄÁ®ÆÂº∑Â§ßÁöÑÁ•ûÁ∂ìÊû∂ÊßãÔºâ‰æÜËôïÁêÜ MGT ÂÅµÊ∏¨‰ΩúÁÇ∫‰∫åÂÖÉÂàÜÈ°û‰ªªÂãôÁöÑÊúâÊïàÊÄß„ÄÇÁâπÂà•Â∞àÊ≥®Êñº SemEval-2024 Á´∂Ë≥ΩÊû∂Êßã‰∏≠ÁöÑÂ≠ê‰ªªÂãô AÔºàÂñÆË™ûËã±Ë™ûÔºâÔºåÊàëÂÄëÊèêÂá∫ÁöÑÁ≥ªÁµ±Âú®Ê∏¨Ë©¶Ë≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫Ü 78.9% ÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂú®ÂèÉËàáËÄÖ‰∏≠ÊéíÂêçÁ¨¨ 57 ‰Ωç„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Âú®ËÄÉÊÖÆÊúâÈôêÁ°¨È´îË≥áÊ∫êÁöÑÂêåÊôÇÊáâÂ∞ç‰∫ÜÈÄô‰∏ÄÊåëÊà∞ÔºåÂæûËÄåÁî¢Áîü‰∫Ü‰∏ÄÂÄãÂú®Ë≠òÂà•‰∫∫È°ûÊí∞ÂØ´ÁöÑÊñáÂ≠óÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÂú®Ê∫ñÁ¢∫Ëæ®Âà• MGT ÊñπÈù¢ÈÅáÂà∞ÊåëÊà∞ÁöÑÁ≥ªÁµ±„ÄÇ

##### **Educational Personalized Learning Path Planning with Large Language Models**
2407.11773v1 by Chee Ng, Yuen Fung

Educational Personalized Learning Path Planning (PLPP) aims to tailor
learning experiences to individual learners' needs, enhancing learning
efficiency and engagement. Despite its potential, traditional PLPP systems
often lack adaptability, interactivity, and transparency. This paper proposes a
novel approach integrating Large Language Models (LLMs) with prompt engineering
to address these challenges. By designing prompts that incorporate
learner-specific information, our method guides LLMs like LLama-2-70B and GPT-4
to generate personalized, coherent, and pedagogically sound learning paths. We
conducted experiments comparing our method with a baseline approach across
various metrics, including accuracy, user satisfaction, and the quality of
learning paths. The results show significant improvements in all areas,
particularly with GPT-4, demonstrating the effectiveness of prompt engineering
in enhancing PLPP. Additional long-term impact analysis further validates our
method's potential to improve learner performance and retention. This research
highlights the promise of LLMs and prompt engineering in advancing personalized
education.

ÊëòË¶ÅÔºöÊïôËÇ≤ÂÄã‰∫∫ÂåñÂ≠∏ÁøíË∑ØÂæëË¶èÂäÉ (PLPP) Êó®Âú®Ê†πÊìöÂÄãÂà•Â≠∏ÁøíËÄÖÁöÑÈúÄÊ±ÇÈáèË∫´ÊâìÈÄ†Â≠∏ÁøíÈ´îÈ©óÔºåÊèêÂçáÂ≠∏ÁøíÊïàÁéáÂíåÂèÉËàáÂ∫¶„ÄÇÂÑòÁÆ° PLPP Á≥ªÁµ±ÂÖ∑ÊúâÊΩõÂäõÔºå‰ΩÜÂÇ≥Áµ±ÁöÑ PLPP Á≥ªÁµ±ÈÄöÂ∏∏Áº∫‰πèÈÅ©ÊáâÊÄß„ÄÅ‰∫íÂãïÊÄßÂíåÈÄèÊòéÂ∫¶„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÊñπÊ≥ïÔºåÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÊèêÁ§∫Â∑•Á®ãÁõ∏ÁµêÂêàÔºå‰ª•ÊáâÂ∞çÈÄô‰∫õÊåëÊà∞„ÄÇÈÄöÈÅéË®≠Ë®àÂåÖÂê´Â≠∏ÁøíËÄÖÁâπÂÆöË≥áË®äÁöÑÊèêÁ§∫ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂºïÂ∞é LLMÔºà‰æãÂ¶Ç LLama-2-70B Âíå GPT-4ÔºâÁîüÊàêÂÄãÊÄßÂåñ„ÄÅÈÄ£Ë≤´‰∏îÂÖ∑ÊúâÊïôÂ≠∏ÊÑèÁæ©ÁöÑÂ≠∏ÁøíË∑ØÂæë„ÄÇÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂØ¶È©óÔºåÂú®Ê∫ñÁ¢∫Â∫¶„ÄÅ‰ΩøÁî®ËÄÖÊªøÊÑèÂ∫¶ÂíåÂ≠∏ÁøíË∑ØÂæëÂìÅË≥™Á≠âÂêÑÁ®ÆÊåáÊ®ô‰∏äÔºåÊØîËºÉ‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïËàáÂü∫Á∑öÊñπÊ≥ï„ÄÇÁµêÊûúÈ°ØÁ§∫Âú®ÊâÄÊúâÈ†òÂüüÈÉΩÊúâÈ°ØËëóÁöÑÈÄ≤Ê≠•ÔºåÁâπÂà•ÊòØ GPT-4ÔºåÈÄôË≠âÊòé‰∫ÜÊèêÁ§∫Â∑•Á®ãÂú®Â¢ûÂº∑ PLPP ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÈÄ≤‰∏ÄÊ≠•ÁöÑÈï∑ÊúüÂΩ±ÈüøÂàÜÊûêÈÄ≤‰∏ÄÊ≠•È©óË≠â‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÊîπÂñÑÂ≠∏ÁøíËÄÖË°®ÁèæÂíå‰øùÁïôÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇÊú¨Á†îÁ©∂Âº∑Ë™ø‰∫Ü LLM ÂíåÊèêÁ§∫Â∑•Á®ãÂú®‰øÉÈÄ≤ÂÄã‰∫∫ÂåñÊïôËÇ≤ÊñπÈù¢ÁöÑÊΩõÂäõ„ÄÇ

##### **XEdgeAI: A Human-centered Industrial Inspection Framework with Data-centric Explainable Edge AI Approach**
2407.11771v1 by Truong Thanh Hung Nguyen, Phuc Truong Loc Nguyen, Hung Cao

Recent advancements in deep learning have significantly improved visual
quality inspection and predictive maintenance within industrial settings.
However, deploying these technologies on low-resource edge devices poses
substantial challenges due to their high computational demands and the inherent
complexity of Explainable AI (XAI) methods. This paper addresses these
challenges by introducing a novel XAI-integrated Visual Quality Inspection
framework that optimizes the deployment of semantic segmentation models on
low-resource edge devices. Our framework incorporates XAI and the Large Vision
Language Model to deliver human-centered interpretability through visual and
textual explanations to end-users. This is crucial for end-user trust and model
interpretability. We outline a comprehensive methodology consisting of six
fundamental modules: base model fine-tuning, XAI-based explanation generation,
evaluation of XAI approaches, XAI-guided data augmentation, development of an
edge-compatible model, and the generation of understandable visual and textual
explanations. Through XAI-guided data augmentation, the enhanced model
incorporating domain expert knowledge with visual and textual explanations is
successfully deployed on mobile devices to support end-users in real-world
scenarios. Experimental results showcase the effectiveness of the proposed
framework, with the mobile model achieving competitive accuracy while
significantly reducing model size. This approach paves the way for the broader
adoption of reliable and interpretable AI tools in critical industrial
applications, where decisions must be both rapid and justifiable.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÈ°ØËëóÊîπÂñÑ‰∫ÜÂ∑•Ê•≠Áí∞Â¢É‰∏≠ÁöÑË¶ñË¶∫ÂìÅË≥™Ê™¢Ê∏¨ÂíåÈ†êÊ∏¨ÊÄßÁ∂≠Ë≠∑„ÄÇ
ÁÑ∂ËÄåÔºåÁî±ÊñºÈ´òË®àÁÆóÈúÄÊ±ÇÂíåÂèØËß£Èáã AI (XAI) ÊñπÊ≥ïÁöÑÂõ∫ÊúâË§áÈõúÊÄßÔºåÂú®‰ΩéË≥áÊ∫êÈÇäÁ∑£Ë£ùÁΩÆ‰∏äÈÉ®ÁΩ≤ÈÄô‰∫õÊäÄË°ìÊúÉÂ∏∂‰æÜÁõ∏Áï∂Â§ßÁöÑÊåëÊà∞„ÄÇÊú¨ÊñáÈÄèÈÅéÂºïÂÖ•‰∏ÄÂÄãÊñ∞Á©éÁöÑ XAI Êï¥ÂêàË¶ñË¶∫ÂìÅË≥™Ê™¢Ê∏¨Êû∂Êßã‰æÜËß£Ê±∫ÈÄô‰∫õÊåëÊà∞ÔºåË©≤Êû∂ÊßãÊúÄ‰Ω≥Âåñ‰∫ÜË™ûÊÑèÂàÜÂâ≤Ê®°ÂûãÂú®‰ΩéË≥áÊ∫êÈÇäÁ∑£Ë£ùÁΩÆ‰∏äÁöÑÈÉ®ÁΩ≤„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÊï¥Âêà‰∫Ü XAI ÂíåÂ§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°ÂûãÔºåÈÄèÈÅéË¶ñË¶∫ÂíåÊñáÂ≠óË™™ÊòéÊèê‰æõ‰ª•‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑË©ÆÈáãÔºåËÆìÊúÄÁµÇ‰ΩøÁî®ËÄÖÁêÜËß£„ÄÇÈÄôÂ∞çÊñºÊúÄÁµÇ‰ΩøÁî®ËÄÖÁöÑ‰ø°‰ªªÂíåÊ®°ÂûãË©ÆÈáãËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÊ¶ÇËø∞‰∫Ü‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÊñπÊ≥ïË´ñÔºåÂåÖÂê´ÂÖ≠ÂÄãÂü∫Êú¨Ê®°ÁµÑÔºöÂü∫Á§éÊ®°ÂûãÂæÆË™ø„ÄÅÂü∫Êñº XAI ÁöÑË™™ÊòéÁî¢Áîü„ÄÅXAI ÊñπÊ≥ïË©ï‰º∞„ÄÅXAI ÂºïÂ∞éË≥áÊñôÊì¥ÂÖÖ„ÄÅÈÇäÁ∑£Áõ∏ÂÆπÊ®°ÂûãÈñãÁôºÔºå‰ª•ÂèäÁî¢ÁîüÂèØÁêÜËß£ÁöÑË¶ñË¶∫ÂíåÊñáÂ≠óË™™Êòé„ÄÇÈÄèÈÅé XAI ÂºïÂ∞éÁöÑË≥áÊñôÊì¥ÂÖÖÔºåÁµêÂêàÈ†òÂüüÂ∞àÂÆ∂Áü•Ë≠ò‰ª•ÂèäË¶ñË¶∫ÂíåÊñáÂ≠óË™™ÊòéÁöÑÂ¢ûÂº∑Ê®°ÂûãÂ∑≤ÊàêÂäüÈÉ®ÁΩ≤Âú®Ë°åÂãïË£ùÁΩÆ‰∏äÔºå‰ª•ÊîØÊè¥ÊúÄÁµÇ‰ΩøÁî®ËÄÖÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑÂ†¥ÊôØ‰∏≠„ÄÇÂØ¶È©óÁµêÊûúÂ±ïÁ§∫‰∫ÜÊâÄÊèêÂá∫Êû∂ÊßãÁöÑÊúâÊïàÊÄßÔºåË°åÂãïÊ®°ÂûãÂú®È°ØËëóÊ∏õÂ∞ëÊ®°ÂûãÂ§ßÂ∞èÁöÑÂêåÊôÇÔºåÈÅîÂà∞‰∫ÜÁ´∂Áà≠ÂäõÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÊ≠§ÊñπÊ≥ïÁÇ∫Âú®ÈóúÈçµÂ∑•Ê•≠ÊáâÁî®‰∏≠Êõ¥Âª£Ê≥õÊé°Áî®ÂèØÈù†‰∏îÂèØËß£ÈáãÁöÑ AI Â∑•ÂÖ∑Èã™Ë∑ØÔºåÂú®ÈÄô‰∫õÊáâÁî®‰∏≠ÔºåÊ±∫Á≠ñÂøÖÈ†àÂø´ÈÄü‰∏îÂêàÁêÜ„ÄÇ

##### **Robust Utility-Preserving Text Anonymization Based on Large Language Models**
2407.11770v1 by Tianyu Yang, Xiaodan Zhu, Iryna Gurevych

Text anonymization is crucial for sharing sensitive data while maintaining
privacy. Existing techniques face the emerging challenges of re-identification
attack ability of Large Language Models (LLMs), which have shown advanced
capability in memorizing detailed information and patterns as well as
connecting disparate pieces of information. In defending against LLM-based
re-identification attacks, anonymization could jeopardize the utility of the
resulting anonymized data in downstream tasks -- the trade-off between privacy
and data utility requires deeper understanding within the context of LLMs. This
paper proposes a framework composed of three LLM-based components -- a privacy
evaluator, a utility evaluator, and an optimization component, which work
collaboratively to perform anonymization. To provide a practical model for
large-scale and real-time environments, we distill the anonymization
capabilities into a lightweight model using Direct Preference Optimization
(DPO). Extensive experiments demonstrate that the proposed models outperform
baseline models, showing robustness in reducing the risk of re-identification
while preserving greater data utility in downstream tasks. Our code and dataset
are available at https://github.com/UKPLab/arxiv2024-rupta.

ÊëòË¶ÅÔºöÊñáÂ≠óÂåøÂêçÂåñÂ∞çÊñºÂú®Á∂≠Ë≠∑Èö±ÁßÅÁöÑÂêåÊôÇÂÖ±‰∫´ÊïèÊÑüË≥áÊñôËá≥ÈóúÈáçË¶Å„ÄÇÁèæÊúâÊäÄË°ìÈù¢Ëá®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈáçÊñ∞Ë≠òÂà•ÊîªÊìäËÉΩÂäõÁöÑÊñ∞ÊåëÊà∞ÔºåLLM Âú®Ë®òÊÜ∂Ë©≥Á¥∞Ë≥áË®äÂíåÊ®°Âºè‰ª•ÂèäÈÄ£Êé•‰∏çÂêåË≥áË®äÊñπÈù¢Â±ïÁèæ‰∫ÜÈÄ≤ÈöéÂäüËÉΩ„ÄÇÂú®Èò≤Á¶¶Âü∫Êñº LLM ÁöÑÈáçÊñ∞Ë≠òÂà•ÊîªÊìäÊôÇÔºåÂåøÂêçÂåñÂèØËÉΩÊúÉÂç±ÂÆ≥ÂåøÂêçÂåñË≥áÊñôÂú®‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÁöÑÊïàÁî® ‚Äî‚Äî Èö±ÁßÅÂíåË≥áÊñôÊïàÁî®‰πãÈñìÁöÑÊ¨äË°°ÈúÄË¶ÅÂú® LLM ÁöÑËÉåÊôØ‰∏ãÊ∑±ÂÖ•ÁêÜËß£„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî±‰∏âÂÄãÂü∫Êñº LLM ÁöÑÁµÑÊàêÈÉ®ÂàÜÁµÑÊàêÁöÑÊ°ÜÊû∂ ‚Äî‚Äî ‰∏ÄÂÄãÈö±ÁßÅË©ï‰º∞Âô®„ÄÅ‰∏ÄÂÄãÊïàÁî®Ë©ï‰º∞Âô®Âíå‰∏ÄÂÄãÊúÄ‰Ω≥ÂåñÁµÑÊàêÈÉ®ÂàÜÔºåÂÆÉÂÄëÂçîÂêåÂ∑•‰Ωú‰ª•Âü∑Ë°åÂåøÂêçÂåñ„ÄÇÁÇ∫‰∫ÜÁÇ∫Â§ßË¶èÊ®°ÂíåÂØ¶ÊôÇÁí∞Â¢ÉÊèê‰æõ‰∏ÄÂÄãÂØ¶Áî®ÁöÑÊ®°ÂûãÔºåÊàëÂÄë‰ΩøÁî®Áõ¥Êé•ÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ (DPO) Â∞áÂåøÂêçÂåñÂäüËÉΩÊèêÁÖâÊàê‰∏ÄÂÄãËºïÈáèÁ¥öÊ®°Âûã„ÄÇÂ§ßÈáèÁöÑÂØ¶È©óË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂÑ™ÊñºÂü∫Ê∫ñÊ®°ÂûãÔºåÂú®Èôç‰ΩéÈáçÊñ∞Ë≠òÂà•È¢®Èö™ÁöÑÂêåÊôÇÔºåÂ±ïÁèæÂá∫Âú®‰øùÁïô‰∏ãÊ∏∏‰ªªÂãô‰∏≠Êõ¥Â§ßË≥áÊñôÊïàÁî®ÊñπÈù¢ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂèØÂú® https://github.com/UKPLab/arxiv2024-rupta ÂèñÂæó„ÄÇ

##### **Vectoring Languages**
2407.11766v1 by Joseph Chen

Recent breakthroughs in large language models (LLM) have stirred up global
attention, and the research has been accelerating non-stop since then.
Philosophers and psychologists have also been researching the structure of
language for decades, but they are having a hard time finding a theory that
directly benefits from the breakthroughs of LLMs. In this article, we propose a
novel structure of language that reflects well on the mechanisms behind
language models and go on to show that this structure is also better at
capturing the diverse nature of language compared to previous methods. An
analogy of linear algebra is adapted to strengthen the basis of this
perspective. We further argue about the difference between this perspective and
the design philosophy for current language models. Lastly, we discuss how this
perspective can lead us to research directions that may accelerate the
improvements of science fastest.

ÊëòË¶ÅÔºöËøëÊúüÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÁ™ÅÁ†¥ÊÄßÈÄ≤Â±ïÂºïËµ∑‰∫ÜÂÖ®ÁêÉÈóúÊ≥®ÔºåËÄåÁõ∏ÈóúÁöÑÁ†îÁ©∂‰πüËá™Ê≠§ÊåÅÁ∫åÂä†ÈÄü„ÄÇ
Âì≤Â≠∏ÂÆ∂ÂíåÂøÉÁêÜÂ≠∏ÂÆ∂Êï∏ÂçÅÂπ¥‰æÜ‰πü‰∏ÄÁõ¥Âú®Á†îÁ©∂Ë™ûË®ÄÁµêÊßãÔºå‰ΩÜ‰ªñÂÄëÂæàÈõ£ÊâæÂà∞‰∏ÄÁ®ÆÁêÜË´ñÔºåËÉΩÁõ¥Êé•Âæû LLM ÁöÑÁ™ÅÁ†¥‰∏≠ÂèóÁõä„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑË™ûË®ÄÁµêÊßãÔºåÂèØ‰ª•ÂæàÂ•ΩÂú∞ÂèçÊò†Ë™ûË®ÄÊ®°ÂûãËÉåÂæåÁöÑÊ©üÂà∂Ôºå‰∏¶ÈÄ≤‰∏ÄÊ≠•Ë°®ÊòéÔºåËàáÂÖàÂâçÁöÑË™ûË®ÄÊ®°ÂûãÁõ∏ÊØîÔºåÈÄôÁ®ÆÁµêÊßãÊõ¥ËÉΩÊçïÊçâË™ûË®ÄÁöÑÂ§öÊ®£ÊÄß„ÄÇÁ∑öÊÄß‰ª£Êï∏ÁöÑÈ°ûÊØîË¢´Áî®‰æÜÂº∑ÂåñÈÄôÁ®ÆËßÄÈªûÁöÑÂü∫Á§é„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ë´ñËø∞‰∫ÜÈÄôÁ®ÆËßÄÈªûËàáÁï∂ÂâçË™ûË®ÄÊ®°ÂûãÁöÑË®≠Ë®àÁêÜÂøµ‰πãÈñìÁöÑÂ∑ÆÁï∞„ÄÇÊúÄÂæåÔºåÊàëÂÄëË®éË´ñ‰∫ÜÈÄôÁ®ÆËßÄÈªûÂ¶Ç‰ΩïÂºïÈ†òÊàëÂÄëÈÄ≤Ë°åÁ†îÁ©∂Ôºå‰ª•ÊúÄÂø´ÈÄüÂ∫¶‰øÉÈÄ≤ÁßëÂ≠∏ÁöÑÈÄ≤Ê≠•„ÄÇ

##### **A Channel Attention-Driven Hybrid CNN Framework for Paddy Leaf Disease Detection**
2407.11753v1 by Pandiyaraju V, Shravan Venkatraman, Abeshek A, Pavan Kumar S, Aravintakshan S A, Senthil Kumar A M, Kannan A

Farmers face various challenges when it comes to identifying diseases in rice
leaves during their early stages of growth, which is a major reason for poor
produce. Therefore, early and accurate disease identification is important in
agriculture to avoid crop loss and improve cultivation. In this research, we
propose a novel hybrid deep learning (DL) classifier designed by extending the
Squeeze-and-Excitation network architecture with a channel attention mechanism
and the Swish ReLU activation function. The channel attention mechanism in our
proposed model identifies the most important feature channels required for
classification during feature extraction and selection. The dying ReLU problem
is mitigated by utilizing the Swish ReLU activation function, and the
Squeeze-andExcitation blocks improve information propagation and cross-channel
interaction. Upon evaluation, our model achieved a high F1-score of 99.76% and
an accuracy of 99.74%, surpassing the performance of existing models. These
outcomes demonstrate the potential of state-of-the-art DL techniques in
agriculture, contributing to the advancement of more efficient and reliable
disease detection systems.

ÊëòË¶ÅÔºöÂú®Ê∞¥Á®ªÁîüÈï∑ÂàùÊúüÔºåËæ≤Ê∞ëÂú®Ëæ®Ë≠òÁ®ªËëâÁñæÁóÖÊôÇÈù¢Ëá®ÂêÑÁ®ÆÊåëÊà∞ÔºåÈÄôÊòØÈÄ†ÊàêÁî¢Èáè‰∏ç‰Ω≥ÁöÑ‰∏ªË¶ÅÂéüÂõ†„ÄÇÂõ†Ê≠§ÔºåÂú®Ëæ≤Ê•≠‰∏≠ÂèäÊó©Ê∫ñÁ¢∫Âú∞Ëæ®Ë≠òÁñæÁóÖÂ∞çÊñºÈÅøÂÖç‰ΩúÁâ©ÊêçÂ§±ÂíåÊîπÂñÑËÄï‰ΩúËá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊ∑∑ÂêàÊ∑±Â∫¶Â≠∏Áøí (DL) ÂàÜÈ°ûÂô®ÔºåÂÖ∂Ë®≠Ë®àÊñπÂºèÁÇ∫Êì¥ÂÖÖ Squeeze-and-Excitation Á∂≤Ë∑ØÊû∂ÊßãÔºå‰∏¶Êê≠ÈÖçÈÄöÈÅìÊ≥®ÊÑèÂäõÊ©üÂà∂Âíå Swish ReLU ÂïüÁî®ÂáΩÊï∏„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊ®°Âûã‰∏≠ÁöÑÈÄöÈÅìÊ≥®ÊÑèÂäõÊ©üÂà∂ÊúÉÂú®ÁâπÂæµËêÉÂèñÂíåÈÅ∏ÊìáÊúüÈñìÊâæÂá∫ÂàÜÈ°ûÊâÄÈúÄÊúÄÈáçË¶ÅÁöÑÁâπÂæµÈÄöÈÅì„ÄÇÈÄèÈÅé‰ΩøÁî® Swish ReLU ÂïüÁî®ÂáΩÊï∏ÔºåÂèØ‰ª•Ê∏õËºï ReLU ÂïèÈ°åÔºåËÄå Squeeze-and-Excitation ÂçÄÂ°äÂâáÂèØ‰ª•ÊîπÂñÑË≥áË®äÂÇ≥ÈÅûÂíåË∑®ÈÄöÈÅì‰∫íÂãï„ÄÇÂú®Ë©ï‰º∞ÊôÇÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÅîÂà∞‰∫Ü 99.76% ÁöÑÈ´ò F1 ÂàÜÊï∏Âíå 99.74% ÁöÑÊ∫ñÁ¢∫ÁéáÔºåË∂ÖË∂ä‰∫ÜÁèæÊúâÊ®°ÂûãÁöÑÊïàËÉΩ„ÄÇÈÄô‰∫õÁµêÊûúË≠âÊòé‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑ DL ÊäÄË°ìÂú®Ëæ≤Ê•≠‰∏≠ÁöÑÊΩõÂäõÔºåÊúâÂä©ÊñºÊé®ÂãïÊõ¥ÊúâÊïàÁéá‰∏îÂèØÈù†ÁöÑÁñæÁóÖÂÅµÊ∏¨Á≥ªÁµ±„ÄÇ

##### **Universal Sound Separation with Self-Supervised Audio Masked Autoencoder**
2407.11745v1 by Junqi Zhao, Xubo Liu, Jinzheng Zhao, Yi Yuan, Qiuqiang Kong, Mark D. Plumbley, Wenwu Wang

Universal sound separation (USS) is a task of separating mixtures of
arbitrary sound sources. Typically, universal separation models are trained
from scratch in a supervised manner, using labeled data. Self-supervised
learning (SSL) is an emerging deep learning approach that leverages unlabeled
data to obtain task-agnostic representations, which can benefit many downstream
tasks. In this paper, we propose integrating a self-supervised pre-trained
model, namely the audio masked autoencoder (A-MAE), into a universal sound
separation system to enhance its separation performance. We employ two
strategies to utilize SSL embeddings: freezing or updating the parameters of
A-MAE during fine-tuning. The SSL embeddings are concatenated with the
short-time Fourier transform (STFT) to serve as input features for the
separation model. We evaluate our methods on the AudioSet dataset, and the
experimental results indicate that the proposed methods successfully enhance
the separation performance of a state-of-the-art ResUNet-based USS model.

ÊëòË¶ÅÔºöÈÄöÁî®ËÅ≤Èü≥ÂàÜÈõ¢ (USS) ÊòØ‰∏ÄÈ†ÖÂ∞á‰ªªÊÑèÈü≥Ê∫êÁöÑÊ∑∑ÂêàÁâ©ÂàÜÈñãÁöÑ‰ªªÂãô„ÄÇÈÄöÂ∏∏ÔºåÈÄöÁî®ÂàÜÈõ¢Ê®°ÂûãÊúÉ‰ΩøÁî®Ê®ôÁ±§Ë≥áÊñô‰ª•Áõ£Áù£ÁöÑÊñπÂºèÂæûÈ†≠ÈñãÂßãË®ìÁ∑¥„ÄÇËá™Áõ£Áù£Â≠∏Áøí (SSL) ÊòØ‰∏ÄÁ®ÆÊñ∞ËààÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÔºåÂÆÉÂà©Áî®Êú™Ê®ôÁ±§ÁöÑË≥áÊñô‰æÜÂèñÂæóËàá‰ªªÂãôÁÑ°ÈóúÁöÑË°®Á§∫ÔºåÈÄôÂèØ‰ª•ËÆìË®±Â§ö‰∏ãÊ∏∏‰ªªÂãôÂèóÁõä„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂª∫Ë≠∞Â∞áËá™Áõ£Áù£È†êË®ìÁ∑¥Ê®°ÂûãÔºàÂç≥Èü≥Ë®äÈÅÆÁΩ©Ëá™ÂãïÁ∑®Á¢ºÂô® (A-MAE)ÔºâÊï¥ÂêàÂà∞ÈÄöÁî®ËÅ≤Èü≥ÂàÜÈõ¢Á≥ªÁµ±‰∏≠Ôºå‰ª•Â¢ûÂº∑ÂÖ∂ÂàÜÈõ¢ÊïàËÉΩ„ÄÇÊàëÂÄëÊé°Áî®ÂÖ©Á®ÆÁ≠ñÁï•‰æÜÂà©Áî® SSL ÂµåÂÖ•ÔºöÂú®ÂæÆË™øÈÅéÁ®ã‰∏≠ÂáçÁµêÊàñÊõ¥Êñ∞ A-MAE ÁöÑÂèÉÊï∏„ÄÇSSL ÂµåÂÖ•ËàáÁü≠ÊôÇË∑ùÂÇÖÁ´ãËëâËΩâÊèõ (STFT) ÈÄ£Êé•Ôºå‰ΩúÁÇ∫ÂàÜÈõ¢Ê®°ÂûãÁöÑËº∏ÂÖ•ÁâπÂæµ„ÄÇÊàëÂÄëÂú® AudioSet Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÊ®°ÂûãÔºåÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÊàêÂäüÂú∞Â¢ûÂº∑‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂü∫Êñº ResUNet ÁöÑ USS Ê®°ÂûãÁöÑÂàÜÈõ¢ÊïàËÉΩ„ÄÇ

##### **How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies**
2407.11733v1 by Alina Leidinger, Richard Rogers

With the widespread availability of LLMs since the release of ChatGPT and
increased public scrutiny, commercial model development appears to have focused
their efforts on 'safety' training concerning legal liabilities at the expense
of social impact evaluation. This mimics a similar trend which we could observe
for search engine autocompletion some years prior. We draw on scholarship from
NLP and search engine auditing and present a novel evaluation task in the style
of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by
using four metrics, namely refusal rates, toxicity, sentiment and regard, with
and without safety system prompts. Our findings indicate an improvement to
stereotyping outputs with the system prompt, but overall a lack of attention by
LLMs under study to certain harms classified as toxic, particularly for prompts
about peoples/ethnicities and sexual orientation. Mentions of intersectional
identities trigger a disproportionate amount of stereotyping. Finally, we
discuss the implications of these findings about stereotyping harms in light of
the coming intermingling of LLMs and search and the choice of stereotyping
mitigation policy to adopt. We address model builders, academics, NLP
practitioners and policy makers, calling for accountability and awareness
concerning stereotyping harms, be it for training data curation, leader board
design and usage, or social impact measurement.

ÊëòË¶ÅÔºöÈö®Ëëó ChatGPT ÁôºÂ∏ÉÂæå LLM Âª£Ê≥õÂèØÁî®‰ª•ÂèäÂÖ¨ÁúæÁöÑÂØ©Êü•Â¢ûÂä†ÔºåÂïÜÊ•≠Ê®°ÂûãÈñãÁôº‰ºº‰πéÂ∞áÂÖ∂ÈáçÈªûÊîæÂú®„ÄåÂÆâÂÖ®„ÄçË®ìÁ∑¥‰∏äÔºå‰ª•ÊáâÂ∞çÊ≥ïÂæãË≤¨‰ªªÔºåËÄåÁäßÁâ≤‰∫ÜÁ§æÊúÉÂΩ±ÈüøË©ï‰º∞„ÄÇÈÄôÊ®°‰ªø‰∫ÜÊàëÂÄëÂπæÂπ¥ÂâçÂú®ÊêúÂ∞ãÂºïÊìéËá™ÂãïÂÆåÊàê‰∏≠ËßÄÂØüÂà∞ÁöÑÈ°û‰ººË∂®Âã¢„ÄÇÊàëÂÄëÂà©Áî® NLP ÂíåÊêúÂ∞ãÂºïÊìéÁ®ΩÊ†∏ÁöÑÂ≠∏Ë°ìÁ†îÁ©∂Ôºå‰∏¶ÊèêÂá∫‰ª•Ëá™ÂãïÂÆåÊàêÊèêÁ§∫ÁöÑÈ¢®Ê†ºÈÄ≤Ë°åÁöÑÊñ∞Á©éË©ï‰º∞‰ªªÂãôÔºå‰ª•Ë©ï‰º∞ LLM ‰∏≠ÁöÑÂàªÊùøÂç∞Ë±°„ÄÇÊàëÂÄë‰ΩøÁî®ÂõõÂÄãÊåáÊ®ôÔºàÊãíÁµïÁéá„ÄÅÊØíÊÄß„ÄÅÊÉÖÁ∑íÂíåÈáçË¶ñÔºâË©ï‰º∞ LLMÔºå‰∏¶‰ΩøÁî®Âíå‰∏ç‰ΩøÁî®ÂÆâÂÖ®Á≥ªÁµ±ÊèêÁ§∫„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÁ≥ªÁµ±ÊèêÁ§∫ÊîπÈÄ≤‰∫ÜÂàªÊùøÂç∞Ë±°Ëº∏Âá∫Ôºå‰ΩÜÁ∏ΩÈ´îËÄåË®ÄÔºåÁ†îÁ©∂‰∏≠ÁöÑ LLM Â∞çÊñºÊüê‰∫õË¢´Ê≠∏È°ûÁÇ∫ÊúâÊØíÁöÑÂÇ∑ÂÆ≥Áº∫‰πèÈóúÊ≥®ÔºåÁâπÂà•ÊòØÈóúÊñº‰∫∫/Á®ÆÊóèÂíåÊÄßÂèñÂêëÁöÑÊèêÁ§∫„ÄÇ‰∫§ÂèâË∫´ÂàÜÁöÑÊèêÂèäÊúÉÂºïÁôº‰∏çÊàêÊØî‰æãÁöÑÂàªÊùøÂç∞Ë±°„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊ†πÊìö LLM ÂíåÊêúÂ∞ãÂç≥Â∞á‰∫§Áπî‰ª•ÂèäÈÅ∏ÊìáË¶ÅÊé°Áî®ÁöÑÂàªÊùøÂç∞Ë±°Á∑©Ëß£ÊîøÁ≠ñÔºåË®éË´ñÈÄô‰∫õÈóúÊñºÂàªÊùøÂç∞Ë±°ÂÇ∑ÂÆ≥ÁöÑÁôºÁèæÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëÂëºÁ±≤Ê®°ÂûãÂª∫ÊßãËÄÖ„ÄÅÂ≠∏ËÄÖ„ÄÅNLP ÂæûÊ•≠ËÄÖÂíåÊîøÁ≠ñÂà∂ÂÆöËÄÖÔºåÂ∞çÂàªÊùøÂç∞Ë±°ÂÇ∑ÂÆ≥Ë≤†Ë≤¨‰∏¶ÊèêÈ´òË™çË≠òÔºåÁÑ°Ë´ñÊòØÈáùÂ∞çË®ìÁ∑¥Ë≥áÊñôÁ≠ñÂ±ï„ÄÅÊéíË°åÊ¶úË®≠Ë®àÂíå‰ΩøÁî®ÔºåÈÇÑÊòØÁ§æÊúÉÂΩ±ÈüøË°°Èáè„ÄÇ

##### **NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks**
2407.11698v1 by Alberto Pirillo, Luca Colombo, Manuel Roveri

Quantization has become increasingly pivotal in addressing the steadily
increasing computational and memory requirements of Deep Neural Networks
(DNNs). By reducing the number of bits used to represent weights and
activations (typically from 32-bit floating-point to 16-bit or 8-bit integers),
quantization reduces the memory footprint, energy consumption, and execution
time of DNN models. However, traditional quantization methods typically focus
on the inference of DNNs, while the training process still relies on
floating-point operations. To date, only one work in the literature has
addressed integer-only training for Multi-Layer Perceptron (MLP) architectures.
This work introduces NITRO-D, a new framework for training arbitrarily deep
integer-only Convolutional Neural Networks (CNNs) that operate entirely< in the
integer-only domain for both training and inference. NITRO-D is the first
framework in the literature enabling the training of integer-only CNNs without
the need to introduce a quantization scheme. Specifically, NITRO-D introduces a
novel architecture integrating multiple integer local-loss blocks, which
include the proposed NITRO Scaling Layer and the NITRO-ReLU activation
function. Additionally, it introduces a novel integer-only learning algorithm
derived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer
specifically designed to operate in an integer-only context. NITRO-D is
implemented in an open-source Python library. Extensive experimental
evaluations demonstrate its effectiveness across several state-of-the-art image
recognition datasets. Results show significant performance improvements from
2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art
solution, and the capability of training integer-only CNN architectures with
minimal accuracy degradation from -0.15% to -4.22% compared to floating-point
LES.

ÊëòË¶ÅÔºö<paragraph>ÈáèÂåñÂú®Ëß£ÂÜ≥Ê∑±Â∫¶Á•ûÁªèÁΩëÁªú (DNN) ‰∏çÊñ≠Â¢ûÂä†ÁöÑËÆ°ÁÆóÂíåÂ≠òÂÇ®ÈúÄÊ±ÇÊñπÈù¢ÂèòÂæóÊÑàÂèëÂÖ≥ÈîÆ„ÄÇÈÄöËøáÂáèÂ∞ëÁî®‰∫éË°®Á§∫ÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑ‰ΩçÊï∞ÔºàÈÄöÂ∏∏‰ªé 32 ‰ΩçÊµÆÁÇπÊï∞ÂáèÂ∞ëÂà∞ 16 ‰ΩçÊàñ 8 ‰ΩçÊï¥Êï∞ÔºâÔºåÈáèÂåñÂèØ‰ª•ÂáèÂ∞ë DNN Ê®°ÂûãÁöÑÂ≠òÂÇ®Á©∫Èó¥„ÄÅËÉΩËÄóÂíåÊâßË°åÊó∂Èó¥„ÄÇ‰ΩÜÊòØÔºå‰º†ÁªüÁöÑÈáèÂåñÊñπÊ≥ïÈÄöÂ∏∏‰∏ìÊ≥®‰∫é DNN ÁöÑÊé®ÁêÜÔºåËÄåËÆ≠ÁªÉËøáÁ®ã‰ªçÁÑ∂‰æùËµñ‰∫éÊµÆÁÇπËøêÁÆó„ÄÇËøÑ‰ªä‰∏∫Ê≠¢ÔºåÊñáÁåÆ‰∏≠Âè™Êúâ‰∏ÄÈ°πÂ∑•‰ΩúËß£ÂÜ≥‰∫ÜÂ§öÂ±ÇÊÑüÁü•Âô® (MLP) Êû∂ÊûÑÁöÑ‰ªÖÊï¥Êï∞ËÆ≠ÁªÉ„ÄÇËøôÈ°πÂ∑•‰Ωú‰ªãÁªç‰∫Ü NITRO-DÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éËÆ≠ÁªÉ‰ªªÊÑèÊ∑±Â∫¶‰ªÖÊï¥Êï∞Âç∑ÁßØÁ•ûÁªèÁΩëÁªú (CNN) ÁöÑÊñ∞Ê°ÜÊû∂ÔºåËØ•Ê°ÜÊû∂Âú®ËÆ≠ÁªÉÂíåÊé®ÁêÜ‰∏≠ÂÆåÂÖ®Âú®‰ªÖÊï¥Êï∞Âüü‰∏≠ËøêË°å„ÄÇNITRO-D ÊòØÊñáÁåÆ‰∏≠Á¨¨‰∏Ä‰∏™ËÉΩÂ§üÂú®‰∏çÂºïÂÖ•ÈáèÂåñÊñπÊ°àÁöÑÊÉÖÂÜµ‰∏ãËÆ≠ÁªÉ‰ªÖÊï¥Êï∞ CNN ÁöÑÊ°ÜÊû∂„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåNITRO-D ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊû∂ÊûÑÔºåÈõÜÊàê‰∫ÜÂ§ö‰∏™Êï¥Êï∞Â±ÄÈÉ®ÊçüÂ§±ÂùóÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÊèêÂá∫ÁöÑ NITRO Áº©ÊîæÂ±ÇÂíå NITRO-ReLU ÊøÄÊ¥ªÂáΩÊï∞„ÄÇÊ≠§Â§ñÔºåÂÆÉËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰ªÖÊï¥Êï∞Â≠¶‰π†ÁÆóÊ≥ïÔºåËØ•ÁÆóÊ≥ïÊ∫êËá™Â±ÄÈÉ®ËØØÂ∑Æ‰ø°Âè∑ (LES)ÔºåÂπ∂Âà©Áî® IntegerSGDÔºà‰∏ÄÁßç‰∏ìÈó®ËÆæËÆ°‰∏∫Âú®‰ªÖÊï¥Êï∞‰∏ä‰∏ãÊñá‰∏≠ËøêË°åÁöÑ‰ºòÂåñÂô®Ôºâ„ÄÇNITRO-D Âú®‰∏Ä‰∏™ÂºÄÊ∫ê Python Â∫ì‰∏≠ÂÆûÁé∞„ÄÇÂπøÊ≥õÁöÑÂÆûÈ™åËØÑ‰º∞ËØÅÊòé‰∫ÜÂÆÉÂú®Âá†‰∏™ÊúÄÂÖàËøõÁöÑÂõæÂÉèËØÜÂà´Êï∞ÊçÆÈõÜ‰∏äÁöÑÊúâÊïàÊÄß„ÄÇÁªìÊûúÊòæÁ§∫Ôºå‰ªÖÊï¥Êï∞ MLP Êû∂ÊûÑÁöÑÊÄßËÉΩ‰ªé 2.47% Âà∞ 5.96% ÊòéÊòæÊèêÈ´òÔºåË∂ÖËøá‰∫ÜÊúÄÂÖàËøõÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÂπ∂‰∏îËÉΩÂ§üËÆ≠ÁªÉ‰ªÖÊï¥Êï∞ CNN Êû∂ÊûÑÔºå‰∏éÊµÆÁÇπ LES Áõ∏ÊØîÔºåÁ≤æÂ∫¶‰∏ãÈôçÂπÖÂ∫¶‰ªé -0.15% Âà∞ -4.22%„ÄÇ</paragraph>

##### **CCoE: A Compact LLM with Collaboration of Experts**
2407.11686v2 by Shaomang Huang, Jianfeng Pan, Hanzhong Zheng

In the domain of Large Language Model (LLM), LLMs demonstrate significant
capabilities in natural language understanding and generation. With the growing
needs of applying LLMs on various domains, it is a research question that how
to efficiently train and build a model that has expertise in different domains
but with a low training cost. We propose CCoE architecture, a framework of
easily coupling multiple strong domain experts together to fuse into a big LLM,
provides a collective way of utilizing the different domain expert LLMs.
Besides, training a large collaborative of multiple expert LLMs requires a high
requirements on training sources. CCoE bypasses this problem through isolating
other experts and train each expert separately. The design of CCoE assembles
multiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE
layer could have one or more expert LLMs. Expert LLMs have different number of
layers and have been well-trained for different domain tasks. Each expert is
fine-tuned to be able to achieve the comparable results with SOTA domain LLMs.
We start from 5 experts in the domain of Code, Math, Law, text-to-SQL and
Medical. The results indicate that our CCoE framework can easily and
efficiently boost nearly 10%-20% performance on original base model in
different domains but using less resources on training, as well as inference.

ÊëòË¶ÅÔºöÂú®Â§ßË™ûË®ÄÊ®°Âûã (LLM) È†òÂüü‰∏≠ÔºåLLM Âú®Ëá™ÁÑ∂Ë™ûË®ÄÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢Â±ïÁèæÂá∫È°ØËëóÁöÑËÉΩÂäõ„ÄÇÈö®ËëóÂú®ÂêÑÂÄãÈ†òÂüüÊáâÁî® LLM ÁöÑÈúÄÊ±ÇÊó•ÁõäÂ¢ûÂä†ÔºåÂ¶Ç‰ΩïÊúâÊïàË®ìÁ∑¥ÂíåÂª∫Á´ã‰∏ÄÂÄãÂú®‰∏çÂêåÈ†òÂüü‰∏≠ÂÖ∑ÂÇôÂ∞àÊ•≠Áü•Ë≠òÔºå‰ΩÜË®ìÁ∑¥ÊàêÊú¨ÂçªÂæà‰ΩéÁöÑÊ®°ÂûãÔºåÊàêÁÇ∫‰∏ÄÂÄãÁ†îÁ©∂Ë™≤È°å„ÄÇÊàëÂÄëÊèêÂá∫ CCoE Êû∂ÊßãÔºå‰∏ÄÂÄãÂ∞áÂ§öÂÄãÂº∑Â§ßÁöÑÈ†òÂüüÂ∞àÂÆ∂ËºïÈ¨ÜÁµêÂêàÂú®‰∏ÄËµ∑‰ª•ËûçÂêàÊàê‰∏ÄÂÄãÂ§ßÂûã LLM ÁöÑÊ°ÜÊû∂ÔºåÊèê‰æõ‰∏ÄÁ®ÆÂÖ±ÂêåÂà©Áî®‰∏çÂêåÈ†òÂüüÂ∞àÂÆ∂ LLM ÁöÑÊñπÂºè„ÄÇÊ≠§Â§ñÔºåË®ìÁ∑¥Â§öÂÄãÂ∞àÂÆ∂ LLM ÁöÑÂ§ßÂûãÂçî‰ΩúÈúÄË¶ÅÂ∞çË®ìÁ∑¥‰æÜÊ∫êÊúâÂæàÈ´òÁöÑË¶ÅÊ±Ç„ÄÇCCoE ÈÄöÈÅéÈöîÈõ¢ÂÖ∂‰ªñÂ∞àÂÆ∂‰∏¶ÂàÜÂà•Ë®ìÁ∑¥ÊØèÂÄãÂ∞àÂÆ∂‰æÜÁπûÈÅéÈÄôÂÄãÂïèÈ°å„ÄÇCCoE ÁöÑË®≠Ë®àÈÄöÈÅé CoEÔºàÂ∞àÂÆ∂Âçî‰ΩúÔºâÂ±§ÁµÑÂêàÂ§öÂÄãÂ∞àÂÆ∂ LLM„ÄÇÊØèÂÄã CoE Â±§ÂèØ‰ª•Êúâ‰∏ÄÂÄãÊàñÂ§öÂÄãÂ∞àÂÆ∂ LLM„ÄÇÂ∞àÂÆ∂ LLM ÂÖ∑Êúâ‰∏çÂêåÁöÑÂ±§Êï∏Ôºå‰∏¶‰∏îÂ∑≤Á∂ìÈáùÂ∞ç‰∏çÂêåÁöÑÈ†òÂüü‰ªªÂãôÈÄ≤Ë°å‰∫ÜÂæàÂ•ΩÁöÑË®ìÁ∑¥„ÄÇÊØèÂÄãÂ∞àÂÆ∂ÈÉΩÁ∂ìÈÅéÂæÆË™øÔºåËÉΩÂ§†ÈÅîÂà∞Ëàá SOTA È†òÂüü LLM Áõ∏Áï∂ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÂæûÁ®ãÂºèÁ¢º„ÄÅÊï∏Â≠∏„ÄÅÊ≥ïÂæã„ÄÅÊñáÂ≠óËΩâ SQL ÂíåÈÜ´Â≠∏È†òÂüüÁöÑ 5 ‰ΩçÂ∞àÂÆ∂ÈñãÂßã„ÄÇÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑ CCoE Ê°ÜÊû∂ÂèØ‰ª•ËºïÈ¨Ü„ÄÅÊúâÊïàÂú∞ÊèêÂçá‰∏çÂêåÈ†òÂüü‰∏≠ÂéüÂßãÂü∫Á§éÊ®°ÂûãËøë 10%-20% ÁöÑÊïàËÉΩÔºå‰ΩÜË®ìÁ∑¥ÂíåÊé®ÁêÜ‰ΩøÁî®ÁöÑË≥áÊ∫êÊõ¥Â∞ë„ÄÇ

##### **MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models**
2407.11681v1 by Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi

As Large Language Models (LLMs) grow dramatically in size, there is an
increasing trend in compressing and speeding up these models. Previous studies
have highlighted the usefulness of gradients for importance scoring in neural
network compressing, especially in pruning medium-size networks. However, the
substantial memory requirements involved in calculating gradients with
backpropagation impede the utilization of gradients in guiding LLM pruning. As
a result, most pruning strategies for LLMs rely on gradient-free criteria, such
as weight magnitudes or a mix of magnitudes and activations. In this paper, we
devise a hybrid pruning criterion, which appropriately integrates magnitude,
activation, and gradient to capitalize on feature map sensitivity for pruning
LLMs. To overcome memory requirement barriers, we estimate gradients using only
forward passes. Based on this, we propose a Memory-effIcieNt structured prunIng
procedure for LLMs (MINI-LLM) to remove no-critical channels and
multi-attention heads. Experimental results demonstrate the superior
performance of MINI-LLM over existing gradient-free methods on three LLMs:
LLaMA, BLOOM, and OPT across various downstream tasks (classification,
multiple-choice, and generation), while MINI-LLM maintains a GPU memory
footprint akin to gradient-free methods.

ÊëòË¶ÅÔºöÈöèÁùÄÂ§ßËØ≠Ë®ÄÊ®°Âûã (LLM) ËßÑÊ®°ÁöÑÊÄ•ÂâßÂ¢ûÈïøÔºåÂéãÁº©ÂíåÂä†ÈÄüËøô‰∫õÊ®°ÂûãÁöÑË∂ãÂäø‰πüÂú®‰∏çÊñ≠Â¢ûÂä†„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Âº∫Ë∞É‰∫ÜÊ¢ØÂ∫¶Âú®Á•ûÁªèÁΩëÁªúÂéãÁº©‰∏≠Áî®‰∫éÈáçË¶ÅÊÄßËØÑÂàÜÁöÑÂÆûÁî®ÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®‰øÆÂâ™‰∏≠Á≠âËßÑÊ®°ÁöÑÁΩëÁªú‰∏≠„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî®ÂèçÂêë‰º†Êí≠ËÆ°ÁÆóÊ¢ØÂ∫¶ÊâÄÊ∂âÂèäÁöÑÂ§ßÈáèÂÜÖÂ≠òÈúÄÊ±ÇÈòªÁ¢ç‰∫ÜÊ¢ØÂ∫¶Âú®ÊåáÂØº LLM ‰øÆÂâ™‰∏≠ÁöÑÂà©Áî®„ÄÇÂõ†Ê≠§ÔºåÂ§ßÂ§öÊï∞ LLM ÁöÑ‰øÆÂâ™Á≠ñÁï•‰æùËµñ‰∫éÊó†Ê¢ØÂ∫¶ÁöÑÊ†áÂáÜÔºå‰æãÂ¶ÇÊùÉÈáçÂ§ßÂ∞èÊàñÂ§ßÂ∞èÂíåÊøÄÊ¥ªÁöÑÊ∑∑Âêà„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Ê∑∑Âêà‰øÆÂâ™Ê†áÂáÜÔºåËØ•Ê†áÂáÜÈÄÇÂΩìÂú∞ÈõÜÊàê‰∫ÜÂ§ßÂ∞è„ÄÅÊøÄÊ¥ªÂíåÊ¢ØÂ∫¶Ôºå‰ª•Âà©Áî®ÁâπÂæÅÂõæÊïèÊÑüÊÄßÊù•‰øÆÂâ™ LLM„ÄÇ‰∏∫‰∫ÜÂÖãÊúçÂÜÖÂ≠òÈúÄÊ±ÇÈöúÁ¢çÔºåÊàë‰ª¨‰ªÖ‰ΩøÁî®ÂâçÂêë‰º†ÈÄíÊù•‰º∞ËÆ°Ê¢ØÂ∫¶„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÁî®‰∫é LLM ÁöÑÂÜÖÂ≠òÈ´òÊïàÁªìÊûÑÂåñ‰øÆÂâ™Á®ãÂ∫è (MINI-LLM)Ôºå‰ª•Âà†Èô§ÈùûÂÖ≥ÈîÆÈÄöÈÅìÂíåÂ§öÂ§¥Ê≥®ÊÑèÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®‰∏â‰∏™ LLMÔºàLLaMA„ÄÅBLOOM Âíå OPTÔºâ‰∏äÔºåMINI-LLM Âú®ÂêÑÁßç‰∏ãÊ∏∏‰ªªÂä°ÔºàÂàÜÁ±ª„ÄÅÂ§öÈ°πÈÄâÊã©ÂíåÁîüÊàêÔºâ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊó†Ê¢ØÂ∫¶ÊñπÊ≥ïÔºåËÄå MINI-LLM ‰øùÊåÅ‰∫Ü‰∏éÊó†Ê¢ØÂ∫¶ÊñπÊ≥ïÁ±ª‰ººÁöÑ GPU ÂÜÖÂ≠òÂç†Áî®Á©∫Èó¥„ÄÇ

##### **SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation**
2407.11676v1 by Yanis Lalou, Th√©o Gnassounou, Antoine Collas, Antoine de Mathelin, Oleksii Kachaiev, Ambroise Odonnat, Alexandre Gramfort, Thomas Moreau, R√©mi Flamary

Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a
labeled source domain to perform well on an unlabeled target domain with some
data distribution shift. While many methods have been proposed in the
literature, fair and realistic evaluation remains an open question,
particularly due to methodological difficulties in selecting hyperparameters in
the unsupervised setting. With SKADA-Bench, we propose a framework to evaluate
DA methods and present a fair evaluation of existing shallow algorithms,
including reweighting, mapping, and subspace alignment. Realistic
hyperparameter selection is performed with nested cross-validation and various
unsupervised model selection scores, on both simulated datasets with controlled
shifts and real-world datasets across diverse modalities, such as images, text,
biomedical, and tabular data with specific feature extraction. Our benchmark
highlights the importance of realistic validation and provides practical
guidance for real-life applications, with key insights into the choice and
impact of model selection approaches. SKADA-Bench is open-source, reproducible,
and can be easily extended with novel DA methods, datasets, and model selection
criteria without requiring re-evaluating competitors. SKADA-Bench is available
on GitHub at https://github.com/scikit-adaptation/skada-bench.

ÊëòË¶ÅÔºöÁÑ°Áõ£Áù£ÂüüÈÅ©Êáâ (DA) ÊòØÂ∞áÂú®Ê®ôÁ±§‰æÜÊ∫êÂüü‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãË™øÊï¥ÁÇ∫Âú®ÂÖ∑ÊúâÊüê‰∫õË≥áÊñôÂàÜ‰ΩàËΩâÁßªÁöÑÊú™Ê®ôÁ±§ÁõÆÊ®ôÂüü‰∏äË°®ÁèæËâØÂ•Ω„ÄÇÈõñÁÑ∂ÊñáÁçª‰∏≠Â∑≤ÊèêÂá∫Ë®±Â§öÊñπÊ≥ïÔºå‰ΩÜÂÖ¨Âπ≥‰∏îÁèæÂØ¶ÁöÑË©ï‰º∞‰ªçÁÑ∂ÊòØ‰∏ÄÂÄãÂÖ¨ÈñãÁöÑÂïèÈ°åÔºåÁâπÂà•ÊòØÂõ†ÁÇ∫Âú®ÁÑ°Áõ£Áù£Áí∞Â¢É‰∏≠ÈÅ∏ÊìáË∂ÖÂèÉÊï∏ÁöÑÊñπÊ≥ïË´ñÂõ∞Èõ£„ÄÇÊúâ‰∫Ü SKADA-BenchÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãË©ï‰º∞ DA ÊñπÊ≥ïÁöÑÊû∂ÊßãÔºå‰∏¶Â∞çÁèæÊúâÁöÑÊ∑∫Â±§ÊºîÁÆóÊ≥ïÈÄ≤Ë°åÂÖ¨Âπ≥ÁöÑË©ï‰º∞ÔºåÂåÖÊã¨ÈáçÊñ∞Âä†Ê¨ä„ÄÅÂ∞çÊáâÂíåÂ≠êÁ©∫ÈñìÂ∞çÈΩä„ÄÇÂú®Ê®°Êì¨Ë≥áÊñôÈõÜÔºàÂÖ∑ÊúâÂèóÊéßËΩâÁßªÔºâÂíåË∑®‰∏çÂêåÊ®°ÂºèÔºà‰æãÂ¶ÇÂΩ±ÂÉè„ÄÅÊñáÂ≠ó„ÄÅÁîüÁâ©ÈÜ´Â≠∏ÂíåÂÖ∑ÊúâÁâπÂÆöÁâπÂæµËêÉÂèñÁöÑË°®Ê†ºË≥áÊñôÔºâÁöÑÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÔºå‰ΩøÁî®ÂµåÂ•ó‰∫§ÂèâÈ©óË≠âÂíåÂêÑÁ®ÆÁÑ°Áõ£Áù£Ê®°ÂûãÈÅ∏ÊìáË©ïÂàÜ‰æÜÂü∑Ë°åÁèæÂØ¶ÁöÑË∂ÖÂèÉÊï∏ÈÅ∏Êìá„ÄÇÊàëÂÄëÁöÑÂü∫Ê∫ñÂº∑Ë™ø‰∫ÜÁèæÂØ¶È©óË≠âÁöÑÈáçË¶ÅÊÄßÔºå‰∏¶ÁÇ∫ÂØ¶ÈöõÊáâÁî®Êèê‰æõÂØ¶Áî®ÁöÑÊåáÂ∞éÔºåÊ∑±ÂÖ•‰∫ÜËß£Ê®°ÂûãÈÅ∏ÊìáÊñπÊ≥ïÁöÑÈÅ∏ÊìáÂíåÂΩ±Èüø„ÄÇSKADA-Bench ÊòØÈñãÊ∫êÁöÑ„ÄÅÂèØË§áË£ΩÁöÑÔºå‰∏¶‰∏îÂèØ‰ª•ËºïÈ¨ÜÂú∞Êì¥ÂÖÖÊñ∞ÁöÑ DA ÊñπÊ≥ï„ÄÅË≥áÊñôÈõÜÂíåÊ®°ÂûãÈÅ∏ÊìáÊ®ôÊ∫ñÔºåËÄå‰∏çÈúÄË¶ÅÈáçÊñ∞Ë©ï‰º∞Á´∂Áà≠ËÄÖ„ÄÇSKADA-Bench ÂèØÂú® GitHub ‰∏äÁöÑ https://github.com/scikit-adaptation/skada-bench ÂèñÂæó„ÄÇ

##### **ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues**
2407.11660v1 by John Mendon√ßa, Isabel Trancoso, Alon Lavie

Despite being heralded as the new standard for dialogue evaluation, the
closed-source nature of GPT-4 poses challenges for the community. Motivated by
the need for lightweight, open source, and multilingual dialogue evaluators,
this paper introduces GenResCoh (Generated Responses targeting Coherence).
GenResCoh is a novel LLM generated dataset comprising over 130k negative and
positive responses and accompanying explanations seeded from XDailyDialog and
XPersona covering English, French, German, Italian, and Chinese. Leveraging
GenResCoh, we propose ECoh (Evaluation of Coherence), a family of evaluators
trained to assess response coherence across multiple languages. Experimental
results demonstrate that ECoh achieves multilingual detection capabilities
superior to the teacher model (GPT-3.5-Turbo) on GenResCoh, despite being based
on a much smaller architecture. Furthermore, the explanations provided by ECoh
closely align in terms of quality with those generated by the teacher model.

ÊëòË¶ÅÔºöÂÑòÁÆ°Ë¢´ÂÆ£ÂÇ≥ÁÇ∫Â∞çË©±Ë©ï‰º∞ÁöÑÊñ∞Ê®ôÊ∫ñÔºå‰ΩÜ GPT-4 ÁöÑÂ∞ÅÈñâÂéüÂßãÁ¢ºÁâπÊÄßÂ∞çÁ§æÁæ§‰æÜË™™ÊòØÂÄãÊåëÊà∞„ÄÇÂü∫ÊñºÂ∞çËºïÈáèÂåñ„ÄÅÈñãÊîæÂéüÂßãÁ¢ºÂíåÂ§öË™ûË®ÄÂ∞çË©±Ë©ï‰º∞Âô®ÁöÑÈúÄÊ±ÇÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü GenResCohÔºàÈáùÂ∞çÁõ∏Âπ≤ÊÄßÁöÑÁîüÊàêÂõûÊáâÔºâ„ÄÇGenResCoh ÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑ LLM ÁîüÊàêË≥áÊñôÈõÜÔºåÂåÖÂê´Ë∂ÖÈÅé 13 Ëê¨ÂÄãË≤†Èù¢ÂíåÊ≠£Èù¢ÂõûÊáâÔºå‰ª•Âèä‰æÜËá™ XDailyDialog Âíå XPersona ÁöÑÁõ∏ÈóúË™™ÊòéÔºåÊ∂µËìãËã±Ë™û„ÄÅÊ≥ïË™û„ÄÅÂæ∑Ë™û„ÄÅÁæ©Â§ßÂà©Ë™ûÂíå‰∏≠Êñá„ÄÇÂà©Áî® GenResCohÔºåÊàëÂÄëÊèêÂá∫‰∫Ü ECohÔºàÁõ∏Âπ≤ÊÄßË©ï‰º∞ÔºâÔºå‰∏ÄÂÄãË©ï‰º∞Âô®ÂÆ∂ÊóèÔºåÁ∂ìÈÅéË®ìÁ∑¥ÂèØ‰ª•Ë©ï‰º∞Â§öÁ®ÆË™ûË®ÄÁöÑÂõûÊáâÁõ∏Âπ≤ÊÄß„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂÑòÁÆ°Âü∫Êñº‰∏ÄÂÄãÂ∞èÂæóÂ§öÁöÑÊû∂ÊßãÔºå‰ΩÜ ECoh Âú® GenResCoh ‰∏äÈÅîÂà∞‰∫ÜÂ§öË™ûË®ÄÂÅµÊ∏¨ËÉΩÂäõÔºåÂÑ™ÊñºÊïôÂ∏´Ê®°Âûã (GPT-3.5-Turbo)„ÄÇÊ≠§Â§ñÔºåECoh Êèê‰æõÁöÑË™™ÊòéÂú®ÂìÅË≥™‰∏äËàáÊïôÂ∏´Ê®°ÂûãÁîüÊàêÁöÑË™™ÊòéÈùûÂ∏∏Êé•Ëøë„ÄÇ

##### **R-SFLLM: Jamming Resilient Framework for Split Federated Learning with Large Language Models**
2407.11654v1 by Aladin Djuhera, Vlad C. Andrei, Xinyang Li, Ullrich J. M√∂nich, Holger Boche, Walid Saad

Split federated learning (SFL) is a compute-efficient paradigm in distributed
machine learning (ML), where components of large ML models are outsourced to
remote servers. A significant challenge in SFL, particularly when deployed over
wireless channels, is the susceptibility of transmitted model parameters to
adversarial jamming that could jeopardize the learning process. This is
particularly pronounced for word embedding parameters in large language models
(LLMs), which are crucial for language understanding. In this paper, rigorous
insights are provided into the influence of jamming LLM word embeddings in SFL
by deriving an expression for the ML training loss divergence and showing that
it is upper-bounded by the mean squared error (MSE). Based on this analysis, a
physical layer framework is developed for resilient SFL with LLMs (R-SFLLM)
over wireless networks. R-SFLLM leverages wireless sensing data to gather
information on the jamming directions-of-arrival (DoAs) for the purpose of
devising a novel, sensing-assisted anti-jamming strategy while jointly
optimizing beamforming, user scheduling, and resource allocation. Extensive
experiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,
achieving close-to-baseline performance across various natural language
processing (NLP) tasks and datasets. The proposed methodology further
introduces an adversarial training component, where controlled noise exposure
significantly enhances the LLM's resilience to perturbed parameters during
training. The results show that more noise-sensitive models, such as RoBERTa,
benefit from this feature, especially when resource allocation is unfair. It is
also shown that worst-case jamming in particular translates into worst-case
model outcomes, thereby necessitating the need for jamming-resilient SFL
protocols.

ÊëòË¶ÅÔºöÂàÜÂâ≤ËÅîÈÇ¶Â≠∏Áøí (SFL) ÊòØ‰∏ÄÁ®ÆÂú®ÂàÜÊï£ÂºèÊ©üÂô®Â≠∏Áøí (ML) ‰∏≠Ë®àÁÆóÊïàÁéáÈ´òÁöÑÁØÑ‰æãÔºåÂÖ∂‰∏≠Â§ßÂûã ML Ê®°ÂûãÁöÑÁµÑÊàêÊúÉÂ§ñÂåÖÂà∞ÈÅ†Á´Ø‰º∫ÊúçÂô®„ÄÇSFL ‰∏≠ÁöÑ‰∏ÄÈ†ÖÈáçÂ§ßÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®ÁÑ°Á∑öÈ†ªÈÅì‰∏äÈÉ®ÁΩ≤ÊôÇÔºåÂú®ÊñºÂÇ≥Ëº∏ÁöÑÊ®°ÂûãÂèÉÊï∏ÂÆπÊòìÂèóÂà∞Â∞çÊäóÊÄßÂπ≤ÊìæÔºåÂèØËÉΩÊúÉÂç±ÂèäÂ≠∏ÁøíÈÅéÁ®ã„ÄÇÈÄôÂ∞çÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠ÁöÑË©ûÂµåÂÖ•ÂèÉÊï∏‰æÜË™™Â∞§ÂÖ∂ÊòéÈ°ØÔºåÈÄô‰∫õÂèÉÊï∏Â∞çÊñºË™ûË®ÄÁêÜËß£Ëá≥ÈóúÈáçË¶Å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÈÄèÈÅéÊé®Â∞é ML Ë®ìÁ∑¥ÊêçÂ§±Â∑ÆÁï∞ÁöÑË°®ÈÅîÂºèÔºå‰∏¶Ë≠âÊòéÂÆÉ‰ª•‰∏äÈôêÂùáÊñπË™§Â∑Æ (MSE) ÁÇ∫ÁïåÔºåÊ∑±ÂÖ•Êé¢Ë®é‰∫ÜÂπ≤Êìæ SFL ‰∏≠ÁöÑ LLM Ë©ûÂµåÂÖ•ÁöÑÂΩ±Èüø„ÄÇÊ†πÊìöÊ≠§ÂàÜÊûêÔºåÈñãÁôº‰∫Ü‰∏ÄÂÄãÁî®ÊñºÁÑ°Á∑öÁ∂≤Ë∑Ø‰∏äÁöÑÂΩàÊÄß SFL Ëàá LLM (R-SFLLM) ÁöÑÁâ©ÁêÜÂ±§Êû∂Êßã„ÄÇR-SFLLM Âà©Áî®ÁÑ°Á∑öÊÑüÊ∏¨Ë≥áÊñô‰æÜÊî∂ÈõÜÂπ≤ÊìæÂà∞‰æÜÊñπÂêë (DoA) ÁöÑË≥áË®äÔºåÁõÆÁöÑÊòØË®≠Ë®à‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊÑüÊ∏¨ËºîÂä©ÊäóÂπ≤ÊìæÁ≠ñÁï•ÔºåÂêåÊôÇÊúÄ‰Ω≥ÂåñÊ≥¢ÊùüÊàêÂΩ¢„ÄÅ‰ΩøÁî®ËÄÖÊéíÁ®ãÂíåË≥áÊ∫êÂàÜÈÖç„ÄÇ‰ΩøÁî® BERT Âíå RoBERTa Ê®°ÂûãÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫Ü R-SFLLM ÁöÑÊúâÊïàÊÄßÔºåÂú®ÂêÑÁ®ÆËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰ªªÂãôÂíåË≥áÊñôÈõÜ‰∏äÂØ¶Áèæ‰∫ÜÊé•ËøëÂü∫Á∑öÁöÑÊïàËÉΩ„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÈÄ≤‰∏ÄÊ≠•ÂºïÂÖ•‰∫ÜÂ∞çÊäóË®ìÁ∑¥ÁµÑÊàêÔºåÂÖ∂‰∏≠ÂèóÊéßÁöÑÈõúË®äÊö¥Èú≤ÊúÉÂú®Ë®ìÁ∑¥ÊúüÈñìÈ°ØËëóÂ¢ûÂº∑ LLM Â∞çÊìæÂãïÂèÉÊï∏ÁöÑÂΩàÊÄß„ÄÇÁµêÊûúË°®ÊòéÔºåÂ∞çÈõúË®äÊõ¥ÊïèÊÑüÁöÑÊ®°ÂûãÔºà‰æãÂ¶Ç RoBERTaÔºâÂèóÁõäÊñºÊ≠§ÂäüËÉΩÔºåÁâπÂà•ÊòØÂú®Ë≥áÊ∫êÂàÜÈÖç‰∏çÂÖ¨Âπ≥ÁöÑÊÉÖÊ≥Å‰∏ã„ÄÇÂÆÉ‰πüË°®ÊòéÔºåÊúÄÂ£ûÊÉÖÊ≥Å‰∏ãÁöÑÂπ≤ÊìæÁâπÂà•ÊúÉËΩâÂåñÁÇ∫ÊúÄÂ£ûÊÉÖÊ≥Å‰∏ãÁöÑÊ®°ÂûãÁµêÊûúÔºåÂõ†Ê≠§ÈúÄË¶ÅÈò≤Âπ≤Êìæ SFL ÂçîÂÆö„ÄÇ

##### **CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical Imaging**
2407.11652v1 by Sunny Gupta, Amit Sethi

Federated Learning (FL) offers a privacy-preserving approach to train models
on decentralized data. Its potential in healthcare is significant, but
challenges arise due to cross-client variations in medical image data,
exacerbated by limited annotations. This paper introduces Cross-Client
Variations Adaptive Federated Learning (CCVA-FL) to address these issues.
CCVA-FL aims to minimize cross-client variations by transforming images into a
common feature space. It involves expert annotation of a subset of images from
each client, followed by the selection of a client with the least data
complexity as the target. Synthetic medical images are then generated using
Scalable Diffusion Models with Transformers (DiT) based on the target client's
annotated images. These synthetic images, capturing diversity and representing
the original data, are shared with other clients. Each client then translates
its local images into the target image space using image-to-image translation.
The translated images are subsequently used in a federated learning setting to
develop a server model. Our results demonstrate that CCVA-FL outperforms
Vanilla Federated Averaging by effectively addressing data distribution
differences across clients without compromising privacy.

ÊëòË¶ÅÔºöËÅîÈÇ¶Â≠¶‰π† (FL) Êèê‰æõ‰∫Ü‰∏ÄÁßçÂú®ÂàÜÊï£ÂºèÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÊ®°ÂûãÁöÑÈöêÁßÅ‰øùÊä§ÊñπÊ≥ï„ÄÇÂÆÉÂú®ÂåªÁñó‰øùÂÅ•‰∏≠ÁöÑÊΩúÂäõÂæàÂ§ßÔºå‰ΩÜÁî±‰∫éÂåªÁñóÂõæÂÉèÊï∞ÊçÆ‰∏≠Â≠òÂú®Ë∑®ÂÆ¢Êà∑Á´ØÂ∑ÆÂºÇÔºåÂõ†Ê≠§Â∏¶Êù•‰∫ÜÊåëÊàòÔºåËÄåÊúâÈôêÁöÑÊ≥®ÈáäÂä†Ââß‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇÊú¨Êñá‰ªãÁªç‰∫ÜË∑®ÂÆ¢Êà∑Á´ØÂ∑ÆÂºÇËá™ÈÄÇÂ∫îËÅîÈÇ¶Â≠¶‰π† (CCVA-FL) Êù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢ò„ÄÇCCVA-FL Êó®Âú®ÈÄöËøáÂ∞ÜÂõæÂÉèËΩ¨Êç¢‰∏∫ÂÖ¨ÂÖ±ÁâπÂæÅÁ©∫Èó¥Êù•ÊúÄÂ∞èÂåñË∑®ÂÆ¢Êà∑Á´ØÂ∑ÆÂºÇ„ÄÇÂÆÉÊ∂âÂèä‰ªéÊØè‰∏™ÂÆ¢Êà∑Á´ØÊ≥®ÈáäÂõæÂÉèÂ≠êÈõÜÁöÑ‰∏ìÂÆ∂Ê≥®ÈáäÔºåÁÑ∂ÂêéÈÄâÊã©Êï∞ÊçÆÂ§çÊùÇÊÄßÊúÄ‰ΩéÁöÑÂÆ¢Êà∑Á´Ø‰Ωú‰∏∫ÁõÆÊ†á„ÄÇÁÑ∂Âêé‰ΩøÁî®Âü∫‰∫éÁõÆÊ†áÂÆ¢Êà∑Á´ØÊ≥®ÈáäÂõæÂÉèÁöÑÂèØÊâ©Â±ïÊâ©Êï£Ê®°Âûã‰∏é Transformer (DiT) ÁîüÊàêÂêàÊàêÂåªÂ≠¶ÂõæÂÉè„ÄÇËøô‰∫õÂêàÊàêÂõæÂÉèÊçïÊçâ‰∫ÜÂ§öÊ†∑ÊÄßÂπ∂‰ª£Ë°®‰∫ÜÂéüÂßãÊï∞ÊçÆÔºå‰∏éÂÖ∂‰ªñÂÆ¢Êà∑Á´ØÂÖ±‰∫´„ÄÇÁÑ∂ÂêéÔºåÊØè‰∏™ÂÆ¢Êà∑Á´Ø‰ΩøÁî®ÂõæÂÉèÂà∞ÂõæÂÉèÁøªËØëÂ∞ÜÂÖ∂Êú¨Âú∞ÂõæÂÉèËΩ¨Êç¢‰∏∫ÁõÆÊ†áÂõæÂÉèÁ©∫Èó¥„ÄÇÁøªËØëÂêéÁöÑÂõæÂÉèÈöèÂêéÂú®ËÅîÈÇ¶Â≠¶‰π†ËÆæÁΩÆ‰∏≠Áî®‰∫éÂºÄÂèëÊúçÂä°Âô®Ê®°Âûã„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåCCVA-FL ÈÄöËøáÊúâÊïàËß£ÂÜ≥Ë∑®ÂÆ¢Êà∑Á´ØÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÂ∑ÆÂºÇÂú®‰∏çÊçüÂÆ≥ÈöêÁßÅÁöÑÊÉÖÂÜµ‰∏ã‰ºò‰∫éÈ¶ôËçâËÅîÈÇ¶Âπ≥Âùá„ÄÇ

##### **A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting**
2407.11638v1 by He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, Tat-Seng Chua

Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation(RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast,
incorporating raw texts in specific complex events and fine-tuning LLMs
significantly improves performance. Moreover, enhanced with retrieval modules,
LLM can effectively capture temporal relational patterns hidden in historical
events. Meanwhile, issues such as popularity bias and the long-tail problem
still persist in LLMs, particularly in the RAG-based method. These findings not
only deepen our understanding of LLM-based event forecasting methods but also
highlight several promising research directions.We consider that this
comprehensive evaluation, along with the identified research opportunities,
will significantly contribute to future research on temporal event forecasting
through LLMs.

ÊëòË¶ÅÔºöËøëÊúüÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁßçËµÑÊñôÊé¢Âãò‰ªªÂä°‰∏≠Â±ïÁé∞Âá∫ÊûÅÂ§ßÁöÑÊΩúÂäõÔºå‰æãÂ¶ÇÁü•ËØÜÈóÆÁ≠î„ÄÅÊï∞Â≠¶Êé®ÁêÜÂíåÂ∏∏ËØÜÊé®ÁêÜ„ÄÇÁÑ∂ËÄåÔºåLLM Âú®Êó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÈù¢ÁöÑÊé®ÁêÜËÉΩÂäõÂ∞öÊú™Ë¢´ÂÖÖÂàÜÊé¢Á¥¢„ÄÇ‰∏∫‰∫ÜÁ≥ªÁªüÊÄßÂú∞Ë∞ÉÊü•ÂÖ∂Âú®Êó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÈù¢ÁöÑËÉΩÂäõÔºåÊàë‰ª¨ÂØπÂü∫‰∫é LLM ÁöÑÊó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÊñπÊ≥ïËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑËØÑ‰º∞„ÄÇÁî±‰∫éÁº∫‰πèÂêåÊó∂ÂåÖÂê´ÂõæË°®ÂíåÊñáÊú¨ËµÑÊñôÁöÑÈ´òÂìÅË¥®Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨È¶ñÂÖàÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ MidEast-TE-mini ÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ„ÄÇÂü∫‰∫éÊ≠§Êï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁ≥ªÂàóÂü∫Á∫øÊñπÊ≥ïÔºåÂÖ∂ÁâπÁÇπÊòØÂêÑÁßçËæìÂÖ•Ê†ºÂºèÂíåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê (RAG) Ê®°Âùó„ÄÇ‰ªéÂπøÊ≥õÁöÑÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨ÂèëÁé∞Áõ¥Êé•Â∞ÜÂéüÂßãÊñáÊú¨Êï¥ÂêàÂà∞ LLM ÁöÑËæìÂÖ•‰∏≠Âπ∂‰∏ç‰ºöÂ¢ûÂº∫Èõ∂Ê¨°Â≠¶‰π†Â§ñÊé®ÊÄßËÉΩ„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂú®ÁâπÂÆöÂ§çÊùÇ‰∫ã‰ª∂‰∏≠Á∫≥ÂÖ•ÂéüÂßãÊñáÊú¨Âπ∂ÂæÆË∞É LLM ‰ºöÊòæËëóÊèêÈ´òÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÈÄöËøáÊ£ÄÁ¥¢Ê®°ÂùóÁöÑÂ¢ûÂº∫ÔºåLLM ÂèØ‰ª•ÊúâÊïàÂú∞ÊçïÊçâÈöêËóèÂú®ÂéÜÂè≤‰∫ã‰ª∂‰∏≠ÁöÑÊó∂Èó¥ÂÖ≥Á≥ªÊ®°Âºè„ÄÇÂêåÊó∂ÔºåËØ∏Â¶ÇÊµÅË°åÂ∫¶ÂÅèÂ∑ÆÂíåÈïøÂ∞æÈóÆÈ¢òÁ≠âÈóÆÈ¢ò‰ªçÁÑ∂Â≠òÂú®‰∫é LLM ‰∏≠ÔºåÂ∞§ÂÖ∂ÊòØÂú®Âü∫‰∫é RAG ÁöÑÊñπÊ≥ï‰∏≠„ÄÇËøô‰∫õÂèëÁé∞‰∏ç‰ªÖÂä†Ê∑±‰∫ÜÊàë‰ª¨ÂØπÂü∫‰∫é LLM ÁöÑ‰∫ã‰ª∂È¢ÑÊµãÊñπÊ≥ïÁöÑÁêÜËß£ÔºåËøòÁ™ÅÂá∫‰∫ÜÂá†‰∏™ÊúâÂâçÊôØÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÊàë‰ª¨ËÆ§‰∏∫ÔºåËøôÈ°πÂÖ®Èù¢ÁöÑËØÑ‰º∞ÔºåËøûÂêåÂ∑≤Á°ÆÂÆöÁöÑÁ†îÁ©∂Êú∫‰ºöÔºåÂ∞ÜÊûÅÂ§ßÂú∞‰øÉËøõÈÄöËøá LLM ËøõË°åÊó∂Èó¥‰∫ã‰ª∂È¢ÑÊµãÁöÑÊú™Êù•Á†îÁ©∂„ÄÇ

##### **Rethinking Fair Graph Neural Networks from Re-balancing**
2407.11624v1 by Zhixun Li, Yushun Dong, Qiang Liu, Jeffrey Xu Yu

Driven by the powerful representation ability of Graph Neural Networks
(GNNs), plentiful GNN models have been widely deployed in many real-world
applications. Nevertheless, due to distribution disparities between different
demographic groups, fairness in high-stake decision-making systems is receiving
increasing attention. Although lots of recent works devoted to improving the
fairness of GNNs and achieved considerable success, they all require
significant architectural changes or additional loss functions requiring more
hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods
can easily match or surpass existing fair GNN methods. We claim that the
imbalance across different demographic groups is a significant source of
unfairness, resulting in imbalanced contributions from each group to the
parameters updating. However, these simple re-balancing methods have their own
shortcomings during training. In this paper, we propose FairGB, Fair Graph
Neural Network via re-Balancing, which mitigates the unfairness of GNNs by
group balancing. Technically, FairGB consists of two modules: counterfactual
node mixup and contribution alignment loss. Firstly, we select counterfactual
pairs across inter-domain and inter-class, and interpolate the ego-networks to
generate new samples. Guided by analysis, we can reveal the debiasing mechanism
of our model by the causal view and prove that our strategy can make sensitive
attributes statistically independent from target labels. Secondly, we reweigh
the contribution of each group according to gradients. By combining these two
modules, they can mutually promote each other. Experimental results on
benchmark datasets show that our method can achieve state-of-the-art results
concerning both utility and fairness metrics. Code is available at
https://github.com/ZhixunLEE/FairGB.

ÊëòË¶ÅÔºö<paragraph>Âú®ÂõæÁ•ûÁªèÁΩëÁªú (GNN) Âº∫Â§ßÁöÑË°®Á§∫ËÉΩÂäõÁöÑÊé®Âä®‰∏ãÔºåÂ§ßÈáèÁöÑ GNN Ê®°ÂûãÂ∑≤ÂπøÊ≥õÈÉ®ÁΩ≤Âú®ËÆ∏Â§öÂÆûÈôÖÂ∫îÁî®‰∏≠„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é‰∏çÂêå‰∫∫Âè£Áæ§‰Ωì‰πãÈó¥ÁöÑÂàÜÂ∏ÉÂ∑ÆÂºÇÔºåÈ´òÈ£éÈô©ÂÜ≥Á≠ñÁ≥ªÁªü‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄßÊ≠£ÂèóÂà∞Ë∂äÊù•Ë∂äÂ§öÁöÑÂÖ≥Ê≥®„ÄÇÂ∞ΩÁÆ°ÊúÄËøëÊúâËÆ∏Â§öËá¥Âäõ‰∫éÊèêÈ´ò GNN ÂÖ¨Âπ≥ÊÄßÁöÑÂ∑•‰ΩúÂπ∂ÂèñÂæó‰∫ÜÁõ∏ÂΩìÂ§ßÁöÑÊàêÂäüÔºå‰ΩÜÂÆÉ‰ª¨ÈÉΩÈúÄË¶ÅÂ§ßÈáèÁöÑÊû∂ÊûÑÊõ¥ÊîπÊàñÈôÑÂä†ÊçüÂ§±ÂáΩÊï∞ÔºåÈúÄË¶ÅÊõ¥Â§öË∂ÖÂèÇÊï∞Ë∞ÉÊï¥„ÄÇ‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊòØÔºåÊàë‰ª¨ÂèëÁé∞ÁÆÄÂçïÁöÑÈáçÊñ∞Âπ≥Ë°°ÊñπÊ≥ïÂèØ‰ª•ËΩªÊùæÂåπÈÖçÊàñË∂ÖË∂äÁé∞ÊúâÁöÑÂÖ¨Âπ≥ GNN ÊñπÊ≥ï„ÄÇÊàë‰ª¨Â£∞Áß∞Ôºå‰∏çÂêå‰∫∫Âè£Áæ§‰Ωì‰πãÈó¥ÁöÑ‰∏çÂπ≥Ë°°ÊòØÈÄ†Êàê‰∏çÂÖ¨Âπ≥ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊ†πÊ∫êÔºåÂØºËá¥ÊØè‰∏™Áæ§‰ΩìÂØπÂèÇÊï∞Êõ¥Êñ∞ÁöÑË¥°ÁåÆ‰∏çÂπ≥Ë°°„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÁÆÄÂçïÁöÑÈáçÊñ∞Âπ≥Ë°°ÊñπÊ≥ïÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÊúâÂÖ∂Ëá™Ë∫´ÁöÑÁº∫ÁÇπ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü FairGBÔºåÂç≥ÈÄöËøáÈáçÊñ∞Âπ≥Ë°°ÂÆûÁé∞ÂÖ¨Âπ≥ÂõæÁ•ûÁªèÁΩëÁªúÔºåÂÆÉÈÄöËøáÁªÑÂπ≥Ë°°Êù•ÂáèËΩª GNN ÁöÑ‰∏çÂÖ¨Âπ≥ÊÄß„ÄÇ‰ªéÊäÄÊúØ‰∏äËÆ≤ÔºåFairGB Áî±‰∏§‰∏™Ê®°ÂùóÁªÑÊàêÔºöÂèç‰∫ãÂÆûËäÇÁÇπÊ∑∑ÂêàÂíåË¥°ÁåÆÂØπÈΩêÊçüÂ§±„ÄÇÈ¶ñÂÖàÔºåÊàë‰ª¨Âú®ÂüüÈó¥ÂíåÁ±ªÈó¥ÈÄâÊã©Âèç‰∫ãÂÆûÂØπÔºåÂπ∂ÊèíÂÄºËá™ÊàëÁΩëÁªú‰ª•ÁîüÊàêÊñ∞Ê†∑Êú¨„ÄÇÂú®ÂàÜÊûêÁöÑÊåáÂØº‰∏ãÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÂõ†ÊûúËßÜÂõæÊè≠Á§∫Êàë‰ª¨Ê®°ÂûãÁöÑÂéªÂÅèÊú∫Âà∂ÔºåÂπ∂ËØÅÊòéÊàë‰ª¨ÁöÑÁ≠ñÁï•ÂèØ‰ª•‰ΩøÊïèÊÑüÂ±ûÊÄßÂú®ÁªüËÆ°‰∏äÁã¨Á´ã‰∫éÁõÆÊ†áÊ†áÁ≠æ„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨Ê†πÊçÆÊ¢ØÂ∫¶ÈáçÊñ∞ÊùÉË°°ÊØè‰∏™ÁªÑÁöÑË¥°ÁåÆ„ÄÇÈÄöËøáÁªìÂêàËøô‰∏§‰∏™Ê®°ÂùóÔºåÂÆÉ‰ª¨ÂèØ‰ª•Áõ∏‰∫í‰øÉËøõ„ÄÇÂü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂèØ‰ª•ÂÆûÁé∞ÊúâÂÖ≥ÊïàÁî®ÂíåÂÖ¨Âπ≥ÊÄßÊåáÊ†áÁöÑÊúÄÂÖàËøõÁªìÊûú„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/ZhixunLEE/FairGB Ëé∑Âæó„ÄÇ</paragraph>

##### **Graph Dimension Attention Networks for Enterprise Credit Assessment**
2407.11615v1 by Shaopeng Wei, Beni Egressy, Xingyan Chen, Yu Zhao, Fuzhen Zhuang, Roger Wattenhofer, Gang Kou

Enterprise credit assessment is critical for evaluating financial risk, and
Graph Neural Networks (GNNs), with their advanced capability to model
inter-entity relationships, are a natural tool to get a deeper understanding of
these financial networks. However, existing GNN-based methodologies
predominantly emphasize entity-level attention mechanisms for contagion risk
aggregation, often overlooking the heterogeneous importance of different
feature dimensions, thus falling short in adequately modeling credit risk
levels. To address this issue, we propose a novel architecture named Graph
Dimension Attention Network (GDAN), which incorporates a dimension-level
attention mechanism to capture fine-grained risk-related characteristics.
Furthermore, we explore the interpretability of the GNN-based method in
financial scenarios and propose a simple but effective data-centric explainer
for GDAN, called GDAN-DistShift. DistShift provides edge-level interpretability
by quantifying distribution shifts during the message-passing process.
Moreover, we collected a real-world, multi-source Enterprise Credit Assessment
Dataset (ECAD) and have made it accessible to the research community since
high-quality datasets are lacking in this field. Extensive experiments
conducted on ECAD demonstrate the effectiveness of our methods. In addition, we
ran GDAN on the well-known datasets SMEsD and DBLP, also with excellent
results.

ÊëòË¶ÅÔºö‰ºÅÊ•≠‰ø°Áî®Ë©ï‰º∞Â∞çÊñºË©ï‰º∞Ë≤°ÂãôÈ¢®Èö™Ëá≥ÈóúÈáçË¶ÅÔºåÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) ÂÖ∑ÊúâÂª∫Ê®°ÂØ¶È´îÈñìÈóú‰øÇÁöÑÂÖàÈÄ≤ËÉΩÂäõÔºåÊòØÊ∑±ÂÖ•‰∫ÜËß£ÈÄô‰∫õÈáëËûçÁ∂≤Ë∑ØÁöÑËá™ÁÑ∂Â∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫Êñº GNN ÁöÑÊñπÊ≥ï‰∏ªË¶ÅÂº∑Ë™øÂØ¶È´îÂ±§Á¥öÁöÑÈóúÊ≥®Ê©üÂà∂ÔºåÁî®ÊñºÂÇ≥ÊüìÈ¢®Èö™ÂΩôÁ∏ΩÔºåÂ∏∏Â∏∏ÂøΩÁï•‰∏çÂêåÁâπÂæµÁ∂≠Â∫¶ÁöÑÁï∞Ë≥™ÈáçË¶ÅÊÄßÔºåÂõ†Ê≠§Âú®ÂÖÖÂàÜÂª∫Ê®°‰ø°Áî®È¢®Èö™Â±§Á¥öÊñπÈù¢ÊúâÊâÄ‰∏çË∂≥„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ÂúñÂΩ¢Á∂≠Â∫¶ÈóúÊ≥®Á∂≤Ë∑Ø (GDAN) ÁöÑÊñ∞Êû∂ÊßãÔºåÂÆÉÁµêÂêà‰∫ÜÁ∂≠Â∫¶Â±§Á¥öÁöÑÈóúÊ≥®Ê©üÂà∂‰æÜÊçïÊçâÁ¥∞Á∑ªÁöÑÈ¢®Èö™Áõ∏ÈóúÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂü∫Êñº GNN ÁöÑÊñπÊ≥ïÂú®Ë≤°ÂãôÊÉÖÂ¢É‰∏≠ÁöÑÂèØËß£ÈáãÊÄßÔºå‰∏¶ÁÇ∫ GDAN ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑ‰ª•Ë≥áÊñôÁÇ∫‰∏≠ÂøÉÁöÑËß£ÈáãÂô®ÔºåÁ®±ÁÇ∫ GDAN-DistShift„ÄÇDistShift ÈÄèÈÅéÈáèÂåñË®äÊÅØÂÇ≥ÈÅûÈÅéÁ®ã‰∏≠ÁöÑÂàÜ‰ΩàËΩâÁßªÔºåÊèê‰æõÈÇäÁ∑£Â±§Á¥öÁöÑÂèØËß£ÈáãÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊî∂ÈõÜ‰∫Ü‰∏ÄÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑÂ§ö‰æÜÊ∫ê‰ºÅÊ•≠‰ø°Áî®Ë©ï‰º∞Ë≥áÊñôÈõÜ (ECAD)Ôºå‰∏¶Â∑≤Êèê‰æõÁµ¶Á†îÁ©∂Á§æÁæ§ÔºåÂõ†ÁÇ∫ÈÄôÂÄãÈ†òÂüüÁº∫‰πèÈ´òÂìÅË≥™ÁöÑË≥áÊñôÈõÜ„ÄÇÂú® ECAD ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂú®ËëóÂêçÁöÑË≥áÊñôÈõÜ SMEsD Âíå DBLP ‰∏äÂü∑Ë°å‰∫Ü GDANÔºå‰πüÁç≤Âæó‰∫ÜÊ•µ‰Ω≥ÁöÑÁµêÊûú„ÄÇ

##### **Bringing AI Participation Down to Scale: A Comment on Open AIs Democratic Inputs to AI Project**
2407.11613v1 by David Moats, Chandrima Ganguly

This commentary piece reviews the recent Open AI Democratic Inputs programme,
which funded 10 teams to design procedures for public participation in
generative AI. While applauding the technical innovations in these projects, we
identify several shared assumptions including the generality of LLMs,
extracting abstract values, soliciting solutions not problems and equating
participation with democracy. We call instead for AI participation which
involves specific communities and use cases and solicits concrete problems to
be remedied. We also find it important that these communities have a stake in
the outcome, including ownership of data or models.

ÊëòË¶ÅÔºöÈÄôÁØáË©ïË´ñÊñáÁ´†ÂõûÈ°ß‰∫ÜÊúÄËøëÁöÑ Open AI Ê∞ë‰∏ªËº∏ÂÖ•Ë®àÁï´Ôºå
Ë©≤Ë®àÁï´Ë≥áÂä©‰∫Ü 10 ÂÄãÂúòÈöäÔºå‰ª•Ë®≠Ë®àÂÖ¨ÁúæÂèÉËàáÁîüÊàêÂºè AI ÁöÑÁ®ãÂ∫è„ÄÇÈõñÁÑ∂ËÆöË≥ûÈÄô‰∫õÂ∞àÊ°à‰∏≠ÁöÑÊäÄË°ìÂâµÊñ∞ÔºåÊàëÂÄë
ÊâæÂá∫ÂπæÂÄãÂÖ±ÂêåÁöÑÂÅáË®≠ÔºåÂåÖÊã¨ LLM ÁöÑÊôÆÈÅçÊÄßÔºå
ÊèêÂèñÊäΩË±°ÂÉπÂÄºÔºåÂæµÊ±ÇËß£Ê±∫ÊñπÊ°àËÄåÈùûÂïèÈ°åÔºå‰ª•ÂèäÂ∞áÂèÉËàáÁ≠âÂêåÊñºÊ∞ë‰∏ª„ÄÇÊàëÂÄëÂëºÁ±≤‰ª• AI ÂèÉËàáÂèñËÄå‰ª£‰πãÔºå
ÂÖ∂‰∏≠Ê∂âÂèäÁâπÂÆöÁ§æÁæ§Âíå‰ΩøÁî®Ê°à‰æãÔºå‰∏¶ÂæµÊ±ÇÂÖ∑È´îÂïèÈ°å‰ª•Áç≤ÂæóË£úÊïë„ÄÇÊàëÂÄë‰πüÁôºÁèæÈÄô‰∫õÁ§æÁæ§Âú®ÊàêÊûú‰∏≠ÂÖ∑ÊúâÂà©ÂÆ≥Èóú‰øÇÈùûÂ∏∏ÈáçË¶ÅÔºåÂåÖÊã¨Ë≥áÊñôÊàñÊ®°ÂûãÁöÑÊâÄÊúâÊ¨ä„ÄÇ

##### **Statistical Reachability Analysis of Stochastic Cyber-Physical Systems under Distribution Shift**
2407.11609v1 by Navid Hashemi, Lars Lindemann, Jyotirmoy V. Deshmukh

Reachability analysis is a popular method to give safety guarantees for
stochastic cyber-physical systems (SCPSs) that takes in a symbolic description
of the system dynamics and uses set-propagation methods to compute an
overapproximation of the set of reachable states over a bounded time horizon.
In this paper, we investigate the problem of performing reachability analysis
for an SCPS that does not have a symbolic description of the dynamics, but
instead is described using a digital twin model that can be simulated to
generate system trajectories. An important challenge is that the simulator
implicitly models a probability distribution over the set of trajectories of
the SCPS; however, it is typical to have a sim2real gap, i.e., the actual
distribution of the trajectories in a deployment setting may be shifted from
the distribution assumed by the simulator. We thus propose a statistical
reachability analysis technique that, given a user-provided threshold
$1-\epsilon$, provides a set that guarantees that any reachable state during
deployment lies in this set with probability not smaller than this threshold.
Our method is based on three main steps: (1) learning a deterministic surrogate
model from sampled trajectories, (2) conducting reachability analysis over the
surrogate model, and (3) employing {\em robust conformal inference} using an
additional set of sampled trajectories to quantify the surrogate model's
distribution shift with respect to the deployed SCPS. To counter conservatism
in reachable sets, we propose a novel method to train surrogate models that
minimizes a quantile loss term (instead of the usual mean squared loss), and a
new method that provides tighter guarantees using conformal inference using a
normalized surrogate error. We demonstrate the effectiveness of our technique
on various case studies.

ÊëòË¶ÅÔºöÂèØÈÅîÊÄßÂàÜÊûêÊòØ‰∏ÄÁ®ÆÊµÅË°åÁöÑÊñπÊ≥ïÔºåÁî®ÊñºÂ∞çÈö®Ê©üÁ∂≤Ë∑ØÁâ©ÁêÜÁ≥ªÁµ± (SCPS) Êèê‰æõÂÆâÂÖ®‰øùË≠âÔºåÂÆÉÊé°Áî®Á≥ªÁµ±ÂãïÊÖãÁöÑÁ¨¶ËôüÊèèËø∞Ôºå‰∏¶‰ΩøÁî®ÈõÜÂêàÂÇ≥Êí≠ÊñπÊ≥ïË®àÁÆóÂá∫Âú®ÊúâÁïåÊôÇÈñìÁØÑÂúçÂÖßÂèØÈÅîÁãÄÊÖãÈõÜÂêàÁöÑÈÅéÂ∫¶Ëøë‰ºº„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫ÜÂ∞çÊ≤íÊúâÂãïÊÖãÁ¨¶ËôüÊèèËø∞ÁöÑ SCPS Âü∑Ë°åÂèØÈÅîÊÄßÂàÜÊûêÁöÑÂïèÈ°åÔºåËÄåÊòØ‰ΩøÁî®ÂèØ‰ª•Ê®°Êì¨‰ª•ÁîüÊàêÁ≥ªÁµ±ËªåË∑°ÁöÑÊï∏‰ΩçÈõôËÉûËÉéÊ®°ÂûãÈÄ≤Ë°åÊèèËø∞„ÄÇ‰∏ÄÂÄãÈáçË¶ÅÁöÑÊåëÊà∞ÊòØÔºåÊ®°Êì¨Âô®Èö±ÂºèÂú∞Â∞ç SCPS ËªåË∑°ÈõÜÂêàÂª∫Ê®°Ê©üÁéáÂàÜ‰ΩàÔºõÁÑ∂ËÄåÔºåÈÄöÂ∏∏ÊúÉÊúâ sim2real Â∑ÆË∑ùÔºåÂç≥ÔºåÈÉ®ÁΩ≤Ë®≠ÂÆö‰∏≠ËªåË∑°ÁöÑÂØ¶ÈöõÂàÜ‰ΩàÂèØËÉΩÊúÉÂÅèÈõ¢Ê®°Êì¨Âô®ÂÅáË®≠ÁöÑÂàÜ‰Ωà„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁµ±Ë®àÂèØÈÅîÊÄßÂàÜÊûêÊäÄË°ìÔºåÁµ¶ÂÆö‰ΩøÁî®ËÄÖÊèê‰æõÁöÑÈñæÂÄº $1-\epsilon$ÔºåÊèê‰æõ‰∏ÄÂÄãÈõÜÂêàÔºå‰øùË≠âÈÉ®ÁΩ≤ÊúüÈñìÁöÑ‰ªª‰ΩïÂèØÈÅîÁãÄÊÖãÈÉΩ‰ΩçÊñºÊ≠§ÈõÜÂêà‰∏≠ÔºåÊ©üÁéá‰∏çÂ∞èÊñºÊ≠§ÈñæÂÄº„ÄÇÊàëÂÄëÁöÑÊäÄË°ìÂü∫Êñº‰∏âÂÄã‰∏ªË¶ÅÊ≠•È©üÔºö(1) ÂæûÊé°Ê®£ËªåË∑°Â≠∏ÁøíÁ¢∫ÂÆöÊÄßÊõø‰ª£Ê®°ÂûãÔºå(2) Â∞çÊõø‰ª£Ê®°ÂûãÈÄ≤Ë°åÂèØÈÅîÊÄßÂàÜÊûêÔºå‰ª•Âèä (3) ‰ΩøÁî®Âè¶‰∏ÄÁµÑÊé°Ê®£ËªåË∑°Êé°Áî®„ÄåÁ©©ÂÅ•ÂÖ±ÂΩ¢Êé®Ë´ñ„Äç‰æÜÈáèÂåñÊõø‰ª£Ê®°ÂûãÁõ∏Â∞çÊñºÂ∑≤ÈÉ®ÁΩ≤ SCPS ÁöÑÂàÜ‰ΩàËΩâÁßª„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÂèØÈÅîÈõÜÂêà‰∏≠ÁöÑ‰øùÂÆà‰∏ªÁæ©ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÊñπÊ≥ï‰æÜË®ìÁ∑¥Êõø‰ª£Ê®°ÂûãÔºå‰ª•ÊúÄÂ∞èÂåñÂàÜ‰ΩçÊï∏ÊêçÂ§±È†ÖÔºàËÄå‰∏çÊòØÈÄöÂ∏∏ÁöÑÂùáÊñπÊêçÂ§±ÔºâÔºå‰ª•Âèä‰∏ÄÁ®Æ‰ΩøÁî®ÂÖ±ÂΩ¢Êé®Ë´ñÁöÑÊñ∞ÊñπÊ≥ïÔºå‰ΩøÁî®Ê≠£Ë¶èÂåñÁöÑÊõø‰ª£Ë™§Â∑ÆÊèê‰æõÊõ¥Âö¥Ê†ºÁöÑ‰øùË≠â„ÄÇÊàëÂÄëÂú®ÂêÑÁ®ÆÊ°à‰æãÁ†îÁ©∂‰∏≠Â±ïÁ§∫‰∫ÜÊàëÂÄëÊäÄË°ìÁöÑÊúâÊïàÊÄß„ÄÇ

##### **The Foundations of Tokenization: Statistical and Computational Concerns**
2407.11606v1 by Juan Luis Gastaldi, John Terilla, Luca Malagutti, Brian DuSell, Tim Vieira, Ryan Cotterell

Tokenization - the practice of converting strings of characters over an
alphabet into sequences of tokens over a vocabulary - is a critical yet
under-theorized step in the NLP pipeline. Notably, it remains the only major
step not fully integrated into widely used end-to-end neural models. This paper
aims to address this theoretical gap by laying the foundations of tokenization
from a formal perspective. By articulating and extending basic properties about
the category of stochastic maps, we propose a unified framework for
representing and analyzing tokenizer models. This framework allows us to
establish general conditions for the use of tokenizers. In particular, we
formally establish the necessary and sufficient conditions for a tokenizer
model to preserve the consistency of statistical estimators. Additionally, we
discuss statistical and computational concerns crucial for the design and
implementation of tokenizer models. The framework and results advanced in this
paper represent a step toward a robust theoretical foundation for neural
language modeling.

ÊëòË¶ÅÔºöÂàÜË©û - Â∞áÂ≠óÊØçË°®‰∏äÁöÑÂ≠óÂÖÉ‰∏≤ËΩâÊèõÁÇ∫Ë©ûÂΩô‰∏äÁöÑÁ¨¶ËôüÂ∫èÂàóÁöÑÂÅöÊ≥ï - ÊòØ NLP ÁÆ°Á∑ö‰∏≠‰∏ÄÂÄãÈóúÈçµ‰ΩÜÁêÜË´ñÂåñ‰∏çË∂≥ÁöÑÊ≠•È©ü„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂÆÉ‰ªçÁÑ∂ÊòØÂîØ‰∏Ä‰∏ÄÂÄãÂ∞öÊú™ÂÆåÂÖ®Êï¥ÂêàÂà∞Âª£Ê≥õ‰ΩøÁî®ÁöÑÁ´ØÂà∞Á´ØÁ•ûÁ∂ìÊ®°Âûã‰∏≠ÁöÑ‰∏ªË¶ÅÊ≠•È©ü„ÄÇÊú¨ÊñáÊó®Âú®ÈÄöÈÅéÂæûÂΩ¢ÂºèÁöÑËßíÂ∫¶Â•†ÂÆöÂàÜË©ûÁöÑÂü∫Á§é‰æÜËß£Ê±∫ÈÄôÂÄãÁêÜË´ñÂ∑ÆË∑ù„ÄÇÈÄöÈÅéÈó°Ëø∞ÂíåÊì¥Â±ïÈóúÊñºÈö®Ê©üÊò†Â∞ÑÈ°ûÂà•ÁöÑÂü∫Êú¨Â±¨ÊÄßÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÁî®ÊñºË°®Á§∫ÂíåÂàÜÊûêÂàÜË©ûÂô®Ê®°ÂûãÁöÑÁµ±‰∏ÄÊ°ÜÊû∂„ÄÇÈÄôÂÄãÊ°ÜÊû∂ËÆìÊàëÂÄëËÉΩÂ§†ÁÇ∫ÂàÜË©ûÂô®ÁöÑ‰ΩøÁî®Âª∫Á´ã‰∏ÄËà¨Ê¢ù‰ª∂„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÊ≠£ÂºèÂª∫Á´ã‰∫ÜÂàÜË©ûÂô®Ê®°Âûã‰øùÊåÅÁµ±Ë®à‰º∞Ë®àÂô®‰∏ÄËá¥ÊÄßÁöÑÂøÖË¶ÅÂíåÂÖÖÂàÜÊ¢ù‰ª∂„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®éË´ñ‰∫ÜÂ∞çÂàÜË©ûÂô®Ê®°ÂûãÁöÑË®≠Ë®àÂíåÂØ¶‰ΩúËá≥ÈóúÈáçË¶ÅÁöÑÁµ±Ë®àÂíåË®àÁÆóÂïèÈ°å„ÄÇÊú¨ÊñáÊèêÂá∫ÁöÑÊ°ÜÊû∂ÂíåÁµêÊûú‰ª£Ë°®‰∫ÜÊúùÂêëÁ©©ÂÅ•ÁöÑÁ•ûÁ∂ìË™ûË®ÄÊ®°ÂûãÁêÜË´ñÂü∫Á§éÈÇÅÂá∫ÁöÑ‰∏ÄÊ≠•„ÄÇ

##### **Enhancing TinyML Security: Study of Adversarial Attack Transferability**
2407.11599v1 by Parin Shah, Yuvaraj Govindarajulu, Pavan Kulkarni, Manojkumar Parmar

The recent strides in artificial intelligence (AI) and machine learning (ML)
have propelled the rise of TinyML, a paradigm enabling AI computations at the
edge without dependence on cloud connections. While TinyML offers real-time
data analysis and swift responses critical for diverse applications, its
devices' intrinsic resource limitations expose them to security risks. This
research delves into the adversarial vulnerabilities of AI models on
resource-constrained embedded hardware, with a focus on Model Extraction and
Evasion Attacks. Our findings reveal that adversarial attacks from powerful
host machines could be transferred to smaller, less secure devices like ESP32
and Raspberry Pi. This illustrates that adversarial attacks could be extended
to tiny devices, underscoring vulnerabilities, and emphasizing the necessity
for reinforced security measures in TinyML deployments. This exploration
enhances the comprehension of security challenges in TinyML and offers insights
for safeguarding sensitive data and ensuring device dependability in AI-powered
edge computing settings.

ÊëòË¶ÅÔºöÊúÄËøëÂú®‰∫∫Â∑•Êô∫ÊÖßÔºàAIÔºâÂíåÊ©üÂô®Â≠∏ÁøíÔºàMLÔºâÁöÑÈÄ≤Â±ïÔºåÊé®Âãï‰∫Ü TinyML ÁöÑÂ¥õËµ∑ÔºåÈÄôÊòØ‰∏ÄÂÄãÂú®ÈÇäÁ∑£ÈÄ≤Ë°å AI Ë®àÁÆóÁöÑÁØÑ‰æãÔºå‰∏çÈúÄË¶Å‰æùË≥¥Èõ≤Á´ØÈÄ£Á∑ö„ÄÇÂÑòÁÆ° TinyML Êèê‰æõ‰∫ÜÂç≥ÊôÇË≥áÊñôÂàÜÊûêÂíåÂø´ÈÄüÂõûÊáâÔºåÈÄôÂ∞çÂêÑÁ®ÆÊáâÁî®Á®ãÂºè‰æÜË™™Ëá≥ÈóúÈáçË¶ÅÔºå‰ΩÜÂÖ∂Ë£ùÁΩÆÂÖßÂú®ÁöÑË≥áÊ∫êÈôêÂà∂‰ΩøÂÆÉÂÄëÈù¢Ëá®ÂÆâÂÖ®È¢®Èö™„ÄÇÈÄôÈ†ÖÁ†îÁ©∂Êé¢Ë®é‰∫ÜÂú®Ë≥áÊ∫êÂèóÈôêÁöÑÂµåÂÖ•ÂºèÁ°¨È´î‰∏äÔºåAI Ê®°ÂûãÁöÑÂ∞çÊäóÊÄßÊºèÊ¥ûÔºåÈáçÈªûÂú®ÊñºÊ®°ÂûãËêÉÂèñÂíåË¶èÈÅøÊîªÊìä„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÂº∑Â§ßÁöÑ‰∏ªÊ©üÈõªËÖ¶ÁöÑÂ∞çÊäóÊÄßÊîªÊìäÂèØ‰ª•ËΩâÁßªÂà∞ËºÉÂ∞è„ÄÅÂÆâÂÖ®ÊÄßËºÉ‰ΩéÁöÑË£ùÁΩÆÔºå‰æãÂ¶Ç ESP32 Âíå Raspberry Pi„ÄÇÈÄôË™™Êòé‰∫ÜÂ∞çÊäóÊÄßÊîªÊìäÂèØ‰ª•Êì¥Â±ïÂà∞ÂæÆÂûãË£ùÁΩÆÔºåÂá∏È°Ø‰∫ÜÊºèÊ¥ûÔºå‰∏¶Âº∑Ë™øÂú® TinyML ÈÉ®ÁΩ≤‰∏≠Âº∑ÂåñÂÆâÂÖ®Êé™ÊñΩÁöÑÂøÖË¶ÅÊÄß„ÄÇÈÄôÈ†ÖÊé¢Ë®éÂ¢ûÂº∑‰∫ÜÂ∞ç TinyML ÂÆâÂÖ®ÊåëÊà∞ÁöÑÁêÜËß£Ôºå‰∏¶Êèê‰æõ‰∫ÜË¶ãËß£ÔºåÁî®Êñº‰øùË≠∑ÊïèÊÑüË≥áÊñôÔºå‰∏¶Á¢∫‰øùÂú® AI È©ÖÂãïÁöÑÈÇäÁ∑£ÈÅãÁÆóË®≠ÂÆö‰∏≠Ë£ùÁΩÆÁöÑÂèØÈù†ÊÄß„ÄÇ

##### **DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised Pre-Training**
2407.11594v1 by Guillermo Jimenez-Perez, Pedro Osorio, Josef Cersovsky, Javier Montalt-Tordera, Jens Hooge, Steffen Vogler, Sadegh Mohammadi

Diffusion models (DMs) have emerged as powerful foundation models for a
variety of tasks, with a large focus in synthetic image generation. However,
their requirement of large annotated datasets for training limits their
applicability in medical imaging, where datasets are typically smaller and
sparsely annotated. We introduce DiNO-Diffusion, a self-supervised method for
training latent diffusion models (LDMs) that conditions the generation process
on image embeddings extracted from DiNO. By eliminating the reliance on
annotations, our training leverages over 868k unlabelled images from public
chest X-Ray (CXR) datasets. Despite being self-supervised, DiNO-Diffusion shows
comprehensive manifold coverage, with FID scores as low as 4.7, and emerging
properties when evaluated in downstream tasks. It can be used to generate
semantically-diverse synthetic datasets even from small data pools,
demonstrating up to 20% AUC increase in classification performance when used
for data augmentation. Images were generated with different sampling strategies
over the DiNO embedding manifold and using real images as a starting point.
Results suggest, DiNO-Diffusion could facilitate the creation of large datasets
for flexible training of downstream AI models from limited amount of real data,
while also holding potential for privacy preservation. Additionally,
DiNO-Diffusion demonstrates zero-shot segmentation performance of up to 84.4%
Dice score when evaluating lung lobe segmentation. This evidences good CXR
image-anatomy alignment, akin to segmenting using textual descriptors on
vanilla DMs. Finally, DiNO-Diffusion can be easily adapted to other medical
imaging modalities or state-of-the-art diffusion models, opening the door for
large-scale, multi-domain image generation pipelines for medical imaging.

ÊëòË¶ÅÔºöÊì¥Êï£Ê®°Âûã (DM) Â∑≤ÊàêÁÇ∫ÂêÑÁ®Æ‰ªªÂãô‰∏≠Âº∑Â§ßÁöÑÂü∫Á§éÊ®°ÂûãÔºåÁâπÂà•ÊòØÂêàÊàêÂΩ±ÂÉèÁîüÊàê„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®Ë®ìÁ∑¥‰∏≠Â∞çÂ§ßÂûãÊ®ôË®ªË≥áÊñôÈõÜÁöÑË¶ÅÊ±ÇÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®ÈÜ´ÁôÇÂΩ±ÂÉè‰∏≠ÁöÑÊáâÁî®ÔºåËÄåÈÜ´ÁôÇÂΩ±ÂÉèÁöÑË≥áÊñôÈõÜÈÄöÂ∏∏ËºÉÂ∞è‰∏îÊ®ôË®ªÁ®ÄÁñè„ÄÇÊàëÂÄëÂºïÂÖ•‰∫Ü DiNO-DiffusionÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºË®ìÁ∑¥Ê¢ù‰ª∂ÁîüÊàêÈÅéÁ®ãÁöÑÊΩõÂú®Êì¥Êï£Ê®°Âûã (LDM) ÁöÑËá™Áõ£Áù£ÊñπÊ≥ïÔºåË©≤ÈÅéÁ®ãÂü∫ÊñºÂæû DiNO ‰∏≠ÊèêÂèñÁöÑÂΩ±ÂÉèÂµåÂÖ•„ÄÇÈÄèÈÅéÊ∂àÈô§Â∞çÊ®ôË®ªÁöÑ‰æùË≥¥ÔºåÊàëÂÄëÁöÑË®ìÁ∑¥Âà©Áî®‰∫Ü‰æÜËá™ÂÖ¨ÂÖ±ËÉ∏ÈÉ® X ÂÖâ (CXR) Ë≥áÊñôÈõÜÁöÑË∂ÖÈÅé 868k ÂºµÊú™Ê®ôË®ªÂΩ±ÂÉè„ÄÇÂÑòÁÆ°ÊòØËá™Áõ£Áù£ÁöÑÔºå‰ΩÜ DiNO-Diffusion È°ØÁ§∫Âá∫ÂÖ®Èù¢ÁöÑÊµÅÂΩ¢Ë¶ÜËìãÔºåFID ÂàÜÊï∏‰ΩéËá≥ 4.7Ôºå‰∏¶‰∏îÂú®Ë©ï‰º∞‰∏ãÊ∏∏‰ªªÂãôÊôÇÂá∫Áèæ‰∫ÜÊñ∞ËààÁöÑÂ±¨ÊÄß„ÄÇÂÆÉÂèØÁî®ÊñºÂæûÂ∞èÂûãË≥áÊñôÂ∫´ÁîüÊàêË™ûÁæ©Â§öÊ®£ÁöÑÂêàÊàêË≥áÊñôÈõÜÔºåÂú®Áî®ÊñºË≥áÊñôÊì¥ÂÖÖÊôÇÔºåÂàÜÈ°ûÊïàËÉΩÊèêÂçáÂπÖÂ∫¶È´òÈÅî 20% AUC„ÄÇÂΩ±ÂÉèÊòØÂú® DiNO ÂµåÂÖ•ÊµÅÂΩ¢‰∏ä‰ΩøÁî®‰∏çÂêåÁöÑÂèñÊ®£Á≠ñÁï•ÁîüÊàêÁöÑÔºå‰∏¶‰ΩøÁî®ÁúüÂØ¶ÂΩ±ÂÉè‰ΩúÁÇ∫Ëµ∑Èªû„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåDiNO-Diffusion ÂèØ‰ª•‰øÉÈÄ≤ÂæûÊúâÈôêÁöÑÁúüÂØ¶Ë≥áÊñô‰∏≠ÈùàÊ¥ªË®ìÁ∑¥‰∏ãÊ∏∏ AI Ê®°ÂûãÁöÑÂ§ßÂûãË≥áÊñôÈõÜÁöÑÂª∫Á´ãÔºåÂêåÊôÇ‰πüÂÖ∑ÊúâÈö±ÁßÅ‰øùË≠∑ÁöÑÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåDiNO-Diffusion Âú®Ë©ï‰º∞ËÇ∫ËëâÂàÜÂâ≤ÊôÇÂ±ïÁ§∫‰∫ÜÈ´òÈÅî 84.4% ÁöÑ Dice ÂàÜÊï∏ÁöÑÈõ∂Ê¨°Â≠∏ÁøíÂàÜÂâ≤ÊïàËÉΩ„ÄÇÈÄôË≠âÊòé‰∫ÜËâØÂ•ΩÁöÑ CXR ÂΩ±ÂÉèËß£ÂâñÂ∞çÈΩäÔºåÈ°û‰ººÊñºÂú®È¶ôËçâ DM ‰∏ä‰ΩøÁî®ÊñáÂ≠óÊèèËø∞Á¨¶ÈÄ≤Ë°åÂàÜÂâ≤„ÄÇÊúÄÂæåÔºåDiNO-Diffusion ÂèØ‰ª•ËºïÈ¨ÜÈÅ©ÊáâÂÖ∂‰ªñÈÜ´ÁôÇÂΩ±ÂÉèÊñπÂºèÊàñÊúÄÂÖàÈÄ≤ÁöÑÊì¥Êï£Ê®°ÂûãÔºåÁÇ∫ÈÜ´ÁôÇÂΩ±ÂÉèÁöÑÂ§ßË¶èÊ®°„ÄÅÂ§öÈ†òÂüüÂΩ±ÂÉèÁîüÊàêÁÆ°ÈÅìÈñãÂïü‰∫ÜÂ§ßÈñÄ„ÄÇ

##### **AdaptEval: Evaluating Large Language Models on Domain Adaptation for Text Summarization**
2407.11591v1 by Anum Afzal, Ribin Chalumattu, Florian Matthes, Laura Mascarell Espuny

Despite the advances in the abstractive summarization task using Large
Language Models (LLM), there is a lack of research that asses their abilities
to easily adapt to different domains. We evaluate the domain adaptation
abilities of a wide range of LLMs on the summarization task across various
domains in both fine-tuning and in-context learning settings. We also present
AdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a
domain benchmark and a set of metrics to facilitate the analysis of domain
adaptation. Our results demonstrate that LLMs exhibit comparable performance in
the in-context learning setting, regardless of their parameter scale.

ÊëòË¶ÅÔºöÂÑòÁÆ°‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÊäΩË±°ÊëòË¶Å‰ªªÂãô‰∏≠ÂèñÂæóÈÄ≤Â±ïÔºå‰ΩÜÁº∫‰πèÁ†îÁ©∂Ë©ï‰º∞ÂÖ∂ËºïÈ¨ÜÈÅ©Êáâ‰∏çÂêåÈ†òÂüüÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëË©ï‰º∞‰∫ÜÂêÑÁ®Æ LLM Âú®ÊëòË¶Å‰ªªÂãô‰∏≠Â∞ç‰∏çÂêåÈ†òÂüüÁöÑÈ†òÂüüÈÅ©ÊáâËÉΩÂäõÔºåÂåÖÊã¨ÂæÆË™øÂíåÊÉÖÂ¢ÉÂ≠∏ÁøíË®≠ÂÆö„ÄÇÊàëÂÄëÈÇÑÊèêÂá∫‰∫Ü AdaptEvalÔºåÈÄôÊòØÁ¨¨‰∏ÄÂÄãÈ†òÂüüÈÅ©ÊáâË©ï‰º∞Â•ó‰ª∂„ÄÇAdaptEval ÂåÖÂê´‰∏ÄÂÄãÈ†òÂüüÂü∫Ê∫ñÂíå‰∏ÄÁµÑÊåáÊ®ôÔºå‰ª•Âà©ÊñºÂàÜÊûêÈ†òÂüüÈÅ©Êáâ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºåÁÑ°Ë´ñ LLM ÁöÑÂèÉÊï∏Ë¶èÊ®°Â¶Ç‰ΩïÔºåÂÆÉÂÄëÂú®ÊÉÖÂ¢ÉÂ≠∏ÁøíË®≠ÂÆö‰∏≠ÈÉΩË°®ÁèæÂá∫Áõ∏Áï∂ÁöÑÊïàËÉΩ„ÄÇ

##### **QVD: Post-training Quantization for Video Diffusion Models**
2407.11585v2 by Shilong Tian, Hong Chen, Chengtao Lv, Yu Liu, Jinyang Guo, Xianglong Liu, Shengxi Li, Hao Yang, Tao Xie

Recently, video diffusion models (VDMs) have garnered significant attention
due to their notable advancements in generating coherent and realistic video
content. However, processing multiple frame features concurrently, coupled with
the considerable model size, results in high latency and extensive memory
consumption, hindering their broader application. Post-training quantization
(PTQ) is an effective technique to reduce memory footprint and improve
computational efficiency. Unlike image diffusion, we observe that the temporal
features, which are integrated into all frame features, exhibit pronounced
skewness. Furthermore, we investigate significant inter-channel disparities and
asymmetries in the activation of video diffusion models, resulting in low
coverage of quantization levels by individual channels and increasing the
challenge of quantization. To address these issues, we introduce the first PTQ
strategy tailored for video diffusion models, dubbed QVD. Specifically, we
propose the High Temporal Discriminability Quantization (HTDQ) method, designed
for temporal features, which retains the high discriminability of quantized
features, providing precise temporal guidance for all video frames. In
addition, we present the Scattered Channel Range Integration (SCRI) method
which aims to improve the coverage of quantization levels across individual
channels. Experimental validations across various models, datasets, and
bit-width settings demonstrate the effectiveness of our QVD in terms of diverse
metrics. In particular, we achieve near-lossless performance degradation on
W8A8, outperforming the current methods by 205.12 in FVD.

ÊëòË¶ÅÔºö<paragraph>ËøëÊúüÔºåÂΩ±ÁâáÊâ©Êï£Ê®°Âûã (VDM) Âõ†ÂÖ∂Âú®ÁîüÊàêËøûË¥Ø‰∏îÈÄºÁúüÁöÑÂΩ±ÁâáÂÜÖÂÆπÊñπÈù¢ÂèñÂæóÊòæËëóËøõÂ±ïËÄåÂ§áÂèóÂÖ≥Ê≥®„ÄÇÁÑ∂ËÄåÔºåÂêåÊó∂Â§ÑÁêÜÂ§ö‰∏™Â∏ßÁâπÂæÅÔºåÂä†‰∏äÊ®°ÂûãËßÑÊ®°Â∫ûÂ§ßÔºåÂØºËá¥Âª∂ËøüÈ´ò‰∏îÂÜÖÂ≠òÊ∂àËÄóÂ§ßÔºåÈòªÁ¢ç‰∫ÜÂÖ∂Êõ¥ÂπøÊ≥õÁöÑÂ∫îÁî®„ÄÇËÆ≠ÁªÉÂêéÈáèÂåñ (PTQ) ÊòØ‰∏ÄÁßçÊúâÊïàÁöÑÊäÄÊúØÔºåÂèØÂáèÂ∞ëÂÜÖÂ≠òÂç†Áî®Âπ∂ÊèêÈ´òËÆ°ÁÆóÊïàÁéá„ÄÇ‰∏éÂõæÂÉèÊâ©Êï£‰∏çÂêåÔºåÊàë‰ª¨ËßÇÂØüÂà∞Êï¥ÂêàÂà∞ÊâÄÊúâÂ∏ßÁâπÂæÅ‰∏≠ÁöÑÊó∂Èó¥ÁâπÂæÅË°®Áé∞Âá∫ÊòéÊòæÁöÑÂÅèÂ∫¶„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨Ë∞ÉÊü•‰∫ÜÂΩ±ÁâáÊâ©Êï£Ê®°ÂûãÊøÄÊ¥ª‰∏≠ÁöÑÊòæËëóÁöÑÈÄöÈÅìÈó¥Â∑ÆÂºÇÂíå‰∏çÂØπÁß∞ÊÄßÔºåÂØºËá¥ÂêÑ‰∏™ÈÄöÈÅìÁöÑÈáèÂåñÁ∫ßÂà´Ë¶ÜÁõñÁéá‰ΩéÔºåÂ¢ûÂä†‰∫ÜÈáèÂåñÁöÑÈöæÂ∫¶„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÁ¨¨‰∏Ä‰∏™‰∏ì‰∏∫ÂΩ±ÁâáÊâ©Êï£Ê®°ÂûãÈáèË∫´ÂÆöÂà∂ÁöÑ PTQ Á≠ñÁï•ÔºåÁß∞‰∏∫ QVD„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÈíàÂØπÊó∂Èó¥ÁâπÂæÅËÆæËÆ°ÁöÑÈ´òÊó∂Èó¥ÂèØËæ®Âà´ÊÄßÈáèÂåñ (HTDQ) ÊñπÊ≥ïÔºåÂÆÉ‰øùÁïô‰∫ÜÈáèÂåñÁâπÂæÅÁöÑÈ´òÂèØËæ®Âà´ÊÄßÔºå‰∏∫ÊâÄÊúâÂΩ±ÁâáÂ∏ßÊèê‰æõÁ≤æÁ°ÆÁöÑÊó∂Èó¥ÊåáÂØº„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂàÜÊï£ÈÄöÈÅìËåÉÂõ¥Êï¥Âêà (SCRI) ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÂêÑ‰∏™ÈÄöÈÅìÁöÑÈáèÂåñÁ∫ßÂà´Ë¶ÜÁõñÁéá„ÄÇË∑®Ë∂äÂêÑÁßçÊ®°Âûã„ÄÅÊï∞ÊçÆÈõÜÂíå‰ΩçÂÆΩËÆæÁΩÆÁöÑÂÆûÈ™åÈ™åËØÅËØÅÊòé‰∫ÜÊàë‰ª¨ÁöÑ QVD Âú®ÂêÑÁßçÊåáÊ†áÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÁâπÂà´ÊòØÔºåÊàë‰ª¨Âú® W8A8 ‰∏äÂÆûÁé∞‰∫ÜÊé•ËøëÊó†ÊçüÁöÑÊÄßËÉΩ‰∏ãÈôçÔºåÂú® FVD ‰∏≠ÊØîÂΩìÂâçÊñπÊ≥ïÈ´òÂá∫ 205.12„ÄÇ</paragraph>

##### **Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification**
2407.11573v1 by Naif Alkhunaizi, Faris Almalik, Rouqaiah Al-Refai, Muzammal Naseer, Karthik Nandakumar

With the advent of large pre-trained transformer models, fine-tuning these
models for various downstream tasks is a critical problem. Paucity of training
data, the existence of data silos, and stringent privacy constraints exacerbate
this fine-tuning problem in the medical imaging domain, creating a strong need
for algorithms that enable collaborative fine-tuning of pre-trained models.
Moreover, the large size of these models necessitates the use of
parameter-efficient fine-tuning (PEFT) to reduce the communication burden in
federated learning. In this work, we systematically investigate various
federated PEFT strategies for adapting a Vision Transformer (ViT) model
(pre-trained on a large natural image dataset) for medical image
classification. Apart from evaluating known PEFT techniques, we introduce new
federated variants of PEFT algorithms such as visual prompt tuning (VPT),
low-rank decomposition of visual prompts, stochastic block attention
fine-tuning, and hybrid PEFT methods like low-rank adaptation (LoRA)+VPT.
Moreover, we perform a thorough empirical analysis to identify the optimal PEFT
method for the federated setting and understand the impact of data distribution
on federated PEFT, especially for out-of-domain (OOD) and non-IID data. The key
insight of this study is that while most federated PEFT methods work well for
in-domain transfer, there is a substantial accuracy vs. efficiency trade-off
when dealing with OOD and non-IID scenarios, which is commonly the case in
medical imaging. Specifically, every order of magnitude reduction in
fine-tuned/exchanged parameters can lead to a 4% drop in accuracy. Thus, the
initial model choice is crucial for federated PEFT. It is preferable to use
medical foundation models learned from in-domain medical image data (if
available) rather than general vision models.

ÊëòË¶ÅÔºö<paragraph>Èö®ËëóÂ§ßÂûãÈ†êË®ìÁ∑¥ËΩâÊèõÂô®Ê®°ÂûãÁöÑÂá∫ÁèæÔºåÈáùÂ∞çÂêÑÁ®Æ‰∏ãÊ∏∏‰ªªÂãôÂæÆË™øÈÄô‰∫õÊ®°ÂûãÊòØ‰∏ÄÂÄãÈóúÈçµÂïèÈ°å„ÄÇË®ìÁ∑¥Ë≥áÊñôÁöÑÁ®ÄÁº∫ÊÄß„ÄÅË≥áÊñôÂ≠§Â≥∂ÁöÑÂ≠òÂú®‰ª•ÂèäÂö¥Ê†ºÁöÑÈö±ÁßÅÈôêÂà∂ÊúÉÂä†ÂäáÈÜ´ÁôÇÂΩ±ÂÉèÈ†òÂüü‰∏≠ÁöÑÂæÆË™øÂïèÈ°åÔºåÈÄôÂ∞çËÉΩËÆìÈ†êË®ìÁ∑¥Ê®°ÂûãÈÄ≤Ë°åÂçî‰ΩúÂæÆË™øÁöÑÊºîÁÆóÊ≥ïÁî¢Áîü‰∫ÜÂº∑ÁÉàÈúÄÊ±Ç„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÊ®°ÂûãÁöÑÈæêÂ§ßË¶èÊ®°ÈúÄË¶Å‰ΩøÁî®ÂèÉÊï∏ÊúâÊïàÂæÆË™ø (PEFT) ‰æÜÈôç‰ΩéËÅØÂêàÂ≠∏Áøí‰∏≠ÁöÑÈÄöË®äË≤†Êìî„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Êé¢Ë®é‰∫ÜÂêÑÁ®ÆËÅØÂêà PEFT Á≠ñÁï•Ôºå‰ª•Ë™øÊï¥Ë¶ñË¶∫ËΩâÊèõÂô® (ViT) Ê®°ÂûãÔºàÂú®Â§ßÂûãËá™ÁÑ∂ÂΩ±ÂÉèË≥áÊñôÈõÜ‰∏äÈ†êÂÖàË®ìÁ∑¥Ôºâ‰ª•ÈÄ≤Ë°åÈÜ´ÁôÇÂΩ±ÂÉèÂàÜÈ°û„ÄÇÈô§‰∫ÜË©ï‰º∞Â∑≤Áü•ÁöÑ PEFT ÊäÄË°ìÂ§ñÔºåÊàëÂÄëÈÇÑÂºïÂÖ•‰∫Ü PEFT ÊºîÁÆóÊ≥ïÁöÑÊñ∞ËÅØÂêàËÆäÈ´îÔºå‰æãÂ¶ÇË¶ñË¶∫ÊèêÁ§∫Ë™øÊï¥ (VPT)„ÄÅË¶ñË¶∫ÊèêÁ§∫ÁöÑ‰ΩéÁß©ÂàÜËß£„ÄÅÈö®Ê©üÂçÄÂ°äÊ≥®ÊÑèÂäõÂæÆË™øÔºå‰ª•Âèä‰ΩéÁß©ÈÅ©Êáâ (LoRA)+VPT Á≠âÊ∑∑Âêà PEFT ÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄ≤Ë°å‰∫ÜÂæπÂ∫ïÁöÑÁ∂ìÈ©óÂàÜÊûêÔºå‰ª•ÊâæÂá∫ËÅØÂêàË®≠ÂÆöÁöÑÊúÄ‰Ω≥ PEFT ÊñπÊ≥ïÔºå‰∏¶‰∫ÜËß£Ë≥áÊñôÂàÜ‰ΩàÂ∞çËÅØÂêà PEFT ÁöÑÂΩ±ÈüøÔºåÁâπÂà•ÊòØÂ∞çÊñºÈ†òÂüüÂ§ñ (OOD) ÂíåÈùûÁç®Á´ãÂêåÂàÜ‰Ωà (non-IID) Ë≥áÊñô„ÄÇÈÄôÈ†ÖÁ†îÁ©∂ÁöÑ‰∏ªË¶ÅË¶ãËß£ÊòØÔºåÂÑòÁÆ°Â§ßÂ§öÊï∏ËÅØÂêà PEFT ÊñπÊ≥ïÈÉΩÈÅ©Áî®ÊñºÈ†òÂüüÂÖßËΩâÁßªÔºå‰ΩÜÂú®ËôïÁêÜ OOD ÂíåÈùûÁç®Á´ãÂêåÂàÜ‰ΩàÂ†¥ÊôØÊôÇÔºåÊúÉÊúâÂ§ßÂπÖÁöÑÊ∫ñÁ¢∫Â∫¶ËàáÊïàÁéáÊäòË°∑ÔºåÈÄôÈÄöÂ∏∏ÊòØÈÜ´ÁôÇÂΩ±ÂÉè‰∏≠ÁöÑÊÉÖÊ≥Å„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂæÆË™ø/‰∫§ÊèõÂèÉÊï∏ÁöÑÊØèÂÄãÊï∏ÈáèÁ¥öÊ∏õÂ∞ëÈÉΩÂèØËÉΩÂ∞éËá¥Ê∫ñÁ¢∫Â∫¶‰∏ãÈôç 4%„ÄÇÂõ†Ê≠§ÔºåÂàùÂßãÊ®°ÂûãÁöÑÈÅ∏ÊìáÂ∞çÊñºËÅØÂêà PEFT Ëá≥ÈóúÈáçË¶Å„ÄÇÊúÄÂ•Ω‰ΩøÁî®ÂæûÈ†òÂüüÂÖßÈÜ´Â≠∏ÂΩ±ÂÉèË≥áÊñôÔºàÂ¶ÇÊûúÊúâÁöÑË©±ÔºâÂ≠∏ÁøíÁöÑÈÜ´Â≠∏Âü∫Á§éÊ®°ÂûãÔºåËÄå‰∏çÊòØ‰∏ÄËà¨Ë¶ñË¶∫Ê®°Âûã„ÄÇ</paragraph>

##### **TGIF: Text-Guided Inpainting Forgery Dataset**
2407.11566v1 by Hannes Mareen, Dimitrios Karageorgiou, Glenn Van Wallendael, Peter Lambert, Symeon Papadopoulos

Digital image manipulation has become increasingly accessible and realistic
with the advent of generative AI technologies. Recent developments allow for
text-guided inpainting, making sophisticated image edits possible with minimal
effort. This poses new challenges for digital media forensics. For example,
diffusion model-based approaches could either splice the inpainted region into
the original image, or regenerate the entire image. In the latter case,
traditional image forgery localization (IFL) methods typically fail. This paper
introduces the Text-Guided Inpainting Forgery (TGIF) dataset, a comprehensive
collection of images designed to support the training and evaluation of image
forgery localization and synthetic image detection (SID) methods. The TGIF
dataset includes approximately 80k forged images, originating from popular
open-source and commercial methods; SD2, SDXL, and Adobe Firefly. Using this
data, we benchmark several state-of-the-art IFL and SID methods. Whereas
traditional IFL methods can detect spliced images, they fail to detect
regenerated inpainted images. Moreover, traditional SID may detect the
regenerated inpainted images to be fake, but cannot localize the inpainted
area. Finally, both types of methods fail when exposed to stronger compression,
while they are less robust to modern compression algorithms, such as WEBP. As
such, this work demonstrates the inefficiency of state-of-the-art detectors on
local manipulations performed by modern generative approaches, and aspires to
help with the development of more capable IFL and SID methods. The dataset can
be downloaded at https://github.com/IDLabMedia/tgif-dataset.

ÊëòË¶ÅÔºöÊï∏‰ΩçÂΩ±ÂÉèËôïÁêÜÈö®ËëóÁîüÊàêÂºè AI ÊäÄË°ìÁöÑÂá∫ÁèæÔºåËÆäÂæóË∂ä‰æÜË∂äÂÆπÊòìÂèñÂæó‰∏îÈÄºÁúü„ÄÇÊúÄËøëÁöÑÁôºÂ±ïÂÖÅË®±ÊñáÂ≠óÂºïÂ∞éÁöÑÂ°´Ë£úÔºåËÆìË§áÈõúÁöÑÂΩ±ÂÉèÁ∑®ËºØËÆäÂæóËºïËÄåÊòìËàâ„ÄÇÈÄôÁÇ∫Êï∏‰ΩçÂ™íÈ´îÈëëË≠òÂ∏∂‰æÜ‰∫ÜÊñ∞ÁöÑÊåëÊà∞„ÄÇ‰æãÂ¶ÇÔºåÂü∫ÊñºÊì¥Êï£Ê®°ÂûãÁöÑÊñπÊ≥ïÂèØ‰ª•Â∞áÂ°´Ë£úÂçÄÂüüÊãºÊé•Ëá≥ÂéüÂßãÂΩ±ÂÉèÔºåÊàñÈáçÊñ∞Áî¢ÁîüÊï¥ÂºµÂΩ±ÂÉè„ÄÇÂú®ÂæåËÄÖÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂÇ≥Áµ±ÁöÑÂΩ±ÂÉèÂÅΩÈÄ†ÂÆö‰Ωç (IFL) ÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÂ§±Êïó„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫ÜÊñáÂ≠óÂºïÂ∞éÂ°´Ë£úÂÅΩÈÄ† (TGIF) Ë≥áÊñôÈõÜÔºåÈÄôÊòØ‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÂΩ±ÂÉèÈõÜÂêàÔºåÊó®Âú®ÊîØÊè¥ÂΩ±ÂÉèÂÅΩÈÄ†ÂÆö‰ΩçÂíåÂêàÊàêÂΩ±ÂÉèÂÅµÊ∏¨ (SID) ÊñπÊ≥ïÁöÑË®ìÁ∑¥ÂíåË©ï‰º∞„ÄÇTGIF Ë≥áÊñôÈõÜÂåÖÂê´Á¥Ñ 80k ÂºµÂÅΩÈÄ†ÂΩ±ÂÉèÔºåÊ∫êËá™ÊñºÁÜ±ÈñÄÁöÑÈñãÊ∫êÂíåÂïÜÊ•≠ÊñπÊ≥ïÔºõSD2„ÄÅSDXL Âíå Adobe Firefly„ÄÇ‰ΩøÁî®ÈÄô‰∫õË≥áÊñôÔºåÊàëÂÄëË©ïÂÆö‰∫ÜÊï∏Á®ÆÊúÄÂÖàÈÄ≤ÁöÑ IFL Âíå SID ÊñπÊ≥ï„ÄÇÈõñÁÑ∂ÂÇ≥Áµ±ÁöÑ IFL ÊñπÊ≥ïÂèØ‰ª•ÂÅµÊ∏¨ÊãºÊé•ÂΩ±ÂÉèÔºå‰ΩÜÁÑ°Ê≥ïÂÅµÊ∏¨ÈáçÊñ∞Áî¢ÁîüÁöÑÂ°´Ë£úÂΩ±ÂÉè„ÄÇÊ≠§Â§ñÔºåÂÇ≥Áµ±ÁöÑ SID ÈõñÁÑ∂ÂèØ‰ª•ÂÅµÊ∏¨ÈáçÊñ∞Áî¢ÁîüÁöÑÂ°´Ë£úÂΩ±ÂÉèÁÇ∫ÂÅáÔºå‰ΩÜÁÑ°Ê≥ïÂÆö‰ΩçÂ°´Ë£úÂçÄÂüü„ÄÇÊúÄÂæåÔºåÈÄôÂÖ©Á®ÆÊñπÊ≥ïÂú®ÈÅáÂà∞ËºÉÂº∑ÁöÑÂ£ìÁ∏ÆÊôÇÈÉΩÊúÉÂ§±ÊïóÔºåËÄåÂÆÉÂÄëÂ∞çÁèæ‰ª£Â£ìÁ∏ÆÊºîÁÆóÊ≥ïÔºà‰æãÂ¶Ç WEBPÔºâÁöÑÈ≠ØÊ£íÊÄßËºÉÂ∑Æ„ÄÇÂõ†Ê≠§ÔºåÈÄôÈ†ÖÁ†îÁ©∂Ë≠âÊòé‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÂÅµÊ∏¨Âô®Â∞çÊñºÁèæ‰ª£ÁîüÊàêÊñπÊ≥ïÂü∑Ë°åÁöÑÂ±ÄÈÉ®ËôïÁêÜÁöÑÊïàÁéá‰Ωé‰∏ãÔºå‰∏¶Â∏åÊúõÊúâÂä©ÊñºÈñãÁôºÊõ¥Âº∑Â§ßÁöÑ IFL Âíå SID ÊñπÊ≥ï„ÄÇË©≤Ë≥áÊñôÈõÜÂèØÊñº https://github.com/IDLabMedia/tgif-dataset ‰∏ãËºâ„ÄÇ

##### **Self-Guided Generation of Minority Samples Using Diffusion Models**
2407.11555v1 by Soobin Um, Jong Chul Ye

We present a novel approach for generating minority samples that live on
low-density regions of a data manifold. Our framework is built upon diffusion
models, leveraging the principle of guided sampling that incorporates an
arbitrary energy-based guidance during inference time. The key defining feature
of our sampler lies in its \emph{self-contained} nature, \ie, implementable
solely with a pretrained model. This distinguishes our sampler from existing
techniques that require expensive additional components (like external
classifiers) for minority generation. Specifically, we first estimate the
likelihood of features within an intermediate latent sample by evaluating a
reconstruction loss w.r.t. its posterior mean. The generation then proceeds
with the minimization of the estimated likelihood, thereby encouraging the
emergence of minority features in the latent samples of subsequent timesteps.
To further improve the performance of our sampler, we provide several
time-scheduling techniques that properly manage the influence of guidance over
inference steps. Experiments on benchmark real datasets demonstrate that our
approach can greatly improve the capability of creating realistic
low-likelihood minority instances over the existing techniques without the
reliance on costly additional elements. Code is available at
\url{https://github.com/soobin-um/sg-minority}.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊñπÊ≥ï‰æÜÁî¢ÁîüÂ∞ëÊï∏Ê®£Êú¨ÔºåÈÄô‰∫õÊ®£Êú¨Â≠òÂú®ÊñºË≥áÊñôÊµÅÂΩ¢‰∏≠ÁöÑ‰ΩéÂØÜÂ∫¶ÂçÄÂüü„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂª∫Á´ãÂú®Êì¥Êï£Ê®°Âûã‰∏äÔºåÂà©Áî®‰∫ÜÂºïÂ∞éÂèñÊ®£ÁöÑÂéüÁêÜÔºåË©≤ÂéüÁêÜÂú®Êé®Ë´ñÊôÇÈñìÂÖßÂåÖÂê´‰∫ÜÂü∫Êñº‰ªªÊÑèËÉΩÈáèÁöÑÂºïÂ∞é„ÄÇÊàëÂÄëÁöÑÂèñÊ®£Âô®ÁöÑÈóúÈçµÂÆöÁæ©ÁâπÂæµÂú®ÊñºÂÖ∂„ÄåËá™ÊàëÂ∞ÅÈñâ„ÄçÁöÑÊú¨Ë≥™Ôºå‰πüÂ∞±ÊòØË™™ÔºåÂÉÖ‰ΩøÁî®È†êË®ìÁ∑¥Ê®°ÂûãÂ∞±ËÉΩÂØ¶Áèæ„ÄÇÈÄô‰ΩøÂæóÊàëÂÄëÁöÑÂèñÊ®£Âô®ÊúâÂà•ÊñºÁèæÊúâÁöÑÊäÄË°ìÔºåÂæåËÄÖÈúÄË¶ÅÊòÇË≤¥ÁöÑÈ°çÂ§ñÁµÑ‰ª∂ÔºàÂ¶ÇÂ§ñÈÉ®ÂàÜÈ°ûÂô®Ôºâ‰æÜÁî¢ÁîüÂ∞ëÊï∏„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÈÄèÈÅéË©ï‰º∞ÈáçÂª∫ÊêçÂ§±Áõ∏Â∞çÊñºÂÖ∂ÂæåÈ©óÂπ≥ÂùáÂÄº‰æÜ‰º∞Ë®à‰∏≠ÈñìÊΩõÂú®Ê®£Êú¨‰∏≠ÁâπÂæµÁöÑÂèØËÉΩÊÄß„ÄÇÁÑ∂ÂæåÔºåÁîüÊàêÊúÉÈö®Ëëó‰º∞Ë®àÂèØËÉΩÊÄßÁöÑÊúÄÂ∞èÂåñËÄåÈÄ≤Ë°åÔºåÂæûËÄå‰øÉÈÄ≤Â∞ëÊï∏ÁâπÂæµÂú®ÂæåÁ∫åÊôÇÈñìÊ≠•Èï∑ÁöÑÊΩõÂú®Ê®£Êú¨‰∏≠Âá∫Áèæ„ÄÇÁÇ∫‰∫ÜÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÊàëÂÄëÂèñÊ®£Âô®ÁöÑÊïàËÉΩÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÂπæÁ®ÆÊôÇÈñìÊéíÁ®ãÊäÄË°ìÔºåÈÄô‰∫õÊäÄË°ìÂèØ‰ª•ÈÅ©Áï∂Âú∞ÁÆ°ÁêÜÂºïÂ∞éÂ∞çÊé®Ë´ñÊ≠•È©üÁöÑÂΩ±Èüø„ÄÇÂü∫Ê∫ñÁúüÂØ¶Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË°®ÊòéÔºåËàáÁèæÊúâÊäÄË°ìÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂèØ‰ª•Â§ßÂπÖÊèêÈ´òÂª∫Á´ãÈÄºÁúüÁöÑ‰ΩéÂèØËÉΩÊÄßÂ∞ëÊï∏ÂÄãÈ´îÁöÑËÉΩÂäõÔºåËÄåÁÑ°ÈúÄ‰æùË≥¥ÊòÇË≤¥ÁöÑÈ°çÂ§ñÂÖÉÁ¥†„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/soobin-um/sg-minority ÂèñÂæó„ÄÇ

##### **Learning Global and Local Features of Power Load Series Through Transformer and 2D-CNN: An image-based Multi-step Forecasting Approach Incorporating Phase Space Reconstruction**
2407.11553v1 by Zihan Tang, Tianyao Ji, Wenhu Tang

As modern power systems continue to evolve, accurate power load forecasting
remains a critical issue. The phase space reconstruction method can effectively
retain the chaotic characteristics of power load from a system dynamics
perspective and thus is a promising knowledge-based preprocessing method for
power load forecasting. However, limited by its fundamental theory, there is
still a gap in implementing a multi-step forecasting scheme in current studies.
To bridge this gap, this study proposes a novel multi-step forecasting approach
by integrating the PSR with neural networks. Firstly, the useful features in
the phase trajectory obtained from the preprocessing of PSR are discussed in
detail. Through mathematical derivation, the equivalent characterization of the
PSR and another time series preprocessing method, patch segmentation, is
demonstrated for the first time. Based on this prior knowledge, an image-based
modeling perspective with the global and local feature extraction strategy is
introduced. Subsequently, a novel deep learning model, namely PSR-GALIEN, is
designed for end-to-end processing, in which the Transformer Encoder and
2D-convolutional neural networks are employed for the extraction of the global
and local patterns in the image, and a multi-layer perception based predictor
is used for the efficient correlation modeling. Then, extensive experiments are
conducted on five real-world benchmark datasets to verify the effectiveness as
well as to have an insight into the detailed properties. The results show that,
comparing it with six state-of-the-art deep learning models, the forecasting
performance of PSR-GALIEN consistently surpasses these baselines, which
achieves superior accuracy in both intra-day and day-ahead forecasting
scenarios. At the same time, a visualization-based method is proposed to
explain the attributions of the forecasting results.

ÊëòË¶ÅÔºöÈö®ËëóÁèæ‰ª£ÈõªÂäõÁ≥ªÁµ±ÊåÅÁ∫åÊºîÈÄ≤ÔºåÊ∫ñÁ¢∫ÁöÑÈõªÂäõË≤†ËºâÈ†êÊ∏¨‰ªçÁÇ∫‰∏ÄÈ†ÖÈóúÈçµË≠∞È°å„ÄÇÁõ∏Á©∫ÈñìÈáçÂª∫Ê≥ïËÉΩÊúâÊïà‰øùÁïôÈõªÂäõË≤†ËºâËá™Á≥ªÁµ±ÂãïÂäõÂ≠∏ËßÄÈªûÁöÑÊ∑∑Ê≤åÁâπÊÄßÔºåÂõ†Ê≠§ÊàêÁÇ∫ÈõªÂäõË≤†ËºâÈ†êÊ∏¨‰∏≠Ê•µÂÖ∑ÂâçÊôØÁöÑÁü•Ë≠òÂûãÂâçËôïÁêÜÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂèóÈôêÊñºÂÖ∂Âü∫Á§éÁêÜË´ñÔºåÁèæË°åÁ†îÁ©∂‰ªçÂ≠òÂú®ÁÑ°Ê≥ïÂØ¶‰ΩúÂ§öÊ≠•È©üÈ†êÊ∏¨Ê©üÂà∂ÁöÑÁº∫Âè£„ÄÇÁÇ∫ÂΩåË£úÊ≠§Áº∫Âè£ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫ÁµêÂêàÁõ∏Á©∫ÈñìÈáçÂª∫Ê≥ïËàáÁ•ûÁ∂ìÁ∂≤Ë∑ØÁöÑÂâµÊñ∞Â§öÊ≠•È©üÈ†êÊ∏¨ÊñπÊ≥ï„ÄÇÈ¶ñÂÖàÔºåË©≥Á¥∞Êé¢Ë®éËá™Áõ∏Á©∫ÈñìÈáçÂª∫Ê≥ïÂâçËôïÁêÜÊâÄÂæóÁõ∏ËªåË∑°‰∏≠ÁöÑÊúâÁî®ÁâπÂæµ„ÄÇÈÄèÈÅéÊï∏Â≠∏Êé®Â∞éÔºåÈ¶ñÊ¨°Ë≠âÊòéÁõ∏Á©∫ÈñìÈáçÂª∫Ê≥ïËàáÂè¶‰∏ÄÊôÇÂ∫èÂâçËôïÁêÜÊñπÊ≥ï‚Äî‚ÄîÂçÄÂ°äÂàÜÂâ≤‚Äî‚ÄîÁöÑÁ≠âÂÉπÁâπÊÄß„ÄÇÂü∫ÊñºÊ≠§ÂÖàÂÇôÁü•Ë≠òÔºåÂºïÂÖ•ÂÖ∑ÂÇôÂÖ®ÂüüËàáÂ±ÄÈÉ®ÁâπÂæµËêÉÂèñÁ≠ñÁï•ÁöÑÂΩ±ÂÉèÂåñÂª∫Ê®°ËßÄÈªû„ÄÇÈö®ÂæåÔºåË®≠Ë®àÂâµÊñ∞ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°Âûã PSR-GALIEN ‰ª•ÈÄ≤Ë°åÁ´ØÂ∞çÁ´ØËôïÁêÜÔºåÂÖ∂‰∏≠Êé°Áî® Transformer Á∑®Á¢ºÂô®Ëàá 2D Êç≤Á©çÁ•ûÁ∂ìÁ∂≤Ë∑ØËêÉÂèñÂΩ±ÂÉè‰∏≠ÁöÑÂÖ®ÂüüËàáÂ±ÄÈÉ®Ê®°ÂºèÔºå‰∏¶‰ΩøÁî®Âü∫ÊñºÂ§öÂ±§ÊÑüÁü•Âô®ÁöÑÈ†êÊ∏¨Âô®ÈÄ≤Ë°åÊúâÊïàÁéáÁöÑÈóúËÅØÊÄßÂª∫Ê®°„ÄÇÊé•ËëóÔºåÂú®‰∫îÂÄãÁúüÂØ¶‰∏ñÁïåÂü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰ª•È©óË≠âÂÖ∂ÊúâÊïàÊÄß‰∏¶Ê∑±ÂÖ•‰∫ÜËß£ÂÖ∂Ë©≥Á¥∞ÁâπÊÄß„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåËàáÂÖ≠Á®ÆÊúÄÂÖàÈÄ≤ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÊ®°ÂûãÁõ∏ÊØîËºÉÔºåPSR-GALIEN ÁöÑÈ†êÊ∏¨ÊïàËÉΩÂßãÁµÇË∂ÖË∂äÈÄô‰∫õÂü∫Ê∫ñÔºåÂú®Êó•ÂÖßËàáÊó•ÂæåÈ†êÊ∏¨ÊÉÖÂ¢É‰∏≠ÂùáÈÅîÂà∞ÂÑ™Áï∞ÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂêåÊôÇÔºåÊèêÂá∫Âü∫ÊñºË¶ñË¶∫ÂåñÁöÑÊñπÊ≥ï‰æÜËß£ÈáãÈ†êÊ∏¨ÁµêÊûúÁöÑÊ≠∏Âõ†„ÄÇ

##### **Optimizing KV Cache Eviction in LLMs: Adaptive Allocation for Enhanced Budget Utilization**
2407.11550v1 by Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

Large Language Models have excelled in various fields but encounter
efficiency limitations due to the extensive KV cache required for long
sequences inference. Many efforts try to evict non-critical cache elements
during runtime, thereby reducing cache size within a given memory budget while
preserving generation quality. Our reexamination of their underlying principles
discerns that prevailing strategies essentially aim to minimize an upper bound
of eviction loss within a specific budget allocation. However, we observe that
the current practice of uniformly allocating budgets across different attention
heads during the eviction procedure tends to degrade the quality of generation
posten-eviction. In light of these findings, we propose a simple yet effective
adaptive allocation algorithm that not only theoretically ensures its loss
upper bound does not exceed that of previous uniform allocation methods, but
also effectively aligns with the characteristics of the self-attention
mechanism, thus practically reducing the upper bound. Further, integrating this
algorithm with two of the most advanced methods yields Ada-SnapKV and
Ada-Pyramid. Extensive experimental validation across 16 datasets and the
Needle-in-a-Haystack test confirm that Ada-SnapKV and Ada-Pyramid achieve
further enhancements, establishing new benchmarks in state-of-the-art
performance.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÂú®ÂêÑÁ®ÆÈ†òÂüüË°®ÁèæÂá∫Ëâ≤Ôºå‰ΩÜÁî±ÊñºÈï∑Â∫èÂàóÊé®Ë´ñÈúÄË¶ÅÂ§ßÈáèÁöÑ KV Âø´ÂèñÔºåÂõ†Ê≠§ÊúÉÈÅáÂà∞ÊïàÁéáÈôêÂà∂„ÄÇË®±Â§öÊñπÊ≥ïÂòóË©¶Âú®Âü∑Ë°åÊúüÈñìÈ©ÖÈÄêÈùûÈóúÈçµÂø´ÂèñÂÖÉÁ¥†ÔºåÂæûËÄåÂú®Áµ¶ÂÆöÁöÑË®òÊÜ∂È´îÈ†êÁÆó‰∏≠Ê∏õÂ∞ëÂø´ÂèñÂ§ßÂ∞èÔºåÂêåÊôÇ‰øùÊåÅÁîüÊàêÂìÅË≥™„ÄÇÊàëÂÄëÈáçÊñ∞ÂØ©Ë¶ñÂÖ∂Âü∫Êú¨ÂéüÂâáÔºåÁôºÁèæÊôÆÈÅçÁöÑÁ≠ñÁï•Âü∫Êú¨‰∏äÊó®Âú®ÊúÄÂ∞èÂåñÁâπÂÆöÈ†êÁÆóÂàÜÈÖç‰∏≠ÁöÑÈ©ÖÈÄêÊêçÂ§±ÁöÑ‰∏äÈôê„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëËßÄÂØüÂà∞Âú®È©ÖÈÄêÈÅéÁ®ã‰∏≠Â∞áÈ†êÁÆóÂùáÂãªÂàÜÈÖçÁµ¶‰∏çÂêåÊ≥®ÊÑèÂäõÂ±§ÁöÑÁèæË°åÂÅöÊ≥ïÂæÄÂæÄÊúÉÈôç‰ΩéÈ©ÖÈÄêÂæåÁöÑÁîüÊàêÂìÅË≥™„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑËá™ÈÅ©ÊáâÂàÜÈÖçÊºîÁÆóÊ≥ïÔºåÂÆÉ‰∏çÂÉÖÂú®ÁêÜË´ñ‰∏äÁ¢∫‰øùÂÖ∂ÊêçÂ§±‰∏äÈôê‰∏çË∂ÖÈÅéÂÖàÂâçÁöÑÂùáÂãªÂàÜÈÖçÊñπÊ≥ïÔºåËÄå‰∏îÊúâÊïàÂú∞Á¨¶ÂêàËá™Ê≥®ÊÑèÂäõÊ©üÂà∂ÁöÑÁâπÊÄßÔºåÂæûËÄåÂØ¶Èöõ‰∏äÈôç‰Ωé‰∫Ü‰∏äÈôê„ÄÇÊ≠§Â§ñÔºåÂ∞áÊ≠§ÊºîÁÆóÊ≥ïËàáÂÖ©Á®ÆÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÊï¥ÂêàÔºåÁî¢Áîü Ada-SnapKV Âíå Ada-Pyramid„ÄÇÂú® 16 ÂÄãË≥áÊñôÈõÜÂíå Needle-in-a-Haystack Ê∏¨Ë©¶‰∏≠ÁöÑÂª£Ê≥õÂØ¶È©óÈ©óË≠âË≠âÂØ¶ÔºåAda-SnapKV Âíå Ada-Pyramid ÈÄ≤‰∏ÄÊ≠•ÊèêÂçáÔºåÂú®ÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ‰∏≠Âª∫Á´ãÊñ∞ÁöÑÂü∫Ê∫ñ„ÄÇ

##### **How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models**
2407.11549v1 by Yin Jou Huang, Rafik Hadfi

Psychological evidence reveals the influence of personality traits on
decision-making. For instance, agreeableness is generally associated with
positive outcomes in negotiations, whereas neuroticism is often linked to less
favorable outcomes. This paper introduces a simulation framework centered on
Large Language Model (LLM) agents endowed with synthesized personality traits.
The agents negotiate within bargaining domains and possess customizable
personalities and objectives. The experimental results show that the behavioral
tendencies of LLM-based simulations could reproduce behavioral patterns
observed in human negotiations. The contribution is twofold. First, we propose
a simulation methodology that investigates the alignment between the linguistic
and economic capabilities of LLM agents. Secondly, we offer empirical insights
into the strategic impact of Big-Five personality traits on the outcomes of
bilateral negotiations. We also provide a case study based on synthesized
bargaining dialogues to reveal intriguing behaviors, including deceitful and
compromising behaviors.

ÊëòË¶ÅÔºöÂøÉÁêÜÂ≠∏Ë≠âÊìöÊè≠Á§∫‰∫Ü‰∫∫Ê†ºÁâπË≥™Â∞çÊ±∫Á≠ñÁöÑÂΩ±Èüø„ÄÇ‰æãÂ¶ÇÔºåÂÆú‰∫∫ÊÄßÈÄöÂ∏∏ËàáË´áÂà§‰∏≠ÁöÑÁ©çÊ•µÁµêÊûúÊúâÈóúÔºåËÄåÁ•ûÁ∂ìË≥™ÂâáÁ∂ìÂ∏∏ËàáËºÉ‰∏çÁêÜÊÉ≥ÁöÑÁµêÊûúÊúâÈóú„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÊ®°Êì¨Êû∂ÊßãÔºåË©≤Êû∂Êßã‰ª•ÂÖ∑ÊúâÁ∂úÂêà‰∫∫Ê†ºÁâπË≥™ÁöÑÂ§ßË™ûË®ÄÊ®°Âûã (LLM) ‰ª£ÁêÜÁÇ∫‰∏≠ÂøÉ„ÄÇ‰ª£ÁêÜÂú®Ë≠∞ÂÉπÈ†òÂüüÂÖßÈÄ≤Ë°åË´áÂà§Ôºå‰∏¶ÊìÅÊúâÂèØËá™Ë®ÇÁöÑ‰∫∫Ê†ºÂíåÁõÆÊ®ô„ÄÇÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÂü∫Êñº LLM ÁöÑÊ®°Êì¨ÁöÑË°åÁÇ∫ÂÇæÂêëÂèØ‰ª•ÈáçÁèæ‰∫∫È°ûË´áÂà§‰∏≠ËßÄÂØüÂà∞ÁöÑË°åÁÇ∫Ê®°Âºè„ÄÇË≤¢ÁçªÊúâÂÖ©ÂÄãÊñπÈù¢„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊ®°Êì¨ÊñπÊ≥ïÔºåË©≤ÊñπÊ≥ïÁ†îÁ©∂‰∫Ü LLM ‰ª£ÁêÜÁöÑË™ûË®ÄÂíåÁ∂ìÊøüËÉΩÂäõ‰πãÈñìÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÂÖ∂Ê¨°ÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÈóúÊñºÂ§ß‰∫î‰∫∫Ê†ºÁâπË≥™Â∞çÈõôÈÇäË´áÂà§ÁµêÊûúÁöÑÁ≠ñÁï•ÊÄßÂΩ±ÈüøÁöÑÂØ¶Ë≠âË¶ãËß£„ÄÇÊàëÂÄëÈÇÑÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÁ∂úÂêàË≠∞ÂÉπÂ∞çË©±ÁöÑÊ°à‰æãÁ†îÁ©∂Ôºå‰ª•Êè≠Á§∫ÊúâË∂£ÁöÑË°åÁÇ∫ÔºåÂåÖÊã¨Ê¨∫È®ôÂíåÂ¶•ÂçîË°åÁÇ∫„ÄÇ

##### **AEMIM: Adversarial Examples Meet Masked Image Modeling**
2407.11537v1 by Wenzhao Xiang, Chang Liu, Hang Su, Hongyang Yu

Masked image modeling (MIM) has gained significant traction for its
remarkable prowess in representation learning. As an alternative to the
traditional approach, the reconstruction from corrupted images has recently
emerged as a promising pretext task. However, the regular corrupted images are
generated using generic generators, often lacking relevance to the specific
reconstruction task involved in pre-training. Hence, reconstruction from
regular corrupted images cannot ensure the difficulty of the pretext task,
potentially leading to a performance decline. Moreover, generating corrupted
images might introduce an extra generator, resulting in a notable computational
burden. To address these issues, we propose to incorporate adversarial examples
into masked image modeling, as the new reconstruction targets. Adversarial
examples, generated online using only the trained models, can directly aim to
disrupt tasks associated with pre-training. Therefore, the incorporation not
only elevates the level of challenge in reconstruction but also enhances
efficiency, contributing to the acquisition of superior representations by the
model. In particular, we introduce a novel auxiliary pretext task that
reconstructs the adversarial examples corresponding to the original images. We
also devise an innovative adversarial attack to craft more suitable adversarial
examples for MIM pre-training. It is noted that our method is not restricted to
specific model architectures and MIM strategies, rendering it an adaptable
plug-in capable of enhancing all MIM methods. Experimental findings
substantiate the remarkable capability of our approach in amplifying the
generalization and robustness of existing MIM methods. Notably, our method
surpasses the performance of baselines on various tasks, including ImageNet,
its variants, and other downstream tasks.

ÊëòË¶ÅÔºö<paragraph>ÈÅÆÁΩ©ÂΩ±ÂÉèÊ®°Âûã (MIM) Âõ†ÂÖ∂Âú®Ë°®ÂæµÂ≠∏Áøí‰∏äÁöÑÈ°ØËëóËÉΩÂäõËÄåÁç≤ÂæóÊ•µÂ§ßÁöÑÈóúÊ≥®„ÄÇ‰ΩúÁÇ∫ÂÇ≥Áµ±ÊñπÊ≥ïÁöÑÊõø‰ª£ÊñπÊ°àÔºåÂæûÊêçÂ£ûÂΩ±ÂÉè‰∏≠ÈáçÂª∫ÊúÄËøëÂ∑≤ÊàêÁÇ∫‰∏ÄÈ†ÖÊúâÂâçÊôØÁöÑÈ†êË®≠‰ªªÂãô„ÄÇÁÑ∂ËÄåÔºå‰∏ÄËà¨ÁöÑÊêçÂ£ûÂΩ±ÂÉè‰ΩøÁî®ÈÄöÁî®Áî¢ÁîüÂô®Áî¢ÁîüÔºåÈÄöÂ∏∏Áº∫‰πèËàáÈ†êË®ìÁ∑¥‰∏≠Ê∂âÂèäÁöÑÁâπÂÆöÈáçÂª∫‰ªªÂãôÁõ∏ÈóúÊÄß„ÄÇÂõ†Ê≠§ÔºåÂæû‰∏ÄËà¨ÊêçÂ£ûÂΩ±ÂÉèÈÄ≤Ë°åÈáçÂª∫ÁÑ°Ê≥ïÁ¢∫‰øùÈ†êË®≠‰ªªÂãôÁöÑÈõ£Â∫¶ÔºåÂèØËÉΩÊúÉÂ∞éËá¥ÊïàËÉΩ‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÁî¢ÁîüÊêçÂ£ûÂΩ±ÂÉèÂèØËÉΩÊúÉÂºïÂÖ•È°çÂ§ñÁöÑÁî¢ÁîüÂô®ÔºåÂ∞éËá¥È°ØËëóÁöÑÈÅãÁÆóË≤†Êìî„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÂª∫Ë≠∞Â∞áÂ∞çÊäóÁØÑ‰æãÁ¥çÂÖ•ÈÅÆÁΩ©ÂΩ±ÂÉèÊ®°ÂûãÔºå‰ΩúÁÇ∫Êñ∞ÁöÑÈáçÂª∫ÁõÆÊ®ô„ÄÇÂ∞çÊäóÁØÑ‰æãÂÉÖ‰ΩøÁî®Ë®ìÁ∑¥ÈÅéÁöÑÊ®°ÂûãÁ∑ö‰∏äÁî¢ÁîüÔºåÂèØ‰ª•Áõ¥Êé•ÈáùÂ∞çËàáÈ†êË®ìÁ∑¥Áõ∏ÈóúÁöÑ‰ªªÂãôÈÄ≤Ë°åÁ†¥Â£û„ÄÇÂõ†Ê≠§ÔºåÈÄôÁ®ÆÁ¥çÂÖ•‰∏çÂÉÖÊèêÂçá‰∫ÜÈáçÂª∫ÁöÑÊåëÊà∞Èõ£Â∫¶ÔºåÈÇÑÊèêÈ´ò‰∫ÜÊïàÁéáÔºåÊúâÂä©ÊñºÊ®°ÂûãÁç≤ÂæóÊõ¥ÂÑ™Ë∂äÁöÑË°®Âæµ„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏ÄÈ†ÖÊñ∞Á©éÁöÑËºîÂä©È†êË®≠‰ªªÂãôÔºåÁî®‰æÜÈáçÂª∫Â∞çÊáâÊñºÂéüÂßãÂΩ±ÂÉèÁöÑÂ∞çÊäóÁØÑ‰æã„ÄÇÊàëÂÄëÈÇÑË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÂ∞çÊäóÊîªÊìäÔºåÁî®ÊñºÁÇ∫ MIM È†êË®ìÁ∑¥Ë£Ω‰ΩúÊõ¥ÂêàÈÅ©ÁöÑÂ∞çÊäóÁØÑ‰æã„ÄÇË´ãÊ≥®ÊÑèÔºåÊàëÂÄëÁöÑÊñπÊ≥ï‰∏çÂèóÁâπÂÆöÊ®°ÂûãÊû∂ÊßãÂíå MIM Á≠ñÁï•ÁöÑÈôêÂà∂Ôºå‰ΩøÂÖ∂ÊàêÁÇ∫‰∏ÄÁ®ÆÈÅ©ÊáâÊÄßÂ§ñÊéõÁ®ãÂºèÔºåËÉΩÂ§†Â¢ûÂº∑ÊâÄÊúâ MIM ÊñπÊ≥ï„ÄÇÂØ¶È©óÁµêÊûúË≠âÂØ¶‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Êì¥Â§ßÁèæÊúâ MIM ÊñπÊ≥ïÁöÑÊ≥õÂåñËÉΩÂäõÂíåÁ©©ÂÅ•ÊÄßÊñπÈù¢ÁöÑÈ°ØËëóËÉΩÂäõ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ÂêÑÁ®Æ‰ªªÂãô‰∏äÁöÑÊïàËÉΩÈÉΩË∂ÖË∂ä‰∫ÜÂü∫Á∑öÔºåÂåÖÊã¨ ImageNet„ÄÅÂÖ∂ËÆäÈ´îÂíåÂÖ∂‰ªñ‰∏ãÊ∏∏‰ªªÂãô„ÄÇ</paragraph>

##### **Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise**
2407.11536v1 by Qimin Yang, Rongsheng Wang, Jiexin Chen, Runqi Su, Tao Tan

Large Language Models (LLMs) have been widely applied in various professional
fields. By fine-tuning the models using domain specific question and answer
datasets, the professional domain knowledge and Q\&A abilities of these models
have significantly improved, for example, medical professional LLMs that use
fine-tuning of doctor-patient Q\&A data exhibit extraordinary disease
diagnostic abilities. However, we observed that despite improvements in
specific domain knowledge, the performance of medical LLM in long-context
understanding has significantly declined, especially compared to general
language models with similar parameters. The purpose of this study is to
investigate the phenomenon of reduced performance in understanding long-context
in medical LLM. We designed a series of experiments to conduct open-book
professional knowledge exams on all models to evaluate their ability to read
long-context. By adjusting the proportion and quantity of general data and
medical data in the process of fine-tuning, we can determine the best data
composition to optimize the professional model and achieve a balance between
long-context performance and specific domain knowledge.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âª£Ê≥õÊáâÁî®ÊñºÂêÑÁ®ÆÂ∞àÊ•≠È†òÂüü„ÄÇÈÄöÈÅé‰ΩøÁî®ÁâπÂÆöÈ†òÂüüÁöÑÂïèÁ≠îË≥áÊñôÈõÜÂæÆË™øÊ®°ÂûãÔºåÈÄô‰∫õÊ®°ÂûãÁöÑÂ∞àÊ•≠È†òÂüüÁü•Ë≠òÂíåÂïèÁ≠îËÉΩÂäõÂ∑≤È°ØËëóÊèêÂçáÔºå‰æãÂ¶ÇÔºå‰ΩøÁî®ÈÜ´Áîü-ÊÇ£ËÄÖÂïèÁ≠îË≥áÊñôÈÄ≤Ë°åÂæÆË™øÁöÑÈÜ´ÁôÇÂ∞àÊ•≠ LLM Â±ïÁèæÂá∫ÈùûÂá°ÁöÑÁñæÁóÖË®∫Êñ∑ËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëËßÄÂØüÂà∞ÔºåÂÑòÁÆ°ÁâπÂÆöÈ†òÂüüÁü•Ë≠òÊúâÊâÄÊèêÂçáÔºå‰ΩÜÈÜ´ÁôÇ LLM Âú®Èï∑Ë™ûÂ¢ÉÁêÜËß£ÊñπÈù¢ÁöÑË°®ÁèæÂçªÂ§ßÂπÖ‰∏ãÈôçÔºåÂ∞§ÂÖ∂ÊòØËàáÂÖ∑ÊúâÈ°û‰ººÂèÉÊï∏ÁöÑ‰∏ÄËà¨Ë™ûË®ÄÊ®°ÂûãÁõ∏ÊØî„ÄÇÊú¨Á†îÁ©∂ÁöÑÁõÆÁöÑÊòØÊé¢Ë®éÈÜ´ÁôÇ LLM Âú®ÁêÜËß£Èï∑Ë™ûÂ¢ÉÊñπÈù¢ÁöÑË°®Áèæ‰∏ãÈôçÁèæË±°„ÄÇÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ≥ªÂàóÂØ¶È©óÔºåÂ∞çÊâÄÊúâÊ®°ÂûãÈÄ≤Ë°åÈñãÊîæÂºèÂ∞àÊ•≠Áü•Ë≠òËÄÉË©¶Ôºå‰ª•Ë©ï‰º∞ÂÆÉÂÄëÈñ±ËÆÄÈï∑Ë™ûÂ¢ÉÁöÑÁêÜËß£ËÉΩÂäõ„ÄÇÈÄöÈÅéË™øÊï¥ÂæÆË™øÈÅéÁ®ã‰∏≠‰∏ÄËà¨Ë≥áÊñôÂíåÈÜ´ÁôÇË≥áÊñôÁöÑÊØî‰æãÂíåÊï∏ÈáèÔºåÊàëÂÄëÂèØ‰ª•Á¢∫ÂÆöÊúÄ‰Ω≥Ë≥áÊñôÁµÑÂêàÔºå‰ª•ÂÑ™ÂåñÂ∞àÊ•≠Ê®°ÂûãÔºå‰∏¶Âú®Èï∑Ë™ûÂ¢ÉË°®ÁèæÂíåÁâπÂÆöÈ†òÂüüÁü•Ë≠ò‰πãÈñìÂèñÂæóÂπ≥Ë°°„ÄÇ

##### **LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices**
2407.11534v1 by Jung Hyun Lee, Jeonghoon Kim, June Yong Yang, Se Jung Kwon, Eunho Yang, Kang Min Yoo, Dongsoo Lee

With the commercialization of large language models (LLMs), weight-activation
quantization has emerged to compress and accelerate LLMs, achieving high
throughput while reducing inference costs. However, existing post-training
quantization (PTQ) techniques for quantizing weights and activations of LLMs
still suffer from non-negligible accuracy drops, especially on massive
multitask language understanding. To address this issue, we propose Low-Rank
Quantization (LRQ) $-$ a simple yet effective post-training weight quantization
method for LLMs that reconstructs the outputs of an intermediate Transformer
block by leveraging low-rank weight-scaling matrices, replacing the
conventional full weight-scaling matrices that entail as many learnable scales
as their associated weights. Thanks to parameter sharing via low-rank
structure, LRQ only needs to learn significantly fewer parameters while
enabling the individual scaling of weights, thus boosting the generalization
capability of quantized LLMs. We show the superiority of LRQ over prior LLM PTQ
works under (i) $8$-bit weight and per-tensor activation quantization, (ii)
$4$-bit weight and $8$-bit per-token activation quantization, and (iii) low-bit
weight-only quantization schemes. Our code is available at
\url{https://github.com/onliwad101/FlexRound_LRQ} to inspire LLM researchers
and engineers.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÂïÜÊ•≠ÂåñÔºåÊ¨äÈáçÊ¥ªÂåñÈáèÂåñÊáâÈÅãËÄåÁîüÔºåÁî®ÊñºÂ£ìÁ∏ÆÂíåÂä†ÈÄü LLMÔºåÂú®Èôç‰ΩéÊé®ÁêÜÊàêÊú¨ÁöÑÂêåÊôÇÂØ¶ÁèæÈ´òÂêûÂêêÈáè„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑ LLM Ê¨äÈáçÂíåÊ¥ªÂåñÈáèÂåñÁöÑË®ìÁ∑¥ÂæåÈáèÂåñÔºàPTQÔºâÊäÄË°ì‰ªçÁÑ∂ÊúÉÂ∞éËá¥ÈùûÂèØÂøΩÁï•ÁöÑÊ∫ñÁ¢∫Â∫¶‰∏ãÈôçÔºåÁâπÂà•ÊòØÂú®Â§ßË¶èÊ®°Â§ö‰ªªÂãôË™ûË®ÄÁêÜËß£‰∏ä„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰ΩéÁß©ÈáèÂåñÔºàLRQÔºâ$-$‰∏ÄÁ®ÆÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑ LLM Ë®ìÁ∑¥ÂæåÊ¨äÈáçÈáèÂåñÊñπÊ≥ïÔºåÂÆÉÈÄöÈÅéÂà©Áî®‰ΩéÁß©Ê¨äÈáçÁ∏ÆÊîæÁü©Èô£‰æÜÈáçÂª∫‰∏≠Èñì Transformer Â°äÁöÑËº∏Âá∫ÔºåÂèñ‰ª£‰∫ÜÂåÖÂê´ËàáÂÖ∂ÈóúËÅØÊ¨äÈáç‰∏ÄÊ®£Â§öÂèØÂ≠∏ÁøíÁ∏ÆÊîæÁöÑÂÇ≥Áµ±ÂÖ®Ê¨äÈáçÁ∏ÆÊîæÁü©Èô£„ÄÇÁî±ÊñºÈÄöÈÅé‰ΩéÁß©ÁµêÊßãÂÖ±‰∫´ÂèÉÊï∏ÔºåLRQ Âè™ÈúÄÂ≠∏ÁøíÈ°ØËëóÊõ¥Â∞ëÁöÑÂèÉÊï∏ÔºåÂêåÊôÇÂïüÁî®Ê¨äÈáçÁöÑÂÄãÂà•Á∏ÆÊîæÔºåÂæûËÄåÊèêÂçáÈáèÂåñ LLM ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫Ü LRQ Âú®‰ª•‰∏ãÊÉÖÊ≥Å‰∏ãÂÑ™ÊñºÂÖàÂâçÁöÑ LLM PTQ Â∑•‰ΩúÔºö(i) 8 ‰ΩçÂÖÉÊ¨äÈáçÂíåÊØèÂÄãÂºµÈáèÊ¥ªÂåñÈáèÂåñÔºå(ii) 4 ‰ΩçÂÖÉÊ¨äÈáçÂíå 8 ‰ΩçÂÖÉÊØèÂÄãÁ¨¶ËôüÊ¥ªÂåñÈáèÂåñÔºå‰ª•Âèä (iii) ‰Ωé‰ΩçÂÖÉÂÉÖÊ¨äÈáçÈáèÂåñÊñπÊ°à„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® \url{https://github.com/onliwad101/FlexRound_LRQ} ÂèñÂæóÔºå‰ª•ÂïüÁôº LLM Á†îÁ©∂‰∫∫Âì°ÂíåÂ∑•Á®ãÂ∏´„ÄÇ

##### **Reasoning with Large Language Models, a Survey**
2407.11511v1 by Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back

Scaling up language models to billions of parameters has opened up
possibilities for in-context learning, allowing instruction tuning and few-shot
learning on tasks that the model was not specifically trained for. This has
achieved breakthrough performance on language tasks such as translation,
summarization, and question-answering. Furthermore, in addition to these
associative "System 1" tasks, recent advances in Chain-of-thought prompt
learning have demonstrated strong "System 2" reasoning abilities, answering a
question in the field of artificial general intelligence whether LLMs can
reason. The field started with the question whether LLMs can solve grade school
math word problems. This paper reviews the rapidly expanding field of
prompt-based reasoning with LLMs. Our taxonomy identifies different ways to
generate, evaluate, and control multi-step reasoning. We provide an in-depth
coverage of core approaches and open problems, and we propose a research agenda
for the near future. Finally, we highlight the relation between reasoning and
prompt-based learning, and we discuss the relation between reasoning,
sequential decision processes, and reinforcement learning. We find that
self-improvement, self-reflection, and some metacognitive abilities of the
reasoning processes are possible through the judicious use of prompts. True
self-improvement and self-reasoning, to go from reasoning with LLMs to
reasoning by LLMs, remains future work.

ÊëòË¶ÅÔºö<paragraph>Â∞áË™ûË®ÄÊ®°ÂûãÊì¥Â±ïÂà∞Êï∏ÂçÅÂÑÑÂÄãÂèÉÊï∏ÈñãÂïü‰∫ÜÊÉÖÂ¢ÉÂ≠∏ÁøíÁöÑÂèØËÉΩÊÄßÔºåÂÖÅË®±Â∞çÊ®°ÂûãÊú™Á∂ìÁâπÂà•Ë®ìÁ∑¥ÁöÑ‰ªªÂãôÈÄ≤Ë°åÊåá‰ª§Ë™øÊï¥ÂíåÂ∞ëÈáèÂ≠∏Áøí„ÄÇÈÄôÂú®ÁøªË≠Ø„ÄÅÊëòË¶ÅÂíåÂïèÁ≠îÁ≠âË™ûË®Ä‰ªªÂãô‰∏äÂØ¶Áèæ‰∫ÜÁ™ÅÁ†¥ÊÄßÁöÑË°®Áèæ„ÄÇÊ≠§Â§ñÔºåÈô§‰∫ÜÈÄô‰∫õËÅØÊÉ≥ÂºèÁöÑ„ÄåÁ≥ªÁµ± 1„Äç‰ªªÂãô‰πãÂ§ñÔºåÊÄùËÄÉÈèàÊèêÁ§∫Â≠∏ÁøíÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÂ±ïÁ§∫‰∫ÜÂº∑Â§ßÁöÑ„ÄåÁ≥ªÁµ± 2„ÄçÊé®ÁêÜËÉΩÂäõÔºåÂõûÁ≠î‰∫Ü‰∫∫Â∑•ÈÄöÁî®Êô∫ÊÖßÈ†òÂüü‰∏≠ LLM ÊòØÂê¶ÂèØ‰ª•Êé®ÁêÜÁöÑÂïèÈ°å„ÄÇË©≤È†òÂüüÂßãÊñº LLM ÊòØÂê¶ËÉΩËß£Ê±∫Â∞èÂ≠∏Êï∏Â≠∏ÊñáÂ≠óÈ°åÁöÑÂïèÈ°å„ÄÇÊú¨ÊñáÂõûÈ°ß‰∫Ü LLM ÊèêÁ§∫ÂºèÊé®ÁêÜÂø´ÈÄüÊì¥Â±ïÁöÑÈ†òÂüü„ÄÇÊàëÂÄëÁöÑÂàÜÈ°ûÊ≥ïË≠òÂà•‰∫ÜÁîüÊàê„ÄÅË©ï‰º∞ÂíåÊéßÂà∂Â§öÊ≠•È©üÊé®ÁêÜÁöÑ‰∏çÂêåÊñπÊ≥ï„ÄÇÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®é‰∫ÜÊ†∏ÂøÉÊñπÊ≥ïÂíåÈñãÊîæÊÄßÂïèÈ°åÔºå‰∏¶ÊèêÂá∫‰∫ÜËøëÊúüÁ†îÁ©∂Ë≠∞Á®ã„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂº∑Ë™ø‰∫ÜÊé®ÁêÜÂíåÊèêÁ§∫ÂºèÂ≠∏Áøí‰πãÈñìÁöÑÈóú‰øÇÔºå‰∏¶Ë®éË´ñ‰∫ÜÊé®ÁêÜ„ÄÅÂ∫èÂàóÊ±∫Á≠ñÈÅéÁ®ãÂíåÂº∑ÂåñÂ≠∏Áøí‰πãÈñìÁöÑÈóú‰øÇ„ÄÇÊàëÂÄëÁôºÁèæÔºåÈÄöÈÅéÊòéÊô∫Âú∞‰ΩøÁî®ÊèêÁ§∫ÔºåÊé®ÁêÜÈÅéÁ®ãÁöÑËá™ÂÆåÂñÑ„ÄÅËá™ÊàëÂèçÊÄùÂíå‰∏Ä‰∫õÂÖÉË™çÁü•ËÉΩÂäõÊòØÂèØËÉΩÁöÑ„ÄÇÁúüÊ≠£ÁöÑËá™ÊàëÂÆåÂñÑÂíåËá™ÊàëÊé®ÁêÜÔºåÂæû‰ΩøÁî® LLM Êé®ÁêÜËΩâËÆäÁÇ∫Áî± LLM Êé®ÁêÜÔºå‰ªçÁÑ∂ÊòØÊú™‰æÜÁöÑÁ†îÁ©∂Â∑•‰Ωú„ÄÇ</paragraph>

##### **Diff-MTS: Temporal-Augmented Conditional Diffusion-based AIGC for Industrial Time Series Towards the Large Model Era**
2407.11501v1 by Lei Ren, Haiteng Wang, Yuanjun Laili

Industrial Multivariate Time Series (MTS) is a critical view of the
industrial field for people to understand the state of machines. However, due
to data collection difficulty and privacy concerns, available data for building
industrial intelligence and industrial large models is far from sufficient.
Therefore, industrial time series data generation is of great importance.
Existing research usually applies Generative Adversarial Networks (GANs) to
generate MTS. However, GANs suffer from unstable training process due to the
joint training of the generator and discriminator. This paper proposes a
temporal-augmented conditional adaptive diffusion model, termed Diff-MTS, for
MTS generation. It aims to better handle the complex temporal dependencies and
dynamics of MTS data. Specifically, a conditional Adaptive Maximum-Mean
Discrepancy (Ada-MMD) method has been proposed for the controlled generation of
MTS, which does not require a classifier to control the generation. It improves
the condition consistency of the diffusion model. Moreover, a Temporal
Decomposition Reconstruction UNet (TDR-UNet) is established to capture complex
temporal patterns and further improve the quality of the synthetic time series.
Comprehensive experiments on the C-MAPSS and FEMTO datasets demonstrate that
the proposed Diff-MTS performs substantially better in terms of diversity,
fidelity, and utility compared with GAN-based methods. These results show that
Diff-MTS facilitates the generation of industrial data, contributing to
intelligent maintenance and the construction of industrial large models.

ÊëòË¶ÅÔºöÂ∑•Ê•≠Â§öÂÖÉÊôÇÈñìÂ∫èÂàó (MTS) ÊòØÂ∑•Ê•≠È†òÂüü‰∏≠ÈóúÈçµÁöÑËßÄÈªûÔºåËÆì‰∫∫ÂÄë‰∫ÜËß£Ê©üÂô®ÁãÄÊÖã„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºË≥áÊñôÊî∂ÈõÜÂõ∞Èõ£ÂíåÈö±ÁßÅÂïèÈ°åÔºåÂèØÁî®ÊñºÂª∫Á´ãÂ∑•Ê•≠Êô∫ÊÖßÂíåÂ∑•Ê•≠Â§ßÂûãÊ®°ÂûãÁöÑË≥áÊñôÈÅ†ÈÅ†‰∏çË∂≥„ÄÇÂõ†Ê≠§ÔºåÂ∑•Ê•≠ÊôÇÈñìÂ∫èÂàóË≥áÊñôÁîüÊàêÈùûÂ∏∏ÈáçË¶Å„ÄÇÁèæÊúâÁ†îÁ©∂ÈÄöÂ∏∏ÊáâÁî®ÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑Ø (GAN) ‰æÜÁî¢Áîü MTS„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁîüÊàêÂô®ÂíåÈëëÂà•Âô®ÁöÑËÅØÂêàË®ìÁ∑¥ÔºåGAN Ë®ìÁ∑¥ÈÅéÁ®ã‰∏çÁ©©ÂÆö„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊôÇÈñìÂ¢ûÂº∑Ê¢ù‰ª∂ÈÅ©ÊáâÊì¥Êï£Ê®°ÂûãÔºåÁ®±ÁÇ∫ Diff-MTSÔºåÁî®Êñº MTS ÁîüÊàê„ÄÇÂÆÉÊó®Âú®Êõ¥Â•ΩÂú∞ËôïÁêÜ MTS Ë≥áÊñôÁöÑË§áÈõúÊôÇÈñì‰æùË≥¥ÊÄßÂíåÂãïÊÖãÊÄß„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊ¢ù‰ª∂ÈÅ©ÊáâÊúÄÂ§ßÂùáÂÄºÂ∑ÆÁï∞ (Ada-MMD) ÊñπÊ≥ïÔºåÁî®ÊñºÂèóÊéßÁîüÊàê MTSÔºåÂÆÉ‰∏çÈúÄË¶ÅÂàÜÈ°ûÂô®‰æÜÊéßÂà∂ÁîüÊàê„ÄÇÂÆÉÊîπÈÄ≤‰∫ÜÊì¥Êï£Ê®°ÂûãÁöÑÊ¢ù‰ª∂‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÂª∫Á´ã‰∫Ü‰∏ÄÂÄãÊôÇÈñìÂàÜËß£ÈáçÂª∫ UNet (TDR-UNet) ‰æÜÊçïÊçâË§áÈõúÁöÑÊôÇÈñìÊ®°ÂºèÔºå‰∏¶ÈÄ≤‰∏ÄÊ≠•ÊèêÈ´òÂêàÊàêÊôÇÈñìÂ∫èÂàóÁöÑÂìÅË≥™„ÄÇÂú® C-MAPSS Âíå FEMTO Ë≥áÊñôÈõÜ‰∏äÁöÑÁ∂úÂêàÂØ¶È©óË°®ÊòéÔºåËàáÂü∫Êñº GAN ÁöÑÊñπÊ≥ïÁõ∏ÊØîÔºåÊâÄÊèêÂá∫ÁöÑ Diff-MTS Âú®Â§öÊ®£ÊÄß„ÄÅ‰øùÁúüÂ∫¶ÂíåÊïàÁî®ÊñπÈù¢Ë°®ÁèæÂæóÈ°ØËëóÊõ¥Â•Ω„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåDiff-MTS ‰øÉÈÄ≤‰∫ÜÂ∑•Ê•≠Ë≥áÊñôÁöÑÁîüÊàêÔºåÊúâÂä©ÊñºÊô∫ÊÖßÁ∂≠Ë≠∑ÂíåÂ∑•Ê•≠Â§ßÂûãÊ®°ÂûãÁöÑÊßãÂª∫„ÄÇ

##### **An AI System for Continuous Knee Osteoarthritis Severity Grading Using Self-Supervised Anomaly Detection with Limited Data**
2407.11500v1 by Niamh Belton, Aonghus Lawlor, Kathleen M. Curran

The diagnostic accuracy and subjectivity of existing Knee Osteoarthritis (OA)
ordinal grading systems has been a subject of on-going debate and concern.
Existing automated solutions are trained to emulate these imperfect systems,
whilst also being reliant on large annotated databases for fully-supervised
training. This work proposes a three stage approach for automated continuous
grading of knee OA that is built upon the principles of Anomaly Detection (AD);
learning a robust representation of healthy knee X-rays and grading disease
severity based on its distance to the centre of normality. In the first stage,
SS-FewSOME is proposed, a self-supervised AD technique that learns the 'normal'
representation, requiring only examples of healthy subjects and <3% of the
labels that existing methods require. In the second stage, this model is used
to pseudo label a subset of unlabelled data as 'normal' or 'anomalous',
followed by denoising of pseudo labels with CLIP. The final stage involves
retraining on labelled and pseudo labelled data using the proposed Dual Centre
Representation Learning (DCRL) which learns the centres of two representation
spaces; normal and anomalous. Disease severity is then graded based on the
distance to the learned centres. The proposed methodology outperforms existing
techniques by margins of up to 24% in terms of OA detection and the disease
severity scores correlate with the Kellgren-Lawrence grading system at the same
level as human expert performance. Code available at
https://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis.

ÊëòË¶ÅÔºöÁèæÊúâËÜùÈ™®ÈóúÁØÄÁÇé (OA)
Â∫èÊï∏ÂàÜÁ¥öÁ≥ªÁµ±ÁöÑË®∫Êñ∑Ê∫ñÁ¢∫Â∫¶Âíå‰∏ªËßÄÊÄß‰∏ÄÁõ¥ÊòØÊåÅÁ∫åÁà≠Ë´ñÂíåÈóúÊ≥®ÁöÑ‰∏ªÈ°å„ÄÇ
ÁèæÊúâÁöÑËá™ÂãïÂåñËß£Ê±∫ÊñπÊ°àÁ∂ìÈÅéË®ìÁ∑¥‰ª•Ê®°Êì¨ÈÄô‰∫õ‰∏çÂÆåÁæéÁöÑÁ≥ªÁµ±Ôºå
ÂêåÊôÇ‰πü‰æùË≥¥ÊñºÂ§ßÂûãË®ªÈáãË≥áÊñôÂ∫´ÈÄ≤Ë°åÂÆåÂÖ®Áõ£Áù£ÁöÑ
Ë®ìÁ∑¥„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏ÄÂÄã‰∏âÈöéÊÆµÊñπÊ≥ïÔºåÁî®ÊñºËÜùÈ™®ÈóúÁØÄÁÇéÁöÑËá™ÂãïÈÄ£Á∫å
ÂàÜÁ¥öÔºåË©≤ÊñπÊ≥ïÂª∫Á´ãÂú®Áï∞Â∏∏Ê™¢Ê∏¨ (AD) ÁöÑÂéüÁêÜ‰πã‰∏äÔºõ
Â≠∏ÁøíÂÅ•Â∫∑ËÜùËìã X ÂÖâÁâáÁöÑÁ©©ÂÅ•Ë°®ÁèæÔºå‰∏¶Ê†πÊìöÂÖ∂ËàáÂ∏∏ÊÖã‰∏≠ÂøÉÁöÑË∑ùÈõ¢Â∞çÁñæÁóÖ
Âö¥ÈáçÁ®ãÂ∫¶ÈÄ≤Ë°åÂàÜÁ¥ö„ÄÇÂú®Á¨¨‰∏ÄÈöéÊÆµÔºå
SS-FewSOME Ë¢´ÊèêÂá∫ÔºåÈÄôÊòØ‰∏ÄÁ®ÆËá™Áõ£Áù£ AD ÊäÄË°ìÔºåÂÆÉÂ≠∏Áøí„ÄåÊ≠£Â∏∏„Äç
Ë°®ÁèæÔºåÂè™ÈúÄË¶ÅÂÅ•Â∫∑ÂèóË©¶ËÄÖÁöÑÁØÑ‰æãÂíå <3% ÁèæÊúâÊñπÊ≥ïÊâÄÈúÄÁöÑ
Ê®ôÁ±§„ÄÇÂú®Á¨¨‰∫åÈöéÊÆµÔºåÊ≠§Ê®°ÂûãÁî®ÊñºÂ∞áÊú™Ê®ôË®òË≥áÊñôÁöÑÂ≠êÈõÜÂÅΩÊ®ôË®òÁÇ∫„ÄåÊ≠£Â∏∏„ÄçÊàñ„ÄåÁï∞Â∏∏„ÄçÔºå
Êé•Ëëó‰ΩøÁî® CLIP Â∞çÂÅΩÊ®ôÁ±§ÈÄ≤Ë°åÂéªÈõúË®ä„ÄÇÊúÄÂæåÁöÑÈöéÊÆµÊ∂âÂèä
‰ΩøÁî®ÊèêÂá∫ÁöÑÈõô‰∏≠ÂøÉË°®Á§∫Â≠∏Áøí (DCRL) Â∞çÊ®ôË®òÂíåÂÅΩÊ®ôË®òË≥áÊñôÈÄ≤Ë°åÈáçÊñ∞Ë®ìÁ∑¥ÔºåË©≤Â≠∏ÁøíÂ≠∏ÁøíÂÖ©ÂÄãË°®Á§∫
Á©∫ÈñìÁöÑ‰∏≠ÂøÉÔºõÊ≠£Â∏∏ÂíåÁï∞Â∏∏„ÄÇÁÑ∂ÂæåÊ†πÊìö
Â≠∏Áøí‰∏≠ÂøÉ‰πãÈñìÁöÑË∑ùÈõ¢Â∞çÁñæÁóÖÂö¥ÈáçÁ®ãÂ∫¶ÈÄ≤Ë°åÂàÜÁ¥ö„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú® OA Ê™¢Ê∏¨ÊñπÈù¢ÊØîÁèæÊúâ
ÊäÄË°ìÈ´òÂá∫ 24%Ôºå‰∏¶‰∏îÁñæÁóÖ
Âö¥ÈáçÁ®ãÂ∫¶ÂæóÂàÜËàá Kellgren-Lawrence ÂàÜÁ¥öÁ≥ªÁµ±Áõ∏ÈóúÔºåËàá‰∫∫È°ûÂ∞àÂÆ∂Ë°®ÁèæÁõ∏Âêå
Á≠âÁ¥ö„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú®
https://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis ÂèñÂæó„ÄÇ

##### **MMSD-Net: Towards Multi-modal Stuttering Detection**
2407.11492v1 by Liangyu Nie, Sudarsana Reddy Kadiri, Ruchit Agrawal

Stuttering is a common speech impediment that is caused by irregular
disruptions in speech production, affecting over 70 million people across the
world. Standard automatic speech processing tools do not take speech ailments
into account and are thereby not able to generate meaningful results when
presented with stuttered speech as input. The automatic detection of stuttering
is an integral step towards building efficient, context-aware speech processing
systems. While previous approaches explore both statistical and neural
approaches for stuttering detection, all of these methods are uni-modal in
nature. This paper presents MMSD-Net, the first multi-modal neural framework
for stuttering detection. Experiments and results demonstrate that
incorporating the visual signal significantly aids stuttering detection, and
our model yields an improvement of 2-17% in the F1-score over existing
state-of-the-art uni-modal approaches.

ÊëòË¶ÅÔºöÂè£ÂêÉÊòØ‰∏ÄÁßçÂ∏∏ËßÅÁöÑË®ÄËØ≠ÈöúÁ¢çÔºåÊòØÁî±Ë®ÄËØ≠‰∫ßÁîü‰∏≠ÁöÑ‰∏çËßÑÂàô‰∏≠Êñ≠ÊâÄÂºïËµ∑ÁöÑÔºåÂΩ±Âìç‰∫ÜÂÖ®ÁêÉË∂ÖËøá 7000 ‰∏á‰∫∫„ÄÇÊ†áÂáÜÁöÑËá™Âä®ËØ≠Èü≥Â§ÑÁêÜÂ∑•ÂÖ∑‰∏ç‰ºöËÄÉËôëË®ÄËØ≠ÈöúÁ¢çÔºåÂõ†Ê≠§Âú®ËæìÂÖ•Âè£ÂêÉËØ≠Èü≥Êó∂Êó†Ê≥ïÁîüÊàêÊúâÊÑè‰πâÁöÑÁªìÊûú„ÄÇÂè£ÂêÉÁöÑËá™Âä®Ê£ÄÊµãÊòØÊûÑÂª∫È´òÊïà‰∏îÂÖ∑ÊúâÊÉÖÂ¢ÉÊÑüÁü•ÁöÑËØ≠Èü≥Â§ÑÁêÜÁ≥ªÁªü‰∏≠‰∏çÂèØÊàñÁº∫ÁöÑ‰∏ÄÊ≠•„ÄÇËôΩÁÑ∂ÂÖàÂâçÁöÑÂÅöÊ≥ïÊé¢Á¥¢‰∫ÜÁªüËÆ°ÂíåÁ•ûÁªèÊñπÊ≥ïÊù•ËøõË°åÂè£ÂêÉÊ£ÄÊµãÔºå‰ΩÜÊâÄÊúâËøô‰∫õÊñπÊ≥ïÊú¨Ë¥®‰∏äÈÉΩÊòØÂçïÊ®°ÊÄÅÁöÑ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü MMSD-NetÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Áî®‰∫éÂè£ÂêÉÊ£ÄÊµãÁöÑÂ§öÊ®°ÊÄÅÁ•ûÁªèÊ°ÜÊû∂„ÄÇÂÆûÈ™åÂíåÁªìÊûúË°®ÊòéÔºåÁªìÂêàËßÜËßâ‰ø°Âè∑ÂèØ‰ª•ÊûÅÂ§ßÂú∞Â∏ÆÂä©Âè£ÂêÉÊ£ÄÊµãÔºåÂπ∂‰∏îÊàë‰ª¨ÁöÑÊ®°ÂûãÂú® F1 ÂàÜÊï∞‰∏äÊØîÁé∞ÊúâÁöÑÊúÄÂÖàËøõÁöÑÂçïÊ®°ÊÄÅÊñπÊ≥ïÊèêÈ´ò‰∫Ü 2-17%„ÄÇ

##### **A Meta-Learning Approach for Multi-Objective Reinforcement Learning in Sustainable Home Environments**
2407.11489v1 by Junlin Lu, Patrick Mannion, Karl Mason

Effective residential appliance scheduling is crucial for sustainable living.
While multi-objective reinforcement learning (MORL) has proven effective in
balancing user preferences in appliance scheduling, traditional MORL struggles
with limited data in non-stationary residential settings characterized by
renewable generation variations. Significant context shifts that can invalidate
previously learned policies. To address these challenges, we extend
state-of-the-art MORL algorithms with the meta-learning paradigm, enabling
rapid, few-shot adaptation to shifting contexts. Additionally, we employ an
auto-encoder (AE)-based unsupervised method to detect environment context
changes. We have also developed a residential energy environment to evaluate
our method using real-world data from London residential settings. This study
not only assesses the application of MORL in residential appliance scheduling
but also underscores the effectiveness of meta-learning in energy management.
Our top-performing method significantly surpasses the best baseline, while the
trained model saves 3.28% on electricity bills, a 2.74% increase in user
comfort, and a 5.9% improvement in expected utility. Additionally, it reduces
the sparsity of solutions by 62.44%. Remarkably, these gains were accomplished
using 96.71% less training data and 61.1% fewer training steps.

ÊëòË¶ÅÔºöÊúâÊïà‰ΩèÂÆÖË®≠ÂÇôÊéíÁ®ãÂ∞çÊñºÊ∞∏Á∫åÁîüÊ¥ªËá≥ÈóúÈáçË¶Å„ÄÇ
ÈõñÁÑ∂Â§öÁõÆÊ®ôÂ¢ûÂº∑Â≠∏Áøí (MORL) Â∑≤Ë¢´Ë≠âÊòéËÉΩÊúâÊïàÂπ≥Ë°°Ë®≠ÂÇôÊéíÁ®ã‰∏≠ÁöÑ‰ΩøÁî®ËÄÖÂÅèÂ•ΩÔºå‰ΩÜÂÇ≥Áµ± MORL Âú®‰ª•ÂÜçÁîüËÉΩÊ∫êËÆäÂåñÁÇ∫ÁâπÂæµÁöÑÈùûÈùúÊ≠¢‰ΩèÂÆÖÁí∞Â¢É‰∏≠ÔºåÊúÉÂõ†Ë≥áÊñôÊúâÈôêËÄåÈÅáÂà∞Âõ∞Èõ£„ÄÇÈáçÂ§ßÁöÑÊÉÖÂ¢ÉËΩâËÆäÂèØËÉΩÊúÉ‰ΩøÂÖàÂâçÂ≠∏ÁøíÁöÑÊîøÁ≠ñÂ§±Êïà„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄë‰ΩøÁî®ÂÖÉÂ≠∏ÁøíÁØÑ‰æã‰æÜÊì¥ÂÖÖÊúÄÂÖàÈÄ≤ÁöÑ MORL ÊºîÁÆóÊ≥ïÔºåÂØ¶ÁèæÂø´ÈÄü„ÄÅÂ∞ëÈáèÈÅ©ÊáâËÆäÂåñÁöÑÊÉÖÂ¢É„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊé°Áî®Âü∫ÊñºËá™ÂãïÁ∑®Á¢ºÂô® (AE) ÁöÑÈùûÁõ£Áù£ÂºèÊñπÊ≥ï‰æÜÂÅµÊ∏¨Áí∞Â¢ÉÊÉÖÂ¢ÉËÆäÂåñ„ÄÇÊàëÂÄëÈÇÑÈñãÁôº‰∫Ü‰∏ÄÂÄã‰ΩèÂÆÖËÉΩÊ∫êÁí∞Â¢ÉÔºå‰ΩøÁî®‰æÜËá™ÂÄ´Êï¶‰ΩèÂÆÖÁí∞Â¢ÉÁöÑÁúüÂØ¶‰∏ñÁïåË≥áÊñô‰æÜË©ï‰º∞ÊàëÂÄëÁöÑÂÅöÊ≥ï„ÄÇÊú¨Á†îÁ©∂‰∏çÂÉÖË©ï‰º∞ MORL Âú®‰ΩèÂÆÖË®≠ÂÇôÊéíÁ®ã‰∏≠ÁöÑÊáâÁî®Ôºå‰πüÂº∑Ë™ø‰∫ÜÂÖÉÂ≠∏ÁøíÂú®ËÉΩÊ∫êÁÆ°ÁêÜ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÊàëÂÄëÊïàËÉΩÊúÄ‰Ω≥ÁöÑÊñπÊ≥ïÈ°ØËëóË∂ÖË∂äÊúÄ‰Ω≥Âü∫Ê∫ñÔºåËÄåË®ìÁ∑¥ÂæåÁöÑÊ®°ÂûãÂèØÁØÄÁúÅ 3.28% ÁöÑÈõªË≤ª„ÄÅ‰ΩøÁî®ËÄÖËàíÈÅ©Â∫¶ÊèêÈ´ò 2.74%ÔºåÈ†êÊúüÊïàÁî®ÊîπÂñÑ 5.9%„ÄÇÊ≠§Â§ñÔºåÂÆÉÈÇÑÂ∞áËß£ÁöÑÁ®ÄÁñèÊÄßÈôç‰Ωé‰∫Ü 62.44%„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÈÄô‰∫õÈÄ≤Â±ïÊòØ‰ΩøÁî®Â∞ë 96.71% ÁöÑË®ìÁ∑¥Ë≥áÊñôÂíåÂ∞ë 61.1% ÁöÑË®ìÁ∑¥Ê≠•È©üÊâÄÈÅîÊàêÁöÑ„ÄÇ

##### **Scientific QA System with Verifiable Answers**
2407.11485v1 by Adela Ljajiƒá, Milo≈° Ko≈°prdiƒá, Bojana Ba≈°aragin, Darija Medvecki, Lorenzo Cassano, Nikola Milo≈°eviƒá

In this paper, we introduce the VerifAI project, a pioneering open-source
scientific question-answering system, designed to provide answers that are not
only referenced but also automatically vetted and verifiable. The components of
the system are (1) an Information Retrieval system combining semantic and
lexical search techniques over scientific papers (PubMed), (2) a
Retrieval-Augmented Generation (RAG) module using fine-tuned generative model
(Mistral 7B) and retrieved articles to generate claims with references to the
articles from which it was derived, and (3) a Verification engine, based on a
fine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task
using SciFACT dataset. The verification engine cross-checks the generated claim
and the article from which the claim was derived, verifying whether there may
have been any hallucinations in generating the claim. By leveraging the
Information Retrieval and RAG modules, Verif.ai excels in generating factual
information from a vast array of scientific sources. At the same time, the
Verification engine rigorously double-checks this output, ensuring its accuracy
and reliability. This dual-stage process plays a crucial role in acquiring and
confirming factual information, significantly enhancing the information
landscape. Our methodology could significantly enhance scientists'
productivity, concurrently fostering trust in applying generative language
models within scientific domains, where hallucinations and misinformation are
unacceptable.

ÊëòË¶ÅÔºöÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü VerifAI Ë®àÁï´ÔºåÈÄôÊòØ‰∏ÄÂÄãÈñãÂâµÊÄßÁöÑÈñãÊîæÂéüÂßãÁ¢ºÁßëÂ≠∏ÂïèÁ≠îÁ≥ªÁµ±ÔºåÊó®Âú®Êèê‰æõ‰∏çÂÉÖÊúâÂèÉËÄÉ‰æùÊìöÔºåËÄå‰∏îÁ∂ìÈÅéËá™ÂãïÂØ©Êü•ÂíåÈ©óË≠âÁöÑÁ≠îÊ°à„ÄÇË©≤Á≥ªÁµ±ÁöÑÁµÑÊàêÈÉ®ÂàÜÁÇ∫Ôºö(1) ‰∏ÄÂÄãË≥áË®äÊ™¢Á¥¢Á≥ªÁµ±ÔºåÁµêÂêàË™ûÊÑèÂíåË©ûÂΩôÊêúÂ∞ãÊäÄË°ìÔºåÁî®ÊñºÁßëÂ≠∏Ë´ñÊñá(PubMed)Ôºõ(2) ‰∏ÄÂÄãÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Ê®°ÁµÑÔºå‰ΩøÁî®ÂæÆË™øÁîüÊàêÊ®°Âûã (Mistral 7B) ÂíåÊ™¢Á¥¢ÁöÑÊñáÁ´†ÔºåÈÄèÈÅéÂºïËø∞ÊñáÁ´†ÁîüÊàê‰∏ªÂºµÔºõ(3) ‰∏ÄÂÄãÈ©óË≠âÂºïÊìéÔºåÂü∫ÊñºÂæÆË™øÁöÑ DeBERTa Âíå XLM-RoBERTa Ê®°ÂûãÔºå‰ΩøÁî® SciFACT Ë≥áÊñôÈõÜÈÄ≤Ë°åËá™ÁÑ∂Ë™ûË®ÄÊé®Ë´ñ‰ªªÂãô„ÄÇÈ©óË≠âÂºïÊìé‰∫§ÂèâÊØîÂ∞çÁî¢ÁîüÁöÑ‰∏ªÂºµÂíå‰∏ªÂºµË°çÁîüÁöÑÊñáÁ´†ÔºåÈ©óË≠âÂú®Áî¢Áîü‰∏ªÂºµÊôÇÊòØÂê¶Âá∫Áèæ‰ªª‰ΩïÂπªË¶∫„ÄÇÈÄèÈÅéÂà©Áî®Ë≥áË®äÊ™¢Á¥¢Âíå RAG Ê®°ÁµÑÔºåVerif.ai ÊìÖÈï∑ÂæûÂ§ßÈáèÁöÑÁßëÂ≠∏‰æÜÊ∫êÁî¢Áîü‰∫ãÂØ¶Ë≥áË®ä„ÄÇÂêåÊôÇÔºåÈ©óË≠âÂºïÊìéÂö¥Ê†ºÂú∞Â∞çÊ≠§Ëº∏Âá∫ÈÄ≤Ë°åÈõôÈáçÊ™¢Êü•ÔºåÁ¢∫‰øùÂÖ∂Ê∫ñÁ¢∫ÊÄßÂíåÂèØÈù†ÊÄß„ÄÇÈÄôÂÄãÈõôÈöéÊÆµÊµÅÁ®ãÂú®Áç≤ÂèñÂíåÁ¢∫Ë™ç‰∫ãÂØ¶Ë≥áË®ä‰∏≠ÁôºÊèÆÈóúÈçµ‰ΩúÁî®ÔºåÂ§ßÂπÖÊèêÂçáË≥áË®äÁí∞Â¢É„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂèØ‰ª•Â§ßÂπÖÊèêÂçáÁßëÂ≠∏ÂÆ∂ÁöÑÁîüÁî¢ÂäõÔºåÂêåÊôÇ‰øÉÈÄ≤Âú®ÁßëÂ≠∏È†òÂüüÊáâÁî®ÁîüÊàêË™ûË®ÄÊ®°ÂûãÁöÑ‰ø°‰ªªÔºåÂú®Ë©≤È†òÂüü‰∏≠ÂπªË¶∫ÂíåÈåØË™§Ë≥áË®äÊòØ‰∏çÂèØÊé•ÂèóÁöÑ„ÄÇ

##### **The Oscars of AI Theater: A Survey on Role-Playing with Language Models**
2407.11484v2 by Nuo Chen, Yang Deng, Jia Li

This survey explores the burgeoning field of role-playing with language
models, focusing on their development from early persona-based models to
advanced character-driven simulations facilitated by Large Language Models
(LLMs). Initially confined to simple persona consistency due to limited model
capabilities, role-playing tasks have now expanded to embrace complex character
portrayals involving character consistency, behavioral alignment, and overall
attractiveness. We provide a comprehensive taxonomy of the critical components
in designing these systems, including data, models and alignment, agent
architecture and evaluation. This survey not only outlines the current
methodologies and challenges, such as managing dynamic personal profiles and
achieving high-level persona consistency but also suggests avenues for future
research in improving the depth and realism of role-playing applications. The
goal is to guide future research by offering a structured overview of current
methodologies and identifying potential areas for improvement. Related
resources and papers are available at
https://github.com/nuochenpku/Awesome-Role-Play-Papers.

ÊëòË¶ÅÔºöÊú¨Ë™øÊü•Êé¢Ë®é‰∫ÜËßíËâ≤ÊâÆÊºîËàáË™ûË®ÄÊ®°ÂûãÁöÑÊñ∞ËààÈ†òÂüüÔºåÈáçÈªûÈóúÊ≥®ÂÖ∂ÂæûÊó©ÊúüÂü∫ÊñºËßíËâ≤ÁöÑÊ®°ÂûãÁôºÂ±ïÂà∞Áî±Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰øÉÊàêÁöÑÈÄ≤ÈöéËßíËâ≤È©ÖÂãïÊ®°Êì¨„ÄÇËßíËâ≤ÊâÆÊºî‰ªªÂãôÊúÄÂàùÂÉÖÈôêÊñºÁ∞°ÂñÆÁöÑËßíËâ≤‰∏ÄËá¥ÊÄßÔºåÂõ†ÁÇ∫Ê®°ÂûãÂäüËÉΩÊúâÈôêÔºåÁèæÂú®Â∑≤Êì¥Â±ïÂà∞ÂåÖÂê´ËßíËâ≤‰∏ÄËá¥ÊÄß„ÄÅË°åÁÇ∫Â∞çÈΩäÂíåÊï¥È´îÂê∏ÂºïÂäõÁöÑË§áÈõúËßíËâ≤ÊèèÁπ™„ÄÇÊàëÂÄëÊèê‰æõ‰∫ÜÈÄô‰∫õÁ≥ªÁµ±Ë®≠Ë®à‰∏≠ÈóúÈçµÁµÑÊàêÁöÑÂÖ®Èù¢ÂàÜÈ°ûÔºåÂåÖÊã¨Ë≥áÊñô„ÄÅÊ®°ÂûãÂíåÂ∞çÈΩä„ÄÅ‰ª£ÁêÜÊû∂ÊßãÂíåË©ï‰º∞„ÄÇÊú¨Ë™øÊü•‰∏çÂÉÖÊ¶ÇËø∞‰∫ÜÁï∂ÂâçÁöÑÊñπÊ≥ïÂíåÊåëÊà∞Ôºå‰æãÂ¶ÇÁÆ°ÁêÜÂãïÊÖãÂÄã‰∫∫Ê™îÊ°àÂíåÂØ¶ÁèæÈ´òÈöéËßíËâ≤‰∏ÄËá¥ÊÄßÔºåÈÇÑÊèêÂá∫‰∫ÜÊîπÈÄ≤ËßíËâ≤ÊâÆÊºîÊáâÁî®Ê∑±Â∫¶ÂíåÁúüÂØ¶ÊÄßÁöÑÊú™‰æÜÁ†îÁ©∂ÈÄîÂæë„ÄÇÁõÆÊ®ôÊòØÈÄèÈÅéÊèê‰æõÁï∂ÂâçÊñπÊ≥ïÁöÑÁµêÊßãÂåñÊ¶ÇËßÄÂíåÊâæÂá∫ÊΩõÂú®ÁöÑÊîπÈÄ≤È†òÂüüÔºå‰æÜÂºïÂ∞éÊú™‰æÜÁöÑÁ†îÁ©∂„ÄÇÁõ∏ÈóúË≥áÊ∫êÂíåË´ñÊñáÂèØÊñº https://github.com/nuochenpku/Awesome-Role-Play-Papers ÂèñÂæó„ÄÇ

##### **Multi-Channel Masked Autoencoder and Comprehensive Evaluations for Reconstructing 12-Lead ECG from Arbitrary Single-Lead ECG**
2407.11481v1 by Jiarong Chen, Wanqing Wu, Tong Liu, Shenda Hong

In the context of cardiovascular diseases (CVD) that exhibit an elevated
prevalence and mortality, the electrocardiogram (ECG) is a popular and standard
diagnostic tool for doctors, commonly utilizing a 12-lead configuration in
clinical practice. However, the 10 electrodes placed on the surface would cause
a lot of inconvenience and discomfort, while the rapidly advancing wearable
devices adopt the reduced-lead or single-lead ECG to reduce discomfort as a
solution in long-term monitoring. Since the single-lead ECG is a subset of
12-lead ECG, it provides insufficient cardiac health information and plays a
substandard role in real-world healthcare applications. Hence, it is necessary
to utilize signal generation technologies to reduce their clinical importance
gap by reconstructing 12-lead ECG from the real single-lead ECG. Specifically,
this study proposes a multi-channel masked autoencoder (MCMA) for this goal. In
the experimental results, the visualized results between the generated and real
signals can demonstrate the effectiveness of the proposed framework. At the
same time, this study introduces a comprehensive evaluation benchmark named
ECGGenEval, encompassing the signal-level, feature-level, and diagnostic-level
evaluations, providing a holistic assessment of 12-lead ECG signals and
generative model. Further, the quantitative experimental results are as
follows, the mean square errors of 0.0178 and 0.0658, correlation coefficients
of 0.7698 and 0.7237 in the signal-level evaluation, the average F1-score with
two generated 12-lead ECG is 0.8319 and 0.7824 in the diagnostic-level
evaluation, achieving the state-of-the-art performance. The open-source code is
publicly available at \url{https://github.com/CHENJIAR3/MCMA}.

ÊëòË¶ÅÔºö<paragraph>Âú®Ë°®ÁèæÂá∫È´òÁõõË°åÁéáÂíåÊ≠ª‰∫°ÁéáÁöÑÂøÉË°ÄÁÆ°ÁñæÁóÖ (CVD) ÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂøÉÈõªÂúñ (ECG) ÊòØ‰∏ÄÁ®ÆÈÜ´ÁîüÂ∏∏Áî®ÁöÑÊ®ôÊ∫ñË®∫Êñ∑Â∑•ÂÖ∑ÔºåÂú®Ëá®Â∫äÂØ¶Âãô‰∏≠ÈÄöÂ∏∏‰ΩøÁî® 12 Â∞éÁ®ãÁµÑÊÖã„ÄÇÁÑ∂ËÄåÔºåÊîæÁΩÆÂú®Ë°®Èù¢ÁöÑ 10 ÂÄãÈõªÊ•µÊúÉÈÄ†ÊàêË®±Â§ö‰∏ç‰æøÂíå‰∏çÈÅ©ÔºåËÄåÂø´ÈÄüÈÄ≤Ê≠•ÁöÑÂèØÁ©øÊà¥ÂºèË£ùÁΩÆÊé°Áî®Ê∏õÂ∞ëÂ∞éÁ®ãÊàñÂñÆÂ∞éÁ®ã ECG ‰æÜÈôç‰Ωé‰∏çÈÅ©Ôºå‰ΩúÁÇ∫Èï∑ÊúüÁõ£Ê∏¨ÁöÑËß£Ê±∫ÊñπÊ°à„ÄÇÁî±ÊñºÂñÆÂ∞éÁ®ã ECG ÊòØ 12 Â∞éÁ®ã ECG ÁöÑÂ≠êÈõÜÔºåÂÆÉÊèê‰æõÁöÑÂÅ•Â∫∑Ë≥áË®ä‰∏çË∂≥ÔºåÂú®ÁúüÂØ¶‰∏ñÁïåÁöÑÈÜ´ÁôÇ‰øùÂÅ•ÊáâÁî®‰∏≠ÊâÆÊºîËëóÊ¨°Ê®ôÊ∫ñÁöÑËßíËâ≤„ÄÇÂõ†Ê≠§ÔºåÊúâÂøÖË¶ÅÂà©Áî®Ë®äËôüÁî¢ÁîüÊäÄË°ì‰æÜÁ∏ÆÂ∞èÂÖ∂Ëá®Â∫äÈáçË¶ÅÊÄßÂ∑ÆË∑ùÔºåÊñπÊ≥ïÊòØÂæûÁúüÂØ¶ÁöÑÂñÆÂ∞éÁ®ã ECG ÈáçÂª∫ 12 Â∞éÁ®ã ECG„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂ§öÈÄöÈÅìÈÅÆÁΩ©Ëá™ÂãïÁ∑®Á¢ºÂô® (MCMA) ‰æÜÈÅîÊàêÊ≠§ÁõÆÊ®ô„ÄÇÂú®ÂØ¶È©óÁµêÊûú‰∏≠ÔºåÁîüÊàêÁöÑË®äËôüËàáÁúüÂØ¶Ë®äËôü‰πãÈñìÁöÑÂèØË¶ñÂåñÁµêÊûúÂèØ‰ª•Ë≠âÊòéÊâÄÊèêÂá∫Êû∂ÊßãÁöÑÊúâÊïàÊÄß„ÄÇÂêåÊôÇÔºåÊú¨Á†îÁ©∂ÂºïÂÖ•‰∫ÜÁ®±ÁÇ∫ ECGGenEval ÁöÑÁ∂úÂêàË©ï‰º∞Âü∫Ê∫ñÔºåÊ∂µËìãË®äËôüÂ±§Á¥ö„ÄÅÁâπÂæµÂ±§Á¥öÂíåË®∫Êñ∑Â±§Á¥öË©ï‰º∞ÔºåÊèê‰æõ 12 Â∞éÁ®ã ECG Ë®äËôüÂíåÁîüÊàêÊ®°ÂûãÁöÑÊï¥È´îË©ï‰º∞„ÄÇÊ≠§Â§ñÔºåÂÆöÈáèÁöÑÂØ¶È©óÁµêÊûúÂ¶Ç‰∏ãÔºåÂú®Ë®äËôüÂ±§Á¥öË©ï‰º∞‰∏≠ÔºåÂùáÊñπË™§Â∑ÆÁÇ∫ 0.0178 Âíå 0.0658ÔºåÁõ∏Èóú‰øÇÊï∏ÁÇ∫ 0.7698 Âíå 0.7237ÔºåÂú®Ë®∫Êñ∑Â±§Á¥öË©ï‰º∞‰∏≠ÔºåÂÖ©ÂÄãÁîüÊàêÁöÑ 12 Â∞éÁ®ã ECG ÁöÑÂπ≥Âùá F1 ÂàÜÊï∏ÁÇ∫ 0.8319 Âíå 0.7824ÔºåÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇÈñãÊîæÂéüÂßãÁ¢ºÂèØ‰ª•Âú® \url{https://github.com/CHENJIAR3/MCMA} ÂÖ¨ÈñãÂèñÂæó„ÄÇ</paragraph>

##### **AIGC for Industrial Time Series: From Deep Generative Models to Large Generative Models**
2407.11480v1 by Lei Ren, Haiteng Wang, Yang Tang, Chunhua Yang

With the remarkable success of generative models like ChatGPT, Artificial
Intelligence Generated Content (AIGC) is undergoing explosive development. Not
limited to text and images, generative models can generate industrial time
series data, addressing challenges such as the difficulty of data collection
and data annotation. Due to their outstanding generation ability, they have
been widely used in Internet of Things, metaverse, and cyber-physical-social
systems to enhance the efficiency of industrial production. In this paper, we
present a comprehensive overview of generative models for industrial time
series from deep generative models (DGMs) to large generative models (LGMs).
First, a DGM-based AIGC framework is proposed for industrial time series
generation. Within this framework, we survey advanced industrial DGMs and
present a multi-perspective categorization. Furthermore, we systematically
analyze the critical technologies required to construct industrial LGMs from
four aspects: large-scale industrial dataset, LGMs architecture for complex
industrial characteristics, self-supervised training for industrial time
series, and fine-tuning of industrial downstream tasks. Finally, we conclude
the challenges and future directions to enable the development of generative
models in industry.

ÊëòË¶ÅÔºöÈö®Ëëó ChatGPT Á≠âÁîüÊàêÂºèÊ®°ÂûãÁöÑÈ°ØËëóÊàêÂäüÔºå‰∫∫Â∑•Êô∫ÊÖßÁîüÊàêÂÖßÂÆπ (AIGC) Ê≠£Á∂ìÊ≠∑ÁàÜÁÇ∏ÊÄßÁöÑÁôºÂ±ï„ÄÇÁîüÊàêÂºèÊ®°Âûã‰∏çÂÉÖÈôêÊñºÊñáÂ≠óÂíåÂΩ±ÂÉèÔºåÈÇÑËÉΩÁîüÊàêÂ∑•Ê•≠ÊôÇÈñìÂ∫èÂàóË≥áÊñôÔºåËß£Ê±∫Ë≥áÊñôÊî∂ÈõÜÂíåË≥áÊñôÊ®ôË®ªÁöÑÂõ∞Èõ£Á≠âÊåëÊà∞„ÄÇÁî±ÊñºÂÖ∂Âá∫Ëâ≤ÁöÑÁîüÊàêËÉΩÂäõÔºåÂ∑≤Âª£Ê≥õÊáâÁî®ÊñºÁâ©ËÅØÁ∂≤„ÄÅÂÖÉÂÆáÂÆôÂíåÁ∂≤Ë∑ØÁâ©ÁêÜÁ§æÊúÉÁ≥ªÁµ±‰∏≠Ôºå‰ª•ÊèêÈ´òÂ∑•Ê•≠ÁîüÁî¢ÁöÑÊïàÁéá„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞çÂ∑•Ê•≠ÊôÇÈñìÂ∫èÂàóÁöÑÁîüÊàêÂºèÊ®°ÂûãÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÊ¶ÇËø∞ÔºåÂæûÊ∑±Â∫¶ÁîüÊàêÂºèÊ®°Âûã (DGM) Âà∞Â§ßÂûãÁîüÊàêÂºèÊ®°Âûã (LGM)„ÄÇÈ¶ñÂÖàÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº DGM ÁöÑ AIGC Êû∂ÊßãÔºåÁî®ÊñºÁîüÊàêÂ∑•Ê•≠ÊôÇÈñìÂ∫èÂàó„ÄÇÂú®ÈÄôÂÄãÊû∂Êßã‰∏≠ÔºåÊàëÂÄëË™øÊü•‰∫ÜÂÖàÈÄ≤ÁöÑÂ∑•Ê•≠ DGMÔºå‰∏¶ÊèêÂá∫‰∫ÜÂ§öËßíÂ∫¶ÂàÜÈ°û„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂæûÂ§ßÂûãÂ∑•Ê•≠Ë≥áÊñôÈõÜ„ÄÅË§áÈõúÂ∑•Ê•≠ÁâπÂæµÁöÑ LGM Êû∂Êßã„ÄÅÂ∑•Ê•≠ÊôÇÈñìÂ∫èÂàóÁöÑËá™Áõ£Áù£Ë®ìÁ∑¥ÂíåÂ∑•Ê•≠‰∏ãÊ∏∏‰ªªÂãôÁöÑÂæÆË™øÁ≠âÂõõÂÄãÊñπÈù¢ÔºåÁ≥ªÁµ±Âú∞ÂàÜÊûê‰∫ÜÊßãÂª∫Â∑•Ê•≠ LGM ÊâÄÈúÄÁöÑÈóúÈçµÊäÄË°ì„ÄÇÊúÄÂæåÔºåÊàëÂÄëÁ∏ΩÁµê‰∫ÜÊåëÊà∞ÂíåÊú™‰æÜÁöÑÊñπÂêëÔºå‰ª•‰øÉÈÄ≤ÁîüÊàêÂºèÊ®°ÂûãÂú®Â∑•Ê•≠‰∏≠ÁöÑÁôºÂ±ï„ÄÇ

##### **XTraffic: A Dataset Where Traffic Meets Incidents with Explainability and More**
2407.11477v1 by Xiaochuan Gou, Ziyue Li, Tian Lan, Junpeng Lin, Zhishuai Li, Bingyu Zhao, Chen Zhang, Di Wang, Xiangliang Zhang

Long-separated research has been conducted on two highly correlated tracks:
traffic and incidents. Traffic track witnesses complicating deep learning
models, e.g., to push the prediction a few percent more accurate, and the
incident track only studies the incidents alone, e.g., to infer the incident
risk. We, for the first time, spatiotemporally aligned the two tracks in a
large-scale region (16,972 traffic nodes) over the whole year of 2023: our
XTraffic dataset includes traffic, i.e., time-series indexes on traffic flow,
lane occupancy, and average vehicle speed, and incidents, whose records are
spatiotemporally-aligned with traffic data, with seven different incident
classes. Additionally, each node includes detailed physical and policy-level
meta-attributes of lanes. Our data can revolutionalize traditional
traffic-related tasks towards higher interpretability and practice: instead of
traditional prediction or classification tasks, we conduct: (1) post-incident
traffic forecasting to quantify the impact of different incidents on traffic
indexes; (2) incident classification using traffic indexes to determine the
incidents types for precautions measures; (3) global causal analysis among the
traffic indexes, meta-attributes, and incidents to give high-level guidance of
the interrelations of various factors; (4) local causal analysis within road
nodes to examine how different incidents affect the road segments' relations.
The dataset is available at http://xaitraffic.github.io.

ÊëòË¶ÅÔºö<paragraph>Èï∑Êúü‰ª•‰æÜÔºåÈáùÂ∞çÂÖ©ÂÄãÈ´òÂ∫¶Áõ∏ÈóúÁöÑËªåÈÅìÈÄ≤Ë°å‰∫ÜÂàÜÈñãÁöÑÁ†îÁ©∂Ôºö
‰∫§ÈÄöÂíå‰∫ãÊïÖ„ÄÇ‰∫§ÈÄöËªåÈÅìË¶ãË≠â‰∫ÜË§áÈõúÁöÑÊ∑±Â∫¶Â≠∏Áøí
Ê®°ÂûãÔºå‰æãÂ¶ÇÔºåÂ∞áÈ†êÊ∏¨ÂÜçÊèêÈ´òÂπæÂÄãÁôæÂàÜÈªûÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåËÄå
‰∫ãÊïÖËªåÈÅìÂÉÖÁ†îÁ©∂ÂñÆÁç®ÁöÑ‰∫ãÊïÖÔºå‰æãÂ¶ÇÔºåÊé®Ë´ñ‰∫ãÊïÖ
È¢®Èö™„ÄÇÊàëÂÄëÈ¶ñÊ¨°Âú®
Â§ßË¶èÊ®°ÂçÄÂüüÔºà16,972 ÂÄã‰∫§ÈÄöÁØÄÈªûÔºâ‰∏≠ÊôÇÁ©∫Â∞çÈΩä‰∫ÜÈÄôÂÖ©ÂÄãËªåÈÅì
2023 Âπ¥ÂÖ®Âπ¥ÔºöÊàëÂÄëÁöÑ
XTraffic Êï∏ÊìöÈõÜÂåÖÊã¨‰∫§ÈÄöÔºåÂç≥‰∫§ÈÄöÊµÅÈáèÁöÑÊôÇÈñìÂ∫èÂàóÁ¥¢ÂºïÔºå
ËªäÈÅìÂç†Áî®ÁéáÂíåÂπ≥ÂùáËªäÈÄüÔºå‰ª•Âèä‰∫ãÊïÖÔºåÂÖ∂Ë®òÈåÑËàá‰∫§ÈÄöÊï∏ÊìöÂú®ÊôÇÁ©∫‰∏≠Â∞çÈΩäÔºåÊúâ‰∏ÉÂÄã‰∏çÂêåÁöÑ‰∫ãÊïÖ
È°ûÂà•„ÄÇÊ≠§Â§ñÔºåÊØèÂÄãÁØÄÈªûÈÉΩÂåÖÂê´ËªäÈÅìÁöÑË©≥Á¥∞Áâ©ÁêÜÂíåÊîøÁ≠ñÁ¥ö
ÂÖÉÂ±¨ÊÄß„ÄÇÊàëÂÄëÁöÑÊï∏ÊìöÂèØ‰ª•ÂæπÂ∫ïÊîπËÆäÂÇ≥Áµ±ÁöÑ
Ëàá‰∫§ÈÄöÁõ∏ÈóúÁöÑ‰ªªÂãôÔºåÊèêÈ´òÂèØËß£ÈáãÊÄßÂíåÂØ¶Ë∏êÊÄßÔºöËÄå‰∏çÊòØ
ÂÇ≥Áµ±ÁöÑÈ†êÊ∏¨ÊàñÂàÜÈ°û‰ªªÂãôÔºåÊàëÂÄëÈÄ≤Ë°åÔºö(1) ‰∫ãÂæå‰∫ãÊïÖ
‰∫§ÈÄöÈ†êÊ∏¨Ôºå‰ª•ÈáèÂåñ‰∏çÂêå‰∫ãÊïÖÂ∞ç‰∫§ÈÄöÁöÑÂΩ±Èüø
Á¥¢ÂºïÔºõ(2) ‰ΩøÁî®‰∫§ÈÄöÁ¥¢ÂºïÁöÑ‰∫ãÊïÖÂàÜÈ°ûÔºå‰ª•Á¢∫ÂÆö‰∫ãÊïÖ
È†êÈò≤Êé™ÊñΩÁöÑÈ°ûÂûãÔºõ(3) ‰∫§ÈÄöÁ¥¢Âºï‰πãÈñìÁöÑÂÖ®Â±ÄÂõ†ÊûúÂàÜÊûêÔºå
ÂÖÉÂ±¨ÊÄßÂíå‰∫ãÊïÖÔºå‰ª•Êèê‰æõÂêÑÁ®ÆÂõ†Á¥†Áõ∏‰∫íÈóú‰øÇÁöÑÈ´òÁ¥öÊåáÂ∞éÔºõ(4) ÈÅìË∑ØÂÖßÁöÑÂ±ÄÈÉ®Âõ†ÊûúÂàÜÊûê
ÁØÄÈªûÔºå‰ª•Ê™¢Êü•‰∏çÂêå‰∫ãÊïÖÂ¶Ç‰ΩïÂΩ±ÈüøÈÅìË∑ØË∑ØÊÆµÁöÑÈóú‰øÇ„ÄÇ
Ë©≤Êï∏ÊìöÈõÜÂèØÂú® http://xaitraffic.github.io ‰∏äÁç≤Âæó„ÄÇ</paragraph>

##### **DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems**
2407.11472v1 by Kaibo He, Chenhui Zuo, Chengtian Ma, Yanan Sui

Learning an effective policy to control high-dimensional, overactuated
systems is a significant challenge for deep reinforcement learning algorithms.
Such control scenarios are often observed in the neural control of vertebrate
musculoskeletal systems. The study of these control mechanisms will provide
insights into the control of high-dimensional, overactuated systems. The
coordination of actuators, known as muscle synergies in neuromechanics, is
considered a presumptive mechanism that simplifies the generation of motor
commands. The dynamical structure of a system is the basis of its function,
allowing us to derive a synergistic representation of actuators. Motivated by
this theory, we propose the Dynamical Synergistic Representation (DynSyn)
algorithm. DynSyn aims to generate synergistic representations from dynamical
structures and perform task-specific, state-dependent adaptation to the
representations to improve motor control. We demonstrate DynSyn's efficiency
across various tasks involving different musculoskeletal models, achieving
state-of-the-art sample efficiency and robustness compared to baseline
algorithms. DynSyn generates interpretable synergistic representations that
capture the essential features of dynamical structures and demonstrates
generalizability across diverse motor tasks.

ÊëòË¶ÅÔºöÂ≠∏Áøí‰∏ÄÂÄãÊúâÊïàÁ≠ñÁï•‰æÜÊéßÂà∂È´òÁ∂≠Â∫¶„ÄÅÈÅéÂ∫¶È©ÖÂãïÁöÑÁ≥ªÁµ±ÔºåÂ∞çÊñºÊ∑±Â∫¶Âº∑ÂåñÂ≠∏ÁøíÊºîÁÆóÊ≥ï‰æÜË™™ÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇ
ÈÄôÁ®ÆÊéßÂà∂Â†¥ÊôØÈÄöÂ∏∏Âú®ËÑäÊ§éÂãïÁâ©ËÇåËÇâÈ™®È™ºÁ≥ªÁµ±ÁöÑÁ•ûÁ∂ìÊéßÂà∂‰∏≠ËßÄÂØüÂà∞„ÄÇ
Â∞çÈÄô‰∫õÊéßÂà∂Ê©üÂà∂ÁöÑÊé¢Ë®éÔºåÂ∞áÊèê‰æõÂ∞çÈ´òÁ∂≠Â∫¶„ÄÅÈÅéÂ∫¶È©ÖÂãïÁ≥ªÁµ±ÊéßÂà∂ÁöÑË¶ãËß£„ÄÇ
Âú®Á•ûÁ∂ìÂäõÂ≠∏‰∏≠Á®±ÁÇ∫ËÇåËÇâÂçîÂêå‰ΩúÁî®ÁöÑËá¥ÂãïÂô®ÂçîË™øÔºåË¢´Ë™çÁÇ∫ÊòØ‰∏ÄÁ®ÆÁ∞°ÂåñÈÅãÂãïÊåá‰ª§Áî¢ÁîüÁöÑÂÅáË®≠Ê©üÂà∂„ÄÇ
Á≥ªÁµ±ÁöÑÂãïÊÖãÁµêÊßãÊòØÂÖ∂ÂäüËÉΩÁöÑÂü∫Á§éÔºå‰ΩøÊàëÂÄëËÉΩÂ§†Êé®Â∞éÂá∫Ëá¥ÂãïÂô®ÁöÑÂçîÂêåË°®Á§∫„ÄÇ
ÂèóÂà∞ÈÄôÂÄãÁêÜË´ñÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂãïÊÖãÂçîÂêåË°®Á§∫ (DynSyn) ÊºîÁÆóÊ≥ï„ÄÇ
DynSyn Êó®Âú®ÂæûÂãïÊÖãÁµêÊßã‰∏≠Áî¢ÁîüÂçîÂêåË°®Á§∫Ôºå‰∏¶Â∞çË°®Á§∫Âü∑Ë°åÁâπÂÆöÊñº‰ªªÂãôÁöÑÁãÄÊÖã‰æùË≥¥ÊÄßÈÅ©ÊáâÔºå‰ª•ÊîπÂñÑÈÅãÂãïÊéßÂà∂„ÄÇ
ÊàëÂÄëÂ±ïÁ§∫‰∫Ü DynSyn Âú®Ê∂âÂèä‰∏çÂêåËÇåËÇâÈ™®È™ºÊ®°ÂûãÁöÑÂêÑÁ®Æ‰ªªÂãô‰∏≠ÁöÑÊïàÁéáÔºåËàáÂü∫Á∑öÊºîÁÆóÊ≥ïÁõ∏ÊØîÔºåÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊ®£Êú¨ÊïàÁéáÂíåÁ©©ÂÅ•ÊÄß„ÄÇ
DynSyn Áî¢ÁîüÂèØËß£ÈáãÁöÑÂçîÂêåË°®Á§∫ÔºåÊçïÊçâÂãïÊÖãÁµêÊßãÁöÑÊú¨Ë≥™ÁâπÂæµÔºå‰∏¶Â±ïÁ§∫‰∫ÜË∑®‰∏çÂêåÈÅãÂãï‰ªªÂãôÁöÑÊ≥õÂåñÊÄß„ÄÇ

##### **Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models**
2407.11470v1 by Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun

In recent years, researchers have proposed numerous benchmarks to evaluate
the impressive coding capabilities of large language models (LLMs). However,
existing benchmarks primarily focus on assessing the correctness of code
generated by LLMs, while neglecting other critical dimensions that also
significantly impact code quality. Therefore, this paper proposes the RACE
benchmark, which comprehensively evaluates the quality of code generated by
LLMs across 4 dimensions: Readability, mAintainability, Correctness, and
Efficiency. Specifically, considering the demand-dependent nature of dimensions
beyond correctness, we design various types of user requirements for each
dimension to assess the model's ability to generate correct code that also
meets user demands. We evaluate 18 representative LLMs on RACE and find that:
1) the current LLMs' ability to generate high-quality code on demand does not
yet meet the requirements of software development; 2) readability serves as a
critical indicator of the overall quality of generated code; 3) most LLMs
exhibit an inherent preference for specific coding style. These findings can
help researchers gain a deeper understanding of the coding capabilities of
current LLMs and shed light on future directions for model improvement.

ÊëòË¶ÅÔºöËøëÂπ¥Êù•ÔºåÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜËÆ∏Â§öÂü∫ÂáÜÊù•ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑÁºñÁ†ÅËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂü∫ÂáÜ‰∏ªË¶Å‰æßÈáç‰∫éËØÑ‰º∞ LLM ÁîüÊàêÁöÑ‰ª£Á†ÅÁöÑÊ≠£Á°ÆÊÄßÔºåËÄåÂøΩÁï•‰∫ÜÂÖ∂‰ªñ‰πüÊòæËëóÂΩ±Âìç‰ª£Á†ÅË¥®ÈáèÁöÑÂÖ≥ÈîÆÁª¥Â∫¶„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü RACE Âü∫ÂáÜÔºåÂÆÉÂÖ®Èù¢ËØÑ‰º∞‰∫Ü LLM ÁîüÊàêÁöÑ‰ª£Á†ÅÂú® 4 ‰∏™Áª¥Â∫¶‰∏äÁöÑË¥®ÈáèÔºöÂèØËØªÊÄß„ÄÅÂèØÁª¥Êä§ÊÄß„ÄÅÊ≠£Á°ÆÊÄßÂíåÊïàÁéá„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåËÄÉËôëÂà∞Ê≠£Á°ÆÊÄß‰πãÂ§ñÁöÑÁª¥Â∫¶ÂØπÈúÄÊ±ÇÁöÑ‰æùËµñÊÄßÔºåÊàë‰ª¨‰∏∫ÊØè‰∏™Áª¥Â∫¶ËÆæËÆ°‰∫Ü‰∏çÂêåÁ±ªÂûãÁöÑÁî®Êà∑ÈúÄÊ±ÇÔºå‰ª•ËØÑ‰º∞Ê®°ÂûãÁîüÊàêÊ≠£Á°Æ‰ª£Á†ÅÁöÑËÉΩÂäõÔºåÂêåÊó∂Êª°Ë∂≥Áî®Êà∑ÈúÄÊ±Ç„ÄÇÊàë‰ª¨Âú® RACE ‰∏äËØÑ‰º∞‰∫Ü 18 ‰∏™ÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑ LLMÔºåÂèëÁé∞Ôºö1ÔºâÂΩìÂâç LLM Ê†πÊçÆÈúÄÊ±ÇÁîüÊàêÈ´òË¥®Èáè‰ª£Á†ÅÁöÑËÉΩÂäõÂ∞öÊú™ËææÂà∞ËΩØ‰ª∂ÂºÄÂèëÁöÑË¶ÅÊ±ÇÔºõ2ÔºâÂèØËØªÊÄßÊòØÁîüÊàê‰ª£Á†ÅÊï¥‰ΩìË¥®ÈáèÁöÑÂÖ≥ÈîÆÊåáÊ†áÔºõ3ÔºâÂ§ßÂ§öÊï∞ LLM ÂØπÁâπÂÆöÁöÑÁºñÁ†ÅÈ£éÊ†ºË°®Áé∞Âá∫Âõ∫ÊúâÁöÑÂÅèÂ•Ω„ÄÇËøô‰∫õÂèëÁé∞ÂèØ‰ª•Â∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòÊõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ÂΩìÂâç LLM ÁöÑÁºñÁ†ÅËÉΩÂäõÔºåÂπ∂‰∏∫Ê®°ÂûãÊîπËøõÁöÑÊú™Êù•ÊñπÂêëÊèê‰æõÂêØÁ§∫„ÄÇ

##### **Investigating Imperceptibility of Adversarial Attacks on Tabular Data: An Empirical Analysis**
2407.11463v1 by Zhipeng He, Chun Ouyang, Laith Alzubaidi, Alistair Barros, Catarina Moreira

Adversarial attacks are a potential threat to machine learning models, as
they can cause the model to make incorrect predictions by introducing
imperceptible perturbations to the input data. While extensively studied in
unstructured data like images, their application to structured data like
tabular data presents unique challenges due to the heterogeneity and intricate
feature interdependencies of tabular data. Imperceptibility in tabular data
involves preserving data integrity while potentially causing misclassification,
underscoring the need for tailored imperceptibility criteria for tabular data.
However, there is currently a lack of standardised metrics for assessing
adversarial attacks specifically targeted at tabular data. To address this gap,
we derive a set of properties for evaluating the imperceptibility of
adversarial attacks on tabular data. These properties are defined to capture
seven perspectives of perturbed data: proximity to original inputs, sparsity of
alterations, deviation to datapoints in the original dataset, sensitivity of
altering sensitive features, immutability of perturbation, feasibility of
perturbed values and intricate feature interdepencies among tabular features.
Furthermore, we conduct both quantitative empirical evaluation and case-based
qualitative examples analysis for seven properties. The evaluation reveals a
trade-off between attack success and imperceptibility, particularly concerning
proximity, sensitivity, and deviation. Although no evaluated attacks can
achieve optimal effectiveness and imperceptibility simultaneously, unbounded
attacks prove to be more promised for tabular data in crafting imperceptible
adversarial examples. The study also highlights the limitation of evaluated
algorithms in controlling sparsity effectively. We suggest incorporating a
sparsity metric in future attack design to regulate the number of perturbed
features.

ÊëòË¶ÅÔºöÂ∞çÊäóÊîªÊìäÂ∞çÊ©üÂô®Â≠∏ÁøíÊ®°Âûã‰æÜË™™ÊòØ‰∏ÄÂÄãÊΩõÂú®ÁöÑÂ®ÅËÑÖÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÊúÉÈÄèÈÅéÂú®Ëº∏ÂÖ•Ë≥áÊñô‰∏≠ÂºïÂÖ•Èõ£‰ª•ÂØüË¶∫ÁöÑÊìæÂãïÔºåÂ∞éËá¥Ê®°ÂûãÂÅöÂá∫‰∏çÊ≠£Á¢∫ÁöÑÈ†êÊ∏¨„ÄÇÈõñÁÑ∂Âú®ÂΩ±ÂÉèÁ≠âÈùûÁµêÊßãÂåñË≥áÊñô‰∏≠Â∑≤Âª£Ê≥õÁ†îÁ©∂Ôºå‰ΩÜÁî±ÊñºË°®Ê†ºË≥áÊñôÁöÑÁï∞Ë≥™ÊÄßÂíåË§áÈõúÁöÑÁâπÊÄßÁõ∏‰∫í‰æùË≥¥ÊÄßÔºåÂ∞áÂÖ∂ÊáâÁî®ÊñºË°®Ê†ºË≥áÊñôÁ≠âÁµêÊßãÂåñË≥áÊñôÊúÉÂ∏∂‰æÜÁç®ÁâπÁöÑÊåëÊà∞„ÄÇË°®Ê†ºË≥áÊñô‰∏≠ÁöÑÈõ£‰ª•ÂØüË¶∫ÊÄßÊ∂âÂèäÂú®ÊΩõÂú®Â∞éËá¥ÈåØË™§ÂàÜÈ°ûÁöÑÂêåÊôÇÔºå‰øùÊåÅË≥áÊñôÁöÑÂÆåÊï¥ÊÄßÔºåÈÄôÂá∏È°Ø‰∫ÜÁÇ∫Ë°®Ê†ºË≥áÊñôÈáèË∫´ÊâìÈÄ†Èõ£‰ª•ÂØüË¶∫ÊÄßÊ®ôÊ∫ñÁöÑÂøÖË¶ÅÊÄß„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÈáùÂ∞çË°®Ê†ºË≥áÊñôÁöÑÂ∞çÊäóÊîªÊìäË©ï‰º∞Ê®ôÊ∫ñÂåñÊåáÊ®ô‰ªçÊúâ‰∏çË∂≥„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÊé®Â∞éÂá∫‰∏ÄÁµÑÁî®ÊñºË©ï‰º∞Ë°®Ê†ºË≥áÊñôÂ∞çÊäóÊîªÊìäÈõ£‰ª•ÂØüË¶∫ÊÄßÁöÑÂ±¨ÊÄß„ÄÇÈÄô‰∫õÂ±¨ÊÄßË¢´ÂÆöÁæ©ÁÇ∫ÊçïÊçâÊìæÂãïË≥áÊñôÁöÑ‰∏ÉÂÄãËßÄÈªûÔºöËàáÂéüÂßãËº∏ÂÖ•ÁöÑÊé•ËøëÁ®ãÂ∫¶„ÄÅËÆäÂãïÁöÑÁ®ÄÁñèÊÄß„ÄÅËàáÂéüÂßãË≥áÊñôÈõÜ‰∏≠Ë≥áÊñôÈªûÁöÑÂÅèÂ∑Æ„ÄÅËÆäÂãïÊïèÊÑüÁâπÊÄßÁöÑÊïèÊÑüÊÄß„ÄÅÊìæÂãïÁöÑ‰∏çËÆäÊÄß„ÄÅÊìæÂãïÂÄºÁöÑÂèØËÉΩÊÄßÔºå‰ª•ÂèäË°®Ê†ºÁâπÊÄß‰πãÈñìË§áÈõúÁöÑÁâπÊÄßÁõ∏‰∫í‰æùË≥¥ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞ç‰∏ÉÂÄãÂ±¨ÊÄßÈÄ≤Ë°åÂÆöÈáèÂØ¶Ë≠âË©ï‰º∞ÂíåÂü∫ÊñºÊ°à‰æãÁöÑÂÆöÊÄßÁØÑ‰æãÂàÜÊûê„ÄÇË©ï‰º∞ÁµêÊûúÊè≠Á§∫‰∫ÜÊîªÊìäÊàêÂäüËàáÈõ£‰ª•ÂØüË¶∫ÊÄß‰πãÈñìÁöÑÊ¨äË°°ÔºåÁâπÂà•ÊòØÈóúÊñºÊé•ËøëÂ∫¶„ÄÅÊïèÊÑüÊÄßÂíåÂÅèÂ∑Æ„ÄÇÂÑòÁÆ°Ê≤íÊúâË©ï‰º∞ÁöÑÊîªÊìäÂèØ‰ª•ÂêåÊôÇÈÅîÂà∞ÊúÄ‰Ω≥ÁöÑÊúâÊïàÊÄßÂíåÈõ£‰ª•ÂØüË¶∫ÊÄßÔºå‰ΩÜÁÑ°ÁïåÊîªÊìäË¢´Ë≠âÊòéÂú®Ë£Ω‰ΩúÈõ£‰ª•ÂØüË¶∫ÁöÑÂ∞çÊäóÁØÑ‰æãÊñπÈù¢Â∞çË°®Ê†ºË≥áÊñôÊõ¥ÊúâÂâçÊôØ„ÄÇÁ†îÁ©∂ÈÇÑÂº∑Ë™ø‰∫ÜË©ï‰º∞ÊºîÁÆóÊ≥ïÂú®ÊúâÊïàÊéßÂà∂Á®ÄÁñèÊÄßÊñπÈù¢ÁöÑÈôêÂà∂„ÄÇÊàëÂÄëÂª∫Ë≠∞Âú®Êú™‰æÜÁöÑÊîªÊìäË®≠Ë®à‰∏≠Á¥çÂÖ•Á®ÄÁñèÊÄßÊåáÊ®ôÔºå‰ª•Ë¶èÁØÑÊìæÂãïÁâπÊÄßÁöÑÊï∏Èáè„ÄÇ

##### **Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights**
2407.11449v1 by Shunqi Mao, Chaoyi Zhang, Hang Su, Hwanjun Song, Igor Shalyminov, Weidong Cai

Contextualized Image Captioning (CIC) evolves traditional image captioning
into a more complex domain, necessitating the ability for multimodal reasoning.
It aims to generate image captions given specific contextual information. This
paper further introduces a novel domain of Controllable Contextualized Image
Captioning (Ctrl-CIC). Unlike CIC, which solely relies on broad context,
Ctrl-CIC accentuates a user-defined highlight, compelling the model to tailor
captions that resonate with the highlighted aspects of the context. We present
two approaches, Prompting-based Controller (P-Ctrl) and Recalibration-based
Controller (R-Ctrl), to generate focused captions. P-Ctrl conditions the model
generation on highlight by prepending captions with highlight-driven prefixes,
whereas R-Ctrl tunes the model to selectively recalibrate the encoder
embeddings for highlighted tokens. Additionally, we design a GPT-4V empowered
evaluator to assess the quality of the controlled captions alongside standard
assessment methods. Extensive experimental results demonstrate the efficient
and effective controllability of our method, charting a new direction in
achieving user-adaptive image captioning. Code is available at
https://github.com/ShunqiM/Ctrl-CIC .

ÊëòË¶ÅÔºöËØ≠Â¢ÉÂõæÂÉèÊèèËø∞ (CIC) Â∞Ü‰º†ÁªüÂõæÂÉèÊèèËø∞ÊºîÂèò‰∏∫Êõ¥Â§çÊùÇÁöÑÈ¢ÜÂüüÔºåÈúÄË¶ÅÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑËÉΩÂäõ„ÄÇÂÆÉÊó®Âú®Ê†πÊçÆÁâπÂÆöÁöÑËØ≠Â¢É‰ø°ÊÅØÁîüÊàêÂõæÂÉèÊèèËø∞„ÄÇÊú¨ÊñáËøõ‰∏ÄÊ≠•‰ªãÁªç‰∫ÜÂèØÊéßËØ≠Â¢ÉÂõæÂÉèÊèèËø∞ (Ctrl-CIC) ÁöÑ‰∏Ä‰∏™Êñ∞È¢ÜÂüü„ÄÇ‰∏é‰ªÖ‰æùËµñÂπøÊ≥õËØ≠Â¢ÉÁöÑ CIC ‰∏çÂêåÔºåCtrl-CIC Âº∫Ë∞ÉÁî®Êà∑ÂÆö‰πâÁöÑÈáçÁÇπÔºåËø´‰ΩøÊ®°ÂûãÂÆöÂà∂‰∏éËØ≠Â¢É‰∏≠Á™ÅÂá∫ÊñπÈù¢‰∫ßÁîüÂÖ±È∏£ÁöÑÊèèËø∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊñπÊ≥ïÔºåÂü∫‰∫éÊèêÁ§∫ÁöÑÊéßÂà∂Âô® (P-Ctrl) ÂíåÂü∫‰∫éÈáçÊñ∞Ê†°ÂáÜÁöÑÊéßÂà∂Âô® (R-Ctrl)Ôºå‰ª•ÁîüÊàêÈáçÁÇπÊèèËø∞„ÄÇP-Ctrl ÈÄöËøá‰ΩøÁî®Á™ÅÂá∫È©±Âä®ÁöÑÂâçÁºÄÊù•Ê∑ªÂä†Á™ÅÂá∫ÊòæÁ§∫ÔºåÂØπÊ®°ÂûãÁîüÊàêËøõË°åÊù°‰ª∂ÂåñÔºåËÄå R-Ctrl Ë∞ÉÊï¥Ê®°Âûã‰ª•ÈÄâÊã©ÊÄßÂú∞ÈáçÊñ∞Ê†°ÂáÜÁ™ÅÂá∫Ê†áËÆ∞ÁöÑÁºñÁ†ÅÂô®ÂµåÂÖ•„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Áî± GPT-4V È©±Âä®ÁöÑËØÑ‰º∞Âô®Êù•ËØÑ‰º∞ÂèóÊéßÊèèËø∞ÁöÑË¥®Èáè‰ª•ÂèäÊ†áÂáÜËØÑ‰º∞ÊñπÊ≥ï„ÄÇÂπøÊ≥õÁöÑÂÆûÈ™åÁªìÊûúËØÅÊòé‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÁöÑÈ´òÊïàÂíåÊúâÊïàÂèØÊéßÊÄßÔºå‰∏∫ÂÆûÁé∞Áî®Êà∑Ëá™ÈÄÇÂ∫îÂõæÂÉèÊèèËø∞ÊåáÊòé‰∫Ü‰∏Ä‰∏™Êñ∞ÊñπÂêë„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/ShunqiM/Ctrl-CIC Ëé∑Âæó„ÄÇ

##### **EARN Fairness: Explaining, Asking, Reviewing and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders**
2407.11442v1 by Lin Luo, Yuri Nakao, Mathieu Chollet, Hiroya Inakoshi, Simone Stumpf

Numerous fairness metrics have been proposed and employed by artificial
intelligence (AI) experts to quantitatively measure bias and define fairness in
AI models. Recognizing the need to accommodate stakeholders' diverse fairness
understandings, efforts are underway to solicit their input. However, conveying
AI fairness metrics to stakeholders without AI expertise, capturing their
personal preferences, and seeking a collective consensus remain challenging and
underexplored. To bridge this gap, we propose a new framework, EARN Fairness,
which facilitates collective metric decisions among stakeholders without
requiring AI expertise. The framework features an adaptable interactive system
and a stakeholder-centered EARN Fairness process to Explain fairness metrics,
Ask stakeholders' personal metric preferences, Review metrics collectively, and
Negotiate a consensus on metric selection. To gather empirical results, we
applied the framework to a credit rating scenario and conducted a user study
involving 18 decision subjects without AI knowledge. We identify their personal
metric preferences and their acceptable level of unfairness in individual
sessions. Subsequently, we uncovered how they reached metric consensus in team
sessions. Our work shows that the EARN Fairness framework enables stakeholders
to express personal preferences and reach consensus, providing practical
guidance for implementing human-centered AI fairness in high-risk contexts.
Through this approach, we aim to harmonize fairness expectations of diverse
stakeholders, fostering more equitable and inclusive AI fairness.

ÊëòË¶ÅÔºö‰∫∫Â∑•Êô∫ËÉΩ (AI) Â∞àÂÆ∂Â∑≤ÊèêÂá∫‰∏¶Êé°Áî®Ë®±Â§öÂÖ¨Âπ≥ÊÄßÊåáÊ®ôÔºå‰ª•ÈáèÂåñË°°ÈáèÂÅèË¶ã‰∏¶ÂÆöÁæ© AI Ê®°Âûã‰∏≠ÁöÑÂÖ¨Âπ≥ÊÄß„ÄÇË™çË≠òÂà∞ÈúÄË¶ÅÈÅ©ÊáâÂà©ÂÆ≥Èóú‰øÇ‰∫∫Â∞çÂÖ¨Âπ≥ÊÄßÁöÑ‰∏çÂêåÁêÜËß£ÔºåÁõÆÂâçÊ≠£Âä™ÂäõÂæµÊ±Ç‰ªñÂÄëÁöÑÊÑèË¶ã„ÄÇÁÑ∂ËÄåÔºåÂêëÊ≤íÊúâ AI Â∞àÊ•≠Áü•Ë≠òÁöÑÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÂÇ≥ÈÅî AI ÂÖ¨Âπ≥ÊÄßÊåáÊ®ô„ÄÅÊéåÊè°‰ªñÂÄëÁöÑÂÄã‰∫∫ÂÅèÂ•ΩÔºå‰∏¶Â∞ãÊ±ÇÈõÜÈ´îÂÖ±Ë≠ò‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰∏îÂ∞öÊú™ÂÖÖÂàÜÊé¢Ë®é„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÈÄô‰∏ÄÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞ÁöÑÊû∂ÊßãÔºåEARN ÂÖ¨Âπ≥ÊÄßÔºåÂÆÉÂèØ‰ª•Âú®‰∏çÈúÄ AI Â∞àÊ•≠Áü•Ë≠òÁöÑÊÉÖÊ≥Å‰∏ãÔºå‰øÉÈÄ≤Âà©ÂÆ≥Èóú‰øÇ‰∫∫‰πãÈñìÁöÑÈõÜÈ´îÊåáÊ®ôÊ±∫Á≠ñ„ÄÇË©≤Êû∂ÊßãÂÖ∑ÊúâÈÅ©ÊáâÊÄß‰∫íÂãïÁ≥ªÁµ±Ôºå‰ª•Âèä‰ª•Âà©ÂÆ≥Èóú‰øÇ‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑ EARN ÂÖ¨Âπ≥ÊÄßÊµÅÁ®ãÔºåÁî®ÊñºË™™ÊòéÂÖ¨Âπ≥ÊÄßÊåáÊ®ô„ÄÅË©¢ÂïèÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÂÄã‰∫∫ÊåáÊ®ôÂÅèÂ•Ω„ÄÅÂÖ±ÂêåÊ™¢Ë¶ñÊåáÊ®ôÔºå‰∏¶ÂçîÂïÜÊåáÊ®ôÈÅ∏ÊìáÁöÑÂÖ±Ë≠ò„ÄÇÁÇ∫‰∫ÜÊî∂ÈõÜÂØ¶Ë≠âÁµêÊûúÔºåÊàëÂÄëÂ∞áÊ≠§Êû∂ÊßãÊáâÁî®Êñº‰ø°Áî®Ë©ïÂàÜÊÉÖÂ¢ÉÔºå‰∏¶ÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†Ö‰ΩøÁî®ËÄÖÁ†îÁ©∂ÔºåÂÖ∂‰∏≠ÂåÖÂê´ 18 ‰ΩçÊ≤íÊúâ AI Áü•Ë≠òÁöÑÊ±∫Á≠ñ‰∏ªÈ´î„ÄÇÊàëÂÄëÂú®ÂÄãÂà•ÊúÉË≠∞‰∏≠ÊâæÂá∫‰ªñÂÄëÁöÑÂÄã‰∫∫ÊåáÊ®ôÂÅèÂ•ΩÂíå‰ªñÂÄëÂèØÊé•ÂèóÁöÑ‰∏çÂÖ¨Âπ≥Á®ãÂ∫¶„ÄÇÈö®ÂæåÔºåÊàëÂÄëÊè≠Èú≤‰ªñÂÄëÂ¶Ç‰ΩïÂú®ÂúòÈöäÊúÉË≠∞‰∏≠ÈÅîÊàêÊåáÊ®ôÂÖ±Ë≠ò„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂È°ØÁ§∫ÔºåEARN ÂÖ¨Âπ≥ÊÄßÊû∂ÊßãËÆìÂà©ÂÆ≥Èóú‰øÇ‰∫∫ËÉΩÂ§†Ë°®ÈÅîÂÄã‰∫∫ÂÅèÂ•Ω‰∏¶ÈÅîÊàêÂÖ±Ë≠òÔºåÁÇ∫Âú®È´òÈ¢®Èö™Áí∞Â¢É‰∏≠ÂØ¶ÊñΩ‰ª•‰∫∫ÁÇ∫‰∏≠ÂøÉÁöÑ AI ÂÖ¨Âπ≥ÊÄßÊèê‰æõÂØ¶ÂãôÊåáÂ∞é„ÄÇÈÄèÈÅéÊ≠§ÊñπÊ≥ïÔºåÊàëÂÄëÊó®Âú®Ë™øÂíå‰∏çÂêåÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑÂÖ¨Âπ≥ÊÄßÊúüÊúõÔºå‰øÉÈÄ≤Êõ¥ÂÖ¨Âπ≥‰∏îÂåÖÂÆπÁöÑ AI ÂÖ¨Âπ≥ÊÄß„ÄÇ

##### **Repurformer: Transformers for Repurposing-Aware Molecule Generation**
2407.11439v1 by Changhun Lee, Gyumin Lee

Generating as diverse molecules as possible with desired properties is
crucial for drug discovery research, which invokes many approaches based on
deep generative models today. Despite recent advancements in these models,
particularly in variational autoencoders (VAEs), generative adversarial
networks (GANs), Transformers, and diffusion models, a significant challenge
known as \textit{the sample bias problem} remains. This problem occurs when
generated molecules targeting the same protein tend to be structurally similar,
reducing the diversity of generation. To address this, we propose leveraging
multi-hop relationships among proteins and compounds. Our model, Repurformer,
integrates bi-directional pretraining with Fast Fourier Transform (FFT) and
low-pass filtering (LPF) to capture complex interactions and generate diverse
molecules. A series of experiments on BindingDB dataset confirm that
Repurformer successfully creates substitutes for anchor compounds that resemble
positive compounds, increasing diversity between the anchor and generated
compounds.

ÊëòË¶ÅÔºöË¶ÅÁîüÊàêÂÖ∑ÊúâÊâÄÈúÄÂ±ûÊÄßÁöÑÂ∞ΩÂèØËÉΩÂ§öÊ®£ÂåñÁöÑÂàÜÂ≠êÂ∞çÊñºËó•Áâ©ÁôºÁèæÁ†îÁ©∂Ëá≥ÈóúÈáçË¶ÅÔºåÈÄôÈ†ÖÁ†îÁ©∂Êé°Áî®‰∫ÜË®±Â§öÂü∫ÊñºÊ∑±Â∫¶ÁîüÊàêÊ®°ÂûãÁöÑÊñπÊ≥ï„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊ®°ÂûãÊúÄËøëÊúâ‰∫ÜÈÄ≤Â±ïÔºåÁâπÂà•ÊòØÂú®ËÆäÂàÜËá™Á∑®Á¢ºÂô® (VAE)„ÄÅÁîüÊàêÂ∞çÊäóÁ∂≤Ë∑Ø (GAN)„ÄÅTransformerÂíåÊì¥Êï£Ê®°ÂûãÊñπÈù¢Ôºå‰ΩÜ‰ªçÂ≠òÂú®‰∏ÄÂÄãÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÁ®±ÁÇ∫„ÄåÊ®£Êú¨ÂÅèÂ∑ÆÂïèÈ°å„Äç„ÄÇÁï∂ÈáùÂ∞çÂêå‰∏ÄËõãÁôΩË≥™Áî¢ÁîüÁöÑÂàÜÂ≠êÂú®ÁµêÊßã‰∏äÂÇæÂêëÊñºÁõ∏‰ººÊôÇÔºåÂ∞±ÊúÉÁôºÁîüÈÄôÂÄãÂïèÈ°åÔºåÈÄ≤ËÄåÈôç‰ΩéÁîüÊàêÁöÑÊ®£Êú¨Â§öÊ®£ÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂª∫Ë≠∞Âà©Áî®ËõãÁôΩË≥™ÂíåÂåñÂêàÁâ©‰πãÈñìÁöÑÂ§öË∑≥Èóú‰øÇ„ÄÇÊàëÂÄëÁöÑÊ®°Âûã Repurformer Â∞áÈõôÂêëÈ†êË®ìÁ∑¥ËàáÂø´ÈÄüÂÇÖÁ´ãËëâËΩâÊèõ (FFT) Âíå‰ΩéÈÄöÊøæÊ≥¢ (LPF) Êï¥ÂêàÂú®‰∏ÄËµ∑Ôºå‰ª•ÊçïÊçâË§áÈõúÁöÑ‰∫§‰∫í‰ΩúÁî®‰∏¶ÁîüÊàêÂ§öÊ®£ÂåñÁöÑÂàÜÂ≠ê„ÄÇ‰∏ÄÁ≥ªÂàóÈáùÂ∞ç BindingDB Ë≥áÊñôÈõÜÁöÑÂØ¶È©óË≠âÂØ¶ÔºåRepurformer ÊàêÂäüÂú∞ÁÇ∫Èå®ÂÆöÂåñÂêàÁâ©ÂâµÈÄ†‰∫ÜÊõø‰ª£Áâ©ÔºåÈÄô‰∫õÊõø‰ª£Áâ©È°û‰ººÊñºÊ≠£ÂêëÂåñÂêàÁâ©ÔºåÂ¢ûÂä†‰∫ÜÈå®ÂÆöÂåñÂêàÁâ©ÂíåÁîüÊàêÂåñÂêàÁâ©‰πãÈñìÁöÑÂ§öÊ®£ÊÄß„ÄÇ

##### **Trust No Bot: Discovering Personal Disclosures in Human-LLM Conversations in the Wild**
2407.11438v1 by Niloofar Mireshghallah, Maria Antoniak, Yash More, Yejin Choi, Golnoosh Farnadi

Measuring personal disclosures made in human-chatbot interactions can provide
a better understanding of users' AI literacy and facilitate privacy research
for large language models (LLMs). We run an extensive, fine-grained analysis on
the personal disclosures made by real users to commercial GPT models,
investigating the leakage of personally identifiable and sensitive information.
To understand the contexts in which users disclose to chatbots, we develop a
taxonomy of tasks and sensitive topics, based on qualitative and quantitative
analysis of naturally occurring conversations. We discuss these potential
privacy harms and observe that: (1) personally identifiable information (PII)
appears in unexpected contexts such as in translation or code editing (48% and
16% of the time, respectively) and (2) PII detection alone is insufficient to
capture the sensitive topics that are common in human-chatbot interactions,
such as detailed sexual preferences or specific drug use habits. We believe
that these high disclosure rates are of significant importance for researchers
and data curators, and we call for the design of appropriate nudging mechanisms
to help users moderate their interactions.

ÊëòË¶ÅÔºöÈÄèÈÅéË°°Èáè‰∫∫È°ûËàáËÅäÂ§©Ê©üÂô®‰∫∫‰∫íÂãï‰∏≠ÊâÄÂÅöÁöÑÂÄã‰∫∫Êè≠Èú≤ÔºåÂèØ‰ª•Êõ¥‰∫ÜËß£‰ΩøÁî®ËÄÖÁöÑ AI Á¥†È§äÔºå‰∏¶‰øÉÈÄ≤Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈö±ÁßÅÁ†îÁ©∂„ÄÇÊàëÂÄëÂ∞çÁúüÂØ¶‰ΩøÁî®ËÄÖÂ∞çÂïÜÊ•≠ GPT Ê®°ÂûãÊâÄÂÅöÁöÑÂÄã‰∫∫Êè≠Èú≤ÈÄ≤Ë°åÂª£Ê≥õÁöÑÁ¥∞Á∑ªÂàÜÊûêÔºåË™øÊü•ÂÄã‰∫∫ÂèØË≠òÂà•ÂíåÊïèÊÑüË≥áË®äÁöÑÊ¥©Êºè„ÄÇÁÇ∫‰∫Ü‰∫ÜËß£‰ΩøÁî®ËÄÖÂú®Âì™‰∫õÊÉÖÊ≥Å‰∏ãÂ∞çËÅäÂ§©Ê©üÂô®‰∫∫Êè≠Èú≤Ë≥áË®äÔºåÊàëÂÄëÊ†πÊìöËá™ÁÑ∂ÁôºÁîüÁöÑÂ∞çË©±ÈÄ≤Ë°åÂÆöÊÄßÂíåÂÆöÈáèÂàÜÊûêÔºåÂà∂ÂÆö‰∫Ü‰∏ÄÂ•ó‰ªªÂãôÂíåÊïèÊÑü‰∏ªÈ°åÂàÜÈ°ûÊ≥ï„ÄÇÊàëÂÄëË®éË´ñÈÄô‰∫õÊΩõÂú®ÁöÑÈö±ÁßÅÂç±ÂÆ≥Ôºå‰∏¶ËßÄÂØüÂà∞Ôºö(1) ÂÄã‰∫∫ÂèØË≠òÂà•Ë≥áË®ä (PII) Âá∫ÁèæÂú®ÊÑèÊÉ≥‰∏çÂà∞ÁöÑÊÉÖÊ≥Å‰∏≠Ôºå‰æãÂ¶ÇÁøªË≠ØÊàñÁ®ãÂºèÁ¢ºÁ∑®ËºØ‰∏≠ (ÂàÜÂà•ÁÇ∫ 48% Âíå 16% ÁöÑÊôÇÈñì)Ôºå‰ª•Âèä (2) ÂÉÖ PII ÂÅµÊ∏¨‰∏çË∂≥‰ª•ÊéåÊè°Âú®‰∫∫È°ûËàáËÅäÂ§©Ê©üÂô®‰∫∫‰∫íÂãï‰∏≠Â∏∏Ë¶ãÁöÑÊïèÊÑü‰∏ªÈ°åÔºå‰æãÂ¶ÇË©≥Á¥∞ÁöÑÊÄßÂÅèÂ•ΩÊàñÁâπÂÆöÁöÑËó•Áâ©‰ΩøÁî®ÁøíÊÖ£„ÄÇÊàëÂÄëÁõ∏‰ø°ÈÄô‰∫õÈ´òÊè≠Èú≤ÁéáÂ∞çÁ†îÁ©∂‰∫∫Âì°ÂíåË≥áÊñôÁÆ°ÁêÜÂì°‰æÜË™™ÈùûÂ∏∏ÈáçË¶ÅÔºåÊàëÂÄëÂëºÁ±≤Ë®≠Ë®àÈÅ©Áï∂ÁöÑÊé®ÂãïÊ©üÂà∂‰æÜÂπ´Âä©‰ΩøÁî®ËÄÖË™øÁØÄ‰ªñÂÄëÁöÑ‰∫íÂãï„ÄÇ

##### **Generally-Occurring Model Change for Robust Counterfactual Explanations**
2407.11426v1 by Ao Xu, Tieru Wu

With the increasing impact of algorithmic decision-making on human lives, the
interpretability of models has become a critical issue in machine learning.
Counterfactual explanation is an important method in the field of interpretable
machine learning, which can not only help users understand why machine learning
models make specific decisions, but also help users understand how to change
these decisions. Naturally, it is an important task to study the robustness of
counterfactual explanation generation algorithms to model changes. Previous
literature has proposed the concept of Naturally-Occurring Model Change, which
has given us a deeper understanding of robustness to model change. In this
paper, we first further generalize the concept of Naturally-Occurring Model
Change, proposing a more general concept of model parameter changes,
Generally-Occurring Model Change, which has a wider range of applicability. We
also prove the corresponding probabilistic guarantees. In addition, we consider
a more specific problem, data set perturbation, and give relevant theoretical
results by combining optimization theory.

ÊëòË¶ÅÔºöÈö®ËëóÊºîÁÆóÊ≥ïÊ±∫Á≠ñÂ∞ç‰∫∫È°ûÁîüÊ¥ªÁî¢ÁîüË∂ä‰æÜË∂äÂ§ßÁöÑÂΩ±ÈüøÔºåÊ®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÂ∑≤ÊàêÁÇ∫Ê©üÂô®Â≠∏Áøí‰∏≠ÁöÑ‰∏ÄÂÄãÈóúÈçµÂïèÈ°å„ÄÇÂèç‰∫ãÂØ¶Ëß£ÈáãÊòØÂèØËß£ÈáãÊ©üÂô®Â≠∏ÁøíÈ†òÂüü‰∏≠ÁöÑ‰∏ÄÁ®ÆÈáçË¶ÅÊñπÊ≥ïÔºåÂÆÉ‰∏çÂÉÖÂèØ‰ª•Âπ´Âä©‰ΩøÁî®ËÄÖ‰∫ÜËß£Ê©üÂô®Â≠∏ÁøíÊ®°ÂûãÂÅöÂá∫ÁâπÂÆöÊ±∫Á≠ñÁöÑÂéüÂõ†ÔºåÈÇÑÂèØ‰ª•Âπ´Âä©‰ΩøÁî®ËÄÖ‰∫ÜËß£Â¶Ç‰ΩïÊîπËÆäÈÄô‰∫õÊ±∫Á≠ñ„ÄÇËá™ÁÑ∂Âú∞ÔºåÁ†îÁ©∂Âèç‰∫ãÂØ¶Ëß£ÈáãÁîüÊàêÊºîÁÆóÊ≥ïÂ∞çÊ®°ÂûãËÆäÂåñÁöÑÁ©©ÂÅ•ÊÄßÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÁöÑ‰ªªÂãô„ÄÇÂÖàÂâçÁöÑÊñáÁçªÊèêÂá∫‰∫ÜËá™ÁÑ∂ÁôºÁîüÁöÑÊ®°ÂûãËÆäÂåñÁöÑÊ¶ÇÂøµÔºåÈÄôËÆìÊàëÂÄëÂ∞çÊ®°ÂûãËÆäÂåñÁöÑÁ©©ÂÅ•ÊÄßÊúâ‰∫ÜÊõ¥Ê∑±ÂÖ•ÁöÑ‰∫ÜËß£„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÈÄ≤‰∏ÄÊ≠•Ê¶ÇÊã¨Ëá™ÁÑ∂ÁôºÁîüÁöÑÊ®°ÂûãËÆäÂåñÁöÑÊ¶ÇÂøµÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊõ¥ÈÄöÁî®ÁöÑÊ®°ÂûãÂèÉÊï∏ËÆäÂåñÁöÑÊ¶ÇÂøµÔºåÂç≥ÊôÆÈÅçÁôºÁîüÁöÑÊ®°ÂûãËÆäÂåñÔºåÂÆÉÂÖ∑ÊúâÊõ¥Âª£Ê≥õÁöÑÈÅ©Áî®ÊÄß„ÄÇÊàëÂÄë‰πüË≠âÊòé‰∫ÜÁõ∏ÊáâÁöÑÊ©üÁéá‰øùË≠â„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëËÄÉÊÖÆ‰∫Ü‰∏ÄÂÄãÊõ¥ÂÖ∑È´îÁöÑÂïèÈ°åÔºåÂç≥Ë≥áÊñôÈõÜÊìæÂãïÔºå‰∏¶ÈÄèÈÅéÁµêÂêàÊúÄ‰Ω≥ÂåñÁêÜË´ñÁµ¶Âá∫Áõ∏ÈóúÁöÑÁêÜË´ñÁµêÊûú„ÄÇ

##### **States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly**
2407.11421v1 by Junhao Chen, Shengding Hu, Zhiyuan Liu, Maosong Sun

Large Language Models (LLMs) exhibit various emergent abilities. Among these
abilities, some might reveal the internal working mechanisms of models. In this
paper, we uncover a novel emergent capability in models: the intrinsic ability
to perform extended sequences of calculations without relying on
chain-of-thought step-by-step solutions. Remarkably, the most advanced models
can directly output the results of two-digit number additions with lengths
extending up to 15 addends. We hypothesize that the model emerges Implicit
Discrete State Representations (IDSRs) within its hidden states and performs
symbolic calculations internally. To test this hypothesis, we design a sequence
of experiments that look into the hidden states. Specifically, we first confirm
that IDSRs exist. Then, we provide interesting observations about the formation
of IDSRs from layer, digit, and sequence perspectives. Finally, we confirm that
models indeed use IDSRs to produce the final answers. However, we also discover
that these state representations are far from lossless in current open-sourced
models, leading to inaccuracies in their final performance. Our work presents a
novel exploration of LLMs' symbolic calculation abilities and the underlying
mechanisms.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â±ïÁèæÂá∫ÂêÑÁ®ÆÊñ∞ËààÁöÑËÉΩÂäõ„ÄÇÂú®ÈÄô‰∫õËÉΩÂäõ‰∏≠ÔºåÊúâ‰∫õÂèØËÉΩÊè≠Á§∫‰∫ÜÊ®°ÂûãÁöÑÂÖßÈÉ®Â∑•‰ΩúÊ©üÂà∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÁôºÁèæ‰∫ÜÊ®°Âûã‰∏≠‰∏ÄÁ®ÆÊñ∞ËààÁöÑÊñ∞ËÉΩÂäõÔºöÂú®‰∏ç‰æùË≥¥ÊñºÈÄêÊ≠•ÁöÑÊÄùÁ∂≠ÈèàËß£Ê±∫ÊñπÊ°àÁöÑÊÉÖÊ≥Å‰∏ãÂü∑Ë°åÊì¥Â±ïÁöÑË®àÁÆóÂ∫èÂàóÁöÑÂÖßÂú®ËÉΩÂäõ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊúÄÂÖàÈÄ≤ÁöÑÊ®°ÂûãÂèØ‰ª•Áõ¥Êé•Ëº∏Âá∫ÂÖ©‰ΩçÊï∏Âä†Ê≥ïÁöÑÁµêÊûúÔºåÈï∑Â∫¶Èï∑ÈÅî 15 ÂÄãÂä†Êï∏„ÄÇÊàëÂÄëÂÅáË®≠Ê®°ÂûãÂú®ÂÖ∂Èö±ËóèÁãÄÊÖã‰∏≠Âá∫Áèæ‰∫ÜÈö±Âê´Èõ¢Êï£ÁãÄÊÖãË°®Á§∫ (IDSR)Ôºå‰∏¶Âú®ÂÖßÈÉ®Âü∑Ë°åÁ¨¶ËôüË®àÁÆó„ÄÇÁÇ∫‰∫ÜÈ©óË≠âÈÄôÂÄãÂÅáË®≠ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ≥ªÂàóÂØ¶È©ó‰æÜÁ†îÁ©∂Èö±ËóèÁãÄÊÖã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÁ¢∫Ë™ç IDSR Á¢∫ÂØ¶Â≠òÂú®„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊèê‰æõ‰∫ÜÈóúÊñºÂæûÂ±§„ÄÅÊï∏Â≠óÂíåÂ∫èÂàóËßíÂ∫¶ÂΩ¢Êàê IDSR ÁöÑÊúâË∂£ËßÄÂØü„ÄÇÊúÄÂæåÔºåÊàëÂÄëÁ¢∫Ë™çÊ®°ÂûãÁ¢∫ÂØ¶‰ΩøÁî® IDSR ‰æÜÁî¢ÁîüÊúÄÁµÇÁ≠îÊ°à„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄë‰πüÁôºÁèæÈÄô‰∫õÁãÄÊÖãË°®Á§∫ÈÅ†ÈùûÁï∂ÂâçÈñãÊ∫êÊ®°Âûã‰∏≠ÁöÑÁÑ°ÊêçÂ§±ÔºåÂ∞éËá¥ÂÖ∂ÊúÄÁµÇÊÄßËÉΩ‰∏çÊ∫ñÁ¢∫„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Â±ïÁ§∫‰∫ÜÂ∞ç LLM ÁöÑÁ¨¶ËôüË®àÁÆóËÉΩÂäõÂíåÂ∫ïÂ±§Ê©üÂà∂ÁöÑÊé¢Á¥¢„ÄÇ

##### **LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data**
2407.11418v1 by Liana Patel, Siddharth Jha, Carlos Guestrin, Matei Zaharia

The semantic capabilities of language models (LMs) have the potential to
enable rich analytics and reasoning over vast knowledge corpora. Unfortunately,
existing systems lack high-level abstractions to perform semantic queries at
scale. We introduce semantic operators, a declarative programming interface
that extends the relational model with composable AI-based operations for
semantic queries over datasets (e.g., sorting or aggregating records using
natural language criteria). Each operator can be implemented and optimized in
multiple ways, opening a rich space for execution plans similar to relational
operators. We implement our operators and several optimizations for them in
LOTUS, an open-source query engine with a Pandas-like API.
  We demonstrate LOTUS' effectiveness across a series of real applications,
including fact-checking, extreme multi-label classification, and search. We
find that LOTUS' programming model is highly expressive, capturing
state-of-the-art query pipelines with low development overhead. Specifically,
on the FEVER dataset, LOTUS' programs can reproduce FacTool, a recent
state-of-the-art fact-checking pipeline, in few lines of code, and implement a
new pipeline that improves accuracy by $9.5\%$, while offering $7-34\times$
lower execution time. In the extreme multi-label classification task on the
BioDEX dataset, LOTUS reproduces state-of-the art result quality with its join
operator, while providing an efficient algorithm that runs $800\times$ faster
than a naive join. In the search and ranking application, LOTUS allows a simple
composition of operators to achieve $5.9 - 49.4\%$ higher nDCG@10 than the
vanilla retriever and re-ranker, while also providing query efficiency, with
$1.67 - 10\times$ lower execution time than LM-based ranking methods used by
prior works. LOTUS is publicly available at
https://github.com/stanford-futuredata/lotus.

ÊëòË¶ÅÔºö<paragraph>Ë™ûË®ÄÊ®°Âûã (LM) ÁöÑË™ûÁæ©ÂäüËÉΩÊúâÊΩõÂäõËÉΩÈáùÂ∞çÈæêÂ§ßÁöÑÁü•Ë≠òË≥áÊñôÂ∫´ÈÄ≤Ë°åË±êÂØåÁöÑÂàÜÊûêÂíåÊé®ÁêÜ„ÄÇ‰∏çÂπ∏ÁöÑÊòØÔºåÁèæÊúâÁöÑÁ≥ªÁµ±Áº∫‰πèÈ´òÈöéÊäΩË±°ÔºåÁÑ°Ê≥ïÂ§ßË¶èÊ®°Âü∑Ë°åË™ûÁæ©Êü•Ë©¢„ÄÇÊàëÂÄëÂºïÂÖ•‰∫ÜË™ûÁæ©ÈÅãÁÆóÂ≠êÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂÆ£ÂëäÂºèÁ®ãÂºèË®≠Ë®à‰ªãÈù¢ÔºåÂÆÉÊì¥ÂÖÖ‰∫ÜÈóú‰øÇÊ®°ÂûãÔºå‰∏¶ÈÄèÈÅéË™ûÁæ©Êü•Ë©¢Ë≥áÊñôÈõÜÔºà‰æãÂ¶ÇÔºå‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄÊ∫ñÂâáÂ∞çË®òÈåÑÈÄ≤Ë°åÊéíÂ∫èÊàñÂΩôÁ∏ΩÔºâÁöÑ AI ÁÇ∫Âü∫Á§éÈÅãÁÆóÈÄ≤Ë°åÁµÑÂêà„ÄÇÊØèÂÄãÈÅãÁÆóÂ≠êÈÉΩÂèØ‰ª•ÈÄèÈÅéÂ§öÁ®ÆÊñπÂºèÂØ¶‰ΩúÂíåÊúÄ‰Ω≥ÂåñÔºåÁÇ∫È°û‰ººÊñºÈóú‰øÇÈÅãÁÆóÂ≠êÁöÑÂü∑Ë°åË®àÁï´ÈñãÂïü‰∫ÜË±êÂØåÁöÑÁ©∫Èñì„ÄÇÊàëÂÄëÂú® LOTUS ‰∏≠ÂØ¶‰Ωú‰∫ÜÊàëÂÄëÁöÑÈÅãÁÆóÂ≠ê‰ª•ÂèäÂÆÉÂÄëÁöÑËã•Âπ≤ÊúÄ‰Ω≥ÂåñÔºåLOTUS ÊòØ‰∏ÄÂÄãÈñãÊ∫êÊü•Ë©¢ÂºïÊìéÔºåÂÖ∑ÊúâÈ°û‰ºº Pandas ÁöÑ API„ÄÇ
ÊàëÂÄëÈÄèÈÅé‰∏ÄÁ≥ªÂàóÂØ¶ÈöõÊáâÁî®Â±ïÁ§∫‰∫Ü LOTUS ÁöÑÊïàËÉΩÔºåÂåÖÊã¨‰∫ãÂØ¶Êü•Ê†∏„ÄÅÊ•µÁ´ØÂ§öÊ®ôÁ±§ÂàÜÈ°ûÂíåÊêúÂ∞ã„ÄÇÊàëÂÄëÁôºÁèæ LOTUS ÁöÑÁ®ãÂºèË®≠Ë®àÊ®°ÂûãÊ•µÂÖ∑Ë°®ÁèæÂäõÔºåÂèØ‰ª•Êì∑ÂèñÊúÄÂÖàÈÄ≤ÁöÑÊü•Ë©¢ÁÆ°Á∑öÔºå‰∏îÈñãÁôºÊàêÊú¨‰Ωé„ÄÇÁâπÂà•ÊòØÂú® FEVER Ë≥áÊñôÈõÜ‰∏äÔºåLOTUS ÁöÑÁ®ãÂºèÂèØ‰ª•Áî®ÂπæË°åÁ®ãÂºèÁ¢ºË§áË£Ω FacToolÔºåÈÄôÊòØ‰∏ÄÂÄãÊúÄËøëÊúÄÂÖàÈÄ≤ÁöÑ‰∫ãÂØ¶Êü•Ê†∏ÁÆ°Á∑öÔºå‰∏¶ÂØ¶‰Ωú‰∏ÄÂÄãÊñ∞ÁöÑÁÆ°Á∑öÔºåÂ∞áÊ∫ñÁ¢∫ÁéáÊèêÈ´ò‰∫Ü 9.5%ÔºåÂêåÊôÇÂü∑Ë°åÊôÇÈñìÈôç‰Ωé‰∫Ü 7-34 ÂÄç„ÄÇÂú® BioDEX Ë≥áÊñôÈõÜ‰∏äÁöÑÊ•µÁ´ØÂ§öÊ®ôÁ±§ÂàÜÈ°û‰ªªÂãô‰∏≠ÔºåLOTUS ‰ΩøÁî®ÂÖ∂ËÅØÁµêÈÅãÁÆóÂ≠êË§áË£Ω‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûúÂìÅË≥™ÔºåÂêåÊôÇÊèê‰æõ‰∫Ü‰∏ÄÂÄãÈ´òÊïàÁöÑÊºîÁÆóÊ≥ïÔºåÂü∑Ë°åÈÄüÂ∫¶ÊØîÂñÆÁ¥îËÅØÁµêÂø´‰∫Ü 800 ÂÄç„ÄÇÂú®ÊêúÂ∞ãÂíåÊéíÂêçÊáâÁî®Á®ãÂºè‰∏≠ÔºåLOTUS ÂÖÅË®±Á∞°ÂñÆÂú∞ÁµÑÂêàÈÅãÁÆóÂ≠êÔºå‰ª•ÂØ¶ÁèæÊØîÈ¶ôËçâÊ™¢Á¥¢Âô®ÂíåÈáçÊñ∞ÊéíÂêçÂô®È´ò 5.9 - 49.4% ÁöÑ nDCG@10ÔºåÂêåÊôÇ‰πüÊèê‰æõÊü•Ë©¢ÊïàÁéáÔºåÂü∑Ë°åÊôÇÈñìÊØîÂÖàÂâçÂ∑•‰Ωú‰ΩøÁî®ÁöÑÂü∫Êñº LM ÁöÑÊéíÂêçÊñπÊ≥ï‰Ωé 1.67 - 10 ÂÄç„ÄÇLOTUS Â∑≤ÂÖ¨ÈñãÊñº https://github.com/stanford-futuredata/lotus„ÄÇ</paragraph>

##### **SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions**
2407.11417v1 by Shicheng Liu, Sina J. Semnani, Harold Triedman, Jialiang Xu, Isaac Dan Zhao, Monica S. Lam

Recent work integrating Large Language Models (LLMs) has led to significant
improvements in the Knowledge Base Question Answering (KBQA) task. However, we
posit that existing KBQA datasets that either have simple questions, use
synthetically generated logical forms, or are based on small knowledge base
(KB) schemas, do not capture the true complexity of KBQA tasks.
  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA
dataset collected from forum discussions on Wikidata's "Request a Query" forum
with 320 decontextualized question-SPARQL pairs. Much more complex than
existing datasets, SPINACH calls for strong KBQA systems that do not rely on
training data to learn the KB schema, but can dynamically explore large and
often incomplete schemas and reason about them.
  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach
that mimics how a human expert would write SPARQLs for such challenging
questions. Experiments on existing datasets show SPINACH's capability in KBQA,
achieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10
datasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%
of the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH
dataset, SPINACH agent outperforms all baselines, including the best
GPT-4-based KBQA agent, by 38.1% in F1.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÊï¥ÂêàÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÂ∑•‰ΩúÂ∑≤Â§ßÂπÖÊîπÂñÑÁü•ËØÜÂ∫ìÈóÆÁ≠î (KBQA) ‰ªªÂä°„ÄÇÁÑ∂ËÄåÔºåÊàë‰ª¨ËÆ§‰∏∫Áé∞ÊúâÁöÑ KBQA Êï∞ÊçÆÈõÜË¶Å‰πàÊúâÁÆÄÂçïÁöÑÈóÆÈ¢òÔºåË¶Å‰πà‰ΩøÁî®ÂêàÊàêÁîüÊàêÁöÑÈÄªËæëÂΩ¢ÂºèÔºåÊàñËÄÖÂü∫‰∫éÂ∞èÂûãÁü•ËØÜÂ∫ì (KB) Êû∂ÊûÑÔºåÂπ∂Êú™ÊçïÊçâÂà∞ KBQA ‰ªªÂä°ÁöÑÁúüÊ≠£Â§çÊùÇÊÄß„ÄÇ
‰∏∫Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü SPINACH Êï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™‰ªé Wikidata ÁöÑ„ÄåËØ∑Ê±ÇÊü•ËØ¢„ÄçËÆ∫Âùõ‰∏äÁöÑËÆ∫ÂùõËÆ®ËÆ∫‰∏≠Êî∂ÈõÜÁöÑ‰∏ìÂÆ∂Ê≥®Èáä KBQA Êï∞ÊçÆÈõÜÔºåÂÖ∂‰∏≠Êúâ 320 ‰∏™ÂéªËØ≠Â¢ÉÂåñÁöÑ question-SPARQL ÂØπ„ÄÇSPINACH ÊØîÁé∞ÊúâÊï∞ÊçÆÈõÜÂ§çÊùÇÂæóÂ§öÔºåÈúÄË¶ÅÂº∫Â§ßÁöÑ KBQA Á≥ªÁªüÔºåËøô‰∫õÁ≥ªÁªü‰∏ç‰æùËµñËÆ≠ÁªÉÊï∞ÊçÆÊù•Â≠¶‰π† KB Êû∂ÊûÑÔºå‰ΩÜÂèØ‰ª•Âä®ÊÄÅÊé¢Á¥¢Â§ßÂûã‰∏îÈÄöÂ∏∏‰∏çÂÆåÊï¥ÁöÑÊû∂ÊûÑÂπ∂ÂØπÂÖ∂ËøõË°åÊé®ÁêÜ„ÄÇ
Èô§‰∫ÜÊï∞ÊçÆÈõÜ‰πãÂ§ñÔºåÊàë‰ª¨ËøòÂºïÂÖ•‰∫Ü SPINACH agentÔºåËøôÊòØ‰∏ÄÁßçÊñ∞ÁöÑ KBQA ÊñπÊ≥ïÔºåÂÆÉÊ®°‰ªø‰∫∫Á±ª‰∏ìÂÆ∂Â¶Ç‰Ωï‰∏∫Â¶ÇÊ≠§ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈóÆÈ¢òÁºñÂÜô SPARQL„ÄÇÁé∞ÊúâÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åÊòæÁ§∫‰∫Ü SPINACH Âú® KBQA ‰∏≠ÁöÑËÉΩÂäõÔºåÂú® QALD-7„ÄÅQALD-9 Plus Âíå QALD-10 Êï∞ÊçÆÈõÜ‰∏äÂàÜÂà´‰ª• F1 ÂàÜÂà´ÊèêÈ´ò‰∫Ü 30.1%„ÄÅ27.0% Âíå 10.0%ÔºåÂπ∂‰∏îÂú® WikiWebQuestions ‰∏äËææÂà∞ÂæÆË∞ÉÁöÑ LLaMA SOTA Ê®°ÂûãÁöÑ 1.6%„ÄÇÂú®Êàë‰ª¨ÁöÑÊñ∞ SPINACH Êï∞ÊçÆÈõÜ‰∏äÔºåSPINACH agent Âú® F1 ‰∏ä‰ºò‰∫éÊâÄÊúâÂü∫ÂáÜÔºåÂåÖÊã¨Âü∫‰∫é GPT-4 ÁöÑÊúÄ‰Ω≥ KBQA agentÔºåÊèêÈ´ò‰∫Ü 38.1%„ÄÇ</paragraph>

##### **Representation Bias in Political Sample Simulations with Large Language Models**
2407.11409v1 by Weihong Qi, Hanjia Lyu, Jiebo Luo

This study seeks to identify and quantify biases in simulating political
samples with Large Language Models, specifically focusing on vote choice and
public opinion. Using the GPT-3.5-Turbo model, we leverage data from the
American National Election Studies, German Longitudinal Election Study, Zuobiao
Dataset, and China Family Panel Studies to simulate voting behaviors and public
opinions. This methodology enables us to examine three types of representation
bias: disparities based on the the country's language, demographic groups, and
political regime types. The findings reveal that simulation performance is
generally better for vote choice than for public opinions, more accurate in
English-speaking countries, more effective in bipartisan systems than in
multi-partisan systems, and stronger in democratic settings than in
authoritarian regimes. These results contribute to enhancing our understanding
and developing strategies to mitigate biases in AI applications within the
field of computational social science.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êó®Âú®Ë≠òÂà•ÂíåÈáèÂåñ‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊ®°Êì¨ÊîøÊ≤ªÊ®£Êú¨‰∏≠ÁöÑÂÅèÂ∑ÆÔºåÁâπÂà•ÈóúÊ≥®ÊäïÁ•®ÈÅ∏ÊìáÂíåÊ∞ëÊÑè„ÄÇ‰ΩøÁî® GPT-3.5-Turbo Ê®°ÂûãÔºåÊàëÂÄëÂà©Áî®ÁæéÂúãÂúãÂÆ∂ÈÅ∏ËàâÁ†îÁ©∂„ÄÅÂæ∑ÂúãÁ∏±ÂêëÈÅ∏ËàâÁ†îÁ©∂„ÄÅÂ∫ßÊ®ôÊï∏ÊìöÈõÜÂíå‰∏≠ÂúãÂÆ∂Â∫≠ËøΩËπ§Ë™øÊü•ÁöÑÊï∏Êìö‰æÜÊ®°Êì¨ÊäïÁ•®Ë°åÁÇ∫ÂíåÊ∞ëÊÑè„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰ΩøÊàëÂÄëËÉΩÂ§†Ê™¢Êü•‰∏âÁ®ÆÈ°ûÂûãÁöÑ‰ª£Ë°®ÊÄßÂÅèÂ∑ÆÔºöÂü∫ÊñºÂúãÂÆ∂Ë™ûË®Ä„ÄÅ‰∫∫Âè£Áæ§È´îÂíåÊîøÊ≤ªÂà∂Â∫¶È°ûÂûãÁöÑÂ∑ÆÁï∞„ÄÇÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÊ®°Êì¨ÊïàËÉΩÈÄöÂ∏∏Âú®ÊäïÁ•®ÈÅ∏ÊìáÊñπÈù¢ÂÑ™ÊñºÊ∞ëÊÑèÔºåÂú®Ëã±Ë™ûÂúãÂÆ∂Êõ¥Ê∫ñÁ¢∫ÔºåÂú®ÂÖ©Èª®Âà∂Á≥ªÁµ±‰∏≠ÊØîÂú®Â§öÈª®Âà∂Á≥ªÁµ±‰∏≠Êõ¥ÊúâÊïàÔºåÂú®Ê∞ë‰∏ªÁí∞Â¢É‰∏≠ÊØîÂú®Â®ÅÊ¨äÊîøÊ¨ä‰∏≠Êõ¥Âº∑„ÄÇÈÄô‰∫õÁµêÊûúÊúâÂä©ÊñºÂä†Ê∑±ÊàëÂÄëÂ∞çË®àÁÆóÁ§æÊúÉÁßëÂ≠∏È†òÂüüÂÖß‰∫∫Â∑•Êô∫ËÉΩÊáâÁî®ÂÅèÂ∑ÆÁöÑÁêÜËß£Ôºå‰∏¶Âà∂ÂÆöÁ≠ñÁï•‰æÜÊ∏õËºïÂÅèÂ∑Æ„ÄÇ

##### **Revisiting the Impact of Pursuing Modularity for Code Generation**
2407.11406v1 by Deokyeong Kang, Ki Jung Seo, Taeuk Kim

Modular programming, which aims to construct the final program by integrating
smaller, independent building blocks, has been regarded as a desirable practice
in software development. However, with the rise of recent code generation
agents built upon large language models (LLMs), a question emerges: is this
traditional practice equally effective for these new tools? In this work, we
assess the impact of modularity in code generation by introducing a novel
metric for its quantitative measurement. Surprisingly, unlike conventional
wisdom on the topic, we find that modularity is not a core factor for improving
the performance of code generation models. We also explore potential
explanations for why LLMs do not exhibit a preference for modular code compared
to non-modular code.

ÊëòË¶ÅÔºöÊ®°ÁµÑÂåñÁ®ãÂºèË®≠Ë®àÊó®Âú®ÈÄèÈÅéÊï¥ÂêàËºÉÂ∞è„ÄÅÁç®Á´ãÁöÑÂª∫ÊßãÂçÄÂ°ä‰æÜÂª∫ÊßãÊúÄÁµÇÁ®ãÂºèÔºå‰∏ÄÁõ¥Ë¢´Ë¶ñÁÇ∫ËªüÈ´îÈñãÁôº‰∏≠ÁêÜÊÉ≥ÁöÑÂÅöÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÈö®ËëóÂª∫Á´ãÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÊúÄÊñ∞Á®ãÂºèÁ¢ºÁî¢Áîü‰ª£ÁêÜÁ®ãÂºèËààËµ∑Ôºå‰∏ÄÂÄãÂïèÈ°åÊµÆÁèæÔºöÈÄôÁ®ÆÂÇ≥Áµ±ÂÅöÊ≥ïÂ∞çÊñºÈÄô‰∫õÊñ∞Â∑•ÂÖ∑ÊòØÂê¶ÂêåÊ®£ÊúâÊïàÔºüÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÂºïÈÄ≤‰∏ÄÁ®ÆÁî®ÊñºÈáèÂåñÊ∏¨ÈáèÁöÑÊñ∞Á©éÊåáÊ®ô‰æÜË©ï‰º∞Ê®°ÁµÑÂåñÂ∞çÁ®ãÂºèÁ¢ºÁî¢ÁîüÁöÑÂΩ±Èüø„ÄÇ‰ª§‰∫∫È©öË®ùÁöÑÊòØÔºåËàáË©≤‰∏ªÈ°å‰∏äÁöÑÂÇ≥Áµ±Êô∫ÊÖß‰∏çÂêåÔºåÊàëÂÄëÁôºÁèæÊ®°ÁµÑÂåñ‰∏¶ÈùûÊîπÂñÑÁ®ãÂºèÁ¢ºÁî¢ÁîüÊ®°ÂûãÊïàËÉΩÁöÑÊ†∏ÂøÉÂõ†Á¥†„ÄÇÊàëÂÄë‰πüÊé¢Ë®é‰∫Ü LLM ËàáÈùûÊ®°ÁµÑÂåñÁ®ãÂºèÁ¢ºÁõ∏ÊØîÔºåÁÇ∫‰Ωï‰∏çÂÅèÂ•ΩÊ®°ÁµÑÂåñÁ®ãÂºèÁ¢ºÁöÑÊΩõÂú®Ëß£Èáã„ÄÇ

##### **DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation**
2407.11394v1 by Jiwook Kim, Seonho Lee, Jaeyo Shin, Jiho Choi, Hyunjung Shim

Score distillation sampling (SDS) has emerged as an effective framework in
text-driven 3D editing tasks due to its inherent 3D consistency. However,
existing SDS-based 3D editing methods suffer from extensive training time and
lead to low-quality results, primarily because these methods deviate from the
sampling dynamics of diffusion models. In this paper, we propose DreamCatalyst,
a novel framework that interprets SDS-based editing as a diffusion reverse
process. Our objective function considers the sampling dynamics, thereby making
the optimization process of DreamCatalyst an approximation of the diffusion
reverse process in editing tasks. DreamCatalyst aims to reduce training time
and improve editing quality. DreamCatalyst presents two modes: (1) a faster
mode, which edits the NeRF scene in only about 25 minutes, and (2) a
high-quality mode, which produces superior results in less than 70 minutes.
Specifically, our high-quality mode outperforms current state-of-the-art NeRF
editing methods both in terms of speed and quality. See more extensive results
on our project page: https://dream-catalyst.github.io.

ÊëòË¶ÅÔºöÂàÜÊï∏Ëí∏È§æÊé°Ê®£ (SDS) Áî±ÊñºÂÖ∂ÂÖßÂú®ÁöÑ 3D ‰∏ÄËá¥ÊÄßÔºåÂ∑≤ÊàêÁÇ∫ÊñáÂ≠óÈ©ÖÂãï 3D Á∑®ËºØ‰ªªÂãô‰∏≠‰∏ÄÂÄãÊúâÊïàÁöÑÊ°ÜÊû∂„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂü∫Êñº SDS ÁöÑ 3D Á∑®ËºØÊñπÊ≥ïË®ìÁ∑¥ÊôÇÈñìÈï∑Ôºå‰∏îÊúÉÁî¢Áîü‰ΩéÂìÅË≥™ÁöÑÁµêÊûúÔºå‰∏ªË¶ÅÊòØÂõ†ÁÇ∫ÈÄô‰∫õÊñπÊ≥ïÂÅèÈõ¢‰∫ÜÊì¥Êï£Ê®°ÂûãÁöÑÊé°Ê®£ÂãïÊÖã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫ DreamCatalystÔºå‰∏ÄÂÄãÂ∞áÂü∫Êñº SDS ÁöÑÁ∑®ËºØËß£ÈáãÁÇ∫ÂèçÂêëÊì¥Êï£ÈÅéÁ®ãÁöÑÊñ∞Á©éÊ°ÜÊû∂„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÂáΩÊï∏ËÄÉÈáè‰∫ÜÊé°Ê®£ÂãïÊÖãÔºåÂæûËÄå‰Ωø DreamCatalyst ÁöÑÊúÄ‰Ω≥ÂåñÈÅéÁ®ãÊàêÁÇ∫Á∑®ËºØ‰ªªÂãô‰∏≠ÂèçÂêëÊì¥Êï£ÈÅéÁ®ãÁöÑËøë‰ººÂÄº„ÄÇDreamCatalyst Êó®Âú®Ê∏õÂ∞ëË®ìÁ∑¥ÊôÇÈñì‰∏¶ÊèêÂçáÁ∑®ËºØÂìÅË≥™„ÄÇDreamCatalyst ÊèêÂá∫ÂÖ©Á®ÆÊ®°ÂºèÔºö(1) ‰∏ÄÁ®ÆËºÉÂø´ÁöÑÊ®°ÂºèÔºåÂÉÖÂú®Á¥Ñ 25 ÂàÜÈêòÂÖßÁ∑®ËºØ NeRF Â†¥ÊôØÔºå‰ª•Âèä (2) ‰∏ÄÁ®ÆÈ´òÂìÅË≥™Ê®°ÂºèÔºåÂèØÂú®‰∏çÂà∞ 70 ÂàÜÈêòÂÖßÁî¢ÁîüÂÑ™Áï∞ÁöÑÁµêÊûú„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÁöÑÈ´òÂìÅË≥™Ê®°ÂºèÂú®ÈÄüÂ∫¶ÂíåÂìÅË≥™ÊñπÈù¢ÈÉΩÂÑ™ÊñºÁõÆÂâçÁöÑ NeRF Á∑®ËºØÊñπÊ≥ï„ÄÇÂú®ÊàëÂÄëÁöÑÂ∞àÊ°àÈ†ÅÈù¢‰∏äÊü•ÁúãÊõ¥ÂÖ®Èù¢ÁöÑÁµêÊûúÔºöhttps://dream-catalyst.github.io„ÄÇ

##### **CIC-BART-SSA: Controllable Image Captioning with Structured Semantic Augmentation**
2407.11393v1 by Kalliopi Basioti, Mohamed A. Abdelsalam, Federico Fancellu, Vladimir Pavlovic, Afsaneh Fazly

Controllable Image Captioning (CIC) aims at generating natural language
descriptions for an image, conditioned on information provided by end users,
e.g., regions, entities or events of interest. However, available
image--language datasets mainly contain captions that describe the entirety of
an image, making them ineffective for training CIC models that can potentially
attend to any subset of regions or relationships. To tackle this challenge, we
propose a novel, fully automatic method to sample additional focused and
visually grounded captions using a unified structured semantic representation
built on top of the existing set of captions associated with an image. We
leverage Abstract Meaning Representation (AMR), a cross-lingual graph-based
semantic formalism, to encode all possible spatio-semantic relations between
entities, beyond the typical spatial-relations-only focus of current methods.
We use this Structured Semantic Augmentation (SSA) framework to augment
existing image--caption datasets with the grounded controlled captions,
increasing their spatial and semantic diversity and focal coverage. We then
develop a new model, CIC-BART-SSA, specifically tailored for the CIC task, that
sources its control signals from SSA-diversified datasets. We empirically show
that, compared to SOTA CIC models, CIC-BART-SSA generates captions that are
superior in diversity and text quality, are competitive in controllability,
and, importantly, minimize the gap between broad and highly focused controlled
captioning performance by efficiently generalizing to the challenging highly
focused scenarios. Code is available at
https://github.com/SamsungLabs/CIC-BART-SSA.

ÊëòË¶ÅÔºöÂèØÊéßÂõæÂÉèÂ≠óÂπïÔºàCICÔºâÊó®Âú®‰∏∫ÂõæÂÉèÁîüÊàêËá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞ÔºåÊù°‰ª∂ÊòØÊ†πÊçÆÊúÄÁªàÁî®Êà∑Êèê‰æõÁöÑ‰ø°ÊÅØÔºå‰æãÂ¶ÇÊÑüÂÖ¥Ë∂£ÁöÑÂå∫Âüü„ÄÅÂÆû‰ΩìÊàñ‰∫ã‰ª∂„ÄÇÁÑ∂ËÄåÔºåÂèØÁî®ÁöÑÂõæÂÉèËØ≠Ë®ÄÊï∞ÊçÆÈõÜ‰∏ªË¶ÅÂåÖÂê´ÊèèËø∞ÂõæÂÉèÊï¥‰ΩìÁöÑÂ≠óÂπïÔºåËøô‰ΩøÂæóÂÆÉ‰ª¨Êó†Ê≥ïÊúâÊïàËÆ≠ÁªÉ CIC Ê®°ÂûãÔºåËÄå CIC Ê®°ÂûãÊúâÂèØËÉΩÂÖ≥Ê≥®‰ªª‰ΩïÂå∫ÂüüÊàñÂÖ≥Á≥ªÂ≠êÈõÜ„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøô‰∏ÄÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑ„ÄÅÂÖ®Ëá™Âä®ÁöÑÊñπÊ≥ïÊù•‰ΩøÁî®Âü∫‰∫éÁé∞Êúâ‰∏éÂõæÂÉèÂÖ≥ËÅîÁöÑÂ≠óÂπïÈõÜÊûÑÂª∫ÁöÑÁªü‰∏ÄÁªìÊûÑÂåñËØ≠‰πâË°®Á§∫Êù•ÈááÊ†∑ÈôÑÂä†ÁöÑ„ÄÅÈõÜ‰∏≠ÁöÑÂíåËßÜËßâÊé•Âú∞ÁöÑÂ≠óÂπï„ÄÇÊàë‰ª¨Âà©Áî®Ë∑®ËØ≠Ë®ÄÂõæÂΩ¢ÂºèËØ≠‰πâÂΩ¢Âºè‰∏ª‰πâ‚Äî‚ÄîÊäΩË±°ÊÑè‰πâË°®Á§∫ÔºàAMRÔºâÊù•ÂØπÂÆû‰Ωì‰πãÈó¥ÁöÑÊâÄÊúâÂèØËÉΩÁöÑÊó∂Á©∫ËØ≠‰πâÂÖ≥Á≥ªËøõË°åÁºñÁ†ÅÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂΩìÂâçÊñπÊ≥ïÈÄöÂ∏∏Âè™ÂÖ≥Ê≥®ÁöÑÁ©∫Èó¥ÂÖ≥Á≥ª„ÄÇÊàë‰ª¨‰ΩøÁî®ËøôÁßçÁªìÊûÑÂåñËØ≠‰πâÂ¢ûÂº∫ÔºàSSAÔºâÊ°ÜÊû∂Êù•‰ΩøÁî®Êé•Âú∞ÊéßÂà∂Â≠óÂπïÂ¢ûÂº∫Áé∞ÊúâÁöÑÂõæÂÉèÂ≠óÂπïÊï∞ÊçÆÈõÜÔºå‰ªéËÄåÂ¢ûÂä†ÂÖ∂Á©∫Èó¥ÂíåËØ≠‰πâÂ§öÊ†∑ÊÄß‰ª•ÂèäÁÑ¶ÁÇπË¶ÜÁõñËåÉÂõ¥„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™Êñ∞Ê®°Âûã CIC-BART-SSAÔºåËØ•Ê®°Âûã‰∏ìÈó®ÈíàÂØπ CIC ‰ªªÂä°ÂÆöÂà∂ÔºåÂÆÉ‰ªé SSA Â§öÂÖÉÂåñÊï∞ÊçÆÈõÜËé∑ÂèñÂÖ∂ÊéßÂà∂‰ø°Âè∑„ÄÇÊàë‰ª¨ÈÄöËøáÂÆûÈ™åËØÅÊòéÔºå‰∏é SOTA CIC Ê®°ÂûãÁõ∏ÊØîÔºåCIC-BART-SSA ÁîüÊàêÁöÑÂ≠óÂπïÂú®Â§öÊ†∑ÊÄßÂíåÊñáÊú¨Ë¥®ÈáèÊñπÈù¢Êõ¥ËÉú‰∏ÄÁ≠πÔºåÂú®ÂèØÊéßÊÄßÊñπÈù¢ÂÖ∑ÊúâÁ´û‰∫âÂäõÔºåËÄå‰∏îÈáçË¶ÅÁöÑÊòØÔºåÂÆÉÊúÄÂ§ßÁ®ãÂ∫¶Âú∞Áº©Â∞è‰∫ÜÂπøÊ≥õÂíåÈ´òÂ∫¶ÈõÜ‰∏≠ÁöÑÂèóÊéßÂ≠óÂπïÊÄßËÉΩ‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºå‰ªéËÄåÊúâÊïàÂú∞Êé®ÂπøÂà∞ÊûÅÂÖ∑ÊåëÊàòÊÄßÁöÑÈ´òÂ∫¶ÈõÜ‰∏≠ÁöÑÂú∫ÊôØ„ÄÇ‰ª£Á†ÅÂèØÂú® https://github.com/SamsungLabs/CIC-BART-SSA Ëé∑Âæó„ÄÇ

##### **InvAgent: A Large Language Model based Multi-Agent System for Inventory Management in Supply Chains**
2407.11384v1 by Yinzhu Quan, Zefang Liu

Supply chain management (SCM) involves coordinating the flow of goods,
information, and finances across various entities to deliver products
efficiently. Effective inventory management is crucial in today's volatile,
uncertain, complex, and ambiguous (VUCA) world. Previous research has
demonstrated the superiority of heuristic methods and reinforcement learning
applications in inventory management. However, the application of large
language models (LLMs) as autonomous agents in multi-agent systems for
inventory management remains underexplored. This study introduces a novel
approach using LLMs to manage multi-agent inventory systems. Leveraging their
zero-shot learning capabilities, our model, InvAgent, enhances resilience and
improves efficiency across the supply chain network. Our contributions include
utilizing LLMs for zero-shot learning to enable adaptive and informed
decision-making without prior training, providing significant explainability
and clarity through Chain-of-Thought (CoT), and demonstrating dynamic
adaptability to varying demand scenarios while minimizing costs and avoiding
stockouts. Extensive evaluations across different scenarios highlight the
efficiency of our model in SCM.

ÊëòË¶ÅÔºö‰æõÊáâÈèàÁÆ°ÁêÜ (SCM) ÂåÖÂê´ÂçîË™øË≤®Áâ©„ÄÅË≥áË®äÂíåË≥áÈáëÂú®ÂêÑÁ®ÆÂØ¶È´î‰πãÈñìÁöÑÊµÅÂãïÔºå‰ª•ÊúâÊïàÁéáÂú∞‰∫§‰ªòÁî¢ÂìÅ„ÄÇÂú®Áèæ‰ªäÊòìËÆä„ÄÅ‰∏çÁ¢∫ÂÆö„ÄÅË§áÈõú‰∏îÊ®°Á≥ä (VUCA) ÁöÑ‰∏ñÁïå‰∏≠ÔºåÊúâÊïàÁöÑÂ∫´Â≠òÁÆ°ÁêÜËá≥ÈóúÈáçË¶Å„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂Â∑≤Ë≠âÊòéÂïüÁôºÂºèÊñπÊ≥ïÂíåÂº∑ÂåñÂ≠∏ÁøíÊáâÁî®Âú®Â∫´Â≠òÁÆ°ÁêÜ‰∏≠ÁöÑÂÑ™Ë∂äÊÄß„ÄÇÁÑ∂ËÄåÔºåÂ∞áÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰ΩúÁÇ∫Â§ö‰∏ªÈ´îÁ≥ªÁµ±‰∏≠Áî®ÊñºÂ∫´Â≠òÁÆ°ÁêÜÁöÑËá™‰∏ª‰ª£ÁêÜ‰∫∫ÁöÑÊáâÁî®‰ªçÊú™Ë¢´ÂÖÖÂàÜÊé¢Ë®é„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî® LLM ‰æÜÁÆ°ÁêÜÂ§ö‰∏ªÈ´îÂ∫´Â≠òÁ≥ªÁµ±ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÊ®°Âûã InvAgent ÈÄèÈÅéÈÅãÁî®ÂÖ∂Èõ∂Ê¨°Â≠∏ÁøíËÉΩÂäõÔºåÂ¢ûÂº∑‰∫ÜÂæ©ÂéüÂäõ‰∏¶ÊèêÂçá‰∫ÜÊï¥ÂÄã‰æõÊáâÈèàÁ∂≤Ë∑ØÁöÑÊïàÁéá„ÄÇÊàëÂÄëÁöÑË≤¢ÁçªÂåÖÊã¨Âà©Áî® LLM ÈÄ≤Ë°åÈõ∂Ê¨°Â≠∏ÁøíÔºå‰ª•Âú®Ê≤íÊúâ‰∫ãÂÖàË®ìÁ∑¥ÁöÑÊÉÖÊ≥Å‰∏ãÂïüÁî®ÈÅ©ÊáâÊÄßÂíåÊòéÊô∫ÁöÑÊ±∫Á≠ñÂà∂ÂÆöÔºåÈÄèÈÅéÊÄùËÄÉÈèà (CoT) Êèê‰æõÈ°ØËëóÁöÑÂèØËß£ÈáãÊÄßÂíåÊ∏ÖÊô∞Â∫¶Ôºå‰∏¶Â±ïÁ§∫Â∞ç‰∏çÂêåÈúÄÊ±ÇÊÉÖÂ¢ÉÁöÑÂãïÊÖãÈÅ©ÊáâËÉΩÂäõÔºåÂêåÊôÇÂ∞áÊàêÊú¨ÈôçËá≥ÊúÄ‰Ωé‰∏¶ÈÅøÂÖçÁº∫Ë≤®„ÄÇÂú®‰∏çÂêåÊÉÖÂ¢É‰∏≠ÁöÑÂª£Ê≥õË©ï‰º∞Á™ÅÈ°Ø‰∫ÜÊàëÂÄëÁöÑÊ®°ÂûãÂú® SCM ‰∏≠ÁöÑÊïàÁéá„ÄÇ

##### **Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts**
2407.11382v2 by Jianhao Li, Tianyu Sun, Zhongdao Wang, Enze Xie, Bailan Feng, Hongbo Zhang, Ze Yuan, Ke Xu, Jiaheng Liu, Ping Luo

This paper proposes an algorithm for automatically labeling 3D objects from
2D point or box prompts, especially focusing on applications in autonomous
driving. Unlike previous arts, our auto-labeler predicts 3D shapes instead of
bounding boxes and does not require training on a specific dataset. We propose
a Segment, Lift, and Fit (SLF) paradigm to achieve this goal. Firstly, we
segment high-quality instance masks from the prompts using the Segment Anything
Model (SAM) and transform the remaining problem into predicting 3D shapes from
given 2D masks. Due to the ill-posed nature of this problem, it presents a
significant challenge as multiple 3D shapes can project into an identical mask.
To tackle this issue, we then lift 2D masks to 3D forms and employ gradient
descent to adjust their poses and shapes until the projections fit the masks
and the surfaces conform to surrounding LiDAR points. Notably, since we do not
train on a specific dataset, the SLF auto-labeler does not overfit to biased
annotation patterns in the training set as other methods do. Thus, the
generalization ability across different datasets improves. Experimental results
on the KITTI dataset demonstrate that the SLF auto-labeler produces
high-quality bounding box annotations, achieving an AP@0.5 IoU of nearly 90\%.
Detectors trained with the generated pseudo-labels perform nearly as well as
those trained with actual ground-truth annotations. Furthermore, the SLF
auto-labeler shows promising results in detailed shape predictions, providing a
potential alternative for the occupancy annotation of dynamic objects.

ÊëòË¶ÅÔºö<paragraph>ÈÄôÁØáË´ñÊñáÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊºîÁÆóÊ≥ïÔºåÁî®ÊñºËá™ÂãïÊ®ôË®ò 3D Áâ©‰ª∂ÔºåÂæû 2D ÈªûÊàñÊñπÊ°ÜÊèêÁ§∫ÔºåÁâπÂà•ËëóÈáçÊñºËá™ÂãïÈßïÈßõÁöÑÊáâÁî®„ÄÇËàáÂÖàÂâçÁöÑÊäÄË°ì‰∏çÂêåÔºåÊàëÂÄëÁöÑËá™ÂãïÊ®ôÁ±§Âô®È†êÊ∏¨ 3D ÂΩ¢ÁãÄÔºåËÄå‰∏çÊòØÈÇäÁïåÊñπÊ°ÜÔºå‰∏¶‰∏î‰∏çÈúÄË¶ÅÈáùÂ∞çÁâπÂÆöË≥áÊñôÈõÜÈÄ≤Ë°åË®ìÁ∑¥„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂàÜÊÆµ„ÄÅÊèêÂçáÂíåÊì¨Âêà (SLF) ÁöÑÁØÑ‰æã‰æÜÈÅîÊàêÈÄôÂÄãÁõÆÊ®ô„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄë‰ΩøÁî® Segment Anything Model (SAM) ÂæûÊèêÁ§∫‰∏≠ÂàÜÊÆµÂá∫È´òÂìÅË≥™ÁöÑÂØ¶‰æãÈÅÆÁΩ©Ôºå‰∏¶Â∞áÂâ©‰∏ãÁöÑÂïèÈ°åËΩâÊèõÁÇ∫ÂæûÁµ¶ÂÆöÁöÑ 2D ÈÅÆÁΩ©È†êÊ∏¨ 3D ÂΩ¢ÁãÄ„ÄÇÁî±ÊñºÈÄôÂÄãÂïèÈ°åÁöÑÊÄßË≥™‰∏ç‰Ω≥ÔºåÂõ†Ê≠§ÂÆÉÊèêÂá∫‰∫ÜÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÂõ†ÁÇ∫Â§öÂÄã 3D ÂΩ¢ÁãÄÂèØ‰ª•ÊäïÂΩ±Âà∞‰∏ÄÂÄãÁõ∏ÂêåÁöÑÈÅÆÁΩ©„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊé•ËëóÂ∞á 2D ÈÅÆÁΩ©ÊèêÂçáÂà∞ 3D ÂΩ¢ÂºèÔºå‰∏¶‰ΩøÁî®Ê¢ØÂ∫¶‰∏ãÈôç‰æÜË™øÊï¥ÂÆÉÂÄëÁöÑÂßøÂã¢ÂíåÂΩ¢ÁãÄÔºåÁõ¥Âà∞ÊäïÂΩ±Á¨¶ÂêàÈÅÆÁΩ©Ôºå‰∏¶‰∏îË°®Èù¢Á¨¶ÂêàÂë®ÂúçÁöÑ LiDAR Èªû„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÁî±ÊñºÊàëÂÄëÊ≤íÊúâÈáùÂ∞çÁâπÂÆöË≥áÊñôÈõÜÈÄ≤Ë°åË®ìÁ∑¥ÔºåÂõ†Ê≠§ SLF Ëá™ÂãïÊ®ôÁ±§Âô®‰∏çÊúÉÈÅéÂ∫¶Êì¨ÂêàË®ìÁ∑¥ÈõÜ‰∏≠ÊúâÂÅèÂ∑ÆÁöÑË®ªËß£Ê®°ÂºèÔºåÂ∞±ÂÉèÂÖ∂‰ªñÊñπÊ≥ï‰∏ÄÊ®£„ÄÇÂõ†Ê≠§ÔºåË∑®‰∏çÂêåË≥áÊñôÈõÜÁöÑÊ≥õÂåñËÉΩÂäõÂæóÂà∞ÊîπÂñÑ„ÄÇÂú® KITTI Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÁµêÊûúË≠âÊòéÔºåSLF Ëá™ÂãïÊ®ôÁ±§Âô®Áî¢Áîü‰∫ÜÈ´òÂìÅË≥™ÁöÑÈÇäÁïåÊñπÊ°ÜË®ªËß£ÔºåÈÅîÂà∞‰∫ÜÂ∞áËøë 90% ÁöÑ AP@0.5 IoU„ÄÇ‰ΩøÁî®Áî¢ÁîüÁöÑÂÅΩÊ®ôÁ±§Ë®ìÁ∑¥ÁöÑÂÅµÊ∏¨Âô®Âü∑Ë°åÂæóÂπæ‰πéÂíå‰ΩøÁî®ÂØ¶ÈöõÂü∫Êú¨‰∫ãÂØ¶Ë®ªËß£Ë®ìÁ∑¥ÁöÑÈÇ£‰∫õ‰∏ÄÊ®£Â•Ω„ÄÇÊ≠§Â§ñÔºåSLF Ëá™ÂãïÊ®ôÁ±§Âô®Âú®Ë©≥Á¥∞ÂΩ¢ÁãÄÈ†êÊ∏¨‰∏≠È°ØÁ§∫Âá∫ÊúâÂ∏åÊúõÁöÑÁµêÊûúÔºåÁÇ∫ÂãïÊÖãÁâ©È´îÁöÑ‰ΩîÁî®Ë®ªËß£Êèê‰æõ‰∫ÜÊΩõÂú®ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇ</paragraph>

##### **Reliable Reasoning Beyond Natural Language**
2407.11373v1 by Nasim Borazjanizadeh, Steven T. Piantadosi

Despite their linguistic competence, Large Language models (LLMs) often
exhibit limitations in their ability to reason reliably and flexibly. To
address this, we propose a neurosymbolic approach that prompts LLMs to extract
and encode all relevant information from a problem statement as logical code
statements, and then use a logic programming language (Prolog) to conduct the
iterative computations of explicit deductive reasoning. Our approach
significantly enhances the performance of LLMs on the standard mathematical
reasoning benchmark, GSM8k, and the Navigate dataset from the BIG-bench
dataset. Additionally, we introduce a novel dataset, the Non-Linear Reasoning
(NLR) dataset, consisting of 55 unique word problems that target the
shortcomings of the next token prediction paradigm of LLMs and require complex
non-linear reasoning but only basic arithmetic skills to solve. Our findings
demonstrate that the integration of Prolog enables LLMs to achieve high
performance on the NLR dataset, which even the most advanced language models
(including GPT4) fail to solve using text only.

ÊëòË¶ÅÔºöÂÑòÁÆ°ÂÖ∑ÂÇôË™ûË®ÄËÉΩÂäõÔºåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Á∂ìÂ∏∏Âú®ÂèØÈù†‰∏îÈùàÊ¥ªÂú∞Êé®ÁêÜÁöÑËÉΩÂäõ‰∏äÂ±ïÁèæÂá∫ÈôêÂà∂„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ê≠§ÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ•ûÁ∂ìÁ¨¶ËôüÊñπÊ≥ïÔºåÊèêÁ§∫ LLM ÂæûÂïèÈ°åÈô≥Ëø∞‰∏≠ÊèêÂèñÂíåÁ∑®Á¢ºÊâÄÊúâÁõ∏ÈóúË≥áË®ä‰ΩúÁÇ∫ÈÇèËºØÁ®ãÂºèÁ¢ºÈô≥Ëø∞ÔºåÁÑ∂Âæå‰ΩøÁî®ÈÇèËºØÁ®ãÂºèË®≠Ë®àË™ûË®Ä (Prolog) ÈÄ≤Ë°åÊòéÁ¢∫ÊºîÁππÊé®ÁêÜÁöÑËø≠‰ª£ÈÅãÁÆó„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÈ°ØËëóÊèêÂçá LLM Âú®Ê®ôÊ∫ñÊï∏Â≠∏Êé®ÁêÜÂü∫Ê∫ñ GSM8k Âíå BIG-bench Ë≥áÊñôÈõÜ‰∏≠ÁöÑ Navigate Ë≥áÊñôÈõÜ‰∏äÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü‰∏ÄÂÄãÊñ∞Á©éÁöÑË≥áÊñôÈõÜÔºåÈùûÁ∑öÊÄßÊé®ÁêÜ (NLR) Ë≥áÊñôÈõÜÔºåÂåÖÂê´ 55 ÂÄãÁç®ÁâπÁöÑÊñáÂ≠óÈ°åÔºåÈáùÂ∞ç LLM ÁöÑ‰∏ã‰∏ÄÂÄãÁ¨¶ËôüÈ†êÊ∏¨ÁØÑ‰æãÁöÑÁº∫ÈªûÔºåÈúÄË¶ÅË§áÈõúÁöÑÈùûÁ∑öÊÄßÊé®ÁêÜÔºå‰ΩÜÂè™ÈúÄË¶ÅÂü∫Êú¨ÁöÑÁÆóË°ìÊäÄÂ∑ßÂ∞±ËÉΩËß£È°å„ÄÇÊàëÂÄëÁöÑÁôºÁèæË≠âÊòéÔºåÊï¥Âêà Prolog ËÉΩËÆì LLM Âú® NLR Ë≥áÊñôÈõÜ‰∏äÈÅîÊàêÈ´òÊïàËÉΩÔºåÈÄôÁîöËá≥ÊòØÈÄ≤ÈöéÁöÑË™ûË®ÄÊ®°ÂûãÔºàÂåÖÂê´ GPT4ÔºâÈÉΩÁÑ°Ê≥ïÂÉÖ‰ΩøÁî®ÊñáÂ≠óËß£È°åÁöÑ„ÄÇ

##### **Estimating Agreement by Chance for Sequence Annotation**
2407.11371v1 by Diya Li, Carolyn Ros√©, Ao Yuan, Chunxiao Zhou

In the field of natural language processing, correction of performance
assessment for chance agreement plays a crucial role in evaluating the
reliability of annotations. However, there is a notable dearth of research
focusing on chance correction for assessing the reliability of sequence
annotation tasks, despite their widespread prevalence in the field. To address
this gap, this paper introduces a novel model for generating random
annotations, which serves as the foundation for estimating chance agreement in
sequence annotation tasks. Utilizing the proposed randomization model and a
related comparison approach, we successfully derive the analytical form of the
distribution, enabling the computation of the probable location of each
annotated text segment and subsequent chance agreement estimation. Through a
combination simulation and corpus-based evaluation, we successfully assess its
applicability and validate its accuracy and efficacy.

ÊëòË¶ÅÔºöÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÈ†òÂüüÔºå‰øÆÊ≠£Ê©üÊúÉ‰∏ÄËá¥ÊÄßÁöÑË°®ÁèæË©ï‰º∞Âú®Ë©ï‰º∞Ë®ªÈáãÁöÑÂèØ‰ø°Â∫¶ÊñπÈù¢ÊâÆÊºîËëóËá≥ÈóúÈáçË¶ÅÁöÑËßíËâ≤„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ°Â∫èÂàóË®ªÈáã‰ªªÂãôÂú®Ë©≤È†òÂüüÂª£Ê≥õÁõõË°åÔºå‰ΩÜÂ∞àÊ≥®ÊñºÊ©üÊúÉ‰øÆÊ≠£‰ª•Ë©ï‰º∞Â∫èÂàóË®ªÈáã‰ªªÂãôÂèØ‰ø°Â∫¶ÁöÑÁ†îÁ©∂ÂçªÁõ∏Áï∂Âå±‰πè„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÁî®ÊñºÁî¢ÁîüÈö®Ê©üË®ªÈáãÁöÑÊñ∞Á©éÊ®°ÂûãÔºå‰ΩúÁÇ∫Âú®Â∫èÂàóË®ªÈáã‰ªªÂãô‰∏≠‰º∞Ë®àÊ©üÊúÉ‰∏ÄËá¥ÊÄßÁöÑÂü∫Á§é„ÄÇÂà©Áî®ÊâÄÊèêÂá∫ÁöÑÈö®Ê©üÂåñÊ®°ÂûãÂíåÁõ∏ÈóúÁöÑÊØîËºÉÊñπÊ≥ïÔºåÊàëÂÄëÊàêÂäüÂú∞Êé®Â∞éÂá∫ÂàÜ‰ΩàÁöÑËß£ÊûêÂΩ¢ÂºèÔºåÈÄ≤ËÄåËÉΩË®àÁÆóÊØèÂÄãË®ªÈáãÊñáÊú¨ÂçÄÊÆµÁöÑÂèØËÉΩ‰ΩçÁΩÆÂíåÂæåÁ∫åÁöÑÊ©üÊúÉ‰∏ÄËá¥ÊÄß‰º∞Ë®à„ÄÇÈÄèÈÅéÁµêÂêàÊ®°Êì¨ÂíåÂü∫ÊñºË™ûÊñôÂ∫´ÁöÑË©ï‰º∞ÔºåÊàëÂÄëÊàêÂäüÂú∞Ë©ï‰º∞‰∫ÜÂÖ∂ÈÅ©Áî®ÊÄßÔºå‰∏¶È©óË≠â‰∫ÜÂÖ∂Ê∫ñÁ¢∫ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ

##### **A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora**
2407.11370v1 by Kentaro Onda, Joonyong Park, Nobuaki Minematsu, Daisuke Saito

We propose a method of simulating the human process of foreign accentuation
using Generative Spoken Language Model (GSLM) only with native speech corpora.
When one listens to spoken words of a foreign language and repeats them, the
repeated speech is often with the accent of that listener's L1. This is said to
be because the spoken words are mentally represented as a sequence of
phonological units of the L1, and those units are used for oral reproduction.
We simulate this process by inputting speech of language A into GSLM of
language B to add B's accent onto the input speech. The process of running ASR
of the L1 for foreign input speech and giving the ASR result to TTS of the L1
can be viewed as a naive implementation of this approach. The results of our
experiments show that the synthesized accent of the output speech is highly
natural, compared to real samples of A generated by speakers whose L1 is B, and
that the degree of accentuation is controllable.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊ®°Êì¨‰∫∫È°ûÂ§ñË™ûÁôºÈü≥ÈÅéÁ®ãÁöÑÊñπÊ≥ïÔºåÂÉÖ‰ΩøÁî®ÁîüÊàêÂºèÂè£Ë™ûË™ûË®ÄÊ®°Âûã (GSLM) ÂíåÊØçË™ûË™ûÊñôÂ∫´„ÄÇÁï∂Êúâ‰∫∫ËÅΩÂà∞Â§ñË™ûÁöÑÂè£Ë™û‰∏¶ÈáçË§áÊôÇÔºåÈáçË§áÁöÑË™ûÈü≥ÈÄöÂ∏∏Â∏∂ÊúâËÅΩËÄÖÁöÑ L1 Âè£Èü≥„ÄÇÊìöË™™ÈÄôÊòØÂõ†ÁÇ∫Âè£Ë™ûÂú®ÂøÉÊô∫‰∏≠Ë¢´Ë°®ÂæµÁÇ∫ L1 ÁöÑÈü≥‰ΩçÂñÆÂÖÉÂ∫èÂàóÔºåËÄåÈÄô‰∫õÂñÆÂÖÉÁî®ÊñºÂè£Ë™ûÂÜçÁèæ„ÄÇÊàëÂÄëÊ®°Êì¨ÈÄôÂÄãÈÅéÁ®ãÔºåÂ∞áË™ûË®Ä A ÁöÑË™ûÈü≥Ëº∏ÂÖ•Âà∞Ë™ûË®Ä B ÁöÑ GSLM ‰∏≠ÔºåÂ∞á B ÁöÑÂè£Èü≥Ê∑ªÂä†Âà∞Ëº∏ÂÖ•Ë™ûÈü≥‰∏≠„ÄÇÁÇ∫Â§ñË™ûËº∏ÂÖ•Ë™ûÈü≥Âü∑Ë°å L1 ÁöÑ ASRÔºå‰∏¶Â∞á ASR ÁµêÊûúÊèê‰æõÁµ¶ L1 ÁöÑ TTS ÁöÑÈÅéÁ®ãÔºåÂèØ‰ª•Ë¶ñÁÇ∫ÈÄôÁ®ÆÊñπÊ≥ïÁöÑÂπºÁ®öÂØ¶Áèæ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåËàá L1 ÁÇ∫ B ÁöÑË¨õËÄÖÁî¢ÁîüÁöÑ A ÁöÑÁúüÂØ¶Ê®£Êú¨Áõ∏ÊØîÔºåËº∏Âá∫Ë™ûÈü≥ÁöÑÂêàÊàêÂè£Èü≥ÈùûÂ∏∏Ëá™ÁÑ∂Ôºå‰∏¶‰∏îÂè£Èü≥Á®ãÂ∫¶ÊòØÂèØ‰ª•ÊéßÂà∂ÁöÑ„ÄÇ

##### **Ancient Korean Archive Translation: Comparison Analysis on Statistical phrase alignment, LLM in-context learning, and inter-methodological approach**
2407.11368v1 by Sojung Lucia Kim, Taehong Jang, Joonmo Ahn

This study aims to compare three methods for translating ancient texts with
sparse corpora: (1) the traditional statistical translation method of phrase
alignment, (2) in-context LLM learning, and (3) proposed inter methodological
approach - statistical machine translation method using sentence piece tokens
derived from unified set of source-target corpus. The performance of the
proposed approach in this study is 36.71 in BLEU score, surpassing the scores
of SOLAR-10.7B context learning and the best existing Seq2Seq model. Further
analysis and discussion are presented.

ÊëòË¶ÅÔºöÊú¨Á†îÁ©∂Êó®Âú®ÊØîËºÉ‰∏âÁ®ÆÁøªË≠ØÁ®ÄÁñèË™ûÊñôÂ∫´ÁöÑÂè§‰ª£ÊñáÊú¨ÁöÑÊñπÊ≥ïÔºö(1) ÂÇ≥Áµ±ÁöÑÁµ±Ë®àÁøªË≠ØÊñπÊ≥ïÁöÑÁü≠Ë™ûÂ∞çÈΩäÔºå(2) ‰∏ä‰∏ãÊñá LLM Â≠∏ÁøíÔºå‰ª•Âèä (3) ÊèêÂá∫ÁöÑÊñπÊ≥ïË´ñÈñìÊñπÊ≥ï - ‰ΩøÁî®Ê∫êÁõÆÊ®ôË™ûÊñôÂ∫´Áµ±‰∏ÄÈõÜÂêà‰∏≠Ë°çÁîüÁöÑÂè•Â≠êÁâáÊÆµ‰ª§ÁâåÁöÑÁµ±Ë®àÊ©üÂô®ÁøªË≠ØÊñπÊ≥ï„ÄÇÊú¨Á†îÁ©∂‰∏≠ÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑ BLEU ÂàÜÊï∏Ë°®ÁèæÁÇ∫ 36.71ÔºåË∂ÖÈÅé‰∫Ü SOLAR-10.7B ‰∏ä‰∏ãÊñáÂ≠∏ÁøíÂíåÁèæÊúâÊúÄ‰Ω≥ Seq2Seq Ê®°ÂûãÁöÑÂàÜÊï∏„ÄÇÊèê‰æõ‰∫ÜÈÄ≤‰∏ÄÊ≠•ÁöÑÂàÜÊûêÂíåË®éË´ñ„ÄÇ

##### **Feature Inference Attack on Shapley Values**
2407.11359v1 by Xinjian Luo, Yangfan Jiang, Xiaokui Xiao

As a solution concept in cooperative game theory, Shapley value is highly
recognized in model interpretability studies and widely adopted by the leading
Machine Learning as a Service (MLaaS) providers, such as Google, Microsoft, and
IBM. However, as the Shapley value-based model interpretability methods have
been thoroughly studied, few researchers consider the privacy risks incurred by
Shapley values, despite that interpretability and privacy are two foundations
of machine learning (ML) models.
  In this paper, we investigate the privacy risks of Shapley value-based model
interpretability methods using feature inference attacks: reconstructing the
private model inputs based on their Shapley value explanations. Specifically,
we present two adversaries. The first adversary can reconstruct the private
inputs by training an attack model based on an auxiliary dataset and black-box
access to the model interpretability services. The second adversary, even
without any background knowledge, can successfully reconstruct most of the
private features by exploiting the local linear correlations between the model
inputs and outputs. We perform the proposed attacks on the leading MLaaS
platforms, i.e., Google Cloud, Microsoft Azure, and IBM aix360. The
experimental results demonstrate the vulnerability of the state-of-the-art
Shapley value-based model interpretability methods used in the leading MLaaS
platforms and highlight the significance and necessity of designing
privacy-preserving model interpretability methods in future studies. To our
best knowledge, this is also the first work that investigates the privacy risks
of Shapley values.

ÊëòË¶ÅÔºö‰ΩúÁÇ∫Âêà‰ΩúÂçöÂºàË´ñ‰∏≠ÁöÑËß£Ê±∫Ê¶ÇÂøµÔºåShapley ÂÄºÂú®Ê®°ÂûãÂèØËß£ÈáãÊÄßÁ†îÁ©∂‰∏≠ÂèóÂà∞È´òÂ∫¶ÈáçË¶ñÔºå‰∏¶Ë¢´ Google„ÄÅMicrosoft Âíå IBM Á≠âÈ†òÂÖàÁöÑÊ©üÂô®Â≠∏ÁøíÂç≥ÊúçÂãô (MLaaS) ‰æõÊáâÂïÜÂª£Ê≥õÊé°Áî®„ÄÇÁÑ∂ËÄåÔºåÂÑòÁÆ°Âü∫Êñº Shapley ÂÄºÁöÑÊ®°ÂûãÂèØËß£ÈáãÊÄßÊñπÊ≥ïÂ∑≤ÂæóÂà∞Ê∑±ÂÖ•Á†îÁ©∂Ôºå‰ΩÜÂæàÂ∞ëÊúâÁ†îÁ©∂‰∫∫Âì°ËÄÉÊÖÆ Shapley ÂÄºÂ∏∂‰æÜÁöÑÈö±ÁßÅÈ¢®Èö™ÔºåÂÑòÁÆ°ÂèØËß£ÈáãÊÄßÂíåÈö±ÁßÅÊòØÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÁöÑÂÖ©ÂÄãÂü∫Á§é„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ΩøÁî®ÁâπÂæµÊé®Ë´ñÊîªÊìä‰æÜÁ†îÁ©∂Âü∫Êñº Shapley ÂÄºÁöÑÊ®°ÂûãÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÈö±ÁßÅÈ¢®Èö™ÔºöÊ†πÊìöÂÖ∂ Shapley ÂÄºËß£ÈáãÈáçÂª∫ÁßÅÊúâÊ®°ÂûãËº∏ÂÖ•„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÂÖ©ÂÄãÂ∞çÊâã„ÄÇÁ¨¨‰∏ÄÂÄãÂ∞çÊâãÂèØ‰ª•ÈÄöÈÅéÂü∫ÊñºËºîÂä©Êï∏ÊìöÈõÜË®ìÁ∑¥ÊîªÊìäÊ®°Âûã‰∏¶ÈªëÁõíË®™ÂïèÊ®°ÂûãÂèØËß£ÈáãÊÄßÊúçÂãô‰æÜÈáçÂª∫ÁßÅÊúâËº∏ÂÖ•„ÄÇÁ¨¨‰∫åÂÄãÂ∞çÊâãÔºåÂç≥‰ΩøÊ≤íÊúâ‰ªª‰ΩïËÉåÊôØÁü•Ë≠òÔºå‰πüÂèØ‰ª•ÈÄöÈÅéÂà©Áî®Ê®°ÂûãËº∏ÂÖ•ÂíåËº∏Âá∫‰πãÈñìÁöÑÂ±ÄÈÉ®Á∑öÊÄßÁõ∏ÈóúÊÄßÊàêÂäüÈáçÂª∫Â§ßÈÉ®ÂàÜÁßÅÊúâÁâπÂæµ„ÄÇÊàëÂÄëÂú®È†òÂÖàÁöÑ MLaaS Âπ≥Âè∞ÔºàÂç≥ Google Cloud„ÄÅMicrosoft Azure Âíå IBM aix360Ôºâ‰∏äÂü∑Ë°åÊèêË≠∞ÁöÑÊîªÊìä„ÄÇÂØ¶È©óÁµêÊûúË≠âÊòé‰∫ÜÈ†òÂÖàÁöÑ MLaaS Âπ≥Âè∞‰∏≠‰ΩøÁî®ÁöÑÂü∫Êñº Shapley ÂÄºÁöÑÊúÄÊñ∞Ê®°ÂûãÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑËÑÜÂº±ÊÄßÔºå‰∏¶Âº∑Ë™ø‰∫ÜÂú®Êú™‰æÜÁöÑÁ†îÁ©∂‰∏≠Ë®≠Ë®àÈö±ÁßÅ‰øùË≠∑Ê®°ÂûãÂèØËß£ÈáãÊÄßÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄßËàáÂøÖË¶ÅÊÄß„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄô‰πüÊòØÁ¨¨‰∏ÄÂÄãÁ†îÁ©∂ Shapley ÂÄºÁöÑÈö±ÁßÅÈ¢®Èö™ÁöÑÂ∑•‰Ωú„ÄÇ

##### **Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models**
2407.11345v1 by Matthew Perez, Aneesha Sampath, Minxue Niu, Emily Mower Provost

Aphasia is a language disorder that can lead to speech errors known as
paraphasias, which involve the misuse, substitution, or invention of words.
Automatic paraphasia detection can help those with Aphasia by facilitating
clinical assessment and treatment planning options. However, most automatic
paraphasia detection works have focused solely on binary detection, which
involves recognizing only the presence or absence of a paraphasia. Multiclass
paraphasia detection represents an unexplored area of research that focuses on
identifying multiple types of paraphasias and where they occur in a given
speech segment. We present novel approaches that use a generative pretrained
transformer (GPT) to identify paraphasias from transcripts as well as two
end-to-end approaches that focus on modeling both automatic speech recognition
(ASR) and paraphasia classification as multiple sequences vs. a single
sequence. We demonstrate that a single sequence model outperforms GPT baselines
for multiclass paraphasia detection.

ÊëòË¶ÅÔºöÂ§±Ë™ûÁóáÊòØ‰∏ÄÁ®ÆË™ûË®ÄÈöúÁ§ôÔºåÂèØËÉΩÂ∞éËá¥Ë®ÄË™ûÈåØË™§ÔºåÁ®±ÁÇ∫ÈåØË™ûÁóáÔºåÂÖ∂‰∏≠Ê∂âÂèäÂ≠óË©ûÁöÑË™§Áî®„ÄÅÊõøÊèõÊàñÂâµÈÄ†„ÄÇ
Ëá™ÂãïÈåØË™ûÁóáÂÅµÊ∏¨ÂèØ‰ª•ÈÄèÈÅé‰øÉÈÄ≤Ëá®Â∫äË©ï‰º∞ÂíåÊ≤ªÁôÇË¶èÂäÉÈÅ∏È†ÖÔºåÂπ´Âä©Â§±Ë™ûÁóáÊÇ£ËÄÖ„ÄÇ
ÁÑ∂ËÄåÔºåÂ§ßÂ§öÊï∏Ëá™ÂãïÈåØË™ûÁóáÂÅµÊ∏¨Â∑•‰ΩúÂÉÖÂ∞àÊ≥®Êñº‰∫åÂÖÉÂÅµÊ∏¨ÔºåÂÖ∂‰∏≠Âè™Ê∂âÂèäËæ®Ë≠òÈåØË™ûÁóáÁöÑÂ≠òÂú®Êàñ‰∏çÂ≠òÂú®„ÄÇ
Â§öÈ°ûÂà•ÈåØË™ûÁóáÂÅµÊ∏¨‰ª£Ë°®‰∫Ü‰∏ÄÂÄãÂ∞öÊú™Êé¢Á¥¢ÁöÑÁ†îÁ©∂È†òÂüüÔºåÂÖ∂Â∞àÊ≥®ÊñºË≠òÂà•Â§öÁ®ÆÈ°ûÂûãÁöÑÈåØË™ûÁóáÔºå‰ª•ÂèäÂÆÉÂÄëÂú®ÁâπÂÆöË™ûÈü≥ÁâáÊÆµ‰∏≠Âá∫ÁèæÁöÑ‰ΩçÁΩÆ„ÄÇ
ÊàëÂÄëÊèêÂá∫‰ΩøÁî®ÁîüÊàêÂºèÈ†êË®ìÁ∑¥ËΩâÊèõÂô® (GPT) ÁöÑÊñ∞ÊñπÊ≥ïÔºåÂæûËΩâÈåÑ‰∏≠Ë≠òÂà•ÈåØË™ûÁóáÔºå‰ª•ÂèäÂÖ©Á®ÆÁ´ØÂ∞çÁ´ØÊñπÊ≥ïÔºåÂÖ∂Â∞àÊ≥®ÊñºÂ∞áËá™ÂãïË™ûÈü≥Ëæ®Ë≠ò (ASR) ÂíåÈåØË™ûÁóáÂàÜÈ°ûÂª∫Ê®°ÁÇ∫Â§öÂÄãÂ∫èÂàóËàáÂñÆ‰∏ÄÂ∫èÂàó„ÄÇ
ÊàëÂÄëË≠âÊòéÂñÆ‰∏ÄÂ∫èÂàóÊ®°ÂûãÂú®Â§öÈ°ûÂà•ÈåØË™ûÁóáÂÅµÊ∏¨ÊñπÈù¢ÂÑ™Êñº GPT Âü∫Ê∫ñ„ÄÇ

##### **COMET: "Cone of experience" enhanced large multimodal model for mathematical problem generation**
2407.11315v1 by Sannyuya Liu, Jintian Feng, Zongkai Yang, Yawei Luo, Qian Wan, Xiaoxuan Shen, Jianwen Sun

The automatic generation of high-quality mathematical problems is practically
valuable in many educational scenarios. Large multimodal model provides a novel
technical approach for the mathematical problem generation because of its wide
success in cross-modal data scenarios. However, the traditional method of
separating problem solving from problem generation and the mainstream
fine-tuning framework of monotonous data structure with homogeneous training
objectives limit the application of large multimodal model in mathematical
problem generation. Addressing these challenges, this paper proposes COMET, a
"Cone of Experience" enhanced large multimodal model for mathematical problem
generation. Firstly, from the perspective of mutual ability promotion and
application logic, we unify stem generation and problem solving into
mathematical problem generation. Secondly, a three-stage fine-turning framework
guided by the "Cone of Experience" is proposed. The framework divides the
fine-tuning data into symbolic experience, iconic experience, and direct
experience to draw parallels with experiences in the career growth of teachers.
Several fine-grained data construction and injection methods are designed in
this framework. Finally, we construct a Chinese multimodal mathematical problem
dataset to fill the vacancy of Chinese multimodal data in this field. Combined
with objective and subjective indicators, experiments on multiple datasets
fully verify the effectiveness of the proposed framework and model.

ÊëòË¶ÅÔºöÂú®Ë®±Â§öÊïôËÇ≤Â†¥ÊôØ‰∏≠ÔºåËá™ÂãïÁî¢ÁîüÈ´òÂìÅË≥™ÁöÑÊï∏Â≠∏È°åÁõÆÂú®ÂØ¶Âãô‰∏äÂæàÊúâÂÉπÂÄº„ÄÇÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°ÂûãÂú®Ë∑®Ê®°ÊÖãË≥áÊñôÂ†¥ÊôØ‰∏≠ÂèñÂæóÂª£Ê≥õÁöÑÊàêÂäüÔºåÁÇ∫Êï∏Â≠∏È°åÁõÆÁî¢ÁîüÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊäÄË°ìÊñπÊ≥ï„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±‰∏äÂ∞áÂïèÈ°åÊ±ÇËß£ËàáÂïèÈ°åÁî¢ÁîüÂàÜÈñãÁöÑÊñπÊ≥ïÔºå‰ª•Âèä‰∏ªÊµÅÂæÆË™øÊ°ÜÊû∂‰ΩøÁî®ÂñÆË™øË≥áÊñôÁµêÊßãÊê≠ÈÖçÂêåË≥™Ë®ìÁ∑¥ÁõÆÊ®ôÔºåÈôêÂà∂‰∫ÜÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°ÂûãÂú®Êï∏Â≠∏È°åÁõÆÁî¢Áîü‰∏≠ÁöÑÊáâÁî®„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü COMETÔºå‰∏ÄÁ®Æ„ÄåÁ∂ìÈ©óÈåêÈ´î„ÄçÂ¢ûÂº∑Â§ßÂûãÂ§öÊ®°ÊÖãÊ®°ÂûãÔºåÁî®ÊñºÊï∏Â≠∏È°åÁõÆÁî¢Áîü„ÄÇÈ¶ñÂÖàÔºåÂæûÁõ∏‰∫íËÉΩÂäõ‰øÉÈÄ≤ÂíåÊáâÁî®ÈÇèËºØÁöÑËßíÂ∫¶ÔºåÊàëÂÄëÂ∞áÈ°åÂππÁî¢ÁîüÂíåÂïèÈ°åÊ±ÇËß£Áµ±‰∏ÄÂà∞Êï∏Â≠∏È°åÁõÆÁî¢Áîü‰∏≠„ÄÇÂÖ∂Ê¨°ÔºåÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî±„ÄåÁ∂ìÈ©óÈåêÈ´î„ÄçÊåáÂ∞éÁöÑ‰∏âÈöéÊÆµÂæÆË™øÊ°ÜÊû∂„ÄÇË©≤Ê°ÜÊû∂Â∞áÂæÆË™øË≥áÊñôÂàÜÁÇ∫Á¨¶ËôüÁ∂ìÈ©ó„ÄÅÂúñÂÉèÁ∂ìÈ©óÂíåÁõ¥Êé•Á∂ìÈ©óÔºå‰ª•ËàáÊïôÂ∏´ËÅ∑Ê•≠ÊàêÈï∑‰∏≠ÁöÑÁ∂ìÈ©óÁõ∏ÂëºÊáâ„ÄÇÂú®Ê≠§Ê°ÜÊû∂‰∏≠Ë®≠Ë®à‰∫ÜÂπæÁ®ÆÁ¥∞Á≤íÂ∫¶ÁöÑË≥áÊñôÂª∫ÊßãÂíåÊ≥®ÂÖ•ÊñπÊ≥ï„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂª∫Êßã‰∫Ü‰∏ÄÂÄã‰∏≠ÊñáÂ§öÊ®°ÊÖãÊï∏Â≠∏È°åÁõÆË≥áÊñôÈõÜÔºå‰ª•Â°´Ë£úË©≤È†òÂüü‰∏≠‰∏≠ÊñáÂ§öÊ®°ÊÖãË≥áÊñôÁöÑÁ©∫ÁôΩ„ÄÇÁµêÂêàÂÆ¢ËßÄÂíå‰∏ªËßÄÊåáÊ®ôÔºåÂú®Â§öÂÄãË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óÂÖÖÂàÜÈ©óË≠â‰∫ÜÊâÄÊèêÂá∫ÁöÑÊ°ÜÊû∂ÂíåÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Large Vision-Language Models as Emotion Recognizers in Context Awareness**
2407.11300v1 by Yuxuan Lei, Dingkang Yang, Zhaoyu Chen, Jiawei Chen, Peng Zhai, Lihua Zhang

Context-aware emotion recognition (CAER) is a complex and significant task
that requires perceiving emotions from various contextual cues. Previous
approaches primarily focus on designing sophisticated architectures to extract
emotional cues from images. However, their knowledge is confined to specific
training datasets and may reflect the subjective emotional biases of the
annotators. Furthermore, acquiring large amounts of labeled data is often
challenging in real-world applications. In this paper, we systematically
explore the potential of leveraging Large Vision-Language Models (LVLMs) to
empower the CAER task from three paradigms: 1) We fine-tune LVLMs on two CAER
datasets, which is the most common way to transfer large models to downstream
tasks. 2) We design zero-shot and few-shot patterns to evaluate the performance
of LVLMs in scenarios with limited data or even completely unseen. In this
case, a training-free framework is proposed to fully exploit the In-Context
Learning (ICL) capabilities of LVLMs. Specifically, we develop an image
similarity-based ranking algorithm to retrieve examples; subsequently, the
instructions, retrieved examples, and the test example are combined to feed
LVLMs to obtain the corresponding sentiment judgment. 3) To leverage the rich
knowledge base of LVLMs, we incorporate Chain-of-Thought (CoT) into our
framework to enhance the model's reasoning ability and provide interpretable
results. Extensive experiments and analyses demonstrate that LVLMs achieve
competitive performance in the CAER task across different paradigms. Notably,
the superior performance in few-shot settings indicates the feasibility of
LVLMs for accomplishing specific tasks without extensive training.

ÊëòË¶ÅÔºöÊÉÖÂ¢ÉÊÑüÁü•ÊÉÖÁ∑íËæ®Ë≠ò (CAER) ÊòØ‰∏ÄÈ†ÖË§áÈõú‰∏îÈáçË¶ÅÁöÑ‰ªªÂãôÔºåÈúÄË¶ÅÂæûÂêÑÁ®ÆÊÉÖÂ¢ÉÁ∑öÁ¥¢‰∏≠ÊÑüÁü•ÊÉÖÁ∑í„ÄÇÂÖàÂâçÁöÑÂÅöÊ≥ï‰∏ªË¶ÅÂ∞àÊ≥®ÊñºË®≠Ë®àÁ≤æÂØÜÊû∂ÊßãÔºåÂæûÂΩ±ÂÉè‰∏≠Êì∑ÂèñÊÉÖÁ∑íÁ∑öÁ¥¢„ÄÇÁÑ∂ËÄåÔºåÂÖ∂Áü•Ë≠òÂÉÖÈôêÊñºÁâπÂÆöË®ìÁ∑¥Ë≥áÊñôÈõÜÔºå‰∏îÂèØËÉΩÂèçÊò†Ê®ôË®ªËÄÖÁöÑ‰∏ªËßÄÊÉÖÁ∑íÂÅèË¶ã„ÄÇÊ≠§Â§ñÔºåÂú®ÁúüÂØ¶‰∏ñÁïåÊáâÁî®‰∏≠ÔºåÂèñÂæóÂ§ßÈáèÁöÑÊ®ôÁ±§Ë≥áÊñôÈÄöÂ∏∏ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Êé¢Ë®éÂà©Áî®Â§ßÂûãË¶ñË¶∫Ë™ûË®ÄÊ®°Âûã (LVLMs) ‰æÜÂº∑Âåñ CAER ‰ªªÂãôÁöÑ‰∏âÁ®ÆÁØÑ‰æãÔºö1) ÊàëÂÄëÂ∞çÂÖ©ÂÄã CAER Ë≥áÊñôÈõÜÂæÆË™ø LVLMsÔºåÈÄôÊòØÂ∞áÂ§ßÂûãÊ®°ÂûãËΩâÁßªÂà∞‰∏ãÊ∏∏‰ªªÂãôÊúÄÂ∏∏Ë¶ãÁöÑÊñπÊ≥ï„ÄÇ2) ÊàëÂÄëË®≠Ë®àÈõ∂Ê¨°Â≠∏ÁøíÂíåÂ∞ëÈáèÂ≠∏ÁøíÊ®°ÂºèÔºå‰ª•Ë©ï‰º∞ LVLMs Âú®Ë≥áÊñôÊúâÈôêÁîöËá≥ÂÆåÂÖ®Êú™Ë¶ãÁöÑÊÉÖÊ≥Å‰∏ãÁöÑÊïàËÉΩ„ÄÇÂú®ÈÄôÁ®ÆÊÉÖÊ≥Å‰∏ãÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂÖçË®ìÁ∑¥Êû∂ÊßãÔºå‰ª•ÂÖÖÂàÜÂà©Áî® LVLMs ÁöÑÊÉÖÂ¢ÉÂ≠∏Áøí (ICL) ËÉΩÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈñãÁôº‰∏ÄÂÄãÂü∫ÊñºÂΩ±ÂÉèÁõ∏‰ººÂ∫¶ÁöÑÊéíÂêçÊºîÁÆóÊ≥ï‰æÜÊì∑ÂèñÁØÑ‰æãÔºõÈö®ÂæåÔºåÂ∞áÊåáÁ§∫„ÄÅÊì∑ÂèñÁöÑÁØÑ‰æãÂíåÊ∏¨Ë©¶ÁØÑ‰æãÁµÑÂêàËµ∑‰æÜÔºåÊèê‰æõÁµ¶ LVLMs ‰ª•ÂèñÂæóÁõ∏ÊáâÁöÑÊÉÖÁ∑íÂà§Êñ∑„ÄÇ3) ÁÇ∫‰∫ÜÂà©Áî® LVLMs Ë±êÂØåÁöÑÁü•Ë≠òÂ∫´ÔºåÊàëÂÄëÂ∞áÊÄùËÄÉÈèà (CoT) Á¥çÂÖ•ÊàëÂÄëÁöÑÊû∂Êßã‰∏≠Ôºå‰ª•Â¢ûÂº∑Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ‰∏¶Êèê‰æõÂèØËß£ÈáãÁöÑÁµêÊûú„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óÂíåÂàÜÊûêË≠âÊòéÔºåLVLMs Âú®‰∏çÂêåÁöÑÁØÑ‰æã‰∏≠ÂØ¶Áèæ‰∫Ü CAER ‰ªªÂãôÁöÑÁ´∂Áà≠ÊïàËÉΩ„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÂú®Â∞ëÈáèÂ≠∏ÁøíË®≠ÂÆö‰∏≠ÁöÑÂÑ™Áï∞ÊïàËÉΩÔºåË°®Á§∫ LVLMs ÁÑ°ÈúÄÂª£Ê≥õË®ìÁ∑¥Âç≥ÂèØÂÆåÊàêÁâπÂÆö‰ªªÂãôÁöÑÂèØË°åÊÄß„ÄÇ

##### **Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems**
2407.11288v1 by Ya≈üar Utku Al√ßalar, Mehmet Ak√ßakaya

Diffusion models have emerged as powerful generative techniques for solving
inverse problems. Despite their success in a variety of inverse problems in
imaging, these models require many steps to converge, leading to slow inference
time. Recently, there has been a trend in diffusion models for employing
sophisticated noise schedules that involve more frequent iterations of
timesteps at lower noise levels, thereby improving image generation and
convergence speed. However, application of these ideas for solving inverse
problems with diffusion models remain challenging, as these noise schedules do
not perform well when using empirical tuning for the forward model
log-likelihood term weights. To tackle these challenges, we propose zero-shot
approximate posterior sampling (ZAPS) that leverages connections to zero-shot
physics-driven deep learning. ZAPS fixes the number of sampling steps, and uses
zero-shot training with a physics-guided loss function to learn log-likelihood
weights at each irregular timestep. We apply ZAPS to the recently proposed
diffusion posterior sampling method as baseline, though ZAPS can also be used
with other posterior sampling diffusion models. We further approximate the
Hessian of the logarithm of the prior using a diagonalization approach with
learnable diagonal entries for computational efficiency. These parameters are
optimized over a fixed number of epochs with a given computational budget. Our
results for various noisy inverse problems, including Gaussian and motion
deblurring, inpainting, and super-resolution show that ZAPS reduces inference
time, provides robustness to irregular noise schedules and improves
reconstruction quality. Code is available at https://github.com/ualcalar17/ZAPS

ÊëòË¶ÅÔºöÊì¥Êï£Ê®°ÂûãÂ∑≤ÊàêÁÇ∫Ëß£Ê±∫ÈÄÜÂïèÈ°åÁöÑÂº∑Â§ßÁîüÊàêÊäÄË°ì„ÄÇÂÑòÁÆ°ÂÆÉÂÄëÂú®ÂêÑÁ®ÆÂΩ±ÂÉèÈÄÜÂïèÈ°å‰∏≠ÂèñÂæóÊàêÂäüÔºå‰ΩÜÈÄô‰∫õÊ®°ÂûãÈúÄË¶ÅË®±Â§öÊ≠•È©üÊâçËÉΩÊî∂ÊñÇÔºåÂ∞éËá¥Êé®Ë´ñÊôÇÈñìËÆäÊÖ¢„ÄÇÊúÄËøëÔºåÊì¥Êï£Ê®°ÂûãÂá∫Áèæ‰∫Ü‰∏ÄÁ®ÆË∂®Âã¢ÔºåÊé°Áî®Á≤æÂØÜÁöÑÈõúË®äÊôÇÁ®ãÔºåÂÖ∂‰∏≠Ê∂âÂèäÂú®ËºÉ‰ΩéÈõúË®äÁ¥öÂà•‰∏ãÂ∞çÊôÇÈñìÊ≠•Èï∑ÈÄ≤Ë°åÊõ¥È†ªÁπÅÁöÑËø≠‰ª£ÔºåÂæûËÄåÊîπÂñÑÂΩ±ÂÉèÁîüÊàêÂíåÊî∂ÊñÇÈÄüÂ∫¶„ÄÇÁÑ∂ËÄåÔºåÂ∞áÈÄô‰∫õÊÉ≥Ê≥ïÊáâÁî®Êñº‰ΩøÁî®Êì¥Êï£Ê®°ÂûãËß£Ê±∫ÈÄÜÂïèÈ°å‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫ÈÄô‰∫õÈõúË®äÊôÇÁ®ãÂú®‰ΩøÁî®Á∂ìÈ©óË™øÊï¥ÈÄ≤Ë°åÂâçÂêëÊ®°ÂûãÂ∞çÊï∏‰ººÁÑ∂È†ÖÊ¨äÈáçÊôÇË°®Áèæ‰∏ç‰Ω≥„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∫õÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÈõ∂Ê¨°Ëøë‰ººÂæåÈ©óÊäΩÊ®£ (ZAPS)ÔºåÂÆÉÂà©Áî®‰∫ÜËàáÈõ∂Ê¨°Áâ©ÁêÜÈ©ÖÂãïÊ∑±Â∫¶Â≠∏ÁøíÁöÑËÅØÁπ´„ÄÇZAPS ‰øÆÂæ©‰∫ÜÊäΩÊ®£Ê≠•È©üÁöÑÊï∏ÈáèÔºå‰∏¶‰ΩøÁî®ÂÖ∑ÊúâÁâ©ÁêÜÊåáÂ∞éÊêçÂ§±ÂáΩÊï∏ÁöÑÈõ∂Ê¨°Ë®ìÁ∑¥‰æÜÂ≠∏ÁøíÊØèÂÄã‰∏çË¶èÂâáÊôÇÈñìÊ≠•Èï∑ÁöÑÂ∞çÊï∏‰ººÁÑ∂Ê¨äÈáç„ÄÇÊàëÂÄëÂ∞á ZAPS ÊáâÁî®ÊñºÊúÄËøëÊèêÂá∫ÁöÑÊì¥Êï£ÂæåÈ©óÊäΩÊ®£ÊñπÊ≥ï‰ΩúÁÇ∫Âü∫Ê∫ñÔºåÂÑòÁÆ° ZAPS ‰πüÂèØÁî®ÊñºÂÖ∂‰ªñÂæåÈ©óÊäΩÊ®£Êì¥Êï£Ê®°Âûã„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®ÂÖ∑ÊúâÂèØÂ≠∏ÁøíÂ∞çËßíÁ∑öÊ¢ùÁõÆÁöÑÂ∞çËßíÂåñÊñπÊ≥ï‰æÜËøë‰ººÂÖàÈ©óÂ∞çÊï∏ÁöÑ HessianÔºå‰ª•ÊèêÈ´òË®àÁÆóÊïàÁéá„ÄÇÈÄô‰∫õÂèÉÊï∏Âú®Áµ¶ÂÆöÁöÑË®àÁÆóÈ†êÁÆó‰∏ãÁ∂ìÈÅéÂõ∫ÂÆöÊï∏ÈáèÁöÑ epoch ÂÑ™Âåñ„ÄÇÊàëÂÄëÂ∞çÂêÑÁ®ÆÈõúË®äÈÄÜÂïèÈ°åÔºàÂåÖÊã¨È´òÊñØÂíåÈÅãÂãïÂéªÊ®°Á≥ä„ÄÅ‰øÆÂæ©ÂíåË∂ÖËß£ÊûêÂ∫¶ÔºâÁöÑÁµêÊûúË°®ÊòéÔºåZAPS Ê∏õÂ∞ë‰∫ÜÊé®Ë´ñÊôÇÈñìÔºåÊèê‰æõ‰∫ÜÂ∞ç‰∏çË¶èÂâáÈõúË®äÊôÇÁ®ãÁöÑÈ≠ØÊ£íÊÄßÔºå‰∏¶ÊèêÈ´ò‰∫ÜÈáçÂª∫ÂìÅË≥™„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/ualcalar17/ZAPS ÂèñÂæó

##### **CLAMS: A System for Zero-Shot Model Selection for Clustering**
2407.11286v1 by Prabhant Singh, Pieter Gijsbers, Murat Onur Yildirim, Elif Ceren Gok, Joaquin Vanschoren

We propose an AutoML system that enables model selection on clustering
problems by leveraging optimal transport-based dataset similarity. Our
objective is to establish a comprehensive AutoML pipeline for clustering
problems and provide recommendations for selecting the most suitable
algorithms, thus opening up a new area of AutoML beyond the traditional
supervised learning settings. We compare our results against multiple
clustering baselines and find that it outperforms all of them, hence
demonstrating the utility of similarity-based automated model selection for
solving clustering applications.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãËá™ÂãïÊ©üÂô®Â≠∏ÁøíÁ≥ªÁµ±ÔºåÂÆÉÈÄèÈÅéÂà©Áî®ÊúÄ‰Ω≥ÂÇ≥Ëº∏Âü∫Á§éÁöÑË≥áÊñôÈõÜÁõ∏‰ººÊÄßÔºåÂú®ÂàÜÁæ§ÂïèÈ°å‰∏äÈÄ≤Ë°åÊ®°ÂûãÈÅ∏Êìá„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÂª∫Á´ã‰∏ÄÂÄãÂÖ®Èù¢ÁöÑËá™ÂãïÊ©üÂô®Â≠∏ÁøíÁÆ°ÈÅìÔºå‰ª•Ëß£Ê±∫ÂàÜÁæ§ÂïèÈ°åÔºå‰∏¶Êèê‰æõÂª∫Ë≠∞Ôºå‰ª•ÈÅ∏ÊìáÊúÄÂêàÈÅ©ÁöÑÊºîÁÆóÊ≥ïÔºåÂæûËÄåÈñãÂïü‰∏ÄÂÄãË∂ÖË∂äÂÇ≥Áµ±Áõ£Áù£ÂºèÂ≠∏ÁøíË®≠ÂÆöÁöÑËá™ÂãïÊ©üÂô®Â≠∏ÁøíÊñ∞È†òÂüü„ÄÇÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÁµêÊûúËàáÂ§öÂÄãÂàÜÁæ§Âü∫Ê∫ñÈÄ≤Ë°åÊØîËºÉÔºå‰∏¶ÁôºÁèæÂÆÉÂÑ™ÊñºÊâÄÊúâÂü∫Ê∫ñÔºåÂõ†Ê≠§Ë≠âÊòé‰∫ÜÂü∫ÊñºÁõ∏‰ººÊÄßÁöÑËá™ÂãïÂåñÊ®°ÂûãÈÅ∏ÊìáÂ∞çÊñºËß£Ê±∫ÂàÜÁæ§ÊáâÁî®Á®ãÂºèÁöÑÊïàÁî®„ÄÇ

##### **Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models**
2407.11282v2 by Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang

Large Language Models (LLMs) are employed across various high-stakes domains,
where the reliability of their outputs is crucial. One commonly used method to
assess the reliability of LLMs' responses is uncertainty estimation, which
gauges the likelihood of their answers being correct. While many studies focus
on improving the accuracy of uncertainty estimations for LLMs, our research
investigates the fragility of uncertainty estimation and explores potential
attacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,
when activated by a specific trigger in the input, manipulates the model's
uncertainty without affecting the final output. Specifically, the proposed
backdoor attack method can alter an LLM's output probability distribution,
causing the probability distribution to converge towards an attacker-predefined
distribution while ensuring that the top-1 prediction remains unchanged. Our
experimental results demonstrate that this attack effectively undermines the
model's self-evaluation reliability in multiple-choice questions. For instance,
we achieved a 100 attack success rate (ASR) across three different triggering
strategies in four models. Further, we investigate whether this manipulation
generalizes across different prompts and domains. This work highlights a
significant threat to the reliability of LLMs and underscores the need for
future defenses against such attacks. The code is available at
https://github.com/qcznlp/uncertainty_attack.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Ë¢´Áî®ÊñºÂêÑÁ®ÆÈ´òÈ¢®Èö™È†òÂüü‰∏≠ÔºåÂÖ∂‰∏≠ÂÖ∂Ëº∏Âá∫ÁöÑÂèØÈù†ÊÄßËá≥ÈóúÈáçË¶Å„ÄÇ‰∏ÄÁ®ÆÂ∏∏Áî®ÁöÑÊñπÊ≥ï‰æÜË©ï‰º∞ LLM ÂõûÊáâÁöÑÂèØÈù†ÊÄßÊòØ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÔºåÂÆÉË°°ÈáèÂÖ∂Á≠îÊ°àÊ≠£Á¢∫ÁöÑÂèØËÉΩÊÄß„ÄÇÂÑòÁÆ°Ë®±Â§öÁ†îÁ©∂Â∞àÊ≥®ÊñºÊèêÈ´ò LLM ‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÁöÑÊ∫ñÁ¢∫ÊÄßÔºå‰ΩÜÊàëÂÄëÁöÑÁ†îÁ©∂Ë™øÊü•‰∫Ü‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÁöÑËÑÜÂº±ÊÄß‰∏¶Êé¢Á¥¢‰∫ÜÊΩõÂú®ÁöÑÊîªÊìä„ÄÇÊàëÂÄëË≠âÊòéÔºåÊîªÊìäËÄÖÂèØ‰ª•Âú® LLM ‰∏≠ÂµåÂÖ•ÂæåÈñÄÔºåÁï∂Ëº∏ÂÖ•‰∏≠ÁöÑÁâπÂÆöËß∏ÁôºÂô®ÊøÄÊ¥ªÊôÇÔºåÂÆÉÊúÉÂú®‰∏çÂΩ±ÈüøÊúÄÁµÇËº∏Âá∫ÁöÑÊÉÖÊ≥Å‰∏ãÊìçÁ∏±Ê®°ÂûãÁöÑ‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊâÄÊèêÂá∫ÁöÑÂæåÈñÄÊîªÊìäÊñπÊ≥ïÂèØ‰ª•ÊîπËÆä LLM ÁöÑËº∏Âá∫Ê¶ÇÁéáÂàÜ‰ΩàÔºåÂ∞éËá¥Ê¶ÇÁéáÂàÜ‰ΩàÊúùËëóÊîªÊìäËÄÖÈ†êÂÖàÂÆöÁæ©ÁöÑÂàÜ‰ΩàÊî∂ÊñÇÔºåÂêåÊôÇÁ¢∫‰øù top-1 È†êÊ∏¨‰øùÊåÅ‰∏çËÆä„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÈÄôÁ®ÆÊîªÊìäÊúâÊïàÂú∞Á†¥Â£û‰∫ÜÊ®°ÂûãÂú®Â§öÈÅ∏È°å‰∏≠ÁöÑËá™ÊàëË©ï‰º∞ÂèØÈù†ÊÄß„ÄÇ‰æãÂ¶ÇÔºåÊàëÂÄëÂú®ÂõõÂÄãÊ®°Âûã‰∏≠ÁöÑ‰∏âÁ®Æ‰∏çÂêåÁöÑËß∏ÁôºÁ≠ñÁï•‰∏≠ÂØ¶Áèæ‰∫Ü 100 ÁöÑÊîªÊìäÊàêÂäüÁéá (ASR)„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ†îÁ©∂‰∫ÜÈÄôÁ®ÆÊìçÁ∏±ÊòØÂê¶Âú®‰∏çÂêåÁöÑÊèêÁ§∫ÂíåÈ†òÂüü‰∏≠ÂæóÂà∞Êé®Âª£„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÂº∑Ë™ø‰∫ÜÂ∞ç LLM ÂèØÈù†ÊÄßÁöÑÈáçÂ§ßÂ®ÅËÑÖÔºå‰∏¶Âº∑Ë™ø‰∫ÜÊú™‰æÜÈúÄË¶ÅÂ∞çÊ≠§È°ûÊîªÊìäÈÄ≤Ë°åÈò≤Á¶¶„ÄÇ‰ª£Á¢ºÂèØÂú® https://github.com/qcznlp/uncertainty_attack ‰∏≠ÊâæÂà∞„ÄÇ

##### **Quality Scalable Quantization Methodology for Deep Learning on Edge**
2407.11260v1 by Salman Abdul Khaliq, Rehan Hafiz

Deep Learning Architectures employ heavy computations and bulk of the
computational energy is taken up by the convolution operations in the
Convolutional Neural Networks. The objective of our proposed work is to reduce
the energy consumption and size of CNN for using machine learning techniques in
edge computing on ubiquitous computing devices. We propose Systematic Quality
Scalable Design Methodology consisting of Quality Scalable Quantization on a
higher abstraction level and Quality Scalable Multipliers at lower abstraction
level. The first component consists of parameter compression where we
approximate representation of values in filters of deep learning models by
encoding in 3 bits. A shift and scale based on-chip decoding hardware is
proposed which can decode these 3-bit representations to recover approximate
filter values. The size of the DNN model is reduced this way and can be sent
over a communication channel to be decoded on the edge computing devices. This
way power is reduced by limiting data bits by approximation. In the second
component we propose a quality scalable multiplier which reduces the number of
partial products by converting numbers in canonic sign digit representations
and further approximating the number by reducing least significant bits. These
quantized CNNs provide almost same ac-curacy as network with original weights
with little or no fine-tuning. The hardware for the adaptive multipliers
utilize gate clocking for reducing energy consumption during multiplications.
The proposed methodology greatly reduces the memory and power requirements of
DNN models making it a feasible approach to deploy Deep Learning on edge
computing. The experiments done on LeNet and ConvNets show an increase upto 6%
of zeros and memory savings upto 82.4919% while keeping the accuracy near the
state of the art.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Â≠∏ÁøíÊû∂ÊßãÊé°Áî®Â§ßÈáèÁöÑÈÅãÁÆóÔºåËÄåÂ§ßÈÉ®ÂàÜÁöÑÈÅãÁÆóËÉΩÈáèÈÉΩË¢´Âç∑Á©çÁ•ûÁ∂ìÁ∂≤Ë∑Ø‰∏≠ÁöÑÂç∑Á©çÈÅãÁÆóÊâÄÂê∏Êî∂„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÂ∑•‰ΩúÁõÆÊ®ôÊòØÈôç‰Ωé CNN ÁöÑËÉΩËÄóÂíåÂ§ßÂ∞èÔºå‰ª•‰æøÂú®ÊôÆÈÅ©ÈÅãÁÆóË£ùÁΩÆ‰∏äÁöÑÈÇäÁ∑£ÈÅãÁÆó‰∏≠‰ΩøÁî®Ê©üÂô®Â≠∏ÁøíÊäÄË°ì„ÄÇÊàëÂÄëÊèêÂá∫Á≥ªÁµ±ÂåñÂìÅË≥™ÂèØÊì¥ÂÖÖË®≠Ë®àÊñπÊ≥ïÔºåÂÖ∂‰∏≠ÂåÖÂê´ËºÉÈ´òÊäΩË±°Â±§Á¥öÁöÑÂìÅË≥™ÂèØÊì¥ÂÖÖÈáèÂåñÂíåËºÉ‰ΩéÊäΩË±°Â±§Á¥öÁöÑÂìÅË≥™ÂèØÊì¥ÂÖÖ‰πòÊ≥ïÂô®„ÄÇÁ¨¨‰∏ÄÂÄãÁµÑ‰ª∂ÂåÖÂê´ÂèÉÊï∏Â£ìÁ∏ÆÔºåÂÖ∂‰∏≠ÊàëÂÄëÈÄèÈÅé 3 ‰ΩçÂÖÉÁ∑®Á¢º‰æÜËøë‰ººË°®Á§∫Ê∑±Â∫¶Â≠∏ÁøíÊ®°Âûã‰∏≠ÊøæÊ≥¢Âô®ÁöÑÂÄº„ÄÇÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº‰ΩçÁßªÂíåÁ∏ÆÊîæÁöÑÊô∂ÁâáËß£Á¢ºÁ°¨È´îÔºåÂÆÉÂèØ‰ª•Ëß£Á¢ºÈÄô‰∫õ 3 ‰ΩçÂÖÉË°®Á§∫‰ª•ÈÇÑÂéüËøë‰ººÁöÑÊøæÊ≥¢Âô®ÂÄº„ÄÇDNN Ê®°ÂûãÁöÑÂ§ßÂ∞èÊúÉ‰ª•ÈÄôÁ®ÆÊñπÂºèÁ∏ÆÂ∞èÔºå‰∏¶‰∏îÂèØ‰ª•ÈÄèÈÅéÈÄöË®äÁÆ°ÈÅìÂÇ≥ÈÄÅÔºå‰ª•‰æøÂú®ÈÇäÁ∑£ÈÅãÁÆóË£ùÁΩÆ‰∏äËß£Á¢º„ÄÇÈÄôÁ®ÆÊñπÂºèÂèØÈÄèÈÅéËøë‰ºº‰æÜÈôêÂà∂Ë≥áÊñô‰ΩçÂÖÉÔºåÈÄ≤ËÄåÈôç‰ΩéÂäüËÄó„ÄÇÂú®Á¨¨‰∫åÂÄãÁµÑ‰ª∂‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂìÅË≥™ÂèØÊì¥ÂÖÖ‰πòÊ≥ïÂô®ÔºåÂÆÉÈÄèÈÅéÂ∞áÊï∏Â≠óËΩâÊèõÁÇ∫Ê≠£Ë¶èÁ¨¶Ëôü‰ΩçÂÖÉË°®Á§∫Ôºå‰∏¶ÈÄ≤‰∏ÄÊ≠•ÈÄèÈÅéÊ∏õÂ∞ëÊúÄ‰ΩéÊúâÊïà‰ΩçÂÖÉ‰æÜËøë‰ººÊï∏Â≠óÔºåÈÄ≤ËÄåÊ∏õÂ∞ëÈÉ®ÂàÜ‰πòÁ©çÁöÑÊï∏Èáè„ÄÇÈÄô‰∫õÈáèÂåñÁöÑ CNN Êèê‰æõÂπæ‰πéËàáÂéüÂßãÊ¨äÈáçÁöÑÁ∂≤Ë∑ØÁõ∏ÂêåÁöÑÊ∫ñÁ¢∫Â∫¶ÔºåÂπæ‰πé‰∏çÈúÄË¶ÅÂæÆË™ø„ÄÇËá™ÈÅ©Êáâ‰πòÊ≥ïÂô®ÁöÑÁ°¨È´îÂà©Áî®ÈñòÊ•µÊôÇËÑà‰æÜÈôç‰Ωé‰πòÊ≥ïÈÅãÁÆó‰∏≠ÁöÑËÉΩËÄó„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂ§ßÂπÖÈôç‰Ωé‰∫Ü DNN Ê®°ÂûãÁöÑË®òÊÜ∂È´îÂíåÂäüËÄóÈúÄÊ±ÇÔºå‰ΩøÂÖ∂ÊàêÁÇ∫Âú®ÈÇäÁ∑£ÈÅãÁÆó‰∏≠ÈÉ®ÁΩ≤Ê∑±Â∫¶Â≠∏ÁøíÁöÑÂèØË°åÊñπÊ≥ï„ÄÇÂú® LeNet Âíå ConvNets ‰∏äÈÄ≤Ë°åÁöÑÂØ¶È©óÈ°ØÁ§∫ÔºåÈõ∂ÁöÑÂ¢ûÂä†ÂπÖÂ∫¶È´òÈÅî 6%ÔºåË®òÊÜ∂È´îÁØÄÁúÅÂπÖÂ∫¶È´òÈÅî 82.4919%ÔºåÂêåÊôÇÂ∞áÊ∫ñÁ¢∫Â∫¶Á∂≠ÊåÅÂú®Êé•ËøëÊúÄÂÖàÈÄ≤ÁöÑÊ∞¥Âπ≥„ÄÇ

##### **Pacer and Runner: Cooperative Learning Framework between Single- and Cross-Domain Sequential Recommendation**
2407.11245v1 by Chung Park, Taesan Kim, Hyungjun Yoon, Junui Hong, Yelim Yu, Mincheol Cho, Minsung Choi, Jaegul Choo

Cross-Domain Sequential Recommendation (CDSR) improves recommendation
performance by utilizing information from multiple domains, which contrasts
with Single-Domain Sequential Recommendation (SDSR) that relies on a historical
interaction within a specific domain. However, CDSR may underperform compared
to the SDSR approach in certain domains due to negative transfer, which occurs
when there is a lack of relation between domains or different levels of data
sparsity. To address the issue of negative transfer, our proposed CDSR model
estimates the degree of negative transfer of each domain and adaptively assigns
it as a weight factor to the prediction loss, to control gradient flows through
domains with significant negative transfer. To this end, our model compares the
performance of a model trained on multiple domains (CDSR) with a model trained
solely on the specific domain (SDSR) to evaluate the negative transfer of each
domain using our asymmetric cooperative network. In addition, to facilitate the
transfer of valuable cues between the SDSR and CDSR tasks, we developed an
auxiliary loss that maximizes the mutual information between the representation
pairs from both tasks on a per-domain basis. This cooperative learning between
SDSR and CDSR tasks is similar to the collaborative dynamics between pacers and
runners in a marathon. Our model outperformed numerous previous works in
extensive experiments on two real-world industrial datasets across ten service
domains. We also have deployed our model in the recommendation system of our
personal assistant app service, resulting in 21.4% increase in click-through
rate compared to existing models, which is valuable to real-world business.

ÊëòË¶ÅÔºöË∑®Á∂≤ÂüüÂ∫èÂàóÊé®Ëñ¶ (CDSR) ÈÄèÈÅéÂà©Áî®Â§öÂÄãÁ∂≤Âüü‰∏≠ÁöÑË≥áË®ä‰æÜÊèêÂçáÊé®Ëñ¶ÊïàËÉΩÔºåÈÄôËàá‰æùË≥¥ÁâπÂÆöÁ∂≤ÂüüÂÖßÊ≠∑Âè≤‰∫íÂãïÁöÑÂñÆ‰∏ÄÁ∂≤ÂüüÂ∫èÂàóÊé®Ëñ¶ (SDSR) ÂΩ¢ÊàêÂ∞çÊØî„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºË≤†Èù¢ËΩâÁßªÔºåCDSR Âú®Êüê‰∫õÁ∂≤Âüü‰∏≠ÂèØËÉΩË°®Áèæ‰∏çÂ¶Ç SDSR ÊñπÊ≥ïÔºåË≤†Èù¢ËΩâÁßªÁôºÁîüÂú®Á∂≤Âüü‰πãÈñìÁº∫‰πèÈóúËÅØÊàñË≥áÊñôÁ®ÄÁñèÁ®ãÂ∫¶‰∏çÂêåÊôÇ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫Ë≤†Èù¢ËΩâÁßªÁöÑÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫ÁöÑ CDSR Ê®°ÂûãÊúÉ‰º∞Ë®àÊØèÂÄãÁ∂≤ÂüüÁöÑË≤†Èù¢ËΩâÁßªÁ®ãÂ∫¶Ôºå‰∏¶Â∞áÂÖ∂Ëá™ÈÅ©ÊáâÂú∞ÊåáÂÆöÁÇ∫È†êÊ∏¨ÊêçÂ§±ÁöÑÊ¨äÈáçÂõ†Â≠êÔºå‰ª•ÊéßÂà∂ÈÄèÈÅéÂÖ∑ÊúâÈ°ØËëóË≤†Èù¢ËΩâÁßªÁöÑÁ∂≤ÂüüÁöÑÊ¢ØÂ∫¶ÊµÅ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÁöÑÊ®°ÂûãÊúÉÊØîËºÉÂú®Â§öÂÄãÁ∂≤Âüü (CDSR) ‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãËàáÂÉÖÂú®ÁâπÂÆöÁ∂≤Âüü (SDSR) ‰∏äË®ìÁ∑¥ÁöÑÊ®°ÂûãÁöÑÊïàËÉΩÔºå‰ª•‰ΩøÁî®ÊàëÂÄëÁöÑÈùûÂ∞çÁ®±Âçî‰ΩúÁ∂≤Ë∑ØË©ï‰º∞ÊØèÂÄãÁ∂≤ÂüüÁöÑË≤†Èù¢ËΩâÁßª„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫Ü‰øÉÈÄ≤ SDSR Âíå CDSR ‰ªªÂãô‰πãÈñìÊúâÂÉπÂÄºÁ∑öÁ¥¢ÁöÑËΩâÁßªÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãËºîÂä©ÊêçÂ§±Ôºå‰ª•ÊúÄÂ§ßÂåñ‰æÜËá™ÂÖ©ÂÄã‰ªªÂãôÁöÑË°®Á§∫Â∞çÂú®ÊØèÂÄãÁ∂≤ÂüüÂü∫Á§é‰∏äÁöÑ‰∫íÊÉ†Ë≥áË®ä„ÄÇSDSR Âíå CDSR ‰ªªÂãô‰πãÈñìÁöÑÈÄôÁ®ÆÂçî‰ΩúÂ≠∏ÁøíÈ°û‰ººÊñºÈ¶¨ÊãâÊùæÊØîË≥Ω‰∏≠ÈÖçÈÄüÂì°ÂíåË∑ëËÄÖ‰πãÈñìÁöÑÂçî‰ΩúÂãïÊÖã„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú®ÂÖ©ÂÄãÁúüÂØ¶‰∏ñÁïåÁî¢Ê•≠Ë≥áÊñôÈõÜ‰∏≠ÁöÑÂçÅÂÄãÊúçÂãôÁ∂≤Âüü‰∏≠ÔºåÂú®Âª£Ê≥õÁöÑÂØ¶È©ó‰∏≠Ë°®ÁèæÂÑ™ÊñºË®±Â§öÂÖàÂâçÁöÑ‰ΩúÂìÅ„ÄÇÊàëÂÄë‰πüÂ∑≤Â∞áÊàëÂÄëÁöÑÊ®°ÂûãÈÉ®ÁΩ≤Âú®ÊàëÂÄëÁöÑÂÄã‰∫∫Âä©ÁêÜÊáâÁî®Á®ãÂºèÊúçÂãôÁöÑÊé®Ëñ¶Á≥ªÁµ±‰∏≠ÔºåËàáÁèæÊúâÊ®°ÂûãÁõ∏ÊØîÔºåÈªûÊìäÁéáÂ¢ûÂä†‰∫Ü 21.4%ÔºåÈÄôÂ∞çÊñºÁúüÂØ¶‰∏ñÁïåÁöÑÊ•≠Âãô‰æÜË™™ÊòØÊúâÂÉπÂÄºÁöÑ„ÄÇ

