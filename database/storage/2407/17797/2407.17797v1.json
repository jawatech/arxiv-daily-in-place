{"2407.17797": {"publish_time": "2024-07-25", "title": "A Unified Understanding of Adversarial Vulnerability Regarding Unimodal Models and Vision-Language Pre-training Models", "paper_summary": "With Vision-Language Pre-training (VLP) models demonstrating powerful\nmultimodal interaction capabilities, the application scenarios of neural\nnetworks are no longer confined to unimodal domains but have expanded to more\ncomplex multimodal V+L downstream tasks. The security vulnerabilities of\nunimodal models have been extensively examined, whereas those of VLP models\nremain challenging. We note that in CV models, the understanding of images\ncomes from annotated information, while VLP models are designed to learn image\nrepresentations directly from raw text. Motivated by this discrepancy, we\ndeveloped the Feature Guidance Attack (FGA), a novel method that uses text\nrepresentations to direct the perturbation of clean images, resulting in the\ngeneration of adversarial images. FGA is orthogonal to many advanced attack\nstrategies in the unimodal domain, facilitating the direct application of rich\nresearch findings from the unimodal to the multimodal scenario. By\nappropriately introducing text attack into FGA, we construct Feature Guidance\nwith Text Attack (FGA-T). Through the interaction of attacking two modalities,\nFGA-T achieves superior attack effects against VLP models. Moreover,\nincorporating data augmentation and momentum mechanisms significantly improves\nthe black-box transferability of FGA-T. Our method demonstrates stable and\neffective attack capabilities across various datasets, downstream tasks, and\nboth black-box and white-box settings, offering a unified baseline for\nexploring the robustness of VLP models.", "paper_summary_zh": "\u900f\u904e\u5c55\u73fe\u5f37\u5927\u7684\u591a\u6a21\u614b\u4e92\u52d5\u80fd\u529b\uff0c\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4 (VLP) \u6a21\u578b\u8b93\u795e\u7d93\u7db2\u8def\u7684\u61c9\u7528\u5834\u666f\u4e0d\u518d\u4fb7\u9650\u65bc\u55ae\u6a21\u614b\u9818\u57df\uff0c\u800c\u662f\u64f4\u5c55\u5230\u66f4\u8907\u96dc\u7684\u591a\u6a21\u614b V+L \u4e0b\u6e38\u4efb\u52d9\u3002\u55ae\u6a21\u614b\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u5df2\u88ab\u5ee3\u6cdb\u63a2\u8a0e\uff0c\u800c VLP \u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u4ecd\u5177\u6709\u6311\u6230\u6027\u3002\u6211\u5011\u6ce8\u610f\u5230\u5728 CV \u6a21\u578b\u4e2d\uff0c\u5c0d\u5f71\u50cf\u7684\u7406\u89e3\u4f86\u81ea\u65bc\u6a19\u8a3b\u8cc7\u8a0a\uff0c\u800c VLP \u6a21\u578b\u5247\u88ab\u8a2d\u8a08\u70ba\u76f4\u63a5\u5f9e\u539f\u59cb\u6587\u5b57\u4e2d\u5b78\u7fd2\u5f71\u50cf\u8868\u793a\u3002\u57fa\u65bc\u6b64\u5dee\u7570\uff0c\u6211\u5011\u958b\u767c\u4e86\u7279\u5fb5\u5f15\u5c0e\u653b\u64ca (FGA)\uff0c\u9019\u662f\u4e00\u7a2e\u4f7f\u7528\u6587\u5b57\u8868\u793a\u4f86\u5f15\u5c0e\u4e7e\u6de8\u5f71\u50cf\u64fe\u52d5\u7684\u65b0\u65b9\u6cd5\uff0c\u9032\u800c\u7522\u751f\u5c0d\u6297\u6027\u5f71\u50cf\u3002FGA \u8207\u55ae\u6a21\u614b\u9818\u57df\u4e2d\u7684\u8a31\u591a\u9032\u968e\u653b\u64ca\u7b56\u7565\u6b63\u4ea4\uff0c\u4fc3\u6210\u4e86\u8c50\u5bcc\u7684\u7814\u7a76\u767c\u73fe\u5f9e\u55ae\u6a21\u614b\u76f4\u63a5\u61c9\u7528\u5230\u591a\u6a21\u614b\u5834\u666f\u3002\u900f\u904e\u9069\u7576\u5730\u5c07\u6587\u5b57\u653b\u64ca\u5f15\u5165 FGA\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u63a1\u7528\u6587\u5b57\u653b\u64ca\u7684\u7279\u5fb5\u5f15\u5c0e (FGA-T)\u3002\u900f\u904e\u5169\u500b\u6a21\u614b\u7684\u4e92\u52d5\u653b\u64ca\uff0cFGA-T \u5c0d VLP \u6a21\u578b\u9054\u5230\u4e86\u512a\u7570\u7684\u653b\u64ca\u6548\u679c\u3002\u6b64\u5916\uff0c\u52a0\u5165\u8cc7\u6599\u64f4\u5145\u548c\u52d5\u91cf\u6a5f\u5236\u986f\u8457\u6539\u5584\u4e86 FGA-T \u7684\u9ed1\u76d2\u8f49\u79fb\u6027\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u3001\u4e0b\u6e38\u4efb\u52d9\u4ee5\u53ca\u9ed1\u76d2\u548c\u767d\u76d2\u8a2d\u5b9a\u4e2d\u5c55\u73fe\u4e86\u7a69\u5b9a\u4e14\u6709\u6548\u7684\u653b\u64ca\u80fd\u529b\uff0c\u70ba\u63a2\u7d22 VLP \u6a21\u578b\u7684\u7a69\u5065\u6027\u63d0\u4f9b\u4e86\u7d71\u4e00\u7684\u57fa\u7dda\u3002", "author": "Haonan Zheng et.al.", "authors": "Haonan Zheng, Xinyang Deng, Wen Jiang, Wenrui Li", "id": "2407.17797v1", "paper_url": "http://arxiv.org/abs/2407.17797v1", "repo": "null"}}