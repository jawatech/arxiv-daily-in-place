{"2502.04923": {"publish_time": "2025-02-07", "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation", "paper_summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.", "paper_summary_zh": "\u4f4e\u79e9\u9069\u61c9 (LoRA) \u5df2\u6210\u70ba\u6587\u5b57\u8f49\u5716\u50cf\u6a21\u578b\u4e2d\u5ee3\u6cdb\u63a1\u7528\u7684\u6280\u8853\uff0c\u80fd\u7cbe\u6e96\u5448\u73fe\u591a\u500b\u4e0d\u540c\u7684\u5143\u7d20\uff0c\u4f8b\u5982\u5b57\u5143\u548c\u6a23\u5f0f\uff0c\u4ee5\u7522\u751f\u591a\u6982\u5ff5\u7684\u5716\u50cf\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u4f5c\u6cd5\u5728\u70ba\u591a\u6982\u5ff5\u5716\u50cf\u751f\u6210\u7d44\u5408\u9019\u4e9b LoRA \u6642\u9762\u81e8\u91cd\u5927\u7684\u6311\u6230\uff0c\u5c0e\u81f4\u7522\u751f\u7684\u5716\u50cf\u54c1\u8cea\u4e0b\u964d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6700\u521d\u900f\u904e\u5085\u7acb\u8449\u983b\u57df\u7684\u900f\u93e1\u4f86\u63a2\u8a0e LoRA \u5728\u53bb\u566a\u904e\u7a0b\u4e2d\u7684\u89d2\u8272\u3002\u6839\u64da\u61c9\u7528\u591a\u500b LoRA \u53ef\u80fd\u6703\u5c0e\u81f4\u300c\u8a9e\u7fa9\u885d\u7a81\u300d\u7684\u5047\u8a2d\uff0c\u6211\u5011\u767c\u73fe\u67d0\u4e9b LoRA \u6703\u653e\u5927\u9ad8\u983b\u7387\u7279\u5fb5\uff0c\u4f8b\u5982\u908a\u7de3\u548c\u7d0b\u7406\uff0c\u800c\u5176\u4ed6 LoRA \u5247\u4e3b\u8981\u95dc\u6ce8\u4f4e\u983b\u7387\u5143\u7d20\uff0c\u5305\u62ec\u6574\u9ad4\u7d50\u69cb\u548c\u5e73\u6ed1\u7684\u8272\u5f69\u6f38\u5c64\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u57fa\u65bc\u983b\u57df\u7684\u6392\u5e8f\u7b56\u7565\uff0c\u4ee5\u78ba\u5b9a\u5728\u63a8\u7406\u671f\u9593\u6574\u5408 LoRA \u7684\u6700\u4f73\u9806\u5e8f\u3002\u8207\u73fe\u6709 LoRA \u878d\u5408\u6280\u8853\u4e2d\u5e38\u898b\u7684\u6a38\u7d20\u6574\u5408\u76f8\u6bd4\uff0c\u6b64\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u7a2e\u6709\u689d\u7406\u4e14\u53ef\u6982\u62ec\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u70ba\u4e86\u5728\u591a LoRA \u7d44\u5408\u4efb\u52d9\u4e2d\u5145\u5206\u5229\u7528\u6211\u5011\u63d0\u51fa\u7684 LoRA \u9806\u5e8f\u5e8f\u5217\u6c7a\u5b9a\u65b9\u6cd5\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5275\u65b0\u7684\u514d\u8a13\u7df4\u67b6\u69cb\uff0c\u7a31\u70ba\u5feb\u53d6\u591a LoRA (CMLoRA)\uff0c\u5176\u8a2d\u8a08\u76ee\u7684\u5728\u65bc\u6709\u6548\u6574\u5408\u591a\u500b LoRA\uff0c\u540c\u6642\u7dad\u6301\u4e00\u81f4\u7684\u5716\u50cf\u751f\u6210\u3002CMLoRA \u64c1\u6709\u9748\u6d3b\u7684\u591a LoRA \u878d\u5408\u4e3b\u5e79\uff0c\u4ee5\u53ca\u91dd\u5c0d\u500b\u5225 LoRA \u91cf\u8eab\u6253\u9020\u7684\u975e\u5747\u52fb\u5feb\u53d6\u7b56\u7565\uff0c\u6709\u6f5b\u529b\u6e1b\u5c11 LoRA \u7d44\u5408\u4e2d\u7684\u8a9e\u7fa9\u885d\u7a81\uff0c\u4e26\u63d0\u5347\u904b\u7b97\u6548\u7387\u3002\u6211\u5011\u7684\u5be6\u9a57\u8a55\u4f30\u8b49\u660e\uff0cCMLoRA \u7684\u8868\u73fe\u986f\u8457\u512a\u65bc\u6700\u5148\u9032\u7684\u514d\u8a13\u7df4 LoRA \u878d\u5408\u65b9\u6cd5\uff0c\u8207 LoraHub\u3001LoRA Composite \u548c LoRA Switch \u76f8\u6bd4\uff0c\u5728 CLIPScore \u4e2d\u5e73\u5747\u63d0\u5347\u4e86 $2.19\\%$\uff0c\u5728 MLLM \u52dd\u7387\u4e2d\u63d0\u5347\u4e86 $11.25\\%$\u3002", "author": "Xiandong Zou et.al.", "authors": "Xiandong Zou, Mingzhu Shen, Christos-Savvas Bouganis, Yiren Zhao", "id": "2502.04923v1", "paper_url": "http://arxiv.org/abs/2502.04923v1", "repo": "null"}}