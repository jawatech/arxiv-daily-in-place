{"2501.12432": {"publish_time": "2025-01-21", "title": "Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation", "paper_summary": "Although current Large Language Models (LLMs) exhibit impressive\ncapabilities, performing complex real-world tasks still requires tool learning.\nMainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to\ninteract with external environments, but they are limited in perceptual scope\nand lack adequate task-planning capability. To address these limitations, other\nstudies introduce the first Search-based Decision Tree (DFSDT), which still\nsuffers from the high computational cost. In this paper, we introduce a novel\nparallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).\nFirst, we transform traditional tree-based tool search paths into Directed\nAcyclic Graph (DAG) structure, generating a high-quality parallel tool\ninvocation dataset. The DTA-Llama is then trained on the dataset to learn to\niteratively divide the current task into several parallel tool invocation\nsub-tasks and aggregate the invocation results to decide the next actions.\nFurthermore, we introduce an efficient inference framework inspired by the\nProcess/Threads mechanism when applying the DTA-Llama to practical tasks.\nExperimental results show that our approach substantially enhances task\nperformance while reducing token consumption and inference time. Llama2-7B,\nusing our method, is comparable to the official parallel function calling\nmethod of GPT-3.5. The relevant code, dataset, and model weights are available\nat https://corn0205.github.io/", "paper_summary_zh": "\u5118\u7ba1\u76ee\u524d\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u57f7\u884c\u8907\u96dc\u7684\u771f\u5be6\u4e16\u754c\u4efb\u52d9\u4ecd\u9700\u8981\u5de5\u5177\u5b78\u7fd2\u3002\u4e3b\u6d41\u65b9\u6cd5\uff08\u4f8b\u5982 CoT/ReAct\uff09\u4f9d\u8cf4\u9010\u6b65\u5de5\u5177\u547c\u53eb\u8207\u5916\u90e8\u74b0\u5883\u4e92\u52d5\uff0c\u4f46\u5b83\u5011\u7684\u611f\u77e5\u7bc4\u570d\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u8db3\u5920\u7684\u4efb\u52d9\u898f\u5283\u80fd\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u5176\u4ed6\u7814\u7a76\u5f15\u5165\u4e86\u7b2c\u4e00\u500b\u57fa\u65bc\u641c\u5c0b\u7684\u6c7a\u7b56\u6a39 (DFSDT)\uff0c\u4f46\u4ecd\u6709\u5f88\u9ad8\u7684\u904b\u7b97\u6210\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u5e73\u884c\u5de5\u5177\u547c\u53eb\u7bc4\u4f8b\uff0cDTA-Llama\uff08\u5206\u800c\u5408\u4e4b Llama\uff09\u3002\u9996\u5148\uff0c\u6211\u5011\u5c07\u50b3\u7d71\u7684\u57fa\u65bc\u6a39\u7684\u5de5\u5177\u641c\u5c0b\u8def\u5f91\u8f49\u63db\u70ba\u6709\u5411\u7121\u74b0\u5716 (DAG) \u7d50\u69cb\uff0c\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u5e73\u884c\u5de5\u5177\u547c\u53eb\u8cc7\u6599\u96c6\u3002\u7136\u5f8c\u5728\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4 DTA-Llama\uff0c\u5b78\u7fd2\u53cd\u8986\u5c07\u7576\u524d\u4efb\u52d9\u5206\u6210\u5e7e\u500b\u5e73\u884c\u5de5\u5177\u547c\u53eb\u5b50\u4efb\u52d9\uff0c\u4e26\u5f59\u7e3d\u547c\u53eb\u7d50\u679c\u4ee5\u6c7a\u5b9a\u5f8c\u7e8c\u52d5\u4f5c\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728\u5c07 DTA-Llama \u61c9\u7528\u65bc\u5be6\u969b\u4efb\u52d9\u6642\uff0c\u5f15\u5165\u4e86\u4e00\u500b\u53d7 Process/Threads \u6a5f\u5236\u555f\u767c\u7684\u9ad8\u6548\u63a8\u8ad6\u6846\u67b6\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5927\u5e45\u63d0\u5347\u4e86\u4efb\u52d9\u6548\u80fd\uff0c\u540c\u6642\u6e1b\u5c11\u4e86\u7b26\u865f\u6d88\u8017\u548c\u63a8\u8ad6\u6642\u9593\u3002\u4f7f\u7528\u6211\u5011\u65b9\u6cd5\u7684 Llama2-7B\uff0c\u53ef\u8207 GPT-3.5 \u7684\u5b98\u65b9\u5e73\u884c\u51fd\u5f0f\u547c\u53eb\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002\u76f8\u95dc\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u96c6\u548c\u6a21\u578b\u6b0a\u91cd\u53ef\u5728 https://corn0205.github.io/ \u53d6\u5f97", "author": "Dongsheng Zhu et.al.", "authors": "Dongsheng Zhu, Weixian Shi, Zhengliang Shi, Zhaochun Ren, Shuaiqiang Wang, Lingyong Yan, Dawei Yin", "id": "2501.12432v1", "paper_url": "http://arxiv.org/abs/2501.12432v1", "repo": "null"}}