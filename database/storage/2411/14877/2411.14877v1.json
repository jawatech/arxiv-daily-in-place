{"2411.14877": {"publish_time": "2024-11-22", "title": "Astro-HEP-BERT: A bidirectional language model for studying the meanings of concepts in astrophysics and high energy physics", "paper_summary": "I present Astro-HEP-BERT, a transformer-based language model specifically\ndesigned for generating contextualized word embeddings (CWEs) to study the\nmeanings of concepts in astrophysics and high-energy physics. Built on a\ngeneral pretrained BERT model, Astro-HEP-BERT underwent further training over\nthree epochs using the Astro-HEP Corpus, a dataset I curated from 21.84 million\nparagraphs extracted from more than 600,000 scholarly articles on arXiv, all\nbelonging to at least one of these two scientific domains. The project\ndemonstrates both the effectiveness and feasibility of adapting a bidirectional\ntransformer for applications in the history, philosophy, and sociology of\nscience (HPSS). The entire training process was conducted using freely\navailable code, pretrained weights, and text inputs, completed on a single\nMacBook Pro Laptop (M2/96GB). Preliminary evaluations indicate that\nAstro-HEP-BERT's CWEs perform comparably to domain-adapted BERT models trained\nfrom scratch on larger datasets for domain-specific word sense disambiguation\nand induction and related semantic change analyses. This suggests that\nretraining general language models for specific scientific domains can be a\ncost-effective and efficient strategy for HPSS researchers, enabling high\nperformance without the need for extensive training from scratch.", "paper_summary_zh": "\u6211\u5c55\u793a\u4e86 Astro-HEP-BERT\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc\u8f49\u63db\u5668\u7684\u8a9e\u8a00\u6a21\u578b\uff0c\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u7522\u751f\u8a9e\u5883\u5316\u55ae\u5b57\u5d4c\u5165 (CWE)\uff0c\u4ee5\u7814\u7a76\u5929\u9ad4\u7269\u7406\u5b78\u548c\u9ad8\u80fd\u7269\u7406\u5b78\u4e2d\u6982\u5ff5\u7684\u542b\u7fa9\u3002\u5efa\u7acb\u5728\u9810\u5148\u8a13\u7df4\u597d\u7684 BERT \u6a21\u578b\u4e4b\u4e0a\uff0cAstro-HEP-BERT \u4f7f\u7528 Astro-HEP \u8a9e\u6599\u5eab\u9032\u884c\u4e86\u4e09\u500b\u6642\u671f\u7684\u9032\u4e00\u6b65\u8a13\u7df4\uff0c\u9019\u500b\u8cc7\u6599\u96c6\u662f\u6211\u5f9e arXiv \u4e0a 600,000 \u591a\u7bc7\u5b78\u8853\u6587\u7ae0\u4e2d\u6458\u9304\u7684 2184 \u842c\u6bb5\u843d\u6574\u7406\u800c\u6210\u7684\uff0c\u6240\u6709\u6587\u7ae0\u90fd\u81f3\u5c11\u5c6c\u65bc\u9019\u5169\u500b\u79d1\u5b78\u9818\u57df\u4e4b\u4e00\u3002\u8a72\u5c08\u6848\u5c55\u793a\u4e86\u8abf\u6574\u96d9\u5411\u8f49\u63db\u5668\u5728\u79d1\u5b78\u53f2\u3001\u54f2\u5b78\u548c\u793e\u6703\u5b78 (HPSS) \u4e2d\u61c9\u7528\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002\u6574\u500b\u8a13\u7df4\u904e\u7a0b\u4f7f\u7528\u514d\u8cbb\u63d0\u4f9b\u7684\u7a0b\u5f0f\u78bc\u3001\u9810\u8a13\u7df4\u6b0a\u91cd\u548c\u6587\u5b57\u8f38\u5165\u9032\u884c\uff0c\u4e26\u5728\u55ae\u4e00\u7684 MacBook Pro \u7b46\u8a18\u578b\u96fb\u8166 (M2/96GB) \u4e0a\u5b8c\u6210\u3002\u521d\u6b65\u8a55\u4f30\u8868\u660e\uff0cAstro-HEP-BERT \u7684 CWE \u8207\u5f9e\u982d\u958b\u59cb\u5728\u8f03\u5927\u578b\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u9818\u57df\u9069\u61c9 BERT \u6a21\u578b\u5728\u9818\u57df\u7279\u5b9a\u7684\u55ae\u5b57\u611f\u5b98\u6d88\u6b67\u3001\u6b78\u7d0d\u548c\u76f8\u95dc\u8a9e\u7fa9\u8b8a\u66f4\u5206\u6790\u65b9\u9762\u8868\u73fe\u76f8\u7576\u3002\u9019\u8868\u660e\u91dd\u5c0d\u7279\u5b9a\u79d1\u5b78\u9818\u57df\u91cd\u65b0\u8a13\u7df4\u4e00\u822c\u8a9e\u8a00\u6a21\u578b\u53ef\u80fd\u662f HPSS \u7814\u7a76\u4eba\u54e1\u4e00\u7a2e\u7d93\u6fdf\u6709\u6548\u4e14\u9ad8\u6548\u7684\u7b56\u7565\uff0c\u7121\u9700\u5f9e\u982d\u958b\u59cb\u9032\u884c\u5927\u91cf\u8a13\u7df4\u5373\u53ef\u5be6\u73fe\u9ad8\u6027\u80fd\u3002", "author": "Arno Simons et.al.", "authors": "Arno Simons", "id": "2411.14877v1", "paper_url": "http://arxiv.org/abs/2411.14877v1", "repo": "null"}}