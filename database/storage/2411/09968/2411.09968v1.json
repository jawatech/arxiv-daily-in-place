{"2411.09968": {"publish_time": "2024-11-15", "title": "Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs", "paper_summary": "The hallucination problem in multimodal large language models (MLLMs) remains\na common issue. Although image tokens occupy a majority of the input sequence\nof MLLMs, there is limited research to explore the relationship between image\ntokens and hallucinations. In this paper, we analyze the distribution of\nattention scores for image tokens across each layer and head of the model,\nrevealing an intriguing and common phenomenon: most hallucinations are closely\nlinked to the pattern of attention sinks in the self-attention matrix of image\ntokens, where shallow layers exhibit dense attention sinks and deeper layers\nshow sparse attention sinks. We further analyze the attention heads of\ndifferent layers and find that heads with high-density attention sink in the\nimage part play a positive role in alleviating hallucinations. In this paper,\nwe propose a training-free method named \\textcolor{red}{\\textbf{E}}nhancing\n\\textcolor{red}{\\textbf{A}}ttention \\textcolor{red}{\\textbf{H}}eads (EAH), an\napproach designed to enhance the convergence of image tokens attention sinks in\nthe shallow layers. EAH identifies the attention head that shows the vision\nsink in a shallow layer and extracts its attention matrix. This attention map\nis then broadcast to other heads in the layer, thereby strengthening the layer\nto pay more attention to the image itself. With extensive experiments, EAH\nshows significant hallucination-mitigating performance on different MLLMs and\nmetrics, proving its effectiveness and generality.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u662f\u4e00\u4e2a\u5e38\u89c1\u95ee\u9898\u3002\u5c3d\u7ba1\u56fe\u50cf\u6807\u8bb0\u5360\u636e\u4e86 MLLM \u8f93\u5165\u5e8f\u5217\u7684\u5927\u90e8\u5206\uff0c\u4f46\u63a2\u7d22\u56fe\u50cf\u6807\u8bb0\u548c\u5e7b\u89c9\u4e4b\u95f4\u5173\u7cfb\u7684\u7814\u7a76\u5374\u5f88\u6709\u9650\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5206\u6790\u4e86\u8de8\u6a21\u578b\u7684\u6bcf\u4e00\u5c42\u548c\u5934\u7684\u56fe\u50cf\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u6570\u5206\u5e03\uff0c\u63ed\u793a\u4e86\u4e00\u4e2a\u6709\u8da3\u4e14\u666e\u904d\u7684\u73b0\u8c61\uff1a\u5927\u591a\u6570\u5e7b\u89c9\u4e0e\u56fe\u50cf\u6807\u8bb0\u7684\u81ea\u6ce8\u610f\u529b\u77e9\u9635\u4e2d\u7684\u6ce8\u610f\u529b\u6c47\u805a\u6a21\u5f0f\u5bc6\u5207\u76f8\u5173\uff0c\u5176\u4e2d\u6d45\u5c42\u8868\u73b0\u51fa\u5bc6\u96c6\u7684\u6ce8\u610f\u529b\u6c47\u805a\uff0c\u800c\u6df1\u5c42\u8868\u73b0\u51fa\u7a00\u758f\u7684\u6ce8\u610f\u529b\u6c47\u805a\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u4e0d\u540c\u5c42\u7684\u6ce8\u610f\u529b\u5934\uff0c\u53d1\u73b0\u56fe\u50cf\u90e8\u5206\u4e2d\u5177\u6709\u9ad8\u5bc6\u5ea6\u6ce8\u610f\u529b\u6c47\u805a\u7684\u6ce8\u610f\u529b\u5934\u5728\u51cf\u8f7b\u5e7b\u89c9\u4e2d\u8d77\u5230\u4e86\u79ef\u6781\u4f5c\u7528\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\textcolor{red}{\\textbf{E}}nhancing \\textcolor{red}{\\textbf{A}}ttention \\textcolor{red}{\\textbf{H}}eads (EAH) \u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65e8\u5728\u589e\u5f3a\u6d45\u5c42\u4e2d\u56fe\u50cf\u6807\u8bb0\u6ce8\u610f\u529b\u6c47\u805a\u6536\u655b\u6027\u7684\u65b9\u6cd5\u3002EAH \u8bc6\u522b\u51fa\u5728\u6d45\u5c42\u4e2d\u663e\u793a\u89c6\u89c9\u6c47\u805a\u7684\u6ce8\u610f\u529b\u5934\u5e76\u63d0\u53d6\u5176\u6ce8\u610f\u529b\u77e9\u9635\u3002\u7136\u540e\u5c06\u6b64\u6ce8\u610f\u529b\u56fe\u5e7f\u64ad\u5230\u5c42\u4e2d\u7684\u5176\u4ed6\u5934\uff0c\u4ece\u800c\u589e\u5f3a\u8be5\u5c42\u5bf9\u56fe\u50cf\u672c\u8eab\u7684\u5173\u6ce8\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0cEAH \u5728\u4e0d\u540c\u7684 MLLM \u548c\u6307\u6807\u4e0a\u663e\u793a\u51fa\u663e\u7740\u7684\u7f13\u89e3\u5e7b\u89c9\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u666e\u904d\u6027\u3002", "author": "Xiaofeng Zhang et.al.", "authors": "Xiaofeng Zhang, Yihao Quan, Chaochen Gu, Chen Shen, Xiaosong Yuan, Shaotian Yan, Hao Cheng, Kaijie Wu, Jieping Ye", "id": "2411.09968v1", "paper_url": "http://arxiv.org/abs/2411.09968v1", "repo": "null"}}