{"2501.17161": {"publish_time": "2025-01-28", "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training", "paper_summary": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used\npost-training techniques for foundation models. However, their roles in\nenhancing model generalization capabilities remain unclear. This paper studies\nthe difference between SFT and RL on generalization and memorization, focusing\non text-based rule variants and visual variants. We introduce GeneralPoints, an\narithmetic reasoning card game, and adopt V-IRL, a real-world navigation\nenvironment, to assess how models trained with SFT and RL generalize to unseen\nvariants in both textual and visual domains. We show that RL, especially when\ntrained with an outcome-based reward, generalizes across both rule-based\ntextual and visual variants. SFT, in contrast, tends to memorize training data\nand struggles to generalize out-of-distribution scenarios. Further analysis\nreveals that RL improves the model's underlying visual recognition\ncapabilities, contributing to its enhanced generalization in the visual domain.\nDespite RL's superior generalization, we show that SFT remains essential for\neffective RL training; SFT stabilizes the model's output format, enabling\nsubsequent RL to achieve its performance gains. These findings demonstrates the\ncapability of RL for acquiring generalizable knowledge in complex, multi-modal\ntasks.", "paper_summary_zh": "\u76e3\u7763\u5fae\u8abf (SFT) \u548c\u5f37\u5316\u5b78\u7fd2 (RL) \u662f\u5ee3\u6cdb\u7528\u65bc\u57fa\u790e\u6a21\u578b\u7684\u8a13\u7df4\u5f8c\u6280\u8853\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u589e\u5f37\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\u4ecd\u4e0d\u660e\u78ba\u3002\u672c\u6587\u63a2\u8a0e\u4e86 SFT \u548c RL \u5728\u6cdb\u5316\u548c\u8a18\u61b6\u529b\u65b9\u9762\u7684\u5dee\u7570\uff0c\u91cd\u9ede\u95dc\u6ce8\u57fa\u65bc\u6587\u672c\u7684\u898f\u5247\u8b8a\u9ad4\u548c\u8996\u89ba\u8b8a\u9ad4\u3002\u6211\u5011\u5f15\u5165\u4e86 GeneralPoints\uff0c\u4e00\u7a2e\u7b97\u8853\u63a8\u7406\u7d19\u724c\u904a\u6232\uff0c\u4e26\u63a1\u7528 V-IRL\uff0c\u4e00\u500b\u771f\u5be6\u4e16\u754c\u7684\u5c0e\u822a\u74b0\u5883\uff0c\u4f86\u8a55\u4f30\u4f7f\u7528 SFT \u548c RL \u8a13\u7df4\u7684\u6a21\u578b\u5982\u4f55\u6cdb\u5316\u5230\u6587\u672c\u548c\u8996\u89ba\u9818\u57df\u4e2d\u672a\u898b\u904e\u7684\u8b8a\u9ad4\u3002\u6211\u5011\u8868\u660e RL\uff0c\u7279\u5225\u662f\u5728\u4f7f\u7528\u57fa\u65bc\u7d50\u679c\u7684\u734e\u52f5\u9032\u884c\u8a13\u7df4\u6642\uff0c\u53ef\u4ee5\u6cdb\u5316\u5230\u57fa\u65bc\u898f\u5247\u7684\u6587\u672c\u548c\u8996\u89ba\u8b8a\u9ad4\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cSFT \u50be\u5411\u65bc\u8a18\u61b6\u8a13\u7df4\u6578\u64da\uff0c\u4e26\u4e14\u96e3\u4ee5\u6cdb\u5316\u5230\u5206\u4f48\u5916\u7684\u5834\u666f\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0cRL \u6539\u5584\u4e86\u6a21\u578b\u7684\u5e95\u5c64\u8996\u89ba\u8b58\u5225\u80fd\u529b\uff0c\u6709\u52a9\u65bc\u5176\u5728\u8996\u89ba\u9818\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u589e\u5f37\u3002\u5118\u7ba1 RL \u5177\u6709\u512a\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u6211\u5011\u8868\u660e SFT \u4ecd\u7136\u5c0d\u65bc\u6709\u6548\u7684 RL \u8a13\u7df4\u81f3\u95dc\u91cd\u8981\uff1bSFT \u7a69\u5b9a\u6a21\u578b\u7684\u8f38\u51fa\u683c\u5f0f\uff0c\u4f7f\u5f8c\u7e8c RL \u80fd\u5920\u5be6\u73fe\u5176\u6027\u80fd\u63d0\u5347\u3002\u9019\u4e9b\u767c\u73fe\u5c55\u793a\u4e86 RL \u5728\u8907\u96dc\u7684\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u7372\u53d6\u53ef\u6cdb\u5316\u77e5\u8b58\u7684\u80fd\u529b\u3002", "author": "Tianzhe Chu et.al.", "authors": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, Yi Ma", "id": "2501.17161v1", "paper_url": "http://arxiv.org/abs/2501.17161v1", "repo": "null"}}