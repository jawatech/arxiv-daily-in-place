{"2407.07296": {"publish_time": "2024-07-10", "title": "Large Language Model-Augmented Auto-Delineation of Treatment Target Volume in Radiation Therapy", "paper_summary": "Radiation therapy (RT) is one of the most effective treatments for cancer,\nand its success relies on the accurate delineation of targets. However, target\ndelineation is a comprehensive medical decision that currently relies purely on\nmanual processes by human experts. Manual delineation is time-consuming,\nlaborious, and subject to interobserver variations. Although the advancements\nin artificial intelligence (AI) techniques have significantly enhanced the\nauto-contouring of normal tissues, accurate delineation of RT target volumes\nremains a challenge. In this study, we propose a visual language model-based RT\ntarget volume auto-delineation network termed Radformer. The Radformer utilizes\na hierarichal vision transformer as the backbone and incorporates large\nlanguage models to extract text-rich features from clinical data. We introduce\na visual language attention module (VLAM) for integrating visual and linguistic\nfeatures for language-aware visual encoding (LAVE). The Radformer has been\nevaluated on a dataset comprising 2985 patients with head-and-neck cancer who\nunderwent RT. Metrics, including the Dice similarity coefficient (DSC),\nintersection over union (IOU), and 95th percentile Hausdorff distance (HD95),\nwere used to evaluate the performance of the model quantitatively. Our results\ndemonstrate that the Radformer has superior segmentation performance compared\nto other state-of-the-art models, validating its potential for adoption in RT\npractice.", "paper_summary_zh": "\u653e\u5c04\u6cbb\u7642 (RT) \u662f\u6700\u6709\u6548\u7684\u764c\u75c7\u6cbb\u7642\u65b9\u6cd5\u4e4b\u4e00\uff0c\u5176\u6210\u529f\u6709\u8cf4\u65bc\u76ee\u6a19\u7684\u6e96\u78ba\u63cf\u7e6a\u3002\u7136\u800c\uff0c\u76ee\u6a19\u63cf\u7e6a\u662f\u4e00\u9805\u5168\u9762\u7684\u91ab\u7642\u6c7a\u7b56\uff0c\u76ee\u524d\u5b8c\u5168\u4f9d\u8cf4\u4eba\u985e\u5c08\u5bb6\u7684\u624b\u52d5\u7a0b\u5e8f\u3002\u624b\u52d5\u63cf\u7e6a\u8017\u6642\u3001\u8cbb\u529b\uff0c\u4e14\u53d7\u89c0\u5bdf\u8005\u9593\u5dee\u7570\u5f71\u97ff\u3002\u5118\u7ba1\u4eba\u5de5\u667a\u6167 (AI) \u6280\u8853\u7684\u9032\u6b65\u5df2\u986f\u8457\u589e\u5f37\u6b63\u5e38\u7d44\u7e54\u7684\u81ea\u52d5\u8f2a\u5ed3\u63cf\u7e6a\uff0c\u4f46 RT \u76ee\u6a19\u9ad4\u7a4d\u7684\u6e96\u78ba\u63cf\u7e6a\u4ecd\u662f\u4e00\u9805\u6311\u6230\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684 RT \u76ee\u6a19\u9ad4\u7a4d\u81ea\u52d5\u63cf\u7e6a\u7db2\u8def\uff0c\u7a31\u70ba Radformer\u3002Radformer \u5229\u7528\u968e\u5c64\u5f0f\u8996\u89baTransformer\u4f5c\u70ba\u4e3b\u5e79\uff0c\u4e26\u6574\u5408\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5f9e\u81e8\u5e8a\u8cc7\u6599\u4e2d\u63d0\u53d6\u8c50\u5bcc\u6587\u5b57\u7279\u5fb5\u3002\u6211\u5011\u5f15\u5165\u4e00\u500b\u8996\u89ba\u8a9e\u8a00\u6ce8\u610f\u529b\u6a21\u7d44 (VLAM)\uff0c\u7528\u65bc\u6574\u5408\u8996\u89ba\u548c\u8a9e\u8a00\u7279\u5fb5\uff0c\u4ee5\u9032\u884c\u8a9e\u8a00\u611f\u77e5\u8996\u89ba\u7de8\u78bc (LAVE)\u3002Radformer \u5df2\u5728\u4e00\u500b\u5305\u542b 2985 \u540d\u63a5\u53d7 RT \u6cbb\u7642\u7684\u982d\u9838\u764c\u60a3\u8005\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a55\u4f30\u3002\u6307\u6a19\uff0c\u5305\u62ec Dice \u76f8\u4f3c\u4fc2\u6578 (DSC)\u3001\u806f\u96c6\u6bd4 (IOU) \u548c\u7b2c 95 \u500b\u767e\u5206\u4f4d\u6578 Hausdorff \u8ddd\u96e2 (HD95)\uff0c\u7528\u65bc\u5b9a\u91cf\u8a55\u4f30\u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u8207\u5176\u4ed6\u6700\u5148\u9032\u7684\u6a21\u578b\u76f8\u6bd4\uff0cRadformer \u5177\u6709\u512a\u7570\u7684\u5206\u5272\u6548\u80fd\uff0c\u9a57\u8b49\u4e86\u5176\u5728 RT \u5be6\u52d9\u4e2d\u61c9\u7528\u7684\u6f5b\u529b\u3002", "author": "Praveenbalaji Rajendran et.al.", "authors": "Praveenbalaji Rajendran, Yong Yang, Thomas R. Niedermayr, Michael Gensheimer, Beth Beadle, Quynh-Thu Le, Lei Xing, Xianjin Dai", "id": "2407.07296v1", "paper_url": "http://arxiv.org/abs/2407.07296v1", "repo": "null"}}