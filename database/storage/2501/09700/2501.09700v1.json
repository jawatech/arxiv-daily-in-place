{"2501.09700": {"publish_time": "2025-01-16", "title": "Cueless EEG imagined speech for subject identification: dataset and benchmarks", "paper_summary": "Electroencephalogram (EEG) signals have emerged as a promising modality for\nbiometric identification. While previous studies have explored the use of\nimagined speech with semantically meaningful words for subject identification,\nmost have relied on additional visual or auditory cues. In this study, we\nintroduce a cueless EEG-based imagined speech paradigm, where subjects imagine\nthe pronunciation of semantically meaningful words without any external cues.\nThis innovative approach addresses the limitations of prior methods by\nrequiring subjects to select and imagine words from a predefined list\nnaturally. The dataset comprises over 4,350 trials from 11 subjects across five\nsessions. We assess a variety of classification methods, including traditional\nmachine learning techniques such as Support Vector Machines (SVM) and XGBoost,\nas well as time-series foundation models and deep learning architectures\nspecifically designed for EEG classification, such as EEG Conformer and Shallow\nConvNet. A session-based hold-out validation strategy was employed to ensure\nreliable evaluation and prevent data leakage. Our results demonstrate\noutstanding classification accuracy, reaching 97.93%. These findings highlight\nthe potential of cueless EEG paradigms for secure and reliable subject\nidentification in real-world applications, such as brain-computer interfaces\n(BCIs).", "paper_summary_zh": "\u8166\u96fb\u5716 (EEG) \u4fe1\u865f\u5df2\u6210\u70ba\u751f\u7269\u8b58\u5225\u4e2d\u6975\u5177\u6f5b\u529b\u7684\u65b9\u5f0f\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u5df2\u63a2\u8a0e\u5728\u4e3b\u984c\u8b58\u5225\u4e2d\u4f7f\u7528\u5177\u6709\u8a9e\u7fa9\u610f\u7fa9\u5b57\u8a5e\u7684\u60f3\u50cf\u8a9e\u8a00\uff0c\u4f46\u5927\u591a\u4f9d\u8cf4\u984d\u5916\u7684\u8996\u89ba\u6216\u807d\u89ba\u63d0\u793a\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u9032\u4e00\u7a2e\u7121\u63d0\u793a\u7684 EEG \u57fa\u65bc\u60f3\u50cf\u8a9e\u8a00\u7684\u7bc4\u4f8b\uff0c\u53d7\u8a66\u8005\u5728\u6c92\u6709\u4efb\u4f55\u5916\u90e8\u63d0\u793a\u7684\u60c5\u6cc1\u4e0b\u60f3\u50cf\u6709\u8a9e\u7fa9\u610f\u7fa9\u5b57\u8a5e\u7684\u767c\u97f3\u3002\u9019\u7a2e\u5275\u65b0\u65b9\u6cd5\u900f\u904e\u8981\u6c42\u53d7\u8a66\u8005\u81ea\u7136\u5730\u5f9e\u9810\u5148\u5b9a\u7fa9\u7684\u6e05\u55ae\u4e2d\u9078\u64c7\u548c\u60f3\u50cf\u5b57\u8a5e\uff0c\u4f86\u89e3\u6c7a\u5148\u524d\u65b9\u6cd5\u7684\u9650\u5236\u3002\u8a72\u8cc7\u6599\u96c6\u5305\u542b\u4f86\u81ea 11 \u4f4d\u53d7\u8a66\u8005\u5728\u4e94\u500b\u968e\u6bb5\u4e2d\u8d85\u904e 4,350 \u6b21\u7684\u8a66\u9a57\u3002\u6211\u5011\u8a55\u4f30\u4e86\u5404\u7a2e\u5206\u985e\u65b9\u6cd5\uff0c\u5305\u62ec\u50b3\u7d71\u6a5f\u5668\u5b78\u7fd2\u6280\u8853\uff0c\u4f8b\u5982\u652f\u63f4\u5411\u91cf\u6a5f (SVM) \u548c XGBoost\uff0c\u4ee5\u53ca\u5c08\u9580\u8a2d\u8a08\u7528\u65bc EEG \u5206\u985e\u7684\u6642\u9593\u5e8f\u5217\u57fa\u790e\u6a21\u578b\u548c\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\uff0c\u4f8b\u5982 EEG Conformer \u548c\u6dfa\u5c64 ConvNet\u3002\u63a1\u7528\u57fa\u65bc\u968e\u6bb5\u7684\u4fdd\u7559\u9a57\u8b49\u7b56\u7565\uff0c\u4ee5\u78ba\u4fdd\u53ef\u9760\u7684\u8a55\u4f30\u4e26\u9632\u6b62\u8cc7\u6599\u5916\u6d29\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\u4e86\u51fa\u8272\u7684\u5206\u985e\u6e96\u78ba\u5ea6\uff0c\u9054\u5230 97.93%\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u7121\u63d0\u793a EEG \u7bc4\u4f8b\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u5b89\u5168\u4e14\u53ef\u9760\u7684\u4e3b\u984c\u8b58\u5225\u6f5b\u529b\uff0c\u4f8b\u5982\u8166\u96fb\u8166\u4ecb\u9762 (BCI)\u3002", "author": "Ali Derakhshesh et.al.", "authors": "Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee", "id": "2501.09700v1", "paper_url": "http://arxiv.org/abs/2501.09700v1", "repo": "https://github.com/alidr79/cueless_eeg_subject_identification"}}