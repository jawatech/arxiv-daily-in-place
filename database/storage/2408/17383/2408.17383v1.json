{"2408.17383": {"publish_time": "2024-08-30", "title": "MoRe Fine-Tuning with 10x Fewer Parameters", "paper_summary": "Parameter-efficient fine-tuning (PEFT) techniques have unlocked the potential\nto cheaply and easily specialize large pretrained models. However, the most\nprominent approaches, like low-rank adapters (LoRA), depend on heuristics or\nrules-of-thumb for their architectural choices -- potentially limiting their\nperformance for new models and architectures. This limitation suggests that\ntechniques from neural architecture search could be used to obtain optimal\nadapter architectures, but these are often expensive and difficult to\nimplement. We address this challenge with Monarch Rectangular Fine-tuning\n(MoRe), a simple framework to search over adapter architectures that relies on\nthe Monarch matrix class. Theoretically, we show that MoRe is more expressive\nthan LoRA. Empirically, our approach is more parameter-efficient and performant\nthan state-of-the-art PEFTs on a range of tasks and models, with as few as 5\\%\nof LoRA's parameters.", "paper_summary_zh": "\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u6280\u8853\u5df2\u91cb\u653e\u4e86\u4ee5\u4f4e\u6210\u672c\u4e14\u8f15\u9b06\u7684\u65b9\u5f0f\u5c0d\u5927\u578b\u9810\u8a13\u7df4\u6a21\u578b\u9032\u884c\u5c08\u696d\u5316\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0c\u6700\u986f\u8457\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u4f4e\u79e9\u9069\u914d\u5668 (LoRA)\uff0c\u4f9d\u8cf4\u65bc\u7d93\u9a57\u6cd5\u5247\u6216\u67b6\u69cb\u9078\u64c7\u7684\u7d93\u9a57\u6cd5\u5247\u2014\u2014\u9019\u53ef\u80fd\u6703\u9650\u5236\u5176\u5728\u65b0\u7684\u6a21\u578b\u548c\u67b6\u69cb\u4e2d\u7684\u6548\u80fd\u3002\u6b64\u9650\u5236\u8868\u660e\u53ef\u4ee5\u5229\u7528\u795e\u7d93\u67b6\u69cb\u641c\u5c0b\u7684\u6280\u8853\u4f86\u53d6\u5f97\u6700\u4f73\u7684\u9069\u914d\u5668\u67b6\u69cb\uff0c\u4f46\u9019\u4e9b\u6280\u8853\u901a\u5e38\u6210\u672c\u9ad8\u4e14\u96e3\u4ee5\u57f7\u884c\u3002\u6211\u5011\u900f\u904e Monarch \u77e9\u5f62\u5fae\u8abf (MoRe) \u4f86\u89e3\u6c7a\u6b64\u6311\u6230\uff0c\u9019\u662f\u4e00\u500b\u7c21\u55ae\u7684\u67b6\u69cb\uff0c\u7528\u65bc\u641c\u5c0b\u4f9d\u8cf4\u65bc Monarch \u77e9\u9663\u985e\u5225\u7684\u9069\u914d\u5668\u67b6\u69cb\u3002\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u5c55\u793a MoRe \u6bd4 LoRA \u66f4\u5177\u8868\u73fe\u529b\u3002\u6839\u64da\u7d93\u9a57\uff0c\u6211\u5011\u7684\u505a\u6cd5\u6bd4\u6700\u5148\u9032\u7684 PEFT \u5728\u4e00\u7cfb\u5217\u4efb\u52d9\u548c\u6a21\u578b\u4e0a\u66f4\u5177\u53c3\u6578\u6548\u7387\u548c\u6548\u80fd\uff0cLoRA \u7684\u53c3\u6578\u5c11\u81f3 5%\u3002", "author": "Wenxuan Tan et.al.", "authors": "Wenxuan Tan, Nicholas Roberts, Tzu-Heng Huang, Jitian Zhao, John Cooper, Samuel Guo, Chengyu Duan, Frederic Sala", "id": "2408.17383v1", "paper_url": "http://arxiv.org/abs/2408.17383v1", "repo": "https://github.com/sprocketlab/sparse_matrix_fine_tuning"}}