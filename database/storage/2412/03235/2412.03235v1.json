{"2412.03235": {"publish_time": "2024-12-04", "title": "Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?", "paper_summary": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u77e5\u5bb9\u6613\u53d7\u5230\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6027\u653b\u51fb\u6216\u8d8a\u72f1\u653b\u51fb\uff0c\u5c3d\u7ba1\u4f7f\u7528\u5b89\u5168\u5fae\u8c03\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u4f46\u8fd9\u4e9b\u653b\u51fb\u6216\u8d8a\u72f1\u653b\u51fb\u4f1a\u5bfc\u81f4\u751f\u6210\u4ee4\u4eba\u53cd\u611f\u7684\u5185\u5bb9\u3002\u867d\u7136\u8f93\u5165\u6807\u8bb0\u7a7a\u95f4\u7684\u5927\u7ef4\u5ea6\u4f7f\u5f97\u627e\u5230\u53ef\u4ee5\u8d8a\u72f1\u8fd9\u4e9b\u6a21\u578b\u7684\u5bf9\u6297\u6027\u63d0\u793a\u4e0d\u53ef\u907f\u514d\uff0c\u4f46\u6211\u4eec\u7684\u76ee\u6807\u662f\u8bc4\u4f30\u7ecf\u8fc7\u5b89\u5168\u5fae\u8c03\u7684 LLM \u662f\u5426\u53ef\u4ee5\u9632\u6b62\u4e0e\u8bed\u4e49\u76f8\u5173\u7684\u81ea\u7136\u63d0\u793a\uff0c\u8fd9\u4e9b\u63d0\u793a\u4e0e\u5f15\u53d1\u5bf9\u9f50\u540e\u5b89\u5168\u54cd\u5e94\u7684\u6709\u6bd2\u79cd\u5b50\u63d0\u793a\u76f8\u5173\u3002\u6211\u4eec\u60ca\u8bb6\u5730\u53d1\u73b0\uff0c\u8bf8\u5982 GPT-4 \u7b49\u6d41\u884c\u7684\u5bf9\u9f50 LLM \u53ef\u4ee5\u4f7f\u7528\u5929\u771f\u7684\u63d0\u793a\u6765\u7834\u574f\uff0c\u751a\u81f3\u8fd9\u4e9b\u63d0\u793a\u5e76\u4e0d\u662f\u4e3a\u4e86\u8d8a\u72f1\u6a21\u578b\u800c\u8bbe\u8ba1\u7684\u3002\u6b64\u5916\uff0c\u6211\u4eec\u51ed\u7ecf\u9a8c\u8868\u660e\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u4ece\u672a\u5bf9\u9f50\u7684\u6a21\u578b\u5f15\u53d1\u51fa\u6709\u6bd2\u53cd\u5e94\u7684\u79cd\u5b50\u63d0\u793a\uff0c\u4eba\u4eec\u53ef\u4ee5\u7cfb\u7edf\u5730\u751f\u6210\u51e0\u4e2a\u8bed\u4e49\u76f8\u5173\u7684\u81ea\u7136\u63d0\u793a\uff0c\u8fd9\u4e9b\u63d0\u793a\u53ef\u4ee5\u8d8a\u72f1\u5bf9\u9f50\u7684 LLM\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u54cd\u5e94\u5f15\u5bfc\u95ee\u9898\u589e\u5f3a (ReG-QA) \u65b9\u6cd5\u6765\u8bc4\u4f30\u5b89\u5168\u5bf9\u9f50\u7684 LLM \u5bf9\u81ea\u7136\u63d0\u793a\u7684\u6cdb\u5316\uff0c\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u672a\u5bf9\u9f50\u7684 LLM (Q \u5230 A) \u7ed9\u5b9a\u4e00\u4e2a\u79cd\u5b50\u95ee\u9898\u751f\u6210\u51e0\u4e2a\u6709\u6bd2\u7b54\u6848\uff0c\u5e76\u8fdb\u4e00\u6b65\u5229\u7528 LLM \u751f\u6210\u53ef\u80fd\u4ea7\u751f\u8fd9\u4e9b\u7b54\u6848\u7684\u95ee\u9898 (A \u5230 Q)\u3002\u6211\u4eec\u6709\u8da3\u5730\u53d1\u73b0\uff0c\u8bf8\u5982 GPT-4o \u7b49\u7ecf\u8fc7\u5b89\u5168\u5fae\u8c03\u7684 LLM \u5bb9\u6613\u4ece\u4e0d\u5b89\u5168\u5185\u5bb9\uff08\u6ca1\u6709\u5426\u8ba4\uff09\u4e2d\u4ea7\u751f\u81ea\u7136\u7684\u8d8a\u72f1\u95ee\u9898\uff0c\u56e0\u6b64\u53ef\u4ee5\u7528\u4e8e\u540e\u8005\uff08A \u5230 Q\uff09\u6b65\u9aa4\u3002\u6211\u4eec\u83b7\u5f97\u7684\u653b\u51fb\u6210\u529f\u7387\u4e0e JailbreakBench \u6392\u884c\u699c\u4e0a\u7684\u9886\u5148\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\u76f8\u5f53/\u66f4\u597d\uff0c\u540c\u65f6\u5bf9\u5e73\u6ed1 LLM \u548c\u540c\u4e49\u8bcd\u66ff\u6362\u7b49\u9632\u5fa1\u63aa\u65bd\u7684\u7a33\u5b9a\u6027\u660e\u663e\u66f4\u9ad8\uff0c\u800c\u8fd9\u4e9b\u9632\u5fa1\u63aa\u65bd\u5bf9\u6392\u884c\u699c\u4e0a\u7684\u6240\u6709\u73b0\u6709\u653b\u51fb\u90fd\u662f\u6709\u6548\u7684\u3002", "author": "Sravanti Addepalli et.al.", "authors": "Sravanti Addepalli, Yerram Varun, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain", "id": "2412.03235v1", "paper_url": "http://arxiv.org/abs/2412.03235v1", "repo": "null"}}