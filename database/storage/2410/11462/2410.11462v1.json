{"2410.11462": {"publish_time": "2024-10-15", "title": "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing", "paper_summary": "Language models strongly rely on frequency information because they maximize\nthe likelihood of tokens during pre-training. As a consequence, language models\ntend to not generalize well to tokens that are seldom seen during training.\nMoreover, maximum likelihood training has been discovered to give rise to\nanisotropy: representations of tokens in a model tend to cluster tightly in a\nhigh-dimensional cone, rather than spreading out over their representational\ncapacity.\n  Our work introduces a method for quantifying the frequency bias of a language\nmodel by assessing sentence-level perplexity with respect to token-level\nfrequency. We then present a method for reducing the frequency bias of a\nlanguage model by inducing a syntactic prior over token representations during\npre-training. Our Syntactic Smoothing method adjusts the maximum likelihood\nobjective function to distribute the learning signal to syntactically similar\ntokens. This approach results in better performance on infrequent English\ntokens and a decrease in anisotropy. We empirically show that the degree of\nanisotropy in a model correlates with its frequency bias.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u6975\u5ea6\u4f9d\u8cf4\u983b\u7387\u8cc7\u8a0a\uff0c\u56e0\u70ba\u5b83\u5011\u5728\u9810\u8a13\u7df4\u671f\u9593\u6700\u5927\u5316\u4e86\u7b26\u865f\u7684\u53ef\u80fd\u6027\u3002\u56e0\u6b64\uff0c\u8a9e\u8a00\u6a21\u578b\u5f80\u5f80\u7121\u6cd5\u5c0d\u8a13\u7df4\u671f\u9593\u5f88\u5c11\u898b\u7684\u7b26\u865f\u9032\u884c\u826f\u597d\u7684\u6982\u5316\u3002\u6b64\u5916\uff0c\u5df2\u767c\u73fe\u6700\u5927\u4f3c\u7136\u8a13\u7df4\u6703\u5c0e\u81f4\u5404\u5411\u7570\u6027\uff1a\u6a21\u578b\u4e2d\u7b26\u865f\u7684\u8868\u793a\u50be\u5411\u65bc\u7dca\u5bc6\u805a\u96c6\u5728\u4e00\u500b\u9ad8\u7dad\u9310\u9ad4\u4e2d\uff0c\u800c\u4e0d\u662f\u64f4\u6563\u5230\u5b83\u5011\u7684\u8868\u793a\u80fd\u529b\u4e4b\u4e0a\u3002\n\u6211\u5011\u7684\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u7a2e\u91cf\u5316\u8a9e\u8a00\u6a21\u578b\u983b\u7387\u504f\u5dee\u7684\u65b9\u6cd5\uff0c\u65b9\u6cd5\u662f\u6839\u64da\u7b26\u865f\u7d1a\u5225\u7684\u983b\u7387\u8a55\u4f30\u53e5\u5b50\u7d1a\u56f0\u60d1\u5ea6\u3002\u7136\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u900f\u904e\u5728\u9810\u8a13\u7df4\u671f\u9593\u5728\u7b26\u865f\u8868\u793a\u4e0a\u8a98\u5c0e\u53e5\u6cd5\u5148\u9a57\u4f86\u6e1b\u5c11\u8a9e\u8a00\u6a21\u578b\u983b\u7387\u504f\u5dee\u7684\u65b9\u6cd5\u3002\u6211\u5011\u7684\u53e5\u6cd5\u5e73\u6ed1\u65b9\u6cd5\u8abf\u6574\u4e86\u6700\u5927\u4f3c\u7136\u76ee\u6a19\u51fd\u6578\uff0c\u4ee5\u5c07\u5b78\u7fd2\u8a0a\u865f\u5206\u914d\u7d66\u53e5\u6cd5\u4e0a\u76f8\u4f3c\u7684\u7b26\u865f\u3002\u9019\u7a2e\u65b9\u6cd5\u6539\u5584\u4e86\u4e0d\u5e38\u898b\u82f1\u6587\u7b26\u865f\u7684\u6548\u80fd\uff0c\u4e26\u6e1b\u5c11\u4e86\u5404\u5411\u7570\u6027\u3002\u6211\u5011\u900f\u904e\u5be6\u8b49\u986f\u793a\uff0c\u6a21\u578b\u4e2d\u7684\u5404\u5411\u7570\u6027\u7a0b\u5ea6\u8207\u5176\u983b\u7387\u504f\u5dee\u76f8\u95dc\u3002", "author": "Richard Diehl Martinez et.al.", "authors": "Richard Diehl Martinez, Zebulon Goriely, Andrew Caines, Paula Buttery, Lisa Beinborn", "id": "2410.11462v1", "paper_url": "http://arxiv.org/abs/2410.11462v1", "repo": "null"}}