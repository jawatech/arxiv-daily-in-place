{"2407.09453": {"publish_time": "2024-07-12", "title": "Weight Block Sparsity: Training, Compilation, and AI Engine Accelerators", "paper_summary": "Nowadays, increasingly larger Deep Neural Networks (DNNs) are being\ndeveloped, trained, and utilized. These networks require significant\ncomputational resources, putting a strain on both advanced and limited devices.\nOur solution is to implement {\\em weight block sparsity}, which is a structured\nsparsity that is friendly to hardware. By zeroing certain sections of the\nconvolution and fully connected layers parameters of pre-trained DNN models, we\ncan efficiently speed up the DNN's inference process. This results in a smaller\nmemory footprint, faster communication, and fewer operations.\n  Our work presents a vertical system that allows for the training of\nconvolution and matrix multiplication weights to exploit 8x8 block sparsity on\na single GPU within a reasonable amount of time. Compilers recognize this\nsparsity and use it for both data compaction and computation splitting into\nthreads. Blocks like these take full advantage of both spatial and temporal\nlocality, paving the way for fast vector operations and memory reuse. By using\nthis system on a Resnet50 model, we were able to reduce the weight by half with\nminimal accuracy loss, resulting in a two-times faster inference speed. We will\npresent performance estimates using accurate and complete code generation for\nAIE2 configuration sets (AMD Versal FPGAs) with Resnet50, Inception V3, and\nVGG16 to demonstrate the necessary synergy between hardware overlay designs and\nsoftware stacks for compiling and executing machine learning applications.", "paper_summary_zh": "\u73fe\u4eca\uff0c\u8d8a\u4f86\u8d8a\u5927\u578b\u7684\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\uff08DNN\uff09\u88ab\u958b\u767c\u3001\u8a13\u7df4\u548c\u4f7f\u7528\u3002\u9019\u4e9b\u7db2\u8def\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u5c0d\u5148\u9032\u548c\u53d7\u9650\u7684\u88dd\u7f6e\u90fd\u9020\u6210\u8ca0\u64d4\u3002\u6211\u5011\u7684\u89e3\u6c7a\u65b9\u6848\u662f\u5be6\u4f5c\u300c\u6b0a\u91cd\u5340\u584a\u7a00\u758f\u6027\u300d\uff0c\u9019\u662f\u4e00\u7a2e\u5c0d\u786c\u9ad4\u53cb\u5584\u7684\u7d50\u69cb\u5316\u7a00\u758f\u6027\u3002\u900f\u904e\u5c07\u9810\u5148\u8a13\u7df4\u597d\u7684 DNN \u6a21\u578b\u7684\u5377\u7a4d\u548c\u5168\u9023\u63a5\u5c64\u53c3\u6578\u7684\u7279\u5b9a\u5340\u584a\u6b78\u96f6\uff0c\u6211\u5011\u53ef\u4ee5\u6709\u6548\u5730\u52a0\u901f DNN \u7684\u63a8\u8ad6\u904e\u7a0b\u3002\u9019\u6703\u7522\u751f\u8f03\u5c0f\u7684\u8a18\u61b6\u9ad4\u4f54\u7528\u7a7a\u9593\u3001\u66f4\u5feb\u7684\u901a\u8a0a\u548c\u8f03\u5c11\u7684\u904b\u7b97\u3002\u6211\u5011\u7684\u6210\u679c\u5c55\u793a\u4e86\u4e00\u500b\u5782\u76f4\u7cfb\u7d71\uff0c\u5141\u8a31\u5728\u5408\u7406\u7684\u6642\u7a0b\u5167\u5728\u55ae\u4e00 GPU \u4e0a\u8a13\u7df4\u5377\u7a4d\u548c\u77e9\u9663\u4e58\u6cd5\u6b0a\u91cd\uff0c\u4ee5\u5229\u7528 8x8 \u5340\u584a\u7a00\u758f\u6027\u3002\u7de8\u8b6f\u5668\u6703\u8fa8\u8b58\u6b64\u7a00\u758f\u6027\uff0c\u4e26\u5c07\u5176\u7528\u65bc\u8cc7\u6599\u58d3\u7e2e\u548c\u904b\u7b97\u62c6\u5206\u70ba\u57f7\u884c\u7dd2\u3002\u50cf\u9019\u6a23\u7684\u5340\u584a\u5145\u5206\u5229\u7528\u7a7a\u9593\u548c\u6642\u9593\u5c40\u90e8\u6027\uff0c\u70ba\u5feb\u901f\u7684\u5411\u91cf\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u91cd\u8907\u4f7f\u7528\u92ea\u8def\u3002\u900f\u904e\u5728 Resnet50 \u6a21\u578b\u4e0a\u4f7f\u7528\u6b64\u7cfb\u7d71\uff0c\u6211\u5011\u80fd\u5920\u5c07\u6b0a\u91cd\u6e1b\u534a\uff0c\u540c\u6642\u5c07\u6e96\u78ba\u5ea6\u640d\u5931\u964d\u5230\u6700\u4f4e\uff0c\u9032\u800c\u5c07\u63a8\u8ad6\u901f\u5ea6\u63d0\u5347\u5169\u500d\u3002\u6211\u5011\u5c07\u4f7f\u7528\u91dd\u5c0d Resnet50\u3001Inception V3 \u548c VGG16 \u7684 AIE2 \u7d44\u614b\u96c6\uff08AMD Versal FPGA\uff09\u7684\u6e96\u78ba\u4e14\u5b8c\u6574\u7684\u7a0b\u5f0f\u78bc\u7522\u751f\uff0c\u4f86\u63d0\u51fa\u6548\u80fd\u4f30\u8a08\uff0c\u4ee5\u5c55\u793a\u786c\u9ad4\u758a\u52a0\u8a2d\u8a08\u8207\u7528\u65bc\u7de8\u8b6f\u548c\u57f7\u884c\u6a5f\u5668\u5b78\u7fd2\u61c9\u7528\u7a0b\u5f0f\u7684\u8edf\u9ad4\u5806\u758a\u4e4b\u9593\u5fc5\u8981\u7684\u5354\u540c\u6548\u61c9\u3002", "author": "Paolo D'Alberto et.al.", "authors": "Paolo D'Alberto, Taehee Jeong, Akshai Jain, Shreyas Manjunath, Mrinal Sarmah, Samuel Hsu Yaswanth Raparti, Nitesh Pipralia", "id": "2407.09453v1", "paper_url": "http://arxiv.org/abs/2407.09453v1", "repo": "null"}}