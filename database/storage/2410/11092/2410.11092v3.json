{"2410.11092": {"publish_time": "2024-10-14", "title": "EchoApex: A General-Purpose Vision Foundation Model for Echocardiography", "paper_summary": "Quantitative evaluation of echocardiography is essential for precise\nassessment of cardiac condition, monitoring disease progression, and guiding\ntreatment decisions. The diverse nature of echo images, including variations in\nprobe types, manufacturers, and pathologies, poses challenges for developing\nartificial intelligent models that can generalize across different clinical\npractice. We introduce EchoApex, the first general-purpose vision foundation\nmodel echocardiography with applications on a variety of clinical practice.\nLeveraging self-supervised learning, EchoApex is pretrained on over 20 million\necho images from 11 clinical centres. By incorporating task-specific decoders\nand adapter modules, we demonstrate the effectiveness of EchoApex on 4\ndifferent kind of clinical applications with 28 sub-tasks, including view\nclassification, interactive structure segmentation, left ventricle hypertrophy\ndetection and automated ejection fraction estimation from view sequences.\nCompared to state-of-the-art task-specific models, EchoApex attains improved\nperformance with a unified image encoding architecture, demonstrating the\nbenefits of model pretraining at scale with in-domain data. Furthermore,\nEchoApex illustrates the potential for developing a general-purpose vision\nfoundation model tailored specifically for echocardiography, capable of\naddressing a diverse range of clinical applications with high efficiency and\nefficacy.", "paper_summary_zh": "\u5b9a\u91cf\u8a55\u4f30\u8d85\u97f3\u6ce2\u5fc3\u52d5\u5716\u5c0d\u65bc\u7cbe\u78ba\u8a55\u4f30\u5fc3\u81df\u72c0\u6cc1\u3001\u76e3\u63a7\u75be\u75c5\u9032\u7a0b\u53ca\u6307\u5c0e\u6cbb\u7642\u6c7a\u7b56\u81f3\u95dc\u91cd\u8981\u3002\u8ff4\u97f3\u5f71\u50cf\u7684\u591a\u6a23\u6027\uff0c\u5305\u62ec\u63a2\u982d\u985e\u578b\u3001\u88fd\u9020\u5546\u548c\u75c5\u7406\u5b78\u7684\u8b8a\u5316\uff0c\u5c0d\u958b\u767c\u80fd\u5920\u5728\u4e0d\u540c\u81e8\u5e8a\u5be6\u52d9\u4e2d\u901a\u7528\u7684 AI \u6a21\u578b\u69cb\u6210\u6311\u6230\u3002\u6211\u5011\u63a8\u51fa EchoApex\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u901a\u7528\u8996\u89ba\u57fa\u790e\u6a21\u578b\u8d85\u97f3\u6ce2\u5fc3\u52d5\u5716\uff0c\u53ef\u61c9\u7528\u65bc\u5404\u7a2e\u81e8\u5e8a\u5be6\u52d9\u3002\u900f\u904e\u5229\u7528\u81ea\u76e3\u7763\u5b78\u7fd2\uff0cEchoApex \u5df2\u63a5\u53d7\u4f86\u81ea 11 \u500b\u81e8\u5e8a\u4e2d\u5fc3\u7684\u8d85\u904e 2000 \u842c\u500b\u8ff4\u97f3\u5f71\u50cf\u9810\u8a13\u7df4\u3002\u900f\u904e\u6574\u5408\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u89e3\u78bc\u5668\u548c\u9069\u914d\u5668\u6a21\u7d44\uff0c\u6211\u5011\u5c55\u793a\u4e86 EchoApex \u5728 4 \u7a2e\u4e0d\u540c\u985e\u578b\u7684\u81e8\u5e8a\u61c9\u7528\u4e2d\u64c1\u6709 28 \u500b\u5b50\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u5305\u62ec\u5f71\u50cf\u5206\u985e\u3001\u4e92\u52d5\u7d50\u69cb\u5206\u5272\u3001\u5de6\u5fc3\u5ba4\u80a5\u5927\u6aa2\u6e2c\u548c\u5f9e\u5f71\u50cf\u5e8f\u5217\u4e2d\u81ea\u52d5\u4f30\u8a08\u5c04\u8840\u5206\u6578\u3002\u8207\u6700\u5148\u9032\u7684\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u6a21\u578b\u76f8\u6bd4\uff0cEchoApex \u63a1\u7528\u7d71\u4e00\u7684\u5f71\u50cf\u7de8\u78bc\u67b6\u69cb\u7372\u5f97\u66f4\u597d\u7684\u6548\u80fd\uff0c\u9019\u8b49\u660e\u4e86\u5927\u898f\u6a21\u6a21\u578b\u9810\u8a13\u7df4\u642d\u914d\u9818\u57df\u5167\u8cc7\u6599\u7684\u597d\u8655\u3002\u6b64\u5916\uff0cEchoApex \u8aaa\u660e\u4e86\u958b\u767c\u5c08\u9580\u91dd\u5c0d\u8d85\u97f3\u6ce2\u5fc3\u52d5\u5716\u7684\u901a\u7528\u8996\u89ba\u57fa\u790e\u6a21\u578b\u7684\u6f5b\u529b\uff0c\u80fd\u5920\u4ee5\u9ad8\u6548\u7387\u548c\u6548\u80fd\u89e3\u6c7a\u5404\u7a2e\u81e8\u5e8a\u61c9\u7528\u3002", "author": "Abdoul Aziz Amadou et.al.", "authors": "Abdoul Aziz Amadou, Yue Zhang, Sebastien Piat, Paul Klein, Ingo Schmuecking, Tiziano Passerini, Puneet Sharma", "id": "2410.11092v3", "paper_url": "http://arxiv.org/abs/2410.11092v3", "repo": "null"}}