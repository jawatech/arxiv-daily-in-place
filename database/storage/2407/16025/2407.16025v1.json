{"2407.16025": {"publish_time": "2024-07-22", "title": "Exploring and Addressing Reward Confusion in Offline Preference Learning", "paper_summary": "Spurious correlations in a reward model's training data can prevent\nReinforcement Learning from Human Feedback (RLHF) from identifying the desired\ngoal and induce unwanted behaviors. This paper shows that offline RLHF is\nsusceptible to reward confusion, especially in the presence of spurious\ncorrelations in offline data. We create a benchmark to study this problem and\npropose a method that can significantly reduce reward confusion by leveraging\ntransitivity of preferences while building a global preference chain with\nactive learning.", "paper_summary_zh": "\u5728\u734e\u52f5\u6a21\u578b\u7684\u8a13\u7df4\u8cc7\u6599\u4e2d\uff0c\u865b\u5047\u7684\u95dc\u806f\u6027\u53ef\u80fd\u6703\u963b\u6b62\u5f9e\u4eba\u985e\u56de\u994b\u4e2d\u9032\u884c\u5f37\u5316\u5b78\u7fd2 (RLHF) \u4f86\u8b58\u5225\u6240\u9700\u7684\u76ee\u6a19\uff0c\u4e26\u8a98\u767c\u4e0d\u9700\u8981\u7684\u884c\u70ba\u3002\u672c\u6587\u986f\u793a\uff0c\u96e2\u7dda RLHF \u5bb9\u6613\u53d7\u5230\u734e\u52f5\u6df7\u6dc6\u7684\u5f71\u97ff\uff0c\u7279\u5225\u662f\u5728\u96e2\u7dda\u8cc7\u6599\u4e2d\u5b58\u5728\u865b\u5047\u95dc\u806f\u6027\u7684\u60c5\u6cc1\u4e0b\u3002\u6211\u5011\u5efa\u7acb\u4e00\u500b\u57fa\u6e96\u4f86\u7814\u7a76\u9019\u500b\u554f\u984c\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u53ef\u4ee5\u900f\u904e\u5728\u5efa\u7acb\u5177\u6709\u4e3b\u52d5\u5b78\u7fd2\u7684\u5168\u5c40\u504f\u597d\u93c8\u6642\u5229\u7528\u504f\u597d\u7684\u905e\u79fb\u6027\uff0c\u4f86\u5927\u5e45\u6e1b\u5c11\u734e\u52f5\u6df7\u6dc6\u3002", "author": "Xin Chen et.al.", "authors": "Xin Chen, Sam Toyer, Florian Shkurti", "id": "2407.16025v1", "paper_url": "http://arxiv.org/abs/2407.16025v1", "repo": "null"}}