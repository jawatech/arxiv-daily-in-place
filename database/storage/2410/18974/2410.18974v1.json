{"2410.18974": {"publish_time": "2024-10-24", "title": "3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation", "paper_summary": "Multi-view image diffusion models have significantly advanced open-domain 3D\nobject generation. However, most existing models rely on 2D network\narchitectures that lack inherent 3D biases, resulting in compromised geometric\nconsistency. To address this challenge, we introduce 3D-Adapter, a plug-in\nmodule designed to infuse 3D geometry awareness into pretrained image diffusion\nmodels. Central to our approach is the idea of 3D feedback augmentation: for\neach denoising step in the sampling loop, 3D-Adapter decodes intermediate\nmulti-view features into a coherent 3D representation, then re-encodes the\nrendered RGBD views to augment the pretrained base model through feature\naddition. We study two variants of 3D-Adapter: a fast feed-forward version\nbased on Gaussian splatting and a versatile training-free version utilizing\nneural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter\nnot only greatly enhances the geometry quality of text-to-multi-view models\nsuch as Instant3D and Zero123++, but also enables high-quality 3D generation\nusing the plain text-to-image Stable Diffusion. Furthermore, we showcase the\nbroad application potential of 3D-Adapter by presenting high quality results in\ntext-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.", "paper_summary_zh": "\u591a\u8996\u5716\u5f71\u50cf\u64f4\u6563\u6a21\u578b\u5728\u958b\u653e\u9818\u57df\u7684 3D \u7269\u4ef6\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u5927\u591a\u6578\u6a21\u578b\u4f9d\u8cf4\u65bc\u7f3a\u4e4f\u5167\u5728 3D \u504f\u5dee\u7684 2D \u7db2\u8def\u67b6\u69cb\uff0c\u5c0e\u81f4\u5e7e\u4f55\u4e00\u81f4\u6027\u53d7\u640d\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 3D-Adapter\uff0c\u9019\u662f\u4e00\u500b\u5916\u639b\u6a21\u7d44\uff0c\u65e8\u5728\u5c07 3D \u5e7e\u4f55\u611f\u77e5\u6ce8\u5165\u9810\u5148\u8a13\u7df4\u7684\u5f71\u50cf\u64f4\u6563\u6a21\u578b\u3002\u6211\u5011\u7684\u65b9\u6cd5\u7684\u6838\u5fc3\u662f 3D \u56de\u994b\u589e\u5f37\u7684\u60f3\u6cd5\uff1a\u5c0d\u65bc\u63a1\u6a23\u5faa\u74b0\u4e2d\u7684\u6bcf\u500b\u53bb\u566a\u6b65\u9a5f\uff0c3D-Adapter \u5c07\u4e2d\u9593\u7684\u591a\u8996\u5716\u7279\u5fb5\u89e3\u78bc\u70ba\u4e00\u500b\u9023\u8cab\u7684 3D \u8868\u793a\uff0c\u7136\u5f8c\u91cd\u65b0\u7de8\u78bc\u6e32\u67d3\u7684 RGBD \u8996\u5716\uff0c\u4ee5\u901a\u904e\u7279\u5fb5\u6dfb\u52a0\u4f86\u589e\u5f37\u9810\u5148\u8a13\u7df4\u7684\u57fa\u672c\u6a21\u578b\u3002\u6211\u5011\u7814\u7a76\u4e86 3D-Adapter \u7684\u5169\u500b\u8b8a\u9ad4\uff1a\u57fa\u65bc\u9ad8\u65af\u5674\u5c04\u7684\u5feb\u901f\u524d\u994b\u7248\u672c\u548c\u5229\u7528\u795e\u7d93\u5834\u548c\u7db2\u683c\u7684\u591a\u529f\u80fd\u7121\u8a13\u7df4\u7248\u672c\u3002\u6211\u5011\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0c3D-Adapter \u4e0d\u50c5\u5927\u5927\u589e\u5f37\u4e86 Instant3D \u548c Zero123++ \u7b49\u6587\u5b57\u5230\u591a\u8996\u5716\u6a21\u578b\u7684\u5e7e\u4f55\u54c1\u8cea\uff0c\u800c\u4e14\u9084\u53ef\u4ee5\u4f7f\u7528\u7d14\u6587\u5b57\u5230\u5f71\u50cf\u7684 Stable Diffusion \u5be6\u73fe\u9ad8\u54c1\u8cea\u7684 3D \u751f\u6210\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86 3D-Adapter \u5728\u6587\u5b57\u5230 3D\u3001\u5f71\u50cf\u5230 3D\u3001\u6587\u5b57\u5230\u7d0b\u7406\u548c\u6587\u5b57\u5230\u982d\u50cf\u4efb\u52d9\u4e2d\u5448\u73fe\u7684\u9ad8\u54c1\u8cea\u7d50\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5ee3\u6cdb\u7684\u61c9\u7528\u6f5b\u529b\u3002", "author": "Hansheng Chen et.al.", "authors": "Hansheng Chen, Bokui Shen, Yulin Liu, Ruoxi Shi, Linqi Zhou, Connor Z. Lin, Jiayuan Gu, Hao Su, Gordon Wetzstein, Leonidas Guibas", "id": "2410.18974v1", "paper_url": "http://arxiv.org/abs/2410.18974v1", "repo": "https://github.com/Lakonik/MVEdit"}}