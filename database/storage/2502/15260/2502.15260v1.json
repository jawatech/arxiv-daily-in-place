{"2502.15260": {"publish_time": "2025-02-21", "title": "LightMamba: Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design", "paper_summary": "State space models (SSMs) like Mamba have recently attracted much attention.\nCompared to Transformer-based large language models (LLMs), Mamba achieves\nlinear computation complexity with the sequence length and demonstrates\nsuperior performance. However, Mamba is hard to accelerate due to the scattered\nactivation outliers and the complex computation dependency, rendering existing\nLLM accelerators inefficient. In this paper, we propose LightMamba that\nco-designs the quantization algorithm and FPGA accelerator architecture for\nefficient Mamba inference. We first propose an FPGA-friendly post-training\nquantization algorithm that features rotation-assisted quantization and\npower-of-two SSM quantization to reduce the majority of computation to 4-bit.\nWe further design an FPGA accelerator that partially unrolls the Mamba\ncomputation to balance the efficiency and hardware costs. Through computation\nreordering as well as fine-grained tiling and fusion, the hardware utilization\nand memory efficiency of the accelerator get drastically improved. We implement\nLightMamba on Xilinx Versal VCK190 FPGA and achieve 4.65x to 6.06x higher\nenergy efficiency over the GPU baseline. When evaluated on Alveo U280 FPGA,\nLightMamba reaches 93 tokens/s, which is 1.43x that of the GPU baseline.", "paper_summary_zh": "\u72c0\u614b\u7a7a\u9593\u6a21\u578b\uff08SSM\uff09\uff0c\u4f8b\u5982 Mamba\uff0c\u6700\u8fd1\u5099\u53d7\u95dc\u6ce8\u3002\n\u8207\u57fa\u65bc Transformer \u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u76f8\u6bd4\uff0cMamba \u5be6\u73fe\u4e86\u7dda\u6027\u8a08\u7b97\u8907\u96dc\u5ea6\uff0c\u4e26\u8868\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u7531\u65bc\u5206\u6563\u7684\u6fc0\u6d3b\u7570\u5e38\u503c\u548c\u8907\u96dc\u7684\u8a08\u7b97\u4f9d\u8cf4\u6027\uff0cMamba \u96e3\u4ee5\u52a0\u901f\uff0c\u5c0e\u81f4\u73fe\u6709\u7684 LLM \u52a0\u901f\u5668\u6548\u7387\u4f4e\u4e0b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LightMamba\uff0c\u5b83\u5354\u540c\u8a2d\u8a08\u4e86\u91cf\u5316\u6f14\u7b97\u6cd5\u548c FPGA \u52a0\u901f\u5668\u67b6\u69cb\uff0c\u4ee5\u5be6\u73fe\u9ad8\u6548\u7684 Mamba \u63a8\u8ad6\u3002\u6211\u5011\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u500b FPGA \u53cb\u5584\u7684\u8a13\u7df4\u5f8c\u91cf\u5316\u6f14\u7b97\u6cd5\uff0c\u5176\u7279\u9ede\u662f\u65cb\u8f49\u8f14\u52a9\u91cf\u5316\u548c 2 \u7684\u51aa SSM \u91cf\u5316\uff0c\u4ee5\u5c07\u5927\u90e8\u5206\u8a08\u7b97\u6e1b\u5c11\u5230 4 \u4f4d\u5143\u3002\u6211\u5011\u9032\u4e00\u6b65\u8a2d\u8a08\u4e86\u4e00\u500b FPGA \u52a0\u901f\u5668\uff0c\u5b83\u90e8\u5206\u5c55\u958b\u4e86 Mamba \u8a08\u7b97\uff0c\u4ee5\u5e73\u8861\u6548\u7387\u548c\u786c\u9ad4\u6210\u672c\u3002\u900f\u904e\u8a08\u7b97\u91cd\u65b0\u6392\u5e8f\u4ee5\u53ca\u7d30\u7c92\u5ea6\u7684\u5e73\u92ea\u548c\u878d\u5408\uff0c\u52a0\u901f\u5668\u7684\u786c\u9ad4\u5229\u7528\u7387\u548c\u8a18\u61b6\u9ad4\u6548\u7387\u5f97\u5230\u4e86\u986f\u8457\u7684\u6539\u5584\u3002\u6211\u5011\u5728 Xilinx Versal VCK190 FPGA \u4e0a\u5be6\u4f5c LightMamba\uff0c\u4e26\u5728 GPU \u57fa\u6e96\u4e0a\u5be6\u73fe\u4e86 4.65 \u500d\u5230 6.06 \u500d\u7684\u80fd\u6548\u63d0\u5347\u3002\u5728 Alveo U280 FPGA \u4e0a\u8a55\u4f30\u6642\uff0cLightMamba \u9054\u5230\u4e86 93 \u500b token/s\uff0c\u662f GPU \u57fa\u6e96\u7684 1.43 \u500d\u3002", "author": "Renjie Wei et.al.", "authors": "Renjie Wei, Songqiang Xu, Linfeng Zhong, Zebin Yang, Qingyu Guo, Yuan Wang, Runsheng Wang, Meng Li", "id": "2502.15260v1", "paper_url": "http://arxiv.org/abs/2502.15260v1", "repo": "null"}}