{"2407.12772": {"publish_time": "2024-07-17", "title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models", "paper_summary": "The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.", "paper_summary_zh": "\u5927\u578b\u57fa\u790e\u6a21\u578b\u7684\u9032\u5c55\u9700\u8981\u5ee3\u6cdb\u6db5\u84cb\u3001\u4f4e\u6210\u672c\u4e14\u96f6\u6c61\u67d3\u7684\u57fa\u6e96\u3002\u5118\u7ba1\u6301\u7e8c\u63a2\u7d22\u8a9e\u8a00\u6a21\u578b\u8a55\u4f30\uff0c\u4f46\u5c0d\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u8a55\u4f30\u7684\u5168\u9762\u7814\u7a76\u4ecd\u7136\u6709\u9650\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 LMMS-EVAL\uff0c\u4e00\u500b\u7d71\u4e00\u4e14\u6a19\u6e96\u5316\u7684\u591a\u6a21\u614b\u57fa\u6e96\u67b6\u69cb\uff0c\u5305\u542b\u8d85\u904e 50 \u9805\u4efb\u52d9\u548c 10 \u500b\u4ee5\u4e0a\u6a21\u578b\uff0c\u4ee5\u4fc3\u9032\u900f\u660e\u4e14\u53ef\u91cd\u73fe\u7684\u8a55\u4f30\u3002\u5118\u7ba1 LMMS-EVAL \u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6db5\u84cb\u7bc4\u570d\uff0c\u4f46\u6211\u5011\u767c\u73fe\u5b83\u5728\u5be6\u73fe\u4f4e\u6210\u672c\u548c\u96f6\u6c61\u67d3\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u8a55\u4f30\u4e09\u96e3\u56f0\u5883\uff0c\u6211\u5011\u9032\u4e00\u6b65\u5f15\u5165\u4e86 LMMS-EVAL LITE\uff0c\u9019\u662f\u4e00\u500b\u7cbe\u7c21\u7684\u8a55\u4f30\u5de5\u5177\u5305\uff0c\u5f37\u8abf\u6db5\u84cb\u7bc4\u570d\u548c\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u591a\u6a21\u614b LIVEBENCH\uff0c\u5b83\u5229\u7528\u6301\u7e8c\u66f4\u65b0\u7684\u65b0\u805e\u548c\u7dda\u4e0a\u8ad6\u58c7\u4f86\u8a55\u4f30\u6a21\u578b\u5728\u91ce\u5916\u7684\u6982\u62ec\u80fd\u529b\uff0c\u63a1\u7528\u4f4e\u6210\u672c\u4e14\u96f6\u6c61\u67d3\u7684\u8a55\u4f30\u65b9\u6cd5\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u7684\u7814\u7a76\u5f37\u8abf\u4e86\u8003\u616e\u8a55\u4f30\u4e09\u96e3\u56f0\u5883\u7684\u91cd\u8981\u6027\uff0c\u4e26\u63d0\u4f9b\u4e86\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u4f86\u6b0a\u8861\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\u7684\u8a55\u4f30\uff0c\u70ba\u66f4\u6709\u6548\u548c\u53ef\u9760\u7684 LMM \u57fa\u6e96\u6e2c\u8a66\u92ea\u5e73\u9053\u8def\u3002\u6211\u5011\u5728 https://github.com/EvolvingLMMs-Lab/lmms-eval \u548c https://huggingface.co/spaces/lmms-lab/LiveBench \u958b\u6e90\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5eab\u4e26\u7dad\u8b77 LIVEBENCH \u7684\u6392\u884c\u699c\u3002", "author": "Kaichen Zhang et.al.", "authors": "Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, Ziwei Liu", "id": "2407.12772v1", "paper_url": "http://arxiv.org/abs/2407.12772v1", "repo": "https://github.com/evolvinglmms-lab/lmms-eval"}}