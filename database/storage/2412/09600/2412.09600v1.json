{"2412.09600": {"publish_time": "2024-12-12", "title": "Owl-1: Omni World Model for Consistent Long Video Generation", "paper_summary": "Video generation models (VGMs) have received extensive attention recently and\nserve as promising candidates for general-purpose large vision models. While\nthey can only generate short videos each time, existing methods achieve long\nvideo generation by iteratively calling the VGMs, using the last-frame output\nas the condition for the next-round generation. However, the last frame only\ncontains short-term fine-grained information about the scene, resulting in\ninconsistency in the long horizon. To address this, we propose an Omni World\nmodeL (Owl-1) to produce long-term coherent and comprehensive conditions for\nconsistent long video generation. As videos are observations of the underlying\nevolving world, we propose to model the long-term developments in a latent\nspace and use VGMs to film them into videos. Specifically, we represent the\nworld with a latent state variable which can be decoded into explicit video\nobservations. These observations serve as a basis for anticipating temporal\ndynamics which in turn update the state variable. The interaction between\nevolving dynamics and persistent state enhances the diversity and consistency\nof the long videos. Extensive experiments show that Owl-1 achieves comparable\nperformance with SOTA methods on VBench-I2V and VBench-Long, validating its\nability to generate high-quality video observations. Code:\nhttps://github.com/huang-yh/Owl.", "paper_summary_zh": "\u5f71\u7247\u751f\u6210\u6a21\u578b\uff08VGM\uff09\u6700\u8fd1\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5e76\u6210\u4e3a\u901a\u7528\u5927\u578b\u89c6\u89c9\u6a21\u578b\u7684\u6709\u529b\u5019\u9009\u8005\u3002\u867d\u7136\u5b83\u4eec\u6bcf\u6b21\u53ea\u80fd\u751f\u6210\u77ed\u5f71\u7247\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u8c03\u7528 VGM\uff0c\u4f7f\u7528\u6700\u540e\u4e00\u5e27\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u8f6e\u751f\u6210\u7684\u6761\u4ef6\u6765\u5b9e\u73b0\u957f\u5f71\u7247\u751f\u6210\u3002\u7136\u800c\uff0c\u6700\u540e\u4e00\u5e27\u4ec5\u5305\u542b\u5173\u4e8e\u573a\u666f\u7684\u77ed\u671f\u7ec6\u7c92\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u957f\u671f\u89c6\u754c\u4e0d\u4e00\u81f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u666f\u4e16\u754c\u6a21\u578b (Owl-1) \u6765\u4ea7\u751f\u957f\u671f\u8fde\u8d2f\u4e14\u5168\u9762\u7684\u6761\u4ef6\uff0c\u4ee5\u5b9e\u73b0\u4e00\u81f4\u7684\u957f\u5f71\u7247\u751f\u6210\u3002\u7531\u4e8e\u5f71\u7247\u662f\u5bf9\u5e95\u5c42\u6f14\u5316\u4e16\u754c\u7684\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u8bae\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u957f\u671f\u53d1\u5c55\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u4f7f\u7528 VGM \u5c06\u5176\u62cd\u6444\u6210\u5f71\u7247\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u7528\u4e00\u4e2a\u6f5c\u5728\u72b6\u6001\u53d8\u91cf\u8868\u793a\u4e16\u754c\uff0c\u53ef\u4ee5\u5c06\u5176\u89e3\u7801\u4e3a\u660e\u786e\u7684\u5f71\u7247\u89c2\u5bdf\u3002\u8fd9\u4e9b\u89c2\u5bdf\u4f5c\u4e3a\u9884\u6d4b\u65f6\u95f4\u52a8\u6001\u7684\u57fa\u7840\uff0c\u8fdb\u800c\u66f4\u65b0\u72b6\u6001\u53d8\u91cf\u3002\u6f14\u5316\u52a8\u6001\u548c\u6301\u4e45\u72b6\u6001\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u589e\u5f3a\u4e86\u957f\u5f71\u7247\u7684\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOwl-1 \u5728 VBench-I2V \u548c VBench-Long \u4e0a\u5b9e\u73b0\u4e86\u4e0e SOTA \u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u751f\u6210\u9ad8\u8d28\u91cf\u5f71\u7247\u89c2\u5bdf\u7684\u80fd\u529b\u3002\u4ee3\u7801\uff1ahttps://github.com/huang-yh/Owl\u3002", "author": "Yuanhui Huang et.al.", "authors": "Yuanhui Huang, Wenzhao Zheng, Yuan Gao, Xin Tao, Pengfei Wan, Di Zhang, Jie Zhou, Jiwen Lu", "id": "2412.09600v1", "paper_url": "http://arxiv.org/abs/2412.09600v1", "repo": "https://github.com/huang-yh/owl"}}