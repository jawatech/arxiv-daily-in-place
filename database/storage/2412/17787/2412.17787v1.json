{"2412.17787": {"publish_time": "2024-12-23", "title": "Cross-Lingual Text-Rich Visual Comprehension: An Information Theory Perspective", "paper_summary": "Recent Large Vision-Language Models (LVLMs) have shown promising reasoning\ncapabilities on text-rich images from charts, tables, and documents. However,\nthe abundant text within such images may increase the model's sensitivity to\nlanguage. This raises the need to evaluate LVLM performance on cross-lingual\ntext-rich visual inputs, where the language in the image differs from the\nlanguage of the instructions. To address this, we introduce XT-VQA\n(Cross-Lingual Text-Rich Visual Question Answering), a benchmark designed to\nassess how LVLMs handle language inconsistency between image text and\nquestions. XT-VQA integrates five existing text-rich VQA datasets and a newly\ncollected dataset, XPaperQA, covering diverse scenarios that require faithful\nrecognition and comprehension of visual information despite language\ninconsistency. Our evaluation of prominent LVLMs on XT-VQA reveals a\nsignificant drop in performance for cross-lingual scenarios, even for models\nwith multilingual capabilities. A mutual information analysis suggests that\nthis performance gap stems from cross-lingual questions failing to adequately\nactivate relevant visual information. To mitigate this issue, we propose\nMVCL-MI (Maximization of Vision-Language Cross-Lingual Mutual Information),\nwhere a visual-text cross-lingual alignment is built by maximizing mutual\ninformation between the model's outputs and visual information. This is\nachieved by distilling knowledge from monolingual to cross-lingual settings\nthrough KL divergence minimization, where monolingual output logits serve as a\nteacher. Experimental results on the XT-VQA demonstrate that MVCL-MI\neffectively reduces the visual-text cross-lingual performance disparity while\npreserving the inherent capabilities of LVLMs, shedding new light on the\npotential practice for improving LVLMs. Codes are available at:\nhttps://github.com/Stardust-y/XTVQA.git", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u5728\u56fe\u8868\u3001\u8868\u683c\u548c\u6587\u6863\u4e2d\u7684\u6587\u672c\u4e30\u5bcc\u7684\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u6709\u5e0c\u671b\u7684\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u6b64\u7c7b\u56fe\u50cf\u4e2d\u4e30\u5bcc\u7684\u6587\u672c\u53ef\u80fd\u4f1a\u589e\u52a0\u6a21\u578b\u5bf9\u8bed\u8a00\u7684\u654f\u611f\u6027\u3002\u8fd9\u5f15\u53d1\u4e86\u9700\u8981\u8bc4\u4f30 LVLM \u5728\u8de8\u8bed\u8a00\u6587\u672c\u4e30\u5bcc\u7684\u89c6\u89c9\u8f93\u5165\u4e0a\u7684\u6027\u80fd\uff0c\u5176\u4e2d\u56fe\u50cf\u4e2d\u7684\u8bed\u8a00\u4e0e\u6307\u4ee4\u7684\u8bed\u8a00\u4e0d\u540c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 XT-VQA\uff08\u8de8\u8bed\u8a00\u6587\u672c\u4e30\u5bcc\u7684\u89c6\u89c9\u95ee\u9898\u89e3\u7b54\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u8bc4\u4f30 LVLM \u5982\u4f55\u5904\u7406\u56fe\u50cf\u6587\u672c\u548c\u95ee\u9898\u4e4b\u95f4\u7684\u8bed\u8a00\u4e0d\u4e00\u81f4\u7684\u57fa\u51c6\u3002XT-VQA \u96c6\u6210\u4e86\u4e94\u4e2a\u73b0\u6709\u7684\u6587\u672c\u4e30\u5bcc\u7684 VQA \u6570\u636e\u96c6\u548c\u4e00\u4e2a\u65b0\u6536\u96c6\u7684\u6570\u636e\u96c6 XPaperQA\uff0c\u6db5\u76d6\u4e86\u9700\u8981\u5fe0\u5b9e\u8bc6\u522b\u548c\u7406\u89e3\u89c6\u89c9\u4fe1\u606f\u7684\u4e0d\u540c\u573a\u666f\uff0c\u5c3d\u7ba1\u5b58\u5728\u8bed\u8a00\u4e0d\u4e00\u81f4\u3002\u6211\u4eec\u5bf9 XT-VQA \u4e0a\u7684\u7a81\u51fa LVLM \u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5373\u4f7f\u5bf9\u4e8e\u5177\u6709\u591a\u8bed\u8a00\u80fd\u529b\u7684\u6a21\u578b\uff0c\u8de8\u8bed\u8a00\u573a\u666f\u7684\u6027\u80fd\u4e5f\u5927\u5e45\u4e0b\u964d\u3002\u4e92\u4fe1\u606f\u5206\u6790\u8868\u660e\uff0c\u8fd9\u79cd\u6027\u80fd\u5dee\u8ddd\u6e90\u4e8e\u8de8\u8bed\u8a00\u95ee\u9898\u672a\u80fd\u5145\u5206\u6fc0\u6d3b\u76f8\u5173\u7684\u89c6\u89c9\u4fe1\u606f\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 MVCL-MI\uff08\u89c6\u89c9\u8bed\u8a00\u8de8\u8bed\u8a00\u4e92\u4fe1\u606f\u7684\u6700\u5927\u5316\uff09\uff0c\u5176\u4e2d\u901a\u8fc7\u6700\u5927\u5316\u6a21\u578b\u8f93\u51fa\u548c\u89c6\u89c9\u4fe1\u606f\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u6784\u5efa\u89c6\u89c9\u6587\u672c\u8de8\u8bed\u8a00\u5bf9\u9f50\u3002\u8fd9\u662f\u901a\u8fc7 KL \u6563\u5ea6\u6700\u5c0f\u5316\u4ece\u5355\u8bed\u8a00\u5230\u8de8\u8bed\u8a00\u8bbe\u7f6e\u63d0\u53d6\u77e5\u8bc6\u6765\u5b9e\u73b0\u7684\uff0c\u5176\u4e2d\u5355\u8bed\u8a00\u8f93\u51fa logit \u5145\u5f53\u6559\u5e08\u3002XT-VQA \u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMVCL-MI \u6709\u6548\u5730\u51cf\u5c11\u4e86\u89c6\u89c9\u6587\u672c\u8de8\u8bed\u8a00\u6027\u80fd\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u7559\u4e86 LVLMs \u7684\u56fa\u6709\u80fd\u529b\uff0c\u4e3a\u6539\u8fdb LVLMs \u7684\u6f5c\u5728\u5b9e\u8df5\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u83b7\u5f97\uff1a\nhttps://github.com/Stardust-y/XTVQA.git</paragraph>", "author": "Xinmiao Yu et.al.", "authors": "Xinmiao Yu, Xiaocheng Feng, Yun Li, Minghui Liao, Ya-Qi Yu, Xiachong Feng, Weihong Zhong, Ruihan Chen, Mengkang Hu, Jihao Wu, Dandan Tu, Duyu Tang, Bing Qin", "id": "2412.17787v1", "paper_url": "http://arxiv.org/abs/2412.17787v1", "repo": "null"}}