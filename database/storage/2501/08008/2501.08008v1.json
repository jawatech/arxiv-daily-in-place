{"2501.08008": {"publish_time": "2025-01-14", "title": "TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for Parameter-Efficient Fine-Tuning", "paper_summary": "The fine-tuning of Large Language Models (LLMs) is pivotal for achieving\noptimal performance across diverse downstream tasks. However, while full\nfine-tuning delivers superior results, it entails significant computational and\nresource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\naddress these challenges by reducing the number of trainable parameters, but\nthey often struggle with rank adjustment efficiency and task-specific\nadaptability. We propose Triangular Adaptive Low-Rank Adaptation\n(TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles,\nwhich dynamically optimizes the allocation of trainable parameters.\nTriAdaptLoRA introduces three key innovations: 1) a triangular split of\ntransformation matrices into lower and upper triangular components to maximize\nparameter utilization, 2) a parameter importance metric based on normalized\nFrobenius norms for efficient adaptation, and 3) an adaptive rank-growth\nstrategy governed by dynamic thresholds, allowing flexible parameter allocation\nacross training steps. Experiments conducted on a variety of natural language\nunderstanding and generation tasks demonstrate that TriAdaptLoRA consistently\noutperforms existing PEFT methods. It achieves superior performance, enhanced\nstability, and reduced computational overhead, particularly under linear\nthreshold-driven rank growth. These results highlight its efficacy as a\nscalable and resource-efficient solution for fine-tuning LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5fae\u8abf\u5c0d\u65bc\u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e2d\u5be6\u73fe\u6700\u4f73\u6548\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u5118\u7ba1\u5b8c\u5168\u5fae\u8abf\u53ef\u63d0\u4f9b\u512a\u7570\u7684\u7d50\u679c\uff0c\u4f46\u5b83\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u548c\u8cc7\u6e90\u6210\u672c\u3002\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\uff08\u4f8b\u5982 LoRA\uff09\u900f\u904e\u6e1b\u5c11\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u6578\u91cf\u4f86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u96e3\u4ee5\u61c9\u4ed8\u79e9\u8abf\u6574\u6548\u7387\u548c\u7279\u5b9a\u4efb\u52d9\u7684\u9069\u61c9\u6027\u3002\u6211\u5011\u63d0\u51fa\u4e09\u89d2\u81ea\u9069\u61c9\u4f4e\u79e9\u9069\u61c9 (TriAdaptLoRA)\uff0c\u9019\u662f\u4e00\u500b\u53d7\u795e\u7d93\u79d1\u5b78\u539f\u7406\u555f\u767c\u7684\u65b0\u7a4e PEFT \u67b6\u69cb\uff0c\u5b83\u52d5\u614b\u6700\u4f73\u5316\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u914d\u7f6e\u3002TriAdaptLoRA \u5f15\u5165\u4e86\u4e09\u9805\u95dc\u9375\u5275\u65b0\uff1a1) \u5c07\u8f49\u63db\u77e9\u9663\u4e09\u89d2\u5f62\u5206\u5272\u70ba\u4e0b\u4e09\u89d2\u5f62\u548c\u4e0a\u4e09\u89d2\u5f62\u7d44\u6210\uff0c\u4ee5\u6700\u5927\u5316\u53c3\u6578\u5229\u7528\u7387\uff0c2) \u4e00\u500b\u57fa\u65bc\u6b63\u898f\u5316\u5f17\u7f85\u8c9d\u5c3c\u70cf\u65af\u7bc4\u6578\u7684\u53c3\u6578\u91cd\u8981\u6027\u5ea6\u91cf\uff0c\u4ee5\u5229\u65bc\u9ad8\u6548\u9069\u61c9\uff0c\u4ee5\u53ca 3) \u4e00\u500b\u7531\u52d5\u614b\u95be\u503c\u63a7\u5236\u7684\u81ea\u9069\u61c9\u79e9\u589e\u9577\u7b56\u7565\uff0c\u5141\u8a31\u5728\u8a13\u7df4\u6b65\u9a5f\u4e2d\u9748\u6d3b\u914d\u7f6e\u53c3\u6578\u3002\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u4efb\u52d9\u4e2d\u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\uff0cTriAdaptLoRA \u6301\u7e8c\u512a\u65bc\u73fe\u6709\u7684 PEFT \u65b9\u6cd5\u3002\u5b83\u5728\u7dda\u6027\u95be\u503c\u9a45\u52d5\u7684\u79e9\u589e\u9577\u4e0b\uff0c\u7279\u5225\u662f\u5be6\u73fe\u4e86\u512a\u7570\u7684\u6548\u80fd\u3001\u589e\u5f37\u7684\u7a69\u5b9a\u6027\u548c\u6e1b\u5c11\u7684\u904b\u7b97\u8ca0\u64d4\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86\u5b83\u4f5c\u70ba\u53ef\u64f4\u5145\u4e14\u8cc7\u6e90\u9ad8\u6548\u7684 LLM \u5fae\u8abf\u89e3\u6c7a\u65b9\u6848\u7684\u6548\u529b\u3002", "author": "Yao Liang et.al.", "authors": "Yao Liang, Yuwei Wang, Yi Zeng", "id": "2501.08008v1", "paper_url": "http://arxiv.org/abs/2501.08008v1", "repo": "null"}}