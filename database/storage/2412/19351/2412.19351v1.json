{"2412.19351": {"publish_time": "2024-12-26", "title": "ETTA: Elucidating the Design Space of Text-to-Audio Models", "paper_summary": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,\nenabling users to enrich their creative workflows with synthetic audio\ngenerated from natural language prompts. Despite this progress, the effects of\ndata, model architecture, training objective functions, and sampling strategies\non target benchmarks are not well understood. With the purpose of providing a\nholistic understanding of the design space of TTA models, we set up a\nlarge-scale empirical experiment focused on diffusion and flow matching models.\nOur contributions include: 1) AF-Synthetic, a large dataset of high quality\nsynthetic captions obtained from an audio understanding model; 2) a systematic\ncomparison of different architectural, training, and inference design choices\nfor TTA models; 3) an analysis of sampling methods and their Pareto curves with\nrespect to generation quality and inference speed. We leverage the knowledge\nobtained from this extensive analysis to propose our best model dubbed\nElucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,\nETTA provides improvements over the baselines trained on publicly available\ndata, while being competitive with models trained on proprietary data. Finally,\nwe show ETTA's improved ability to generate creative audio following complex\nand imaginative captions -- a task that is more challenging than current\nbenchmarks.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u6587\u5b57\u8f49\u97f3\u8a0a (TTA) \u5408\u6210\u6280\u8853\u7372\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f7f\u7528\u6236\u80fd\u5920\u900f\u904e\u5f9e\u81ea\u7136\u8a9e\u8a00\u63d0\u793a\u7522\u751f\u7684\u5408\u6210\u97f3\u8a0a\u4f86\u8c50\u5bcc\u5176\u5275\u4f5c\u6d41\u7a0b\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u9032\u5c55\uff0c\u4f46\u8cc7\u6599\u3001\u6a21\u578b\u67b6\u69cb\u3001\u8a13\u7df4\u76ee\u6a19\u51fd\u6578\u548c\u62bd\u6a23\u7b56\u7565\u5c0d\u76ee\u6a19\u57fa\u6e96\u7684\u5f71\u97ff\u4ecd\u672a\u88ab\u5145\u5206\u4e86\u89e3\u3002\u70ba\u4e86\u5168\u9762\u4e86\u89e3 TTA \u6a21\u578b\u7684\u8a2d\u8a08\u7a7a\u9593\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u91dd\u5c0d\u64f4\u6563\u548c\u6d41\u52d5\u5339\u914d\u6a21\u578b\u7684\u5927\u898f\u6a21\u5be6\u8b49\u5be6\u9a57\u3002\u6211\u5011\u7684\u8ca2\u737b\u5305\u62ec\uff1a1) AF-Synthetic\uff0c\u4e00\u500b\u5f9e\u97f3\u8a0a\u7406\u89e3\u6a21\u578b\u4e2d\u7372\u5f97\u7684\u9ad8\u54c1\u8cea\u5408\u6210\u6a19\u984c\u7684\u5927\u578b\u8cc7\u6599\u96c6\uff1b2) \u5c0d TTA \u6a21\u578b\u7684\u4e0d\u540c\u67b6\u69cb\u3001\u8a13\u7df4\u548c\u63a8\u8ad6\u8a2d\u8a08\u9078\u64c7\u9032\u884c\u7cfb\u7d71\u6bd4\u8f03\uff1b3) \u5c0d\u62bd\u6a23\u65b9\u6cd5\u53ca\u5176\u5728\u751f\u6210\u54c1\u8cea\u548c\u63a8\u8ad6\u901f\u5ea6\u65b9\u9762\u7684\u5e15\u7d2f\u6258\u66f2\u7dda\u9032\u884c\u5206\u6790\u3002\u6211\u5011\u5229\u7528\u5f9e\u9019\u9805\u5ee3\u6cdb\u5206\u6790\u4e2d\u7372\u5f97\u7684\u77e5\u8b58\u4f86\u63d0\u51fa\u6211\u5011\u6700\u597d\u7684\u6a21\u578b\uff0c\u7a31\u70ba Elucidated Text-To-Audio (ETTA)\u3002\u5728 AudioCaps \u548c MusicCaps \u4e0a\u9032\u884c\u8a55\u4f30\u6642\uff0cETTA \u5c0d\u4f7f\u7528\u516c\u958b\u8cc7\u6599\u8a13\u7df4\u7684\u57fa\u7dda\u6a21\u578b\u63d0\u4f9b\u4e86\u6539\u9032\uff0c\u540c\u6642\u8207\u4f7f\u7528\u5c08\u6709\u8cc7\u6599\u8a13\u7df4\u7684\u6a21\u578b\u5177\u6709\u7af6\u722d\u529b\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86 ETTA \u5728\u751f\u6210\u9075\u5faa\u8907\u96dc\u4e14\u5bcc\u6709\u60f3\u50cf\u529b\u7684\u6a19\u984c\u7684\u5275\u610f\u97f3\u8a0a\u65b9\u9762\u7684\u6539\u9032\u80fd\u529b\u2014\u2014\u9019\u9805\u4efb\u52d9\u6bd4\u76ee\u524d\u7684\u57fa\u6e96\u66f4\u5177\u6311\u6230\u6027\u3002", "author": "Sang-gil Lee et.al.", "authors": "Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro", "id": "2412.19351v1", "paper_url": "http://arxiv.org/abs/2412.19351v1", "repo": "null"}}