{"2406.12442": {"publish_time": "2024-06-18", "title": "Abstraction-of-Thought Makes Language Models Better Reasoners", "paper_summary": "Abstract reasoning, the ability to reason from the abstract essence of a\nproblem, serves as a key to generalization in human reasoning. However,\neliciting language models to perform reasoning with abstraction remains\nunexplored. This paper seeks to bridge this gap by introducing a novel\nstructured reasoning format called Abstraction-of-Thought (AoT). The uniqueness\nof AoT lies in its explicit requirement for varying levels of abstraction\nwithin the reasoning process. This approach could elicit language models to\nfirst contemplate on the abstract level before incorporating concrete details,\nwhich is overlooked by the prevailing step-by-step Chain-of-Thought (CoT)\nmethod. To align models with the AoT format, we present AoT Collection, a\ngeneric finetuning dataset consisting of 348k high-quality samples with AoT\nreasoning processes, collected via an automated and scalable pipeline. We\nfinetune a wide range of language models with AoT Collection and conduct\nextensive evaluations on 23 unseen tasks from the challenging benchmark\nBig-Bench Hard. Experimental results indicate that models aligned to AoT\nreasoning format substantially outperform those aligned to CoT in many\nreasoning tasks.", "paper_summary_zh": "\u62bd\u8c61\u63a8\u7406\uff0c\u5f9e\u554f\u984c\u7684\u62bd\u8c61\u672c\u8cea\u9032\u884c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u662f\u4eba\u985e\u63a8\u7406\u4e2d\u6982\u5316\u7684\u95dc\u9375\u3002\u7136\u800c\uff0c\u5f15\u767c\u8a9e\u8a00\u6a21\u578b\u9032\u884c\u62bd\u8c61\u63a8\u7406\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u901a\u904e\u5f15\u5165\u4e00\u7a2e\u7a31\u70ba\u62bd\u8c61\u601d\u7dad (AoT) \u7684\u65b0\u7d50\u69cb\u63a8\u7406\u683c\u5f0f\u4f86\u5f4c\u88dc\u9019\u4e00\u5dee\u8ddd\u3002AoT \u7684\u7368\u7279\u6027\u5728\u65bc\u5b83\u5c0d\u63a8\u7406\u904e\u7a0b\u4e2d\u4e0d\u540c\u62bd\u8c61\u5c64\u7d1a\u7684\u660e\u78ba\u8981\u6c42\u3002\u9019\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u5f15\u767c\u8a9e\u8a00\u6a21\u578b\u5728\u7d0d\u5165\u5177\u9ad4\u7d30\u7bc0\u4e4b\u524d\u9996\u5148\u601d\u8003\u62bd\u8c61\u5c64\u7d1a\uff0c\u800c\u9019\u6b63\u662f\u73fe\u884c\u7684\u9010\u6b65\u601d\u8003\u93c8 (CoT) \u65b9\u6cd5\u6240\u5ffd\u7565\u7684\u3002\u70ba\u4e86\u4f7f\u6a21\u578b\u8207 AoT \u683c\u5f0f\u4fdd\u6301\u4e00\u81f4\uff0c\u6211\u5011\u63d0\u51fa\u4e86 AoT \u96c6\u5408\uff0c\u9019\u662f\u4e00\u500b\u901a\u7528\u7684\u5fae\u8abf\u8cc7\u6599\u96c6\uff0c\u5305\u542b 348k \u500b\u5177\u6709 AoT \u63a8\u7406\u904e\u7a0b\u7684\u9ad8\u54c1\u8cea\u7bc4\u4f8b\uff0c\u4e26\u900f\u904e\u81ea\u52d5\u5316\u4e14\u53ef\u64f4\u5145\u7684\u7ba1\u9053\u6536\u96c6\u3002\u6211\u5011\u4f7f\u7528 AoT \u96c6\u5408\u5c0d\u5404\u7a2e\u8a9e\u8a00\u6a21\u578b\u9032\u884c\u5fae\u8abf\uff0c\u4e26\u5c0d\u4f86\u81ea\u5177\u6709\u6311\u6230\u6027\u7684\u57fa\u6e96 Big-Bench Hard \u7684 23 \u500b\u672a\u898b\u4efb\u52d9\u9032\u884c\u5ee3\u6cdb\u8a55\u4f30\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207 AoT \u63a8\u7406\u683c\u5f0f\u4fdd\u6301\u4e00\u81f4\u7684\u6a21\u578b\u5728\u8a31\u591a\u63a8\u7406\u4efb\u52d9\u4e2d\u660e\u986f\u512a\u65bc\u8207 CoT \u4fdd\u6301\u4e00\u81f4\u7684\u6a21\u578b\u3002", "author": "Ruixin Hong et.al.", "authors": "Ruixin Hong, Hongming Zhang, Xiaoman Pan, Dong Yu, Changshui Zhang", "id": "2406.12442v1", "paper_url": "http://arxiv.org/abs/2406.12442v1", "repo": "null"}}