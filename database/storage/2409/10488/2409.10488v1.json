{"2409.10488": {"publish_time": "2024-09-16", "title": "Do Pre-trained Vision-Language Models Encode Object States?", "paper_summary": "For a vision-language model (VLM) to understand the physical world, such as\ncause and effect, a first step is to capture the temporal dynamics of the\nvisual world, for example how the physical states of objects evolve over time\n(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs\npre-trained on web-scale data learn to encode object states, which can be\nextracted with zero-shot text prompts. We curate an object state recognition\ndataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models\ntrained with contrastive and generative objectives. We observe that while these\nstate-of-the-art vision-language models can reliably perform object\nrecognition, they consistently fail to accurately distinguish the objects'\nphysical states. Through extensive experiments, we identify three areas for\nimprovements for VLMs to better encode object states, namely the quality of\nobject localization, the architecture to bind concepts to objects, and the\nobjective to learn discriminative visual and language encoders on object\nstates. Data and code are released.", "paper_summary_zh": "\u5c0d\u65bc\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4f86\u8aaa\uff0c\u8981\u7406\u89e3\u7269\u7406\u4e16\u754c\uff0c\u4f8b\u5982\u56e0\u679c\u95dc\u4fc2\uff0c\u7b2c\u4e00\u6b65\u662f\u6355\u6349\u8996\u89ba\u4e16\u754c\u7684\u6642\u9593\u52d5\u614b\uff0c\u4f8b\u5982\u7269\u9ad4\u7684\u7269\u7406\u72c0\u614b\u5982\u4f55\u96a8\u6642\u9593\u6f14\u8b8a\uff08\u4f8b\u5982\uff0c\u4e00\u500b\u5b8c\u6574\u7684\u860b\u679c\u8b8a\u6210\u4e00\u500b\u5207\u7247\u7684\u860b\u679c\uff09\u3002\u6211\u5011\u7684\u8ad6\u6587\u65e8\u5728\u63a2\u8a0e\u5728\u7db2\u8def\u898f\u6a21\u6578\u64da\u4e0a\u9810\u5148\u8a13\u7df4\u7684 VLM \u662f\u5426\u5b78\u6703\u7de8\u78bc\u7269\u4ef6\u72c0\u614b\uff0c\u800c\u9019\u4e9b\u72c0\u614b\u53ef\u4ee5\u7528\u96f6\u6b21\u5b78\u7fd2\u6587\u5b57\u63d0\u793a\u63d0\u53d6\u3002\u6211\u5011\u7b56\u5283\u4e86\u4e00\u500b\u7269\u4ef6\u72c0\u614b\u8fa8\u8b58\u8cc7\u6599\u96c6 ChangeIt-Frames\uff0c\u4e26\u8a55\u4f30\u4e5d\u500b\u958b\u653e\u539f\u59cb\u78bc\u7684 VLM\uff0c\u5305\u62ec\u4f7f\u7528\u5c0d\u6bd4\u548c\u751f\u6210\u76ee\u6a19\u8a13\u7df4\u7684\u6a21\u578b\u3002\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u5118\u7ba1\u9019\u4e9b\u6700\u5148\u9032\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u53ef\u9760\u5730\u57f7\u884c\u7269\u4ef6\u8fa8\u8b58\uff0c\u4f46\u5b83\u5011\u59cb\u7d42\u7121\u6cd5\u6e96\u78ba\u5340\u5206\u7269\u4ef6\u7684\u7269\u7406\u72c0\u614b\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u627e\u51fa\u4e09\u500b VLM \u6539\u9032\u9818\u57df\uff0c\u4ee5\u66f4\u597d\u5730\u7de8\u78bc\u7269\u4ef6\u72c0\u614b\uff0c\u5373\u7269\u4ef6\u5b9a\u4f4d\u7684\u54c1\u8cea\u3001\u5c07\u6982\u5ff5\u8207\u7269\u4ef6\u7d50\u5408\u7684\u67b6\u69cb\uff0c\u4ee5\u53ca\u5728\u7269\u4ef6\u72c0\u614b\u4e0a\u5b78\u7fd2\u5224\u5225\u5f0f\u8996\u89ba\u548c\u8a9e\u8a00\u7de8\u78bc\u5668\u7684\u76ee\u6a19\u3002\u8cc7\u6599\u548c\u7a0b\u5f0f\u78bc\u5df2\u767c\u5e03\u3002", "author": "Kaleb Newman et.al.", "authors": "Kaleb Newman, Shijie Wang, Yuan Zang, David Heffren, Chen Sun", "id": "2409.10488v1", "paper_url": "http://arxiv.org/abs/2409.10488v1", "repo": "null"}}