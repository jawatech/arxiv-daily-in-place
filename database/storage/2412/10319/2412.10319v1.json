{"2412.10319": {"publish_time": "2024-12-13", "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods", "paper_summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.", "paper_summary_zh": "\u9577\u8a9e\u5883 LLM \u5df2\u555f\u7528\u8a31\u591a\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u8207\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u6548\u7387\u76f8\u95dc\u7684\u91cd\u5927\u6311\u6230\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u5df2\u91dd\u5c0d\u9577\u8a9e\u5883\u63a8\u8ad6\u958b\u767c\u6700\u4f73\u5316\uff0c\u96c6\u4e2d\u5728 KV \u5feb\u53d6\u4e0a\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u6e96\u6e2c\u8a66\u901a\u5e38\u5728\u55ae\u4e00\u8981\u6c42\u4e2d\u9032\u884c\u8a55\u4f30\uff0c\u5ffd\u7565\u4e86 KV \u5feb\u53d6\u5728\u5be6\u969b\u4f7f\u7528\u4e2d\u7684\u5b8c\u6574\u751f\u547d\u9031\u671f\u3002\u9019\u7a2e\u758f\u5ffd\u7279\u5225\u95dc\u9375\uff0c\u56e0\u70ba KV \u5feb\u53d6\u91cd\u8907\u4f7f\u7528\u5df2\u5728 LLM \u63a8\u8ad6\u67b6\u69cb\u4e2d\u5ee3\u6cdb\u63a1\u7528\uff0c\u4f8b\u5982 vLLM \u548c SGLang\uff0c\u4ee5\u53ca LLM \u4f9b\u61c9\u5546\uff0c\u5305\u62ec OpenAI\u3001Microsoft\u3001Google \u548c Anthropic\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 SCBench\uff08SharedContextBench\uff09\uff0c\u9019\u662f\u4e00\u500b\u5168\u9762\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u7528 KV \u5feb\u53d6\u70ba\u4e2d\u5fc3\u7684\u89c0\u9ede\u8a55\u4f30\u9577\u8a9e\u5883\u65b9\u6cd5\uff1a1) KV \u5feb\u53d6\u7522\u751f\uff0c2) KV \u5feb\u53d6\u58d3\u7e2e\uff0c3) KV \u5feb\u53d6\u64f7\u53d6\uff0c4) KV \u5feb\u53d6\u8f09\u5165\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cSCBench \u4f7f\u7528\u5177\u6709\u5171\u7528\u8a9e\u5883\u7684\u6e2c\u8a66\u7bc4\u4f8b\uff0c\u7bc4\u570d\u6db5\u84cb\u5169\u7a2e\u5171\u7528\u8a9e\u5883\u6a21\u5f0f\u7684 12 \u500b\u4efb\u52d9\uff0c\u6db5\u84cb\u9577\u8a9e\u5883\u529f\u80fd\u7684\u56db\u7a2e\u985e\u5225\uff1a\u5b57\u4e32\u64f7\u53d6\u3001\u8a9e\u610f\u64f7\u53d6\u3001\u5168\u57df\u8cc7\u8a0a\u548c\u591a\u4efb\u52d9\u3002\u900f\u904e\u5b83\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u5c0d\u516b\u7a2e\u985e\u5225\u9577\u8a9e\u5883\u89e3\u6c7a\u65b9\u6848\u7684\u5ee3\u6cdb KV \u5feb\u53d6\u70ba\u4e2d\u5fc3\u5206\u6790\uff0c\u5305\u62ec\u9598\u63a7\u7dda\u6027 RNN\u3001Mamba-Attention \u6df7\u5408\u9ad4\uff0c\u4ee5\u53ca\u7a00\u758f\u6ce8\u610f\u529b\u3001KV \u5feb\u53d6\u6368\u68c4\u3001\u91cf\u5316\u3001\u64f7\u53d6\u3001\u8f09\u5165\u548c\u63d0\u793a\u58d3\u7e2e\u7b49\u6709\u6548\u65b9\u6cd5\u3002\u8a55\u4f30\u662f\u5728 8 \u500b\u9577\u8a9e\u5883 LLM \u4e0a\u9032\u884c\u7684\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0csub-O(n) \u8a18\u61b6\u9ad4\u65b9\u6cd5\u5728\u591a\u8f2a\u60c5\u6cc1\u4e0b\u6703\u53d7\u5230\u5f71\u97ff\uff0c\u800c\u5177\u6709 O(n) \u8a18\u61b6\u9ad4\u548c sub-O(n^2) \u9810\u5148\u586b\u5165\u904b\u7b97\u7684\u7a00\u758f\u7de8\u78bc\u5247\u8868\u73fe\u5f97\u5f88\u597d\u3002\u52d5\u614b\u7a00\u758f\u6027\u7522\u751f\u6bd4\u975c\u614b\u6a21\u5f0f\u66f4\u5177\u8868\u73fe\u529b\u7684 KV \u5feb\u53d6\uff0c\u800c\u6df7\u5408\u67b6\u69cb\u4e2d\u7684\u5c64\u7d1a\u7a00\u758f\u6027\u5247\u4ee5\u5f37\u52c1\u7684\u6548\u80fd\u964d\u4f4e\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728\u9577\u751f\u6210\u60c5\u6cc1\u4e2d\u767c\u73fe\u4e86\u6ce8\u610f\u529b\u5206\u4f48\u8f49\u79fb\u554f\u984c\u3002https://aka.ms/SCBench\u3002", "author": "Yucheng Li et.al.", "authors": "Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu", "id": "2412.10319v1", "paper_url": "http://arxiv.org/abs/2412.10319v1", "repo": "null"}}