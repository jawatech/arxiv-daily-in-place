
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-18**|**Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts**|Haoxiang Wang et.al.|[2406.12845v1](http://arxiv.org/abs/2406.12845v1)|[link](https://github.com/RLHFlow/RLHF-Reward-Modeling)|
|**2024-06-18**|**Synergizing Foundation Models and Federated Learning: A Survey**|Shenghui Li et.al.|[2406.12844v1](http://arxiv.org/abs/2406.12844v1)|null|
|**2024-06-18**|**Demystifying Higher-Order Graph Neural Networks**|Maciej Besta et.al.|[2406.12841v1](http://arxiv.org/abs/2406.12841v1)|null|
|**2024-06-18**|**LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation**|Seyedarmin Azizi et.al.|[2406.12832v1](http://arxiv.org/abs/2406.12832v1)|[link](https://github.com/arminazizi98/lamda)|
|**2024-06-18**|**VIA: A Spatiotemporal Video Adaptation Framework for Global and Local Video Editing**|Jing Gu et.al.|[2406.12831v1](http://arxiv.org/abs/2406.12831v1)|null|
|**2024-06-18**|**What Are the Odds? Language Models Are Capable of Probabilistic Reasoning**|Akshay Paruchuri et.al.|[2406.12830v1](http://arxiv.org/abs/2406.12830v1)|null|
|**2024-06-18**|**From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries**|Hitesh Wadhwa et.al.|[2406.12824v1](http://arxiv.org/abs/2406.12824v1)|null|
|**2024-06-18**|**Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?**|Pinzhen Chen et.al.|[2406.12822v1](http://arxiv.org/abs/2406.12822v1)|null|
|**2024-06-18**|**Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation**|Nikolas Koutsoubis et.al.|[2406.12815v1](http://arxiv.org/abs/2406.12815v1)|[link](https://github.com/niko-k98/awesome-list-federated-learning-review)|
|**2024-06-18**|**Adversarial Attacks on Multimodal Agents**|Chen Henry Wu et.al.|[2406.12814v1](http://arxiv.org/abs/2406.12814v1)|[link](https://github.com/chenwu98/agent-attack)|
|**2024-06-18**|**Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?**|Zhe Yang et.al.|[2406.12809v1](http://arxiv.org/abs/2406.12809v1)|null|
|**2024-06-18**|**Graph Neural Networks in Histopathology: Emerging Trends and Future Directions**|Siemen Brussee et.al.|[2406.12808v1](http://arxiv.org/abs/2406.12808v1)|null|
|**2024-06-18**|**Probabilistic Temporal Prediction of Continuous Disease Trajectories and Treatment Effects Using Neural SDEs**|Joshua Durso-Finley et.al.|[2406.12807v1](http://arxiv.org/abs/2406.12807v1)|null|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806v1](http://arxiv.org/abs/2406.12806v1)|null|
|**2024-06-18**|**ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools**|Team GLM et.al.|[2406.12793v1](http://arxiv.org/abs/2406.12793v1)|null|
|**2024-06-18**|**UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions**|Xunzhi Wang et.al.|[2406.12784v1](http://arxiv.org/abs/2406.12784v1)|null|
|**2024-06-18**|**Composited-Nested-Learning with Data Augmentation for Nested Named Entity Recognition**|Xingming Liao et.al.|[2406.12779v1](http://arxiv.org/abs/2406.12779v1)|null|
|**2024-06-18**|**Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries**|Eden Biran et.al.|[2406.12775v1](http://arxiv.org/abs/2406.12775v1)|null|
|**2024-06-18**|**Formatics & dairy industry coalition: AI trends and present challenges**|Silvia García-Méndez et.al.|[2406.12770v1](http://arxiv.org/abs/2406.12770v1)|null|
|**2024-06-18**|**Latent Intuitive Physics: Learning to Transfer Hidden Physics from A 3D Video**|Xiangming Zhu et.al.|[2406.12769v1](http://arxiv.org/abs/2406.12769v1)|null|
|**2024-06-18**|**Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding Dataset from Ruo Zhi Ba**|Ruiqi He et.al.|[2406.12754v1](http://arxiv.org/abs/2406.12754v1)|null|
|**2024-06-18**|**OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI**|Zhen Huang et.al.|[2406.12753v1](http://arxiv.org/abs/2406.12753v1)|[link](https://github.com/gair-nlp/olympicarena)|
|**2024-06-18**|**TSI-Bench: Benchmarking Time Series Imputation**|Wenjie Du et.al.|[2406.12747v1](http://arxiv.org/abs/2406.12747v1)|[link](https://github.com/WenjieDu/PyPOTS)|
|**2024-06-18**|**Rationale-based Ensemble of Multiple QA Strategies for Zero-shot Knowledge-based VQA**|Miaoyu Li et.al.|[2406.12746v1](http://arxiv.org/abs/2406.12746v1)|null|
|**2024-06-18**|**Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning**|Bingchen Zhao et.al.|[2406.12742v1](http://arxiv.org/abs/2406.12742v1)|[link](https://github.com/dtennant/mirb_eval)|
|**2024-06-18**|**Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages**|Fabian David Schmidt et.al.|[2406.12739v1](http://arxiv.org/abs/2406.12739v1)|null|
|**2024-06-18**|**Large Language Model as a Universal Clinical Multi-task Decoder**|Yujiang Wu et.al.|[2406.12738v1](http://arxiv.org/abs/2406.12738v1)|null|
|**2024-06-18**|**Automatic generation of insights from workers' actions in industrial workflows with explainable Machine Learning**|Francisco de Arriba-Pérez et.al.|[2406.12732v1](http://arxiv.org/abs/2406.12732v1)|null|
|**2024-06-18**|**Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction**|Atharva Naik et.al.|[2406.12725v1](http://arxiv.org/abs/2406.12725v1)|null|
|**2024-06-18**|**On the Robustness of Language Models for Tabular Question Answering**|Kushal Raj Bhandari et.al.|[2406.12719v1](http://arxiv.org/abs/2406.12719v1)|null|
|**2024-06-18**|**AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention**|Wenbin An et.al.|[2406.12718v1](http://arxiv.org/abs/2406.12718v1)|null|
|**2024-06-18**|**Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning: Lessons Learned**|Du Yin et.al.|[2406.12709v1](http://arxiv.org/abs/2406.12709v1)|null|
|**2024-06-18**|**AgentReview: Exploring Peer Review Dynamics with LLM Agents**|Yiqiao Jin et.al.|[2406.12708v1](http://arxiv.org/abs/2406.12708v1)|null|
|**2024-06-18**|**Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction**|Haoqiu Yan et.al.|[2406.12707v1](http://arxiv.org/abs/2406.12707v1)|[link](https://github.com/haoqiu-yan/perceptiveagent)|
|**2024-06-18**|**Jailbreak Paradox: The Achilles' Heel of LLMs**|Abhinav Rao et.al.|[2406.12702v1](http://arxiv.org/abs/2406.12702v1)|null|
|**2024-06-18**|**Online-Adaptive Anomaly Detection for Defect Identification in Aircraft Assembly**|Siddhant Shete et.al.|[2406.12698v1](http://arxiv.org/abs/2406.12698v1)|null|
|**2024-06-18**|**XXLTraffic: Expanding and Extremely Long Traffic Dataset for Ultra-Dynamic Forecasting Challenges**|Du Yin et.al.|[2406.12693v1](http://arxiv.org/abs/2406.12693v1)|null|
|**2024-06-18**|**MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL**|Arian Askari et.al.|[2406.12692v1](http://arxiv.org/abs/2406.12692v1)|null|
|**2024-06-18**|**Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia**|Ankit Aich et.al.|[2406.12687v1](http://arxiv.org/abs/2406.12687v1)|null|
|**2024-06-18**|**Measuring Psychological Depth in Language Models**|Fabrice Harel-Canada et.al.|[2406.12680v1](http://arxiv.org/abs/2406.12680v1)|null|
|**2024-06-18**|**Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping**|Ankit Aich et.al.|[2406.12679v1](http://arxiv.org/abs/2406.12679v1)|null|
|**2024-06-18**|**Estimating Knowledge in Large Language Models Without Generating a Single Token**|Daniela Gottesman et.al.|[2406.12673v1](http://arxiv.org/abs/2406.12673v1)|null|
|**2024-06-18**|**Sparsifying dimensionality reduction of PDE solution data with Bregman learning**|Tjeerd Jan Heeringa et.al.|[2406.12672v1](http://arxiv.org/abs/2406.12672v1)|null|
|**2024-06-18**|**Stealth edits for provably fixing or attacking large language models**|Oliver J. Sutton et.al.|[2406.12670v1](http://arxiv.org/abs/2406.12670v1)|[link](https://github.com/qinghua-zhou/stealth-edits)|
|**2024-06-18**|**CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis**|Saranya Venkatraman et.al.|[2406.12665v1](http://arxiv.org/abs/2406.12665v1)|[link](https://github.com/saranya-venkatraman/multi_llm_story_writing)|
|**2024-06-18**|**Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?**|Mingqian Feng et.al.|[2406.12663v1](http://arxiv.org/abs/2406.12663v1)|null|
|**2024-06-18**|**Investigating the Role of Explainability and AI Literacy in User Compliance**|Niklas Kühl et.al.|[2406.12660v1](http://arxiv.org/abs/2406.12660v1)|null|
|**2024-06-18**|**Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review**|Debalina Ghosh Paul et.al.|[2406.12655v1](http://arxiv.org/abs/2406.12655v1)|null|
|**2024-06-18**|**Transforming Surgical Interventions with Embodied Intelligence for Ultrasound Robotics**|Huan Xu et.al.|[2406.12651v1](http://arxiv.org/abs/2406.12651v1)|null|
|**2024-06-18**|**Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models**|Hengyi Wang et.al.|[2406.12649v1](http://arxiv.org/abs/2406.12649v1)|null|
|**2024-06-18**|**An Empirical Study on the Fairness of Foundation Models for Multi-Organ Image Segmentation**|Qin Li et.al.|[2406.12646v1](http://arxiv.org/abs/2406.12646v1)|null|
|**2024-06-18**|**Evaluating Transparency of Machine Generated Fact Checking Explanations**|Rui Xing et.al.|[2406.12645v1](http://arxiv.org/abs/2406.12645v1)|[link](https://github.com/ruixing76/transparent-fcexp)|
|**2024-06-18**|**Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models**|Devichand Budagam et.al.|[2406.12644v1](http://arxiv.org/abs/2406.12644v1)|[link](https://github.com/devichand579/HPT)|
|**2024-06-18**|**DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?**|Zhouhong Gu et.al.|[2406.12641v1](http://arxiv.org/abs/2406.12641v1)|null|
|**2024-06-18**|**Ask-before-Plan: Proactive Language Agents for Real-World Planning**|Xuan Zhang et.al.|[2406.12639v1](http://arxiv.org/abs/2406.12639v1)|[link](https://github.com/magicgh/ask-before-plan)|
|**2024-06-18**|**ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation**|Debalina Ghosh Paul et.al.|[2406.12635v1](http://arxiv.org/abs/2406.12635v1)|null|
|**2024-06-18**|**News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation**|Andreea Iana et.al.|[2406.12634v1](http://arxiv.org/abs/2406.12634v1)|[link](https://github.com/andreeaiana/nase)|
|**2024-06-18**|**SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation**|Yixia Li et.al.|[2406.12629v1](http://arxiv.org/abs/2406.12629v1)|[link](https://github.com/X1AOX1A/SeTAR)|
|**2024-06-18**|**Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges**|Aman Singh Thakur et.al.|[2406.12624v1](http://arxiv.org/abs/2406.12624v1)|null|
|**2024-06-18**|**What makes two models think alike?**|Jeanne Salle et.al.|[2406.12620v1](http://arxiv.org/abs/2406.12620v1)|null|
|**2024-06-18**|**EUvsDisinfo: a Dataset for Multilingual Detection of Pro-Kremlin Disinformation in News Articles**|João A. Leite et.al.|[2406.12614v1](http://arxiv.org/abs/2406.12614v1)|null|
|**2024-06-18**|**Rapid Language Adaptation for Multilingual E2E Speech Recognition Using Encoder Prompting**|Yosuke Kashiwagi et.al.|[2406.12611v1](http://arxiv.org/abs/2406.12611v1)|null|
|**2024-06-18**|**Bridging Local Details and Global Context in Text-Attributed Graphs**|Yaoke Wang et.al.|[2406.12608v1](http://arxiv.org/abs/2406.12608v1)|null|
|**2024-06-18**|**Low-Redundant Optimization for Large Language Model Alignment**|Zhipeng Chen et.al.|[2406.12606v1](http://arxiv.org/abs/2406.12606v1)|[link](https://github.com/rucaibox/allo)|
|**2024-06-18**|**PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval**|Tuan-Luc Huynh et.al.|[2406.12593v1](http://arxiv.org/abs/2406.12593v1)|null|
|**2024-06-18**|**UIFV: Data Reconstruction Attack in Vertical Federated Learning**|Jirui Yang et.al.|[2406.12588v1](http://arxiv.org/abs/2406.12588v1)|null|
|**2024-06-18**|**Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling**|Yao-Ching Yu et.al.|[2406.12585v1](http://arxiv.org/abs/2406.12585v1)|[link](https://github.com/yaoching0/gac)|
|**2024-06-18**|**Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models**|Eldar Kurtic et.al.|[2406.12572v1](http://arxiv.org/abs/2406.12572v1)|[link](https://github.com/ist-daslab/mathador-lm)|
|**2024-06-18**|**Applying Ensemble Methods to Model-Agnostic Machine-Generated Text Detection**|Ivan Ong et.al.|[2406.12570v1](http://arxiv.org/abs/2406.12570v1)|null|
|**2024-06-18**|**RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation**|Shuting Wang et.al.|[2406.12566v1](http://arxiv.org/abs/2406.12566v1)|null|
|**2024-06-18**|**Low-Resource Machine Translation through the Lens of Personalized Federated Learning**|Viktor Moskvoretskii et.al.|[2406.12564v1](http://arxiv.org/abs/2406.12564v1)|[link](https://github.com/vityavitalich/meritfed)|
|**2024-06-18**|**Bayesian Data Selection**|Julian Rodemann et.al.|[2406.12560v1](http://arxiv.org/abs/2406.12560v1)|null|
|**2024-06-18**|**Offline Imitation Learning with Model-based Reverse Augmentation**|Jie-Jing Shao et.al.|[2406.12550v1](http://arxiv.org/abs/2406.12550v1)|null|
|**2024-06-18**|**MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts**|Dominik Macko et.al.|[2406.12549v1](http://arxiv.org/abs/2406.12549v1)|null|
|**2024-06-18**|**P-Tailor: Customizing Personality Traits for Language Models via Mixture of Specialized LoRA Experts**|Yuhao Dan et.al.|[2406.12548v1](http://arxiv.org/abs/2406.12548v1)|null|
|**2024-06-18**|**Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models**|Philipp Mondorf et.al.|[2406.12546v1](http://arxiv.org/abs/2406.12546v1)|null|
|**2024-06-18**|**Variational Distillation of Diffusion Policies into Mixture of Experts**|Hongyi Zhou et.al.|[2406.12538v1](http://arxiv.org/abs/2406.12538v1)|null|
|**2024-06-18**|**LLM4MSR: An LLM-Enhanced Paradigm for Multi-Scenario Recommendation**|Yuhao Wang et.al.|[2406.12529v1](http://arxiv.org/abs/2406.12529v1)|null|
|**2024-06-18**|**FuseGen: PLM Fusion for Data-generation based Zero-shot Learning**|Tianyuan Zou et.al.|[2406.12527v1](http://arxiv.org/abs/2406.12527v1)|null|
|**2024-06-18**|**Code-Optimise: Self-Generated Preference Data for Correctness and Efficiency**|Leonidas Gee et.al.|[2406.12502v1](http://arxiv.org/abs/2406.12502v1)|null|
|**2024-06-18**|**Autonomous navigation of catheters and guidewires in mechanical thrombectomy using inverse reinforcement learning**|Harry Robertshaw et.al.|[2406.12499v1](http://arxiv.org/abs/2406.12499v1)|null|
|**2024-06-18**|**LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document Summarization**|Masafumi Enomoto et.al.|[2406.12494v1](http://arxiv.org/abs/2406.12494v1)|null|
|**2024-06-18**|**The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions**|Stefan Sylvius Wagner et.al.|[2406.12480v1](http://arxiv.org/abs/2406.12480v1)|null|
|**2024-06-18**|**RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding**|Linrui Xu et.al.|[2406.12479v1](http://arxiv.org/abs/2406.12479v1)|[link](https://github.com/geox-lab/rs-gpt4v)|
|**2024-06-18**|**Exploring Intra and Inter-language Consistency in Embeddings with ICA**|Rongzhi Li et.al.|[2406.12474v1](http://arxiv.org/abs/2406.12474v1)|null|
|**2024-06-18**|**Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation**|Branislav Pecher et.al.|[2406.12471v1](http://arxiv.org/abs/2406.12471v1)|null|
|**2024-06-18**|**Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities**|Baolong Bi et.al.|[2406.12468v1](http://arxiv.org/abs/2406.12468v1)|null|
|**2024-06-18**|**RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes**|Xiaoshan Yu et.al.|[2406.12465v1](http://arxiv.org/abs/2406.12465v1)|null|
|**2024-06-18**|**A Neural Column Generation Approach to the Vehicle Routing Problem with Two-Dimensional Loading and Last-In-First-Out Constraints**|Yifan Xia et.al.|[2406.12454v1](http://arxiv.org/abs/2406.12454v1)|[link](https://github.com/xyfffff/NCG-for-2L-CVRP)|
|**2024-06-18**|**Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine**|Rui Yang et.al.|[2406.12449v1](http://arxiv.org/abs/2406.12449v1)|null|
|**2024-06-18**|**Abstraction-of-Thought Makes Language Models Better Reasoners**|Ruixin Hong et.al.|[2406.12442v1](http://arxiv.org/abs/2406.12442v1)|null|
|**2024-06-18**|**Federated Learning with Limited Node Labels**|Bisheng Tang et.al.|[2406.12435v1](http://arxiv.org/abs/2406.12435v1)|null|
|**2024-06-18**|**PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers**|Myeonghwa Lee et.al.|[2406.12430v1](http://arxiv.org/abs/2406.12430v1)|[link](https://github.com/myeon9h/planrag)|
|**2024-06-18**|**PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems**|Kentaro Mitsui et.al.|[2406.12428v1](http://arxiv.org/abs/2406.12428v1)|null|
|**2024-06-18**|**Open-Source Web Service with Morphological Dictionary-Supplemented Deep Learning for Morphosyntactic Analysis of Czech**|Milan Straka et.al.|[2406.12422v1](http://arxiv.org/abs/2406.12422v1)|[link](https://github.com/ufal/udpipe)|
|**2024-06-18**|**MMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling**|Philipp Seeberger et.al.|[2406.12420v1](http://arxiv.org/abs/2406.12420v1)|null|
|**2024-06-18**|**Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models**|Hongbang Yuan et.al.|[2406.12416v1](http://arxiv.org/abs/2406.12416v1)|null|
|**2024-06-18**|**PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models**|Tao Fan et.al.|[2406.12403v1](http://arxiv.org/abs/2406.12403v1)|null|
|**2024-06-18**|**Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling**|Irfan Robbani et.al.|[2406.12402v1](http://arxiv.org/abs/2406.12402v1)|null|
|**2024-06-18**|**A Cutting-Edge Deep Learning Method For Enhancing IoT Security**|Nadia Ansar et.al.|[2406.12400v1](http://arxiv.org/abs/2406.12400v1)|null|

#### Abstracts
##### **Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts**
2406.12845v1 by Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang

Reinforcement learning from human feedback (RLHF) has emerged as the primary
method for aligning large language models (LLMs) with human preferences. The
RLHF process typically starts by training a reward model (RM) using human
preference data. Conventional RMs are trained on pairwise responses to the same
user request, with relative ratings indicating which response humans prefer.
The trained RM serves as a proxy for human preferences. However, due to the
black-box nature of RMs, their outputs lack interpretability, as humans cannot
intuitively understand why an RM thinks a response is good or not. As RMs act
as human preference proxies, we believe they should be human-interpretable to
ensure that their internal decision processes are consistent with human
preferences and to prevent reward hacking in LLM alignment. To build RMs with
interpretable preferences, we propose a two-stage approach: i) train an
Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional
absolute-rating data, each dimension corresponding to a human-interpretable
objective (e.g., honesty, verbosity, safety); ii) employ a Mixture-of-Experts
(MoE) strategy with a gating network that automatically selects the most
suitable reward objectives based on the context. We efficiently trained an
ArmoRM with Llama-3 8B and a gating network consisting of a shallow MLP on top
of the ArmoRM. Our trained model, ArmoRM-Llama3-8B, obtains state-of-the-art
performance on RewardBench, a benchmark evaluating RMs for language modeling.
Notably, the performance of our model surpasses the LLM-as-a-judge method with
GPT-4 judges by a margin, and approaches the performance of the much larger
Nemotron-4 340B reward model.

摘要：人類回饋強化學習 (RLHF) 已成為將大型語言模型 (LLM) 與人類偏好保持一致的主要方法。RLHF 過程通常從使用人類偏好數據訓練獎勵模型 (RM) 開始。傳統的 RM 是針對同一個使用者要求的成對回應進行訓練，相對評分表示人類偏好的回應。訓練後的 RM 作為人類偏好的代理。然而，由於 RM 的黑盒性質，其輸出缺乏可解釋性，因為人類無法直觀地理解 RM 認為一個回應是好還是不好的原因。由於 RM 作為人類偏好代理，我們相信它們應該是人類可解釋的，以確保其內部決策過程與人類偏好一致，並防止 LLM 對齊中的獎勵破解。為了建立具有可解釋偏好的 RM，我們提出了一個兩階段方法：i) 使用多維絕對評分數據訓練一個絕對評分多目標獎勵模型 (ArmoRM)，每個維度對應一個人類可解釋的目標（例如，誠實、冗長、安全）；ii) 採用混合專家 (MoE) 策略，使用閘控網路，根據上下文自動選擇最合適的獎勵目標。我們使用 Llama-3 8B 有效訓練了一個 ArmoRM，並在 ArmoRM 上建立了一個由淺層 MLP 組成的閘控網路。我們訓練好的模型 ArmoRM-Llama3-8B 在 RewardBench 上獲得了最先進的效能，RewardBench 是評估語言建模的 RM 的基準。值得注意的是，我們的模型的效能超越了使用 GPT-4 評審員的 LLM 作為評審員的方法，並接近更大規模的 Nemotron-4 340B 獎勵模型的效能。

##### **Synergizing Foundation Models and Federated Learning: A Survey**
2406.12844v1 by Shenghui Li, Fanghua Ye, Meng Fang, Jiaxu Zhao, Yun-Hin Chan, Edith C. -H. Ngai, Thiemo Voigt

The recent development of Foundation Models (FMs), represented by large
language models, vision transformers, and multimodal models, has been making a
significant impact on both academia and industry. Compared with small-scale
models, FMs have a much stronger demand for high-volume data during the
pre-training phase. Although general FMs can be pre-trained on data collected
from open sources such as the Internet, domain-specific FMs need proprietary
data, posing a practical challenge regarding the amount of data available due
to privacy concerns. Federated Learning (FL) is a collaborative learning
paradigm that breaks the barrier of data availability from different
participants. Therefore, it provides a promising solution to customize and
adapt FMs to a wide range of domain-specific tasks using distributed datasets
whilst preserving privacy. This survey paper discusses the potentials and
challenges of synergizing FL and FMs and summarizes core techniques, future
directions, and applications. A periodically updated paper collection on FM-FL
is available at https://github.com/lishenghui/awesome-fm-fl.

摘要：基礎模型 (FM) 的最新發展，以大型語言模型、視覺Transformer和多模態模型為代表，對學術界和產業界產生了重大影響。與小規模模型相比，FM 在預訓練階段對大量資料的需求強得多。儘管一般的 FM 可以使用從網際網路等開放來源收集的資料進行預訓練，但特定領域的 FM 需要專有資料，由於隱私問題，這對可用的資料量構成了實際挑戰。聯邦學習 (FL) 是一種協作學習範例，打破了來自不同參與者資料可用性的障礙。因此，它提供了一個有前途的解決方案，可以使用分散式資料集自訂和調整 FM 以適應廣泛的特定領域任務，同時保護隱私。這篇調查論文討論了協同 FL 和 FM 的潛力和挑戰，並總結了核心技術、未來方向和應用。可以在 https://github.com/lishenghui/awesome-fm-fl 上找到定期更新的 FM-FL 論文集。

##### **Demystifying Higher-Order Graph Neural Networks**
2406.12841v1 by Maciej Besta, Florian Scheidl, Lukas Gianinazzi, Shachar Klaiman, Jürgen Müller, Torsten Hoefler

Higher-order graph neural networks (HOGNNs) are an important class of GNN
models that harness polyadic relations between vertices beyond plain edges.
They have been used to eliminate issues such as over-smoothing or
over-squashing, to significantly enhance the accuracy of GNN predictions, to
improve the expressiveness of GNN architectures, and for numerous other goals.
A plethora of HOGNN models have been introduced, and they come with diverse
neural architectures, and even with different notions of what the
"higher-order" means. This richness makes it very challenging to appropriately
analyze and compare HOGNN models, and to decide in what scenario to use
specific ones. To alleviate this, we first design an in-depth taxonomy and a
blueprint for HOGNNs. This facilitates designing models that maximize
performance. Then, we use our taxonomy to analyze and compare the available
HOGNN models. The outcomes of our analysis are synthesized in a set of insights
that help to select the most beneficial GNN model in a given scenario, and a
comprehensive list of challenges and opportunities for further research into
more powerful HOGNNs.

摘要：高階圖神經網路 (HOGNN) 是 GNN 模型的重要類別，它利用頂點之間的多元關係，而不仅仅是純粹的邊緣。
它們已被用於消除過度平滑或過度壓縮等問題，以顯著提高 GNN 預測的準確性，以提高 GNN 架構的表達能力，以及實現許多其他目標。
已經引入了大量的 HOGNN 模型，它們具有多樣的神經架構，甚至對於「高階」的含義也有不同的概念。這種豐富性使得適當地分析和比較 HOGNN 模型，並決定在什麼情況下使用特定的模型，變得非常具有挑戰性。為了緩解這一點，我們首先設計了一個深入的分類法和 HOGNN 的藍圖。這有助於設計最大化效能的模型。然後，我們使用我們的分類法來分析和比較可用的 HOGNN 模型。我們的分析結果綜合在一組見解中，這些見解有助於在給定的情況下選擇最有益的 GNN 模型，並提供了一個全面的挑戰和機會清單，以進一步研究更強大的 HOGNN。

##### **LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation**
2406.12832v1 by Seyedarmin Azizi, Souvik Kundu, Massoud Pedram

Low-rank adaptation (LoRA) has become the default approach to fine-tune large
language models (LLMs) due to its significant reduction in trainable
parameters. However, trainable parameter demand for LoRA increases with
increasing model embedding dimensions, leading to high compute costs.
Additionally, its backward updates require storing high-dimensional
intermediate activations and optimizer states, demanding high peak GPU memory.
In this paper, we introduce large model fine-tuning via spectrally decomposed
low-dimensional adaptation (LaMDA), a novel approach to fine-tuning large
language models, which leverages low-dimensional adaptation to achieve
significant reductions in trainable parameters and peak GPU memory footprint.
LaMDA freezes a first projection matrix (PMA) in the adaptation path while
introducing a low-dimensional trainable square matrix, resulting in substantial
reductions in trainable parameters and peak GPU memory usage. LaMDA gradually
freezes a second projection matrix (PMB) during the early fine-tuning stages,
reducing the compute cost associated with weight updates to enhance parameter
efficiency further. We also present an enhancement, LaMDA++, incorporating a
``lite-weight" adaptive rank allocation for the LoRA path via normalized
spectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++
across various tasks, including natural language understanding with the GLUE
benchmark, text summarization, natural language generation, and complex
reasoning on different LLMs. Results show that LaMDA matches or surpasses the
performance of existing alternatives while requiring up to 17.7x fewer
parameter updates and up to 1.32x lower peak GPU memory usage during
fine-tuning. Code will be publicly available.

摘要：低階改編 (LoRA) 已成為微調大型語言模型 (LLM) 的預設方法，因為它大幅減少了可訓練參數。然而，LoRA 對可訓練參數的需求會隨著模型嵌入維度增加而增加，導致高運算成本。此外，其反向更新需要儲存高維中間活化和最佳化器狀態，需要高峰值 GPU 記憶體。在本文中，我們介紹了透過光譜分解低維改編 (LaMDA) 進行大型模型微調，這是一種微調大型語言模型的新方法，它利用低維改編來大幅減少可訓練參數和峰值 GPU 記憶體使用量。LaMDA 凍結了改編路徑中的第一個投影矩陣 (PMA)，同時引入了一個低維可訓練方陣，從而大幅減少了可訓練參數和峰值 GPU 記憶體使用量。LaMDA 在微調的早期階段逐漸凍結了第二個投影矩陣 (PMB)，減少了與權重更新相關的運算成本，進一步提高了參數效率。我們還提出了一項增強功能 LaMDA++，它透過預訓練模型權重的規範化光譜分析，為 LoRA 路徑納入了「輕量級」自適應秩分配。我們在各種任務中評估了 LaMDA/LaMDA++，包括使用 GLUE 基準的自然語言理解、文字摘要、自然語言生成以及對不同 LLM 進行的複雜推理。結果顯示，LaMDA 在需要較少多達 17.7 倍的參數更新和在微調期間降低多達 1.32 倍的峰值 GPU 記憶體使用量的同時，匹配或超越了現有替代方案的效能。程式碼將公開提供。

##### **VIA: A Spatiotemporal Video Adaptation Framework for Global and Local Video Editing**
2406.12831v1 by Jing Gu, Yuwei Fang, Ivan Skorokhodov, Peter Wonka, Xinya Du, Sergey Tulyakov, Xin Eric Wang

Video editing stands as a cornerstone of digital media, from entertainment
and education to professional communication. However, previous methods often
overlook the necessity of comprehensively understanding both global and local
contexts, leading to inaccurate and inconsistency edits in the spatiotemporal
dimension, especially for long videos. In this paper, we introduce VIA, a
unified spatiotemporal VIdeo Adaptation framework for global and local video
editing, pushing the limits of consistently editing minute-long videos. First,
to ensure local consistency within individual frames, the foundation of VIA is
a novel test-time editing adaptation method, which adapts a pre-trained image
editing model for improving consistency between potential editing directions
and the text instruction, and adapts masked latent variables for precise local
control. Furthermore, to maintain global consistency over the video sequence,
we introduce spatiotemporal adaptation that adapts consistent attention
variables in key frames and strategically applies them across the whole
sequence to realize the editing effects. Extensive experiments demonstrate
that, compared to baseline methods, our VIA approach produces edits that are
more faithful to the source videos, more coherent in the spatiotemporal
context, and more precise in local control. More importantly, we show that VIA
can achieve consistent long video editing in minutes, unlocking the potentials
for advanced video editing tasks over long video sequences.

摘要：影片編輯是數位媒體的基石，從娛樂、教育到專業溝通皆是。然而，以往的方法常常忽略全面理解全球和在地脈絡的必要性，導致在時空維度上產生不準確且不一致的編輯，特別是對於長影片。在本文中，我們介紹 VIA，一個統一的時空影片改編架構，用於全球和在地影片編輯，突破連貫編輯長達數分鐘影片的限制。首先，為了確保個別畫格內的在地一致性，VIA 的基礎是一個新穎的測試時間編輯改編方法，它改編一個預先訓練好的影像編輯模型，用於改善潛在編輯方向與文字指令間的一致性，並改編遮罩潛在變數以進行精確的在地控制。此外，為了在影片序列上維持全球一致性，我們引入時空改編，它改編關鍵畫格中一致的注意變數，並策略性地將它們應用於整個序列，以實現編輯效果。廣泛的實驗證明，與基準方法相比，我們的 VIA 方法產生的編輯更忠於原始影片、在時空脈絡中更連貫、在在地控制上更精確。更重要的是，我們展示 VIA 能在數分鐘內達成一致的長影片編輯，為長影片序列的高階影片編輯任務解鎖潛力。

##### **What Are the Odds? Language Models Are Capable of Probabilistic Reasoning**
2406.12830v1 by Akshay Paruchuri, Jake Garrison, Shun Liao, John Hernandez, Jacob Sunshine, Tim Althoff, Xin Liu, Daniel McDuff

Language models (LM) are capable of remarkably complex linguistic tasks;
however, numerical reasoning is an area in which they frequently struggle. An
important but rarely evaluated form of reasoning is understanding probability
distributions. In this paper, we focus on evaluating the probabilistic
reasoning capabilities of LMs using idealized and real-world statistical
distributions. We perform a systematic evaluation of state-of-the-art LMs on
three tasks: estimating percentiles, drawing samples, and calculating
probabilities. We evaluate three ways to provide context to LMs 1) anchoring
examples from within a distribution or family of distributions, 2) real-world
context, 3) summary statistics on which to base a Normal approximation. Models
can make inferences about distributions, and can be further aided by the
incorporation of real-world context, example shots and simplified assumptions,
even if these assumptions are incorrect or misspecified. To conduct this work,
we developed a comprehensive benchmark distribution dataset with associated
question-answer pairs that we will release publicly.

摘要：語言模型 (LM) 能夠執行相當複雜的語言任務；
然而，數值推理是一個它們經常會遇到的難題。一個重要但很少被評估的推理形式是理解機率分佈。在本文中，我們專注於使用理想化和真實世界的統計分佈來評估 LM 的機率推理能力。我們對最先進的 LM 進行了一項系統評估，任務有三個：估計百分位數、繪製樣本，以及計算機率。我們評估了三種向 LM 提供背景資訊的方法：1) 錨定來自分佈或分佈族內的範例，2) 真實世界的背景資訊，3) 用於作為常態近似基礎的摘要統計資料。模型可以對分佈進行推論，並且可以進一步透過納入真實世界的背景資訊、範例抽樣和簡化的假設來協助，即使這些假設是不正確或錯誤的。為了進行這項工作，我們開發了一個全面的基準分佈資料集，其中包含相關的問題解答對，我們將公開發布。

##### **From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries**
2406.12824v1 by Hitesh Wadhwa, Rahul Seetharaman, Somyaa Aggarwal, Reshmi Ghosh, Samyadeep Basu, Soundararajan Srinivasan, Wenlong Zhao, Shreyas Chaudhari, Ehsan Aghazadeh

Retrieval Augmented Generation (RAG) enriches the ability of language models
to reason using external context to augment responses for a given user prompt.
This approach has risen in popularity due to practical applications in various
applications of language models in search, question/answering, and chat-bots.
However, the exact nature of how this approach works isn't clearly understood.
In this paper, we mechanistically examine the RAG pipeline to highlight that
language models take shortcut and have a strong bias towards utilizing only the
context information to answer the question, while relying minimally on their
parametric memory. We probe this mechanistic behavior in language models with:
(i) Causal Mediation Analysis to show that the parametric memory is minimally
utilized when answering a question and (ii) Attention Contributions and
Knockouts to show that the last token residual stream do not get enriched from
the subject token in the question, but gets enriched from other informative
tokens in the context. We find this pronounced shortcut behaviour true across
both LLaMa and Phi family of models.

摘要：檢索增強生成（RAG）豐富了語言模型的能力，可利用外部內容進行推理，以擴充給定使用者提示的回應。此方法因在搜尋、問答和聊天機器人等語言模型的各種應用中具有實際應用而廣受歡迎。然而，此方法的運作方式確切性質尚未清楚了解。在本文中，我們機械地檢查 RAG 管線，以強調語言模型採取捷徑，且強烈偏好僅利用內容資訊來回答問題，同時將對其參數化記憶體的依賴降至最低。我們使用下列方法探討語言模型中的此機械行為：(i) 因果中介分析，以顯示在回答問題時，參數化記憶體的使用率很低，以及 (ii) 注意力貢獻和擊倒，以顯示最後一個代幣殘差串流並未從問題中的主詞代幣中獲得豐富，而是從內容中的其他資訊性代幣中獲得豐富。我們發現這種明顯的捷徑行為在 LLaMa 和 Phi 系列模型中都是正確的。

##### **Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?**
2406.12822v1 by Pinzhen Chen, Simon Yu, Zhicheng Guo, Barry Haddow

Large language models, particularly multilingual ones, are designed, claimed,
and expected to cater to native speakers of varied languages. We hypothesise
that the current practices of fine-tuning and evaluating these models may
mismatch this intention owing to a heavy reliance on translation, which can
introduce translation artefacts and defects. It remains unknown whether the
nature of the instruction data has an impact on the model output; on the other
hand, it remains questionable whether translated test sets can capture such
nuances. Due to the often coupled practices of using translated data in both
stages, such imperfections could have been overlooked. This work investigates
these issues by using controlled native or translated data during instruction
tuning and evaluation stages and observing model results. Experiments on eight
base models and eight different benchmarks reveal that native or generation
benchmarks display a notable difference between native and translated
instruction data especially when model performance is high, whereas other types
of test sets cannot. Finally, we demonstrate that regularization is beneficial
to bridging this gap on structured but not generative tasks.

摘要：大型語言模型，尤其是多語言模型，被設計、聲稱和預期用於迎合各種語言的母語人士。我們假設，由於過度依賴翻譯，這些模型微調和評估的當前做法可能會與此意圖不符，這可能會引入翻譯人工製品和缺陷。指令數據的性質是否會對模型輸出產生影響仍然未知；另一方面，翻譯的測試集是否能捕捉到這些細微差別仍然值得懷疑。由於在兩個階段中經常使用翻譯數據的做法，這些缺陷可能被忽視了。這項工作通過在指令調整和評估階段使用受控的本機或翻譯數據並觀察模型結果來調查這些問題。對八個基本模型和八個不同基準的實驗表明，當模型性能很高時，本機或生成基準在本機和翻譯指令數據之間顯示出顯著差異，而其他類型的測試集則不能。最後，我們證明規範化有助於彌合結構化任務而非生成任務上的這一差距。

##### **Privacy Preserving Federated Learning in Medical Imaging with Uncertainty Estimation**
2406.12815v1 by Nikolas Koutsoubis, Yasin Yilmaz, Ravi P. Ramachandran, Matthew Schabath, Ghulam Rasool

Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable
advancements, particularly in healthcare. Within medical imaging, ML models
hold the promise of improving disease diagnoses, treatment planning, and
post-treatment monitoring. Various computer vision tasks like image
classification, object detection, and image segmentation are poised to become
routine in clinical analysis. However, privacy concerns surrounding patient
data hinder the assembly of large training datasets needed for developing and
training accurate, robust, and generalizable models. Federated Learning (FL)
emerges as a compelling solution, enabling organizations to collaborate on ML
model training by sharing model training information (gradients) rather than
data (e.g., medical images). FL's distributed learning framework facilitates
inter-institutional collaboration while preserving patient privacy. However,
FL, while robust in privacy preservation, faces several challenges. Sensitive
information can still be gleaned from shared gradients that are passed on
between organizations during model training. Additionally, in medical imaging,
quantifying model confidence\uncertainty accurately is crucial due to the noise
and artifacts present in the data. Uncertainty estimation in FL encounters
unique hurdles due to data heterogeneity across organizations. This paper
offers a comprehensive review of FL, privacy preservation, and uncertainty
estimation, with a focus on medical imaging. Alongside a survey of current
research, we identify gaps in the field and suggest future directions for FL
research to enhance privacy and address noisy medical imaging data challenges.

摘要：機器學習 (ML) 和人工智慧 (AI) 已推動顯著的進展，特別是在醫療保健方面。在醫學影像中，ML 模型有望改善疾病診斷、治療規劃和治療後監控。各種電腦視覺任務，例如影像分類、物件偵測和影像分割，都準備好在臨床分析中成為常規。然而，圍繞患者資料的隱私問題阻礙了組建大型訓練資料集，而這對於開發和訓練準確、強健且可概化的模型是必要的。聯邦學習 (FL) 成為一個引人注目的解決方案，使組織能夠透過分享模型訓練資訊 (梯度) 而不是資料（例如醫學影像）來協作進行 ML 模型訓練。FL 的分散式學習架構促進了機構間的協作，同時保護了患者隱私。然而，FL 雖然在隱私保護方面很強大，但仍面臨許多挑戰。敏感資訊仍然可以從組織在模型訓練期間傳遞的共享梯度中收集。此外，在醫學影像中，由於資料中存在雜訊和人工製品，因此準確量化模型信心/不確定性至關重要。由於組織間資料異質性，FL 中的不確定性估計會遇到獨特障礙。本文全面回顧了 FL、隱私保護和不確定性估計，重點放在醫學影像上。除了對當前研究進行調查外，我們還找出該領域的差距，並提出 FL 研究的未來方向，以增強隱私並解決雜訊醫學影像資料的挑戰。

##### **Adversarial Attacks on Multimodal Agents**
2406.12814v1 by Chen Henry Wu, Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, Aditi Raghunathan

Vision-enabled language models (VLMs) are now used to build autonomous
multimodal agents capable of taking actions in real environments. In this
paper, we show that multimodal agents raise new safety risks, even though
attacking agents is more challenging than prior attacks due to limited access
to and knowledge about the environment. Our attacks use adversarial text
strings to guide gradient-based perturbation over one trigger image in the
environment: (1) our captioner attack attacks white-box captioners if they are
used to process images into captions as additional inputs to the VLM; (2) our
CLIP attack attacks a set of CLIP models jointly, which can transfer to
proprietary VLMs. To evaluate the attacks, we curated VisualWebArena-Adv, a set
of adversarial tasks based on VisualWebArena, an environment for web-based
multimodal agent tasks. Within an L-infinity norm of $16/256$ on a single
image, the captioner attack can make a captioner-augmented GPT-4V agent execute
the adversarial goals with a 75% success rate. When we remove the captioner or
use GPT-4V to generate its own captions, the CLIP attack can achieve success
rates of 21% and 43%, respectively. Experiments on agents based on other VLMs,
such as Gemini-1.5, Claude-3, and GPT-4o, show interesting differences in their
robustness. Further analysis reveals several key factors contributing to the
attack's success, and we also discuss the implications for defenses as well.
Project page: https://chenwu.io/attack-agent Code and data:
https://github.com/ChenWu98/agent-attack

摘要：<paragraph>現在，具備視覺功能的語言模型 (VLM) 被用於建構自主多模態代理，能夠在真實環境中採取行動。在本文中，我們展示多模態代理會帶來新的安全風險，即使攻擊代理比先前的攻擊更具挑戰性，因為對環境的存取和知識有限。我們的攻擊使用對抗性文字字串來引導環境中一個觸發影像上的基於梯度的擾動：(1) 我們的標題攻擊攻擊白盒標題器，如果它們用於將影像處理成標題作為 VLM 的額外輸入；(2) 我們的 CLIP 攻擊共同攻擊一組 CLIP 模型，可以轉移到專有 VLM。為了評估這些攻擊，我們策劃了 VisualWebArena-Adv，這是一組基於 VisualWebArena 的對抗性任務，VisualWebArena 是用於基於網路的多模態代理任務的環境。在單一影像上，L-infinity 範數為 $16/256$，標題攻擊可以讓標題器增強的 GPT-4V 代理以 75% 的成功率執行對抗性目標。當我們移除標題器或使用 GPT-4V 產生自己的標題時，CLIP 攻擊可以分別達到 21% 和 43% 的成功率。基於其他 VLM 的代理的實驗，例如 Gemini-1.5、Claude-3 和 GPT-4o，顯示出它們的穩健性有有趣的差異。進一步的分析揭示了幾個導致攻擊成功的關鍵因素，我們也討論了對防禦的影響。專案頁面：https://chenwu.io/attack-agent 程式碼和資料：https://github.com/ChenWu98/agent-attack</paragraph>

##### **Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?**
2406.12809v1 by Zhe Yang, Yichang Zhang, Tianyu Liu, Jian Yang, Junyang Lin, Chang Zhou, Zhifang Sui

Large language models (LLMs) have demonstrated impressive capabilities, but
still suffer from inconsistency issues (e.g. LLMs can react differently to
disturbances like rephrasing or inconsequential order change). In addition to
these inconsistencies, we also observe that LLMs, while capable of solving hard
problems, can paradoxically fail at easier ones. To evaluate this hard-to-easy
inconsistency, we develop the ConsisEval benchmark, where each entry comprises
a pair of questions with a strict order of difficulty. Furthermore, we
introduce the concept of consistency score to quantitatively measure this
inconsistency and analyze the potential for improvement in consistency by
relative consistency score. Based on comprehensive experiments across a variety
of existing models, we find: (1) GPT-4 achieves the highest consistency score
of 92.2\% but is still inconsistent to specific questions due to distraction by
redundant information, misinterpretation of questions, etc.; (2) models with
stronger capabilities typically exhibit higher consistency, but exceptions also
exist; (3) hard data enhances consistency for both fine-tuning and in-context
learning. Our data and code will be publicly available on GitHub.

摘要：大型語言模型 (LLM) 已展現出令人印象深刻的能力，但
仍有前後不一致的問題（例如，LLM 對改寫或無關順序變更等干擾的反應可能不同）。除了
這些不一致之外，我們還觀察到，LLM 在解決困難問題的同時，卻可能在較容易的問題上出現矛盾的失敗。為了評估這種難題易題的不一致性，我們開發了 ConsisEval 基準，其中的每一項都包含一對難度順序嚴格的問題。此外，我們
引入了前後一致性評分這個概念，以量化測量這種不一致性，並透過相對前後一致性評分分析前後一致性改進的可能性。根據對各種現有模型進行的綜合實驗，我們發現：(1) GPT-4 達到了最高的前後一致性評分 92.2%，但由於冗餘資訊的干擾、對問題的誤解等原因，在特定問題上仍前後不一致；(2) 能力較強的模型通常表現出較高的前後一致性，但也存在例外；(3) 困難的資料增強了微調和情境學習的一致性。我們的資料和程式碼將在 GitHub 上公開。

##### **Graph Neural Networks in Histopathology: Emerging Trends and Future Directions**
2406.12808v1 by Siemen Brussee, Giorgio Buzzanca, Anne M. R. Schrader, Jesper Kers

Histopathological analysis of Whole Slide Images (WSIs) has seen a surge in
the utilization of deep learning methods, particularly Convolutional Neural
Networks (CNNs). However, CNNs often fall short in capturing the intricate
spatial dependencies inherent in WSIs. Graph Neural Networks (GNNs) present a
promising alternative, adept at directly modeling pairwise interactions and
effectively discerning the topological tissue and cellular structures within
WSIs. Recognizing the pressing need for deep learning techniques that harness
the topological structure of WSIs, the application of GNNs in histopathology
has experienced rapid growth. In this comprehensive review, we survey GNNs in
histopathology, discuss their applications, and exploring emerging trends that
pave the way for future advancements in the field. We begin by elucidating the
fundamentals of GNNs and their potential applications in histopathology.
Leveraging quantitative literature analysis, we identify four emerging trends:
Hierarchical GNNs, Adaptive Graph Structure Learning, Multimodal GNNs, and
Higher-order GNNs. Through an in-depth exploration of these trends, we offer
insights into the evolving landscape of GNNs in histopathological analysis.
Based on our findings, we propose future directions to propel the field
forward. Our analysis serves to guide researchers and practitioners towards
innovative approaches and methodologies, fostering advancements in
histopathological analysis through the lens of graph neural networks.

摘要：全幻燈片影像 (WSI) 的組織病理學分析已經在深度學習方法的應用中激增，特別是卷積神經網路 (CNN)。然而，CNN 在擷取 WSI 中固有的複雜空間依賴性方面常常不足。圖神經網路 (GNN) 提出了一個有希望的替代方案，擅長直接建模成對交互作用並有效辨別 WSI 中的拓撲組織和細胞結構。認識到對利用 WSI 拓撲結構的深度學習技術的迫切需求，GNN 在組織病理學中的應用經歷了快速增長。在這個全面的回顧中，我們調查了組織病理學中的 GNN，討論了它們的應用，並探索了為該領域的未來進展鋪平道路的新興趨勢。我們從闡明 GNN 的基礎及其在組織病理學中的潛在應用開始。利用定量文獻分析，我們確定了四個新興趨勢：分層 GNN、自適應圖形結構學習、多模態 GNN 和高階 GNN。透過深入探討這些趨勢，我們提供了對組織病理學分析中 GNN 不斷變化的格局的見解。根據我們的研究結果，我們提出了推動該領域前進的未來方向。我們的分析旨在引導研究人員和從業者採用創新方法和方法論，透過圖神經網路的視角促進組織病理學分析的進步。

##### **Probabilistic Temporal Prediction of Continuous Disease Trajectories and Treatment Effects Using Neural SDEs**
2406.12807v1 by Joshua Durso-Finley, Berardino Barile, Jean-Pierre Falet, Douglas L. Arnold, Nick Pawlowski, Tal Arbel

Personalized medicine based on medical images, including predicting future
individualized clinical disease progression and treatment response, would have
an enormous impact on healthcare and drug development, particularly for
diseases (e.g. multiple sclerosis (MS)) with long term, complex, heterogeneous
evolutions and no cure. In this work, we present the first stochastic causal
temporal framework to model the continuous temporal evolution of disease
progression via Neural Stochastic Differential Equations (NSDE). The proposed
causal inference model takes as input the patient's high dimensional images
(MRI) and tabular data, and predicts both factual and counterfactual
progression trajectories on different treatments in latent space. The NSDE
permits the estimation of high-confidence personalized trajectories and
treatment effects. Extensive experiments were performed on a large,
multi-centre, proprietary dataset of patient 3D MRI and clinical data acquired
during several randomized clinical trials for MS treatments. Our results
present the first successful uncertainty-based causal Deep Learning (DL) model
to: (a) accurately predict future patient MS disability evolution (e.g. EDSS)
and treatment effects leveraging baseline MRI, and (b) permit the discovery of
subgroups of patients for which the model has high confidence in their response
to treatment even in clinical trials which did not reach their clinical
endpoints.

摘要：基於醫學影像的個人化醫療，包括預測未來個人化臨床疾病進程和治療反應，將對醫療保健和藥物開發產生巨大影響，特別是對於長期、複雜、異質性演化且無法治癒的疾病（例如多發性硬化症 (MS)）。在這項工作中，我們提出了第一個隨機因果時間框架，透過神經隨機微分方程式 (NSDE) 對疾病進程的連續時間演化進行建模。所提出的因果推論模型以病患的高維度影像（MRI）和表格資料作為輸入，並預測潛在空間中不同治療的實際和反事實進程軌跡。NSDE 允許估計高可信度的個人化軌跡和治療效果。在針對 MS 治療進行的幾項隨機臨床試驗中，對一個大型、多中心、專有資料集的病患 3D MRI 和臨床資料進行了廣泛的實驗。我們的結果展示了第一個成功的基於不確定性的因果深度學習 (DL) 模型：(a) 準確預測未來的病患 MS 殘疾演化（例如 EDSS）和利用基線 MRI 的治療效果，以及 (b) 允許發現即使在未達到臨床終點的臨床試驗中，模型對其治療反應具有高度信心的病患子群。

##### **Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**
2406.12806v1 by Zehao Wang, Dong Jae Kim, Tse-Hsun Chen

Configuration settings are essential for tailoring software behavior to meet
specific performance requirements. However, incorrect configurations are
widespread, and identifying those that impact system performance is challenging
due to the vast number and complexity of possible settings. In this work, we
present PerfSense, a lightweight framework that leverages Large Language Models
(LLMs) to efficiently identify performance-sensitive configurations with
minimal overhead. PerfSense employs LLM agents to simulate interactions between
developers and performance engineers using advanced prompting techniques such
as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of
seven open-source Java systems demonstrates that PerfSense achieves an average
accuracy of 64.77% in classifying performance-sensitive configurations,
outperforming both our LLM baseline (50.36%) and the previous state-of-the-art
method (61.75%). Notably, our prompt chaining technique improves recall by 10%
to 30% while maintaining similar precision levels. Additionally, a manual
analysis of 362 misclassifications reveals common issues, including LLMs'
misunderstandings of requirements (26.8%). In summary, PerfSense significantly
reduces manual effort in classifying performance-sensitive configurations and
offers valuable insights for future LLM-based code analysis research.

摘要：設定組態對於調整軟體行為以符合特定效能需求至關重要。然而，不正確的組態很普遍，且由於可能的設定數量龐大且複雜，因此找出影響系統效能的設定具有挑戰性。在這項工作中，我們提出 PerfSense，一個輕量級的架構，它利用大型語言模型 (LLM) 以最小的開銷有效地找出效能敏感的組態。PerfSense 使用 LLM 代理程式來模擬開發人員和效能工程師之間的互動，使用進階提示技術，例如提示鏈接和檢索增強生成 (RAG)。我們對七個開源 Java 系統的評估顯示，PerfSense 在分類效能敏感組態方面達到了 64.77% 的平均準確度，優於我們的 LLM 基準 (50.36%) 和先前的最先進方法 (61.75%)。值得注意的是，我們的提示鏈接技術將召回率提高了 10% 至 30%，同時維持類似的精確度水準。此外，對 362 個錯誤分類的手動分析揭露了常見的問題，包括 LLM 對需求的誤解 (26.8%)。總之，PerfSense 大幅減少了分類效能敏感組態的手動工作，並為未來的基於 LLM 的程式碼分析研究提供了寶貴的見解。

##### **ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools**
2406.12793v1 by Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang

We introduce ChatGLM, an evolving family of large language models that we
have been developing over time. This report primarily focuses on the GLM-4
language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent
our most capable models that are trained with all the insights and lessons
gained from the preceding three generations of ChatGLM. To date, the GLM-4
models are pre-trained on ten trillions of tokens mostly in Chinese and
English, along with a small set of corpus from 24 languages, and aligned
primarily for Chinese and English usage. The high-quality alignment is achieved
via a multi-stage post-training process, which involves supervised fine-tuning
and learning from human feedback. Evaluations show that GLM-4 1) closely rivals
or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH,
BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following
as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long
context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by
AlignBench. The GLM-4 All Tools model is further aligned to understand user
intent and autonomously decide when and which tool(s) touse -- including web
browser, Python interpreter, text-to-image model, and user-defined functions --
to effectively complete complex tasks. In practical applications, it matches
and even surpasses GPT-4 All Tools in tasks like accessing online information
via web browsing and solving math problems using Python interpreter. Over the
course, we have open-sourced a series of models, including ChatGLM-6B (three
generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting
over 10 million downloads on Hugging face in the year 2023 alone. The open
models can be accessed through https://github.com/THUDM and
https://huggingface.co/THUDM.

摘要：<paragraph>我們推出 ChatGLM，這是一個隨著時間發展的大型語言模型系列。本報告主要關注 GLM-4 語言系列，其中包括 GLM-4、GLM-4-Air 和 GLM-4-9B。它們代表我們最優秀的模型，並利用從前三代 ChatGLM 中獲得的所有見解和經驗教訓進行訓練。迄今為止，GLM-4 模型已在數萬億個主要為中文和英文的符號上進行預訓練，並包含一小組來自 24 種語言的語料庫，並主要針對中文和英文使用進行對齊。高品質對齊是通過多階段後訓練過程實現的，其中涉及監督微調和從人類回饋中學習。評估顯示 GLM-4 1) 在 MMLU、GSM8K、MATH、BBH、GPQA 和 HumanEval 等一般指標方面與 GPT-4 接近或優於 GPT-4，2) 在 IFEval 衡量的指令遵循方面接近 GPT-4-Turbo，3) 在長上下文任務中與 GPT-4 Turbo (128K) 和 Claude 3 相匹配，以及 4) 在 AlignBench 衡量的中文對齊方面優於 GPT-4。GLM-4 All Tools 模型進一步對齊以了解用戶意圖並自主決定何時以及使用哪種工具（包括網路瀏覽器、Python 解譯器、文字轉圖像模型和使用者定義函式）來有效完成複雜任務。在實際應用中，它在通過網路瀏覽存取線上資訊和使用 Python 解譯器解決數學問題等任務中與 GPT-4 All Tools 相匹配甚至超越 GPT-4 All Tools。在此過程中，我們開源了一系列模型，包括 ChatGLM-6B（三代）、GLM-4-9B（128K、1M）、GLM-4V-9B、WebGLM 和 CodeGeeX，僅在 2023 年一年就在 Hugging face 上吸引了超過 1000 萬次的下載。開放模型可透過 https://github.com/THUDM 和 https://huggingface.co/THUDM 存取。</paragraph>

##### **UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions**
2406.12784v1 by Xunzhi Wang, Zhuowei Zhang, Qiongyu Li, Gaonan Chen, Mengting Hu, Zhiyu li, Bitong Luo, Hang Gao, Zhixin Han, Haotian Wang

The rapid development of large language models (LLMs) has shown promising
practical results. However, their low interpretability often leads to errors in
unforeseen circumstances, limiting their utility. Many works have focused on
creating comprehensive evaluation systems, but previous benchmarks have
primarily assessed problem-solving abilities while neglecting the response's
uncertainty, which may result in unreliability. Recent methods for measuring
LLM reliability are resource-intensive and unable to test black-box models. To
address this, we propose UBENCH, a comprehensive benchmark for evaluating LLM
reliability. UBENCH includes 3,978 multiple-choice questions covering
knowledge, language, understanding, and reasoning abilities. Experimental
results show that UBENCH has achieved state-of-the-art performance, while its
single-sampling method significantly saves computational resources compared to
baseline methods that require multiple samplings. Additionally, based on
UBENCH, we evaluate the reliability of 15 popular LLMs, finding GLM4 to be the
most outstanding, closely followed by GPT-4. We also explore the impact of
Chain-of-Thought prompts, role-playing prompts, option order, and temperature
on LLM reliability, analyzing the varying effects on different LLMs.

摘要：大型語言模型 (LLM) 的快速發展已展現出有前景的實際成果。然而，它們的低可解釋性常常導致在無法預見的情況下發生錯誤，限制了它們的效用。許多作品專注於建立全面的評估系統，但先前的基準主要評估了解決問題的能力，而忽略了回應的不確定性，這可能會導致不可靠性。最近用於測量 LLM 可靠性的方法需要大量資源，而且無法測試黑盒模型。為了解決這個問題，我們提出了 UBENCH，這是一個用於評估 LLM 可靠性的綜合基準。UBENCH 包含 3,978 個多選題，涵蓋知識、語言、理解和推理能力。實驗結果顯示，UBENCH 已達到最先進的效能，而它的單一抽樣方法與需要多重抽樣的基本方法相比，顯著節省了運算資源。此外，根據 UBENCH，我們評估了 15 個流行 LLM 的可靠性，發現 GLM4 最為傑出，緊隨其後的是 GPT-4。我們還探討了思想鏈提示、角色扮演提示、選項順序和溫度對 LLM 可靠性的影響，分析了對不同 LLM 的不同影響。

##### **Composited-Nested-Learning with Data Augmentation for Nested Named Entity Recognition**
2406.12779v1 by Xingming Liao, Nankai Lin, Haowen Li, Lianglun Cheng, Zhuowei Wang, Chong Chen

Nested Named Entity Recognition (NNER) focuses on addressing overlapped
entity recognition. Compared to Flat Named Entity Recognition (FNER), annotated
resources are scarce in the corpus for NNER. Data augmentation is an effective
approach to address the insufficient annotated corpus. However, there is a
significant lack of exploration in data augmentation methods for NNER. Due to
the presence of nested entities in NNER, existing data augmentation methods
cannot be directly applied to NNER tasks. Therefore, in this work, we focus on
data augmentation for NNER and resort to more expressive structures,
Composited-Nested-Label Classification (CNLC) in which constituents are
combined by nested-word and nested-label, to model nested entities. The dataset
is augmented using the Composited-Nested-Learning (CNL). In addition, we
propose the Confidence Filtering Mechanism (CFM) for a more efficient selection
of generated data. Experimental results demonstrate that this approach results
in improvements in ACE2004 and ACE2005 and alleviates the impact of sample
imbalance.

摘要：嵌套命名實體識別 (NNER) 重點在於解決重疊實體識別。相較於平面命名實體識別 (FNER)，在 NNER 的語料庫中，註解資源稀少。資料擴充是一種解決註解語料庫不足的有效方法。然而，在 NNER 的資料擴充方法中，顯著缺乏探索。由於 NNER 中存在嵌套實體，現有的資料擴充方法無法直接應用於 NNER 任務。因此，在這項工作中，我們專注於 NNER 的資料擴充，並訴諸更具表現力的結構，即複合嵌套標籤分類 (CNLC)，其中成分透過嵌套詞和嵌套標籤組合，來建模嵌套實體。資料集使用複合嵌套學習 (CNL) 進行擴充。此外，我們提出信心過濾機制 (CFM) 以更有效率地選擇所產生的資料。實驗結果證明此方法可改善 ACE2004 和 ACE2005，並減輕樣本不平衡的影響。

##### **Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries**
2406.12775v1 by Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, Amir Globerson

Large language models (LLMs) can solve complex multi-step problems, but
little is known about how these computations are implemented internally.
Motivated by this, we study how LLMs answer multi-hop queries such as "The
spouse of the performer of Imagine is". These queries require two information
extraction steps: a latent one for resolving the first hop ("the performer of
Imagine") into the bridge entity (John Lennon), and one for resolving the
second hop ("the spouse of John Lennon") into the target entity (Yoko Ono).
Understanding how the latent step is computed internally is key to
understanding the overall computation. By carefully analyzing the internal
computations of transformer-based LLMs, we discover that the bridge entity is
resolved in the early layers of the model. Then, only after this resolution,
the two-hop query is solved in the later layers. Because the second hop
commences in later layers, there could be cases where these layers no longer
encode the necessary knowledge for correctly predicting the answer. Motivated
by this, we propose a novel "back-patching" analysis method whereby a hidden
representation from a later layer is patched back to an earlier layer. We find
that in up to 57% of previously incorrect cases there exists a back-patch that
results in the correct generation of the answer, showing that the later layers
indeed sometimes lack the needed functionality. Overall our methods and
findings open further opportunities for understanding and improving latent
reasoning in transformer-based LLMs.

摘要：大型語言模型 (LLM) 能解決複雜的多步驟問題，但對於這些計算是如何在內部執行的，我們所知甚少。受此啟發，我們研究 LLM 如何回答多跳查詢，例如「Imagine 的表演者的配偶是」。這些查詢需要兩個資訊抽取步驟：一個潛在步驟，用於將第一跳（「Imagine 的表演者」）解析為橋接實體（約翰藍儂），另一個步驟，用於將第二跳（「約翰藍儂的配偶」）解析為目標實體（小野洋子）。了解潛在步驟如何在內部計算是了解整體計算的關鍵。透過仔細分析基於轉換器的 LLM 的內部計算，我們發現橋接實體是在模型的早期層解析的。然後，僅在這個解析之後，兩跳查詢才在後續層解析。由於第二跳在後續層開始，因此可能會出現這些層不再編碼正確預測答案所需知識的情況。受此啟發，我們提出了一種新穎的「反向修補」分析方法，藉此將後續層的隱藏表示修補回早期層。我們發現，在先前不正確的案例中，有高達 57% 的案例存在反向修補，這會導致正確產生答案，顯示後續層確實有時候缺乏所需的機能。總體而言，我們的各種方法和發現為進一步了解和改進基於轉換器的 LLM 中的潛在推理開闢了更多機會。

##### **Formatics & dairy industry coalition: AI trends and present challenges**
2406.12770v1 by Silvia García-Méndez, Francisco de Arriba-Pérez, María del Carmen Somoza-López

Artificial Intelligence (AI) can potentially transform the industry,
enhancing the production process and minimizing manual, repetitive tasks.
Accordingly, the synergy between high-performance computing and powerful
mathematical models enables the application of sophisticated data analysis
procedures like Machine Learning. However, challenges exist regarding
effective, efficient, and flexible processing to generate valuable knowledge.
Consequently, this work comprehensively describes industrial challenges where
AI can be exploited, focusing on the dairy industry. The conclusions presented
can help researchers apply novel approaches for cattle monitoring and farmers
by proposing advanced technological solutions to their needs.

摘要：人工智慧（AI）有潛力轉變產業，
增強生產流程並將手動、重複性的任務降到最低。
因此，高性能運算與強大數學模型之間的綜效，
能應用在精密資料分析的程序中，例如機器學習。
然而，在產生有價值的知識過程中，
仍存在著有效、效率和彈性處理的挑戰。
因此，本研究全面性地描述了產業挑戰，
說明 AI 能如何被利用，並專注於乳製品產業。
提出的結論能幫助研究人員應用創新的方法來監控牛隻，
並透過提出先進的技術解決方案來滿足農民的需求。

##### **Latent Intuitive Physics: Learning to Transfer Hidden Physics from A 3D Video**
2406.12769v1 by Xiangming Zhu, Huayu Deng, Haochen Yuan, Yunbo Wang, Xiaokang Yang

We introduce latent intuitive physics, a transfer learning framework for
physics simulation that can infer hidden properties of fluids from a single 3D
video and simulate the observed fluid in novel scenes. Our key insight is to
use latent features drawn from a learnable prior distribution conditioned on
the underlying particle states to capture the invisible and complex physical
properties. To achieve this, we train a parametrized prior learner given visual
observations to approximate the visual posterior of inverse graphics, and both
the particle states and the visual posterior are obtained from a learned neural
renderer. The converged prior learner is embedded in our probabilistic physics
engine, allowing us to perform novel simulations on unseen geometries,
boundaries, and dynamics without knowledge of the true physical parameters. We
validate our model in three ways: (i) novel scene simulation with the learned
visual-world physics, (ii) future prediction of the observed fluid dynamics,
and (iii) supervised particle simulation. Our model demonstrates strong
performance in all three tasks.

摘要：我們引入了潛在直觀物理，一種用於物理模擬的轉移學習框架，它可以從單個 3D 視訊推斷流體的隱藏屬性，並在新的場景中模擬觀察到的流體。我們的關鍵見解是使用從可學習先驗分佈中提取的潛在特徵，以捕獲不可見且複雜的物理屬性，該分佈以基礎粒子狀態為條件。為實現此目的，我們訓練一個參數化先驗學習器，給定視覺觀察，以近似逆圖形的視覺後驗，並且粒子狀態和視覺後驗都從學習的神經渲染器中獲得。收斂的先驗學習器嵌入在我們的機率物理引擎中，使我們能夠對未見過的幾何形狀、邊界和動力學執行新的模擬，而無需了解真正的物理參數。我們以三種方式驗證我們的模型：(i) 使用學習的視覺世界物理進行新場景模擬，(ii) 預測觀察到的流體動力學的未來，以及 (iii) 監督粒子模擬。我們的模型在所有三個任務中都展現出強大的效能。

##### **Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding Dataset from Ruo Zhi Ba**
2406.12754v1 by Ruiqi He, Yushu He, Longju Bai, Jiarui Liu, Zhenjie Sun, Zenghao Tang, He Wang, Hanchen Xia, Naihao Deng

Existing humor datasets and evaluations predominantly focus on English,
lacking resources for culturally nuanced humor in non-English languages like
Chinese. To address this gap, we construct Chumor, a dataset sourced from Ruo
Zhi Ba (RZB), a Chinese Reddit-like platform dedicated to sharing
intellectually challenging and culturally specific jokes. We annotate
explanations for each joke and evaluate human explanations against two
state-of-the-art LLMs, GPT-4o and ERNIE Bot, through A/B testing by native
Chinese speakers. Our evaluation shows that Chumor is challenging even for SOTA
LLMs, and the human explanations for Chumor jokes are significantly better than
explanations generated by the LLMs.

摘要：現有的幽默資料集和評估主要集中在英語上，缺乏針對非英語語言（如中文）中帶有文化細微差别的幽默資源。為了解決這個差距，我們構建了 Chumor，一個從 Ruo Zhi Ba (RZB) 獲取的資料集，RZB 是類似 Reddit 的中文平台，專門用於分享具有智力挑戰性和文化特色的笑話。我們為每個笑話添加註解說明，並通過讓中文母語人士進行 A/B 測試，將人類的說明與兩個最先進的 LLM（GPT-4o 和 ERNIE Bot）進行評估。我們的評估表明，即使對於 SOTA LLM，Chumor 也是具有挑戰性的，而人類對 Chumor 笑話的解釋顯著優於 LLM 生成的解釋。

##### **OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI**
2406.12753v1 by Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, Pengfei Liu

The evolution of Artificial Intelligence (AI) has been significantly
accelerated by advancements in Large Language Models (LLMs) and Large
Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning
abilities in problem-solving and scientific discovery (i.e., AI4Science) once
exclusive to human intellect. To comprehensively evaluate current models'
performance in cognitive reasoning abilities, we introduce OlympicArena, which
includes 11,163 bilingual problems across both text-only and interleaved
text-image modalities. These challenges encompass a wide range of disciplines
spanning seven fields and 62 international Olympic competitions, rigorously
examined for data leakage. We argue that the challenges in Olympic competition
problems are ideal for evaluating AI's cognitive reasoning due to their
complexity and interdisciplinary nature, which are essential for tackling
complex scientific challenges and facilitating discoveries. Beyond evaluating
performance across various disciplines using answer-only criteria, we conduct
detailed experiments and analyses from multiple perspectives. We delve into the
models' cognitive reasoning abilities, their performance across different
modalities, and their outcomes in process-level evaluations, which are vital
for tasks requiring complex reasoning with lengthy solutions. Our extensive
evaluations reveal that even advanced models like GPT-4o only achieve a 39.97%
overall accuracy, illustrating current AI limitations in complex reasoning and
multimodal integration. Through the OlympicArena, we aim to advance AI towards
superintelligence, equipping it to address more complex challenges in science
and beyond. We also provide a comprehensive set of resources to support AI
research, including a benchmark dataset, an open-source annotation platform, a
detailed evaluation tool, and a leaderboard with automatic submission features.

摘要：人工智慧（AI）的演進因大型語言模型（LLM）和大規模多模態模型（LMM）的進展而大幅加速，逐漸展現出在問題解決和科學發現（即 AI4Science）中潛在的認知推理能力，這些能力曾僅限於人類智力。為了全面評估當前模型在認知推理能力中的表現，我們引入了 OlympicArena，其中包含 11,163 個雙語問題，涵蓋純文字和交錯文字影像模式。這些挑戰涵蓋了跨越七個領域和 62 項國際奧林匹克競賽的廣泛學科，並嚴格檢查了資料外洩。我們認為，奧林匹克競賽問題中的挑戰由於其複雜性和跨學科性質，非常適合評估 AI 的認知推理，這對於應對複雜的科學挑戰和促進發現至關重要。除了使用僅答案標準評估不同學科的表現外，我們還從多個角度進行了詳細的實驗和分析。我們深入探討了模型的認知推理能力、它們在不同模式下的表現，以及它們在過程層級評估中的結果，這對於需要複雜推理和冗長解決方案的任務至關重要。我們廣泛的評估表明，即使是像 GPT-4o 這樣的先進模型也只達到了 39.97% 的整體準確度，說明了當前 AI 在複雜推理和多模態整合方面的限制。透過 OlympicArena，我們旨在推進 AI 朝向超級智慧，使其能夠應對科學和更廣泛領域中更複雜的挑戰。我們還提供了一套全面的資源來支援 AI 研究，包括基準資料集、開源標註平台、詳細的評估工具，以及具有自動提交功能的排行榜。

##### **TSI-Bench: Benchmarking Time Series Imputation**
2406.12747v1 by Wenjie Du, Jun Wang, Linglong Qian, Yiyuan Yang, Fanxing Liu, Zepu Wang, Zina Ibrahim, Haoxin Liu, Zhiyuan Zhao, Yingjie Zhou, Wenjia Wang, Kaize Ding, Yuxuan Liang, B. Aditya Prakash, Qingsong Wen

Effective imputation is a crucial preprocessing step for time series
analysis. Despite the development of numerous deep learning algorithms for time
series imputation, the community lacks standardized and comprehensive benchmark
platforms to effectively evaluate imputation performance across different
settings. Moreover, although many deep learning forecasting algorithms have
demonstrated excellent performance, whether their modeling achievements can be
transferred to time series imputation tasks remains unexplored. To bridge these
gaps, we develop TSI-Bench, the first (to our knowledge) comprehensive
benchmark suite for time series imputation utilizing deep learning techniques.
The TSI-Bench pipeline standardizes experimental settings to enable fair
evaluation of imputation algorithms and identification of meaningful insights
into the influence of domain-appropriate missingness ratios and patterns on
model performance. Furthermore, TSI-Bench innovatively provides a systematic
paradigm to tailor time series forecasting algorithms for imputation purposes.
Our extensive study across 34,804 experiments, 28 algorithms, and 8 datasets
with diverse missingness scenarios demonstrates TSI-Bench's effectiveness in
diverse downstream tasks and potential to unlock future directions in time
series imputation research and analysis. The source code and experiment logs
are available at https://github.com/WenjieDu/AwesomeImputation.

摘要：有效的插补是时间序列分析中至关重要的预处理步骤。尽管已经开发出许多用于时间序列插补的深度学习算法，但社区缺乏标准化且全面的基准平台来有效评估不同设置下的插补性能。此外，尽管许多深度学习预测算法已经展示出优异的性能，但其建模成果是否可以转移到时间序列插补任务中仍未得到探索。为了弥补这些差距，我们开发了 TSI-Bench，这是（据我们所知）第一个利用深度学习技术的全面时间序列插补基准套件。TSI-Bench 管道标准化了实验设置，以支持插补算法的公平评估，并识别出对模型性能产生影响的领域适当缺失率和模式的有意义的见解。此外，TSI-Bench 创新性地提供了一个系统范例，用于定制时间序列预测算法以用于插补目的。我们对 34,804 个实验、28 个算法和 8 个具有不同缺失情况的数据集进行的广泛研究表明，TSI-Bench 在不同的下游任务中是有效的，并且有可能为时间序列插补研究和分析解锁未来的方向。源代码和实验日志可在 https://github.com/WenjieDu/AwesomeImputation 中获得。

##### **Rationale-based Ensemble of Multiple QA Strategies for Zero-shot Knowledge-based VQA**
2406.12746v1 by Miaoyu Li, Haoxin Li, Zilin Du, Boyang Li

Knowledge-based Visual Qustion-answering (K-VQA) necessitates the use of
background knowledge beyond what is depicted in the image. Current zero-shot
K-VQA methods usually translate an image to a single type of textual decision
context and use a text-based model to answer the question based on it, which
conflicts with the fact that K-VQA questions often require the combination of
multiple question-answering strategies. In light of this, we propose
Rationale-based Ensemble of Answer Context Tactics (REACT) to achieve a dynamic
ensemble of multiple question-answering tactics, comprising Answer Candidate
Generation (ACG) and Rationale-based Strategy Fusion (RSF). In ACG, we generate
three distinctive decision contexts to provide different strategies for each
question, resulting in the generation of three answer candidates. RSF generates
automatic and mechanistic rationales from decision contexts for each candidate,
allowing the model to select the correct answer from all candidates. We conduct
comprehensive experiments on the OK-VQA and A-OKVQA datasets, and our method
significantly outperforms state-of-the-art LLM-based baselines on all datasets.

摘要：基於知識的視覺問答（K-VQA）需要使用背景知識，而這些知識並非圖像中所描繪的。目前的零次學習 K-VQA 方法通常會將圖像轉換成單一類型的文字決策脈絡，並使用基於文字的模型根據該脈絡回答問題，這與 K-VQA 問題通常需要結合多種問答策略的事實相衝突。有鑑於此，我們提出了基於推理的答案脈絡策略組合（REACT），以實現多種問答策略的動態組合，包括答案候選產生（ACG）和基於推理的策略融合（RSF）。在 ACG 中，我們產生三個不同的決策脈絡，為每個問題提供不同的策略，從而產生三個答案候選。RSF 為每個候選從決策脈絡中產生自動且機械的推理，讓模型能夠從所有候選中選擇正確答案。我們對 OK-VQA 和 A-OKVQA 資料集進行了全面的實驗，我們的模型在所有資料集上都顯著優於最先進的基於 LLM 的基準。

##### **Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning**
2406.12742v1 by Bingchen Zhao, Yongshuo Zong, Letian Zhang, Timothy Hospedales

The advancement of large language models (LLMs) has significantly broadened
the scope of applications in natural language processing, with multi-modal LLMs
extending these capabilities to integrate and interpret visual data. However,
existing benchmarks for visual language models (VLMs) predominantly focus on
single-image inputs, neglecting the crucial aspect of multi-image
understanding. In this paper, we introduce a Multi-Image Relational Benchmark
MIRB, designed to evaluate VLMs' ability to compare, analyze, and reason across
multiple images. Our benchmark encompasses four categories: perception, visual
world knowledge, reasoning, and multi-hop reasoning. Through a comprehensive
evaluation of a wide range of open-source and closed-source models, we
demonstrate that while open-source VLMs were shown to approach the performance
of GPT-4V in single-image tasks, a significant performance gap remains in
multi-image reasoning tasks. Our findings also reveal that even the
state-of-the-art GPT-4V model struggles with our benchmark, underscoring the
need for further research and development in this area. We believe our
contribution of MIRB could serve as a testbed for developing the
next-generation multi-modal models.

摘要：大型語言模型 (LLM) 的進步大幅擴展了自然語言處理的應用範圍，而多模態 LLM 則將這些功能擴展到整合和詮釋視覺資料。然而，現有的視覺語言模型 (VLM) 基準主要關注單一影像輸入，忽略了多影像理解的關鍵面向。在本文中，我們引入了多影像關係基準 MIRB，旨在評估 VLM 在多張影像中比較、分析和推理的能力。我們的基準包含四個類別：感知、視覺世界知識、推理和多跳推理。透過對廣泛的開源和閉源模型進行全面評估，我們證明了雖然開源 VLM 已被證明能接近 GPT-4V 在單一影像任務中的效能，但在多影像推理任務中仍存在顯著的效能差距。我們的發現也揭示了即使是現今最先進的 GPT-4V 模型在我們的基準測試中仍有困難，這凸顯了進一步研究和開發此領域的必要性。我們相信 MIRB 的貢獻可以作為開發下一代多模態模型的測試平台。

##### **Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages**
2406.12739v1 by Fabian David Schmidt, Philipp Borchert, Ivan Vulić, Goran Glavaš

LLMs have become a go-to solution not just for text generation, but also for
natural language understanding (NLU) tasks. Acquiring extensive knowledge
through language modeling on web-scale corpora, they excel on English NLU, yet
struggle to extend their NLU capabilities to underrepresented languages. In
contrast, machine translation models (MT) produce excellent multilingual
representations, resulting in strong translation performance even for
low-resource languages. MT encoders, however, lack the knowledge necessary for
comprehensive NLU that LLMs obtain through language modeling training on
immense corpora. In this work, we get the best both worlds by integrating MT
encoders directly into LLM backbones via sample-efficient self-distillation.
The resulting MT-LLMs preserve the inherent multilingual representational
alignment from the MT encoder, allowing lower-resource languages to tap into
the rich knowledge embedded in English-centric LLMs. Merging the MT encoder and
LLM in a single model, we mitigate the propagation of translation errors and
inference overhead of MT decoding inherent to discrete translation-based
cross-lingual transfer (e.g., translate-test). Evaluation spanning three
prominent NLU tasks and 127 predominantly low-resource languages renders
MT-LLMs highly effective in cross-lingual transfer. MT-LLMs substantially and
consistently outperform translate-test based on the same MT model, showing that
we truly unlock multilingual language understanding for LLMs.

摘要：LLM 不僅成為文本生成的首選解決方案，也成為自然語言理解 (NLU) 任務的首選解決方案。透過在網路規模語料庫上進行語言模型化來獲取廣泛知識，它們在英文 NLU 中表現出色，但卻難以將其 NLU 能力擴展到代表性不足的語言。相比之下，機器翻譯模型 (MT) 可產生優異的多語言表徵，即使對於低資源語言也能產生強大的翻譯效能。然而，MT 編碼器缺少透過語言模型在龐大語料庫上訓練而獲得的，對於全面性 NLU 所需的知識。在這項工作中，我們透過樣本有效率的自我蒸餾，將 MT 編碼器直接整合到 LLM 主幹中，取得兩全其美。產生的 MT-LLM 保留了 MT 編碼器中固有的多語言表徵對齊，讓低資源語言能夠利用以英文為中心的 LLM 中蘊含的豐富知識。在單一模型中合併 MT 編碼器和 LLM，我們減輕了基於離散翻譯的跨語言轉移 (例如翻譯測試) 中固有的翻譯錯誤傳播和 MT 解碼推論負擔。評估涵蓋三個重要的 NLU 任務和 127 種以低資源語言為主的語言，顯示 MT-LLM 在跨語言轉移中非常有效。MT-LLM 大幅且持續優於基於相同 MT 模型的翻譯測試，顯示我們確實為 LLM 解鎖了多語言語言理解。

##### **Large Language Model as a Universal Clinical Multi-task Decoder**
2406.12738v1 by Yujiang Wu, Hongjian Song, Jiawen Zhang, Xumeng Wen, Shun Zheng, Jiang Bian

The development of effective machine learning methodologies for enhancing the
efficiency and accuracy of clinical systems is crucial. Despite significant
research efforts, managing a plethora of diversified clinical tasks and
adapting to emerging new tasks remain significant challenges. This paper
presents a novel paradigm that employs a pre-trained large language model as a
universal clinical multi-task decoder. This approach leverages the flexibility
and diversity of language expressions to handle task topic variations and
associated arguments. The introduction of a new task simply requires the
addition of a new instruction template. We validate this framework across
hundreds of tasks, demonstrating its robustness in facilitating multi-task
predictions, performing on par with traditional multi-task learning and
single-task learning approaches. Moreover, it shows exceptional adaptability to
new tasks, with impressive zero-shot performance in some instances and superior
data efficiency in few-shot scenarios. This novel approach offers a unified
solution to manage a wide array of new and emerging tasks in clinical
applications.

摘要：<paragraph>開發有效的機器學習方法，以提升臨床系統的效率和準確性至關重要。儘管研究付出相當大的努力，管理大量多樣化的臨床任務和適應新興任務仍然是重大的挑戰。本文提出了一種新的範例，採用預先訓練的大型語言模型作為通用的臨床多任務解碼器。此方法利用語言表達的靈活性與多樣性來處理任務主題變化和相關論點。引入新任務只需要新增一個新的指令範本。我們驗證了這個架構在數百個任務中，證明了它在促進多任務預測方面的穩健性，執行與傳統多任務學習和單任務學習方法相當。此外，它展現出對新任務的卓越適應性，在某些情況下具有令人印象深刻的零次學習效能，在少量學習場景中具有優異的資料效率。這種新方法提供了一個統一的解決方案，來管理臨床應用中各種新的和新興任務。</paragraph>

##### **Automatic generation of insights from workers' actions in industrial workflows with explainable Machine Learning**
2406.12732v1 by Francisco de Arriba-Pérez, Silvia García-Méndez, Javier Otero-Mosquera, Francisco J. González-Castaño, Felipe Gil-Castiñeira

New technologies such as Machine Learning (ML) gave great potential for
evaluating industry workflows and automatically generating key performance
indicators (KPIs). However, despite established standards for measuring the
efficiency of industrial machinery, there is no precise equivalent for workers'
productivity, which would be highly desirable given the lack of a skilled
workforce for the next generation of industry workflows. Therefore, an ML
solution combining data from manufacturing processes and workers' performance
for that goal is required. Additionally, in recent times intense effort has
been devoted to explainable ML approaches that can automatically explain their
decisions to a human operator, thus increasing their trustworthiness. We
propose to apply explainable ML solutions to differentiate between expert and
inexpert workers in industrial workflows, which we validate at a quality
assessment industrial workstation. Regarding the methodology used, input data
are captured by a manufacturing machine and stored in a NoSQL database. Data
are processed to engineer features used in automatic classification and to
compute workers' KPIs to predict their level of expertise (with all
classification metrics exceeding 90 %). These KPIs, and the relevant features
in the decisions are textually explained by natural language expansion on an
explainability dashboard. These automatic explanations made it possible to
infer knowledge from expert workers for inexpert workers. The latter
illustrates the interest of research in self-explainable ML for automatically
generating insights to improve productivity in industrial workflows.

摘要：機器學習 (ML) 等新技術為評估產業工作流程和自動生成關鍵績效指標 (KPI) 帶來了巨大的潛力。然而，儘管有既定的標準來衡量工業機械的效率，但對於工人的生產力卻沒有精確的等價物，這在下一代產業工作流程缺乏熟練勞動力的情況下是非常需要的。因此，需要一種結合製造流程和工人績效數據的 ML 解决方案來達成此目標。此外，近年來，人們投入了大量精力來解釋 ML 方法，這些方法可以自動向人機操作員解釋其決策，從而提高其可信度。我們建議將可解釋的 ML 解决方案應用於區分產業工作流程中的專家和非專家工人，我們在一個品質評估工業工作站驗證了這一點。關於所使用的技術，輸入數據由製造機器擷取並儲存在 NoSQL 資料庫中。處理數據以設計用於自動分類的功能，並計算工人的 KPI 以預測他們的專業知識水準（所有分類指標均超過 90%）。這些 KPI 和決策中的相關功能由可解釋性儀表板上的自然語言擴充以文字方式解釋。這些自動解釋使得可以從專家工人那裡推論出知識，以供非專家工人使用。後者說明了自解釋性 ML 研究在自動生成見解以提高產業工作流程生產力方面的興趣。

##### **Can Large Language Models Code Like a Linguist?: A Case Study in Low Resource Sound Law Induction**
2406.12725v1 by Atharva Naik, Kexun Zhang, Nathaniel Robinson, Aravind Mysore, Clayton Marr, Hong Sng Rebecca Byrnes, Anna Cai, Kalvin Chang, David Mortensen

Historical linguists have long written a kind of incompletely formalized
''program'' that converts reconstructed words in an ancestor language into
words in one of its attested descendants that consist of a series of ordered
string rewrite functions (called sound laws). They do this by observing pairs
of words in the reconstructed language (protoforms) and the descendent language
(reflexes) and constructing a program that transforms protoforms into reflexes.
However, writing these programs is error-prone and time-consuming. Prior work
has successfully scaffolded this process computationally, but fewer researchers
have tackled Sound Law Induction (SLI), which we approach in this paper by
casting it as Programming by Examples. We propose a language-agnostic solution
that utilizes the programming ability of Large Language Models (LLMs) by
generating Python sound law programs from sound change examples. We evaluate
the effectiveness of our approach for various LLMs, propose effective methods
to generate additional language-agnostic synthetic data to fine-tune LLMs for
SLI, and compare our method with existing automated SLI methods showing that
while LLMs lag behind them they can complement some of their weaknesses.

摘要：歷史語言學家長期以來撰寫一種不完全形式化的「程式」，將祖語中重建的詞彙轉換成其已證後代之一種語言中的詞彙，該語言由一系列已排序的字串改寫函數（稱為音律）組成。他們透過觀察重建語言（原形）和後代語言（反映）中的詞彙對，並建構一個將原形轉換為反映的程式來執行此操作。不過，撰寫這些程式容易出錯且耗時。先前的工作已成功以運算方式架構此流程，但較少研究人員著手處理音律歸納 (SLI)，而我們在本文中將其視為範例程式設計來探討。我們提出一個與語言無關的解決方案，透過從音變範例產生 Python 音律程式，來利用大型語言模型 (LLM) 的程式設計能力。我們評估各種 LLM 的方法有效性，提出有效的方法來產生額外的與語言無關的合成資料，以微調 LLM 以進行 SLI，並將我們的模型與現有的自動化 SLI 方法進行比較，顯示 LLM 雖然落後於這些方法，但它們可以彌補某些弱點。

##### **On the Robustness of Language Models for Tabular Question Answering**
2406.12719v1 by Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao

Large Language Models (LLMs), originally shown to ace various text
comprehension tasks have also remarkably been shown to tackle table
comprehension tasks without specific training. While previous research has
explored LLM capabilities with tabular dataset tasks, our study assesses the
influence of $\textit{in-context learning}$,$ \textit{model scale}$,
$\textit{instruction tuning}$, and $\textit{domain biases}$ on Tabular Question
Answering (TQA). We evaluate the robustness of LLMs on Wikipedia-based
$\textbf{WTQ}$ and financial report-based $\textbf{TAT-QA}$ TQA datasets,
focusing on their ability to robustly interpret tabular data under various
augmentations and perturbations. Our findings indicate that instructions
significantly enhance performance, with recent models like Llama3 exhibiting
greater robustness over earlier versions. However, data contamination and
practical reliability issues persist, especially with WTQ. We highlight the
need for improved methodologies, including structure-aware self-attention
mechanisms and better handling of domain-specific tabular data, to develop more
reliable LLMs for table comprehension.

摘要：大型語言模型（LLM）最初被證明可以應對各種文本理解任務，而且還特別被證明可以在沒有特定訓練的情況下應對表格理解任務。雖然先前的研究已經探索了 LLM 在表格資料集任務中的能力，但我們的研究評估了「情境學習」、「模型規模」、「指令調整」和「領域偏差」對表格問題解答 (TQA) 的影響。我們評估了 LLM 在基於維基百科的「WTQ」和基於財務報表的「TAT-QA」TQA 資料集上的穩健性，重點關注它們在各種擴充和擾動下穩健解釋表格數據的能力。我們的研究結果表明，指令可以顯著提高效能，而像 Llama3 這樣的最新模型比早期版本表現出更強的穩健性。然而，數據污染和實際可靠性問題依然存在，特別是在 WTQ 中。我們強調需要改進方法論，包括結構感知自注意力機制和對特定領域表格數據的更好處理，以開發出更可靠的 LLM 來進行表格理解。

##### **AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention**
2406.12718v1 by Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, QianYing Wang, Guang Dai, Ping Chen, Shijian Lu

Despite their great success across various multimodal tasks, Large
Vision-Language Models (LVLMs) are facing a prevalent problem with object
hallucinations, where the generated textual responses are inconsistent with
ground-truth objects in the given image. This paper investigates various LVLMs
and pinpoints attention deficiency toward discriminative local image features
as one root cause of object hallucinations. Specifically, LVLMs predominantly
attend to prompt-independent global image features, while failing to capture
prompt-relevant local features, consequently undermining the visual grounding
capacity of LVLMs and leading to hallucinations. To this end, we propose
Assembly of Global and Local Attention (AGLA), a training-free and
plug-and-play approach that mitigates object hallucinations by exploring an
ensemble of global features for response generation and local features for
visual discrimination simultaneously. Our approach exhibits an image-prompt
matching scheme that captures prompt-relevant local features from images,
leading to an augmented view of the input image where prompt-relevant content
is reserved while irrelevant distractions are masked. With the augmented view,
a calibrated decoding distribution can be derived by integrating generative
global features from the original image and discriminative local features from
the augmented image. Extensive experiments show that AGLA consistently
mitigates object hallucinations and enhances general perception capability for
LVLMs across various discriminative and generative benchmarks. Our code will be
released at https://github.com/Lackel/AGLA.

摘要：儘管大型視覺語言模型 (LVLMs) 在各種多模態任務中取得巨大成功，但它們正面臨一個普遍的問題，即物件幻覺，其中產生的文字回應與給定影像中的真實物件不一致。本文探討了各種 LVLMs，並指出對區辨性局部影像特徵的注意力不足是物件幻覺的一個根本原因。具體來說，LVLMs 主要關注與提示無關的整體影像特徵，而無法擷取與提示相關的局部特徵，因此損害了 LVLMs 的視覺基礎能力，並導致幻覺。為此，我們提出了整體和局部注意力組裝 (AGLA)，這是一種免訓練且即插即用的方法，透過同時探索整體特徵以產生回應和局部特徵以進行視覺區辨，來減輕物件幻覺。我們的做法展示了一個影像提示比對機制，從影像中擷取與提示相關的局部特徵，從而導致輸入影像的擴增檢視，其中保留與提示相關的內容，同時遮蔽無關的干擾。透過擴增檢視，可以透過整合來自原始影像的生成整體特徵和來自擴增影像的區辨性局部特徵，來衍生校準的解碼分佈。廣泛的實驗表明，AGLA 持續減輕物件幻覺，並增強 LVLMs 在各種區辨性和生成基準中的整體感知能力。我們的程式碼將在 https://github.com/Lackel/AGLA 發布。

##### **Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning: Lessons Learned**
2406.12709v1 by Du Yin, Jinliang Deng, Shuang Ao, Zechen Li, Hao Xue, Arian Prabowo, Renhe Jiang, Xuan Song, Flora Salim

Training models on spatio-temporal (ST) data poses an open problem due to the
complicated and diverse nature of the data itself, and it is challenging to
ensure the model's performance directly trained on the original ST data. While
limiting the variety of training data can make training easier, it can also
lead to a lack of knowledge and information for the model, resulting in a
decrease in performance. To address this challenge, we presented an innovative
paradigm that incorporates three separate forms of curriculum learning
specifically targeting from spatial, temporal, and quantile perspectives.
Furthermore, our framework incorporates a stacking fusion module to combine
diverse information from three types of curriculum learning, resulting in a
strong and thorough learning process. We demonstrated the effectiveness of this
framework with extensive empirical evaluations, highlighting its better
performance in addressing complex ST challenges. We provided thorough ablation
studies to investigate the effectiveness of our curriculum and to explain how
it contributes to the improvement of learning efficiency on ST data.

摘要：訓練模型處理時空 (ST) 資料由於資料本身複雜且多樣化而構成一項公開的問題，而且要確保模型的效能直接訓練在原始 ST 資料上是一項挑戰。雖然限制訓練資料的多樣性可以讓訓練更輕鬆，但它也可能導致模型缺乏知識和資訊，造成效能降低。為了應對這項挑戰，我們提出了一種創新的模式，其中結合了三種不同的課程學習形式，特別針對空間、時間和分位數觀點。此外，我們的架構結合了一個堆疊融合模組，以結合來自三種類型課程學習的不同資訊，進而產生一個強大且徹底的學習歷程。我們透過廣泛的經驗評估證明了這個架構的有效性，突顯了它在處理複雜 ST 挑戰方面的優異效能。我們提供了徹底的消融研究，以探討我們課程的有效性，並說明它是如何有助於提升 ST 資料的學習效率。

##### **AgentReview: Exploring Peer Review Dynamics with LLM Agents**
2406.12708v1 by Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, Jindong Wang

Peer review is fundamental to the integrity and advancement of scientific
publication. Traditional methods of peer review analyses often rely on
exploration and statistics of existing peer review data, which do not
adequately address the multivariate nature of the process, account for the
latent variables, and are further constrained by privacy concerns due to the
sensitive nature of the data. We introduce AgentReview, the first large
language model (LLM) based peer review simulation framework, which effectively
disentangles the impacts of multiple latent factors and addresses the privacy
issue. Our study reveals significant insights, including a notable 37.1%
variation in paper decisions due to reviewers' biases, supported by
sociological theories such as the social influence theory, altruism fatigue,
and authority bias. We believe that this study could offer valuable insights to
improve the design of peer review mechanisms.

摘要：同儕審查是科學出版的完整性和進步的基礎。傳統的同儕審查分析方法通常依賴於對現有同儕審查資料的探討和統計，這並不能充分解決這個過程的多元性質，說明潛在變數，而且由於資料的敏感性，進一步受到隱私問題的限制。我們引入了 AgentReview，這是第一個基於大型語言模型 (LLM) 的同儕審查模擬架構，它有效地解開了多重潛在因素的影響，並解決了隱私問題。我們的研究揭示了重要的見解，包括由於審查者的偏見而導致的論文決策有顯著的 37.1% 差異，這得到了社會影響理論、利他疲勞和權威偏誤等社會學理論的支持。我們相信這項研究可以提供有價值的見解，以改善同儕審查機制的設計。

##### **Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction**
2406.12707v1 by Haoqiu Yan, Yongxin Zhu, Kai Zheng, Bing Liu, Haoyu Cao, Deqiang Jiang, Linli Xu

Large Language Model (LLM)-enhanced agents become increasingly prevalent in
Human-AI communication, offering vast potential from entertainment to
professional domains. However, current multi-modal dialogue systems overlook
the acoustic information present in speech, which is crucial for understanding
human communication nuances. This oversight can lead to misinterpretations of
speakers' intentions, resulting in inconsistent or even contradictory responses
within dialogues. To bridge this gap, in this paper, we propose
PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern
deeper or more subtle meanings beyond the literal interpretations of words
through the integration of speech modality perception. Employing LLMs as a
cognitive core, PerceptiveAgent perceives acoustic information from input
speech and generates empathetic responses based on speaking styles described in
natural language. Experimental results indicate that PerceptiveAgent excels in
contextual understanding by accurately discerning the speakers' true intentions
in scenarios where the linguistic meaning is either contrary to or inconsistent
with the speaker's true feelings, producing more nuanced and expressive spoken
dialogues. Code is publicly available at:
\url{https://github.com/Haoqiu-Yan/PerceptiveAgent}.

摘要：大型語言模型 (LLM) 增強的代理在人機溝通中日益普遍，在娛樂到專業領域提供廣泛的潛力。然而，當前的多模態對話系統忽視了語音中存在的聲學資訊，這對於理解人類溝通的細微差別至關重要。這種疏忽可能會導致對說話者意圖的誤解，從而導致對話中出現不一致甚至矛盾的回應。為了彌合這一差距，在本文中，我們提出了 PerceptiveAgent，這是一個同理心多模態對話系統，旨在透過整合語音模態感知，辨別出文字表層詮釋之外更深或更微妙的意義。PerceptiveAgent 以 LLM 作為認知核心，感知輸入語音中的聲學資訊，並根據以自然語言描述的說話風格產生同理心的回應。實驗結果表明，PerceptiveAgent 在脈絡理解方面表現出色，在語言意義與說話者的真實感受相反或不一致的情況下，準確地辨別出說話者的真實意圖，產生更細緻且富有表現力的口語對話。程式碼公開於：\url{https://github.com/Haoqiu-Yan/PerceptiveAgent}。

##### **Jailbreak Paradox: The Achilles' Heel of LLMs**
2406.12702v1 by Abhinav Rao, Monojit Choudhury, Somak Aditya

We introduce two paradoxes concerning jailbreak of foundation models: First,
it is impossible to construct a perfect jailbreak classifier, and second, a
weaker model cannot consistently detect whether a stronger (in a
pareto-dominant sense) model is jailbroken or not. We provide formal proofs for
these paradoxes and a short case study on Llama and GPT4-o to demonstrate this.
We discuss broader theoretical and practical repercussions of these results.

摘要：我們提出了兩個關於基礎模型越獄的悖論：首先，不可能建構一個完美的越獄分類器，其次，較弱的模型無法持續偵測較強的（在帕雷托主導意義上）模型是否越獄。我們提供了這些悖論的正式證明，並對 Llama 和 GPT4-o 進行了簡短的案例研究以證明這一點。我們討論了這些結果更廣泛的理論和實際影響。

##### **Online-Adaptive Anomaly Detection for Defect Identification in Aircraft Assembly**
2406.12698v1 by Siddhant Shete, Dennis Mronga, Ankita Jadhav, Frank Kirchner

Anomaly detection deals with detecting deviations from established patterns
within data. It has various applications like autonomous driving, predictive
maintenance, and medical diagnosis. To improve anomaly detection accuracy,
transfer learning can be applied to large, pre-trained models and adapt them to
the specific application context. In this paper, we propose a novel framework
for online-adaptive anomaly detection using transfer learning. The approach
adapts to different environments by selecting visually similar training images
and online fitting a normality model to EfficientNet features extracted from
the training subset. Anomaly detection is then performed by computing the
Mahalanobis distance between the normality model and the test image features.
Different similarity measures (SIFT/FLANN, Cosine) and normality models (MVG,
OCSVM) are employed and compared with each other. We evaluate the approach on
different anomaly detection benchmarks and data collected in controlled
laboratory settings. Experimental results showcase a detection accuracy
exceeding 0.975, outperforming the state-of-the-art ET-NET approach.

摘要：異常偵測處理偵測資料中既有模式的偏差。它有各種應用，例如自動駕駛、預測性維護和醫療診斷。為了改善異常偵測的準確性，轉移學習可以應用於大型預訓練模型，並將它們適應到特定的應用情境中。在本文中，我們提出了一個新的框架，用於使用轉移學習進行線上自適應異常偵測。此方法透過選擇視覺上相似的訓練影像，並線上擬合一個常態模型到從訓練子集中萃取的 EfficientNet 特徵，來適應不同的環境。然後透過計算常態模型和測試影像特徵之間的馬氏距離來執行異常偵測。採用不同的相似性度量（SIFT/FLANN、餘弦）和常態模型（MVG、OCSVM），並相互比較。我們在不同的異常偵測基準和受控實驗室設定中收集的資料上評估此方法。實驗結果顯示偵測準確度超過 0.975，優於最先進的 ET-NET 方法。

##### **XXLTraffic: Expanding and Extremely Long Traffic Dataset for Ultra-Dynamic Forecasting Challenges**
2406.12693v1 by Du Yin, Hao Xue, Arian Prabowo, Shuang Ao, Flora Salim

Traffic forecasting is crucial for smart cities and intelligent
transportation initiatives, where deep learning has made significant progress
in modeling complex spatio-temporal patterns in recent years. However, current
public datasets have limitations in reflecting the ultra-dynamic nature of
real-world scenarios, characterized by continuously evolving infrastructures,
varying temporal distributions, and temporal gaps due to sensor downtimes or
changes in traffic patterns. These limitations inevitably restrict the
practical applicability of existing traffic forecasting datasets. To bridge
this gap, we present XXLTraffic, the largest available public traffic dataset
with the longest timespan and increasing number of sensor nodes over the
multiple years observed in the data, curated to support research in
ultra-dynamic forecasting. Our benchmark includes both typical time-series
forecasting settings with hourly and daily aggregated data and novel
configurations that introduce gaps and down-sample the training size to better
simulate practical constraints. We anticipate the new XXLTraffic will provide a
fresh perspective for the time-series and traffic forecasting communities. It
would also offer a robust platform for developing and evaluating models
designed to tackle ultra-dynamic and extremely long forecasting problems. Our
dataset supplements existing spatio-temporal data resources and leads to new
research directions in this domain.

摘要：交通預測對於智慧城市和智慧交通計畫來說至關重要，深度學習在近年來於建模複雜時空模式方面取得顯著進展。然而，當前公開的資料集在反映真實世界情境的超動態本質上存在限制，其特徵在於持續演進的基礎建設、變動的時間分佈，以及由於感測器停機時間或交通模式改變所造成的時間間隔。這些限制不可避免地限制了現有交通預測資料集的實際適用性。為了彌合這個差距，我們提出了 XXLTraffic，這是目前最大的公開交通資料集，具有最長的時程和多年來資料中觀察到的感測器節點數量持續增加，策劃用於支援超動態預測的研究。我們的基準包括具有每小時和每日彙總資料的典型時間序列預測設定，以及引入間隔和向下取樣訓練大小的新配置，以更好地模擬實際限制。我們預期新的 XXLTraffic 將為時間序列和交通預測社群提供新的觀點。它還將提供一個強大的平台，用於開發和評估旨在解決超動態和極長預測問題的模型。我們的資料集補充了現有的時空資料資源，並導致了這個領域新的研究方向。

##### **MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL**
2406.12692v1 by Arian Askari, Christian Poelitz, Xinye Tang

Self-correction in text-to-SQL is the process of prompting large language
model (LLM) to revise its previously incorrectly generated SQL, and commonly
relies on manually crafted self-correction guidelines by human experts that are
not only labor-intensive to produce but also limited by the human ability in
identifying all potential error patterns in LLM responses. We introduce MAGIC,
a novel multi-agent method that automates the creation of the self-correction
guideline. MAGIC uses three specialized agents: a manager, a correction, and a
feedback agent. These agents collaborate on the failures of an LLM-based method
on the training set to iteratively generate and refine a self-correction
guideline tailored to LLM mistakes, mirroring human processes but without human
involvement. Our extensive experiments show that MAGIC's guideline outperforms
expert human's created ones. We empirically find out that the guideline
produced by MAGIC enhance the interpretability of the corrections made,
providing insights in analyzing the reason behind the failures and successes of
LLMs in self-correction. We make all agent interactions publicly available to
the research community, to foster further research in this area, offering a
synthetic dataset for future explorations into automatic self-correction
guideline generation.

摘要：文本轉 SQL 中的自我修正，是促使大型語言模型 (LLM) 修改其先前錯誤產生的 SQL 程序，且通常依賴人工專家手動製作的自我修正準則，不僅製作費時，也受限於人類辨識 LLM 回應中所有潛在錯誤模式的能力。我們引進 MAGIC，這是一種新穎的多重代理方法，能自動建立自我修正準則。MAGIC 使用三個專門代理：管理員、修正器和回饋代理。這些代理針對 LLM 基礎方法在訓練組中的失敗進行協作，反覆產生並改善針對 LLM 錯誤量身打造的自我修正準則，反映人類程序，但無需人類參與。我們廣泛的實驗顯示，MAGIC 的準則優於人工專家建立的準則。我們實證發現，MAGIC 產生的準則能提升修正的詮釋性，提供分析 LLM 在自我修正中失敗和成功的背後原因的見解。我們讓所有代理互動公開於研究社群，以促進此領域的進一步研究，提供一個合成資料集，供未來探索自動自我修正準則產生。

##### **Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia**
2406.12687v1 by Ankit Aich, Avery Quynh, Pamela Osseyi, Amy Pinkham, Philip Harvey, Brenda Curtis, Colin Depp, Natalie Parde

NLP in mental health has been primarily social media focused. Real world
practitioners also have high case loads and often domain specific variables, of
which modern LLMs lack context. We take a dataset made by recruiting 644
participants, including individuals diagnosed with Bipolar Disorder (BD),
Schizophrenia (SZ), and Healthy Controls (HC). Participants undertook tasks
derived from a standardized mental health instrument, and the resulting data
were transcribed and annotated by experts across five clinical variables. This
paper demonstrates the application of contemporary language models in
sequence-to-sequence tasks to enhance mental health research. Specifically, we
illustrate how these models can facilitate the deployment of mental health
instruments, data collection, and data annotation with high accuracy and
scalability. We show that small models are capable of annotation for
domain-specific clinical variables, data collection for mental-health
instruments, and perform better then commercial large models.

摘要：NLP 在心理健康领域主要关注社交媒体。现实世界中的从业者也有很高的病例量，并且经常有特定领域的变量，而现代 LLM 缺乏背景。我们采用了一个由 644 名参与者组成的数据集，包括被诊断患有双相情感障碍 (BD)、精神分裂症 (SZ) 和健康对照 (HC) 的个体。参与者执行了源自标准化心理健康工具的任务，由此产生的数据由专家在五个临床变量中进行转录和注释。本文展示了当代语言模型在序列到序列任务中的应用，以增强心理健康研究。具体来说，我们说明了这些模型如何促进心理健康工具的部署、数据收集和数据注释，并具有高准确性和可扩展性。我们表明，小型模型能够对特定领域的临床变量进行注释、收集心理健康工具的数据，并且比商用大型模型表现得更好。

##### **Measuring Psychological Depth in Language Models**
2406.12680v1 by Fabrice Harel-Canada, Hanyu Zhou, Sreya Mupalla, Zeynep Yildiz, Amit Sahai, Nanyun Peng

Evaluations of creative stories generated by large language models (LLMs)
often focus on objective properties of the text, such as its style, coherence,
and toxicity. While these metrics are indispensable, they do not speak to a
story's subjective, psychological impact from a reader's perspective. We
introduce the Psychological Depth Scale (PDS), a novel framework rooted in
literary theory that measures an LLM's ability to produce authentic and
narratively complex stories that provoke emotion, empathy, and engagement. We
empirically validate our framework by showing that humans can consistently
evaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore
techniques for automating the PDS to easily scale future analyses. GPT-4o,
combined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an
average Spearman correlation of $0.51$ with human judgment while Llama-3-70B
scores as high as 0.68 for empathy. Finally, we compared the depth of stories
authored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed
or were statistically indistinguishable from highly-rated human-written stories
sourced from Reddit. By shifting the focus from text to reader, the
Psychological Depth Scale is a validated, automated, and systematic means of
measuring the capacity of LLMs to connect with humans through the stories they
tell.

摘要：大型語言模型 (LLM) 所產生的創意故事評量，通常會專注於文本的客觀屬性，例如其風格、連貫性和毒性。雖然這些指標不可或缺，但它們並未說明從讀者的角度來看，故事的主觀心理影響。我們引入了心理深度量表 (PDS)，這是一個植基於文學理論的新穎架構，用於衡量 LLM 產生真實且敘事複雜的故事的能力，這些故事能引起情緒、同理心和投入感。我們透過展示人類能根據 PDS 持續評估故事（Krippendorff's alpha 為 0.72）來驗證我們的架構。我們也探討自動化 PDS 的技術，以便輕鬆擴充未來的分析。GPT-4o 結合新穎的混合角色 (MoP) 提示策略，在人類判斷中達到平均 0.51 的 Spearman 相關性，而 Llama-3-70B 的同理心分數高達 0.68。最後，我們比較了人類和 LLM 創作的故事深度。令人驚訝的是，GPT-4 的故事超越或在統計上與取自 Reddit 的高評分人類撰寫故事無異。透過將焦點從文本轉移到讀者，心理深度量表是一種經過驗證、自動化且系統化的方式，用於衡量 LLM 透過他們所述故事與人類建立聯繫的能力。

##### **Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping**
2406.12679v1 by Ankit Aich, Tingting Liu, Salvatore Giorgi, Kelsey Isman, Lyle Ungar, Brenda Curtis

Large Language Models (LLMs) are increasingly being used in educational and
learning applications. Research has demonstrated that controlling for style, to
fit the needs of the learner, fosters increased understanding, promotes
inclusion, and helps with knowledge distillation. To understand the
capabilities and limitations of contemporary LLMs in style control, we
evaluated five state-of-the-art models: GPT-3.5, GPT-4, GPT-4o, Llama-3, and
Mistral-instruct- 7B across two style control tasks. We observed significant
inconsistencies in the first task, with model performances averaging between
5th and 8th grade reading levels for tasks intended for first-graders, and
standard deviations up to 27.6. For our second task, we observed a
statistically significant improvement in performance from 0.02 to 0.26.
However, we find that even without stereotypes in reference texts, LLMs often
generated culturally insensitive content during their tasks. We provide a
thorough analysis and discussion of the results.

摘要：大型語言模型（LLM）正越來越廣泛地用於教育和學習應用程式中。研究表明，控制風格以符合學習者的需求，有助於促進理解、提升包容性，並協助知識提煉。為了了解當代 LLM 在風格控制方面的能力和限制，我們評估了五種最先進的模型：GPT-3.5、GPT-4、GPT-4o、Llama-3 和 Mistral-instruct-7B，涵蓋兩個風格控制任務。我們在第一個任務中觀察到顯著的不一致性，模型表現平均介於一年級學生任務的五年級和八年級閱讀程度之間，標準差高達 27.6。在我們的第二個任務中，我們觀察到表現從 0.02 顯著提升至 0.26。然而，我們發現即使參考文本中沒有刻板印象，LLM 在其任務中仍經常產生文化上不敏感的內容。我們提供了對結果的徹底分析和討論。

##### **Estimating Knowledge in Large Language Models Without Generating a Single Token**
2406.12673v1 by Daniela Gottesman, Mor Geva

To evaluate knowledge in large language models (LLMs), current methods query
the model and then evaluate its generated responses. In this work, we ask
whether evaluation can be done $\textit{before}$ the model has generated any
text. Concretely, is it possible to estimate how knowledgeable a model is about
a certain entity, only from its internal computation? We study this question
with two tasks: given a subject entity, the goal is to predict (a) the ability
of the model to answer common questions about the entity, and (b) the
factuality of responses generated by the model about the entity. Experiments
with a variety of LLMs show that KEEN, a simple probe trained over internal
subject representations, succeeds at both tasks - strongly correlating with
both the QA accuracy of the model per-subject and FActScore, a recent
factuality metric in open-ended generation. Moreover, KEEN naturally aligns
with the model's hedging behavior and faithfully reflects changes in the
model's knowledge after fine-tuning. Lastly, we show a more interpretable yet
equally performant variant of KEEN, which highlights a small set of tokens that
correlates with the model's lack of knowledge. Being simple and lightweight,
KEEN can be leveraged to identify gaps and clusters of entity knowledge in
LLMs, and guide decisions such as augmenting queries with retrieval.

摘要：<paragraph>為了評估大型語言模型 (LLM) 中的知識，目前的方法會查詢模型，然後評估其產生的回應。在這項工作中，我們詢問是否可以在模型產生任何文字$\textit{之前}$進行評估。具體來說，是否可以僅從模型的內部運算來估計模型對某個實體的了解程度？我們使用兩個任務來研究這個問題：給定一個主題實體，目標是預測 (a) 模型回答有關實體的常見問題的能力，以及 (b) 模型產生的有關實體的回應的事實性。使用各種 LLM 的實驗表明，KEEN 是一個訓練在內部主體表示上的簡單探針，它在兩個任務中都成功了 - 與模型的每個主體的問答準確度和 FActScore（開放式生成的最新事實性指標）密切相關。此外，KEEN 自然地與模型的避險行為保持一致，並忠實地反映了微調後模型知識的變化。最後，我們展示了 KEEN 一個更具可解釋性但性能相同的變體，它強調了一組與模型缺乏知識相關的小型代幣。KEEN 簡單且輕量級，可用於識別 LLM 中實體知識的差距和群集，並指導決策，例如使用擷取來擴充查詢。</paragraph>

##### **Sparsifying dimensionality reduction of PDE solution data with Bregman learning**
2406.12672v1 by Tjeerd Jan Heeringa, Christoph Brune, Mengwu Guo

Classical model reduction techniques project the governing equations onto a
linear subspace of the original state space. More recent data-driven techniques
use neural networks to enable nonlinear projections. Whilst those often enable
stronger compression, they may have redundant parameters and lead to suboptimal
latent dimensionality. To overcome these, we propose a multistep algorithm that
induces sparsity in the encoder-decoder networks for effective reduction in the
number of parameters and additional compression of the latent space. This
algorithm starts with sparsely initialized a network and training it using
linearized Bregman iterations. These iterations have been very successful in
computer vision and compressed sensing tasks, but have not yet been used for
reduced-order modelling. After the training, we further compress the latent
space dimensionality by using a form of proper orthogonal decomposition. Last,
we use a bias propagation technique to change the induced sparsity into an
effective reduction of parameters. We apply this algorithm to three
representative PDE models: 1D diffusion, 1D advection, and 2D
reaction-diffusion. Compared to conventional training methods like Adam, the
proposed method achieves similar accuracy with 30% less parameters and a
significantly smaller latent space.

摘要：經典模型簡約技術將控制方程式投影到原始狀態空間的線性子空間。最近的數據驅動技術使用神經網路來啟用非線性投影。雖然這些技術通常能實現更強的壓縮，但它們可能具有冗餘參數，並導致次優的潛在維度。為了克服這些問題，我們提出了一種多步驟演算法，它在編碼器-解碼器網路中引入了稀疏性，以有效減少參數數量並進一步壓縮潛在空間。此演算法從稀疏初始化網路開始，並使用線性化 Bregman 迭代進行訓練。這些迭代在電腦視覺和壓縮感測任務中非常成功，但尚未用於降階建模。在訓練之後，我們進一步壓縮潛在空間維度，方法是使用適當正交分解形式。最後，我們使用偏差傳播技術將誘導的稀疏性轉換為參數的有效減少。我們將此演算法應用於三個具代表性的偏微分方程模型：1D diffusion、1D advection 和 2D reaction-diffusion。與 Adam 等傳統訓練方法相比，所提出的方法以少 30% 的參數和顯著更小的潛在空間實現了類似的準確度。

##### **Stealth edits for provably fixing or attacking large language models**
2406.12670v1 by Oliver J. Sutton, Qinghua Zhou, Wei Wang, Desmond J. Higham, Alexander N. Gorban, Alexander Bastounis, Ivan Y. Tyukin

We reveal new methods and the theoretical foundations of techniques for
editing large language models. We also show how the new theory can be used to
assess the editability of models and to expose their susceptibility to
previously unknown malicious attacks. Our theoretical approach shows that a
single metric (a specific measure of the intrinsic dimensionality of the
model's features) is fundamental to predicting the success of popular editing
approaches, and reveals new bridges between disparate families of editing
methods. We collectively refer to these approaches as stealth editing methods,
because they aim to directly and inexpensively update a model's weights to
correct the model's responses to known hallucinating prompts without otherwise
affecting the model's behaviour, without requiring retraining. By carefully
applying the insight gleaned from our theoretical investigation, we are able to
introduce a new network block -- named a jet-pack block -- which is optimised
for highly selective model editing, uses only standard network operations, and
can be inserted into existing networks. The intrinsic dimensionality metric
also determines the vulnerability of a language model to a stealth attack: a
small change to a model's weights which changes its response to a single
attacker-chosen prompt. Stealth attacks do not require access to or knowledge
of the model's training data, therefore representing a potent yet previously
unrecognised threat to redistributed foundation models. They are
computationally simple enough to be implemented in malware in many cases.
Extensive experimental results illustrate and support the method and its
theoretical underpinnings. Demos and source code for editing language models
are available at https://github.com/qinghua-zhou/stealth-edits.

摘要：<paragraph>我們揭露了編輯大型語言模型的技術的新方法和理論基礎。我們也展示了新理論如何用於評估模型的可編輯性，並揭露它們對以前未知的惡意攻擊的敏感性。我們的理論方法表明，一個單一指標（模型特徵的內在維度的特定測量）對於預測流行編輯方法的成功至關重要，並揭示了不同編輯方法系列之間的新橋樑。我們將這些方法統稱為隱形編輯方法，因為它們旨在直接且低成本地更新模型的權重，以修正模型對已知幻覺提示的回應，而不會影響模型的行為，而無需重新訓練。通過仔細應用從我們的理論研究中獲得的見解，我們能夠引入一個新的網路區塊——稱為噴射背包區塊——它針對高度選擇性的模型編輯進行了優化，僅使用標準網路操作，並且可以插入到現有的網路中。內在維度指標也決定了語言模型對隱形攻擊的脆弱性：對模型權重的微小改變，它會改變模型對單個攻擊者選擇的提示的回應。隱形攻擊不需要訪問或了解模型的訓練資料，因此代表了對重新分發基礎模型的潛在但以前未被認識到的威脅。在許多情況下，它們在計算上足夠簡單，可以在惡意軟體中實現。廣泛的實驗結果說明並支持了該方法及其理論基礎。編輯語言模型的演示和源代碼可在 https://github.com/qinghua-zhou/stealth-edits 獲得。</paragraph>

##### **CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis**
2406.12665v1 by Saranya Venkatraman, Nafis Irtiza Tripto, Dongwon Lee

The rise of unifying frameworks that enable seamless interoperability of
Large Language Models (LLMs) has made LLM-LLM collaboration for open-ended
tasks a possibility. Despite this, there have not been efforts to explore such
collaborative writing. We take the next step beyond human-LLM collaboration to
explore this multi-LLM scenario by generating the first exclusively
LLM-generated collaborative stories dataset called CollabStory. We focus on
single-author ($N=1$) to multi-author (up to $N=5$) scenarios, where multiple
LLMs co-author stories. We generate over 32k stories using open-source
instruction-tuned LLMs. Further, we take inspiration from the PAN tasks that
have set the standard for human-human multi-author writing tasks and analysis.
We extend their authorship-related tasks for multi-LLM settings and present
baselines for LLM-LLM collaboration. We find that current baselines are not
able to handle this emerging scenario. Thus, CollabStory is a resource that
could help propel an understanding as well as the development of techniques to
discern the use of multiple LLMs. This is crucial to study in the context of
writing tasks since LLM-LLM collaboration could potentially overwhelm ongoing
challenges related to plagiarism detection, credit assignment, maintaining
academic integrity in educational settings, and addressing copyright
infringement concerns. We make our dataset and code available at
\texttt{\url{https://github.com/saranya-venkatraman/multi_llm_story_writing}}.

摘要：<paragraph>統一框架的興起，讓大型語言模型 (LLM) 能夠無縫地相互操作，這使得 LLM-LLM 協作成為開放式任務的可能性。儘管如此，尚未有探索這種協作寫作的努力。我們踏出人機協作的下一步，透過產生第一個專門由 LLM 生成的協作故事資料集 CollabStory 來探索這種多 LLM 的場景。我們專注於單一作者 ($N=1$) 到多作者 (最多 $N=5$) 的場景，其中多個 LLM 共同撰寫故事。我們使用開源指令微調的 LLM 產生超過 32k 個故事。此外，我們從 PAN 任務中汲取靈感，PAN 任務已為人機多作者寫作任務和分析設定了標準。我們擴展了他們的作者相關任務，以適用於多 LLM 設定，並為 LLM-LLM 協作提出基準。我們發現目前的基準無法處理這種新興的場景。因此，CollabStory 是一個資源，可以幫助推進對多個 LLM 使用的理解和技術開發。這對於寫作任務的研究至關重要，因為 LLM-LLM 協作可能會對剽竊偵測、學分分配、維護教育環境的學術誠信以及解決版權侵權問題等持續的挑戰造成影響。我們在 \texttt{\url{https://github.com/saranya-venkatraman/multi_llm_story_writing}} 提供我們的資料集和程式碼。</paragraph>

##### **Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?**
2406.12663v1 by Mingqian Feng, Yunlong Tang, Zeliang Zhang, Chenliang Xu

Large Vision-Language Models (LVLMs) excel in integrating visual and
linguistic contexts to produce detailed content, facilitating applications such
as image captioning. However, using LVLMs to generate descriptions often faces
the challenge of object hallucination (OH), where the output text misrepresents
actual objects in the input image. While previous studies attribute the
occurrence of OH to the inclusion of more details, our study finds technical
flaws in existing metrics, leading to unreliable evaluations of models and
conclusions about OH. This has sparked a debate on the question: Do more
details always introduce more hallucinations in LVLM-based image captioning?
  In this paper, we address this debate by proposing a novel decoding strategy,
Differentiated Beam Decoding (DBD), along with a reliable new set of evaluation
metrics: CLIP-Precision, CLIP-Recall, and CLIP-F1. DBD decodes the wealth of
information hidden in visual input into distinct language representations
called unit facts in parallel. This decoding is achieved via a well-designed
differential score that guides the parallel search and candidate screening. The
selected unit facts are then aggregated to generate the final caption. Our
proposed metrics evaluate the comprehensiveness and accuracy of image captions
by comparing the embedding groups of ground-truth image regions and generated
text partitions. Extensive experiments on the Visual Genome dataset validate
the effectiveness of our approach, demonstrating that it produces detailed
descriptions while maintaining low hallucination levels.

摘要：大型視覺語言模型 (LVLMs) 擅長整合視覺和語言脈絡以產生詳細內容，促進影像標題等應用程式。然而，使用 LVLMs 產生描述時，常會面臨物件幻覺 (OH) 的挑戰，其中輸出文字錯誤呈現輸入影像中的實際物件。儘管先前的研究將 OH 的發生歸因於包含更多細節，但我們的研究發現現有指標存在技術缺陷，導致模型評估和關於 OH 的結論不可靠。這引發了一場辯論：在基於 LVLM 的影像標題中，更多細節是否總是會引入更多幻覺？在本文中，我們透過提出一個新穎的解碼策略，即差異化波束解碼 (DBD)，以及一組可靠的新評估指標：CLIP 精確度、CLIP 召回率和 CLIP-F1 來解決這場辯論。DBD 將隱藏在視覺輸入中的大量資訊解碼成不同的語言表示，稱為單元事實。這種解碼是透過精心設計的差異分數來實現的，該分數引導並行搜尋和候選篩選。然後將選定的單元事實匯總起來以產生最終標題。我們提出的指標透過比較真實影像區域和生成文字分區的嵌入群組來評估影像標題的全面性和準確性。在 Visual Genome 資料集上進行的廣泛實驗驗證了我們方法的有效性，證明它可以在維持低幻覺水準的同時產生詳細的描述。

##### **Investigating the Role of Explainability and AI Literacy in User Compliance**
2406.12660v1 by Niklas Kühl, Christian Meske, Maximilian Nitsche, Jodie Lobana

AI is becoming increasingly common across different domains. However, as
sophisticated AI-based systems are often black-boxed, rendering the
decision-making logic opaque, users find it challenging to comply with their
recommendations. Although researchers are investigating Explainable AI (XAI) to
increase the transparency of the underlying machine learning models, it is
unclear what types of explanations are effective and what other factors
increase compliance. To better understand the interplay of these factors, we
conducted an experiment with 562 participants who were presented with the
recommendations of an AI and two different types of XAI. We find that users'
compliance increases with the introduction of XAI but is also affected by AI
literacy. We also find that the relationships between AI literacy XAI and
users' compliance are mediated by the users' mental model of AI. Our study has
several implications for successfully designing AI-based systems utilizing XAI.

摘要：隨著 AI 在不同領域變得越來越普遍。然而，由於複雜的 AI 系統通常是黑箱作業，使得決策邏輯不透明，使用者發現很難遵守建議。儘管研究人員正在研究可解釋 AI (XAI) 以提高基礎機器學習模型的透明度，但目前尚不清楚哪些類型的解釋有效，以及哪些其他因素會提高相容性。為了更好地了解這些因素之間的交互作用，我們對 562 名參與者進行了一項實驗，這些參與者被提供了 AI 的建議和兩種不同類型的 XAI。我們發現使用者的相容性隨著 XAI 的引入而增加，但也會受到 AI 素養的影響。我們還發現 AI 素養 XAI 與使用者相容性之間的關係是由使用者對 AI 的心智模型所調節的。我們的研究對成功設計利用 XAI 的 AI 系統有幾個啟示。

##### **Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review**
2406.12655v1 by Debalina Ghosh Paul, Hong Zhu, Ian Bayley

With the rapid development of Large Language Models (LLMs), a large number of
machine learning models have been developed to assist programming tasks
including the generation of program code from natural language input. However,
how to evaluate such LLMs for this task is still an open problem despite of the
great amount of research efforts that have been made and reported to evaluate
and compare them. This paper provides a critical review of the existing work on
the testing and evaluation of these tools with a focus on two key aspects: the
benchmarks and the metrics used in the evaluations. Based on the review,
further research directions are discussed.

摘要：隨著大型語言模型 (LLM) 的快速發展，已開發出大量機器學習模型來協助程式設計任務，包括從自然語言輸入產生程式碼。然而，如何評估此類 LLM 仍是一個開放性問題，儘管已投入大量研究工作並據以評估和比較它們。本文對這些工具的測試和評估現有工作進行了批判性回顧，重點關注兩個關鍵方面：評估中使用的基準和指標。根據回顧，討論了進一步的研究方向。

##### **Transforming Surgical Interventions with Embodied Intelligence for Ultrasound Robotics**
2406.12651v1 by Huan Xu, Jinlin Wu, Guanglin Cao, Zhen Chen, Zhen Lei, Hongbin Liu

Ultrasonography has revolutionized non-invasive diagnostic methodologies,
significantly enhancing patient outcomes across various medical domains.
Despite its advancements, integrating ultrasound technology with robotic
systems for automated scans presents challenges, including limited command
understanding and dynamic execution capabilities. To address these challenges,
this paper introduces a novel Ultrasound Embodied Intelligence system that
synergistically combines ultrasound robots with large language models (LLMs)
and domain-specific knowledge augmentation, enhancing ultrasound robots'
intelligence and operational efficiency. Our approach employs a dual strategy:
firstly, integrating LLMs with ultrasound robots to interpret doctors' verbal
instructions into precise motion planning through a comprehensive understanding
of ultrasound domain knowledge, including APIs and operational manuals;
secondly, incorporating a dynamic execution mechanism, allowing for real-time
adjustments to scanning plans based on patient movements or procedural errors.
We demonstrate the effectiveness of our system through extensive experiments,
including ablation studies and comparisons across various models, showcasing
significant improvements in executing medical procedures from verbal commands.
Our findings suggest that the proposed system improves the efficiency and
quality of ultrasound scans and paves the way for further advancements in
autonomous medical scanning technologies, with the potential to transform
non-invasive diagnostics and streamline medical workflows.

摘要：超音波徹底改變了非侵入性診斷方法，大幅提升各種醫療領域的患者治療成果。儘管有這些進展，但將超音波技術與機器人系統整合以進行自動化掃描會產生挑戰，包括有限的指令理解和動態執行能力。為了應對這些挑戰，本文介紹了一種創新的超音波具身智慧系統，該系統將超音波機器人與大型語言模型 (LLM) 和特定領域的知識擴充結合在一起，增強超音波機器人的智慧和操作效率。我們的做法採用雙重策略：首先，將 LLM 與超音波機器人整合，透過全面理解超音波領域知識（包括 API 和操作手冊），將醫生的口頭指示轉換為精確的動作規劃；其次，加入動態執行機制，允許根據患者移動或程序錯誤即時調整掃描計畫。我們透過廣泛的實驗（包括消融研究和各種模型的比較）證明了我們系統的有效性，展示了根據口頭指令執行醫療程序的顯著進步。我們的研究結果表明，所提出的系統改善了超音波掃描的效率和品質，並為自主醫療掃描技術的進一步發展鋪路，有潛力轉型非侵入性診斷並簡化醫療工作流程。

##### **Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models**
2406.12649v1 by Hengyi Wang, Shiwei Tan, Hao Wang

Vision transformers (ViTs) have emerged as a significant area of focus,
particularly for their capacity to be jointly trained with large language
models and to serve as robust vision foundation models. Yet, the development of
trustworthy explanation methods for ViTs has lagged, particularly in the
context of post-hoc interpretations of ViT predictions. Existing sub-image
selection approaches, such as feature-attribution and conceptual models, fall
short in this regard. This paper proposes five desiderata for explaining ViTs
-- faithfulness, stability, sparsity, multi-level structure, and parsimony --
and demonstrates the inadequacy of current methods in meeting these criteria
comprehensively. We introduce a variational Bayesian explanation framework,
dubbed ProbAbilistic Concept Explainers (PACE), which models the distributions
of patch embeddings to provide trustworthy post-hoc conceptual explanations.
Our qualitative analysis reveals the distributions of patch-level concepts,
elucidating the effectiveness of ViTs by modeling the joint distribution of
patch embeddings and ViT's predictions. Moreover, these patch-level
explanations bridge the gap between image-level and dataset-level explanations,
thus completing the multi-level structure of PACE. Through extensive
experiments on both synthetic and real-world datasets, we demonstrate that PACE
surpasses state-of-the-art methods in terms of the defined desiderata.

摘要：視覺Transformer (ViT) 已成為一個重要的關注領域，
特別是它們與大型語言模型聯合訓練並作為強大的視覺基礎模型的能力。然而，
可信的 ViT 解釋方法的開發落後了，特別是在 ViT 預測的後設解釋的背景下。現有的子影像
選擇方法，例如特徵歸因和概念模型，在這方面做得並不好。本文提出了解釋 ViT 的五個理想條件
——忠實性、穩定性、稀疏性、多層級結構和簡潔性——並證明了當前方法在全面滿足這些標準方面的不足。我們引入了一個變分貝氏解釋框架，
稱為概率概念解釋器 (PACE)，它對 patch 嵌入的分布進行建模，以提供可信的後設概念解釋。
我們的定性分析揭示了 patch 級別概念的分布，通過對 patch 嵌入和 ViT 預測的聯合分布進行建模，闡明了 ViT 的有效性。此外，這些 patch 級別
解釋彌合了影像級別和資料集級別解釋之間的差距，從而完成了 PACE 的多層級結構。通過對合成和真實世界資料集進行廣泛的
實驗，我們證明了 PACE 在定義的理想條件方面超越了最先進的方法。

##### **An Empirical Study on the Fairness of Foundation Models for Multi-Organ Image Segmentation**
2406.12646v1 by Qin Li, Yizhe Zhang, Yan Li, Jun Lyu, Meng Liu, Longyu Sun, Mengting Sun, Qirong Li, Wenyue Mao, Xinran Wu, Yajing Zhang, Yinghua Chu, Shuo Wang, Chengyan Wang

The segmentation foundation model, e.g., Segment Anything Model (SAM), has
attracted increasing interest in the medical image community. Early pioneering
studies primarily concentrated on assessing and improving SAM's performance
from the perspectives of overall accuracy and efficiency, yet little attention
was given to the fairness considerations. This oversight raises questions about
the potential for performance biases that could mirror those found in
task-specific deep learning models like nnU-Net. In this paper, we explored the
fairness dilemma concerning large segmentation foundation models. We
prospectively curate a benchmark dataset of 3D MRI and CT scans of the organs
including liver, kidney, spleen, lung and aorta from a total of 1056 healthy
subjects with expert segmentations. Crucially, we document demographic details
such as gender, age, and body mass index (BMI) for each subject to facilitate a
nuanced fairness analysis. We test state-of-the-art foundation models for
medical image segmentation, including the original SAM, medical SAM and SAT
models, to evaluate segmentation efficacy across different demographic groups
and identify disparities. Our comprehensive analysis, which accounts for
various confounding factors, reveals significant fairness concerns within these
foundational models. Moreover, our findings highlight not only disparities in
overall segmentation metrics, such as the Dice Similarity Coefficient but also
significant variations in the spatial distribution of segmentation errors,
offering empirical evidence of the nuanced challenges in ensuring fairness in
medical image segmentation.

摘要：例如，任何分割模型（SAM）等分割基础模型在医学影像社群中已引起越来越多的兴趣。早期开创性研究主要集中于从整体准确性和效率的角度评估和改进 SAM 的性能，但很少关注公平性考量。这种疏忽引起了人们对性能偏差的质疑，这些偏差可能反映在 nnU-Net 等特定任务深度学习模型中发现的偏差。在本文中，我们探讨了与大型分割基础模型有关的公平性困境。我们前瞻性地整理了一个基准数据集，其中包含来自 1056 名健康受试者的器官（包括肝脏、肾脏、脾脏、肺和主动脉）的 3D MRI 和 CT 扫描，并由专家进行分割。至关重要的是，我们记录了每个受试者的性别、年龄和体重指数 (BMI) 等人口统计详细信息，以促进细致入微的公平性分析。我们测试了用于医学图像分割的最新基础模型，包括原始 SAM、医学 SAM 和 SAT 模型，以评估不同人口群体的分割效果并找出差异。我们的综合分析考虑了各种混杂因素，揭示了这些基础模型中存在的重大公平性问题。此外，我们的研究结果不仅突出了整体分割指标（例如骰子相似系数）的差异，还突出了分割错误的空间分布的显着差异，为确保医学图像分割中的公平性提供了细微挑战的经验证据。

##### **Evaluating Transparency of Machine Generated Fact Checking Explanations**
2406.12645v1 by Rui Xing, Timothy Baldwin, Jey Han Lau

An important factor when it comes to generating fact-checking explanations is
the selection of evidence: intuitively, high-quality explanations can only be
generated given the right evidence. In this work, we investigate the impact of
human-curated vs. machine-selected evidence for explanation generation using
large language models. To assess the quality of explanations, we focus on
transparency (whether an explanation cites sources properly) and utility
(whether an explanation is helpful in clarifying a claim). Surprisingly, we
found that large language models generate similar or higher quality
explanations using machine-selected evidence, suggesting carefully curated
evidence (by humans) may not be necessary. That said, even with the best model,
the generated explanations are not always faithful to the sources, suggesting
further room for improvement in explanation generation for fact-checking.

摘要：在生成查核解釋時，一個重要的因素是證據的選擇：直覺上，只有在給定正確證據的情況下，才能產生高品質的解釋。在這項工作中，我們探討了人工策劃與機器選擇的證據對使用大型語言模型進行解釋產生的影響。為了評估解釋的品質，我們專注於透明度（解釋是否正確引用來源）和效用（解釋是否有助於釐清主張）。令人驚訝的是，我們發現大型語言模型使用機器選擇的證據產生了相似或更高的品質解釋，這表明仔細策劃的證據（由人類）可能並非必要。話雖如此，即使使用最好的模型，產生的解釋也不總是忠實於來源，這表明查核解釋的生成仍有進步空間。

##### **Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models**
2406.12644v1 by Devichand Budagam, Sankalp KJ, Ashutosh Kumar, Vinija Jain, Aman Chadha

Assessing the effectiveness of large language models (LLMs) in addressing
diverse tasks is essential for comprehending their strengths and weaknesses.
Conventional evaluation techniques typically apply a single prompting strategy
uniformly across datasets, not considering the varying degrees of task
complexity. We introduce the Hierarchical Prompting Taxonomy (HPT), a taxonomy
that employs a Hierarchical Prompt Framework (HPF) composed of five unique
prompting strategies, arranged from the simplest to the most complex, to assess
LLMs more precisely and to offer a clearer perspective. This taxonomy assigns a
score, called the Hierarchical Prompting Score (HP-Score), to datasets as well
as LLMs based on the rules of the taxonomy, providing a nuanced understanding
of their ability to solve diverse tasks and offering a universal measure of
task complexity. Additionally, we introduce the Adaptive Hierarchical Prompt
framework, which automates the selection of appropriate prompting strategies
for each task. This study compares manual and adaptive hierarchical prompt
frameworks using four instruction-tuned LLMs, namely Llama 3 8B, Phi 3 3.8B,
Mistral 7B, and Gemma 7B, across four datasets: BoolQ, CommonSenseQA (CSQA),
IWSLT-2017 en-fr (IWSLT), and SamSum. Experiments demonstrate the effectiveness
of HPT, providing a reliable way to compare different tasks and LLM
capabilities. This paper leads to the development of a universal evaluation
metric that can be used to evaluate both the complexity of the datasets and the
capabilities of LLMs. The implementation of both manual HPF and adaptive HPF is
publicly available.

摘要：<paragraph>評估大型語言模型 (LLM) 在處理不同任務中的有效性對於理解其優缺點至關重要。傳統評估技術通常在所有資料集上統一應用單一提示策略，而不考慮任務複雜度的不同程度。我們引入了分層提示分類法 (HPT)，這是一種分類法，採用由五種獨特提示策略組成的分層提示框架 (HPF)，從最簡單到最複雜，以更精確地評估 LLM 並提供更清晰的觀點。此分類法根據分類法的規則，為資料集和 LLM 分配一個稱為分層提示分數 (HP 分數) 的分數，提供對其解決不同任務的能力的細緻理解，並提供任務複雜度的通用衡量標準。此外，我們引入了自適應分層提示框架，它自動化了為每個任務選擇適當提示策略的過程。本研究使用四個指令調整的 LLM（即 Llama 3 8B、Phi 3 3.8B、Mistral 7B 和 Gemma 7B）在四個資料集（BoolQ、常識問答 (CSQA)、IWSLT-2017 en-fr (IWSLT) 和 SamSum）上比較了手動和自適應分層提示框架。實驗證明了 HPT 的有效性，提供了一種比較不同任務和 LLM 能力的可靠方法。本文導致開發了一個通用評估指標，可用於評估資料集的複雜性和 LLM 的能力。手動 HPF 和自適應 HPF 的實現已公開。</paragraph>

##### **DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?**
2406.12641v1 by Zhouhong Gu, Lin Zhang, Xiaoxuan Zhu, Jiangjie Chen, Wenhao Huang, Yikai Zhang, Shusen Wang, Zheyu Ye, Yan Gao, Hongwei Feng, Yanghua Xiao

Detecting evidence within the context is a key step in the process of
reasoning task. Evaluating and enhancing the capabilities of LLMs in evidence
detection will strengthen context-based reasoning performance. This paper
proposes a benchmark called DetectBench for verifying the ability to detect and
piece together implicit evidence within a long context. DetectBench contains
3,928 multiple-choice questions, with an average of 994 tokens per question.
Each question contains an average of 4.55 pieces of implicit evidence, and
solving the problem typically requires 7.62 logical jumps to find the correct
answer. To enhance the performance of LLMs in evidence detection, this paper
proposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that
the existing LLMs' abilities to detect evidence in long contexts are far
inferior to humans. However, the Detective Reasoning Prompt effectively
enhances the capability of powerful LLMs in evidence detection, while the
Finetuning method shows significant effects in enhancing the performance of
weaker LLMs. Moreover, when the abilities of LLMs in evidence detection are
improved, their final reasoning performance is also enhanced accordingly.

摘要：在推理任务中，在语境中检测证据是关键的一步。评估和增强 LLM 在证据检测中的能力将强化基于语境的推理性能。本文提出了一个名为 DetectBench 的基准，用于验证在长语境中检测和拼凑隐式证据的能力。DetectBench 包含 3,928 个多项选择题，每个问题平均有 994 个标记。每个问题平均包含 4.55 个隐式证据，解决问题通常需要 7.62 个逻辑跳跃才能找到正确答案。为了增强 LLM 在证据检测中的性能，本文提出了侦探推理提示和微调。实验表明，现有 LLM 在长语境中检测证据的能力远逊于人类。然而，侦探推理提示有效地增强了强大的 LLM 在证据检测中的能力，而微调方法在增强较弱 LLM 的性能方面显示出显着效果。此外，当 LLM 在证据检测中的能力得到提高时，其最终推理性能也会相应提高。

##### **Ask-before-Plan: Proactive Language Agents for Real-World Planning**
2406.12639v1 by Xuan Zhang, Yang Deng, Zifeng Ren, See-Kiong Ng, Tat-Seng Chua

The evolution of large language models (LLMs) has enhanced the planning
capabilities of language agents in diverse real-world scenarios. Despite these
advancements, the potential of LLM-powered agents to comprehend ambiguous user
instructions for reasoning and decision-making is still under exploration. In
this work, we introduce a new task, Proactive Agent Planning, which requires
language agents to predict clarification needs based on user-agent conversation
and agent-environment interaction, invoke external tools to collect valid
information, and generate a plan to fulfill the user's demands. To study this
practical problem, we establish a new benchmark dataset, Ask-before-Plan. To
tackle the deficiency of LLMs in proactive planning, we propose a novel
multi-agent framework, Clarification-Execution-Planning (\texttt{CEP}), which
consists of three agents specialized in clarification, execution, and planning.
We introduce the trajectory tuning scheme for the clarification agent and
static execution agent, as well as the memory recollection mechanism for the
dynamic execution agent. Extensive evaluations and comprehensive analyses
conducted on the Ask-before-Plan dataset validate the effectiveness of our
proposed framework.

摘要：大型語言模型 (LLM) 的演進提升了語言代理在各種真實世界場景中的規劃能力。儘管有這些進展，由 LLM 驅動的代理理解含糊不清的使用者指令以進行推理和決策的潛力仍處於探索階段。在這項工作中，我們引入了一項新任務，主動代理規劃，它要求語言代理根據使用者代理對話和代理環境互動來預測澄清需求，調用外部工具來收集有效資訊，並制定計畫以滿足使用者的需求。為了研究這個實際問題，我們建立了一個新的基準資料集，在計畫之前詢問。為了解決 LLM 在主動規劃中的不足，我們提出了一個新穎的多代理架構，澄清執行規劃 (\texttt{CEP})，它包含三個專門負責澄清、執行和規劃的代理。我們為澄清代理和靜態執行代理引入了軌跡調整方案，以及為動態執行代理引入了記憶回憶機制。在在計畫之前詢問資料集上進行的廣泛評估和綜合分析驗證了我們提出的架構的有效性。

##### **ScenEval: A Benchmark for Scenario-Based Evaluation of Code Generation**
2406.12635v1 by Debalina Ghosh Paul, Hong Zhu, Ian Bayley

In the scenario-based evaluation of machine learning models, a key problem is
how to construct test datasets that represent various scenarios. The
methodology proposed in this paper is to construct a benchmark and attach
metadata to each test case. Then a test system can be constructed with test
morphisms that filter the test cases based on metadata to form a dataset.
  The paper demonstrates this methodology with large language models for code
generation. A benchmark called ScenEval is constructed from problems in
textbooks, an online tutorial website and Stack Overflow. Filtering by scenario
is demonstrated and the test sets are used to evaluate ChatGPT for Java code
generation.
  Our experiments found that the performance of ChatGPT decreases with the
complexity of the coding task. It is weakest for advanced topics like
multi-threading, data structure algorithms and recursive methods. The Java code
generated by ChatGPT tends to be much shorter than reference solution in terms
of number of lines, while it is more likely to be more complex in both
cyclomatic and cognitive complexity metrics, if the generated code is correct.
However, the generated code is more likely to be less complex than the
reference solution if the code is incorrect.

摘要：在基於情境的機器學習模型評估中，一個關鍵問題是如何建構代表各種情境的測試資料集。本論文提出的方法是建構一個基準，並將元資料附加到每個測試案例。然後，可以建構一個測試系統，其中包含測試態射，這些態射會根據元資料過濾測試案例以形成資料集。本文展示了此方法與用於產生程式碼的大型語言模型。一個稱為 ScenEval 的基準是從教科書、線上教學網站和 Stack Overflow 中的問題建構的。示範了按情境過濾，並使用測試集來評估 ChatGPT 的 Java 程式碼產生。我們的實驗發現，ChatGPT 的效能會隨著編碼任務的複雜性而降低。對於多執行緒、資料結構演算法和遞迴方法等進階主題，它的表現最弱。ChatGPT 產生的 Java 程式碼在行數方面往往比參考解答短很多，但如果產生的程式碼是正確的，則在迴圈複雜度和認知複雜度指標方面更有可能更複雜。然而，如果程式碼不正確，則產生的程式碼很可能比參考解答不那麼複雜。

##### **News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation**
2406.12634v1 by Andreea Iana, Fabian David Schmidt, Goran Glavaš, Heiko Paulheim

Rapidly growing numbers of multilingual news consumers pose an increasing
challenge to news recommender systems in terms of providing customized
recommendations. First, existing neural news recommenders, even when powered by
multilingual language models (LMs), suffer substantial performance losses in
zero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of
fine-tuning the backbone LM of a neural recommender on task-specific data is
computationally expensive and infeasible in few-shot recommendation and
cold-start setups, where data is scarce or completely unavailable. In this
work, we propose a news-adapted sentence encoder (NaSE), domain-specialized
from a pretrained massively multilingual sentence encoder (SE). To this end, we
construct and leverage PolyNews and PolyNewsParallel, two multilingual
news-specific corpora. With the news-adapted multilingual SE in place, we test
the effectiveness of (i.e., question the need for) supervised fine-tuning for
news recommendation, and propose a simple and strong baseline based on (i)
frozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE
achieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot
news recommendation.

摘要：隨著多語言新聞消費者數量快速增長，對新聞推薦系統在提供客製化推薦方面提出了越來越大的挑戰。首先，現有的神經新聞推薦系統，即使由多語言語言模型 (LM) 支援，在零次學習跨語言轉移 (ZS-XLT) 中也會遭受顯著的效能損失。其次，在神經推薦系統上微調主幹 LM 以執行特定任務資料的現行模式在次數學習推薦和冷啟動設定中需要大量運算，而且不可行，因為資料稀少或完全不可用。在這項工作中，我們提出一個新聞適應句子編碼器 (NaSE)，從預先訓練的大規模多語言句子編碼器 (SE) 中進行領域專門化。為此，我們建構並利用 PolyNews 和 PolyNewsParallel 這兩個多語言新聞特定語料庫。有了新聞適應的多語言 SE，我們測試監督微調對新聞推薦的有效性（即質疑其必要性），並提出一個基於 (i) 凍結 NaSE 嵌入和 (ii) 晚期點擊行為融合的簡單且強大的基準。我們證明 NaSE 在真正的冷啟動和次數學習新聞推薦中實現了 ZS-XLT 中的最新效能。

##### **SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation**
2406.12629v1 by Yixia Li, Boya Xiong, Guanhua Chen, Yun Chen

Out-of-distribution (OOD) detection is crucial for the safe deployment of
neural networks. Existing CLIP-based approaches perform OOD detection by
devising novel scoring functions or sophisticated fine-tuning methods. In this
work, we propose SeTAR, a novel, training-free OOD detection method that
leverages selective low-rank approximation of weight matrices in
vision-language and vision-only models. SeTAR enhances OOD detection via
post-hoc modification of the model's weight matrices using a simple greedy
search algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning
extension optimizing model performance for OOD detection tasks. Extensive
evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior
performance, reducing the false positive rate by up to 18.95% and 36.80%
compared to zero-shot and fine-tuning baselines. Ablation studies further
validate our approach's effectiveness, robustness, and generalizability across
different model backbones. Our work offers a scalable, efficient solution for
OOD detection, setting a new state-of-the-art in this area.

摘要：異配分 (OOD) 偵測對於神經網路的安全部署至關重要。現有的基於 CLIP 的方法透過設計新穎的評分函數或複雜的微調方法來執行 OOD 偵測。在這項工作中，我們提出 SeTAR，這是一種新穎的、免訓練的 OOD 偵測方法，它利用了視覺語言和純視覺模型中權重矩陣的選擇性低秩近似。SeTAR 透過使用簡單的貪婪搜尋演算法對模型的權重矩陣進行事後修改來增強 OOD 偵測。基於 SeTAR，我們進一步提出 SeTAR+FT，這是一種微調擴充，用於最佳化模型在 OOD 偵測任務中的效能。在 ImageNet1K 和 Pascal-VOC 基準上的廣泛評估顯示了 SeTAR 的優異效能，與零次學習和微調基準線相比，將假陽性率降低了 18.95% 和 36.80%。消融研究進一步驗證了我們的方法在不同模型主幹上的有效性、穩健性和概括性。我們的研究提供了一個可擴充、高效的 OOD 偵測解決方案，為這個領域設定了一個新的最先進水準。

##### **Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges**
2406.12624v1 by Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes

Offering a promising solution to the scalability challenges associated with
human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an
approach to evaluating large language models (LLMs). However, there are still
many open questions about the strengths and weaknesses of this paradigm, and
what potential biases it may hold. In this paper, we present a comprehensive
study of the performance of various LLMs acting as judges. We leverage TriviaQA
as a benchmark for assessing objective knowledge reasoning of LLMs and evaluate
them alongside human annotations which we found to have a high inter-annotator
agreement. Our study includes 9 judge models and 9 exam taker models -- both
base and instruction-tuned. We assess the judge model's alignment across
different model sizes, families, and judge prompts. Among other results, our
research rediscovers the importance of using Cohen's kappa as a metric of
alignment as opposed to simple percent agreement, showing that judges with high
percent agreement can still assign vastly different scores. We find that both
Llama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but in
terms of ranking exam taker models, they are outperformed by both JudgeLM-7B
and the lexical judge Contains, which have up to 34 points lower human
alignment. Through error analysis and various other studies, including the
effects of instruction length and leniency bias, we hope to provide valuable
lessons for using LLMs as judges in the future.

摘要：針對與人工評量相關的可擴充性挑戰提供有前景的解決方案，LLM 作為評審典範正迅速獲得關注，作為評量大型語言模型 (LLM) 的方法。然而，關於此典範的優缺點以及潛在偏誤，仍有許多問題尚未解決。在此論文中，我們提出了一項針對各種擔任評審角色的 LLM 的效能進行全面研究。我們利用 TriviaQA 作為評量 LLM 客觀知識推理的基準，並根據我們發現具有高度標註者間一致性的標註，對其進行評量。我們的研究包含 9 個評審模型和 9 個受試者模型，包含基礎模型和經過指令微調的模型。我們評估評審模型在不同模型大小、模型類型和評審提示下的對齊度。在其他結果中，我們的研究重新發現使用 Cohen's kappa 作為對齊度量度的重要性，而不是使用簡單的百分比一致性，顯示評審具有高百分比一致性，但仍可能分配出差異極大的分數。我們發現 Llama-3 70B 和 GPT-4 Turbo 與人類的對齊度都很優秀，但就對受試者模型進行排名而言，JudgeLM-7B 和詞彙評審 Contains 的表現都比它們好，其人類對齊度低達 34 個百分點。透過錯誤分析和各種其他研究，包括指令長度和寬容度偏差的影響，我們希望為未來使用 LLM 作為評審提供寶貴的經驗。

##### **What makes two models think alike?**
2406.12620v1 by Jeanne Salle, Louis Jalouzot, Nur Lan, Emmanuel Chemla, Yair Lakretz

Do architectural differences significantly affect the way models represent
and process language? We propose a new approach, based on metric-learning
encoding models (MLEMs), as a first step to answer this question. The approach
provides a feature-based comparison of how any two layers of any two models
represent linguistic information. We apply the method to BERT, GPT-2 and Mamba.
Unlike previous methods, MLEMs offer a transparent comparison, by identifying
the specific linguistic features responsible for similarities and differences.
More generally, the method uses formal, symbolic descriptions of a domain, and
use these to compare neural representations. As such, the approach can
straightforwardly be extended to other domains, such as speech and vision, and
to other neural systems, including human brains.

摘要：架構差異是否會顯著影響模型呈現和處理語言的方式？我們提出一個新的方法，以度量學習編碼模型 (MLEM) 為基礎，作為回答這個問題的第一步。此方法提供一個基於特徵的比較，說明任何兩個模型的任意兩層如何呈現語言資訊。我們將此方法應用於 BERT、GPT-2 和 Mamba。與先前的模型不同，MLEM 透過找出對相似性和差異性負責任的特定語言特徵，提供透明的比較。更普遍來說，此方法使用正式的、象徵性的領域說明，並使用這些說明來比較神經表徵。因此，此方法可以很直接地擴展到其他領域，例如語音和視覺，以及其他神經系統，包括人腦。

##### **EUvsDisinfo: a Dataset for Multilingual Detection of Pro-Kremlin Disinformation in News Articles**
2406.12614v1 by João A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, Carolina Scarton

This work introduces EUvsDisinfo, a multilingual dataset of trustworthy and
disinformation articles related to pro-Kremlin themes. It is sourced directly
from the debunk articles written by experts leading the EUvsDisinfo project.
Our dataset is the largest to-date resource in terms of the overall number of
articles and distinct languages. It also provides the largest topical and
temporal coverage. Using this dataset, we investigate the dissemination of
pro-Kremlin disinformation across different languages, uncovering
language-specific patterns targeting specific disinformation topics. We further
analyse the evolution of topic distribution over an eight-year period, noting a
significant surge in disinformation content before the full-scale invasion of
Ukraine in 2022. Lastly, we demonstrate the dataset's applicability in training
models to effectively distinguish between disinformation and trustworthy
content in multilingual settings.

摘要：本研究介紹 EUvsDisinfo，這是一個與親克里姆林宮主題相關的可信和錯誤資訊文章的多語言資料集。它直接來自由領導 EUvsDisinfo 專案的專家撰寫的揭穿文章。我們的資料集是迄今為止在文章總數和不同語言方面最大的資源。它還提供了最大的主題和時間範圍。使用此資料集，我們調查了親克里姆林宮錯誤資訊在不同語言中的傳播，揭示了針對特定錯誤資訊主題的特定語言模式。我們進一步分析了八年來主題分佈的演變，注意到在 2022 年全面入侵烏克蘭之前，錯誤資訊內容大幅激增。最後，我們展示了該資料集在訓練模型以有效區分多語言環境中的錯誤資訊和可信內容方面的適用性。

##### **Rapid Language Adaptation for Multilingual E2E Speech Recognition Using Encoder Prompting**
2406.12611v1 by Yosuke Kashiwagi, Hayato Futami, Emiru Tsunoo, Siddhant Arora, Shinji Watanabe

End-to-end multilingual speech recognition models handle multiple languages
through a single model, often incorporating language identification to
automatically detect the language of incoming speech. Since the common scenario
is where the language is already known, these models can perform as
language-specific by using language information as prompts, which is
particularly beneficial for attention-based encoder-decoder architectures.
However, the Connectionist Temporal Classification (CTC) approach, which
enhances recognition via joint decoding and multi-task training, does not
normally incorporate language prompts due to its conditionally independent
output tokens. To overcome this, we introduce an encoder prompting technique
within the self-conditioned CTC framework, enabling language-specific
adaptation of the CTC model in a zero-shot manner. Our method has shown to
significantly reduce errors by 28% on average and by 41% on low-resource
languages.

摘要：端到端多語言語音辨識模型透過單一模型處理多種語言，通常會結合語言辨識來自動偵測輸入語音的語言。由於常見的情況是語言已經已知，這些模型可以透過使用語言資訊作為提示來執行語言特定任務，這對於基於注意力的編碼器-解碼器架構特別有幫助。然而，連接時序分類 (CTC) 方法會透過聯合解碼和多任務訓練來增強辨識，通常不會結合語言提示，因為其條件獨立的輸出符號。為了克服這個問題，我們在自條件 CTC 架構中引入編碼提示技術，以零次學習的方式啟用 CTC 模型的語言特定適應。我們的技術已證明可以平均減少 28% 的錯誤，並減少低資源語言 41% 的錯誤。

##### **Bridging Local Details and Global Context in Text-Attributed Graphs**
2406.12608v1 by Yaoke Wang, Yun Zhu, Wenqiao Zhang, Yueting Zhuang, Yunfei Li, Siliang Tang

Representation learning on text-attributed graphs (TAGs) is vital for
real-world applications, as they combine semantic textual and contextual
structural information. Research in this field generally consist of two main
perspectives: local-level encoding and global-level aggregating, respectively
refer to textual node information unification (e.g., using Language Models) and
structure-augmented modeling (e.g., using Graph Neural Networks). Most existing
works focus on combining different information levels but overlook the
interconnections, i.e., the contextual textual information among nodes, which
provides semantic insights to bridge local and global levels. In this paper, we
propose GraphBridge, a multi-granularity integration framework that bridges
local and global perspectives by leveraging contextual textual information,
enhancing fine-grained understanding of TAGs. Besides, to tackle scalability
and efficiency challenges, we introduce a graphaware token reduction module.
Extensive experiments across various models and datasets show that our method
achieves state-of-theart performance, while our graph-aware token reduction
module significantly enhances efficiency and solves scalability issues.

摘要：文本属性图 (TAG) 上的表征学习对于实际应用至关重要，因为它们结合了语义文本和上下文结构信息。该领域的研究所涉及的两个主要观点：局部编码和全局聚合，分别指文本节点信息统一（例如，使用语言模型）和结构增强建模（例如，使用图神经网络）。大多数现有工作侧重于结合不同的信息级别，但忽略了相互联系，即节点之间的上下文文本信息，它提供了语义见解以桥接局部和全局级别。在本文中，我们提出了 GraphBridge，这是一个多粒度集成框架，它通过利用上下文文本信息来桥接局部和全局视角，增强了对 TAG 的细粒度理解。此外，为了解决可扩展性和效率挑战，我们引入了一个图感知令牌缩减模块。跨各种模型和数据集的广泛实验表明，我们的方法实现了最先进的性能，而我们的图感知令牌缩减模块显着提高了效率并解决了可扩展性问题。

##### **Low-Redundant Optimization for Large Language Model Alignment**
2406.12606v1 by Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Jingyuan Wang, Ji-Rong Wen

Large language models (LLMs) are still struggling in aligning with human
preference in complex tasks and scenarios. They are prone to overfit into the
unexpected patterns or superficial styles in the training data. We conduct an
empirical study that only selects the top-10\% most updated parameters in LLMs
for alignment training, and see improvements in the convergence process and
final performance. It indicates the existence of redundant neurons in LLMs for
alignment training. To reduce its influence, we propose a low-redundant
alignment method named \textbf{ALLO}, focusing on optimizing the most related
neurons with the most useful supervised signals. Concretely, we first identify
the neurons that are related to the human preference data by a gradient-based
strategy, then identify the alignment-related key tokens by reward models for
computing loss. Besides, we also decompose the alignment process into the
forgetting and learning stages, where we first forget the tokens with unaligned
knowledge and then learn aligned knowledge, by updating different ratios of
neurons, respectively. Experimental results on 10 datasets have shown the
effectiveness of ALLO. Our code and data are available at
\url{https://github.com/RUCAIBox/ALLO}.

摘要：大型語言模型 (LLM) 在複雜任務和場景中仍難以與人類偏好保持一致。它們容易過度擬合訓練資料中的意外模式或表面風格。我們進行了一項實證研究，僅選取 LLM 中更新幅度最高的前 10% 參數進行對齊訓練，並觀察到收斂過程和最終效能的改善。這表明 LLM 中存在多餘的神經元，可用於對齊訓練。為了減少其影響，我們提出了一種名為 \textbf{ALLO} 的低冗餘對齊方法，專注於最佳化與最有用的監督訊號最相關的神經元。具體來說，我們首先通過基於梯度的策略識別與人類偏好資料相關的神經元，然後通過獎勵模型識別與對齊相關的關鍵代碼，以計算損失。此外，我們還將對齊過程分解為遺忘和學習階段，在這些階段中，我們首先遺忘具有未對齊知識的代碼，然後通過分別更新不同比例的神經元來學習對齊的知識。在 10 個資料集上的實驗結果顯示了 ALLO 的有效性。我們的程式碼和資料可在 \url{https://github.com/RUCAIBox/ALLO} 取得。

##### **PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval**
2406.12593v1 by Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do

Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)
for efficient document retrieval without relying on external indexes. However,
DSIs need full re-training to handle updates in dynamic corpora, causing
significant computational inefficiencies. We introduce PromptDSI, a
rehearsal-free, prompt-based approach for instance-wise incremental learning in
document retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of
DSI, leveraging its powerful representation to efficiently index new corpora
while maintaining a balance between stability and plasticity. We eliminate the
initial forward pass of prompt-based continual learning methods that doubles
training and inference time. Moreover, we propose a topic-aware prompt pool
that employs neural topic embeddings as fixed keys. This strategy ensures
diverse and effective prompt usage, addressing the challenge of parameter
underutilization caused by the collapse of the query-key matching mechanism.
Our empirical evaluations demonstrate that PromptDSI matches IncDSI in managing
forgetting while significantly enhancing recall by over 4% on new corpora.

摘要：可微分搜尋索引 (DSI) 利用預先訓練的語言模型 (PLM)
進行有效的文件檢索，而無需依賴外部索引。然而，
DSI 需要進行完整的重新訓練才能處理動態語料庫中的更新，這會造成
顯著的運算效率低下。我們介紹了 PromptDSI，一種
無需彩排、基於提示的即時式增量學習方法，用於
文件檢索。PromptDSI 將提示附加到 DSI 的凍結 PLM 編碼器，利用其強大的表示來有效地索引新的語料庫
同時在穩定性和可塑性之間取得平衡。我們消除了基於提示的持續學習方法的初始前向傳遞，這會使
訓練和推理時間加倍。此外，我們提出了主題感知提示池
採用神經主題嵌入作為固定鍵。此策略可確保
提示使用多樣化且有效，解決了查詢鍵匹配機制崩潰導致的參數利用不足的挑戰。
我們的經驗評估表明，PromptDSI 在管理
遺忘時與 IncDSI 相匹配，同時在新的語料庫中將召回率顯著提高了 4% 以上。

##### **UIFV: Data Reconstruction Attack in Vertical Federated Learning**
2406.12588v1 by Jirui Yang, Peng Chen, Zhihui Lu, Qiang Duan, Yubing Bao

Vertical Federated Learning (VFL) facilitates collaborative machine learning
without the need for participants to share raw private data. However, recent
studies have revealed privacy risks where adversaries might reconstruct
sensitive features through data leakage during the learning process. Although
data reconstruction methods based on gradient or model information are somewhat
effective, they reveal limitations in VFL application scenarios. This is
because these traditional methods heavily rely on specific model structures
and/or have strict limitations on application scenarios. To address this, our
study introduces the Unified InverNet Framework into VFL, which yields a novel
and flexible approach (dubbed UIFV) that leverages intermediate feature data to
reconstruct original data, instead of relying on gradients or model details.
The intermediate feature data is the feature exchanged by different
participants during the inference phase of VFL. Experiments on four datasets
demonstrate that our methods significantly outperform state-of-the-art
techniques in attack precision. Our work exposes severe privacy vulnerabilities
within VFL systems that pose real threats to practical VFL applications and
thus confirms the necessity of further enhancing privacy protection in the VFL
architecture.

摘要：垂直聯合學習 (VFL) 促進合作機器學習，無需參與者分享原始私人資料。然而，最近的研究揭示了隱私風險，其中對手可能會透過學習過程中資料外洩來重建敏感特徵。雖然基於梯度或模型資訊的資料重建方法有一定效果，但它們在 VFL 應用場景中顯示出限制。這是因為這些傳統方法嚴重依賴特定模型結構和/或對應用場景有嚴格限制。為了解決這個問題，我們的研究將統一的 InverNet 框架引入 VFL，產生一種新穎且靈活的方法 (稱為 UIFV)，利用中間特徵資料重建原始資料，而不是依賴梯度或模型細節。中間特徵資料是參與者在 VFL 的推論階段交換的特徵。在四個資料集上的實驗證明，我們的模型在攻擊精度方面顯著優於現有技術。我們的研究揭露了 VFL 系統中嚴重的隱私漏洞，對實際的 VFL 應用構成真正的威脅，因此確認了進一步加強 VFL 架構中隱私保護的必要性。

##### **Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling**
2406.12585v1 by Yao-Ching Yu, Chun-Chih Kuo, Ziqi Ye, Yu-Cheng Chang, Yueh-Se Li

Ensembling multiple models has always been an effective approach to push the
limits of existing performance and is widely used in classification tasks by
simply averaging the classification probability vectors from multiple
classifiers to achieve better accuracy. However, in the thriving open-source
Large Language Model (LLM) community, ensembling methods are rare and typically
limited to ensembling the full-text outputs of LLMs, such as selecting the best
output using a ranker, which leads to underutilization of token-level
probability information. In this paper, we treat the Generation of each token
by LLMs as a Classification (GaC) for ensembling. This approach fully exploits
the probability information at each generation step and better prevents LLMs
from producing early incorrect tokens that lead to snowballing errors. In
experiments, we ensemble state-of-the-art LLMs on several benchmarks, including
exams, mathematics and reasoning, and observe that our method breaks the
existing community performance ceiling. Furthermore, we observed that most of
the tokens in the answer are simple and do not affect the correctness of the
final answer. Therefore, we also experimented with ensembling only key tokens,
and the results showed better performance with lower latency across benchmarks.

摘要：整合多個模型一直是推動現有效能極限的有效方法，並廣泛用於分類任務中，只需平均多個分類器的分類機率向量即可獲得更好的準確度。然而，在蓬勃發展的開源大型語言模型 (LLM) 社群中，整合方法很少見，而且通常僅限於整合 LLM 的全文輸出，例如使用排名器選擇最佳輸出，這導致無法充分利用代幣級別的機率資訊。在本文中，我們將 LLM 產生的每個代幣視為分類 (GaC) 以進行整合。此方法充分利用每個產生步驟中的機率資訊，並能更好地防止 LLM 產生導致錯誤累積的早期錯誤代幣。在實驗中，我們在幾個基準上整合了最先進的 LLM，包括考試、數學和推理，並觀察到我們的方法打破了現有社群效能上限。此外，我們觀察到答案中的大多數代幣都很簡單，不會影響最終答案的正確性。因此，我們也嘗試僅整合關鍵代幣，結果顯示在所有基準上效能更好，且延遲時間更低。

##### **Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models**
2406.12572v1 by Eldar Kurtic, Amir Moeini, Dan Alistarh

We introduce Mathador-LM, a new benchmark for evaluating the mathematical
reasoning on large language models (LLMs), combining ruleset interpretation,
planning, and problem-solving. This benchmark is inspired by the Mathador game,
where the objective is to reach a target number using basic arithmetic
operations on a given set of base numbers, following a simple set of rules. We
show that, across leading LLMs, we obtain stable average performance while
generating benchmark instances dynamically, following a target difficulty
level. Thus, our benchmark alleviates concerns about test-set leakage into
training data, an issue that often undermines popular benchmarks. Additionally,
we conduct a comprehensive evaluation of both open and closed-source
state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary
models struggle with Mathador-LM, scoring significantly lower than average 5th
graders. This stands in stark contrast to their strong performance on popular
mathematical reasoning benchmarks.

摘要：我們介紹 Mathador-LM，這是一個用於評估大型語言模型 (LLM) 中數學推理的新基準，結合規則集解釋、規劃和問題解決。此基準靈感來自 Mathador 遊戲，目標是使用給定的一組基本數字進行基本算術運算，並遵循一組簡單的規則來達到目標數字。我們表明，在領先的 LLM 中，我們在動態產生基準實例時獲得穩定的平均效能，同時遵循目標難度等級。因此，我們的基準減輕了測試集洩漏到訓練資料的疑慮，這是一個經常破壞熱門基準的問題。此外，我們對 Mathador-LM 上的開放和閉源最新 LLM 進行了全面的評估。我們的發現表明，當代模型在 Mathador-LM 上表現不佳，得分顯著低於平均 5 年級學生。這與他們在熱門數學推理基準上的強勁表現形成鮮明對比。

##### **Applying Ensemble Methods to Model-Agnostic Machine-Generated Text Detection**
2406.12570v1 by Ivan Ong, Boon King Quek

In this paper, we study the problem of detecting machine-generated text when
the large language model (LLM) it is possibly derived from is unknown. We do so
by apply ensembling methods to the outputs from DetectGPT classifiers (Mitchell
et al. 2023), a zero-shot model for machine-generated text detection which is
highly accurate when the generative (or base) language model is the same as the
discriminative (or scoring) language model. We find that simple summary
statistics of DetectGPT sub-model outputs yield an AUROC of 0.73 (relative to
0.61) while retaining its zero-shot nature, and that supervised learning
methods sharply boost the accuracy to an AUROC of 0.94 but require a training
dataset. This suggests the possibility of further generalisation to create a
highly-accurate, model-agnostic machine-generated text detector.

摘要：在本文中，我們探討了在不知道機器生成的文字可能來自哪個大型語言模型 (LLM) 時，偵測機器生成文字的問題。我們透過將群集方法套用在 DetectGPT 分類器的輸出上來做到這一點（Mitchell 等人，2023 年），DetectGPT 分類器是機器生成文字偵測的零次學習模型，當生成式（或基礎）語言模型與判別式（或評分）語言模型相同時，其準確度非常高。我們發現，DetectGPT 子模型輸出的簡單摘要統計數據產生了 0.73 的 AUROC（相對於 0.61），同時保留了其零次學習的特性，而監督式學習方法將準確度大幅提升至 0.94 的 AUROC，但需要一個訓練資料集。這表示有進一步概括的可能性，以建立一個高準確度、與模型無關的機器生成文字偵測器。

##### **RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation**
2406.12566v1 by Shuting Wang, Xin Xu, Mang Wang, Weipeng Chen, Yutao Zhu, Zhicheng Dou

Retrieval-augmented generation (RAG) effectively addresses issues of static
knowledge and hallucination in large language models. Existing studies mostly
focus on question scenarios with clear user intents and concise answers.
However, it is prevalent that users issue broad, open-ended queries with
diverse sub-intents, for which they desire rich and long-form answers covering
multiple relevant aspects. To tackle this important yet underexplored problem,
we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect
explorer to identify potential sub-aspects of input questions, a multi-faceted
retriever to build a candidate pool of diverse external documents related to
these sub-aspects, and a generative list-wise ranker, which is a key module to
provide the top-k most valuable documents for the final generator. These ranked
documents sufficiently cover various query aspects and are aware of the
generator's preferences, hence incentivizing it to produce rich and
comprehensive responses for users. The training of our ranker involves a
supervised fine-tuning stage to ensure the basic coverage of documents, and a
reinforcement learning stage to align downstream LLM's preferences to the
ranking of documents. Experimental results on two publicly available datasets
prove that our framework effectively and efficiently provides comprehensive and
satisfying responses to users.

摘要：檢索增強生成 (RAG) 有效地解決大型語言模型中靜態知識和幻覺的問題。現有研究主要集中在具有明確使用者意圖和簡潔答案的問題場景。然而，使用者經常提出廣泛、開放式的查詢，其中包含不同的子意圖，他們希望獲得豐富且長篇的答案，涵蓋多個相關方面。為了解決這個重要但尚未充分探討的問題，我們提出了一個新的 RAG 框架，即 RichRAG。它包括一個子方面探索器，用於識別輸入問題的潛在子方面，一個多方面的檢索器，用於建立與這些子方面相關的不同外部文件的候選池，以及一個生成式列表級別排名器，這是一個關鍵模組，用於為最終生成器提供最有價值的 top-k 文件。這些排名文件充分涵蓋了各種查詢方面，並且了解生成器的偏好，因此激勵它為使用者產生豐富且全面的回應。我們排名器的訓練包括一個監督微調階段，以確保文件的基本涵蓋範圍，以及一個強化學習階段，以將下游 LLM 的偏好與文件的排名保持一致。在兩個公開可用的資料集上的實驗結果證明，我們的框架有效且高效地為使用者提供了全面且令人滿意的回應。

##### **Low-Resource Machine Translation through the Lens of Personalized Federated Learning**
2406.12564v1 by Viktor Moskvoretskii, Nazarii Tupitsa, Chris Biemann, Samuel Horváth, Eduard Gorbunov, Irina Nikishina

We present a new approach based on the Personalized Federated Learning
algorithm MeritFed that can be applied to Natural Language Tasks with
heterogeneous data. We evaluate it on the Low-Resource Machine Translation
task, using the dataset from the Large-Scale Multilingual Machine Translation
Shared Task (Small Track #2) and the subset of Sami languages from the
multilingual benchmark for Finno-Ugric languages. In addition to its
effectiveness, MeritFed is also highly interpretable, as it can be applied to
track the impact of each language used for training. Our analysis reveals that
target dataset size affects weight distribution across auxiliary languages,
that unrelated languages do not interfere with the training, and auxiliary
optimizer parameters have minimal impact. Our approach is easy to apply with a
few lines of code, and we provide scripts for reproducing the experiments at
https://github.com/VityaVitalich/MeritFed

摘要：我們提出一個新的方法，基於個人化聯邦學習演算法 MeritFed，可應用於具有異質性資料的自然語言任務。我們在低資源機器翻譯任務上對其進行評估，使用來自大規模多語言機器翻譯共享任務（小軌道 2）的資料集和芬烏格爾語言多語言基準中的薩米語言子集。除了其有效性之外，MeritFed 的可解釋性也很高，因為它可用於追蹤用於訓練的每種語言的影響。我們的分析表明，目標資料集大小會影響輔助語言中的權重分佈，不相關的語言不會干擾訓練，而輔助優化器參數的影響很小。我們的做法很容易應用，只需幾行程式碼，我們提供了腳本，可在 https://github.com/VityaVitalich/MeritFed 上重現實驗。

##### **Bayesian Data Selection**
2406.12560v1 by Julian Rodemann

A wide range of machine learning algorithms iteratively add data to the
training sample. Examples include semi-supervised learning, active learning,
multi-armed bandits, and Bayesian optimization. We embed this kind of data
addition into decision theory by framing data selection as a decision problem.
This paves the way for finding Bayes-optimal selections of data. For the
illustrative case of self-training in semi-supervised learning, we derive the
respective Bayes criterion. We further show that deploying this criterion
mitigates the issue of confirmation bias by empirically assessing our method
for generalized linear models, semi-parametric generalized additive models, and
Bayesian neural networks on simulated and real-world data.

摘要：各種機器學習演算法會反覆將資料新增到訓練樣本中，例如半監督式學習、主動學習、多臂賭博機和貝氏最佳化。我們透過將資料選取建構為決策問題，將此類資料新增嵌入決策理論中。這為找出資料的貝氏最佳選取鋪路。針對半監督式學習中自我訓練的說明性案例，我們推導出相關的貝氏準則。我們進一步證明，部署此準則可透過經驗評估我們針對廣義線性模型、半參數廣義加成模型以及模擬和實際資料上的貝氏神經網路的方法，來減輕確認偏誤的問題。

##### **Offline Imitation Learning with Model-based Reverse Augmentation**
2406.12550v1 by Jie-Jing Shao, Hao-Sen Shi, Lan-Zhe Guo, Yu-Feng Li

In offline Imitation Learning (IL), one of the main challenges is the
\textit{covariate shift} between the expert observations and the actual
distribution encountered by the agent, because it is difficult to determine
what action an agent should take when outside the state distribution of the
expert demonstrations. Recently, the model-free solutions introduce the
supplementary data and identify the latent expert-similar samples to augment
the reliable samples during learning. Model-based solutions build forward
dynamic models with conservatism quantification and then generate additional
trajectories in the neighborhood of expert demonstrations. However, without
reward supervision, these methods are often over-conservative in the
out-of-expert-support regions, because only in states close to expert-observed
states can there be a preferred action enabling policy optimization. To
encourage more exploration on expert-unobserved states, we propose a novel
model-based framework, called offline Imitation Learning with Self-paced
Reverse Augmentation (SRA). Specifically, we build a reverse dynamic model from
the offline demonstrations, which can efficiently generate trajectories leading
to the expert-observed states in a self-paced style. Then, we use the
subsequent reinforcement learning method to learn from the augmented
trajectories and transit from expert-unobserved states to expert-observed
states. This framework not only explores the expert-unobserved states but also
guides maximizing long-term returns on these states, ultimately enabling
generalization beyond the expert data. Empirical results show that our proposal
could effectively mitigate the covariate shift and achieve the state-of-the-art
performance on the offline imitation learning benchmarks. Project website:
\url{https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/}.

摘要：<paragraph>在離線模仿學習（IL）中，主要挑戰之一是專家觀察與代理遭遇的實際分佈之間的「協變數位移」，因為難以判斷代理在專家示範狀態分佈之外時應採取什麼行動。最近，無模型的解決方案引入了補充資料，並識別出潛在的類似專家的樣本，以在學習期間增加可靠的樣本。基於模型的解決方案建立了具有保守量化的前向動態模型，然後在專家示範的鄰域中產生額外的軌跡。然而，在沒有獎勵監督的情況下，這些方法在專家支援區域之外通常過於保守，因為只有在接近專家觀察狀態的狀態中，才會有首選動作能進行策略最佳化。為了鼓勵在專家未觀察到的狀態中進行更多探索，我們提出了一個新的基於模型的框架，稱為具有自定進度反向擴充（SRA）的離線模仿學習。具體來說，我們從離線示範中建立一個反向動態模型，它可以有效地產生以自定進度方式導致專家觀察狀態的軌跡。然後，我們使用後續的強化學習方法從擴充的軌跡中學習，並從專家未觀察到的狀態過渡到專家觀察到的狀態。此框架不僅探索了專家未觀察到的狀態，還指導最大化這些狀態的長期回報，最終實現超越專家資料的概括。實證結果表明，我們的提案可以有效減輕協變數位移，並在離線模仿學習基準上實現最先進的效能。專案網站：\url{https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/}。</paragraph>

##### **MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts**
2406.12549v1 by Dominik Macko, Jakub Kopal, Robert Moro, Ivan Srba

Recent LLMs are able to generate high-quality multilingual texts,
indistinguishable for humans from authentic human-written ones. Research in
machine-generated text detection is however mostly focused on the English
language and longer texts, such as news articles, scientific papers or student
essays. Social-media texts are usually much shorter and often feature informal
language, grammatical errors, or distinct linguistic items (e.g., emoticons,
hashtags). There is a gap in studying the ability of existing methods in
detection of such texts, reflected also in the lack of existing multilingual
benchmark datasets. To fill this gap we propose the first multilingual (22
languages) and multi-platform (5 social media platforms) dataset for
benchmarking machine-generated text detection in the social-media domain,
called MultiSocial. It contains 472,097 texts, of which about 58k are
human-written and approximately the same amount is generated by each of 7
multilingual LLMs. We use this benchmark to compare existing detection methods
in zero-shot as well as fine-tuned form. Our results indicate that the
fine-tuned detectors have no problem to be trained on social-media texts and
that the platform selection for training matters.

摘要：最近的 LLM 能生成高品质的多语言文本，
人类无法与真实的人类书写文本区分开来。机器生成的文本检测研究
然而，主要集中在英语和较长的文本上，例如新闻文章、科学论文或学生
文章。社交媒体文本通常要短得多，并且经常使用非正式语言、语法错误或独特的语言项目（例如表情符号、
主题标签）。在研究现有方法检测此类文本的能力方面存在差距，这也反映在缺乏现有的多语言
基准数据集。为了填补这一空白，我们提出了第一个多语言（22
语言）和多平台（5 个社交媒体平台）数据集，用于对社交媒体领域的机器生成文本检测进行基准测试，
称为 MultiSocial。它包含 472,097 个文本，其中约 58k 是人类编写的，大约相同数量是由 7 个多语言 LLM 中的每一个生成的。我们使用此基准来比较现有的检测方法
在零样本以及微调形式中。我们的结果表明，微调检测器在社交媒体文本上进行训练没有问题，并且
训练的平台选择很重要。

##### **P-Tailor: Customizing Personality Traits for Language Models via Mixture of Specialized LoRA Experts**
2406.12548v1 by Yuhao Dan, Jie Zhou, Qin Chen, Junfeng Tian, Liang He

Personalized large language models (LLMs) have attracted great attention in
many applications, such as intelligent education and emotional support. Most
work focuses on controlling the character settings based on the profile (e.g.,
age, skill, experience, and so on). Conversely, the psychological theory-based
personality traits with implicit expression and behavior are not well modeled,
limiting their potential application in more specialized fields such as the
psychological counseling agents. In this paper, we propose a mixture of experts
(MoE)-based personalized LLMs, named P-tailor, to model the Big Five
Personality Traits. Particularly, we learn specialized LoRA experts to
represent various traits, such as openness, conscientiousness, extraversion,
agreeableness and neuroticism. Then, we integrate P-Tailor with a personality
specialization loss, promoting experts to specialize in distinct personality
traits, thereby enhancing the efficiency of model parameter utilization. Due to
the lack of datasets, we also curate a high-quality personality crafting
dataset (PCD) to learn and develop the ability to exhibit different personality
traits across various topics. We conduct extensive experiments to verify the
great performance and effectiveness of P-Tailor in manipulation of the
fine-grained personality traits of LLMs.

摘要：個人化大型語言模型 (LLM) 在許多應用中備受關注，例如智慧教育和情緒支持。大多數工作都專注於根據個人資料（例如年齡、技能、經驗等）控制角色設定。相反地，基於心理理論的性格特質，其隱含表達和行為並未得到很好的建模，這限制了它們在心理諮商代理等更專業領域的潛在應用。在本文中，我們提出了一個基於專家混合 (MoE) 的個人化 LLM，稱為 P-tailor，以建模大五人格特質。特別是，我們學習專業的 LoRA 專家來表示各種特質，例如開放性、盡責性、外向性、親和性和神經質。然後，我們將 P-Tailor 與人格專業化損失整合，促使專家專注於不同的性格特質，從而提高模型參數利用率。由於缺乏資料集，我們還策劃了一個高品質的人格製作資料集 (PCD) 來學習和培養在各種主題中表現出不同人格特質的能力。我們進行了廣泛的實驗，以驗證 P-Tailor 在操縱 LLM 的細緻人格特質方面的出色效能和有效性。

##### **Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models**
2406.12546v1 by Philipp Mondorf, Barbara Plank

Knights and knaves problems represent a classic genre of logical puzzles
where characters either tell the truth or lie. The objective is to logically
deduce each character's identity based on their statements. The challenge
arises from the truth-telling or lying behavior, which influences the logical
implications of each statement. Solving these puzzles requires not only direct
deductions from individual statements, but the ability to assess the
truthfulness of statements by reasoning through various hypothetical scenarios.
As such, knights and knaves puzzles serve as compelling examples of
suppositional reasoning. In this paper, we introduce $\textit{TruthQuest}$, a
benchmark for suppositional reasoning based on the principles of knights and
knaves puzzles. Our benchmark presents problems of varying complexity,
considering both the number of characters and the types of logical statements
involved. Evaluations on $\textit{TruthQuest}$ show that large language models
like Llama 3 and Mixtral-8x7B exhibit significant difficulties solving these
tasks. A detailed error analysis of the models' output reveals that
lower-performing models exhibit a diverse range of reasoning errors, frequently
failing to grasp the concept of truth and lies. In comparison, more proficient
models primarily struggle with accurately inferring the logical implications of
potentially false statements.

摘要：騎士與惡棍問題代表了一種經典的邏輯謎題類型，其中人物不是說真話就是說謊。目標是根據角色的陳述邏輯地推論出每個角色的身份。挑戰來自說真話或說謊的行為，這會影響每個陳述的邏輯含義。解決這些謎題不僅需要從個別陳述中直接推論，還需要通過推理各種假設場景來評估陳述的真實性。因此，騎士與惡棍謎題是假設推理的有力範例。在本文中，我們介紹了 $\textit{TruthQuest}$，一個基於騎士與惡棍謎題原理的假設推理基準。我們的基準提出了不同複雜程度的問題，同時考慮了角色數量和涉及的邏輯陳述類型。在 $\textit{TruthQuest}$ 上的評估顯示，像 Llama 3 和 Mixtral-8x7B 這樣的大語言模型在解決這些任務時遇到了顯著困難。對模型輸出的詳細錯誤分析表明，表現較差的模型表現出各種推理錯誤，經常無法掌握真假概念。相比之下，表現較好的模型主要難以準確推斷出潛在錯誤陳述的邏輯含義。

##### **Variational Distillation of Diffusion Policies into Mixture of Experts**
2406.12538v1 by Hongyi Zhou, Denis Blessing, Ge Li, Onur Celik, Xiaogang Jia, Gerhard Neumann, Rudolf Lioutikov

This work introduces Variational Diffusion Distillation (VDD), a novel method
that distills denoising diffusion policies into Mixtures of Experts (MoE)
through variational inference. Diffusion Models are the current
state-of-the-art in generative modeling due to their exceptional ability to
accurately learn and represent complex, multi-modal distributions. This ability
allows Diffusion Models to replicate the inherent diversity in human behavior,
making them the preferred models in behavior learning such as Learning from
Human Demonstrations (LfD). However, diffusion models come with some drawbacks,
including the intractability of likelihoods and long inference times due to
their iterative sampling process. The inference times, in particular, pose a
significant challenge to real-time applications such as robot control. In
contrast, MoEs effectively address the aforementioned issues while retaining
the ability to represent complex distributions but are notoriously difficult to
train. VDD is the first method that distills pre-trained diffusion models into
MoE models, and hence, combines the expressiveness of Diffusion Models with the
benefits of Mixture Models. Specifically, VDD leverages a decompositional upper
bound of the variational objective that allows the training of each expert
separately, resulting in a robust optimization scheme for MoEs. VDD
demonstrates across nine complex behavior learning tasks, that it is able to:
i) accurately distill complex distributions learned by the diffusion model, ii)
outperform existing state-of-the-art distillation methods, and iii) surpass
conventional methods for training MoE.

摘要：<paragraph>這項工作介紹了變異擴散蒸餾 (VDD)，一種透過變異推論將去噪擴散策略蒸餾到專家混合 (MoE) 的新方法。擴散模型是當前生成模型中的最新技術，因為它們具有準確學習和表示複雜的多模態分佈的非凡能力。這種能力使擴散模型能夠複製人類行為中固有的多樣性，使其成為行為學習（例如從人類示範 (LfD) 中學習）中的首選模型。然而，擴散模型也有一些缺點，包括由於其迭代採樣過程而導致的似然度難以處理和推論時間長。特別是推論時間對機器人控制等實時應用構成重大挑戰。相比之下，MoE 有效地解決了上述問題，同時保留了表示複雜分佈的能力，但眾所周知，訓練起來很困難。VDD 是第一個將預訓練的擴散模型蒸餾到 MoE 模型的方法，因此結合了擴散模型的表現力和混合模型的優點。具體來說，VDD 利用變異目標的分解上限，允許分別訓練每個專家，從而為 MoE 提供穩健的最佳化方案。VDD 在九項複雜的行為學習任務中證明，它能夠：i) 準確地蒸餾擴散模型學習到的複雜分佈，ii) 優於現有的最先進的蒸餾方法，以及 iii) 超越訓練 MoE 的傳統方法。</paragraph>

##### **LLM4MSR: An LLM-Enhanced Paradigm for Multi-Scenario Recommendation**
2406.12529v1 by Yuhao Wang, Yichao Wang, Zichuan Fu, Xiangyang Li, Xiangyu Zhao, Huifeng Guo, Ruiming Tang

As the demand for more personalized recommendation grows and a dramatic boom
in commercial scenarios arises, the study on multi-scenario recommendation
(MSR) has attracted much attention, which uses the data from all scenarios to
simultaneously improve their recommendation performance. However, existing
methods tend to integrate insufficient scenario knowledge and neglect learning
personalized cross-scenario preferences, thus leading to suboptimal performance
and inadequate interpretability. Meanwhile, though large language model (LLM)
has shown great capability of reasoning and capturing semantic information, the
high inference latency and high computation cost of tuning hinder its
implementation in industrial recommender systems. To fill these gaps, we
propose an effective efficient interpretable LLM-enhanced paradigm LLM4MSR in
this work. Specifically, we first leverage LLM to uncover multi-level knowledge
including scenario correlations and users' cross-scenario interests from the
designed scenario- and user-level prompt without fine-tuning the LLM, then
adopt hierarchical meta networks to generate multi-level meta layers to
explicitly improves the scenario-aware and personalized recommendation
capability. Our experiments on KuaiSAR-small, KuaiSAR, and Amazon datasets
validate two significant advantages of LLM4MSR: (i) the effectiveness and
compatibility with different multi-scenario backbone models (achieving 1.5%,
1%, and 40% AUC improvement on three datasets), (ii) high efficiency and
deployability on industrial recommender systems, and (iii) improved
interpretability. The implemented code and data is available to ease
reproduction.

摘要：<paragraph>隨著對更個人化推薦的需求增長，以及商業場景出現顯著的蓬勃發展，多場景推薦 (MSR) 的研究引起了廣泛關注，它使用來自所有場景的數據來同時改善其推薦效能。然而，現有方法傾向於整合不足的場景知識，並且忽略學習個人化的跨場景偏好，從而導致次優的效能和不足的詮釋性。同時，儘管大型語言模型 (LLM) 已展現出推理和擷取語義資訊的強大能力，但調整的高推理延遲和高運算成本阻礙了它在產業推薦系統中的實作。為了填補這些差距，我們在這項工作中提出了一個有效且高效的詮釋性 LLM 增強範例 LLM4MSR。具體來說，我們首先利用 LLM 來揭示多層級知識，包括場景關聯性和使用者的跨場景興趣，而無需微調 LLM，然後採用階層式元網路來產生多層級元層，以明確改善場景感知和個人化推薦能力。我們在 KuaiSAR-small、KuaiSAR 和 Amazon 資料集上的實驗驗證了 LLM4MSR 的兩個顯著優勢：(i) 與不同的多場景主幹模型的有效性和相容性（在三個資料集上實現 1.5%、1% 和 40% 的 AUC 提升），(ii) 產業推薦系統的高效率和可部署性，以及 (iii) 改善的詮釋性。已實作的程式碼和資料可供使用，以簡化重現。</paragraph>

##### **FuseGen: PLM Fusion for Data-generation based Zero-shot Learning**
2406.12527v1 by Tianyuan Zou, Yang Liu, Peng Li, Jianqing Zhang, Jingjing Liu, Ya-Qin Zhang

Data generation-based zero-shot learning, although effective in training
Small Task-specific Models (STMs) via synthetic datasets generated by
Pre-trained Language Models (PLMs), is often limited by the low quality of such
synthetic datasets. Previous solutions have primarily focused on single PLM
settings, where synthetic datasets are typically restricted to specific
sub-spaces and often deviate from real-world distributions, leading to severe
distribution bias. To mitigate such bias, we propose FuseGen, a novel data
generation-based zero-shot learning framework that introduces a new criteria
for subset selection from synthetic datasets via utilizing multiple PLMs and
trained STMs. The chosen subset provides in-context feedback to each PLM,
enhancing dataset quality through iterative data generation. Trained STMs are
then used for sample re-weighting as well, further improving data quality.
Extensive experiments across diverse tasks demonstrate that FuseGen
substantially outperforms existing methods, highly effective in boosting STM
performance in a PLM-agnostic way. Code is provided in
https://github.com/LindaLydia/FuseGen.

摘要：基於資料產生的零次學習雖然在訓練小型任務特定模型 (STM) 方面很有效，但透過預訓練語言模型 (PLM) 生成的合成資料集，卻經常受到資料集品質低落所限制。先前的解決方案主要專注於單一 PLM 設定，其中合成資料集通常侷限於特定子空間，而且經常偏離真實世界的分布，導致嚴重的分佈偏差。為了減輕這種偏差，我們提出 FuseGen，這是一個基於資料產生的零次學習框架，它引入了新的準則，透過使用多個 PLM 和訓練過的 STM 從合成資料集中進行子集選擇。所選取的子集會提供情境內的回饋給每個 PLM，透過反覆的資料產生來提升資料集品質。訓練過的 STM 隨後也會用於樣本重新加權，進一步提升資料品質。在各種任務中進行的廣泛實驗證明 FuseGen 大幅優於現有方法，在提升 STM 效能方面非常有效，而且不依賴 PLM。程式碼已提供於 https://github.com/LindaLydia/FuseGen。

##### **Code-Optimise: Self-Generated Preference Data for Correctness and Efficiency**
2406.12502v1 by Leonidas Gee, Milan Gritta, Gerasimos Lampouras, Ignacio Iacobacci

Code Language Models have been trained to generate accurate solutions,
typically with no regard for runtime. On the other hand, previous works that
explored execution optimisation have observed corresponding drops in functional
correctness. To that end, we introduce Code-Optimise, a framework that
incorporates both correctness (passed, failed) and runtime (quick, slow) as
learning signals via self-generated preference data. Our framework is both
lightweight and robust as it dynamically selects solutions to reduce
overfitting while avoiding a reliance on larger models for learning signals.
Code-Optimise achieves significant improvements in pass@k while decreasing the
competitive baseline runtimes by an additional 6% for in-domain data and up to
3% for out-of-domain data. As a byproduct, the average length of the generated
solutions is reduced by up to 48% on MBPP and 23% on HumanEval, resulting in
faster and cheaper inference. The generated data and codebase will be
open-sourced at www.open-source.link.

摘要：程式碼語言模型已經被訓練成產生精確的解決方案，通常不考慮執行時間。另一方面，先前探索執行最佳化的作品已經觀察到相應的功能正確性下降。為此，我們引進 Code-Optimise，一個將正確性（通過、失敗）和執行時間（快速、慢速）都納入作為學習訊號的框架，透過自產的偏好資料。我們的框架既輕量又強大，因為它動態選擇解決方案來降低過度擬合，同時避免依賴較大的模型來學習訊號。Code-Optimise 在 pass@k 方面取得顯著的進步，同時將競爭基準執行時間進一步降低 6%，用於領域內資料，並將領域外資料降低高達 3%。作為副產品，產生的解決方案的平均長度在 MBPP 上減少了 48%，在 HumanEval 上減少了 23%，導致更快速、更便宜的推論。產生的資料和程式碼庫將在 www.open-source.link 開源。

##### **Autonomous navigation of catheters and guidewires in mechanical thrombectomy using inverse reinforcement learning**
2406.12499v1 by Harry Robertshaw, Lennart Karstensen, Benjamin Jackson, Alejandro Granados, Thomas C. Booth

Purpose: Autonomous navigation of catheters and guidewires can enhance
endovascular surgery safety and efficacy, reducing procedure times and operator
radiation exposure. Integrating tele-operated robotics could widen access to
time-sensitive emergency procedures like mechanical thrombectomy (MT).
Reinforcement learning (RL) shows potential in endovascular navigation, yet its
application encounters challenges without a reward signal. This study explores
the viability of autonomous navigation in MT vasculature using inverse RL (IRL)
to leverage expert demonstrations. Methods: This study established a
simulation-based training and evaluation environment for MT navigation. We used
IRL to infer reward functions from expert behaviour when navigating a guidewire
and catheter. We utilized soft actor-critic to train models with various reward
functions and compared their performance in silico. Results: We demonstrated
feasibility of navigation using IRL. When evaluating single versus dual device
(i.e. guidewire versus catheter and guidewire) tracking, both methods achieved
high success rates of 95% and 96%, respectively. Dual-tracking, however,
utilized both devices mimicking an expert. A success rate of 100% and procedure
time of 22.6 s were obtained when training with a reward function obtained
through reward shaping. This outperformed a dense reward function (96%, 24.9 s)
and an IRL-derived reward function (48%, 59.2 s). Conclusions: We have
contributed to the advancement of autonomous endovascular intervention
navigation, particularly MT, by employing IRL. The results underscore the
potential of using reward shaping to train models, offering a promising avenue
for enhancing the accessibility and precision of MT. We envisage that future
research can extend our methodology to diverse anatomical structures to enhance
generalizability.

摘要：<paragraph>目的：導管和導絲的自主導航可以增強血管內手術的安全性和有效性，縮短手術時間並減少操作者輻射暴露。整合遠程操作機器人技術可以擴大獲得時間敏感緊急手術（例如機械血栓切除術 (MT)）的機會。強化學習 (RL) 在血管內導航中顯示出潛力，但其應用在沒有獎勵信號的情況下會遇到挑戰。本研究探討了使用逆向 RL (IRL) 來利用專家示範在 MT 脈管系統中進行自主導航的可行性。方法：本研究建立了基於模擬的 MT 導航訓練和評估環境。我們使用 IRL 從專家在導引導絲和導管時表現的行為中推斷獎勵函數。我們利用軟性動作評論員訓練具有各種獎勵函數的模型，並比較它們在矽中的表現。結果：我們展示了使用 IRL 導航的可行性。在評估單一與雙重裝置（即導絲與導管和導絲）追蹤時，兩種方法分別達到了 95% 和 96% 的高成功率。然而，雙重追蹤利用了模仿專家的兩種裝置。在使用通過獎勵成形獲得的獎勵函數進行訓練時，獲得了 100% 的成功率和 22.6 秒的手術時間。這優於密集獎勵函數 (96%，24.9 秒) 和 IRL 衍生的獎勵函數 (48%，59.2 秒)。結論：我們通過採用 IRL 為自主血管內介入導航，特別是 MT 的進步做出了貢獻。結果強調了使用獎勵成形訓練模型的潛力，為提高 MT 的可及性和精確性提供了一條有希望的途徑。我們預計未來的研究可以將我們的技術方法擴展到不同的解剖結構，以增強概括性。</paragraph>

##### **LightPAL: Lightweight Passage Retrieval for Open Domain Multi-Document Summarization**
2406.12494v1 by Masafumi Enomoto, Kunihiro Takeoka, Kosuke Akimoto, Kiril Gashteovski, Masafumi Oyamada

Open-Domain Multi-Document Summarization (ODMDS) is crucial for addressing
diverse information needs, which aims to generate a summary as answer to user's
query, synthesizing relevant content from multiple documents in a large
collection. Existing approaches that first find relevant passages and then
generate a summary using a language model are inadequate for ODMDS. This is
because open-ended queries often require additional context for the retrieved
passages to cover the topic comprehensively, making it challenging to retrieve
all relevant passages initially. While iterative retrieval methods have been
explored for multi-hop question answering (MQA), they are impractical for ODMDS
due to high latency from repeated large language model (LLM) inference for
reasoning. To address this issue, we propose LightPAL, a lightweight passage
retrieval method for ODMDS that constructs a graph representing passage
relationships using an LLM during indexing and employs random walk instead of
iterative reasoning and retrieval at inference time. Experiments on ODMDS
benchmarks show that LightPAL outperforms baseline retrievers in summary
quality while being significantly more efficient than an iterative MQA
approach.

摘要：開放領域多文件摘要 (ODMDS) 對於解決不同的資訊需求至關重要，其目的是根據使用者的查詢產生摘要作為答案，綜合來自大型集合中多個文件的相關內容。現有的方法是先找到相關段落，然後使用語言模型產生摘要，對於 ODMDS 來說是不夠的。這是因為開放式查詢通常需要額外的內容，才能讓擷取的段落全面涵蓋主題，這使得一開始就擷取所有相關段落具有挑戰性。雖然已經探索了反覆擷取的方法用於多跳問題解答 (MQA)，但由於反覆進行大型語言模型 (LLM) 推論以進行推理，因此它們不適用於 ODMDS，因為這會導致高延遲。為了解決這個問題，我們提出了 LightPAL，這是一種針對 ODMDS 的輕量級段落擷取方法，它在索引時使用 LLM 建立表示段落關係的圖，並在推論時使用隨機遊走，而不是反覆推理和擷取。在 ODMDS 基準測試中的實驗顯示，LightPAL 在摘要品質上優於基線擷取器，同時比反覆 MQA 方法有效率得多。

##### **The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions**
2406.12480v1 by Stefan Sylvius Wagner, Maike Behrendt, Marc Ziegele, Stefan Harmeling

Stance detection holds great potential for enhancing the quality of online
political discussions, as it has shown to be useful for summarizing
discussions, detecting misinformation, and evaluating opinion distributions.
Usually, transformer-based models are used directly for stance detection, which
require large amounts of data. However, the broad range of debate questions in
online political discussion creates a variety of possible scenarios that the
model is faced with and thus makes data acquisition for model training
difficult. In this work, we show how to leverage LLM-generated synthetic data
to train and improve stance detection agents for online political
discussions:(i) We generate synthetic data for specific debate questions by
prompting a Mistral-7B model and show that fine-tuning with the generated
synthetic data can substantially improve the performance of stance detection.
(ii) We examine the impact of combining synthetic data with the most
informative samples from an unlabelled dataset. First, we use the synthetic
data to select the most informative samples, second, we combine both these
samples and the synthetic data for fine-tuning. This approach reduces labelling
effort and consistently surpasses the performance of the baseline model that is
trained with fully labeled data. Overall, we show in comprehensive experiments
that LLM-generated data greatly improves stance detection performance for
online political discussions.

摘要：立場偵測對於提升線上政治討論的品質具有極大的潛力，因為它已被證明對於總結討論、偵測錯誤資訊和評估意見分佈很有用。通常，基於轉換器的模型會直接用於立場偵測，這需要大量的資料。然而，線上政治討論中廣泛的辯論問題產生了模型面臨的各種可能情境，因此使得模型訓練的資料取得困難。在這項工作中，我們展示如何利用 LLM 生成的合成資料來訓練和改善線上政治討論的立場偵測代理：(i) 我們透過提示 Mistral-7B 模型來產生特定辯論問題的合成資料，並展示使用產生的合成資料進行微調可以大幅改善立場偵測的效能。(ii) 我們探討結合合成資料與未標籤資料集中最有資訊性的範例的影響。首先，我們使用合成資料來選擇最有資訊性的範例，其次，我們結合這些範例和合成資料進行微調。這種方法減少了標籤工作，並始終超越使用完全標記資料訓練的基準模型的效能。總體而言，我們在綜合實驗中展示，LLM 生成的資料極大地改善了線上政治討論的立場偵測效能。

##### **RS-GPT4V: A Unified Multimodal Instruction-Following Dataset for Remote Sensing Image Understanding**
2406.12479v1 by Linrui Xu, Ling Zhao, Wang Guo, Qiujun Li, Kewang Long, Kaiqi Zou, Yuhan Wang, Haifeng Li

The remote sensing image intelligence understanding model is undergoing a new
profound paradigm shift which has been promoted by multi-modal large language
model (MLLM), i.e. from the paradigm learning a domain model (LaDM) shifts to
paradigm learning a pre-trained general foundation model followed by an
adaptive domain model (LaGD). Under the new LaGD paradigm, the old datasets,
which have led to advances in RSI intelligence understanding in the last
decade, are no longer suitable for fire-new tasks. We argued that a new dataset
must be designed to lighten tasks with the following features: 1)
Generalization: training model to learn shared knowledge among tasks and to
adapt to different tasks; 2) Understanding complex scenes: training model to
understand the fine-grained attribute of the objects of interest, and to be
able to describe the scene with natural language; 3) Reasoning: training model
to be able to realize high-level visual reasoning. In this paper, we designed a
high-quality, diversified, and unified multimodal instruction-following dataset
for RSI understanding produced by GPT-4V and existing datasets, which we called
RS-GPT4V. To achieve generalization, we used a (Question, Answer) which was
deduced from GPT-4V via instruction-following to unify the tasks such as
captioning and localization; To achieve complex scene, we proposed a
hierarchical instruction description with local strategy in which the
fine-grained attributes of the objects and their spatial relationships are
described and global strategy in which all the local information are integrated
to yield detailed instruction descript; To achieve reasoning, we designed
multiple-turn QA pair to provide the reasoning ability for a model. The
empirical results show that the fine-tuned MLLMs by RS-GPT4V can describe
fine-grained information. The dataset is available at:
https://github.com/GeoX-Lab/RS-GPT4V.

摘要：遙感影像智慧理解模型正經歷一場新的重大典範轉移，這是由多模態大型語言模型 (MLLM) 推動的，即從學習領域模型 (LaDM) 的典範轉移到學習預訓練通用基礎模型，然後再學習自適應領域模型 (LaGD) 的典範。在新的 LaGD 典範下，在過去十年中導致 RSI 智慧理解進步的舊資料集不再適合全新的任務。我們認為必須設計一個新的資料集，以減輕具有以下特徵的任務：1) 概括化：訓練模型學習任務之間的共享知識，並適應不同的任務；2) 理解複雜場景：訓練模型理解感興趣物體的細粒度屬性，並能夠用自然語言描述場景；3) 推理：訓練模型能夠實現高層級視覺推理。在本文中，我們設計了一個高品質、多樣化且統一的多模態指令遵循資料集，用於 RSI 理解，由 GPT-4V 和現有資料集生成，我們稱之為 RS-GPT4V。為了實現概括化，我們使用了一個 (問題、答案)，該 (問題、答案) 是通過指令遵循從 GPT-4V 推導出來的，以統一諸如標題和定位之類的任務；為了實現複雜場景，我們提出了一個具有局部策略的分層指令描述，其中描述了物體的細粒度屬性及其空間關係，以及將所有局部資訊整合起來以產生詳細指令描述的全局策略；為了實現推理，我們設計了多輪問答對，為模型提供推理能力。經驗結果表明，RS-GPT4V 微調的 MLLM 可以描述細粒度資訊。資料集可在以下網址取得：
https://github.com/GeoX-Lab/RS-GPT4V。

##### **Exploring Intra and Inter-language Consistency in Embeddings with ICA**
2406.12474v1 by Rongzhi Li, Takeru Matsuda, Hitomi Yanaka

Word embeddings represent words as multidimensional real vectors,
facilitating data analysis and processing, but are often challenging to
interpret. Independent Component Analysis (ICA) creates clearer semantic axes
by identifying independent key features. Previous research has shown ICA's
potential to reveal universal semantic axes across languages. However, it
lacked verification of the consistency of independent components within and
across languages. We investigated the consistency of semantic axes in two ways:
both within a single language and across multiple languages. We first probed
into intra-language consistency, focusing on the reproducibility of axes by
performing ICA multiple times and clustering the outcomes. Then, we
statistically examined inter-language consistency by verifying those axes'
correspondences using statistical tests. We newly applied statistical methods
to establish a robust framework that ensures the reliability and universality
of semantic axes.

摘要：詞嵌入將詞彙表示為多維實數向量，
促進數據分析和處理，但通常難以解讀。獨立成分分析 (ICA) 通過識別獨立關鍵特徵來建立更清晰的語義軸。先前的研究表明 ICA 有潛力揭示跨語言的通用語義軸。然而，它缺乏對語言內部和跨語言獨立成分一致性的驗證。我們以兩種方式研究了語義軸的一致性：一種在單一語言內，另一種在多種語言間。我們首先探討語言內部的一致性，重點關注通過多次執行 ICA 和對結果進行聚類來實現軸的可複製性。然後，我們通過使用統計檢驗驗證這些軸的對應關係，在統計上檢查語言間的一致性。我們新應用了統計方法來建立一個穩健的框架，以確保語義軸的可靠性和普遍性。

##### **Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation**
2406.12471v1 by Branislav Pecher, Jan Cegin, Robert Belanec, Jakub Simko, Ivan Srba, Maria Bielikova

While fine-tuning of pre-trained language models generally helps to overcome
the lack of labelled training samples, it also displays model performance
instability. This instability mainly originates from randomness in
initialisation or data shuffling. To address this, researchers either modify
the training process or augment the available samples, which typically results
in increased computational costs. We propose a new mitigation strategy, called
Delayed Ensemble with Noisy Interpolation (DENI), that leverages the strengths
of ensembling, noise regularisation and model interpolation, while retaining
computational efficiency. We compare DENI with 9 representative mitigation
strategies across 3 models, 4 tuning strategies and 7 text classification
datasets. We show that: 1) DENI outperforms the best performing mitigation
strategy (Ensemble), while using only a fraction of its cost; 2) the mitigation
strategies are beneficial for parameter-efficient fine-tuning (PEFT) methods,
outperforming full fine-tuning in specific cases; and 3) combining DENI with
data augmentation often leads to even more effective instability mitigation.

摘要：微调预训练语言模型虽然通常有助于克服标记训练样本的缺乏，但也显示出模型性能的不稳定性。这种不稳定性主要源于初始化或数据混洗中的随机性。为了解决这个问题，研究人员要么修改训练过程，要么增加可用样本，这通常会导致计算成本增加。我们提出了一种新的缓解策略，称为带有噪声插值的延迟集成（DENI），它利用了集成、噪声正则化和模型插值的优势，同时保留了计算效率。我们在 3 个模型、4 个调整策略和 7 个文本分类数据集中将 DENI 与 9 种代表性缓解策略进行了比较。我们展示了：1) DENI 优于性能最佳的缓解策略（集成），同时仅使用其一小部分成本；2) 缓解策略有利于参数高效微调（PEFT）方法，在特定情况下优于完全微调；3) 将 DENI 与数据增强相结合通常会导致更有效的缓解不稳定性。

##### **Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities**
2406.12468v1 by Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, Xueqi Cheng

The parametric knowledge memorized by large language models (LLMs) becomes
outdated quickly. In-context editing (ICE) is currently the most effective
method for updating the knowledge of LLMs. Recent advancements involve
enhancing ICE by modifying the decoding strategy, obviating the need for
altering internal model structures or adjusting external prompts. However, this
enhancement operates across the entire sequence generation, encompassing a
plethora of non-critical tokens. In this work, we introduce $\textbf{A}$daptive
$\textbf{T}$oken $\textbf{Bias}$er ($\textbf{ATBias}$), a new decoding
technique designed to enhance ICE. It focuses on the tokens that are mostly
related to knowledge during decoding, biasing their logits by matching key
entities related to new and parametric knowledge. Experimental results show
that ATBias significantly enhances ICE performance, achieving up to a 32.3%
improvement over state-of-the-art ICE methods while incurring only half the
latency. ATBias not only improves the knowledge editing capabilities of ICE but
can also be widely applied to LLMs with negligible cost.

摘要：大型語言模型（LLM）記憶的參數化知識會快速過時。目前，語境編輯（ICE）是更新 LLM 知識最有效的方法。最近的進展包括透過修改解碼策略來增強 ICE，無需改變內部模型結構或調整外部提示。然而，這種增強作用於整個序列生成，涵蓋了大量的非關鍵代碼。在這項工作中，我們引入了 $\textbf{A}$daptive $\textbf{T}$oken $\textbf{Bias}$er（$\textbf{ATBias}$），這是一種新的解碼技術，旨在增強 ICE。它專注於解碼過程中與知識最相關的代碼，透過比對與新參數化知識相關的主要實體，來偏向它們的 logit。實驗結果顯示，ATBias 大幅增強了 ICE 的效能，比最先進的 ICE 方法提升了 32.3%，同時僅產生一半的延遲。ATBias 不僅改善了 ICE 的知識編輯能力，還能廣泛應用於 LLM，且成本極低。

##### **RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes**
2406.12465v1 by Xiaoshan Yu, Chuan Qin, Dazhong Shen, Shangshang Yang, Haiping Ma, Hengshu Zhu, Xingyi Zhang

In the realm of education, both independent learning and group learning are
esteemed as the most classic paradigms. The former allows learners to
self-direct their studies, while the latter is typically characterized by
teacher-directed scenarios. Recent studies in the field of intelligent
education have leveraged deep temporal models to trace the learning process,
capturing the dynamics of students' knowledge states, and have achieved
remarkable performance. However, existing approaches have primarily focused on
modeling the independent learning process, with the group learning paradigm
receiving less attention. Moreover, the reciprocal effect between the two
learning processes, especially their combined potential to foster holistic
student development, remains inadequately explored. To this end, in this paper,
we propose RIGL, a unified Reciprocal model to trace knowledge states at both
the individual and group levels, drawing from the Independent and Group
Learning processes. Specifically, we first introduce a time frame-aware
reciprocal embedding module to concurrently model both student and group
response interactions across various time frames. Subsequently, we employ
reciprocal enhanced learning modeling to fully exploit the comprehensive and
complementary information between the two behaviors. Furthermore, we design a
relation-guided temporal attentive network, comprised of dynamic graph modeling
coupled with a temporal self-attention mechanism. It is used to delve into the
dynamic influence of individual and group interactions throughout the learning
processes. Conclusively, we introduce a bias-aware contrastive learning module
to bolster the stability of the model's training. Extensive experiments on four
real-world educational datasets clearly demonstrate the effectiveness of the
proposed RIGL model.

摘要：<paragraph>在教育領域中，獨立學習和群體學習被尊崇為最經典的典範。前者允許學習者自我指導他們的學習，而後者通常以教師指導的場景為特徵。智能教育領域的最新研究利用深度時間模型追蹤學習歷程，捕捉學生知識狀態的動態，並取得顯著的表現。然而，現有的方法主要專注於對獨立學習歷程建模，而群體學習典範則較少受到關注。此外，兩種學習歷程之間的交互作用，特別是它們促進學生整體發展的綜合潛力，仍未得到充分的探討。為此，我們在本文中提出 RIGL，一個統一的交互模型，用於追蹤個人和群體層級的知識狀態，並從獨立和群體學習歷程中汲取靈感。具體來說，我們首先引入一個時間框架感知的交互嵌入模組，用於同時對不同時間框架內的學生和群體反應交互進行建模。隨後，我們採用交互增強學習建模，以充分利用兩種行為之間的全面且互補的資訊。此外，我們設計了一個關係導向的時間注意力網路，由動態圖形建模與時間自我注意力機制組成。它用於深入探討在整個學習歷程中個人和群體交互的動態影響。最後，我們引入一個偏誤感知對比學習模組，以加強模型訓練的穩定性。在四個真實世界的教育資料集上進行的廣泛實驗清楚地證明了所提出的 RIGL 模型的有效性。</paragraph>

##### **A Neural Column Generation Approach to the Vehicle Routing Problem with Two-Dimensional Loading and Last-In-First-Out Constraints**
2406.12454v1 by Yifan Xia, Xiangyi Zhang

The vehicle routing problem with two-dimensional loading constraints
(2L-CVRP) and the last-in-first-out (LIFO) rule presents significant practical
and algorithmic challenges. While numerous heuristic approaches have been
proposed to address its complexity, stemming from two NP-hard problems: the
vehicle routing problem (VRP) and the two-dimensional bin packing problem
(2D-BPP), less attention has been paid to developing exact algorithms. Bridging
this gap, this article presents an exact algorithm that integrates advanced
machine learning techniques, specifically a novel combination of attention and
recurrence mechanisms. This integration accelerates the state-of-the-art exact
algorithm by a median of 29.79% across various problem instances. Moreover, the
proposed algorithm successfully resolves an open instance in the standard
test-bed, demonstrating significant improvements brought about by the
incorporation of machine learning models. Code is available at
https://github.com/xyfffff/NCG-for-2L-CVRP.

摘要：具有二维装载限制的车辆路径问题 (2L-CVRP) 和后进先出 (LIFO) 规则提出了重大的实际和算法挑战。虽然已经提出了许多启发式方法来解决其复杂性，但这些方法源自两个 NP 难题：车辆路径问题 (VRP) 和二维装箱问题 (2D-BPP)，对开发精确算法的关注较少。为了弥补这一差距，本文提出了一种精确算法，该算法集成了先进的机器学习技术，特别是注意力和循环机制的新颖组合。这种集成将最先进的精确算法在各种问题实例中平均加速了 29.79%。此外，所提出的算法成功地解决了标准测试平台中的一个开放实例，展示了通过结合机器学习模型带来的重大改进。代码可在 https://github.com/xyfffff/NCG-for-2L-CVRP 获得。

##### **Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine**
2406.12449v1 by Rui Yang, Yilin Ning, Emilia Keppo, Mingxuan Liu, Chuan Hong, Danielle S Bitterman, Jasmine Chiat Ling Ong, Daniel Shu Wei Ting, Nan Liu

Generative artificial intelligence (AI) has brought revolutionary innovations
in various fields, including medicine. However, it also exhibits limitations.
In response, retrieval-augmented generation (RAG) provides a potential
solution, enabling models to generate more accurate contents by leveraging the
retrieval of external knowledge. With the rapid advancement of generative AI,
RAG can pave the way for connecting this transformative technology with medical
applications and is expected to bring innovations in equity, reliability, and
personalization to health care.

摘要：生成式人工智能 (AI) 已為包括醫學在內的各個領域帶來革命性的創新。然而，它也表現出局限性。為了解決這個問題，檢索增強生成 (RAG) 提供了一個潛在的解決方案，使模型能夠透過利用外部知識的檢索來產生更準確的內容。隨著生成式 AI 的快速進展，RAG 可以為將這項轉型技術與醫療應用相連鋪路，並有望為醫療保健帶來公平性、可靠性和個人化的創新。

##### **Abstraction-of-Thought Makes Language Models Better Reasoners**
2406.12442v1 by Ruixin Hong, Hongming Zhang, Xiaoman Pan, Dong Yu, Changshui Zhang

Abstract reasoning, the ability to reason from the abstract essence of a
problem, serves as a key to generalization in human reasoning. However,
eliciting language models to perform reasoning with abstraction remains
unexplored. This paper seeks to bridge this gap by introducing a novel
structured reasoning format called Abstraction-of-Thought (AoT). The uniqueness
of AoT lies in its explicit requirement for varying levels of abstraction
within the reasoning process. This approach could elicit language models to
first contemplate on the abstract level before incorporating concrete details,
which is overlooked by the prevailing step-by-step Chain-of-Thought (CoT)
method. To align models with the AoT format, we present AoT Collection, a
generic finetuning dataset consisting of 348k high-quality samples with AoT
reasoning processes, collected via an automated and scalable pipeline. We
finetune a wide range of language models with AoT Collection and conduct
extensive evaluations on 23 unseen tasks from the challenging benchmark
Big-Bench Hard. Experimental results indicate that models aligned to AoT
reasoning format substantially outperform those aligned to CoT in many
reasoning tasks.

摘要：抽象推理，從問題的抽象本質進行推理的能力，是人類推理中概化的關鍵。然而，引發語言模型進行抽象推理仍未被探索。本文旨在通過引入一種稱為抽象思維 (AoT) 的新結構推理格式來彌補這一差距。AoT 的獨特性在於它對推理過程中不同抽象層級的明確要求。這種方法可以引發語言模型在納入具體細節之前首先思考抽象層級，而這正是現行的逐步思考鏈 (CoT) 方法所忽略的。為了使模型與 AoT 格式保持一致，我們提出了 AoT 集合，這是一個通用的微調資料集，包含 348k 個具有 AoT 推理過程的高品質範例，並透過自動化且可擴充的管道收集。我們使用 AoT 集合對各種語言模型進行微調，並對來自具有挑戰性的基準 Big-Bench Hard 的 23 個未見任務進行廣泛評估。實驗結果表明，與 AoT 推理格式保持一致的模型在許多推理任務中明顯優於與 CoT 保持一致的模型。

##### **Federated Learning with Limited Node Labels**
2406.12435v1 by Bisheng Tang, Xiaojun Chen, Shaopu Wang, Yuexin Xuan, Zhendong Zhao

Subgraph federated learning (SFL) is a research methodology that has gained
significant attention for its potential to handle distributed graph-structured
data. In SFL, the local model comprises graph neural networks (GNNs) with a
partial graph structure. However, some SFL models have overlooked the
significance of missing cross-subgraph edges, which can lead to local GNNs
being unable to message-pass global representations to other parties' GNNs.
Moreover, existing SFL models require substantial labeled data, which limits
their practical applications. To overcome these limitations, we present a novel
SFL framework called FedMpa that aims to learn cross-subgraph node
representations. FedMpa first trains a multilayer perceptron (MLP) model using
a small amount of data and then propagates the federated feature to the local
structures. To further improve the embedding representation of nodes with local
subgraphs, we introduce the FedMpae method, which reconstructs the local graph
structure with an innovation view that applies pooling operation to form
super-nodes. Our extensive experiments on six graph datasets demonstrate that
FedMpa is highly effective in node classification. Furthermore, our ablation
experiments verify the effectiveness of FedMpa.

摘要：子圖聯合學習 (SFL) 是一種研究方法，因其處理分散式圖形結構數據的潛力而備受關注。在 SFL 中，局部模型包含具有部分圖形結構的圖形神經網絡 (GNN)。然而，一些 SFL 模型忽視了缺少跨子圖邊緣的重要性，這可能導致局部 GNN 無法將全局表示訊息傳遞給其他方的 GNN。此外，現有的 SFL 模型需要大量的標籤數據，這限制了它們的實際應用。為了克服這些限制，我們提出了一個名為 FedMpa 的新穎 SFL 框架，旨在學習跨子圖節點表示。FedMpa 首先使用少量數據訓練多層感知器 (MLP) 模型，然後將聯合特徵傳播到局部結構。為了進一步改善具有局部子圖的節點的嵌入表示，我們引入了 FedMpae 方法，該方法通過應用池化操作形成超級節點來重建局部圖形結構。我們在六個圖形數據集上進行的廣泛實驗表明，FedMpa 在節點分類中非常有效。此外，我們的消融實驗驗證了 FedMpa 的有效性。

##### **PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers**
2406.12430v1 by Myeonghwa Lee, Seonho An, Min-Soo Kim

In this paper, we conduct a study to utilize LLMs as a solution for decision
making that requires complex data analysis. We define Decision QA as the task
of answering the best decision, $d_{best}$, for a decision-making question $Q$,
business rules $R$ and a database $D$. Since there is no benchmark that can
examine Decision QA, we propose Decision QA benchmark, DQA. It has two
scenarios, Locating and Building, constructed from two video games (Europa
Universalis IV and Victoria 3) that have almost the same goal as Decision QA.
To address Decision QA effectively, we also propose a new RAG technique called
the iterative plan-then-retrieval augmented generation (PlanRAG). Our
PlanRAG-based LM generates the plan for decision making as the first step, and
the retriever generates the queries for data analysis as the second step. The
proposed method outperforms the state-of-the-art iterative RAG method by 15.8%
in the Locating scenario and by 7.4% in the Building scenario, respectively. We
release our code and benchmark at https://github.com/myeon9h/PlanRAG.

摘要：<paragraph>在本文中，我們進行一項研究，利用 LLM 作為決策的解決方案，而決策需要複雜的數據分析。我們將決策問答定義為回答最佳決策 $d_{best}$，針對決策問題 $Q$、業務規則 $R$ 和資料庫 $D$。由於沒有基準可以檢驗決策問答，因此我們提出決策問答基準，即 DQA。它有兩個情境，定位和建立，建構自兩個電子遊戲（歐陸風雲 IV 和 Victoria 3），它們的目標與決策問答幾乎相同。為了有效解決決策問答，我們也提出一個新的 RAG 技術，稱為反覆規劃後檢索擴充生成 (PlanRAG)。我們的基於 PlanRAG 的 LM 在第一步產生決策規劃，而檢索器在第二步產生資料分析的查詢。所提出的方法在定位情境中比最先進的反覆 RAG 方法高出 15.8%，在建立情境中高出 7.4%。我們在 https://github.com/myeon9h/PlanRAG 釋出我們的程式碼和基準。</paragraph>

##### **PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems**
2406.12428v1 by Kentaro Mitsui, Koh Mitsuda, Toshiaki Wakatsuki, Yukiya Hono, Kei Sawada

Multimodal language models that process both text and speech have a potential
for applications in spoken dialogue systems. However, current models face two
major challenges in response generation latency: (1) generating a spoken
response requires the prior generation of a written response, and (2) speech
sequences are significantly longer than text sequences. This study addresses
these issues by extending the input and output sequences of the language model
to support the parallel generation of text and speech. Our experiments on
spoken question answering tasks demonstrate that our approach improves latency
while maintaining the quality of response content. Additionally, we show that
latency can be further reduced by generating speech in multiple sequences. Demo
samples are available at https://rinnakk.github.io/research/publications/PSLM.

摘要：具備處理文字和語音的多模態語言模型，在對話系統中具有潛在的應用。然而，目前的模型在回應產生延遲方面面臨兩項重大挑戰：(1) 產生語音回應需要先產生書面回應，(2) 語音序列顯著長於文字序列。本研究透過擴充語言模型的輸入和輸出序列來解決這些問題，以支援文字和語音的並行產生。我們在口說問答任務上的實驗證明，我們的做法改善了延遲，同時維持回應內容的品質。此外，我們證明透過產生多個序列的語音可以進一步降低延遲。示範範例可於 https://rinnakk.github.io/research/publications/PSLM 取得。

##### **Open-Source Web Service with Morphological Dictionary-Supplemented Deep Learning for Morphosyntactic Analysis of Czech**
2406.12422v1 by Milan Straka, Jana Straková

We present an open-source web service for Czech morphosyntactic analysis. The
system combines a deep learning model with rescoring by a high-precision
morphological dictionary at inference time. We show that our hybrid method
surpasses two competitive baselines: While the deep learning model ensures
generalization for out-of-vocabulary words and better disambiguation, an
improvement over an existing morphological analyser MorphoDiTa, at the same
time, the deep learning model benefits from inference-time guidance of a
manually curated morphological dictionary. We achieve 50% error reduction in
lemmatization and 58% error reduction in POS tagging over MorphoDiTa, while
also offering dependency parsing. The model is trained on one of the currently
largest Czech morphosyntactic corpora, the PDT-C 1.0, with the trained models
available at https://hdl.handle.net/11234/1-5293. We provide the tool as a web
service deployed at https://lindat.mff.cuni.cz/services/udpipe/. The source
code is available at GitHub (https://github.com/ufal/udpipe/tree/udpipe-2),
along with a Python client for a simple use. The documentation for the models
can be found at https://ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model.

摘要：<paragraph>我們提供一個捷克形態句法分析的開源網路服務。系統結合深度學習模型，並在推論時間透過高精確度形態詞典重新評分。我們顯示我們的混合方法超越兩個競爭基線：深度學習模型確保非詞彙表單詞彙的概化和更好的消歧義，同時改善現有的形態分析器 MorphoDiTa，深度學習模型從人工策劃的形態詞典推論時間指導中受益。我們在詞形還原中實現 50% 的錯誤減少，在詞性標記中實現 58% 的錯誤減少，同時也提供依賴解析。模型訓練於目前最大的捷克形態句法語料庫之一 PDT-C 1.0，訓練好的模型可在 https://hdl.handle.net/11234/1-5293 取得。我們提供工具作為網路服務，部署於 https://lindat.mff.cuni.cz/services/udpipe/。原始碼可在 GitHub（https://github.com/ufal/udpipe/tree/udpipe-2）取得，以及一個簡單使用的 Python 程式碼。模型的說明文件可在 https://ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model 取得。</paragraph>

##### **MMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling**
2406.12420v1 by Philipp Seeberger, Dominik Wagner, Korbinian Riedhammer

With the advancement of multimedia technologies, news documents and
user-generated content are often represented as multiple modalities, making
Multimedia Event Extraction (MEE) an increasingly important challenge. However,
recent MEE methods employ weak alignment strategies and data augmentation with
simple classification models, which ignore the capabilities of natural
language-formulated event templates for the challenging Event Argument
Extraction (EAE) task. In this work, we focus on EAE and address this issue by
introducing a unified template filling model that connects the textual and
visual modalities via textual prompts. This approach enables the exploitation
of cross-ontology transfer and the incorporation of event-specific semantics.
Experiments on the M2E2 benchmark demonstrate the effectiveness of our
approach. Our system surpasses the current SOTA on textual EAE by +7% F1, and
performs generally better than the second-best systems for multimedia EAE.

摘要：隨著多媒體技術的進步，新聞文件和使用者產生的內容經常以多種形式呈現，讓多媒體事件萃取 (MEE) 成為日益重要的挑戰。然而，最近的 MEE 方法採用了弱對齊策略和資料擴充與簡單分類模型，忽略了自然語言公式化事件範本在具挑戰性的事件論元萃取 (EAE) 任務中的能力。在這項工作中，我們專注於 EAE，並透過引入一個統一的範本填寫模型來解決這個問題，該模型透過文字提示將文字和視覺模式連接起來。這種方法可以利用跨本體轉移和納入事件特定語義。在 M2E2 基準上的實驗證明了我們方法的有效性。我們的系統在文字 EAE 上超越了目前的 SOTA，F1 分數提高了 +7%，並且在多媒體 EAE 上的表現普遍優於第二好的系統。

##### **Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models**
2406.12416v1 by Hongbang Yuan, Yubo Chen, Pengfei Cao, Zhuoran Jin, Kang Liu, Jun Zhao

Large language models (LLMs) have achieved remarkable success but still tend
to generate factually erroneous responses, a phenomenon known as hallucination.
A recent trend is to use preference learning to fine-tune models to align with
factuality. However, existing work primarily evaluates fine-tuned models on
in-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets
remains underexplored. In this paper, we conduct a comprehensive evaluation of
the factuality of different models tuned by various preference learning
algorithms and demonstrate that their performance on OOD datasets either
increases minimally or decreases. Subsequently, we reveal that the main cause
of model's failure to uphold factuality under a distribution shift is
\textbf{under-alignment}, rather than \textbf{over-alignment}, by analyzing the
token distribution shift of the models before and after tuning. Finally, we
propose \textbf{APEFT} (\textbf{A}tomic \textbf{P}reference \textbf{E}nhanced
\textbf{F}actuality \textbf{T}uning), a framework that enhances model's
awareness of factuality at the granularity of individual facts. Extensive
experiments demonstrate that APEFT improves model performance by an average of
$\boldsymbol{3.45\%}$ on both ID and OOD datasets, which is highly effective.

摘要：大型語言模型 (LLM) 已經取得顯著的成功，但仍傾向於產生事實錯誤的回應，這是一種稱為幻覺的現象。最近的趨勢是使用偏好學習來微調模型以符合事實。然而，現有的工作主要在域內 (ID) 資料集上評估微調模型，而域外 (OOD) 資料集上的事實性仍未得到充分探討。在本文中，我們對由各種偏好學習演算法調整的不同模型的事實性進行了全面的評估，並證明它們在 OOD 資料集上的效能不是最低限度地增加就是減少。隨後，我們揭示了模型無法在分佈轉移下維持事實性的主要原因是「對齊不足」，而不是「對齊過度」，方法是分析模型在調整前後的符號分佈轉移。最後，我們提出「APEFT」（原子偏好增強事實調整），這是一個框架，它增強了模型對個別事實粒度的事實認識。大量的實驗證明，APEFT 在 ID 和 OOD 資料集上平均提高了模型效能 $\boldsymbol{3.45\%}$，這非常有效。

##### **PDSS: A Privacy-Preserving Framework for Step-by-Step Distillation of Large Language Models**
2406.12403v1 by Tao Fan, Yan Kang, Weijing Chen, Hanlin Gu, Yuanfeng Song, Lixin Fan, Kai Chen, Qiang Yang

In the context of real-world applications, leveraging large language models
(LLMs) for domain-specific tasks often faces two major challenges:
domain-specific knowledge privacy and constrained resources. To address these
issues, we propose PDSS, a privacy-preserving framework for step-by-step
distillation of LLMs. PDSS works on a server-client architecture, wherein
client transmits perturbed prompts to the server's LLM for rationale
generation. The generated rationales are then decoded by the client and used to
enrich the training of task-specific small language model(SLM) within a
multi-task learning paradigm. PDSS introduces two privacy protection
strategies: the Exponential Mechanism Strategy and the Encoder-Decoder
Strategy, balancing prompt privacy and rationale usability. Experiments
demonstrate the effectiveness of PDSS in various text generation tasks,
enabling the training of task-specific SLM with enhanced performance while
prioritizing data privacy protection.

摘要：在現實世界應用的背景下，利用大型語言模型 (LLM) 執行特定領域任務時，通常會面臨兩項重大挑戰：特定領域的知識隱私和受限資源。為了解決這些問題，我們提出了 PDSS，一個用於 LLM 分步萃取的隱私保護架構。PDSS 採用伺服器 - 客戶端架構，其中客戶端將擾動提示傳送至伺服器的 LLM 以產生依據。然後，客戶端會解碼產生的依據，並用於豐富多任務學習範例中特定任務小型語言模型 (SLM) 的訓練。PDSS 導入了兩種隱私保護策略：指數機制策略和編碼器 - 解碼器策略，平衡提示隱私和依據可用性。實驗證明了 PDSS 在各種文字產生任務中的有效性，讓訓練特定任務的 SLM 能夠增強效能，同時優先考量資料隱私保護。

##### **Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling**
2406.12402v1 by Irfan Robbani, Paul Reisert, Naoya Inoue, Surawat Pothong, Camélia Guerraoui, Wenzhi Wang, Shoichi Naito, Jungmin Choi, Kentaro Inui

Prior research in computational argumentation has mainly focused on scoring
the quality of arguments, with less attention on explicating logical errors. In
this work, we introduce four sets of explainable templates for common informal
logical fallacies designed to explicate a fallacy's implicit logic. Using our
templates, we conduct an annotation study on top of 400 fallacious arguments
taken from LOGIC dataset and achieve a high agreement score (Krippendorf's
alpha of 0.54) and reasonable coverage (0.83). Finally, we conduct an
experiment for detecting the structure of fallacies and discover that
state-of-the-art language models struggle with detecting fallacy templates
(0.47 accuracy). To facilitate research on fallacies, we make our dataset and
guidelines publicly available.

摘要：先前的計算論證研究主要集中在評分
論證的品質，較少關注解釋邏輯謬誤。在
這項工作中，我們介紹了四組可解釋的範本，用於常見的非正式
邏輯謬誤，旨在解釋謬誤的隱含邏輯。使用我們的
範本，我們對來自 LOGIC 資料集的 400 個謬誤論證進行註釋研究，並獲得高一致性分數（Krippendorf
的 alpha 為 0.54）和合理的涵蓋範圍（0.83）。最後，我們進行了一項
實驗，用於檢測謬誤的結構，並發現
最先進的語言模型難以檢測謬誤範本
（0.47 準確度）。為了促進對謬誤的研究，我們公開了我們的資料集和
指南。

##### **A Cutting-Edge Deep Learning Method For Enhancing IoT Security**
2406.12400v1 by Nadia Ansar, Mohammad Sadique Ansari, Mohammad Sharique, Aamina Khatoon, Md Abdul Malik, Md Munir Siddiqui

There have been significant issues given the IoT, with heterogeneity of
billions of devices and with a large amount of data. This paper proposed an
innovative design of the Internet of Things (IoT) Environment Intrusion
Detection System (or IDS) using Deep Learning-integrated Convolutional Neural
Networks (CNN) and Long Short-Term Memory (LSTM) networks. Our model, based on
the CICIDS2017 dataset, achieved an accuracy of 99.52% in classifying network
traffic as either benign or malicious. The real-time processing capability,
scalability, and low false alarm rate in our model surpass some traditional IDS
approaches and, therefore, prove successful for application in today's IoT
networks. The development and the performance of the model, with possible
applications that may extend to other related fields of adaptive learning
techniques and cross-domain applicability, are discussed. The research
involving deep learning for IoT cybersecurity offers a potent solution for
significantly improving network security.

摘要：由於物聯網的異質性，包含數十億的裝置和大量資料，因此產生了重大的問題。本文提出了一種創新的物聯網 (IoT) 環境入侵偵測系統 (IDS) 設計，使用深度學習整合的卷積神經網路 (CNN) 和長期短期記憶 (LSTM) 網路。我們的模型基於 CICIDS2017 資料集，在將網路流量分類為良性或惡意時達到了 99.52% 的準確度。我們的模型中的即時處理能力、可擴充性和低誤報率超越了一些傳統的 IDS 方法，因此證明了成功應用於當今的 IoT 網路。討論了模型的開發和效能，以及可能擴展到自適應學習技術和其他相關領域的應用和跨網域應用。涉及物聯網網路安全的深度學習研究為顯著改善網路安全提供了強有力的解決方案。

