{"2411.17030": {"publish_time": "2024-11-26", "title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks", "paper_summary": "We introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D\nrepresentation model pre-trained on large-scale 3D-language dataset for\nembodied tasks. Our g3D-LF processes posed RGB-D images from agents to encode\nfeature fields for: 1) Novel view representation predictions from any position\nin the 3D scene; 2) Generations of BEV maps centered on the agent; 3) Querying\ntargets using multi-granularity language within the above-mentioned\nrepresentations. Our representation can be generalized to unseen environments,\nenabling real-time construction and dynamic updates. By volume rendering latent\nfeatures along sampled rays and integrating semantic and spatial relationships\nthrough multiscale encoders, our g3D-LF produces representations at different\nscales and perspectives, aligned with multi-granularity language, via\nmulti-level contrastive learning. Furthermore, we prepare a large-scale\n3D-language dataset to align the representations of the feature fields with\nlanguage. Extensive experiments on Vision-and-Language Navigation under both\nPanorama and Monocular settings, Zero-shot Object Navigation, and Situated\nQuestion Answering tasks highlight the significant advantages and effectiveness\nof our g3D-LF for embodied tasks.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86\u901a\u7528 3D \u8a9e\u8a00\u7279\u5fb5\u5834 (g3D-LF)\uff0c\u4e00\u7a2e\u91dd\u5c0d\u9ad4\u73fe\u4efb\u52d9\u9810\u5148\u8a13\u7df4\u65bc\u5927\u578b 3D \u8a9e\u8a00\u8cc7\u6599\u96c6\u4e0a\u7684 3D \u8868\u793a\u6a21\u578b\u3002\u6211\u5011\u7684 g3D-LF \u8655\u7406\u4f86\u81ea\u4ee3\u7406\u7684\u59ff\u52e2 RGB-D \u5f71\u50cf\uff0c\u4ee5\u7de8\u78bc\u7279\u5fb5\u5834\uff1a1) \u5f9e 3D \u5834\u666f\u4e2d\u7684\u4efb\u4f55\u4f4d\u7f6e\u9032\u884c\u65b0\u7a4e\u8996\u5716\u8868\u793a\u9810\u6e2c\uff1b2) \u4ee5\u4ee3\u7406\u70ba\u4e2d\u5fc3\u7684 BEV \u5730\u5716\u751f\u6210\uff1b3) \u5728\u4e0a\u8ff0\u8868\u793a\u4e2d\u4f7f\u7528\u591a\u7c92\u5ea6\u8a9e\u8a00\u67e5\u8a62\u76ee\u6a19\u3002\u6211\u5011\u7684\u8868\u793a\u53ef\u4ee5\u63a8\u5ee3\u5230\u672a\u898b\u904e\u7684\u74b0\u5883\uff0c\u5be6\u73fe\u5373\u6642\u5efa\u69cb\u548c\u52d5\u614b\u66f4\u65b0\u3002\u901a\u904e\u6cbf\u63a1\u6a23\u5149\u7dda\u9ad4\u7a4d\u6e32\u67d3\u6f5b\u5728\u7279\u5fb5\uff0c\u4e26\u900f\u904e\u591a\u5c3a\u5ea6\u7de8\u78bc\u5668\u6574\u5408\u8a9e\u7fa9\u548c\u7a7a\u9593\u95dc\u4fc2\uff0c\u6211\u5011\u7684 g3D-LF \u900f\u904e\u591a\u5c64\u5c0d\u6bd4\u5b78\u7fd2\uff0c\u7522\u751f\u4e0d\u540c\u5c3a\u5ea6\u548c\u89c0\u9ede\u7684\u8868\u793a\uff0c\u8207\u591a\u7c92\u5ea6\u8a9e\u8a00\u5c0d\u9f4a\u3002\u6b64\u5916\uff0c\u6211\u5011\u6e96\u5099\u4e86\u4e00\u500b\u5927\u578b 3D \u8a9e\u8a00\u8cc7\u6599\u96c6\uff0c\u4ee5\u5c07\u7279\u5fb5\u5834\u7684\u8868\u793a\u8207\u8a9e\u8a00\u5c0d\u9f4a\u3002\u5728\u5168\u666f\u548c\u55ae\u773c\u8a2d\u5b9a\u3001\u96f6\u6b21\u7269\u9ad4\u5c0e\u822a\u4ee5\u53ca\u60c5\u5883\u554f\u7b54\u4efb\u52d9\u4e0b\u7684\u8996\u89ba\u548c\u8a9e\u8a00\u5c0e\u822a\u4efb\u52d9\u7684\u5ee3\u6cdb\u5be6\u9a57\u7a81\u986f\u4e86\u6211\u5011\u7684 g3D-LF \u5728\u9ad4\u73fe\u4efb\u52d9\u4e2d\u7684\u986f\u8457\u512a\u52e2\u548c\u6709\u6548\u6027\u3002", "author": "Zihan Wang et.al.", "authors": "Zihan Wang, Gim Hee Lee", "id": "2411.17030v1", "paper_url": "http://arxiv.org/abs/2411.17030v1", "repo": "https://github.com/MrZihan/g3D-LF"}}