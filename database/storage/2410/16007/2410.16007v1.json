{"2410.16007": {"publish_time": "2024-10-21", "title": "Are Language Model Logits Calibrated?", "paper_summary": "Some information is factual (e.g., \"Paris is in France\"), whereas other\ninformation is probabilistic (e.g., \"the coin flip will be a [Heads/Tails].\").\nWe believe that good Language Models (LMs) should understand and reflect this\nnuance. Our work investigates this by testing if LMs' output probabilities are\ncalibrated to their textual contexts. We define model \"calibration\" as the\ndegree to which the output probabilities of candidate tokens are aligned with\nthe relative likelihood that should be inferred from the given context. For\nexample, if the context concerns two equally likely options (e.g., heads or\ntails for a fair coin), the output probabilities should reflect this. Likewise,\ncontext that concerns non-uniformly likely events (e.g., rolling a six with a\ndie) should also be appropriately captured with proportionate output\nprobabilities. We find that even in simple settings the best LMs (1) are poorly\ncalibrated, and (2) have systematic biases (e.g., preferred colors and\nsensitivities to word orderings). For example, gpt-4o-mini often picks the\nfirst of two options presented in the prompt regardless of the options' implied\nlikelihood, whereas Llama-3.1-8B picks the second. Our other consistent finding\nis mode-collapse: Instruction-tuned models often over-allocate probability mass\non a single option. These systematic biases introduce non-intuitive model\nbehavior, making models harder for users to understand.", "paper_summary_zh": "\u67d0\u4e9b\u8cc7\u8a0a\u662f\u4e8b\u5be6\uff08\u4f8b\u5982\uff1a\u300c\u5df4\u9ece\u5728\u6cd5\u570b\u300d\uff09\uff0c\u800c\u5176\u4ed6\u8cc7\u8a0a\u5247\u662f\u6a5f\u7387\u6027\u7684\uff08\u4f8b\u5982\uff1a\u300c\u62cb\u786c\u5e63\u6703\u662f [\u6b63\u9762/\u53cd\u9762]\u300d\uff09\u3002\u6211\u5011\u76f8\u4fe1\u826f\u597d\u7684\u8a9e\u8a00\u6a21\u578b (LM) \u61c9\u7406\u89e3\u4e26\u53cd\u6620\u9019\u7a2e\u5dee\u7570\u3002\u6211\u5011\u7684\u7814\u7a76\u900f\u904e\u6e2c\u8a66\u8a9e\u8a00\u6a21\u578b\u7684\u8f38\u51fa\u6a5f\u7387\u662f\u5426\u6821\u6e96\u81f3\u5176\u6587\u5b57\u8108\u7d61\u4f86\u63a2\u8a0e\u9019\u4e00\u9ede\u3002\u6211\u5011\u5c07\u6a21\u578b\u300c\u6821\u6e96\u300d\u5b9a\u7fa9\u70ba\u5019\u9078\u8a5e\u5f59\u7684\u8f38\u51fa\u6a5f\u7387\u8207\u61c9\u5f9e\u7d66\u5b9a\u8108\u7d61\u63a8\u8ad6\u51fa\u7684\u76f8\u5c0d\u53ef\u80fd\u6027\u4e00\u81f4\u7684\u7a0b\u5ea6\u3002\u4f8b\u5982\uff0c\u5982\u679c\u8108\u7d61\u6d89\u53ca\u5169\u500b\u53ef\u80fd\u6027\u76f8\u7b49\u7684\u9078\u9805\uff08\u4f8b\u5982\u516c\u5e73\u786c\u5e63\u7684\u6b63\u9762\u6216\u53cd\u9762\uff09\uff0c\u8f38\u51fa\u6a5f\u7387\u61c9\u53cd\u6620\u9019\u4e00\u9ede\u3002\u540c\u6a23\u5730\uff0c\u6d89\u53ca\u53ef\u80fd\u6027\u4e0d\u5747\u7b49\u7684\u4e8b\u4ef6\uff08\u4f8b\u5982\u7528\u9ab0\u5b50\u64f2\u51fa\u516d\u9ede\uff09\u7684\u8108\u7d61\u4e5f\u61c9\u900f\u904e\u76f8\u61c9\u7684\u8f38\u51fa\u6a5f\u7387\u9069\u7576\u5730\u6355\u6349\u3002\u6211\u5011\u767c\u73fe\uff0c\u5373\u4f7f\u5728\u7c21\u55ae\u7684\u8a2d\u5b9a\u4e2d\uff0c\u6700\u4f73\u7684\u8a9e\u8a00\u6a21\u578b (1) \u6821\u6e96\u4e0d\u826f\uff0c\u4e14 (2) \u5177\u6709\u7cfb\u7d71\u6027\u504f\u5dee\uff08\u4f8b\u5982\uff0c\u504f\u597d\u7684\u984f\u8272\u548c\u5c0d\u5b57\u5e8f\u7684\u654f\u611f\u6027\uff09\u3002\u4f8b\u5982\uff0cgpt-4o-mini \u901a\u5e38\u6703\u9078\u64c7\u63d0\u793a\u4e2d\u5448\u73fe\u7684\u5169\u500b\u9078\u9805\u4e2d\u7684\u7b2c\u4e00\u500b\uff0c\u800c\u4e0d\u7ba1\u9078\u9805\u6697\u793a\u7684\u53ef\u80fd\u6027\u5982\u4f55\uff0c\u800c Llama-3.1-8B \u5247\u6703\u9078\u64c7\u7b2c\u4e8c\u500b\u3002\u6211\u5011\u53e6\u4e00\u500b\u4e00\u81f4\u7684\u767c\u73fe\u662f\u6a21\u5f0f\u5d29\u6f70\uff1a\u7d93\u904e\u6307\u4ee4\u5fae\u8abf\u7684\u6a21\u578b\u901a\u5e38\u6703\u904e\u5ea6\u5206\u914d\u55ae\u4e00\u9078\u9805\u7684\u6a5f\u7387\u8cea\u91cf\u3002\u9019\u4e9b\u7cfb\u7d71\u6027\u504f\u5dee\u6703\u5c0e\u81f4\u975e\u76f4\u89ba\u7684\u6a21\u578b\u884c\u70ba\uff0c\u8b93\u4f7f\u7528\u8005\u66f4\u96e3\u7406\u89e3\u6a21\u578b\u3002", "author": "Charles Lovering et.al.", "authors": "Charles Lovering, Michael Krumdick, Viet Dac Lai, Nilesh Kumar, Varshini Reddy, Rik Koncel-Kedziorski, Chris Tanner", "id": "2410.16007v1", "paper_url": "http://arxiv.org/abs/2410.16007v1", "repo": "null"}}