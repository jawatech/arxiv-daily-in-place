{"2408.16768": {"publish_time": "2024-08-29", "title": "SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners", "paper_summary": "We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 SAM2Point\uff0c\u9019\u662f\u4e00\u7a2e\u521d\u6b65\u63a2\u7d22\uff0c\u5c07 Segment Anything Model 2 (SAM 2) \u6539\u7de8\u70ba\u96f6\u6b21\u5b78\u7fd2\u548c\u53ef\u63d0\u793a\u7684 3D \u5206\u5272\u3002SAM2Point \u5c07\u4efb\u4f55 3D \u8cc7\u6599\u8a6e\u91cb\u70ba\u4e00\u7cfb\u5217\u591a\u65b9\u5411\u5f71\u7247\uff0c\u4e26\u5229\u7528 SAM 2 \u9032\u884c 3D \u7a7a\u9593\u5206\u5272\uff0c\u800c\u7121\u9700\u9032\u4e00\u6b65\u8a13\u7df4\u6216 2D-3D \u6295\u5f71\u3002\u6211\u5011\u7684\u67b6\u69cb\u652f\u63f4\u5404\u7a2e\u63d0\u793a\u985e\u578b\uff0c\u5305\u62ec 3D \u9ede\u3001\u65b9\u584a\u548c\u906e\u7f69\uff0c\u4e26\u4e14\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u5834\u666f\u4e2d\u6cdb\u5316\uff0c\u4f8b\u5982 3D \u7269\u4ef6\u3001\u5ba4\u5167\u5834\u666f\u3001\u6236\u5916\u74b0\u5883\u548c\u539f\u59cb\u7a00\u758f LiDAR\u3002\u5728\u591a\u500b 3D \u8cc7\u6599\u96c6\u4e0a\u7684\u793a\u7bc4\uff0c\u4f8b\u5982 Objaverse\u3001S3DIS\u3001ScanNet\u3001Semantic3D \u548c KITTI\uff0c\u7a81\u51fa\u4e86 SAM2Point \u7684\u5f37\u5927\u6cdb\u5316\u80fd\u529b\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u6211\u5011\u5c55\u793a\u4e86 SAM \u5728 3D \u4e2d\u6700\u5fe0\u5be6\u7684\u5be6\u4f5c\uff0c\u9019\u53ef\u4ee5\u4f5c\u70ba\u672a\u4f86\u53ef\u63d0\u793a 3D \u5206\u5272\u7814\u7a76\u7684\u8d77\u9ede\u3002\u7dda\u4e0a\u793a\u7bc4\uff1ahttps://huggingface.co/spaces/ZiyuG/SAM2Point\u3002\u7a0b\u5f0f\u78bc\uff1ahttps://github.com/ZiyuGuo99/SAM2Point\u3002", "author": "Ziyu Guo et.al.", "authors": "Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Chengzhuo Tong, Peng Gao, Chunyuan Li, Pheng-Ann Heng", "id": "2408.16768v1", "paper_url": "http://arxiv.org/abs/2408.16768v1", "repo": "https://github.com/ziyuguo99/sam2point"}}