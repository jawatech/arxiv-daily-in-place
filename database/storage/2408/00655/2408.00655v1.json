{"2408.00655": {"publish_time": "2024-08-01", "title": "SentenceVAE: Faster, Longer and More Accurate Inference with Next-sentence Prediction for Large Language Models", "paper_summary": "Contemporary large language models (LLMs) predominantly utilize a next-token\nprediction method for inference, which significantly impedes their processing\nspeed. In this paper, we introduce a novel inference methodology termed\nnext-sentence prediction, aimed at enhancing the inference efficiency of LLMs.\nWe present SentenceVAE, a tiny model consisting of an encoder and a decoder.\nThe encoder effectively condenses the information within a sentence into a\nsingular token, while the decoder reconstructs this compressed data back into\nits original sentential form. By integrating SentenceVAE into the input and\noutput layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference approach, markedly accelerating inference\nspeeds. SentenceVAE also maintains the integrity of the original semantic\ncontent by segmenting the text into sentences, thereby preserving accuracy\nwhile boosting inference speeds. Compared to traditional LLMs, SLLMs process\nfewer tokens over equivalent context lengths, significantly reducing memory\ndemands for Self-Attention computations and facilitating the handling of longer\ncontexts. Our experimental findings reveal that this method can increase\ninference speeds by 204~365%, reduce perplexity (PPL) to 46~75% of its original\nmetric, and decrease memory overhead by 86~91% for the same context length. The\nadvantages of this approach are further amplified with increases in model\nparameters.", "paper_summary_zh": "\u7576\u4ee3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e3b\u8981\u5229\u7528\u4e0b\u4e00\u500b\u4ee3\u78bc\u9810\u6e2c\u65b9\u6cd5\u9032\u884c\u63a8\u8ad6\uff0c\u9019\u986f\u8457\u5730\u963b\u7919\u4e86\u5b83\u5011\u7684\u8655\u7406\u901f\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7684\u63a8\u8ad6\u65b9\u6cd5\uff0c\u7a31\u70ba\u4e0b\u4e00\u500b\u53e5\u5b50\u9810\u6e2c\uff0c\u65e8\u5728\u63d0\u9ad8 LLM \u7684\u63a8\u8ad6\u6548\u7387\u3002\u6211\u5011\u63d0\u51fa\u4e86 SentenceVAE\uff0c\u9019\u662f\u4e00\u500b\u7531\u7de8\u78bc\u5668\u548c\u89e3\u78bc\u5668\u7d44\u6210\u7684\u5c0f\u6a21\u578b\u3002\u7de8\u78bc\u5668\u6709\u6548\u5730\u5c07\u53e5\u5b50\u4e2d\u7684\u8cc7\u8a0a\u58d3\u7e2e\u6210\u4e00\u500b\u55ae\u4e00\u4ee3\u78bc\uff0c\u800c\u89e3\u78bc\u5668\u5c07\u9019\u4e9b\u58d3\u7e2e\u8cc7\u6599\u89e3\u69cb\u56de\u5176\u539f\u59cb\u7684\u53e5\u5b50\u5f62\u5f0f\u3002\u900f\u904e\u5c07 SentenceVAE \u6574\u5408\u5230 LLM \u7684\u8f38\u5165\u548c\u8f38\u51fa\u5c64\uff0c\u6211\u5011\u958b\u767c\u4e86\u53e5\u5b50\u5c64\u7d1a LLM (SLLM)\uff0c\u5b83\u63a1\u7528\u9010\u53e5\u63a8\u8ad6\u65b9\u6cd5\uff0c\u986f\u8457\u5730\u52a0\u901f\u4e86\u63a8\u8ad6\u901f\u5ea6\u3002SentenceVAE \u4e5f\u900f\u904e\u5c07\u6587\u5b57\u5206\u6bb5\u6210\u53e5\u5b50\u4f86\u7dad\u8b77\u539f\u59cb\u8a9e\u7fa9\u5167\u5bb9\u7684\u5b8c\u6574\u6027\uff0c\u5f9e\u800c\u63d0\u5347\u63a8\u8ad6\u901f\u5ea6\u7684\u540c\u6642\u4e5f\u7dad\u6301\u6e96\u78ba\u6027\u3002\u8207\u50b3\u7d71\u7684 LLM \u76f8\u6bd4\uff0cSLLM \u8655\u7406\u8f03\u5c11\u4ee3\u78bc\uff0c\u4f46\u5177\u6709\u7b49\u6548\u7684\u5167\u5bb9\u9577\u5ea6\uff0c\u5927\u5e45\u6e1b\u5c11\u81ea\u6ce8\u610f\u529b\u8a08\u7b97\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u4e26\u4fc3\u9032\u8655\u7406\u8f03\u9577\u7684\u5167\u5bb9\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u9019\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u5c07\u63a8\u8ad6\u901f\u5ea6\u63d0\u5347 204~365%\uff0c\u5c07\u56f0\u60d1\u5ea6 (PPL) \u964d\u4f4e\u5230\u5176\u539f\u59cb\u6307\u6a19\u7684 46~75%\uff0c\u4e26\u5728\u76f8\u540c\u7684\u5167\u5bb9\u9577\u5ea6\u4e0b\u5c07\u8a18\u61b6\u9ad4\u958b\u92b7\u6e1b\u5c11 86~91%\u3002\u96a8\u8457\u6a21\u578b\u53c3\u6578\u7684\u589e\u52a0\uff0c\u9019\u7a2e\u65b9\u6cd5\u7684\u512a\u52e2\u9032\u4e00\u6b65\u64f4\u5927\u3002", "author": "Hongjun An et.al.", "authors": "Hongjun An, Yifan Chen, Xiaozhen Qiao, Zhe Sun, Xuelong Li", "id": "2408.00655v1", "paper_url": "http://arxiv.org/abs/2408.00655v1", "repo": "null"}}