{"2412.07112": {"publish_time": "2024-12-10", "title": "Maya: An Instruction Finetuned Multilingual Multimodal Model", "paper_summary": "The rapid development of large Vision-Language Models (VLMs) has led to\nimpressive results on academic benchmarks, primarily in widely spoken\nlanguages. However, significant gaps remain in the ability of current VLMs to\nhandle low-resource languages and varied cultural contexts, largely due to a\nlack of high-quality, diverse, and safety-vetted data. Consequently, these\nmodels often struggle to understand low-resource languages and cultural nuances\nin a manner free from toxicity. To address these limitations, we introduce\nMaya, an open-source Multimodal Multilingual model. Our contributions are\nthreefold: 1) a multilingual image-text pretraining dataset in eight languages,\nbased on the LLaVA pretraining dataset; 2) a thorough analysis of toxicity\nwithin the LLaVA dataset, followed by the creation of a novel toxicity-free\nversion across eight languages; and 3) a multilingual image-text model\nsupporting these languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7684\u5feb\u901f\u767c\u5c55\u5df2\u5728\u5b78\u8853\u57fa\u6e96\u4e0a\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u679c\uff0c\u4e3b\u8981\u662f\u5728\u5ee3\u6cdb\u4f7f\u7528\u7684\u8a9e\u8a00\u4e2d\u3002\u7136\u800c\uff0c\u7531\u65bc\u7f3a\u4e4f\u9ad8\u54c1\u8cea\u3001\u591a\u6a23\u5316\u4e14\u7d93\u904e\u5b89\u5168\u5be9\u67e5\u7684\u8cc7\u6599\uff0c\u76ee\u524d VLM \u5728\u8655\u7406\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u548c\u5404\u7a2e\u6587\u5316\u80cc\u666f\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u6709\u986f\u8457\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u9019\u4e9b\u6a21\u578b\u5e38\u5e38\u96e3\u4ee5\u7406\u89e3\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u548c\u6587\u5316\u7d30\u5fae\u5dee\u5225\uff0c\u800c\u4e14\u7121\u6cd5\u907f\u514d\u6709\u6bd2\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 Maya\uff0c\u4e00\u500b\u958b\u653e\u539f\u59cb\u78bc\u7684\u591a\u6a21\u614b\u591a\u8a9e\u8a00\u6a21\u578b\u3002\u6211\u5011\u7684\u8ca2\u737b\u6709\u4e09\u65b9\u9762\uff1a1) \u4e00\u500b\u57fa\u65bc LLaVA \u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u7684\u516b\u7a2e\u8a9e\u8a00\u7684\u591a\u8a9e\u8a00\u5716\u50cf\u6587\u5b57\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\uff1b2) \u5c0d LLaVA \u8cc7\u6599\u96c6\u4e2d\u7684\u6bd2\u6027\u9032\u884c\u5fb9\u5e95\u5206\u6790\uff0c\u7136\u5f8c\u5728\u516b\u7a2e\u8a9e\u8a00\u4e2d\u5efa\u7acb\u4e00\u500b\u65b0\u7684\u7121\u6bd2\u7248\u672c\uff1b3) \u4e00\u500b\u652f\u63f4\u9019\u4e9b\u8a9e\u8a00\u7684\u591a\u8a9e\u8a00\u5716\u50cf\u6587\u5b57\u6a21\u578b\uff0c\u589e\u5f37\u8996\u89ba\u8a9e\u8a00\u4efb\u52d9\u4e2d\u7684\u6587\u5316\u548c\u8a9e\u8a00\u7406\u89e3\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/nahidalam/maya \u53d6\u5f97\u3002", "author": "Nahid Alam et.al.", "authors": "Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna S Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, S M Iftekhar Uddin, Shayekh Bin Islam, Roshan Santhosh, Snegha A, Drishti Sharma, Chen Liu, Isha Chaturvedi, Genta Indra Winata, Ashvanth. S, Snehanshu Mukherjee, Alham Fikri Aji", "id": "2412.07112v1", "paper_url": "http://arxiv.org/abs/2412.07112v1", "repo": "https://github.com/nahidalam/maya"}}