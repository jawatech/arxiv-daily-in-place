{"2405.05572": {"publish_time": "2024-05-09", "title": "From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences", "paper_summary": "Current computational approaches for analysing or generating code-mixed\nsentences do not explicitly model \"naturalness\" or \"acceptability\" of\ncode-mixed sentences, but rely on training corpora to reflect distribution of\nacceptable code-mixed sentences. Modelling human judgement for the\nacceptability of code-mixed text can help in distinguishing natural code-mixed\ntext and enable quality-controlled generation of code-mixed text. To this end,\nwe construct Cline - a dataset containing human acceptability judgements for\nEnglish-Hindi (en-hi) code-mixed text. Cline is the largest of its kind with\n16,642 sentences, consisting of samples sourced from two sources: synthetically\ngenerated code-mixed text and samples collected from online social media. Our\nanalysis establishes that popular code-mixing metrics such as CMI, Number of\nSwitch Points, Burstines, which are used to filter/curate/compare code-mixed\ncorpora have low correlation with human acceptability judgements, underlining\nthe necessity of our dataset. Experiments using Cline demonstrate that simple\nMultilayer Perceptron (MLP) models trained solely on code-mixing metrics are\noutperformed by fine-tuned pre-trained Multilingual Large Language Models\n(MLLMs). Specifically, XLM-Roberta and Bernice outperform IndicBERT across\ndifferent configurations in challenging data settings. Comparison with\nChatGPT's zero and fewshot capabilities shows that MLLMs fine-tuned on larger\ndata outperform ChatGPT, providing scope for improvement in code-mixed tasks.\nZero-shot transfer from English-Hindi to English-Telugu acceptability judgments\nusing our model checkpoints proves superior to random baselines, enabling\napplication to other code-mixed language pairs and providing further avenues of\nresearch. We publicly release our human-annotated dataset, trained checkpoints,\ncode-mix corpus, and code for data generation and model training.", "paper_summary_zh": "<paragraph>\u76ee\u524d\u7684\u8a08\u7b97\u65b9\u6cd5\u7528\u65bc\u5206\u6790\u6216\u751f\u6210\u6df7\u5408\u8a9e\u8a00\u53e5\u5b50\uff0c\u4e26\u672a\u660e\u78ba\u6a21\u64ec\u6df7\u5408\u8a9e\u8a00\u53e5\u5b50\u7684\u300c\u81ea\u7136\u6027\u300d\u6216\u300c\u53ef\u63a5\u53d7\u6027\u300d\uff0c\u800c\u662f\u4f9d\u8cf4\u8a13\u7df4\u8a9e\u6599\u5eab\u4f86\u53cd\u6620\u53ef\u63a5\u53d7\u6df7\u5408\u8a9e\u8a00\u53e5\u5b50\u7684\u5206\u4f48\u3002\u6a21\u64ec\u4eba\u985e\u5c0d\u6df7\u5408\u8a9e\u8a00\u6587\u5b57\u53ef\u63a5\u53d7\u6027\u7684\u5224\u65b7\uff0c\u6709\u52a9\u65bc\u5340\u5206\u81ea\u7136\u7684\u6df7\u5408\u8a9e\u8a00\u6587\u5b57\uff0c\u4e26\u80fd\u6709\u54c1\u8cea\u63a7\u7ba1\u5730\u751f\u6210\u6df7\u5408\u8a9e\u8a00\u6587\u5b57\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5efa\u69cb\u4e86 Cline\uff0c\u4e00\u500b\u5305\u542b\u4eba\u985e\u5c0d\u82f1\u8a9e-\u5370\u5730\u8a9e (en-hi) \u6df7\u5408\u8a9e\u8a00\u6587\u5b57\u53ef\u63a5\u53d7\u6027\u5224\u65b7\u7684\u8cc7\u6599\u96c6\u3002Cline \u662f\u540c\u985e\u8cc7\u6599\u96c6\u4e2d\u898f\u6a21\u6700\u5927\u7684\uff0c\u5305\u542b 16,642 \u500b\u53e5\u5b50\uff0c\u7531\u5169\u500b\u4f86\u6e90\u7684\u6a23\u672c\u7d44\u6210\uff1a\u5408\u6210\u7522\u751f\u7684\u6df7\u5408\u8a9e\u8a00\u6587\u5b57\u548c\u5f9e\u7dda\u4e0a\u793e\u7fa4\u5a92\u9ad4\u6536\u96c6\u7684\u6a23\u672c\u3002\u6211\u5011\u7684\u5206\u6790\u78ba\u7acb\u4e86\u6d41\u884c\u7684\u6df7\u5408\u8a9e\u8a00\u6307\u6a19\uff0c\u4f8b\u5982 CMI\u3001\u5207\u63db\u9ede\u6578\u3001Burstines\uff0c\u9019\u4e9b\u6307\u6a19\u7528\u65bc\u904e\u6ffe/\u6574\u7406/\u6bd4\u8f03\u6df7\u5408\u8a9e\u8a00\u8a9e\u6599\u5eab\uff0c\u8207\u4eba\u985e\u7684\u53ef\u63a5\u53d7\u6027\u5224\u65b7\u76f8\u95dc\u6027\u5f88\u4f4e\uff0c\u5f37\u8abf\u4e86\u6211\u5011\u8cc7\u6599\u96c6\u7684\u5fc5\u8981\u6027\u3002\u4f7f\u7528 Cline \u7684\u5be6\u9a57\u8b49\u660e\uff0c\u50c5\u6839\u64da\u6df7\u5408\u8a9e\u8a00\u6307\u6a19\u8a13\u7df4\u7684\u7c21\u55ae\u591a\u5c64\u611f\u77e5\u5668 (MLP) \u6a21\u578b\uff0c\u5176\u8868\u73fe\u4e0d\u5982\u5fae\u8abf\u5f8c\u7684\u9810\u8a13\u7df4\u591a\u8a9e\u8a00\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM)\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cXLM-Roberta \u548c Bernice \u5728\u5177\u6709\u6311\u6230\u6027\u7684\u8cc7\u6599\u8a2d\u5b9a\u4e2d\uff0c\u5728\u4e0d\u540c\u7684\u914d\u7f6e\u4e2d\u90fd\u512a\u65bc IndicBERT\u3002\u8207 ChatGPT \u7684\u96f6\u6b21\u5b78\u7fd2\u548c\u5c11\u6b21\u5b78\u7fd2\u80fd\u529b\u7684\u6bd4\u8f03\u986f\u793a\uff0c\u7d93\u904e\u8f03\u5927\u8cc7\u6599\u5fae\u8abf\u7684 MLLM \u512a\u65bc ChatGPT\uff0c\u70ba\u6df7\u5408\u8a9e\u8a00\u4efb\u52d9\u7684\u6539\u9032\u63d0\u4f9b\u4e86\u7a7a\u9593\u3002\u4f7f\u7528\u6211\u5011\u7684\u6a21\u578b\u6aa2\u67e5\u9ede\uff0c\u5f9e\u82f1\u8a9e-\u5370\u5730\u8a9e\u5230\u82f1\u8a9e-\u6cf0\u76e7\u56fa\u8a9e\u7684\u53ef\u63a5\u53d7\u6027\u5224\u65b7\u7684\u96f6\u6b21\u5b78\u7fd2\u8f49\u79fb\uff0c\u8b49\u660e\u512a\u65bc\u96a8\u6a5f\u57fa\u6e96\uff0c\u80fd\u5920\u61c9\u7528\u65bc\u5176\u4ed6\u6df7\u5408\u8a9e\u8a00\u8a9e\u8a00\u5c0d\uff0c\u4e26\u63d0\u4f9b\u9032\u4e00\u6b65\u7684\u7814\u7a76\u9014\u5f91\u3002\u6211\u5011\u516c\u958b\u767c\u5e03\u6211\u5011\u7684\u4eba\u5de5\u6a19\u8a3b\u8cc7\u6599\u96c6\u3001\u8a13\u7df4\u6aa2\u67e5\u9ede\u3001\u6df7\u5408\u8a9e\u8a00\u8a9e\u6599\u5eab\uff0c\u4ee5\u53ca\u7528\u65bc\u8cc7\u6599\u751f\u6210\u548c\u6a21\u578b\u8a13\u7df4\u7684\u7a0b\u5f0f\u78bc\u3002</paragraph>", "author": "Prashant Kodali et.al.", "authors": "Prashant Kodali, Anmol Goel, Likhith Asapu, Vamshi Krishna Bonagiri, Anirudh Govil, Monojit Choudhury, Manish Shrivastava, Ponnurangam Kumaraguru", "id": "2405.05572v1", "paper_url": "http://arxiv.org/abs/2405.05572v1", "repo": "null"}}