{"2501.16356": {"publish_time": "2025-01-20", "title": "Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations", "paper_summary": "Large Language Models (LLMs) are increasingly being used to simulate\nhuman-like decision making in agent-based financial market models (ABMs). As\nmodels become more powerful and accessible, researchers can now incorporate\nindividual LLM decisions into ABM environments. However, integration may\nintroduce inherent biases that need careful evaluation. In this paper we test\nthree state-of-the-art GPT models for bias using two model sampling approaches:\none-shot and few-shot API queries. We observe significant variations in\ndistributions of outputs between specific models, and model sub versions, with\nGPT-4o-Mini-2024-07-18 showing notably better performance (32-43% yes\nresponses) compared to GPT-4-0125-preview's extreme bias (98-99% yes\nresponses). We show that sampling methods and model sub-versions significantly\nimpact results: repeated independent API calls produce different distributions\ncompared to batch sampling within a single call. While no current GPT model can\nsimultaneously achieve a uniform distribution and Markovian properties in\none-shot testing, few-shot sampling can approach uniform distributions under\ncertain conditions. We explore the Temperature parameter, providing a\ndefinition and comparative results. We further compare our results to true\nrandom binary series and test specifically for the common human bias of\nNegative Recency - finding LLMs have a mixed ability to 'beat' humans in this\none regard. These findings emphasise the critical importance of careful LLM\nintegration into ABMs for financial markets and more broadly.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)  zunehmend zur Simulation\nmenschlicher Entscheidungsfindung in agentenbasierten Finanzmarktmodellen (ABM) eingesetzt. Da\nModelle leistungsf\u00e4higer und zug\u00e4nglicher werden, k\u00f6nnen Forscher jetzt\neinzelne LLM-Entscheidungen in ABM-Umgebungen integrieren. Die Integration kann jedoch\ninh\u00e4rente Verzerrungen einf\u00fchren, die sorgf\u00e4ltig bewertet werden m\u00fcssen. In diesem Artikel testen wir\ndrei hochmoderne GPT-Modelle auf Verzerrungen unter Verwendung zweier Modell-Sampling-Ans\u00e4tze:\nOne-Shot- und Few-Shot-API-Abfragen. Wir beobachten signifikante Unterschiede in\nVerteilungen von Ausgaben zwischen bestimmten Modellen und Modellunterversionen, wobei\nGPT-4o-Mini-2024-07-18 eine deutlich bessere Leistung zeigt (32-43 % Ja-Antworten) im Vergleich zu GPT-4-0125-Vorschau extreme Verzerrung (98-99 % Ja-Antworten). Wir zeigen, dass Sampling-Methoden und Modellunterversionen die Ergebnisse erheblich beeinflussen: Wiederholte unabh\u00e4ngige API-Aufrufe erzeugen unterschiedliche Verteilungen im Vergleich zur Batch-Abtastung innerhalb eines einzelnen Aufrufs. Zwar kann kein aktuelles GPT-Modell gleichzeitig eine gleichm\u00e4\u00dfige Verteilung und markovsche Eigenschaften im One-Shot-Test erreichen, aber Few-Shot-Sampling kann sich unter bestimmten Bedingungen gleichm\u00e4\u00dfigen Verteilungen ann\u00e4hern. Wir untersuchen den Temperaturparameter und liefern eine\nDefinition und Vergleichsergebnisse. Dar\u00fcber hinaus vergleichen wir unsere Ergebnisse mit echten\nzuf\u00e4lligen Bin\u00e4rreihen und testen speziell auf die h\u00e4ufige menschliche Verzerrung von\nNegative Rezenz - Feststellung, dass LLMs eine gemischte F\u00e4higkeit haben, Menschen in diesem Fall zu \u201eschlagen\u201c.\neine Hinsicht. Diese Ergebnisse unterstreichen die entscheidende Bedeutung einer sorgf\u00e4ltigen LLM-Integration in ABMs f\u00fcr Finanzm\u00e4rkte und dar\u00fcber hinaus.", "author": "Alicia Vidler et.al.", "authors": "Alicia Vidler, Toby Walsh", "id": "2501.16356v1", "paper_url": "http://arxiv.org/abs/2501.16356v1", "repo": "null"}}