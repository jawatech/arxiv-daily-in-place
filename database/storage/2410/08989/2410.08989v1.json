{"2410.08989": {"publish_time": "2024-10-11", "title": "SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning", "paper_summary": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks.", "paper_summary_zh": "\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u88ab\u8b49\u660e\u5c0d\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u6709\u6548\u3002\u7136\u800c\uff0c\u96a8\u8457 LLM \u7684\u898f\u6a21\u8d8a\u4f86\u8d8a\u5927\uff0c\u53cd\u5411\u50b3\u64ad\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\u4e5f\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u9ad8\u3002\u96f6\u968e (ZO) \u6700\u4f73\u5316\u65b9\u6cd5\u900f\u904e\u4f7f\u7528\u524d\u5411\u50b3\u905e\u4f86\u4f30\u8a08\u68af\u5ea6\uff0c\u63d0\u4f9b\u4e86\u4e00\u7a2e\u8a18\u61b6\u9ad4\u6548\u7387\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u68af\u5ea6\u4f30\u8a08\u7684\u8b8a\u7570\u901a\u5e38\u6703\u96a8\u6a21\u578b\u7684\u53c3\u6578\u7dad\u5ea6$\\unicode{x2013}$\u4e00\u500b LLM \u7684\u91cd\u5927\u554f\u984c\u800c\u7dda\u6027\u589e\u52a0\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u96a8\u6a5f\u5b50\u7a7a\u9593\u96f6\u968e (SubZero) \u6700\u4f73\u5316\uff0c\u4ee5\u89e3\u6c7a LLM \u7684\u9ad8\u7dad\u5ea6\u6240\u5e36\u4f86\u7684\u6311\u6230\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u5c08\u70ba LLM \u91cf\u8eab\u6253\u9020\u7684\u4f4e\u79e9\u64fe\u52d5\uff0c\u5b83\u5728\u6539\u5584\u8a13\u7df4\u6548\u80fd\u7684\u540c\u6642\uff0c\u986f\u8457\u964d\u4f4e\u4e86\u8a18\u61b6\u9ad4\u6d88\u8017\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u68af\u5ea6\u4f30\u8a08\u8207\u53cd\u5411\u50b3\u64ad\u68af\u5ea6\u975e\u5e38\u63a5\u8fd1\uff0c\u8207\u50b3\u7d71\u7684 ZO \u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u8f03\u4f4e\u7684\u8b8a\u7570\uff0c\u4e26\u4e14\u8207 SGD \u7d50\u5408\u4f7f\u7528\u6642\u53ef\u4ee5\u78ba\u4fdd\u6536\u6582\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207\u5404\u7a2e\u8a9e\u8a00\u5efa\u6a21\u4efb\u52d9\u4e2d\u7684\u6a19\u6e96 ZO \u65b9\u6cd5\uff08\u4f8b\u5982 MeZO\uff09\u76f8\u6bd4\uff0cSubZero \u589e\u5f37\u4e86\u5fae\u8abf\u6548\u80fd\u4e26\u5be6\u73fe\u4e86\u66f4\u5feb\u7684\u6536\u6582\u3002", "author": "Ziming Yu et.al.", "authors": "Ziming Yu, Pan Zhou, Sike Wang, Jia Li, Hua Huang", "id": "2410.08989v1", "paper_url": "http://arxiv.org/abs/2410.08989v1", "repo": "null"}}