{"2502.11862": {"publish_time": "2025-02-17", "title": "Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu", "paper_summary": "In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affects the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an encrypted version of Manchu texts. Our results indicate that\nhigh-quality dictionaries and good parallel examples are very helpful, while\ngrammars hardly help. In a follow-up study, we showcase a promising application\nof in-context MT: parallel data augmentation as a way to bootstrap the\nconventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.", "paper_summary_zh": "\u8a9e\u5883\u6a5f\u5668\u7ffb\u8b6f (MT) \u8207\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7d50\u5408\uff0c\u5c0d\u65bc\u4f4e\u8cc7\u6e90 MT \u4f86\u8aaa\u662f\u4e00\u7a2e\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u8f15\u6613\u5229\u7528\u8a9e\u6cd5\u66f8\u548c\u5b57\u5178\u7b49\u8a9e\u8a00\u8cc7\u6e90\u3002\u6b64\u985e\u8cc7\u6e90\u901a\u5e38\u6703\u9078\u64c7\u6027\u5730\u6574\u5408\u5230\u63d0\u793a\u4e2d\uff0c\u8b93 LLM \u80fd\u5920\u900f\u904e\u5176\u8a9e\u5883\u5b78\u7fd2\u80fd\u529b (ICL) \u76f4\u63a5\u57f7\u884c\u7ffb\u8b6f\uff0c\u800c\u7121\u9700\u4efb\u4f55\u7279\u5b9a\u8a13\u7df4\u3002\u7136\u800c\uff0c\u6bcf\u7a2e\u985e\u578b\u7684\u8cc7\u6e90\uff08\u4f8b\u5982\u5b57\u5178\u3001\u8a9e\u6cd5\u66f8\u548c\u64f7\u53d6\u7684\u5e73\u884c\u7bc4\u4f8b\uff09\u7684\u76f8\u5c0d\u91cd\u8981\u6027\u4e26\u4e0d\u660e\u78ba\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u672c\u7814\u7a76\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u6bcf\u9805\u8cc7\u6e90\u53ca\u5176\u54c1\u8cea\u5982\u4f55\u5f71\u97ff\u7ffb\u8b6f\u6548\u80fd\uff0c\u4e26\u4ee5\u6eff\u8a9e\u4f5c\u70ba\u6211\u5011\u7684\u6848\u4f8b\u7814\u7a76\u3002\u70ba\u4e86\u79fb\u9664 LLM \u53c3\u6578\u4e2d\u7de8\u78bc\u7684\u4efb\u4f55\u6eff\u8a9e\u5148\u5099\u77e5\u8b58\uff0c\u4e26\u627e\u51fa ICL \u7684\u5f71\u97ff\uff0c\u6211\u5011\u4e5f\u5c0d\u6eff\u8a9e\u6587\u672c\u7684\u52a0\u5bc6\u7248\u672c\u9032\u884c\u5be6\u9a57\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u9ad8\u54c1\u8cea\u7684\u5b57\u5178\u548c\u826f\u597d\u7684\u5e73\u884c\u7bc4\u4f8b\u975e\u5e38\u6709\u5e6b\u52a9\uff0c\u800c\u8a9e\u6cd5\u5e7e\u4e4e\u6c92\u6709\u5e6b\u52a9\u3002\u5728\u5f8c\u7e8c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u8a9e\u5883 MT \u7684\u4e00\u500b\u6709\u524d\u666f\u7684\u61c9\u7528\uff1a\u5e73\u884c\u6578\u64da\u64f4\u5145\uff0c\u4f5c\u70ba\u5f15\u5c0e\u50b3\u7d71 MT \u6a21\u578b\u7684\u4e00\u7a2e\u65b9\u5f0f\u3002\u7576\u55ae\u8a9e\u8cc7\u6599\u8c50\u5bcc\u6642\uff0c\u900f\u904e\u8a9e\u5883 MT \u7522\u751f\u5408\u6210\u5e73\u884c\u8cc7\u6599\u63d0\u4f9b\u4e86\u4e00\u689d\u9014\u5f91\uff0c\u53ef\u4ee5\u6e1b\u8f15\u8cc7\u6599\u77ed\u7f3a\uff0c\u4e26\u5efa\u69cb\u6709\u6548\u4e14\u9ad8\u6548\u7684\u4f4e\u8cc7\u6e90\u795e\u7d93 MT \u7cfb\u7d71\u3002", "author": "Renhao Pei et.al.", "authors": "Renhao Pei, Yihong Liu, Peiqin Lin, Fran\u00e7ois Yvon, Hinrich Sch\u00fctze", "id": "2502.11862v1", "paper_url": "http://arxiv.org/abs/2502.11862v1", "repo": "null"}}