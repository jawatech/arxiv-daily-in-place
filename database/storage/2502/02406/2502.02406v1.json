{"2502.02406": {"publish_time": "2025-02-04", "title": "LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models", "paper_summary": "Cross-attention is commonly adopted in multimodal large language models\n(MLLMs) for integrating visual information into the language backbone. However,\nin applications with large visual inputs, such as video understanding,\nprocessing a large number of visual tokens in cross-attention layers leads to\nhigh memory demands and often necessitates distributed computation across\nmultiple GPUs. Existing distributed attention mechanisms face significant\ncommunication overheads, making cross-attention layers a critical bottleneck\nfor efficient training and inference of MLLMs. To address this, we propose\nLV-XAttn, a distributed, exact cross-attention mechanism with minimal\ncommunication overhead. We observe that in applications involving large visual\ninputs the size of the query block is typically much smaller than that of the\nkey-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally\non each GPU and exchange smaller query blocks across GPUs. We also introduce an\nefficient activation recomputation technique enabling support for longer visual\ncontext. We theoretically analyze the communication benefits of LV-XAttn and\nshow that it can achieve speedups for a wide range of models. Our evaluations\nwith mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to\n5.58$\\times$ end-to-end speedup compared to existing approaches.", "paper_summary_zh": "\u4ea4\u53c9\u6ce8\u610f\u529b\u901a\u5e38\u5728\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u4e2d\u91c7\u7528\uff0c\u7528\u4e8e\u5c06\u89c6\u89c9\u4fe1\u606f\u6574\u5408\u5230\u8bed\u8a00\u4e3b\u5e72\u4e2d\u3002\u7136\u800c\uff0c\u5728\u5177\u6709\u5927\u578b\u89c6\u89c9\u8f93\u5165\uff08\u4f8b\u5982\u89c6\u9891\u7406\u89e3\uff09\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u5728\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u4e2d\u5904\u7406\u5927\u91cf\u89c6\u89c9\u6807\u8bb0\u4f1a\u5bfc\u81f4\u9ad8\u5185\u5b58\u9700\u6c42\uff0c\u5e76\u4e14\u901a\u5e38\u9700\u8981\u8de8\u591a\u4e2a GPU \u5206\u5e03\u5f0f\u8ba1\u7b97\u3002\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u6ce8\u610f\u529b\u673a\u5236\u9762\u4e34\u7740\u5927\u91cf\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u4f7f\u5f97\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6210\u4e3a MLLM \u7684\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u5173\u952e\u74f6\u9888\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LV-XAttn\uff0c\u4e00\u79cd\u5206\u5e03\u5f0f\u3001\u7cbe\u786e\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5177\u6709\u6700\u5c0f\u7684\u901a\u4fe1\u5f00\u9500\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5728\u6d89\u53ca\u5927\u578b\u89c6\u89c9\u8f93\u5165\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u67e5\u8be2\u5757\u7684\u5927\u5c0f\u901a\u5e38\u8fdc\u5c0f\u4e8e\u952e\u503c\u5757\u7684\u5927\u5c0f\u3002\u56e0\u6b64\uff0c\u5728 LV-XAttn \u4e2d\uff0c\u6211\u4eec\u5c06\u5927\u578b\u952e\u503c\u5757\u672c\u5730\u4fdd\u5b58\u5728\u6bcf\u4e2a GPU \u4e0a\uff0c\u5e76\u5728 GPU \u4e4b\u95f4\u4ea4\u6362\u8f83\u5c0f\u7684\u67e5\u8be2\u5757\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6fc0\u6d3b\u91cd\u65b0\u8ba1\u7b97\u6280\u672f\uff0c\u652f\u6301\u66f4\u957f\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86 LV-XAttn \u7684\u901a\u4fe1\u4f18\u52bf\uff0c\u5e76\u8868\u660e\u5b83\u53ef\u4ee5\u4e3a\u5e7f\u6cdb\u7684\u6a21\u578b\u5b9e\u73b0\u52a0\u901f\u3002\u6211\u4eec\u4f7f\u7528 mPLUG-Owl3 \u548c OpenFlamingo \u6a21\u578b\u8fdb\u884c\u7684\u8bc4\u4f30\u53d1\u73b0\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cLV-XAttn \u53ef\u5b9e\u73b0\u9ad8\u8fbe 5.58 \u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "author": "Tzu-Tao Chang et.al.", "authors": "Tzu-Tao Chang, Shivaram Venkataraman", "id": "2502.02406v1", "paper_url": "http://arxiv.org/abs/2502.02406v1", "repo": "null"}}