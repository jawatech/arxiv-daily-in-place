{"2405.18425": {"publish_time": "2024-05-28", "title": "ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention", "paper_summary": "Recently, linear complexity sequence modeling networks have achieved modeling\ncapabilities similar to Vision Transformers on a variety of computer vision\ntasks, while using fewer FLOPs and less memory. However, their advantage in\nterms of actual runtime speed is not significant. To address this issue, we\nintroduce Gated Linear Attention (GLA) for vision, leveraging its superior\nhardware-awareness and efficiency. We propose direction-wise gating to capture\n1D global context through bidirectional modeling and a 2D gating locality\ninjection to adaptively inject 2D local details into 1D global context. Our\nhardware-aware implementation further merges forward and backward scanning into\na single kernel, enhancing parallelism and reducing memory cost and latency.\nThe proposed model, ViG, offers a favorable trade-off in accuracy, parameters,\nand FLOPs on ImageNet and downstream tasks, outperforming popular Transformer\nand CNN-based models. Notably, ViG-S matches DeiT-B's accuracy while using only\n27% of the parameters and 20% of the FLOPs, running 2$\\times$ faster on\n$224\\times224$ images. At $1024\\times1024$ resolution, ViG-T uses 5.2$\\times$\nfewer FLOPs, saves 90% GPU memory, runs 4.8$\\times$ faster, and achieves 20.7%\nhigher top-1 accuracy than DeiT-T. These results position ViG as an efficient\nand scalable solution for visual representation learning. Code is available at\n\\url{https://github.com/hustvl/ViG}.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u7dda\u6027\u8907\u96dc\u5ea6\u5e8f\u5217\u6a21\u578b\u7db2\u8def\u5728\u5404\u7a2e\u96fb\u8166\u8996\u89ba\u4efb\u52d9\u4e2d\uff0c\u4ee5\u8f03\u5c11\u7684\u6d6e\u9ede\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\uff0c\u9054\u5230\u4e86\u8207\u8996\u89ba\u8f49\u63db\u5668\u985e\u4f3c\u7684\u5efa\u6a21\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u5be6\u969b\u57f7\u884c\u901f\u5ea6\u4e0a\u7684\u512a\u52e2\u4e26\u4e0d\u986f\u8457\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u8996\u89ba\u9580\u63a7\u7dda\u6027\u6ce8\u610f\u529b\uff08GLA\uff09\uff0c\u5229\u7528\u5176\u512a\u7570\u7684\u786c\u9ad4\u611f\u77e5\u548c\u6548\u7387\u3002\u6211\u5011\u63d0\u51fa\u65b9\u5411\u9598\u63a7\uff0c\u4ee5\u900f\u904e\u96d9\u5411\u5efa\u6a21\u64f7\u53d6\u4e00\u7dad\u5168\u5c40\u80cc\u666f\uff0c\u4ee5\u53ca\u4e8c\u7dad\u9598\u63a7\u5340\u57df\u6ce8\u5165\uff0c\u4ee5\u81ea\u9069\u61c9\u5730\u5c07\u4e8c\u7dad\u5c40\u90e8\u7d30\u7bc0\u6ce8\u5165\u4e00\u7dad\u5168\u5c40\u80cc\u666f\u3002\u6211\u5011\u786c\u9ad4\u611f\u77e5\u7684\u5be6\u73fe\u9032\u4e00\u6b65\u5c07\u6b63\u5411\u548c\u53cd\u5411\u6383\u63cf\u5408\u4f75\u5230\u55ae\u4e00\u6838\u5fc3\uff0c\u589e\u5f37\u4e26\u884c\u6027\u4e26\u964d\u4f4e\u8a18\u61b6\u9ad4\u6210\u672c\u548c\u5ef6\u9072\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b ViG \u5728 ImageNet \u548c\u4e0b\u6e38\u4efb\u52d9\u4e2d\u63d0\u4f9b\u4e86\u6e96\u78ba\u5ea6\u3001\u53c3\u6578\u548c\u6d6e\u9ede\u904b\u7b97\u4e4b\u9593\u6709\u5229\u7684\u6b0a\u8861\uff0c\u512a\u65bc\u6d41\u884c\u7684 Transformer \u548c\u57fa\u65bc CNN \u7684\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cViG-S \u5728\u50c5\u4f7f\u7528 27% \u7684\u53c3\u6578\u548c 20% \u7684\u6d6e\u9ede\u904b\u7b97\u7684\u60c5\u6cc1\u4e0b\uff0c\u8207 DeiT-B \u7684\u6e96\u78ba\u5ea6\u76f8\u5339\u914d\uff0c\u5728 224\u00d7224 \u5f71\u50cf\u4e0a\u57f7\u884c\u901f\u5ea6\u5feb 2 \u500d\u3002\u5728 1024\u00d71024 \u89e3\u6790\u5ea6\u4e0b\uff0cViG-T \u4f7f\u7528\u7684\u6d6e\u9ede\u904b\u7b97\u6e1b\u5c11\u4e86 5.2 \u500d\uff0c\u7bc0\u7701\u4e86 90% \u7684 GPU \u8a18\u61b6\u9ad4\uff0c\u57f7\u884c\u901f\u5ea6\u5feb\u4e86 4.8 \u500d\uff0c\u4e26\u4e14\u6bd4 DeiT-T \u9054\u5230\u4e86\u9ad8\u51fa 20.7% \u7684\u9802\u7d1a 1 \u6e96\u78ba\u5ea6\u3002\u9019\u4e9b\u7d50\u679c\u5c07 ViG \u5b9a\u4f4d\u70ba\u4e00\u7a2e\u7528\u65bc\u8996\u89ba\u8868\u5fb5\u5b78\u7fd2\u7684\u9ad8\u6548\u4e14\u53ef\u64f4\u5145\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 \\url{https://github.com/hustvl/ViG} \u4e2d\u53d6\u5f97\u3002</paragraph>", "author": "Bencheng Liao et.al.", "authors": "Bencheng Liao, Xinggang Wang, Lianghui Zhu, Qian Zhang, Chang Huang", "id": "2405.18425v2", "paper_url": "http://arxiv.org/abs/2405.18425v2", "repo": "https://github.com/hustvl/vig"}}