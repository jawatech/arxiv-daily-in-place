{"2502.09620": {"publish_time": "2025-02-13", "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs", "paper_summary": "Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL", "paper_summary_zh": "<paragraph>\u7de8\u78bc\u5668\u514d\u8cbb\u67b6\u69cb\u5df2\u5728 2D \u8996\u89ba\u9818\u57df\u4e2d\u521d\u6b65\u63a2\u7d22\uff0c\u4f46\u5b83\u5011\u662f\u5426\u80fd\u6709\u6548\u61c9\u7528\u65bc 3D \u7406\u89e3\u5834\u666f\u4ecd\u662f\u4e00\u500b\u958b\u653e\u7684\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5c0d\u7de8\u78bc\u5668\u514d\u8cbb\u67b6\u69cb\u6f5b\u529b\u7684\u9996\u6b21\u5168\u9762\u8abf\u67e5\uff0c\u4ee5\u514b\u670d\u57fa\u65bc\u7de8\u78bc\u5668\u7684 3D \u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u7684\u6311\u6230\u3002\u9019\u4e9b\u6311\u6230\u5305\u62ec\u7121\u6cd5\u9069\u61c9\u4e0d\u540c\u7684\u9ede\u96f2\u89e3\u6790\u5ea6\uff0c\u4e14\u4f86\u81ea\u7de8\u78bc\u5668\u7684\u9ede\u7279\u5fb5\u7121\u6cd5\u6eff\u8db3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8a9e\u7fa9\u9700\u6c42\u3002\u6211\u5011\u8b58\u5225\u51fa 3D LMM \u7684\u95dc\u9375\u65b9\u9762\uff0c\u4ee5\u79fb\u9664\u7de8\u78bc\u5668\u4e26\u8b93 LLM \u627f\u64d4 3D \u7de8\u78bc\u5668\u7684\u89d2\u8272\uff1a1) \u6211\u5011\u5728\u9810\u8a13\u7df4\u968e\u6bb5\u63d0\u51fa LLM \u5d4c\u5165\u5f0f\u8a9e\u7fa9\u7de8\u78bc\u7b56\u7565\uff0c\u63a2\u7d22\u5404\u7a2e\u9ede\u96f2\u81ea\u6211\u76e3\u7763\u640d\u5931\u7684\u5f71\u97ff\u3002\u6211\u5011\u63d0\u51fa\u6df7\u5408\u8a9e\u7fa9\u640d\u5931\u4f86\u63d0\u53d6\u9ad8\u968e\u8a9e\u7fa9\u30022) \u6211\u5011\u5728\u6307\u4ee4\u8abf\u6574\u968e\u6bb5\u5f15\u5165\u5206\u5c64\u5e7e\u4f55\u805a\u5408\u7b56\u7565\u3002\u9019\u5c07\u6b78\u7d0d\u504f\u5dee\u7d0d\u5165 LLM \u65e9\u671f\u5c64\uff0c\u4ee5\u5c08\u6ce8\u65bc\u9ede\u96f2\u7684\u5c40\u90e8\u7d30\u7bc0\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u7b2c\u4e00\u500b\u7121\u7de8\u78bc\u5668 3D LMM\uff0cENEL\u3002\u6211\u5011\u7684 7B \u6a21\u578b\u8207\u7576\u524d\u6700\u5148\u9032\u7684\u6a21\u578b ShapeLLM-13B \u76f8\u5ab2\u7f8e\uff0c\u5206\u5225\u5728\u5206\u985e\u3001\u5b57\u5e55\u548c VQA \u4efb\u52d9\u4e2d\u9054\u5230 55.0%\u300150.92% \u548c 42.7%\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u7121\u7de8\u78bc\u5668\u67b6\u69cb\u6975\u6709\u671b\u53d6\u4ee3\u57fa\u65bc\u7de8\u78bc\u5668\u7684\u67b6\u69cb\u5728 3D \u7406\u89e3\u9818\u57df\u7684\u61c9\u7528\u3002\u7a0b\u5f0f\u78bc\u767c\u5e03\u65bc https://github.com/Ivan-Tang-3D/ENEL</paragraph>", "author": "Yiwen Tang et.al.", "authors": "Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao", "id": "2502.09620v1", "paper_url": "http://arxiv.org/abs/2502.09620v1", "repo": "null"}}