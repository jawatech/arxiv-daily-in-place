{"2409.02813": {"publish_time": "2024-09-04", "title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark", "paper_summary": "This paper introduces MMMU-Pro, a robust version of the Massive\nMulti-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.\nMMMU-Pro rigorously assesses multimodal models' true understanding and\nreasoning capabilities through a three-step process based on MMMU: (1)\nfiltering out questions answerable by text-only models, (2) augmenting\ncandidate options, and (3) introducing a vision-only input setting where\nquestions are embedded within images. This setting challenges AI to truly \"see\"\nand \"read\" simultaneously, testing a fundamental human cognitive skill of\nseamlessly integrating visual and textual information. Results show that model\nperformance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%\nto 26.9% across models. We explore the impact of OCR prompts and Chain of\nThought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT\ngenerally improves performance. MMMU-Pro provides a more rigorous evaluation\ntool, closely mimicking real-world scenarios and offering valuable directions\nfor future research in multimodal AI.", "paper_summary_zh": "\u672c\u6587\u4ecb\u7d39 MMMU-Pro\uff0c\u9019\u662f\u5927\u898f\u6a21\u591a\u9818\u57df\u591a\u6a21\u614b\u7406\u89e3\u8207\u63a8\u7406 (MMMU) \u8a55\u91cf\u7684\u5f37\u5316\u7248\u672c\u3002MMMU-Pro \u900f\u904e\u4ee5\u4e0b\u57fa\u65bc MMMU \u7684\u4e09\u6b65\u9a5f\u6d41\u7a0b\uff0c\u56b4\u8b39\u8a55\u4f30\u591a\u6a21\u614b\u6a21\u578b\u7684\u771f\u6b63\u7406\u89e3\u8207\u63a8\u7406\u80fd\u529b\uff1a(1) \u904e\u6ffe\u51fa\u50c5\u9760\u6587\u5b57\u6a21\u578b\u5373\u53ef\u56de\u7b54\u7684\u554f\u984c\uff0c(2) \u589e\u52a0\u5019\u9078\u9078\u9805\uff0c\u4ee5\u53ca (3) \u5f15\u5165\u50c5\u9650\u8996\u89ba\u8f38\u5165\u7684\u8a2d\u5b9a\uff0c\u5176\u4e2d\u554f\u984c\u6703\u5d4c\u5165\u5728\u5f71\u50cf\u4e2d\u3002\u6b64\u8a2d\u5b9a\u6311\u6230 AI \u540c\u6642\u300c\u89c0\u770b\u300d\u548c\u300c\u95b1\u8b80\u300d\uff0c\u6e2c\u8a66\u4eba\u985e\u7121\u7e2b\u6574\u5408\u8996\u89ba\u548c\u6587\u5b57\u8cc7\u8a0a\u7684\u57fa\u672c\u8a8d\u77e5\u6280\u80fd\u3002\u7d50\u679c\u986f\u793a\uff0c\u6a21\u578b\u5728 MMMU-Pro \u7684\u8868\u73fe\u5927\u5e45\u4f4e\u65bc MMMU\uff0c\u5404\u6a21\u578b\u7684\u8868\u73fe\u7bc4\u570d\u5f9e 16.8% \u5230 26.9%\u3002\u6211\u5011\u63a2\u8a0e OCR \u63d0\u793a\u548c\u601d\u8003\u93c8 (CoT) \u63a8\u7406\u7684\u5f71\u97ff\uff0c\u767c\u73fe OCR \u63d0\u793a\u7684\u5f71\u97ff\u5f88\u5c0f\uff0c\u800c CoT \u901a\u5e38\u6703\u63d0\u5347\u8868\u73fe\u3002MMMU-Pro \u63d0\u4f9b\u66f4\u56b4\u8b39\u7684\u8a55\u91cf\u5de5\u5177\uff0c\u80fd\u7cbe\u78ba\u6a21\u64ec\u771f\u5be6\u4e16\u754c\u7684\u5834\u666f\uff0c\u4e26\u70ba\u591a\u6a21\u614b AI \u7684\u672a\u4f86\u7814\u7a76\u63d0\u4f9b\u6709\u50f9\u503c\u7684\u65b9\u5411\u3002", "author": "Xiang Yue et.al.", "authors": "Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, Graham Neubig", "id": "2409.02813v1", "paper_url": "http://arxiv.org/abs/2409.02813v1", "repo": "null"}}