{"2410.24159": {"publish_time": "2024-10-31", "title": "GPT or BERT: why not both?", "paper_summary": "We present a simple way to merge masked language modeling with causal\nlanguage modeling. This hybrid training objective results in a model that\ncombines the strengths of both modeling paradigms within a single transformer\nstack: GPT-BERT can be transparently used like any standard causal or masked\nlanguage model. We test the pretraining process that enables this flexible\nbehavior on the BabyLM Challenge 2024. The results show that the hybrid\npretraining outperforms masked-only or causal-only models. We openly release\nthe models, training corpora and code.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7c21\u55ae\u7684\u65b9\u6cd5\uff0c\u5c07\u906e\u853d\u8a9e\u8a00\u6a21\u578b\u8207\u56e0\u679c\u8a9e\u8a00\u6a21\u578b\u5408\u4f75\u3002\u9019\u7a2e\u6df7\u5408\u8a13\u7df4\u76ee\u6a19\u6703\u7522\u751f\u4e00\u500b\u6a21\u578b\uff0c\u7d50\u5408\u4e86\u55ae\u4e00Transformer\u5806\u758a\u4e2d\u5169\u7a2e\u6a21\u578b\u7bc4\u4f8b\u7684\u512a\u9ede\uff1aGPT-BERT \u53ef\u4ee5\u50cf\u4efb\u4f55\u6a19\u6e96\u56e0\u679c\u6216\u906e\u853d\u8a9e\u8a00\u6a21\u578b\u4e00\u6a23\u900f\u660e\u5730\u4f7f\u7528\u3002\u6211\u5011\u5728 BabyLM Challenge 2024 \u4e0a\u6e2c\u8a66\u4e86\u8b93\u9019\u7a2e\u9748\u6d3b\u884c\u70ba\u5f97\u4ee5\u5be6\u73fe\u7684\u9810\u8a13\u7df4\u904e\u7a0b\u3002\u7d50\u679c\u986f\u793a\uff0c\u6df7\u5408\u9810\u8a13\u7df4\u512a\u65bc\u50c5\u906e\u853d\u6216\u50c5\u56e0\u679c\u6a21\u578b\u3002\u6211\u5011\u516c\u958b\u767c\u5e03\u6a21\u578b\u3001\u8a13\u7df4\u8a9e\u6599\u5eab\u548c\u7a0b\u5f0f\u78bc\u3002", "author": "Lucas Georges Gabriel Charpentier et.al.", "authors": "Lucas Georges Gabriel Charpentier, David Samuel", "id": "2410.24159v1", "paper_url": "http://arxiv.org/abs/2410.24159v1", "repo": "https://github.com/ltgoslo/gpt-bert"}}