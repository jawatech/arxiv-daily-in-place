
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-18**|**Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts**|German Gritsai et.al.|[2410.14677v1](http://arxiv.org/abs/2410.14677v1)|null|
|**2024-10-18**|**SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment**|Qin Liu et.al.|[2410.14676v1](http://arxiv.org/abs/2410.14676v1)|null|
|**2024-10-18**|**Enhancing Large Language Models' Situated Faithfulness to External Contexts**|Yukun Huang et.al.|[2410.14675v1](http://arxiv.org/abs/2410.14675v1)|[link](https://github.com/kkkevinkkkkk/situated_faithfulness)|
|**2024-10-18**|**BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities**|Shaozhe Hao et.al.|[2410.14672v1](http://arxiv.org/abs/2410.14672v1)|[link](https://github.com/haoosz/BiGR)|
|**2024-10-18**|**NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples**|Baiqi Li et.al.|[2410.14669v1](http://arxiv.org/abs/2410.14669v1)|null|
|**2024-10-18**|**MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps**|Xiongtao Zhou et.al.|[2410.14668v1](http://arxiv.org/abs/2410.14668v1)|[link](https://github.com/alenai97/miceval)|
|**2024-10-18**|**DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph**|Maitreya Prafulla Chitale et.al.|[2410.14666v1](http://arxiv.org/abs/2410.14666v1)|null|
|**2024-10-18**|**Real-time Fake News from Adversarial Feedback**|Sanxing Chen et.al.|[2410.14651v1](http://arxiv.org/abs/2410.14651v1)|null|
|**2024-10-18**|**Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs**|Runchu Tian et.al.|[2410.14641v1](http://arxiv.org/abs/2410.14641v1)|[link](https://github.com/Rachum-thu/LongPiBench)|
|**2024-10-18**|**GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings**|Raghuveer Thirukovalluru et.al.|[2410.14635v1](http://arxiv.org/abs/2410.14635v1)|null|
|**2024-10-18**|**Diverging Preferences: When do Annotators Disagree and do Models Know?**|Michael JQ Zhang et.al.|[2410.14632v1](http://arxiv.org/abs/2410.14632v1)|null|
|**2024-10-18**|**On the Regularization of Learnable Embeddings for Time Series Processing**|Luca Butera et.al.|[2410.14630v1](http://arxiv.org/abs/2410.14630v1)|null|
|**2024-10-18**|**CELI: Controller-Embedded Language Model Interactions**|Jan-Samuel Wagner et.al.|[2410.14627v1](http://arxiv.org/abs/2410.14627v1)|null|
|**2024-10-18**|**You Shall Know a Tool by the Traces it Leaves: The Predictability of Sentiment Analysis Tools**|Daniel Baumartz et.al.|[2410.14626v1](http://arxiv.org/abs/2410.14626v1)|null|
|**2024-10-18**|**Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor Environments**|Mariusz Wisniewski et.al.|[2410.14616v1](http://arxiv.org/abs/2410.14616v1)|[link](https://github.com/mazqtpopx/cranfield-navigation-gym)|
|**2024-10-18**|**Asymptotically Optimal Change Detection for Unnormalized Pre- and Post-Change Distributions**|Arman Adibi et.al.|[2410.14615v1](http://arxiv.org/abs/2410.14615v1)|null|
|**2024-10-18**|**DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual Distillation in Conversational Search**|Simon Lupart et.al.|[2410.14609v1](http://arxiv.org/abs/2410.14609v1)|null|
|**2024-10-18**|**Streaming Deep Reinforcement Learning Finally Works**|Mohamed Elsayed et.al.|[2410.14606v1](http://arxiv.org/abs/2410.14606v1)|null|
|**2024-10-18**|**How Does Data Diversity Shape the Weight Landscape of Neural Networks?**|Yang Ba et.al.|[2410.14602v1](http://arxiv.org/abs/2410.14602v1)|null|
|**2024-10-18**|**Teaching Models to Balance Resisting and Accepting Persuasion**|Elias Stengel-Eskin et.al.|[2410.14596v1](http://arxiv.org/abs/2410.14596v1)|[link](https://github.com/esteng/persuasion_balanced_training)|
|**2024-10-18**|**Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases**|Elias Lumer et.al.|[2410.14594v1](http://arxiv.org/abs/2410.14594v1)|null|
|**2024-10-18**|**Temporal Fair Division of Indivisible Items**|Edith Elkind et.al.|[2410.14593v1](http://arxiv.org/abs/2410.14593v1)|null|
|**2024-10-18**|**Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a Continuum**|Ryan Soh-Eun Shim et.al.|[2410.14589v1](http://arxiv.org/abs/2410.14589v1)|null|
|**2024-10-18**|**Neural Combinatorial Clustered Bandits for Recommendation Systems**|Baran Atalar et.al.|[2410.14586v1](http://arxiv.org/abs/2410.14586v1)|null|
|**2024-10-18**|**Do LLMs estimate uncertainty well in instruction-following?**|Juyeon Heo et.al.|[2410.14582v1](http://arxiv.org/abs/2410.14582v1)|null|
|**2024-10-18**|**Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection**|Aaron Alvarado Kristanto Julistiono et.al.|[2410.14581v1](http://arxiv.org/abs/2410.14581v1)|null|
|**2024-10-18**|**Towards Unsupervised Validation of Anomaly-Detection Models**|Lihi Idan et.al.|[2410.14579v1](http://arxiv.org/abs/2410.14579v1)|null|
|**2024-10-18**|**Large Language Models Are Overparameterized Text Encoders**|Thennal D K et.al.|[2410.14578v1](http://arxiv.org/abs/2410.14578v1)|null|
|**2024-10-18**|**MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts**|Rachel S. Y. Teo et.al.|[2410.14574v1](http://arxiv.org/abs/2410.14574v1)|[link](https://github.com/rachtsy/momentumsmoe)|
|**2024-10-18**|**Building Trust in Black-box Optimization: A Comprehensive Framework for Explainability**|Nazanin Nezami et.al.|[2410.14573v1](http://arxiv.org/abs/2410.14573v1)|null|
|**2024-10-18**|**TransBox: EL++-closed Ontology Embedding**|Hui Yang et.al.|[2410.14571v1](http://arxiv.org/abs/2410.14571v1)|null|
|**2024-10-18**|**When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**|Hanna Kim et.al.|[2410.14569v1](http://arxiv.org/abs/2410.14569v1)|null|
|**2024-10-18**|**RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions**|Zhiyuan Peng et.al.|[2410.14567v1](http://arxiv.org/abs/2410.14567v1)|null|
|**2024-10-18**|**Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization**|Frederic Kirstein et.al.|[2410.14545v1](http://arxiv.org/abs/2410.14545v1)|[link](https://github.com/FKIRSTE/emnlp2024-personalized-meeting-sum)|
|**2024-10-18**|**Less is More: Selective Reduction of CT Data for Self-Supervised Pre-Training of Deep Learning Models with Contrastive Learning Improves Downstream Classification Performance**|Daniel Wolf et.al.|[2410.14524v1](http://arxiv.org/abs/2410.14524v1)|[link](https://github.com/Wolfda95/Less_is_More)|
|**2024-10-18**|**Do LLMs "know" internally when they follow instructions?**|Juyeon Heo et.al.|[2410.14516v1](http://arxiv.org/abs/2410.14516v1)|null|
|**2024-10-18**|**Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media**|Owen Cook et.al.|[2410.14515v1](http://arxiv.org/abs/2410.14515v1)|[link](https://github.com/minieggz/ruc-misinfo)|
|**2024-10-18**|**LEAD: Latent Realignment for Human Motion Diffusion**|Nefeli Andreou et.al.|[2410.14508v1](http://arxiv.org/abs/2410.14508v1)|null|
|**2024-10-18**|**SignAttention: On the Interpretability of Transformer Models for Sign Language Translation**|Pedro Alejandro Dal Bianco et.al.|[2410.14506v1](http://arxiv.org/abs/2410.14506v1)|null|
|**2024-10-18**|**ANT: Adaptive Noise Schedule for Time Series Diffusion Models**|Seunghan Lee et.al.|[2410.14488v1](http://arxiv.org/abs/2410.14488v1)|[link](https://github.com/seunghan96/ant)|
|**2024-10-18**|**DRL Optimization Trajectory Generation via Wireless Network Intent-Guided Diffusion Models for Optimizing Resource Allocation**|Junjie Wu et.al.|[2410.14481v1](http://arxiv.org/abs/2410.14481v1)|null|
|**2024-10-18**|**Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of Language Models**|James Vo et.al.|[2410.14480v1](http://arxiv.org/abs/2410.14480v1)|null|
|**2024-10-18**|**How Do Training Methods Influence the Utilization of Vision Models?**|Paul Gavrikov et.al.|[2410.14470v1](http://arxiv.org/abs/2410.14470v1)|null|
|**2024-10-18**|**The Propensity for Density in Feed-forward Models**|Nandi Schoots et.al.|[2410.14461v1](http://arxiv.org/abs/2410.14461v1)|null|
|**2024-10-18**|**Toward Generalizing Visual Brain Decoding to Unseen Subjects**|Xiangtao Kong et.al.|[2410.14445v1](http://arxiv.org/abs/2410.14445v1)|[link](https://github.com/xiangtaokong/tgbd)|
|**2024-10-18**|**A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference**|You Wu et.al.|[2410.14442v1](http://arxiv.org/abs/2410.14442v1)|[link](https://github.com/whyNLP/LCKV)|
|**2024-10-18**|**Learning to refine domain knowledge for biological network inference**|Peiwen Li et.al.|[2410.14436v1](http://arxiv.org/abs/2410.14436v1)|null|
|**2024-10-18**|**FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models**|Rui Hu et.al.|[2410.14429v1](http://arxiv.org/abs/2410.14429v1)|null|
|**2024-10-18**|**Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation**|Shuai Zhao et.al.|[2410.14425v1](http://arxiv.org/abs/2410.14425v1)|[link](https://github.com/shuaizhao95/Unlearning)|
|**2024-10-18**|**Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion**|Denitsa Saynova et.al.|[2410.14405v1](http://arxiv.org/abs/2410.14405v1)|null|
|**2024-10-18**|**SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning**|Magdalena Wysocka et.al.|[2410.14399v1](http://arxiv.org/abs/2410.14399v1)|null|
|**2024-10-18**|**Generative AI, Pragmatics, and Authenticity in Second Language Learning**|Robert Godwin-Jones` et.al.|[2410.14395v1](http://arxiv.org/abs/2410.14395v1)|null|
|**2024-10-18**|**Debug Smarter, Not Harder: AI Agents for Error Resolution in Computational Notebooks**|Konstantin Grotov et.al.|[2410.14393v1](http://arxiv.org/abs/2410.14393v1)|null|
|**2024-10-18**|**Analyzing Context Utilization of LLMs in Document-Level Translation**|Wafaa Mohammed et.al.|[2410.14391v1](http://arxiv.org/abs/2410.14391v1)|null|
|**2024-10-18**|**SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery**|Enneng Yang et.al.|[2410.14389v1](http://arxiv.org/abs/2410.14389v1)|[link](https://github.com/ennengyang/surgeryv2)|
|**2024-10-18**|**How Do Multilingual Models Remember? Investigating Multilingual Factual Recall Mechanisms**|Constanza Fierro et.al.|[2410.14387v1](http://arxiv.org/abs/2410.14387v1)|null|
|**2024-10-18**|**Fine-Tuning Pre-trained Language Models for Robust Causal Representation Learning**|Jialin Yu et.al.|[2410.14375v1](http://arxiv.org/abs/2410.14375v1)|null|
|**2024-10-18**|**CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic**|Huaiyuan Yao et.al.|[2410.14368v1](http://arxiv.org/abs/2410.14368v1)|[link](https://github.com/hyan-yao/comal)|
|**2024-10-18**|**Efficiently Computing Susceptibility to Context in Language Models**|Tianyu Liu et.al.|[2410.14361v1](http://arxiv.org/abs/2410.14361v1)|null|
|**2024-10-18**|**A Scientific Machine Learning Approach for Predicting and Forecasting Battery Degradation in Electric Vehicles**|Sharv Murgai et.al.|[2410.14347v1](http://arxiv.org/abs/2410.14347v1)|null|
|**2024-10-18**|**Critical Questions Generation: Motivation and Challenges**|Blanca Calvo Figueras et.al.|[2410.14335v1](http://arxiv.org/abs/2410.14335v1)|[link](https://github.com/hitz-zentroa/critical_questions_generation)|
|**2024-10-18**|**LoGU: Long-form Generation with Uncertainty Expressions**|Ruihan Yang et.al.|[2410.14309v1](http://arxiv.org/abs/2410.14309v1)|null|
|**2024-10-18**|**SwaQuAD-24: QA Benchmark Dataset in Swahili**|Alfred Malengo Kondoro et.al.|[2410.14289v1](http://arxiv.org/abs/2410.14289v1)|null|
|**2024-10-18**|**EcomEdit: An Automated E-commerce Knowledge Editing Framework for Enhanced Product and Purchase Intention Understanding**|Ching Ming Samuel Lau et.al.|[2410.14276v1](http://arxiv.org/abs/2410.14276v1)|null|
|**2024-10-18**|**REEF: Representation Encoding Fingerprints for Large Language Models**|Jie Zhang et.al.|[2410.14273v1](http://arxiv.org/abs/2410.14273v1)|[link](https://github.com/tmylla/reef)|
|**2024-10-18**|**MoDification: Mixture of Depths Made Easy**|Chen Zhang et.al.|[2410.14268v1](http://arxiv.org/abs/2410.14268v1)|null|
|**2024-10-18**|**Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation**|Edward et.al.|[2410.14262v1](http://arxiv.org/abs/2410.14262v1)|null|
|**2024-10-18**|**Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement**|Zihao Cheng et.al.|[2410.14259v1](http://arxiv.org/abs/2410.14259v1)|null|
|**2024-10-18**|**Revisiting SLO and Goodput Metrics in LLM Serving**|Zhibin Wang et.al.|[2410.14257v1](http://arxiv.org/abs/2410.14257v1)|null|
|**2024-10-18**|**Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas**|Xiang Hu et.al.|[2410.14255v1](http://arxiv.org/abs/2410.14255v1)|null|
|**2024-10-18**|**Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation**|Shuo Tang et.al.|[2410.14251v1](http://arxiv.org/abs/2410.14251v1)|null|
|**2024-10-18**|**Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models**|Olga Loginova et.al.|[2410.14248v1](http://arxiv.org/abs/2410.14248v1)|null|
|**2024-10-18**|**Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction**|Manuel Brenner et.al.|[2410.14240v1](http://arxiv.org/abs/2410.14240v1)|[link](https://github.com/DurstewitzLab/ALRNN-DSR)|
|**2024-10-18**|**A Novel Method to Metigate Demographic and Expert Bias in ICD Coding with Causal Inference**|Bin Zhang et.al.|[2410.14236v1](http://arxiv.org/abs/2410.14236v1)|null|
|**2024-10-18**|**Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning**|Gaurav Arora et.al.|[2410.14235v1](http://arxiv.org/abs/2410.14235v1)|null|
|**2024-10-18**|**Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework**|Zhen Tao et.al.|[2410.14231v1](http://arxiv.org/abs/2410.14231v1)|null|
|**2024-10-18**|**Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model**|Li Yuan et.al.|[2410.14225v1](http://arxiv.org/abs/2410.14225v1)|null|
|**2024-10-18**|**Formal Explanations for Neuro-Symbolic AI**|Sushmita Paul et.al.|[2410.14219v1](http://arxiv.org/abs/2410.14219v1)|null|
|**2024-10-18**|**Paths-over-Graph: Knowledge Graph Enpowered Large Language Model Reasoning**|Xingyu Tan et.al.|[2410.14211v1](http://arxiv.org/abs/2410.14211v1)|null|
|**2024-10-18**|**Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning**|Xiaochuan Li et.al.|[2410.14208v1](http://arxiv.org/abs/2410.14208v1)|[link](https://github.com/cxcscmu/montessori-instruct)|
|**2024-10-18**|**MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations**|Vishal Vivek Saley et.al.|[2410.14204v1](http://arxiv.org/abs/2410.14204v1)|null|
|**2024-10-18**|**Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs**|SeongYeub Chu et.al.|[2410.14202v1](http://arxiv.org/abs/2410.14202v1)|null|
|**2024-10-18**|**E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model**|Haoran Lai et.al.|[2410.14200v1](http://arxiv.org/abs/2410.14200v1)|null|
|**2024-10-18**|**Supervised Chain of Thought**|Xiang Zhang et.al.|[2410.14198v1](http://arxiv.org/abs/2410.14198v1)|null|
|**2024-10-18**|**Speciesism in Natural Language Processing Research**|Masashi Takeshita et.al.|[2410.14194v1](http://arxiv.org/abs/2410.14194v1)|null|
|**2024-10-18**|**MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time**|Mozhi Zhang et.al.|[2410.14184v1](http://arxiv.org/abs/2410.14184v1)|[link](https://github.com/Jihuai-wpy/MetaAlign)|
|**2024-10-18**|**LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs**|Yujun Zhou et.al.|[2410.14182v1](http://arxiv.org/abs/2410.14182v1)|null|
|**2024-10-18**|**XForecast: Evaluating Natural Language Explanations for Time Series Forecasting**|Taha Aksu et.al.|[2410.14180v1](http://arxiv.org/abs/2410.14180v1)|null|
|**2024-10-18**|**MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems**|Zifeng Zhu et.al.|[2410.14179v1](http://arxiv.org/abs/2410.14179v1)|null|
|**2024-10-18**|**LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems**|Nan Xu et.al.|[2410.14166v1](http://arxiv.org/abs/2410.14166v1)|null|
|**2024-10-18**|**Automated Genre-Aware Article Scoring and Feedback Using Large Language Models**|Chihang Wang et.al.|[2410.14165v1](http://arxiv.org/abs/2410.14165v1)|null|
|**2024-10-18**|**Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning**|Jiacheng Ye et.al.|[2410.14157v1](http://arxiv.org/abs/2410.14157v1)|null|
|**2024-10-18**|**Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models**|Wei Jie Yeo et.al.|[2410.14155v1](http://arxiv.org/abs/2410.14155v1)|null|
|**2024-10-18**|**RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training**|Muhe Ding et.al.|[2410.14154v1](http://arxiv.org/abs/2410.14154v1)|null|
|**2024-10-18**|**SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent**|Jiarui Ji et.al.|[2410.14152v1](http://arxiv.org/abs/2410.14152v1)|null|
|**2024-10-18**|**Utilizing Large Language Models for Event Deconstruction to Enhance Multimodal Aspect-Based Sentiment Analysis**|Xiaoyong Huang et.al.|[2410.14150v1](http://arxiv.org/abs/2410.14150v1)|null|
|**2024-10-18**|**Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment**|Chenhang Cui et.al.|[2410.14148v1](http://arxiv.org/abs/2410.14148v1)|null|
|**2024-10-18**|**CausalChat: Interactive Causal Model Development and Refinement Using Large Language Models**|Yanming Zhang et.al.|[2410.14146v1](http://arxiv.org/abs/2410.14146v1)|null|
|**2024-10-18**|**CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using Large Language Models**|June M. Liu et.al.|[2410.14145v1](http://arxiv.org/abs/2410.14145v1)|null|
|**2024-10-18**|**A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models**|Chenyang Zhang et.al.|[2410.14144v1](http://arxiv.org/abs/2410.14144v1)|null|

#### Abstracts
##### **Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts**
2410.14677v1 by German Gritsai, Anastasia Voznyuk, Andrey Grabovoy, Yury Chekhovich

The rapid development of autoregressive Large Language Models (LLMs) has
significantly improved the quality of generated texts, necessitating reliable
machine-generated text detectors. A huge number of detectors and collections
with AI fragments have emerged, and several detection methods even showed
recognition quality up to 99.9% according to the target metrics in such
collections. However, the quality of such detectors tends to drop dramatically
in the wild, posing a question: Are detectors actually highly trustworthy or do
their high benchmark scores come from the poor quality of evaluation datasets?
In this paper, we emphasise the need for robust and qualitative methods for
evaluating generated data to be secure against bias and low generalising
ability of future model. We present a systematic review of datasets from
competitions dedicated to AI-generated content detection and propose methods
for evaluating the quality of datasets containing AI-generated fragments. In
addition, we discuss the possibility of using high-quality generated data to
achieve two goals: improving the training of detection models and improving the
training datasets themselves. Our contribution aims to facilitate a better
understanding of the dynamics between human and machine text, which will
ultimately support the integrity of information in an increasingly automated
world.

摘要：隨著自迴歸大型語言模型 (LLM) 的快速發展，已顯著提升生成文字的品質，因此有必要使用可靠的機器產生的文字偵測器。已出現大量偵測器與包含 AI 片段的集合，根據這些集合中的目標指標，甚至有幾種偵測方法顯示出高達 99.9% 的辨識品質。然而，此類偵測器的品質在實際應用中往往會大幅下降，這引發了一個問題：偵測器實際上是否高度可信，還是其高基準分數來自於評估資料集的品質不佳？在本文中，我們強調需要穩健且定性的方法來評估生成資料，以確保未來模型不會有偏差且具備低泛化能力。我們對專門用於 AI 產生的內容偵測的競賽中所使用的資料集進行系統性回顧，並提出評估包含 AI 產生的片段的資料集品質的方法。此外，我們討論了使用高品質生成資料來達成兩個目標的可能性：改善偵測模型的訓練以及改善訓練資料集本身。我們的貢獻旨在促進對人類與機器文字之間動態的更深入了解，這最終將支持在日益自動化的世界中資訊的完整性。

##### **SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment**
2410.14676v1 by Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen

Existing preference alignment is a one-size-fits-all alignment mechanism,
where the part of the large language model (LLM) parametric knowledge with
non-preferred features is uniformly blocked to all the users. However, this
part of knowledge can be useful to advanced users whose expertise qualifies
them to handle these information. The one-size-fits-all alignment mechanism
undermines LLM's utility for these qualified users. To address this problem, we
propose SudoLM, a framework that lets LLMs learn access control over specific
parametric knowledge for users with different credentials via authorization
alignment. SudoLM allows authorized users to unlock their access to all the
parametric knowledge with an assigned SUDO key while blocking access to
non-qualified users. Experiments on two application scenarios demonstrate that
SudoLM effectively controls the user's access to the parametric knowledge and
maintains its general utility.

摘要：現有的偏好比對是一種一體適用的比對機制，
其中大型語言模型 (LLM) 參數知識的部分具有
非首選功能，對所有使用者均一律封鎖。然而，這
部分知識對專家使用者來說可能很有用，因為他們的專業知識有資格
處理這些資訊。一體適用的比對機制會破壞 LLM 對這些合格使用者的效用。為了解決這個問題，我們
提出 SudoLM，一個框架，讓 LLM 透過授權比對學習對具有不同憑證的使用者存取特定
參數知識的存取控制。SudoLM 允許授權使用者使用指定的 SUDO 金鑰解鎖他們對所有
參數知識的存取，同時封鎖非合格使用者的存取。在兩個應用場景的實驗中證明，
SudoLM 有效地控制使用者對參數知識的存取，並維持其一般效用。

##### **Enhancing Large Language Models' Situated Faithfulness to External Contexts**
2410.14675v1 by Yukun Huang, Sanxing Chen, Hongyi Cai, Bhuwan Dhingra

Large Language Models (LLMs) are often augmented with external information as
contexts, but this external information can sometimes be inaccurate or even
intentionally misleading. We argue that robust LLMs should demonstrate situated
faithfulness, dynamically calibrating their trust in external information based
on their confidence in the internal knowledge and the external context. To
benchmark this capability, we evaluate LLMs across several QA datasets,
including a newly created dataset called RedditQA featuring in-the-wild
incorrect contexts sourced from Reddit posts. We show that when provided with
both correct and incorrect contexts, both open-source and proprietary models
tend to overly rely on external information, regardless of its factual
accuracy. To enhance situated faithfulness, we propose two approaches:
Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning
(RCR). SCR enables models to self-access the confidence of external information
relative to their own internal knowledge to produce the most accurate answer.
RCR, in contrast, extracts explicit confidence signals from the LLM and
determines the final answer using predefined rules. Our results show that for
LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR
outperforms RCR, achieving improvements of up to 24.2% over a direct input
augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR
outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct
Preference Optimization (CR-DPO) method improves performance on both seen and
unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In
addition to quantitative results, we offer insights into the relative strengths
of SCR and RCR. Our findings highlight promising avenues for improving situated
faithfulness in LLMs. The data and code are released.

摘要：大型語言模型 (LLM) 通常會以外部資訊作為情境進行擴充，但這些外部資訊有時可能不正確，甚至故意具有誤導性。我們認為，強健的 LLM 應展現情境忠實度，根據其對內部知識和外部情境的信心，動態校準其對外部資訊的信任。為了評量此項能力，我們針對多個問答資料集評估 LLM，包括一個新建立的資料集 RedditQA，其特點是從 Reddit 貼文中擷取的真實不正確情境。我們發現，當提供正確和不正確的情境時，不論是開放原始碼或專有模型，都傾向過度依賴外部資訊，而不管其事實正確性。為了增強情境忠實度，我們提出兩種方法：自導信心推理 (SCR) 和基於規則的信心推理 (RCR)。SCR 使模型能夠自行存取外部資訊的信心，相對於其自身的內部知識，以產生最準確的答案。相反地，RCR 從 LLM 中擷取明確的信心訊號，並使用預定義的規則來決定最終答案。我們的結果顯示，對於具有強大推理能力的 LLM，例如 GPT-4o 和 GPT-4o mini，SCR 優於 RCR，與直接輸入擴充基準相比，改進幅度高達 24.2%。相反地，對於較小的模型，例如 Llama-3-8B，RCR 優於 SCR。使用我們提出的信心推理直接偏好最佳化 (CR-DPO) 方法微調 SCR，可以提高已見和未見資料集的效能，在 Llama-3-8B 上產生平均 8.9% 的改進。除了量化結果外，我們還提供了對 SCR 和 RCR 相對優勢的見解。我們的發現突顯了改善 LLM 中情境忠實度的有前景途徑。資料和程式碼已發布。

##### **BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities**
2410.14672v1 by Shaozhe Hao, Xuantong Liu, Xianbiao Qi, Shihao Zhao, Bojia Zi, Rong Xiao, Kai Han, Kwan-Yee K. Wong

We introduce BiGR, a novel conditional image generation model using compact
binary latent codes for generative training, focusing on enhancing both
generation and representation capabilities. BiGR is the first conditional
generative model that unifies generation and discrimination within the same
framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a
binary transcoder for binary code prediction. Additionally, we introduce a
novel entropy-ordered sampling method to enable efficient image generation.
Extensive experiments validate BiGR's superior performance in generation
quality, as measured by FID-50k, and representation capabilities, as evidenced
by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization
across various vision tasks, enabling applications such as image inpainting,
outpainting, editing, interpolation, and enrichment, without the need for
structural modifications. Our findings suggest that BiGR unifies generative and
discriminative tasks effectively, paving the way for further advancements in
the field.

摘要：我們引進 BiGR，這是一種新穎的條件圖像生成模型，使用緊湊的二進制潛在碼進行生成訓練，專注於增強生成和表示能力。BiGR 是第一個在同一架構內統一生成和判別的條件生成模型。BiGR 具有二進制分詞器、遮罩建模機制和二進制轉碼器，用於二進制碼預測。此外，我們引入了一種新穎的熵排序採樣方法，以實現高效的圖像生成。廣泛的實驗驗證了 BiGR 在生成品質方面的優異性能，如 FID-50k 所測量，以及表示能力，如線性探測準確度所證明。此外，BiGR 展示了跨各種視覺任務的零次學習泛化，實現了圖像修復、外繪、編輯、插值和豐富化等應用，而無需進行結構修改。我們的研究結果表明，BiGR 有效地統一了生成和判別任務，為該領域的進一步發展鋪平了道路。

##### **NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples**
2410.14669v1 by Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, Deva Ramanan

Vision-language models (VLMs) have made significant progress in recent
visual-question-answering (VQA) benchmarks that evaluate complex
visio-linguistic reasoning. However, are these models truly effective? In this
work, we show that VLMs still struggle with natural images and questions that
humans can easily answer, which we term natural adversarial samples. We also
find it surprisingly easy to generate these VQA samples from natural image-text
corpora using off-the-shelf models like CLIP and ChatGPT. We propose a
semi-automated approach to collect a new benchmark, NaturalBench, for reliably
evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a
$\textbf{vision-centric}$ design by pairing each question with two images that
yield different answers, preventing blind solutions from answering without
using the images. This makes NaturalBench more challenging than previous
benchmarks that can be solved with commonsense priors. We evaluate 53
state-of-the-art VLMs on NaturalBench, showing that models like
LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o
lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is
hard from two angles: (1) Compositionality: Solving NaturalBench requires
diverse visio-linguistic skills, including understanding attribute bindings,
object relationships, and advanced reasoning like logic and counting. To this
end, unlike prior work that uses a single tag per sample, we tag each
NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)
Biases: NaturalBench exposes severe biases in VLMs, as models often choose the
same answer regardless of the image. Lastly, we apply our benchmark curation
method to diverse data sources, including long captions (over 100 words) and
non-English languages like Chinese and Hindi, highlighting its potential for
dynamic evaluations of VLMs.

摘要：<paragraph>視覺語言模型 (VLM) 在最近的視覺問答 (VQA) 基準測試中取得顯著進展，這些基準測試評估了複雜的視覺語言推理能力。然而，這些模型是否真正有效？在這項工作中，我們展示了 VLM 仍難以處理人類可以輕鬆回答的自然影像和問題，我們將其稱為自然對抗樣本。我們還發現，使用現成的模型（例如 CLIP 和 ChatGPT）從自然影像文字語料庫中生成這些 VQA 樣本出奇地容易。我們提出了一種半自動化方法來收集一個新的基準測試 NaturalBench，以便使用 10,000 個經人類驗證的 VQA 樣本可靠地評估 VLM。至關重要的是，我們採用了**以視覺為中心**的設計，將每個問題與兩張產生不同答案的影像配對，防止盲解在不使用影像的情況下回答問題。這使得 NaturalBench 比可以使用常識先驗來解決的先前基準測試更具挑戰性。我們在 NaturalBench 上評估了 53 個最先進的 VLM，結果顯示 LLaVA-OneVision、Cambrian-1、Llama3.2-Vision、Molmo、Qwen2-VL，甚至 GPT-4o 等模型落後於人類表現 (超過 90%) 50%-70%。我們從兩個角度分析了 NaturalBench 的難點：(1) 組合性：解決 NaturalBench 需要多樣化的視覺語言技能，包括理解屬性繫結、物件關係以及高級推理，例如邏輯和計數。為此，與先前使用每個樣本一個標籤的工作不同，我們使用 1 到 8 個技能標籤標記每個 NaturalBench 樣本，以進行細微的評估。(2) 偏差：NaturalBench 揭露了 VLM 中嚴重的偏差，因為模型通常會選擇相同的答案，而不管影像為何。最後，我們將基準策展方法應用於各種數據來源，包括長標題（超過 100 個字）和非英語語言（例如中文和印地語），突顯了其對 VLM 進行動態評估的潛力。</paragraph>

##### **MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps**
2410.14668v1 by Xiongtao Zhou, Jie He, Lanyu Chen, jingyu li, Haojing Chen, Victor Gutierrez Basulto, Jeff Z. Pan, Hanjie Chen

Multimodal Chain of Thought (MCoT) is a popular prompting strategy for
improving the performance of multimodal large language models (MLLMs) across a
range of complex reasoning tasks. Despite its popularity, there is a notable
absence of automated methods for evaluating the quality of reasoning steps in
MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation
(MiCEval), a framework designed to assess the correctness of reasoning chains
by evaluating the quality of both the description and each reasoning step. The
evaluation of the description component focuses on the accuracy of the image
descriptions, while the reasoning step evaluates the quality of each step as it
is conditionally generated based on the preceding steps. MiCEval is built upon
a fine-grained dataset with annotations that rate each step according to
correctness, relevance, and informativeness. Extensive experiments on four
state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more
closely with human judgments compared to existing methods based on cosine
similarity or fine-tuning approaches. MiCEval datasets and code can be found in
https://github.com/alenai97/MiCEval.

摘要：多模態思維鏈（MCoT）是一種流行的提示策略，用於提升多模態大型語言模型（MLLM）在各種複雜推理任務中的表現。儘管它很受歡迎，但對於評估 MCoT 中推理步驟品質的自動化方法卻明顯不足。為了解決這個問題，我們提出了多模態思維鏈評估（MiCEval），一個旨在評估推理鏈正確性的框架，方法是評估描述和每個推理步驟的品質。描述組成的評估重點在於影像描述的準確性，而推理步驟則評估每個步驟在根據前序步驟有條件產生的品質。MiCEval 建立在一個細緻的資料集上，其中包含根據正確性、相關性和資訊性對每個步驟進行評分的註解。針對四個最先進的 MLLM 進行的廣泛實驗顯示，使用 MiCEval 進行的逐步評估與人類的判斷更為一致，與基於餘弦相似性或微調方法的現有方法相比。MiCEval 資料集和程式碼可在 https://github.com/alenai97/MiCEval 中找到。

##### **DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph**
2410.14666v1 by Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra

Summarizing movie screenplays presents a unique set of challenges compared to
standard document summarization. Screenplays are not only lengthy, but also
feature a complex interplay of characters, dialogues, and scenes, with numerous
direct and subtle relationships and contextual nuances that are difficult for
machine learning models to accurately capture and comprehend. Recent attempts
at screenplay summarization focus on fine-tuning transformer-based pre-trained
models, but these models often fall short in capturing long-term dependencies
and latent relationships, and frequently encounter the "lost in the middle"
issue. To address these challenges, we introduce DiscoGraMS, a novel resource
that represents movie scripts as a movie character-aware discourse graph (CaD
Graph). This approach is well-suited for various downstream tasks, such as
summarization, question-answering, and salience detection. The model aims to
preserve all salient information, offering a more comprehensive and faithful
representation of the screenplay's content. We further explore a baseline
method that combines the CaD Graph with the corresponding movie script through
a late fusion of graph and text modalities, and we present very initial
promising results.

摘要：與標準文件摘要相比，摘要電影劇本呈現出一組獨特的挑戰。劇本不僅篇幅冗長，而且還具有角色、對話和場景的複雜相互作用，其中包含許多直接和微妙的關係以及語境細微差別，這對於機器學習模型來說難以準確捕捉和理解。最近對劇本摘要的嘗試專注於微調基於轉換器的預訓練模型，但這些模型在捕捉長期依賴性和潛在關係方面常常不足，並且經常遇到「在中間迷失」的問題。為了應對這些挑戰，我們引入了 DiscoGraMS，這是一種新穎的資源，它將電影腳本表示為一個電影角色感知話語圖（CaD 圖）。這種方法非常適合各種下游任務，例如摘要、問答和顯著性檢測。該模型旨在保留所有顯著信息，提供對劇本內容更全面、更忠實的表示。我們進一步探討了一種基線方法，該方法通過圖形和文本模式的後期融合將 CaD 圖與對應的電影腳本結合起來，並且我們展示了非常有希望的初步結果。

##### **Real-time Fake News from Adversarial Feedback**
2410.14651v1 by Sanxing Chen, Yukun Huang, Bhuwan Dhingra

We show that existing evaluations for fake news detection based on
conventional sources, such as claims on fact-checking websites, result in an
increasing accuracy over time for LLM-based detectors -- even after their
knowledge cutoffs. This suggests that recent popular political claims, which
form the majority of fake news on such sources, are easily classified using
surface-level shallow patterns. Instead, we argue that a proper fake news
detection dataset should test a model's ability to reason factually about the
current world by retrieving and reading related evidence. To this end, we
develop a novel pipeline that leverages natural language feedback from a
RAG-based detector to iteratively modify real-time news into deceptive fake
news that challenges LLMs. Our iterative rewrite decreases the binary
classification AUC by an absolute 17.5 percent for a strong RAG GPT-4o
detector. Our experiments reveal the important role of RAG in both detecting
and generating fake news, as retrieval-free LLM detectors are vulnerable to
unseen events and adversarial attacks, while feedback from RAG detection helps
discover more deceitful patterns in fake news.

摘要：我們證明，基於傳統來源（例如事實查核網站上的聲明）的現有假新聞偵測評估，會隨著時間推移而提高 LLM 為基礎的偵測器的準確度，即使在它們的知識截止之後。這表明此類來源上的近期熱門政治聲明，構成此類來源上大多數假新聞，很容易使用表面層次的淺層模式進行分類。相反，我們認為適當的假新聞偵測資料集應測試模型根據相關證據檢索和閱讀，對當前世界進行事實推理的能力。為此，我們開發了一個創新的管道，利用基於 RAG 的偵測器的自然語言回饋，以反覆修改即時新聞，使其成為挑戰 LLM 的欺騙性假新聞。我們的反覆改寫將強大的 RAG GPT-4o 偵測器的二元分類 AUC 絕對降低了 17.5 個百分點。我們的實驗揭示了 RAG 在偵測和產生假新聞中扮演的重要角色，因為無檢索的 LLM 偵測器容易受到未見事件和對抗性攻擊，而 RAG 偵測的回饋有助於發現假新聞中更多具有欺騙性的模式。

##### **Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs**
2410.14641v1 by Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang, Xiaojiang Liu

Positional bias in large language models (LLMs) hinders their ability to
effectively process long inputs. A prominent example is the "lost in the
middle" phenomenon, where LLMs struggle to utilize relevant information
situated in the middle of the input. While prior research primarily focuses on
single pieces of relevant information, real-world applications often involve
multiple relevant information pieces. To bridge this gap, we present
LongPiBench, a benchmark designed to assess positional bias involving multiple
pieces of relevant information. Thorough experiments are conducted with five
commercial and six open-source models. These experiments reveal that while most
current models are robust against the "lost in the middle" issue, there exist
significant biases related to the spacing of relevant information pieces. These
findings highlight the importance of evaluating and reducing positional biases
to advance LLM's capabilities.

摘要：大型語言模型 (LLM) 中的位置偏差會阻礙其有效處理長輸入的能力。一個顯著的例子是「迷失在中間」現象，其中 LLM 難以利用位於輸入中間的相关資訊。儘管先前的研究主要關注單一相關資訊，但現實世界的應用通常涉及多個相關資訊。為了彌合這個差距，我們提出了 LongPiBench，這是一個基準，旨在評估涉及多個相關資訊的位置偏差。對五個商業和六個開源模型進行了徹底的實驗。這些實驗表明，雖然大多數當前模型對「迷失在中間」問題具有魯棒性，但與相關資訊間距相關的顯著偏差確實存在。這些發現強調了評估和減少位置偏差以提升 LLM 能力的重要性。

##### **GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings**
2410.14635v1 by Raghuveer Thirukovalluru, Bhuwan Dhingra

Training-free embedding methods directly leverage pretrained large language
models (LLMs) to embed text, bypassing the costly and complex procedure of
contrastive learning. Previous training-free embedding methods have mainly
focused on optimizing embedding prompts and have overlooked the benefits of
utilizing the generative abilities of LLMs. We propose a novel method, GenEOL,
which uses LLMs to generate diverse transformations of a sentence that preserve
its meaning, and aggregates the resulting embeddings of these transformations
to enhance the overall sentence embedding. GenEOL significantly outperforms the
existing training-free embedding methods by an average of 2.85 points across
several LLMs on the sentence semantic text similarity (STS) benchmark. Our
analysis shows that GenEOL stabilizes representation quality across LLM layers
and is robust to perturbations of embedding prompts. GenEOL also achieves
notable gains on multiple clustering, reranking and pair-classification tasks
from the MTEB benchmark.

摘要：無需訓練的嵌入方法直接利用預先訓練好的大型語言模型 (LLM) 來嵌入文字，繞過對比式學習的昂貴且複雜的程序。先前的無需訓練的嵌入方法主要專注於最佳化嵌入提示，且忽略了利用 LLM 的生成能力的好處。我們提出了一種新穎的方法 GenEOL，它使用 LLM 來產生保留其含義的句子之不同轉換，並聚合這些轉換的嵌入結果，以增強整體句子嵌入。GenEOL 在句子語義文字相似性 (STS) 基準上，透過多個 LLM，平均高出現有無需訓練的嵌入方法 2.85 分。我們的分析顯示，GenEOL 穩定 LLM 層間的表示品質，且對於嵌入提示的擾動具有穩健性。GenEOL 也在 MTEB 基準中的多個分群、重新排序和成對分類任務中取得顯著進展。

##### **Diverging Preferences: When do Annotators Disagree and do Models Know?**
2410.14632v1 by Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin

We examine diverging preferences in human-labeled preference datasets. We
develop a taxonomy of disagreement sources spanning 10 categories across four
high-level classes -- task underspecification, response style, refusals, and
annotation errors. We find that the majority of disagreements are in opposition
with standard reward modeling approaches, which are designed with the
assumption that annotator disagreement is noise. We then explore how these
findings impact two areas of LLM development: reward modeling and evaluation.
In our experiments, we demonstrate how standard reward modeling methods, like
the Bradley-Terry model, fail to differentiate whether a given preference
judgment is the result of unanimous agreement among annotators or the majority
opinion among diverging user preferences. We also find that these tendencies
are also echoed by popular LLM-as-Judge evaluation methods, which consistently
identify a winning response in cases of diverging preferences. These findings
highlight remaining challenges in LLM evaluations, which are greatly influenced
by divisive features like response style, and in developing pluralistically
aligned LLMs. To address these issues, we develop methods for identifying
diverging preferences to mitigate their influence on evaluation and training.

摘要：我們檢視人類標記偏好資料集中不同的偏好。我們開發了一個涵蓋四個高級別類別中 10 個類別的分歧來源分類法——任務未明確、回應風格、拒絕和註解錯誤。我們發現大多數分歧與標準獎勵建模方法相反，這些方法的設計基於註解者的分歧是雜訊的假設。然後，我們探討這些發現如何影響 LLM 開發的兩個領域：獎勵建模和評估。在我們的實驗中，我們展示了標準獎勵建模方法（例如 Bradley-Terry 模型）如何無法區分給定的偏好判斷是註解者一致同意還是不同使用者偏好中的多數意見。我們還發現這些趨勢也反映在流行的 LLM-as-Judge 評估方法中，這些方法在不同偏好的情況下始終確定獲勝的回應。這些發現突出了 LLM 評估中仍然存在的挑戰，這些挑戰很大程度上受到回應風格等分裂特徵的影響，並且在開發多元化的 LLM 時遇到了挑戰。為了解決這些問題，我們開發了識別不同偏好的方法，以減輕其對評估和訓練的影響。

##### **On the Regularization of Learnable Embeddings for Time Series Processing**
2410.14630v1 by Luca Butera, Giovanni De Felice, Andrea Cini, Cesare Alippi

In processing multiple time series, accounting for the individual features of
each sequence can be challenging. To address this, modern deep learning methods
for time series analysis combine a shared (global) model with local layers,
specific to each time series, often implemented as learnable embeddings.
Ideally, these local embeddings should encode meaningful representations of the
unique dynamics of each sequence. However, when these are learned end-to-end as
parameters of a forecasting model, they may end up acting as mere sequence
identifiers. Shared processing blocks may then become reliant on such
identifiers, limiting their transferability to new contexts. In this paper, we
address this issue by investigating methods to regularize the learning of local
learnable embeddings for time series processing. Specifically, we perform the
first extensive empirical study on the subject and show how such
regularizations consistently improve performance in widely adopted
architectures. Furthermore, we show that methods preventing the co-adaptation
of local and global parameters are particularly effective in this context. This
hypothesis is validated by comparing several methods preventing the downstream
models from relying on sequence identifiers, going as far as completely
resetting the embeddings during training. The obtained results provide an
important contribution to understanding the interplay between learnable local
parameters and shared processing layers: a key challenge in modern time series
processing models and a step toward developing effective foundation models for
time series.

摘要：在處理多個時間序列時，考量每個序列的個別特徵可能具有挑戰性。為了解決這個問題，用於時間序列分析的現代深度學習方法會結合共用（全域）模型與局部層，特定於每個時間序列，通常實作為可學習的嵌入。理想情況下，這些局部嵌入應編碼每個序列獨特動態的有意義表示。然而，當這些動態作為預測模型的參數以端對端的方式學習時，它們可能會最終僅作為序列識別碼。共用處理區塊隨後可能會依賴於此類識別碼，限制其可轉移性至新的脈絡。在本文中，我們透過探討局部可學習嵌入學習的正則化方法來解決這個問題，以進行時間序列處理。具體來說，我們針對這個主題執行第一個廣泛的實證研究，並展示此類正則化如何持續改善廣泛採用的架構中的效能。此外，我們展示防止局部和全域參數共適應的方法在此脈絡中特別有效。此假設透過比較多種防止下游模型依賴序列識別碼的方法獲得驗證，甚至在訓練期間完全重設嵌入。所獲得的結果為了解可學習局部參數與共用處理層之間的交互作用提供了重要的貢獻：現代時間序列處理模型中的關鍵挑戰，以及邁向開發時間序列的有效基礎模型的一步。

##### **CELI: Controller-Embedded Language Model Interactions**
2410.14627v1 by Jan-Samuel Wagner, Dave DeCaprio, Abishek Chiffon Muthu Raja, Jonathan M. Holman, Lauren K. Brady, Sky C. Cheung, Hosein Barzekar, Eric Yang, Mark Anthony Martinez II, David Soong, Sriram Sridhar, Han Si, Brandon W. Higgs, Hisham Hamadeh, Scott Ogden

We introduce Controller-Embedded Language Model Interactions (CELI), a
framework that integrates control logic directly within language model (LM)
prompts, facilitating complex, multi-stage task execution. CELI addresses
limitations of existing prompt engineering and workflow optimization techniques
by embedding control logic directly within the operational context of language
models, enabling dynamic adaptation to evolving task requirements. Our
framework transfers control from the traditional programming execution
environment to the LMs, allowing them to autonomously manage computational
workflows while maintaining seamless interaction with external systems and
functions. CELI supports arbitrary function calls with variable arguments,
bridging the gap between LMs' adaptive reasoning capabilities and conventional
software paradigms' structured control mechanisms. To evaluate CELI's
versatility and effectiveness, we conducted case studies in two distinct
domains: code generation (HumanEval benchmark) and multi-stage content
generation (Wikipedia-style articles). The results demonstrate notable
performance improvements across a range of domains. CELI achieved a 4.9
percentage point improvement over the best reported score of the baseline GPT-4
model on the HumanEval code generation benchmark. In multi-stage content
generation, 94.4% of CELI-produced Wikipedia-style articles met or exceeded
first draft quality when optimally configured, with 44.4% achieving high
quality. These outcomes underscore CELI's potential for optimizing AI-driven
workflows across diverse computational domains.

摘要：<paragraph>我們引入了控制器嵌入式語言模型互動 (CELI)，一種將控制邏輯直接整合到語言模型 (LM) 提示中的框架，促進了複雜的多階段任務執行。CELI 透過將控制邏輯直接嵌入語言模型的操作環境中，解決了現有提示工程和工作流程最佳化技術的限制，實現了對不斷變化的任務需求的動態適應。我們的框架將控制權從傳統的程式執行環境轉移到 LM，讓它們能夠自主管理計算工作流程，同時與外部系統和功能保持無縫互動。CELI 支援使用變數引數的任意函式呼叫，彌合了 LM 的自適應推理能力與傳統軟體範例的結構化控制機制之間的差距。為了評估 CELI 的多功能性和有效性，我們在兩個不同的領域進行了案例研究：程式碼生成 (HumanEval 基準) 和多階段內容生成 (維基百科風格的文章)。結果證明在各種領域中都有顯著的效能提升。在 HumanEval 程式碼生成基準中，CELI 的表現比基線 GPT-4 模型的最佳報告分數提升了 4.9 個百分點。在多階段內容生成中，94.4% 的 CELI 生成的維基百科風格文章在最佳配置下達到或超過初稿品質，其中 44.4% 達到高品質。這些結果強調了 CELI 在優化跨不同計算領域的 AI 驅動工作流程方面的潛力。</paragraph>

##### **You Shall Know a Tool by the Traces it Leaves: The Predictability of Sentiment Analysis Tools**
2410.14626v1 by Daniel Baumartz, Mevlüt Bagci, Alexander Henlein, Maxim Konca, Andy Lücking, Alexander Mehler

If sentiment analysis tools were valid classifiers, one would expect them to
provide comparable results for sentiment classification on different kinds of
corpora and for different languages. In line with results of previous studies
we show that sentiment analysis tools disagree on the same dataset. Going
beyond previous studies we show that the sentiment tool used for sentiment
annotation can even be predicted from its outcome, revealing an algorithmic
bias of sentiment analysis. Based on Twitter, Wikipedia and different news
corpora from the English, German and French languages, our classifiers separate
sentiment tools with an averaged F1-score of 0.89 (for the English corpora). We
therefore warn against taking sentiment annotations as face value and argue for
the need of more and systematic NLP evaluation studies.

摘要：如果情緒分析工具是有效的分類器，人們會期待它們對不同類型的語料庫和不同語言的情緒分類提供可比較的結果。根據先前的研究結果，我們表明情緒分析工具不同意相同的數據集。超越先前的研究，我們表明用於情緒標註的情緒工具甚至可以從其結果中預測，揭示了情緒分析的演算法偏誤。基於 Twitter、維基百科和來自英語、德語和法語的不同新聞語料庫，我們的分類器以平均 F1 分數 0.89（對於英語語料庫）區分情緒工具。因此，我們警告不要將情緒標註視為表面價值，並主張需要更多且系統性的 NLP 評估研究。

##### **Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor Environments**
2410.14616v1 by Mariusz Wisniewski, Paraskevas Chatzithanos, Weisi Guo, Antonios Tsourdos

Deep Reinforcement learning (DRL) is used to enable autonomous navigation in
unknown environments. Most research assume perfect sensor data, but real-world
environments may contain natural and artificial sensor noise and denial. Here,
we present a benchmark of both well-used and emerging DRL algorithms in a
navigation task with configurable sensor denial effects. In particular, we are
interested in comparing how different DRL methods (e.g. model-free PPO vs.
model-based DreamerV3) are affected by sensor denial. We show that DreamerV3
outperforms other methods in the visual end-to-end navigation task with a
dynamic goal - and other methods are not able to learn this. Furthermore,
DreamerV3 generally outperforms other methods in sensor-denied environments. In
order to improve robustness, we use adversarial training and demonstrate an
improved performance in denied environments, although this generally comes with
a performance cost on the vanilla environments. We anticipate this benchmark of
different DRL methods and the usage of adversarial training to be a starting
point for the development of more elaborate navigation strategies that are
capable of dealing with uncertain and denied sensor readings.

摘要：深度強化學習 (DRL) 用於在未知環境中實現自主導航。大多數研究假設感測器資料完美無缺，但現實世界的環境可能包含自然和人工感測器雜訊和拒絕。在此，我們提出了一個基準，在具有可設定感測器拒絕效果的導航任務中，同時使用廣泛使用和新興的 DRL 演算法。特別是，我們有興趣比較不同的 DRL 方法（例如無模型的 PPO 與基於模型的 DreamerV3）如何受到感測器拒絕的影響。我們展示了 DreamerV3 在具有動態目標的視覺端到端導航任務中優於其他方法，而其他方法無法學習這一點。此外，DreamerV3 通常在感測器拒絕的環境中優於其他方法。為了提高穩健性，我們使用對抗訓練並展示在拒絕的環境中改進的效能，儘管這通常會對香草環境造成效能成本。我們預期這個不同 DRL 方法的基準和對抗訓練的使用將成為開發更精細導航策略的起點，這些策略能夠處理不確定的和拒絕的感測器讀數。

##### **Asymptotically Optimal Change Detection for Unnormalized Pre- and Post-Change Distributions**
2410.14615v1 by Arman Adibi, Sanjeev Kulkarni, H. Vincent Poor, Taposh Banerjee, Vahid Tarokh

This paper addresses the problem of detecting changes when only unnormalized
pre- and post-change distributions are accessible. This situation happens in
many scenarios in physics such as in ferromagnetism, crystallography,
magneto-hydrodynamics, and thermodynamics, where the energy models are
difficult to normalize.
  Our approach is based on the estimation of the Cumulative Sum (CUSUM)
statistics, which is known to produce optimal performance. We first present an
intuitively appealing approximation method. Unfortunately, this produces a
biased estimator of the CUSUM statistics and may cause performance degradation.
We then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM)
algorithm based on thermodynamic integration (TI) in order to estimate the
log-ratio of normalizing constants of pre- and post-change distributions. It is
proved that this approach gives an unbiased estimate of the log-partition
function and the CUSUM statistics, and leads to an asymptotically optimal
performance. Moreover, we derive a relationship between the required sample
size for thermodynamic integration and the desired detection delay performance,
offering guidelines for practical parameter selection. Numerical studies are
provided demonstrating the efficacy of our approach.

摘要：本文探讨了在只能获得未归一化的变更前和变更后分布时检测变更的问题。这种情况在许多物理场景中发生，例如铁磁性、晶体学、磁流体动力学和热力学，其中能量模型难以归一化。
我们的方法基于累积和（CUSUM）统计的估计，已知这种方法可以产生最佳性能。我们首先提出一种直观吸引人的近似方法。不幸的是，这会产生 CUSUM 统计的有偏估计，并可能导致性能下降。然后，我们提出了基于热力学积分（TI）的对数分配近似累积和（LPA-CUSUM）算法，以估计变更前和变更后分布的归一化常数的对数比。证明了该方法对对数分配函数和 CUSUM 统计提供了无偏估计，并导致渐近最优性能。此外，我们推导出热力学积分所需的样本量与所需的检测延迟性能之间的关系，为实际参数选择提供了指导。提供的数值研究证明了我们方法的有效性。

##### **DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual Distillation in Conversational Search**
2410.14609v1 by Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas

Conversational Search (CS) is the task of retrieving relevant documents from
a corpus within a conversational context, combining retrieval with
conversational context modeling. With the explosion of Large Language Models
(LLMs), the CS field has seen major improvements with LLMs rewriting user
queries, accounting for conversational context. However, engaging LLMs at
inference time harms efficiency. Current methods address this by distilling
embeddings from human-rewritten queries to learn the context modeling task.
Yet, these approaches predominantly focus on context modeling, and only treat
the contrastive component of the retrieval task within a
distillation-independent loss term. To address these limitations, we propose a
new distillation method, as a relaxation of the previous objective, unifying
retrieval and context modeling. We relax the existing training objectives by
distilling similarity scores between conversations and documents, rather than
relying solely on representation learning. Our proposed distillation objective
allows for more freedom in the representation space and leverages the
contrastive nature of document relevance. Through experiments on Learned Sparse
Retrieval (LSR) across 5 CS datasets, our approach demonstrates substantial
improvements in both in-domain and out-of-domain retrieval performance,
outperforming state-of-the-art with gains of up to 6 points in recall for
out-of-domain datasets. Additionally, through the relaxation of the objective,
we propose a multi-teacher distillation, using multiple LLMs as teachers,
yielding additional gains, and outperforming the teachers themselves in
in-domain experiments. Finally, analysis of the sparsity of the models reveals
that our distillation allows for better control over the sparsity of the
trained models.

摘要：對話式搜尋 (CS) 是一項從語料庫中擷取相關文件的工作，它在對話式脈絡中結合擷取和對話式脈絡建模。隨著大型語言模型 (LLM) 的爆炸性發展，CS 領域在 LLM 改寫使用者查詢、考量對話式脈絡方面取得重大進展。然而，在推論時間使用 LLM 會損害效率。目前的解決方法是從人類改寫的查詢中萃取嵌入，以學習脈絡建模任務。然而，這些方法主要集中在脈絡建模上，而且僅在獨立於萃取的損失函數中處理擷取任務的對比元件。為了解決這些限制，我們提出了一種新的萃取方法，作為先前目標的放寬，統一了擷取和脈絡建模。我們透過萃取對話和文件之間的相似度分數，而不是僅依賴於表徵學習，來放寬現有的訓練目標。我們提出的萃取目標允許在表徵空間中有更大的自由度，並利用文件相關性的對比性質。透過在 5 個 CS 資料集上對學習的稀疏擷取 (LSR) 進行實驗，我們的做法在領域內和領域外擷取效能上都展現出顯著的進步，在領域外資料集的召回率方面優於現有技術，增益高達 6 點。此外，透過放寬目標，我們提出了一種多教師萃取，使用多個 LLM 作為教師，產生額外的增益，並在領域內實驗中優於教師本身。最後，對模型稀疏性的分析顯示，我們的萃取允許更好地控制訓練模型的稀疏性。

##### **Streaming Deep Reinforcement Learning Finally Works**
2410.14606v1 by Mohamed Elsayed, Gautham Vasan, A. Rupam Mahmood

Natural intelligence processes experience as a continuous stream, sensing,
acting, and learning moment-by-moment in real time. Streaming learning, the
modus operandi of classic reinforcement learning (RL) algorithms like
Q-learning and TD, mimics natural learning by using the most recent sample
without storing it. This approach is also ideal for resource-constrained,
communication-limited, and privacy-sensitive applications. However, in deep RL,
learners almost always use batch updates and replay buffers, making them
computationally expensive and incompatible with streaming learning. Although
the prevalence of batch deep RL is often attributed to its sample efficiency, a
more critical reason for the absence of streaming deep RL is its frequent
instability and failure to learn, which we refer to as stream barrier. This
paper introduces the stream-x algorithms, the first class of deep RL algorithms
to overcome stream barrier for both prediction and control and match sample
efficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite,
and Atari Games, we demonstrate stream barrier in existing algorithms and
successful stable learning with our stream-x algorithms: stream Q, stream AC,
and stream TD, achieving the best model-free performance in DM Control Dog
environments. A set of common techniques underlies the stream-x algorithms,
enabling their success with a single set of hyperparameters and allowing for
easy extension to other algorithms, thereby reviving streaming RL.

摘要：自然智能處理經驗作為一個連續的串流，在實時中感測、行動和學習。串流學習，經典強化學習 (RL) 演算法的運作模式，例如 Q 學習和 TD，透過使用最新的範例（而不儲存它）來模擬自然學習。這種方法也適用於資源受限、通訊受限和隱私敏感的應用程式。然而，在深度 RL 中，學習者幾乎總是使用批次更新和重播緩衝區，這使得它們在計算上很昂貴，並且與串流學習不相容。儘管批次深度 RL 的盛行通常歸因於其範例效率，但深度串流 RL 缺席的一個更重要的原因是其頻繁的不穩定性和學習失敗，我們稱之為串流屏障。本文介紹了 stream-x 演算法，這是第一類克服串流屏障的深度 RL 演算法，用於預測和控制，並且符合批次 RL 的範例效率。透過在 Mujoco Gym、DM 控制套件和 Atari 遊戲中的實驗，我們展示了現有演算法中的串流屏障，以及我們 stream-x 演算法成功的穩定學習：串流 Q、串流 AC 和串流 TD，在 DM 控制狗環境中實現最佳的無模型效能。一組常見的技術是 stream-x 演算法的基礎，讓它們能夠使用單一組超參數成功，並允許輕鬆延伸到其他演算法，從而恢復串流 RL。

##### **How Does Data Diversity Shape the Weight Landscape of Neural Networks?**
2410.14602v1 by Yang Ba, Michelle V. Mancenido, Rong Pan

To enhance the generalization of machine learning models to unseen data,
techniques such as dropout, weight decay ($L_2$ regularization), and noise
augmentation are commonly employed. While regularization methods (i.e., dropout
and weight decay) are geared toward adjusting model parameters to prevent
overfitting, data augmentation increases the diversity of the input training
set, a method purported to improve accuracy and calibration error. In this
paper, we investigate the impact of each of these techniques on the parameter
space of neural networks, with the goal of understanding how they alter the
weight landscape in transfer learning scenarios. To accomplish this, we employ
Random Matrix Theory to analyze the eigenvalue distributions of pre-trained
models, fine-tuned using these techniques but using different levels of data
diversity, for the same downstream tasks. We observe that diverse data
influences the weight landscape in a similar fashion as dropout. Additionally,
we compare commonly used data augmentation methods with synthetic data created
by generative models. We conclude that synthetic data can bring more diversity
into real input data, resulting in a better performance on out-of-distribution
test instances.

摘要：為了增強機器學習模型對未見資料的概化能力，常使用諸如中輟、權重衰減（$L_2$ 正則化）和雜訊擴充等技術。雖然正則化方法（例如中輟和權重衰減）旨在調整模型參數以防止過度擬合，但資料擴充增加了輸入訓練組的多樣性，這是一種據稱可以改善準確性和校準誤差的方法。在本文中，我們探討了這些技術對神經網路參數空間的影響，目的是了解它們如何在轉移學習場景中改變權重景觀。為了達成此目標，我們採用隨機矩陣理論來分析預訓練模型的特徵值分佈，使用這些技術進行微調，但使用不同程度的資料多樣性，以執行相同的下游任務。我們觀察到，多樣化的資料以類似於中輟的方式影響權重景觀。此外，我們將常用的資料擴充方法與生成模型所建立的合成資料進行比較。我們得出結論，合成資料可以為真實輸入資料帶來更多樣性，從而提高對分布外測試例項的效能。

##### **Teaching Models to Balance Resisting and Accepting Persuasion**
2410.14596v1 by Elias Stengel-Eskin, Peter Hase, Mohit Bansal

Large language models (LLMs) are susceptible to persuasion, which can pose
risks when models are faced with an adversarial interlocutor. We take a first
step towards defending models against persuasion while also arguing that
defense against adversarial (i.e. negative) persuasion is only half of the
equation: models should also be able to accept beneficial (i.e. positive)
persuasion to improve their answers. We show that optimizing models for only
one side results in poor performance on the other. In order to balance positive
and negative persuasion, we introduce Persuasion-Balanced Training (or PBT),
which leverages multi-agent recursive dialogue trees to create data and trains
models via preference optimization to accept persuasion when appropriate. PBT
consistently improves resistance to misinformation and resilience to being
challenged while also resulting in the best overall performance on holistic
data containing both positive and negative persuasion. Crucially, we show that
PBT models are better teammates in multi-agent debates. We find that without
PBT, pairs of stronger and weaker models have unstable performance, with the
order in which the models present their answers determining whether the team
obtains the stronger or weaker model's performance. PBT leads to better and
more stable results and less order dependence, with the stronger model
consistently pulling the weaker one up.

摘要：大型語言模型（LLM）容易受到說服，當模型面對對抗性對話者時，這可能會造成風險。我們採取第一步來保護模型免於說服，同時也主張防禦對抗性（即負面）說服僅是等式的一半：模型也應該能夠接受有益的（即正面的）說服來改善其答案。我們表明，僅針對一方優化模型會導致另一方的效能不佳。為了平衡正面和負面說服，我們引入了說服平衡訓練（或 PBT），它利用多主體遞迴對話樹來建立資料，並透過偏好最佳化來訓練模型，以便在適當時接受說服。PBT 持續提高對錯誤資訊的抵抗力，並提高應對挑戰的韌性，同時在包含正面和負面說服的整體資料中也產生最佳的整體效能。至關重要的是，我們表明 PBT 模型在多主體辯論中是更好的隊友。我們發現，沒有 PBT，成對的較強和較弱模型效能不穩定，模型呈現其答案的順序決定了團隊獲得較強或較弱模型效能。PBT 導向更好、更穩定的結果，以及較少的順序依賴性，較強的模型持續提升較弱的模型。

##### **Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases**
2410.14594v1 by Elias Lumer

Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks
like secure database interactions and multi-agent code development. However,
scaling tool capacity beyond agent reasoning or model limits remains a
challenge. In this paper, we address these challenges by introducing Toolshed
Knowledge Bases, a tool knowledge base (vector database) designed to store
enhanced tool representations and optimize tool selection for large-scale
tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a
novel ensemble of tool-applied advanced retrieval-augmented generation (RAG)
techniques across the pre-retrieval, intra-retrieval, and post-retrieval
phases, without requiring model fine-tuning. During pre-retrieval, tool
documents are enhanced with key information and stored in the Toolshed
Knowledge Base. Intra-retrieval focuses on query planning and transformation to
increase retrieval accuracy. Post-retrieval refines the retrieved tool
documents and enables self-reflection. Furthermore, by varying both the total
number of tools (tool-M) an Agent has access to and the tool selection
threshold (top-k), we address trade-offs between retrieval accuracy, agent
performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute
improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools
benchmark datasets, respectively (Recall@5).

摘要：最近在工具型代理 (LLM) 中的工具配备上的进步已经能执行复杂的任务，例如安全的数据库交互和多代理代码开发。然而，将工具容量扩展到代理推理或模型极限之外仍然是一个挑战。在本文中，我们通过引入工具库知识库来解决这些挑战，该工具知识库（向量数据库）旨在存储增强的工具表示并优化大规模工具型代理的工具选择。此外，我们提出了高级 RAG 工具融合，一种跨预检索、检索内和检索后阶段的新型工具应用高级检索增强生成 (RAG) 技术的集成，而无需模型微调。在预检索期间，工具文档会得到关键信息的增强并存储在工具库知识库中。检索内侧重于查询规划和转换，以提高检索准确性。检索后会优化检索到的工具文档并支持自省。此外，通过改变代理可访问的工具总数（工具 M）和工具选择阈值（top-k），我们解决了检索准确性、代理性能和标记成本之间的权衡。我们的方法在 ToolE 单工具、ToolE 多工具和 Seal-Tools 基准数据集上分别实现了 46%、56% 和 47% 的绝对改进（召回率@5）。

##### **Temporal Fair Division of Indivisible Items**
2410.14593v1 by Edith Elkind, Alexander Lam, Mohamad Latifian, Tzeh Yuan Neoh, Nicholas Teh

We study a fair division model where indivisible items arrive sequentially,
and must be allocated immediately and irrevocably. Previous work on online fair
division has shown impossibility results in achieving approximate envy-freeness
under these constraints. In contrast, we consider an informed setting where the
algorithm has complete knowledge of future items, and aim to ensure that the
cumulative allocation at each round satisfies approximate envy-freeness --
which we define as temporal envy-freeness up to one item (TEF1). We focus on
settings where items can be exclusively goods or exclusively chores. For goods,
while TEF1 allocations may not always exist, we identify several special cases
where they do -- two agents, two item types, generalized binary valuations,
unimodal preferences -- and provide polynomial-time algorithms for these cases.
We also prove that determining the existence of a TEF1 allocation is NP-hard.
For chores, we establish analogous results for the special cases, but present a
slightly weaker intractability result. We also establish the incompatibility
between TEF1 and Pareto-optimality, with the implication that it is intractable
to find a TEF1 allocation that maximizes any $p$-mean welfare, even for two
agents.

摘要：<paragraph>我們研究一個公平分配模型，其中不可分割的物品會依序抵達，並且必須立即且不可撤銷地分配。先前關於線上公平分配的研究顯示，在這些限制下，不可能實現近似無妒。相反地，我們考慮一個知情設定，其中演算法對未來的物品有完整的認識，並旨在確保每一輪的累積分配滿足近似無妒——我們將其定義為最多一項的暫時無妒 (TEF1)。我們專注於物品可以是純粹的物品或純粹的家務的設定。對於物品，儘管 TEF1 分配可能並不總是存在，但我們找出它們存在的幾個特殊情況——兩個代理人、兩種物品類型、廣義二元估值、單峰偏好——並為這些情況提供多項式時間演算法。我們也證明確定 TEF1 分配的存在是 NP-hard。對於家務，我們為特殊情況建立類似的結果，但提出一個稍微弱一點的難以處理的結果。我們也建立 TEF1 和帕累托最優之間的不相容性，暗示找出一個 TEF1 分配以最大化任何 p 均值福利，即使對於兩個代理人，也是難以處理的。</paragraph>

##### **Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a Continuum**
2410.14589v1 by Ryan Soh-Eun Shim, Barbara Plank

There is increasing interest in looking at dialects in NLP. However, most
work to date still treats dialects as discrete categories. For instance,
evaluative work in variation-oriented NLP for English often works with Indian
English or African-American Venacular English as homogeneous categories (Faisal
et al., 2024; Ziems et al., 2023), yet even within one variety there is
substantial variation. We examine within-dialect variation and show that
performance critically varies within categories. We measure speech-to-text
performance on Italian dialects, and empirically observe a geographical
performance disparity. This disparity correlates substantially (-0.5) with
linguistic similarity to the highest performing dialect variety. We
cross-examine our results against dialectometry methods, and interpret the
performance disparity to be due to a bias towards dialects that are more
similar to the standard variety in the speech-to-text model examined. We
additionally leverage geostatistical methods to predict zero-shot performance
at unseen sites, and find the incorporation of geographical information to
substantially improve prediction performance, indicating there to be
geographical structure in the performance distribution.

摘要：近來在 NLP 中觀察方言的興趣日益增加。然而，迄今為止，大多數工作仍將方言視為離散類別。例如，針對英語的變異導向 NLP 中的評量工作，通常將印度英語或非裔美國人白話英語視為同質類別（Faisal 等人，2024 年；Ziems 等人，2023 年），但即使在一個變體內部也存在顯著的變異。我們探討方言內部變異，並顯示在類別內部，效能有顯著的差異。我們測量義大利方言的語音轉文字效能，並透過實證觀察到地理效能差異。這種差異與效能最高的方言變體的語言相似性顯著相關（-0.5）。我們根據方言測量法檢視我們的結果，並將效能差異解釋為偏向在所檢視的語音轉文字模型中更類似標準變體的方言。此外，我們利用地統計方法預測未見地點的零次學習效能，並發現地理資訊的納入顯著改善預測效能，這表示效能分佈中存在地理結構。

##### **Neural Combinatorial Clustered Bandits for Recommendation Systems**
2410.14586v1 by Baran Atalar, Carlee Joe-Wong

We consider the contextual combinatorial bandit setting where in each round,
the learning agent, e.g., a recommender system, selects a subset of "arms,"
e.g., products, and observes rewards for both the individual base arms, which
are a function of known features (called "context"), and the super arm (the
subset of arms), which is a function of the base arm rewards. The agent's goal
is to simultaneously learn the unknown reward functions and choose the
highest-reward arms. For example, the "reward" may represent a user's
probability of clicking on one of the recommended products. Conventional bandit
models, however, employ restrictive reward function models in order to obtain
performance guarantees. We make use of deep neural networks to estimate and
learn the unknown reward functions and propose Neural UCB Clustering
(NeUClust), which adopts a clustering approach to select the super arm in every
round by exploiting underlying structure in the context space. Unlike prior
neural bandit works, NeUClust uses a neural network to estimate the super arm
reward and select the super arm, thus eliminating the need for a known
optimization oracle. We non-trivially extend prior neural combinatorial bandit
works to prove that NeUClust achieves
$\widetilde{O}\left(\widetilde{d}\sqrt{T}\right)$ regret, where $\widetilde{d}$
is the effective dimension of a neural tangent kernel matrix, $T$ the number of
rounds. Experiments on real world recommendation datasets show that NeUClust
achieves better regret and reward than other contextual combinatorial and
neural bandit algorithms.

摘要：<paragraph>我們考慮情境組合式賭徒問題，在每一輪中，學習代理（例如推薦系統）會選擇一個「手臂」的子集（例如產品），並觀察個別基本手臂的獎勵，這些獎勵是已知特徵（稱為「情境」）的函數，以及超級手臂（手臂的子集），這是基本手臂獎勵的函數。代理的目標是同時學習未知的獎勵函數，並選擇最高獎勵的手臂。例如，「獎勵」可能代表使用者點擊推薦產品之一的機率。然而，傳統的賭徒模型採用限制性獎勵函數模型，以獲得效能保證。我們利用深度神經網路來估計和學習未知的獎勵函數，並提出神經 UCB 聚類（NeUClust），它採用聚類方法來選擇每一輪的超級手臂，藉由利用情境空間中的底層結構。與先前的神經賭徒工作不同，NeUClust 使用神經網路來估計超級手臂獎勵並選擇超級手臂，因此消除了已知最佳化預言的需求。我們非平凡地擴展先前的神經組合式賭徒工作，以證明 NeUClust 達到
$\widetilde{O}\left(\widetilde{d}\sqrt{T}\right)$ 後悔，其中 $\widetilde{d}$
是神經切線核矩陣的有效維度，$T$ 是輪數。在真實世界推薦資料集上的實驗表明，NeUClust 比其他情境組合式和神經賭徒演算法獲得更好的後悔和獎勵。</paragraph>

##### **Do LLMs estimate uncertainty well in instruction-following?**
2410.14582v1 by Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain

Large language models (LLMs) could be valuable personal AI agents across
various domains, provided they can precisely follow user instructions. However,
recent studies have shown significant limitations in LLMs'
instruction-following capabilities, raising concerns about their reliability in
high-stakes applications. Accurately estimating LLMs' uncertainty in adhering
to instructions is critical to mitigating deployment risks. We present, to our
knowledge, the first systematic evaluation of the uncertainty estimation
abilities of LLMs in the context of instruction-following. Our study identifies
key challenges with existing instruction-following benchmarks, where multiple
factors are entangled with uncertainty stems from instruction-following,
complicating the isolation and comparison across methods and models. To address
these issues, we introduce a controlled evaluation setup with two benchmark
versions of data, enabling a comprehensive comparison of uncertainty estimation
methods under various conditions. Our findings show that existing uncertainty
methods struggle, particularly when models make subtle errors in instruction
following. While internal model states provide some improvement, they remain
inadequate in more complex scenarios. The insights from our controlled
evaluation setups provide a crucial understanding of LLMs' limitations and
potential for uncertainty estimation in instruction-following tasks, paving the
way for more trustworthy AI agents.

摘要：大型語言模型 (LLM) 可成為各種領域中寶貴的個人 AI 代理，只要它們能精確遵循使用者的指示。然而，最近的研究顯示 LLM 在遵循指示的能力上存在顯著的限制，引發了人們對其在高風險應用中可靠性的擔憂。精確估計 LLM 在遵循指示時的的不確定性對於降低部署風險至關重要。據我們所知，我們提出了 LLM 在遵循指示的背景下不確定性估計能力的第一個系統評估。我們的研究確定了現有遵循指示基準的關鍵挑戰，其中多個因素與不確定性糾纏在一起，導致遵循指示的複雜性，使得跨方法和模型的隔離和比較變得複雜。為了解決這些問題，我們引入了具有兩個基準版本資料的受控評估設定，從而可以在各種條件下對不確定性估計方法進行全面比較。我們的研究結果表明，現有的不確定性方法存在困難，特別是在模型在遵循指示時出現細微錯誤的情況下。雖然內部模型狀態提供了一些改進，但它們在更複雜的場景中仍然不足。我們受控評估設定中的見解提供了對 LLM 在遵循指示任務中不確定性估計的限制和潛力的關鍵理解，為更值得信賴的 AI 代理鋪平了道路。

##### **Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection**
2410.14581v1 by Aaron Alvarado Kristanto Julistiono, Davoud Ataee Tarzanagh, Navid Azizan

Attention mechanisms have revolutionized several domains of artificial
intelligence, such as natural language processing and computer vision, by
enabling models to selectively focus on relevant parts of the input data. While
recent work has characterized the optimization dynamics of gradient descent
(GD) in attention-based models and the structural properties of its preferred
solutions, less is known about more general optimization algorithms such as
mirror descent (MD). In this paper, we investigate the convergence properties
and implicit biases of a family of MD algorithms tailored for softmax attention
mechanisms, with the potential function chosen as the $p$-th power of the
$\ell_p$-norm. Specifically, we show that these algorithms converge in
direction to a generalized hard-margin SVM with an $\ell_p$-norm objective when
applied to a classification problem using a softmax attention model. Notably,
our theoretical results reveal that the convergence rate is comparable to that
of traditional GD in simpler models, despite the highly nonlinear and nonconvex
nature of the present problem. Additionally, we delve into the joint
optimization dynamics of the key-query matrix and the decoder, establishing
conditions under which this complex joint optimization converges to their
respective hard-margin SVM solutions. Lastly, our numerical experiments on real
data demonstrate that MD algorithms improve generalization over standard GD and
excel in optimal token selection.

摘要：注意力机制通过让模型有选择地关注输入数据中的相关部分，彻底改变了人工智能的几个领域，例如自然语言处理和计算机视觉。虽然最近的研究描述了基于注意力的模型中梯度下降 (GD) 的优化动态及其首选解决方案的结构属性，但对于镜像下降 (MD) 等更通用的优化算法了解较少。在本文中，我们研究了专为 softmax 注意力机制设计的 MD 算法系列的收敛属性和隐式偏差，其中潜在函数选择为 $\ell_p$-范数的 $p$-次幂。具体来说，我们表明当这些算法应用于使用 softmax 注意力模型的分类问题时，它们在方向上收敛到具有 $\ell_p$-范数目标的广义硬边距 SVM。值得注意的是，我们的理论结果表明，尽管当前问题的非线性和非凸性，收敛速度与更简单模型中的传统 GD 相当。此外，我们深入研究了键查询矩阵和解码器的联合优化动态，建立了复杂联合优化收敛到其各自的硬边距 SVM 解的条件。最后，我们在真实数据上的数值实验表明，MD 算法改进了对标准 GD 的泛化，并在最佳标记选择中表现出色。

##### **Towards Unsupervised Validation of Anomaly-Detection Models**
2410.14579v1 by Lihi Idan

Unsupervised validation of anomaly-detection models is a highly challenging
task. While the common practices for model validation involve a labeled
validation set, such validation sets cannot be constructed when the underlying
datasets are unlabeled. The lack of robust and efficient unsupervised
model-validation techniques presents an acute challenge in the implementation
of automated anomaly-detection pipelines, especially when there exists no prior
knowledge of the model's performance on similar datasets. This work presents a
new paradigm to automated validation of anomaly-detection models, inspired by
real-world, collaborative decision-making mechanisms. We focus on two
commonly-used, unsupervised model-validation tasks -- model selection and model
evaluation -- and provide extensive experimental results that demonstrate the
accuracy and robustness of our approach on both tasks.

摘要：異常偵測模型的無監督驗證是一項極具挑戰性的任務。雖然模型驗證的常見做法涉及標記驗證集，但在基礎資料集未標記時無法建構此類驗證集。缺乏穩健且有效的無監督模型驗證技術在自動化異常偵測管線的實作中造成嚴峻挑戰，特別是在對模型在類似資料集上的效能沒有先驗知識時。本研究提出了一種新的範例，用於異常偵測模型的自動化驗證，其靈感來自於現實世界的協作決策機制。我們專注於兩個常見的無監督模型驗證任務：模型選擇和模型評估，並提供廣泛的實驗結果，證明了我們的方法在兩個任務上的準確性和穩健性。

##### **Large Language Models Are Overparameterized Text Encoders**
2410.14578v1 by Thennal D K, Tim Fischer, Chris Biemann

Large language models (LLMs) demonstrate strong performance as text embedding
models when finetuned with supervised contrastive training. However, their
large size balloons inference time and memory requirements. In this paper, we
show that by pruning the last $p\%$ layers of an LLM before supervised training
for only 1000 steps, we can achieve a proportional reduction in memory and
inference time. We evaluate four different state-of-the-art LLMs on text
embedding tasks and find that our method can prune up to 30\% of layers with
negligible impact on performance and up to 80\% with only a modest drop. With
only three lines of code, our method is easily implemented in any pipeline for
transforming LLMs to text encoders. We also propose $\text{L}^3 \text{Prune}$,
a novel layer-pruning strategy based on the model's initial loss that provides
two optimal pruning configurations: a large variant with negligible performance
loss and a small variant for resource-constrained settings. On average, the
large variant prunes 21\% of the parameters with a $-0.3$ performance drop, and
the small variant only suffers from a $-5.1$ decrease while pruning 74\% of the
model. We consider these results strong evidence that LLMs are
overparameterized for text embedding tasks, and can be easily pruned.

摘要：大型語言模型 (LLM) 在經過監督對比訓練微調後，展現出強大的文本嵌入模型效能。然而，它們的龐大規模會膨脹推論時間和記憶體需求。在本文中，我們展示出透過在監督訓練前修剪 LLM 的最後 $p\%$ 層，僅進行 1000 個步驟，我們可以按比例減少記憶體和推論時間。我們在文本嵌入任務中評估了四種不同的最先進 LLM，並發現我們的模型可以修剪多達 30% 的層，對效能影響微乎其微，而修剪多達 80% 時效能僅略微下降。我們的模型僅需三行程式碼，便能輕鬆實作在任何將 LLM 轉換為文字編碼器的管道中。我們也提出 $\text{L}^3 \text{Prune}$，這是一種基於模型初始損失的新穎層修剪策略，可提供兩種最佳修剪配置：效能損失微乎其微的大型變體，以及適用於資源受限設定的小型變體。平均而言，大型變體修剪了 21% 的參數，效能下降 -0.3，而小型變體僅下降 -5.1，同時修剪了模型的 74%。我們認為這些結果有力地證明了 LLM 對於文本嵌入任務來說過度參數化，並且可以輕鬆地進行修剪。

##### **MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts**
2410.14574v1 by Rachel S. Y. Teo, Tan M. Nguyen

Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled
scalability in deep learning. SMoE has the potential to exponentially increase
parameter count while maintaining the efficiency of the model by only
activating a small subset of these parameters for a given sample. However, it
has been observed that SMoE suffers from unstable training and has difficulty
adapting to new distributions, leading to the model's lack of robustness to
data contamination. To overcome these limitations, we first establish a
connection between the dynamics of the expert representations in SMoEs and
gradient descent on a multi-objective optimization problem. Leveraging our
framework, we then integrate momentum into SMoE and propose a new family of
SMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate
that MomentumSMoE is more stable and robust than SMoE. In particular, we verify
the advantages of MomentumSMoE over SMoE on a variety of practical tasks
including ImageNet-1K object recognition and WikiText-103 language modeling. We
demonstrate the applicability of MomentumSMoE to many types of SMoE models,
including those in the Sparse MoE model for vision (V-MoE) and the Generalist
Language Model (GLaM). We also show that other advanced momentum-based
optimization methods, such as Adam, can be easily incorporated into the
MomentumSMoE framework for designing new SMoE models with even better
performance, almost negligible additional computation cost, and simple
implementations.

摘要：稀疏专家混合（SMoE）已成为解锁深度学习中无与伦比的可扩展性的关键。SMoE 有可能呈指数级增加参数计数，同时仅通过激活给定样本中这些参数的一个小子集来维持模型的效率。然而，据观察，SMoE 训练不稳定，并且难以适应新的分布，导致模型对数据污染缺乏鲁棒性。为了克服这些限制，我们首先在 SMoE 中专家表示的动态与多目标优化问题的梯度下降之间建立了联系。利用我们的框架，我们随后将动量整合到 SMoE 中，并提出了一系列名为 MomentumSMoE 的新 SMoE。我们从理论上证明并通过数字演示了 MomentumSMoE 比 SMoE 更稳定、更稳健。特别是，我们在各种实际任务上验证了 MomentumSMoE 相对于 SMoE 的优势，包括 ImageNet-1K 对象识别和 WikiText-103 语言建模。我们展示了 MomentumSMoE 对多种类型 SMoE 模型的适用性，包括视觉稀疏 MoE 模型 (V-MoE) 和通用语言模型 (GLaM)。我们还表明，其他基于动量的优化方法，例如 Adam，可以轻松地整合到 MomentumSMoE 框架中，用于设计具有更好性能、几乎可以忽略不计的额外计算成本和简单实现的新 SMoE 模型。

##### **Building Trust in Black-box Optimization: A Comprehensive Framework for Explainability**
2410.14573v1 by Nazanin Nezami, Hadis Anahideh

Optimizing costly black-box functions within a constrained evaluation budget
presents significant challenges in many real-world applications. Surrogate
Optimization (SO) is a common resolution, yet its proprietary nature introduced
by the complexity of surrogate models and the sampling core (e.g., acquisition
functions) often leads to a lack of explainability and transparency. While
existing literature has primarily concentrated on enhancing convergence to
global optima, the practical interpretation of newly proposed strategies
remains underexplored, especially in batch evaluation settings. In this paper,
we propose \emph{Inclusive} Explainability Metrics for Surrogate Optimization
(IEMSO), a comprehensive set of model-agnostic metrics designed to enhance the
transparency, trustworthiness, and explainability of the SO approaches. Through
these metrics, we provide both intermediate and post-hoc explanations to
practitioners before and after performing expensive evaluations to gain trust.
We consider four primary categories of metrics, each targeting a specific
aspect of the SO process: Sampling Core Metrics, Batch Properties Metrics,
Optimization Process Metrics, and Feature Importance. Our experimental
evaluations demonstrate the significant potential of the proposed metrics
across different benchmarks.

摘要：在受限的評估預算中最佳化高成本的黑箱函數，在許多現實世界的應用中會帶來重大的挑戰。代理最佳化 (SO) 是一種常見的解決方案，但代理模型的複雜性和取樣核心（例如，收購函數）所帶來的專有特性，通常會導致缺乏可解釋性和透明度。雖然現有的文獻主要集中於增強對全局最優值的收斂性，但對新提出的策略的實際解釋仍未充分探討，尤其是在批次評估設定中。在本文中，我們提出代理最佳化的「包容性」可解釋性指標 (IEMSO)，這是一套全面的模型不可知指標，旨在增強 SO 方法的透明度、可信度和可解釋性。透過這些指標，我們在執行昂貴的評估之前和之後，為實務人員提供中間和事後解釋，以建立信任。我們考慮四種類別的主要指標，每個指標都針對 SO 流程的特定方面：取樣核心指標、批次屬性指標、最佳化流程指標和特徵重要性。我們的實驗評估證明了所提出的指標在不同基準測試中的顯著潛力。

##### **TransBox: EL++-closed Ontology Embedding**
2410.14571v1 by Hui Yang, Jiaoyan Chen, Uli Sattler

OWL (Web Ontology Language) ontologies, which are able to represent both
relational and type facts as standard knowledge graphs and complex domain
knowledge in Description Logic (DL) axioms, are widely adopted in domains such
as healthcare and bioinformatics. Inspired by the success of knowledge graph
embeddings, embedding OWL ontologies has gained significant attention in recent
years. Current methods primarily focus on learning embeddings for atomic
concepts and roles, enabling the evaluation based on normalized axioms through
specially designed score functions. However, they often neglect the embedding
of complex concepts, making it difficult to infer with more intricate axioms.
This limitation reduces their effectiveness in advanced reasoning tasks, such
as Ontology Learning and ontology-mediated Query Answering. In this paper, we
propose EL++-closed ontology embeddings which are able to represent any logical
expressions in DL via composition. Furthermore, we develop TransBox, an
effective EL++-closed ontology embedding method that can handle many-to-one,
one-to-many and many-to-many relations. Our extensive experiments demonstrate
that TransBox often achieves state-of-the-art performance across various
real-world datasets for predicting complex axioms.

摘要：OWL（Web Ontology Language）本体，能够将关系和类型事实表示为标准知识图和描述逻辑 (DL) 公理中的复杂领域知识，在医疗保健和生物信息学等领域得到广泛采用。受知识图嵌入的成功启发，嵌入 OWL 本体近年来备受关注。当前方法主要集中在学习原子概念和角色的嵌入，通过专门设计的评分函数，支持基于归一化公理的评估。然而，它们经常忽略复杂概念的嵌入，这使得难以推断出更复杂的公理。这种限制降低了它们在高级推理任务（例如本体学习和本体介导查询应答）中的有效性。在本文中，我们提出了 EL++ 封闭本体嵌入，它能够通过组合来表示 DL 中的任何逻辑表达式。此外，我们开发了 TransBox，一种有效的 EL++ 封闭本体嵌入方法，可以处理多对一、一对多和多对多关系。我们广泛的实验表明，TransBox 在预测复杂公理的各种真实世界数据集上通常都能达到最先进的性能。

##### **When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs**
2410.14569v1 by Hanna Kim, Minkyoo Song, Seung Ho Na, Seungwon Shin, Kimin Lee

Recent advancements in Large Language Models (LLMs) have established them as
agentic systems capable of planning and interacting with various tools. These
LLM agents are often paired with web-based tools, enabling access to diverse
sources and real-time information. Although these advancements offer
significant benefits across various applications, they also increase the risk
of malicious use, particularly in cyberattacks involving personal information.
In this work, we investigate the risks associated with misuse of LLM agents in
cyberattacks involving personal data. Specifically, we aim to understand: 1)
how potent LLM agents can be when directed to conduct cyberattacks, 2) how
cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it
becomes to launch cyberattacks using LLM agents. We examine three attack
scenarios: the collection of Personally Identifiable Information (PII), the
generation of impersonation posts, and the creation of spear-phishing emails.
Our experiments reveal the effectiveness of LLM agents in these attacks: LLM
agents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of
impersonation posts created by LLM agents were evaluated as authentic, and the
click rate for links in spear phishing emails created by LLM agents reached up
to 46.67%. Additionally, our findings underscore the limitations of existing
safeguards in contemporary commercial LLMs, emphasizing the urgent need for
more robust security measures to prevent the misuse of LLM agents.

摘要：大型語言模型 (LLM) 的最新進展已將其確立為能夠規劃和與各種工具互動的代理系統。這些 LLM 代理通常與基於網路的工具配對，可以存取各種來源和即時資訊。儘管這些進展為各種應用程式提供了顯著的優點，但它們也增加了惡意使用的風險，特別是在涉及個人資訊的網路攻擊中。在這項工作中，我們調查了在網路攻擊中誤用 LLM 代理所帶來的風險，特別是涉及個人資料。具體來說，我們旨在了解：1) 將 LLM 代理導向執行網路攻擊時，它們的潛力有多大，2) 網路攻擊如何透過基於網路的工具得到加強，以及 3) 使用 LLM 代理發動網路攻擊變得有多麼容易且經濟實惠。我們探討了三種攻擊情境：收集個人可識別資訊 (PII)、產生冒充貼文，以及建立魚叉式網路釣魚電子郵件。我們的實驗揭示了 LLM 代理在這些攻擊中的有效性：LLM 代理在收集 PII 時達到了高達 95.9% 的準確度，LLM 代理建立的冒充貼文中有高達 93.9% 被評估為真實，LLM 代理建立的魚叉式網路釣魚電子郵件中的連結點擊率達到 46.67%。此外，我們的研究結果強調了當代商業 LLM 中現有防護措施的限制，強調迫切需要更強大的安全措施來防止 LLM 代理被濫用。

##### **RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions**
2410.14567v1 by Zhiyuan Peng, Jinming Nian, Alexandre Evfimievski, Yi Fang

Conversational AI agents use Retrieval Augmented Generation (RAG) to provide
verifiable document-grounded responses to user inquiries. However, many natural
questions do not have good answers: about 25\% contain false
assumptions~\cite{Yu2023:CREPE}, and over 50\% are
ambiguous~\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve
their responses to confusing questions. This paper presents a novel synthetic
data generation method to efficiently create a diverse set of context-grounded
confusing questions from a given document corpus. We conduct an empirical
comparative evaluation of several large language models as RAG agents to
measure the accuracy of confusion detection and appropriate response
generation. We contribute a benchmark dataset to the public domain.

摘要：對話式 AI 代理使用檢索增強生成 (RAG) 來提供可驗證的基於文件回應給使用者的查詢。然而，許多自然問題沒有好的答案：約 25% 包含錯誤的假設~\cite{Yu2023:CREPE}，而超過 50% 是模稜兩可的~\cite{Min2020:AmbigQA}。RAG 代理需要高品質的資料來改善他們對令人困惑問題的回應。本文提出了一種新穎的合成資料生成方法，可以有效率地從給定的文件語料庫中建立一組多樣化的基於背景的令人困惑的問題。我們對幾個大型語言模型進行實證比較評估，作為 RAG 代理，以衡量混淆檢測的準確性和適當的回應生成。我們將基準資料集貢獻給公有領域。

##### **Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization**
2410.14545v1 by Frederic Kirstein, Terry Ruas, Robert Kratel, Bela Gipp

Meeting summarization is crucial in digital communication, but existing
solutions struggle with salience identification to generate personalized,
workable summaries, and context understanding to fully comprehend the meetings'
content. Previous attempts to address these issues by considering related
supplementary resources (e.g., presentation slides) alongside transcripts are
hindered by models' limited context sizes and handling the additional
complexities of the multi-source tasks, such as identifying relevant
information in additional files and seamlessly aligning it with the meeting
content. This work explores multi-source meeting summarization considering
supplementary materials through a three-stage large language model approach:
identifying transcript passages needing additional context, inferring relevant
details from supplementary materials and inserting them into the transcript,
and generating a summary from this enriched transcript. Our multi-source
approach enhances model understanding, increasing summary relevance by ~9% and
producing more content-rich outputs. We introduce a personalization protocol
that extracts participant characteristics and tailors summaries accordingly,
improving informativeness by ~10%. This work further provides insights on
performance-cost trade-offs across four leading model families, including
edge-device capable options. Our approach can be extended to similar complex
generative tasks benefitting from additional resources and personalization,
such as dialogue systems and action planning.

摘要：會議摘要在數位溝通中至關重要，但現有的解決方案在找出重點以產生個人化、可行的摘要，以及理解脈絡以充分理解會議內容方面面臨挑戰。先前嘗試透過考慮與逐字紀錄並列的相關補充資源（例如簡報投影片）來解決這些問題，但受到模型受限的脈絡大小以及處理多來源任務額外複雜性的阻礙，例如找出額外檔案中的相關資訊，並將其與會議內容無縫對齊。這項研究探討多來源會議摘要，透過三階段大型語言模型方法來考量補充資料：找出需要額外脈絡的逐字紀錄段落、從補充資料推論相關細節並將其插入逐字紀錄，以及從這份豐富的逐字紀錄產生摘要。我們的多來源方法增強了模型理解，將摘要相關性提升約 9%，並產生更多內容豐富的輸出。我們引進個人化協定，用來萃取參與者特徵並據此調整摘要，將資訊量提升約 10%。這項研究進一步提供關於四個主要模型系列的效能成本取捨的見解，包括邊緣裝置可用的選項。我們的做法可以延伸到從額外資源和個人化中受益的類似複雜生成任務，例如對話系統和行動計畫。

##### **Less is More: Selective Reduction of CT Data for Self-Supervised Pre-Training of Deep Learning Models with Contrastive Learning Improves Downstream Classification Performance**
2410.14524v1 by Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Michael Götz, Timo Ropinski

Self-supervised pre-training of deep learning models with contrastive
learning is a widely used technique in image analysis. Current findings
indicate a strong potential for contrastive pre-training on medical images.
However, further research is necessary to incorporate the particular
characteristics of these images. We hypothesize that the similarity of medical
images hinders the success of contrastive learning in the medical imaging
domain. To this end, we investigate different strategies based on deep
embedding, information theory, and hashing in order to identify and reduce
redundancy in medical pre-training datasets. The effect of these different
reduction strategies on contrastive learning is evaluated on two pre-training
datasets and several downstream classification tasks. In all of our
experiments, dataset reduction leads to a considerable performance gain in
downstream tasks, e.g., an AUC score improvement from 0.78 to 0.83 for the
COVID CT Classification Grand Challenge, 0.97 to 0.98 for the OrganSMNIST
Classification Challenge and 0.73 to 0.83 for a brain hemorrhage classification
task. Furthermore, pre-training is up to nine times faster due to the dataset
reduction. In conclusion, the proposed approach highlights the importance of
dataset quality and provides a transferable approach to improve contrastive
pre-training for classification downstream tasks on medical images.

摘要：深度學習模型的對比學習自監督預訓練是一種廣泛用於影像分析的技術。目前的發現顯示對比預訓練在醫學影像上具有強大的潛力。然而，進一步的研究對於納入這些影像的特定特徵是必要的。我們假設醫學影像的相似性阻礙了對比學習在醫學影像領域的成功。為此，我們研究了基於深度嵌入、資訊理論和雜湊的不同策略，以識別和減少醫學預訓練資料集中的冗餘。這些不同的簡化策略對比學習的影響在兩個預訓練資料集和幾個下游分類任務中進行評估。在我們所有的實驗中，資料集簡化都導致了下游任務的顯著效能提升，例如，COVID CT 分類大挑戰的 AUC 分數從 0.78 提升至 0.83，OrganSMNIST 分類挑戰從 0.97 提升至 0.98，腦出血分類任務從 0.73 提升至 0.83。此外，由於資料集簡化，預訓練速度最高可提升九倍。總之，所提出的方法突顯了資料集品質的重要性，並提供了一個可轉移的方法來改善醫學影像上分類下游任務的對比預訓練。

##### **Do LLMs "know" internally when they follow instructions?**
2410.14516v1 by Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Shirley Ren, Udhay Nallasamy, Andy Miller, Kwan Ho Ryan Chan, Jaya Narain

Instruction-following is crucial for building AI agents with large language
models (LLMs), as these models must adhere strictly to user-provided
constraints and guidelines. However, LLMs often fail to follow even simple and
clear instructions. To improve instruction-following behavior and prevent
undesirable outputs, a deeper understanding of how LLMs' internal states relate
to these outcomes is required. Our analysis of LLM internal states reveal a
dimension in the input embedding space linked to successful
instruction-following. We demonstrate that modifying representations along this
dimension improves instruction-following success rates compared to random
changes, without compromising response quality. Further investigation reveals
that this dimension is more closely related to the phrasing of prompts rather
than the inherent difficulty of the task or instructions. This discovery also
suggests explanations for why LLMs sometimes fail to follow clear instructions
and why prompt engineering is often effective, even when the content remains
largely unchanged. This work provides insight into the internal workings of
LLMs' instruction-following, paving the way for reliable LLM agents.

摘要：對於建構具備大型語言模型 (LLM) 的 AI 代理而言，遵循指令至關重要，因為這些模型必須嚴格遵守使用者提供的約束和準則。然而，LLM 經常無法遵循簡單且明確的指令。為了改善遵循指令的行為並防止產生不良輸出，需要更深入地了解 LLM 的內部狀態如何與這些結果相關。我們對 LLM 內部狀態的分析揭示了輸入嵌入空間中與成功遵循指令相關的一個面向。我們證明，與隨機變更相比，沿著這個面向修改表示可以提高遵循指令的成功率，而不會損害回應品質。進一步的調查顯示，這個面向與提示的措辭更為相關，而非任務或指令的固有難度。這個發現也解釋了為什麼 LLM 有時無法遵循明確的指令，以及為什麼提示工程通常很有效，即使內容在很大程度上保持不變。這項工作提供了對 LLM 遵循指令的內部運作的見解，為可靠的 LLM 代理鋪平了道路。

##### **Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media**
2410.14515v1 by Owen Cook, Charlie Grimshaw, Ben Wu, Sophie Dillon, Jack Hicks, Luke Jones, Thomas Smith, Matyas Szert, Xingyi Song

Misinformation spreads rapidly on social media, confusing the truth and
targetting potentially vulnerable people. To effectively mitigate the negative
impact of misinformation, it must first be accurately detected before applying
a mitigation strategy, such as X's community notes, which is currently a manual
process. This study takes a knowledge-based approach to misinformation
detection, modelling the problem similarly to one of natural language
inference. The EffiARA annotation framework is introduced, aiming to utilise
inter- and intra-annotator agreement to understand the reliability of each
annotator and influence the training of large language models for
classification based on annotator reliability. In assessing the EffiARA
annotation framework, the Russo-Ukrainian Conflict Knowledge-Based
Misinformation Classification Dataset (RUC-MCD) was developed and made publicly
available. This study finds that sample weighting using annotator reliability
performs the best, utilising both inter- and intra-annotator agreement and
soft-label training. The highest classification performance achieved using
Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.

摘要：错误讯息在社交媒体上快速散播，混淆了真相，并针对可能容易受骗的人。为了有效减轻错误讯息的负面影响，必须先准确地侦测出错误讯息，然后再应用缓解策略，例如 X 的社群笔记，这目前是一个手动的程序。本研究采取基于知识的方法来侦测错误讯息，将问题建模为类似于自然语言推论的问题。引入了 EffiARA 标注框架，旨在利用标注者之间和标注者内部的一致性来了解每个标注者的可靠性，并影响大型语言模型的训练，以便根据标注者的可靠性进行分类。在评估 EffiARA 标注框架时，开发了俄乌冲突知识型错误讯息分类数据集 (RUC-MCD)，并公开提供。本研究发现，使用标注者可靠性进行样本加权表现最佳，同时利用标注者之间和标注者内部的一致性，以及软标签训练。使用 Llama-3.2-1B 达到的最高分类效能，使用 TwHIN-BERT-large 时，宏观 F1 分别为 0.757 和 0.740。

##### **LEAD: Latent Realignment for Human Motion Diffusion**
2410.14508v1 by Nefeli Andreou, Xi Wang, Victoria Fernández Abrevaya, Marie-Paule Cani, Yiorgos Chrysanthou, Vicky Kalogeiton

Our goal is to generate realistic human motion from natural language. Modern
methods often face a trade-off between model expressiveness and text-to-motion
alignment. Some align text and motion latent spaces but sacrifice
expressiveness; others rely on diffusion models producing impressive motions,
but lacking semantic meaning in their latent space. This may compromise
realism, diversity, and applicability. Here, we address this by combining
latent diffusion with a realignment mechanism, producing a novel, semantically
structured space that encodes the semantics of language. Leveraging this
capability, we introduce the task of textual motion inversion to capture novel
motion concepts from a few examples. For motion synthesis, we evaluate LEAD on
HumanML3D and KIT-ML and show comparable performance to the state-of-the-art in
terms of realism, diversity, and text-motion consistency. Our qualitative
analysis and user study reveal that our synthesized motions are sharper, more
human-like and comply better with the text compared to modern methods. For
motion textual inversion, our method demonstrates improved capacity in
capturing out-of-distribution characteristics in comparison to traditional
VAEs.

摘要：我们的目标是根据自然语言生成逼真的真人动作。现代方法通常会在模型表现力与文本到动作的比对之间面临权衡。一些方法比对了文本和动作的潜在空间，但牺牲了表现力；其他方法依靠扩散模型来制作令人印象深刻的动作，但其潜在空间缺乏语义含义。这可能会损害真实性、多样性和适用性。在这里，我们通过将潜在扩散与重新比对机制相结合来解决这个问题，从而生成一个新颖的、语义结构化的空间，对语言的语义进行编码。利用这种能力，我们引入了文本动作反演任务，以从几个示例中捕捉新颖的动作概念。对于动作合成，我们在 HumanML3D 和 KIT-ML 上评估了 LEAD，并展示了与最先进技术相当的性能，包括真实性、多样性和文本动作一致性。我们的定性分析和用户研究表明，与现代方法相比，我们合成的动作更清晰、更像真人，并且与文本的吻合度更高。对于动作文本反演，我们的方法在捕捉分布外特征方面展示了比传统 VAE 更强的能力。

##### **SignAttention: On the Interpretability of Transformer Models for Sign Language Translation**
2410.14506v1 by Pedro Alejandro Dal Bianco, Oscar Agustín Stanchi, Facundo Manuel Quiroga, Franco Ronchetti, Enzo Ferrante

This paper presents the first comprehensive interpretability analysis of a
Transformer-based Sign Language Translation (SLT) model, focusing on the
translation from video-based Greek Sign Language to glosses and text.
Leveraging the Greek Sign Language Dataset, we examine the attention mechanisms
within the model to understand how it processes and aligns visual input with
sequential glosses. Our analysis reveals that the model pays attention to
clusters of frames rather than individual ones, with a diagonal alignment
pattern emerging between poses and glosses, which becomes less distinct as the
number of glosses increases. We also explore the relative contributions of
cross-attention and self-attention at each decoding step, finding that the
model initially relies on video frames but shifts its focus to previously
predicted tokens as the translation progresses. This work contributes to a
deeper understanding of SLT models, paving the way for the development of more
transparent and reliable translation systems essential for real-world
applications.

摘要：本文提供了基於 Transformer 的手語翻譯 (SLT) 模型的第一個全面可解釋性分析，重點關注從影片希臘手語翻譯成符號和文字。
利用希臘手語資料集，我們檢視模型中的注意力機制，以了解它如何處理和將視覺輸入與順序符號對齊。我們的分析顯示，模型關注的是成群的畫面，而不是個別畫面，姿勢和符號之間出現對角線對齊模式，隨著符號數量的增加，這種對齊模式變得不那麼明顯。我們還探討了在每個解碼步驟中交叉注意力和自我注意力的相對貢獻，發現模型最初依賴於影片畫面，但隨著翻譯的進行，它將重點轉移到先前預測的符號上。這項工作有助於更深入地了解 SLT 模型，為開發更透明且可靠的翻譯系統鋪平道路，這對於現實世界的應用至關重要。

##### **ANT: Adaptive Noise Schedule for Time Series Diffusion Models**
2410.14488v1 by Seunghan Lee, Kibok Lee, Taeyoung Park

Advances in diffusion models for generative artificial intelligence have
recently propagated to the time series (TS) domain, demonstrating
state-of-the-art performance on various tasks. However, prior works on TS
diffusion models often borrow the framework of existing works proposed in other
domains without considering the characteristics of TS data, leading to
suboptimal performance. In this work, we propose Adaptive Noise schedule for
Time series diffusion models (ANT), which automatically predetermines proper
noise schedules for given TS datasets based on their statistics representing
non-stationarity. Our intuition is that an optimal noise schedule should
satisfy the following desiderata: 1) It linearly reduces the non-stationarity
of TS data so that all diffusion steps are equally meaningful, 2) the data is
corrupted to the random noise at the final step, and 3) the number of steps is
sufficiently large. The proposed method is practical for use in that it
eliminates the necessity of finding the optimal noise schedule with a small
additional cost to compute the statistics for given datasets, which can be done
offline before training. We validate the effectiveness of our method across
various tasks, including TS forecasting, refinement, and generation, on
datasets from diverse domains. Code is available at this repository:
https://github.com/seunghan96/ANT.

摘要：生成式人工智能的擴散模型的進展最近已傳播到時間序列 (TS) 領域，在各種任務上展現最先進的效能。然而，先前針對 TS 擴散模型的研究，經常借用其他領域中既有研究的架構，而未考慮 TS 資料的特性，導致效能不佳。在這項研究中，我們提出適用於時間序列擴散模型的自適應雜訊排程 (ANT)，它會根據代表非平穩性的統計資料，自動預先決定適當的雜訊排程，以提供給 TS 資料集。我們的直覺是，最佳的雜訊排程應滿足下列條件：1) 線性減少 TS 資料的非平穩性，以便所有擴散步驟都同樣有意義，2) 資料在最後一個步驟會損毀為隨機雜訊，以及 3) 步驟數目足夠多。所提出的方法在於它消除了尋找最佳雜訊排程的必要性，而僅需付出少量額外成本來計算給定資料集的統計資料，這可以在訓練前離線完成，因此很實用。我們在各種任務（包括 TS 預測、調整和生成）中驗證了我們方法的有效性，資料集來自不同的領域。程式碼可在下列存放庫取得：https://github.com/seunghan96/ANT。

##### **DRL Optimization Trajectory Generation via Wireless Network Intent-Guided Diffusion Models for Optimizing Resource Allocation**
2410.14481v1 by Junjie Wu, Xuming Fang, Dusit Niyato, Jiacheng Wang, Jingyu Wang

With the rapid advancements in wireless communication fields, including
low-altitude economies, 6G, and Wi-Fi, the scale of wireless networks continues
to expand, accompanied by increasing service quality demands. Traditional deep
reinforcement learning (DRL)-based optimization models can improve network
performance by solving non-convex optimization problems intelligently. However,
they heavily rely on online deployment and often require extensive initial
training. Online DRL optimization models typically make accurate decisions
based on current channel state distributions. When these distributions change,
their generalization capability diminishes, which hinders the responsiveness
essential for real-time and high-reliability wireless communication networks.
Furthermore, different users have varying quality of service (QoS) requirements
across diverse scenarios, and conventional online DRL methods struggle to
accommodate this variability. Consequently, exploring flexible and customized
AI strategies is critical. We propose a wireless network intent (WNI)-guided
trajectory generation model based on a generative diffusion model (GDM). This
model can be generated and fine-tuned in real time to achieve the objective and
meet the constraints of target intent networks, significantly reducing state
information exposure during wireless communication. Moreover, The WNI-guided
optimization trajectory generation can be customized to address differentiated
QoS requirements, enhancing the overall quality of communication in future
intelligent networks. Extensive simulation results demonstrate that our
approach achieves greater stability in spectral efficiency variations and
outperforms traditional DRL optimization models in dynamic communication
systems.

摘要：隨著無線通訊領域的快速進展，包括低空經濟、6G 和 Wi-Fi，無線網路的規模持續擴大，伴隨著對服務品質需求的提升。傳統的深度強化學習 (DRL) 優化模型可以透過智慧地解決非凸優化問題來提升網路效能。然而，它們高度依賴線上部署，而且通常需要大量的初始訓練。線上 DRL 優化模型通常會根據目前的頻道狀態分佈做出精準的決策。當這些分佈改變時，它們的泛化能力會下降，這會妨礙對即時且高可靠度無線通訊網路至關重要的回應能力。此外，不同的使用者在不同的場景中有不同的服務品質 (QoS) 要求，而傳統的線上 DRL 方法難以因應這種變異性。因此，探索彈性且客製化的 AI 策略至關重要。我們提出一個基於生成擴散模型 (GDM) 的無線網路意圖 (WNI) 引導軌跡生成模型。這個模型可以在即時產生並微調，以達成目標並滿足目標意圖網路的限制，大幅減少無線通訊中的狀態資訊暴露。此外，WNI 引導的優化軌跡生成可以客製化以滿足不同的 QoS 要求，提升未來智慧網路中通訊的整體品質。廣泛的模擬結果證明，我們的做法在頻譜效率變異中達到更高的穩定性，而且在動態通訊系統中優於傳統的 DRL 優化模型。

##### **Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of Language Models**
2410.14480v1 by James Vo

As large language models (LLMs) continue to advance, the need for precise and
efficient evaluation metrics becomes more pressing. Traditional approaches,
while informative, often face limitations in computational demands and
interpretability. In this paper, we introduce a novel hybrid evaluation method
that integrates two established techniques: entropy derived from covariance
matrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing
hidden states from LLMs, then computes the covariance matrix and MNN from these
representations. We further calculate the entropy of the covariance matrix to
capture uncertainty and redundancy in the model's outputs. By combining these
metrics into a composite score, we offer a comprehensive evaluation framework
that balances accuracy with computational efficiency. Additionally, our
approach allows for flexibility in adjusting the weightings between entropy and
MNN, tailoring the evaluation for different objectives. Through a series of
experiments on various LLMs, we demonstrate the robustness and efficacy of our
method, offering deeper insights into model performance. This work contributes
to the ongoing development of LLM evaluation and opens avenues for future
innovations in model assessment techniques.

摘要：隨著大型語言模型 (LLM) 持續進步，對精確且有效率的評估指標的需求變得更加迫切。傳統方法雖然具有參考價值，但通常在運算需求和可解釋性方面面臨限制。在本文中，我們介紹了一種新穎的混合評估方法，它整合了兩種既定的技術：從協方差矩陣中提取的熵和矩陣核範數 (MNN)。我們的評估方法首先對 LLM 中的隱藏狀態進行正規化，然後從這些表示中計算協方差矩陣和 MNN。我們進一步計算協方差矩陣的熵，以捕捉模型輸出中的不確定性和冗餘。通過將這些指標組合成一個綜合評分，我們提供了一個全面的評估框架，它平衡了準確性和運算效率。此外，我們的評估方法允許靈活調整熵和 MNN 之間的權重，針對不同的目標調整評估。透過對各種 LLM 進行一系列實驗，我們證明了我們的方法的穩健性和有效性，提供了對模型效能更深入的見解。這項工作有助於 LLM 評估的持續發展，並為模型評估技術的未來創新開啟了道路。

##### **How Do Training Methods Influence the Utilization of Vision Models?**
2410.14470v1 by Paul Gavrikov, Shashank Agnihotri, Margret Keuper, Janis Keuper

Not all learnable parameters (e.g., weights) contribute equally to a neural
network's decision function. In fact, entire layers' parameters can sometimes
be reset to random values with little to no impact on the model's decisions. We
revisit earlier studies that examined how architecture and task complexity
influence this phenomenon and ask: is this phenomenon also affected by how we
train the model? We conducted experimental evaluations on a diverse set of
ImageNet-1k classification models to explore this, keeping the architecture and
training data constant but varying the training pipeline. Our findings reveal
that the training method strongly influences which layers become critical to
the decision function for a given task. For example, improved training regimes
and self-supervised training increase the importance of early layers while
significantly under-utilizing deeper layers. In contrast, methods such as
adversarial training display an opposite trend. Our preliminary results extend
previous findings, offering a more nuanced understanding of the inner mechanics
of neural networks.
  Code: https://github.com/paulgavrikov/layer_criticality

摘要：並非所有可學習參數（例如權重）都能平等地促成神經網路的決策函數。事實上，有時可以將整層的參數重置為隨機值，而對模型的決策幾乎沒有影響。我們重新探討了先前探討架構和任務複雜度如何影響此現象的研究，並提出問題：此現象是否也受我們訓練模型的方式影響？我們對 ImageNet-1k 分類模型的多樣化集合進行了實驗性評估，以探索這一點，保持架構和訓練資料不變，但改變訓練管道。我們的研究結果顯示，訓練方法強烈影響哪些層對於給定任務的決策函數至關重要。例如，改善的訓練方式和自我監督訓練增加了早期層的重要性，同時顯著地減少了深度層的利用率。相反，對抗訓練等方法則呈現相反的趨勢。我們的初步結果擴展了先前的研究結果，提供了對神經網路內部機制的更細緻理解。
程式碼：https://github.com/paulgavrikov/layer_criticality

##### **The Propensity for Density in Feed-forward Models**
2410.14461v1 by Nandi Schoots, Alex Jackson, Ali Kholmovaia, Peter McBurney, Murray Shanahan

Does the process of training a neural network to solve a task tend to use all
of the available weights even when the task could be solved with fewer weights?
To address this question we study the effects of pruning fully connected,
convolutional and residual models while varying their widths. We find that the
proportion of weights that can be pruned without degrading performance is
largely invariant to model size. Increasing the width of a model has little
effect on the density of the pruned model relative to the increase in absolute
size of the pruned network. In particular, we find substantial prunability
across a large range of model sizes, where our biggest model is 50 times as
wide as our smallest model. We explore three hypotheses that could explain
these findings.

摘要：訓練神經網路以解決任務的過程是否傾向於使用所有可用的權重，即使該任務可以用更少的權重來解決？
為了回答這個問題，我們研究了在改變寬度的同時，修剪全連接、卷積和殘差模型的效果。我們發現，可以在不降低效能的情況下修剪的權重比例在很大程度上與模型大小無關。增加模型的寬度對修剪模型的密度幾乎沒有影響，相對於修剪網路的絕對大小而言。特別是，我們發現各種模型大小都有大量的可修剪性，其中我們最大的模型比我們最小的模型寬 50 倍。我們探討了三種可以解釋這些發現的假設。

##### **Toward Generalizing Visual Brain Decoding to Unseen Subjects**
2410.14445v1 by Xiangtao Kong, Kexin Huang, Ping Li, Lei Zhang

Visual brain decoding aims to decode visual information from human brain
activities. Despite the great progress, one critical limitation of current
brain decoding research lies in the lack of generalization capability to unseen
subjects. Prior works typically focus on decoding brain activity of individuals
based on the observation that different subjects exhibit different brain
activities, while it remains unclear whether brain decoding can be generalized
to unseen subjects. This study aims to answer this question. We first
consolidate an image-fMRI dataset consisting of stimulus-image and
fMRI-response pairs, involving 177 subjects in the movie-viewing task of the
Human Connectome Project (HCP). This dataset allows us to investigate the brain
decoding performance with the increase of participants. We then present a
learning paradigm that applies uniform processing across all subjects, instead
of employing different network heads or tokenizers for individuals as in
previous methods, which can accommodate a large number of subjects to explore
the generalization capability across different subjects. A series of
experiments are conducted and we have the following findings. First, the
network exhibits clear generalization capabilities with the increase of
training subjects. Second, the generalization capability is common to popular
network architectures (MLP, CNN and Transformer). Third, the generalization
performance is affected by the similarity between subjects. Our findings reveal
the inherent similarities in brain activities across individuals. With the
emerging of larger and more comprehensive datasets, it is possible to train a
brain decoding foundation model in the future.Codes and models can be found at
https://github.com/Xiangtaokong/TGBD.

摘要：視覺大腦解碼旨在從人腦活動中解碼視覺資訊。儘管取得了巨大進展，但當前大腦解碼研究的一個關鍵限制在於缺乏對未見受試者的概化能力。先前的研究通常專注於根據不同受試者表現出不同的腦活動來解碼個體的腦活動，而大腦解碼是否能概化到未見受試者仍不清楚。本研究旨在回答這個問題。我們首先整合一個影像 fMRI 資料集，其中包含刺激影像和 fMRI 反應配對，涉及電影觀看任務中的人類連接組計畫 (HCP) 的 177 位受試者。此資料集讓我們能夠隨著參與者增加來調查大腦解碼效能。然後，我們提出一個學習範例，對所有受試者套用一致的處理，而不是像先前方法那樣為個人採用不同的網路區塊或標記化器，這能容納大量的受試者來探索不同受試者之間的概化能力。進行了一系列實驗，我們有以下發現。首先，網路隨著訓練受試者的增加而表現出明顯的概化能力。其次，概化能力在流行的網路架構（MLP、CNN 和 Transformer）中是常見的。第三，概化效能受受試者之間相似性的影響。我們的發現揭示了個人之間腦活動的內在相似性。隨著更大、更全面的資料集的出現，未來有可能訓練一個大腦解碼基礎模型。程式碼和模型可以在 https://github.com/Xiangtaokong/TGBD 中找到。

##### **A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference**
2410.14442v1 by You Wu, Haoyi Wu, Kewei Tu

Recently, sharing key-value (KV) cache across layers has been found effective
in efficient inference of large language models (LLMs). To systematically
investigate different techniques of cross-layer KV sharing, we propose a
unified framework that covers several recent methods and their novel variants.
We conduct comprehensive experiments on all the configurations of the
framework, evaluating their generation throughput and performance in language
modeling and downstream tasks. We find that when reducing the size of the KV
cache by 2x, most configurations can achieve competitive performance to and
higher throughput than standard transformers, but when further reducing the
size of the KV cache, pairing queries of all layers with KVs of upper layers
can better maintain performance, although it also introduces additional
training cost and prefilling latency. We hope that this work will help users
choose the appropriate approach according to their requirements and facilitate
research on the acceleration of LLM inference.

摘要：最近，發現跨層共享鍵值 (KV) 快取在大型語言模型 (LLM) 的高效推理中很有效。為了系統地研究跨層 KV 共享的不同技術，我們提出了一個統一的框架，涵蓋了幾種最近的方法及其新穎的變體。我們對框架的所有配置進行了全面的實驗，評估了它們在語言建模和下游任務中的生成吞吐量和效能。我們發現，當將 KV 快取的大小減少 2 倍時，大多數配置都可以達到與標準Transformer相當的效能和更高的吞吐量，但當進一步減少 KV 快取的大小時，將所有層的查詢與上層的 KV 配對可以更好地維持效能，儘管它也會引入額外的訓練成本和預填充延遲。我們希望這項工作將幫助使用者根據自己的需求選擇適當的方法，並促進對 LLM 推理加速的研究。

##### **Learning to refine domain knowledge for biological network inference**
2410.14436v1 by Peiwen Li, Menghua Wu

Perturbation experiments allow biologists to discover causal relationships
between variables of interest, but the sparsity and high dimensionality of
these data pose significant challenges for causal structure learning
algorithms. Biological knowledge graphs can bootstrap the inference of causal
structures in these situations, but since they compile vastly diverse
information, they can bias predictions towards well-studied systems.
Alternatively, amortized causal structure learning algorithms encode inductive
biases through data simulation and train supervised models to recapitulate
these synthetic graphs. However, realistically simulating biology is arguably
even harder than understanding a specific system. In this work, we take
inspiration from both strategies and propose an amortized algorithm for
refining domain knowledge, based on data observations. On real and synthetic
datasets, we show that our approach outperforms baselines in recovering ground
truth causal graphs and identifying errors in the prior knowledge with limited
interventional data.

摘要：擾動實驗讓生物學家能夠找出感興趣變數之間的因果關係，但這些資料的稀疏性和高維度性對因果結構學習演算法構成重大挑戰。生物學知識圖譜可以在這些情況下引導因果結構的推論，但由於它們編譯了非常多樣化的資訊，因此可能會將預測偏向於研究完善的系統。或者，攤銷因果結構學習演算法通過資料模擬編碼歸納偏差，並訓練監督式模型以概括這些合成圖譜。然而，現實地模擬生物學可以說是比理解特定系統更困難。在這項工作中，我們從這兩種策略中汲取靈感，並提出一個攤銷演算法，用於根據資料觀察結果精煉領域知識。在真實和合成資料集上，我們展示了我們的做法在恢復真實因果圖譜和識別先驗知識中的錯誤方面優於基線，且介入資料有限。

##### **FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models**
2410.14429v1 by Rui Hu, Qian He, Gaofeng He, Jiedong Zhuang, Huang Chen, Huafeng Liu, Huamin Wang

Modeling and producing lifelike clothed human images has attracted
researchers' attention from different areas for decades, with the complexity
from highly articulated and structured content. Rendering algorithms decompose
and simulate the imaging process of a camera, while are limited by the accuracy
of modeled variables and the efficiency of computation. Generative models can
produce impressively vivid human images, however still lacking in
controllability and editability. This paper studies photorealism enhancement of
rendered images, leveraging generative power from diffusion models on the
controlled basis of rendering. We introduce a novel framework to translate
rendered images into their realistic counterparts, which consists of two
stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG).
In DKI, we adopt positive (real) domain finetuning and negative (rendered)
domain embedding to inject knowledge into a pretrained Text-to-image (T2I)
diffusion model. In RIG, we generate the realistic image corresponding to the
input rendered image, with a Texture-preserving Attention Control (TAC) to
preserve fine-grained clothing textures, exploiting the decoupled features
encoded in the UNet structure. Additionally, we introduce SynFashion dataset,
featuring high-quality digital clothing images with diverse textures. Extensive
experimental results demonstrate the superiority and effectiveness of our
method in rendered-to-real image translation.

摘要：數十年來，建模和製作逼真的穿著人類影像一直吸引著不同領域的研究人員的注意，其複雜性來自於高度關節化和結構化的內容。渲染演算法分解並模擬相機的成像過程，同時受到建模變數的準確性和運算效率的限制。生成模型可以產生令人印象深刻的生動人類影像，然而仍然缺乏可控性和可編輯性。本文研究了渲染影像的寫實主義增強，利用了擴散模型的生成能力，在渲染的受控基礎上。我們引入了一個新的框架，將渲染影像轉換成它們的逼真對應物，其中包含兩個階段：領域知識注入 (DKI) 和逼真影像生成 (RIG)。在 DKI 中，我們採用正向（真實）領域微調和負向（渲染）領域嵌入，將知識注入預訓練的文字轉影像 (T2I) 擴散模型中。在 RIG 中，我們產生與輸入渲染影像對應的逼真影像，並使用紋理保留注意力控制 (TAC) 來保留細緻的服裝紋理，利用 UNet 結構中編碼的解耦特徵。此外，我們引入了 SynFashion 資料集，其中包含具有不同紋理的高品質數位服裝影像。廣泛的實驗結果證明了我們的方法在渲染到真實影像轉換中的優越性和有效性。

##### **Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation**
2410.14425v1 by Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Meihuizi Jia, Yichao Feng, Luu Anh Tuan

Parameter-efficient fine-tuning (PEFT) can bridge the gap between large
language models (LLMs) and downstream tasks. However, PEFT has been proven
vulnerable to malicious attacks. Research indicates that poisoned LLMs, even
after PEFT, retain the capability to activate internalized backdoors when input
samples contain predefined triggers. In this paper, we introduce a novel
weak-to-strong unlearning algorithm to defend against backdoor attacks based on
feature alignment knowledge distillation, named W2SDefense. Specifically, we
first train a small-scale language model through full-parameter fine-tuning to
serve as the clean teacher model. Then, this teacher model guides the
large-scale poisoned student model in unlearning the backdoor, leveraging PEFT.
Theoretical analysis suggests that W2SDefense has the potential to enhance the
student model's ability to unlearn backdoor features, preventing the activation
of the backdoor. We conduct experiments on text classification tasks involving
three state-of-the-art language models and three different backdoor attack
algorithms. Our empirical results demonstrate the outstanding performance of
W2SDefense in defending against backdoor attacks without compromising model
performance.

摘要：參數高效微調 (PEFT) 可以彌合大型語言模型 (LLM) 和下游任務之間的差距。然而，PEFT 已被證明容易受到惡意攻擊。研究表明，中毒的 LLM，即使在 PEFT 之後，仍有能力在輸入範例包含預定義觸發器時啟動內部後門。在本文中，我們引入了一種新穎的弱到強的遺忘演算法，以基於特徵對齊知識蒸餾來防禦後門攻擊，稱為 W2SDefense。具體來說，我們首先通過全參數微調訓練一個小規模語言模型，作為乾淨的教師模型。然後，這個教師模型指導大規模中毒的學生模型在利用 PEFT 遺忘後門。理論分析表明，W2SDefense 有可能增強學生模型遺忘後門特徵的能力，防止後門被啟動。我們對涉及三個最先進語言模型和三個不同後門攻擊演算法的文本分類任務進行了實驗。我們的實證結果證明了 W2SDefense 在防禦後門攻擊方面的出色表現，而不會損害模型性能。

##### **Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion**
2410.14405v1 by Denitsa Saynova, Lovisa Hagström, Moa Johansson, Richard Johansson, Marco Kuhlmann

Previous interpretations of language models (LMs) miss important distinctions
in how these models process factual information. For example, given the query
"Astrid Lindgren was born in" with the corresponding completion "Sweden", no
difference is made between whether the prediction was based on having the exact
knowledge of the birthplace of the Swedish author or assuming that a person
with a Swedish-sounding name was born in Sweden. In this paper, we investigate
four different prediction scenarios for which the LM can be expected to show
distinct behaviors. These scenarios correspond to different levels of model
reliability and types of information being processed - some being less
desirable for factual predictions. To facilitate precise interpretations of LMs
for fact completion, we propose a model-specific recipe called PrISM for
constructing datasets with examples of each scenario based on a set of
diagnostic criteria. We apply a popular interpretability method, causal tracing
(CT), to the four prediction scenarios and find that while CT produces
different results for each scenario, aggregations over a set of mixed examples
may only represent the results from the scenario with the strongest measured
signal. In summary, we contribute tools for a more granular study of fact
completion in language models and analyses that provide a more nuanced
understanding of how LMs process fact-related queries.

摘要：先前的語言模型 (LM) 解釋遺漏了這些模型如何處理事實資訊的重要區別。例如，給定查詢「Astrid Lindgren 出生於」並完成「瑞典」，不論預測是基於對瑞典作者出生地確切知識，還是假設名字聽起來像瑞典人的出生於瑞典，兩者之間沒有區別。在本文中，我們探討了四種不同的預測情境，預計 LM 會表現出不同的行為。這些情境對應於不同的模型可靠性層級和正在處理的資訊類型，其中一些對於事實預測而言較不理想。為了便於對 LM 進行精確解釋以完成事實，我們提出了一個稱為 PrISM 的特定模型配方，用於根據一組診斷準則構建具有每種情境範例的資料集。我們將一種流行的可解釋性方法因果追蹤 (CT) 套用到四種預測情境，發現儘管 CT 為每種情境產生不同的結果，但對一組混合範例的聚合可能只代表測量訊號最強的情境的結果。總之，我們為更細緻地研究語言模型中的事實完成提供了工具，並提供了更細緻的分析，說明 LM 如何處理與事實相關的查詢。

##### **SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning**
2410.14399v1 by Magdalena Wysocka, Danilo S. Carvalho, Oskar Wysocki, Marco Valentino, Andre Freitas

Syllogistic reasoning is crucial for Natural Language Inference (NLI). This
capability is particularly significant in specialized domains such as
biomedicine, where it can support automatic evidence interpretation and
scientific discovery. This paper presents SylloBio-NLI, a novel framework that
leverages external ontologies to systematically instantiate diverse syllogistic
arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language
Models (LLMs) on identifying valid conclusions and extracting supporting
evidence across 28 syllogistic schemes instantiated with human genome pathways.
Extensive experiments reveal that biomedical syllogistic reasoning is
particularly challenging for zero-shot LLMs, which achieve an average accuracy
between 70% on generalized modus ponens and 23% on disjunctive syllogism. At
the same time, we found that few-shot prompting can boost the performance of
different LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper
analysis shows that both techniques exhibit high sensitivity to superficial
lexical variations, highlighting a dependency between reliability, models'
architecture, and pre-training regime. Overall, our results indicate that,
while in-context examples have the potential to elicit syllogistic reasoning in
LLMs, existing models are still far from achieving the robustness and
consistency required for safe biomedical NLI applications.

摘要：三段論推理對於自然語言推論 (NLI) 至關重要。此能力在生物醫學等專業領域中特別重要，因為它可以支持自動證據解釋和科學發現。本文提出了 SylloBio-NLI，一個利用外部本体論系統化地為生物醫學 NLI 實例化不同三段論論證的新框架。我們使用 SylloBio-NLI 來評估大型語言模型 (LLM) 在識別有效結論和提取支持證據方面的能力，這些證據跨越了 28 個使用人類基因組路徑實例化的三段論架構。廣泛的實驗表明，生物醫學三段論推理對於零次學習 LLM 來說特別具有挑戰性，它們在廣義肯定式中達到 70% 的平均準確度，在析取三段論中達到 23% 的平均準確度。同時，我們發現少次學習提示可以提升不同 LLM 的性能，包括 Gemma (+14%) 和 LLama-3 (+43%)。然而，更深入的分析表明，這兩種技術都對表面的詞彙變化表現出高度敏感性，突出了可靠性、模型架構和預訓練機制之間的依賴性。總的來說，我們的結果表明，雖然上下文範例有潛力引發 LLM 中的三段論推理，但現有模型仍遠未達到安全生物醫學 NLI 應用所需的穩健性和一致性。

##### **Generative AI, Pragmatics, and Authenticity in Second Language Learning**
2410.14395v1 by Robert Godwin-Jones`

There are obvious benefits to integrating generative AI (artificial
intelligence) into language learning and teaching. Those include using AI as a
language tutor, creating learning materials, or assessing learner output.
However, due to how AI systems under-stand human language, based on a
mathematical model using statistical probability, they lack the lived
experience to be able to use language with the same social aware-ness as
humans. Additionally, there are built-in linguistic and cultural biases based
on their training data which is mostly in English and predominantly from
Western sources. Those facts limit AI suitability for some language learning
interactions. Stud-ies have clearly shown that systems such as ChatGPT often do
not produce language that is pragmatically appropriate. The lack of linguistic
and cultural authenticity has important implications for how AI is integrated
into second language acquisition as well as in instruction targeting
development of intercultural communication compe-tence.

摘要：將生成式人工智慧（人工智慧）整合到語言學習和教學中，具有顯而易見的好處。這些好處包括使用人工智慧作為語言導師、建立學習材料或評量學習者的產出。
然而，由於人工智慧系統對人類語言的理解，是基於使用統計機率的數學模型，因此他們缺乏生活經驗，無法像人類一樣在社交意識上使用語言。此外，基於主要為英文且多數來自西方來源的訓練資料，存在內建的語言和文化偏見。這些事實限制了人工智慧對某些語言學習互動的適用性。研究明確顯示，例如 ChatGPT 的系統通常不會產生實用適當的語言。語言和文化真實性的缺乏，對人工智慧如何整合到第二語言習得，以及針對培養跨文化溝通能力的教學，具有重大的影響。

##### **Debug Smarter, Not Harder: AI Agents for Error Resolution in Computational Notebooks**
2410.14393v1 by Konstantin Grotov, Artem Borzilov, Maksim Krivobok, Timofey Bryksin, Yaroslav Zharov

Computational notebooks became indispensable tools for research-related
development, offering unprecedented interactivity and flexibility in the
development process. However, these benefits come at the cost of
reproducibility and an increased potential for bugs. With the rise of
code-fluent Large Language Models empowered with agentic techniques, smart
bug-fixing tools with a high level of autonomy have emerged. However, those
tools are tuned for classical script programming and still struggle with
non-linear computational notebooks. In this paper, we present an AI agent
designed specifically for error resolution in a computational notebook. We have
developed an agentic system capable of exploring a notebook environment by
interacting with it -- similar to how a user would -- and integrated the system
into the JetBrains service for collaborative data science called Datalore. We
evaluate our approach against the pre-existing single-action solution by
comparing costs and conducting a user study. Users rate the error resolution
capabilities of the agentic system higher but experience difficulties with UI.
We share the results of the study and consider them valuable for further
improving user-agent collaboration.

摘要：計算筆記本已成為與研究相關的開發中不可或缺的工具，在開發過程中提供了前所未有的互動性和靈活性。然而，這些好處是以可複製性和增加錯誤的潛力為代價的。隨著具備代理技術的代碼流暢大型語言模型的興起，具備高度自主性的智慧錯誤修正工具應運而生。然而，這些工具是針對經典腳本程式設計進行調整的，並且仍然難以應付非線性計算筆記本。在本文中，我們提出了一個專門為計算筆記本中的錯誤解析而設計的人工智慧代理。我們已經開發了一個代理系統，它能夠透過與筆記本環境互動來探索該環境——類似於使用者的方式——並將該系統整合到 JetBrains 服務中，以進行協作資料科學，稱為 Datalore。我們透過比較成本並進行使用者研究，針對現有的單一動作解決方案評估我們的做法。使用者對代理系統的錯誤解析功能評價較高，但在使用者介面上遇到困難。我們分享研究結果，並認為它們對於進一步改善使用者代理協作很有價值。

##### **Analyzing Context Utilization of LLMs in Document-Level Translation**
2410.14391v1 by Wafaa Mohammed, Vlad Niculae

Large language models (LLM) are increasingly strong contenders in machine
translation. We study document-level translation, where some words cannot be
translated without context from outside the sentence. We investigate the
ability of prominent LLMs to utilize context by analyzing models' robustness to
perturbed and randomized document context. We find that LLMs' improved
document-translation performance is not always reflected in pronoun translation
performance. We highlight the need for context-aware finetuning of LLMs with a
focus on relevant parts of the context to improve their reliability for
document-level translation.

摘要：大型語言模型 (LLM) 在機器翻譯中越來越強大。我們研究文件級別翻譯，其中某些詞彙無法在句子外部的語境中翻譯。我們調查了傑出的 LLM 利用語境的能​​力，方法是分析模型對擾動和隨機化文件語境的穩健性。我們發現 LLM 改進的文件翻譯性能並非總是反映在代詞翻譯性能中。我們強調需要針對 LLM 進行語境感知微調，重點關注語境的相關部分，以提高其在文件級別翻譯中的可靠性。

##### **SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery**
2410.14389v1 by Enneng Yang, Li Shen, Zhenyi Wang, Guibing Guo, Xingwei Wang, Xiaocun Cao, Jie Zhang, Dacheng Tao

Model merging-based multitask learning (MTL) offers a promising approach for
performing MTL by merging multiple expert models without requiring access to
raw training data. However, in this paper, we examine the merged model's
representation distribution and uncover a critical issue of "representation
bias". This bias arises from a significant distribution gap between the
representations of the merged and expert models, leading to the suboptimal
performance of the merged MTL model. To address this challenge, we first
propose a representation surgery solution called Surgery. Surgery is a
lightweight, task-specific module that aligns the final layer representations
of the merged model with those of the expert models, effectively alleviating
bias and improving the merged model's performance. Despite these improvements,
a performance gap remains compared to the traditional MTL method. Further
analysis reveals that representation bias phenomena exist at each layer of the
merged model, and aligning representations only in the last layer is
insufficient for fully reducing systemic bias because biases introduced at each
layer can accumulate and interact in complex ways. To tackle this, we then
propose a more comprehensive solution, deep representation surgery (also called
SurgeryV2), which mitigates representation bias across all layers, and thus
bridges the performance gap between model merging-based MTL and traditional
MTL. Finally, we design an unsupervised optimization objective to optimize both
the Surgery and SurgeryV2 modules. Our experimental results show that
incorporating these modules into state-of-the-art (SOTA) model merging schemes
leads to significant performance gains. Notably, our SurgeryV2 scheme reaches
almost the same level as individual expert models or the traditional MTL model.
The code is available at \url{https://github.com/EnnengYang/SurgeryV2}.

摘要：<paragraph>基於模型合併的多任務學習 (MTL) 提供了一種有前途的方法，可透過合併多個專家模型來執行 MTL，而無需存取原始訓練資料。然而，在本文中，我們檢視了合併模型的表示分佈，並發現了一個「表示偏差」的關鍵問題。此偏差源自於合併模型與專家模型的表示之間的顯著分佈差距，導致合併 MTL 模型的次佳效能。為了應對這個挑戰，我們首先提出了一個稱為 Surgery 的表示手術解決方案。Surgery 是一個輕量級、特定於任務的模組，它會將合併模型的最終層表示與專家模型的表示對齊，有效減輕偏差並改善合併模型的效能。儘管有這些改進，但與傳統 MTL 方法相比，效能差距仍然存在。進一步的分析顯示，表示偏差現象存在於合併模型的每一層，而僅在最後一層對齊表示不足以完全減少系統性偏差，因為在每一層引入的偏差可能會以複雜的方式累積和交互。為了解決這個問題，我們接著提出了一個更全面的解決方案，深度表示手術（也稱為 SurgeryV2），它可以減輕所有層的表示偏差，從而彌合基於模型合併的 MTL 和傳統 MTL 之間的效能差距。最後，我們設計了一個無監督的最佳化目標，以最佳化 Surgery 和 SurgeryV2 模組。我們的實驗結果顯示，將這些模組納入最先進 (SOTA) 的模型合併方案會帶來顯著的效能提升。值得注意的是，我們的 SurgeryV2 方案達到了與個別專家模型或傳統 MTL 模型幾乎相同的水準。程式碼可在 \url{https://github.com/EnnengYang/SurgeryV2} 取得。</paragraph>

##### **How Do Multilingual Models Remember? Investigating Multilingual Factual Recall Mechanisms**
2410.14387v1 by Constanza Fierro, Negar Foroutan, Desmond Elliott, Anders Søgaard

Large Language Models (LLMs) store and retrieve vast amounts of factual
knowledge acquired during pre-training. Prior research has localized and
identified mechanisms behind knowledge recall; however, it has primarily
focused on English monolingual models. The question of how these processes
generalize to other languages and multilingual LLMs remains unexplored. In this
paper, we address this gap by conducting a comprehensive analysis of two highly
multilingual LLMs. We assess the extent to which previously identified
components and mechanisms of factual recall in English apply to a multilingual
context. Then, we examine when language plays a role in the recall process,
uncovering evidence of language-independent and language-dependent mechanisms.

摘要：大型語言模型 (LLM) 會儲存並檢索在預先訓練期間取得的大量事實知識。先前的研究已將知識回憶背後的機制本地化並加以識別；然而，其主要關注點在於英文單語模型。這些程序如何推廣到其他語言和多語種 LLM 的問題仍未探討。在本文中，我們透過對兩個高度多語種的 LLM 進行全面分析來解決此差距。我們評估先前識別的英文事實回憶組成部分和機制在多語種環境中適用的程度。接著，我們檢視語言在回憶程序中扮演的角色，揭露語言無關和語言相關機制的證據。

##### **Fine-Tuning Pre-trained Language Models for Robust Causal Representation Learning**
2410.14375v1 by Jialin Yu, Yuxiang Zhou, Yulan He, Nevin L. Zhang, Ricardo Silva

The fine-tuning of pre-trained language models (PLMs) has been shown to be
effective across various domains. By using domain-specific supervised data, the
general-purpose representation derived from PLMs can be transformed into a
domain-specific representation. However, these methods often fail to generalize
to out-of-domain (OOD) data due to their reliance on non-causal
representations, often described as spurious features. Existing methods either
make use of adjustments with strong assumptions about lack of hidden common
causes, or mitigate the effect of spurious features using multi-domain data. In
this work, we investigate how fine-tuned pre-trained language models aid
generalizability from single-domain scenarios under mild assumptions, targeting
more general and practical real-world scenarios. We show that a robust
representation can be derived through a so-called causal front-door adjustment,
based on a decomposition assumption, using fine-tuned representations as a
source of data augmentation. Comprehensive experiments in both synthetic and
real-world settings demonstrate the superior generalizability of the proposed
method compared to existing approaches. Our work thus sheds light on the domain
generalization problem by introducing links between fine-tuning and causal
mechanisms into representation learning.

摘要：預訓練語言模型 (PLM) 的微調已被證明在各種領域中有效。透過使用特定於領域的監督資料，從 PLM 衍生的通用表示可以轉換為特定於領域的表示。然而，這些方法通常無法概括為非領域 (OOD) 資料，因為它們依賴於非因果表示，通常被描述為虛假特徵。現有方法會利用對隱藏共同原因缺乏的強假設進行調整，或使用多領域資料來減輕虛假特徵的影響。在這項工作中，我們探討微調預訓練語言模型如何幫助在溫和假設下從單一領域場景進行概括，針對更一般且實用的真實世界場景。我們展示了可以透過所謂的因果前門調整來衍生穩健的表示，這基於分解假設，使用微調表示作為資料擴充的來源。在合成和真實世界設定中的全面實驗證明了所提出的方法與現有方法相比具有優越的概括能力。因此，我們的研究透過在表示學習中引入微調和因果機制之間的連結，來闡明領域概括問題。

##### **CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic**
2410.14368v1 by Huaiyuan Yao, Longchao Da, Vishnu Nandam, Justin Turnau, Zhiwei Liu, Linsey Pang, Hua Wei

The integration of autonomous vehicles into urban traffic has great potential
to improve efficiency by reducing congestion and optimizing traffic flow
systematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent
LLMs), a framework designed to address the mixed-autonomy traffic problem by
collaboration among autonomous vehicles to optimize traffic flow. CoMAL is
built upon large language models, operating in an interactive traffic
simulation environment. It utilizes a Perception Module to observe surrounding
agents and a Memory Module to store strategies for each agent. The overall
workflow includes a Collaboration Module that encourages autonomous vehicles to
discuss the effective strategy and allocate roles, a reasoning engine to
determine optimal behaviors based on assigned roles, and an Execution Module
that controls vehicle actions using a hybrid approach combining rule-based
models. Experimental results demonstrate that CoMAL achieves superior
performance on the Flow benchmark. Additionally, we evaluate the impact of
different language models and compare our framework with reinforcement learning
approaches. It highlights the strong cooperative capability of LLM agents and
presents a promising solution to the mixed-autonomy traffic challenge. The code
is available at https://github.com/Hyan-Yao/CoMAL.

摘要：自動駕駛車輛整合到都市交通中，有很大的潛力可以透過減少擁堵和系統性地最佳化交通流量來提升效率。在本文中，我們介紹 CoMAL（協作多重代理 LLM），一個旨在透過自動駕駛車輛之間的協作來最佳化交通流量，以解決混合自動化交通問題的架構。CoMAL 建立在大型語言模型之上，在互動式交通模擬環境中運作。它利用感知模組來觀察周圍的代理，並利用記憶模組來儲存每位代理的策略。整體工作流程包含一個協作模組，鼓勵自動駕駛車輛討論有效的策略並分配角色；一個推理引擎，根據分配的角色來決定最佳行為；以及一個執行模組，使用結合規則式模型的混合方法來控制車輛動作。實驗結果顯示，CoMAL 在 Flow 基準測試中取得優異的表現。此外，我們評估不同語言模型的影響，並將我們的架構與強化學習方法進行比較。它突顯了 LLM 代理強大的合作能力，並為混合自動化交通挑戰提供了有希望的解決方案。程式碼可在 https://github.com/Hyan-Yao/CoMAL 取得。

##### **Efficiently Computing Susceptibility to Context in Language Models**
2410.14361v1 by Tianyu Liu, Kevin Du, Mrinmaya Sachan, Ryan Cotterell

One strength of modern language models is their ability to incorporate
information from a user-input context when answering queries. However, they are
not equally sensitive to the subtle changes to that context. To quantify this,
Du et al. (2024) gives an information-theoretic metric to measure such
sensitivity. Their metric, susceptibility, is defined as the degree to which
contexts can influence a model's response to a query at a distributional level.
However, exactly computing susceptibility is difficult and, thus, Du et al.
(2024) falls back on a Monte Carlo approximation. Due to the large number of
samples required, the Monte Carlo approximation is inefficient in practice. As
a faster alternative, we propose Fisher susceptibility, an efficient method to
estimate the susceptibility based on Fisher information. Empirically, we
validate that Fisher susceptibility is comparable to Monte Carlo estimated
susceptibility across a diverse set of query domains despite its being
$70\times$ faster. Exploiting the improved efficiency, we apply Fisher
susceptibility to analyze factors affecting the susceptibility of language
models. We observe that larger models are as susceptible as smaller ones.

摘要：語言模型的一項優勢是，它們在回答查詢時，能夠納入使用者輸入脈絡中的資訊。然而，它們對於脈絡中的細微變化並非同樣敏感。為了量化這一點，Du et al. (2024) 給出了一種資訊理論量度來測量這種敏感度。他們的量度，即敏感度，定義為脈絡在分配層級上影響模型對查詢回應的程度。然而，精確計算敏感度很困難，因此 Du et al. (2024) 退而求其次，採用蒙地卡羅近似法。由於需要大量的樣本，蒙地卡羅近似法在實務上效率不彰。作為一個較快的替代方案，我們提出 Fisher 敏感度，一種基於 Fisher 資訊來估計敏感度的有效方法。根據經驗，我們驗證了 Fisher 敏感度與蒙地卡羅估計的敏感度相當，儘管它快了 $70\times$，但適用於各種不同的查詢領域。利用這種提升的效率，我們應用 Fisher 敏感度來分析影響語言模型敏感度的因素。我們觀察到，較大的模型與較小的模型一樣敏感。

##### **A Scientific Machine Learning Approach for Predicting and Forecasting Battery Degradation in Electric Vehicles**
2410.14347v1 by Sharv Murgai, Hrishikesh Bhagwat, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat

Carbon emissions are rising at an alarming rate, posing a significant threat
to global efforts to mitigate climate change. Electric vehicles have emerged as
a promising solution, but their reliance on lithium-ion batteries introduces
the critical challenge of battery degradation. Accurate prediction and
forecasting of battery degradation over both short and long time spans are
essential for optimizing performance, extending battery life, and ensuring
effective long-term energy management. This directly influences the
reliability, safety, and sustainability of EVs, supporting their widespread
adoption and aligning with key UN SDGs. In this paper, we present a novel
approach to the prediction and long-term forecasting of battery degradation
using Scientific Machine Learning framework which integrates domain knowledge
with neural networks, offering more interpretable and scientifically grounded
solutions for both predicting short-term battery health and forecasting
degradation over extended periods. This hybrid approach captures both known and
unknown degradation dynamics, improving predictive accuracy while reducing data
requirements. We incorporate ground-truth data to inform our models, ensuring
that both the predictions and forecasts reflect practical conditions. The model
achieved MSE of 9.90 with the UDE and 11.55 with the NeuralODE, in experimental
data, a loss of 1.6986 with the UDE, and a MSE of 2.49 in the NeuralODE,
demonstrating the enhanced precision of our approach. This integration of
data-driven insights with SciML's strengths in interpretability and scalability
allows for robust battery management. By enhancing battery longevity and
minimizing waste, our approach contributes to the sustainability of energy
systems and accelerates the global transition toward cleaner, more responsible
energy solutions, aligning with the UN's SDG agenda.

摘要：碳排放量正以驚人的速度上升，對全球減緩氣候變遷的努力構成重大威脅。電動車已成為一個有希望的解決方案，但它們依賴鋰離子電池，引入了電池劣化這個嚴峻的挑戰。準確預測和預測電池在短期和長期內的劣化對於最佳化效能、延長電池壽命以及確保有效的長期能源管理至關重要。這直接影響電動車的可靠性、安全性與永續性，支持它們的廣泛採用，並與聯合國永續發展目標保持一致。在本文中，我們提出了一種新的方法，使用科學機器學習框架來預測和長期預測電池劣化，該框架將領域知識與神經網路整合在一起，為預測短期電池健康狀況和預測長期劣化提供更具可解釋性和科學依據的解決方案。這種混合方法捕捉已知和未知的劣化動態，在減少資料需求的同時提高預測準確度。我們納入真實資料來告知我們的模型，確保預測和預測都反映實際情況。該模型在實驗資料中使用 UDE 達到 9.90 的 MSE，使用 NeuralODE 達到 11.55，使用 UDE 損失 1.6986，使用 NeuralODE 達到 2.49 的 MSE，證明了我們方法的精確度有所提升。將資料驅動的洞察與 SciML 在可解釋性和可擴充性方面的優勢相結合，可以實現強大的電池管理。透過提升電池壽命和減少浪費，我們的做法有助於能源系統永續發展，並加速全球朝向更清潔、更負責任的能源解決方案過渡，與聯合國永續發展目標議程保持一致。

##### **Critical Questions Generation: Motivation and Challenges**
2410.14335v1 by Blanca Calvo Figueras, Rodrigo Agerri

The development of Large Language Models (LLMs) has brought impressive
performances on mitigation strategies against misinformation, such as
counterargument generation. However, LLMs are still seriously hindered by
outdated knowledge and by their tendency to generate hallucinated content. In
order to circumvent these issues, we propose a new task, namely, Critical
Questions Generation, consisting of processing an argumentative text to
generate the critical questions (CQs) raised by it. In argumentation theory CQs
are tools designed to lay bare the blind spots of an argument by pointing at
the information it could be missing. Thus, instead of trying to deploy LLMs to
produce knowledgeable and relevant counterarguments, we use them to question
arguments, without requiring any external knowledge. Research on CQs Generation
using LLMs requires a reference dataset for large scale experimentation. Thus,
in this work we investigate two complementary methods to create such a
resource: (i) instantiating CQs templates as defined by Walton's argumentation
theory and (ii), using LLMs as CQs generators. By doing so, we contribute with
a procedure to establish what is a valid CQ and conclude that, while LLMs are
reasonable CQ generators, they still have a wide margin for improvement in this
task.

摘要：大型語言模型 (LLM) 的發展在對抗錯誤訊息的緩解策略上帶來令人印象深刻的表現，例如反論生成。然而，LLM 仍受到過時知識和生成幻覺內容的傾向嚴重阻礙。為了迴避這些問題，我們提出一個新任務，即關鍵問題生成，包括處理論證文字以產生它提出的關鍵問題 (CQ)。在論證理論中，CQ 是旨在透過指出論證可能遺漏的資訊來揭露論證盲點的工具。因此，我們不試圖部署 LLM 來產生知識淵博且相關的反論，而是使用它們來質疑論證，而不需要任何外部知識。使用 LLM 進行 CQ 生成的研究需要一個參考資料集進行大規模實驗。因此，在這項工作中，我們探討了兩種互補的方法來建立這樣的資源：(i) 將 CQ 模板實例化為 Walton 的論證理論所定義的模板，以及 (ii) 使用 LLM 作為 CQ 生成器。透過這樣做，我們為建立什麼是有效 CQ 的程序做出貢獻，並得出結論，雖然 LLM 是合理的 CQ 生成器，但它們在這個任務中仍有很大的改進空間。

##### **LoGU: Long-form Generation with Uncertainty Expressions**
2410.14309v1 by Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, Deqing Yang

While Large Language Models (LLMs) demonstrate impressive capabilities, they
still struggle with generating factually incorrect content (i.e.,
hallucinations). A promising approach to mitigate this issue is enabling models
to express uncertainty when unsure. Previous research on uncertainty modeling
has primarily focused on short-form QA, but realworld applications often
require much longer responses. In this work, we introduce the task of Long-form
Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty
Suppression, where models hesitate to express uncertainty, and Uncertainty
Misalignment, where models convey uncertainty inaccurately. To tackle these
challenges, we propose a refinement-based data collection framework and a
two-stage training pipeline. Our framework adopts a divide-and-conquer
strategy, refining uncertainty based on atomic claims. The collected data are
then used in training through supervised fine-tuning (SFT) and direct
preference optimization (DPO) to enhance uncertainty expression. Extensive
experiments on three long-form instruction following datasets show that our
method significantly improves accuracy, reduces hallucinations, and maintains
the comprehensiveness of responses.

摘要：儘管大型語言模型 (LLM) 展現出令人印象深刻的能力，它們在產生事實不正確的內容（即幻覺）方面仍有困難。減輕此問題的一個有前途的方法是讓模型在不確定時表達不確定性。先前關於不確定性建模的研究主要集中於簡短問答，但現實世界的應用通常需要更長的回應。在這項工作中，我們引入了不確定性長篇生成 (LoGU) 的任務。我們找出兩個關鍵挑戰：不確定性抑制，模型猶豫表達不確定性，以及不確定性錯位，模型不準確地傳達不確定性。為了應對這些挑戰，我們提出了一個基於細化的資料收集架構和一個兩階段訓練管道。我們的架構採用分而治之的策略，根據原子陳述細化不確定性。然後將收集到的資料用於透過監督微調 (SFT) 和直接偏好最佳化 (DPO) 進行訓練，以增強不確定性表達。在三個長篇指令遵循資料集上的廣泛實驗顯示，我們的模型顯著提高了準確性，減少了幻覺，並維持了回應的全面性。

##### **SwaQuAD-24: QA Benchmark Dataset in Swahili**
2410.14289v1 by Alfred Malengo Kondoro

This paper proposes the creation of a Swahili Question Answering (QA)
benchmark dataset, aimed at addressing the underrepresentation of Swahili in
natural language processing (NLP). Drawing from established benchmarks like
SQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing
high-quality, annotated question-answer pairs that capture the linguistic
diversity and complexity of Swahili. The dataset is designed to support a
variety of applications, including machine translation, information retrieval,
and social services like healthcare chatbots. Ethical considerations, such as
data privacy, bias mitigation, and inclusivity, are central to the dataset
development. Additionally, the paper outlines future expansion plans to include
domain-specific content, multimodal integration, and broader crowdsourcing
efforts. The Swahili QA dataset aims to foster technological innovation in East
Africa and provide an essential resource for NLP research and applications in
low-resource languages.

摘要：本文提議建立史瓦希里語問答 (QA) 基準資料集，旨在解決自然語言處理 (NLP) 中史瓦希里語的代表性不足問題。從 SQuAD、GLUE、KenSwQuAD 和 KLUE 等既有基準中汲取靈感，該資料集將專注於提供高品質、註解的問答對，以捕捉史瓦希里語的語言多樣性和複雜性。該資料集旨在支援各種應用，包括機器翻譯、資訊檢索和醫療聊天機器人等社會服務。資料集開發的重點是道德考量，例如資料隱私、減輕偏差和包容性。此外，本文概述了未來擴充計畫，包括納入特定領域的內容、多模態整合和更廣泛的群眾外包工作。史瓦希里語問答資料集旨在促進東非的技術創新，並為低資源語言的 NLP 研究和應用提供重要的資源。

##### **EcomEdit: An Automated E-commerce Knowledge Editing Framework for Enhanced Product and Purchase Intention Understanding**
2410.14276v1 by Ching Ming Samuel Lau, Weiqi Wang, Haochen Shi, Baixuan Xu, Jiaxin Bai, Yangqiu Song

Knowledge Editing (KE) aims to correct and update factual information in
Large Language Models (LLMs) to ensure accuracy and relevance without
computationally expensive fine-tuning. Though it has been proven effective in
several domains, limited work has focused on its application within the
e-commerce sector. However, there are naturally occurring scenarios that make
KE necessary in this domain, such as the timely updating of product features
and trending purchase intentions by customers, which necessitate further
exploration. In this paper, we pioneer the application of KE in the e-commerce
domain by presenting ECOMEDIT, an automated e-commerce knowledge editing
framework tailored for e-commerce-related knowledge and tasks. Our framework
leverages more powerful LLMs as judges to enable automatic knowledge conflict
detection and incorporates conceptualization to enhance the semantic coverage
of the knowledge to be edited. Through extensive experiments, we demonstrate
the effectiveness of ECOMEDIT in improving LLMs' understanding of product
descriptions and purchase intentions. We also show that LLMs, after our
editing, can achieve stronger performance on downstream e-commerce tasks.

摘要：知識編輯 (KE) 旨在修正和更新大型語言模型 (LLM) 中的事實資訊，以確保準確性和相關性，同時避免計算成本高昂的微調。儘管已證明它在多個領域中有效，但專注於其在電子商務領域中應用的工作有限。然而，在這個領域中自然會出現需要 KE 的場景，例如及時更新產品功能和客戶的趨勢購買意圖，這需要進一步探索。在本文中，我們率先在電子商務領域應用 KE，提出 ECOMEDIT，一個針對電子商務相關知識和任務量身打造的自動化電子商務知識編輯框架。我們的框架利用更強大的 LLM 作為判斷者，以實現自動知識衝突檢測，並結合概念化以增強待編輯知識的語義涵蓋範圍。透過廣泛的實驗，我們證明了 ECOMEDIT 在改善 LLM 對產品描述和購買意圖的理解方面的有效性。我們還表明，經過我們的編輯後，LLM 可以在下游電子商務任務中實現更強的效能。

##### **REEF: Representation Encoding Fingerprints for Large Language Models**
2410.14273v1 by Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, Jing Shao

Protecting the intellectual property of open-source Large Language Models
(LLMs) is very important, because training LLMs costs extensive computational
resources and data. Therefore, model owners and third parties need to identify
whether a suspect model is a subsequent development of the victim model. To
this end, we propose a training-free REEF to identify the relationship between
the suspect and victim models from the perspective of LLMs' feature
representations. Specifically, REEF computes and compares the centered kernel
alignment similarity between the representations of a suspect model and a
victim model on the same samples. This training-free REEF does not impair the
model's general capabilities and is robust to sequential fine-tuning, pruning,
model merging, and permutations. In this way, REEF provides a simple and
effective way for third parties and models' owners to protect LLMs'
intellectual property together. The code is available at
https://github.com/tmylla/REEF.

摘要：保護開源大型語言模型 (LLM) 的智慧財產權非常重要，因為訓練 LLM 需要大量的運算資源和資料。因此，模型所有者和第三方需要辨識可疑模型是否是受害模型的後續開發。為此，我們提出一個無需訓練的 REEF，從 LLM 特徵表示的角度來辨識可疑模型和受害模型之間的關係。具體來說，REEF 計算並比較可疑模型和受害模型在相同樣本上的中心化核對齊相似性。這個無需訓練的 REEF 沒有損害模型的一般功能，並且對於循序漸進的微調、剪枝、模型合併和排列組合具有穩健性。這樣，REEF 為第三方和模型所有者提供了一種簡單且有效的方法，可以共同保護 LLM 的智慧財產權。程式碼可於 https://github.com/tmylla/REEF 取得。

##### **MoDification: Mixture of Depths Made Easy**
2410.14268v1 by Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song

Long-context efficiency has recently become a trending topic in serving large
language models (LLMs). And mixture of depths (MoD) is proposed as a perfect
fit to bring down both latency and memory. In this paper, however, we discover
that MoD can barely transform existing LLMs without costly training over an
extensive number of tokens. To enable the transformations from any LLMs to MoD
ones, we showcase top-k operator in MoD should be promoted to threshold-p
operator, and refinement to architecture and data should also be crafted along.
All these designs form our method termed MoDification. Through a comprehensive
set of experiments covering model scales from 3B to 70B, we exhibit
MoDification strikes an excellent balance between efficiency and effectiveness.
MoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in
memory compared to original LLMs especially in long-context applications.

摘要：最近，長文本效率已成為服務大型語言模型 (LLM) 的熱門話題。深度混合 (MoD) 被提議為降低延遲和記憶體的完美解決方案。然而，在本文中，我們發現 MoD 幾乎無法轉換現有的 LLM，而無需在大量 token 上進行代價高昂的訓練。為了實現從任何 LLM 轉換為 MoD，我們展示了 MoD 中的 top-k 算子應提升為閾值-p 算子，並且還應調整架構和數據。所有這些設計構成了我們稱為 MoDification 的方法。透過涵蓋從 3B 到 70B 的模型規模的全面實驗，我們展示了 MoDification 在效率和有效性之間取得了極佳的平衡。與原始 LLM 相比，MoDification 可以實現延遲速度最高提升 ~1.2 倍，記憶體減少 ~1.8 倍，尤其是在長文本應用中。

##### **Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation**
2410.14262v1 by Edward, Kwartler, Matthew Berman, Alan Aqrawi

This study explores the ability of Large Language Model (LLM) agents to
detect and correct hallucinations in AI-generated content. A primary agent was
tasked with creating a blog about a fictional Danish artist named Flipfloppidy,
which was then reviewed by another agent for factual inaccuracies. Most LLMs
hallucinated the existence of this artist. Across 4,900 test runs involving
various combinations of primary and reviewing agents, advanced AI models such
as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in
identifying hallucinations and successfully revised outputs in 85% to 100% of
cases following feedback. These findings underscore the potential of advanced
AI models to significantly enhance the accuracy and reliability of generated
content, providing a promising approach to improving AI workflow orchestration.

摘要：這項研究探討大型語言模型 (LLM) 代理偵測和修正 AI 生成的內容中幻覺的能力。主要代理被賦予建立一個關於虛構丹麥藝術家 Flipfloppidy 的部落格，然後由另一個代理審查事實不正確之處。大多數 LLM 幻覺了這位藝術家的存在。在涉及主要和審查代理的各種組合的 4,900 次測試運行中，進階 AI 模型（例如 Llama3-70b 和 GPT-4 變體）在辨識幻覺方面表現出近乎完美的準確性，並在 85% 到 100% 的案例中根據回饋成功地修改了輸出。這些發現強調了進階 AI 模型大幅提升生成內容的準確性和可靠性的潛力，提供了一種有希望的方法來改善 AI 工作流程編排。

##### **Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement**
2410.14259v1 by Zihao Cheng, Li Zhou, Feng Jiang, Benyou Wang, Haizhou Li

The rapid development of large language models (LLMs), like ChatGPT, has
resulted in the widespread presence of LLM-generated content on social media
platforms, raising concerns about misinformation, data biases, and privacy
violations, which can undermine trust in online discourse. While detecting
LLM-generated content is crucial for mitigating these risks, current methods
often focus on binary classification, failing to address the complexities of
real-world scenarios like human-AI collaboration. To move beyond binary
classification and address these challenges, we propose a new paradigm for
detecting LLM-generated content. This approach introduces two novel tasks: LLM
Role Recognition (LLM-RR), a multi-class classification task that identifies
specific roles of LLM in content generation, and LLM Influence Measurement
(LLM-IM), a regression task that quantifies the extent of LLM involvement in
content creation. To support these tasks, we propose LLMDetect, a benchmark
designed to evaluate detectors' performance on these new tasks. LLMDetect
includes the Hybrid News Detection Corpus (HNDC) for training detectors, as
well as DetectEval, a comprehensive evaluation suite that considers five
distinct cross-context variations and multi-intensity variations within the
same LLM role. This allows for a thorough assessment of detectors'
generalization and robustness across diverse contexts. Our empirical validation
of 10 baseline detection methods demonstrates that fine-tuned PLM-based models
consistently outperform others on both tasks, while advanced LLMs face
challenges in accurately detecting their own generated content. Our
experimental results and analysis offer insights for developing more effective
detection models for LLM-generated content. This research enhances the
understanding of LLM-generated content and establishes a foundation for more
nuanced detection methodologies.

摘要：大型語言模型 (LLM) 如 ChatGPT 的快速發展，導致 LLM 生成的內容廣泛出現在社群媒體平台上，引發了關於錯誤資訊、資料偏差和隱私侵犯的疑慮，這可能破壞線上討論的信任感。儘管偵測 LLM 生成的內容對於減輕這些風險至關重要，但目前的方法通常著重於二元分類，未能解決現實世界情境（例如人類與 AI 合作）的複雜性。為了超越二元分類並解決這些挑戰，我們提出了一個偵測 LLM 生成的內容的新範例。此方法引入了兩個新任務：LLM 角色辨識 (LLM-RR)，一個多類別分類任務，用於識別 LLM 在內容產生中的特定角色；以及 LLM 影響力測量 (LLM-IM)，一個回歸任務，用於量化 LLM 在內容創作中的參與程度。為了支援這些任務，我們提出了 LLMDetect，一個基準，用於評估偵測器在這些新任務上的效能。LLMDetect 包含了混合新聞偵測語料庫 (HNDC)，用於訓練偵測器，以及 DetectEval，一個全面的評量套件，它考慮了同一個 LLM 角色中的五種不同的跨情境變化和多強度變化。這允許對偵測器的泛化性和在不同情境中的穩健性進行徹底評估。我們對 10 種基線偵測方法的實證驗證表明，經過微調的基於 PLM 的模型在兩個任務上都持續優於其他模型，而進階的 LLM 在準確偵測其自己產生的內容時面臨挑戰。我們的實驗結果和分析提供了見解，用於開發更有效的 LLM 生成的內容偵測模型。這項研究增強了對 LLM 生成的內容的理解，並為更細緻的偵測方法奠定了基礎。

##### **Revisiting SLO and Goodput Metrics in LLM Serving**
2410.14257v1 by Zhibin Wang, Shipeng Li, Yuhang Zhou, Xue Li, Rong Gu, Nguyen Cam-Tu, Chen Tian, Sheng Zhong

Large language models (LLMs) have achieved remarkable performance and are
widely deployed in various applications, while the serving of LLM inference has
raised concerns about user experience and serving throughput. Accordingly,
service level objectives (SLOs) and goodput-the number of requests that meet
SLOs per second-are introduced to evaluate the performance of LLM serving.
However, existing metrics fail to capture the nature of user experience. We
observe two ridiculous phenomena in existing metrics: 1) delaying token
delivery can smooth the tail time between tokens (tail TBT) of a request and 2)
dropping the request that fails to meet the SLOs midway can improve goodput.
  In this paper, we revisit SLO and goodput metrics in LLM serving and propose
a unified metric framework smooth goodput including SLOs and goodput to reflect
the nature of user experience in LLM serving. The framework can adapt to
specific goals of different tasks by setting parameters. We re-evaluate the
performance of different LLM serving systems under multiple workloads based on
this unified framework and provide possible directions for future optimization
of existing strategies. We hope that this framework can provide a unified
standard for evaluating LLM serving and foster researches in the field of LLM
serving optimization to move in a cohesive direction.

摘要：大型語言模型 (LLM) 已取得卓越的效能，並廣泛部署於各種應用程式中，而 LLM 推論的服務已引發對使用者體驗和服務傳輸量的疑慮。因此，服務等級目標 (SLO) 和傳輸量（每秒符合 SLO 的請求數量）被引入以評估 LLM 服務的效能。然而，現有的指標無法捕捉使用者體驗的本質。我們在現有的指標中觀察到兩個荒謬的現象：1) 延遲令牌傳遞可以平滑請求的令牌之間的尾端時間 (tail TBT)，2) 放棄無法滿足 SLO 的請求可以改善傳輸量。在本文中，我們重新檢視 LLM 服務中的 SLO 和傳輸量指標，並提出一個統一的指標架構平滑傳輸量，包括 SLO 和傳輸量，以反映 LLM 服務中使用者體驗的本質。該架構可以透過設定參數來適應不同任務的特定目標。我們根據這個統一的架構，在多個工作負載下重新評估不同 LLM 服務系統的效能，並為現有策略的未來最佳化提供可能的方針。我們希望這個架構可以為評估 LLM 服務提供一個統一的標準，並促進 LLM 服務最佳化領域的研究朝向一個一致的方向前進。

##### **Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas**
2410.14255v1 by Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan

Scientific innovation is pivotal for humanity, and harnessing large language
models (LLMs) to generate research ideas could transform discovery. However,
existing LLMs often produce simplistic and repetitive suggestions due to their
limited ability in acquiring external knowledge for innovation. To address this
problem, we introduce an enhanced planning and search methodology designed to
boost the creative potential of LLM-based systems. Our approach involves an
iterative process to purposely plan the retrieval of external knowledge,
progressively enriching the idea generation with broader and deeper insights.
Validation through automated and human assessments indicates that our framework
substantially elevates the quality of generated ideas, particularly in novelty
and diversity. The number of unique novel ideas produced by our framework is
3.4 times higher than without it. Moreover, our method outperforms the current
state-of-the-art, generating at least 2.5 times more top-rated ideas based on
170 seed papers in a Swiss Tournament evaluation.

摘要：科學創新對人類至關重要，利用大型語言模型 (LLM) 來產生研究想法可以轉化發現。然而，現有的 LLM 由於獲取外部知識以進行創新的能力有限，因此通常會產生簡化且重複的建議。為了解決這個問題，我們引入了一種增強的規劃和搜尋方法，旨在提升基於 LLM 的系統的創造潛力。我們的做法包括一個反覆的過程，以有目的地規劃外部知識的擷取，逐步用更廣泛和更深入的見解豐富想法的產生。透過自動化和人工評估驗證表明，我們的框架大幅提升了所產生想法的品質，特別是在新穎性和多樣性方面。我們的框架產生的獨特新穎想法數量比沒有框架的情況高出 3.4 倍。此外，我們的模型優於目前的最新技術，根據瑞士錦標賽評估中的 170 篇種子論文，至少產生了 2.5 倍以上的頂級想法。

##### **Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation**
2410.14251v1 by Shuo Tang, Xianghe Pang, Zexi Liu, Bohan Tang, Rui Ye, Xiaowen Dong, Yanfeng Wang, Siheng Chen

Post-training is essential for enabling large language models (LLMs) to
follow human instructions. Inspired by the recent success of using LLMs to
simulate human society, we leverage multi-agent simulation to automatically
generate diverse text-based scenarios, capturing a wide range of real-world
human needs. We propose MATRIX, a multi-agent simulator that creates realistic
and scalable scenarios. Leveraging these outputs, we introduce a novel
scenario-driven instruction generator MATRIX-Gen for controllable and highly
realistic data synthesis. Extensive experiments demonstrate that our framework
effectively generates both general and domain-specific data. Notably, on
AlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on
datasets synthesized by MATRIX-Gen with just 20K instruction-response pairs,
outperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M
pairs; see our project at https://github.com/ShuoTang123/MATRIX-Gen.

摘要：訓練後對於大型語言模型 (LLM) 來說非常重要，可以讓它們遵循人類的指示。受到最近使用 LLM 模擬人類社會的成功啟發，我們利用多重代理模擬自動產生多樣化的基於文字的場景，捕捉廣泛的真實世界人類需求。我們建議使用 MATRIX，一種多重代理模擬器，可以建立寫實且具備擴充性的場景。利用這些輸出，我們介紹一個新的場景驅動式指示產生器 MATRIX-Gen，用於可控且高度寫實的資料合成。廣泛的實驗證明我們的架構有效地產生一般性和特定領域的資料。特別是在 AlpacaEval 2 和 Arena-Hard 基準上，Llama-3-8B-Base 僅使用 20K 指示回應對，在 MATRIX-Gen 合成的資料集上進行後續訓練，其效能優於 Meta 的 Llama-3-8B-Instruct 模型，而後者是在超過 10M 對上進行訓練；請參閱我們的專案 https://github.com/ShuoTang123/MATRIX-Gen。

##### **Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models**
2410.14248v1 by Olga Loginova, Oleksandr Bezrukov, Alexey Kravets

Evaluating Video Language Models (VLMs) is a challenging task. Due to its
transparency, Multiple-Choice Question Answering (MCQA) is widely used to
measure the performance of these models through accuracy. However, existing
MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to
selection bias, when models disproportionately favor certain answer options
based on positional patterns observed during training. In this work, we conduct
a comprehensive empirical analysis of several VLM architectures across major
datasets designed to assess complex video-focused reasoning. We identify where
the bias is most pronounced and demonstrate to what extent model responses
reflect genuine understanding of video content and related questions, as
opposed to reliance on arbitrary patterns or superficial cues, such as answer
position. By decomposing the MCQA task and adapting fairness bias metrics to
VLMs, we introduce a post-processing calibration technique BOLD to balance this
bias. Our results show that reducing selection bias improves not only debiasing
metrics but also overall model performance, including Accuracy and F1 Mean
score. Our method, by suppressing "blind guessing", offers a more cost- and
time-effective approach to mitigating selection bias compared to existing
techniques. This study represents the first focused investigation of selection
bias in video-to-text LLM-powered models.

摘要：評估影片語言模型 (VLM) 是一項具有挑戰性的任務。由於其透明性，多選題問答 (MCQA) 廣泛用於透過準確度來衡量這些模型的效能。然而，現有的 MCQA 基準無法捕捉到 VLM 的完整推理能力，原因在於選擇偏差，當模型基於訓練期間觀察到的位置模式而過度偏好某些答案選項時，就會發生這種情況。在這項研究中，我們對多個 VLM 架構進行全面的經驗分析，這些架構跨越了旨在評估複雜影片焦點推理的主要資料集。我們找出偏差最明顯的地方，並展示模型回應在多大程度上反映了對影片內容和相關問題的真正理解，而不是依賴於任意模式或表面線索，例如答案位置。透過分解 MCQA 任務並將公平性偏差指標調整為 VLM，我們引入後處理校正技術 BOLD 來平衡此偏差。我們的結果顯示，減少選擇偏差不僅改善了去偏差指標，也改善了整體模型效能，包括準確度和 F1 平均分數。我們的技術透過抑制「盲猜」，提供一種與現有技術相比更具成本效益和時間效益的方法來減輕選擇偏差。這項研究代表了對影片到文字 LLM 驅動模型中選擇偏差的首次重點調查。

##### **Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction**
2410.14240v1 by Manuel Brenner, Christoph Jürgen Hemmer, Zahra Monfared, Daniel Durstewitz

Dynamical systems (DS) theory is fundamental for many areas of science and
engineering. It can provide deep insights into the behavior of systems evolving
in time, as typically described by differential or recursive equations. A
common approach to facilitate mathematical tractability and interpretability of
DS models involves decomposing nonlinear DS into multiple linear DS separated
by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are
popular in engineering and a frequent choice in mathematics for analyzing the
topological properties of DS. However, hand-crafting such models is tedious and
only possible for very low-dimensional scenarios, while inferring them from
data usually gives rise to unnecessarily complex representations with very many
linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks
(AL-RNNs) which automatically and robustly produce most parsimonious PWL
representations of DS from time series data, using as few PWL nonlinearities as
possible. AL-RNNs can be efficiently trained with any SOTA algorithm for
dynamical systems reconstruction (DSR), and naturally give rise to a symbolic
encoding of the underlying DS that provably preserves important topological
properties. We show that for the Lorenz and R\"ossler systems, AL-RNNs
discover, in a purely data-driven way, the known topologically minimal PWL
representations of the corresponding chaotic attractors. We further illustrate
on two challenging empirical datasets that interpretable symbolic encodings of
the dynamics can be achieved, tremendously facilitating mathematical and
computational analysis of the underlying systems.

摘要：動力系統 (DS) 理論是許多科學和工程領域的基礎。它可以深入了解系統隨著時間演變的行為，通常用微分或遞迴方程式描述。一種常見的方法是透過將非線性 DS 分解為多個線性 DS，再由切換流形分隔，也就是分段線性 (PWL) 系統，以利於數學上的可追蹤性和 DS 模型的可詮釋性。PWL 模型在工程中很受歡迎，而且是數學中用於分析 DS 拓撲性質的常見選擇。然而，手工打造此類模型很繁瑣，而且僅適用於非常低維度的場景，而從資料中推論它們通常會產生不必要的複雜表示，其中包含非常多線性子區域。在此，我們介紹幾乎線性遞迴神經網路 (AL-RNN)，它可以自動且穩健地產生 DS 的最簡約 PWL 表示，使用最少可能的 PWL 非線性。AL-RNN 可以使用任何 SOTA 演算法進行有效訓練，用於動態系統重建 (DSR)，並自然產生基礎 DS 的符號編碼，可證明保留重要的拓撲性質。我們顯示對於 Lorenz 和 R\"ossler 系統，AL-RNN 以純粹資料驅動的方式發現已知的對應混沌吸引子的拓撲最小 PWL 表示。我們進一步說明了兩個具有挑戰性的經驗資料集，可以達成動態的可詮釋符號編碼，極大地促進了基礎系統的數學和計算分析。

##### **A Novel Method to Metigate Demographic and Expert Bias in ICD Coding with Causal Inference**
2410.14236v1 by Bin Zhang, Junli Wang

ICD(International Classification of Diseases) coding involves assigning ICD
codes to patients visit based on their medical notes. Considering ICD coding as
a multi-label text classification task, researchers have developed
sophisticated methods. Despite progress, these models often suffer from label
imbalance and may develop spurious correlations with demographic factors.
Additionally, while human coders assign ICD codes, the inclusion of irrelevant
information from unrelated experts introduces biases. To combat these issues,
we propose a novel method to mitigate Demographic and Expert biases in ICD
coding through Causal Inference (DECI). We provide a novel causality-based
interpretation in ICD Coding that models make predictions by three distinct
pathways. And based counterfactual reasoning, DECI mitigate demographic and
expert biases. Experimental results show that DECI outperforms state-of-the-art
models, offering a significant advancement in accurate and unbiased ICD coding.

摘要：ICD（國際疾病分類）編碼涉及根據患者的病歷為患者就診分配 ICD 代碼。將 ICD 編碼視為多標籤文本分類任務，研究人員已經開發了精密的模型。儘管取得了進展，但這些模型常常會遭受標籤不平衡的困擾，並且可能會與人口統計因素產生虛假的相關性。此外，雖然人類編碼器會分配 ICD 代碼，但納入來自不相關專家的無關信息會引入偏差。為了解決這些問題，我們提出了一種新方法，通過因果推理 (DECI) 來減輕 ICD 編碼中的人口統計和專家偏差。我們在 ICD 編碼中提供了一個新的基於因果關係的解釋，即模型通過三條不同的途徑進行預測。並基於反事實推理，DECI 減輕了人口統計和專家偏差。實驗結果表明，DECI 優於最先進的模型，在準確且無偏差的 ICD 編碼方面提供了顯著的進步。

##### **Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning**
2410.14235v1 by Gaurav Arora, Srujana Merugu, Shreya Jain, Vaibhav Saxena

Reasoning and linguistic skills form the cornerstone of human intelligence,
facilitating problem-solving and decision-making. Recent advances in Large
Language Models (LLMs) have led to impressive linguistic capabilities and
emergent reasoning behaviors, fueling widespread adoption across application
domains. However, LLMs still struggle with complex reasoning tasks,
highlighting their systemic limitations. In this work, we focus on evaluating
whether LLMs have the requisite representations to reason using two
foundational relationships: "equivalence" and "inheritance". We introduce novel
tasks and benchmarks spanning six languages and observe that current SOTA LLMs
often produce conflicting answers to the same questions across languages in
17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.
To enhance consistency across languages, we propose novel "Compositional
Representations" where tokens are represented as composition of equivalent
tokens across languages, with resulting conflict reduction (up to -4.7%)
indicating benefits of shared LLM representations.

摘要：推理和語言能力是人類智慧的基石，
促進了解決問題和決策制定。大型語言模型 (LLM) 的最新進展帶來了令人印象深刻的語言能力和
新興的推理行為，推動了跨應用領域的廣泛採用。然而，LLM 仍然難以應付複雜的推理任務，
突顯了它們的系統性限制。在這項工作中，我們專注於評估 LLM 是否具有使用兩種
基本關係：「等價」和「繼承」進行推理所需的表徵。我們引入了跨越六種語言的新任務和基準，並觀察到當前的 SOTA LLM
通常在 17.3-57.5% 的情況下對同一問題產生相互矛盾的答案，並在高達 37.2% 的情況下違反繼承約束。
為了增強跨語言的一致性，我們提出了新的「組合表徵」，其中符號被表示為跨語言等價符號的組合，結果衝突減少（最多 -4.7%）
表示共享 LLM 表徵的好處。

##### **Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework**
2410.14231v1 by Zhen Tao, Zhiyu Li, Runyu Chen, Dinghao Xi, Wei Xu

Large language models (LLMs) have transformed human writing by enhancing
grammar correction, content expansion, and stylistic refinement. However, their
widespread use raises concerns about authorship, originality, and ethics, even
potentially threatening scholarly integrity. Existing detection methods, which
mainly rely on single-feature analysis and binary classification, often fail to
effectively identify LLM-generated text in academic contexts. To address these
challenges, we propose a novel Multi-level Fine-grained Detection (MFD)
framework that detects LLM-generated text by integrating low-level structural,
high-level semantic, and deep-level linguistic features, while conducting
sentence-level evaluations of lexicon, grammar, and syntax for comprehensive
analysis. To improve detection of subtle differences in LLM-generated text and
enhance robustness against paraphrasing, we apply two mainstream evasion
techniques to rewrite the text. These variations, along with original texts,
are used to train a text encoder via contrastive learning, extracting
high-level semantic features of sentence to boost detection generalization.
Furthermore, we leverage advanced LLM to analyze the entire text and extract
deep-level linguistic features, enhancing the model's ability to capture
complex patterns and nuances while effectively incorporating contextual
information. Extensive experiments on public datasets show that the MFD model
outperforms existing methods, achieving an MAE of 0.1346 and an accuracy of
88.56%. Our research provides institutions and publishers with an effective
mechanism to detect LLM-generated text, mitigating risks of compromised
authorship. Educators and editors can use the model's predictions to refine
verification and plagiarism prevention protocols, ensuring adherence to
standards.

摘要：大型語言模型 (LLM) 透過增強文法修正、內容擴充和文體潤飾，轉變了人類寫作方式。然而，它們的廣泛使用引發了關於作者身份、原創性與道德的疑慮，甚至潛在威脅到學術的完整性。現有的偵測方法主要依賴於單一特徵分析和二元分類，往往無法在學術脈絡中有效識別 LLM 生成的文字。為了應對這些挑戰，我們提出了一個新穎的多層級細粒度偵測 (MFD) 架構，透過整合低層級結構、高層級語意和深層級語言特徵來偵測 LLM 生成的文字，同時對詞彙、文法和句法進行句子層級的評估，以進行全面的分析。為了改善對 LLM 生成的文字中細微差異的偵測，並增強對改寫的健壯性，我們採用了兩種主流的規避技術來改寫文字。這些變體連同原始文字，用於透過對比學習訓練文字編碼器，提取句子的高層級語意特徵，以提升偵測的概化能力。此外，我們利用先進的 LLM 來分析整個文字並提取深層級語言特徵，增強模型捕捉複雜模式和細微差别的能力，同時有效地納入上下文資訊。在公共資料集上進行的廣泛實驗顯示，MFD 模型優於現有方法，MAE 達到 0.1346，準確率達到 88.56%。我們的研究為機構和出版商提供了偵測 LLM 生成的文字的有效機制，降低了作者身份受損的風險。教育工作者和編輯可以使用模型的預測來改善驗證和預防抄襲的協定，確保遵守標準。

##### **Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model**
2410.14225v1 by Li Yuan, Yi Cai, Junsheng Huang

Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task
that aims to extract entities and their relations from text-image pairs in
social media posts. Existing methods for JMERE require large amounts of labeled
data. However, gathering and annotating fine-grained multimodal data for JMERE
poses significant challenges. Initially, we construct diverse and comprehensive
multimodal few-shot datasets fitted to the original data distribution. To
address the insufficient information in the few-shot setting, we introduce the
\textbf{K}nowledge-\textbf{E}nhanced \textbf{C}ross-modal \textbf{P}rompt
\textbf{M}odel (KECPM) for JMERE. This method can effectively address the
problem of insufficient information in the few-shot setting by guiding a large
language model to generate supplementary background knowledge. Our proposed
method comprises two stages: (1) a knowledge ingestion stage that dynamically
formulates prompts based on semantic similarity guide ChatGPT generating
relevant knowledge and employs self-reflection to refine the knowledge; (2) a
knowledge-enhanced language model stage that merges the auxiliary knowledge
with the original input and utilizes a transformer-based model to align with
JMERE's required output format. We extensively evaluate our approach on a
few-shot dataset derived from the JMERE dataset, demonstrating its superiority
over strong baselines in terms of both micro and macro F$_1$ scores.
Additionally, we present qualitative analyses and case studies to elucidate the
effectiveness of our model.

摘要：<paragraph>聯合多模態實體關係抽取 (JMERE) 是一項具有挑戰性的任務，旨在從社群媒體文章中的文字影像對中抽取實體及其關係。現有的 JMERE 方法需要大量的標籤資料。然而，收集和標註 JMERE 的細粒度多模態資料會帶來重大的挑戰。最初，我們構建了多樣化且全面的多模態少量資料集，以符合原始資料分佈。為了解決少量資料設定中的資訊不足問題，我們引入了\textbf{K}nowledge-\textbf{E}nhanced \textbf{C}ross-modal \textbf{P}rompt \textbf{M}odel (KECPM) for JMERE。此方法可以透過引導大型語言模型產生補充背景知識來有效解決少量資料設定中的資訊不足問題。我們提出的方法包含兩個階段：(1) 知識吸收階段，根據語義相似性指南動態制定提示，引導 ChatGPT 產生相關知識，並利用自我反省來精進知識；(2) 知識增強語言模型階段，將輔助知識與原始輸入合併，並利用基於轉換器的模型與 JMERE 所需的輸出格式對齊。我們廣泛評估了我們的方法在從 JMERE 資料集衍生的少量資料集上的表現，證明其在微觀和巨觀 F$_1$ 分數方面都優於強大的基準。此外，我們提出定性分析和案例研究，以闡明我們模型的有效性。</paragraph>

##### **Formal Explanations for Neuro-Symbolic AI**
2410.14219v1 by Sushmita Paul, Jinqiang Yu, Jip J. Dekker, Alexey Ignatiev, Peter J. Stuckey

Despite the practical success of Artificial Intelligence (AI), current neural
AI algorithms face two significant issues. First, the decisions made by neural
architectures are often prone to bias and brittleness. Second, when a chain of
reasoning is required, neural systems often perform poorly. Neuro-symbolic
artificial intelligence is a promising approach that tackles these (and other)
weaknesses by combining the power of neural perception and symbolic reasoning.
Meanwhile, the success of AI has made it critical to understand its behaviour,
leading to the development of explainable artificial intelligence (XAI). While
neuro-symbolic AI systems have important advantages over purely neural AI, we
still need to explain their actions, which are obscured by the interactions of
the neural and symbolic components. To address the issue, this paper proposes a
formal approach to explaining the decisions of neuro-symbolic systems. The
approach hinges on the use of formal abductive explanations and on solving the
neuro-symbolic explainability problem hierarchically. Namely, it first computes
a formal explanation for the symbolic component of the system, which serves to
identify a subset of the individual parts of neural information that needs to
be explained. This is followed by explaining only those individual neural
inputs, independently of each other, which facilitates succinctness of
hierarchical formal explanations and helps to increase the overall performance
of the approach. Experimental results for a few complex reasoning tasks
demonstrate practical efficiency of the proposed approach, in comparison to
purely neural systems, from the perspective of explanation size, explanation
time, training time, model sizes, and the quality of explanations reported.

摘要：儘管人工智慧 (AI) 在實務上獲得成功，目前的神經網路 AI 演算法面臨兩個重大問題。首先，神經網路架構做出的決策常常容易出現偏差和脆弱性。其次，當需要推理鏈時，神經系統常常表現不佳。神經符號人工智慧是一種有前途的方法，它結合了神經感知和符號推理的力量，來解決這些（和其他）弱點。同時，AI 的成功使得理解其行為變得至關重要，這導致了可解釋人工智慧 (XAI) 的發展。雖然神經符號 AI 系統比純粹的神經網路 AI 具有重要的優勢，但我們仍然需要解釋其行為，而這些行為被神經元和符號組件的交互作用所掩蓋。為了解決這個問題，本文提出了一個正式的方法來解釋神經符號系統的決策。該方法依賴於形式約化解釋的使用，並分層解決神經符號可解釋性問題。具體來說，它首先為系統的符號組件計算一個形式解釋，用於識別需要解釋的神經資訊的個別部分的子集。接著只解釋那些個別的神經輸入，它們彼此獨立，這有助於分層形式解釋的簡潔性，並有助於提高方法的整體效能。幾個複雜推理任務的實驗結果證明了所提出的方法的實務效率，與純粹的神經系統相比，從解釋大小、解釋時間、訓練時間、模型大小和報告的解釋品質的角度來看。

##### **Paths-over-Graph: Knowledge Graph Enpowered Large Language Model Reasoning**
2410.14211v1 by Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang

Large Language Models (LLMs) have achieved impressive results in various
tasks but struggle with hallucination problems and lack of relevant knowledge,
especially in deep complex reasoning and knowledge-intensive tasks. Knowledge
Graphs (KGs), which capture vast amounts of facts in a structured format, offer
a reliable source of knowledge for reasoning. However, existing KG-based LLM
reasoning methods face challenges like handling multi-hop reasoning,
multi-entity questions, and effectively utilizing graph structures. To address
these issues, we propose Paths-over-Graph (PoG), a novel method that enhances
LLM reasoning by integrating knowledge reasoning paths from KGs, improving the
interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and
multi-entity questions through a three-phase dynamic multi-hop path
exploration, which combines the inherent knowledge of LLMs with factual
knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant
information from the graph exploration first and introduces efficient
three-step pruning techniques that incorporate graph structures, LLM prompting,
and a pre-trained language model (e.g., SBERT) to effectively narrow down the
explored candidate paths. This ensures all reasoning paths contain highly
relevant information captured from KGs, making the reasoning faithful and
interpretable in problem-solving. PoG innovatively utilizes graph structure to
prune the irrelevant noise and represents the first method to implement
multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive
experiments on five benchmark KGQA datasets demonstrate PoG outperforms the
state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an
average accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo
surpasses ToG with GPT-4 by up to 23.9%.

摘要：大型語言模型 (LLM) 在各種任務中取得令人印象深刻的成果，但卻難以克服幻覺問題，且缺乏相關知識，尤其是在深入複雜的推理和知識密集型任務中。知識圖譜 (KG) 以結構化格式擷取大量事實，為推理提供可靠的知識來源。然而，現有的基於 KG 的 LLM 推理方法面臨處理多跳推理、多實體問題和有效利用圖形結構等挑戰。為了解決這些問題，我們提出圖形路徑 (PoG)，這是一種創新的方法，透過整合來自 KG 的知識推理路徑來增強 LLM 推理，進而提升 LLM 輸出的可解釋性和真實性。PoG 透過三階段動態多跳路徑探索來處理多跳和多實體問題，將 LLM 的內在知識與來自 KG 的事實知識結合起來。為了提高效率，PoG 首先從圖形探索中修剪不相關的資訊，並引入有效的三步驟修剪技術，結合圖形結構、LLM 提示和預先訓練的語言模型 (例如 SBERT)，以有效縮小探索的候選路徑。這確保所有推理路徑都包含從 KG 中擷取的高度相關資訊，使推理在問題解決中保持真實且可解釋。PoG 創新地利用圖形結構來修剪無關的雜訊，並首次實作在 KG 上針對 LLM 推理任務進行多實體深度路徑偵測的方法。在五個基準 KGQA 資料集上進行的全面實驗證明，PoG 在 GPT-3.5-Turbo 和 GPT-4 上優於最先進的方法 ToG，平均準確度提升 18.9%。值得注意的是，搭載 GPT-3.5-Turbo 的 PoG 比搭載 GPT-4 的 ToG 高出 23.9%。

##### **Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning**
2410.14208v1 by Xiaochuan Li, Zichun Yu, Chenyan Xiong

Synthetic data has been widely used to train large language models, but their
generative nature inevitably introduces noisy, non-informative, and misleading
learning signals. In this paper, we propose Montessori-Instruct, a novel data
synthesis framework that tailors the data synthesis ability of the teacher
language model toward the student language model's learning process.
Specifically, we utilize local data influence of synthetic training data points
on students to characterize students' learning preferences. Then, we train the
teacher model with Direct Preference Optimization (DPO) to generate synthetic
data tailored toward student learning preferences. Experiments with
Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and
MT-Bench demonstrate that Montessori-Instruct significantly outperforms
standard synthesis methods by 18.35\% and 46.24\% relatively. Our method also
beats data synthesized by a stronger teacher model, GPT-4o. Further analysis
confirms the benefits of teacher's learning to generate more influential
training data in the student's improved learning, the advantages of local data
influence in accurately measuring student preferences, and the robustness of
Montessori-Instruct across different student models. Our code and data are
open-sourced at https://github.com/cxcscmu/Montessori-Instruct.

摘要：合成數據已被廣泛用於訓練大型語言模型，但其生成性質不可避免地會引入有雜訊、無意義且具有誤導性的學習訊號。在本文中，我們提出 Montessori-Instruct，一個新穎的數據合成架構，它將教師語言模型的數據合成能力調整為學生語言模型的學習過程。具體來說，我們利用合成訓練數據點對學生的局部數據影響來描述學生的學習偏好。然後，我們使用直接偏好最佳化 (DPO) 訓練教師模型，以生成針對學生學習偏好量身打造的合成數據。在 Alpaca Eval 和 MT-Bench 上使用 Llama3-8B-Instruct（教師）和 Llama3-8B（學生）進行的實驗表明，Montessori-Instruct 明顯優於標準合成方法，分別高出 18.35% 和 46.24%。我們的模型也優於由更強大的教師模型 GPT-4o 合成的數據。進一步的分析證實了教師學習對生成更有影響力的訓練數據以改善學生學習的好處、局部數據影響在準確衡量學生偏好方面的優點，以及 Montessori-Instruct 在不同學生模型中的穩健性。我們的程式碼和數據在 https://github.com/cxcscmu/Montessori-Instruct 開源。

##### **MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations**
2410.14204v1 by Vishal Vivek Saley, Goonjan Saha, Rocktim Jyoti Das, Dinesh Raghu, Mausam

Medical task-oriented dialogue systems can assist doctors by collecting
patient medical history, aiding in diagnosis, or guiding treatment selection,
thereby reducing doctor burnout and expanding access to medical services.
However, doctor-patient dialogue datasets are not readily available, primarily
due to privacy regulations. Moreover, existing datasets lack comprehensive
annotations involving medical slots and their different attributes, such as
symptoms and their onset, progression, and severity. These comprehensive
annotations are crucial for accurate diagnosis. Finally, most existing datasets
are non-English, limiting their utility for the larger research community.
  In response, we introduce MediTOD, a new dataset of doctor-patient dialogues
in English for the medical history-taking task. Collaborating with doctors, we
devise a questionnaire-based labeling scheme tailored to the medical domain.
Then, medical professionals create the dataset with high-quality comprehensive
annotations, capturing medical slots and their attributes. We establish
benchmarks in supervised and few-shot settings on MediTOD for natural language
understanding, policy learning, and natural language generation subtasks,
evaluating models from both TOD and biomedical domains. We make MediTOD
publicly available for future research.

摘要：醫學任務導向對話系統可以協助醫生收集病患病歷、協助診斷或引導治療選擇，從而減少醫生的倦怠感並擴大獲得醫療服務的機會。然而，醫生與病患對話的資料集並不容易取得，這主要是因為隱私法規。此外，現有的資料集缺乏包含醫療插槽及其不同屬性的全面註解，例如症狀及其發作、進展和嚴重程度。這些全面的註解對於準確診斷至關重要。最後，大多數現有的資料集都是非英語的，這限制了它們對更大研究社群的效用。為了解決這個問題，我們引入了 MediTOD，這是一個新的英文醫生與病患對話資料集，用於病歷收集任務。我們與醫生合作，設計了一個針對醫療領域量身打造的基於問卷的標記方案。然後，醫療專業人員使用高品質的全面註解建立資料集，擷取醫療插槽及其屬性。我們在 MediTOD 上建立了監督式和少量樣本設定的基準，用於自然語言理解、策略學習和自然語言生成子任務，評估來自 TOD 和生物醫學領域的模型。我們公開 MediTOD 以供未來研究使用。

##### **Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs**
2410.14202v1 by SeongYeub Chu, JongWoo Kim, Bryan Wong, MunYong Yi

Existing automated essay scoring (AES) has solely relied on essay text
without using explanatory rationales for the scores, thereby forgoing an
opportunity to capture the specific aspects evaluated by rubric indicators in a
fine-grained manner. This paper introduces Rationale-based Multiple Trait
Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates
prompt-engineering-based large language models (LLMs) with a fine-tuning-based
essay scoring model using a smaller large language model (S-LLM). RMTS uses an
LLM-based trait-wise rationale generation system where a separate LLM agent
generates trait-specific rationales based on rubric guidelines, which the
scoring model uses to accurately predict multi-trait scores. Extensive
experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,
show that RMTS significantly outperforms state-of-the-art models and vanilla
S-LLMs in trait-specific scoring. By assisting quantitative assessment with
fine-grained qualitative rationales, RMTS enhances the trait-wise reliability,
providing partial explanations about essays.

摘要：現有的自動化論文評分（AES）僅依賴論文文本，而不使用評分的說明性依據，從而放棄了以細緻的方式擷取評分指標中評估的特定面向的機會。本文介紹了基於依據的多特質評分（RMTS），這是一種多特質論文評分的新方法，它將基於提示工程的大型語言模型（LLM）與使用較小型大型語言模型（S-LLM）的基於微調的論文評分模型整合在一起。RMTS 使用基於 LLM 的特質明智依據生成系統，其中一個獨立的 LLM 代理根據評分準則生成特定特質的依據，評分模型使用這些依據準確預測多特質評分。在基準資料集（包括 ASAP、ASAP++ 和 Feedback Prize）上進行的廣泛實驗表明，RMTS 在特定特質評分方面顯著優於最先進的模型和香草 S-LLM。通過使用細緻的定性依據協助量化評估，RMTS 增強了特質明智的可靠性，提供了關於論文的部分說明。

##### **E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model**
2410.14200v1 by Haoran Lai, Zihang Jiang, Qingsong Yao, Rongsheng Wang, Zhiyang He, Xiaodong Tao, Wei Wei, Weifu Lv, S. Kevin Zhou

The development of 3D medical vision-language models holds significant
potential for disease diagnosis and patient treatment. However, compared to 2D
medical images, 3D medical images, such as CT scans, face challenges related to
limited training data and high dimension, which severely restrict the progress
of 3D medical vision-language models. To address these issues, we collect a
large amount of unlabeled 3D CT data and utilize self-supervised learning to
construct a 3D visual foundation model for extracting 3D visual features. Then,
we apply 3D spatial convolutions to aggregate and project high-level image
features, reducing computational complexity while preserving spatial
information. We also construct two instruction-tuning datasets based on BIMCV-R
and CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates
superior performance compared to existing methods in report generation, visual
question answering, and disease diagnosis. Code and data will be made publicly
available soon.

摘要：3D 醫學視覺語言模型的發展對於疾病診斷和患者治療具有重要的潛力。然而，與 2D 醫學影像相比，3D 醫學影像（例如 CT 掃描）面臨著訓練資料有限和維度高等挑戰，這嚴重限制了 3D 醫學視覺語言模型的進展。為了解決這些問題，我們收集了大量的未標記 3D CT 資料，並利用自監督學習來構建一個 3D 視覺基礎模型以提取 3D 視覺特徵。然後，我們應用 3D 空間卷積來匯總和投影高階影像特徵，在保留空間資訊的同時降低運算複雜度。我們還根據 BIMCV-R 和 CT-RATE 建構了兩個指令調整資料集，以微調 3D 視覺語言模型。與現有方法相比，我們的模型在報告生成、視覺問答和疾病診斷方面表現出優異的效能。程式碼和資料將很快公開。

##### **Supervised Chain of Thought**
2410.14198v1 by Xiang Zhang, Dujian Ding

Large Language Models (LLMs) have revolutionized natural language processing
and hold immense potential for advancing Artificial Intelligence. However, the
core architecture of most mainstream LLMs -- the Transformer -- has inherent
limitations in computational depth, rendering them theoretically incapable of
solving many reasoning tasks that demand increasingly deep computations. Chain
of Thought (CoT) prompting has emerged as a technique to address these
architectural limitations, as evidenced by several theoretical studies. It
offers a promising approach to solving complex reasoning tasks that were
previously beyond the capabilities of these models. Despite its successes, CoT
and its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a
"one-prompt-for-all" approach, using a single prompt structure (e.g., "think
step by step") for a wide range of tasks -- from counting and sorting to
solving mathematical and algorithmic problems. This approach poses significant
challenges for models to generate the correct reasoning steps, as the model
must navigate through a vast prompt template space to find the appropriate
template for each task. In this work, we build upon previous theoretical
analyses of CoT to demonstrate how the one-prompt-for-all approach can
negatively affect the computability of LLMs. We partition the solution search
space into two: the prompt space and the answer space. Our findings show that
task-specific supervision is essential for navigating the prompt space
accurately and achieving optimal performance. Through experiments with
state-of-the-art LLMs, we reveal a gap in reasoning performance when
supervision is applied versus when it is not.

摘要：大型語言模型 (LLM) 徹底改變了自然語言處理，並具備促進人工智慧發展的巨大潛力。然而，大多數主流 LLM 的核心架構（Transformer）在計算深度方面有其內在限制，理論上無法解決許多需要越來越深入計算的推理任務。思維鏈 (CoT) 提示已成為解決這些架構限制的一種技術，這已由幾項理論研究證實。它提供了一個有前途的方法來解決複雜的推理任務，這些任務以前超出了這些模型的能力。儘管取得了成功，CoT 及其變體（例如思維樹、思維圖等）依賴於「一提示適用所有」的方法，對各種任務（從計數和排序到解決數學和演算法問題）使用單一的提示結構（例如，「逐步思考」）。這種方法對模型產生正確的推理步驟構成了重大挑戰，因為模型必須在廣泛的提示範本空間中導航，才能為每個任務找到適當的範本。在這項工作中，我們建立在 CoT 先前的理論分析之上，說明「一提示適用所有」的方法如何對 LLM 的可計算性產生負面影響。我們將解的搜尋空間分為兩部分：提示空間和答案空間。我們的研究結果表明，特定於任務的監督對於準確導航提示空間並實現最佳效能至關重要。透過使用最先進的 LLM 進行實驗，我們揭示了在應用監督與未應用監督時推理效能的差距。

##### **Speciesism in Natural Language Processing Research**
2410.14194v1 by Masashi Takeshita, Rafal Rzepka

Natural Language Processing (NLP) research on AI Safety and social bias in AI
has focused on safety for humans and social bias against human minorities.
However, some AI ethicists have argued that the moral significance of nonhuman
animals has been ignored in AI research. Therefore, the purpose of this study
is to investigate whether there is speciesism, i.e., discrimination against
nonhuman animals, in NLP research. First, we explain why nonhuman animals are
relevant in NLP research. Next, we survey the findings of existing research on
speciesism in NLP researchers, data, and models and further investigate this
problem in this study. The findings of this study suggest that speciesism
exists within researchers, data, and models, respectively. Specifically, our
survey and experiments show that (a) among NLP researchers, even those who
study social bias in AI, do not recognize speciesism or speciesist bias; (b)
among NLP data, speciesist bias is inherent in the data annotated in the
datasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models,
exhibit speciesist bias by default. Finally, we discuss how we can reduce
speciesism in NLP research.

摘要：自然語言處理 (NLP) 在 AI 安全性和 AI 中的社會偏見的研究，一直專注於人類安全和針對人類少數群體的社會偏見。然而，一些 AI 倫理學家認為，在 AI 研究中忽視了非人類動物的道德意義。因此，本研究的目的是調查 NLP 研究中是否存在物種歧視，即對非人類動物的歧視。首先，我們解釋為什麼非人類動物與 NLP 研究相關。接下來，我們調查現有研究對 NLP 研究人員、資料和模型中物種歧視的發現，並進一步在本研究中調查此問題。本研究的發現表明，物種歧視存在於研究人員、資料和模型中。具體來說，我們的調查和實驗表明：(a) 在 NLP 研究人員中，即使是那些研究 AI 中的社會偏見的人，也不承認物種歧視或物種歧視偏見；(b) 在 NLP 資料中，物種歧視偏見存在於用於評估 NLP 模型的資料集中標註的資料中；(c) OpenAI GPT，最近的 NLP 模型，預設表現出物種歧視偏見。最後，我們討論如何減少 NLP 研究中的物種歧視。

##### **MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time**
2410.14184v1 by Mozhi Zhang, Pengyu Wang, Chenkun Tan, Mianqiu Huang, Dong Zhang, Yaqian Zhou, Xipeng Qiu

Large Language Models (LLMs) acquire extensive knowledge and remarkable
abilities from extensive text corpora, making them powerful tools for various
applications. To make LLMs more usable, aligning them with human preferences is
essential. Existing alignment techniques, such as Reinforcement Learning from
Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed
predefined preferences directly within the model's parameters. These methods,
however, often result in a static alignment that can not account for the
diversity of human preferences in practical applications. In response to this
challenge, we propose an effective method, \textbf{MetaAlign}, which aims to
help LLMs dynamically align with various explicit or implicit preferences
specified at inference time. Experimental results show that LLMs optimized on
our meticulously constructed MetaAlign Dataset can effectively align with any
preferences specified at the inference stage, validating the feasibility of
MetaAlign. We hope that our work can provide some insights into the alignment
of language models.

摘要：大型語言模型 (LLM) 從大量的文字語料庫中獲取廣泛的知識和非凡的能力，使其成為各種應用程序的強大工具。為了使 LLM 更易於使用，讓它們與人類偏好保持一致至關重要。現有的對齊技術，例如人類回饋強化學習 (RLHF) 和直接偏好最佳化 (DPO)，通常會將預定義的偏好直接嵌入模型參數中。然而，這些方法通常會導致靜態對齊，無法說明實際應用中人類偏好的多樣性。為了應對這一挑戰，我們提出了一種有效的方法，即 \textbf{MetaAlign}，旨在幫助 LLM 動態地與在推理時指定的不同顯式或隱式偏好保持一致。實驗結果表明，在我們精心構建的 MetaAlign 數據集上優化的 LLM 可以有效地與推理階段指定的任何偏好保持一致，驗證了 MetaAlign 的可行性。我們希望我們的研究成果能為語言模型的對齊提供一些見解。

##### **LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs**
2410.14182v1 by Yujun Zhou, Jingdong Yang, Kehan Guo, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang

Laboratory accidents pose significant risks to human life and property,
underscoring the importance of robust safety protocols. Despite advancements in
safety training, laboratory personnel may still unknowingly engage in unsafe
practices. With the increasing reliance on large language models (LLMs) for
guidance in various fields, including laboratory settings, there is a growing
concern about their reliability in critical safety-related decision-making.
Unlike trained human researchers, LLMs lack formal lab safety education,
raising questions about their ability to provide safe and accurate guidance.
Existing research on LLM trustworthiness primarily focuses on issues such as
ethical compliance, truthfulness, and fairness but fails to fully cover
safety-critical real-world applications, like lab safety. To address this gap,
we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive
evaluation framework based on a new taxonomy aligned with Occupational Safety
and Health Administration (OSHA) protocols. This benchmark includes 765
multiple-choice questions verified by human experts, assessing LLMs and vision
language models (VLMs) performance in lab safety contexts. Our evaluations
demonstrate that while GPT-4o outperforms human participants, it is still prone
to critical errors, highlighting the risks of relying on LLMs in
safety-critical environments. Our findings emphasize the need for specialized
benchmarks to accurately assess the trustworthiness of LLMs in real-world
safety applications.

摘要：實驗室事故對人類生命和財產構成重大風險，
強調了健全安全規程的重要性。儘管安全訓練進步了，
實驗室人員可能仍不知不覺地從事不安全的
實務。隨著對大型語言模型 (LLM) 的依賴日益增加，
在各種領域（包括實驗室環境）中尋求指導，
人們越來越擔心它們在關鍵安全相關決策制定中的可靠性。
與受過訓練的人類研究員不同，LLM 缺乏正規的實驗室安全教育，
引發了人們對它們提供安全且準確指導的能力的質疑。
現有的 LLM 可信度研究主要關注道德合規性、真實性和公平性等問題，
但未能完全涵蓋安全至上的實際應用，例如實驗室安全。為了解決這個差距，
我們提出了實驗室安全基準 (LabSafety Bench)，這是一個全面的
評估框架，基於與職業安全和健康管理局 (OSHA) 規程相符的新分類法。這個基準包括 765
個人類專家驗證的多選題，評估 LLM 和視覺
語言模型 (VLM) 在實驗室安全情境中的表現。我們的評估
表明，儘管 GPT-4o 優於人類參與者，但它仍然容易發生
關鍵錯誤，突顯了在
安全至上的環境中依賴 LLM 的風險。我們的研究結果強調了
需要專門的基準來準確評估 LLM 在實際
安全應用中的可信度。

##### **XForecast: Evaluating Natural Language Explanations for Time Series Forecasting**
2410.14180v1 by Taha Aksu, Chenghao Liu, Amrita Saha, Sarah Tan, Caiming Xiong, Doyen Sahoo

Time series forecasting aids decision-making, especially for stakeholders who
rely on accurate predictions, making it very important to understand and
explain these models to ensure informed decisions. Traditional explainable AI
(XAI) methods, which underline feature or temporal importance, often require
expert knowledge. In contrast, natural language explanations (NLEs) are more
accessible to laypeople. However, evaluating forecast NLEs is difficult due to
the complex causal relationships in time series data. To address this, we
introduce two new performance metrics based on simulatability, assessing how
well a human surrogate can predict model forecasts using the explanations.
Experiments show these metrics differentiate good from poor explanations and
align with human judgments. Utilizing these metrics, we further evaluate the
ability of state-of-the-art large language models (LLMs) to generate
explanations for time series data, finding that numerical reasoning, rather
than model size, is the main factor influencing explanation quality.

摘要：時間序列預測有助於決策制定，特別是對於依賴準確預測的利害關係人，因此了解和解釋這些模型以確保明智的決策非常重要。強調特徵或時間重要性的傳統可解釋 AI (XAI) 方法通常需要專家知識。相比之下，自然語言解釋 (NLE) 更容易讓外行人理解。然而，由於時間序列資料中複雜的因果關係，評估預測 NLE 很困難。為了解決這個問題，我們引入了兩個基於可模擬性的新效能指標，評估人類代理人使用解釋預測模型預測的程度。實驗表明，這些指標可以區分好解釋和差解釋，並且與人類的判斷一致。利用這些指標，我們進一步評估了最先進的大語言模型 (LLM) 為時間序列資料生成解釋的能力，發現數字推理，而不是模型大小，是影響解釋品質的主要因素。

##### **MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems**
2410.14179v1 by Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, Meng Jiang

Multimodal Large Language Models (MLLMs) have demonstrated impressive
abilities across various tasks, including visual question answering and chart
comprehension, yet existing benchmarks for chart-related tasks fall short in
capturing the complexity of real-world multi-chart scenarios. Current
benchmarks primarily focus on single-chart tasks, neglecting the multi-hop
reasoning required to extract and integrate information from multiple charts,
which is essential in practical applications. To fill this gap, we introduce
MultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:
direct question answering, parallel question answering, comparative reasoning,
and sequential reasoning. Our evaluation of a wide range of MLLMs reveals
significant performance gaps compared to humans. These results highlight the
challenges in multi-chart comprehension and the potential of MultiChartQA to
drive advancements in this field. Our code and data are available at
https://github.com/Zivenzhu/Multi-chart-QA

摘要：多模態大型語言模型 (M L L M) 已展現出令人印象深刻的能力，可執行各項任務，包括視覺問答和圖表理解，然而現有的圖表相關任務基準並未完全掌握真實世界多圖表情境的複雜性。目前的基準主要著重於單圖表任務，忽略了從多個圖表中擷取和整合資訊所需的多重跳躍推理，這在實際應用中至關重要。為了填補這個缺口，我們引入了 MultiChartQA，這是一個基準，用於評估 M L L M 在四個關鍵領域的能力：直接問答、平行問答、比較推理和順序推理。我們對各種 M L L M 的評估顯示，與人類相比，效能有顯著差距。這些結果突顯了多圖表理解的挑戰，以及 MultiChartQA 推動此領域進步的潛力。我們的程式碼和資料可在 https://github.com/Zivenzhu/Multi-chart-QA 取得

##### **LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems**
2410.14166v1 by Nan Xu, Xuezhe Ma

Interestingly, LLMs yet struggle with some basic tasks that humans find
trivial to handle, e.g., counting the number of character r's in the word
"strawberry". There are several popular conjectures (e.g., tokenization,
architecture and training data) regarding the reason for deficiency of LLMs in
simple word-based counting problems, sharing the similar belief that such
failure stems from model pretraining hence probably inevitable during
deployment. In this paper, we carefully design multiple evaluation settings to
investigate validity of prevalent conjectures. Meanwhile, we measure
transferability of advanced mathematical and coding reasoning capabilities from
specialized LLMs to simple counting tasks. Although specialized LLMs suffer
from counting problems as well, we find conjectures about inherent deficiency
of LLMs invalid and further seek opportunities to elicit knowledge and
capabilities from LLMs that are beneficial to counting tasks. Compared with
strategies such as finetuning and in-context learning that are commonly adopted
to enhance performance on new or challenging tasks, we show that engaging
reasoning is the most robust and efficient way to help LLMs better perceive
tasks with more accurate responses.
  We hope our conjecture validation design could provide insights into the
study of future critical failure modes of LLMs. Based on challenges in
transferring advanced capabilities to much simpler tasks, we call for more
attention to model capability acquisition and evaluation. We also highlight the
importance of cultivating consciousness of "reasoning before responding" during
model pretraining.

摘要：有趣的是，大型语言模型在一些对人类来说很容易处理的基本任务上仍有困难，例如计算单词“strawberry”中字符r的数量。对于大型语言模型在简单的基于单词的计数问题中表现不佳的原因，有几种流行的猜想（例如，标记化、架构和训练数据），这些猜想都认为这种失败源于模型预训练，因此在部署过程中可能是不可避免的。在本文中，我们仔细设计了多个评估设置来调查流行猜想的有效性。同时，我们测量了高级数学和编码推理能力从专门的大型语言模型到简单计数任务的可迁移性。尽管专门的大型语言模型也存在计数问题，但我们发现关于大型语言模型固有缺陷的猜想是无效的，并进一步寻求机会从大型语言模型中获取对计数任务有益的知识和能力。与通常采用来增强新任务或具有挑战性任务的性能的微调和上下文学习等策略相比，我们表明，参与推理是帮助大型语言模型更好地感知任务并做出更准确响应的最稳健、最有效的方法。我们希望我们的猜想验证设计可以为研究大型语言模型未来的关键故障模式提供见解。基于在将高级能力转移到更简单的任务中遇到的挑战，我们呼吁更多地关注模型能力的获取和评估。我们还强调在模型预训练期间培养“在响应之前推理”意识的重要性。

##### **Automated Genre-Aware Article Scoring and Feedback Using Large Language Models**
2410.14165v1 by Chihang Wang, Yuxin Dong, Zhenhong Zhang, Ruotong Wang, Shuo Wang, Jiajing Chen

This paper focuses on the development of an advanced intelligent article
scoring system that not only assesses the overall quality of written work but
also offers detailed feature-based scoring tailored to various article genres.
By integrating the pre-trained BERT model with the large language model
Chat-GPT, the system gains a deep understanding of both the content and
structure of the text, enabling it to provide a thorough evaluation along with
targeted suggestions for improvement. Experimental results demonstrate that
this system outperforms traditional scoring methods across multiple public
datasets, particularly in feature-based assessments, offering a more accurate
reflection of the quality of different article types. Moreover, the system
generates personalized feedback to assist users in enhancing their writing
skills, underscoring the potential and practical value of automated scoring
technologies in educational contexts.

摘要：本文重點在於開發一種先進的智慧文章評分系統，此系統不僅評估寫作作品的整體品質，還能提供針對各種文章類型量身打造的、基於特色的詳細評分。透過將預先訓練的 BERT 模型與大型語言模型 Chat-GPT 整合，此系統深入了解文本的內容與結構，能提供全面的評估，以及有針對性的改進建議。實驗結果顯示，此系統在多個公開資料集中的表現優於傳統評分方法，特別是在基於特色的評量中，能更準確地反映不同文章類型的品質。此外，此系統會產生個人化的回饋，協助使用者提升寫作技巧，強調自動評分技術在教育環境中的潛力和實用價值。

##### **Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning**
2410.14157v1 by Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong

Autoregressive language models, despite their impressive capabilities,
struggle with complex reasoning and long-term planning tasks. We introduce
discrete diffusion models as a novel solution to these challenges. Through the
lens of subgoal imbalance, we demonstrate how diffusion models effectively
learn difficult subgoals that elude autoregressive approaches. We propose
Multi-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on
difficulty during learning. On complex tasks like Countdown, Sudoku, and
Boolean Satisfiability Problems, MDM significantly outperforms autoregressive
models without using search techniques. For instance, MDM achieves 91.5\% and
100\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\% and
20.7\% for autoregressive models. Our work highlights the potential of
diffusion-based approaches in advancing AI capabilities for sophisticated
language understanding and problem-solving tasks.

摘要：儘管自迴歸語言模型功能強大，
但在複雜推理和長期規劃任務上仍有困難。我們提出離散擴散模型作為解決這些挑戰的新穎方法。透過子目標失衡的觀點，我們展示擴散模型如何有效學習自迴歸方法無法達成的困難子目標。我們提出多粒度擴散模型 (MDM)，在學習過程中根據難度對子目標進行優先排序。在倒數、數獨和布林可滿足性問題等複雜任務中，MDM 在不使用搜尋技術的情況下明顯優於自迴歸模型。例如，MDM 在倒數和數獨中的準確率分別達到 91.5% 和 100%，而自迴歸模型的準確率分別為 45.8% 和 20.7%。我們的研究突顯了基於擴散的方法在提升 AI 能力以進行精密的語言理解和問題解決任務方面的潛力。

##### **Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models**
2410.14155v1 by Wei Jie Yeo, Ranjan Satapthy, Erik Cambria

Large Language Models (LLMs) are capable of generating persuasive Natural
Language Explanations (NLEs) to justify their answers. However, the
faithfulness of these explanations should not be readily trusted at face value.
Recent studies have proposed various methods to measure the faithfulness of
NLEs, typically by inserting perturbations at the explanation or feature level.
We argue that these approaches are neither comprehensive nor correctly designed
according to the established definition of faithfulness. Moreover, we highlight
the risks of grounding faithfulness findings on out-of-distribution samples. In
this work, we leverage a causal mediation technique called activation patching,
to measure the faithfulness of an explanation towards supporting the explained
answer. Our proposed metric, Causal Faithfulness quantifies the consistency of
causal attributions between explanations and the corresponding model outputs as
the indicator of faithfulness. We experimented across models varying from 2B to
27B parameters and found that models that underwent alignment tuning tend to
produce more faithful and plausible explanations. We find that Causal
Faithfulness is a promising improvement over existing faithfulness tests by
taking into account the model's internal computations and avoiding out of
distribution concerns that could otherwise undermine the validity of
faithfulness assessments. We release the code in
\url{https://github.com/wj210/Causal-Faithfulness}

摘要：大型語言模型 (LLM) 能夠產生有說服力的自然語言解釋 (NLE) 來證明其答案。然而，這些解釋的忠實度不應輕易地被視為表面價值。最近的研究提出了各種方法來衡量 NLE 的忠實度，通常是透過在解釋或特徵層級插入擾動。我們認為這些方法既不全面，也不符合忠實度的既定定義而正確設計。此外，我們強調將忠實度發現建立在分布外樣本上的風險。在這項工作中，我們利用因果中介技術（稱為激活修補），來衡量解釋對支持所解釋答案的忠實度。我們提出的指標「因果忠實度」量化了解釋與對應模型輸出之間因果歸因的一致性，作為忠實度的指標。我們在參數從 2B 到 27B 變化不同的模型中進行了實驗，發現經過比對調整的模型往往會產生更忠實且合理的解釋。我們發現，因果忠實度透過考量模型的內部運算並避免可能會損害忠實度評估的有效性的分布外問題，對現有的忠實度測試來說是一個有希望的改進。我們在\url{https://github.com/wj210/Causal-Faithfulness}中釋出程式碼

##### **RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training**
2410.14154v1 by Muhe Ding, Yang Ma, Pengda Qin, Jianlong Wu, Yuhong Li, Liqiang Nie

Multimodal Large Language Models (MLLMs) have recently received substantial
interest, which shows their emerging potential as general-purpose models for
various vision-language tasks. MLLMs involve significant external knowledge
within their parameters; however, it is challenging to continually update these
models with the latest knowledge, which involves huge computational costs and
poor interpretability. Retrieval augmentation techniques have proven to be
effective plugins for both LLMs and MLLMs. In this study, we propose multimodal
adaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training
(RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering
the redundant information within vision modality, we first leverage the
question to instruct the extraction of visual information through interactions
with one set of learnable queries, minimizing irrelevant interference during
retrieval and generation. Besides, we introduce a pre-trained multimodal
adaptive fusion module to achieve question text-to-multimodal retrieval and
integration of multimodal knowledge by projecting visual and language
modalities into a unified semantic space. Furthermore, we present an Adaptive
Selection Knowledge Generation (ASKG) strategy to train the generator to
autonomously discern the relevance of retrieved knowledge, which realizes
excellent denoising performance. Extensive experiments on open multimodal
question-answering datasets demonstrate that RA-BLIP achieves significant
performance and surpasses the state-of-the-art retrieval-augmented models.

摘要：多模态大型语言模型 (MLLM) 近来备受关注，显示其作为各种视觉语言任务通用模型的新兴潜力。MLLM 在其参数中包含大量外部知识；然而，持续使用最新知识更新这些模型具有挑战性，这涉及巨大的计算成本和较差的可解释性。检索增强技术已被证明是 LLM 和 MLLM 的有效插件。在这项研究中，我们提出了多模态自适应检索增强引导语言图像预训练 (RA-BLIP)，一个针对各种 MLLM 的新颖检索增强框架。考虑到视觉模态中的冗余信息，我们首先利用问题通过与一组可学习查询的交互来指导视觉信息的提取，最大程度减少检索和生成过程中的无关干扰。此外，我们引入了一个预训练的多模态自适应融合模块，通过将视觉和语言模态投射到一个统一语义空间中，实现问题文本到多模态检索和多模态知识的集成。此外，我们提出了一个自适应选择知识生成 (ASKG) 策略，以训练生成器自主辨别检索知识的相关性，从而实现出色的去噪性能。在开放的多模态问答数据集上的广泛实验表明，RA-BLIP 取得了显著的性能，并超越了最先进的检索增强模型。

##### **SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent**
2410.14152v1 by Jiarui Ji, Yang Li, Hongtao Liu, Zhicheng Du, Zhewei Wei, Weiran Shen, Qi Qi, Yankai Lin

Public scarce resource allocation plays a crucial role in economics as it
directly influences the efficiency and equity in society. Traditional studies
including theoretical model-based, empirical study-based and simulation-based
methods encounter limitations due to the idealized assumption of complete
information and individual rationality, as well as constraints posed by limited
available data. In this work, we propose an innovative framework, SRAP-Agent
(Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based
Agent), which integrates Large Language Models (LLMs) into economic
simulations, aiming to bridge the gap between theoretical models and real-world
dynamics. Using public housing allocation scenarios as a case study, we conduct
extensive policy simulation experiments to verify the feasibility and
effectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm
with certain optimization objectives. The source code can be found in
https://github.com/jijiarui-cather/SRAPAgent_Framework

摘要：公共稀缺資源配置在經濟學中扮演著至關重要的角色，因為它直接影響著社會中的效率和公平。傳統的研究，包括基於理論模型的研究、基於實證研究的研究和基於模擬的研究方法，由於對完全資訊和個人理性的理想化假設，以及可用資料有限的限制，而遇到了限制。在這項工作中，我們提出了一個創新的框架，SRAP-Agent（使用 LLM 為基礎的代理模擬和最佳化稀缺資源配置政策），它將大型語言模型（LLM）整合到經濟模擬中，旨在彌合理論模型與現實世界動態之間的差距。使用公共住房配置情境作為案例研究，我們進行了廣泛的政策模擬實驗，以驗證 SRAP-Agent 的可行性和有效性，並採用具有特定最佳化目標的政策最佳化演算法。原始碼可以在 https://github.com/jijiarui-cather/SRAPAgent_Framework 中找到

##### **Utilizing Large Language Models for Event Deconstruction to Enhance Multimodal Aspect-Based Sentiment Analysis**
2410.14150v1 by Xiaoyong Huang, Heli Sun, Qunshu Gao, Wenjie Huang, Ruichen Cao

With the rapid development of the internet, the richness of User-Generated
Contentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis
(MABSA) a research hotspot. Existing studies have achieved certain results in
MABSA, but they have not effectively addressed the analytical challenges in
scenarios where multiple entities and sentiments coexist. This paper
innovatively introduces Large Language Models (LLMs) for event decomposition
and proposes a reinforcement learning framework for Multimodal Aspect-based
Sentiment Analysis (MABSA-RL) framework. This framework decomposes the original
text into a set of events using LLMs, reducing the complexity of analysis,
introducing reinforcement learning to optimize model parameters. Experimental
results show that MABSA-RL outperforms existing advanced methods on two
benchmark datasets. This paper provides a new research perspective and method
for multimodal aspect-level sentiment analysis.

摘要：隨著網路的快速發展，使用者產製內容的豐富度持續增加，使得多模態面向基礎情緒分析（MABSA）成為研究熱點。現有研究在 MABSA 上已取得一定成果，但對於多個實體與情緒共存的場景，分析挑戰尚未有效解決。本文創新地引入大型語言模型（LLM）進行事件分解，並提出一個用於多模態面向基礎情緒分析（MABSA-RL）框架的強化學習架構。此架構利用 LLM 將原始文字分解為一組事件，降低分析的複雜度，並引入強化學習來優化模型參數。實驗結果顯示，MABSA-RL 在兩個基準資料集上優於現有的進階方法。本文為多模態面向基礎情緒分析提供新的研究視角與方法。

##### **Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment**
2410.14148v1 by Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, Tat-Seng Chua

The recent advancements in large language models (LLMs) and pre-trained
vision models have accelerated the development of vision-language large models
(VLLMs), enhancing the interaction between visual and linguistic modalities.
Despite their notable success across various domains, VLLMs face challenges in
modality alignment, which can lead to issues like hallucinations and unsafe
content generation. Current alignment techniques often rely on coarse feedback
and external datasets, limiting scalability and performance. In this paper, we
propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel
self-alignment method that utilizes the model's own visual encoder as a
fine-grained verifier to improve vision-language alignment without the need for
additional data. By leveraging token-level feedback from the vision encoder,
FiSAO significantly improves vision-language alignment, even surpassing
traditional preference tuning methods that require additional data. Through
both theoretical analysis and experimental validation, we demonstrate that
FiSAO effectively addresses the misalignment problem in VLLMs, marking the
first instance of token-level rewards being applied to such models.

摘要：近期大型语言模型 (LLM) 和预训练视觉模型的进步加速了视觉语言大型模型 (VLLM) 的发展，增强了视觉和语言模态之间的互动。尽管在各个领域取得了显著成功，但 VLLM 在模态对齐方面面临挑战，这可能导致出现幻觉和不安全的内容生成等问题。当前的对齐技术通常依赖于粗略的反馈和外部数据集，限制了可扩展性和性能。在本文中，我们提出了 FiSAO（细粒度自对齐优化），这是一种新颖的自对齐方法，它利用模型本身的视觉编码器作为细粒度验证器来改进视觉语言对齐，而无需额外数据。通过利用视觉编码器的标记级反馈，FiSAO 显着改进了视觉语言对齐，甚至超越了需要额外数据的传统偏好调整方法。通过理论分析和实验验证，我们证明了 FiSAO 有效地解决了 VLLM 中的错位问题，标志着首次将标记级奖励应用于此类模型。

##### **CausalChat: Interactive Causal Model Development and Refinement Using Large Language Models**
2410.14146v1 by Yanming Zhang, Akshith Kota, Eric Papenhausen, Klaus Mueller

Causal networks are widely used in many fields to model the complex
relationships between variables. A recent approach has sought to construct
causal networks by leveraging the wisdom of crowds through the collective
participation of humans. While this can yield detailed causal networks that
model the underlying phenomena quite well, it requires a large number of
individuals with domain understanding. We adopt a different approach:
leveraging the causal knowledge that large language models, such as OpenAI's
GPT-4, have learned by ingesting massive amounts of literature. Within a
dedicated visual analytics interface, called CausalChat, users explore single
variables or variable pairs recursively to identify causal relations, latent
variables, confounders, and mediators, constructing detailed causal networks
through conversation. Each probing interaction is translated into a tailored
GPT-4 prompt and the response is conveyed through visual representations which
are linked to the generated text for explanations. We demonstrate the
functionality of CausalChat across diverse data contexts and conduct user
studies involving both domain experts and laypersons.

摘要：因果网络在许多领域被广泛用于对变量之间的复杂关系进行建模。最近一种方法试图通过集体参与人类来利用群体智慧构建因果网络。虽然这可以产生非常好的对底层现象进行建模的详细因果网络，但它需要大量具有领域理解力的人员。我们采用了一种不同的方法：利用大型语言模型（例如 OpenAI 的 GPT-4）通过摄取大量文献所学到的因果知识。在称为 CausalChat 的专用可视化分析界面中，用户递归探索单个变量或变量对以识别因果关系、潜在变量、混杂因素和中介，通过对话构建详细的因果网络。每次探查交互都会被翻译成一个定制的 GPT-4 提示，并且响应通过可视化表示传达，这些表示链接到生成的文本以进行解释。我们展示了 CausalChat 在不同数据上下文中的功能，并进行了涉及领域专家和外行人士的用户研究。

##### **CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using Large Language Models**
2410.14145v1 by June M. Liu, He Cao, Renliang Sun, Rui Wang, Yu Li, Jiaxing Zhang

Generating emotionally appropriate responses in conversations with large
language models presents a significant challenge due to the complexities of
human emotions and cognitive processes, which remain largely underexplored in
their critical role in social interactions. In this study, we introduce a
two-stage automatic data generation framework to create CAPE, a Chinese dataset
named Cognitive Appraisal theory-based Emotional corpus. This corpus
facilitates the generation of dialogues with contextually appropriate emotional
responses by accounting for diverse personal and situational factors. We
propose two tasks utilizing this dataset: emotion prediction and next utterance
prediction. Both automated and human evaluations demonstrate that agents
trained on our dataset can deliver responses that are more aligned with human
emotional expressions. Our study shows the potential for advancing emotional
expression in conversational agents, paving the way for more nuanced and
meaningful human-computer interactions.

摘要：在與大型語言模型的對話中產生適當的情緒反應，由於人類情緒和認知過程的複雜性，而構成了一項重大挑戰，這些複雜性在社會互動中扮演著重要的角色，但仍未受到充分的探討。在此研究中，我們引入了一個兩階段自動資料產生架構來建立 CAPE，一個名為「認知評鑑理論為基礎的情緒語料庫」的中文資料集。此語料庫透過考量不同的個人和情境因素，促進產生具有適當情緒反應的對話。我們提出兩個利用此資料集的任務：情緒預測和下一個話語預測。自動化和人工評估都顯示，在我們的資料集上訓練的代理程式，可以提供與人類情緒表達更一致的反應。我們的研究顯示了在對話代理程式中推進情緒表達的潛力，為更細緻入微且有意義的人機互動鋪路。

##### **A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models**
2410.14144v1 by Chenyang Zhang, Jiayi Lin, Haibo Tong, Bingxuan Hou, Dongyu Zhang, Jialin Li, Junli Wang

Large language models (LLMs) show remarkable abilities with instruction
tuning. However, they fail to achieve ideal tasks when lacking high-quality
instruction tuning data on target tasks. Multi-Aspect Controllable Text
Generation (MCTG) is a representative task for this dilemma, where aspect
datasets are usually biased and correlated. Existing work exploits additional
model structures and strategies for solutions, limiting adaptability to LLMs.
To activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based
on data augmentation. We analyze bias and correlations in traditional datasets,
and address these concerns with augmented control attributes and sentences.
Augmented datasets are feasible for instruction tuning. In our experiments,
LLMs perform better in MCTG after data augmentation, with a 20% accuracy rise
and less aspect correlations.

摘要：大型語言模型 (LLM) 在指令調整方面表現出非凡的能力。然而，當缺乏針對目標任務的高品質指令調整數據時，它們無法實現理想的任務。多方面可控文本生成 (MCTG) 是這個困境的代表性任務，其中方面數據集通常有偏差且相關。現有工作利用額外的模型結構和策略來解決問題，限制了對 LLM 的適應性。為了激活 LLM 的 MCTG 能力，我們提出了一個基於數據增強的輕量級 MCTG 管道。我們分析了傳統數據集中的偏差和相關性，並使用增強的控制屬性和句子來解決這些問題。增強的數據集適用於指令調整。在我們的實驗中，LLM 在數據增強後在 MCTG 中表現得更好，準確度提高了 20%，方面相關性降低了。

