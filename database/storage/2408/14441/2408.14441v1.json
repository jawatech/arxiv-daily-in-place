{"2408.14441": {"publish_time": "2024-08-26", "title": "Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification", "paper_summary": "Exploiting both audio and visual modalities for video classification is a\nchallenging task, as the existing methods require large model architectures,\nleading to high computational complexity and resource requirements. Smaller\narchitectures, on the other hand, struggle to achieve optimal performance. In\nthis paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that\nintroduces a compact model architecture specifically designed to capture\nintricate audio-visual relationships in video data. Through extensive\nexperiments on the challenging YouTube-8M dataset, we demonstrate that\nAttend-Fusion achieves an F1 score of 75.64\\% with only 72M parameters, which\nis comparable to the performance of larger baseline models such as\nFully-Connected Late Fusion (75.96\\% F1 score, 341M parameters). Attend-Fusion\nachieves similar performance to the larger baseline model while reducing the\nmodel size by nearly 80\\%, highlighting its efficiency in terms of model\ncomplexity. Our work demonstrates that the Attend-Fusion model effectively\ncombines audio and visual information for video classification, achieving\ncompetitive performance with significantly reduced model size. This approach\nopens new possibilities for deploying high-performance video understanding\nsystems in resource-constrained environments across various applications.", "paper_summary_zh": "\u5229\u7528\u97f3\u8a0a\u548c\u8996\u89ba\u5169\u7a2e\u65b9\u5f0f\u9032\u884c\u5f71\u7247\u5206\u985e\u662f\u4e00\u9805\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u56e0\u70ba\u73fe\u6709\u65b9\u6cd5\u9700\u8981\u5927\u578b\u6a21\u578b\u67b6\u69cb\uff0c\u5c0e\u81f4\u904b\u7b97\u8907\u96dc\u5ea6\u548c\u8cc7\u6e90\u9700\u6c42\u9ad8\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u8f03\u5c0f\u7684\u67b6\u69cb\u96e3\u4ee5\u9054\u5230\u6700\u4f73\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa Attend-Fusion\uff0c\u9019\u662f\u4e00\u7a2e\u97f3\u8a0a\u8996\u89ba (AV) \u878d\u5408\u65b9\u6cd5\uff0c\u5b83\u5f15\u9032\u4e86\u4e00\u500b\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u64f7\u53d6\u5f71\u7247\u8cc7\u6599\u4e2d\u8907\u96dc\u97f3\u8a0a\u8996\u89ba\u95dc\u4fc2\u7684\u7cbe\u7c21\u6a21\u578b\u67b6\u69cb\u3002\u900f\u904e\u5728\u5177\u6709\u6311\u6230\u6027\u7684 YouTube-8M \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e Attend-Fusion \u53ea\u4f7f\u7528 72M \u500b\u53c3\u6578\u5c31\u80fd\u9054\u5230 75.64% \u7684 F1 \u5206\u6578\uff0c\u9019\u8207\u8f03\u5927\u7684\u57fa\u6e96\u6a21\u578b\uff08\u4f8b\u5982\u5168\u9023\u63a5\u5f8c\u878d\u5408\uff0875.96% F1 \u5206\u6578\uff0c341M \u500b\u53c3\u6578\uff09\uff09\u7684\u6548\u80fd\u76f8\u7576\u3002Attend-Fusion \u9054\u5230\u8207\u8f03\u5927\u7684\u57fa\u6e96\u6a21\u578b\u76f8\u4f3c\u7684\u6548\u80fd\uff0c\u540c\u6642\u5c07\u6a21\u578b\u5927\u5c0f\u6e1b\u5c11\u4e86\u5c07\u8fd1 80%\uff0c\u7a81\u986f\u4e86\u5b83\u5728\u6a21\u578b\u8907\u96dc\u5ea6\u65b9\u9762\u7684\u6548\u7387\u3002\u6211\u5011\u7684\u7814\u7a76\u8b49\u660e Attend-Fusion \u6a21\u578b\u6709\u6548\u5730\u7d50\u5408\u4e86\u97f3\u8a0a\u548c\u8996\u89ba\u8cc7\u8a0a\u9032\u884c\u5f71\u7247\u5206\u985e\uff0c\u5728\u986f\u8457\u6e1b\u5c11\u6a21\u578b\u5927\u5c0f\u7684\u60c5\u6cc1\u4e0b\u9054\u5230\u5177\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002\u9019\u7a2e\u65b9\u6cd5\u70ba\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u90e8\u7f72\u9ad8\u6027\u80fd\u5f71\u7247\u7406\u89e3\u7cfb\u7d71\u958b\u555f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u9069\u7528\u65bc\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u3002", "author": "Mahrukh Awan et.al.", "authors": "Mahrukh Awan, Asmar Nadeem, Muhammad Junaid Awan, Armin Mustafa, Syed Sameed Husain", "id": "2408.14441v1", "paper_url": "http://arxiv.org/abs/2408.14441v1", "repo": "null"}}