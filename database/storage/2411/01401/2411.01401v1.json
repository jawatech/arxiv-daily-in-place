{"2411.01401": {"publish_time": "2024-11-03", "title": "Pre-trained Molecular Language Models with Random Functional Group Masking", "paper_summary": "Recent advancements in computational chemistry have leveraged the power of\ntrans-former-based language models, such as MoLFormer, pre-trained using a vast\namount of simplified molecular-input line-entry system (SMILES) sequences, to\nunderstand and predict molecular properties and activities, a critical step in\nfields like drug discovery and materials science. To further improve\nperformance, researchers have introduced graph neural networks with graph-based\nmolecular representations, such as GEM, incorporating the topology, geometry,\n2D or even 3D structures of molecules into pre-training. While most of\nmolecular graphs in existing studies were automatically converted from SMILES\nsequences, it is to assume that transformer-based language models might be able\nto implicitly learn structure-aware representations from SMILES sequences. In\nthis paper, we propose \\ours{} -- a SMILES-based \\underline{\\em M}olecular\n\\underline{\\em L}anguage \\underline{\\em M}odel, which randomly masking SMILES\nsubsequences corresponding to specific molecular \\underline{\\em F}unctional\n\\underline{\\em G}roups to incorporate structure information of atoms during the\npre-training phase. This technique aims to compel the model to better infer\nmolecular structures and properties, thus enhancing its predictive\ncapabilities. Extensive experimental evaluations across 11 benchmark\nclassification and regression tasks in the chemical domain demonstrate the\nrobustness and superiority of \\ours{}. Our findings reveal that \\ours{}\noutperforms existing pre-training models, either based on SMILES or graphs, in\n9 out of the 11 downstream tasks, ranking as a close second in the remaining\nones.", "paper_summary_zh": "<paragraph>\u8a08\u7b97\u5316\u5b78\u7684\u8fd1\u671f\u9032\u5c55\u5df2\u5229\u7528\u8f49\u63db\u5668\u8a9e\u8a00\u6a21\u578b\u7684\u529b\u91cf\uff0c\u4f8b\u5982 MoLFormer\uff0c\u4f7f\u7528\u5927\u91cf\u7c21\u5316\u5206\u5b50\u8f38\u5165\u7dda\u689d\u8f38\u5165\u7cfb\u7d71 (SMILES) \u5e8f\u5217\u9032\u884c\u9810\u8a13\u7df4\uff0c\u4ee5\u4e86\u89e3\u548c\u9810\u6e2c\u5206\u5b50\u7279\u6027\u548c\u6d3b\u6027\uff0c\u9019\u662f\u85e5\u7269\u767c\u73fe\u548c\u6750\u6599\u79d1\u5b78\u7b49\u9818\u57df\u7684\u91cd\u8981\u6b65\u9a5f\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u63d0\u5347\u6548\u80fd\uff0c\u7814\u7a76\u4eba\u54e1\u5f15\u5165\u4e86\u5177\u6709\u5716\u5f62\u70ba\u57fa\u790e\u7684\u5206\u5b50\u8868\u793a\u7684\u5716\u5f62\u795e\u7d93\u7db2\u8def\uff0c\u4f8b\u5982 GEM\uff0c\u5c07\u5206\u5b50\u7684\u62d3\u6a38\u3001\u5e7e\u4f55\u30012D \u751a\u81f3 3D \u7d50\u69cb\u7d0d\u5165\u9810\u8a13\u7df4\u4e2d\u3002\u96d6\u7136\u73fe\u6709\u7814\u7a76\u4e2d\u7684\u5927\u591a\u6578\u5206\u5b50\u5716\u5f62\u90fd\u662f\u5f9e SMILES \u5e8f\u5217\u81ea\u52d5\u8f49\u63db\u800c\u4f86\u7684\uff0c\u4f46\u53ef\u4ee5\u5047\u8a2d\u57fa\u65bc\u8f49\u63db\u5668\u7684\u8a9e\u8a00\u6a21\u578b\u53ef\u80fd\u80fd\u5920\u5f9e SMILES \u5e8f\u5217\u4e2d\u96b1\u5f0f\u5b78\u7fd2\u7d50\u69cb\u611f\u77e5\u8868\u793a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa \\ours{} -- \u4e00\u500b\u57fa\u65bc SMILES \u7684\\underline{\\em M}olecular\\underline{\\em L}anguage \\underline{\\em M}odel\uff0c\u5b83\u96a8\u6a5f\u906e\u853d\u5c0d\u61c9\u65bc\u7279\u5b9a\u5206\u5b50\\underline{\\em F}unctional\\underline{\\em G}roups \u7684 SMILES \u5b50\u5e8f\u5217\uff0c\u4ee5\u5728\u9810\u8a13\u7df4\u968e\u6bb5\u7d0d\u5165\u539f\u5b50\u7684\u7d50\u69cb\u8cc7\u8a0a\u3002\u6b64\u6280\u8853\u65e8\u5728\u5f37\u5236\u6a21\u578b\u66f4\u597d\u5730\u63a8\u65b7\u5206\u5b50\u7d50\u69cb\u548c\u7279\u6027\uff0c\u5f9e\u800c\u589e\u5f37\u5176\u9810\u6e2c\u80fd\u529b\u3002\u5728\u5316\u5b78\u9818\u57df\u7684 11 \u500b\u57fa\u6e96\u5206\u985e\u548c\u56de\u6b78\u4efb\u52d9\u4e2d\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8a55\u4f30\u8b49\u660e\u4e86 \\ours{} \u7684\u7a69\u5065\u6027\u548c\u512a\u8d8a\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\\ours{} \u5728 11 \u500b\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684 9 \u500b\u4efb\u52d9\u4e2d\u512a\u65bc\u73fe\u6709\u7684\u9810\u8a13\u7df4\u6a21\u578b\uff08\u57fa\u65bc SMILES \u6216\u5716\u5f62\uff09\uff0c\u5728\u5269\u4e0b\u7684\u4efb\u52d9\u4e2d\u6392\u540d\u7b2c\u4e8c\u3002</paragraph>", "author": "Tianhao Peng et.al.", "authors": "Tianhao Peng, Yuchen Li, Xuhong Li, Jiang Bian, Zeke Xie, Ning Sui, Shahid Mumtaz, Yanwu Xu, Linghe Kong, Haoyi Xiong", "id": "2411.01401v1", "paper_url": "http://arxiv.org/abs/2411.01401v1", "repo": "null"}}