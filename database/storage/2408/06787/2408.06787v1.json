{"2408.06787": {"publish_time": "2024-08-13", "title": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion", "paper_summary": "Classical knowledge graph completion (KGC) methods rely solely on structural\ninformation, struggling with the inherent sparsity of knowledge graphs (KGs).\nLarge Language Models (LLMs) learn extensive knowledge from large corpora with\npowerful context modeling, which is ideal for mitigating the limitations of\nprevious methods. Directly fine-tuning LLMs offers great capability but comes\nat the cost of huge time and memory consumption, while utilizing frozen LLMs\nyields suboptimal results. In this work, we aim to leverage LLMs for KGC\neffectively and efficiently. We capture the context-aware hidden states of\nknowledge triples by employing prompts to stimulate the intermediate layers of\nLLMs. We then train a data-efficient classifier on these hidden states to\nharness the inherent capabilities of frozen LLMs in KGC. We also generate\nentity descriptions with subgraph sampling on KGs, reducing the ambiguity of\ntriplets and enriching the knowledge representation. Extensive experiments on\nstandard benchmarks showcase the efficiency and effectiveness of our approach.\nWe outperform classical KGC methods on most datasets and match the performance\nof fine-tuned LLMs. Additionally, compared to fine-tuned LLMs, we boost GPU\nmemory efficiency by \\textbf{$188\\times$} and speed up training+inference by\n\\textbf{$13.48\\times$}.", "paper_summary_zh": "\u50b3\u7d71\u77e5\u8b58\u5716\u8b5c\u5b8c\u6210 (KGC) \u65b9\u6cd5\u50c5\u4f9d\u8cf4\u7d50\u69cb\u5316\u8cc7\u8a0a\uff0c\u96e3\u4ee5\u61c9\u5c0d\u77e5\u8b58\u5716\u8b5c (KG) \u5167\u5728\u7684\u7a00\u758f\u6027\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5f9e\u5927\u578b\u8a9e\u6599\u5eab\u4e2d\u5b78\u7fd2\u5ee3\u6cdb\u7684\u77e5\u8b58\uff0c\u4e26\u5177\u5099\u5f37\u5927\u7684\u60c5\u5883\u5efa\u6a21\u80fd\u529b\uff0c\u9019\u5c0d\u65bc\u7de9\u89e3\u5148\u524d\u65b9\u6cd5\u7684\u9650\u5236\u975e\u5e38\u7406\u60f3\u3002\u76f4\u63a5\u5fae\u8abf LLM \u53ef\u63d0\u4f9b\u5f37\u5927\u7684\u80fd\u529b\uff0c\u4f46\u4ee3\u50f9\u662f\u8017\u8cbb\u5927\u91cf\u6642\u9593\u548c\u8a18\u61b6\u9ad4\uff0c\u800c\u5229\u7528\u51cd\u7d50\u7684 LLM \u5247\u6703\u7522\u751f\u6b21\u4f73\u7d50\u679c\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u6709\u6548\u4e14\u9ad8\u6548\u5730\u5229\u7528 LLM \u4f86\u9032\u884c KGC\u3002\u6211\u5011\u900f\u904e\u4f7f\u7528\u63d0\u793a\u4f86\u523a\u6fc0 LLM \u7684\u4e2d\u9593\u5c64\uff0c\u6355\u6349\u5230\u77e5\u8b58\u4e09\u5143\u7d44\u7684\u60c5\u5883\u611f\u77e5\u96b1\u85cf\u72c0\u614b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5728\u9019\u4e9b\u96b1\u85cf\u72c0\u614b\u4e0a\u8a13\u7df4\u4e00\u500b\u8cc7\u6599\u6709\u6548\u7387\u7684\u5206\u985e\u5668\uff0c\u4ee5\u5229\u7528\u51cd\u7d50 LLM \u5728 KGC \u4e2d\u7684\u5167\u5728\u80fd\u529b\u3002\u6211\u5011\u9084\u900f\u904e\u5728 KG \u4e0a\u9032\u884c\u5b50\u5716\u62bd\u6a23\u4f86\u7522\u751f\u5be6\u9ad4\u63cf\u8ff0\uff0c\u6e1b\u5c11\u4e09\u5143\u7d44\u7684\u6a21\u7cca\u6027\u4e26\u8c50\u5bcc\u77e5\u8b58\u8868\u793a\u3002\u6a19\u6e96\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u5c55\u793a\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002\u6211\u5011\u5728\u591a\u6578\u8cc7\u6599\u96c6\u4e0a\u512a\u65bc\u50b3\u7d71\u7684 KGC \u65b9\u6cd5\uff0c\u4e26\u8207\u5fae\u8abf\u5f8c\u7684 LLM \u9054\u5230\u76f8\u540c\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u8207\u5fae\u8abf\u5f8c\u7684 LLM \u76f8\u6bd4\uff0c\u6211\u5011\u5c07 GPU \u8a18\u61b6\u9ad4\u6548\u7387\u63d0\u5347\u4e86 **$188\\times$**\uff0c\u4e26\u5c07\u8a13\u7df4 + \u63a8\u8ad6\u901f\u5ea6\u63d0\u5347\u4e86 **$13.48\\times$**\u3002", "author": "Bo Xue et.al.", "authors": "Bo Xue, Yi Xu, Yunchong Song, Yiming Pang, Yuyang Ren, Jiaxin Ding, Luoyi Fu, Xinbing Wang", "id": "2408.06787v1", "paper_url": "http://arxiv.org/abs/2408.06787v1", "repo": "null"}}