{"2407.20337": {"publish_time": "2024-07-29", "title": "Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities", "paper_summary": "Discerning between authentic content and that generated by advanced AI\nmethods has become increasingly challenging. While previous research primarily\naddresses the detection of fake faces, the identification of generated natural\nimages has only recently surfaced. This prompted the recent exploration of\nsolutions that employ foundation vision-and-language models, like CLIP.\nHowever, the CLIP embedding space is optimized for global image-to-text\nalignment and is not inherently designed for deepfake detection, neglecting the\npotential benefits of tailored training and local image features. In this\nstudy, we propose CoDE (Contrastive Deepfake Embeddings), a novel embedding\nspace specifically designed for deepfake detection. CoDE is trained via\ncontrastive learning by additionally enforcing global-local similarities. To\nsustain the training of our model, we generate a comprehensive dataset that\nfocuses on images generated by diffusion models and encompasses a collection of\n9.2 million images produced by using four different generators. Experimental\nresults demonstrate that CoDE achieves state-of-the-art accuracy on the newly\ncollected dataset, while also showing excellent generalization capabilities to\nunseen image generators. Our source code, trained models, and collected dataset\nare publicly available at: https://github.com/aimagelab/CoDE.", "paper_summary_zh": "\u5340\u5206\u771f\u5be6\u5167\u5bb9\u548c\u7531\u9032\u968e\u4eba\u5de5\u667a\u6167\u65b9\u6cd5\u7522\u751f\u7684\u5167\u5bb9\u5df2\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u5177\u6709\u6311\u6230\u6027\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u91dd\u5c0d\u5047\u81c9\u7684\u5075\u6e2c\uff0c\u4f46\u751f\u6210\u81ea\u7136\u5f71\u50cf\u7684\u8fa8\u8b58\u76f4\u5230\u6700\u8fd1\u624d\u6d6e\u73fe\u3002\u9019\u4fc3\u4f7f\u6700\u8fd1\u63a2\u8a0e\u63a1\u7528\u57fa\u790e\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u7136\u800c\uff0cCLIP \u5d4c\u5165\u7a7a\u9593\u91dd\u5c0d\u5168\u7403\u5f71\u50cf\u5c0d\u6587\u5b57\u6bd4\u5c0d\u9032\u884c\u6700\u4f73\u5316\uff0c\u4e26\u975e\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u6df1\u5ea6\u507d\u9020\u5075\u6e2c\uff0c\u5ffd\u7565\u4e86\u5ba2\u88fd\u5316\u8a13\u7df4\u548c\u5c40\u90e8\u5f71\u50cf\u7279\u5fb5\u7684\u6f5b\u5728\u597d\u8655\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa CoDE\uff08\u5c0d\u6bd4\u6df1\u5ea6\u507d\u9020\u5d4c\u5165\uff09\uff0c\u4e00\u7a2e\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u6df1\u5ea6\u507d\u9020\u5075\u6e2c\u7684\u65b0\u5d4c\u5165\u7a7a\u9593\u3002CoDE \u900f\u904e\u5c0d\u6bd4\u5b78\u7fd2\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u984d\u5916\u5f37\u5236\u57f7\u884c\u5168\u5c40\u5c40\u90e8\u76f8\u4f3c\u6027\u3002\u70ba\u4e86\u6301\u7e8c\u8a13\u7df4\u6211\u5011\u7684\u6a21\u578b\uff0c\u6211\u5011\u7522\u751f\u4e86\u4e00\u500b\u7d9c\u5408\u8cc7\u6599\u96c6\uff0c\u5c08\u6ce8\u65bc\u7531\u64f4\u6563\u6a21\u578b\u7522\u751f\u7684\u5f71\u50cf\uff0c\u4e26\u5305\u542b\u4f7f\u7528\u56db\u7a2e\u4e0d\u540c\u7522\u751f\u5668\u7522\u751f\u7684 920 \u842c\u5f35\u5f71\u50cf\u96c6\u5408\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0cCoDE \u5728\u65b0\u6536\u96c6\u7684\u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6e96\u78ba\u5ea6\uff0c\u540c\u6642\u4e5f\u5c0d\u672a\u898b\u904e\u7684\u5f71\u50cf\u7522\u751f\u5668\u5c55\u73fe\u51fa\u6975\u4f73\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u5011\u7684\u539f\u59cb\u78bc\u3001\u8a13\u7df4\u904e\u7684\u6a21\u578b\u548c\u6536\u96c6\u7684\u8cc7\u6599\u96c6\u516c\u958b\u65bc\uff1ahttps://github.com/aimagelab/CoDE\u3002", "author": "Lorenzo Baraldi et.al.", "authors": "Lorenzo Baraldi, Federico Cocchi, Marcella Cornia, Lorenzo Baraldi, Alessandro Nicolosi, Rita Cucchiara", "id": "2407.20337v1", "paper_url": "http://arxiv.org/abs/2407.20337v1", "repo": "https://github.com/aimagelab/code"}}