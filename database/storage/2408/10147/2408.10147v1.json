{"2408.10147": {"publish_time": "2024-08-19", "title": "In-Context Learning with Representations: Contextual Generalization of Trained Transformers", "paper_summary": "In-context learning (ICL) refers to a remarkable capability of pretrained\nlarge language models, which can learn a new task given a few examples during\ninference. However, theoretical understanding of ICL is largely under-explored,\nparticularly whether transformers can be trained to generalize to unseen\nexamples in a prompt, which will require the model to acquire contextual\nknowledge of the prompt for generalization. This paper investigates the\ntraining dynamics of transformers by gradient descent through the lens of\nnon-linear regression tasks. The contextual generalization here can be attained\nvia learning the template function for each task in-context, where all template\nfunctions lie in a linear space with $m$ basis functions. We analyze the\ntraining dynamics of one-layer multi-head transformers to in-contextly predict\nunlabeled inputs given partially labeled prompts, where the labels contain\nGaussian noise and the number of examples in each prompt are not sufficient to\ndetermine the template. Under mild assumptions, we show that the training loss\nfor a one-layer multi-head transformer converges linearly to a global minimum.\nMoreover, the transformer effectively learns to perform ridge regression over\nthe basis functions. To our knowledge, this study is the first provable\ndemonstration that transformers can learn contextual (i.e., template)\ninformation to generalize to both unseen examples and tasks when prompts\ncontain only a small number of query-answer pairs.", "paper_summary_zh": "\u60c5\u5883\u5b78\u7fd2 (ICL) \u662f\u4e00\u7a2e\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u986f\u8457\u529f\u80fd\uff0c\u5b83\u53ef\u4ee5\u5728\u63a8\u7406\u671f\u9593\u6839\u64da\u4e00\u4e9b\u7bc4\u4f8b\u5b78\u7fd2\u4e00\u9805\u65b0\u4efb\u52d9\u3002\u7136\u800c\uff0c\u5c0d\u65bc ICL \u7684\u7406\u8ad6\u7406\u89e3\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u7279\u5225\u662fTransformer\u662f\u5426\u53ef\u4ee5\u63a5\u53d7\u8a13\u7df4\u4ee5\u6982\u5316\u63d0\u793a\u4e2d\u672a\u898b\u904e\u7684\u7bc4\u4f8b\uff0c\u9019\u5c07\u8981\u6c42\u6a21\u578b\u7fd2\u5f97\u63d0\u793a\u7684\u8108\u7d61\u77e5\u8b58\u4ee5\u9032\u884c\u6982\u5316\u3002\u672c\u6587\u900f\u904e\u975e\u7dda\u6027\u56de\u6b78\u4efb\u52d9\u7684\u900f\u93e1\uff0c\u7814\u7a76Transformer\u7684\u68af\u5ea6\u4e0b\u964d\u8a13\u7df4\u52d5\u614b\u3002\u6b64\u8655\u7684\u8108\u7d61\u6982\u5316\u53ef\u900f\u904e\u5728\u60c5\u5883\u4e2d\u5b78\u7fd2\u6bcf\u500b\u4efb\u52d9\u7684\u7bc4\u672c\u51fd\u6578\u4f86\u9054\u6210\uff0c\u5176\u4e2d\u6240\u6709\u7bc4\u672c\u51fd\u6578\u90fd\u4f4d\u65bc\u5177\u6709 $m$ \u500b\u57fa\u5e95\u51fd\u6578\u7684\u7dda\u6027\u7a7a\u9593\u4e2d\u3002\u6211\u5011\u5206\u6790\u55ae\u5c64\u591a\u982dTransformer\u7684\u8a13\u7df4\u52d5\u614b\uff0c\u4ee5\u5728\u90e8\u5206\u6a19\u8a18\u63d0\u793a\u4e2d\u9810\u6e2c\u672a\u6a19\u8a18\u8f38\u5165\uff0c\u5176\u4e2d\u6a19\u7c64\u5305\u542b\u9ad8\u65af\u566a\u8072\uff0c\u4e14\u6bcf\u500b\u63d0\u793a\u4e2d\u7684\u7bc4\u4f8b\u6578\u91cf\u4e0d\u8db3\u4ee5\u78ba\u5b9a\u7bc4\u672c\u3002\u5728\u6eab\u548c\u7684\u5047\u8a2d\u4e0b\uff0c\u6211\u5011\u8868\u660e\u55ae\u5c64\u591a\u982dTransformer\u7684\u8a13\u7df4\u640d\u5931\u7dda\u6027\u6536\u6582\u5230\u5168\u5c40\u6700\u5c0f\u503c\u3002\u6b64\u5916\uff0cTransformer\u6709\u6548\u5730\u5b78\u7fd2\u5c0d\u57fa\u5e95\u51fd\u6578\u57f7\u884c\u5dba\u56de\u6b78\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u9805\u7814\u7a76\u662f\u7b2c\u4e00\u500b\u53ef\u8b49\u660eTransformer\u53ef\u4ee5\u5b78\u7fd2\u8108\u7d61\uff08\u5373\u7bc4\u672c\uff09\u8cc7\u8a0a\uff0c\u4ee5\u5728\u63d0\u793a\u50c5\u5305\u542b\u5c11\u6578\u67e5\u8a62\u7b54\u6848\u914d\u5c0d\u6642\uff0c\u6982\u5316\u5230\u672a\u898b\u904e\u7684\u7bc4\u4f8b\u548c\u4efb\u52d9\u3002", "author": "Tong Yang et.al.", "authors": "Tong Yang, Yu Huang, Yingbin Liang, Yuejie Chi", "id": "2408.10147v1", "paper_url": "http://arxiv.org/abs/2408.10147v1", "repo": "null"}}