{"2405.10542": {"publish_time": "2024-05-17", "title": "Benchmarking Large Language Models on CFLUE -- A Chinese Financial Language Understanding Evaluation Dataset", "paper_summary": "In light of recent breakthroughs in large language models (LLMs) that have\nrevolutionized natural language processing (NLP), there is an urgent need for\nnew benchmarks to keep pace with the fast development of LLMs. In this paper,\nwe propose CFLUE, the Chinese Financial Language Understanding Evaluation\nbenchmark, designed to assess the capability of LLMs across various dimensions.\nSpecifically, CFLUE provides datasets tailored for both knowledge assessment\nand application assessment. In knowledge assessment, it consists of 38K+\nmultiple-choice questions with associated solution explanations. These\nquestions serve dual purposes: answer prediction and question reasoning. In\napplication assessment, CFLUE features 16K+ test instances across distinct\ngroups of NLP tasks such as text classification, machine translation, relation\nextraction, reading comprehension, and text generation. Upon CFLUE, we conduct\na thorough evaluation of representative LLMs. The results reveal that only\nGPT-4 and GPT-4-turbo achieve an accuracy exceeding 60\\% in answer prediction\nfor knowledge assessment, suggesting that there is still substantial room for\nimprovement in current LLMs. In application assessment, although GPT-4 and\nGPT-4-turbo are the top two performers, their considerable advantage over\nlightweight LLMs is noticeably diminished. The datasets and scripts associated\nwith CFLUE are openly accessible at https://github.com/aliyun/cflue.", "paper_summary_zh": "\u9451\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8fd1\u671f\u7a81\u7834\u5df2\u5fb9\u5e95\u6539\u8b8a\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP)\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u65b0\u7684\u57fa\u6e96\u4f86\u8ddf\u4e0a LLM \u7684\u5feb\u901f\u767c\u5c55\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa CFLUE\uff0c\u5373\u4e2d\u6587\u91d1\u878d\u8a9e\u8a00\u7406\u89e3\u8a55\u4f30\u57fa\u6e96\uff0c\u65e8\u5728\u8a55\u4f30 LLM \u5728\u5404\u500b\u9762\u5411\u7684\u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cCFLUE \u63d0\u4f9b\u4e86\u5c08\u9580\u91dd\u5c0d\u77e5\u8b58\u8a55\u4f30\u548c\u61c9\u7528\u8a55\u4f30\u800c\u8a2d\u8a08\u7684\u8cc7\u6599\u96c6\u3002\u5728\u77e5\u8b58\u8a55\u4f30\u4e2d\uff0c\u5b83\u5305\u542b 38K+ \u500b\u9078\u64c7\u984c\uff0c\u4e26\u9644\u6709\u89e3\u6c7a\u65b9\u6848\u8aaa\u660e\u3002\u9019\u4e9b\u554f\u984c\u6709\u5169\u500b\u76ee\u7684\uff1a\u7b54\u6848\u9810\u6e2c\u548c\u554f\u984c\u63a8\u7406\u3002\u5728\u61c9\u7528\u8a55\u4f30\u4e2d\uff0cCFLUE \u5728\u4e0d\u540c\u7684 NLP \u4efb\u52d9\u7d44\u4e2d\u63d0\u4f9b\u4e86 16K+ \u500b\u6e2c\u8a66\u5be6\u4f8b\uff0c\u4f8b\u5982\u6587\u5b57\u5206\u985e\u3001\u6a5f\u5668\u7ffb\u8b6f\u3001\u95dc\u4fc2\u62bd\u53d6\u3001\u95b1\u8b80\u7406\u89e3\u548c\u6587\u5b57\u751f\u6210\u3002\u5728 CFLUE \u4e0a\uff0c\u6211\u5011\u5c0d\u4ee3\u8868\u6027 LLM \u9032\u884c\u4e86\u5fb9\u5e95\u7684\u8a55\u4f30\u3002\u7d50\u679c\u8868\u660e\uff0c\u53ea\u6709 GPT-4 \u548c GPT-4-turbo \u5728\u77e5\u8b58\u8a55\u4f30\u7684\u7b54\u6848\u9810\u6e2c\u4e2d\u9054\u5230 60% \u4ee5\u4e0a\u7684\u6e96\u78ba\u5ea6\uff0c\u9019\u8868\u660e\u7576\u524d LLM \u4ecd\u6709\u5f88\u5927\u7684\u6539\u9032\u7a7a\u9593\u3002\u5728\u61c9\u7528\u8a55\u4f30\u4e2d\uff0c\u5118\u7ba1 GPT-4 \u548c GPT-4-turbo \u662f\u8868\u73fe\u6700\u597d\u7684\u5169\u500b\uff0c\u4f46\u5b83\u5011\u76f8\u8f03\u65bc\u8f15\u91cf\u7d1a LLM \u7684\u986f\u8457\u512a\u52e2\u5df2\u660e\u986f\u964d\u4f4e\u3002\u8207 CFLUE \u76f8\u95dc\u7684\u8cc7\u6599\u96c6\u548c\u8173\u672c\u53ef\u5728 https://github.com/aliyun/cflue \u516c\u958b\u53d6\u5f97\u3002", "author": "Jie Zhu et.al.", "authors": "Jie Zhu, Junhui Li, Yalong Wen, Lifan Guo", "id": "2405.10542v1", "paper_url": "http://arxiv.org/abs/2405.10542v1", "repo": "https://github.com/aliyun/cflue"}}