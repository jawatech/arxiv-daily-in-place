{"2501.09291": {"publish_time": "2025-01-16", "title": "LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport", "paper_summary": "Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.", "paper_summary_zh": "\u81ea\u52d5\u5316\u97f3\u8a0a\u5b57\u5e55\u88fd\u4f5c\u662f\u4e00\u7a2e\u70ba\u97f3\u8a0a\u5167\u5bb9\u7522\u751f\u6587\u5b57\u63cf\u8ff0\u7684\u4efb\u52d9\uff0c\u800c\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u63a2\u8a0e\u4f7f\u7528\u8996\u89ba\u8cc7\u8a0a\u4f86\u63d0\u5347\u5b57\u5e55\u54c1\u8cea\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u6280\u8853\u901a\u5e38\u7121\u6cd5\u6709\u6548\u878d\u5408\u97f3\u8a0a\u548c\u8996\u89ba\u8cc7\u6599\uff0c\u932f\u5931\u4e86\u5404\u500b\u65b9\u5f0f\u7684\u91cd\u8981\u8a9e\u610f\u63d0\u793a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 LAVCap\uff0c\u4e00\u500b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u70ba\u57fa\u790e\u7684\u97f3\u8a0a\u8996\u89ba\u5b57\u5e55\u88fd\u4f5c\u67b6\u69cb\uff0c\u5b83\u6709\u6548\u5730\u5c07\u8996\u89ba\u8cc7\u8a0a\u8207\u97f3\u8a0a\u6574\u5408\uff0c\u4ee5\u63d0\u5347\u97f3\u8a0a\u5b57\u5e55\u88fd\u4f5c\u7684\u8868\u73fe\u3002LAVCap \u63a1\u7528\u57fa\u65bc\u6700\u512a\u50b3\u8f38\u7684\u5c0d\u9f4a\u640d\u5931\u4f86\u6a4b\u63a5\u97f3\u8a0a\u548c\u8996\u89ba\u7279\u5fb5\u4e4b\u9593\u7684\u65b9\u5f0f\u5dee\u8ddd\uff0c\u9032\u800c\u5be6\u73fe\u66f4\u6709\u6548\u7684\u8a9e\u610f\u8403\u53d6\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u6700\u512a\u50b3\u8f38\u6ce8\u610f\u529b\u6a21\u7d44\uff0c\u5b83\u4f7f\u7528\u6700\u512a\u50b3\u8f38\u5206\u914d\u5716\u4f86\u63d0\u5347\u97f3\u8a0a\u8996\u89ba\u878d\u5408\u3002\u7d50\u5408\u6700\u512a\u8a13\u7df4\u7b56\u7565\uff0c\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u67b6\u69cb\u7684\u5404\u500b\u7d44\u6210\u90e8\u5206\u90fd\u5341\u5206\u6709\u6548\u3002LAVCap \u5728 AudioCaps \u8cc7\u6599\u96c6\u4e0a\u512a\u65bc\u73fe\u6709\u7684\u6700\u5148\u9032\u6280\u8853\uff0c\u800c\u4e14\u4e26\u672a\u4f9d\u8cf4\u5927\u578b\u8cc7\u6599\u96c6\u6216\u5f8c\u8655\u7406\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap \u53d6\u5f97\u3002", "author": "Kyeongha Rho et.al.", "authors": "Kyeongha Rho, Hyeongkeun Lee, Valentio Iverson, Joon Son Chung", "id": "2501.09291v1", "paper_url": "http://arxiv.org/abs/2501.09291v1", "repo": "https://github.com/naver-intel-co-lab/gaudi-lavcap"}}