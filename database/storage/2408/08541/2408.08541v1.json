{"2408.08541": {"publish_time": "2024-08-16", "title": "Where is the signal in tokenization space?", "paper_summary": "Large Language Models (LLMs) are typically shipped with tokenizers that\ndeterministically encode text into so-called canonical token sequences, to\nwhich the LLMs assign probability values. One common assumption is that the\nprobability of a piece of text is the probability of its canonical token\nsequence. However, the tokenization of a string is not unique: e.g., the Llama2\ntokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same\ntext. In this paper, we study non-canonical tokenizations. We prove that, given\na string, it is computationally hard to find the most likely tokenization for\nan autoregressive LLM, as well as to compute the marginal probability over all\npossible tokenizations. We then show how the marginal is, in most cases,\nindistinguishable from the canonical probability. Surprisingly, we then\nempirically demonstrate the existence of a significant amount of signal hidden\nwithin tokenization space. Notably, by simply aggregating the probabilities of\nnon-canonical tokenizations, we achieve improvements across a range of LLM\nevaluation benchmarks for a variety of architectures, including transformers\nand state space models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u901a\u5e38\u9644\u5e36\u7528\u65bc\u5c07\u6587\u5b57\u78ba\u5b9a\u6027\u7de8\u78bc\u70ba\u6240\u8b02\u7684\u6b63\u898f\u7b26\u865f\u5e8f\u5217\u7684\u7b26\u865f\u5316\u5668\uff0cLLM \u6703\u70ba\u5176\u6307\u5b9a\u6a5f\u7387\u503c\u3002\u4e00\u500b\u5e38\u898b\u7684\u5047\u8a2d\u662f\uff0c\u4e00\u6bb5\u6587\u5b57\u7684\u6a5f\u7387\u5c31\u662f\u5176\u6b63\u898f\u7b26\u865f\u5e8f\u5217\u7684\u6a5f\u7387\u3002\u7136\u800c\uff0c\u5b57\u4e32\u7684\u7b26\u865f\u5316\u4e26\u4e0d\u552f\u4e00\uff1a\u4f8b\u5982\uff0cLlama2 \u7b26\u865f\u5316\u5668\u5c07\u7b26\u865f\u7de8\u78bc\u70ba [Tok,ens]\uff0c\u4f46 [Tok,en,s] \u4e5f\u4ee3\u8868\u76f8\u540c\u7684\u6587\u5b57\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u975e\u6b63\u898f\u7b26\u865f\u5316\u3002\u6211\u5011\u8b49\u660e\uff0c\u7d66\u5b9a\u4e00\u500b\u5b57\u4e32\uff0c\u5c0d\u65bc\u81ea\u8ff4\u6b78 LLM \u4f86\u8aaa\uff0c\u8981\u627e\u51fa\u6700\u53ef\u80fd\u7684\u7b26\u865f\u5316\u5728\u8a08\u7b97\u4e0a\u5f88\u56f0\u96e3\uff0c\u800c\u4e14\u8981\u8a08\u7b97\u6240\u6709\u53ef\u80fd\u7684\u7b26\u865f\u5316\u7684\u908a\u969b\u6a5f\u7387\u4e5f\u5f88\u56f0\u96e3\u3002\u7136\u5f8c\u6211\u5011\u5c55\u793a\u908a\u969b\u6a5f\u7387\u5728\u591a\u6578\u60c5\u6cc1\u4e0b\u8207\u6b63\u898f\u6a5f\u7387\u7121\u6cd5\u5340\u5206\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u63a5\u8457\u4ee5\u7d93\u9a57\u65b9\u5f0f\u8b49\u660e\u7b26\u865f\u5316\u7a7a\u9593\u4e2d\u96b1\u85cf\u8457\u5927\u91cf\u7684\u8a0a\u865f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u53ea\u8981\u7c21\u55ae\u5730\u5f59\u7e3d\u975e\u6b63\u898f\u7b26\u865f\u5316\u7684\u6a5f\u7387\uff0c\u6211\u5011\u5c31\u80fd\u5728\u5404\u7a2e LLM \u8a55\u4f30\u57fa\u6e96\u4e2d\u7372\u5f97\u9032\u6b65\uff0c\u5305\u62ecTransformer\u548c\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u7b49\u5404\u7a2e\u67b6\u69cb\u3002", "author": "Renato Lui Geh et.al.", "authors": "Renato Lui Geh, Honghua Zhang, Kareem Ahmed, Benjie Wang, Guy Van den Broeck", "id": "2408.08541v1", "paper_url": "http://arxiv.org/abs/2408.08541v1", "repo": "null"}}