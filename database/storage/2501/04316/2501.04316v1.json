{"2501.04316": {"publish_time": "2025-01-08", "title": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring Contexts", "paper_summary": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making and\noutcomes remains understudied, particularly in generative settings. In this\nwork, we examine the fairness of LLM-based hiring systems through two\nreal-world tasks: resume summarization and retrieval. By constructing a\nsynthetic resume dataset and curating job postings, we investigate whether\nmodel behavior differs across demographic groups and is sensitive to\ndemographic perturbations. Our findings reveal that race-based differences\nappear in approximately 10% of generated summaries, while gender-based\ndifferences occur in only 1%. In the retrieval setting, all evaluated models\ndisplay non-uniform selection patterns across demographic groups and exhibit\nhigh sensitivity to both gender and race-based perturbations. Surprisingly,\nretrieval models demonstrate comparable sensitivity to non-demographic changes,\nsuggesting that fairness issues may stem, in part, from general brittleness\nissues. Overall, our results indicate that LLM-based hiring systems, especially\nat the retrieval stage, can exhibit notable biases that lead to discriminatory\noutcomes in real-world contexts.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)  zunehmend in hochkar\u00e4tigen Anwendungen wie der Personalbeschaffung eingesetzt, doch ihr Potenzial f\u00fcr unfaire Entscheidungsfindung und Ergebnisse bleibt unerforscht, insbesondere in generativen Umgebungen. In dieser Arbeit untersuchen wir die Fairness von LLM-basierten Einstellungssystemen anhand von zwei realen Aufgaben: Lebenslaufzusammenfassung und -abruf. Durch die Erstellung eines synthetischen Lebenslaufdatensatzes und die Zusammenstellung von Stellenausschreibungen untersuchen wir, ob sich das Modellverhalten zwischen demografischen Gruppen unterscheidet und f\u00fcr demografische St\u00f6rungen empfindlich ist. Unsere Ergebnisse zeigen, dass rassenbedingte Unterschiede in etwa 10 % der generierten Zusammenfassungen auftreten, w\u00e4hrend geschlechtsspezifische Unterschiede nur in 1 % auftreten. In der Abrufumgebung zeigen alle bewerteten Modelle ungleichm\u00e4\u00dfige Auswahlmuster \u00fcber demografische Gruppen hinweg und weisen eine hohe Empfindlichkeit sowohl gegen\u00fcber geschlechtsspezifischen als auch rassenbedingten St\u00f6rungen auf. \u00dcberraschenderweise zeigen Abrufmodelle eine vergleichbare Empfindlichkeit gegen\u00fcber nicht-demografischen Ver\u00e4nderungen, was darauf hindeutet, dass Fairnessprobleme teilweise auf allgemeine Spr\u00f6digkeitsprobleme zur\u00fcckzuf\u00fchren sein k\u00f6nnten. Insgesamt deuten unsere Ergebnisse darauf hin, dass LLM-basierte Einstellungssysteme, insbesondere in der Abrufphase, erhebliche Verzerrungen aufweisen k\u00f6nnen, die in realen Kontexten zu diskriminierenden Ergebnissen f\u00fchren.", "author": "Preethi Seshadri et.al.", "authors": "Preethi Seshadri, Seraphina Goldfarb-Tarrant", "id": "2501.04316v1", "paper_url": "http://arxiv.org/abs/2501.04316v1", "repo": "null"}}