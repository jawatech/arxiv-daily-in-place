{"2501.07086": {"publish_time": "2025-01-13", "title": "Boosting Text-To-Image Generation via Multilingual Prompting in Large Multimodal Models", "paper_summary": "Previous work on augmenting large multimodal models (LMMs) for text-to-image\n(T2I) generation has focused on enriching the input space of in-context\nlearning (ICL). This includes providing a few demonstrations and optimizing\nimage descriptions to be more detailed and logical. However, as demand for more\ncomplex and flexible image descriptions grows, enhancing comprehension of input\ntext within the ICL paradigm remains a critical yet underexplored area. In this\nwork, we extend this line of research by constructing parallel multilingual\nprompts aimed at harnessing the multilingual capabilities of LMMs. More\nspecifically, we translate the input text into several languages and provide\nthe models with both the original text and the translations. Experiments on two\nLMMs across 3 benchmarks show that our method, PMT2I, achieves superior\nperformance in general, compositional, and fine-grained assessments, especially\nin human preference alignment. Additionally, with its advantage of generating\nmore diverse images, PMT2I significantly outperforms baseline prompts when\nincorporated with reranking methods. Our code and parallel multilingual data\ncan be found at https://github.com/takagi97/PMT2I.", "paper_summary_zh": "\u5148\u524d\u7684\u7814\u7a76\u96c6\u4e2d\u65bc\u64f4\u5145\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u4ee5\u9032\u884c\u6587\u5b57\u8f49\u5f71\u50cf (T2I) \u751f\u6210\uff0c\u91cd\u9ede\u5728\u65bc\u8c50\u5bcc\u60c5\u5883\u5b78\u7fd2 (ICL) \u7684\u8f38\u5165\u7a7a\u9593\u3002\u9019\u5305\u62ec\u63d0\u4f9b\u4e00\u4e9b\u793a\u7bc4\u548c\u6700\u4f73\u5316\u5f71\u50cf\u63cf\u8ff0\uff0c\u4f7f\u5176\u66f4\u8a73\u7d30\u4e14\u66f4\u5177\u908f\u8f2f\u6027\u3002\u7136\u800c\uff0c\u96a8\u8457\u5c0d\u66f4\u8907\u96dc\u4e14\u66f4\u9748\u6d3b\u7684\u5f71\u50cf\u63cf\u8ff0\u7684\u9700\u6c42\u589e\u52a0\uff0c\u5728 ICL \u5178\u7bc4\u4e2d\u589e\u5f37\u5c0d\u8f38\u5165\u6587\u5b57\u7684\u7406\u89e3\u4ecd\u7136\u662f\u4e00\u500b\u95dc\u9375\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u9818\u57df\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5efa\u69cb\u5e73\u884c\u591a\u8a9e\u8a00\u63d0\u793a\u4f86\u64f4\u5c55\u9019\u689d\u7814\u7a76\u8def\u7dda\uff0c\u65e8\u5728\u5229\u7528 LMM \u7684\u591a\u8a9e\u8a00\u80fd\u529b\u3002\u66f4\u5177\u9ad4\u5730\u8aaa\uff0c\u6211\u5011\u5c07\u8f38\u5165\u6587\u5b57\u7ffb\u8b6f\u6210\u591a\u7a2e\u8a9e\u8a00\uff0c\u4e26\u5411\u6a21\u578b\u63d0\u4f9b\u539f\u59cb\u6587\u5b57\u548c\u7ffb\u8b6f\u3002\u5728 3 \u500b\u57fa\u6e96\u4e0a\u5c0d\u5169\u500b LMM \u9032\u884c\u7684\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684 PMT2I \u65b9\u6cd5\u5728\u4e00\u822c\u3001\u69cb\u6210\u548c\u7d30\u7dfb\u7684\u8a55\u4f30\u4e2d\u90fd\u80fd\u7372\u5f97\u512a\u7570\u7684\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u65b9\u9762\u3002\u6b64\u5916\uff0cPMT2I \u5177\u6709\u7522\u751f\u66f4\u591a\u6a23\u5316\u5f71\u50cf\u7684\u512a\u52e2\uff0c\u5728\u8207\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u7d50\u5408\u4f7f\u7528\u6642\uff0c\u5176\u6548\u80fd\u986f\u8457\u512a\u65bc\u57fa\u7dda\u63d0\u793a\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u5e73\u884c\u591a\u8a9e\u8a00\u8cc7\u6599\u53ef\u4ee5\u5728 https://github.com/takagi97/PMT2I \u627e\u5230\u3002", "author": "Yongyu Mu et.al.", "authors": "Yongyu Mu, Hengyu Li, Junxin Wang, Xiaoxuan Zhou, Chenglong Wang, Yingfeng Luo, Qiaozhi He, Tong Xiao, Guocheng Chen, Jingbo Zhu", "id": "2501.07086v1", "paper_url": "http://arxiv.org/abs/2501.07086v1", "repo": "https://github.com/takagi97/pmt2i"}}