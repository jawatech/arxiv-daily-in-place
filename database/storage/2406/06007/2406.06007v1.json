{"2406.06007": {"publish_time": "2024-06-10", "title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models", "paper_summary": "Artificial intelligence has significantly impacted medical applications,\nparticularly with the advent of Medical Large Vision Language Models\n(Med-LVLMs), sparking optimism for the future of automated and personalized\nhealthcare. However, the trustworthiness of Med-LVLMs remains unverified,\nposing significant risks for future model deployment. In this paper, we\nintroduce CARES and aim to comprehensively evaluate the Trustworthiness of\nMed-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs\nacross five dimensions, including trustfulness, fairness, safety, privacy, and\nrobustness. CARES comprises about 41K question-answer pairs in both closed and\nopen-ended formats, covering 16 medical image modalities and 27 anatomical\nregions. Our analysis reveals that the models consistently exhibit concerns\nregarding trustworthiness, often displaying factual inaccuracies and failing to\nmaintain fairness across different demographic groups. Furthermore, they are\nvulnerable to attacks and demonstrate a lack of privacy awareness. We publicly\nrelease our benchmark and code in https://github.com/richard-peng-xia/CARES.", "paper_summary_zh": "\u4eba\u5de5\u667a\u80fd\u5df2\u986f\u8457\u5f71\u97ff\u91ab\u7642\u61c9\u7528\uff0c\n\u5c24\u5176\u662f\u5728\u91ab\u7642\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (Med-LVLMs) \u51fa\u73fe\u5f8c\uff0c\u70ba\u81ea\u52d5\u5316\u548c\u500b\u4eba\u5316\u91ab\u7642\u4fdd\u5065\u7684\u672a\u4f86\u5e36\u4f86\u6a02\u89c0\u3002\u7136\u800c\uff0cMed-LVLMs \u7684\u53ef\u4fe1\u5ea6\u4ecd\u672a\u5f97\u5230\u9a57\u8b49\uff0c\u5c0d\u672a\u4f86\u7684\u6a21\u578b\u90e8\u7f72\u69cb\u6210\u91cd\u5927\u98a8\u96aa\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 CARES\uff0c\u65e8\u5728\u5168\u9762\u8a55\u4f30 Med-LVLMs \u5728\u91ab\u7642\u9818\u57df\u7684\u53ef\u4fe1\u5ea6\u3002\u6211\u5011\u5f9e\u53ef\u4fe1\u5ea6\u3001\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u3001\u96b1\u79c1\u548c\u7a69\u5065\u6027\u7b49\u4e94\u500b\u9762\u5411\u8a55\u4f30 Med-LVLMs \u7684\u53ef\u4fe1\u5ea6\u3002CARES \u5305\u542b\u7d04 41K \u500b\u5c01\u9589\u5f0f\u548c\u958b\u653e\u5f0f\u683c\u5f0f\u7684\u554f\u984c\u89e3\u7b54\u914d\u5c0d\uff0c\u6db5\u84cb 16 \u7a2e\u91ab\u5b78\u5f71\u50cf\u6a21\u5f0f\u548c 27 \u500b\u89e3\u5256\u5340\u57df\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u9019\u4e9b\u6a21\u578b\u59cb\u7d42\u8868\u73fe\u51fa\u95dc\u65bc\u53ef\u4fe1\u5ea6\u7684\u7591\u616e\uff0c\u7d93\u5e38\u986f\u793a\u51fa\u4e8b\u5be6\u4e0a\u7684\u4e0d\u6e96\u78ba\u6027\uff0c\u4e26\u4e14\u7121\u6cd5\u5728\u4e0d\u540c\u7684\u4eba\u53e3\u7fa4\u9ad4\u4e2d\u4fdd\u6301\u516c\u5e73\u6027\u3002\u6b64\u5916\uff0c\u5b83\u5011\u5bb9\u6613\u53d7\u5230\u653b\u64ca\uff0c\u4e26\u4e14\u7f3a\u4e4f\u96b1\u79c1\u610f\u8b58\u3002\u6211\u5011\u516c\u958b\u767c\u5e03\u6211\u5011\u7684\u57fa\u6e96\u548c\u4ee3\u78bc\uff0c\u7db2\u5740\u70ba https://github.com/richard-peng-xia/CARES\u3002", "author": "Peng Xia et.al.", "authors": "Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, Wenhao Zheng, Zhaoyang Wang, Xiao Wang, Xuchao Zhang, Chetan Bansal, Marc Niethammer, Junzhou Huang, Hongtu Zhu, Yun Li, Jimeng Sun, Zongyuan Ge, Gang Li, James Zou, Huaxiu Yao", "id": "2406.06007v1", "paper_url": "http://arxiv.org/abs/2406.06007v1", "repo": "https://github.com/richard-peng-xia/cares"}}