{"2411.14698": {"publish_time": "2024-11-22", "title": "Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation", "paper_summary": "Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,\noften achieving state-of-the-art performance in various tasks. However, their\nsubstantial computational and memory demands, due to billions of parameters,\nhinder deployment in resource-constrained environments. A promising solution is\nknowledge distillation, where LLMs transfer reasoning capabilities to Small\nLanguage Models (SLMs, $\\le$ 1B parameters), enabling wider deployment on\nlow-resource devices. Existing methods primarily focus on generating\nhigh-quality reasoning rationales for distillation datasets but often neglect\nthe critical role of data quantity and quality. To address these challenges, we\npropose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'\nmathematical reasoning capabilities. In the initialization stage, a\ndistillation dataset is constructed by prompting LLMs to pair mathematical\nproblems with corresponding reasoning rationales. We classify problems into\neasy and hard categories based on SLM performance. For easy problems, LLMs\ngenerate more complex variations, while for hard problems, new questions of\nsimilar complexity are synthesized. In addition, we propose a multi-round\ndistillation paradigm to iteratively enrich the distillation datasets, thereby\nprogressively improving the mathematical reasoning abilities of SLMs.\nExperimental results demonstrate that our method can make SLMs achieve SOTA\nmathematical reasoning performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u51fa\u8272\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5404\u9805\u4efb\u52d9\u4e2d\u7d93\u5e38\u9054\u5230\u6700\u5148\u9032\u7684\u8868\u73fe\u3002\u7136\u800c\uff0c\u7531\u65bc\u6578\u5341\u5104\u500b\u53c3\u6578\uff0c\u5b83\u5011\u9f90\u5927\u7684\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\u963b\u7919\u4e86\u5728\u8cc7\u6e90\u53d7\u9650\u74b0\u5883\u4e2d\u7684\u90e8\u7f72\u3002\u4e00\u500b\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\u662f\u77e5\u8b58\u84b8\u993e\uff0c\u5176\u4e2d LLM \u5c07\u63a8\u7406\u80fd\u529b\u8f49\u79fb\u5230\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM\uff0c$\\le$ 1B \u53c3\u6578)\uff0c\u5f9e\u800c\u80fd\u5920\u5728\u4f4e\u8cc7\u6e90\u88dd\u7f6e\u4e0a\u66f4\u5ee3\u6cdb\u5730\u90e8\u7f72\u3002\u73fe\u6709\u65b9\u6cd5\u4e3b\u8981\u5c08\u6ce8\u65bc\u70ba\u84b8\u993e\u8cc7\u6599\u96c6\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u63a8\u7406\u4f9d\u64da\uff0c\u4f46\u5e38\u5e38\u5ffd\u7565\u8cc7\u6599\u6578\u91cf\u548c\u54c1\u8cea\u7684\u95dc\u9375\u4f5c\u7528\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u56de\u994b\u9a45\u52d5\u84b8\u993e (FDD) \u67b6\u69cb\uff0c\u4ee5\u589e\u5f37 SLM \u7684\u6578\u5b78\u63a8\u7406\u80fd\u529b\u3002\u5728\u521d\u59cb\u5316\u968e\u6bb5\uff0c\u901a\u904e\u63d0\u793a LLM \u5c07\u6578\u5b78\u554f\u984c\u8207\u5c0d\u61c9\u7684\u63a8\u7406\u4f9d\u64da\u914d\u5c0d\u4f86\u5efa\u69cb\u4e00\u500b\u84b8\u993e\u8cc7\u6599\u96c6\u3002\u6211\u5011\u6839\u64da SLM \u7684\u8868\u73fe\u5c07\u554f\u984c\u5206\u985e\u70ba\u5bb9\u6613\u548c\u56f0\u96e3\u7684\u985e\u5225\u3002\u5c0d\u65bc\u5bb9\u6613\u7684\u554f\u984c\uff0cLLM \u6703\u7522\u751f\u66f4\u8907\u96dc\u7684\u8b8a\u5316\uff0c\u800c\u5c0d\u65bc\u56f0\u96e3\u7684\u554f\u984c\uff0c\u5247\u6703\u5408\u6210\u985e\u4f3c\u8907\u96dc\u5ea6\u7684\u65b0\u554f\u984c\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u591a\u8f2a\u84b8\u993e\u7bc4\u4f8b\uff0c\u4ee5\u53cd\u8986\u8c50\u5bcc\u84b8\u993e\u8cc7\u6599\u96c6\uff0c\u5f9e\u800c\u9010\u6b65\u63d0\u5347 SLM \u7684\u6578\u5b78\u63a8\u7406\u80fd\u529b\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u8b93 SLM \u9054\u5230\u6578\u5b78\u63a8\u7406\u8868\u73fe\u7684 SOTA \u6c34\u6e96\u3002", "author": "Xunyu Zhu et.al.", "authors": "Xunyu Zhu, Jian Li, Can Ma, Weiping Wang", "id": "2411.14698v1", "paper_url": "http://arxiv.org/abs/2411.14698v1", "repo": "null"}}