{"2405.19327": {"publish_time": "2024-05-29", "title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series", "paper_summary": "Large Language Models (LLMs) have made great strides in recent years to\nachieve unprecedented performance across different tasks. However, due to\ncommercial interest, the most competitive models like GPT, Gemini, and Claude\nhave been gated behind proprietary interfaces without disclosing the training\ndetails. Recently, many institutions have open-sourced several strong LLMs like\nLLaMA-3, comparable to existing closed-source LLMs. However, only the model's\nweights are provided with most details (e.g., intermediate checkpoints,\npre-training corpus, and training code, etc.) being undisclosed. To improve the\ntransparency of LLMs, the research community has formed to open-source truly\nopen LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training\ncorpus and training code) are being provided. These models have greatly\nadvanced the scientific study of these large models including their strengths,\nweaknesses, biases and risks. However, we observe that the existing truly open\nLLMs on reasoning, knowledge, and coding tasks are still inferior to existing\nstate-of-the-art LLMs with similar model sizes. To this end, we open-source\nMAP-Neo, a highly capable and transparent bilingual language model with 7B\nparameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the\nfirst fully open-sourced bilingual LLM with comparable performance compared to\nexisting state-of-the-art LLMs. Moreover, we open-source all details to\nreproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning\npipeline, checkpoints, and well-optimized training/evaluation framework are\nprovided. Finally, we hope our MAP-Neo will enhance and strengthen the open\nresearch community and inspire more innovations and creativities to facilitate\nthe further improvements of LLMs.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8fd1\u5e74\u4f86\u5728\u5404\u9805\u4efb\u52d9\u4e0a\u53d6\u5f97\u4e86\u9577\u8db3\u9032\u6b65\uff0c\u9054\u5230\u4e86\u524d\u6240\u672a\u6709\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u7531\u65bc\u5546\u696d\u5229\u76ca\uff0c\u50cf GPT\u3001Gemini \u548c Claude \u7b49\u6700\u5177\u7af6\u722d\u529b\u7684\u6a21\u578b\u90fd\u88ab\u96b1\u85cf\u5728\u5c08\u6709\u4ecb\u9762\u4e4b\u5f8c\uff0c\u800c\u672a\u516c\u958b\u8a13\u7df4\u7d30\u7bc0\u3002\u6700\u8fd1\uff0c\u8a31\u591a\u6a5f\u69cb\u958b\u653e\u4e86\u591a\u500b\u5f37\u5927\u7684 LLM \u7684\u539f\u59cb\u78bc\uff0c\u4f8b\u5982\u8207\u73fe\u6709\u7684\u9589\u6e90 LLM \u76f8\u7576\u7684 LLaMA-3\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u7d30\u7bc0\uff08\u4f8b\u5982\u4e2d\u9593\u6aa2\u67e5\u9ede\u3001\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u548c\u8a13\u7df4\u7a0b\u5f0f\u78bc\u7b49\uff09\u90fd\u672a\u516c\u958b\uff0c\u53ea\u6709\u6a21\u578b\u6b0a\u91cd\u88ab\u63d0\u4f9b\u3002\u70ba\u4e86\u63d0\u5347 LLM \u7684\u900f\u660e\u5ea6\uff0c\u7814\u7a76\u793e\u7fa4\u5df2\u7d44\u6210\u958b\u653e\u539f\u59cb\u78bc\u771f\u6b63\u958b\u653e\u7684 LLM\uff08\u4f8b\u5982 Pythia\u3001Amber\u3001OLMo\uff09\uff0c\u5176\u4e2d\u63d0\u4f9b\u4e86\u66f4\u591a\u7d30\u7bc0\uff08\u4f8b\u5982\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u548c\u8a13\u7df4\u7a0b\u5f0f\u78bc\uff09\u3002\u9019\u4e9b\u6a21\u578b\u5927\u5e45\u63a8\u52d5\u4e86\u9019\u4e9b\u5927\u578b\u6a21\u578b\u7684\u79d1\u5b78\u7814\u7a76\uff0c\u5305\u62ec\u5b83\u5011\u7684\u512a\u9ede\u3001\u7f3a\u9ede\u3001\u504f\u5dee\u548c\u98a8\u96aa\u3002\u7136\u800c\uff0c\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u73fe\u6709\u7684\u771f\u6b63\u958b\u653e\u7684 LLM \u5728\u63a8\u7406\u3001\u77e5\u8b58\u548c\u7de8\u78bc\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u4ecd\u7136\u905c\u65bc\u73fe\u6709\u5177\u6709\u985e\u4f3c\u6a21\u578b\u898f\u6a21\u7684\u6700\u65b0 LLM\u3002\u70ba\u6b64\uff0c\u6211\u5011\u958b\u653e\u4e86 MAP-Neo \u7684\u539f\u59cb\u78bc\uff0c\u9019\u662f\u4e00\u500b\u529f\u80fd\u5f37\u5927\u4e14\u900f\u660e\u7684\u96d9\u8a9e\u8a9e\u8a00\u6a21\u578b\uff0c\u5177\u6709 7B \u500b\u53c3\u6578\uff0c\u4e26\u5f9e 4.5T \u7684\u9ad8\u54c1\u8cea\u4ee3\u5e63\u4e2d\u5f9e\u982d\u8a13\u7df4\u3002\u6211\u5011\u7684 MAP-Neo \u662f\u7b2c\u4e00\u500b\u5b8c\u5168\u958b\u653e\u539f\u59cb\u78bc\u7684\u96d9\u8a9e LLM\uff0c\u5176\u6548\u80fd\u53ef\u8207\u73fe\u6709\u7684\u6700\u65b0 LLM \u76f8\u5ab2\u7f8e\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u653e\u4e86\u6240\u6709\u7d30\u7bc0\u4ee5\u91cd\u73fe MAP-Neo\uff0c\u5176\u4e2d\u63d0\u4f9b\u4e86\u6e05\u7406\u904e\u7684\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u3001\u8cc7\u6599\u6e05\u7406\u7ba1\u9053\u3001\u6aa2\u67e5\u9ede\u548c\u7d93\u904e\u512a\u5316\u7684\u8a13\u7df4/\u8a55\u4f30\u6846\u67b6\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684 MAP-Neo \u80fd\u5920\u63d0\u5347\u548c\u5f37\u5316\u958b\u653e\u7684\u7814\u7a76\u793e\u7fa4\uff0c\u4e26\u6fc0\u767c\u66f4\u591a\u5275\u65b0\u548c\u5275\u610f\uff0c\u4ee5\u4fc3\u9032 LLM \u7684\u9032\u4e00\u6b65\u6539\u9032\u3002</paragraph>", "author": "Ge Zhang et.al.", "authors": "Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen", "id": "2405.19327v1", "paper_url": "http://arxiv.org/abs/2405.19327v1", "repo": "null"}}