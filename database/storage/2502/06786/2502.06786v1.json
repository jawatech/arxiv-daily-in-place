{"2502.06786": {"publish_time": "2025-02-10", "title": "Matryoshka Quantization", "paper_summary": "Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to $10\\%$ more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.", "paper_summary_zh": "\u91cf\u5316\u6a21\u578b\u6743\u91cd\u5bf9\u4e8e\u964d\u4f4e\u5927\u578b\u6a21\u578b\u7684\u901a\u4fe1\u548c\u63a8\u7406\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u91cf\u5316\u6a21\u578b\uff08\u7279\u522b\u662f\u91cf\u5316\u4e3a int4 \u6216 int2 \u7b49\u4f4e\u7cbe\u5ea6\uff09\u9700\u8981\u5728\u6a21\u578b\u8d28\u91cf\u4e0a\u8fdb\u884c\u6743\u8861\uff1b\u4f17\u6240\u5468\u77e5\uff0cint2 \u5c24\u5176\u4f1a\u4e25\u91cd\u964d\u4f4e\u6a21\u578b\u8d28\u91cf\u3002\u56e0\u6b64\uff0c\u4ece\u4e1a\u8005\u901a\u5e38\u88ab\u8feb\u7ef4\u62a4\u5177\u6709\u4e0d\u540c\u91cf\u5316\u7ea7\u522b\u7684\u591a\u4e2a\u6a21\u578b\uff0c\u6216\u63d0\u4f9b\u6700\u80fd\u6ee1\u8db3\u8d28\u91cf\u5ef6\u8fdf\u6743\u8861\u7684\u5355\u4e2a\u6a21\u578b\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u8bf8\u5982 int8 \u4e4b\u7c7b\u7684\u6574\u6570\u6570\u636e\u7c7b\u578b\u5929\u751f\u5c31\u5177\u6709\u5d4c\u5957\uff08\u5957\u5a03\uff09\u7ed3\u6784\uff0c\u5176\u4e2d\u8f83\u5c0f\u7684\u4f4d\u5bbd\u6574\u6570\uff08\u5982 int4 \u6216 int2\uff09\u5d4c\u5957\u5728\u6700\u91cd\u8981\u7684\u4f4d\u4e2d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u5957\u5a03\u91cf\u5316 (MatQuant)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u91cf\u5316\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u9700\u8981\u591a\u4e2a\u91cf\u5316\u6a21\u578b\u7684\u6311\u6218\u3002\u5b83\u5141\u8bb8\u8bad\u7ec3\u548c\u7ef4\u62a4\u4e00\u4e2a\u6a21\u578b\uff0c\u7136\u540e\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u7cbe\u5ea6\u7ea7\u522b\u63d0\u4f9b\u670d\u52a1\u3002\u6b64\u5916\uff0c\u7531\u4e8e MatQuant \u63d0\u4f9b\u7684\u8054\u5408\u8bad\u7ec3\u548c\u8054\u5408\u84b8\u998f\u6b63\u5219\u5316\uff0cMatQuant \u63d0\u53d6\u7684 int2 \u7cbe\u5ea6\u6a21\u578b\u6bd4\u6807\u51c6 int2 \u91cf\u5316\uff08\u4f7f\u7528 QAT \u6216 OmniQuant \u7b49\u6280\u672f\uff09\u51c6\u786e\u5ea6\u63d0\u9ad8\u4e86 10%\u3002\u8fd9\u4ee3\u8868\u4e86\u6a21\u578b\u91cf\u5316\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u4e8b\u5b9e\u8bc1\u660e\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u914d\u65b9\uff0cint2 FFN \u91cf\u5316\u7684 Gemma-2 9B \u6a21\u578b\u6bd4 int8 FFN \u91cf\u5316\u7684 Gemma-2 2B \u6a21\u578b\u66f4\u51c6\u786e\u3002", "author": "Pranav Nair et.al.", "authors": "Pranav Nair, Puranjay Datta, Jeff Dean, Prateek Jain, Aditya Kusupati", "id": "2502.06786v1", "paper_url": "http://arxiv.org/abs/2502.06786v1", "repo": "null"}}