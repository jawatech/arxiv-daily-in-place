{"2410.18570": {"publish_time": "2024-10-24", "title": "Zero-shot Object Navigation with Vision-Language Models Reasoning", "paper_summary": "Object navigation is crucial for robots, but traditional methods require\nsubstantial training data and cannot be generalized to unknown environments.\nZero-shot object navigation (ZSON) aims to address this challenge, allowing\nrobots to interact with unknown objects without specific training data.\nLanguage-driven zero-shot object navigation (L-ZSON) is an extension of ZSON\nthat incorporates natural language instructions to guide robot navigation and\ninteraction with objects. In this paper, we propose a novel Vision Language\nmodel with a Tree-of-thought Network (VLTNet) for L-ZSON. VLTNet comprises four\nmain modules: vision language model understanding, semantic mapping,\ntree-of-thought reasoning and exploration, and goal identification. Among these\nmodules, Tree-of-Thought (ToT) reasoning and exploration module serves as a\ncore component, innovatively using the ToT reasoning framework for navigation\nfrontier selection during robot exploration. Compared to conventional frontier\nselection without reasoning, navigation using ToT reasoning involves multi-path\nreasoning processes and backtracking when necessary, enabling globally informed\ndecision-making with higher accuracy. Experimental results on PASTURE and\nRoboTHOR benchmarks demonstrate the outstanding performance of our model in\nLZSON, particularly in scenarios involving complex natural language as target\ninstructions.", "paper_summary_zh": "\u7269\u9ad4\u5c0e\u822a\u5c0d\u65bc\u6a5f\u5668\u4eba\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u50b3\u7d71\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u8a13\u7df4\u6578\u64da\uff0c\u4e14\u7121\u6cd5\u63a8\u5ee3\u5230\u672a\u77e5\u7684\u74b0\u5883\u4e2d\u3002\u96f6\u6b21\u5b78\u7fd2\u7269\u9ad4\u5c0e\u822a (ZSON) \u65e8\u5728\u89e3\u6c7a\u9019\u500b\u6311\u6230\uff0c\u8b93\u6a5f\u5668\u4eba\u53ef\u4ee5\u5728\u6c92\u6709\u7279\u5b9a\u8a13\u7df4\u6578\u64da\u7684\u60c5\u6cc1\u4e0b\u8207\u672a\u77e5\u7684\u7269\u9ad4\u4e92\u52d5\u3002\u8a9e\u8a00\u9a45\u52d5\u7684\u96f6\u6b21\u5b78\u7fd2\u7269\u9ad4\u5c0e\u822a (L-ZSON) \u662f ZSON \u7684\u5ef6\u4f38\uff0c\u5b83\u7d50\u5408\u4e86\u81ea\u7136\u8a9e\u8a00\u6307\u4ee4\u4f86\u5f15\u5c0e\u6a5f\u5668\u4eba\u7684\u5c0e\u822a\u548c\u8207\u7269\u9ad4\u7684\u4e92\u52d5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7d50\u5408\u4e86\u601d\u8003\u6a39\u7db2\u8def (VLTNet) \u7684\u65b0\u7a4e\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u7528\u65bc L-ZSON\u3002VLTNet \u5305\u542b\u56db\u500b\u4e3b\u8981\u6a21\u7d44\uff1a\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7406\u89e3\u3001\u8a9e\u7fa9\u5c0d\u61c9\u3001\u601d\u8003\u6a39\u63a8\u7406\u8207\u63a2\u7d22\uff0c\u4ee5\u53ca\u76ee\u6a19\u8b58\u5225\u3002\u5728\u9019\u4e9b\u6a21\u7d44\u4e2d\uff0c\u601d\u8003\u6a39 (ToT) \u63a8\u7406\u8207\u63a2\u7d22\u6a21\u7d44\u4f5c\u70ba\u6838\u5fc3\u5143\u4ef6\uff0c\u5275\u65b0\u5730\u4f7f\u7528 ToT \u63a8\u7406\u67b6\u69cb\uff0c\u5728\u6a5f\u5668\u4eba\u63a2\u7d22\u671f\u9593\u9032\u884c\u5c0e\u822a\u524d\u6cbf\u9078\u64c7\u3002\u8207\u6c92\u6709\u63a8\u7406\u7684\u50b3\u7d71\u524d\u6cbf\u9078\u64c7\u76f8\u6bd4\uff0c\u4f7f\u7528 ToT \u63a8\u7406\u7684\u5c0e\u822a\u6d89\u53ca\u591a\u8def\u5f91\u63a8\u7406\u7a0b\u5e8f\uff0c\u4e26\u5728\u5fc5\u8981\u6642\u56de\u6eaf\uff0c\u5f9e\u800c\u5be6\u73fe\u5177\u6709\u66f4\u9ad8\u7cbe\u78ba\u5ea6\u7684\u5168\u5c40\u77e5\u60c5\u6c7a\u7b56\u3002\u5728 PASTURE \u548c RoboTHOR \u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u7684\u6a21\u578b\u5728 LZSON \u4e2d\u7684\u51fa\u8272\u8868\u73fe\uff0c\u7279\u5225\u662f\u5728\u6d89\u53ca\u8907\u96dc\u81ea\u7136\u8a9e\u8a00\u4f5c\u70ba\u76ee\u6a19\u6307\u4ee4\u7684\u60c5\u6cc1\u4e2d\u3002", "author": "Congcong Wen et.al.", "authors": "Congcong Wen, Yisiyuan Huang, Hao Huang, Yanjia Huang, Shuaihang Yuan, Yu Hao, Hui Lin, Yu-Shen Liu, Yi Fang", "id": "2410.18570v1", "paper_url": "http://arxiv.org/abs/2410.18570v1", "repo": "null"}}