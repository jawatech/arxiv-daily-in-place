{"2410.05261": {"publish_time": "2024-10-07", "title": "TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and Grounding with 16x Fewer Tokens", "paper_summary": "Reading dense text and locating objects within images are fundamental\nabilities for Large Vision-Language Models (LVLMs) tasked with advanced jobs.\nPrevious LVLMs, including superior proprietary models like GPT-4o, have\nstruggled to excel in both tasks simultaneously. Moreover, previous LVLMs with\nfine-grained perception cost thousands of tokens per image, making them\nresource-intensive. We present TextHawk2, a bilingual LVLM featuring efficient\nfine-grained perception and demonstrating cutting-edge performance across\ngeneral-purpose, OCR, and grounding tasks with 16 times fewer image tokens.\nCritical improvements include: (1) Token Compression: Building on the efficient\narchitecture of its predecessor, TextHawk2 significantly reduces the number of\ntokens per image by 16 times, facilitating training and deployment of the\nTextHawk series with minimal resources. (2) Visual Encoder Reinforcement: We\nenhance the visual encoder through LVLM co-training, unlocking its potential\nfor previously unseen tasks like Chinese OCR and grounding. (3) Data Diversity:\nWe maintain a comparable scale of 100 million samples while diversifying the\nsources of pre-training data. We assess TextHawk2 across multiple benchmarks,\nwhere it consistently delivers superior performance and outperforms\nclosed-source models of similar scale, such as achieving 78.4% accuracy on\nOCRBench, 81.4% accuracy on ChartQA, 89.6% ANLS on DocVQA, and 88.1%\naccuracy@0.5 on RefCOCOg-test.", "paper_summary_zh": "\u95b1\u8b80\u5bc6\u96c6\u6587\u5b57\u548c\u5b9a\u4f4d\u5f71\u50cf\u4e2d\u7684\u7269\u4ef6\u662f\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u57f7\u884c\u9032\u968e\u5de5\u4f5c\u6642\u7684\u57fa\u672c\u80fd\u529b\u3002\n\u5305\u542b GPT-4o \u7b49\u512a\u79c0\u5c08\u6709\u6a21\u578b\u5728\u5167\u7684\u5148\u524d\u7684 LVLMs \u5728\u540c\u6642\u57f7\u884c\u9019\u5169\u500b\u5de5\u4f5c\u6642\u90fd\u96e3\u4ee5\u8868\u73fe\u51fa\u8272\u3002\n\u6b64\u5916\uff0c\u5177\u6709\u7d30\u7dfb\u77e5\u89ba\u7684\u5148\u524d LVLMs \u6bcf\u5f35\u5f71\u50cf\u7684\u6210\u672c\u9ad8\u9054\u6578\u5343\u500b\u4ee3\u5e63\uff0c\u9019\u8b93\u5b83\u5011\u6d88\u8017\u5927\u91cf\u8cc7\u6e90\u3002\n\u6211\u5011\u63d0\u51fa TextHawk2\uff0c\u9019\u662f\u4e00\u500b\u5177\u5099\u9ad8\u6548\u7d30\u7dfb\u77e5\u89ba\u7684\u96d9\u8a9e LVLM\uff0c\u4e26\u5728\u4e00\u822c\u7528\u9014\u3001OCR \u548c\u57fa\u790e\u5de5\u4f5c\u4e2d\u5c55\u73fe\u51fa\u5148\u9032\u7684\u6548\u80fd\uff0c\u800c\u4e14\u5f71\u50cf\u4ee3\u5e63\u6e1b\u5c11\u4e86 16 \u500d\u3002\n\u91cd\u8981\u7684\u6539\u9032\u5305\u62ec\uff1a(1) \u4ee3\u5e63\u58d3\u7e2e\uff1a\u5efa\u69cb\u65bc\u5176\u524d\u8eab\u7684\u9ad8\u6548\u67b6\u69cb\u4e0a\uff0cTextHawk2 \u5c07\u6bcf\u5f35\u5f71\u50cf\u7684\u4ee3\u5e63\u6578\u91cf\u5927\u5e45\u6e1b\u5c11 16 \u500d\uff0c\u4ee5\u6700\u5c11\u7684\u8cc7\u6e90\u4fc3\u9032 TextHawk \u7cfb\u5217\u7684\u8a13\u7df4\u548c\u90e8\u7f72\u3002(2) \u8996\u89ba\u7de8\u78bc\u5668\u5f37\u5316\uff1a\u6211\u5011\u900f\u904e LVLM \u5171\u540c\u8a13\u7df4\u4f86\u5f37\u5316\u8996\u89ba\u7de8\u78bc\u5668\uff0c\u767c\u63ee\u5176\u5728\u5148\u524d\u672a\u898b\u7684\u5de5\u4f5c\uff08\u4f8b\u5982\u4e2d\u6587 OCR \u548c\u57fa\u790e\uff09\u4e2d\u7684\u6f5b\u529b\u3002(3) \u8cc7\u6599\u591a\u6a23\u6027\uff1a\u6211\u5011\u7dad\u6301 1 \u5104\u500b\u6a23\u672c\u7684\u53ef\u6bd4\u898f\u6a21\uff0c\u540c\u6642\u4f7f\u9810\u8a13\u7df4\u8cc7\u6599\u7684\u4f86\u6e90\u591a\u6a23\u5316\u3002\u6211\u5011\u5728\u591a\u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u8a55\u4f30 TextHawk2\uff0c\u5b83\u59cb\u7d42\u63d0\u4f9b\u512a\u7570\u7684\u6548\u80fd\uff0c\u4e26\u512a\u65bc\u898f\u6a21\u985e\u4f3c\u7684\u9589\u6e90\u6a21\u578b\uff0c\u4f8b\u5982\u5728 OCRBench \u4e0a\u9054\u5230 78.4% \u7684\u6e96\u78ba\u5ea6\u3001\u5728 ChartQA \u4e0a\u9054\u5230 81.4% \u7684\u6e96\u78ba\u5ea6\u3001\u5728 DocVQA \u4e0a\u9054\u5230 89.6% \u7684 ANLS\uff0c\u4ee5\u53ca\u5728 RefCOCOg-test \u4e0a\u9054\u5230 88.1% \u7684\u6e96\u78ba\u5ea6@0.5\u3002", "author": "Ya-Qi Yu et.al.", "authors": "Ya-Qi Yu, Minghui Liao, Jiwen Zhang, Jihao Wu", "id": "2410.05261v1", "paper_url": "http://arxiv.org/abs/2410.05261v1", "repo": "null"}}