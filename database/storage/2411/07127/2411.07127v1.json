{"2411.07127": {"publish_time": "2024-11-11", "title": "Benchmarking LLMs' Judgments with No Gold Standard", "paper_summary": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4ecb\u7d39 GEM\uff08\u4e92\u4fe1\u606f\u751f\u6210\u4f30\u8a08\u5668\uff09\uff0c\u4e00\u7a2e\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u8a9e\u8a00\u751f\u6210\u80fd\u529b\u7684\u8a55\u4f30\u6307\u6a19\uff0c\u7279\u5225\u662f\u5728\u751f\u6210\u8cc7\u8a0a\u6027\u5224\u65b7\u6642\uff0c\u7121\u9700\u9ec3\u91d1\u6a19\u6e96\u53c3\u8003\u3002GEM \u64f4\u5c55\u4e86\u6211\u5011\u53ef\u4ee5\u5c0d LLM \u751f\u6210\u6548\u80fd\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\u7684\u5834\u666f\uff0c\u5f9e\u50b3\u7d71\u5834\u666f\uff08\u4f8b\u5982\u6a5f\u5668\u7ffb\u8b6f\u548c\u6458\u8981\uff0c\u5176\u4e2d\u9ec3\u91d1\u6a19\u6e96\u53c3\u8003\u5f88\u5bb9\u6613\u53d6\u5f97\uff09\u5230\u6c92\u6709\u660e\u78ba\u9ec3\u91d1\u6a19\u6e96\u7684\u4e3b\u89c0\u4efb\u52d9\uff08\u4f8b\u5982\u5b78\u8853\u540c\u884c\u8a55\u5be9\uff09\u3002\n  GEM \u4f7f\u7528\u751f\u6210\u6a21\u578b\u4f86\u4f30\u8a08\u5019\u9078\u56de\u61c9\u548c\u53c3\u8003\u56de\u61c9\u4e4b\u9593\u7684\u4e92\u4fe1\u606f\uff0c\u800c\u4e0d\u8981\u6c42\u53c3\u8003\u662f\u9ec3\u91d1\u6a19\u6e96\u3002\u5728\u4eba\u985e\u6a19\u8a3b\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u4e2d\uff0c\u8207\u6700\u5148\u9032\u7684 GPT-4o Examiner \u76f8\u6bd4\uff0cGEM \u5c55\u793a\u51fa\u8207\u4eba\u985e\u8a55\u5206\u5177\u6709\u7af6\u722d\u529b\u7684\u95dc\u806f\u6027\uff0c\u4e26\u4e14\u512a\u65bc\u6240\u6709\u5176\u4ed6\u57fa\u7dda\u3002\u6b64\u5916\uff0cGEM \u5c0d\u7b56\u7565\u6027\u64cd\u4f5c\uff08\u4f8b\u5982\u6539\u5beb\u6216\u5ef6\u4f38\uff09\u5177\u6709\u66f4\u5f37\u5927\u7684\u7a69\u5065\u6027\uff0c\u9019\u4e9b\u64cd\u4f5c\u6703\u5728 GPT-4o Examiner \u4e0b\u4eba\u70ba\u5730\u63d0\u9ad8\u5206\u6578\u3002\n  \u6211\u5011\u9084\u63d0\u51fa\u4e86 GRE-bench\uff08\u751f\u6210\u8a55\u8ad6\u8a55\u4f30\u57fa\u6e96\uff09\uff0c\u5b83\u6839\u64da LLM \u751f\u6210\u9ad8\u54c1\u8cea\u5b78\u8853\u7814\u7a76\u8ad6\u6587\u540c\u884c\u8a55\u5be9\u7684\u80fd\u529b\u4f86\u8a55\u4f30 LLM\u3002\u7531\u65bc GRE-bench \u57fa\u65bc GEM\uff0c\u56e0\u6b64\u5b83\u7e7c\u627f\u4e86\u5176\u7a69\u5065\u6027\u3002\u6b64\u5916\uff0cGRE-bench \u900f\u904e\u6bcf\u5e74\u4f7f\u7528\u5927\u91cf\u65b0\u958b\u653e\u5b58\u53d6\u7814\u7a76\u8ad6\u6587\u548c\u540c\u884c\u8a55\u5be9\u4f86\u898f\u907f\u8cc7\u6599\u6c61\u67d3\u554f\u984c\uff08\u6216\u8cc7\u6599\u5916\u6d29\uff09\u3002\u6211\u5011\u5c55\u793a\u4e86 GRE-bench \u5728 ICLR2023 \u8cc7\u6599\u96c6\u4e0a\u4f7f\u7528\u5404\u7a2e\u6d41\u884c LLM \u7684\u540c\u884c\u8a55\u5be9\u80fd\u529b\u7684\u7d50\u679c\u3002</paragraph>", "author": "Shengwei Xu et.al.", "authors": "Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong", "id": "2411.07127v1", "paper_url": "http://arxiv.org/abs/2411.07127v1", "repo": "https://github.com/yx-lu/benchmarking-llms--judgments-with-no-gold-standard"}}