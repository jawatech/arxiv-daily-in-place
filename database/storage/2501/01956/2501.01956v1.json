{"2501.01956": {"publish_time": "2025-01-03", "title": "Metadata Conditioning Accelerates Language Model Pre-training", "paper_summary": "The vast diversity of styles, domains, and quality levels present in language\nmodel pre-training corpora is essential in developing general model\ncapabilities, but efficiently learning and deploying the correct behaviors\nexemplified in each of these heterogeneous data sources is challenging. To\naddress this, we propose a new method, termed Metadata Conditioning then\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\ntext during training and later uses a cooldown phase with only the standard\ntext, thereby enabling the model to function normally even without metadata.\nMeCo significantly accelerates pre-training across different model scales (600M\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\ninstance, a 1.6B language model trained with MeCo matches the downstream task\nperformance of standard pre-training while using 33% less data. Additionally,\nMeCo enables us to steer language models by conditioning the inference prompt\non either real or fabricated metadata that encodes the desired properties of\nthe output: for example, prepending wikipedia.org to reduce harmful generations\nor factquizmaster.com (fabricated) to improve common knowledge task\nperformance. We also demonstrate that MeCo is compatible with different types\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\ncomputational overhead, and demonstrates promise in producing more capable and\nsteerable language models.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u4e2d\u98a8\u683c\u3001\u9818\u57df\u548c\u54c1\u8cea\u5c64\u7d1a\u7684\u591a\u6a23\u6027\u5c0d\u65bc\u958b\u767c\u4e00\u822c\u6a21\u578b\u80fd\u529b\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u6709\u6548\u7387\u5730\u5b78\u7fd2\u4e26\u90e8\u7f72\u5728\u9019\u4e9b\u7570\u8cea\u6027\u8cc7\u6599\u4f86\u6e90\u4e2d\u5448\u73fe\u7684\u6b63\u78ba\u884c\u70ba\u662f\u4e00\u9805\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u7a31\u70ba\u300c\u5f8c\u7f6e\u51b7\u537b\u7684\u5143\u8cc7\u6599\u689d\u4ef6\u5316\u300d\uff08MeCo\uff09\uff0c\u4ee5\u5728\u9810\u8a13\u7df4\u671f\u9593\u7d0d\u5165\u984d\u5916\u7684\u5b78\u7fd2\u63d0\u793a\u3002MeCo \u9996\u5148\u5728\u8a13\u7df4\u671f\u9593\u63d0\u4f9b\u5143\u8cc7\u6599\uff08\u4f8b\u5982\u7db2\u5740\uff0c\u5982 en.wikipedia.org\uff09\u548c\u6587\u5b57\uff0c\u7136\u5f8c\u5728\u53ea\u6709\u6a19\u6e96\u6587\u5b57\u7684\u51b7\u537b\u968e\u6bb5\u4f7f\u7528\uff0c\u5f9e\u800c\u8b93\u6a21\u578b\u5373\u4f7f\u6c92\u6709\u5143\u8cc7\u6599\u4e5f\u80fd\u6b63\u5e38\u904b\u4f5c\u3002MeCo \u5927\u5e45\u52a0\u901f\u4e86\u4e0d\u540c\u6a21\u578b\u898f\u6a21\uff08600M \u5230 8B \u53c3\u6578\uff09\u548c\u8a13\u7df4\u4f86\u6e90\uff08C4\u3001RefinedWeb \u548c DCLM\uff09\u7684\u9810\u8a13\u7df4\u3002\u4f8b\u5982\uff0c\u4f7f\u7528 MeCo \u8a13\u7df4\u7684 1.6B \u8a9e\u8a00\u6a21\u578b\u5728\u4f7f\u7528\u5c11 33% \u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\uff0c\u8207\u6a19\u6e96\u9810\u8a13\u7df4\u7684\u4e0b\u6e38\u4efb\u52d9\u6548\u80fd\u76f8\u7b26\u3002\u6b64\u5916\uff0cMeCo \u8b93\u6211\u5011\u80fd\u5920\u900f\u904e\u5c0d\u63a8\u8ad6\u63d0\u793a\u9032\u884c\u689d\u4ef6\u5316\uff0c\u4ee5\u7de8\u78bc\u8f38\u51fa\u6240\u9700\u5c6c\u6027\u7684\u771f\u5be6\u6216\u865b\u69cb\u5143\u8cc7\u6599\u4f86\u5f15\u5c0e\u8a9e\u8a00\u6a21\u578b\uff1a\u4f8b\u5982\uff0c\u5728\u524d\u9762\u52a0\u4e0a wikipedia.org \u4ee5\u6e1b\u5c11\u6709\u5bb3\u7684\u751f\u6210\uff0c\u6216\u52a0\u4e0a factquizmaster.com\uff08\u865b\u69cb\uff09\u4ee5\u6539\u5584\u4e00\u822c\u77e5\u8b58\u4efb\u52d9\u6548\u80fd\u3002\u6211\u5011\u4e5f\u8b49\u660e MeCo \u8207\u4e0d\u540c\u985e\u578b\u7684\u5143\u8cc7\u6599\u76f8\u5bb9\uff0c\u4f8b\u5982\u6a21\u578b\u7522\u751f\u7684\u4e3b\u984c\u3002MeCo \u975e\u5e38\u7c21\u55ae\uff0c\u4e0d\u6703\u589e\u52a0\u904b\u7b97\u8ca0\u64d4\uff0c\u4e26\u8b49\u660e\u4e86\u5728\u7522\u751f\u66f4\u6709\u80fd\u529b\u4e14\u53ef\u5f15\u5c0e\u7684\u8a9e\u8a00\u6a21\u578b\u65b9\u9762\u5f88\u6709\u524d\u666f\u3002", "author": "Tianyu Gao et.al.", "authors": "Tianyu Gao, Alexander Wettig, Luxi He, Yihe Dong, Sadhika Malladi, Danqi Chen", "id": "2501.01956v1", "paper_url": "http://arxiv.org/abs/2501.01956v1", "repo": "null"}}