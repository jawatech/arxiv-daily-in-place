{"2412.12883": {"publish_time": "2024-12-17", "title": "A Comparative Study of Pruning Methods in Transformer-based Time Series Forecasting", "paper_summary": "The current landscape in time-series forecasting is dominated by\nTransformer-based models. Their high parameter count and corresponding demand\nin computational resources pose a challenge to real-world deployment,\nespecially for commercial and scientific applications with low-power embedded\ndevices. Pruning is an established approach to reduce neural network parameter\ncount and save compute. However, the implications and benefits of pruning\nTransformer-based models for time series forecasting are largely unknown. To\nclose this gap, we provide a comparative benchmark study by evaluating\nunstructured and structured pruning on various state-of-the-art multivariate\ntime series models. We study the effects of these pruning strategies on model\npredictive performance and computational aspects like model size, operations,\nand inference time. Our results show that certain models can be pruned even up\nto high sparsity levels, outperforming their dense counterpart. However,\nfine-tuning pruned models is necessary. Furthermore, we demonstrate that even\nwith corresponding hardware and software support, structured pruning is unable\nto provide significant time savings.", "paper_summary_zh": "\u76ee\u524d\u6642\u5e8f\u9810\u6e2c\u7684\u4e3b\u6d41\u65b9\u6cd5\u662f\u57fa\u65bc Transformer \u7684\u6a21\u578b\u3002\u9019\u4e9b\u6a21\u578b\u5177\u6709\u9ad8\u53c3\u6578\u8a08\u6578\uff0c\u5c0d\u904b\u7b97\u8cc7\u6e90\u7684\u9700\u6c42\u4e5f\u76f8\u5c0d\u61c9\u5730\u9ad8\uff0c\u9019\u5c0d\u5be6\u969b\u90e8\u7f72\u69cb\u6210\u4e86\u4e00\u9805\u6311\u6230\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u63a1\u7528\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u88dd\u7f6e\u7684\u5546\u696d\u548c\u79d1\u5b78\u61c9\u7528\u3002\u526a\u679d\u662f\u4e00\u7a2e\u65e2\u5b9a\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u6e1b\u5c11\u795e\u7d93\u7db2\u8def\u53c3\u6578\u8a08\u6578\u4e26\u7bc0\u7701\u904b\u7b97\u3002\u7136\u800c\uff0c\u526a\u679d Transformer \u6a21\u578b\u5c0d\u6642\u5e8f\u9810\u6e2c\u7684\u5f71\u97ff\u548c\u597d\u8655\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u77e5\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u9805\u6bd4\u8f03\u57fa\u6e96\u7814\u7a76\uff0c\u901a\u904e\u8a55\u4f30\u5404\u7a2e\u6700\u5148\u9032\u7684\u591a\u5143\u6642\u5e8f\u6a21\u578b\u4e0a\u7684\u975e\u7d50\u69cb\u5316\u548c\u7d50\u69cb\u5316\u526a\u679d\u3002\u6211\u5011\u7814\u7a76\u4e86\u9019\u4e9b\u526a\u679d\u7b56\u7565\u5c0d\u6a21\u578b\u9810\u6e2c\u6548\u80fd\u548c\u904b\u7b97\u65b9\u9762\u7684\u5f71\u97ff\uff0c\u4f8b\u5982\u6a21\u578b\u5927\u5c0f\u3001\u904b\u7b97\u548c\u63a8\u8ad6\u6642\u9593\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u67d0\u4e9b\u6a21\u578b\u751a\u81f3\u53ef\u4ee5\u88ab\u526a\u679d\u5230\u9ad8\u7a00\u758f\u5ea6\u7d1a\u5225\uff0c\u4e26\u512a\u65bc\u5176\u5bc6\u96c6\u5c0d\u61c9\u9805\u3002\u7136\u800c\uff0c\u5fae\u8abf\u526a\u679d\u6a21\u578b\u662f\u5fc5\u8981\u7684\u3002\u6b64\u5916\uff0c\u6211\u5011\u8b49\u660e\uff0c\u5373\u4f7f\u6709\u76f8\u61c9\u7684\u786c\u9ad4\u548c\u8edf\u9ad4\u652f\u63f4\uff0c\u7d50\u69cb\u5316\u526a\u679d\u4e5f\u7121\u6cd5\u63d0\u4f9b\u986f\u8457\u7684\u6642\u9593\u7bc0\u7701\u3002", "author": "Nicholas Kiefer et.al.", "authors": "Nicholas Kiefer, Arvid Weyrauch, Muhammed \u00d6z, Achim Streit, Markus G\u00f6tz, Charlotte Debus", "id": "2412.12883v1", "paper_url": "http://arxiv.org/abs/2412.12883v1", "repo": "null"}}