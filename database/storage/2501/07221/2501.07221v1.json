{"2501.07221": {"publish_time": "2025-01-13", "title": "Exploring the Use of Contrastive Language-Image Pre-Training for Human Posture Classification: Insights from Yoga Pose Analysis", "paper_summary": "Accurate human posture classification in images and videos is crucial for\nautomated applications across various fields, including work safety, physical\nrehabilitation, sports training, or daily assisted living. Recently, multimodal\nlearning methods, such as Contrastive Language-Image Pretraining (CLIP), have\nadvanced significantly in jointly understanding images and text. This study\naims to assess the effectiveness of CLIP in classifying human postures,\nfocusing on its application in yoga. Despite the initial limitations of the\nzero-shot approach, applying transfer learning on 15,301 images (real and\nsynthetic) with 82 classes has shown promising results. The article describes\nthe full procedure for fine-tuning, including the choice for image description\nsyntax, models and hyperparameters adjustment. The fine-tuned CLIP model,\ntested on 3826 images, achieves an accuracy of over 85%, surpassing the current\nstate-of-the-art of previous works on the same dataset by approximately 6%, its\ntraining time being 3.5 times lower than what is needed to fine-tune a\nYOLOv8-based model. For more application-oriented scenarios, with smaller\ndatasets of six postures each, containing 1301 and 401 training images, the\nfine-tuned models attain an accuracy of 98.8% and 99.1%, respectively.\nFurthermore, our experiments indicate that training with as few as 20 images\nper pose can yield around 90% accuracy in a six-class dataset. This study\ndemonstrates that this multimodal technique can be effectively used for yoga\npose classification, and possibly for human posture classification, in general.\nAdditionally, CLIP inference time (around 7 ms) supports that the model can be\nintegrated into automated systems for posture evaluation, e.g., for developing\na real-time personal yoga assistant for performance assessment.", "paper_summary_zh": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u51c6\u786e\u5730\u5bf9\u4eba\u4f53\u59ff\u52bf\u8fdb\u884c\u5206\u7c7b\u5bf9\u4e8e\u5404\u4e2a\u9886\u57df\u7684\u81ea\u52a8\u5316\u5e94\u7528\u7a0b\u5e8f\u81f3\u5173\u91cd\u8981\uff0c\u5305\u62ec\u5de5\u4f5c\u5b89\u5168\u3001\u8eab\u4f53\u5eb7\u590d\u3001\u4f53\u80b2\u8bad\u7ec3\u6216\u65e5\u5e38\u8f85\u52a9\u751f\u6d3b\u3002\u6700\u8fd1\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u5bf9\u6bd4\u8bed\u8a00\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP)\uff09\u5728\u8054\u5408\u7406\u89e3\u56fe\u50cf\u548c\u6587\u672c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30 CLIP \u5728\u4eba\u4f53\u59ff\u52bf\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728\u745c\u4f3d\u4e2d\u7684\u5e94\u7528\u3002\u5c3d\u7ba1\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u6700\u521d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u5bf9 15,301 \u5f20\u56fe\u50cf\uff08\u771f\u5b9e\u548c\u5408\u6210\uff09\u5e94\u7528\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528 82 \u4e2a\u7c7b\u522b\uff0c\u5df2\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u672c\u6587\u63cf\u8ff0\u4e86\u5fae\u8c03\u7684\u5b8c\u6574\u8fc7\u7a0b\uff0c\u5305\u62ec\u56fe\u50cf\u63cf\u8ff0\u8bed\u6cd5\u3001\u6a21\u578b\u548c\u8d85\u53c2\u6570\u8c03\u6574\u7684\u9009\u62e9\u3002\u7ecf\u8fc7\u5fae\u8c03\u7684 CLIP \u6a21\u578b\u5728 3826 \u5f20\u56fe\u50cf\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u51c6\u786e\u7387\u8d85\u8fc7 85%\uff0c\u6bd4\u4ee5\u524d\u5728\u540c\u4e00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7814\u7a76\u7684\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u9ad8\u51fa\u7ea6 6%\uff0c\u5176\u8bad\u7ec3\u65f6\u95f4\u6bd4\u5fae\u8c03\u57fa\u4e8e YOLOv8 \u7684\u6a21\u578b\u6240\u9700\u65f6\u95f4\u4f4e 3.5 \u500d\u3002\u5bf9\u4e8e\u66f4\u9762\u5411\u5e94\u7528\u7a0b\u5e8f\u7684\u573a\u666f\uff0c\u6bcf\u4e2a\u59ff\u52bf\u7684\u6570\u636e\u96c6\u8f83\u5c0f\uff0c\u5404\u5305\u542b 1301 \u5f20\u548c 401 \u5f20\u8bad\u7ec3\u56fe\u50cf\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5206\u522b\u8fbe\u5230 98.8% \u548c 99.1% \u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6bcf\u4e2a\u59ff\u52bf\u4ec5\u4f7f\u7528 20 \u5f20\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u516d\u7c7b\u6570\u636e\u96c6\u4e2d\u7684\u51c6\u786e\u7387\u53ef\u8fbe\u5230\u7ea6 90%\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u591a\u6a21\u6001\u6280\u672f\u53ef\u6709\u6548\u7528\u4e8e\u745c\u4f3d\u59ff\u52bf\u5206\u7c7b\uff0c\u751a\u81f3\u53ef\u7528\u4e8e\u4e00\u822c\u7684\u4eba\u4f53\u59ff\u52bf\u5206\u7c7b\u3002\u6b64\u5916\uff0cCLIP \u63a8\u65ad\u65f6\u95f4\uff08\u7ea6 7 \u6beb\u79d2\uff09\u652f\u6301\u5c06\u8be5\u6a21\u578b\u96c6\u6210\u5230\u59ff\u52bf\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u4e2d\uff0c\u4f8b\u5982\uff0c\u7528\u4e8e\u5f00\u53d1\u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\u7684\u5b9e\u65f6\u4e2a\u4eba\u745c\u4f3d\u52a9\u624b\u3002", "author": "Andrzej D. Dobrzycki et.al.", "authors": "Andrzej D. Dobrzycki, Ana M. Bernardos, Luca Bergesio, Andrzej Pomirski, Daniel S\u00e1ez-Trigueros", "id": "2501.07221v1", "paper_url": "http://arxiv.org/abs/2501.07221v1", "repo": "null"}}