{"2410.18035": {"publish_time": "2024-10-23", "title": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning", "paper_summary": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are\nhighly effective parameter-efficient fine-tuning (PEFT) methods. However, they\nintroduce significant latency in multi-tenant settings due to the LoRA modules\nand MOE routers added to multiple linear modules in the Transformer layer. To\naddress this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel\nand efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods\nby considering each LoRA module as an expert and employing a prompt-aware\nrouting mechanism. This mechanism calculates expert routing results once before\ngenerating the first new token and reuses these results for subsequent tokens,\nreducing latency. Extensive experiments and analysis on commonsense reasoning\ntasks, math reasoning tasks, and widely used LLM evaluation benchmarks\ndemonstrate that MiLoRA consistently outperforms strong PEFT baselines with\ncomparable tunable parameter budgets. Additionally, MiLoRA significantly\nreduces latency in multi-tenant settings compared to previous LoRA-based\nmethods.", "paper_summary_zh": "\u4f4e\u79e9\u9069\u61c9 (LoRA) \u53ca\u5176\u6df7\u5408\u5c08\u5bb6 (MOE) \u8b8a\u9ad4\u662f\u9ad8\u6548\u7387\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7531\u65bc\u5728 Transformer \u5c64\u4e2d\u52a0\u5165\u4e86 LoRA \u6a21\u7d44\u548c MOE \u8def\u7531\u5668\u5230\u591a\u500b\u7dda\u6027\u6a21\u7d44\u4e2d\uff0c\u5b83\u5011\u5728\u591a\u79df\u6236\u8a2d\u5b9a\u4e2d\u5f15\u5165\u4e86\u986f\u8457\u7684\u5ef6\u9072\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4f4e\u79e9\u9069\u61c9\u6df7\u5408 (MiLoRA)\uff0c\u4e00\u7a2e\u65b0\u7a4e\u4e14\u9ad8\u6548\u7684 LoRA \u8b8a\u9ad4\u3002MiLoRA \u8207\u5148\u524d\u7684 MOE \u98a8\u683c LoRA \u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u5c07\u6bcf\u500b LoRA \u6a21\u7d44\u8996\u70ba\u5c08\u5bb6\uff0c\u4e26\u63a1\u7528\u63d0\u793a\u611f\u77e5\u8def\u7531\u6a5f\u5236\u3002\u6b64\u6a5f\u5236\u5728\u7522\u751f\u7b2c\u4e00\u500b\u65b0\u7b26\u865f\u4e4b\u524d\u8a08\u7b97\u4e00\u6b21\u5c08\u5bb6\u8def\u7531\u7d50\u679c\uff0c\u4e26\u5c07\u9019\u4e9b\u7d50\u679c\u91cd\u8907\u4f7f\u7528\u65bc\u5f8c\u7e8c\u7b26\u865f\uff0c\u5f9e\u800c\u964d\u4f4e\u5ef6\u9072\u3002\u5728\u5e38\u8b58\u63a8\u7406\u4efb\u52d9\u3001\u6578\u5b78\u63a8\u7406\u4efb\u52d9\u548c\u5ee3\u6cdb\u4f7f\u7528\u7684 LLM \u8a55\u4f30\u57fa\u6e96\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u548c\u5206\u6790\u8868\u660e\uff0cMiLoRA \u6301\u7e8c\u512a\u65bc\u5177\u6709\u53ef\u6bd4\u8f03\u53ef\u8abf\u6574\u53c3\u6578\u9810\u7b97\u7684\u5f37\u5927 PEFT \u57fa\u6e96\u3002\u6b64\u5916\uff0c\u8207\u5148\u524d\u7684\u57fa\u65bc LoRA \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cMiLoRA \u5728\u591a\u79df\u6236\u8a2d\u5b9a\u4e2d\u986f\u8457\u964d\u4f4e\u4e86\u5ef6\u9072\u3002", "author": "Jingfan Zhang et.al.", "authors": "Jingfan Zhang, Yi Zhao, Dan Chen, Xing Tian, Huanran Zheng, Wei Zhu", "id": "2410.18035v1", "paper_url": "http://arxiv.org/abs/2410.18035v1", "repo": "null"}}