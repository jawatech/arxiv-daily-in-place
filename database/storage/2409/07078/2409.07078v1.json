{"2409.07078": {"publish_time": "2024-09-11", "title": "Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout", "paper_summary": "In this paper, we present our solution for the Second Multimodal Emotion\nRecognition Challenge Track 1(MER2024-SEMI). To enhance the accuracy and\ngeneralization performance of emotion recognition, we propose several methods\nfor Multimodal Emotion Recognition. Firstly, we introduce EmoVCLIP, a model\nfine-tuned based on CLIP using vision-language prompt learning, designed for\nvideo-based emotion recognition tasks. By leveraging prompt learning on CLIP,\nEmoVCLIP improves the performance of pre-trained CLIP on emotional videos.\nAdditionally, to address the issue of modality dependence in multimodal fusion,\nwe employ modality dropout for robust information fusion. Furthermore, to aid\nBaichuan in better extracting emotional information, we suggest using GPT-4 as\nthe prompt for Baichuan. Lastly, we utilize a self-training strategy to\nleverage unlabeled videos. In this process, we use unlabeled videos with\nhigh-confidence pseudo-labels generated by our model and incorporate them into\nthe training set. Experimental results demonstrate that our model ranks 1st in\nthe MER2024-SEMI track, achieving an accuracy of 90.15% on the test set.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u6211\u5011\u5c0d\u7b2c\u4e8c\u5c46\u591a\u6a21\u614b\u60c5\u7dd2\u8b58\u5225\u6311\u6230\u8cfd\u8ecc\u9053 1 (MER2024-SEMI) \u7684\u89e3\u6c7a\u65b9\u6848\u3002\u70ba\u4e86\u589e\u5f37\u60c5\u7dd2\u8b58\u5225\u7684\u6e96\u78ba\u6027\u548c\u6cdb\u5316\u6548\u80fd\uff0c\u6211\u5011\u91dd\u5c0d\u591a\u6a21\u614b\u60c5\u7dd2\u8b58\u5225\u63d0\u51fa\u5e7e\u7a2e\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u6211\u5011\u4ecb\u7d39 EmoVCLIP\uff0c\u4e00\u7a2e\u57fa\u65bc CLIP \u5fae\u8abf\u7684\u6a21\u578b\uff0c\u4f7f\u7528\u8996\u89ba\u8a9e\u8a00\u63d0\u793a\u5b78\u7fd2\uff0c\u5c08\u70ba\u57fa\u65bc\u5f71\u7247\u7684\u60c5\u7dd2\u8b58\u5225\u4efb\u52d9\u800c\u8a2d\u8a08\u3002\u900f\u904e\u5728 CLIP \u4e0a\u5229\u7528\u63d0\u793a\u5b78\u7fd2\uff0cEmoVCLIP \u6539\u5584\u4e86\u9810\u8a13\u7df4 CLIP \u5728\u60c5\u7dd2\u5f71\u7247\u4e0a\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u89e3\u6c7a\u591a\u6a21\u614b\u878d\u5408\u4e2d\u7684\u6a21\u614b\u4f9d\u8cf4\u6027\u554f\u984c\uff0c\u6211\u5011\u63a1\u7528\u6a21\u614b\u4e2d\u65b7\uff0c\u4ee5\u5be6\u73fe\u7a69\u5065\u7684\u8cc7\u8a0a\u878d\u5408\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u5354\u52a9\u767e\u5ddd\u66f4\u597d\u5730\u64f7\u53d6\u60c5\u7dd2\u8cc7\u8a0a\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528 GPT-4 \u4f5c\u70ba\u767e\u5ddd\u7684\u63d0\u793a\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5229\u7528\u81ea\u8a13\u7df4\u7b56\u7565\u4f86\u5229\u7528\u672a\u6a19\u8a18\u7684\u5f71\u7247\u3002\u5728\u6b64\u904e\u7a0b\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u672a\u6a19\u8a18\u7684\u5f71\u7247\uff0c\u5176\u4e2d\u5305\u542b\u7531\u6211\u5011\u7684\u6a21\u578b\u7522\u751f\u7684\u9ad8\u53ef\u4fe1\u5ea6\u507d\u6a19\u7c64\uff0c\u4e26\u5c07\u5b83\u5011\u7d0d\u5165\u8a13\u7df4\u7d44\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 MER2024-SEMI \u8ecc\u9053\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5728\u6e2c\u8a66\u7d44\u4e0a\u9054\u5230 90.15% \u7684\u6e96\u78ba\u5ea6\u3002", "author": "Anbin QI et.al.", "authors": "Anbin QI, Zhongliang Liu, Xinyong Zhou, Jinba Xiao, Fengrun Zhang, Qi Gan, Ming Tao, Gaozheng Zhang, Lu Zhang", "id": "2409.07078v1", "paper_url": "http://arxiv.org/abs/2409.07078v1", "repo": "null"}}