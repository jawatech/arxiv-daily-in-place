{"2411.13323": {"publish_time": "2024-11-20", "title": "Are Large Language Models Memorizing Bug Benchmarks?", "paper_summary": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage.\n  In this paper, we systematically evaluate popular LLMs to assess their\nsusceptibility to data leakage from widely used bug benchmarks. To identify\npotential leakage, we use multiple metrics, including a study of benchmark\nmembership within commonly used training datasets, as well as analyses of\nnegative log-likelihood and n-gram accuracy. Our findings show that certain\nmodels, in particular codegen-multi, exhibit significant evidence of\nmemorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u5404\u7a2e\u8edf\u9ad4\u5de5\u7a0b\u4efb\u52d9\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\uff0c\u5305\u62ec\u7a0b\u5f0f\u78bc\u7522\u751f\u3001\u932f\u8aa4\u5075\u6e2c\u548c\u4fee\u5fa9\u3002\u70ba\u4e86\u8a55\u4f30\u9019\u4e9b\u9818\u57df\u4e2d\u7684\u6a21\u578b\u6548\u80fd\uff0c\u5df2\u958b\u767c\u51fa\u8a31\u591a\u5305\u542b\u8edf\u9ad4\u5c08\u6848\u4e2d\u771f\u5be6\u932f\u8aa4\u7684\u932f\u8aa4\u57fa\u6e96\u6e2c\u8a66\u3002\u7136\u800c\uff0c\u8edf\u9ad4\u5de5\u7a0b\u793e\u7fa4\u4e2d\u65e5\u76ca\u95dc\u6ce8\u7684\u662f\uff0c\u7531\u65bc\u8cc7\u6599\u5916\u6d29\u7684\u98a8\u96aa\uff0c\u9019\u4e9b\u57fa\u6e96\u6e2c\u8a66\u53ef\u80fd\u7121\u6cd5\u53ef\u9760\u5730\u53cd\u6620\u771f\u6b63\u7684 LLM \u6548\u80fd\u3002\u5118\u7ba1\u6709\u6b64\u7591\u616e\uff0c\u4f46\u91dd\u5c0d\u91cf\u5316\u6f5b\u5728\u5916\u6d29\u5f71\u97ff\u7684\u7814\u7a76\u537b\u5341\u5206\u6709\u9650\u3002\n\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u71b1\u9580 LLM\uff0c\u4ee5\u8a55\u4f30\u5b83\u5011\u5c0d\u5ee3\u6cdb\u4f7f\u7528\u7684\u932f\u8aa4\u57fa\u6e96\u6e2c\u8a66\u4e2d\u8cc7\u6599\u5916\u6d29\u7684\u654f\u611f\u6027\u3002\u70ba\u4e86\u8b58\u5225\u6f5b\u5728\u7684\u5916\u6d29\uff0c\u6211\u5011\u4f7f\u7528\u591a\u7a2e\u6307\u6a19\uff0c\u5305\u62ec\u7814\u7a76\u57fa\u6e96\u6e2c\u8a66\u6210\u54e1\u8cc7\u683c\u5728\u5e38\u7528\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\u4e2d\u7684\u60c5\u6cc1\uff0c\u4ee5\u53ca\u5c0d\u8ca0\u5c0d\u6578\u4f3c\u7136\u548c n-gram \u7cbe\u78ba\u5ea6\u7684\u5206\u6790\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u67d0\u4e9b\u6a21\u578b\uff0c\u7279\u5225\u662f codegen-multi\uff0c\u5728 Defects4J \u7b49\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5c55\u73fe\u51fa\u986f\u8457\u7684\u8a18\u61b6\u8b49\u64da\uff0c\u800c\u8a13\u7df4\u65bc\u8f03\u5927\u578b\u8cc7\u6599\u96c6\uff08\u4f8b\u5982 LLaMa 3.1\uff09\u7684\u8f03\u65b0\u6a21\u578b\u5247\u5c55\u73fe\u51fa\u6709\u9650\u7684\u5916\u6d29\u8de1\u8c61\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86\u4ed4\u7d30\u9078\u64c7\u57fa\u6e96\u6e2c\u8a66\u548c\u63a1\u7528\u7a69\u5065\u6307\u6a19\u4ee5\u5145\u5206\u8a55\u4f30\u6a21\u578b\u529f\u80fd\u7684\u5fc5\u8981\u6027\u3002", "author": "Daniel Ramos et.al.", "authors": "Daniel Ramos, Claudia Mamede, Kush Jain, Paulo Canelas, Catarina Gamboa, Claire Le Goues", "id": "2411.13323v1", "paper_url": "http://arxiv.org/abs/2411.13323v1", "repo": "null"}}