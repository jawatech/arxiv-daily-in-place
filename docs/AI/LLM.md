
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-16**|**RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval**|Di Liu et.al.|[2409.10516v1](http://arxiv.org/abs/2409.10516v1)|null|
|**2024-09-16**|**DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction**|John Wu et.al.|[2409.10504v1](http://arxiv.org/abs/2409.10504v1)|null|
|**2024-09-16**|**Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles**|Kulin Shah et.al.|[2409.10502v1](http://arxiv.org/abs/2409.10502v1)|null|
|**2024-09-16**|**MusicLIME: Explainable Multimodal Music Understanding**|Theodoros Sotirou et.al.|[2409.10496v1](http://arxiv.org/abs/2409.10496v1)|[link](https://github.com/iamtheo2000/musiclime)|
|**2024-09-16**|**Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation**|Noah Buchanan et.al.|[2409.10494v1](http://arxiv.org/abs/2409.10494v1)|null|
|**2024-09-16**|**Flash STU: Fast Spectral Transform Units**|Y. Isabel Liu et.al.|[2409.10489v2](http://arxiv.org/abs/2409.10489v2)|null|
|**2024-09-16**|**Do Pre-trained Vision-Language Models Encode Object States?**|Kaleb Newman et.al.|[2409.10488v1](http://arxiv.org/abs/2409.10488v1)|null|
|**2024-09-16**|**Schrodinger's Memory: Large Language Models**|Wei Wang et.al.|[2409.10482v2](http://arxiv.org/abs/2409.10482v2)|null|
|**2024-09-16**|**Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance**|Simone Maurizio La Cava et.al.|[2409.10481v1](http://arxiv.org/abs/2409.10481v1)|null|
|**2024-09-16**|**MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion**|Lehong Wu et.al.|[2409.10473v1](http://arxiv.org/abs/2409.10473v1)|null|
|**2024-09-16**|**Meta-Whisper: Speech-Based Meta-ICL for ASR on Low-Resource Languages**|Ming-Hao Hsu et.al.|[2409.10429v1](http://arxiv.org/abs/2409.10429v1)|null|
|**2024-09-16**|**HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models**|Vineet Bhat et.al.|[2409.10419v1](http://arxiv.org/abs/2409.10419v1)|null|
|**2024-09-16**|**A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**|Zhang Zheng et.al.|[2409.10403v1](http://arxiv.org/abs/2409.10403v1)|null|
|**2024-09-16**|**Instigating Cooperation among LLM Agents Using Adaptive Information Modulation**|Qiliang Chen et.al.|[2409.10372v1](http://arxiv.org/abs/2409.10372v1)|null|
|**2024-09-16**|**Robust image representations with counterfactual contrastive learning**|Mélanie Roschewitz et.al.|[2409.10365v1](http://arxiv.org/abs/2409.10365v1)|[link](https://github.com/biomedia-mira/counterfactual-contrastive)|
|**2024-09-16**|**2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-Speech Gesture Generation?**|Téo Guichoux et.al.|[2409.10357v1](http://arxiv.org/abs/2409.10357v1)|null|
|**2024-09-16**|**Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation**|Tianrui Song et.al.|[2409.10343v1](http://arxiv.org/abs/2409.10343v1)|null|
|**2024-09-16**|**Detecting Sexism in German Online Newspaper Comments with Open-Source Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks 1 and 2, Closed Track)**|Florian Bremm et.al.|[2409.10341v1](http://arxiv.org/abs/2409.10341v1)|null|
|**2024-09-16**|**Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs**|Mehrad Soltani et.al.|[2409.10340v1](http://arxiv.org/abs/2409.10340v1)|null|
|**2024-09-16**|**The 20 questions game to distinguish large language models**|Gurvan Richardeau et.al.|[2409.10338v1](http://arxiv.org/abs/2409.10338v1)|null|
|**2024-09-16**|**InfoDisent: Explainability of Image Classification Models by Information Disentanglement**|Łukasz Struski et.al.|[2409.10329v1](http://arxiv.org/abs/2409.10329v1)|null|
|**2024-09-16**|**Know your limits! Optimize the robot's behavior through self-awareness**|Esteve Valls Mascaro et.al.|[2409.10308v1](http://arxiv.org/abs/2409.10308v1)|null|
|**2024-09-16**|**On Synthetic Texture Datasets: Challenges, Creation, and Curation**|Blaine Hoak et.al.|[2409.10297v1](http://arxiv.org/abs/2409.10297v1)|null|
|**2024-09-16**|**MGSA: Multi-granularity Graph Structure Attention for Knowledge Graph-to-Text Generation**|Shanshan Wang et.al.|[2409.10294v1](http://arxiv.org/abs/2409.10294v1)|null|
|**2024-09-16**|**ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework**|Jiahao Yuan et.al.|[2409.10289v1](http://arxiv.org/abs/2409.10289v1)|null|
|**2024-09-16**|**DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis**|Fa-Ting Hong et.al.|[2409.10281v1](http://arxiv.org/abs/2409.10281v1)|null|
|**2024-09-16**|**Cognitive Kernel: An Open-source Agent System towards Generalist Autopilots**|Hongming Zhang et.al.|[2409.10277v1](http://arxiv.org/abs/2409.10277v1)|null|
|**2024-09-16**|**Causal Discovery in Recommender Systems: Example and Discussion**|Emanuele Cavenaghi et.al.|[2409.10271v1](http://arxiv.org/abs/2409.10271v1)|null|
|**2024-09-16**|**FGR-Net:Interpretable fundus imagegradeability classification based on deepreconstruction learning**|Saif Khalid et.al.|[2409.10246v1](http://arxiv.org/abs/2409.10246v1)|null|
|**2024-09-16**|**From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes the Emoji Potential in LLMs**|Navya Jain et.al.|[2409.10245v1](http://arxiv.org/abs/2409.10245v1)|null|
|**2024-09-16**|**Hedging Is Not All You Need: A Simple Baseline for Online Learning Under Haphazard Inputs**|Himanshu Buckchash et.al.|[2409.10242v1](http://arxiv.org/abs/2409.10242v1)|null|
|**2024-09-16**|**Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models**|Weihao Ye et.al.|[2409.10197v1](http://arxiv.org/abs/2409.10197v1)|null|
|**2024-09-16**|**NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions**|Zhixi Cai et.al.|[2409.10196v1](http://arxiv.org/abs/2409.10196v1)|null|
|**2024-09-16**|**LLMs for clinical risk prediction**|Mohamed Rezk et.al.|[2409.10191v1](http://arxiv.org/abs/2409.10191v1)|null|
|**2024-09-16**|**Augmenting Automatic Speech Recognition Models with Disfluency Detection**|Robin Amann et.al.|[2409.10177v2](http://arxiv.org/abs/2409.10177v2)|null|
|**2024-09-16**|**jina-embeddings-v3: Multilingual Embeddings With Task LoRA**|Saba Sturua et.al.|[2409.10173v2](http://arxiv.org/abs/2409.10173v2)|null|
|**2024-09-16**|**Quantile Regression for Distributional Reward Models in RLHF**|Nicolai Dorka et.al.|[2409.10164v1](http://arxiv.org/abs/2409.10164v1)|null|
|**2024-09-16**|**AutoPET Challenge III: Testing the Robustness of Generalized Dice Focal Loss trained 3D Residual UNet for FDG and PSMA Lesion Segmentation from Whole-Body PET/CT Images**|Shadab Ahamed et.al.|[2409.10151v1](http://arxiv.org/abs/2409.10151v1)|[link](https://github.com/ahxmeds/autosegnet2024)|
|**2024-09-16**|**LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge**|Hamed Babaei Giglou et.al.|[2409.10146v1](http://arxiv.org/abs/2409.10146v1)|null|
|**2024-09-16**|**Advancing Towards a Marine Digital Twin Platform: Modeling the Mar Menor Coastal Lagoon Ecosystem in the South Western Mediterranean**|Yu Ye et.al.|[2409.10134v1](http://arxiv.org/abs/2409.10134v1)|null|
|**2024-09-16**|**StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models**|Baolong Bi et.al.|[2409.10132v1](http://arxiv.org/abs/2409.10132v1)|null|
|**2024-09-16**|**Industry 6.0: New Generation of Industry driven by Generative AI and Swarm of Heterogeneous Robots**|Artem Lykov et.al.|[2409.10106v1](http://arxiv.org/abs/2409.10106v1)|null|
|**2024-09-16**|**Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT**|Ryota Komatsu et.al.|[2409.10103v1](http://arxiv.org/abs/2409.10103v1)|[link](https://github.com/ryota-komatsu/speaker_disentangled_hubert)|
|**2024-09-16**|**Trustworthiness in Retrieval-Augmented Generation Systems: A Survey**|Yujia Zhou et.al.|[2409.10102v1](http://arxiv.org/abs/2409.10102v1)|[link](https://github.com/smallporridge/trustworthyrag)|
|**2024-09-16**|**LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain**|Le Xiao et.al.|[2409.10077v1](http://arxiv.org/abs/2409.10077v1)|null|
|**2024-09-16**|**Increasing faithfulness in human-human dialog summarization with Spoken Language Understanding tasks**|Eunice Akani et.al.|[2409.10070v1](http://arxiv.org/abs/2409.10070v1)|null|
|**2024-09-16**|**MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM**|Sijie Ji et.al.|[2409.10064v1](http://arxiv.org/abs/2409.10064v1)|null|
|**2024-09-16**|**Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective**|Van-Cuong Pham et.al.|[2409.10053v1](http://arxiv.org/abs/2409.10053v1)|null|
|**2024-09-16**|**Benchmarking Large Language Model Uncertainty for Prompt Optimization**|Pei-Fu Guo et.al.|[2409.10044v1](http://arxiv.org/abs/2409.10044v1)|[link](https://github.com/0frett/po-uncertainty-benchmarking)|
|**2024-09-16**|**On the Diagram of Thought**|Yifan Zhang et.al.|[2409.10038v1](http://arxiv.org/abs/2409.10038v1)|[link](https://github.com/diagram-of-thought/diagram-of-thought)|
|**2024-09-16**|**Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs**|Haichuan Hu et.al.|[2409.10033v2](http://arxiv.org/abs/2409.10033v2)|[link](https://github.com/tomsawyerhu/gpt-o1-on-quixbugs)|
|**2024-09-16**|**AttnMod: Attention-Based New Art Styles**|Shih-Chieh Su et.al.|[2409.10028v1](http://arxiv.org/abs/2409.10028v1)|null|
|**2024-09-16**|**E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models**|Chan Kim et.al.|[2409.10027v1](http://arxiv.org/abs/2409.10027v1)|null|
|**2024-09-16**|**AceParse: A Comprehensive Dataset with Diverse Structured Texts for Academic Literature Parsing**|Huawei Ji et.al.|[2409.10016v1](http://arxiv.org/abs/2409.10016v1)|[link](https://github.com/JHW5981/AceParse)|
|**2024-09-16**|**HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making**|Sumera Anjum et.al.|[2409.10011v1](http://arxiv.org/abs/2409.10011v1)|null|
|**2024-09-16**|**SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL**|Ke Shen et.al.|[2409.10007v1](http://arxiv.org/abs/2409.10007v1)|[link](https://github.com/neuripspublishingresearchcode/select-sql)|
|**2024-09-16**|**FreeMark: A Non-Invasive White-Box Watermarking for Deep Neural Networks**|Yuzhang Chen et.al.|[2409.09996v1](http://arxiv.org/abs/2409.09996v1)|null|
|**2024-09-16**|**Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system**|Shailja Gupta et.al.|[2409.09989v1](http://arxiv.org/abs/2409.09989v1)|null|
|**2024-09-16**|**Deep Graph Anomaly Detection: A Survey and New Perspectives**|Hezhe Qiao et.al.|[2409.09957v1](http://arxiv.org/abs/2409.09957v1)|[link](https://github.com/mala-lab/awesome-deep-graph-anomaly-detection)|
|**2024-09-16**|**Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for Fine-grained Text Evaluations**|Abe Bohan Hou et.al.|[2409.09947v1](http://arxiv.org/abs/2409.09947v1)|null|
|**2024-09-16**|**Fault Analysis And Predictive Maintenance Of Induction Motor Using Machine Learning**|Kavana Venkatesh et.al.|[2409.09944v1](http://arxiv.org/abs/2409.09944v1)|null|
|**2024-09-16**|**Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges**|Vinay Samuel et.al.|[2409.09927v1](http://arxiv.org/abs/2409.09927v1)|null|
|**2024-09-16**|**SFR-RAG: Towards Contextually Faithful LLMs**|Xuan-Phi Nguyen et.al.|[2409.09916v1](http://arxiv.org/abs/2409.09916v1)|null|
|**2024-09-16**|**Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors**|Joseph Suh et.al.|[2409.09905v1](http://arxiv.org/abs/2409.09905v1)|null|
|**2024-09-15**|**Acquiring Pronunciation Knowledge from Transcribed Speech Audio via Multi-task Learning**|Siqi Sun et.al.|[2409.09891v1](http://arxiv.org/abs/2409.09891v1)|null|
|**2024-09-15**|**REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models**|Teerapong Panboonyuen et.al.|[2409.09877v2](http://arxiv.org/abs/2409.09877v2)|[link](https://github.com/kaopanboonyuen/reg)|
|**2024-09-15**|**Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent**|Pavel Osinenko et.al.|[2409.09869v1](http://arxiv.org/abs/2409.09869v1)|null|
|**2024-09-15**|**Towards Kinetic Manipulation of the Latent Space**|Diego Porres et.al.|[2409.09867v1](http://arxiv.org/abs/2409.09867v1)|[link](https://github.com/pdillis/stylegan3-fun)|
|**2024-09-15**|**Constructing a Singing Style Caption Dataset**|Hyunjong Ok et.al.|[2409.09866v1](http://arxiv.org/abs/2409.09866v1)|[link](https://github.com/hj-ok/s2cap)|
|**2024-09-15**|**A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View**|Jing Ma et.al.|[2409.09858v1](http://arxiv.org/abs/2409.09858v1)|null|
|**2024-09-15**|**A Benchmark Dataset with Larger Context for Non-Factoid Question Answering over Islamic Text**|Faiza Qamar et.al.|[2409.09844v1](http://arxiv.org/abs/2409.09844v1)|null|
|**2024-09-15**|**Generating Synthetic Free-text Medical Records with Low Re-identification Risk using Masked Language Modeling**|Samuel Belkadi et.al.|[2409.09831v2](http://arxiv.org/abs/2409.09831v2)|null|
|**2024-09-15**|**Latent Diffusion Models for Controllable RNA Sequence Generation**|Kaixuan Huang et.al.|[2409.09828v1](http://arxiv.org/abs/2409.09828v1)|null|
|**2024-09-15**|**GP-GPT: Large Language Model for Gene-Phenotype Mapping**|Yanjun Lyu et.al.|[2409.09825v1](http://arxiv.org/abs/2409.09825v1)|null|
|**2024-09-15**|**Causal Inference with Large Language Model: A Survey**|Jing Ma et.al.|[2409.09822v1](http://arxiv.org/abs/2409.09822v1)|null|
|**2024-09-15**|**Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion**|Hui Shen et.al.|[2409.09808v1](http://arxiv.org/abs/2409.09808v1)|[link](https://github.com/aiot-mlsys-lab/famba-v)|
|**2024-09-15**|**Multiple Rotation Averaging with Constrained Reweighting Deep Matrix Factorization**|Shiqi Li et.al.|[2409.09790v1](http://arxiv.org/abs/2409.09790v1)|null|
|**2024-09-15**|**Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models**|Yuan-Hong Liao et.al.|[2409.09788v1](http://arxiv.org/abs/2409.09788v1)|null|
|**2024-09-15**|**BEnDEM:A Boltzmann Sampler Based on Bootstrapped Denoising Energy Matching**|RuiKang OuYang et.al.|[2409.09787v1](http://arxiv.org/abs/2409.09787v1)|null|
|**2024-09-15**|**Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition**|Chao-Han Huck Yang et.al.|[2409.09785v2](http://arxiv.org/abs/2409.09785v2)|null|
|**2024-09-15**|**Enhancing Lesion Segmentation in PET/CT Imaging with Deep Learning and Advanced Data Preprocessing Techniques**|Jiayi Liu et.al.|[2409.09784v1](http://arxiv.org/abs/2409.09784v1)|[link](https://github.com/jiayiliu-pku/dc2024)|
|**2024-09-15**|**Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting**|Qiaoyi Xue et.al.|[2409.09766v1](http://arxiv.org/abs/2409.09766v1)|[link](https://github.com/jiayiliu-pku/ap2024)|
|**2024-09-15**|**ELMI: Interactive and Intelligent Sign Language Translation of Lyrics for Song Signing**|Suhyeon Yoo et.al.|[2409.09760v1](http://arxiv.org/abs/2409.09760v1)|null|
|**2024-09-15**|**Explore the Hallucination on Low-level Perception for MLLMs**|Yinan Sun et.al.|[2409.09748v1](http://arxiv.org/abs/2409.09748v1)|null|
|**2024-09-15**|**Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with Toxicity and Incivility Data**|Bastián González-Bustamante et.al.|[2409.09741v1](http://arxiv.org/abs/2409.09741v1)|null|
|**2024-09-15**|**PersonaMark: Personalized LLM watermarking for model protection and user attribution**|Yuehan Zhang et.al.|[2409.09739v1](http://arxiv.org/abs/2409.09739v1)|null|
|**2024-09-15**|**From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare**|Ming Li et.al.|[2409.09727v1](http://arxiv.org/abs/2409.09727v1)|null|
|**2024-09-15**|**Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents**|Justas Andriuškevičius et.al.|[2409.09717v1](http://arxiv.org/abs/2409.09717v1)|null|
|**2024-09-15**|**AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs**|Madhusudan Ghosh et.al.|[2409.09704v1](http://arxiv.org/abs/2409.09704v1)|[link](https://github.com/shrimonmuke0202/alpapico)|
|**2024-09-15**|**GFlowNet Pretraining with Inexpensive Rewards**|Mohit Pandey et.al.|[2409.09702v1](http://arxiv.org/abs/2409.09702v1)|null|
|**2024-09-15**|**Anatomy of Machines for Markowitz: Decision-Focused Learning for Mean-Variance Portfolio Optimization**|Junhyeong Lee et.al.|[2409.09684v1](http://arxiv.org/abs/2409.09684v1)|null|
|**2024-09-15**|**ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models**|Inhwa Song et.al.|[2409.09662v2](http://arxiv.org/abs/2409.09662v2)|null|
|**2024-09-15**|**Leveraging Open-Source Large Language Models for Native Language Identification**|Yee Man Ng et.al.|[2409.09659v1](http://arxiv.org/abs/2409.09659v1)|null|
|**2024-09-15**|**Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example**|Yuanning Huang et.al.|[2409.09652v1](http://arxiv.org/abs/2409.09652v1)|null|
|**2024-09-15**|**Self-supervised Learning for Acoustic Few-Shot Classification**|Jingyong Liang et.al.|[2409.09647v1](http://arxiv.org/abs/2409.09647v1)|null|
|**2024-09-15**|**A Simple HMM with Self-Supervised Representations for Phone Segmentation**|Gene-Ping Yang et.al.|[2409.09646v1](http://arxiv.org/abs/2409.09646v1)|null|
|**2024-09-15**|**Towards understanding evolution of science through language model series**|Junjie Dong et.al.|[2409.09636v1](http://arxiv.org/abs/2409.09636v1)|null|
|**2024-09-15**|**Confidence Estimation for LLM-Based Dialogue State Tracking**|Yi-Jyun Sun et.al.|[2409.09629v1](http://arxiv.org/abs/2409.09629v1)|null|
|**2024-09-15**|**Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot Event-based Recognition**|Zongyou Yu et.al.|[2409.09628v1](http://arxiv.org/abs/2409.09628v1)|[link](https://github.com/chrisyu-zz/pure-event-based-recognition-based-llm)|
|**2024-09-15**|**Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics**|Yi Ren et.al.|[2409.09626v1](http://arxiv.org/abs/2409.09626v1)|null|

#### Abstracts
##### **RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval**
2409.10516v1 by Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu

Transformer-based large Language Models (LLMs) become increasingly important
in various domains. However, the quadratic time complexity of attention
operation poses a significant challenge for scaling to longer contexts due to
the extremely high inference latency and GPU memory consumption for caching
key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free
approach to accelerate attention computation. To leverage the dynamic sparse
property of attention, RetrievalAttention builds approximate nearest neighbor
search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most
relevant ones via vector search during generation. Due to the
out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf
ANNS indexes still need to scan O(N) (usually 30% of all keys) data for
accurate retrieval, which fails to exploit the high sparsity.
RetrievalAttention first identifies the OOD challenge of ANNS-based attention,
and addresses it via an attention-aware vector search algorithm that can adapt
to queries and only access 1--3% of data, thus achieving a sub-linear time
complexity. RetrievalAttention greatly reduces the inference cost of
long-context LLM with much lower GPU memory requirements while maintaining the
model accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for
serving 128K tokens in LLMs with 8B parameters, which is capable of generating
one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).

摘要：<paragraph>基於 Transformer 的大型語言模型 (LLM) 在各種領域變得越來越重要。然而，注意力運算的二次時間複雜度對擴展到更長的內容提出了重大挑戰，原因是極高的推論延遲和用於快取鍵值 (KV) 向量的高 GPU 記憶體消耗。本文提出 RetrievalAttention，這是一種免訓練的方法，用於加速注意力運算。為了利用注意力的動態稀疏屬性，RetrievalAttention 在 CPU 記憶體中建立近似最近鄰搜尋 (ANNS) 索引，並在生成期間透過向量搜尋擷取最相關的向量。由於查詢向量和金鑰向量之間的分布外 (OOD)，現成的 ANNS 索引仍需要掃描 O(N)（通常為所有金鑰的 30%）資料以進行準確的擷取，這無法利用高稀疏性。RetrievalAttention 首先找出基於 ANNS 的注意力的 OOD 挑戰，並透過可適應查詢且僅存取 1--3% 資料的注意力感知向量搜尋演算法來解決此問題，從而實現次線性時間複雜度。RetrievalAttention 大幅降低了長內容 LLM 的推論成本，同時 GPU 記憶體需求也低很多，但仍維持模型準確度。特別是，RetrievalAttention 只需要 16GB GPU 記憶體就能在具備 8B 參數的 LLM 中提供 128K 個符號，這可以在單一 NVIDIA RTX4090（24GB）上以 0.188 秒產生一個符號。</paragraph>

##### **DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction**
2409.10504v1 by John Wu, David Wu, Jimeng Sun

Predicting high-dimensional or extreme multilabels, such as in medical
coding, requires both accuracy and interpretability. Existing works often rely
on local interpretability methods, failing to provide comprehensive
explanations of the overall mechanism behind each label prediction within a
multilabel set. We propose a mechanistic interpretability module called
DIctionary Label Attention (\method) that disentangles uninterpretable dense
embeddings into a sparse embedding space, where each nonzero element (a
dictionary feature) represents a globally learned medical concept. Through
human evaluations, we show that our sparse embeddings are more human
understandable than its dense counterparts by at least 50 percent. Our
automated dictionary feature identification pipeline, leveraging large language
models (LLMs), uncovers thousands of learned medical concepts by examining and
summarizing the highest activating tokens for each dictionary feature. We
represent the relationships between dictionary features and medical codes
through a sparse interpretable matrix, enhancing the mechanistic and global
understanding of the model's predictions while maintaining competitive
performance and scalability without extensive human annotation.

摘要：在醫療編碼等領域中，預測高維度或極端的多標籤需要準確性和可解釋性。現有的工作通常依賴於局部可解釋性方法，無法對多標籤集中每個標籤預測背後的整體機制提供全面的解釋。我們提出了一個稱為字典標籤注意力的機械可解釋性模組（\method），它將不可解釋的稠密嵌入解開成一個稀疏嵌入空間，其中每個非零元素（一個字典特徵）代表一個全局學習的醫療概念。透過人為評估，我們表明我們的稀疏嵌入比其稠密對應物更易於人類理解，至少提高了 50%。我們自動化的字典特徵識別管道利用大型語言模型 (LLM)，透過檢查和總結每個字典特徵的最高激活代碼，揭示了數千個學習到的醫療概念。我們透過一個稀疏的可解釋矩陣來表示字典特徵與醫療代碼之間的關係，增強了對模型預測的機械和全局理解，同時在沒有廣泛的人工註解的情況下，保持了競爭效能和可擴充性。

##### **Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles**
2409.10502v1 by Kulin Shah, Nishanth Dikkala, Xin Wang, Rina Panigrahy

Causal language modeling using the Transformer architecture has yielded
remarkable capabilities in Large Language Models (LLMs) over the last few
years. However, the extent to which fundamental search and reasoning
capabilities emerged within LLMs remains a topic of ongoing debate. In this
work, we study if causal language modeling can learn a complex task such as
solving Sudoku puzzles. To solve a Sudoku, the model is first required to
search over all empty cells of the puzzle to decide on a cell to fill and then
apply an appropriate strategy to fill the decided cell. Sometimes, the
application of a strategy only results in thinning down the possible values in
a cell rather than concluding the exact value of the cell. In such cases,
multiple strategies are applied one after the other to fill a single cell. We
observe that Transformer models trained on this synthetic task can indeed learn
to solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly)
when trained on a logical sequence of steps taken by a solver. We find that
training Transformers with the logical sequence of steps is necessary and
without such training, they fail to learn Sudoku. We also extend our analysis
to Zebra puzzles (known as Einstein puzzles) and show that the model solves
$92.04 \%$ of the puzzles fully correctly. In addition, we study the internal
representations of the trained Transformer and find that through linear
probing, we can decode information about the set of possible values in any
given cell from them, pointing to the presence of a strong reasoning engine
implicit in the Transformer weights.

摘要：<paragraph>在过去几年中，使用 Transformer 架构的因果语言建模在大型语言模型 (LLM) 中产生了非凡的能力。然而，LLM 中基本搜索和推理能力的程度如何仍然是一个持续争论的话题。在这项工作中，我们研究因果语言建模是否可以学习一项复杂的任务，例如解决数独谜题。要解决数独谜题，首先要求模型搜索谜题的所有空单元格，以决定要填写的单元格，然后应用适当的策略来填充分配的单元格。有时，策略的应用只会减少单元格中的可能值，而不是得出单元格的确切值。在这种情况下，多个策略一个接一个地应用于填充单个单元格。我们观察到，在此合成任务上训练的 Transformer 模型确实可以学会解决数独谜题（我们的模型完全正确地解决了 94.21% 的谜题），当根据解决者采取的逻辑步骤序列进行训练时。我们发现，使用逻辑步骤序列训练 Transformer 是必要的，如果没有这样的训练，它们将无法学习数独。我们还将我们的分析扩展到斑马谜题（称为爱因斯坦谜题），并表明该模型完全正确地解决了 92.04% 的谜题。此外，我们研究了经过训练的 Transformer 的内部表示，并发现通过线性探测，我们可以从中解码有关任何给定单元格中可能值集合的信息，这表明 Transformer 权重中存在一个强大的推理引擎。</paragraph>

##### **MusicLIME: Explainable Multimodal Music Understanding**
2409.10496v1 by Theodoros Sotirou, Vassilis Lyberatos, Orfeas Menis Mastromichalakis, Giorgos Stamou

Multimodal models are critical for music understanding tasks, as they capture
the complex interplay between audio and lyrics. However, as these models become
more prevalent, the need for explainability grows-understanding how these
systems make decisions is vital for ensuring fairness, reducing bias, and
fostering trust. In this paper, we introduce MusicLIME, a model-agnostic
feature importance explanation method designed for multimodal music models.
Unlike traditional unimodal methods, which analyze each modality separately
without considering the interaction between them, often leading to incomplete
or misleading explanations, MusicLIME reveals how audio and lyrical features
interact and contribute to predictions, providing a holistic view of the
model's decision-making. Additionally, we enhance local explanations by
aggregating them into global explanations, giving users a broader perspective
of model behavior. Through this work, we contribute to improving the
interpretability of multimodal music models, empowering users to make informed
choices, and fostering more equitable, fair, and transparent music
understanding systems.

摘要：多模態模型對於音樂理解任務至關重要，因為它們捕捉了音訊和歌詞之間複雜的交互作用。然而，隨著這些模型變得越來越普遍，對於可解釋性的需求也隨之增加，了解這些系統如何做出決策對於確保公平性、減少偏見和建立信任至關重要。在本文中，我們介紹了 MusicLIME，一種與模型無關的功能重要性解釋方法，專為多模態音樂模型設計。與傳統的單模態方法不同，後者分別分析每個模態而不考慮它們之間的交互作用，通常會導致不完整或誤導性的解釋，MusicLIME 揭示了音訊和歌詞特徵如何交互並促成預測，提供了模型決策制定過程的整體視角。此外，我們通過將局部解釋匯總成全局解釋來增強局部解釋，使用戶對模型行為有更廣泛的了解。透過這項工作，我們有助於提高多模態音樂模型的可解釋性，使用戶能夠做出明智的選擇，並促進更公平、公正和透明的音樂理解系統。

##### **Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation**
2409.10494v1 by Noah Buchanan, Susan Gauch, Quan Mai

This paper presents a diffusion-based recommender system that incorporates
classifier-free guidance. Most current recommender systems provide
recommendations using conventional methods such as collaborative or
content-based filtering. Diffusion is a new approach to generative AI that
improves on previous generative AI approaches such as Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in
a recommender system that mirrors the sequence users take when browsing and
rating items. Although a few current recommender systems incorporate diffusion,
they do not incorporate classifier-free guidance, a new innovation in diffusion
models as a whole. In this paper, we present a diffusion recommender system
that augments the underlying recommender system model for improved performance
and also incorporates classifier-free guidance. Our findings show improvements
over state-of-the-art recommender systems for most metrics for several
recommendation tasks on a variety of datasets. In particular, our approach
demonstrates the potential to provide better recommendations when data is
sparse.

摘要：本文提出了一個基於擴散的推薦系統，它結合了無分類器引導。大多數當前推薦系統使用協同或基於內容的過濾等傳統方法提供推薦。擴散是一種生成式 AI 的新方法，它改進了先前的生成式 AI 方法，例如變異自動編碼器 (VAE) 和生成對抗網路 (GAN)。我們在推薦系統中整合了擴散，它反映了使用者在瀏覽和評分項目時的順序。儘管少數當前推薦系統整合了擴散，但它們沒有整合無分類器引導，這是擴散模型整體的一項創新。在本文中，我們提出了一個擴散推薦系統，它擴充了底層推薦系統模型以提高效能，並整合了無分類器引導。我們的研究結果顯示，在各種資料集的幾個推薦任務中，我們的發現優於大多數指標的最新推薦系統。特別是，我們的做法證明了在資料稀疏時提供更好推薦的潛力。

##### **Flash STU: Fast Spectral Transform Units**
2409.10489v2 by Y. Isabel Liu, Windsor Nguyen, Yagiz Devre, Evan Dogariu, Anirudha Majumdar, Elad Hazan

This paper describes an efficient, open source PyTorch implementation of the
Spectral Transform Unit. We investigate sequence prediction tasks over several
modalities including language, robotics, and simulated dynamical systems. We
find that for the same parameter count, the STU and its variants outperform the
Transformer as well as other leading state space models across various
modalities.

摘要：本文描述了一個高效的、開源的 Spectral Transform Unit PyTorch 實作。我們針對多種方式探討序列預測任務，包括語言、機器人和模擬動力系統。我們發現對於相同的參數數量，STU 及其變體優於 Transformer 以及各種方式中其他領先的狀態空間模型。

##### **Do Pre-trained Vision-Language Models Encode Object States?**
2409.10488v1 by Kaleb Newman, Shijie Wang, Yuan Zang, David Heffren, Chen Sun

For a vision-language model (VLM) to understand the physical world, such as
cause and effect, a first step is to capture the temporal dynamics of the
visual world, for example how the physical states of objects evolve over time
(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs
pre-trained on web-scale data learn to encode object states, which can be
extracted with zero-shot text prompts. We curate an object state recognition
dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models
trained with contrastive and generative objectives. We observe that while these
state-of-the-art vision-language models can reliably perform object
recognition, they consistently fail to accurately distinguish the objects'
physical states. Through extensive experiments, we identify three areas for
improvements for VLMs to better encode object states, namely the quality of
object localization, the architecture to bind concepts to objects, and the
objective to learn discriminative visual and language encoders on object
states. Data and code are released.

摘要：對於視覺語言模型 (VLM) 來說，要理解物理世界，例如因果關係，第一步是捕捉視覺世界的時間動態，例如物體的物理狀態如何隨時間演變（例如，一個完整的蘋果變成一個切片的蘋果）。我們的論文旨在探討在網路規模數據上預先訓練的 VLM 是否學會編碼物件狀態，而這些狀態可以用零次學習文字提示提取。我們策劃了一個物件狀態辨識資料集 ChangeIt-Frames，並評估九個開放原始碼的 VLM，包括使用對比和生成目標訓練的模型。我們觀察到，儘管這些最先進的視覺語言模型可以可靠地執行物件辨識，但它們始終無法準確區分物件的物理狀態。透過廣泛的實驗，我們找出三個 VLM 改進領域，以更好地編碼物件狀態，即物件定位的品質、將概念與物件結合的架構，以及在物件狀態上學習判別式視覺和語言編碼器的目標。資料和程式碼已發布。

##### **Schrodinger's Memory: Large Language Models**
2409.10482v2 by Wei Wang, Qing Li

Memory is the foundation of all human activities; without memory, it would be
nearly impossible for people to perform any task in daily life. With the
development of Large Language Models (LLMs), their language capabilities are
becoming increasingly comparable to those of humans. But do LLMs have memory?
Based on current performance, LLMs do appear to exhibit memory. So, what is the
underlying mechanism of this memory? Previous research has lacked a deep
exploration of LLMs' memory capabilities and the underlying theory. In this
paper, we use Universal Approximation Theorem (UAT) to explain the memory
mechanism in LLMs. We also conduct experiments to verify the memory
capabilities of various LLMs, proposing a new method to assess their abilities
based on these memory ability. We argue that LLM memory operates like
Schr\"odinger's memory, meaning that it only becomes observable when a specific
memory is queried. We can only determine if the model retains a memory based on
its output in response to the query; otherwise, it remains indeterminate.
Finally, we expand on this concept by comparing the memory capabilities of the
human brain and LLMs, highlighting the similarities and differences in their
operational mechanisms.

摘要：記憶是所有人類活動的基礎；沒有記憶，人類幾乎不可能執行日常生活中任何任務。隨著大型語言模型 (LLM) 的發展，它們的語言能力正變得越來越接近人類。但 LLM 是否有記憶？根據目前的表現，LLM 似乎確實展現了記憶。那麼，這種記憶的底層機制是什麼？先前的研究缺乏對 LLM 的記憶能力和底層理論的深入探討。在本文中，我們使用通用逼近定理 (UAT) 來解釋 LLM 中的記憶機制。我們還進行實驗來驗證各種 LLM 的記憶能力，提出了一種基於這些記憶能力評估其能力的新方法。我們認為 LLM 記憶就像薛丁格的記憶一樣，也就是說，只有在查詢特定記憶時才會變得可觀察。我們只能根據模型對查詢的回應來確定模型是否保留了記憶；否則，它仍然是不確定的。最後，我們通過比較人腦和 LLM 的記憶能力，擴展了這個概念，強調了它們運作機制中的相似性和差異性。

##### **Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance**
2409.10481v1 by Simone Maurizio La Cava, Sara Concas, Ruben Tolosana, Roberto Casula, Giulia Orrù, Martin Drahansky, Julian Fierrez, Gian Luca Marcialis

3D face reconstruction (3DFR) algorithms are based on specific assumptions
tailored to distinct application scenarios. These assumptions limit their use
when acquisition conditions, such as the subject's distance from the camera or
the camera's characteristics, are different than expected, as typically happens
in video surveillance. Additionally, 3DFR algorithms follow various strategies
to address the reconstruction of a 3D shape from 2D data, such as statistical
model fitting, photometric stereo, or deep learning. In the present study, we
explore the application of three 3DFR algorithms representative of the SOTA,
employing each one as the template set generator for a face verification
system. The scores provided by each system are combined by score-level fusion.
We show that the complementarity induced by different 3DFR algorithms improves
performance when tests are conducted at never-seen-before distances from the
camera and camera characteristics (cross-distance and cross-camera settings),
thus encouraging further investigations on multiple 3DFR-based approaches.

摘要：3D 人臉重建 (3DFR) 演算法基於特定假設，針對不同的應用場景量身打造。這些假設限制了其使用，當擷取條件（例如主體與相機的距離或相機的特性）與預期不同時，例如在視訊監控中通常發生的情況。此外，3DFR 演算法採用各種策略來解決從 2D 資料重建 3D 形狀的問題，例如統計模型擬合、光度立體或深度學習。在本研究中，我們探討了三種代表 SOTA 的 3DFR 演算法的應用，並將每種演算法用作人臉驗證系統的範本設定產生器。每個系統提供的分數會透過分數層級融合結合。我們顯示由不同 3DFR 演算法引發的互補性在從未見過的相機距離和相機特性（跨距離和跨相機設定）進行測試時會提升效能，因此鼓勵進一步研究基於多重 3DFR 的方法。

##### **MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion**
2409.10473v1 by Lehong Wu, Lilang Lin, Jiahang Zhang, Yiyang Ma, Jiaying Liu

Self-supervised learning has proved effective for skeleton-based human action
understanding. However, previous works either rely on contrastive learning that
suffers false negative problems or are based on reconstruction that learns too
much unessential low-level clues, leading to limited representations for
downstream tasks. Recently, great advances have been made in generative
learning, which is naturally a challenging yet meaningful pretext task to model
the general underlying data distributions. However, the representation learning
capacity of generative models is under-explored, especially for the skeletons
with spacial sparsity and temporal redundancy. To this end, we propose Masked
Conditional Diffusion (MacDiff) as a unified framework for human skeleton
modeling. For the first time, we leverage diffusion models as effective
skeleton representation learners. Specifically, we train a diffusion decoder
conditioned on the representations extracted by a semantic encoder. Random
masking is applied to encoder inputs to introduce a information bottleneck and
remove redundancy of skeletons. Furthermore, we theoretically demonstrate that
our generative objective involves the contrastive learning objective which
aligns the masked and noisy views. Meanwhile, it also enforces the
representation to complement for the noisy view, leading to better
generalization performance. MacDiff achieves state-of-the-art performance on
representation learning benchmarks while maintaining the competence for
generative tasks. Moreover, we leverage the diffusion model for data
augmentation, significantly enhancing the fine-tuning performance in scenarios
with scarce labeled data. Our project is available at
https://lehongwu.github.io/ECCV24MacDiff/.

摘要：<paragraph>自监督学习已被证明对基于骨架的人体动作理解有效。然而，先前的工作要么依赖于对比学习，对比学习存在假阴性问题，要么基于重建，重建学习了太多不必要的低级线索，导致下游任务的表示有限。最近，生成式学习取得了很大进展，这自然是一个具有挑战性但有意义的借口任务，可以对一般潜在数据分布进行建模。然而，生成模型的表示学习能力尚未得到充分探索，特别是对于具有空间稀疏性和时间冗余性的骨架。为此，我们提出了掩码条件扩散 (MacDiff) 作为人体骨架建模的统一框架。我们首次利用扩散模型作为有效的骨架表示学习器。具体来说，我们训练了一个扩散解码器，该解码器以语义编码器提取的表示为条件。随机掩码应用于编码器输入，以引入信息瓶颈并去除骨架的冗余。此外，我们从理论上证明了我们的生成目标涉及对比学习目标，该目标将掩码和噪声视图对齐。同时，它还强制表示补充噪声视图，从而带来更好的泛化性能。MacDiff 在表示学习基准上实现了最先进的性能，同时保持了生成任务的能力。此外，我们利用扩散模型进行数据增强，在标记数据稀缺的情况下显著提高了微调性能。我们的项目可在 https://lehongwu.github.io/ECCV24MacDiff/ 获得。</paragraph>

##### **Meta-Whisper: Speech-Based Meta-ICL for ASR on Low-Resource Languages**
2409.10429v1 by Ming-Hao Hsu, Kuan Po Huang, Hung-yi Lee

This paper presents Meta-Whisper, a novel approach to improve automatic
speech recognition (ASR) for low-resource languages using the Whisper model. By
leveraging Meta In-Context Learning (Meta-ICL) and a k-Nearest Neighbors (KNN)
algorithm for sample selection, Meta-Whisper enhances Whisper's ability to
recognize speech in unfamiliar languages without extensive fine-tuning.
Experiments on the ML-SUPERB dataset show that Meta-Whisper significantly
reduces the Character Error Rate (CER) for low-resource languages compared to
the original Whisper model. This method offers a promising solution for
developing more adaptable multilingual ASR systems, particularly for languages
with limited resources.

摘要：這篇論文提出了 Meta-Whisper，一種使用 Whisper 模型來改善低資源語言的自動語音辨識 (ASR) 的新方法。透過利用元語境學習 (Meta-ICL) 和 k-最近鄰居 (KNN) 演算法來進行樣本選擇，Meta-Whisper 增強了 Whisper 在不進行大量微調的情況下辨識陌生語言語音的能力。在 ML-SUPERB 資料集上的實驗顯示，與原始 Whisper 模型相比，Meta-Whisper 大幅降低了低資源語言的字元錯誤率 (CER)。這種方法為開發更具適應性的多語言 ASR 系統提供了有前景的解決方案，特別是對於資源有限的語言。

##### **HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models**
2409.10419v1 by Vineet Bhat, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami

Robots interacting with humans through natural language can unlock numerous
applications such as Referring Grasp Synthesis (RGS). Given a text query, RGS
determines a stable grasp pose to manipulate the referred object in the robot's
workspace. RGS comprises two steps: visual grounding and grasp pose estimation.
Recent studies leverage powerful Vision-Language Models (VLMs) for visually
grounding free-flowing natural language in real-world robotic execution.
However, comparisons in complex, cluttered environments with multiple instances
of the same object are lacking. This paper introduces HiFi-CS, featuring
hierarchical application of Featurewise Linear Modulation (FiLM) to fuse image
and text embeddings, enhancing visual grounding for complex attribute rich text
queries encountered in robotic grasping. Visual grounding associates an object
in 2D/3D space with natural language input and is studied in two scenarios:
Closed and Open Vocabulary. HiFi-CS features a lightweight decoder combined
with a frozen VLM and outperforms competitive baselines in closed vocabulary
settings while being 100x smaller in size. Our model can effectively guide
open-set object detectors like GroundedSAM to enhance open-vocabulary
performance. We validate our approach through real-world RGS experiments using
a 7-DOF robotic arm, achieving 90.33\% visual grounding accuracy in 15 tabletop
scenes. We include our codebase in the supplementary material.

摘要：機器人透過自然語言與人類互動，能夠解鎖許多應用程式，例如參考抓取合成 (RGS)。針對文字查詢，RGS 會決定一個穩定的抓取姿勢，以便在機器人的工作空間中操作所參考的物件。RGS 包含兩個步驟：視覺基礎和抓取姿勢估計。最近的研究利用強大的視覺語言模型 (VLM) 來視覺化基礎，將自然語言自由流動到真實世界的機器人執行。然而，在複雜、混亂的環境中，對於同一個物件有多個實例的比較卻有所欠缺。本文介紹了 HiFi-CS，其特點是分層應用特徵線性調變 (FiLM) 來融合影像和文字嵌入，增強視覺基礎，以應對機器人抓取中遇到的複雜屬性豐富文字查詢。視覺基礎將 2D/3D 空間中的物件與自然語言輸入關聯起來，並在兩個場景中進行研究：封閉和開放詞彙。HiFi-CS 具有輕量級解碼器，並結合凍結的 VLM，在封閉詞彙設定中優於競爭基準，同時大小卻小了 100 倍。我們的模型可以有效地引導開放式物件偵測器，例如 GroundedSAM，以增強開放詞彙效能。我們透過使用 7-DOF 機器手臂進行真實世界的 RGS 實驗來驗證我們的做法，在 15 個桌面場景中達到 90.33% 的視覺基礎準確度。我們在補充資料中包含我們的程式碼庫。

##### **A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration**
2409.10403v1 by Zhang Zheng

This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.

摘要：本文提出了一种基于提示学习框架的知识增强疾病诊断方法。该方法从与临床病例相关的外部知识图谱中检索结构化知识，对其进行编码，并将其注入到提示模板中，以增强语言模型对任务的理解和推理能力。我们在三个公共数据集上进行了实验：CHIP-CTC、IMCS-V2-NER 和 KUAKE-QTR。结果表明，所提出的方法在多个评估指标上明显优于现有模型，在 CHIP-CTC 数据集上的 F1 得分提高了 2.4%，在 IMCS-V2-NER 数据集上提高了 3.1%，在 KUAKE-QTR 数据集上提高了 4.2%。此外，消融研究证实了知识注入模块的关键作用，因为移除此模块会导致 F1 得分显着下降。实验结果表明，所提出的方法不仅有效提高了疾病诊断的准确性，而且增强了预测的可解释性，为临床诊断提供了更可靠的支持和证据。

##### **Instigating Cooperation among LLM Agents Using Adaptive Information Modulation**
2409.10372v1 by Qiliang Chen, Alireza, Ilami, Nunzio Lore, Babak Heydari

This paper introduces a novel framework combining LLM agents as proxies for
human strategic behavior with reinforcement learning (RL) to engage these
agents in evolving strategic interactions within team environments. Our
approach extends traditional agent-based simulations by using strategic LLM
agents (SLA) and introducing dynamic and adaptive governance through a
pro-social promoting RL agent (PPA) that modulates information access across
agents in a network, optimizing social welfare and promoting pro-social
behavior. Through validation in iterative games, including the prisoner
dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.
The PPA agent effectively learns to adjust information transparency, resulting
in enhanced cooperation rates. This framework offers significant insights into
AI-mediated social dynamics, contributing to the deployment of AI in real-world
team settings.

摘要：本文介紹一個創新的框架，結合 LLM 代理作為人類策略行為的代理，並使用強化學習 (RL) 讓這些代理參與團隊環境中的策略互動演進。我們的做法透過使用策略 LLM 代理 (SLA) 和透過促進社會行為的 RL 代理 (PPA) 引入動態和適應式治理來擴充傳統的基於代理的模擬，PPA 調節網路中代理之間的資訊存取，最佳化社會福利並促進社會行為。透過在重複遊戲（包括囚徒困境）中驗證，我們證明 SLA 代理展現出細微的策略適應。PPA 代理有效地學習調整資訊透明度，導致合作率提升。這個框架提供關於 AI 媒介的社會動態的重要見解，有助於在真實世界的團隊設定中部署 AI。

##### **Robust image representations with counterfactual contrastive learning**
2409.10365v1 by Mélanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker

Contrastive pretraining can substantially increase model generalisation and
downstream performance. However, the quality of the learned representations is
highly dependent on the data augmentation strategy applied to generate positive
pairs. Positive contrastive pairs should preserve semantic meaning while
discarding unwanted variations related to the data acquisition domain.
Traditional contrastive pipelines attempt to simulate domain shifts through
pre-defined generic image transformations. However, these do not always mimic
realistic and relevant domain variations for medical imaging such as scanner
differences. To tackle this issue, we herein introduce counterfactual
contrastive learning, a novel framework leveraging recent advances in causal
image synthesis to create contrastive positive pairs that faithfully capture
relevant domain variations. Our method, evaluated across five datasets
encompassing both chest radiography and mammography data, for two established
contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive
learning in terms of robustness to acquisition shift. Notably, counterfactual
contrastive learning achieves superior downstream performance on both
in-distribution and on external datasets, especially for images acquired with
scanners under-represented in the training set. Further experiments show that
the proposed framework extends beyond acquisition shifts, with models trained
with counterfactual contrastive learning substantially improving subgroup
performance across biological sex.

摘要：對比預訓練可以大幅提升模型的泛化能力和下游效能。然而，學習到的表徵品質高度依賴於用來產生正向配對的資料擴充策略。正向對比配對應當保留語意意義，同時捨棄與資料擷取領域相關的不必要變異。傳統的對比管線會嘗試透過預先定義的通用影像轉換來模擬領域轉移。然而，這些轉換並不總是能模仿醫療影像的實際且相關領域變異，例如掃描儀的差異。為了解決這個問題，我們在此提出反事實對比學習，一個利用因果影像合成近期進展來建立忠實捕捉相關領域變異的對比正向配對的新穎架構。我們的做法在涵蓋胸部 X 光和乳房攝影資料的五個資料集上進行評估，對於兩個已建立的對比目標（SimCLR 和 DINO-v2），在對於擷取轉移的穩健性方面優於標準對比學習。值得注意的是，反事實對比學習在內部分佈和外部資料集上都能達成優異的下游效能，特別是對於訓練集中代表性不足的掃描儀所擷取的影像。進一步的實驗顯示，所提出的架構延伸到擷取轉移之外，使用反事實對比學習訓練的模型大幅提升了生物性別的子群效能。

##### **2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-Speech Gesture Generation?**
2409.10357v1 by Téo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud

Co-speech gestures are fundamental for communication. The advent of recent
deep learning techniques has facilitated the creation of lifelike, synchronous
co-speech gestures for Embodied Conversational Agents. "In-the-wild" datasets,
aggregating video content from platforms like YouTube via human pose detection
technologies, provide a feasible solution by offering 2D skeletal sequences
aligned with speech. Concurrent developments in lifting models enable the
conversion of these 2D sequences into 3D gesture databases. However, it is
important to note that the 3D poses estimated from the 2D extracted poses are,
in essence, approximations of the ground-truth, which remains in the 2D domain.
This distinction raises questions about the impact of gesture representation
dimensionality on the quality of generated motions - a topic that, to our
knowledge, remains largely unexplored. Our study examines the effect of using
either 2D or 3D joint coordinates as training data on the performance of
speech-to-gesture deep generative models. We employ a lifting model for
converting generated 2D pose sequences into 3D and assess how gestures created
directly in 3D stack up against those initially generated in 2D and then
converted to 3D. We perform an objective evaluation using widely used metrics
in the gesture generation field as well as a user study to qualitatively
evaluate the different approaches.

摘要：共時手勢對於溝通至關重要。最近深度學習技術的出現促進了擬真、同步共時手勢的建立，用於具身對話代理。「野外」數據集通過人體姿勢檢測技術彙集來自 YouTube 等平台的影片內容，提供了一個可行的解決方案，即提供與語音對齊的 2D 骨骼序列。提升模型的同步發展使得這些 2D 序列能夠轉換為 3D 手勢數據庫。然而，重要的是要注意，從 2D 提取的手勢估計的 3D 姿勢本質上是對真實情況的近似，而真實情況仍然在 2D 領域。這種區別引發了有關手勢表示維度對生成動作品質影響的問題，據我們所知，這是一個在很大程度上尚未探索的主題。我們的研究探討了使用 2D 或 3D 關節坐標作為訓練資料對語音到手勢深度生成模型效能的影響。我們採用一個提升模型，將生成的 2D 姿勢序列轉換為 3D，並評估直接在 3D 中建立的手勢與最初在 2D 中生成然後轉換為 3D 的手勢相比如何。我們使用手勢生成領域中廣泛使用的指標進行客觀評估，並進行使用者研究以定性評估不同的方法。

##### **Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation**
2409.10343v1 by Tianrui Song, Wenshuo Chao, Hao Liu

Implicit feedback, often used to build recommender systems, unavoidably
confronts noise due to factors such as misclicks and position bias. Previous
studies have attempted to alleviate this by identifying noisy samples based on
their diverged patterns, such as higher loss values, and mitigating the noise
through sample dropping or reweighting. Despite the progress, we observe
existing approaches struggle to distinguish hard samples and noise samples, as
they often exhibit similar patterns, thereby limiting their effectiveness in
denoising recommendations. To address this challenge, we propose a Large
Language Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,
we construct an LLM-based scorer to evaluate the semantic consistency of items
with the user preference, which is quantified based on summarized historical
user interactions. The resulting scores are used to assess the hardness of
samples for the pointwise or pairwise training objectives. To ensure
efficiency, we introduce a variance-based sample pruning strategy to filter
potential hard samples before scoring. Besides, we propose an iterative
preference update module designed to continuously refine summarized user
preference, which may be biased due to false-positive user-item interactions.
Extensive experiments on three real-world datasets and four backbone
recommenders demonstrate the effectiveness of our approach.

摘要：隱含回饋通常用於建立推薦系統，不可避免地會遇到雜訊，原因在於誤點和位置偏差等因素。先前的研究已嘗試透過識別雜訊樣本（根據它們的發散模式，例如較高的損失值）並透過樣本刪除或重新加權來減輕雜訊。儘管有進展，我們觀察到現有方法難以區分困難樣本和雜訊樣本，因為它們通常表現出類似的模式，從而限制了它們在去噪推薦中的有效性。為了應對這一挑戰，我們提出了一個大型語言模型增強的困難樣本去噪 (LLMHD) 框架。具體來說，我們建構了一個基於 LLM 的評分器來評估項目與使用者偏好的語義一致性，該一致性是根據總結的歷史使用者互動來量化的。產生的分數用於評估樣本對於逐點或成對訓練目標的難度。為了確保效率，我們引入了一個基於變異的樣本修剪策略，在評分之前過濾潛在的困難樣本。此外，我們提出了一個迭代偏好更新模組，旨在持續改善總結的使用者偏好，而這些偏好可能會因為誤判的使用者-項目互動而產生偏差。在三個真實世界資料集和四個主幹推薦系統上的廣泛實驗證明了我們方法的有效性。

##### **Detecting Sexism in German Online Newspaper Comments with Open-Source Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks 1 and 2, Closed Track)**
2409.10341v1 by Florian Bremm, Patrick Gustav Blaneck, Tobias Bornheim, Niklas Grieger, Stephan Bialonski

Sexism in online media comments is a pervasive challenge that often manifests
subtly, complicating moderation efforts as interpretations of what constitutes
sexism can vary among individuals. We study monolingual and multilingual
open-source text embeddings to reliably detect sexism and misogyny in
German-language online comments from an Austrian newspaper. We observed
classifiers trained on text embeddings to mimic closely the individual
judgements of human annotators. Our method showed robust performance in the
GermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1
score of 0.597 (4th place, as reported on Codabench). It also accurately
predicted the distribution of human annotations in GerMS-Detect Subtask 2, with
an average Jensen-Shannon distance of 0.301 (2nd place). The computational
efficiency of our approach suggests potential for scalable applications across
various languages and linguistic contexts.

摘要：網路媒體留言中的性別歧視是一種普遍的挑戰，通常以微妙的方式表現出來，讓審核工作變得複雜，因為人們對什麼構成性別歧視的解釋可能有所不同。我們研究單語和多語的開源文字嵌入，以可靠地偵測奧地利報紙德語網路留言中的性別歧視和厭女症。我們觀察到在文字嵌入上訓練的分類器，可以緊密模仿人類標註者的個人判斷。我們的技術在 GermEval 2024 GerMS-Detect 子任務 1 挑戰中展現了強健的表現，達到了平均巨觀 F1 分數 0.597（第 4 名，根據 Codabench 的報告）。它也準確預測了 GerMS-Detect 子任務 2 中人類標註的分布，平均 Jensen-Shannon 距離為 0.301（第 2 名）。我們的技術在運算效率上的表現，顯示出跨各種語言和語言環境的可擴充應用潛力。

##### **Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs**
2409.10340v1 by Mehrad Soltani, Luis Rueda

Hypergraphs tackle the limitations of traditional graphs by introducing {\em
hyperedges}. While graph edges connect only two nodes, hyperedges connect an
arbitrary number of nodes along their edges. Also, the underlying
message-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the
form of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and
more complex structural information than traditional Graph Neural Networks
(GNNs). More recently, the idea of overlapping subgraphs has emerged. These
subgraphs can capture more information about subgroups of vertices without
limiting one vertex belonging to just one group, allowing vertices to belong to
multiple groups or subgraphs. In addition, one of the most important problems
in graph clustering is to find densest overlapping subgraphs (DOS). In this
paper, we propose a solution to the DOS problem via Agglomerative Greedy
Enumeration (DOSAGE) algorithm as a novel approach to enhance the process of
generating the densest overlapping subgraphs and, hence, a robust construction
of the hypergraphs. Experiments on standard benchmarks show that the DOSAGE
algorithm significantly outperforms the HGNNs and six other methods on the node
classification task.

摘要：超圖透過引入{\em超邊}來解決傳統圖表的限制。圖表的邊緣僅連接兩個節點，而超邊則沿著邊緣連接任意數量的節點。此外，超圖神經網路 (HGNN) 中的基礎訊息傳遞機制採用頂點-超邊-頂點的形式，這讓 HGNN 能夠擷取和利用比傳統圖神經網路 (GNN) 更豐富且更複雜的結構資訊。最近，重疊子圖的概念應運而生。這些子圖可以擷取更多關於頂點子群的資訊，而不會限制一個頂點只屬於一個群組，允許頂點屬於多個群組或子圖。此外，圖形聚類中最重要的問題之一是找到最密集的重疊子圖 (DOS)。在本文中，我們透過 Agglomerative Greedy Enumeration (DOSAGE) 演算法提出 DOS 問題的解決方案，作為一種增強生成最密集重疊子圖的程序的新方法，進而穩健地建構超圖。標準基準上的實驗顯示，DOSAGE 演算法在節點分類任務上顯著優於 HGNN 和其他六種方法。

##### **The 20 questions game to distinguish large language models**
2409.10338v1 by Gurvan Richardeau, Erwan Le Merrer, Camilla Penzo, Gilles Tredan

In a parallel with the 20 questions game, we present a method to determine
whether two large language models (LLMs), placed in a black-box context, are
the same or not. The goal is to use a small set of (benign) binary questions,
typically under 20. We formalize the problem and first establish a baseline
using a random selection of questions from known benchmark datasets, achieving
an accuracy of nearly 100% within 20 questions. After showing optimal bounds
for this problem, we introduce two effective questioning heuristics able to
discriminate 22 LLMs by using half as many questions for the same task. These
methods offer significant advantages in terms of stealth and are thus of
interest to auditors or copyright owners facing suspicions of model leaks.

摘要：在與 20 個問題遊戲相似的遊戲中，我們提出了一種方法來確定
是否將兩個大型語言模型 (LLM) 放置在黑盒子上下文中，
相同或不同。目標是使用一組小的（良性的）二元問題，
通常在 20 個以下。我們形式化問題，並首先使用從已知基準資料集中隨機選擇的問題建立基準，
在 20 個問題內達到近 100% 的準確度。在顯示此問題的最佳界限後，我們介紹了兩個有效的詢問啟發法
能夠區分 22 個 LLM，同時使用一半的問題來完成相同的任務。這些
方法在隱身方面提供了顯著的優勢，因此對於面對模型洩漏嫌疑的審計員或版權所有者而言很有意義。

##### **InfoDisent: Explainability of Image Classification Models by Information Disentanglement**
2409.10329v1 by Łukasz Struski, Jacek Tabor

Understanding the decisions made by image classification networks is a
critical area of research in deep learning. This task is traditionally divided
into two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc
methods, such as GradCam, aim to interpret the decisions of pre-trained models
by identifying regions of the image where the network focuses its attention.
However, these methods provide only a high-level overview, making it difficult
to fully understand the network's decision-making process. Conversely,
intrinsic methods, like prototypical parts models, offer a more detailed
understanding of network predictions but are constrained by specific
architectures, training methods, and datasets.
  In this paper, we introduce InfoDisent, a hybrid model that combines the
advantages of both approaches. By utilizing an information bottleneck,
InfoDisent disentangles the information in the final layer of a pre-trained
deep network, enabling the breakdown of classification decisions into basic,
understandable atomic components. Unlike standard prototypical parts
approaches, InfoDisent can interpret the decisions of pre-trained
classification networks and be used for making classification decisions,
similar to intrinsic models. We validate the effectiveness of InfoDisent on
benchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and Stanford
Dogs for both convolutional and transformer backbones.

摘要：理解图像分类网络做出的决策是深度学习中一个关键的研究领域。这项任务传统上分为两种不同的方法：事后方法和内在方法。事后方法，例如 GradCam，旨在通过识别网络关注图像区域来解释预训练模型的决策。然而，这些方法仅提供高级概述，难以完全理解网络的决策过程。相反，原型部分模型等内在方法提供了对网络预测的更详细理解，但受到特定架构、训练方法和数据集的限制。在本文中，我们介绍了 InfoDisent，这是一个结合了两种方法优势的混合模型。通过利用信息瓶颈，InfoDisent 解开了预训练深度网络最后一层中的信息，从而将分类决策分解为基本的、可理解的原子组件。与标准原型部分方法不同，InfoDisent 可以解释预训练分类网络的决策，并用于做出分类决策，类似于内在模型。我们在 ImageNet、CUB-200-2011、Stanford Cars 和 Stanford Dogs 等基准数据集上验证了 InfoDisent 的有效性，用于卷积和转换器骨干。

##### **Know your limits! Optimize the robot's behavior through self-awareness**
2409.10308v1 by Esteve Valls Mascaro, Dongheui Lee

As humanoid robots transition from labs to real-world environments, it is
essential to democratize robot control for non-expert users. Recent human-robot
imitation algorithms focus on following a reference human motion with high
precision, but they are susceptible to the quality of the reference motion and
require the human operator to simplify its movements to match the robot's
capabilities. Instead, we consider that the robot should understand and adapt
the reference motion to its own abilities, facilitating the operator's task.
For that, we introduce a deep-learning model that anticipates the robot's
performance when imitating a given reference. Then, our system can generate
multiple references given a high-level task command, assign a score to each of
them, and select the best reference to achieve the desired robot behavior. Our
Self-AWare model (SAW) ranks potential robot behaviors based on various
criteria, such as fall likelihood, adherence to the reference motion, and
smoothness. We integrate advanced motion generation, robot control, and SAW in
one unique system, ensuring optimal robot behavior for any task command. For
instance, SAW can anticipate falls with 99.29% accuracy. For more information
check our project page: https://evm7.github.io/Self-AWare

摘要：當類人機器人從實驗室轉移到真實世界的環境中時，讓非專家使用者民主化機器人控制至關重要。最近的人機模擬演算法專注於以高精度追蹤參考人類動作，但它們容易受到參考動作的品質影響，並要求人類操作員簡化其動作以匹配機器人的能力。相反地，我們認為機器人應該了解並調整參考動作以適應其自身的能力，從而簡化操作員的任務。為此，我們引入一個深度學習模型，它可以預測機器人在模擬給定參考時的效果。然後，我們的系統可以根據高階任務命令產生多個參考，為每個參考分配一個分數，並選擇最佳參考以實現所需的機器人行為。我們的 Self-AWare 模型 (SAW) 根據各種標準對潛在的機器人行為進行排名，例如跌倒機率、對參考動作的遵守程度和流暢度。我們將先進的動作產生、機器人控制和 SAW 整合到一個獨特的系統中，確保任何任務命令都能獲得最佳的機器人行為。例如，SAW 可以預測跌倒，準確率高達 99.29%。有關更多資訊，請查看我們的專案頁面：https://evm7.github.io/Self-AWare

##### **On Synthetic Texture Datasets: Challenges, Creation, and Curation**
2409.10297v1 by Blaine Hoak, Patrick McDaniel

The influence of textures on machine learning models has been an ongoing
investigation, specifically in texture bias/learning, interpretability, and
robustness. However, due to the lack of large and diverse texture data
available, the findings in these works have been limited, as more comprehensive
evaluations have not been feasible. Image generative models are able to provide
data creation at scale, but utilizing these models for texture synthesis has
been unexplored and poses additional challenges both in creating accurate
texture images and validating those images. In this work, we introduce an
extensible methodology and corresponding new dataset for generating
high-quality, diverse texture images capable of supporting a broad set of
texture-based tasks. Our pipeline consists of: (1) developing prompts from a
range of descriptors to serve as input to text-to-image models, (2) adopting
and adapting Stable Diffusion pipelines to generate and filter the
corresponding images, and (3) further filtering down to the highest quality
images. Through this, we create the Prompted Textures Dataset (PTD), a dataset
of 362,880 texture images that span 56 textures. During the process of
generating images, we find that NSFW safety filters in image generation
pipelines are highly sensitive to texture (and flag up to 60\% of our texture
images), uncovering a potential bias in these models and presenting unique
challenges when working with texture data. Through both standard metrics and a
human evaluation, we find that our dataset is high quality and diverse.

摘要：機器學習模型中紋理的影響一直是持續的調查，特別是在紋理偏差/學習、可解釋性和魯棒性方面。然而，由於缺乏大量且多樣化的紋理數據，這些作品中的發現受到限制，因為更全面的評估並不可行。圖像生成模型能夠大規模地提供數據創建，但將這些模型用於紋理合成尚未被探索，並且在創建準確的紋理圖像和驗證這些圖像方面提出了額外的挑戰。在這項工作中，我們引入了一個可擴展的方法和相應的新數據集，用於生成能夠支持廣泛的基於紋理任務的高品質、多樣化的紋理圖像。我們的管道包括：(1) 從一系列描述符中開發提示，作為文本到圖像模型的輸入，(2) 採用和改編 Stable Diffusion 管道來生成和過濾相應的圖像，(3) 進一步過濾到最高品質的圖像。通過這種方式，我們創建了提示紋理數據集 (PTD)，一個包含 362,880 個紋理圖像的數據集，跨越 56 個紋理。在生成圖像的過程中，我們發現圖像生成管道中的 NSFW 安全過濾器對紋理高度敏感（並標記出我們紋理圖像的 60%），揭示了這些模型中的潛在偏差，並在處理紋理數據時提出了獨特的挑戰。通過標準指標和人工評估，我們發現我們的數據集質量高且多樣化。

##### **MGSA: Multi-granularity Graph Structure Attention for Knowledge Graph-to-Text Generation**
2409.10294v1 by Shanshan Wang, Chun Zhang, Ning Zhang

The Knowledge Graph-to-Text Generation task aims to convert structured
knowledge graphs into coherent and human-readable natural language text. Recent
efforts in this field have focused on enhancing pre-trained language models
(PLMs) by incorporating graph structure information to capture the intricate
structure details of knowledge graphs. However, most of these approaches tend
to capture only single-granularity structure information, concentrating either
on the relationships between entities within the original graph or on the
relationships between words within the same entity or across different
entities. This narrow focus results in a significant limitation: models that
concentrate solely on entity-level structure fail to capture the nuanced
semantic relationships between words, while those that focus only on word-level
structure overlook the broader relationships between original entire entities.
To overcome these limitations, this paper introduces the Multi-granularity
Graph Structure Attention (MGSA), which is based on PLMs. The encoder of the
model architecture features an entity-level structure encoding module, a
word-level structure encoding module, and an aggregation module that
synthesizes information from both structure. This multi-granularity structure
encoding approach allows the model to simultaneously capture both entity-level
and word-level structure information, providing a more comprehensive
understanding of the knowledge graph's structure information, thereby
significantly improving the quality of the generated text. We conducted
extensive evaluations of the MGSA model using two widely recognized KG-to-Text
Generation benchmark datasets, WebNLG and EventNarrative, where it consistently
outperformed models that rely solely on single-granularity structure
information, demonstrating the effectiveness of our approach.

摘要：知識圖譜到文字生成任務旨在將結構化知識圖譜轉換為連貫且人類可讀的自然語言文字。最近在這個領域的研究集中於透過納入圖形結構資訊來增強預訓練語言模型 (PLM)，以擷取知識圖譜的複雜結構細節。然而，這些方法大多傾向於僅擷取單一粒度的結構資訊，專注於原始圖形中實體之間的關係或同一個實體或不同實體之間的單字關係。這種狹隘的焦點導致一個顯著的限制：僅專注於實體層級結構的模型無法擷取單字之間細微的語義關係，而僅專注於單字層級結構的模型則忽略了原始整個實體之間的更廣泛關係。為了克服這些限制，本文引入了基於 PLM 的多粒度圖形結構注意力 (MGSA)。模型架構的編碼器具有實體層級結構編碼模組、單字層級結構編碼模組和一個從兩個結構中綜合資訊的聚合模組。這種多粒度結構編碼方法允許模型同時擷取實體層級和單字層級結構資訊，提供對知識圖譜結構資訊更全面的理解，從而顯著提升生成文字的品質。我們使用兩個廣泛認可的 KG 到文字生成基準資料集 WebNLG 和 EventNarrative 對 MGSA 模型進行廣泛評估，在這些資料集上，它始終優於僅依賴單一粒度結構資訊的模型，證明了我們方法的有效性。

##### **ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework**
2409.10289v1 by Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem

Empathetic response generation necessitates the integration of emotional and
intentional dynamics to foster meaningful interactions. Existing research
either neglects the intricate interplay between emotion and intent, leading to
suboptimal controllability of empathy, or resorts to large language models
(LLMs), which incur significant computational overhead. In this paper, we
introduce ReflectDiffu, a lightweight and comprehensive framework for
empathetic response generation. This framework incorporates emotion contagion
to augment emotional expressiveness and employs an emotion-reasoning mask to
pinpoint critical emotional elements. Additionally, it integrates intent
mimicry within reinforcement learning for refinement during diffusion. By
harnessing an intent twice reflect the mechanism of
Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional
decision-making into precise intent actions, thereby addressing empathetic
response misalignments stemming from emotional misrecognition. Through
reflection, the framework maps emotional states to intents, markedly enhancing
both response empathy and flexibility. Comprehensive experiments reveal that
ReflectDiffu outperforms existing models regarding relevance, controllability,
and informativeness, achieving state-of-the-art results in both automatic and
human evaluations.

摘要：同理心反应生成需要整合情绪和意图动态，以促进有意义的互动。现有研究要么忽视情绪和意图之间的复杂相互作用，导致同理心的可控性不佳，要么诉诸于大型语言模型 (LLM)，这会产生巨大的计算开销。在本文中，我们介绍了 ReflectDiffu，这是一个轻量级且全面的同理心反应生成框架。此框架结合了情绪传染以增强情绪表达，并采用了情绪推理掩码来精确定位关键情绪元素。此外，它将意图模仿整合到强化学习中，以在扩散过程中进行优化。通过利用意图两次反映探索-采样-校正机制，ReflectDiffu 巧妙地将情绪决策转化为精确的意图动作，从而解决了源于情绪识别错误的同理心反应错位问题。通过反思，该框架将情绪状态映射到意图，显着增强了反应同理心和灵活性。综合实验表明，ReflectDiffu 在相关性、可控性和信息量方面优于现有模型，在自动和人工评估中均取得了最先进的结果。

##### **DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical Diffusion for Audio-driven Talking Head Synthesis**
2409.10281v1 by Fa-Ting Hong, Yunfei Liu, Yu Li, Changyin Zhou, Fei Yu, Dan Xu

Audio-driven talking head synthesis strives to generate lifelike video
portraits from provided audio. The diffusion model, recognized for its superior
quality and robust generalization, has been explored for this task. However,
establishing a robust correspondence between temporal audio cues and
corresponding spatial facial expressions with diffusion models remains a
significant challenge in talking head generation. To bridge this gap, we
present DreamHead, a hierarchical diffusion framework that learns
spatial-temporal correspondences in talking head synthesis without compromising
the model's intrinsic quality and adaptability.~DreamHead learns to predict
dense facial landmarks from audios as intermediate signals to model the spatial
and temporal correspondences.~Specifically, a first hierarchy of
audio-to-landmark diffusion is first designed to predict temporally smooth and
accurate landmark sequences given audio sequence signals. Then, a second
hierarchy of landmark-to-image diffusion is further proposed to produce
spatially consistent facial portrait videos, by modeling spatial
correspondences between the dense facial landmark and appearance. Extensive
experiments show that proposed DreamHead can effectively learn spatial-temporal
consistency with the designed hierarchical diffusion and produce high-fidelity
audio-driven talking head videos for multiple identities.

摘要：音频驱动的说话人头部合成力求根据提供的音频生成逼真的视频
人像。扩散模型以其卓越的品质和稳健的泛化而著称，已被探索用于此任务。然而，
在扩散模型中建立时间音频提示与相应空间面部表情之间的稳健对应关系仍然是
说话人头部生成中的重大挑战。为了弥合这一差距，我们提出了 DreamHead，这是一个分层扩散框架，它学习
说话人头部合成中的时空对应关系，同时不损害模型的内在品质和适应性。~DreamHead 学习从音频中预测密集的面部地标作为中间信号，以模拟空间
和时间对应关系。~具体来说，首先设计了一个音频到地标扩散的第一层级，以预测给定音频序列信号的时间平滑且准确的地标序列。然后，进一步提出了地标到图像扩散的第二层级，以通过建模密集的面部地标和外观之间的空间对应关系来生成空间一致的面部肖像视频。广泛的实验表明，提出的 DreamHead 可以通过设计的分层扩散有效地学习时空
一致性，并为多个身份生成高保真音频驱动的说话人头部视频。

##### **Cognitive Kernel: An Open-source Agent System towards Generalist Autopilots**
2409.10277v1 by Hongming Zhang, Xiaoman Pan, Hongwei Wang, Kaixin Ma, Wenhao Yu, Dong Yu

We introduce Cognitive Kernel, an open-source agent system towards the goal
of generalist autopilots. Unlike copilot systems, which primarily rely on users
to provide essential state information (e.g., task descriptions) and assist
users by answering questions or auto-completing contents, autopilot systems
must complete tasks from start to finish independently, which requires the
system to acquire the state information from the environments actively. To
achieve this, an autopilot system should be capable of understanding user
intents, actively gathering necessary information from various real-world
sources, and making wise decisions. Cognitive Kernel adopts a model-centric
design. In our implementation, the central policy model (a fine-tuned LLM)
initiates interactions with the environment using a combination of atomic
actions, such as opening files, clicking buttons, saving intermediate results
to memory, or calling the LLM itself. This differs from the widely used
environment-centric design, where a task-specific environment with predefined
actions is fixed, and the policy model is limited to selecting the correct
action from a given set of options. Our design facilitates seamless information
flow across various sources and provides greater flexibility. We evaluate our
system in three use cases: real-time information management, private
information management, and long-term memory management. The results
demonstrate that Cognitive Kernel achieves better or comparable performance to
other closed-source systems in these scenarios. Cognitive Kernel is fully
dockerized, ensuring everyone can deploy it privately and securely. We
open-source the system and the backbone model to encourage further research on
LLM-driven autopilot systems.

摘要：<paragraph>我們推出認知核心，一個朝向通用自動駕駛儀目標的開源代理系統。與副駕駛系統不同，副駕駛系統主要依賴使用者提供必要的狀態資訊（例如任務描述），並透過回答問題或自動完成內容來協助使用者，自動駕駛系統必須從頭到尾獨立完成任務，這需要系統主動從環境中獲取狀態資訊。為了達成此目的，自動駕駛系統應具備理解使用者意圖、主動從各種真實世界來源收集必要資訊，以及做出明智決定的能力。認知核心採用以模型為中心のデザイン。在我們的實作中，中央策略模型（微調後的 LLM）使用原子動作的組合來啟動與環境的互動，例如開啟檔案、按鈕點擊、將中間結果儲存到記憶體，或呼叫 LLM 本身。這與廣泛使用的以環境為中心のデザイン不同，在以環境為中心のデザイン中，具備預定義動作的特定任務環境是固定的，而策略模型僅限於從一組選項中選擇正確的動作。我們的設計促進各種來源之間的無縫資訊流動，並提供更大的靈活性。我們在三個使用案例中評估我們的系統：即時資訊管理、私人資訊管理和長期記憶管理。結果證明，在這些場景中，認知核心實現了比其他閉源系統更好或相當的效能。認知核心完全採用 Docker 化，確保每個人都可以私密且安全地部署它。我們開放原始碼系統和主幹模型，以鼓勵進一步研究 LLM 驅動的自動駕駛系統。</paragraph>

##### **Causal Discovery in Recommender Systems: Example and Discussion**
2409.10271v1 by Emanuele Cavenaghi, Fabio Stella, Markus Zanker

Causality is receiving increasing attention by the artificial intelligence
and machine learning communities. This paper gives an example of modelling a
recommender system problem using causal graphs. Specifically, we approached the
causal discovery task to learn a causal graph by combining observational data
from an open-source dataset with prior knowledge. The resulting causal graph
shows that only a few variables effectively influence the analysed feedback
signals. This contrasts with the recent trend in the machine learning community
to include more and more variables in massive models, such as neural networks.

摘要：因果關係越來越受到人工智慧和機器學習社群的關注。本文提供了一個使用因果圖形建模推薦系統問題的範例。具體來說，我們採用因果發現任務，結合來自開放原始碼資料集的觀察資料和先驗知識，來學習因果圖形。結果產生的因果圖形顯示，只有少數變數有效地影響分析後的回饋訊號。這與機器學習社群最近的趨勢形成對比，即在龐大的模型（例如神經網路）中納入越來越多的變數。

##### **FGR-Net:Interpretable fundus imagegradeability classification based on deepreconstruction learning**
2409.10246v1 by Saif Khalid, Hatem A. Rashwan, Saddam Abdulwahab, Mohamed Abdel-Nasser, Facundo Manuel Quiroga, Domenec Puig

The performance of diagnostic Computer-Aided Design (CAD) systems for retinal
diseases depends on the quality of the retinal images being screened. Thus,
many studies have been developed to evaluate and assess the quality of such
retinal images. However, most of them did not investigate the relationship
between the accuracy of the developed models and the quality of the
visualization of interpretability methods for distinguishing between gradable
and non-gradable retinal images. Consequently, this paper presents a novel
framework called FGR-Net to automatically assess and interpret underlying
fundus image quality by merging an autoencoder network with a classifier
network. The FGR-Net model also provides an interpretable quality assessment
through visualizations. In particular, FGR-Net uses a deep autoencoder to
reconstruct the input image in order to extract the visual characteristics of
the input fundus images based on self-supervised learning. The extracted
features by the autoencoder are then fed into a deep classifier network to
distinguish between gradable and ungradable fundus images. FGR-Net is evaluated
with different interpretability methods, which indicates that the autoencoder
is a key factor in forcing the classifier to focus on the relevant structures
of the fundus images, such as the fovea, optic disk, and prominent blood
vessels. Additionally, the interpretability methods can provide visual feedback
for ophthalmologists to understand how our model evaluates the quality of
fundus images. The experimental results showed the superiority of FGR-Net over
the state-of-the-art quality assessment methods, with an accuracy of 89% and an
F1-score of 87%.

摘要：<paragraph>診斷性電腦輔助設計 (CAD) 系統對於視網膜疾病的表現取決於所篩檢視網膜影像的品質。因此，許多研究已開發用於評估並評定此類視網膜影像的品質。然而，大多數研究並未調查已開發模型的準確性與區分可評級和不可評級視網膜影像之可解釋性方法的視覺化品質之間的關係。因此，本文提出一個名為 FGR-Net 的新型架構，用於透過合併自動編碼器網路與分類器網路來自動評估和詮釋基礎眼底影像品質。FGR-Net 模型也透過視覺化提供可解釋的品質評估。特別是，FGR-Net 使用深度自動編碼器重建輸入影像，以根據自我監督學習萃取輸入眼底影像的視覺特徵。自動編碼器萃取的特徵接著會輸入深度分類器網路，以區分可評級和不可評級眼底影像。FGR-Net 透過不同的可解釋性方法進行評估，這表示自動編碼器是強迫分類器專注於眼底影像相關結構（例如黃斑、視神經盤和主要的血管）的關鍵因素。此外，可解釋性方法可以提供視覺回饋，讓眼科醫師了解我們的模型如何評估眼底影像的品質。實驗結果顯示 FGR-Net 優於最先進的品質評估方法，準確度達 89%，F1 分數為 87%。</paragraph>

##### **From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes the Emoji Potential in LLMs**
2409.10245v1 by Navya Jain, Zekun Wu, Cristian Munoz, Airlie Hilliard, Adriano Koshiyama, Emre Kazim, Philip Treleaven

As the demand for human-like interactions with LLMs continues to grow, so
does the interest in manipulating their personality traits, which has emerged
as a key area of research. Methods like prompt-based In-Context Knowledge
Editing (IKE) and gradient-based Model Editor Networks (MEND) have been
explored but show irregularity and variability. IKE depends on the prompt,
leading to variability and sensitivity, while MEND yields inconsistent and
gibberish outputs. To address this, we employed Opinion QA Based
Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank
Adaptation (QLORA), to manipulate the Big Five personality traits: Openness,
Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,
models such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,
despite their absence in the PEFT data. For instance, Llama-2-7B-chat generated
emojis in 99.5% of extraversion-related test instances, while
Mistral-8B-Instruct did so in 92.5% of openness-related test instances.
Explainability analysis indicated that the LLMs used emojis intentionally to
express these traits. This paper provides a number of novel contributions.
First, introducing an Opinion QA dataset for PEFT-driven personality
manipulation; second, developing metric models to benchmark LLM personality
traits; third, demonstrating PEFT's superiority over IKE in personality
manipulation; and finally, analyzing and validating emoji usage through
explainability methods such as mechanistic interpretability and in-context
learning explainability methods.

摘要：<paragraph>隨著對 LLM 類人互動需求持續增長，操縱其人格特質的興趣也隨之增加，這已成為研究的一個關鍵領域。基於提示的語境知識編輯 (IKE) 和基於梯度的模型編輯器網路 (MEND) 等方法已經過探討，但表現出不規則性和可變性。IKE 取決於提示，導致可變性和敏感性，而 MEND 產生不一致且無意義的輸出。為了解決這個問題，我們採用了基於意見問答的參數高效微調 (PEFT)，特別是量化低秩適應 (QLORA)，來操縱五大性格特質：開放性、盡責性、外向性、親和性和神經質。在 PEFT 之後，Mistral-7B-Instruct 和 Llama-2-7B-chat 等模型開始產生表情符號，儘管它們在 PEFT 資料中不存在。例如，Llama-2-7B-chat 在 99.5% 的外向性相關測試例項中產生了表情符號，而 Mistral-8B-Instruct 在 92.5% 的開放性相關測試例項中產生了表情符號。可解釋性分析表明，LLM 故意使用表情符號來表達這些特質。本文提供了一些新穎的貢獻。首先，引入意見問答資料集，用於 PEFT 驅動的人格操縱；其次，開發度量模型來評量 LLM 人格特質；第三，展示 PEFT 在人格操縱方面優於 IKE；最後，通過可解釋性方法（例如機制可解釋性和語境學習可解釋性方法）分析和驗證表情符號的使用。</paragraph>

##### **Hedging Is Not All You Need: A Simple Baseline for Online Learning Under Haphazard Inputs**
2409.10242v1 by Himanshu Buckchash, Momojit Biswas, Rohit Agarwal, Dilip K. Prasad

Handling haphazard streaming data, such as data from edge devices, presents a
challenging problem. Over time, the incoming data becomes inconsistent, with
missing, faulty, or new inputs reappearing. Therefore, it requires models that
are reliable. Recent methods to solve this problem depend on a hedging-based
solution and require specialized elements like auxiliary dropouts, forked
architectures, and intricate network design. We observed that hedging can be
reduced to a special case of weighted residual connection; this motivated us to
approximate it with plain self-attention. In this work, we propose HapNet, a
simple baseline that is scalable, does not require online backpropagation, and
is adaptable to varying input types. All present methods are restricted to
scaling with a fixed window; however, we introduce a more complex problem of
scaling with a variable window where the data becomes positionally
uncorrelated, and cannot be addressed by present methods. We demonstrate that a
variant of the proposed approach can work even for this complex scenario. We
extensively evaluated the proposed approach on five benchmarks and found
competitive performance.

摘要：處理隨機串流資料（例如來自邊緣設備的資料）會出現一個具有挑戰性的問題。隨著時間推移，輸入資料會變得不一致，會出現遺失、有缺陷或新的輸入資料。因此，這需要可靠的模型。解決此問題的近期方法取決於基於對沖的解決方案，並需要專門的元素，例如輔助中斷、分叉架構和複雜的網路設計。我們觀察到，對沖可以簡化為加權殘差連接的特殊情況；這促使我們使用純粹的自我關注來近似它。在這項工作中，我們提出 HapNet，這是一個簡單的基準，具有可擴充性、不需要線上反向傳播，並且可以適應不同的輸入類型。所有現有方法都受限於使用固定視窗進行擴充；然而，我們引入了使用變動視窗進行擴充的更複雜問題，其中資料在位置上不再相關，且無法透過現有方法來處理。我們證明了所提出方法的變體甚至可以適用於這種複雜的情況。我們廣泛評估了所提出的方法在五個基準測試中的表現，發現其效能具有競爭力。

##### **Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models**
2409.10197v1 by Weihao Ye, Qiong Wu, Wenhao Lin, Yiyi Zhou

Recent progress in Multimodal Large Language Models(MLLMs) often use large
image tokens to compensate the visual shortcoming of MLLMs, which not only
exhibits obvious redundancy but also greatly exacerbates the already high
computation. Token pruning is an effective solution for speeding up MLLMs, but
when and how to drop tokens still remains a challenge. In this paper, we
propose a novel and training-free approach for the effective visual token
pruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning
recipe for MLLMs according to a pre-defined budget. Specifically, FitPrune
considers token pruning as a statistical problem of MLLM and its objective is
to find out an optimal pruning scheme that can minimize the divergence of the
attention distributions before and after pruning. In practice, FitPrune can be
quickly accomplished based on the attention statistics from a small batch of
inference data, avoiding the expensive trials of MLLMs. According to the
pruning recipe, an MLLM can directly remove the redundant visual tokens of
different examples during inference. To validate FitPrune, we apply it to a set
of recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct
extensive experiments on a set of benchmarks. The experimental results show
that our FitPrune can not only reduce the computational complexity to a large
extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT
with only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in
about 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.

摘要：多模态大语言模型 (MLLM) 的最新进展经常使用大型图像标记来弥补 MLLM 的视觉缺陷，这不仅表现出明显的冗余，而且还极大地加剧了本已很高的计算量。标记剪枝是加速 MLLM 的有效解决方案，但何时以及如何丢弃标记仍然是一个挑战。在本文中，我们提出了一种新颖且无需训练的方法，用于有效地对 MLLM 的视觉标记进行剪枝，称为 FitPrune，它可以根据预定义的预算快速生成 MLLM 的完整剪枝方案。具体来说，FitPrune 将标记剪枝视为 MLLM 的统计问题，其目标是找出一种最优剪枝方案，该方案可以最小化剪枝前后注意力分布的差异。在实践中，FitPrune 可以根据一小批推理数据的注意力统计数据快速完成，避免了对 MLLM 进行昂贵的试验。根据剪枝方案，MLLM 可以在推理过程中直接移除不同示例的冗余视觉标记。为了验证 FitPrune，我们将其应用于一组最新的 MLLM，包括 LLaVA-1.5、LLaVA-HR 和 LLaVA-NEXT，并在基准测试集上进行了广泛的实验。实验结果表明，我们的 FitPrune 不仅可以将计算复杂度降低很大程度，同时保持高性能，例如，LLaVA-NEXT 的 FLOP 减少了 -54.9%，而准确度仅下降了 0.5%。值得注意的是，剪枝方案可以在大约 5 分钟内获得。我们的代码可在 https://github.com/ywh187/FitPrune 获得。

##### **NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions**
2409.10196v1 by Zhixi Cai, Cristian Rojas Cardenas, Kevin Leo, Chenyuan Zhang, Kal Backman, Hanbing Li, Boying Li, Mahsa Ghorbanali, Stavya Datta, Lizhen Qu, Julian Gutierrez Santiago, Alexey Ignatiev, Yuan-Fang Li, Mor Vered, Peter J Stuckey, Maria Garcia de la Banda, Hamid Rezatofighi

This paper addresses the problem of autonomous UAV search missions, where a
UAV must locate specific Entities of Interest (EOIs) within a time limit, based
on brief descriptions in large, hazard-prone environments with keep-out zones.
The UAV must perceive, reason, and make decisions with limited and uncertain
information. We propose NEUSIS, a compositional neuro-symbolic system designed
for interpretable UAV search and navigation in realistic scenarios. NEUSIS
integrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to
process raw sensory inputs, maintains a probabilistic world model for
environment representation, and uses a hierarchical planning component (SNaC)
for efficient path planning. Experimental results from simulated urban search
missions using AirSim and Unreal Engine show that NEUSIS outperforms a
state-of-the-art (SOTA) vision-language model and a SOTA search planning model
in success rate, search efficiency, and 3D localization. These results
demonstrate the effectiveness of our compositional neuro-symbolic approach in
handling complex, real-world scenarios, making it a promising solution for
autonomous UAV systems in search missions.

摘要：本文探讨了自主无人机搜索任务的问题，其中无人机必须在时间限制内基于大型危险环境中禁止进入区域的简要描述来定位特定的目标实体 (EOI)。无人机必须感知、推理并在信息有限且不确定的情况下做出决策。我们提出了 NEUSIS，这是一种组合神经符号系统，旨在用于在现实场景中进行可解释的无人机搜索和导航。NEUSIS 集成了神经符号视觉感知、推理和接地 (GRiD) 来处理原始感官输入，维护环境表示的概率世界模型，并使用分层规划组件 (SNaC) 进行高效路径规划。使用 AirSim 和虚幻引擎进行的模拟城市搜索任务的实验结果表明，NEUSIS 在成功率、搜索效率和 3D 定位方面优于最先进 (SOTA) 视觉语言模型和 SOTA 搜索规划模型。这些结果证明了我们的组合神经符号方法在处理复杂现实世界场景中的有效性，使其成为搜索任务中自主无人机系统的有前途的解决方案。

##### **LLMs for clinical risk prediction**
2409.10191v1 by Mohamed Rezk, Patricia Cabanillas Silva, Fried-Michael Dahlweid

This study compares the efficacy of GPT-4 and clinalytix Medical AI in
predicting the clinical risk of delirium development. Findings indicate that
GPT-4 exhibited significant deficiencies in identifying positive cases and
struggled to provide reliable probability estimates for delirium risk, while
clinalytix Medical AI demonstrated superior accuracy. A thorough analysis of
the large language model's (LLM) outputs elucidated potential causes for these
discrepancies, consistent with limitations reported in extant literature. These
results underscore the challenges LLMs face in accurately diagnosing conditions
and interpreting complex clinical data. While LLMs hold substantial potential
in healthcare, they are currently unsuitable for independent clinical
decision-making. Instead, they should be employed in assistive roles,
complementing clinical expertise. Continued human oversight remains essential
to ensure optimal outcomes for both patients and healthcare providers.

摘要：本研究比較 GPT-4 和 clinalytix Medical AI 在預測譫妄發展的臨床風險方面的效能。研究結果表明，GPT-4 在識別陽性病例方面表現出顯著缺陷，並且難以提供譫妄風險的可靠機率估計，而 clinalytix Medical AI 則表現出優越的準確性。對大型語言模型 (LLM) 輸出的徹底分析闡明了這些差異的潛在原因，與現有文獻中報導的限制一致。這些結果強調了 LLM 在準確診斷疾病和解釋複雜臨床數據方面面臨的挑戰。儘管 LLM 在醫療保健領域具有巨大的潛力，但它們目前不適合獨立進行臨床決策。相反，它們應該用於輔助角色，補充臨床專業知識。持續的人類監督對於確保患者和醫療保健提供者的最佳結果仍然至關重要。

##### **Augmenting Automatic Speech Recognition Models with Disfluency Detection**
2409.10177v2 by Robin Amann, Zhaolin Li, Barbara Bruno, Jan Niehues

Speech disfluency commonly occurs in conversational and spontaneous speech.
However, standard Automatic Speech Recognition (ASR) models struggle to
accurately recognize these disfluencies because they are typically trained on
fluent transcripts. Current research mainly focuses on detecting disfluencies
within transcripts, overlooking their exact location and duration in the
speech. Additionally, previous work often requires model fine-tuning and
addresses limited types of disfluencies.
  In this work, we present an inference-only approach to augment any ASR model
with the ability to detect open-set disfluencies. We first demonstrate that ASR
models have difficulty transcribing speech disfluencies. Next, this work
proposes a modified Connectionist Temporal Classification(CTC)-based forced
alignment algorithm from \cite{kurzinger2020ctc} to predict word-level
timestamps while effectively capturing disfluent speech. Additionally, we
develop a model to classify alignment gaps between timestamps as either
containing disfluent speech or silence. This model achieves an accuracy of
81.62% and an F1-score of 80.07%. We test the augmentation pipeline of
alignment gap detection and classification on a disfluent dataset. Our results
show that we captured 74.13% of the words that were initially missed by the
transcription, demonstrating the potential of this pipeline for downstream
tasks.

摘要：言語の流暢性の乱れは、会話や自発的な言語でよく見られます。
しかし、標準的な自動音声認識 (ASR) モデルは、通常は流暢な文字起こしでトレーニングされるため、これらの流暢性の乱れを正確に認識することが困難です。現在の研究は主に文字起こし内の流暢性の乱れの検出に焦点を当てており、音声内の正確な位置と持続時間を無視しています。さらに、以前の作業では、多くの場合モデルの微調整が必要であり、限定的なタイプの流暢性の乱れに対処します。
この作業では、任意の ASR モデルを拡張してオープンセットの流暢性の乱れを検出する機能を備えた推論専用アプローチを紹介します。最初に、ASR モデルが音声の流暢性の乱れを文字起こしするのに苦労していることを実証します。次に、この作業では、\cite{kurzinger2020ctc} から修正された接続主義時系列分類 (CTC) ベースの強制アライメントアルゴリズムを提案して、流暢でない音声を効果的にキャプチャしながら単語レベルのタイムスタンプを予測します。さらに、タイムスタンプ間の配置ギャップを流暢でない音声または無音のいずれかとして分類するモデルを開発します。このモデルは 81.62% の精度と 80.07% の F1 スコアを達成します。流暢でないデータセットで配置ギャップの検出と分類の拡張パイプラインをテストします。その結果、当初文字起こしで失われた単語の 74.13% をキャプチャしたことが示され、このパイプラインの下流タスクの可能性が実証されました。

##### **jina-embeddings-v3: Multilingual Embeddings With Task LoRA**
2409.10173v2 by Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Andreas Koukounas, Nan Wang, Han Xiao

We introduce jina-embeddings-v3, a novel text embedding model with 570
million parameters, achieves state-of-the-art performance on multilingual data
and long-context retrieval tasks, supporting context lengths of up to 8192
tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)
adapters to generate high-quality embeddings for query-document retrieval,
clustering, classification, and text matching. Additionally, Matryoshka
Representation Learning is integrated into the training process, allowing
flexible truncation of embedding dimensions without compromising performance.
Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the
latest proprietary embeddings from OpenAI and Cohere on English tasks, while
achieving superior performance compared to multilingual-e5-large-instruct
across all multilingual tasks.

摘要：我們推出 jina-embeddings-v3，一種具有 5.7 億個參數的新型文字嵌入模型，在多語言數據和長內容擷取任務上獲得了最先進的效能，支援長達 8192 個 token 的內容長度。此模型包含一組特定於任務的低秩適應 (LoRA) 適配器，以產生用於查詢文件擷取、分群、分類和文字比對的高品質嵌入。此外，Matryoshka 表徵學習已整合到訓練過程中，允許靈活地截斷嵌入維度，而不會影響效能。在 MTEB 基準上的評估顯示，jina-embeddings-v3 在英文任務上優於 OpenAI 和 Cohere 的最新專有嵌入，同時在所有多語言任務上都優於 multilingual-e5-large-instruct。

##### **Quantile Regression for Distributional Reward Models in RLHF**
2409.10164v1 by Nicolai Dorka

Reinforcement learning from human feedback (RLHF) has become a key method for
aligning large language models (LLMs) with human preferences through the use of
reward models. However, traditional reward models typically generate point
estimates, which oversimplify the diversity and complexity of human values and
preferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel
approach to reward modeling that learns a distribution over rewards instead of
a single scalar value. Our method uses quantile regression to estimate a full,
potentially multimodal distribution over preferences, providing a more powerful
and nuanced representation of preferences. This distributional approach can
better capture the diversity of human values, addresses label noise, and
accommodates conflicting preferences by modeling them as distinct modes in the
distribution. Our experimental results show that QRM outperforms comparable
traditional point-estimate models on RewardBench. Furthermore, we demonstrate
that the additional information provided by the distributional estimates can be
utilized in downstream applications, such as risk-aware reinforcement learning,
resulting in LLM policies that generate fewer extremely negative responses. Our
code and model are released at https://github.com/Nicolinho/QRM.

摘要：人類回饋強化學習 (RLHF) 已成為透過使用獎勵模型，將大型語言模型 (LLM) 與人類偏好對齊的一種關鍵方法。然而，傳統的獎勵模型通常會產生點估計，這過度簡化了人類價值觀和偏好的多樣性和複雜性。在本文中，我們介紹了分位數獎勵模型 (QRM)，這是一種新的獎勵建模方法，它會學習獎勵分佈，而不是單一標量值。我們的模型使用分位數回歸來估計偏好的完整多模態分佈，提供更強大且細緻的偏好表示。這種分佈式方法可以更好地捕捉人類價值觀的多樣性，解決標籤雜訊，並透過將其建模為分佈中的不同模式來容納相互衝突的偏好。我們的實驗結果表明，QRM 在 RewardBench 上優於可比較的傳統點估計模型。此外，我們證明了分佈式估計所提供的額外資訊可用於下游應用程式，例如風險感知強化學習，這會產生更少極端負面回應的 LLM 政策。我們的程式碼和模型已在 https://github.com/Nicolinho/QRM 發布。

##### **AutoPET Challenge III: Testing the Robustness of Generalized Dice Focal Loss trained 3D Residual UNet for FDG and PSMA Lesion Segmentation from Whole-Body PET/CT Images**
2409.10151v1 by Shadab Ahamed

Automated segmentation of cancerous lesions in PET/CT scans is a crucial
first step in quantitative image analysis. However, training deep learning
models for segmentation with high accuracy is particularly challenging due to
the variations in lesion size, shape, and radiotracer uptake. These lesions can
appear in different parts of the body, often near healthy organs that also
exhibit considerable uptake, making the task even more complex. As a result,
creating an effective segmentation model for routine PET/CT image analysis is
challenging. In this study, we utilized a 3D Residual UNet model and employed
the Generalized Dice Focal Loss function to train the model on the AutoPET
Challenge 2024 dataset. We conducted a 5-fold cross-validation and used an
average ensembling technique using the models from the five folds. In the
preliminary test phase for Task-1, the average ensemble achieved a mean Dice
Similarity Coefficient (DSC) of 0.6687, mean false negative volume (FNV) of
10.9522 ml and mean false positive volume (FPV) 2.9684 ml. More details about
the algorithm can be found on our GitHub repository:
https://github.com/ahxmeds/autosegnet2024.git. The training code has been
shared via the repository: https://github.com/ahxmeds/autopet2024.git.

摘要：自動化分割 PET/CT 掃描中的癌性病灶是定量影像分析中的關鍵第一步。然而，訓練深度學習模型進行高精確度分割特別具有挑戰性，因為病灶大小、形狀和放射性藥物攝取變化很大。這些病灶可能出現在身體的不同部位，通常靠近也表現出顯著攝取的健康器官，這使得任務更加複雜。因此，為例行 PET/CT 影像分析建立一個有效的分割模型具有挑戰性。在這項研究中，我們利用了 3D Residual UNet 模型，並採用廣義 Dice 焦點損失函數在 AutoPET Challenge 2024 資料集上訓練模型。我們進行了 5 倍交叉驗證，並使用來自五個區塊的模型進行平均集成技術。在任務 1 的初步測試階段，平均集成實現了平均 Dice 相似性係數 (DSC) 為 0.6687，平均假陰性體積 (FNV) 為 10.9522 毫升和平均假陽性體積 (FPV) 為 2.9684 毫升。有關演算法的更多詳細資訊可以在我們的 GitHub 儲存庫中找到：https://github.com/ahxmeds/autosegnet2024.git。訓練程式碼已透過儲存庫分享：https://github.com/ahxmeds/autopet2024.git。

##### **LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge**
2409.10146v1 by Hamed Babaei Giglou, Jennifer D'Souza, Sören Auer

This paper outlines the LLMs4OL 2024, the first edition of the Large Language
Models for Ontology Learning Challenge. LLMs4OL is a community development
initiative collocated with the 23rd International Semantic Web Conference
(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology
Learning (OL), a vital process for enhancing the web with structured knowledge
to improve interoperability. By leveraging LLMs, the challenge aims to advance
understanding and innovation in OL, aligning with the goals of the Semantic Web
to create a more intelligent and user-friendly web. In this paper, we give an
overview of the 2024 edition of the LLMs4OL challenge and summarize the
contributions.

摘要：本文概述了 LLMs4OL 2024，大型語言模型用於本體學習挑戰的第一版。LLMs4OL 是一個社區發展倡議，與第 23 屆國際語義網大會 (ISWC) 共同舉辦，旨在探索大型語言模型 (LLM) 在本體學習 (OL) 中的潛力，這是一個增強網路結構化知識以提高互操作性的重要過程。通過利用大型語言模型，該挑戰旨在推進 OL 的理解和創新，與語義網的目標一致，以創造一個更智慧、更友善的網路。在本文中，我們概述了 2024 年版 LLMs4OL 挑戰，並總結了貢獻。

##### **Advancing Towards a Marine Digital Twin Platform: Modeling the Mar Menor Coastal Lagoon Ecosystem in the South Western Mediterranean**
2409.10134v1 by Yu Ye, Aurora González-Vidal, Alejandro Cisterna-García, Angel Pérez-Ruzafa, Miguel A. Zamora Izquierdo, Antonio F. Skarmeta

Coastal marine ecosystems face mounting pressures from anthropogenic
activities and climate change, necessitating advanced monitoring and modeling
approaches for effective management. This paper pioneers the development of a
Marine Digital Twin Platform aimed at modeling the Mar Menor Coastal Lagoon
Ecosystem in the Region of Murcia. The platform leverages Artificial
Intelligence to emulate complex hydrological and ecological models,
facilitating the simulation of what-if scenarios to predict ecosystem responses
to various stressors. We integrate diverse datasets from public sources to
construct a comprehensive digital representation of the lagoon's dynamics. The
platform's modular design enables real-time stakeholder engagement and informed
decision-making in marine management. Our work contributes to the ongoing
discourse on advancing marine science through innovative digital twin
technologies.

摘要：沿海海洋生態系統面臨來自人為活動和氣候變遷日益增加的壓力，這需要先進的監測和建模方法來進行有效的管理。本文開創了海洋數位孿生平台的開發，旨在模擬穆爾西亞地區的馬爾梅諾爾沿海潟湖生態系統。該平台利用人工智慧來模擬複雜的水文和生態模型，促進模擬假設情境以預測生態系統對各種壓力源的反應。我們整合來自公共來源的多元資料集，以建構潟湖動態的全面數位表示。該平台的模組化設計能讓利害關係人即時參與，並在海洋管理中做出明智的決策。我們的研究有助於透過創新的數位孿生技術推進海洋科學的持續論述。

##### **StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models**
2409.10132v1 by Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Junfeng Fang, Xueqi Cheng

As the modern tool of choice for question answering, large language models
(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve
such ideal question-answering systems, locating and then editing outdated
knowledge in the natural language outputs is a general target of popular
knowledge editing methods. However, this target is challenging, as both
identifying which tokens to edit in the reasoning steps and ensuring the
coherence of the revised reasoning chain are difficult tasks. We argue that
these challenges stem from the unstructured nature of natural language outputs.
To address the above challenges, we propose $\textbf{Stru}$ctural
$\textbf{Edit}$ing ($\textbf{StruEdit}$), an improved baseline for knowledge
editing. We first prompt LLMs to produce structured outputs consisting of
reasoning triplets. Then, StruEdit removes any potentially outdated knowledge
and efficiently refills the structured outputs with up-to-date information in a
single step. Experimental results show that StruEdit consistently delivers the
highest accuracy with lowest latency compared with other knowledge editing
methods.

摘要：作為現代問答的首選工具，大型語言模型 (LLM) 預計會提供具備最新知識的答案。為了達成理想的問答系統，在自然語言輸出中找出並編輯過時的知識，是常見的知識編輯方法的總目標。然而，這個目標具有挑戰性，因為在推理步驟中找出要編輯的代碼，並確保已修改推理鏈的連貫性，都是困難的任務。我們認為，這些挑戰源自自然語言輸出的非結構化性質。為了應對上述挑戰，我們提出 $\textbf{結構}$$\textbf{編輯}$ ($\textbf{StruEdit}$)，一個改進的知識編輯基準。我們首先提示 LLM 產生由推理三元組組成的結構化輸出。然後，StruEdit 會移除任何潛在過時的知識，並在單一步驟中有效地用最新資訊填入結構化輸出。實驗結果顯示，與其他知識編輯方法相比，StruEdit 始終提供最高準確度和最低延遲。

##### **Industry 6.0: New Generation of Industry driven by Generative AI and Swarm of Heterogeneous Robots**
2409.10106v1 by Artem Lykov, Miguel Altamirano Cabrera, Mikhail Konenkov, Valerii Serpiva, Koffivi Fid`ele Gbagbe, Ali Alabbas, Aleksey Fedoseev, Luis Moreno, Muhammad Haris Khan, Ziang Guo, Dzmitry Tsetserukou

This paper presents the concept of Industry 6.0, introducing the world's
first fully automated production system that autonomously handles the entire
product design and manufacturing process based on user-provided natural
language descriptions. By leveraging generative AI, the system automates
critical aspects of production, including product blueprint design, component
manufacturing, logistics, and assembly. A heterogeneous swarm of robots, each
equipped with individual AI through integration with Large Language Models
(LLMs), orchestrates the production process. The robotic system includes
manipulator arms, delivery drones, and 3D printers capable of generating
assembly blueprints. The system was evaluated using commercial and open-source
LLMs, functioning through APIs and local deployment. A user study demonstrated
that the system reduces the average production time to 119.10 minutes,
significantly outperforming a team of expert human developers, who averaged
528.64 minutes (an improvement factor of 4.4). Furthermore, in the product
blueprinting stage, the system surpassed human CAD operators by an
unprecedented factor of 47, completing the task in 0.5 minutes compared to 23.5
minutes. This breakthrough represents a major leap towards fully autonomous
manufacturing.

摘要：本文提出了工業 6.0 的概念，介紹了世界上第一個完全自動化的生產系統，該系統根據使用者提供的自然語言描述，自主處理整個產品設計和製造流程。透過利用生成式 AI，該系統自動化了生產過程中的關鍵面向，包括產品藍圖設計、組件製造、物流和組裝。一群異質的機器人，每個機器人都透過與大型語言模型 (LLM) 整合而配備了個別 AI，協調生產流程。機器人系統包括機械手臂、運送無人機和 3D 印表機，能夠產生組裝藍圖。該系統使用商業和開源 LLM 進行評估，透過 API 和本地部署運作。使用者研究顯示，該系統將平均生產時間縮短至 119.10 分鐘，顯著優於一群專家人類開發人員，他們的平均時間為 528.64 分鐘（改善因子為 4.4）。此外，在產品藍圖階段，該系統以空前的 47 倍超越了人類 CAD 操作員，在 0.5 分鐘內完成任務，而人類則需要 23.5 分鐘。這項突破代表了朝向完全自動化製造邁出的一大步。

##### **Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT**
2409.10103v1 by Ryota Komatsu, Takahiro Shinozaki

Self-supervised speech representation learning has become essential for
extracting meaningful features from untranscribed audio. Recent advances
highlight the potential of deriving discrete symbols from the features
correlated with linguistic units, which enables text-less training across
diverse tasks. In particular, sentence-level Self-Distillation of the
pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech
frame representations extracted from an intermediate Transformer layer. In
SD-HuBERT, sentence-level representation is accumulated from speech frame
features through self-attention layers using a special CLS token. However, we
observe that the information aggregated in the CLS token correlates more with
speaker identity than with linguistic content. To address this, we propose a
speech-only self-supervised fine-tuning approach that separates syllabic units
from speaker information. Our method introduces speaker perturbation as data
augmentation and adopts a frame-level training objective to prevent the CLS
token from aggregating paralinguistic information. Experimental results show
that our approach surpasses the current state-of-the-art method in most
syllable segmentation and syllabic unit quality metrics on Librispeech,
underscoring its effectiveness in promoting syllabic organization within
speech-only models.

摘要：自我监督语音表征学习已成为从未转录音频中提取有意义特征的必要方法。最近的进展凸显了从与语言单位相关的特征中推导出离散符号的潜力，这使得跨不同任务进行无文本训练成为可能。特别是，预训练 HuBERT（SD-HuBERT）的句子级自蒸馏在从中间 Transformer 层提取的潜在语音帧表征中诱导出音节结构。在 SD-HuBERT 中，句子级表征通过使用特殊 CLS 标记通过自注意力层从语音帧特征中累积。然而，我们观察到 CLS 标记中聚合的信息与说话者身份的相关性高于语言内容。为了解决这个问题，我们提出了一种仅语音自我监督微调方法，该方法将音节单位与说话者信息分开。我们的方法将说话者扰动作为数据增强，并采用帧级训练目标来防止 CLS 标记聚合副语言信息。实验结果表明，我们的方法在 Librispeech 上超越了当前最先进的方法，在大多数音节分割和音节单位质量指标上都取得了超越，这突出了其在仅语音模型中促进音节组织的有效性。

##### **Trustworthiness in Retrieval-Augmented Generation Systems: A Survey**
2409.10102v1 by Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, Philip S. Yu

Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal
paradigm in the development of Large Language Models (LLMs). While much of the
current research in this field focuses on performance optimization,
particularly in terms of accuracy and efficiency, the trustworthiness of RAG
systems remains an area still under exploration. From a positive perspective,
RAG systems are promising to enhance LLMs by providing them with useful and
up-to-date knowledge from vast external databases, thereby mitigating the
long-standing problem of hallucination. While from a negative perspective, RAG
systems are at the risk of generating undesirable contents if the retrieved
information is either inappropriate or poorly utilized. To address these
concerns, we propose a unified framework that assesses the trustworthiness of
RAG systems across six key dimensions: factuality, robustness, fairness,
transparency, accountability, and privacy. Within this framework, we thoroughly
review the existing literature on each dimension. Additionally, we create the
evaluation benchmark regarding the six dimensions and conduct comprehensive
evaluations for a variety of proprietary and open-source models. Finally, we
identify the potential challenges for future research based on our
investigation results. Through this work, we aim to lay a structured foundation
for future investigations and provide practical insights for enhancing the
trustworthiness of RAG systems in real-world applications.

摘要：檢索增強式生成（RAG）已迅速發展成為大型語言模型（LLM）發展中的關鍵範例。雖然此領域的許多現行研究都著重於效能最佳化，特別是在準確度和效率方面，但 RAG 系統的信賴度仍是仍在探索中的領域。從正面的角度來看，RAG 系統有望透過提供來自廣大外部資料庫的有用且最新的知識來增強 LLM，進而減輕長期存在的幻覺問題。然而，從負面的角度來看，如果檢索到的資訊不適當或使用不當，RAG 系統就有產生不良內容的風險。為了解決這些問題，我們提出一個統一的架構，用來評估 RAG 系統在六個關鍵面向上的信賴度：事實性、穩健性、公平性、透明度、問責制和隱私權。在此架構中，我們徹底檢視了各面向的現有文獻。此外，我們建立了關於六個面向的評估基準，並對各種專有和開放原始碼模型進行了全面的評估。最後，我們根據我們的調查結果，找出未來研究的潛在挑戰。透過這項工作，我們旨在為未來的調查奠定結構化的基礎，並提供實用的見解，以提升 RAG 系統在真實世界應用中的信賴度。

##### **LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain**
2409.10077v1 by Le Xiao, Yunfei Xu, Jing Zhao

Domain-specific Named Entity Recognition (NER), whose goal is to recognize
domain-specific entities and their categories, provides an important support
for constructing domain knowledge graphs. Currently, deep learning-based
methods are widely used and effective in NER tasks, but due to the reliance on
large-scale labeled data. As a result, the scarcity of labeled data in a
specific domain will limit its application.Therefore, many researches started
to introduce few-shot methods and achieved some results. However, the entity
structures in specific domains are often complex, and the current few-shot
methods are difficult to adapt to NER tasks with complex features.Taking the
Chinese coal chemical industry domain as an example,there exists a complex
structure of multiple entities sharing a single entity, as well as multiple
relationships for the same pair of entities, which affects the NER task under
the sample less condition.In this paper, we propose a Large Language Models
(LLMs)-based entity recognition framework LLM-DER for the domain-specific
entity recognition problem in Chinese, which enriches the entity information by
generating a list of relationships containing entity types through LLMs, and
designing a plausibility and consistency evaluation method to remove
misrecognized entities, which can effectively solve the complex structural
entity recognition problem in a specific domain.The experimental results of
this paper on the Resume dataset and the self-constructed coal chemical dataset
Coal show that LLM-DER performs outstandingly in domain-specific entity
recognition, not only outperforming the existing GPT-3.5-turbo baseline, but
also exceeding the fully-supervised baseline, verifying its effectiveness in
entity recognition.

摘要：<paragraph>領域特定命名實體辨識（NER），其目標是辨識領域特定實體及其類別，為建構領域知識圖譜提供重要的支援。目前，基於深度學習的方法廣泛用於 NER 任務且十分有效，但由於依賴於大規模標記資料。因此，特定領域中標記資料的稀少會限制其應用。因此，許多研究開始引入少量樣本方法並獲得一些成果。然而，特定領域中的實體結構通常很複雜，而目前的少量樣本方法難以適應具有複雜特徵的 NER 任務。以中國煤化工產業領域為例，存在多個實體共用單一實體的複雜結構，以及同一對實體有多重關係，這會影響樣本較少條件下的 NER 任務。在本文中，我們提出了一個基於大型語言模型（LLM）的實體辨識架構 LLM-DER，用於中文領域特定實體辨識問題，通過 LLM 生成包含實體類型的關係清單，並設計一個合理性和一致性評估方法來移除辨識錯誤的實體，從而可以有效解決特定領域中複雜結構實體辨識問題。本文在 Resume 資料集和自建煤化工資料集 Coal 上的實驗結果表明，LLM-DER 在領域特定實體辨識中表現出色，不僅優於現有的 GPT-3.5-turbo 基準，還超過了完全監督的基線，驗證了其在實體辨識中的有效性。</paragraph>

##### **Increasing faithfulness in human-human dialog summarization with Spoken Language Understanding tasks**
2409.10070v1 by Eunice Akani, Benoit Favre, Frederic Bechet, Romain Gemignani

Dialogue summarization aims to provide a concise and coherent summary of
conversations between multiple speakers. While recent advancements in language
models have enhanced this process, summarizing dialogues accurately and
faithfully remains challenging due to the need to understand speaker
interactions and capture relevant information. Indeed, abstractive models used
for dialog summarization may generate summaries that contain inconsistencies.
We suggest using the semantic information proposed for performing Spoken
Language Understanding (SLU) in human-machine dialogue systems for
goal-oriented human-human dialogues to obtain a more semantically faithful
summary regarding the task. This study introduces three key contributions:
First, we propose an exploration of how incorporating task-related information
can enhance the summarization process, leading to more semantically accurate
summaries. Then, we introduce a new evaluation criterion based on task
semantics. Finally, we propose a new dataset version with increased annotated
data standardized for research on task-oriented dialogue summarization. The
study evaluates these methods using the DECODA corpus, a collection of French
spoken dialogues from a call center. Results show that integrating models with
task-related information improves summary accuracy, even with varying word
error rates.

摘要：對話摘要旨在提供多位說話者之間對話的簡潔且連貫的摘要。雖然語言模型的最新進展增強了這個過程，但由於需要理解說話者的互動並擷取相關資訊，因此準確且忠實地摘要對話仍然具有挑戰性。事實上，用於對話摘要的抽象模型可能會產生包含不一致性的摘要。我們建議在目標導向的人人對話中使用為在人機對話系統中執行口語理解 (SLU) 而提出的語義資訊，以獲得關於任務的語義更忠實的摘要。本研究介紹了三個關鍵貢獻：首先，我們提出探討如何整合任務相關資訊可以增強摘要處理程序，進而產生語義更準確的摘要。接著，我們引入一個基於任務語義的新評量標準。最後，我們提出一個新的資料集版本，其中增加了標準化的註解資料，以供研究任務導向的對話摘要。本研究使用 DECODA 語料庫評估這些方法，該語料庫是一個來自呼叫中心的法語口語對話集合。結果顯示，即使字元錯誤率不同，將模型與任務相關資訊整合在一起也會改善摘要的準確性。

##### **MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM**
2409.10064v1 by Sijie Ji, Xinzhe Zheng, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava

Mental health disorders are among the most prevalent diseases worldwide,
affecting nearly one in four people. Despite their widespread impact, the
intervention rate remains below 25%, largely due to the significant cooperation
required from patients for both diagnosis and intervention. The core issue
behind this low treatment rate is stigma, which discourages over half of those
affected from seeking help. This paper presents MindGuard, an accessible,
stigma-free, and professional mobile mental healthcare system designed to
provide mental health first aid. The heart of MindGuard is an innovative edge
LLM, equipped with professional mental health knowledge, that seamlessly
integrates objective mobile sensor data with subjective Ecological Momentary
Assessment records to deliver personalized screening and intervention
conversations. We conduct a broad evaluation of MindGuard using open datasets
spanning four years and real-world deployment across various mobile devices
involving 20 subjects for two weeks. Remarkably, MindGuard achieves results
comparable to GPT-4 and outperforms its counterpart with more than 10 times the
model size. We believe that MindGuard paves the way for mobile LLM
applications, potentially revolutionizing mental healthcare practices by
substituting self-reporting and intervention conversations with passive,
integrated monitoring within daily life, thus ensuring accessible and
stigma-free mental health support.

摘要：心理健康疾病是全球最普遍的疾病之一，
影響了近四分之一的人。儘管其影響廣泛，
但介入率仍低於 25%，很大程度上是因為
患者在診斷和介入時需要大量配合。背後導致
治療率低下的核心問題是污名化，這讓超過一半的
受影響者不願意尋求幫助。本文提出了 MindGuard，一個
易於取得、無污名化且專業的手機心理保健系統，旨在
提供心理急救。MindGuard 的核心是一個創新的邊緣
大型語言模型 (LLM)，具備專業的心理健康知識，它能無縫
整合客觀的手機感測器資料與主觀的生態瞬時評估記錄，提供
個人化的篩檢和介入對話。我們使用橫跨四年的開放資料集
對 MindGuard 進行廣泛的評估，並在各種行動裝置上進行為期
兩週、涉及 20 位受試者的實際部署。值得注意的是，MindGuard
達到的結果與 GPT-4 相當，並且優於模型規模大於其 10 倍以上的
對應模型。我們相信 MindGuard 為行動 LLM 應用鋪平了道路，
有可能透過在日常生活中進行被動、整合的監控來取代自我報告
和介入對話，從而徹底改變心理保健實務，進而確保可取得且
無污名化的精神健康支持。

##### **Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective**
2409.10053v1 by Van-Cuong Pham, Thien Huu Nguyen

Activation Editing, which involves directly editting the internal
representations of large language models (LLMs) to alter their behaviors and
achieve desired properties, has emerged as a promising area of research.
Existing works primarily treat LLMs' activations as points in space and modify
them by adding steering vectors. However, this approach is limited in its
ability to achieve greater performance improvement while maintaining the
necessary consistency of activation magnitudes. To overcome these issues, we
propose a novel editing method that views activations in terms of their
directions and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),
mimics the rotation transformation, thus preserving activation norms and
resulting in an improved performance on various safety benchmarks.

摘要：激活編輯涉及直接編輯大型語言模型 (LLM) 的內部表示，以改變其行為並實現所需的屬性，已成為一個有前途的研究領域。現有作品主要將 LLM 的激活視為空間中的點，並透過添加引導向量來修改它們。然而，此方法在維持激活幅度的必要一致性的同時，提升效能的幅度有限。為了克服這些問題，我們提出了一種新穎的編輯方法，將激活視為其方向和幅度。我們的這種方法稱為 Householder 偽旋轉 (HPR)，模擬旋轉轉換，從而保留激活範數並提升各種安全基準的效能。

##### **Benchmarking Large Language Model Uncertainty for Prompt Optimization**
2409.10044v1 by Pei-Fu Guo, Yun-Da Tsai, Shou-De Lin

Prompt optimization algorithms for Large Language Models (LLMs) excel in
multi-step reasoning but still lack effective uncertainty estimation. This
paper introduces a benchmark dataset to evaluate uncertainty metrics, focusing
on Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis
of models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that
current metrics align more with Answer Uncertainty, which reflects output
confidence and diversity, rather than Correctness Uncertainty, highlighting the
need for improved metrics that are optimization-objective-aware to better guide
prompt optimization. Our code and dataset are available at
https://github.com/0Frett/PO-Uncertainty-Benchmarking.

摘要：提示優化演算法對於大型語言模型 (LLM) 在多步驟推理中表現優異，但仍缺乏有效的未確定性估計。本文介紹一個基準資料集，用於評估未確定性指標，重點在於答案、正確性、隨機性和認識論的不確定性。透過分析 GPT-3.5-Turbo 和 Meta-Llama-3.1-8B-Instruct 等模型，我們顯示目前的指標更符合答案的不確定性，這反映了輸出的信心和多樣性，而不是正確性的不確定性，強調需要改進以最佳化目標為導向的指標，以更好地引導提示最佳化。我們的程式碼和資料集可在 https://github.com/0Frett/PO-Uncertainty-Benchmarking 取得。

##### **On the Diagram of Thought**
2409.10038v1 by Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao

We introduce Diagram of Thought (DoT), a framework that models iterative
reasoning in large language models (LLMs) as the construction of a directed
acyclic graph (DAG) within a single model. Unlike traditional approaches that
represent reasoning as linear chains or trees, DoT organizes propositions,
critiques, refinements, and verifications into a cohesive DAG structure,
allowing the model to explore complex reasoning pathways while maintaining
logical consistency. Each node in the diagram corresponds to a proposition that
has been proposed, critiqued, refined, or verified, enabling the LLM to
iteratively improve its reasoning through natural language feedback. By
leveraging auto-regressive next-token prediction with role-specific tokens, DoT
facilitates seamless transitions between proposing ideas and critically
evaluating them, providing richer feedback than binary signals. Furthermore, we
formalize the DoT framework using Topos Theory, providing a mathematical
foundation that ensures logical consistency and soundness in the reasoning
process. This approach enhances both the training and inference processes
within a single LLM, eliminating the need for multiple models or external
control mechanisms. DoT offers a conceptual framework for designing
next-generation reasoning-specialized models, emphasizing training efficiency,
robust reasoning capabilities, and theoretical grounding. The code is available
at https://github.com/diagram-of-thought/diagram-of-thought.

摘要：我們介紹了思想圖（DoT），這是一個框架，它將大型語言模型（LLM）中的迭代推理建模為在單一模型內建構一個有向無環圖（DAG）。與將推理表示為線性鏈或樹的傳統方法不同，DoT 將命題、批判、修正和驗證組織成一個有凝聚力的 DAG 結構，允許模型探索複雜的推理路徑，同時保持邏輯一致性。圖表中的每個節點對應於一個已被提出、批判、修正或驗證的命題，使 LLM 能夠通過自然語言回饋迭代地改進其推理。通過利用具有角色特定標記的自動回歸下一個標記預測，DoT 促進了提出想法和批判性評估它們之間的無縫過渡，提供了比二元信號更豐富的回饋。此外，我們使用拓撲理論形式化了 DoT 框架，提供了一個數學基礎，以確保推理過程中的邏輯一致性和健全性。這種方法增強了單一 LLM 內的訓練和推理過程，消除了對多個模型或外部控制機制的需要。DoT 為設計下一代推理專用模型提供了一個概念框架，強調訓練效率、強大的推理能力和理論基礎。代碼可在 https://github.com/diagram-of-thought/diagram-of-thought 獲得。

##### **Can GPT-O1 Kill All Bugs? An Evaluation of GPT-Family LLMs on QuixBugs**
2409.10033v2 by Haichuan Hu, Ye Shang, Guolin Xu, Congqing He, Quanjun Zhang

LLMs have long demonstrated remarkable effectiveness in automatic program
repair (APR), with OpenAI's ChatGPT being one of the most widely used models in
this domain. Through continuous iterations and upgrades of GPT-family models,
their performance in fixing bugs has already reached state-of-the-art levels.
However, there are few works comparing the effectiveness and variations of
different versions of GPT-family models on APR. In this work, inspired by the
recent public release of the GPT-o1 models, we conduct the first study to
compare the effectiveness of different versions of the GPT-family models in
APR. We evaluate the performance of the latest version of the GPT-family models
(i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT
on APR. We conduct an empirical study of the four GPT-family models against
other LLMs and APR techniques on the QuixBugs benchmark from multiple
evaluation perspectives, including repair success rate, repair cost, response
length, and behavior patterns. The results demonstrate that O1's repair
capability exceeds that of prior GPT-family models, successfully fixing all 40
bugs in the benchmark. Our work can serve as a foundation for further in-depth
exploration of the applications of GPT-family models in APR.

摘要：大型語言模型在自動程式修復 (APR) 中長期以來展現出驚人的效果，而 OpenAI 的 ChatGPT 是此領域使用最廣泛的模型之一。透過 GPT 家族模型的持續迭代和升級，它們在修復錯誤方面的效能已達到最先進的水平。然而，比較不同版本的 GPT 家族模型在 APR 上的效能和差異性的研究很少。在這項研究中，受到 GPT-o1 模型最近公開發布的啟發，我們進行了第一個研究，以比較不同版本的 GPT 家族模型在 APR 中的效能。我們評估了最新版本的 GPT 家族模型（即 O1-preview 和 O1-mini）、GPT-4o 和歷史版本的 ChatGPT 在 APR 上的效能。我們針對 QuixBugs 基準，根據修復成功率、修復成本、回應長度和行為模式等多個評估角度，對這四個 GPT 家族模型與其他大型語言模型和 APR 技術進行實證研究。結果表明，O1 的修復能力超過了先前的 GPT 家族模型，成功修復了基準中的所有 40 個錯誤。我們的研究可以作為進一步深入探討 GPT 家族模型在 APR 中應用基礎。

##### **AttnMod: Attention-Based New Art Styles**
2409.10028v1 by Shih-Chieh Su

Imagine a human artist looking at the generated photo of a diffusion model,
and hoping to create a painting out of it. There could be some feature of the
object in the photo that the artist wants to emphasize, some color to disperse,
some silhouette to twist, or some part of the scene to be materialized. These
intentions can be viewed as the modification of the cross attention from the
text prompt onto UNet, during the desoising diffusion. This work presents
AttnMod, to modify attention for creating new unpromptable art styles out of
existing diffusion models. The style-creating behavior is studied across
different setups.

摘要：想像一位人類藝術家看著擴散模型產生的照片，並希望以此為基礎創作一幅畫作。照片中的物體可能有一些特徵是藝術家想要強調的，一些顏色需要分散，一些輪廓需要扭曲，或有些場景需要具體化。這些意圖可視為在去噪擴散過程中，將文字提示從交叉注意力修改為 UNet。這項工作提出了 AttnMod，用於修改注意力，以從既有的擴散模型中創造出新的、無法提示的藝術風格。在不同的設定中研究了這種創造風格的行為。

##### **E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models**
2409.10027v1 by Chan Kim, Keonwoo Kim, Mintaek Oh, Hanbi Baek, Jiyang Lee, Donghwi Jung, Soojin Woo, Younkyung Woo, John Tucker, Roya Firoozi, Seung-Woo Seo, Mac Schwager, Seong-Woo Kim

Large language models (LLMs) have shown significant potential in guiding
embodied agents to execute language instructions across a range of tasks,
including robotic manipulation and navigation. However, existing methods are
primarily designed for static environments and do not leverage the agent's own
experiences to refine its initial plans. Given that real-world environments are
inherently stochastic, initial plans based solely on LLMs' general knowledge
may fail to achieve their objectives, unlike in static scenarios. To address
this limitation, this study introduces the Experience-and-Emotion Map (E2Map),
which integrates not only LLM knowledge but also the agent's real-world
experiences, drawing inspiration from human emotional responses. The proposed
methodology enables one-shot behavior adjustments by updating the E2Map based
on the agent's experiences. Our evaluation in stochastic navigation
environments, including both simulations and real-world scenarios, demonstrates
that the proposed method significantly enhances performance in stochastic
environments compared to existing LLM-based approaches. Code and supplementary
materials are available at https://e2map.github.io/.

摘要：大型語言模型 (LLM) 在指導具象代理人執行各種任務的語言指令方面展現了顯著的潛力，包括機器人操作和導航。然而，現有方法主要設計用於靜態環境，且未利用代理人本身的經驗來優化其初始計畫。考量到現實世界環境本質上是隨機的，僅基於 LLM 一般知識的初始計畫可能無法達成其目標，這與靜態場景不同。為了解決這個限制，本研究引入了體驗與情緒地圖 (E2Map)，它不僅整合了 LLM 知識，還整合了代理人在現實世界中的經驗，並從人類情緒反應中汲取靈感。所提出的方法論能透過根據代理人的經驗更新 E2Map，來進行一次性的行為調整。我們在隨機導航環境（包括模擬和現實世界場景）中的評估結果顯示，與現有的基於 LLM 的方法相比，所提出的方法顯著提升了在隨機環境中的效能。程式碼和補充資料可於 https://e2map.github.io/ 取得。

##### **AceParse: A Comprehensive Dataset with Diverse Structured Texts for Academic Literature Parsing**
2409.10016v1 by Huawei Ji, Cheng Deng, Bo Xue, Zhouyang Jin, Jiaxin Ding, Xiaoying Gan, Luoyi Fu, Xinbing Wang, Chenghu Zhou

With the development of data-centric AI, the focus has shifted from
model-driven approaches to improving data quality. Academic literature, as one
of the crucial types, is predominantly stored in PDF formats and needs to be
parsed into texts before further processing. However, parsing diverse
structured texts in academic literature remains challenging due to the lack of
datasets that cover various text structures. In this paper, we introduce
AceParse, the first comprehensive dataset designed to support the parsing of a
wide range of structured texts, including formulas, tables, lists, algorithms,
and sentences with embedded mathematical expressions. Based on AceParse, we
fine-tuned a multimodal model, named AceParser, which accurately parses various
structured texts within academic literature. This model outperforms the
previous state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard
Similarity, demonstrating the potential of multimodal models in academic
literature parsing. Our dataset is available at
https://github.com/JHW5981/AceParse.

摘要：隨著以資料為中心的 AI 發展，重點已從模型驅動的方法轉移到改善資料品質。學術文獻作為其中一種關鍵類型，主要儲存在 PDF 格式中，且需要在進一步處理之前解析成文字。然而，由於缺乏涵蓋各種文字結構的資料集，解析學術文獻中多樣化的結構化文字仍然具有挑戰性。在本文中，我們介紹了 AceParse，這是第一個綜合性資料集，旨在支援解析廣泛的結構化文字，包括公式、表格、清單、演算法和包含嵌入式數學運算式的句子。根據 AceParse，我們微調了一個多模式模型，名為 AceParser，它可以準確地解析學術文獻中的各種結構化文字。此模型在 F1 分數方面優於先前的最新技術 4.1%，在 Jaccard 相似性方面優於 5%，證明了多模式模型在學術文獻解析中的潛力。我們的資料集可在 https://github.com/JHW5981/AceParse 取得。

##### **HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making**
2409.10011v1 by Sumera Anjum, Hanzhi Zhang, Wenjun Zhou, Eun Jin Paek, Xiaopeng Zhao, Yunhe Feng

Large language models (LLMs) have significantly advanced natural language
processing tasks, yet they are susceptible to generating inaccurate or
unreliable responses, a phenomenon known as hallucination. In critical domains
such as health and medicine, these hallucinations can pose serious risks. This
paper introduces HALO, a novel framework designed to enhance the accuracy and
reliability of medical question-answering (QA) systems by focusing on the
detection and mitigation of hallucinations. Our approach generates multiple
variations of a given query using LLMs and retrieves relevant information from
external open knowledge bases to enrich the context. We utilize maximum
marginal relevance scoring to prioritize the retrieved context, which is then
provided to LLMs for answer generation, thereby reducing the risk of
hallucinations. The integration of LangChain further streamlines this process,
resulting in a notable and robust increase in the accuracy of both open-source
and commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%
to 70%). This framework underscores the critical importance of addressing
hallucinations in medical QA systems, ultimately improving clinical
decision-making and patient care. The open-source HALO is available at:
https://github.com/ResponsibleAILab/HALO.

摘要：大型語言模型 (LLM) 已大幅提升自然語言處理任務，但它們容易產生不準確或不可靠的回應，這現象稱為幻覺。在健康和醫學等關鍵領域，這些幻覺可能會造成嚴重的風險。本論文介紹 HALO，一種新穎的架構，旨在透過專注於偵測和減輕幻覺，來提升醫療問答 (QA) 系統的準確性和可靠性。我們的做法是使用 LLM 產生給定查詢的多個變體，並從外部開放知識庫中擷取相關資訊，以豐富內容。我們利用最大邊際相關性評分來優先處理擷取的內容，然後提供給 LLM 以產生答案，從而降低幻覺的風險。LangChain 的整合進一步簡化了這個流程，導致開放原始碼和商業 LLM，例如 Llama-3.1（從 44% 到 65%）和 ChatGPT（從 56% 到 70%）的準確性顯著且穩健地提升。這個架構強調了在醫療問答系統中解決幻覺的重要性，最終改善了臨床決策制定和患者照護。開放原始碼 HALO 可在以下網址取得：https://github.com/ResponsibleAILab/HALO。

##### **SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL**
2409.10007v1 by Ke Shen, Mayank Kejriwal

In recent years,Text-to-SQL, the problem of automatically converting
questions posed in natural language to formal SQL queries, has emerged as an
important problem at the intersection of natural language processing and data
management research. Large language models (LLMs) have delivered impressive
performance when used in an off-the-shelf performance, but still fall
significantly short of expected expert-level performance. Errors are especially
probable when a nuanced understanding is needed of database schemas, questions,
and SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a
novel in-context learning solution that uses an algorithmic combination of
chain-of-thought (CoT) prompting, self-correction, and ensemble methods to
yield a new state-of-the-art result on challenging Text-to-SQL benchmarks.
Specifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL
achieves 84.2% execution accuracy on the Spider leaderboard's development set,
exceeding both the best results of other baseline GPT-3.5-Turbo-based solutions
(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the
leaderboard.

摘要：近年来，文本到 SQL，将自然语言中提出的问题自动转换为正式 SQL 查询的问题，已成为自然语言处理和数据管理研究交叉领域的一个重要问题。大型语言模型 (LLM) 在现成性能中使用时提供了令人印象深刻的性能，但仍远未达到预期的专家级性能。当需要对数据库架构、问题和 SQL 子句有细致的理解以进行适当的文本到 SQL 转换时，尤其容易出错。我们介绍了 SelECT-SQL，这是一种新颖的上下文学习解决方案，它使用思想链 (CoT) 提示、自我纠正和集成方法的算法组合，在具有挑战性的文本到 SQL 基准上产生新的最先进的结果。具体来说，当使用 GPT-3.5-Turbo 作为基础 LLM 进行配置时，SelECT-SQL 在 Spider 排行榜的开发集上实现了 84.2% 的执行准确率，超过了其他基于 GPT-3.5-Turbo 的基线解决方案的最佳结果 (81.1%)，以及排行榜上报告的 GPT-4 结果的峰值性能 (83.5%)。

##### **FreeMark: A Non-Invasive White-Box Watermarking for Deep Neural Networks**
2409.09996v1 by Yuzhang Chen, Jiangnan Zhu, Yujie Gu, Minoru Kuribayashi, Kouichi Sakurai

Deep neural networks (DNNs) have achieved significant success in real-world
applications. However, safeguarding their intellectual property (IP) remains
extremely challenging. Existing DNN watermarking for IP protection often
require modifying DNN models, which reduces model performance and limits their
practicality.
  This paper introduces FreeMark, a novel DNN watermarking framework that
leverages cryptographic principles without altering the original host DNN
model, thereby avoiding any reduction in model performance. Unlike traditional
DNN watermarking methods, FreeMark innovatively generates secret keys from a
pre-generated watermark vector and the host model using gradient descent. These
secret keys, used to extract watermark from the model's activation values, are
securely stored with a trusted third party, enabling reliable watermark
extraction from suspect models. Extensive experiments demonstrate that FreeMark
effectively resists various watermark removal attacks while maintaining high
watermark capacity.

摘要：深度神經網路 (DNN) 在真實世界的應用中已取得顯著成功。然而，保護其智慧財產權 (IP) 仍然極具挑戰性。現有的 DNN 浮水印用於 IP 保護通常需要修改 DNN 模型，這會降低模型效能並限制其實用性。
本文介紹了 FreeMark，一個新穎的 DNN 浮水印框架，它利用密碼學原理，而不會改變原始主機 DNN 模型，從而避免任何模型效能的降低。與傳統的 DNN 浮水印方法不同，FreeMark 創新地從預先生成的浮水印向量和主機模型中使用梯度下降來產生秘密金鑰。這些用於從模型的激活值中提取浮水印的秘密金鑰安全地儲存在受信任的第三方中，從而能夠從可疑模型中可靠地提取浮水印。廣泛的實驗表明，FreeMark 有效地抵抗各種浮水印移除攻擊，同時保持高浮水印容量。

##### **Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system**
2409.09989v1 by Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh

This paper provides a comprehensive survey of sentiment analysis within the
context of artificial intelligence (AI) and large language models (LLMs).
Sentiment analysis, a critical aspect of natural language processing (NLP), has
evolved significantly from traditional rule-based methods to advanced deep
learning techniques. This study examines the historical development of
sentiment analysis, highlighting the transition from lexicon-based and
pattern-based approaches to more sophisticated machine learning and deep
learning models. Key challenges are discussed, including handling bilingual
texts, detecting sarcasm, and addressing biases. The paper reviews
state-of-the-art approaches, identifies emerging trends, and outlines future
research directions to advance the field. By synthesizing current methodologies
and exploring future opportunities, this survey aims to understand sentiment
analysis in the AI and LLM context thoroughly.

摘要：本文全面探討在人工智慧 (AI) 和大型語言模型 (LLM) 的脈絡中進行情緒分析。情緒分析是自然語言處理 (NLP) 的一個重要面向，已經從傳統的基於規則的方法顯著演進到先進的深度學習技術。本研究探討情緒分析的歷史發展，重點說明從基於詞彙和基於模式的方法轉變到更精密的機器學習和深度學習模型。本文討論了關鍵挑戰，包括處理雙語文本、偵測諷刺和解決偏見。本文回顧了最先進的方法，找出新興趨勢，並概述未來的研究方向以推進此領域。透過綜合當前方法並探索未來機會，本調查旨在全面了解 AI 和 LLM 脈絡中的情緒分析。

##### **Deep Graph Anomaly Detection: A Survey and New Perspectives**
2409.09957v1 by Hezhe Qiao, Hanghang Tong, Bo An, Irwin King, Charu Aggarwal, Guansong Pang

Graph anomaly detection (GAD), which aims to identify unusual graph instances
(nodes, edges, subgraphs, or graphs), has attracted increasing attention in
recent years due to its significance in a wide range of applications. Deep
learning approaches, graph neural networks (GNNs) in particular, have been
emerging as a promising paradigm for GAD, owing to its strong capability in
capturing complex structure and/or node attributes in graph data. Considering
the large number of methods proposed for GNN-based GAD, it is of paramount
importance to summarize the methodologies and findings in the existing GAD
studies, so that we can pinpoint effective model designs for tackling open GAD
problems. To this end, in this work we aim to present a comprehensive review of
deep learning approaches for GAD. Existing GAD surveys are focused on
task-specific discussions, making it difficult to understand the technical
insights of existing methods and their limitations in addressing some unique
challenges in GAD. To fill this gap, we first discuss the problem complexities
and their resulting challenges in GAD, and then provide a systematic review of
current deep GAD methods from three novel perspectives of methodology,
including GNN backbone design, proxy task design for GAD, and graph anomaly
measures. To deepen the discussions, we further propose a taxonomy of 13
fine-grained method categories under these three perspectives to provide more
in-depth insights into the model designs and their capabilities. To facilitate
the experiments and validation, we also summarize a collection of widely-used
GAD datasets and empirical comparison. We further discuss multiple open
problems to inspire more future high-quality research. A continuously updated
repository for datasets, links to the codes of algorithms, and empirical
comparison is available at
https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.

摘要：圖形異常偵測 (GAD) 旨在識別異常的圖形實例 (節點、邊緣、子圖形或圖形)，近年來由於其在廣泛應用中的重要性而備受關注。深度學習方法，特別是圖形神經網路 (GNN) 已成為 GAD 的一個有前途的範例，因為它具有擷取圖形資料中複雜結構和/或節點屬性的強大能力。考慮到為基於 GNN 的 GAD 提出的方法數量龐大，因此總結現有 GAD 研究中的方法論和發現至關重要，以便我們可以找出解決開放式 GAD 問題的有效模型設計。為此，在這項工作中，我們旨在對 GAD 的深度學習方法提出全面的回顧。現有的 GAD 調查集中在特定任務的討論上，這使得難以理解現有方法的技術見解及其在解決 GAD 中一些獨特挑戰方面的局限性。為了填補這一空白，我們首先討論問題的複雜性和它們在 GAD 中產生的挑戰，然後從方法論的三个新觀點對當前的深度 GAD 方法進行系統性回顧，包括 GNN 主幹設計、GAD 的代理任務設計和圖形異常測量。為了加深討論，我們進一步提出了這三個觀點下 13 個細粒度方法類別的分類法，以提供對模型設計及其功能的更深入見解。為了促進實驗和驗證，我們還總結了一系列廣泛使用的 GAD 資料集和實證比較。我們進一步討論了多個開放性問題，以激勵更多未來的優質研究。一個持續更新的資料集存放庫、演算法程式碼的連結和實證比較可在 https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection 取得。

##### **Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for Fine-grained Text Evaluations**
2409.09947v1 by Abe Bohan Hou, William Jurayj, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme

Large Language Models (LLMs) show promise as a writing aid for professionals
performing legal analyses. However, LLMs can often hallucinate in this setting,
in ways difficult to recognize by non-professionals and existing text
evaluation metrics. In this work, we pose the question: when can
machine-generated legal analysis be evaluated as acceptable? We introduce the
neutral notion of gaps, as opposed to hallucinations in a strict erroneous
sense, to refer to the difference between human-written and machine-generated
legal analysis. Gaps do not always equate to invalid generation. Working with
legal experts, we consider the CLERC generation task proposed in Hou et al.
(2024b), leading to a taxonomy, a fine-grained detector for predicting gap
categories, and an annotated dataset for automatic evaluation. Our best
detector achieves 67% F1 score and 80% precision on the test set. Employing
this detector as an automated metric on legal analysis generated by SOTA LLMs,
we find around 80% contain hallucinations of different kinds.

摘要：大型語言模型 (LLM) 作為法律分析專業人員的寫作輔助工具，顯示出前景。然而，LLM 在此設定中常常會出現幻覺，而非專業人士和現有的文字評估指標很難辨識。在這項工作中，我們提出了一個問題：機器產生的法律分析何時才能被評估為可接受？我們引入了間隙的中立概念，與嚴格錯誤意義上的幻覺相反，用於指涉人類撰寫和機器產生的法律分析之間的差異。間隙並不總是等於無效產生。在與法律專家合作時，我們考慮了 Hou 等人 (2024b) 中提出的 CLERC 生成任務，從而產生了一個分類法、一個用於預測間隙類別的細粒度檢測器，以及一個用於自動評估的註釋資料集。我們最好的檢測器在測試集中達到了 67% 的 F1 分數和 80% 的準確率。使用此檢測器作為 SOTA LLM 生成的法律分析的自動化指標，我們發現大約 80% 包含不同類型的幻覺。

##### **Fault Analysis And Predictive Maintenance Of Induction Motor Using Machine Learning**
2409.09944v1 by Kavana Venkatesh, Neethi M

Induction motors are one of the most crucial electrical equipment and are
extensively used in industries in a wide range of applications. This paper
presents a machine learning model for the fault detection and classification of
induction motor faults by using three phase voltages and currents as inputs.
The aim of this work is to protect vital electrical components and to prevent
abnormal event progression through early detection and diagnosis. This work
presents a fast forward artificial neural network model to detect some of the
commonly occurring electrical faults like overvoltage, under voltage, single
phasing, unbalanced voltage, overload, ground fault. A separate model free
monitoring system wherein the motor itself acts like a sensor is presented and
the only monitored signals are the input given to the motor. Limits for current
and voltage values are set for the faulty and healthy conditions, which is done
by a classifier. Real time data from a 0.33 HP induction motor is used to train
and test the neural network. The model so developed analyses the voltage and
current values given at a particular instant and classifies the data into no
fault or the specific fault. The model is then interfaced with a real motor to
accurately detect and classify the faults so that further necessary action can
be taken.

摘要：感應馬達是最重要的電氣設備之一，廣泛用於各種產業應用。本文提出一個機器學習模型，使用三相電壓和電流作為輸入，用於感應馬達故障的故障偵測和分類。這項工作的目的是保護重要的電氣元件，並透過早期偵測和診斷來防止異常事件的發生。這項工作提出了一個快速前饋人工神經網路模型，用於偵測一些常見的電氣故障，例如過電壓、欠電壓、單相、電壓不平衡、過載、接地故障。提出了一個單獨的模型自由監控系統，其中馬達本身就像一個感測器，而唯一監控的訊號是輸入給馬達的訊號。電流和電壓值的限制設定為故障和健康狀況，這是由分類器完成的。來自 0.33 HP 感應馬達的即時資料用於訓練和測試神經網路。如此開發的模型會分析特定時刻給定的電壓和電流值，並將資料分類為無故障或特定故障。然後將模型與真實馬達介接，以準確偵測和分類故障，以便採取進一步必要的措施。

##### **Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges**
2409.09927v1 by Vinay Samuel, Yue Zhou, Henry Peng Zou

As large language models achieve increasingly impressive results, questions
arise about whether such performance is from generalizability or mere data
memorization. Thus, numerous data contamination detection methods have been
proposed. However, these approaches are often validated with traditional
benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness
when evaluating state-of-the-art LLMs on the contamination of more challenging
benchmarks. To address this gap and provide a dual investigation of SOTA LLM
contamination status and detection method robustness, we evaluate five
contamination detection approaches with four state-of-the-art LLMs across eight
challenging datasets often used in modern LLM evaluation. Our analysis reveals
that (1) Current methods have non-trivial limitations in their assumptions and
practical applications; (2) Notable difficulties exist in detecting
contamination introduced during instruction fine-tuning with answer
augmentation; and (3) Limited consistencies between SOTA contamination
detection techniques. These findings highlight the complexity of contamination
detection in advanced LLMs and the urgent need for further research on robust
and generalizable contamination evaluation. Our code is available at
https://github.com/vsamuel2003/data-contamination.

摘要：隨著大型語言模型取得越來越令人印象深刻的成果，人們開始質疑這些表現是來自於泛化能力還是單純的資料記憶。因此，已經提出了許多資料污染偵測方法。然而，這些方法通常使用傳統基準和早期 LLM 進行驗證，對於在更具挑戰性的基準上評估最先進 LLM 的污染情況時，其有效性仍存在不確定性。為了解決這個差距並對 SOTA LLM 污染狀態和偵測方法的穩健性進行雙重調查，我們使用四個最先進的 LLM 評估了八個現代 LLM 評估中常用的具有挑戰性的資料集中的五種污染偵測方法。我們的分析顯示：(1) 目前的許多方法在其假設和實際應用中都有非顯著的限制；(2) 在使用答案擴充進行指令微調期間引入的污染偵測存在顯著的困難；(3) SOTA 污染偵測技術之間的一致性有限。這些發現突顯了先進 LLM 中污染偵測的複雜性，以及對穩健且可泛化的污染評估進一步研究的迫切需求。我們的程式碼可在 https://github.com/vsamuel2003/data-contamination 取得。

##### **SFR-RAG: Towards Contextually Faithful LLMs**
2409.09916v1 by Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, Shafiq Joty

Retrieval Augmented Generation (RAG), a paradigm that integrates external
contextual information with large language models (LLMs) to enhance factual
accuracy and relevance, has emerged as a pivotal area in generative AI. The
LLMs used in RAG applications are required to faithfully and completely
comprehend the provided context and users' questions, avoid hallucination,
handle unanswerable, counterfactual or otherwise low-quality and irrelevant
contexts, perform complex multi-hop reasoning and produce reliable citations.
In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with
an emphasis on context-grounded generation and hallucination minimization. We
also present ContextualBench, a new evaluation framework compiling multiple
popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with
consistent RAG settings to ensure reproducibility and consistency in model
assessments. Experimental results demonstrate that our SFR-RAG-9B model
outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving
state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with
significantly fewer parameters. The model is also shown to be resilient to
alteration in the contextual information and behave appropriately when relevant
context is removed. Additionally, the SFR-RAG model maintains competitive
performance in general instruction-following tasks and function-calling
capabilities.

摘要：檢索擴增生成（RAG）是一種將外部上下文資訊與大型語言模型（LLM）整合以增強事實準確度和相關性的範例，已成為生成式 AI 中的關鍵領域。RAG 應用程式中使用的 LLM 必須忠實且完整地理解提供的上下文和使用者的問題，避免產生幻覺、處理無法回答、反事實或其他低品質和不相關的上下文，執行複雜的多跳推理並產生可靠的引文。在本文中，我們介紹 SFR-RAG，這是一個經過指令微調的小型 LLM，重點在於以上下文為基礎的生成和幻覺最小化。我們還提供了 ContextualBench，這是一個新的評估框架，編制了多個流行且多樣化的 RAG 基準，例如 HotpotQA 和 TriviaQA，並採用一致的 RAG 設定，以確保模型評估的可重複性和一致性。實驗結果表明，我們的 SFR-RAG-9B 模型優於領先的基準，例如 Command-R+（104B）和 GPT-4o，在 ContextualBench 中的 7 個基準中的 3 個中取得了最先進的結果，且參數明顯較少。該模型還顯示出對上下文資訊變化的韌性，並在移除相關上下文時表現得當。此外，SFR-RAG 模型在一般的指令遵循任務和函式呼叫能力中維持了競爭力。

##### **Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors**
2409.09905v1 by Joseph Suh, Suhong Moon, Minwoo Kang, David M. Chan

Assessing personality traits using large language models (LLMs) has emerged
as an interesting and challenging area of research. While previous methods
employ explicit questionnaires, often derived from the Big Five model of
personality, we hypothesize that LLMs implicitly encode notions of personality
when modeling next-token responses. To demonstrate this, we introduce a novel
approach that uncovers latent personality dimensions in LLMs by applying
singular value de-composition (SVD) to the log-probabilities of
trait-descriptive adjectives. Our experiments show that LLMs "rediscover" core
personality traits such as extraversion, agreeableness, conscientiousness,
neuroticism, and openness without relying on direct questionnaire inputs, with
the top-5 factors corresponding to Big Five traits explaining 74.3% of the
variance in the latent space. Moreover, we can use the derived principal
components to assess personality along the Big Five dimensions, and achieve
improvements in average personality prediction accuracy of up to 5% over
fine-tuned models, and up to 21% over direct LLM-based scoring techniques.

摘要：使用大型语言模型 (LLM) 评估人格特质已成为一个有趣且具有挑战性的研究领域。虽然以前的方法采用明确的问卷，通常源自于人格五大模型，但我们假设 LLM 在建模下一个标记响应时会隐式编码人格概念。为了证明这一点，我们引入了一种新颖的方法，通过对特质描述性形容词的对数概率应用奇异值分解 (SVD) 来揭示 LLM 中的潜在人格维度。我们的实验表明，LLM 在不依赖直接问卷输入的情况下“重新发现”了核心人格特质，如外向性、宜人性、尽责性、神经质和开放性，前 5 个因素对应于五大特质，解释了潜在空间中 74.3% 的方差。此外，我们可以使用派生的主成分来评估五大维度上的人格，并且与经过微调的模型相比，平均人格预测准确率提高了 5%，与基于 LLM 的直接评分技术相比，提高了 21%。

##### **Acquiring Pronunciation Knowledge from Transcribed Speech Audio via Multi-task Learning**
2409.09891v1 by Siqi Sun, Korin Richmond

Recent work has shown the feasibility and benefit of bootstrapping an
integrated sequence-to-sequence (Seq2Seq) linguistic frontend from a
traditional pipeline-based frontend for text-to-speech (TTS). To overcome the
fixed lexical coverage of bootstrapping training data, previous work has
proposed to leverage easily accessible transcribed speech audio as an
additional training source for acquiring novel pronunciation knowledge for
uncovered words, which relies on an auxiliary ASR model as part of a cumbersome
implementation flow. In this work, we propose an alternative method to leverage
transcribed speech audio as an additional training source, based on multi-task
learning (MTL). Experiments show that, compared to a baseline Seq2Seq frontend,
the proposed MTL-based method reduces PER from 2.5% to 1.6% for those word
types covered exclusively in transcribed speech audio, achieving a similar
performance to the previous method but with a much simpler implementation flow.

摘要：最近的研究顯示了從傳統的基於管道的前端中自舉一個整合的序列到序列 (Seq2Seq) 語言前端的可能性和好處，用於文字轉語音 (TTS)。為了克服自舉訓練資料的固定詞彙覆蓋範圍，先前的研究提議利用容易取得的轉錄語音音訊作為額外的訓練來源，用於獲得未涵蓋單字的新發音知識，這依賴於輔助 ASR 模型作為繁瑣的實作流程的一部分。在這項研究中，我們提出一個替代方法，利用轉錄語音音訊作為額外的訓練來源，基於多任務學習 (MTL)。實驗顯示，與基線 Seq2Seq 前端相比，提出的基於 MTL 的方法將那些僅在轉錄語音音訊中涵蓋的字詞類型的 PER 從 2.5% 降低到 1.6%，達到與先前方法相似的效能，但實作流程簡單許多。

##### **REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models**
2409.09877v2 by Teerapong Panboonyuen

This paper introduces a novel framework for detecting and segmenting critical
road assets on Thai highways using an advanced Refined Generalized Focal Loss
(REG) formulation. Integrated into state-of-the-art vision-based detection and
segmentation models, the proposed method effectively addresses class imbalance
and the challenges of localizing small, underrepresented road elements,
including pavilions, pedestrian bridges, information signs, single-arm poles,
bus stops, warning signs, and concrete guardrails. To improve both detection
and segmentation accuracy, a multi-task learning strategy is adopted,
optimizing REG across multiple tasks. REG is further enhanced by incorporating
a spatial-contextual adjustment term, which accounts for the spatial
distribution of road assets, and a probabilistic refinement that captures
prediction uncertainty in complex environments, such as varying lighting
conditions and cluttered backgrounds. Our rigorous mathematical formulation
demonstrates that REG minimizes localization and classification errors by
applying adaptive weighting to hard-to-detect instances while down-weighting
easier examples. Experimental results show a substantial performance
improvement, achieving a mAP50 of 80.34 and an F1-score of 77.87, significantly
outperforming conventional methods. This research underscores the capability of
advanced loss function refinements to enhance the robustness and accuracy of
road asset detection and segmentation, thereby contributing to improved road
safety and infrastructure management. For an in-depth discussion of the
mathematical background and related methods, please refer to previous work
available at \url{https://github.com/kaopanboonyuen/REG}.

摘要：<paragraph>本文介紹了一個新穎的架構，用於使用先進的 Refined Generalized Focal Loss (REG) 公式偵測和區分泰國高速公路上的重要道路資產。該方法整合到最先進的基於視覺的偵測和區分模型中，有效地解決了類別不平衡的問題，以及定位小型、代表性不足道路元素的挑戰，包括涼亭、人行天橋、資訊標誌、單臂桿、公車站牌、警告標誌和混凝土護欄。為了提高偵測和區分準確度，採用了多任務學習策略，在多個任務中最佳化 REG。REG 進一步透過納入空間脈絡調整項進行增強，該項考慮了道路資產的空間分佈，以及捕捉複雜環境中預測不確定性的機率修正，例如不同的照明條件和混亂的背景。我們嚴謹的數學公式證明，REG 透過對難以偵測的實例應用自適應加權，同時降低較容易範例的權重，將定位和分類錯誤降到最低。實驗結果顯示出顯著的效能提升，達到 80.34 的 mAP50 和 77.87 的 F1 分數，顯著優於傳統方法。這項研究強調了先進損失函數修正的能力，可增強道路資產偵測和區分的穩健性和準確性，進而改善道路安全和基礎設施管理。如需深入探討數學背景和相關方法，請參閱發表於 \url{https://github.com/kaopanboonyuen/REG} 的先前研究。</paragraph>

##### **Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent**
2409.09869v1 by Pavel Osinenko, Grigory Yaremenko, Roman Zashchitin, Anton Bolychev, Sinan Ibrahim, Dmitrii Dobriborsci

This work presents and showcases a novel reinforcement learning agent called
Critic As Lyapunov Function (CALF) which is model-free and ensures online
environment, in other words, dynamical system stabilization. Online means that
in each learning episode, the said environment is stabilized. This, as
demonstrated in a case study with a mobile robot simulator, greatly improves
the overall learning performance. The base actor-critic scheme of CALF is
analogous to SARSA. The latter did not show any success in reaching the target
in our studies. However, a modified version thereof, called SARSA-m here, did
succeed in some learning scenarios. Still, CALF greatly outperformed the said
approach. CALF was also demonstrated to improve a nominal stabilizer provided
to it. In summary, the presented agent may be considered a viable approach to
fusing classical control with reinforcement learning. Its concurrent approaches
are mostly either offline or model-based, like, for instance, those that fuse
model-predictive control into the agent.

摘要：本研究提出並展示了一種稱為 Lyapunov 函數批評者 (CALF) 的新型強化學習代理，它無模型且確保線上環境，換句話說，動態系統穩定。線上表示在每個學習情節中，所述環境會穩定下來。正如在行動機器人模擬器的案例研究中所展示的那樣，這極大地改善了整體學習表現。CALF 的基本動作批評者方案類似於 SARSA。後者在我們的研究中未顯示出任何達到目標的成功。然而，其修改版本，在此稱為 SARSA-m，確實在一些學習場景中獲得成功。儘管如此，CALF 仍大大優於所述方法。CALF 也被證明可以改進提供給它的標稱穩定器。總之，提出的代理可以被視為將經典控制與強化學習融合的可行方法。它的並發方法大多是離線或基於模型的，例如將模型預測控制融合到代理中的方法。

##### **Towards Kinetic Manipulation of the Latent Space**
2409.09867v1 by Diego Porres

The latent space of many generative models are rich in unexplored valleys and
mountains. The majority of tools used for exploring them are so far limited to
Graphical User Interfaces (GUIs). While specialized hardware can be used for
this task, we show that a simple feature extraction of pre-trained
Convolutional Neural Networks (CNNs) from a live RGB camera feed does a very
good job at manipulating the latent space with simple changes in the scene,
with vast room for improvement. We name this new paradigm Visual-reactive
Interpolation, and the full code can be found at
https://github.com/PDillis/stylegan3-fun.

摘要：許多生成模型的潛在空間豐富且有許多未探索的谷地和山脈。目前用於探索它們的大多數工具僅限於圖形使用者介面 (GUI)。雖然可以將專用硬體用於此任務，但我們展示了從即時 RGB 相機饋送中預先訓練的卷積神經網路 (CNN) 的簡單特徵萃取，在場景中進行簡單變更時，就能在潛在空間中進行操作，而且有很大的改進空間。我們將此新範例命名為視覺反應式插值，而完整的程式碼可以在 https://github.com/PDillis/stylegan3-fun 中找到。

##### **Constructing a Singing Style Caption Dataset**
2409.09866v1 by Hyunjong Ok, Jaeho Lee

Singing voice synthesis and conversion have emerged as significant subdomains
of voice generation, leading to much demands on prompt-conditioned generation.
Unlike common voice data, generating a singing voice requires an understanding
of various associated vocal and musical characteristics, such as the vocal tone
of the singer or emotional expressions. However, existing open-source
audio-text datasets for voice generation tend to capture only a very limited
range of attributes, often missing musical characteristics of the audio. To
fill this gap, we introduce S2Cap, an audio-text pair dataset with a diverse
set of attributes. S2Cap consists of pairs of textual prompts and music audio
samples with a wide range of vocal and musical attributes, including pitch,
volume, tempo, mood, singer's gender and age, and musical genre and emotional
expression. Utilizing S2Cap, we suggest an effective novel baseline algorithm
for singing style captioning. Singing style captioning is a relative task to
voice generation that generates text descriptions of vocal characteristics,
which we first suggested. First, to mitigate the misalignment between the audio
encoder and the text decoder, we present a novel mechanism called CRESCENDO,
which utilizes positive-pair similarity learning to synchronize the embedding
spaces of a pretrained audio encoder to get similar embeddings with a text
encoder. We additionally supervise the model using the singer's voice, which is
demixed by the accompaniment. This supervision allows the model to more
accurately capture vocal characteristics, leading to improved singing style
captions that better reflect the style of the singer. The dataset and the codes
are available at \bulurl{https://github.com/HJ-Ok/S2cap}.

摘要：歌唱聲音合成與轉換已成為語音生成的重要子領域，對提示條件生成產生許多需求。與常見的語音資料不同，生成歌唱聲音需要了解各種相關的聲樂和音樂特徵，例如歌手的聲調或情緒表達。然而，現有的開放原始碼語音生成音訊文字資料集往往只能擷取非常有限的屬性範圍，經常遺漏音訊的音樂特徵。為了填補這個空白，我們引入了 S2Cap，一個具有多樣化屬性集的音訊文字配對資料集。S2Cap 包含文字提示和音樂音訊範例配對，具有廣泛的聲樂和音樂屬性，包括音高、音量、速度、情緒、歌手的性別和年齡，以及音樂類型和情緒表達。利用 S2Cap，我們建議一種用於歌唱風格標註的有效新穎基準演算法。歌唱風格標註是語音生成的相對任務，可產生聲樂特徵的文字描述，這是我們首先提出的。首先，為了減輕音訊編碼器和文字解碼器之間的未對齊，我們提出了一種稱為 CRESCENDO 的新機制，它利用正對相似性學習來同步預訓練音訊編碼器的嵌入空間，以獲得與文字編碼器類似的嵌入。我們另外使用歌手的聲音監督模型，該聲音是由伴奏解混的。此監督允許模型更準確地擷取聲樂特徵，從而產生更好的歌唱風格標註，更能反映歌手的風格。資料集和程式碼可在 \bulurl{https://github.com/HJ-Ok/S2cap} 取得。

##### **A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View**
2409.09858v1 by Jing Ma

Graph machine learning (GML) has been successfully applied across a wide
range of tasks. Nonetheless, GML faces significant challenges in generalizing
over out-of-distribution (OOD) data, which raises concerns about its wider
applicability. Recent advancements have underscored the crucial role of
causality-driven approaches in overcoming these generalization challenges.
Distinct from traditional GML methods that primarily rely on statistical
dependencies, causality-focused strategies delve into the underlying causal
mechanisms of data generation and model prediction, thus significantly
improving the generalization of GML across different environments. This paper
offers a thorough review of recent progress in causality-involved GML
generalization. We elucidate the fundamental concepts of employing causality to
enhance graph model generalization and categorize the various approaches,
providing detailed descriptions of their methodologies and the connections
among them. Furthermore, we explore the incorporation of causality in other
related important areas of trustworthy GML, such as explanation, fairness, and
robustness. Concluding with a discussion on potential future research
directions, this review seeks to articulate the continuing development and
future potential of causality in enhancing the trustworthiness of graph machine
learning.

摘要：圖形機器學習 (GML) 已成功應用於廣泛的任務中。儘管如此，GML 在推廣到分布外 (OOD) 資料時面臨重大挑戰，這引發了對其更廣泛適用性的擔憂。最近的進展強調了因果驅動方法在克服這些概化挑戰中的關鍵作用。不同於主要依賴統計依賴性的傳統 GML 方法，專注於因果關係的策略深入探討資料生成和模型預測的底層因果機制，從而顯著改善了 GML 在不同環境中的概化。本文全面回顧了因果關係參與的 GML 概化的最新進展。我們闡明了使用因果關係來增強圖形模型概化的基本概念，並對各種方法進行分類，詳細描述它們的方法論和它們之間的聯繫。此外，我們探討了因果關係在其他相關的重要可信賴 GML 領域中的應用，例如解釋、公平性和穩健性。本綜述以對潛在未來研究方向的討論作為結尾，旨在闡明因果關係在增強圖形機器學習的可信度方面的持續發展和未來潛力。

##### **A Benchmark Dataset with Larger Context for Non-Factoid Question Answering over Islamic Text**
2409.09844v1 by Faiza Qamar, Seemab Latif, Rabia Latif

Accessing and comprehending religious texts, particularly the Quran (the
sacred scripture of Islam) and Ahadith (the corpus of the sayings or traditions
of the Prophet Muhammad), in today's digital era necessitates efficient and
accurate Question-Answering (QA) systems. Yet, the scarcity of QA systems
tailored specifically to the detailed nature of inquiries about the Quranic
Tafsir (explanation, interpretation, context of Quran for clarity) and Ahadith
poses significant challenges. To address this gap, we introduce a comprehensive
dataset meticulously crafted for QA purposes within the domain of Quranic
Tafsir and Ahadith. This dataset comprises a robust collection of over 73,000
question-answer pairs, standing as the largest reported dataset in this
specialized domain. Importantly, both questions and answers within the dataset
are meticulously enriched with contextual information, serving as invaluable
resources for training and evaluating tailored QA systems. However, while this
paper highlights the dataset's contributions and establishes a benchmark for
evaluating QA performance in the Quran and Ahadith domains, our subsequent
human evaluation uncovered critical insights regarding the limitations of
existing automatic evaluation techniques. The discrepancy between automatic
evaluation metrics, such as ROUGE scores, and human assessments became
apparent. The human evaluation indicated significant disparities: the model's
verdict consistency with expert scholars ranged between 11% to 20%, while its
contextual understanding spanned a broader spectrum of 50% to 90%. These
findings underscore the necessity for evaluation techniques that capture the
nuances and complexities inherent in understanding religious texts, surpassing
the limitations of traditional automatic metrics.

摘要：<paragraph>在當今的數位時代，存取和理解宗教文本，特別是古蘭經（伊斯蘭教的聖典）和聖訓（先知穆罕默德的言論或傳統），需要有效且準確的問答（QA）系統。然而，專門針對古蘭經註釋（解釋、詮釋、古蘭經的背景以求清晰）和聖訓的詳細詢問性質而量身打造的 QA 系統相當稀少，這構成了重大的挑戰。為了解決這個差距，我們在古蘭經註釋和聖訓的領域中，針對 QA 目的精心製作了一個全面的資料集。這個資料集包含超過 73,000 個問題回答對，是這個專業領域中已報告的最大資料集。重要的是，資料集中的問題和答案都經過仔細地豐富了背景資訊，作為訓練和評估量身打造的 QA 系統的寶貴資源。然而，儘管本文突出了資料集的貢獻，並為評估古蘭經和聖訓領域中的 QA 效能建立了一個基準，我們後續的人類評估揭露了有關現有自動評估技術的限制的重要見解。自動評估指標（例如 ROUGE 分數）和人類評估之間的差異變得明顯。人類評估顯示出顯著的差異：模型的判決與專家學者的吻合度在 11% 到 20% 之間，而其背景理解則涵蓋了 50% 到 90% 的更廣泛範圍。這些發現強調了評估技術的必要性，這些技術可以捕捉理解宗教文本中固有的細微差別和複雜性，超越傳統自動指標的限制。</paragraph>

##### **Generating Synthetic Free-text Medical Records with Low Re-identification Risk using Masked Language Modeling**
2409.09831v2 by Samuel Belkadi, Libo Ren, Nicolo Micheletti, Lifeng Han, Goran Nenadic

In this paper, we present a system that generates synthetic free-text medical
records, such as discharge summaries, admission notes and doctor
correspondences, using Masked Language Modeling (MLM). Our system is designed
to preserve the critical information of the records while introducing
significant diversity and minimizing re-identification risk. The system
incorporates a de-identification component that uses Philter to mask Protected
Health Information (PHI), followed by a Medical Entity Recognition (NER) model
to retain key medical information. We explore various masking ratios and
mask-filling techniques to balance the trade-off between diversity and fidelity
in the synthetic outputs without affecting overall readability. Our results
demonstrate that the system can produce high-quality synthetic data with
significant diversity while achieving a HIPAA-compliant PHI recall rate of 0.96
and a low re-identification risk of 0.035. Furthermore, downstream evaluations
using a NER task reveal that the synthetic data can be effectively used to
train models with performance comparable to those trained on real data. The
flexibility of the system allows it to be adapted for specific use cases,
making it a valuable tool for privacy-preserving data generation in medical
research and healthcare applications.

摘要：在本文中，我們提出一個系統，使用遮罩語言模型 (MLM) 來產生合成式自由文本醫療記錄，例如出院摘要、入院記錄和醫生信件。我們的系統旨在保留記錄中的關鍵資訊，同時引入顯著的多樣性並將再識別風險降到最低。該系統包含一個去識別組件，使用 Philter 來遮罩受保護的健康資訊 (PHI)，然後使用醫學實體識別 (NER) 模型來保留關鍵的醫學資訊。我們探索各種遮罩比率和遮罩填充技術，以平衡合成輸出中多樣性和保真度之間的取捨，同時不影響整體可讀性。我們的結果表明，該系統可以產生具有顯著多樣性的高品質合成資料，同時實現符合 HIPAA 的 PHI 召回率 0.96 和 0.035 的低再識別風險。此外，使用 NER 任務的下游評估表明，合成資料可用於有效訓練模型，其效能與使用真實資料訓練的模型相當。該系統的靈活性允許它適應特定使用案例，使其成為醫學研究和醫療保健應用中用於保護隱私的資料生成的寶貴工具。

##### **Latent Diffusion Models for Controllable RNA Sequence Generation**
2409.09828v1 by Kaixuan Huang, Yukang Yang, Kaidi Fu, Yanyi Chu, Le Cong, Mengdi Wang

This paper presents RNAdiffusion, a latent diffusion model for generating and
optimizing discrete RNA sequences. RNA is a particularly dynamic and versatile
molecule in biological processes. RNA sequences exhibit high variability and
diversity, characterized by their variable lengths, flexible three-dimensional
structures, and diverse functions. We utilize pretrained BERT-type models to
encode raw RNAs into token-level biologically meaningful representations. A
Q-Former is employed to compress these representations into a fixed-length set
of latent vectors, with an autoregressive decoder trained to reconstruct RNA
sequences from these latent variables. We then develop a continuous diffusion
model within this latent space. To enable optimization, we train reward
networks to estimate functional properties of RNA from the latent variables. We
employ gradient-based guidance during the backward diffusion process, aiming to
generate RNA sequences that are optimized for higher rewards. Empirical
experiments confirm that RNAdiffusion generates non-coding RNAs that align with
natural distributions across various biological indicators. We fine-tuned the
diffusion model on untranslated regions (UTRs) of mRNA and optimize sample
sequences for protein translation efficiencies. Our guided diffusion model
effectively generates diverse UTR sequences with high Mean Ribosome Loading
(MRL) and Translation Efficiency (TE), surpassing baselines. These results hold
promise for studies on RNA sequence-function relationships, protein synthesis,
and enhancing therapeutic RNA design.

摘要：本文提出了 RNAdiffusion，一種用於生成和最佳化離散 RNA 序列的潛在擴散模型。RNA 是一種在生物過程中特別動態且多功能的分子。RNA 序列表現出高度的可變性和多樣性，其特徵在於其長度可變、三維結構靈活且功能多樣。我們利用預訓練的 BERT 類型模型將原始 RNA 編碼成具有生物學意義的符號級別表示。採用 Q-Former 將這些表示壓縮成一組固定長度的潛在向量，並訓練自迴歸解碼器從這些潛在變量中重建 RNA 序列。然後，我們在這個潛在空間中開發一個連續擴散模型。為了啟用最佳化，我們訓練獎勵網絡從潛在變量中估計 RNA 的功能特性。我們在向後擴散過程中採用基於梯度的指導，旨在生成針對更高獎勵進行最佳化的 RNA 序列。經驗實驗證實，RNAdiffusion 生成的非編碼 RNA 與各種生物指標的自然分佈一致。我們對 mRNA 的非轉譯區 (UTR) 微調了擴散模型，並針對蛋白質轉譯效率最佳化樣本序列。我們指導擴散模型有效地生成了具有高平均核糖體載入量 (MRL) 和轉譯效率 (TE) 的多樣化 UTR 序列，超越了基線。這些結果有望促進對 RNA 序列-功能關係、蛋白質合成和增強治療性 RNA 設計的研究。

##### **GP-GPT: Large Language Model for Gene-Phenotype Mapping**
2409.09825v1 by Yanjun Lyu, Zihao Wu, Lu Zhang, Jing Zhang, Yiwei Li, Wei Ruan, Zhengliang Liu, Xiaowei Yu, Chao Cao, Tong Chen, Minheng Chen, Yan Zhuang, Xiang Li, Rongjie Liu, Chao Huang, Wentao Li, Tianming Liu, Dajiang Zhu

Pre-trained large language models(LLMs) have attracted increasing attention
in biomedical domains due to their success in natural language processing.
However, the complex traits and heterogeneity of multi-sources genomics data
pose significant challenges when adapting these models to the bioinformatics
and biomedical field. To address these challenges, we present GP-GPT, the first
specialized large language model for genetic-phenotype knowledge representation
and genomics relation analysis. Our model is fine-tuned in two stages on a
comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,
and medical genetics, derived from multiple large-scale validated datasets and
scientific publications. GP-GPT demonstrates proficiency in accurately
retrieving medical genetics information and performing common genomics analysis
tasks, such as genomics information retrieval and relationship determination.
Comparative experiments across domain-specific tasks reveal that GP-GPT
outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These
results highlight GP-GPT's potential to enhance genetic disease relation
research and facilitate accurate and efficient analysis in the fields of
genomics and medical genetics. Our investigation demonstrated the subtle
changes of bio-factor entities' representations in the GP-GPT, which suggested
the opportunities for the application of LLMs to advancing gene-phenotype
research.

摘要：預先訓練好的大型語言模型 (LLM) 由於在自然語言處理方面取得成功，因此在生物醫學領域中備受關注。
然而，多來源基因組數據的複雜特徵和異質性在將這些模型應用於生物資訊學和生物醫學領域時，構成了重大挑戰。為了應對這些挑戰，我們提出了 GP-GPT，這是第一個用於遺傳表型知識表徵和基因組關係分析的專業大型語言模型。我們的模型分兩個階段進行微調，一個綜合語料庫包含超過 3,000,000 個基因組學、蛋白質組學和醫學遺傳學中的術語，這些術語來自多個經過驗證的大規模數據集和科學出版物。GP-GPT 顯示出準確擷取醫學遺傳學資訊和執行常見基因組分析任務（例如基因組資訊擷取和關係確定）的能力。跨領域特定任務的比較實驗顯示，GP-GPT 優於最先進的 LLM，包括 Llama2、Llama3 和 GPT-4。這些結果突出了 GP-GPT 在加強遺傳疾病關係研究以及促進基因組學和醫學遺傳學領域中準確而有效分析的潛力。我們的調查證明了 GP-GPT 中生物因子實體表徵的細微變化，這表明了將 LLM 應用於推進基因表型研究的機會。

##### **Causal Inference with Large Language Model: A Survey**
2409.09822v1 by Jing Ma

Causal inference has been a pivotal challenge across diverse domains such as
medicine and economics, demanding a complicated integration of human knowledge,
mathematical reasoning, and data mining capabilities. Recent advancements in
natural language processing (NLP), particularly with the advent of large
language models (LLMs), have introduced promising opportunities for traditional
causal inference tasks. This paper reviews recent progress in applying LLMs to
causal inference, encompassing various tasks spanning different levels of
causation. We summarize the main causal problems and approaches, and present a
comparison of their evaluation results in different causal scenarios.
Furthermore, we discuss key findings and outline directions for future
research, underscoring the potential implications of integrating LLMs in
advancing causal inference methodologies.

摘要：因果推論一直是各個領域的關鍵挑戰，例如醫學和經濟學，需要結合人類知識、數學推理和資料探勘能力。自然語言處理 (NLP) 的最新進展，特別是大語言模型 (LLM) 的出現，為傳統因果推論任務帶來了絕佳的機會。本文回顧了將 LLM 應用於因果推論的最新進展，涵蓋了不同因果層級的各種任務。我們總結了主要的因果問題和方法，並比較了它們在不同因果情境中的評估結果。此外，我們討論了關鍵發現，並概述了未來研究的方向，強調了整合 LLM 以促進因果推論方法的潛在影響。

##### **Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion**
2409.09808v1 by Hui Shen, Zhongwei Wan, Xin Wang, Mi Zhang

Mamba and Vision Mamba (Vim) models have shown their potential as an
alternative to methods based on Transformer architecture. This work introduces
Fast Mamba for Vision (Famba-V), a cross-layer token fusion technique to
enhance the training efficiency of Vim models. The key idea of Famba-V is to
identify and fuse similar tokens across different Vim layers based on a suit of
cross-layer strategies instead of simply applying token fusion uniformly across
all the layers that existing works propose. We evaluate the performance of
Famba-V on CIFAR-100. Our results show that Famba-V is able to enhance the
training efficiency of Vim models by reducing both training time and peak
memory usage during training. Moreover, the proposed cross-layer strategies
allow Famba-V to deliver superior accuracy-efficiency trade-offs. These results
all together demonstrate Famba-V as a promising efficiency enhancement
technique for Vim models.

摘要：Mamba 和 Vision Mamba (Vim) 模型已顯示其作為基於 Transformer 架構方法的替代方案的潛力。這項工作引入了 Fast Mamba for Vision (Famba-V)，一種跨層次標記融合技術，以增強 Vim 模型的訓練效率。Famba-V 的關鍵思想是根據跨層次策略組，識別並融合不同 Vim 層次之間的相似標記，而不是像現有工作所建議的那樣，僅將標記融合均勻地應用於所有層次。我們在 CIFAR-100 上評估 Famba-V 的效能。我們的結果顯示，Famba-V 能夠透過減少訓練時間和訓練期間的記憶體使用高峰，來增強 Vim 模型的訓練效率。此外，建議的跨層次策略允許 Famba-V 提供優越的準確度效率權衡。這些結果共同證明 Famba-V 是一種有前途的 Vim 模型效率增強技術。

##### **Multiple Rotation Averaging with Constrained Reweighting Deep Matrix Factorization**
2409.09790v1 by Shiqi Li, Jihua Zhu, Yifan Xie, Naiwen Hu, Mingchen Zhu, Zhongyu Li, Di Wang

Multiple rotation averaging plays a crucial role in computer vision and
robotics domains. The conventional optimization-based methods optimize a
nonlinear cost function based on certain noise assumptions, while most previous
learning-based methods require ground truth labels in the supervised training
process. Recognizing the handcrafted noise assumption may not be reasonable in
all real-world scenarios, this paper proposes an effective rotation averaging
method for mining data patterns in a learning manner while avoiding the
requirement of labels. Specifically, we apply deep matrix factorization to
directly solve the multiple rotation averaging problem in unconstrained linear
space. For deep matrix factorization, we design a neural network model, which
is explicitly low-rank and symmetric to better suit the background of multiple
rotation averaging. Meanwhile, we utilize a spanning tree-based edge filtering
to suppress the influence of rotation outliers. What's more, we also adopt a
reweighting scheme and dynamic depth selection strategy to further improve the
robustness. Our method synthesizes the merit of both optimization-based and
learning-based methods. Experimental results on various datasets validate the
effectiveness of our proposed method.

摘要：多重旋转平均在计算机视觉和机器人领域中扮演着至关重要的角色。传统的基于优化的方法优化了一个基于某些噪声假设的非线性成本函数，而大多数先前的基于学习的方法在监督训练过程中需要地面实况标签。认识到手工制作的噪声假设在所有实际场景中可能不合理，本文提出了一种有效的旋转平均方法，用于以学习的方式挖掘数据模式，同时避免了对标签的要求。具体来说，我们应用深度矩阵分解来直接解决无约束线性空间中的多重旋转平均问题。对于深度矩阵分解，我们设计了一个神经网络模型，它明确地是低秩且对称的，以更好地适应多重旋转平均的背景。同时，我们利用基于生成树的边缘滤波来抑制旋转异常值的影响。此外，我们还采用重新加权方案和动态深度选择策略来进一步提高鲁棒性。我们的方法综合了基于优化和基于学习的方法的优点。在各种数据集上的实验结果验证了我们提出的方法的有效性。

##### **Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models**
2409.09788v1 by Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna

Despite recent advances demonstrating vision-language models' (VLMs)
abilities to describe complex relationships in images using natural language,
their capability to quantitatively reason about object sizes and distances
remains underexplored. In this work, we introduce a manually annotated
benchmark, Q-Spatial Bench, with 271 questions across five categories designed
for quantitative spatial reasoning and systematically investigate the
performance of state-of-the-art VLMs on this task. Our analysis reveals that
reasoning about distances between objects is particularly challenging for SoTA
VLMs; however, some VLMs significantly outperform others, with an over 40-point
gap between the two best performing models. We also make the surprising
observation that the success rate of the top-performing VLM increases by 19
points when a reasoning path using a reference object emerges naturally in the
response. Inspired by this observation, we develop a zero-shot prompting
technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial
questions using reference objects as visual cues. By instructing VLMs to use
reference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro,
Gemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30
points, respectively. We emphasize that these significant improvements are
obtained without needing more data, model architectural modifications, or
fine-tuning.

摘要：儘管最近的進展展示了視覺語言模型 (VLM) 使用自然語言描述影像中複雜關係的能力，但它們對物件大小和距離進行量化推理的能力仍未被充分探討。在本文中，我們引入了一個手動註解基準 Q-Spatial Bench，其中包含 271 個問題，橫跨五個類別，專門用於量化空間推理，並系統性地研究了最先進的 VLM 在這項任務上的表現。我們的分析顯示，對物件之間距離的推理對 SoTA VLM 來說特別具有挑戰性；然而，一些 VLM 的表現明顯優於其他 VLM，表現最好的兩個模型之間的差距超過 40 分。我們還驚訝地觀察到，當在回應中自然出現使用參考物件的推理路徑時，表現最佳的 VLM 的成功率增加了 19 分。受到這一觀察的啟發，我們開發了一種零次提示技術 SpatialPrompt，它鼓勵 VLM 使用參考物件作為視覺提示來回答量化空間問題。通過 SpatialPrompt 指導 VLM 在其推理路徑中使用參考物件，Gemini 1.5 Pro、Gemini 1.5 Flash 和 GPT-4V 分別將其成功率提高了 40、20 和 30 分。我們強調，這些顯著的改進是在不需要更多資料、模型架構修改或微調的情況下獲得的。

##### **BEnDEM:A Boltzmann Sampler Based on Bootstrapped Denoising Energy Matching**
2409.09787v1 by RuiKang OuYang, Bo Qiang, José Miguel Hernández-Lobato

Developing an efficient sampler capable of generating independent and
identically distributed (IID) samples from a Boltzmann distribution is a
crucial challenge in scientific research, e.g. molecular dynamics. In this
work, we intend to learn neural samplers given energy functions instead of data
sampled from the Boltzmann distribution. By learning the energies of the noised
data, we propose a diffusion-based sampler, ENERGY-BASED DENOISING ENERGY
MATCHING, which theoretically has lower variance and more complexity compared
to related works. Furthermore, a novel bootstrapping technique is applied to
EnDEM to balance between bias and variance. We evaluate EnDEM and BEnDEM on a
2-dimensional 40 Gaussian Mixture Model (GMM) and a 4-particle double-welling
potential (DW-4). The experimental results demonstrate that BEnDEM can achieve
state-of-the-art performance while being more robust.

摘要：開發一個能從波茲曼分佈產生獨立且同分布 (IID) 樣本的高效取樣器，是科學研究（例如分子動力學）中的一項重要挑戰。在這項工作中，我們打算學習神經取樣器，給定能量函數，而不是從波茲曼分佈中取樣到的資料。透過學習雜訊資料的能量，我們提出一個基於擴散的取樣器，基於能量的去雜訊能量匹配，理論上與相關工作相比，具有較低的變異數和較高的複雜度。此外，將一種新穎的自舉技術應用於 EnDEM，以平衡偏差和變異。我們在 2 維 40 高斯混合模型 (GMM) 和 4 粒子雙阱位能 (DW-4) 上評估 EnDEM 和 BEnDEM。實驗結果表明，BEnDEM 可以實現最先進的效能，同時更具有穩健性。

##### **Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition**
2409.09785v2 by Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke

Given recent advances in generative AI technology, a key question is how
large language models (LLMs) can enhance acoustic modeling tasks using text
decoding results from a frozen, pretrained automatic speech recognition (ASR)
model. To explore new capabilities in language modeling for speech processing,
we introduce the generative speech transcription error correction (GenSEC)
challenge. This challenge comprises three post-ASR language modeling tasks: (i)
post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion
recognition. These tasks aim to emulate future LLM-based agents handling
voice-based interfaces while remaining accessible to a broad audience by
utilizing open pretrained language models or agent-based APIs. We also discuss
insights from baseline evaluations, as well as lessons learned for designing
future evaluations.

摘要：鉴于生成式 AI 技术的最新进展，一个关键问题是如何使用来自冻结的预训练自动语音识别 (ASR) 模型的文本解码结果来增强声学建模任务的大语言模型 (LLM)。为了探索语言建模在语音处理中的新功能，我们引入了生成式语音转录错误更正 (GenSEC) 挑战。此挑战包括三个 ASR 后语言建模任务：(i) ASR 后转录更正，(ii) 说话者标记，以及 (iii) 情绪识别。这些任务旨在模拟基于 LLM 的未来代理处理基于语音的界面，同时通过利用开放的预训练语言模型或基于代理的 API，让广泛的受众能够访问它们。我们还讨论了基准评估的见解，以及为设计未来的评估所吸取的教训。

##### **Enhancing Lesion Segmentation in PET/CT Imaging with Deep Learning and Advanced Data Preprocessing Techniques**
2409.09784v1 by Jiayi Liu, Qiaoyi Xue, Youdan Feng, Tianming Xu, Kaixin Shen, Chuyun Shen, Yuhang Shi

The escalating global cancer burden underscores the critical need for precise
diagnostic tools in oncology. This research employs deep learning to enhance
lesion segmentation in PET/CT imaging, utilizing a dataset of 900 whole-body
FDG-PET/CT and 600 PSMA-PET/CT studies from the AutoPET challenge III. Our
methodical approach includes robust preprocessing and data augmentation
techniques to ensure model robustness and generalizability. We investigate the
influence of non-zero normalization and modifications to the data augmentation
pipeline, such as the introduction of RandGaussianSharpen and adjustments to
the Gamma transform parameter. This study aims to contribute to the
standardization of preprocessing and augmentation strategies in PET/CT imaging,
potentially improving the diagnostic accuracy and the personalized management
of cancer patients. Our code will be open-sourced and available at
https://github.com/jiayiliu-pku/DC2024.

摘要：全球癌症负担不断加重，这凸显了肿瘤学中对精确诊断工具的关键需求。本研究采用深度学习来增强 PET/CT 成像中的病灶分割，利用来自 AutoPET 挑战 III 的 900 个全身 FDG-PET/CT 和 600 个 PSMA-PET/CT 研究的数据集。我们的方法论方法包括鲁棒的预处理和数据增强技术，以确保模型的鲁棒性和可推广性。我们研究了非零归一化和数据增强管道修改的影响，例如引入 RandGaussianSharpen 和调整 Gamma 转换参数。本研究旨在为 PET/CT 成像中的预处理和增强策略的标准化做出贡献，从而有可能提高癌症患者的诊断准确性和个性化管理。我们的代码将开源，可在 https://github.com/jiayiliu-pku/DC2024 获得。

##### **Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting**
2409.09766v1 by Qiaoyi Xue, Youdan Feng, Jiayi Liu, Tianming Xu, Kaixin Shen, Chuyun Shen, Yuhang Shi

This study explores a workflow for automated segmentation of lesions in FDG
and PSMA PET/CT images. Due to the substantial differences in image
characteristics between FDG and PSMA, specialized preprocessing steps are
required. Utilizing YOLOv8 for data classification, the FDG and PSMA images are
preprocessed separately before feeding them into the segmentation models,
aiming to improve lesion segmentation accuracy. The study focuses on evaluating
the performance of automated segmentation workflow for multitracer PET images.
The findings are expected to provide critical insights for enhancing diagnostic
workflows and patient-specific treatment plans. Our code will be open-sourced
and available at https://github.com/jiayiliu-pku/AP2024.

摘要：本研究探讨了 FDG 和 PSMA PET/CT 影像中病灶自动分割的工作流程。由于 FDG 和 PSMA 的影像特征存在很大差异，因此需要专门的预处理步骤。利用 YOLOv8 进行数据分类，在将 FDG 和 PSMA 影像输入分割模型之前对它们分别进行预处理，旨在提高病灶分割的准确性。本研究重点评估了多示踪剂 PET 影像自动分割工作流程的性能。研究结果有望为增强诊断工作流程和患者特定治疗计划提供关键见解。我们的代码将开源，可在 https://github.com/jiayiliu-pku/AP2024 获得。

##### **ELMI: Interactive and Intelligent Sign Language Translation of Lyrics for Song Signing**
2409.09760v1 by Suhyeon Yoo, Khai N. Truong, Young-Ho Kim

d/Deaf and hearing song-signers become prevalent on video-sharing platforms,
but translating songs into sign language remains cumbersome and inaccessible.
Our formative study revealed the challenges song-signers face, including
semantic, syntactic, expressive, and rhythmic considerations in translations.
We present ELMI, an accessible song-signing tool that assists in translating
lyrics into sign language. ELMI enables users to edit glosses line-by-line,
with real-time synced lyric highlighting and music video snippets. Users can
also chat with a large language model-driven AI to discuss meaning, glossing,
emoting, and timing. Through an exploratory study with 13 song-signers, we
examined how ELMI facilitates their workflows and how song-signers leverage and
receive an LLM-driven chat for translation. Participants successfully adopted
ELMI to song-signing, with active discussions on the fly. They also reported
improved confidence and independence in their translations, finding ELMI
encouraging, constructive, and informative. We discuss design implications for
leveraging LLMs in culturally sensitive song-signing translations.

摘要：在視訊分享平台上，聾人和聽人的手語歌曲表演者越來越普遍，
但將歌曲翻譯成手語仍然很繁瑣且難以取得。
我們的形成性研究揭示了手語歌曲表演者面臨的挑戰，包括
翻譯中的語義、句法、表達和節奏考量。
我們提出 ELMI，一個協助將歌詞翻譯成手語的無障礙手語歌曲工具。ELMI 使用者可以逐行編輯註釋，
並即時同步歌詞重點和音樂影片片段。使用者還可以
與大型語言模型驅動的 AI 聊天，討論意義、註釋、表達和時機。透過與 13 位手語歌曲表演者進行的探索性研究，我們
探討 ELMI 如何促進其工作流程，以及手語歌曲表演者如何利用和
接收 LLM 驅動的聊天進行翻譯。參與者成功採用
ELMI 進行手語歌曲表演，並即時進行積極討論。他們也報告
在翻譯中增加了信心和獨立性，發現 ELMI 具有鼓勵性、建設性和資訊性。我們討論了在文化敏感的手語歌曲翻譯中利用 LLM 的設計意涵。

##### **Explore the Hallucination on Low-level Perception for MLLMs**
2409.09748v1 by Yinan Sun, Zicheng Zhang, Haoning Wu, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min

The rapid development of Multi-modality Large Language Models (MLLMs) has
significantly influenced various aspects of industry and daily life, showcasing
impressive capabilities in visual perception and understanding. However, these
models also exhibit hallucinations, which limit their reliability as AI
systems, especially in tasks involving low-level visual perception and
understanding. We believe that hallucinations stem from a lack of explicit
self-awareness in these models, which directly impacts their overall
performance. In this paper, we aim to define and evaluate the self-awareness of
MLLMs in low-level visual perception and understanding tasks. To this end, we
present QL-Bench, a benchmark settings to simulate human responses to low-level
vision, investigating self-awareness in low-level visual perception through
visual question answering related to low-level attributes such as clarity and
lighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990
single images and 1,999 image pairs, each accompanied by an open-ended question
about its low-level features. Through the evaluation of 15 MLLMs, we
demonstrate that while some models exhibit robust low-level visual
capabilities, their self-awareness remains relatively underdeveloped. Notably,
for the same model, simpler questions are often answered more accurately than
complex ones. However, self-awareness appears to improve when addressing more
challenging questions. We hope that our benchmark will motivate further
research, particularly focused on enhancing the self-awareness of MLLMs in
tasks involving low-level visual perception and understanding.

摘要：多模态大型语言模型 (MLLM) 的快速发展已显着影响了产业和日常生活的各个方面，展示了在视觉感知和理解方面的惊人能力。然而，这些模型也会出现幻觉，这限制了它们作为人工智能系统的可靠性，尤其是在涉及低级视觉感知和理解的任务中。我们认为幻觉源于这些模型缺乏明确的自我意识，这直接影响了它们的整体性能。在本文中，我们旨在定义和评估 MLLM 在低级视觉感知和理解任务中的自我意识。为此，我们提出了 QL-Bench，这是一个基准设置，用于模拟人类对低级视觉的反应，通过与低级属性（如清晰度和光照）相关的视觉问题解答来调查低级视觉感知中的自我意识。具体来说，我们构建了 LLSAVisionQA 数据集，其中包含 2,990 张单张图像和 1,999 张图像对，每张图像或图像对都附带一个关于其低级特征的开放式问题。通过对 15 个 MLLM 的评估，我们证明了虽然一些模型表现出强大的低级视觉能力，但它们的自我意识仍然相对欠发达。值得注意的是，对于同一模型，简单的问题通常比复杂的问题回答得更准确。然而，在解决更具挑战性的问题时，自我意识似乎有所提高。我们希望我们的基准将激励进一步的研究，特别是专注于增强 MLLM 在涉及低级视觉感知和理解的任务中的自我意识。

##### **Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with Toxicity and Incivility Data**
2409.09741v1 by Bastián González-Bustamante

This article benchmarked the ability of OpenAI's GPTs and a number of
open-source LLMs to perform annotation tasks on political content. We used a
novel protest event dataset comprising more than three million digital
interactions and created a gold standard that includes ground-truth labels
annotated by human coders about toxicity and incivility on social media. We
included in our benchmark Google's Perspective algorithm, which, along with
GPTs, was employed throughout their respective APIs while the open-source LLMs
were deployed locally. The findings show that Perspective API using a laxer
threshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot
classification annotations. In addition, Nous Hermes 2 and Mistral OpenOrca,
with a smaller number of parameters, are able to perform the task with high
performance, being attractive options that could offer good trade-offs between
performance, implementing costs and computing time. Ancillary findings using
experiments setting different temperature levels show that although GPTs tend
to show not only excellent computing time but also overall good levels of
reliability, only open-source LLMs ensure full reproducibility in the
annotation.

摘要：本文比較了 OpenAI 的 GPT 和許多開源 LLM 在政治內容上執行標註任務的能力。我們使用了一個新穎的抗議事件資料集，其中包含超過三百萬筆數位互動，並建立了一個黃金標準，其中包含由人類編碼器標註的關於社群媒體上毒性和不文明行為的真實標籤。我們在基準中包含了 Google 的 Perspective 演算法，該演算法與 GPT 一起在其各自的 API 中使用，而開源 LLM 則是在本地部署。研究結果顯示，使用較寬鬆閾值的 Perspective API、GPT-4o 和 Nous Hermes 2 Mixtral 的表現優於其他 LLM 的零次分類標註。此外，參數較少的 Nous Hermes 2 和 Mistral OpenOrca 能夠以高性能執行任務，是具有吸引力的選項，可以在性能、實作成本和運算時間之間取得良好的平衡。使用設定不同溫度層級的實驗的輔助發現顯示，儘管 GPT 不僅展現出色的運算時間，而且整體可靠性也很好，但只有開源 LLM 能確保在標註中完全重現。

##### **PersonaMark: Personalized LLM watermarking for model protection and user attribution**
2409.09739v1 by Yuehan Zhang, Peizhuo Lv, Yinpeng Liu, Yongqiang Ma, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, Jiawei Liu

The rapid development of LLMs brings both convenience and potential threats.
As costumed and private LLMs are widely applied, model copyright protection has
become important. Text watermarking is emerging as a promising solution to
AI-generated text detection and model protection issues. However, current text
watermarks have largely ignored the critical need for injecting different
watermarks for different users, which could help attribute the watermark to a
specific individual. In this paper, we explore the personalized text
watermarking scheme for LLM copyright protection and other scenarios, ensuring
accountability and traceability in content generation. Specifically, we propose
a novel text watermarking method PersonaMark that utilizes sentence structure
as the hidden medium for the watermark information and optimizes the
sentence-level generation algorithm to minimize disruption to the model's
natural generation process. By employing a personalized hashing function to
inject unique watermark signals for different users, personalized watermarked
text can be obtained. Since our approach performs on sentence level instead of
token probability, the text quality is highly preserved. The injection process
of unique watermark signals for different users is time-efficient for a large
number of users with the designed multi-user hashing function. As far as we
know, we achieved personalized text watermarking for the first time through
this. We conduct an extensive evaluation of four different LLMs in terms of
perplexity, sentiment polarity, alignment, readability, etc. The results
demonstrate that our method maintains performance with minimal perturbation to
the model's behavior, allows for unbiased insertion of watermark information,
and exhibits strong watermark recognition capabilities.

摘要：大型語言模型的快速發展既帶來便利，也帶來潛在威脅。
隨著客製化和私人大型語言模型的廣泛應用，模型版權保護變得重要。文字浮水印作為解決 AI 生成的文字偵測和模型保護問題的潛在解決方案而出現。然而，目前的文字浮水印在很大程度上忽視了為不同的使用者注入不同浮水印的關鍵需求，這有助於將浮水印歸因於特定個人。在本文中，我們探討了大型語言模型版權保護和其他場景的個人化文字浮水印方案，確保內容生成的問責制和可追溯性。具體來說，我們提出了一種新穎的文字浮水印方法 PersonaMark，它利用句子結構作為浮水印資訊的隱藏媒介，並最佳化句子級別的生成演算法，以最大程度地減少對模型自然生成過程的干擾。通過使用個人化雜湊函數為不同的使用者注入唯一的浮水印信號，可以獲得個人化的浮水印文字。由於我們的做法是在句子級別而不是令牌機率上執行，因此文字品質得到了高度保留。通過設計的多使用者雜湊函數，為大量使用者注入唯一的浮水印信號的過程是時間有效的。據我們所知，我們首次透過此方式實現了個人化文字浮水印。我們對四種不同的大型語言模型進行了廣泛的評估，包括困惑度、情緒極性、對齊、可讀性等。結果表明，我們的模型在對模型行為的最小擾動下保持性能，允許無偏差地插入浮水印資訊，並展現出強大的浮水印識別能力。

##### **From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare**
2409.09727v1 by Ming Li, Pengcheng Xu, Junjie Hu, Zeyu Tang, Guang Yang

Federated learning holds great potential for enabling large-scale healthcare
research and collaboration across multiple centres while ensuring data privacy
and security are not compromised. Although numerous recent studies suggest or
utilize federated learning based methods in healthcare, it remains unclear
which ones have potential clinical utility. This review paper considers and
analyzes the most recent studies up to May 2024 that describe federated
learning based methods in healthcare. After a thorough review, we find that the
vast majority are not appropriate for clinical use due to their methodological
flaws and/or underlying biases which include but are not limited to privacy
concerns, generalization issues, and communication costs. As a result, the
effectiveness of federated learning in healthcare is significantly compromised.
To overcome these challenges, we provide recommendations and promising
opportunities that might be implemented to resolve these problems and improve
the quality of model development in federated learning with healthcare.

摘要：聯邦學習在確保資料隱私和安全不致受損的情況下，為大型醫療保健研究和跨多個中心合作提供了巨大潛力。儘管許多最近的研究建議或利用基於聯邦學習的方法進行醫療保健，但哪些具有潛在的臨床效用仍不清楚。本評論文章考慮並分析了截至 2024 年 5 月描述基於聯邦學習方法的醫療保健的最新研究。在徹底檢閱後，我們發現絕大多數不適合臨床使用，因為它們存在方法論缺陷和/或潛在偏差，包括但不限於隱私問題、概化問題和通訊成本。因此，聯邦學習在醫療保健中的效力受到顯著影響。為了克服這些挑戰，我們提供了建議和有希望的機會，這些機會可能會被實施以解決這些問題並提高醫療保健中聯邦學習模型開發的品質。

##### **Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents**
2409.09717v1 by Justas Andriuškevičius, Junzi Sun

Recent developments in language models have created new opportunities in air
traffic control studies. The current focus is primarily on text and
language-based use cases. However, these language models may offer a higher
potential impact in the air traffic control domain, thanks to their ability to
interact with air traffic environments in an embodied agent form. They also
provide a language-like reasoning capability to explain their decisions, which
has been a significant roadblock for the implementation of automatic air
traffic control.
  This paper investigates the application of a language model-based agent with
function-calling and learning capabilities to resolve air traffic conflicts
without human intervention. The main components of this research are
foundational large language models, tools that allow the agent to interact with
the simulator, and a new concept, the experience library. An innovative part of
this research, the experience library, is a vector database that stores
synthesized knowledge that agents have learned from interactions with the
simulations and language models.
  To evaluate the performance of our language model-based agent, both
open-source and closed-source models were tested. The results of our study
reveal significant differences in performance across various configurations of
the language model-based agents. The best-performing configuration was able to
solve almost all 120 but one imminent conflict scenarios, including up to four
aircraft at the same time. Most importantly, the agents are able to provide
human-level text explanations on traffic situations and conflict resolution
strategies.

摘要：<paragraph>語言模型的最新發展在空中交通管制研究中創造了新的機會。目前的重點主要在於基於文字和語言的用例。但是，由於這些語言模型能夠以具體的代理形式與空中交通環境互動，因此它們在空中交通管制領域可能具有更高的潛在影響。它們還提供了類似語言的推理能力來解釋其決策，這一直是實施自動空中交通管制的重大障礙。
  本文探討了應用具備函數呼叫和學習能力的基於語言模型的代理來解決空中交通衝突，而無需人工干預。本研究的主要組成部分是基礎大型語言模型、允許代理與模擬器互動的工具，以及一個新概念，即體驗庫。本研究的創新部分，體驗庫，是一個向量資料庫，它儲存了代理從與模擬和語言模型的互動中學到的綜合知識。
  為了評估我們基於語言模型的代理的效能，我們測試了開源和閉源模型。我們研究的結果顯示，基於語言模型的代理的各種配置在效能上存在顯著差異。效能最佳的配置能夠解決幾乎所有 120 個衝突情境（但有一個除外），包括同時處理多達四架飛機。最重要的是，這些代理能夠對交通狀況和衝突解決策略提供人類層級的文字解釋。</paragraph>

##### **AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs**
2409.09704v1 by Madhusudan Ghosh, Shrimon Mukherjee, Asmit Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar, Debasis Ganguly

In recent years, there has been a surge in the publication of clinical trial
reports, making it challenging to conduct systematic reviews. Automatically
extracting Population, Intervention, Comparator, and Outcome (PICO) from
clinical trial studies can alleviate the traditionally time-consuming process
of manually scrutinizing systematic reviews. Existing approaches of PICO frame
extraction involves supervised approach that relies on the existence of
manually annotated data points in the form of BIO label tagging. Recent
approaches, such as In-Context Learning (ICL), which has been shown to be
effective for a number of downstream NLP tasks, require the use of labeled
examples. In this work, we adopt ICL strategy by employing the pretrained
knowledge of Large Language Models (LLMs), gathered during the pretraining
phase of an LLM, to automatically extract the PICO-related terminologies from
clinical trial documents in unsupervised set up to bypass the availability of
large number of annotated data instances. Additionally, to showcase the highest
effectiveness of LLM in oracle scenario where large number of annotated samples
are available, we adopt the instruction tuning strategy by employing Low Rank
Adaptation (LORA) to conduct the training of gigantic model in low resource
environment for the PICO frame extraction task. Our empirical results show that
our proposed ICL-based framework produces comparable results on all the version
of EBM-NLP datasets and the proposed instruction tuned version of our framework
produces state-of-the-art results on all the different EBM-NLP datasets. Our
project is available at \url{https://github.com/shrimonmuke0202/AlpaPICO.git}.

摘要：<paragraph>近年來，臨床試驗報告的出版數量激增，這使得進行系統性回顧具有挑戰性。自動從臨床試驗研究中提取人群、干預、比較和結果 (PICO) 可以減輕傳統上耗時的系統性回顧手動審查過程。現有的 PICO 框架提取方法涉及依賴以 BIO 標籤標記形式存在的手動註解數據點的監督式方法。最近的方法，例如語境學習 (ICL)，已被證明對許多下游 NLP 任務有效，需要使用標記範例。在這項工作中，我們採用 ICL 策略，利用大型語言模型 (LLM) 的預訓練知識，在 LLM 的預訓練階段收集，以在無監督設置中自動從臨床試驗文件中提取與 PICO 相關的術語，以繞過大量註解數據實例的可用性。此外，為了展示 LLM 在預言機場景中的最高效率，其中有大量註解樣本可用，我們採用指令調整策略，採用低秩適應 (LORA) 來在低資源環境中對大型模型進行訓練，以進行 PICO 框架提取任務。我們的實證結果表明，我們提出的基於 ICL 的框架在所有版本的 EBM-NLP 數據集上產生了可比較的結果，而我們框架提出的指令調整版本在所有不同的 EBM-NLP 數據集上產生了最先進的結果。我們的項目可在以下網址獲得：\url{https://github.com/shrimonmuke0202/AlpaPICO.git}。</paragraph>

##### **GFlowNet Pretraining with Inexpensive Rewards**
2409.09702v1 by Mohit Pandey, Gopeshh Subbaraj, Emmanuel Bengio

Generative Flow Networks (GFlowNets), a class of generative models have
recently emerged as a suitable framework for generating diverse and
high-quality molecular structures by learning from unnormalized reward
distributions. Previous works in this direction often restrict exploration by
using predefined molecular fragments as building blocks, limiting the chemical
space that can be accessed. In this work, we introduce Atomic GFlowNets
(A-GFNs), a foundational generative model leveraging individual atoms as
building blocks to explore drug-like chemical space more comprehensively. We
propose an unsupervised pre-training approach using offline drug-like molecule
datasets, which conditions A-GFNs on inexpensive yet informative molecular
descriptors such as drug-likeliness, topological polar surface area, and
synthetic accessibility scores. These properties serve as proxy rewards,
guiding A-GFNs towards regions of chemical space that exhibit desirable
pharmacological properties. We further our method by implementing a
goal-conditioned fine-tuning process, which adapts A-GFNs to optimize for
specific target properties. In this work, we pretrain A-GFN on the ZINC15
offline dataset and employ robust evaluation metrics to show the effectiveness
of our approach when compared to other relevant baseline methods in drug
design.

摘要：生成式流動網路 (GFlowNets) 是一類生成式模型，最近已成為生成多樣化且高品質分子結構的合適框架，方法是從未正規化的獎勵分佈中學習。此方向先前的研究通常透過使用預定義的分子片段作為建構區塊來限制探索，這限制了可存取的化學空間。在這項研究中，我們引入了原子生成式流動網路 (A-GFN)，這是一個基礎生成式模型，利用個別原子作為建構區塊，以更全面地探索類藥物化學空間。我們提出了一種使用離線類藥物分子資料集的無監督預訓練方法，該方法以低成本但有資訊性的分子描述符（例如類藥性、拓撲極性表面積和合成可及性分數）為條件，對 A-GFN 進行條件設定。這些屬性作為代理獎勵，引導 A-GFN 朝向展現理想藥理特性的化學空間區域。我們透過實作目標條件微調程序進一步改進方法，該程序調整 A-GFN 以最佳化特定目標屬性。在這項研究中，我們在 ZINC15 離線資料集上預訓練 A-GFN，並採用穩健的評估指標，在與其他相關基線方法進行比較時，顯示我們的方法的有效性在藥物設計中。

##### **Anatomy of Machines for Markowitz: Decision-Focused Learning for Mean-Variance Portfolio Optimization**
2409.09684v1 by Junhyeong Lee, Inwoo Tae, Yongjae Lee

Markowitz laid the foundation of portfolio theory through the mean-variance
optimization (MVO) framework. However, the effectiveness of MVO is contingent
on the precise estimation of expected returns, variances, and covariances of
asset returns, which are typically uncertain. Machine learning models are
becoming useful in estimating uncertain parameters, and such models are trained
to minimize prediction errors, such as mean squared errors (MSE), which treat
prediction errors uniformly across assets. Recent studies have pointed out that
this approach would lead to suboptimal decisions and proposed Decision-Focused
Learning (DFL) as a solution, integrating prediction and optimization to
improve decision-making outcomes. While studies have shown DFL's potential to
enhance portfolio performance, the detailed mechanisms of how DFL modifies
prediction models for MVO remain unexplored. This study aims to investigate how
DFL adjusts stock return prediction models to optimize decisions in MVO,
addressing the question: "MSE treats the errors of all assets equally, but how
does DFL reduce errors of different assets differently?" Answering this will
provide crucial insights into optimal stock return prediction for constructing
efficient portfolios.

摘要：馬可維茲透過平均數-變異數最佳化（MVO）架構奠定了投資組合理論的基礎。然而，MVO 的有效性取決於預期報酬、變異數和資產報酬的共變異數的精確估計，而這些通常是不確定的。機器學習模型在估計不確定的參數方面變得有用，並且這些模型經過訓練以最小化預測誤差，例如均方誤差（MSE），均方誤差對所有資產的預測誤差進行統一處理。最近的研究指出，這種方法會導致次優決策，並提出以決策為中心的學習（DFL）作為解決方案，整合預測和最佳化以改善決策制定成果。雖然研究已顯示 DFL 改善投資組合績效的潛力，但 DFL 如何修改 MVO 的預測模型的詳細機制仍未探討。本研究旨在探討 DFL 如何調整股票報酬預測模型以最佳化 MVO 中的決策，探討這個問題：「MSE 以相同的方式處理所有資產的誤差，但 DFL 如何以不同的方式減少不同資產的誤差？」回答這個問題將提供對最佳股票報酬預測的關鍵見解，以建構有效的投資組合。

##### **ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models**
2409.09662v2 by Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim

Expressing stressful experiences in words is proven to improve mental and
physical health, but individuals often disengage with writing interventions as
they struggle to organize their thoughts and emotions. Reflective prompts have
been used to provide direction, and large language models (LLMs) have
demonstrated the potential to provide tailored guidance. Current systems often
limit users' flexibility to direct their reflections. We thus present
ExploreSelf, an LLM-driven application designed to empower users to control
their reflective journey. ExploreSelf allows users to receive adaptive support
through dynamically generated questions. Through an exploratory study with 19
participants, we examine how participants explore and reflect on personal
challenges using ExploreSelf. Our findings demonstrate that participants valued
the balance between guided support and freedom to control their reflective
journey, leading to deeper engagement and insight. Building on our findings, we
discuss implications for designing LLM-driven tools that promote user
empowerment through effective reflective practices.

摘要：已證實用言語表達壓力經驗有助於改善心理和身體健康，但個人常常放棄寫作介入，因為他們在整理思緒和情緒時會遇到困難。反思提示已被用來提供方向，而大型語言模型 (LLM) 已證明有提供客製化指導的潛力。目前的系統通常會限制使用者引導其反思的靈活性。因此，我們提出了 ExploreSelf，這是一個由 LLM 驅動的應用程式，旨在授權使用者控制其反思旅程。ExploreSelf 允許使用者透過動態產生的問題來接收適應性支援。透過一項與 19 位參與者進行的探索性研究，我們探討參與者如何使用 ExploreSelf 來探索和反思個人挑戰。我們的研究結果表明，參與者重視引導式支援與控制其反思旅程的自由之間的平衡，這會帶來更深入的參與和洞察力。根據我們的研究結果，我們討論了設計 LLM 驅動工具的含意，這些工具透過有效的反思實務促進使用者賦權。

##### **Leveraging Open-Source Large Language Models for Native Language Identification**
2409.09659v1 by Yee Man Ng, Ilia Markov

Native Language Identification (NLI) - the task of identifying the native
language (L1) of a person based on their writing in the second language (L2) -
has applications in forensics, marketing, and second language acquisition.
Historically, conventional machine learning approaches that heavily rely on
extensive feature engineering have outperformed transformer-based language
models on this task. Recently, closed-source generative large language models
(LLMs), e.g., GPT-4, have demonstrated remarkable performance on NLI in a
zero-shot setting, including promising results in open-set classification.
However, closed-source LLMs have many disadvantages, such as high costs and
undisclosed nature of training data. This study explores the potential of using
open-source LLMs for NLI. Our results indicate that open-source LLMs do not
reach the accuracy levels of closed-source LLMs when used out-of-the-box.
However, when fine-tuned on labeled training data, open-source LLMs can achieve
performance comparable to that of commercial LLMs.

摘要：母語識別 (NLI) - 根據個人以第二語言 (L2) 書寫的內容來識別其母語 (L1) 的任務 - 在法醫學、行銷和第二語言習得方面有應用。
從歷史上看，高度依賴於廣泛特徵工程的傳統機器學習方法在此任務上優於基於轉換器的語言模型。最近，閉源生成式大語言模型 (LLM)，例如 GPT-4，在零次學習設置中展示了 NLI 的卓越性能，包括在開放集分類中取得的令人滿意的結果。
然而，閉源 LLM 有許多缺點，例如高成本和訓練資料的未公開性質。本研究探討了使用開源 LLM 進行 NLI 的可能性。我們的結果表明，開源 LLM 在開箱即用時無法達到閉源 LLM 的準確度等級。
然而，在標記訓練資料上進行微調時，開源 LLM 可以實現與商業 LLM 相當的性能。

##### **Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example**
2409.09652v1 by Yuanning Huang

This paper investigates gender bias in Large Language Model (LLM)-generated
teacher evaluations in higher education setting, focusing on evaluations
produced by GPT-4 across six academic subjects. By applying a comprehensive
analytical framework that includes Odds Ratio (OR) analysis, Word Embedding
Association Test (WEAT), sentiment analysis, and contextual analysis, this
paper identified patterns of gender-associated language reflecting societal
stereotypes. Specifically, words related to approachability and support were
used more frequently for female instructors, while words related to
entertainment were predominantly used for male instructors, aligning with the
concepts of communal and agentic behaviors. The study also found moderate to
strong associations between male salient adjectives and male names, though
career and family words did not distinctly capture gender biases. These
findings align with prior research on societal norms and stereotypes,
reinforcing the notion that LLM-generated text reflects existing biases.

摘要：本研究探討大型語言模型 (LLM) 在高等教育環境中產生的教師評量中的性別偏見，重點關注 GPT-4 在六個學術科目中產生的評量。透過應用包含機率比 (OR) 分析、詞嵌入關聯測試 (WEAT)、情緒分析和情境分析的全面分析架構，本研究找出反映社會刻板印象的性別相關語言模式。具體來說，與女性教師相關的詞彙較常使用親切和支持，而與男性教師相關的詞彙則主要使用娛樂，這與群體和能動行為的概念一致。研究還發現男性顯著形容詞和男性姓名之間有中度到強烈的關聯，儘管職業和家庭詞彙並未明確捕捉到性別偏見。這些發現與先前關於社會規範和刻板印象的研究一致，強化了 LLM 生成的文字反映既有偏見的概念。

##### **Self-supervised Learning for Acoustic Few-Shot Classification**
2409.09647v1 by Jingyong Liang, Bernd Meyer, Issac Ning Lee, Thanh-Toan Do

Labelled data are limited and self-supervised learning is one of the most
important approaches for reducing labelling requirements. While it has been
extensively explored in the image domain, it has so far not received the same
amount of attention in the acoustic domain. Yet, reducing labelling is a key
requirement for many acoustic applications. Specifically in bioacoustic, there
are rarely sufficient labels for fully supervised learning available. This has
led to the widespread use of acoustic recognisers that have been pre-trained on
unrelated data for bioacoustic tasks. We posit that training on the actual task
data and combining self-supervised pre-training with few-shot classification is
a superior approach that has the ability to deliver high accuracy even when
only a few labels are available. To this end, we introduce and evaluate a new
architecture that combines CNN-based preprocessing with feature extraction
based on state space models (SSMs). This combination is motivated by the fact
that CNN-based networks alone struggle to capture temporal information
effectively, which is crucial for classifying acoustic signals. SSMs,
specifically S4 and Mamba, on the other hand, have been shown to have an
excellent ability to capture long-range dependencies in sequence data. We
pre-train this architecture using contrastive learning on the actual task data
and subsequent fine-tuning with an extremely small amount of labelled data. We
evaluate the performance of this proposed architecture for ($n$-shot,
$n$-class) classification on standard benchmarks as well as real-world data.
Our evaluation shows that it outperforms state-of-the-art architectures on the
few-shot classification problem.

摘要：<paragraph>標籤資料有限，而自監督學習是減少標籤需求最重要的途徑之一。雖然它已在影像領域廣泛探索，但到目前為止，在聲學領域尚未獲得相同的關注。然而，減少標籤是許多聲學應用的一項關鍵需求。特別是在生物聲學中，很少有足夠的標籤可供完全監督學習使用。這導致廣泛使用聲學辨識器，這些辨識器已針對生物聲學任務在不相關資料上進行預訓練。我們假設在實際任務資料上進行訓練，並將自監督預訓練與少樣本分類相結合，是一種優越的方法，即使只有少數標籤可用，也能提供高準確度。為此，我們引入並評估了一種新的架構，它結合了基於 CNN 的預處理與基於狀態空間模型 (SSM) 的特徵萃取。這種結合的動機在於，僅基於 CNN 的網路難以有效擷取時間資訊，而這對於分類聲學訊號至關重要。另一方面，SSM，特別是 S4 和 Mamba，已被證明具有擷取序列資料中長程依賴性的出色能力。我們使用對比學習在實際任務資料上預訓練此架構，並隨後使用極少量標籤資料進行微調。我們評估了此提議架構在標準基準測試和真實世界資料上的 ($n$-樣本，$n$-類別) 分類效能。我們的評估顯示，它在少樣本分類問題上優於最先進的架構。</paragraph>

##### **A Simple HMM with Self-Supervised Representations for Phone Segmentation**
2409.09646v1 by Gene-Ping Yang, Hao Tang

Despite the recent advance in self-supervised representations, unsupervised
phonetic segmentation remains challenging. Most approaches focus on improving
phonetic representations with self-supervised learning, with the hope that the
improvement can transfer to phonetic segmentation. In this paper, contrary to
recent approaches, we show that peak detection on Mel spectrograms is a strong
baseline, better than many self-supervised approaches. Based on this finding,
we propose a simple hidden Markov model that uses self-supervised
representations and features at the boundaries for phone segmentation. Our
results demonstrate consistent improvements over previous approaches, with a
generalized formulation allowing versatile design adaptations.

摘要：儘管自監督表徵最近有進展，但非監督音標分段仍然具有挑戰性。大多數方法都專注於透過自監督學習來改善音標表徵，並希望改善的部分能夠轉移到音標分段。在本文中，與最近的方法相反，我們證明了梅爾頻譜圖上的峰值檢測是一個強大的基準，優於許多自監督方法。根據這一發現，我們提出了一個簡單的隱藏馬可夫模型，該模型使用自監督表徵和邊界的特徵進行音標分段。我們的結果證明了與以前的方法相比，始終如一地改進，並具有允許通用設計適應的廣義公式。

##### **Towards understanding evolution of science through language model series**
2409.09636v1 by Junjie Dong, Zhuoqi Lyu, Qing Ke

We introduce AnnualBERT, a series of language models designed specifically to
capture the temporal evolution of scientific text. Deviating from the
prevailing paradigms of subword tokenizations and "one model to rule them all",
AnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model
pretrained from scratch on the full-text of 1.7 million arXiv papers published
until 2008 and a collection of progressively trained models on arXiv papers at
an annual basis. We demonstrate the effectiveness of AnnualBERT models by
showing that they not only have comparable performances in standard tasks but
also achieve state-of-the-art performances on domain-specific NLP tasks as well
as link prediction tasks in the arXiv citation network. We then utilize probing
tasks to quantify the models' behavior in terms of representation learning and
forgetting as time progresses. Our approach enables the pretrained models to
not only improve performances on scientific text processing tasks but also to
provide insights into the development of scientific discourse over time. The
series of the models is available at https://huggingface.co/jd445/AnnualBERTs.

摘要：我們介紹 AnnualBERT，這是一系列專門設計用於捕捉科學文本時間演化的語言模型。偏離分詞標記和「一個模型統治所有」的主流模式，AnnualBERT 採用整個單詞作為標記，並由一個基礎 RoBERTa 模型組成，該模型從頭開始預訓練在 2008 年之前發表的 170 萬篇 arXiv 全文上，以及每年在 arXiv 論文上訓練的一系列模型。我們通過展示 AnnualBERT 模型不僅在標準任務中具有可比的性能，而且在特定領域的 NLP 任務以及 arXiv 引文網路中的連結預測任務中也取得了最先進的性能，證明了 AnnualBERT 模型的有效性。然後，我們利用探測任務來量化模型在表示學習和隨著時間推移而遺忘方面的行為。我們的做法使預訓練模型不僅可以改善科學文本處理任務的性能，還可以深入了解科學論述的發展。該系列模型可在 https://huggingface.co/jd445/AnnualBERTs 中獲得。

##### **Confidence Estimation for LLM-Based Dialogue State Tracking**
2409.09629v1 by Yi-Jyun Sun, Suvodip Dey, Dilek Hakkani-Tur, Gokhan Tur

Estimation of a model's confidence on its outputs is critical for
Conversational AI systems based on large language models (LLMs), especially for
reducing hallucination and preventing over-reliance. In this work, we provide
an exhaustive exploration of methods, including approaches proposed for open-
and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty
to improve the reliability of LLM-generated responses, specifically focusing on
dialogue state tracking (DST) in task-oriented dialogue systems (TODS).
Regardless of the model type, well-calibrated confidence scores are essential
to handle uncertainties, thereby improving model performance. We evaluate four
methods for estimating confidence scores based on softmax, raw token scores,
verbalized confidences, and a combination of these methods, using the area
under the curve (AUC) metric to assess calibration, with higher AUC indicating
better calibration. We also enhance these with a self-probing mechanism,
proposed for closed models. Furthermore, we assess these methods using an
open-weight model fine-tuned for the task of DST, achieving superior joint goal
accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can
result in enhanced AUC performance, indicating better confidence score
calibration.

摘要：對模型輸出結果的信心估計對於基於大型語言模型 (LLM) 的對話式 AI 系統至關重要，特別是在減少幻覺和防止過度依賴方面。在這項工作中，我們提供了方法的詳盡探討，包括針對開放和封閉權重 LLM 提出方法，旨在量化和利用模型不確定性，以提高 LLM 生成的回應的可靠性，特別關注任務導向對話系統 (TODS) 中的對話狀態追蹤 (DST)。無論模型類型如何，校準良好的信心分數對於處理不確定性至關重要，從而提高模型效能。我們評估了四種基於 softmax、原始符號分數、口頭表達的信心以及這些方法的組合來估計信心分數的方法，使用曲線下面積 (AUC) 指標來評估校準，AUC 越高表示校準越好。我們還使用針對封閉模型提出的自探測機制來增強這些方法。此外，我們使用針對 DST 任務進行微調的開放權重模型來評估這些方法，並獲得了出色的聯合目標準確度 (JGA)。我們的研究結果還表明，微調開放權重 LLM 可以提高 AUC 效能，這表示更好的信心分數校準。

##### **Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot Event-based Recognition**
2409.09628v1 by Zongyou Yu, Qiang Qu, Xiaoming Chen, Chen Wang

Recent advancements in event-based zero-shot object recognition have
demonstrated promising results. However, these methods heavily depend on
extensive training and are inherently constrained by the characteristics of
CLIP. To the best of our knowledge, this research is the first study to explore
the understanding capabilities of large language models (LLMs) for event-based
visual content. We demonstrate that LLMs can achieve event-based object
recognition without additional training or fine-tuning in conjunction with
CLIP, effectively enabling pure zero-shot event-based recognition.
Particularly, we evaluate the ability of GPT-4o / 4turbo and two other
open-source LLMs to directly recognize event-based visual content. Extensive
experiments are conducted across three benchmark datasets, systematically
assessing the recognition accuracy of these models. The results show that LLMs,
especially when enhanced with well-designed prompts, significantly improve
event-based zero-shot recognition performance. Notably, GPT-4o outperforms the
compared models and exceeds the recognition accuracy of state-of-the-art
event-based zero-shot methods on N-ImageNet by five orders of magnitude. The
implementation of this paper is available at
\url{https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM}.

摘要：最近在基于事件的零样本物体识别方面取得的进展展示了有希望的结果。然而，这些方法严重依赖于广泛的训练，并且本质上受到 CLIP 特性的限制。据我们所知，这项研究是第一个探索大型语言模型 (LLM) 对基于事件的视觉内容的理解能力的研究。我们证明，LLM 可以实现基于事件的物体识别，而无需额外的训练或微调，同时结合 CLIP，从而有效地实现纯零样本基于事件的识别。特别是，我们评估了 GPT-4o / 4turbo 和其他两个开源 LLM 直接识别基于事件的视觉内容的能力。在三个基准数据集上进行了广泛的实验，系统地评估了这些模型的识别准确性。结果表明，LLM，尤其是在经过精心设计的提示增强后，显着提高了基于事件的零样本识别性能。值得注意的是，GPT-4o 优于比较模型，并且在 N-ImageNet 上超过了最先进的基于事件的零样本方法的识别准确性五个数量级。本文的实现可在 \url{https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM} 中找到。

##### **Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics**
2409.09626v1 by Yi Ren, Danica J. Sutherland

Obtaining compositional mappings is important for the model to generalize
well compositionally. To better understand when and how to encourage the model
to learn such mappings, we study their uniqueness through different
perspectives. Specifically, we first show that the compositional mappings are
the simplest bijections through the lens of coding length (i.e., an upper bound
of their Kolmogorov complexity). This property explains why models having such
mappings can generalize well. We further show that the simplicity bias is
usually an intrinsic property of neural network training via gradient descent.
That partially explains why some models spontaneously generalize well when they
are trained appropriately.

摘要：取得組成對應對於模型在組成上推廣至關重要。為了更好地理解模型何時以及如何學習此類對應，我們從不同角度研究其獨特性。具體來說，我們首先表明組成對應是通過編碼長度（即，Kolmogorov 複雜度的上限）透鏡的最簡單的雙射。此屬性解釋了為什麼具有此類對應的模型可以很好地推廣。我們進一步表明，簡單性偏差通常是通過梯度下降進行神經網絡訓練的固有屬性。這部分解釋了為什麼某些模型在適當訓練時會自發地很好地推廣。

