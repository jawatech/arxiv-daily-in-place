{"2410.18640": {"publish_time": "2024-10-24", "title": "Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model", "paper_summary": "Aligning language models (LMs) with human preferences has become a key area\nof research, enabling these models to meet diverse user needs better. Inspired\nby weak-to-strong generalization, where a strong LM fine-tuned on labels\ngenerated by a weaker model can consistently outperform its weak supervisor, we\nextend this idea to model alignment. In this work, we observe that the\nalignment behavior in weaker models can be effectively transferred to stronger\nmodels and even exhibit an amplification effect. Based on this insight, we\npropose a method called Weak-to-Strong Preference Optimization (WSPO), which\nachieves strong model alignment by learning the distribution differences before\nand after the alignment of the weak model. Experiments demonstrate that WSPO\ndelivers outstanding performance, improving the win rate of Qwen2-7B-Instruct\non Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04\nlength-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our\nresults suggest that using the weak model to elicit a strong model with a high\nalignment ability is feasible.", "paper_summary_zh": "\u5c07\u8a9e\u8a00\u6a21\u578b (LM) \u8207\u4eba\u985e\u504f\u597d\u76f8\u7b26\u5df2\u6210\u70ba\u7814\u7a76\u7684\u4e00\u5927\u91cd\u9ede\uff0c\u8b93\u9019\u4e9b\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6eff\u8db3\u4e0d\u540c\u7684\u4f7f\u7528\u8005\u9700\u6c42\u3002\u5728\u5f31\u5230\u5f37\u7684\u6982\u5316\u555f\u767c\u4e0b\uff0c\u4e00\u500b\u91dd\u5c0d\u7531\u8f03\u5f31\u6a21\u578b\u7522\u751f\u7684\u6a19\u7c64\u9032\u884c\u5fae\u8abf\u7684\u5f37\u5927 LM\uff0c\u80fd\u6301\u7e8c\u512a\u65bc\u5176\u8f03\u5f31\u7684\u76e3\u7763\u8005\uff0c\u6211\u5011\u5c07\u6b64\u6982\u5ff5\u5ef6\u4f38\u5230\u6a21\u578b\u5c0d\u9f4a\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u8f03\u5f31\u6a21\u578b\u4e2d\u7684\u5c0d\u9f4a\u884c\u70ba\u53ef\u4ee5\u6709\u6548\u5730\u8f49\u79fb\u5230\u8f03\u5f37\u7684\u6a21\u578b\uff0c\u751a\u81f3\u5c55\u73fe\u51fa\u653e\u5927\u6548\u679c\u3002\u57fa\u65bc\u6b64\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u540d\u70ba\u5f31\u5230\u5f37\u504f\u597d\u6700\u4f73\u5316 (WSPO) \u7684\u65b9\u6cd5\uff0c\u900f\u904e\u5728\u5f31\u6a21\u578b\u5c0d\u9f4a\u524d\u5f8c\u5b78\u7fd2\u5206\u4f48\u5dee\u7570\uff0c\u9054\u6210\u5f37\u5927\u7684\u6a21\u578b\u5c0d\u9f4a\u3002\u5be6\u9a57\u8b49\u660e WSPO \u80fd\u63d0\u4f9b\u5091\u51fa\u7684\u6548\u80fd\uff0c\u5c07 Qwen2-7B-Instruct \u5728 Arena-Hard \u4e0a\u7684\u7372\u52dd\u7387\u5f9e 39.70 \u63d0\u5347\u81f3 49.60\uff0c\u5728 AlpacaEval 2 \u4e0a\u9054\u5230\u986f\u8457\u7684 47.04 \u9577\u5ea6\u63a7\u5236\u7372\u52dd\u7387\uff0c\u4e26\u5728 MT-bench \u4e0a\u7372\u5f97 7.33 \u5206\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u4f7f\u7528\u5f31\u6a21\u578b\u4f86\u5f15\u51fa\u5177\u6709\u9ad8\u5ea6\u5c0d\u9f4a\u80fd\u529b\u7684\u5f37\u5927\u6a21\u578b\u662f\u53ef\u884c\u7684\u3002", "author": "Wenhong Zhu et.al.", "authors": "Wenhong Zhu, Zhiwei He, Xiaofeng Wang, Pengfei Liu, Rui Wang", "id": "2410.18640v1", "paper_url": "http://arxiv.org/abs/2410.18640v1", "repo": "null"}}