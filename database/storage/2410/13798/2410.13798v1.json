{"2410.13798": {"publish_time": "2024-10-17", "title": "Learning Graph Quantized Tokenizers for Transformers", "paper_summary": "Transformers serve as the backbone architectures of Foundational Models,\nwhere a domain-specific tokenizer helps them adapt to various domains. Graph\nTransformers (GTs) have recently emerged as a leading model in geometric deep\nlearning, outperforming Graph Neural Networks (GNNs) in various graph learning\ntasks. However, the development of tokenizers for graphs has lagged behind\nother modalities, with existing approaches relying on heuristics or GNNs\nco-trained with Transformers. To address this, we introduce GQT (\\textbf{G}raph\n\\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from\nTransformer training by leveraging multi-task graph self-supervised learning,\nyielding robust and generalizable graph tokens. Furthermore, the GQT utilizes\nResidual Vector Quantization (RVQ) to learn hierarchical discrete tokens,\nresulting in significantly reduced memory requirements and improved\ngeneralization capabilities. By combining the GQT with token modulation, a\nTransformer encoder achieves state-of-the-art performance on 16 out of 18\nbenchmarks, including large-scale homophilic and heterophilic datasets. The\ncode is available at: https://github.com/limei0307/graph-tokenizer", "paper_summary_zh": "Transformer \u64d4\u4efb\u57fa\u790e\u6a21\u578b\u7684\u9aa8\u5e79\u67b6\u69cb\uff0c\n\u5176\u4e2d\u7279\u5b9a\u9818\u57df\u7684 tokenizer \u5e6b\u52a9\u5b83\u5011\u9069\u61c9\u5404\u7a2e\u9818\u57df\u3002\u5716\u5f62\nTransformer (GT) \u6700\u8fd1\u5df2\u6210\u70ba\u5e7e\u4f55\u6df1\u5ea6\u5b78\u7fd2\u7684\u9818\u5148\u6a21\u578b\uff0c\u5728\u5404\u7a2e\u5716\u5f62\u5b78\u7fd2\n\u4efb\u52d9\u4e2d\u8868\u73fe\u512a\u65bc\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN)\u3002\u7136\u800c\uff0c\u5716\u5f62\u7684 tokenizer \u958b\u767c\u843d\u5f8c\n\u65bc\u5176\u4ed6\u6a21\u5f0f\uff0c\u73fe\u6709\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u555f\u767c\u6cd5\u6216\u8207 Transformer \u5171\u540c\u8a13\u7df4\u7684 GNN\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u9032 GQT\uff08**G**raph\n**Q**uantized **T**okenizer\uff09\uff0c\u5b83\u900f\u904e\u5229\u7528\u591a\u4efb\u52d9\u5716\u5f62\u81ea\u6211\u76e3\u7763\u5f0f\u5b78\u7fd2\uff0c\u5c07 tokenizer \u8a13\u7df4\u8207 Transformer \u8a13\u7df4\u5206\u958b\uff0c\u7522\u751f\u5f37\u5065\u4e14\u5177\u6cdb\u5316\u6027\u7684\u5716\u5f62\u4ee3\u78bc\u3002\u6b64\u5916\uff0cGQT \u5229\u7528\u6b98\u5dee\u5411\u91cf\u91cf\u5316 (RVQ) \u4f86\u5b78\u7fd2\u968e\u5c64\u5f0f\u96e2\u6563\u4ee3\u78bc\uff0c\n\u5927\u5e45\u6e1b\u5c11\u8a18\u61b6\u9ad4\u9700\u6c42\u4e26\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u900f\u904e\u7d50\u5408 GQT \u8207\u4ee3\u78bc\u8abf\u8b8a\uff0cTransformer \u7de8\u78bc\u5668\u5728 18 \u500b\u57fa\u6e96\u4e2d\u7684 16 \u500b\u57fa\u6e96\u4e0a\u9054\u6210\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5305\u62ec\u5927\u898f\u6a21\u540c\u8cea\u6027\u548c\u7570\u8cea\u6027\u8cc7\u6599\u96c6\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc\u4e0b\u5217\u7db2\u5740\u53d6\u5f97\uff1ahttps://github.com/limei0307/graph-tokenizer", "author": "Limei Wang et.al.", "authors": "Limei Wang, Kaveh Hassani, Si Zhang, Dongqi Fu, Baichuan Yuan, Weilin Cong, Zhigang Hua, Hao Wu, Ning Yao, Bo Long", "id": "2410.13798v1", "paper_url": "http://arxiv.org/abs/2410.13798v1", "repo": "https://github.com/limei0307/graph-tokenizer"}}