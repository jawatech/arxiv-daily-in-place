{"2406.06474": {"publish_time": "2024-06-10", "title": "Towards a Personal Health Large Language Model", "paper_summary": "In health, most large language model (LLM) research has focused on clinical\ntasks. However, mobile and wearable devices, which are rarely integrated into\nsuch tasks, provide rich, longitudinal data for personal health monitoring.\nHere we present Personal Health Large Language Model (PH-LLM), fine-tuned from\nGemini for understanding and reasoning over numerical time-series personal\nhealth data. We created and curated three datasets that test 1) production of\npersonalized insights and recommendations from sleep patterns, physical\nactivity, and physiological responses, 2) expert domain knowledge, and 3)\nprediction of self-reported sleep outcomes. For the first task we designed 857\ncase studies in collaboration with domain experts to assess real-world\nscenarios in sleep and fitness. Through comprehensive evaluation of\ndomain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not\nstatistically different from expert performance in fitness and, while experts\nremain superior for sleep, fine-tuning PH-LLM provided significant improvements\nin using relevant domain knowledge and personalizing information for sleep\ninsights. We evaluated PH-LLM domain knowledge using multiple choice sleep\nmedicine and fitness examinations. PH-LLM achieved 79% on sleep and 88% on\nfitness, exceeding average scores from a sample of human experts. Finally, we\ntrained PH-LLM to predict self-reported sleep quality outcomes from textual and\nmultimodal encoding representations of wearable data, and demonstrate that\nmultimodal encoding is required to match performance of specialized\ndiscriminative models. Although further development and evaluation are\nnecessary in the safety-critical personal health domain, these results\ndemonstrate both the broad knowledge and capabilities of Gemini models and the\nbenefit of contextualizing physiological data for personal health applications\nas done with PH-LLM.", "paper_summary_zh": "\u5728\u5065\u5eb7\u9818\u57df\uff0c\u5927\u591a\u6578\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7814\u7a76\u90fd\u5c08\u6ce8\u65bc\u81e8\u5e8a\u4efb\u52d9\u3002\u7136\u800c\uff0c\u884c\u52d5\u88dd\u7f6e\u548c\u7a7f\u6234\u5f0f\u88dd\u7f6e\u5f88\u5c11\u6574\u5408\u5230\u6b64\u985e\u4efb\u52d9\u4e2d\uff0c\u4f46\u5b83\u5011\u6703\u63d0\u4f9b\u8c50\u5bcc\u7684\u7e31\u5411\u8cc7\u6599\uff0c\u7528\u65bc\u500b\u4eba\u5065\u5eb7\u76e3\u63a7\u3002\u5728\u9019\u88e1\uff0c\u6211\u5011\u63d0\u51fa\u500b\u4eba\u5065\u5eb7\u5927\u578b\u8a9e\u8a00\u6a21\u578b (PH-LLM)\uff0c\u7d93\u904e Gemini \u5fae\u8abf\uff0c\u7528\u65bc\u7406\u89e3\u548c\u63a8\u7406\u6578\u503c\u6642\u9593\u5e8f\u5217\u500b\u4eba\u5065\u5eb7\u8cc7\u6599\u3002\u6211\u5011\u5efa\u7acb\u4e26\u7b56\u5283\u4e86\u4e09\u500b\u6e2c\u8a66\u8cc7\u6599\u96c6\uff0c\u7528\u65bc\u6e2c\u8a66 1) \u5f9e\u7761\u7720\u6a21\u5f0f\u3001\u8eab\u9ad4\u6d3b\u52d5\u548c\u751f\u7406\u53cd\u61c9\u4e2d\u7522\u751f\u500b\u4eba\u5316\u898b\u89e3\u548c\u5efa\u8b70\uff0c2) \u5c08\u5bb6\u9818\u57df\u77e5\u8b58\uff0c\u4ee5\u53ca 3) \u9810\u6e2c\u81ea\u6211\u5831\u544a\u7684\u7761\u7720\u7d50\u679c\u3002\u5c0d\u65bc\u7b2c\u4e00\u500b\u4efb\u52d9\uff0c\u6211\u5011\u8207\u9818\u57df\u5c08\u5bb6\u5408\u4f5c\u8a2d\u8a08\u4e86 857 \u500b\u6848\u4f8b\u7814\u7a76\uff0c\u4ee5\u8a55\u4f30\u7761\u7720\u548c\u5065\u8eab\u7684\u771f\u5be6\u60c5\u6cc1\u3002\u900f\u904e\u5c0d\u7279\u5b9a\u9818\u57df\u8a55\u5206\u6a19\u6e96\u7684\u5168\u9762\u8a55\u4f30\uff0c\u6211\u5011\u89c0\u5bdf\u5230 Gemini Ultra 1.0 \u548c PH-LLM \u5728\u5065\u8eab\u65b9\u9762\u7684\u8868\u73fe\u8207\u5c08\u5bb6\u8868\u73fe\u6c92\u6709\u7d71\u8a08\u5b78\u5dee\u7570\uff0c\u800c\u5c08\u5bb6\u5728\u7761\u7720\u65b9\u9762\u7684\u8868\u73fe\u4ecd\u7136\u8f03\u4f73\uff0c\u4f46\u5fae\u8abf PH-LLM \u5728\u4f7f\u7528\u76f8\u95dc\u9818\u57df\u77e5\u8b58\u548c\u500b\u4eba\u5316\u7761\u7720\u898b\u89e3\u8cc7\u8a0a\u65b9\u9762\u63d0\u4f9b\u4e86\u986f\u8457\u6539\u9032\u3002\u6211\u5011\u4f7f\u7528\u591a\u9078\u984c\u7761\u7720\u91ab\u5b78\u548c\u5065\u8eab\u6aa2\u67e5\u8a55\u4f30 PH-LLM \u9818\u57df\u77e5\u8b58\u3002PH-LLM \u5728\u7761\u7720\u65b9\u9762\u9054\u5230 79%\uff0c\u5728\u5065\u8eab\u65b9\u9762\u9054\u5230 88%\uff0c\u8d85\u904e\u4e86\u90e8\u5206\u4eba\u985e\u5c08\u5bb6\u7684\u5e73\u5747\u5206\u6578\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8a13\u7df4 PH-LLM \u5f9e\u53ef\u7a7f\u6234\u8cc7\u6599\u7684\u6587\u5b57\u548c\u591a\u6a21\u614b\u7de8\u78bc\u8868\u793a\u4e2d\u9810\u6e2c\u81ea\u6211\u5831\u544a\u7684\u7761\u7720\u54c1\u8cea\u7d50\u679c\uff0c\u4e26\u8b49\u660e\u591a\u6a21\u614b\u7de8\u78bc\u5c0d\u65bc\u5339\u914d\u7279\u6b8a\u8fa8\u5225\u6a21\u578b\u7684\u6548\u80fd\u662f\u5fc5\u8981\u7684\u3002\u5118\u7ba1\u5728\u5b89\u5168\u95dc\u9375\u7684\u500b\u4eba\u5065\u5eb7\u9818\u57df\u4e2d\u9700\u8981\u9032\u4e00\u6b65\u7684\u958b\u767c\u548c\u8a55\u4f30\uff0c\u4f46\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86 Gemini \u6a21\u578b\u7684\u5ee3\u6cdb\u77e5\u8b58\u548c\u529f\u80fd\uff0c\u4ee5\u53ca\u4f7f\u7528 PH-LLM \u5c0d\u751f\u7406\u8cc7\u6599\u9032\u884c\u60c5\u5883\u5316\u4ee5\u7528\u65bc\u500b\u4eba\u5065\u5eb7\u61c9\u7528\u7a0b\u5f0f\u7684\u512a\u9ede\u3002", "author": "Justin Cosentino et.al.", "authors": "Justin Cosentino, Anastasiya Belyaeva, Xin Liu, Nicholas A. Furlotte, Zhun Yang, Chace Lee, Erik Schenck, Yojan Patel, Jian Cui, Logan Douglas Schneider, Robby Bryant, Ryan G. Gomes, Allen Jiang, Roy Lee, Yun Liu, Javier Perez, Jameson K. Rogers, Cathy Speed, Shyam Tailor, Megan Walker, Jeffrey Yu, Tim Althoff, Conor Heneghan, John Hernandez, Mark Malhotra, Leor Stern, Yossi Matias, Greg S. Corrado, Shwetak Patel, Shravya Shetty, Jiening Zhan, Shruthi Prabhakara, Daniel McDuff, Cory Y. McLean", "id": "2406.06474v1", "paper_url": "http://arxiv.org/abs/2406.06474v1", "repo": "null"}}