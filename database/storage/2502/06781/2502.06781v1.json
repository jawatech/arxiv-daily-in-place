{"2502.06781": {"publish_time": "2025-02-10", "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning", "paper_summary": "Reasoning abilities, especially those for solving complex math problems, are\ncrucial components of general intelligence. Recent advances by proprietary\ncompanies, such as o-series models of OpenAI, have made remarkable progress on\nreasoning tasks. However, the complete technical details remain unrevealed, and\nthe techniques that are believed certainly to be adopted are only reinforcement\nlearning (RL) and the long chain of thoughts. This paper proposes a new RL\nframework, termed OREAL, to pursue the performance limit that can be achieved\nthrough \\textbf{O}utcome \\textbf{RE}w\\textbf{A}rd-based reinforcement\n\\textbf{L}earning for mathematical reasoning tasks, where only binary outcome\nrewards are easily accessible. We theoretically prove that behavior cloning on\npositive trajectories from best-of-N (BoN) sampling is sufficient to learn the\nKL-regularized optimal policy in binary feedback environments. This formulation\nfurther implies that the rewards of negative samples should be reshaped to\nensure the gradient consistency between positive and negative samples. To\nalleviate the long-existing difficulties brought by sparse rewards in RL, which\nare even exacerbated by the partial correctness of the long chain of thought\nfor reasoning tasks, we further apply a token-level reward model to sample\nimportant tokens in reasoning trajectories for learning. With OREAL, for the\nfirst time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,\nbeing on par with 32B models. OREAL-32B also surpasses previous 32B models\ntrained by distillation with 95.0 pass@1 accuracy on MATH-500. Our\ninvestigation also indicates the importance of initial policy models and\ntraining queries for RL. Code, models, and data will be released to benefit\nfuture research\\footnote{https://github.com/InternLM/OREAL}.", "paper_summary_zh": "<paragraph>\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u89e3\u6c7a\u8907\u96dc\u6578\u5b78\u554f\u984c\u7684\u80fd\u529b\uff0c\u662f\u6574\u9ad4\u667a\u529b\u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\u3002\u6700\u8fd1\uff0cOpenAI \u7684 o \u7cfb\u5217\u6a21\u578b\u7b49\u5c08\u6709\u516c\u53f8\u7684\u9032\u6b65\uff0c\u5728\u63a8\u7406\u4efb\u52d9\u4e0a\u53d6\u5f97\u4e86\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0c\u5b8c\u6574\u7684\u6280\u8853\u7d30\u7bc0\u4ecd\u672a\u516c\u958b\uff0c\u800c\u88ab\u8a8d\u70ba\u80af\u5b9a\u6703\u63a1\u7528\u7684\u6280\u8853\u53ea\u6709\u5f37\u5316\u5b78\u7fd2 (RL) \u548c\u9577\u93c8\u601d\u8003\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7684 RL \u6846\u67b6\uff0c\u7a31\u70ba OREAL\uff0c\u4ee5\u8ffd\u6c42\u53ef\u901a\u904e\u9762\u5411\u7d50\u679c\u734e\u52f5\u7684\u5f37\u5316\u5b78\u7fd2 (RL) \u4f86\u5be6\u73fe\u6578\u5b78\u63a8\u7406\u4efb\u52d9\u7684\u6548\u80fd\u6975\u9650\uff0c\u5176\u4e2d\u53ea\u6709\u4e8c\u5143\u7d50\u679c\u734e\u52f5\u5bb9\u6613\u7372\u5f97\u3002\u6211\u5011\u5f9e\u7406\u8ad6\u4e0a\u8b49\u660e\uff0c\u5728\u6700\u4f73 N (BoN) \u63a1\u6a23\u4e2d\u5c0d\u6b63\u9762\u8ecc\u8de1\u9032\u884c\u884c\u70ba\u8907\u88fd\uff0c\u8db3\u4ee5\u5728\u4e8c\u5143\u56de\u994b\u74b0\u5883\u4e2d\u5b78\u7fd2 KL \u6b63\u5247\u5316\u7684\u6700\u4f73\u7b56\u7565\u3002\u9019\u500b\u516c\u5f0f\u9032\u4e00\u6b65\u6697\u793a\uff0c\u61c9\u91cd\u65b0\u8abf\u6574\u8ca0\u9762\u7bc4\u4f8b\u7684\u734e\u52f5\uff0c\u4ee5\u78ba\u4fdd\u6b63\u9762\u548c\u8ca0\u9762\u7bc4\u4f8b\u4e4b\u9593\u7684\u68af\u5ea6\u4e00\u81f4\u6027\u3002\u70ba\u4e86\u6e1b\u8f15 RL \u4e2d\u7a00\u758f\u734e\u52f5\u5e36\u4f86\u7684\u9577\u671f\u5b58\u5728\u7684\u56f0\u96e3\uff0c\u800c\u63a8\u7406\u4efb\u52d9\u4e2d\u9577\u93c8\u601d\u8003\u7684\u90e8\u5206\u6b63\u78ba\u6027\u66f4\u8b93\u60c5\u6cc1\u96ea\u4e0a\u52a0\u971c\uff0c\u6211\u5011\u9032\u4e00\u6b65\u61c9\u7528\u4ee3\u5e63\u7d1a\u5225\u7684\u734e\u52f5\u6a21\u578b\uff0c\u4ee5\u5728\u63a8\u7406\u8ecc\u8de1\u4e2d\u63a1\u6a23\u91cd\u8981\u7684\u4ee3\u5e63\u9032\u884c\u5b78\u7fd2\u3002\u6709\u4e86 OREAL\uff0c\u4e00\u500b 7B \u6a21\u578b\u9996\u6b21\u53ef\u4ee5\u5728 MATH-500 \u4e0a\u901a\u904e RL \u7372\u5f97 94.0 \u7684 pass@1 \u6e96\u78ba\u7387\uff0c\u8207 32B \u6a21\u578b\u4e0d\u76f8\u4e0a\u4e0b\u3002OREAL-32B \u4e5f\u8d85\u8d8a\u4e86\u4e4b\u524d\u901a\u904e\u77e5\u8b58\u84b8\u993e\u8a13\u7df4\u7684 32B \u6a21\u578b\uff0c\u5728 MATH-500 \u4e0a\u7372\u5f97 95.0 \u7684 pass@1 \u6e96\u78ba\u7387\u3002\u6211\u5011\u7684\u8abf\u67e5\u9084\u8868\u660e\u521d\u59cb\u7b56\u7565\u6a21\u578b\u548c RL \u7684\u8a13\u7df4\u67e5\u8a62\u975e\u5e38\u91cd\u8981\u3002\u7a0b\u5f0f\u78bc\u3001\u6a21\u578b\u548c\u8cc7\u6599\u5c07\u6703\u91cb\u51fa\uff0c\u4ee5\u9020\u798f\u672a\u4f86\u7684\u7814\u7a76\\footnote{https://github.com/InternLM/OREAL}\u3002</paragraph>", "author": "Chengqi Lyu et.al.", "authors": "Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, Kai Chen", "id": "2502.06781v1", "paper_url": "http://arxiv.org/abs/2502.06781v1", "repo": "null"}}