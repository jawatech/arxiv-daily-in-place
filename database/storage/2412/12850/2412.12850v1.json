{"2412.12850": {"publish_time": "2024-12-17", "title": "Boosting Fine-Grained Visual Anomaly Detection with Coarse-Knowledge-Aware Adversarial Learning", "paper_summary": "Many unsupervised visual anomaly detection methods train an auto-encoder to\nreconstruct normal samples and then leverage the reconstruction error map to\ndetect and localize the anomalies. However, due to the powerful modeling and\ngeneralization ability of neural networks, some anomalies can also be well\nreconstructed, resulting in unsatisfactory detection and localization accuracy.\nIn this paper, a small coarsely-labeled anomaly dataset is first collected.\nThen, a coarse-knowledge-aware adversarial learning method is developed to\nalign the distribution of reconstructed features with that of normal features.\nThe alignment can effectively suppress the auto-encoder's reconstruction\nability on anomalies and thus improve the detection accuracy. Considering that\nanomalies often only occupy very small areas in anomalous images, a patch-level\nadversarial learning strategy is further developed. Although no patch-level\nanomalous information is available, we rigorously prove that by simply viewing\nany patch features from anomalous images as anomalies, the proposed\nknowledge-aware method can also align the distribution of reconstructed patch\nfeatures with the normal ones. Experimental results on four medical datasets\nand two industrial datasets demonstrate the effectiveness of our method in\nimproving the detection and localization performance.", "paper_summary_zh": "\u8a31\u591a\u7121\u76e3\u7763\u8996\u89ba\u7570\u5e38\u5075\u6e2c\u65b9\u6cd5\u6703\u8a13\u7df4\u81ea\u52d5\u7de8\u78bc\u5668\u4f86\u91cd\u5efa\u6b63\u5e38\u6a23\u672c\uff0c\u7136\u5f8c\u5229\u7528\u91cd\u5efa\u8aa4\u5dee\u5716\u4f86\u5075\u6e2c\u548c\u5b9a\u4f4d\u7570\u5e38\u3002\u7136\u800c\uff0c\u7531\u65bc\u795e\u7d93\u7db2\u8def\u5f37\u5927\u7684\u5efa\u6a21\u548c\u6982\u5316\u80fd\u529b\uff0c\u4e00\u4e9b\u7570\u5e38\u4e5f\u53ef\u4ee5\u88ab\u826f\u597d\u5730\u91cd\u5efa\uff0c\u5c0e\u81f4\u4e0d\u4ee4\u4eba\u6eff\u610f\u7684\u5075\u6e2c\u548c\u5b9a\u4f4d\u6e96\u78ba\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u9996\u5148\u6536\u96c6\u4e86\u4e00\u500b\u5c0f\u578b\u7c97\u7565\u6a19\u8a18\u7684\u7570\u5e38\u8cc7\u6599\u96c6\u3002\u7136\u5f8c\uff0c\u958b\u767c\u4e86\u4e00\u500b\u7c97\u7565\u77e5\u8b58\u611f\u77e5\u5c0d\u6297\u5b78\u7fd2\u65b9\u6cd5\uff0c\u4ee5\u5c07\u91cd\u5efa\u7279\u5fb5\u7684\u5206\u5e03\u8207\u6b63\u5e38\u7279\u5fb5\u7684\u5206\u5e03\u5c0d\u9f4a\u3002\u5c0d\u9f4a\u53ef\u4ee5\u6709\u6548\u5730\u6291\u5236\u81ea\u52d5\u7de8\u78bc\u5668\u5c0d\u7570\u5e38\u7684\u91cd\u5efa\u80fd\u529b\uff0c\u5f9e\u800c\u63d0\u9ad8\u5075\u6e2c\u6e96\u78ba\u5ea6\u3002\u8003\u616e\u5230\u7570\u5e38\u901a\u5e38\u53ea\u4f54\u7570\u5e38\u5f71\u50cf\u4e2d\u5f88\u5c0f\u7684\u5340\u57df\uff0c\u9032\u4e00\u6b65\u958b\u767c\u4e86\u5340\u584a\u7d1a\u5c0d\u6297\u5b78\u7fd2\u7b56\u7565\u3002\u5118\u7ba1\u6c92\u6709\u5340\u584a\u7d1a\u7570\u5e38\u8cc7\u8a0a\u53ef\u7528\uff0c\u4f46\u6211\u5011\u56b4\u683c\u8b49\u660e\uff0c\u53ea\u9700\u5c07\u7570\u5e38\u5f71\u50cf\u4e2d\u7684\u4efb\u4f55\u5340\u584a\u7279\u5fb5\u8996\u70ba\u7570\u5e38\uff0c\u6240\u63d0\u51fa\u7684\u77e5\u8b58\u611f\u77e5\u65b9\u6cd5\u4e5f\u53ef\u4ee5\u5c07\u91cd\u5efa\u5340\u584a\u7279\u5fb5\u7684\u5206\u5e03\u8207\u6b63\u5e38\u7279\u5fb5\u5c0d\u9f4a\u3002\u5728\u56db\u500b\u91ab\u5b78\u8cc7\u6599\u96c6\u548c\u5169\u500b\u5de5\u696d\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u6539\u5584\u5075\u6e2c\u548c\u5b9a\u4f4d\u6548\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Qingqing Fang et.al.", "authors": "Qingqing Fang, Qinliang Su, Wenxi Lv, Wenchao Xu, Jianxing Yu", "id": "2412.12850v1", "paper_url": "http://arxiv.org/abs/2412.12850v1", "repo": "null"}}