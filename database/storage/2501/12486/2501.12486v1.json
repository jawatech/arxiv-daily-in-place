{"2501.12486": {"publish_time": "2025-01-21", "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws", "paper_summary": "Pruning eliminates unnecessary parameters in neural networks; it offers a\npromising solution to the growing computational demands of large language\nmodels (LLMs). While many focus on post-training pruning, sparse\npre-training--which combines pruning and pre-training into a single\nphase--provides a simpler alternative. In this work, we present the first\nsystematic exploration of optimal sparse pre-training configurations for LLMs\nthrough an examination of 80 unique pruning schedules across different sparsity\nlevels and training durations. We find that initiating pruning at 25% of total\ntraining compute and concluding at 75% achieves near-optimal final evaluation\nloss. These findings provide valuable insights for efficient and effective\nsparse pre-training of LLMs. Furthermore, we propose a new scaling law that\nmodifies the Chinchilla scaling law to use the average parameter count over\npre-training. Through empirical and theoretical validation, we demonstrate that\nthis modified scaling law accurately models evaluation loss for both sparsely\nand densely pre-trained LLMs, unifying scaling laws across pre-training\nparadigms. Our findings indicate that while sparse pre-training achieves the\nsame final model quality as dense pre-training for equivalent compute budgets,\nit provides substantial benefits through reduced model size, enabling\nsignificant potential computational savings during inference.", "paper_summary_zh": "\u4fee\u526a\u6d88\u9664\u4e86\u795e\u7d93\u7db2\u8def\u4e2d\u4e0d\u5fc5\u8981\u7684\u53c3\u6578\uff1b\u5b83\u70ba\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e0d\u65b7\u589e\u9577\u7684\u904b\u7b97\u9700\u6c42\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u96d6\u7136\u8a31\u591a\u4eba\u5c08\u6ce8\u65bc\u8a13\u7df4\u5f8c\u4fee\u526a\uff0c\u4f46\u7a00\u758f\u9810\u8a13\u7df4\uff08\u5c07\u4fee\u526a\u548c\u9810\u8a13\u7df4\u7d50\u5408\u5230\u4e00\u500b\u968e\u6bb5\uff09\u63d0\u4f9b\u4e86\u4e00\u500b\u66f4\u7c21\u55ae\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u901a\u904e\u6aa2\u67e5 80 \u500b\u4e0d\u540c\u7684\u7a00\u758f\u5ea6\u7d1a\u5225\u548c\u8a13\u7df4\u6301\u7e8c\u6642\u9593\u7684\u7368\u7279\u4fee\u526a\u6642\u9593\u8868\uff0c\u5c0d LLM \u7684\u6700\u4f73\u7a00\u758f\u9810\u8a13\u7df4\u914d\u7f6e\u9032\u884c\u4e86\u9996\u6b21\u7cfb\u7d71\u6027\u63a2\u7d22\u3002\u6211\u5011\u767c\u73fe\uff0c\u5f9e\u7e3d\u8a13\u7df4\u904b\u7b97\u7684 25% \u958b\u59cb\u4fee\u526a\u4e26\u5728 75% \u7d50\u675f\uff0c\u53ef\u4ee5\u9054\u5230\u63a5\u8fd1\u6700\u4f73\u7684\u6700\u7d42\u8a55\u4f30\u640d\u5931\u3002\u9019\u4e9b\u767c\u73fe\u70ba LLM \u7684\u9ad8\u6548\u4e14\u6709\u6548\u7684\u7a00\u758f\u9810\u8a13\u7df4\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u65b0\u7684\u7e2e\u653e\u5b9a\u5f8b\uff0c\u8a72\u5b9a\u5f8b\u4fee\u6539\u4e86 Chinchilla \u7e2e\u653e\u5b9a\u5f8b\uff0c\u4ee5\u4f7f\u7528\u9810\u8a13\u7df4\u671f\u9593\u7684\u5e73\u5747\u53c3\u6578\u8a08\u6578\u3002\u901a\u904e\u7d93\u9a57\u548c\u7406\u8ad6\u9a57\u8b49\uff0c\u6211\u5011\u8b49\u660e\u4e86\u9019\u500b\u4fee\u6539\u5f8c\u7684\u7e2e\u653e\u5b9a\u5f8b\u53ef\u4ee5\u6e96\u78ba\u5730\u6a21\u64ec\u7a00\u758f\u548c\u5bc6\u96c6\u9810\u8a13\u7df4 LLM \u7684\u8a55\u4f30\u640d\u5931\uff0c\u7d71\u4e00\u4e86\u9810\u8a13\u7df4\u7bc4\u4f8b\u4e2d\u7684\u7e2e\u653e\u5b9a\u5f8b\u3002\u6211\u5011\u7684\u767c\u73fe\u8868\u660e\uff0c\u96d6\u7136\u7a00\u758f\u9810\u8a13\u7df4\u5728\u76f8\u540c\u7684\u904b\u7b97\u9810\u7b97\u4e0b\u5be6\u73fe\u4e86\u8207\u5bc6\u96c6\u9810\u8a13\u7df4\u76f8\u540c\u7684\u6700\u7d42\u6a21\u578b\u54c1\u8cea\uff0c\u4f46\u5b83\u901a\u904e\u6e1b\u5c11\u6a21\u578b\u5927\u5c0f\u63d0\u4f9b\u4e86\u986f\u8457\u7684\u512a\u9ede\uff0c\u5f9e\u800c\u53ef\u4ee5\u5728\u63a8\u7406\u671f\u9593\u5be6\u73fe\u986f\u8457\u7684\u6f5b\u5728\u904b\u7b97\u7bc0\u7701\u3002", "author": "Tian Jin et.al.", "authors": "Tian Jin, Ahmed Imtiaz Humayun, Utku Evci, Suvinay Subramanian, Amir Yazdanbakhsh, Dan Alistarh, Gintare Karolina Dziugaite", "id": "2501.12486v1", "paper_url": "http://arxiv.org/abs/2501.12486v1", "repo": "null"}}