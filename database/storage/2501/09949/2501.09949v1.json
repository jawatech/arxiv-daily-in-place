{"2501.09949": {"publish_time": "2025-01-17", "title": "MultiPruner: Balanced Structure Removal in Foundation Models", "paper_summary": "Recently, state-of-the-art approaches for pruning large pre-trained models\n(LPMs) have demonstrated that the training-free removal of non-critical\nresidual blocks in Transformers is viable for reducing model size, achieving\nresults that outperform previous training-free pruning approaches. Motivated by\nthese findings, we extend BlockPruner (Zhong et al., 2024) and propose\nMultiPruner, a pruning approach that surpasses recent training-free pruning\nmethods by adopting a multidimensional, iterative, fine-grained pruning\nstrategy. In MultiPruner, multidimensional pruning reinstates the structural\nbalance in block-pruned models by sequentially compressing along three\ndimensions: i) residual blocks, ii) channels of multilayer perceptrons (MLP),\nand iii) attention heads. This solution enhances zero-shot accuracy on\ndownstream tasks compared to other techniques while improving model compression\nratios, producing compressed models with fewer computing and memory\nrequirements. Extensive experiments demonstrate the advantages of the proposed\nmethod across various large pre-trained models. The code and pruning\nconfigurations are available at\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.", "paper_summary_zh": "<paragraph>\u8fd1\u4f86\uff0c\u4fee\u526a\u5927\u578b\u9810\u8a13\u7df4\u6a21\u578b (LPM) \u7684\u6700\u65b0\u65b9\u6cd5\u5df2\u8b49\u660e\uff0c\u5728 Transformer \u4e2d\u79fb\u9664\u975e\u5fc5\u8981\u7684\u6b98\u5dee\u5340\u584a\u7121\u9700\u8a13\u7df4\uff0c\u5c31\u80fd\u6709\u6548\u7e2e\u5c0f\u6a21\u578b\u5927\u5c0f\uff0c\u4e26\u9054\u6210\u512a\u65bc\u5148\u524d\u7121\u8a13\u7df4\u4fee\u526a\u65b9\u6cd5\u7684\u7d50\u679c\u3002\u53d7\u9019\u4e9b\u767c\u73fe\u555f\u767c\uff0c\u6211\u5011\u64f4\u5145 BlockPruner (Zhong \u7b49\u4eba\uff0c2024)\uff0c\u4e26\u63d0\u51fa MultiPruner\uff0c\u9019\u662f\u4e00\u7a2e\u4fee\u526a\u65b9\u6cd5\uff0c\u63a1\u7528\u591a\u7dad\u3001\u53cd\u8986\u3001\u7d30\u7dfb\u7684\u4fee\u526a\u7b56\u7565\uff0c\u8d85\u8d8a\u4e86\u6700\u8fd1\u7684\u7121\u8a13\u7df4\u4fee\u526a\u65b9\u6cd5\u3002\u5728 MultiPruner \u4e2d\uff0c\u591a\u7dad\u4fee\u526a\u900f\u904e\u6cbf\u8457\u4e09\u500b\u7dad\u5ea6\u9806\u5e8f\u58d3\u7e2e\u4f86\u91cd\u5efa\u5340\u584a\u4fee\u526a\u6a21\u578b\u7684\u7d50\u69cb\u5e73\u8861\uff1ai) \u6b98\u5dee\u5340\u584a\u3001ii) \u591a\u5c64\u611f\u77e5\u5668 (MLP) \u7684\u901a\u9053\uff0c\u4ee5\u53ca iii) \u6ce8\u610f\u529b\u5c64\u3002\u8207\u5176\u4ed6\u6280\u8853\u76f8\u6bd4\uff0c\u6b64\u89e3\u6c7a\u65b9\u6848\u589e\u5f37\u4e86\u4e0b\u6e38\u4efb\u52d9\u7684\u96f6\u6b21\u5b78\u7fd2\u6e96\u78ba\u5ea6\uff0c\u540c\u6642\u6539\u5584\u6a21\u578b\u58d3\u7e2e\u7387\uff0c\u7522\u751f\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\u8f03\u4f4e\u7684\u58d3\u7e2e\u6a21\u578b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u5404\u7a2e\u5927\u578b\u9810\u8a13\u7df4\u6a21\u578b\u4e2d\u7684\u512a\u9ede\u3002\u7a0b\u5f0f\u78bc\u548c\u4fee\u526a\u7d44\u614b\u53ef\u5728 https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning \u53d6\u5f97\u3002</paragraph>", "author": "J. Pablo Mu\u00f1oz et.al.", "authors": "J. Pablo Mu\u00f1oz, Jinjie Yuan, Nilesh Jain", "id": "2501.09949v1", "paper_url": "http://arxiv.org/abs/2501.09949v1", "repo": "https://github.com/intellabs/hardware-aware-automated-machine-learning"}}