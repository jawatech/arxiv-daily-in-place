{"2502.02368": {"publish_time": "2025-02-04", "title": "Evaluating the Effectiveness of LLMs in Fixing Maintainability Issues in Real-World Projects", "paper_summary": "Large Language Models (LLMs) have gained attention for addressing coding\nproblems, but their effectiveness in fixing code maintainability remains\nunclear. This study evaluates LLMs capability to resolve 127 maintainability\nissues from 10 GitHub repositories. We use zero-shot prompting for Copilot Chat\nand Llama 3.1, and few-shot prompting with Llama only. The LLM-generated\nsolutions are assessed for compilation errors, test failures, and new\nmaintainability problems. Llama with few-shot prompting successfully fixed\n44.9% of the methods, while Copilot Chat and Llama zero-shot fixed 32.29% and\n30%, respectively. However, most solutions introduced errors or new\nmaintainability issues. We also conducted a human study with 45 participants to\nevaluate the readability of 51 LLM-generated solutions. The human study showed\nthat 68.63% of participants observed improved readability. Overall, while LLMs\nshow potential for fixing maintainability issues, their introduction of errors\nhighlights their current limitations.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u89e3\u6c7a\u7de8\u78bc\u554f\u984c\u65b9\u9762\u5099\u53d7\u95dc\u6ce8\uff0c\u4f46\u5b83\u5011\u5728\u4fee\u5fa9\u7a0b\u5f0f\u78bc\u53ef\u7dad\u8b77\u6027\u65b9\u9762\u7684\u6548\u80fd\u4ecd\u4e0d\u660e\u78ba\u3002\u672c\u7814\u7a76\u8a55\u4f30\u4e86 LLM \u89e3\u6c7a\u4f86\u81ea 10 \u500b GitHub \u5132\u5b58\u5eab\u7684 127 \u500b\u53ef\u7dad\u8b77\u6027\u554f\u984c\u7684\u80fd\u529b\u3002\u6211\u5011\u5c0d Copilot Chat \u548c Llama 3.1 \u4f7f\u7528\u96f6\u6b21\u63d0\u793a\uff0c\u50c5\u5c0d Llama \u4f7f\u7528\u5c11\u6b21\u63d0\u793a\u3002\u8a55\u4f30 LLM \u751f\u6210\u7684\u89e3\u6c7a\u65b9\u6848\u7684\u7de8\u8b6f\u932f\u8aa4\u3001\u6e2c\u8a66\u5931\u6557\u548c\u65b0\u7684\u53ef\u7dad\u8b77\u6027\u554f\u984c\u3002\u4f7f\u7528\u5c11\u6b21\u63d0\u793a\u7684 Llama \u6210\u529f\u4fee\u5fa9\u4e86 44.9% \u7684\u65b9\u6cd5\uff0c\u800c Copilot Chat \u548c Llama \u96f6\u6b21\u63d0\u793a\u5206\u5225\u4fee\u5fa9\u4e86 32.29% \u548c 30%\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u89e3\u6c7a\u65b9\u6848\u5f15\u5165\u4e86\u932f\u8aa4\u6216\u65b0\u7684\u53ef\u7dad\u8b77\u6027\u554f\u984c\u3002\u6211\u5011\u9084\u9032\u884c\u4e86\u4e00\u9805\u5305\u542b 45 \u540d\u53c3\u8207\u8005\u7684\u7814\u7a76\uff0c\u4ee5\u8a55\u4f30 51 \u500b LLM \u751f\u6210\u7684\u89e3\u6c7a\u65b9\u6848\u7684\u53ef\u8b80\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c68.63% \u7684\u53c3\u8207\u8005\u89c0\u5bdf\u5230\u53ef\u8b80\u6027\u6709\u6240\u6539\u5584\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u96d6\u7136 LLM \u5728\u4fee\u5fa9\u53ef\u7dad\u8b77\u6027\u554f\u984c\u65b9\u9762\u986f\u793a\u51fa\u6f5b\u529b\uff0c\u4f46\u5b83\u5011\u5f15\u5165\u7684\u932f\u8aa4\u7a81\u986f\u4e86\u5b83\u5011\u76ee\u524d\u7684\u5c40\u9650\u6027\u3002", "author": "Henrique Nunes et.al.", "authors": "Henrique Nunes, Eduardo Figueiredo, Larissa Rocha, Sarah Nadi, Fischer Ferreira, Geanderson Esteves", "id": "2502.02368v1", "paper_url": "http://arxiv.org/abs/2502.02368v1", "repo": "null"}}