{"2405.05777": {"publish_time": "2024-05-09", "title": "Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the S\u00e1mi Language", "paper_summary": "S\\'ami, an indigenous language group comprising multiple languages, faces\ndigital marginalization due to the limited availability of data and\nsophisticated language models designed for its linguistic intricacies. This\nwork focuses on increasing technological participation for the S\\'ami language.\nWe draw the attention of the ML community towards the language modeling problem\nof Ultra Low Resource (ULR) languages. ULR languages are those for which the\namount of available textual resources is very low, and the speaker count for\nthem is also very low. ULRLs are also not supported by mainstream Large\nLanguage Models (LLMs) like ChatGPT, due to which gathering artificial training\ndata for them becomes even more challenging. Mainstream AI foundational model\ndevelopment has given less attention to this category of languages. Generally,\nthese languages have very few speakers, making it hard to find them. However,\nit is important to develop foundational models for these ULR languages to\npromote inclusion and the tangible abilities and impact of LLMs. To this end,\nwe have compiled the available S\\'ami language resources from the web to create\na clean dataset for training language models. In order to study the behavior of\nmodern LLM models with ULR languages (S\\'ami), we have experimented with\ndifferent kinds of LLMs, mainly at the order of $\\sim$ seven billion\nparameters. We have also explored the effect of multilingual LLM training for\nULRLs. We found that the decoder-only models under a sequential multilingual\ntraining scenario perform better than joint multilingual training, whereas\nmultilingual training with high semantic overlap, in general, performs better\nthan training from scratch.This is the first study on the S\\'ami language for\nadapting non-statistical language models that use the latest developments in\nthe field of natural language processing (NLP).", "paper_summary_zh": "S'ami \u662f\u4e00\u7a2e\u5305\u542b\u591a\u7a2e\u8a9e\u8a00\u7684\u539f\u4f4f\u6c11\u8a9e\u8a00\u7fa4\uff0c\u7531\u65bc\u7f3a\u4e4f\u8cc7\u6599\u548c\u91dd\u5c0d\u5176\u8a9e\u8a00\u8907\u96dc\u6027\u800c\u8a2d\u8a08\u7684\u7cbe\u7dfb\u8a9e\u8a00\u6a21\u578b\uff0c\u9762\u81e8\u6578\u4f4d\u908a\u7de3\u5316\u3002\u9019\u9805\u5de5\u4f5c\u5c08\u6ce8\u65bc\u589e\u52a0 S'ami \u8a9e\u8a00\u7684\u79d1\u6280\u53c3\u8207\u3002\u6211\u5011\u5c07\u6a5f\u5668\u5b78\u7fd2\u793e\u7fa4\u7684\u6ce8\u610f\u529b\u5f15\u5c0e\u81f3\u6975\u4f4e\u8cc7\u6e90 (ULR) \u8a9e\u8a00\u7684\u8a9e\u8a00\u5efa\u6a21\u554f\u984c\u3002ULR \u8a9e\u8a00\u662f\u6307\u53ef\u7528\u6587\u5b57\u8cc7\u6e90\u975e\u5e38\u5c11\uff0c\u4e14\u8aaa\u8a71\u8005\u6578\u91cf\u4e5f\u5f88\u5c11\u7684\u8a9e\u8a00\u3002ULRL \u4e5f\u4e0d\u53d7\u4e3b\u6d41\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff08\u4f8b\u5982 ChatGPT\uff09\u652f\u63f4\uff0c\u56e0\u6b64\u70ba\u5176\u6536\u96c6\u4eba\u5de5\u8a13\u7df4\u8cc7\u6599\u8b8a\u5f97\u66f4\u5177\u6311\u6230\u6027\u3002\u4e3b\u6d41 AI \u57fa\u790e\u6a21\u578b\u958b\u767c\u8f03\u5c11\u95dc\u6ce8\u9019\u985e\u8a9e\u8a00\u3002\u4e00\u822c\u4f86\u8aaa\uff0c\u9019\u4e9b\u8a9e\u8a00\u7684\u4f7f\u7528\u8005\u5f88\u5c11\uff0c\u56e0\u6b64\u5f88\u96e3\u627e\u5230\u4ed6\u5011\u3002\u7136\u800c\uff0c\u958b\u767c\u9019\u4e9b ULR \u8a9e\u8a00\u7684\u57fa\u790e\u6a21\u578b\u975e\u5e38\u91cd\u8981\uff0c\u4ee5\u4fc3\u9032\u5305\u5bb9\u6027\u4ee5\u53ca LLM \u7684\u5be6\u969b\u80fd\u529b\u548c\u5f71\u97ff\u529b\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5df2\u5f9e\u7db2\u8def\u4e0a\u7de8\u8b6f\u53ef\u7528\u7684 S'ami \u8a9e\u8a00\u8cc7\u6e90\uff0c\u4ee5\u5efa\u7acb\u4e00\u500b\u7528\u65bc\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u7684\u4e7e\u6de8\u8cc7\u6599\u96c6\u3002\u70ba\u4e86\u7814\u7a76\u73fe\u4ee3 LLM \u6a21\u578b\u5728 ULR \u8a9e\u8a00\uff08S'ami\uff09\u4e2d\u7684\u884c\u70ba\uff0c\u6211\u5011\u5df2\u91dd\u5c0d\u4e0d\u540c\u985e\u578b\u7684 LLM \u9032\u884c\u5be6\u9a57\uff0c\u4e3b\u8981\u7d04\u70ba 70 \u5104\u500b\u53c3\u6578\u3002\u6211\u5011\u4e5f\u63a2\u8a0e\u4e86\u591a\u8a9e\u8a00 LLM \u8a13\u7df4\u5c0d ULRL \u7684\u5f71\u97ff\u3002\u6211\u5011\u767c\u73fe\uff0c\u5728\u9806\u5e8f\u591a\u8a9e\u8a00\u8a13\u7df4\u5834\u666f\u4e0b\u7684\u50c5\u89e3\u78bc\u5668\u6a21\u578b\u8868\u73fe\u512a\u65bc\u806f\u5408\u591a\u8a9e\u8a00\u8a13\u7df4\uff0c\u800c\u4e00\u822c\u800c\u8a00\uff0c\u8a9e\u610f\u91cd\u758a\u5ea6\u9ad8\u7684\u591a\u8a9e\u8a00\u8a13\u7df4\u8868\u73fe\u512a\u65bc\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u3002\u9019\u662f\u7b2c\u4e00\u500b\u91dd\u5c0d S'ami \u8a9e\u8a00\u7684\u7814\u7a76\uff0c\u7528\u65bc\u8abf\u6574\u975e\u7d71\u8a08\u8a9e\u8a00\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u4f7f\u7528\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u9818\u57df\u7684\u6700\u65b0\u767c\u5c55\u3002", "author": "Ronny Paul et.al.", "authors": "Ronny Paul, Himanshu Buckchash, Shantipriya Parida, Dilip K. Prasad", "id": "2405.05777v1", "paper_url": "http://arxiv.org/abs/2405.05777v1", "repo": "null"}}