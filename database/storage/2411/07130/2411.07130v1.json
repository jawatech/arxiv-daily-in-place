{"2411.07130": {"publish_time": "2024-11-11", "title": "Retrieval or Global Context Understanding? On Many-Shot In-Context Learning for Long-Context Evaluation", "paper_summary": "Language models (LMs) have demonstrated an improved capacity to handle\nlong-context information, yet existing long-context benchmarks primarily\nmeasure LMs' retrieval abilities with extended inputs, e.g., pinpointing a\nshort phrase from long-form text. Therefore, they may fall short when\nevaluating models' global context understanding capacity, such as synthesizing\nand reasoning over content across input to generate the response. In this\npaper, we study long-context language model (LCLM) evaluation through many-shot\nin-context learning (ICL). Concretely, we identify the skills each ICL task\nrequires, and examine models' long-context capabilities on them. We first ask:\nWhat types of ICL tasks benefit from additional demonstrations, and are these\ntasks effective at evaluating LCLMs? We find that classification and\nsummarization tasks show notable performance improvements with additional\ndemonstrations, while translation and reasoning tasks do not exhibit clear\ntrends. This suggests the classification tasks predominantly test models'\nretrieval skills. Next, we ask: To what extent does each task require retrieval\nskills versus global context understanding from LCLMs? We develop metrics to\ncategorize ICL tasks into two groups: (i) retrieval tasks that require strong\nretrieval ability to pinpoint relevant examples, and (ii) global context\nunderstanding tasks that necessitate a deeper comprehension of the full input.\nWe find that not all datasets can effectively evaluate these long-context\ncapabilities. To address this gap, we introduce a new many-shot ICL benchmark,\nMANYICLBENCH, designed to characterize LCLMs' retrieval and global context\nunderstanding capabilities separately. Benchmarking 11 open-weight LCLMs with\nMANYICLBENCH, we find that while state-of-the-art models perform well in\nretrieval tasks up to 64k tokens, many show significant drops in global context\ntasks at just 16k tokens.", "paper_summary_zh": "<paragraph>\u8a9e\u8a00\u6a21\u578b (LM) \u5df2\u5c55\u73fe\u51fa\u8655\u7406\u9577\u8a9e\u5883\u8cc7\u8a0a\u7684\u9032\u6b65\u80fd\u529b\uff0c\u4f46\u73fe\u6709\u7684\u9577\u8a9e\u5883\u57fa\u6e96\u4e3b\u8981\u6e2c\u91cf LM \u7684\u64f7\u53d6\u80fd\u529b\uff0c\u8f38\u5165\u5167\u5bb9\u8f03\u9577\uff0c\u4f8b\u5982\u5f9e\u9577\u7bc7\u6587\u5b57\u4e2d\u7cbe\u78ba\u627e\u51fa\u77ed\u8a9e\u3002\u56e0\u6b64\uff0c\u5728\u8a55\u4f30\u6a21\u578b\u7684\u6574\u9ad4\u8a9e\u5883\u7406\u89e3\u80fd\u529b\u6642\uff0c\u4f8b\u5982\u7d9c\u5408\u548c\u63a8\u7406\u8f38\u5165\u4e2d\u7684\u5167\u5bb9\u4ee5\u7522\u751f\u56de\u61c9\uff0c\u9019\u4e9b\u57fa\u6e96\u53ef\u80fd\u6703\u4e0d\u8db3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u591a\u8f2a\u6b21\u8a9e\u5883\u4e2d\u5b78\u7fd2 (ICL) \u7814\u7a76\u9577\u8a9e\u5883\u8a9e\u8a00\u6a21\u578b (LCLM) \u8a55\u4f30\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u627e\u51fa\u6bcf\u500b ICL \u4efb\u52d9\u6240\u9700\u7684\u6280\u80fd\uff0c\u4e26\u6aa2\u8996\u6a21\u578b\u5728\u9019\u4e9b\u6280\u80fd\u4e0a\u7684\u9577\u8a9e\u5883\u80fd\u529b\u3002\u6211\u5011\u9996\u5148\u8a62\u554f\uff1a\u54ea\u4e9b\u985e\u578b\u7684 ICL \u4efb\u52d9\u53ef\u4ee5\u5f9e\u984d\u5916\u7684\u793a\u7bc4\u4e2d\u53d7\u76ca\uff0c\u9019\u4e9b\u4efb\u52d9\u5728\u8a55\u4f30 LCLM \u6642\u662f\u5426\u6709\u6548\uff1f\u6211\u5011\u767c\u73fe\u5206\u985e\u548c\u6458\u8981\u4efb\u52d9\u5728\u984d\u5916\u7684\u793a\u7bc4\u4e2d\u8868\u73fe\u51fa\u986f\u8457\u7684\u9032\u6b65\uff0c\u800c\u7ffb\u8b6f\u548c\u63a8\u7406\u4efb\u52d9\u4e26\u672a\u5c55\u73fe\u51fa\u660e\u78ba\u7684\u8da8\u52e2\u3002\u9019\u8868\u793a\u5206\u985e\u4efb\u52d9\u4e3b\u8981\u6e2c\u8a66\u6a21\u578b\u7684\u64f7\u53d6\u6280\u80fd\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u8a62\u554f\uff1a\u6bcf\u500b\u4efb\u52d9\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u9700\u8981 LCLM \u7684\u64f7\u53d6\u6280\u80fd\u548c\u6574\u9ad4\u8a9e\u5883\u7406\u89e3\uff1f\u6211\u5011\u958b\u767c\u6307\u6a19\u5c07 ICL \u4efb\u52d9\u5206\u985e\u70ba\u5169\u7d44\uff1a(i) \u9700\u8981\u5f37\u5927\u64f7\u53d6\u80fd\u529b\u4f86\u7cbe\u78ba\u627e\u51fa\u76f8\u95dc\u7bc4\u4f8b\u7684\u64f7\u53d6\u4efb\u52d9\uff0c\u4ee5\u53ca (ii) \u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u5b8c\u6574\u8f38\u5165\u7684\u6574\u9ad4\u8a9e\u5883\u7406\u89e3\u4efb\u52d9\u3002\u6211\u5011\u767c\u73fe\u4e26\u975e\u6240\u6709\u8cc7\u6599\u96c6\u90fd\u80fd\u6709\u6548\u8a55\u4f30\u9019\u4e9b\u9577\u8a9e\u5883\u80fd\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u9032\u4e00\u500b\u65b0\u7684\u591a\u8f2a\u6b21 ICL \u57fa\u6e96\uff0cMANYICLBENCH\uff0c\u65e8\u5728\u5206\u5225\u63cf\u8ff0 LCLM \u7684\u64f7\u53d6\u548c\u6574\u9ad4\u8a9e\u5883\u7406\u89e3\u80fd\u529b\u3002\u4f7f\u7528 MANYICLBENCH \u5c0d 11 \u500b\u958b\u653e\u6b0a\u91cd\u7684 LCLM \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u6211\u5011\u767c\u73fe\u96d6\u7136\u6700\u5148\u9032\u7684\u6a21\u578b\u5728\u9577\u9054 64k \u500b\u8a5e\u5f59\u7684\u64f7\u53d6\u4efb\u52d9\u4e2d\u8868\u73fe\u826f\u597d\uff0c\u4f46\u8a31\u591a\u6a21\u578b\u5728\u50c5 16k \u500b\u8a5e\u5f59\u7684\u6574\u9ad4\u8a9e\u5883\u4efb\u52d9\u4e2d\u986f\u8457\u4e0b\u964d\u3002</paragraph>", "author": "Kaijian Zou et.al.", "authors": "Kaijian Zou, Muhammad Khalifa, Lu Wang", "id": "2411.07130v1", "paper_url": "http://arxiv.org/abs/2411.07130v1", "repo": "null"}}