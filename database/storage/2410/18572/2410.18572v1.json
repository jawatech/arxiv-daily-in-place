{"2410.18572": {"publish_time": "2024-10-24", "title": "Taipan: Efficient and Expressive State Space Language Models with Selective Attention", "paper_summary": "Efficient long-context language modeling remains a significant challenge in\nNatural Language Processing (NLP). While Transformers dominate language tasks,\nthey struggle with long sequences due to quadratic computational complexity in\ntraining and linearly scaling memory costs during inference. Recent State Space\nModels (SSMs) such as Mamba offer alternatives with constant memory usage, but\nthey underperform in tasks requiring extensive in-context retrieval. We\nintroduce Taipan, a novel hybrid architecture that combines Mamba-2 with\nSelective Attention Layers (SALs). These SALs identify tokens requiring\nlong-range interactions, remove less important features, and then augment their\nrepresentations using the attention module. This approach balances Mamba's\nefficiency with Transformer-like performance in memory-intensive tasks. By\nconstraining the attention budget, Taipan extends accurate predictions to\ncontext lengths of up to 1 million tokens while preserving computational\nefficiency. Our experiments demonstrate Taipan's superior performance across\nvarious scales and tasks, offering a promising solution for efficient\nlong-context language modeling.", "paper_summary_zh": "\u9ad8\u6548\u9577\u6587\u672c\u8a9e\u8a00\u5efa\u6a21\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u7684\u6311\u6230\u3002\u96d6\u7136 Transformer \u4e3b\u5c0e\u8a9e\u8a00\u4efb\u52d9\uff0c\u4f46\u7531\u65bc\u8a13\u7df4\u4e2d\u7684\u4e8c\u6b21\u8a08\u7b97\u8907\u96dc\u5ea6\u548c\u63a8\u8ad6\u671f\u9593\u7dda\u6027\u64f4\u5c55\u7684\u8a18\u61b6\u9ad4\u6210\u672c\uff0c\u5b83\u5011\u5728\u8655\u7406\u9577\u5e8f\u5217\u6642\u6703\u9047\u5230\u56f0\u96e3\u3002\u6700\u8fd1\u7684\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM)\uff0c\u4f8b\u5982 Mamba\uff0c\u63d0\u4f9b\u4e86\u5177\u6709\u6046\u5b9a\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5b83\u5011\u5728\u9700\u8981\u5927\u91cf\u4e0a\u4e0b\u6587\u6aa2\u7d22\u7684\u4efb\u52d9\u4e2d\u8868\u73fe\u4e0d\u4f73\u3002\u6211\u5011\u4ecb\u7d39\u4e86 Taipan\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u6df7\u5408\u67b6\u69cb\uff0c\u5b83\u5c07 Mamba-2 \u8207\u9078\u64c7\u6027\u6ce8\u610f\u5c64 (SAL) \u76f8\u7d50\u5408\u3002\u9019\u4e9b SAL \u8b58\u5225\u9700\u8981\u9032\u884c\u9577\u8ddd\u96e2\u4ea4\u4e92\u7684\u7b26\u865f\uff0c\u79fb\u9664\u8f03\u4e0d\u91cd\u8981\u7684\u7279\u5fb5\uff0c\u7136\u5f8c\u4f7f\u7528\u6ce8\u610f\u6a21\u7d44\u64f4\u5145\u5b83\u5011\u7684\u8868\u793a\u3002\u9019\u7a2e\u65b9\u6cd5\u5e73\u8861\u4e86 Mamba \u7684\u6548\u7387\u8207 Transformer \u5728\u8a18\u61b6\u5bc6\u96c6\u578b\u4efb\u52d9\u4e2d\u7684\u985e\u4f3c\u6548\u80fd\u3002\u901a\u904e\u9650\u5236\u6ce8\u610f\u529b\u9810\u7b97\uff0cTaipan \u5c07\u6e96\u78ba\u7684\u9810\u6e2c\u64f4\u5c55\u5230\u9577\u9054 100 \u842c\u500b\u7b26\u865f\u7684\u4e0a\u4e0b\u6587\u9577\u5ea6\uff0c\u540c\u6642\u4fdd\u6301\u8a08\u7b97\u6548\u7387\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\u4e86 Taipan \u5728\u5404\u7a2e\u898f\u6a21\u548c\u4efb\u52d9\u4e2d\u7684\u5353\u8d8a\u6548\u80fd\uff0c\u70ba\u9ad8\u6548\u9577\u6587\u672c\u8a9e\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u666f\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Chien Van Nguyen et.al.", "authors": "Chien Van Nguyen, Huy Huu Nguyen, Thang M. Pham, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Ryan A. Rossi, Trung Bui, Viet Dac Lai, Franck Dernoncourt, Thien Huu Nguyen", "id": "2410.18572v1", "paper_url": "http://arxiv.org/abs/2410.18572v1", "repo": "null"}}