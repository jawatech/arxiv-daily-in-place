{"2407.11300": {"publish_time": "2024-07-16", "title": "Large Vision-Language Models as Emotion Recognizers in Context Awareness", "paper_summary": "Context-aware emotion recognition (CAER) is a complex and significant task\nthat requires perceiving emotions from various contextual cues. Previous\napproaches primarily focus on designing sophisticated architectures to extract\nemotional cues from images. However, their knowledge is confined to specific\ntraining datasets and may reflect the subjective emotional biases of the\nannotators. Furthermore, acquiring large amounts of labeled data is often\nchallenging in real-world applications. In this paper, we systematically\nexplore the potential of leveraging Large Vision-Language Models (LVLMs) to\nempower the CAER task from three paradigms: 1) We fine-tune LVLMs on two CAER\ndatasets, which is the most common way to transfer large models to downstream\ntasks. 2) We design zero-shot and few-shot patterns to evaluate the performance\nof LVLMs in scenarios with limited data or even completely unseen. In this\ncase, a training-free framework is proposed to fully exploit the In-Context\nLearning (ICL) capabilities of LVLMs. Specifically, we develop an image\nsimilarity-based ranking algorithm to retrieve examples; subsequently, the\ninstructions, retrieved examples, and the test example are combined to feed\nLVLMs to obtain the corresponding sentiment judgment. 3) To leverage the rich\nknowledge base of LVLMs, we incorporate Chain-of-Thought (CoT) into our\nframework to enhance the model's reasoning ability and provide interpretable\nresults. Extensive experiments and analyses demonstrate that LVLMs achieve\ncompetitive performance in the CAER task across different paradigms. Notably,\nthe superior performance in few-shot settings indicates the feasibility of\nLVLMs for accomplishing specific tasks without extensive training.", "paper_summary_zh": "\u60c5\u5883\u611f\u77e5\u60c5\u7dd2\u8fa8\u8b58 (CAER) \u662f\u4e00\u9805\u8907\u96dc\u4e14\u91cd\u8981\u7684\u4efb\u52d9\uff0c\u9700\u8981\u5f9e\u5404\u7a2e\u60c5\u5883\u7dda\u7d22\u4e2d\u611f\u77e5\u60c5\u7dd2\u3002\u5148\u524d\u7684\u505a\u6cd5\u4e3b\u8981\u5c08\u6ce8\u65bc\u8a2d\u8a08\u7cbe\u5bc6\u67b6\u69cb\uff0c\u5f9e\u5f71\u50cf\u4e2d\u64f7\u53d6\u60c5\u7dd2\u7dda\u7d22\u3002\u7136\u800c\uff0c\u5176\u77e5\u8b58\u50c5\u9650\u65bc\u7279\u5b9a\u8a13\u7df4\u8cc7\u6599\u96c6\uff0c\u4e14\u53ef\u80fd\u53cd\u6620\u6a19\u8a3b\u8005\u7684\u4e3b\u89c0\u60c5\u7dd2\u504f\u898b\u3002\u6b64\u5916\uff0c\u5728\u771f\u5be6\u4e16\u754c\u61c9\u7528\u4e2d\uff0c\u53d6\u5f97\u5927\u91cf\u7684\u6a19\u7c64\u8cc7\u6599\u901a\u5e38\u5177\u6709\u6311\u6230\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u5229\u7528\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u4f86\u5f37\u5316 CAER \u4efb\u52d9\u7684\u4e09\u7a2e\u7bc4\u4f8b\uff1a1) \u6211\u5011\u5c0d\u5169\u500b CAER \u8cc7\u6599\u96c6\u5fae\u8abf LVLMs\uff0c\u9019\u662f\u5c07\u5927\u578b\u6a21\u578b\u8f49\u79fb\u5230\u4e0b\u6e38\u4efb\u52d9\u6700\u5e38\u898b\u7684\u65b9\u6cd5\u30022) \u6211\u5011\u8a2d\u8a08\u96f6\u6b21\u5b78\u7fd2\u548c\u5c11\u91cf\u5b78\u7fd2\u6a21\u5f0f\uff0c\u4ee5\u8a55\u4f30 LVLMs \u5728\u8cc7\u6599\u6709\u9650\u751a\u81f3\u5b8c\u5168\u672a\u898b\u7684\u60c5\u6cc1\u4e0b\u7684\u6548\u80fd\u3002\u5728\u9019\u7a2e\u60c5\u6cc1\u4e0b\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u514d\u8a13\u7df4\u67b6\u69cb\uff0c\u4ee5\u5145\u5206\u5229\u7528 LVLMs \u7684\u60c5\u5883\u5b78\u7fd2 (ICL) \u80fd\u529b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u958b\u767c\u4e00\u500b\u57fa\u65bc\u5f71\u50cf\u76f8\u4f3c\u5ea6\u7684\u6392\u540d\u6f14\u7b97\u6cd5\u4f86\u64f7\u53d6\u7bc4\u4f8b\uff1b\u96a8\u5f8c\uff0c\u5c07\u6307\u793a\u3001\u64f7\u53d6\u7684\u7bc4\u4f8b\u548c\u6e2c\u8a66\u7bc4\u4f8b\u7d44\u5408\u8d77\u4f86\uff0c\u63d0\u4f9b\u7d66 LVLMs \u4ee5\u53d6\u5f97\u76f8\u61c9\u7684\u60c5\u7dd2\u5224\u65b7\u30023) \u70ba\u4e86\u5229\u7528 LVLMs \u8c50\u5bcc\u7684\u77e5\u8b58\u5eab\uff0c\u6211\u5011\u5c07\u601d\u8003\u93c8 (CoT) \u7d0d\u5165\u6211\u5011\u7684\u67b6\u69cb\u4e2d\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e26\u63d0\u4f9b\u53ef\u89e3\u91cb\u7684\u7d50\u679c\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u548c\u5206\u6790\u8b49\u660e\uff0cLVLMs \u5728\u4e0d\u540c\u7684\u7bc4\u4f8b\u4e2d\u5be6\u73fe\u4e86 CAER \u4efb\u52d9\u7684\u7af6\u722d\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5c11\u91cf\u5b78\u7fd2\u8a2d\u5b9a\u4e2d\u7684\u512a\u7570\u6548\u80fd\uff0c\u8868\u793a LVLMs \u7121\u9700\u5ee3\u6cdb\u8a13\u7df4\u5373\u53ef\u5b8c\u6210\u7279\u5b9a\u4efb\u52d9\u7684\u53ef\u884c\u6027\u3002", "author": "Yuxuan Lei et.al.", "authors": "Yuxuan Lei, Dingkang Yang, Zhaoyu Chen, Jiawei Chen, Peng Zhai, Lihua Zhang", "id": "2407.11300v1", "paper_url": "http://arxiv.org/abs/2407.11300v1", "repo": "null"}}