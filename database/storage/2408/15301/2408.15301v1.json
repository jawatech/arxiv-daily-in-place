{"2408.15301": {"publish_time": "2024-08-27", "title": "The Uniqueness of LLaMA3-70B with Per-Channel Quantization: An Empirical Study", "paper_summary": "We have observed a distinctive quantization-related behavior in the\nLLaMA3/3.1-70B models that is absent in both the LLaMA2-70B and\nLLaMA3/3.1-8B/405B models. Quantization is a crucial technique for deploying\nlarge language models (LLMs) efficiently. Among various bit widths and\nrepresentations for weights and activations, the 8-bit integer weight and 8-bit\ninteger activation (W8A8) configuration is particularly popular due to its\nwidespread hardware support. However, the impact of W8A8 post-training\nquantization on model accuracy remains contentious. While several studies have\nsuggested calibrating either weights or activations to mitigate accuracy\ndegradation, a comprehensive solution has yet to be identified. In this paper,\nwe empirically investigate multiple LLMs featured on an open LLM leaderboard,\ndiscovering that the LLaMA3-70B model series have a unique accuracy degradation\nbehavior with W8A8 per-channel post-training quantization. In contrast, other\nmodel series such as LLaMA2, LLaMA3-8B, Qwen, Mixtral, Mistral, Phi-3, and\nFalcon demonstrate robust performance with W8A8, sometimes surpassing their\nFP16 counterparts. Contrary to previous assertions attributing degradation to\nthe large dynamic range of activations, our findings indicate that the weight\ndistribution of the LLaMA3-70B is the primary factor behind the vulnerability.\nBy meticulously analyzing the distinct characteristics of weight distributions\nacross Transformer blocks, we propose a mixed strategy with less than 3% of the\nlayers enabling finer W8A8 quantization granularity, while the remaining 97% of\nlayers retain the per-channel configuration. As a result, the average accuracy\nof LLaMA3-70B-W8A8 is increased from 45.5% to 73.4% (just 0.7% shy of\nLLaMA3-70B-FP16) across eight reasoning tasks. Notably, our method requires\nneither calibration nor fine-tuning.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5728 LLaMA3/3.1-70B \u6a21\u578b\u4e2d\u89c0\u5bdf\u5230\u4e00\u7a2e\u7368\u7279\u7684\u91cf\u5316\u76f8\u95dc\u884c\u70ba\uff0c\u800c LLaMA2-70B \u548c LLaMA3/3.1-8B/405B \u6a21\u578b\u4e2d\u6c92\u6709\u9019\u7a2e\u884c\u70ba\u3002\u91cf\u5316\u662f\u6709\u6548\u90e8\u7f72\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u4e00\u9805\u95dc\u9375\u6280\u8853\u3002\u5728\u5404\u7a2e\u4f4d\u5143\u5bec\u5ea6\u548c\u6b0a\u91cd\u8207\u6fc0\u6d3b\u7684\u8868\u793a\u6cd5\u4e2d\uff0c8 \u4f4d\u5143\u6574\u6578\u6b0a\u91cd\u548c 8 \u4f4d\u5143\u6574\u6578\u6fc0\u6d3b (W8A8) \u7d44\u614b\u7279\u5225\u53d7\u6b61\u8fce\uff0c\u56e0\u70ba\u5b83\u5ee3\u6cdb\u7372\u5f97\u786c\u9ad4\u652f\u63f4\u3002\u7136\u800c\uff0cW8A8 \u8a13\u7df4\u5f8c\u91cf\u5316\u5c0d\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u97ff\u4ecd\u7136\u5b58\u5728\u722d\u8b70\u3002\u96d6\u7136\u6709\u5e7e\u9805\u7814\u7a76\u5efa\u8b70\u6821\u6b63\u6b0a\u91cd\u6216\u6fc0\u6d3b\u4ee5\u6e1b\u8f15\u7cbe\u5ea6\u7684\u964d\u4f4e\uff0c\u4f46\u4ecd\u672a\u627e\u51fa\u4e00\u500b\u5168\u9762\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6839\u64da\u516c\u958b\u7684 LLM \u6392\u884c\u699c\u5be6\u8b49\u8abf\u67e5\u4e86\u591a\u500b LLM\uff0c\u767c\u73fe LLaMA3-70B \u6a21\u578b\u7cfb\u5217\u5728 W8A8 \u6bcf\u901a\u9053\u8a13\u7df4\u5f8c\u91cf\u5316\u4e2d\u5177\u6709\u7368\u7279\u7684\u7cbe\u5ea6\u964d\u4f4e\u884c\u70ba\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5176\u4ed6\u6a21\u578b\u7cfb\u5217\uff0c\u4f8b\u5982 LLaMA2\u3001LLaMA3-8B\u3001Qwen\u3001Mixtral\u3001Mistral\u3001Phi-3 \u548c Falcon\uff0c\u5247\u5c55\u73fe\u51fa\u5c0d W8A8 \u7684\u5f37\u5065\u6548\u80fd\uff0c\u6709\u6642\u751a\u81f3\u8d85\u8d8a\u5176 FP16 \u5c0d\u61c9\u9805\u3002\u8207\u5148\u524d\u5c07\u964d\u4f4e\u6b78\u56e0\u65bc\u6fc0\u6d3b\u7684\u5927\u52d5\u614b\u7bc4\u570d\u7684\u4e3b\u5f35\u76f8\u53cd\uff0c\u6211\u5011\u7684\u767c\u73fe\u8868\u660e LLaMA3-70B \u7684\u6b0a\u91cd\u5206\u4f48\u662f\u9020\u6210\u6613\u53d7\u5f71\u97ff\u7684\u4e3b\u8981\u56e0\u7d20\u3002\u900f\u904e\u4ed4\u7d30\u5206\u6790 Transformer \u5340\u584a\u4e2d\u6b0a\u91cd\u5206\u4f48\u7684\u4e0d\u540c\u7279\u5fb5\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u6df7\u5408\u7b56\u7565\uff0c\u5176\u4e2d\u4e0d\u5230 3% \u7684\u5c64\u555f\u7528\u4e86\u66f4\u7cbe\u7d30\u7684 W8A8 \u91cf\u5316\u7c92\u5ea6\uff0c\u800c\u5176\u9918 97% \u7684\u5c64\u5247\u4fdd\u7559\u6bcf\u901a\u9053\u7d44\u614b\u3002\u56e0\u6b64\uff0cLLaMA3-70B-W8A8 \u7684\u5e73\u5747\u6e96\u78ba\u5ea6\u5f9e 45.5% \u63d0\u9ad8\u5230 73.4%\uff08\u50c5\u6bd4 LLaMA3-70B-FP16 \u4f4e 0.7%\uff09\uff0c\u6db5\u84cb\u516b\u9805\u63a8\u7406\u4efb\u52d9\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u65e2\u4e0d\u9700\u8981\u6821\u6b63\uff0c\u4e5f\u4e0d\u9700\u8981\u5fae\u8abf\u3002</paragraph>", "author": "Minghai Qin et.al.", "authors": "Minghai Qin", "id": "2408.15301v1", "paper_url": "http://arxiv.org/abs/2408.15301v1", "repo": "null"}}