{"2405.02583": {"publish_time": "2024-05-04", "title": "Explainable Interface for Human-Autonomy Teaming: A Survey", "paper_summary": "Nowadays, large-scale foundation models are being increasingly integrated\ninto numerous safety-critical applications, including human-autonomy teaming\n(HAT) within transportation, medical, and defence domains. Consequently, the\ninherent 'black-box' nature of these sophisticated deep neural networks\nheightens the significance of fostering mutual understanding and trust between\nhumans and autonomous systems. To tackle the transparency challenges in HAT,\nthis paper conducts a thoughtful study on the underexplored domain of\nExplainable Interface (EI) in HAT systems from a human-centric perspective,\nthereby enriching the existing body of research in Explainable Artificial\nIntelligence (XAI). We explore the design, development, and evaluation of EI\nwithin XAI-enhanced HAT systems. To do so, we first clarify the distinctions\nbetween these concepts: EI, explanations and model explainability, aiming to\nprovide researchers and practitioners with a structured understanding. Second,\nwe contribute to a novel framework for EI, addressing the unique challenges in\nHAT. Last, our summarized evaluation framework for ongoing EI offers a holistic\nperspective, encompassing model performance, human-centered factors, and group\ntask objectives. Based on extensive surveys across XAI, HAT, psychology, and\nHuman-Computer Interaction (HCI), this review offers multiple novel insights\ninto incorporating XAI into HAT systems and outlines future directions.", "paper_summary_zh": "", "author": "Xiangqi Kong et.al.", "authors": "Xiangqi Kong,Yang Xing,Antonios Tsourdos,Ziyue Wang,Weisi Guo,Adolfo Perrusquia,Andreas Wikander", "id": "2405.02583v1", "paper_url": "http://arxiv.org/abs/2405.02583v1", "repo": "null"}}