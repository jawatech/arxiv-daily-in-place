{"2406.10303": {"publish_time": "2024-06-14", "title": "A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations", "paper_summary": "Large Language Models (LLMs) have demonstrated surprising performance across\nvarious natural language processing tasks. Recently, medical LLMs enhanced with\ndomain-specific knowledge have exhibited excellent capabilities in medical\nconsultation and diagnosis. These models can smoothly simulate doctor-patient\ndialogues and provide professional medical advice. Most medical LLMs are\ndeveloped through continued training of open-source general LLMs, which require\nsignificantly fewer computational resources than training LLMs from scratch.\nAdditionally, this approach offers better protection of patient privacy\ncompared to API-based solutions. This survey systematically explores how to\ntrain medical LLMs based on general LLMs. It covers: (a) how to acquire\ntraining corpus and construct customized medical training sets, (b) how to\nchoose a appropriate training paradigm, (c) how to choose a suitable evaluation\nbenchmark, and (d) existing challenges and promising future research directions\nare discussed. This survey can provide guidance for the development of LLMs\nfocused on various medical applications, such as medical education, diagnostic\nplanning, and clinical assistants.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u9a5a\u4eba\u7684\u8868\u73fe\u3002\u6700\u8fd1\uff0c\u589e\u5f37\u4e86\u7279\u5b9a\u9818\u57df\u77e5\u8b58\u7684\u91ab\u7642 LLM \u5df2\u5728\u91ab\u7642\u8aee\u8a62\u548c\u8a3a\u65b7\u65b9\u9762\u5c55\u73fe\u51fa\u512a\u7570\u7684\u80fd\u529b\u3002\u9019\u4e9b\u6a21\u578b\u53ef\u4ee5\u9806\u5229\u6a21\u64ec\u91ab\u75c5\u5c0d\u8a71\u4e26\u63d0\u4f9b\u5c08\u696d\u7684\u91ab\u7642\u5efa\u8b70\u3002\u5927\u591a\u6578\u91ab\u7642 LLM \u90fd\u662f\u900f\u904e\u6301\u7e8c\u8a13\u7df4\u958b\u6e90\u7684\u4e00\u822c LLM \u800c\u958b\u767c\u7684\uff0c\u8207\u5f9e\u982d\u8a13\u7df4 LLM \u76f8\u6bd4\uff0c\u6240\u9700\u904b\u7b97\u8cc7\u6e90\u660e\u986f\u8f03\u5c11\u3002\u6b64\u5916\uff0c\u8207\u57fa\u65bc API \u7684\u89e3\u6c7a\u65b9\u6848\u76f8\u6bd4\uff0c\u9019\u7a2e\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u75c5\u60a3\u96b1\u79c1\u4fdd\u8b77\u3002\u9019\u9805\u8abf\u67e5\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u5982\u4f55\u6839\u64da\u4e00\u822c LLM \u8a13\u7df4\u91ab\u7642 LLM\u3002\u5b83\u6db5\u84cb\uff1a(a) \u5982\u4f55\u53d6\u5f97\u8a13\u7df4\u8a9e\u6599\u5eab\u4e26\u5efa\u69cb\u5ba2\u88fd\u5316\u7684\u91ab\u7642\u8a13\u7df4\u7d44\uff0c(b) \u5982\u4f55\u9078\u64c7\u9069\u7576\u7684\u8a13\u7df4\u7bc4\u4f8b\uff0c(c) \u5982\u4f55\u9078\u64c7\u5408\u9069\u7684\u8a55\u91cf\u57fa\u6e96\uff0c\u4ee5\u53ca (d) \u8a0e\u8ad6\u73fe\u6709\u7684\u6311\u6230\u548c\u6709\u524d\u666f\u7684\u672a\u4f86\u7814\u7a76\u65b9\u5411\u3002\u9019\u9805\u8abf\u67e5\u53ef\u4ee5\u70ba\u91dd\u5c0d\u5404\u7a2e\u91ab\u7642\u61c9\u7528\uff08\u4f8b\u5982\u91ab\u5b78\u6559\u80b2\u3001\u8a3a\u65b7\u898f\u5283\u548c\u81e8\u5e8a\u52a9\u7406\uff09\u7684 LLM \u958b\u767c\u63d0\u4f9b\u6307\u5c0e\u3002", "author": "Jinqiang Wang et.al.", "authors": "Jinqiang Wang, Huansheng Ning, Yi Peng, Qikai Wei, Daniel Tesfai, Wenwei Mao, Tao Zhu, Runhe Huang", "id": "2406.10303v1", "paper_url": "http://arxiv.org/abs/2406.10303v1", "repo": "null"}}