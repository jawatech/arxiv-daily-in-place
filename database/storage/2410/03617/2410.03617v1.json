{"2410.03617": {"publish_time": "2024-10-04", "title": "What Matters for Model Merging at Scale?", "paper_summary": "Model merging aims to combine multiple expert models into a more capable\nsingle model, offering benefits such as reduced storage and serving costs,\nimproved generalization, and support for decentralized model development.\nDespite its promise, previous studies have primarily focused on merging a few\nsmall models. This leaves many unanswered questions about the effect of scaling\nmodel size and how it interplays with other key factors -- like the base model\nquality and number of expert models -- , to affect the merged model's\nperformance. This work systematically evaluates the utility of model merging at\nscale, examining the impact of these different factors. We experiment with\nmerging fully fine-tuned models using 4 popular merging methods -- Averaging,\nTask~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B\nparameters and merging up to 8 different expert models. We evaluate the merged\nmodels on both held-in tasks, i.e., the expert's training tasks, and zero-shot\ngeneralization to unseen held-out tasks. Our experiments provide several new\ninsights about model merging at scale and the interplay between different\nfactors. First, we find that merging is more effective when experts are created\nfrom strong base models, i.e., models with good zero-shot performance. Second,\nlarger models facilitate easier merging. Third merging consistently improves\ngeneralization capabilities. Notably, when merging 8 large expert models, the\nmerged models often generalize better compared to the multitask trained models.\nFourth, we can better merge more expert models when working with larger models.\nFifth, different merging methods behave very similarly at larger scales.\nOverall, our findings shed light on some interesting properties of model\nmerging while also highlighting some limitations. We hope that this study will\nserve as a reference point on large-scale merging for upcoming research.", "paper_summary_zh": "\u6a21\u578b\u5408\u4f75\u65e8\u5728\u5c07\u591a\u500b\u5c08\u5bb6\u6a21\u578b\u5408\u4f75\u6210\u4e00\u500b\u66f4\u5f37\u5927\u7684\u55ae\u4e00\u6a21\u578b\uff0c\u63d0\u4f9b\u8af8\u5982\u964d\u4f4e\u5132\u5b58\u548c\u670d\u52d9\u6210\u672c\u3001\u6539\u5584\u6cdb\u5316\u4ee5\u53ca\u652f\u63f4\u5206\u6563\u5f0f\u6a21\u578b\u958b\u767c\u7b49\u597d\u8655\u3002\u5118\u7ba1\u6709\u5176\u627f\u8afe\uff0c\u4f46\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5408\u4f75\u5c11\u6578\u5c0f\u578b\u6a21\u578b\u3002\u9019\u7559\u4e0b\u4e86\u8a31\u591a\u95dc\u65bc\u64f4\u5145\u6a21\u578b\u898f\u6a21\u7684\u5f71\u97ff\u4ee5\u53ca\u5b83\u5982\u4f55\u8207\u5176\u4ed6\u95dc\u9375\u56e0\u7d20\uff08\u4f8b\u5982\u57fa\u672c\u6a21\u578b\u54c1\u8cea\u548c\u5c08\u5bb6\u6a21\u578b\u6578\u91cf\uff09\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u5f71\u97ff\u5408\u4f75\u6a21\u578b\u6548\u80fd\u7684\u672a\u89e3\u554f\u984c\u3002\u9019\u9805\u5de5\u4f5c\u7cfb\u7d71\u6027\u5730\u8a55\u4f30\u4e86\u6a21\u578b\u5408\u4f75\u5728\u898f\u6a21\u4e0a\u7684\u6548\u7528\uff0c\u6aa2\u8996\u4e86\u9019\u4e9b\u4e0d\u540c\u56e0\u7d20\u7684\u5f71\u97ff\u3002\u6211\u5011\u4f7f\u7528 4 \u7a2e\u6d41\u884c\u7684\u5408\u4f75\u65b9\u6cd5\uff08\u5e73\u5747\u3001Task~Arithmetic\u3001Dare \u548c TIES\uff09\u5c0d\u7d93\u904e\u5145\u5206\u5fae\u8abf\u7684\u6a21\u578b\u9032\u884c\u5408\u4f75\u5be6\u9a57\uff0c\u6a21\u578b\u898f\u6a21\u7bc4\u570d\u5f9e 1B-64B \u53c3\u6578\uff0c\u4e26\u5408\u4f75\u591a\u9054 8 \u500b\u4e0d\u540c\u7684\u5c08\u5bb6\u6a21\u578b\u3002\u6211\u5011\u5728\u5169\u7a2e\u4fdd\u7559\u4efb\u52d9\uff08\u5373\u5c08\u5bb6\u7684\u8a13\u7df4\u4efb\u52d9\uff09\u4e0a\u8a55\u4f30\u5408\u4f75\u6a21\u578b\uff0c\u4ee5\u53ca\u5c0d\u672a\u898b\u4fdd\u7559\u4efb\u52d9\u9032\u884c\u96f6\u6b21\u5b78\u7fd2\u6cdb\u5316\u3002\u6211\u5011\u7684\u5be6\u9a57\u63d0\u4f9b\u4e86\u95dc\u65bc\u898f\u6a21\u5316\u6a21\u578b\u5408\u4f75\u548c\u4e0d\u540c\u56e0\u7d20\u4e4b\u9593\u76f8\u4e92\u4f5c\u7528\u7684\u5e7e\u500b\u65b0\u898b\u89e3\u3002\u9996\u5148\uff0c\u6211\u5011\u767c\u73fe\u7576\u5c08\u5bb6\u662f\u7531\u5f37\u5927\u7684\u57fa\u672c\u6a21\u578b\uff08\u5373\u5177\u6709\u826f\u597d\u96f6\u6b21\u5b78\u7fd2\u6548\u80fd\u7684\u6a21\u578b\uff09\u5efa\u7acb\u6642\uff0c\u5408\u4f75\u66f4\u6709\u6548\u3002\u5176\u6b21\uff0c\u8f03\u5927\u7684\u6a21\u578b\u6709\u52a9\u65bc\u66f4\u8f15\u9b06\u5730\u5408\u4f75\u3002\u7b2c\u4e09\uff0c\u5408\u4f75\u59cb\u7d42\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u5408\u4f75 8 \u500b\u5927\u578b\u5c08\u5bb6\u6a21\u578b\u6642\uff0c\u5408\u4f75\u6a21\u578b\u901a\u5e38\u6bd4\u591a\u4efb\u52d9\u8a13\u7df4\u6a21\u578b\u6cdb\u5316\u5f97\u66f4\u597d\u3002\u7b2c\u56db\uff0c\u5728\u8655\u7406\u8f03\u5927\u578b\u6a21\u578b\u6642\uff0c\u6211\u5011\u53ef\u4ee5\u66f4\u597d\u5730\u5408\u4f75\u66f4\u591a\u5c08\u5bb6\u6a21\u578b\u3002\u7b2c\u4e94\uff0c\u4e0d\u540c\u7684\u5408\u4f75\u65b9\u6cd5\u5728\u8f03\u5927\u898f\u6a21\u6642\u8868\u73fe\u975e\u5e38\u76f8\u4f3c\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u6a21\u578b\u5408\u4f75\u7684\u4e00\u4e9b\u6709\u8da3\u7279\u6027\uff0c\u540c\u6642\u4e5f\u7a81\u51fa\u4e86\u67d0\u4e9b\u9650\u5236\u3002\u6211\u5011\u5e0c\u671b\u9019\u9805\u7814\u7a76\u5c07\u4f5c\u70ba\u5373\u5c07\u9032\u884c\u7684\u7814\u7a76\u4e2d\u5927\u898f\u6a21\u5408\u4f75\u7684\u53c3\u8003\u9ede\u3002", "author": "Prateek Yadav et.al.", "authors": "Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, Tsendsuren Munkhdalai", "id": "2410.03617v1", "paper_url": "http://arxiv.org/abs/2410.03617v1", "repo": "null"}}