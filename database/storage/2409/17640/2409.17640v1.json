{"2409.17640": {"publish_time": "2024-09-26", "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training on an Assistant Task for a Target Task", "paper_summary": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.", "paper_summary_zh": "\u9577\u6587\u6458\u8981\u9010\u6f38\u6210\u70ba\u6709\u6548\u8655\u7406\u5927\u91cf\u8cc7\u8a0a\u7684\u5fc5\u8981\u689d\u4ef6\uff0c\u5c0d\u65bc GPT \u548c LLaMA \u5bb6\u65cf\u7b49\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u8aaa\u4ecd\u7136\u662f\u4e00\u9805\u6311\u6230\uff0c\u56e0\u70ba\u958b\u653e\u539f\u59cb\u78bc\u8a13\u7df4\u8cc7\u6599\u96c6\u4e0d\u8db3\uff0c\u800c\u4e14\u9700\u8981\u8655\u7406\u5927\u91cf\u8108\u7d61\u7d30\u7bc0\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u96f6\u6b21\u65b9\u8f49\u79fb\u5b78\u7fd2\u67b6\u69cb\uff0c\u7c21\u7a31\u70ba T3\uff0c\u7528\u65bc\u5728\u8f14\u52a9\u4efb\u52d9\u4e0a\u53cd\u8986\u8a13\u7df4\u57fa\u6e96 LLM \u4ee5\u57f7\u884c\u76ee\u6a19\u4efb\u52d9\uff0c\u5176\u4e2d\u524d\u8005\u61c9\u8a72\u64c1\u6709\u66f4\u8c50\u5bcc\u7684\u8cc7\u6599\u8cc7\u6e90\uff0c\u4e26\u4e14\u8207\u5f8c\u8005\u5171\u4eab\u7d50\u69cb\u6216\u8a9e\u7fa9\u76f8\u4f3c\u6027\u3002\u5728\u5be6\u52d9\u4e0a\uff0cT3 \u900f\u904e\u5229\u7528\u554f\u984c\u89e3\u7b54\u4f5c\u70ba\u8f14\u52a9\u4efb\u52d9\u4f86\u8655\u7406\u9577\u6587\u6458\u8981\u4efb\u52d9\uff0c\u4e26\u9032\u4e00\u6b65\u9a57\u8b49\u5176\u5728 BBC summary\u3001NarraSum\u3001FairytaleQA \u548c NLQuAD \u8cc7\u6599\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u8207\u4e09\u500b\u57fa\u6e96 LLM \u76f8\u6bd4\uff0cROUGE \u63d0\u5347\u4e86\u8fd1 14%\u3001BLEU \u63d0\u5347\u4e86 35%\uff0cFactscore \u63d0\u5347\u4e86 16%\uff0c\u986f\u793a\u5176\u5728\u66f4\u591a\u8f14\u52a9\u76ee\u6a19\u4efb\u52d9\u7d44\u5408\u4e2d\u7684\u6f5b\u529b\u3002", "author": "Xindi Tong et.al.", "authors": "Xindi Tong, Yujin Zhu, Shijian Fan, Liang Xu", "id": "2409.17640v1", "paper_url": "http://arxiv.org/abs/2409.17640v1", "repo": "null"}}