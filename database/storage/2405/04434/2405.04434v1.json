{"2405.04434": {"publish_time": "2024-05-07", "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model", "paper_summary": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model\ncharacterized by economical training and efficient inference. It comprises 236B\ntotal parameters, of which 21B are activated for each token, and supports a\ncontext length of 128K tokens. DeepSeek-V2 adopts innovative architectures\nincluding Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees\nefficient inference through significantly compressing the Key-Value (KV) cache\ninto a latent vector, while DeepSeekMoE enables training strong models at an\neconomical cost through sparse computation. Compared with DeepSeek 67B,\nDeepSeek-V2 achieves significantly stronger performance, and meanwhile saves\n42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum\ngeneration throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality\nand multi-source corpus consisting of 8.1T tokens, and further perform\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock\nits potential. Evaluation results show that, even with only 21B activated\nparameters, DeepSeek-V2 and its chat versions still achieve top-tier\nperformance among open-source models. The model checkpoints are available at\n\"https://github.com/deepseek-ai/DeepSeek-V2\".", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa DeepSeek-V2\uff0c\u4e00\u7a2e\u5f37\u5927\u7684\u6df7\u5408\u5c08\u5bb6 (MoE) \u8a9e\u8a00\u6a21\u578b\uff0c\u5176\u7279\u9ede\u662f\u7d93\u6fdf\u8a13\u7df4\u548c\u9ad8\u6548\u63a8\u8ad6\u3002\u5b83\u5305\u542b 236B \u500b\u7e3d\u53c3\u6578\uff0c\u5176\u4e2d 21B \u500b\u88ab\u6fc0\u6d3b\u7528\u65bc\u6bcf\u500b\u7b26\u865f\uff0c\u4e26\u652f\u63f4 128K \u500b\u7b26\u865f\u7684\u4e0a\u4e0b\u6587\u9577\u5ea6\u3002DeepSeek-V2 \u63a1\u7528\u5275\u65b0\u7684\u67b6\u69cb\uff0c\u5305\u62ec\u591a\u982d\u6f5b\u5728\u6ce8\u610f\u529b (MLA) \u548c DeepSeekMoE\u3002MLA \u900f\u904e\u5c07\u9375\u503c (KV) \u5feb\u53d6\u5927\u5e45\u58d3\u7e2e\u5230\u6f5b\u5728\u5411\u91cf\u4e2d\uff0c\u4f86\u4fdd\u8b49\u9ad8\u6548\u63a8\u8ad6\uff0c\u800c DeepSeekMoE \u900f\u904e\u7a00\u758f\u8a08\u7b97\uff0c\u4ee5\u7d93\u6fdf\u6210\u672c\u8a13\u7df4\u5f37\u5927\u7684\u6a21\u578b\u3002\u8207 DeepSeek 67B \u76f8\u6bd4\uff0cDeepSeek-V2 \u9054\u5230\u4e86\u986f\u8457\u66f4\u5f37\u7684\u6548\u80fd\uff0c\u540c\u6642\u7bc0\u7701\u4e86 42.5% \u7684\u8a13\u7df4\u6210\u672c\uff0c\u5c07 KV \u5feb\u53d6\u6e1b\u5c11\u4e86 93.3%\uff0c\u4e26\u5c07\u6700\u5927\u751f\u6210\u91cf\u63d0\u5347\u81f3 5.76 \u500d\u3002\u6211\u5011\u5728\u4e00\u500b\u7531 8.1T \u500b\u7b26\u865f\u7d44\u6210\u7684\u9ad8\u54c1\u8cea\u548c\u591a\u4f86\u6e90\u8a9e\u6599\u5eab\u4e0a\u9810\u8a13\u7df4 DeepSeek-V2\uff0c\u4e26\u9032\u4e00\u6b65\u57f7\u884c\u76e3\u7763\u5fae\u8abf (SFT) \u548c\u5f37\u5316\u5b78\u7fd2 (RL) \u4ee5\u5145\u5206\u767c\u63ee\u5176\u6f5b\u529b\u3002\u8a55\u4f30\u7d50\u679c\u986f\u793a\uff0c\u5373\u4f7f\u53ea\u6709 21B \u500b\u5df2\u555f\u7528\u7684\u53c3\u6578\uff0cDeepSeek-V2 \u53ca\u5176\u804a\u5929\u7248\u672c\u4ecd\u53ef\u5728\u958b\u6e90\u6a21\u578b\u4e2d\u9054\u5230\u9802\u7d1a\u6548\u80fd\u3002\u6a21\u578b\u6aa2\u67e5\u9ede\u53ef\u5728\u300chttps://github.com/deepseek-ai/DeepSeek-V2\u300d\u53d6\u5f97\u3002", "author": "DeepSeek-AI et.al.", "authors": "DeepSeek-AI", "id": "2405.04434v1", "paper_url": "http://arxiv.org/abs/2405.04434v1", "repo": "https://github.com/deepseek-ai/deepseek-v2"}}