{"2501.14035": {"publish_time": "2025-01-23", "title": "Human-Alignment Influences the Utility of AI-assisted Decision Making", "paper_summary": "Whenever an AI model is used to predict a relevant (binary) outcome in\nAI-assisted decision making, it is widely agreed that, together with each\nprediction, the model should provide an AI confidence value. However, it has\nbeen unclear why decision makers have often difficulties to develop a good\nsense on when to trust a prediction using AI confidence values. Very recently,\nCorvelo Benz and Gomez Rodriguez have argued that, for rational decision\nmakers, the utility of AI-assisted decision making is inherently bounded by the\ndegree of alignment between the AI confidence values and the decision maker's\nconfidence on their own predictions. In this work, we empirically investigate\nto what extent the degree of alignment actually influences the utility of\nAI-assisted decision making. To this end, we design and run a large-scale human\nsubject study (n=703) where participants solve a simple decision making task -\nan online card game - assisted by an AI model with a steerable degree of\nalignment. Our results show a positive association between the degree of\nalignment and the utility of AI-assisted decision making. In addition, our\nresults also show that post-processing the AI confidence values to achieve\nmulticalibration with respect to the participants' confidence on their own\npredictions increases both the degree of alignment and the utility of\nAI-assisted decision making.", "paper_summary_zh": "\u6bcf\u7576 AI \u6a21\u578b\u7528\u65bc\u9810\u6e2c AI \u8f14\u52a9\u6c7a\u7b56\u4e2d\u7684\u76f8\u95dc\uff08\u4e8c\u5143\uff09\u7d50\u679c\u6642\uff0c\u4eba\u5011\u666e\u904d\u540c\u610f\uff0c\u9664\u4e86\u6bcf\u500b\u9810\u6e2c\u5916\uff0c\u6a21\u578b\u9084\u61c9\u63d0\u4f9b AI \u4fe1\u5fc3\u503c\u3002\u7136\u800c\uff0c\u4e00\u76f4\u4e0d\u6e05\u695a\u70ba\u4f55\u6c7a\u7b56\u8005\u7d93\u5e38\u96e3\u4ee5\u57f9\u990a\u5728\u4f55\u6642\u4f7f\u7528 AI \u4fe1\u5fc3\u503c\u4f86\u4fe1\u4efb\u9810\u6e2c\u7684\u826f\u597d\u610f\u8b58\u3002\u6700\u8fd1\uff0cCorvelo Benz \u548c Gomez Rodriguez \u63d0\u51fa\uff0c\u5c0d\u65bc\u7406\u6027\u6c7a\u7b56\u8005\u800c\u8a00\uff0cAI \u8f14\u52a9\u6c7a\u7b56\u7684\u6548\u7528\u672c\u8cea\u4e0a\u53d7\u9650\u65bc AI \u4fe1\u5fc3\u503c\u8207\u6c7a\u7b56\u8005\u5c0d\u81ea\u5df1\u9810\u6e2c\u7684\u4fe1\u5fc3\u4e4b\u9593\u7684\u4e00\u81f4\u6027\u7a0b\u5ea6\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5be6\u8b49\u7814\u7a76\u4e86\u4e00\u81f4\u6027\u7a0b\u5ea6\u5be6\u969b\u4e0a\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u5f71\u97ff AI \u8f14\u52a9\u6c7a\u7b56\u7684\u6548\u7528\u3002\u70ba\u6b64\uff0c\u6211\u5011\u8a2d\u8a08\u4e26\u57f7\u884c\u4e86\u4e00\u9805\u5927\u898f\u6a21\u7684\u4eba\u985e\u53d7\u8a66\u8005\u7814\u7a76\uff08n=703\uff09\uff0c\u53c3\u8207\u8005\u5728 AI \u6a21\u578b\u7684\u5354\u52a9\u4e0b\u89e3\u6c7a\u4e00\u500b\u7c21\u55ae\u7684\u6c7a\u7b56\u4efb\u52d9\u2014\u2014\u4e00\u6b3e\u7dda\u4e0a\u7d19\u724c\u904a\u6232\uff0c\u5176\u4e2d AI \u6a21\u578b\u7684\u4e00\u81f4\u6027\u7a0b\u5ea6\u53ef\u63a7\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\u4e00\u81f4\u6027\u7a0b\u5ea6\u8207 AI \u8f14\u52a9\u6c7a\u7b56\u7684\u6548\u7528\u4e4b\u9593\u5b58\u5728\u6b63\u76f8\u95dc\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7d50\u679c\u9084\u986f\u793a\uff0c\u5c0d AI \u4fe1\u5fc3\u503c\u9032\u884c\u5f8c\u8655\u7406\u4ee5\u5be6\u73fe\u76f8\u5c0d\u65bc\u53c3\u8207\u8005\u5c0d\u81ea\u5df1\u9810\u6e2c\u7684\u4fe1\u5fc3\u7684\u591a\u6821\u6e96\uff0c\u65e2\u80fd\u63d0\u9ad8\u4e00\u81f4\u6027\u7a0b\u5ea6\uff0c\u53c8\u80fd\u63d0\u9ad8 AI \u8f14\u52a9\u6c7a\u7b56\u7684\u6548\u7528\u3002", "author": "Nina L. Corvelo Benz et.al.", "authors": "Nina L. Corvelo Benz, Manuel Gomez Rodriguez", "id": "2501.14035v1", "paper_url": "http://arxiv.org/abs/2501.14035v1", "repo": "https://github.com/networks-learning/human-alignment-study"}}