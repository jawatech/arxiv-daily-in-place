{"2410.22211": {"publish_time": "2024-10-29", "title": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity Understanding", "paper_summary": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities\ncoupled with their corresponding instruction. For QA annotation, we take a\ncost-effective human-LLM collaborative approach, where the existing annotation\nis augmented with LLM-generated QA pairs that are later verified by humans. We\nthen provide the benchmark results to set the baseline performance on ProMQA.\nOur experiment reveals a significant gap between human performance and that of\ncurrent systems, including competitive proprietary multimodal models. We hope\nour dataset sheds light on new aspects of models' multimodal understanding\ncapabilities.", "paper_summary_zh": "\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u7a0b\u5e8f\u5316\u6d3b\u52a8\u4e2d\u534f\u52a9\u4eba\u7c7b\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u5728\u8fd9\u4e9b\u6d3b\u52a8\u4e2d\uff0c\u4eba\u4eec\u9075\u5faa\u6307\u4ee4\u6765\u5b9e\u73b0\u76ee\u6807\u3002\u5c3d\u7ba1\u6709\u4e0d\u540c\u7684\u5e94\u7528\u573a\u666f\uff0c\u4f46\u7cfb\u7edf\u901a\u5e38\u5728\u4f20\u7edf\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f8b\u5982\u52a8\u4f5c\u8bc6\u522b\u6216\u65f6\u95f4\u52a8\u4f5c\u5206\u5272\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u8bc4\u4f30\u6570\u636e\u96c6 ProMQA\uff0c\u7528\u4e8e\u8861\u91cf\u7cfb\u7edf\u5728\u9762\u5411\u5e94\u7528\u7a0b\u5e8f\u7684\u573a\u666f\u4e2d\u7684\u8fdb\u6b65\u3002ProMQA \u7531 401 \u4e2a\u591a\u6a21\u6001\u7a0b\u5e8f\u5316\u95ee\u7b54\u5bf9\u7ec4\u6210\uff0c\u5176\u4e2d\u5305\u542b\u7528\u6237\u8bb0\u5f55\u7684\u7a0b\u5e8f\u5316\u6d3b\u52a8\u548c\u76f8\u5e94\u7684\u6307\u4ee4\u3002\u5bf9\u4e8e\u95ee\u7b54\u6ce8\u91ca\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u4eba\u5de5-LLM \u534f\u4f5c\u65b9\u6cd5\uff0c\u5176\u4e2d\u73b0\u6709\u7684\u6ce8\u91ca\u901a\u8fc7 LLM \u751f\u6210\u7684\u95ee\u7b54\u5bf9\u8fdb\u884c\u6269\u5145\uff0c\u7136\u540e\u7531\u4eba\u5de5\u9a8c\u8bc1\u3002\u7136\u540e\uff0c\u6211\u4eec\u63d0\u4f9b\u57fa\u51c6\u7ed3\u679c\u4ee5\u8bbe\u5b9a ProMQA \u7684\u57fa\u7ebf\u6027\u80fd\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u4eba\u7c7b\u8868\u73b0\u4e0e\u5f53\u524d\u7cfb\u7edf\uff08\u5305\u62ec\u6709\u7ade\u4e89\u529b\u7684\u4e13\u6709\u591a\u6a21\u6001\u6a21\u578b\uff09\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u5e0c\u671b\u6211\u4eec\u7684\u6570\u636e\u96c6\u80fd\u4e3a\u6a21\u578b\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u7684\u65b0\u65b9\u9762\u63d0\u4f9b\u542f\u793a\u3002", "author": "Kimihiro Hasegawa et.al.", "authors": "Kimihiro Hasegawa, Wiradee Imrattanatrai, Zhi-Qi Cheng, Masaki Asada, Susan Holm, Yuran Wang, Ken Fukuda, Teruko Mitamura", "id": "2410.22211v1", "paper_url": "http://arxiv.org/abs/2410.22211v1", "repo": "https://github.com/kimihiroh/promqa"}}