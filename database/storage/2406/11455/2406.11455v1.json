{"2406.11455": {"publish_time": "2024-06-17", "title": "Adaptive Reinforcement Learning Planning: Harnessing Large Language Models for Complex Information Extraction", "paper_summary": "Existing research on large language models (LLMs) shows that they can solve\ninformation extraction tasks through multi-step planning. However, their\nextraction behavior on complex sentences and tasks is unstable, emerging issues\nsuch as false positives and missing elements. We observe that decomposing\ncomplex extraction tasks and extracting them step by step can effectively\nimprove LLMs' performance, and the extraction orders of entities significantly\naffect the final results of LLMs. This paper proposes a two-stage multi-step\nmethod for LLM-based information extraction and adopts the RL framework to\nexecute the multi-step planning. We regard sequential extraction as a Markov\ndecision process, build an LLM-based extraction environment, design a decision\nmodule to adaptively provide the optimal order for sequential entity extraction\non different sentences, and utilize the DDQN algorithm to train the decision\nmodel. We also design the rewards and evaluation metrics suitable for the\nextraction results of LLMs. We conduct extensive experiments on multiple public\ndatasets to demonstrate the effectiveness of our method in improving the\ninformation extraction capabilities of LLMs.", "paper_summary_zh": "\u73fe\u6709\u7684\u5927\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7814\u7a76\u8868\u660e\uff0c\u5b83\u5011\u53ef\u4ee5\u900f\u904e\u591a\u6b65\u9a5f\u898f\u5283\u4f86\u89e3\u6c7a\u8cc7\u8a0a\u8403\u53d6\u4efb\u52d9\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u8907\u96dc\u53e5\u5b50\u548c\u4efb\u52d9\u4e0a\u7684\u8403\u53d6\u884c\u70ba\u4e26\u4e0d\u7a69\u5b9a\uff0c\u51fa\u73fe\u4e86\u8af8\u5982\u5047\u967d\u6027\u548c\u907a\u6f0f\u5143\u7d20\u7b49\u554f\u984c\u3002\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u5c07\u8907\u96dc\u7684\u8403\u53d6\u4efb\u52d9\u5206\u89e3\u4e26\u9010\u6b65\u8403\u53d6\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u5347 LLM \u7684\u6548\u80fd\uff0c\u800c\u5be6\u9ad4\u7684\u8403\u53d6\u9806\u5e8f\u6703\u986f\u8457\u5f71\u97ff LLM \u7684\u6700\u7d42\u7d50\u679c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc LLM \u7684\u8cc7\u8a0a\u8403\u53d6\u7684\u5169\u968e\u6bb5\u591a\u6b65\u9a5f\u65b9\u6cd5\uff0c\u4e26\u63a1\u7528 RL \u6846\u67b6\u4f86\u57f7\u884c\u591a\u6b65\u9a5f\u898f\u5283\u3002\u6211\u5011\u5c07\u9806\u5e8f\u8403\u53d6\u8996\u70ba\u4e00\u500b\u99ac\u53ef\u592b\u6c7a\u7b56\u904e\u7a0b\uff0c\u5efa\u7acb\u4e00\u500b\u57fa\u65bc LLM \u7684\u8403\u53d6\u74b0\u5883\uff0c\u8a2d\u8a08\u4e00\u500b\u6c7a\u7b56\u6a21\u7d44\uff0c\u4ee5\u81ea\u9069\u61c9\u7684\u65b9\u5f0f\u63d0\u4f9b\u4e0d\u540c\u53e5\u5b50\u4e2d\u9806\u5e8f\u5be6\u9ad4\u8403\u53d6\u7684\u6700\u4f73\u9806\u5e8f\uff0c\u4e26\u5229\u7528 DDQN \u6f14\u7b97\u6cd5\u4f86\u8a13\u7df4\u6c7a\u7b56\u6a21\u578b\u3002\u6211\u5011\u9084\u8a2d\u8a08\u4e86\u9069\u5408 LLM \u8403\u53d6\u7d50\u679c\u7684\u734e\u52f5\u548c\u8a55\u4f30\u6307\u6a19\u3002\u6211\u5011\u5728\u591a\u500b\u516c\u958b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4ee5\u8b49\u660e\u6211\u5011\u7684\u65b9\u6cd5\u5728\u63d0\u5347 LLM \u7684\u8cc7\u8a0a\u8403\u53d6\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Zepeng Ding et.al.", "authors": "Zepeng Ding, Ruiyang Ke, Wenhao Huang, Guochao Jiang, Yanda Li, Deqing Yang, Yanghua Xiao, Jiaqing Liang", "id": "2406.11455v1", "paper_url": "http://arxiv.org/abs/2406.11455v1", "repo": "null"}}