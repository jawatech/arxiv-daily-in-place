{"2409.12865": {"publish_time": "2024-09-19", "title": "KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning", "paper_summary": "Knowledge graph reasoning plays a vital role in various applications and has\ngarnered considerable attention. Recently, path-based methods have achieved\nimpressive performance. However, they may face limitations stemming from\nconstraints in message-passing neural networks, such as missing paths and\ninformation over-squashing. In this paper, we revisit the application of\ntransformers for knowledge graph reasoning to address the constraints faced by\npath-based methods and propose a novel method KnowFormer.KnowFormer utilizes a\ntransformer architecture to perform reasoning on knowledge graphs from the\nmessage-passing perspective, rather than reasoning by textual information like\nprevious pretrained language model based methods. Specifically, we define the\nattention computation based on the query prototype of knowledge graph\nreasoning, facilitating convenient construction and efficient optimization. To\nincorporate structural information into the self-attention mechanism, we\nintroduce structure-aware modules to calculate query, key, and value\nrespectively. Additionally, we present an efficient attention computation\nmethod for better scalability. Experimental results demonstrate the superior\nperformance of KnowFormer compared to prominent baseline methods on both\ntransductive and inductive benchmarks.", "paper_summary_zh": "\u77e5\u8b58\u5716\u8b5c\u63a8\u7406\u5728\u5404\u7a2e\u61c9\u7528\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\uff0c\u4e26\u5df2\u7372\u5f97\u76f8\u7576\u5927\u7684\u95dc\u6ce8\u3002\u6700\u8fd1\uff0c\u57fa\u65bc\u8def\u5f91\u7684\u65b9\u6cd5\u5df2\u53d6\u5f97\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8868\u73fe\u3002\u7136\u800c\uff0c\u5b83\u5011\u53ef\u80fd\u6703\u9762\u81e8\u6e90\u81ea\u8a0a\u606f\u50b3\u905e\u795e\u7d93\u7db2\u8def\u4e2d\u7684\u9650\u5236\uff0c\u4f8b\u5982\u8def\u5f91\u907a\u5931\u548c\u8cc7\u8a0a\u904e\u5ea6\u58d3\u7e2e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u91cd\u65b0\u63a2\u8a0eTransformer\u5728\u77e5\u8b58\u5716\u8b5c\u63a8\u7406\u4e2d\u7684\u61c9\u7528\uff0c\u4ee5\u89e3\u6c7a\u57fa\u65bc\u8def\u5f91\u7684\u65b9\u6cd5\u6240\u9762\u81e8\u7684\u9650\u5236\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u65b9\u6cd5 KnowFormer\u3002KnowFormer \u5229\u7528Transformer\u67b6\u69cb\u5f9e\u8a0a\u606f\u50b3\u905e\u7684\u89d2\u5ea6\u5c0d\u77e5\u8b58\u5716\u8b5c\u57f7\u884c\u63a8\u7406\uff0c\u800c\u4e0d\u662f\u50cf\u4e4b\u524d\u7684\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u6240\u4f9d\u8cf4\u7684\u6587\u672c\u8cc7\u8a0a\u90a3\u6a23\u9032\u884c\u63a8\u7406\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u6839\u64da\u77e5\u8b58\u5716\u8b5c\u63a8\u7406\u7684\u67e5\u8a62\u539f\u578b\u5b9a\u7fa9\u6ce8\u610f\u529b\u7684\u904b\u7b97\uff0c\u4fc3\u9032\u4fbf\u5229\u7684\u5efa\u69cb\u548c\u6709\u6548\u7684\u6700\u4f73\u5316\u3002\u70ba\u4e86\u5c07\u7d50\u69cb\u8cc7\u8a0a\u7d0d\u5165\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u6211\u5011\u5f15\u5165\u7d50\u69cb\u611f\u77e5\u6a21\u7d44\u4f86\u5206\u5225\u8a08\u7b97\u67e5\u8a62\u3001\u9375\u548c\u503c\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u6709\u6548\u7387\u7684\u6ce8\u610f\u529b\u904b\u7b97\u65b9\u6cd5\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u53ef\u64f4\u5145\u6027\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0cKnowFormer \u5728\u8f49\u5c0e\u548c\u6b78\u7d0d\u57fa\u6e96\u4e0a\u90fd\u512a\u65bc\u5091\u51fa\u7684\u57fa\u7dda\u65b9\u6cd5\u3002", "author": "Junnan Liu et.al.", "authors": "Junnan Liu, Qianren Mao, Weifeng Jiang, Jianxin Li", "id": "2409.12865v1", "paper_url": "http://arxiv.org/abs/2409.12865v1", "repo": "null"}}