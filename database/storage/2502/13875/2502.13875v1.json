{"2502.13875": {"publish_time": "2025-02-19", "title": "MEX: Memory-efficient Approach to Referring Multi-Object Tracking", "paper_summary": "Referring Multi-Object Tracking (RMOT) is a relatively new concept that has\nrapidly gained traction as a promising research direction at the intersection\nof computer vision and natural language processing. Unlike traditional\nmulti-object tracking, RMOT identifies and tracks objects and incorporates\ntextual descriptions for object class names, making the approach more\nintuitive. Various techniques have been proposed to address this challenging\nproblem; however, most require the training of the entire network due to their\nend-to-end nature. Among these methods, iKUN has emerged as a particularly\npromising solution. Therefore, we further explore its pipeline and enhance its\nperformance. In this paper, we introduce a practical module dubbed\nMemory-Efficient Cross-modality -- MEX. This memory-efficient technique can be\ndirectly applied to off-the-shelf trackers like iKUN, resulting in significant\narchitectural improvements. Our method proves effective during inference on a\nsingle GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI\ndataset, which offers diverse autonomous driving scenes with relevant language\nexpressions, is particularly useful for studying this problem. Empirically, our\nmethod demonstrates effectiveness and efficiency regarding HOTA tracking\nscores, substantially improving memory allocation and processing speed.", "paper_summary_zh": "\u591a\u76ee\u6a19\u8ffd\u8e64 (RMOT) \u662f\u4e00\u500b\u76f8\u5c0d\u8f03\u65b0\u7684\u6982\u5ff5\uff0c\u5b83\u5728\u96fb\u8166\u8996\u89ba\u548c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u7684\u4ea4\u53c9\u9818\u57df\u4e2d\u8fc5\u901f\u7372\u5f97\u95dc\u6ce8\uff0c\u6210\u70ba\u4e00\u500b\u6709\u524d\u9014\u7684\u7814\u7a76\u65b9\u5411\u3002\u8207\u50b3\u7d71\u591a\u76ee\u6a19\u8ffd\u8e64\u4e0d\u540c\uff0cRMOT \u8b58\u5225\u4e26\u8ffd\u8e64\u76ee\u6a19\uff0c\u4e26\u5c07\u76ee\u6a19\u985e\u5225\u540d\u7a31\u7684\u6587\u5b57\u63cf\u8ff0\u7d0d\u5165\u5176\u4e2d\uff0c\u4f7f\u9019\u7a2e\u65b9\u6cd5\u66f4\u76f4\u89c0\u3002\u5df2\u7d93\u63d0\u51fa\u5404\u7a2e\u6280\u8853\u4f86\u89e3\u6c7a\u9019\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u554f\u984c\uff1b\u7136\u800c\uff0c\u7531\u65bc\u5176\u7aef\u5230\u7aef\u7279\u6027\uff0c\u5927\u591a\u6578\u6280\u8853\u90fd\u9700\u8981\u8a13\u7df4\u6574\u500b\u7db2\u8def\u3002\u5728\u9019\u4e9b\u65b9\u6cd5\u4e2d\uff0ciKUN \u5df2\u6210\u70ba\u4e00\u500b\u7279\u5225\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u56e0\u6b64\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63a2\u7d22\u5176\u7ba1\u9053\u4e26\u589e\u5f37\u5176\u6548\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u500b\u7a31\u70ba Memory-Efficient Cross-modality -- MEX \u7684\u5be6\u7528\u6a21\u7d44\u3002\u9019\u7a2e\u8a18\u61b6\u9ad4\u9ad8\u6548\u7684\u6280\u8853\u53ef\u4ee5\u76f4\u63a5\u61c9\u7528\u65bc\u73fe\u6210\u7684\u8ffd\u8e64\u5668\uff0c\u4f8b\u5982 iKUN\uff0c\u5f9e\u800c\u5e36\u4f86\u986f\u8457\u7684\u67b6\u69cb\u6539\u9032\u3002\u6211\u5011\u7684\u6280\u8853\u8b49\u660e\u4e86\u5728\u55ae\u500b\u5177\u6709 4 GB \u8a18\u61b6\u9ad4\u7684 GPU \u4e0a\u9032\u884c\u63a8\u7406\u7684\u6709\u6548\u6027\u3002\u5728\u5404\u7a2e\u57fa\u6e96\u4e2d\uff0cRefer-KITTI \u8cc7\u6599\u96c6\u63d0\u4f9b\u4e86\u5177\u6709\u76f8\u95dc\u8a9e\u8a00\u8868\u9054\u7684\u591a\u6a23\u5316\u81ea\u52d5\u99d5\u99db\u5834\u666f\uff0c\u5c0d\u65bc\u7814\u7a76\u9019\u500b\u554f\u984c\u7279\u5225\u6709\u7528\u3002\u6839\u64da\u7d93\u9a57\uff0c\u6211\u5011\u7684\u6280\u8853\u5728 HOTA \u8ffd\u8e64\u5206\u6578\u65b9\u9762\u8b49\u660e\u4e86\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u986f\u8457\u6539\u5584\u4e86\u8a18\u61b6\u9ad4\u914d\u7f6e\u548c\u8655\u7406\u901f\u5ea6\u3002", "author": "Huu-Thien Tran et.al.", "authors": "Huu-Thien Tran, Phuoc-Sang Pham, Thai-Son Tran, Khoa Luu", "id": "2502.13875v1", "paper_url": "http://arxiv.org/abs/2502.13875v1", "repo": "null"}}