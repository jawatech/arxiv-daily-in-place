{"2411.03569": {"publish_time": "2024-11-06", "title": "Towards Personalized Federated Learning via Comprehensive Knowledge Distillation", "paper_summary": "Federated learning is a distributed machine learning paradigm designed to\nprotect data privacy. However, data heterogeneity across various clients\nresults in catastrophic forgetting, where the model rapidly forgets previous\nknowledge while acquiring new knowledge. To address this challenge,\npersonalized federated learning has emerged to customize a personalized model\nfor each client. However, the inherent limitation of this mechanism is its\nexcessive focus on personalization, potentially hindering the generalization of\nthose models. In this paper, we present a novel personalized federated learning\nmethod that uses global and historical models as teachers and the local model\nas the student to facilitate comprehensive knowledge distillation. The\nhistorical model represents the local model from the last round of client\ntraining, containing historical personalized knowledge, while the global model\nrepresents the aggregated model from the last round of server aggregation,\ncontaining global generalized knowledge. By applying knowledge distillation, we\neffectively transfer global generalized knowledge and historical personalized\nknowledge to the local model, thus mitigating catastrophic forgetting and\nenhancing the general performance of personalized models. Extensive\nexperimental results demonstrate the significant advantages of our method.", "paper_summary_zh": "\u806f\u90a6\u5b78\u7fd2\u662f\u4e00\u7a2e\u5206\u6563\u5f0f\u6a5f\u5668\u5b78\u7fd2\u7bc4\u4f8b\uff0c\u65e8\u5728\u4fdd\u8b77\u8cc7\u6599\u96b1\u79c1\u3002\u7136\u800c\uff0c\u4e0d\u540c\u7528\u6236\u7aef\u4e4b\u9593\u7684\u8cc7\u6599\u7570\u8cea\u6027\u6703\u5c0e\u81f4\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u6a21\u578b\u5728\u7372\u53d6\u65b0\u77e5\u8b58\u7684\u540c\u6642\u6703\u8fc5\u901f\u907a\u5fd8\u5148\u524d\u7684\u77e5\u8b58\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u500b\u6027\u5316\u806f\u90a6\u5b78\u7fd2\u61c9\u904b\u800c\u751f\uff0c\u70ba\u6bcf\u500b\u7528\u6236\u7aef\u81ea\u8a02\u500b\u6027\u5316\u6a21\u578b\u3002\u7136\u800c\uff0c\u9019\u7a2e\u6a5f\u5236\u7684\u56fa\u6709\u9650\u5236\u5728\u65bc\u904e\u5ea6\u91cd\u8996\u500b\u6027\u5316\uff0c\u53ef\u80fd\u6703\u963b\u7919\u9019\u4e9b\u6a21\u578b\u7684\u6cdb\u5316\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u500b\u6027\u5316\u806f\u90a6\u5b78\u7fd2\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u5168\u5c40\u6a21\u578b\u548c\u6b77\u53f2\u6a21\u578b\u4f5c\u70ba\u6559\u5e2b\uff0c\u4e26\u5c07\u672c\u5730\u6a21\u578b\u4f5c\u70ba\u5b78\u751f\uff0c\u4ee5\u4fc3\u9032\u5168\u9762\u7684\u77e5\u8b58\u84b8\u993e\u3002\u6b77\u53f2\u6a21\u578b\u8868\u793a\u4f86\u81ea\u4e0a\u6b21\u7528\u6236\u7aef\u8a13\u7df4\u7684\u672c\u5730\u6a21\u578b\uff0c\u5305\u542b\u6b77\u53f2\u500b\u6027\u5316\u77e5\u8b58\uff0c\u800c\u5168\u5c40\u6a21\u578b\u8868\u793a\u4f86\u81ea\u4e0a\u6b21\u4f3a\u670d\u5668\u805a\u5408\u7684\u805a\u5408\u6a21\u578b\uff0c\u5305\u542b\u5168\u5c40\u6982\u5316\u77e5\u8b58\u3002\u901a\u904e\u61c9\u7528\u77e5\u8b58\u84b8\u993e\uff0c\u6211\u5011\u6709\u6548\u5730\u5c07\u5168\u5c40\u6982\u5316\u77e5\u8b58\u548c\u6b77\u53f2\u500b\u6027\u5316\u77e5\u8b58\u50b3\u905e\u5230\u672c\u5730\u6a21\u578b\uff0c\u5f9e\u800c\u6e1b\u8f15\u707d\u96e3\u6027\u907a\u5fd8\u4e26\u589e\u5f37\u500b\u6027\u5316\u6a21\u578b\u7684\u6574\u9ad4\u6548\u80fd\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u986f\u8457\u512a\u52e2\u3002", "author": "Pengju Wang et.al.", "authors": "Pengju Wang, Bochao Liu, Weijia Guo, Yong Li, Shiming Ge", "id": "2411.03569v1", "paper_url": "http://arxiv.org/abs/2411.03569v1", "repo": "null"}}