{"2412.03275": {"publish_time": "2024-12-04", "title": "AntLM: Bridging Causal and Masked Language Models", "paper_summary": "Causal Language Modeling (CLM) and Masked Language Modeling (MLM) are two\nmainstream learning paradigms based on Transformer networks, specifically the\nDecoder-only and Encoder-only architectures. The strengths of each paradigm in\ndownstream tasks have shown a mix of advantages and disadvantages. In the past\nBabyLM Challenge 2023, although the MLM paradigm achieved the best average\nperformance, the CLM paradigm demonstrated significantly faster convergence\nrates. For the BabyLM Challenge 2024, we propose a novel language modeling\nparadigm named $\\textbf{AntLM}$, which integrates both CLM and MLM to leverage\nthe advantages of these two classic paradigms. We chose the strict-small track\nand conducted experiments on two foundation models: BabyLlama, representing\nCLM, and LTG-BERT, representing MLM. During the training process for specific\nfoundation models, we alternate between applying CLM or MLM training objectives\nand causal or bidirectional attention masks. Experimental results show that\ncombining the two pretraining objectives leverages their strengths, enhancing\noverall training performance. Under the same epochs, $AntLM_{BabyLlama}$\nimproves Macro-average by 1%, and $AntLM_{LTG-BERT}$ achieves a 2.2% increase\nover the baselines.", "paper_summary_zh": "\u56e0\u679c\u8bed\u8a00\u6a21\u578b (CLM) \u548c\u906e\u853d\u8bed\u8a00\u6a21\u578b (MLM) \u662f\u57fa\u4e8e Transformer \u7f51\u7edc\u7684\u4e24\u79cd\u4e3b\u6d41\u5b66\u4e60\u8303\u4f8b\uff0c\u7279\u522b\u662f\u4ec5\u89e3\u7801\u5668\u548c\u4ec5\u7f16\u7801\u5668\u67b6\u6784\u3002\u6bcf\u79cd\u8303\u4f8b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u90fd\u8868\u73b0\u51fa\u4f18\u52bf\u548c\u52a3\u52bf\u7684\u7ed3\u5408\u3002\u5728\u8fc7\u53bb\u7684 BabyLM Challenge 2023 \u4e2d\uff0c\u5c3d\u7ba1 MLM \u8303\u4f8b\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u5e73\u5747\u6027\u80fd\uff0c\u4f46 CLM \u8303\u4f8b\u5374\u8868\u73b0\u51fa\u660e\u663e\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002\u5bf9\u4e8e BabyLM Challenge 2024\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a $\\textbf{AntLM}$ \u7684\u65b0\u8bed\u8a00\u5efa\u6a21\u8303\u4f8b\uff0c\u5b83\u6574\u5408\u4e86 CLM \u548c MLM \u4ee5\u5229\u7528\u8fd9\u4e24\u79cd\u7ecf\u5178\u8303\u4f8b\u7684\u4f18\u52bf\u3002\u6211\u4eec\u9009\u62e9\u4e86\u4e25\u683c\u7684\u5c0f\u578b\u8f68\u9053\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u7840\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff1a\u4ee3\u8868 CLM \u7684 BabyLlama \u548c\u4ee3\u8868 MLM \u7684 LTG-BERT\u3002\u5728\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u5728\u5e94\u7528 CLM \u6216 MLM \u8bad\u7ec3\u76ee\u6807\u548c\u56e0\u679c\u6216\u53cc\u5411\u6ce8\u610f\u529b\u63a9\u7801\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u8fd9\u4e24\u4e2a\u9884\u8bad\u7ec3\u76ee\u6807\u53ef\u4ee5\u5229\u7528\u5b83\u4eec\u7684\u4f18\u52bf\uff0c\u4ece\u800c\u589e\u5f3a\u6574\u4f53\u8bad\u7ec3\u6027\u80fd\u3002\u5728\u76f8\u540c\u7684 epoch \u4e0b\uff0c$AntLM_{BabyLlama}$ \u5c06\u5b8f\u5e73\u5747\u63d0\u9ad8\u4e86 1%\uff0c\u800c $AntLM_{LTG-BERT}$ \u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e86 2.2%\u3002", "author": "Xinru Yu et.al.", "authors": "Xinru Yu, Bin Guo, Shiwei Luo, Jie Wang, Tao Ji, Yuanbin Wu", "id": "2412.03275v1", "paper_url": "http://arxiv.org/abs/2412.03275v1", "repo": "null"}}