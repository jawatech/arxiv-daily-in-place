
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-28**|**Notes on Applicability of GPT-4 to Document Understanding**|Łukasz Borchmann et.al.|[2405.18433v1](http://arxiv.org/abs/2405.18433v1)|null|
|**2024-05-28**|**DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention**|Lianghui Zhu et.al.|[2405.18428v1](http://arxiv.org/abs/2405.18428v1)|[link](https://github.com/hustvl/dig)|
|**2024-05-28**|**Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets**|Khen Cohen et.al.|[2405.18427v1](http://arxiv.org/abs/2405.18427v1)|null|
|**2024-05-28**|**ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention**|Bencheng Liao et.al.|[2405.18425v1](http://arxiv.org/abs/2405.18425v1)|[link](https://github.com/hustvl/vig)|
|**2024-05-28**|**Why are Visually-Grounded Language Models Bad at Image Classification?**|Yuhui Zhang et.al.|[2405.18415v1](http://arxiv.org/abs/2405.18415v1)|[link](https://github.com/yuhui-zh15/vlmclassifier)|
|**2024-05-28**|**Don't Forget to Connect! Improving RAG with Graph-based Reranking**|Jialin Dong et.al.|[2405.18414v1](http://arxiv.org/abs/2405.18414v1)|null|
|**2024-05-28**|**RACCooN: Remove, Add, and Change Video Content with Auto-Generated Narratives**|Jaehong Yoon et.al.|[2405.18406v1](http://arxiv.org/abs/2405.18406v1)|null|
|**2024-05-28**|**WIDIn: Wording Image for Domain-Invariant Representation in Single-Source Domain Generalization**|Jiawei Ma et.al.|[2405.18405v1](http://arxiv.org/abs/2405.18405v1)|null|
|**2024-05-28**|**Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass**|Ethan Shen et.al.|[2405.18400v1](http://arxiv.org/abs/2405.18400v1)|[link](https://github.com/raivnlab/superposeddecoding)|
|**2024-05-28**|**MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations**|Zhangyu Wang et.al.|[2405.18395v1](http://arxiv.org/abs/2405.18395v1)|[link](https://github.com/Octopolugal/MC-GTA)|
|**2024-05-28**|**A Review and Implementation of Object Detection Models and Optimizations for Real-time Medical Mask Detection during the COVID-19 Pandemic**|Ioanna Gogou et.al.|[2405.18387v1](http://arxiv.org/abs/2405.18387v1)|[link](https://github.com/joangog/object-detection)|
|**2024-05-28**|**Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning**|Yixiao Zhang et.al.|[2405.18386v1](http://arxiv.org/abs/2405.18386v1)|[link](https://github.com/ldzhangyx/instruct-MusicGen)|
|**2024-05-28**|**Brain Tumor Segmentation (BraTS) Challenge 2024: Meningioma Radiotherapy Planning Automated Segmentation**|Dominic LaBella et.al.|[2405.18383v1](http://arxiv.org/abs/2405.18383v1)|null|
|**2024-05-28**|**OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning**|Pengxiang Li et.al.|[2405.18380v1](http://arxiv.org/abs/2405.18380v1)|[link](https://github.com/pixeli99/owlore)|
|**2024-05-28**|**LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models**|Anthony Sarah et.al.|[2405.18377v1](http://arxiv.org/abs/2405.18377v1)|null|
|**2024-05-28**|**Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning**|Phakphum Artkaew et.al.|[2405.18375v1](http://arxiv.org/abs/2405.18375v1)|null|
|**2024-05-28**|**PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework**|Eshaan Agarwal et.al.|[2405.18369v1](http://arxiv.org/abs/2405.18369v1)|null|
|**2024-05-28**|**Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs**|Somnath Kumar et.al.|[2405.18359v1](http://arxiv.org/abs/2405.18359v1)|null|
|**2024-05-28**|**MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning**|Somnath Kumar et.al.|[2405.18358v1](http://arxiv.org/abs/2405.18358v1)|null|
|**2024-05-28**|**Faithful Logical Reasoning via Symbolic Chain-of-Thought**|Jundong Xu et.al.|[2405.18357v1](http://arxiv.org/abs/2405.18357v1)|[link](https://github.com/aiden0526/symbcot)|
|**2024-05-28**|**A System for Automatic English Text Expansion**|Silvia García Méndez et.al.|[2405.18350v1](http://arxiv.org/abs/2405.18350v1)|null|
|**2024-05-28**|**Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation**|Anjanava Biswas et.al.|[2405.18346v1](http://arxiv.org/abs/2405.18346v1)|null|
|**2024-05-28**|**The Battle of LLMs: A Comparative Study in Conversational QA Tasks**|Aryan Rangapur et.al.|[2405.18344v1](http://arxiv.org/abs/2405.18344v1)|null|
|**2024-05-28**|**Interpretable classification of wiki-review streams**|Silvia García Méndez et.al.|[2405.18335v1](http://arxiv.org/abs/2405.18335v1)|null|
|**2024-05-28**|**Frustratingly Easy Test-Time Adaptation of Vision-Language Models**|Matteo Farina et.al.|[2405.18330v1](http://arxiv.org/abs/2405.18330v1)|[link](https://github.com/farinamatteo/zero)|
|**2024-05-28**|**Histopathology Based AI Model Predicts Anti-Angiogenic Therapy Response in Renal Cancer Clinical Trial**|Jay Jasti et.al.|[2405.18327v1](http://arxiv.org/abs/2405.18327v1)|null|
|**2024-05-28**|**DSDL: Data Set Description Language for Bridging Modalities and Tasks in AI Data**|Bin Wang et.al.|[2405.18315v1](http://arxiv.org/abs/2405.18315v1)|[link](https://github.com/opendatalab/dsdl-sdk)|
|**2024-05-28**|**Joint Lemmatization and Morphological Tagging with LEMMING**|Thomas Muller et.al.|[2405.18308v1](http://arxiv.org/abs/2405.18308v1)|null|
|**2024-05-28**|**Semantic are Beacons: A Semantic Perspective for Unveiling Parameter-Efficient Fine-Tuning in Knowledge Learning**|Renzhi Wang et.al.|[2405.18292v1](http://arxiv.org/abs/2405.18292v1)|null|
|**2024-05-28**|**FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning**|Zihui Wang et.al.|[2405.18291v1](http://arxiv.org/abs/2405.18291v1)|null|
|**2024-05-28**|**MODL: Multilearner Online Deep Learning**|Antonios Valkanas et.al.|[2405.18281v1](http://arxiv.org/abs/2405.18281v1)|null|
|**2024-05-28**|**Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach**|Camilo Chacón Sartori et.al.|[2405.18272v1](http://arxiv.org/abs/2405.18272v1)|[link](https://github.com/camilochs/metaheuristics-llms)|
|**2024-05-28**|**Text-only Synthesis for Image Captioning**|Qing Zhou et.al.|[2405.18258v1](http://arxiv.org/abs/2405.18258v1)|null|
|**2024-05-28**|**Active Use of Latent Constituency Representation in both Humans and Large Language Models**|Wei Liu et.al.|[2405.18241v1](http://arxiv.org/abs/2405.18241v1)|[link](https://github.com/y1ny/WordDeletion)|
|**2024-05-28**|**A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models**|Chengxing Xie et.al.|[2405.18208v1](http://arxiv.org/abs/2405.18208v1)|null|
|**2024-05-28**|**IAPT: Instruction-Aware Prompt Tuning for Large Language Models**|Wei Zhu et.al.|[2405.18203v1](http://arxiv.org/abs/2405.18203v1)|null|
|**2024-05-28**|**Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour Cloning**|Vitalis Vosylius et.al.|[2405.18196v1](http://arxiv.org/abs/2405.18196v1)|null|
|**2024-05-28**|**AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across Any Scenario**|Yuhan Li et.al.|[2405.18172v1](http://arxiv.org/abs/2405.18172v1)|null|
|**2024-05-28**|**Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing**|Wei Zhao et.al.|[2405.18166v1](http://arxiv.org/abs/2405.18166v1)|[link](https://github.com/ledllm/ledllm)|
|**2024-05-28**|**Time Series Representation Models**|Robert Leppich et.al.|[2405.18165v1](http://arxiv.org/abs/2405.18165v1)|[link](https://github.com/robertleppich/tsrm)|
|**2024-05-28**|**Back to the Drawing Board for Fair Representation Learning**|Angéline Pouget et.al.|[2405.18161v1](http://arxiv.org/abs/2405.18161v1)|null|
|**2024-05-28**|**4-bit Shampoo for Memory-Efficient Network Training**|Sike Wang et.al.|[2405.18144v1](http://arxiv.org/abs/2405.18144v1)|null|
|**2024-05-28**|**Unlocking Futures: A Natural Language Driven Career Prediction System for Computer Science and Software Engineering Students**|Sakir Hossain Faruque et.al.|[2405.18139v1](http://arxiv.org/abs/2405.18139v1)|[link](https://github.com/sakir101/career-prediction-main)|
|**2024-05-28**|**Exploiting LLM Quantization**|Kazuki Egashira et.al.|[2405.18137v1](http://arxiv.org/abs/2405.18137v1)|null|
|**2024-05-28**|**Low-Resource Crop Classification from Multi-Spectral Time Series Using Lossless Compressors**|Wei Cheng et.al.|[2405.18119v1](http://arxiv.org/abs/2405.18119v1)|[link](https://github.com/qinfengsama/compressor-based-crop-mapping)|
|**2024-05-28**|**Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting**|Hongda Sun et.al.|[2405.18113v1](http://arxiv.org/abs/2405.18113v1)|null|
|**2024-05-28**|**ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator**|Junda Zhu et.al.|[2405.18111v1](http://arxiv.org/abs/2405.18111v1)|null|
|**2024-05-28**|**A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation**|Kai Chen et.al.|[2405.18106v1](http://arxiv.org/abs/2405.18106v1)|null|
|**2024-05-28**|**LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins**|Yuchen Xia et.al.|[2405.18092v1](http://arxiv.org/abs/2405.18092v1)|[link](https://github.com/yuchenxia/llmdrivensimulation)|
|**2024-05-28**|**Design Principles for Falsifiable, Replicable and Reproducible Empirical ML Research**|Daniel Vranješ et.al.|[2405.18077v1](http://arxiv.org/abs/2405.18077v1)|null|
|**2024-05-28**|**Towards Dialogues for Joint Human-AI Reasoning and Value Alignment**|Elfia Bezou-Vrakatseli et.al.|[2405.18073v1](http://arxiv.org/abs/2405.18073v1)|null|
|**2024-05-28**|**A Survey of Latent Factor Models in Recommender Systems**|Hind I. Alshbanat et.al.|[2405.18068v1](http://arxiv.org/abs/2405.18068v1)|null|
|**2024-05-28**|**EffoVPR: Effective Foundation Model Utilization for Visual Place Recognition**|Issar Tzachor et.al.|[2405.18065v1](http://arxiv.org/abs/2405.18065v1)|null|
|**2024-05-28**|**Automated Real-World Sustainability Data Generation from Images of Buildings**|Peter J Bentley et.al.|[2405.18064v1](http://arxiv.org/abs/2405.18064v1)|null|
|**2024-05-28**|**Context is Important in Depressive Language: A Study of the Interaction Between the Sentiments and Linguistic Markers in Reddit Discussions**|Neha Sharma et.al.|[2405.18061v1](http://arxiv.org/abs/2405.18061v1)|null|
|**2024-05-28**|**PRFashion24: A Dataset for Sentiment Analysis of Fashion Products Reviews in Persian**|Mehrimah Amirpour et.al.|[2405.18060v1](http://arxiv.org/abs/2405.18060v1)|null|
|**2024-05-28**|**2BP: 2-Stage Backpropagation**|Christopher Rae et.al.|[2405.18047v1](http://arxiv.org/abs/2405.18047v1)|null|
|**2024-05-28**|**Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew Resilience**|Thanh Trung Huynh et.al.|[2405.18040v1](http://arxiv.org/abs/2405.18040v1)|[link](https://github.com/thanhtrunghuynh93/fastfedul)|
|**2024-05-28**|**Instruction Tuning with Retrieval-based Examples Ranking for Aspect-based Sentiment Analysis**|Guangmin Zheng et.al.|[2405.18035v1](http://arxiv.org/abs/2405.18035v1)|null|
|**2024-05-28**|**Edinburgh Clinical NLP at MEDIQA-CORR 2024: Guiding Large Language Models with Hints**|Aryo Pradipta Gema et.al.|[2405.18028v1](http://arxiv.org/abs/2405.18028v1)|null|
|**2024-05-28**|**TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models**|Jaewoo Ahn et.al.|[2405.18027v1](http://arxiv.org/abs/2405.18027v1)|null|
|**2024-05-28**|**Unveiling the Power of Diffusion Features For Personalized Segmentation and Retrieval**|Dvir Samuel et.al.|[2405.18025v1](http://arxiv.org/abs/2405.18025v1)|null|
|**2024-05-28**|**MultiADE: A Multi-domain Benchmark for Adverse Drug Event Extraction**|Xiang Dai et.al.|[2405.18015v1](http://arxiv.org/abs/2405.18015v1)|null|
|**2024-05-28**|**Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model**|Wenbing Li et.al.|[2405.18014v1](http://arxiv.org/abs/2405.18014v1)|null|
|**2024-05-28**|**Exploring Context Window of Large Language Models via Decomposed Positional Vectors**|Zican Dong et.al.|[2405.18009v1](http://arxiv.org/abs/2405.18009v1)|null|
|**2024-05-28**|**MAVIN: Multi-Action Video Generation with Diffusion Models via Transition Video Infilling**|Bowen Zhang et.al.|[2405.18003v1](http://arxiv.org/abs/2405.18003v1)|[link](https://github.com/18445864529/mavin)|
|**2024-05-28**|**Source Echo Chamber: Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop**|Yuqi Zhou et.al.|[2405.17998v1](http://arxiv.org/abs/2405.17998v1)|null|
|**2024-05-28**|**DMT-JEPA: Discriminative Masked Targets for Joint-Embedding Predictive Architecture**|Shentong Mo et.al.|[2405.17995v1](http://arxiv.org/abs/2405.17995v1)|null|
|**2024-05-28**|**fMRI predictors based on language models of increasing complexity recover brain left lateralization**|Laurent Bonnasse-Gahot et.al.|[2405.17992v1](http://arxiv.org/abs/2405.17992v1)|null|
|**2024-05-28**|**VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections**|Roy Miles et.al.|[2405.17991v1](http://arxiv.org/abs/2405.17991v1)|null|
|**2024-05-28**|**Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering**|Anirudh Phukan et.al.|[2405.17980v1](http://arxiv.org/abs/2405.17980v1)|null|
|**2024-05-28**|**FASTopic: A Fast, Adaptive, Stable, and Transferable Topic Modeling Paradigm**|Xiaobao Wu et.al.|[2405.17978v1](http://arxiv.org/abs/2405.17978v1)|null|
|**2024-05-28**|**Aligning to Thousands of Preferences via System Message Generalization**|Seongyun Lee et.al.|[2405.17977v1](http://arxiv.org/abs/2405.17977v1)|null|
|**2024-05-28**|**Yuan 2.0-M32: Mixture of Experts with Attention Router**|Shaohua Wu et.al.|[2405.17976v1](http://arxiv.org/abs/2405.17976v1)|[link](https://github.com/ieit-yuan/yuan2.0-m32)|
|**2024-05-28**|**Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations**|Yi-Pei Chen et.al.|[2405.17974v1](http://arxiv.org/abs/2405.17974v1)|null|
|**2024-05-28**|**Knowledge Circuits in Pretrained Transformers**|Yunzhi Yao et.al.|[2405.17969v1](http://arxiv.org/abs/2405.17969v1)|[link](https://github.com/zjunlp/knowledgecircuits)|
|**2024-05-28**|**Transformer and Hybrid Deep Learning Based Models for Machine-Generated Text Detection**|Teodor-George Marchitan et.al.|[2405.17964v1](http://arxiv.org/abs/2405.17964v1)|null|
|**2024-05-28**|**Attention-based sequential recommendation system using multimodal data**|Hyungtaik Oh et.al.|[2405.17959v1](http://arxiv.org/abs/2405.17959v1)|null|
|**2024-05-28**|**Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion**|Xiaobao Wu et.al.|[2405.17957v1](http://arxiv.org/abs/2405.17957v1)|[link](https://github.com/bobxwu/cfdtm)|
|**2024-05-28**|**Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives**|Anirudhan Badrinath et.al.|[2405.17956v1](http://arxiv.org/abs/2405.17956v1)|null|
|**2024-05-28**|**Self-Guiding Exploration for Combinatorial Problems**|Zangir Iklassov et.al.|[2405.17950v1](http://arxiv.org/abs/2405.17950v1)|[link](https://github.com/zangir/llm-for-cp)|
|**2024-05-28**|**Self-supervised Pre-training for Transferable Multi-modal Perception**|Xiaohao Xu et.al.|[2405.17942v1](http://arxiv.org/abs/2405.17942v1)|null|
|**2024-05-28**|**World Models for General Surgical Grasping**|Hongbin Lin et.al.|[2405.17940v1](http://arxiv.org/abs/2405.17940v1)|null|
|**2024-05-28**|**Tool Learning with Large Language Models: A Survey**|Changle Qu et.al.|[2405.17935v1](http://arxiv.org/abs/2405.17935v1)|[link](https://github.com/quchangle1/llm-tool-survey)|
|**2024-05-28**|**Proof of Quality: A Costless Paradigm for Trustless Generative AI Model Inference on Blockchains**|Zhenjie Zhang et.al.|[2405.17934v1](http://arxiv.org/abs/2405.17934v1)|null|
|**2024-05-28**|**Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment**|Keming Lu et.al.|[2405.17931v1](http://arxiv.org/abs/2405.17931v1)|null|
|**2024-05-28**|**The Evolution of Multimodal Model Architectures**|Shakti N. Wadekar et.al.|[2405.17927v1](http://arxiv.org/abs/2405.17927v1)|null|
|**2024-05-28**|**Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models**|Longze Chen et.al.|[2405.17915v1](http://arxiv.org/abs/2405.17915v1)|null|
|**2024-05-28**|**OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision**|Junjie Wang et.al.|[2405.17913v1](http://arxiv.org/abs/2405.17913v1)|[link](https://github.com/xiaomoguhz/ov-dquo)|
|**2024-05-28**|**Boosting Protein Language Models with Negative Sample Mining**|Yaoyao Xu et.al.|[2405.17902v1](http://arxiv.org/abs/2405.17902v1)|[link](https://github.com/logo-cuhksz/nm-transformer)|
|**2024-05-28**|**Enhancing Emotion Recognition in Conversation through Emotional Cross-Modal Fusion and Inter-class Contrastive Learning**|Haoxiang Shi et.al.|[2405.17900v1](http://arxiv.org/abs/2405.17900v1)|null|
|**2024-05-28**|**FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic Prediction**|Zhonghang Li et.al.|[2405.17898v1](http://arxiv.org/abs/2405.17898v1)|[link](https://github.com/hkuds/flashst)|
|**2024-05-28**|**White-box Multimodal Jailbreaks Against Large Vision-Language Models**|Ruofan Wang et.al.|[2405.17894v1](http://arxiv.org/abs/2405.17894v1)|null|
|**2024-05-28**|**Arithmetic Reasoning with LLM: Prolog Generation & Permutation**|Xiaocheng Yang et.al.|[2405.17893v1](http://arxiv.org/abs/2405.17893v1)|null|
|**2024-05-28**|**SLMRec: Empowering Small Language Models for Sequential Recommendation**|Wujiang Xu et.al.|[2405.17890v1](http://arxiv.org/abs/2405.17890v1)|null|
|**2024-05-28**|**Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment**|Jiaxiang Li et.al.|[2405.17888v1](http://arxiv.org/abs/2405.17888v1)|null|
|**2024-05-28**|**An Information Theoretic Metric for Evaluating Unlearning Models**|Dongjae Jeon et.al.|[2405.17878v1](http://arxiv.org/abs/2405.17878v1)|null|
|**2024-05-28**|**NUTS, NARS, and Speech**|D. van der Sluis et.al.|[2405.17874v1](http://arxiv.org/abs/2405.17874v1)|null|
|**2024-05-28**|**MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization**|Tianchen Zhao et.al.|[2405.17873v1](http://arxiv.org/abs/2405.17873v1)|null|
|**2024-05-28**|**Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment**|Xin Xiao et.al.|[2405.17871v1](http://arxiv.org/abs/2405.17871v1)|null|

#### Abstracts
##### **Notes on Applicability of GPT-4 to Document Understanding**
2405.18433v1 by Łukasz Borchmann

We perform a missing, reproducible evaluation of all publicly available GPT-4
family models concerning the Document Understanding field, where it is
frequently required to comprehend text spacial arrangement and visual clues in
addition to textual semantics. Benchmark results indicate that though it is
hard to achieve satisfactory results with text-only models, GPT-4 Vision Turbo
performs well when one provides both text recognized by an external OCR engine
and document images on the input. Evaluation is followed by analyses that
suggest possible contamination of textual GPT-4 models and indicate the
significant performance drop for lengthy documents.

摘要：我們對所有公開的 GPT-4 家族模型進行了遺漏且可重製的評估，涉及文件理解領域，在該領域中，除了文本語義之外，通常需要理解文本空間排列和視覺線索。基準結果表明，儘管僅使用文本模型很難獲得令人滿意的結果，但當有人同時提供外部 OCR 引擎識別的文本和輸入中的文件圖像時，GPT-4 Vision Turbo 表現良好。評估後進行分析，表明文本 GPT-4 模型可能受到汙染，並表明長文件的效能顯著下降。

##### **DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention**
2405.18428v1 by Lianghui Zhu, Zilong Huang, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, Xinggang Wang

Diffusion models with large-scale pre-training have achieved significant
success in the field of visual content generation, particularly exemplified by
Diffusion Transformers (DiT). However, DiT models have faced challenges with
scalability and quadratic complexity efficiency. In this paper, we aim to
leverage the long sequence modeling capability of Gated Linear Attention (GLA)
Transformers, expanding its applicability to diffusion models. We introduce
Diffusion Gated Linear Attention Transformers (DiG), a simple, adoptable
solution with minimal parameter overhead, following the DiT design, but
offering superior efficiency and effectiveness. In addition to better
performance than DiT, DiG-S/2 exhibits $2.5\times$ higher training speed than
DiT-S/2 and saves $75.7\%$ GPU memory at a resolution of $1792 \times 1792$.
Moreover, we analyze the scalability of DiG across a variety of computational
complexity. DiG models, with increased depth/width or augmentation of input
tokens, consistently exhibit decreasing FID. We further compare DiG with other
subquadratic-time diffusion models. With the same model size, DiG-XL/2 is
$4.2\times$ faster than the recent Mamba-based diffusion model at a $1024$
resolution, and is $1.8\times$ faster than DiT with CUDA-optimized
FlashAttention-2 under the $2048$ resolution. All these results demonstrate its
superior efficiency among the latest diffusion models. Code is released at
https://github.com/hustvl/DiG.

摘要：<paragraph>具有大規模預訓練的擴散模型在視覺內容生成領域取得了顯著的成功，特別是擴散Transformer (DiT) 的範例。然而，DiT 模型在可擴充性和二次複雜度效率方面面臨挑戰。在本文中，我們旨在利用門控線性注意力 (GLA) Transformer的長序列建模能力，擴展其在擴散模型中的應用性。我們引入了擴散門控線性注意力Transformer (DiG)，這是一個簡單、可採用的解決方案，具有最小的參數開銷，遵循 DiT 設計，但提供卓越的效率和有效性。除了比 DiT 更好的性能外，DiG-S/2 的訓練速度比 DiT-S/2 高出 $2.5\times$，並在 $1792 \times 1792$ 的解析度下節省了 $75.7\%$ 的 GPU 記憶體。此外，我們分析了 DiG 在各種計算複雜度下的可擴充性。DiG 模型在增加深度/寬度或輸入標記的增強下，始終表現出遞減的 FID。我們進一步將 DiG 與其他次二次時間擴散模型進行比較。在相同模型大小下，DiG-XL/2 在 $1024$ 解析度下比最近基於 Mamba 的擴散模型快 $4.2\times$，並且在 $2048$ 解析度下比具有 CUDA 優化的 FlashAttention-2 的 DiT 快 $1.8\times$。所有這些結果都證明了它在最新擴散模型中的卓越效率。程式碼已發布在 https://github.com/hustvl/DiG。</paragraph>

##### **Classifying Overlapping Gaussian Mixtures in High Dimensions: From Optimal Classifiers to Neural Nets**
2405.18427v1 by Khen Cohen, Noam Levi, Yaron Oz

We derive closed-form expressions for the Bayes optimal decision boundaries
in binary classification of high dimensional overlapping Gaussian mixture model
(GMM) data, and show how they depend on the eigenstructure of the class
covariances, for particularly interesting structured data. We empirically
demonstrate, through experiments on synthetic GMMs inspired by real-world data,
that deep neural networks trained for classification, learn predictors which
approximate the derived optimal classifiers. We further extend our study to
networks trained on authentic data, observing that decision thresholds
correlate with the covariance eigenvectors rather than the eigenvalues,
mirroring our GMM analysis. This provides theoretical insights regarding neural
networks' ability to perform probabilistic inference and distill statistical
patterns from intricate distributions.

摘要：<paragraph>我們推導出高維重疊高斯混合模型 (GMM) 資料二元分類中，貝氏最佳決策邊界的閉合形式表達式，並展示它們如何取決於類別共變數的本徵結構，特別是對於結構化的有趣資料。我們透過受真實世界資料啟發的合成 GMM 實驗，經驗性地證明了針對分類所訓練的深度神經網路，會學習近似於推導出的最佳分類器的預測因子。我們進一步將研究延伸到訓練於真實資料上的網路，觀察到決策閾值與共變數特徵向量相關，而不是特徵值，這反映了我們的 GMM 分析。這提供了關於神經網路執行機率推論和從複雜分佈中萃取統計模式的能力的理論見解。</paragraph>

##### **ViG: Linear-complexity Visual Sequence Learning with Gated Linear Attention**
2405.18425v1 by Bencheng Liao, Xinggang Wang, Lianghui Zhu, Qian Zhang, Chang Huang

Recently, linear complexity sequence modeling networks have achieved modeling
capabilities similar to Vision Transformers on a variety of computer vision
tasks, while using fewer FLOPs and less memory. However, their advantage in
terms of actual runtime speed is not significant. To address this issue, we
introduce Gated Linear Attention (GLA) for vision, leveraging its superior
hardware-awareness and efficiency. We propose direction-wise gating to capture
1D global context through bidirectional modeling and a 2D gating locality
injection to adaptively inject 2D local details into 1D global context. Our
hardware-aware implementation further merges forward and backward scanning into
a single kernel, enhancing parallelism and reducing memory cost and latency.
The proposed model, \name{}, offers a favorable trade-off in accuracy,
parameters, and FLOPs on ImageNet and downstream tasks, outperforming popular
Transformer and CNN-based models. Notably, \name{}-S matches DeiT-B's accuracy
while using only 27\% of the parameters and 20\% of the FLOPs, running
2$\times$ faster on $224\times224$ images. At $1024\times1024$ resolution,
\name{}-T uses 5.2$\times$ fewer FLOPs, saves 90\% GPU memory, runs 4.8$\times$
faster, and achieves 20.7\% higher top-1 accuracy than DeiT-T. These results
position \name{} as an efficient and scalable solution for visual
representation learning. Code is available at
\url{https://github.com/hustvl/ViG}.

摘要：<paragraph>最近，线性复杂度序列建模网络在各种计算机视觉任务上实现了与视觉 Transformer 类似的建模能力，同时使用了更少的 FLOP 和更少的内存。然而，它们在实际运行速度方面的优势并不明显。为了解决这个问题，我们引入了用于视觉的 Gate 线性注意力 (GLA)，利用其出色的硬件感知和效率。我们提出方向门控来通过双向建模捕捉 1D 全局上下文，并通过 2D 门控局部注入将 2D 局部细节自适应地注入 1D 全局上下文。我们注重硬件的实现进一步将正向和反向扫描合并到一个内核中，增强了并行性并降低了内存成本和延迟。所提出的模型 \name{} 在准确性、参数和 ImageNet 及下游任务上的 FLOP 上提供了有利的权衡，优于流行的 Transformer 和基于 CNN 的模型。值得注意的是，\name{}-S 匹配了 DeiT-B 的准确性，同时只使用了 27% 的参数和 20% 的 FLOP，在 $224\times224$ 图像上运行速度提高了 2 倍。在 $1024\times1024$ 分辨率下，\name{}-T 使用的 FLOP 减少了 5.2 倍，节省了 90% 的 GPU 内存，运行速度提高了 4.8 倍，并且比 DeiT-T 的 top-1 准确率提高了 20.7%。这些结果将 \name{} 定位为视觉表征学习的高效且可扩展的解决方案。代码可在 \url{https://github.com/hustvl/ViG} 获得。</paragraph>

##### **Why are Visually-Grounded Language Models Bad at Image Classification?**
2405.18415v1 by Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, Serena Yeung-Levy

Image classification is one of the most fundamental capabilities of machine
vision intelligence. In this work, we revisit the image classification task
using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We
find that existing proprietary and public VLMs, despite often using CLIP as a
vision encoder and having many more parameters, significantly underperform CLIP
on standard image classification benchmarks like ImageNet. To understand the
reason, we explore several hypotheses concerning the inference algorithms,
training objectives, and data processing in VLMs. Our analysis reveals that the
primary cause is data-related: critical information for image classification is
encoded in the VLM's latent space but can only be effectively decoded with
enough training data. Specifically, there is a strong correlation between the
frequency of class exposure during VLM training and instruction-tuning and the
VLM's performance in those classes; when trained with sufficient data, VLMs can
match the accuracy of state-of-the-art classification models. Based on these
findings, we enhance a VLM by integrating classification-focused datasets into
its training, and demonstrate that the enhanced classification performance of
the VLM transfers to its general capabilities, resulting in an improvement of
11.8% on the newly collected ImageWikiQA dataset.

摘要：影像分類是機器視覺智能最基本的技能之一。在這項工作中，我們使用視覺基礎語言模型（VLM），例如 GPT-4V 和 LLaVA，重新探討影像分類任務。我們發現現有的專有和公開 VLM，儘管經常使用 CLIP 作為視覺編碼器，且具有更多參數，但在 ImageNet 等標準影像分類基準上卻明顯不如 CLIP。為了了解原因，我們探討了關於 VLM 中的推論演算法、訓練目標和資料處理的幾個假設。我們的分析顯示，主要原因與資料有關：影像分類的關鍵資訊編碼在 VLM 的潛在空間中，但只有在有足夠的訓練資料時才能有效解碼。具體來說，VLM 訓練期間類別曝光的頻率與指令微調，以及 VLM 在這些類別中的表現之間存在很強的相關性；當使用足夠的資料訓練時，VLM 可以達到最先進分類模型的準確度。根據這些發現，我們透過將以分類為重點的資料集整合到訓練中來增強 VLM，並證明 VLM 增強的分類效能轉移到其一般能力，進而使新收集的 ImageWikiQA 資料集提升了 11.8%。

##### **Don't Forget to Connect! Improving RAG with Graph-based Reranking**
2405.18414v1 by Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, Anton Tsitsulin

Retrieval Augmented Generation (RAG) has greatly improved the performance of
Large Language Model (LLM) responses by grounding generation with context from
existing documents. These systems work well when documents are clearly relevant
to a question context. But what about when a document has partial information,
or less obvious connections to the context? And how should we reason about
connections between documents? In this work, we seek to answer these two core
questions about RAG generation. We introduce G-RAG, a reranker based on graph
neural networks (GNNs) between the retriever and reader in RAG. Our method
combines both connections between documents and semantic information (via
Abstract Meaning Representation graphs) to provide a context-informed ranker
for RAG. G-RAG outperforms state-of-the-art approaches while having smaller
computational footprint. Additionally, we assess the performance of PaLM 2 as a
reranker and find it to significantly underperform G-RAG. This result
emphasizes the importance of reranking for RAG even when using Large Language
Models.

摘要：檢索增強生成 (RAG) 已大幅提升大型語言模型 (LLM) 回應的效能，方法是將生成基礎於現有文件中的內容。當文件與問題內容明確相關時，這些系統運作良好。但當文件有部分資訊或與內容的關聯性較不顯著時呢？我們又該如何推論文件之間的關聯性？在這項研究中，我們尋求回答這兩個關於 RAG 生成的核心問題。我們引入了 G-RAG，一種基於 RAG 中檢索器和讀取器之間的圖形神經網路 (GNN) 的重新排序器。我們的技術結合了文件之間的關聯性與語義資訊（透過抽象意義表徵圖形），為 RAG 提供一個有脈絡的排序器。G-RAG 的表現超越現有技術，同時運算資源需求較小。此外，我們評估了 PaLM 2 作為重新排序器的表現，發現其表現顯著低於 G-RAG。這個結果強調了重新排序對 RAG 的重要性，即使在使用大型語言模型時也是如此。

##### **RACCooN: Remove, Add, and Change Video Content with Auto-Generated Narratives**
2405.18406v1 by Jaehong Yoon, Shoubin Yu, Mohit Bansal

Recent video generative models primarily rely on carefully written text
prompts for specific tasks, like inpainting or style editing. They require
labor-intensive textual descriptions for input videos, hindering their
flexibility to adapt personal/raw videos to user specifications. This paper
proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video
generative framework that supports multiple video editing capabilities such as
removal, addition, and modification, through a unified pipeline. RACCooN
consists of two principal stages: Video-to-Paragraph (V2P) and
Paragraph-to-Video (P2V). In the V2P stage, we automatically describe video
scenes in well-structured natural language, capturing both the holistic context
and focused object details. Subsequently, in the P2V stage, users can
optionally refine these descriptions to guide the video diffusion model,
enabling various modifications to the input video, such as removing, changing
subjects, and/or adding new objects. The proposed approach stands out from
other methods through several significant contributions: (1) RACCooN suggests a
multi-granular spatiotemporal pooling strategy to generate well-structured
video descriptions, capturing both the broad context and object details without
requiring complex human annotations, simplifying precise video content editing
based on text for users. (2) Our video generative model incorporates
auto-generated narratives or instructions to enhance the quality and accuracy
of the generated content. It supports the addition of video objects,
inpainting, and attribute modification within a unified framework, surpassing
existing video editing and inpainting benchmarks. The proposed framework
demonstrates impressive versatile capabilities in video-to-paragraph
generation, video content editing, and can be incorporated into other SoTA
video generative models for further enhancement.

摘要：<paragraph>最近的影片生成模型主要仰賴仔細撰寫的文字提示，以執行特定任務，例如修復或風格編輯。它們需要大量文字描述作為輸入影片，這會妨礙它們靈活地調整個人/原始影片以符合使用者規格。本文提出 RACCooN，一個多功能且友善的影片轉段落轉影片生成架構，透過統一的管道支援多種影片編輯功能，例如移除、新增和修改。RACCooN 包含兩個主要階段：影片轉段落 (V2P) 和段落轉影片 (P2V)。在 V2P 階段，我們會自動以結構良好的自然語言描述影片場景，同時擷取整體脈絡和聚焦的物件細節。接著，在 P2V 階段，使用者可以選擇調整這些描述，以引導影片擴散模型，對輸入影片進行各種修改，例如移除、變更主體和/或新增新物件。所提出的方法透過幾個重要的貢獻，在其他方法中脫穎而出：(1) RACCooN 建議採用多顆粒時空池化策略來產生結構良好的影片描述，同時擷取廣泛的脈絡和物件細節，而無需複雜的人工註解，簡化了基於文字的精確影片內容編輯。 (2) 我們的影片生成模型結合自動產生的敘述或說明，以提升生成內容的品質和準確度。它支援在統一架構中新增影片物件、修復和屬性修改，超越現有的影片編輯和修復基準。所提出的架構展現出令人印象深刻的多功能能力，在影片轉段落生成、影片內容編輯方面，並且可以整合到其他 SoTA 影片生成模型中以進一步提升。</paragraph>

##### **WIDIn: Wording Image for Domain-Invariant Representation in Single-Source Domain Generalization**
2405.18405v1 by Jiawei Ma, Yulei Niu, Shiyuan Huang, Guangxing Han, Shih-Fu Chang

Language has been useful in extending the vision encoder to data from diverse
distributions without empirical discovery in training domains. However, as the
image description is mostly at coarse-grained level and ignores visual details,
the resulted embeddings are still ineffective in overcoming complexity of
domains at inference time. We present a self-supervision framework WIDIn,
Wording Images for Domain-Invariant representation, to disentangle
discriminative visual representation, by only leveraging data in a single
domain and without any test prior. Specifically, for each image, we first
estimate the language embedding with fine-grained alignment, which can be
consequently used to adaptively identify and then remove domain-specific
counterpart from the raw visual embedding. WIDIn can be applied to both
pretrained vision-language models like CLIP, and separately trained uni-modal
models like MoCo and BERT. Experimental studies on three domain generalization
datasets demonstrate the effectiveness of our approach.

摘要：語言在將視覺編碼器擴展到來自不同分佈的資料中，而無需在訓練領域中進行經驗發現方面很有用。然而，由於影像描述大多處於粗略的層級，且忽略了視覺細節，因此產生的嵌入式仍然無法有效克服推理時間中領域的複雜性。我們提出了一個自我監督架構 WIDIn，用於領域不變表示的文字影像，以僅利用單一領域中的資料，且無任何先驗測試來解開歧視性視覺表示。具體來說，對於每個影像，我們首先使用細粒度對齊來估計語言嵌入式，然後可以使用它來自適應性地識別並從原始視覺嵌入式中移除特定於領域的對應部分。WIDIn 可以應用於預先訓練好的視覺語言模型（例如 CLIP）和單獨訓練的單一模式模型（例如 MoCo 和 BERT）。在三個領域概化資料集上的實驗研究證明了我們方法的有效性。

##### **Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass**
2405.18400v1 by Ethan Shen, Alan Fan, Sarah M Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati

Many applications today provide users with multiple auto-complete drafts as
they type, including GitHub's code completion, Gmail's smart compose, and
Apple's messaging auto-suggestions. Under the hood, language models support
this by running an autoregressive inference pass to provide a draft.
Consequently, providing $k$ drafts to the user requires running an expensive
language model $k$ times. To alleviate the computation cost of running $k$
inference passes, we propose Superposed Decoding, a new decoding algorithm that
generates $k$ drafts at the computation cost of one autoregressive inference
pass. We achieve this by feeding a superposition of the $k$ most recent token
embeddings from the drafts as input to the next decoding step of the language
model. At every inference step we combine the $k$ drafts with the top-$k$
tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an
n-gram interpolation with minimal compute overhead to filter out incoherent
generations. Our experiments show that $k$ drafts from Superposed Decoding are
at least as coherent and factual as Nucleus Sampling and Greedy Decoding
respectively, while being at least $2.44\times$ faster for $k\ge3$. In a
compute-normalized setting, user evaluations demonstrably favor text generated
by Superposed Decoding over Nucleus Sampling. Code and more examples
open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.

摘要：當今許多應用程式會在使用者輸入時提供多個自動完成草稿，包括 GitHub 的程式碼完成、Gmail 的智慧撰寫，以及 Apple 的訊息自動建議。在幕後，語言模型透過執行自迴歸推論傳遞來提供草稿，進而支援此功能。因此，提供 $k$ 個草稿給使用者需要執行昂貴的語言模型 $k$ 次。為了減輕執行 $k$ 次推論傳遞的運算成本，我們提出疊置解碼，這是一種新的解碼演算法，它以一次自迴歸推論傳遞的運算成本產生 $k$ 個草稿。我們透過將草稿中 $k$ 個最新的標記嵌入的疊置提供給語言模型的下一步解碼作為輸入，來達成此目標。在每個推論步驟中，我們將 $k$ 個草稿與前 $k$ 個標記結合，以取得 $k^2$ 個新草稿，並快取 $k$ 個最可能的選項，使用具有最小運算負擔的 n-gram 內插法，以篩選出不連貫的生成。我們的實驗顯示，疊置解碼的 $k$ 個草稿至少與 Nucleus Sampling 和 Greedy Decoding 一樣連貫且具有事實依據，同時對於 $k\ge3$ 來說，速度至少快 $2.44\times$。在運算正規化的設定中，使用者評估明顯偏好疊置解碼所產生的文字，勝過 Nucleus Sampling。程式碼和更多範例在 https://github.com/RAIVNLab/SuperposedDecoding 開源。

##### **MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations**
2405.18395v1 by Zhangyu Wang, Gengchen Mai, Krzysztof Janowicz, Ni Lao

A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis
tasks, such as grouping vehicle sensor trajectories, can be formulated as
clustering with given metric constraints. Existing metric-constrained
clustering algorithms overlook the rich correlation between feature similarity
and metric distance, i.e., metric autocorrelation. The model-based variations
of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance,
yet suffer from computational instability and complexity by using a
metric-constrained Expectation-Maximization procedure. In order to address
these two problems, we propose a novel clustering algorithm, MC-GTA
(Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its
objective is only composed of pairwise weighted sums of feature similarity
terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel
multivariate generalization of classic semivariogram). We show that MC-GTA is
effectively minimizing the total hinge loss for intra-cluster observation pairs
not passing goodness-of-fit tests, i.e., statistically not originating from the
same distribution. Experiments on 1D/2D synthetic and real-world datasets
demonstrate that MC-GTA successfully incorporates metric autocorrelation. It
outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in
NMI) with faster and stabler optimization (>10x speedup).

摘要：<paragraph>廣泛的 (多變量) 時間 (1D) 和空間 (2D) 資料分析任務，例如群組車輛感測器軌跡，可以表述為具有給定度量約束的群集。現有的度量約束群集演算法忽略了特徵相似度和度量距離之間的豐富相關性，即度量自相關。這些群集演算法的基於模型的變異 (例如 TICC 和 STICC) 達到了 SOTA 效能，但由於使用了度量約束的期望最大化程序而導致計算不穩定性和複雜性。為了解決這兩個問題，我們提出了一種新的群集演算法，MC-GTA (透過具有自相關的適配度檢定進行的基於模型的群集)。其目標僅由特徵相似度項 (平方 Wasserstein-2 距離) 和度量自相關項 (經典半變異函數的新多變量概化) 的成對加權和組成。我們展示了 MC-GTA 有效地最小化了未通過適配度檢定的群集內觀測對的總鉸鏈損失，即統計上並非來自同一個分佈。在 1D/2D 合成和真實世界資料集上的實驗證明了 MC-GTA 成功地納入了度量自相關。它以大幅度的優勢 (ARI 中高達 14.3%，NMI 中高達 32.1%) 優於強大的基準，同時具有更快速且穩定的最佳化 (>10 倍加速)。</paragraph>

##### **A Review and Implementation of Object Detection Models and Optimizations for Real-time Medical Mask Detection during the COVID-19 Pandemic**
2405.18387v1 by Ioanna Gogou, Dimitrios Koutsomitropoulos

Convolutional Neural Networks (CNN) are commonly used for the problem of
object detection thanks to their increased accuracy. Nevertheless, the
performance of CNN-based detection models is ambiguous when detection speed is
considered. To the best of our knowledge, there has not been sufficient
evaluation of the available methods in terms of the speed/accuracy trade-off in
related literature. This work assesses the most fundamental object detection
models on the Common Objects in Context (COCO) dataset with respect to this
trade-off, their memory consumption, and computational and storage cost. Next,
we select a highly efficient model called YOLOv5 to train on the topical and
unexplored dataset of human faces with medical masks, the Properly-Wearing
Masked Faces Dataset (PWMFD), and analyze the benefits of specific optimization
techniques for real-time medical mask detection: transfer learning, data
augmentations, and a Squeeze-and-Excitation attention mechanism. Using our
findings in the context of the COVID-19 pandemic, we propose an optimized model
based on YOLOv5s using transfer learning for the detection of correctly and
incorrectly worn medical masks that surpassed more than two times in speed (69
frames per second) the state-of-the-art model SE-YOLOv3 on the PWMFD dataset
while maintaining the same level of mean Average Precision (67%).

摘要：卷積神經網路 (CNN) 因其準確度高而常被用於目標偵測問題。然而，當考量偵測速度時，基於 CNN 的偵測模型效能卻模稜兩可。據我們所知，相關文獻中尚未對可用方法進行足夠的評估，以了解速度/準確度的權衡取捨。本研究針對 Common Objects in Context (COCO) 資料集評估最基本的目標偵測模型，考量上述權衡取捨、記憶體消耗，以及運算和儲存成本。接著，我們選擇一個名為 YOLOv5 的高效率模型，在主題性且尚未探索的人臉戴醫用口罩資料集 Properly-Wearing Masked Faces Dataset (PWMFD) 上進行訓練，並分析特定最佳化技術對即時醫用口罩偵測的優點：遷移學習、資料擴充，以及 Squeeze-and-Excitation 注意力機制。我們在 COVID-19 疫情的背景下運用研究結果，提出一個基於 YOLOv5s 的最佳化模型，使用遷移學習偵測正確和錯誤配戴的醫用口罩，在 PWMFD 資料集上的速度比最先進的 SE-YOLOv3 模型快兩倍以上 (每秒 69 幀)，同時維持相同等級的平均準確度 (67%)。

##### **Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning**
2405.18386v1 by Yixiao Zhang, Yukara Ikemiya, Woosung Choi, Naoki Murata, Marco A. Martínez-Ramírez, Liwei Lin, Gus Xia, Wei-Hsiang Liao, Yuki Mitsufuji, Simon Dixon

Recent advances in text-to-music editing, which employ text queries to modify
music (e.g.\ by changing its style or adjusting instrumental components),
present unique challenges and opportunities for AI-assisted music creation.
Previous approaches in this domain have been constrained by the necessity to
train specific editing models from scratch, which is both resource-intensive
and inefficient; other research uses large language models to predict edited
music, resulting in imprecise audio reconstruction. To Combine the strengths
and address these limitations, we introduce Instruct-MusicGen, a novel approach
that finetunes a pretrained MusicGen model to efficiently follow editing
instructions such as adding, removing, or separating stems. Our approach
involves a modification of the original MusicGen architecture by incorporating
a text fusion module and an audio fusion module, which allow the model to
process instruction texts and audio inputs concurrently and yield the desired
edited music. Remarkably, Instruct-MusicGen only introduces 8% new parameters
to the original MusicGen model and only trains for 5K steps, yet it achieves
superior performance across all tasks compared to existing baselines, and
demonstrates performance comparable to the models trained for specific tasks.
This advancement not only enhances the efficiency of text-to-music editing but
also broadens the applicability of music language models in dynamic music
production environments.

摘要：最近文本转音乐编辑的进展，使用文本查询来修改音乐（例如，通过改变其风格或调整乐器组件），为人工智能辅助音乐创作提出了独特的挑战和机遇。此领域的先前方法受到从头开始训练特定编辑模型的必要性的限制，这既耗费资源又效率低下；其他研究使用大型语言模型来预测编辑后的音乐，导致音频重建不精确。为了结合优势并解决这些限制，我们引入了 Instruct-MusicGen，这是一种新颖的方法，它微调了预训练的 MusicGen 模型以有效地遵循编辑指令，例如添加、删除或分离母带。我们的方法涉及通过合并文本融合模块和音频融合模块来修改原始 MusicGen 架构，这使模型能够同时处理指令文本和音频输入并产生所需的编辑音乐。值得注意的是，Instruct-MusicGen 仅向原始 MusicGen 模型引入了 8% 的新参数，并且仅训练了 5K 步，但它在所有任务上都比现有的基线实现了卓越的性能，并且展示了与为特定任务训练的模型相当的性能。这一进步不仅提高了文本转音乐编辑的效率，还拓宽了音乐语言模型在动态音乐制作环境中的适用性。

##### **Brain Tumor Segmentation (BraTS) Challenge 2024: Meningioma Radiotherapy Planning Automated Segmentation**
2405.18383v1 by Dominic LaBella, Katherine Schumacher, Michael Mix, Kevin Leu, Shan McBurney-Lin, Pierre Nedelec, Javier Villanueva-Meyer, Jonathan Shapey, Tom Vercauteren, Kazumi Chia, Omar Al-Salihi, Justin Leu, Lia Halasz, Yury Velichko, Chunhao Wang, John Kirkpatrick, Scott Floyd, Zachary J. Reitman, Trey Mullikin, Ulas Bagci, Sean Sachdev, Jona A. Hattangadi-Gluth, Tyler Seibert, Nikdokht Farid, Connor Puett, Matthew W. Pease, Kevin Shiue, Syed Muhammad Anwar, Shahriar Faghani, Muhammad Ammar Haider, Pranav Warman, Jake Albrecht, András Jakab, Mana Moassefi, Verena Chung, Alejandro Aristizabal, Alexandros Karargyris, Hasan Kassem, Sarthak Pati, Micah Sheller, Christina Huang, Aaron Coley, Siddharth Ghanta, Alex Schneider, Conrad Sharp, Rachit Saluja, Florian Kofler, Philipp Lohmann, Phillipp Vollmuth, Louis Gagnon, Maruf Adewole, Hongwei Bran Li, Anahita Fathi Kazerooni, Nourel Hoda Tahon, Udunna Anazodo, Ahmed W. Moawad, Bjoern Menze, Marius George Linguraru, Mariam Aboian, Benedikt Wiestler, Ujjwal Baid, Gian-Marco Conte, Andreas M. T. Rauschecker, Ayman Nada, Aly H. Abayazeed, Raymond Huang, Maria Correia de Verdier, Jeffrey D. Rudie, Spyridon Bakas, Evan Calabrese

The 2024 Brain Tumor Segmentation Meningioma Radiotherapy (BraTS-MEN-RT)
challenge aims to advance automated segmentation algorithms using the largest
known multi-institutional dataset of radiotherapy planning brain MRIs with
expert-annotated target labels for patients with intact or post-operative
meningioma that underwent either conventional external beam radiotherapy or
stereotactic radiosurgery. Each case includes a defaced 3D post-contrast
T1-weighted radiotherapy planning MRI in its native acquisition space,
accompanied by a single-label "target volume" representing the gross tumor
volume (GTV) and any at-risk post-operative site. Target volume annotations
adhere to established radiotherapy planning protocols, ensuring consistency
across cases and institutions. For pre-operative meningiomas, the target volume
encompasses the entire GTV and associated nodular dural tail, while for
post-operative cases, it includes at-risk resection cavity margins as
determined by the treating institution. Case annotations were reviewed and
approved by expert neuroradiologists and radiation oncologists. Participating
teams will develop, containerize, and evaluate automated segmentation models
using this comprehensive dataset. Model performance will be assessed using the
lesion-wise Dice Similarity Coefficient and the 95% Hausdorff distance. The
top-performing teams will be recognized at the Medical Image Computing and
Computer Assisted Intervention Conference in October 2024. BraTS-MEN-RT is
expected to significantly advance automated radiotherapy planning by enabling
precise tumor segmentation and facilitating tailored treatment, ultimately
improving patient outcomes.

摘要：2024 年腦瘤分割腦膜瘤放射治療 (BraTS-MEN-RT) 挑戰旨在使用已知最大的放射治療規劃腦部 MRI 多機構資料集，來推進自動分割演算法，其中包含接受傳統體外放射治療或立體定向放射外科手術的完整或術後腦膜瘤患者的專家標註目標標籤。每個案例都包含一個去識別化的 3D 對比後 T1 加權放射治療規劃 MRI，在原生擷取空間中，並附有一個代表總腫瘤體積 (GTV) 和任何有風險的術後部位的單標籤「目標體積」。目標體積註解遵循既定的放射治療規劃協議，確保各個案例和機構之間的一致性。對於術前腦膜瘤，目標體積包含整個 GTV 和相關結節性硬腦膜尾，而對於術後病例，則包括由治療機構確定的有風險的切除腔隙邊緣。案例註解已由專家神經放射科醫師和放射腫瘤科醫師審查並核准。參與團隊將使用這個全面的資料集來開發、封裝和評估自動分割模型。模型效能將使用病灶明智的 Dice 相似性係數和 95% Hausdorff 距離進行評估。表現最佳的團隊將在 2024 年 10 月的醫學影像運算和電腦輔助介入會議上獲得肯定。預計 BraTS-MEN-RT 將透過實現精確的腫瘤分割和促進客製化治療來大幅推進自動放射治療規劃，最終改善患者的治療結果。

##### **OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning**
2405.18380v1 by Pengxiang Li, Lu Yin, Xiaowei Gao, Shiwei Liu

The rapid advancements in Large Language Models (LLMs) have revolutionized
various natural language processing tasks. However, the substantial size of
LLMs presents significant challenges in training or fine-tuning. While
parameter-efficient approaches such as low-rank adaptation (LoRA) have gained
popularity, they often compromise performance compared to full-rank
fine-tuning. In this paper, we propose Outlier-weighed Layerwise Sampled
Low-Rank Projection (OwLore), a new memory-efficient fine-tuning approach,
inspired by the layerwise outlier distribution of LLMs, which dynamically
samples pre-trained layers to fine-tune instead of adding additional adaptors.
We first interpret the outlier phenomenon through the lens of Heavy-Tailed
Self-Regularization theory (HT-SR), discovering that layers with more outliers
tend to be more heavy-tailed and consequently better trained. Inspired by this
finding, OwLore strategically assigns higher sampling probabilities to layers
with more outliers to better leverage the knowledge stored in pre-trained LLMs.
To further mitigate the memory demands of fine-tuning, we integrate gradient
low-rank projection into our approach, which facilitates each layer to be
efficiently trained in a low-rank manner. By incorporating the efficient
characteristics of low-rank and optimal layerwise sampling, OwLore
significantly improves the memory-performance trade-off in LLM pruning. Our
extensive experiments across various architectures, including LLaMa2, LLaMa3,
and Mistral, demonstrate that OwLore consistently outperforms baseline
approaches, including full fine-tuning. Specifically, it achieves up to a 1.1%
average accuracy gain on the Commonsense Reasoning benchmark, a 3.0%
improvement on MMLU, and a notable 10% boost on MT-Bench, while being more
memory efficient. OwLore allows us to fine-tune LLaMa2-7B with only 21GB of
memory.

摘要：大型語言模型 (LLM) 的快速進展已經徹底改變了各種自然語言處理任務。然而，LLM 龐大的規模在訓練或微調方面帶來了顯著的挑戰。雖然低秩適應 (LoRA) 等參數高效方法已經廣受歡迎，但與全秩微調相比，它們通常會影響效能。在本文中，我們提出異常加權層級採樣低秩投影 (OwLore)，這是一種新的記憶體高效微調方法，靈感來自 LLM 的層級異常分佈，它動態採樣預訓練層進行微調，而不是新增額外的適配器。我們首先透過重尾自正規化理論 (HT-SR) 的角度來詮釋異常現象，發現具有較多異常值的層級傾向於較重尾，因此訓練效果較佳。受到此發現的啟發，OwLore 策略性地將較高的採樣機率分配給具有較多異常值的層級，以更好地利用儲存在預訓練 LLM 中的知識。為了進一步降低微調的記憶體需求，我們將梯度低秩投影整合到我們的做法中，這有助於以低秩方式有效率地訓練每個層級。透過結合低秩和最佳層級採樣的有效特性，OwLore 大幅改善了 LLM 剪枝中的記憶體效能權衡。我們在各種架構（包括 LLaMa2、LLaMa3 和 Mistral）中進行的廣泛實驗證明，OwLore 持續優於基線方法，包括全微調。具體來說，它在常識推理基準上獲得高達 1.1% 的平均準確度提升，在 MMLU 上提升 3.0%，在 MT-Bench 上提升 10%，同時更具記憶體效率。OwLore 讓我們能夠僅使用 21GB 的記憶體微調 LLaMa2-7B。

##### **LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models**
2405.18377v1 by Anthony Sarah, Sharath Nittur Sridhar, Maciej Szankin, Sairam Sundaresan

The abilities of modern large language models (LLMs) in solving natural
language processing, complex reasoning, sentiment analysis and other tasks have
been extraordinary which has prompted their extensive adoption. Unfortunately,
these abilities come with very high memory and computational costs which
precludes the use of LLMs on most hardware platforms. To mitigate this, we
propose an effective method of finding Pareto-optimal network architectures
based on LLaMA2-7B using one-shot NAS. In particular, we fine-tune LLaMA2-7B
only once and then apply genetic algorithm-based search to find smaller, less
computationally complex network architectures. We show that, for certain
standard benchmark tasks, the pre-trained LLaMA2-7B network is unnecessarily
large and complex. More specifically, we demonstrate a 1.5x reduction in model
size and 1.3x speedup in throughput for certain tasks with negligible drop in
accuracy. In addition to finding smaller, higher-performing network
architectures, our method does so more effectively and efficiently than certain
pruning or sparsification techniques. Finally, we demonstrate how quantization
is complementary to our method and that the size and complexity of the networks
we find can be further decreased using quantization. We believe that our work
provides a way to automatically create LLMs which can be used on less expensive
and more readily available hardware platforms.

摘要：現代大型語言模型 (LLM) 在解決自然語言處理、複雜推理、情緒分析和其他任務的能力非凡，這促使它們被廣泛採用。不幸的是，這些能力伴隨著極高的記憶體和運算成本，這使得 LLM 無法在大多數硬體平台上使用。為了減輕這個問題，我們提出了一種基於 LLaMA2-7B 使用一次性 NAS 尋找 Pareto 最佳網路架構的有效方法。特別是，我們只微調 LLaMA2-7B 一次，然後應用基於遺傳演算法的搜尋來尋找較小、運算複雜度較低的網路架構。我們展示了，對於某些標準基準任務，預先訓練的 LLaMA2-7B 網路過於龐大且複雜。更具體地說，我們展示了模型大小減少了 1.5 倍，某些任務的處理速度加快了 1.3 倍，而準確度幾乎沒有下降。除了找到更小、效能更高的網路架構之外，我們的這種方法比某些剪枝或稀疏化技術更有效率。最後，我們展示了量化如何補充我們的這種方法，以及我們找到的網路的大小和複雜度可以使用量化進一步降低。我們相信，我們的這項工作提供了一種自動建立 LLM 的方法，這些 LLM 可用於價格較低且更容易取得的硬體平台上。

##### **Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning**
2405.18375v1 by Phakphum Artkaew

Commonsense reasoning is one of the important aspect of natural language
understanding, with several benchmarks developed to evaluate it. However, only
a few of these benchmarks are available in languages other than English.
Developing parallel benchmarks facilitates cross-lingual evaluation, enabling a
better understanding of different languages. This research introduces a
collection of Winograd Schemas in Thai, a novel dataset designed to evaluate
commonsense reasoning capabilities in the context of the Thai language.
  Through a methodology involving native speakers, professional translators,
and thorough validation, the schemas aim to closely reflect Thai language
nuances, idioms, and cultural references while maintaining ambiguity and
commonsense challenges. We evaluate the performance of popular large language
models on this benchmark, revealing their strengths, limitations, and providing
insights into the current state-of-the-art. Results indicate that while models
like GPT-4 and Claude-3-Opus achieve high accuracy in English, their
performance significantly drops in Thai, highlighting the need for further
advancements in multilingual commonsense reasoning.

摘要：常識推理是自然語言理解的重要面向之一，已有許多基準可用於評估它。然而，只有少數幾個基準可用於除英語以外的語言。開發平行基準有助於跨語言評估，讓不同語言能有更好的理解。本研究引入了泰語的 Winograd 模式集合，這是一個新穎的資料集，旨在評估泰語語境中的常識推理能力。透過包含母語人士、專業翻譯人員和徹底驗證的方法，這些模式旨在緊密反映泰語的細微差別、慣用語和文化參照，同時保持歧義性和常識挑戰。我們評估了熱門大型語言模型在此基準上的表現，揭示了它們的優點、限制，並提供了對當前最先進技術的見解。結果表明，雖然 GPT-4 和 Claude-3-Opus 等模型在英語中取得了很高的準確性，但它們在泰語中的表現卻大幅下降，這凸顯了多語言常識推理進一步發展的必要性。

##### **PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework**
2405.18369v1 by Eshaan Agarwal, Vivek Dani, Tanuja Ganu, Akshay Nambi

Large language models (LLMs) have revolutionized AI across diverse domains,
showcasing remarkable capabilities. Central to their success is the concept of
prompting, which guides model output generation. However, manual prompt
engineering is labor-intensive and domain-specific, necessitating automated
solutions. This paper introduces PromptWizard, a novel framework leveraging
LLMs to iteratively synthesize and refine prompts tailored to specific tasks.
Unlike existing approaches, PromptWizard optimizes both prompt instructions and
in-context examples, maximizing model performance. The framework iteratively
refines prompts by mutating instructions and incorporating negative examples to
deepen understanding and ensure diversity. It further enhances both
instructions and examples with the aid of a critic, synthesizing new
instructions and examples enriched with detailed reasoning steps for optimal
performance. PromptWizard offers several key features and capabilities,
including computational efficiency compared to state-of-the-art approaches,
adaptability to scenarios with varying amounts of training data, and
effectiveness with smaller LLMs. Rigorous evaluation across 35 tasks on 8
datasets demonstrates PromptWizard's superiority over existing prompt
strategies, showcasing its efficacy and scalability in prompt optimization.

摘要：大型語言模型 (LLM) 在各種領域徹底革新了 AI，展現出非凡的能力。它們成功的核心在於提示的概念，這引導了模型輸出生成。然而，手動提示工程需要大量人力且特定於領域，因此需要自動化解決方案。本文介紹了 PromptWizard，這是一個新穎的框架，利用 LLM 迭代合成和精煉針對特定任務量身定制的提示。與現有方法不同，PromptWizard 同時最佳化提示說明和情境範例，最大化模型效能。該框架透過變異說明並納入負面範例來迭代精煉提示，以加深理解並確保多樣性。它進一步透過批評者的協助來增強說明和範例，合成新的說明和範例，並加入詳細的推理步驟以獲得最佳效能。PromptWizard 提供了幾個關鍵功能，包括與最先進的方法相比的運算效率、適應具有不同訓練資料量的情境，以及使用較小的 LLM 的有效性。在 8 個資料集上的 35 個任務中進行的嚴格評估證明了 PromptWizard 優於現有的提示策略，展示了其在提示最佳化中的功效和可擴充性。

##### **Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs**
2405.18359v1 by Somnath Kumar, Vaibhav Balloli, Mercy Ranjit, Kabir Ahuja, Tanuja Ganu, Sunayana Sitaram, Kalika Bali, Akshay Nambi

Large language models (LLMs) are at the forefront of transforming numerous
domains globally. However, their inclusivity and effectiveness remain limited
for non-Latin scripts and low-resource languages. This paper tackles the
imperative challenge of enhancing the multilingual performance of LLMs without
extensive training or fine-tuning. Through systematic investigation and
evaluation of diverse languages using popular question-answering (QA) datasets,
we present novel techniques that unlock the true potential of LLMs in a
polyglot landscape. Our approach encompasses three key strategies that yield
significant improvements in multilingual proficiency. First, by meticulously
optimizing prompts tailored for polyglot LLMs, we unlock their latent
capabilities, resulting in substantial performance boosts across languages.
Second, we introduce a new hybrid approach that synergizes LLM Retrieval
Augmented Generation (RAG) with multilingual embeddings and achieves improved
multilingual task performance. Finally, we introduce a novel learning approach
that dynamically selects the optimal prompt strategy, LLM model, and embedding
model per query at run-time. This dynamic adaptation maximizes the efficacy of
LLMs across languages, outperforming best static and random strategies.
Additionally, our approach adapts configurations in both offline and online
settings, and can seamlessly adapt to new languages and datasets, leading to
substantial advancements in multilingual understanding and generation across
diverse languages.

摘要：大型語言模型 (LLM) 處於全球無數領域轉型的最前線。然而，其包容性和有效性對於非拉丁字母和低資源語言來說仍然有限。本文應對了在沒有大量訓練或微調的情況下提升 LLM 多語言效能的當務之急。透過使用流行的問答 (QA) 資料集對多種語言進行系統性調查和評估，我們提出了創新的技術，在多語言環境中釋放 LLM 的真正潛力。我們的解決方案包含三項關鍵策略，可顯著提升多語言能力。首先，透過細緻優化針對多語言 LLM 量身打造的提示，我們釋放了其潛在能力，進而大幅提升跨語言的效能。其次，我們引入一種新的混合方法，將 LLM 檢索擴增生成 (RAG) 與多語言嵌入式結合，並達成提升的多語言任務效能。最後，我們引入一種創新的學習方法，可在執行時針對每個查詢動態選擇最佳提示策略、LLM 模型和嵌入式模型。這種動態調整能最大化 LLM 跨語言的效能，優於最佳靜態和隨機策略。此外，我們的解決方案會在離線和線上設定中調整組態，並能無縫調整至新的語言和資料集，進而大幅提升跨多種語言的多語言理解和生成能力。

##### **MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning**
2405.18358v1 by Somnath Kumar, Yash Gadhia, Tanuja Ganu, Akshay Nambi

Recent advancements in Multi-modal Large Language Models (MLLMs) have
significantly improved their performance in tasks combining vision and
language. However, challenges persist in detailed multi-modal understanding,
comprehension of complex tasks, and reasoning over multi-modal information.
This paper introduces MMCTAgent, a novel multi-modal critical thinking agent
framework designed to address the inherent limitations of current MLLMs in
complex visual reasoning tasks. Inspired by human cognitive processes and
critical thinking, MMCTAgent iteratively analyzes multi-modal information,
decomposes queries, plans strategies, and dynamically evolves its reasoning.
Additionally, MMCTAgent incorporates critical thinking elements such as
verification of final answers and self-reflection through a novel approach that
defines a vision-based critic and identifies task-specific evaluation criteria,
thereby enhancing its decision-making abilities. Through rigorous evaluations
across various image and video understanding benchmarks, we demonstrate that
MMCTAgent (with and without the critic) outperforms both foundational MLLMs and
other tool-augmented pipelines.

摘要：最近多模态大型语言模型 (MLLM) 的进步显著改善了它们在结合视觉和语言的任务中的性能。然而，在详细的多模态理解、复杂任务的理解以及对多模态信息进行推理方面仍然存在挑战。本文介绍了 MMCTAgent，这是一个新颖的多模态批判性思维代理框架，旨在解决当前 MLLM 在复杂视觉推理任务中的固有局限性。受人类认知过程和批判性思维的启发，MMCTAgent 迭代分析多模态信息，分解查询，规划策略，并动态地发展其推理。此外，MMCTAgent 结合了批判性思维元素，例如通过一种新颖的方法验证最终答案和自我反省，该方法定义了一个基于视觉的批评者并识别了特定任务的评估标准，从而增强了其决策能力。通过对各种图像和视频理解基准进行严格的评估，我们证明了 MMCTAgent（有和没有批评者）都优于基础 MLLM 和其他工具增强管道。

##### **Faithful Logical Reasoning via Symbolic Chain-of-Thought**
2405.18357v1 by Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu

While the recent Chain-of-Thought (CoT) technique enhances the reasoning
ability of large language models (LLMs) with the theory of mind, it might still
struggle in handling logical reasoning that relies much on symbolic expressions
and rigid deducing rules. To strengthen the logical reasoning capability of
LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully
LLM-based framework that integrates symbolic expressions and logic rules with
CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates
the natural language context into the symbolic format, and then 2) derives a
step-by-step plan to solve the problem with symbolic logical rules, 3) followed
by a verifier to check the translation and reasoning chain. Via thorough
evaluations on 5 standard datasets with both First-Order Logic and Constraint
Optimization symbolic expressions, SymbCoT shows striking improvements over the
CoT method consistently, meanwhile refreshing the current state-of-the-art
performances. We further demonstrate that our system advances in more faithful,
flexible, and explainable logical reasoning. To our knowledge, this is the
first to combine symbolic expressions and rules into CoT for logical reasoning
with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.

摘要：儘管近期鏈式思維 (CoT) 技術透過心智理論增強大型語言模型 (LLM) 的推理能力，它在處理高度依賴符號表達式和嚴格推論規則的邏輯推理方面可能仍有困難。為了強化 LLM 的邏輯推理能力，我們提出了一種新穎的符號鏈式思維，即 SymbCoT，一個完全基於 LLM 的架構，它將符號表達式和邏輯規則與 CoT 提示整合在一起。在技術上，SymbCoT 建立在 LLM 之上，1) 首先將自然語言情境轉換為符號格式，然後 2) 透過符號邏輯規則推導出解決問題的逐步計畫，3) 接著透過驗證器來檢查轉換和推理鏈。透過在 5 個標準資料集上進行徹底評估，包含一階邏輯和約束最佳化符號表達式，SymbCoT 持續展現出比 CoT 方法顯著的進步，同時更新了當前最先進的效能。我們進一步證明我們的系統在更忠實、靈活且可解釋的邏輯推理方面取得進展。據我們所知，這是第一個將符號表達式和規則結合到 CoT 中，以進行 LLM 的邏輯推理。程式碼開放於 https://github.com/Aiden0526/SymbCoT。

##### **A System for Automatic English Text Expansion**
2405.18350v1 by Silvia García Méndez, Milagros Fernández Gavilanes, Enrique Costa Montenegro, Jonathan Juncal Martínez, Francisco Javier González Castaño, Ehud Reiter

We present an automatic text expansion system to generate English sentences,
which performs automatic Natural Language Generation (NLG) by combining
linguistic rules with statistical approaches. Here, "automatic" means that the
system can generate coherent and correct sentences from a minimum set of words.
From its inception, the design is modular and adaptable to other languages.
This adaptability is one of its greatest advantages. For English, we have
created the highly precise aLexiE lexicon with wide coverage, which represents
a contribution on its own. We have evaluated the resulting NLG library in an
Augmentative and Alternative Communication (AAC) proof of concept, both
directly (by regenerating corpus sentences) and manually (from annotations)
using a popular corpus in the NLG field. We performed a second analysis by
comparing the quality of text expansion in English to Spanish, using an ad-hoc
Spanish-English parallel corpus. The system might also be applied to other
domains such as report and news generation.

摘要：我們提出一個自動文字擴充系統來產生英文句子，
它透過結合語言規則與統計方法來執行自動自然語言產生 (NLG)。在此，「自動」意指
系統能從一組最少單字產生連貫且正確的句子。
從其開端以來，設計便是模組化的，且能適應其他語言。
此適應性是其最大的優勢之一。對於英文，我們已建立了高精準度且涵蓋廣泛的 aLexiE 詞彙，本身就代表了一項貢獻。我們在擴充式與另類溝通 (AAC) 概念驗證中評估產生的 NLG 函式庫，兩者皆
直接（透過重新產生語料庫句子）和手動（從註解）
使用 NLG 領域中一個熱門的語料庫。我們透過使用一個特別建立的西班牙文-英文平行語料庫，來比較英文和西班牙文的文字擴充品質，執行第二次分析。系統也可能應用於其他領域，例如報告和新聞產生。

##### **Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation**
2405.18346v1 by Anjanava Biswas, Wrick Talukdar

Comprehensive clinical documentation is crucial for effective healthcare
delivery, yet it poses a significant burden on healthcare professionals,
leading to burnout, increased medical errors, and compromised patient safety.
This paper explores the potential of generative AI (Artificial Intelligence) to
streamline the clinical documentation process, specifically focusing on
generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior,
Intervention, Response, Plan) notes. We present a case study demonstrating the
application of natural language processing (NLP) and automatic speech
recognition (ASR) technologies to transcribe patient-clinician interactions,
coupled with advanced prompting techniques to generate draft clinical notes
using large language models (LLMs). The study highlights the benefits of this
approach, including time savings, improved documentation quality, and enhanced
patient-centered care. Additionally, we discuss ethical considerations, such as
maintaining patient confidentiality and addressing model biases, underscoring
the need for responsible deployment of generative AI in healthcare settings.
The findings suggest that generative AI has the potential to revolutionize
clinical documentation practices, alleviating administrative burdens and
enabling healthcare professionals to focus more on direct patient care.

摘要：全面的臨床文件對於有效的醫療保健服務至關重要，但它對醫療保健專業人員造成了重大負擔，導致倦怠、醫療錯誤增加以及患者安全受到影響。本文探討了生成式 AI（人工智慧）簡化臨床文件流程的可能性，特別專注於生成 SOAP（主觀、客觀、評估、計畫）和 BIRP（行為、介入、反應、計畫）記錄。我們提出了一個案例研究，展示了自然語言處理（NLP）和自動語音辨識（ASR）技術在轉錄患者與臨床醫師互動方面的應用，以及結合先進的提示技術，使用大型語言模型（LLM）生成臨床記錄草稿。這項研究強調了這種方法的好處，包括節省時間、改善文件品質，以及增強以患者為中心的照護。此外，我們討論了道德考量，例如維護患者機密性和解決模型偏差，強調在醫療保健環境中負責任地部署生成式 AI 的必要性。研究結果表明，生成式 AI 有可能徹底改變臨床文件實務，減輕行政負擔，並使醫療保健專業人員能夠更多地專注於直接的患者照護。

##### **The Battle of LLMs: A Comparative Study in Conversational QA Tasks**
2405.18344v1 by Aryan Rangapur, Aman Rangapur

Large language models have gained considerable interest for their impressive
performance on various tasks. Within this domain, ChatGPT and GPT-4, developed
by OpenAI, and the Gemini, developed by Google, have emerged as particularly
popular among early adopters. Additionally, Mixtral by Mistral AI and Claude by
Anthropic are newly released, further expanding the landscape of advanced
language models. These models are viewed as disruptive technologies with
applications spanning customer service, education, healthcare, and finance.
More recently, Mistral has entered the scene, captivating users with its unique
ability to generate creative content. Understanding the perspectives of these
users is crucial, as they can offer valuable insights into the potential
strengths, weaknesses, and overall success or failure of these technologies in
various domains. This research delves into the responses generated by ChatGPT,
GPT-4, Gemini, Mixtral and Claude across different Conversational QA corpora.
Evaluation scores were meticulously computed and subsequently compared to
ascertain the overall performance of these models. Our study pinpointed
instances where these models provided inaccurate answers to questions, offering
insights into potential areas where they might be susceptible to errors. In
essence, this research provides a comprehensive comparison and evaluation of
these state of-the-art language models, shedding light on their capabilities
while also highlighting potential areas for improvement

摘要：大型語言模型因其在各種任務上的出色表現而備受關注。在這個領域中，由 OpenAI 開發的 ChatGPT 和 GPT-4，以及由 Google 開發的 Gemini，在早期採用者中特別受歡迎。此外，Mistral AI 的 Mixtral 和 Anthropic 的 Claude 是最新發布的，進一步擴展了先進語言模型的版圖。這些模型被視為破壞性技術，其應用涵蓋客戶服務、教育、醫療保健和金融。最近，Mistral 進入了這個領域，其獨特創造內容的能力吸引了用戶。了解這些用戶的觀點至關重要，因為他們可以提供寶貴的見解，了解這些技術在各個領域的潛在優勢、劣勢以及整體成功或失敗。本研究深入探討了 ChatGPT、GPT-4、Gemini、Mixtral 和 Claude 在不同的對話式問答語料庫中產生的回應。仔細計算評分，然後進行比較，以確定這些模型的整體表現。我們的研究精確指出這些模型提供不準確答案的問題，提供了對它們可能容易出錯的潛在領域的見解。從本質上講，本研究對這些最先進的語言模型進行了全面的比較和評估，闡明了它們的能力，同時也突出了潛在的改進領域。

##### **Interpretable classification of wiki-review streams**
2405.18335v1 by Silvia García Méndez, Fátima Leal, Benedita Malheiro, Juan Carlos Burguillo Rial

Wiki articles are created and maintained by a crowd of editors, producing a
continuous stream of reviews. Reviews can take the form of additions, reverts,
or both. This crowdsourcing model is exposed to manipulation since neither
reviews nor editors are automatically screened and purged. To protect articles
against vandalism or damage, the stream of reviews can be mined to classify
reviews and profile editors in real-time. The goal of this work is to
anticipate and explain which reviews to revert. This way, editors are informed
why their edits will be reverted. The proposed method employs stream-based
processing, updating the profiling and classification models on each incoming
event. The profiling uses side and content-based features employing Natural
Language Processing, and editor profiles are incrementally updated based on
their reviews. Since the proposed method relies on self-explainable
classification algorithms, it is possible to understand why a review has been
classified as a revert or a non-revert. In addition, this work contributes an
algorithm for generating synthetic data for class balancing, making the final
classification fairer. The proposed online method was tested with a real data
set from Wikivoyage, which was balanced through the aforementioned synthetic
data generation. The results attained near-90 % values for all evaluation
metrics (accuracy, precision, recall, and F-measure).

摘要：維基百科文章由一群編輯建立並維護，產生持續的審查串流。審查可以採取新增、還原或兩者的形式。這種群眾外包模式容易受到操縱，因為審查和編輯都不會自動篩選和清除。為了保護文章免於遭到破壞或損壞，可以挖掘審查串流來分類審查並對編輯進行實時設定。這項工作的目標是預測並說明要還原哪些審查。這樣一來，編輯就能知道為什麼他們的編輯會被還原。所提出的方法採用基於串流的處理，在每個輸入事件中更新設定和分類模型。設定使用基於側面和內容的功能，採用自然語言處理，並根據編輯的審查逐步更新編輯設定檔。由於所提出的方法依賴於可自我解釋的分類演算法，因此可以理解為什麼審查會被分類為還原或非還原。此外，這項工作提供了一種用於產生類別平衡的合成資料演算法，使最終分類更公平。所提出的線上方法已使用來自 Wikivoyage 的真實資料集進行測試，該資料集透過上述合成資料產生進行平衡。結果在所有評估指標（準確度、精確度、召回率和 F 值）中都達到接近 90% 的值。

##### **Frustratingly Easy Test-Time Adaptation of Vision-Language Models**
2405.18330v1 by Matteo Farina, Gianni Franchi, Giovanni Iacca, Massimiliano Mancini, Elisa Ricci

Vision-Language Models seamlessly discriminate among arbitrary semantic
categories, yet they still suffer from poor generalization when presented with
challenging examples. For this reason, Episodic Test-Time Adaptation (TTA)
strategies have recently emerged as powerful techniques to adapt VLMs in the
presence of a single unlabeled image. The recent literature on TTA is dominated
by the paradigm of prompt tuning by Marginal Entropy Minimization, which,
relying on online backpropagation, inevitably slows down inference while
increasing memory. In this work, we theoretically investigate the properties of
this approach and unveil that a surprisingly strong TTA method lies dormant and
hidden within it. We term this approach ZERO (TTA with "zero" temperature),
whose design is both incredibly effective and frustratingly simple: augment N
times, predict, retain the most confident predictions, and marginalize after
setting the Softmax temperature to zero. Remarkably, ZERO requires a single
batched forward pass through the vision encoder only and no backward passes. We
thoroughly evaluate our approach following the experimental protocol
established in the literature and show that ZERO largely surpasses or compares
favorably w.r.t. the state-of-the-art while being almost 10x faster and 13x
more memory-friendly than standard Test-Time Prompt Tuning. Thanks to its
simplicity and comparatively negligible computation, ZERO can serve as a strong
baseline for future work in this field. The code is available at
https://github.com/FarinaMatteo/zero.

摘要：視覺語言模型能順利區分任意的語義類別，但在遇到具挑戰性的範例時，仍會出現概化能力不佳的問題。因此，情境測試時間適應 (TTA) 策略最近浮現為強大的技術，以便在單一未標記影像存在的情況下，調整 VLM。最近關於 TTA 的文獻，主要由邊際熵最小化提示調整的範例所主導，這仰賴線上反向傳播，不可避免地會在增加記憶體的同時，減緩推論。在這項工作中，我們從理論上探討這種方法的特性，並揭示一種令人驚訝的強大 TTA 方法潛藏其中，且尚未被發現。我們將此方法稱為 ZERO（溫度為「零」的 TTA），其設計既非常有效，又令人沮喪地簡單：擴充 N 次、預測、保留最有信心的預測，並在將 Softmax 溫度設為零後進行邊際化。值得注意的是，ZERO 只需要透過視覺編碼器進行單一批次的前向傳遞，而不需要任何反向傳遞。我們徹底評估了我們的做法，遵循文獻中建立的實驗協定，並顯示 ZERO 在幾乎比標準測試時間提示調整快 10 倍且省記憶體 13 倍的情況下，大幅超越或比較有利於最新技術。由於其簡單性和相對可忽略的運算，ZERO 可以作為此領域未來工作的強大基準。程式碼可在 https://github.com/FarinaMatteo/zero 取得。

##### **Histopathology Based AI Model Predicts Anti-Angiogenic Therapy Response in Renal Cancer Clinical Trial**
2405.18327v1 by Jay Jasti, Hua Zhong, Vandana Panwar, Vipul Jarmale, Jeffrey Miyata, Deyssy Carrillo, Alana Christie, Dinesh Rakheja, Zora Modrusan, Edward Ernest Kadel III, Niha Beig, Mahrukh Huseni, James Brugarolas, Payal Kapur, Satwik Rajaram

Predictive biomarkers of treatment response are lacking for metastatic clear
cell renal cell carcinoma (ccRCC), a tumor type that is treated with
angiogenesis inhibitors, immune checkpoint inhibitors, mTOR inhibitors and a
HIF2 inhibitor. The Angioscore, an RNA-based quantification of angiogenesis, is
arguably the best candidate to predict anti-angiogenic (AA) response. However,
the clinical adoption of transcriptomic assays faces several challenges
including standardization, time delay, and high cost. Further, ccRCC tumors are
highly heterogenous, and sampling multiple areas for sequencing is impractical.
Here we present a novel deep learning (DL) approach to predict the Angioscore
from ubiquitous histopathology slides. To overcome the lack of
interpretability, one of the biggest limitations of typical DL models, our
model produces a visual vascular network which is the basis of the model's
prediction. To test its reliability, we applied this model to multiple cohorts
including a clinical trial dataset. Our model accurately predicts the RNA-based
Angioscore on multiple independent cohorts (spearman correlations of 0.77 and
0.73). Further, the predictions help unravel meaningful biology such as
association of angiogenesis with grade, stage, and driver mutation status.
Finally, we find our model can predict response to AA therapy, in both a
real-world cohort and the IMmotion150 clinical trial. The predictive power of
our model vastly exceeds that of CD31, a marker of vasculature, and nearly
rivals the performance (c-index 0.66 vs 0.67) of the ground truth RNA-based
Angioscore at a fraction of the cost. By providing a robust yet interpretable
prediction of the Angioscore from histopathology slides alone, our approach
offers insights into angiogenesis biology and AA treatment response.

摘要：<paragraph>對於轉移性透明細胞腎細胞癌 (ccRCC)，一種用於治療血管生成抑制劑、免疫檢查點抑制劑、mTOR 抑制劑和 HIF2 抑制劑的腫瘤類型，目前缺乏治療反應的預測性生物標記。Angioscore 是一種基於 RNA 的血管生成量化，可以說是預測抗血管生成 (AA) 反應的最佳候選者。然而，轉錄組檢測的臨床應用面臨著標準化、時間延遲和高成本等多項挑戰。此外，ccRCC 腫瘤高度異質，對多個區域進行取樣以進行測序並不切實際。在此，我們提出了一種新穎的深度學習 (DL) 方法，可從普遍存在的組織病理切片中預測 Angioscore。為了克服可解釋性的缺乏，這是典型 DL 模型最大的限制之一，我們的模型產生了一個視覺血管網路，這是模型預測的基礎。為了測試其可靠性，我們將此模型應用於多個群組，包括臨床試驗資料集。我們的模型準確預測了多個獨立群組的基於 RNA 的 Angioscore（spearman 相關係數為 0.77 和 0.73）。此外，這些預測有助於解開有意義的生物學，例如血管生成與等級、階段和驅動突變狀態的關聯。最後，我們發現我們的模型可以預測對 AA 治療的反應，無論是在現實世界群組還是 IMmotion150 臨床試驗中。我們模型的預測能力遠遠超過血管標記 CD31，並且幾乎與基於 RNA 的真實 Angioscore 的效能（c 指數 0.66 對 0.67）相當，而成本卻只是後者的零頭。通過僅從組織病理切片中提供對 Angioscore 的強大且可解釋的預測，我們的方法提供了對血管生成生物學和 AA 治療反應的見解。</paragraph>

##### **DSDL: Data Set Description Language for Bridging Modalities and Tasks in AI Data**
2405.18315v1 by Bin Wang, Linke Ouyang, Fan Wu, Wenchang Ning, Xiao Han, Zhiyuan Zhao, Jiahui Peng, Yiying Jiang, Dahua Lin, Conghui He

In the era of artificial intelligence, the diversity of data modalities and
annotation formats often renders data unusable directly, requiring
understanding and format conversion before it can be used by researchers or
developers with different needs. To tackle this problem, this article
introduces a framework called Dataset Description Language (DSDL) that aims to
simplify dataset processing by providing a unified standard for AI datasets.
DSDL adheres to the three basic practical principles of generic, portable, and
extensible, using a unified standard to express data of different modalities
and structures, facilitating the dissemination of AI data, and easily extending
to new modalities and tasks. The standardized specifications of DSDL reduce the
workload for users in data dissemination, processing, and usage. To further
improve user convenience, we provide predefined DSDL templates for various
tasks, convert mainstream datasets to comply with DSDL specifications, and
provide comprehensive documentation and DSDL tools. These efforts aim to
simplify the use of AI data, thereby improving the efficiency of AI
development.

摘要：在人工智能時代，數據模態和標註格式的多樣性經常導致數據無法直接使用，需要在研究人員或開發人員使用前進行理解和格式轉換以滿足不同的需求。為了解決這個問題，本文介紹了一個名為數據集描述語言（DSDL）的框架，旨在通過為 AI 數據集提供統一標準來簡化數據集處理。DSDL 遵循通用、可攜式和可擴展的三個基本實用原則，使用統一的標準來表示不同模態和結構的數據，促進 AI 數據的傳播，並輕鬆擴展到新的模態和任務。DSDL 的標準化規範減少了用戶在數據傳播、處理和使用中的工作量。為了進一步提高用戶便利性，我們為各種任務提供了預定義的 DSDL 模板，將主流數據集轉換為符合 DSDL 規範，並提供全面的文檔和 DSDL 工具。這些努力旨在簡化 AI 數據的使用，從而提高 AI 開發的效率。

##### **Joint Lemmatization and Morphological Tagging with LEMMING**
2405.18308v1 by Thomas Muller, Ryan Cotterell, Alexander Fraser, Hinrich Schütze

We present LEMMING, a modular log-linear model that jointly models
lemmatization and tagging and supports the integration of arbitrary global
features. It is trainable on corpora annotated with gold standard tags and
lemmata and does not rely on morphological dictionaries or analyzers. LEMMING
sets the new state of the art in token-based statistical lemmatization on six
languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05
to 1.58. We also give empirical evidence that jointly modeling morphological
tags and lemmata is mutually beneficial.

摘要：我們提出 LEMMING，一個模組化對數線性模型，同時建模詞形還原和標記，並支援整合任意全局特徵。它可以在標註有標準標籤和詞形的語料庫上進行訓練，並且不依賴於形態字典或分析器。LEMMING 在六種語言上設定了基於詞元的統計詞形還原新技術；例如，對於捷克語詞形還原，我們將錯誤率降低了 60%，從 4.05 降至 1.58。我們還提供了經驗證據，證明同時建模形態標籤和詞形對彼此都有利。

##### **Semantic are Beacons: A Semantic Perspective for Unveiling Parameter-Efficient Fine-Tuning in Knowledge Learning**
2405.18292v1 by Renzhi Wang, Piji Li

Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of
Large Language Models (LLMs) to various downstream applications. However, the
effectiveness of the PEFT diminishes notably when downstream tasks require
accurate learning of factual knowledge. In this paper, we adopt a semantic
perspective to investigate this phenomenon, uncovering the reasons behind
PEFT's limitations in knowledge learning task. Our findings reveal that: (1)
PEFT presents a notable risk of pushing the model away from the intended
knowledge target; (2) multiple knowledge interfere with each other, and such
interference suppresses the learning and expression of knowledge features.
Based on these insights, we introduce a data filtering strategy to exclude data
that is detrimental to knowledge learning and a re-weighted learning strategy
to make the model attentive to semantic distance during knowledge learning.
Experimental results demonstrate the effectiveness of the proposed method on
open-source large language model, further validate the semantic challenge in
PEFT, thus paving the way for future research.

摘要：參數有效微調 (PEFT) 方法能有效地將大型語言模型 (LLM) 調整至各種下游應用。然而，當下游任務需要準確學習事實知識時，PEFT 的有效性會顯著降低。在本文中，我們採用語義觀點來探討這種現象，揭示 PEFT 在知識學習任務中的限制背後的原因。我們的研究結果表明：(1) PEFT 會帶來顯著風險，將模型推離預期的知識目標；(2) 多重知識會相互干擾，而這種干擾會抑制知識特徵的學習和表達。根據這些見解，我們引入資料過濾策略，排除對知識學習有害的資料，並重新加權學習策略，讓模型在知識學習過程中注意語義距離。實驗結果證明了所提出的方法對開源大型語言模型的有效性，進一步驗證了 PEFT 中的語義挑戰，從而為未來的研究鋪平道路。

##### **FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning**
2405.18291v1 by Zihui Wang, Zheng Wang, Lingjuan Lyu, Zhaopeng Peng, Zhicheng Yang, Chenglu Wen, Rongshan Yu, Cheng Wang, Xiaoliang Fan

Collaborative fairness stands as an essential element in federated learning
to encourage client participation by equitably distributing rewards based on
individual contributions. Existing methods primarily focus on adjusting
gradient allocations among clients to achieve collaborative fairness. However,
they frequently overlook crucial factors such as maintaining consistency across
local models and catering to the diverse requirements of high-contributing
clients. This oversight inevitably decreases both fairness and model accuracy
in practice. To address these issues, we propose FedSAC, a novel Federated
learning framework with dynamic Submodel Allocation for Collaborative fairness,
backed by a theoretical convergence guarantee. First, we present the concept of
"bounded collaborative fairness (BCF)", which ensures fairness by tailoring
rewards to individual clients based on their contributions. Second, to
implement the BCF, we design a submodel allocation module with a theoretical
guarantee of fairness. This module incentivizes high-contributing clients with
high-performance submodels containing a diverse range of crucial neurons,
thereby preserving consistency across local models. Third, we further develop a
dynamic aggregation module to adaptively aggregate submodels, ensuring the
equitable treatment of low-frequency neurons and consequently enhancing overall
model accuracy. Extensive experiments conducted on three public benchmarks
demonstrate that FedSAC outperforms all baseline methods in both fairness and
model accuracy. We see this work as a significant step towards incentivizing
broader client participation in federated learning. The source code is
available at https://github.com/wangzihuixmu/FedSAC.

摘要：協作公平性作為聯邦學習中的一個基本要素，通過基於個人貢獻公平分配獎勵來鼓勵客戶參與。現有方法主要集中於調整客戶之間的梯度分配，以實現協作公平性。然而，它們經常忽視關鍵因素，例如維護本地模型的一致性，以及迎合高貢獻客戶的多樣化需求。這種疏忽在實踐中不可避免地降低了公平性和模型準確性。為了解決這些問題，我們提出了 FedSAC，一個具有動態子模型分配的協作公平性的新聯邦學習框架，並得到理論收斂保證的支持。首先，我們提出了「有界協作公平性 (BCF)」的概念，它通過根據客戶的貢獻調整獎勵來確保公平性。其次，為了實施 BCF，我們設計了一個具有理論公平性保證的子模型分配模組。這個模組激勵高貢獻客戶使用包含各種關鍵神經元的高效能子模型，從而保持本地模型的一致性。第三，我們進一步開發了一個動態聚合模組來自適應地聚合子模型，確保對低頻神經元進行公平處理，從而提高整體模型準確性。在三個公共基準上進行的廣泛實驗表明，FedSAC 在公平性和模型準確性方面優於所有基線方法。我們認為這項工作是激勵更廣泛的客戶參與聯邦學習的重要一步。原始碼可在 https://github.com/wangzihuixmu/FedSAC 中取得。

##### **MODL: Multilearner Online Deep Learning**
2405.18281v1 by Antonios Valkanas, Boris N. Oreshkin, Mark Coates

Online deep learning solves the problem of learning from streams of data,
reconciling two opposing objectives: learn fast and learn deep. Existing work
focuses almost exclusively on exploring pure deep learning solutions, which are
much better suited to handle the "deep" than the "fast" part of the online
learning equation. In our work, we propose a different paradigm, based on a
hybrid multilearner approach. First, we develop a fast online logistic
regression learner. This learner does not rely on backpropagation. Instead, it
uses closed form recursive updates of model parameters, handling the fast
learning part of the online learning problem. We then analyze the existing
online deep learning theory and show that the widespread ODL approach,
currently operating at complexity $O(L^2)$ in terms of the number of layers
$L$, can be equivalently implemented in $O(L)$ complexity. This further leads
us to the cascaded multilearner design, in which multiple shallow and deep
learners are co-trained to solve the online learning problem in a cooperative,
synergistic fashion. We show that this approach achieves state-of-the-art
results on common online learning datasets, while also being able to handle
missing features gracefully. Our code is publicly available at
https://github.com/AntonValk/MODL.

摘要：線上深度學習解決了從資料串流中學習的問題，調和了兩個對立的目標：快速學習和深入學習。現有研究幾乎完全專注於探索純深度學習解決方案，而這更適合處理線上學習方程式中的「深入」而非「快速」部分。在我們的研究中，我們提出一個不同的範例，基於混合多學習者方法。首先，我們開發一個快速的線上邏輯迴歸學習器。這個學習器不依賴反向傳播。相反，它使用模型參數的封閉形式遞迴更新，處理線上學習問題的快速學習部分。然後我們分析現有的線上深度學習理論，並展示廣泛的 ODL 方法，目前在層數 $L$ 方面以 $O(L^2)$ 的複雜度運作，可以用 $O(L)$ 的複雜度等效實作。這進一步引領我們到串聯多學習者設計，其中多個淺層和深層學習器共同訓練，以協作、協同的方式解決線上學習問題。我們展示這個方法在常見的線上學習資料集上取得最先進的結果，同時也能優雅地處理遺失的特徵。我們的程式碼公開於 https://github.com/AntonValk/MODL。

##### **Metaheuristics and Large Language Models Join Forces: Towards an Integrated Optimization Approach**
2405.18272v1 by Camilo Chacón Sartori, Christian Blum, Filippo Bistaffa, Guillem Rodríguez Corominas

Since the rise of Large Language Models (LLMs) a couple of years ago,
researchers in metaheuristics (MHs) have wondered how to use their power in a
beneficial way within their algorithms. This paper introduces a novel approach
that leverages LLMs as pattern recognition tools to improve MHs. The resulting
hybrid method, tested in the context of a social network-based combinatorial
optimization problem, outperforms existing state-of-the-art approaches that
combine machine learning with MHs regarding the obtained solution quality. By
carefully designing prompts, we demonstrate that the output obtained from LLMs
can be used as problem knowledge, leading to improved results. Lastly, we
acknowledge LLMs' potential drawbacks and limitations and consider it essential
to examine them to advance this type of research further.

摘要：隨著大型語言模型（LLM）在幾年前興起，元啟發式（MH）的研究人員開始思考如何在其演算法中善用其力量。本文介紹一種新穎的方法，將 LLM 作為模式辨識工具來提升 MH。這種混合方法在基於社群網路的組合最佳化問題中進行測試，其效能優於現有的結合機器學習與 MH 的最先進方法，在取得的解的品質方面。透過仔細設計提示，我們證明從 LLM 取得的輸出可用作問題知識，進而改善結果。最後，我們承認 LLM 的潛在缺點和限制，並認為審查這些缺點和限制對於進一步推進這類研究至關重要。

##### **Text-only Synthesis for Image Captioning**
2405.18258v1 by Qing Zhou, Junlin Huang, Qiang Li, Junyu Gao, Qi Wang

From paired image-text training to text-only training for image captioning,
the pursuit of relaxing the requirements for high-cost and large-scale
annotation of good quality data remains consistent. In this paper, we propose
Text-only Synthesis for Image Captioning (ToCa), which further advances this
relaxation with fewer human labor and less computing time. Specifically, we
deconstruct caption text into structures and lexical words, which serve as the
fundamental components of the caption. By combining different structures and
lexical words as inputs to the large language model, massive captions that
contain various patterns of lexical words are generated. This method not only
approaches the target domain but also surpasses it by generating new captions,
thereby enhancing the zero-shot generalization ability of the model.
Considering the different levels of data access in the real world, we define
three synthesis scenarios: cross-domain synthesis, in-domain synthesis, and
data-efficient synthesis. Experiments in these scenarios demonstrate the
generalizability, transferability and practicability of ToCa with a nearly 5
CIDEr improvement for zero-shot cross-domain captioning and a maximum increase
of over 20 CIDEr for data-efficient captioning.

摘要：從成對影像文字訓練到僅文字訓練的影像標題，
追求放寬對高成本且大規模標註優質資料的需求仍然是一致的。在本文中，我們提出
僅文字合成影像標題 (ToCa)，這進一步放寬了這個需求，減少人力和運算時間。具體來說，我們
將標題文字解構為結構和詞彙，作為標題的基本組成部分。透過將不同的結構和
詞彙作為輸入到大語言模型，產生包含各種詞彙模式的大量標題。此方法不僅
接近目標領域，而且透過產生新的標題超越目標領域，從而增強模型的零次學習泛化能力。
考慮到現實世界中資料存取的不同層級，我們定義了
三種合成情境：跨領域合成、領域內合成和資料有效率合成。在這些情境中的實驗證明了 ToCa 的泛化性、可傳輸性和實用性，零次學習跨領域標題的 CIDEr 改善了近 5，資料有效率標題的最大增加超過 20 CIDEr。

##### **Active Use of Latent Constituency Representation in both Humans and Large Language Models**
2405.18241v1 by Wei Liu, Ming Xiang, Nai Ding

Understanding how sentences are internally represented in the human brain, as
well as in large language models (LLMs) such as ChatGPT, is a major challenge
for cognitive science. Classic linguistic theories propose that the brain
represents a sentence by parsing it into hierarchically organized constituents.
In contrast, LLMs do not explicitly parse linguistic constituents and their
latent representations remains poorly explained. Here, we demonstrate that
humans and LLMs construct similar latent representations of hierarchical
linguistic constituents by analyzing their behaviors during a novel one-shot
learning task, in which they infer which words should be deleted from a
sentence. Both humans and LLMs tend to delete a constituent, instead of a
nonconstituent word string. In contrast, a naive sequence processing model that
has access to word properties and ordinal positions does not show this
property. Based on the word deletion behaviors, we can reconstruct the latent
constituency tree representation of a sentence for both humans and LLMs. These
results demonstrate that a latent tree-structured constituency representation
can emerge in both the human brain and LLMs.

摘要：了解人類大腦內部如何表示句子，以及像 ChatGPT 這樣的大型語言模型 (LLM)，是認知科學的一項重大挑戰。經典語言學理論提出，大腦通過將句子分析為層級組織的成分來表示句子。相比之下，LLM 沒有明確分析語言成分，其潛在表示仍然解釋不清。在這裡，我們證明了人類和 LLM 通過分析他們在一個新的一次性學習任務中的行為，構建了類似的層級語言成分的潛在表示，在這個任務中，他們推斷出應該從句子中刪除哪些詞。人類和 LLM 都傾向於刪除一個成分，而不是一個非成分詞串。相比之下，一個可以訪問詞彙屬性和序數位置的幼稚序列處理模型沒有顯示這個屬性。基於詞彙刪除行為，我們可以重建人類和 LLM 的句子的潛在成分樹表示。這些結果表明，潛在樹狀成分表示可以在人類大腦和 LLM 中出現。

##### **A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models**
2405.18208v1 by Chengxing Xie, Difan Zou

Recent studies have highlighted their proficiency in some simple tasks like
writing and coding through various reasoning strategies. However, LLM agents
still struggle with tasks that require comprehensive planning, a process that
challenges current models and remains a critical research issue. In this study,
we concentrate on travel planning, a Multi-Phases planning problem, that
involves multiple interconnected stages, such as outlining, information
gathering, and planning, often characterized by the need to manage various
constraints and uncertainties. Existing reasoning approaches have struggled to
effectively address this complex task. Our research aims to address this
challenge by developing a human-like planning framework for LLM agents, i.e.,
guiding the LLM agent to simulate various steps that humans take when solving
Multi-Phases problems. Specifically, we implement several strategies to enable
LLM agents to generate a coherent outline for each travel query, mirroring
human planning patterns. Additionally, we integrate Strategy Block and
Knowledge Block into our framework: Strategy Block facilitates information
collection, while Knowledge Block provides essential information for detailed
planning. Through our extensive experiments, we demonstrate that our framework
significantly improves the planning capabilities of LLM agents, enabling them
to tackle the travel planning task with improved efficiency and effectiveness.
Our experimental results showcase the exceptional performance of the proposed
framework; when combined with GPT-4-Turbo, it attains $10\times$ the
performance gains in comparison to the baseline framework deployed on
GPT-4-Turbo.

摘要：最近的研究強調了 LLM 代理在一些簡單任務（例如透過各種推理策略進行寫作和編碼）上的熟練度。然而，LLM 代理在需要全面規劃的任務上仍有困難，這個過程挑戰了當前的模型，並仍然是一個重要的研究議題。在這項研究中，我們專注於旅遊規劃，這是一個多階段規劃問題，涉及多個相互關聯的階段，例如概述、資訊收集和規劃，通常以需要管理各種限制和不確定性為特徵。現有的推理方法難以有效地解決這個複雜的任務。我們的研究旨在透過為 LLM 代理開發一個類人規劃架構來應對這個挑戰，也就是引導 LLM 代理模擬人類在解決多階段問題時採取的各種步驟。具體來說，我們實施了幾種策略，讓 LLM 代理能夠為每個旅遊查詢產生一個連貫的綱要，反映人類的規劃模式。此外，我們將策略區塊和知識區塊整合到我們的架構中：策略區塊促進資訊收集，而知識區塊則提供詳細規劃的必要資訊。透過我們廣泛的實驗，我們證明了我們的架構顯著地改善了 LLM 代理的規劃能力，使它們能夠以更高的效率和效能來應對旅遊規劃任務。我們的實驗結果展示了所提出的架構的卓越效能；當與 GPT-4-Turbo 結合使用時，與部署在 GPT-4-Turbo 上的基準架構相比，它達到了 $10\times$ 的效能提升。

##### **IAPT: Instruction-Aware Prompt Tuning for Large Language Models**
2405.18203v1 by Wei Zhu, Aaron Xuxiang Tian, Congrui Yin, Yuan Ni, Xiaoling Wang, Guotong Xie

Soft prompt tuning is a widely studied parameter-efficient fine-tuning
method. However, it has a clear drawback: many soft tokens must be inserted
into the input sequences to guarantee downstream performance. As a result, soft
prompt tuning is less considered than Low-rank adaptation (LoRA) in the large
language modeling (LLM) era. In this work, we propose a novel prompt tuning
method, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft
tokens. First, we install a parameter-efficient soft prompt generator at each
Transformer layer to generate idiosyncratic soft prompts for each input
instruction. The generated soft prompts can be seen as a semantic summary of
the input instructions and can effectively guide the output generation. Second,
the soft prompt generators are modules with a bottleneck architecture
consisting of a self-attention pooling operation, two linear projections, and
an activation function. Pilot experiments show that prompt generators at
different Transformer layers require different activation functions. Thus, we
propose to learn the idiosyncratic activation functions for prompt generators
automatically with the help of rational functions. We have conducted
experiments on various tasks, and the experimental results demonstrate that (a)
our IAPT method can outperform the recent baselines with comparable tunable
parameters. (b) Our IAPT method is more efficient than LoRA under the
single-backbone multi-tenant setting.

摘要：軟提示調整是一種廣泛研究的參數有效微調方法。然而，它有一個明顯的缺點：必須將許多軟標記插入輸入序列中以保證下游性能。因此，在大型語言建模 (LLM) 時代，軟提示調整不如低秩適應 (LoRA) 受重視。在這項工作中，我們提出了一種新穎的提示調整方法，即指令感知提示調整 (IAPT)，它只需要四個軟標記。首先，我們在每個 Transformer 層安裝一個參數高效的軟提示生成器，為每個輸入指令生成獨特的軟提示。生成的軟提示可以看作是輸入指令的語義摘要，並且可以有效地指導輸出生成。其次，軟提示生成器是具有瓶頸架構的模組，包括自注意力池化操作、兩個線性投影和一個激活函數。試驗表明，不同 Transformer 層的提示生成器需要不同的激活函數。因此，我們提出使用有理函數的幫助自動學習提示生成器的獨特激活函數。我們對各種任務進行了實驗，實驗結果表明：(a) 我們的 IAPT 方法可以在具有可比較可調參數的情況下優於最近的基準。(b) 在單骨幹多租戶設置下，我們的 IAPT 方法比 LoRA 更有效率。

##### **Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour Cloning**
2405.18196v1 by Vitalis Vosylius, Younggyo Seo, Jafar Uruç, Stephen James

In the field of Robot Learning, the complex mapping between high-dimensional
observations such as RGB images and low-level robotic actions, two inherently
very different spaces, constitutes a complex learning problem, especially with
limited amounts of data. In this work, we introduce Render and Diffuse (R&D) a
method that unifies low-level robot actions and RGB observations within the
image space using virtual renders of the 3D model of the robot. Using this
joint observation-action representation it computes low-level robot actions
using a learnt diffusion process that iteratively updates the virtual renders
of the robot. This space unification simplifies the learning problem and
introduces inductive biases that are crucial for sample efficiency and spatial
generalisation. We thoroughly evaluate several variants of R&D in simulation
and showcase their applicability on six everyday tasks in the real world. Our
results show that R&D exhibits strong spatial generalisation capabilities and
is more sample efficient than more common image-to-action methods.

摘要：在機器人學習領域中，高維度觀察（例如 RGB 影像）和低階機器人動作之間的複雜對應關係，這兩個本質上非常不同的空間，構成了一個複雜的學習問題，特別是在資料量有限的情況下。在這項工作中，我們引入了渲染和擴散 (R&D)，一種使用機器人 3D 模型的虛擬渲染，在影像空間內統一低階機器人動作和 RGB 觀察的方法。使用此聯合觀察動作表示，它使用學習擴散過程計算低階機器人動作，該過程會反覆更新機器人的虛擬渲染。這個空間統一簡化了學習問題，並引入了對樣本效率和空間概括至關重要的歸納偏差。我們在模擬中徹底評估了 R&D 的幾個變體，並展示了它們在現實世界中六項日常任務中的適用性。我們的結果表明，R&D 表現出強大的空間概括能力，並且比更常見的影像到動作方法更具樣本效率。

##### **AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across Any Scenario**
2405.18172v1 by Yuhan Li, Hao Zhou, Wenxiang Shang, Ran Lin, Xuanhong Chen, Bingbing Ni

While image-based virtual try-on has made significant strides, emerging
approaches still fall short of delivering high-fidelity and robust fitting
images across various scenarios, as their models suffer from issues of
ill-fitted garment styles and quality degrading during the training process,
not to mention the lack of support for various combinations of attire.
Therefore, we first propose a lightweight, scalable, operator known as Hydra
Block for attire combinations. This is achieved through a parallel attention
mechanism that facilitates the feature injection of multiple garments from
conditionally encoded branches into the main network. Secondly, to
significantly enhance the model's robustness and expressiveness in real-world
scenarios, we evolve its potential across diverse settings by synthesizing the
residuals of multiple models, as well as implementing a mask region boost
strategy to overcome the instability caused by information leakage in existing
models. Equipped with the above design, AnyFit surpasses all baselines on
high-resolution benchmarks and real-world data by a large gap, excelling in
producing well-fitting garments replete with photorealistic and rich details.
Furthermore, AnyFit's impressive performance on high-fidelity virtual try-ons
in any scenario from any image, paves a new path for future research within the
fashion community.

摘要：儘管基於影像的虛擬試穿已取得顯著進展，但新興方法仍無法在各種場景中提供高保真和穩健的合身影像，因為其模型存在服裝款式不合身和訓練過程中品質下降的問題，更別提缺乏對各種服裝組合的支持。因此，我們首先提出一個輕量級、可擴充的運算元，稱為 Hydra Block，用於服裝組合。這透過一個並行注意力機制實現，該機制促進了將多件服裝的特徵從條件編碼分支注入主網路中。其次，為了顯著增強模型在真實世界場景中的穩健性和表現力，我們透過合成多個模型的殘差，並實作遮罩區域提升策略來克服現有模型中資訊外洩造成的不安定性，從而提升其在不同設定中的潛力。有了上述設計，AnyFit 在高解析度基準和真實世界資料上都大幅超越所有基準，擅長製作合身的服裝，並具有逼真且豐富的細節。此外，AnyFit 在任何影像中任何場景的高保真虛擬試穿上的出色表現，為時尚社群中的未來研究鋪出了一條新路。

##### **Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing**
2405.18166v1 by Wei Zhao, Zhe Li, Yige Li, Ye Zhang, Jun Sun

Large language models (LLMs) are increasingly being adopted in a wide range
of real-world applications. Despite their impressive performance, recent
studies have shown that LLMs are vulnerable to deliberately crafted adversarial
prompts even when aligned via Reinforcement Learning from Human Feedback or
supervised fine-tuning. While existing defense methods focus on either
detecting harmful prompts or reducing the likelihood of harmful responses
through various means, defending LLMs against jailbreak attacks based on the
inner mechanisms of LLMs remains largely unexplored. In this work, we
investigate how LLMs response to harmful prompts and propose a novel defense
method termed \textbf{L}ayer-specific \textbf{Ed}iting (LED) to enhance the
resilience of LLMs against jailbreak attacks. Through LED, we reveal that
several critical \textit{safety layers} exist among the early layers of LLMs.
We then show that realigning these safety layers (and some selected additional
layers) with the decoded safe response from selected target layers can
significantly improve the alignment of LLMs against jailbreak attacks.
Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the
effectiveness of LED, which effectively defends against jailbreak attacks while
maintaining performance on benign prompts. Our code is available at
\url{https://github.com/ledllm/ledllm}.

摘要：大型語言模型 (LLM) 愈來愈廣泛地用於各種真實世界的應用中。儘管它們的表現令人印象深刻，但最近的研究表明，LLM 容易受到蓄意設計的對抗性提示的影響，即使是透過人類回饋的強化學習或監督微調來調整。現有的防禦方法著重於偵測有害提示或透過各種方式降低有害回應的可能性，而針對 LLM 內部機制的越獄攻擊進行防禦仍鮮少被探討。在此研究中，我們探討 LLM 如何回應有害提示，並提出稱為\textbf{L}ayer-specific \textbf{Ed}iting (LED) 的新防禦方法，以增強 LLM 對抗越獄攻擊的韌性。透過 LED，我們揭示了 LLM 的早期層級中存在數個重要的\textit{安全層級}。然後我們展示，重新調整這些安全層級（以及一些選定的其他層級）與選定目標層級中解碼的安全回應，可以顯著改善 LLM 對抗越獄攻擊的調整。針對各種 LLM（例如 Llama2、Mistral）進行的廣泛實驗顯示了 LED 的效能，它有效地防禦了越獄攻擊，同時在良性提示上維持效能。我們的程式碼可以在\url{https://github.com/ledllm/ledllm}取得。

##### **Time Series Representation Models**
2405.18165v1 by Robert Leppich, Vanessa Borst, Veronika Lesch, Samuel Kounev

Time series analysis remains a major challenge due to its sparse
characteristics, high dimensionality, and inconsistent data quality. Recent
advancements in transformer-based techniques have enhanced capabilities in
forecasting and imputation; however, these methods are still resource-heavy,
lack adaptability, and face difficulties in integrating both local and global
attributes of time series. To tackle these challenges, we propose a new
architectural concept for time series analysis based on introspection. Central
to this concept is the self-supervised pretraining of Time Series
Representation Models (TSRMs), which once learned can be easily tailored and
fine-tuned for specific tasks, such as forecasting and imputation, in an
automated and resource-efficient manner. Our architecture is equipped with a
flexible and hierarchical representation learning process, which is robust
against missing data and outliers. It can capture and learn both local and
global features of the structure, semantics, and crucial patterns of a given
time series category, such as heart rate data. Our learned time series
representation models can be efficiently adapted to a specific task, such as
forecasting or imputation, without manual intervention. Furthermore, our
architecture's design supports explainability by highlighting the significance
of each input value for the task at hand. Our empirical study using four
benchmark datasets shows that, compared to investigated state-of-the-art
baseline methods, our architecture improves imputation and forecasting errors
by up to 90.34% and 71.54%, respectively, while reducing the required trainable
parameters by up to 92.43%. The source code is available at
https://github.com/RobertLeppich/TSRM.

摘要：時間序列分析由於其稀疏特徵、高維度和不一致的資料品質，仍然是一項重大挑戰。最近在基於Transformer的技術方面取得的進展增強了預測和估算的能力；然而，這些方法仍然資源密集、缺乏適應性，並且難以整合時間序列的局部和全局屬性。為了應對這些挑戰，我們提出了一個基於內省的時間序列分析新架構概念。這個概念的核心是時間序列表示模型 (TSRM) 的自監督預訓練，一旦學會，就可以輕鬆地針對特定任務（例如預測和估算）進行調整和微調，以自動化和資源有效的方式進行。我們的架構配備了一個靈活的分層表示學習過程，它對缺失資料和異常值具有魯棒性。它可以擷取和學習給定時間序列類別（例如心率資料）的結構、語義和關鍵模式的局部和全局特徵。我們學習的時間序列表示模型可以有效地適應特定任務，例如預測或估算，而無需人工干預。此外，我們的架構設計支援可解釋性，強調每個輸入值對手邊任務的重要性。我們使用四個基準資料集進行的實證研究表明，與所調查的最新基準方法相比，我們的架構將估算和預測誤差分別提高了 90.34% 和 71.54%，同時將所需的訓練參數減少了 92.43%。原始碼可在 https://github.com/RobertLeppich/TSRM 獲得。

##### **Back to the Drawing Board for Fair Representation Learning**
2405.18161v1 by Angéline Pouget, Nikola Jovanović, Mark Vero, Robin Staab, Martin Vechev

The goal of Fair Representation Learning (FRL) is to mitigate biases in
machine learning models by learning data representations that enable high
accuracy on downstream tasks while minimizing discrimination based on sensitive
attributes. The evaluation of FRL methods in many recent works primarily
focuses on the tradeoff between downstream fairness and accuracy with respect
to a single task that was used to approximate the utility of representations
during training (proxy task). This incentivizes retaining only features
relevant to the proxy task while discarding all other information. In extreme
cases, this can cause the learned representations to collapse to a trivial,
binary value, rendering them unusable in transfer settings. In this work, we
argue that this approach is fundamentally mismatched with the original
motivation of FRL, which arises from settings with many downstream tasks
unknown at training time (transfer tasks). To remedy this, we propose to
refocus the evaluation protocol of FRL methods primarily around the performance
on transfer tasks. A key challenge when conducting such an evaluation is the
lack of adequate benchmarks. We address this by formulating four criteria that
a suitable evaluation procedure should fulfill. Based on these, we propose
TransFair, a benchmark that satisfies these criteria, consisting of novel
variations of popular FRL datasets with carefully calibrated transfer tasks. In
this setting, we reevaluate state-of-the-art FRL methods, observing that they
often overfit to the proxy task, which causes them to underperform on certain
transfer tasks. We further highlight the importance of task-agnostic learning
signals for FRL methods, as they can lead to more transferrable
representations.

摘要：公平表徵學習 (FRL) 的目標是透過學習資料表徵，在降低基於敏感屬性的歧視的同時，提升下游任務的高準確度，以減輕機器學習模型中的偏差。最近許多作品中對 FRL 方法的評估主要集中於下游公平性和準確性之間的權衡，而這僅針對用於近似訓練期間表徵效用的單一任務（代理任務）。這會誘使僅保留與代理任務相關的功能，同時捨棄所有其他資訊。在極端情況下，這可能導致學習到的表徵簡化成一個微不足道的二元值，使其無法在轉移設定中使用。在這項工作中，我們主張這種方法與 FRL 的原始動機根本不匹配，而 FRL 的原始動機來自於在訓練時未知許多下游任務（轉移任務）的設定。為了補救這一點，我們建議重新將 FRL 方法的評估協定主要集中在轉移任務的效能上。在進行此類評估時，一個關鍵挑戰是缺乏適當的基準。我們透過制定適當的評估程序應滿足的四個準則來解決這個問題。根據這些準則，我們提出 TransFair，一個滿足這些準則的基準，它包含經過仔細校準的轉移任務，以及流行 FRL 資料集的新穎變體。在這個設定中，我們重新評估最先進的 FRL 方法，觀察到它們經常過度擬合代理任務，這導致它們在某些轉移任務上的表現不佳。我們進一步強調任務不可知學習訊號對 FRL 方法的重要性，因為它們可以產生更具可轉移性的表徵。

##### **4-bit Shampoo for Memory-Efficient Network Training**
2405.18144v1 by Sike Wang, Jia Li, Pan Zhou, Hua Huang

Second-order optimizers, maintaining a matrix termed a preconditioner, are
superior to first-order optimizers in both theory and practice. The states
forming the preconditioner and its inverse root restrict the maximum size of
models trained by second-order optimizers. To address this, compressing 32-bit
optimizer states to lower bitwidths has shown promise in reducing memory usage.
However, current approaches only pertain to first-order optimizers. In this
paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit
Shampoo, maintaining performance similar to that of 32-bit ones. We show that
quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is
remarkably better than quantizing the preconditioner itself both theoretically
and experimentally. By rectifying the orthogonality of the quantized
eigenvector matrix, we enhance the approximation of the preconditioner's
eigenvector matrix, which also benefits the computation of its inverse 4-th
root. Besides, we find that linear square quantization slightly outperforms
dynamic tree quantization when quantizing second-order optimizer states.
Evaluation on various networks for image classification demonstrates that our
4-bit Shampoo achieves comparable test accuracy to its 32-bit counterpart while
being more memory-efficient. The source code will be made available.

摘要：二階最佳化器，維護一個稱為預處理器的矩陣，在理論和實務上都優於一階最佳化器。形成預處理器及其逆根的狀態會限制二階最佳化器訓練模型的最大規模。為了解決這個問題，將 32 位元最佳化器狀態壓縮成較低的位元寬度，已展現出在減少記憶體使用量方面很有前景。然而，目前的方法僅適用於一階最佳化器。在本文中，我們提出第一個 4 位元二階最佳化器，以 4 位元 Shampoo 為例，維持與 32 位元最佳化器相似的效能。我們展示出，將預處理器的特徵向量矩陣量化為 4 位元 Shampoo 在理論和實驗上都顯著優於量化預處理器本身。透過修正量化特徵向量矩陣的正交性，我們增強了對預處理器特徵向量矩陣的近似，這也有助於計算其逆 4 次根。此外，我們發現線性平方量化在量化二階最佳化器狀態時，略優於動態樹量化。在各種網路上的影像分類評估證明，我們的 4 位元 Shampoo 達到與其 32 位元對應版本相當的測試準確度，同時更具記憶體效率。原始碼將會公開。

##### **Unlocking Futures: A Natural Language Driven Career Prediction System for Computer Science and Software Engineering Students**
2405.18139v1 by Sakir Hossain Faruque, Sharun Akter Khushbu, Sharmin Akter

A career is a crucial aspect for any person to fulfill their desires through
hard work. During their studies, students cannot find the best career
suggestions unless they receive meaningful guidance tailored to their skills.
Therefore, we developed an AI-assisted model for early prediction to provide
better career suggestions. Although the task is difficult, proper guidance can
make it easier. Effective career guidance requires understanding a student's
academic skills, interests, and skill-related activities. In this research, we
collected essential information from Computer Science (CS) and Software
Engineering (SWE) students to train a machine learning (ML) model that predicts
career paths based on students' career-related information. To adequately train
the models, we applied Natural Language Processing (NLP) techniques and
completed dataset pre-processing. For comparative analysis, we utilized
multiple classification ML algorithms and deep learning (DL) algorithms. This
study contributes valuable insights to educational advising by providing
specific career suggestions based on the unique features of CS and SWE
students. Additionally, the research helps individual CS and SWE students find
suitable jobs that match their skills, interests, and skill-related activities.

摘要：職業是任何人透過努力工作來實現其慾望的關鍵面向。在求學期間，學生無法找到最適合的職業建議，除非他們獲得針對其技能量身打造的有意義指導。因此，我們開發了一個 AI 輔助模型，用於早期預測，以提供更好的職業建議。儘管這項任務很困難，但適當的指導可以讓它變得更容易。有效的職業指導需要了解學生的學業技能、興趣和與技能相關的活動。在本研究中，我們從電腦科學 (CS) 和軟體工程 (SWE) 學生收集必要資訊，以訓練一個機器學習 (ML) 模型，該模型會根據學生的職業相關資訊來預測職業道路。為了適當地訓練這些模型，我們應用自然語言處理 (NLP) 技術，並完成資料集前處理。為了進行比較分析，我們利用多種分類 ML 演算法和深度學習 (DL) 演算法。這項研究透過提供基於 CS 和 SWE 學生獨特特徵的具體職業建議，為教育諮詢提供了寶貴的見解。此外，這項研究有助於個別 CS 和 SWE 學生找到符合其技能、興趣和與技能相關活動的合適工作。

##### **Exploiting LLM Quantization**
2405.18137v1 by Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev

Quantization leverages lower-precision weights to reduce the memory usage of
large language models (LLMs) and is a key technique for enabling their
deployment on commodity hardware. While LLM quantization's impact on utility
has been extensively explored, this work for the first time studies its adverse
effects from a security perspective. We reveal that widely used quantization
methods can be exploited to produce a harmful quantized LLM, even though the
full-precision counterpart appears benign, potentially tricking users into
deploying the malicious quantized model. We demonstrate this threat using a
three-staged attack framework: (i) first, we obtain a malicious LLM through
fine-tuning on an adversarial task; (ii) next, we quantize the malicious model
and calculate constraints that characterize all full-precision models that map
to the same quantized model; (iii) finally, using projected gradient descent,
we tune out the poisoned behavior from the full-precision model while ensuring
that its weights satisfy the constraints computed in step (ii). This procedure
results in an LLM that exhibits benign behavior in full precision but when
quantized, it follows the adversarial behavior injected in step (i). We
experimentally demonstrate the feasibility and severity of such an attack
across three diverse scenarios: vulnerable code generation, content injection,
and over-refusal attack. In practice, the adversary could host the resulting
full-precision model on an LLM community hub such as Hugging Face, exposing
millions of users to the threat of deploying its malicious quantized version on
their devices.

摘要：量化利用低精度權重來減少大型語言模型 (LLM) 的記憶體使用量，並且是讓它們能夠在商品硬體上部署的關鍵技術。儘管 LLM 量化對效用的影響已廣泛探討，但這項工作首次從安全角度研究其負面影響。我們揭露廣泛使用的量化方法可以被利用來產生有害的量化 LLM，即使全精度對應物看起來是良性的，潛在地誘騙使用者部署惡意的量化模型。我們使用三階段攻擊架構來展示此威脅：(i) 首先，我們透過在對抗任務上進行微調來獲得惡意 LLM；(ii) 接著，我們量化惡意模型並計算出描述所有映射到相同量化模型的全精度模型的約束；(iii) 最後，使用投影梯度下降，我們調整出全精度模型中的中毒行為，同時確保其權重滿足在步驟 (ii) 中計算出的約束。此程序會產生一個 LLM，它在全精度時表現出良性行為，但在量化時，它會遵循在步驟 (i) 中注入的對抗行為。我們在三種不同的情境中以實驗方式證明此類攻擊的可行性和嚴重性：易受攻擊的程式碼產生、內容注入和過度拒絕攻擊。在實務上，對手可以將產生的全精度模型託管在 LLM 社群中心，例如 Hugging Face，讓數百萬使用者面臨在其裝置上部署其惡意量化版本的威脅。

##### **Low-Resource Crop Classification from Multi-Spectral Time Series Using Lossless Compressors**
2405.18119v1 by Wei Cheng, Hongrui Ye, Xiao Wen, Jiachen Zhang, Jiping Xu, Feifan Zhang

Deep learning has significantly improved the accuracy of crop classification
using multispectral temporal data. However, these models have complex
structures with numerous parameters, requiring large amounts of data and costly
training. In low-resource situations with fewer labeled samples, deep learning
models perform poorly due to insufficient data. Conversely, compressors are
data-type agnostic, and non-parametric methods do not bring underlying
assumptions. Inspired by this insight, we propose a non-training alternative to
deep learning models, aiming to address these situations. Specifically, the
Symbolic Representation Module is proposed to convert the reflectivity into
symbolic representations. The symbolic representations are then
cross-transformed in both the channel and time dimensions to generate symbolic
embeddings. Next, the Multi-scale Normalised Compression Distance (MNCD) is
designed to measure the correlation between any two symbolic embeddings.
Finally, based on the MNCDs, high quality crop classification can be achieved
using only a k-nearest-neighbor classifier kNN. The entire framework is
ready-to-use and lightweight. Without any training, it outperformed, on
average, 7 advanced deep learning models trained at scale on three benchmark
datasets. It also outperforms more than half of these models in the few-shot
setting with sparse crop labels. Therefore, the high performance and robustness
of our non-training framework makes it truly applicable to real-world crop
mapping. Codes are available at:
https://github.com/qinfengsama/Compressor-Based-Crop-Mapping.

摘要：深度学习已大幅提升作物分类的准确度，使用多光谱时间数据。然而，这些模型的结构复杂，参数众多，需要大量数据和高成本的训练。在低资源情况下，带标签样本较少，深度学习模型由于数据不足而表现不佳。相反，压缩器与数据类型无关，非参数方法不会带来潜在假设。受此启发，我们提出了一种深度学习模型的非训练替代方案，旨在解决这些情况。具体来说，提出了符号表征模块，将反射率转换为符号表征。然后在通道和时间维度中交叉转换符号表征，以生成符号嵌入。接下来，设计了多尺度归一化压缩距离 (MNCD)，以测量任意两个符号嵌入之间的相关性。最后，基于 MNCD，仅使用 k 近邻分类器 kNN 即可实现高质量的作物分类。整个框架即用且轻量级。在没有经过任何训练的情况下，它平均优于三个基准数据集上大规模训练的 7 个先进深度学习模型。在稀疏作物标签的少样本设置中，它也优于一半以上的这些模型。因此，我们非训练框架的高性能和鲁棒性使其真正适用于实际作物制图。代码可在此处获得：https://github.com/qinfengsama/Compressor-Based-Crop-Mapping。

##### **Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting**
2405.18113v1 by Hongda Sun, Hongzhan Lin, Haiyu Yan, Chen Zhu, Yang Song, Xin Gao, Shuo Shang, Rui Yan

The emergence of online recruitment services has revolutionized the
traditional landscape of job seeking and recruitment, necessitating the
development of high-quality industrial applications to improve person-job
fitting. Existing methods generally rely on modeling the latent semantics of
resumes and job descriptions and learning a matching function between them.
Inspired by the powerful role-playing capabilities of Large Language Models
(LLMs), we propose to introduce a mock interview process between LLM-played
interviewers and candidates. The mock interview conversations can provide
additional evidence for candidate evaluation, thereby augmenting traditional
person-job fitting based solely on resumes and job descriptions. However,
characterizing these two roles in online recruitment still presents several
challenges, such as developing the skills to raise interview questions,
formulating appropriate answers, and evaluating two-sided fitness. To this end,
we propose MockLLM, a novel applicable framework that divides the person-job
matching process into two modules: mock interview generation and two-sided
evaluation in handshake protocol, jointly enhancing their performance through
collaborative behaviors between interviewers and candidates. We design a
role-playing framework as a multi-role and multi-behavior paradigm to enable a
single LLM agent to effectively behave with multiple functions for both
parties. Moreover, we propose reflection memory generation and dynamic prompt
modification techniques to refine the behaviors of both sides, enabling
continuous optimization of the augmented additional evidence. Extensive
experimental results show that MockLLM can achieve the best performance on
person-job matching accompanied by high mock interview quality, envisioning its
emerging application in real online recruitment in the future.

摘要：線上招募服務的出現，徹底改變了求職和招募的傳統模式，因此需要開發高品質的產業應用程式，以改善人職配對。現有方法通常仰賴模擬履歷和職務說明的潛在語意，並學習它們之間的配對功能。受到大型語言模型 (LLM) 強大的角色扮演能力啟發，我們建議在 LLM 扮演的面試官和應徵者之間，導入模擬面試流程。模擬面試對話可以提供額外的證據，用於應徵者評估，從而擴增僅根據履歷和職務說明所做的傳統人職配對。然而，在線上招募中，這兩個角色的特性仍然存在一些挑戰，例如培養提出面試問題、擬定適當答案以及評估雙方適性的技能。為此，我們提出 MockLLM，一個新穎且適用的架構，將人職配對流程分為兩個模組：模擬面試產生，以及握手協定的雙向評估，透過面試官和應徵者之間的協作行為，共同提升其效能。我們設計一個角色扮演架構，作為一個多角色和多行為的範例，以使單一 LLM 代理人能夠有效地為雙方執行多項功能。此外，我們提出反省記憶產生和動態提示修改技術，以改善雙方的行為，讓擴增的額外證據能夠持續最佳化。廣泛的實驗結果顯示，MockLLM 能夠在人職配對中達成最佳效能，並伴隨著高品質的模擬面試，預見其在未來真實線上招募中的新興應用。

##### **ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator**
2405.18111v1 by Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, Lei Sha

Large language model (LLM) has proven to benefit a lot from retrieval
augmentation in alleviating hallucinations confronted with knowledge-intensive
questions. Retrieval-augmented generation (RAG) adopts IR-based techniques
utilizing semantic-relevant documents as the generator's input context and
realizes external knowledge injection. However, on today's Internet which is
flooded with content generated by LLMs, there are too many "related yet
useless" documents or even fake knowledge fabricated by LLMs, which will
introduce extra noise to the generator and distract it from giving correct
results. To this end, we regard the training of the RAG generator model as a
multi-agent adversarial-defensive system, guiding the generator to have a
better taste of whether a specific document helps answer the question through
the Adversarial Tuning in a Multi-agent (ATM) system to strengthen the
generator's robustness in an RAG pipeline. After rounds of multi-agent
iterative tuning, we find that the ATM Generator can eventually discriminate
useful documents amongst LLM fabrications and achieve better performance than
strong baselines.

摘要：大型語言模型 (LLM) 已證明從檢索增強中受益良多，以減輕在面對知識密集型問題時產生的幻覺。檢索增強生成 (RAG) 採用基於 IR 的技術，利用語義相關文件作為生成器的輸入上下文，並實現外部知識注入。然而，在當今充斥著 LLM 生成的內容的網際網路上，有太多「相關但無用」的文件，甚至是 LLM 編造的假知識，這會為生成器引入額外的雜訊，並使其無法給出正確的結果。為此，我們將 RAG 生成器模型的訓練視為一個多主體對抗防禦系統，透過多主體中的對抗調整 (ATM) 系統引導生成器更能判斷特定文件是否有助於回答問題，以增強 RAG 管線中生成器的穩健性。經過多輪多主體迭代調整後，我們發現 ATM 生成器最終可以在 LLM 編造中區分有用的文件，並實現比強大基準更好的效能。

##### **A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation**
2405.18106v1 by Kai Chen, Ye Wang, Yitong Li, Aiping Li, Han Yu, Xin Song

Temporal knowledge graph (TKG) reasoning has two settings: interpolation
reasoning and extrapolation reasoning. Both of them draw plenty of research
interest and have great significance. Methods of the former de-emphasize the
temporal correlations among facts sequences, while methods of the latter
require strict chronological order of knowledge and ignore inferring clues
provided by missing facts of the past. These limit the practicability of TKG
applications as almost all of the existing TKG reasoning methods are designed
specifically to address either one setting. To this end, this paper proposes an
original Temporal PAth-based Reasoning (TPAR) model for both the interpolation
and extrapolation reasoning. TPAR performs a neural-driven symbolic reasoning
fashion that is robust to ambiguous and noisy temporal data and with fine
interpretability as well. Comprehensive experiments show that TPAR outperforms
SOTA methods on the link prediction task for both the interpolation and the
extrapolation settings. A novel pipeline experimental setting is designed to
evaluate the performances of SOTA combinations and the proposed TPAR towards
interpolation and extrapolation reasoning. More diverse experiments are
conducted to show the robustness and interpretability of TPAR.

摘要：時序知識圖譜 (TKG) 推理有兩種設定：內插推理和外插推理。兩者都引起許多研究興趣，且具有重大意義。前者的方法不強調事實序列之間的時間關聯性，而後者的方法需要嚴格的知識時間順序，並忽略推論過去缺失事實提供的線索。這些限制了 TKG 應用的實用性，因為幾乎所有現有的 TKG 推理方法都是專門設計來解決其中一種設定。為此，本文提出了一個基於時間路徑的推理 (TPAR) 模型，用於內插和外插推理。TPAR 執行神經驅動的符號推理方式，對模稜兩可且有雜訊的時間資料具有穩健性，且具有良好的可解釋性。全面的實驗表明，對於內插和外插設定的連結預測任務，TPAR 的表現優於 SOTA 方法。設計了一個新穎的管線實驗設定，用於評估 SOTA 組合和提出的 TPAR 對內插和外插推理的效能。進行更多樣化的實驗，以顯示 TPAR 的穩健性和可解釋性。

##### **LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins**
2405.18092v1 by Yuchen Xia, Daniel Dittler, Nasser Jazdi, Haonan Chen, Michael Weyrich

This paper presents a novel design of a multi-agent system framework that
applies a large language model (LLM) to automate the parametrization of process
simulations in digital twins. We propose a multi-agent framework that includes
four types of agents: observation, reasoning, decision and summarization. By
enabling dynamic interaction between LLM agents and simulation model, the
developed system can automatically explore the parametrization of the
simulation and use heuristic reasoning to determine a set of parameters to
control the simulation to achieve an objective. The proposed approach enhances
the simulation model by infusing it with heuristics from LLM and enables
autonomous search for feasible parametrization to solve a user task.
Furthermore, the system has the potential to increase user-friendliness and
reduce the cognitive load on human users by assisting in complex
decision-making processes. The effectiveness and functionality of the system
are demonstrated through a case study, and the visualized demos are available
at a GitHub Repository: https://github.com/YuchenXia/LLMDrivenSimulation

摘要：這篇論文提出了一個多代理系統架構的新穎設計，該架構應用大型語言模型 (LLM) 來自動化數位分身中流程模擬的參數化。我們提出一個多代理架構，其中包括四種類型的代理：觀察、推理、決策和摘要。透過啟用 LLM 代理和模擬模型之間的動態互動，開發出的系統可以自動探索模擬的參數化，並使用啟發式推理來確定一組參數以控制模擬以達成目標。所提出的方法透過注入來自 LLM 的啟發式方法來增強模擬模型，並針對可行的參數化進行自主搜尋以解決使用者任務。此外，該系統有潛力透過協助複雜的決策制定流程來增加使用者友善度並降低人類使用者的認知負擔。系統的有效性和功能透過案例研究得到證明，視覺化示範可在 GitHub 存放庫中取得：https://github.com/YuchenXia/LLMDrivenSimulation

##### **Design Principles for Falsifiable, Replicable and Reproducible Empirical ML Research**
2405.18077v1 by Daniel Vranješ, Oliver Niggemann

Empirical research plays a fundamental role in the machine learning domain.
At the heart of impactful empirical research lies the development of clear
research hypotheses, which then shape the design of experiments. The execution
of experiments must be carried out with precision to ensure reliable results,
followed by statistical analysis to interpret these outcomes. This process is
key to either supporting or refuting initial hypotheses. Despite its
importance, there is a high variability in research practices across the
machine learning community and no uniform understanding of quality criteria for
empirical research. To address this gap, we propose a model for the empirical
research process, accompanied by guidelines to uphold the validity of empirical
research. By embracing these recommendations, greater consistency, enhanced
reliability and increased impact can be achieved.

摘要：經驗研究在機器學習領域中扮演著基礎性的角色。
在有影響力的經驗研究的核心是明確的研究假設的發展，這進而塑造了實驗的設計。
實驗的執行必須精確地進行以確保可靠的結果，接著進行統計分析來詮釋這些結果。
這個程序是支持或反駁最初假設的關鍵。儘管其重要性，
在機器學習社群中，研究實務存在高度變異，且對於經驗研究的品質標準沒有統一的理解。
為了解決這個差距，我們提出了一個經驗研究程序模型，並附上維持經驗研究有效性的準則。
透過採用這些建議，可以達成更佳的一致性、增強的可靠性，並增加影響力。

##### **Towards Dialogues for Joint Human-AI Reasoning and Value Alignment**
2405.18073v1 by Elfia Bezou-Vrakatseli, Oana Cocarascu, Sanjay Modgil

We argue that enabling human-AI dialogue, purposed to support joint reasoning
(i.e., 'inquiry'), is important for ensuring that AI decision making is aligned
with human values and preferences. In particular, we point to logic-based
models of argumentation and dialogue, and suggest that the traditional focus on
persuasion dialogues be replaced by a focus on inquiry dialogues, and the
distinct challenges that joint inquiry raises. Given recent dramatic advances
in the performance of large language models (LLMs), and the anticipated
increase in their use for decision making, we provide a roadmap for research
into inquiry dialogues for supporting joint human-LLM reasoning tasks that are
ethically salient, and that thereby require that decisions are value aligned.

摘要：我們主張，讓人機對話能夠支持共同推理（即「探究」）非常重要，這能確保 AI 決策符合人類價值觀和偏好。特別是，我們指出基於邏輯的論證和對話模型，並建議將傳統上關注說服對話的焦點轉移到探究對話，以及共同探究提出的不同挑戰。鑑於大型語言模型 (LLM) 效能近期大幅提升，以及預期它們在決策制定中的使用將增加，我們提供了一份研究探究對話的路線圖，以支援倫理上顯著的人機 LLM 共同推理任務，並因此要求決策符合價值觀。

##### **A Survey of Latent Factor Models in Recommender Systems**
2405.18068v1 by Hind I. Alshbanat, Hafida Benhidour, Said Kerrache

Recommender systems are essential tools in the digital era, providing
personalized content to users in areas like e-commerce, entertainment, and
social media. Among the many approaches developed to create these systems,
latent factor models have proven particularly effective. This survey
systematically reviews latent factor models in recommender systems, focusing on
their core principles, methodologies, and recent advancements. The literature
is examined through a structured framework covering learning data, model
architecture, learning strategies, and optimization techniques. The analysis
includes a taxonomy of contributions and detailed discussions on the types of
learning data used, such as implicit feedback, trust, and content data, various
models such as probabilistic, nonlinear, and neural models, and an exploration
of diverse learning strategies like online learning, transfer learning, and
active learning. Furthermore, the survey addresses the optimization strategies
used to train latent factor models, improving their performance and
scalability. By identifying trends, gaps, and potential research directions,
this survey aims to provide valuable insights for researchers and practitioners
looking to advance the field of recommender systems.

摘要：推薦系統在數位時代中是不可或缺的工具，在電子商務、娛樂和社群媒體等領域中提供個人化的內容給使用者。在眾多用來建立這些系統的方法中，潛在因子模型已被證實特別有效。這項調查有系統地檢視推薦系統中的潛在因子模型，專注於其核心原則、方法和最近的進展。這份文獻透過一個結構化的架構來探討，涵蓋學習資料、模型架構、學習策略和最佳化技術。分析包括貢獻分類和對所使用學習資料類型的詳細討論，例如隱式回饋、信任和內容資料，各種模型，例如機率模型、非線性模型和神經網路模型，以及對各種學習策略的探討，例如線上學習、遷移學習和主動學習。此外，這項調查探討用來訓練潛在因子模型的最佳化策略，以提升其效能和可擴充性。透過找出趨勢、差距和潛在的研究方向，這項調查旨在為尋求提升推薦系統領域的研究人員和從業人員提供有價值的見解。

##### **EffoVPR: Effective Foundation Model Utilization for Visual Place Recognition**
2405.18065v1 by Issar Tzachor, Boaz Lerner, Matan Levy, Michael Green, Tal Berkovitz Shalev, Gavriel Habib, Dvir Samuel, Noam Korngut Zailer, Or Shimshi, Nir Darshan, Rami Ben-Ari

The task of Visual Place Recognition (VPR) is to predict the location of a
query image from a database of geo-tagged images. Recent studies in VPR have
highlighted the significant advantage of employing pre-trained foundation
models like DINOv2 for the VPR task. However, these models are often deemed
inadequate for VPR without further fine-tuning on task-specific data. In this
paper, we propose a simple yet powerful approach to better exploit the
potential of a foundation model for VPR. We first demonstrate that features
extracted from self-attention layers can serve as a powerful re-ranker for VPR.
Utilizing these features in a zero-shot manner, our method surpasses previous
zero-shot methods and achieves competitive results compared to supervised
methods across multiple datasets. Subsequently, we demonstrate that a
single-stage method leveraging internal ViT layers for pooling can generate
global features that achieve state-of-the-art results, even when reduced to a
dimensionality as low as 128D. Nevertheless, incorporating our local foundation
features for re-ranking, expands this gap. Our approach further demonstrates
remarkable robustness and generalization, achieving state-of-the-art results,
with a significant gap, in challenging scenarios, involving occlusion,
day-night variations, and seasonal changes.

摘要：視覺場景辨識 (VPR) 的任務是從地理標籤影像資料庫中預測查詢影像的位置。VPR 的近期研究強調了在 VPR 任務中採用預先訓練好的基礎模型（如 DINOv2）的顯著優勢。然而，這些模型通常被認為在沒有針對特定任務資料進行進一步微調的情況下，不足以用於 VPR。在本文中，我們提出了一個簡單但強大的方法，以更好地發揮基礎模型在 VPR 中的潛力。我們首先證明從自我注意層中提取的特徵可以用作 VPR 的強大重新排名器。我們的模型以零次學習的方式使用這些特徵，超越了先前的零次學習方法，並在多個資料集上達到了與監督式方法相當的競爭力。隨後，我們證明了一個利用內部 ViT 層進行池化的單階段方法，即使降至低至 128D 的維度，也能產生達到最先進結果的全局特徵。儘管如此，結合我們用於重新排名的局部基礎特徵，擴大了這個差距。我們的模型進一步證明了卓越的穩健性和泛化能力，在涉及遮擋、晝夜變化和季節變化的挑戰性場景中，達到了最先進的結果，並有顯著的差距。

##### **Automated Real-World Sustainability Data Generation from Images of Buildings**
2405.18064v1 by Peter J Bentley, Soo Ling Lim, Rajat Mathur, Sid Narang

When data on building features is unavailable, the task of determining how to
improve that building in terms of carbon emissions becomes infeasible. We show
that from only a set of images, a Large Language Model with appropriate prompt
engineering and domain knowledge can successfully estimate a range of building
features relevant for sustainability calculations. We compare our novel
image-to-data method with a ground truth comprising real building data for 47
apartments and achieve accuracy better than a human performing the same task.
We also demonstrate that the method can generate tailored recommendations to
the owner on how best to improve their properties and discuss methods to scale
the approach.

摘要：當建築物特徵的數據不可用時，決定如何改善該建築物在碳排放方面的任務變得不可行。我們展示，僅從一組圖像中，具有適當提示工程和領域知識的大語言模型就能成功估計出一系列與永續計算相關的建築特徵。我們將我們新穎的影像轉數據方法與包含 47 間公寓真實建築數據的地面實況進行比較，並取得比人類執行相同任務更高的準確度。我們也展示該方法能產生針對業主的客製化建議，說明如何最佳改善他們的財產，並討論擴展該方法的方法。

##### **Context is Important in Depressive Language: A Study of the Interaction Between the Sentiments and Linguistic Markers in Reddit Discussions**
2405.18061v1 by Neha Sharma, Kairit Sirts

Research exploring linguistic markers in individuals with depression has
demonstrated that language usage can serve as an indicator of mental health.
This study investigates the impact of discussion topic as context on linguistic
markers and emotional expression in depression, using a Reddit dataset to
explore interaction effects. Contrary to common findings, our sentiment
analysis revealed a broader range of emotional intensity in depressed
individuals, with both higher negative and positive sentiments than controls.
This pattern was driven by posts containing no emotion words, revealing the
limitations of the lexicon based approaches in capturing the full emotional
context. We observed several interesting results demonstrating the importance
of contextual analyses. For instance, the use of 1st person singular pronouns
and words related to anger and sadness correlated with increased positive
sentiments, whereas a higher rate of present-focused words was associated with
more negative sentiments. Our findings highlight the importance of discussion
contexts while interpreting the language used in depression, revealing that the
emotional intensity and meaning of linguistic markers can vary based on the
topic of discussion.

摘要：探討憂鬱症患者語言標記的研究已證明，語言使用可以作為心理健康的指標。本研究使用 Reddit 資料集探討互動效應，調查討論主題作為脈絡對憂鬱症患者語言標記和情緒表達的影響。與常見的發現相反，我們的語意分析揭示了憂鬱症患者更廣泛的情緒強度，同時具有比對照組更高和更低的情緒。此模式是由不包含情緒字詞的貼文所驅動，揭示了基於詞彙的方法在捕捉完整情緒脈絡上的限制。我們觀察到幾個有趣的結果，證明了脈絡分析的重要性。例如，使用第一人稱單數代名詞和與憤怒和悲傷相關的字詞與增加的正面情緒相關，而較高的現在時焦點字詞使用率則與更負面的情緒相關。我們的發現強調了在詮釋憂鬱症中使用的語言時，討論脈絡的重要性，揭示了語言標記的情緒強度和意義會根據討論的主題而有所不同。

##### **PRFashion24: A Dataset for Sentiment Analysis of Fashion Products Reviews in Persian**
2405.18060v1 by Mehrimah Amirpour, Reza Azmi

The PRFashion24 dataset is a comprehensive Persian dataset collected from
various online fashion stores, spanning from April 2020 to March 2024. With
767,272 reviews, it is the first dataset in its kind that encompasses diverse
categories within the fashion industry in the Persian language. The goal of
this study is to harness deep learning techniques, specifically Long Short-Term
Memory (LSTM) networks and a combination of Bidirectional LSTM and
Convolutional Neural Network (BiLSTM-CNN), to analyze and reveal sentiments
towards online fashion shopping. The LSTM model yielded an accuracy of 81.23%,
while the BiLSTM-CNN model reached 82.89%. This research aims not only to
introduce a diverse dataset in the field of fashion but also to enhance the
public's understanding of opinions on online fashion shopping, which
predominantly reflect a positive sentiment. Upon publication, both the
optimized models and the PRFashion24 dataset will be available on GitHub.

摘要：PRFashion24 資料集是一個從各種線上時裝商店收集而來的波斯語綜合資料集，時間跨度從 2020 年 4 月到 2024 年 3 月。它擁有 767,272 則評論，是同類資料集中第一個涵蓋波斯語時裝產業中各種類別的資料集。本研究的目標是利用深度學習技術，特別是長短期記憶 (LSTM) 網路和雙向 LSTM 與捲積神經網路 (BiLSTM-CNN) 的組合，來分析並揭露針對線上時裝購物的觀感。LSTM 模型的準確度為 81.23%，而 BiLSTM-CNN 模型的準確度達到 82.89%。本研究不僅旨在介紹時裝領域中一個多樣化的資料集，也旨在加深大眾對線上時裝購物意見的理解，而這些意見主要反映出正面觀感。在公開後，最佳化模型和 PRFashion24 資料集都將在 GitHub 上提供。

##### **2BP: 2-Stage Backpropagation**
2405.18047v1 by Christopher Rae, Joseph K. L. Lee, James Richings

As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed
the memory capacity of a single accelerator, necessitating the sharding of
model parameters across multiple accelerators. Pipeline parallelism is a
commonly used sharding strategy for training large DNNs. However, current
implementations of pipeline parallelism are being unintentionally bottlenecked
by the automatic differentiation tools provided by ML frameworks. This paper
introduces 2-stage backpropagation (2BP). By splitting the backward propagation
step into two separate stages, we can reduce idle compute time. We tested 2BP
on various model architectures and pipelining schedules, achieving increases in
throughput in all cases. Using 2BP, we were able to achieve a 1.70x increase in
throughput compared to traditional methods when training a LLaMa-like
transformer with 7 billion parameters across 4 GPUs.

摘要：隨著深度神經網路 (DNN) 在規模和複雜度上不斷增加，它們通常會超過單個加速器的記憶體容量，因此必須將模型參數分片到多個加速器。管線平行化是一種常用的分片策略，用於訓練大型 DNN。然而，目前管線並行的實作方式會受到 ML 框架提供的自動微分工具無意間的瓶頸限制。本文介紹了 2 階段反向傳播 (2BP)。透過將反向傳播步驟分為兩個獨立的階段，我們可以減少閒置運算時間。我們在各種模型架構和管線排程上測試了 2BP，在所有情況下都達到了吞吐量的提升。使用 2BP，我們能夠在使用 4 個 GPU 訓練具有 70 億個參數的類似 LLaMa 的Transformer時，與傳統方法相比，吞吐量提升了 1.70 倍。

##### **Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew Resilience**
2405.18040v1 by Thanh Trung Huynh, Trong Bang Nguyen, Phi Le Nguyen, Thanh Tam Nguyen, Matthias Weidlich, Quoc Viet Hung Nguyen, Karl Aberer

Federated learning (FL) has recently emerged as a compelling machine learning
paradigm, prioritizing the protection of privacy for training data. The
increasing demand to address issues such as ``the right to be forgotten'' and
combat data poisoning attacks highlights the importance of techniques, known as
\textit{unlearning}, which facilitate the removal of specific training data
from trained FL models. Despite numerous unlearning methods proposed for
centralized learning, they often prove inapplicable to FL due to fundamental
differences in the operation of the two learning paradigms. Consequently,
unlearning in FL remains in its early stages, presenting several challenges.
Many existing unlearning solutions in FL require a costly retraining process,
which can be burdensome for clients. Moreover, these methods are primarily
validated through experiments, lacking theoretical assurances. In this study,
we introduce Fast-FedUL, a tailored unlearning method for FL, which eliminates
the need for retraining entirely. Through meticulous analysis of the target
client's influence on the global model in each round, we develop an algorithm
to systematically remove the impact of the target client from the trained
model. In addition to presenting empirical findings, we offer a theoretical
analysis delineating the upper bound of our unlearned model and the exact
retrained model (the one obtained through retraining using untargeted clients).
Experimental results with backdoor attack scenarios indicate that Fast-FedUL
effectively removes almost all traces of the target client, while retaining the
knowledge of untargeted clients (obtaining a high accuracy of up to 98\% on the
main task). Significantly, Fast-FedUL attains the lowest time complexity,
providing a speed that is 1000 times faster than retraining. Our source code is
publicly available at \url{https://github.com/thanhtrunghuynh93/fastFedUL}.

摘要：<paragraph>联邦学习 (FL) 最近已成为一种引人注目的机器学习范例，优先保护训练数据的隐私。解决“被遗忘权”等问题和打击数据中毒攻击的日益增长的需求凸显了被称为“取消学习”的技术的重要性，该技术有助于从训练过的 FL 模型中删除特定的训练数据。尽管针对集中式学习提出了许多取消学习方法，但由于两种学习范例的操作存在根本差异，这些方法通常被证明不适用于 FL。因此，FL 中的取消学习仍处于早期阶段，面临着许多挑战。FL 中许多现有的取消学习解决方案都需要昂贵的重新训练过程，这可能会给客户端带来负担。此外，这些方法主要通过实验验证，缺乏理论保证。在这项研究中，我们介绍了 Fast-FedUL，这是一种针对 FL 的定制取消学习方法，它完全消除了重新训练的需要。通过细致分析目标客户端在每一轮中对全局模型的影响，我们开发了一种算法来系统地从训练过的模型中移除目标客户端的影响。除了提供实证结果外，我们还提供了一个理论分析，描述了我们取消学习模型的上界和精确的重新训练模型（通过使用非目标客户端重新训练获得的模型）。带有后门攻击场景的实验结果表明，Fast-FedUL 有效地去除了目标客户端几乎所有的痕迹，同时保留了非目标客户端的知识（在主任务上获得了高达 98% 的高准确度）。值得注意的是，Fast-FedUL 实现了最低的时间复杂度，其速度比重新训练快 1000 倍。我们的源代码可在 \url{https://github.com/thanhtrunghuynh93/fastFedUL} 公开获得。</paragraph>

##### **Instruction Tuning with Retrieval-based Examples Ranking for Aspect-based Sentiment Analysis**
2405.18035v1 by Guangmin Zheng, Jin Wang, Liang-Chih Yu, Xuejie Zhang

Aspect-based sentiment analysis (ABSA) identifies sentiment information
related to specific aspects and provides deeper market insights to businesses
and organizations. With the emergence of large language models (LMs), recent
studies have proposed using fixed examples for instruction tuning to
reformulate ABSA as a generation task. However, the performance is sensitive to
the selection of in-context examples; several retrieval methods are based on
surface similarity and are independent of the LM generative objective. This
study proposes an instruction learning method with retrieval-based example
ranking for ABSA tasks. For each target sample, an LM was applied as a scorer
to estimate the likelihood of the output given the input and a candidate
example as the prompt, and training examples were labeled as positive or
negative by ranking the scores. An alternating training schema is proposed to
train both the retriever and LM. Instructional prompts can be constructed using
high-quality examples. The LM is used for both scoring and inference, improving
the generation efficiency without incurring additional computational costs or
training difficulties. Extensive experiments on three ABSA subtasks verified
the effectiveness of the proposed method, demonstrating its superiority over
various strong baseline models. Code and data are released at
https://anonymous.4open.science/r/IT-RER-ABSA-181F.

摘要：基於面向方面的觀點分析（ABSA）識別與特定面向相關的情緒資訊，並為企業和組織提供更深入的市場洞察。隨著大型語言模型（LM）的出現，最近的研究提出使用固定範例進行指令微調，以將 ABSA 重新表述為生成任務。然而，效能會受到語境範例選擇的影響；多種檢索方法基於表面相似性，且與 LM 生成目標無關。本研究提出了一種具有基於檢索的範例排序的指令學習方法，用於 ABSA 任務。對於每個目標樣本，將 LM 作為評分器應用，以估計在輸入和候選範例作為提示的情況下輸出的可能性，並通過對分數進行排序將訓練範例標記為正面或負面。提出了一個交替訓練架構來訓練檢索器和 LM。可以使用高品質範例構建指令提示。LM 用於評分和推論，從而提高生成效率，而不會產生額外的計算成本或訓練難度。在三個 ABSA 子任務上的廣泛實驗驗證了所提出方法的有效性，證明了其優於各種強大的基線模型。程式碼和資料發布於 https://anonymous.4open.science/r/IT-RER-ABSA-181F。

##### **Edinburgh Clinical NLP at MEDIQA-CORR 2024: Guiding Large Language Models with Hints**
2405.18028v1 by Aryo Pradipta Gema, Chaeeun Lee, Pasquale Minervini, Luke Daines, T. Ian Simpson, Beatrice Alex

The MEDIQA-CORR 2024 shared task aims to assess the ability of Large Language
Models (LLMs) to identify and correct medical errors in clinical notes. In this
study, we evaluate the capability of general LLMs, specifically GPT-3.5 and
GPT-4, to identify and correct medical errors with multiple prompting
strategies. Recognising the limitation of LLMs in generating accurate
corrections only via prompting strategies, we propose incorporating error-span
predictions from a smaller, fine-tuned model in two ways: 1) by presenting it
as a hint in the prompt and 2) by framing it as multiple-choice questions from
which the LLM can choose the best correction. We found that our proposed
prompting strategies significantly improve the LLM's ability to generate
corrections. Our best-performing solution with 8-shot + CoT + hints ranked
sixth in the shared task leaderboard. Additionally, our comprehensive analyses
show the impact of the location of the error sentence, the prompted role, and
the position of the multiple-choice option on the accuracy of the LLM. This
prompts further questions about the readiness of LLM to be implemented in
real-world clinical settings.

摘要：MEDIQA-CORR 2024 共享任務旨在評估大型語言模型 (LLM) 在臨床筆記中識別和更正醫療錯誤的能力。在此研究中，我們評估通用 LLM，特別是 GPT-3.5 和 GPT-4，識別和更正醫療錯誤的能力，並採用多種提示策略。認識到 LLM 僅通過提示策略生成準確更正的限制，我們建議以兩種方式整合來自較小的微調模型的錯誤範圍預測：1) 在提示中將其表示為提示，以及 2) 將其設定為多選題，LLM 可以從中選擇最佳更正。我們發現我們提出的提示策略顯著提高了 LLM 生成更正的能力。我們在共享任務排行榜中排名第六的最佳執行方案是 8 次嘗試 + CoT + 提示。此外，我們全面的分析顯示了錯誤句子位置、提示角色和多選項位置對 LLM 準確性的影響。這引發了進一步的問題，即 LLM 是否已準備好實施在現實世界的臨床環境中。

##### **TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models**
2405.18027v1 by Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, Gunhee Kim

While Large Language Models (LLMs) can serve as agents to simulate human
behaviors (i.e., role-playing agents), we emphasize the importance of
point-in-time role-playing. This situates characters at specific moments in the
narrative progression for three main reasons: (i) enhancing users' narrative
immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom
role-playing. To accurately represent characters at specific time points,
agents must avoid character hallucination, where they display knowledge that
contradicts their characters' identities and historical timelines. We introduce
TimeChara, a new benchmark designed to evaluate point-in-time character
hallucination in role-playing LLMs. Comprising 10,895 instances generated
through an automated pipeline, this benchmark reveals significant hallucination
issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this
challenge, we propose Narrative-Experts, a method that decomposes the reasoning
steps and utilizes narrative experts to reduce point-in-time character
hallucinations effectively. Still, our findings with TimeChara highlight the
ongoing challenges of point-in-time character hallucination, calling for
further study.

摘要：大型語言模型 (LLM) 雖然可以作為模擬人類行為的代理（即角色扮演代理），但我們強調即時角色扮演的重要性。這讓角色處於敘事進程中的特定時刻，原因有三個：(i) 增強使用者的敘事沉浸感，(ii) 避免劇透，以及 (iii) 促進粉絲角色扮演的參與。為了在特定時間點準確呈現角色，代理必須避免角色幻覺，即展現與角色身份和歷史時間線相矛盾的知識。我們引入了 TimeChara，一個新的基準，旨在評估角色扮演 LLM 中的即時角色幻覺。這個基準包含透過自動化管道產生的 10,895 個實例，揭示了目前最先進的 LLM（例如 GPT-4o）中嚴重的幻覺問題。為了應對這個挑戰，我們提出了敘事專家，一種分解推理步驟並利用敘事專家來有效減少即時角色幻覺的方法。儘管如此，我們對 TimeChara 的發現突顯了即時角色幻覺的持續挑戰，需要進一步研究。

##### **Unveiling the Power of Diffusion Features For Personalized Segmentation and Retrieval**
2405.18025v1 by Dvir Samuel, Rami Ben-Ari, Matan Levy, Nir Darshan, Gal Chechik

Personalized retrieval and segmentation aim to locate specific instances
within a dataset based on an input image and a short description of the
reference instance. While supervised methods are effective, they require
extensive labeled data for training. Recently, self-supervised foundation
models have been introduced to these tasks showing comparable results to
supervised methods. However, a significant flaw in these models is evident:
they struggle to locate a desired instance when other instances within the same
class are presented. In this paper, we explore text-to-image diffusion models
for these tasks. Specifically, we propose a novel approach called PDM for
Personalized Features Diffusion Matching, that leverages intermediate features
of pre-trained text-to-image models for personalization tasks without any
additional training. PDM demonstrates superior performance on popular retrieval
and segmentation benchmarks, outperforming even supervised methods. We also
highlight notable shortcomings in current instance and segmentation datasets
and propose new benchmarks for these tasks.

摘要：個人化檢索和分割旨在根據輸入影像和參考實例的簡短描述，在資料集中找到特定實例。雖然監督式方法很有效，但它們需要大量的標籤資料進行訓練。最近，自監督基礎模型已被引入這些任務，顯示出與監督式方法相當的結果。然而，這些模型有一個明顯的重大缺陷：當呈現同一類別中的其他實例時，它們難以找到所需的實例。在本文中，我們探索了這些任務的文字到影像擴散模型。具體來說，我們提出了一種名為 PDM 的新方法，用於個性化特徵擴散匹配，它利用預訓練文字到影像模型的中間特徵，用於個性化任務，而無需任何額外訓練。PDM 在流行的檢索和分割基準上展示了優異的性能，甚至優於監督式方法。我們還強調了當前實例和分割資料集中的顯著缺點，並為這些任務提出了新的基準。

##### **MultiADE: A Multi-domain Benchmark for Adverse Drug Event Extraction**
2405.18015v1 by Xiang Dai, Sarvnaz Karimi, Abeed Sarker, Ben Hachey, Cecile Paris

Objective. Active adverse event surveillance monitors Adverse Drug Events
(ADE) from different data sources, such as electronic health records, medical
literature, social media and search engine logs. Over years, many datasets are
created, and shared tasks are organised to facilitate active adverse event
surveillance. However, most-if not all-datasets or shared tasks focus on
extracting ADEs from a particular type of text. Domain generalisation-the
ability of a machine learning model to perform well on new, unseen domains
(text types)-is under-explored. Given the rapid advancements in natural
language processing, one unanswered question is how far we are from having a
single ADE extraction model that are effective on various types of text, such
as scientific literature and social media posts}. Methods. We contribute to
answering this question by building a multi-domain benchmark for adverse drug
event extraction, which we named MultiADE. The new benchmark comprises several
existing datasets sampled from different text types and our newly created
dataset-CADECv2, which is an extension of CADEC (Karimi, et al., 2015),
covering online posts regarding more diverse drugs than CADEC. Our new dataset
is carefully annotated by human annotators following detailed annotation
guidelines. Conclusion. Our benchmark results show that the generalisation of
the trained models is far from perfect, making it infeasible to be deployed to
process different types of text. In addition, although intermediate transfer
learning is a promising approach to utilising existing resources, further
investigation is needed on methods of domain adaptation, particularly
cost-effective methods to select useful training instances.

摘要：<paragraph>目的。主动不良事件监测从电子健康记录、医学文献、社交媒体和搜索引擎日志等不同数据源监测不良药物事件 (ADE)。多年来，创建了许多数据集，并组织了共享任务以促进主动不良事件监测。然而，大多数（如果不是全部）数据集或共享任务都专注于从特定类型的文本中提取 ADE。领域泛化——机器学习模型在新领域（文本类型）中表现良好的能力——尚未得到充分探索。鉴于自然语言处理的快速发展，一个尚未回答的问题是我们离拥有一个对各种类型的文本（如科学文献和社交媒体帖子）有效的单一 ADE 提取模型还有多远。方法。我们通过构建一个多领域不良药物事件提取基准来回答这个问题，我们将其命名为 MultiADE。新的基准包含几个从不同文本类型中抽取的现有数据集和我们新创建的数据集-CADECv2，它是 CADEC（Karimi 等人，2015 年）的扩展，涵盖了比 CADEC 更广泛的药物的在线帖子。我们的新数据集由人类注释者按照详细的注释指南仔细注释。结论。我们的基准结果表明，训练模型的泛化远不完美，使其无法部署到处理不同类型的文本中。此外，尽管中间迁移学习是利用现有资源的一种有前途的方法，但仍需要进一步研究领域自适应的方法，特别是选择有用训练实例的经济有效的方法。</paragraph>

##### **Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model**
2405.18014v1 by Wenbing Li, Hang Zhou, Zikai Song, Wei Yang

The essence of multi-modal fusion lies in exploiting the complementary
information inherent in diverse modalities. However, prevalent fusion methods
rely on traditional neural architectures and are inadequately equipped to
capture the dynamics of interactions across modalities, particularly in
presence of complex intra- and inter-modality correlations. Recent advancements
in State Space Models (SSMs), notably exemplified by the Mamba model, have
emerged as promising contenders. Particularly, its state evolving process
implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an
appealing direction. However, fusing multiple modalities is challenging for
SSMs due to its hardware-aware parallelism designs. To this end, this paper
proposes the Coupled SSM model, for coupling state chains of multiple
modalities while maintaining independence of intra-modality state processes.
Specifically, in our coupled scheme, we devise an inter-modal hidden states
transition scheme, in which the current state is dependent on the states of its
own chain and that of the neighbouring chains at the previous time-step. To
fully comply with the hardware-aware parallelism, we devise an expedite coupled
state transition scheme and derive its corresponding global convolution kernel
for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through
multi-domain input verify the effectiveness of our model compared to current
state-of-the-art methods, improved F1-Score by 0.4\%, 0.9\%, and 2.3\% on the
three datasets respectively, 49\% faster inference and 83.7\% GPU memory save.
The results demonstrate that Coupled Mamba model is capable of enhanced
multi-modal fusion.

摘要：多模态融合的本质在于利用各种模态中固有的互补信息。然而，流行的融合方法依赖于传统的网络架构，并且在捕捉模态间交互的动态方面装备不足，尤其是在存在复杂的模态内和模态间相关性的情况下。状态空间模型 (SSM) 的最新进展，特别是以 Mamba 模型为代表，已成为有希望的竞争者。特别是，其状态演化过程暗示了更强的模态融合范式，使得 SSM 上的多模态融合成为一个有吸引力的方向。然而，由于其硬件感知并行设计，对 SSM 来说融合多个模态具有挑战性。为此，本文提出了耦合 SSM 模型，用于耦合多个模态的状态链，同时保持模态内状态过程的独立性。具体来说，在我们的耦合方案中，我们设计了一个模态间隐状态转换方案，其中当前状态取决于其自身链的状态以及前一时间步的相邻链的状态。为了完全符合硬件感知并行性，我们设计了一个快速的耦合状态转换方案，并推导出其对应的全局卷积核以实现并行性。通过多域输入在 CMU-MOSEI、CH-SIMS、CH-SIMSV2 上进行的广泛实验验证了我们模型与当前最先进方法相比的有效性，分别在三个数据集上将 F1 分数提高了 0.4%、0.9% 和 2.3%，推理速度提高了 49%，GPU 内存节省了 83.7%。结果表明，耦合 Mamba 模型能够增强多模态融合。

##### **Exploring Context Window of Large Language Models via Decomposed Positional Vectors**
2405.18009v1 by Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingbing Wang, Zhen Tian, Weipeng Chen, Ji-Rong Wen

Transformer-based large language models (LLMs) typically have a limited
context window, resulting in significant performance degradation when
processing text beyond the length of the context window. Extensive studies have
been proposed to extend the context window and achieve length extrapolation of
LLMs, but there is still a lack of in-depth interpretation of these approaches.
In this study, we explore the positional information within and beyond the
context window for deciphering the underlying mechanism of LLMs. By using a
mean-based decomposition method, we disentangle positional vectors from hidden
states of LLMs and analyze their formation and effect on attention.
Furthermore, when texts exceed the context window, we analyze the change of
positional vectors in two settings, i.e., direct extrapolation and context
window extension. Based on our findings, we design two training-free context
window extension methods, positional vector replacement and attention window
extension. Experimental results show that our methods can effectively extend
the context window length.

摘要：<paragraph>基於 Transformer 的大型語言模型 (LLM) 通常具有有限的上下文視窗，導致在處理超過上下文視窗長度的文字時，效能會顯著下降。已經提出許多研究來延伸上下文視窗並達成 LLM 的長度外推，但對於這些方法仍缺乏深入的詮釋。在這個研究中，我們探討了上下文視窗內外的位置資訊，以解碼 LLM 的底層機制。透過使用基於平均值的分解方法，我們將位置向量從 LLM 的隱藏狀態中解開，並分析它們的形成和對注意力的影響。此外，當文字超過上下文視窗時，我們分析了位置向量在兩種設定下的變化，即直接外推和上下文視窗延伸。根據我們的發現，我們設計了兩種無需訓練的上下文視窗延伸方法，位置向量替換和注意力視窗延伸。實驗結果顯示，我們的這些方法可以有效地延伸上下文視窗長度。</paragraph>

##### **MAVIN: Multi-Action Video Generation with Diffusion Models via Transition Video Infilling**
2405.18003v1 by Bowen Zhang, Xiaofei Xie, Haotian Lu, Na Ma, Tianlin Li, Qing Guo

Diffusion-based video generation has achieved significant progress, yet
generating multiple actions that occur sequentially remains a formidable task.
Directly generating a video with sequential actions can be extremely
challenging due to the scarcity of fine-grained action annotations and the
difficulty in establishing temporal semantic correspondences and maintaining
long-term consistency. To tackle this, we propose an intuitive and
straightforward solution: splicing multiple single-action video segments
sequentially. The core challenge lies in generating smooth and natural
transitions between these segments given the inherent complexity and
variability of action transitions. We introduce MAVIN (Multi-Action Video
INfilling model), designed to generate transition videos that seamlessly
connect two given videos, forming a cohesive integrated sequence. MAVIN
incorporates several innovative techniques to address challenges in the
transition video infilling task. Firstly, a consecutive noising strategy
coupled with variable-length sampling is employed to handle large infilling
gaps and varied generation lengths. Secondly, boundary frame guidance (BFG) is
proposed to address the lack of semantic guidance during transition generation.
Lastly, a Gaussian filter mixer (GFM) dynamically manages noise initialization
during inference, mitigating train-test discrepancy while preserving generation
flexibility. Additionally, we introduce a new metric, CLIP-RS (CLIP Relative
Smoothness), to evaluate temporal coherence and smoothness, complementing
traditional quality-based metrics. Experimental results on horse and tiger
scenarios demonstrate MAVIN's superior performance in generating smooth and
coherent video transitions compared to existing methods.

摘要：基於擴散的影片生成已取得顯著進展，但生成多個按順序發生的動作仍然是一項艱鉅的任務。由於缺乏細粒度的動作註解，且難以建立時間語義對應並維持長期一致性，因此直接生成具有順序動作的影片可能極具挑戰性。為了解決這個問題，我們提出了一個直觀且簡單的解決方案：按順序拼接多個單動作影片片段。核心挑戰在於生成這些片段之間的平滑且自然的過渡，因為動作過渡具有內在的複雜性和可變性。我們引入了 MAVIN（多動作影片填充模型），旨在生成過渡影片，將兩個給定的影片無縫連接，形成一個有凝聚力的整合序列。MAVIN 結合了多種創新技術來應對過渡影片填充任務中的挑戰。首先，採用連續噪聲策略與可變長度採樣，以處理大型填充間隙和不同的生成長度。其次，提出了邊界幀引導 (BFG) 來解決過渡生成期間缺乏語義引導的問題。最後，高斯濾波器混合器 (GFM) 在推理期間動態管理噪聲初始化，在保留生成靈活性的同時減輕訓練測試差異。此外，我們引入了新的指標 CLIP-RS（CLIP 相對平滑度），以評估時間相干性和平滑度，作為傳統基於品質的指標的補充。在馬和老虎場景的實驗結果證明，與現有方法相比，MAVIN 在生成平滑且連貫的影片過渡方面具有優異的性能。

##### **Source Echo Chamber: Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop**
2405.17998v1 by Yuqi Zhou, Sunhao Dai, Liang Pang, Gang Wang, Zhenhua Dong, Jun Xu, Ji-Rong Wen

Recently, researchers have uncovered that neural retrieval models prefer
AI-generated content (AIGC), called source bias. Compared to active search
behavior, recommendation represents another important means of information
acquisition, where users are more prone to source bias. Furthermore, delving
into the recommendation scenario, as AIGC becomes integrated within the
feedback loop involving users, data, and the recommender system, it
progressively contaminates the candidate items, the user interaction history,
and ultimately, the data used to train the recommendation models. How and to
what extent the source bias affects the neural recommendation models within
feedback loop remains unknown. In this study, we extend the investigation of
source bias into the realm of recommender systems, specifically examining its
impact across different phases of the feedback loop. We conceptualize the
progression of AIGC integration into the recommendation content ecosystem in
three distinct phases-HGC dominate, HGC-AIGC coexist, and AIGC dominance-each
representing past, present, and future states, respectively. Through extensive
experiments across three datasets from diverse domains, we demonstrate the
prevalence of source bias and reveal a potential digital echo chamber with
source bias amplification throughout the feedback loop. This trend risks
creating a recommender ecosystem with limited information source, such as AIGC,
being disproportionately recommended. To counteract this bias and prevent its
escalation in the feedback loop, we introduce a black-box debiasing method that
maintains model impartiality towards both HGC and AIGC. Our experimental
results validate the effectiveness of the proposed debiasing method, confirming
its potential to disrupt the feedback loop.

摘要：<paragraph>最近，研究人員發現神經檢索模型偏好 AI 生成的內容 (AIGC)，稱為來源偏差。與主動搜尋行為相比，推薦代表了另一種重要的資訊獲取方式，使用者更容易出現來源偏差。此外，深入探討推薦情境，由於 AIGC 已整合到涉及使用者、資料和推薦系統的回饋迴路中，它會逐漸污染候選項目、使用者互動歷程，以及最終用於訓練推薦模型的資料。來源偏差如何以及在何種程度上影響回饋迴路中的神經推薦模型，目前仍未知。在這項研究中，我們將來源偏差的調查擴展到推薦系統領域，特別檢驗其對回饋迴路不同階段的影響。我們將 AIGC 整合到推薦內容生態系統的進程概念化為三個不同的階段：HGC 主導、HGC-AIGC 共存和 AIGC 主導，每個階段分別代表過去、現在和未來。透過對來自不同領域的三個資料集進行廣泛的實驗，我們證明了來源偏差的普遍性，並揭示了一個潛在的數位迴音室，其中來源偏差會在整個回饋迴路中放大。這種趨勢有風險會創造一個推薦生態系統，其中資訊來源有限，例如 AIGC，會被不成比例地推薦。為了對抗這種偏差並防止其在回饋迴路中升級，我們引入了一種黑盒去偏方法，該方法維持模型對 HGC 和 AIGC 的公正性。我們的實驗結果驗證了所提出的去偏方法的有效性，證實了其破壞回饋迴路潛力的可能性。</paragraph>

##### **DMT-JEPA: Discriminative Masked Targets for Joint-Embedding Predictive Architecture**
2405.17995v1 by Shentong Mo, Sukmin Yun

The joint-embedding predictive architecture (JEPA) recently has shown
impressive results in extracting visual representations from unlabeled imagery
under a masking strategy. However, we reveal its disadvantages, notably its
insufficient understanding of local semantics. This deficiency originates from
masked modeling in the embedding space, resulting in a reduction of
discriminative power and can even lead to the neglect of critical local
semantics. To bridge this gap, we introduce DMT-JEPA, a novel masked modeling
objective rooted in JEPA, specifically designed to generate discriminative
latent targets from neighboring information. Our key idea is simple: we
consider a set of semantically similar neighboring patches as a target of a
masked patch. To be specific, the proposed DMT-JEPA (a) computes feature
similarities between each masked patch and its corresponding neighboring
patches to select patches having semantically meaningful relations, and (b)
employs lightweight cross-attention heads to aggregate features of neighboring
patches as the masked targets. Consequently, DMT-JEPA demonstrates strong
discriminative power, offering benefits across a diverse spectrum of downstream
tasks. Through extensive experiments, we demonstrate our effectiveness across
various visual benchmarks, including ImageNet-1K image classification, ADE20K
semantic segmentation, and COCO object detection tasks. Code is available at:
\url{https://github.com/DMTJEPA/DMTJEPA}.

摘要：聯合嵌入預測架構 (JEPA) 近期在遮罩策略下從未標註的影像中萃取視覺表徵方面展現令人印象深刻的成果。然而，我們揭露了它的缺點，特別是它對局部語意的理解不足。這個缺陷源自於嵌入空間中的遮罩模型，導致判別力的降低，甚至可能忽略關鍵的局部語意。為了彌補這個差距，我們引入了 DMT-JEPA，一種源自於 JEPA 的新型遮罩模型目標，專門設計用於從鄰近資訊中產生判別性的潛在目標。我們的關鍵想法很簡單：我們將一組語意相似的鄰近區塊視為遮罩區塊的目標。具體來說，所提出的 DMT-JEPA (a) 計算每個遮罩區塊與其對應鄰近區塊之間的特徵相似性，以選擇具有語意關聯的區塊，以及 (b) 使用輕量級交叉注意力頭部將鄰近區塊的特徵彙總為遮罩目標。因此，DMT-JEPA 展現強大的判別力，在各種下游任務中提供好處。透過廣泛的實驗，我們證明了我們在各種視覺基準上的有效性，包括 ImageNet-1K 影像分類、ADE20K 語意分割和 COCO 物件偵測任務。程式碼可在以下網址取得：\url{https://github.com/DMTJEPA/DMTJEPA}。

##### **fMRI predictors based on language models of increasing complexity recover brain left lateralization**
2405.17992v1 by Laurent Bonnasse-Gahot, Christophe Pallier

Over the past decade, studies of naturalistic language processing where
participants are scanned while listening to continuous text have flourished.
Using word embeddings at first, then large language models, researchers have
created encoding models to analyze the brain signals. Presenting these models
with the same text as the participants allows to identify brain areas where
there is a significant correlation between the functional magnetic resonance
imaging (fMRI) time series and the ones predicted by the models' artificial
neurons. One intriguing finding from these studies is that they have revealed
highly symmetric bilateral activation patterns, somewhat at odds with the
well-known left lateralization of language processing. Here, we report analyses
of an fMRI dataset where we manipulate the complexity of large language models,
testing 28 pretrained models from 8 different families, ranging from 124M to
14.2B parameters. First, we observe that the performance of models in
predicting brain responses follows a scaling law, where the fit with brain
activity increases linearly with the logarithm of the number of parameters of
the model (and its performance on natural language processing tasks). Second,
we show that a left-right asymmetry gradually appears as model size increases,
and that the difference in left-right brain correlations also follows a scaling
law. Whereas the smallest models show no asymmetry, larger models fit better
and better left hemispheric activations than right hemispheric ones. This
finding reconciles computational analyses of brain activity using large
language models with the classic observation from aphasic patients showing left
hemisphere dominance for language.

摘要：<paragraph>在過去十年中，研究人員在參與者聆聽連續文本時進行掃描的自然語言處理研究蓬勃發展。
研究人員首先使用詞嵌入，然後使用大型語言模型，創建編碼模型來分析腦部訊號。向這些模型呈現與參與者相同的文本，可以識別腦部區域，在該區域中，功能性磁共振造影 (fMRI) 時間序列與模型的人工神經元所預測的序列之間存在顯著相關性。這些研究中一個有趣的發現是，它們揭示了高度對稱的雙邊激活模式，這與語言處理中眾所周知的左半球側化現象有些矛盾。在這裡，我們報告了 fMRI 資料集的分析，其中我們操縱大型語言模型的複雜性，測試了來自 8 個不同家族的 28 個預訓練模型，範圍從 124M 到 14.2B 個參數。首先，我們觀察到模型在預測腦部反應方面的表現遵循一個比例定律，其中與腦部活動的吻合度隨著模型參數數量的對數線性增加（以及它在自然語言處理任務中的表現）。其次，我們表明，隨著模型大小的增加，左右不對稱性逐漸出現，並且左右腦相關性的差異也遵循一個比例定律。儘管最小的模型沒有顯示出不對稱性，但較大的模型比右半球激活更適合和更好的左半球激活。這一發現調和了使用大型語言模型對腦部活動的計算分析與失語症患者的經典觀察，後者顯示左半球在語言方面佔優勢。</paragraph>

##### **VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections**
2405.17991v1 by Roy Miles, Pradyumna Reddy, Ismail Elezi, Jiankang Deng

Large language models (LLMs) have recently emerged as powerful tools for
tackling many language-processing tasks. Despite their success, training and
fine-tuning these models is still far too computationally and memory intensive.
In this paper, we identify and characterise the important components needed for
effective model convergence using gradient descent. In doing so we find that
the intermediate activations used to implement backpropagation can be
excessively compressed without incurring any degradation in performance. This
result leads us to a cheap and memory-efficient algorithm for both fine-tuning
and pre-training LLMs. The proposed algorithm simply divides the tokens up into
smaller sub-tokens before projecting them onto a fixed 1-dimensional subspace
during the forward pass. These features are then coarsely reconstructed during
the backward pass to implement the update rules. We confirm the effectiveness
of our algorithm as being complimentary to many state-of-the-art PEFT methods
on the VTAB-1k fine-tuning benchmark. Furthermore, we outperform QLoRA for
fine-tuning LLaMA and show competitive performance against other
memory-efficient pre-training methods on the large-scale C4 dataset.

摘要：大型語言模型 (LLM) 最近已成為處理許多語言處理任務的強大工具。儘管它們很成功，但訓練和微調這些模型在計算和記憶體上仍過於密集。在本文中，我們識別並描述了使用梯度下降進行有效模型收斂所需的重要組成部分。在這樣做的過程中，我們發現用於實作反向傳播的中間活化可以過度壓縮，而不會造成效能下降。這個結果讓我們找到一個便宜且記憶體效率高的演算法，可用於微調和 LLM 預訓練。所提出的演算法只是在正向傳遞期間將符號分成較小的子符號，然後將它們投影到固定的 1 維子空間。然後在反向傳遞期間粗略重建這些特徵以實作更新規則。我們確認我們的演算法的有效性，因為它可以補充 VTAB-1k 微調基準上的許多最新 PEFT 方法。此外，我們在微調 LLaMA 時優於 QLoRA，並在大型 C4 資料集上展現出與其他記憶體效率高的預訓練方法競爭的效能。

##### **Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering**
2405.17980v1 by Anirudh Phukan, Shwetha Somasundaram, Apoorv Saxena, Koustava Goswami, Balaji Vasan Srinivasan

With the enhancement in the field of generative artificial intelligence (AI),
contextual question answering has become extremely relevant. Attributing model
generations to the input source document is essential to ensure trustworthiness
and reliability. We observe that when large language models (LLMs) are used for
contextual question answering, the output answer often consists of text copied
verbatim from the input prompt which is linked together with "glue text"
generated by the LLM. Motivated by this, we propose that LLMs have an inherent
awareness from where the text was copied, likely captured in the hidden states
of the LLM. We introduce a novel method for attribution in contextual question
answering, leveraging the hidden state representations of LLMs. Our approach
bypasses the need for extensive model retraining and retrieval model overhead,
offering granular attributions and preserving the quality of generated answers.
Our experimental results demonstrate that our method performs on par or better
than GPT-4 at identifying verbatim copied segments in LLM generations and in
attributing these segments to their source. Importantly, our method shows
robust performance across various LLM architectures, highlighting its broad
applicability. Additionally, we present Verifiability-granular, an attribution
dataset which has token level annotations for LLM generations in the contextual
question answering setup.

摘要：<paragraph>隨著生成式人工智慧 (AI) 領域的進步，
情境式問題回答變得極為相關。將模型生成歸因於輸入來源文件對於確保可信度
和可靠性至關重要。我們觀察到，當大型語言模型 (LLM) 用於
情境式問題回答時，輸出答案通常包含從輸入提示中逐字複製的文字，並與 LLM 生成的「黏合文字」連結在一起。有鑑於此，我們提出 LLM 具有從何處複製文字的內在意識，這可能存在於 LLM 的隱藏狀態中。我們引入了一種新的方法，用於情境式問題回答中的歸因，利用 LLM 的隱藏狀態表示。我們的做法避免了廣泛模型再訓練和檢索模型開銷的需求，提供了細緻的歸因並保留了生成答案的品質。
我們的實驗結果表明，我們的方法在識別 LLM 生成中的逐字複製片段以及將這些片段歸因於其來源方面，表現與 GPT-4 相當或更好。重要的是，我們的模型在各種 LLM 架構中都展現了穩健的效能，突顯了其廣泛的適用性。此外，我們提出了 Verifiability-granular，這是一個歸因資料集，其中包含 LLM 生成在情境式問題回答設定中的代幣級註解。</paragraph>

##### **FASTopic: A Fast, Adaptive, Stable, and Transferable Topic Modeling Paradigm**
2405.17978v1 by Xiaobao Wu, Thong Nguyen, Delvin Ce Zhang, William Yang Wang, Anh Tuan Luu

Topic models have been evolving rapidly over the years, from conventional to
recent neural models. However, existing topic models generally struggle with
either effectiveness, efficiency, or stability, highly impeding their practical
applications. In this paper, we propose FASTopic, a fast, adaptive, stable, and
transferable topic model. FASTopic follows a new paradigm: Dual
Semantic-relation Reconstruction (DSR). Instead of previous conventional,
neural VAE-based or clustering-based methods, DSR discovers latent topics by
reconstruction through modeling the semantic relations among document, topic,
and word embeddings. This brings about a neat and efficient topic modeling
framework. We further propose a novel Embedding Transport Plan (ETP) method.
Rather than early straightforward approaches, ETP explicitly regularizes the
semantic relations as optimal transport plans. This addresses the relation bias
issue and thus leads to effective topic modeling. Extensive experiments on
benchmark datasets demonstrate that our FASTopic shows superior effectiveness,
efficiency, adaptivity, stability, and transferability, compared to
state-of-the-art baselines across various scenarios. Our code is available at
https://github.com/bobxwu/FASTopic .

摘要：主題模型近年來快速演進，從傳統模型到最近的神經網路模型。然而，現有的主題模型通常在效能、效率或穩定性方面有所不足，嚴重阻礙了它們的實際應用。在本文中，我們提出 FASTopic，一個快速、自適應、穩定且可轉移的主題模型。FASTopic 遵循一個新的範例：雙語義關係重建 (DSR)。DSR 透過重建，透過文件、主題和字詞嵌入之間的語義關係建模，來發現潛在主題，而不是先前的傳統、基於神經網路 VAE 或基於群集的方法。這帶來了簡潔且有效率的主題建模架構。我們進一步提出了一種新的嵌入傳輸計畫 (ETP) 方法。ETP 明確地將語義關係正規化為最佳傳輸計畫，而不是早期直接的方法。這解決了關係偏差問題，因此導致有效的主題建模。在基準資料集上的廣泛實驗證明，與各種情境中的最新基準相比，我們的 FASTopic 顯示出優異的效能、效率、自適應性、穩定性和可轉移性。我們的程式碼可在 https://github.com/bobxwu/FASTopic 取得。

##### **Aligning to Thousands of Preferences via System Message Generalization**
2405.17977v1 by Seongyun Lee, Sue Hyun Park, Seungone Kim, Minjoon Seo

Although humans inherently have diverse values, current large language model
(LLM) alignment methods often assume that aligning LLMs with the general
public's preferences is optimal. A major challenge in adopting a more
individualized approach to LLM alignment is its lack of scalability, as it
involves repeatedly acquiring preference data and training new reward models
and LLMs for each individual's preferences. To address these challenges, we
propose a new paradigm where users specify what they value most within the
system message, steering the LLM's generation behavior to better align with the
user's intentions. However, a naive application of such an approach is
non-trivial since LLMs are typically trained on a uniform system message (e.g.,
"You are a helpful assistant") which limits their ability to generalize to
diverse, unseen system messages. To improve this generalization, we create the
Multifaceted Collection, a preference dataset with 192k combinations of values
beyond generic helpfulness and harmlessness, spanning 65k user instructions.
Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts
from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)
by adding various unseen system messages that reflect user preferences. Janus
achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct
v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks
focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto
v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%
margin, underscoring that training with a vast array of system messages could
also enhance alignment to the general public's preference as well. Our code,
dataset, benchmark, and models are available at
https://github.com/kaistAI/Janus.

摘要：儘管人類天生具有多元價值觀，但現有的巨量語言模型 (LLM) 校準方法通常假設將 LLM 與一般大眾的喜好校準是最佳的。採用更個人化的 LLM 校準方法的主要挑戰在於其缺乏可擴充性，因為它涉及重複取得喜好資料並針對每個個人的喜好訓練新的獎勵模型和 LLM。為了應對這些挑戰，我們提出了一種新的範例，使用者可以在系統訊息中指定他們最重視的內容，引導 LLM 的生成行為以更好地符合使用者的意圖。然而，這種方法的單純應用並非易事，因為 LLM 通常是在統一的系統訊息（例如「您是一位有用的助理」）上訓練，這限制了它們對多樣且未見過的系統訊息進行概括的能力。為了改善這種概括，我們建立了多面向集合，一個包含 192k 種價值觀組合的首選資料集，超越了通用的有益性和無害性，涵蓋了 65k 條使用者說明。使用這個資料集，我們訓練了一個名為 Janus 的 7B LLM，並在 5 個基準（AlpacaEval 2.0、FLASK、Koala、MT-Bench 和 Self-Instruct）中的 921 個提示上對其進行測試，方法是加入各種反映使用者喜好的未見過系統訊息。Janus 分別以 75.2%、72.4% 和 66.4% 的平手加獲勝率擊敗了 Mistral 7B Instruct v0.2、GPT-3.5 Turbo 和 GPT-4。令人意外的是，在三個著重於回應有益性的基準（AlpacaEval 2.0、MT-Bench、Arena Hard Auto v0.1）上，Janus 也以 +4.0%、+0.1%、+3.0% 的幅度勝過 LLaMA 3 8B Instruct，這強調了使用大量系統訊息進行訓練也可以增強與一般大眾喜好的校準。我們的程式碼、資料集、基準和模型可在 https://github.com/kaistAI/Janus 取得。

##### **Yuan 2.0-M32: Mixture of Experts with Attention Router**
2405.17976v1 by Shaohua Wu, Jiangang Luo, Xi Chen, Lingjun Li, Xudong Zhao, Tong Yu, Chao Wang, Yue Wang, Fei Wang, Weixu Qiao, Houbo He, Zeru Zhang, Zeyu Sun, Junxiong Mao, Chong Shen

Yuan 2.0-M32, with a similar base architecture as Yuan-2.0 2B, uses a
mixture-of-experts architecture with 32 experts of which 2 experts are active.
A new router network, Attention Router, is proposed and adopted for a more
efficient selection of experts, which boosts the accuracy of 3.8% compared to
the model with classical router network. Yuan 2.0-M32 is trained with 2000B
tokens from scratch, and the training computation consumption is only 9.25% of
a dense model at the same parameter scale. Yuan 2.0-M32 demonstrates
competitive capability on coding, math, and various domains of expertise, with
only 3.7B active parameters of 40B in total, and 7.4 GFlops forward computation
per token, both of which are only 1/19 of Llama3-70B. Yuan 2.0-M32 surpass
Llama3-70B on MATH and ARC-Challenge benchmark, with accuracy of 55.89 and 95.8
respectively. The models and source codes of Yuan 2.0-M32 are released at
Github.

摘要：Yuan 2.0-M32，其基本架構與 Yuan-2.0 2B 相似，使用混合專家架構，其中有 32 位專家，其中 2 位專家處於活躍狀態。提出並採用了一個新的路由器網路，即 Attention Router，以更有效地選擇專家，與採用經典路由器網路的模型相比，準確度提升了 3.8%。Yuan 2.0-M32 使用 2000B 的代幣從頭開始訓練，並且訓練計算消耗僅為同參數規模下稠密模型的 9.25%。Yuan 2.0-M32 在編碼、數學和各種專業領域展示了競爭力，總共只有 40B 的 3.7B 個活躍參數，並且每個代幣的正向計算為 7.4 GFlops，這兩者都只有 Llama3-70B 的 1/19。Yuan 2.0-M32 在 MATH 和 ARC-Challenge 基準測試中超越了 Llama3-70B，準確度分別為 55.89 和 95.8。Yuan 2.0-M32 的模型和源代碼已在 Github 上發布。

##### **Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations**
2405.17974v1 by Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto

Enhancing user engagement through personalization in conversational agents
has gained significance, especially with the advent of large language models
that generate fluent responses. Personalized dialogue generation, however, is
multifaceted and varies in its definition -- ranging from instilling a persona
in the agent to capturing users' explicit and implicit cues. This paper seeks
to systemically survey the recent landscape of personalized dialogue
generation, including the datasets employed, methodologies developed, and
evaluation metrics applied. Covering 22 datasets, we highlight benchmark
datasets and newer ones enriched with additional features. We further analyze
17 seminal works from top conferences between 2021-2023 and identify five
distinct types of problems. We also shed light on recent progress by LLMs in
personalized dialogue generation. Our evaluation section offers a comprehensive
summary of assessment facets and metrics utilized in these works. In
conclusion, we discuss prevailing challenges and envision prospect directions
for future research in personalized dialogue generation.

摘要：透過對話式代理中的個人化來提升使用者參與度已變得重要，特別是隨著產生流利回應的大語言模型的出現。然而，個人化對話生成是多方面的，並且在定義上有所不同——從在代理中灌輸一個角色到捕捉使用者的明示和暗示線索。本文旨在系統性地探討個人化對話生成的最新概況，包括所採用的資料集、開發的方法論和應用的評估指標。涵蓋 22 個資料集，我們重點介紹基準資料集和新增附加功能的較新資料集。我們進一步分析了 2021-2023 年間頂尖會議中的 17 篇開創性著作，並找出五種類型的問題。我們也說明了 LLM 在個人化對話生成中的最新進展。我們的評估部分全面總結了這些著作中使用的評估面向和指標。最後，我們討論了普遍存在的挑戰，並預想個人化對話生成未來研究的前景方向。

##### **Knowledge Circuits in Pretrained Transformers**
2405.17969v1 by Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen

The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, has allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuit holds
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.

摘要：現代大型語言模型的卓越能力源於其參數中編碼的龐大知識庫，讓它們能夠感知世界並參與推理。這些模型如何儲存知識的內部運作方式一直是研究人員高度關注和研究的主題。迄今為止，大多數研究都集中在這些模型中的孤立元件，例如多層感知器和注意力頭。在本文中，我們深入探討語言模型的計算圖，以揭示表達特定知識中至關重要的知識電路。使用 GPT2 和 TinyLLAMA 進行的實驗讓我們能夠觀察某些資訊頭、關係頭和多層感知器如何在模型中協同編碼知識。此外，我們評估了當前知識編輯技術對這些知識電路的影響，對這些編輯方法的功能和限制提供了更深入的見解。最後，我們利用知識電路分析和詮釋語言模型行為，例如幻覺和情境學習。我們相信知識電路有潛力促進我們對 Transformer 的理解，並指導知識編輯的改進設計。代碼和資料可在 https://github.com/zjunlp/KnowledgeCircuits 中取得。

##### **Transformer and Hybrid Deep Learning Based Models for Machine-Generated Text Detection**
2405.17964v1 by Teodor-George Marchitan, Claudiu Creanga, Liviu P. Dinu

This paper describes the approach of the UniBuc - NLP team in tackling the
SemEval 2024 Task 8: Multigenerator, Multidomain, and Multilingual Black-Box
Machine-Generated Text Detection. We explored transformer-based and hybrid deep
learning architectures. For subtask B, our transformer-based model achieved a
strong \textbf{second-place} out of $77$ teams with an accuracy of
\textbf{86.95\%}, demonstrating the architecture's suitability for this task.
However, our models showed overfitting in subtask A which could potentially be
fixed with less fine-tunning and increasing maximum sequence length. For
subtask C (token-level classification), our hybrid model overfit during
training, hindering its ability to detect transitions between human and
machine-generated text.

摘要：本文描述了 UniBuc - NLP 團隊在解決 SemEval 2024 任務 8：多產生器、多領域和多語言黑盒機器產生的文字偵測中的方法。我們探索了基於轉換器和混合深度學習的架構。對於子任務 B，我們的基於轉換器的模型在 77 個團隊中獲得了強勁的**第二名**，準確率為**86.95%**，證明了該架構適合此任務。然而，我們的模型在子任務 A 中表現出過度擬合，這有可能透過減少微調和增加最大序列長度來修正。對於子任務 C（令牌級別分類），我們的混合模型在訓練期間過度擬合，阻礙了它偵測人類和機器產生的文字之間轉換的能力。

##### **Attention-based sequential recommendation system using multimodal data**
2405.17959v1 by Hyungtaik Oh, Wonkeun Jo, Dongil Kim

Sequential recommendation systems that model dynamic preferences based on a
use's past behavior are crucial to e-commerce. Recent studies on these systems
have considered various types of information such as images and texts. However,
multimodal data have not yet been utilized directly to recommend products to
users. In this study, we propose an attention-based sequential recommendation
method that employs multimodal data of items such as images, texts, and
categories. First, we extract image and text features from pre-trained VGG and
BERT and convert categories into multi-labeled forms. Subsequently, attention
operations are performed independent of the item sequence and multimodal
representations. Finally, the individual attention information is integrated
through an attention fusion function. In addition, we apply multitask learning
loss for each modality to improve the generalization performance. The
experimental results obtained from the Amazon datasets show that the proposed
method outperforms those of conventional sequential recommendation systems.

摘要：採用根據使用者的過往行為建模動態偏好的序列推薦系統，對於電子商務至關重要。最近針對這些系統的研究考慮了各種資訊類型，例如圖片和文字。然而，多模態資料尚未直接用於向使用者推薦產品。在本研究中，我們提出一個基於注意力的序列推薦方法，採用項目多模態資料，例如圖片、文字和類別。首先，我們從預先訓練的 VGG 和 BERT 中提取圖片和文字特徵，並將類別轉換成多標籤形式。隨後，注意運算獨立於項目序列和多模態表示進行。最後，透過注意力融合函數整合個別注意力資訊。此外，我們對每個模態套用多任務學習損失，以改善泛化效能。從 Amazon 資料集取得的實驗結果顯示，提出的方法優於傳統序列推薦系統。

##### **Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion**
2405.17957v1 by Xiaobao Wu, Xinshuai Dong, Liangming Pan, Thong Nguyen, Anh Tuan Luu

Dynamic topic models track the evolution of topics in sequential documents,
which have derived various applications like trend analysis and opinion mining.
However, existing models suffer from repetitive topic and unassociated topic
issues, failing to reveal the evolution and hindering further applications. To
address these issues, we break the tradition of simply chaining topics in
existing work and propose a novel neural \modelfullname. We introduce a new
evolution-tracking contrastive learning method that builds the similarity
relations among dynamic topics. This not only tracks topic evolution but also
maintains topic diversity, mitigating the repetitive topic issue. To avoid
unassociated topics, we further present an unassociated word exclusion method
that consistently excludes unassociated words from discovered topics. Extensive
experiments demonstrate our model significantly outperforms state-of-the-art
baselines, tracking topic evolution with high-quality topics, showing better
performance on downstream tasks, and remaining robust to the hyperparameter for
evolution intensities. Our code is available at https://github.com/bobxwu/CFDTM .

摘要：動態主題模型追蹤順序文件中的主題演化，
衍生出趨勢分析和意見探勘等各種應用。
然而，現有模型會出現重複主題和無關主題
問題，無法揭示演化並阻礙進一步的應用。為了
解決這些問題，我們打破現有工作中僅鏈接主題的傳統，並提出一個新穎的神經網路 \modelfullname。我們引進一種新的演化追蹤對比學習方法，建立動態主題之間的相似性關係。這不僅追蹤主題演化，也維持主題多樣性，減輕重複主題問題。為了避免無關主題，我們進一步提出一個無關詞彙排除方法，持續從發現的主題中排除無關詞彙。廣泛的實驗證明，我們的模型顯著優於最先進的基準，以高品質主題追蹤主題演化，在下游任務中展現更好的效能，並對演化強度的超參數保持穩健。我們的程式碼可在 https://github.com/bobxwu/CFDTM 取得。

##### **Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives**
2405.17956v1 by Anirudhan Badrinath, Prabhat Agarwal, Jiajing Xu

For aligning large language models (LLMs), prior work has leveraged
reinforcement learning via human feedback (RLHF) or variations of direct
preference optimization (DPO). While DPO offers a simpler framework based on
maximum likelihood estimation, it compromises on the ability to tune language
models to easily maximize non-differentiable and non-binary objectives
according to the LLM designer's preferences (e.g., using simpler language or
minimizing specific kinds of harmful content). These may neither align with
user preferences nor even be able to be captured tractably by binary preference
data. To leverage the simplicity and performance of DPO with the
generalizability of RL, we propose a hybrid approach between DPO and RLHF. With
a simple augmentation to the implicit reward decomposition of DPO, we allow for
tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL.
The proposed method, Hybrid Preference Optimization (HPO), shows the ability to
effectively generalize to both user preferences and auxiliary designer
objectives, while preserving alignment performance across a range of
challenging benchmarks and model sizes.

摘要：對於對齊大型語言模型 (LLM)，先前的研究利用了透過人類回饋的強化學習 (RLHF) 或直接偏好最佳化 (DPO) 的變異。雖然 DPO 提供了一個基於最大似然估計的更簡單架構，但它在根據 LLM 設計者的偏好調整語言模型以輕鬆最大化不可微分和非二元目標的能力上有所妥協（例如，使用更簡單的語言或最小化特定類型的有害內容）。這些可能既不符合使用者的偏好，甚至無法透過二元偏好資料進行有效擷取。為了利用 DPO 的簡單性和效能以及 RL 的可概化性，我們提出 DPO 和 RLHF 之間的混合方法。透過對 DPO 的隱含獎勵分解進行簡單的擴充，我們允許調整 LLM 以使用離線 RL 最大化一組任意的輔助獎勵。所提出的方法，混合偏好最佳化 (HPO)，顯示了有效概化到使用者偏好和輔助設計者目標的能力，同時在各種具有挑戰性的基準和模型大小中保留對齊效能。

##### **Self-Guiding Exploration for Combinatorial Problems**
2405.17950v1 by Zangir Iklassov, Yali Du, Farkhad Akimov, Martin Takac

Large Language Models (LLMs) have become pivotal in addressing reasoning
tasks across diverse domains, including arithmetic, commonsense, and symbolic
reasoning. They utilize prompting techniques such as Exploration-of-Thought,
Decomposition, and Refinement to effectively navigate and solve intricate
tasks. Despite these advancements, the application of LLMs to Combinatorial
Problems (CPs), known for their NP-hardness and critical roles in logistics and
resource management remains underexplored. To address this gap, we introduce a
novel prompting strategy: Self-Guiding Exploration (SGE), designed to enhance
the performance of solving CPs. SGE operates autonomously, generating multiple
thought trajectories for each CP task. It then breaks these trajectories down
into actionable subtasks, executes them sequentially, and refines the results
to ensure optimal outcomes. We present our research as the first to apply LLMs
to a broad range of CPs and demonstrate that SGE outperforms existing prompting
strategies by over 27.84% in CP optimization performance. Additionally, SGE
achieves a 2.46% higher accuracy over the best existing results in other
reasoning tasks (arithmetic, commonsense, and symbolic).

摘要：大型語言模型 (LLM) 已成為解決跨越不同領域的推理任務的關鍵，包括算術、常識和符號推理。他們利用探索思想、分解和改進等提示技術，有效地導航和解決複雜的任務。儘管有這些進展，但 LLM 在組合問題 (CP) 中的應用仍然未被充分探索，而 CP 以其 NP 難度和在物流和資源管理中的關鍵作用而聞名。為了解決這個差距，我們引入了一種新穎的提示策略：自我引導探索 (SGE)，旨在增強解決 CP 的性能。SGE 自主運作，為每個 CP 任務產生多個思考軌跡。然後它將這些軌跡分解為可操作的子任務，依序執行它們，並改進結果以確保最佳結果。我們的研究是第一個將 LLM 應用於廣泛的 CP 的研究，並證明 SGE 在 CP 最佳化性能方面比現有的提示策略高出 27.84%。此外，SGE 在其他推理任務（算術、常識和符號）中比現有最佳結果高出 2.46%。

##### **Self-supervised Pre-training for Transferable Multi-modal Perception**
2405.17942v1 by Xiaohao Xu, Tianyi Zhang, Jinrong Yang, Matthew Johnson-Roberson, Xiaonan Huang

In autonomous driving, multi-modal perception models leveraging inputs from
multiple sensors exhibit strong robustness in degraded environments. However,
these models face challenges in efficiently and effectively transferring
learned representations across different modalities and tasks. This paper
presents NeRF-Supervised Masked Auto Encoder (NS-MAE), a self-supervised
pre-training paradigm for transferable multi-modal representation learning.
NS-MAE is designed to provide pre-trained model initializations for efficient
and high-performance fine-tuning. Our approach uses masked multi-modal
reconstruction in neural radiance fields (NeRF), training the model to
reconstruct missing or corrupted input data across multiple modalities.
Specifically, multi-modal embeddings are extracted from corrupted LiDAR point
clouds and images, conditioned on specific view directions and locations. These
embeddings are then rendered into projected multi-modal feature maps using
neural rendering techniques. The original multi-modal signals serve as
reconstruction targets for the rendered feature maps, facilitating
self-supervised representation learning. Extensive experiments demonstrate the
promising transferability of NS-MAE representations across diverse multi-modal
and single-modal perception models. This transferability is evaluated on
various 3D perception downstream tasks, such as 3D object detection and BEV map
segmentation, using different amounts of fine-tuning labeled data. Our code
will be released to support the community.

摘要：在自動駕駛中，多模態感知模型利用來自多個感測器的輸入，在惡劣的環境中表現出強大的魯棒性。然而，這些模型在有效率且有效地將學習到的表徵轉移到不同的模態和任務中時面臨挑戰。本文提出神經輻射場 (NeRF) 監督遮罩自動編碼器 (NS-MAE)，這是一種用於可轉移多模態表徵學習的自監督預訓練範例。NS-MAE 被設計為提供預訓練模型初始化，以進行有效率且高性能的微調。我們的做法使用神經輻射場 (NeRF) 中的遮罩多模態重建，訓練模型以重建跨多個模態的遺失或損壞輸入資料。具體來說，多模態嵌入從損壞的 LiDAR 點雲和影像中提取，依據特定的視圖方向和位置為條件。然後使用神經渲染技術將這些嵌入渲染到投影的多模態特徵圖中。原始的多模態訊號用作渲染特徵圖的重建目標，促進自監督表徵學習。廣泛的實驗證明了 NS-MAE 表徵在各種多模態和單模態感知模型中具有良好的可轉移性。這種可轉移性在各種 3D 感知下游任務中進行評估，例如 3D 物件偵測和 BEV 地圖分割，使用不同數量的微調標籤資料。我們的程式碼將發布以支援社群。

##### **World Models for General Surgical Grasping**
2405.17940v1 by Hongbin Lin, Bin Li, Chun Wai Wong, Juan Rojas, Xiangyu Chu, Kwok Wai Samuel Au

Intelligent vision control systems for surgical robots should adapt to
unknown and diverse objects while being robust to system disturbances. Previous
methods did not meet these requirements due to mainly relying on pose
estimation and feature tracking. We propose a world-model-based deep
reinforcement learning framework "Grasp Anything for Surgery" (GAS), that
learns a pixel-level visuomotor policy for surgical grasping, enhancing both
generality and robustness. In particular, a novel method is proposed to
estimate the values and uncertainties of depth pixels for a rigid-link object's
inaccurate region based on the empirical prior of the object's size; both depth
and mask images of task objects are encoded to a single compact 3-channel image
(size: 64x64x3) by dynamically zooming in the mask regions, minimizing the
information loss. The learned controller's effectiveness is extensively
evaluated in simulation and in a real robot. Our learned visuomotor policy
handles: i) unseen objects, including 5 types of target grasping objects and a
robot gripper, in unstructured real-world surgery environments, and ii)
disturbances in perception and control. Note that we are the first work to
achieve a unified surgical control system that grasps diverse surgical objects
using different robot grippers on real robots in complex surgery scenes
(average success rate: 69%). Our system also demonstrates significant
robustness across 6 conditions including background variation, target
disturbance, camera pose variation, kinematic control error, image noise, and
re-grasping after the gripped target object drops from the gripper. Videos and
codes can be found on our project page: https://linhongbin.github.io/gas/.

摘要：<paragraph>用於外科機器人的智能視覺控制系統應適應未知且多樣化的物體，同時對系統干擾具有魯棒性。由於主要依賴姿勢估計和特徵追蹤，先前的辦法並未滿足這些需求。我們提出一個基於世界模型的深度強化學習架構「Grasp Anything for Surgery」(GAS)，它學習一種用於外科抓握的像素級視覺運動策略，增強了通用性和魯棒性。特別是，提出了一種新方法，根據物體大小的經驗先驗，估計剛性連接物體不準確區域的深度像素值和不確定性；任務物體的深度和遮罩影像被編碼成一個單一的緊湊 3 通道影像（大小：64x64x3），通過動態縮放遮罩區域，將資訊損失降至最低。學習到的控制器在模擬和真實機器人中得到了廣泛評估。我們學習到的視覺運動策略處理：i) 未見過的物體，包括 5 種類型的目標抓握物體和一個機器人夾具，在非結構化的真實世界手術環境中，以及 ii) 感知和控制中的干擾。請注意，我們是第一個實現統一外科控制系統的工作，該系統使用不同的機器人夾具在複雜的手術場景中抓取不同的外科物體（平均成功率：69%）。我們的系統還展示了在 6 種情況下的顯著魯棒性，包括背景變化、目標干擾、相機姿勢變化、運動控制誤差、影像雜訊以及夾持目標物體從夾具掉落後重新抓取。影片和程式碼可以在我們的專案頁面找到：https://linhongbin.github.io/gas/。</paragraph>

##### **Tool Learning with Large Language Models: A Survey**
2405.17935v1 by Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, Ji-Rong Wen

Recently, tool learning with large language models (LLMs) has emerged as a
promising paradigm for augmenting the capabilities of LLMs to tackle highly
complex problems. Despite growing attention and rapid advancements in this
field, the existing literature remains fragmented and lacks systematic
organization, posing barriers to entry for newcomers. This gap motivates us to
conduct a comprehensive survey of existing works on tool learning with LLMs. In
this survey, we focus on reviewing existing literature from the two primary
aspects (1) why tool learning is beneficial and (2) how tool learning is
implemented, enabling a comprehensive understanding of tool learning with LLMs.
We first explore the "why" by reviewing both the benefits of tool integration
and the inherent benefits of the tool learning paradigm from six specific
aspects. In terms of "how", we systematically review the literature according
to a taxonomy of four key stages in the tool learning workflow: task planning,
tool selection, tool calling, and response generation. Additionally, we provide
a detailed summary of existing benchmarks and evaluation methods, categorizing
them according to their relevance to different stages. Finally, we discuss
current challenges and outline potential future directions, aiming to inspire
both researchers and industrial developers to further explore this emerging and
promising area.

摘要：<paragraph>最近，使用大型语言模型 (LLM) 的工具学习已成为一种很有前景的范例，用于增强 LLM 的能力以解决高度复杂的问题。尽管该领域备受关注且发展迅速，但现有文献仍然支离破碎，缺乏系统化的组织，对新手构成进入障碍。这一差距促使我们对 LLM 的工具学习的现有工作进行全面的调查。在本次调查中，我们重点审查了现有文献的两个主要方面：(1) 为什么工具学习有益，以及 (2) 如何实施工具学习，从而全面了解 LLM 的工具学习。我们首先通过从六个具体方面回顾工具集成的优势和工具学习范式的内在优势来探讨“为什么”。在“如何”方面，我们根据工具学习工作流程中的四个关键阶段的分类系统地回顾了文献：任务规划、工具选择、工具调用和响应生成。此外，我们还提供了现有基准和评估方法的详细摘要，并根据它们与不同阶段的相关性对其进行分类。最后，我们讨论了当前的挑战并概述了潜在的未来方向，旨在激发研究人员和工业开发人员进一步探索这一新兴且有前景的领域。</paragraph>

##### **Proof of Quality: A Costless Paradigm for Trustless Generative AI Model Inference on Blockchains**
2405.17934v1 by Zhenjie Zhang, Yuyang Rao, Hao Xiao, Xiaokui Xiao, Yin Yang

Generative AI models, such as GPT-4 and Stable Diffusion, have demonstrated
powerful and disruptive capabilities in natural language and image tasks.
However, deploying these models in decentralized environments remains
challenging. Unlike traditional centralized deployment, systematically
guaranteeing the integrity of AI model services in fully decentralized
environments, particularly on trustless blockchains, is both crucial and
difficult. In this paper, we present a new inference paradigm called
\emph{proof of quality} (PoQ) to enable the deployment of arbitrarily large
generative models on blockchain architecture. Unlike traditional approaches
based on validating inference procedures, such as ZKML or OPML, our PoQ
paradigm focuses on the outcome quality of model inference. Using lightweight
BERT-based cross-encoders as our underlying quality evaluation model, we design
and implement PQML, the first practical protocol for real-world NLP generative
model inference on blockchains, tailored for popular open-source models such as
Llama 3 and Mixtral. Our analysis demonstrates that our protocol is robust
against adversarial but rational participants in ecosystems, where lazy or
dishonest behavior results in fewer benefits compared to well-behaving
participants. The computational overhead of validating the quality evaluation
is minimal, allowing quality validators to complete the quality check within a
second, even using only a CPU. Preliminary simulation results show that PoQ
consensus is generated in milliseconds, 1,000 times faster than any existing
scheme.

摘要：生成式 AI 模型，如 GPT-4 和 Stable Diffusion，已在自然語言和影像任務中展現強大且具破壞性的能力。然而，在分散式環境中部署這些模型仍然具有挑戰性。與傳統的集中式部署不同，在完全分散式環境中系統性地保證 AI 模型服務的完整性，特別是在不可信的區塊鏈上，既至關重要又困難。在本文中，我們提出了一種新的推理範例，稱為「質量證明」（PoQ），以在區塊鏈架構上部署任意大的生成式模型。與基於驗證推理程序的傳統方法（例如 ZKML 或 OPML）不同，我們的 PoQ 範例著重於模型推理的結果品質。我們使用輕量級的 BERT 為基礎的交叉編碼器作為我們底層的品質評估模型，設計並實作了 PQML，這是第一個用於區塊鏈上實際自然語言處理生成模型推理的實用協議，專為 Llama 3 和 Mixtral 等流行的開源模型量身打造。我們的分析表明，我們的協議對於生態系統中具有敵意但理性的參與者具有穩健性，其中懶惰或不誠實的行為所產生的利益低於表現良好的參與者。驗證品質評估的運算開銷很小，讓品質驗證者即使只使用 CPU，也能在不到一秒的時間內完成品質檢查。初步模擬結果顯示，PoQ 共識在毫秒內產生，比任何現有方案快 1,000 倍。

##### **Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment**
2405.17931v1 by Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, Chang Zhou

Effectively aligning Large Language Models (LLMs) with human-centric values
while preventing the degradation of abilities acquired through Pre-training and
Supervised Fine-tuning (SFT) poses a central challenge in Reinforcement
Learning from Human Feedback (RLHF). In this paper, we first discover that
interpolating RLHF and SFT model parameters can adjust the trade-off between
human preference and basic capabilities, thereby reducing the alignment tax at
the cost of alignment reward. Inspired by this, we propose integrating the RL
policy and SFT models at each optimization step in RLHF to continuously
regulate the training direction, introducing the Online Merging Optimizer.
Specifically, we merge gradients with the parameter differences between SFT and
pretrained models, effectively steering the gradient towards maximizing rewards
in the direction of SFT optimization. We demonstrate that our optimizer works
well with different LLM families, such as Qwen and LLaMA, across various model
sizes ranging from 1.8B to 8B, various RLHF algorithms like DPO and KTO, and
existing model merging methods. It significantly enhances alignment reward
while mitigating alignment tax, achieving higher overall performance across 14
benchmarks.

摘要：有效地將大型語言模型 (LLM) 與以人為中心的價值觀保持一致，同時防止通過預訓練和監督微調 (SFT) 獲得的能力下降，這對強化學習從人類回饋 (RLHF) 中提出了核心挑戰。在本文中，我們首先發現插補 RLHF 和 SFT 模型參數可以調整人類偏好和基本能力之間的權衡，從而以對齊獎勵為代價降低對齊成本。受此啟發，我們提出在 RLHF 中的每個優化步驟中整合 RL 策略和 SFT 模型，以持續調節訓練方向，引入線上合併優化器。具體來說，我們將梯度與 SFT 和預訓練模型之間的參數差異合併，有效地引導梯度朝著 SFT 優化的方向最大化獎勵。我們證明了我們的優化器適用於不同的 LLM 家族，例如 Qwen 和 LLaMA，跨越從 1.8B 到 8B 的各種模型大小、各種 RLHF 演算法，例如 DPO 和 KTO，以及現有的模型合併方法。它顯著提高了對齊獎勵，同時減輕了對齊成本，在 14 個基準測試中實現了更高的整體性能。

##### **The Evolution of Multimodal Model Architectures**
2405.17927v1 by Shakti N. Wadekar, Abhishek Chaurasia, Aman Chadha, Eugenio Culurciello

This work uniquely identifies and characterizes four prevalent multimodal
model architectural patterns in the contemporary multimodal landscape.
Systematically categorizing models by architecture type facilitates monitoring
of developments in the multimodal domain. Distinct from recent survey papers
that present general information on multimodal architectures, this research
conducts a comprehensive exploration of architectural details and identifies
four specific architectural types. The types are distinguished by their
respective methodologies for integrating multimodal inputs into the deep neural
network model. The first two types (Type A and B) deeply fuses multimodal
inputs within the internal layers of the model, whereas the following two types
(Type C and D) facilitate early fusion at the input stage. Type-A employs
standard cross-attention, whereas Type-B utilizes custom-designed layers for
modality fusion within the internal layers. On the other hand, Type-C utilizes
modality-specific encoders, while Type-D leverages tokenizers to process the
modalities at the model's input stage. The identified architecture types aid
the monitoring of any-to-any multimodal model development. Notably, Type-C and
Type-D are currently favored in the construction of any-to-any multimodal
models. Type-C, distinguished by its non-tokenizing multimodal model
architecture, is emerging as a viable alternative to Type-D, which utilizes
input-tokenizing techniques. To assist in model selection, this work highlights
the advantages and disadvantages of each architecture type based on data and
compute requirements, architecture complexity, scalability, simplification of
adding modalities, training objectives, and any-to-any multimodal generation
capability.

摘要：<paragraph>這項研究獨特地識別並描述了當代多模態領域中四種流行的多模態模型架構模式。
系統性地按架構類型對模型進行分類，有助於監控多模態領域的發展。
與最近發表的提供多模態架構一般資訊的綜述論文不同，這項研究對架構細節進行了全面探討，並識別出四種類型的特定架構。
這些類型由其將多模態輸入整合到深度神經網路模型中的各自方法區分。
前兩種類型（類型 A 和 B）在模型的內部層深度融合多模態輸入，而後兩種類型（類型 C 和 D）則在輸入階段促進早期融合。
類型 A 採用標準的交叉注意力，而類型 B 則利用自訂設計的層在內部層進行模態融合。
另一方面，類型 C 利用特定於模態的編碼器，而類型 D 則利用分詞器在模型的輸入階段處理模態。
已識別的架構類型有助於監控任何到任何多模態模型的開發。
值得注意的是，類型 C 和類型 D 目前在任何到任何多模態模型的建構中受到青睞。
類型 C 以其非分詞多模態模型架構為特點，正成為利用輸入分詞技術的類型 D 的可行替代方案。
為了協助模型選擇，這項研究根據資料和運算需求、架構複雜性、可擴充性、簡化模態新增、訓練目標和任何到任何多模態生成能力，重點說明了每種類型架構的優缺點。</paragraph>

##### **Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models**
2405.17915v1 by Longze Chen, Ziqiang Liu, Wanwei He, Yunshui Li, Run Luo, Min Yang

Long-context modeling capabilities are important for large language models
(LLMs) in various applications. However, directly training LLMs with long
context windows is insufficient to enhance this capability since some training
samples do not exhibit strong semantic dependencies across long contexts. In
this study, we propose a data mining framework \textbf{ProLong} that can assign
each training sample with a long dependency score, which can be used to rank
and filter samples that are more advantageous for enhancing long-context
modeling abilities in LLM training. Specifically, we first use delta perplexity
scores to measure the \textit{Dependency Strength} between text segments in a
given document. Then we refine this metric based on the \textit{Dependency
Distance} of these segments to incorporate spatial relationships across
long-contexts. Final results are calibrated with a \textit{Dependency
Specificity} metric to prevent trivial dependencies introduced by repetitive
patterns. Moreover, a random sampling approach is proposed to optimize the
computational efficiency of ProLong. Comprehensive experiments on multiple
benchmarks indicate that ProLong effectively identifies documents that carry
long dependencies and LLMs trained on these documents exhibit significantly
enhanced long-context modeling capabilities.

摘要：長語境建模能力對於大型語言模型 (LLM) 在各種應用中非常重要。然而，直接使用長語境窗口訓練 LLM 無法增強此能力，因為有些訓練範例並未在長語境中展現強烈的語義依賴性。在此研究中，我們提出一個資料探勘架構「ProLong」，它可以為每個訓練範例分配一個長依賴性分數，可用來對範例進行排序和篩選，以強化 LLM 訓練中增強長語境建模能力的優勢。具體而言，我們首先使用 delta 困惑度分數測量給定文件中的文字區段之間的「依賴性強度」。然後，我們根據這些區段的「依賴性距離」調整此指標，以納入長語境中的空間關係。最後的結果使用「依賴性特異性」指標進行校正，以防止重複模式所產生的瑣碎依賴性。此外，我們提出了一個隨機抽樣方法來最佳化 ProLong 的運算效率。在多個基準上的全面實驗顯示，ProLong 可以有效識別具有長依賴性的文件，而針對這些文件訓練的 LLM 則展現出顯著增強的長語境建模能力。

##### **OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision**
2405.17913v1 by Junjie Wang, Bin Chen, Bin Kang, Yulin Li, YiChi Chen, Weizhi Xian, Huifeng Chang

Open-Vocabulary Detection (OVD) aims to detect objects from novel categories
beyond the base categories on which the detector is trained. However, existing
open-vocabulary detectors trained on known category data tend to assign higher
confidence to trained categories and confuse novel categories with background.
To resolve this, we propose OV-DQUO, an \textbf{O}pen-\textbf{V}ocabulary DETR
with \textbf{D}enoising text \textbf{Q}uery training and open-world
\textbf{U}nknown \textbf{O}bjects supervision. Specifically, we introduce a
wildcard matching method that enables the detector to learn from pairs of
unknown objects recognized by the open-world detector and text embeddings with
general semantics, mitigating the confidence bias between base and novel
categories. Additionally, we propose a denoising text query training strategy
that synthesizes additional noisy query-box pairs from open-world unknown
objects to trains the detector through contrastive learning, enhancing its
ability to distinguish novel objects from the background. We conducted
extensive experiments on the challenging OV-COCO and OV-LVIS benchmarks,
achieving new state-of-the-art results of 45.6 AP50 and 39.3 mAP on novel
categories respectively, without the need for additional training data. Models
and code are released at https://github.com/xiaomoguhz/OV-DQUO

摘要：開放詞彙偵測 (OVD) 旨在偵測訓練偵測器時基礎類別以外的新穎類別物件。然而，訓練於已知類別資料的現有開放詞彙偵測器傾向於將較高信心指派給訓練類別，並將新穎類別與背景混淆。為了解決這個問題，我們提出 OV-DQUO，一個具有去噪文字查詢訓練和開放世界未知物件監督的開放詞彙 DETR。具體來說，我們引入了一個萬用字元比對方法，讓偵測器能夠從開放世界偵測器辨識出的未知物件對和具有一般語意的文字嵌入學習，減輕基礎和新穎類別之間的信心偏差。此外，我們提出了一個去噪文字查詢訓練策略，該策略從開放世界未知物件合成額外的雜訊查詢方塊對，以透過對比學習訓練偵測器，增強其將新穎物件與背景區分開來的能力。我們在具有挑戰性的 OV-COCO 和 OV-LVIS 基準上進行了廣泛的實驗，分別在新的類別上達到了 45.6 AP50 和 39.3 mAP 的最新技術水準，而無需額外的訓練資料。模型和程式碼已在 https://github.com/xiaomoguhz/OV-DQUO 發布

##### **Boosting Protein Language Models with Negative Sample Mining**
2405.17902v1 by Yaoyao Xu, Xinjian Zhao, Xiaozhuang Song, Benyou Wang, Tianshu Yu

We introduce a pioneering methodology for boosting large language models in
the domain of protein representation learning. Our primary contribution lies in
the refinement process for correlating the over-reliance on co-evolution
knowledge, in a way that networks are trained to distill invaluable insights
from negative samples, constituted by protein pairs sourced from disparate
categories. By capitalizing on this novel approach, our technique steers the
training of transformer-based models within the attention score space. This
advanced strategy not only amplifies performance but also reflects the nuanced
biological behaviors exhibited by proteins, offering aligned evidence with
traditional biological mechanisms such as protein-protein interaction. We
experimentally observed improved performance on various tasks over datasets, on
top of several well-established large protein models. This innovative paradigm
opens up promising horizons for further progress in the realms of protein
research and computational biology.

摘要：我們提出了一種開創性的方法，用於提升大型語言模型在蛋白質表徵學習領域中的能力。我們的主要貢獻在於，針對過度依賴共演化知識的精煉過程，讓網路訓練從負面樣本中萃取出寶貴的見解，這些負面樣本是由來自不同類別的蛋白質對所構成。透過利用這種新穎的方法，我們的技術引導基於 Transformer 的模型在注意力分數空間中進行訓練。這種先進的策略不僅提升了效能，也反映了蛋白質所表現出的細微生物行為，並提供了與傳統生物機制（例如蛋白質-蛋白質交互作用）一致的證據。我們在各種任務中對多個已建立的大型蛋白質模型進行實驗，觀察到在資料集上的效能提升。這種創新的範例為蛋白質研究和計算生物學領域的進一步進展開啟了充滿希望的新視野。

##### **Enhancing Emotion Recognition in Conversation through Emotional Cross-Modal Fusion and Inter-class Contrastive Learning**
2405.17900v1 by Haoxiang Shi, Xulong Zhang, Ning Cheng, Yong Zhang, Jun Yu, Jing Xiao, Jianzong Wang

The purpose of emotion recognition in conversation (ERC) is to identify the
emotion category of an utterance based on contextual information. Previous ERC
methods relied on simple connections for cross-modal fusion and ignored the
information differences between modalities, resulting in the model being unable
to focus on modality-specific emotional information. At the same time, the
shared information between modalities was not processed to generate emotions.
Information redundancy problem. To overcome these limitations, we propose a
cross-modal fusion emotion prediction network based on vector connections. The
network mainly includes two stages: the multi-modal feature fusion stage based
on connection vectors and the emotion classification stage based on fused
features. Furthermore, we design a supervised inter-class contrastive learning
module based on emotion labels. Experimental results confirm the effectiveness
of the proposed method, demonstrating excellent performance on the IEMOCAP and
MELD datasets.

摘要：對話中情緒辨識（ERC）的目的是根據脈絡資訊辨識發話的情緒類別。先前 ERC 方法仰賴簡單的連結進行跨模態融合，且忽略了模態之間資訊的差異，導致模型無法專注於模態特定的情緒資訊。同時，模態之間的共用資訊並未經過處理以產生情緒，造成資訊冗餘問題。為了克服這些限制，我們提出一個基於向量連結的跨模態融合情緒預測網路。該網路主要包含兩個階段：基於連結向量的多模態特徵融合階段，以及基於融合特徵的情緒分類階段。此外，我們設計了一個基於情緒標籤的監督式類間對比學習模組。實驗結果證實了所提出方法的有效性，在 IEMOCAP 和 MELD 資料集上展現出優異的效能。

##### **FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic Prediction**
2405.17898v1 by Zhonghang Li, Lianghao Xia, Yong Xu, Chao Huang

The objective of traffic prediction is to accurately forecast and analyze the
dynamics of transportation patterns, considering both space and time. However,
the presence of distribution shift poses a significant challenge in this field,
as existing models struggle to generalize well when faced with test data that
significantly differs from the training distribution. To tackle this issue,
this paper introduces a simple and universal spatio-temporal prompt-tuning
framework-FlashST, which adapts pre-trained models to the specific
characteristics of diverse downstream datasets, improving generalization in
diverse traffic prediction scenarios. Specifically, the FlashST framework
employs a lightweight spatio-temporal prompt network for in-context learning,
capturing spatio-temporal invariant knowledge and facilitating effective
adaptation to diverse scenarios. Additionally, we incorporate a distribution
mapping mechanism to align the data distributions of pre-training and
downstream data, facilitating effective knowledge transfer in spatio-temporal
forecasting. Empirical evaluations demonstrate the effectiveness of our FlashST
across different spatio-temporal prediction tasks using diverse urban datasets.
Code is available at https://github.com/HKUDS/FlashST.

摘要：交通预测的目标是准确预测和分析交通模式的动态，同时考虑空间和时间。然而，分布变化的存在对该领域提出了重大挑战，因为现有模型在面对与训练分布显着不同的测试数据时难以很好地泛化。为了解决这个问题，本文介绍了一个简单且通用的时空提示调整框架 FlashST，该框架将预训练模型调整为不同下游数据集的特定特征，从而提高了在不同交通预测场景中的泛化能力。具体来说，FlashST 框架采用轻量级的时空提示网络进行上下文学习，捕获时空不变知识并促进对不同场景的有效适应。此外，我们结合了一种分布映射机制来对齐预训练和下游数据的分布，从而促进时空预测中的有效知识转移。实证评估表明，我们的 FlashST 在使用不同城市数据集的不同时空预测任务中都表现出有效性。代码可在 https://github.com/HKUDS/FlashST 获得。

##### **White-box Multimodal Jailbreaks Against Large Vision-Language Models**
2405.17894v1 by Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, Yu-Gang Jiang

Recent advancements in Large Vision-Language Models (VLMs) have underscored
their superiority in various multimodal tasks. However, the adversarial
robustness of VLMs has not been fully explored. Existing methods mainly assess
robustness through unimodal adversarial attacks that perturb images, while
assuming inherent resilience against text-based attacks. Different from
existing attacks, in this work we propose a more comprehensive strategy that
jointly attacks both text and image modalities to exploit a broader spectrum of
vulnerability within VLMs. Specifically, we propose a dual optimization
objective aimed at guiding the model to generate affirmative responses with
high toxicity. Our attack method begins by optimizing an adversarial image
prefix from random noise to generate diverse harmful responses in the absence
of text input, thus imbuing the image with toxic semantics. Subsequently, an
adversarial text suffix is integrated and co-optimized with the adversarial
image prefix to maximize the probability of eliciting affirmative responses to
various harmful instructions. The discovered adversarial image prefix and text
suffix are collectively denoted as a Universal Master Key (UMK). When
integrated into various malicious queries, UMK can circumvent the alignment
defenses of VLMs and lead to the generation of objectionable content, known as
jailbreaks. The experimental results demonstrate that our universal attack
strategy can effectively jailbreak MiniGPT-4 with a 96% success rate,
highlighting the vulnerability of VLMs and the urgent need for new alignment
strategies.

摘要：近期大型视觉语言模型 (VLM) 的进步强调了它们在各种多模态任务中的优势。然而，VLM 的对抗鲁棒性尚未得到充分探索。现有方法主要通过扰动图像的单模态对抗攻击来评估鲁棒性，同时假设对基于文本的攻击具有内在的弹性。与现有攻击不同，在这项工作中，我们提出了一种更全面的策略，同时攻击文本和图像模态，以利用 VLM 中更广泛的漏洞范围。具体来说，我们提出了一个双重优化目标，旨在引导模型生成具有高毒性的肯定性响应。我们的攻击方法从随机噪声中优化对抗图像前缀开始，以在没有文本输入的情况下生成各种有害响应，从而为图像注入有毒语义。随后，将对抗文本后缀与对抗图像前缀集成并共同优化，以最大化对各种有害指令做出肯定性响应的概率。发现的对抗图像前缀和文本后缀统称为通用主密钥 (UMK)。当集成到各种恶意查询中时，UMK 可以规避 VLM 的对齐防御，并导致生成被称为越狱的令人反感的内容。实验结果表明，我们的通用攻击策略可以有效地以 96% 的成功率越狱 MiniGPT-4，突显了 VLM 的脆弱性以及对新对齐策略的迫切需求。

##### **Arithmetic Reasoning with LLM: Prolog Generation & Permutation**
2405.17893v1 by Xiaocheng Yang, Bingsen Chen, Yik-Cheung Tam

Instructing large language models (LLMs) to solve elementary school math
problems has shown great success using Chain of Thought (CoT). However, the CoT
approach relies on an LLM to generate a sequence of arithmetic calculations
which can be prone to cascaded calculation errors. We hypothesize that an LLM
should focus on extracting predicates and generating symbolic formulas from the
math problem description so that the underlying calculation can be done via an
external code interpreter. We investigate using LLM to generate Prolog programs
to solve mathematical questions. Experimental results show that our
Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K
benchmark across three distinct LLMs. In addition, given the insensitive
ordering of predicates and symbolic formulas in Prolog, we propose to permute
the ground truth predicates for more robust LLM training via data augmentation.

摘要：指導大型語言模型 (LLM) 解決小學數學問題，已使用思維鏈 (CoT) 顯示出巨大的成功。然而，CoT 方法依賴於 LLM 來產生一連串的算術計算，這可能會導致連鎖的計算錯誤。我們假設 LLM 應專注於從數學問題描述中提取謂詞和產生符號公式，以便基礎計算可透過外部程式碼解釋器完成。我們研究使用 LLM 來產生 Prolog 程式，以解決數學問題。實驗結果顯示，我們的 Prolog 基礎算術問題求解在 GSM8K 基準測試中，超越了三個不同的 LLM 中的 CoT 產生。此外，鑑於 Prolog 中謂詞和符號公式的順序不敏感，我們建議排列基本事實謂詞，以透過資料擴充進行更穩健的 LLM 訓練。

##### **SLMRec: Empowering Small Language Models for Sequential Recommendation**
2405.17890v1 by Wujiang Xu, Zujie Liang, Jiaojiao Han, Xuying Ning, Wenfang Lin, Linxun Chen, Feng Wei, Yongfeng Zhang

The sequential Recommendation (SR) task involves predicting the next item a
user is likely to interact with, given their past interactions. The SR models
examine the sequence of a user's actions to discern more complex behavioral
patterns and temporal dynamics. Recent research demonstrates the great impact
of LLMs on sequential recommendation systems, either viewing sequential
recommendation as language modeling or serving as the backbone for user
representation. Although these methods deliver outstanding performance, there
is scant evidence of the necessity of a large language model and how large the
language model is needed, especially in the sequential recommendation scene.
Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to
apply a LLM-based model in real-world platforms that often need to process
billions of traffic logs daily. In this paper, we explore the influence of
LLMs' depth by conducting extensive experiments on large-scale industry
datasets. Surprisingly, we discover that most intermediate layers of LLMs are
redundant. Motivated by this insight, we empower small language models for SR,
namely SLMRec, which adopt a simple yet effective knowledge distillation
method. Moreover, SLMRec is orthogonal to other post-training efficiency
techniques, such as quantization and pruning, so that they can be leveraged in
combination. Comprehensive experimental results illustrate that the proposed
SLMRec model attains the best performance using only 13% of the parameters
found in LLM-based recommendation models, while simultaneously achieving up to
6.6x and 8.0x speedups in training and inference time costs, respectively.

摘要：序列推薦 (SR) 任務涉及預測使用者根據過去互動，接下來可能會互動的項目。SR 模型會檢視使用者的動作序列，以辨別更複雜的行為模式和時間動態。最近的研究顯示，LLM 對序列推薦系統有很大的影響，無論是將序列推薦視為語言模型，或作為使用者表徵的骨幹。儘管這些方法提供了傑出的效能，但鮮少證據顯示大型語言模型的必要性，以及需要多大的語言模型，特別是在序列推薦場景中。同時，由於 LLM 龐大，在現實世界平台中應用 LLM 為基礎的模型既低效又不切實際，因為這些平台通常需要每天處理數十億個流量記錄。在本文中，我們透過在大型產業資料集上進行廣泛的實驗，探討 LLM 深度的影響。令人驚訝的是，我們發現 LLM 的大多數中間層都是多餘的。受到這個見解的啟發，我們為 SR 賦能小型語言模型，即 SLMRec，它採用一種簡單但有效的知識提煉方法。此外，SLMRec 與其他訓練後效率技術正交，例如量化和剪枝，因此可以結合使用。全面的實驗結果說明，建議的 SLMRec 模型只使用 LLM 為基礎的推薦模型中 13% 的參數，就能獲得最佳效能，同時在訓練和推理時間成本分別達到最高 6.6 倍和 8.0 倍的加速。

##### **Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment**
2405.17888v1 by Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong

Aligning human preference and value is an important requirement for
contemporary foundation models. State-of-the-art techniques such as
Reinforcement Learning from Human Feedback (RLHF) often consist of two stages:
1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from
human demonstration data; 2) Preference learning, where preference data is used
to learn a reward model, which is in turn used by a reinforcement learning (RL)
step to fine-tune the model. Such reward model serves as a proxy to human
preference, and it is critical to guide the RL step towards improving the model
quality. In this work, we argue that the SFT stage significantly benefits from
learning a reward model as well. Instead of using the human demonstration data
directly via supervised learning, we propose to leverage an Inverse
Reinforcement Learning (IRL) technique to (explicitly or implicitly) build an
reward model, while learning the policy model. This approach leads to new SFT
algorithms that are not only efficient to implement, but also promote the
ability to distinguish between the preferred and non-preferred continuations.
Moreover, we identify a connection between the proposed IRL based approach, and
certain self-play approach proposed recently, and showed that self-play is a
special case of modeling a reward-learning agent. Theoretically, we show that
the proposed algorithms converge to the stationary solutions of the IRL
problem. Empirically, we align 1B and 7B models using proposed methods and
evaluate them on a reward benchmark model and the HuggingFace Open LLM
Leaderboard. The proposed methods show significant performance improvement over
existing SFT approaches. Our results indicate that it is beneficial to
explicitly or implicitly leverage reward learning throughout the entire
alignment process.

摘要：對齊人類偏好和價值觀是當代基礎模型的一項重要需求。最先進的技術，例如人類回饋強化學習 (RLHF)，通常包含兩個階段：1) 監督微調 (SFT)，其中模型通過從人類示範數據中學習進行微調；2) 偏好學習，其中偏好數據用於學習獎勵模型，而獎勵模型反過來又由強化學習 (RL) 步驟用於微調模型。這樣的獎勵模型作為人類偏好的代理，對於指導 RL 步驟改進模型品質至關重要。在這項工作中，我們認為 SFT 階段也顯著受益於學習獎勵模型。我們建議利用逆強化學習 (IRL) 技術（顯式或隱式地）構建獎勵模型，同時學習策略模型，而不是直接通過監督學習使用人類示範數據。這種方法導致新的 SFT 演算法，這些演算法不僅易於實作，而且還能提升區分首選和非首選延續的能力。此外，我們找出所提出的基於 IRL 的方法與最近提出的某些自玩方法之間的關聯，並表明自玩是建模獎勵學習代理的特殊情況。在理論上，我們表明所提出的演算法收斂於 IRL 問題的平穩解。在經驗上，我們使用所提出的方法對齊 1B 和 7B 模型，並在獎勵基準模型和 HuggingFace Open LLM 排行榜上對它們進行評估。所提出的方法顯示出比現有的 SFT 方法有顯著的性能提升。我們的結果表明，在整個對齊過程中顯式或隱式地利用獎勵學習是有益的。

##### **An Information Theoretic Metric for Evaluating Unlearning Models**
2405.17878v1 by Dongjae Jeon, Wonje Jeung, Taeheon Kim, Albert No, Jonghyun Choi

Machine unlearning (MU) addresses privacy concerns by removing information of
`forgetting data' samples from trained models. Typically, evaluating MU methods
involves comparing unlearned models to those retrained from scratch without
forgetting data, using metrics such as membership inference attacks (MIA) and
accuracy measurements. These evaluations implicitly assume that if the output
logits of the unlearned and retrained models are similar, the unlearned model
has successfully forgotten the data. Here, we challenge if this assumption is
valid. In particular, we conduct a simple experiment of training only the last
layer of a given original model using a novel masked-distillation technique
while keeping the rest fixed. Surprisingly, simply altering the last layer
yields favorable outcomes in the existing evaluation metrics, while the model
does not successfully unlearn the samples or classes. For better evaluating the
MU methods, we propose a metric that quantifies the residual information about
forgetting data samples in intermediate features using mutual information,
called information difference index or IDI for short. The IDI provides a
comprehensive evaluation of MU methods by efficiently analyzing the internal
structure of DNNs. Our metric is scalable to large datasets and adaptable to
various model architectures. Additionally, we present COLapse-and-Align (COLA),
a simple contrastive-based method that effectively unlearns intermediate
features.

摘要：机器去学习（MU）通过移除训练模型中“遗忘数据”样本的信息来解决隐私问题。通常，评估 MU 方法涉及将未学习的模型与从头开始重新训练的模型进行比较，而不忘记数据，使用诸如成员资格推断攻击 (MIA) 和准确度测量等指标。这些评估隐含地假设，如果未学习和重新训练的模型的输出 logit 相似，则未学习的模型已成功忘记数据。在此，我们质疑此假设是否有效。特别是，我们仅使用一种新颖的掩码蒸馏技术训练给定原始模型的最后一层，同时保持其余部分固定，从而进行一个简单的实验。令人惊讶的是，仅仅改变最后一层就会在现有的评估指标中产生有利的结果，而该模型并不能成功地忘记样本或类别。为了更好地评估 MU 方法，我们提出了一种度量标准，该度量标准使用互信息量化中间特征中关于遗忘数据样本的残留信息，简称信息差异指数或 IDI。IDI 通过有效分析 DNN 的内部结构，提供对 MU 方法的综合评估。我们的指标可扩展到大型数据集，并适用于各种模型架构。此外，我们提出了 COLapse-and-Align (COLA)，这是一种简单的基于对比的方法，可以有效地取消中间特征的学习。

##### **NUTS, NARS, and Speech**
2405.17874v1 by D. van der Sluis

To investigate whether "Intelligence is the capacity of an
information-processing system to adapt to its environment while operating with
insufficient knowledge and resources", we look at utilising the non axiomatic
reasoning system (NARS) for speech recognition. This article presents NUTS:
raNdom dimensionality redUction non axiomaTic reasoning few Shot learner for
perception. NUTS consists of naive dimensionality reduction, some
pre-processing, and then non axiomatic reasoning (NARS). With only 2 training
examples NUTS performs similarly to the Whisper Tiny model for discrete word
identification.

摘要：為了探討「智慧是資訊處理系統在知識和資源不足的情況下適應其環境的能力」，我們利用非公理推理系統 (NARS) 進行語音辨識。本文提出 NUTS：隨機降維、非公理推理、少量樣本學習者，用於感知。NUTS 包含樸素降維、一些前處理，然後是非公理推理 (NARS)。NUTS 僅需 2 個訓練範例，就能執行與 Whisper Tiny 模型類似的離散字詞辨識。

##### **MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization**
2405.17873v1 by Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, Yu Wang

Diffusion models have achieved significant visual generation quality.
However, their significant computational and memory costs pose challenge for
their application on resource-constrained mobile devices or even desktop GPUs.
Recent few-step diffusion models reduces the inference time by reducing the
denoising steps. However, their memory consumptions are still excessive. The
Post Training Quantization (PTQ) replaces high bit-width FP representation with
low-bit integer values (INT4/8) , which is an effective and efficient technique
to reduce the memory cost. However, when applying to few-step diffusion models,
existing quantization methods face challenges in preserving both the image
quality and text alignment. To address this issue, we propose an
mixed-precision quantization framework - MixDQ. Firstly, We design specialized
BOS-aware quantization method for highly sensitive text embedding quantization.
Then, we conduct metric-decoupled sensitivity analysis to measure the
sensitivity of each layer. Finally, we develop an integer-programming-based
method to conduct bit-width allocation. While existing quantization methods
fall short at W8A8, MixDQ could achieve W8A8 without performance loss, and W4A8
with negligible visual degradation. Compared with FP16, we achieve 3-4x
reduction in model size and memory cost, and 1.45x latency speedup.

摘要：擴散模型已達到顯著的視覺生成品質。
然而，它們顯著的運算和記憶體成本對其在資源受限的行動裝置或甚至桌上型 GPU 上的應用構成挑戰。
最近的幾步擴散模型透過減少去噪步驟來縮短推論時間。
然而，它們的記憶體消耗仍然過多。
訓練後量化 (PTQ) 以低位元整數值 (INT4/8) 取代高位元寬度 FP 表示，這是一種有效且高效的技術，可降低記憶體成本。
然而，在應用於幾步擴散模型時，現有的量化方法在保留影像品質和文字對齊方面面臨挑戰。
為了解決這個問題，我們提出一個混合精度量化架構 - MixDQ。
首先，我們設計了專門的 BOS 感知量化方法，用於高度敏感的文字嵌入量化。
然後，我們進行指標解耦的敏感度分析，以測量每層的敏感度。
最後，我們開發了一個基於整數規劃的方法來進行位元寬度分配。
雖然現有的量化方法在 W8A8 上表現不佳，但 MixDQ 能夠在不損失效能的情況下實現 W8A8，並以極小的視覺劣化實現 W4A8。
與 FP16 相比，我們將模型大小和記憶體成本降低了 3-4 倍，並將延遲速度提高了 1.45 倍。

##### **Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment**
2405.17871v1 by Xin Xiao, Bohong Wu, Jiacong Wang, Chunyuan Li, Xun Zhou, Haoyuan Guo

Existing image-text modality alignment in Vision Language Models (VLMs)
treats each text token equally in an autoregressive manner. Despite being
simple and effective, this method results in sub-optimal cross-modal alignment
by over-emphasizing the text tokens that are less correlated with or even
contradictory with the input images. In this paper, we advocate for assigning
distinct contributions for each text token based on its visual correlation.
Specifically, we present by contrasting image inputs, the difference in
prediction logits on each text token provides strong guidance of visual
correlation. We therefore introduce Contrastive ALignment (CAL), a simple yet
effective re-weighting strategy that prioritizes training visually correlated
tokens. Our experimental results demonstrate that CAL consistently improves
different types of VLMs across different resolutions and model sizes on various
benchmark datasets. Importantly, our method incurs minimal additional
computational overhead, rendering it highly efficient compared to alternative
data scaling strategies. Codes are available at
https://github.com/foundation-multimodal-models/CAL.

摘要：現有的視覺語言模型 (VLM) 中的影像文字模態對齊，以自迴歸的方式平均處理每個文字符號。儘管這種方法簡單且有效，但它會過度強調與輸入影像相關性較低，甚至與其矛盾的文字符號，進而導致次佳的跨模態對齊。在本文中，我們主張根據每個文字符號的視覺相關性，為其分配不同的貢獻。具體來說，我們透過對比影像輸入，提供每個文字符號的預測邏輯斯迴歸中的差異，以提供視覺相關性的強有力指導。因此，我們引入了對比對齊 (CAL)，這是一種簡單但有效的重新加權策略，可優先訓練視覺相關的符號。我們的實驗結果表明，CAL 在各種基準資料集上，持續改善不同解析度和模型大小的各種 VLM。重要的是，我們的方法會產生最小的額外運算負擔，與其他資料擴充策略相比，效率極高。程式碼可在 https://github.com/foundation-multimodal-models/CAL 取得。

