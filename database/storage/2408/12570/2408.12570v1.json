{"2408.12570": {"publish_time": "2024-08-22", "title": "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale", "paper_summary": "We present Jamba-1.5, new instruction-tuned large language models based on\nour Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts\narchitecture, providing high throughput and low memory usage across context\nlengths, while retaining the same or better quality as Transformer models. We\nrelease two model sizes: Jamba-1.5-Large, with 94B active parameters, and\nJamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a\nvariety of conversational and instruction-following capabilties, and have an\neffective context length of 256K tokens, the largest amongst open-weight\nmodels. To support cost-effective inference, we introduce ExpertsInt8, a novel\nquantization technique that allows fitting Jamba-1.5-Large on a machine with 8\n80GB GPUs when processing 256K-token contexts without loss of quality. When\nevaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models\nachieve excellent results while providing high throughput and outperforming\nother open-weight models on long-context benchmarks. The model weights for both\nsizes are publicly available under the Jamba Open Model License and we release\nExpertsInt8 as open source.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa Jamba-1.5\uff0c\u9019\u662f\u57fa\u65bc\u6211\u5011 Jamba \u67b6\u69cb\u7684\u65b0\u6307\u4ee4\u8abf\u6821\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u3002Jamba \u662f Transformer-Mamba \u5c08\u5bb6\u6df7\u5408\u9ad4\u6df7\u5408\u67b6\u69cb\uff0c\u5728\u5404\u7a2e\u5167\u5bb9\u9577\u5ea6\u4e2d\u63d0\u4f9b\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff0c\u540c\u6642\u4fdd\u6709\u8207 Transformer \u6a21\u578b\u76f8\u540c\u6216\u66f4\u597d\u7684\u54c1\u8cea\u3002\u6211\u5011\u767c\u5e03\u5169\u7a2e\u6a21\u578b\u5927\u5c0f\uff1aJamba-1.5-Large\uff0c\u6709 94B \u6d3b\u52d5\u53c3\u6578\uff0c\u4ee5\u53ca Jamba-1.5-Mini\uff0c\u6709 12B \u6d3b\u52d5\u53c3\u6578\u3002\u5169\u7a2e\u6a21\u578b\u90fd\u91dd\u5c0d\u5404\u7a2e\u5c0d\u8a71\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u9032\u884c\u5fae\u8abf\uff0c\u4e26\u5177\u6709 256K \u4ee4\u724c\u7684\u6709\u6548\u5167\u5bb9\u9577\u5ea6\uff0c\u662f\u958b\u653e\u6b0a\u91cd\u6a21\u578b\u4e2d\u6700\u9577\u7684\u3002\u70ba\u4e86\u652f\u63f4\u5177\u6210\u672c\u6548\u76ca\u7684\u63a8\u8ad6\uff0c\u6211\u5011\u5f15\u5165\u4e86 ExpertsInt8\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u91cf\u5316\u6280\u8853\uff0c\u53ef\u5728\u8655\u7406 256K \u4ee4\u724c\u5167\u5bb9\u6642\u8b93 Jamba-1.5-Large \u7b26\u5408\u642d\u8f09 8 \u500b 80GB GPU \u7684\u6a5f\u5668\uff0c\u4e14\u4e0d\u640d\u5931\u54c1\u8cea\u3002\u5728\u91dd\u5c0d\u4e00\u7d44\u5b78\u8853\u548c\u804a\u5929\u6a5f\u5668\u4eba\u57fa\u6e96\u9032\u884c\u8a55\u4f30\u6642\uff0cJamba-1.5 \u6a21\u578b\u5728\u63d0\u4f9b\u9ad8\u541e\u5410\u91cf\u7684\u540c\u6642\uff0c\u5728\u9577\u5167\u5bb9\u57fa\u6e96\u4e0a\u512a\u65bc\u5176\u4ed6\u958b\u653e\u6b0a\u91cd\u6a21\u578b\uff0c\u4e26\u7372\u5f97\u512a\u7570\u7684\u7d50\u679c\u3002\u5169\u7a2e\u5927\u5c0f\u7684\u6a21\u578b\u6b0a\u91cd\u5728 Jamba \u958b\u653e\u6a21\u578b\u6388\u6b0a\u4e0b\u516c\u958b\u63d0\u4f9b\uff0c\u6211\u5011\u5c07 ExpertsInt8 \u4f5c\u70ba\u958b\u653e\u539f\u59cb\u78bc\u767c\u5e03\u3002", "author": "Jamba Team et.al.", "authors": "Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, Yoav Shoham", "id": "2408.12570v1", "paper_url": "http://arxiv.org/abs/2408.12570v1", "repo": "null"}}