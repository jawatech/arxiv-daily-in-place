{"2412.15660": {"publish_time": "2024-12-20", "title": "Adaptable and Precise: Enterprise-Scenario LLM Function-Calling Capability Training Pipeline", "paper_summary": "Enterprises possess a vast array of API assets scattered across various\nfunctions, forming the backbone of existing business processes. By leveraging\nthese APIs as functional tools, enterprises can design diverse,\nscenario-specific agent applications, driven by on-premise function-calling\nmodels as the core engine. However, generic models often fail to meet\nenterprise requirements in terms of computational efficiency, output accuracy,\nand stability, necessitating scenario-specific adaptation. In this paper, we\npropose a training pipeline for function-calling capabilities tailored to\nreal-world business scenarios. This pipeline includes the synthesis and\naugmentation of scenario-specific function-calling data, model fine-tuning, and\nperformance evaluation and analysis. Using this pipeline, we generated 1,260\nfully AI-generated samples and 1,035 augmented manually-labeled samples in\ndigital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as\nthe base model and fine-tuned using the LoRA method on four GPUs with 24GB\nVRAM. Our fine-tuned model demonstrated outstanding performance in evaluations\nand practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test\nset. These results validate the reliability of the proposed pipeline for\ntraining scenario-specific function-calling models.", "paper_summary_zh": "\u4f01\u696d\u64c1\u6709\u5ee3\u6cdb\u7684 API \u8cc7\u7522\uff0c\u6563\u4f48\u65bc\u5404\u7a2e\u529f\u80fd\u4e2d\uff0c\u5f62\u6210\u73fe\u6709\u696d\u52d9\u6d41\u7a0b\u7684\u9aa8\u5e79\u3002\u900f\u904e\u5229\u7528\u9019\u4e9b API \u4f5c\u70ba\u529f\u80fd\u5de5\u5177\uff0c\u4f01\u696d\u53ef\u4ee5\u8a2d\u8a08\u591a\u6a23\u5316\u7684\u3001\u7279\u5b9a\u65bc\u5834\u666f\u7684\u4ee3\u7406\u61c9\u7528\u7a0b\u5f0f\uff0c\u4ee5\u5167\u90e8\u51fd\u5f0f\u547c\u53eb\u6a21\u578b\u4f5c\u70ba\u6838\u5fc3\u5f15\u64ce\u3002\u7136\u800c\uff0c\u901a\u7528\u6a21\u578b\u901a\u5e38\u7121\u6cd5\u6eff\u8db3\u4f01\u696d\u5728\u904b\u7b97\u6548\u7387\u3001\u8f38\u51fa\u6e96\u78ba\u5ea6\u548c\u7a69\u5b9a\u6027\u65b9\u9762\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u7279\u5b9a\u65bc\u5834\u666f\u7684\u8abf\u6574\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u91dd\u5c0d\u5be6\u969b\u696d\u52d9\u5834\u666f\u91cf\u8eab\u6253\u9020\u7684\u51fd\u5f0f\u547c\u53eb\u80fd\u529b\u8a13\u7df4\u7ba1\u9053\u3002\u6b64\u7ba1\u9053\u5305\u62ec\u5408\u6210\u548c\u64f4\u5145\u7279\u5b9a\u65bc\u5834\u666f\u7684\u51fd\u5f0f\u547c\u53eb\u8cc7\u6599\u3001\u6a21\u578b\u5fae\u8abf\uff0c\u4ee5\u53ca\u6548\u80fd\u8a55\u4f30\u548c\u5206\u6790\u3002\u4f7f\u7528\u9019\u500b\u7ba1\u9053\uff0c\u6211\u5011\u5728\u6578\u4f4d\u4eba\u529b\u8cc7\u6e90\u4ee3\u7406\u5834\u666f\u4e2d\u751f\u6210\u4e86 1,260 \u500b\u5b8c\u5168\u7531 AI \u751f\u6210\u7684\u7bc4\u4f8b\u548c 1,035 \u500b\u64f4\u5145\u7684\u624b\u52d5\u6a19\u8a18\u7bc4\u4f8b\u3002Qwen2.5-Coder-7B-Instruct \u6a21\u578b\u88ab\u7528\u4f5c\u57fa\u790e\u6a21\u578b\uff0c\u4e26\u4f7f\u7528 LoRA \u65b9\u6cd5\u5728\u56db\u500b\u5177\u6709 24GB VRAM \u7684 GPU \u4e0a\u9032\u884c\u5fae\u8abf\u3002\u6211\u5011\u5fae\u8abf\u5f8c\u7684\u6a21\u578b\u5728\u8a55\u4f30\u548c\u5be6\u969b\u61c9\u7528\u4e2d\u5c55\u73fe\u51fa\u5091\u51fa\u7684\u6548\u80fd\uff0c\u5728\u6e2c\u8a66\u96c6\u4e0a\u7684\u6e96\u78ba\u5ea6\u8d85\u8d8a\u4e86 GPT-4 \u548c GPT-4o\u3002\u9019\u4e9b\u7d50\u679c\u9a57\u8b49\u4e86\u6240\u63d0\u51fa\u7684\u7ba1\u9053\u5c0d\u65bc\u8a13\u7df4\u7279\u5b9a\u65bc\u5834\u666f\u7684\u51fd\u5f0f\u547c\u53eb\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "author": "Guancheng Zeng et.al.", "authors": "Guancheng Zeng, Wentao Ding, Beining Xu, Chi Zhang, Wenqiang Han, Gang Li, Jingjing Mo, Pengxu Qiu, Xinran Tao, Wang Tao, Haowen Hu", "id": "2412.15660v1", "paper_url": "http://arxiv.org/abs/2412.15660v1", "repo": "null"}}