{"2406.07081": {"publish_time": "2024-06-11", "title": "Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning", "paper_summary": "Large language models (LLMs) exhibit outstanding performance in machine\ntranslation via in-context learning. In contrast to sentence-level translation,\ndocument-level translation (DOCMT) by LLMs based on in-context learning faces\ntwo major challenges: firstly, document translations generated by LLMs are\noften incoherent; secondly, the length of demonstration for in-context learning\nis usually limited. To address these issues, we propose a Context-Aware\nPrompting method (CAP), which enables LLMs to generate more accurate, cohesive,\nand coherent translations via in-context learning. CAP takes into account\nmulti-level attention, selects the most relevant sentences to the current one\nas context, and then generates a summary from these collected sentences.\nSubsequently, sentences most similar to the summary are retrieved from the\ndatastore as demonstrations, which effectively guide LLMs in generating\ncohesive and coherent translations. We conduct extensive experiments across\nvarious DOCMT tasks, and the results demonstrate the effectiveness of our\napproach, particularly in zero pronoun translation (ZPT) and literary\ntranslation tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6a5f\u5668\u7ffb\u8b6f\u4e2d\u900f\u904e\u60c5\u5883\u5b78\u7fd2\u5c55\u73fe\u51fa\u5091\u51fa\u7684\u8868\u73fe\u3002\u8207\u53e5\u5b50\u5c64\u7d1a\u7ffb\u8b6f\u76f8\u6bd4\uff0cLLM \u57fa\u65bc\u60c5\u5883\u5b78\u7fd2\u7684\u6587\u4ef6\u5c64\u7d1a\u7ffb\u8b6f (DOCMT) \u9762\u81e8\u5169\u9805\u4e3b\u8981\u6311\u6230\uff1a\u9996\u5148\uff0cLLM \u751f\u6210\u7684\u6587\u4ef6\u7ffb\u8b6f\u5e38\u5e38\u4e0d\u9023\u8cab\uff1b\u5176\u6b21\uff0c\u60c5\u5883\u5b78\u7fd2\u7684\u793a\u7bc4\u9577\u5ea6\u901a\u5e38\u6709\u9650\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u60c5\u5883\u611f\u77e5\u63d0\u793a\u65b9\u6cd5 (CAP)\uff0c\u5b83\u80fd\u8b93 LLM \u900f\u904e\u60c5\u5883\u5b78\u7fd2\u7522\u751f\u66f4\u6e96\u78ba\u3001\u6709\u51dd\u805a\u529b\u4e14\u9023\u8cab\u7684\u7ffb\u8b6f\u3002CAP \u8003\u91cf\u4e86\u591a\u5c64\u7d1a\u6ce8\u610f\u529b\uff0c\u9078\u64c7\u8207\u7576\u524d\u53e5\u5b50\u6700\u76f8\u95dc\u7684\u53e5\u5b50\u4f5c\u70ba\u60c5\u5883\uff0c\u7136\u5f8c\u5f9e\u9019\u4e9b\u6536\u96c6\u7684\u53e5\u5b50\u7522\u751f\u6458\u8981\u3002\u96a8\u5f8c\uff0c\u5f9e\u8cc7\u6599\u5eab\u4e2d\u64f7\u53d6\u8207\u6458\u8981\u6700\u76f8\u4f3c\u7684\u53e5\u5b50\u4f5c\u70ba\u793a\u7bc4\uff0c\u6709\u6548\u5f15\u5c0e LLM \u7522\u751f\u6709\u51dd\u805a\u529b\u4e14\u9023\u8cab\u7684\u7ffb\u8b6f\u3002\u6211\u5011\u5728\u5404\u7a2e DOCMT \u4efb\u52d9\u4e2d\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u5225\u662f\u5728\u96f6\u4ee3\u540d\u8a5e\u7ffb\u8b6f (ZPT) \u548c\u6587\u5b78\u7ffb\u8b6f\u4efb\u52d9\u4e2d\u3002", "author": "Menglong Cui et.al.", "authors": "Menglong Cui, Jiangcun Du, Shaolin Zhu, Deyi Xiong", "id": "2406.07081v1", "paper_url": "http://arxiv.org/abs/2406.07081v1", "repo": "null"}}