{"2502.21309": {"publish_time": "2025-02-28", "title": "FANformer: Improving Large Language Models Through Effective Periodicity Modeling", "paper_summary": "Periodicity, as one of the most important basic characteristics, lays the\nfoundation for facilitating structured knowledge acquisition and systematic\ncognitive processes within human learning paradigms. However, the potential\nflaws of periodicity modeling in Transformer affect the learning efficiency and\nestablishment of underlying principles from data for large language models\n(LLMs) built upon it. In this paper, we demonstrate that integrating effective\nperiodicity modeling can improve the learning efficiency and performance of\nLLMs. We introduce FANformer, which integrates Fourier Analysis Network (FAN)\ninto attention mechanism to achieve efficient periodicity modeling, by\nmodifying the feature projection process of attention mechanism. Extensive\nexperimental results on language modeling show that FANformer consistently\noutperforms Transformer when scaling up model size and training tokens,\nunderscoring its superior learning efficiency. To further validate the\neffectiveness of FANformer, we pretrain a FANformer-1B on 1 trillion tokens.\nFANformer-1B exhibits marked improvements on downstream tasks compared to\nopen-source LLMs with similar model parameters or training tokens. The results\nposition FANformer as an effective and promising architecture for advancing\nLLMs.", "paper_summary_zh": "<paragraph>\u9031\u671f\u6027\u4f5c\u70ba\u6700\u91cd\u8981\u7684\u57fa\u672c\u7279\u5fb5\u4e4b\u4e00\uff0c\u70ba\u4eba\u985e\u5b78\u7fd2\u7bc4\u5f0f\u4e2d\u7684\u7d50\u69cb\u5316\u77e5\u8b58\u7372\u53d6\u548c\u7cfb\u7d71\u8a8d\u77e5\u904e\u7a0b\u5960\u5b9a\u4e86\u57fa\u790e\u3002\u7136\u800c\uff0cTransformer \u4e2d\u9031\u671f\u6027\u5efa\u6a21\u7684\u6f5b\u5728\u7f3a\u9677\u6703\u5f71\u97ff\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5b78\u7fd2\u6548\u7387\u548c\u5f9e\u6578\u64da\u4e2d\u5efa\u7acb\u57fa\u672c\u539f\u5247\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6574\u5408\u6709\u6548\u7684\u9031\u671f\u6027\u5efa\u6a21\u53ef\u4ee5\u63d0\u9ad8 LLM \u7684\u5b78\u7fd2\u6548\u7387\u548c\u6027\u80fd\u3002\u6211\u5011\u5f15\u5165\u4e86 FANformer\uff0c\u5b83\u5c07\u5085\u7acb\u8449\u5206\u6790\u7db2\u7d61 (FAN) \u6574\u5408\u5230\u6ce8\u610f\u529b\u6a5f\u5236\u4e2d\uff0c\u901a\u904e\u4fee\u6539\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u7279\u5fb5\u6295\u5f71\u904e\u7a0b\u4f86\u5be6\u73fe\u9ad8\u6548\u7684\u9031\u671f\u6027\u5efa\u6a21\u3002\u5927\u91cf\u7684\u8a9e\u8a00\u5efa\u6a21\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u7576\u64f4\u5c55\u6a21\u578b\u5927\u5c0f\u548c\u8a13\u7df4token\u6642\uff0cFANformer \u7684\u8868\u73fe\u4e00\u76f4\u512a\u65bc Transformer\uff0c\u7a81\u986f\u4e86\u5176\u5353\u8d8a\u7684\u5b78\u7fd2\u6548\u7387\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u9a57\u8b49 FANformer \u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5728 1 \u5146\u500btoken\u4e0a\u9810\u8a13\u7df4\u4e86\u4e00\u500b FANformer-1B \u6a21\u578b\u3002\u8207\u5177\u6709\u76f8\u4f3c\u6a21\u578b\u53c3\u6578\u6216\u8a13\u7df4token\u7684\u958b\u6e90 LLM \u76f8\u6bd4\uff0cFANformer-1B \u5728\u4e0b\u6e38\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u986f\u8457\u7684\u6539\u9032\u3002\u9019\u4e9b\u7d50\u679c\u8868\u660e\uff0cFANformer \u662f\u4e00\u7a2e\u6709\u6548\u4e14\u6709\u524d\u666f\u7684\u67b6\u69cb\uff0c\u53ef\u4ee5\u63a8\u9032 LLM \u7684\u767c\u5c55\u3002</paragraph>\n", "author": "Yihong Dong et.al.", "authors": "Yihong Dong, Ge Li, Xue Jiang, Yongding Tao, Kechi Zhang, Hao Zhu, Huanyu Liu, Jiazheng Ding, Jia Li, Jinliang Deng, Hong Mei", "id": "2502.21309v1", "paper_url": "http://arxiv.org/abs/2502.21309v1", "repo": "null"}}