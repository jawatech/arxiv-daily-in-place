{"2411.08521": {"publish_time": "2024-11-13", "title": "SAD-TIME: a Spatiotemporal-fused network for depression detection with Automated multi-scale Depth-wise and TIME-interval-related common feature extractor", "paper_summary": "Background and Objective: Depression is a severe mental disorder, and\naccurate diagnosis is pivotal to the cure and rehabilitation of people with\ndepression. However, the current questionnaire-based diagnostic methods could\nbring subjective biases and may be denied by subjects. In search of a more\nobjective means of diagnosis, researchers have begun to experiment with deep\nlearning-based methods for identifying depressive disorders in recent years.\nMethods: In this study, a novel Spatiotemporal-fused network with Automated\nmulti-scale Depth-wise and TIME-interval-related common feature extractor\n(SAD-TIME) is proposed. SAD-TIME incorporates an automated nodes' common\nfeatures extractor (CFE), a spatial sector (SpS), a modified temporal sector\n(TeS), and a domain adversarial learner (DAL). The CFE includes a multi-scale\ndepth-wise 1D-convolutional neural network and a time-interval embedding\ngenerator, where the unique information of each channel is preserved. The SpS\nfuses the functional connectivity with the distance-based connectivity\ncontaining spatial position of EEG electrodes. A multi-head-attention graph\nconvolutional network is also applied in the SpS to fuse the features from\ndifferent EEG channels. The TeS is based on long short-term memory and graph\ntransformer networks, where the temporal information of different time-windows\nis fused. Moreover, the DAL is used after the SpS to obtain the\ndomain-invariant feature. Results: Experimental results under tenfold\ncross-validation show that the proposed SAD-TIME method achieves 92.00% and\n94.00% depression classification accuracies on two datasets, respectively, in\ncross-subject mode. Conclusion: SAD-TIME is a robust depression detection\nmodel, where the automatedly-generated features, the SpS and the TeS assist the\nclassification performance with the fusion of the innate spatiotemporal\ninformation in the EEG signals.", "paper_summary_zh": "<paragraph>\u80cc\u666f\u8207\u76ee\u6a19\uff1a\u6182\u9b31\u75c7\u662f\u4e00\u7a2e\u56b4\u91cd\u7684\u7cbe\u795e\u75be\u75c5\uff0c\u800c\u6e96\u78ba\u7684\u8a3a\u65b7\u5c0d\u65bc\u6182\u9b31\u75c7\u60a3\u8005\u7684\u6cbb\u7642\u548c\u5fa9\u5065\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u57fa\u65bc\u554f\u5377\u7684\u8a3a\u65b7\u65b9\u6cd5\u53ef\u80fd\u6703\u5e36\u4f86\u4e3b\u89c0\u504f\u8aa4\uff0c\u4e14\u53ef\u80fd\u906d\u5230\u53d7\u8a66\u8005\u5426\u8a8d\u3002\u70ba\u4e86\u5c0b\u6c42\u66f4\u5ba2\u89c0\u7684\u8a3a\u65b7\u65b9\u5f0f\uff0c\u7814\u7a76\u4eba\u54e1\u8fd1\u5e74\u4f86\u958b\u59cb\u5617\u8a66\u4f7f\u7528\u57fa\u65bc\u6df1\u5ea6\u5b78\u7fd2\u7684\u65b9\u6cd5\u4f86\u8b58\u5225\u6182\u9b31\u75c7\u3002\u65b9\u6cd5\uff1a\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u500b\u7d50\u5408\u81ea\u52d5\u5316\u591a\u5c3a\u5ea6\u6df1\u5ea6\u548c\u6642\u9593\u9593\u9694\u76f8\u95dc\u5171\u7528\u7279\u5fb5\u8403\u53d6\u5668\u7684\u6642\u7a7a\u878d\u5408\u7db2\u8def\uff08SAD-TIME\uff09\u3002SAD-TIME \u6574\u5408\u4e86\u4e00\u500b\u81ea\u52d5\u5316\u7bc0\u9ede\u5171\u7528\u7279\u5fb5\u8403\u53d6\u5668\uff08CFE\uff09\u3001\u4e00\u500b\u7a7a\u9593\u5340\u584a\uff08SpS\uff09\u3001\u4e00\u500b\u4fee\u6539\u904e\u7684\u6642\u9593\u5340\u584a\uff08TeS\uff09\u548c\u4e00\u500b\u9818\u57df\u5c0d\u6297\u5b78\u7fd2\u5668\uff08DAL\uff09\u3002CFE \u5305\u542b\u4e00\u500b\u591a\u5c3a\u5ea6\u6df1\u5ea6 1D \u6372\u7a4d\u795e\u7d93\u7db2\u8def\u548c\u4e00\u500b\u6642\u9593\u9593\u9694\u5d4c\u5165\u7522\u751f\u5668\uff0c\u5176\u4e2d\u6bcf\u500b\u901a\u9053\u7684\u7368\u7279\u8cc7\u8a0a\u90fd\u88ab\u4fdd\u7559\u4e0b\u4f86\u3002SpS \u5c07\u529f\u80fd\u9023\u63a5\u8207\u5305\u542b\u8166\u96fb\u5716\u96fb\u6975\u7a7a\u9593\u4f4d\u7f6e\u7684\u57fa\u65bc\u8ddd\u96e2\u9023\u63a5\u878d\u5408\u5728\u4e00\u8d77\u3002\u4e00\u500b\u591a\u982d\u6ce8\u610f\u529b\u5716\u5f62\u5377\u7a4d\u7db2\u8def\u4e5f\u61c9\u7528\u65bc SpS \u4e2d\uff0c\u4ee5\u878d\u5408\u4f86\u81ea\u4e0d\u540c\u8166\u96fb\u5716\u901a\u9053\u7684\u7279\u5fb5\u3002TeS \u57fa\u65bc\u9577\u671f\u77ed\u671f\u8a18\u61b6\u548c\u5716\u5f62Transformer\u7db2\u8def\uff0c\u5176\u4e2d\u4e0d\u540c\u6642\u9593\u7a97\u7684\u6642\u9593\u8cc7\u8a0a\u88ab\u878d\u5408\u5728\u4e00\u8d77\u3002\u6b64\u5916\uff0cDAL \u5728 SpS \u4e4b\u5f8c\u88ab\u7528\u65bc\u7372\u53d6\u9818\u57df\u4e0d\u8b8a\u7279\u5fb5\u3002\u7d50\u679c\uff1a\u5728\u5341\u500d\u4ea4\u53c9\u9a57\u8b49\u4e0b\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6240\u63d0\u51fa\u7684 SAD-TIME \u65b9\u6cd5\u5728\u8de8\u4e3b\u9ad4\u6a21\u5f0f\u4e0b\u5206\u5225\u5728\u5169\u500b\u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86 92.00% \u548c 94.00% \u7684\u6182\u9b31\u75c7\u5206\u985e\u6e96\u78ba\u5ea6\u3002\u7d50\u8ad6\uff1aSAD-TIME \u662f\u4e00\u500b\u5f37\u5927\u7684\u6182\u9b31\u75c7\u6aa2\u6e2c\u6a21\u578b\uff0c\u5176\u4e2d\u81ea\u52d5\u7522\u751f\u7684\u7279\u5fb5\u3001SpS \u548c TeS \u900f\u904e\u878d\u5408\u8166\u96fb\u5716\u8a0a\u865f\u4e2d\u56fa\u6709\u7684\u6642\u7a7a\u8cc7\u8a0a\u4f86\u5354\u52a9\u5206\u985e\u8868\u73fe\u3002</paragraph>", "author": "Han-Guang Wang et.al.", "authors": "Han-Guang Wang, Hui-Rang Hou, Li-Cheng Jin, Chen-Yang Xu, Zhong-Yi Zhang, Qing-Hao Meng", "id": "2411.08521v1", "paper_url": "http://arxiv.org/abs/2411.08521v1", "repo": "null"}}