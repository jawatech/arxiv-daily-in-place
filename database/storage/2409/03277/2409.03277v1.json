{"2409.03277": {"publish_time": "2024-09-05", "title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding", "paper_summary": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.", "paper_summary_zh": "\u81ea\u52d5\u5716\u8868\u7406\u89e3\u5c0d\u65bc\u5167\u5bb9\u7406\u89e3\u548c\u6587\u4ef6\u89e3\u6790\u81f3\u95dc\u91cd\u8981\u3002\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5df2\u900f\u904e\u7279\u5b9a\u9818\u57df\u7684\u5c0d\u9f4a\u548c\u5fae\u8abf\uff0c\u5728\u5716\u8868\u7406\u89e3\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5c0d\u9f4a\u8a13\u7df4\u5728\u5716\u8868\u9818\u57df\u7684\u61c9\u7528\u4ecd\u8655\u65bc\u63a2\u7d22\u4e0d\u8db3\u7684\u968e\u6bb5\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 ChartMoE\uff0c\u5b83\u63a1\u7528\u5c08\u5bb6\u6df7\u5408 (MoE) \u67b6\u69cb\u4f86\u53d6\u4ee3\u50b3\u7d71\u7684\u7dda\u6027\u6295\u5f71\uff0c\u4ee5\u5f4c\u5408\u6a21\u614b\u5dee\u8ddd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u900f\u904e\u4e0d\u540c\u7684\u5c0d\u9f4a\u4efb\u52d9\u4f86\u8a13\u7df4\u591a\u500b\u7dda\u6027\u9023\u63a5\u5668\uff0c\u9019\u4e9b\u9023\u63a5\u5668\u88ab\u7528\u4f5c\u4e0d\u540c\u5c08\u5bb6\u7684\u57fa\u790e\u521d\u59cb\u5316\u53c3\u6578\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86 ChartMoE-Align\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b\u8d85\u904e 90 \u842c\u500b\u5716\u8868-\u8868\u683c-JSON-\u7a0b\u5f0f\u78bc\u56db\u5143\u7d44\u7684\u8cc7\u6599\u96c6\uff0c\u7528\u65bc\u57f7\u884c\u4e09\u9805\u5c0d\u9f4a\u4efb\u52d9\uff08\u5716\u8868-\u8868\u683c/JSON/\u7a0b\u5f0f\u78bc\uff09\u3002\u7d50\u5408\u9999\u8349\u9023\u63a5\u5668\uff0c\u6211\u5011\u4ee5\u56db\u7a2e\u4e0d\u540c\u7684\u65b9\u5f0f\u521d\u59cb\u5316\u4e0d\u540c\u7684\u5c08\u5bb6\uff0c\u4e26\u63a1\u7528\u9ad8\u54c1\u8cea\u7684\u77e5\u8b58\u5b78\u7fd2\u4f86\u9032\u4e00\u6b65\u512a\u5316 MoE \u9023\u63a5\u5668\u548c LLM \u53c3\u6578\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86 MoE \u9023\u63a5\u5668\u548c\u6211\u5011\u7684\u521d\u59cb\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u4f8b\u5982\uff0cChartMoE \u5c07 ChartQA \u57fa\u6e96\u4e0a\u5148\u524d\u6700\u5148\u9032\u7684\u6280\u8853\u7684\u6e96\u78ba\u5ea6\u5f9e 80.48% \u63d0\u5347\u5230 84.64%\u3002", "author": "Zhengzhuo Xu et.al.", "authors": "Zhengzhuo Xu, Bowen Qu, Yiyan Qi, Sinan Du, Chengjin Xu, Chun Yuan, Jian Guo", "id": "2409.03277v1", "paper_url": "http://arxiv.org/abs/2409.03277v1", "repo": "null"}}