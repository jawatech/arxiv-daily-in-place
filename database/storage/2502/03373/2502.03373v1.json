{"2502.03373": {"publish_time": "2025-02-05", "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs", "paper_summary": "Scaling inference compute enhances reasoning in large language models (LLMs),\nwith long chains-of-thought (CoTs) enabling strategies like backtracking and\nerror correction. Reinforcement learning (RL) has emerged as a crucial method\nfor developing these capabilities, yet the conditions under which long CoTs\nemerge remain unclear, and RL training requires careful design choices. In this\nstudy, we systematically investigate the mechanics of long CoT reasoning,\nidentifying the key factors that enable models to generate long CoT\ntrajectories. Through extensive supervised fine-tuning (SFT) and RL\nexperiments, we present four main findings: (1) While SFT is not strictly\nnecessary, it simplifies training and improves efficiency; (2) Reasoning\ncapabilities tend to emerge with increased training compute, but their\ndevelopment is not guaranteed, making reward shaping crucial for stabilizing\nCoT length growth; (3) Scaling verifiable reward signals is critical for RL. We\nfind that leveraging noisy, web-extracted solutions with filtering mechanisms\nshows strong potential, particularly for out-of-distribution (OOD) tasks such\nas STEM reasoning; and (4) Core abilities like error correction are inherently\npresent in base models, but incentivizing these skills effectively for complex\ntasks via RL demands significant compute, and measuring their emergence\nrequires a nuanced approach. These insights provide practical guidance for\noptimizing training strategies to enhance long CoT reasoning in LLMs. Our code\nis available at: https://github.com/eddycmu/demystify-long-cot.", "paper_summary_zh": "<paragraph>\u64f4\u5c55\u63a8\u7406\u904b\u7b97\u80fd\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u63a8\u7406\uff0c\n\u5177\u5099\u9577\u601d\u7dad\u93c8 (CoT) \u80fd\u555f\u7528\u56de\u6eaf\u548c\n\u932f\u8aa4\u4fee\u6b63\u7b49\u7b56\u7565\u3002\u5f37\u5316\u5b78\u7fd2 (RL) \u5df2\u6210\u70ba\u958b\u767c\u9019\u4e9b\u80fd\u529b\u7684\u4e00\u7a2e\u91cd\u8981\u65b9\u6cd5\uff0c\n\u4f46\u9577 CoT \u6d6e\u73fe\u7684\u689d\u4ef6\u4ecd\u4e0d\u6e05\u6670\uff0c\u800c RL \u8a13\u7df4\u9700\u8981\u8b39\u614e\u7684\u8a2d\u8a08\u9078\u64c7\u3002\u5728\u9019\u500b\n\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u9577 CoT \u63a8\u7406\u7684\u6a5f\u5236\uff0c\n\u627e\u51fa\u8b93\u6a21\u578b\u7522\u751f\u9577 CoT \u8ecc\u8de1\u7684\u4e3b\u8981\u56e0\u7d20\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u76e3\u7763\u5fae\u8abf (SFT) \u548c RL\n\u5be6\u9a57\uff0c\u6211\u5011\u63d0\u51fa\u56db\u9805\u4e3b\u8981\u767c\u73fe\uff1a(1) \u96d6\u7136 SFT \u4e26\u975e\u7d55\u5c0d\u5fc5\u8981\uff0c\u4f46\u5b83\u7c21\u5316\u4e86\u8a13\u7df4\u4e26\u63d0\u5347\u6548\u7387\uff1b(2) \u63a8\u7406\n\u80fd\u529b\u50be\u5411\u65bc\u96a8\u8457\u8a13\u7df4\u904b\u7b97\u589e\u52a0\u800c\u6d6e\u73fe\uff0c\u4f46\u5176\u767c\u5c55\u4e26\u975e\u6709\u4fdd\u8b49\uff0c\u56e0\u6b64\u734e\u52f5\u5851\u9020\u5c0d\u65bc\u7a69\u5b9a\nCoT \u9577\u5ea6\u589e\u9577\u81f3\u95dc\u91cd\u8981\uff1b(3) \u64f4\u5c55\u53ef\u9a57\u8b49\u7684\u734e\u52f5\u8a0a\u865f\u5c0d\u65bc RL \u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\n\u767c\u73fe\u5229\u7528\u5e36\u6709\u904e\u6ffe\u6a5f\u5236\u7684 noisy \u7db2\u8def\u63d0\u53d6\u89e3\u6c7a\u65b9\u6848\u986f\u793a\u51fa\u5f37\u5927\u7684\u6f5b\u529b\uff0c\u7279\u5225\u662f\u5c0d\u65bc STEM \u63a8\u7406\u7b49\n\u975e\u5206\u4f48 (OOD) \u4efb\u52d9\uff1b(4) \u932f\u8aa4\u4fee\u6b63\u7b49\u6838\u5fc3\u80fd\u529b\u672c\u8cea\u4e0a\u5b58\u5728\u65bc\u57fa\u790e\u6a21\u578b\u4e2d\uff0c\u4f46\u900f\u904e RL \u6709\u6548\u5730\u6fc0\u52f5\u9019\u4e9b\u6280\u80fd\u4ee5\u57f7\u884c\u8907\u96dc\n\u4efb\u52d9\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\uff0c\u800c\u8861\u91cf\u5b83\u5011\u7684\u6d6e\u73fe\u9700\u8981\u4e00\u7a2e\u7d30\u7dfb\u7684\u65b9\u6cd5\u3002\u9019\u4e9b\u898b\u89e3\u70ba\u6700\u4f73\u5316\u8a13\u7df4\u7b56\u7565\u4ee5\u589e\u5f37 LLM \u4e2d\u7684\u9577 CoT \u63a8\u7406\u63d0\u4f9b\u4e86\u5be6\u7528\u7684\u6307\u5c0e\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1ahttps://github.com/eddycmu/demystify-long-cot\u3002</paragraph>", "author": "Edward Yeo et.al.", "authors": "Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue", "id": "2502.03373v1", "paper_url": "http://arxiv.org/abs/2502.03373v1", "repo": "null"}}