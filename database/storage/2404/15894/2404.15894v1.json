{"2404.15894": {"publish_time": "2024-04-24", "title": "Assessing The Potential Of Mid-Sized Language Models For Clinical QA", "paper_summary": "Large language models, such as GPT-4 and Med-PaLM, have shown impressive\nperformance on clinical tasks; however, they require access to compute, are\nclosed-source, and cannot be deployed on device. Mid-size models such as\nBioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B avoid these drawbacks, but\ntheir capacity for clinical tasks has been understudied. To help assess their\npotential for clinical use and help researchers decide which model they should\nuse, we compare their performance on two clinical question-answering (QA)\ntasks: MedQA and consumer query answering. We find that Mistral 7B is the best\nperforming model, winning on all benchmarks and outperforming models trained\nspecifically for the biomedical domain. While Mistral 7B's MedQA score of 63.0%\napproaches the original Med-PaLM, and it often can produce plausible responses\nto consumer health queries, room for improvement still exists. This study\nprovides the first head-to-head assessment of open source mid-sized models on\nclinical tasks.", "paper_summary_zh": "", "author": "Elliot Bolton et.al.", "authors": "Elliot Bolton,Betty Xiong,Vijaytha Muralidharan,Joel Schamroth,Vivek Muralidharan,Christopher D. Manning,Roxana Daneshjou", "id": "2404.15894v1", "paper_url": "http://arxiv.org/abs/2404.15894v1", "repo": "null"}}