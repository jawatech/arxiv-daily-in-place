{"2407.14133": {"publish_time": "2024-07-19", "title": "I Know About \"Up\"! Enhancing Spatial Reasoning in Visual Language Models Through 3D Reconstruction", "paper_summary": "Visual Language Models (VLMs) are essential for various tasks, particularly\nvisual reasoning tasks, due to their robust multi-modal information\nintegration, visual reasoning capabilities, and contextual awareness. However,\nexisting \\VLMs{}' visual spatial reasoning capabilities are often inadequate,\nstruggling even with basic tasks such as distinguishing left from right. To\naddress this, we propose the \\ours{} model, designed to enhance the visual\nspatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D\nreconstruction model for obtaining different views of the input images and\nincorporates a prompting mechanism to further improve visual spatial reasoning.\nExperimental results on four visual spatial reasoning datasets show that our\n\\ours{} achieves up to 19.48% accuracy improvement, which indicates the\neffectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7531\u65bc\u5176\u5f37\u5927\u7684\u591a\u6a21\u5f0f\u8cc7\u8a0a\u6574\u5408\u3001\u8996\u89ba\u63a8\u7406\u80fd\u529b\u548c\u60c5\u5883\u611f\u77e5\uff0c\u5c0d\u65bc\u5404\u7a2e\u4efb\u52d9\u4f86\u8aaa\u81f3\u95dc\u91cd\u8981\uff0c\u7279\u5225\u662f\u8996\u89ba\u63a8\u7406\u4efb\u52d9\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 \\VLMs{} \u7684\u8996\u89ba\u7a7a\u9593\u63a8\u7406\u80fd\u529b\u901a\u5e38\u4e0d\u8db3\uff0c\u751a\u81f3\u96e3\u4ee5\u61c9\u4ed8\u5340\u5206\u5de6\u53f3\u7b49\u57fa\u672c\u4efb\u52d9\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 \\ours{} \u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f37 VLM \u7684\u8996\u89ba\u7a7a\u9593\u63a8\u7406\u80fd\u529b\u3002ZeroVLM \u63a1\u7528 Zero-1-to-3\uff0c\u9019\u662f\u4e00\u500b 3D \u91cd\u5efa\u6a21\u578b\uff0c\u7528\u65bc\u53d6\u5f97\u8f38\u5165\u5f71\u50cf\u7684\u4e0d\u540c\u8996\u89d2\uff0c\u4e26\u7d50\u5408\u63d0\u793a\u6a5f\u5236\u9032\u4e00\u6b65\u6539\u5584\u8996\u89ba\u7a7a\u9593\u63a8\u7406\u3002\u5728\u56db\u500b\u8996\u89ba\u7a7a\u9593\u63a8\u7406\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6211\u5011\u7684 \\ours{} \u7684\u6e96\u78ba\u7387\u63d0\u9ad8\u4e86 19.48%\uff0c\u9019\u8868\u793a\u6211\u5011 ZeroVLM \u7684 3D \u91cd\u5efa\u548c\u63d0\u793a\u6a5f\u5236\u662f\u6709\u6548\u7684\u3002", "author": "Zaiqiao Meng et.al.", "authors": "Zaiqiao Meng, Hao Zhou, Yifang Chen", "id": "2407.14133v1", "paper_url": "http://arxiv.org/abs/2407.14133v1", "repo": "null"}}