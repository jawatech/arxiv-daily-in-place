{"2501.08617": {"publish_time": "2025-01-15", "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation", "paper_summary": "Generative AI systems like foundation models (FMs) must align well with human\nvalues to ensure their behavior is helpful and trustworthy. While Reinforcement\nLearning from Human Feedback (RLHF) has shown promise for optimizing model\nperformance using human judgments, existing RLHF pipelines predominantly rely\non immediate feedback, which can fail to accurately reflect the downstream\nimpact of an interaction on users' utility. We demonstrate that feedback based\non evaluators' foresight estimates of downstream consequences systematically\ninduces Goodhart's Law dynamics, incentivizing misaligned behaviors like\nsycophancy and deception and ultimately degrading user outcomes. To alleviate\nthis, we propose decoupling evaluation from prediction by refocusing RLHF on\nhindsight feedback. Our theoretical analysis reveals that conditioning\nevaluator feedback on downstream observations mitigates misalignment and\nimproves expected human utility, even when these observations are simulated by\nthe AI system itself. To leverage this insight in a practical alignment\nalgorithm, we introduce Reinforcement Learning from Hindsight Simulation\n(RLHS), which first simulates plausible consequences and then elicits feedback\nto assess what behaviors were genuinely beneficial in hindsight. We apply RLHS\nto two widely-employed online and offline preference optimization methods --\nProximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) --\nand show empirically that misalignment is significantly reduced with both\nmethods. Through an online human user study, we show that RLHS consistently\noutperforms RLHF in helping users achieve their goals and earns higher\nsatisfaction ratings, despite being trained solely with simulated hindsight\nfeedback. These results underscore the importance of focusing on long-term\nconsequences, even simulated ones, to mitigate misalignment in RLHF.", "paper_summary_zh": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7cfb\u7d71\uff0c\u5982\u57fa\u790e\u6a21\u578b (FM)\uff0c\u5fc5\u9808\u8207\u4eba\u985e\u50f9\u503c\u89c0\u4fdd\u6301\u4e00\u81f4\uff0c\u4ee5\u78ba\u4fdd\u5176\u884c\u70ba\u662f\u6709\u5e6b\u52a9\u4e14\u503c\u5f97\u4fe1\u8cf4\u7684\u3002\u96d6\u7136\u900f\u904e\u4eba\u985e\u56de\u994b\u9032\u884c\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5df2\u986f\u793a\u51fa\u5229\u7528\u4eba\u985e\u5224\u65b7\u4f86\u6700\u4f73\u5316\u6a21\u578b\u6548\u80fd\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u73fe\u6709\u7684 RLHF \u7ba1\u7dda\u4e3b\u8981\u4f9d\u8cf4\u5373\u6642\u56de\u994b\uff0c\u9019\u53ef\u80fd\u7121\u6cd5\u6e96\u78ba\u53cd\u6620\u4e92\u52d5\u5c0d\u4f7f\u7528\u8005\u6548\u7528\u7684\u4e0b\u6e38\u5f71\u97ff\u3002\u6211\u5011\u8b49\u660e\uff0c\u57fa\u65bc\u8a55\u4f30\u8005\u5c0d\u4e0b\u6e38\u5f8c\u679c\u7684\u524d\u77bb\u6027\u4f30\u8a08\u7684\u56de\u994b\u7cfb\u7d71\u6027\u5730\u8a98\u767c\u53e4\u5fb7\u54c8\u7279\u5b9a\u5f8b\u52d5\u614b\uff0c\u9f13\u52f5\u62cd\u99ac\u5c41\u548c\u6b3a\u9a19\u7b49\u4e0d\u4e00\u81f4\u7684\u884c\u70ba\uff0c\u4e26\u6700\u7d42\u964d\u4f4e\u4f7f\u7528\u8005\u6210\u679c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u900f\u904e\u91cd\u65b0\u95dc\u6ce8 RLHF \u5728\u4e8b\u5f8c\u56de\u994b\u4e0a\u4f86\u89e3\u9664\u8a55\u4f30\u8207\u9810\u6e2c\u7684\u95dc\u806f\u3002\u6211\u5011\u7684\u7406\u8ad6\u5206\u6790\u8868\u660e\uff0c\u5728\u8a55\u4f30\u8005\u56de\u994b\u4e2d\u52a0\u5165\u4e0b\u6e38\u89c0\u5bdf\u7d50\u679c\u53ef\u4ee5\u6e1b\u8f15\u4e0d\u4e00\u81f4\u6027\uff0c\u4e26\u63d0\u9ad8\u9810\u671f\u7684\u4f7f\u7528\u8005\u6548\u7528\uff0c\u5373\u4f7f\u9019\u4e9b\u89c0\u5bdf\u7d50\u679c\u662f\u7531 AI \u7cfb\u7d71\u672c\u8eab\u6a21\u64ec\u7684\u3002\u70ba\u4e86\u5728\u5be6\u969b\u5c0d\u9f4a\u6f14\u7b97\u6cd5\u4e2d\u904b\u7528\u9019\u500b\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e8b\u5f8c\u6a21\u64ec\u5f37\u5316\u5b78\u7fd2 (RLHS)\uff0c\u5b83\u6703\u5148\u6a21\u64ec\u5408\u7406\u7684\u5f8c\u679c\uff0c\u7136\u5f8c\u5f15\u767c\u56de\u994b\uff0c\u4ee5\u8a55\u4f30\u54ea\u4e9b\u884c\u70ba\u5728\u4e8b\u5f8c\u771f\u7684\u6709\u76ca\u3002\u6211\u5011\u5c07 RLHS \u61c9\u7528\u65bc\u5169\u7a2e\u5ee3\u6cdb\u4f7f\u7528\u7684\u7dda\u4e0a\u548c\u96e2\u7dda\u504f\u597d\u6700\u4f73\u5316\u65b9\u6cd5\u2014\u2014\u8fd1\u7aef\u7b56\u7565\u6700\u4f73\u5316 (PPO) \u548c\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\u2014\u2014\u4e26\u900f\u904e\u5be6\u8b49\u986f\u793a\uff0c\u9019\u5169\u7a2e\u65b9\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\u90fd\u986f\u8457\u964d\u4f4e\u3002\u900f\u904e\u4e00\u9805\u7dda\u4e0a\u4eba\u985e\u4f7f\u7528\u8005\u7814\u7a76\uff0c\u6211\u5011\u8b49\u660e\u4e86 RLHS \u5728\u5e6b\u52a9\u4f7f\u7528\u8005\u9054\u6210\u76ee\u6a19\u65b9\u9762\u59cb\u7d42\u512a\u65bc RLHF\uff0c\u4e26\u7372\u5f97\u66f4\u9ad8\u7684\u6eff\u610f\u5ea6\u8a55\u5206\uff0c\u5118\u7ba1\u50c5\u4f7f\u7528\u6a21\u64ec\u4e8b\u5f8c\u56de\u994b\u9032\u884c\u8a13\u7df4\u3002\u9019\u4e9b\u7d50\u679c\u5f37\u8abf\u4e86\u95dc\u6ce8\u9577\u671f\u5f8c\u679c\uff08\u5373\u4f7f\u662f\u6a21\u64ec\u7684\u5f8c\u679c\uff09\u4ee5\u6e1b\u8f15 RLHF \u4e2d\u4e0d\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\u3002", "author": "Kaiqu Liang et.al.", "authors": "Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fern\u00e1ndez Fisac", "id": "2501.08617v1", "paper_url": "http://arxiv.org/abs/2501.08617v1", "repo": "null"}}