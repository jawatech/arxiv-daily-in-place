{"2501.14577": {"publish_time": "2025-01-24", "title": "ZETA: Leveraging Z-order Curves for Efficient Top-k Attention", "paper_summary": "Over recent years, the Transformer has become a fundamental building block\nfor sequence modeling architectures. Yet at its core is the use of\nself-attention, whose memory and computational cost grow quadratically with the\nsequence length $N$, rendering it prohibitively expensive for long sequences. A\npromising approach is top-$k$ attention, which selects only the $k$ most\nrelevant tokens and achieves performance comparable to vanilla self-attention\nwhile significantly reducing space and computational demands. However, causal\nmasks require the current query token to only attend to past tokens, preventing\nthe existing top-$k$ attention method from efficiently searching for the most\nrelevant tokens in parallel, thereby limiting training efficiency. In this\nwork, we propose ZETA, leveraging \\textbf{Z}-Order Curves for\n\\textbf{E}fficient \\textbf{T}op-$k$ \\textbf{A}ttention, to enable parallel\nquerying of past tokens for entire sequences. % in both space and time\ncomplexity of $\\mathcal{O}(N \\log N)$. We first theoretically show that the\nchoice of key and query dimensions involves a trade-off between the curse of\ndimensionality and the preservation of relative distances after projection. In\nlight of this insight, we propose reducing the dimensionality of keys and\nqueries in contrast to values and further leverage $Z$-order curves to map\nlow-dimensional keys and queries into \\emph{one}-dimensional space, which\npermits parallel sorting, thereby largely improving the efficiency for top-$k$\ntoken selection. Experimental results demonstrate that ZETA matches the\nperformance of standard attention on the synthetic \\textsc{Multi-Query\nAssociative Recall} task and outperforms attention and its variants on\n\\textsc{Long Range Arena} and \\textsc{WikiText-103} language modeling.", "paper_summary_zh": "<paragraph>\u8fd1\u5e74\u6765\uff0cTransformer \u5df2\u6210\u4e3a\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u7684\u57fa\u672c\u6784\u5efa\u6a21\u5757\u3002\u7136\u800c\uff0c\u5176\u6838\u5fc3\u662f\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\uff0c\u5176\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u968f\u5e8f\u5217\u957f\u5ea6 $N$ \u4e8c\u6b21\u589e\u957f\uff0c\u4f7f\u5f97\u5176\u5bf9\u4e8e\u957f\u5e8f\u5217\u7684\u5f00\u9500\u8fc7\u5927\u3002\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u662f top-$k$ \u6ce8\u610f\u529b\uff0c\u5b83\u4ec5\u9009\u62e9 $k$ \u4e2a\u6700\u76f8\u5173\u7684\u6807\u8bb0\uff0c\u5e76\u5b9e\u73b0\u4e0e\u9999\u8349\u81ea\u6ce8\u610f\u529b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u7a7a\u95f4\u548c\u8ba1\u7b97\u9700\u6c42\u3002\u7136\u800c\uff0c\u56e0\u679c\u63a9\u7801\u8981\u6c42\u5f53\u524d\u67e5\u8be2\u6807\u8bb0\u4ec5\u5173\u6ce8\u8fc7\u53bb\u6807\u8bb0\uff0c\u4ece\u800c\u963b\u6b62\u73b0\u6709\u7684 top-$k$ \u6ce8\u610f\u529b\u65b9\u6cd5\u5e76\u884c\u6709\u6548\u5730\u641c\u7d22\u6700\u76f8\u5173\u7684\u6807\u8bb0\uff0c\u4ece\u800c\u9650\u5236\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ZETA\uff0c\u5229\u7528 \\textbf{Z} \u9636\u66f2\u7ebf\u5b9e\u73b0 \\textbf{E}fficient \\textbf{T}op-$k$ \\textbf{A}ttention\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u6574\u4e2a\u5e8f\u5217\u4e2d\u8fc7\u53bb\u6807\u8bb0\u7684\u5e76\u884c\u67e5\u8be2\u3002% \u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u4e0a\u5747\u4e3a $\\mathcal{O}(N \\log N)$\u3002\u6211\u4eec\u9996\u5148\u4ece\u7406\u8bba\u4e0a\u8868\u660e\uff0c\u952e\u548c\u67e5\u8be2\u7ef4\u5ea6\u7684\u9009\u62e9\u6d89\u53ca\u7ef4\u5ea6\u707e\u96be\u548c\u6295\u5f71\u540e\u76f8\u5bf9\u8ddd\u79bb\u7684\u4fdd\u7559\u4e4b\u95f4\u7684\u6743\u8861\u3002\u6839\u636e\u8fd9\u4e00\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u964d\u4f4e\u952e\u548c\u67e5\u8be2\u7684\u7ef4\u5ea6\uff0c\u4e0e\u503c\u5f62\u6210\u5bf9\u6bd4\uff0c\u5e76\u8fdb\u4e00\u6b65\u5229\u7528 $Z$ \u9636\u66f2\u7ebf\u5c06\u4f4e\u7ef4\u952e\u548c\u67e5\u8be2\u6620\u5c04\u5230\\emph{\u4e00}\u7ef4\u7a7a\u95f4\uff0c\u8fd9\u5141\u8bb8\u5e76\u884c\u6392\u5e8f\uff0c\u4ece\u800c\u6781\u5927\u5730\u63d0\u9ad8\u4e86 top-$k$ \u6807\u8bb0\u9009\u62e9\u7684\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cZETA \u5728\u5408\u6210 \\textsc{\u591a\u67e5\u8be2\u5173\u8054\u53ec\u56de} \u4efb\u52a1\u4e0a\u4e0e\u6807\u51c6\u6ce8\u610f\u529b\u7684\u6027\u80fd\u76f8\u5339\u914d\uff0c\u5e76\u5728 \\textsc{\u957f\u7a0b\u7ade\u6280\u573a} \u548c \\textsc{WikiText-103} \u8bed\u8a00\u5efa\u6a21\u4e0a\u4f18\u4e8e\u6ce8\u610f\u529b\u53ca\u5176\u53d8\u4f53\u3002</paragraph>", "author": "Qiuhao Zeng et.al.", "authors": "Qiuhao Zeng, Jerry Huang, Peng Lu, Gezheng Xu, Boxing Chen, Charles Ling, Boyu Wang", "id": "2501.14577v1", "paper_url": "http://arxiv.org/abs/2501.14577v1", "repo": "null"}}