{"2410.01639": {"publish_time": "2024-10-02", "title": "Moral Alignment for LLM Agents", "paper_summary": "Decision-making agents based on pre-trained Large Language Models (LLMs) are\nincreasingly being deployed across various domains of human activity. While\ntheir applications are currently rather specialized, several research efforts\nare under way to develop more generalist agents. As LLM-based systems become\nmore agentic, their influence on human activity will grow and the transparency\nof this will decrease. Consequently, developing effective methods for aligning\nthem to human values is vital.\n  The prevailing practice in alignment often relies on human preference data\n(e.g., in RLHF or DPO), in which values are implicit and are essentially\ndeduced from relative preferences over different model outputs. In this work,\ninstead of relying on human feedback, we introduce the design of reward\nfunctions that explicitly encode core human values for Reinforcement\nLearning-based fine-tuning of foundation agent models. Specifically, we use\nintrinsic rewards for the moral alignment of LLM agents.\n  We evaluate our approach using the traditional philosophical frameworks of\nDeontological Ethics and Utilitarianism, quantifying moral rewards for agents\nin terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)\nenvironment. We also show how moral fine-tuning can be deployed to enable an\nagent to unlearn a previously developed selfish strategy. Finally, we find that\ncertain moral strategies learned on the IPD game generalize to several other\nmatrix game environments. In summary, we demonstrate that fine-tuning with\nintrinsic rewards is a promising general solution for aligning LLM agents to\nhuman values, and it might represent a more transparent and cost-effective\nalternative to currently predominant alignment techniques.", "paper_summary_zh": "<paragraph>\u57fa\u65bc\u9810\u5148\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6c7a\u7b56\u4ee3\u7406\u6b63\u9010\u6f38\u90e8\u7f72\u65bc\u4eba\u985e\u6d3b\u52d5\u7684\u5404\u500b\u9818\u57df\u3002\u5118\u7ba1\u5176\u61c9\u7528\u76ee\u524d\u76f8\u7576\u5c08\u696d\u5316\uff0c\u4f46\u6709\u8a31\u591a\u7814\u7a76\u6b63\u81f4\u529b\u65bc\u958b\u767c\u66f4\u901a\u7528\u7684\u4ee3\u7406\u3002\u96a8\u8457\u57fa\u65bc LLM \u7684\u7cfb\u7d71\u8b8a\u5f97\u66f4\u5177\u4ee3\u7406\u6027\uff0c\u5176\u5c0d\u4eba\u985e\u6d3b\u52d5\u7684\u5f71\u97ff\u5c07\u6703\u589e\u9577\uff0c\u800c\u5176\u900f\u660e\u5ea6\u5c07\u6703\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u958b\u767c\u6709\u6548\u7684\u65b9\u6cd5\u4f86\u5c07\u5176\u8207\u4eba\u985e\u50f9\u503c\u89c0\u4fdd\u6301\u4e00\u81f4\u81f3\u95dc\u91cd\u8981\u3002\n\u8abf\u6574\u4e2d\u7684\u666e\u904d\u505a\u6cd5\u901a\u5e38\u4f9d\u8cf4\u65bc\u4eba\u985e\u504f\u597d\u6578\u64da\uff08\u4f8b\u5982\u5728 RLHF \u6216 DPO \u4e2d\uff09\uff0c\u5176\u4e2d\u50f9\u503c\u89c0\u662f\u96b1\u542b\u7684\uff0c\u4e26\u4e14\u57fa\u672c\u4e0a\u662f\u5f9e\u4e0d\u540c\u6a21\u578b\u8f38\u51fa\u7684\u76f8\u5c0d\u504f\u597d\u4e2d\u63a8\u5c0e\u51fa\u4f86\u7684\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4e26\u975e\u4f9d\u8cf4\u65bc\u4eba\u985e\u56de\u994b\uff0c\u800c\u662f\u5f15\u5165\u4e86\u734e\u52f5\u51fd\u6578\u7684\u8a2d\u8a08\uff0c\u8a72\u51fd\u6578\u660e\u78ba\u7de8\u78bc\u4e86\u4eba\u985e\u6838\u5fc3\u50f9\u503c\u89c0\uff0c\u7528\u65bc\u5f37\u5316\u5b78\u7fd2\u57fa\u790e\u4ee3\u7406\u6a21\u578b\u7684\u5fae\u8abf\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4f7f\u7528\u5167\u5728\u734e\u52f5\u4f86\u9032\u884c LLM \u4ee3\u7406\u7684\u9053\u5fb7\u8abf\u6574\u3002\n\u6211\u5011\u4f7f\u7528\u50b3\u7d71\u54f2\u5b78\u6846\u67b6\u7fa9\u52d9\u502b\u7406\u5b78\u548c\u529f\u5229\u4e3b\u7fa9\u4f86\u8a55\u4f30\u6211\u5011\u7684\u505a\u6cd5\uff0c\u6839\u64da\u91cd\u8907\u56da\u5f92\u56f0\u5883 (IPD) \u74b0\u5883\u4e2d\u7684\u884c\u52d5\u548c\u5f8c\u679c\u5c0d\u4ee3\u7406\u7684\u9053\u5fb7\u734e\u52f5\u9032\u884c\u91cf\u5316\u3002\u6211\u5011\u9084\u5c55\u793a\u4e86\u5982\u4f55\u90e8\u7f72\u9053\u5fb7\u5fae\u8abf\u4ee5\u4f7f\u4ee3\u7406\u53d6\u6d88\u5148\u524d\u958b\u767c\u7684\u81ea\u79c1\u7b56\u7565\u3002\u6700\u5f8c\uff0c\u6211\u5011\u767c\u73fe IPD \u904a\u6232\u4e2d\u5b78\u5230\u7684\u67d0\u4e9b\u9053\u5fb7\u7b56\u7565\u53ef\u4ee5\u63a8\u5ee3\u5230\u5176\u4ed6\u5e7e\u500b\u77e9\u9663\u904a\u6232\u74b0\u5883\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u8b49\u660e\u4e86\u4f7f\u7528\u5167\u5728\u734e\u52f5\u9032\u884c\u5fae\u8abf\u662f\u5c07 LLM \u4ee3\u7406\u8207\u4eba\u985e\u50f9\u503c\u89c0\u4fdd\u6301\u4e00\u81f4\u7684\u6709\u524d\u9014\u7684\u901a\u7528\u89e3\u6c7a\u65b9\u6848\uff0c\u5b83\u53ef\u80fd\u4ee3\u8868\u4e86\u6bd4\u76ee\u524d\u4e3b\u8981\u7684\u8abf\u6574\u6280\u8853\u66f4\u900f\u660e\u4e14\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002</paragraph>", "author": "Elizaveta Tennant et.al.", "authors": "Elizaveta Tennant, Stephen Hailes, Mirco Musolesi", "id": "2410.01639v1", "paper_url": "http://arxiv.org/abs/2410.01639v1", "repo": "null"}}