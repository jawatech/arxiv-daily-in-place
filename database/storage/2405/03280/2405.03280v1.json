{"2405.03280": {"publish_time": "2024-05-06", "title": "Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity", "paper_summary": "Reconstructing human dynamic vision from brain activity is a challenging task\nwith great scientific significance. The difficulty stems from two primary\nissues: (1) vision-processing mechanisms in the brain are highly intricate and\nnot fully revealed, making it challenging to directly learn a mapping between\nfMRI and video; (2) the temporal resolution of fMRI is significantly lower than\nthat of natural videos. To overcome these issues, this paper propose a\ntwo-stage model named Mind-Animator, which achieves state-of-the-art\nperformance on three public datasets. Specifically, during the fMRI-to-feature\nstage, we decouple semantic, structural, and motion features from fMRI through\nfMRI-vision-language tri-modal contrastive learning and sparse causal\nattention. In the feature-to-video stage, these features are merged to videos\nby an inflated Stable Diffusion. We substantiate that the reconstructed video\ndynamics are indeed derived from fMRI, rather than hallucinations of the\ngenerative model, through permutation tests. Additionally, the visualization of\nvoxel-wise and ROI-wise importance maps confirms the neurobiological\ninterpretability of our model.", "paper_summary_zh": "", "author": "Yizhuo Lu et.al.", "authors": "Yizhuo Lu,Changde Du,Chong Wang,Xuanliu Zhu,Liuyun Jiang,Huiguang He", "id": "2405.03280v1", "paper_url": "http://arxiv.org/abs/2405.03280v1", "repo": "null"}}