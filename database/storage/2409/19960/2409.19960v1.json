{"2409.19960": {"publish_time": "2024-09-30", "title": "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "paper_summary": "Zero-shot inference, where pre-trained models perform tasks without specific\ntraining data, is an exciting emergent ability of large models like CLIP.\nAlthough there has been considerable exploration into enhancing zero-shot\nabilities in image captioning (IC) for popular datasets such as MSCOCO and\nFlickr8k, these approaches fall short with fine-grained datasets like CUB, FLO,\nUCM-Captions, and Sydney-Captions. These datasets require captions to discern\nbetween visually and semantically similar classes, focusing on detailed object\nparts and their attributes. To overcome this challenge, we introduce\nTRaining-Free Object-Part Enhancement (TROPE). TROPE enriches a base caption\nwith additional object-part details using object detector proposals and Natural\nLanguage Processing techniques. It complements rather than alters the base\ncaption, allowing seamless integration with other captioning methods and\noffering users enhanced flexibility. Our evaluations show that TROPE\nconsistently boosts performance across all tested zero-shot IC approaches and\nachieves state-of-the-art results on fine-grained IC datasets.", "paper_summary_zh": "\u96f6\u6b21\u5b78\u7fd2\u63a8\u7406\uff0c\u5176\u4e2d\u9810\u8a13\u7df4\u6a21\u578b\u5728\u6c92\u6709\u7279\u5b9a\u8a13\u7df4\u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\u57f7\u884c\u4efb\u52d9\uff0c\u662f CLIP \u7b49\u5927\u578b\u6a21\u578b\u4ee4\u4eba\u8208\u596e\u7684\u65b0\u8208\u80fd\u529b\u3002\u5118\u7ba1\u5df2\u7d93\u5c0d\u589e\u5f37\u6d41\u884c\u8cc7\u6599\u96c6\uff08\u4f8b\u5982 MSCOCO \u548c Flickr8k\uff09\u4e2d\u5f71\u50cf\u6a19\u984c\uff08IC\uff09\u7684\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\u9032\u884c\u4e86\u5927\u91cf\u7684\u63a2\u7d22\uff0c\u4f46\u9019\u4e9b\u65b9\u6cd5\u5728 CUB\u3001FLO\u3001UCM-Captions \u548c Sydney-Captions \u7b49\u7d30\u7c92\u5ea6\u8cc7\u6599\u96c6\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002\u9019\u4e9b\u8cc7\u6599\u96c6\u9700\u8981\u6a19\u984c\u4f86\u5340\u5206\u8996\u89ba\u4e0a\u548c\u8a9e\u7fa9\u4e0a\u76f8\u4f3c\u7684\u985e\u5225\uff0c\u91cd\u9ede\u95dc\u6ce8\u8a73\u7d30\u7684\u7269\u4ef6\u90e8\u5206\u53ca\u5176\u5c6c\u6027\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86\u7121\u8a13\u7df4\u7269\u4ef6\u90e8\u5206\u589e\u5f37\uff08TROPE\uff09\u3002TROPE \u4f7f\u7528\u7269\u4ef6\u5075\u6e2c\u5efa\u8b70\u548c\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u6280\u8853\uff0c\u4f7f\u7528\u984d\u5916\u7684\u7269\u4ef6\u90e8\u5206\u7d30\u7bc0\u4f86\u8c50\u5bcc\u57fa\u672c\u6a19\u984c\u3002\u5b83\u88dc\u5145\u800c\u4e0d\u662f\u6539\u8b8a\u57fa\u672c\u6a19\u984c\uff0c\u5141\u8a31\u8207\u5176\u4ed6\u6a19\u984c\u65b9\u6cd5\u7121\u7e2b\u6574\u5408\uff0c\u4e26\u70ba\u4f7f\u7528\u8005\u63d0\u4f9b\u589e\u5f37\u7684\u9748\u6d3b\u6027\u3002\u6211\u5011\u7684\u8a55\u4f30\u8868\u660e\uff0cTROPE \u5728\u6240\u6709\u6e2c\u8a66\u7684\u96f6\u6b21\u5b78\u7fd2 IC \u65b9\u6cd5\u4e2d\u6301\u7e8c\u63d0\u5347\u6548\u80fd\uff0c\u4e26\u5728\u7d30\u7c92\u5ea6 IC \u8cc7\u6599\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u9032\u7684\u7d50\u679c\u3002", "author": "Joshua Feinglass et.al.", "authors": "Joshua Feinglass, Yezhou Yang", "id": "2409.19960v1", "paper_url": "http://arxiv.org/abs/2409.19960v1", "repo": "null"}}