{"2501.17286": {"publish_time": "2025-01-28", "title": "Fine-Tuning Open-Source Large Language Models to Improve Their Performance on Radiation Oncology Tasks: A Feasibility Study to Investigate Their Potential Clinical Applications in Radiation Oncology", "paper_summary": "Background: The radiation oncology clinical practice involves many steps\nrelying on the dynamic interplay of abundant text data. Large language models\nhave displayed remarkable capabilities in processing complex text information.\nBut their direct applications in specific fields like radiation oncology remain\nunderexplored.\n  Purpose: This study aims to investigate whether fine-tuning LLMs with domain\nknowledge can improve the performance on Task (1) treatment regimen generation,\nTask (2) treatment modality selection (photon, proton, electron, or\nbrachytherapy), and Task (3) ICD-10 code prediction in radiation oncology.\n  Methods: Data for 15,724 patient cases were extracted. Cases where patients\nhad a single diagnostic record, and a clearly identifiable primary treatment\nplan were selected for preprocessing and manual annotation to have 7,903 cases\nof the patient diagnosis, treatment plan, treatment modality, and ICD-10 code.\nEach case was used to construct a pair consisting of patient diagnostics\ndetails and an answer (treatment regimen, treatment modality, or ICD-10 code\nrespectively) for the supervised fine-tuning of these three tasks. Open source\nLLaMA2-7B and Mistral-7B models were utilized for the fine-tuning with the\nLow-Rank Approximations method. Accuracy and ROUGE-1 score were reported for\nthe fine-tuned models and original models. Clinical evaluation was performed on\nTask (1) by radiation oncologists, while precision, recall, and F-1 score were\nevaluated for Task (2) and (3). One-sided Wilcoxon signed-rank tests were used\nto statistically analyze the results.\n  Results: Fine-tuned LLMs outperformed original LLMs across all tasks with\np-value <= 0.001. Clinical evaluation demonstrated that over 60% of the\nfine-tuned LLMs-generated treatment regimens were clinically acceptable.\nPrecision, recall, and F1-score showed improved performance of fine-tuned LLMs.", "paper_summary_zh": "<paragraph>\u80cc\u666f\uff1a\u653e\u5c04\u80bf\u7624\u4e34\u5e8a\u5b9e\u8df5\u6d89\u53ca\u8bb8\u591a\u6b65\u9aa4\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u4f9d\u8d56\u4e8e\u4e30\u5bcc\u6587\u672c\u6570\u636e\u7684\u52a8\u6001\u4ea4\u4e92\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7684\u6587\u672c\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002\u4f46\u5b83\u4eec\u5728\u653e\u5c04\u80bf\u7624\u7b49\u7279\u5b9a\u9886\u57df\u7684\u76f4\u63a5\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\n\u76ee\u7684\uff1a\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u5fae\u8c03 LLM \u662f\u5426\u53ef\u4ee5\u63d0\u9ad8\u4efb\u52a1 (1) \u6cbb\u7597\u65b9\u6848\u751f\u6210\u3001\u4efb\u52a1 (2) \u6cbb\u7597\u65b9\u5f0f\u9009\u62e9\uff08\u5149\u5b50\u3001\u8d28\u5b50\u3001\u7535\u5b50\u6216\u8fd1\u8ddd\u79bb\u653e\u5c04\u6cbb\u7597\uff09\u548c\u4efb\u52a1 (3) \u653e\u5c04\u80bf\u7624\u4e2d ICD-10 \u4ee3\u7801\u9884\u6d4b\u7684\u6027\u80fd\u3002\n\u65b9\u6cd5\uff1a\u63d0\u53d6\u4e86 15,724 \u4f8b\u60a3\u8005\u75c5\u4f8b\u7684\u6570\u636e\u3002\u9009\u62e9\u4e86\u60a3\u8005\u6709\u5355\u4e00\u8bca\u65ad\u8bb0\u5f55\u4e14\u6709\u660e\u786e\u53ef\u8bc6\u522b\u7684\u4e3b\u8981\u6cbb\u7597\u8ba1\u5212\u7684\u75c5\u4f8b\uff0c\u8fdb\u884c\u9884\u5904\u7406\u548c\u624b\u52a8\u6ce8\u91ca\uff0c\u5f97\u5230 7,903 \u4f8b\u60a3\u8005\u8bca\u65ad\u3001\u6cbb\u7597\u8ba1\u5212\u3001\u6cbb\u7597\u65b9\u5f0f\u548c ICD-10 \u4ee3\u7801\u3002\u6bcf\u4e2a\u75c5\u4f8b\u90fd\u7528\u4e8e\u6784\u5efa\u4e00\u5bf9\uff0c\u5305\u62ec\u60a3\u8005\u8bca\u65ad\u8be6\u60c5\u548c\u7b54\u6848\uff08\u5206\u522b\u662f\u6cbb\u7597\u65b9\u6848\u3001\u6cbb\u7597\u65b9\u5f0f\u6216 ICD-10 \u4ee3\u7801\uff09\uff0c\u7528\u4e8e\u8fd9\u4e09\u4e2a\u4efb\u52a1\u7684\u76d1\u7763\u5fae\u8c03\u3002\u5f00\u6e90 LLaMA2-7B \u548c Mistral-7B \u6a21\u578b\u88ab\u7528\u4e8e\u4f7f\u7528\u4f4e\u79e9\u903c\u8fd1\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\u3002\u62a5\u544a\u4e86\u5fae\u8c03\u6a21\u578b\u548c\u539f\u59cb\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c ROUGE-1 \u5206\u6570\u3002\u4efb\u52a1 (1) \u7531\u653e\u5c04\u80bf\u7624\u79d1\u533b\u5e08\u8fdb\u884c\u4e34\u5e8a\u8bc4\u4f30\uff0c\u800c\u4efb\u52a1 (2) \u548c (3) \u5219\u8bc4\u4f30\u4e86\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548c F-1 \u5206\u6570\u3002\u5355\u4fa7 Wilcoxon \u7b26\u53f7\u79e9\u68c0\u9a8c\u7528\u4e8e\u5bf9\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002\n\u7ed3\u679c\uff1a\u5fae\u8c03\u540e\u7684 LLM \u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u539f\u59cb LLM\uff0cp \u503c <= 0.001\u3002\u4e34\u5e8a\u8bc4\u4f30\u8868\u660e\uff0c\u8d85\u8fc7 60% \u7684\u5fae\u8c03 LLM \u751f\u6210\u7684\u6cbb\u7597\u65b9\u6848\u5728\u4e34\u5e8a\u4e0a\u662f\u53ef\u63a5\u53d7\u7684\u3002\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548c F1 \u5206\u6570\u663e\u793a\u5fae\u8c03\u540e\u7684 LLM \u6027\u80fd\u5f97\u5230\u6539\u5584\u3002</paragraph>", "author": "Peilong Wang et.al.", "authors": "Peilong Wang, Zhengliang Liu, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Xiang Li, Quanzheng Li, Brady S. Laughlin, Diego Santos Toesca, Sujay A. Vora, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu", "id": "2501.17286v1", "paper_url": "http://arxiv.org/abs/2501.17286v1", "repo": "null"}}