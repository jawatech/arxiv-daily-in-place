{"2410.10414": {"publish_time": "2024-10-14", "title": "On Calibration of LLM-based Guard Models for Reliable Content Moderation", "paper_summary": "Large language models (LLMs) pose significant risks due to the potential for\ngenerating harmful content or users attempting to evade guardrails. Existing\nstudies have developed LLM-based guard models designed to moderate the input\nand output of threat LLMs, ensuring adherence to safety policies by blocking\ncontent that violates these protocols upon deployment. However, limited\nattention has been given to the reliability and calibration of such guard\nmodels. In this work, we empirically conduct comprehensive investigations of\nconfidence calibration for 9 existing LLM-based guard models on 12 benchmarks\nin both user input and model output classification. Our findings reveal that\ncurrent LLM-based guard models tend to 1) produce overconfident predictions, 2)\nexhibit significant miscalibration when subjected to jailbreak attacks, and 3)\ndemonstrate limited robustness to the outputs generated by different types of\nresponse models. Additionally, we assess the effectiveness of post-hoc\ncalibration methods to mitigate miscalibration. We demonstrate the efficacy of\ntemperature scaling and, for the first time, highlight the benefits of\ncontextual calibration for confidence calibration of guard models, particularly\nin the absence of validation sets. Our analysis and experiments underscore the\nlimitations of current LLM-based guard models and provide valuable insights for\nthe future development of well-calibrated guard models toward more reliable\ncontent moderation. We also advocate for incorporating reliability evaluation\nof confidence calibration when releasing future LLM-based guard models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7531\u65bc\u6f5b\u5728\u751f\u6210\u6709\u5bb3\u5167\u5bb9\u6216\u4f7f\u7528\u8005\u4f01\u5716\u898f\u907f\u9632\u8b77\u63aa\u65bd\u7684\u98a8\u96aa\uff0c\u56e0\u6b64\u69cb\u6210\u91cd\u5927\u98a8\u96aa\u3002\u73fe\u6709\u7814\u7a76\u5df2\u958b\u767c\u51fa\u57fa\u65bc LLM \u7684\u9632\u8b77\u6a21\u578b\uff0c\u65e8\u5728\u8abf\u7bc0\u5a01\u8105 LLM \u7684\u8f38\u5165\u548c\u8f38\u51fa\uff0c\u78ba\u4fdd\u5728\u90e8\u7f72\u6642\u5c01\u9396\u9055\u53cd\u9019\u4e9b\u5354\u5b9a\u7684\u5167\u5bb9\uff0c\u5f9e\u800c\u9075\u5b88\u5b89\u5168\u653f\u7b56\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u6b64\u985e\u9632\u8b77\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u6821\u6e96\uff0c\u537b\u9bae\u5c11\u53d7\u5230\u95dc\u6ce8\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u91dd\u5c0d 9 \u500b\u73fe\u6709\u7684\u57fa\u65bc LLM \u7684\u9632\u8b77\u6a21\u578b\uff0c\u5728 12 \u500b\u57fa\u6e96\u4e0a\u5c0d\u4f7f\u7528\u8005\u8f38\u5165\u548c\u6a21\u578b\u8f38\u51fa\u5206\u985e\u9032\u884c\u5168\u9762\u7684\u4fe1\u5fc3\u6821\u6e96\u8abf\u67e5\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u76ee\u524d\u7684\u57fa\u65bc LLM \u7684\u9632\u8b77\u6a21\u578b\u5f80\u5f80\u6703 1) \u7522\u751f\u904e\u5ea6\u81ea\u4fe1\u7684\u9810\u6e2c\uff0c2) \u5728\u906d\u53d7\u8d8a\u7344\u653b\u64ca\u6642\u8868\u73fe\u51fa\u986f\u8457\u7684\u6821\u6e96\u4e0d\u7576\uff0c\u4ee5\u53ca 3) \u5c0d\u4e0d\u540c\u985e\u578b\u7684\u56de\u61c9\u6a21\u578b\u6240\u7522\u751f\u7684\u8f38\u51fa\u8868\u73fe\u51fa\u6709\u9650\u7684\u7a69\u5065\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a55\u4f30\u4e8b\u5f8c\u6821\u6e96\u65b9\u6cd5\u5728\u6e1b\u8f15\u6821\u6e96\u4e0d\u7576\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u6211\u5011\u5c55\u793a\u4e86\u6eab\u5ea6\u8abf\u6574\u7684\u529f\u6548\uff0c\u4e26\u9996\u6b21\u5f37\u8abf\u4e86\u60c5\u5883\u6821\u6e96\u5c0d\u65bc\u9632\u8b77\u6a21\u578b\u7684\u4fe1\u5fc3\u6821\u6e96\uff08\u7279\u5225\u662f\u5728\u6c92\u6709\u9a57\u8b49\u96c6\u7684\u60c5\u6cc1\u4e0b\uff09\u6240\u5e36\u4f86\u7684\u76ca\u8655\u3002\u6211\u5011\u7684\u5206\u6790\u548c\u5be6\u9a57\u5f37\u8abf\u4e86\u76ee\u524d\u57fa\u65bc LLM \u7684\u9632\u8b77\u6a21\u578b\u7684\u9650\u5236\uff0c\u4e26\u70ba\u672a\u4f86\u958b\u767c\u6821\u6e96\u826f\u597d\u7684\u9632\u8b77\u6a21\u578b\u4ee5\u5be6\u73fe\u66f4\u53ef\u9760\u7684\u5167\u5bb9\u5be9\u6838\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002\u6211\u5011\u4e5f\u4e3b\u5f35\u5728\u91cb\u51fa\u672a\u4f86\u7684\u57fa\u65bc LLM \u7684\u9632\u8b77\u6a21\u578b\u6642\uff0c\u7d0d\u5165\u4fe1\u5fc3\u6821\u6e96\u7684\u53ef\u9760\u6027\u8a55\u4f30\u3002", "author": "Hongfu Liu et.al.", "authors": "Hongfu Liu, Hengguan Huang, Hao Wang, Xiangming Gu, Ye Wang", "id": "2410.10414v1", "paper_url": "http://arxiv.org/abs/2410.10414v1", "repo": "null"}}