{"2502.09560": {"publish_time": "2025-02-13", "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents", "paper_summary": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.", "paper_summary_zh": "<paragraph>\u5229\u7528\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4f86\u5efa\u7acb\u5177\u8eab\u4ee3\u7406\uff0c\u63d0\u4f9b\u4e86\u89e3\u6c7a\u73fe\u5be6\u4e16\u754c\u4efb\u52d9\u7684\u6709\u524d\u666f\u9014\u5f91\u3002\u5118\u7ba1\u4ee5\u8a9e\u8a00\u70ba\u4e2d\u5fc3\u7684\u5177\u8eab\u4ee3\u7406\u5df2\u7372\u5f97\u5927\u91cf\u95dc\u6ce8\uff0c\u4f46\u7531\u65bc\u7f3a\u4e4f\u5168\u9762\u7684\u8a55\u4f30\u6846\u67b6\uff0c\u57fa\u65bc MLLM \u7684\u5177\u8eab\u4ee3\u7406\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u70ba\u4e86\u5f4c\u88dc\u9019\u4e00\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 EmbodiedBench\uff0c\u9019\u662f\u4e00\u500b\u5ee3\u6cdb\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u65e8\u5728\u8a55\u4f30\u4ee5\u8996\u89ba\u70ba\u5c0e\u5411\u7684\u5177\u8eab\u4ee3\u7406\u3002EmbodiedBench \u7684\u7279\u9ede\uff1a(1) \u8de8\u8d8a\u56db\u500b\u74b0\u5883\u7684 1,128 \u9805\u591a\u6a23\u5316\u6e2c\u8a66\u4efb\u52d9\uff0c\u7bc4\u570d\u5f9e\u9ad8\u5c64\u7d1a\u8a9e\u7fa9\u4efb\u52d9\uff08\u4f8b\u5982\uff0c\u5bb6\u5ead\uff09\u5230\u6d89\u53ca\u539f\u5b50\u52d5\u4f5c\u7684\u4f4e\u5c64\u7d1a\u4efb\u52d9\uff08\u4f8b\u5982\uff0c\u5c0e\u822a\u548c\u64cd\u4f5c\uff09\uff1b\u4ee5\u53ca (2) \u516d\u500b\u7cbe\u5fc3\u7b56\u5283\u7684\u5b50\u96c6\uff0c\u7528\u65bc\u8a55\u4f30\u57fa\u672c\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u4f8b\u5982\u5e38\u8b58\u63a8\u7406\u3001\u8907\u96dc\u6307\u4ee4\u7406\u89e3\u3001\u7a7a\u9593\u611f\u77e5\u3001\u8996\u89ba\u611f\u77e5\u548c\u9577\u671f\u898f\u5283\u3002\u901a\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u5728 EmbodiedBench \u4e2d\u8a55\u4f30\u4e86 13 \u500b\u9818\u5148\u7684\u5c08\u6709\u548c\u958b\u6e90 MLLM\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff1aMLLM \u5728\u9ad8\u5c64\u7d1a\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5728\u4f4e\u5c64\u7d1a\u64cd\u4f5c\u4e2d\u9047\u5230\u56f0\u96e3\uff0c\u8868\u73fe\u6700\u597d\u7684\u6a21\u578b GPT-4o \u5e73\u5747\u5f97\u5206\u50c5\u70ba 28.9%\u3002EmbodiedBench \u63d0\u4f9b\u4e86\u4e00\u500b\u591a\u65b9\u9762\u7684\u6a19\u6e96\u5316\u8a55\u4f30\u5e73\u53f0\uff0c\u4e0d\u50c5\u7a81\u51fa\u4e86\u73fe\u6709\u6311\u6230\uff0c\u9084\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\u4f86\u63a8\u9032\u57fa\u65bc MLLM \u7684\u5177\u8eab\u4ee3\u7406\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://embodiedbench.github.io/ \u53d6\u5f97\u3002</paragraph>", "author": "Rui Yang et.al.", "authors": "Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang", "id": "2502.09560v1", "paper_url": "http://arxiv.org/abs/2502.09560v1", "repo": "null"}}