{"2501.08167": {"publish_time": "2025-01-14", "title": "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data", "paper_summary": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLMs as judges offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nThis research contributes to the growing body of knowledge on AI assisted text\nanalysis. We discuss limitations and provide recommendations for future\nresearch, emphasizing the need for careful consideration when generalizing LLM\njudge models across various contexts and use cases.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u5feb\u901f\u9032\u5c55\u89e3\u9396\u4e86\u5728\u8655\u7406\u548c\u7e3d\u7d50\u975e\u7d50\u69cb\u5316\u6587\u672c\u6578\u64da\u65b9\u9762\u7684\u975e\u51e1\u80fd\u529b\u3002\u9019\u5c0d\u8c50\u5bcc\u7684\u958b\u653e\u5f0f\u6578\u64da\u96c6\u7684\u5206\u6790\u6709\u5f71\u97ff\uff0c\u4f8b\u5982\u8abf\u67e5\u56de\u61c9\uff0c\u5176\u4e2d LLM \u627f\u8afe\u6709\u6548\u5730\u63d0\u7149\u95dc\u9375\u4e3b\u984c\u548c\u60c5\u7dd2\u3002\u7136\u800c\uff0c\u96a8\u8457\u7d44\u7e54\u8d8a\u4f86\u8d8a\u591a\u5730\u6c42\u52a9\u65bc\u9019\u4e9b\u5f37\u5927\u7684 AI \u7cfb\u7d71\u4f86\u7406\u89e3\u6587\u672c\u53cd\u994b\uff0c\u4e00\u500b\u95dc\u9375\u554f\u984c\u51fa\u73fe\u4e86\uff0c\u6211\u5011\u80fd\u76f8\u4fe1 LLM \u80fd\u6e96\u78ba\u5730\u4ee3\u8868\u9019\u4e9b\u57fa\u65bc\u6587\u672c\u7684\u6578\u64da\u96c6\u6240\u5305\u542b\u7684\u89c0\u9ede\u55ce\uff1f\u96d6\u7136 LLM \u5728\u751f\u6210\u985e\u4f3c\u4eba\u985e\u7684\u6458\u8981\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5176\u8f38\u51fa\u53ef\u80fd\u7121\u610f\u9593\u504f\u96e2\u539f\u59cb\u56de\u61c9\u7684\u771f\u6b63\u5be6\u8cea\u7684\u98a8\u96aa\u3002LLM \u751f\u6210\u7684\u8f38\u51fa\u8207\u6578\u64da\u4e2d\u5b58\u5728\u7684\u5be6\u969b\u4e3b\u984c\u4e4b\u9593\u7684\u5dee\u7570\u53ef\u80fd\u5c0e\u81f4\u6709\u7f3a\u9677\u7684\u6c7a\u7b56\u5236\u5b9a\uff0c\u5c0d\u7d44\u7e54\u7522\u751f\u6df1\u9060\u7684\u5f71\u97ff\u3002\u672c\u7814\u7a76\u8abf\u67e5\u4e86 LLM \u4f5c\u70ba\u8a55\u4f30\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u4ee5\u8a55\u4f30\u5176\u4ed6 LLM \u751f\u6210\u7684\u6458\u8981\u7684\u4e3b\u984c\u4e00\u81f4\u6027\u3002\u6211\u5011\u5229\u7528 Anthropic Claude \u6a21\u578b\u5f9e\u958b\u653e\u5f0f\u8abf\u67e5\u56de\u61c9\u4e2d\u751f\u6210\u4e3b\u984c\u6458\u8981\uff0c\u4e9e\u99ac\u905c\u7684 Titan Express\u3001Nova Pro \u548c Meta \u7684 Llama \u4f5c\u70ba LLM \u8a55\u5be9\u3002\u4f7f\u7528 Cohen's kappa\u3001Spearman's rho \u548c Krippendorff's alpha \u5c07 LLM \u4f5c\u70ba\u8a55\u5be9\u7684\u65b9\u6cd5\u8207\u4eba\u985e\u8a55\u4f30\u9032\u884c\u4e86\u6bd4\u8f03\uff0c\u9a57\u8b49\u4e86\u50b3\u7d71\u4ee5\u4eba\u70ba\u4e2d\u5fc3\u7684\u8a55\u4f30\u65b9\u6cd5\u7684\u53ef\u64f4\u5c55\u66ff\u4ee3\u65b9\u6848\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u96d6\u7136 LLM \u4f5c\u70ba\u8a55\u5be9\u63d0\u4f9b\u4e86\u4e00\u500b\u8207\u4eba\u985e\u8a55\u5206\u54e1\u76f8\u7576\u7684\u53ef\u64f4\u5c55\u89e3\u6c7a\u65b9\u6848\uff0c\u4f46\u4eba\u985e\u4ecd\u7136\u53ef\u80fd\u64c5\u9577\u6aa2\u6e2c\u5fae\u5999\u7684\u3001\u7279\u5b9a\u65bc\u4e0a\u4e0b\u6587\u7684\u7d30\u5fae\u5dee\u5225\u3002\u672c\u7814\u7a76\u6709\u52a9\u65bc\u589e\u52a0\u6709\u95dc AI \u8f14\u52a9\u6587\u672c\u5206\u6790\u7684\u77e5\u8b58\u9ad4\u7cfb\u3002\u6211\u5011\u8a0e\u8ad6\u4e86\u5c40\u9650\u6027\u4e26\u63d0\u4f9b\u4e86\u5c0d\u672a\u4f86\u7814\u7a76\u7684\u5efa\u8b70\uff0c\u5f37\u8abf\u5728\u5404\u7a2e\u80cc\u666f\u548c\u7528\u4f8b\u4e2d\u6982\u62ec LLM \u8a55\u5be9\u6a21\u578b\u6642\u9700\u8981\u4ed4\u7d30\u8003\u616e\u3002", "author": "Rewina Bedemariam et.al.", "authors": "Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar", "id": "2501.08167v1", "paper_url": "http://arxiv.org/abs/2501.08167v1", "repo": "null"}}