{"2406.19358": {"publish_time": "2024-06-27", "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models", "paper_summary": "Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.", "paper_summary_zh": "\u60c5\u7dd2\u5206\u6790\u662f\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4e2d\u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\u3002\u591a\u8a9e\u8a00\u9810\u8a13\u7df4\u6a21\u578b\uff08\u4f8b\u5982 XLM-R \u548c mT5\uff09\u7684\u9032\u6b65\uff0c\u4fc3\u4f7f\u4eba\u5011\u5c0d\u8de8\u8a9e\u8a00\u60c5\u7dd2\u5206\u6790\u7522\u751f\u8d8a\u4f86\u8d8a\u5927\u7684\u8208\u8da3\u3002\u8fd1\u671f\u51fa\u73fe\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5927\u5e45\u63d0\u5347\u4e00\u822c NLP \u4efb\u52d9\uff0c\u7136\u800c\uff0c\u6b64\u985e LLM \u5728\u8de8\u8a9e\u8a00\u60c5\u7dd2\u5206\u6790\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u7372\u5f97\u5145\u5206\u7814\u7a76\u3002\u672c\u7814\u7a76\u9032\u884c\u5be6\u8b49\u5206\u6790\uff0c\u4ee5\u6bd4\u8f03\u516c\u958b\u7684\u5c0f\u578b\u591a\u8a9e\u8a00\u8a9e\u8a00\u6a21\u578b (SMLM)\uff08\u4f8b\u5982 XLM-R\uff09\u8207\u4ee5\u82f1\u8a9e\u70ba\u4e2d\u5fc3\u7684 LLM\uff08\u4f8b\u5982 Llama-3\uff09\u5728\u82f1\u8a9e\u3001\u897f\u73ed\u7259\u8a9e\u3001\u6cd5\u8a9e\u548c\u4e2d\u6587\u7684\u60c5\u7dd2\u5206\u6790\u4e2d\u7684\u8de8\u8a9e\u8a00\u8f49\u79fb\u80fd\u529b\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u5728\u516c\u958b\u6a21\u578b\u4e2d\uff0cSMLM \u76f8\u8f03\u65bc LLM\uff0c\u5c55\u73fe\u51fa\u512a\u7570\u7684\u96f6\u6b21\u5b78\u7fd2\u8de8\u8a9e\u8a00\u6548\u80fd\u3002\u7136\u800c\uff0c\u5728\u5c11\u6b21\u5b78\u7fd2\u7684\u8de8\u8a9e\u8a00\u8a2d\u5b9a\u4e2d\uff0c\u516c\u958b LLM \u5c55\u73fe\u51fa\u589e\u5f37\u7684\u9069\u61c9\u6f5b\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u89c0\u5bdf\u5230\u5c08\u6709\u7684 GPT-3.5 \u548c GPT-4 \u5728\u96f6\u6b21\u5b78\u7fd2\u8de8\u8a9e\u8a00\u80fd\u529b\u4e2d\u9818\u5148\uff0c\u4f46\u5728\u5c11\u6b21\u5b78\u7fd2\u5834\u666f\u4e2d\u537b\u843d\u5f8c\u65bc\u516c\u958b\u6a21\u578b\u3002", "author": "Xiliang Zhu et.al.", "authors": "Xiliang Zhu, Shayna Gardiner, Tere Rold\u00e1n, David Rossouw", "id": "2406.19358v1", "paper_url": "http://arxiv.org/abs/2406.19358v1", "repo": "null"}}