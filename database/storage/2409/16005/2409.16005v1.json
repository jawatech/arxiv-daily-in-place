{"2409.16005": {"publish_time": "2024-09-24", "title": "Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character Pre-training in LLMs", "paper_summary": "The integration of large language models (LLMs) with pre-trained speech\nmodels has opened up new avenues in automatic speech recognition (ASR). While\nLLMs excel in multimodal understanding tasks, effectively leveraging their\ncapabilities for ASR remains a significant challenge. This paper presents a\nnovel training approach to enhance LLM performance in ASR tasks. We propose\npre-training LLMs on Pinyin embedding sequences, which represent pronunciation\nfeatures, to generate corresponding Chinese characters. This step enables the\nLLM to adapt to generating text from pronunciation features before encountering\nreal speech data. Furthermore, we fine-tune the LoRA parameters to enhance the\nLLM's understanding of speech modality information. In AISHELL-1 corpus, our\napproach yields a 9.5% relative improvement in ASR tasks compared to the\nbaseline without Pinyi-to-Character pre-training. Additionally, incorporating\nauxiliary text data for Pinyi-to-Character pre-training further boosts\nperformance, achieving a 19.0% relative improvement.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u9810\u8a13\u7df4\u8a9e\u97f3\u6a21\u578b\u7684\u6574\u5408\uff0c\u70ba\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u958b\u555f\u4e86\u65b0\u7684\u9014\u5f91\u3002\u5118\u7ba1 LLM \u5728\u591a\u6a21\u614b\u7406\u89e3\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u6709\u6548\u5229\u7528\u5176\u529f\u80fd\u9032\u884c ASR \u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u8a13\u7df4\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f37 LLM \u5728 ASR \u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u3002\u6211\u5011\u5efa\u8b70\u5728\u8868\u793a\u767c\u97f3\u7279\u5fb5\u7684\u62fc\u97f3\u5d4c\u5165\u5e8f\u5217\u4e0a\u9810\u8a13\u7df4 LLM\uff0c\u4ee5\u7522\u751f\u76f8\u61c9\u7684\u4e2d\u6587\u6f22\u5b57\u3002\u6b64\u6b65\u9a5f\u4f7f LLM \u80fd\u5920\u5728\u9047\u5230\u771f\u5be6\u8a9e\u97f3\u6578\u64da\u4e4b\u524d\uff0c\u9069\u61c9\u5f9e\u767c\u97f3\u7279\u5fb5\u7522\u751f\u6587\u672c\u3002\u6b64\u5916\uff0c\u6211\u5011\u5fae\u8abf LoRA \u53c3\u6578\u4ee5\u589e\u5f37 LLM \u5c0d\u8a9e\u97f3\u6a21\u614b\u8cc7\u8a0a\u7684\u7406\u89e3\u3002\u5728 AISHELL-1 \u8a9e\u6599\u5eab\u4e2d\uff0c\u8207\u6c92\u6709\u62fc\u97f3\u8f49\u63db\u70ba\u5b57\u5143\u7684\u9810\u8a13\u7df4\u7684\u57fa\u6e96\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728 ASR \u4efb\u52d9\u4e2d\u7522\u751f\u4e86 9.5% \u7684\u76f8\u5c0d\u6539\u9032\u3002\u6b64\u5916\uff0c\u5c07\u8f14\u52a9\u6587\u672c\u6578\u64da\u7d0d\u5165\u62fc\u97f3\u8f49\u63db\u70ba\u5b57\u5143\u7684\u9810\u8a13\u7df4\u9032\u4e00\u6b65\u63d0\u5347\u4e86\u6548\u80fd\uff0c\u9054\u5230\u4e86 19.0% \u7684\u76f8\u5c0d\u6539\u9032\u3002", "author": "Yang Yuhang et.al.", "authors": "Yang Yuhang, Peng Yizhou, Eng Siong Chng, Xionghu Zhong", "id": "2409.16005v1", "paper_url": "http://arxiv.org/abs/2409.16005v1", "repo": "null"}}