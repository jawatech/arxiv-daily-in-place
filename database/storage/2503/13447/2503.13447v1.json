{"2503.13447": {"publish_time": "2025-03-17", "title": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts", "paper_summary": "One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9032\u884c\u8907\u96dc\u63a8\u7406\u6642\uff0c\u9762\u81e8\u7684\u4e00\u500b\u95dc\u9375\u6311\u6230\u662f\u5b83\u5011\u4f9d\u8cf4\u65bc\u5f9e\u8a13\u7df4\u6578\u64da\u4e2d\u5339\u914d\u63a8\u7406\u6a21\u5f0f\uff0c\u800c\u4e0d\u662f\u4e3b\u52d5\u9078\u64c7\u6700\u5408\u9069\u7684\u8a8d\u77e5\u7b56\u7565\u4f86\u89e3\u6c7a\u7d66\u5b9a\u4efb\u52d9\u3002\u73fe\u6709\u65b9\u6cd5\u5f37\u52a0\u4e86\u56fa\u5b9a\u7684\u8a8d\u77e5\u7d50\u69cb\uff0c\u96d6\u7136\u53ef\u4ee5\u63d0\u9ad8\u7279\u5b9a\u4efb\u52d9\u7684\u6027\u80fd\uff0c\u4f46\u5728\u4e0d\u540c\u5834\u666f\u4e0b\u7f3a\u4e4f\u9069\u61c9\u6027\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 METASCALE\uff0c\u9019\u662f\u4e00\u500b\u57fa\u65bc\u5143\u601d\u7dad\u7684\u6e2c\u8a66\u6642\u8abf\u6574\u6846\u67b6\uff0c\u5143\u601d\u7dad\u662f\u91dd\u5c0d\u6bcf\u500b\u4efb\u52d9\u91cf\u8eab\u5b9a\u5236\u7684\u81ea\u9069\u61c9\u601d\u7dad\u7b56\u7565\u3002METASCALE \u521d\u59cb\u5316\u4e00\u500b\u5019\u9078\u5143\u601d\u7dad\u6c60\uff0c\u7136\u5f8c\u4f7f\u7528\u5e36\u6709\u7f6e\u4fe1\u4e0a\u9650\u9078\u64c7\u7684\u591a\u81c2\u8001\u864e\u6a5f\u7b97\u6cd5\uff0c\u5728\u734e\u52f5\u6a21\u578b\u7684\u6307\u5c0e\u4e0b\uff0c\u8fed\u4ee3\u5730\u9078\u64c7\u548c\u8a55\u4f30\u5b83\u5011\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u589e\u5f37\u9069\u61c9\u6027\uff0c\u907a\u50b3\u7b97\u6cd5\u6703\u6f14\u5316\u9ad8\u56de\u5831\u7684\u5143\u601d\u7dad\uff0c\u96a8\u8457\u6642\u9593\u7684\u63a8\u79fb\u4e0d\u65b7\u5b8c\u5584\u548c\u64f4\u5c55\u7b56\u7565\u6c60\u3002\u901a\u904e\u5728\u63a8\u7406\u6642\u52d5\u614b\u63d0\u51fa\u548c\u512a\u5316\u5143\u601d\u7dad\uff0cMETASCALE \u63d0\u9ad8\u4e86\u5404\u7a2e\u4efb\u52d9\u7684\u6e96\u78ba\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cMetaScale \u7684\u8868\u73fe\u4e00\u76f4\u512a\u65bc\u6a19\u6e96\u63a8\u7406\u65b9\u6cd5\uff0c\u5728 GPT-4o \u7684 Arena-Hard \u4e2d\uff0c\u52dd\u7387\u63d0\u9ad8\u4e86 11%\uff0c\u5728\u98a8\u683c\u63a7\u5236\u4e0b\uff0c\u52dd\u7387\u8d85\u904e\u4e86 o1-mini 0.9%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cMETASCALE \u96a8\u8457\u63a1\u6a23\u9810\u7b97\u7684\u589e\u52a0\u800c\u66f4\u6709\u6548\u5730\u64f4\u5c55\uff0c\u4e26\u7522\u751f\u66f4\u7d50\u69cb\u5316\u3001\u5c08\u5bb6\u7d1a\u7684\u97ff\u61c9\u3002\n", "author": "Qin Liu et.al.", "authors": "Qin Liu, Wenxuan Zhou, Nan Xu, James Y. Huang, Fei Wang, Sheng Zhang, Hoifung Poon, Muhao Chen", "id": "2503.13447v1", "paper_url": "http://arxiv.org/abs/2503.13447v1", "repo": "null"}}