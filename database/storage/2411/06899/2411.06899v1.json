{"2411.06899": {"publish_time": "2024-11-11", "title": "LongSafetyBench: Long-Context LLMs Struggle with Safety Issues", "paper_summary": "With the development of large language models (LLMs), the sequence length of\nthese models continues to increase, drawing significant attention to\nlong-context language models. However, the evaluation of these models has been\nprimarily limited to their capabilities, with a lack of research focusing on\ntheir safety. Existing work, such as ManyShotJailbreak, has to some extent\ndemonstrated that long-context language models can exhibit safety concerns.\nHowever, the methods used are limited and lack comprehensiveness. In response,\nwe introduce \\textbf{LongSafetyBench}, the first benchmark designed to\nobjectively and comprehensively evaluate the safety of long-context models.\nLongSafetyBench consists of 10 task categories, with an average length of\n41,889 words. After testing eight long-context language models on\nLongSafetyBench, we found that existing models generally exhibit insufficient\nsafety capabilities. The proportion of safe responses from most mainstream\nlong-context LLMs is below 50\\%. Moreover, models' safety performance in\nlong-context scenarios does not always align with that in short-context\nscenarios. Further investigation revealed that long-context models tend to\noverlook harmful content within lengthy texts. We also proposed a simple yet\neffective solution, allowing open-source models to achieve performance\ncomparable to that of top-tier closed-source models. We believe that\nLongSafetyBench can serve as a valuable benchmark for evaluating the safety\ncapabilities of long-context language models. We hope that our work will\nencourage the broader community to pay attention to the safety of long-context\nmodels and contribute to the development of solutions to improve the safety of\nlong-context LLMs.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u767c\u5c55\uff0c\u9019\u4e9b\u6a21\u578b\u7684\u5e8f\u5217\u9577\u5ea6\u6301\u7e8c\u589e\u52a0\uff0c\u5438\u5f15\u4e86\u4eba\u5011\u5c0d\u9577\u8a9e\u5883\u8a9e\u8a00\u6a21\u578b\u7684\u6975\u5927\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u5c0d\u9019\u4e9b\u6a21\u578b\u7684\u8a55\u4f30\u4e3b\u8981\u4fb7\u9650\u65bc\u5b83\u5011\u7684\u80fd\u529b\uff0c\u800c\u7f3a\u4e4f\u91dd\u5c0d\u5176\u5b89\u5168\u6027\u9032\u884c\u7684\u7814\u7a76\u3002\u73fe\u6709\u7684\u7814\u7a76\uff0c\u4f8b\u5982 ManyShotJailbreak\uff0c\u5728\u67d0\u7a2e\u7a0b\u5ea6\u4e0a\u8b49\u660e\u4e86\u9577\u8a9e\u5883\u8a9e\u8a00\u6a21\u578b\u53ef\u80fd\u6703\u51fa\u73fe\u5b89\u5168\u554f\u984c\u3002\u7136\u800c\uff0c\u6240\u4f7f\u7528\u7684\u65b9\u6cd5\u6709\u9650\u4e14\u7f3a\u4e4f\u5168\u9762\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 \\textbf{LongSafetyBench}\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u65e8\u5728\u5ba2\u89c0\u4e14\u5168\u9762\u8a55\u4f30\u9577\u8a9e\u5883\u6a21\u578b\u5b89\u5168\u6027\u7684\u57fa\u6e96\u3002LongSafetyBench \u5305\u542b 10 \u500b\u4efb\u52d9\u985e\u5225\uff0c\u5e73\u5747\u9577\u5ea6\u70ba 41,889 \u500b\u5b57\u8a5e\u3002\u5728 LongSafetyBench \u4e0a\u6e2c\u8a66\u4e86\u516b\u500b\u9577\u8a9e\u5883\u8a9e\u8a00\u6a21\u578b\u5f8c\uff0c\u6211\u5011\u767c\u73fe\u73fe\u6709\u6a21\u578b\u666e\u904d\u8868\u73fe\u51fa\u5b89\u5168\u6027\u4e0d\u8db3\u3002\u5927\u591a\u6578\u4e3b\u6d41\u9577\u8a9e\u5883 LLM \u7684\u5b89\u5168\u56de\u61c9\u6bd4\u4f8b\u4f4e\u65bc 50%\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u9577\u8a9e\u5883\u5834\u666f\u4e2d\u7684\u5b89\u5168\u6027\u8868\u73fe\u4e26\u4e0d\u7e3d\u662f\u8207\u5728\u77ed\u8a9e\u5883\u5834\u666f\u4e2d\u7684\u8868\u73fe\u4e00\u81f4\u3002\u9032\u4e00\u6b65\u7684\u8abf\u67e5\u986f\u793a\uff0c\u9577\u8a9e\u5883\u6a21\u578b\u50be\u5411\u65bc\u5ffd\u7565\u9577\u7bc7\u6587\u5b57\u4e2d\u7684\u6709\u5bb3\u5167\u5bb9\u3002\u6211\u5011\u9084\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u8b93\u958b\u6e90\u6a21\u578b\u80fd\u5920\u5be6\u73fe\u8207\u9802\u7d1a\u9589\u6e90\u6a21\u578b\u76f8\u7576\u7684\u8868\u73fe\u3002\u6211\u5011\u76f8\u4fe1 LongSafetyBench \u53ef\u4ee5\u4f5c\u70ba\u8a55\u4f30\u9577\u8a9e\u5883\u8a9e\u8a00\u6a21\u578b\u5b89\u5168\u6027\u80fd\u529b\u7684\u5bf6\u8cb4\u57fa\u6e96\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684\u7814\u7a76\u80fd\u9f13\u52f5\u66f4\u5ee3\u6cdb\u7684\u793e\u7fa4\u95dc\u6ce8\u9577\u8a9e\u5883\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u4e26\u70ba\u958b\u767c\u89e3\u6c7a\u65b9\u6848\u4ee5\u6539\u5584\u9577\u8a9e\u5883 LLM \u7684\u5b89\u5168\u6027\u505a\u51fa\u8ca2\u737b\u3002", "author": "Mianqiu Huang et.al.", "authors": "Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang", "id": "2411.06899v1", "paper_url": "http://arxiv.org/abs/2411.06899v1", "repo": "null"}}