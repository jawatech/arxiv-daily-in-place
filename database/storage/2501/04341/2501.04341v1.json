{"2501.04341": {"publish_time": "2025-01-08", "title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting", "paper_summary": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2.", "paper_summary_zh": "\u93c8\u5f0f\u601d\u7dad (CoT) \u63d0\u793a\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u589e\u5f37\u8907\u96dc\u63a8\u7406\u7684\u4e3b\u5c0e\u7bc4\u4f8b\u3002\u5b83\u5f15\u5c0e LLM \u63d0\u51fa\u591a\u6b65\u9a5f\u63a8\u7406\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u751f\u6210\u6700\u7d42\u7b54\u6848\u3002\n\u7136\u800c\uff0c\u7576\u63a8\u7406\u6240\u9700\u7684\u91cd\u8981\u8cc7\u8a0a\u662f\u96b1\u542b\u6216\u907a\u5931\u6642\uff0cCoT \u6703\u9047\u5230\u56f0\u96e3\u3002\u9019\u662f\u56e0\u70ba CoT \u5f37\u8abf\u63a8\u7406\u6b65\u9a5f\u7684\u9806\u5e8f\uff0c\u540c\u6642\u5ffd\u7565\u4e86\u65e9\u671f\u8403\u53d6\u57fa\u672c\u8cc7\u8a0a\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba\u53cd\u8986\u6458\u8981\u9810\u63d0\u793a (ISP^2) \u7684\u9810\u63d0\u793a\u65b9\u6cd5\uff0c\u4ee5\u5728\u672a\u660e\u78ba\u63d0\u4f9b\u95dc\u9375\u8cc7\u8a0a\u6642\u6539\u5584 LLM \u63a8\u7406\u3002\u9996\u5148\uff0c\u8403\u53d6\u5be6\u9ad4\u53ca\u5176\u5c0d\u61c9\u63cf\u8ff0\u4ee5\u5f62\u6210\u6f5b\u5728\u95dc\u9375\u8cc7\u8a0a\u5c0d\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u4f7f\u7528\u53ef\u9760\u6027\u8a55\u5206\u8a55\u4f30\u9019\u4e9b\u5c0d\uff0c\u7136\u5f8c\u5c07\u6392\u540d\u6700\u4f4e\u7684\u5169\u500b\u5c0d\u5408\u4f75\u6210\u4e00\u500b\u65b0\u7684\u5be6\u9ad4\u63cf\u8ff0\u3002\u6b64\u904e\u7a0b\u6703\u91cd\u8907\u9032\u884c\uff0c\u76f4\u5230\u7372\u5f97\u552f\u4e00\u7684\u95dc\u9375\u8cc7\u8a0a\u5c0d\u3002\u6700\u5f8c\uff0c\u5c07\u8a72\u5c0d\u9023\u540c\u539f\u59cb\u554f\u984c\u8f38\u5165 LLM \u4ee5\u7522\u751f\u7b54\u6848\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6539\u9032\u4e86 7.1%\u3002\u8207\u50b3\u7d71\u63d0\u793a\u4e0d\u540c\uff0cISP^2 \u63a1\u7528\u9810\u63d0\u793a\u7684\u6b78\u7d0d\u65b9\u6cd5\uff0c\u63d0\u4f9b\u9748\u6d3b\u6574\u5408\u5230\u4e0d\u540c\u7684\u63a8\u7406\u6846\u67b6\u4e2d\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/zdhgreat/ISP-2 \u53d6\u5f97\u3002", "author": "Dong-Hai Zhu et.al.", "authors": "Dong-Hai Zhu, Yu-Jie Xiong, Jia-Chen Zhang, Xi-Jiong Xie, Chun-Ming Xia", "id": "2501.04341v1", "paper_url": "http://arxiv.org/abs/2501.04341v1", "repo": "https://github.com/zdhgreat/isp-2"}}