{"2406.17557": {"publish_time": "2024-06-25", "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale", "paper_summary": "The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6548\u80fd\u53d6\u6c7a\u65bc\u5176\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u7684\u54c1\u8cea\u548c\u898f\u6a21\u3002\u7136\u800c\uff0c\u6700\u5148\u9032\u7684\u958b\u653e LLM\uff08\u4f8b\u5982 Llama 3 \u548c Mixtral\uff09\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u4e26\u672a\u516c\u958b\uff0c\u800c\u4e14\u6211\u5011\u5c0d\u65bc\u5b83\u5011\u7684\u5efa\u7acb\u65b9\u5f0f\u6240\u77e5\u751a\u5c11\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 FineWeb\uff0c\u9019\u662f\u4e00\u500b\u7531 96 \u500b Common Crawl \u5feb\u7167\u884d\u751f\u7684 15 \u5146\u500b\u4ee3\u5e63\u7684\u8cc7\u6599\u96c6\uff0c\u5b83\u7522\u751f\u7684 LLM \u6548\u80fd\u512a\u65bc\u5176\u4ed6\u958b\u653e\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u3002\u70ba\u4e86\u589e\u9032\u6211\u5011\u5c0d\u65bc\u5982\u4f55\u6700\u4f73\u7b56\u5283\u9ad8\u54c1\u8cea\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u7684\u7406\u89e3\uff0c\u6211\u5011\u4ed4\u7d30\u8a18\u9304\u4e26\u6d88\u878d\u4e86 FineWeb \u4e2d\u4f7f\u7528\u7684\u6240\u6709\u8a2d\u8a08\u9078\u64c7\uff0c\u5305\u62ec\u5c0d\u91cd\u8907\u8cc7\u6599\u522a\u9664\u548c\u904e\u6ffe\u7b56\u7565\u7684\u6df1\u5165\u63a2\u8a0e\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86 FineWeb-Edu\uff0c\u9019\u662f\u4e00\u500b\u5f9e FineWeb \u4e2d\u904e\u6ffe\u51fa\u7684 1.3 \u5146\u500b\u4ee3\u5e63\u7684\u6559\u80b2\u6587\u672c\u96c6\u5408\u3002\u5728 FineWeb-Edu \u4e0a\u9810\u8a13\u7df4\u7684 LLM \u5728\u77e5\u8b58\u548c\u63a8\u7406\u5bc6\u96c6\u7684\u57fa\u6e96\uff08\u4f8b\u5982 MMLU \u548c ARC\uff09\u4e0a\u5c55\u73fe\u51fa\u986f\u8457\u66f4\u597d\u7684\u6548\u80fd\u3002\u9664\u4e86\u6211\u5011\u7684\u8cc7\u6599\u96c6\u4e4b\u5916\uff0c\u6211\u5011\u9084\u516c\u958b\u767c\u5e03\u4e86\u6211\u5011\u7684\u8cc7\u6599\u7b56\u5283\u7a0b\u5f0f\u78bc\u5eab\u4ee5\u53ca\u5728\u6211\u5011\u7684\u6d88\u878d\u5be6\u9a57\u4e2d\u8a13\u7df4\u7684\u6240\u6709\u6a21\u578b\u3002", "author": "Guilherme Penedo et.al.", "authors": "Guilherme Penedo, Hynek Kydl\u00ed\u010dek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf", "id": "2406.17557v1", "paper_url": "http://arxiv.org/abs/2406.17557v1", "repo": "null"}}