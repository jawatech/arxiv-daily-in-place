{"2501.13080": {"publish_time": "2025-01-22", "title": "Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment", "paper_summary": "Large Language Models (LLMs) have demonstrated powerful capabilities that\nrender them valuable in different applications, including conversational AI\nproducts. It is paramount to ensure the security and reliability of these\nproducts by mitigating their vulnerabilities towards malicious user\ninteractions, which can lead to the exposure of great risks and reputational\nrepercussions. In this work, we present a comprehensive study on the efficacy\nof fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs\nthat serve as input moderation guardrails. We systematically explore various\ntuning methods by leveraging a small set of training data to adapt these models\nas proxy defense mechanisms to detect malicious inputs and provide a reasoning\nfor their verdicts, thereby preventing the exploitation of conversational\nagents. We rigorously evaluate the efficacy and robustness of different tuning\nstrategies to generalize across diverse adversarial and malicious query types.\nOur experimental results outline the potential of alignment processes tailored\nto a varied range of harmful input queries, even with constrained data\nresources. These techniques significantly enhance the safety of conversational\nAI systems and provide a feasible framework for deploying more secure and\ntrustworthy AI-driven interactions.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u5f37\u5927\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u4e0d\u540c\u7684\u61c9\u7528\u7a0b\u5f0f\u4e2d\u6709\u50f9\u503c\uff0c\u5305\u62ec\u5c0d\u8a71\u5f0f AI \u7522\u54c1\u3002\u900f\u904e\u6e1b\u8f15\u5176\u5c0d\u60e1\u610f\u4f7f\u7528\u8005\u4e92\u52d5\u7684\u6f0f\u6d1e\uff0c\u78ba\u4fdd\u9019\u4e9b\u7522\u54c1\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u95dc\u91cd\u8981\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u91cd\u5927\u98a8\u96aa\u548c\u540d\u8b7d\u640d\u5bb3\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c0d\u5fae\u8abf\u548c\u8abf\u6574\u4e0d\u540c LLM \u7684\u601d\u8003\u93c8 (CoT) \u56de\u61c9\u7684\u529f\u6548\u9032\u884c\u5168\u9762\u7814\u7a76\uff0c\u9019\u4e9b\u56de\u61c9\u53ef\u7528\u4f5c\u8f38\u5165\u5be9\u6838\u9632\u8b77\u63aa\u65bd\u3002\u6211\u5011\u7cfb\u7d71\u6027\u5730\u63a2\u7d22\u5404\u7a2e\u8abf\u6574\u65b9\u6cd5\uff0c\u5229\u7528\u4e00\u5c0f\u7d44\u8a13\u7df4\u8cc7\u6599\u4f86\u8abf\u6574\u9019\u4e9b\u6a21\u578b\uff0c\u4f5c\u70ba\u4ee3\u7406\u9632\u79a6\u6a5f\u5236\u4f86\u5075\u6e2c\u60e1\u610f\u8f38\u5165\u4e26\u70ba\u5176\u5224\u6c7a\u63d0\u4f9b\u7406\u7531\uff0c\u5f9e\u800c\u9632\u6b62\u5c0d\u8a71\u5f0f\u4ee3\u7406\u7684\u5229\u7528\u3002\u6211\u5011\u56b4\u683c\u8a55\u4f30\u4e0d\u540c\u8abf\u6574\u7b56\u7565\u7684\u529f\u6548\u548c\u7a69\u5065\u6027\uff0c\u4ee5\u6982\u62ec\u5404\u7a2e\u5c0d\u6297\u6027\u548c\u60e1\u610f\u67e5\u8a62\u985e\u578b\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u6982\u8ff0\u4e86\u8abf\u6574\u6d41\u7a0b\u7684\u6f5b\u529b\uff0c\u9019\u4e9b\u6d41\u7a0b\u91dd\u5c0d\u5404\u7a2e\u6709\u5bb3\u8f38\u5165\u67e5\u8a62\u9032\u884c\u8abf\u6574\uff0c\u5373\u4f7f\u5728\u8cc7\u6599\u8cc7\u6e90\u53d7\u9650\u7684\u60c5\u6cc1\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u9019\u4e9b\u6280\u8853\u986f\u8457\u589e\u5f37\u4e86\u5c0d\u8a71\u5f0f AI \u7cfb\u7d71\u7684\u5b89\u5168\u6027\uff0c\u4e26\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u884c\u7684\u6846\u67b6\uff0c\u7528\u65bc\u90e8\u7f72\u66f4\u5b89\u5168\u4e14\u503c\u5f97\u4fe1\u8cf4\u7684 AI \u9a45\u52d5\u4e92\u52d5\u3002", "author": "Melissa Kazemi Rad et.al.", "authors": "Melissa Kazemi Rad, Huy Nghiem, Andy Luo, Sahil Wadhwa, Mohammad Sorower, Stephen Rawls", "id": "2501.13080v1", "paper_url": "http://arxiv.org/abs/2501.13080v1", "repo": "null"}}