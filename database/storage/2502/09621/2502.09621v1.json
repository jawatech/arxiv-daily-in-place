{"2502.09621": {"publish_time": "2025-02-13", "title": "MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency", "paper_summary": "Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/", "paper_summary_zh": "<paragraph>\u900f\u904e\u601d\u7dad\u93c8\uff08CoT\uff09\u56de\u7b54\u554f\u984c\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5c0d\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b\uff08LMM\uff09\u7684\u5f71\u97ff\u4ecd\u7f3a\u4e4f\u7cfb\u7d71\u6027\u7684\u8a55\u4f30\u548c\u6df1\u5165\u63a2\u8a0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 MME-CoT\uff0c\u4e00\u500b\u5c08\u9580\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u7528\u65bc\u8a55\u4f30 LMM \u7684 CoT \u63a8\u7406\u6548\u80fd\uff0c\u6db5\u84cb\u516d\u500b\u9818\u57df\uff1a\u6578\u5b78\u3001\u79d1\u5b78\u3001OCR\u3001\u908f\u8f2f\u3001\u6642\u7a7a\u548c\u4e00\u822c\u5834\u666f\u3002\u4f5c\u70ba\u8a72\u9818\u57df\u7684\u7b2c\u4e00\u500b\u5168\u9762\u6027\u7814\u7a76\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5168\u9762\u7684\u8a55\u4f30\u5957\u4ef6\uff0c\u5305\u542b\u4e09\u500b\u5275\u65b0\u7684\u6307\u6a19\uff0c\u7528\u65bc\u8a55\u4f30\u63a8\u7406\u54c1\u8cea\u3001\u7a69\u5065\u6027\u548c\u6548\u7387\uff0c\u4e26\u9054\u5230\u7d30\u5fae\u7684\u5c64\u7d1a\u3002\u900f\u904e\u5229\u7528\u7b56\u5c55\u7684\u9ad8\u54c1\u8cea\u8cc7\u6599\u548c\u7368\u7279\u7684\u8a55\u4f30\u7b56\u7565\uff0c\u6211\u5011\u5c0d\u6700\u5148\u9032\u7684 LMM \u9032\u884c\u6df1\u5165\u5206\u6790\uff0c\u767c\u73fe\u4e86\u5e7e\u500b\u95dc\u9375\u898b\u89e3\uff1a1\uff09\u5177\u6709\u53cd\u601d\u6a5f\u5236\u7684\u6a21\u578b\u5c55\u73fe\u51fa\u512a\u7570\u7684 CoT \u54c1\u8cea\uff0c\u5176\u4e2d Kimi k1.5 \u512a\u65bc GPT-4o\uff0c\u4e26\u5c55\u73fe\u51fa\u6700\u9ad8\u54c1\u8cea\u7684\u7d50\u679c\uff1b2\uff09CoT \u63d0\u793a\u901a\u5e38\u6703\u964d\u4f4e LMM \u5728\u611f\u77e5\u5bc6\u96c6\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\uff0c\u9019\u8868\u793a\u6f5b\u5728\u6709\u5bb3\u7684\u904e\u5ea6\u601d\u8003\u884c\u70ba\uff1b3\uff09\u5118\u7ba1 CoT \u54c1\u8cea\u5f88\u9ad8\uff0c\u4f46\u5177\u6709\u53cd\u601d\u80fd\u529b\u7684 LMM \u5728\u4e00\u822c\u56de\u61c9\u548c\u81ea\u6211\u4fee\u6b63\u968e\u6bb5\u90fd\u5c55\u73fe\u51fa\u986f\u8457\u7684\u4f4e\u6548\u7387\u3002\u6211\u5011\u5e0c\u671b MME-CoT \u80fd\u4f5c\u70ba\u4fc3\u9032 LMM \u4e2d\u591a\u6a21\u614b\u63a8\u7406\u7684\u57fa\u790e\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://mmecot.github.io/</paragraph>", "author": "Dongzhi Jiang et.al.", "authors": "Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li", "id": "2502.09621v1", "paper_url": "http://arxiv.org/abs/2502.09621v1", "repo": "null"}}