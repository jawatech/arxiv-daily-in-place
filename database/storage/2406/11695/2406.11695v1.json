{"2406.11695": {"publish_time": "2024-06-17", "title": "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "paper_summary": "Language Model Programs, i.e. sophisticated pipelines of modular language\nmodel (LM) calls, are increasingly advancing NLP tasks, but they require\ncrafting prompts that are jointly effective for all modules. We study prompt\noptimization for LM programs, i.e. how to update these prompts to maximize a\ndownstream metric without access to module-level labels or gradients. To make\nthis tractable, we factorize our problem into optimizing the free-form\ninstructions and few-shot demonstrations of every module and introduce several\nstrategies to craft task-grounded instructions and navigate credit assignment\nacross modules. Our strategies include (i) program- and data-aware techniques\nfor proposing effective instructions, (ii) a stochastic mini-batch evaluation\nfunction for learning a surrogate model of our objective, and (iii) a\nmeta-optimization procedure in which we refine how LMs construct proposals over\ntime. Using these insights we develop MIPRO, a novel optimizer that outperforms\nbaselines on five of six diverse LM programs using a best-in-class open-source\nmodel (Llama-3-8B), by as high as 12.9% accuracy. We will release our new\noptimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u7a0b\u5f0f\uff0c\u5373\u6a21\u7d44\u5316\u8a9e\u8a00\u6a21\u578b (LM) \u547c\u53eb\u7684\u8907\u96dc\u7ba1\u7dda\uff0c\u6b63\u65e5\u76ca\u63d0\u5347 NLP \u4efb\u52d9\uff0c\u4f46\u5b83\u5011\u9700\u8981\u88fd\u4f5c\u5c0d\u6240\u6709\u6a21\u7d44\u90fd\u5171\u540c\u6709\u6548\u7684\u63d0\u793a\u3002\u6211\u5011\u7814\u7a76 LM \u7a0b\u5f0f\u7684\u63d0\u793a\u6700\u4f73\u5316\uff0c\u5373\u5982\u4f55\u5728\u6c92\u6709\u6a21\u7d44\u5c64\u7d1a\u6a19\u7c64\u6216\u68af\u5ea6\u7684\u72c0\u6cc1\u4e0b\u66f4\u65b0\u9019\u4e9b\u63d0\u793a\uff0c\u4ee5\u6700\u5927\u5316\u4e0b\u6e38\u6307\u6a19\u3002\u70ba\u4e86\u4f7f\u9019\u500b\u554f\u984c\u53ef\u8655\u7406\uff0c\u6211\u5011\u5c07\u554f\u984c\u5206\u89e3\u70ba\u6700\u4f73\u5316\u6bcf\u4e00\u500b\u6a21\u7d44\u7684\u81ea\u7531\u5f62\u5f0f\u8aaa\u660e\u548c\u5c11\u91cf\u793a\u7bc4\uff0c\u4e26\u5f15\u5165\u591a\u7a2e\u7b56\u7565\u4f86\u88fd\u4f5c\u4ee5\u4efb\u52d9\u70ba\u57fa\u790e\u7684\u8aaa\u660e\uff0c\u4e26\u5728\u6a21\u7d44\u9593\u5c0e\u822a\u4fe1\u7528\u5206\u914d\u3002\u6211\u5011\u7684\u7b56\u7565\u5305\u62ec\uff1a(i) \u63d0\u51fa\u6709\u6548\u8aaa\u660e\u7684\u7a0b\u5f0f\u548c\u8cc7\u6599\u611f\u77e5\u6280\u8853\uff0c(ii) \u5b78\u7fd2\u76ee\u6a19\u66ff\u4ee3\u6a21\u578b\u7684\u96a8\u6a5f\u5c0f\u6279\u6b21\u8a55\u4f30\u51fd\u6578\uff0c\u4ee5\u53ca (iii) \u6211\u5011\u5728\u5176\u4e2d\u7cbe\u9032 LM \u96a8\u8457\u6642\u9593\u63a8\u79fb\u5efa\u69cb\u63d0\u6848\u65b9\u5f0f\u7684\u5143\u6700\u4f73\u5316\u7a0b\u5e8f\u3002\u5229\u7528\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u958b\u767c\u4e86 MIPRO\uff0c\u4e00\u7a2e\u65b0\u7684\u6700\u4f73\u5316\u5668\uff0c\u5b83\u5728\u4f7f\u7528\u6700\u4f73\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b (Llama-3-8B) \u7684\u516d\u500b\u4e0d\u540c\u7684 LM \u7a0b\u5f0f\u4e2d\u7684\u4e94\u500b\u7a0b\u5f0f\u4e2d\u512a\u65bc\u57fa\u6e96\uff0c\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86 12.9%\u3002\u6211\u5011\u5c07\u5728 https://github.com/stanfordnlp/dspy \u4e0a\u767c\u5e03\u6211\u5011\u7684\u65b0\u7684\u6700\u4f73\u5316\u5668\u548c DSPy \u57fa\u6e96", "author": "Krista Opsahl-Ong et.al.", "authors": "Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, Omar Khattab", "id": "2406.11695v1", "paper_url": "http://arxiv.org/abs/2406.11695v1", "repo": "https://github.com/stanfordnlp/dspy"}}