{"2411.13451": {"publish_time": "2024-11-20", "title": "AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations", "paper_summary": "State-of-the-art multimodal web agents, powered by Multimodal Large Language\nModels (MLLMs), can autonomously execute many web tasks by processing user\ninstructions and interacting with graphical user interfaces (GUIs). Current\nstrategies for building web agents rely on (i) the generalizability of\nunderlying MLLMs and their steerability via prompting, and (ii) large-scale\nfine-tuning of MLLMs on web-related tasks. However, web agents still struggle\nto automate tasks on unseen websites and domains, limiting their applicability\nto enterprise-specific and proprietary platforms. Beyond generalization from\nlarge-scale pre-training and fine-tuning, we propose building agents for\nfew-shot adaptability using human demonstrations. We introduce the AdaptAgent\nframework that enables both proprietary and open-weights multimodal web agents\nto adapt to new websites and domains using few human demonstrations (up to 2).\nOur experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show\nthat using in-context demonstrations (for proprietary models) or\nmeta-adaptation demonstrations (for meta-learned open-weights models) boosts\ntask success rate by 3.36% to 7.21% over non-adapted state-of-the-art models,\ncorresponding to a relative increase of 21.03% to 65.75%. Furthermore, our\nadditional analyses (a) show the effectiveness of multimodal demonstrations\nover text-only ones, (b) shed light on the influence of different data\nselection strategies during meta-learning on the generalization of the agent,\nand (c) demonstrate the effect of number of few-shot examples on the web\nagent's success rate. Overall, our results unlock a complementary axis for\ndeveloping widely applicable multimodal web agents beyond large-scale\npre-training and fine-tuning, emphasizing few-shot adaptability.", "paper_summary_zh": "<paragraph>\u7531\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u9a71\u52a8\u7684\u6700\u5148\u8fdb\u591a\u6a21\u6001\u7f51\u7edc\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u5904\u7406\u7528\u6237\u6307\u4ee4\u5e76\u4e0e\u56fe\u5f62\u7528\u6237\u754c\u9762 (GUI) \u4ea4\u4e92\u6765\u81ea\u4e3b\u6267\u884c\u8bb8\u591a\u7f51\u7edc\u4efb\u52a1\u3002\u5f53\u524d\u6784\u5efa\u7f51\u7edc\u4ee3\u7406\u7684\u7b56\u7565\u4f9d\u8d56\u4e8e (i) \u57fa\u7840 MLLM \u7684\u6cdb\u5316\u6027\u548c\u901a\u8fc7\u63d0\u793a\u8fdb\u884c\u7684\u53ef\u64cd\u7eb5\u6027\uff0c\u4ee5\u53ca (ii) \u5728\u4e0e\u7f51\u7edc\u76f8\u5173\u7684\u4efb\u52a1\u4e0a\u5bf9 MLLM \u8fdb\u884c\u5927\u89c4\u6a21\u5fae\u8c03\u3002\u7136\u800c\uff0c\u7f51\u7edc\u4ee3\u7406\u4ecd\u7136\u96be\u4ee5\u5728\u770b\u4e0d\u89c1\u7684\u7f51\u7ad9\u548c\u57df\u4e0a\u81ea\u52a8\u6267\u884c\u4efb\u52a1\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u7279\u5b9a\u4e8e\u4f01\u4e1a\u548c\u4e13\u6709\u5e73\u53f0\u7684\u9002\u7528\u6027\u3002\u9664\u4e86\u4ece\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u8fdb\u884c\u6cdb\u5316\u4e4b\u5916\uff0c\u6211\u4eec\u5efa\u8bae\u4f7f\u7528\u4eba\u7c7b\u6f14\u793a\u6784\u5efa\u5177\u6709\u5c11\u91cf\u9002\u5e94\u6027\u7684\u4ee3\u7406\u3002\u6211\u4eec\u5f15\u5165\u4e86 AdaptAgent \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u4e13\u6709\u548c\u5f00\u653e\u6743\u91cd\u7684\u591a\u6a21\u6001\u7f51\u7edc\u4ee3\u7406\u80fd\u591f\u4f7f\u7528\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\uff08\u6700\u591a 2 \u4e2a\uff09\u9002\u5e94\u65b0\u7684\u7f51\u7ad9\u548c\u57df\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u6d41\u884c\u57fa\u51c6 Mind2Web \u548c VisualWebArena \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u60c5\u5883\u4e2d\u6f14\u793a\uff08\u9488\u5bf9\u4e13\u6709\u6a21\u578b\uff09\u6216\u5143\u9002\u5e94\u6f14\u793a\uff08\u9488\u5bf9\u5143\u5b66\u4e60\u5f00\u653e\u6743\u91cd\u6a21\u578b\uff09\u5c06\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e86 3.36% \u81f3 7.21%\uff0c\u9ad8\u4e8e\u672a\u9002\u5e94\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u76f8\u5f53\u4e8e\u76f8\u5bf9\u589e\u52a0\u4e86 21.03% \u81f3 65.75%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u9644\u52a0\u5206\u6790 (a) \u663e\u793a\u4e86\u591a\u6a21\u6001\u6f14\u793a\u4f18\u4e8e\u4ec5\u6587\u672c\u6f14\u793a\u7684\u6709\u6548\u6027\uff0c(b) \u9610\u660e\u4e86\u5143\u5b66\u4e60\u671f\u95f4\u4e0d\u540c\u6570\u636e\u9009\u62e9\u7b56\u7565\u5bf9\u4ee3\u7406\u6cdb\u5316\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca (c) \u8bc1\u660e\u4e86\u5c11\u91cf\u793a\u4f8b\u6570\u91cf\u5bf9\u7f51\u7edc\u4ee3\u7406\u6210\u529f\u7387\u7684\u5f71\u54cd\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u4e3a\u5f00\u53d1\u5e7f\u6cdb\u9002\u7528\u7684\u591a\u6a21\u6001\u7f51\u7edc\u4ee3\u7406\u89e3\u9501\u4e86\u4e00\u4e2a\u8865\u5145\u8f74\uff0c\u8d85\u51fa\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u5f3a\u8c03\u4e86\u5c11\u91cf\u9002\u5e94\u6027\u3002</paragraph>", "author": "Gaurav Verma et.al.", "authors": "Gaurav Verma, Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Tucker Balch, Manuela Veloso", "id": "2411.13451v1", "paper_url": "http://arxiv.org/abs/2411.13451v1", "repo": "null"}}