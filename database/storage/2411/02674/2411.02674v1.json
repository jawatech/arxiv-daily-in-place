{"2411.02674": {"publish_time": "2024-11-04", "title": "Wave Network: An Ultra-Small Language Model", "paper_summary": "We propose an innovative token representation and update method in a new\nultra-small language model: the Wave network. Specifically, we use a\n\\textbf{complex vector} to represent each token, encoding both global and local\nsemantics of the input text. A \\textbf{complex vector} consists of two\ncomponents: a magnitude vector representing the \\textit{global semantics} of\nthe input text, and a phase vector capturing the \\textit{relationships between\nindividual tokens and global semantics}. Experiments on the AG News text\nclassification task demonstrate that, when generating complex vectors from\nrandomly initialized token embeddings, our single-layer Wave Network achieves\n90.91\\% accuracy with wave interference and 91.66\\% with wave modulation --\noutperforming a single Transformer layer using BERT pre-trained embeddings by\n19.23\\% and 19.98\\%, respectively, and approaching the accuracy of the\npre-trained and fine-tuned BERT base model (94.64\\%). Additionally, compared to\nBERT base, the Wave Network reduces video memory usage and training time by\n77.34\\% and 85.62\\% during wave modulation. In summary, we used a\n2.4-million-parameter small language model to achieve accuracy comparable to a\n100-million-parameter BERT model in text classification.", "paper_summary_zh": "<paragraph>\u6211\u5011\u5728\u4e00\u500b\u65b0\u7684\u8d85\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\u63d0\u51fa\u5275\u65b0\u7684\u4ee3\u5e63\u8868\u793a\u548c\u66f4\u65b0\u65b9\u6cd5\uff1aWave \u7db2\u8def\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u4f7f\u7528\u4e00\u500b**\u8907\u6578\u5411\u91cf**\u4f86\u8868\u793a\u6bcf\u500b\u4ee3\u5e63\uff0c\u7de8\u78bc\u8f38\u5165\u6587\u672c\u7684\u5168\u5c40\u548c\u5c40\u90e8\u8a9e\u7fa9\u3002\u4e00\u500b**\u8907\u6578\u5411\u91cf**\u5305\u542b\u5169\u500b\u7d44\u6210\u90e8\u5206\uff1a\u4e00\u500b\u8868\u793a\u8f38\u5165\u6587\u672c\u7684**\u5168\u5c40\u8a9e\u7fa9**\u7684\u5e45\u5ea6\u5411\u91cf\uff0c\u4ee5\u53ca\u4e00\u500b\u6355\u6349**\u500b\u5225\u4ee3\u5e63\u548c\u5168\u5c40\u8a9e\u7fa9\u4e4b\u9593\u7684\u95dc\u4fc2**\u7684\u76f8\u4f4d\u5411\u91cf\u3002\u5728 AG \u65b0\u805e\u6587\u672c\u5206\u985e\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0c\u7576\u5f9e\u96a8\u6a5f\u521d\u59cb\u5316\u7684\u4ee3\u5e63\u5d4c\u5165\u4e2d\u751f\u6210\u8907\u6578\u5411\u91cf\u6642\uff0c\u6211\u5011\u7684\u55ae\u5c64 Wave \u7db2\u8def\u5728\u6ce2\u5e72\u6d89\u4e0b\u9054\u5230 90.91% \u7684\u6e96\u78ba\u5ea6\uff0c\u5728\u6ce2\u8abf\u88fd\u4e0b\u9054\u5230 91.66% \u7684\u6e96\u78ba\u5ea6\u2014\u2014\u5206\u5225\u6bd4\u4f7f\u7528 BERT \u9810\u8a13\u7df4\u5d4c\u5165\u7684\u55ae\u5c64 Transformer \u5c64\u9ad8\u51fa 19.23% \u548c 19.98%\uff0c\u4e26\u4e14\u63a5\u8fd1\u9810\u8a13\u7df4\u548c\u5fae\u8abf\u7684 BERT \u57fa\u790e\u6a21\u578b\uff0894.64%\uff09\u7684\u6e96\u78ba\u5ea6\u3002\u6b64\u5916\uff0c\u8207 BERT \u57fa\u790e\u6a21\u578b\u76f8\u6bd4\uff0cWave \u7db2\u8def\u5728\u6ce2\u8abf\u88fd\u671f\u9593\u5c07\u8996\u8a0a\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u548c\u8a13\u7df4\u6642\u9593\u5206\u5225\u6e1b\u5c11\u4e86 77.34% \u548c 85.62%\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u4f7f\u7528\u4e86\u4e00\u500b 240 \u842c\u53c3\u6578\u7684\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u5728\u6587\u672c\u5206\u985e\u4e2d\u9054\u5230\u4e86\u8207 1 \u5104\u53c3\u6578 BERT \u6a21\u578b\u76f8\u7576\u7684\u6e96\u78ba\u5ea6\u3002</paragraph>", "author": "Xin Zhang et.al.", "authors": "Xin Zhang, Victor S. Sheng", "id": "2411.02674v1", "paper_url": "http://arxiv.org/abs/2411.02674v1", "repo": "null"}}