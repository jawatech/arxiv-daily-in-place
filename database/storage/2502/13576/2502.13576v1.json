{"2502.13576": {"publish_time": "2025-02-19", "title": "Beyond One-Size-Fits-All: Tailored Benchmarks for Efficient Evaluation", "paper_summary": "Evaluating models on large benchmarks is very resource-intensive, especially\nduring the period of rapid model evolution. Existing efficient evaluation\nmethods estimate the performance of target models by testing them only on a\nsmall and static coreset of the benchmark, which is derived from the publicly\navailable evaluation results of source models. These methods rely on the\nassumption that target models have high prediction consistency with source\nmodels. However, we demonstrate that it doesn't generalize well in practice. To\nalleviate the inconsistency issue, we present TailoredBench, a method that\nconducts customized evaluation tailored to each target model. Specifically, a\nGlobal-coreset is first constructed as a probe to identify the most consistent\nsource models for each target model with an adaptive source model selection\nstrategy. Afterwards, a scalable K-Medoids clustering algorithm is proposed to\nextend the Global-coreset to a tailored Native-coreset for each target model.\nAccording to the predictions on Native-coresets, we obtain the performance of\ntarget models on the whole benchmark with a calibrated estimation strategy.\nComprehensive experiments on 5 benchmarks across over 300 models demonstrate\nthat compared to best performing baselines, TailoredBench achieves an average\nreduction of 31.4% in MAE of accuracy estimates under the same inference\nbudgets, showcasing strong effectiveness and generalizability.", "paper_summary_zh": "\u5728\u5927\u578b\u57fa\u51c6\u4e0a\u8a55\u4f30\u6a21\u578b\u975e\u5e38\u8017\u8cbb\u8cc7\u6e90\uff0c\u7279\u5225\u662f\u5728\u6a21\u578b\u5feb\u901f\u6f14\u5316\u7684\u6642\u671f\u3002\u73fe\u6709\u7684\u9ad8\u6548\u8a55\u4f30\u65b9\u6cd5\u900f\u904e\u50c5\u5728\u57fa\u6e96\u7684\u5c0f\u578b\u4e14\u975c\u614b\u6838\u5fc3\u96c6\u4e0a\u5c0d\u76ee\u6a19\u6a21\u578b\u9032\u884c\u6e2c\u8a66\u4f86\u4f30\u8a08\u5176\u6548\u80fd\uff0c\u6838\u5fc3\u96c6\u53d6\u81ea\u516c\u958b\u7684\u4f86\u6e90\u6a21\u578b\u8a55\u4f30\u7d50\u679c\u3002\u9019\u4e9b\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u76ee\u6a19\u6a21\u578b\u8207\u4f86\u6e90\u6a21\u578b\u5177\u6709\u9ad8\u5ea6\u9810\u6e2c\u4e00\u81f4\u6027\u7684\u5047\u8a2d\u3002\u7136\u800c\uff0c\u6211\u5011\u8b49\u660e\u9019\u5728\u5be6\u52d9\u4e2d\u7121\u6cd5\u5f88\u597d\u5730\u6982\u5316\u3002\u70ba\u4e86\u6e1b\u8f15\u4e0d\u4e00\u81f4\u6027\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa TailoredBench\uff0c\u9019\u662f\u4e00\u7a2e\u91dd\u5c0d\u6bcf\u500b\u76ee\u6a19\u6a21\u578b\u9032\u884c\u5ba2\u88fd\u5316\u8a55\u4f30\u7684\u65b9\u6cd5\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u9996\u5148\u5efa\u69cb\u4e00\u500b Global-coreset \u4f5c\u70ba\u63a2\u91dd\uff0c\u4ee5\u8b58\u5225\u6bcf\u500b\u76ee\u6a19\u6a21\u578b\u6700\u4e00\u81f4\u7684\u4f86\u6e90\u6a21\u578b\uff0c\u4e26\u63a1\u7528\u81ea\u9069\u61c9\u4f86\u6e90\u6a21\u578b\u9078\u64c7\u7b56\u7565\u3002\u4e4b\u5f8c\uff0c\u63d0\u51fa\u4e00\u500b\u53ef\u64f4\u5145\u7684 K-Medoids \u805a\u985e\u6f14\u7b97\u6cd5\uff0c\u5c07 Global-coreset \u5ef6\u4f38\u70ba\u6bcf\u500b\u76ee\u6a19\u6a21\u578b\u7684\u5ba2\u88fd\u5316 Native-coreset\u3002\u6839\u64da Native-coreset \u7684\u9810\u6e2c\uff0c\u6211\u5011\u4ee5\u6821\u6e96\u7684\u4f30\u8a08\u7b56\u7565\u53d6\u5f97\u76ee\u6a19\u6a21\u578b\u5728\u6574\u500b\u57fa\u6e96\u4e0a\u7684\u6548\u80fd\u3002\u91dd\u5c0d\u8d85\u904e 300 \u500b\u6a21\u578b\u7684 5 \u500b\u57fa\u6e96\u9032\u884c\u7684\u5168\u9762\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u6548\u80fd\u6700\u4f73\u7684\u57fa\u6e96\u7dda\u76f8\u6bd4\uff0cTailoredBench \u5728\u76f8\u540c\u7684\u63a8\u8ad6\u9810\u7b97\u4e0b\uff0c\u6e96\u78ba\u5ea6\u4f30\u8a08\u7684 MAE \u5e73\u5747\u6e1b\u5c11 31.4%\uff0c\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u6709\u6548\u6027\u548c\u6982\u5316\u80fd\u529b\u3002", "author": "Peiwen Yuan et.al.", "authors": "Peiwen Yuan, Yueqi Zhang, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li", "id": "2502.13576v1", "paper_url": "http://arxiv.org/abs/2502.13576v1", "repo": "null"}}