{"2406.14848": {"publish_time": "2024-06-21", "title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models", "paper_summary": "Recent studies have demonstrated the effectiveness of using large language\nlanguage models (LLMs) in passage ranking. The listwise approaches, such as\nRankGPT, have become new state-of-the-art in this task. However, the efficiency\nof RankGPT models is limited by the maximum context length and relatively high\nlatency of LLM inference. To address these issues, in this paper, we propose\nPE-Rank, leveraging the single passage embedding as a good context compression\nfor efficient listwise passage reranking. By treating each passage as a special\ntoken, we can directly input passage embeddings into LLMs, thereby reducing\ninput length. Additionally, we introduce an inference method that dynamically\nconstrains the decoding space to these special tokens, accelerating the\ndecoding process. For adapting the model to reranking, we employ listwise\nlearning to rank loss for training. Evaluation results on multiple benchmarks\ndemonstrate that PE-Rank significantly improves efficiency in both prefilling\nand decoding, while maintaining competitive ranking effectiveness. {The Code is\navailable at \\url{https://github.com/liuqi6777/pe_rank}.}", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u5c55\u793a\u4e86\u5728\u6bb5\u843d\u6392\u5e8f\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6709\u6548\u6027\u3002\u5217\u8868\u5f0f\u65b9\u6cd5\uff08\u4f8b\u5982 RankGPT\uff09\u5df2\u6210\u4e3a\u6b64\u4efb\u52a1\u4e2d\u7684\u65b0\u6280\u672f\u3002\u7136\u800c\uff0cRankGPT \u6a21\u578b\u7684\u6548\u7387\u53d7\u5230\u6700\u5927\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c LLM \u63a8\u65ad\u7684\u76f8\u5bf9\u9ad8\u5ef6\u8fdf\u7684\u9650\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u5728\u672c\u6587\u4e2d\u63d0\u51fa\u4e86 PE-Rank\uff0c\u5229\u7528\u5355\u4e2a\u6bb5\u843d\u5d4c\u5165\u4f5c\u4e3a\u9ad8\u6548\u5217\u8868\u5f0f\u6bb5\u843d\u91cd\u65b0\u6392\u5e8f\u7684\u826f\u597d\u4e0a\u4e0b\u6587\u538b\u7f29\u3002\u901a\u8fc7\u5c06\u6bcf\u4e2a\u6bb5\u843d\u89c6\u4e3a\u4e00\u4e2a\u7279\u6b8a\u6807\u8bb0\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5c06\u6bb5\u843d\u5d4c\u5165\u8f93\u5165\u5230 LLM \u4e2d\uff0c\u4ece\u800c\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u63a8\u7406\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u52a8\u6001\u5730\u5c06\u89e3\u7801\u7a7a\u95f4\u9650\u5236\u5728\u8fd9\u4e9b\u7279\u6b8a\u6807\u8bb0\u4e0a\uff0c\u4ece\u800c\u52a0\u901f\u89e3\u7801\u8fc7\u7a0b\u3002\u4e3a\u4e86\u4f7f\u6a21\u578b\u9002\u5e94\u91cd\u65b0\u6392\u5e8f\uff0c\u6211\u4eec\u91c7\u7528\u5217\u8868\u5f0f\u5b66\u4e60\u6765\u5bf9\u8bad\u7ec3\u8fdb\u884c\u6392\u5e8f\u635f\u5931\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cPE-Rank \u5728\u9884\u586b\u5145\u548c\u89e3\u7801\u65b9\u9762\u90fd\u663e\u7740\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6392\u540d\u6709\u6548\u6027\u3002{\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/liuqi6777/pe_rank} \u4e2d\u627e\u5230\u3002}", "author": "Qi Liu et.al.", "authors": "Qi Liu, Bo Wang, Nan Wang, Jiaxin Mao", "id": "2406.14848v1", "paper_url": "http://arxiv.org/abs/2406.14848v1", "repo": "https://github.com/liuqi6777/pe_rank"}}