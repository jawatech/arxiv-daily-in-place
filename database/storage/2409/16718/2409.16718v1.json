{"2409.16718": {"publish_time": "2024-09-25", "title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "paper_summary": "Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed\nthe success of prompt tuning and adapter tuning, while the classic model\nfine-tuning on inherent parameters seems to be overlooked. It is believed that\nfine-tuning the parameters of VLMs with few-shot samples corrupts the\npre-trained knowledge since fine-tuning the CLIP model even degrades\nperformance. In this paper, we revisit this viewpoint, and propose a new\nperspective: fine-tuning the specific parameters instead of all will uncover\nthe power of classic model fine-tuning on VLMs. Through our meticulous study,\nwe propose ClipFit, a simple yet effective method to fine-tune CLIP without\nintroducing any overhead of extra parameters. We demonstrate that by only\nfine-tuning the specific bias terms and normalization layers, ClipFit can\nimprove the performance of zero-shot CLIP by 7.27\\% average harmonic mean\naccuracy. Lastly, to understand how fine-tuning in CLIPFit affects the\npre-trained models, we conducted extensive experimental analyses w.r.t. changes\nin internal parameters and representations. We found that low-level text bias\nlayers and the first layer normalization layer change much more than other\nlayers. The code is available at \\url{https://github.com/minglllli/CLIPFit}.", "paper_summary_zh": "\u6700\u8fd1\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u5fae\u8c03\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u89c1\u8bc1\u4e86\u63d0\u793a\u5fae\u8c03\u548c\u9002\u914d\u5668\u5fae\u8c03\u7684\u6210\u529f\uff0c\u800c\u7ecf\u5178\u6a21\u578b\u5bf9\u56fa\u6709\u53c2\u6570\u7684\u5fae\u8c03\u4f3c\u4e4e\u88ab\u5ffd\u89c6\u4e86\u3002\u4eba\u4eec\u8ba4\u4e3a\uff0c\u7528\u5c11\u91cf\u6837\u672c\u5fae\u8c03 VLM \u7684\u53c2\u6570\u4f1a\u7834\u574f\u9884\u8bad\u7ec3\u7684\u77e5\u8bc6\uff0c\u56e0\u4e3a\u5373\u4f7f\u5fae\u8c03 CLIP \u6a21\u578b\u4e5f\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u8fd9\u4e2a\u89c2\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u89c2\u70b9\uff1a\u5fae\u8c03\u7279\u5b9a\u53c2\u6570\u800c\u4e0d\u662f\u6240\u6709\u53c2\u6570\u5c06\u63ed\u793a\u7ecf\u5178\u6a21\u578b\u5fae\u8c03\u5728 VLM \u4e0a\u7684\u5f3a\u5927\u529f\u80fd\u3002\u901a\u8fc7\u6211\u4eec\u7ec6\u81f4\u7684\u7814\u7a76\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ClipFit\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f15\u5165\u4efb\u4f55\u989d\u5916\u53c2\u6570\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u5bf9 CLIP \u8fdb\u884c\u5fae\u8c03\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4ec5\u901a\u8fc7\u5fae\u8c03\u7279\u5b9a\u7684\u504f\u5dee\u9879\u548c\u5f52\u4e00\u5316\u5c42\uff0cClipFit \u53ef\u4ee5\u5c06\u96f6\u6837\u672c CLIP \u7684\u6027\u80fd\u63d0\u9ad8 7.27% \u7684\u5e73\u5747\u8c10\u6ce2\u5e73\u5747\u51c6\u786e\u7387\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u4e86\u89e3 ClipFit \u4e2d\u7684\u5fae\u8c03\u5982\u4f55\u5f71\u54cd\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6211\u4eec\u5bf9\u5185\u90e8\u53c2\u6570\u548c\u8868\u793a\u7684\u53d8\u5316\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5206\u6790\u3002\u6211\u4eec\u53d1\u73b0\u4f4e\u7ea7\u6587\u672c\u504f\u5dee\u5c42\u548c\u7b2c\u4e00\u5c42\u5f52\u4e00\u5316\u5c42\u6bd4\u5176\u4ed6\u5c42\u53d8\u5316\u66f4\u5927\u3002\u4ee3\u7801\u53ef\u5728 \\url{https://github.com/minglllli/CLIPFit} \u83b7\u5f97\u3002", "author": "Ming Li et.al.", "authors": "Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, Nie Lin, Masashi Sugiyama", "id": "2409.16718v1", "paper_url": "http://arxiv.org/abs/2409.16718v1", "repo": "https://github.com/minglllli/clipfit"}}