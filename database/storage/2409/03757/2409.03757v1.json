{"2409.03757": {"publish_time": "2024-09-05", "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding", "paper_summary": "Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks.", "paper_summary_zh": "\u8907\u96dc\u7684 3D \u5834\u666f\u7406\u89e3\u7372\u5f97\u8d8a\u4f86\u8d8a\u591a\u7684\u95dc\u6ce8\uff0c\u5834\u666f\u7de8\u78bc\u7b56\u7565\u5728\u9019\u500b\u6210\u529f\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u7136\u800c\uff0c\u5404\u7a2e\u5834\u666f\u7684\u6700\u4f73\u5834\u666f\u7de8\u78bc\u7b56\u7565\u4ecd\u7136\u4e0d\u660e\u78ba\uff0c\u7279\u5225\u662f\u8207\u5b83\u5011\u57fa\u65bc\u5f71\u50cf\u7684\u5c0d\u61c9\u7269\u76f8\u6bd4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u9805\u5168\u9762\u7684\u7814\u7a76\uff0c\u63a2\u8a0e\u4e86\u5404\u7a2e\u8996\u89ba\u7de8\u78bc\u6a21\u578b\u4ee5\u9032\u884c 3D \u5834\u666f\u7406\u89e3\uff0c\u4e26\u78ba\u5b9a\u4e86\u6bcf\u500b\u6a21\u578b\u5728\u4e0d\u540c\u5834\u666f\u4e2d\u7684\u512a\u9ede\u548c\u9650\u5236\u3002\u6211\u5011\u7684\u8a55\u4f30\u6db5\u84cb\u4e86\u4e03\u500b\u8996\u89ba\u57fa\u790e\u7de8\u78bc\u5668\uff0c\u5305\u62ec\u57fa\u65bc\u5f71\u50cf\u3001\u57fa\u65bc\u5f71\u7247\u548c 3D \u57fa\u790e\u6a21\u578b\u3002\u6211\u5011\u5728\u56db\u9805\u4efb\u52d9\u4e2d\u8a55\u4f30\u4e86\u9019\u4e9b\u6a21\u578b\uff1a\u8996\u89ba\u8a9e\u8a00\u5834\u666f\u63a8\u7406\u3001\u8996\u89ba\u63a5\u5730\u3001\u5206\u5272\u548c\u914d\u6e96\uff0c\u6bcf\u9805\u4efb\u52d9\u90fd\u5c08\u6ce8\u65bc\u5834\u666f\u7406\u89e3\u7684\u4e0d\u540c\u65b9\u9762\u3002\u6211\u5011\u7684\u8a55\u4f30\u7522\u751f\u4e86\u95dc\u9375\u767c\u73fe\uff1aDINOv2 \u8868\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\uff0c\u5f71\u7247\u6a21\u578b\u5728\u7269\u4ef6\u5c64\u7d1a\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u64f4\u6563\u6a21\u578b\u53d7\u76ca\u65bc\u5e7e\u4f55\u4efb\u52d9\uff0c\u800c\u8a9e\u8a00\u9810\u8a13\u7df4\u6a21\u578b\u5728\u8a9e\u8a00\u76f8\u95dc\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u4ee4\u4eba\u610f\u5916\u7684\u9650\u5236\u3002\u9019\u4e9b\u898b\u89e3\u6311\u6230\u4e86\u4e00\u4e9b\u50b3\u7d71\u7684\u7406\u89e3\uff0c\u63d0\u4f9b\u4e86\u5229\u7528\u8996\u89ba\u57fa\u790e\u6a21\u578b\u7684\u65b0\u89c0\u9ede\uff0c\u4e26\u5f37\u8abf\u4e86\u5728\u672a\u4f86\u7684\u8996\u89ba\u8a9e\u8a00\u548c\u5834\u666f\u7406\u89e3\u4efb\u52d9\u4e2d\u9700\u8981\u66f4\u9748\u6d3b\u7684\u7de8\u78bc\u5668\u9078\u64c7\u3002", "author": "Yunze Man et.al.", "authors": "Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liang-Yan Gui, Yu-Xiong Wang", "id": "2409.03757v1", "paper_url": "http://arxiv.org/abs/2409.03757v1", "repo": "https://github.com/yunzeman/lexicon3d"}}