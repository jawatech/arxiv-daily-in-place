{"2502.07780": {"publish_time": "2025-02-11", "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models", "paper_summary": "Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e NLP \u4efb\u52d9\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u5b83\u5011\u9f90\u5927\u7684\u904b\u7b97\u6210\u672c\u9650\u5236\u4e86\u5b83\u5011\u7684\u5ee3\u6cdb\u4f7f\u7528\uff0c\u7279\u5225\u662f\u5728\u5373\u6642\u61c9\u7528\u7a0b\u5f0f\u4e2d\u3002\u7d50\u69cb\u5316\u526a\u679d\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u901a\u904e\u58d3\u7e2e\u6a21\u578b\u4e26\u76f4\u63a5\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u52a0\u901f\u6539\u9032\uff0c\u800c\u4e0d\u7ba1\u786c\u9ad4\u74b0\u5883\u5982\u4f55\u3002\u540c\u6642\uff0c\u6a21\u578b\u7684\u4e0d\u540c\u7d44\u6210\u90e8\u5206\u5c0d\u526a\u679d\u8868\u73fe\u51fa\u4e0d\u540c\u7684\u654f\u611f\u6027\uff0c\u8981\u6c42\u9032\u884c\\emph{\u975e\u5747\u52fb}\u6a21\u578b\u58d3\u7e2e\u3002\u7136\u800c\uff0c\u526a\u679d\u65b9\u6cd5\u4e0d\u50c5\u61c9\u8a72\u8b58\u5225\u51fa\u4e00\u500b\u6709\u80fd\u529b\u7684\u5b50\u7d50\u69cb\uff0c\u9084\u61c9\u8a72\u8003\u616e\u58d3\u7e2e\u5f8c\u8a13\u7df4\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa \\sysname\uff0c\u4e00\u7a2e\u7528\u65bc\\emph{\u8a13\u7df4\u611f\u77e5}\u7d50\u69cb\u5316\u526a\u679d\u7684\u65b9\u6cd5\u3002\\sysname \u5efa\u7acb\u5728\u4e00\u500b\u9032\u5316\u641c\u7d22\u904e\u7a0b\u4e2d\uff0c\u901a\u904e\u7a81\u8b8a\u5728\u6bcf\u4e00\u4ee3\u7522\u751f\u591a\u500b\u5f8c\u4ee3\u6a21\u578b\uff0c\u4e26\u9078\u64c7\u6700\u9069\u5408\u751f\u5b58\u7684\u6a21\u578b\u3002\u70ba\u4e86\u8a55\u4f30\u5f8c\u8a13\u7df4\u7684\u6548\u679c\uff0c\u6211\u5011\u5728\u5f8c\u4ee3\u7fa4\u9ad4\u4e2d\u52a0\u5165\u4e00\u500b\u8f15\u91cf\u7d1a\u7684\u591a\u6b65\u9a5f\u8a13\u7df4\u904e\u7a0b\uff0c\u9010\u6f38\u589e\u52a0\u4ee4\u724c\u7684\u6578\u91cf\u4e26\u5728\u6bcf\u500b\u9078\u64c7\u968e\u6bb5\u6dd8\u6c70\u8868\u73fe\u4e0d\u4f73\u7684\u6a21\u578b\u3002\u6211\u5011\u901a\u904e\u5c0d Llama-2-7B\u3001Llama-3.1-8B \u548c Qwen-2.5-14B-Instruct \u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u7684\u6a21\u578b\uff0c\u5be6\u73fe\u4e86\u7d50\u69cb\u5316\u526a\u679d\u7684\u6700\u65b0\u6548\u80fd\u3002\u4f8b\u5982\uff0c\\sysname \u8d85\u8d8a\u4e86 ShearedLlama\uff0c\u540c\u6642\u5728\u58d3\u7e2e\u5f8c\u8a13\u7df4\u4e2d\u9700\u8981\u7684\u8a13\u7df4\u8cc7\u6599\u6e1b\u5c11\u4e86 $5\\times$\u3002", "author": "Shengkun Tang et.al.", "authors": "Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, Dan Alistarh", "id": "2502.07780v1", "paper_url": "http://arxiv.org/abs/2502.07780v1", "repo": "null"}}