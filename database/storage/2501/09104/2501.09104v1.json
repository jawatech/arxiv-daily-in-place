{"2501.09104": {"publish_time": "2025-01-15", "title": "A Non-autoregressive Model for Joint STT and TTS", "paper_summary": "In this paper, we take a step towards jointly modeling automatic speech\nrecognition (STT) and speech synthesis (TTS) in a fully non-autoregressive way.\nWe develop a novel multimodal framework capable of handling the speech and text\nmodalities as input either individually or together. The proposed model can\nalso be trained with unpaired speech or text data owing to its multimodal\nnature. We further propose an iterative refinement strategy to improve the STT\nand TTS performance of our model such that the partial hypothesis at the output\ncan be fed back to the input of our model, thus iteratively improving both STT\nand TTS predictions. We show that our joint model can effectively perform both\nSTT and TTS tasks, outperforming the STT-specific baseline in all tasks and\nperforming competitively with the TTS-specific baseline across a wide range of\nevaluation metrics.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a1\u53d6\u6b65\u9a5f\u4ee5\u5b8c\u5168\u975e\u81ea\u8ff4\u6b78\u65b9\u5f0f\u806f\u5408\u5efa\u6a21\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (STT) \u548c\u8a9e\u97f3\u5408\u6210 (TTS)\u3002\n\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u591a\u6a21\u614b\u6846\u67b6\uff0c\u80fd\u5920\u5c07\u8a9e\u97f3\u548c\u6587\u5b57\u6a21\u614b\u500b\u5225\u6216\u4e00\u8d77\u7576\u4f5c\u8f38\u5165\u8655\u7406\u3002\u7531\u65bc\u5176\u591a\u6a21\u614b\u6027\u8cea\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4e5f\u53ef\u4ee5\u7528\u672a\u914d\u5c0d\u7684\u8a9e\u97f3\u6216\u6587\u5b57\u8cc7\u6599\u8a13\u7df4\u3002\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e00\u500b\u53cd\u8986\u6539\u9032\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684 STT \u548c TTS \u6548\u80fd\uff0c\u8b93\u8f38\u51fa\u7aef\u7684\u5c40\u90e8\u5047\u8a2d\u53ef\u4ee5\u56de\u994b\u5230\u6a21\u578b\u7684\u8f38\u5165\u7aef\uff0c\u5f9e\u800c\u53cd\u8986\u6539\u9032 STT \u548c TTS \u9810\u6e2c\u3002\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u7684\u806f\u5408\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u57f7\u884c STT \u548c TTS \u4efb\u52d9\uff0c\u5728\u6240\u6709\u4efb\u52d9\u4e2d\u90fd\u512a\u65bc\u7279\u5b9a\u65bc STT \u7684\u57fa\u6e96\uff0c\u4e26\u4e14\u5728\u5ee3\u6cdb\u7684\u8a55\u4f30\u6307\u6a19\u4e2d\u8207\u7279\u5b9a\u65bc TTS \u7684\u57fa\u6e96\u8868\u73fe\u5f97\u5177\u6709\u7af6\u722d\u529b\u3002", "author": "Vishal Sunder et.al.", "authors": "Vishal Sunder, Brian Kingsbury, George Saon, Samuel Thomas, Slava Shechtman Hagai Aronowitz, Eric Fosler-Lussier, Luis Lastras", "id": "2501.09104v1", "paper_url": "http://arxiv.org/abs/2501.09104v1", "repo": "null"}}