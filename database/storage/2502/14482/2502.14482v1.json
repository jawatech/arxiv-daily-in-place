{"2502.14482": {"publish_time": "2025-02-20", "title": "NLoRA: Nystr\u00f6m-Initiated Low-Rank Adaptation for Large Language Models", "paper_summary": "Parameter-efficient fine-tuning (PEFT) is essential for adapting large\nlanguage models (LLMs), with low-rank adaptation (LoRA) being the most popular\napproach. However, LoRA suffers from slow convergence, and some recent LoRA\nvariants, such as PiSSA, primarily rely on Singular Value Decomposition (SVD)\nfor initialization, leading to expensive computation. To mitigate these\nproblems, we use the Nystr\\\"om method, which follows a three-matrix\nmanipulation. We first introduce StructuredLoRA (SLoRA), which investigates\nadding a small intermediate matrix between the low-rank matrices A and B.\nSecondly, we propose Nystr\\\"omLoRA (NLoRA), which leverages Nystr\\\"om-based\ninitialization for SLoRA to improve its effectiveness and efficiency. Finally,\nwe propose IntermediateTune (IntTune), which explores fine-tuning exclusively\non the intermediate matrix of NLoRA to further boost LLM efficiency. We\nevaluate our methods on five natural language generation (NLG) tasks and eight\nnatural language understanding (NLU) tasks. On GSM8K, SLoRA and NLoRA achieve\naccuracies of 56.48% and 57.70%, surpassing LoRA by 33.52% and 36.41%, with\nonly 3.67 million additional trainable parameters. IntTune improves average NLG\nperformance over LoRA by 7.45% while using only 1.25% of its parameters. These\nresults demonstrate the efficiency and effectiveness of our approach in\nenhancing model performance with minimal parameter overhead.", "paper_summary_zh": "\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u5c0d\u65bc\u8abf\u6574\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u81f3\u95dc\u91cd\u8981\uff0c\u5176\u4e2d\u4f4e\u79e9\u8abf\u6574 (LoRA) \u662f\u6700\u53d7\u6b61\u8fce\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0cLoRA \u5b58\u5728\u6536\u6582\u901f\u5ea6\u6162\u7684\u554f\u984c\uff0c\u800c\u4e00\u4e9b\u6700\u8fd1\u7684 LoRA \u8b8a\u9ad4\uff0c\u4f8b\u5982 PiSSA\uff0c\u4e3b\u8981\u4f9d\u8cf4\u5947\u7570\u503c\u5206\u89e3 (SVD) \u9032\u884c\u521d\u59cb\u5316\uff0c\u5c0e\u81f4\u904b\u7b97\u6210\u672c\u9ad8\u6602\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u4f7f\u7528\u4e86 Nystr\\\"om \u65b9\u6cd5\uff0c\u5b83\u9075\u5faa\u4e09\u77e9\u9663\u64cd\u4f5c\u3002\u6211\u5011\u9996\u5148\u4ecb\u7d39 StructuredLoRA (SLoRA)\uff0c\u5b83\u7814\u7a76\u5728\u4f4e\u79e9\u77e9\u9663 A \u548c B \u4e4b\u9593\u6dfb\u52a0\u4e00\u500b\u5c0f\u7684\u4e2d\u9593\u77e9\u9663\u3002\u5176\u6b21\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Nystr\\\"omLoRA (NLoRA)\uff0c\u5b83\u5229\u7528\u57fa\u65bc Nystr\\\"om \u7684\u521d\u59cb\u5316\u65b9\u6cd5\u70ba SLoRA \u63d0\u5347\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 IntermediateTune (IntTune)\uff0c\u5b83\u63a2\u8a0e\u4e86\u50c5\u5c0d NLoRA \u7684\u4e2d\u9593\u77e9\u9663\u9032\u884c\u5fae\u8abf\uff0c\u4ee5\u9032\u4e00\u6b65\u63d0\u5347 LLM \u6548\u7387\u3002\u6211\u5011\u5728\u4e94\u9805\u81ea\u7136\u8a9e\u8a00\u751f\u6210 (NLG) \u4efb\u52d9\u548c\u516b\u9805\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u4efb\u52d9\u4e0a\u8a55\u4f30\u4e86\u6211\u5011\u7684\u9019\u4e9b\u65b9\u6cd5\u3002\u5728 GSM8K \u4e0a\uff0cSLoRA \u548c NLoRA \u5206\u5225\u9054\u5230\u4e86 56.48% \u548c 57.70% \u7684\u6e96\u78ba\u7387\uff0c\u6bd4 LoRA \u9ad8\u51fa 33.52% \u548c 36.41%\uff0c\u800c\u50c5\u589e\u52a0\u4e86 367 \u842c\u500b\u53ef\u8a13\u7df4\u53c3\u6578\u3002IntTune \u5728\u50c5\u4f7f\u7528 LoRA 1.25% \u7684\u53c3\u6578\u7684\u60c5\u6cc1\u4e0b\uff0c\u5c07\u5e73\u5747 NLG \u6548\u80fd\u63d0\u5347\u4e86 7.45%\u3002\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u4ee5\u6700\u5c11\u7684\u53c3\u6578\u958b\u92b7\u63d0\u5347\u6a21\u578b\u6548\u80fd\u65b9\u9762\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "author": "Chenlu Guo et.al.", "authors": "Chenlu Guo, Yuan Wu, Yi Chang", "id": "2502.14482v1", "paper_url": "http://arxiv.org/abs/2502.14482v1", "repo": "null"}}