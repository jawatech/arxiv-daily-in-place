{"2408.01986": {"publish_time": "2024-08-04", "title": "DeMansia: Mamba Never Forgets Any Tokens", "paper_summary": "This paper examines the mathematical foundations of transformer\narchitectures, highlighting their limitations particularly in handling long\nsequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM),\nand LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia\nintegrates state space models with token labeling techniques to enhance\nperformance in image classification tasks, efficiently addressing the\ncomputational challenges posed by traditional transformers. The architecture,\nbenchmark, and comparisons with contemporary models demonstrate DeMansia's\neffectiveness. The implementation of this paper is available on GitHub at\nhttps://github.com/catalpaaa/DeMansia", "paper_summary_zh": "\u672c\u6587\u63a2\u8a0e\u4e86 Transformer \u67b6\u69cb\u7684\u6578\u5b78\u57fa\u790e\uff0c\u7279\u5225\u5f37\u8abf\u4e86\u5b83\u5011\u5728\u8655\u7406\u9577\u5e8f\u5217\u6642\u7684\u9650\u5236\u3002\u6211\u5011\u63a2\u8a0e\u4e86 Mamba\u3001Vision Mamba (ViM) \u548c LV-ViT \u7b49\u5148\u6c7a\u689d\u4ef6\u6a21\u578b\uff0c\u70ba\u6211\u5011\u63d0\u51fa\u7684\u67b6\u69cb DeMansia \u92ea\u5e73\u4e86\u9053\u8def\u3002DeMansia \u5c07\u72c0\u614b\u7a7a\u9593\u6a21\u578b\u8207\u6a19\u8a18\u6280\u8853\u6574\u5408\uff0c\u4ee5\u589e\u5f37\u5716\u50cf\u5206\u985e\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u6709\u6548\u5730\u89e3\u6c7a\u50b3\u7d71 Transformer \u6240\u5e36\u4f86\u7684\u8a08\u7b97\u6311\u6230\u3002\u6b64\u67b6\u69cb\u3001\u57fa\u6e96\u548c\u8207\u7576\u4ee3\u6a21\u578b\u7684\u6bd4\u8f03\u8b49\u660e\u4e86 DeMansia \u7684\u6548\u80fd\u3002\u672c\u6587\u7684\u5be6\u4f5c\u53ef\u5728 GitHub \u4e0a\u53d6\u5f97\uff0c\u7db2\u5740\u70ba https://github.com/catalpaaa/DeMansia", "author": "Ricky Fang et.al.", "authors": "Ricky Fang", "id": "2408.01986v1", "paper_url": "http://arxiv.org/abs/2408.01986v1", "repo": "null"}}