{"2405.20835": {"publish_time": "2024-05-31", "title": "Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs", "paper_summary": "Post-Training Quantization (PTQ) enhances the efficiency of Large Language\nModels (LLMs) by enabling faster operation and compatibility with more\naccessible hardware through reduced memory usage, at the cost of small\nperformance drops. We explore the role of calibration sets in PTQ, specifically\ntheir effect on hidden activations in various notable open-source LLMs.\nCalibration sets are crucial for evaluating activation magnitudes and\nidentifying outliers, which can distort the quantization range and negatively\nimpact performance. Our analysis reveals a marked contrast in quantization\neffectiveness across models. The older OPT model, upon which much of the\nquantization literature is based, shows significant performance deterioration\nand high susceptibility to outliers with varying calibration sets. In contrast,\nnewer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B\ndemonstrate strong robustness, with Mistral 7B showing near-immunity to\noutliers and stable activations. These findings suggest a shift in PTQ\nstrategies might be needed. As advancements in pre-training methods reduce the\nrelevance of outliers, there is an emerging need to reassess the fundamentals\nof current quantization literature. The emphasis should pivot towards\noptimizing inference speed, rather than primarily focusing on outlier\npreservation, to align with the evolving characteristics of state-of-the-art\nLLMs.", "paper_summary_zh": "\u8a13\u7df4\u5f8c\u91cf\u5316 (PTQ) \u900f\u904e\u6e1b\u5c11\u8a18\u61b6\u9ad4\u7528\u91cf\u4f86\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6548\u7387\uff0c\u8b93\u904b\u4f5c\u901f\u5ea6\u66f4\u5feb\uff0c\u4e26\u8207\u66f4\u5bb9\u6613\u53d6\u5f97\u7684\u786c\u9ad4\u76f8\u5bb9\uff0c\u4f46\u4ee3\u50f9\u662f\u6548\u80fd\u7565\u5fae\u4e0b\u964d\u3002\u6211\u5011\u63a2\u8a0e\u6821\u6b63\u7d44\u5728 PTQ \u4e2d\u7684\u89d2\u8272\uff0c\u7279\u5225\u662f\u5b83\u5011\u5c0d\u5404\u7a2e\u77e5\u540d\u958b\u6e90 LLM \u4e2d\u96b1\u85cf\u6fc0\u6d3b\u7684\u5f71\u97ff\u3002\u6821\u6b63\u7d44\u5c0d\u65bc\u8a55\u4f30\u6fc0\u6d3b\u5e45\u5ea6\u548c\u627e\u51fa\u7570\u5e38\u503c\u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u7570\u5e38\u503c\u6703\u626d\u66f2\u91cf\u5316\u7bc4\u570d\u4e26\u5c0d\u6548\u80fd\u7522\u751f\u8ca0\u9762\u5f71\u97ff\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u91cf\u5316\u6548\u679c\u6709\u986f\u8457\u5dee\u7570\u3002\u8a31\u591a\u91cf\u5316\u6587\u737b\u6240\u4f9d\u64da\u7684\u8f03\u820a OPT \u6a21\u578b\uff0c\u5728\u4f7f\u7528\u4e0d\u540c\u7684\u6821\u6b63\u7d44\u6642\uff0c\u6548\u80fd\u6703\u986f\u8457\u4e0b\u964d\uff0c\u4e14\u5c0d\u7570\u5e38\u503c\u9ad8\u5ea6\u654f\u611f\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cLlama-2 7B\u3001Llama-3 8B\u3001Command-R 35B \u548c Mistral 7B \u7b49\u8f03\u65b0\u7684\u6a21\u578b\u8868\u73fe\u51fa\u5f37\u5927\u7684\u7a69\u5065\u6027\uff0c\u5176\u4e2d Mistral 7B \u5c0d\u7570\u5e38\u503c\u5e7e\u4e4e\u514d\u75ab\uff0c\u4e14\u6fc0\u6d3b\u7a69\u5b9a\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\uff0cPTQ \u7b56\u7565\u53ef\u80fd\u9700\u8981\u6539\u8b8a\u3002\u96a8\u8457\u9810\u8a13\u7df4\u65b9\u6cd5\u7684\u9032\u6b65\u964d\u4f4e\u7570\u5e38\u503c\u7684\u91cd\u8981\u6027\uff0c\u91cd\u65b0\u8a55\u4f30\u73fe\u6709\u91cf\u5316\u6587\u737b\u7684\u57fa\u672c\u539f\u7406\u5df2\u6210\u7576\u52d9\u4e4b\u6025\u3002\u91cd\u9ede\u61c9\u8f49\u5411\u6700\u4f73\u5316\u63a8\u8ad6\u901f\u5ea6\uff0c\u800c\u4e0d\u662f\u4e3b\u8981\u95dc\u6ce8\u7570\u5e38\u503c\u4fdd\u7559\uff0c\u4ee5\u7b26\u5408\u6700\u5148\u9032 LLM \u7684\u6f14\u5316\u7279\u6027\u3002", "author": "Davide Paglieri et.al.", "authors": "Davide Paglieri, Saurabh Dash, Tim Rockt\u00e4schel, Jack Parker-Holder", "id": "2405.20835v2", "paper_url": "http://arxiv.org/abs/2405.20835v2", "repo": "null"}}