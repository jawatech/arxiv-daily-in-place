{"2412.09283": {"publish_time": "2024-12-12", "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption", "paper_summary": "Text-to-video generation has evolved rapidly in recent years, delivering\nremarkable results. Training typically relies on video-caption paired data,\nwhich plays a crucial role in enhancing generation performance. However,\ncurrent video captions often suffer from insufficient details, hallucinations\nand imprecise motion depiction, affecting the fidelity and consistency of\ngenerated videos. In this work, we propose a novel instance-aware structured\ncaption framework, termed InstanceCap, to achieve instance-level and\nfine-grained video caption for the first time. Based on this scheme, we design\nan auxiliary models cluster to convert original video into instances to enhance\ninstance fidelity. Video instances are further used to refine dense prompts\ninto structured phrases, achieving concise yet precise descriptions.\nFurthermore, a 22K InstanceVid dataset is curated for training, and an\nenhancement pipeline that tailored to InstanceCap structure is proposed for\ninference. Experimental results demonstrate that our proposed InstanceCap\nsignificantly outperform previous models, ensuring high fidelity between\ncaptions and videos while reducing hallucinations.", "paper_summary_zh": "\u6587\u5b57\u8f6c\u89c6\u9891\u751f\u6210\u8fd1\u5e74\u6765\u53d1\u5c55\u8fc5\u901f\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\u3002\u8bad\u7ec3\u901a\u5e38\u4f9d\u8d56\u4e8e\u89c6\u9891\u5b57\u5e55\u914d\u5bf9\u6570\u636e\uff0c\u8fd9\u5bf9\u63d0\u9ad8\u751f\u6210\u6027\u80fd\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u89c6\u9891\u5b57\u5e55\u5e38\u5e38\u5b58\u5728\u7ec6\u8282\u4e0d\u8db3\u3001\u51fa\u73b0\u5e7b\u89c9\u548c\u52a8\u4f5c\u63cf\u8ff0\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u751f\u6210\u89c6\u9891\u7684\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f8b\u611f\u77e5\u7ed3\u6784\u5316\u5b57\u5e55\u6846\u67b6\uff0c\u79f0\u4e3a InstanceCap\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5b9e\u4f8b\u7ea7\u548c\u7ec6\u7c92\u5ea6\u7684\u89c6\u9891\u5b57\u5e55\u3002\u57fa\u4e8e\u6b64\u65b9\u6848\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f85\u52a9\u6a21\u578b\u96c6\u7fa4\uff0c\u5c06\u539f\u59cb\u89c6\u9891\u8f6c\u6362\u4e3a\u5b9e\u4f8b\uff0c\u4ee5\u589e\u5f3a\u5b9e\u4f8b\u4fdd\u771f\u5ea6\u3002\u89c6\u9891\u5b9e\u4f8b\u8fdb\u4e00\u6b65\u7528\u4e8e\u5c06\u5bc6\u96c6\u63d0\u793a\u7cbe\u70bc\u4e3a\u7ed3\u6784\u5316\u77ed\u8bed\uff0c\u4ece\u800c\u5b9e\u73b0\u7b80\u6d01\u800c\u7cbe\u786e\u7684\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u8fd8\u6574\u7406\u4e86\u4e00\u4e2a 22K InstanceVid \u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\uff0c\u5e76\u9488\u5bf9 InstanceCap \u7ed3\u6784\u63d0\u51fa\u4e86\u4e00\u6761\u589e\u5f3a\u7684\u7ba1\u9053\u7528\u4e8e\u63a8\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684 InstanceCap \u660e\u663e\u4f18\u4e8e\u4ee5\u524d\u7684\u6a21\u578b\uff0c\u786e\u4fdd\u4e86\u5b57\u5e55\u548c\u89c6\u9891\u4e4b\u95f4\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002", "author": "Tiehan Fan et.al.", "authors": "Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, Ying Tai", "id": "2412.09283v1", "paper_url": "http://arxiv.org/abs/2412.09283v1", "repo": "null"}}