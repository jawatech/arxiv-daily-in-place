{"2501.00958": {"publish_time": "2025-01-01", "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining", "paper_summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.", "paper_summary_zh": "<paragraph>\u8207\u5f71\u50cf\u6587\u5b57\u914d\u5c0d\u8cc7\u6599\u76f8\u6bd4\uff0c\u4ea4\u932f\u8a9e\u6599\u5eab\u80fd\u8b93\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u66f4\u50cf\u4eba\u985e\u4e00\u6a23\u81ea\u7136\u5730\u7406\u89e3\u4e16\u754c\u3002\u7136\u800c\uff0c\u6b64\u985e\u73fe\u6709\u8cc7\u6599\u96c6\u662f\u7531\u7db2\u9801\u722c\u53d6\u800c\u4f86\uff0c\u9762\u81e8\u77e5\u8b58\u5bc6\u5ea6\u4f4e\u3001\u5f71\u50cf\u6587\u5b57\u95dc\u4fc2\u9b06\u6563\uff0c\u4ee5\u53ca\u5f71\u50cf\u9593\u908f\u8f2f\u9023\u8cab\u6027\u5dee\u7684\u6311\u6230\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u7db2\u969b\u7db2\u8def\u4e0a\u6709\u5927\u91cf\u6559\u5b78\u5f71\u7247\uff08\u4f8b\u5982\u7dda\u4e0a\u5e7e\u4f55\u8ab2\u7a0b\uff09\uff0c\u88ab\u5ee3\u6cdb\u7528\u65bc\u4eba\u985e\u5b78\u7fd2\u57fa\u790e\u79d1\u76ee\uff0c\u4f46\u9019\u4e9b\u5bf6\u8cb4\u8cc7\u6e90\u5728 VLM \u8a13\u7df4\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5177\u6709\u8c50\u5bcc\u57fa\u790e\u77e5\u8b58\u7684\u9ad8\u54c1\u8cea**\u591a\u6a21\u614b\u6559\u79d1\u66f8**\u8a9e\u6599\u5eab\uff0c\u7528\u65bc VLM \u9810\u8a13\u7df4\u3002\u5b83\u6536\u96c6\u4e86\u8d85\u904e 2.5 \u5e74\u7684\u6559\u5b78\u5f71\u7247\uff0c\u7e3d\u8a08 22,000 \u5c0f\u6642\u7684\u8ab2\u7a0b\u3002\u6211\u5011\u9996\u5148\u4f7f\u7528 LLM \u63d0\u51fa\u7684\u5206\u985e\u6cd5\u7cfb\u7d71\u6027\u5730\u6536\u96c6\u6559\u5b78\u5f71\u7247\u3002\u7136\u5f8c\uff0c\u6211\u5011\u9010\u6b65\u5f9e\u5f71\u7247\u4e2d\u8403\u53d6\u548c\u7cbe\u7149\u8996\u89ba\uff08\u95dc\u9375\u5f71\u683c\uff09\u3001\u97f3\u8a0a\uff08ASR\uff09\u548c\u6587\u5b57\u77e5\u8b58\uff08OCR\uff09\uff0c\u4e26\u6839\u64da\u6642\u9593\u9806\u5e8f\u7d44\u7e54\u6210\u5f71\u50cf\u6587\u5b57\u4ea4\u932f\u8a9e\u6599\u5eab\u3002\u8207\u5176\u5c0d\u61c9\u7684\u6559\u79d1\u66f8\u76f8\u6bd4\uff0c\u6211\u5011\u4ee5\u5f71\u7247\u70ba\u4e2d\u5fc3\u7684\u6559\u79d1\u66f8\u63d0\u4f9b\u4e86\u66f4\u9023\u8cab\u7684\u8108\u7d61\u3001\u66f4\u8c50\u5bcc\u7684\u77e5\u8b58\uff0c\u4ee5\u53ca\u66f4\u597d\u7684\u5f71\u50cf\u6587\u5b57\u5c0d\u9f4a\u3002\u5be6\u9a57\u8b49\u660e\u4e86\u5b83\u512a\u7570\u7684\u9810\u8a13\u7df4\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u9700\u8981\u77e5\u8b58\u548c\u63a8\u7406\u7684\u4efb\u52d9\u4e2d\uff0c\u4f8b\u5982 ScienceQA \u548c MathVista\u3002\u6b64\u5916\uff0c\u5728\u6211\u5011\u7684\u6559\u79d1\u66f8\u4e0a\u9810\u8a13\u7df4\u7684 VLM \u5c55\u73fe\u51fa\u5091\u51fa\u7684\u4ea4\u932f\u8108\u7d61\u611f\u77e5\u80fd\u529b\uff0c\u5728\u4efb\u52d9\u89e3\u6c7a\u4e2d\u904b\u7528\u8996\u89ba\u548c\u6587\u5b57\u63d0\u793a\u9032\u884c\u5c11\u91cf\u6b21\u6578\u7684\u8108\u7d61\u63a8\u8ad6~\\footnote{\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook} \u53d6\u5f97}\u3002</paragraph>", "author": "Wenqi Zhang et.al.", "authors": "Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, Lidong Bing", "id": "2501.00958v1", "paper_url": "http://arxiv.org/abs/2501.00958v1", "repo": "null"}}