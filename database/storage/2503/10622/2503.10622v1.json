{"2503.10622": {"publish_time": "2025-03-13", "title": "Transformers without Normalization", "paper_summary": "Normalization layers are ubiquitous in modern neural networks and have long\nbeen considered essential. This work demonstrates that Transformers without\nnormalization can achieve the same or better performance using a remarkably\nsimple technique. We introduce Dynamic Tanh (DyT), an element-wise operation\n$DyT($x$) = \\tanh(\\alpha $x$)$, as a drop-in replacement for normalization\nlayers in Transformers. DyT is inspired by the observation that layer\nnormalization in Transformers often produces tanh-like, $S$-shaped input-output\nmappings. By incorporating DyT, Transformers without normalization can match or\nexceed the performance of their normalized counterparts, mostly without\nhyperparameter tuning. We validate the effectiveness of Transformers with DyT\nacross diverse settings, ranging from recognition to generation, supervised to\nself-supervised learning, and computer vision to language models. These\nfindings challenge the conventional understanding that normalization layers are\nindispensable in modern neural networks, and offer new insights into their role\nin deep networks.", "paper_summary_zh": "<paragraph>\u6b63\u898f\u5316\u5c64\u5728\u73fe\u4ee3\u795e\u7d93\u7db2\u8def\u4e2d\u7121\u8655\u4e0d\u5728\uff0c\u9577\u671f\u4ee5\u4f86\u4e00\u76f4\u88ab\u8a8d\u70ba\u662f\u4e0d\u53ef\u6216\u7f3a\u7684\u3002\u9019\u9805\u5de5\u4f5c\u8b49\u660e\u4e86\u6c92\u6709\u6b63\u898f\u5316\u7684 Transformer \u53ef\u4ee5\u4f7f\u7528\u4e00\u7a2e\u975e\u5e38\u7c21\u55ae\u7684\u6280\u8853\u9054\u5230\u76f8\u540c\u6216\u66f4\u597d\u7684\u6548\u80fd\u3002\u6211\u5011\u5f15\u5165\u4e86\u52d5\u614b Tanh (DyT)\uff0c\u4e00\u7a2e\u5143\u7d20\u64cd\u4f5c$DyT($x$) = \\tanh(\\alpha $x$)\uff0c\u4f5c\u70ba Transformer \u4e2d\u6b63\u898f\u5316\u5c64\u7684\u66ff\u4ee3\u54c1\u3002DyT \u7684\u9748\u611f\u4f86\u81ea\u65bc Transformer \u4e2d\u7684\u5c64\u6b63\u898f\u5316\u901a\u5e38\u6703\u7522\u751f\u985e\u4f3c tanh \u7684 S \u5f62\u8f38\u5165\u8f38\u51fa\u6620\u5c04\u7684\u89c0\u5bdf\u3002\u901a\u904e\u7d50\u5408 DyT\uff0c\u6c92\u6709\u6b63\u898f\u5316\u7684 Transformer \u53ef\u4ee5\u9054\u5230\u6216\u8d85\u904e\u5176\u6b63\u898f\u5316\u6a21\u578b\u7684\u6548\u80fd\uff0c\u800c\u4e14\u5927\u591a\u6578\u60c5\u6cc1\u4e0b\u7121\u9700\u8abf\u6574\u8d85\u53c3\u6578\u3002\u6211\u5011\u9a57\u8b49\u4e86\u5728\u5404\u7a2e\u8a2d\u5b9a\u4e0b\u4f7f\u7528 DyT \u7684 Transformer \u7684\u6709\u6548\u6027\uff0c\u7bc4\u570d\u5f9e\u8b58\u5225\u5230\u751f\u6210\u3001\u76e3\u7763\u5f0f\u5b78\u7fd2\u5230\u81ea\u76e3\u7763\u5f0f\u5b78\u7fd2\uff0c\u4ee5\u53ca\u96fb\u8166\u8996\u89ba\u5230\u8a9e\u8a00\u6a21\u578b\u3002\u9019\u4e9b\u767c\u73fe\u6311\u6230\u4e86\u6b63\u898f\u5316\u5c64\u5728\u73fe\u4ee3\u795e\u7d93\u7db2\u8def\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u50b3\u7d71\u7406\u89e3\uff0c\u4e26\u70ba\u5b83\u5011\u5728\u6df1\u5ea6\u7db2\u8def\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\u3002</paragraph>\n", "author": "Jiachen Zhu et.al.", "authors": "Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu", "id": "2503.10622v1", "paper_url": "http://arxiv.org/abs/2503.10622v1", "repo": "null"}}