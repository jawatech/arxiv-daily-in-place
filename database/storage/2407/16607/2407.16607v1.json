{"2407.16607": {"publish_time": "2024-07-23", "title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?", "paper_summary": "The pretraining data of today's strongest language models is opaque. In\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about the pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.", "paper_summary_zh": "<paragraph>\u7576\u524d\u6700\u5f37\u5927\u7684\u8a9e\u8a00\u6a21\u578b\u7684\u9810\u8a13\u7df4\u6578\u64da\u662f\u6a21\u7cca\u4e0d\u6e05\u7684\u3002\u7279\u5225\u662f\uff0c\u5c0d\u65bc\u5404\u7a2e\u9818\u57df\u6216\u8a9e\u8a00\u6240\u5360\u6bd4\u4f8b\u7684\u4e86\u89e3\u751a\u5c11\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8655\u7406\u4e86\u4e00\u9805\u7a31\u70ba\u6578\u64da\u6df7\u5408\u63a8\u8ad6\u7684\u4efb\u52d9\uff0c\u65e8\u5728\u63ed\u793a\u8a13\u7df4\u6578\u64da\u7684\u5206\u5e03\u5f0f\u7d44\u6210\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u653b\u64ca\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u57fa\u65bc\u4e00\u500b\u4ee5\u524d\u88ab\u5ffd\u8996\u7684\u4fe1\u606f\u4f86\u6e90\u2014\u2014\u5b57\u7bc0\u5c0d\u7de8\u78bc (BPE) \u5206\u8a5e\u5668\uff0c\u5b83\u88ab\u7d55\u5927\u591a\u6578\u73fe\u4ee3\u8a9e\u8a00\u6a21\u578b\u4f7f\u7528\u3002\u6211\u5011\u7684\u95dc\u9375\u898b\u89e3\u662f\uff0cBPE \u5206\u8a5e\u5668\u5b78\u7fd2\u5230\u7684\u5408\u4f75\u898f\u5247\u7684\u6709\u5e8f\u5217\u8868\u81ea\u7136\u5730\u63ed\u793a\u4e86\u5176\u8a13\u7df4\u6578\u64da\u4e2d\u8a5e\u983b\u7684\u4fe1\u606f\uff1a\u7b2c\u4e00\u6b21\u5408\u4f75\u662f\u6700\u5e38\u898b\u7684\u5b57\u7bc0\u5c0d\uff0c\u7b2c\u4e8c\u6b21\u5408\u4f75\u662f\u6700\u5e38\u898b\u7684\u5c0d\uff0c\u4f9d\u6b64\u985e\u63a8\u3002\u7d66\u5b9a\u5206\u8a5e\u5668\u7684\u5408\u4f75\u5217\u8868\u4ee5\u53ca\u6bcf\u500b\u611f\u8208\u8da3\u985e\u5225\u7684\u6578\u64da\u6a23\u672c\uff0c\u6211\u5011\u5236\u5b9a\u4e86\u4e00\u500b\u7dda\u6027\u898f\u5283\uff0c\u7528\u65bc\u6c42\u89e3\u5206\u8a5e\u5668\u8a13\u7df4\u96c6\u4e2d\u6bcf\u500b\u985e\u5225\u7684\u6bd4\u4f8b\u3002\u91cd\u8981\u7684\u662f\uff0c\u5728\u5206\u8a5e\u5668\u8a13\u7df4\u6578\u64da\u4ee3\u8868\u9810\u8a13\u7df4\u6578\u64da\u7684\u7bc4\u570d\u5167\uff0c\u6211\u5011\u9593\u63a5\u4e86\u89e3\u4e86\u9810\u8a13\u7df4\u6578\u64da\u3002\u5728\u53d7\u63a7\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u8868\u660e\u6211\u5011\u7684\u653b\u64ca\u4ee5\u9ad8\u7cbe\u5ea6\u6062\u5fa9\u4e86\u5728\u5df2\u77e5\u81ea\u7136\u8a9e\u8a00\u3001\u7de8\u7a0b\u8a9e\u8a00\u548c\u6578\u64da\u6e90\u6df7\u5408\u7269\u4e0a\u8a13\u7df4\u7684\u5206\u8a5e\u5668\u7684\u6df7\u5408\u6bd4\u4f8b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07\u6211\u5011\u7684\u505a\u6cd5\u61c9\u7528\u65bc\u6700\u8fd1\u767c\u5e03\u7684\u958b\u7bb1\u5373\u7528\u5206\u8a5e\u5668\u8207 LMs\u3002\u6211\u5011\u78ba\u8a8d\u4e86\u95dc\u65bc\u9019\u4e9b\u6a21\u578b\u7684\u8a31\u591a\u516c\u958b\u62ab\u9732\u7684\u4fe1\u606f\uff0c\u9084\u505a\u51fa\u4e86\u5e7e\u500b\u65b0\u7684\u63a8\u8ad6\uff1aGPT-4o \u7684\u5206\u8a5e\u5668\u6bd4\u5176\u524d\u8f29\u66f4\u52a0\u591a\u8a9e\u8a00\uff0c\u5728 39% \u7684\u975e\u82f1\u8a9e\u6578\u64da\u4e0a\u9032\u884c\u8a13\u7df4\uff1bLlama3 \u4e3b\u8981\u70ba\u591a\u8a9e\u8a00\uff0848%\uff09\u4f7f\u7528\u64f4\u5c55\u4e86 GPT-3.5 \u7684\u5206\u8a5e\u5668\uff1bGPT-3.5 \u548c Claude \u7684\u5206\u8a5e\u5668\u4e3b\u8981\u5728\u4ee3\u78bc\uff08~60%\uff09\u4e0a\u9032\u884c\u8a13\u7df4\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684\u5de5\u4f5c\u80fd\u70ba\u9810\u8a13\u7df4\u6578\u64da\u7684\u7576\u524d\u8a2d\u8a08\u5be6\u52d9\u63d0\u4f9b\u555f\u793a\uff0c\u4e26\u6fc0\u52f5\u7e7c\u7e8c\u7814\u7a76 LMs \u7684\u6578\u64da\u6df7\u5408\u63a8\u8ad6\u3002</paragraph>", "author": "Jonathan Hayase et.al.", "authors": "Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith", "id": "2407.16607v1", "paper_url": "http://arxiv.org/abs/2407.16607v1", "repo": "null"}}