{"2410.19503": {"publish_time": "2024-10-25", "title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models", "paper_summary": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) being particularly\nnotable for reducing the mismatch between training and inference. However, SGOs\noften produce noisy and biased sequences, which can lead to misguidance from\nthe teacher model, especially in long sequences. To mitigate these challenges,\nwe propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel\napproach that strategically incorporates the teacher model during the student's\nsequence generation. SWITCH identifies discrepancies between the token\nprobabilities of the teacher and student models, allowing the teacher to\nintervene selectively, particularly in long sequences that are more prone to\nteacher misguidance. Extensive experimental results across three model families\nand five instruction-following datasets show that SWITCH surpasses traditional\nKD methods, particularly excelling in the generation of long sequential data.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5b83\u5011\u4ecd\u9762\u81e8\u8207\u9ad8\u63a8\u8ad6\u6210\u672c\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\u76f8\u95dc\u7684\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u77e5\u8b58\u84b8\u993e (KD) \u5df2\u6210\u70ba\u4e00\u7a2e\u6d41\u884c\u7684\u6a21\u578b\u58d3\u7e2e\u65b9\u6cd5\uff0c\u5176\u4e2d\u5b78\u751f\u7522\u751f\u7684\u8f38\u51fa (SGO) \u7279\u5225\u503c\u5f97\u6ce8\u610f\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u6e1b\u5c11\u8a13\u7df4\u548c\u63a8\u8ad6\u4e4b\u9593\u7684\u4e0d\u5339\u914d\u3002\u7136\u800c\uff0cSGO \u7d93\u5e38\u7522\u751f\u96dc\u8a0a\u548c\u6709\u504f\u5dee\u7684\u5e8f\u5217\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u6559\u5e2b\u6a21\u578b\u7684\u8aa4\u5c0e\uff0c\u7279\u5225\u662f\u5728\u9577\u5e8f\u5217\u4e2d\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa SWITCH\uff08\u8207\u6559\u5e2b\u5171\u540c\u5b78\u7fd2\u77e5\u8b58\u84b8\u993e\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5b78\u751f\u7684\u5e8f\u5217\u751f\u6210\u904e\u7a0b\u4e2d\u7b56\u7565\u6027\u5730\u7d0d\u5165\u6559\u5e2b\u6a21\u578b\u3002SWITCH \u8b58\u5225\u6559\u5e2b\u548c\u5b78\u751f\u6a21\u578b\u7684\u6a19\u8a18\u6a5f\u7387\u4e4b\u9593\u7684\u5dee\u7570\uff0c\u5141\u8a31\u6559\u5e2b\u9078\u64c7\u6027\u5730\u4ecb\u5165\uff0c\u7279\u5225\u662f\u5728\u8f03\u9577\u7684\u5e8f\u5217\u4e2d\uff0c\u9019\u4e9b\u5e8f\u5217\u66f4\u5bb9\u6613\u53d7\u5230\u6559\u5e2b\u8aa4\u5c0e\u3002\u5728\u4e09\u500b\u6a21\u578b\u7cfb\u5217\u548c\u4e94\u500b\u6307\u4ee4\u9075\u5faa\u8cc7\u6599\u96c6\u4e2d\u7684\u5ee3\u6cdb\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0cSWITCH \u8d85\u8d8a\u4e86\u50b3\u7d71\u7684 KD \u65b9\u6cd5\uff0c\u7279\u5225\u662f\u5728\u751f\u6210\u9577\u5e8f\u5217\u8cc7\u6599\u65b9\u9762\u8868\u73fe\u51fa\u8272\u3002", "author": "Jahyun Koo et.al.", "authors": "Jahyun Koo, Yerin Hwang, Yongil Kim, Taegwan Kang, Hyunkyung Bae, Kyomin Jung", "id": "2410.19503v1", "paper_url": "http://arxiv.org/abs/2410.19503v1", "repo": "null"}}