{"2406.15330": {"publish_time": "2024-06-21", "title": "Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance", "paper_summary": "Large language models (LLMs) have revolutionized lots of fields of research.\nAlthough it is well-known that fine-tuning is essential for enhancing the\ncapabilities of LLMs, existing research suggests that there is potential\nredundancy in the fine-tuning process and therefore proposes to update only a\nsubset of parameters. However, these methods fail to leverage the task-specific\ninformation to identify important parameters during training. Based on the\ninsight that gradients inherently contain information on task-specific data, we\npropose Gradient-Mask Tuning (GMT), a method that selectively updates\nparameters during training based on their gradient information. Specifically,\nwe compute the absolute values of the gradients and apply masking to those with\nrelatively smaller magnitudes. Our empirical results across various tasks\ndemonstrate that GMT not only outperforms traditional fine-tuning methods but\nalso elevates the upper limits of LLM performance. Further analysis indicates\nthat GMT exhibits insensitivity to mask ratio and possesses computational\nefficiency comparable to vanilla SFT.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fb9\u5e95\u6539\u8b8a\u4e86\u8a31\u591a\u7814\u7a76\u9818\u57df\u3002\n\u5118\u7ba1\u773e\u6240\u5468\u77e5\u5fae\u8abf\u5c0d\u65bc\u63d0\u5347 LLM \u7684\u80fd\u529b\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u73fe\u6709\u7814\u7a76\u8868\u660e\u5fae\u8abf\u904e\u7a0b\u4e2d\u5b58\u5728\u6f5b\u5728\u7684\u5197\u9918\uff0c\u56e0\u6b64\u5efa\u8b70\u50c5\u66f4\u65b0\u53c3\u6578\u5b50\u96c6\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u7279\u5b9a\u4efb\u52d9\u7684\u8cc7\u8a0a\u4f86\u8b58\u5225\u8a13\u7df4\u671f\u9593\u7684\u91cd\u8981\u53c3\u6578\u3002\u57fa\u65bc\u68af\u5ea6\u672c\u8cea\u4e0a\u5305\u542b\u7279\u5b9a\u4efb\u52d9\u8cc7\u6599\u7684\u8cc7\u8a0a\u7684\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u68af\u5ea6\u906e\u7f69\u8abf\u6574 (GMT)\uff0c\u9019\u662f\u4e00\u7a2e\u5728\u8a13\u7df4\u671f\u9593\u6839\u64da\u5176\u68af\u5ea6\u8cc7\u8a0a\u9078\u64c7\u6027\u66f4\u65b0\u53c3\u6578\u7684\u65b9\u6cd5\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8a08\u7b97\u68af\u5ea6\u7684\u7d55\u5c0d\u503c\uff0c\u4e26\u5c0d\u5e45\u5ea6\u76f8\u5c0d\u8f03\u5c0f\u7684\u68af\u5ea6\u5957\u7528\u906e\u7f69\u3002\u6211\u5011\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u5be6\u8b49\u7d50\u679c\u8868\u660e\uff0cGMT \u4e0d\u50c5\u512a\u65bc\u50b3\u7d71\u7684\u5fae\u8abf\u65b9\u6cd5\uff0c\u800c\u4e14\u9084\u63d0\u5347\u4e86 LLM \u6548\u80fd\u7684\u4e0a\u9650\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0cGMT \u5c0d\u906e\u7f69\u6bd4\u4f8b\u4e0d\u654f\u611f\uff0c\u4e26\u4e14\u5177\u6709\u8207\u9999\u8349 SFT \u76f8\u7576\u7684\u904b\u7b97\u6548\u7387\u3002", "author": "Haoling Li et.al.", "authors": "Haoling Li, Xin Zhang, Xiao Liu, Yeyun Gong, Yifan Wang, Yujiu Yang, Qi Chen, Peng Cheng", "id": "2406.15330v1", "paper_url": "http://arxiv.org/abs/2406.15330v1", "repo": "null"}}