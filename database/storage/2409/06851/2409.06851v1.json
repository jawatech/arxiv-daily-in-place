{"2409.06851": {"publish_time": "2024-09-10", "title": "LIME-M: Less Is More for Evaluation of MLLMs", "paper_summary": "With the remarkable success achieved by Multimodal Large Language Models\n(MLLMs), numerous benchmarks have been designed to assess MLLMs' ability to\nguide their development in image perception tasks (e.g., image captioning and\nvisual question answering). However, the existence of numerous benchmarks\nresults in a substantial computational burden when evaluating model performance\nacross all of them. Moreover, these benchmarks contain many overly simple\nproblems or challenging samples, which do not effectively differentiate the\ncapabilities among various MLLMs. To address these challenges, we propose a\npipeline to process the existing benchmarks, which consists of two modules: (1)\nSemi-Automated Screening Process and (2) Eliminating Answer Leakage. The\nSemi-Automated Screening Process filters out samples that cannot distinguish\nthe model's capabilities by synthesizing various MLLMs and manually evaluating\nthem. The Eliminate Answer Leakage module filters samples whose answers can be\ninferred without images. Finally, we curate the LIME-M: Less Is More for\nEvaluation of Multimodal LLMs, a lightweight Multimodal benchmark that can more\neffectively evaluate the performance of different models. Our experiments\ndemonstrate that: LIME-M can better distinguish the performance of different\nMLLMs with fewer samples (24% of the original) and reduced time (23% of the\noriginal); LIME-M eliminates answer leakage, focusing mainly on the information\nwithin images; The current automatic metric (i.e., CIDEr) is insufficient for\nevaluating MLLMs' capabilities in captioning. Moreover, removing the caption\ntask score when calculating the overall score provides a more accurate\nreflection of model performance differences. All our codes and data are\nreleased at https://github.com/kangreen0210/LIME-M.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u53d6\u5f97\u986f\u8457\u6210\u529f\uff0c\u5df2\u7d93\u8a2d\u8a08\u4e86\u8a31\u591a\u57fa\u6e96\u4f86\u8a55\u4f30 MLLM \u5728\u5f71\u50cf\u611f\u77e5\u4efb\u52d9\uff08\u4f8b\u5982\u5f71\u50cf\u6a19\u984c\u548c\u8996\u89ba\u554f\u7b54\uff09\u4e2d\u5f15\u5c0e\u5176\u767c\u5c55\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u773e\u591a\u57fa\u6e96\u7684\u5b58\u5728\u6703\u5728\u8a55\u4f30\u6240\u6709\u6a21\u578b\u6548\u80fd\u6642\u9020\u6210\u5927\u91cf\u7684\u904b\u7b97\u8ca0\u64d4\u3002\u6b64\u5916\uff0c\u9019\u4e9b\u57fa\u6e96\u5305\u542b\u8a31\u591a\u904e\u65bc\u7c21\u55ae\u7684\u554f\u984c\u6216\u5177\u6311\u6230\u6027\u7684\u7bc4\u4f8b\uff0c\u7121\u6cd5\u6709\u6548\u5340\u5206\u5404\u7a2e MLLM \u7684\u529f\u80fd\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u8655\u7406\u73fe\u6709\u57fa\u6e96\u7684\u7ba1\u9053\uff0c\u5176\u4e2d\u5305\u542b\u5169\u500b\u6a21\u7d44\uff1a(1) \u534a\u81ea\u52d5\u7be9\u9078\u7a0b\u5e8f\u548c (2) \u6d88\u9664\u7b54\u6848\u5916\u6d29\u3002\u534a\u81ea\u52d5\u7be9\u9078\u7a0b\u5e8f\u6703\u7be9\u9078\u7121\u6cd5\u5340\u5206\u6a21\u578b\u529f\u80fd\u7684\u7bc4\u4f8b\uff0c\u65b9\u6cd5\u662f\u5408\u6210\u5404\u7a2e MLLM \u4e26\u624b\u52d5\u8a55\u4f30\u5b83\u5011\u3002\u6d88\u9664\u7b54\u6848\u5916\u6d29\u6a21\u7d44\u6703\u7be9\u9078\u51fa\u7b54\u6848\u53ef\u4ee5\u5728\u6c92\u6709\u5f71\u50cf\u7684\u60c5\u6cc1\u4e0b\u63a8\u8ad6\u51fa\u4f86\u7684\u7bc4\u4f8b\u3002\u6700\u5f8c\uff0c\u6211\u5011\u7b56\u5283\u4e86 LIME-M\uff1a\u66f4\u5c11\u5373\u66f4\u591a\uff0c\u7528\u65bc\u8a55\u4f30\u591a\u6a21\u614b LLM\uff0c\u9019\u662f\u4e00\u500b\u8f15\u91cf\u7d1a\u7684\u591a\u6a21\u614b\u57fa\u6e96\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u8a55\u4f30\u4e0d\u540c\u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff1aLIME-M \u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u7bc4\u4f8b\uff08\u539f\u59cb\u7684 24%\uff09\u548c\u66f4\u77ed\u7684\u6642\u9593\uff08\u539f\u59cb\u7684 23%\uff09\u4f86\u5340\u5206\u4e0d\u540c MLLM \u7684\u6548\u80fd\uff1bLIME-M \u6d88\u9664\u4e86\u7b54\u6848\u5916\u6d29\uff0c\u4e3b\u8981\u95dc\u6ce8\u5f71\u50cf\u4e2d\u7684\u8cc7\u8a0a\uff1b\u76ee\u524d\u7684\u81ea\u52d5\u8a55\u91cf\u6307\u6a19\uff08\u5373 CIDEr\uff09\u4e0d\u8db3\u4ee5\u8a55\u4f30 MLLM \u5728\u6a19\u984c\u4e2d\u7684\u529f\u80fd\u3002\u6b64\u5916\uff0c\u5728\u8a08\u7b97\u6574\u9ad4\u5206\u6578\u6642\u79fb\u9664\u6a19\u984c\u4efb\u52d9\u5206\u6578\uff0c\u53ef\u4ee5\u66f4\u6e96\u78ba\u5730\u53cd\u6620\u6a21\u578b\u6548\u80fd\u7684\u5dee\u7570\u3002\u6211\u5011\u6240\u6709\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u90fd\u767c\u4f48\u5728 https://github.com/kangreen0210/LIME-M\u3002</paragraph>", "author": "Kang Zhu et.al.", "authors": "Kang Zhu, Qianbo Zang, Shian Jia, Siwei Wu, Feiteng Fang, Yizhi Li, Shuyue Guo, Tianyu Zheng, Bo Li, Haoning Wu, Xingwei Qu, Jian Yang, Zachary Liu, Xiang Yue, J. H. Liu, Chenghua Lin, Min Yang, Shiwen Ni, Wenhao Huang, Ge Zhang", "id": "2409.06851v1", "paper_url": "http://arxiv.org/abs/2409.06851v1", "repo": "https://github.com/kangreen0210/lime-m"}}