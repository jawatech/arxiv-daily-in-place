{"2406.02539": {"publish_time": "2024-06-04", "title": "Parrot: Multilingual Visual Instruction Tuning", "paper_summary": "The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V\nhas marked a significant step towards artificial general intelligence. Existing\nmethods mainly focus on aligning vision encoders with LLMs through supervised\nfine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs'\ninherent ability to react to multiple languages progressively deteriorate as\nthe training process evolves. We empirically find that the imbalanced SFT\ndatasets, primarily composed of English-centric image-text pairs, lead to\nsignificantly reduced performance in non-English languages. This is due to the\nfailure of aligning the vision encoder and LLM with multilingual tokens during\nthe SFT process. In this paper, we introduce Parrot, a novel method that\nutilizes textual guidance to drive visual token alignment at the language\nlevel. Parrot makes the visual tokens condition on diverse language inputs and\nuses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.\nSpecifically, to enhance non-English visual tokens alignment, we compute the\ncross-attention using the initial visual features and textual embeddings, the\nresult of which is then fed into the MoE router to select the most relevant\nexperts. The selected experts subsequently convert the initial visual tokens\ninto language-specific visual tokens. Moreover, considering the current lack of\nbenchmarks for evaluating multilingual capabilities within the field, we\ncollect and make available a Massive Multilingual Multimodal Benchmark which\nincludes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our\nmethod not only demonstrates state-of-the-art performance on multilingual\nMMBench and MMMB, but also excels across a broad range of multimodal tasks.\nBoth the source code and the training dataset of Parrot will be made publicly\navailable.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f8b\u5982 GPT-4V\uff0c\u5df2\u6807\u5fd7\u7740\u671d\u7740\u4eba\u5de5\u667a\u80fd\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5c06\u89c6\u89c9\u7f16\u7801\u5668\u4e0e LLM \u5bf9\u9f50\uff0c\u4ee5\u8d4b\u4e88 LLM \u591a\u6a21\u6001\u80fd\u529b\uff0c\u4f7f MLLM \u968f\u7740\u8bad\u7ec3\u8fc7\u7a0b\u7684\u53d1\u5c55\u800c\u9010\u6e10\u4e27\u5931\u5bf9\u591a\u79cd\u8bed\u8a00\u505a\u51fa\u53cd\u5e94\u7684\u56fa\u6709\u80fd\u529b\u3002\u6211\u4eec\u51ed\u7ecf\u9a8c\u53d1\u73b0\uff0c\u4e0d\u5e73\u8861\u7684 SFT \u6570\u636e\u96c6\uff08\u4e3b\u8981\u7531\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u6587\u672c\u5bf9\u7ec4\u6210\uff09\u5bfc\u81f4\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u6027\u80fd\u663e\u7740\u4e0b\u964d\u3002\u8fd9\u662f\u7531\u4e8e\u5728 SFT \u8fc7\u7a0b\u4e2d\u89c6\u89c9\u7f16\u7801\u5668\u548c LLM \u4e0e\u591a\u8bed\u8a00\u6807\u8bb0\u5bf9\u9f50\u5931\u8d25\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Parrot\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u6587\u672c\u6307\u5bfc\u5728\u8bed\u8a00\u7ea7\u522b\u9a71\u52a8\u89c6\u89c9\u6807\u8bb0\u5bf9\u9f50\u3002Parrot \u4f7f\u89c6\u89c9\u6807\u8bb0\u4f9d\u8d56\u4e8e\u4e0d\u540c\u7684\u8bed\u8a00\u8f93\u5165\uff0c\u5e76\u4f7f\u7528\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6765\u4fc3\u8fdb\u591a\u8bed\u8a00\u6807\u8bb0\u7684\u5bf9\u9f50\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u4e86\u589e\u5f3a\u975e\u82f1\u8bed\u89c6\u89c9\u6807\u8bb0\u5bf9\u9f50\uff0c\u6211\u4eec\u4f7f\u7528\u521d\u59cb\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u5d4c\u5165\u8ba1\u7b97\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u7136\u540e\u5c06\u5176\u7ed3\u679c\u8f93\u5165 MoE \u8def\u7531\u5668\u4ee5\u9009\u62e9\u6700\u76f8\u5173\u7684\u4e13\u5bb6\u3002\u9009\u5b9a\u7684\u4e13\u5bb6\u968f\u540e\u5c06\u521d\u59cb\u89c6\u89c9\u6807\u8bb0\u8f6c\u6362\u4e3a\u7279\u5b9a\u8bed\u8a00\u7684\u89c6\u89c9\u6807\u8bb0\u3002\u6b64\u5916\uff0c\u8003\u8651\u5230\u5f53\u524d\u7f3a\u4e4f\u7528\u4e8e\u8bc4\u4f30\u8be5\u9886\u57df\u5185\u591a\u8bed\u8a00\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6211\u4eec\u6536\u96c6\u5e76\u63d0\u4f9b\u4e86\u5305\u542b 6 \u79cd\u8bed\u8a00\u300115 \u4e2a\u7c7b\u522b\u548c 12,000 \u4e2a\u95ee\u9898\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u79f0\u4e3a MMMB\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u5728\u591a\u8bed\u8a00 MMBench \u548c MMMB \u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800c\u4e14\u5728\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002Parrot \u7684\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u6570\u636e\u96c6\u90fd\u5c06\u516c\u5f00\u3002", "author": "Hai-Long Sun et.al.", "authors": "Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye", "id": "2406.02539v1", "paper_url": "http://arxiv.org/abs/2406.02539v1", "repo": "null"}}