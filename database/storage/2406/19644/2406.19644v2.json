{"2406.19644": {"publish_time": "2024-06-28", "title": "Beyond Human Preferences: Exploring Reinforcement Learning Trajectory Evaluation and Improvement through LLMs", "paper_summary": "Reinforcement learning (RL) faces challenges in evaluating policy\ntrajectories within intricate game tasks due to the difficulty in designing\ncomprehensive and precise reward functions. This inherent difficulty curtails\nthe broader application of RL within game environments characterized by diverse\nconstraints. Preference-based reinforcement learning (PbRL) presents a\npioneering framework that capitalizes on human preferences as pivotal reward\nsignals, thereby circumventing the need for meticulous reward engineering.\nHowever, obtaining preference data from human experts is costly and\ninefficient, especially under conditions marked by complex constraints. To\ntackle this challenge, we propose a LLM-enabled automatic preference generation\nframework named LLM4PG , which harnesses the capabilities of large language\nmodels (LLMs) to abstract trajectories, rank preferences, and reconstruct\nreward functions to optimize conditioned policies. Experiments on tasks with\ncomplex language constraints demonstrated the effectiveness of our LLM-enabled\nreward functions, accelerating RL convergence and overcoming stagnation caused\nby slow or absent progress under original reward structures. This approach\nmitigates the reliance on specialized human knowledge and demonstrates the\npotential of LLMs to enhance RL's effectiveness in complex environments in the\nwild.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u5728\u8a55\u4f30\u7b56\u7565\u8ecc\u8de1\u6642\uff0c\u7531\u65bc\u96e3\u4ee5\u8a2d\u8a08\u5168\u9762\u4e14\u7cbe\u78ba\u7684\u56de\u994b\u51fd\u6578\uff0c\u56e0\u6b64\u5728\u8907\u96dc\u7684\u904a\u6232\u4efb\u52d9\u4e2d\u9762\u81e8\u6311\u6230\u3002\u9019\u7a2e\u56fa\u6709\u7684\u56f0\u96e3\u9650\u5236\u4e86 RL \u5728\u5177\u6709\u4e0d\u540c\u9650\u5236\u7684\u904a\u6232\u74b0\u5883\u4e2d\u66f4\u5ee3\u6cdb\u7684\u61c9\u7528\u3002\u57fa\u65bc\u504f\u597d\u7684\u5f37\u5316\u5b78\u7fd2 (PbRL) \u63d0\u51fa\u4e86\u4e00\u500b\u958b\u5275\u6027\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u4eba\u985e\u504f\u597d\u4f5c\u70ba\u95dc\u9375\u56de\u994b\u8a0a\u865f\uff0c\u5f9e\u800c\u8ff4\u907f\u4e86\u5c0d\u7cbe\u7d30\u56de\u994b\u5de5\u7a0b\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u5f9e\u4eba\u985e\u5c08\u5bb6\u90a3\u88e1\u7372\u53d6\u504f\u597d\u6578\u64da\u65e2\u6602\u8cb4\u53c8\u4f4e\u6548\uff0c\u5c24\u5176\u662f\u5728\u53d7\u8907\u96dc\u9650\u5236\u689d\u4ef6\u7d04\u675f\u7684\u60c5\u6cc1\u4e0b\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba LLM4PG \u7684 LLM \u555f\u7528\u81ea\u52d5\u504f\u597d\u751f\u6210\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u529f\u80fd\u4f86\u62bd\u8c61\u8ecc\u8de1\u3001\u6392\u5217\u504f\u597d\uff0c\u4e26\u91cd\u5efa\u56de\u994b\u51fd\u6578\u4ee5\u6700\u4f73\u5316\u689d\u4ef6\u7b56\u7565\u3002\u5728\u5177\u6709\u8907\u96dc\u8a9e\u8a00\u9650\u5236\u7684\u4efb\u52d9\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011 LLM \u555f\u7528\u56de\u994b\u51fd\u6578\u7684\u6709\u6548\u6027\uff0c\u52a0\u901f\u4e86 RL \u6536\u6582\uff0c\u4e26\u514b\u670d\u4e86\u5728\u539f\u59cb\u56de\u994b\u7d50\u69cb\u4e0b\u9032\u5ea6\u7de9\u6162\u6216\u6c92\u6709\u9032\u5ea6\u9020\u6210\u7684\u505c\u6eef\u3002\u9019\u7a2e\u65b9\u6cd5\u6e1b\u8f15\u4e86\u5c0d\u5c08\u696d\u4eba\u985e\u77e5\u8b58\u7684\u4f9d\u8cf4\uff0c\u4e26\u5c55\u793a\u4e86 LLM \u5728\u589e\u5f37 RL \u5728\u91ce\u5916\u8907\u96dc\u74b0\u5883\u4e2d\u7684\u6709\u6548\u6027\u65b9\u9762\u7684\u6f5b\u529b\u3002", "author": "Zichao Shen et.al.", "authors": "Zichao Shen, Tianchen Zhu, Qingyun Sun, Shiqi Gao, Jianxin Li", "id": "2406.19644v2", "paper_url": "http://arxiv.org/abs/2406.19644v2", "repo": "null"}}