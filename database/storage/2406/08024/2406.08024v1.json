{"2406.08024": {"publish_time": "2024-06-12", "title": "Fewer Tokens and Fewer Videos: Extending Video Understanding Abilities in Large Vision-Language Models", "paper_summary": "Amidst the advancements in image-based Large Vision-Language Models\n(image-LVLM), the transition to video-based models (video-LVLM) is hindered by\nthe limited availability of quality video data. This paper addresses the\nchallenge by leveraging the visual commonalities between images and videos to\nefficiently evolve image-LVLMs into video-LVLMs. We present a cost-effective\nvideo-LVLM that enhances model architecture, introduces innovative training\nstrategies, and identifies the most effective types of video instruction data.\nOur innovative weighted token sampler significantly compresses the visual token\nnumbers of each video frame, effectively cutting computational expenses. We\nalso find that judiciously using just 10% of the video data, compared to prior\nvideo-LVLMs, yields impressive results during various training phases.\nMoreover, we delve into the influence of video instruction data in\nlimited-resource settings, highlighting the significance of incorporating video\ntraining data that emphasizes temporal understanding to enhance model\nperformance. The resulting Fewer Tokens and Fewer Videos LVLM (FTFV-LVLM)\nexhibits exceptional performance across video and image benchmarks, validating\nour model's design and training approaches.", "paper_summary_zh": "\u5728\u57fa\u65bc\u5f71\u50cf\u7684\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (image-LVLM) \u9032\u5c55\u4e2d\uff0c\u8f49\u63db\u70ba\u57fa\u65bc\u5f71\u7247\u7684\u6a21\u578b (video-LVLM) \u53d7\u5230\u5f71\u7247\u8cc7\u6599\u54c1\u8cea\u6709\u9650\u7684\u963b\u7919\u3002\u672c\u6587\u900f\u904e\u5229\u7528\u5f71\u50cf\u548c\u5f71\u7247\u4e4b\u9593\u7684\u8996\u89ba\u5171\u6027\uff0c\u63d0\u51fa\u4e00\u500b\u89e3\u6c7a\u65b9\u6848\uff0c\u6709\u6548\u5730\u5c07 image-LVLM \u8f49\u8b8a\u70ba video-LVLM\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5177\u6709\u6210\u672c\u6548\u76ca\u7684 video-LVLM\uff0c\u5b83\u589e\u5f37\u4e86\u6a21\u578b\u67b6\u69cb\u3001\u5f15\u9032\u5275\u65b0\u7684\u8a13\u7df4\u7b56\u7565\uff0c\u4e26\u627e\u51fa\u6700\u6709\u6548\u7684\u5f71\u7247\u6559\u5b78\u8cc7\u6599\u985e\u578b\u3002\u6211\u5011\u5275\u65b0\u7684\u52a0\u6b0a\u4ee3\u5e63\u53d6\u6a23\u5668\u5927\u5e45\u58d3\u7e2e\u6bcf\u500b\u5f71\u7247\u756b\u683c\u7684\u8996\u89ba\u4ee3\u5e63\u6578\u91cf\uff0c\u6709\u6548\u5730\u964d\u4f4e\u904b\u7b97\u6210\u672c\u3002\u6211\u5011\u4e5f\u767c\u73fe\uff0c\u8207\u5148\u524d\u7684 video-LVLM \u76f8\u6bd4\uff0c\u660e\u667a\u5730\u53ea\u4f7f\u7528 10% \u7684\u5f71\u7247\u8cc7\u6599\uff0c\u5c31\u80fd\u5728\u5404\u7a2e\u8a13\u7df4\u968e\u6bb5\u7522\u751f\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7d50\u679c\u3002\u6b64\u5916\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u5f71\u7247\u6559\u5b78\u8cc7\u6599\u5728\u8cc7\u6e90\u6709\u9650\u7684\u8a2d\u5b9a\u4e2d\u7684\u5f71\u97ff\uff0c\u5f37\u8abf\u7d0d\u5165\u5f37\u8abf\u6642\u9593\u7406\u89e3\u7684\u5f71\u7247\u8a13\u7df4\u8cc7\u6599\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u6548\u80fd\u7684\u91cd\u8981\u6027\u3002\u7522\u751f\u7684\u66f4\u5c11\u4ee3\u5e63\u548c\u66f4\u5c11\u5f71\u7247 LVLM (FTFV-LVLM) \u5728\u5f71\u7247\u548c\u5f71\u50cf\u57fa\u6e96\u4e0a\u5c55\u73fe\u51fa\u5091\u51fa\u7684\u6548\u80fd\uff0c\u9a57\u8b49\u4e86\u6211\u5011\u6a21\u578b\u7684\u8a2d\u8a08\u548c\u8a13\u7df4\u65b9\u6cd5\u3002", "author": "Shimin Chen et.al.", "authors": "Shimin Chen, Yitian Yuan, Shaoxiang Chen, Zequn Jie, Lin Ma", "id": "2406.08024v1", "paper_url": "http://arxiv.org/abs/2406.08024v1", "repo": "null"}}