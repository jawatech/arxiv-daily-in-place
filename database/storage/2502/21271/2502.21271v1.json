{"2502.21271": {"publish_time": "2025-02-28", "title": "Adaptive Keyframe Sampling for Long Video Understanding", "paper_summary": "Multimodal large language models (MLLMs) have enabled open-world visual\nunderstanding by injecting visual input as extra tokens into large language\nmodels (LLMs) as contexts. However, when the visual input changes from a single\nimage to a long video, the above paradigm encounters difficulty because the\nvast amount of video tokens has significantly exceeded the maximal capacity of\nMLLMs. Therefore, existing video-based MLLMs are mostly established upon\nsampling a small portion of tokens from input data, which can cause key\ninformation to be lost and thus produce incorrect answers. This paper presents\na simple yet effective algorithm named Adaptive Keyframe Sampling (AKS). It\ninserts a plug-and-play module known as keyframe selection, which aims to\nmaximize the useful information with a fixed number of video tokens. We\nformulate keyframe selection as an optimization involving (1) the relevance\nbetween the keyframes and the prompt, and (2) the coverage of the keyframes\nover the video, and present an adaptive algorithm to approximate the best\nsolution. Experiments on two long video understanding benchmarks validate that\nAdaptive Keyframe Sampling improves video QA accuracy (beyond strong baselines)\nupon selecting informative keyframes. Our study reveals the importance of\ninformation pre-filtering in video-based MLLMs. Code is available at\nhttps://github.com/ncTimTang/AKS.", "paper_summary_zh": "<paragraph>\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u80fd\u5920\u900f\u904e\u5c07\u8996\u89ba\u8f38\u5165\u4f5c\u70ba\u984d\u5916\u7684\u6a19\u8a18\u6ce8\u5165\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f5c\u70ba\u4e0a\u4e0b\u6587\uff0c\u5f9e\u800c\u5be6\u73fe\u958b\u653e\u4e16\u754c\u7684\u8996\u89ba\u7406\u89e3\u3002\u7136\u800c\uff0c\u7576\u8996\u89ba\u8f38\u5165\u5f9e\u55ae\u500b\u5716\u50cf\u8b8a\u70ba\u9577\u5f71\u7247\u6642\uff0c\u4e0a\u8ff0\u6a21\u5f0f\u5c31\u6703\u9047\u5230\u56f0\u96e3\uff0c\u56e0\u70ba\u5927\u91cf\u7684\u5f71\u7247\u6a19\u8a18\u5df2\u5927\u5927\u8d85\u904e MLLM \u7684\u6700\u5927\u5bb9\u91cf\u3002\u56e0\u6b64\uff0c\u73fe\u6709\u7684\u57fa\u65bc\u5f71\u7247\u7684 MLLM \u5927\u591a\u5efa\u7acb\u5728\u5f9e\u8f38\u5165\u6578\u64da\u4e2d\u63a1\u6a23\u4e00\u5c0f\u90e8\u5206\u6a19\u8a18\u7684\u57fa\u790e\u4e0a\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u95dc\u9375\u4fe1\u606f\u7684\u4e1f\u5931\uff0c\u5f9e\u800c\u7522\u751f\u932f\u8aa4\u7684\u7b54\u6848\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u7c21\u55ae\u800c\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u7a31\u70ba\u81ea\u9069\u61c9\u95dc\u9375\u5e40\u63a1\u6a23 (AKS)\u3002\u5b83\u63d2\u5165\u4e86\u4e00\u500b\u7a31\u70ba\u95dc\u9375\u5e40\u9078\u64c7\u7684\u5373\u63d2\u5373\u7528\u6a21\u7d44\uff0c\u65e8\u5728\u4ee5\u56fa\u5b9a\u6578\u91cf\u7684\u5f71\u7247\u6a19\u8a18\u6700\u5927\u5316\u6709\u7528\u4fe1\u606f\u3002\u6211\u5011\u5c07\u95dc\u9375\u5e40\u9078\u64c7\u516c\u5f0f\u5316\u70ba\u4e00\u500b\u6d89\u53ca (1) \u95dc\u9375\u5e40\u8207\u63d0\u793a\u4e4b\u9593\u7684\u76f8\u95dc\u6027\uff0c\u4ee5\u53ca (2) \u95dc\u9375\u5e40\u5c0d\u5f71\u7247\u7684\u8986\u84cb\u7bc4\u570d\u7684\u512a\u5316\u554f\u984c\uff0c\u4e26\u63d0\u51fa\u4e86\u4e00\u7a2e\u81ea\u9069\u61c9\u7b97\u6cd5\u4f86\u903c\u8fd1\u6700\u4f73\u89e3\u6c7a\u65b9\u6848\u3002\u5728\u5169\u500b\u9577\u5f71\u7247\u7406\u89e3\u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u9a57\u8b49\uff0c\u81ea\u9069\u61c9\u95dc\u9375\u5e40\u63a1\u6a23\u5728\u9078\u64c7\u4fe1\u606f\u8c50\u5bcc\u7684\u95dc\u9375\u5e40\u5f8c\uff0c\u63d0\u9ad8\u4e86\u5f71\u7247\u554f\u7b54\u7684\u6e96\u78ba\u6027\uff08\u8d85\u8d8a\u4e86\u5f37\u5927\u7684\u57fa\u6e96\uff09\u3002\u6211\u5011\u7684\u7814\u7a76\u63ed\u793a\u4e86\u4fe1\u606f\u9810\u904e\u6ffe\u5728\u57fa\u65bc\u5f71\u7247\u7684 MLLM \u4e2d\u7684\u91cd\u8981\u6027\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/ncTimTang/AKS \u7372\u5f97\u3002</paragraph>\n", "author": "Xi Tang et.al.", "authors": "Xi Tang, Jihao Qiu, Lingxi Xie, Yunjie Tian, Jianbin Jiao, Qixiang Ye", "id": "2502.21271v1", "paper_url": "http://arxiv.org/abs/2502.21271v1", "repo": "null"}}