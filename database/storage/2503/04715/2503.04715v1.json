{"2503.04715": {"publish_time": "2025-03-06", "title": "Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining", "paper_summary": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6a6b\u8de8\u5404\u7a2e\u4efb\u52d9\u7684\u9a5a\u4eba\u80fd\u529b\u73fe\u5df2\u5f97\u5230\u5145\u5206\u8b49\u5be6\uff0c\u4f46\u8981\u6709\u6548\u90e8\u7f72\u5b83\u5011\uff0c\u9700\u8981\u4ed4\u7d30\u7684\u8d85\u53c3\u6578\u512a\u5316\u3002\u900f\u904e\u5c0d\u5404\u7a2e\u914d\u7f6e\u9032\u884c\u7db2\u683c\u641c\u7d22\u7684\u5ee3\u6cdb\u5be6\u9a57\u7814\u7a76\uff0c\u6211\u5011\u767c\u73fe\u4e86\u63a7\u5236\u9019\u4e9b\u8d85\u53c3\u6578\u7684\u901a\u7528\u7e2e\u653e\u5b9a\u5f8b\uff1a\u6700\u4f73\u5b78\u7fd2\u7387\u8207\u6a21\u578b\u53c3\u6578\u548c\u6578\u64da\u5927\u5c0f\u90fd\u9075\u5faa\u51aa\u5f8b\u95dc\u4fc2\uff0c\u800c\u6700\u4f73\u6279\u91cf\u5927\u5c0f\u4e3b\u8981\u8207\u6578\u64da\u5927\u5c0f\u6210\u6bd4\u4f8b\u3002\u6211\u5011\u7684\u5206\u6790\u63ed\u793a\u4e86\u5728\u56fa\u5b9a\u6a21\u578b\u548c\u6578\u64da\u5927\u5c0f\u689d\u4ef6\u4e0b\uff0c\u8d85\u53c3\u6578\u7684\u51f8\u512a\u5316\u683c\u5c40\u3002\u9019\u7a2e\u51f8\u6027\u610f\u5473\u8457\u5b58\u5728\u4e00\u500b\u6700\u4f73\u8d85\u53c3\u6578\u5e73\u53f0\u671f\u3002\u6211\u5011\u70ba\u793e\u7fa4\u8ca2\u737b\u4e86\u4e00\u500b\u901a\u7528\u7684\u3001\u5373\u63d2\u5373\u7528\u7684\u6700\u4f73\u8d85\u53c3\u6578\u5de5\u5177\u3002\u5b83\u5728\u6e2c\u8a66\u96c6\u4e0a\u7684\u4f30\u8a08\u503c\u8207\u900f\u904e\u5fb9\u5e95\u641c\u7d22\u627e\u5230\u7684\u5168\u5c40\u6700\u4f73 LLM \u6027\u80fd\u50c5\u76f8\u5dee 0.07%\u3002\u9019\u4e9b\u5b9a\u5f8b\u5728\u6a21\u578b\u7a00\u758f\u6027\u3001\u8a13\u7df4\u6578\u64da\u5206\u4f48\u548c\u6a21\u578b\u5f62\u72c0\u7684\u8b8a\u5316\u65b9\u9762\u8868\u73fe\u51fa\u986f\u8457\u7684\u7a69\u5065\u6027\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u7d71\u4e00\u4e0d\u540c\u6a21\u578b\u5f62\u72c0\u548c\u7d50\u69cb\uff08\u4f8b\u5982\u5c08\u5bb6\u6df7\u5408\u6a21\u578b\u548c\u5bc6\u96c6\u8b8a\u63db\u5668\uff09\u4ee5\u53ca\u5efa\u7acb\u8de8\u4e0d\u540c\u6578\u64da\u5206\u4f48\u7684\u6700\u4f73\u8d85\u53c3\u6578\u7e2e\u653e\u5b9a\u5f8b\u7684\u5de5\u4f5c\u3002\u9019\u500b\u8a73\u76e1\u7684\u512a\u5316\u904e\u7a0b\u9700\u8981\u5927\u91cf\u7684\u8a08\u7b97\u8cc7\u6e90\uff0c\u4f7f\u7528\u4e86\u8fd1\u4e00\u767e\u842c\u500b NVIDIA H800 GPU \u5c0f\u6642\u4f86\u5f9e\u982d\u8a13\u7df4 3,700 \u500b\u4e0d\u540c\u5927\u5c0f\u548c\u8d85\u53c3\u6578\u7684 LLM\uff0c\u4e26\u7e3d\u5171\u6d88\u8017\u4e86\u5927\u7d04 100 \u5146\u500b token\u3002\u70ba\u4e86\u4fc3\u9032\u53ef\u91cd\u8907\u6027\u548c\u9032\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u5011\u5c07\u900f\u904e\u6211\u5011\u6307\u5b9a\u7684\u5b58\u5132\u5eab https://step-law.github.io/ \u9010\u6b65\u767c\u5e03\u6240\u6709\u640d\u5931\u6e2c\u91cf\u503c\u548c\u6a21\u578b\u6aa2\u67e5\u9ede\u3002</paragraph>\n", "author": "Houyi Li et.al.", "authors": "Houyi Li, Wenzheng Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Yangshijie Xu, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang", "id": "2503.04715v1", "paper_url": "http://arxiv.org/abs/2503.04715v1", "repo": "null"}}