{"2502.12964": {"publish_time": "2025-02-18", "title": "Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs", "paper_summary": "Large Language Models (LLMs) often generate outputs that lack grounding in\nreal-world facts, a phenomenon known as hallucinations. Prior research has\nassociated hallucinations with model uncertainty, leveraging this relationship\nfor hallucination detection and mitigation. In this paper, we challenge the\nunderlying assumption that all hallucinations are associated with uncertainty.\nUsing knowledge detection and uncertainty measurement methods, we demonstrate\nthat models can hallucinate with high certainty even when they have the correct\nknowledge. We further show that high-certainty hallucinations are consistent\nacross models and datasets, distinctive enough to be singled out, and challenge\nexisting mitigation methods. Our findings reveal an overlooked aspect of\nhallucinations, emphasizing the need to understand their origins and improve\nmitigation strategies to enhance LLM safety. The code is available at\nhttps://github.com/technion-cs-nlp/Trust_me_Im_wrong .", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7d93\u5e38\u7522\u751f\u7f3a\u4e4f\u771f\u5be6\u4e16\u754c\u4e8b\u5be6\u6839\u64da\u7684\u8f38\u51fa\uff0c\u9019\u7a2e\u73fe\u8c61\u7a31\u70ba\u5e7b\u89ba\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u5c07\u5e7b\u89ba\u8207\u6a21\u578b\u4e0d\u78ba\u5b9a\u6027\u806f\u7e6b\u8d77\u4f86\uff0c\u5229\u7528\u9019\u7a2e\u95dc\u4fc2\u9032\u884c\u5e7b\u89ba\u5075\u6e2c\u548c\u7de9\u89e3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6311\u6230\u6240\u6709\u5e7b\u89ba\u90fd\u8207\u4e0d\u78ba\u5b9a\u6027\u76f8\u95dc\u7684\u57fa\u672c\u5047\u8a2d\u3002\u4f7f\u7528\u77e5\u8b58\u5075\u6e2c\u548c\u4e0d\u78ba\u5b9a\u6027\u6e2c\u91cf\u65b9\u6cd5\uff0c\u6211\u5011\u8b49\u660e\u6a21\u578b\u5373\u4f7f\u64c1\u6709\u6b63\u78ba\u7684\u77e5\u8b58\uff0c\u4e5f\u80fd\u4ee5\u9ad8\u5ea6\u78ba\u5b9a\u6027\u7522\u751f\u5e7b\u89ba\u3002\u6211\u5011\u9032\u4e00\u6b65\u8868\u660e\uff0c\u9ad8\u78ba\u5b9a\u6027\u5e7b\u89ba\u5728\u6a21\u578b\u548c\u8cc7\u6599\u96c6\u4e4b\u9593\u662f\u4e00\u81f4\u7684\uff0c\u8db3\u5920\u7368\u7279\u4ee5\u81f3\u65bc\u53ef\u4ee5\u55ae\u7368\u6311\u9078\u51fa\u4f86\uff0c\u4e26\u6311\u6230\u73fe\u6709\u7684\u7de9\u89e3\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u5e7b\u89ba\u7684\u4e00\u500b\u88ab\u5ffd\u8996\u7684\u65b9\u9762\uff0c\u5f37\u8abf\u9700\u8981\u4e86\u89e3\u5176\u8d77\u6e90\u4e26\u6539\u9032\u7de9\u89e3\u7b56\u7565\u4ee5\u589e\u5f37 LLM \u5b89\u5168\u6027\u3002\u53ef\u4ee5\u5728 https://github.com/technion-cs-nlp/Trust_me_Im_wrong \u627e\u5230\u7a0b\u5f0f\u78bc\u3002", "author": "Adi Simhi et.al.", "authors": "Adi Simhi, Itay Itzhak, Fazl Barez, Gabriel Stanovsky, Yonatan Belinkov", "id": "2502.12964v1", "paper_url": "http://arxiv.org/abs/2502.12964v1", "repo": "null"}}