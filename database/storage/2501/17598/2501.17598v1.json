{"2501.17598": {"publish_time": "2025-01-29", "title": "Semantic Consistency Regularization with Large Language Models for Semi-supervised Sentiment Analysis", "paper_summary": "Accurate sentiment analysis of texts is crucial for a variety of\napplications, such as understanding customer feedback, monitoring market\ntrends, and detecting public sentiment. However, manually annotating large\nsentiment corpora for supervised learning is labor-intensive and\ntime-consuming. Therefore, it is essential and effective to develop a\nsemi-supervised method for the sentiment analysis task. Although some methods\nhave been proposed for semi-supervised text classification, they rely on the\nintrinsic information within the unlabeled data and the learning capability of\nthe NLP model, which lack generalization ability to the sentiment analysis\nscenario and may prone to overfit. Inspired by the ability of pretrained Large\nLanguage Models (LLMs) in following instructions and generating coherent text,\nwe propose a Semantic Consistency Regularization with Large Language Models\n(SCR) framework for semi-supervised sentiment analysis. We introduce two\nprompting strategies to semantically enhance unlabeled text using LLMs. The\nfirst is Entity-based Enhancement (SCR-EE), which involves extracting entities\nand numerical information, and querying the LLM to reconstruct the textual\ninformation. The second is Concept-based Enhancement (SCR-CE), which directly\nqueries the LLM with the original sentence for semantic reconstruction.\nSubsequently, the LLM-augmented data is utilized for a consistency loss with\nconfidence thresholding, which preserves high-quality agreement samples to\nprovide additional supervision signals during training. Furthermore, to fully\nutilize the uncertain unlabeled data samples, we propose a class re-assembling\nstrategy inspired by the class space shrinking theorem. Experiments show our\nmethod achieves remarkable performance over prior semi-supervised methods.", "paper_summary_zh": "\u6e96\u78ba\u7684\u60c5\u611f\u5206\u6790\u5c0d\u65bc\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u81f3\u95dc\u91cd\u8981\uff0c\u4f8b\u5982\u4e86\u89e3\u5ba2\u6236\u56de\u994b\u3001\u76e3\u63a7\u5e02\u5834\u8da8\u52e2\u548c\u5075\u6e2c\u516c\u773e\u60c5\u7dd2\u3002\u7136\u800c\uff0c\u624b\u52d5\u8a3b\u89e3\u5927\u578b\u60c5\u611f\u8a9e\u6599\u5eab\u4ee5\u9032\u884c\u76e3\u7763\u5f0f\u5b78\u7fd2\u65e2\u8cbb\u529b\u53c8\u8cbb\u6642\u3002\u56e0\u6b64\uff0c\u958b\u767c\u4e00\u7a2e\u534a\u76e3\u7763\u5f0f\u7684\u60c5\u611f\u5206\u6790\u4efb\u52d9\u65b9\u6cd5\u81f3\u95dc\u91cd\u8981\u4e14\u6709\u6548\u3002\u5118\u7ba1\u5df2\u7d93\u63d0\u51fa\u4e86\u4e00\u4e9b\u534a\u76e3\u7763\u5f0f\u6587\u672c\u5206\u985e\u65b9\u6cd5\uff0c\u4f46\u5b83\u5011\u4f9d\u8cf4\u65bc\u672a\u6a19\u8a18\u8cc7\u6599\u4e2d\u7684\u5167\u5728\u8cc7\u8a0a\u548c NLP \u6a21\u578b\u7684\u5b78\u7fd2\u80fd\u529b\uff0c\u9019\u7f3a\u4e4f\u5c0d\u60c5\u611f\u5206\u6790\u60c5\u5883\u7684\u6982\u62ec\u80fd\u529b\uff0c\u4e26\u4e14\u53ef\u80fd\u5bb9\u6613\u904e\u5ea6\u64ec\u5408\u3002\u53d7\u5230\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9075\u5faa\u8aaa\u660e\u548c\u751f\u6210\u9023\u8cab\u6587\u672c\u65b9\u9762\u7684\u80fd\u529b\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5177\u6709\u5927\u578b\u8a9e\u8a00\u6a21\u578b (SCR) \u7684\u8a9e\u7fa9\u4e00\u81f4\u6027\u6b63\u5247\u5316\u6846\u67b6\uff0c\u7528\u65bc\u534a\u76e3\u7763\u5f0f\u60c5\u611f\u5206\u6790\u3002\u6211\u5011\u5f15\u5165\u4e86\u5169\u7a2e\u63d0\u793a\u7b56\u7565\uff0c\u4ee5\u4f7f\u7528 LLM \u8a9e\u7fa9\u589e\u5f37\u672a\u6a19\u8a18\u6587\u672c\u3002\u7b2c\u4e00\u500b\u662f\u57fa\u65bc\u5be6\u9ad4\u7684\u589e\u5f37 (SCR-EE)\uff0c\u5b83\u6d89\u53ca\u63d0\u53d6\u5be6\u9ad4\u548c\u6578\u5b57\u8cc7\u8a0a\uff0c\u4e26\u67e5\u8a62 LLM \u4ee5\u91cd\u5efa\u6587\u672c\u8cc7\u8a0a\u3002\u7b2c\u4e8c\u500b\u662f\u57fa\u65bc\u6982\u5ff5\u7684\u589e\u5f37 (SCR-CE)\uff0c\u5b83\u76f4\u63a5\u67e5\u8a62\u5177\u6709\u8a9e\u7fa9\u91cd\u5efa\u529f\u80fd\u7684\u539f\u59cb\u53e5\u5b50\u7684 LLM\u3002\u96a8\u5f8c\uff0c\u5c07 LLM \u64f4\u5145\u7684\u8cc7\u6599\u7528\u65bc\u5177\u6709\u4fe1\u5fc3\u95be\u503c\u7684\u7a20\u5bc6\u640d\u5931\uff0c\u9019\u4fdd\u7559\u4e86\u9ad8\u54c1\u8cea\u7684\u4e00\u81f4\u6027\u6a23\u672c\uff0c\u4ee5\u4fbf\u5728\u8a13\u7df4\u671f\u9593\u63d0\u4f9b\u984d\u5916\u7684\u76e3\u7763\u8a0a\u865f\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u5145\u5206\u5229\u7528\u4e0d\u78ba\u5b9a\u7684\u672a\u6a19\u8a18\u8cc7\u6599\u6a23\u672c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u985e\u5225\u91cd\u65b0\u7d44\u88dd\u7b56\u7565\uff0c\u5176\u9748\u611f\u4f86\u81ea\u985e\u5225\u7a7a\u9593\u7e2e\u6e1b\u5b9a\u7406\u3002\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u6bd4\u5148\u524d\u7684\u534a\u76e3\u7763\u5f0f\u65b9\u6cd5\u53d6\u5f97\u4e86\u986f\u8457\u7684\u6548\u80fd\u3002", "author": "Kunrong Li et.al.", "authors": "Kunrong Li, Xinyu Liu, Zhen Chen", "id": "2501.17598v1", "paper_url": "http://arxiv.org/abs/2501.17598v1", "repo": "null"}}