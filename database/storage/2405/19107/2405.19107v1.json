{"2405.19107": {"publish_time": "2024-05-29", "title": "Offline Regularised Reinforcement Learning for Large Language Models Alignment", "paper_summary": "The dominant framework for alignment of large language models (LLM), whether\nthrough reinforcement learning from human feedback or direct preference\noptimisation, is to learn from preference data. This involves building datasets\nwhere each element is a quadruplet composed of a prompt, two independent\nresponses (completions of the prompt) and a human preference between the two\nindependent responses, yielding a preferred and a dis-preferred response. Such\ndata is typically scarce and expensive to collect. On the other hand,\n\\emph{single-trajectory} datasets where each element is a triplet composed of a\nprompt, a response and a human feedback is naturally more abundant. The\ncanonical element of such datasets is for instance an LLM's response to a\nuser's prompt followed by a user's feedback such as a thumbs-up/down.\nConsequently, in this work, we propose DRO, or \\emph{Direct Reward\nOptimisation}, as a framework and associated algorithms that do not require\npairwise preferences. DRO uses a simple mean-squared objective that can be\nimplemented in various ways. We validate our findings empirically, using T5\nencoder-decoder language models, and show DRO's performance over selected\nbaselines such as Kahneman-Tversky Optimization (KTO). Thus, we confirm that\nDRO is a simple and empirically compelling method for single-trajectory policy\noptimisation.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0d\u9f4a\u7684\u4e3b\u6d41\u67b6\u69cb\uff0c\u7121\u8ad6\u662f\u900f\u904e\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2\u6216\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316\uff0c\u90fd\u662f\u5f9e\u504f\u597d\u8cc7\u6599\u4e2d\u5b78\u7fd2\u3002\u9019\u6d89\u53ca\u5efa\u7acb\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u6bcf\u500b\u5143\u7d20\u90fd\u662f\u4e00\u500b\u56db\u5143\u7d44\uff0c\u7531\u63d0\u793a\u3001\u5169\u500b\u7368\u7acb\u7684\u56de\u61c9\uff08\u63d0\u793a\u7684\u5b8c\u6210\uff09\u548c\u4eba\u985e\u5c0d\u5169\u500b\u7368\u7acb\u56de\u61c9\u4e4b\u9593\u7684\u504f\u597d\u7d44\u6210\uff0c\u7522\u751f\u4e00\u500b\u504f\u597d\u7684\u56de\u61c9\u548c\u4e00\u500b\u4e0d\u504f\u597d\u7684\u56de\u61c9\u3002\u6b64\u985e\u8cc7\u6599\u901a\u5e38\u7a00\u7f3a\u4e14\u6536\u96c6\u6210\u672c\u9ad8\u6602\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6bcf\u500b\u5143\u7d20\u90fd\u662f\u7531\u63d0\u793a\u3001\u56de\u61c9\u548c\u4eba\u985e\u56de\u994b\u7d44\u6210\u7684\u4e00\u500b\u4e09\u5143\u7d44\uff0c\u55ae\u8ecc\u8cc7\u6599\u96c6\u81ea\u7136\u66f4\u8c50\u5bcc\u3002\u6b64\u985e\u8cc7\u6599\u96c6\u7684\u5178\u578b\u5143\u7d20\u4f8b\u5982 LLM \u5c0d\u4f7f\u7528\u8005\u63d0\u793a\u7684\u56de\u61c9\uff0c\u5f8c\u9762\u63a5\u8457\u4f7f\u7528\u8005\u7684\u56de\u994b\uff0c\u4f8b\u5982\u6309\u8b9a/\u6309\u5012\u8b9a\u3002\u56e0\u6b64\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa DRO\uff0c\u6216\u76f4\u63a5\u56de\u994b\u6700\u4f73\u5316\uff0c\u4f5c\u70ba\u4e00\u500b\u67b6\u69cb\u548c\u76f8\u95dc\u6f14\u7b97\u6cd5\uff0c\u4e0d\u9700\u8981\u6210\u5c0d\u504f\u597d\u3002DRO \u4f7f\u7528\u4e00\u500b\u7c21\u55ae\u7684\u5747\u65b9\u76ee\u6a19\uff0c\u53ef\u4ee5\u7528\u5404\u7a2e\u65b9\u5f0f\u5be6\u4f5c\u3002\u6211\u5011\u4f7f\u7528 T5 \u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u8a9e\u8a00\u6a21\u578b\uff0c\u4ee5\u7d93\u9a57\u65b9\u5f0f\u9a57\u8b49\u6211\u5011\u7684\u767c\u73fe\uff0c\u4e26\u5c55\u793a DRO \u5728\u9078\u64c7\u7684\u57fa\u6e96\uff08\u4f8b\u5982 Kahneman-Tversky \u6700\u4f73\u5316 (KTO)\uff09\u4e0a\u7684\u6548\u80fd\u3002\u56e0\u6b64\uff0c\u6211\u5011\u78ba\u8a8d DRO \u662f\u4e00\u7a2e\u7c21\u55ae\u4e14\u7d93\u9a57\u4e0a\u4ee4\u4eba\u4fe1\u670d\u7684\u55ae\u8ecc\u653f\u7b56\u6700\u4f73\u5316\u65b9\u6cd5\u3002", "author": "Pierre Harvey Richemond et.al.", "authors": "Pierre Harvey Richemond, Yunhao Tang, Daniel Guo, Daniele Calandriello, Mohammad Gheshlaghi Azar, Rafael Rafailov, Bernardo Avila Pires, Eugene Tarassov, Lucas Spangher, Will Ellsworth, Aliaksei Severyn, Jonathan Mallinson, Lior Shani, Gil Shamir, Rishabh Joshi, Tianqi Liu, Remi Munos, Bilal Piot", "id": "2405.19107v1", "paper_url": "http://arxiv.org/abs/2405.19107v1", "repo": "null"}}