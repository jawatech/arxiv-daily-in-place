{"2407.04047": {"publish_time": "2024-07-04", "title": "Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis", "paper_summary": "This paper investigates the use of unsupervised text-to-speech synthesis\n(TTS) as a data augmentation method to improve accented speech recognition. TTS\nsystems are trained with a small amount of accented speech training data and\ntheir pseudo-labels rather than manual transcriptions, and hence unsupervised.\nThis approach enables the use of accented speech data without manual\ntranscriptions to perform data augmentation for accented speech recognition.\nSynthetic accented speech data, generated from text prompts by using the TTS\nsystems, are then combined with available non-accented speech data to train\nautomatic speech recognition (ASR) systems. ASR experiments are performed in a\nself-supervised learning framework using a Wav2vec2.0 model which was\npre-trained on large amount of unsupervised accented speech data. The accented\nspeech data for training the unsupervised TTS are read speech, selected from\nL2-ARCTIC and British Isles corpora, while spontaneous conversational speech\nfrom the Edinburgh international accents of English corpus are used as the\nevaluation data. Experimental results show that Wav2vec2.0 models which are\nfine-tuned to downstream ASR task with synthetic accented speech data,\ngenerated by the unsupervised TTS, yield up to 6.1% relative word error rate\nreductions compared to a Wav2vec2.0 baseline which is fine-tuned with the\nnon-accented speech data from Librispeech corpus.", "paper_summary_zh": "\u9019\u7bc7\u8ad6\u6587\u63a2\u8a0e\u4e86\u5c07\u975e\u76e3\u7763\u6587\u5b57\u8f49\u8a9e\u97f3\u5408\u6210 (TTS) \u7528\u4f5c\u8cc7\u6599\u64f4\u5145\u65b9\u6cd5\uff0c\u4ee5\u6539\u5584\u91cd\u97f3\u8a9e\u97f3\u8fa8\u8b58\u3002TTS \u7cfb\u7d71\u4f7f\u7528\u5c11\u91cf\u91cd\u97f3\u8a9e\u97f3\u8a13\u7df4\u8cc7\u6599\u548c\u5176\u507d\u6a19\u7c64\uff08\u800c\u975e\u624b\u52d5\u8f49\u9304\uff09\u9032\u884c\u8a13\u7df4\uff0c\u56e0\u6b64\u662f\u975e\u76e3\u7763\u7684\u3002\u6b64\u65b9\u6cd5\u80fd\u4f7f\u7528\u91cd\u97f3\u8a9e\u97f3\u8cc7\u6599\uff0c\u800c\u7121\u9700\u624b\u52d5\u8f49\u9304\uff0c\u5c31\u80fd\u70ba\u91cd\u97f3\u8a9e\u97f3\u8fa8\u8b58\u57f7\u884c\u8cc7\u6599\u64f4\u5145\u3002\u7531 TTS \u7cfb\u7d71\u4f7f\u7528\u6587\u5b57\u63d0\u793a\u7522\u751f\u7684\u5408\u6210\u91cd\u97f3\u8a9e\u97f3\u8cc7\u6599\uff0c\u6703\u8207\u73fe\u6709\u7684\u975e\u91cd\u97f3\u8a9e\u97f3\u8cc7\u6599\u7d50\u5408\uff0c\u4ee5\u8a13\u7df4\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u7cfb\u7d71\u3002ASR \u5be6\u9a57\u662f\u5728\u81ea\u76e3\u7763\u5b78\u7fd2\u67b6\u69cb\u4e2d\u9032\u884c\uff0c\u4f7f\u7528 Wav2vec2.0 \u6a21\u578b\uff0c\u8a72\u6a21\u578b\u7d93\u904e\u5927\u91cf\u975e\u76e3\u7763\u91cd\u97f3\u8a9e\u97f3\u8cc7\u6599\u9810\u8a13\u7df4\u3002\u7528\u65bc\u8a13\u7df4\u975e\u76e3\u7763 TTS \u7684\u91cd\u97f3\u8a9e\u97f3\u8cc7\u6599\u662f\u6717\u8b80\u8a9e\u97f3\uff0c\u9078\u81ea L2-ARCTIC \u548c\u82f1\u570b\u7fa4\u5cf6\u8a9e\u6599\u5eab\uff0c\u800c\u611b\u4e01\u5821\u570b\u969b\u82f1\u8a9e\u53e3\u97f3\u8a9e\u6599\u5eab\u4e2d\u7684\u81ea\u767c\u5c0d\u8a71\u8a9e\u97f3\u5247\u7528\u4f5c\u8a55\u4f30\u8cc7\u6599\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u91dd\u5c0d\u4e0b\u6e38 ASR \u4efb\u52d9\u9032\u884c\u5fae\u8abf\u7684 Wav2vec2.0 \u6a21\u578b\uff0c\u4f7f\u7528\u975e\u76e3\u7763 TTS \u751f\u6210\u7684\u5408\u6210\u91cd\u97f3\u8a9e\u97f3\u8cc7\u6599\uff0c\u8207\u4f7f\u7528 Librispeech \u8a9e\u6599\u5eab\u4e2d\u7684\u975e\u91cd\u97f3\u8a9e\u97f3\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u7684 Wav2vec2.0 \u57fa\u6e96\u76f8\u6bd4\uff0c\u76f8\u5c0d\u5b57\u5143\u932f\u8aa4\u7387\u964d\u4f4e\u4e86 6.1%\u3002", "author": "Cong-Thanh Do et.al.", "authors": "Cong-Thanh Do, Shuhei Imai, Rama Doddipatla, Thomas Hain", "id": "2407.04047v1", "paper_url": "http://arxiv.org/abs/2407.04047v1", "repo": "null"}}