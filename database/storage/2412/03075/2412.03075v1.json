{"2412.03075": {"publish_time": "2024-12-04", "title": "ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error Correction", "paper_summary": "Automatic speech Recognition (ASR) is a fundamental and important task in the\nfield of speech and natural language processing. It is an inherent building\nblock in many applications such as voice assistant, speech translation, etc.\nDespite the advancement of ASR technologies in recent years, it is still\ninevitable for modern ASR systems to have a substantial number of erroneous\nrecognition due to environmental noise, ambiguity, etc. Therefore, the error\ncorrection in ASR is crucial.\n  Motivated by this, this paper studies ASR error correction in the Chinese\nlanguage, which is one of the most popular languages and enjoys a large number\nof users in the world. We first create a benchmark dataset named \\emph{ASR-EC}\nthat contains a wide spectrum of ASR errors generated by industry-grade ASR\nsystems. To the best of our knowledge, it is the first Chinese ASR error\ncorrection benchmark. Then, inspired by the recent advances in \\emph{large\nlanguage models (LLMs)}, we investigate how to harness the power of LLMs to\ncorrect ASR errors. We apply LLMs to ASR error correction in three paradigms.\nThe first paradigm is prompting, which is further categorized as zero-shot,\nfew-shot, and multi-step. The second paradigm is finetuning, which finetunes\nLLMs with ASR error correction data. The third paradigm is multi-modal\naugmentation, which collectively utilizes the audio and ASR transcripts for\nerror correction. Extensive experiments reveal that prompting is not effective\nfor ASR error correction. Finetuning is effective only for a portion of LLMs.\nMulti-modal augmentation is the most effective method for error correction and\nachieves state-of-the-art performance.", "paper_summary_zh": "<paragraph>\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u662f\u8a9e\u97f3\u8207\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u9818\u57df\u4e2d\u7684\u4e00\u9805\u57fa\u672c\u4e14\u91cd\u8981\u7684\u4efb\u52d9\u3002\u5b83\u662f\u8a31\u591a\u61c9\u7528\u7a0b\u5f0f\u4e2d\u56fa\u6709\u7684\u7d44\u6210\u90e8\u5206\uff0c\u4f8b\u5982\u8a9e\u97f3\u52a9\u7406\u3001\u8a9e\u97f3\u7ffb\u8b6f\u7b49\u3002\u5118\u7ba1\u8fd1\u5e74\u4f86 ASR \u6280\u8853\u9032\u6b65\uff0c\u4f46\u73fe\u4ee3 ASR \u7cfb\u7d71\u4ecd\u96e3\u514d\u6703\u56e0\u74b0\u5883\u566a\u97f3\u3001\u6b67\u7fa9\u7b49\u56e0\u7d20\u7522\u751f\u5927\u91cf\u932f\u8aa4\u8fa8\u8b58\u3002\u56e0\u6b64\uff0cASR \u4e2d\u7684\u932f\u8aa4\u6821\u6b63\u81f3\u95dc\u91cd\u8981\u3002\n\u53d7\u6b64\u555f\u767c\uff0c\u672c\u6587\u7814\u7a76\u4e86\u4e2d\u6587 ASR \u932f\u8aa4\u6821\u6b63\uff0c\u4e2d\u6587\u662f\u6700\u6d41\u884c\u7684\u8a9e\u8a00\u4e4b\u4e00\uff0c\u5728\u5168\u7403\u64c1\u6709\u5927\u91cf\u7684\u4f7f\u7528\u8005\u3002\u6211\u5011\u9996\u5148\u5efa\u7acb\u4e86\u4e00\u500b\u540d\u70ba \\emph{ASR-EC} \u7684\u57fa\u6e96\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u7531\u7522\u696d\u7d1a ASR \u7cfb\u7d71\u7522\u751f\u7684\u5404\u7a2e ASR \u932f\u8aa4\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u4e2d\u6587 ASR \u932f\u8aa4\u6821\u6b63\u57fa\u6e96\u3002\u63a5\u8457\uff0c\u53d7\u5230 \\emph{\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)} \u8fd1\u671f\u9032\u5c55\u7684\u555f\u767c\uff0c\u6211\u5011\u63a2\u8a0e\u5982\u4f55\u5229\u7528 LLM \u7684\u529b\u91cf\u4f86\u6821\u6b63 ASR \u932f\u8aa4\u3002\u6211\u5011\u5c07 LLM \u61c9\u7528\u65bc ASR \u932f\u8aa4\u6821\u6b63\u7684\u4e09\u7a2e\u7bc4\u4f8b\u3002\u7b2c\u4e00\u500b\u7bc4\u4f8b\u662f\u63d0\u793a\uff0c\u9032\u4e00\u6b65\u5206\u985e\u70ba\u96f6\u6b21\u5b78\u7fd2\u3001\u5c11\u6b21\u5b78\u7fd2\u548c\u591a\u6b65\u9a5f\u3002\u7b2c\u4e8c\u500b\u7bc4\u4f8b\u662f\u5fae\u8abf\uff0c\u4f7f\u7528 ASR \u932f\u8aa4\u6821\u6b63\u8cc7\u6599\u5fae\u8abf LLM\u3002\u7b2c\u4e09\u500b\u7bc4\u4f8b\u662f\u591a\u6a21\u5f0f\u64f4\u5145\uff0c\u5171\u540c\u5229\u7528\u97f3\u8a0a\u548c ASR \u8f49\u9304\u9032\u884c\u932f\u8aa4\u6821\u6b63\u3002\u5927\u91cf\u7684\u5be6\u9a57\u986f\u793a\uff0c\u63d0\u793a\u5c0d\u65bc ASR \u932f\u8aa4\u6821\u6b63\u7121\u6548\u3002\u5fae\u8abf\u50c5\u5c0d\u90e8\u5206 LLM \u6709\u6548\u3002\u591a\u6a21\u5f0f\u64f4\u5145\u662f\u932f\u8aa4\u6821\u6b63\u6700\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4e26\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002</paragraph>", "author": "Victor Junqiu Wei et.al.", "authors": "Victor Junqiu Wei, Weicheng Wang, Di Jiang, Yuanfeng Song, Lu Wang", "id": "2412.03075v1", "paper_url": "http://arxiv.org/abs/2412.03075v1", "repo": "null"}}