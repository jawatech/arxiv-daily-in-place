{"2502.09331": {"publish_time": "2025-02-13", "title": "Beyond English: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual LLMs", "paper_summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs) across diverse tasks, English remains the dominant language for LLM\nresearch and development. So, when working with a different language, this has\nled to the widespread practice of pre-translation, i.e., translating the task\nprompt into English before inference. Selective pre-translation, a more\nsurgical approach, focuses on translating specific prompt components. However,\nits current use is sporagic and lacks a systematic research foundation.\nConsequently, the optimal pre-translation strategy for various multilingual\nsettings and tasks remains unclear. In this work, we aim to uncover the optimal\nsetup for pre-translation by systematically assessing its use. Specifically, we\nview the prompt as a modular entity, composed of four functional parts:\ninstruction, context, examples, and output, either of which could be translated\nor not. We evaluate pre-translation strategies across 35 languages covering\nboth low and high-resource languages, on various tasks including Question\nAnswering (QA), Natural Language Inference (NLI), Named Entity Recognition\n(NER), and Abstractive Summarization. Our experiments show the impact of\nfactors as similarity to English, translation quality and the size of\npre-trained data, on the model performance with pre-translation. We suggest\npractical guidelines for choosing optimal strategies in various multilingual\nsettings.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u591a\u8a9e\u8a00\u80fd\u529b\u6709\u9032\u6b65\uff0c\u82f1\u8a9e\u4ecd\u7136\u662f LLM \u7814\u7a76\u548c\u958b\u767c\u7684\u4e3b\u5c0e\u8a9e\u8a00\u3002\u56e0\u6b64\uff0c\u5728\u4f7f\u7528\u4e0d\u540c\u8a9e\u8a00\u6642\uff0c\u9019\u5c0e\u81f4\u4e86\u9810\u7ffb\u8b6f\u7684\u5ee3\u6cdb\u5be6\u52d9\uff0c\u5373\u5728\u63a8\u7406\u4e4b\u524d\u5c07\u4efb\u52d9\u63d0\u793a\u7ffb\u8b6f\u6210\u82f1\u8a9e\u3002\u9078\u64c7\u6027\u9810\u7ffb\u8b6f\u662f\u4e00\u7a2e\u66f4\u7cbe\u6e96\u7684\u65b9\u6cd5\uff0c\u5c08\u6ce8\u65bc\u7ffb\u8b6f\u7279\u5b9a\u63d0\u793a\u7d44\u6210\u90e8\u5206\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u4f7f\u7528\u662f\u96f6\u661f\u7684\uff0c\u7f3a\u4e4f\u7cfb\u7d71\u6027\u7684\u7814\u7a76\u57fa\u790e\u3002\u56e0\u6b64\uff0c\u5404\u7a2e\u591a\u8a9e\u8a00\u8a2d\u5b9a\u548c\u4efb\u52d9\u7684\u6700\u4f73\u9810\u7ffb\u8b6f\u7b56\u7565\u4ecd\u4e0d\u6e05\u695a\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u65e8\u5728\u900f\u904e\u7cfb\u7d71\u6027\u8a55\u4f30\u9810\u7ffb\u8b6f\u7684\u4f7f\u7528\uff0c\u627e\u51fa\u5176\u6700\u4f73\u8a2d\u5b9a\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u63d0\u793a\u8996\u70ba\u4e00\u500b\u6a21\u7d44\u5316\u5be6\u9ad4\uff0c\u7531\u56db\u500b\u529f\u80fd\u90e8\u5206\u7d44\u6210\uff1a\u8aaa\u660e\u3001\u80cc\u666f\u3001\u7bc4\u4f8b\u548c\u8f38\u51fa\uff0c\u5176\u4e2d\u4efb\u4f55\u4e00\u500b\u90fd\u53ef\u4ee5\u7ffb\u8b6f\u6216\u4e0d\u7ffb\u8b6f\u3002\u6211\u5011\u5728 35 \u7a2e\u8a9e\u8a00\u4e2d\u8a55\u4f30\u9810\u7ffb\u8b6f\u7b56\u7565\uff0c\u6db5\u84cb\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u548c\u9ad8\u8cc7\u6e90\u8a9e\u8a00\uff0c\u4ee5\u53ca\u5404\u7a2e\u4efb\u52d9\uff0c\u5305\u62ec\u554f\u7b54 (QA)\u3001\u81ea\u7136\u8a9e\u8a00\u63a8\u7406 (NLI)\u3001\u547d\u540d\u5be6\u9ad4\u8b58\u5225 (NER) \u548c\u62bd\u8c61\u6458\u8981\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\u4e86\u8207\u82f1\u8a9e\u7684\u76f8\u4f3c\u6027\u3001\u7ffb\u8b6f\u54c1\u8cea\u548c\u9810\u8a13\u7df4\u8cc7\u6599\u5927\u5c0f\u7b49\u56e0\u7d20\u5c0d\u9810\u7ffb\u8b6f\u6a21\u578b\u6548\u80fd\u7684\u5f71\u97ff\u3002\u6211\u5011\u5efa\u8b70\u5728\u5404\u7a2e\u591a\u8a9e\u8a00\u8a2d\u5b9a\u4e2d\u9078\u64c7\u6700\u4f73\u7b56\u7565\u7684\u5be6\u7528\u6307\u5357\u3002", "author": "Itai Mondshine et.al.", "authors": "Itai Mondshine, Tzuf Paz-Argaman, Reut Tsarfaty", "id": "2502.09331v1", "paper_url": "http://arxiv.org/abs/2502.09331v1", "repo": "null"}}