{"2412.14581": {"publish_time": "2024-12-19", "title": "CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation", "paper_summary": "With the adoption of retrieval-augmented generation (RAG), large language\nmodels (LLMs) are expected to ground their generation to the retrieved\ncontexts. Yet, this is hindered by position bias of LLMs, failing to evenly\nattend to all contexts. Previous work has addressed this by synthesizing\ncontexts with perturbed positions of gold segment, creating a\nposition-diversified train set. We extend this intuition to propose consistency\nregularization with augmentation and distillation. First, we augment each\ntraining instance with its position perturbation to encourage consistent\npredictions, regardless of ordering. We also distill behaviors of this pair,\nalthough it can be counterproductive in certain RAG scenarios where the given\norder from the retriever is crucial for generation quality. We thus propose\nCORD, balancing COnsistency and Rank Distillation. CORD adaptively samples\nnoise-controlled perturbations from an interpolation space, ensuring both\nconsistency and respect for the rank prior. Empirical results show this balance\nenables CORD to outperform consistently in diverse RAG benchmarks.", "paper_summary_zh": "\u96a8\u8457\u6aa2\u7d22\u589e\u5f37\u5f0f\u751f\u6210\uff08RAG\uff09\u7684\u63a1\u7528\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u9810\u8a08\u6703\u5c07\u5176\u751f\u6210\u57fa\u790e\u65bc\u6aa2\u7d22\u5230\u7684\u5167\u5bb9\u3002\u7136\u800c\uff0c\u9019\u6703\u53d7\u5230 LLM \u7684\u4f4d\u7f6e\u504f\u5dee\u6240\u963b\u7919\uff0c\u7121\u6cd5\u5e73\u5747\u95dc\u6ce8\u6240\u6709\u5167\u5bb9\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u900f\u904e\u5408\u6210\u5177\u6709\u64fe\u52d5\u9ec3\u91d1\u5340\u6bb5\u4f4d\u7f6e\u7684\u5167\u5bb9\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u9032\u800c\u5efa\u7acb\u4e00\u500b\u4f4d\u7f6e\u591a\u5143\u5316\u7684\u8a13\u7df4\u7d44\u3002\u6211\u5011\u5ef6\u4f38\u9019\u500b\u76f4\u89ba\uff0c\u63d0\u51fa\u5177\u5099\u589e\u5f37\u548c\u84b8\u993e\u7684\u4e00\u81f4\u6027\u898f\u7bc4\u5316\u3002\u9996\u5148\uff0c\u6211\u5011\u70ba\u6bcf\u500b\u8a13\u7df4\u5be6\u4f8b\u589e\u5f37\u5176\u4f4d\u7f6e\u64fe\u52d5\uff0c\u4ee5\u9f13\u52f5\u4e00\u81f4\u7684\u9810\u6e2c\uff0c\u7121\u8ad6\u9806\u5e8f\u70ba\u4f55\u3002\u6211\u5011\u4e5f\u6703\u84b8\u993e\u9019\u5c0d\u7684\u884c\u70ba\uff0c\u5118\u7ba1\u5728\u7d66\u5b9a\u7684\u6aa2\u7d22\u5668\u9806\u5e8f\u5c0d\u751f\u6210\u54c1\u8cea\u81f3\u95dc\u91cd\u8981\u7684\u7279\u5b9a RAG \u5834\u666f\u4e2d\uff0c\u9019\u53ef\u80fd\u6703\u9069\u5f97\u5176\u53cd\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa CORD\uff0c\u5e73\u8861\u4e00\u81f4\u6027\u548c\u79e9\u84b8\u993e\u3002CORD \u5f9e\u63d2\u503c\u7a7a\u9593\u81ea\u9069\u61c9\u5730\u53d6\u6a23\u53d7\u96dc\u8a0a\u63a7\u5236\u7684\u64fe\u52d5\uff0c\u78ba\u4fdd\u4e00\u81f4\u6027\u4e26\u5c0a\u91cd\u79e9\u5148\u9a57\u3002\u5be6\u8b49\u7d50\u679c\u986f\u793a\uff0c\u9019\u7a2e\u5e73\u8861\u4f7f CORD \u80fd\u5920\u5728\u4e0d\u540c\u7684 RAG \u57fa\u6e96\u4e2d\u6301\u7e8c\u8868\u73fe\u51fa\u8272\u3002", "author": "Youngwon Lee et.al.", "authors": "Youngwon Lee, Seung-won Hwang, Daniel Campos, Filip Grali\u0144ski, Zhewei Yao, Yuxiong He", "id": "2412.14581v1", "paper_url": "http://arxiv.org/abs/2412.14581v1", "repo": "null"}}