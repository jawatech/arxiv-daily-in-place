{"2404.10975": {"publish_time": "2024-04-17", "title": "Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans and Language Models", "paper_summary": "As AI systems like language models are increasingly integrated into\ndecision-making processes affecting people's lives, it's critical to ensure\nthat these systems have sound moral reasoning. To test whether they do, we need\nto develop systematic evaluations. We provide a framework that uses a language\nmodel to translate causal graphs that capture key aspects of moral dilemmas\ninto prompt templates. With this framework, we procedurally generated a large\nand diverse set of moral dilemmas -- the OffTheRails benchmark -- consisting of\n50 scenarios and 400 unique test items. We collected moral permissibility and\nintention judgments from human participants for a subset of our items and\ncompared these judgments to those from two language models (GPT-4 and Claude-2)\nacross eight conditions. We find that moral dilemmas in which the harm is a\nnecessary means (as compared to a side effect) resulted in lower permissibility\nand higher intention ratings for both participants and language models. The\nsame pattern was observed for evitable versus inevitable harmful outcomes.\nHowever, there was no clear effect of whether the harm resulted from an agent's\naction versus from having omitted to act. We discuss limitations of our prompt\ngeneration pipeline and opportunities for improving scenarios to increase the\nstrength of experimental effects.", "paper_summary_zh": "\u96a8\u8457\u50cf\u8a9e\u8a00\u6a21\u578b\u7684\u4eba\u5de5\u667a\u6167\u7cfb\u7d71\u65e5\u76ca\u6574\u5408\u5230\u5f71\u97ff\u4eba\u5011\u751f\u6d3b\u7684\u6c7a\u7b56\u5236\u5b9a\u904e\u7a0b\u4e2d\uff0c\u78ba\u4fdd\u9019\u4e9b\u7cfb\u7d71\u5177\u6709\u5065\u5168\u7684\u9053\u5fb7\u63a8\u7406\u81f3\u95dc\u91cd\u8981\u3002\u70ba\u4e86\u6e2c\u8a66\u5b83\u5011\u662f\u5426\u5177\u5099\u9019\u6a23\u7684\u80fd\u529b\uff0c\u6211\u5011\u9700\u8981\u5236\u5b9a\u7cfb\u7d71\u6027\u7684\u8a55\u4f30\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u6846\u67b6\uff0c\u8a72\u6846\u67b6\u4f7f\u7528\u8a9e\u8a00\u6a21\u578b\u4f86\u8f49\u63db\u56e0\u679c\u5716\uff0c\u9019\u4e9b\u56e0\u679c\u5716\u6355\u6349\u4e86\u9053\u5fb7\u56f0\u5883\u7684\u95dc\u9375\u65b9\u9762\uff0c\u4e26\u8f49\u63db\u6210\u63d0\u793a\u7bc4\u672c\u3002\u6709\u4e86\u9019\u500b\u6846\u67b6\uff0c\u6211\u5011\u7a0b\u5e8f\u5316\u5730\u751f\u6210\u4e86\u4e00\u7d44\u9f90\u5927\u4e14\u591a\u6a23\u5316\u7684\u9053\u5fb7\u56f0\u5883\u2014\u2014OffTheRails \u57fa\u6e96\u2014\u2014\u5305\u542b 50 \u500b\u5834\u666f\u548c 400 \u500b\u7368\u7279\u7684\u6e2c\u8a66\u9805\u76ee\u3002\u6211\u5011\u70ba\u6211\u5011\u9805\u76ee\u7684\u5b50\u96c6\u6536\u96c6\u4e86\u4eba\u985e\u53c3\u8207\u8005\u7684\u9053\u5fb7\u5bb9\u8a31\u6027\u548c\u610f\u5716\u5224\u65b7\uff0c\u4e26\u5c07\u9019\u4e9b\u5224\u65b7\u8207\u4f86\u81ea\u5169\u500b\u8a9e\u8a00\u6a21\u578b\uff08GPT-4 \u548c Claude-2\uff09\u5728\u516b\u500b\u689d\u4ef6\u4e0b\u7684\u5224\u65b7\u9032\u884c\u4e86\u6bd4\u8f03\u3002\u6211\u5011\u767c\u73fe\uff0c\u5728\u50b7\u5bb3\u662f\u5fc5\u8981\u624b\u6bb5\uff08\u8207\u526f\u4f5c\u7528\u76f8\u6bd4\uff09\u7684\u9053\u5fb7\u56f0\u5883\u4e2d\uff0c\u53c3\u8207\u8005\u548c\u8a9e\u8a00\u6a21\u578b\u7684\u5bb9\u8a31\u5ea6\u8f03\u4f4e\uff0c\u610f\u5716\u8a55\u5206\u8f03\u9ad8\u3002\u5c0d\u65bc\u53ef\u907f\u514d\u8207\u4e0d\u53ef\u907f\u514d\u7684\u6709\u5bb3\u7d50\u679c\uff0c\u4e5f\u89c0\u5bdf\u5230\u4e86\u76f8\u540c\u7684\u6a21\u5f0f\u3002\u7136\u800c\uff0c\u50b7\u5bb3\u662f\u6e90\u65bc\u4ee3\u7406\u4eba\u7684\u884c\u70ba\u9084\u662f\u6e90\u65bc\u758f\u5ffd\u884c\u70ba\uff0c\u4e26\u6c92\u6709\u660e\u986f\u7684\u5f71\u97ff\u3002\u6211\u5011\u8a0e\u8ad6\u4e86\u63d0\u793a\u751f\u6210\u7ba1\u7dda\u7684\u9650\u5236\uff0c\u4ee5\u53ca\u6539\u9032\u5834\u666f\u4ee5\u589e\u5f37\u5be6\u9a57\u6548\u679c\u5f37\u5ea6\u7684\u6a5f\u6703\u3002", "author": "Jan-Philipp Fr\u00e4nken et.al.", "authors": "Jan-Philipp Fr\u00e4nken, Kanishk Gandhi, Tori Qiu, Ayesha Khawaja, Noah D. Goodman, Tobias Gerstenberg", "id": "2404.10975v1", "paper_url": "http://arxiv.org/abs/2404.10975v1", "repo": "https://github.com/cicl-stanford/moral-evals"}}