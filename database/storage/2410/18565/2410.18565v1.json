{"2410.18565": {"publish_time": "2024-10-24", "title": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation", "paper_summary": "We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for\nPolish language processing. Trained on curated Polish corpora, this model\naddresses key challenges in language model development through innovative\ntechniques. These include Weighted Instruction Cross-Entropy Loss, which\nbalances the learning of different instruction types, and Adaptive Learning\nRate, which dynamically adjusts the learning rate based on training progress.\nTo evaluate performance, we created the Open PL LLM Leaderboard and Polish\nMT-Bench, novel frameworks assessing various NLP tasks and conversational\nabilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9\npercentage point increase in average score compared to Mistral-7B-v0.1 on the\nRAG Reader task. It also excels in the Polish MT-Bench, particularly in\nReasoning (6.15/10) and Role-playing (7.83/10) categories. This model\nrepresents a substantial advancement in Polish language AI, offering a powerful\ntool for diverse linguistic applications and setting new benchmarks in the\nfield.", "paper_summary_zh": "\u6211\u5011\u63a8\u51fa Bielik 7B v0.1\uff0c\u9019\u662f\u4e00\u500b\u91dd\u5c0d\u6ce2\u862d\u8a9e\u8655\u7406\u7684 70 \u5104\u53c3\u6578\u751f\u6210\u6587\u672c\u6a21\u578b\u3002\u6b64\u6a21\u578b\u91dd\u5c0d\u7d93\u904e\u7b56\u5c55\u7684\u6ce2\u862d\u8a9e\u8a9e\u6599\u5eab\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u900f\u904e\u5275\u65b0\u6280\u8853\u4f86\u89e3\u6c7a\u8a9e\u8a00\u6a21\u578b\u958b\u767c\u4e2d\u7684\u4e3b\u8981\u6311\u6230\u3002\u9019\u4e9b\u6280\u8853\u5305\u62ec\u52a0\u6b0a\u6307\u4ee4\u4ea4\u53c9\u71b5\u640d\u5931\uff0c\u5b83\u5e73\u8861\u4e86\u4e0d\u540c\u6307\u4ee4\u985e\u578b\u7684\u5b78\u7fd2\uff0c\u4ee5\u53ca\u81ea\u9069\u61c9\u5b78\u7fd2\u7387\uff0c\u5b83\u6703\u6839\u64da\u8a13\u7df4\u9032\u5ea6\u52d5\u614b\u8abf\u6574\u5b78\u7fd2\u7387\u3002\u70ba\u4e86\u8a55\u4f30\u6548\u80fd\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u958b\u653e\u5f0f PL LLM \u6392\u884c\u699c\u548c\u6ce2\u862d\u8a9e MT-Bench\uff0c\u9019\u4e9b\u662f\u8a55\u4f30\u5404\u7a2e NLP \u4efb\u52d9\u548c\u5c0d\u8a71\u80fd\u529b\u7684\u65b0\u7a4e\u67b6\u69cb\u3002\u8207 RAG Reader \u4efb\u52d9\u4e2d\u7684 Mistral-7B-v0.1 \u76f8\u6bd4\uff0cBielik 7B v0.1 \u6709\u986f\u8457\u7684\u9032\u6b65\uff0c\u5e73\u5747\u5206\u6578\u63d0\u9ad8\u4e86 9 \u500b\u767e\u5206\u9ede\u3002\u5b83\u4e5f\u5728\u6ce2\u862d\u8a9e MT-Bench \u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u7279\u5225\u662f\u5728\u63a8\u7406 (6.15/10) \u548c\u89d2\u8272\u626e\u6f14 (7.83/10) \u985e\u5225\u4e2d\u3002\u6b64\u6a21\u578b\u4ee3\u8868\u4e86\u6ce2\u862d\u8a9e AI \u7684\u91cd\u5927\u9032\u6b65\uff0c\u70ba\u5404\u7a2e\u8a9e\u8a00\u61c9\u7528\u7a0b\u5f0f\u63d0\u4f9b\u4e86\u5f37\u5927\u7684\u5de5\u5177\uff0c\u4e26\u5728\u8a72\u9818\u57df\u6a39\u7acb\u4e86\u65b0\u7684\u57fa\u6e96\u3002", "author": "Krzysztof Ociepa et.al.", "authors": "Krzysztof Ociepa, \u0141ukasz Flis, Krzysztof Wr\u00f3bel, Adrian Gwo\u017adziej, Remigiusz Kinas", "id": "2410.18565v1", "paper_url": "http://arxiv.org/abs/2410.18565v1", "repo": "null"}}