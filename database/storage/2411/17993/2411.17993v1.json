{"2411.17993": {"publish_time": "2024-11-27", "title": "DRS: Deep Question Reformulation With Structured Output", "paper_summary": "Question answering is a fundamental capability of large language models\n(LLMs). However, when people encounter completely new knowledge texts, they\noften ask questions that the text cannot answer due to a lack of understanding\nof the knowledge. Recent research shows that large language models identify the\nunanswerability of questions, but they lack the ability to help people\nreformulate their questions. Even powerful models like GPT-3.5 perform poorly\nin this regard. To enhance the ability of LLMs to assist humans in\nreformulating questions to extract relevant knowledge from new documents, we\npropose a zero-shot method called DRS: Deep Question Reformulation With\nStructured Output. Our proposed method leverages large language models and the\nDFS-based algorithm to iteratively search for possible entity combinations and\nconstrain the output with certain entities, effectively improving the\ncapabilities of large language models in this area. Extensive experimental\nresults show that our zero-shot DRS method significantly improves the\nreformulation accuracy of GPT-3.5 from 23.03% to 70.42% and effectively\nimproves the score of open-source large language models, such as Gemma2-9B,\nfrom 26.35% to 56.75%.", "paper_summary_zh": "\u554f\u7b54\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u57fa\u672c\u80fd\u529b\u3002\u7136\u800c\uff0c\u7576\u4eba\u5011\u9047\u5230\u5168\u65b0\u7684\u77e5\u8b58\u6587\u672c\u6642\uff0c\u4ed6\u5011\u7d93\u5e38\u6703\u63d0\u51fa\u6587\u672c\u7121\u6cd5\u56de\u7b54\u7684\u554f\u984c\uff0c\u56e0\u70ba\u4ed6\u5011\u5c0d\u77e5\u8b58\u7406\u89e3\u4e0d\u8db3\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u8fa8\u8b58\u554f\u984c\u7684\u4e0d\u53ef\u56de\u7b54\u6027\uff0c\u4f46\u5b83\u5011\u7f3a\u4e4f\u5e6b\u52a9\u4eba\u5011\u91cd\u65b0\u8868\u8ff0\u554f\u984c\u7684\u80fd\u529b\u3002\u5373\u4f7f\u662f\u50cf GPT-3.5 \u9019\u6a23\u5f37\u5927\u7684\u6a21\u578b\u5728\u9019\u65b9\u9762\u8868\u73fe\u4e5f\u4e0d\u4f73\u3002\u70ba\u4e86\u589e\u5f37 LLM \u5354\u52a9\u4eba\u985e\u91cd\u65b0\u8868\u8ff0\u554f\u984c\u4ee5\u5f9e\u65b0\u6587\u4ef6\u4e2d\u63d0\u53d6\u76f8\u95dc\u77e5\u8b58\u7684\u80fd\u529b\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7a31\u70ba DRS \u7684\u96f6\u6b21\u5b78\u7fd2\u65b9\u6cd5\uff1a\u5177\u6709\u7d50\u69cb\u5316\u8f38\u51fa\u7684\u6df1\u5ea6\u554f\u984c\u91cd\u65b0\u8868\u8ff0\u3002\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u548c\u57fa\u65bc DFS \u7684\u6f14\u7b97\u6cd5\uff0c\u53cd\u8986\u641c\u5c0b\u53ef\u80fd\u7684\u5be6\u9ad4\u7d44\u5408\uff0c\u4e26\u4ee5\u7279\u5b9a\u5be6\u9ad4\u7d04\u675f\u8f38\u51fa\uff0c\u6709\u6548\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u9019\u65b9\u9762\u7684\u80fd\u529b\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u96f6\u6b21\u5b78\u7fd2 DRS \u65b9\u6cd5\u5c07 GPT-3.5 \u7684\u91cd\u65b0\u8868\u8ff0\u6e96\u78ba\u5ea6\u5f9e 23.03% \u5927\u5e45\u63d0\u5347\u81f3 70.42%\uff0c\u4e26\u6709\u6548\u63d0\u5347\u958b\u6e90\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08\u4f8b\u5982 Gemma2-9B\uff09\u7684\u5206\u6578\uff0c\u5f9e 26.35% \u63d0\u5347\u81f3 56.75%\u3002", "author": "Zhecheng Li et.al.", "authors": "Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, Kai-Wei Chang", "id": "2411.17993v1", "paper_url": "http://arxiv.org/abs/2411.17993v1", "repo": "null"}}