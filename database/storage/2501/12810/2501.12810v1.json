{"2501.12810": {"publish_time": "2025-01-22", "title": "Machine Learning Modeling for Multi-order Human Visual Motion Processing", "paper_summary": "Our research aims to develop machines that learn to perceive visual motion as\ndo humans. While recent advances in computer vision (CV) have enabled DNN-based\nmodels to accurately estimate optical flow in naturalistic images, a\nsignificant disparity remains between CV models and the biological visual\nsystem in both architecture and behavior. This disparity includes humans'\nability to perceive the motion of higher-order image features (second-order\nmotion), which many CV models fail to capture because of their reliance on the\nintensity conservation law. Our model architecture mimics the cortical V1-MT\nmotion processing pathway, utilizing a trainable motion energy sensor bank and\na recurrent graph network. Supervised learning employing diverse naturalistic\nvideos allows the model to replicate psychophysical and physiological findings\nabout first-order (luminance-based) motion perception. For second-order motion,\ninspired by neuroscientific findings, the model includes an additional sensing\npathway with nonlinear preprocessing before motion energy sensing, implemented\nusing a simple multilayer 3D CNN block. When exploring how the brain acquired\nthe ability to perceive second-order motion in natural environments, in which\npure second-order signals are rare, we hypothesized that second-order\nmechanisms were critical when estimating robust object motion amidst optical\nfluctuations, such as highlights on glossy surfaces. We trained our\ndual-pathway model on novel motion datasets with varying material properties of\nmoving objects. We found that training to estimate object motion from\nnon-Lambertian materials naturally endowed the model with the capacity to\nperceive second-order motion, as can humans. The resulting model effectively\naligns with biological systems while generalizing to both first- and\nsecond-order motion phenomena in natural scenes.", "paper_summary_zh": "<paragraph>\u6211\u5011\u7684\u7814\u7a76\u65e8\u5728\u958b\u767c\u51fa\u80fd\u50cf\u4eba\u985e\u4e00\u6a23\u5b78\u7fd2\u611f\u77e5\u8996\u89ba\u904b\u52d5\u7684\u6a5f\u5668\u3002\u5118\u7ba1\u96fb\u8166\u8996\u89ba (CV) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u8b93\u57fa\u65bc\u6df1\u5ea6\u795e\u7d93\u7db2\u8def (DNN) \u7684\u6a21\u578b\u80fd\u6e96\u78ba\u4f30\u8a08\u81ea\u7136\u5f71\u50cf\u4e2d\u7684\u5149\u6d41\uff0c\u4f46 CV \u6a21\u578b\u8207\u751f\u7269\u8996\u89ba\u7cfb\u7d71\u5728\u67b6\u69cb\u548c\u884c\u70ba\u4e0a\u4ecd\u6709\u986f\u8457\u5dee\u7570\u3002\u9019\u7a2e\u5dee\u7570\u5305\u62ec\u4eba\u985e\u611f\u77e5\u9ad8\u968e\u5f71\u50cf\u7279\u5fb5\uff08\u4e8c\u968e\u904b\u52d5\uff09\u7684\u80fd\u529b\uff0c\u7531\u65bc\u4f9d\u8cf4\u5f37\u5ea6\u5b88\u6046\u5b9a\u5f8b\uff0c\u8a31\u591a CV \u6a21\u578b\u7121\u6cd5\u6355\u6349\u5230\u9019\u4e00\u9ede\u3002\u6211\u5011\u7684\u6a21\u578b\u67b6\u69cb\u6a21\u64ec\u4e86\u76ae\u8cea V1-MT \u904b\u52d5\u8655\u7406\u8def\u5f91\uff0c\u5229\u7528\u53ef\u8a13\u7df4\u7684\u904b\u52d5\u80fd\u91cf\u611f\u6e2c\u5668\u7d44\u548c\u905e\u8ff4\u5716\u5f62\u7db2\u8def\u3002\u63a1\u7528\u591a\u6a23\u5316\u81ea\u7136\u5f71\u7247\u7684\u76e3\u7763\u5f0f\u5b78\u7fd2\uff0c\u8b93\u6a21\u578b\u80fd\u5920\u8907\u88fd\u95dc\u65bc\u4e00\u968e\uff08\u57fa\u65bc\u4eae\u5ea6\uff09\u904b\u52d5\u611f\u77e5\u7684\u5fc3\u7406\u7269\u7406\u5b78\u548c\u751f\u7406\u5b78\u767c\u73fe\u3002\u5c0d\u65bc\u4e8c\u968e\u904b\u52d5\uff0c\u53d7\u795e\u7d93\u79d1\u5b78\u767c\u73fe\u7684\u555f\u767c\uff0c\u8a72\u6a21\u578b\u5305\u542b\u4e86\u4e00\u500b\u984d\u5916\u7684\u611f\u6e2c\u8def\u5f91\uff0c\u5728\u904b\u52d5\u80fd\u91cf\u611f\u6e2c\u4e4b\u524d\u9032\u884c\u975e\u7dda\u6027\u9810\u8655\u7406\uff0c\u4e26\u4f7f\u7528\u7c21\u55ae\u7684\u591a\u5c64 3D CNN \u584a\u9032\u884c\u5be6\u4f5c\u3002\u5728\u63a2\u8a0e\u5927\u8166\u5982\u4f55\u7372\u5f97\u5728\u81ea\u7136\u74b0\u5883\u4e2d\u611f\u77e5\u4e8c\u968e\u904b\u52d5\u7684\u80fd\u529b\u6642\uff0c\u5176\u4e2d\u7d14\u4e8c\u968e\u8a0a\u865f\u5f88\u5c11\u898b\uff0c\u6211\u5011\u5047\u8a2d\u5728\u4f30\u8a08\u5149\u5b78\u6ce2\u52d5\u4e2d\u7684\u7a69\u5065\u7269\u9ad4\u904b\u52d5\u6642\uff0c\u4e8c\u968e\u6a5f\u5236\u81f3\u95dc\u91cd\u8981\uff0c\u4f8b\u5982\u5149\u6ed1\u8868\u9762\u4e0a\u7684\u4eae\u9ede\u3002\u6211\u5011\u5728\u5177\u6709\u4e0d\u540c\u79fb\u52d5\u7269\u9ad4\u6750\u6599\u5c6c\u6027\u7684\u65b0\u904b\u52d5\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u4e86\u6211\u5011\u7684\u96d9\u8def\u5f91\u6a21\u578b\u3002\u6211\u5011\u767c\u73fe\uff0c\u5f9e\u975e\u6717\u4f2f\u9ad4\u6750\u6599\u4f30\u8a08\u7269\u9ad4\u904b\u52d5\u7684\u8a13\u7df4\u81ea\u7136\u800c\u7136\u5730\u8ce6\u4e88\u4e86\u6a21\u578b\u611f\u77e5\u4e8c\u968e\u904b\u52d5\u7684\u80fd\u529b\uff0c\u5c31\u50cf\u4eba\u985e\u4e00\u6a23\u3002\u7531\u6b64\u7522\u751f\u7684\u6a21\u578b\u6709\u6548\u5730\u8207\u751f\u7269\u7cfb\u7d71\u4fdd\u6301\u4e00\u81f4\uff0c\u540c\u6642\u63a8\u5ee3\u5230\u81ea\u7136\u5834\u666f\u4e2d\u7684\u4e00\u968e\u548c\u4e8c\u968e\u904b\u52d5\u73fe\u8c61\u3002</paragraph>", "author": "Zitang Sun et.al.", "authors": "Zitang Sun, Yen-Ju Chen, Yung-Hao Yang, Yuan Li, Shin'ya Nishida", "id": "2501.12810v1", "paper_url": "http://arxiv.org/abs/2501.12810v1", "repo": "https://github.com/anoymized/multi-order-motion-model"}}