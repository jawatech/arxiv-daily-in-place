{"2501.11223": {"publish_time": "2025-01-20", "title": "Reasoning Language Models: A Blueprint", "paper_summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending large language models\n(LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary\nnature, and complex architectures - uniquely combining Reinforcement Learning\n(RL), search heuristics, and LLMs - present accessibility and scalability\nchallenges. To address these, we propose a comprehensive blueprint that\norganizes RLM components into a modular framework, based on a survey and\nanalysis of all RLM works. This blueprint incorporates diverse reasoning\nstructures (chains, trees, graphs, and nested forms), reasoning strategies\n(e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models\nand others), and supervision schemes (Output-Based and Process-Based\nSupervision). We also provide detailed mathematical formulations and\nalgorithmic specifications to simplify RLM implementation. By showing how\nschemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as\nspecial cases, we demonstrate the blueprint's versatility and unifying\npotential. To illustrate its utility, we introduce x1, a modular implementation\nfor rapid RLM prototyping and experimentation. Using x1 and a literature\nreview, we provide key insights, such as multi-phase training for policy and\nvalue models, and the importance of familiar training distributions. Finally,\nwe outline how RLMs can integrate with a broader LLM ecosystem, including tools\nand databases. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\nexperimentation.", "paper_summary_zh": "\u63a8\u7406\u8a9e\u8a00\u6a21\u578b (RLM)\uff0c\u4e5f\u7a31\u70ba\u5927\u578b\u63a8\u7406\u6a21\u578b (LRM)\uff0c\u4f8b\u5982 OpenAI \u7684 o1 \u548c o3\u3001DeepSeek-V3 \u548c\u963f\u91cc\u5df4\u5df4\u7684 QwQ\uff0c\u900f\u904e\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u5148\u9032\u7684\u63a8\u7406\u6a5f\u5236\u76f8\u7d50\u5408\uff0c\u91cd\u65b0\u5b9a\u7fa9\u4e86 AI \u7684\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u3002\u7136\u800c\uff0c\u5176\u9ad8\u6602\u7684\u6210\u672c\u3001\u5c08\u6709\u6027\u8cea\u548c\u8907\u96dc\u7684\u67b6\u69cb\uff08\u7368\u7279\u5730\u7d50\u5408\u4e86\u5f37\u5316\u5b78\u7fd2 (RL)\u3001\u641c\u5c0b\u555f\u767c\u6cd5\u548c LLM\uff09\u63d0\u51fa\u4e86\u53ef\u53ca\u6027\u548c\u53ef\u64f4\u5145\u6027\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5168\u9762\u7684\u85cd\u5716\uff0c\u6839\u64da\u5c0d\u6240\u6709 RLM \u4f5c\u54c1\u7684\u8abf\u67e5\u548c\u5206\u6790\uff0c\u5c07 RLM \u7d44\u4ef6\u7d44\u7e54\u6210\u4e00\u500b\u6a21\u7d44\u5316\u6846\u67b6\u3002\u6b64\u85cd\u5716\u5305\u542b\u591a\u6a23\u5316\u7684\u63a8\u7406\u7d50\u69cb\uff08\u93c8\u3001\u6a39\u3001\u5716\u548c\u5de2\u72c0\u5f62\u5f0f\uff09\u3001\u63a8\u7406\u7b56\u7565\uff08\u4f8b\u5982\u8499\u5730\u5361\u7f85\u6a39\u641c\u5c0b\u3001\u6ce2\u675f\u641c\u5c0b\uff09\u3001RL \u6982\u5ff5\uff08\u7b56\u7565\u3001\u50f9\u503c\u6a21\u578b\u7b49\uff09\u548c\u76e3\u7763\u65b9\u6848\uff08\u57fa\u65bc\u8f38\u51fa\u7684\u76e3\u7763\u548c\u57fa\u65bc\u6d41\u7a0b\u7684\u76e3\u7763\uff09\u3002\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u8a73\u7d30\u7684\u6578\u5b78\u516c\u5f0f\u548c\u6f14\u7b97\u6cd5\u898f\u7bc4\uff0c\u4ee5\u7c21\u5316 RLM \u5be6\u4f5c\u3002\u900f\u904e\u5c55\u793a LLaMA-Berry\u3001QwQ\u3001Journey Learning \u548c Graph of Thoughts \u7b49\u65b9\u6848\u5982\u4f55\u4f5c\u70ba\u7279\u6b8a\u60c5\u6cc1\uff0c\u6211\u5011\u5c55\u793a\u4e86\u85cd\u5716\u7684\u591a\u529f\u80fd\u6027\u548c\u7d71\u4e00\u7684\u6f5b\u529b\u3002\u70ba\u4e86\u8aaa\u660e\u5176\u6548\u7528\uff0c\u6211\u5011\u5f15\u5165\u4e86 x1\uff0c\u4e00\u500b\u7528\u65bc\u5feb\u901f RLM \u539f\u578b\u88fd\u4f5c\u548c\u5be6\u9a57\u7684\u6a21\u7d44\u5316\u5be6\u4f5c\u3002\u4f7f\u7528 x1 \u548c\u6587\u737b\u56de\u9867\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u95dc\u9375\u898b\u89e3\uff0c\u4f8b\u5982\u653f\u7b56\u548c\u50f9\u503c\u6a21\u578b\u7684\u591a\u968e\u6bb5\u8a13\u7df4\uff0c\u4ee5\u53ca\u719f\u6089\u8a13\u7df4\u5206\u4f48\u7684\u91cd\u8981\u6027\u3002\u6700\u5f8c\uff0c\u6211\u5011\u6982\u8ff0\u4e86 RLM \u5982\u4f55\u8207\u66f4\u5ee3\u6cdb\u7684 LLM \u751f\u614b\u7cfb\u7d71\u6574\u5408\uff0c\u5305\u62ec\u5de5\u5177\u548c\u8cc7\u6599\u5eab\u3002\u6211\u5011\u7684\u7814\u7a76\u63ed\u958b\u4e86 RLM \u5efa\u69cb\u7684\u795e\u79d8\u9762\u7d17\uff0c\u6c11\u4e3b\u5316\u4e86\u5148\u9032\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e26\u4fc3\u9032\u4e86\u5275\u65b0\uff0c\u65e8\u5728\u900f\u904e\u964d\u4f4e RLM \u958b\u767c\u548c\u5be6\u9a57\u7684\u969c\u7919\uff0c\u4f86\u7e2e\u5c0f\u300c\u5bcc\u88d5 AI\u300d\u548c\u300c\u8ca7\u7aae AI\u300d\u4e4b\u9593\u7684\u5dee\u8ddd\u3002", "author": "Maciej Besta et.al.", "authors": "Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwa\u015bniewski, J\u00fcrgen M\u00fcller, \u0141ukasz Flis, Hannes Eberhard, Hubert Niewiadomski, Torsten Hoefler", "id": "2501.11223v1", "paper_url": "http://arxiv.org/abs/2501.11223v1", "repo": "https://github.com/spcl/x1"}}