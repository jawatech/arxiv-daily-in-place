{"2502.02407": {"publish_time": "2025-02-04", "title": "Avoiding spurious sharpness minimization broadens applicability of SAM", "paper_summary": "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs).", "paper_summary_zh": "\u66f2\u7387\u6b63\u5219\u5316\u6280\u672f\uff0c\u4f8b\u5982 Sharpness Aware Minimization (SAM)\uff0c\u5728\u63d0\u9ad8\u89c6\u89c9\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u663e\u793a\u51fa\u6781\u5927\u7684\u524d\u666f\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0 SAM \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u7b49\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u5e38\u4f1a\u964d\u4f4e\u6027\u80fd\u2014\u2014\u5373\u4f7f\u8ba1\u7b97\u9884\u7b97\u589e\u52a0\u4e86\u4e00\u500d\u3002\u6211\u4eec\u8c03\u67e5\u4e86\u4e0d\u540c\u9886\u57df\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u53d1\u73b0 SAM \u5728 NLP \u73af\u5883\u4e2d\u4e3b\u8981\u53d7 logit \u7edf\u8ba1\u91cf\u7684\u6b63\u5219\u5316\u5f71\u54cd\u2014\u2014\u800c\u4e0d\u662f\u6539\u5584\u51fd\u6570\u672c\u8eab\u7684\u51e0\u4f55\u5f62\u72b6\u3002\u6211\u4eec\u5229\u7528\u8fd9\u4e00\u89c2\u5bdf\u7ed3\u679c\u5f00\u53d1\u4e86\u4e00\u79cd\u6211\u4eec\u79f0\u4e4b\u4e3a Functional-SAM \u7684\u66ff\u4ee3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4ec5\u901a\u8fc7\u4fee\u6539\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u7684\u6574\u4f53\u51fd\u6570\u7684\u7edf\u8ba1\u91cf\u6765\u5bf9\u66f2\u7387\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u5e76\u901a\u8fc7 logit \u64cd\u4f5c\u907f\u514d\u865a\u5047\u6700\u5c0f\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8ba4\u4e3a\u5bf9 SAM \u5fae\u6270\u8fdb\u884c\u9884\u5904\u7406\u4e5f\u53ef\u4ee5\u9632\u6b62\u865a\u5047\u6700\u5c0f\u5316\uff0c\u5e76\u4e14\u5f53\u4e0e Functional-SAM \u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u5b83\u4f1a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002\u6211\u4eec\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u56fa\u5b9a\u957f\u5ea6\u548c Chinchilla \u98ce\u683c\u7684\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\uff0c\u5728\u7ecf\u8fc7\u76f8\u540c\u6b65\u6570\u7684\u8bad\u7ec3\u540e\uff0c\u5728\u5404\u79cd\u6a21\u578b\u89c4\u6a21\uff08\u5305\u62ec\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\uff09\u4e0a\u663e\u793a\u51fa\u4f18\u4e8e AdamW \u548c SAM \u57fa\u7ebf\u7684\u6027\u80fd\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u7a81\u51fa\u4e86\u66f4\u7cbe\u786e\u8868\u5f81\u66f2\u7387\u5728\u5c06\u66f2\u7387\u6b63\u5219\u5316\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u91cd\u8981\u6027\u3002", "author": "Sidak Pal Singh et.al.", "authors": "Sidak Pal Singh, Hossein Mobahi, Atish Agarwala, Yann Dauphin", "id": "2502.02407v1", "paper_url": "http://arxiv.org/abs/2502.02407v1", "repo": "null"}}