{"2411.10129": {"publish_time": "2024-11-15", "title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation", "paper_summary": "Generating accurate code review comments remains a significant challenge due\nto the inherently diverse and non-unique nature of the task output. Large\nlanguage models pretrained on both programming and natural language data tend\nto perform well in code-oriented tasks. However, large-scale pretraining is not\nalways feasible due to its environmental impact and project-specific\ngeneralizability issues. In this work, first we fine-tune open-source Large\nlanguage models (LLM) in parameter-efficient, quantized low-rank (QLoRA)\nfashion on consumer-grade hardware to improve review comment generation. Recent\nstudies demonstrate the efficacy of augmenting semantic metadata information\ninto prompts to boost performance in other code-related tasks. To explore this\nin code review activities, we also prompt proprietary, closed-source LLMs\naugmenting the input code patch with function call graphs and code summaries.\nBoth of our strategies improve the review comment generation performance, with\nfunction call graph augmented few-shot prompting on the GPT-3.5 model\nsurpassing the pretrained baseline by around 90% BLEU-4 score on the\nCodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA\nfine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging\nfrom 25% to 83% performance improvement) on this task. An additional human\nevaluation study further validates our experimental findings, reflecting\nreal-world developers' perceptions of LLM-generated code review comments based\non relevant qualitative metrics.", "paper_summary_zh": "<paragraph>\u7522\u751f\u6e96\u78ba\u7684\u7a0b\u5f0f\u78bc\u5be9\u67e5\u8a55\u8ad6\u4ecd\u7136\u662f\u4e00\u500b\u91cd\u5927\u6311\u6230\uff0c\u56e0\u70ba\u4efb\u52d9\u8f38\u51fa\u7684\u672c\u8cea\u4e0a\u662f\u591a\u6a23\u4e14\u975e\u7368\u7279\u7684\u3002\u5728\u7a0b\u5f0f\u8a2d\u8a08\u548c\u81ea\u7136\u8a9e\u8a00\u8cc7\u6599\u4e0a\u9032\u884c\u9810\u8a13\u7df4\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5f80\u5f80\u5728\u4ee5\u7a0b\u5f0f\u78bc\u70ba\u5c0e\u5411\u7684\u4efb\u52d9\u4e2d\u8868\u73fe\u826f\u597d\u3002\u7136\u800c\uff0c\u7531\u65bc\u5176\u5c0d\u74b0\u5883\u7684\u5f71\u97ff\u548c\u5c08\u6848\u7279\u5b9a\u7684\u4e00\u822c\u5316\u554f\u984c\uff0c\u5927\u898f\u6a21\u9810\u8a13\u7df4\u4e26\u975e\u7e3d\u662f\u53ef\u884c\u7684\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u5728\u53c3\u6578\u6709\u6548\u3001\u91cf\u5316\u7684\u4f4e\u79e9 (QLoRA) \u65b9\u5f0f\u4e2d\u5fae\u8abf\u958b\u6e90\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u5728\u6d88\u8cbb\u7d1a\u786c\u9ad4\u4e0a\u6539\u5584\u5be9\u67e5\u8a55\u8ad6\u7684\u7522\u751f\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8b49\u660e\u4e86\u5728\u63d0\u793a\u4e2d\u589e\u52a0\u8a9e\u7fa9\u5143\u8cc7\u6599\u8cc7\u8a0a\u4ee5\u63d0\u5347\u5176\u4ed6\u8207\u7a0b\u5f0f\u78bc\u76f8\u95dc\u4efb\u52d9\u4e2d\u6548\u80fd\u7684\u529f\u6548\u3002\u70ba\u4e86\u5728\u7a0b\u5f0f\u78bc\u5be9\u67e5\u6d3b\u52d5\u4e2d\u63a2\u7d22\u9019\u4e00\u9ede\uff0c\u6211\u5011\u4e5f\u63d0\u793a\u5c08\u6709\u7684\u3001\u9589\u6e90 LLM\uff0c\u4f7f\u7528\u51fd\u6578\u547c\u53eb\u5716\u548c\u7a0b\u5f0f\u78bc\u6458\u8981\u4f86\u589e\u52a0\u8f38\u5165\u7a0b\u5f0f\u78bc\u4fee\u88dc\u7a0b\u5f0f\u3002\u6211\u5011\u7684\u5169\u7a2e\u7b56\u7565\u90fd\u6539\u5584\u4e86\u5be9\u67e5\u8a55\u8ad6\u7522\u751f\u7684\u6548\u80fd\uff0c\u5728 GPT-3.5 \u6a21\u578b\u4e0a\u4f7f\u7528\u51fd\u6578\u547c\u53eb\u5716\u589e\u52a0\u7684\u5c11\u91cf\u63d0\u793a\uff0c\u5728 CodeReviewer \u8cc7\u6599\u96c6\u4e0a\u8d85\u8d8a\u4e86\u9810\u8a13\u7df4\u57fa\u6e96\uff0cBLEU-4 \u5206\u6578\u63d0\u9ad8\u4e86\u7d04 90%\u3002\u6b64\u5916\uff0c\u5c11\u91cf\u63d0\u793a\u7684 Gemini-1.0 Pro\u3001QLoRA \u5fae\u8abf\u7684 Code Llama \u548c Llama 3.1 \u6a21\u578b\u5728\u6b64\u4efb\u52d9\u4e0a\u9054\u5230\u4e86\u6709\u7af6\u722d\u529b\u7684\u7d50\u679c\uff08\u6548\u80fd\u63d0\u5347\u7bc4\u570d\u70ba 25% \u81f3 83%\uff09\u3002\u984d\u5916\u7684\u4f7f\u7528\u8005\u8a55\u4f30\u7814\u7a76\u9032\u4e00\u6b65\u9a57\u8b49\u4e86\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\uff0c\u53cd\u6620\u4e86\u5be6\u969b\u958b\u767c\u4eba\u54e1\u5c0d LLM \u7522\u751f\u7684\u7a0b\u5f0f\u78bc\u5be9\u67e5\u8a55\u8ad6\u7684\u770b\u6cd5\uff0c\u9019\u4e9b\u770b\u6cd5\u57fa\u65bc\u76f8\u95dc\u7684\u5b9a\u6027\u6307\u6a19\u3002</paragraph>", "author": "Md. Asif Haider et.al.", "authors": "Md. Asif Haider, Ayesha Binte Mostofa, Sk. Sabit Bin Mosaddek, Anindya Iqbal, Toufique Ahmed", "id": "2411.10129v1", "paper_url": "http://arxiv.org/abs/2411.10129v1", "repo": "null"}}