{"2410.06735": {"publish_time": "2024-10-09", "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "paper_summary": "Recent large language models (LLMs) have demonstrated remarkable\ngeneralization abilities in mathematics and logical reasoning tasks. Prior\nresearch indicates that LLMs pre-trained with programming language data exhibit\nhigh mathematical and reasoning abilities; however, this causal relationship\nhas not been rigorously tested. Our research aims to verify which programming\nlanguages and features during pre-training affect logical inference\nperformance. Specifically, we pre-trained decoder-based language models from\nscratch using datasets from ten programming languages (e.g., Python, C, Java)\nand three natural language datasets (Wikipedia, Fineweb, C4) under identical\nconditions. Thereafter, we evaluated the trained models in a few-shot\nin-context learning setting on logical reasoning tasks: FLD and bAbi, which do\nnot require commonsense or world knowledge. The results demonstrate that nearly\nall models trained with programming languages consistently outperform those\ntrained with natural languages, indicating that programming languages contain\nfactors that elicit logic inference performance. In addition, we found that\nmodels trained with programming languages exhibit a better ability to follow\ninstructions compared to those trained with natural languages. Further analysis\nreveals that the depth of Abstract Syntax Trees representing parsed results of\nprograms also affects logical reasoning performance. These findings will offer\ninsights into the essential elements of pre-training for acquiring the\nfoundational abilities of LLMs.", "paper_summary_zh": "\u8fd1\u671f\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u793a\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u7f16\u7a0b\u8bed\u8a00\u6570\u636e\u9884\u5148\u8bad\u7ec3\u7684 LLM \u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u6570\u5b66\u548c\u63a8\u7406\u80fd\u529b\uff1b\u7136\u800c\uff0c\u8fd9\u79cd\u56e0\u679c\u5173\u7cfb\u5c1a\u672a\u7ecf\u8fc7\u4e25\u683c\u7684\u6d4b\u8bd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u9884\u8bad\u7ec3\u671f\u95f4\u54ea\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u529f\u80fd\u4f1a\u5f71\u54cd\u903b\u8f91\u63a8\u7406\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u4ece\u5934\u5f00\u59cb\u4f7f\u7528\u6765\u81ea\u5341\u79cd\u7f16\u7a0b\u8bed\u8a00\uff08\u4f8b\u5982 Python\u3001C\u3001Java\uff09\u548c\u4e09\u79cd\u81ea\u7136\u8bed\u8a00\u6570\u636e\u96c6\uff08\u7ef4\u57fa\u767e\u79d1\u3001Fineweb\u3001C4\uff09\u7684\u6570\u636e\u96c6\u9884\u5148\u8bad\u7ec3\u4e86\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u6761\u4ef6\u76f8\u540c\u3002\u6b64\u540e\uff0c\u6211\u4eec\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\uff08FLD \u548c bAbi\uff09\u7684\u5c11\u91cf\u955c\u5934\u60c5\u5883\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u4e86\u8bad\u7ec3\u540e\u7684\u6a21\u578b\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4e0d\u9700\u8981\u5e38\u8bc6\u6216\u4e16\u754c\u77e5\u8bc6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u51e0\u4e4e\u6240\u6709\u4f7f\u7528\u7f16\u7a0b\u8bed\u8a00\u8bad\u7ec3\u7684\u6a21\u578b\u90fd\u6bd4\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u5f97\u66f4\u597d\uff0c\u8fd9\u8868\u660e\u7f16\u7a0b\u8bed\u8a00\u5305\u542b\u5f15\u53d1\u903b\u8f91\u63a8\u7406\u6027\u80fd\u7684\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u4f7f\u7528\u7f16\u7a0b\u8bed\u8a00\u8bad\u7ec3\u7684\u6a21\u578b\u4e0e\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\u3002\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u8868\u660e\uff0c\u8868\u793a\u7a0b\u5e8f\u89e3\u6790\u7ed3\u679c\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\u7684\u6df1\u5ea6\u4e5f\u4f1a\u5f71\u54cd\u903b\u8f91\u63a8\u7406\u6027\u80fd\u3002\u8fd9\u4e9b\u53d1\u73b0\u5c06\u4e3a\u83b7\u53d6 LLM \u7684\u57fa\u7840\u80fd\u529b\u7684\u9884\u8bad\u7ec3\u7684\u57fa\u672c\u8981\u7d20\u63d0\u4f9b\u89c1\u89e3\u3002", "author": "Fumiya Uchiyama et.al.", "authors": "Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo", "id": "2410.06735v1", "paper_url": "http://arxiv.org/abs/2410.06735v1", "repo": "null"}}