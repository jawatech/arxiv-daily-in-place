{"2411.01952": {"publish_time": "2024-11-04", "title": "Evaluating the quality of published medical research with ChatGPT", "paper_summary": "Evaluating the quality of published research is time-consuming but important\nfor departmental evaluations, appointments, and promotions. Previous research\nhas shown that ChatGPT can score articles for research quality, with the\nresults correlating positively with an indicator of quality in all fields\nexcept Clinical Medicine. This article investigates this anomaly with the\nlargest dataset yet and a more detailed analysis. The results showed that\nChatGPT 4o-mini scores for articles submitted to the UK's Research Excellence\nFramework (REF) 2021 Unit of Assessment (UoA) 1 Clinical Medicine correlated\npositively (r=0.134, n=9872) with departmental mean REF scores, against a\ntheoretical maximum correlation of r=0.226 (due to the departmental averaging\ninvolved). At the departmental level, mean ChatGPT scores correlated more\nstrongly with departmental mean REF scores (r=0.395, n=31). For the 100\njournals with the most articles in UoA 1, their mean ChatGPT score correlated\nstrongly with their REF score (r=0.495) but negatively with their citation rate\n(r=-0.148). Journal and departmental anomalies in these results point to\nChatGPT being ineffective at assessing the quality of research in prestigious\nmedical journals or research directly affecting human health, or both.\nNevertheless, the results give evidence of ChatGPT's ability to assess research\nquality overall for Clinical Medicine, so now there is evidence of its ability\nin all academic fields.", "paper_summary_zh": "<paragraph>\u8a55\u4f30\u5df2\u767c\u8868\u7684\u54c1\u8cea\u7814\u7a76\u5f88\u8017\u6642\uff0c\u4f46\u5c0d\u65bc\u90e8\u9580\u8a55\u9451\u3001\u4efb\u547d\u548c\u6649\u5347\u4f86\u8aaa\u5f88\u91cd\u8981\u3002\u5148\u524d\u7684\u7814\u7a76\u986f\u793a\uff0cChatGPT \u53ef\u4ee5\u70ba\u7814\u7a76\u54c1\u8cea\u8a55\u5206\uff0c\u5176\u7d50\u679c\u8207\u6240\u6709\u9818\u57df\uff08\u81e8\u5e8a\u91ab\u5b78\u9664\u5916\uff09\u7684\u54c1\u8cea\u6307\u6a19\u5448\u6b63\u76f8\u95dc\u3002\u672c\u6587\u4f7f\u7528\u8fc4\u4eca\u70ba\u6b62\u6700\u5927\u7684\u8cc7\u6599\u96c6\u548c\u66f4\u8a73\u7d30\u7684\u5206\u6790\u4f86\u63a2\u8a0e\u9019\u7a2e\u7570\u5e38\u73fe\u8c61\u3002\u7d50\u679c\u986f\u793a\uff0c\u63d0\u4ea4\u7d66\u82f1\u570b\u7814\u7a76\u5353\u8d8a\u67b6\u69cb (REF) 2021 \u8a55\u4f30\u55ae\u4f4d (UoA) 1 \u81e8\u5e8a\u91ab\u5b78\u7684 ChatGPT 4o-mini \u5206\u6578\u8207\u90e8\u9580\u5e73\u5747 REF \u5206\u6578\u5448\u6b63\u76f8\u95dc\uff08r=0.134\uff0cn=9872\uff09\uff0c\u800c\u7406\u8ad6\u6700\u5927\u76f8\u95dc\u4fc2\u6578\u70ba r=0.226\uff08\u7531\u65bc\u6d89\u53ca\u90e8\u9580\u5e73\u5747\uff09\u3002\u5728\u90e8\u9580\u5c64\u7d1a\uff0c\u5e73\u5747 ChatGPT \u5206\u6578\u8207\u90e8\u9580\u5e73\u5747 REF \u5206\u6578\u76f8\u95dc\u6027\u66f4\u5f37\uff08r=0.395\uff0cn=31\uff09\u3002\u5c0d\u65bc UoA 1 \u4e2d\u6587\u7ae0\u6700\u591a\u7684 100 \u672c\u671f\u520a\uff0c\u5176\u5e73\u5747 ChatGPT \u5206\u6578\u8207\u5176 REF \u5206\u6578\u5448\u5f37\u6b63\u76f8\u95dc\uff08r=0.495\uff09\uff0c\u4f46\u8207\u5176\u5f15\u7528\u7387\u5448\u8ca0\u76f8\u95dc\uff08r=-0.148\uff09\u3002\u9019\u4e9b\u7d50\u679c\u4e2d\u7684\u671f\u520a\u548c\u90e8\u9580\u7570\u5e38\u73fe\u8c61\u8868\u660e\uff0cChatGPT \u7121\u6cd5\u8a55\u4f30\u8072\u671b\u5353\u8457\u7684\u91ab\u5b78\u671f\u520a\u6216\u76f4\u63a5\u5f71\u97ff\u4eba\u985e\u5065\u5eb7\u7684\u7814\u7a76\uff08\u6216\u5169\u8005\uff09\u7684\u54c1\u8cea\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u7d50\u679c\u8b49\u660e\u4e86 ChatGPT \u6574\u9ad4\u8a55\u4f30\u81e8\u5e8a\u91ab\u5b78\u7814\u7a76\u54c1\u8cea\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u73fe\u5728\u6709\u8b49\u64da\u8b49\u660e\u5176\u5728\u6240\u6709\u5b78\u8853\u9818\u57df\u7684\u80fd\u529b\u3002</paragraph>", "author": "Mike Thelwall et.al.", "authors": "Mike Thelwall, Xiaorui Jiang, Peter A. Bath", "id": "2411.01952v1", "paper_url": "http://arxiv.org/abs/2411.01952v1", "repo": "null"}}