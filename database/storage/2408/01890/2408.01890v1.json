{"2408.01890": {"publish_time": "2024-08-04", "title": "Cross-layer Attention Sharing for Large Language Models", "paper_summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.", "paper_summary_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6f14\u8fdb\uff0c\u6a21\u578b\u6df1\u5ea6\u548c\u53c2\u6570\u6570\u91cf\u7684\u589e\u52a0\u5bfc\u81f4\u4e86\u5927\u91cf\u7684\u5197\u4f59\u3002\u4e3a\u4e86\u63d0\u9ad8\u6ce8\u610f\u529b\u673a\u5236\u7684\u6548\u7387\uff0c\u4ee5\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u538b\u7f29 KV \u7f13\u5b58\u6216\u5206\u7ec4\u6ce8\u610f\u529b\u5934\uff0c\u800c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u7565\u4e86\u5c42\u4e4b\u95f4\u7684\u5197\u4f59\u3002\u6211\u4eec\u5bf9\u5404\u79cd LLM \u7684\u7efc\u5408\u5206\u6790\u8868\u660e\uff0c\u5728\u5927\u591a\u6570\u5c42\u4e2d\uff0c\u6ce8\u610f\u529b\u6a21\u5f0f\u9ad8\u5ea6\u76f8\u4f3c\u3002\u901a\u8fc7\u8de8\u5c42\u5171\u4eab\u6ce8\u610f\u529b\u6743\u91cd\u6765\u8282\u7701\u8ba1\u7b97\u662f\u76f4\u89c2\u7684\u3002\u7136\u800c\uff0c\u8fdb\u4e00\u6b65\u7684\u5206\u6790\u63ed\u793a\u4e86\u4e24\u4e2a\u6311\u6218\uff1a (1) \u76f4\u63a5\u5171\u4eab\u6743\u91cd\u77e9\u9635\u800c\u4e0d\u4ed4\u7ec6\u91cd\u65b0\u6392\u5217\u6ce8\u610f\u529b\u5934\u88ab\u8bc1\u660e\u662f\u65e0\u6548\u7684\uff1b(2) \u6d45\u5c42\u5bb9\u6613\u53d7\u5230\u6ce8\u610f\u529b\u6743\u91cd\u7684\u5c0f\u504f\u5dee\u7684\u5f71\u54cd\u3002\u53d7\u8fd9\u4e9b\u89c1\u89e3\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86 LiSA\uff0c\u4e00\u79cd\u7ecf\u8fc7\u826f\u597d\u8bad\u7ec3\u7684 LLM \u4e2d\u81ea\u6ce8\u610f\u529b\u7684\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u54c1\u3002LiSA \u91c7\u7528\u5fae\u5c0f\u7684\u524d\u9988\u7f51\u7edc\u6765\u5bf9\u9f50\u76f8\u90bb\u5c42\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u91c7\u7528\u4f4e\u79e9\u77e9\u9635\u6765\u903c\u8fd1\u5c42\u7ea7\u6ce8\u610f\u529b\u6743\u91cd\u4e2d\u7684\u5dee\u5f02\u3002\u6db5\u76d6 13 \u4e2a\u5178\u578b\u57fa\u51c6\u7684\u8bc4\u4f30\u8868\u660e\uff0cLiSA \u5728\u51c6\u786e\u6027\u548c\u56f0\u60d1\u5ea6\u65b9\u9762\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u54cd\u5e94\u8d28\u91cf\uff0c\u540c\u65f6\u5c06 53-84% \u7684\u603b\u5c42\u4e2d\u7684\u5197\u4f59\u6ce8\u610f\u529b\u8ba1\u7b97\u51cf\u5c11\u4e86\u3002\u6211\u4eec\u5bf9 LiSA \u7684\u5b9e\u73b0\u5b9e\u73b0\u4e86 Q \u548c K \u7684 6 \u500d\u538b\u7f29\uff0cLLaMA3-8B \u7684\u6700\u5927\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 19.5%\uff0cLLaMA2-7B \u7684\u6700\u5927\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 32.3%\u3002", "author": "Yongyu Mu et.al.", "authors": "Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Qiaozhi He, Murun Yang, Tong Xiao, Jingbo Zhu", "id": "2408.01890v1", "paper_url": "http://arxiv.org/abs/2408.01890v1", "repo": "null"}}