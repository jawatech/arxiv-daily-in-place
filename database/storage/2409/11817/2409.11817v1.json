{"2409.11817": {"publish_time": "2024-09-18", "title": "EFCM: Efficient Fine-tuning on Compressed Models for deployment of large models in medical image analysis", "paper_summary": "The recent development of deep learning large models in medicine shows\nremarkable performance in medical image analysis and diagnosis, but their large\nnumber of parameters causes memory and inference latency challenges. Knowledge\ndistillation offers a solution, but the slide-level gradients cannot be\nbackpropagated for student model updates due to high-resolution pathological\nimages and slide-level labels. This study presents an Efficient Fine-tuning on\nCompressed Models (EFCM) framework with two stages: unsupervised feature\ndistillation and fine-tuning. In the distillation stage, Feature Projection\nDistillation (FPD) is proposed with a TransScan module for adaptive receptive\nfield adjustment to enhance the knowledge absorption capability of the student\nmodel. In the slide-level fine-tuning stage, three strategies (Reuse CLAM,\nRetrain CLAM, and End2end Train CLAM (ETC)) are compared. Experiments are\nconducted on 11 downstream datasets related to three large medical models:\nRETFound for retina, MRM for chest X-ray, and BROW for histopathology. The\nexperimental results demonstrate that the EFCM framework significantly improves\naccuracy and efficiency in handling slide-level pathological image problems,\neffectively addressing the challenges of deploying large medical models.\nSpecifically, it achieves a 4.33% increase in ACC and a 5.2% increase in AUC\ncompared to the large model BROW on the TCGA-NSCLC and TCGA-BRCA datasets. The\nanalysis of model inference efficiency highlights the high efficiency of the\ndistillation fine-tuning method.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u5728\u91ab\u5b78\u9818\u57df\u4e2d\u6df1\u5ea6\u5b78\u7fd2\u5927\u578b\u6a21\u578b\u7684\u767c\u5c55\uff0c\u5728\u91ab\u5b78\u5f71\u50cf\u5206\u6790\u548c\u8a3a\u65b7\u65b9\u9762\u5c55\u73fe\u4e86\u5353\u8d8a\u7684\u8868\u73fe\uff0c\u4f46\u5176\u9f90\u5927\u7684\u53c3\u6578\u91cf\u537b\u5c0e\u81f4\u8a18\u61b6\u9ad4\u548c\u63a8\u8ad6\u5ef6\u9072\u7684\u6311\u6230\u3002\u77e5\u8b58\u84b8\u993e\u63d0\u4f9b\u4e86\u4e00\u7a2e\u89e3\u6c7a\u65b9\u6848\uff0c\u4f46\u7531\u65bc\u9ad8\u89e3\u6790\u5ea6\u7684\u75c5\u7406\u5f71\u50cf\u548c\u5e7b\u71c8\u7247\u5c64\u7d1a\u6a19\u7c64\uff0c\u5e7b\u71c8\u7247\u5c64\u7d1a\u7684\u68af\u5ea6\u7121\u6cd5\u53cd\u5411\u50b3\u64ad\u4ee5\u66f4\u65b0\u5b78\u751f\u6a21\u578b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u500b\u58d3\u7e2e\u6a21\u578b\u7684\u6709\u6548\u5fae\u8abf (EFCM) \u67b6\u69cb\uff0c\u5305\u542b\u5169\u500b\u968e\u6bb5\uff1a\u7121\u76e3\u7763\u7279\u5fb5\u84b8\u993e\u548c\u5fae\u8abf\u3002\u5728\u84b8\u993e\u968e\u6bb5\uff0c\u63d0\u51fa\u7279\u5fb5\u6295\u5f71\u84b8\u993e (FPD) \u8207 TransScan \u6a21\u7d44\uff0c\u4ee5\u9032\u884c\u9069\u61c9\u6027\u611f\u53d7\u91ce\u8abf\u6574\uff0c\u4ee5\u589e\u5f37\u5b78\u751f\u6a21\u578b\u7684\u77e5\u8b58\u5438\u6536\u80fd\u529b\u3002\u5728\u5e7b\u71c8\u7247\u5c64\u7d1a\u5fae\u8abf\u968e\u6bb5\uff0c\u6bd4\u8f03\u4e86\u4e09\u7a2e\u7b56\u7565\uff08\u91cd\u8907\u4f7f\u7528 CLAM\u3001\u91cd\u65b0\u8a13\u7df4 CLAM \u548c\u7aef\u5c0d\u7aef\u8a13\u7df4 CLAM (ETC)\uff09\u3002\u91dd\u5c0d\u8207\u4e09\u500b\u5927\u578b\u91ab\u5b78\u6a21\u578b\u76f8\u95dc\u7684 11 \u500b\u4e0b\u6e38\u8cc7\u6599\u96c6\u9032\u884c\u4e86\u5be6\u9a57\uff1a\u91dd\u5c0d\u8996\u7db2\u819c\u7684 RETFound\u3001\u91dd\u5c0d\u80f8\u90e8 X \u5149\u7684 MRM \u548c\u91dd\u5c0d\u7d44\u7e54\u75c5\u7406\u5b78\u7684 BROW\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cEFCM \u67b6\u69cb\u986f\u8457\u63d0\u5347\u4e86\u8655\u7406\u5e7b\u71c8\u7247\u5c64\u7d1a\u75c5\u7406\u5f71\u50cf\u554f\u984c\u7684\u6e96\u78ba\u6027\u548c\u6548\u7387\uff0c\u6709\u6548\u61c9\u5c0d\u4e86\u90e8\u7f72\u5927\u578b\u91ab\u5b78\u6a21\u578b\u7684\u6311\u6230\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u8207\u5927\u578b\u6a21\u578b BROW \u76f8\u6bd4\uff0c\u5b83\u5728 TCGA-NSCLC \u548c TCGA-BRCA \u8cc7\u6599\u96c6\u4e0a\u5206\u5225\u63d0\u5347\u4e86 4.33% \u7684 ACC \u548c 5.2% \u7684 AUC\u3002\u6a21\u578b\u63a8\u8ad6\u6548\u7387\u7684\u5206\u6790\u7a81\u51fa\u4e86\u84b8\u993e\u5fae\u8abf\u65b9\u6cd5\u7684\u9ad8\u6548\u7387\u3002</paragraph>", "author": "Shaojie Li et.al.", "authors": "Shaojie Li, Zhaoshuo Diao", "id": "2409.11817v1", "paper_url": "http://arxiv.org/abs/2409.11817v1", "repo": "null"}}