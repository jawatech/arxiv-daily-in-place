
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-05-21**|**Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**|William Brandon et.al.|[2405.12981v1](http://arxiv.org/abs/2405.12981v1)|null|
|**2024-05-21**|**Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale**|Shriram Chennakesavalu et.al.|[2405.12961v1](http://arxiv.org/abs/2405.12961v1)|null|
|**2024-05-21**|**Strategic Deployment of Honeypots in Blockchain-based IoT Systems**|Daniel Commey et.al.|[2405.12951v1](http://arxiv.org/abs/2405.12951v1)|null|
|**2024-05-21**|**Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models**|Zhangyue Yin et.al.|[2405.12939v1](http://arxiv.org/abs/2405.12939v1)|null|
|**2024-05-21**|**Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs**|Bilgehan Sel et.al.|[2405.12933v1](http://arxiv.org/abs/2405.12933v1)|null|
|**2024-05-21**|**Code-mixed Sentiment and Hate-speech Prediction**|Anjali Yadav et.al.|[2405.12929v1](http://arxiv.org/abs/2405.12929v1)|null|
|**2024-05-21**|**Panmodal Information Interaction**|Chirag Shah et.al.|[2405.12923v1](http://arxiv.org/abs/2405.12923v1)|null|
|**2024-05-21**|**G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation**|Xingyuan Pan et.al.|[2405.12915v1](http://arxiv.org/abs/2405.12915v1)|null|
|**2024-05-21**|**Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment**|Holli Sargeant et.al.|[2405.12910v1](http://arxiv.org/abs/2405.12910v1)|null|
|**2024-05-21**|**Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents**|San Kim et.al.|[2405.12900v1](http://arxiv.org/abs/2405.12900v1)|null|
|**2024-05-21**|**Investigating Persuasion Techniques in Arabic: An Empirical Study Leveraging Large Language Models**|Abdurahmman Alzahrani et.al.|[2405.12884v1](http://arxiv.org/abs/2405.12884v1)|null|
|**2024-05-21**|**Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in Remote Sensing Images**|Xiaofei Yu et.al.|[2405.12875v1](http://arxiv.org/abs/2405.12875v1)|[link](https://github.com/fay-y/diffusion-rscc)|
|**2024-05-21**|**Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics**|Liming Wu et.al.|[2405.12868v1](http://arxiv.org/abs/2405.12868v1)|null|
|**2024-05-21**|**LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language**|James Requeima et.al.|[2405.12856v1](http://arxiv.org/abs/2405.12856v1)|null|
|**2024-05-21**|**Training and inference in the ReckON RSNN architecture implemented on a MPSoC**|Alejandro Linares-Barranco et.al.|[2405.12849v1](http://arxiv.org/abs/2405.12849v1)|null|
|**2024-05-21**|**Large Language Models Meet NLP: A Survey**|Libo Qin et.al.|[2405.12819v1](http://arxiv.org/abs/2405.12819v1)|null|
|**2024-05-21**|**FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information**|Dongseong Hwang et.al.|[2405.12807v1](http://arxiv.org/abs/2405.12807v1)|null|
|**2024-05-21**|**What Have We Achieved on Non-autoregressive Translation?**|Yafu Li et.al.|[2405.12788v1](http://arxiv.org/abs/2405.12788v1)|null|
|**2024-05-21**|**Transformer in Touch: A Survey**|Jing Gao et.al.|[2405.12779v1](http://arxiv.org/abs/2405.12779v1)|null|
|**2024-05-21**|**Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances**|Hanlei Zhang et.al.|[2405.12775v1](http://arxiv.org/abs/2405.12775v1)|null|
|**2024-05-21**|**Progress Measures for Grokking on Real-world Datasets**|Satvik Golechha et.al.|[2405.12755v1](http://arxiv.org/abs/2405.12755v1)|null|
|**2024-05-21**|**Neural Operator for Accelerating Coronal Magnetic Field Model**|Yutao Du et.al.|[2405.12754v1](http://arxiv.org/abs/2405.12754v1)|null|
|**2024-05-21**|**Generative AI and Large Language Models for Cyber Security: All Insights You Need**|Mohamed Amine Ferrag et.al.|[2405.12750v1](http://arxiv.org/abs/2405.12750v1)|null|
|**2024-05-21**|**The Echoes of Multilinguality: Tracing Cultural Value Shifts during LM Fine-tuning**|Rochelle Choenni et.al.|[2405.12744v1](http://arxiv.org/abs/2405.12744v1)|null|
|**2024-05-21**|**RecGPT: Generative Pre-training for Text-based Recommendation**|Hoang Ngo et.al.|[2405.12715v1](http://arxiv.org/abs/2405.12715v1)|null|
|**2024-05-21**|**From Human-to-Human to Human-to-Bot Conversations in Software Engineering**|Ranim Khojah et.al.|[2405.12712v1](http://arxiv.org/abs/2405.12712v1)|null|
|**2024-05-21**|**A Masked Semi-Supervised Learning Approach for Otago Micro Labels Recognition**|Meng Shang et.al.|[2405.12711v1](http://arxiv.org/abs/2405.12711v1)|null|
|**2024-05-21**|**Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting**|Omar Hamed et.al.|[2405.12705v1](http://arxiv.org/abs/2405.12705v1)|null|
|**2024-05-21**|**OLAPH: Improving Factuality in Biomedical Long-form Question Answering**|Minbyul Jeong et.al.|[2405.12701v1](http://arxiv.org/abs/2405.12701v1)|[link](https://github.com/dmis-lab/olaph)|
|**2024-05-21**|**Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text**|Yafu Li et.al.|[2405.12689v1](http://arxiv.org/abs/2405.12689v1)|null|
|**2024-05-21**|**A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges**|Huangjun Shen et.al.|[2405.12669v1](http://arxiv.org/abs/2405.12669v1)|null|
|**2024-05-21**|**Mitigating Overconfidence in Out-of-Distribution Detection by Capturing Extreme Activations**|Mohammad Azizmalayeri et.al.|[2405.12658v1](http://arxiv.org/abs/2405.12658v1)|null|
|**2024-05-21**|**Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction**|Yu-Hsiang Lin et.al.|[2405.12656v1](http://arxiv.org/abs/2405.12656v1)|[link](https://github.com/exiled1143/retrieval-augmented-language-model-for-multi-label-knowledge-graph-link-prediction)|
|**2024-05-21**|**Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable Term Frequency**|Hyeongjin Kim et.al.|[2405.12648v1](http://arxiv.org/abs/2405.12648v1)|null|
|**2024-05-21**|**Exploration of Masked and Causal Language Modelling for Text Generation**|Nicolo Micheletti et.al.|[2405.12630v1](http://arxiv.org/abs/2405.12630v1)|null|
|**2024-05-21**|**Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition**|Matteo Bortoletto et.al.|[2405.12621v1](http://arxiv.org/abs/2405.12621v1)|null|
|**2024-05-21**|**Quantifying Emergence in Large Language Models**|Hang Chen et.al.|[2405.12617v1](http://arxiv.org/abs/2405.12617v1)|[link](https://github.com/zodiark-ch/emergence-of-llms)|
|**2024-05-21**|**Tagengo: A Multilingual Chat Dataset**|Peter Devine et.al.|[2405.12612v1](http://arxiv.org/abs/2405.12612v1)|null|
|**2024-05-21**|**Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming**|Jiaxu Liu et.al.|[2405.12604v1](http://arxiv.org/abs/2405.12604v1)|null|
|**2024-05-21**|**Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression**|Peiyu Liu et.al.|[2405.12591v1](http://arxiv.org/abs/2405.12591v1)|null|
|**2024-05-21**|**Mining the Explainability and Generalization: Fact Verification Based on Self-Instruction**|Guangyao Lu et.al.|[2405.12579v1](http://arxiv.org/abs/2405.12579v1)|null|
|**2024-05-21**|**ProtT3: Protein-to-Text Generation for Text-based Protein Understanding**|Zhiyuan Liu et.al.|[2405.12564v1](http://arxiv.org/abs/2405.12564v1)|[link](https://github.com/acharkq/prott3)|
|**2024-05-21**|**DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing Outcomes from Sensor Data and Expert Knowledge**|Bufang Yang et.al.|[2405.12541v1](http://arxiv.org/abs/2405.12541v1)|null|
|**2024-05-21**|**PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference**|Dongjie Yang et.al.|[2405.12532v1](http://arxiv.org/abs/2405.12532v1)|null|
|**2024-05-21**|**SirLLM: Streaming Infinite Retentive LLM**|Yao Yao et.al.|[2405.12528v1](http://arxiv.org/abs/2405.12528v1)|[link](https://github.com/zoeyyao27/sirllm)|
|**2024-05-21**|**Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models**|Jiaqi Li et.al.|[2405.12523v1](http://arxiv.org/abs/2405.12523v1)|null|
|**2024-05-21**|**Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models**|Charles O'Neill et.al.|[2405.12522v1](http://arxiv.org/abs/2405.12522v1)|null|
|**2024-05-21**|**MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation**|Zhaoning Yu et.al.|[2405.12519v1](http://arxiv.org/abs/2405.12519v1)|null|
|**2024-05-21**|**EntropyStop: Unsupervised Deep Outlier Detection with Loss Entropy**|Yihong Huang et.al.|[2405.12502v1](http://arxiv.org/abs/2405.12502v1)|null|
|**2024-05-21**|**Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks**|Xin-Chun Li et.al.|[2405.12489v1](http://arxiv.org/abs/2405.12489v1)|null|
|**2024-05-21**|**Time Matters: Enhancing Pre-trained News Recommendation Models with Robust User Dwell Time Injection**|Hao Jiang et.al.|[2405.12486v1](http://arxiv.org/abs/2405.12486v1)|null|
|**2024-05-21**|**GASE: Graph Attention Sampling with Edges Fusion for Solving Vehicle Routing Problems**|Zhenwei Wang et.al.|[2405.12475v1](http://arxiv.org/abs/2405.12475v1)|null|
|**2024-05-21**|**Learning Partially Aligned Item Representation for Cross-Domain Sequential Recommendation**|Mingjia Yin et.al.|[2405.12473v1](http://arxiv.org/abs/2405.12473v1)|null|
|**2024-05-21**|**Leveraging Diverse Data Generation for Adaptable Zero-Shot Dialogue State Tracking**|James D. Finch et.al.|[2405.12468v1](http://arxiv.org/abs/2405.12468v1)|null|
|**2024-05-21**|**Stochastic Learning of Computational Resource Usage as Graph Structured Multimarginal Schr√∂dinger Bridge**|Georgiy A. Bondar et.al.|[2405.12463v1](http://arxiv.org/abs/2405.12463v1)|null|
|**2024-05-21**|**Boosting X-formers with Structured Matrix for Long Sequence Time Series Forecasting**|Zhicheng Zhang et.al.|[2405.12462v1](http://arxiv.org/abs/2405.12462v1)|null|
|**2024-05-21**|**WorldAfford: Affordance Grounding based on Natural Language Instructions**|Changmao Chen et.al.|[2405.12461v1](http://arxiv.org/abs/2405.12461v1)|null|
|**2024-05-21**|**PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4**|Seif Abukhalaf et.al.|[2405.12450v1](http://arxiv.org/abs/2405.12450v1)|null|
|**2024-05-21**|**Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation**|Qingyao Li et.al.|[2405.12442v1](http://arxiv.org/abs/2405.12442v1)|null|
|**2024-05-21**|**CoCo Matrix: Taxonomy of Cognitive Contributions in Co-writing with Intelligent Agents**|Ruyuan Wan et.al.|[2405.12438v1](http://arxiv.org/abs/2405.12438v1)|null|
|**2024-05-21**|**Resolving Word Vagueness with Scenario-guided Adapter for Natural Language Inference**|Yonghao Liu et.al.|[2405.12434v1](http://arxiv.org/abs/2405.12434v1)|null|
|**2024-05-21**|**LLM+Reasoning+Planning for supporting incomplete user queries in presence of APIs**|Sudhir Agarwal et.al.|[2405.12433v1](http://arxiv.org/abs/2405.12433v1)|null|
|**2024-05-20**|**A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback**|Kihyun Kim et.al.|[2405.12421v1](http://arxiv.org/abs/2405.12421v1)|null|
|**2024-05-20**|**Targeted Multilingual Adaptation for Low-resource Language Families**|C. M. Downey et.al.|[2405.12413v1](http://arxiv.org/abs/2405.12413v1)|null|
|**2024-05-20**|**Diffusion for World Modeling: Visual Details Matter in Atari**|Eloi Alonso et.al.|[2405.12399v1](http://arxiv.org/abs/2405.12399v1)|[link](https://github.com/eloialonso/diamond)|
|**2024-05-20**|**Layout Agnostic Human Activity Recognition in Smart Homes through Textual Descriptions Of Sensor Triggers (TDOST)**|Megha Thukral et.al.|[2405.12368v1](http://arxiv.org/abs/2405.12368v1)|null|
|**2024-05-20**|**Question-Based Retrieval using Atomic Units for Enterprise RAG**|Vatsal Raina et.al.|[2405.12363v1](http://arxiv.org/abs/2405.12363v1)|null|
|**2024-05-20**|**Perturbing the Gradient for Alleviating Meta Overfitting**|Manas Gogoi et.al.|[2405.12299v1](http://arxiv.org/abs/2405.12299v1)|null|
|**2024-05-20**|**Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**|Guanglin Zhou et.al.|[2405.12217v1](http://arxiv.org/abs/2405.12217v1)|[link](https://github.com/jameszhou-gl/icl-distribution-shift)|
|**2024-05-20**|**MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**|Hongwei Liu et.al.|[2405.12209v1](http://arxiv.org/abs/2405.12209v1)|[link](https://github.com/open-compass/mathbench)|
|**2024-05-20**|**Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models**|Tong Zeng et.al.|[2405.12206v1](http://arxiv.org/abs/2405.12206v1)|[link](https://github.com/sciosci/cite-worthiness)|
|**2024-05-20**|**Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving**|Aniket Didolkar et.al.|[2405.12205v1](http://arxiv.org/abs/2405.12205v1)|null|
|**2024-05-20**|**Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution**|Xihaier Luo et.al.|[2405.12202v1](http://arxiv.org/abs/2405.12202v1)|null|
|**2024-05-20**|**Multi-order Graph Clustering with Adaptive Node-level Weight Learning**|Ye Liu et.al.|[2405.12183v1](http://arxiv.org/abs/2405.12183v1)|[link](https://github.com/scutft-ml/mogc)|
|**2024-05-20**|**Building Temporal Kernels with Orthogonal Polynomials**|Yan Ru Pei et.al.|[2405.12179v1](http://arxiv.org/abs/2405.12179v1)|[link](https://github.com/peabrane/pleiades)|
|**2024-05-20**|**CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**|Haoxiang Shi et.al.|[2405.12174v1](http://arxiv.org/abs/2405.12174v1)|null|
|**2024-05-20**|**Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**|Xiaobo Liang et.al.|[2405.12163v1](http://arxiv.org/abs/2405.12163v1)|[link](https://github.com/dropreg/fennec)|
|**2024-05-20**|**Bangladeshi Native Vehicle Detection in Wild**|Bipin Saha et.al.|[2405.12150v1](http://arxiv.org/abs/2405.12150v1)|[link](https://github.com/bipin-saha/bnvd)|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147v1](http://arxiv.org/abs/2405.12147v1)|null|
|**2024-05-20**|**MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**|Ting Jiang et.al.|[2405.12130v1](http://arxiv.org/abs/2405.12130v1)|[link](https://github.com/kongds/mora)|
|**2024-05-20**|**Prompt Learning for Generalized Vehicle Routing**|Fei Liu et.al.|[2405.12262v1](http://arxiv.org/abs/2405.12262v1)|null|
|**2024-05-20**|**Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**|Zhankui He et.al.|[2405.12119v1](http://arxiv.org/abs/2405.12119v1)|null|
|**2024-05-20**|**Linguistic Structure from a Bottleneck on Sequential Information Processing**|Richard Futrell et.al.|[2405.12109v1](http://arxiv.org/abs/2405.12109v1)|null|
|**2024-05-20**|**Imp: Highly Capable Large Multimodal Models for Mobile Devices**|Zhenwei Shao et.al.|[2405.12107v1](http://arxiv.org/abs/2405.12107v1)|[link](https://github.com/milvlg/imp)|
|**2024-05-20**|**DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**|Hao Chen et.al.|[2405.12100v1](http://arxiv.org/abs/2405.12100v1)|null|
|**2024-05-20**|**Distributional Semantics, Holism, and the Instability of Meaning**|Jumbly Grindrod et.al.|[2405.12084v1](http://arxiv.org/abs/2405.12084v1)|null|
|**2024-05-20**|**Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model**|Chen Huang et.al.|[2405.12081v1](http://arxiv.org/abs/2405.12081v1)|null|
|**2024-05-20**|**AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements**|Calvin Yeung et.al.|[2405.12070v1](http://arxiv.org/abs/2405.12070v1)|[link](https://github.com/calvinyeungck/3d-shot-posture-dataset)|
|**2024-05-20**|**CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models**|Tong Zhang et.al.|[2405.12063v1](http://arxiv.org/abs/2405.12063v1)|[link](https://github.com/zt991211/clamber)|
|**2024-05-20**|**STYLE: Improving Domain Transferability of Asking Clarification Questions in Large Language Model Powered Conversational Agents**|Yue Chen et.al.|[2405.12059v1](http://arxiv.org/abs/2405.12059v1)|null|
|**2024-05-20**|**Unveiling factors influencing judgment variation in Sentiment Analysis with Natural Language Processing and Statistics**|Olga Kellert et.al.|[2405.12055v1](http://arxiv.org/abs/2405.12055v1)|null|
|**2024-05-20**|**EXACT: Towards a platform for empirically benchmarking Machine Learning model explanation methods**|Benedict Clark et.al.|[2405.12261v1](http://arxiv.org/abs/2405.12261v1)|null|
|**2024-05-20**|**KG-RAG: Bridging the Gap Between Knowledge and Creativity**|Diego Sanmartin et.al.|[2405.12035v1](http://arxiv.org/abs/2405.12035v1)|null|
|**2024-05-20**|**Can AI Relate: Testing Large Language Model Response for Mental Health Support**|Saadia Gabriel et.al.|[2405.12021v1](http://arxiv.org/abs/2405.12021v1)|null|
|**2024-05-20**|**Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning**|Hai Zhang et.al.|[2405.12001v1](http://arxiv.org/abs/2405.12001v1)|null|
|**2024-05-20**|**A review on the use of large language models as virtual tutors**|Silvia Garc√≠a-M√©ndez et.al.|[2405.11983v1](http://arxiv.org/abs/2405.11983v1)|null|
|**2024-05-20**|**Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space**|Qianmei Liu et.al.|[2405.11982v1](http://arxiv.org/abs/2405.11982v1)|null|
|**2024-05-20**|**SM-DTW: Stability Modulated Dynamic Time Warping for signature verification**|Antonio Parziale et.al.|[2405.11978v1](http://arxiv.org/abs/2405.11978v1)|null|
|**2024-05-20**|**Conditional Shift-Robust Conformal Prediction for Graph Neural Network**|S. Akansha et.al.|[2405.11968v1](http://arxiv.org/abs/2405.11968v1)|null|
|**2024-05-20**|**Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home**|Tatiana V. Afanasieva et.al.|[2405.11967v1](http://arxiv.org/abs/2405.11967v1)|null|

#### Abstracts
##### **Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**
2405.12981v1 by William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, Jonathan Ragan Kelly

Key-value (KV) caching plays an essential role in accelerating decoding for
transformer-based autoregressive large language models (LLMs). However, the
amount of memory required to store the KV cache can become prohibitive at long
sequence lengths and large batch sizes. Since the invention of the transformer,
two of the most effective interventions discovered for reducing the size of the
KV cache have been Multi-Query Attention (MQA) and its generalization,
Grouped-Query Attention (GQA). MQA and GQA both modify the design of the
attention block so that multiple query heads can share a single key/value head,
reducing the number of distinct key/value heads by a large factor while only
minimally degrading accuracy. In this paper, we show that it is possible to
take Multi-Query Attention a step further by also sharing key and value heads
between adjacent layers, yielding a new attention design we call Cross-Layer
Attention (CLA). With CLA, we find that it is possible to reduce the size of
the KV cache by another 2x while maintaining nearly the same accuracy as
unmodified MQA. In experiments training 1B- and 3B-parameter models from
scratch, we demonstrate that CLA provides a Pareto improvement over the
memory/accuracy tradeoffs which are possible with traditional MQA, enabling
inference with longer sequence lengths and larger batch sizes than would
otherwise be possible

ÊëòË¶ÅÔºöÈóúÈçµÂÄº (KV) Âø´ÂèñÂú®Âä†ÈÄüËΩâÊèõÂô®ÂºèËá™Ëø¥Ê≠∏Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËß£Á¢ºÈÅéÁ®ã‰∏≠ÊâÆÊºîËëó‰∏çÂèØÊàñÁº∫ÁöÑËßíËâ≤„ÄÇÁÑ∂ËÄåÔºåÂú®Â∫èÂàóÈï∑Â∫¶ÂíåÊâπÊ¨°Â§ßÂ∞èËºÉÂ§ßÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂÑ≤Â≠ò KV Âø´ÂèñÊâÄÈúÄÁöÑË®òÊÜ∂È´îÈáèÂèØËÉΩÈÅéÊñºÈæêÂ§ß„ÄÇËá™ËΩâÊèõÂô®ÁôºÊòé‰ª•‰æÜÔºåÂ∑≤ÁôºÁèæÁî®ÊñºÁ∏ÆÊ∏õ KV Âø´ÂèñÂ§ßÂ∞èÊúÄÊúâÊïàÁöÑÂÖ©ÂÄã‰ªãÂÖ•Êé™ÊñΩÂàÜÂà•ÁÇ∫Â§öÈáçÊü•Ë©¢Ê≥®ÊÑèÂäõ (MQA) ÂèäÂÖ∂Âª£Áæ©ÂåñÁâàÊú¨Áæ§ÁµÑÊü•Ë©¢Ê≥®ÊÑèÂäõ (GQA)„ÄÇMQA Âíå GQA ÈÉΩ‰øÆÊîπ‰∫ÜÊ≥®ÊÑèÂäõÂçÄÂ°äÁöÑË®≠Ë®àÔºåËÆìÂ§öÂÄãÊü•Ë©¢È†≠ÈÉ®ÂèØ‰ª•ÂÖ±Áî®‰∏ÄÂÄãÈáëÈë∞/ÂÄºÈ†≠ÈÉ®ÔºåÂ§ßÂπÖÊ∏õÂ∞ëÁõ∏Áï∞ÈáëÈë∞/ÂÄºÈ†≠ÈÉ®ÁöÑÊï∏ÈáèÔºåÂêåÊôÇÂÉÖÈÄ†ÊàêÊ•µÂ∞èÁöÑÊ∫ñÁ¢∫Â∫¶Èôç‰Ωé„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÈÄ≤‰∏ÄÊ≠•ÂÖ±Áî®Áõ∏ÈÑ∞Â±§‰πãÈñìÁöÑÈáëÈë∞ÂíåÂÄºÈ†≠ÈÉ®ÔºåËÆìÂ§öÈáçÊü•Ë©¢Ê≥®ÊÑèÂäõÊõ¥‰∏ä‰∏ÄÂ±§Ê®ìÔºå‰∏¶Áî¢Áîü‰∏ÄÁ®ÆÊàëÂÄëÁ®±‰πãÁÇ∫Ë∑®Â±§Ê≥®ÊÑèÂäõ (CLA) ÁöÑÊñ∞Ê≥®ÊÑèÂäõË®≠Ë®à„ÄÇÈÄèÈÅé CLAÔºåÊàëÂÄëÁôºÁèæÂèØ‰ª•ÂÜçÂ∞á KV Âø´ÂèñÁöÑÂ∞∫ÂØ∏Á∏ÆÂ∞è 2 ÂÄçÔºåÂêåÊôÇÁ∂≠ÊåÅËàáÊú™‰øÆÊîπ MQA Ëøë‰πéÁõ∏ÂêåÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂú®ÂæûÈ†≠Ë®ìÁ∑¥ 1B Âíå 3B ÂèÉÊï∏Ê®°ÂûãÁöÑÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëË≠âÊòé‰∫Ü CLA Âú®Ë®òÊÜ∂È´î/Ê∫ñÁ¢∫Â∫¶Ê¨äË°°ÊñπÈù¢Êèê‰æõ‰∫ÜÂ∏ïÁ¥ØÊâòÊîπÂñÑÔºåË∂ÖË∂ä‰∫ÜÂÇ≥Áµ± MQA ÁöÑÂèØËÉΩÊÄßÔºåËÆìÊé®Ë´ñÂæó‰ª•‰ΩøÁî®Êõ¥Èï∑ÁöÑÂ∫èÂàóÈï∑Â∫¶ÂíåÊõ¥Â§ßÁöÑÊâπÊ¨°Â§ßÂ∞èÔºåËÄåÈÄôÂú®ÂÖ∂‰ªñÊÉÖÊ≥Å‰∏ãÊòØÁÑ°Ê≥ïÂØ¶ÁèæÁöÑ

##### **Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale**
2405.12961v1 by Shriram Chennakesavalu, Frank Hu, Sebastian Ibarraran, Grant M. Rotskoff

Searching through chemical space is an exceptionally challenging problem
because the number of possible molecules grows combinatorially with the number
of atoms. Large, autoregressive models trained on databases of chemical
compounds have yielded powerful generators, but we still lack robust strategies
for generating molecules with desired properties. This molecular search problem
closely resembles the "alignment" problem for large language models, though for
many chemical tasks we have a specific and easily evaluable reward function.
Here, we introduce an algorithm called energy rank alignment (ERA) that
leverages an explicit reward function to produce a gradient-based objective
that we use to optimize autoregressive policies. We show theoretically that
this algorithm is closely related to proximal policy optimization (PPO) and
direct preference optimization (DPO), but has a minimizer that converges to an
ideal Gibbs-Boltzmann distribution with the reward playing the role of an
energy function. Furthermore, this algorithm is highly scalable, does not
require reinforcement learning, and performs well relative to DPO when the
number of preference observations per pairing is small. We deploy this approach
to align molecular transformers to generate molecules with externally specified
properties and find that it does so robustly, searching through diverse parts
of chemical space. While our focus here is on chemical search, we also obtain
excellent results on an AI supervised task for LLM alignment, showing that the
method is scalable and general.

ÊëòË¶ÅÔºö<paragraph>Âú®ÂåñÂ≠∏Á©∫Èñì‰∏≠ÈÄ≤Ë°åÊêúÂ∞ãÊòØ‰∏ÄÂÄãÊ•µÂÖ∑ÊåëÊà∞ÊÄßÁöÑÂïèÈ°åÔºåÂõ†ÁÇ∫ÂèØËÉΩÁöÑÂàÜÂ≠êÊï∏ÈáèÊúÉÈö®ËëóÂéüÂ≠êÊï∏ÈáèÁöÑÂ¢ûÂä†ËÄåÁµÑÂêàÂºèÂ¢ûÈï∑„ÄÇÂú®ÂåñÂ≠∏ÂåñÂêàÁâ©Ë≥áÊñôÂ∫´‰∏äË®ìÁ∑¥ÁöÑÂ§ßÂûãËá™Ëø¥Ê≠∏Ê®°ÂûãÂ∑≤Á∂ìÁî¢Áîü‰∫ÜÂº∑Â§ßÁöÑÁîüÊàêÂô®Ôºå‰ΩÜÊàëÂÄë‰ªçÁÑ∂Áº∫‰πèÁî®ÊñºÁîüÊàêÂÖ∑ÊúâÊâÄÈúÄÂ±¨ÊÄßÁöÑÂàÜÂ≠êÁöÑÁ©©ÂÅ•Á≠ñÁï•„ÄÇÈÄôÂÄãÂàÜÂ≠êÊêúÂ∞ãÂïèÈ°åËàáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑ„ÄåÊØîÂ∞ç„ÄçÂïèÈ°åÈùûÂ∏∏Áõ∏‰ººÔºåÂÑòÁÆ°Â∞çÊñºË®±Â§öÂåñÂ≠∏‰ªªÂãôÔºåÊàëÂÄëÊúâ‰∏ÄÂÄãÂÖ∑È´î‰∏îÊòìÊñºË©ï‰º∞ÁöÑÂõûÈ•ãÂáΩÊï∏„ÄÇÂú®Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∏ÄÁ®ÆÁ®±ÁÇ∫ËÉΩÈáèÁ≠âÁ¥öÊØîÂ∞ç (ERA) ÁöÑÊºîÁÆóÊ≥ïÔºåÂÆÉÂà©Áî®ÊòéÁ¢∫ÁöÑÂõûÈ•ãÂáΩÊï∏‰æÜÁî¢ÁîüÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑÁõÆÊ®ôÔºåÊàëÂÄë‰ΩøÁî®Ë©≤ÁõÆÊ®ô‰æÜÊúÄ‰Ω≥ÂåñËá™Ëø¥Ê≠∏Á≠ñÁï•„ÄÇÊàëÂÄëÂú®ÁêÜË´ñ‰∏äË≠âÊòé‰∫ÜÈÄôÂÄãÊºîÁÆóÊ≥ïËàáËøëÁ´ØÁ≠ñÁï•ÊúÄ‰Ω≥Âåñ (PPO) ÂíåÁõ¥Êé•ÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ (DPO) ÂØÜÂàáÁõ∏ÈóúÔºå‰ΩÜÂÖ∑ÊúâÊî∂ÊñÇÂà∞ÁêÜÊÉ≥ Gibbs-Boltzmann ÂàÜÂ∏ÉÁöÑÊúÄÂ∞èÂåñÂô®ÔºåÂÖ∂‰∏≠ÂõûÈ•ãÊâÆÊºîËÉΩÈáèÂáΩÊï∏ÁöÑËßíËâ≤„ÄÇÊ≠§Â§ñÔºåÈÄôÂÄãÊºîÁÆóÊ≥ïÂÖ∑ÊúâÈ´òÂ∫¶ÁöÑÂèØÊì¥ÂÖÖÊÄßÔºå‰∏çÈúÄË¶ÅÂº∑ÂåñÂ≠∏ÁøíÔºå‰∏¶‰∏îÂú®ÊØèÊ¨°ÈÖçÂ∞çÁöÑÂÅèÂ•ΩËßÄÂØüÊï∏ÈáèËºÉÂ∞ëÊôÇÔºåÁõ∏Â∞çÊñº DPO ÁöÑË°®ÁèæËâØÂ•Ω„ÄÇÊàëÂÄëÊé°Áî®ÈÄôÁ®ÆÊñπÊ≥ï‰æÜÊØîÂ∞çÂàÜÂ≠êËΩâÊèõÂô®Ôºå‰ª•ÁîüÊàêÂÖ∑ÊúâÂ§ñÈÉ®ÊåáÂÆöÂ±¨ÊÄßÁöÑÂàÜÂ≠êÔºå‰∏¶ÁôºÁèæÂÆÉËÉΩÁ©©ÂÅ•Âú∞ÈÄ≤Ë°åÊêúÂ∞ãÔºåÂú®ÂåñÂ≠∏Á©∫ÈñìÁöÑ‰∏çÂêåÈÉ®ÂàÜ‰∏≠ÈÄ≤Ë°åÊêúÂ∞ã„ÄÇÈõñÁÑ∂ÊàëÂÄëÈÄôË£°ÁöÑÈáçÈªûÊòØÂåñÂ≠∏ÊêúÂ∞ãÔºå‰ΩÜÊàëÂÄë‰πüÂú® LLM ÊØîÂ∞çÁöÑ‰∫∫Â∑•Êô∫ÊÖßÁõ£Áù£‰ªªÂãô‰∏≠Áç≤Âæó‰∫ÜÊ•µ‰Ω≥ÁöÑÁµêÊûúÔºåÈÄôË°®ÊòéÈÄôÂÄãÊñπÊ≥ïÂÖ∑ÊúâÂèØÊì¥ÂÖÖÊÄßÂíåÈÄöÁî®ÊÄß„ÄÇ</paragraph>

##### **Strategic Deployment of Honeypots in Blockchain-based IoT Systems**
2405.12951v1 by Daniel Commey, Sena Hounsinou, Garth V. Crosby

This paper addresses the challenge of enhancing cybersecurity in
Blockchain-based Internet of Things (BIoTs) systems, which are increasingly
vulnerable to sophisticated cyberattacks. It introduces an AI-powered system
model for the dynamic deployment of honeypots, utilizing an Intrusion Detection
System (IDS) integrated with smart contract functionalities on IoT nodes. This
model enables the transformation of regular nodes into decoys in response to
suspicious activities, thereby strengthening the security of BIoT networks. The
paper analyses strategic interactions between potential attackers and the
AI-enhanced IDS through a game-theoretic model, specifically Bayesian games.
The model focuses on understanding and predicting sophisticated attacks that
may initially appear normal, emphasizing strategic decision-making, optimized
honeypot deployment, and adaptive strategies in response to evolving attack
patterns.

ÊëòË¶ÅÔºöÊú¨ÊñáÊé¢Ë®é‰∫ÜÊèêÂçáÂçÄÂ°äÈèàÁâ©ËÅØÁ∂≤ (BIoTs) Á≥ªÁµ±‰∏≠Á∂≤Ë∑ØÂÆâÂÖ®ÊÄßÁöÑÊåëÊà∞ÔºåÈÄô‰∫õÁ≥ªÁµ±Ë∂ä‰æÜË∂äÂÆπÊòìÂèóÂà∞Ë§áÈõúÁöÑÁ∂≤Ë∑ØÊîªÊìä„ÄÇÂÆÉÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁî± AI È©ÖÂãïÁöÑÁ≥ªÁµ±Ê®°ÂûãÔºåÁî®ÊñºÂãïÊÖãÈÉ®ÁΩ≤Ë™òÊçïÂô®ÔºåÂà©Áî®ËàáÁâ©ËÅØÁ∂≤ÁØÄÈªû‰∏äÁöÑÊô∫ÊÖßÂêàÁ¥ÑÂäüËÉΩÊï¥ÂêàÁöÑÂÖ•‰æµÂÅµÊ∏¨Á≥ªÁµ± (IDS)„ÄÇÊ≠§Ê®°ÂûãËÉΩÂ∞á‰∏ÄËà¨ÁØÄÈªûËΩâÊèõÁÇ∫Ë™òÈ§åÔºå‰ª•ÂõûÊáâÂèØÁñëÊ¥ªÂãïÔºåÈÄ≤ËÄåÂº∑Âåñ BIoT Á∂≤Ë∑ØÁöÑÂÆâÂÖ®ÊÄß„ÄÇÊú¨ÊñáÈÄèÈÅéÂçöÂºàË´ñÊ®°ÂûãÔºåÁâπÂà•ÊòØË≤ùÊ∞èÈÅäÊà≤ÔºåÂàÜÊûêÊΩõÂú®ÊîªÊìäËÄÖËàá AI Â¢ûÂº∑ÁöÑ IDS ‰πãÈñìÁöÑÁ≠ñÁï•‰∫íÂãï„ÄÇÊ≠§Ê®°ÂûãÂ∞àÊ≥®Êñº‰∫ÜËß£ÂíåÈ†êÊ∏¨‰∏ÄÈñãÂßãÁúãËµ∑‰æÜÊ≠£Â∏∏ÁöÑË§áÈõúÊîªÊìäÔºåÂº∑Ë™øÁ≠ñÁï•Ê±∫Á≠ñ„ÄÅÊúÄ‰Ω≥ÂåñË™òÊçïÂô®ÈÉ®ÁΩ≤Ôºå‰ª•ÂèäÊ†πÊìö‰∏çÊñ∑ËÆäÂåñÁöÑÊîªÊìäÊ®°ÂºèÊé°ÂèñÈÅ©ÊáâÁ≠ñÁï•„ÄÇ

##### **Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models**
2405.12939v1 by Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Tianxiang Sun, Cheng Chang, Qinyuan Cheng, Ding Wang, Xiaofeng Mou, Xipeng Qiu, XuanJing Huang

Recent advancements in Chain-of-Thought prompting have facilitated
significant breakthroughs for Large Language Models (LLMs) in complex reasoning
tasks. Current research enhances the reasoning performance of LLMs by sampling
multiple reasoning chains and ensembling based on the answer frequency.
However, this approach fails in scenarios where the correct answers are in the
minority. We identify this as a primary factor constraining the reasoning
capabilities of LLMs, a limitation that cannot be resolved solely based on the
predicted answers. To address this shortcoming, we introduce a hierarchical
reasoning aggregation framework AoR (Aggregation of Reasoning), which selects
answers based on the evaluation of reasoning chains. Additionally, AoR
incorporates dynamic sampling, adjusting the number of reasoning chains in
accordance with the complexity of the task. Experimental results on a series of
complex reasoning tasks show that AoR outperforms prominent ensemble methods.
Further analysis reveals that AoR not only adapts various LLMs but also
achieves a superior performance ceiling when compared to current methods.

ÊëòË¶ÅÔºöÊúÄËøëÂú®ÊÄùËÄÉÈìæÊèêÁ§∫ÊñπÈù¢ÁöÑËøõÊ≠•‰øÉËøõ‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÈáçÂ§ßÁ™ÅÁ†¥„ÄÇÁõÆÂâçÁöÑÁ†îÁ©∂ÈÄöËøáÂØπÂ§ö‰∏™Êé®ÁêÜÈìæËøõË°åÈááÊ†∑Âπ∂Ê†πÊçÆÁ≠îÊ°àÈ¢ëÁéáËøõË°åÈõÜÊàêÔºåÂ¢ûÂº∫‰∫Ü LLM ÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïÂú®Ê≠£Á°ÆÁ≠îÊ°àÂ±û‰∫éÂ∞ëÊï∞Ê¥æÁöÑÊÉÖÂÜµ‰∏ã‰ºöÂ§±Ë¥•„ÄÇÊàë‰ª¨Â∞ÜÂÖ∂Á°ÆÂÆö‰∏∫Âà∂Á∫¶ LLM Êé®ÁêÜËÉΩÂäõÁöÑ‰∏ªË¶ÅÂõ†Á¥†ÔºåËøôÁßçÈôêÂà∂‰∏çËÉΩ‰ªÖÂü∫‰∫éÈ¢ÑÊµãÁ≠îÊ°àÊù•Ëß£ÂÜ≥„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÁº∫ÁÇπÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÂàÜÂ±ÇÊé®ÁêÜËÅöÂêàÊ°ÜÊû∂ AoRÔºàÊé®ÁêÜËÅöÂêàÔºâÔºåÂÆÉÊ†πÊçÆÊé®ÁêÜÈìæÁöÑËØÑ‰º∞Êù•ÈÄâÊã©Á≠îÊ°à„ÄÇÊ≠§Â§ñÔºåAoR ÁªìÂêà‰∫ÜÂä®ÊÄÅÈááÊ†∑ÔºåÊ†πÊçÆ‰ªªÂä°ÁöÑÂ§çÊùÇÊÄßË∞ÉÊï¥Êé®ÁêÜÈìæÁöÑÊï∞Èáè„ÄÇÂú®‰∏ÄÁ≥ªÂàóÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAoR ‰ºò‰∫éÁ™ÅÂá∫ÁöÑÈõÜÊàêÊñπÊ≥ï„ÄÇËøõ‰∏ÄÊ≠•ÁöÑÂàÜÊûêË°®ÊòéÔºåAoR ‰∏ç‰ªÖÈÄÇÂ∫îÂêÑÁßç LLMÔºåËÄå‰∏î‰∏éÂΩìÂâçÊñπÊ≥ïÁõ∏ÊØîÔºåËøòÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÊÄßËÉΩ‰∏äÈôê„ÄÇ

##### **Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs**
2405.12933v1 by Bilgehan Sel, Priya Shanmugasundaram, Mohammad Kachuee, Kun Zhou, Ruoxi Jia, Ming Jin

Large Language Models (LLMs) have shown remarkable capabilities in tasks such
as summarization, arithmetic reasoning, and question answering. However, they
encounter significant challenges in the domain of moral reasoning and ethical
decision-making, especially in complex scenarios with multiple stakeholders.
This paper introduces the Skin-in-the-Game (SKIG) framework, aimed at enhancing
moral reasoning in LLMs by exploring decisions' consequences from multiple
stakeholder perspectives. Central to SKIG's mechanism is simulating
accountability for actions, which, alongside empathy exercises and risk
assessment, is pivotal to its effectiveness. We validate SKIG's performance
across various moral reasoning benchmarks with proprietary and opensource LLMs,
and investigate its crucial components through extensive ablation analyses.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÊëòË¶Å„ÄÅÁÆóË°ìÊé®ÁêÜÂíåÂïèÈ°åËß£Á≠îÁ≠â‰ªªÂãô‰∏≠Â±ïÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂú®ÈÅìÂæ∑Êé®ÁêÜÂíåÈÅìÂæ∑Ê±∫Á≠ñÈ†òÂüü‰∏≠ÈÅáÂà∞ÈáçÂ§ßÊåëÊà∞ÔºåÁâπÂà•ÊòØÂú®Ê∂âÂèäÂ§öÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑË§áÈõúÂ†¥ÊôØ‰∏≠„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü Skin-in-the-Game (SKIG) Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöÈÅéÂæûÂ§öÂÄãÂà©ÂÆ≥Èóú‰øÇ‰∫∫ÁöÑËßíÂ∫¶Êé¢Ë®éÊ±∫Á≠ñÁöÑÂæåÊûú‰æÜÂ¢ûÂº∑ LLM ÁöÑÈÅìÂæ∑Êé®ÁêÜ„ÄÇSKIG Ê©üÂà∂ÁöÑÊ†∏ÂøÉÊòØÊ®°Êì¨Â∞çË°åÂãïÁöÑË≤†Ë≤¨ÔºåÈÄôËàáÂêåÁêÜÂøÉÁ∑¥ÁøíÂíåÈ¢®Èö™Ë©ï‰º∞‰∏ÄËµ∑ÔºåÂ∞çÂÖ∂ÊúâÊïàÊÄßËá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄë‰ΩøÁî®Â∞àÊúâÂíåÈñãÊ∫ê LLM Âú®ÂêÑÁ®ÆÈÅìÂæ∑Êé®ÁêÜÂü∫Ê∫ñ‰∏äÈ©óË≠â‰∫Ü SKIG ÁöÑÊÄßËÉΩÔºå‰∏¶ÈÄöÈÅéÂª£Ê≥õÁöÑÊ∂àËûçÂàÜÊûêÁ†îÁ©∂‰∫ÜÂÖ∂ÈóúÈçµÁµÑÊàêÈÉ®ÂàÜ„ÄÇ

##### **Code-mixed Sentiment and Hate-speech Prediction**
2405.12929v1 by Anjali Yadav, Tanya Garg, Matej Klemen, Matej Ulcar, Basant Agarwal, Marko Robnik Sikonja

Code-mixed discourse combines multiple languages in a single text. It is
commonly used in informal discourse in countries with several official
languages, but also in many other countries in combination with English or
neighboring languages. As recently large language models have dominated most
natural language processing tasks, we investigated their performance in
code-mixed settings for relevant tasks. We first created four new bilingual
pre-trained masked language models for English-Hindi and English-Slovene
languages, specifically aimed to support informal language. Then we performed
an evaluation of monolingual, bilingual, few-lingual, and massively
multilingual models on several languages, using two tasks that frequently
contain code-mixed text, in particular, sentiment analysis and offensive
language detection in social media texts. The results show that the most
successful classifiers are fine-tuned bilingual models and multilingual models,
specialized for social media texts, followed by non-specialized massively
multilingual and monolingual models, while huge generative models are not
competitive. For our affective problems, the models mostly perform slightly
better on code-mixed data compared to non-code-mixed data.

ÊëòË¶ÅÔºö<paragraph>‰ª£Á¢ºÊ∑∑ÂêàË™ûÁØáÂú®ÂñÆ‰∏ÄÊñáÊú¨‰∏≠ÁµêÂêàÂ§öÁ®ÆË™ûË®Ä„ÄÇÂú®ÊìÅÊúâÊï∏Á®ÆÂÆòÊñπË™ûË®ÄÁöÑÂúãÂÆ∂ÔºåÂÆÉÈÄöÂ∏∏Áî®ÊñºÈùûÊ≠£ÂºèË™ûÁØáÔºå‰ΩÜÂÆÉ‰πüÁî®ÊñºË®±Â§öÂÖ∂‰ªñÂúãÂÆ∂ÔºåËàáËã±Ë™ûÊàñÈÑ∞ËøëË™ûË®ÄÁµêÂêà‰ΩøÁî®„ÄÇÁî±ÊñºÊúÄËøëÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰∏ªÂ∞é‰∫ÜÂ§ßÂ§öÊï∏Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰ªªÂãôÔºåÊàëÂÄëË™øÊü•‰∫ÜÂÆÉÂÄëÂú®‰ª£Á¢ºÊ∑∑ÂêàË®≠ÂÆö‰∏≠Âü∑Ë°åÁõ∏Èóú‰ªªÂãôÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÈ¶ñÂÖàÁÇ∫Ëã±Ë™û-Âç∞Âú∞Ë™ûÂíåËã±Ë™û-ÊñØÊ¥õÁ∂≠Â∞º‰∫ûË™ûÂª∫Á´ã‰∫ÜÂõõÂÄãÊñ∞ÁöÑÈõôË™ûÈ†êË®ìÁ∑¥ÈÅÆÁΩ©Ë™ûË®ÄÊ®°ÂûãÔºåÁâπÂà•Áî®ÊñºÊîØÊè¥ÈùûÊ≠£ÂºèË™ûË®Ä„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂ∞çÂñÆË™û„ÄÅÈõôË™û„ÄÅÂ∞ëË™ûÂíåÂ§ßÈáèÂ§öË™ûÊ®°ÂûãÈÄ≤Ë°å‰∫ÜË©ï‰º∞Ôºå‰ΩøÁî®ÂÖ©Á®ÆÁ∂ìÂ∏∏ÂåÖÂê´‰ª£Á¢ºÊ∑∑ÂêàÊñáÊú¨ÁöÑ‰ªªÂãôÔºåÁâπÂà•ÊòØÂú®Á§æ‰∫§Â™íÈ´îÊñáÊú¨‰∏≠ÁöÑÊÉÖÁ∑íÂàÜÊûêÂíåÊîªÊìäÊÄßË™ûË®ÄÂÅµÊ∏¨„ÄÇÁµêÊûúÈ°ØÁ§∫ÔºåÊúÄÊàêÂäüÁöÑÂàÜÈ°ûÂô®ÊòØÈáùÂ∞çÁ§æ‰∫§Â™íÈ´îÊñáÊú¨ÈÄ≤Ë°åÂæÆË™øÁöÑÈõôË™ûÊ®°ÂûãÂíåÂ§öË™ûÊ®°ÂûãÔºåÂÖ∂Ê¨°ÊòØÈùûÂ∞àÊ•≠ÁöÑÂ§ßÈáèÂ§öË™ûÊ®°ÂûãÂíåÂñÆË™ûÊ®°ÂûãÔºåËÄåÂ∑®Â§ßÁöÑÁîüÊàêÊ®°ÂûãÂâáÊ≤íÊúâÁ´∂Áà≠Âäõ„ÄÇÂ∞çÊñºÊàëÂÄëÁöÑÊÑüÊÉÖÂïèÈ°åÔºåÈÄô‰∫õÊ®°ÂûãÂú®‰ª£Á¢ºÊ∑∑ÂêàË≥áÊñô‰∏äÁöÑÂü∑Ë°åË°®ÁèæÂ§ßÂ§öÁï•ÂÑ™ÊñºÈùû‰ª£Á¢ºÊ∑∑ÂêàË≥áÊñô„ÄÇ</paragraph>

##### **Panmodal Information Interaction**
2405.12923v1 by Chirag Shah, Ryen W. White

The emergence of generative artificial intelligence (GenAI) is transforming
information interaction. For decades, search engines such as Google and Bing
have been the primary means of locating relevant information for the general
population. They have provided search results in the same standard format (the
so-called "10 blue links"). The recent ability to chat via natural language
with AI-based agents and have GenAI automatically synthesize answers in
real-time (grounded in top-ranked results) is changing how people interact with
and consume information at massive scale. These two information interaction
modalities (traditional search and AI-powered chat) coexist in current search
engines, either loosely coupled (e.g., as separate options/tabs) or tightly
coupled (e.g., integrated as a chat answer embedded directly within a
traditional search result page). We believe that the existence of these two
different modalities, and potentially many others, is creating an opportunity
to re-imagine the search experience, capitalize on the strengths of many
modalities, and develop systems and strategies to support seamless flow between
them. We refer to these as panmodal experiences. Unlike monomodal experiences,
where only one modality is available and/or used for the task at hand, panmodal
experiences make multiple modalities available to users (multimodal), directly
support transitions between modalities (crossmodal), and seamlessly combine
modalities to tailor task assistance (transmodal). While our focus is search
and chat, with learnings from insights from a survey of over 100 individuals
who have recently performed common tasks on these two modalities, we also
present a more general vision for the future of information interaction using
multiple modalities and the emergent capabilities of GenAI.

ÊëòË¶ÅÔºöÁîüÊàêÂºè‰∫∫Â∑•Êô∫ËÉΩ (GenAI) ÁöÑÂá∫ÁèæÊ≠£Âú®ËΩâËÆäË≥áË®ä‰∫íÂãï„ÄÇÊï∏ÂçÅÂπ¥‰æÜÔºåGoogle Âíå Bing Á≠âÊêúÂ∞ãÂºïÊìé‰∏ÄÁõ¥ÊòØÊôÆÁæÖÂ§ßÁúæÂ∞ãÊâæÁõ∏ÈóúË≥áË®äÁöÑ‰∏ªË¶ÅÁÆ°ÈÅì„ÄÇ‰ªñÂÄë‰ª•Áõ∏ÂêåÁöÑÊ®ôÊ∫ñÊ†ºÂºèÔºàÊâÄË¨ÇÁöÑ„Äå10 ÂÄãËóçËâ≤ÈÄ£Áµê„ÄçÔºâÊèê‰æõÊêúÂ∞ãÁµêÊûú„ÄÇËøë‰æÜÔºåÈÄèÈÅéËá™ÁÑ∂Ë™ûË®ÄËàá AI ‰ª£ÁêÜËÅäÂ§©Ôºå‰∏¶ËÆì GenAI Ëá™ÂãïÁ∂úÂêàÁ≠îÊ°àÔºàÂª∫Á´ãÊñºÊéíÂêçÊúÄÈ´òÁöÑÁµêÊûúÔºâÁöÑËÉΩÂäõÔºåÊ≠£Âú®ÊîπËÆä‰∫∫ÂÄë‰∫íÂãïÂíåÂ§ßÈáèÊ∂àË≤ªË≥áË®äÁöÑÊñπÂºè„ÄÇÈÄôÂÖ©Á®ÆË≥áË®ä‰∫íÂãïÊ®°ÂºèÔºàÂÇ≥Áµ±ÊêúÂ∞ãÂíå AI È©ÖÂãïÁöÑËÅäÂ§©ÔºâÂú®ÁõÆÂâçÁöÑÊêúÂ∞ãÂºïÊìé‰∏≠‰∏¶Â≠òÔºåÂÆÉÂÄëÂèØËÉΩÊòØÈ¨ÜÊï£ÁµêÂêàÔºà‰æãÂ¶ÇÔºå‰ΩúÁÇ∫Áç®Á´ãÈÅ∏È†Ö/ÂàÜÈ†ÅÔºâÔºåÊàñÁ∑äÂØÜÁµêÂêàÔºà‰æãÂ¶ÇÔºåÊï¥ÂêàÁÇ∫Áõ¥Êé•ÂµåÂÖ•ÂÇ≥Áµ±ÊêúÂ∞ãÁµêÊûúÈ†ÅÈù¢‰∏≠ÁöÑËÅäÂ§©Á≠îÊ°àÔºâ„ÄÇÊàëÂÄëÁõ∏‰ø°ÔºåÈÄôÂÖ©Á®Æ‰∏çÂêåÊ®°ÂºèÁöÑÂ≠òÂú®Ôºå‰ª•ÂèäÊΩõÂú®ÁöÑË®±Â§öÂÖ∂‰ªñÊ®°ÂºèÔºåÊ≠£Âú®ÂâµÈÄ†‰∏ÄÂÄãÊ©üÊúÉÔºåÂèØ‰ª•ÈáçÊñ∞ÊßãÊÉ≥ÊêúÂ∞ãÈ´îÈ©óÔºåÂà©Áî®Ë®±Â§öÊ®°ÂºèÁöÑÂÑ™Âã¢Ôºå‰∏¶ÈñãÁôºÁ≥ªÁµ±ÂíåÁ≠ñÁï•‰æÜÊîØÊè¥ÂÆÉÂÄë‰πãÈñìÁöÑÁÑ°Á∏´ÊµÅÂãï„ÄÇÊàëÂÄëÂ∞áÈÄô‰∫õÁ®±ÁÇ∫Ê≥õÊ®°ÊÖãÈ´îÈ©ó„ÄÇËàáÂñÆÊ®°ÊÖãÈ´îÈ©ó‰∏çÂêåÔºåÂñÆÊ®°ÊÖãÈ´îÈ©óÂÉÖÊèê‰æõ‰∏ÄÁ®ÆÊ®°ÂºèÔºåÊàñÂÉÖ‰ΩøÁî®‰∏ÄÁ®ÆÊ®°Âºè‰æÜÂü∑Ë°åÊâãÈÇäÁöÑ‰ªªÂãôÔºåÊ≥õÊ®°ÊÖãÈ´îÈ©óËÆì‰ΩøÁî®ËÄÖÂèØ‰ª•‰ΩøÁî®Â§öÁ®ÆÊ®°ÂºèÔºàÂ§öÊ®°ÂºèÔºâÔºåÁõ¥Êé•ÊîØÊè¥Âú®Ê®°Âºè‰πãÈñìÁöÑËΩâÊèõÔºàË∑®Ê®°ÂºèÔºâÔºå‰∏¶ÁÑ°Á∏´Âú∞ÁµêÂêàÊ®°Âºè‰ª•ÂÆ¢Ë£ΩÂåñ‰ªªÂãôÂçîÂä©ÔºàË∑®Ê®°ÂºèÔºâ„ÄÇÈõñÁÑ∂ÊàëÂÄëÁöÑÈáçÈªûÊòØÊêúÂ∞ãÂíåËÅäÂ§©Ôºå‰∏¶ÂæûÊúÄËøëÂú®ÂÖ©Á®ÆÊ®°Âºè‰∏äÂü∑Ë°åÂ∏∏Ë¶ã‰ªªÂãôÁöÑ 100 Â§ö‰ΩçÂÄã‰∫∫ÁöÑË¶ãËß£‰∏≠Â≠∏ÁøíÔºå‰ΩÜÊàëÂÄë‰πüÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊõ¥ÈÄöÁî®ÁöÑÈ°òÊôØÔºåÁî®ÊñºÈÄèÈÅéÂ§öÁ®ÆÊ®°ÂºèÂíå GenAI ÁöÑÊñ∞ËààËÉΩÂäõÈÄ≤Ë°åÊú™‰æÜË≥áË®ä‰∫íÂãï„ÄÇ

##### **G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation**
2405.12915v1 by Xingyuan Pan, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Shanbo Cheng

Large Language Models (LLMs) have demonstrated remarkable abilities in
general scenarios. Instruction finetuning empowers them to align with humans in
various tasks. Nevertheless, the Diversity and Quality of the instruction data
remain two main challenges for instruction finetuning. With regard to this, in
this paper, we propose a novel gradient-based method to automatically select
high-quality and diverse instruction finetuning data for machine translation.
Our key innovation centers around analyzing how individual training examples
influence the model during training. Specifically, we select training examples
that exert beneficial influences on the model as high-quality ones by means of
Influence Function plus a small high-quality seed dataset. Moreover, to enhance
the diversity of the training data we maximize the variety of influences they
have on the model by clustering on their gradients and resampling. Extensive
experiments on WMT22 and FLORES translation tasks demonstrate the superiority
of our methods, and in-depth analysis further validates their effectiveness and
generalization.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®‰∏ÄËà¨Â†¥ÊôØ‰∏≠Â±ïÁèæÂá∫ÈùûÂá°ÁöÑËÉΩÂäõ„ÄÇÊåáÁ§∫ÂæÆË™ø‰ΩøÂÆÉÂÄëËÉΩÂ§†Âú®ÂêÑÁ®Æ‰ªªÂãô‰∏≠Ëàá‰∫∫È°û‰øùÊåÅ‰∏ÄËá¥„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÊåáÁ§∫Êï∏ÊìöÁöÑÂ§öÊ®£ÊÄßÂíåÂìÅË≥™‰ªçÁÑ∂ÊòØÊåáÁ§∫ÂæÆË™øÁöÑÂÖ©È†Ö‰∏ªË¶ÅÊåëÊà∞„ÄÇÊúâÈëëÊñºÊ≠§ÔºåÊàëÂÄëÂú®Êú¨Êñá‰∏≠ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂü∫ÊñºÊ¢ØÂ∫¶ÁöÑËá™ÂãïÈÅ∏ÊìáÊ©üÂô®ÁøªË≠ØÁöÑÈ´òÂìÅË≥™‰∏îÂ§öÊ®£ÂåñÁöÑÊåáÁ§∫ÂæÆË™øÊï∏ÊìöÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÈóúÈçµÂâµÊñ∞ÂúçÁπûÂàÜÊûêÂÄãÂà•Ë®ìÁ∑¥ÁØÑ‰æãÂú®Ë®ìÁ∑¥ÊúüÈñìÂ¶Ç‰ΩïÂΩ±ÈüøÊ®°Âûã„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈÅ∏ÊìáÂ∞çÊ®°ÂûãÁî¢ÁîüÊúâÁõäÂΩ±ÈüøÁöÑË®ìÁ∑¥ÁØÑ‰æãÔºå‰∏¶ÈÄèÈÅéÂΩ±ÈüøÂáΩÊï∏Âä†‰∏äÂ∞ëÈáèÈ´òÂìÅË≥™Á®ÆÂ≠êË≥áÊñôÈõÜ‰æÜ‰ΩúÁÇ∫È´òÂìÅË≥™ÁöÑË®ìÁ∑¥ÁØÑ‰æã„ÄÇÊ≠§Â§ñÔºåÁÇ∫‰∫ÜÂ¢ûÂº∑Ë®ìÁ∑¥Êï∏ÊìöÁöÑÂ§öÊ®£ÊÄßÔºåÊàëÂÄëÈÄèÈÅéÂ∞çÂÆÉÂÄëÁöÑÊ¢ØÂ∫¶ÈÄ≤Ë°åÂàÜÁæ§ÂíåÈáçÊñ∞ÂèñÊ®£ÔºåÂ∞áÂÆÉÂÄëÂ∞çÊ®°ÂûãÁöÑÂΩ±ÈüøÊúÄÂ§ßÂåñ„ÄÇÂú® WMT22 Âíå FLORES ÁøªË≠Ø‰ªªÂãô‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÂÑ™Ë∂äÊÄßÔºåÊ∑±ÂÖ•ÂàÜÊûêÈÄ≤‰∏ÄÊ≠•È©óË≠â‰∫ÜÂÆÉÂÄëÁöÑÊúâÊïàÊÄßÂíåÊ¶ÇÊã¨ÊÄß„ÄÇ

##### **Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment**
2405.12910v1 by Holli Sargeant, Ahmed Izzidien, Felix Steffek

This paper addresses a critical gap in legal analytics by developing and
applying a novel taxonomy for topic modelling summary judgment cases in the
United Kingdom. Using a curated dataset of summary judgment cases, we use the
Large Language Model Claude 3 Opus to explore functional topics and trends. We
find that Claude 3 Opus correctly classified the topic with an accuracy of
87.10%. The analysis reveals distinct patterns in the application of summary
judgments across various legal domains. As case law in the United Kingdom is
not originally labelled with keywords or a topic filtering option, the findings
not only refine our understanding of the thematic underpinnings of summary
judgments but also illustrate the potential of combining traditional and
AI-driven approaches in legal classification. Therefore, this paper provides a
new and general taxonomy for UK law. The implications of this work serve as a
foundation for further research and policy discussions in the field of judicial
administration and computational legal research methodologies.

ÊëòË¶ÅÔºöÊú¨Ë´ñÊñáÈÄèÈÅéÈñãÁôº‰∏¶ÊáâÁî®‰∏ÄÁ®ÆÊñ∞ÁöÑÂàÜÈ°ûÊ≥ïÔºåÈáùÂ∞ç‰∏ªÈ°åÂª∫Ê®°ÊëòË¶ÅÂà§Ê±∫Ê°à‰ª∂Ôºå‰æÜÊé¢Ë®éÊ≥ïÂæãÂàÜÊûê‰∏≠ÁöÑ‰∏ÄÂÄãÈóúÈçµÁº∫Âè£Ôºå‰∏¶Â∞áÂÖ∂ÊáâÁî®ÊñºËã±Âúã„ÄÇÊàëÂÄë‰ΩøÁî®Á≠ñÂ±ïÁöÑÊëòË¶ÅÂà§Ê±∫Ê°à‰ª∂Ë≥áÊñôÈõÜÔºå‰ΩøÁî®Â§ßÂûãË™ûË®ÄÊ®°Âûã Claude 3 Opus ‰æÜÊé¢Ë®éÂäüËÉΩÊÄß‰∏ªÈ°åÂíåË∂®Âã¢„ÄÇÊàëÂÄëÁôºÁèæ Claude 3 Opus Ê≠£Á¢∫ÂàÜÈ°û‰∏ªÈ°åÁöÑÊ∫ñÁ¢∫ÁéáÁÇ∫ 87.10%„ÄÇÂàÜÊûêÊè≠Á§∫‰∫ÜÂú®ÂêÑÁ®ÆÊ≥ïÂæãÈ†òÂüü‰∏≠ÊáâÁî®ÊëòË¶ÅÂà§Ê±∫ÁöÑ‰∏çÂêåÊ®°Âºè„ÄÇÁî±ÊñºËã±ÂúãÁöÑÂà§‰æãÊ≥ïÊúÄÂàù‰∏¶Êú™Ê®ôË®òÈóúÈçµÂ≠óÊàñ‰∏ªÈ°åÈÅéÊøæÈÅ∏È†ÖÔºåÂõ†Ê≠§ÈÄô‰∫õÁôºÁèæ‰∏çÂÉÖËÉΩÊîπÂñÑÊàëÂÄëÂ∞çÊëòË¶ÅÂà§Ê±∫‰∏ªÈ°åÂü∫Á§éÁöÑÁêÜËß£ÔºåÈÇÑËÉΩË™™ÊòéÂÇ≥Áµ±ÊñπÊ≥ïÂíå AI È©ÖÂãïÊñπÊ≥ïÂú®Ê≥ïÂæãÂàÜÈ°û‰∏≠ÁµêÂêàÁöÑÊΩõÂäõ„ÄÇÂõ†Ê≠§ÔºåÊú¨ÊñáÊèê‰æõ‰∫ÜËã±ÂúãÊ≥ïÂæãÁöÑ‰∏ÄÂÄãÊñ∞ÁöÑÈÄöÁî®ÂàÜÈ°ûÊ≥ï„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÁöÑÊÑèÁæ©ÁÇ∫Âè∏Ê≥ïË°åÊîøÂíåË®àÁÆóÊ≥ïÂæãÁ†îÁ©∂ÊñπÊ≥ïÈ†òÂüüÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÂíåÊîøÁ≠ñË®éË´ñÂ•†ÂÆö‰∫ÜÂü∫Á§é„ÄÇ

##### **Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents**
2405.12900v1 by San Kim, Gary Geunbae Lee

Recent advancements in open-domain dialogue systems have been propelled by
the emergence of high-quality large language models (LLMs) and various
effective training methodologies. Nevertheless, the presence of toxicity within
these models presents a significant challenge that can potentially diminish the
user experience. In this study, we introduce an innovative training algorithm,
an improvement upon direct preference optimization (DPO), called adversarial
DPO (ADPO). The ADPO algorithm is designed to train models to assign higher
probability distributions to preferred responses and lower distributions to
unsafe responses, which are self-generated using the toxic control token. We
demonstrate that ADPO enhances the model's resilience against harmful
conversations while minimizing performance degradation. Furthermore, we
illustrate that ADPO offers a more stable training procedure compared to the
traditional DPO. To the best of our knowledge, this is the first adaptation of
the DPO algorithm that directly incorporates harmful data into the generative
model, thereby reducing the need to artificially create safe dialogue data.

ÊëòË¶ÅÔºöÈñãÊîæÈ†òÂüüÂ∞çË©±Á≥ªÁµ±ÁöÑÊúÄÊñ∞ÈÄ≤Â±ïÊòØÁî±È´òÂìÅË≥™Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂíåÂêÑÁ®ÆÊúâÊïàË®ìÁ∑¥ÊñπÊ≥ïÁöÑÂá∫ÁèæÊâÄÊé®Âãï„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊ®°Âûã‰∏≠Â≠òÂú®ÁöÑÊØíÊÄßÊòØ‰∏ÄÂÄãÈáçÂ§ßÊåëÊà∞ÔºåÂèØËÉΩÊúÉÈôç‰Ωé‰ΩøÁî®ËÄÖÈ´îÈ©ó„ÄÇÂú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑË®ìÁ∑¥ÊºîÁÆóÊ≥ïÔºå‰∏ÄÁ®ÆÁõ¥Êé•ÂÅèÂ•ΩÊúÄ‰Ω≥ÂåñÁöÑÊîπÈÄ≤ÔºåÁ®±ÁÇ∫Â∞çÊäóÊÄß DPO (ADPO)„ÄÇADPO ÊºîÁÆóÊ≥ïÊó®Âú®Ë®ìÁ∑¥Ê®°ÂûãÂ∞áËºÉÈ´òÁöÑÊ©üÁéáÂàÜ‰ΩàÂàÜÈÖçÁµ¶ÂÅèÂ•ΩÁöÑÂõûÊáâÔºå‰∏¶Â∞áËºÉ‰ΩéÁöÑÊ©üÁéáÂàÜ‰ΩàÂàÜÈÖçÁµ¶‰∏çÂÆâÂÖ®ÁöÑÂõûÊáâÔºåÈÄô‰∫õÂõûÊáâÊòØ‰ΩøÁî®ÊúâÊØíÊéßÂà∂‰ª£Á¢ºËá™Áîü„ÄÇÊàëÂÄëË≠âÊòé ADPO ÊèêÂçá‰∫ÜÊ®°ÂûãÂ∞çÊúâÂÆ≥Â∞çË©±ÁöÑÂæ©ÂéüÂäõÔºåÂêåÊôÇÂ∞áÊïàËÉΩ‰∏ãÈôçÈôçÂà∞ÊúÄ‰Ωé„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË™™Êòé ADPO ËàáÂÇ≥Áµ± DPO Áõ∏ÊØîÊèê‰æõ‰∫ÜÊõ¥Á©©ÂÆöÁöÑË®ìÁ∑¥Á®ãÂ∫è„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØ DPO ÊºîÁÆóÊ≥ïÈ¶ñÊ¨°Â∞áÊúâÂÆ≥Ë≥áÊñôÁõ¥Êé•Á¥çÂÖ•ÁîüÊàêÊ®°ÂûãÔºåÂæûËÄåÊ∏õÂ∞ë‰∫Ü‰∫∫Â∑•Âª∫Á´ãÂÆâÂÖ®Â∞çË©±Ë≥áÊñôÁöÑÈúÄÊ±Ç„ÄÇ

##### **Investigating Persuasion Techniques in Arabic: An Empirical Study Leveraging Large Language Models**
2405.12884v1 by Abdurahmman Alzahrani, Eyad Babkier, Faisal Yanbaawi, Firas Yanbaawi, Hassan Alhuzali

In the current era of digital communication and widespread use of social
media, it is crucial to develop an understanding of persuasive techniques
employed in written text. This knowledge is essential for effectively
discerning accurate information and making informed decisions. To address this
need, this paper presents a comprehensive empirical study focused on
identifying persuasive techniques in Arabic social media content. To achieve
this objective, we utilize Pre-trained Language Models (PLMs) and leverage the
ArAlEval dataset, which encompasses two tasks: binary classification to
determine the presence or absence of persuasion techniques, and multi-label
classification to identify the specific types of techniques employed in the
text. Our study explores three different learning approaches by harnessing the
power of PLMs: feature extraction, fine-tuning, and prompt engineering
techniques. Through extensive experimentation, we find that the fine-tuning
approach yields the highest results on the aforementioned dataset, achieving an
f1-micro score of 0.865 and an f1-weighted score of 0.861. Furthermore, our
analysis sheds light on an interesting finding. While the performance of the
GPT model is relatively lower compared to the other approaches, we have
observed that by employing few-shot learning techniques, we can enhance its
results by up to 20\%. This offers promising directions for future research and
exploration in this topic\footnote{Upon Acceptance, the source code will be
released on GitHub.}.

ÊëòË¶ÅÔºö<paragraph>Âú®Êï∏‰ΩçÈÄöË®äÂíåÁ§æÁæ§Â™íÈ´îÂª£Ê≥õ‰ΩøÁî®ÁöÑÁï∂‰ªäÊôÇ‰ª£ÔºåÂüπÈ§äÂ∞çÊñºÊõ∏Èù¢ÊñáÂ≠ó‰∏≠ÊâÄ‰ΩøÁî®Ë™™ÊúçÊäÄÂ∑ßÁöÑÁêÜËß£Ëá≥ÈóúÈáçË¶Å„ÄÇÈÄôÈ†ÖÁü•Ë≠òÂ∞çÊñºÊúâÊïàËæ®Âà•Ê∫ñÁ¢∫Ë≥áË®äÂíåÂÅöÂá∫ÊòéÊô∫Ê±∫Á≠ñËá≥ÈóúÈáçË¶Å„ÄÇÁÇ∫‰∫ÜÊªøË∂≥ÈÄôÈ†ÖÈúÄÊ±ÇÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÈ†ÖÂÖ®Èù¢ÁöÑÂØ¶Ë≠âÁ†îÁ©∂ÔºåÈáçÈªûÂú®ÊñºËæ®Ë≠òÈòøÊãâ‰ºØÁ§æÁæ§Â™íÈ´îÂÖßÂÆπ‰∏≠ÁöÑË™™ÊúçÊäÄÂ∑ß„ÄÇÁÇ∫ÈÅîÊàêÊ≠§ÁõÆÊ®ôÔºåÊàëÂÄëÂà©Áî®È†êÂÖàË®ìÁ∑¥ÁöÑË™ûË®ÄÊ®°Âûã (PLM)Ôºå‰∏¶ÈÅãÁî® ArAlEval Ë≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ÂÖ©È†Ö‰ªªÂãôÔºö‰∫åÂÖÉÂàÜÈ°û‰ª•Âà§Êñ∑Ë™™ÊúçÊäÄÂ∑ßÁöÑÂ≠òÂú®ËàáÂê¶Ôºå‰ª•ÂèäÂ§öÊ®ôÁ±§ÂàÜÈ°û‰ª•Ëæ®Ë≠òÊñáÂ≠ó‰∏≠ÊâÄ‰ΩøÁî®ÁöÑÁâπÂÆöÊäÄÂ∑ßÈ°ûÂûã„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÈÄèÈÅéÈÅãÁî® PLM ÁöÑÂäõÈáèÔºåÊé¢Ë®é‰∫Ü‰∏âÁ®Æ‰∏çÂêåÁöÑÂ≠∏ÁøíÊñπÊ≥ïÔºöÁâπÂæµÊèêÂèñ„ÄÅÂæÆË™øÂíåÊèêÁ§∫Â∑•Á®ãÊäÄË°ì„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊàëÂÄëÁôºÁèæÂæÆË™øÊñπÊ≥ïÂú®‰∏äËø∞Ë≥áÊñôÈõÜ‰∏äÁî¢Áîü‰∫ÜÊúÄÈ´òÁöÑÁµêÊûúÔºåÈÅîÂà∞‰∫Ü f1-micro ÂàÜÊï∏ 0.865 Âíå f1-weighted ÂàÜÊï∏ 0.861„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁöÑÂàÜÊûêÊè≠Èú≤‰∫Ü‰∏ÄÂÄãÊúâË∂£ÁöÑÁôºÁèæ„ÄÇÈõñÁÑ∂ GPT Ê®°ÂûãÁöÑÊïàËÉΩËàáÂÖ∂‰ªñÊñπÊ≥ïÁõ∏ÊØîÁõ∏Â∞çËºÉ‰ΩéÔºå‰ΩÜÊàëÂÄëËßÄÂØüÂà∞ÈÄèÈÅéÊé°Áî®Â∞ëÊ¨°Â≠∏ÁøíÊäÄË°ìÔºåÊàëÂÄëÂèØ‰ª•Â∞áÂÖ∂ÁµêÊûúÊèêÂçáÂ§öÈÅî 20%„ÄÇÈÄôÁÇ∫Ê≠§‰∏ªÈ°åÊú™‰æÜÁöÑÁ†îÁ©∂ÂíåÊé¢Á¥¢Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÊñπÂêë\footnote{Âú®Êé•ÂèóÂæåÔºåÂéüÂßãÁ¢ºÂ∞áÂú® GitHub ‰∏äÈáãÂá∫„ÄÇ}„ÄÇ</paragraph>

##### **Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in Remote Sensing Images**
2405.12875v1 by Xiaofei Yu, Yitong Li, Jie Ma

Remote sensing image change captioning (RSICC) aims at generating human-like
language to describe the semantic changes between bi-temporal remote sensing
image pairs. It provides valuable insights into environmental dynamics and land
management. Unlike conventional change captioning task, RSICC involves not only
retrieving relevant information across different modalities and generating
fluent captions, but also mitigating the impact of pixel-level differences on
terrain change localization. The pixel problem due to long time span decreases
the accuracy of generated caption. Inspired by the remarkable generative power
of diffusion model, we propose a probabilistic diffusion model for RSICC to
solve the aforementioned problems. In training process, we construct a noise
predictor conditioned on cross modal features to learn the distribution from
the real caption distribution to the standard Gaussian distribution under the
Markov chain. Meanwhile, a cross-mode fusion and a stacking self-attention
module are designed for noise predictor in the reverse process. In testing
phase, the well-trained noise predictor helps to estimate the mean value of the
distribution and generate change captions step by step. Extensive experiments
on the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC and
its individual components. The quantitative results showcase superior
performance over existing methods across both traditional and newly augmented
metrics. The code and materials will be available online at
https://github.com/Fay-Y/Diffusion-RSCC.

ÊëòË¶ÅÔºöÈÅôÊÑüÂΩ±ÂÉèËÆäÂåñÊ®ôÈ°å (RSICC) ÁöÑÁõÆÊ®ôÊòØÁî¢ÁîüÈ°û‰ºº‰∫∫È°ûÁöÑË™ûË®Ä‰æÜÊèèËø∞ÈõôÊôÇÁõ∏ÈÅôÊÑüÂΩ±ÂÉèÂ∞ç‰πãÈñìÁöÑË™ûÁæ©ËÆäÂåñ„ÄÇÂÆÉÊèê‰æõ‰∫ÜÂ∞çÁí∞Â¢ÉÂãïÊÖãÂíåÂúüÂú∞ÁÆ°ÁêÜÁöÑÂØ∂Ë≤¥Ë¶ãËß£„ÄÇËàáÂÇ≥Áµ±ÁöÑËÆäÂåñÊ®ôÈ°å‰ªªÂãô‰∏çÂêåÔºåRSICC ‰∏çÂÉÖÊ∂âÂèäË∑®‰∏çÂêåÊ®°ÊÖãÊì∑ÂèñÁõ∏ÈóúË≥áË®äÂíåÁî¢ÁîüÊµÅÂà©ÁöÑÊ®ôÈ°åÔºåÈÇÑÊ∏õËºï‰∫ÜÂÉèÁ¥†Á¥öÂ∑ÆÁï∞Â∞çÂú∞ÂΩ¢ËÆäÂåñÂÆö‰ΩçÁöÑÂΩ±Èüø„ÄÇÁî±ÊñºÊôÇÈñìË∑®Â∫¶Èï∑ËÄåÁî¢ÁîüÁöÑÂÉèÁ¥†ÂïèÈ°åÈôç‰Ωé‰∫ÜÁîüÊàêÊ®ôÈ°åÁöÑÊ∫ñÁ¢∫Â∫¶„ÄÇÂèóÂà∞Êì¥Êï£Ê®°ÂûãÂçìË∂äÁöÑÁîüÊàêËÉΩÂäõÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü RSICC ÁöÑÊ©üÁéáÊì¥Êï£Ê®°Âûã‰æÜËß£Ê±∫‰∏äËø∞ÂïèÈ°å„ÄÇÂú®Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÔºåÊàëÂÄëÂª∫Êßã‰∏ÄÂÄã‰ª•Ë∑®Ê®°ÊÖãÁâπÂæµÁÇ∫Ê¢ù‰ª∂ÁöÑÈõúË®äÈ†êÊ∏¨Âô®Ôºå‰ª•Â≠∏ÁøíÂæûÁúüÂØ¶Ê®ôÈ°åÂàÜ‰ΩàÂà∞È¶¨ÂèØÂ§´Èèà‰∏ãÁöÑÊ®ôÊ∫ñÈ´òÊñØÂàÜ‰ΩàÁöÑÂàÜ‰Ωà„ÄÇÂêåÊôÇÔºåÂú®ÂèçÂêëÈÅéÁ®ã‰∏≠ÔºåÁÇ∫ÈõúË®äÈ†êÊ∏¨Âô®Ë®≠Ë®à‰∫ÜË∑®Ê®°ÂºèËûçÂêàÂíåÂ†ÜÁñäËá™Ê≥®ÊÑèÂäõÊ®°ÁµÑ„ÄÇÂú®Ê∏¨Ë©¶ÈöéÊÆµÔºåË®ìÁ∑¥ËâØÂ•ΩÁöÑÈõúË®äÈ†êÊ∏¨Âô®ÊúâÂä©Êñº‰º∞Ë®àÂàÜ‰ΩàÁöÑÂπ≥ÂùáÂÄº‰∏¶ÈÄêÊ≠•Áî¢ÁîüËÆäÂåñÊ®ôÈ°å„ÄÇÂú® LEVIR-CC Ë≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊàëÂÄëÁöÑ Diffusion-RSCC ÂèäÂÖ∂ÂÄãÂà•ÂÖÉ‰ª∂ÁöÑÊúâÊïàÊÄß„ÄÇÂÆöÈáèÁµêÊûúÂ±ïÁ§∫‰∫ÜÂú®ÂÇ≥Áµ±ÂíåÊñ∞Êì¥ÂÖÖÁöÑÊåáÊ®ô‰∏äÂÑ™ÊñºÁèæÊúâÊñπÊ≥ïÁöÑÂá∫Ëâ≤ÊïàËÉΩ„ÄÇÁ®ãÂºèÁ¢ºÂíåÊùêÊñôÂ∞áÂú® https://github.com/Fay-Y/Diffusion-RSCC ‰∏äÁ∑ö‰∏äÊèê‰æõ„ÄÇ

##### **Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics**
2405.12868v1 by Liming Wu, Zhichao Hou, Jirui Yuan, Yu Rong, Wenbing Huang

Learning to represent and simulate the dynamics of physical systems is a
crucial yet challenging task. Existing equivariant Graph Neural Network (GNN)
based methods have encapsulated the symmetry of physics, \emph{e.g.},
translations, rotations, etc, leading to better generalization ability.
Nevertheless, their frame-to-frame formulation of the task overlooks the
non-Markov property mainly incurred by unobserved dynamics in the environment.
In this paper, we reformulate dynamics simulation as a spatio-temporal
prediction task, by employing the trajectory in the past period to recover the
Non-Markovian interactions. We propose Equivariant Spatio-Temporal Attentive
Graph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to
fulfill our purpose. At its core, we design a novel Equivariant Discrete
Fourier Transform (EDFT) to extract periodic patterns from the history frames,
and then construct an Equivariant Spatial Module (ESM) to accomplish spatial
message passing, and an Equivariant Temporal Module (ETM) with the forward
attention and equivariant pooling mechanisms to aggregate temporal message. We
evaluate our model on three real datasets corresponding to the molecular-,
protein- and macro-level. Experimental results verify the effectiveness of
ESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.

ÊëòË¶ÅÔºöÂ≠∏ÁøíË°®Á§∫ÂíåÊ®°Êì¨Áâ©ÁêÜÁ≥ªÁµ±ÁöÑÂãïÊÖãÊòØ‰∏ÄÂÄãËá≥ÈóúÈáçË¶Å‰∏îÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰ªªÂãô„ÄÇÁèæÊúâÁöÑÁ≠âËÆäÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Âü∫ÊñºÂ∞ÅË£ùÁâ©ÁêÜÂ∞çÁ®±ÊÄßÁöÑÊñπÊ≥ïÔºå‰æãÂ¶ÇÂπ≥Áßª„ÄÅÊóãËΩâÁ≠âÔºåÂæûËÄåÊèêÈ´ò‰∫ÜÊ≥õÂåñËÉΩÂäõ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂÆÉÂÄëÈÄêÂπÄÂà∂ÂÆö‰ªªÂãôÁöÑÊñπÂºèÂøΩË¶ñ‰∫Ü‰∏ªË¶ÅÁî±Áí∞Â¢É‰∏≠Êú™ËßÄÂØüÂà∞ÁöÑÂãïÊÖãÂ∞éËá¥ÁöÑÈùûÈ¶¨ÂèØÂ§´ÊÄßË≥™„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂ∞áÂãïÊÖãÊ®°Êì¨ÈáçÊñ∞Ë°®Ëø∞ÁÇ∫ÊôÇÁ©∫È†êÊ∏¨‰ªªÂãôÔºåÈÄöÈÅéÊé°Áî®ÈÅéÂéª‰∏ÄÊÆµÊôÇÈñìÁöÑËªåË∑°‰æÜÊÅ¢Âæ©ÈùûÈ¶¨ÂèØÂ§´‰∫§‰∫í‰ΩúÁî®„ÄÇÊàëÂÄëÊèêÂá∫‰∫ÜÁ≠âËÆäÊôÇÁ©∫Ê≥®ÊÑèÂäõÂúñÁ•ûÁ∂ìÁ∂≤Ë∑Ø (ESTAG)ÔºåÈÄôÊòØÊôÇÁ©∫ GNN ÁöÑÁ≠âËÆäÁâàÊú¨Ôºå‰ª•ÂØ¶ÁèæÊàëÂÄëÁöÑÁõÆÁöÑ„ÄÇÂú®ÂÆÉÁöÑÊ†∏ÂøÉÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÁ≠âËÆäÈõ¢Êï£ÂÇÖÁ´ãËëâËÆäÊèõ (EDFT) ‰æÜÂæûÊ≠∑Âè≤ÂπÄ‰∏≠ÊèêÂèñÈÄ±ÊúüÊÄßÊ®°ÂºèÔºåÁÑ∂ÂæåÊßãÈÄ†‰∏ÄÂÄãÁ≠âËÆäÁ©∫ÈñìÊ®°ÁµÑ (ESM) ‰æÜÂÆåÊàêÁ©∫ÈñìË®äÊÅØÂÇ≥ÈÅûÔºå‰ª•Âèä‰∏ÄÂÄãÂÖ∑ÊúâÂâçÈ•ãÊ≥®ÊÑèÂäõÂíåÁ≠âËÆäÊ±†ÂåñÊ©üÂà∂ÁöÑÁ≠âËÆäÊôÇÈñìÊ®°ÁµÑ (ETM) ‰æÜÂåØÁ∏ΩÊôÇÈñìË®äÊÅØ„ÄÇÊàëÂÄëÂú®Â∞çÊáâÊñºÂàÜÂ≠ê„ÄÅËõãÁôΩË≥™ÂíåÂ∑®ËßÄÂ±§Á¥öÁöÑ‰∏âÂÄãÁúüÂØ¶Ë≥áÊñôÈõÜ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÊ®°Âûã„ÄÇÂØ¶È©óÁµêÊûúÈ©óË≠â‰∫Ü ESTAG ËàáÂÖ∏ÂûãÁöÑÊôÇÁ©∫ GNN ÂíåÁ≠âËÆä GNN Áõ∏ÊØîÁöÑÊúâÊïàÊÄß„ÄÇ

##### **LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language**
2405.12856v1 by James Requeima, John Bronskill, Dami Choi, Richard E. Turner, David Duvenaud

Machine learning practitioners often face significant challenges in formally
integrating their prior knowledge and beliefs into predictive models, limiting
the potential for nuanced and context-aware analyses. Moreover, the expertise
needed to integrate this prior knowledge into probabilistic modeling typically
limits the application of these models to specialists. Our goal is to build a
regression model that can process numerical data and make probabilistic
predictions at arbitrary locations, guided by natural language text which
describes a user's prior knowledge. Large Language Models (LLMs) provide a
useful starting point for designing such a tool since they 1) provide an
interface where users can incorporate expert insights in natural language and
2) provide an opportunity for leveraging latent problem-relevant knowledge
encoded in LLMs that users may not have themselves. We start by exploring
strategies for eliciting explicit, coherent numerical predictive distributions
from LLMs. We examine these joint predictive distributions, which we call LLM
Processes, over arbitrarily-many quantities in settings such as forecasting,
multi-dimensional regression, black-box optimization, and image modeling. We
investigate the practical details of prompting to elicit coherent predictive
distributions, and demonstrate their effectiveness at regression. Finally, we
demonstrate the ability to usefully incorporate text into numerical
predictions, improving predictive performance and giving quantitative structure
that reflects qualitative descriptions. This lets us begin to explore the rich,
grounded hypothesis space that LLMs implicitly encode.

ÊëòË¶ÅÔºöÊ©üÂô®Â≠∏ÁøíÂæûÊ•≠‰∫∫Âì°Âú®Ê≠£ÂºèÂ∞áÂÖàÈ©óÁü•Ë≠òÂíå‰ø°ÂøµÊï¥ÂêàÂà∞È†êÊ∏¨Ê®°Âûã‰∏≠ÊôÇÔºåÁ∂ìÂ∏∏ÊúÉÈÅáÂà∞ÈáçÂ§ßÁöÑÊåëÊà∞ÔºåÈÄôÊúÉÈôêÂà∂Á¥∞Á∑ªÂÖ•ÂæÆ‰∏îÂÖ∑ÂÇôÊÉÖÂ¢ÉÊÑüÁü•ÁöÑÂàÜÊûêÊΩõÂäõ„ÄÇÊ≠§Â§ñÔºåÂ∞áÊ≠§ÂÖàÈ©óÁü•Ë≠òÊï¥ÂêàÂà∞Ê©üÁéáÊ®°Âûã‰∏≠ÊâÄÈúÄÁöÑÂ∞àÊ•≠Áü•Ë≠òÈÄöÂ∏∏ÊúÉÂ∞áÈÄô‰∫õÊ®°ÂûãÁöÑÊáâÁî®ÈôêÂà∂Âú®Â∞àÂÆ∂Ë∫´‰∏ä„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÂª∫Á´ã‰∏ÄÂÄãÂõûÊ≠∏Ê®°ÂûãÔºåË©≤Ê®°ÂûãÂèØ‰ª•ËôïÁêÜÊï∏ÂÄºË≥áÊñô‰∏¶Âú®‰ªªÊÑè‰ΩçÁΩÆÈÄ≤Ë°åÊ©üÁéáÈ†êÊ∏¨Ôºå‰∏¶Áî±ÊèèËø∞‰ΩøÁî®ËÄÖÂÖàÈ©óÁü•Ë≠òÁöÑËá™ÁÑ∂Ë™ûË®ÄÊñáÂ≠óÂä†‰ª•ÂºïÂ∞é„ÄÇÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁÇ∫Ë®≠Ë®àÊ≠§È°ûÂ∑•ÂÖ∑Êèê‰æõ‰∫ÜÊúâÁî®ÁöÑËµ∑ÈªûÔºåÂõ†ÁÇ∫ÂÆÉÂÄë 1) Êèê‰æõ‰∫Ü‰∏ÄÂÄã‰ªãÈù¢Ôºå‰ΩøÁî®ËÄÖÂèØ‰ª•Âú®ÂÖ∂‰∏≠‰ª•Ëá™ÁÑ∂Ë™ûË®ÄÁ¥çÂÖ•Â∞àÂÆ∂Ë¶ãËß£Ôºå‰ª•Âèä 2) Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊ©üÊúÉÔºåÂèØ‰ª•Âà©Áî®Á∑®Á¢ºÂú® LLM ‰∏≠ÁöÑÊΩõÂú®ÂïèÈ°åÁõ∏ÈóúÁü•Ë≠òÔºåËÄå‰ΩøÁî®ËÄÖÂèØËÉΩËá™Â∑±Ê≤íÊúâÈÄô‰∫õÁü•Ë≠ò„ÄÇÊàëÂÄëÈ¶ñÂÖàÊé¢Á¥¢Âæû LLM ÂºïÂá∫ÊòéÁ¢∫„ÄÅÈÄ£Ë≤´ÁöÑÊï∏ÂÄºÈ†êÊ∏¨ÂàÜ‰ΩàÁöÑÁ≠ñÁï•„ÄÇÊàëÂÄëÊ™¢È©óÈÄô‰∫õËÅØÂêàÈ†êÊ∏¨ÂàÜ‰ΩàÔºåÊàëÂÄëÁ®±‰πãÁÇ∫ LLM Á®ãÂ∫èÔºåÂÆÉÂÄëÂú®È†êÊ∏¨„ÄÅÂ§öÁ∂≠ÂõûÊ≠∏„ÄÅÈªëÁõíÊúÄ‰Ω≥ÂåñÂíåÂΩ±ÂÉèÂª∫Ê®°Á≠âË®≠ÂÆö‰∏≠Ê∂µËìã‰ªªÊÑèÂ§ßÈáèÁöÑÊï∏Èáè„ÄÇÊàëÂÄëË™øÊü•ÂºïÁôºÈÄ£Ë≤´È†êÊ∏¨ÂàÜ‰ΩàÁöÑÂØ¶ÈöõÊèêÁ§∫Á¥∞ÁØÄÔºå‰∏¶Â±ïÁ§∫ÂÆÉÂÄëÂú®ÂõûÊ≠∏‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÊúÄÂæåÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂ∞áÊñáÂ≠óÊúâÊïàÁ¥çÂÖ•Êï∏ÂÄºÈ†êÊ∏¨ÁöÑËÉΩÂäõÔºåÊîπÂñÑÈ†êÊ∏¨ÊïàËÉΩ‰∏¶Êèê‰æõÂèçÊò†ÂÆöÊÄßÊèèËø∞ÁöÑÈáèÂåñÁµêÊßã„ÄÇÈÄôËÆìÊàëÂÄëÈñãÂßãÊé¢Á¥¢ LLM Èö±Âê´Á∑®Á¢ºÁöÑË±êÂØå„ÄÅÁ¥ÆÂØ¶ÁöÑÂÅáË®≠Á©∫Èñì„ÄÇ

##### **Training and inference in the ReckON RSNN architecture implemented on a MPSoC**
2405.12849v1 by Alejandro Linares-Barranco, Luciano Prono, Robert Lengenstein, Giacomo Indiveri, Charlotte Frenkel

With the rise of artificial intelligence, biological neuron models are being
used to implement neural networks that can learn certain tasks after a training
phase. One type of such networks are spiking neural networks (SNNs) that rely
on a simplified model for biological neurons, the Integrate and Fire neuron.
Several accelerators have emerged to implement SNNs with this kind of neuron.
The ReckON system is one of these that allows both the training and execution
of a recurrent SNN. The ReckON architecture, implemented on a custom ASIC, can
be fully described using a hardware description language. In this work, we
adapt the Verilog description to implement it on a Xilinx Multiprocessor System
on Chip system (MPSoC). We present the circuits required for the efficient
operation of the system, and a Python framework to use it on the Pynq ZU
platform. We validate the architecture and implementation in two different
scenarios, and show how the simulated accuracy is preserved with a peak
performance of 3.8M events processed per second.

ÊëòË¶ÅÔºöÈö®Ëëó‰∫∫Â∑•Êô∫ÊÖßÁöÑÂ¥õËµ∑ÔºåÁîüÁâ©Á•ûÁ∂ìÂÖÉÊ®°ÂûãË¢´Áî®ÊñºÂØ¶ÁèæÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºåËÆìÂÖ∂Âú®Ë®ìÁ∑¥ÈöéÊÆµÂæåËÉΩÂ§†Â≠∏ÁøíÁâπÂÆö‰ªªÂãô„ÄÇÂÖ∂‰∏≠‰∏ÄÁ®ÆÊ≠§È°ûÁ∂≤Ë∑ØÊòØÂ∞ñÂ≥∞Á•ûÁ∂ìÁ∂≤Ë∑Ø (SNN)ÔºåÂÆÉ‰æùË≥¥ÊñºÁîüÁâ©Á•ûÁ∂ìÂÖÉÁöÑÁ∞°ÂåñÊ®°ÂûãÔºåÂç≥Á©çÂàÜËàáÁôºÂ∞ÑÁ•ûÁ∂ìÂÖÉ„ÄÇÂ∑≤Á∂ìÂá∫Áèæ‰∫ÜÂ§öÁ®ÆÂä†ÈÄüÂô®ÔºåÁî®Êñº‰ΩøÁî®ÈÄôÁ®ÆÁ•ûÁ∂ìÂÖÉ‰æÜÂØ¶Áèæ SNN„ÄÇReckON Á≥ªÁµ±Â∞±ÊòØÂÖ∂‰∏≠‰πã‰∏ÄÔºåÂÆÉÂÖÅË®±Ë®ìÁ∑¥ÂíåÂü∑Ë°åÈÅûËø¥ SNN„ÄÇReckON Êû∂ÊßãÂØ¶‰ΩúÊñºÂÆ¢Ë£ΩÂåñ ASIC ‰∏äÔºåÂèØ‰ª•‰ΩøÁî®Á°¨È´îÊèèËø∞Ë™ûË®ÄÂÆåÊï¥ÊèèËø∞„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëË™øÊï¥‰∫Ü Verilog ÊèèËø∞Ôºå‰ª•‰æøÂú® Xilinx Â§öËôïÁêÜÂô®Á≥ªÁµ±ÂñÆÊô∂ÁâáÁ≥ªÁµ± (MPSoC) ‰∏äÂØ¶ÁèæÂÆÉ„ÄÇÊàëÂÄëÂ±ïÁ§∫‰∫ÜÁ≥ªÁµ±ÊúâÊïàÈÅã‰ΩúÊâÄÈúÄÁöÑÈõªË∑ØÔºå‰ª•Âèä‰∏ÄÂÄã Python Ê°ÜÊû∂Ôºå‰ª•‰æøÂú® Pynq ZU Âπ≥Âè∞‰∏ä‰ΩøÁî®ÂÆÉ„ÄÇÊàëÂÄëÂú®ÂÖ©Á®Æ‰∏çÂêåÁöÑÊÉÖÂ¢É‰∏≠È©óË≠â‰∫ÜÊû∂ÊßãÂíåÂØ¶‰ΩúÔºå‰∏¶Â±ïÁ§∫‰∫ÜÊ®°Êì¨Ê∫ñÁ¢∫Â∫¶Â¶Ç‰Ωï‰ª•ÊØèÁßíËôïÁêÜ 380 Ëê¨ÂÄã‰∫ã‰ª∂ÁöÑÂ≥∞ÂÄºÊïàËÉΩ‰øùÁïô‰∏ã‰æÜ„ÄÇ

##### **Large Language Models Meet NLP: A Survey**
2405.12819v1 by Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, Philip S. Yu

While large language models (LLMs) like ChatGPT have shown impressive
capabilities in Natural Language Processing (NLP) tasks, a systematic
investigation of their potential in this field remains largely unexplored. This
study aims to address this gap by exploring the following questions: (1) How
are LLMs currently applied to NLP tasks in the literature? (2) Have traditional
NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for
NLP? To answer these questions, we take the first step to provide a
comprehensive overview of LLMs in NLP. Specifically, we first introduce a
unified taxonomy including (1) parameter-frozen application and (2)
parameter-tuning application to offer a unified perspective for understanding
the current progress of LLMs in NLP. Furthermore, we summarize the new
frontiers and the associated challenges, aiming to inspire further
groundbreaking advancements. We hope this work offers valuable insights into
the {potential and limitations} of LLMs in NLP, while also serving as a
practical guide for building effective LLMs in NLP.

ÊëòË¶ÅÔºöÂÑòÁÆ°Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â¶Ç ChatGPT Â∑≤Âú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ‰ªªÂãô‰∏≠Â±ïÁèæÂá∫‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºå‰ΩÜÂ∞çÂÖ∂Âú®Ë©≤È†òÂüüÁöÑÊΩõÂäõÈÄ≤Ë°åÁ≥ªÁµ±ÊÄßË™øÊü•‰ªçÊú™Âª£Ê≥õÊé¢Ë®é„ÄÇÊú¨Á†îÁ©∂Êó®Âú®ÈÄèÈÅéÊé¢Ë®é‰ª•‰∏ãÂïèÈ°å‰æÜËß£Ê±∫Ê≠§‰∏ÄÂ∑ÆË∑ùÔºö(1) LLM ÁõÆÂâçÂ¶Ç‰ΩïÂú®ÊñáÁçª‰∏≠ÊáâÁî®Êñº NLP ‰ªªÂãôÔºü(2) ÂÇ≥Áµ±ÁöÑ NLP ‰ªªÂãôÊòØÂê¶Â∑≤ÈÄèÈÅé LLM Ëß£Ê±∫Ôºü(3) LLM ÁöÑ NLP Êú™‰æÜÊòØ‰ªÄÈ∫ºÔºüÁÇ∫‰∫ÜÂõûÁ≠îÈÄô‰∫õÂïèÈ°åÔºåÊàëÂÄëÊé°ÂèñÁ¨¨‰∏ÄÊ≠•ÔºåÊèê‰æõ LLM Âú® NLP ‰∏≠ÁöÑÂÖ®Èù¢Ê¶ÇËø∞„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖà‰ªãÁ¥π‰∏ÄÂÄãÁµ±‰∏ÄÁöÑÂàÜÈ°ûÊ≥ïÔºåÂåÖÊã¨ (1) ÂèÉÊï∏ÂáçÁµêÊáâÁî®Âíå (2) ÂèÉÊï∏Ë™øÊï¥ÊáâÁî®Ôºå‰ª•Êèê‰æõ‰∏ÄÂÄãÁµ±‰∏ÄÁöÑËßÄÈªû‰æÜ‰∫ÜËß£ LLM Âú® NLP ‰∏≠ÁöÑÁï∂ÂâçÈÄ≤Â∫¶„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁ∏ΩÁµê‰∫ÜÊñ∞ÁöÑÈ†òÂüüÂíåÁõ∏ÈóúÊåëÊà∞ÔºåÊó®Âú®ÊøÄÂãµÈÄ≤‰∏ÄÊ≠•ÁöÑÁ™ÅÁ†¥ÊÄßÈÄ≤Â±ï„ÄÇÊàëÂÄëÂ∏åÊúõÈÄôÈ†ÖÂ∑•‰ΩúËÉΩÊèê‰æõÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰∫ÜËß£ LLM Âú® NLP ‰∏≠ÁöÑ{ÊΩõÂäõËàáÈôêÂà∂}ÔºåÂêåÊôÇ‰πü‰ΩúÁÇ∫Âú® NLP ‰∏≠Âª∫ÊßãÊúâÊïà LLM ÁöÑÂØ¶Áî®ÊåáÂçó„ÄÇ

##### **FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information**
2405.12807v1 by Dongseong Hwang

This paper establishes a mathematical foundation for the Adam optimizer,
elucidating its connection to natural gradient descent through Riemannian and
information geometry. We rigorously analyze the diagonal empirical Fisher
information matrix (FIM) in Adam, clarifying all detailed approximations and
advocating for the use of log probability functions as loss, which should be
based on discrete distributions, due to the limitations of empirical FIM. Our
analysis uncovers flaws in the original Adam algorithm, leading to proposed
corrections such as enhanced momentum calculations, adjusted bias corrections,
and gradient clipping. We refine the weight decay term based on our theoretical
framework. Our modified algorithm, Fisher Adam (FAdam), demonstrates superior
performance across diverse domains including LLM, ASR, and VQ-VAE, achieving
state-of-the-art results in ASR.

ÊëòË¶ÅÔºöÈÄôÁØáË´ñÊñáÁÇ∫ Adam ÊúÄ‰Ω≥ÂåñÂô®Âª∫Á´ã‰∫Ü‰∏ÄÂÄãÊï∏Â≠∏Âü∫Á§éÔºå
Èó°Êòé‰∫ÜÂÆÉËàáÈÄèÈÅéÈªéÊõºÂíåË≥áË®äÂπæ‰ΩïÁöÑËá™ÁÑ∂Ê¢ØÂ∫¶‰∏ãÈôçÁöÑÈóúËÅØ„ÄÇÊàëÂÄëÂö¥Ë¨πÂú∞ÂàÜÊûê‰∫Ü Adam ‰∏≠ÁöÑÂ∞çËßíÁ∂ìÈ©ó Fisher Ë≥áË®äÁü©Èô£ (FIM)ÔºåÈáêÊ∏ÖÊâÄÊúâË©≥Á¥∞ÁöÑËøë‰ººÔºå‰∏¶ÊèêÂÄ°‰ΩøÁî®Â∞çÊï∏Ê©üÁéáÂáΩÊï∏‰ΩúÁÇ∫ÊêçÂ§±ÔºåÈÄôÊáâÂü∫ÊñºÈõ¢Êï£ÂàÜ‰ΩàÔºåÂõ†ÁÇ∫Á∂ìÈ©ó FIM ÊúâÂÖ∂ÈôêÂà∂„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÊè≠Èú≤‰∫ÜÂéüÂßã Adam ÊºîÁÆóÊ≥ïÁöÑÁº∫Èô∑ÔºåÂ∞éËá¥Âª∫Ë≠∞ÈÄ≤Ë°å‰øÆÊ≠£Ôºå‰æãÂ¶ÇÂ¢ûÂº∑ÂãïÈáèË®àÁÆó„ÄÅË™øÊï¥ÂÅèÂ∑Æ‰øÆÊ≠£ÂíåÊ¢ØÂ∫¶Ë£ÅÂâ™„ÄÇÊàëÂÄëÊ†πÊìöÊàëÂÄëÁöÑÁêÜË´ñÊû∂ÊßãÊîπÂñÑ‰∫ÜÊ¨äÈáçË°∞Ê∏õÈ†Ö„ÄÇÊàëÂÄëÊîπËâØÁöÑÊºîÁÆóÊ≥ï Fisher Adam (FAdam) Âú® LLM„ÄÅASR Âíå VQ-VAE Á≠â‰∏çÂêåÈ†òÂüüÂ±ïÁ§∫‰∫ÜÂçìË∂äÁöÑÊïàËÉΩÔºåÂú® ASR ‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÁµêÊûú„ÄÇ

##### **What Have We Achieved on Non-autoregressive Translation?**
2405.12788v1 by Yafu Li, Huajian Zhang, Jianhao Yan, Yongjing Yin, Yue Zhang

Recent advances have made non-autoregressive (NAT) translation comparable to
autoregressive methods (AT). However, their evaluation using BLEU has been
shown to weakly correlate with human annotations. Limited research compares
non-autoregressive translation and autoregressive translation comprehensively,
leaving uncertainty about the true proximity of NAT to AT. To address this gap,
we systematically evaluate four representative NAT methods across various
dimensions, including human evaluation. Our empirical results demonstrate that
despite narrowing the performance gap, state-of-the-art NAT still underperforms
AT under more reliable evaluation metrics. Furthermore, we discover that
explicitly modeling dependencies is crucial for generating natural language and
generalizing to out-of-distribution sequences.

ÊëòË¶ÅÔºöÊúÄËøëÁöÑÈÄ≤Â±ï‰ΩøÈùûËá™Ëø¥Ê≠∏ (NAT) ÁøªË≠ØËàáËá™Ëø¥Ê≠∏ÊñπÊ≥ï (AT) Áõ∏Â™≤Áæé„ÄÇÁÑ∂ËÄåÔºå‰ΩøÁî® BLEU Ë©ï‰º∞ÂÆÉÂÄëÂ∑≤Ë¢´Ë≠âÊòéËàá‰∫∫È°ûË®ªÈáãÁõ∏ÈóúÊÄßËºÉÂº±„ÄÇÊúâÈôêÁöÑÁ†îÁ©∂ÂÖ®Èù¢ÊØîËºÉ‰∫ÜÈùûËá™Ëø¥Ê≠∏ÁøªË≠ØÂíåËá™Ëø¥Ê≠∏ÁøªË≠ØÔºåÂ∞ç NAT Ëàá AT ÁöÑÁúüÂØ¶Êé•ËøëÁ®ãÂ∫¶Áïô‰∏ã‰∏çÁ¢∫ÂÆöÊÄß„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÁ≥ªÁµ±ÊÄßÂú∞Ë©ï‰º∞‰∫ÜÂõõÁ®ÆÂÖ∑Êúâ‰ª£Ë°®ÊÄßÁöÑ NAT ÊñπÊ≥ïÔºåÂåÖÊã¨‰∫∫È°ûË©ï‰º∞Âú®ÂÖßÁöÑÂêÑÁ®ÆÈù¢Âêë„ÄÇÊàëÂÄëÁöÑÂØ¶Ë≠âÁµêÊûúË°®ÊòéÔºåÂÑòÁÆ°Á∏ÆÂ∞è‰∫ÜÊïàËÉΩÂ∑ÆË∑ùÔºå‰ΩÜÊúÄÂÖàÈÄ≤ÁöÑ NAT Âú®Êõ¥ÂèØÈù†ÁöÑË©ï‰º∞ÊåáÊ®ô‰∏ã‰ªçÁÑ∂Ë°®Áèæ‰∏çÂ¶Ç AT„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁôºÁèæÊòéÁ¢∫Âª∫Ê®°‰æùË≥¥ÊÄßÂ∞çÊñºÁîüÊàêËá™ÁÑ∂Ë™ûË®ÄÂíåÊ¶ÇÊã¨Âà∞ÂàÜÂ∏ÉÂ§ñÂ∫èÂàóËá≥ÈóúÈáçË¶Å„ÄÇ

##### **Transformer in Touch: A Survey**
2405.12779v1 by Jing Gao, Ning Cheng, Bin Fang, Wenjuan Han

The Transformer model, initially achieving significant success in the field
of natural language processing, has recently shown great potential in the
application of tactile perception. This review aims to comprehensively outline
the application and development of Transformers in tactile technology. We first
introduce the two fundamental concepts behind the success of the Transformer:
the self-attention mechanism and large-scale pre-training. Then, we delve into
the application of Transformers in various tactile tasks, including but not
limited to object recognition, cross-modal generation, and object manipulation,
offering a concise summary of the core methodologies, performance benchmarks,
and design highlights. Finally, we suggest potential areas for further research
and future work, aiming to generate more interest within the community, tackle
existing challenges, and encourage the use of Transformer models in the tactile
field.

ÊëòË¶ÅÔºöTransformer Ê®°ÂûãÊúÄÂàùÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÈ†òÂüüÂèñÂæó‰∫ÜÈ°ØËëóÁöÑÊàêÂäüÔºåÊúÄËøëÂú®Ëß∏Ë¶∫ÊÑüÁü•ÁöÑÊáâÁî®‰∏≠‰πüÂ±ïÁèæ‰∫ÜÂ∑®Â§ßÁöÑÊΩõÂäõ„ÄÇÊú¨Á∂úËø∞Êó®Âú®ÂÖ®Èù¢Ê¶ÇËø∞ Transformer Âú®Ëß∏Ë¶∫ÊäÄË°ì‰∏≠ÁöÑÊáâÁî®ÂíåÁôºÂ±ï„ÄÇÊàëÂÄëÈ¶ñÂÖà‰ªãÁ¥π Transformer ÊàêÂäüËÉåÂæåÁöÑÂÖ©ÂÄãÂü∫Êú¨Ê¶ÇÂøµÔºöËá™Ê≥®ÊÑèÂäõÊ©üÂà∂ÂíåÂ§ßË¶èÊ®°È†êË®ìÁ∑¥„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®é Transformer Âú®ÂêÑÁ®ÆËß∏Ë¶∫‰ªªÂãô‰∏≠ÁöÑÊáâÁî®ÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôêÊñºÁâ©È´îË≠òÂà•„ÄÅË∑®Ê®°ÊÖãÁîüÊàêÂíåÁâ©È´îÊìç‰ΩúÔºåÁ∞°Ë¶ÅÁ∏ΩÁµê‰∫ÜÊ†∏ÂøÉÊñπÊ≥ï„ÄÅÊÄßËÉΩÂü∫Ê∫ñÂíåË®≠Ë®à‰∫ÆÈªû„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫‰∫ÜÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂ÂíåÊú™‰æÜÂ∑•‰ΩúÁöÑÊΩõÂú®È†òÂüüÔºåÊó®Âú®ÊøÄÁôºÁ§æÂçÄÂÖßÈÉ®Êõ¥Â§öÁöÑËààË∂£ÔºåÊáâÂ∞çÁèæÊúâÊåëÊà∞Ôºå‰∏¶ÈºìÂãµÂú®Ëß∏Ë¶∫È†òÂüü‰ΩøÁî® Transformer Ê®°Âûã„ÄÇ

##### **Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances**
2405.12775v1 by Hanlei Zhang, Hua Xu, Fei Long, Xin Wang, Kai Gao

Discovering the semantics of multimodal utterances is essential for
understanding human language and enhancing human-machine interactions. Existing
methods manifest limitations in leveraging nonverbal information for discerning
complex semantics in unsupervised scenarios. This paper introduces a novel
unsupervised multimodal clustering method (UMC), making a pioneering
contribution to this field. UMC introduces a unique approach to constructing
augmentation views for multimodal data, which are then used to perform
pre-training to establish well-initialized representations for subsequent
clustering. An innovative strategy is proposed to dynamically select
high-quality samples as guidance for representation learning, gauged by the
density of each sample's nearest neighbors. Besides, it is equipped to
automatically determine the optimal value for the top-$K$ parameter in each
cluster to refine sample selection. Finally, both high- and low-quality samples
are used to learn representations conducive to effective clustering. We build
baselines on benchmark multimodal intent and dialogue act datasets. UMC shows
remarkable improvements of 2-6\% scores in clustering metrics over
state-of-the-art methods, marking the first successful endeavor in this domain.
The complete code and data are available at https://github.com/thuiar/UMC.

ÊëòË¶ÅÔºöÁôºÁèæÂ§öÊ®°ÊÖãË™ûÂè•ÁöÑË™ûÁæ©Â∞çÊñºÁêÜËß£‰∫∫È°ûË™ûË®ÄÂíåÂ¢ûÂº∑‰∫∫Ê©ü‰∫íÂãïËá≥ÈóúÈáçË¶Å„ÄÇÁèæÊúâÊñπÊ≥ïÂú®Âà©Áî®ÈùûË™ûË®ÄË≥áË®ä‰æÜËæ®Âà•ÁÑ°Áõ£Áù£Â†¥ÊôØ‰∏≠ÁöÑË§áÈõúË™ûÁæ©ÊôÇÔºåË°®ÁèæÂá∫ÂÖ∂ÈôêÂà∂„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÁÑ°Áõ£Áù£Â§öÊ®°ÊÖãËÅöÈ°ûÊñπÊ≥ï (UMC)ÔºåÁÇ∫Ê≠§È†òÂüüÂÅöÂá∫‰∫ÜÈñãÂâµÊÄßÁöÑË≤¢Áçª„ÄÇUMC ‰ªãÁ¥π‰∫Ü‰∏ÄÁ®ÆÊßãÂª∫Â§öÊ®°ÊÖãË≥áÊñôÊì¥ÂÖÖÊ™¢Ë¶ñÁöÑÁç®ÁâπÊñπÊ≥ïÔºåÁÑ∂ÂæåÁî®ÊñºÂü∑Ë°åÈ†êË®ìÁ∑¥Ôºå‰ª•Âª∫Á´ãÂæåÁ∫åËÅöÈ°ûÁöÑËâØÂ•ΩÂàùÂßãÂåñË°®Á§∫„ÄÇÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑÁ≠ñÁï•ÔºåÊ†πÊìöÊØèÂÄãÊ®£Êú¨ÊúÄËøëÈÑ∞Â±ÖÁöÑÂØÜÂ∫¶ÔºåÂãïÊÖãÈÅ∏ÊìáÈ´òÂìÅË≥™Ê®£Êú¨‰ΩúÁÇ∫Ë°®Á§∫Â≠∏ÁøíÁöÑÊåáÂ∞é„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÖ∑ÂÇôËá™ÂãïÁ¢∫ÂÆöÊØèÂÄãÁæ§ÈõÜ‰∏≠È†ÇÈÉ®-$K$ ÂèÉÊï∏ÁöÑÊúÄ‰Ω≥ÂÄºÁöÑË£ùÂÇôÔºå‰ª•ÂÑ™ÂåñÊ®£Êú¨ÈÅ∏Êìá„ÄÇÊúÄÂæåÔºåÈ´òÂìÅË≥™Âíå‰ΩéÂìÅË≥™Ê®£Êú¨ÈÉΩÁî®ÊñºÂ≠∏ÁøíÊúâÂä©ÊñºÊúâÊïàËÅöÈ°ûÁöÑË°®Á§∫„ÄÇÊàëÂÄëÂú®Âü∫Ê∫ñÂ§öÊ®°ÊÖãÊÑèÂúñÂíåÂ∞çË©±Ë°åÁÇ∫Ë≥áÊñôÈõÜ‰∏äÂª∫Á´ãÂü∫Ê∫ñ„ÄÇUMC Âú®ËÅöÈ°ûÊåáÊ®ô‰∏äÈ°ØÁ§∫Âá∫ÊØîÊúÄÂÖàÈÄ≤ÁöÑÊñπÊ≥ïÈ´òÂá∫ 2-6% ÁöÑÈ°ØËëóÊîπÈÄ≤ÔºåÊ®ôË™åËëóÂú®ÈÄôÂÄãÈ†òÂüüÁöÑÈ¶ñÊ¨°ÊàêÂäüÂòóË©¶„ÄÇÂÆåÊï¥ÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú® https://github.com/thuiar/UMC ÂèñÂæó„ÄÇ

##### **Progress Measures for Grokking on Real-world Datasets**
2405.12755v1 by Satvik Golechha

Grokking, a phenomenon where machine learning models generalize long after
overfitting, has been primarily observed and studied in algorithmic tasks. This
paper explores grokking in real-world datasets using deep neural networks for
classification under the cross-entropy loss. We challenge the prevalent
hypothesis that the $L_2$ norm of weights is the primary cause of grokking by
demonstrating that grokking can occur outside the expected range of weight
norms. To better understand grokking, we introduce three new progress measures:
activation sparsity, absolute weight entropy, and approximate local circuit
complexity. These measures are conceptually related to generalization and
demonstrate a stronger correlation with grokking in real-world datasets
compared to weight norms. Our findings suggest that while weight norms might
usually correlate with grokking and our progress measures, they are not
causative, and our proposed measures provide a better understanding of the
dynamics of grokking.

ÊëòË¶ÅÔºöÊ†ºÁæÖÈáëÁèæË±°Ôºå‰∏ÄÁ®ÆÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂú®ÈÅéÂ∫¶Êì¨ÂêàÂæå‰ªçËÉΩÊ≥õÂåñÁöÑÁèæË±°Ôºå‰∏ªË¶ÅÂú®ÊºîÁÆóÊ≥ï‰ªªÂãô‰∏≠ËßÄÂØüÂíåÁ†îÁ©∂„ÄÇÊú¨Êñá‰ΩøÁî®Ê∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑ØÂú®‰∫§ÂèâÁÜµÊêçÂ§±‰∏ãÈÄ≤Ë°åÂàÜÈ°ûÔºåÊé¢Ë®éÊ†ºÁæÖÈáëÂú®ÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏≠ÁöÑË°®Áèæ„ÄÇÊàëÂÄëÊåëÊà∞‰∫ÜÊµÅË°åÁöÑÂÅáË®≠ÔºåÂç≥Ê¨äÈáçÁöÑ $L_2$ ÁØÑÊï∏ÊòØÊ†ºÁæÖÈáëÁöÑ‰∏ªË¶ÅÂéüÂõ†ÔºåË≠âÊòé‰∫ÜÊ†ºÁæÖÈáëÂèØ‰ª•Âú®È†êÊúüÁöÑÊ¨äÈáçÁØÑÊï∏ÁØÑÂúç‰πãÂ§ñÁôºÁîü„ÄÇÁÇ∫‰∫ÜÊõ¥Â•ΩÂú∞ÁêÜËß£Ê†ºÁæÖÈáëÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏âÈ†ÖÊñ∞ÁöÑÈÄ≤Â∫¶Ë°°ÈáèÊ®ôÊ∫ñÔºöÊøÄÊ¥ªÁ®ÄÁñèÊÄß„ÄÅÁµïÂ∞çÊ¨äÈáçÁÜµÂíåËøë‰ººÂ±ÄÈÉ®ÈõªË∑ØË§áÈõúÂ∫¶„ÄÇÈÄô‰∫õË°°ÈáèÊ®ôÊ∫ñÂú®Ê¶ÇÂøµ‰∏äËàáÊ≥õÂåñÁõ∏ÈóúÔºå‰∏¶Ë≠âÊòéËàáÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏≠ÁöÑÊ†ºÁæÖÈáëÁõ∏ÊØîÔºåËàáÊ¨äÈáçÁØÑÊï∏ÊúâÊõ¥Âº∑ÁöÑÁõ∏ÈóúÊÄß„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºåÈõñÁÑ∂Ê¨äÈáçÁØÑÊï∏ÈÄöÂ∏∏ÂèØËÉΩËàáÊ†ºÁæÖÈáëÂíåÊàëÂÄëÁöÑÈÄ≤Â∫¶Ë°°ÈáèÊ®ôÊ∫ñÁõ∏ÈóúÔºå‰ΩÜÂÆÉÂÄë‰∏¶ÈùûÂõ†ÊûúÈóú‰øÇÔºåËÄåÊàëÂÄëÊèêÂá∫ÁöÑË°°ÈáèÊ®ôÊ∫ñÂèØ‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Ê†ºÁæÖÈáëÁöÑÂãïÊÖã„ÄÇ

##### **Neural Operator for Accelerating Coronal Magnetic Field Model**
2405.12754v1 by Yutao Du, Qin Li, Raghav Gnanasambandam, Mengnan Du, Haimin Wang, Bo Shen

Studying the sun's outer atmosphere is challenging due to its complex
magnetic fields impacting solar activities. Magnetohydrodynamics (MHD)
simulations help model these interactions but are extremely time-consuming
(usually on a scale of days). Our research applies the Fourier Neural Operator
(FNO) to accelerate the coronal magnetic field modeling, specifically, the
Bifrost MHD model. We apply Tensorized FNO (TFNO) to generate solutions from
partial differential equations (PDEs) over a 3D domain efficiently. TFNO's
performance is compared with other deep learning methods, highlighting its
accuracy and scalability. Physics analysis confirms that TFNO is reliable and
capable of accelerating MHD simulations with high precision. This advancement
improves efficiency in data handling, enhances predictive capabilities, and
provides a better understanding of magnetic topologies.

ÊëòË¶ÅÔºöÁî±ÊñºÂ§™ÈôΩÂ§ñÂ§ßÊ∞£Â±§Ë§áÈõúÁöÑÁ£ÅÂ†¥ÊúÉÂΩ±ÈüøÂ§™ÈôΩÊ¥ªÂãïÔºåÂõ†Ê≠§Á†îÁ©∂Â§™ÈôΩÂ§ñÂ§ßÊ∞£Â±§ÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÁ£ÅÊµÅÈ´îÂãïÂäõÂ≠∏ (MHD) Ê®°Êì¨ÊúâÂä©ÊñºÂª∫Á´ãÈÄô‰∫õ‰∫§‰∫í‰ΩúÁî®ÁöÑÊ®°ÂûãÔºå‰ΩÜÊ•µÁÇ∫ËÄóÊôÇÔºàÈÄöÂ∏∏‰ª•Â§©ÁÇ∫ÂñÆ‰ΩçÔºâ„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂Â∞áÂÇÖÁ´ãËëâÁ•ûÁ∂ìÁÆóÂ≠ê (FNO) ÊáâÁî®ÊñºÂä†ÈÄüÊó•ÂÜïÁ£ÅÂ†¥Âª∫Ê®°ÔºåÁâπÂà•ÊòØ Bifrost MHD Ê®°Âûã„ÄÇÊàëÂÄëÊáâÁî®ÂºµÈáèÂåñ FNO (TFNO) ‰æÜÊúâÊïàÂú∞Ê†πÊìö 3D Âüü‰∏äÁöÑÂÅèÂæÆÂàÜÊñπÁ®ãÂºè (PDE) ÁîüÊàêËß£„ÄÇTFNO ÁöÑÊïàËÉΩËàáÂÖ∂‰ªñÊ∑±Â∫¶Â≠∏ÁøíÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉÔºåÁ™ÅÂá∫‰∫ÜÂÆÉÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØÊì¥ÂÖÖÊÄß„ÄÇÁâ©ÁêÜÂàÜÊûêË≠âÂØ¶ TFNO ÊòØÂèØÈù†ÁöÑÔºå‰∏¶‰∏îËÉΩÂ§†‰ª•È´òÁ≤æÂ∫¶Âä†ÈÄü MHD Ê®°Êì¨„ÄÇÊ≠§ÈÄ≤Â±ïÊèêÂçá‰∫ÜË≥áÊñôËôïÁêÜÁöÑÊïàÁéáÔºåÂ¢ûÂº∑‰∫ÜÈ†êÊ∏¨ËÉΩÂäõÔºå‰∏¶‰∏îÊèê‰æõ‰∫ÜÂ∞çÁ£ÅÂ†¥ÊãìÊí≤ÁµêÊßãÁöÑÊõ¥Ê∑±ÂÖ•‰∫ÜËß£„ÄÇ

##### **Generative AI and Large Language Models for Cyber Security: All Insights You Need**
2405.12750v1 by Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour Mechri, Norbert Tihanyi

This paper provides a comprehensive review of the future of cybersecurity
through Generative AI and Large Language Models (LLMs). We explore LLM
applications across various domains, including hardware design security,
intrusion detection, software engineering, design verification, cyber threat
intelligence, malware detection, and phishing detection. We present an overview
of LLM evolution and its current state, focusing on advancements in models such
as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends
to LLM vulnerabilities, such as prompt injection, insecure output handling,
data poisoning, DDoS attacks, and adversarial instructions. We delve into
mitigation strategies to protect these models, providing a comprehensive look
at potential attack scenarios and prevention techniques. Furthermore, we
evaluate the performance of 42 LLM models in cybersecurity knowledge and
hardware security, highlighting their strengths and weaknesses. We thoroughly
evaluate cybersecurity datasets for LLM training and testing, covering the
lifecycle from data creation to usage and identifying gaps for future research.
In addition, we review new strategies for leveraging LLMs, including techniques
like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human
Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank
Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim
to enhance real-time cybersecurity defenses and improve the sophistication of
LLM applications in threat detection and response. Our paper provides a
foundational understanding and strategic direction for integrating LLMs into
future cybersecurity frameworks, emphasizing innovation and robust model
deployment to safeguard against evolving cyber threats.

ÊëòË¶ÅÔºöÊú¨ÊñáÂÖ®Èù¢Êé¢Ë®é‰∫ÜÈÄèÈÅéÁîüÊàêÂºè AI ÂíåÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰æÜ‰øùÈöúÁ∂≤Ë∑ØÂÆâÂÖ®ÁöÑÊú™‰æÜ„ÄÇÊàëÂÄëÊé¢Ë®é‰∫Ü LLM Âú®ÂêÑÂÄãÈ†òÂüüÁöÑÊáâÁî®ÔºåÂåÖÊã¨Á°¨È´îË®≠Ë®àÂÆâÂÖ®ÊÄß„ÄÅÂÖ•‰æµÂÅµÊ∏¨„ÄÅËªüÈ´îÂ∑•Á®ã„ÄÅË®≠Ë®àÈ©óË≠â„ÄÅÁ∂≤Ë∑ØÂ®ÅËÑÖÊÉÖÂ†±„ÄÅÊÉ°ÊÑèËªüÈ´îÂÅµÊ∏¨ÂíåÁ∂≤Ë∑ØÈá£È≠öÂÅµÊ∏¨„ÄÇÊàëÂÄëÊ¶ÇËø∞‰∫Ü LLM ÁöÑÊºîÈÄ≤ÂèäÂÖ∂ÁèæÊ≥ÅÔºåÈáçÈªûÈóúÊ≥® GPT-4„ÄÅGPT-3.5„ÄÅMixtral-8x7B„ÄÅBERT„ÄÅFalcon2 Âíå LLaMA Á≠âÊ®°ÂûãÁöÑÈÄ≤Â±ï„ÄÇÊàëÂÄëÁöÑÂàÜÊûêÂª∂‰º∏Âà∞ LLM ÁöÑÊºèÊ¥ûÔºå‰æãÂ¶ÇÊèêÁ§∫Ê≥®ÂÖ•„ÄÅ‰∏çÂÆâÂÖ®ÁöÑËº∏Âá∫ËôïÁêÜ„ÄÅË≥áÊñô‰∏≠ÊØí„ÄÅDDoS ÊîªÊìäÂíåÂ∞çÊäóÊÄßÊåá‰ª§„ÄÇÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®é‰∫Ü‰øùË≠∑ÈÄô‰∫õÊ®°ÂûãÁöÑÁ∑©Ëß£Á≠ñÁï•ÔºåÂÖ®Èù¢‰∫ÜËß£ÊΩõÂú®ÁöÑÊîªÊìäÊÉÖÂ¢ÉÂíåÈ†êÈò≤ÊäÄË°ì„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË©ï‰º∞‰∫Ü 42 ÂÄã LLM Ê®°ÂûãÂú®Á∂≤Ë∑ØÂÆâÂÖ®Áü•Ë≠òÂíåÁ°¨È´îÂÆâÂÖ®ÊÄßÊñπÈù¢ÁöÑÊïàËÉΩÔºåÈáçÈªûË™™ÊòéÂÆÉÂÄëÁöÑÂÑ™ÈªûÂíåÁº∫Èªû„ÄÇÊàëÂÄëÂæπÂ∫ïË©ï‰º∞‰∫Ü LLM Ë®ìÁ∑¥ÂíåÊ∏¨Ë©¶ÁöÑÁ∂≤Ë∑ØÂÆâÂÖ®Ë≥áÊñôÈõÜÔºåÊ∂µËìãÂæûË≥áÊñôÂª∫Á´ãÂà∞‰ΩøÁî®ÁöÑÁîüÂëΩÈÄ±ÊúüÔºå‰∏¶ÊâæÂá∫Êú™‰æÜÁ†îÁ©∂ÁöÑÂ∑ÆË∑ù„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂõûÈ°ß‰∫ÜÂà©Áî® LLM ÁöÑÊñ∞Á≠ñÁï•ÔºåÂåÖÊã¨Âçä‰∫åÊ¨°ÈáèÂåñ (HQQ)„ÄÅÂÖ∑Êúâ‰∫∫È°ûÂõûÈ•ãÁöÑÂº∑ÂåñÂ≠∏Áøí (RLHF)„ÄÅÁõ¥Êé•ÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ (DPO)„ÄÅÈáèÂåñ‰ΩéÁß©ÈÅ©ÈÖçÂô® (QLoRA) ÂíåÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Á≠âÊäÄË°ì„ÄÇÈÄô‰∫õË¶ãËß£Êó®Âú®Â¢ûÂº∑Âç≥ÊôÇÁ∂≤Ë∑ØÂÆâÂÖ®Èò≤Á¶¶Ôºå‰∏¶ÊèêÂçá LLM ÊáâÁî®Âú®Â®ÅËÑÖÂÅµÊ∏¨ÂíåÂõûÊáâÊñπÈù¢ÁöÑË§áÈõúÊÄß„ÄÇÊàëÂÄëÁöÑË´ñÊñáÊèê‰æõ‰∫ÜÂü∫Á§éÁêÜËß£ÂíåÁ≠ñÁï•ÊñπÂêëÔºåÁî®ÊñºÂ∞á LLM Êï¥ÂêàÂà∞Êú™‰æÜÁöÑÁ∂≤Ë∑ØÂÆâÂÖ®Êû∂Êßã‰∏≠ÔºåÂº∑Ë™øÂâµÊñ∞ÂíåÁ©©ÂÅ•ÁöÑÊ®°ÂûãÈÉ®ÁΩ≤Ôºå‰ª•Èò≤ÁØÑ‰∏çÊñ∑ÊºîËÆäÁöÑÁ∂≤Ë∑ØÂ®ÅËÑÖ„ÄÇ

##### **The Echoes of Multilinguality: Tracing Cultural Value Shifts during LM Fine-tuning**
2405.12744v1 by Rochelle Choenni, Anne Lauscher, Ekaterina Shutova

Texts written in different languages reflect different culturally-dependent
beliefs of their writers. Thus, we expect multilingual LMs (MLMs), that are
jointly trained on a concatenation of text in multiple languages, to encode
different cultural values for each language. Yet, as the 'multilinguality' of
these LMs is driven by cross-lingual sharing, we also have reason to belief
that cultural values bleed over from one language into another. This limits the
use of MLMs in practice, as apart from being proficient in generating text in
multiple languages, creating language technology that can serve a community
also requires the output of LMs to be sensitive to their biases (Naous et al.,
2023). Yet, little is known about how cultural values emerge and evolve in MLMs
(Hershcovich et al., 2022a). We are the first to study how languages can exert
influence on the cultural values encoded for different test languages, by
studying how such values are revised during fine-tuning. Focusing on the
fine-tuning stage allows us to study the interplay between value shifts when
exposed to new linguistic experience from different data sources and languages.
Lastly, we use a training data attribution method to find patterns in the
fine-tuning examples, and the languages that they come from, that tend to
instigate value shifts.

ÊëòË¶ÅÔºö‰ª•‰∏çÂêåË™ûË®ÄÂØ´ÊàêÁöÑÊñáÊú¨ÂèçÊò†‰∫Ü‰ΩúËÄÖ‰∏çÂêåÁöÑÊñáÂåñ‰æùË≥¥ÊÄß‰ø°Âøµ„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÈ†êÊúüÂú®Â§öÁ®ÆË™ûË®ÄÊñáÊú¨‰∏≤Êé•‰∏äÈÄ≤Ë°åËÅØÂêàË®ìÁ∑¥ÁöÑÂ§öË™ûË®Ä LMÔºàMLMÔºâÔºåÊúÉÈáùÂ∞çÊØèÁ®ÆË™ûË®ÄÁ∑®Á¢º‰∏çÂêåÁöÑÊñáÂåñÂÉπÂÄº„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÈÄô‰∫õ LM ÁöÑ„ÄåÂ§öË™ûË®ÄÊÄß„ÄçÊòØÁî±Ë∑®Ë™ûË®ÄÂÖ±‰∫´È©ÖÂãïÁöÑÔºåÊàëÂÄë‰πüÊúâÁêÜÁî±Áõ∏‰ø°ÊñáÂåñÂÉπÂÄºÊúÉÂæû‰∏ÄÁ®ÆË™ûË®ÄÊª≤ÈÄèÂà∞Âè¶‰∏ÄÁ®ÆË™ûË®Ä„ÄÇÈÄôÈôêÂà∂‰∫Ü MLM Âú®ÂØ¶Âãô‰∏äÁöÑ‰ΩøÁî®ÔºåÂõ†ÁÇ∫Èô§‰∫ÜËÉΩÁÜüÁ∑¥ÁîüÊàêÂ§öÁ®ÆË™ûË®ÄÁöÑÊñáÊú¨‰πãÂ§ñÔºåÂâµÈÄ†ËÉΩÊúçÂãôÁ§æÁæ§ÁöÑË™ûË®ÄÊäÄË°ì‰πüË¶ÅÊ±Ç LM ÁöÑËº∏Âá∫Â∞çÂÖ∂ÂÅèË¶ãÊïèÊÑüÔºàNaous et al., 2023Ôºâ„ÄÇÁÑ∂ËÄåÔºåÊàëÂÄëÂ∞çÊñáÂåñÂÉπÂÄºÂ¶Ç‰ΩïÂú® MLM ‰∏≠Âá∫ÁèæÂíåÊºîÂåñÁü•‰πãÁîöÂ∞ëÔºàHershcovich et al., 2022aÔºâ„ÄÇÊàëÂÄëÁéáÂÖàÁ†îÁ©∂Ë™ûË®ÄÂ¶Ç‰ΩïÂ∞çÈáùÂ∞ç‰∏çÂêåÊ∏¨Ë©¶Ë™ûË®ÄÁ∑®Á¢ºÁöÑÊñáÂåñÂÉπÂÄºÊñΩÂä†ÂΩ±ÈüøÔºåÊñπÊ≥ïÊòØÁ†îÁ©∂ÈÄô‰∫õÂÉπÂÄºÂú®ÂæÆË™øÈÅéÁ®ã‰∏≠Â¶Ç‰ΩïË¢´‰øÆÊ≠£„ÄÇÂ∞àÊ≥®ÊñºÂæÆË™øÈöéÊÆµËÆìÊàëÂÄëÂæó‰ª•Á†îÁ©∂Áï∂Êé•Ëß∏Âà∞‰æÜËá™‰∏çÂêåË≥áÊñô‰æÜÊ∫êÂíåË™ûË®ÄÁöÑÊñ∞Ë™ûË®ÄÁ∂ìÈ©óÊôÇÔºåÂÉπÂÄºËΩâËÆä‰πãÈñìÁöÑ‰∫§‰∫í‰ΩúÁî®„ÄÇÊúÄÂæåÔºåÊàëÂÄë‰ΩøÁî®Ë®ìÁ∑¥Ë≥áÊñôÊ≠∏Âõ†ÊñπÊ≥ï‰æÜÊâæÂá∫ÂæÆË™øÁØÑ‰æã‰∏≠ÁöÑÊ®°ÂºèÔºå‰ª•ÂèäÂÆÉÂÄë‰æÜËá™ÁöÑË™ûË®ÄÔºåÈÄô‰∫õÊ®°ÂºèÂæÄÂæÄÊúÉÂºïÁôºÂÉπÂÄºËΩâËÆä„ÄÇ

##### **RecGPT: Generative Pre-training for Text-based Recommendation**
2405.12715v1 by Hoang Ngo, Dat Quoc Nguyen

We present the first domain-adapted and fully-trained large language model,
RecGPT-7B, and its instruction-following variant, RecGPT-7B-Instruct, for
text-based recommendation. Experimental results on rating prediction and
sequential recommendation tasks show that our model, RecGPT-7B-Instruct,
outperforms previous strong baselines. We are releasing our RecGPT models as
well as their pre-training and fine-tuning datasets to facilitate future
research and downstream applications in text-based recommendation. Public
"huggingface" links to our RecGPT models and datasets are available at:
https://github.com/VinAIResearch/RecGPT

ÊëòË¶ÅÔºöÊàëÂÄëÂ±ïÁ§∫Á¨¨‰∏ÄÂÄãÈ†òÂüüÈÅ©Êáâ‰∏îÂÆåÂÖ®Ë®ìÁ∑¥Â•ΩÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã RecGPT-7BÔºå‰ª•ÂèäÂÖ∂ÈÅµÂæ™Êåá‰ª§ÁöÑËÆäÈ´î RecGPT-7B-InstructÔºåÁî®ÊñºÂü∫ÊñºÊñáÂ≠óÁöÑÊé®Ëñ¶„ÄÇË©ïÂàÜÈ†êÊ∏¨ÂíåÈ†ÜÂ∫èÊé®Ëñ¶‰ªªÂãôÁöÑÂØ¶È©óÁµêÊûúË°®ÊòéÔºåÊàëÂÄëÁöÑÊ®°Âûã RecGPT-7B-Instruct ÂÑ™ÊñºÂÖàÂâçÁöÑÂº∑Â§ßÂü∫Ê∫ñ„ÄÇÊàëÂÄëÊ≠£Âú®ÁôºÂ∏ÉÊàëÂÄëÁöÑ RecGPT Ê®°Âûã‰ª•ÂèäÂÆÉÂÄëÁöÑÈ†êË®ìÁ∑¥ÂíåÂæÆË™øÊï∏ÊìöÈõÜÔºå‰ª•‰øÉÈÄ≤Êú™‰æÜÂü∫ÊñºÊñáÂ≠óÁöÑÊé®Ëñ¶ÁöÑÁ†îÁ©∂Âíå‰∏ãÊ∏∏ÊáâÁî®„ÄÇÂèØÂú®‰ª•‰∏ãÁ∂≤ÂùÄÂèñÂæóÊàëÂÄë RecGPT Ê®°ÂûãÂíåÊï∏ÊìöÈõÜÁöÑÂÖ¨Èñã„Äåhuggingface„ÄçÈÄ£ÁµêÔºöhttps://github.com/VinAIResearch/RecGPT

##### **From Human-to-Human to Human-to-Bot Conversations in Software Engineering**
2405.12712v1 by Ranim Khojah, Francisco Gomes de Oliveira Neto, Philipp Leitner

Software developers use natural language to interact not only with other
humans, but increasingly also with chatbots. These interactions have different
properties and flow differently based on what goal the developer wants to
achieve and who they interact with. In this paper, we aim to understand the
dynamics of conversations that occur during modern software development after
the integration of AI and chatbots, enabling a deeper recognition of the
advantages and disadvantages of including chatbot interactions in addition to
human conversations in collaborative work. We compile existing conversation
attributes with humans and NLU-based chatbots and adapt them to the context of
software development. Then, we extend the comparison to include LLM-powered
chatbots based on an observational study. We present similarities and
differences between human-to-human and human-to-bot conversations, also
distinguishing between NLU- and LLM-based chatbots. Furthermore, we discuss how
understanding the differences among the conversation styles guides the
developer on how to shape their expectations from a conversation and
consequently support the communication within a software team. We conclude that
the recent conversation styles that we observe with LLM-chatbots can not
replace conversations with humans due to certain attributes regarding social
aspects despite their ability to support productivity and decrease the
developers' mental load.

ÊëòË¶ÅÔºöËªüÈ´îÈñãÁôº‰∫∫Âì°‰∏çÂÉÖ‰ΩøÁî®Ëá™ÁÑ∂Ë™ûË®ÄËàáÂÖ∂‰ªñ‰∫∫‰∫íÂãïÔºå‰πüË∂ä‰æÜË∂äÂ∏∏ËàáËÅäÂ§©Ê©üÂô®‰∫∫‰∫íÂãï„ÄÇÈÄô‰∫õ‰∫íÂãïÂÖ∑Êúâ‰∏çÂêåÁöÑÂ±¨ÊÄßÔºå‰∏¶‰∏îÊ†πÊìöÈñãÁôº‰∫∫Âì°ÊÉ≥Ë¶ÅÈÅîÊàêÁöÑÁõÆÊ®ôÂíå‰ªñÂÄë‰∫íÂãïÁöÑÂ∞çË±°ÔºåËÄåÊúâ‰∏çÂêåÁöÑÊµÅÁ®ã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊó®Âú®‰∫ÜËß£Âú®Êï¥Âêà AI ÂíåËÅäÂ§©Ê©üÂô®‰∫∫ÂæåÔºåÁèæ‰ª£ËªüÈ´îÈñãÁôº‰∏≠ÁôºÁîüÁöÑÂ∞çË©±ÂãïÊÖãÔºåÈÄ≤‰∏ÄÊ≠•‰∫ÜËß£Âú®Âçî‰ΩúÂ∑•‰Ωú‰∏≠Âä†ÂÖ•ËÅäÂ§©Ê©üÂô®‰∫∫‰∫íÂãïÁöÑÂÑ™Áº∫Èªû„ÄÇÊàëÂÄëÁ∑®Ë≠Ø‰∫ÜËàá‰∫∫È°ûÂíåÂü∫Êñº NLU ÁöÑËÅäÂ§©Ê©üÂô®‰∫∫ÁöÑÁèæÊúâÂ∞çË©±Â±¨ÊÄßÔºå‰∏¶Â∞áÂÆÉÂÄëË™øÊï¥Âà∞ËªüÈ´îÈñãÁôºÁöÑËÑàÁµ°‰∏≠„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÊ†πÊìöËßÄÂØüÁ†îÁ©∂ÔºåÂ∞áÊØîËºÉÊì¥Â±ïÂà∞ÂåÖÊã¨Áî± LLM È©ÖÂãïÁöÑËÅäÂ§©Ê©üÂô®‰∫∫„ÄÇÊàëÂÄëÂëàÁèæ‰∫∫Â∞ç‰∫∫Ëàá‰∫∫Â∞çÊ©üÂô®‰∫∫Â∞çË©±‰πãÈñìÁöÑÁõ∏‰ººÊÄßÂíåÂ∑ÆÁï∞Ôºå‰πüÂçÄÂàÜÂü∫Êñº NLU Âíå LLM ÁöÑËÅäÂ§©Ê©üÂô®‰∫∫„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëË®éË´ñÂ¶Ç‰Ωï‰∫ÜËß£Â∞çË©±È¢®Ê†º‰πãÈñìÁöÑÂ∑ÆÁï∞ÔºåÂºïÂ∞éÈñãÁôº‰∫∫Âì°Â¶Ç‰ΩïÂ°ëÈÄ†‰ªñÂÄëÂ∞çÂ∞çË©±ÁöÑÊúüÊúõÔºå‰∏¶Âõ†Ê≠§ÊîØÊè¥ËªüÈ´îÂúòÈöäÂÖßÁöÑÊ∫ùÈÄö„ÄÇÊàëÂÄëÂæóÂá∫ÁµêË´ñÔºåÊàëÂÄëÂú® LLM ËÅäÂ§©Ê©üÂô®‰∫∫‰∏≠ËßÄÂØüÂà∞ÁöÑÊúÄÊñ∞Â∞çË©±È¢®Ê†ºÔºåÂÑòÁÆ°ÂÆÉÂÄëÊúâÊîØÊè¥ÁîüÁî¢ÂäõÂíåÈôç‰ΩéÈñãÁôº‰∫∫Âì°ÂøÉÁêÜË≤†ÊìîÁöÑËÉΩÂäõÔºå‰ΩÜÁî±ÊñºÊüê‰∫õÈóúÊñºÁ§æ‰∫§ÊñπÈù¢ÁöÑÂ±¨ÊÄßÔºåÁÑ°Ê≥ïÂèñ‰ª£Ëàá‰∫∫È°ûÁöÑÂ∞çË©±„ÄÇ

##### **A Masked Semi-Supervised Learning Approach for Otago Micro Labels Recognition**
2405.12711v1 by Meng Shang, Lenore Dedeyne, Jolan Dupont, Laura Vercauteren, Nadjia Amini, Laurence Lapauw, Evelien Gielen, Sabine Verschueren, Carolina Varon, Walter De Raedt, Bart Vanrumste

The Otago Exercise Program (OEP) serves as a vital rehabilitation initiative
for older adults, aiming to enhance their strength and balance, and
consequently prevent falls. While Human Activity Recognition (HAR) systems have
been widely employed in recognizing the activities of individuals, existing
systems focus on the duration of macro activities (i.e. a sequence of
repetitions of the same exercise), neglecting the ability to discern micro
activities (i.e. the individual repetitions of the exercises), in the case of
OEP. This study presents a novel semi-supervised machine learning approach
aimed at bridging this gap in recognizing the micro activities of OEP. To
manage the limited dataset size, our model utilizes a Transformer encoder for
feature extraction, subsequently classified by a Temporal Convolutional Network
(TCN). Simultaneously, the Transformer encoder is employed for masked
unsupervised learning to reconstruct input signals. Results indicate that the
masked unsupervised learning task enhances the performance of the supervised
learning (classification task), as evidenced by f1-scores surpassing the
clinically applicable threshold of 0.8. From the micro activities, two
clinically relevant outcomes emerge: counting the number of repetitions of each
exercise and calculating the velocity during chair rising. These outcomes
enable the automatic monitoring of exercise intensity and difficulty in the
daily lives of older adults.

ÊëòË¶ÅÔºöÂ•ßÂ°îÂì•ÈÅãÂãïË®àÁï´ (OEP) ÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÁöÑÂæ©ÂÅ•Ë®àÁï´Ôºå
Êó®Âú®Â¢ûÂº∑ËÄÅÂπ¥‰∫∫ÁöÑËÇåÂäõÂíåÂπ≥Ë°°Ôºå
ÈÄ≤ËÄåÈ†êÈò≤Ë∑åÂÄí„ÄÇÂÑòÁÆ°‰∫∫È°ûÊ¥ªÂãïËæ®Ë≠ò (HAR) Á≥ªÁµ±
Â∑≤Ë¢´Âª£Ê≥õÁî®ÊñºËæ®Ë≠òÂÄã‰∫∫ÁöÑÊ¥ªÂãïÔºåÁèæÊúâÁöÑ
Á≥ªÁµ±ËëóÈáçÊñºÂ∑®ËßÄÊ¥ªÂãïÁöÑÊåÅÁ∫åÊôÇÈñìÔºàÂç≥
ÈáçË§áÁõ∏ÂêåÈÅãÂãïÁöÑÈ†ÜÂ∫èÔºâÔºåÂøΩÁï•‰∫ÜËæ®Âà•ÂæÆËßÄ
Ê¥ªÂãïÔºàÂç≥ÈÅãÂãïÁöÑÂÄãÂà•ÈáçË§áÔºâÁöÑËÉΩÂäõÔºåÂ∞±
OEP ËÄåË®Ä„ÄÇÊú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂçäÁõ£Áù£Ê©üÂô®Â≠∏ÁøíÊñπÊ≥ïÔºå
Êó®Âú®ÂΩåÂêàËæ®Ë≠ò OEP ÂæÆËßÄÊ¥ªÂãïÁöÑÂ∑ÆË∑ù„ÄÇÁÇ∫‰∫Ü
ÁÆ°ÁêÜÊúâÈôêÁöÑË≥áÊñôÈõÜÂ§ßÂ∞èÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂà©Áî® Transformer Á∑®Á¢ºÂô®ÈÄ≤Ë°å
ÁâπÂæµËêÉÂèñÔºåÈö®ÂæåÁî±ÊôÇÂ∫èÂç∑Á©çÁ∂≤Ë∑Ø (TCN) ÂàÜÈ°û„ÄÇÂêåÊôÇÔºåTransformer Á∑®Á¢ºÂô®Ë¢´Áî®ÊñºÈÅÆËîΩÁÑ°Áõ£Áù£Â≠∏Áøí‰ª•ÈáçÂª∫Ëº∏ÂÖ•Ë®äËôü„ÄÇÁµêÊûúË°®ÊòéÔºåÈÅÆËîΩÁÑ°Áõ£Áù£Â≠∏Áøí‰ªªÂãôÂ¢ûÂº∑‰∫ÜÁõ£Áù£ÂºèÂ≠∏ÁøíÔºàÂàÜÈ°û‰ªªÂãôÔºâÁöÑÊïàËÉΩÔºåÈÄôÁî± f1 ÂàÜÊï∏Ë∂ÖÈÅé 0.8 ÁöÑËá®Â∫äÈÅ©Áî®ÈñæÂÄºÊâÄË≠âÂØ¶„ÄÇÂæûÂæÆËßÄÊ¥ªÂãï‰∏≠ÔºåÂá∫Áèæ‰∫ÜÂÖ©ÂÄãËá®Â∫ä‰∏äÁõ∏ÈóúÁöÑÁµêÊûúÔºöË®àÁÆóÊØèÂÄãÈÅãÂãïÁöÑÈáçË§áÊ¨°Êï∏ÂíåË®àÁÆóÊ§ÖÂ≠ê‰∏äÂçáÊôÇÁöÑÈÄüÁéá„ÄÇÈÄô‰∫õÁµêÊûúËÉΩËá™ÂãïÁõ£ÊéßËÄÅÂπ¥‰∫∫Êó•Â∏∏ÁîüÊ¥ª‰∏≠ÁöÑÈÅãÂãïÂº∑Â∫¶ÂíåÈõ£Â∫¶„ÄÇ

##### **Multimodal Adaptive Inference for Document Image Classification with Anytime Early Exiting**
2405.12705v1 by Omar Hamed, Souhail Bakkali, Marie-Francine Moens, Matthew Blaschko, Jordy Van Landeghem

This work addresses the need for a balanced approach between performance and
efficiency in scalable production environments for visually-rich document
understanding (VDU) tasks. Currently, there is a reliance on large document
foundation models that offer advanced capabilities but come with a heavy
computational burden. In this paper, we propose a multimodal early exit (EE)
model design that incorporates various training strategies, exit layer types
and placements. Our goal is to achieve a Pareto-optimal balance between
predictive performance and efficiency for multimodal document image
classification. Through a comprehensive set of experiments, we compare our
approach with traditional exit policies and showcase an improved
performance-efficiency trade-off. Our multimodal EE design preserves the
model's predictive capabilities, enhancing both speed and latency. This is
achieved through a reduction of over 20% in latency, while fully retaining the
baseline accuracy. This research represents the first exploration of multimodal
EE design within the VDU community, highlighting as well the effectiveness of
calibration in improving confidence scores for exiting at different layers.
Overall, our findings contribute to practical VDU applications by enhancing
both performance and efficiency.

ÊëòË¶ÅÔºöÈÄôÈ†ÖÂ∑•‰ΩúÊé¢Ë®é‰∫ÜÂú®Ë¶ñË¶∫Ë±êÂØåÊñá‰ª∂ÁêÜËß£ (VDU) ‰ªªÂãôÁöÑÂèØÊì¥ÂÖÖÁîüÁî¢Áí∞Â¢É‰∏≠ÔºåÂú®ÊïàËÉΩÂíåÊïàÁéá‰πãÈñìÂèñÂæóÂπ≥Ë°°ÊñπÊ≥ïÁöÑÂøÖË¶ÅÊÄß„ÄÇÁõÆÂâçÔºåÊàëÂÄë‰æùË≥¥ÊñºÂ§ßÂûãÊñá‰ª∂Âü∫Á§éÊ®°ÂûãÔºåÈÄô‰∫õÊ®°ÂûãÊèê‰æõÈÄ≤ÈöéÂäüËÉΩÔºå‰ΩÜ‰º¥Èö®ËëóÁπÅÈáçÁöÑÈÅãÁÆóË≤†Êìî„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫Â§öÊ®°ÊÖãÊó©ÊúüÈÄÄÂá∫ (EE) Ê®°ÂûãË®≠Ë®àÔºåÂÆÉÁµêÂêà‰∫ÜÂêÑÁ®ÆË®ìÁ∑¥Á≠ñÁï•„ÄÅÈÄÄÂá∫Â±§È°ûÂûãÂíåÈÖçÁΩÆ„ÄÇÊàëÂÄëÁöÑÁõÆÊ®ôÊòØÈáùÂ∞çÂ§öÊ®°ÊÖãÊñá‰ª∂ÂΩ±ÂÉèÂàÜÈ°ûÔºåÂú®È†êÊ∏¨ÊïàËÉΩÂíåÊïàÁéá‰πãÈñìÂèñÂæóÂ∏ïÈõ∑ÊâòÊúÄÂÑ™Âπ≥Ë°°„ÄÇÈÄèÈÅé‰∏ÄÁµÑÂÖ®Èù¢ÁöÑÂØ¶È©óÔºåÊàëÂÄëÂ∞áÊàëÂÄëÁöÑÊñπÊ≥ïËàáÂÇ≥Áµ±ÈÄÄÂá∫ÊîøÁ≠ñÈÄ≤Ë°åÊØîËºÉÔºå‰∏¶Â±ïÁ§∫Âá∫ÊîπËâØÁöÑÊïàËÉΩÊïàÁéáÊ¨äË°°„ÄÇÊàëÂÄëÁöÑÂ§öÊ®°ÊÖã EE Ë®≠Ë®à‰øùÁïô‰∫ÜÊ®°ÂûãÁöÑÈ†êÊ∏¨ËÉΩÂäõÔºåÂêåÊôÇÊèêÂçá‰∫ÜÈÄüÂ∫¶ÂíåÂª∂ÈÅ≤„ÄÇÈÄôÊòØÈÄèÈÅéÊ∏õÂ∞ëË∂ÖÈÅé 20% ÁöÑÂª∂ÈÅ≤‰æÜÂØ¶ÁèæÁöÑÔºåÂêåÊôÇÂÆåÂÖ®‰øùÁïôÂü∫Ê∫ñÊ∫ñÁ¢∫Â∫¶„ÄÇÈÄôÈ†ÖÁ†îÁ©∂‰ª£Ë°®‰∫Ü VDU Á§æÁæ§‰∏≠È¶ñÊ¨°Êé¢Ë®éÂ§öÊ®°ÊÖã EE Ë®≠Ë®àÔºåÂêåÊôÇÂº∑Ë™ø‰∫ÜÊ†°Ê≠£Â∞çÊñºÊîπÈÄ≤Âú®‰∏çÂêåÂ±§ÈÄÄÂá∫ÊôÇÁöÑ‰ø°ÂøÉÂàÜÊï∏ÁöÑÊúâÊïàÊÄß„ÄÇÁ∏ΩÈ´îËÄåË®ÄÔºåÊàëÂÄëÁöÑÁôºÁèæÈÄèÈÅéÊèêÂçáÊïàËÉΩÂíåÊïàÁéáÔºåÁÇ∫ÂØ¶ÈöõÁöÑ VDU ÊáâÁî®ÂÅöÂá∫‰∫ÜË≤¢Áçª„ÄÇ

##### **OLAPH: Improving Factuality in Biomedical Long-form Question Answering**
2405.12701v1 by Minbyul Jeong, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, Jaewoo Kang

In the medical domain, numerous scenarios necessitate the long-form
generation ability of large language models (LLMs). Specifically, when
addressing patients' questions, it is essential that the model's response
conveys factual claims, highlighting the need for an automated method to
evaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset
reconstructed using long-form question-answering datasets related to the
biomedical domain. We use MedLFQA to facilitate the automatic evaluations of
factuality. We also propose OLAPH, a simple and novel framework that enables
the improvement of factuality through automatic evaluations. The OLAPH
framework iteratively trains LLMs to mitigate hallucinations using sampling
predictions and preference optimization. In other words, we iteratively set the
highest-scoring response as a preferred response derived from sampling
predictions and train LLMs to align with the preferred response that improves
factuality. We highlight that, even on evaluation metrics not used during
training, LLMs trained with our OLAPH framework demonstrate significant
performance improvement in factuality. Our findings reveal that a 7B LLM
trained with our OLAPH framework can provide long answers comparable to the
medical experts' answers in terms of factuality. We believe that our work could
shed light on gauging the long-text generation ability of LLMs in the medical
domain. Our code and datasets are available at
https://github.com/dmis-lab/OLAPH}{https://github.com/dmis-lab/OLAPH.

ÊëòË¶ÅÔºö<paragraph>Âú®ÈÜ´ÁôÇÈ†òÂüüÔºåË®±Â§öÊÉÖÂ¢ÉÈÉΩÈúÄË¶ÅÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Áî¢ÁîüÈï∑ÁØáÊñáÂ≠óÁöÑËÉΩÂäõ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂú®ÂõûÁ≠îÊÇ£ËÄÖÂïèÈ°åÊôÇÔºåÊ®°ÂûãÁöÑÂõûÊáâÂÇ≥ÈÅî‰∫ãÂØ¶ËÅ≤ÊòéËá≥ÈóúÈáçË¶ÅÔºåÈÄôÁ™ÅÈ°Ø‰∫ÜËá™ÂãïÂåñÊñπÊ≥ï‰æÜË©ï‰º∞ÈÄô‰∫õËÅ≤ÊòéÁöÑÂøÖË¶ÅÊÄß„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü MedLFQAÔºåÈÄôÊòØ‰∏ÄÂÄã‰ΩøÁî®ËàáÁîüÁâ©ÈÜ´Â≠∏È†òÂüüÁõ∏ÈóúÁöÑÈï∑ÁØáÂïèÁ≠îË≥áÊñôÈõÜÈáçÂª∫ÁöÑÂü∫Ê∫ñË≥áÊñôÈõÜ„ÄÇÊàëÂÄë‰ΩøÁî® MedLFQA ‰æÜ‰øÉÈÄ≤‰∫ãÂØ¶ÊÄßÁöÑËá™ÂãïË©ï‰º∞„ÄÇÊàëÂÄëÈÇÑÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰∏îÊñ∞Á©éÁöÑÊ°ÜÊû∂ OLAPHÔºåÂÆÉËÉΩÂ§†ÈÄèÈÅéËá™ÂãïË©ï‰º∞‰æÜÊîπÂñÑ‰∫ãÂØ¶ÊÄß„ÄÇOLAPH Ê°ÜÊû∂ÂèçË¶ÜË®ìÁ∑¥ LLMÔºå‰ª•‰ΩøÁî®ÊäΩÊ®£È†êÊ∏¨ÂíåÂÅèÂ•ΩÊúÄ‰Ω≥Âåñ‰æÜÊ∏õËºïÂπªË¶∫„ÄÇÊèõÂè•Ë©±Ë™™ÔºåÊàëÂÄëÂèçË¶ÜÂ∞áÊúÄÈ´òÂàÜÂõûÊáâË®≠ÂÆöÁÇ∫ÂæûÊäΩÊ®£È†êÊ∏¨‰∏≠ÂæóÂá∫ÁöÑÈ¶ñÈÅ∏ÂõûÊáâÔºå‰∏¶Ë®ìÁ∑¥ LLM ËàáÊîπÂñÑ‰∫ãÂØ¶ÊÄßÁöÑÈ¶ñÈÅ∏ÂõûÊáâ‰øùÊåÅ‰∏ÄËá¥„ÄÇÊàëÂÄëÂº∑Ë™øÔºåÂç≥‰ΩøÂú®Ë®ìÁ∑¥ÊúüÈñìÊú™‰ΩøÁî®Ë©ï‰º∞ÊåáÊ®ôÔºå‰ΩøÁî®ÊàëÂÄëÁöÑ OLAPH Ê°ÜÊû∂Ë®ìÁ∑¥ÁöÑ LLM ‰πüÂú®‰∫ãÂØ¶ÊÄßÊñπÈù¢Ë°®ÁèæÂá∫È°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºå‰ΩøÁî®ÊàëÂÄëÁöÑ OLAPH Ê°ÜÊû∂Ë®ìÁ∑¥ÁöÑ 7B LLM Âú®‰∫ãÂØ¶ÊÄßÊñπÈù¢ÂèØ‰ª•Êèê‰æõËàáÈÜ´ÁôÇÂ∞àÂÆ∂Á≠îÊ°àÁõ∏Áï∂ÁöÑÈï∑ÁØáÂõûÁ≠î„ÄÇÊàëÂÄëÁõ∏‰ø°ÔºåÊàëÂÄëÁöÑÁ†îÁ©∂ÂèØ‰ª•Âπ´Âä©Ë©ï‰º∞ LLM Âú®ÈÜ´ÁôÇÈ†òÂüüÁöÑÈï∑ÁØáÊñáÂ≠óÁî¢ÁîüËÉΩÂäõ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÈõÜÂèØÂú® https://github.com/dmis-lab/OLAPH Áç≤Âæó„ÄÇ</paragraph>

##### **Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text**
2405.12689v1 by Yafu Li, Zhilin Wang, Leyang Cui, Wei Bi, Shuming Shi, Yue Zhang

AI-generated text detection has attracted increasing attention as powerful
language models approach human-level generation. Limited work is devoted to
detecting (partially) AI-paraphrased texts. However, AI paraphrasing is
commonly employed in various application scenarios for text refinement and
diversity. To this end, we propose a novel detection framework, paraphrased
text span detection (PTD), aiming to identify paraphrased text spans within a
text. Different from text-level detection, PTD takes in the full text and
assigns each of the sentences with a score indicating the paraphrasing degree.
We construct a dedicated dataset, PASTED, for paraphrased text span detection.
Both in-distribution and out-of-distribution results demonstrate the
effectiveness of PTD models in identifying AI-paraphrased text spans.
Statistical and model analysis explains the crucial role of the surrounding
context of the paraphrased text spans. Extensive experiments show that PTD
models can generalize to versatile paraphrasing prompts and multiple
paraphrased text spans. We release our resources at
https://github.com/Linzwcs/PASTED.

ÊëòË¶ÅÔºöAI ÁîüÊàêÁöÑÊñáÂ≠óÂÅµÊ∏¨Â∑≤Âê∏ÂºïË∂ä‰æÜË∂äÂ§öÁöÑÈóúÊ≥®ÔºåÂõ†ÁÇ∫Âº∑Â§ßÁöÑË™ûË®ÄÊ®°ÂûãÊé•Ëøë‰∫∫È°ûÂ±§Á¥öÁöÑÁîüÊàê„ÄÇÊúâÈôêÁöÑÂ∑•‰ΩúËá¥ÂäõÊñºÂÅµÊ∏¨ÔºàÈÉ®ÂàÜÔºâAI ÊîπÂØ´ÁöÑÊñáÂ≠ó„ÄÇÁÑ∂ËÄåÔºåAI ÊîπÂØ´ÈÄöÂ∏∏Áî®ÊñºÂêÑÁ®ÆÊáâÁî®Â†¥ÊôØ‰∏≠Ôºå‰ª•ÈÄ≤Ë°åÊñáÂ≠óÁ≤æÁÖâÂíåÂ§öÂÖÉÂåñ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞Á©éÁöÑÂÅµÊ∏¨Êû∂ÊßãÔºåÊîπÂØ´ÊñáÂ≠óË∑®Ë∑ùÂÅµÊ∏¨ (PTD)ÔºåÊó®Âú®Ë≠òÂà•ÊñáÂ≠ó‰∏≠ÁöÑÊîπÂØ´ÊñáÂ≠óË∑®Ë∑ù„ÄÇ‰∏çÂêåÊñºÊñáÂ≠óÂ±§Á¥öÂÅµÊ∏¨ÔºåPTD ÊúÉËº∏ÂÖ•ÂÆåÊï¥ÊñáÂ≠óÔºå‰∏¶ÁÇ∫ÊØèÂÄãÂè•Â≠êÊåáÂÆö‰∏ÄÂÄãÂàÜÊï∏ÔºåË°®Á§∫ÊîπÂØ´Á®ãÂ∫¶„ÄÇÊàëÂÄëÂª∫Êßã‰∫Ü‰∏ÄÂÄãÂ∞àÈñÄÁöÑË≥áÊñôÈõÜ PASTEDÔºåÁî®ÊñºÊîπÂØ´ÊñáÂ≠óË∑®Ë∑ùÂÅµÊ∏¨„ÄÇÂàÜ‰ΩàÂÖßÂíåÂàÜ‰ΩàÂ§ñÁµêÊûúÈÉΩË≠âÊòé‰∫Ü PTD Ê®°ÂûãÂú®Ë≠òÂà• AI ÊîπÂØ´ÊñáÂ≠óË∑®Ë∑ùÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇÁµ±Ë®àÂíåÊ®°ÂûãÂàÜÊûêË™™Êòé‰∫ÜÊîπÂØ´ÊñáÂ≠óË∑®Ë∑ùÂë®ÂúçÁí∞Â¢ÉÁöÑÈáçË¶ÅËßíËâ≤„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåPTD Ê®°ÂûãÂèØ‰ª•Ê¶ÇÊã¨ÁÇ∫Â§öÂäüËÉΩÊîπÂØ´ÊèêÁ§∫ÂíåÂ§öÂÄãÊîπÂØ´ÊñáÂ≠óË∑®Ë∑ù„ÄÇÊàëÂÄëÂú® https://github.com/Linzwcs/PASTED ‰∏äÁôºÂ∏ÉÊàëÂÄëÁöÑË≥áÊ∫ê„ÄÇ

##### **A Survey on Multi-modal Machine Translation: Tasks, Methods and Challenges**
2405.12669v1 by Huangjun Shen, Liangying Shao, Wenbo Li, Zhibin Lan, Zhanyu Liu, Jinsong Su

In recent years, multi-modal machine translation has attracted significant
interest in both academia and industry due to its superior performance. It
takes both textual and visual modalities as inputs, leveraging visual context
to tackle the ambiguities in source texts. In this paper, we begin by offering
an exhaustive overview of 99 prior works, comprehensively summarizing
representative studies from the perspectives of dominant models, datasets, and
evaluation metrics. Afterwards, we analyze the impact of various factors on
model performance and finally discuss the possible research directions for this
task in the future. Over time, multi-modal machine translation has developed
more types to meet diverse needs. Unlike previous surveys confined to the early
stage of multi-modal machine translation, our survey thoroughly concludes these
emerging types from different aspects, so as to provide researchers with a
better understanding of its current state.

ÊëòË¶ÅÔºöËøëÂπ¥‰æÜÔºåÂ§öÊ®°ÊÖãÊ©üÂô®ÁøªË≠ØÁî±ÊñºÂÖ∂ÂçìË∂äÁöÑÊÄßËÉΩÔºåÂú®Â≠∏Ë°ìÁïåÂíåÁî¢Ê•≠ÁïåÈÉΩÂºïËµ∑‰∫ÜÊ•µÂ§ßÁöÑËààË∂£„ÄÇÂÆÉÂ∞áÊñáÊú¨ÂíåË¶ñË¶∫Ê®°Âºè‰ΩúÁÇ∫Ëº∏ÂÖ•ÔºåÂà©Áî®Ë¶ñË¶∫Ë™ûÂ¢É‰æÜËß£Ê±∫Ê∫êÊñáÊú¨‰∏≠ÁöÑÊ≠ßÁæ©„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÈ¶ñÂÖàÊèê‰æõ 99 È†ÖÂÖàÂâçÂ∑•‰ΩúÁöÑË©≥Áõ°Ê¶ÇËø∞ÔºåÂÖ®Èù¢Á∏ΩÁµê‰∫ÜÂæû‰∏ªË¶ÅÊ®°Âûã„ÄÅË≥áÊñôÈõÜÂíåË©ï‰º∞ÊåáÊ®ôËßíÂ∫¶ÁöÑ‰ª£Ë°®ÊÄßÁ†îÁ©∂„ÄÇ‰πãÂæåÔºåÊàëÂÄëÂàÜÊûê‰∫ÜÂêÑÁ®ÆÂõ†Á¥†Â∞çÊ®°ÂûãÊïàËÉΩÁöÑÂΩ±ÈüøÔºåÊúÄÂæåË®éË´ñ‰∫ÜÊú™‰æÜÊ≠§‰ªªÂãôÂèØËÉΩÁöÑÁ†îÁ©∂ÊñπÂêë„ÄÇÈö®ËëóÊôÇÈñìÊé®ÁßªÔºåÂ§öÊ®°ÊÖãÊ©üÂô®ÁøªË≠ØÂ∑≤ÁôºÂ±ïÂá∫Êõ¥Â§öÈ°ûÂûãÔºå‰ª•ÊªøË∂≥‰∏çÂêåÁöÑÈúÄÊ±Ç„ÄÇËàáÂÖàÂâçÂÉÖÈôêÊñºÂ§öÊ®°ÊÖãÊ©üÂô®ÁøªË≠ØÊó©ÊúüÈöéÊÆµÁöÑË™øÊü•‰∏çÂêåÔºåÊàëÂÄëÁöÑË™øÊü•Âæû‰∏çÂêåÈù¢ÂêëÂæπÂ∫ïÁ∏ΩÁµê‰∫ÜÈÄô‰∫õÊñ∞ËààÈ°ûÂûãÔºå‰ª•‰æøËÆìÁ†îÁ©∂‰∫∫Âì°Êõ¥‰∫ÜËß£ÂÖ∂ÁèæÁãÄ„ÄÇ

##### **Mitigating Overconfidence in Out-of-Distribution Detection by Capturing Extreme Activations**
2405.12658v1 by Mohammad Azizmalayeri, Ameen Abu-Hanna, Giovanni Cin√†

Detecting out-of-distribution (OOD) instances is crucial for the reliable
deployment of machine learning models in real-world scenarios. OOD inputs are
commonly expected to cause a more uncertain prediction in the primary task;
however, there are OOD cases for which the model returns a highly confident
prediction. This phenomenon, denoted as "overconfidence", presents a challenge
to OOD detection. Specifically, theoretical evidence indicates that
overconfidence is an intrinsic property of certain neural network
architectures, leading to poor OOD detection. In this work, we address this
issue by measuring extreme activation values in the penultimate layer of neural
networks and then leverage this proxy of overconfidence to improve on several
OOD detection baselines. We test our method on a wide array of experiments
spanning synthetic data and real-world data, tabular and image datasets,
multiple architectures such as ResNet and Transformer, different training loss
functions, and include the scenarios examined in previous theoretical work.
Compared to the baselines, our method often grants substantial improvements,
with double-digit increases in OOD detection AUC, and it does not damage
performance in any scenario.

ÊëòË¶ÅÔºöÂÅµÊ∏¨Âá∫ÂàÜÂ∏É (OOD) ÁØÑ‰æãÂ∞çÊñºÊ©üÂô®Â≠∏ÁøíÊ®°ÂûãÂú®ÁúüÂØ¶‰∏ñÁïåÂ†¥ÊôØ‰∏≠ÂèØÈù†Âú∞ÈÉ®ÁΩ≤Ëá≥ÈóúÈáçË¶Å„ÄÇOOD Ëº∏ÂÖ•ÈÄöÂ∏∏È†êÊúüÊúÉÂú®‰∏ªË¶Å‰ªªÂãô‰∏≠ÈÄ†ÊàêÊõ¥‰∏çÁ¢∫ÂÆöÁöÑÈ†êÊ∏¨ÔºõÁÑ∂ËÄåÔºåÊúâ‰∫õ OOD Ê°à‰æãÊúÉËÆìÊ®°ÂûãÂõûÂÇ≥È´òÂ∫¶Ëá™‰ø°ÁöÑÈ†êÊ∏¨„ÄÇÈÄôÂÄãÁèæË±°Á®±ÁÇ∫„ÄåÈÅéÂ∫¶Ëá™‰ø°„ÄçÔºåÂ∞ç OOD ÂÅµÊ∏¨ÊßãÊàêÊåëÊà∞„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÁêÜË´ñË≠âÊìöÊåáÂá∫ÔºåÈÅéÂ∫¶Ëá™‰ø°ÊòØÁâπÂÆöÁ•ûÁ∂ìÁ∂≤Ë∑ØÊû∂ÊßãÁöÑÂÖßÂú®Â±¨ÊÄßÔºåÂ∞éËá¥ OOD ÂÅµÊ∏¨‰∏ç‰Ω≥„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéÊ∏¨ÈáèÁ•ûÁ∂ìÁ∂≤Ë∑ØÂÄíÊï∏Á¨¨‰∫åÂ±§‰∏≠ÁöÑÊ•µÁ´ØÂïüÁî®ÂÄº‰æÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÁÑ∂ÂæåÂà©Áî®ÈÄôÂÄãÈÅéÂ∫¶Ëá™‰ø°ÁöÑ‰ª£ÁêÜ‰æÜÊîπÂñÑÂπæÂÄã OOD ÂÅµÊ∏¨Âü∫Ê∫ñ„ÄÇÊàëÂÄëÂú®Ê∂µËìãÂêàÊàêË≥áÊñôÂíåÁúüÂØ¶‰∏ñÁïåË≥áÊñô„ÄÅË°®Ê†ºÂíåÂΩ±ÂÉèË≥áÊñôÈõÜ„ÄÅÂ§öÁ®ÆÊû∂ÊßãÔºà‰æãÂ¶Ç ResNet Âíå TransformerÔºâ„ÄÅ‰∏çÂêåË®ìÁ∑¥ÊêçÂ§±ÂáΩÊï∏ÁöÑÂêÑÁ®ÆÂØ¶È©ó‰∏≠Ê∏¨Ë©¶ÊñπÊ≥ïÔºå‰∏¶Á¥çÂÖ•ÂÖàÂâçÁêÜË´ñÂ∑•‰Ωú‰∏≠Ê™¢È©óÁöÑÂ†¥ÊôØ„ÄÇËàáÂü∫Ê∫ñÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ®°ÂûãÈÄöÂ∏∏ËÉΩÂ§ßÂπÖÊîπÂñÑÔºåOOD ÂÅµÊ∏¨ AUC Â¢ûÂä†ÂÖ©‰ΩçÊï∏Ôºå‰∏î‰∏çÊúÉÊêçÂÆ≥‰ªª‰ΩïÂ†¥ÊôØÁöÑÊïàËÉΩ„ÄÇ

##### **Retrieval-Augmented Language Model for Extreme Multi-Label Knowledge Graph Link Prediction**
2405.12656v1 by Yu-Hsiang Lin, Huang-Ting Shieh, Chih-Yu Liu, Kuang-Ting Lee, Hsiao-Cheng Chang, Jing-Lun Yang, Yu-Sheng Lin

Extrapolation in Large language models (LLMs) for open-ended inquiry
encounters two pivotal issues: (1) hallucination and (2) expensive training
costs. These issues present challenges for LLMs in specialized domains and
personalized data, requiring truthful responses and low fine-tuning costs.
Existing works attempt to tackle the problem by augmenting the input of a
smaller language model with information from a knowledge graph (KG). However,
they have two limitations: (1) failing to extract relevant information from a
large one-hop neighborhood in KG and (2) applying the same augmentation
strategy for KGs with different characteristics that may result in low
performance. Moreover, open-ended inquiry typically yields multiple responses,
further complicating extrapolation. We propose a new task, the extreme
multi-label KG link prediction task, to enable a model to perform extrapolation
with multiple responses using structured real-world knowledge. Our retriever
identifies relevant one-hop neighbors by considering entity, relation, and
textual data together. Our experiments demonstrate that (1) KGs with different
characteristics require different augmenting strategies, and (2) augmenting the
language model's input with textual data improves task performance
significantly. By incorporating the retrieval-augmented framework with KG, our
framework, with a small parameter size, is able to extrapolate based on a given
KG. The code can be obtained on GitHub:
https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ‰∏≠Áî®ÊñºÈñãÊîæÂºèÊé¢Á©∂ÁöÑÊé®Êñ∑ÊúÉÈÅ≠ÈÅáÂÖ©ÂÄãÈóúÈçµÂïèÈ°åÔºö(1) ÂπªË¶∫Âíå (2) ÊòÇË≤¥ÁöÑË®ìÁ∑¥ÊàêÊú¨„ÄÇÈÄô‰∫õÂïèÈ°åÁÇ∫Â∞àÈñÄÈ†òÂüüÂíåÂÄã‰∫∫ÂåñÊï∏Êìö‰∏≠ÁöÑ LLM Â∏∂‰æÜÊåëÊà∞ÔºåÈúÄË¶ÅÁúüÂØ¶ÁöÑÂõûÊáâÂíå‰ΩéÂæÆË™øÊàêÊú¨„ÄÇÁèæÊúâ‰ΩúÂìÅÂòóË©¶ÈÄöÈÅé‰ΩøÁî®‰æÜËá™Áü•Ë≠òÂúñ (KG) ÁöÑË≥áË®äÊì¥ÂÖÖËºÉÂ∞èÂûãË™ûË®ÄÊ®°ÂûãÁöÑËº∏ÂÖ•‰æÜËß£Ê±∫ÂïèÈ°å„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÊúâÂÖ©ÂÄãÈôêÂà∂Ôºö(1) ÁÑ°Ê≥ïÂæû KG ‰∏≠ÁöÑÂª£Â§ß‰∏ÄË∑≥ÈÑ∞Âüü‰∏≠ÊèêÂèñÁõ∏ÈóúË≥áË®äÔºå‰ª•Âèä (2) Â∞çÂÖ∑Êúâ‰∏çÂêåÁâπÂæµÁöÑ KG ÊáâÁî®Áõ∏ÂêåÁöÑÊì¥ÂÖÖÁ≠ñÁï•ÔºåÈÄôÂèØËÉΩÊúÉÂ∞éËá¥‰ΩéÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÈñãÊîæÂºèÊé¢Á©∂ÈÄöÂ∏∏ÊúÉÁî¢ÁîüÂ§öÈáçÂõûÊáâÔºåÈÄ≤‰∏ÄÊ≠•Ë§áÈõúÂåñÊé®Êñ∑„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊñ∞‰ªªÂãôÔºåÂç≥Ê•µÁ´ØÂ§öÊ®ôÁ±§ KG ÈÄ£ÁµêÈ†êÊ∏¨‰ªªÂãôÔºå‰ª•‰ΩøÊ®°ÂûãËÉΩÂ§†‰ΩøÁî®ÁµêÊßãÂåñÁöÑÁúüÂØ¶‰∏ñÁïåÁü•Ë≠òÂü∑Ë°åÂÖ∑ÊúâÂ§öÈáçÂõûÊáâÁöÑÊé®Êñ∑„ÄÇÊàëÂÄëÁöÑÊ™¢Á¥¢Âô®ÈÄöÈÅéÂêåÊôÇËÄÉÊÖÆÂØ¶È´î„ÄÅÈóú‰øÇÂíåÊñáÂ≠óË≥áÊñô‰æÜË≠òÂà•Áõ∏ÈóúÁöÑ‰∏ÄË∑≥ÈÑ∞Â±Ö„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË≠âÊòéÔºö(1) ÂÖ∑Êúâ‰∏çÂêåÁâπÂæµÁöÑ KG ÈúÄË¶Å‰∏çÂêåÁöÑÊì¥ÂÖÖÁ≠ñÁï•Ôºå‰ª•Âèä (2) ‰ΩøÁî®ÊñáÂ≠óË≥áÊñôÊì¥ÂÖÖË™ûË®ÄÊ®°ÂûãÁöÑËº∏ÂÖ•ÊúÉÈ°ØËëóÊîπÂñÑ‰ªªÂãôÊïàËÉΩ„ÄÇÈÄèÈÅéÂ∞áÊ™¢Á¥¢Êì¥ÂÖÖÊ°ÜÊû∂Ëàá KG Êï¥ÂêàÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂Âú®ÂèÉÊï∏Ë¶èÊ®°ËºÉÂ∞èÁöÑÊÉÖÊ≥Å‰∏ãÔºåËÉΩÂ§†Ê†πÊìöÁµ¶ÂÆöÁöÑ KG ÈÄ≤Ë°åÊé®Êñ∑„ÄÇ‰ª£Á¢ºÂèØ‰ª•Âú® GitHub ‰∏äÂèñÂæóÔºö
https://github.com/exiled1143/Retrieval-Augmented-Language-Model-for-Multi-Label-Knowledge-Graph-Link-Prediction.git</paragraph>

##### **Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable Term Frequency**
2405.12648v1 by Hyeongjin Kim, Sangwon Kim, Dasom Ahn, Jong Taek Lee, Byoung Chul Ko

Scene graph generation (SGG) is an important task in image understanding
because it represents the relationships between objects in an image as a graph
structure, making it possible to understand the semantic relationships between
objects intuitively. Previous SGG studies used a message-passing neural
networks (MPNN) to update features, which can effectively reflect information
about surrounding objects. However, these studies have failed to reflect the
co-occurrence of objects during SGG generation. In addition, they only
addressed the long-tail problem of the training dataset from the perspectives
of sampling and learning methods. To address these two problems, we propose
CooK, which reflects the Co-occurrence Knowledge between objects, and the
learnable term frequency-inverse document frequency (TF-l-IDF) to solve the
long-tail problem. We applied the proposed model to the SGG benchmark dataset,
and the results showed a performance improvement of up to 3.8% compared with
existing state-of-the-art models in SGGen subtask. The proposed method exhibits
generalization ability from the results obtained, showing uniform performance
improvement for all MPNN models.

ÊëòË¶ÅÔºöÂ†¥ÊôØÂúñÁîüÊàêÔºàSGGÔºâÊòØÂΩ±ÂÉèÁêÜËß£‰∏≠ÁöÑ‰∏ÄÈ†ÖÈáçË¶Å‰ªªÂãôÔºåÂõ†ÁÇ∫ÂÆÉÂ∞áÂΩ±ÂÉè‰∏≠Áâ©È´î‰πãÈñìÁöÑÈóú‰øÇË°®Á§∫ÁÇ∫ÂúñÂΩ¢ÁµêÊßãÔºåËÆìÁõ¥ËßÄÂú∞ÁêÜËß£Áâ©È´î‰πãÈñìÁöÑË™ûÁæ©Èóú‰øÇÊàêÁÇ∫ÂèØËÉΩ„ÄÇÂÖàÂâçÁöÑ SGG Á†îÁ©∂‰ΩøÁî®Ë®äÊÅØÂÇ≥ÈÅûÁ•ûÁ∂ìÁ∂≤Ë∑ØÔºàMPNNÔºâ‰æÜÊõ¥Êñ∞ÁâπÂæµÔºåÂèØ‰ª•ÊúâÊïàÂèçÊò†Âë®ÂúçÁâ©È´îÁöÑË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÁ†îÁ©∂Êú™ËÉΩÂèçÊò†Áâ©È´îÂú® SGG ÁîüÊàêÊúüÈñìÁöÑÂÖ±ÁèæÊÄß„ÄÇÊ≠§Â§ñÔºåÂÆÉÂÄëÂÉÖÂæûÂèñÊ®£ÂíåÂ≠∏ÁøíÊñπÊ≥ïÁöÑËßíÂ∫¶‰æÜËß£Ê±∫Ë®ìÁ∑¥Ë≥áÊñôÈõÜÁöÑÈï∑Â∞æÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÖ©ÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü CooKÔºåÂÆÉÂèçÊò†‰∫ÜÁâ©È´î‰πãÈñìÁöÑÂÖ±ÁèæÁü•Ë≠òÔºå‰ª•ÂèäÂèØÂ≠∏ÁøíÁöÑË©ûÈ†ª-ÈÄÜÊñá‰ª∂È†ªÁéáÔºàTF-l-IDFÔºâ‰æÜËß£Ê±∫Èï∑Â∞æÂïèÈ°å„ÄÇÊàëÂÄëÂ∞áÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÊáâÁî®Êñº SGG Âü∫Ê∫ñË≥áÊñôÈõÜÔºåÁµêÊûúÈ°ØÁ§∫Ëàá SGGen Â≠ê‰ªªÂãô‰∏≠ÁèæÊúâÁöÑÊúÄÂÖàÈÄ≤Ê®°ÂûãÁõ∏ÊØîÔºåÊïàËÉΩÊèêÂçá‰∫Ü 3.8%„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂæûÁç≤ÂæóÁöÑÁµêÊûú‰∏≠Â±ïÁèæÂá∫Ê¶ÇÂåñËÉΩÂäõÔºåÈ°ØÁ§∫ÊâÄÊúâ MPNN Ê®°ÂûãÁöÑÊïàËÉΩÈÉΩÊúâÂùáÂãªÁöÑÊèêÂçá„ÄÇ

##### **Exploration of Masked and Causal Language Modelling for Text Generation**
2405.12630v1 by Nicolo Micheletti, Samuel Belkadi, Lifeng Han, Goran Nenadic

Large Language Models (LLMs) have revolutionised the field of Natural
Language Processing (NLP) and have achieved state-of-the-art performance in
practically every task in this field. However, the prevalent approach used in
text generation, Causal Language Modelling (CLM), which generates text
sequentially from left to right, inherently limits the freedom of the model,
which does not decide when and where each token is generated. In contrast,
Masked Language Modelling (MLM), primarily used for language understanding
tasks, can generate tokens anywhere in the text and any order. This paper
conducts an extensive comparison of MLM and CLM approaches for text generation
tasks. To do so, we pre-train several language models of comparable sizes on
three different datasets, namely 1) medical discharge summaries, 2) movie plot
synopses, and 3) authorship verification datasets. To assess the quality of the
generations, we first employ quantitative metrics and then perform a
qualitative human evaluation to analyse coherence and grammatical correctness.
In addition, we evaluate the usefulness of the generated texts by using them in
three different downstream tasks: 1) Entity Recognition, 2) Text
Classification, and 3) Authorship Verification. The results show that MLM
consistently outperforms CLM in text generation across all datasets, with
higher quantitative scores and better coherence in the generated text. The
study also finds \textit{no strong correlation} between the quality of the
generated text and the performance of the models in the downstream tasks. With
this study, we show that MLM for text generation has great potential for future
research and provides direction for future studies in this area.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂæπÂ∫ïÊîπËÆä‰∫ÜËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) È†òÂüüÔºå‰∏¶Âú®Ë©≤È†òÂüüÁöÑÂπæ‰πéÊØè‰∏ÄÈ†Ö‰ªªÂãô‰∏≠ÈÉΩÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊÄßËÉΩ„ÄÇÁÑ∂ËÄåÔºåÊñáÊú¨ÁîüÊàê‰∏≠‰ΩøÁî®ÁöÑÊµÅË°åÊñπÊ≥ïÂõ†ÊûúË™ûË®ÄÂª∫Ê®° (CLM)ÔºåÂÆÉÂæûÂ∑¶Âà∞Âè≥ÊåâÈ†ÜÂ∫èÁîüÊàêÊñáÊú¨ÔºåÂæûÊú¨Ë≥™‰∏äÈôêÂà∂‰∫ÜÊ®°ÂûãÁöÑËá™Áî±Â∫¶ÔºåÂÆÉ‰∏çÊúÉÊ±∫ÂÆö‰ΩïÊôÇ‰ΩïÂú∞ÁîüÊàêÊØèÂÄãÁ¨¶Ëôü„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºå‰∏ªË¶ÅÁî®ÊñºË™ûË®ÄÁêÜËß£‰ªªÂãôÁöÑÈÅÆÁΩ©Ë™ûË®ÄÂª∫Ê®° (MLM) ÂèØ‰ª•Êåâ‰ªª‰ΩïÈ†ÜÂ∫èÂú®ÊñáÊú¨‰∏≠ÁöÑ‰ªª‰Ωï‰ΩçÁΩÆÁîüÊàêÁ¨¶Ëôü„ÄÇÊú¨ÊñáÂ∞ç MLM Âíå CLM ÊñπÊ≥ïÂú®ÊñáÊú¨ÁîüÊàê‰ªªÂãô‰∏≠ÁöÑÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÊØîËºÉ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂú®‰∏âÂÄã‰∏çÂêåÁöÑÊï∏ÊìöÈõÜ‰∏äÈ†êË®ìÁ∑¥‰∫ÜÂπæÂÄãÂ§ßÂ∞èÁõ∏Áï∂ÁöÑË™ûË®ÄÊ®°ÂûãÔºåÂç≥ 1) ÈÜ´ÁôÇÂá∫Èô¢ÊëòË¶Å„ÄÅ2) ÈõªÂΩ±ÊÉÖÁØÄÁ∞°‰ªãÂíå 3) ‰ΩúËÄÖÈ©óË≠âÊï∏ÊìöÈõÜ„ÄÇÁÇ∫‰∫ÜË©ï‰º∞ÁîüÊàêÁöÑË≥™ÈáèÔºåÊàëÂÄëÈ¶ñÂÖàÊé°Áî®ÂÆöÈáèÊåáÊ®ôÔºåÁÑ∂ÂæåÈÄ≤Ë°åÂÆöÊÄßÁöÑ‰∫∫Â∑•Ë©ï‰º∞Ôºå‰ª•ÂàÜÊûêÈÄ£Ë≤´ÊÄßÂíåË™ûÊ≥ïÊ≠£Á¢∫ÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÄöÈÅéÂú®‰∏âÂÄã‰∏çÂêåÁöÑ‰∏ãÊ∏∏‰ªªÂãô‰∏≠‰ΩøÁî®ÁîüÊàêÁöÑÊñáÊú¨‰æÜË©ï‰º∞ÂÖ∂ÊúâÁî®ÊÄßÔºö1) ÂØ¶È´îË≠òÂà•„ÄÅ2) ÊñáÊú¨ÂàÜÈ°ûÂíå 3) ‰ΩúËÄÖÈ©óË≠â„ÄÇÁµêÊûúË°®ÊòéÔºåMLM Âú®ÊâÄÊúâÊï∏ÊìöÈõÜ‰∏äÁöÑÊñáÊú¨ÁîüÊàê‰∏≠ÂßãÁµÇÂÑ™Êñº CLMÔºåÁîüÊàêÁöÑÊñáÊú¨ÂÖ∑ÊúâÊõ¥È´òÁöÑÂÆöÈáèÂàÜÊï∏ÂíåÊõ¥Â•ΩÁöÑÈÄ£Ë≤´ÊÄß„ÄÇË©≤Á†îÁ©∂ÈÇÑÁôºÁèæÁîüÊàêÁöÑÊñáÊú¨Ë≥™ÈáèËàáÊ®°ÂûãÂú®‰∏ãÊ∏∏‰ªªÂãô‰∏≠ÁöÑÊÄßËÉΩ‰πãÈñì\textit{Ê≤íÊúâÂº∑Áõ∏ÈóúÊÄß}„ÄÇÈÄöÈÅéÈÄôÈ†ÖÁ†îÁ©∂ÔºåÊàëÂÄëË°®ÊòéÊñáÊú¨ÁîüÊàêÁöÑ MLM Â∞çÊñºÊú™‰æÜÁöÑÁ†îÁ©∂ÂÖ∑ÊúâÂ∑®Â§ßÊΩõÂäõÔºå‰∏¶ÁÇ∫Ë©≤È†òÂüüÁöÑÊú™‰æÜÁ†îÁ©∂Êèê‰æõ‰∫ÜÊñπÂêë„ÄÇ</paragraph>

##### **Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition**
2405.12621v1 by Matteo Bortoletto, Constantin Ruhdorfer, Adnen Abdessaied, Lei Shi, Andreas Bulling

Recent work on dialogue-based collaborative plan acquisition (CPA) has
suggested that Theory of Mind (ToM) modelling can improve missing knowledge
prediction in settings with asymmetric skill-sets and knowledge. Although ToM
was claimed to be important for effective collaboration, its real impact on
this novel task remains under-explored. By representing plans as graphs and by
exploiting task-specific constraints we show that, as performance on CPA nearly
doubles when predicting one's own missing knowledge, the improvements due to
ToM modelling diminish. This phenomenon persists even when evaluating existing
baseline methods. To better understand the relevance of ToM for CPA, we report
a principled performance comparison of models with and without ToM features.
Results across different models and ablations consistently suggest that learned
ToM features are indeed more likely to reflect latent patterns in the data with
no perceivable link to ToM. This finding calls for a deeper understanding of
the role of ToM in CPA and beyond, as well as new methods for modelling and
evaluating mental states in computational collaborative agents.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÂÖ≥‰∫éÂü∫‰∫éÂØπËØùÁöÑÂçè‰ΩúËÆ°ÂàíËé∑Âèñ (CPA) ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂøÉÊô∫ÁêÜËÆ∫ (ToM) Âª∫Ê®°ÂèØ‰ª•ÊîπÂñÑÂú®ÊäÄËÉΩÂíåÁü•ËØÜ‰∏çÂØπÁß∞ÁöÑÊÉÖÂÜµ‰∏ãÁº∫Â§±Áü•ËØÜÁöÑÈ¢ÑÊµã„ÄÇËôΩÁÑ∂ ToM Ë¢´ËÆ§‰∏∫ÂØπÊúâÊïàÁöÑÂçè‰ΩúÂæàÈáçË¶ÅÔºå‰ΩÜÂÆÉÂØπËøôÈ°πÊñ∞‰ªªÂä°ÁöÑÂÆûÈôÖÂΩ±Âìç‰ªçÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢„ÄÇÈÄöËøáÂ∞ÜËÆ°ÂàíË°®Á§∫‰∏∫ÂõæË°®Âπ∂Âà©Áî®ÁâπÂÆö‰∫é‰ªªÂä°ÁöÑÁ∫¶ÊùüÔºåÊàë‰ª¨Ë°®ÊòéÔºåÂΩìÈ¢ÑÊµãËá™Â∑±Áº∫Â§±ÁöÑÁü•ËØÜÊó∂ÔºåCPA ÁöÑÊÄßËÉΩÂá†‰πéÁøªÂÄçÔºåÁî±‰∫é ToM Âª∫Ê®°ËÄå‰∫ßÁîüÁöÑÊîπËøõ‰ºöÂáèÂ∞ë„ÄÇÂç≥‰ΩøÂú®ËØÑ‰º∞Áé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ïÊó∂ÔºåËøôÁßçÁé∞Ë±°‰ªçÁÑ∂Â≠òÂú®„ÄÇ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞ÁêÜËß£ ToM ÂØπ CPA ÁöÑÁõ∏ÂÖ≥ÊÄßÔºåÊàë‰ª¨Êä•Âëä‰∫ÜÊúâÂíåÊ≤°Êúâ ToM ÁâπÂæÅÁöÑÊ®°ÂûãÁöÑÂéüÂàôÊÄßÊÄßËÉΩÊØîËæÉ„ÄÇ‰∏çÂêåÊ®°ÂûãÂíåÊ∂àËûçÁöÑÁªìÊûú‰∏ÄËá¥Ë°®ÊòéÔºåÂ≠¶‰π†ÁöÑ ToM ÁâπÂæÅÂÆûÈôÖ‰∏äÊõ¥ÊúâÂèØËÉΩÂèçÊò†Êï∞ÊçÆ‰∏≠ÁöÑÊΩúÂú®Ê®°ÂºèÔºåËÄå‰∏é ToM Ê≤°ÊúâÊòéÊòæËÅîÁ≥ª„ÄÇËøô‰∏ÄÂèëÁé∞Ë¶ÅÊ±ÇÊàë‰ª¨Êõ¥Ê∑±ÂÖ•Âú∞‰∫ÜËß£ ToM Âú® CPA ÂèäÂÖ∂‰πãÂ§ñÁöÑ‰ΩúÁî®Ôºå‰ª•ÂèäÂØπËÆ°ÁÆóÂçè‰Ωú‰ª£ÁêÜ‰∏≠ÁöÑÂøÉÁêÜÁä∂ÊÄÅËøõË°åÂª∫Ê®°ÂíåËØÑ‰º∞ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇ</paragraph>

##### **Quantifying Emergence in Large Language Models**
2405.12617v1 by Hang Chen, Xinyu Yang, Jiaying Zhu, Wenya Wang

Emergence, broadly conceptualized as the ``intelligent'' behaviors of LLMs,
has recently been studied and proved challenging to quantify due to the lack of
a measurable definition. Most commonly, it has been estimated statistically
through model performances across extensive datasets and tasks, which consumes
significant resources. In addition, such estimation is difficult to interpret
and may not accurately reflect the models' intrinsic emergence. In this work,
we propose a quantifiable solution for estimating emergence. Inspired by
emergentism in dynamics, we quantify the strength of emergence by comparing the
entropy reduction of the macroscopic (semantic) level with that of the
microscopic (token) level, both of which are derived from the representations
within the transformer block. Using a low-cost estimator, our quantification
method demonstrates consistent behaviors across a suite of LMs (GPT-2, GEMMA,
etc.) under both in-context learning (ICL) and natural sentences. Empirical
results show that (1) our method gives consistent measurements which align with
existing observations based on performance metrics, validating the
effectiveness of our emergence quantification; (2) our proposed metric uncovers
novel emergence patterns such as the correlations between the variance of our
metric and the number of ``shots'' in ICL, which further suggests a new way of
interpreting hallucinations in LLMs; (3) we offer a potential solution towards
estimating the emergence of larger and closed-resource LMs via smaller LMs like
GPT-2. Our codes are available at:
https://github.com/Zodiark-ch/Emergence-of-LLMs/.

ÊëòË¶ÅÔºö<paragraph>Âá∫ÁèæÔºåÂª£Ê≥õÊ¶ÇÂøµÂåñÁÇ∫ LLM ÁöÑ„ÄåÊô∫ÊÖß„ÄçË°åÁÇ∫Ôºå
ÊúÄËøëÂ∑≤Ë¢´Á†îÁ©∂‰∏¶Ë≠âÂØ¶Èõ£‰ª•ÈáèÂåñÔºåÂõ†ÁÇ∫Áº∫‰πèÂèØË°°ÈáèÁöÑÂÆöÁæ©„ÄÇÊúÄÂ∏∏Ë¶ãÁöÑÊòØÔºåÂÆÉÂ∑≤ÈÄèÈÅéÊ®°ÂûãÂú®Âª£Ê≥õÁöÑË≥áÊñôÈõÜÂíå‰ªªÂãô‰∏≠ÁöÑË°®ÁèæÁµ±Ë®à‰º∞Ë®àÔºåÈÄôÊ∂àËÄó‰∫ÜÂ§ßÈáèÁöÑË≥áÊ∫ê„ÄÇÊ≠§Â§ñÔºåÈÄôÁ®Æ‰º∞Ë®àÈõ£‰ª•Ëß£ÈáãÔºåÂèØËÉΩÁÑ°Ê≥ïÊ∫ñÁ¢∫ÂèçÊò†Ê®°ÂûãÁöÑÂÖßÂú®Âá∫Áèæ„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰º∞Ë®àÂá∫ÁèæÁöÑÂèØÈáèÂåñËß£Ê±∫ÊñπÊ°à„ÄÇÂèóÂà∞ÂãïÂäõÂ≠∏Âá∫Áèæ‰∏ªÁæ©ÁöÑÂïüÁôºÔºåÊàëÂÄëÈÄèÈÅéÊØîËºÉÂ∑®ËßÄÔºàË™ûÁæ©ÔºâÂ±§Á¥öÂíåÂæÆËßÄÔºàÁ¨¶ËôüÔºâÂ±§Á¥öÁöÑÁÜµÊ∏õÂ∞ë‰æÜÈáèÂåñÂá∫ÁèæÁöÑÂº∑Â∫¶ÔºåÂÖ©ËÄÖÈÉΩ‰æÜËá™ÊñºTransformerÂçÄÂ°ä‰∏≠ÁöÑË°®Á§∫„ÄÇ‰ΩøÁî®‰ΩéÊàêÊú¨‰º∞Ë®àÂô®ÔºåÊàëÂÄëÁöÑÈáèÂåñÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÂú®ÊÉÖÂ¢ÉÂ≠∏Áøí (ICL) ÂíåËá™ÁÑ∂Âè•Â≠êÁöÑÊÉÖÊ≥Å‰∏ãÔºåÂú® LMsÔºàGPT-2„ÄÅGEMMA Á≠âÔºâ‰∏≠ÁöÑ‰∏ÄËá¥Ë°åÁÇ∫„ÄÇÂØ¶Ë≠âÁµêÊûúÈ°ØÁ§∫ÔºåÔºà1ÔºâÊàëÂÄëÁöÑÈáèÂåñÊñπÊ≥ïËàáÂü∫ÊñºÊïàËÉΩÊåáÊ®ôÁöÑÁèæÊúâËßÄÂØüÁµêÊûú‰∏ÄËá¥ÔºåÈ©óË≠â‰∫ÜÊàëÂÄëÂá∫ÁèæÈáèÂåñÁöÑÊúâÊïàÊÄßÔºõÔºà2ÔºâÊàëÂÄëÊèêÂá∫ÁöÑÊåáÊ®ôÊè≠Á§∫‰∫ÜÊñ∞ÁöÑÂá∫ÁèæÊ®°ÂºèÔºå‰æãÂ¶ÇÊàëÂÄëÁöÑÊåáÊ®ôÁöÑËÆäÁï∞Êï∏Ëàá ICL ‰∏≠„ÄåÊ¨°Êï∏„Äç‰πãÈñìÁöÑÁõ∏ÈóúÊÄßÔºåÈÄ≤‰∏ÄÊ≠•ÊèêÂá∫‰∫ÜËß£Èáã LLM ‰∏≠ÂπªË¶∫ÁöÑÊñ∞ÊñπÊ≥ïÔºõÔºà3ÔºâÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãÊΩõÂú®ÁöÑËß£Ê±∫ÊñπÊ°àÔºåÈÄèÈÅéËºÉÂ∞èÁöÑ LLMÔºà‰æãÂ¶Ç GPT-2Ôºâ‰æÜ‰º∞Ë®àËºÉÂ§ß‰∏îÂ∞ÅÈñâË≥áÊ∫êÁöÑ LLM ÁöÑÂá∫Áèæ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºö
https://github.com/Zodiark-ch/Emergence-of-LLMs/.</paragraph>

##### **Tagengo: A Multilingual Chat Dataset**
2405.12612v1 by Peter Devine

Open source large language models (LLMs) have shown great improvements in
recent times. However, many of these models are focused solely on popular
spoken languages. We present a high quality dataset of more than 70k
prompt-response pairs in 74 languages which consist of human generated prompts
and synthetic responses. We use this dataset to train a state-of-the-art open
source English LLM to chat multilingually. We evaluate our model on MT-Bench
chat benchmarks in 6 languages, finding that our multilingual model outperforms
previous state-of-the-art open source LLMs across each language. We further
find that training on more multilingual data is beneficial to the performance
in a chosen target language (Japanese) compared to simply training on only data
in that language. These results indicate the necessity of training on large
amounts of high quality multilingual data to make a more accessible LLM.

ÊëòË¶ÅÔºöÈñãÊîæÂéüÂßãÁ¢ºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËøëÊúüÂ±ïÁèæÂá∫È°ØËëóÁöÑÈÄ≤Ê≠•„ÄÇÁÑ∂ËÄåÔºåË®±Â§öÊ≠§È°ûÊ®°ÂûãÂÉÖÂ∞àÊ≥®ÊñºÁÜ±ÈñÄÁöÑÂè£Ë™ûË™ûË®Ä„ÄÇÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÈ´òÂìÅË≥™ÁöÑË≥áÊñôÈõÜÔºåÂÖ∂‰∏≠ÂåÖÂê´ 74 Á®ÆË™ûË®Ä‰∏≠Ë∂ÖÈÅé 70k ÂÄãÊèêÁ§∫ÂõûÊáâÈÖçÂ∞çÔºåÈÄô‰∫õÈÖçÂ∞çÁî±‰∫∫È°ûÁî¢ÁîüÁöÑÊèêÁ§∫ÂíåÂêàÊàêÂõûÊáâÁµÑÊàê„ÄÇÊàëÂÄë‰ΩøÁî®Ê≠§Ë≥áÊñôÈõÜË®ìÁ∑¥‰∏ÄÂÄãÊúÄÂÖàÈÄ≤ÁöÑÈñãÊîæÂéüÂßãÁ¢ºËã±Êñá LLMÔºå‰ª•ÈÄ≤Ë°åÂ§öË™ûË®ÄËÅäÂ§©„ÄÇÊàëÂÄëÂú® 6 Á®ÆË™ûË®ÄÁöÑ MT-Bench ËÅäÂ§©Âü∫Ê∫ñ‰∏äË©ï‰º∞ÊàëÂÄëÁöÑÊ®°ÂûãÔºåÁôºÁèæÊàëÂÄëÁöÑÂ§öË™ûË®ÄÊ®°ÂûãÂú®ÊØèÁ®ÆË™ûË®Ä‰∏≠ÈÉΩÂÑ™ÊñºÂÖàÂâçÁöÑÊúÄÂÖàÈÄ≤ÈñãÊîæÂéüÂßãÁ¢º LLM„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•ÁôºÁèæÔºåËàáÂÉÖË®ìÁ∑¥Ë©≤Ë™ûË®Ä‰∏≠ÁöÑË≥áÊñôÁõ∏ÊØîÔºåË®ìÁ∑¥Êõ¥Â§öÂ§öË™ûË®ÄË≥áÊñôÊúâÂä©ÊñºÂú®ÊâÄÈÅ∏ÁõÆÊ®ôË™ûË®ÄÔºàÊó•Ë™ûÔºâ‰∏≠ÁöÑË°®Áèæ„ÄÇÈÄô‰∫õÁµêÊûúË°®ÊòéÔºåÈúÄË¶ÅË®ìÁ∑¥Â§ßÈáèÈ´òÂìÅË≥™ÁöÑÂ§öË™ûË®ÄË≥áÊñôÔºåÊâçËÉΩË£Ω‰ΩúÂá∫Êõ¥ÊòìÊñº‰ΩøÁî®ÁöÑ LLM„ÄÇ

##### **Tiny Refinements Elicit Resilience: Toward Efficient Prefix-Model Against LLM Red-Teaming**
2405.12604v1 by Jiaxu Liu, Xiangyu Yin, Sihao Wu, Jianhong Wang, Meng Fang, Xinping Yi, Xiaowei Huang

With the proliferation of red-teaming strategies for Large Language Models
(LLMs), the deficiency in the literature about improving the safety and
robustness of LLM defense strategies is becoming increasingly pronounced. This
paper introduces the LLM-based \textbf{sentinel} model as a plug-and-play
prefix module designed to reconstruct the input prompt with just a few ($<30$)
additional tokens, effectively reducing toxicity in responses from target LLMs.
The sentinel model naturally overcomes the \textit{parameter inefficiency} and
\textit{limited model accessibility} for fine-tuning large target models. We
employ an interleaved training regimen using Proximal Policy Optimization (PPO)
to optimize both red team and sentinel models dynamically, incorporating a
value head-sharing mechanism inspired by the multi-agent centralized critic to
manage the complex interplay between agents. Our extensive experiments across
text-to-text and text-to-image demonstrate the effectiveness of our approach in
mitigating toxic outputs, even when dealing with larger models like
\texttt{Llama-2}, \texttt{GPT-3.5} and \texttt{Stable-Diffusion}, highlighting
the potential of our framework in enhancing safety and robustness in various
applications.

ÊëòË¶ÅÔºöÈö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÁ¥ÖÈöäÁ≠ñÁï•Êì¥Êï£ÔºåÈóúÊñºÊîπÂñÑ LLM Èò≤Á¶¶Á≠ñÁï•ÂÆâÂÖ®ÊÄßËàáÁ©©ÂÅ•ÊÄßÁöÑÊñáÁçª‰∏çË∂≥‰πãËôïÊó•ÁõäÊòéÈ°Ø„ÄÇÊú¨Êñá‰ªãÁ¥πÂü∫Êñº LLM ÁöÑ**Âì®ÂÖµ**Ê®°ÂûãÔºå‰ΩúÁÇ∫‰∏ÄÂÄãÂç≥ÊèíÂç≥Áî®ÁöÑÂâçÁ∂¥Ê®°ÁµÑÔºåÊó®Âú®ÂÉÖ‰ΩøÁî®Â∞ëÊï∏Ôºà$<30$ÔºâÈ°çÂ§ñ‰ª£Âπ£ÈáçÂª∫Ëº∏ÂÖ•ÊèêÁ§∫ÔºåÊúâÊïàÈôç‰ΩéÁõÆÊ®ô LLM ÂõûÊáâ‰∏≠ÁöÑÊØíÊÄß„ÄÇÂì®ÂÖµÊ®°ÂûãËá™ÁÑ∂ÂÖãÊúç‰∫ÜÂæÆË™øÂ§ßÂûãÁõÆÊ®ôÊ®°ÂûãÁöÑ**ÂèÉÊï∏ÊïàÁéá‰Ωé‰∏ã**Âíå**Ê®°ÂûãÂèØÂ≠òÂèñÊÄßÊúâÈôê**„ÄÇÊàëÂÄëÊé°Áî®Á©øÊèíË®ìÁ∑¥ÊñπÂºèÔºå‰ΩøÁî®ËøëÁ´ØÁ≠ñÁï•ÊúÄ‰Ω≥ÂåñÔºàPPOÔºâÂãïÊÖãÊúÄ‰Ω≥ÂåñÁ¥ÖÈöäÂíåÂì®ÂÖµÊ®°ÂûãÔºå‰∏¶ÁµêÂêàÂèóÂ§öÈáç‰ª£ÁêÜÈõÜ‰∏≠ÂºèÊâπË©ïËÄÖÂïüÁôºÁöÑÂÉπÂÄºÈ†≠ÈÉ®ÂÖ±‰∫´Ê©üÂà∂Ôºå‰æÜÁÆ°ÁêÜ‰ª£ÁêÜ‰πãÈñìÁöÑË§áÈõú‰∫§‰∫í„ÄÇÊàëÂÄëÂú®ÊñáÂ≠óËΩâÊñáÂ≠óÂíåÊñáÂ≠óËΩâÂΩ±ÂÉèÁöÑÂª£Ê≥õÂØ¶È©ó‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ïÂú®Ê∏õËºïÊúâÊØíËº∏Âá∫ÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåÂç≥‰ΩøÂú®ËôïÁêÜÂÉè \texttt{Llama-2}„ÄÅ\texttt{GPT-3.5} Âíå \texttt{Stable-Diffusion} Á≠âËºÉÂ§ßÂûãÊ®°ÂûãÊôÇ‰πüÊòØÂ¶ÇÊ≠§ÔºåÁ™ÅÈ°Ø‰∫ÜÊàëÂÄëÁöÑÊû∂ÊßãÂú®Â¢ûÂº∑ÂêÑÁ®ÆÊáâÁî®‰∏≠ÂÆâÂÖ®ÊÄßËàáÁ©©ÂÅ•ÊÄßÁöÑÊΩõÂäõ„ÄÇ

##### **Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression**
2405.12591v1 by Peiyu Liu, Ze-Feng Gao, Wayne Xin Zhao, Yipeng Ma, Tao Wang, Ji-Rong Wen

Key-value~(KV) caching is an important technique to accelerate the inference
of large language models~(LLMs), but incurs significant memory overhead. To
compress the size of KV cache, existing methods often compromise precision or
require extra data for calibration, limiting their practicality in LLM
deployment. In this paper, we introduce \textbf{DecoQuant}, a novel data-free
low-bit quantization technique based on tensor decomposition methods, to
effectively compress KV cache. Our core idea is to adjust the outlier
distribution of the original matrix by performing tensor decomposition, so that
the quantization difficulties are migrated from the matrix to decomposed local
tensors. Specially, we find that outliers mainly concentrate on small local
tensors, while large tensors tend to have a narrower value range. Based on this
finding, we propose to apply low-bit quantization to the large tensor, while
maintaining high-precision representation for the small tensor. Furthermore, we
utilize the proposed quantization method to compress the KV cache of LLMs to
accelerate the inference and develop an efficient dequantization kernel
tailored specifically for DecoQuant. Through extensive experiments, DecoQuant
demonstrates remarkable efficiency gains, showcasing up to a $\sim$75\%
reduction in memory footprint while maintaining comparable generation quality.

ÊëòË¶ÅÔºöÈóúÈçµÂÄº~(KV) Âø´ÂèñÊòØ‰∏ÄÁ®ÆÂä†ÈÄüÂ§ßÂûãË™ûË®ÄÊ®°Âûã~(LLM) Êé®Ë´ñÁöÑÈáçË¶ÅÊäÄË°ìÔºå‰ΩÜÊúÉÈÄ†ÊàêÈ°ØËëóÁöÑË®òÊÜ∂È´îÈñãÈä∑„ÄÇÁÇ∫‰∫ÜÂ£ìÁ∏Æ KV Âø´ÂèñÁöÑÂ§ßÂ∞èÔºåÁèæÊúâÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÊäòË°∑Á≤æÂ∫¶ÊàñÈúÄË¶ÅÈ°çÂ§ñÁöÑÊ†°Ê≠£Ë≥áÊñôÔºåÈÄôÊúÉÈôêÂà∂ÂÆÉÂÄëÂú® LLM ÈÉ®ÁΩ≤‰∏≠ÁöÑÂØ¶Áî®ÊÄß„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü \textbf{DecoQuant}ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂü∫ÊñºÂºµÈáèÂàÜËß£ÊñπÊ≥ïÁöÑÊñ∞ÂûãÁÑ°Ë≥áÊñô‰Ωé‰ΩçÂÖÉÈáèÂåñÊäÄË°ìÔºåÂèØÊúâÊïàÂ£ìÁ∏Æ KV Âø´Âèñ„ÄÇÊàëÂÄëÁöÑÊ†∏ÂøÉÊ¶ÇÂøµÊòØÈÄèÈÅéÂü∑Ë°åÂºµÈáèÂàÜËß£‰æÜË™øÊï¥ÂéüÂßãÁü©Èô£ÁöÑÁï∞Â∏∏ÂÄºÂàÜ‰ΩàÔºå‰ª•‰æøÂ∞áÈáèÂåñÈõ£Â∫¶ÂæûÁü©Èô£ËΩâÁßªÂà∞ÂàÜËß£ÁöÑÂ±ÄÈÉ®ÂºµÈáè„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÁôºÁèæÁï∞Â∏∏ÂÄº‰∏ªË¶ÅÈõÜ‰∏≠Âú®Â∞èÂûãÂ±ÄÈÉ®ÂºµÈáè‰∏äÔºåËÄåÂ§ßÂûãÂºµÈáèÂæÄÂæÄÂÖ∑ÊúâËºÉÁ™ÑÁöÑÂÄºÂüü„ÄÇÊ†πÊìöÈÄôÂÄãÁôºÁèæÔºåÊàëÂÄëÂª∫Ë≠∞Â∞á‰Ωé‰ΩçÂÖÉÈáèÂåñÊáâÁî®ÊñºÂ§ßÂûãÂºµÈáèÔºåÂêåÊôÇÁ∂≠ÊåÅÂ∞èÂûãÂºµÈáèÁöÑÁ≤æÁ¢∫Â∫¶Ë°®Á§∫„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂà©Áî®ÊâÄÊèêÂá∫ÁöÑÈáèÂåñÊñπÊ≥ï‰æÜÂ£ìÁ∏Æ LLM ÁöÑ KV Âø´ÂèñÔºå‰ª•Âä†ÈÄüÊé®Ë´ñÔºå‰∏¶ÈñãÁôº‰∫Ü‰∏ÄÂÄãÂ∞àÈñÄÈáùÂ∞ç DecoQuant ÁöÑÈ´òÊïàÂéªÈáèÂåñÊ†∏ÂøÉ„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑÂØ¶È©óÔºåDecoQuant Â±ïÁ§∫‰∫ÜÈ°ØËëóÁöÑÊïàÁéáÊèêÂçáÔºåÂ±ïÁ§∫Âá∫Ë®òÊÜ∂È´î‰ΩøÁî®ÈáèÊ∏õÂ∞ë‰∫ÜÁ¥Ñ $\sim$75%ÔºåÂêåÊôÇÁ∂≠ÊåÅ‰∫ÜÂèØÊØîËºÉÁöÑÁî¢ÁîüÂìÅË≥™„ÄÇ

##### **Mining the Explainability and Generalization: Fact Verification Based on Self-Instruction**
2405.12579v1 by Guangyao Lu, Yulin Liu

Fact-checking based on commercial LLMs has become mainstream. Although these
methods offer high explainability, it falls short in accuracy compared to
traditional fine-tuning approaches, and data security is also a significant
concern. In this paper, we propose a self-instruction based fine-tuning
approach for fact-checking that balances accuracy and explainability. Our
method consists of Data Augmentation and Improved DPO fine-tuning. The former
starts by instructing the model to generate both positive and negative
explanations based on claim-evidence pairs and labels, then sampling the
dataset according to our customized difficulty standards. The latter employs
our proposed improved DPO to fine-tune the model using the generated samples.
We fine-tune the smallest-scale LLaMA-7B model and evaluate it on the
challenging fact-checking datasets FEVEROUS and HOVER, utilizing four
fine-tuning methods and three few-shot learning methods for comparison. The
experiments demonstrate that our approach not only retains accuracy comparable
to, or even surpassing, traditional fine-tuning methods, but also generates
fluent explanation text. Moreover, it also exhibit high generalization
performance. Our method is the first to leverage self-supervised learning for
fact-checking and innovatively combines contrastive learning and improved DPO
in fine-tuning LLMs, as shown in the experiments.

ÊëòË¶ÅÔºöÂü∫ÊñºÂïÜÊ•≠ LLM ÁöÑ‰∫ãÂØ¶Êü•Ê†∏Â∑≤ÊàêÁÇ∫‰∏ªÊµÅ„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊñπÊ≥ïÊèê‰æõ‰∫ÜÈ´òÂ∫¶ÁöÑÂèØËß£ÈáãÊÄßÔºå‰ΩÜËàáÂÇ≥Áµ±ÁöÑÂæÆË™øÊñπÊ≥ïÁõ∏ÊØîÔºåÂÖ∂Ê∫ñÁ¢∫ÊÄßËºÉÂ∑ÆÔºåËÄå‰∏îË≥áÊñôÂÆâÂÖ®ÊÄß‰πüÊòØ‰∏ÄÂÄãÈáçÂ§ßÁöÑÂïèÈ°å„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºËá™ÊïôÂ≠∏ÁöÑÂæÆË™øÊñπÊ≥ïÔºåÁî®ÊñºÂú®Ê∫ñÁ¢∫ÊÄßÂíåÂèØËß£ÈáãÊÄß‰πãÈñìÂèñÂæóÂπ≥Ë°°„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂåÖÊã¨Ë≥áÊñôÊì¥ÂÖÖÂíåÊîπÈÄ≤ÁöÑ DPO ÂæÆË™ø„ÄÇÂâçËÄÖÈ¶ñÂÖàÊåáÂ∞éÊ®°ÂûãÊ†πÊìöËÅ≤Êòé-Ë≠âÊìöÂ∞çÂíåÊ®ôÁ±§ÁîüÊàêÊ≠£Èù¢ÂíåË≤†Èù¢ÁöÑËß£ÈáãÔºåÁÑ∂ÂæåÊ†πÊìöÊàëÂÄëËá™Ë®ÇÁöÑÈõ£Â∫¶Ê®ôÊ∫ñÂ∞çË≥áÊñôÈõÜÈÄ≤Ë°åÊäΩÊ®£„ÄÇÂæåËÄÖÊé°Áî®ÊàëÂÄëÊèêÂá∫ÁöÑÊîπÈÄ≤ DPO ‰æÜ‰ΩøÁî®ÁîüÊàêÁöÑÊ®£Êú¨ÂæÆË™øÊ®°Âûã„ÄÇÊàëÂÄëÂæÆË™ø‰∫ÜÊúÄÂ∞èË¶èÊ®°ÁöÑ LLaMA-7B Ê®°ÂûãÔºå‰∏¶Âú®ÂÖ∑ÊúâÊåëÊà∞ÊÄßÁöÑ‰∫ãÂØ¶Êü•Ê†∏Ë≥áÊñôÈõÜ FEVEROUS Âíå HOVER ‰∏äÂ∞çÂÖ∂ÈÄ≤Ë°åË©ï‰º∞ÔºåÂà©Áî®ÂõõÁ®ÆÂæÆË™øÊñπÊ≥ïÂíå‰∏âÁ®ÆÂ∞ëÊ®£Êú¨Â≠∏ÁøíÊñπÊ≥ïÈÄ≤Ë°åÊØîËºÉ„ÄÇÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÂÅöÊ≥ï‰∏çÂÉÖ‰øùÊåÅ‰∫ÜËàáÂÇ≥Áµ±ÂæÆË™øÊñπÊ≥ïÁõ∏Áï∂ÁîöËá≥Êõ¥È´òÁöÑÊ∫ñÁ¢∫ÊÄßÔºåËÄå‰∏îÈÇÑÁîüÊàê‰∫ÜÊµÅÊö¢ÁöÑËß£ÈáãÊñáÂ≠ó„ÄÇÊ≠§Â§ñÔºåÂÆÉÈÇÑË°®ÁèæÂá∫ÂæàÈ´òÁöÑÊ≥õÂåñÊïàËÉΩ„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÊòØÁ¨¨‰∏ÄÂÄãÂà©Áî®Ëá™ÊàëÁõ£Áù£Â≠∏ÁøíÈÄ≤Ë°å‰∫ãÂØ¶Êü•Ê†∏Ôºå‰∏¶Âú®ÂæÆË™ø LLM ‰∏≠ÂâµÊñ∞Âú∞ÁµêÂêà‰∫ÜÂ∞çÊØîÂ≠∏ÁøíÂíåÊîπÈÄ≤ÁöÑ DPOÔºåÂ¶ÇÂØ¶È©óÊâÄÁ§∫„ÄÇ

##### **ProtT3: Protein-to-Text Generation for Text-based Protein Understanding**
2405.12564v1 by Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, Tat-Seng Chua

Language Models (LMs) excel in understanding textual descriptions of
proteins, as evident in biomedical question-answering tasks. However, their
capability falters with raw protein data, such as amino acid sequences, due to
a deficit in pretraining on such data. Conversely, Protein Language Models
(PLMs) can understand and convert protein data into high-quality
representations, but struggle to process texts. To address their limitations,
we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based
Protein Understanding. ProtT3 empowers an LM to understand protein sequences of
amino acids by incorporating a PLM as its protein understanding module,
enabling effective protein-to-text generation. This collaboration between PLM
and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges
the modality gap between the PLM's representation space and the LM's input
space. Unlike previous studies focusing on protein property prediction and
protein-text retrieval, we delve into the largely unexplored field of
protein-to-text generation. To facilitate comprehensive benchmarks and promote
future research, we establish quantitative evaluations for protein-text
modeling tasks, including protein captioning, protein question-answering, and
protein-text retrieval. Our experiments show that ProtT3 substantially
surpasses current baselines, with ablation studies further highlighting the
efficacy of its core components. Our code is available at
https://github.com/acharkq/ProtT3.

ÊëòË¶ÅÔºöË™ûË®ÄÊ®°Âûã (LM) ÊìÖÈï∑ÁêÜËß£ËõãÁôΩË≥™ÁöÑÊñáÂ≠óÊèèËø∞ÔºåÈÄôÂú®ÁîüÁâ©ÈÜ´Â≠∏ÂïèÈ°åËß£Á≠î‰ªªÂãô‰∏≠ÂæàÊòéÈ°Ø„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÁº∫‰πèÊ≠§È°ûÊï∏ÊìöÁöÑÈ†êË®ìÁ∑¥ÔºåÂÆÉÂÄëÂú®ËôïÁêÜÂéüÂßãËõãÁôΩË≥™Êï∏ÊìöÔºà‰æãÂ¶ÇËÉ∫Âü∫ÈÖ∏Â∫èÂàóÔºâÊôÇÊúÉÂá∫ÁèæËÉΩÂäõ‰∏çË∂≥ÁöÑÊÉÖÊ≥Å„ÄÇÁõ∏ÂèçÂú∞ÔºåËõãÁôΩË≥™Ë™ûË®ÄÊ®°Âûã (PLM) ÂèØ‰ª•ÁêÜËß£‰∏¶Â∞áËõãÁôΩË≥™Êï∏ÊìöËΩâÊèõÁÇ∫È´òÂìÅË≥™ÁöÑË°®Á§∫Ôºå‰ΩÜÈõ£‰ª•ËôïÁêÜÊñáÊú¨„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÂÆÉÂÄëÁöÑÈôêÂà∂ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü ProtT3Ôºå‰∏ÄÂÄãËõãÁôΩË≥™Âà∞ÊñáÊú¨ÁîüÊàêÊ°ÜÊû∂ÔºåÁî®ÊñºÂü∫ÊñºÊñáÊú¨ÁöÑËõãÁôΩË≥™ÁêÜËß£„ÄÇProtT3 ÈÄèÈÅéÂ∞á PLM Á¥çÂÖ•ÂÖ∂ËõãÁôΩË≥™ÁêÜËß£Ê®°ÁµÑÔºåË≥¶‰∫à LM ÁêÜËß£ËÉ∫Âü∫ÈÖ∏ËõãÁôΩË≥™Â∫èÂàóÁöÑËÉΩÂäõÔºåÈÄ≤ËÄåËÉΩÊúâÊïàÂú∞ÈÄ≤Ë°åËõãÁôΩË≥™Âà∞ÊñáÊú¨ÁöÑÁîüÊàê„ÄÇPLM Âíå LM ‰πãÈñìÁöÑÈÄôÁ®ÆÂçî‰ΩúÊòØÁî±Ë∑®Ê®°ÊÖãÊäïÂΩ±Âô®ÔºàÂç≥ Q-FormerÔºâ‰øÉÊàêÁöÑÔºåÂÆÉÂΩåÂêà‰∫Ü PLM ÁöÑË°®Á§∫Á©∫ÈñìÂíå LM ÁöÑËº∏ÂÖ•Á©∫Èñì‰πãÈñìÁöÑÊ®°ÊÖãÂ∑ÆË∑ù„ÄÇËàáÂÖàÂâçÂ∞àÊ≥®ÊñºËõãÁôΩË≥™Â±¨ÊÄßÈ†êÊ∏¨ÂíåËõãÁôΩË≥™Âà∞ÊñáÊú¨Ê™¢Á¥¢ÁöÑÁ†îÁ©∂‰∏çÂêåÔºåÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®é‰∫ÜËõãÁôΩË≥™Âà∞ÊñáÊú¨ÁîüÊàêÁöÑÂª£ÈóäÊú™Êé¢Á¥¢È†òÂüü„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤ÂÖ®Èù¢ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÂíåÊú™‰æÜÁöÑÁ†îÁ©∂ÔºåÊàëÂÄëÁÇ∫ËõãÁôΩË≥™Âà∞ÊñáÊú¨Âª∫Ê®°‰ªªÂãôÔºàÂåÖÊã¨ËõãÁôΩË≥™Ê®ôË®ª„ÄÅËõãÁôΩË≥™ÂïèÈ°åËß£Á≠îÂíåËõãÁôΩË≥™Âà∞ÊñáÊú¨Ê™¢Á¥¢ÔºâÂª∫Á´ã‰∫ÜÂÆöÈáèË©ï‰º∞„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåProtT3 ÊòéÈ°ØÂÑ™ÊñºÁõÆÂâçÁöÑÂü∫Ê∫ñÔºåÊ∂àËûçÁ†îÁ©∂ÈÄ≤‰∏ÄÊ≠•Á™ÅÂá∫‰∫ÜÂÖ∂Ê†∏ÂøÉÁµÑ‰ª∂ÁöÑÂäüÊïà„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/acharkq/ProtT3 ÂèñÂæó„ÄÇ

##### **DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing Outcomes from Sensor Data and Expert Knowledge**
2405.12541v1 by Bufang Yang, Siyang Jiang, Lilin Xu, Kaiwei Liu, Hai Li, Guoliang Xing, Hongkai Chen, Xiaofan Jiang, Zhenyu Yan

Large language models (LLMs) have the potential to transform digital
healthcare, as evidenced by recent advances in LLM-based virtual doctors.
However, current approaches rely on patient's subjective descriptions of
symptoms, causing increased misdiagnosis. Recognizing the value of daily data
from smart devices, we introduce a novel LLM-based multi-turn consultation
virtual doctor system, DrHouse, which incorporates three significant
contributions: 1) It utilizes sensor data from smart devices in the diagnosis
process, enhancing accuracy and reliability. 2) DrHouse leverages continuously
updating medical databases such as Up-to-Date and PubMed to ensure our model
remains at diagnostic standard's forefront. 3) DrHouse introduces a novel
diagnostic algorithm that concurrently evaluates potential diseases and their
likelihood, facilitating more nuanced and informed medical assessments. Through
multi-turn interactions, DrHouse determines the next steps, such as accessing
daily data from smart devices or requesting in-lab tests, and progressively
refines its diagnoses. Evaluations on three public datasets and our
self-collected datasets show that DrHouse can achieve up to an 18.8% increase
in diagnosis accuracy over the state-of-the-art baselines. The results of a
32-participant user study show that 75% medical experts and 91.7% patients are
willing to use DrHouse.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊúâÊΩõÂäõËΩâËÆäÊï∏‰ΩçÈÜ´ÁôÇ‰øùÂÅ•ÔºåÈÄôÈªûÂæû LLM ÁÇ∫Âü∫Á§éÁöÑËôõÊì¨ÈÜ´Â∏´ÊúÄËøëÁöÑÈÄ≤Â±ï‰∏≠Â∞±ËÉΩÁúãÂá∫„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÂÅöÊ≥ï‰æùË≥¥ÊñºÊÇ£ËÄÖ‰∏ªËßÄÁöÑÁóáÁãÄÊèèËø∞ÔºåÂ∞éËá¥Ë™§Ë®∫Â¢ûÂä†„ÄÇÊàëÂÄëË™çË≠òÂà∞‰æÜËá™Êô∫ÊÖßË£ùÁΩÆÁöÑÊØèÊó•Ë≥áÊñôÂÉπÂÄºÔºåÂõ†Ê≠§ÊàëÂÄëÂºïÈÄ≤‰∏ÄÂÄãÊñ∞Á©éÁöÑ LLM ÁÇ∫Âü∫Á§éÁöÑÂ§öËº™Ë´ÆË©¢ËôõÊì¨ÈÜ´Â∏´Á≥ªÁµ± DrHouseÔºåÂÆÉÂåÖÂê´‰∏âÂÄãÈáçË¶ÅÁöÑË≤¢ÁçªÔºö1) ÂÆÉÂú®Ë®∫Êñ∑ÈÅéÁ®ã‰∏≠Âà©Áî®‰æÜËá™Êô∫ÊÖßË£ùÁΩÆÁöÑÊÑüÊ∏¨Âô®Ë≥áÊñôÔºåÊèêÂçáÊ∫ñÁ¢∫Â∫¶ÂíåÂèØÈù†Â∫¶„ÄÇ2) DrHouse ÂÖÖÂàÜÂà©Áî®ÊåÅÁ∫åÊõ¥Êñ∞ÁöÑÈÜ´ÁôÇË≥áÊñôÂ∫´Ôºå‰æãÂ¶Ç Up-to-Date Âíå PubMedÔºå‰ª•Á¢∫‰øùÊàëÂÄëÁöÑÊ®°ÂûãÁ∂≠ÊåÅÂú®Ë®∫Êñ∑Ê®ôÊ∫ñÁöÑÊúÄÂâçÁ∑ö„ÄÇ3) DrHouse ÂºïÈÄ≤‰∏ÄÁ®ÆÊñ∞Á©éÁöÑË®∫Êñ∑ÊºîÁÆóÊ≥ïÔºåÂÆÉÂêåÊôÇË©ï‰º∞ÊΩõÂú®ÁñæÁóÖÂèäÂÖ∂ÂèØËÉΩÊÄßÔºå‰øÉÈÄ≤Êõ¥Á¥∞Á∑ª‰∏îÊòéÊô∫ÁöÑÈÜ´ÁôÇË©ï‰º∞„ÄÇÈÄèÈÅéÂ§öËº™‰∫íÂãïÔºåDrHouse Ê±∫ÂÆö‰∏ã‰∏ÄÊ≠•Ôºå‰æãÂ¶ÇÂèñÂæó‰æÜËá™Êô∫ÊÖßË£ùÁΩÆÁöÑÊØèÊó•Ë≥áÊñôÊàñË¶ÅÊ±ÇÂØ¶È©óÂÆ§Ê™¢È©óÔºå‰∏¶ÈÄêÊ≠•ÊîπÂñÑÂÖ∂Ë®∫Êñ∑„ÄÇÈáùÂ∞ç‰∏âÂÄãÂÖ¨ÈñãË≥áÊñôÈõÜÂíåÊàëÂÄëËá™Ë°åÊî∂ÈõÜÁöÑË≥áÊñôÈõÜÈÄ≤Ë°åË©ï‰º∞È°ØÁ§∫ÔºåDrHouse Âú®Ë®∫Êñ∑Ê∫ñÁ¢∫Â∫¶ÊñπÈù¢ÂèØ‰ª•ÊØîÁèæÊúâÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÁ∑öÊèêÈ´ò 18.8%„ÄÇ‰∏ÄÂÄãÊúâ 32 ‰ΩçÂèÉËàáËÄÖÁöÑ‰ΩøÁî®ËÄÖÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫Ôºå75% ÁöÑÈÜ´ÁôÇÂ∞àÂÆ∂Âíå 91.7% ÁöÑÊÇ£ËÄÖÈ°òÊÑè‰ΩøÁî® DrHouse„ÄÇ

##### **PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference**
2405.12532v1 by Dongjie Yang, XiaoDong Han, Yan Gao, Yao Hu, Shilin Zhang, Hai Zhao

Large Language Models (LLMs) have shown remarkable comprehension abilities
but face challenges in GPU memory usage during inference, hindering their
scalability for real-time applications like chatbots. To accelerate inference,
we store computed keys and values (KV cache) in the GPU memory. Existing
methods study the KV cache compression to reduce memory by pruning the
pre-computed KV cache. However, they neglect the inter-layer dependency between
layers and huge memory consumption in pre-computation. To explore these
deficiencies, we find that the number of crucial keys and values that influence
future generations decreases layer by layer and we can extract them by the
consistency in attention weights. Based on the findings, we propose
PyramidInfer, a method that compresses the KV cache by layer-wise retaining
crucial context. PyramidInfer saves significant memory by computing fewer keys
and values without sacrificing performance. Experimental results show
PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU
memory reduction in KV cache.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Â±ïÁèæÂá∫ÂçìË∂äÁöÑÁêÜËß£ËÉΩÂäõÔºå‰ΩÜÂú®Êé®Ë´ñÈÅéÁ®ã‰∏≠Èù¢Ëá® GPU Ë®òÊÜ∂È´î‰ΩøÁî®ÁéáÁöÑÊåëÊà∞ÔºåÈòªÁ§ô‰∫ÜÂÆÉÂÄëÂú®ËÅäÂ§©Ê©üÂô®‰∫∫Á≠âÂç≥ÊôÇÊáâÁî®Á®ãÂºè‰∏≠ÁöÑÂèØÊì¥ÂÖÖÊÄß„ÄÇÁÇ∫‰∫ÜÂä†ÈÄüÊé®Ë´ñÔºåÊàëÂÄëÂ∞áË®àÁÆóÂá∫ÁöÑÈáëÈë∞ÂíåÂÄº (KV Âø´Âèñ) ÂÑ≤Â≠òÂú® GPU Ë®òÊÜ∂È´î‰∏≠„ÄÇÁèæÊúâÊñπÊ≥ïÁ†îÁ©∂ KV Âø´ÂèñÂ£ìÁ∏ÆÔºåÈÄèÈÅé‰øÆÂâ™È†êÂÖàË®àÁÆóÁöÑ KV Âø´Âèñ‰æÜÊ∏õÂ∞ëË®òÊÜ∂È´î„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄëÂøΩÁï•‰∫ÜÂ±§ËàáÂ±§‰πãÈñìÁöÑÂ±§ÈñìÁõ∏‰æùÊÄßÔºå‰ª•ÂèäÈ†êÂÖàË®àÁÆó‰∏≠ÁöÑÈæêÂ§ßË®òÊÜ∂È´îÊ∂àËÄó„ÄÇÁÇ∫‰∫ÜÊé¢Ë®éÈÄô‰∫õ‰∏çË∂≥‰πãËôïÔºåÊàëÂÄëÁôºÁèæÂΩ±ÈüøÂæå‰ª£ÁöÑÈáëÈë∞ÂíåÂÄºÊï∏ÈáèÊúÉÈÄêÂ±§ÈÅûÊ∏õÔºåËÄå‰∏îÊàëÂÄëÂèØ‰ª•ÈÄèÈÅéÊ≥®ÊÑèÂäõÊ¨äÈáçÁöÑÁõ∏ÂÆπÊÄß‰æÜËêÉÂèñÂÆÉÂÄë„ÄÇÊ†πÊìöÈÄô‰∫õÁôºÁèæÔºåÊàëÂÄëÊèêÂá∫ PyramidInferÔºåÈÄôÊòØ‰∏ÄÁ®ÆÈÄèÈÅéÈÄêÂ±§‰øùÁïôÈóúÈçµÂÖßÂÆπ‰æÜÂ£ìÁ∏Æ KV Âø´ÂèñÁöÑÊñπÊ≥ï„ÄÇPyramidInfer ÈÄèÈÅéË®àÁÆóËºÉÂ∞ëÁöÑÈáëÈë∞ÂíåÂÄº‰æÜÁØÄÁúÅÂ§ßÈáèË®òÊÜ∂È´îÔºåÂêåÊôÇ‰∏çÁäßÁâ≤ÊïàËÉΩ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåËàá Accelerate Áõ∏ÊØîÔºåPyramidInfer Â∞á KV Âø´ÂèñÁöÑ GPU Ë®òÊÜ∂È´îÊ∏õÂ∞ë‰∫Ü 54% ‰ª•‰∏äÔºåÂêåÊôÇÂ∞áËôïÁêÜÈáèÊèêÂçá‰∫Ü 2.2 ÂÄç„ÄÇ

##### **SirLLM: Streaming Infinite Retentive LLM**
2405.12528v1 by Yao Yao, Zuchao Li, Hai Zhao

As Large Language Models (LLMs) become increasingly prevalent in various
domains, their ability to process inputs of any length and maintain a degree of
memory becomes essential. However, the one-off input of overly long texts is
limited, as studies have shown that when input lengths exceed the LLMs'
pre-trained text length, there is a dramatic decline in text generation
capabilities. Moreover, simply extending the length of pre-training texts is
impractical due to the difficulty in obtaining long text data and the
substantial memory consumption costs this would entail for LLMs. Recent efforts
have employed streaming inputs to alleviate the pressure of excessively long
text inputs, but this approach can significantly impair the model's long-term
memory capabilities.
  Motivated by this challenge, we introduce Streaming Infinite Retentive LLM
(SirLLM), which allows LLMs to maintain longer memory during infinite-length
dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy
metric and a memory decay mechanism to filter key phrases, endowing LLMs with
both long-lasting and flexible memory. We designed three distinct tasks and
constructed three datasets to measure the effectiveness of SirLLM from various
angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our
experimental results robustly demonstrate that SirLLM can achieve stable and
significant improvements across different LLMs and tasks, compellingly proving
its effectiveness. When having a coversation, "A sir could forget himself," but
SirLLM never does! Our code is publicly available at
https://github.com/Zoeyyao27/SirLLM

ÊëòË¶ÅÔºö<paragraph>Èö®ËëóÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Âú®ÂêÑÁ®ÆÈ†òÂüü‰∏≠Êó•ÁõäÊôÆÂèäÔºåÂÆÉÂÄëËôïÁêÜ‰ªª‰ΩïÈï∑Â∫¶ÁöÑËº∏ÂÖ•‰∏¶‰øùÊåÅ‰∏ÄÂÆöÁ®ãÂ∫¶Ë®òÊÜ∂ÁöÑËÉΩÂäõËÆäÂæóËá≥ÈóúÈáçË¶Å„ÄÇÁÑ∂ËÄåÔºåÈÅéÂ∫¶ÂÜóÈï∑ÁöÑÊñáÊú¨ÁöÑ‰∏ÄÊ¨°ÊÄßËº∏ÂÖ•ÊòØÊúâÈôêÁöÑÔºåÂõ†ÁÇ∫Á†îÁ©∂Ë°®ÊòéÔºåÁï∂Ëº∏ÂÖ•Èï∑Â∫¶Ë∂ÖÈÅé LLM ÁöÑÈ†êË®ìÁ∑¥ÊñáÊú¨Èï∑Â∫¶ÊôÇÔºåÊñáÊú¨ÁîüÊàêËÉΩÂäõÊúÉÊÄ•Âäá‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÂÉÖÂÉÖÂª∂Èï∑È†êË®ìÁ∑¥ÊñáÊú¨ÁöÑÈï∑Â∫¶ÊòØ‰∏çÂàáÂØ¶ÈöõÁöÑÔºåÂõ†ÁÇ∫Èõ£‰ª•Áç≤ÂèñÈï∑ÊñáÊú¨Êï∏ÊìöÔºå‰∏¶‰∏îÈÄôÂ∞áÁµ¶ LLM Â∏∂‰æÜÂ∑®Â§ßÁöÑÂÖßÂ≠òÊ∂àËÄóÊàêÊú¨„ÄÇÊúÄËøëÁöÑÂä™ÂäõÊé°Áî®‰∏≤ÊµÅËº∏ÂÖ•‰æÜÁ∑©Ëß£ÈÅéÈï∑ÊñáÊú¨Ëº∏ÂÖ•ÁöÑÂ£ìÂäõÔºå‰ΩÜÈÄôÁ®ÆÊñπÊ≥ïÊúÉÈ°ØËëóÊêçÂÆ≥Ê®°ÂûãÁöÑÈï∑ÊúüË®òÊÜ∂ËÉΩÂäõ„ÄÇ
ÂèóÊ≠§ÊåëÊà∞ÁöÑÊøÄÂãµÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü‰∏≤ÊµÅÁÑ°Èôê‰øùÁïô LLM (SirLLM)ÔºåÂÆÉÂÖÅË®± LLM Âú®ÁÑ°ÈôêÈï∑ÁöÑÂ∞çË©±‰∏≠‰øùÊåÅÊõ¥Èï∑ÁöÑË®òÊÜ∂ÔºåËÄåÁÑ°ÈúÄÈÄ≤Ë°åÂæÆË™ø„ÄÇSirLLM Âà©Áî® Token Entropy Â∫¶ÈáèÂíåË®òÊÜ∂Ë°∞Ê∏õÊ©üÂà∂‰æÜÈÅéÊøæÈóúÈçµÁü≠Ë™ûÔºåË≥¶‰∫à LLM ÊåÅ‰πÖ‰∏îÈùàÊ¥ªÁöÑË®òÊÜ∂„ÄÇÊàëÂÄëË®≠Ë®à‰∫Ü‰∏âÂÄã‰∏çÂêåÁöÑ‰ªªÂãôÔºå‰∏¶ÊßãÂª∫‰∫Ü‰∏âÂÄãÊï∏ÊìöÈõÜÔºåÂæû‰∏çÂêåÁöÑËßíÂ∫¶Ë°°Èáè SirLLM ÁöÑÊúâÊïàÊÄßÔºö(1) DailyDialogÔºõ(2) Grocery ShoppingÔºõ(3) Rock-Paper-Scissors„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÊúâÂäõÂú∞Ë≠âÊòéÔºåSirLLM ÂèØ‰ª•Ë∑®‰∏çÂêåÁöÑ LLM Âíå‰ªªÂãôÂØ¶ÁèæÁ©©ÂÆö‰∏îÈ°ØËëóÁöÑÊîπÈÄ≤ÔºåÊúâÂäõÂú∞Ë≠âÊòé‰∫ÜÂÆÉÁöÑÊúâÊïàÊÄß„ÄÇÂú®ÈÄ≤Ë°åÂ∞çË©±ÊôÇÔºå‚Äú‰∏Ä‰ΩçÂÖàÁîüÂèØËÉΩÊúÉÂøòË®òËá™Â∑±‚ÄùÔºå‰ΩÜ SirLLM Ê∞∏ÈÅ†‰∏çÊúÉÔºÅÊàëÂÄëÁöÑ‰ª£Á¢ºÂèØÂú® https://github.com/Zoeyyao27/SirLLM ÂÖ¨ÈñãÁç≤Âæó</paragraph>

##### **Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models**
2405.12523v1 by Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, Sheng Bi

Machine unlearning empowers individuals with the `right to be forgotten' by
removing their private or sensitive information encoded in machine learning
models. However, it remains uncertain whether MU can be effectively applied to
Multimodal Large Language Models (MLLMs), particularly in scenarios of
forgetting the leaked visual data of concepts. To overcome the challenge, we
propose an efficient method, Single Image Unlearning (SIU), to unlearn the
visual recognition of a concept by fine-tuning a single associated image for
few steps. SIU consists of two key aspects: (i) Constructing Multifaceted
fine-tuning data. We introduce four targets, based on which we construct
fine-tuning data for the concepts to be forgotten; (ii) Jointly training loss.
To synchronously forget the visual recognition of concepts and preserve the
utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence
Loss combined with Cross Entropy loss. Alongside our method, we establish
MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics
for its evaluation. Experimental results on MMUBench show that SIU completely
surpasses the performance of existing methods. Furthermore, we surprisingly
find that SIU can avoid invasive membership inference attacks and jailbreak
attacks. To the best of our knowledge, we are the first to explore MU in MLLMs.
We will release the code and benchmark in the near future.

ÊëòË¶ÅÔºöÊ©üÂô®ÂéªÂ≠∏ÁøíË≥¶‰∫àÂÄã‰∫∫„ÄåË¢´ÈÅ∫ÂøòÁöÑÊ¨äÂà©„ÄçÔºåÊñπÊ≥ïÊòØÁßªÈô§Á∑®Á¢ºÂú®Ê©üÂô®Â≠∏ÁøíÊ®°Âûã‰∏≠ÁöÑÁßÅ‰∫∫ÊàñÊïèÊÑüË≥áË®ä„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâç‰ªç‰∏çÁ¢∫ÂÆö MU ÊòØÂê¶ËÉΩÊúâÊïàÂ•óÁî®Âú®Â§öÊ®°ÊÖãÂ§ßÂûãË™ûË®ÄÊ®°Âûã (MLLM)ÔºåÁâπÂà•ÊòØÂú®ÈÅ∫ÂøòÊ¶ÇÂøµÂ§ñÊ¥©Ë¶ñË¶∫Ë≥áÊñôÁöÑÂ†¥ÊôØ‰∏≠„ÄÇÁÇ∫‰∫ÜÂÖãÊúçÊ≠§ÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÊúâÊïàÁéáÁöÑÊñπÊ≥ïÔºåÂñÆ‰∏ÄÂΩ±ÂÉèÂéªÂ≠∏Áøí (SIU)ÔºåÈÄèÈÅéÂæÆË™øÂñÆ‰∏ÄÈóúËÅØÂΩ±ÂÉèÂπæÂÄãÊ≠•È©üÔºå‰æÜÂéªÂ≠∏ÁøíÊ¶ÇÂøµÁöÑË¶ñË¶∫Ëæ®Ë≠ò„ÄÇSIU ÂåÖÂê´ÂÖ©ÂÄãÈóúÈçµÈù¢ÂêëÔºö(i) Âª∫ÊßãÂ§öÈù¢ÂêëÂæÆË™øË≥áÊñô„ÄÇÊàëÂÄëÂºïÂÖ•ÂõõÂÄãÁõÆÊ®ôÔºåÊ†πÊìöÈÄô‰∫õÁõÆÊ®ôÂª∫ÊßãË¶ÅÈÅ∫ÂøòÊ¶ÇÂøµÁöÑÂæÆË™øË≥áÊñôÔºõ(ii) ËÅØÂêàË®ìÁ∑¥ÊêçÂ§±„ÄÇÁÇ∫‰∫ÜÂêåÊ≠•ÈÅ∫ÂøòÊ¶ÇÂøµÁöÑË¶ñË¶∫Ëæ®Ë≠òÔºå‰∏¶‰øùÁïô MLLM ÁöÑÊïàÁî®ÔºåÊàëÂÄëÈÄèÈÅé‰∏ÄÂÄãÊñ∞Á©éÁöÑÈõôÈÅÆÁΩ© KL-Êï£Â∫¶ÊêçÂ§±ÔºåÁµêÂêà‰∫§ÂèâÁÜµÊêçÂ§±Ôºå‰æÜÂæÆË™ø MLLM„ÄÇÈô§‰∫ÜÊàëÂÄëÁöÑÊñπÊ≥ï‰πãÂ§ñÔºåÊàëÂÄëÈÇÑÂª∫Á´ã‰∫Ü MMUBenchÔºå‰∏ÄÂÄã MLLM ‰∏≠ MU ÁöÑÊñ∞Âü∫Ê∫ñÔºå‰∏¶ÂºïÂÖ•‰∏ÄÁµÑÁî®ÊñºË©ï‰º∞ÁöÑÊåáÊ®ô„ÄÇMMUBench ‰∏äÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåSIU ÂÆåÂÖ®Ë∂ÖË∂äÁèæÊúâÊñπÊ≥ïÁöÑÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈ©öË®ùÂú∞ÁôºÁèæÔºåSIU ÂèØ‰ª•ÈÅøÂÖç‰æµÂÖ•ÊÄßÁöÑÊàêÂì°Êé®Ë´ñÊîªÊìäÂíåË∂äÁçÑÊîªÊìä„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÊàëÂÄëÊòØÁ¨¨‰∏ÄÂÄãÊé¢Á¥¢ MLLM ‰∏≠ MU ÁöÑ‰∫∫„ÄÇÊàëÂÄëÂ∞áÂú®‰∏ç‰πÖÁöÑÂ∞á‰æÜÈáãÂá∫Á®ãÂºèÁ¢ºÂíåÂü∫Ê∫ñ„ÄÇ

##### **Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models**
2405.12522v1 by Charles O'Neill, Thang Bui

This paper introduces an efficient and robust method for discovering
interpretable circuits in large language models using discrete sparse
autoencoders. Our approach addresses key limitations of existing techniques,
namely computational complexity and sensitivity to hyperparameters. We propose
training sparse autoencoders on carefully designed positive and negative
examples, where the model can only correctly predict the next token for the
positive examples. We hypothesise that learned representations of attention
head outputs will signal when a head is engaged in specific computations. By
discretising the learned representations into integer codes and measuring the
overlap between codes unique to positive examples for each head, we enable
direct identification of attention heads involved in circuits without the need
for expensive ablations or architectural modifications. On three well-studied
tasks - indirect object identification, greater-than comparisons, and docstring
completion - the proposed method achieves higher precision and recall in
recovering ground-truth circuits compared to state-of-the-art baselines, while
reducing runtime from hours to seconds. Notably, we require only 5-10 text
examples for each task to learn robust representations. Our findings highlight
the promise of discrete sparse autoencoders for scalable and efficient
mechanistic interpretability, offering a new direction for analysing the inner
workings of large language models.

ÊëòË¶ÅÔºöÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÁ®Æ‰ΩøÁî®Èõ¢Êï£Á®ÄÁñèËá™ÂãïÁ∑®Á¢ºÂô®Âú®Â§ßÂûãË™ûË®ÄÊ®°Âûã‰∏≠ÁôºÁèæÂèØËß£ÈáãÈõªË∑ØÁöÑÊúâÊïà‰∏îÁ©©ÂÅ•ÁöÑÊñπÊ≥ï„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïËß£Ê±∫‰∫ÜÁèæÊúâÊäÄË°ìÁöÑ‰∏ªË¶ÅÈôêÂà∂ÔºåÂç≥Ë®àÁÆóË§áÈõúÂ∫¶ÂíåÂ∞çË∂ÖÂèÉÊï∏ÁöÑÊïèÊÑüÊÄß„ÄÇÊàëÂÄëÂª∫Ë≠∞Âú®Á≤æÂøÉË®≠Ë®àÁöÑÊ≠£Ë≤†ÁØÑ‰æã‰∏äË®ìÁ∑¥Á®ÄÁñèËá™ÂãïÁ∑®Á¢ºÂô®ÔºåÂÖ∂‰∏≠Ê®°ÂûãÂè™ËÉΩÊ≠£Á¢∫È†êÊ∏¨Ê≠£ÁØÑ‰æãÁöÑ‰∏ã‰∏Ä‰ª£Âπ£„ÄÇÊàëÂÄëÂÅáË®≠Ê≥®ÊÑèÈ†≠Ëº∏Âá∫Â≠∏ÁøíÂà∞ÁöÑË°®Á§∫ÊúÉÂú®È†≠ÈÉ®ÂèÉËàáÁâπÂÆöË®àÁÆóÊôÇÁôºÂá∫‰ø°Ëôü„ÄÇÈÄöÈÅéÂ∞áÂ≠∏ÁøíÂà∞ÁöÑË°®Á§∫Èõ¢Êï£ÂåñÁÇ∫Êï¥Êï∏‰ª£Á¢º‰∏¶Ê∏¨ÈáèÊØèÂÄãÈ†≠ÈÉ®ÁöÑÊ≠£ÁØÑ‰æãÂîØ‰∏Ä‰ª£Á¢º‰πãÈñìÁöÑÈáçÁñäÔºåÊàëÂÄëÂèØ‰ª•Âú®ÁÑ°ÈúÄÊòÇË≤¥ÁöÑÊ∂àËûçÊàñÊû∂Êßã‰øÆÊîπÁöÑÊÉÖÊ≥Å‰∏ãÁõ¥Êé•Ë≠òÂà•ÂèÉËàáÈõªË∑ØÁöÑÊ≥®ÊÑèÈ†≠ÈÉ®„ÄÇÂú®‰∏âÈ†ÖÁ†îÁ©∂ËâØÂ•ΩÁöÑ‰ªªÂãôÔºàÈñìÊé•Â∞çË±°Ë≠òÂà•„ÄÅÂ§ßÊñºÊØîËºÉÂíåÊñá‰ª∂Â≠ó‰∏≤ÂÆåÊàêÔºâ‰∏≠ÔºåÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïÂú®ÊÅ¢Âæ©ÁúüÂØ¶ÈõªË∑ØÊñπÈù¢ÂØ¶Áèæ‰∫ÜÊØîÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÊõ¥È´òÁöÑÁ≤æÁ¢∫Â∫¶ÂíåÂè¨ÂõûÁéáÔºåÂêåÊôÇÂ∞áÈÅãË°åÊôÇÈñìÂæûÊï∏Â∞èÊôÇÁ∏ÆÁü≠Âà∞Êï∏Áßí„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÊØèÂÄã‰ªªÂãôÂè™ÈúÄË¶Å 5-10 ÂÄãÊñáÊú¨ÁØÑ‰æã‰æÜÂ≠∏ÁøíÁ©©ÂÅ•ÁöÑË°®Á§∫„ÄÇÊàëÂÄëÁöÑÁôºÁèæÁ™ÅÂá∫‰∫ÜÈõ¢Êï£Á®ÄÁñèËá™ÂãïÁ∑®Á¢ºÂô®Âú®ÂèØÊì¥ÂÖÖ‰∏îÊúâÊïàÊ©üÂà∂ÂèØËß£ÈáãÊÄßÊñπÈù¢ÁöÑÊΩõÂäõÔºåÁÇ∫ÂàÜÊûêÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂÖßÈÉ®ÈÅã‰ΩúÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊñπÂêë„ÄÇ

##### **MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation**
2405.12519v1 by Zhaoning Yu, Hongyang Gao

Graph Neural Networks (GNNs) have shown remarkable success in molecular
tasks, yet their interpretability remains challenging. Traditional model-level
explanation methods like XGNN and GNNInterpreter often fail to identify valid
substructures like rings, leading to questionable interpretability. This
limitation stems from XGNN's atom-by-atom approach and GNNInterpreter's
reliance on average graph embeddings, which overlook the essential structural
elements crucial for molecules. To address these gaps, we introduce an
innovative \textbf{M}otif-b\textbf{A}sed \textbf{G}NN \textbf{E}xplainer (MAGE)
that uses motifs as fundamental units for generating explanations. Our approach
begins with extracting potential motifs through a motif decomposition
technique. Then, we utilize an attention-based learning method to identify
class-specific motifs. Finally, we employ a motif-based graph generator for
each class to create molecular graph explanations based on these class-specific
motifs. This novel method not only incorporates critical substructures into the
explanations but also guarantees their validity, yielding results that are
human-understandable. Our proposed method's effectiveness is demonstrated
through quantitative and qualitative assessments conducted on six real-world
molecular datasets.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Âú®ÂàÜÂ≠ê‰ªªÂãô‰∏≠Â±ïÁèæÂá∫È°ØËëóÁöÑÊàêÂäüÔºå‰ΩÜÂÖ∂ÂèØËß£ÈáãÊÄß‰ªçÂÖ∑ÊåëÊà∞ÊÄß„ÄÇÂÇ≥Áµ±ÁöÑÊ®°ÂûãÂ±§Á¥öËß£ÈáãÊñπÊ≥ïÔºåÂ¶Ç XGNN Âíå GNNInterpreterÔºåÈÄöÂ∏∏ÁÑ°Ê≥ïË≠òÂà•ÊúâÊïàÁöÑÂ≠êÁµêÊßãÔºà‰æãÂ¶ÇÁí∞ÔºâÔºåÂ∞éËá¥ÂèØËß£ÈáãÊÄßÊúâÁñëÊÖÆ„ÄÇÊ≠§ÈôêÂà∂Ê∫êËá™Êñº XGNN ÁöÑÈÄêÂéüÂ≠êÊñπÊ≥ïÔºå‰ª•Âèä GNNInterpreter ‰æùË≥¥ÊñºÂπ≥ÂùáÂúñÂΩ¢ÂµåÂÖ•ÔºåÈÄôÂøΩÁï•‰∫ÜÂ∞çÂàÜÂ≠êËá≥ÈóúÈáçË¶ÅÁöÑÂü∫Êú¨ÁµêÊßãÂÖÉÁ¥†„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÈÄ≤‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑÂü∫ÊñºÂü∫Â∫èÁöÑ GNN Ëß£ÈáãÂô® (MAGE)ÔºåÂÆÉ‰ΩøÁî®Âü∫Â∫è‰ΩúÁÇ∫Áî¢ÁîüËß£ÈáãÁöÑÂü∫Êú¨ÂñÆ‰Ωç„ÄÇÊàëÂÄëÁöÑÂÅöÊ≥ïÂæûÈÄèÈÅéÂü∫Â∫èÂàÜËß£ÊäÄË°ìÊèêÂèñÊΩõÂú®Âü∫Â∫èÈñãÂßã„ÄÇÊé•ËëóÔºåÊàëÂÄëÂà©Áî®Âü∫ÊñºÊ≥®ÊÑèÂäõÁöÑÂ≠∏ÁøíÊñπÊ≥ï‰æÜË≠òÂà•ÁâπÂÆöÈ°ûÂà•ÁöÑÂü∫Â∫è„ÄÇÊúÄÂæåÔºåÊàëÂÄëÁÇ∫ÊØèÂÄãÈ°ûÂà•Êé°Áî®Âü∫ÊñºÂü∫Â∫èÁöÑÂúñÂΩ¢Áî¢ÁîüÂô®Ôºå‰ª•Ê†πÊìöÈÄô‰∫õÁâπÂÆöÈ°ûÂà•ÁöÑÂü∫Â∫èÂª∫Á´ãÂàÜÂ≠êÂúñÂΩ¢Ëß£Èáã„ÄÇÈÄôÁ®ÆÊñ∞Á©éÁöÑÊñπÊ≥ï‰∏çÂÉÖÂ∞áÈóúÈçµÁöÑÂ≠êÁµêÊßãÁ¥çÂÖ•Ëß£Èáã‰∏≠ÔºåÈÇÑ‰øùË≠â‰∫ÜÂÆÉÂÄëÁöÑÊúâÊïàÊÄßÔºåÁî¢Áîü‰∫∫È°ûÂèØ‰ª•ÁêÜËß£ÁöÑÁµêÊûú„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÂ∑≤ÈÄèÈÅéÂ∞çÂÖ≠ÂÄãÁúüÂØ¶‰∏ñÁïåÁöÑÂàÜÂ≠êË≥áÊñôÈõÜÈÄ≤Ë°åÁöÑÈáèÂåñÂíåË≥™ÂåñË©ï‰º∞ÂæóÂà∞Ë≠âÊòé„ÄÇ

##### **EntropyStop: Unsupervised Deep Outlier Detection with Loss Entropy**
2405.12502v1 by Yihong Huang, Yuang Zhang, Liping Wang, Fan Zhang, Xuemin Lin

Unsupervised Outlier Detection (UOD) is an important data mining task. With
the advance of deep learning, deep Outlier Detection (OD) has received broad
interest. Most deep UOD models are trained exclusively on clean datasets to
learn the distribution of the normal data, which requires huge manual efforts
to clean the real-world data if possible. Instead of relying on clean datasets,
some approaches directly train and detect on unlabeled contaminated datasets,
leading to the need for methods that are robust to such conditions. Ensemble
methods emerged as a superior solution to enhance model robustness against
contaminated training sets. However, the training time is greatly increased by
the ensemble.
  In this study, we investigate the impact of outliers on the training phase,
aiming to halt training on unlabeled contaminated datasets before performance
degradation. Initially, we noted that blending normal and anomalous data causes
AUC fluctuations, a label-dependent measure of detection accuracy. To
circumvent the need for labels, we propose a zero-label entropy metric named
Loss Entropy for loss distribution, enabling us to infer optimal stopping
points for training without labels. Meanwhile, we theoretically demonstrate
negative correlation between entropy metric and the label-based AUC. Based on
this, we develop an automated early-stopping algorithm, EntropyStop, which
halts training when loss entropy suggests the maximum model detection
capability. We conduct extensive experiments on ADBench (including 47 real
datasets), and the overall results indicate that AutoEncoder (AE) enhanced by
our approach not only achieves better performance than ensemble AEs but also
requires under 1\% of training time. Lastly, our proposed metric and
early-stopping approach are evaluated on other deep OD models, exhibiting their
broad potential applicability.

ÊëòË¶ÅÔºöÁÑ°Áõ£Áù£Áï∞Â∏∏ÂÄºÂÅµÊ∏¨ (UOD) ÊòØ‰∏ÄÈ†ÖÈáçË¶ÅÁöÑË≥áÊñôÊé¢Âãò‰ªªÂãô„ÄÇÈö®ËëóÊ∑±Â∫¶Â≠∏ÁøíÁöÑÈÄ≤Â±ïÔºåÊ∑±Â∫¶Áï∞Â∏∏ÂÄºÂÅµÊ∏¨ (OD) Â∑≤Âª£ÂèóÈóúÊ≥®„ÄÇÂ§ßÂ§öÊï∏Ê∑±Â∫¶ UOD Ê®°ÂûãÂÉÖÂú®‰πæÊ∑®ÁöÑË≥áÊñôÈõÜ‰∏äË®ìÁ∑¥Ôºå‰ª•‰∫ÜËß£Ê≠£Â∏∏Ë≥áÊñôÁöÑÂàÜÂ∏ÉÔºåÂ¶ÇÊûúÂèØËÉΩÁöÑË©±ÔºåÈÄôÈúÄË¶ÅÂ§ßÈáèÊâãÂãïÂ∑•‰Ωú‰æÜÊ∏ÖÁêÜÁúüÂØ¶‰∏ñÁïåÁöÑË≥áÊñô„ÄÇÊúâ‰∫õÊñπÊ≥ï‰∏¶Èùû‰æùË≥¥‰πæÊ∑®ÁöÑË≥áÊñôÈõÜÔºåËÄåÊòØÁõ¥Êé•Âú®Êú™Ê®ôÁ±§ÁöÑÂèóÊ±°ÊüìË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥ÂíåÂÅµÊ∏¨ÔºåÂ∞éËá¥ÈúÄË¶ÅÂ∞çÊ≠§È°ûÁãÄÊ≥ÅÂÖ∑ÊúâÁ©©ÂÅ•ÊÄßÁöÑÊñπÊ≥ï„ÄÇÊï¥È´îÊñπÊ≥ïÂ∑≤ÊàêÁÇ∫Â¢ûÂº∑Ê®°ÂûãÂ∞çÂèóÊ±°ÊüìË®ìÁ∑¥ÈõÜÁ©©ÂÅ•ÊÄßÁöÑÂÑ™Áï∞Ëß£Ê±∫ÊñπÊ°à„ÄÇÁÑ∂ËÄåÔºåÊï¥È´îÊúÉÂ§ßÂπÖÂ¢ûÂä†Ë®ìÁ∑¥ÊôÇÈñì„ÄÇ
Âú®Êú¨Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÊé¢Ë®éÁï∞Â∏∏ÂÄºÂ∞çË®ìÁ∑¥ÈöéÊÆµÁöÑÂΩ±ÈüøÔºåÊó®Âú®Êö´ÂÅúÂú®Êú™Ê®ôÁ±§ÁöÑÂèóÊ±°ÊüìË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°åË®ìÁ∑¥Ôºå‰ª•ÈÅøÂÖçÊïàËÉΩ‰∏ãÈôç„ÄÇÊúÄÂàùÔºåÊàëÂÄëÊ≥®ÊÑèÂà∞Ê∑∑ÂêàÊ≠£Â∏∏ÂíåÁï∞Â∏∏Ë≥áÊñôÊúÉÂ∞éËá¥ AUC Ê≥¢ÂãïÔºåÈÄôÊòØ‰∏ÄÁ®ÆËàáÊ®ôÁ±§Áõ∏ÈóúÁöÑÂÅµÊ∏¨Ê∫ñÁ¢∫Â∫¶Ê∏¨Èáè„ÄÇÁÇ∫‰∫ÜÈÅøÂÖçÈúÄË¶ÅÊ®ôÁ±§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂêçÁÇ∫ÊêçÂ§±ÁÜµÁöÑÈõ∂Ê®ôÁ±§ÁÜµÊåáÊ®ôÔºåÁî®ÊñºÊêçÂ§±ÂàÜ‰ΩàÔºå‰ΩøÊàëÂÄëËÉΩÂ§†Êé®Ë´ñÂá∫Ê≤íÊúâÊ®ôÁ±§ÁöÑÊúÄ‰Ω≥Ë®ìÁ∑¥ÂÅúÊ≠¢Èªû„ÄÇÂêåÊôÇÔºåÊàëÂÄëÂú®ÁêÜË´ñ‰∏äË≠âÊòé‰∫ÜÁÜµÊåáÊ®ôÂíåÂü∫ÊñºÊ®ôÁ±§ÁöÑ AUC ‰πãÈñìÂ≠òÂú®Ë≤†Áõ∏ÈóúÊÄß„ÄÇÂü∫ÊñºÊ≠§ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÁ®ÆËá™ÂãïÊó©ÊúüÂÅúÊ≠¢ÊºîÁÆóÊ≥ï EntropyStopÔºåÁï∂ÊêçÂ§±ÁÜµÂª∫Ë≠∞ÊúÄÂ§ßÊ®°ÂûãÂÅµÊ∏¨ËÉΩÂäõÊôÇÔºå‰æøÊúÉÂÅúÊ≠¢Ë®ìÁ∑¥„ÄÇÊàëÂÄëÂú® ADBenchÔºàÂåÖÊã¨ 47 ÂÄãÁúüÂØ¶Ë≥áÊñôÈõÜÔºâ‰∏äÈÄ≤Ë°åÂª£Ê≥õÁöÑÂØ¶È©óÔºåÊï¥È´îÁµêÊûúË°®ÊòéÔºåÁî±ÊàëÂÄëÁöÑÊñπÊ≥ïÂ¢ûÂº∑ÁöÑËá™ÂãïÁ∑®Á¢ºÂô® (AE) ‰∏çÂÉÖÊØîÊï¥È´î AE ÈÅîÂà∞Êõ¥Â•ΩÁöÑÊïàËÉΩÔºåËÄå‰∏îË®ìÁ∑¥ÊôÇÈñì‰∏çÂà∞ 1%„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊåáÊ®ôÂíåÊó©ÊúüÂÅúÊ≠¢ÊñπÊ≥ïÂú®ÂÖ∂‰ªñÊ∑±Â∫¶ OD Ê®°Âûã‰∏äÈÄ≤Ë°åË©ï‰º∞ÔºåÂ±ïÁ§∫‰∫ÜÂÆÉÂÄëÂª£Ê≥õÁöÑÊΩõÂú®ÊáâÁî®ÊÄß„ÄÇ

##### **Exploring and Exploiting the Asymmetric Valley of Deep Neural Networks**
2405.12489v1 by Xin-Chun Li, Jin-Lin Tang, Bo Zhang, Lan Li, De-Chuan Zhan

Exploring the loss landscape offers insights into the inherent principles of
deep neural networks (DNNs). Recent work suggests an additional asymmetry of
the valley beyond the flat and sharp ones, yet without thoroughly examining its
causes or implications. Our study methodically explores the factors affecting
the symmetry of DNN valleys, encompassing (1) the dataset, network
architecture, initialization, and hyperparameters that influence the
convergence point; and (2) the magnitude and direction of the noise for 1D
visualization. Our major observation shows that the {\it degree of sign
consistency} between the noise and the convergence point is a critical
indicator of valley symmetry. Theoretical insights from the aspects of ReLU
activation and softmax function could explain the interesting phenomenon. Our
discovery propels novel understanding and applications in the scenario of Model
Fusion: (1) the efficacy of interpolating separate models significantly
correlates with their sign consistency ratio, and (2) imposing sign alignment
during federated learning emerges as an innovative approach for model parameter
alignment.

ÊëòË¶ÅÔºöÊé¢Á¥¢ÊêçÂ§±ÊôØËßÄÊúâÂä©ÊñºÊ∑±ÂÖ•‰∫ÜËß£Ê∑±Â∫¶Á•ûÁ∂ìÁ∂≤Ë∑Ø (DNN) ÁöÑÂÖßÂú®ÂéüÁêÜ„ÄÇÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÈô§‰∫ÜÂπ≥Âù¶ÂíåÈô°Â≥≠ÁöÑË∞∑Â∫ï‰πãÂ§ñÔºåÂ±±Ë∞∑ÈÇÑÂ≠òÂú®È°çÂ§ñÁöÑ‰∏çÂ∞çÁ®±ÊÄßÔºå‰ΩÜÂ∞öÊú™ÂæπÂ∫ïÊ™¢Ë¶ñÂÖ∂ÊàêÂõ†ÊàñÂΩ±Èüø„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÊñπÊ≥ïÊÄßÂú∞Êé¢Ë®éÂΩ±Èüø DNN Â±±Ë∞∑Â∞çÁ®±ÊÄßÁöÑÂõ†Á¥†ÔºåÂåÖÊã¨ (1) Ë≥áÊñôÈõÜ„ÄÅÁ∂≤Ë∑ØÊû∂Êßã„ÄÅÂàùÂßãÂåñÂíåÂΩ±ÈüøÊî∂ÊñÇÈªûÁöÑË∂ÖÂèÉÊï∏Ôºõ‰ª•Âèä (2) 1D ÂèØË¶ñÂåñÁöÑÈõúË®äÂ§ßÂ∞èÂíåÊñπÂêë„ÄÇÊàëÂÄëÁöÑÈáçÂ§ßËßÄÂØüÁµêÊûúÈ°ØÁ§∫ÔºåÈõúË®äËàáÊî∂ÊñÇÈªû‰πãÈñìÁöÑ„ÄåÁ¨¶Ëôü‰∏ÄËá¥ÊÄßÁ®ãÂ∫¶„ÄçÊòØÂ±±Ë∞∑Â∞çÁ®±ÊÄßÁöÑÈóúÈçµÊåáÊ®ô„ÄÇReLU ÊøÄÊ¥ªÂíå softmax ÂáΩÊï∏ÊñπÈù¢ÁöÑÁêÜË´ñË¶ãËß£ÂèØ‰ª•Ëß£ÈáãÈÄôÂÄãÊúâË∂£ÁöÑÁèæË±°„ÄÇÊàëÂÄëÁöÑÁôºÁèæÊé®Âãï‰∫ÜÊ®°ÂûãËûçÂêàÂ†¥ÊôØ‰∏≠Êñ∞ÁöÑÁêÜËß£ÂíåÊáâÁî®Ôºö(1) ÊèíÂÄºÂñÆÁç®Ê®°ÂûãÁöÑÂäüÊïàËàáÂÖ∂Á¨¶Ëôü‰∏ÄËá¥ÊÄßÊØîÁéáÈ°ØËëóÁõ∏ÈóúÔºå‰ª•Âèä (2) Âú®ËÅØÂêàÂ≠∏ÁøíÊúüÈñìÂº∑Âä†Á¨¶ËôüÂ∞çÈΩäÊàêÁÇ∫Ê®°ÂûãÂèÉÊï∏Â∞çÈΩäÁöÑ‰∏ÄÁ®ÆÂâµÊñ∞ÊñπÊ≥ï„ÄÇ

##### **Time Matters: Enhancing Pre-trained News Recommendation Models with Robust User Dwell Time Injection**
2405.12486v1 by Hao Jiang, Chuanzhen Li, Mingxiao An

Large Language Models (LLMs) have revolutionized text comprehension, leading
to State-of-the-Art (SOTA) news recommendation models that utilize LLMs for
in-depth news understanding. Despite this, accurately modeling user preferences
remains challenging due to the inherent uncertainty of click behaviors.
Techniques like multi-head attention in Transformers seek to alleviate this by
capturing interactions among clicks, yet they fall short in integrating
explicit feedback signals. User Dwell Time emerges as a powerful indicator,
offering the potential to enhance the weak signals emanating from clicks.
Nonetheless, its real-world applicability is questionable, especially when
dwell time data collection is subject to delays. To bridge this gap, this paper
proposes two novel and robust dwell time injection strategies, namely Dwell
time Weight (DweW) and Dwell time Aware (DweA). Dwe} concentrates on refining
Effective User Clicks through detailed analysis of dwell time, integrating with
initial behavioral inputs to construct a more robust user preference. DweA
empowers the model with awareness of dwell time information, thereby
facilitating autonomous adjustment of attention values in user modeling. This
enhancement sharpens the model's ability to accurately identify user
preferences. In our experiment using the real-world news dataset from MSN
website, we validated that our two strategies significantly improve
recommendation performance, favoring high-quality news. Crucially, our
approaches exhibit robustness to user dwell time information, maintaining their
ability to recommend high-quality content even in extreme cases where dwell
time data is entirely missing.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤ÂæπÂ∫ïÊîπËÆäÊñáÂ≠óÁêÜËß£ÔºåÈÄ≤ËÄåÁî¢ÁîüÂà©Áî® LLM Ê∑±ÂÖ•ÁêÜËß£Êñ∞ËÅûÁöÑÊúÄÊñ∞Êé®Ëñ¶Ê®°Âûã„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÁî±ÊñºÈªûÊìäË°åÁÇ∫ÁöÑÂÖßÂú®‰∏çÁ¢∫ÂÆöÊÄßÔºåÁ≤æÁ¢∫Âª∫Ê®°‰ΩøÁî®ËÄÖÂÅèÂ•Ω‰ªçÁÑ∂ÂÖ∑ÊúâÊåëÊà∞ÊÄß„ÄÇTransformer‰∏≠ÁöÑÂ§öÈ†≠Ê≥®ÊÑèÂäõÁ≠âÊäÄË°ìË©¶ÂúñÈÄèÈÅéÊçïÊçâÈªûÊìä‰πãÈñìÁöÑ‰∫íÂãï‰æÜÊ∏õËºïÈÄôÁ®ÆÊÉÖÊ≥ÅÔºå‰ΩÜÂÆÉÂÄëÂú®Êï¥ÂêàÊòéÁ¢∫ÁöÑÂõûÈ•ãË®äËôüÊñπÈù¢‰ªçÊúâ‰∏çË∂≥„ÄÇ‰ΩøÁî®ËÄÖÂÅúÁïôÊôÇÈñìÊàêÁÇ∫‰∏ÄÂÄãÂº∑ÊúâÂäõÁöÑÊåáÊ®ôÔºåÊèê‰æõÂ¢ûÂº∑‰æÜËá™ÈªûÊìäÁöÑÂº±Ë®äËôüÁöÑÊΩõÂäõ„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂÆÉÁöÑÂØ¶ÈöõÊáâÁî®ÊÄß‰ªçÊúâÁñëÂïèÔºåÁâπÂà•ÊòØÂú®ÂÅúÁïôÊôÇÈñìË≥áÊñôÊî∂ÈõÜÊúÉÂª∂ÈÅ≤ÊôÇ„ÄÇÁÇ∫‰∫ÜÂΩåË£úÈÄôÂÄãÂ∑ÆË∑ùÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÂÖ©Á®ÆÊñ∞Á©é‰∏îÂº∑Â§ßÁöÑÂÅúÁïôÊôÇÈñìÊ≥®ÂÖ•Á≠ñÁï•ÔºåÂç≥ÂÅúÁïôÊôÇÈñìÂä†Ê¨ä (DweW) ÂíåÂÅúÁïôÊôÇÈñìÊÑüÁü• (DweA)„ÄÇDwe} Â∞àÊ≥®ÊñºÈÄèÈÅéË©≥Á¥∞ÂàÜÊûêÂÅúÁïôÊôÇÈñì‰æÜÁ≤æÁÖâÊúâÊïàÁöÑ‰ΩøÁî®ËÄÖÈªûÊìäÔºå‰∏¶ËàáÂàùÂßãË°åÁÇ∫Ëº∏ÂÖ•Êï¥ÂêàÔºå‰ª•Âª∫ÊßãÊõ¥Âº∑Â§ßÁöÑ‰ΩøÁî®ËÄÖÂÅèÂ•Ω„ÄÇDweA ËÆìÊ®°ÂûãÂÖ∑ÂÇôÂÅúÁïôÊôÇÈñìË≥áË®äÁöÑÊÑüÁü•ÔºåÂæûËÄå‰øÉÈÄ≤‰ΩøÁî®ËÄÖÂª∫Ê®°‰∏≠Ê≥®ÊÑèÂäõÂÄºÁöÑËá™‰∏ªË™øÊï¥„ÄÇÈÄôÁ®ÆÂ¢ûÂº∑‰ΩøÊ®°ÂûãÊõ¥Á≤æÊ∫ñÂú∞Ë≠òÂà•‰ΩøÁî®ËÄÖÂÅèÂ•ΩÁöÑËÉΩÂäõÊõ¥ÁÇ∫ÊïèÈä≥„ÄÇÂú®ÊàëÂÄë‰ΩøÁî®‰æÜËá™ MSN Á∂≤Á´ôÁöÑÁúüÂØ¶‰∏ñÁïåÊñ∞ËÅûË≥áÊñôÈõÜÈÄ≤Ë°åÁöÑÂØ¶È©ó‰∏≠ÔºåÊàëÂÄëÈ©óË≠â‰∫ÜÊàëÂÄëÁöÑÂÖ©Á®ÆÁ≠ñÁï•È°ØËëóÊîπÂñÑ‰∫ÜÊé®Ëñ¶ÊïàËÉΩÔºåÂÅèÂ•ΩÈ´òÂìÅË≥™ÁöÑÊñ∞ËÅû„ÄÇËá≥ÈóúÈáçË¶ÅÁöÑÊòØÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÂ±ïÁèæÂá∫Â∞ç‰ΩøÁî®ËÄÖÂÅúÁïôÊôÇÈñìË≥áË®äÁöÑÁ©©ÂÅ•ÊÄßÔºåÂç≥‰ΩøÂú®ÂÅúÁïôÊôÇÈñìË≥áÊñôÂÆåÂÖ®ÈÅ∫Â§±ÁöÑÊ•µÁ´ØÊÉÖÊ≥Å‰∏ãÔºåÂÆÉÂÄë‰ªçËÉΩÊé®Ëñ¶È´òÂìÅË≥™ÁöÑÂÖßÂÆπ„ÄÇ

##### **GASE: Graph Attention Sampling with Edges Fusion for Solving Vehicle Routing Problems**
2405.12475v1 by Zhenwei Wang, Ruibin Bai, Fazlullah Khan, Ender Ozcan, Tiehua Zhang

Learning-based methods have become increasingly popular for solving vehicle
routing problems due to their near-optimal performance and fast inference
speed. Among them, the combination of deep reinforcement learning and graph
representation allows for the abstraction of node topology structures and
features in an encoder-decoder style. Such an approach makes it possible to
solve routing problems end-to-end without needing complicated heuristic
operators designed by domain experts. Existing research studies have been
focusing on novel encoding and decoding structures via various neural network
models to enhance the node embedding representation. Despite the sophisticated
approaches applied, there is a noticeable lack of consideration for the
graph-theoretic properties inherent to routing problems. Moreover, the
potential ramifications of inter-nodal interactions on the decision-making
efficacy of the models have not been adequately explored. To bridge this gap,
we propose an adaptive Graph Attention Sampling with the Edges Fusion framework
(GASE),where nodes' embedding is determined through attention calculation from
certain highly correlated neighbourhoods and edges, utilizing a filtered
adjacency matrix. In detail, the selections of particular neighbours and
adjacency edges are led by a multi-head attention mechanism, contributing
directly to the message passing and node embedding in graph attention sampling
networks. Furthermore, we incorporate an adaptive actor-critic algorithm with
policy improvements to expedite the training convergence. We then conduct
comprehensive experiments against baseline methods on learning-based VRP tasks
from different perspectives. Our proposed model outperforms the existing
methods by 2.08\%-6.23\% and shows stronger generalization ability, achieving
state-of-the-art performance on randomly generated instances and real-world
datasets.

ÊëòË¶ÅÔºö<paragraph>Âü∫ÊñºÂ≠∏ÁøíÁöÑÊñπÊ≥ïÁî±ÊñºÂÖ∂Ëøë‰πéÊúÄ‰Ω≥ÁöÑÊïàËÉΩÂíåÂø´ÈÄüÁöÑÊé®Ë´ñÈÄüÂ∫¶ÔºåÂ∑≤Âú®Ëß£Ê±∫ËªäËºõË∑ØÁ∑öÂïèÈ°å‰∏äËÆäÂæóË∂ä‰æÜË∂äÊôÆÈÅç„ÄÇÂÖ∂‰∏≠ÔºåÊ∑±Â∫¶Âº∑ÂåñÂ≠∏ÁøíÂíåÂúñÂΩ¢Ë°®Á§∫ÁöÑÁµêÂêàÂÖÅË®±Âú®Á∑®Á¢ºÂô®-Ëß£Á¢ºÂô®Ê®£Âºè‰∏≠ÊäΩË±°ÁØÄÈªûÊãìÊí≤ÁµêÊßãÂíåÁâπÂæµ„ÄÇÈÄôÁ®ÆÊñπÊ≥ï‰ΩøÂæóÂú®‰∏çÈúÄË¶ÅÁî±È†òÂüüÂ∞àÂÆ∂Ë®≠Ë®àÁöÑË§áÈõúÂïüÁôºÂºèÈÅãÁÆóÂ≠êÊÉÖÊ≥Å‰∏ãÔºåÂ∞±ËÉΩËß£Ê±∫Á´ØÂà∞Á´ØÁöÑË∑ØÁ∑öÂïèÈ°å„ÄÇÁèæÊúâÁöÑÁ†îÁ©∂‰∏ÄÁõ¥Â∞àÊ≥®ÊñºÈÄöÈÅéÂêÑÁ®ÆÁ•ûÁ∂ìÁ∂≤Ë∑ØÊ®°ÂûãÂ∞çÊñ∞ÁöÑÁ∑®Á¢ºÂíåËß£Á¢ºÁµêÊßãÈÄ≤Ë°åÁ†îÁ©∂Ôºå‰ª•Â¢ûÂº∑ÁØÄÈªûÂµåÂÖ•ÂºèË°®Á§∫„ÄÇÂÑòÁÆ°ÈÅãÁî®‰∫ÜË§áÈõúÁöÑÊñπÊ≥ïÔºå‰ΩÜÂ∞çÊñºË∑ØÁ∑öÂïèÈ°åÂõ∫ÊúâÁöÑÂúñË´ñÁâπÊÄßÂçªÊòéÈ°ØÁº∫‰πèËÄÉÈáè„ÄÇÊ≠§Â§ñÔºåÁØÄÈªûÈñì‰∫íÂãïÂ∞çÊ®°ÂûãÊ±∫Á≠ñÊïàËÉΩÁöÑÊΩõÂú®ÂΩ±ÈüøÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Ë®é„ÄÇÁÇ∫‰∫ÜÂΩåÂêàÁêÜË´ñËàáÂØ¶ÂãôÁöÑÂ∑ÆË∑ùÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆËá™ÈÅ©ÊáâÂúñÊ≥®ÊÑèÂäõÊäΩÊ®£ÂíåÈÇäÁ∑£ËûçÂêàÊ°ÜÊû∂ (GASE)ÔºåÂÖ∂‰∏≠ÁØÄÈªûÁöÑÂµåÂÖ•ÊòØÈÄöÈÅéÂæûÊüê‰∫õÈ´òÂ∫¶Áõ∏ÈóúÁöÑÈÑ∞ÂüüÂíåÈÇäÁ∑£ÈÄ≤Ë°åÊ≥®ÊÑèÂäõË®àÁÆóÔºåÂà©Áî®ÈÅéÊøæÁöÑÈÑ∞Êé•Áü©Èô£‰æÜÁ¢∫ÂÆöÁöÑ„ÄÇË©≥Á¥∞‰æÜË™™ÔºåÁâπÂÆöÈÑ∞ÂüüÂíåÈÑ∞Êé•ÈÇäÁ∑£ÁöÑÈÅ∏ÊìáÊòØÁî±Â§öÈ†≠Ê≥®ÊÑèÂäõÊ©üÂà∂ÂºïÂ∞éÁöÑÔºåÁõ¥Êé•ÊúâÂä©ÊñºË®äÊÅØÂÇ≥ÈÅûÂíåÂúñÊ≥®ÊÑèÂäõÊäΩÊ®£Á∂≤Ë∑Ø‰∏≠ÁöÑÁØÄÈªûÂµåÂÖ•„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÁµêÂêà‰∫Ü‰∏ÄÂÄãËá™ÈÅ©ÊáâÁöÑÂãï‰Ωú-Ë©ïË´ñÊºîÁÆóÊ≥ïÂíåÁ≠ñÁï•ÊîπÈÄ≤Ôºå‰ª•Âä†ÈÄüË®ìÁ∑¥Êî∂ÊñÇ„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÈáùÂ∞çÂü∫ÊñºÂ≠∏ÁøíÁöÑ VRP ‰ªªÂãôÔºåÂæû‰∏çÂêåÁöÑËßíÂ∫¶Â∞çÂü∫Ê∫ñÊñπÊ≥ïÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑÂØ¶È©ó„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊ®°ÂûãÊØîÁèæÊúâÊñπÊ≥ïÁöÑÊïàËÉΩÈ´òÂá∫ 2.08%-6.23%Ôºå‰∏¶‰∏îÈ°ØÁ§∫Âá∫Êõ¥Âº∑ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂú®Èö®Ê©üÁîüÊàêÁöÑÂØ¶‰æãÂíåÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩ„ÄÇ</paragraph>

##### **Learning Partially Aligned Item Representation for Cross-Domain Sequential Recommendation**
2405.12473v1 by Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Zhi Li, Sirui Zhao, Defu Lian, Enhong Chen

Cross-domain sequential recommendation (CDSR) aims to uncover and transfer
users' sequential preferences across multiple recommendation domains. While
significant endeavors have been made, they primarily concentrated on developing
advanced transfer modules and aligning user representations using
self-supervised learning techniques. However, the problem of aligning item
representations has received limited attention, and misaligned item
representations can potentially lead to sub-optimal sequential modeling and
user representation alignment. To this end, we propose a model-agnostic
framework called \textbf{C}ross-domain item representation \textbf{A}lignment
for \textbf{C}ross-\textbf{D}omain \textbf{S}equential \textbf{R}ecommendation
(\textbf{CA-CDSR}), which achieves sequence-aware generation and adaptively
partial alignment for item representations. Specifically, we first develop a
sequence-aware feature augmentation strategy, which captures both collaborative
and sequential item correlations, thus facilitating holistic item
representation generation. Next, we conduct an empirical study to investigate
the partial representation alignment problem from a spectrum perspective. It
motivates us to devise an adaptive spectrum filter, achieving partial alignment
adaptively. Furthermore, the aligned item representations can be fed into
different sequential encoders to obtain user representations. The entire
framework is optimized in a multi-task learning paradigm with an annealing
strategy. Extensive experiments have demonstrated that CA-CDSR can surpass
state-of-the-art baselines by a significant margin and can effectively align
items in representation spaces to enhance performance.

ÊëòË¶ÅÔºöË∑®Á∂≤ÂüüÂ∫èÂàóÊé®Ëñ¶ÔºàCDSRÔºâÊó®Âú®Êè≠Á§∫ÂíåÂÇ≥ÈÅû‰ΩøÁî®ËÄÖÂú®Â§öÂÄãÊé®Ëñ¶Á∂≤Âüü‰∏≠ÁöÑÂ∫èÂàóÂÅèÂ•Ω„ÄÇÂÑòÁÆ°Â∑≤ÂÅöÂá∫ÈáçÂ§ßÂä™ÂäõÔºå‰ΩÜ‰∏ªË¶ÅÈõÜ‰∏≠Âú®ÈñãÁôºÂÖàÈÄ≤ÁöÑÂÇ≥Ëº∏Ê®°ÁµÑÂíå‰ΩøÁî®Ëá™ÊàëÁõ£Áù£Â≠∏ÁøíÊäÄË°ìÂ∞çÈΩä‰ΩøÁî®ËÄÖË°®Âæµ„ÄÇÁÑ∂ËÄåÔºåÂ∞çÈΩäÈ†ÖÁõÆË°®ÂæµÁöÑÂïèÈ°åÂèóÂà∞ÁöÑÈóúÊ≥®ÊúâÈôêÔºåËÄåÊú™Â∞çÈΩäÁöÑÈ†ÖÁõÆË°®ÂæµÂèØËÉΩÊúÉÂ∞éËá¥Ê¨°‰Ω≥Â∫èÂàóÂª∫Ê®°Âíå‰ΩøÁî®ËÄÖË°®ÂæµÂ∞çÈΩä„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËàáÊ®°ÂûãÁÑ°ÈóúÁöÑÊ°ÜÊû∂ÔºåÁ®±ÁÇ∫Ë∑®Á∂≤ÂüüÈ†ÖÁõÆË°®ÂæµÂ∞çÈΩäÔºåÁî®ÊñºË∑®Á∂≤ÂüüÂ∫èÂàóÊé®Ëñ¶ÔºàCA-CDSRÔºâÔºåÂÆÉÂØ¶Áèæ‰∫ÜÂ∫èÂàóÊÑüÁü•ÁîüÊàêÂíåÈ†ÖÁõÆË°®ÂæµÁöÑËá™ÈÅ©ÊáâÈÉ®ÂàÜÂ∞çÈΩä„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàÈñãÁôº‰∫Ü‰∏ÄÂÄãÂ∫èÂàóÊÑüÁü•ÁâπÂæµÊì¥ÂÖÖÁ≠ñÁï•ÔºåÂÆÉÊçïÁç≤Âçî‰ΩúÂíåÂ∫èÂàóÈ†ÖÁõÆÈóúËÅØÔºåÂæûËÄå‰øÉÈÄ≤Êï¥È´îÈ†ÖÁõÆË°®ÂæµÁîüÊàê„ÄÇÊé•‰∏ã‰æÜÔºåÊàëÂÄëÈÄ≤Ë°å‰∫Ü‰∏ÄÈ†ÖÂØ¶Ë≠âÁ†îÁ©∂ÔºåÂæûÈ†ªË≠úËßíÂ∫¶Êé¢Ë®éÈÉ®ÂàÜË°®ÂæµÂ∞çÈΩäÂïèÈ°å„ÄÇÂÆÉÊøÄÂãµÊàëÂÄëË®≠Ë®à‰∏ÄÂÄãËá™ÈÅ©ÊáâÈ†ªË≠úÊøæÊ≥¢Âô®ÔºåËá™ÈÅ©ÊáâÂú∞ÂØ¶ÁèæÈÉ®ÂàÜÂ∞çÈΩä„ÄÇÊ≠§Â§ñÔºåÂ∞çÈΩäÁöÑÈ†ÖÁõÆË°®ÂæµÂèØ‰ª•Ëº∏ÂÖ•‰∏çÂêåÁöÑÂ∫èÂàóÁ∑®Á¢ºÂô®‰ª•Áç≤Âèñ‰ΩøÁî®ËÄÖË°®Âæµ„ÄÇÊï¥ÂÄãÊ°ÜÊû∂Âú®ÂÖ∑ÊúâÈÄÄÁÅ´Á≠ñÁï•ÁöÑÂ§ö‰ªªÂãôÂ≠∏ÁøíÁØÑ‰æã‰∏≠ÈÄ≤Ë°åÊúÄ‰Ω≥Âåñ„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåCA-CDSR ÂèØ‰ª•È°ØËëóË∂ÖË∂äÊúÄÂÖàÈÄ≤ÁöÑÂü∫Ê∫ñÔºå‰∏¶ÂèØ‰ª•ÊúâÊïàÂú∞Â∞çÈΩäË°®ÂæµÁ©∫Èñì‰∏≠ÁöÑÈ†ÖÁõÆ‰ª•Â¢ûÂº∑ÊïàËÉΩ„ÄÇ

##### **Leveraging Diverse Data Generation for Adaptable Zero-Shot Dialogue State Tracking**
2405.12468v1 by James D. Finch, Boxin Zhao, Jinho D. Choi

This work demonstrates that substantial gains in zero-shot dialogue state
tracking (DST) accuracy can be achieved by increasing the diversity of training
data using synthetic data generation techniques. Current DST training resources
are severely limited in the number of application domains and slot types they
cover due to the high costs of data collection, resulting in limited
adaptability to new domains. The presented work overcomes this challenge using
a novel, fully automatic data generation approach to create synthetic zero-shot
DST training resources. Unlike previous approaches for generating DST data, the
presented approach generates entirely new application domains to generate
dialogues, complete with silver dialogue state annotations and slot
descriptions. This approach is used to create the D0T dataset for training
zero-shot DST models, which covers an unprecedented 1,000+ domains. Experiments
performed on the MultiWOZ benchmark indicate that training models on diverse
synthetic data yields a performance improvement of +6.7% Joint Goal Accuracy,
achieving results competitive with much larger models.

ÊëòË¶ÅÔºöËøôÈ°πÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöËøá‰ΩøÁî®ÂêàÊàêÊï∞ÊçÆÁîüÊàêÊäÄÊúØÊù•Â¢ûÂä†ËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ§öÊ†∑ÊÄßÔºåÂèØ‰ª•Â§ßÂπÖÊèêÂçáÈõ∂Ê¨°Â≠¶‰π†ÂØπËØùÁä∂ÊÄÅËøΩË∏™ (DST) ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁî±‰∫éÊï∞ÊçÆÊî∂ÈõÜÊàêÊú¨È´òÊòÇÔºåÂΩìÂâçÁöÑ DST ËÆ≠ÁªÉËµÑÊ∫êÂú®Â∫îÁî®È¢ÜÂüüÂíåÊßΩ‰ΩçÁ±ªÂûãÊï∞Èáè‰∏äÂèóÂà∞‰∏•ÈáçÈôêÂà∂ÔºåÂØºËá¥ÈÄÇÂ∫îÊñ∞È¢ÜÂüüÁöÑÁÅµÊ¥ªÊÄßÊúâÈôê„ÄÇËøôÈ°πÁ†îÁ©∂ÈÄöËøá‰ΩøÁî®‰∏ÄÁßçÊñ∞È¢ñÁöÑÂÖ®Ëá™Âä®Êï∞ÊçÆÁîüÊàêÊñπÊ≥ïÊù•ÂÖãÊúçËøô‰∏ÄÊåëÊàòÔºå‰ªéËÄåÂàõÂª∫ÂêàÊàêÈõ∂Ê¨°Â≠¶‰π† DST ËÆ≠ÁªÉËµÑÊ∫ê„ÄÇ‰∏é‰πãÂâçÁîüÊàê DST Êï∞ÊçÆÁöÑÊñπÊ≥ï‰∏çÂêåÔºåËøôÁßçÊñπÊ≥ïÁîüÊàê‰∫ÜÂÖ®Êñ∞ÁöÑÂ∫îÁî®È¢ÜÂüüÊù•ÁîüÊàêÂØπËØùÔºåÂπ∂ÈôÑÊúâÈì∂Ë¥®ÂØπËØùÁä∂ÊÄÅÊ≥®ÈáäÂíåÊßΩ‰ΩçÊèèËø∞„ÄÇÊ≠§ÊñπÊ≥ïÁî®‰∫éÂàõÂª∫ D0T Êï∞ÊçÆÈõÜÔºåÁî®‰∫éËÆ≠ÁªÉÈõ∂Ê¨°Â≠¶‰π† DST Ê®°ÂûãÔºåËØ•Êï∞ÊçÆÈõÜÊ∂µÁõñ‰∫ÜÂâçÊâÄÊú™ÊúâÁöÑ 1,000 Â§ö‰∏™È¢ÜÂüü„ÄÇÂú® MultiWOZ Âü∫ÂáÜ‰∏äËøõË°åÁöÑÂÆûÈ™åË°®ÊòéÔºåÂú®Â§öÊ†∑ÂåñÁöÑÂêàÊàêÊï∞ÊçÆ‰∏äËÆ≠ÁªÉÊ®°ÂûãÂèØÂ∞ÜËÅîÂêàÁõÆÊ†áÂáÜÁ°ÆÂ∫¶ÊèêÈ´ò +6.7%ÔºåÂèñÂæó‰∫Ü‰∏éÊõ¥Â§ßÊ®°ÂûãÁõ∏ÂΩìÁöÑÁªìÊûú„ÄÇ

##### **Stochastic Learning of Computational Resource Usage as Graph Structured Multimarginal Schr√∂dinger Bridge**
2405.12463v1 by Georgiy A. Bondar, Robert Gifford, Linh Thi Xuan Phan, Abhishek Halder

We propose to learn the time-varying stochastic computational resource usage
of software as a graph structured Schr\"odinger bridge problem. In general,
learning the computational resource usage from data is challenging because
resources such as the number of CPU instructions and the number of last level
cache requests are both time-varying and statistically correlated. Our proposed
method enables learning the joint time-varying stochasticity in computational
resource usage from the measured profile snapshots in a nonparametric manner.
The method can be used to predict the most-likely time-varying distribution of
computational resource availability at a desired time. We provide detailed
algorithms for stochastic learning in both single and multi-core cases, discuss
the convergence guarantees, computational complexities, and demonstrate their
practical use in two case studies: a single-core nonlinear model predictive
controller, and a synthetic multi-core software.

ÊëòË¶ÅÔºöÊàëÂÄëÊèêÂá∫Â≠∏ÁøíÊôÇÈñìËÆäÂãïÈö®Ê©üË®àÁÆóË≥áÊ∫ê‰ΩøÁî®ÁéáÁöÑËªüÈ´îÔºå‰ΩúÁÇ∫ÂúñÂΩ¢ÁµêÊßãËñõ‰∏ÅÊ†ºÊ©ãÊ®ëÂïèÈ°å„ÄÇ‰∏ÄËà¨‰æÜË™™ÔºåÂæûË≥áÊñô‰∏≠Â≠∏ÁøíË®àÁÆóË≥áÊ∫ê‰ΩøÁî®ÁéáÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫Ë≥áÊ∫êÔºà‰æãÂ¶Ç CPU Êåá‰ª§Êï∏ÂíåÊúÄÂæåÂ±§Á¥öÂø´ÂèñË¶ÅÊ±ÇÊï∏ÔºâÂêåÊôÇÂÖ∑ÊúâÊôÇÈñìËÆäÁï∞ÊÄßÂíåÁµ±Ë®àÁõ∏ÈóúÊÄß„ÄÇÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïËÉΩÂ§†‰ª•ÈùûÂèÉÊï∏ÊñπÂºèÂæûÊ∏¨ÈáèËº™ÂªìÂø´ÁÖß‰∏≠Â≠∏ÁøíË®àÁÆóË≥áÊ∫ê‰ΩøÁî®Áéá‰∏≠ÁöÑËÅØÂêàÊôÇÈñìËÆäÁï∞Èö®Ê©üÊÄß„ÄÇÊ≠§ÊñπÊ≥ïÂèØÁî®ÊñºÈ†êÊ∏¨Âú®ÊâÄÈúÄÊôÇÈñìÂÖßÊúÄÊúâÂèØËÉΩÁöÑÊôÇÈñìËÆäÁï∞Ë®àÁÆóË≥áÊ∫êÂèØÁî®ÊÄßÂàÜ‰Ωà„ÄÇÊàëÂÄëÊèê‰æõÂñÆÊ†∏ÂíåÂ§öÊ†∏ÊÉÖÊ≥Å‰∏ãÈö®Ê©üÂ≠∏ÁøíÁöÑË©≥Á¥∞ÊºîÁÆóÊ≥ïÔºåË®éË´ñÊî∂ÊñÇ‰øùË≠â„ÄÅË®àÁÆóË§áÈõúÂ∫¶Ôºå‰∏¶Â±ïÁ§∫ÂÆÉÂÄëÂú®ÂÖ©ÂÄãÊ°à‰æãÁ†îÁ©∂‰∏≠ÁöÑÂØ¶ÈöõÁî®ÈÄîÔºöÂñÆÊ†∏ÈùûÁ∑öÊÄßÊ®°ÂûãÈ†êÊ∏¨ÊéßÂà∂Âô®ÂíåÂêàÊàêÂ§öÊ†∏ËªüÈ´î„ÄÇ

##### **Boosting X-formers with Structured Matrix for Long Sequence Time Series Forecasting**
2405.12462v1 by Zhicheng Zhang, Yong Wang, Shaoqi Tan, Bowei Xia, Yujie Luo

Transformer-based models for long sequence time series forecasting (LSTF)
problems have gained significant attention due to their exceptional forecasting
precision. As the cornerstone of these models, the self-attention mechanism
poses a challenge to efficient training and inference due to its quadratic time
complexity. In this article, we propose a novel architectural design for
Transformer-based models in LSTF, leveraging a substitution framework that
incorporates Surrogate Attention Blocks and Surrogate FFN Blocks. The framework
aims to boost any well-designed model's efficiency without sacrificing its
accuracy. We further establish the equivalence of the Surrogate Attention Block
to the self-attention mechanism in terms of both expressiveness and
trainability. Through extensive experiments encompassing nine Transformer-based
models across five time series tasks, we observe an average performance
improvement of 9.45% while achieving a significant reduction in model size by
46%

ÊëòË¶ÅÔºöÂü∫Êñº Transformer ÁöÑÈï∑Â∫èÂàóÊôÇÈñìÂ∫èÂàóÈ†êÊ∏¨ (LSTF) ÂïèÈ°åÊ®°ÂûãÁî±ÊñºÂÖ∂Âá∫Ëâ≤ÁöÑÈ†êÊ∏¨Á≤æÂ∫¶ËÄåÂÇôÂèóÈóúÊ≥®„ÄÇ‰ΩúÁÇ∫ÈÄô‰∫õÊ®°ÂûãÁöÑÂü∫Áü≥ÔºåËá™ÊàëÊ≥®ÊÑèÊ©üÂà∂Áî±ÊñºÂÖ∂‰∫åÊ¨°ÊôÇÈñìË§áÈõúÂ∫¶Â∞çÈ´òÊïàË®ìÁ∑¥ÂíåÊé®ÁêÜÊèêÂá∫‰∫ÜÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫Êñº Transformer ÁöÑ LSTF Ê®°ÂûãÁöÑÊñ∞Êû∂ÊßãË®≠Ë®àÔºåÂà©Áî®‰∫Ü‰∏ÄÂÄãÂåÖÂê´‰ª£ÁêÜÊ≥®ÊÑèÂçÄÂ°äÂíå‰ª£ÁêÜ FFN ÂçÄÂ°äÁöÑÊõøÊèõÊ°ÜÊû∂„ÄÇË©≤Ê°ÜÊû∂Êó®Âú®ÊèêÈ´ò‰ªª‰ΩïË®≠Ë®àËâØÂ•ΩÁöÑÊ®°ÂûãÁöÑÊïàÁéáÔºåËÄå‰∏çÊúÉÁäßÁâ≤ÂÖ∂Ê∫ñÁ¢∫ÊÄß„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Âª∫Á´ã‰∫Ü‰ª£ÁêÜÊ≥®ÊÑèÂçÄÂ°äËàáËá™ÊàëÊ≥®ÊÑèÊ©üÂà∂Âú®Ë°®ÈÅîËÉΩÂäõÂíåÂèØË®ìÁ∑¥ÊÄßÊñπÈù¢ÁöÑÁ≠âÂÉπÊÄß„ÄÇÈÄöÈÅéÊ∂µËìã‰∫îÂÄãÊôÇÈñìÂ∫èÂàó‰ªªÂãôÁöÑ‰πùÂÄãÂü∫Êñº Transformer ÁöÑÊ®°ÂûãÁöÑÂª£Ê≥õÂØ¶È©óÔºåÊàëÂÄëËßÄÂØüÂà∞Âπ≥ÂùáÊÄßËÉΩÊèêÈ´ò‰∫Ü 9.45%ÔºåÂêåÊôÇÊ®°ÂûãÂ§ßÂ∞èÈ°ØËëóÊ∏õÂ∞ë‰∫Ü 46%

##### **WorldAfford: Affordance Grounding based on Natural Language Instructions**
2405.12461v1 by Changmao Chen, Yuren Cong, Zhen Kan

Affordance grounding aims to localize the interaction regions for the
manipulated objects in the scene image according to given instructions. A
critical challenge in affordance grounding is that the embodied agent should
understand human instructions and analyze which tools in the environment can be
used, as well as how to use these tools to accomplish the instructions. Most
recent works primarily supports simple action labels as input instructions for
localizing affordance regions, failing to capture complex human objectives.
Moreover, these approaches typically identify affordance regions of only a
single object in object-centric images, ignoring the object context and
struggling to localize affordance regions of multiple objects in complex scenes
for practical applications. To address this concern, for the first time, we
introduce a new task of affordance grounding based on natural language
instructions, extending it from previously using simple labels for complex
human instructions. For this new task, we propose a new framework, WorldAfford.
We design a novel Affordance Reasoning Chain-of-Thought Prompting to reason
about affordance knowledge from LLMs more precisely and logically.
Subsequently, we use SAM and CLIP to localize the objects related to the
affordance knowledge in the image. We identify the affordance regions of the
objects through an affordance region localization module. To benchmark this new
task and validate our framework, an affordance grounding dataset, LLMaFF, is
constructed. We conduct extensive experiments to verify that WorldAfford
performs state-of-the-art on both the previous AGD20K and the new LLMaFF
dataset. In particular, WorldAfford can localize the affordance regions of
multiple objects and provide an alternative when objects in the environment
cannot fully match the given instruction.

ÊëòË¶ÅÔºöaffordance grounding Êó®Âú®Ê†πÊçÆÁªôÂÆöÁöÑÊåá‰ª§ÂØπÂú∫ÊôØÂõæÂÉè‰∏≠ÂèóÊìçÁ∫µÂØπË±°ÁöÑ‰∫§‰∫íÂå∫ÂüüËøõË°åÂÆö‰Ωç„ÄÇaffordance grounding ‰∏≠ÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆÊåëÊàòÂú®‰∫éÔºåÂÖ∑Ë∫´‰ª£ÁêÜÂ∫îËØ•ÁêÜËß£‰∫∫Á±ªÊåá‰ª§Âπ∂ÂàÜÊûêÁéØÂ¢É‰∏≠Âì™‰∫õÂ∑•ÂÖ∑ÂèØ‰ª•‰ΩøÁî®Ôºå‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®Ëøô‰∫õÂ∑•ÂÖ∑Êù•ÂÆåÊàêÊåá‰ª§„ÄÇÊúÄËøëÁöÑÂ§ßÂ§öÊï∞Â∑•‰Ωú‰∏ªË¶ÅÊîØÊåÅÁÆÄÂçïÁöÑÂä®‰ΩúÊ†áÁ≠æ‰Ωú‰∏∫ÂÆö‰Ωç affordance Âå∫ÂüüÁöÑËæìÂÖ•Êåá‰ª§ÔºåÊú™ËÉΩÊçïÊçâÂ§çÊùÇÁöÑ‰∫∫Á±ªÁõÆÊ†á„ÄÇÊ≠§Â§ñÔºåËøô‰∫õÊñπÊ≥ïÈÄöÂ∏∏Âè™ËØÜÂà´‰ª•ÂØπË±°‰∏∫‰∏≠ÂøÉÁöÑÂõæÂÉè‰∏≠Âçï‰∏™ÂØπË±°ÁöÑ affordance Âå∫ÂüüÔºåÂøΩÁï•‰∫ÜÂØπË±°‰∏ä‰∏ãÊñáÔºåÂπ∂‰∏îÈöæ‰ª•Âú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÂÆö‰ΩçÂ§ö‰∏™ÂØπË±°ÁöÑ affordance Âå∫Âüü‰ª•Áî®‰∫éÂÆûÈôÖÂ∫îÁî®„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨È¶ñÊ¨°ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§ÁöÑ affordance grounding Êñ∞‰ªªÂä°ÔºåÂ∞ÜÂÖ∂‰ªé‰ª•Ââç‰ΩøÁî®ÁÆÄÂçïÊ†áÁ≠æÊâ©Â±ïÂà∞Â§çÊùÇÁöÑ‰∫∫Á±ªÊåá‰ª§„ÄÇÂØπ‰∫éËøô‰∏™Êñ∞‰ªªÂä°ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Êñ∞Ê°ÜÊû∂ WorldAfford„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Êñ∞È¢ñÁöÑ Affordance Êé®ÁêÜÊÄùÁª¥ÈìæÊèêÁ§∫Ôºå‰ª•Êõ¥Á≤æÁ°ÆÂíåÂêà‰πéÈÄªËæëÂú∞Êé®ÁêÜÊù•Ëá™ LLM ÁöÑ affordance Áü•ËØÜ„ÄÇÈöèÂêéÔºåÊàë‰ª¨‰ΩøÁî® SAM Âíå CLIP Êù•ÂÆö‰ΩçÂõæÂÉè‰∏≠‰∏é affordance Áü•ËØÜÁõ∏ÂÖ≥ÁöÑÂØπË±°„ÄÇÊàë‰ª¨ÈÄöËøá affordance Âå∫ÂüüÂÆö‰ΩçÊ®°ÂùóËØÜÂà´ÂØπË±°ÁöÑ affordance Âå∫Âüü„ÄÇ‰∏∫‰∫ÜÂØπËøô‰∏™Êñ∞‰ªªÂä°ËøõË°åÂü∫ÂáÜÊµãËØïÂπ∂È™åËØÅÊàë‰ª¨ÁöÑÊ°ÜÊû∂ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ affordance grounding Êï∞ÊçÆÈõÜ LLMaFF„ÄÇÊàë‰ª¨ËøõË°å‰∫ÜÂπøÊ≥õÁöÑÂÆûÈ™åÊù•È™åËØÅ WorldAfford Âú®‰πãÂâçÁöÑ AGD20K ÂíåÊñ∞ÁöÑ LLMaFF Êï∞ÊçÆÈõÜ‰∏äÈÉΩË°®Áé∞Âá∫ÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇÁâπÂà´ÊòØÔºåWorldAfford ÂèØ‰ª•ÂÆö‰ΩçÂ§ö‰∏™ÂØπË±°ÁöÑ affordance Âå∫ÂüüÔºåÂπ∂Âú®ÁéØÂ¢É‰∏≠ÁöÑÂØπË±°Êó†Ê≥ïÂÆåÂÖ®ÂåπÈÖçÁªôÂÆöÊåá‰ª§Êó∂Êèê‰æõÊõø‰ª£ÊñπÊ°à„ÄÇ

##### **PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4**
2405.12450v1 by Seif Abukhalaf, Mohammad Hamdaqa, Foutse Khomh

The rapid progress of AI-powered programming assistants, such as GitHub
Copilot, has facilitated the development of software applications. These
assistants rely on large language models (LLMs), which are foundation models
(FMs) that support a wide range of tasks related to understanding and
generating language. LLMs have demonstrated their ability to express UML model
specifications using formal languages like the Object Constraint Language
(OCL). However, the context size of the prompt is limited by the number of
tokens an LLM can process. This limitation becomes significant as the size of
UML class models increases. In this study, we introduce PathOCL, a novel
path-based prompt augmentation technique designed to facilitate OCL generation.
PathOCL addresses the limitations of LLMs, specifically their token processing
limit and the challenges posed by large UML class models. PathOCL is based on
the concept of chunking, which selectively augments the prompts with a subset
of UML classes relevant to the English specification. Our findings demonstrate
that PathOCL, compared to augmenting the complete UML class model
(UML-Augmentation), generates a higher number of valid and correct OCL
constraints using the GPT-4 model. Moreover, the average prompt size crafted
using PathOCL significantly decreases when scaling the size of the UML class
models.

ÊëòË¶ÅÔºöAI È©ÖÂãïÁ®ãÂºèË®≠Ë®àÂä©ÁêÜÁöÑÂø´ÈÄüÈÄ≤Â±ïÔºå‰æãÂ¶Ç GitHub CopilotÔºå‰øÉÈÄ≤‰∫ÜËªüÈ´îÊáâÁî®ÁöÑÈñãÁôº„ÄÇÈÄô‰∫õÂä©ÁêÜ‰æùË≥¥ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM)ÔºåÂÆÉÂÄëÊòØÊîØÊè¥ËàáÁêÜËß£ÂíåÁî¢ÁîüË™ûË®ÄÁõ∏ÈóúÁöÑÂª£Ê≥õ‰ªªÂãôÁöÑÂü∫Á§éÊ®°Âûã (FM)„ÄÇLLM Â∑≤Â±ïÁ§∫ÂÆÉÂÄë‰ΩøÁî®Ê≠£ÂºèË™ûË®ÄÔºà‰æãÂ¶ÇÁâ©‰ª∂Á¥ÑÊùüË™ûË®Ä (OCL)ÔºâË°®ÈÅî UML Ê®°ÂûãË¶èÁØÑÁöÑËÉΩÂäõ„ÄÇÁÑ∂ËÄåÔºåÊèêÁ§∫ÁöÑ‰∏ä‰∏ãÊñáÂ§ßÂ∞èÂèóÂà∞ LLM ÂèØ‰ª•ËôïÁêÜÁöÑË®òËôüÊï∏ÈáèÈôêÂà∂„ÄÇÈö®Ëëó UML È°ûÂà•Ê®°ÂûãÁöÑË¶èÊ®°Â¢ûÂä†ÔºåÈÄôÂÄãÈôêÂà∂ËÆäÂæóÈ°ØËëó„ÄÇÂú®ÈÄôÈ†ÖÁ†îÁ©∂‰∏≠ÔºåÊàëÂÄë‰ªãÁ¥π‰∫Ü PathOCLÔºåÈÄôÊòØ‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÂü∫ÊñºË∑ØÂæëÁöÑÊèêÁ§∫Êì¥ÂÖÖÊäÄË°ìÔºåÊó®Âú®‰øÉÈÄ≤ OCL Áî¢Áîü„ÄÇPathOCL Ëß£Ê±∫‰∫Ü LLM ÁöÑÈôêÂà∂ÔºåÁâπÂà•ÊòØÂÆÉÂÄëÁöÑË®òËôüËôïÁêÜÈôêÂà∂ÂíåÂ§ßÂûã UML È°ûÂà•Ê®°ÂûãÂ∏∂‰æÜÁöÑÊåëÊà∞„ÄÇPathOCL Âü∫ÊñºÂàÜÂ°äÁöÑÊ¶ÇÂøµÔºåÂÆÉÊúâÈÅ∏ÊìáÂú∞‰ΩøÁî®ËàáËã±ÊñáË¶èÁØÑÁõ∏ÈóúÁöÑ UML È°ûÂà•Â≠êÈõÜ‰æÜÊì¥ÂÖÖÊèêÁ§∫„ÄÇÊàëÂÄëÁöÑÁôºÁèæË°®ÊòéÔºåËàáÊì¥ÂÖÖÂÆåÊï¥ÁöÑ UML È°ûÂà•Ê®°Âûã (UML-Augmentation) Áõ∏ÊØîÔºåPathOCL ‰ΩøÁî® GPT-4 Ê®°ÂûãÁî¢Áîü‰∫ÜÊõ¥Â§öÊúâÊïà‰∏îÊ≠£Á¢∫ÁöÑ OCL Á¥ÑÊùü„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî® PathOCL Ë£Ω‰ΩúÁöÑÂπ≥ÂùáÊèêÁ§∫Â§ßÂ∞èÂú®Êì¥ÂÖÖ UML È°ûÂà•Ê®°ÂûãÁöÑË¶èÊ®°ÊôÇÈ°ØËëóÊ∏õÂ∞ë„ÄÇ

##### **Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation**
2405.12442v1 by Qingyao Li, Wei Xia, Kounianhua Du, Qiji Zhang, Weinan Zhang, Ruiming Tang, Yong Yu

Concept recommendation aims to suggest the next concept for learners to study
based on their knowledge states and the human knowledge system. While knowledge
states can be predicted using knowledge tracing models, previous approaches
have not effectively integrated the human knowledge system into the process of
designing these educational models. In the era of rapidly evolving Large
Language Models (LLMs), many fields have begun using LLMs to generate and
encode text, introducing external knowledge. However, integrating LLMs into
concept recommendation presents two urgent challenges: 1) How to construct text
for concepts that effectively incorporate the human knowledge system? 2) How to
adapt non-smooth, anisotropic text encodings effectively for concept
recommendation? In this paper, we propose a novel Structure and Knowledge Aware
Representation learning framework for concept Recommendation (SKarREC). We
leverage factual knowledge from LLMs as well as the precedence and succession
relationships between concepts obtained from the knowledge graph to construct
textual representations of concepts. Furthermore, we propose a graph-based
adapter to adapt anisotropic text embeddings to the concept recommendation
task. This adapter is pre-trained through contrastive learning on the knowledge
graph to get a smooth and structure-aware concept representation. Then, it's
fine-tuned through the recommendation task, forming a
text-to-knowledge-to-recommendation adaptation pipeline, which effectively
constructs a structure and knowledge-aware concept representation. Our method
does a better job than previous adapters in transforming text encodings for
application in concept recommendation. Extensive experiments on real-world
datasets demonstrate the effectiveness of the proposed approach.

ÊëòË¶ÅÔºöÊ¶ÇÂøµÊé®Ëñ¶Êó®Âú®Ê†πÊìöÂ≠∏ÁøíËÄÖÁöÑÁü•Ë≠òÁãÄÊÖãÂíå‰∫∫È°ûÁü•Ë≠òÁ≥ªÁµ±ÔºåÂª∫Ë≠∞Â≠∏ÁøíËÄÖÂ≠∏ÁøíÁöÑ‰∏ã‰∏ÄÂÄãÊ¶ÇÂøµ„ÄÇÈõñÁÑ∂Áü•Ë≠òÁãÄÊÖãÂèØ‰ª•‰ΩøÁî®Áü•Ë≠òËøΩËπ§Ê®°Âûã‰æÜÈ†êÊ∏¨Ôºå‰ΩÜÂÖàÂâçÁöÑÂÅöÊ≥ï‰∏¶Êú™ÊúâÊïàÂú∞Â∞á‰∫∫È°ûÁü•Ë≠òÁ≥ªÁµ±Êï¥ÂêàÂà∞Ë®≠Ë®àÈÄô‰∫õÊïôËÇ≤Ê®°ÂûãÁöÑÈÅéÁ®ã‰∏≠„ÄÇÂú®Âø´ÈÄüÁôºÂ±ïÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊôÇ‰ª£ÔºåË®±Â§öÈ†òÂüüÂ∑≤ÈñãÂßã‰ΩøÁî® LLM ‰æÜÁî¢ÁîüÂíåÁ∑®Á¢ºÊñáÂ≠óÔºåÂºïÂÖ•Â§ñÈÉ®Áü•Ë≠ò„ÄÇÁÑ∂ËÄåÔºåÂ∞á LLM Êï¥ÂêàÂà∞Ê¶ÇÂøµÊé®Ëñ¶‰∏≠ÊúÉÂá∫ÁèæÂÖ©ÂÄãËø´ÂàáÁöÑÊåëÊà∞Ôºö1) Â¶Ç‰ΩïÂª∫ÊßãÊúâÊïàÂú∞Êï¥Âêà‰∫∫È°ûÁü•Ë≠òÁ≥ªÁµ±ÁöÑÊ¶ÇÂøµÊñáÂ≠óÔºü2) Â¶Ç‰ΩïÊúâÊïàÂú∞Ë™øÊï¥ÈùûÂπ≥Êªë„ÄÅÂêÑÂêëÁï∞ÊÄßÁöÑÊñáÂ≠óÁ∑®Á¢º‰ª•ÈÄ≤Ë°åÊ¶ÇÂøµÊé®Ëñ¶ÔºüÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÁµêÊßãÂíåÁü•Ë≠òÊÑüÁü•Ë°®Á§∫Â≠∏ÁøíÊû∂ÊßãÔºåÁî®ÊñºÊ¶ÇÂøµÊé®Ëñ¶ (SKarREC)„ÄÇÊàëÂÄëÂà©Áî® LLM ÁöÑ‰∫ãÂØ¶Áü•Ë≠ò‰ª•ÂèäÂæûÁü•Ë≠òÂúñË≠ú‰∏≠Áç≤ÂæóÁöÑÊ¶ÇÂøµ‰πãÈñìÁöÑÂÖàÂæåÈóú‰øÇ‰æÜÂª∫ÊßãÊ¶ÇÂøµÁöÑÊñáÂ≠óË°®Á§∫„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂü∫ÊñºÂúñË°®ÁöÑÈÅ©ÈÖçÂô®Ôºå‰ª•Â∞áÂêÑÂêëÁï∞ÊÄßÁöÑÊñáÂ≠óÂµåÂÖ•Ë™øÊï¥Âà∞Ê¶ÇÂøµÊé®Ëñ¶‰ªªÂãô„ÄÇÈÄôÂÄãÈÅ©ÈÖçÂô®ÈÄèÈÅéÂ∞çÊØîÂ≠∏ÁøíÂú®Áü•Ë≠òÂúñË≠ú‰∏äÈÄ≤Ë°åÈ†êË®ìÁ∑¥Ôºå‰ª•Áç≤ÂæóÂπ≥Êªë‰∏îÂÖ∑ÊúâÁµêÊßãÊÑüÁü•ÁöÑÊ¶ÇÂøµË°®Á§∫„ÄÇÁÑ∂ÂæåÔºåÈÄèÈÅéÊé®Ëñ¶‰ªªÂãôÈÄ≤Ë°åÂæÆË™øÔºåÂΩ¢ÊàêÂæûÊñáÂ≠óÂà∞Áü•Ë≠òÂÜçÂà∞Êé®Ëñ¶ÁöÑÈÅ©ÈÖçÁÆ°ÈÅìÔºåÊúâÊïàÂú∞Âª∫Êßã‰∏ÄÂÄãÁµêÊßãÂíåÁü•Ë≠òÊÑüÁü•ÁöÑÊ¶ÇÂøµË°®Á§∫„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÂú®ËΩâÊèõÊñáÂ≠óÁ∑®Á¢º‰ª•ÊáâÁî®ÊñºÊ¶ÇÂøµÊé®Ëñ¶ÊñπÈù¢ÔºåÊØîÂÖàÂâçÁöÑÈÅ©ÈÖçÂô®ÂÅöÂæóÊõ¥Â•Ω„ÄÇÂú®ÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂª£Ê≥õÂØ¶È©óË≠âÊòé‰∫ÜÊâÄÊèêÂá∫ÊñπÊ≥ïÁöÑÊúâÊïàÊÄß„ÄÇ

##### **CoCo Matrix: Taxonomy of Cognitive Contributions in Co-writing with Intelligent Agents**
2405.12438v1 by Ruyuan Wan, Simret Gebreegziabhe, Toby Jia-Jun Li, Karla Badillo-Urquiola

In recent years, there has been a growing interest in employing intelligent
agents in writing. Previous work emphasizes the evaluation of the quality of
end product-whether it was coherent and polished, overlooking the journey that
led to the product, which is an invaluable dimension of the creative process.
To understand how to recognize human efforts in co-writing with intelligent
writing systems, we adapt Flower and Hayes' cognitive process theory of writing
and propose CoCo Matrix, a two-dimensional taxonomy of entropy and information
gain, to depict the new human-agent co-writing model. We define four quadrants
and situate thirty-four published systems within the taxonomy. Our research
found that low entropy and high information gain systems are under-explored,
yet offer promising future directions in writing tasks that benefit from the
agent's divergent planning and the human's focused translation. CoCo Matrix,
not only categorizes different writing systems but also deepens our
understanding of the cognitive processes in human-agent co-writing. By
analyzing minimal changes in the writing process, CoCo Matrix serves as a proxy
for the writer's mental model, allowing writers to reflect on their
contributions. This reflection is facilitated through the measured metrics of
information gain and entropy, which provide insights irrespective of the
writing system used.

ÊëòË¶ÅÔºöËøëÂπ¥Êù•Ôºå‰∫∫‰ª¨ÂØπÂú®ÂÜô‰Ωú‰∏≠‰ΩøÁî®Êô∫ËÉΩ‰ª£ÁêÜË∂äÊù•Ë∂äÊÑüÂÖ¥Ë∂£„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂ÈáçÁÇπÂú®‰∫éËØÑ‰º∞ÊúÄÁªà‰∫ßÂìÅÁöÑË¥®Èáè‚Äî‚ÄîÂÆÉÊòØÂê¶ËøûË¥Ø‰∏îÂÆåÂñÑÔºåËÄåÂøΩÁï•‰∫ÜÂØºËá¥ËØ•‰∫ßÂìÅËØûÁîüÁöÑËøáÁ®ãÔºåËÄåËøôÊ≠£ÊòØÂàõ‰ΩúËøáÁ®ã‰∏≠‰∏Ä‰∏™ÂÆùË¥µÁöÑÁª¥Â∫¶„ÄÇ‰∏∫‰∫ÜÁêÜËß£Â¶Ç‰ΩïÂú®‰∏éÊô∫ËÉΩÂÜô‰ΩúÁ≥ªÁªüÂêà‰ΩúÂÜô‰Ωú‰∏≠ËØÜÂà´‰∫∫ÁöÑÂä™ÂäõÔºåÊàë‰ª¨ÈááÁî®‰∫Ü Flower Âíå Hayes ÁöÑÂÜô‰ΩúËÆ§Áü•ËøáÁ®ãÁêÜËÆ∫ÔºåÂπ∂ÊèêÂá∫‰∫Ü CoCo MatrixÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ≥‰∫éÁÜµÂíå‰ø°ÊÅØÂ¢ûÁõäÁöÑ‰∫åÁª¥ÂàÜÁ±ªÊ≥ïÔºå‰ª•ÊèèËø∞Êñ∞ÁöÑ„ÄÅ‰∫∫‰∏é‰ª£ÁêÜÂêà‰ΩúÂÜô‰ΩúÁöÑÊ®°Âûã„ÄÇÊàë‰ª¨ÂÆö‰πâ‰∫ÜÂõõ‰∏™Ë±°ÈôêÔºåÂπ∂Âú®ÂàÜÁ±ªÊ≥ï‰∏≠ÂÆö‰Ωç‰∫Ü 34 ‰∏™Â∑≤ÂèëÂ∏ÉÁ≥ªÁªü„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÂèëÁé∞Ôºå‰ΩéÁÜµÂíåÈ´ò‰ø°ÊÅØÂ¢ûÁõäÁ≥ªÁªüÂ∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÊé¢Á¥¢Ôºå‰ΩÜÂú®ÂèóÁõä‰∫é‰ª£ÁêÜÁöÑÂèëÊï£ËßÑÂàíÂíå‰∫∫ÁöÑ‰∏ìÊ≥®ÁøªËØëÁöÑÂÜô‰Ωú‰ªªÂä°‰∏≠Êèê‰æõ‰∫ÜÊúâÂ∏åÊúõÁöÑÊú™Êù•ÊñπÂêë„ÄÇCoCo Matrix ‰∏ç‰ªÖÂØπ‰∏çÂêåÁöÑÂÜô‰ΩúÁ≥ªÁªüËøõË°å‰∫ÜÂàÜÁ±ªÔºåËøòÂä†Ê∑±‰∫ÜÊàë‰ª¨ÂØπ‰∫∫‰∏é‰ª£ÁêÜÂêà‰ΩúÂÜô‰Ωú‰∏≠ÁöÑËÆ§Áü•ËøáÁ®ãÁöÑÁêÜËß£„ÄÇÈÄöËøáÂàÜÊûêÂÜô‰ΩúËøáÁ®ã‰∏≠ÁöÑÊúÄÂ∞èÂèòÂåñÔºåCoCo Matrix ÂèØ‰ª•‰Ωú‰∏∫‰ΩúÂÆ∂ÂøÉÊô∫Ê®°ÂûãÁöÑ‰ª£ÁêÜÔºåËÆ©‰ΩúÂÆ∂ËÉΩÂ§üÂèçÊÄùËá™Â∑±ÁöÑË¥°ÁåÆ„ÄÇËøôÁßçÂèçÊÄùÊòØÈÄöËøá‰ø°ÊÅØÂ¢ûÁõäÂíåÁÜµÁöÑÊµãÈáèÊåáÊ†áÂÆûÁé∞ÁöÑÔºåËøô‰∫õÊåáÊ†áÊèê‰æõ‰∫Ü‰∏éÊâÄ‰ΩøÁî®ÁöÑÂÜô‰ΩúÁ≥ªÁªüÊó†ÂÖ≥ÁöÑËßÅËß£„ÄÇ

##### **Resolving Word Vagueness with Scenario-guided Adapter for Natural Language Inference**
2405.12434v1 by Yonghao Liu, Mengyu Li, Di Liang, Ximing Li, Fausto Giunchiglia, Lan Huang, Xiaoyue Feng, Renchu Guan

Natural Language Inference (NLI) is a crucial task in natural language
processing that involves determining the relationship between two sentences,
typically referred to as the premise and the hypothesis. However, traditional
NLI models solely rely on the semantic information inherent in independent
sentences and lack relevant situational visual information, which can hinder a
complete understanding of the intended meaning of the sentences due to the
ambiguity and vagueness of language. To address this challenge, we propose an
innovative ScenaFuse adapter that simultaneously integrates large-scale
pre-trained linguistic knowledge and relevant visual information for NLI tasks.
Specifically, we first design an image-sentence interaction module to
incorporate visuals into the attention mechanism of the pre-trained model,
allowing the two modalities to interact comprehensively. Furthermore, we
introduce an image-sentence fusion module that can adaptively integrate visual
information from images and semantic information from sentences. By
incorporating relevant visual information and leveraging linguistic knowledge,
our approach bridges the gap between language and vision, leading to improved
understanding and inference capabilities in NLI tasks. Extensive benchmark
experiments demonstrate that our proposed ScenaFuse, a scenario-guided
approach, consistently boosts NLI performance.

ÊëòË¶ÅÔºöËá™ÁÑ∂Ë™ûË®ÄÊé®ÁêÜ (NLI) ÊòØËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ‰∏≠ÁöÑ‰∏ÄÈ†ÖÈóúÈçµ‰ªªÂãôÔºåÊ∂âÂèäÁ¢∫ÂÆöÂÖ©ÂÄãÂè•Â≠ê‰πãÈñìÁöÑÈóú‰øÇÔºåÈÄöÂ∏∏Á®±ÁÇ∫ÂâçÊèêÂíåÂÅáË®≠„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±ÁöÑ NLI Ê®°ÂûãÂÉÖ‰æùË≥¥ÊñºÁç®Á´ãÂè•Â≠ê‰∏≠Âõ∫ÊúâÁöÑË™ûÁæ©Ë≥áË®äÔºå‰∏¶‰∏îÁº∫‰πèÁõ∏ÈóúÁöÑÊÉÖÂ¢ÉË¶ñË¶∫Ë≥áË®äÔºåÈÄôÂèØËÉΩÊúÉÈòªÁ§ôÂ∞çÂè•Â≠êÁöÑÈ†êÊúüÂê´Áæ©ÁöÑÂÆåÊï¥ÁêÜËß£ÔºåÂõ†ÁÇ∫Ë™ûË®ÄÁöÑÊ®°Á≥äÊÄßÂíåÂê´Á≥äÊÄß„ÄÇÁÇ∫‰∫ÜÊáâÂ∞çÈÄô‰∏ÄÊåëÊà∞ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂâµÊñ∞ÁöÑ ScenaFuse ÈÅ©ÈÖçÂô®ÔºåÂÆÉÂêåÊôÇÊï¥Âêà‰∫ÜÂ§ßË¶èÊ®°È†êË®ìÁ∑¥ÁöÑË™ûË®ÄÁü•Ë≠òÂíåÁõ∏ÈóúË¶ñË¶∫Ë≥áË®äÔºåÁî®Êñº NLI ‰ªªÂãô„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÈ¶ñÂÖàË®≠Ë®à‰∫Ü‰∏ÄÂÄãÂΩ±ÂÉèÂè•Â≠ê‰∫íÂãïÊ®°ÁµÑÔºåÂ∞áË¶ñË¶∫ÊïàÊûúÁ¥çÂÖ•È†êË®ìÁ∑¥Ê®°ÂûãÁöÑÊ≥®ÊÑèÊ©üÂà∂ÔºåËÆìÈÄôÂÖ©Á®ÆÊ®°ÂºèÂÖ®Èù¢‰∫íÂãï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÂºïÂÖ•‰∫ÜÂΩ±ÂÉèÂè•Â≠êËûçÂêàÊ®°ÁµÑÔºåÂÆÉÂèØ‰ª•ÈÅ©ÊáâÊÄßÂú∞Êï¥Âêà‰æÜËá™ÂΩ±ÂÉèÁöÑË¶ñË¶∫Ë≥áË®äÂíå‰æÜËá™Âè•Â≠êÁöÑË™ûÁæ©Ë≥áË®ä„ÄÇÈÄèÈÅéÊï¥ÂêàÁõ∏ÈóúË¶ñË¶∫Ë≥áË®ä‰∏¶Âà©Áî®Ë™ûË®ÄÁü•Ë≠òÔºåÊàëÂÄëÁöÑÂÅöÊ≥ïÁ∏ÆÂ∞è‰∫ÜË™ûË®ÄÂíåË¶ñË¶∫‰πãÈñìÁöÑÂ∑ÆË∑ùÔºåÈÄ≤ËÄåÊèêÂçá NLI ‰ªªÂãô‰∏≠ÁöÑÁêÜËß£ÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÂª£Ê≥õÁöÑÂü∫Ê∫ñÂØ¶È©óË≠âÊòéÔºåÊàëÂÄëÊèêÂá∫ÁöÑ ScenaFuse ÊòØ‰∏ÄÁ®ÆÊÉÖÂ¢ÉÂºïÂ∞éÂºèÊñπÊ≥ïÔºåÂèØÊåÅÁ∫åÊèêÂçá NLI ÁöÑÊïàËÉΩ„ÄÇ

##### **LLM+Reasoning+Planning for supporting incomplete user queries in presence of APIs**
2405.12433v1 by Sudhir Agarwal, Anu Sreepathy, David H. Alonso, Prarit Lamba

Recent availability of Large Language Models (LLMs) has led to the
development of numerous LLM-based approaches aimed at providing natural
language interfaces for various end-user tasks. These end-user tasks in turn
can typically be accomplished by orchestrating a given set of APIs. In
practice, natural language task requests (user queries) are often incomplete,
i.e., they may not contain all the information required by the APIs. While LLMs
excel at natural language processing (NLP) tasks, they frequently hallucinate
on missing information or struggle with orchestrating the APIs. The key idea
behind our proposed approach is to leverage logical reasoning and classical AI
planning along with an LLM for accurately answering user queries including
identification and gathering of any missing information in these queries. Our
approach uses an LLM and ASP (Answer Set Programming) solver to translate a
user query to a representation in Planning Domain Definition Language (PDDL)
via an intermediate representation in ASP. We introduce a special API
"get_info_api" for gathering missing information. We model all the APIs as PDDL
actions in a way that supports dataflow between the APIs. Our approach then
uses a classical AI planner to generate an orchestration of API calls
(including calls to get_info_api) to answer the user query. Our evaluation
results show that our approach significantly outperforms a pure LLM based
approach by achieving over 95\% success rate in most cases on a dataset
containing complete and incomplete single goal and multi-goal queries where the
multi-goal queries may or may not require dataflow among the APIs.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) ÁöÑÂèØÁî®ÊÄß‰øÉÊàê‰∫ÜËÆ∏Â§öÂü∫‰∫é LLM ÁöÑÊñπÊ≥ïÁöÑÂèëÂ±ïÔºåËøô‰∫õÊñπÊ≥ïÊó®Âú®‰∏∫ÂêÑÁßçÊúÄÁªàÁî®Êà∑‰ªªÂä°Êèê‰æõËá™ÁÑ∂ËØ≠Ë®ÄÁïåÈù¢„ÄÇËøô‰∫õÊúÄÁªàÁî®Êà∑‰ªªÂä°ÂèçËøáÊù•ÈÄöÂ∏∏ÂèØ‰ª•ÈÄöËøáÁºñÊéíÁªôÂÆöÁöÑ API ÈõÜÊù•ÂÆåÊàê„ÄÇÂú®ÂÆûË∑µ‰∏≠ÔºåËá™ÁÑ∂ËØ≠Ë®Ä‰ªªÂä°ËØ∑Ê±ÇÔºàÁî®Êà∑Êü•ËØ¢ÔºâÈÄöÂ∏∏ÊòØ‰∏çÂÆåÊï¥ÁöÑÔºåÂç≥ÂÆÉ‰ª¨ÂèØËÉΩ‰∏çÂåÖÂê´ API ÊâÄÈúÄÁöÑÊâÄÊúâ‰ø°ÊÅØ„ÄÇËôΩÁÑ∂ LLM Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ (NLP) ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂÆÉ‰ª¨ÁªèÂ∏∏ÂØπÁº∫Â§±ÁöÑ‰ø°ÊÅØ‰∫ßÁîüÂπªËßâÊàñÈöæ‰ª•ÁºñÊéí API„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÊñπÊ≥ïËÉåÂêéÁöÑÂÖ≥ÈîÆÊÄùÊÉ≥ÊòØÂà©Áî®ÈÄªËæëÊé®ÁêÜÂíåÁªèÂÖ∏ AI ËßÑÂàí‰ª•Âèä LLM Êù•ÂáÜÁ°ÆÂõûÁ≠îÁî®Êà∑Êü•ËØ¢ÔºåÂåÖÊã¨ËØÜÂà´ÂíåÊî∂ÈõÜËøô‰∫õÊü•ËØ¢‰∏≠‰ªª‰ΩïÁº∫Â§±ÁöÑ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÁî® LLM Âíå ASPÔºàAnswer Set ProgrammingÔºâÊ±ÇËß£Âô®ÔºåÈÄöËøá ASP ‰∏≠ÁöÑ‰∏≠Èó¥Ë°®Á§∫Â∞ÜÁî®Êà∑Êü•ËØ¢ËΩ¨Êç¢‰∏∫ËßÑÂàíÂüüÂÆö‰πâËØ≠Ë®Ä (PDDL) ‰∏≠ÁöÑË°®Á§∫„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ÁâπÊÆä API ‚Äúget_info_api‚Äù Êù•Êî∂ÈõÜÁº∫Â§±ÁöÑ‰ø°ÊÅØ„ÄÇÊàë‰ª¨Â∞ÜÊâÄÊúâ API Âª∫Ê®°‰∏∫ PDDL Âä®‰ΩúÔºå‰ª•ÊîØÊåÅ API ‰πãÈó¥ÁöÑÊï∞ÊçÆÊµÅ„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÁî®ÁªèÂÖ∏ AI ËßÑÂàíÂô®ÁîüÊàê API Ë∞ÉÁî®ÁºñÊéíÔºàÂåÖÊã¨ÂØπ get_info_api ÁöÑË∞ÉÁî®ÔºâÊù•ÂõûÁ≠îÁî®Êà∑Êü•ËØ¢„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÊòéÊòæ‰ºò‰∫éÂü∫‰∫éÁ∫Ø LLM ÁöÑÊñπÊ≥ïÔºåÂú®ÂåÖÂê´ÂÆåÊï¥Âíå‰∏çÂÆåÊï¥ÂçïÁõÆÊ†áÂíåÂ§öÁõÆÊ†áÊü•ËØ¢ÁöÑÊï∞ÊçÆÈõÜ‰∏äÔºåÂú®Â§ßÂ§öÊï∞ÊÉÖÂÜµ‰∏ãÂÆûÁé∞‰∫ÜË∂ÖËøá 95% ÁöÑÊàêÂäüÁéáÔºåÂÖ∂‰∏≠Â§öÁõÆÊ†áÊü•ËØ¢ÂèØËÉΩÈúÄË¶Å‰πüÂèØËÉΩ‰∏çÈúÄË¶Å API ‰πãÈó¥ÁöÑÊï∞ÊçÆÊµÅ„ÄÇ</paragraph>

##### **A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback**
2405.12421v1 by Kihyun Kim, Jiawei Zhang, Pablo A. Parrilo, Asuman Ozdaglar

Inverse Reinforcement Learning (IRL) and Reinforcement Learning from Human
Feedback (RLHF) are pivotal methodologies in reward learning, which involve
inferring and shaping the underlying reward function of sequential
decision-making problems based on observed human demonstrations and feedback.
Most prior work in reward learning has relied on prior knowledge or assumptions
about decision or preference models, potentially leading to robustness issues.
In response, this paper introduces a novel linear programming (LP) framework
tailored for offline reward learning. Utilizing pre-collected trajectories
without online exploration, this framework estimates a feasible reward set from
the primal-dual optimality conditions of a suitably designed LP, and offers an
optimality guarantee with provable sample efficiency. Our LP framework also
enables aligning the reward functions with human feedback, such as pairwise
trajectory comparison data, while maintaining computational tractability and
sample efficiency. We demonstrate that our framework potentially achieves
better performance compared to the conventional maximum likelihood estimation
(MLE) approach through analytical examples and numerical experiments.

ÊëòË¶ÅÔºöÈÄÜÂêëÂº∑ÂåñÂ≠∏Áøí (IRL) Ëàá‰∫∫È°ûÂõûÈ•ãÂº∑ÂåñÂ≠∏Áøí (RLHF) ÊòØÁçéÂãµÂ≠∏Áøí‰∏≠ÁöÑÈóúÈçµÊñπÊ≥ïÔºåÊ∂âÂèäÊ†πÊìöËßÄÂØüÂà∞ÁöÑ‰∫∫È°ûÁ§∫ÁØÑÂíåÂõûÈ•ãÊé®Ë´ñÂíåÂ°ëÈÄ†Â∫èË≤´Ê±∫Á≠ñÂïèÈ°åÁöÑÂü∫Á§éÁçéÂãµÂáΩÊï∏„ÄÇÂ§ßÂ§öÊï∏ÂÖàÂâçÁöÑÁçéÂãµÂ≠∏ÁøíÂ∑•‰Ωú‰æùË≥¥ÊñºÊ±∫Á≠ñÊàñÂÅèÂ•ΩÊ®°ÂûãÁöÑÂÖàÈ©óÁü•Ë≠òÊàñÂÅáË®≠ÔºåÈÄôÂèØËÉΩÊúÉÂ∞éËá¥Á©©ÂÅ•ÊÄßÂïèÈ°å„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊú¨Êñá‰ªãÁ¥π‰∫Ü‰∏ÄÂÄãÈáùÂ∞çÈõ¢Á∑öÁçéÂãµÂ≠∏ÁøíÈáèË∫´ÂÆöÂà∂ÁöÑÊñ∞ÂûãÁ∑öÊÄßË¶èÂäÉ (LP) Ê°ÜÊû∂„ÄÇÂà©Áî®È†êÂÖàÊî∂ÈõÜÁöÑËªåË∑°ËÄåÁÑ°ÈúÄÂú®Á∑öÊé¢Á¥¢ÔºåÊ≠§Ê°ÜÊû∂ÂæûÈÅ©Áï∂Ë®≠Ë®àÁöÑ LP ÁöÑÂéüÂßãÂ∞çÂÅ∂ÊúÄÂÑ™Ê¢ù‰ª∂‰º∞Ë®à‰∏ÄÂÄãÂèØË°åÁöÑÁçéÂãµÈõÜÔºå‰∏¶Êèê‰æõÂÖ∑ÊúâÂèØË≠âÊòéÊ®£Êú¨ÊïàÁéáÁöÑÊúÄÂÑ™‰øùË≠â„ÄÇÊàëÂÄëÁöÑ LP Ê°ÜÊû∂ÈÇÑËÉΩÂ§†Â∞áÁçéÂãµÂáΩÊï∏Ëàá‰∫∫È°ûÂõûÈ•ãÂ∞çÈΩäÔºå‰æãÂ¶ÇÊàêÂ∞çËªåË∑°ÊØîËºÉÊï∏ÊìöÔºåÂêåÊôÇ‰øùÊåÅË®àÁÆóÂèØËôïÁêÜÊÄßÂíåÊ®£Êú¨ÊïàÁéá„ÄÇÊàëÂÄëË≠âÊòéÔºåËàáÂÇ≥Áµ±ÁöÑÊúÄÂ§ß‰ººÁÑ∂‰º∞Ë®à (MLE) ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàëÂÄëÁöÑÊ°ÜÊû∂ÊúâÂèØËÉΩÈÄöÈÅéÂàÜÊûêÁØÑ‰æãÂíåÊï∏ÂÄºÂØ¶È©óÂØ¶ÁèæÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ

##### **Targeted Multilingual Adaptation for Low-resource Language Families**
2405.12413v1 by C. M. Downey, Terra Blevins, Dhwani Serai, Dwija Parikh, Shane Steinert-Threlkeld

The "massively-multilingual" training of multilingual models is known to
limit their utility in any one language, and they perform particularly poorly
on low-resource languages. However, there is evidence that low-resource
languages can benefit from targeted multilinguality, where the model is trained
on closely related languages. To test this approach more rigorously, we
systematically study best practices for adapting a pre-trained model to a
language family. Focusing on the Uralic family as a test case, we adapt XLM-R
under various configurations to model 15 languages; we then evaluate the
performance of each experimental setting on two downstream tasks and 11
evaluation languages. Our adapted models significantly outperform mono- and
multilingual baselines. Furthermore, a regression analysis of hyperparameter
effects reveals that adapted vocabulary size is relatively unimportant for
low-resource languages, and that low-resource languages can be aggressively
up-sampled during training at little detriment to performance in high-resource
languages. These results introduce new best practices for performing language
adaptation in a targeted setting.

ÊëòË¶ÅÔºöÂ∑≤Áü•Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑ„ÄåÂ§ßËßÑÊ®°Â§öËØ≠Ë®Ä„ÄçËÆ≠ÁªÉ‰ºöÈôêÂà∂ÂÖ∂Âú®‰ªª‰Ωï‰∏ÄÁßçËØ≠Ë®Ä‰∏≠ÁöÑÂÆûÁî®ÊÄßÔºåËÄå‰∏îÂú®‰ΩéËµÑÊ∫êËØ≠Ë®Ä‰∏≠Ë°®Áé∞ÁâπÂà´Â∑Æ„ÄÇÁÑ∂ËÄåÔºåÊúâËØÅÊçÆË°®Êòé‰ΩéËµÑÊ∫êËØ≠Ë®ÄÂèØ‰ª•ÂèóÁõä‰∫éÊúâÈíàÂØπÊÄßÁöÑÂ§öËØ≠Ë®ÄÊÄßÔºåÂÖ∂‰∏≠Ê®°ÂûãÊòØÂú®ÂØÜÂàáÁõ∏ÂÖ≥ÁöÑËØ≠Ë®Ä‰∏äËøõË°åËÆ≠ÁªÉÁöÑ„ÄÇ‰∏∫‰∫ÜÊõ¥‰∏•Ê†ºÂú∞ÊµãËØïËøôÁßçÊñπÊ≥ïÔºåÊàë‰ª¨Á≥ªÁªüÂú∞Á†îÁ©∂‰∫ÜÂ∞ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÈÄÇÂ∫îÂà∞ËØ≠Ë®ÄÂÆ∂ÊóèÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ‰ª•‰πåÊãâÂ∞îËØ≠Á≥ª‰∏∫ÊµãËØïÊ°à‰æãÔºåÊàë‰ª¨Ê†πÊçÆÂêÑÁßçÈÖçÁΩÆË∞ÉÊï¥‰∫Ü XLM-R ‰ª•ÂØπ 15 ÁßçËØ≠Ë®ÄËøõË°åÂª∫Ê®°ÔºõÁÑ∂ÂêéÔºåÊàë‰ª¨Âú®‰∏§‰∏™‰∏ãÊ∏∏‰ªªÂä°Âíå 11 ÁßçËØÑ‰º∞ËØ≠Ë®Ä‰∏äËØÑ‰º∞‰∫ÜÊØè‰∏™ÂÆûÈ™åËÆæÁΩÆÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨Ë∞ÉÊï¥ÂêéÁöÑÊ®°ÂûãÊòéÊòæ‰ºò‰∫éÂçïËØ≠Ë®ÄÂíåÂ§öËØ≠Ë®ÄÂü∫Á∫ø„ÄÇÊ≠§Â§ñÔºåÂØπË∂ÖÂèÇÊï∞ÊïàÂ∫îÁöÑÂõûÂΩíÂàÜÊûêË°®ÊòéÔºåÂØπ‰∫é‰ΩéËµÑÊ∫êËØ≠Ë®ÄÊù•ËØ¥ÔºåË∞ÉÊï¥ÂêéÁöÑËØçÊ±áÈáèÂ§ßÂ∞èÁõ∏ÂØπ‰∏çÈáçË¶ÅÔºåÂπ∂‰∏îÂú®ËÆ≠ÁªÉÊúüÈó¥ÂèØ‰ª•ÁßØÊûÅÂú∞ÂØπ‰ΩéËµÑÊ∫êËØ≠Ë®ÄËøõË°å‰∏äÈááÊ†∑ÔºåËÄåÂØπÈ´òËµÑÊ∫êËØ≠Ë®ÄÁöÑÊÄßËÉΩÂá†‰πéÊ≤°ÊúâÊçüÂÆ≥„ÄÇËøô‰∫õÁªìÊûú‰∏∫Âú®ÊúâÈíàÂØπÊÄßÁöÑËÆæÁΩÆ‰∏≠ÊâßË°åËØ≠Ë®ÄÈÄÇÂ∫îÂºïÂÖ•‰∫ÜÊñ∞ÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ

##### **Diffusion for World Modeling: Visual Details Matter in Atari**
2405.12399v1 by Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, Fran√ßois Fleuret

World models constitute a promising approach for training reinforcement
learning agents in a safe and sample-efficient manner. Recent world models
predominantly operate on sequences of discrete latent variables to model
environment dynamics. However, this compression into a compact discrete
representation may ignore visual details that are important for reinforcement
learning. Concurrently, diffusion models have become a dominant approach for
image generation, challenging well-established methods modeling discrete
latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a
Model Of eNvironment Dreams), a reinforcement learning agent trained in a
diffusion world model. We analyze the key design choices that are required to
make diffusion suitable for world modeling, and demonstrate how improved visual
details can lead to improved agent performance. DIAMOND achieves a mean human
normalized score of 1.46 on the competitive Atari 100k benchmark; a new best
for agents trained entirely within a world model. To foster future research on
diffusion for world modeling, we release our code, agents and playable world
models at https://github.com/eloialonso/diamond.

ÊëòË¶ÅÔºö‰∏ñÁïåÊ®°ÂûãÊßãÊàê‰∫Ü‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Áî®Êñº‰ª•ÂÆâÂÖ®‰∏îÊ®£Êú¨ÊúâÊïàÁöÑÊñπÂºèË®ìÁ∑¥Âº∑ÂåñÂ≠∏Áøí‰ª£ÁêÜ„ÄÇÊúÄËøëÁöÑ‰∏ñÁïåÊ®°Âûã‰∏ªË¶ÅÂú®Èõ¢Êï£ÊΩõÂú®ËÆäÊï∏Â∫èÂàó‰∏äÈÅã‰ΩúÔºå‰ª•Ê®°Êì¨Áí∞Â¢ÉÂãïÊÖã„ÄÇÁÑ∂ËÄåÔºåÈÄôÁ®ÆÂ£ìÁ∏ÆÊàê‰∏ÄÂÄãÁ∑äÊπäÁöÑÈõ¢Êï£Ë°®Á§∫ÂèØËÉΩÊúÉÂøΩÁï•Â∞çÂº∑ÂåñÂ≠∏ÁøíÂæàÈáçË¶ÅÁöÑË¶ñË¶∫Á¥∞ÁØÄ„ÄÇÂêåÊôÇÔºåÊì¥Êï£Ê®°ÂûãÂ∑≤ÊàêÁÇ∫ÂúñÂÉèÁîüÊàêÁöÑÈ°ØËëóÊñπÊ≥ïÔºåÊåëÊà∞‰∫ÜÂ∞çÈõ¢Êï£ÊΩõÂú®ËÆäÊï∏ÈÄ≤Ë°åÂª∫Ê®°ÁöÑÊó¢ÂÆöÊñπÊ≥ï„ÄÇÂèóÈÄôÁ®ÆÁØÑÂºèËΩâËÆäÁöÑÊøÄÂãµÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü DIAMONDÔºàDIffusion As a Model Of eNvironment DreamsÔºâÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂú®Êì¥Êï£‰∏ñÁïåÊ®°Âûã‰∏≠Ë®ìÁ∑¥ÁöÑÂº∑ÂåñÂ≠∏Áøí‰ª£ÁêÜ„ÄÇÊàëÂÄëÂàÜÊûê‰∫Ü‰ΩøÊì¥Êï£ÈÅ©Âêà‰∏ñÁïåÂª∫Ê®°ÊâÄÈúÄÁöÑÈóúÈçµË®≠Ë®àÈÅ∏ÊìáÔºå‰∏¶Â±ïÁ§∫‰∫ÜÊîπÈÄ≤ÁöÑË¶ñË¶∫Á¥∞ÁØÄÂ¶Ç‰ΩïËÉΩÊèêÂçá‰ª£ÁêÜÊïàËÉΩ„ÄÇDIAMOND Âú®Á´∂Áà≠ÊøÄÁÉàÁöÑ Atari 100k Âü∫Ê∫ñÊ∏¨Ë©¶‰∏≠Áç≤Âæó‰∫Ü 1.46 ÁöÑÂπ≥Âùá‰∫∫È°ûÊ®ôÊ∫ñÂåñÂàÜÊï∏ÔºõÈÄôÊòØÂÆåÂÖ®Âú®‰∏ñÁïåÊ®°ÂûãÂÖßË®ìÁ∑¥ÁöÑ‰ª£ÁêÜÁöÑÊñ∞ÊúÄ‰Ω≥ÂàÜÊï∏„ÄÇÁÇ∫‰∫Ü‰øÉÈÄ≤Êú™‰æÜÂ∞ç‰∏ñÁïåÂª∫Ê®°Êì¥Êï£ÁöÑÁ†îÁ©∂ÔºåÊàëÂÄëÂú® https://github.com/eloialonso/diamond ‰∏äÁôºÂ∏É‰∫ÜÊàëÂÄëÁöÑÁ®ãÂºèÁ¢º„ÄÅ‰ª£ÁêÜÂíåÂèØÁé©‰∏ñÁïåÊ®°Âûã„ÄÇ

##### **Layout Agnostic Human Activity Recognition in Smart Homes through Textual Descriptions Of Sensor Triggers (TDOST)**
2405.12368v1 by Megha Thukral, Sourish Gunesh Dhekane, Shruthi K. Hiremath, Harish Haresamudram, Thomas Ploetz

Human activity recognition (HAR) using ambient sensors in smart homes has
numerous applications for human healthcare and wellness. However, building
general-purpose HAR models that can be deployed to new smart home environments
requires a significant amount of annotated sensor data and training overhead.
Most smart homes vary significantly in their layouts, i.e., floor plans and the
specifics of sensors embedded, resulting in low generalizability of HAR models
trained for specific homes. We address this limitation by introducing a novel,
layout-agnostic modeling approach for HAR systems in smart homes that utilizes
the transferrable representational capacity of natural language descriptions of
raw sensor data. To this end, we generate Textual Descriptions Of Sensor
Triggers (TDOST) that encapsulate the surrounding trigger conditions and
provide cues for underlying activities to the activity recognition models.
Leveraging textual embeddings, rather than raw sensor data, we create activity
recognition systems that predict standard activities across homes without
either (re-)training or adaptation on target homes. Through an extensive
evaluation, we demonstrate the effectiveness of TDOST-based models in unseen
smart homes through experiments on benchmarked CASAS datasets. Furthermore, we
conduct a detailed analysis of how the individual components of our approach
affect downstream activity recognition performance.

ÊëòË¶ÅÔºöÂà©Áî®Êô∫ÊÖßÂÆ∂Â∫≠‰∏≠Áí∞Â¢ÉÊÑüÊ∏¨Âô®ÈÄ≤Ë°å‰∫∫È°ûÊ¥ªÂãïËæ®Ë≠ò (HAR) Âú®‰∫∫È°ûÈÜ´ÁôÇ‰øùÂÅ•ÂíåÂÅ•Â∫∑ÊñπÈù¢ÊúâË®±Â§öÊáâÁî®„ÄÇÁÑ∂ËÄåÔºåÂª∫Á´ãÂèØÈÉ®ÁΩ≤Âà∞Êñ∞Êô∫ÊÖßÂÆ∂Â∫≠Áí∞Â¢ÉÁöÑÈÄöÁî® HAR Ê®°ÂûãÈúÄË¶ÅÂ§ßÈáèÁöÑË®ªËß£ÊÑüÊ∏¨Âô®Ë≥áÊñôÂíåË®ìÁ∑¥ÈñãÈä∑„ÄÇÂ§ßÂ§öÊï∏Êô∫ÊÖßÂÆ∂Â∫≠ÁöÑÈÖçÁΩÆÂ∑ÆÁï∞ÂæàÂ§ßÔºå‰æãÂ¶ÇÊ®ìÂ±§Âπ≥Èù¢ÂúñÂíåÂµåÂÖ•ÂºèÊÑüÊ∏¨Âô®ÁöÑË¶èÊ†ºÔºåÂ∞éËá¥ÁÇ∫ÁâπÂÆöÂÆ∂Â∫≠Ë®ìÁ∑¥ÁöÑ HAR Ê®°ÂûãÁöÑÊ¶ÇÊã¨ÊÄßÂæà‰Ωé„ÄÇÊàëÂÄëÈÄèÈÅéÂºïÂÖ•‰∏ÄÁ®ÆÂâµÊñ∞ÁöÑ„ÄÅËàáÈÖçÁΩÆÁÑ°ÈóúÁöÑÊô∫ÊÖßÂÆ∂Â∫≠ HAR Á≥ªÁµ±Âª∫Ê®°ÊñπÊ≥ï‰æÜËß£Ê±∫ÈÄôÂÄãÈôêÂà∂ÔºåË©≤ÊñπÊ≥ïÂà©Áî®Ëá™ÁÑ∂Ë™ûË®ÄÊèèËø∞ÂéüÂßãÊÑüÊ∏¨Âô®Ë≥áÊñôÁöÑÂèØËΩâÁßªË°®Á§∫ËÉΩÂäõ„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÁî¢ÁîüÊÑüÊ∏¨Âô®Ëß∏ÁôºÊñáÂ≠óÊèèËø∞ (TDOST)ÔºåÂÆÉÂåÖÂê´Âë®ÂúçÁöÑËß∏ÁôºÊ¢ù‰ª∂Ôºå‰∏¶ÁÇ∫Ê¥ªÂãïËæ®Ë≠òÊ®°ÂûãÊèê‰æõÂ∫ïÂ±§Ê¥ªÂãïÁöÑÁ∑öÁ¥¢„ÄÇÊàëÂÄëÂà©Áî®ÊñáÂ≠óÂµåÂÖ•ÔºåËÄåÈùûÂéüÂßãÊÑüÊ∏¨Âô®Ë≥áÊñôÔºåÂª∫Á´ãÊ¥ªÂãïËæ®Ë≠òÁ≥ªÁµ±ÔºåÂèØÂú®ÂÆ∂Â∫≠‰∏≠È†êÊ∏¨Ê®ôÊ∫ñÊ¥ªÂãïÔºåËÄåÁÑ°ÈúÄÂú®ÁõÆÊ®ôÂÆ∂Â∫≠‰∏≠ÈÄ≤Ë°åÔºàÈáçÊñ∞ÔºâË®ìÁ∑¥ÊàñË™øÊï¥„ÄÇÈÄèÈÅéÂª£Ê≥õÁöÑË©ï‰º∞ÔºåÊàëÂÄëÂú®Âü∫Ê∫ñ CASAS Ë≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©ó‰∏≠Ë≠âÊòé‰∫Ü TDOST Ê®°ÂûãÂú®Êú™Ë¶ãÈÅéÁöÑÊô∫ÊÖßÂÆ∂Â∫≠‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ∞çÊàëÂÄëÊñπÊ≥ïÁöÑÂêÑÂÄãÁµÑÊàêÈÉ®ÂàÜÂ¶Ç‰ΩïÂΩ±Èüø‰∏ãÊ∏∏Ê¥ªÂãïËæ®Ë≠òÊïàËÉΩÈÄ≤Ë°å‰∫ÜË©≥Á¥∞ÁöÑÂàÜÊûê„ÄÇ

##### **Question-Based Retrieval using Atomic Units for Enterprise RAG**
2405.12363v1 by Vatsal Raina, Mark Gales

Enterprise retrieval augmented generation (RAG) offers a highly flexible
framework for combining powerful large language models (LLMs) with internal,
possibly temporally changing, documents. In RAG, documents are first chunked.
Relevant chunks are then retrieved for a specific user query, which are passed
as context to a synthesizer LLM to generate the query response. However, the
retrieval step can limit performance, as incorrect chunks can lead the
synthesizer LLM to generate a false response. This work proposes a zero-shot
adaptation of standard dense retrieval steps for more accurate chunk recall.
Specifically, a chunk is first decomposed into atomic statements. A set of
synthetic questions are then generated on these atoms (with the chunk as the
context). Dense retrieval involves finding the closest set of synthetic
questions, and associated chunks, to the user query. It is found that retrieval
with the atoms leads to higher recall than retrieval with chunks. Further
performance gain is observed with retrieval using the synthetic questions
generated over the atoms. Higher recall at the retrieval step enables higher
performance of the enterprise LLM using the RAG pipeline.

ÊëòË¶ÅÔºö‰ºÅÊ•≠Ê™¢Á¥¢Â¢ûÂº∑ÁîüÊàê (RAG) Êèê‰æõ‰∫Ü‰∏ÄÂÄãÈ´òÂ∫¶ÂΩàÊÄßÁöÑÊ°ÜÊû∂ÔºåÁî®ÊñºÂ∞áÂº∑Â§ßÁöÑÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËàáÂÖßÈÉ®ÔºàÂèØËÉΩÂú®ÊôÇÈñì‰∏äÊúÉËÆäÂãïÁöÑÔºâÊñá‰ª∂ÁµêÂêàËµ∑‰æÜ„ÄÇÂú® RAG ‰∏≠ÔºåÊñá‰ª∂ÊúÉÂÖàÈÄ≤Ë°åÂàÜÂ°ä„ÄÇÊé•ËëóÈáùÂ∞çÁâπÂÆö‰ΩøÁî®ËÄÖÊü•Ë©¢Ê™¢Á¥¢Áõ∏ÈóúÁöÑÂçÄÂ°äÔºå‰∏¶Â∞áÈÄô‰∫õÂçÄÂ°ä‰ΩúÁÇ∫ËÉåÊôØÂÇ≥ÈÅûÁµ¶ÂêàÊàêÂô® LLMÔºå‰ª•Áî¢ÁîüÊü•Ë©¢ÂõûÊáâ„ÄÇÁÑ∂ËÄåÔºåÊ™¢Á¥¢Ê≠•È©üÂèØËÉΩÊúÉÈôêÂà∂ÊïàËÉΩÔºåÂõ†ÁÇ∫‰∏çÊ≠£Á¢∫ÁöÑÂçÄÂ°äÂèØËÉΩÊúÉÂ∞éËá¥ÂêàÊàêÂô® LLM Áî¢ÁîüÈåØË™§ÁöÑÂõûÊáâ„ÄÇÈÄôÈ†ÖÂ∑•‰ΩúÊèêÂá∫‰∫Ü‰∏ÄÂÄãÊ®ôÊ∫ñÂØÜÈõÜÊ™¢Á¥¢Ê≠•È©üÁöÑÈõ∂Ê¨°Â≠∏ÁøíÈÅ©ÊáâÔºå‰ª•ÊèêÈ´òÂçÄÂ°äÂè¨ÂõûÁöÑÊ∫ñÁ¢∫ÊÄß„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊúÉÂÖàÂ∞áÂçÄÂ°äÂàÜËß£ÊàêÂéüÂ≠êÈô≥Ëø∞„ÄÇÊé•ËëóÂú®ÈÄô‰∫õÂéüÂ≠êÔºà‰ª•ÂçÄÂ°ä‰ΩúÁÇ∫ËÉåÊôØÔºâ‰∏äÁî¢Áîü‰∏ÄÁµÑÂêàÊàêÂºèÂïèÈ°å„ÄÇÂØÜÈõÜÊ™¢Á¥¢Ê∂âÂèäÂ∞ãÊâæËàá‰ΩøÁî®ËÄÖÊü•Ë©¢ÊúÄÊé•ËøëÁöÑ‰∏ÄÁµÑÂêàÊàêÂºèÂïèÈ°åÂíåÈóúËÅØÂçÄÂ°ä„ÄÇÁôºÁèæ‰ΩøÁî®ÂéüÂ≠êÈÄ≤Ë°åÊ™¢Á¥¢ÊúÉÊØî‰ΩøÁî®ÂçÄÂ°äÈÄ≤Ë°åÊ™¢Á¥¢Áî¢ÁîüÊõ¥È´òÁöÑÂè¨ÂõûÁéá„ÄÇ‰ΩøÁî®Âú®ÂéüÂ≠ê‰∏äÁî¢ÁîüÁöÑÂêàÊàêÂºèÂïèÈ°åÈÄ≤Ë°åÊ™¢Á¥¢ÊôÇÔºåÈÇÑËßÄÂØüÂà∞ÊïàËÉΩÈÄ≤‰∏ÄÊ≠•ÊèêÂçá„ÄÇÊ™¢Á¥¢Ê≠•È©üÁöÑÂè¨ÂõûÁéáË∂äÈ´òÔºå‰ΩøÁî® RAG ÁÆ°Á∑öÁöÑ‰ºÅÊ•≠ LLM ÊïàËÉΩÂ∞±Ë∂äÈ´ò„ÄÇ

##### **Perturbing the Gradient for Alleviating Meta Overfitting**
2405.12299v1 by Manas Gogoi, Sambhavi Tiwari, Shekhar Verma

The reason for Meta Overfitting can be attributed to two factors: Mutual
Non-exclusivity and the Lack of diversity, consequent to which a single global
function can fit the support set data of all the meta-training tasks and fail
to generalize to new unseen tasks. This issue is evidenced by low error rates
on the meta-training tasks, but high error rates on new tasks. However, there
can be a number of novel solutions to this problem keeping in mind any of the
two objectives to be attained, i.e. to increase diversity in the tasks and to
reduce the confidence of the model for some of the tasks. In light of the
above, this paper proposes a number of solutions to tackle meta-overfitting on
few-shot learning settings, such as few-shot sinusoid regression and few shot
classification. Our proposed approaches demonstrate improved generalization
performance compared to state-of-the-art baselines for learning in a
non-mutually exclusive task setting. Overall, this paper aims to provide
insights into tackling overfitting in meta-learning and to advance the field
towards more robust and generalizable models.

ÊëòË¶ÅÔºöMeta ÈÅéÊì¨ÂêàÁöÑÂéüÂõ†ÂèØÊ≠∏Âõ†ÊñºÂÖ©ÂÄãÂõ†Á¥†ÔºöÁõ∏‰∫íÈùûÁç®‰ΩîÊÄßÂíåÁº∫‰πèÂ§öÊ®£ÊÄßÔºåÂõ†Ê≠§ÂñÆ‰∏ÄÂÖ®ÂüüÂáΩÊï∏ÂèØ‰ª•Êì¨ÂêàÊâÄÊúâÂÖÉË®ìÁ∑¥‰ªªÂãôÁöÑÊîØÊè¥ÈõÜË≥áÊñôÔºå‰ΩÜÁÑ°Ê≥ïÊ¶ÇÂåñÂà∞Êñ∞ÁöÑÊú™Ë¶ã‰ªªÂãô„ÄÇÊ≠§ÂïèÈ°åÁöÑË≠âÊìöÊòØÂÖÉË®ìÁ∑¥‰ªªÂãôÁöÑÈåØË™§Áéá‰ΩéÔºå‰ΩÜÊñ∞‰ªªÂãôÁöÑÈåØË™§ÁéáÈ´ò„ÄÇÁÑ∂ËÄåÔºåÂèØ‰ª•ÈáùÂ∞çÈÄôÂÄãÂïèÈ°åÊèêÂá∫Ë®±Â§öÊñ∞Á©éÁöÑËß£Ê±∫ÊñπÊ°àÔºåÂêåÊôÇËÄÉÊÖÆË¶ÅÈÅîÊàêÁöÑÂÖ©ÂÄãÁõÆÊ®ôÔºåÂç≥Â¢ûÂä†‰ªªÂãôÁöÑÂ§öÊ®£ÊÄß‰∏¶Èôç‰ΩéÊ®°ÂûãÂ∞çÊüê‰∫õ‰ªªÂãôÁöÑ‰ø°ÂøÉ„ÄÇÊúâÈëëÊñºÊ≠§ÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∫õËß£Ê±∫ÊñπÊ°à‰æÜËß£Ê±∫Â∞ëÊ¨°Â≠∏ÁøíË®≠ÂÆö‰∏≠ÁöÑÂÖÉÈÅéÊì¨ÂêàÔºå‰æãÂ¶ÇÂ∞ëÊ¨°Ê≠£Âº¶Ëø¥Ê≠∏ÂíåÂ∞ëÊ¨°ÂàÜÈ°û„ÄÇËàáÂ≠∏Áøí‰∏≠ÈùûÁõ∏‰∫íÊéí‰ªñ‰ªªÂãôË®≠ÂÆöÁöÑÊúÄÊñ∞Âü∫Ê∫ñÁ∑öÁõ∏ÊØîÔºåÊàëÂÄëÊèêÂá∫ÁöÑÊñπÊ≥ïÂ±ïÁ§∫‰∫ÜÊîπÈÄ≤ÁöÑÊ¶ÇÂåñÊïàËÉΩ„ÄÇÁ∏ΩËÄåË®Ä‰πãÔºåÊú¨ÊñáÊó®Âú®Êèê‰æõË¶ãËß£‰æÜËß£Ê±∫ÂÖÉÂ≠∏Áøí‰∏≠ÁöÑÈÅéÊì¨ÂêàÔºå‰∏¶Êé®ÂãïË©≤È†òÂüüÊúùÂêëÊõ¥Á©©ÂÅ•‰∏îÂèØÊ¶ÇÂåñÁöÑÊ®°Âûã„ÄÇ

##### **Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**
2405.12217v1 by Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Salman Khan, Xin Gao, Lina Yao

Recent studies indicate that large multimodal models (LMMs) are highly robust
against natural distribution shifts, often surpassing previous baselines.
Despite this, domain-specific adaptation is still necessary, particularly in
specialized areas like healthcare. Due to the impracticality of fine-tuning
LMMs given their vast parameter space, this work investigates in-context
learning (ICL) as an effective alternative for enhancing LMMs' adaptability. We
find that the success of ICL heavily relies on the choice of demonstration,
mirroring challenges seen in large language models but introducing unique
complexities for LMMs facing distribution shifts. Our study addresses this by
evaluating an unsupervised ICL method, TopKNearestPR, which selects in-context
examples through a nearest example search based on feature similarity. We
uncover that its effectiveness is limited by the deficiencies of pre-trained
vision encoders under distribution shift scenarios. To address these
challenges, we propose InvariantSelectPR, a novel method leveraging
Class-conditioned Contrastive Invariance (CCI) for more robust demonstration
selection. Specifically, CCI enhances pre-trained vision encoders by improving
their discriminative capabilities across different classes and ensuring
invariance to domain-specific variations. This enhancement allows the encoders
to effectively identify and retrieve the most informative examples, which are
then used to guide LMMs in adapting to new query samples under varying
distributions. Our experiments show that InvariantSelectPR substantially
improves the adaptability of LMMs, achieving significant performance gains on
benchmark datasets, with a 34.2%$\uparrow$ accuracy increase in 7-shot on
Camelyon17 and 16.9%$\uparrow$ increase in 7-shot on HAM10000 compared to the
baseline zero-shot performance.

ÊëòË¶ÅÔºö<paragraph>ÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°Âûã (LMM) ÂØπ‰∫éËá™ÁÑ∂ÂàÜÂ∏É‰ΩçÁßªÂÖ∑ÊúâÈ´òÂ∫¶È≤ÅÊ£íÊÄßÔºåÈÄöÂ∏∏Ë∂ÖË∂ä‰πãÂâçÁöÑÂü∫ÂáÜ„ÄÇ
Â∞ΩÁÆ°Â¶ÇÊ≠§ÔºåÈ¢ÜÂüüÁâπÂÆöÈÄÇÂ∫î‰ªçÁÑ∂ÊòØÂøÖË¶ÅÁöÑÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂåªÁñó‰øùÂÅ•Á≠â‰∏ì‰∏öÈ¢ÜÂüü„ÄÇÁî±‰∫é LMM ÂèÇÊï∞Á©∫Èó¥Â∑®Â§ßÔºåÂæÆË∞É‰∏çÂàáÂÆûÈôÖÔºåÂõ†Ê≠§ËøôÈ°πÂ∑•‰ΩúÁ†îÁ©∂‰∫ÜËØ≠Â¢ÉÂ≠¶‰π† (ICL) ‰Ωú‰∏∫Â¢ûÂº∫ LMM ÈÄÇÂ∫îÊÄßÁöÑÊúâÊïàÊõø‰ª£ÊñπÊ°à„ÄÇÊàë‰ª¨
ÂèëÁé∞ ICL ÁöÑÊàêÂäüÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂÜ≥‰∫éÊºîÁ§∫ÁöÑÈÄâÊã©ÔºåËøôÂèçÊò†‰∫ÜÂú®Â§ßËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁúãÂà∞ÁöÑÊåëÊàòÔºå‰ΩÜ‰∏∫Èù¢‰∏¥ÂàÜÂ∏É‰ΩçÁßªÁöÑ LMM ÂºïÂÖ•‰∫ÜÁã¨ÁâπÁöÑÂ§çÊùÇÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ÈÄöËøáËØÑ‰º∞Êó†ÁõëÁù£ ICL ÊñπÊ≥ï TopKNearestPR Êù•Ëß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåËØ•ÊñπÊ≥ïÈÄöËøáÂü∫‰∫éÁâπÂæÅÁõ∏‰ººÊÄßÁöÑÊúÄËøëÁ§∫‰æãÊêúÁ¥¢Êù•ÈÄâÊã©ËØ≠Â¢ÉÁ§∫‰æã„ÄÇÊàë‰ª¨
ÂèëÁé∞ÂÖ∂ÊúâÊïàÊÄßÂèóÂà∞È¢ÑËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®Âú®ÂàÜÂ∏É‰ΩçÁßªÂú∫ÊôØ‰∏ãÁöÑÁº∫Èô∑ÁöÑÈôêÂà∂„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õ
ÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü InvariantSelectPRÔºåËøôÊòØ‰∏ÄÁßçÂà©Áî®Á±ªÊù°‰ª∂ÂØπÊØî‰∏çÂèòÊÄß (CCI) ËøõË°åÊõ¥Á®≥ÂÅ•ÁöÑÊºîÁ§∫ÈÄâÊã©ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåCCI ÈÄöËøáÊèêÈ´òÈ¢ÑËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®Âú®‰∏çÂêåÁ±ªÂà´ÁöÑÂà§Âà´ËÉΩÂäõÂπ∂Á°Æ‰øù
ÂØπÁâπÂÆöÈ¢ÜÂüüÁöÑÂèòÂåñ‰øùÊåÅ‰∏çÂèòÊÄßÊù•Â¢ûÂº∫ÂÆÉ‰ª¨„ÄÇËøôÁßçÂ¢ûÂº∫‰ΩøÁºñÁ†ÅÂô®ËÉΩÂ§üÊúâÊïàËØÜÂà´ÂíåÊ£ÄÁ¥¢‰ø°ÊÅØÊúÄ‰∏∞ÂØåÁöÑÁ§∫‰æãÔºåÁÑ∂ÂêéÁî®‰∫éÊåáÂØº LMM Âú®‰∏çÂêåÂàÜÂ∏É‰∏ãÈÄÇÂ∫îÊñ∞ÁöÑÊü•ËØ¢Ê†∑Êú¨„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåInvariantSelectPR Â§ßÂ§ßÊèêÈ´ò‰∫Ü LMM ÁöÑÈÄÇÂ∫îÊÄßÔºåÂú®Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂú® Camelyon17 ‰∏äÁöÑ 7 Ê¨°ÊãçÊëÑÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü 34.2%ÔºåÂú® HAM10000 ‰∏äÁöÑ 7 Ê¨°ÊãçÊëÑÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü 16.9%Ôºå‰∏é
Âü∫Á∫øÈõ∂Ê¨°ÊãçÊëÑÊÄßËÉΩÁõ∏ÊØî„ÄÇ</paragraph>

##### **MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**
2405.12209v1 by Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, Kai Chen

Recent advancements in large language models (LLMs) have showcased
significant improvements in mathematics. However, traditional math benchmarks
like GSM8k offer a unidimensional perspective, falling short in providing a
holistic assessment of the LLMs' math capabilities. To address this gap, we
introduce MathBench, a new benchmark that rigorously assesses the mathematical
capabilities of large language models. MathBench spans a wide range of
mathematical disciplines, offering a detailed evaluation of both theoretical
understanding and practical problem-solving skills. The benchmark progresses
through five distinct stages, from basic arithmetic to college mathematics, and
is structured to evaluate models at various depths of knowledge. Each stage
includes theoretical questions and application problems, allowing us to measure
a model's mathematical proficiency and its ability to apply concepts in
practical scenarios. MathBench aims to enhance the evaluation of LLMs'
mathematical abilities, providing a nuanced view of their knowledge
understanding levels and problem solving skills in a bilingual context. The
project is released at https://github.com/open-compass/MathBench .

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ËøëÊúüÈÄ≤Â±ïÂ∑≤Â±ïÁ§∫Âá∫Êï∏Â≠∏ÊñπÈù¢ÁöÑÈáçÂ§ßÊîπÈÄ≤„ÄÇÁÑ∂ËÄåÔºåÂÇ≥Áµ±Êï∏Â≠∏Âü∫Ê∫ñÔºà‰æãÂ¶Ç GSM8kÔºâÊèê‰æõ‰∫Ü‰∏ÄÁ®ÆÂñÆÂêëÂ∫¶ËßÄÈªûÔºåÁÑ°Ê≥ïÂÖ®Èù¢Ë©ï‰º∞ LLM ÁöÑÊï∏Â≠∏ËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂ∑ÆË∑ùÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü MathBenchÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞ÁöÑÂü∫Ê∫ñÔºåÂèØ‰ª•Âö¥Ê†ºË©ï‰º∞Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÊï∏Â≠∏ËÉΩÂäõ„ÄÇMathBench Ê∂µËìã‰∫ÜÂª£Ê≥õÁöÑÊï∏Â≠∏Â≠∏ÁßëÔºåÂ∞çÁêÜË´ñÁêÜËß£ÂíåÂØ¶ÈöõÂïèÈ°åËß£Ê±∫ÊäÄËÉΩÈÄ≤Ë°å‰∫ÜË©≥Á¥∞Ë©ï‰º∞„ÄÇÂü∫Ê∫ñÂàÜÁÇ∫‰∫îÂÄã‰∏çÂêåÁöÑÈöéÊÆµÔºåÂæûÂü∫Á§éÁÆóË°ìÂà∞Â§ßÂ≠∏Êï∏Â≠∏Ôºå‰∏¶Êó®Âú®Ë©ï‰º∞Ê®°ÂûãÂú®‰∏çÂêåÁü•Ë≠òÊ∑±Â∫¶‰∏ãÁöÑË°®Áèæ„ÄÇÊØèÂÄãÈöéÊÆµÈÉΩÂåÖÂê´ÁêÜË´ñÂïèÈ°åÂíåÊáâÁî®ÂïèÈ°åÔºåËÆìÊàëÂÄëÂèØ‰ª•Ë°°ÈáèÊ®°ÂûãÁöÑÊï∏Â≠∏ËÉΩÂäõÂèäÂÖ∂Âú®ÂØ¶ÈöõÂ†¥ÊôØ‰∏≠ÊáâÁî®Ê¶ÇÂøµÁöÑËÉΩÂäõ„ÄÇMathBench Êó®Âú®Âä†Âº∑Â∞ç LLM Êï∏Â≠∏ËÉΩÂäõÁöÑË©ï‰º∞ÔºåÂú®ÈõôË™ûÁí∞Â¢É‰∏≠Êèê‰æõÂÖ∂Áü•Ë≠òÁêÜËß£Â±§Ê¨°ÂíåÂïèÈ°åËß£Ê±∫ÊäÄËÉΩÁöÑÁ¥∞Á∑ªËßÄÈªû„ÄÇË©≤Â∞àÊ°àÂ∑≤Âú® https://github.com/open-compass/MathBench ÁôºÂ∏É„ÄÇ

##### **Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models**
2405.12206v1 by Tong Zeng, Daniel E. Acuna

Scientist learn early on how to cite scientific sources to support their
claims. Sometimes, however, scientists have challenges determining where a
citation should be situated -- or, even worse, fail to cite a source
altogether. Automatically detecting sentences that need a citation (i.e.,
citation worthiness) could solve both of these issues, leading to more robust
and well-constructed scientific arguments. Previous researchers have applied
machine learning to this task but have used small datasets and models that do
not take advantage of recent algorithmic developments such as attention
mechanisms in deep learning. We hypothesize that we can develop significantly
accurate deep learning architectures that learn from large supervised datasets
constructed from open access publications. In this work, we propose a
Bidirectional Long Short-Term Memory (BiLSTM) network with attention mechanism
and contextual information to detect sentences that need citations. We also
produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset,
which is orders of magnitude larger than previous datasets. Our experiments
show that our architecture achieves state of the art performance on the
standard ACL-ARC dataset ($F_{1}=0.507$) and exhibits high performance
($F_{1}=0.856$) on the new PMOA-CITE. Moreover, we show that it can transfer
learning across these datasets. We further use interpretable models to
illuminate how specific language is used to promote and inhibit citations. We
discover that sections and surrounding sentences are crucial for our improved
predictions. We further examined purported mispredictions of the model, and
uncovered systematic human mistakes in citation behavior and source data. This
opens the door for our model to check documents during pre-submission and
pre-archival procedures. We make this new dataset, the code, and a web-based
tool available to the community.

ÊëòË¶ÅÔºö<paragraph>ÁßëÂ≠∏ÂÆ∂ÂæàÊó©Â∞±Â≠∏ÊúÉÂ¶Ç‰ΩïÂºïÁî®ÁßëÂ≠∏ÊñáÁçª‰æÜÊîØÊåÅ‰ªñÂÄëÁöÑË™™Ê≥ï„ÄÇÁÑ∂ËÄåÔºåÊúâÊôÇÂÄôÔºåÁßëÂ≠∏ÂÆ∂Âú®Ê±∫ÂÆöÂºïÊñáÊáâÁΩÆÊñº‰ΩïËôïÊôÇÊúÉÈÅáÂà∞ÊåëÊà∞ÔºåÊàñËÄÖÊõ¥Á≥üÁöÑÊòØÔºåÊ†πÊú¨Ê≤íÊúâÂºïÁî®‰æÜÊ∫ê„ÄÇËá™ÂãïÂÅµÊ∏¨ÈúÄË¶ÅÂºïÁî®ÁöÑÂè•Â≠êÔºàÂç≥ÂºïÁî®ÁöÑÂÉπÂÄºÔºâÂèØ‰ª•Ëß£Ê±∫ÈÄôÂÖ©ÂÄãÂïèÈ°åÔºåÂæûËÄåÁî¢ÁîüÊõ¥Âº∑Â§ß‰∏îÁµêÊßãËâØÂ•ΩÁöÑÁßëÂ≠∏Ë´ñË≠â„ÄÇÂÖàÂâçÁöÑÁ†îÁ©∂‰∫∫Âì°Â∑≤Â∞áÊ©üÂô®Â≠∏ÁøíÊáâÁî®ÊñºÊ≠§‰ªªÂãôÔºå‰ΩÜ‰ΩøÁî®‰∫ÜÂ∞èÂûãË≥áÊñôÈõÜÂíåÊ®°ÂûãÔºåÈÄô‰∫õÊ®°Âûã‰∏¶Êú™Âà©Áî®Ê∑±Â∫¶Â≠∏Áøí‰∏≠ÁöÑÊúÄÊñ∞ÊºîÁÆóÊ≥ïÁôºÂ±ïÔºå‰æãÂ¶ÇÊ≥®ÊÑèÂäõÊ©üÂà∂„ÄÇÊàëÂÄëÂÅáË®≠ÊàëÂÄëÂèØ‰ª•ÈñãÁôºÂá∫ÂæûÁî±ÈñãÊîæÁç≤ÂèñÂá∫ÁâàÁâ©Âª∫ÊßãÁöÑÂ§ßÂûãÁõ£Áù£ÂºèË≥áÊñôÈõÜ‰∏≠Â≠∏ÁøíÁöÑÈ°ØËëóÊ∫ñÁ¢∫Ê∑±Â∫¶Â≠∏ÁøíÊû∂Êßã„ÄÇÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÂÖ∑ÊúâÊ≥®ÊÑèÂäõÊ©üÂà∂Âíå‰∏ä‰∏ãÊñáË≥áË®äÁöÑÈõôÂêëÈï∑Áü≠ÊúüË®òÊÜ∂ (BiLSTM) Á∂≤Ë∑ØÔºå‰ª•ÂÅµÊ∏¨ÈúÄË¶ÅÂºïÁî®ÁöÑÂè•Â≠ê„ÄÇÊàëÂÄëÈÇÑÊ†πÊìö PubMed ÈñãÊîæÁç≤ÂèñÂ≠êÈõÜË£Ω‰Ωú‰∫Ü‰∏ÄÂÄãÊñ∞ÁöÑÂ§ßÂûãË≥áÊñôÈõÜ (PMOA-CITE)ÔºåÂÖ∂Êï∏ÈáèÁ¥öÂ§ßÊñºÂÖàÂâçÁöÑË≥áÊñôÈõÜ„ÄÇÊàëÂÄëÁöÑÂØ¶È©óË°®ÊòéÔºåÊàëÂÄëÁöÑÊû∂ÊßãÂú®Ê®ôÊ∫ñ ACL-ARC Ë≥áÊñôÈõÜ ($F_{1}=0.507$) ‰∏äÈÅîÂà∞‰∫ÜÊúÄÂÖàÈÄ≤ÁöÑÊïàËÉΩÔºå‰∏¶Âú®Êñ∞ÁöÑ PMOA-CITE ‰∏äË°®ÁèæÂá∫È´òÊïàËÉΩ ($F_{1}=0.856$)„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂÆÉÂèØ‰ª•Âú®ÈÄô‰∫õË≥áÊñôÈõÜ‰πãÈñìËΩâÁßªÂ≠∏Áøí„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•‰ΩøÁî®ÂèØËß£ÈáãÊ®°Âûã‰æÜË™™ÊòéÂ¶Ç‰Ωï‰ΩøÁî®ÁâπÂÆöË™ûË®Ä‰æÜ‰øÉÈÄ≤ÂíåÊäëÂà∂ÂºïÁî®„ÄÇÊàëÂÄëÁôºÁèæÁ´†ÁØÄÂíåÂë®ÂúçÁöÑÂè•Â≠êÂ∞çÊñºÊàëÂÄëÊîπÈÄ≤ÁöÑÈ†êÊ∏¨Ëá≥ÈóúÈáçË¶Å„ÄÇÊàëÂÄëÈÄ≤‰∏ÄÊ≠•Ê™¢Êü•‰∫ÜÊ®°ÂûãÁöÑÂÅáÂÆöÈåØË™§È†êÊ∏¨Ôºå‰∏¶ÁôºÁèæ‰∫ÜÂºïÁî®Ë°åÁÇ∫Âíå‰æÜÊ∫êË≥áÊñô‰∏≠ÁöÑ‰∫∫ÁÇ∫Á≥ªÁµ±ÊÄßÈåØË™§„ÄÇÈÄôÁÇ∫ÊàëÂÄëÁöÑÊ®°ÂûãÂú®Êèê‰∫§ÂâçÂíåÊ≠∏Ê™îÂâçÁ®ãÂ∫è‰∏≠Ê™¢Êü•Êñá‰ª∂ÈñãÂïü‰∫ÜÂ§ßÈñÄ„ÄÇÊàëÂÄëÂ∞áÈÄôÂÄãÊñ∞Ë≥áÊñôÈõÜ„ÄÅÁ®ãÂºèÁ¢ºÂíå‰∏ÄÂÄãÂü∫ÊñºÁ∂≤Ë∑ØÁöÑÂ∑•ÂÖ∑Êèê‰æõÁµ¶Á§æÁæ§‰ΩøÁî®„ÄÇ</paragraph>

##### **Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving**
2405.12205v1 by Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, Sanjeev Arora

Metacognitive knowledge refers to humans' intuitive knowledge of their own
thinking and reasoning processes. Today's best LLMs clearly possess some
reasoning processes. The paper gives evidence that they also have metacognitive
knowledge, including ability to name skills and procedures to apply given a
task. We explore this primarily in context of math reasoning, developing a
prompt-guided interaction procedure to get a powerful LLM to assign sensible
skill labels to math questions, followed by having it perform semantic
clustering to obtain coarser families of skill labels. These coarse skill
labels look interpretable to humans.
  To validate that these skill labels are meaningful and relevant to the LLM's
reasoning processes we perform the following experiments. (a) We ask GPT-4 to
assign skill labels to training questions in math datasets GSM8K and MATH. (b)
When using an LLM to solve the test questions, we present it with the full list
of skill labels and ask it to identify the skill needed. Then it is presented
with randomly selected exemplar solved questions associated with that skill
label. This improves accuracy on GSM8k and MATH for several strong LLMs,
including code-assisted models. The methodology presented is domain-agnostic,
even though this article applies it to math problems.

ÊëòË¶ÅÔºöÂÖÉËÆ§Áü•Áü•ËØÜÊòØÊåá‰∫∫Á±ªÂØπËá™Ë∫´ÊÄùËÄÉÂíåÊé®ÁêÜËøáÁ®ãÁöÑÁõ¥ËßâÁü•ËØÜ„ÄÇÂΩì‰ªäÊúÄÂ•ΩÁöÑ LLM ÊòæÁÑ∂Êã•Êúâ‰∏Ä‰∫õÊé®ÁêÜËøáÁ®ã„ÄÇÊú¨ÊñáÊèê‰æõ‰∫ÜËØÅÊçÆË°®ÊòéÂÆÉ‰ª¨‰πüÂÖ∑ÊúâÂÖÉËÆ§Áü•Áü•ËØÜÔºåÂåÖÊã¨Ê†πÊçÆÁªôÂÆöÁöÑ‰ªªÂä°Êù•ÂëΩÂêçÊäÄËÉΩÂíåÂ∫îÁî®Á®ãÂ∫èÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨‰∏ªË¶ÅÂú®Êï∞Â≠¶Êé®ÁêÜÁöÑËÉåÊôØ‰∏ãÊé¢Á¥¢Ëøô‰∏ÄÁÇπÔºåÂºÄÂèë‰∫Ü‰∏Ä‰∏™ÊèêÁ§∫ÂºïÂØºÁöÑ‰∫§‰∫íÁ®ãÂ∫èÔºåËÆ©‰∏Ä‰∏™Âº∫Â§ßÁöÑ LLM ‰∏∫Êï∞Â≠¶ÈóÆÈ¢òÂàÜÈÖçÊòéÊô∫ÁöÑÊäÄËÉΩÊ†áÁ≠æÔºåÁÑ∂ÂêéÂØπÂÖ∂ÊâßË°åËØ≠‰πâËÅöÁ±ª‰ª•Ëé∑ÂæóÊõ¥Á≤óÁï•ÁöÑÊäÄËÉΩÊ†áÁ≠æÊóè„ÄÇËøô‰∫õÁ≤óÁï•ÁöÑÊäÄËÉΩÊ†áÁ≠æÁúãËµ∑Êù•ÂØπ‰∫∫Á±ªÊù•ËØ¥ÊòØÂèØËß£ÈáäÁöÑ„ÄÇ
‰∏∫‰∫ÜÈ™åËØÅËøô‰∫õÊäÄËÉΩÊ†áÁ≠æÂØπ LLM ÁöÑÊé®ÁêÜËøáÁ®ãÊúâÊÑè‰πâ‰∏îÁõ∏ÂÖ≥ÔºåÊàë‰ª¨ËøõË°å‰∫Ü‰ª•‰∏ãÂÆûÈ™å„ÄÇ(a) Êàë‰ª¨Ë¶ÅÊ±Ç GPT-4 ‰∏∫Êï∞Â≠¶Êï∞ÊçÆÈõÜ GSM8K Âíå MATH ‰∏≠ÁöÑËÆ≠ÁªÉÈóÆÈ¢òÂàÜÈÖçÊäÄËÉΩÊ†áÁ≠æ„ÄÇ(b) ÂΩì‰ΩøÁî® LLM Êù•Ëß£ÂÜ≥ÊµãËØïÈóÆÈ¢òÊó∂ÔºåÊàë‰ª¨ÂêëÂÖ∂Êèê‰æõÂÆåÊï¥ÁöÑÊäÄËÉΩÊ†áÁ≠æÂàóË°®ÔºåÂπ∂Ë¶ÅÊ±ÇÂÖ∂ËØÜÂà´ÊâÄÈúÄÁöÑÊäÄËÉΩ„ÄÇÁÑ∂ÂêéÂêëÂÖ∂Â±ïÁ§∫‰∏éËØ•ÊäÄËÉΩÊ†áÁ≠æÁõ∏ÂÖ≥ËÅîÁöÑÈöèÊú∫ÈÄâÊã©ÁöÑÁ§∫‰æãÊ±ÇËß£ÈóÆÈ¢ò„ÄÇËøôÊèêÈ´ò‰∫Ü GSM8k Âíå MATH ÂØπÂá†‰∏™Âº∫Â§ßÁöÑ LLM ÁöÑÂáÜÁ°ÆÊÄßÔºåÂåÖÊã¨‰ª£Á†ÅËæÖÂä©Ê®°Âûã„ÄÇÊâÄÊèêÂá∫ÁöÑÊñπÊ≥ïËÆ∫‰∏éÈ¢ÜÂüüÊó†ÂÖ≥ÔºåÂ∞ΩÁÆ°Êú¨ÊñáÂ∞ÜÂÖ∂Â∫îÁî®‰∫éÊï∞Â≠¶ÈóÆÈ¢ò„ÄÇ

##### **Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution**
2405.12202v1 by Xihaier Luo, Xiaoning Qian, Byung-Jun Yoon

In this work, we present an arbitrary-scale super-resolution (SR) method to
enhance the resolution of scientific data, which often involves complex
challenges such as continuity, multi-scale physics, and the intricacies of
high-frequency signals. Grounded in operator learning, the proposed method is
resolution-invariant. The core of our model is a hierarchical neural operator
that leverages a Galerkin-type self-attention mechanism, enabling efficient
learning of mappings between function spaces. Sinc filters are used to
facilitate the information transfer across different levels in the hierarchy,
thereby ensuring representation equivalence in the proposed neural operator.
Additionally, we introduce a learnable prior structure that is derived from the
spectral resizing of the input data. This loss prior is model-agnostic and is
designed to dynamically adjust the weighting of pixel contributions, thereby
balancing gradients effectively across the model. We conduct extensive
experiments on diverse datasets from different domains and demonstrate
consistent improvements compared to strong baselines, which consist of various
state-of-the-art SR methods.

ÊëòË¶ÅÔºöÂú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®Æ‰ªªÊÑèÂ∞∫Â∫¶ÁöÑË∂ÖËß£ÊûêÂ∫¶ (SR) ÊñπÊ≥ïÔºå‰ª•Â¢ûÂº∑ÁßëÂ≠∏Êï∏ÊìöÁöÑÂàÜËæ®ÁéáÔºåÈÄôÈÄöÂ∏∏Ê∂âÂèäÈÄ£Á∫åÊÄß„ÄÅÂ§öÂ∞∫Â∫¶Áâ©ÁêÜ‰ª•ÂèäÈ´òÈ†ªÁéáË®äËôüÁöÑË§áÈõúÊÄßÁ≠âÊåëÊà∞„ÄÇË©≤ÊñπÊ≥ï‰ª•ÁÆóÂ≠êÂ≠∏ÁøíÁÇ∫Âü∫Á§éÔºåÂÖ∑ÊúâËß£ÊûêÂ∫¶‰∏çËÆäÊÄß„ÄÇÊàëÂÄëÊ®°ÂûãÁöÑÊ†∏ÂøÉÊòØ‰∏ÄÂÄãÂàÜÂ±§Á•ûÁ∂ìÁÆóÂ≠êÔºåÂÆÉÂà©Áî®‰∫Ü‰ºΩÈÅºÈáëÈ°ûÂûãËá™Ê≥®ÊÑèÂäõÊ©üÂà∂ÔºåÂØ¶Áèæ‰∫ÜÂáΩÊï∏Á©∫Èñì‰πãÈñìÂ∞çÊáâÈóú‰øÇÁöÑÊúâÊïàÂ≠∏Áøí„ÄÇSinc ÊøæÊ≥¢Âô®Áî®Êñº‰øÉÈÄ≤Â±§Ê¨°ÁµêÊßã‰∏≠‰∏çÂêåÂ±§Á¥ö‰πãÈñìÁöÑË≥áË®äÂÇ≥ÈÅûÔºåÂæûËÄåÁ¢∫‰øù‰∫ÜÊâÄÊèêÂá∫ÁöÑÁ•ûÁ∂ìÁÆóÂ≠ê‰∏≠ÁöÑË°®Á§∫Á≠âÂÉπÊÄß„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÈÇÑÂºïÂÖ•‰∫Ü‰∏ÄÂÄãÂèØÂ≠∏ÁøíÁöÑÂÖàÈ©óÁµêÊßãÔºåÂÆÉ‰æÜËá™Ëº∏ÂÖ•Êï∏ÊìöÁöÑÈ†ªË≠úË™øÊï¥„ÄÇÊ≠§ÊêçÂ§±ÂÖàÈ©óËàáÊ®°ÂûãÁÑ°ÈóúÔºåÊó®Âú®ÂãïÊÖãË™øÊï¥ÂÉèÁ¥†Ë≤¢ÁçªÁöÑÊ¨äÈáçÔºåÂæûËÄåÊúâÊïàÂú∞Âπ≥Ë°°Êï¥ÂÄãÊ®°Âûã‰∏≠ÁöÑÊ¢ØÂ∫¶„ÄÇÊàëÂÄëÂ∞ç‰æÜËá™‰∏çÂêåÈ†òÂüüÁöÑ‰∏çÂêåÊï∏ÊìöÈõÜÈÄ≤Ë°å‰∫ÜÂª£Ê≥õÁöÑÂØ¶È©óÔºå‰∏¶Â±ïÁ§∫‰∫ÜËàáÂº∑Âü∫Á∑öÁõ∏ÊØîÁöÑ‰∏ÄËá¥ÊîπÈÄ≤ÔºåÈÄô‰∫õÂü∫Á∑öÂåÖÊã¨ÂêÑÁ®ÆÊúÄÂÖàÈÄ≤ÁöÑ SR ÊñπÊ≥ï„ÄÇ

##### **Multi-order Graph Clustering with Adaptive Node-level Weight Learning**
2405.12183v1 by Ye Liu, Xuelei Lin, Yejia Chen, Reynold Cheng

Current graph clustering methods emphasize individual node and edge con
nections, while ignoring higher-order organization at the level of motif. Re
cently, higher-order graph clustering approaches have been designed by motif
based hypergraphs. However, these approaches often suffer from hypergraph
fragmentation issue seriously, which degrades the clustering performance
greatly. Moreover, real-world graphs usually contain diverse motifs, with nodes
participating in multiple motifs. A key challenge is how to achieve precise
clustering results by integrating information from multiple motifs at the node
level. In this paper, we propose a multi-order graph clustering model (MOGC) to
integrate multiple higher-order structures and edge connections at node level.
MOGC employs an adaptive weight learning mechanism to au tomatically adjust the
contributions of different motifs for each node. This not only tackles
hypergraph fragmentation issue but enhances clustering accuracy. MOGC is
efficiently solved by an alternating minimization algo rithm. Experiments on
seven real-world datasets illustrate the effectiveness of MOGC.

ÊëòË¶ÅÔºöÁõÆÂâçÂúñÂΩ¢ËÅöÈ°ûÊñπÊ≥ïÂº∑Ë™øÂÄãÂà•ÁØÄÈªûÂíåÈÇäÁ∑£ÈÄ£Êé•ÔºåËÄåÂøΩÁï•‰∫ÜÊØçÈ°åÂ±§Á¥öÁöÑÈ´òÈöéÁµÑÁπî„ÄÇÊúÄËøëÔºåÈ´òÈöéÂúñÂΩ¢ËÅöÈ°ûÊñπÊ≥ïÂ∑≤Áî±Âü∫ÊñºÊØçÈ°åÁöÑË∂ÖÂúñË®≠Ë®à„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÈÄöÂ∏∏ÊúÉÈÅ≠ÂèóË∂ÖÂúñÁ¢éÁâáÂåñÂïèÈ°åÁöÑÂö¥ÈáçÂΩ±ÈüøÔºåÈÄôÊúÉÂ§ßÂπÖÈôç‰ΩéËÅöÈ°ûÊïàËÉΩ„ÄÇÊ≠§Â§ñÔºåÁúüÂØ¶‰∏ñÁïåÁöÑÂúñÂΩ¢ÈÄöÂ∏∏ÂåÖÂê´Â§öÁ®ÆÊØçÈ°åÔºå‰∏îÁØÄÈªûÂèÉËàáÂ§öÂÄãÊØçÈ°å„ÄÇ‰∏ÄÈ†ÖÈóúÈçµÊåëÊà∞ÊòØÂ¶Ç‰ΩïÈÄèÈÅéÊï¥ÂêàÁØÄÈªûÂ±§Á¥ö‰æÜËá™Â§öÂÄãÊØçÈ°åÁöÑË≥áË®äÔºå‰æÜÈÅîÊàêÁ≤æÁ¢∫ÁöÑËÅöÈ°ûÁµêÊûú„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÂ§öÈöéÂúñÂΩ¢ËÅöÈ°ûÊ®°Âûã (MOGC)Ôºå‰ª•Êï¥ÂêàÁØÄÈªûÂ±§Á¥öÁöÑÂ§öÂÄãÈ´òÈöéÁµêÊßãÂíåÈÇäÁ∑£ÈÄ£Êé•„ÄÇMOGC Êé°Áî®Ëá™ÈÅ©ÊáâÊ¨äÈáçÂ≠∏ÁøíÊ©üÂà∂ÔºåËá™ÂãïË™øÊï¥ÊØèÂÄãÁØÄÈªû‰∏çÂêåÊØçÈ°åÁöÑË≤¢Áçª„ÄÇÈÄô‰∏çÂÉÖËß£Ê±∫‰∫ÜË∂ÖÂúñÁ¢éÁâáÂåñÂïèÈ°åÔºå‰πüÂ¢ûÂº∑‰∫ÜËÅöÈ°ûÊ∫ñÁ¢∫Â∫¶„ÄÇMOGC ÂèØÈÄèÈÅé‰∫§ÊõøÊúÄÂ∞èÂåñÊºîÁÆóÊ≥ïÊúâÊïàÁéáÂú∞Ëß£Ê±∫„ÄÇÂú®‰∏ÉÂÄãÁúüÂØ¶‰∏ñÁïåË≥áÊñôÈõÜ‰∏äÁöÑÂØ¶È©óË™™Êòé‰∫Ü MOGC ÁöÑÊúâÊïàÊÄß„ÄÇ

##### **Building Temporal Kernels with Orthogonal Polynomials**
2405.12179v1 by Yan Ru Pei, Olivier Coenen

We introduce a class of models named PLEIADES (PoLynomial Expansion In
Adaptive Distributed Event-based Systems), which contains temporal convolution
kernels generated from orthogonal polynomial basis functions. We focus on
interfacing these networks with event-based data to perform online
spatiotemporal classification and detection with low latency. By virtue of
using structured temporal kernels and event-based data, we have the freedom to
vary the sample rate of the data along with the discretization step-size of the
network without additional finetuning. We experimented with three event-based
benchmarks and obtained state-of-the-art results on all three by large margins
with significantly smaller memory and compute costs. We achieved: 1) 99.59%
accuracy with 192K parameters on the DVS128 hand gesture recognition dataset
and 100% with a small additional output filter; 2) 99.58% test accuracy with
277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with
576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.

ÊëòË¶ÅÔºö<paragraph>Êàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁ±ªÂêç‰∏∫ PLEIADESÔºàËá™ÈÄÇÂ∫îÂàÜÂ∏ÉÂºè‰∫ã‰ª∂È©±Âä®Á≥ªÁªü‰∏≠ÁöÑÂ§öÈ°πÂºèÊâ©Â±ïÔºâÁöÑÊ®°ÂûãÔºåÂÖ∂‰∏≠ÂåÖÂê´Áî±Ê≠£‰∫§Â§öÈ°πÂºèÂü∫ÂáΩÊï∞ÁîüÊàêÁöÑÊó∂Â∫èÂç∑ÁßØÊ†∏„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÂ∞ÜËøô‰∫õÁΩëÁªú‰∏éÂü∫‰∫é‰∫ã‰ª∂ÁöÑÊï∞ÊçÆËøõË°åÊé•Âè£Ôºå‰ª•ÊâßË°åÂÖ∑Êúâ‰ΩéÂª∂ËøüÁöÑÂú®Á∫øÊó∂Á©∫ÂàÜÁ±ªÂíåÊ£ÄÊµã„ÄÇÁî±‰∫é‰ΩøÁî®‰∫ÜÁªìÊûÑÂåñÁöÑÊó∂Â∫èÊ†∏ÂíåÂü∫‰∫é‰∫ã‰ª∂ÁöÑÊï∞ÊçÆÔºåÊàë‰ª¨ÂèØ‰ª•Âú®‰∏çËøõË°åÈ¢ùÂ§ñÂæÆË∞ÉÁöÑÊÉÖÂÜµ‰∏ãËá™Áî±ÊîπÂèòÊï∞ÊçÆÁöÑÈááÊ†∑Áéá‰ª•ÂèäÁΩëÁªúÁöÑÁ¶ªÊï£ÂåñÊ≠•Èïø„ÄÇÊàë‰ª¨ÂØπ‰∏â‰∏™Âü∫‰∫é‰∫ã‰ª∂ÁöÑÂü∫ÂáÜËøõË°å‰∫ÜÂÆûÈ™åÔºåÂú®ÊâÄÊúâ‰∏â‰∏™Âü∫ÂáÜ‰∏äÈÉΩËé∑Âæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂπ∂‰∏îÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂíåËÆ°ÁÆóÊàêÊú¨„ÄÇÊàë‰ª¨ÂÆûÁé∞‰∫ÜÔºö1ÔºâÂú® DVS128 ÊâãÂäøËØÜÂà´Êï∞ÊçÆÈõÜ‰∏äÔºå‰ΩøÁî® 192K ÂèÇÊï∞ÂÆûÁé∞‰∫Ü 99.59% ÁöÑÂáÜÁ°ÆÂ∫¶Ôºå‰ΩøÁî®‰∏Ä‰∏™Â∞èÁöÑÈôÑÂä†ËæìÂá∫Êª§Ê≥¢Âô®ÂÆûÁé∞‰∫Ü 100% ÁöÑÂáÜÁ°ÆÂ∫¶Ôºõ2ÔºâÂú® AIS 2024 ÁúºÂä®ËøΩË∏™ÊåëÊàò‰∏≠Ôºå‰ΩøÁî® 277K ÂèÇÊï∞ÂÆûÁé∞‰∫Ü 99.58% ÁöÑÊµãËØïÂáÜÁ°ÆÂ∫¶Ôºõ3ÔºâÂú® PROPHESEE 1 Áôæ‰∏áÂÉèÁ¥†Ê±ΩËΩ¶Ê£ÄÊµãÊï∞ÊçÆÈõÜ‰∏≠Ôºå‰ΩøÁî® 576k ÂèÇÊï∞ÂÆûÁé∞‰∫Ü 0.556 mAP„ÄÇ</paragraph>

##### **CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**
2405.12174v1 by Haoxiang Shi, Jiaan Wang, Jiarong Xu, Cen Wang, Tetsuya Sakai

Text-to-Table aims to generate structured tables to convey the key
information from unstructured documents. Existing text-to-table datasets are
typically oriented English, limiting the research in non-English languages.
Meanwhile, the emergence of large language models (LLMs) has shown great
success as general task solvers in multi-lingual settings (e.g., ChatGPT),
theoretically enabling text-to-table in other languages. In this paper, we
propose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this
task. Our preliminary analysis of English text-to-table datasets highlights two
key factors for dataset construction: data diversity and data hallucination.
Inspired by this, the CT-Eval dataset selects a popular Chinese
multidisciplinary online encyclopedia as the source and covers 28 domains to
ensure data diversity. To minimize data hallucination, we first train an LLM to
judge and filter out the task samples with hallucination, then employ human
annotators to clean the hallucinations in the validation and testing sets.
After this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we
evaluate the performance of open-source and closed-source LLMs. Our results
reveal that zero-shot LLMs (including GPT-4) still have a significant
performance gap compared with human judgment. Furthermore, after fine-tuning,
open-source LLMs can significantly improve their text-to-table ability,
outperforming GPT-4 by a large margin. In short, CT-Eval not only helps
researchers evaluate and quickly understand the Chinese text-to-table ability
of existing LLMs but also serves as a valuable resource to significantly
improve the text-to-table performance of LLMs.

ÊëòË¶ÅÔºöÊñáÊú¨Âà∞Ë°®Ê†ºÊó®Âú®ÁîüÊàêÁªìÊûÑÂåñË°®Ê†ºÊù•‰º†ËææÈùûÁªìÊûÑÂåñÊñáÊ°£‰∏≠ÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇÁé∞ÊúâÁöÑÊñáÊú¨Âà∞Ë°®Ê†ºÊï∞ÊçÆÈõÜÈÄöÂ∏∏Èù¢ÂêëËã±ËØ≠ÔºåÈôêÂà∂‰∫ÜÈùûËã±ËØ≠ËØ≠Ë®ÄÁöÑÁ†îÁ©∂„ÄÇ‰∏éÊ≠§ÂêåÊó∂ÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂá∫Áé∞Â∑≤ÊòæÁ§∫Âá∫‰Ωú‰∏∫Â§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ÁöÑÈÄöÁî®‰ªªÂä°Ëß£ÂÜ≥ËÄÖÁöÑÂ∑®Â§ßÊàêÂäüÔºà‰æãÂ¶Ç ChatGPTÔºâÔºåÁêÜËÆ∫‰∏äÂèØ‰ª•Âú®ÂÖ∂‰ªñËØ≠Ë®Ä‰∏≠ÂÆûÁé∞ÊñáÊú¨Âà∞Ë°®Ê†º„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™‰∏≠ÊñáÊñáÊú¨Âà∞Ë°®Ê†ºÊï∞ÊçÆÈõÜ CT-EvalÔºå‰ª•ÂØπ LLM Âú®Ê≠§‰ªªÂä°‰∏äËøõË°åÂü∫ÂáÜÊµãËØï„ÄÇÊàë‰ª¨ÂØπËã±ËØ≠ÊñáÊú¨Âà∞Ë°®Ê†ºÊï∞ÊçÆÈõÜÁöÑÂàùÊ≠•ÂàÜÊûêÁ™ÅÂá∫‰∫ÜÊï∞ÊçÆÈõÜÊûÑÂª∫ÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÂõ†Á¥†ÔºöÊï∞ÊçÆÂ§öÊ†∑ÊÄßÂíåÊï∞ÊçÆÂπªËßâ„ÄÇÂèóÊ≠§ÂêØÂèëÔºåCT-Eval Êï∞ÊçÆÈõÜÈÄâÊã©‰∫Ü‰∏Ä‰∏™ÊµÅË°åÁöÑ‰∏≠ÊñáÂ§öÂ≠¶ÁßëÂú®Á∫øÁôæÁßëÂÖ®‰π¶‰Ωú‰∏∫Êù•Ê∫êÔºåÂπ∂Ê∂µÁõñ 28 ‰∏™È¢ÜÂüü‰ª•Á°Æ‰øùÊï∞ÊçÆÂ§öÊ†∑ÊÄß„ÄÇ‰∏∫‰∫ÜÊúÄÂ§ßÁ®ãÂ∫¶Âú∞ÂáèÂ∞ëÊï∞ÊçÆÂπªËßâÔºåÊàë‰ª¨È¶ñÂÖàËÆ≠ÁªÉ‰∏Ä‰∏™ LLM Êù•Âà§Êñ≠Âπ∂ËøáÊª§ÊéâÂ∏¶ÊúâÂπªËßâÁöÑ‰ªªÂä°Ê†∑Êú¨ÔºåÁÑ∂ÂêéÈõáÁî®‰∫∫Â∑•Ê≥®ÈáäÂëòÊù•Ê∏ÖÁêÜÈ™åËØÅÂíåÊµãËØïÈõÜ‰∏≠ÁöÑÂπªËßâ„ÄÇÁªèËøáËøô‰∏™ËøáÁ®ãÔºåCT-Eval ÂåÖÂê´ 88.6K ‰∏™‰ªªÂä°Ê†∑Êú¨„ÄÇ‰ΩøÁî® CT-EvalÔºåÊàë‰ª¨ËØÑ‰º∞‰∫ÜÂºÄÊ∫êÂíåÈó≠Ê∫ê LLM ÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåÈõ∂Ê†∑Êú¨ LLMÔºàÂåÖÊã¨ GPT-4Ôºâ‰∏é‰∫∫Á±ªÂà§Êñ≠Áõ∏ÊØî‰ªçÁÑ∂Â≠òÂú®ÊòæÁùÄÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÊ≠§Â§ñÔºåÂú®ÂæÆË∞É‰πãÂêéÔºåÂºÄÊ∫ê LLM ÂèØ‰ª•ÊòæÁùÄÊèêÈ´òÂÖ∂ÊñáÊú¨Âà∞Ë°®Ê†ºÁöÑËÉΩÂäõÔºåÂ§ßÂπÖ‰ºò‰∫é GPT-4„ÄÇÁÆÄËÄåË®Ä‰πãÔºåCT-Eval ‰∏ç‰ªÖÂ∏ÆÂä©Á†îÁ©∂‰∫∫ÂëòËØÑ‰º∞ÂíåÂø´ÈÄü‰∫ÜËß£Áé∞Êúâ LLM ÁöÑ‰∏≠ÊñáÊñáÊú¨Âà∞Ë°®Ê†ºËÉΩÂäõÔºåËÄå‰∏îËøò‰Ωú‰∏∫‰∏ÄÁßçÂÆùË¥µÁöÑËµÑÊ∫êÊù•ÊòæÁùÄÊèêÈ´ò LLM ÁöÑÊñáÊú¨Âà∞Ë°®Ê†ºÊÄßËÉΩ„ÄÇ

##### **Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**
2405.12163v1 by Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, Min Zhang

The rapid advancement of large language models has given rise to a plethora
of applications across a myriad of real-world tasks, mainly centered on
aligning with human intent. However, the complexities inherent in human intent
necessitate a dependence on labor-intensive and time-consuming human
evaluation. To alleviate this constraint, we delve into the paradigm of
employing open-source large language models as evaluators, aligning with the
prevailing trend of utilizing GPT-4. Particularly, we present a step-by-step
evaluation framework: \textbf{Fennec}, capable of \textbf{F}ine-grained
\textbf{E}valuatio\textbf{N} and correctio\textbf{N} \textbf{E}xtended through
bran\textbf{C}hing and bridging. Specifically, the branching operation dissects
the evaluation task into various dimensions and granularities, thereby
alleviating the challenges associated with evaluation. Concurrently, the
bridging operation amalgamates diverse training datasets, augmenting the
variety of evaluation tasks. In experimental trials, our 7B model consistently
outperforms open-source larger-scale evaluation models across various widely
adopted benchmarks in terms of both \textit{Agreement} and
\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ
the fine-grained correction capabilities induced by the evaluation model to
refine multiple model responses, and the results show that the refinement
elevates the quality of responses, leading to an improvement of 1-2 points on
the MT-Bench. Our code is available at
Github\footnote{\url{https://github.com/dropreg/Fennec}}.

ÊëòË¶ÅÔºö<paragraph>Â§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑÂø´ÈÄüÈÄ≤Â±ïÂ∑≤Áî¢ÁîüÂ§ßÈáèÊáâÁî®ÔºåÊ©´Ë∑®ÁÑ°Êï∏ÁúüÂØ¶‰∏ñÁïåÁöÑ‰ªªÂãôÔºå‰∏ªË¶ÅÈõÜ‰∏≠Âú®Ëàá‰∫∫È°ûÊÑèÂúñ‰øùÊåÅ‰∏ÄËá¥„ÄÇÁÑ∂ËÄåÔºå‰∫∫È°ûÊÑèÂúñ‰∏≠Âõ∫ÊúâÁöÑË§áÈõúÊÄßÈúÄË¶Å‰æùË≥¥ÊñºÂãûÂãïÂØÜÈõÜ‰∏îËÄóÊôÇÁöÑË©ï‰º∞„ÄÇÁÇ∫‰∫ÜÁ∑©Ëß£ÈÄôÁ®ÆÈôêÂà∂ÔºåÊàëÂÄëÊ∑±ÂÖ•Êé¢Ë®éÊé°Áî®ÈñãÊîæÂéüÂßãÁ¢ºÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰ΩúÁÇ∫Ë©ï‰º∞ËÄÖÁöÑÁØÑ‰æãÔºåËàáÂà©Áî® GPT-4 ÁöÑÁõõË°åË∂®Âã¢‰øùÊåÅ‰∏ÄËá¥„ÄÇÁâπÂà•ÊòØÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈÄêÊ≠•Ë©ï‰º∞Ê°ÜÊû∂Ôºö**Fennec**ÔºåËÉΩÂ§†ÈÄ≤Ë°å**F**ine-grained**E**valuatio**N** and correctio**N** **E**xtended through bran**C**hing and bridging„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÂàÜÊîØÊìç‰ΩúÂ∞áË©ï‰º∞‰ªªÂãôÂâñÊûêÊàêÂêÑÁ®ÆÁ∂≠Â∫¶ÂíåÁ≤íÂ∫¶ÔºåÂæûËÄåÁ∑©Ëß£ËàáË©ï‰º∞Áõ∏ÈóúÁöÑÊåëÊà∞„ÄÇÂêåÊôÇÔºåÊ©ãÊé•Êìç‰ΩúÂêà‰Ωµ‰∫Ü‰∏çÂêåÁöÑË®ìÁ∑¥Ë≥áÊñôÈõÜÔºåÂ¢ûÂä†‰∫ÜË©ï‰º∞‰ªªÂãôÁöÑÂ§öÊ®£ÊÄß„ÄÇÂú®ÂØ¶È©óË©¶È©ó‰∏≠ÔºåÊàëÂÄëÁöÑ 7B Ê®°ÂûãÂú®ÂêÑÁ®ÆÂª£Ê≥õÊé°Áî®ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶‰∏≠ÔºåÁÑ°Ë´ñÂú®**‰∏ÄËá¥ÊÄß**Âíå**‰∏ÄËá¥ÊÄß**ÊñπÈù¢ÔºåÈÉΩÊåÅÁ∫åÂÑ™ÊñºÈñãÊîæÂéüÂßãÁ¢ºÁöÑÂ§ßË¶èÊ®°Ë©ï‰º∞Ê®°ÂûãÔºåÊé•Ëøë GPT-4 ÁöÑËÉΩÂäõ„ÄÇÊàëÂÄëÊé°Áî®Ë©ï‰º∞Ê®°ÂûãË™òÂ∞éÁöÑÁ¥∞Á≤íÂ∫¶Ê†°Ê≠£ËÉΩÂäõ‰æÜÊîπÂñÑÂ§öÂÄãÊ®°ÂûãÂõûÊáâÔºåÁµêÊûúË°®ÊòéÔºåÈÄôÁ®ÆÊîπÂñÑÊèêÂçá‰∫ÜÂõûÊáâÁöÑÂìÅË≥™ÔºåÂ∞éËá¥ MT-Bench ‰∏äÊèêÈ´ò‰∫Ü 1-2 ÂàÜ„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂèØ‰ª•Âú® Github\footnote{\url{https://github.com/dropreg/Fennec}} ‰∏≠ÂèñÂæó„ÄÇ</paragraph>

##### **Bangladeshi Native Vehicle Detection in Wild**
2405.12150v1 by Bipin Saha, Md. Johirul Islam, Shaikh Khaled Mostaque, Aditya Bhowmik, Tapodhir Karmakar Taton, Md. Nakib Hayat Chowdhury, Mamun Bin Ibne Reaz

The success of autonomous navigation relies on robust and precise vehicle
recognition, hindered by the scarcity of region-specific vehicle detection
datasets, impeding the development of context-aware systems. To advance
terrestrial object detection research, this paper proposes a native vehicle
detection dataset for the most commonly appeared vehicle classes in Bangladesh.
17 distinct vehicle classes have been taken into account, with fully annotated
81542 instances of 17326 images. Each image width is set to at least 1280px.
The dataset's average vehicle bounding box-to-image ratio is 4.7036. This
Bangladesh Native Vehicle Dataset (BNVD) has accounted for several
geographical, illumination, variety of vehicle sizes, and orientations to be
more robust on surprised scenarios. In the context of examining the BNVD
dataset, this work provides a thorough assessment with four successive You Only
Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset's
effectiveness is methodically evaluated and contrasted with other vehicle
datasets already in use. The BNVD dataset exhibits mean average precision(mAP)
at 50% intersection over union (IoU) is 0.848 corresponding precision and
recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643
at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset
serves as a reliable representation of vehicle distribution and presents
considerable complexities.

ÊëòË¶ÅÔºö<paragraph>Ëá™‰∏ªÂ∞éËà™ÁöÑÊàêÂäüÊúâË≥¥ÊñºÂº∑ÂÅ•‰∏îÁ≤æÁ¢∫ÁöÑËªäËºõËæ®Ë≠òÔºåËÄåÈÄôÂçªÂèóÂà∞ÁâπÂÆöÂçÄÂüüËªäËºõÂÅµÊ∏¨Ë≥áÊñôÈõÜÁöÑÂå±‰πèÊâÄÈòªÁ§ôÔºåÈÄ≤ËÄåÂ¶®Á§ô‰∫ÜÊÉÖÂ¢ÉÊÑüÁü•Á≥ªÁµ±ÁöÑÁôºÂ±ï„ÄÇÁÇ∫‰∫ÜÊé®ÂãïÂú∞Èù¢Áâ©È´îÂÅµÊ∏¨ÁöÑÁ†îÁ©∂ÔºåÊú¨ÁØáË´ñÊñáÊèêÂá∫‰∫Ü‰∏ÄÂÄãÈáùÂ∞çÂ≠üÂä†ÊãâÂúãÊúÄÂ∏∏Ë¶ãËªäËºõÈ°ûÂà•ÁöÑÂéüÁîüËªäËºõÂÅµÊ∏¨Ë≥áÊñôÈõÜ„ÄÇÊàëÂÄëËÄÉÊÖÆ‰∫Ü 17 ÂÄã‰∏çÂêåÁöÑËªäËºõÈ°ûÂà•Ôºå‰∏¶ÈáùÂ∞ç 17326 ÂºµÂΩ±ÂÉèÈÄ≤Ë°å‰∫ÜÂÆåÊï¥ÁöÑÊ®ôË®ªÔºåÂÖ±Ê®ôË®ª‰∫Ü 81542 ÂÄãÂØ¶‰æã„ÄÇÊØèÂºµÂΩ±ÂÉèÁöÑÂØ¨Â∫¶ÈÉΩË®≠ÂÆöÁÇ∫Ëá≥Â∞ë 1280 ÂÉèÁ¥†„ÄÇË©≤Ë≥áÊñôÈõÜÁöÑÂπ≥ÂùáËªäËºõÈÇäÁïåÊ°ÜËàáÂΩ±ÂÉèÊØî‰æãÁÇ∫ 4.7036„ÄÇÈÄôÂÄãÂ≠üÂä†ÊãâÂúãÂéüÁîüËªäËºõË≥áÊñôÈõÜ (BNVD) ËÄÉÊÖÆ‰∫ÜÂ§öÁ®ÆÂú∞ÁêÜ„ÄÅÂÖâÁÖß„ÄÅËªäËºõÂ§ßÂ∞èÂíåÊñπÂêëÔºå‰ª•‰æøÂú®ÊÑèÂ§ñÁöÑÊÉÖÊ≥Å‰∏ãÊõ¥ÁÇ∫Âº∑ÂÅ•„ÄÇÂú®Ê™¢Êü• BNVD Ë≥áÊñôÈõÜÁöÑËÑàÁµ°‰∏ãÔºåÊú¨Á†îÁ©∂Êèê‰æõ‰∫ÜÂ∞çÂõõÂÄãÈÄ£Á∫åÁöÑ You Only Look Once (YOLO) Ê®°ÂûãÁöÑÂæπÂ∫ïË©ï‰º∞ÔºåÂàÜÂà•ÊòØ YOLO v5„ÄÅv6„ÄÅv7 Âíå v8„ÄÇÊàëÂÄëÊúâÁ≥ªÁµ±Âú∞Ë©ï‰º∞‰∫ÜÈÄô‰∫õË≥áÊñôÈõÜÁöÑÊúâÊïàÊÄßÔºå‰∏¶ËàáÂÖ∂‰ªñÁèæÊúâËªäËºõË≥áÊñôÈõÜÈÄ≤Ë°åÂ∞çÊØî„ÄÇBNVD Ë≥áÊñôÈõÜÂú® 50% ‰∫§ÈõÜ‰∏¶ÈõÜ (IoU) ‰∏äÂ±ïÁèæÂá∫ 0.848 ÁöÑÂπ≥ÂùáÊ∫ñÁ¢∫Â∫¶ (mAP)ÔºåÂ∞çÊáâÁöÑÊ∫ñÁ¢∫ÁéáÂíåÂè¨ÂõûÁéáÂàÜÂà•ÁÇ∫ 0.841 Âíå 0.774„ÄÇÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÂú® 0.5 Âà∞ 0.95 ÁöÑ IoU ÁØÑÂúçÂÖßÔºåmAP ÁÇ∫ 0.643„ÄÇÂØ¶È©óÈ°ØÁ§∫ÔºåBNVD Ë≥áÊñôÈõÜÂèØ‰ΩúÁÇ∫ËªäËºõÂàÜ‰ΩàÁöÑÂèØÈù†Ë°®Á§∫Ôºå‰∏¶ÂëàÁèæÂá∫Áõ∏Áï∂ÁöÑË§áÈõúÊÄß„ÄÇ</paragraph>

##### **Eliciting Problem Specifications via Large Language Models**
2405.12147v1 by Robert E. Wray, James R. Kirk, John E. Laird

Cognitive systems generally require a human to translate a problem definition
into some specification that the cognitive system can use to attempt to solve
the problem or perform the task. In this paper, we illustrate that large
language models (LLMs) can be utilized to map a problem class, defined in
natural language, into a semi-formal specification that can then be utilized by
an existing reasoning and learning system to solve instances from the problem
class. We present the design of LLM-enabled cognitive task analyst agent(s).
Implemented with LLM agents, this system produces a definition of problem
spaces for tasks specified in natural language. LLM prompts are derived from
the definition of problem spaces in the AI literature and general
problem-solving strategies (Polya's How to Solve It). A cognitive system can
then use the problem-space specification, applying domain-general problem
solving strategies ("weak methods" such as search), to solve multiple instances
of problems from the problem class. This result, while preliminary, suggests
the potential for speeding cognitive systems research via disintermediation of
problem formulation while also retaining core capabilities of cognitive
systems, such as robust inference and online learning.

ÊëòË¶ÅÔºöË™çÁü•Á≥ªÁµ±ÈÄöÂ∏∏ÈúÄË¶Å‰∫∫È°ûÂ∞áÂïèÈ°åÂÆöÁæ©ËΩâÊèõÁÇ∫Ë™çÁü•Á≥ªÁµ±ÂèØÂà©Áî®‰æÜÂòóË©¶Ëß£Ê±∫ÂïèÈ°åÊàñÂü∑Ë°å‰ªªÂãôÁöÑÊüê‰∫õË¶èÊ†º„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëË™™ÊòéÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÂèØÁî®ÊñºÂ∞á‰ª•Ëá™ÁÑ∂Ë™ûË®ÄÂÆöÁæ©ÁöÑÂïèÈ°åÈ°ûÂà•Â∞çÊáâÂà∞ÂçäÊ≠£ÂºèË¶èÊ†ºÔºåÁÑ∂ÂæåÁèæÊúâÁöÑÊé®ÁêÜÂíåÂ≠∏ÁøíÁ≥ªÁµ±ÂèØÂà©Áî®Ë©≤Ë¶èÊ†º‰æÜËß£Ê±∫ÂïèÈ°åÈ°ûÂà•‰∏≠ÁöÑÂØ¶‰æã„ÄÇÊàëÂÄëÊèêÂá∫ LLM ÂïüÁî®ÁöÑË™çÁü•‰ªªÂãôÂàÜÊûê‰ª£ÁêÜÁöÑË®≠Ë®à„ÄÇÊ≠§Á≥ªÁµ±Âà©Áî® LLM ‰ª£ÁêÜÂØ¶‰ΩúÔºåÂèØÁî¢Áîü‰ª•Ëá™ÁÑ∂Ë™ûË®ÄÊåáÂÆöÁöÑ‰ªªÂãôÂïèÈ°åÁ©∫ÈñìÂÆöÁæ©„ÄÇLLM ÊèêÁ§∫‰æÜËá™ AI ÊñáÁçª‰∏≠ÂïèÈ°åÁ©∫ÈñìÁöÑÂÆöÁæ©Âíå‰∏ÄËà¨ÂïèÈ°åËß£Ê±∫Á≠ñÁï•ÔºàÊ≥¢Âà©‰∫ûÁöÑÂ¶Ç‰ΩïËß£È°åÔºâ„ÄÇË™çÁü•Á≥ªÁµ±Êé•ËëóÂèØ‰ª•‰ΩøÁî®ÂïèÈ°åÁ©∫ÈñìË¶èÊ†ºÔºåÂ•óÁî®È†òÂüüÈÄöÁî®ÁöÑÂïèÈ°åËß£Ê±∫Á≠ñÁï•Ôºà„ÄåÂº±ÊñπÊ≥ï„ÄçÔºå‰æãÂ¶ÇÊêúÂ∞ãÔºâÔºå‰æÜËß£Ê±∫ÂïèÈ°åÈ°ûÂà•‰∏≠Â§öÂÄãÂïèÈ°åÂØ¶‰æã„ÄÇÊ≠§ÁµêÊûúÈõñÁÑ∂ÂàùÊ≠•Ôºå‰ΩÜÈ°ØÁ§∫Âá∫ÈÄèÈÅéÂïèÈ°åË°®Ëø∞ÁöÑÂéª‰∏≠‰ªãÂåñ‰æÜÂä†ÈÄüË™çÁü•Á≥ªÁµ±Á†îÁ©∂ÁöÑÊΩõÂäõÔºåÂêåÊôÇ‰πü‰øùÁïôË™çÁü•Á≥ªÁµ±ÁöÑÊ†∏ÂøÉÂäüËÉΩÔºå‰æãÂ¶ÇÂº∑ÂÅ•ÁöÑÊé®Ë´ñÂíåÁ∑ö‰∏äÂ≠∏Áøí„ÄÇ

##### **MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**
2405.12130v1 by Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang

Low-rank adaptation is a popular parameter-efficient fine-tuning method for
large language models. In this paper, we analyze the impact of low-rank
updating, as implemented in LoRA. Our findings suggest that the low-rank
updating mechanism may limit the ability of LLMs to effectively learn and
memorize new knowledge. Inspired by this observation, we propose a new method
called MoRA, which employs a square matrix to achieve high-rank updating while
maintaining the same number of trainable parameters. To achieve it, we
introduce the corresponding non-parameter operators to reduce the input
dimension and increase the output dimension for the square matrix. Furthermore,
these operators ensure that the weight can be merged back into LLMs, which
makes our method can be deployed like LoRA. We perform a comprehensive
evaluation of our method across five tasks: instruction tuning, mathematical
reasoning, continual pretraining, memory and pretraining. Our method
outperforms LoRA on memory-intensive tasks and achieves comparable performance
on other tasks.

ÊëòË¶ÅÔºö‰ΩéÁß©ÈÅ©ÊáâÊòØ‰∏ÄÁ®ÆÂª£ÂèóÊ≠°Ëøé‰∏îÂèÉÊï∏ÊïàÁéáÈ´òÁöÑÂæÆË™øÊñπÊ≥ïÔºåÈÅ©Áî®ÊñºÂ§ßÂûãË™ûË®ÄÊ®°Âûã„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂàÜÊûê‰∫Ü‰ΩéÁß©Êõ¥Êñ∞ÁöÑÂΩ±ÈüøÔºåÂ∞±ÂÉèÂú® LoRA ‰∏≠ÂØ¶‰ΩúÁöÑÈÇ£Ê®£„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúË°®ÊòéÔºå‰ΩéÁß©Êõ¥Êñ∞Ê©üÂà∂ÂèØËÉΩÊúÉÈôêÂà∂Â§ßÂûãË™ûË®ÄÊ®°ÂûãÊúâÊïàÂ≠∏ÁøíÂíåË®òÊÜ∂Êñ∞Áü•Ë≠òÁöÑËÉΩÂäõ„ÄÇÂèóÂà∞ÈÄô‰∏ÄËßÄÂØüÁµêÊûúÁöÑÂïüÁôºÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂêçÁÇ∫ MoRA ÁöÑÊñ∞ÊñπÊ≥ïÔºåÂÆÉÊé°Áî®‰∏ÄÂÄãÊñπÈô£‰æÜÂØ¶ÁèæÈ´òÁß©Êõ¥Êñ∞ÔºåÂêåÊôÇ‰øùÊåÅÁõ∏ÂêåÊï∏ÈáèÁöÑÂèØË®ìÁ∑¥ÂèÉÊï∏„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫ÜÁõ∏ÊáâÁöÑÈùûÂèÉÊï∏ÈÅãÁÆóÂ≠êÔºå‰ª•Èôç‰ΩéËº∏ÂÖ•Á∂≠Â∫¶‰∏¶Â¢ûÂä†ÊñπÈô£ÁöÑËº∏Âá∫Á∂≠Â∫¶„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÈÅãÁÆóÂ≠êÁ¢∫‰øùÊ¨äÈáçÂèØ‰ª•Âêà‰ΩµÂõûÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÔºåÈÄô‰ΩøÂæóÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•ÂÉè LoRA ‰∏ÄÊ®£ÈÉ®ÁΩ≤„ÄÇÊàëÂÄëÂ∞çÊàëÂÄëÁöÑÊ®°ÂûãÈÄ≤Ë°å‰∫ÜÂÖ®Èù¢ÁöÑË©ï‰º∞ÔºåÊ∂µËìã‰∫Ü‰∫îÈ†Ö‰ªªÂãôÔºöÊåá‰ª§ÂæÆË™ø„ÄÅÊï∏Â≠∏Êé®ÁêÜ„ÄÅÊåÅÁ∫åÈ†êË®ìÁ∑¥„ÄÅË®òÊÜ∂ÂíåÈ†êË®ìÁ∑¥„ÄÇÊàëÂÄëÁöÑÊ®°ÂûãÂú®Ë®òÊÜ∂ÂØÜÈõÜÂûã‰ªªÂãô‰∏äÂÑ™Êñº LoRAÔºå‰∏¶Âú®ÂÖ∂‰ªñ‰ªªÂãô‰∏äÂØ¶Áèæ‰∫ÜÂèØÊØîÁöÑÊïàËÉΩ„ÄÇ

##### **Prompt Learning for Generalized Vehicle Routing**
2405.12262v1 by Fei Liu, Xi Lin, Weiduo Liao, Zhenkun Wang, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan

Neural combinatorial optimization (NCO) is a promising learning-based
approach to solving various vehicle routing problems without much manual
algorithm design. However, the current NCO methods mainly focus on the
in-distribution performance, while the real-world problem instances usually
come from different distributions. A costly fine-tuning approach or generalized
model retraining from scratch could be needed to tackle the out-of-distribution
instances. Unlike the existing methods, this work investigates an efficient
prompt learning approach in NCO for cross-distribution adaptation. To be
concrete, we propose a novel prompt learning method to facilitate fast
zero-shot adaptation of a pre-trained model to solve routing problem instances
from different distributions. The proposed model learns a set of prompts among
various distributions and then selects the best-matched one to prompt a
pre-trained attention model for each problem instance. Extensive experiments
show that the proposed prompt learning approach facilitates the fast adaptation
of pre-trained routing models. It also outperforms existing generalized models
on both in-distribution prediction and zero-shot generalization to a diverse
set of new tasks. Our code implementation is available online
https://github.com/FeiLiu36/PromptVRP.

ÊëòË¶ÅÔºöÁ•ûÁ∂ìÁµÑÂêàÊúÄ‰Ω≥Âåñ (NCO) ÊòØ‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÂü∫ÊñºÂ≠∏ÁøíÁöÑÊñπÊ≥ïÔºåÁî®ÊñºËß£Ê±∫ÂêÑÁ®ÆËªäËºõË∑ØÁ∑öÂïèÈ°åÔºåËÄåÁÑ°ÈúÄÂ§™Â§öÊâãÂãïÊºîÁÆóÊ≥ïË®≠Ë®à„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑ NCO ÊñπÊ≥ï‰∏ªË¶ÅÈóúÊ≥®ÊñºÂàÜÈÖçÂÖßÊïàËÉΩÔºåËÄåÁúüÂØ¶‰∏ñÁïåÁöÑÂïèÈ°åÂØ¶‰æãÈÄöÂ∏∏‰æÜËá™‰∏çÂêåÁöÑÂàÜÈÖç„ÄÇÂèØËÉΩÈúÄË¶ÅÊòÇË≤¥ÁöÑÂæÆË™øÊñπÊ≥ïÊàñÂæûÈ†≠ÈñãÂßãÈÄ≤Ë°åÂª£Áæ©Ê®°ÂûãÂÜçË®ìÁ∑¥ÔºåÊâçËÉΩËôïÁêÜÂàÜÈÖçÂ§ñÂØ¶‰æã„ÄÇËàáÁèæÊúâÊñπÊ≥ï‰∏çÂêåÔºåÈÄôÈ†ÖÂ∑•‰ΩúÁ†îÁ©∂‰∫Ü NCO ‰∏≠‰∏ÄÁ®ÆÊúâÊïàÁöÑÊèêÁ§∫Â≠∏ÁøíÊñπÊ≥ïÔºåÁî®ÊñºË∑®ÂàÜÈÖçÈÅ©Êáâ„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞Á©éÁöÑÊèêÁ§∫Â≠∏ÁøíÊñπÊ≥ïÔºå‰ª•‰øÉÈÄ≤È†êË®ìÁ∑¥Ê®°ÂûãÂø´ÈÄüÈõ∂Ê¨°ÈÅ©ÊáâÔºå‰ª•Ëß£Ê±∫‰æÜËá™‰∏çÂêåÂàÜÈÖçÁöÑË∑ØÁî±ÂïèÈ°åÂØ¶‰æã„ÄÇÊâÄÊèêÂá∫ÁöÑÊ®°ÂûãÂ≠∏Áøí‰∫ÜÂêÑÁ®ÆÂàÜÈÖç‰πãÈñìÁöÑ‰∏ÄÁµÑÊèêÁ§∫ÔºåÁÑ∂ÂæåÈÅ∏ÊìáÊúÄ‰Ω≥ÂåπÈÖçÁöÑ‰∏ÄÂÄã‰æÜÊèêÁ§∫ÊØèÂÄãÂïèÈ°åÂØ¶‰æãÁöÑÈ†êË®ìÁ∑¥Ê≥®ÊÑèÂäõÊ®°Âûã„ÄÇÂª£Ê≥õÁöÑÂØ¶È©óË°®ÊòéÔºåÊâÄÊèêÂá∫ÁöÑÊèêÁ§∫Â≠∏ÁøíÊñπÊ≥ï‰øÉËøõ‰∫ÜÈ†êË®ìÁ∑¥Ë∑ØÁî±Ê®°ÂûãÁöÑÂø´ÈÄüÈÅ©Êáâ„ÄÇÂÆÉÂú®ÂàÜÈÖçÂÖßÈ†êÊ∏¨ÂíåÈõ∂Ê¨°Ê≥õÂåñÂà∞ÂêÑÁ®ÆÊñ∞‰ªªÂãô‰∏ä‰πüÂÑ™ÊñºÁèæÊúâÁöÑÂª£Áæ©Ê®°Âûã„ÄÇÊàëÂÄëÁöÑÁ®ãÂºèÁ¢ºÂØ¶‰ΩúÂèØ‰ª•Âú®Á∑ö‰∏äÂèñÂæó https://github.com/FeiLiu36/PromptVRP„ÄÇ

##### **Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**
2405.12119v1 by Zhankui He, Zhouhang Xie, Harald Steck, Dawen Liang, Rahul Jha, Nathan Kallus, Julian McAuley

Large language models (LLMs) are revolutionizing conversational recommender
systems by adeptly indexing item content, understanding complex conversational
contexts, and generating relevant item titles. However, controlling the
distribution of recommended items remains a challenge. This leads to suboptimal
performance due to the failure to capture rapidly changing data distributions,
such as item popularity, on targeted conversational recommendation platforms.
In conversational recommendation, LLMs recommend items by generating the titles
(as multiple tokens) autoregressively, making it difficult to obtain and
control the recommendations over all items. Thus, we propose a
Reindex-Then-Adapt (RTA) framework, which converts multi-token item titles into
single tokens within LLMs, and then adjusts the probability distributions over
these single-token item titles accordingly. The RTA framework marries the
benefits of both LLMs and traditional recommender systems (RecSys):
understanding complex queries as LLMs do; while efficiently controlling the
recommended item distributions in conversational recommendations as traditional
RecSys do. Our framework demonstrates improved accuracy metrics across three
different conversational recommendation datasets and two adaptation settings

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÈÄèÈÅéÈùàÊ¥ªÂú∞Á¥¢ÂºïÈ†ÖÁõÆÂÖßÂÆπ„ÄÅÁêÜËß£Ë§áÈõúÁöÑÂ∞çË©±ËÑàÁµ°Ôºå‰ª•ÂèäÁî¢ÁîüÁõ∏ÈóúÁöÑÈ†ÖÁõÆÊ®ôÈ°åÔºåÂæπÂ∫ïÈù©Êñ∞Â∞çË©±Êé®Ëñ¶Á≥ªÁµ±„ÄÇÁÑ∂ËÄåÔºåÊéßÂà∂Êé®Ëñ¶È†ÖÁõÆÁöÑÂàÜ‰Ωà‰ªçÁÑ∂ÊòØ‰∏ÄÈ†ÖÊåëÊà∞„ÄÇÈÄôÊúÉÂ∞éËá¥Ê¨°‰Ω≥ÊïàËÉΩÔºåÂõ†ÁÇ∫ÁÑ°Ê≥ïÊì∑ÂèñÂø´ÈÄüËÆäÂåñÁöÑË≥áÊñôÂàÜ‰ΩàÔºå‰æãÂ¶ÇÁõÆÊ®ôÂ∞çË©±Êé®Ëñ¶Âπ≥Âè∞‰∏äÁöÑÈ†ÖÁõÆÁÜ±ÈñÄÁ®ãÂ∫¶„ÄÇÂú®Â∞çË©±Êé®Ëñ¶‰∏≠ÔºåLLM ÈÄèÈÅéËá™Ëø¥Ê≠∏Áî¢ÁîüÊ®ôÈ°åÔºà‰ΩúÁÇ∫Â§öÂÄãÁ¨¶ËôüÔºâÔºåÈÄô‰ΩøÂæóÈõ£‰ª•ÂèñÂæóÂíåÊéßÂà∂ÊâÄÊúâÈ†ÖÁõÆÁöÑÊé®Ëñ¶„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄëÊèêÂá∫‰∏ÄÂÄãÈáçÊñ∞Á¥¢ÂºïÂÜçË™øÊï¥ (RTA) Êû∂ÊßãÔºåÂ∞áÂ§öÁ¨¶ËôüÈ†ÖÁõÆÊ®ôÈ°åËΩâÊèõÁÇ∫ LLM ‰∏≠ÁöÑÂñÆ‰∏ÄÁ¨¶ËôüÔºåÁÑ∂ÂæåÁõ∏ÊáâÂú∞Ë™øÊï¥ÈÄô‰∫õÂñÆ‰∏ÄÁ¨¶ËôüÈ†ÖÁõÆÊ®ôÈ°åÁöÑÊ©üÁéáÂàÜ‰Ωà„ÄÇRTA Êû∂ÊßãÁµêÂêà‰∫Ü LLM ÂíåÂÇ≥Áµ±Êé®Ëñ¶Á≥ªÁµ± (RecSys) ÁöÑÂÑ™ÈªûÔºöÂÉè LLM ‰∏ÄÊ®£ÁêÜËß£Ë§áÈõúÁöÑÊü•Ë©¢ÔºõÂêåÊôÇÂÉèÂÇ≥Áµ± RecSys ‰∏ÄÊ®£ÊúâÊïàÊéßÂà∂Â∞çË©±Êé®Ëñ¶‰∏≠ÁöÑÊé®Ëñ¶È†ÖÁõÆÂàÜ‰Ωà„ÄÇÊàëÂÄëÁöÑÊû∂ÊßãÂú®‰∏âÂÄã‰∏çÂêåÁöÑÂ∞çË©±Êé®Ëñ¶Ë≥áÊñôÈõÜÂíåÂÖ©ÂÄãË™øÊï¥Ë®≠ÂÆö‰∏≠Â±ïÁ§∫‰∫ÜÊîπÂñÑÁöÑÊ∫ñÁ¢∫Â∫¶ÊåáÊ®ô

##### **Linguistic Structure from a Bottleneck on Sequential Information Processing**
2405.12109v1 by Richard Futrell, Michael Hahn

Human language is a unique form of communication in the natural world,
distinguished by its structured nature. Most fundamentally, it is systematic,
meaning that signals can be broken down into component parts that are
individually meaningful -- roughly, words -- which are combined in a regular
way to form sentences. Furthermore, the way in which these parts are combined
maintains a kind of locality: words are usually concatenated together, and they
form contiguous phrases, keeping related parts of sentences close to each
other. We address the challenge of understanding how these basic properties of
language arise from broader principles of efficient communication under
information processing constraints. Here we show that natural-language-like
systematicity arises from minimization of excess entropy, a measure of
statistical complexity that represents the minimum amount of information
necessary for predicting the future of a sequence based on its past. In
simulations, we show that codes that minimize excess entropy factorize their
source distributions into approximately independent components, and then
express those components systematically and locally. Next, in a series of
massively cross-linguistic corpus studies, we show that human languages are
structured to have low excess entropy at the level of phonology, morphology,
syntax, and semantics. Our result suggests that human language performs a
sequential generalization of Independent Components Analysis on the statistical
distribution over meanings that need to be expressed. It establishes a link
between the statistical and algebraic structure of human language, and
reinforces the idea that the structure of human language may have evolved to
minimize cognitive load while maximizing communicative expressiveness.

ÊëòË¶ÅÔºö‰∫∫È°ûË™ûË®ÄÊòØËá™ÁÑ∂Áïå‰∏≠‰∏ÄÁ®ÆÁç®ÁâπÁöÑÊ∫ùÈÄöÂΩ¢ÂºèÔºå
‰ª•ÂÖ∂ÁµêÊßãÂåñÁöÑÊÄßË≥™ÁÇ∫ÁâπËâ≤„ÄÇÊúÄÊ†πÊú¨ÁöÑÊòØÔºåÂÆÉÊòØÁ≥ªÁµ±ÊÄßÁöÑÔºå
ÈÄôË°®Á§∫Á¨¶ËôüÂèØ‰ª•ÂàÜËß£ÊàêÂÄãÂà•ÊúâÊÑèÁæ©ÁöÑÁµÑÊàêÈÉ®ÂàÜ‚Äî‚ÄîÂ§ßËá¥‰∏äÔºå
Â∞±ÊòØÂñÆÂ≠ó‚Äî‚ÄîÈÄô‰∫õÂñÆÂ≠ó‰ª•Ë¶èÂâáÁöÑÊñπÂºèÁµÑÂêàÊàêÂè•Â≠ê„ÄÇÊ≠§Â§ñÔºåÈÄô‰∫õÈÉ®ÂàÜÁµÑÂêàÁöÑÊñπÂºè
Á∂≠ÊåÅËëó‰∏ÄÁ®ÆÂ±ÄÈÉ®ÊÄßÔºöÂñÆÂ≠óÈÄöÂ∏∏ÊúÉ‰∏≤ËÅØÂú®‰∏ÄËµ∑Ôºå‰∏¶ÂΩ¢ÊàêÈÄ£Á∫åÁöÑÁâáË™ûÔºå
ËÆìÂè•Â≠êÁöÑÁõ∏ÈóúÈÉ®ÂàÜÁ∑äÂØÜÁõ∏ÈÄ£„ÄÇÊàëÂÄëÊé¢Ë®é‰∫ÜÂ¶Ç‰ΩïÁêÜËß£ÈÄô‰∫õË™ûË®ÄÂü∫Êú¨Â±¨ÊÄß
Âú®Ë≥áË®äËôïÁêÜÈôêÂà∂‰∏ãÔºåÂ¶Ç‰ΩïÂæûÊúâÊïàÊ∫ùÈÄöÁöÑÊõ¥Âª£Ê≥õÂéüÂâá‰∏≠Áî¢Áîü„ÄÇÂú®ÈÄôË£°ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü
È°ûËá™ÁÑ∂Ë™ûË®ÄÁöÑÁ≥ªÁµ±ÊÄß‰æÜËá™ÊñºÈÅéÂâ©ÁÜµÁöÑÊúÄÂ∞èÂåñÔºå‰∏ÄÁ®ÆÁµ±Ë®àË§áÈõúÊÄßÁöÑÊ∏¨ÈáèÔºå
ÂÆÉË°®Á§∫Ê†πÊìöÂ∫èÂàóÁöÑÈÅéÂéªÈ†êÊ∏¨ÂÖ∂Êú™‰æÜÁöÑÂøÖË¶ÅË≥áË®äÊúÄÂ∞èÈáè„ÄÇÂú®
Ê®°Êì¨‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊúÄÂ∞èÂåñÈÅéÂâ©ÁÜµÁöÑÁ¢ºÂ∞áÂÖ∂‰æÜÊ∫êÂàÜ‰ΩàÂàÜËß£Êàê
Ëøë‰ººÁç®Á´ãÁöÑÁµÑÊàêÈÉ®ÂàÜÔºåÁÑ∂Âæå‰ª•Á≥ªÁµ±‰∏îÂ±ÄÈÉ®ÁöÑÂΩ¢ÂºèË°®ÈÅîÈÄô‰∫õÁµÑÊàêÈÉ®ÂàÜ„ÄÇÊé•‰∏ã‰æÜÔºå
Âú®‰∏ÄÁ≥ªÂàóÁöÑÂ§ßË¶èÊ®°Ë∑®Ë™ûË®ÄË™ûÊñôÂ∫´Á†îÁ©∂‰∏≠ÔºåÊàëÂÄëÂ±ïÁ§∫‰∫Ü‰∫∫È°ûË™ûË®ÄÁöÑÁµêÊßã
Âú®Èü≥Èüª„ÄÅÂΩ¢ÊÖã„ÄÅÂè•Ê≥ïÂíåË™ûÁæ©ÁöÑÂ±§Á¥ö‰∏äÂÖ∑Êúâ‰ΩéÈÅéÂâ©ÁÜµ„ÄÇÊàëÂÄëÁöÑÁµêÊûúË°®ÊòéÔºå
‰∫∫È°ûË™ûË®ÄÂ∞çÈúÄË¶ÅË°®ÈÅîÁöÑÊÑèÁæ©ÁöÑÁµ±Ë®àÂàÜ‰ΩàÂü∑Ë°åÁç®Á´ãÊàêÂàÜÂàÜÊûêÁöÑÈ†ÜÂ∫èÊ¶ÇÂåñ„ÄÇÂÆÉÂª∫Á´ã‰∫Ü
‰∫∫È°ûË™ûË®ÄÁöÑÁµ±Ë®àÂíå‰ª£Êï∏ÁµêÊßã‰πãÈñìÁöÑÈÄ£ÁµêÔºå‰∏¶Âº∑Âåñ‰∫Ü‰∫∫È°ûË™ûË®ÄÁöÑÁµêÊßãÂèØËÉΩÊºîÂåñÁÇ∫
ÊúÄÂ∞èÂåñË™çÁü•Ë≤†ÊìîÔºåÂêåÊôÇÊúÄÂ§ßÂåñÊ∫ùÈÄöË°®ÈÅîÂäõÁöÑÊÉ≥Ê≥ï„ÄÇ

##### **Imp: Highly Capable Large Multimodal Models for Mobile Devices**
2405.12107v1 by Zhenwei Shao, Zhou Yu, Jun Yu, Xuecheng Ouyang, Lihao Zheng, Zhenbiao Gai, Mingyang Wang, Jiajun Ding

By harnessing the capabilities of large language models (LLMs), recent large
multimodal models (LMMs) have shown remarkable versatility in open-world
multimodal understanding. Nevertheless, they are usually parameter-heavy and
computation-intensive, thus hindering their applicability in
resource-constrained scenarios. To this end, several lightweight LMMs have been
proposed successively to maximize the capabilities under constrained scale
(e.g., 3B). Despite the encouraging results achieved by these methods, most of
them only focus on one or two aspects of the design space, and the key design
choices that influence model capability have not yet been thoroughly
investigated. In this paper, we conduct a systematic study for lightweight LMMs
from the aspects of model architecture, training strategy, and training data.
Based on our findings, we obtain Imp -- a family of highly capable LMMs at the
2B-4B scales. Notably, our Imp-3B model steadily outperforms all the existing
lightweight LMMs of similar size, and even surpasses the state-of-the-art LMMs
at the 13B scale. With low-bit quantization and resolution reduction
techniques, our Imp model can be deployed on a Qualcomm Snapdragon 8Gen3 mobile
chip with a high inference speed of about 13 tokens/s.

ÊëòË¶ÅÔºö<paragraph>ÈÄèÈÅéÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑËÉΩÂäõÔºåÊúÄËøëÁöÑÂ§ßÂûãÂ§öÊ®°ÊÖãÊ®°Âûã (LMM) Âú®ÈñãÊîæ‰∏ñÁïåÂ§öÊ®°ÊÖãÁêÜËß£‰∏≠Â±ïÁèæ‰∫ÜÈ©ö‰∫∫ÁöÑÂ§öÂäüËÉΩÊÄß„ÄÇÂÑòÁÆ°Â¶ÇÊ≠§ÔºåÂÆÉÂÄëÈÄöÂ∏∏ÂèÉÊï∏ÁπÅÂ§ö‰∏îË®àÁÆóÂØÜÈõÜÔºåÂõ†Ê≠§ÈòªÁ§ô‰∫ÜÂÆÉÂÄëÂú®Ë≥áÊ∫êÂèóÈôêÂ†¥ÊôØ‰∏≠ÁöÑÊáâÁî®„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÂ∑≤Á∂ìÈô∏Á∫åÊèêÂá∫‰∫ÜÂπæÁ®ÆËºïÈáèÁ¥ö LMMÔºå‰ª•ÊúÄÂ§ßÂåñÂèóÈôêË¶èÊ®°Ôºà‰æãÂ¶Ç 3BÔºâ‰∏ãÁöÑËÉΩÂäõ„ÄÇÂÑòÁÆ°ÈÄô‰∫õÊñπÊ≥ïÂèñÂæó‰∫Ü‰ª§‰∫∫ÈºìËàûÁöÑÊàêÊûúÔºå‰ΩÜÂÆÉÂÄëÂ§ßÂ§öÂè™ÈóúÊ≥®Ë®≠Ë®àÁ©∫ÈñìÁöÑ‰∏ÄÂÄãÊàñÂÖ©ÂÄãÊñπÈù¢ÔºåËÄåÂΩ±ÈüøÊ®°ÂûãËÉΩÂäõÁöÑÈóúÈçµË®≠Ë®àÈÅ∏ÊìáÂ∞öÊú™ÂæóÂà∞ÂæπÂ∫ïÁ†îÁ©∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂæûÊ®°ÂûãÊû∂Êßã„ÄÅË®ìÁ∑¥Á≠ñÁï•ÂíåË®ìÁ∑¥Êï∏ÊìöÁ≠âÊñπÈù¢Â∞çËºïÈáèÁ¥ö LMM ÈÄ≤Ë°å‰∫ÜÁ≥ªÁµ±ÊÄßÁ†îÁ©∂„ÄÇÊ†πÊìöÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÔºåÊàëÂÄëÁç≤Âæó‰∫Ü Imp‚Äî‚Äî‰∏ÄÁ≥ªÂàóÂú® 2B-4B Ë¶èÊ®°‰∏äÂÖ∑ÊúâÈ´òÂ∫¶ËÉΩÂäõÁöÑ LMM„ÄÇÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÊàëÂÄëÁöÑ Imp-3B Ê®°ÂûãÁ©©ÂÆöÂú∞ÂÑ™ÊñºÊâÄÊúâÁèæÊúâÈ°û‰ººË¶èÊ®°ÁöÑËºïÈáèÁ¥ö LMMÔºåÁîöËá≥Ë∂ÖË∂ä‰∫Ü 13B Ë¶èÊ®°ÁöÑÊúÄÊñ∞ LMM„ÄÇÈÄèÈÅé‰Ωé‰ΩçÂÖÉÈáèÂåñÂíåËß£ÊûêÂ∫¶Èôç‰ΩéÊäÄË°ìÔºåÊàëÂÄëÁöÑ Imp Ê®°ÂûãÂèØ‰ª•ÈÉ®ÁΩ≤Âú® Qualcomm Snapdragon 8Gen3 Ë°åÂãïÊô∂Áâá‰∏äÔºåÂÖ∑ÊúâÁ¥Ñ 13 ÂÄãÁ¨¶Ëôü/ÁßíÁöÑÈ´òÊé®Ë´ñÈÄüÂ∫¶„ÄÇ</paragraph>

##### **DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**
2405.12100v1 by Hao Chen, Biaojie Zeng, Xin Lin, Liang He, Aimin Zhou

Math world problems correction(MWPC) is a novel task dedicated to rectifying
reasoning errors in the process of solving mathematical problems. In this
paper, leveraging the advancements in large language models (LLMs), we address
two key objectives:(1) Distinguishing between mathematical reasoning and error
correction; (2) Exploring strategies to enhance the error correction
capabilities of LLMs in mathematics to solve MWPC task. We noticed that, in
real-time education,assisting students in recognizing their mistakes is more
crucial than simply providing correct answers. However, current research tends
to prioritize obtaining accurate solutions to math problems rather than
correcting potentially incorrect ones. Therefore, we modify the research
paradigm, demonstrating that improving mathematical reasoning abilities does
not equate to mastery in error correction. Meanwhile, we propose a novel method
called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in
error correction. In experiments, DOP has shown outstanding performance,
highlighting its significant impact. We argue that in mathematical education,
the demand for outstanding correctors surpasses that for proficient reasoners.
Codes and data are available on
https://github.com/ChenhaoEcnuCS/Reason-Correct.

ÊëòË¶ÅÔºöÊï∏Â≠∏‰∏ñÁïåÂïèÈ°å‰øÆÊ≠£ (MWPC) ÊòØ‰∏ÄÈ†ÖÊñ∞Á©éÁöÑ‰ªªÂãôÔºåÂ∞àÈñÄÁî®ÊñºÁ≥æÊ≠£Ëß£Ê±∫Êï∏Â≠∏ÂïèÈ°åÈÅéÁ®ã‰∏≠ÁöÑÊé®ÁêÜÈåØË™§„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÂà©Áî®Â§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÈÄ≤Ê≠•ÔºåÊàëÂÄëËß£Ê±∫‰∫ÜÂÖ©ÂÄãÈóúÈçµÁõÆÊ®ôÔºö(1) ÂçÄÂàÜÊï∏Â≠∏Êé®ÁêÜÂíåÈåØË™§‰øÆÊ≠£Ôºõ(2) Êé¢Á¥¢Â¢ûÂº∑ LLM Âú®Êï∏Â≠∏‰∏≠ÈåØË™§‰øÆÊ≠£ËÉΩÂäõ‰ª•Ëß£Ê±∫ MWPC ‰ªªÂãôÁöÑÁ≠ñÁï•„ÄÇÊàëÂÄëÊ≥®ÊÑèÂà∞ÔºåÂú®ÂØ¶ÊôÇÊïôËÇ≤‰∏≠ÔºåÂçîÂä©Â≠∏ÁîüË™çË≠ò‰ªñÂÄëÁöÑÈåØË™§ÊØîÂÉÖÊèê‰æõÊ≠£Á¢∫Á≠îÊ°àÊõ¥ÁÇ∫ÈóúÈçµ„ÄÇÁÑ∂ËÄåÔºåÁõÆÂâçÁöÑÁ†îÁ©∂ÊâÄÂÇæÂêëÊñºÂÑ™ÂÖàÂèñÂæóÊï∏Â≠∏ÂïèÈ°åÁöÑÊ∫ñÁ¢∫Ëß£ÔºåËÄå‰∏çÊòØÁ≥æÊ≠£ÊΩõÂú®ÁöÑÈåØË™§Ëß£„ÄÇÂõ†Ê≠§ÔºåÊàëÂÄë‰øÆÊîπ‰∫ÜÁ†îÁ©∂ÁØÑ‰æãÔºåË≠âÊòéÊîπÈÄ≤Êï∏Â≠∏Êé®ÁêÜËÉΩÂäõ‰∏¶‰∏çÁ≠âÊñºÁ≤æÈÄöÈåØË™§‰øÆÊ≠£„ÄÇÂêåÊôÇÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Ë®∫Êñ∑Â∞éÂêëÊèêÁ§∫ (DOP) ÁöÑÊñ∞ÊñπÊ≥ïÔºåÊó®Âú®‰øÉÈÄ≤ LLM Âú®ÈåØË™§‰øÆÊ≠£ÊñπÈù¢Ë°®ÁèæÂá∫Ëâ≤„ÄÇÂú®ÂØ¶È©ó‰∏≠ÔºåDOP Â∑≤Â±ïÁèæÂá∫ÂÇëÂá∫ÁöÑÊïàËÉΩÔºåÁ™ÅÈ°ØÂÖ∂È°ØËëóÂΩ±Èüø„ÄÇÊàëÂÄëË™çÁÇ∫Âú®Êï∏Â≠∏ÊïôËÇ≤‰∏≠ÔºåÂ∞çÂÇëÂá∫‰øÆÊ≠£ËÄÖÁöÑÈúÄÊ±ÇË∂ÖË∂ä‰∫ÜÂ∞çÁÜüÁ∑¥Êé®ÁêÜËÄÖÁöÑÈúÄÊ±Ç„ÄÇÁ®ãÂºèÁ¢ºÂíåË≥áÊñôÂèØÂú® https://github.com/ChenhaoEcnuCS/Reason-Correct ÂèñÂæó„ÄÇ

##### **Distributional Semantics, Holism, and the Instability of Meaning**
2405.12084v1 by Jumbly Grindrod, J. D. Porter, Nat Hansen

Current language models are built on the so-called distributional semantic
approach to linguistic meaning that has the distributional hypothesis at its
core. The distributional hypothesis involves a holistic conception of word
meaning: the meaning of a word depends upon its relations to other words in the
model. A standard objection to meaning holism is the charge of instability: any
change in the meaning properties of a linguistic system (a human speaker, for
example) would lead to many changes or possibly a complete change in the entire
system. When the systems in question are trying to communicate with each other,
it has been argued that instability of this kind makes communication impossible
(Fodor and Lepore 1992, 1996, 1999). In this article, we examine whether the
instability objection poses a problem for distributional models of meaning.
First, we distinguish between distinct forms of instability that these models
could exhibit, and we argue that only one such form is relevant for
understanding the relation between instability and communication: what we call
differential instability. Differential instability is variation in the relative
distances between points in a space, rather than variation in the absolute
position of those points. We distinguish differential and absolute instability
by constructing two of our own models, a toy model constructed from the text of
two novels, and a more sophisticated model constructed using the Word2vec
algorithm from a combination of Wikipedia and SEP articles. We demonstrate the
two forms of instability by showing how these models change as the corpora they
are constructed from increase in size.

ÊëòË¶ÅÔºöÁï∂ÂâçË™ûË®ÄÊ®°ÂûãÂª∫Á´ãÂú®ÊâÄË¨ÇÁöÑÂàÜÈÖçË™ûÁæ©ÊñπÊ≥ï‰∏äÔºå‰ª•Ë™ûË®ÄÊÑèÁæ©ÁÇ∫Ê†∏ÂøÉÔºåÂàÜÈÖçÂÅáË®≠ÁÇ∫ÂÖ∂Ê†∏ÂøÉ„ÄÇÂàÜÈÖçÂÅáË®≠Ê∂âÂèäÂñÆË©ûÊÑèÁæ©ÁöÑÊï¥È´îÊ¶ÇÂøµÔºöÂñÆË©ûÁöÑÊÑèÁæ©ÂèñÊ±∫ÊñºÂÆÉËàáÊ®°Âûã‰∏≠ÂÖ∂‰ªñÂñÆË©ûÁöÑÈóú‰øÇ„ÄÇÂ∞çÊÑèÁæ©Êï¥È´îË´ñÁöÑÊ®ôÊ∫ñÂèçÂ∞çÊÑèË¶ãÊòØ‰∏çÁ©©ÂÆöÊÄßÁöÑÊåáÊéßÔºöË™ûË®ÄÁ≥ªÁµ±Ôºà‰æãÂ¶Ç‰∫∫È°ûË™™Ë©±ËÄÖÔºâÁöÑÊÑèÁæ©Â±¨ÊÄß‰∏≠ÁöÑ‰ªª‰ΩïËÆäÂåñÈÉΩÊúÉÂ∞éËá¥Ë®±Â§öËÆäÂåñÔºåÊàñËÄÖÂèØËÉΩÂ∞éËá¥Êï¥ÂÄãÁ≥ªÁµ±ÁöÑÂÆåÂÖ®ËÆäÂåñ„ÄÇÁï∂ÊúâÂïèÈ°åÁöÑÁ≥ªÁµ±ÂòóË©¶Áõ∏‰∫íÈÄö‰ø°ÊôÇÔºåÊúâ‰∫∫Ë™çÁÇ∫ÈÄôÁ®Æ‰∏çÁ©©ÂÆöÊÄß‰ΩøÂæóÈÄö‰ø°ËÆäÂæó‰∏çÂèØËÉΩÔºàFodor and Lepore 1992, 1996, 1999Ôºâ„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÊé¢Ë®é‰∫Ü‰∏çÁ©©ÂÆöÊÄßÂèçÂ∞çÊÑèË¶ãÊòØÂê¶Â∞çÊÑèÁæ©ÁöÑÂàÜÈÖçÊ®°ÂûãÊßãÊàêÂïèÈ°å„ÄÇÈ¶ñÂÖàÔºåÊàëÂÄëÂçÄÂàÜÈÄô‰∫õÊ®°ÂûãÂèØËÉΩË°®ÁèæÂá∫ÁöÑ‰∏çÂêåÂΩ¢ÂºèÁöÑ‰∏çÁ©©ÂÆöÊÄßÔºåÊàëÂÄëË™çÁÇ∫Âè™Êúâ‰∏ÄÁ®ÆÂΩ¢ÂºèËàáÁêÜËß£‰∏çÁ©©ÂÆöÊÄßÂíåÈÄö‰ø°‰πãÈñìÁöÑÈóú‰øÇÁõ∏ÈóúÔºöÊàëÂÄëÁ®±‰πãÁÇ∫Â∑ÆÁï∞‰∏çÁ©©ÂÆöÊÄß„ÄÇÂ∑ÆÁï∞‰∏çÁ©©ÂÆöÊÄßÊòØÁ©∫Èñì‰∏≠ÂêÑÈªû‰πãÈñìÁõ∏Â∞çË∑ùÈõ¢ÁöÑËÆäÂåñÔºåËÄå‰∏çÊòØÈÄô‰∫õÈªûÁµïÂ∞ç‰ΩçÁΩÆÁöÑËÆäÂåñ„ÄÇÊàëÂÄëÈÄöÈÅéÊßãÂª∫ÊàëÂÄëËá™Â∑±ÁöÑÂÖ©ÂÄãÊ®°Âûã‰æÜÂçÄÂàÜÂ∑ÆÁï∞ÂíåÁµïÂ∞ç‰∏çÁ©©ÂÆöÊÄßÔºå‰∏ÄÂÄãÁé©ÂÖ∑Ê®°ÂûãÁî±ÂÖ©ÈÉ®Â∞èË™™ÁöÑÊñáÊú¨ÊßãÂª∫Ôºå‰∏ÄÂÄãÊõ¥Á≤æÁ∑ªÁöÑÊ®°ÂûãÁî± Word2vec ÊºîÁÆóÊ≥ï‰ΩøÁî®Á∂≠Âü∫ÁôæÁßëÂíå SEP ÊñáÁ´†ÁöÑÁµÑÂêàÊßãÂª∫„ÄÇÊàëÂÄëÈÄöÈÅéÂ±ïÁ§∫ÈÄô‰∫õÊ®°ÂûãÂ¶Ç‰ΩïÈö®ËëóÊßãÂª∫ÂÆÉÂÄëÁöÑË™ûÊñôÂ∫´Â§ßÂ∞èÁöÑÂ¢ûÂä†ËÄåÊîπËÆäÔºå‰æÜË≠âÊòéÈÄôÂÖ©Á®ÆÂΩ¢ÂºèÁöÑ‰∏çÁ©©ÂÆöÊÄß„ÄÇ

##### **Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model**
2405.12081v1 by Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Ido Dagan

To obtain high-quality annotations under limited budget, semi-automatic
annotation methods are commonly used, where a portion of the data is annotated
by experts and a model is then trained to complete the annotations for the
remaining data. However, these methods mainly focus on selecting informative
data for expert annotations to improve the model predictive ability (i.e.,
triage-to-human data), while the rest of the data is indiscriminately assigned
to model annotation (i.e., triage-to-model data). This may lead to
inefficiencies in budget allocation for annotations, as easy data that the
model could accurately annotate may be unnecessarily assigned to the expert,
and hard data may be misclassified by the model. As a result, the overall
annotation quality may be compromised. To address this issue, we propose a
selective annotation framework called SANT. It effectively takes advantage of
both the triage-to-human and triage-to-model data through the proposed
error-aware triage and bi-weighting mechanisms. As such, informative or hard
data is assigned to the expert for annotation, while easy data is handled by
the model. Experimental results show that SANT consistently outperforms other
baselines, leading to higher-quality annotation through its proper allocation
of data to both expert and model workers. We provide pioneering work on data
annotation within budget constraints, establishing a landmark for future
triage-based annotation studies.

ÊëòË¶ÅÔºöÁÇ∫‰∫ÜÂú®ÊúâÈôêÁöÑÈ†êÁÆó‰∏ãÂèñÂæóÈ´òÂìÅË≥™ÁöÑË®ªËß£ÔºåÈÄöÂ∏∏ÊúÉ‰ΩøÁî®ÂçäËá™ÂãïË®ªËß£ÊñπÊ≥ïÔºåÂÖ∂‰∏≠‰∏ÄÈÉ®ÂàÜË≥áÊñôÁî±Â∞àÂÆ∂Ë®ªËß£ÔºåÁÑ∂ÂæåË®ìÁ∑¥Ê®°Âûã‰æÜÂÆåÊàêÂÖ∂È§òË≥áÊñôÁöÑË®ªËß£„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ï‰∏ªË¶ÅËëóÈáçÊñºÈÅ∏ÊìáÂÖ∑Ë≥áË®äÊÄßÁöÑË≥áÊñôÈÄ≤Ë°åÂ∞àÂÆ∂Ë®ªËß£Ôºå‰ª•ÊèêÂçáÊ®°ÂûãÁöÑÈ†êÊ∏¨ËÉΩÂäõÔºà‰∫¶Âç≥ÔºåÂàÜÊµÅÂà∞‰∫∫È°ûÁöÑË≥áÊñôÔºâÔºåËÄåÂÖ∂È§òË≥áÊñôÂâá‰∏çÂä†ÂçÄÂà•Âú∞ÂàÜÈÖçÁµ¶Ê®°ÂûãË®ªËß£Ôºà‰∫¶Âç≥ÔºåÂàÜÊµÅÂà∞Ê®°ÂûãÁöÑË≥áÊñôÔºâ„ÄÇÈÄôÂèØËÉΩÊúÉÂ∞éËá¥Ë®ªËß£È†êÁÆóÂàÜÈÖçÁöÑ‰ΩéÊïàÁéáÔºåÂõ†ÁÇ∫Ê®°ÂûãÂèØ‰ª•Ê∫ñÁ¢∫Ë®ªËß£ÁöÑÂÆπÊòìË≥áÊñôÂèØËÉΩÊúÉ‰∏çÂøÖË¶ÅÂú∞ÂàÜÈÖçÁµ¶Â∞àÂÆ∂ÔºåËÄåÂõ∞Èõ£ÁöÑË≥áÊñôÂèØËÉΩÊúÉË¢´Ê®°ÂûãÈåØË™§ÂàÜÈ°û„ÄÇÂõ†Ê≠§ÔºåÊï¥È´îË®ªËß£ÂìÅË≥™ÂèØËÉΩÊúÉÂèóÂà∞ÂΩ±Èüø„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ®±ÁÇ∫ SANT ÁöÑÈÅ∏ÊìáÊÄßË®ªËß£Êû∂Êßã„ÄÇÂÆÉÊúâÊïàÂú∞Âà©Áî®‰∫ÜÂàÜÊµÅÂà∞‰∫∫È°ûÂíåÂàÜÊµÅÂà∞Ê®°ÂûãÁöÑË≥áÊñôÔºåÈÄèÈÅéÊèêÂá∫ÁöÑÈåØË™§ÊÑüÁü•ÂàÜÊµÅÂíåÈõôÈáçÂä†Ê¨äÊ©üÂà∂„ÄÇÂõ†Ê≠§ÔºåÂÖ∑Ë≥áË®äÊÄßÊàñÂõ∞Èõ£ÁöÑË≥áÊñôÊúÉÂàÜÈÖçÁµ¶Â∞àÂÆ∂ÈÄ≤Ë°åË®ªËß£ÔºåËÄåÂÆπÊòìÁöÑË≥áÊñôÂâáÁî±Ê®°ÂûãËôïÁêÜ„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåSANT ÊåÅÁ∫åÂÑ™ÊñºÂÖ∂‰ªñÂü∫Ê∫ñÔºåÈÄèÈÅéÈÅ©Áï∂Âú∞Â∞áË≥áÊñôÂàÜÈÖçÁµ¶Â∞àÂÆ∂ÂíåÊ®°ÂûãÂ∑•‰Ωú‰∫∫Âì°ÔºåÁî¢ÁîüÊõ¥È´òÂìÅË≥™ÁöÑË®ªËß£„ÄÇÊàëÂÄëÊèê‰æõ‰∫ÜÂú®È†êÁÆóÈôêÂà∂‰∏ãÈÄ≤Ë°åË≥áÊñôË®ªËß£ÁöÑÈñãÂâµÊÄßÂ∑•‰ΩúÔºåÁÇ∫Êú™‰æÜÁöÑÂü∫ÊñºÂàÜÊµÅÁöÑË®ªËß£Á†îÁ©∂Âª∫Á´ã‰∫Ü‰∏ÄÂÄãÈáåÁ®ãÁ¢ë„ÄÇ

##### **AutoSoccerPose: Automated 3D posture Analysis of Soccer Shot Movements**
2405.12070v1 by Calvin Yeung, Kenjiro Ide, Keisuke Fujii

Image understanding is a foundational task in computer vision, with recent
applications emerging in soccer posture analysis. However, existing publicly
available datasets lack comprehensive information, notably in the form of
posture sequences and 2D pose annotations. Moreover, current analysis models
often rely on interpretable linear models (e.g., PCA and regression), limiting
their capacity to capture non-linear spatiotemporal relationships in complex
and diverse scenarios. To address these gaps, we introduce the 3D Shot Posture
(3DSP) dataset in soccer broadcast videos, which represents the most extensive
sports image dataset with 2D pose annotations to our knowledge. Additionally,
we present the 3DSP-GRAE (Graph Recurrent AutoEncoder) model, a non-linear
approach for embedding pose sequences. Furthermore, we propose AutoSoccerPose,
a pipeline aimed at semi-automating 2D and 3D pose estimation and posture
analysis. While achieving full automation proved challenging, we provide a
foundational baseline, extending its utility beyond the scope of annotated
data. We validate AutoSoccerPose on SoccerNet and 3DSP datasets, and present
posture analysis results based on 3DSP. The dataset, code, and models are
available at: https://github.com/calvinyeungck/3D-Shot-Posture-Dataset.

ÊëòË¶ÅÔºöÂΩ±ÂÉèÁêÜËß£ÊòØÈõªËÖ¶Ë¶ñË¶∫‰∏≠ÁöÑÂü∫Á§é‰ªªÂãôÔºåÊúÄËøëÂú®Ë∂≥ÁêÉÂßøÂã¢ÂàÜÊûê‰∏≠Âá∫Áèæ‰∫ÜÊñ∞ÁöÑÊáâÁî®„ÄÇÁÑ∂ËÄåÔºåÁèæÊúâÁöÑÂÖ¨ÈñãÂèØÁî®Ë≥áÊñôÈõÜÁº∫‰πèÂÖ®Èù¢ÁöÑË≥áË®äÔºåÁâπÂà•ÊòØÂßøÂã¢Â∫èÂàóÂíå 2D ÂßøÂã¢Ë®ªËß£ÁöÑÂΩ¢Âºè„ÄÇÊ≠§Â§ñÔºåÁõÆÂâçÁöÑÂàÜÊûêÊ®°ÂûãÈÄöÂ∏∏‰æùË≥¥ÊñºÂèØËß£ÈáãÁöÑÁ∑öÊÄßÊ®°ÂûãÔºà‰æãÂ¶ÇÔºåPCA ÂíåÂõûÊ≠∏ÔºâÔºåÈÄôÈôêÂà∂‰∫ÜÂÆÉÂÄëÂú®Ë§áÈõú‰∏îÂ§öÊ®£ÂåñÁöÑÂ†¥ÊôØ‰∏≠ÊçïÊçâÈùûÁ∑öÊÄßÊôÇÁ©∫Èóú‰øÇÁöÑËÉΩÂäõ„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄô‰∫õÂ∑ÆË∑ùÔºåÊàëÂÄëÂú®Ë∂≥ÁêÉÂª£Êí≠ÂΩ±Áâá‰∏≠ÂºïÂÖ•‰∫Ü 3D Â∞ÑÈñÄÂßøÂã¢ (3DSP) Ë≥áÊñôÈõÜÔºåÂÆÉ‰ª£Ë°®‰∫ÜÊàëÂÄëÊâÄÁü•ÊúÄÂª£Ê≥õÁöÑÂÖ∑Êúâ 2D ÂßøÂã¢Ë®ªËß£ÁöÑÈÅãÂãïÂΩ±ÂÉèË≥áÊñôÈõÜ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü 3DSP-GRAEÔºàÂúñÂΩ¢ÈÅûËø¥Ëá™ÂãïÁ∑®Á¢ºÂô®ÔºâÊ®°ÂûãÔºåÈÄôÊòØ‰∏ÄÁ®ÆÁî®ÊñºÂµåÂÖ•ÂßøÂã¢Â∫èÂàóÁöÑÈùûÁ∑öÊÄßÊñπÊ≥ï„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü AutoSoccerPoseÔºåÈÄôÊòØ‰∏ÄÂÄãÊó®Âú®ÂçäËá™ÂãïÂåñ 2D Âíå 3D ÂßøÂã¢‰º∞Ë®àÂíåÂßøÂã¢ÂàÜÊûêÁöÑÁÆ°ÈÅì„ÄÇÂÑòÁÆ°ÂØ¶ÁèæÂÆåÂÖ®Ëá™ÂãïÂåñË¢´Ë≠âÊòéÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºå‰ΩÜÊàëÂÄëÊèê‰æõ‰∫Ü‰∏ÄÂÄãÂü∫Á§éÂü∫Ê∫ñÔºåÂ∞áÂÖ∂ÊïàÁî®Êì¥Â±ïÂà∞Ë®ªËß£Ë≥áÊñôÁöÑÁØÑÂúç‰πãÂ§ñ„ÄÇÊàëÂÄëÂú® SoccerNet Âíå 3DSP Ë≥áÊñôÈõÜ‰∏äÈ©óË≠â‰∫Ü AutoSoccerPoseÔºå‰∏¶Ê†πÊìö 3DSP ÂëàÁèæÂßøÂã¢ÂàÜÊûêÁµêÊûú„ÄÇË≥áÊñôÈõÜ„ÄÅÁ®ãÂºèÁ¢ºÂíåÊ®°ÂûãÂèØÂú®‰ª•‰∏ã‰ΩçÁΩÆÂèñÂæóÔºöhttps://github.com/calvinyeungck/3D-Shot-Posture-Dataset„ÄÇ

##### **CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models**
2405.12063v1 by Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, Tat-Seng Chua

Large language models (LLMs) are increasingly used to meet user information
needs, but their effectiveness in dealing with user queries that contain
various types of ambiguity remains unknown, ultimately risking user trust and
satisfaction. To this end, we introduce CLAMBER, a benchmark for evaluating
LLMs using a well-organized taxonomy. Building upon the taxonomy, we construct
~12K high-quality data to assess the strengths, weaknesses, and potential risks
of various off-the-shelf LLMs. Our findings indicate the limited practical
utility of current LLMs in identifying and clarifying ambiguous user queries,
even enhanced by chain-of-thought (CoT) and few-shot prompting. These
techniques may result in overconfidence in LLMs and yield only marginal
enhancements in identifying ambiguity. Furthermore, current LLMs fall short in
generating high-quality clarifying questions due to a lack of conflict
resolution and inaccurate utilization of inherent knowledge. In this paper,
CLAMBER presents a guidance and promotes further research on proactive and
trustworthy LLMs. Our dataset is available at
https://github.com/zt991211/CLAMBER

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) ÊÑà‰æÜÊÑàÂ∏∏Ë¢´Áî®ÊñºÊªøË∂≥‰ΩøÁî®ËÄÖÁöÑË≥áË®äÈúÄÊ±ÇÔºå‰ΩÜÂÆÉÂÄëÂú®ËôïÁêÜÂåÖÂê´ÂêÑÁ®ÆÊ≠ßÁæ©ÁöÑ‰ΩøÁî®ËÄÖÊü•Ë©¢ÊôÇÁöÑÊúâÊïàÊÄß‰ªç‰∏çÂæóËÄåÁü•ÔºåÊúÄÁµÇÊúÉÂΩ±Èüø‰ΩøÁî®ËÄÖÁöÑ‰ø°‰ªªÂíåÊªøÊÑèÂ∫¶„ÄÇÁÇ∫Ê≠§ÔºåÊàëÂÄëÂºïÂÖ•‰∫Ü CLAMBERÔºå‰∏ÄÂÄãÁî®Êñº‰ΩøÁî®‰∫ïÁÑ∂ÊúâÂ∫èÁöÑÂàÜÈ°ûÊ≥ïË©ï‰º∞ LLM ÁöÑÂü∫Ê∫ñ„ÄÇÂú®ÂàÜÈ°ûÊ≥ïÁöÑÂü∫Á§é‰∏äÔºåÊàëÂÄëÂª∫Êßã‰∫ÜÁ¥Ñ 12K ÂÄãÈ´òÂìÅË≥™ÁöÑË≥áÊñôÔºå‰ª•Ë©ï‰º∞ÂêÑÁ®ÆÁèæÊàêÁöÑ LLM ÁöÑÂÑ™Èªû„ÄÅÁº∫ÈªûÂíåÊΩõÂú®È¢®Èö™„ÄÇÊàëÂÄëÁöÑÁ†îÁ©∂ÁµêÊûúÈ°ØÁ§∫ÔºåÁèæÊúâ LLM Âú®Ë≠òÂà•ÂíåÈáêÊ∏ÖÊúâÊ≠ßÁæ©ÁöÑ‰ΩøÁî®ËÄÖÊü•Ë©¢ÊñπÈù¢ÁöÑÂØ¶ÈöõÊïàÁî®ÊúâÈôêÔºåÂç≥‰ΩøÈÄèÈÅéÊÄùËÄÉÈèà (CoT) ÂíåÂ∞ëÈáèÊèêÁ§∫Âä†‰ª•Â¢ûÂº∑„ÄÇÈÄô‰∫õÊäÄË°ìÂèØËÉΩÊúÉÂ∞éËá¥Â∞ç LLM ÈÅéÂ∫¶Ëá™‰ø°ÔºåËÄå‰∏îÂú®Ë≠òÂà•Ê≠ßÁæ©ÊñπÈù¢ÂÉÖËÉΩÂ∏∂‰æÜÈÇäÈöõÊîπÂñÑ„ÄÇÊ≠§Â§ñÔºåÁèæÊúâ LLM Âú®Áî¢ÁîüÈ´òÂìÅË≥™ÁöÑÊæÑÊ∏ÖÂïèÈ°åÊñπÈù¢‰πüÊúâÊâÄ‰∏çË∂≥ÔºåÂéüÂõ†ÊòØÁº∫‰πèË°ùÁ™ÅËß£Ê±∫Âíå‰∏çÊ∫ñÁ¢∫Âà©Áî®ÂÖßÂú®Áü•Ë≠ò„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåCLAMBER Êèê‰æõ‰∫Ü‰∏ÄÂÄãÊåáÂ∞éÊñπÈáùÔºå‰∏¶‰øÉÈÄ≤Â∞ç‰∏ªÂãï‰∏îÂÄºÂæó‰ø°Ë≥¥ÁöÑ LLM ÁöÑÈÄ≤‰∏ÄÊ≠•Á†îÁ©∂„ÄÇÊàëÂÄëÁöÑË≥áÊñôÈõÜÂèØÂú® https://github.com/zt991211/CLAMBER ÂèñÂæó

##### **STYLE: Improving Domain Transferability of Asking Clarification Questions in Large Language Model Powered Conversational Agents**
2405.12059v1 by Yue Chen, Chen Huang, Yang Deng, Wenqiang Lei, Dingnan Jin, Jia Liu, Tat-Seng Chua

Equipping a conversational search engine with strategies regarding when to
ask clarification questions is becoming increasingly important across various
domains. Attributing to the context understanding capability of LLMs and their
access to domain-specific sources of knowledge, LLM-based clarification
strategies feature rapid transfer to various domains in a post-hoc manner.
However, they still struggle to deliver promising performance on unseen
domains, struggling to achieve effective domain transferability. We take the
first step to investigate this issue and existing methods tend to produce
one-size-fits-all strategies across diverse domains, limiting their search
effectiveness. In response, we introduce a novel method, called Style, to
achieve effective domain transferability. Our experimental results indicate
that Style bears strong domain transferability, resulting in an average search
performance improvement of ~10% on four unseen domains.

ÊëòË¶ÅÔºöÈö®ËëóÂ∞çË©±ÂºèÊêúÂ∞ãÂºïÊìéÂú®ÂêÑÁ®ÆÈ†òÂüü‰∏≠Êó•ÁõäÈáçË¶ÅÔºåÂõ†Ê≠§ÈúÄË¶ÅÁÇ∫ÂÖ∂ÈÖçÂÇôÊúâÈóú‰ΩïÊôÇÊèêÂá∫ÊæÑÊ∏ÖÂïèÈ°åÁöÑÁ≠ñÁï•„ÄÇÂü∫Êñº LLM Â∞çËÑàÁµ°ÁêÜËß£ÁöÑËÉΩÂäõÂèäÂÖ∂Â≠òÂèñÁâπÂÆöÈ†òÂüüÁü•Ë≠ò‰æÜÊ∫êÁöÑËÉΩÂäõÔºåÂü∫Êñº LLM ÁöÑÊæÑÊ∏ÖÁ≠ñÁï•Âú®‰∫ãÂæåÊñπÂºè‰∏≠ÂÖ∑ÂÇôÂø´ÈÄüËΩâÁßªËá≥ÂêÑÁ®ÆÈ†òÂüüÁöÑÂäüËÉΩ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÂÄë‰ªçÁÑ∂Èõ£‰ª•Âú®Êú™Ë¶ãÈÅéÁöÑÈ†òÂüü‰∏≠Êèê‰æõÊúâÂâçÈÄîÁöÑÊïàËÉΩÔºåÈõ£‰ª•ÈÅîÊàêÊúâÊïàÁöÑÈ†òÂüüÂèØËΩâÁßªÊÄß„ÄÇÊàëÂÄëÊé°ÂèñÁ¨¨‰∏ÄÊ≠•‰æÜÊé¢Ë®éÈÄôÂÄãÂïèÈ°åÔºåÁèæÊúâÊñπÊ≥ïÂÇæÂêëÊñºÂú®ÂêÑÁ®ÆÈ†òÂüü‰∏≠Áî¢Áîü‰∏ÄÈ´îÈÅ©Áî®ÁöÑÁ≠ñÁï•ÔºåÈôêÂà∂ÂÖ∂ÊêúÂ∞ãÊïàÊûú„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëÂºïÈÄ≤‰∏ÄÁ®ÆÁ®±ÁÇ∫ Style ÁöÑÊñ∞ÊñπÊ≥ïÔºå‰ª•ÈÅîÊàêÊúâÊïàÁöÑÈ†òÂüüÂèØËΩâÁßªÊÄß„ÄÇÊàëÂÄëÁöÑÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåStyle ÂÖ∑ÊúâÂº∑Â§ßÁöÑÈ†òÂüüÂèØËΩâÁßªÊÄßÔºåÂ∞éËá¥Âú®ÂõõÂÄãÊú™Ë¶ãÈÅéÁöÑÈ†òÂüü‰∏≠Âπ≥ÂùáÊêúÂ∞ãÊïàËÉΩÊèêÂçáÁ¥Ñ 10%„ÄÇ

##### **Unveiling factors influencing judgment variation in Sentiment Analysis with Natural Language Processing and Statistics**
2405.12055v1 by Olga Kellert, Carlos G√≥mez-Rodr√≠guez, Mahmud Uz Zaman

TripAdvisor reviews and comparable data sources play an important role in
many tasks in Natural Language Processing (NLP), providing a data basis for the
identification and classification of subjective judgments, such as hotel or
restaurant reviews, into positive or negative polarities. This study explores
three important factors influencing variation in crowdsourced polarity
judgments, focusing on TripAdvisor reviews in Spanish. Three hypotheses are
tested: the role of Part Of Speech (POS), the impact of sentiment words such as
"tasty", and the influence of neutral words like "ok" on judgment variation.
The study's methodology employs one-word titles, demonstrating their efficacy
in studying polarity variation of words. Statistical tests on mean equality are
performed on word groups of our interest. The results of this study reveal that
adjectives in one-word titles tend to result in lower judgment variation
compared to other word types or POS. Sentiment words contribute to lower
judgment variation as well, emphasizing the significance of sentiment words in
research on polarity judgments, and neutral words are associated with higher
judgment variation as expected. However, these effects cannot be always
reproduced in longer titles, which suggests that longer titles do not represent
the best data source for testing the ambiguity of single words due to the
influence on word polarity by other words like negation in longer titles. This
empirical investigation contributes valuable insights into the factors
influencing polarity variation of words, providing a foundation for NLP
practitioners that aim to capture and predict polarity judgments in Spanish and
for researchers that aim to understand factors influencing judgment variation.

ÊëòË¶ÅÔºö<paragraph>TripAdvisor Ë©ïË´ñÂíåÈ°û‰ººË≥áÊñô‰æÜÊ∫êÂú®Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜ (NLP) ÁöÑË®±Â§ö‰ªªÂãô‰∏≠ÊâÆÊºîÈáçË¶ÅËßíËâ≤ÔºåÊèê‰æõË≥áÊñôÂü∫Á§éÔºåÁî®ÊñºË≠òÂà•ÂíåÂàÜÈ°û‰∏ªËßÄÂà§Êñ∑Ôºå‰æãÂ¶ÇÈ£ØÂ∫óÊàñÈ§êÂª≥Ë©ïË´ñÔºåÁÇ∫Ê≠£ÂêëÊàñË≤†ÂêëÊ•µÊÄß„ÄÇÊ≠§Á†îÁ©∂Êé¢Ë®éÂΩ±ÈüøÁæ§ÁúæÂ§ñÂåÖÊ•µÊÄßÂà§Êñ∑ËÆäÂåñÁöÑ‰∏âÂÄãÈáçË¶ÅÂõ†Á¥†ÔºåÈáçÈªûÂú®ÊñºË•øÁè≠ÁâôÊñáÁöÑ TripAdvisor Ë©ïË´ñ„ÄÇÊ∏¨Ë©¶‰∫Ü‰∏âÂÄãÂÅáË®≠ÔºöË©ûÊÄß (POS) ÁöÑËßíËâ≤„ÄÅÁæéÂë≥Á≠âÊÉÖÊÑüË©ûÁöÑÂΩ±ÈüøÔºå‰ª•ÂèäÂÉè„Äåok„ÄçÁ≠â‰∏≠ÊÄßË©ûÂ∞çÂà§Êñ∑ËÆäÂåñÁöÑÂΩ±Èüø„ÄÇÊ≠§Á†îÁ©∂ÁöÑÊñπÊ≥ïÊé°Áî®ÂñÆÂ≠óÊ®ôÈ°åÔºåË≠âÊòéÂÖ∂Âú®Á†îÁ©∂Ë©ûÂΩôÊ•µÊÄßËÆäÂåñÁöÑÊïàËÉΩ„ÄÇÂ∞çÊàëÂÄëÊÑüËààË∂£ÁöÑË©ûÁæ§Âü∑Ë°åÂπ≥ÂùáÁõ∏Á≠âÊÄßÁµ±Ë®àÊ™¢ÂÆö„ÄÇÊ≠§Á†îÁ©∂ÁöÑÁµêÊûúÈ°ØÁ§∫ÔºåÂñÆÂ≠óÊ®ôÈ°å‰∏≠ÁöÑÂΩ¢ÂÆπË©ûÂÇæÂêëÊñºÈÄ†ÊàêËºÉ‰ΩéÁöÑÂà§Êñ∑ËÆäÂåñÔºåÁõ∏ËºÉÊñºÂÖ∂‰ªñË©ûÈ°ûÊàñË©ûÊÄß„ÄÇÊÉÖÊÑüË©û‰πüÊúâÂä©ÊñºÈôç‰ΩéÂà§Êñ∑ËÆäÂåñÔºåÂº∑Ë™øÊÉÖÊÑüË©ûÂú®Ê•µÊÄßÂà§Êñ∑Á†îÁ©∂‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåËÄå‰∏≠ÊÄßË©ûÂâáÈ†êÊúüÊúÉËàáËºÉÈ´òÁöÑÂà§Êñ∑ËÆäÂåñÁõ∏Èóú„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊïàÊáâÁÑ°Ê≥ïÁ∏ΩÊòØÂú®ËºÉÈï∑ÁöÑÊ®ôÈ°å‰∏≠ÈáçÁèæÔºåÈÄôË°®Á§∫ËºÉÈï∑ÁöÑÊ®ôÈ°å‰∏¶ÈùûÊ∏¨Ë©¶ÂñÆÂ≠óÊ≠ßÁæ©ÁöÑÊúÄ‰Ω≥Ë≥áÊñô‰æÜÊ∫êÔºåÂõ†ÁÇ∫Âú®ËºÉÈï∑ÁöÑÊ®ôÈ°å‰∏≠ÔºåÂê¶ÂÆöÁ≠âÂÖ∂‰ªñË©ûÂΩôÊúÉÂΩ±ÈüøË©ûÂΩôÊ•µÊÄß„ÄÇÈÄôÈ†ÖÂØ¶Ë≠âÁ†îÁ©∂Êèê‰æõ‰∫ÜÊúâÂÉπÂÄºÁöÑË¶ãËß£Ôºå‰∫ÜËß£ÂΩ±ÈüøË©ûÂΩôÊ•µÊÄßËÆäÂåñÁöÑÂõ†Á¥†ÔºåÁÇ∫ NLP ÂæûÊ•≠‰∫∫Âì°Â•†ÂÆöÂü∫Á§éÔºåÁõÆÊ®ôÂú®ÊñºÊì∑ÂèñÂíåÈ†êÊ∏¨Ë•øÁè≠ÁâôÊñáÁöÑÊ•µÊÄßÂà§Êñ∑Ôºå‰ª•ÂèäÁÇ∫Á†îÁ©∂‰∫∫Âì°Â•†ÂÆöÂü∫Á§éÔºåÁõÆÊ®ôÂú®Êñº‰∫ÜËß£ÂΩ±ÈüøÂà§Êñ∑ËÆäÂåñÁöÑÂõ†Á¥†„ÄÇ</paragraph>

##### **EXACT: Towards a platform for empirically benchmarking Machine Learning model explanation methods**
2405.12261v1 by Benedict Clark, Rick Wilming, Artur Dox, Paul Eschenbach, Sami Hached, Daniel Jin Wodke, Michias Taye Zewdie, Uladzislau Bruila, Marta Oliveira, Hjalmar Schulz, Luca Matteo Cornils, Danny Panknin, Ahc√®ne Boubekki, Stefan Haufe

The evolving landscape of explainable artificial intelligence (XAI) aims to
improve the interpretability of intricate machine learning (ML) models, yet
faces challenges in formalisation and empirical validation, being an inherently
unsupervised process. In this paper, we bring together various benchmark
datasets and novel performance metrics in an initial benchmarking platform, the
Explainable AI Comparison Toolkit (EXACT), providing a standardised foundation
for evaluating XAI methods. Our datasets incorporate ground truth explanations
for class-conditional features, and leveraging novel quantitative metrics, this
platform assesses the performance of post-hoc XAI methods in the quality of the
explanations they produce. Our recent findings have highlighted the limitations
of popular XAI methods, as they often struggle to surpass random baselines,
attributing significance to irrelevant features. Moreover, we show the
variability in explanations derived from different equally performing model
architectures. This initial benchmarking platform therefore aims to allow XAI
researchers to test and assure the high quality of their newly developed
methods.

ÊëòË¶ÅÔºöÂèØËß£Èáã‰∫∫Â∑•Êô∫ÊÖß (XAI) ‰∏çÊñ∑ÊºîÈÄ≤ÁöÑÈ†òÂüüÊó®Âú®ÊîπÂñÑË§áÈõúÊ©üÂô®Â≠∏Áøí (ML) Ê®°ÂûãÁöÑÂèØËß£ÈáãÊÄßÔºå‰ΩÜÁî±ÊñºÂÖ∂Êú¨Ë≥™‰∏ä‰∏çÂèóÁõ£Áù£ÁöÑÁ®ãÂ∫èÔºåÂõ†Ê≠§Âú®ÂΩ¢ÂºèÂåñÂíåÁ∂ìÈ©óÈ©óË≠âÊñπÈù¢Èù¢Ëá®ÊåëÊà∞„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂú®‰∏ÄÂÄãÂàùÂßãÂü∫Ê∫ñÊ∏¨Ë©¶Âπ≥Âè∞‰∏≠ÂΩôÈõÜ‰∫ÜÂêÑÁ®ÆÂü∫Ê∫ñË≥áÊñôÈõÜÂíåÊñ∞Á©éÁöÑÊïàËÉΩÊåáÊ®ôÔºåÂç≥ÂèØËß£Èáã AI ÊØîËºÉÂ∑•ÂÖ∑ÂåÖ (EXACT)ÔºåÁÇ∫Ë©ï‰º∞ XAI ÊñπÊ≥ïÊèê‰æõÊ®ôÊ∫ñÂåñÁöÑÂü∫Á§é„ÄÇÊàëÂÄëÁöÑË≥áÊñôÈõÜÁµêÂêà‰∫ÜÈ°ûÊ¢ù‰ª∂ÁâπÂæµÁöÑÂü∫Êú¨‰∫ãÂØ¶Ëß£ÈáãÔºå‰∏¶Âà©Áî®Êñ∞Á©éÁöÑÈáèÂåñÊåáÊ®ôÔºåÊ≠§Âπ≥Âè∞Ë©ï‰º∞‰∫ãÂæå XAI ÊñπÊ≥ïÂú®ÂÆÉÂÄëÁî¢ÁîüÁöÑËß£ÈáãÂìÅË≥™‰∏≠ÁöÑÊïàËÉΩ„ÄÇÊàëÂÄëÊúÄËøëÁöÑÁôºÁèæÂº∑Ë™ø‰∫ÜÊµÅË°åÁöÑ XAI ÊñπÊ≥ïÁöÑÈôêÂà∂ÔºåÂõ†ÁÇ∫ÂÆÉÂÄëÁ∂ìÂ∏∏Èõ£‰ª•Ë∂ÖË∂äÈö®Ê©üÂü∫Ê∫ñÁ∑öÔºåÂ∞áÈáçË¶ÅÊÄßÊ≠∏Âõ†ÊñºÁÑ°ÈóúÁöÑÁâπÂæµ„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÂæû‰∏çÂêåÊÄßËÉΩÁõ∏ÂêåÁöÑÊ®°ÂûãÊû∂Êßã‰∏≠ÂæóÂá∫ÁöÑËß£Èáã‰∏≠ÁöÑËÆäÁï∞ÊÄß„ÄÇÂõ†Ê≠§ÔºåÊ≠§ÂàùÂßãÂü∫Ê∫ñÊ∏¨Ë©¶Âπ≥Âè∞Êó®Âú®ËÆì XAI Á†îÁ©∂‰∫∫Âì°Ê∏¨Ë©¶‰∏¶Á¢∫‰øùÂÖ∂Êñ∞ÈñãÁôºÊñπÊ≥ïÁöÑÈ´òÂìÅË≥™„ÄÇ

##### **KG-RAG: Bridging the Gap Between Knowledge and Creativity**
2405.12035v1 by Diego Sanmartin

Ensuring factual accuracy while maintaining the creative capabilities of
Large Language Model Agents (LMAs) poses significant challenges in the
development of intelligent agent systems. LMAs face prevalent issues such as
information hallucinations, catastrophic forgetting, and limitations in
processing long contexts when dealing with knowledge-intensive tasks. This
paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)
pipeline, a novel framework designed to enhance the knowledge capabilities of
LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities
of LLMs, thereby significantly reducing the reliance on the latent knowledge of
LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then
performs information retrieval over the newly created graph to perform KGQA
(Knowledge Graph Question Answering). The retrieval methodology leverages a
novel algorithm called Chain of Explorations (CoE) which benefits from LLMs
reasoning to explore nodes and relationships within the KG sequentially.
Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable
improvements in the reduction of hallucinated content and suggest a promising
path toward developing intelligent systems adept at handling
knowledge-intensive tasks.

ÊëòË¶ÅÔºöÂú®Á¢∫‰øù‰∫ãÂØ¶Ê∫ñÁ¢∫ÊÄßÁöÑÂêåÊôÇÔºå‰øùÊåÅÂ§ßÂûãË™ûË®ÄÊ®°Âûã‰ª£ÁêÜÔºàLMAÔºâÁöÑÂâµÂª∫ËÉΩÂäõÔºåÂ∞çÊô∫ÊÖßÂûã‰ª£ÁêÜÁ≥ªÁµ±ÁöÑÈñãÁôºÊßãÊàê‰∫ÜÈáçÂ§ßÁöÑÊåëÊà∞„ÄÇLMA Èù¢Ëá®ÊôÆÈÅçÁöÑÂïèÈ°åÔºå‰æãÂ¶ÇË≥áË®äÂπªË¶∫„ÄÅÁÅΩÈõ£ÊÄßÈÅ∫ÂøòÔºå‰ª•ÂèäÂú®ËôïÁêÜÁü•Ë≠òÂØÜÈõÜÂûã‰ªªÂãôÊôÇËôïÁêÜÈï∑ËÑàÁµ°ÁöÑÈôêÂà∂„ÄÇÊú¨Êñá‰ªãÁ¥π‰∫Ü KG-RAGÔºàÁü•Ë≠òÂúñË≠úÊ™¢Á¥¢Â¢ûÂº∑ÁîüÊàêÔºâÁÆ°ÈÅìÔºåÈÄôÊòØ‰∏ÄÂÄãÊñ∞Á©éÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄèÈÅéÂ∞áÁµêÊßãÂåñÁöÑÁü•Ë≠òÂúñË≠úÔºàKGÔºâËàá LLM ÁöÑÂäüËÉΩÊï¥ÂêàÂú®‰∏ÄËµ∑Ôºå‰æÜÂ¢ûÂº∑ LMA ÁöÑÁü•Ë≠òËÉΩÂäõÔºåÂæûËÄåÈ°ØËëóÊ∏õÂ∞ëÂ∞ç LLM ÊΩõÂú®Áü•Ë≠òÁöÑ‰æùË≥¥„ÄÇKG-RAG ÁÆ°ÈÅìÂæûÈùûÁµêÊßãÂåñÊñáÂ≠ó‰∏≠Âª∫Êßã‰∏ÄÂÄã KGÔºåÁÑ∂ÂæåÂ∞çÊñ∞Âª∫Á´ãÁöÑÂúñË≠úÂü∑Ë°åË≥áË®äÊ™¢Á¥¢Ôºå‰ª•Âü∑Ë°å KGQAÔºàÁü•Ë≠òÂúñË≠úÂïèÁ≠îÔºâ„ÄÇÊ™¢Á¥¢ÊñπÊ≥ïÂà©Áî®‰∫Ü‰∏ÄÁ®ÆÁ®±ÁÇ∫Êé¢Á¥¢ÈèàÔºàCoEÔºâÁöÑÊñ∞ÊºîÁÆóÊ≥ïÔºåË©≤ÊºîÁÆóÊ≥ïÂèóÁõäÊñº LLM Êé®ÁêÜÔºå‰ª•Âæ™Â∫èÊé¢Á¥¢ KG ‰∏≠ÁöÑÁØÄÈªûÂíåÈóú‰øÇ„ÄÇÂú® ComplexWebQuestions Ë≥áÊñôÈõÜ‰∏äÁöÑÂàùÊ≠•ÂØ¶È©óË≠âÊòé‰∫ÜÂú®Ê∏õÂ∞ëÂπªË¶∫ÂÖßÂÆπÊñπÈù¢ÊúâÈ°ØËëóÁöÑÊîπÈÄ≤Ôºå‰∏¶ÊöóÁ§∫‰∫Ü‰∏ÄÊ¢ùÊúâÂ∏åÊúõÁöÑÈÅìË∑ØÔºåÊúùËëóÈñãÁôºÊìÖÈï∑ËôïÁêÜÁü•Ë≠òÂØÜÈõÜÂûã‰ªªÂãôÁöÑÊô∫ÊÖßÂûãÁ≥ªÁµ±ÈÇÅÈÄ≤„ÄÇ

##### **Can AI Relate: Testing Large Language Model Response for Mental Health Support**
2405.12021v1 by Saadia Gabriel, Isha Puri, Xuhai Xu, Matteo Malgaroli, Marzyeh Ghassemi

Large language models (LLMs) are already being piloted for clinical use in
hospital systems like NYU Langone, Dana-Farber and the NHS. A proposed
deployment use case is psychotherapy, where a LLM-powered chatbot can treat a
patient undergoing a mental health crisis. Deployment of LLMs for mental health
response could hypothetically broaden access to psychotherapy and provide new
possibilities for personalizing care. However, recent high-profile failures,
like damaging dieting advice offered by the Tessa chatbot to patients with
eating disorders, have led to doubt about their reliability in high-stakes and
safety-critical settings.
  In this work, we develop an evaluation framework for determining whether LLM
response is a viable and ethical path forward for the automation of mental
health treatment. Using human evaluation with trained clinicians and automatic
quality-of-care metrics grounded in psychology research, we compare the
responses provided by peer-to-peer responders to those provided by a
state-of-the-art LLM.
  We show that LLMs like GPT-4 use implicit and explicit cues to infer patient
demographics like race. We then show that there are statistically significant
discrepancies between patient subgroups: Responses to Black posters
consistently have lower empathy than for any other demographic group (2%-13%
lower than the control group). Promisingly, we do find that the manner in which
responses are generated significantly impacts the quality of the response. We
conclude by proposing safety guidelines for the potential deployment of LLMs
for mental health response.

ÊëòË¶ÅÔºöÂ§ßÂûãË™ûË®ÄÊ®°Âûã (LLM) Â∑≤Âú®Á¥êÁ¥ÑÂ§ßÂ≠∏ÊúóÊ†ºÂ∞º„ÄÅÈÅîÁ¥çÊ≥ï‰ºØÂíåËã±ÂúãÂúãÂÆ∂ÈÜ´ÁôÇÊúçÂãôÈ´îÁ≥ªÁ≠âÈÜ´Èô¢Á≥ªÁµ±‰∏≠Ë©¶Ë°åÁî®ÊñºËá®Â∫äÁî®ÈÄî„ÄÇÂª∫Ë≠∞ÁöÑÈÉ®ÁΩ≤Áî®‰æãÊòØÂøÉÁêÜÊ≤ªÁôÇÔºåÂÖ∂‰∏≠Áî± LLM È©ÖÂãïÁöÑËÅäÂ§©Ê©üÂô®‰∫∫ÂèØ‰ª•Ê≤ªÁôÇÊ≠£Âú®Á∂ìÊ≠∑ÂøÉÁêÜÂÅ•Â∫∑Âç±Ê©üÁöÑÊÇ£ËÄÖ„ÄÇÂÅáË®≠ LLM ÈÉ®ÁΩ≤ÊñºÂøÉÁêÜÂÅ•Â∫∑ÊáâÂ∞çÔºåÂèØ‰ª•Êì¥Â§ßÂøÉÁêÜÊ≤ªÁôÇÁöÑÊôÆÂèäÂ∫¶Ôºå‰∏¶Êèê‰æõÊñ∞ÁöÑÂèØËÉΩÊÄß‰æÜÂØ¶ÁèæÂÄãÊÄßÂåñË≠∑ÁêÜ„ÄÇÁÑ∂ËÄåÔºåÊúÄËøëÂÇôÂèóÁüöÁõÆÁöÑÂ§±ÊïóÔºå‰æãÂ¶Ç Tessa ËÅäÂ§©Ê©üÂô®‰∫∫ÁÇ∫È£≤È£üÂ§±Ë™øÊÇ£ËÄÖÊèê‰æõÁöÑÊúâÂÆ≥È£≤È£üÂª∫Ë≠∞ÔºåËÆì‰∫∫ÂÄëÂ∞çÂÖ∂Âú®È´òÈ¢®Èö™ÂíåÂÆâÂÖ®ÈóúÈçµÁí∞Â¢É‰∏≠ÁöÑÂèØÈù†ÊÄßÁî¢Áîü‰∫ÜÊá∑Áñë„ÄÇ
  Âú®ÈÄôÈ†ÖÂ∑•‰Ωú‰∏≠ÔºåÊàëÂÄëÈñãÁôº‰∫Ü‰∏ÄÂÄãË©ï‰º∞Ê°ÜÊû∂ÔºåÁî®ÊñºÁ¢∫ÂÆö LLM ÂõûÊáâÊòØÂê¶ÊòØ‰∏ÄÂÄãÂèØË°å‰∏îÂêà‰πéÈÅìÂæ∑ÁöÑÈÄîÂæëÔºåÁî®ÊñºËá™ÂãïÂåñÂøÉÁêÜÂÅ•Â∫∑Ê≤ªÁôÇ„ÄÇÊàëÂÄëÂà©Áî®Á∂ìÈÅéÂüπË®ìÁöÑËá®Â∫äÈÜ´ÁîüÈÄ≤Ë°åÁöÑ‰∫∫È°ûË©ï‰º∞ÂíåÂü∫ÊñºÂøÉÁêÜÂ≠∏Á†îÁ©∂ÁöÑËá™ÂãïË≥™ÈáèË≠∑ÁêÜÊåáÊ®ôÔºåÊØîËºÉ‰∫ÜÈªûÂ∞çÈªûÂõûÊáâËÄÖÊèê‰æõÁöÑÂõûÊáâËàáÊúÄÂÖàÈÄ≤ÁöÑ LLM Êèê‰æõÁöÑÂõûÊáâ„ÄÇ
  ÊàëÂÄëÂ±ïÁ§∫‰∫Ü GPT-4 Á≠â LLM ‰ΩøÁî®Èö±ÂºèÂíåÈ°ØÂºèÁ∑öÁ¥¢‰æÜÊé®Êñ∑ÊÇ£ËÄÖ‰∫∫Âè£Áµ±Ë®àË≥áÊñôÔºå‰æãÂ¶ÇÁ®ÆÊóè„ÄÇÁÑ∂ÂæåÊàëÂÄëÂ±ïÁ§∫‰∫ÜÊÇ£ËÄÖ‰∫ûÁµÑ‰πãÈñìÂ≠òÂú®Áµ±Ë®à‰∏äÈ°ØËëóÁöÑÂ∑ÆÁï∞ÔºöÂ∞çÈªë‰∫∫Êµ∑Â†±ÁöÑÂõûÊáâÂßãÁµÇ‰ΩéÊñº‰ªª‰ΩïÂÖ∂‰ªñ‰∫∫Âè£Áµ±Ë®àÁµÑÔºàÊØîÂ∞çÁÖßÁµÑ‰Ωé 2%-13%Ôºâ„ÄÇÂèØÂñúÁöÑÊòØÔºåÊàëÂÄëÁ¢∫ÂØ¶ÁôºÁèæÂõûÊáâÁöÑÁîüÊàêÊñπÂºèÊúÉÈ°ØËëóÂΩ±ÈüøÂõûÊáâÁöÑÂìÅË≥™„ÄÇÊúÄÂæåÔºåÊàëÂÄëÊèêÂá∫ LLM Âú®ÂøÉÁêÜÂÅ•Â∫∑ÊáâÂ∞ç‰∏≠ÊΩõÂú®ÈÉ®ÁΩ≤ÁöÑÂÆâÂÖ®ÊåáÂçó„ÄÇ

##### **Scrutinize What We Ignore: Reining Task Representation Shift In Context-Based Offline Meta Reinforcement Learning**
2405.12001v1 by Hai Zhang, Boyuan Zheng, Anqi Guo, Tianying Ji, Pheng-Ann Heng, Junqiao Zhao, Lanqing Li

Offline meta reinforcement learning (OMRL) has emerged as a promising
approach for interaction avoidance and strong generalization performance by
leveraging pre-collected data and meta-learning techniques. Previous
context-based approaches predominantly rely on the intuition that maximizing
the mutual information between the task and the task representation ($I(Z;M)$)
can lead to performance improvements. Despite achieving attractive results, the
theoretical justification of performance improvement for such intuition has
been lacking. Motivated by the return discrepancy scheme in the model-based RL
field, we find that maximizing $I(Z;M)$ can be interpreted as consistently
raising the lower bound of the expected return for a given policy conditioning
on the optimal task representation. However, this optimization process ignores
the task representation shift between two consecutive updates, which may lead
to performance improvement collapse. To address this problem, we turn to use
the framework of performance difference bound to consider the impacts of task
representation shift explicitly. We demonstrate that by reining the task
representation shift, it is possible to achieve monotonic performance
improvements, thereby showcasing the advantage against previous approaches. To
make it practical, we design an easy yet highly effective algorithm RETRO
(\underline{RE}ining \underline{T}ask \underline{R}epresentation shift in
context-based \underline{O}ffline meta reinforcement learning) with only adding
one line of code compared to the backbone. Empirical results validate its
state-of-the-art (SOTA) asymptotic performance, training stability and
training-time consumption on MuJoCo and MetaWorld benchmarks.

ÊëòË¶ÅÔºöÈõ¢Á∑öÂÖÉÂ¢ûÂº∑Â≠∏Áøí (OMRL) Â∑≤ÊàêÁÇ∫‰∏ÄÁ®ÆÊúâÂâçÈÄîÁöÑÊñπÊ≥ïÔºåÂÆÉÈÄèÈÅéÂà©Áî®È†êÂÖàÊî∂ÈõÜÁöÑË≥áÊñôÂíåÂÖÉÂ≠∏ÁøíÊäÄË°ìÔºå‰æÜÈÅøÂÖç‰∫íÂãï‰∏¶ÊèêÂçáÂº∑Â§ßÁöÑÊ≥õÂåñÊïàËÉΩ„ÄÇÂÖàÂâçÁöÑÂü∫ÊñºËÑàÁµ°ÁöÑÊñπÊ≥ï‰∏ªË¶Å‰æùË≥¥Êñº‰∏ÄÂÄãÁõ¥Ë¶∫ÔºåÂç≥ÊúÄÂ§ßÂåñ‰ªªÂãôËàá‰ªªÂãôË°®Á§∫‰πãÈñìÁöÑ‰∫í‰ø°ÊÅØ ($I(Z;M)$) ÂèØ‰ª•ÊèêÂçáÊïàËÉΩ„ÄÇÂÑòÁÆ°Áç≤Âæó‰∫Ü‰ª§‰∫∫ÊªøÊÑèÁöÑÁµêÊûúÔºå‰ΩÜÂ∞çÊñºÈÄôÁ®ÆÁõ¥Ë¶∫ÁöÑÊïàËÉΩÊèêÂçáÔºåÂçªÁº∫‰πèÁêÜË´ñ‰æùÊìö„ÄÇÂú®Âü∫ÊñºÊ®°ÂûãÁöÑ RL È†òÂüüÁöÑÂõûÂ†±Â∑ÆÁï∞ÊñπÊ°àÁöÑÂïüÁôº‰∏ãÔºåÊàëÂÄëÁôºÁèæÊúÄÂ§ßÂåñ $I(Z;M)$ ÂèØ‰ª•Ëß£ÈáãÁÇ∫‰∏ÄËá¥Âú∞ÊèêÈ´òÁµ¶ÂÆöÁ≠ñÁï•Âú®ÊúÄ‰Ω≥‰ªªÂãôË°®Á§∫‰∏äÁöÑÊ¢ù‰ª∂È†êÊúüÂõûÂ†±ÁöÑ‰∏ãÈôê„ÄÇÁÑ∂ËÄåÔºåÈÄôÂÄãÊúÄ‰Ω≥ÂåñÁ®ãÂ∫èÂøΩÁï•‰∫ÜÂÖ©ÂÄãÈÄ£Á∫åÊõ¥Êñ∞‰πãÈñìÁöÑ‰ªªÂãôË°®Á§∫ËΩâÁßªÔºåÈÄôÂèØËÉΩÊúÉÂ∞éËá¥ÊïàËÉΩÊèêÂçáÂ¥©ÊΩ∞„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊàëÂÄëËΩâËÄå‰ΩøÁî®ÊïàËÉΩÂ∑ÆÁï∞ÁïåÈôêÁöÑÊû∂ÊßãÔºå‰æÜÊòéÁ¢∫ËÄÉÊÖÆ‰ªªÂãôË°®Á§∫ËΩâÁßªÁöÑÂΩ±Èüø„ÄÇÊàëÂÄëË≠âÊòéÔºåÈÄèÈÅéÁ¥ÑÊùü‰ªªÂãôË°®Á§∫ËΩâÁßªÔºåÂèØ‰ª•ÂØ¶ÁèæÂñÆË™øÁöÑÊïàËÉΩÊèêÂçáÔºåÂæûËÄåÂ±ïÁ§∫Âá∫Áõ∏ËºÉÊñºÂÖàÂâçÊñπÊ≥ïÁöÑÂÑ™Âã¢„ÄÇÁÇ∫‰∫Ü‰ΩøÂÖ∂Êõ¥ÂØ¶Áî®ÔºåÊàëÂÄëË®≠Ë®à‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÈ´òÊïàÁéáÁöÑÊºîÁÆóÊ≥ï RETROÔºàÂú®Âü∫ÊñºËÑàÁµ°ÁöÑÈõ¢Á∑öÂÖÉÂ¢ûÂº∑Â≠∏Áøí‰∏≠Á¥ÑÊùü‰ªªÂãôË°®Á§∫ËΩâÁßªÔºâÔºåÂÆÉËàá‰∏ªÂππÁõ∏ÊØîÂè™Â¢ûÂä†‰∫ÜÁ®ãÂºèÁ¢ºÁöÑ‰∏ÄË°å„ÄÇÁ∂ìÈ©óÁµêÊûúÈ©óË≠â‰∫ÜÂÖ∂Âú® MuJoCo Âíå MetaWorld Âü∫Ê∫ñ‰∏äÁöÑÊúÄÂÖàÈÄ≤ (SOTA) ÁöÑÊº∏ËøëÊïàËÉΩ„ÄÅË®ìÁ∑¥Á©©ÂÆöÊÄßÂíåË®ìÁ∑¥ÊôÇÈñìÊ∂àËÄó„ÄÇ

##### **A review on the use of large language models as virtual tutors**
2405.11983v1 by Silvia Garc√≠a-M√©ndez, Francisco de Arriba-P√©rez, Mar√≠a del Carmen Somoza-L√≥pez

Transformer architectures contribute to managing long-term dependencies for
Natural Language Processing, representing one of the most recent changes in the
field. These architectures are the basis of the innovative, cutting-edge Large
Language Models (LLMs) that have produced a huge buzz in several fields and
industrial sectors, among the ones education stands out. Accordingly, these
generative Artificial Intelligence-based solutions have directed the change in
techniques and the evolution in educational methods and contents, along with
network infrastructure, towards high-quality learning. Given the popularity of
LLMs, this review seeks to provide a comprehensive overview of those solutions
designed specifically to generate and evaluate educational materials and which
involve students and teachers in their design or experimental plan. To the best
of our knowledge, this is the first review of educational applications (e.g.,
student assessment) of LLMs. As expected, the most common role of these systems
is as virtual tutors for automatic question generation. Moreover, the most
popular models are GTP-3 and BERT. However, due to the continuous launch of new
generative models, new works are expected to be published shortly.

ÊëòË¶ÅÔºöTransformer Êû∂ÊßãÊúâÂä©ÊñºÁÆ°ÁêÜËá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÁöÑÈï∑Êúü‰æùË≥¥ÊÄßÔºå‰ª£Ë°®Ë©≤È†òÂüüÊúÄÊñ∞ËÆäÈù©‰πã‰∏Ä„ÄÇÈÄô‰∫õÊû∂ÊßãÊòØÂâµÊñ∞„ÄÅÂ∞ñÁ´ØÁöÑÂ∑®ÈáèË™ûË®ÄÊ®°Âûã (LLM) ÁöÑÂü∫Á§éÔºåÈÄô‰∫õÊ®°ÂûãÂú®Â§öÂÄãÈ†òÂüüÂíåÁî¢Ê•≠ÈÉ®ÈñÄÂºïËµ∑Ê•µÂ§ßËø¥ÈüøÔºåÂÖ∂‰∏≠ÊïôËÇ≤È†òÂüüÂ∞§ÁÇ∫Á™ÅÂá∫„ÄÇÂõ†Ê≠§ÔºåÈÄô‰∫õÂü∫ÊñºÁîüÊàêÂºè‰∫∫Â∑•Êô∫ÊÖßÁöÑËß£Ê±∫ÊñπÊ°àÂºïÂ∞é‰∫ÜÊäÄË°ìËÆäÈù©Ôºå‰ª•ÂèäÊïôËÇ≤ÊñπÊ≥ïÂíåÂÖßÂÆπÁöÑÊºîÈÄ≤Ôºå‰ª•ÂèäÁ∂≤Ë∑ØÂü∫Á§éË®≠ÊñΩÔºåÊúùÂêëÈ´òÂìÅË≥™Â≠∏Áøí„ÄÇÈëëÊñº LLM ÁöÑÊôÆÂèäÊÄßÔºåÊú¨ÁØáË©ïË´ñÊó®Âú®Êèê‰æõÂ∞çÈÄô‰∫õËß£Ê±∫ÊñπÊ°àÁöÑÂÖ®Èù¢Ê¶ÇËø∞ÔºåÈÄô‰∫õËß£Ê±∫ÊñπÊ°àÂ∞àÈñÄË®≠Ë®àÁî®ÊñºÁî¢ÁîüÂíåË©ï‰º∞ÊïôËÇ≤ÊùêÊñôÔºå‰∏¶ËÆìÂ≠∏ÁîüÂíåÊïôÂ∏´ÂèÉËàáÂÖ∂Ë®≠Ë®àÊàñÂØ¶È©óË®àÁï´„ÄÇÊìöÊàëÂÄëÊâÄÁü•ÔºåÈÄôÊòØÁ¨¨‰∏ÄÁØá LLM ÊïôËÇ≤ÊáâÁî®Ôºà‰æãÂ¶ÇÂ≠∏ÁîüË©ïÈáèÔºâÁöÑË©ïË´ñ„ÄÇ‰∏çÂá∫ÊâÄÊñôÔºåÈÄô‰∫õÁ≥ªÁµ±ÊúÄÂ∏∏Ë¶ãÁöÑËßíËâ≤ÊòØ‰ΩúÁÇ∫Ëá™ÂãïÁî¢ÁîüÂïèÈ°åÁöÑËôõÊì¨Â∞éÂ∏´„ÄÇÊ≠§Â§ñÔºåÊúÄÂèóÊ≠°ËøéÁöÑÊ®°ÂûãÊòØ GTP-3 Âíå BERT„ÄÇÁÑ∂ËÄåÔºåÁî±ÊñºÊåÅÁ∫åÊé®Âá∫Êñ∞ÁöÑÁîüÊàêÂºèÊ®°ÂûãÔºåÈ†êË®à‰∏ç‰πÖÂæåÂ∞áÊúÉÁôºË°®Êñ∞ÁöÑ‰ΩúÂìÅ„ÄÇ

##### **Robust Deep Reinforcement Learning with Adaptive Adversarial Perturbations in Action Space**
2405.11982v1 by Qianmei Liu, Yufei Kuang, Jie Wang

Deep reinforcement learning (DRL) algorithms can suffer from modeling errors
between the simulation and the real world. Many studies use adversarial
learning to generate perturbation during training process to model the
discrepancy and improve the robustness of DRL. However, most of these
approaches use a fixed parameter to control the intensity of the adversarial
perturbation, which can lead to a trade-off between average performance and
robustness. In fact, finding the optimal parameter of the perturbation is
challenging, as excessive perturbations may destabilize training and compromise
agent performance, while insufficient perturbations may not impart enough
information to enhance robustness. To keep the training stable while improving
robustness, we propose a simple but effective method, namely, Adaptive
Adversarial Perturbation (A2P), which can dynamically select appropriate
adversarial perturbations for each sample. Specifically, we propose an adaptive
adversarial coefficient framework to adjust the effect of the adversarial
perturbation during training. By designing a metric for the current intensity
of the perturbation, our method can calculate the suitable perturbation levels
based on the current relative performance. The appealing feature of our method
is that it is simple to deploy in real-world applications and does not require
accessing the simulator in advance. The experiments in MuJoCo show that our
method can improve the training stability and learn a robust policy when
migrated to different test environments. The code is available at
https://github.com/Lqm00/A2P-SAC.

ÊëòË¶ÅÔºöÊ∑±Â∫¶Âº∑ÂåñÂ≠∏Áøí (DRL) ÊºîÁÆóÊ≥ïÂèØËÉΩÊúÉÂèóÂà∞Ê®°Êì¨ËàáÁúüÂØ¶‰∏ñÁïå‰πãÈñìÁöÑÂª∫Ê®°Ë™§Â∑ÆÂΩ±Èüø„ÄÇË®±Â§öÁ†îÁ©∂‰ΩøÁî®Â∞çÊäóÂ≠∏ÁøíÂú®Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠Áî¢ÁîüÊìæÂãïÔºå‰ª•Ê®°Êì¨Â∑ÆÁï∞‰∏¶ÊîπÂñÑ DRL ÁöÑÁ©©ÂÅ•ÊÄß„ÄÇÁÑ∂ËÄåÔºåÈÄô‰∫õÊñπÊ≥ïÂ§ßÂ§ö‰ΩøÁî®Âõ∫ÂÆöÂèÉÊï∏‰æÜÊéßÂà∂Â∞çÊäóÊìæÂãïÁöÑÂº∑Â∫¶ÔºåÈÄôÂèØËÉΩÂ∞éËá¥Âπ≥ÂùáÊïàËÉΩÂíåÁ©©ÂÅ•ÊÄß‰πãÈñìÁöÑÂèñÊç®„ÄÇ‰∫ãÂØ¶‰∏äÔºåÊâæÂà∞ÊìæÂãïÁöÑÊúÄ‰Ω≥ÂèÉÊï∏ÂÖ∑ÊúâÊåëÊà∞ÊÄßÔºåÂõ†ÁÇ∫ÈÅéÂ∫¶ÁöÑÊìæÂãïÂèØËÉΩÊúÉÁ†¥Â£ûË®ìÁ∑¥‰∏¶ÊêçÂÆ≥‰ª£ÁêÜÊïàËÉΩÔºåËÄå‰∏çË∂≥ÁöÑÊìæÂãïÂèØËÉΩÁÑ°Ê≥ïÊèê‰æõË∂≥Â§†ÁöÑË≥áË®ä‰æÜÂ¢ûÂº∑Á©©ÂÅ•ÊÄß„ÄÇÁÇ∫‰∫ÜÂú®ÊîπÂñÑÁ©©ÂÅ•ÊÄßÁöÑÂêåÊôÇ‰øùÊåÅË®ìÁ∑¥Á©©ÂÆöÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰ΩÜÊúâÊïàÁöÑÊñπÊ≥ïÔºåÂç≥Ëá™ÈÅ©ÊáâÂ∞çÊäóÊìæÂãï (A2P)ÔºåÂÆÉÂèØ‰ª•ÂãïÊÖãÂú∞ÁÇ∫ÊØèÂÄãÊ®£Êú¨ÈÅ∏ÊìáÈÅ©Áï∂ÁöÑÂ∞çÊäóÊìæÂãï„ÄÇÂÖ∑È´î‰æÜË™™ÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÂÄãËá™ÈÅ©ÊáâÂ∞çÊäó‰øÇÊï∏Ê°ÜÊû∂Ôºå‰ª•Ë™øÊï¥Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠Â∞çÊäóÊìæÂãïÁöÑÂΩ±Èüø„ÄÇÈÄèÈÅéË®≠Ë®à‰∏ÄÂÄãÁî®ÊñºÊ∏¨ÈáèÊìæÂãïÁï∂ÂâçÂº∑Â∫¶ÁöÑÊåáÊ®ôÔºåÊàëÂÄëÁöÑÊ®°ÂûãÂèØ‰ª•Ê†πÊìöÁï∂ÂâçÁöÑÁõ∏Â∞çÊïàËÉΩË®àÁÆóÈÅ©Áï∂ÁöÑÊìæÂãïÁ¥öÂà•„ÄÇÊàëÂÄëÁöÑÊñπÊ≥ïÂÖ∑ÊúâÂê∏ÂºïÂäõÁöÑÁâπÈªûÊòØÔºåÂÆÉÊòìÊñºÈÉ®ÁΩ≤Âú®ÂØ¶ÈöõÊáâÁî®‰∏≠Ôºå‰∏¶‰∏î‰∏çÈúÄË¶Å‰∫ãÂÖàÂ≠òÂèñÊ®°Êì¨Âô®„ÄÇMuJoCo ‰∏≠ÁöÑÂØ¶È©óË°®ÊòéÔºåÁï∂ÊàëÂÄëÁöÑÊ®°ÂûãËΩâÁßªÂà∞‰∏çÂêåÁöÑÊ∏¨Ë©¶Áí∞Â¢ÉÊôÇÔºåÂÆÉÂèØ‰ª•ÊîπÂñÑË®ìÁ∑¥Á©©ÂÆöÊÄß‰∏¶Â≠∏ÁøíÁ©©ÂÅ•ÁöÑÊîøÁ≠ñ„ÄÇÁ®ãÂºèÁ¢ºÂèØÂú® https://github.com/Lqm00/A2P-SAC ÂèñÂæó„ÄÇ

##### **SM-DTW: Stability Modulated Dynamic Time Warping for signature verification**
2405.11978v1 by Antonio Parziale, Moises Diaz, Miguel A. Ferrer, Angelo Marcelli

Building upon findings in computational model of handwriting learning and
execution, we introduce the concept of stability to explain the difference
between the actual movements performed during multiple execution of the
subject's signature, and conjecture that the most stable parts of the signature
should play a paramount role in evaluating the similarity between a questioned
signature and the reference ones during signature verification. We then
introduce the Stability Modulated Dynamic Time Warping algorithm for
incorporating the stability regions, i.e. the most similar parts between two
signatures, into the distance measure between a pair of signatures computed by
the Dynamic Time Warping for signature verification. Experiments were conducted
on two datasets largely adopted for performance evaluation. Experimental
results show that the proposed algorithm improves the performance of the
baseline system and compares favourably with other top performing signature
verification systems.

ÊëòË¶ÅÔºöÂª∫Á´ãÂú®ÊâãÂØ´Â≠∏ÁøíÂíåÂü∑Ë°åÈÅãÁÆóÊ®°ÂûãÁöÑÁôºÁèæ‰∏äÔºåÊàëÂÄëÂºïÂÖ•Á©©ÂÆöÊÄßÁöÑÊ¶ÇÂøµ‰æÜËß£ÈáãÂú®‰∏ªÈ´îÁ∞ΩÂêçÂ§öÊ¨°Âü∑Ë°åÊúüÈñìÂØ¶ÈöõÂü∑Ë°åÁöÑÂãï‰Ωú‰πãÈñìÁöÑÂ∑ÆÁï∞Ôºå‰∏¶Êé®Ê∏¨Á∞ΩÂêç‰∏≠ÊúÄÁ©©ÂÆöÁöÑÈÉ®ÂàÜÊáâÂú®Á∞ΩÂêçÈ©óË≠âÊúüÈñìË©ï‰º∞ÊúâÁñëÂïèÁöÑÁ∞ΩÂêçËàáÂèÉËÄÉÁ∞ΩÂêç‰πãÈñìÁöÑÁõ∏‰ººÊÄßÊôÇÁôºÊèÆËá≥ÈóúÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇÁÑ∂ÂæåÔºåÊàëÂÄëÂºïÂÖ•Á©©ÂÆöÊÄßË™øË£ΩÂãïÊÖãÊôÇÈñìË¶èÊï¥ÊºîÁÆóÊ≥ïÔºå‰ª•Â∞áÁ©©ÂÆöÂçÄÂüüÔºàÂç≥ÂÖ©ÂÄãÁ∞ΩÂêç‰πãÈñìÊúÄÁõ∏‰ººÁöÑÈÉ®ÂàÜÔºâÁ¥çÂÖ•Áî±ÂãïÊÖãÊôÇÈñìË¶èÊï¥ÊºîÁÆóÊ≥ïË®àÁÆóÂá∫ÁöÑÁ∞ΩÂêçÂ∞ç‰πãÈñìÁöÑË∑ùÈõ¢Ê∏¨Èáè‰∏≠Ôºå‰ª•ÈÄ≤Ë°åÁ∞ΩÂêçÈ©óË≠â„ÄÇÂú®ÂÖ©ÂÄãÂª£Ê≥õÊé°Áî®ÊñºÊïàËÉΩË©ï‰º∞ÁöÑË≥áÊñôÈõÜ‰∏äÈÄ≤Ë°å‰∫ÜÂØ¶È©ó„ÄÇÂØ¶È©óÁµêÊûúÈ°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÊºîÁÆóÊ≥ïÊîπÂñÑ‰∫ÜÂü∫Ê∫ñÁ≥ªÁµ±ÁöÑÊïàËÉΩÔºå‰∏¶‰∏îËàáÂÖ∂‰ªñÈ†ÇÂ∞ñÁöÑÁ∞ΩÂêçÈ©óË≠âÁ≥ªÁµ±Áõ∏ÊØî‰πã‰∏ãË°®ÁèæËâØÂ•Ω„ÄÇ

##### **Conditional Shift-Robust Conformal Prediction for Graph Neural Network**
2405.11968v1 by S. Akansha

Graph Neural Networks (GNNs) have emerged as potent tools for predicting
outcomes in graph-structured data. Despite their efficacy, a significant
drawback of GNNs lies in their limited ability to provide robust uncertainty
estimates, posing challenges to their reliability in contexts where errors
carry significant consequences. Moreover, GNNs typically excel in
in-distribution settings, assuming that training and test data follow identical
distributions: a condition often unmet in real-world graph data scenarios. In
this article, we leverage conformal prediction, a widely recognized statistical
technique for quantifying uncertainty by transforming predictive model outputs
into prediction sets, to address uncertainty quantification in GNN predictions
amidst conditional shift \footnote{Representing the change in conditional
probability distribution $P(label |input)$ from source domain to target
domain.} in graph-based semi-supervised learning (SSL). Additionally, we
propose a novel loss function aimed at refining model predictions by minimizing
conditional shift in latent stages. Termed Conditional Shift Robust (CondSR)
conformal prediction for GNNs, our approach CondSR is model-agnostic and
adaptable to various classification models. We validate the effectiveness of
our method on standard graph benchmark datasets, integrating it with
state-of-the-art GNNs in node classification tasks. The code implementation is
publicly available for further exploration and experimentation.

ÊëòË¶ÅÔºöÂúñÂΩ¢Á•ûÁ∂ìÁ∂≤Ë∑Ø (GNN) Â∑≤ÊàêÁÇ∫È†êÊ∏¨ÂúñÂΩ¢ÁµêÊßãË≥áÊñô‰∏≠ÁµêÊûúÁöÑÂº∑Â§ßÂ∑•ÂÖ∑„ÄÇÂÑòÁÆ°ÂÖ∂ÊúâÊïàÔºå‰ΩÜ GNN ÁöÑ‰∏ÄÂÄãÈáçÂ§ßÁº∫ÈªûÂú®ÊñºÂÖ∂Êèê‰æõÁ©©ÂÅ•‰∏çÁ¢∫ÂÆöÊÄß‰º∞Ë®àÁöÑËÉΩÂäõÊúâÈôêÔºåÈÄôÂ∞çÂÖ∂Âú®ÈåØË™§ÊúÉÈÄ†ÊàêÈáçÂ§ßÂæåÊûúÁöÑÁí∞Â¢É‰∏≠ÁöÑÂèØÈù†ÊÄßÊßãÊàêÊåëÊà∞„ÄÇÊ≠§Â§ñÔºåGNN ÈÄöÂ∏∏Âú®ÂêåÂàÜ‰ΩàË®≠ÂÆö‰∏≠Ë°®ÁèæÂá∫Ëâ≤ÔºåÂÅáË®≠Ë®ìÁ∑¥ÂíåÊ∏¨Ë©¶Ë≥áÊñôÈÅµÂæ™Áõ∏ÂêåÁöÑÂàÜÈÖçÔºöÈÄôÊòØ‰∏ÄÂÄãÂú®ÁèæÂØ¶‰∏ñÁïåÂúñÂΩ¢Ë≥áÊñôÂ†¥ÊôØ‰∏≠Á∂ìÂ∏∏ÁÑ°Ê≥ïÊªøË∂≥ÁöÑÊ¢ù‰ª∂„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàëÂÄëÂà©Áî®ÂÖ±ÂΩ¢È†êÊ∏¨ÔºåÈÄôÊòØ‰∏ÄÁ®ÆÂª£Ê≥õË™çÂèØÁöÑÁµ±Ë®àÊäÄË°ìÔºåÈÄèÈÅéÂ∞áÈ†êÊ∏¨Ê®°ÂûãËº∏Âá∫ËΩâÊèõÁÇ∫È†êÊ∏¨ÈõÜÂêà‰æÜÈáèÂåñ‰∏çÁ¢∫ÂÆöÊÄßÔºå‰ª•Ëß£Ê±∫Âú®ÂúñÂΩ¢ÂçäÁõ£Áù£Â≠∏Áøí (SSL) ‰∏≠Ê¢ù‰ª∂ËΩâÁßª‰∏ã GNN È†êÊ∏¨‰∏≠ÁöÑ‰∏çÁ¢∫ÂÆöÊÄßÈáèÂåñÂïèÈ°å\footnote{Ë°®Á§∫Ê¢ù‰ª∂Ê©üÁéáÂàÜ‰Ωà $P(label |input)$ Âæû‰æÜÊ∫êÁ∂≤ÂüüÂà∞ÁõÆÊ®ôÁ∂≤ÂüüÁöÑËÆäÂåñ„ÄÇ}„ÄÇÊ≠§Â§ñÔºåÊàëÂÄëÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÊñ∞ÁöÑÊêçÂ§±ÂáΩÊï∏ÔºåÊó®Âú®ÈÄèÈÅéÊúÄÂ∞èÂåñÊΩõÂú®ÈöéÊÆµ‰∏≠ÁöÑÊ¢ù‰ª∂ËΩâÁßª‰æÜÊîπÂñÑÊ®°ÂûãÈ†êÊ∏¨„ÄÇÊàëÂÄëÁöÑ CondSR ÊñπÊ≥ïÁ®±ÁÇ∫Ê¢ù‰ª∂ËΩâÁßªÁ©©ÂÅ• (CondSR) ÂÖ±ÂΩ¢È†êÊ∏¨ÔºåÈÅ©Áî®Êñº GNNÔºåÂÆÉËàáÊ®°ÂûãÁÑ°ÈóúÔºå‰∏îÈÅ©Áî®ÊñºÂêÑÁ®ÆÂàÜÈ°ûÊ®°Âûã„ÄÇÊàëÂÄëÂú®Ê®ôÊ∫ñÂúñÂΩ¢Âü∫Ê∫ñË≥áÊñôÈõÜ‰∏äÈ©óË≠â‰∫ÜÊàëÂÄëÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºå‰∏¶Â∞áÂÖ∂ËàáÁØÄÈªûÂàÜÈ°û‰ªªÂãô‰∏≠ÁöÑÊúÄÂÖàÈÄ≤ GNN Êï¥Âêà„ÄÇÁ®ãÂºèÁ¢ºÂØ¶‰ΩúÂ∑≤ÂÖ¨ÈñãÔºåÂèØ‰æõÈÄ≤‰∏ÄÊ≠•Êé¢Á¥¢ÂíåÂØ¶È©ó„ÄÇ

##### **Recommender Algorithm for Supporting Self-Management of CVD Risk Factors in an Adult Population at Home**
2405.11967v1 by Tatiana V. Afanasieva, Pavel V. Platov, Anastasia I. Medvedeva

One of the new trends in the development of recommendation algorithms is the
dissemination of their capabilities to support the population in managing their
health. This article focuses on the problem of improving the effectiveness of
cardiovascular diseases (CVD) prevention, since CVD is the leading cause of
death worldwide. To address this issue, a knowledge-based recommendation
algorithm was proposed to support self-management of CVD risk factors in adults
at home. The proposed algorithm is based on the original multidimensional
recommendation model and on a new user profile model, which includes predictive
assessments of CVD health in addition to its current ones as outlined in
official guidelines. The main feature of the proposed algorithm is the
combination of rule-based logic with the capabilities of a large language model
in generating human-like text for explanatory component of multidimensional
recommendation. The verification and evaluation of the proposed algorithm
showed the usefulness of the proposed recommendation algorithm for supporting
adults in self-management of their CVD risk factors at home. As follows from
the comparison with similar knowledge-based recommendation algorithms, the
proposed algorithm evaluates a larger number of CVD risk factors and has a
greater information and semantic capacity of the generated recommendations.

ÊëòË¶ÅÔºöÊé®Ëñ¶ÊºîÁÆóÊ≥ïÁôºÂ±ïÁöÑÊñ∞Ë∂®Âã¢‰πã‰∏ÄÊòØÂÇ≥Êí≠ÂÖ∂ËÉΩÂäõÔºå‰ª•ÂçîÂä©Ê∞ëÁúæÁÆ°ÁêÜËá™Ë∫´ÂÅ•Â∫∑„ÄÇÊú¨ÊñáÈáçÈªûÊé¢Ë®éÊîπÂñÑÂøÉË°ÄÁÆ°ÁñæÁóÖÔºàCVDÔºâÈ†êÈò≤ÁöÑÊúâÊïàÊÄßÔºåÂõ†ÁÇ∫ CVD ÊòØÂÖ®ÁêÉ‰∏ªË¶ÅÁöÑÊ≠ª‰∫°ÂéüÂõ†„ÄÇÁÇ∫‰∫ÜËß£Ê±∫ÈÄôÂÄãÂïèÈ°åÔºåÊèêÂá∫‰∫Ü‰∏ÄÁ®ÆÂü∫ÊñºÁü•Ë≠òÁöÑÊé®Ëñ¶ÊºîÁÆóÊ≥ïÔºå‰ª•Âú®ÂÆ∂‰∏≠ÊîØÊè¥Êàê‰∫∫Ëá™ÊàëÁÆ°ÁêÜ CVD È¢®Èö™Âõ†Â≠ê„ÄÇÊâÄÊèêÂá∫ÁöÑÊºîÁÆóÊ≥ïÂü∫ÊñºÂéüÂßãÁöÑÂ§öÁ∂≠Â∫¶Êé®Ëñ¶Ê®°ÂûãÂíåÊñ∞ÁöÑ‰ΩøÁî®ËÄÖËº™ÂªìÊ®°ÂûãÔºåÂÖ∂‰∏≠Èô§‰∫ÜÂÆòÊñπÊåáÂçó‰∏≠Ê¶ÇËø∞ÁöÑÁèæÊúâË©ï‰º∞Â§ñÔºåÈÇÑÂåÖÊã¨ CVD ÂÅ•Â∫∑ÁöÑÈ†êÊ∏¨Ë©ï‰º∞„ÄÇÊâÄÊèêÂá∫ÁöÑÊºîÁÆóÊ≥ï‰∏ªË¶ÅÁâπËâ≤ÊòØÂ∞áÂü∫ÊñºË¶èÂâáÁöÑÈÇèËºØËàáÂ§ßÂûãË™ûË®ÄÊ®°ÂûãÁöÑËÉΩÂäõÁõ∏ÁµêÂêàÔºå‰ª•Áî¢ÁîüÈ°û‰∫∫ÊñáÂ≠óÔºå‰ΩúÁÇ∫Â§öÁ∂≠Â∫¶Êé®Ëñ¶ÁöÑË™™ÊòéÊÄßÁµÑÊàêÈÉ®ÂàÜ„ÄÇÊâÄÊèêÂá∫ÁöÑÊºîÁÆóÊ≥ïÁöÑÈ©óË≠âÂíåË©ï‰º∞È°ØÁ§∫ÔºåÊâÄÊèêÂá∫ÁöÑÊé®Ëñ¶ÊºîÁÆóÊ≥ïÂú®ÂçîÂä©Êàê‰∫∫Âú®ÂÆ∂‰∏≠Ëá™ÊàëÁÆ°ÁêÜÂÖ∂ CVD È¢®Èö™Âõ†Â≠êÊñπÈù¢ÂæàÊúâÁî®„ÄÇÂæûËàáÈ°û‰ººÁöÑÂü∫ÊñºÁü•Ë≠òÁöÑÊé®Ëñ¶ÊºîÁÆóÊ≥ïÁöÑÊØîËºÉ‰∏≠ÂæóÁü•ÔºåÊâÄÊèêÂá∫ÁöÑÊºîÁÆóÊ≥ïË©ï‰º∞‰∫ÜÊõ¥Â§ö CVD È¢®Èö™Âõ†Â≠êÔºå‰∏¶‰∏îÁî¢ÁîüÁöÑÂª∫Ë≠∞ÂÖ∑ÊúâÊõ¥Â§ßÁöÑË≥áË®äÂíåË™ûÁæ©ÂÆπÈáè„ÄÇ

