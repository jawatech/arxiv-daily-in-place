{"2406.05967": {"publish_time": "2024-06-10", "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark", "paper_summary": "Visual Question Answering (VQA) is an important task in multimodal AI, and it\nis often used to test the ability of vision-language models to understand and\nreason on knowledge present in both visual and textual data. However, most of\nthe current VQA models use datasets that are primarily focused on English and a\nfew major world languages, with images that are typically Western-centric.\nWhile recent efforts have tried to increase the number of languages covered on\nVQA datasets, they still lack diversity in low-resource languages. More\nimportantly, although these datasets often extend their linguistic range via\ntranslation or some other approaches, they usually keep images the same,\nresulting in narrow cultural representation. To address these limitations, we\nconstruct CVQA, a new Culturally-diverse multilingual Visual Question Answering\nbenchmark, designed to cover a rich set of languages and cultures, where we\nengage native speakers and cultural experts in the data collection process. As\na result, CVQA includes culturally-driven images and questions from across 28\ncountries on four continents, covering 26 languages with 11 scripts, providing\na total of 9k questions. We then benchmark several Multimodal Large Language\nModels (MLLMs) on CVQA, and show that the dataset is challenging for the\ncurrent state-of-the-art models. This benchmark can serve as a probing\nevaluation suite for assessing the cultural capability and bias of multimodal\nmodels and hopefully encourage more research efforts toward increasing cultural\nawareness and linguistic diversity in this field.", "paper_summary_zh": "\u8996\u89ba\u554f\u7b54 (VQA) \u662f\u591a\u6a21\u614b AI \u4e2d\u7684\u4e00\u9805\u91cd\u8981\u4efb\u52d9\uff0c\u4e14\u5e38\u88ab\u7528\u65bc\u6e2c\u8a66\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u63a8\u7406\u8996\u89ba\u548c\u6587\u5b57\u8cc7\u6599\u4e2d\u77e5\u8b58\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u5927\u90e8\u5206\u7684 VQA \u6a21\u578b\u4f7f\u7528\u7684\u8cc7\u6599\u96c6\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8a9e\u548c\u4e00\u4e9b\u4e3b\u8981\u7684\u4e16\u754c\u8a9e\u8a00\uff0c\u4e14\u4f7f\u7528\u7684\u5716\u50cf\u901a\u5e38\u4ee5\u897f\u65b9\u70ba\u4e2d\u5fc3\u3002\u5118\u7ba1\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u5617\u8a66\u589e\u52a0 VQA \u8cc7\u6599\u96c6\u4e2d\u6db5\u84cb\u7684\u8a9e\u8a00\u6578\u91cf\uff0c\u4f46\u5728\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u4e2d\u4ecd\u7136\u7f3a\u4e4f\u591a\u6a23\u6027\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5118\u7ba1\u9019\u4e9b\u8cc7\u6599\u96c6\u7d93\u5e38\u900f\u904e\u7ffb\u8b6f\u6216\u5176\u4ed6\u65b9\u6cd5\u4f86\u64f4\u5c55\u5176\u8a9e\u8a00\u7bc4\u570d\uff0c\u4f46\u5b83\u5011\u901a\u5e38\u6703\u4fdd\u7559\u76f8\u540c\u7684\u5716\u50cf\uff0c\u5c0e\u81f4\u6587\u5316\u5448\u73fe\u72f9\u9698\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5efa\u69cb\u4e86 CVQA\uff0c\u4e00\u500b\u65b0\u7684\u6587\u5316\u591a\u5143\u591a\u8a9e\u8a00\u8996\u89ba\u554f\u7b54\u57fa\u6e96\uff0c\u65e8\u5728\u6db5\u84cb\u8c50\u5bcc\u7684\u8a9e\u8a00\u548c\u6587\u5316\uff0c\u5728\u8cc7\u6599\u6536\u96c6\u904e\u7a0b\u4e2d\uff0c\u6211\u5011\u8058\u8acb\u4e86\u6bcd\u8a9e\u4eba\u58eb\u548c\u6587\u5316\u5c08\u5bb6\u3002\u56e0\u6b64\uff0cCVQA \u5305\u542b\u4f86\u81ea\u56db\u5927\u6d32 28 \u500b\u570b\u5bb6\u7684\u6587\u5316\u9a45\u52d5\u5716\u50cf\u548c\u554f\u984c\uff0c\u6db5\u84cb 26 \u7a2e\u8a9e\u8a00\u548c 11 \u7a2e\u6587\u5b57\uff0c\u7e3d\u5171\u63d0\u4f9b\u4e86 9k \u500b\u554f\u984c\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5728 CVQA \u4e0a\u5c0d\u5e7e\u500b\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u4e26\u986f\u793a\u8a72\u8cc7\u6599\u96c6\u5c0d\u76ee\u524d\u7684\u6700\u65b0\u6a21\u578b\u4f86\u8aaa\u5177\u6709\u6311\u6230\u6027\u3002\u6b64\u57fa\u6e96\u6e2c\u8a66\u53ef\u7528\u4f5c\u63a2\u6e2c\u8a55\u4f30\u5957\u4ef6\uff0c\u7528\u65bc\u8a55\u4f30\u591a\u6a21\u614b\u6a21\u578b\u7684\u6587\u5316\u80fd\u529b\u548c\u504f\u898b\uff0c\u4e26\u6709\u671b\u9f13\u52f5\u66f4\u591a\u7814\u7a76\u5de5\u4f5c\uff0c\u4ee5\u63d0\u9ad8\u8a72\u9818\u57df\u7684\u6587\u5316\u610f\u8b58\u548c\u8a9e\u8a00\u591a\u6a23\u6027\u3002", "author": "David Romero et.al.", "authors": "David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hern\u00e1n Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay Gala, Jiahui Geng, Jesus-German Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando D'Haro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodr\u00edguez-Cantelar, M\u00e9lanie Jouitteau, Mihail Mihaylov, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome Etori, Olivier Niyomugisha, Paula M\u00f3nica Silva, Pranjal Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago G\u00f3ngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, Alham Fikri Aji", "id": "2406.05967v1", "paper_url": "http://arxiv.org/abs/2406.05967v1", "repo": "null"}}