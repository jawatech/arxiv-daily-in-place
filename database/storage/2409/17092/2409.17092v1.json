{"2409.17092": {"publish_time": "2024-09-25", "title": "Accumulator-Aware Post-Training Quantization", "paper_summary": "Several recent studies have investigated low-precision accumulation,\nreporting improvements in throughput, power, and area across various platforms.\nHowever, the accompanying proposals have only considered the quantization-aware\ntraining (QAT) paradigm, in which models are fine-tuned or trained from scratch\nwith quantization in the loop. As models continue to grow in size, QAT\ntechniques become increasingly more expensive, which has motivated the recent\nsurge in post-training quantization (PTQ) research. To the best of our\nknowledge, ours marks the first formal study of accumulator-aware quantization\nin the PTQ setting. To bridge this gap, we introduce AXE, a practical framework\nof accumulator-aware extensions designed to endow overflow avoidance guarantees\nto existing layer-wise PTQ algorithms. We theoretically motivate AXE and\ndemonstrate its flexibility by implementing it on top of two state-of-the-art\nPTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage\naccumulation for the first time, opening the door for full datapath\noptimization and scaling to large language models (LLMs). We evaluate AXE\nacross image classification and language generation models, and observe\nsignificant improvements in the trade-off between accumulator bit width and\nmodel accuracy over baseline methods.", "paper_summary_zh": "\u6700\u8fd1\u6709\u8bb8\u591a\u7814\u7a76\u8c03\u67e5\u4e86\u4f4e\u7cbe\u5ea6\u7d2f\u52a0\uff0c\u62a5\u544a\u4e86\u5728\u5404\u79cd\u5e73\u53f0\u4e0a\u541e\u5410\u91cf\u3001\u529f\u8017\u548c\u9762\u79ef\u7684\u6539\u8fdb\u3002\u7136\u800c\uff0c\u968f\u9644\u7684\u63d0\u6848\u53ea\u8003\u8651\u4e86\u91cf\u5316\u611f\u77e5\u8bad\u7ec3 (QAT) \u8303\u4f8b\uff0c\u5176\u4e2d\u6a21\u578b\u7ecf\u8fc7\u5fae\u8c03\u6216\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u5e76\u4e14\u5728\u5faa\u73af\u4e2d\u8fdb\u884c\u91cf\u5316\u3002\u968f\u7740\u6a21\u578b\u5c3a\u5bf8\u6301\u7eed\u589e\u957f\uff0cQAT \u6280\u672f\u53d8\u5f97\u8d8a\u6765\u8d8a\u6602\u8d35\uff0c\u8fd9\u4fc3\u4f7f\u8bad\u7ec3\u540e\u91cf\u5316 (PTQ) \u7814\u7a76\u6700\u8fd1\u6fc0\u589e\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u7684\u7814\u7a76\u6807\u5fd7\u7740\u5728 PTQ \u8bbe\u7f6e\u4e2d\u9996\u6b21\u6b63\u5f0f\u7814\u7a76\u4e86\u7d2f\u52a0\u5668\u611f\u77e5\u91cf\u5316\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 AXE\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u7d2f\u52a0\u5668\u611f\u77e5\u6269\u5c55\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u73b0\u6709\u7684\u9010\u5c42 PTQ \u7b97\u6cd5\u63d0\u4f9b\u6ea2\u51fa\u907f\u514d\u4fdd\u8bc1\u3002\u6211\u4eec\u5728\u7406\u8bba\u4e0a\u6fc0\u53d1\u4e86 AXE\uff0c\u5e76\u901a\u8fc7\u5728\u4e24\u79cd\u6700\u5148\u8fdb\u7684 PTQ \u7b97\u6cd5 GPFQ \u548c OPTQ \u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u5b83\u6765\u8bc1\u660e\u5176\u7075\u6d3b\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06 AXE \u6982\u62ec\u4e3a\u9996\u6b21\u652f\u6301\u591a\u7ea7\u7d2f\u52a0\uff0c\u4e3a\u5b8c\u5168\u6570\u636e\u8def\u5f84\u4f18\u5316\u548c\u6269\u5c55\u5230\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u6253\u5f00\u4e86\u5927\u95e8\u3002\u6211\u4eec\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u8a00\u751f\u6210\u6a21\u578b\u4e2d\u8bc4\u4f30\u4e86 AXE\uff0c\u5e76\u89c2\u5bdf\u5230\u5728\u7d2f\u52a0\u5668\u4f4d\u5bbd\u548c\u6a21\u578b\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u65b9\u9762\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u7684\u6539\u8fdb\u3002", "author": "Ian Colbert et.al.", "authors": "Ian Colbert, Fabian Grob, Giuseppe Franco, Jinjie Zhang, Rayan Saab", "id": "2409.17092v1", "paper_url": "http://arxiv.org/abs/2409.17092v1", "repo": "null"}}