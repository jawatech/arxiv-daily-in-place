{"2408.03506": {"publish_time": "2024-08-07", "title": "1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data", "paper_summary": "This paper presents a compute-efficient approach to pre-training a Language\nModel-the \"1.5-Pints\"-in only 9 days, while outperforming state-of-the-art\nmodels as an instruction-following assistant.Based on MT-Bench (a benchmark\nthat emulates human judgments), 1.5-Pints outperforms Apple's OpenELM and\nMicrosoft's Phi.This is achieved by a carefully curated pre-training dataset of\n57 billion tokens, using a mix of automated workflows and manual human review.\nThe selection of the dataset prioritizes content that is considered expository\nand \"textbook-like\" to aid the model in reasoning and logical deduction,\nculminating in its overall ability as a strong and versatile AI model. In terms\nof the model architecture, we employed a modified Mistral tokenizer, alongside\na Llama-2 architecture for wider compatibility. For training, we adopted the\nmethodologies used by StableLM, TinyLlama, and Huggingface Zephyr. 1.5-Pints\ndemonstrates that by focusing on data quality over quantity in LLM training, we\ncan significantly reduce training time and resources required. We believe this\napproach will not only make pre-training more accessible but also reduce our\ncarbon footprint. Our findings and resources from this research are\nopen-sourced, aiming to facilitate further advancements in the field. The\n1.5-Pints model is available in two versions: 2K and 16K context windows.", "paper_summary_zh": "<paragraph>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u8a08\u7b97\u6548\u7387\u9ad8\u7684\u8a13\u7df4\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u77ed\u77ed 9 \u5929\u5167\u9810\u8a13\u7df4\u4e00\u500b\u8a9e\u8a00\u6a21\u578b\u300c1.5-Pints\u300d\uff0c\u540c\u6642\u5728\u4f5c\u70ba\u6307\u4ee4\u8ffd\u8e64\u52a9\u7406\u7684\u8868\u73fe\u4e0a\u512a\u65bc\u6700\u5148\u9032\u7684\u6a21\u578b\u3002\u6839\u64da MT-Bench\uff08\u4e00\u500b\u6a21\u64ec\u4eba\u985e\u5224\u65b7\u7684\u57fa\u6e96\u6e2c\u8a66\uff09\uff0c1.5-Pints \u512a\u65bc Apple \u7684 OpenELM \u548c Microsoft \u7684 Phi\u3002\u9019\u662f\u900f\u904e\u4e00\u500b\u7d93\u904e\u4ed4\u7d30\u7b56\u5283\u7684 570 \u5104\u500b\u7b26\u865f\u7684\u9810\u8a13\u7df4\u8cc7\u6599\u96c6\u5be6\u73fe\u7684\uff0c\u7d50\u5408\u4e86\u81ea\u52d5\u5316\u5de5\u4f5c\u6d41\u7a0b\u548c\u4eba\u5de5\u5be9\u67e5\u3002\u8cc7\u6599\u96c6\u7684\u9078\u64c7\u512a\u5148\u8003\u616e\u88ab\u8996\u70ba\u8aaa\u660e\u6027\u548c\u300c\u6559\u79d1\u66f8\u5f0f\u300d\u7684\u5167\u5bb9\uff0c\u4ee5\u5e6b\u52a9\u6a21\u578b\u9032\u884c\u63a8\u7406\u548c\u908f\u8f2f\u6f14\u7e79\uff0c\u6700\u7d42\u5f62\u6210\u5176\u4f5c\u70ba\u4e00\u500b\u5f37\u5927\u4e14\u7528\u9014\u5ee3\u6cdb\u7684 AI \u6a21\u578b\u7684\u6574\u9ad4\u80fd\u529b\u3002\u5728\u6a21\u578b\u67b6\u69cb\u65b9\u9762\uff0c\u6211\u5011\u63a1\u7528\u4e86\u4e00\u500b\u4fee\u6539\u904e\u7684 Mistral \u5206\u8a5e\u5668\uff0c\u4ee5\u53ca\u4e00\u500b Llama-2 \u67b6\u69cb\u4ee5\u5be6\u73fe\u66f4\u5ee3\u6cdb\u7684\u76f8\u5bb9\u6027\u3002\u5728\u8a13\u7df4\u65b9\u9762\uff0c\u6211\u5011\u63a1\u7528\u4e86 StableLM\u3001TinyLlama \u548c Huggingface Zephyr \u6240\u4f7f\u7528\u7684\u8a13\u7df4\u65b9\u6cd5\u30021.5-Pints \u8b49\u660e\u4e86\u900f\u904e\u5728 LLM \u8a13\u7df4\u4e2d\u91cd\u8996\u8cc7\u6599\u54c1\u8cea\u800c\u975e\u6578\u91cf\uff0c\u6211\u5011\u53ef\u4ee5\u5927\u5e45\u6e1b\u5c11\u8a13\u7df4\u6642\u9593\u548c\u6240\u9700\u7684\u8cc7\u6e90\u3002\u6211\u5011\u76f8\u4fe1\u9019\u7a2e\u65b9\u6cd5\u4e0d\u50c5\u6703\u8b93\u9810\u8a13\u7df4\u66f4\u5bb9\u6613\u53d6\u5f97\uff0c\u9084\u80fd\u6e1b\u5c11\u6211\u5011\u7684\u78b3\u8db3\u8de1\u3002\u6211\u5011\u5f9e\u9019\u9805\u7814\u7a76\u4e2d\u7372\u5f97\u7684\u767c\u73fe\u548c\u8cc7\u6e90\u90fd\u662f\u958b\u6e90\u7684\uff0c\u76ee\u7684\u662f\u4fc3\u9032\u8a72\u9818\u57df\u7684\u9032\u4e00\u6b65\u767c\u5c55\u30021.5-Pints \u6a21\u578b\u63d0\u4f9b\u5169\u500b\u7248\u672c\uff1a2K \u548c 16K \u7684\u4e0a\u4e0b\u6587\u8996\u7a97\u3002</paragraph>", "author": "Calvin Tan et.al.", "authors": "Calvin Tan, Jerome Wang", "id": "2408.03506v1", "paper_url": "http://arxiv.org/abs/2408.03506v1", "repo": "https://github.com/Pints-AI/1.5-Pints"}}