{"2501.17260": {"publish_time": "2025-01-28", "title": "ViT-2SPN: Vision Transformer-based Dual-Stream Self-Supervised Pretraining Networks for Retinal OCT Classification", "paper_summary": "Optical Coherence Tomography (OCT) is a non-invasive imaging modality\nessential for diagnosing various eye diseases. Despite its clinical\nsignificance, developing OCT-based diagnostic tools faces challenges, such as\nlimited public datasets, sparse annotations, and privacy concerns. Although\ndeep learning has made progress in automating OCT analysis, these challenges\nremain unresolved. To address these limitations, we introduce the Vision\nTransformer-based Dual-Stream Self-Supervised Pretraining Network (ViT-2SPN), a\nnovel framework designed to enhance feature extraction and improve diagnostic\naccuracy. ViT-2SPN employs a three-stage workflow: Supervised Pretraining,\nSelf-Supervised Pretraining (SSP), and Supervised Fine-Tuning. The pretraining\nphase leverages the OCTMNIST dataset (97,477 unlabeled images across four\ndisease classes) with data augmentation to create dual-augmented views. A\nVision Transformer (ViT-Base) backbone extracts features, while a negative\ncosine similarity loss aligns feature representations. Pretraining is conducted\nover 50 epochs with a learning rate of 0.0001 and momentum of 0.999.\nFine-tuning is performed on a stratified 5.129% subset of OCTMNIST using\n10-fold cross-validation. ViT-2SPN achieves a mean AUC of 0.93, accuracy of\n0.77, precision of 0.81, recall of 0.75, and an F1 score of 0.76, outperforming\nexisting SSP-based methods.", "paper_summary_zh": "\u5149\u5b78\u76f8\u5e72\u65b7\u5c64\u6383\u63cf\uff08OCT\uff09\u662f\u4e00\u7a2e\u975e\u4fb5\u5165\u5f0f\u5f71\u50cf\u6a21\u5f0f\uff0c\u5c0d\u65bc\u8a3a\u65b7\u5404\u7a2e\u773c\u75be\u81f3\u95dc\u91cd\u8981\u3002\u5118\u7ba1\u5176\u81e8\u5e8a\u610f\u7fa9\u91cd\u5927\uff0c\u4f46\u958b\u767c\u57fa\u65bc OCT \u7684\u8a3a\u65b7\u5de5\u5177\u9762\u81e8\u6311\u6230\uff0c\u4f8b\u5982\u516c\u5171\u6578\u64da\u96c6\u6709\u9650\u3001\u8a3b\u89e3\u7a00\u758f\u548c\u96b1\u79c1\u554f\u984c\u3002\u5118\u7ba1\u6df1\u5ea6\u5b78\u7fd2\u5728\u81ea\u52d5\u5316 OCT \u5206\u6790\u65b9\u9762\u53d6\u5f97\u4e86\u9032\u5c55\uff0c\u4f46\u9019\u4e9b\u6311\u6230\u4ecd\u7136\u6c92\u6709\u89e3\u6c7a\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86\u57fa\u65bc Vision Transformer \u7684\u96d9\u6d41\u81ea\u76e3\u7763\u9810\u8a13\u7df4\u7db2\u8def\uff08ViT-2SPN\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f37\u7279\u5fb5\u63d0\u53d6\u4e26\u63d0\u9ad8\u8a3a\u65b7\u6e96\u78ba\u6027\u3002ViT-2SPN \u63a1\u7528\u4e09\u968e\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff1a\u76e3\u7763\u9810\u8a13\u7df4\u3001\u81ea\u76e3\u7763\u9810\u8a13\u7df4\uff08SSP\uff09\u548c\u76e3\u7763\u5fae\u8abf\u3002\u9810\u8a13\u7df4\u968e\u6bb5\u5229\u7528 OCTMNIST \u6578\u64da\u96c6\uff08\u8de8\u8d8a\u56db\u500b\u75be\u75c5\u985e\u5225\u7684 97,477 \u5f35\u672a\u6a19\u8a18\u5f71\u50cf\uff09\u548c\u6578\u64da\u64f4\u5145\u4f86\u5efa\u7acb\u96d9\u91cd\u64f4\u5145\u7684\u6aa2\u8996\u3002\u8996\u89ba\u8f49\u63db\u5668\uff08ViT-Base\uff09\u4e3b\u5e79\u63d0\u53d6\u7279\u5fb5\uff0c\u800c\u8ca0\u9918\u5f26\u76f8\u4f3c\u5ea6\u640d\u5931\u5247\u6821\u6e96\u7279\u5fb5\u8868\u793a\u3002\u9810\u8a13\u7df4\u5728 50 \u500b\u4e16\u4ee3\u4e2d\u9032\u884c\uff0c\u5b78\u7fd2\u7387\u70ba 0.0001\uff0c\u52d5\u80fd\u70ba 0.999\u3002\u5fae\u8abf\u5728 OCTMNIST \u7684\u5206\u5c64 5.129% \u5b50\u96c6\u4e0a\u57f7\u884c\uff0c\u4f7f\u7528 10 \u500d\u4ea4\u53c9\u9a57\u8b49\u3002ViT-2SPN \u9054\u5230\u4e86 0.93 \u7684\u5e73\u5747 AUC\u30010.77 \u7684\u6e96\u78ba\u7387\u30010.81 \u7684\u7cbe\u78ba\u5ea6\u30010.75 \u7684\u53ec\u56de\u7387\u548c 0.76 \u7684 F1 \u5206\u6578\uff0c\u512a\u65bc\u73fe\u6709\u7684\u57fa\u65bc SSP \u7684\u65b9\u6cd5\u3002", "author": "Mohammadreza Saraei et.al.", "authors": "Mohammadreza Saraei, Igor Kozak, Eung-Joo Lee", "id": "2501.17260v1", "paper_url": "http://arxiv.org/abs/2501.17260v1", "repo": "null"}}