{"2410.03159": {"publish_time": "2024-10-04", "title": "Autoregressive Moving-average Attention Mechanism for Time Series Forecasting", "paper_summary": "We propose an Autoregressive (AR) Moving-average (MA) attention structure\nthat can adapt to various linear attention mechanisms, enhancing their ability\nto capture long-range and local temporal patterns in time series. In this\npaper, we first demonstrate that, for the time series forecasting (TSF) task,\nthe previously overlooked decoder-only autoregressive Transformer model can\nachieve results comparable to the best baselines when appropriate tokenization\nand training methods are applied. Moreover, inspired by the ARMA model from\nstatistics and recent advances in linear attention, we introduce the full ARMA\nstructure into existing autoregressive attention mechanisms. By using an\nindirect MA weight generation method, we incorporate the MA term while\nmaintaining the time complexity and parameter size of the underlying efficient\nattention models. We further explore how indirect parameter generation can\nproduce implicit MA weights that align with the modeling requirements for local\ntemporal impacts. Experimental results show that incorporating the ARMA\nstructure consistently improves the performance of various AR attentions on TSF\ntasks, achieving state-of-the-art results.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e00\u500b\u81ea\u8ff4\u6b78 (AR) \u79fb\u52d5\u5e73\u5747 (MA) \u6ce8\u610f\u529b\u7d50\u69cb\uff0c\u5b83\u53ef\u4ee5\u9069\u61c9\u5404\u7a2e\u7dda\u6027\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u589e\u5f37\u5b83\u5011\u6355\u6349\u6642\u9593\u5e8f\u5217\u4e2d\u9577\u8ddd\u96e2\u548c\u5c40\u90e8\u6642\u9593\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u5148\u8b49\u660e\uff0c\u5c0d\u65bc\u6642\u9593\u5e8f\u5217\u9810\u6e2c (TSF) \u4efb\u52d9\uff0c\u4ee5\u524d\u88ab\u5ffd\u8996\u7684\u50c5\u89e3\u78bc\u5668\u81ea\u8ff4\u6b78 Transformer \u6a21\u578b\u53ef\u4ee5\u5728\u61c9\u7528\u9069\u7576\u7684\u6a19\u8a18\u5316\u548c\u8a13\u7df4\u65b9\u6cd5\u6642\uff0c\u9054\u5230\u8207\u6700\u4f73\u57fa\u7dda\u76f8\u7576\u7684\u7d50\u679c\u3002\u6b64\u5916\uff0c\u53d7\u5230\u7d71\u8a08\u5b78\u4e2d\u7684 ARMA \u6a21\u578b\u548c\u7dda\u6027\u6ce8\u610f\u529b\u7684\u6700\u65b0\u9032\u5c55\u7684\u555f\u767c\uff0c\u6211\u5011\u5c07\u5b8c\u6574\u7684 ARMA \u7d50\u69cb\u5f15\u5165\u73fe\u6709\u7684\u81ea\u8ff4\u6b78\u6ce8\u610f\u529b\u6a5f\u5236\u3002\u901a\u904e\u4f7f\u7528\u9593\u63a5 MA \u6b0a\u91cd\u751f\u6210\u65b9\u6cd5\uff0c\u6211\u5011\u5728\u4fdd\u6301\u5e95\u5c64\u9ad8\u6548\u6ce8\u610f\u529b\u6a21\u578b\u7684\u6642\u9593\u8907\u96dc\u5ea6\u548c\u53c3\u6578\u5927\u5c0f\u7684\u540c\u6642\uff0c\u7d0d\u5165\u4e86 MA \u9805\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a2\u8a0e\u4e86\u9593\u63a5\u53c3\u6578\u751f\u6210\u5982\u4f55\u7522\u751f\u8207\u5c40\u90e8\u6642\u9593\u5f71\u97ff\u7684\u5efa\u6a21\u9700\u6c42\u4e00\u81f4\u7684\u96b1\u5f0f MA \u6b0a\u91cd\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u7d0d\u5165 ARMA \u7d50\u69cb\u6301\u7e8c\u6539\u5584\u5404\u7a2e AR \u6ce8\u610f\u529b\u5728 TSF \u4efb\u52d9\u4e0a\u7684\u6027\u80fd\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u7d50\u679c\u3002", "author": "Jiecheng Lu et.al.", "authors": "Jiecheng Lu, Xu Han, Yan Sun, Shihao Yang", "id": "2410.03159v1", "paper_url": "http://arxiv.org/abs/2410.03159v1", "repo": "null"}}