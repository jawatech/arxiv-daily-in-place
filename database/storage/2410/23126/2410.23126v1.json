{"2410.23126": {"publish_time": "2024-10-30", "title": "Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes", "paper_summary": "We study the optimal memorization capacity of modern Hopfield models and\nKernelized Hopfield Models (KHMs), a transformer-compatible class of Dense\nAssociative Memories. We present a tight analysis by establishing a connection\nbetween the memory configuration of KHMs and spherical codes from information\ntheory. Specifically, we treat the stored memory set as a specialized spherical\ncode. This enables us to cast the memorization problem in KHMs into a point\narrangement problem on a hypersphere. We show that the optimal capacity of KHMs\noccurs when the feature space allows memories to form an optimal spherical\ncode. This unique perspective leads to: (i) An analysis of how KHMs achieve\noptimal memory capacity, and identify corresponding necessary conditions.\nImportantly, we establish an upper capacity bound that matches the well-known\nexponential lower bound in the literature. This provides the first tight and\noptimal asymptotic memory capacity for modern Hopfield models. (ii) A\nsub-linear time algorithm $\\mathtt{U}\\text{-}\\mathtt{Hop}$+ to reach KHMs'\noptimal capacity. (iii) An analysis of the scaling behavior of the required\nfeature dimension relative to the number of stored memories. These efforts\nimprove both the retrieval capability of KHMs and the representation learning\nof corresponding transformers. Experimentally, we provide thorough numerical\nresults to back up theoretical findings.", "paper_summary_zh": "\u6211\u5011\u7814\u7a76\u73fe\u4ee3 Hopfield \u6a21\u578b\u548c\u6838\u5316 Hopfield \u6a21\u578b (KHM) \u7684\u6700\u4f73\u8a18\u61b6\u5bb9\u91cf\uff0c\u9019\u662f\u4e00\u500b\u8207Transformer\u76f8\u5bb9\u7684\u7a20\u5bc6\u806f\u60f3\u8a18\u61b6\u985e\u5225\u3002\u6211\u5011\u900f\u904e\u5efa\u7acb KHM \u7684\u8a18\u61b6\u9ad4\u7d44\u614b\u8207\u8cc7\u8a0a\u7406\u8ad6\u4e2d\u7684\u7403\u5f62\u78bc\u4e4b\u9593\u7684\u9023\u7d50\uff0c\u63d0\u51fa\u56b4\u8b39\u7684\u5206\u6790\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u5132\u5b58\u7684\u8a18\u61b6\u9ad4\u7d44\u8996\u70ba\u4e00\u500b\u7279\u6b8a\u7684\u7403\u5f62\u78bc\u3002\u9019\u4f7f\u6211\u5011\u80fd\u5920\u5c07 KHM \u4e2d\u7684\u8a18\u61b6\u9ad4\u554f\u984c\u8f49\u63db\u70ba\u8d85\u7403\u9762\u4e0a\u7684\u9ede\u6392\u5217\u554f\u984c\u3002\u6211\u5011\u8b49\u660e KHM \u7684\u6700\u4f73\u5bb9\u91cf\u767c\u751f\u5728\u7279\u5fb5\u7a7a\u9593\u5141\u8a31\u8a18\u61b6\u9ad4\u5f62\u6210\u6700\u4f73\u7403\u5f62\u78bc\u6642\u3002\u9019\u500b\u7368\u7279\u7684\u89c0\u9ede\u5c0e\u81f4\uff1a(i) \u5c0d KHM \u5982\u4f55\u9054\u5230\u6700\u4f73\u8a18\u61b6\u9ad4\u5bb9\u91cf\u7684\u5206\u6790\uff0c\u4e26\u627e\u51fa\u5c0d\u61c9\u7684\u5fc5\u8981\u689d\u4ef6\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u5bb9\u91cf\u4e0a\u9650\uff0c\u8207\u6587\u737b\u4e2d\u8457\u540d\u7684\u6307\u6578\u4e0b\u9650\u76f8\u5339\u914d\u3002\u9019\u70ba\u73fe\u4ee3 Hopfield \u6a21\u578b\u63d0\u4f9b\u4e86\u7b2c\u4e00\u500b\u56b4\u8b39\u4e14\u6700\u4f73\u7684\u6f38\u8fd1\u8a18\u61b6\u9ad4\u5bb9\u91cf\u3002(ii) \u4e00\u500b\u6b21\u7dda\u6027\u6642\u9593\u6f14\u7b97\u6cd5 $\\mathtt{U}\\text{-}\\mathtt{Hop}$+\uff0c\u4ee5\u9054\u5230 KHM \u7684\u6700\u4f73\u5bb9\u91cf\u3002(iii) \u5c0d\u6240\u9700\u7279\u5fb5\u7dad\u5ea6\u76f8\u5c0d\u65bc\u5132\u5b58\u8a18\u61b6\u9ad4\u6578\u91cf\u7684\u7e2e\u653e\u884c\u70ba\u7684\u5206\u6790\u3002\u9019\u4e9b\u52aa\u529b\u540c\u6642\u6539\u5584\u4e86 KHM \u7684\u6aa2\u7d22\u80fd\u529b\u548c\u5c0d\u61c9Transformer\u7684\u8868\u5fb5\u5b78\u7fd2\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u8a73\u76e1\u7684\u6578\u503c\u7d50\u679c\uff0c\u4ee5\u652f\u6301\u7406\u8ad6\u767c\u73fe\u3002", "author": "Jerry Yao-Chieh Hu et.al.", "authors": "Jerry Yao-Chieh Hu, Dennis Wu, Han Liu", "id": "2410.23126v1", "paper_url": "http://arxiv.org/abs/2410.23126v1", "repo": "null"}}