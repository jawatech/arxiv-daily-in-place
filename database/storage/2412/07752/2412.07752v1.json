{"2412.07752": {"publish_time": "2024-12-10", "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware", "paper_summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}", "paper_summary_zh": "\u5118\u7ba1 Transformer \u548c\u5176\u4ed6\u53ef\u5e73\u884c\u8655\u7406\u5e8f\u5217\u7684\u795e\u7d93\u7db2\u8def\u67b6\u69cb\u770b\u8d77\u4f86\u50cf\u662f\u5e8f\u5217\u5efa\u6a21\u7684\u73fe\u4eca\u6280\u8853\u6c34\u6e96\uff0c\u4f46\u5b83\u5011\u7279\u5225\u7f3a\u4e4f\u72c0\u614b\u8ffd\u8e64\u529f\u80fd\u3002\u9019\u4e9b\u529f\u80fd\u5c0d\u65bc\u6642\u9593\u5e8f\u5217\u4efb\u52d9\u548c\u908f\u8f2f\u63a8\u7406\u975e\u5e38\u91cd\u8981\u3002\u50b3\u7d71\u7684 RNN\uff0c\u4f8b\u5982 LSTM \u548c GRU\uff0c\u4ee5\u53ca\u73fe\u4ee3\u8b8a\u9ad4\uff0c\u4f8b\u5982 sLSTM\uff0c\u78ba\u5be6\u5177\u6709\u9019\u4e9b\u529f\u80fd\uff0c\u4f46\u4ee3\u50f9\u662f\u56b4\u683c\u7684\u9806\u5e8f\u8655\u7406\u3002\u5118\u7ba1\u9019\u901a\u5e38\u88ab\u8996\u70ba\u4e00\u500b\u56b4\u91cd\u7684\u9650\u5236\uff0c\u4f46\u6211\u5011\u5c55\u793a\u4e86\u9019\u4e9b\u7db2\u8def\u900f\u904e\u6211\u5011\u5728 Triton \u548c CUDA \u4e2d\u7684\u786c\u9ad4\u6700\u4f73\u5316 FlashRNN\uff0c\u5982\u4f55\u4f7f\u7528\u73fe\u4ee3 GPU \u5c07\u6838\u5fc3\u6700\u4f73\u5316\u5230\u66ab\u5b58\u5668\u5c64\u7d1a\uff0c\u9032\u800c\u8b8a\u5f97\u6709\u591a\u5feb\u3002\u6211\u5011\u4ee5\u4e26\u884c\u5316\u8b8a\u9ad4\u64f4\u5145\u50b3\u7d71\u7684 RNN\uff0c\u8a72\u8b8a\u9ad4\u8655\u7406\u591a\u500b\u96b1\u85cf\u72c0\u614b\u8f03\u5c0f\u7684 RNN\uff0c\u985e\u4f3c\u65bc Transformer \u4e2d\u7684\u982d\u90e8\u8655\u7406\u3002\u70ba\u4e86\u5728\u4e0d\u540c\u7684 GPU \u8b8a\u9ad4\u4e0a\u555f\u7528\u5f48\u6027\uff0c\u6211\u5011\u5f15\u9032\u4e86\u4e00\u500b\u65b0\u7684\u6700\u4f73\u5316\u67b6\u69cb\uff0c\u7528\u65bc\u786c\u9ad4\u5167\u90e8\u5feb\u53d6\u5927\u5c0f\u3001\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u8655\u7406\u3002\u5b83\u4f7f\u7528\u985e\u591a\u9762\u9ad4\u7684\u7d04\u675f\u4f86\u6a21\u64ec\u786c\u9ad4\u8a2d\u5b9a\uff0c\u5305\u62ec\u53ef\u9664\u6027\u7684\u6982\u5ff5\u3002\u9019\u52a0\u901f\u4e86\u6211\u5011 ConstrINT \u51fd\u5f0f\u5eab\u4e2d\u7684\u4e00\u822c\u6574\u6578\u7d04\u675f\u6eff\u8db3\u554f\u984c (\u6574\u6578 CSP) \u7684\u89e3\u6c7a\u7a0b\u5e8f\u3002\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u7684\u6838\u5fc3\u53ef\u4ee5\u6bd4\u9999\u8349 PyTorch \u5be6\u4f5c\u5feb 50 \u500d\uff0c\u4e26\u5141\u8a31\u6bd4\u6211\u5011\u7684 Triton \u5be6\u4f5c\u5927 40 \u500d\u7684\u96b1\u85cf\u5927\u5c0f\u3002\u6211\u5011\u7684\u958b\u6e90\u6838\u5fc3\u548c\u6700\u4f73\u5316\u51fd\u5f0f\u5eab\u5728\u6b64\u91cb\u51fa\uff0c\u4ee5\u4fc3\u9032\u72c0\u614b\u8ffd\u8e64\u555f\u7528 RNN \u548c\u5e8f\u5217\u5efa\u6a21\u7684\u7814\u7a76\uff1a\\url{https://github.com/NX-AI/flashrnn}", "author": "Korbinian P\u00f6ppel et.al.", "authors": "Korbinian P\u00f6ppel, Maximilian Beck, Sepp Hochreiter", "id": "2412.07752v1", "paper_url": "http://arxiv.org/abs/2412.07752v1", "repo": "null"}}