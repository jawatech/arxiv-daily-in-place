{"2407.11536": {"publish_time": "2024-07-16", "title": "Fine-Tuning Medical Language Models for Enhanced Long-Contextual Understanding and Domain Expertise", "paper_summary": "Large Language Models (LLMs) have been widely applied in various professional\nfields. By fine-tuning the models using domain specific question and answer\ndatasets, the professional domain knowledge and Q\\&A abilities of these models\nhave significantly improved, for example, medical professional LLMs that use\nfine-tuning of doctor-patient Q\\&A data exhibit extraordinary disease\ndiagnostic abilities. However, we observed that despite improvements in\nspecific domain knowledge, the performance of medical LLM in long-context\nunderstanding has significantly declined, especially compared to general\nlanguage models with similar parameters. The purpose of this study is to\ninvestigate the phenomenon of reduced performance in understanding long-context\nin medical LLM. We designed a series of experiments to conduct open-book\nprofessional knowledge exams on all models to evaluate their ability to read\nlong-context. By adjusting the proportion and quantity of general data and\nmedical data in the process of fine-tuning, we can determine the best data\ncomposition to optimize the professional model and achieve a balance between\nlong-context performance and specific domain knowledge.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5ee3\u6cdb\u61c9\u7528\u65bc\u5404\u7a2e\u5c08\u696d\u9818\u57df\u3002\u901a\u904e\u4f7f\u7528\u7279\u5b9a\u9818\u57df\u7684\u554f\u7b54\u8cc7\u6599\u96c6\u5fae\u8abf\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u7684\u5c08\u696d\u9818\u57df\u77e5\u8b58\u548c\u554f\u7b54\u80fd\u529b\u5df2\u986f\u8457\u63d0\u5347\uff0c\u4f8b\u5982\uff0c\u4f7f\u7528\u91ab\u751f-\u60a3\u8005\u554f\u7b54\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u7684\u91ab\u7642\u5c08\u696d LLM \u5c55\u73fe\u51fa\u975e\u51e1\u7684\u75be\u75c5\u8a3a\u65b7\u80fd\u529b\u3002\u7136\u800c\uff0c\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u5118\u7ba1\u7279\u5b9a\u9818\u57df\u77e5\u8b58\u6709\u6240\u63d0\u5347\uff0c\u4f46\u91ab\u7642 LLM \u5728\u9577\u8a9e\u5883\u7406\u89e3\u65b9\u9762\u7684\u8868\u73fe\u537b\u5927\u5e45\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u8207\u5177\u6709\u985e\u4f3c\u53c3\u6578\u7684\u4e00\u822c\u8a9e\u8a00\u6a21\u578b\u76f8\u6bd4\u3002\u672c\u7814\u7a76\u7684\u76ee\u7684\u662f\u63a2\u8a0e\u91ab\u7642 LLM \u5728\u7406\u89e3\u9577\u8a9e\u5883\u65b9\u9762\u7684\u8868\u73fe\u4e0b\u964d\u73fe\u8c61\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u7cfb\u5217\u5be6\u9a57\uff0c\u5c0d\u6240\u6709\u6a21\u578b\u9032\u884c\u958b\u653e\u5f0f\u5c08\u696d\u77e5\u8b58\u8003\u8a66\uff0c\u4ee5\u8a55\u4f30\u5b83\u5011\u95b1\u8b80\u9577\u8a9e\u5883\u7684\u7406\u89e3\u80fd\u529b\u3002\u901a\u904e\u8abf\u6574\u5fae\u8abf\u904e\u7a0b\u4e2d\u4e00\u822c\u8cc7\u6599\u548c\u91ab\u7642\u8cc7\u6599\u7684\u6bd4\u4f8b\u548c\u6578\u91cf\uff0c\u6211\u5011\u53ef\u4ee5\u78ba\u5b9a\u6700\u4f73\u8cc7\u6599\u7d44\u5408\uff0c\u4ee5\u512a\u5316\u5c08\u696d\u6a21\u578b\uff0c\u4e26\u5728\u9577\u8a9e\u5883\u8868\u73fe\u548c\u7279\u5b9a\u9818\u57df\u77e5\u8b58\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002", "author": "Qimin Yang et.al.", "authors": "Qimin Yang, Rongsheng Wang, Jiexin Chen, Runqi Su, Tao Tan", "id": "2407.11536v1", "paper_url": "http://arxiv.org/abs/2407.11536v1", "repo": "null"}}