{"2404.15946": {"publish_time": "2024-04-24", "title": "Mammo-CLIP: Leveraging Contrastive Language-Image Pre-training (CLIP) for Enhanced Breast Cancer Diagnosis with Multi-view Mammography", "paper_summary": "Although fusion of information from multiple views of mammograms plays an\nimportant role to increase accuracy of breast cancer detection, developing\nmulti-view mammograms-based computer-aided diagnosis (CAD) schemes still faces\nchallenges and no such CAD schemes have been used in clinical practice. To\novercome the challenges, we investigate a new approach based on Contrastive\nLanguage-Image Pre-training (CLIP), which has sparked interest across various\nmedical imaging tasks. By solving the challenges in (1) effectively adapting\nthe single-view CLIP for multi-view feature fusion and (2) efficiently\nfine-tuning this parameter-dense model with limited samples and computational\nresources, we introduce Mammo-CLIP, the first multi-modal framework to process\nmulti-view mammograms and corresponding simple texts. Mammo-CLIP uses an early\nfeature fusion strategy to learn multi-view relationships in four mammograms\nacquired from the CC and MLO views of the left and right breasts. To enhance\nlearning efficiency, plug-and-play adapters are added into CLIP image and text\nencoders for fine-tuning parameters and limiting updates to about 1% of the\nparameters. For framework evaluation, we assembled two datasets\nretrospectively. The first dataset, comprising 470 malignant and 479 benign\ncases, was used for few-shot fine-tuning and internal evaluation of the\nproposed Mammo-CLIP via 5-fold cross-validation. The second dataset, including\n60 malignant and 294 benign cases, was used to test generalizability of\nMammo-CLIP. Study results show that Mammo-CLIP outperforms the state-of-art\ncross-view transformer in AUC (0.841 vs. 0.817, 0.837 vs. 0.807) on both\ndatasets. It also surpasses previous two CLIP-based methods by 20.3% and 14.3%.\nThis study highlights the potential of applying the finetuned vision-language\nmodels for developing next-generation, image-text-based CAD schemes of breast\ncancer.", "paper_summary_zh": "<paragraph>\u5118\u7ba1\u878d\u5408\u591a\u8996\u89d2\u4e73\u623f\u651d\u5f71\u7684\u8cc7\u8a0a\u5728\u63d0\u5347\u4e73\u764c\u5075\u6e2c\u6e96\u78ba\u5ea6\u4e0a\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u958b\u767c\u57fa\u65bc\u591a\u8996\u89d2\u4e73\u623f\u651d\u5f71\u7684\u96fb\u8166\u8f14\u52a9\u8a3a\u65b7 (CAD) \u67b6\u69cb\u4ecd\u9762\u81e8\u6311\u6230\uff0c\u4e14\u76ee\u524d\u5c1a\u672a\u6709\u6b64\u985e CAD \u67b6\u69cb\u7528\u65bc\u81e8\u5e8a\u5be6\u52d9\u3002\u70ba\u514b\u670d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u7814\u7a76\u4e86\u4e00\u7a2e\u57fa\u65bc\u5c0d\u6bd4\u5f0f\u8a9e\u8a00\u5f71\u50cf\u9810\u8a13\u7df4 (CLIP) \u7684\u65b0\u65b9\u6cd5\uff0c\u9019\u5df2\u5728\u5404\u7a2e\u91ab\u5b78\u5f71\u50cf\u4efb\u52d9\u4e2d\u5f15\u8d77\u8208\u8da3\u3002\u900f\u904e\u89e3\u6c7a (1) \u6709\u6548\u8abf\u6574\u55ae\u8996\u89d2 CLIP \u4ee5\u9032\u884c\u591a\u8996\u89d2\u7279\u5fb5\u878d\u5408\uff0c\u4ee5\u53ca (2) \u6709\u6548\u5fae\u8abf\u6b64\u53c3\u6578\u5bc6\u96c6\u6a21\u578b\uff0c\u4e14\u50c5\u4f7f\u7528\u6709\u9650\u6a23\u672c\u548c\u904b\u7b97\u8cc7\u6e90\u7684\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 Mammo-CLIP\uff0c\u9019\u662f\u4e00\u500b\u8655\u7406\u591a\u8996\u89d2\u4e73\u623f\u651d\u5f71\u548c\u5c0d\u61c9\u7c21\u77ed\u6587\u5b57\u7684\u7b2c\u4e00\u500b\u591a\u6a21\u5f0f\u67b6\u69cb\u3002Mammo-CLIP \u4f7f\u7528\u65e9\u671f\u7279\u5fb5\u878d\u5408\u7b56\u7565\u4f86\u5b78\u7fd2\u5f9e\u5de6\u53f3\u4e73\u623f\u7684 CC \u548c MLO \u8996\u89d2\u53d6\u5f97\u7684\u56db\u500b\u4e73\u623f\u651d\u5f71\u7684\u591a\u8996\u89d2\u95dc\u4fc2\u3002\u70ba\u63d0\u5347\u5b78\u7fd2\u6548\u7387\uff0c\u5c07\u5373\u63d2\u5373\u7528\u9069\u914d\u5668\u52a0\u5165 CLIP \u5f71\u50cf\u548c\u6587\u5b57\u7de8\u78bc\u5668\u4e2d\uff0c\u4ee5\u5fae\u8abf\u53c3\u6578\uff0c\u4e26\u5c07\u66f4\u65b0\u9650\u5236\u5728\u7d04 1% \u7684\u53c3\u6578\u3002\u70ba\u9032\u884c\u67b6\u69cb\u8a55\u4f30\uff0c\u6211\u5011\u56de\u6eaf\u6027\u5730\u5f59\u6574\u4e86\u5169\u500b\u8cc7\u6599\u96c6\u3002\u7b2c\u4e00\u500b\u8cc7\u6599\u96c6\u5305\u542b 470 \u500b\u60e1\u6027\u548c 479 \u500b\u826f\u6027\u75c5\u4f8b\uff0c\u7528\u65bc\u5c11\u91cf\u5fae\u8abf\u548c\u900f\u904e 5 \u500d\u4ea4\u53c9\u9a57\u8b49\u5c0d\u6240\u63d0\u51fa\u7684 Mammo-CLIP \u9032\u884c\u5167\u90e8\u8a55\u4f30\u3002\u7b2c\u4e8c\u500b\u8cc7\u6599\u96c6\u5305\u542b 60 \u500b\u60e1\u6027\u548c 294 \u500b\u826f\u6027\u75c5\u4f8b\uff0c\u7528\u65bc\u6e2c\u8a66 Mammo-CLIP \u7684\u6982\u62ec\u6027\u3002\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0cMammo-CLIP \u5728\u5169\u500b\u8cc7\u6599\u96c6\u4e0a\u90fd\u512a\u65bc\u6700\u5148\u9032\u7684\u8de8\u8996\u89d2Transformer\uff0cAUC \u5206\u5225\u70ba (0.841 \u5c0d 0.817\u30010.837 \u5c0d 0.807)\u3002\u5b83\u4e5f\u6bd4\u5148\u524d\u7684\u5169\u7a2e\u57fa\u65bc CLIP \u7684\u65b9\u6cd5\u9ad8\u51fa 20.3% \u548c 14.3%\u3002\u672c\u7814\u7a76\u7a81\u986f\u4e86\u61c9\u7528\u5fae\u8abf\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u4f86\u958b\u767c\u4e73\u764c\u65b0\u4e00\u4ee3\u5f71\u50cf\u6587\u5b57\u578b CAD \u67b6\u69cb\u7684\u6f5b\u529b\u3002</paragraph>", "author": "Xuxin Chen et.al.", "authors": "Xuxin Chen, Yuheng Li, Mingzhe Hu, Ella Salari, Xiaoqian Chen, Richard L. J. Qiu, Bin Zheng, Xiaofeng Yang", "id": "2404.15946v1", "paper_url": "http://arxiv.org/abs/2404.15946v1", "repo": "null"}}