{"2410.04734": {"publish_time": "2024-10-07", "title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models", "paper_summary": "Although reward models have been successful in improving multimodal large\nlanguage models, the reward models themselves remain brutal and contain minimal\ninformation. Notably, existing reward models only mimic human annotations by\nassigning only one binary feedback to any text, no matter how long the text is.\nIn the realm of multimodal language models, where models are required to\nprocess both images and texts, a naive reward model may learn implicit biases\ntoward texts and become less grounded in images. In this paper, we propose a\n$\\textbf{T}$oken-$\\textbf{L}$evel $\\textbf{D}$etective $\\textbf{R}$eward Model\n($\\textbf{TLDR}$) to provide fine-grained annotations to each text token. We\nfirst introduce a perturbation-based method to generate synthetic hard\nnegatives and their token-level labels to train TLDR models. Then we show the\nrich usefulness of TLDR models both in assisting off-the-shelf models to\nself-correct their generations, and in serving as a hallucination evaluation\ntool. Finally, we show that TLDR models can significantly speed up human\nannotation by 3 times to acquire a broader range of high-quality vision\nlanguage data.", "paper_summary_zh": "\u96d6\u7136\u734e\u52f5\u6a21\u578b\u5728\u6539\u5584\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u65b9\u9762\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u734e\u52f5\u6a21\u578b\u672c\u8eab\u4ecd\u7136\u5f88\u6b98\u9177\u4e14\u5305\u542b\u6700\u5c11\u8cc7\u8a0a\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u73fe\u6709\u7684\u734e\u52f5\u6a21\u578b\u50c5\u900f\u904e\u5c07\u4e00\u500b\u4e8c\u5143\u56de\u994b\u5206\u914d\u7d66\u4efb\u4f55\u6587\u5b57\u4f86\u6a21\u4eff\u4eba\u985e\u8a3b\u89e3\uff0c\u7121\u8ad6\u6587\u5b57\u6709\u591a\u9577\u3002\u5728\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b\u9818\u57df\u4e2d\uff0c\u6a21\u578b\u9700\u8981\u8655\u7406\u5f71\u50cf\u548c\u6587\u5b57\uff0c\u4e00\u500b\u5929\u771f\u7684\u734e\u52f5\u6a21\u578b\u53ef\u80fd\u6703\u5b78\u7fd2\u5c0d\u6587\u5b57\u7684\u96b1\u542b\u504f\u898b\uff0c\u4e14\u4e0d\u592a\u4f9d\u8cf4\u65bc\u5f71\u50cf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b $\\textbf{T}$oken-$\\textbf{L}$evel $\\textbf{D}$etective $\\textbf{R}$eward Model\uff08$\\textbf{TLDR}$\uff09\u4f86\u70ba\u6bcf\u500b\u6587\u5b57\u4ee3\u5e63\u63d0\u4f9b\u7d30\u7dfb\u7684\u8a3b\u89e3\u3002\u6211\u5011\u9996\u5148\u4ecb\u7d39\u4e00\u500b\u57fa\u65bc\u64fe\u52d5\u7684\u65b9\u6cd5\u4f86\u751f\u6210\u5408\u6210\u96e3\u8ca0\u4f8b\u53ca\u5176\u4ee3\u5e63\u7d1a\u5225\u6a19\u7c64\u4ee5\u8a13\u7df4 TLDR \u6a21\u578b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86 TLDR \u6a21\u578b\u7684\u8c50\u5bcc\u5be6\u7528\u6027\uff0c\u65e2\u53ef\u4ee5\u5354\u52a9\u73fe\u6210\u7684\u6a21\u578b\u81ea\u6211\u4fee\u6b63\u5176\u751f\u6210\uff0c\u53c8\u53ef\u4ee5\u4f5c\u70ba\u5e7b\u89ba\u8a55\u4f30\u5de5\u5177\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c55\u793a\u4e86 TLDR \u6a21\u578b\u53ef\u4ee5\u5c07\u4eba\u985e\u8a3b\u89e3\u7684\u901f\u5ea6\u986f\u8457\u63d0\u9ad8 3 \u500d\uff0c\u4ee5\u7372\u53d6\u66f4\u5ee3\u6cdb\u7684\u9ad8\u54c1\u8cea\u8996\u89ba\u8a9e\u8a00\u8cc7\u6599\u3002", "author": "Deqing Fu et.al.", "authors": "Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, Lawrence Chen", "id": "2410.04734v1", "paper_url": "http://arxiv.org/abs/2410.04734v1", "repo": "null"}}