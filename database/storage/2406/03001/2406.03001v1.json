{"2406.03001": {"publish_time": "2024-06-05", "title": "EdgeSync: Faster Edge-model Updating via Adaptive Continuous Learning for Video Data Drift", "paper_summary": "Real-time video analytics systems typically place models with fewer weights\non edge devices to reduce latency. The distribution of video content features\nmay change over time for various reasons (i.e. light and weather change) ,\nleading to accuracy degradation of existing models, to solve this problem,\nrecent work proposes a framework that uses a remote server to continually train\nand adapt the lightweight model at edge with the help of complex model.\nHowever, existing analytics approaches leave two challenges untouched: firstly,\nretraining task is compute-intensive, resulting in large model update delays;\nsecondly, new model may not fit well enough with the data distribution of the\ncurrent video stream. To address these challenges, in this paper, we present\nEdgeSync, EdgeSync filters the samples by considering both timeliness and\ninference results to make training samples more relevant to the current video\ncontent as well as reduce the update delay, to improve the quality of training,\nEdgeSync also designs a training management module that can efficiently adjusts\nthe model training time and training order on the runtime. By evaluating real\ndatasets with complex scenes, our method improves about 3.4% compared to\nexisting methods and about 10% compared to traditional means.", "paper_summary_zh": "\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u7cfb\u7edf\u901a\u5e38\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u653e\u7f6e\u6743\u91cd\u8f83\u5c11\u7684\u6a21\u578b\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002\u89c6\u9891\u5185\u5bb9\u7279\u5f81\u7684\u5206\u5e03\u53ef\u80fd\u56e0\u5404\u79cd\u539f\u56e0\uff08\u5373\u5149\u7ebf\u548c\u5929\u6c14\u53d8\u5316\uff09\u800c\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u4ece\u800c\u5bfc\u81f4\u73b0\u6709\u6a21\u578b\u7684\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6700\u8fd1\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8fdc\u7a0b\u670d\u52a1\u5668\u6765\u6301\u7eed\u8bad\u7ec3\u548c\u8c03\u6574\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5728\u590d\u6742\u6a21\u578b\u7684\u5e2e\u52a9\u4e0b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5206\u6790\u65b9\u6cd5\u7559\u4e0b\u4e86\u4e24\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff1a\u9996\u5148\uff0c\u91cd\u65b0\u8bad\u7ec3\u4efb\u52a1\u8ba1\u7b97\u5bc6\u96c6\uff0c\u5bfc\u81f4\u6a21\u578b\u66f4\u65b0\u5ef6\u8fdf\u5927\uff1b\u5176\u6b21\uff0c\u65b0\u6a21\u578b\u53ef\u80fd\u4e0e\u5f53\u524d\u89c6\u9891\u6d41\u7684\u6570\u636e\u5206\u5e03\u4e0d\u591f\u5339\u914d\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 EdgeSync\uff0cEdgeSync \u901a\u8fc7\u8003\u8651\u53ca\u65f6\u6027\u548c\u63a8\u7406\u7ed3\u679c\u6765\u8fc7\u6ee4\u6837\u672c\uff0c\u4ee5\u4f7f\u8bad\u7ec3\u6837\u672c\u4e0e\u5f53\u524d\u89c6\u9891\u5185\u5bb9\u66f4\u76f8\u5173\uff0c\u5e76\u51cf\u5c11\u66f4\u65b0\u5ef6\u8fdf\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u8d28\u91cf\uff0cEdgeSync \u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8bad\u7ec3\u7ba1\u7406\u6a21\u5757\uff0c\u53ef\u4ee5\u5728\u8fd0\u884c\u65f6\u6709\u6548\u5730\u8c03\u6574\u6a21\u578b\u8bad\u7ec3\u65f6\u95f4\u548c\u8bad\u7ec3\u987a\u5e8f\u3002\u901a\u8fc7\u8bc4\u4f30\u5177\u6709\u590d\u6742\u573a\u666f\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u63d0\u9ad8\u4e86\u7ea6 3.4%\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\u63d0\u9ad8\u4e86\u7ea6 10%\u3002", "author": "Peng Zhao et.al.", "authors": "Peng Zhao, Runchu Dong, Guiqin Wang, Cong Zhao", "id": "2406.03001v1", "paper_url": "http://arxiv.org/abs/2406.03001v1", "repo": "null"}}