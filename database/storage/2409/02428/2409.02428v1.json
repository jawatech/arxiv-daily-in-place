{"2409.02428": {"publish_time": "2024-09-04", "title": "Large Language Models as Efficient Reward Function Searchers for Custom-Environment Multi-Objective Reinforcement Learning", "paper_summary": "Leveraging large language models (LLMs) for designing reward functions\ndemonstrates significant potential. However, achieving effective design and\nimprovement of reward functions in reinforcement learning (RL) tasks with\ncomplex custom environments and multiple requirements presents considerable\nchallenges. In this paper, we enable LLMs to be effective white-box searchers,\nhighlighting their advanced semantic understanding capabilities. Specifically,\nwe generate reward components for each explicit user requirement and employ the\nreward critic to identify the correct code form. Then, LLMs assign weights to\nthe reward components to balance their values and iteratively search and\noptimize these weights based on the context provided by the training log\nanalyzer, while adaptively determining the search step size. We applied the\nframework to an underwater information collection RL task without direct human\nfeedback or reward examples (zero-shot). The reward critic successfully correct\nthe reward code with only one feedback for each requirement, effectively\npreventing irreparable errors that can occur when reward function feedback is\nprovided in aggregate. The effective initialization of weights enables the\nacquisition of different reward functions within the Pareto solution set\nwithout weight search. Even in the case where a weight is 100 times off, fewer\nthan four iterations are needed to obtain solutions that meet user\nrequirements. The framework also works well with most prompts utilizing GPT-3.5\nTurbo, since it does not require advanced numerical understanding or\ncalculation.", "paper_summary_zh": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5728\u5177\u6709\u590d\u6742\u81ea\u5b9a\u4e49\u73af\u5883\u548c\u591a\u91cd\u8981\u6c42\u7684\u5f3a\u5316\u5b66\u4e60 (RL) \u4efb\u52a1\u4e2d\u5b9e\u73b0\u5956\u52b1\u51fd\u6570\u7684\u6709\u6548\u8bbe\u8ba1\u548c\u6539\u8fdb\u63d0\u51fa\u4e86\u76f8\u5f53\u5927\u7684\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4f7f LLM \u6210\u4e3a\u6709\u6548\u7684\u767d\u76d2\u641c\u7d22\u5668\uff0c\u7a81\u51fa\u4e86\u5b83\u4eec\u5148\u8fdb\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u4e3a\u6bcf\u4e2a\u660e\u786e\u7684\u7528\u6237\u9700\u6c42\u751f\u6210\u5956\u52b1\u7ec4\u4ef6\uff0c\u5e76\u4f7f\u7528\u5956\u52b1\u6279\u8bc4\u8005\u6765\u8bc6\u522b\u6b63\u786e\u7684\u4ee3\u7801\u5f62\u5f0f\u3002\u7136\u540e\uff0cLLM \u4e3a\u5956\u52b1\u7ec4\u4ef6\u5206\u914d\u6743\u91cd\u4ee5\u5e73\u8861\u5176\u4ef7\u503c\uff0c\u5e76\u57fa\u4e8e\u8bad\u7ec3\u65e5\u5fd7\u5206\u6790\u5668\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u8fed\u4ee3\u641c\u7d22\u548c\u4f18\u5316\u8fd9\u4e9b\u6743\u91cd\uff0c\u540c\u65f6\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u641c\u7d22\u6b65\u957f\u3002\u6211\u4eec\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u6c34\u4e0b\u4fe1\u606f\u6536\u96c6 RL \u4efb\u52a1\uff0c\u65e0\u9700\u76f4\u63a5\u7684\u4eba\u5de5\u53cd\u9988\u6216\u5956\u52b1\u793a\u4f8b\uff08\u96f6\u6b21\u5b66\u4e60\uff09\u3002\u5956\u52b1\u6279\u8bc4\u8005\u4ec5\u4f7f\u7528\u6bcf\u4e2a\u8981\u6c42\u7684\u4e00\u4e2a\u53cd\u9988\u5c31\u6210\u529f\u5730\u7ea0\u6b63\u4e86\u5956\u52b1\u4ee3\u7801\uff0c\u6709\u6548\u5730\u9632\u6b62\u4e86\u5728\u6c47\u603b\u63d0\u4f9b\u5956\u52b1\u51fd\u6570\u53cd\u9988\u65f6\u53ef\u80fd\u53d1\u751f\u7684\u4e0d\u53ef\u4fee\u590d\u7684\u9519\u8bef\u3002\u6743\u91cd\u7684\u6709\u6548\u521d\u59cb\u5316\u4f7f\u5f97\u65e0\u9700\u6743\u91cd\u641c\u7d22\u5373\u53ef\u5728\u5e15\u7d2f\u6258\u89e3\u96c6\u4e2d\u83b7\u53d6\u4e0d\u540c\u7684\u5956\u52b1\u51fd\u6570\u3002\u5373\u4f7f\u5728\u6743\u91cd\u76f8\u5dee 100 \u500d\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u53ea\u9700\u8981\u4e0d\u5230\u56db\u6b21\u8fed\u4ee3\u5373\u53ef\u83b7\u5f97\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u6846\u67b6\u8fd8\u9002\u7528\u4e8e\u4f7f\u7528 GPT-3.5 Turbo \u7684\u5927\u591a\u6570\u63d0\u793a\uff0c\u56e0\u4e3a\u5b83\u4e0d\u9700\u8981\u9ad8\u7ea7\u7684\u6570\u5b57\u7406\u89e3\u6216\u8ba1\u7b97\u3002", "author": "Guanwen Xie et.al.", "authors": "Guanwen Xie, Jingzehua Xu, Yiyuan Yang, Shuai Zhang", "id": "2409.02428v1", "paper_url": "http://arxiv.org/abs/2409.02428v1", "repo": "null"}}