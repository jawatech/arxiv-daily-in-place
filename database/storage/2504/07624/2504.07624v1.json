{"2504.07624": {"publish_time": "2025-04-10", "title": "ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in Large Language Models", "paper_summary": "Retrieval Augmented Generation (RAG) has enjoyed increased attention in the\nrecent past and recent advancements in Large Language Models (LLMs) have\nhighlighted the importance of integrating world knowledge into these systems.\nCurrent RAG methodologies often modify the internal architecture of pre-trained\nlanguage models (PLMs) or rely on textifying knowledge graphs (KGs), which is\ninefficient in terms of token usage. This paper introduces ConceptFormer, a new\napproach to augment LLMs with structured knowledge from KGs, such as Wikidata,\nwithout altering their internal structure or relying on textual input of KGs.\nConceptFormer operates in the LLM embedding vector space, creating and\ninjecting \\emph{concept vectors} that encapsulate the information of the KG\nnodes directly. Trained in conjunction with a frozen LLM, ConceptFormer\ngenerates a comprehensive lookup table that maps KG nodes to their respective\nconcept vectors. The approach aims to enhance the factual recall capabilities\nof LLMs by enabling them to process these concept vectors natively, thus\nenriching them with structured world knowledge in an efficient and scalable\nmanner. Our experiments demonstrate that the addition of concept vectors to\nGPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to\n272\\% when tested on sentences from Wikipedia and up to 348\\% on synthetically\ngenerated sentences. Even injecting only a single concept vector into the\nprompt increases factual recall ability (Hit@10) by up to 213\\% on Wikipedia\nsentences, significantly outperforming RAG with graph textification while\nconsuming 130x fewer input tokens.", "paper_summary_zh": "<paragraph>\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u8fd1\u671f\u53d7\u5230\u8d8a\u4f86\u8d8a\u591a\u7684\u95dc\u6ce8\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u4e5f\u51f8\u986f\u4e86\u5c07\u4e16\u754c\u77e5\u8b58\u6574\u5408\u5230\u9019\u4e9b\u7cfb\u7d71\u4e2d\u7684\u91cd\u8981\u6027\u3002\u76ee\u524d\u7684 RAG \u65b9\u6cd5\u901a\u5e38\u6703\u4fee\u6539\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u7684\u5167\u90e8\u67b6\u69cb\uff0c\u6216\u4f9d\u8cf4\u77e5\u8b58\u5716\u8b5c (KG) \u7684\u6587\u672c\u5316\uff0c\u9019\u5728 token \u4f7f\u7528\u65b9\u9762\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u4ecb\u7d39\u4e86 ConceptFormer\uff0c\u9019\u662f\u4e00\u7a2e\u4f7f\u7528\u4f86\u81ea KG\uff08\u4f8b\u5982 Wikidata\uff09\u7684\u7d50\u69cb\u5316\u77e5\u8b58\u4f86\u589e\u5f37 LLM \u7684\u65b0\u65b9\u6cd5\uff0c\u7121\u9700\u66f4\u6539\u5176\u5167\u90e8\u7d50\u69cb\u6216\u4f9d\u8cf4 KG \u7684\u6587\u672c\u8f38\u5165\u3002ConceptFormer \u5728 LLM \u5d4c\u5165\u5411\u91cf\u7a7a\u9593\u4e2d\u904b\u4f5c\uff0c\u5275\u5efa\u4e26\u6ce8\u5165\\emph{\u6982\u5ff5\u5411\u91cf}\uff0c\u76f4\u63a5\u5c01\u88dd KG \u7bc0\u9ede\u7684\u4fe1\u606f\u3002ConceptFormer \u8207\u51cd\u7d50\u7684 LLM \u4e00\u8d77\u8a13\u7df4\uff0c\u751f\u6210\u4e00\u500b\u5168\u9762\u7684\u67e5\u627e\u8868\uff0c\u5c07 KG \u7bc0\u9ede\u6620\u5c04\u5230\u5b83\u5011\u5404\u81ea\u7684\u6982\u5ff5\u5411\u91cf\u3002\u8a72\u65b9\u6cd5\u65e8\u5728\u901a\u904e\u4f7f LLM \u80fd\u591f\u539f\u751f\u8655\u7406\u9019\u4e9b\u6982\u5ff5\u5411\u91cf\uff0c\u5f9e\u800c\u63d0\u9ad8 LLM \u7684\u4e8b\u5be6\u53ec\u56de\u80fd\u529b\uff0c\u4ee5\u9ad8\u6548\u4e14\u53ef\u64f4\u5c55\u7684\u65b9\u5f0f\u7528\u7d50\u69cb\u5316\u7684\u4e16\u754c\u77e5\u8b58\u8c50\u5bcc\u5b83\u5011\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u5728 Wikipedia \u53e5\u5b50\u4e0a\u9032\u884c\u6e2c\u8a66\u6642\uff0c\u5411 GPT-2 0.1B \u6dfb\u52a0\u6982\u5ff5\u5411\u91cf\u53ef\u5c07\u5176\u4e8b\u5be6\u53ec\u56de\u80fd\u529b (Hit@10) \u986f\u8457\u63d0\u9ad8\u591a\u9054 272%\uff0c\u5728\u7d9c\u5408\u751f\u6210\u7684\u53e5\u5b50\u4e0a\u5247\u63d0\u9ad8\u591a\u9054 348%\u3002\u5373\u4f7f\u50c5\u5728\u63d0\u793a\u4e2d\u6ce8\u5165\u55ae\u500b\u6982\u5ff5\u5411\u91cf\uff0cWikipedia \u53e5\u5b50\u4e0a\u7684\u4e8b\u5be6\u53ec\u56de\u80fd\u529b (Hit@10) \u4e5f\u80fd\u63d0\u9ad8\u591a\u9054 213%\uff0c\u986f\u8457\u512a\u65bc\u4f7f\u7528\u5716\u8b5c\u6587\u672c\u5316\u7684 RAG\uff0c\u540c\u6642\u4f7f\u7528\u7684\u8f38\u5165 token \u6e1b\u5c11\u4e86 130 \u500d\u3002</paragraph>\n", "author": "Joel Barmettler et.al.", "authors": "Joel Barmettler, Abraham Bernstein, Luca Rossetto", "id": "2504.07624v1", "paper_url": "http://arxiv.org/abs/2504.07624v1", "repo": "null"}}