{"2501.18119": {"publish_time": "2025-01-30", "title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models", "paper_summary": "Due to the presence of the natural gap between Knowledge Graph (KG)\nstructures and the natural language, the effective integration of holistic\nstructural information of KGs with Large Language Models (LLMs) has emerged as\na significant question. To this end, we propose a two-stage framework to learn\nand apply quantized codes for each entity, aiming for the seamless integration\nof KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR)\nmethod is proposed to compress both KG structural and semantic knowledge into\ndiscrete codes (\\ie, tokens) that align the format of language sentences. We\nfurther design KG instruction-following data by viewing these learned codes as\nfeatures to directly input to LLMs, thereby achieving seamless integration. The\nexperiment results demonstrate that SSQR outperforms existing unsupervised\nquantized methods, producing more distinguishable codes. Further, the\nfine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link\nprediction and triple classification tasks, utilizing only 16 tokens per entity\ninstead of thousands in conventional prompting methods.", "paper_summary_zh": "\u7531\u65bc\u77e5\u8b58\u5716\u8b5c (KG) \u7d50\u69cb\u8207\u81ea\u7136\u8a9e\u8a00\u4e4b\u9593\u5b58\u5728\u81ea\u7136\u5dee\u8ddd\uff0c\u5c07 KG \u7684\u6574\u9ad4\u7d50\u69cb\u8cc7\u8a0a\u8207\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6709\u6548\u6574\u5408\u5df2\u6210\u70ba\u4e00\u500b\u91cd\u8981\u7684\u554f\u984c\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5169\u968e\u6bb5\u67b6\u69cb\u4f86\u5b78\u7fd2\u548c\u61c9\u7528\u6bcf\u500b\u5be6\u9ad4\u7684\u91cf\u5316\u78bc\uff0c\u65e8\u5728\u5c07 KG \u8207 LLM \u7121\u7e2b\u6574\u5408\u3002\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u500b\u81ea\u76e3\u7763\u91cf\u5316\u8868\u793a (SSQR) \u65b9\u6cd5\uff0c\u5c07 KG \u7d50\u69cb\u548c\u8a9e\u7fa9\u77e5\u8b58\u58d3\u7e2e\u6210\u96e2\u6563\u78bc\uff08\u5373\uff0c\u7b26\u865f\uff09\uff0c\u4ee5\u5c0d\u9f4a\u8a9e\u8a00\u53e5\u5b50\u7684\u683c\u5f0f\u3002\u6211\u5011\u9032\u4e00\u6b65\u8a2d\u8a08 KG \u6307\u4ee4\u9075\u5faa\u8cc7\u6599\uff0c\u5c07\u9019\u4e9b\u5b78\u7fd2\u5230\u7684\u78bc\u8996\u70ba\u76f4\u63a5\u8f38\u5165 LLM \u7684\u7279\u5fb5\uff0c\u5f9e\u800c\u5be6\u73fe\u7121\u7e2b\u6574\u5408\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cSSQR \u512a\u65bc\u73fe\u6709\u7684\u7121\u76e3\u7763\u91cf\u5316\u65b9\u6cd5\uff0c\u7522\u751f\u66f4\u5177\u5340\u5225\u6027\u7684\u78bc\u3002\u6b64\u5916\uff0c\u5fae\u8abf\u5f8c\u7684 LLaMA2 \u548c LLaMA3.1 \u5728 KG \u9023\u7d50\u9810\u6e2c\u548c\u4e09\u5143\u5206\u985e\u4efb\u52d9\u4e0a\u4e5f\u5177\u6709\u512a\u7570\u7684\u6027\u80fd\uff0c\u6bcf\u500b\u5be6\u9ad4\u50c5\u4f7f\u7528 16 \u500b\u7b26\u865f\uff0c\u800c\u4e0d\u662f\u50b3\u7d71\u63d0\u793a\u65b9\u6cd5\u4e2d\u7684\u6578\u5343\u500b\u3002", "author": "Qika Lin et.al.", "authors": "Qika Lin, Tianzhe Zhao, Kai He, Zhen Peng, Fangzhi Xu, Ling Huang, Jingying Ma, Mengling Feng", "id": "2501.18119v1", "paper_url": "http://arxiv.org/abs/2501.18119v1", "repo": "null"}}