{"2409.08372": {"publish_time": "2024-09-12", "title": "FedProphet: Memory-Efficient Federated Adversarial Training via Theoretic-Robustness and Low-Inconsistency Cascade Learning", "paper_summary": "Federated Learning (FL) provides a strong privacy guarantee by enabling local\ntraining across edge devices without training data sharing, and Federated\nAdversarial Training (FAT) further enhances the robustness against adversarial\nexamples, promoting a step toward trustworthy artificial intelligence. However,\nFAT requires a large model to preserve high accuracy while achieving strong\nrobustness, and it is impractically slow when directly training with\nmemory-constrained edge devices due to the memory-swapping latency. Moreover,\nexisting memory-efficient FL methods suffer from poor accuracy and weak\nrobustness in FAT because of inconsistent local and global models, i.e.,\nobjective inconsistency.\n  In this paper, we propose FedProphet, a novel FAT framework that can achieve\nmemory efficiency, adversarial robustness, and objective consistency\nsimultaneously. FedProphet partitions the large model into small cascaded\nmodules such that the memory-constrained devices can conduct adversarial\ntraining module-by-module. A strong convexity regularization is derived to\ntheoretically guarantee the robustness of the whole model, and we show that the\nstrong robustness implies low objective inconsistency in FedProphet. We also\ndevelop a training coordinator on the server of FL, with Adaptive Perturbation\nAdjustment for utility-robustness balance and Differentiated Module Assignment\nfor objective inconsistency mitigation. FedProphet empirically shows a\nsignificant improvement in both accuracy and robustness compared to previous\nmemory-efficient methods, achieving almost the same performance of end-to-end\nFAT with 80% memory reduction and up to 10.8x speedup in training time.", "paper_summary_zh": "<paragraph>\u8054\u90a6\u5b66\u4e60 (FL) \u501f\u7531\u542f\u7528\u8fb9\u7f18\u8bbe\u5907\u8fdb\u884c\u672c\u5730\u8bad\u7ec3\uff0c\u4e14\u4e0d\u5206\u4eab\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u4f9b\u5f3a\u529b\u7684\u9690\u79c1\u4fdd\u8bc1\uff0c\u800c\u8054\u90a6\u5bf9\u6297\u8bad\u7ec3 (FAT) \u8fdb\u4e00\u6b65\u589e\u5f3a\u5bf9\u6297\u8303\u4f8b\u7684\u7a33\u5065\u6027\uff0c\u4fc3\u8fdb\u8fc8\u5411\u53ef\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u3002\u7136\u800c\uff0cFAT \u9700\u8981\u5927\u578b\u6a21\u578b\u6765\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u5f3a\u7a33\u5065\u6027\uff0c\u5e76\u4e14\u76f4\u63a5\u4f7f\u7528\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u8fdb\u884c\u8bad\u7ec3\u65f6\uff0c\u7531\u4e8e\u5185\u5b58\u4ea4\u6362\u5ef6\u8fdf\uff0c\u5176\u5b9e\u9645\u4e0a\u5f88\u6162\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u5185\u5b58\u9ad8\u6548 FL \u65b9\u6cd5\u5728 FAT \u4e2d\u906d\u53d7\u51c6\u786e\u6027\u5dee\u548c\u7a33\u5065\u6027\u5f31\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u56e0\u4e3a\u672c\u5730\u6a21\u578b\u548c\u5168\u5c40\u6a21\u578b\u4e0d\u4e00\u81f4\uff0c\u5373\u76ee\u6807\u4e0d\u4e00\u81f4\u3002\n  \u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa FedProphet\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684 FAT \u6846\u67b6\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u5185\u5b58\u6548\u7387\u3001\u5bf9\u6297\u7a33\u5065\u6027\u548c\u76ee\u6807\u4e00\u81f4\u6027\u3002FedProphet \u5c06\u5927\u578b\u6a21\u578b\u5206\u5272\u6210\u5c0f\u578b\u7ea7\u8054\u6a21\u5757\uff0c\u4ee5\u4fbf\u5185\u5b58\u53d7\u9650\u7684\u8bbe\u5907\u53ef\u4ee5\u9010\u6a21\u5757\u5730\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\u3002\u63a8\u5bfc\u51fa\u5f3a\u51f8\u6027\u6b63\u5219\u5316\u6765\u4ece\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6574\u4e2a\u6a21\u578b\u7684\u7a33\u5065\u6027\uff0c\u5e76\u4e14\u6211\u4eec\u8868\u660e\u5f3a\u7a33\u5065\u6027\u8868\u793a FedProphet \u4e2d\u7684\u4f4e\u76ee\u6807\u4e0d\u4e00\u81f4\u6027\u3002\u6211\u4eec\u8fd8\u5728 FL \u7684\u670d\u52a1\u5668\u4e0a\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bad\u7ec3\u534f\u8c03\u5668\uff0c\u5176\u4e2d\u5305\u542b\u7528\u4e8e\u6548\u7528\u7a33\u5065\u6027\u5e73\u8861\u7684\u81ea\u9002\u5e94\u6270\u52a8\u8c03\u6574\u548c\u7528\u4e8e\u7f13\u89e3\u76ee\u6807\u4e0d\u4e00\u81f4\u6027\u7684\u5dee\u5f02\u5316\u6a21\u5757\u5206\u914d\u3002FedProphet \u5728\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u65b9\u9762\u5747\u663e\u793a\u51fa\u663e\u7740\u7684\u6539\u5584\uff0c\u4e0e\u4e4b\u524d\u7684\u5185\u5b58\u9ad8\u6548\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u4e0e\u7aef\u5230\u7aef FAT \u51e0\u4e4e\u76f8\u540c\u7684\u6027\u80fd\uff0c\u5185\u5b58\u51cf\u5c11\u4e86 80%\uff0c\u8bad\u7ec3\u65f6\u95f4\u52a0\u5feb\u4e86 10.8 \u500d\u3002</paragraph>", "author": "Minxue Tang et.al.", "authors": "Minxue Tang, Yitu Wang, Jingyang Zhang, Louis DiValentin, Aolin Ding, Amin Hass, Yiran Chen, Hai \"Helen\" Li", "id": "2409.08372v1", "paper_url": "http://arxiv.org/abs/2409.08372v1", "repo": "null"}}