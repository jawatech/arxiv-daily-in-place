{"2406.11833": {"publish_time": "2024-06-17", "title": "MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs", "paper_summary": "Generating natural and meaningful responses to communicate with multi-modal\nhuman inputs is a fundamental capability of Large Vision-Language\nModels(LVLMs). While current open-source LVLMs demonstrate promising\nperformance in simplified scenarios such as single-turn single-image input,\nthey fall short in real-world conversation scenarios such as following\ninstructions in a long context history with multi-turn and multi-images.\nExisting LVLM benchmarks primarily focus on single-choice questions or\nshort-form responses, which do not adequately assess the capabilities of LVLMs\nin real-world human-AI interaction applications. Therefore, we introduce MMDU,\na comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning\ndataset, designed to evaluate and improve LVLMs' abilities in multi-turn and\nmulti-image conversations. We employ the clustering algorithm to ffnd the\nrelevant images and textual descriptions from the open-source Wikipedia and\nconstruct the question-answer pairs by human annotators with the assistance of\nthe GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and\n27 turns, which is at least 5x longer than previous benchmarks and poses\nchallenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs\nusing MMDU reveals that open-source LVLMs lag behind closed-source counterparts\ndue to limited conversational instruction tuning data. We demonstrate that\nffne-tuning open-source LVLMs on MMDU-45k signiffcantly address this gap,\ngenerating longer and more accurate conversations, and improving scores on MMDU\nand existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:+1.2%). Our\ncontributions pave the way for bridging the gap between current LVLM models and\nreal-world application demands. This project is available at\nhttps://github.com/Liuziyu77/MMDU.", "paper_summary_zh": "\u751f\u6210\u81ea\u7136\u4e14\u6709\u610f\u7fa9\u7684\u56de\u61c9\uff0c\u8207\u591a\u6a21\u614b\u4eba\u985e\u8f38\u5165\u9032\u884c\u6e9d\u901a\uff0c\u662f\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u7684\u57fa\u672c\u80fd\u529b\u3002\u96d6\u7136\u76ee\u524d\u7684\u958b\u6e90 LVLMs \u5728\u7c21\u5316\u7684\u5834\u666f\u4e2d\u5c55\u73fe\u51fa\u6709\u5e0c\u671b\u7684\u8868\u73fe\uff0c\u4f8b\u5982\u55ae\u6b21\u8f2a\u63db\u55ae\u4e00\u5f71\u50cf\u8f38\u5165\uff0c\u4f46\u5b83\u5011\u5728\u73fe\u5be6\u4e16\u754c\u7684\u5c0d\u8a71\u5834\u666f\u4e2d\u537b\u8868\u73fe\u4e0d\u4f73\uff0c\u4f8b\u5982\u5728\u5177\u6709\u591a\u8f2a\u63db\u548c\u591a\u5f71\u50cf\u7684\u9577\u8a9e\u5883\u6b77\u53f2\u4e2d\u9075\u5faa\u6307\u793a\u3002\u73fe\u6709\u7684 LVLM \u57fa\u6e96\u4e3b\u8981\u95dc\u6ce8\u55ae\u9078\u984c\u6216\u77ed\u7bc7\u56de\u61c9\uff0c\u9019\u4e26\u4e0d\u80fd\u5145\u5206\u8a55\u4f30 LVLMs \u5728\u73fe\u5be6\u4e16\u754c\u7684\u4eba\u5de5\u667a\u6167\u4e92\u52d5\u61c9\u7528\u4e2d\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 MMDU\uff0c\u4e00\u500b\u5168\u9762\u7684\u57fa\u6e96\u6e2c\u8a66\uff0c\u4ee5\u53ca MMDU-45k\uff0c\u4e00\u500b\u5927\u898f\u6a21\u7684\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u96c6\uff0c\u65e8\u5728\u8a55\u4f30\u548c\u63d0\u5347 LVLMs \u5728\u591a\u8f2a\u63db\u548c\u591a\u5f71\u50cf\u5c0d\u8a71\u4e2d\u7684\u80fd\u529b\u3002\u6211\u5011\u63a1\u7528\u7fa4\u96c6\u6f14\u7b97\u6cd5\u5f9e\u958b\u6e90\u7dad\u57fa\u767e\u79d1\u4e2d\u627e\u51fa\u76f8\u95dc\u7684\u5f71\u50cf\u548c\u6587\u5b57\u63cf\u8ff0\uff0c\u4e26\u5728 GPT-4o \u6a21\u578b\u7684\u5354\u52a9\u4e0b\uff0c\u7531\u4eba\u5de5\u8a3b\u89e3\u8005\u5efa\u69cb\u554f\u7b54\u914d\u5c0d\u3002MMDU \u6700\u591a\u6709 18k \u500b\u5f71\u50cf + \u6587\u5b57\u7b26\u865f\u300120 \u500b\u5f71\u50cf\u548c 27 \u500b\u8f2a\u63db\uff0c\u9019\u81f3\u5c11\u6bd4\u5148\u524d\u7684\u57fa\u6e96\u6e2c\u8a66\u9577 5 \u500d\uff0c\u4e26\u5c0d\u76ee\u524d\u7684 LVLMs \u69cb\u6210\u6311\u6230\u3002\u6211\u5011\u4f7f\u7528 MMDU \u5c0d 15 \u500b\u5177\u4ee3\u8868\u6027\u7684 LVLMs \u9032\u884c\u6df1\u5165\u5206\u6790\uff0c\u767c\u73fe\u958b\u6e90 LVLMs \u7531\u65bc\u5c0d\u8a71\u5f0f\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u6709\u9650\uff0c\u800c\u843d\u5f8c\u65bc\u9589\u6e90\u5c0d\u61c9\u7a0b\u5f0f\u3002\u6211\u5011\u8b49\u660e\u4e86\u5728 MMDU-45k \u4e0a\u5fae\u8abf\u958b\u6e90 LVLMs \u53ef\u4ee5\u986f\u8457\u89e3\u6c7a\u6b64\u5dee\u8ddd\uff0c\u7522\u751f\u66f4\u9577\u4e14\u66f4\u6e96\u78ba\u7684\u5c0d\u8a71\uff0c\u4e26\u63d0\u5347 MMDU \u548c\u73fe\u6709\u57fa\u6e96\u6e2c\u8a66\u7684\u5206\u6578 (MMStar\uff1a+1.1%\uff0cMathVista\uff1a+1.5%\uff0cChartQA\uff1a+1.2%)\u3002\u6211\u5011\u7684\u8ca2\u737b\u70ba\u7e2e\u5c0f\u76ee\u524d LVLM \u6a21\u578b\u8207\u73fe\u5be6\u4e16\u754c\u61c9\u7528\u9700\u6c42\u4e4b\u9593\u7684\u5dee\u8ddd\u92ea\u8def\u3002\u9019\u500b\u5c08\u6848\u53ef\u5728 https://github.com/Liuziyu77/MMDU \u53d6\u5f97\u3002", "author": "Ziyu Liu et.al.", "authors": "Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, Jiaqi Wang", "id": "2406.11833v1", "paper_url": "http://arxiv.org/abs/2406.11833v1", "repo": "https://github.com/liuziyu77/mmdu"}}