{"2410.10360": {"publish_time": "2024-10-14", "title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning", "paper_summary": "Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, due to potential conflicts between internal and external knowledge, as\nwell as retrieval noise, LLMs often struggle to effectively integrate external\nevidence, leading to a decline in performance. Although existing methods\nattempt to tackle these challenges, they often struggle to strike a balance\nbetween model adherence and robustness, resulting in significant learning\nvariance. Inspired by human cognitive processes, we propose Parenting, a novel\nframework that decouples adherence and robustness within the parameter space of\nLLMs. Specifically, Parenting utilizes a key parameter mining method based on\nforward activation gain to identify and isolate the crucial parameter units\nthat are strongly linked to adherence and robustness. Then, Parenting employs a\ntype-guided tailored tuning strategy, applying specific and appropriate\nfine-tuning methods to parameter units representing different capabilities,\naiming to achieve a balanced enhancement of adherence and robustness. Extensive\nexperiments on various datasets and models validate the effectiveness and\ngeneralizability of our methods.", "paper_summary_zh": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5e7b\u89c9\u751f\u6210\u548c\u77e5\u8bc6\u8fc7\u65f6\u65b9\u9762\u9762\u4e34\u7684\u95ee\u9898\uff0c\u65b9\u6cd5\u662f\u7ed3\u5408\u5916\u90e8\u68c0\u7d22\u7684\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5185\u90e8\u548c\u5916\u90e8\u77e5\u8bc6\u4e4b\u95f4\u6f5c\u5728\u7684\u51b2\u7a81\uff0c\u4ee5\u53ca\u68c0\u7d22\u566a\u58f0\uff0cLLM \u7ecf\u5e38\u96be\u4ee5\u6709\u6548\u5730\u6574\u5408\u5916\u90e8\u8bc1\u636e\uff0c\u4ece\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u5b83\u4eec\u5f80\u5f80\u96be\u4ee5\u5728\u6a21\u578b\u7684\u4f9d\u4ece\u6027\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4ece\u800c\u5bfc\u81f4\u4e25\u91cd\u7684\u5b66\u4e60\u5dee\u5f02\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Parenting\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u5728 LLM \u7684\u53c2\u6570\u7a7a\u95f4\u5185\u89e3\u8026\u4e86\u4f9d\u4ece\u6027\u548c\u9c81\u68d2\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0cParenting \u5229\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u5411\u6fc0\u6d3b\u589e\u76ca\u7684\u5173\u952e\u53c2\u6570\u6316\u6398\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u9694\u79bb\u4e0e\u4f9d\u4ece\u6027\u548c\u9c81\u68d2\u6027\u5bc6\u5207\u76f8\u5173\u7684\u5173\u952e\u53c2\u6570\u5355\u5143\u3002\u7136\u540e\uff0cParenting \u91c7\u7528\u7c7b\u578b\u6307\u5bfc\u7684\u5b9a\u5236\u8c03\u6574\u7b56\u7565\uff0c\u5bf9\u4ee3\u8868\u4e0d\u540c\u529f\u80fd\u7684\u53c2\u6570\u5355\u5143\u5e94\u7528\u7279\u5b9a\u4e14\u9002\u5f53\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u4f9d\u4ece\u6027\u548c\u9c81\u68d2\u6027\u7684\u5e73\u8861\u589e\u5f3a\u3002\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002", "author": "Yongxin Xu et.al.", "authors": "Yongxin Xu, Ruizhe Zhang, Xinke Jiang, Yujie Feng, Yuzhen Xiao, Xinyu Ma, Runchuan Zhu, Xu Chu, Junfeng Zhao, Yasha Wang", "id": "2410.10360v1", "paper_url": "http://arxiv.org/abs/2410.10360v1", "repo": "null"}}