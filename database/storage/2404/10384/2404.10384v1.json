{"2404.10384": {"publish_time": "2024-04-16", "title": "Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering", "paper_summary": "Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform\nsurprisingly well and outperform human experts on many tasks. However, in many\ndomain-specific evaluations, these LLMs often suffer from hallucination\nproblems due to insufficient training of relevant corpus. Furthermore,\nfine-tuning large models may face problems such as the LLMs are not open source\nor the construction of high-quality domain instruction is difficult. Therefore,\nstructured knowledge databases such as knowledge graph can better provide\ndomain background knowledge for LLMs and make full use of the reasoning and\nanalysis capabilities of LLMs. In some previous works, LLM was called multiple\ntimes to determine whether the current triplet was suitable for inclusion in\nthe subgraph when retrieving subgraphs through a question. Especially for the\nquestion that require a multi-hop reasoning path, frequent calls to LLM will\nconsume a lot of computing power. Moreover, when choosing the reasoning path,\nLLM will be called once for each step, and if one of the steps is selected\nincorrectly, it will lead to the accumulation of errors in the following steps.\nIn this paper, we integrated and optimized a pipeline for selecting reasoning\npaths from KG based on LLM, which can reduce the dependency on LLM. In\naddition, we propose a simple and effective subgraph retrieval method based on\nchain of thought (CoT) and page rank which can returns the paths most likely to\ncontain the answer. We conduct experiments on three datasets: GenMedGPT-5k\n[14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using\nfewer LLM calls can achieve the same results as previous SOTAs models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f8b\u5982 GPT3.5\u3001GPT4 \u548c LLAMA2 \u5728\u8a31\u591a\u4efb\u52d9\u4e2d\u8868\u73fe\u5f97\u975e\u5e38\u597d\uff0c\u4e26\u4e14\u512a\u65bc\u4eba\u985e\u5c08\u5bb6\u3002\u7136\u800c\uff0c\u5728\u8a31\u591a\u7279\u5b9a\u9818\u57df\u7684\u8a55\u4f30\u4e2d\uff0c\u9019\u4e9b LLM \u7531\u65bc\u76f8\u95dc\u8a9e\u6599\u8a13\u7df4\u4e0d\u8db3\uff0c\u5e38\u5e38\u6703\u51fa\u73fe\u5e7b\u89ba\u554f\u984c\u3002\u6b64\u5916\uff0c\u5fae\u8abf\u5927\u578b\u6a21\u578b\u53ef\u80fd\u6703\u9762\u81e8\u554f\u984c\uff0c\u4f8b\u5982 LLM \u4e0d\u662f\u958b\u6e90\u7684\uff0c\u6216\u8005\u96e3\u4ee5\u5efa\u69cb\u9ad8\u54c1\u8cea\u7684\u9818\u57df\u6307\u4ee4\u3002\u56e0\u6b64\uff0c\u7d50\u69cb\u5316\u77e5\u8b58\u8cc7\u6599\u5eab\uff0c\u4f8b\u5982\u77e5\u8b58\u5716\u8b5c\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u70ba LLM \u63d0\u4f9b\u9818\u57df\u80cc\u666f\u77e5\u8b58\uff0c\u4e26\u5145\u5206\u5229\u7528 LLM \u7684\u63a8\u7406\u548c\u5206\u6790\u80fd\u529b\u3002\u5728\u4e00\u4e9b\u5148\u524d\u7684\u7814\u7a76\u4e2d\uff0c\u5728\u900f\u904e\u554f\u984c\u6aa2\u7d22\u5b50\u5716\u6642\uff0cLLM \u88ab\u547c\u53eb\u591a\u6b21\u4ee5\u78ba\u5b9a\u76ee\u524d\u7684\u5143\u7d44\u662f\u5426\u9069\u5408\u7d0d\u5165\u5b50\u5716\u4e2d\u3002\u7279\u5225\u662f\u5c0d\u65bc\u9700\u8981\u591a\u8df3\u63a8\u7406\u8def\u5f91\u7684\u554f\u984c\uff0c\u983b\u7e41\u547c\u53eb LLM \u5c07\u6703\u6d88\u8017\u5927\u91cf\u7684\u904b\u7b97\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5728\u9078\u64c7\u63a8\u7406\u8def\u5f91\u6642\uff0cLLM \u5c07\u6703\u88ab\u547c\u53eb\u4e00\u6b21\u4ee5\u57f7\u884c\u6bcf\u500b\u6b65\u9a5f\uff0c\u5982\u679c\u5176\u4e2d\u4e00\u500b\u6b65\u9a5f\u88ab\u932f\u8aa4\u9078\u64c7\uff0c\u5c07\u6703\u5c0e\u81f4\u5f8c\u7e8c\u6b65\u9a5f\u4e2d\u932f\u8aa4\u7684\u7d2f\u7a4d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6574\u5408\u4e26\u6700\u4f73\u5316\u4e86\u4e00\u500b\u57fa\u65bc LLM \u5f9e KG \u4e2d\u9078\u64c7\u63a8\u7406\u8def\u5f91\u7684\u7ba1\u9053\uff0c\u9019\u53ef\u4ee5\u6e1b\u5c11\u5c0d LLM \u7684\u4f9d\u8cf4\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u601d\u8003\u93c8 (CoT) \u548c PageRank \u7684\u7c21\u55ae\u4e14\u6709\u6548\u5b50\u5716\u6aa2\u7d22\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u56de\u50b3\u6700\u6709\u53ef\u80fd\u5305\u542b\u7b54\u6848\u7684\u8def\u5f91\u3002\u6211\u5011\u5728\u4e09\u500b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5be6\u9a57\uff1aGenMedGPT-5k [14]\u3001WebQuestions [2] \u548c CMCQA [21]\u3002\u6700\u5f8c\uff0cRoK \u53ef\u4ee5\u8b49\u660e\u4f7f\u7528\u8f03\u5c11\u7684 LLM \u547c\u53eb\u53ef\u4ee5\u9054\u5230\u8207\u5148\u524d\u7684 SOTAs \u6a21\u578b\u76f8\u540c\u7684\u6548\u679c\u3002", "author": "Yuqi Wang et.al.", "authors": "Yuqi Wang, Boran Jiang, Yi Luo, Dawei He, Peng Cheng, Liangcai Gao", "id": "2404.10384v1", "paper_url": "http://arxiv.org/abs/2404.10384v1", "repo": "null"}}