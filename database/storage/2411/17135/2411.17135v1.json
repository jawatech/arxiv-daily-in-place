{"2411.17135": {"publish_time": "2024-11-26", "title": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided Reward Ensemble", "paper_summary": "Employing large language models (LLMs) to enable embodied agents has become\npopular, yet it presents several limitations in practice. In this work, rather\nthan using LLMs directly as agents, we explore their use as tools for embodied\nagent learning. Specifically, to train separate agents via offline\nreinforcement learning (RL), an LLM is used to provide dense reward feedback on\nindividual actions in training datasets. In doing so, we present a\nconsistency-guided reward ensemble framework (CoREN), designed for tackling\ndifficulties in grounding LLM-generated estimates to the target environment\ndomain. The framework employs an adaptive ensemble of spatio-temporally\nconsistent rewards to derive domain-grounded rewards in the training datasets,\nthus enabling effective offline learning of embodied agents in different\nenvironment domains. Experiments with the VirtualHome benchmark demonstrate\nthat CoREN significantly outperforms other offline RL agents, and it also\nachieves comparable performance to state-of-the-art LLM-based agents with 8B\nparameters, despite CoREN having only 117M parameters for the agent policy\nnetwork and using LLMs only for training.", "paper_summary_zh": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u542f\u7528\u5177\u8eab\u4ee3\u7406\u5df2\u53d8\u5f97\u5f88\u6d41\u884c\uff0c\u4f46\u5b83\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u4e00\u4e9b\u9650\u5236\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e0d\u662f\u76f4\u63a5\u5c06 LLM \u7528\u4f5c\u4ee3\u7406\uff0c\u800c\u662f\u63a2\u7d22\u5c06\u5176\u7528\u4f5c\u5177\u8eab\u4ee3\u7406\u5b66\u4e60\u7684\u5de5\u5177\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u4e86\u901a\u8fc7\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60 (RL) \u8bad\u7ec3\u5355\u72ec\u7684\u4ee3\u7406\uff0cLLM \u7528\u4e8e\u5bf9\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u5404\u4e2a\u52a8\u4f5c\u63d0\u4f9b\u5bc6\u96c6\u7684\u5956\u52b1\u53cd\u9988\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e00\u81f4\u6027\u5f15\u5bfc\u5956\u52b1\u96c6\u6210\u6846\u67b6 (CoREN)\uff0c\u65e8\u5728\u89e3\u51b3\u5c06 LLM \u751f\u6210\u7684\u4f30\u8ba1\u4e0e\u76ee\u6807\u73af\u5883\u57df\u8054\u7cfb\u8d77\u6765\u65f6\u7684\u56f0\u96be\u3002\u8be5\u6846\u67b6\u91c7\u7528\u65f6\u7a7a\u4e00\u81f4\u5956\u52b1\u7684\u81ea\u9002\u5e94\u96c6\u6210\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u63a8\u5bfc\u51fa\u57df\u57fa\u7840\u5956\u52b1\uff0c\u4ece\u800c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u73af\u5883\u57df\u4e2d\u6709\u6548\u5730\u79bb\u7ebf\u5b66\u4e60\u5177\u8eab\u4ee3\u7406\u3002\u4f7f\u7528 VirtualHome \u57fa\u51c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoREN \u660e\u663e\u4f18\u4e8e\u5176\u4ed6\u79bb\u7ebf RL \u4ee3\u7406\uff0c\u5e76\u4e14\u5c3d\u7ba1 CoREN \u7684\u4ee3\u7406\u7b56\u7565\u7f51\u7edc\u53ea\u6709 1.17 \u4ebf\u4e2a\u53c2\u6570\uff0c\u5e76\u4e14\u53ea\u5c06 LLM \u7528\u4e8e\u8bad\u7ec3\uff0c\u4f46\u5b83\u4e5f\u5b9e\u73b0\u4e86\u4e0e\u57fa\u4e8e LLM \u7684\u6700\u5148\u8fdb\u4ee3\u7406\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540e\u8005\u6709 80 \u4ebf\u4e2a\u53c2\u6570\u3002", "author": "Yujeong Lee et.al.", "authors": "Yujeong Lee, Sangwoo Shin, Wei-Jin Park, Honguk Woo", "id": "2411.17135v1", "paper_url": "http://arxiv.org/abs/2411.17135v1", "repo": "null"}}