{"2410.12130": {"publish_time": "2024-10-16", "title": "Iter-AHMCL: Alleviate Hallucination for Large Language Model via Iterative Model-level Contrastive Learning", "paper_summary": "The development of Large Language Models (LLMs) has significantly advanced\nvarious AI applications in commercial and scientific research fields, such as\nscientific literature summarization, writing assistance, and knowledge graph\nconstruction. However, a significant challenge is the high risk of\nhallucination during LLM inference, which can lead to security concerns like\nfactual inaccuracies, inconsistent information, and fabricated content. To\ntackle this issue, it is essential to develop effective methods for reducing\nhallucination while maintaining the original capabilities of the LLM. This\npaper introduces a novel approach called Iterative Model-level Contrastive\nLearning (Iter-AHMCL) to address hallucination. This method modifies the\nrepresentation layers of pre-trained LLMs by using contrastive `positive' and\n`negative' models, trained on data with and without hallucinations. By\nleveraging the differences between these two models, we create a more\nstraightforward pathway to eliminate hallucinations, and the iterative nature\nof contrastive learning further enhances performance. Experimental validation\non four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen)\nfinetuning with a specially designed dataset shows that our approach achieves\nan average improvement of 10.1 points on the TruthfulQA benchmark.\nComprehensive experiments demonstrate the effectiveness of Iter-AHMCL in\nreducing hallucination while maintaining the general capabilities of LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u767c\u5c55\u5728\u5546\u696d\u548c\u79d1\u5b78\u7814\u7a76\u9818\u57df\u986f\u8457\u63a8\u52d5\u4e86\u5404\u7a2e AI \u61c9\u7528\uff0c\u4f8b\u5982\u79d1\u5b78\u6587\u737b\u6458\u8981\u3001\u5beb\u4f5c\u8f14\u52a9\u548c\u77e5\u8b58\u5716\u8b5c\u5efa\u69cb\u3002\u7136\u800c\uff0c\u4e00\u500b\u91cd\u5927\u7684\u6311\u6230\u662f LLM \u63a8\u8ad6\u4e2d\u5e7b\u89ba\u7684\u9ad8\u98a8\u96aa\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u5b89\u5168\u554f\u984c\uff0c\u4f8b\u5982\u4e8b\u5be6\u4e0d\u6b63\u78ba\u3001\u8cc7\u8a0a\u4e0d\u4e00\u81f4\u548c\u634f\u9020\u5167\u5bb9\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u958b\u767c\u6709\u6548\u7684\u65b9\u6cd5\u4f86\u6e1b\u5c11\u5e7b\u89ba\uff0c\u540c\u6642\u4fdd\u6301 LLM \u7684\u539f\u59cb\u529f\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u7a31\u70ba\u53cd\u8986\u6a21\u578b\u5c64\u7d1a\u5c0d\u6bd4\u5b78\u7fd2 (Iter-AHMCL) \u7684\u65b0\u65b9\u6cd5\u4f86\u89e3\u6c7a\u5e7b\u89ba\u3002\u6b64\u65b9\u6cd5\u900f\u904e\u4f7f\u7528\u5c0d\u6bd4\u7684\u300c\u6b63\u5411\u300d\u548c\u300c\u8ca0\u5411\u300d\u6a21\u578b\u4f86\u4fee\u6539\u9810\u5148\u8a13\u7df4\u7684 LLM \u7684\u8868\u793a\u5c64\uff0c\u9019\u4e9b\u6a21\u578b\u662f\u5728\u6709\u548c\u6c92\u6709\u5e7b\u89ba\u7684\u8cc7\u6599\u4e0a\u8a13\u7df4\u7684\u3002\u900f\u904e\u5229\u7528\u9019\u5169\u500b\u6a21\u578b\u4e4b\u9593\u7684\u5dee\u7570\uff0c\u6211\u5011\u5275\u9020\u4e86\u4e00\u689d\u66f4\u76f4\u63a5\u7684\u9014\u5f91\u4f86\u6d88\u9664\u5e7b\u89ba\uff0c\u800c\u5c0d\u6bd4\u5b78\u7fd2\u7684\u8fed\u4ee3\u6027\u8cea\u9032\u4e00\u6b65\u589e\u5f37\u4e86\u6548\u80fd\u3002\u5728\u56db\u500b\u9810\u5148\u8a13\u7df4\u7684\u57fa\u790e LLM (LLaMA2\u3001Alpaca\u3001LLaMA3 \u548c Qwen) \u4e0a\u9032\u884c\u7684\u5be6\u9a57\u9a57\u8b49\uff0c\u4f7f\u7528\u7279\u5225\u8a2d\u8a08\u7684\u8cc7\u6599\u96c6\u9032\u884c\u5fae\u8abf\uff0c\u986f\u793a\u6211\u5011\u7684\u505a\u6cd5\u5728 TruthfulQA \u57fa\u6e96\u4e0a\u5e73\u5747\u63d0\u5347\u4e86 10.1 \u5206\u3002\u5168\u9762\u7684\u5be6\u9a57\u8b49\u660e\u4e86 Iter-AHMCL \u5728\u6e1b\u5c11\u5e7b\u89ba\u7684\u540c\u6642\uff0c\u7dad\u6301 LLM \u4e00\u822c\u529f\u80fd\u7684\u6709\u6548\u6027\u3002", "author": "Huiwen Wu et.al.", "authors": "Huiwen Wu, Xiaohan Li, Xiaogang Xu, Jiafei Wu, Deyi Zhang, Zhe Liu", "id": "2410.12130v1", "paper_url": "http://arxiv.org/abs/2410.12130v1", "repo": "null"}}