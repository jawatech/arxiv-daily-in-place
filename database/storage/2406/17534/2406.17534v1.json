{"2406.17534": {"publish_time": "2024-06-25", "title": "Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification", "paper_summary": "Hierarchical text classification (HTC) is an important task with broad\napplications, while few-shot HTC has gained increasing interest recently. While\nin-context learning (ICL) with large language models (LLMs) has achieved\nsignificant success in few-shot learning, it is not as effective for HTC\nbecause of the expansive hierarchical label sets and extremely-ambiguous\nlabels. In this work, we introduce the first ICL-based framework with LLM for\nfew-shot HTC. We exploit a retrieval database to identify relevant\ndemonstrations, and an iterative policy to manage multi-layer hierarchical\nlabels. Particularly, we equip the retrieval database with HTC label-aware\nrepresentations for the input texts, which is achieved by continual training on\na pretrained language model with masked language modeling (MLM), layer-wise\nclassification (CLS, specifically for HTC), and a novel divergent contrastive\nlearning (DCL, mainly for adjacent semantically-similar labels) objective.\nExperimental results on three benchmark datasets demonstrate superior\nperformance of our method, and we can achieve state-of-the-art results in\nfew-shot HTC.", "paper_summary_zh": "\u5c64\u7d1a\u6587\u672c\u5206\u985e (HTC) \u662f\u4e00\u9805\u91cd\u8981\u7684\u4efb\u52d9\uff0c\u5177\u6709\u5ee3\u6cdb\u7684\u61c9\u7528\uff0c\u800c\u5c11\u6a23\u672c HTC \u8fd1\u4f86\u5099\u53d7\u95dc\u6ce8\u3002\u96d6\u7136\u5177\u5099\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u60c5\u5883\u5167\u5b78\u7fd2 (ICL) \u5df2\u5728\u5c11\u6a23\u672c\u5b78\u7fd2\u4e2d\u7372\u5f97\u986f\u8457\u6210\u529f\uff0c\u4f46\u7531\u65bc\u9f90\u5927\u7684\u5c64\u7d1a\u6a19\u7c64\u7d44\u548c\u6975\u5176\u6a21\u7cca\u7684\u6a19\u7c64\uff0c\u5b83\u5c0d HTC \u800c\u8a00\u4e26\u975e\u90a3\u9ebc\u6709\u6548\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u7b2c\u4e00\u500b\u57fa\u65bc ICL \u7684 LLM \u6846\u67b6\uff0c\u7528\u65bc\u5c11\u6a23\u672c HTC\u3002\u6211\u5011\u5229\u7528\u6aa2\u7d22\u8cc7\u6599\u5eab\u4f86\u8b58\u5225\u76f8\u95dc\u793a\u7bc4\uff0c\u4e26\u63a1\u7528\u53cd\u8986\u904b\u7b97\u653f\u7b56\u4f86\u7ba1\u7406\u591a\u5c64\u5c64\u7d1a\u6a19\u7c64\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u4f7f\u7528 HTC \u6a19\u7c64\u611f\u77e5\u8868\u5fb5\u4f86\u70ba\u6aa2\u7d22\u8cc7\u6599\u5eab\u914d\u5099\u8f38\u5165\u6587\u672c\uff0c\u9019\u53ef\u900f\u904e\u6301\u7e8c\u5728\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u4e0a\u9032\u884c\u906e\u7f69\u8a9e\u8a00\u5efa\u6a21 (MLM)\u3001\u5c64\u7d1a\u5206\u985e (CLS\uff0c\u7279\u5225\u91dd\u5c0d HTC) \u548c\u4e00\u7a2e\u65b0\u7a4e\u7684\u767c\u6563\u5c0d\u6bd4\u5b78\u7fd2 (DCL\uff0c\u4e3b\u8981\u91dd\u5c0d\u76f8\u9130\u7684\u8a9e\u7fa9\u76f8\u4f3c\u6a19\u7c64) \u76ee\u6a19\u4f86\u9054\u6210\u3002\u5728\u4e09\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u512a\u7570\u6548\u80fd\uff0c\u800c\u4e14\u6211\u5011\u53ef\u4ee5\u5728\u5c11\u6a23\u672c HTC \u4e2d\u7372\u5f97\u6700\u5148\u9032\u7684\u7d50\u679c\u3002", "author": "Huiyao Chen et.al.", "authors": "Huiyao Chen, Yu Zhao, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang, Min Zhang", "id": "2406.17534v1", "paper_url": "http://arxiv.org/abs/2406.17534v1", "repo": "null"}}