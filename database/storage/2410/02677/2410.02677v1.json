{"2410.02677": {"publish_time": "2024-10-03", "title": "CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of LLMs", "paper_summary": "To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.", "paper_summary_zh": "<paragraph>\u70ba\u4e86\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u4e0d\u540c\u7684\u6587\u5316\u4e2d\u63d0\u4f9b\u66f4\u591a\u5e6b\u52a9\uff0c\u64c1\u6709\u6709\u6548\u7684\u6587\u5316\u77e5\u8b58\u57fa\u6e96\u4f86\u8861\u91cf\u548c\u8ffd\u8e64\u6211\u5011\u7684\u9032\u5ea6\u81f3\u95dc\u91cd\u8981\u3002\u6709\u6548\u7684\u57fa\u6e96\u9700\u8981\u5f37\u5065\u3001\u591a\u5143\u4e14\u5177\u6709\u6311\u6230\u6027\u3002\u6211\u5011\u63a8\u51fa CulturalBench\uff1a\u4e00\u7d44 1,227 \u500b\u7531\u4eba\u985e\u64b0\u5beb\u4e26\u7d93\u904e\u4eba\u985e\u9a57\u8b49\u7684\u554f\u984c\uff0c\u7528\u65bc\u6709\u6548\u8a55\u4f30 LLM \u7684\u6587\u5316\u77e5\u8b58\uff0c\u6db5\u84cb 45 \u500b\u5168\u7403\u5340\u57df\uff0c\u5305\u62ec\u5b5f\u52a0\u62c9\u3001\u8f9b\u5df4\u5a01\u548c\u79d8\u9b6f\u7b49\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5340\u57df\u3002\u554f\u984c\u2014\u2014\u6bcf\u500b\u554f\u984c\u90fd\u7d93\u904e\u4e94\u4f4d\u7368\u7acb\u8a3b\u89e3\u8005\u9a57\u8b49\u2014\u2014\u6db5\u84cb\u5f9e\u98df\u7269\u504f\u597d\u5230\u554f\u5019\u79ae\u5100\u7b49 17 \u500b\u4e0d\u540c\u7684\u4e3b\u984c\u3002\u6211\u5011\u5728\u5169\u7a2e\u8a2d\u7f6e\u4e0a\u8a55\u4f30\u6a21\u578b\uff1aCulturalBench-Easy \u548c CulturalBench-Hard\uff0c\u5b83\u5011\u7684\u554f\u984c\u76f8\u540c\uff0c\u4f46\u8a62\u554f\u65b9\u5f0f\u4e0d\u540c\u3002\u6211\u5011\u767c\u73fe LLM \u5c0d\u6b64\u985e\u8a2d\u7f6e\u5dee\u7570\u5f88\u654f\u611f\uff08\u4f8b\u5982\uff0cGPT-4o \u7684\u5dee\u7570\u70ba 27.3%\uff09\u3002\u8207\u4eba\u985e\u8868\u73fe\uff08\u6e96\u78ba\u7387 92.6%\uff09\u76f8\u6bd4\uff0cCulturalBench-Hard \u5c0d\u524d\u6cbf LLM \u4f86\u8aaa\u66f4\u5177\u6311\u6230\u6027\uff0c\u8868\u73fe\u6700\u4f73\u7684\u6a21\u578b (GPT-4o) \u50c5\u70ba 61.5%\uff0c\u8868\u73fe\u6700\u5dee\u7684\u6a21\u578b (Llama3-8b) \u70ba 21.4%\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe LLM \u7d93\u5e38\u96e3\u4ee5\u61c9\u5c0d\u5177\u6709\u591a\u500b\u6b63\u78ba\u7b54\u6848\u7684\u68d8\u624b\u554f\u984c\uff08\u4f8b\u5982\uff0c\u4e2d\u570b\u4eba\u901a\u5e38\u4f7f\u7528\u4ec0\u9ebc\u9910\u5177\uff1f\uff09\uff0c\u9019\u8868\u660e\u5b83\u5011\u50be\u5411\u65bc\u6536\u6582\u5230\u4e00\u500b\u7b54\u6848\u3002\u6211\u5011\u7684\u7d50\u679c\u9084\u8868\u660e\uff0cOpenAI GPT-4o \u5728\u8207\u6240\u6709\u5340\u57df\uff08\u5927\u6d0b\u6d32\u9664\u5916\uff09\u76f8\u95dc\u7684\u554f\u984c\u4e0a\u90fd\u660e\u986f\u512a\u65bc\u5176\u4ed6\u5c08\u6709\u548c\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u6240\u6709\u6a21\u578b\u5728\u8207\u5357\u7f8e\u548c\u4e2d\u6771\u76f8\u95dc\u7684\u554f\u984c\u4e0a\u90fd\u6301\u7e8c\u8868\u73fe\u4e0d\u4f73\u3002</paragraph>", "author": "Yu Ying Chiu et.al.", "authors": "Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi", "id": "2410.02677v1", "paper_url": "http://arxiv.org/abs/2410.02677v1", "repo": "null"}}