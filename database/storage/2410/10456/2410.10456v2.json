{"2410.10456": {"publish_time": "2024-10-14", "title": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs", "paper_summary": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE)\narchitectures offer a promising approach to managing computational costs while\nscaling up model parameters. Conventional MoE-based LLMs typically employ\nstatic Top-K routing, which activates a fixed and equal number of experts for\neach token regardless of their significance within the context. In this paper,\nwe propose a novel Ada-K routing strategy that dynamically adjusts the number\nof activated experts for each token, thereby improving the balance between\ncomputational efficiency and model performance. Specifically, our strategy\nincorporates learnable and lightweight allocator modules that decide customized\nexpert resource allocation tailored to the contextual needs for each token.\nThese allocators are designed to be fully pluggable, making it broadly\napplicable across all mainstream MoE-based LLMs. We leverage the Proximal\nPolicy Optimization (PPO) algorithm to facilitate an end-to-end learning\nprocess for this non-differentiable decision-making framework. Extensive\nevaluations on four popular baseline models demonstrate that our Ada-K routing\nmethod significantly outperforms conventional Top-K routing. Compared to Top-K,\nour method achieves over 25% reduction in FLOPs and more than 20% inference\nspeedup while still improving performance across various benchmarks. Moreover,\nthe training of Ada-K is highly efficient. Even for Mixtral-8x22B, a MoE-based\nLLM with more than 140B parameters, the training time is limited to 8 hours.\nDetailed analysis shows that harder tasks, middle layers, and content words\ntend to activate more experts, providing valuable insights for future adaptive\nMoE system designs. Both the training code and model checkpoints will be\npublicly available.", "paper_summary_zh": "<paragraph>\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6642\u4ee3\uff0c\u6df7\u5408\u5c08\u5bb6 (MoE) \u67b6\u69cb\u63d0\u4f9b\u4e86\u4e00\u7a2e\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u64f4\u5145\u6a21\u578b\u53c3\u6578\u7684\u540c\u6642\u7ba1\u7406\u904b\u7b97\u6210\u672c\u3002\u57fa\u65bc MoE \u7684\u50b3\u7d71 LLM \u901a\u5e38\u63a1\u7528\u975c\u614b Top-K \u8def\u7531\uff0c\u7121\u8ad6\u7b26\u865f\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u91cd\u8981\u6027\u5982\u4f55\uff0c\u90fd\u6703\u70ba\u6bcf\u500b\u7b26\u865f\u555f\u7528\u56fa\u5b9a\u4e14\u76f8\u7b49\u7684\u5c08\u5bb6\u6578\u91cf\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684 Ada-K \u8def\u7531\u7b56\u7565\uff0c\u5b83\u6703\u52d5\u614b\u8abf\u6574\u6bcf\u500b\u7b26\u865f\u7684\u5df2\u555f\u7528\u5c08\u5bb6\u6578\u91cf\uff0c\u5f9e\u800c\u6539\u5584\u904b\u7b97\u6548\u7387\u548c\u6a21\u578b\u6548\u80fd\u4e4b\u9593\u7684\u5e73\u8861\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u7b56\u7565\u5305\u542b\u53ef\u5b78\u7fd2\u4e14\u8f15\u91cf\u5316\u7684\u914d\u7f6e\u5668\u6a21\u7d44\uff0c\u9019\u4e9b\u6a21\u7d44\u6703\u91dd\u5c0d\u6bcf\u500b\u7b26\u865f\u7684\u4e0a\u4e0b\u6587\u9700\u6c42\u6c7a\u5b9a\u81ea\u8a02\u7684\u5c08\u5bb6\u8cc7\u6e90\u914d\u7f6e\u3002\u9019\u4e9b\u914d\u7f6e\u5668\u88ab\u8a2d\u8a08\u70ba\u5b8c\u5168\u53ef\u63d2\u5165\uff0c\u4f7f\u5176\u5ee3\u6cdb\u9069\u7528\u65bc\u6240\u6709\u4e3b\u6d41\u7684\u57fa\u65bc MoE \u7684 LLM\u3002\u6211\u5011\u5229\u7528\u8fd1\u7aef\u7b56\u7565\u6700\u4f73\u5316 (PPO) \u6f14\u7b97\u6cd5\u4f86\u4fc3\u9032\u9019\u500b\u4e0d\u53ef\u5fae\u5206\u6c7a\u7b56\u5236\u5b9a\u67b6\u69cb\u7684\u7aef\u5c0d\u7aef\u5b78\u7fd2\u904e\u7a0b\u3002\u5c0d\u56db\u500b\u6d41\u884c\u7684\u57fa\u7dda\u6a21\u578b\u9032\u884c\u5ee3\u6cdb\u8a55\u4f30\uff0c\u8b49\u660e\u6211\u5011\u7684 Ada-K \u8def\u7531\u65b9\u6cd5\u986f\u8457\u512a\u65bc\u50b3\u7d71\u7684 Top-K \u8def\u7531\u3002\u8207 Top-K \u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 FLOP \u65b9\u9762\u6e1b\u5c11\u4e86 25% \u4ee5\u4e0a\uff0c\u63a8\u7406\u901f\u5ea6\u52a0\u5feb\u4e86 20% \u4ee5\u4e0a\uff0c\u540c\u6642\u4ecd\u6539\u5584\u4e86\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0cAda-K \u7684\u8a13\u7df4\u975e\u5e38\u6709\u6548\u7387\u3002\u5373\u4f7f\u5c0d\u65bc Mixtral-8x22B\uff0c\u4e00\u500b\u5177\u6709\u8d85\u904e 140B \u53c3\u6578\u7684\u57fa\u65bc MoE \u7684 LLM\uff0c\u8a13\u7df4\u6642\u9593\u4e5f\u9650\u5236\u5728 8 \u5c0f\u6642\u5167\u3002\u8a73\u7d30\u5206\u6790\u986f\u793a\uff0c\u8f03\u56f0\u96e3\u7684\u4efb\u52d9\u3001\u4e2d\u9593\u5c64\u548c\u5167\u5bb9\u8a5e\u50be\u5411\u65bc\u555f\u7528\u66f4\u591a\u5c08\u5bb6\uff0c\u9019\u70ba\u672a\u4f86\u7684\u81ea\u9069\u61c9 MoE \u7cfb\u7d71\u8a2d\u8a08\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002\u8a13\u7df4\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u6aa2\u67e5\u9ede\u5c07\u516c\u958b\u63d0\u4f9b\u3002</paragraph>", "author": "Tongtian Yue et.al.", "authors": "Tongtian Yue, Longteng Guo, Jie Cheng, Xuange Gao, Jing Liu", "id": "2410.10456v2", "paper_url": "http://arxiv.org/abs/2410.10456v2", "repo": "null"}}