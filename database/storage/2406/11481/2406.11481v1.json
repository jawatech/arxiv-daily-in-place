{"2406.11481": {"publish_time": "2024-06-17", "title": "Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms", "paper_summary": "Reinforcement Learning (RL) serves as a versatile framework for sequential\ndecision-making, finding applications across diverse domains such as robotics,\nautonomous driving, recommendation systems, supply chain optimization, biology,\nmechanics, and finance. The primary objective in these applications is to\nmaximize the average reward. Real-world scenarios often necessitate adherence\nto specific constraints during the learning process.\n  This monograph focuses on the exploration of various model-based and\nmodel-free approaches for Constrained RL within the context of average reward\nMarkov Decision Processes (MDPs). The investigation commences with an\nexamination of model-based strategies, delving into two foundational methods -\noptimism in the face of uncertainty and posterior sampling. Subsequently, the\ndiscussion transitions to parametrized model-free approaches, where the\nprimal-dual policy gradient-based algorithm is explored as a solution for\nconstrained MDPs. The monograph provides regret guarantees and analyzes\nconstraint violation for each of the discussed setups.\n  For the above exploration, we assume the underlying MDP to be ergodic.\nFurther, this monograph extends its discussion to encompass results tailored\nfor weakly communicating MDPs, thereby broadening the scope of its findings and\ntheir relevance to a wider range of practical scenarios.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u662f\u4e00\u500b\u901a\u7528\u7684\u6846\u67b6\uff0c\u53ef\u7528\u65bc\u9806\u5e8f\u6c7a\u7b56\u5236\u5b9a\uff0c\u4e26\u5728\u6a5f\u5668\u4eba\u6280\u8853\u3001\u81ea\u52d5\u99d5\u99db\u3001\u63a8\u85a6\u7cfb\u7d71\u3001\u4f9b\u61c9\u93c8\u6700\u4f73\u5316\u3001\u751f\u7269\u5b78\u3001\u529b\u5b78\u548c\u91d1\u878d\u7b49\u4e0d\u540c\u9818\u57df\u4e2d\u627e\u5230\u61c9\u7528\u3002\u9019\u4e9b\u61c9\u7528\u4e2d\u7684\u4e3b\u8981\u76ee\u6a19\u662f\u6700\u5927\u5316\u5e73\u5747\u734e\u52f5\u3002\u73fe\u5be6\u4e16\u754c\u7684\u5834\u666f\u901a\u5e38\u9700\u8981\u5728\u5b78\u7fd2\u904e\u7a0b\u4e2d\u9075\u5b88\u7279\u5b9a\u7684\u7d04\u675f\u3002\n\u672c\u5c08\u984c\u5074\u91cd\u65bc\u63a2\u7d22\u5404\u7a2e\u57fa\u65bc\u6a21\u578b\u548c\u7121\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u5728\u5e73\u5747\u734e\u52f5\u99ac\u53ef\u592b\u6c7a\u7b56\u904e\u7a0b (MDP) \u7684\u80cc\u666f\u4e0b\u9032\u884c\u7d04\u675f RL\u3002\u8abf\u67e5\u5f9e\u5c0d\u57fa\u65bc\u6a21\u578b\u7684\u7b56\u7565\u7684\u6aa2\u67e5\u958b\u59cb\uff0c\u6df1\u5165\u63a2\u8a0e\u5169\u7a2e\u57fa\u790e\u65b9\u6cd5\u2014\u2014\u9762\u5c0d\u4e0d\u78ba\u5b9a\u6027\u548c\u5f8c\u9a57\u62bd\u6a23\u7684\u6a02\u89c0\u4e3b\u7fa9\u3002\u96a8\u5f8c\uff0c\u8a0e\u8ad6\u904e\u6e21\u5230\u53c3\u6578\u5316\u7684\u7121\u6a21\u578b\u65b9\u6cd5\uff0c\u5176\u4e2d\u57fa\u65bc\u539f\u59cb\u5c0d\u5076\u7b56\u7565\u68af\u5ea6\u7684\u6f14\u7b97\u6cd5\u88ab\u63a2\u7d22\u70ba\u7d04\u675f MDP \u7684\u89e3\u6c7a\u65b9\u6848\u3002\u672c\u5c08\u984c\u63d0\u4f9b\u4e86\u5f8c\u6094\u4fdd\u8b49\uff0c\u4e26\u5206\u6790\u4e86\u6bcf\u500b\u8a0e\u8ad6\u8a2d\u5b9a\u7684\u7d04\u675f\u9055\u898f\u3002\n\u5c0d\u65bc\u4e0a\u8ff0\u63a2\u7d22\uff0c\u6211\u5011\u5047\u8a2d\u57fa\u790e MDP \u662f ergodic\u3002\u6b64\u5916\uff0c\u672c\u5c08\u984c\u5c07\u5176\u8a0e\u8ad6\u64f4\u5c55\u5230\u5305\u542b\u91dd\u5c0d\u5f31\u901a\u4fe1 MDP \u91cf\u8eab\u6253\u9020\u7684\u7d50\u679c\uff0c\u5f9e\u800c\u64f4\u5927\u4e86\u5176\u767c\u73fe\u7684\u7bc4\u570d\u53ca\u5176\u8207\u66f4\u5ee3\u6cdb\u5be6\u969b\u5834\u666f\u76f8\u95dc\u6027\u3002", "author": "Vaneet Aggarwal et.al.", "authors": "Vaneet Aggarwal, Washim Uddin Mondal, Qinbo Bai", "id": "2406.11481v1", "paper_url": "http://arxiv.org/abs/2406.11481v1", "repo": "null"}}