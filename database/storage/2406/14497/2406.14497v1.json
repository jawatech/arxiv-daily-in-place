{"2406.14497": {"publish_time": "2024-06-20", "title": "CodeRAG-Bench: Can Retrieval Augment Code Generation?", "paper_summary": "While language models (LMs) have proven remarkably adept at generating code,\nmany programs are challenging for LMs to generate using their parametric\nknowledge alone. Providing external contexts such as library documentation can\nfacilitate generating accurate and functional code. Despite the success of\nretrieval-augmented generation (RAG) in various text-oriented tasks, its\npotential for improving code generation remains under-explored. In this work,\nwe conduct a systematic, large-scale analysis by asking: in what scenarios can\nretrieval benefit code generation models? and what challenges remain? We first\ncurate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three\ncategories of code generation tasks, including basic programming, open-domain,\nand repository-level problems. We aggregate documents from five sources for\nmodels to retrieve contexts: competition solutions, online tutorials, library\ndocumentation, StackOverflow posts, and GitHub repositories. We examine\ntop-performing models on CodeRAG-Bench by providing contexts retrieved from one\nor multiple sources. While notable gains are made in final code generation by\nretrieving high-quality contexts across various settings, our analysis reveals\nroom for improvement -- current retrievers still struggle to fetch useful\ncontexts especially with limited lexical overlap, and generators fail to\nimprove with limited context lengths or abilities to integrate additional\ncontexts. We hope CodeRAG-Bench serves as an effective testbed to encourage\nfurther development of advanced code-oriented RAG methods.", "paper_summary_zh": "\u5118\u7ba1\u8a9e\u8a00\u6a21\u578b (LM) \u5df2\u8b49\u660e\u5728\u751f\u6210\u7a0b\u5f0f\u78bc\u65b9\u9762\u975e\u5e38\u719f\u7df4\uff0c\u4f46\u8a31\u591a\u7a0b\u5f0f\u5c0d LM \u800c\u8a00\uff0c\u50c5\u4f7f\u7528\u5176\u53c3\u6578\u5316\u77e5\u8b58\u751f\u6210\u7a0b\u5f0f\u78bc\u4ecd\u5177\u6311\u6230\u6027\u3002\u63d0\u4f9b\u5916\u90e8\u5167\u5bb9\uff08\u4f8b\u5982\u51fd\u5f0f\u5eab\u6587\u4ef6\uff09\u6709\u52a9\u65bc\u7522\u751f\u6e96\u78ba\u4e14\u5177\u529f\u80fd\u6027\u7684\u7a0b\u5f0f\u78bc\u3002\u5118\u7ba1\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u5728\u5404\u7a2e\u4ee5\u6587\u5b57\u70ba\u5c0e\u5411\u7684\u4efb\u52d9\u4e2d\u7372\u5f97\u6210\u529f\uff0c\u4f46\u5176\u6539\u5584\u7a0b\u5f0f\u78bc\u751f\u6210\u7684\u6f5b\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u900f\u904e\u63d0\u554f\u9032\u884c\u7cfb\u7d71\u6027\u3001\u5927\u898f\u6a21\u7684\u5206\u6790\uff1a\u5728\u54ea\u4e9b\u60c5\u6cc1\u4e0b\u6aa2\u7d22\u6709\u52a9\u65bc\u7a0b\u5f0f\u78bc\u751f\u6210\u6a21\u578b\uff1f\u9084\u6709\u54ea\u4e9b\u6311\u6230\uff1f\u6211\u5011\u9996\u5148\u7b56\u5283\u4e86\u4e00\u500b\u5168\u9762\u7684\u8a55\u91cf\u57fa\u6e96 CodeRAG-Bench\uff0c\u5176\u4e2d\u5305\u542b\u4e09\u985e\u7a0b\u5f0f\u78bc\u751f\u6210\u4efb\u52d9\uff0c\u5305\u62ec\u57fa\u672c\u7a0b\u5f0f\u8a2d\u8a08\u3001\u958b\u653e\u9818\u57df\u548c\u5132\u5b58\u5eab\u7d1a\u5225\u554f\u984c\u3002\u6211\u5011\u5f9e\u4e94\u500b\u4f86\u6e90\u5f59\u7e3d\u6587\u4ef6\uff0c\u4f9b\u6a21\u578b\u6aa2\u7d22\u5167\u5bb9\uff1a\u7af6\u8cfd\u89e3\u7b54\u3001\u7dda\u4e0a\u6559\u5b78\u3001\u51fd\u5f0f\u5eab\u6587\u4ef6\u3001StackOverflow \u6587\u7ae0\u548c GitHub \u5132\u5b58\u5eab\u3002\u6211\u5011\u900f\u904e\u63d0\u4f9b\u5f9e\u4e00\u500b\u6216\u591a\u500b\u4f86\u6e90\u6aa2\u7d22\u7684\u5167\u5bb9\uff0c\u6aa2\u8996 CodeRAG-Bench \u4e0a\u6548\u80fd\u6700\u4f73\u7684\u6a21\u578b\u3002\u5118\u7ba1\u900f\u904e\u5728\u5404\u7a2e\u8a2d\u5b9a\u4e2d\u6aa2\u7d22\u9ad8\u54c1\u8cea\u5167\u5bb9\uff0c\u5728\u6700\u7d42\u7a0b\u5f0f\u78bc\u751f\u6210\u65b9\u9762\u53d6\u5f97\u986f\u8457\u9032\u5c55\uff0c\u4f46\u6211\u5011\u7684\u5206\u6790\u986f\u793a\u4ecd\u6709\u9032\u6b65\u7a7a\u9593\u2014\u2014\u76ee\u524d\u7684\u6aa2\u7d22\u5668\u4ecd\u96e3\u4ee5\u64f7\u53d6\u6709\u7528\u7684\u5167\u5bb9\uff0c\u7279\u5225\u662f\u5728\u8a5e\u5f59\u91cd\u758a\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\uff0c\u800c\u4e14\u751f\u6210\u5668\u7121\u6cd5\u5728\u5167\u5bb9\u9577\u5ea6\u6709\u9650\u6216\u6574\u5408\u984d\u5916\u5167\u5bb9\u7684\u80fd\u529b\u4e0b\u9032\u884c\u6539\u5584\u3002\u6211\u5011\u5e0c\u671b CodeRAG-Bench \u80fd\u4f5c\u70ba\u4e00\u500b\u6709\u6548\u7684\u6e2c\u8a66\u5e73\u53f0\uff0c\u4ee5\u9f13\u52f5\u9032\u4e00\u6b65\u958b\u767c\u9032\u968e\u7684\u7a0b\u5f0f\u78bc\u5c0e\u5411 RAG \u65b9\u6cd5\u3002", "author": "Zora Zhiruo Wang et.al.", "authors": "Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing Xie, Graham Neubig, Daniel Fried", "id": "2406.14497v1", "paper_url": "http://arxiv.org/abs/2406.14497v1", "repo": "null"}}