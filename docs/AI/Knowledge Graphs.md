
### Knowledge Graphs
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-16**|**FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression**|Zhenheng Tang et.al.|[2410.12707v1](http://arxiv.org/abs/2410.12707v1)|null|
|**2024-10-16**|**The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph**|Minghao Wu et.al.|[2410.12458v1](http://arxiv.org/abs/2410.12458v1)|null|
|**2024-10-16**|**PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking**|Markus J. Buehler et.al.|[2410.12375v1](http://arxiv.org/abs/2410.12375v1)|null|
|**2024-10-16**|**Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large Language Models and Knowledge Graphs**|Lei Sun et.al.|[2410.12298v1](http://arxiv.org/abs/2410.12298v1)|null|
|**2024-10-16**|**Towards LLM-based Cognitive Models of Students with Misconceptions**|Shashank Sonkar et.al.|[2410.12294v1](http://arxiv.org/abs/2410.12294v1)|null|
|**2024-10-16**|**Comprehending Knowledge Graphs with Large Language Models for Recommender Systems**|Ziqiang Cui et.al.|[2410.12229v1](http://arxiv.org/abs/2410.12229v1)|null|
|**2024-10-16**|**Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with Large Language Models for Multi-Behavior Recommendations**|Luyi Ma et.al.|[2410.12228v1](http://arxiv.org/abs/2410.12228v1)|null|
|**2024-10-16**|**Iter-AHMCL: Alleviate Hallucination for Large Language Model via Iterative Model-level Contrastive Learning**|Huiwen Wu et.al.|[2410.12130v1](http://arxiv.org/abs/2410.12130v1)|null|
|**2024-10-15**|**Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning**|Guangxin Su et.al.|[2410.12096v1](http://arxiv.org/abs/2410.12096v1)|null|
|**2024-10-15**|**A Survey on Deep Tabular Learning**|Shriyank Somvanshi et.al.|[2410.12034v1](http://arxiv.org/abs/2410.12034v1)|null|
|**2024-10-15**|**Causal Reasoning in Large Language Models: A Knowledge Graph Approach**|Yejin Kim et.al.|[2410.11588v1](http://arxiv.org/abs/2410.11588v1)|null|
|**2024-10-15**|**Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for Drug Development**|Tengfei Ma et.al.|[2410.11550v1](http://arxiv.org/abs/2410.11550v1)|null|
|**2024-10-15**|**AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data**|Xinjie Zhao et.al.|[2410.11531v1](http://arxiv.org/abs/2410.11531v1)|null|
|**2024-10-15**|**Do LLMs Have the Generalization Ability in Conducting Causal Inference?**|Chen Wang et.al.|[2410.11385v1](http://arxiv.org/abs/2410.11385v1)|[link](https://github.com/prayingsociety/ci_bench)|
|**2024-10-15**|**Enhance Graph Alignment for Large Language Models**|Haitong Luo et.al.|[2410.11370v1](http://arxiv.org/abs/2410.11370v1)|null|
|**2024-10-15**|**Unleashing the Power of LLMs as Multi-Modal Encoders for Text and Graph-Structured Data**|Jiacheng Lin et.al.|[2410.11235v1](http://arxiv.org/abs/2410.11235v1)|null|
|**2024-10-15**|**Tree of Attributes Prompt Learning for Vision-Language Models**|Tong Ding et.al.|[2410.11201v1](http://arxiv.org/abs/2410.11201v1)|null|
|**2024-10-14**|**Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs**|Haozhen Zhang et.al.|[2410.11001v1](http://arxiv.org/abs/2410.11001v1)|[link](https://github.com/ulab-uiuc/gor)|
|**2024-10-14**|**NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models**|Yanbiao Ji et.al.|[2410.10743v1](http://arxiv.org/abs/2410.10743v1)|null|
|**2024-10-14**|**GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs**|Yun Zhu et.al.|[2410.10329v2](http://arxiv.org/abs/2410.10329v2)|[link](https://github.com/zhuyun97/graphclip)|
|**2024-10-14**|**Unified Representation of Genomic and Biomedical Concepts through Multi-Task, Multi-Source Contrastive Learning**|Hongyi Yuan et.al.|[2410.10144v1](http://arxiv.org/abs/2410.10144v1)|null|
|**2024-10-14**|**Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?**|Yifan Feng et.al.|[2410.10083v2](http://arxiv.org/abs/2410.10083v2)|[link](https://github.com/imoonlab/llm4hypergraph)|
|**2024-10-13**|**Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent Simulation**|Jiarui Ji et.al.|[2410.09824v1](http://arxiv.org/abs/2410.09824v1)|null|
|**2024-10-13**|**A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based Extract-Generate Model**|Shengxiang Gao et.al.|[2410.09773v1](http://arxiv.org/abs/2410.09773v1)|null|
|**2024-10-13**|**Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG**|Xinxi Chen et.al.|[2410.09699v1](http://arxiv.org/abs/2410.09699v1)|null|
|**2024-10-12**|**LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning**|Jiachun Li et.al.|[2410.09541v1](http://arxiv.org/abs/2410.09541v1)|[link](https://github.com/bugmakerzzz/linked_code)|
|**2024-10-12**|**Text Classification using Graph Convolutional Networks: A Comprehensive Survey**|Syed Mustafa Haider Rizvi et.al.|[2410.09399v1](http://arxiv.org/abs/2410.09399v1)|null|
|**2024-10-12**|**Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation**|Jinyoung Park et.al.|[2410.09350v1](http://arxiv.org/abs/2410.09350v1)|null|
|**2024-10-11**|**Natural Language Counterfactual Explanations for Graphs Using Large Language Models**|Flavio Giorgi et.al.|[2410.09295v1](http://arxiv.org/abs/2410.09295v1)|[link](https://github.com/flaat/llm-graph-cf)|
|**2024-10-11**|**ReasonPlanner: Enhancing Autonomous Planning in Dynamic Environments with Temporal Knowledge Graphs and LLMs**|Minh Pham Dinh et.al.|[2410.09252v1](http://arxiv.org/abs/2410.09252v1)|null|
|**2024-10-11**|**Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware Perspective**|Bo Ni et.al.|[2410.08985v1](http://arxiv.org/abs/2410.08985v1)|null|
|**2024-10-11**|**When Graph meets Multimodal: Benchmarking on Multimodal Attributed Graphs Learning**|Hao Yan et.al.|[2410.09132v1](http://arxiv.org/abs/2410.09132v1)|[link](https://github.com/sktsherlock/atg)|
|**2024-10-11**|**GIVE: Structured Reasoning with Knowledge Graph Inspired Veracity Extrapolation**|Jiashu He et.al.|[2410.08475v1](http://arxiv.org/abs/2410.08475v1)|null|
|**2024-10-10**|**Privately Learning from Graphs with Applications in Fine-tuning Large Language Models**|Haoteng Yin et.al.|[2410.08299v1](http://arxiv.org/abs/2410.08299v1)|[link](https://github.com/graph-com/pvgalm)|
|**2024-10-10**|**Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering**|Yuan Sui et.al.|[2410.08085v1](http://arxiv.org/abs/2410.08085v1)|null|
|**2024-10-10**|**Disease Entity Recognition and Normalization is Improved with Large Language Model Derived Synthetic Normalized Mentions**|Kuleen Sasse et.al.|[2410.07951v1](http://arxiv.org/abs/2410.07951v1)|null|
|**2024-10-10**|**Benchmarking Agentic Workflow Generation**|Shuofei Qiao et.al.|[2410.07869v1](http://arxiv.org/abs/2410.07869v1)|null|
|**2024-10-10**|**KRAG Framework for Enhancing LLMs in the Legal Domain**|Nguyen Ha Thanh et.al.|[2410.07551v1](http://arxiv.org/abs/2410.07551v1)|null|
|**2024-10-10**|**MKGL: Mastery of a Three-Word Language**|Lingbing Guo et.al.|[2410.07526v1](http://arxiv.org/abs/2410.07526v1)|null|
|**2024-10-09**|**InstructG2I: Synthesizing Images from Multimodal Attributed Graphs**|Bowen Jin et.al.|[2410.07157v1](http://arxiv.org/abs/2410.07157v1)|[link](https://github.com/PeterGriffinJin/InstructG2I)|
|**2024-10-09**|**CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages**|Pretam Ray et.al.|[2410.06944v1](http://arxiv.org/abs/2410.06944v1)|null|
|**2024-10-09**|**Tree of Problems: Improving structured problem solving with compositionality**|Armel Zebaze et.al.|[2410.06634v1](http://arxiv.org/abs/2410.06634v1)|null|
|**2024-10-09**|**Multi-Task Program Error Repair and Explanatory Diagnosis**|Zhenyu Xu et.al.|[2410.07271v1](http://arxiv.org/abs/2410.07271v1)|null|
|**2024-10-08**|**Counterfactual Causal Inference in Natural Language with Large Language Models**|GaÃ«l Gendron et.al.|[2410.06392v1](http://arxiv.org/abs/2410.06392v1)|[link](https://github.com/strong-ai-lab/counterfactual-llm-inference)|
|**2024-10-08**|**Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA**|Wenyu Huang et.al.|[2410.06121v1](http://arxiv.org/abs/2410.06121v1)|null|
|**2024-10-08**|**LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs**|Vincent Emonet et.al.|[2410.06062v2](http://arxiv.org/abs/2410.06062v2)|null|
|**2024-10-08**|**Jet Expansions of Residual Computation**|Yihong Chen et.al.|[2410.06024v1](http://arxiv.org/abs/2410.06024v1)|null|
|**2024-10-08**|**A large collection of bioinformatics question-query pairs over federated knowledge graphs: methodology and applications**|Jerven Bolleman et.al.|[2410.06010v1](http://arxiv.org/abs/2410.06010v1)|null|
|**2024-10-08**|**LightRAG: Simple and Fast Retrieval-Augmented Generation**|Zirui Guo et.al.|[2410.05779v1](http://arxiv.org/abs/2410.05779v1)|[link](https://github.com/hkuds/lightrag)|
|**2024-10-08**|**Information Discovery in e-Commerce**|Zhaochun Ren et.al.|[2410.05763v2](http://arxiv.org/abs/2410.05763v2)|null|
|**2024-10-08**|**Vector-ICL: In-context Learning with Continuous Vector Representations**|Yufan Zhuang et.al.|[2410.05629v1](http://arxiv.org/abs/2410.05629v1)|[link](https://github.com/EvanZhuang/vector-icl)|
|**2024-10-07**|**Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives**|Xinliang Frederick Zhang et.al.|[2410.05558v1](http://arxiv.org/abs/2410.05558v1)|null|
|**2024-10-07**|**Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents**|Yuwei Hu et.al.|[2410.05130v1](http://arxiv.org/abs/2410.05130v1)|null|
|**2024-10-07**|**Leverage Knowledge Graph and Large Language Model for Law Article Recommendation: A Case Study of Chinese Criminal Law**|Yongming Chen et.al.|[2410.04949v1](http://arxiv.org/abs/2410.04949v1)|null|
|**2024-10-07**|**GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA**|Xinyu Wang et.al.|[2410.04790v1](http://arxiv.org/abs/2410.04790v1)|null|
|**2024-10-06**|**Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval**|Pengcheng Jiang et.al.|[2410.04585v1](http://arxiv.org/abs/2410.04585v1)|[link](https://github.com/pat-jj/KARE)|
|**2024-10-06**|**Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support**|Abdul Muqtadir et.al.|[2410.10853v1](http://arxiv.org/abs/2410.10853v1)|null|
|**2024-10-04**|**Leveraging Social Determinants of Health in Alzheimer's Research Using LLM-Augmented Literature Mining and Knowledge Graphs**|Tianqi Shang et.al.|[2410.09080v1](http://arxiv.org/abs/2410.09080v1)|null|
|**2024-10-04**|**Empowering Domain-Specific Language Models with Graph-Oriented Databases: A Paradigm Shift in Performance and Model Maintenance**|Ricardo Di Pasquale et.al.|[2410.03867v1](http://arxiv.org/abs/2410.03867v1)|null|
|**2024-10-04**|**GraphRouter: A Graph-based Router for LLM Selections**|Tao Feng et.al.|[2410.03834v1](http://arxiv.org/abs/2410.03834v1)|null|
|**2024-10-04**|**Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing**|Jeongwoo Kang et.al.|[2410.03357v1](http://arxiv.org/abs/2410.03357v1)|[link](https://github.com/Emvista/Meta-XAMR-2024)|
|**2024-10-04**|**Enriching Ontologies with Disjointness Axioms using Large Language Models**|Elias Crum et.al.|[2410.03235v1](http://arxiv.org/abs/2410.03235v1)|[link](https://github.com/n28div/llm-disjointness)|
|**2024-10-04**|**How Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension**|Xinnan Dai et.al.|[2410.05298v1](http://arxiv.org/abs/2410.05298v1)|null|
|**2024-10-03**|**LLMCO2: Advancing Accurate Carbon Footprint Prediction for LLM Inferences**|Zhenxiao Fu et.al.|[2410.02950v1](http://arxiv.org/abs/2410.02950v1)|null|
|**2024-10-03**|**Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization**|Ryan C. Barron et.al.|[2410.02721v1](http://arxiv.org/abs/2410.02721v1)|null|
|**2024-10-03**|**A Schema-aware Logic Reformulation for Graph Reachability**|Davide Di Pierro et.al.|[2410.02533v1](http://arxiv.org/abs/2410.02533v1)|null|
|**2024-10-03**|**Language Models are Graph Learners**|Zhe Xu et.al.|[2410.02296v1](http://arxiv.org/abs/2410.02296v1)|null|
|**2024-10-03**|**GraphIC: A Graph-Based In-Context Example Retrieval Model for Multi-Step Reasoning**|Jiale Fu et.al.|[2410.02203v1](http://arxiv.org/abs/2410.02203v1)|null|
|**2024-10-03**|**G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models**|Zhaoning Yu et.al.|[2410.02198v1](http://arxiv.org/abs/2410.02198v1)|null|
|**2024-10-02**|**FLAG: Financial Long Document Classification via AMR-based GNN**|Bolun "Namir" Xia et.al.|[2410.02024v2](http://arxiv.org/abs/2410.02024v2)|[link](https://github.com/namir0806/flag)|
|**2024-10-02**|**Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in Graph Tasks**|Hamed Firooz et.al.|[2410.01985v1](http://arxiv.org/abs/2410.01985v1)|null|
|**2024-10-02**|**LLM+KG@VLDB'24 Workshop Summary**|Arijit Khan et.al.|[2410.01978v1](http://arxiv.org/abs/2410.01978v1)|null|
|**2024-10-02**|**Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering**|Klaus-Rudolf Kladny et.al.|[2410.01660v1](http://arxiv.org/abs/2410.01660v1)|null|
|**2024-10-02**|**HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation**|Yuntong Hu et.al.|[2410.03761v1](http://arxiv.org/abs/2410.03761v1)|null|
|**2024-10-02**|**LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion**|Dexuan Ding et.al.|[2410.01506v2](http://arxiv.org/abs/2410.01506v2)|null|
|**2024-10-02**|**Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering**|Yu Zhang et.al.|[2410.01401v1](http://arxiv.org/abs/2410.01401v1)|[link](https://github.com/EchoDreamer/Q-KGR)|
|**2024-10-02**|**Unveiling Language Skills under Circuits**|Hang Chen et.al.|[2410.01334v1](http://arxiv.org/abs/2410.01334v1)|[link](https://github.com/zodiark-ch/language-skill-of-llms)|
|**2024-10-01**|**From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems**|Ali Mohammadjafari et.al.|[2410.01066v1](http://arxiv.org/abs/2410.01066v1)|null|
|**2024-09-30**|**GUNDAM: Aligning Large Language Models with Graph Understanding**|Sheng Ouyang et.al.|[2409.20053v2](http://arxiv.org/abs/2409.20053v2)|null|
|**2024-09-30**|**Enhancing High-order Interaction Awareness in LLM-based Recommender Model**|Xinfeng Wang et.al.|[2409.19979v2](http://arxiv.org/abs/2409.19979v2)|[link](https://github.com/WangXFng/ELMRec)|
|**2024-09-29**|**CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering**|Yike Wu et.al.|[2409.19753v2](http://arxiv.org/abs/2409.19753v2)|[link](https://github.com/wuyike2000/CoTKR)|
|**2024-09-29**|**Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models**|Xin Li et.al.|[2409.19667v1](http://arxiv.org/abs/2409.19667v1)|[link](https://github.com/bupt-gamma/prograph)|
|**2024-09-28**|**Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs**|Zheng Wang et.al.|[2409.19401v1](http://arxiv.org/abs/2409.19401v1)|null|
|**2024-09-27**|**CLLMate: A Multimodal LLM for Weather and Climate Events Forecasting**|Haobo Li et.al.|[2409.19058v1](http://arxiv.org/abs/2409.19058v1)|null|
|**2024-09-27**|**AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow**|Huizi Yu et.al.|[2409.18924v2](http://arxiv.org/abs/2409.18924v2)|null|
|**2024-09-27**|**Soft Measures for Extracting Causal Collective Intelligence**|Maryam Berijanian et.al.|[2409.18911v1](http://arxiv.org/abs/2409.18911v1)|[link](https://github.com/kuldeep7688/soft-measures-causal-intelligence)|
|**2024-09-27**|**OpenObject-NAV: Open-Vocabulary Object-Oriented Navigation Based on Dynamic Carrier-Relationship Scene Graph**|Yujie Tang et.al.|[2409.18743v1](http://arxiv.org/abs/2409.18743v1)|null|
|**2024-09-27**|**Rehearsing Answers to Probable Questions with Perspective-Taking**|Yung-Yu Shih et.al.|[2409.18678v1](http://arxiv.org/abs/2409.18678v1)|null|
|**2024-09-26**|**LowREm: A Repository of Word Embeddings for 87 Low-Resource Languages Enhanced with Multilingual Graph Knowledge**|Daniil Gurgurov et.al.|[2409.18193v1](http://arxiv.org/abs/2409.18193v1)|null|
|**2024-09-26**|**A Survey of Spatio-Temporal EEG data Analysis: from Models to Applications**|Pengfei Wang et.al.|[2410.08224v1](http://arxiv.org/abs/2410.08224v1)|null|
|**2024-09-26**|**Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study**|Zahra Sepasdar et.al.|[2409.17580v1](http://arxiv.org/abs/2409.17580v1)|null|
|**2024-09-25**|**Probing Omissions and Distortions in Transformer-based RDF-to-Text Models**|Juliette Faille et.al.|[2409.16707v1](http://arxiv.org/abs/2409.16707v1)|null|
|**2024-09-25**|**GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning**|Zhe-Rui Yang et.al.|[2409.16670v1](http://arxiv.org/abs/2409.16670v1)|null|
|**2024-09-24**|**Cyber Knowledge Completion Using Large Language Models**|Braden K Webb et.al.|[2409.16176v1](http://arxiv.org/abs/2409.16176v1)|null|
|**2024-09-24**|**Konstruktor: A Strong Baseline for Simple Knowledge Graph Question Answering**|Maria Lysyuk et.al.|[2409.15902v1](http://arxiv.org/abs/2409.15902v1)|[link](https://github.com/s-nlp/konstruktor)|
|**2024-09-24**|**Symmetries and Expressive Requirements for Learning General Policies**|Dominik Drexler et.al.|[2409.15892v1](http://arxiv.org/abs/2409.15892v1)|null|
|**2024-09-23**|**GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation**|Brendan Hogan Rappazzo et.al.|[2409.15566v1](http://arxiv.org/abs/2409.15566v1)|null|
|**2024-09-23**|**KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems**|Zixuan Wang et.al.|[2409.14908v1](http://arxiv.org/abs/2409.14908v1)|null|
|**2024-09-23**|**End-to-End Graph Flattening Method for Large Language Models**|Bin Hong et.al.|[2409.14880v1](http://arxiv.org/abs/2409.14880v1)|null|
|**2024-09-22**|**RACOON: An LLM-based Framework for Retrieval-Augmented Column Type Annotation with a Knowledge Graph**|Linxi Wei et.al.|[2409.14556v1](http://arxiv.org/abs/2409.14556v1)|null|

#### Abstracts
##### **FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression**
2410.12707v1 by Zhenheng Tang, Xueze Kang, Yiming Yin, Xinglin Pan, Yuxin Wang, Xin He, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Amelie Chi Zhou, Bo Li, Bingsheng He, Xiaowen Chu

To alleviate hardware scarcity in training large deep neural networks (DNNs),
particularly large language models (LLMs), we present FusionLLM, a
decentralized training system designed and implemented for training DNNs using
geo-distributed GPUs across different computing clusters or individual devices.
Decentralized training faces significant challenges regarding system design and
efficiency, including: 1) the need for remote automatic differentiation (RAD),
2) support for flexible model definitions and heterogeneous software, 3)
heterogeneous hardware leading to low resource utilization or the straggler
problem, and 4) slow network communication. To address these challenges, in the
system design, we represent the model as a directed acyclic graph of operators
(OP-DAG). Each node in the DAG represents the operator in the DNNs, while the
edge represents the data dependency between operators. Based on this design, 1)
users are allowed to customize any DNN without caring low-level operator
implementation; 2) we enable the task scheduling with the more fine-grained
sub-tasks, offering more optimization space; 3) a DAG runtime executor can
implement RAD withour requiring the consistent low-level ML framework versions.
  To enhance system efficiency, we implement a workload estimator and design an
OP-Fence scheduler to cluster devices with similar bandwidths together and
partition the DAG to increase throughput. Additionally, we propose an AdaTopK
compressor to adaptively compress intermediate activations and gradients at the
slowest communication links. To evaluate the convergence and efficiency of our
system and algorithms, we train ResNet-101 and GPT-2 on three real-world
testbeds using 48 GPUs connected with 8 Mbps~10 Gbps networks. Experimental
results demonstrate that our system and method can achieve 1.45 - 9.39x speedup
compared to baseline methods while ensuring convergence.

æè¦ï¼<paragraph>çºäºæ¸è¼è¨ç·´å¤§åæ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) çç¡¬é«ç­ç¼ºåé¡ï¼å°¤å¶æ¯å¤§åèªè¨æ¨¡å (LLM)ï¼æåæåºäº FusionLLMï¼ä¸ååæ£å¼è¨ç·´ç³»çµ±ï¼å¶è¨­è¨åå¯¦ä½æ¯ç¨æ¼è¨ç·´è·¨ä¸åéç®å¢éæåå¥è£ç½®çå°çåæ£å¼ GPU ç DNNãåæ£å¼è¨ç·´å¨ç³»çµ±è¨­è¨åæçæ¹é¢é¢è¨éå¤§ææ°ï¼åæ¬ï¼1) éè¦é ç«¯èªåå¾®å (RAD)ï¼2) æ¯æ´å½æ§çæ¨¡åå®ç¾©åç°è³ªè»é«ï¼3) ç°è³ªç¡¬é«å°è´è³æºå©ç¨çä½æè½å¾åé¡ï¼ä»¥å 4) ç¶²è·¯éè¨éåº¦æ¢ãçºäºæå°éäºææ°ï¼å¨ç³»çµ±è¨­è¨ä¸­ï¼æåå°æ¨¡åè¡¨ç¤ºçºä¸åæåéå¾ªç°å (OP-DAG) çéç®å­ãDAG ä¸­çæ¯åç¯é»ä»£è¡¨ DNN ä¸­çéç®å­ï¼èéç·£ä»£è¡¨éç®å­ä¹éçè³æä¾è³´æ§ãåºæ¼æ­¤è¨­è¨ï¼1) ä½¿ç¨èå¯ä»¥èªè¨ä»»ä½ DNNï¼èä¸ç¨èæ®ä½ééç®å­å¯¦ä½ï¼2) æååç¨ä»»åæç¨ï¼ä¸¦ä½¿ç¨æ´ç´°ç·»çå­ä»»åï¼æä¾æ´å¤æä½³åç©ºéï¼3) DAG å·è¡æéå·è¡å¨å¯ä»¥å¯¦ä½ RADï¼èä¸éè¦ä¸è´çä½é ML æ¶æ§çæ¬ãçºäºæåç³»çµ±æçï¼æåå¯¦ä½ä¸åå·¥ä½è² è¼ä¼°è¨å¨ï¼ä¸¦è¨­è¨ä¸å OP-Fence æç¨å¨ï¼å°é »å¯¬é¡ä¼¼çè£ç½®åçµå¨ä¸èµ·ï¼ä¸¦åå² DAG ä»¥å¢å èçéãæ­¤å¤ï¼æåæåºä¸å AdaTopK å£ç¸®å¨ï¼ä»¥èªé©ææ¹å¼å£ç¸®ææ¢éè¨é£çµä¸çä¸­éåååæ¢¯åº¦ãçºäºè©ä¼°æåç³»çµ±åæ¼ç®æ³çæ¶ææ§åæçï¼æåå¨ä¸åçå¯¦ä¸ççæ¸¬è©¦å¹³å°ä¸è¨ç·´ ResNet-101 å GPT-2ï¼ä½¿ç¨ 48 å GPU é£æ¥å° 8 Mbps~10 Gbps ç¶²è·¯ãå¯¦é©çµæè¡¨æï¼æåçç³»çµ±åæ¹æ³å¯ä»¥æ¯åºæºæ¹æ³å¿« 1.45 - 9.39 åï¼åæç¢ºä¿æ¶æã</paragraph>

##### **The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph**
2410.12458v1 by Minghao Wu, Thuy-Trang Vu, Lizhen Qu, Gholamreza Haffari

The performance of large language models (LLMs) in natural language
processing (NLP) tasks is significantly influenced by the quality and diversity
of data used for supervised fine-tuning (SFT). Current data selection methods
often focus solely on quality or diversity, leading to underperforming models
due to suboptimal training data. In this paper, we introduce GraphFilter, a
novel method that represents the dataset as a bipartite graph, linking
sentences to their constituent n-grams. This representation effectively
captures the relationships between sentences and linguistic patterns,
facilitating the selection of sentences that enhance n-gram diversity. To
balance quality and diversity during selection, we propose a priority function
that combines the quality metric with the diversity metric in a multiplicative
manner. GraphFilter iteratively selects high-priority sentences, updates the
bipartite graph by removing covered n-grams, and re-calculates priorities to
reflect the evolving data landscape. We conduct extensive experiments using
three model backbones across six widely used benchmarks. The results
demonstrate that GraphFilter outperforms all nine baseline approaches,
achieving superior model performance and computational efficiency. Our analyses
validate the effectiveness of our design choices, examine the subsets selected
by GraphFilter and other methods, highlight the importance of instruction
diversity, and explore the role of quality and diversity in relation to subset
sizes. GraphFilter establishes a new foundation for effective data selection
strategies, encouraging further research in data selection for LLMs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨èªç¶èªè¨èç (NLP) ä»»åä¸­çè¡¨ç¾ï¼åå°ç¨æ¼ç£ç£å¾®èª¿ (SFT) çè³æåè³ªåå¤æ¨£æ§é¡¯èå½±é¿ãç®åçè³æé¸åæ¹æ³éå¸¸åªéæ³¨åè³ªæå¤æ¨£æ§ï¼å°è´è¨ç·´è³ææ¬¡ä½³ï¼é²èé ææ¨¡åè¡¨ç¾ä¸ä½³ãå¨æ¬æä¸­ï¼æåä»ç´¹ GraphFilterï¼ä¸ç¨®æ°ç©çæ¹æ³ï¼å®å°è³æéè¡¨ç¤ºçºäºé¨åï¼å°å¥å­é£çµå°å¶çµæ n-gramãéç¨®è¡¨ç¤ºæ¹å¼ææææå¥å­åèªè¨æ¨¡å¼ä¹éçéä¿ï¼æå©æ¼é¸æè½æå n-gram å¤æ¨£æ§çå¥å­ãçºäºå¨é¸åéç¨ä¸­å¹³è¡¡åè³ªåå¤æ¨£æ§ï¼æåæåºåªåå½æ¸ï¼ä»¥ä¹æ³æ¹å¼çµååè³ªææ¨åå¤æ¨£æ§ææ¨ãGraphFilter è¿­ä»£é¸åé«åªåç´å¥å­ï¼ééç§»é¤å·²æ¶µè n-gram ä¾æ´æ°äºé¨åï¼ä¸¦éæ°è¨ç®åªåç´ä»¥åæ ä¸æ·è®åçè³ææ¨£è²ãæåä½¿ç¨ä¸åæ¨¡åä¸»å¹¹å¨å­åå»£æ³ä½¿ç¨çåºæºä¸é²è¡å»£æ³çå¯¦é©ãçµæé¡¯ç¤ºï¼GraphFilter åªæ¼ææä¹ç¨®åºç·æ¹æ³ï¼éå°åè¶çæ¨¡åæè½åéç®æçãæåçåæé©è­äºæåè¨­è¨é¸æçæææ§ï¼æª¢é© GraphFilter åå¶ä»æ¹æ³é¸åçå­éï¼å¼·èª¿æä»¤å¤æ¨£æ§çéè¦æ§ï¼ä¸¦æ¢è¨åè³ªåå¤æ¨£æ§èå­éå¤§å°çéä¿ãGraphFilter çºææçè³æé¸åç­ç¥å¥ å®æ°çåºç¤ï¼é¼åµé²ä¸æ­¥ç ç©¶ LLM çè³æé¸åã

##### **PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking**
2410.12375v1 by Markus J. Buehler

PRefLexOR (Preference-based Recursive Language Modeling for Exploratory
Optimization of Reasoning) combines preference optimization with concepts from
Reinforcement Learning to enable models to self-teach through iterative
reasoning improvements. We propose a recursive learning approach that engages
the model in multi-step reasoning, revisiting, and refining intermediate steps
before producing a final output in training and inference phases. Through
multiple training stages, the model first learns to align its reasoning with
accurate decision paths by optimizing the log odds between preferred and
non-preferred responses. During this process, PRefLexOR builds a dynamic
knowledge graph by generating questions from random text chunks and
retrieval-augmentation to contextualize relevant details from the entire
training corpus. In the second stage, preference optimization enhances model
performance by using rejection sampling to fine-tune reasoning quality by
continually producing in-situ training data while masking the reasoning steps.
Recursive optimization within a thinking token framework introduces iterative
feedback loops, where the model refines reasoning, achieving deeper coherence,
consistency, and adaptability. Implemented in small language models with only 3
billion parameters, we should that even tiny models can iteratively teach
themselves to reason with greater depth and reflectivity. Our implementation is
straightforward and can be incorporated into any existing pretrained LLM. We
focus our examples on applications in biological materials science and
demonstrate the method in a variety of case studies that range from in-domain
to cross-domain applications. Using reasoning strategies that include thinking
and reflection modalities we build a multi-agent recursive self-improving
inference approach to successively improve responses via repeated sampling in
inference time.

æè¦ï¼PRefLexORï¼ç¨æ¼æ¢ç´¢æ§æ¨çåªåçåºæ¼åå¥½çéè¿´èªè¨å»ºæ¨¡ï¼å°åå¥½åªåèå¼·åå­¸ç¿ä¸­çæ¦å¿µç¸çµåï¼ä½¿æ¨¡åè½å¤ ééåè¦æ¨çæ¹é²ä¾èªææå­¸ãæåæåºäºä¸ç¨®éè¿´å­¸ç¿æ¹æ³ï¼è®æ¨¡ååèå¤æ­¥é©æ¨çãéæ°å¯©è¦åæ¹é²ä¸­éæ­¥é©ï¼ç¶å¾å¨è¨ç·´åæ¨çéæ®µç¢çæçµè¼¸åºãééå¤åè¨ç·´éæ®µï¼æ¨¡åé¦åå­¸ç¿ééåªåé¦é¸åéé¦é¸é¿æä¹éçå°æ¸å¹¾çï¼ä½¿å¶æ¨çèæºç¢ºçæ±ºç­è·¯å¾ä¿æä¸è´ãå¨æ­¤éç¨ä¸­ï¼PRefLexOR ééå¾é¨æ©ææ¬å¡çæåé¡åæª¢ç´¢å¢å¼·ä¾æ§å»ºä¸ååæç¥è­åï¼å¾æ´åè¨ç·´èªæåº«ä¸­æåç¸éç´°ç¯ä»¥é²è¡èªå¢åãå¨ç¬¬äºéæ®µï¼åå¥½åªåééä½¿ç¨æçµæ¡æ¨£ä¾å¾®èª¿æ¨çè³ªéï¼å¾èå¢å¼·æ¨¡åæ§è½ï¼åæé£çºç¢çåä½è¨ç·´æ¸æï¼åææ©èæ¨çæ­¥é©ãå¨æèä»¤çæ¡æ¶å§é²è¡éè¿´åªåæå¼å¥è¿­ä»£åé¥è¿´è·¯ï¼å¶ä¸­æ¨¡åææ¹é²æ¨çï¼å¾èå¯¦ç¾æ´æ·±å¥çé£è²«æ§ãä¸è´æ§åé©ææ§ãå¨åªæ 30 åååæ¸çå°èªè¨æ¨¡åä¸­å¯¦ç¾ï¼æåæè©²è®å³ä½¿æ¯å¾å°çæ¨¡åä¹è½ééè¿­ä»£çæ¹å¼ææèªå·±ä»¥æ´å¤§çæ·±åº¦ååæè½åé²è¡æ¨çãæåçå¯¦ç¾éå¸¸ç´æ¥ï¼å¯ä»¥æ´åå°ä»»ä½ç¾æçé è¨ç·´ LLM ä¸­ãæåå°æåçç¤ºä¾éé»æ¾å¨çç©ææç§å­¸æç¨ä¸ï¼ä¸¦å¨å¾åå§å°è·¨åæç¨ç­åç¨®æ¡ä¾ç ç©¶ä¸­æ¼ç¤ºäºè©²æ¹æ³ãä½¿ç¨åæ¬æèååææ¨¡å¼å¨å§çæ¨çç­ç¥ï¼æåæ§å»ºäºä¸åå¤ä»£çéè¿´èªææ¹é²æ¨çæ¹æ³ï¼ä»¥ééå¨æ¨çæééè¤æ¡æ¨£ä¾é£çºæ¹é²é¿æã

##### **Pyramid-Driven Alignment: Pyramid Principle Guided Integration of Large Language Models and Knowledge Graphs**
2410.12298v1 by Lei Sun, Xinchen Wang, Youdi Li

Large Language Models (LLMs) possess impressive reasoning abilities but are
prone to generating incorrect information, often referred to as hallucinations.
While incorporating external Knowledge Graphs (KGs) can partially mitigate this
issue, existing methods primarily treat KGs as static knowledge repositories,
overlooking the critical disparity between KG and LLM knowledge, and failing to
fully exploit the reasoning capabilities inherent in KGs. To address these
limitations, we propose Pyramid-Driven Alignment (PDA), a novel framework for
seamlessly integrating LLMs with KGs. PDA utilizes Pyramid Principle analysis
to construct a hierarchical pyramid structure. This structure is designed to
reflect the input question and generate more validated deductive knowledge,
thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive
integration. Furthermore, PDA employs a recursive mechanism to harness the
underlying reasoning abilities of KGs, resulting in more accurate knowledge
retrieval for question-answering tasks. Our experimental results reveal a
substantial performance advantage of PDA over state-of-the-art baselines, with
improvements reaching 26.70% and 26.78%.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ææä»¤äººå°è±¡æ·±å»çæ¨çè½åï¼ä½å®¹æç¢çä¸æ­£ç¢ºçè³è¨ï¼éå¸¸ç¨±çºå¹»è¦ºãéç¶å å¥å¤é¨ç¥è­åè­ (KG) å¯ä»¥é¨åç·©è§£æ­¤åé¡ï¼ä½ç¾ææ¹æ³ä¸»è¦å° KG è¦çºéæç¥è­å²å­åº«ï¼å¿½ç¥äº KG å LLM ç¥è­ä¹éçééµå·®ç°ï¼ä¸¦ä¸æªè½ååå©ç¨ KG ä¸­åºæçæ¨çè½åãçºäºè§£æ±ºéäºéå¶ï¼æåæåºäºéå­å¡é©åå°é½ (PDA)ï¼éæ¯ä¸ç¨®å° LLM è KG ç¡ç¸«æ´åçæ°ç©æ¶æ§ãPDA å©ç¨éå­å¡åçåæä¾æ§å»ºéå±¤å¼éå­å¡çµæ§ãæ­¤çµæ§æ¨å¨åæ è¼¸å¥åé¡ä¸¦ç¢çæ´å¤é©è­çæ¼ç¹¹ç¥è­ï¼å¾èå¢å¼· LLM å KG çå°é½ä¸¦ç¢ºä¿æ´ç·å¯çæ´åãæ­¤å¤ï¼PDA æ¡ç¨éè¿´æ©å¶ä¾å©ç¨ KG çåºæ¬æ¨çè½åï¼å¾èæ´æºç¢ºå°æ·ååé¡è§£ç­ä»»åçç¥è­ãæåçå¯¦é©çµæé¡¯ç¤ºï¼PDA ç¸è¼æ¼æåé²çåºæºå·æé¡¯èçæè½åªå¢ï¼æ¹é²å¹åº¦éå° 26.70% å 26.78%ã

##### **Towards LLM-based Cognitive Models of Students with Misconceptions**
2410.12294v1 by Shashank Sonkar, Xinghe Chen, Naiming Liu, Richard G. Baraniuk, Mrinmaya Sachan

Accurately modeling student cognition is crucial for developing effective
AI-driven educational technologies. A key challenge is creating realistic
student models that satisfy two essential properties: (1) accurately
replicating specific misconceptions, and (2) correctly solving problems where
these misconceptions are not applicable. This dual requirement reflects the
complex nature of student understanding, where misconceptions coexist with
correct knowledge. This paper investigates whether Large Language Models (LLMs)
can be instruction-tuned to meet this dual requirement and effectively simulate
student thinking in algebra. We introduce MalAlgoPy, a novel Python library
that generates datasets reflecting authentic student solution patterns through
a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,
we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned
to faithfully emulate realistic student behavior. Our findings reveal that LLMs
trained on misconception examples can efficiently learn to replicate errors.
However, the training diminishes the model's ability to solve problems
correctly, particularly for problem types where the misconceptions are not
applicable, thus failing to satisfy second property of CSMs. We demonstrate
that by carefully calibrating the ratio of correct to misconception examples in
the training data - sometimes as low as 0.25 - it is possible to develop CSMs
that satisfy both properties. Our insights enhance our understanding of
AI-based student models and pave the way for effective adaptive learning
systems.

æè¦ï¼æºç¢ºå»ºæ¨¡å­¸ççèªç¥å°æ¼éç¼ææç AI é©åæè²æè¡è³ééè¦ãä¸åééµææ°æ¯å»ºç«ç¬¦åä»¥ä¸å©ååºæ¬å±¬æ§çé¼çå­¸çæ¨¡åï¼(1) æºç¢ºè¤è£½ç¹å®é¯èª¤è§å¿µï¼ä»¥å (2) æ­£ç¢ºè§£æ±ºéäºé¯èª¤è§å¿µä¸é©ç¨çåé¡ãæ­¤éééæ±åæ äºå­¸ççè§£çè¤éæ§ï¼å¶ä¸­é¯èª¤è§å¿µèæ­£ç¢ºç¥è­ä¸¦å­ãæ¬ææ¢è¨å¤§åèªè¨æ¨¡å (LLM) æ¯å¦å¯ä»¥éå°æä»¤é²è¡èª¿æ´ï¼ä»¥æ»¿è¶³æ­¤éééæ±ï¼ä¸¦æææ¨¡æ¬å­¸çå¨ä»£æ¸ä¸­çæèãæåå¼å¥äº MalAlgoPyï¼éæ¯ä¸åæ°ç©ç Python å½å¼åº«ï¼ééä»£æ¸åé¡è§£æ±ºçåå½¢åè¡¨ç¤ºï¼ç¢çåæ çå¯¦å­¸çè§£æ±ºæ¨¡å¼çè³æéãå©ç¨ MalAlgoPyï¼æåå®ç¾©ä¸¦æª¢è¦èªç¥å­¸çæ¨¡å (CSM)ï¼ä¹å°±æ¯éå°æä»¤é²è¡èª¿æ´ç LLMï¼ä»¥å¿ å¯¦å°æ¨¡æ¬çå¯¦å­¸ççè¡çºãæåçç ç©¶çµæé¡¯ç¤ºï¼éå°é¯èª¤è§å¿µç¯ä¾é²è¡è¨ç·´ç LLM å¯ä»¥ææå°å­¸ç¿è¤è£½é¯èª¤ãç¶èï¼è¨ç·´æéä½æ¨¡åæ­£ç¢ºè§£æ±ºåé¡çè½åï¼ç¹å¥æ¯å°æ¼é¯èª¤è§å¿µä¸é©ç¨çåé¡é¡åï¼å æ­¤ç¡æ³æ»¿è¶³ CSM çç¬¬äºåå±¬æ§ãæåè­æï¼ééä»ç´°æ ¡æºè¨ç·´è³æä¸­æ­£ç¢ºç¯ä¾èé¯èª¤è§å¿µç¯ä¾çæ¯ä¾ï¼ææä½è³ 0.25ï¼ï¼å¯ä»¥éç¼åææ»¿è¶³éå©åå±¬æ§ç CSMãæåçè¦è§£å¢é²äºæåå°åºæ¼ AI çå­¸çæ¨¡åççè§£ï¼ä¸¦çºææçé©ææ§å­¸ç¿ç³»çµ±éªè·¯ã

##### **Comprehending Knowledge Graphs with Large Language Models for Recommender Systems**
2410.12229v1 by Ziqiang Cui, Yunpeng Weng, Xing Tang, Fuyuan Lyu, Dugang Liu, Xiuqiang He, Chen Ma

Recently, the introduction of knowledge graphs (KGs) has significantly
advanced recommender systems by facilitating the discovery of potential
associations between items. However, existing methods still face several
limitations. First, most KGs suffer from missing facts or limited scopes. This
can lead to biased knowledge representations, thereby constraining the model's
performance. Second, existing methods typically convert textual information
into IDs, resulting in the loss of natural semantic connections between
different items. Third, existing methods struggle to capture high-order
relationships in global KGs due to their inefficient layer-by-layer information
propagation mechanisms, which are prone to introducing significant noise. To
address these limitations, we propose a novel method called CoLaKG, which
leverages large language models (LLMs) for knowledge-aware recommendation. The
extensive world knowledge and remarkable reasoning capabilities of LLMs enable
them to supplement KGs. Additionally, the strong text comprehension abilities
of LLMs allow for a better understanding of semantic information. Based on
this, we first extract subgraphs centered on each item from the KG and convert
them into textual inputs for the LLM. The LLM then outputs its comprehension of
these item-centered subgraphs, which are subsequently transformed into semantic
embeddings. Furthermore, to utilize the global information of the KG, we
construct an item-item graph using these semantic embeddings, which can
directly capture higher-order associations between items. Both the semantic
embeddings and the structural information from the item-item graph are
effectively integrated into the recommendation model through our designed
representation alignment and neighbor augmentation modules. Extensive
experiments on four real-world datasets demonstrate the superiority of our
method.

æè¦ï¼<paragraph>æè¿ï¼ç¥è­åè­ (KG) çå¼å¥ééä¿é²é ç®ä¹éæ½å¨éè¯çç¼ç¾ï¼é¡¯èæåæ¨è¦ç³»çµ±ãç¶èï¼ç¾ææ¹æ³ä»é¢è¨å¹¾åéå¶ãé¦åï¼å¤§å¤æ¸ KG é½å­å¨äºå¯¦ç¼ºå¤±æç¯ååéçåé¡ãéå¯è½å°è´æåå·®çç¥è­è¡¨å¾µï¼é²èéå¶æ¨¡åçæè½ãå¶æ¬¡ï¼ç¾ææ¹æ³éå¸¸æå°æå­è³è¨è½æçº IDï¼å°è´ä¸åé ç®ä¹éèªç¶èªç¾©é£çµçéºå¤±ãç¬¬ä¸ï¼ç¾ææ¹æ³é£ä»¥ææå¨ç KG ä¸­çé«ééä¿ï¼åå å¨æ¼å¶ä½æççéå±¤è³è¨å³æ­æ©å¶å®¹æå¼å¥é¡¯èéè¨ãçºäºè§£æ±ºéäºéå¶ï¼æåæåºäºä¸ç¨®ç¨±çº CoLaKG çæ°æ¹æ³ï¼å®å©ç¨å¤§åèªè¨æ¨¡å (LLM) é²è¡ç¥è­æç¥æ¨è¦ãLLM å»£æ³çä¸çç¥è­ååè¶çæ¨çè½åä½¿å®åè½å¤ è£å KGãæ­¤å¤ï¼LLM å¼·å¤§çæå­çè§£è½åæå©æ¼æ´æ·±å¥å°çè§£èªç¾©è³è¨ãåºæ¼æ­¤ï¼æåé¦åå¾ KG ä¸­æ·åä»¥æ¯åé ç®çºä¸­å¿çå­åï¼ä¸¦å°å®åè½æçº LLM çæå­è¼¸å¥ãç¶å¾ï¼LLM æè¼¸åºå¶å°éäºä»¥é ç®çºä¸­å¿çå­åççè§£ï¼éäºçè§£æ¥èæè½æçºèªç¾©åµå¥ãæ­¤å¤ï¼çºäºå©ç¨ KG çå¨çè³è¨ï¼æåä½¿ç¨éäºèªç¾©åµå¥å»ºæ§ä¸åé ç®-é ç®åï¼å®å¯ä»¥ç´æ¥ææé ç®ä¹éçé«ééè¯ãèªç¾©åµå¥åä¾èªé ç®-é ç®åççµæ§è³è¨é½æééæåè¨­è¨çè¡¨å¾µæ¯å°åé°åæ´åæ¨¡çµææå°æ´åå°æ¨è¦æ¨¡åä¸­ãå¨ååçå¯¦ä¸çè³æéä¸é²è¡çå»£æ³å¯¦é©è­æäºæåæ¹æ³çåªè¶æ§ã</paragraph>

##### **Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with Large Language Models for Multi-Behavior Recommendations**
2410.12228v1 by Luyi Ma, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sushant Kumar, Kannan Achan

Integrating diverse data modalities is crucial for enhancing the performance
of personalized recommendation systems. Traditional models, which often rely on
singular data sources, lack the depth needed to accurately capture the
multifaceted nature of item features and user behaviors. This paper introduces
a novel framework for multi-behavior recommendations, leveraging the fusion of
triple-modality, which is visual, textual, and graph data through alignment
with large language models (LLMs). By incorporating visual information, we
capture contextual and aesthetic item characteristics; textual data provides
insights into user interests and item features in detail; and graph data
elucidates relationships within the item-behavior heterogeneous graphs. Our
proposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs
to align and integrate these three modalities, achieving a comprehensive
representation of user behaviors. The LLM models the user's interactions
including behaviors and item features in natural languages. Initially, the LLM
is warmed up using only natural language-based prompts. We then devise the
modality fusion module based on cross-attention and self-attention mechanisms
to integrate different modalities from other models into the same embedding
space and incorporate them into an LLM. Extensive experiments demonstrate the
effectiveness of our approach in improving recommendation accuracy. Further
ablation studies validate the effectiveness of our model design and benefits of
the TMF.

æè¦ï¼æ´ååç¨®è³æåæå°æ¼æååäººåæ¨è¦ç³»çµ±çæè½è³ééè¦ãå³çµ±æ¨¡åç¶å¸¸ä¾è³´å®ä¸è³æä¾æºï¼ç¼ºä¹ææé ç®ç¹å¾µåä½¿ç¨èè¡çºå¤é¢åæ¬è³ªæéçæ·±åº¦ãæ¬æä»ç´¹äºä¸ååµæ°çå¤è¡çºæ¨è¦æ¶æ§ï¼å©ç¨è¦è¦ºãæå­ååå½¢è³æçä¸éåæèåï¼ééèå¤§åèªè¨æ¨¡å (LLM) å°é½ä¾å¯¦ç¾ãééç´å¥è¦è¦ºè³è¨ï¼æåææèçµ¡åç¾å­¸é ç®ç¹å¾µï¼æå­è³æè©³ç´°æä¾ä½¿ç¨èèè¶£åé ç®ç¹å¾µçè¦è§£ï¼åå½¢è³æé¡æé ç®è¡çºç°è³ªåå½¢ä¸­çéä¿ãæåæåºçæ¨¡åç¨±çºä¸éåæèå (TMF)ï¼å©ç¨ LLM çåéä¾å°é½åæ´åéä¸ç¨®åæï¼éæä½¿ç¨èè¡çºçå¨é¢è¡¨å¾µãLLM ä»¥èªç¶èªè¨å»ºæ¨¡ä½¿ç¨èçäºåï¼åæ¬è¡çºåé ç®ç¹å¾µãæåï¼LLM åä½¿ç¨åºæ¼èªç¶èªè¨çæç¤ºé²è¡ç±èº«ãç¶å¾æåæ ¹æäº¤åæ³¨æååèªææ³¨æåæ©å¶è¨­è¨åæèåæ¨¡çµï¼å°ä¾èªå¶ä»æ¨¡åçä¸ååææ´åå°ç¸åçåµå¥ç©ºéï¼ä¸¦å°å®åç´å¥ LLMãå»£æ³çå¯¦é©è­æäºæåçæ¹æ³å¨æåæ¨è¦æºç¢ºåº¦æ¹é¢çæææ§ãé²ä¸æ­¥çæ¶èç ç©¶é©è­äºæåæ¨¡åè¨­è¨çæææ§ä»¥å TMF çå¥½èã

##### **Iter-AHMCL: Alleviate Hallucination for Large Language Model via Iterative Model-level Contrastive Learning**
2410.12130v1 by Huiwen Wu, Xiaohan Li, Xiaogang Xu, Jiafei Wu, Deyi Zhang, Zhe Liu

The development of Large Language Models (LLMs) has significantly advanced
various AI applications in commercial and scientific research fields, such as
scientific literature summarization, writing assistance, and knowledge graph
construction. However, a significant challenge is the high risk of
hallucination during LLM inference, which can lead to security concerns like
factual inaccuracies, inconsistent information, and fabricated content. To
tackle this issue, it is essential to develop effective methods for reducing
hallucination while maintaining the original capabilities of the LLM. This
paper introduces a novel approach called Iterative Model-level Contrastive
Learning (Iter-AHMCL) to address hallucination. This method modifies the
representation layers of pre-trained LLMs by using contrastive `positive' and
`negative' models, trained on data with and without hallucinations. By
leveraging the differences between these two models, we create a more
straightforward pathway to eliminate hallucinations, and the iterative nature
of contrastive learning further enhances performance. Experimental validation
on four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen)
finetuning with a specially designed dataset shows that our approach achieves
an average improvement of 10.1 points on the TruthfulQA benchmark.
Comprehensive experiments demonstrate the effectiveness of Iter-AHMCL in
reducing hallucination while maintaining the general capabilities of LLMs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çç¼å±å¨åæ¥­åç§å­¸ç ç©¶é åé¡¯èæ¨åäºåç¨® AI æç¨ï¼ä¾å¦ç§å­¸æç»æè¦ãå¯«ä½è¼å©åç¥è­åè­å»ºæ§ãç¶èï¼ä¸åéå¤§çææ°æ¯ LLM æ¨è«ä¸­å¹»è¦ºçé«é¢¨éªï¼éå¯è½æå°è´å®å¨åé¡ï¼ä¾å¦äºå¯¦ä¸æ­£ç¢ºãè³è¨ä¸ä¸è´åæé å§å®¹ãçºäºè§£æ±ºéååé¡ï¼éç¼ææçæ¹æ³ä¾æ¸å°å¹»è¦ºï¼åæä¿æ LLM çåå§åè½è³ééè¦ãæ¬æä»ç´¹äºä¸ç¨®ç¨±çºåè¦æ¨¡åå±¤ç´å°æ¯å­¸ç¿ (Iter-AHMCL) çæ°æ¹æ³ä¾è§£æ±ºå¹»è¦ºãæ­¤æ¹æ³ééä½¿ç¨å°æ¯çãæ­£åãåãè² åãæ¨¡åä¾ä¿®æ¹é åè¨ç·´ç LLM çè¡¨ç¤ºå±¤ï¼éäºæ¨¡åæ¯å¨æåæ²æå¹»è¦ºçè³æä¸è¨ç·´çãééå©ç¨éå©åæ¨¡åä¹éçå·®ç°ï¼æååµé äºä¸æ¢æ´ç´æ¥çéå¾ä¾æ¶é¤å¹»è¦ºï¼èå°æ¯å­¸ç¿çè¿­ä»£æ§è³ªé²ä¸æ­¥å¢å¼·äºæè½ãå¨ååé åè¨ç·´çåºç¤ LLM (LLaMA2ãAlpacaãLLaMA3 å Qwen) ä¸é²è¡çå¯¦é©é©è­ï¼ä½¿ç¨ç¹å¥è¨­è¨çè³æéé²è¡å¾®èª¿ï¼é¡¯ç¤ºæåçåæ³å¨ TruthfulQA åºæºä¸å¹³åæåäº 10.1 åãå¨é¢çå¯¦é©è­æäº Iter-AHMCL å¨æ¸å°å¹»è¦ºçåæï¼ç¶­æ LLM ä¸è¬åè½çæææ§ã

##### **Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning**
2410.12096v1 by Guangxin Su, Yifan Zhu, Wenjie Zhang, Hanchen Wang, Ying Zhang

Graph representation learning, involving both node features and graph
structures, is crucial for real-world applications but often encounters
pervasive noise. State-of-the-art methods typically address noise by focusing
separately on node features with large language models (LLMs) and on graph
structures with graph structure learning models (GSLMs). In this paper, we
introduce LangGSL, a robust framework that integrates the complementary
strengths of pre-trained language models and GSLMs to jointly enhance both node
feature and graph structure learning. In LangGSL, we first leverage LLMs to
filter noise in the raw data and extract valuable cleaned information as
features, enhancing the synergy of downstream models. During the mutual
learning phase in LangGSL, the core idea is to leverage the relatively small
language model (LM) to process local attributes and generate reliable
pseudo-labels and informative node embeddings, which are then integrated into
the GSLM's prediction phase. This approach enriches the global context and
enhances overall performance. Meanwhile, GSLM refines the evolving graph
structure constructed from the LM's output, offering updated labels back to the
LM as additional guidance, thus facilitating a more effective mutual learning
process. The LM and GSLM work synergistically, complementing each other's
strengths and offsetting weaknesses within a variational information-maximizing
framework, resulting in enhanced node features and a more robust graph
structure. Extensive experiments on diverse graph datasets of varying scales
and across different task scenarios demonstrate the scalability and
effectiveness of the proposed approach.

æè¦ï¼åè¡¨è¡¨ç¤ºå­¸ç¿æ¢æ¶åç¯é»ç¹å¾µåæ¶ååå½¢çµæ§ï¼å°æ¼ç¾å¯¦ä¸ççæç¨è³ééè¦ï¼ä½ç¶å¸¸æéå°æ®éçåªé³ãæåé²çæ¹æ³éå¸¸ééåå¥éæ³¨å·æå¤§åèªè¨æ¨¡å (LLM) çç¯é»ç¹å¾µåå·æåå½¢çµæ§å­¸ç¿æ¨¡å (GSLM) çåå½¢çµæ§ä¾è§£æ±ºåªé³åé¡ãå¨æ¬æä¸­ï¼æåä»ç´¹äº LangGSLï¼éæ¯ä¸åå¼·å¤§çæ¡æ¶ï¼å®æ´åäºé è¨ç·´èªè¨æ¨¡åå GSLM çäºè£åªå¢ï¼ä»¥å±åå¢å¼·ç¯é»ç¹å¾µååå½¢çµæ§å­¸ç¿ãå¨ LangGSL ä¸­ï¼æåé¦åå©ç¨ LLM ä¾éæ¿¾åå§æ¸æä¸­çåªé³ï¼ä¸¦æåæå¹å¼çå·²æ¸çä¿¡æ¯ä½çºç¹å¾µï¼å¢å¼·ä¸æ¸¸æ¨¡åçååä½ç¨ãå¨ LangGSL ä¸­çç¸äºå­¸ç¿éæ®µï¼æ ¸å¿ææ³æ¯å©ç¨ç¸å°è¼å°çèªè¨æ¨¡å (LM) ä¾èçå±é¨å±¬æ§ä¸¦çæå¯é çå½æ¨ç±¤åä¿¡æ¯è±å¯çç¯é»åµå¥ï¼ç¶å¾å°å®åéæå° GSLM çé æ¸¬éæ®µãéç¨®æ¹æ³è±å¯äºå¨å±ä¸ä¸æä¸¦å¢å¼·äºæ´é«æ§è½ãåæï¼GSLM åªåäºå¾ LM è¼¸åºæ§å»ºçæ¼ååå½¢çµæ§ï¼å°æ´æ°çæ¨ç±¤ä½çºéå æå°åé¥çµ¦ LMï¼å¾èä¿é²æ´ææçç¸äºå­¸ç¿éç¨ãLM å GSLM ååå·¥ä½ï¼å¨è®åä¿¡æ¯æå¤§åæ¡æ¶å§äºè£åèªçåªå¢ä¸¦å½è£å¼±é»ï¼å¾èå¢å¼·ç¯é»ç¹å¾µä¸¦å½¢ææ´å¼·å¤§çåå½¢çµæ§ãå¨ä¸åè¦æ¨¡åä¸åä»»åå ´æ¯çå¤æ¨£ååå½¢æ¸æéä¸é²è¡çå»£æ³å¯¦é©è­æäºææåºæ¹æ³çå¯æ´å±æ§åæææ§ã

##### **A Survey on Deep Tabular Learning**
2410.12034v1 by Shriyank Somvanshi, Subasish Das, Syed Aaqib Javed, Gian Antariksa, Ahmed Hossain

Tabular data, widely used in industries like healthcare, finance, and
transportation, presents unique challenges for deep learning due to its
heterogeneous nature and lack of spatial structure. This survey reviews the
evolution of deep learning models for tabular data, from early fully connected
networks (FCNs) to advanced architectures like TabNet, SAINT, TabTranSELU, and
MambaNet. These models incorporate attention mechanisms, feature embeddings,
and hybrid architectures to address tabular data complexities. TabNet uses
sequential attention for instance-wise feature selection, improving
interpretability, while SAINT combines self-attention and intersample attention
to capture complex interactions across features and data points, both advancing
scalability and reducing computational overhead. Hybrid architectures such as
TabTransformer and FT-Transformer integrate attention mechanisms with
multi-layer perceptrons (MLPs) to handle categorical and numerical data, with
FT-Transformer adapting transformers for tabular datasets. Research continues
to balance performance and efficiency for large datasets. Graph-based models
like GNN4TDL and GANDALF combine neural networks with decision trees or graph
structures, enhancing feature representation and mitigating overfitting in
small datasets through advanced regularization techniques. Diffusion-based
models like the Tabular Denoising Diffusion Probabilistic Model (TabDDPM)
generate synthetic data to address data scarcity, improving model robustness.
Similarly, models like TabPFN and Ptab leverage pre-trained language models,
incorporating transfer learning and self-supervised techniques into tabular
tasks. This survey highlights key advancements and outlines future research
directions on scalability, generalization, and interpretability in diverse
tabular data applications.

æè¦ï¼<paragraph>è¡¨æ ¼è³æå»£æ³æç¨æ¼é«çä¿å¥ãéèåéè¼¸ç­ç¢æ¥­ï¼ç±æ¼å¶ç°è³ªæ§ä¸ç¼ºä¹ç©ºéçµæ§ï¼å æ­¤å°æ·±åº¦å­¸ç¿æåºäºç¨ç¹çææ°ãéé èª¿æ¥åé¡§äºè¡¨æ ¼è³ææ·±åº¦å­¸ç¿æ¨¡åçæ¼é²ï¼å¾æ©æçå¨é£æ¥ç¶²è·¯ (FCN) å° TabNetãSAINTãTabTranSELU å MambaNet ç­åé²æ¶æ§ãéäºæ¨¡åçµåäºæ³¨æåæ©å¶ãç¹å¾µåµå¥åæ··åæ¶æ§ï¼ä»¥è§£æ±ºè¡¨æ ¼è³æçè¤éæ§ãTabNet ä½¿ç¨åºåæ³¨æåé²è¡éä¾ç¹å¾µé¸åï¼æåå¯è§£éæ§ï¼è SAINT çµåäºèªææ³¨æååè·¨æ¨£æ¬æ³¨æåï¼ä»¥ææç¹å¾µåè³æé»ä¹éçè¤éäºåï¼åææåå¯æ´åæ§ä¸¦æ¸å°éç®è² æãTabTransformer å FT-Transformer ç­æ··åæ¶æ§å°æ³¨æåæ©å¶èå¤å±¤æç¥å¨ (MLP) æ´åï¼ä»¥èçé¡å¥è³æåæ¸å¼è³æï¼å¶ä¸­ FT-Transformer å° transformer é©æå°è¡¨æ ¼è³æéãç ç©¶æçºå¨å¤§åè³æéçæè½åæçä¹éåå¾å¹³è¡¡ãåºæ¼åå½¢çæ¨¡åï¼ä¾å¦ GNN4TDL å GANDALFï¼å°ç¥ç¶ç¶²è·¯èæ±ºç­æ¨¹æåå½¢çµæ§çµåï¼ééåé²çæ­£ååæè¡å¢å¼·ç¹å¾µè¡¨ç¤ºä¸¦æ¸è¼å°è³æéä¸­çéåº¦æ¬åãåºæ¼æ´æ£çæ¨¡åï¼ä¾å¦è¡¨æ ¼å»åªæ´æ£æ©çæ¨¡å (TabDDPM)ï¼æç¢çåæè³æä»¥è§£æ±ºè³æç¨å°çåé¡ï¼é²èæåæ¨¡åçç©©å¥æ§ãé¡ä¼¼å°ï¼TabPFN å Ptab ç­æ¨¡åå©ç¨é åè¨ç·´çèªè¨æ¨¡åï¼å°é·ç§»å­¸ç¿åèªæç£ç£æè¡èå¥è¡¨æ ¼ä»»åä¸­ãéé èª¿æ¥éé»èªªæäºééµé²å±ï¼ä¸¦æ¦è¿°äºå¨åç¨®è¡¨æ ¼è³ææç¨ä¸­å¯æ´åæ§ãæ¦æ¬æ§åå¯è§£éæ§çæªä¾ç ç©¶æ¹åã</paragraph>

##### **Causal Reasoning in Large Language Models: A Knowledge Graph Approach**
2410.11588v1 by Yejin Kim, Eojin Kang, Juae Kim, H. Howie Huang

Large language models (LLMs) typically improve performance by either
retrieving semantically similar information, or enhancing reasoning abilities
through structured prompts like chain-of-thought. While both strategies are
considered crucial, it remains unclear which has a greater impact on model
performance or whether a combination of both is necessary. This paper answers
this question by proposing a knowledge graph (KG)-based random-walk reasoning
approach that leverages causal relationships. We conduct experiments on the
commonsense question answering task that is based on a KG. The KG inherently
provides both relevant information, such as related entity keywords, and a
reasoning structure through the connections between nodes. Experimental results
show that the proposed KG-based random-walk reasoning method improves the
reasoning ability and performance of LLMs. Interestingly, incorporating three
seemingly irrelevant sentences into the query using KG-based random-walk
reasoning enhances LLM performance, contrary to conventional wisdom. These
findings suggest that integrating causal structures into prompts can
significantly improve reasoning capabilities, providing new insights into the
role of causality in optimizing LLM performance.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) éå¸¸ééæ·åèªæä¸ç¸ä¼¼çè³è¨ï¼æéééå¼æèç­çµæ§åæç¤ºå¢å¼·æ¨çè½åï¼ä¾æåæè½ãåç®¡éå©ç¨®ç­ç¥é½è¢«èªçºè³ééè¦ï¼ä½ç®åä»ä¸æ¸æ¥åªä¸ç¨®å°æ¨¡åæè½å½±é¿è¼å¤§ï¼ææ¯å¦éè¦çµåå©èãæ¬æééæåºä¸ååºæ¼ç¥è­åè­ (KG) çé¨æ©æ¼«æ­¥æ¨çæ¹æ³ï¼ä¾åç­éååé¡ï¼éåæ¹æ³å©ç¨äºå æéä¿ãæåå¨åºæ¼ KG çå¸¸è­åç­ä»»åä¸é²è¡å¯¦é©ãKG æ¬èº«å°±æä¾äºç¸éè³è¨ï¼ä¾å¦ç¸éå¯¦é«ééµå­ï¼ä»¥åééç¯é»ä¹éçé£çµæä¾çæ¨ççµæ§ãå¯¦é©çµæé¡¯ç¤ºï¼æåºçåºæ¼ KG çé¨æ©æ¼«æ­¥æ¨çæ¹æ³æ¹åäº LLM çæ¨çè½ååæè½ãæè¶£çæ¯ï¼èå³çµ±è§å¿µç¸åï¼ä½¿ç¨åºæ¼ KG çé¨æ©æ¼«æ­¥æ¨çå°ä¸åçä¼¼ç¡éçå¥å­ç´å¥æ¥è©¢ä¸­ï¼å¯ä»¥æå LLM çæè½ãéäºç¼ç¾è¡¨æï¼å°å æçµæ§æ´åå°æç¤ºä¸­å¯ä»¥é¡¯èæåæ¨çè½åï¼ä¸¦çºå æéä¿å¨æä½³å LLM æè½ä¸­ææ®æ¼çè§è²æä¾æ°çè¦è§£ã

##### **Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for Drug Development**
2410.11550v1 by Tengfei Ma, Xuan Lin, Tianle Li, Chaoyi Li, Long Chen, Peng Zhou, Xibao Cai, Xinyu Yang, Daojian Zeng, Dongsheng Cao, Xiangxiang Zeng

Large Language Models (LLMs) have recently demonstrated remarkable
performance in general tasks across various fields. However, their
effectiveness within specific domains such as drug development remains
challenges. To solve these challenges, we introduce \textbf{Y-Mol}, forming a
well-established LLM paradigm for the flow of drug development. Y-Mol is a
multiscale biomedical knowledge-guided LLM designed to accomplish tasks across
lead compound discovery, pre-clinic, and clinic prediction. By integrating
millions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,
Y-Mol augments the reasoning capability in the biomedical domain by learning
from a corpus of publications, knowledge graphs, and expert-designed synthetic
data. The capability is further enriched with three types of drug-oriented
instructions: description-based prompts from processed publications,
semantic-based prompts for extracting associations from knowledge graphs, and
template-based prompts for understanding expert knowledge from biomedical
tools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously
execute the downstream tasks across the entire process of drug development,
including virtual screening, drug design, pharmacological properties
prediction, and drug-related interaction prediction. Our extensive evaluations
of various biomedical sources demonstrate that Y-Mol significantly outperforms
general-purpose LLMs in discovering lead compounds, predicting molecular
properties, and identifying drug interaction events.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) è¿æå¨ååé åçéç¨ä»»åä¸­å±ç¤ºåºé¡¯èçè¡¨ç¾ãç¶èï¼å®åå¨ç¹å®é åï¼ä¾å¦è¥ç©éç¼ï¼ä¸­çæè½ä»æå¾å å¼·ãçºäºè§£æ±ºéäºææ°ï¼æåå¼å¥äº **Y-Mol**ï¼å½¢æäºä¸åå®åç LLM å¸ç¯ï¼ç¨æ¼è¥ç©éç¼æµç¨ãY-Mol æ¯ä¸åå¤å°ºåº¦ççç©é«å­¸ç¥è­å¼å° LLMï¼æ¨å¨å®æåå°ååç©ç¼ç¾ãè¨åºååè¨åºé æ¸¬ç­ä»»åãééæ´åæ¸ç¾è¬åå¤å°ºåº¦ççç©é«å­¸ç¥è­ï¼ä¸¦ä½¿ç¨ LLaMA2 ä½çºåºç¤ LLMï¼Y-Mol å¾åºçç©ãç¥è­åè­åå°å®¶è¨­è¨çåæè³æä¸­å­¸ç¿ï¼å¢å¼·äºçç©é«å­¸é åçæ¨çè½åãå¶è½åé²ä¸æ­¥ééä¸ç¨®é¡åçè¥ç©å°åæä»¤å¾å°è±å¯ï¼å·²èçåºçç©çåºæ¼æè¿°çæç¤ºãç¨æ¼å¾ç¥è­åè­ä¸­æåéè¯çåºæ¼èªç¾©çæç¤ºï¼ä»¥åç¨æ¼çè§£çç©é«å­¸å·¥å·ä¸­å°å®¶ç¥è­çåºæ¼ç¯æ¬çæç¤ºãæ­¤å¤ï¼Y-Mol æä¾äºä¸çµ LLM å¸ç¯ï¼å¯ä»¥å¨æ´åè¥ç©éç¼éç¨ä¸­èªä¸»å·è¡ä¸æ¸¸ä»»åï¼åæ¬èæ¬ç¯©é¸ãè¥ç©è¨­è¨ãè¥çç¹æ§é æ¸¬åè¥ç©ç¸éäº¤äºé æ¸¬ãæåå°åç¨®çç©é«å­¸ä¾æºçå»£æ³è©ä¼°è¡¨æï¼Y-Mol å¨ç¼ç¾åå°ååç©ãé æ¸¬åå­ç¹æ§åè­å¥è¥ç©äº¤äºäºä»¶æ¹é¢é¡¯èåªæ¼éç¨ LLMã

##### **AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data**
2410.11531v1 by Xinjie Zhao, Moritz Blum, Rui Yang, Boming Yang, Luis MÃ¡rquez Carpintero, MÃ³nica Pina-Navarro, Tony Wang, Xin Li, Huitao Li, Yanran Fu, Rongrong Wang, Juntao Zhang, Irene Li

Large Language Models~(LLMs) have demonstrated capabilities across various
applications but face challenges such as hallucination, limited reasoning
abilities, and factual inconsistencies, especially when tackling complex,
domain-specific tasks like question answering~(QA). While Knowledge
Graphs~(KGs) have been shown to help mitigate these issues, research on the
integration of LLMs with background KGs remains limited. In particular, user
accessibility and the flexibility of the underlying KG have not been thoroughly
explored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based
Interaction and Graphical Representation), a platform for knowledge management
through natural language interaction. It integrates knowledge extraction,
integration, and real-time visualization. AGENTiGraph employs a multi-agent
architecture to dynamically interpret user intents, manage tasks, and integrate
new knowledge, ensuring adaptability to evolving user requirements and data
contexts. Our approach demonstrates superior performance in knowledge graph
interactions, particularly for complex domain-specific tasks. Experimental
results on a dataset of 3,500 test cases show AGENTiGraph significantly
outperforms state-of-the-art zero-shot baselines, achieving 95.12\% accuracy in
task classification and 90.45\% success rate in task execution. User studies
corroborate its effectiveness in real-world scenarios. To showcase versatility,
we extended AGENTiGraph to legislation and healthcare domains, constructing
specialized KGs capable of answering complex queries in legal and medical
contexts.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å¨åç¨®æç¨ä¸­å±ç¾å¶è½åï¼ä½ä»é¢è¨å¹»è¦ºãæ¨çè½åæéåäºå¯¦ä¸ä¸è´ç­ææ°ï¼å°¤å¶æ¯å¨èçè¤éçç¹å®é åä»»åï¼ä¾å¦åç­ (QA) æãéç¶ç¥è­åè­ (KG) å·²è¢«è­ææå©æ¼ç·©è§£éäºåé¡ï¼ä½ LLM èèæ¯ KG æ´åçç ç©¶ä»ç¶æéãç¹å¥æ¯ï¼ä½¿ç¨èçå¯åæ§ååºå±¤ KG çéæ´»æ§å°æªå¾å°å¾¹åºæ¢è¨ãæåå¼å¥äº AGENTiGraphï¼ç¨æ¼ä»»ååäºåååå½¢è¡¨ç¤ºçèªé©æçæå¼æï¼ï¼ä¸åééèªç¶èªè¨äºåé²è¡ç¥è­ç®¡ççå¹³å°ãå®æ´åäºç¥è­èåãæ´ååå³æè¦è¦ºåãAGENTiGraph æ¡ç¨å¤ä»£çæ¶æ§ï¼ä»¥åæè§£è®ä½¿ç¨èçæåãç®¡çä»»åä¸¦æ´åæ°ç¥è­ï¼ç¢ºä¿é©æä¸æ·è®åçä½¿ç¨èéæ±åè³æèçµ¡ãæåçåæ³å¨ç¥è­åè­äºåä¸­å±ç¾åºåªç°çæè½ï¼ç¹å¥æ¯å°æ¼è¤éçç¹å®é åä»»åãå¨ 3,500 åæ¸¬è©¦æ¡ä¾çè³æéä¸é²è¡çå¯¦é©çµæé¡¯ç¤ºï¼AGENTiGraph æé¡¯åªæ¼æåé²çé¶æ¬¡å­¸ç¿åºæºï¼å¨ä»»ååé¡ä¸­éå° 95.12% çæºç¢ºåº¦ï¼å¨ä»»åå·è¡ä¸­éå° 90.45% çæåçãä½¿ç¨èç ç©¶è­å¯¦äºå®å¨çå¯¦ä¸çå ´æ¯ä¸­çæææ§ãçºäºå±ç¤ºå¶å¤åè½æ§ï¼æåå° AGENTiGraph å»¶ä¼¸å°æ³å¾åé«çä¿å¥é åï¼å»ºæ§äºè½å¤ åç­æ³å¾åé«çèçµ¡ä¸­è¤éæ¥è©¢çå°æ¥­ç¥è­åè­ã

##### **Do LLMs Have the Generalization Ability in Conducting Causal Inference?**
2410.11385v1 by Chen Wang, Dongming Zhao, Bo Wang, Ruifang He, Yuexian Hou

In causal inference, generalization capability refers to the ability to
conduct causal inference methods on new data to estimate the causal-effect
between unknown phenomenon, which is crucial for expanding the boundaries of
knowledge. Studies have evaluated the causal inference capabilities of Large
Language Models (LLMs) concerning known phenomena, yet the generalization
capabilities of LLMs concerning unseen phenomena remain unexplored. In this
paper, we selected four tasks: Causal Path Discovery (CP), Backdoor Adjustment
(BA), Factual Inference (FI), and Counterfactual Inference (CI) as
representatives of causal inference tasks. To generate evaluation questions
about previously unseen phenomena in new data on the four tasks, we propose a
benchmark generation framework, which employs randomly generated graphs and
node names to formulate questions within hypothetical new causal scenarios.
Based on this framework, we compile a benchmark dataset of varying levels of
question complexity. We extensively tested the generalization capabilities of
five leading LLMs across four tasks. Experiment results reveal that while LLMs
exhibit good generalization performance in solving simple CP, FI, and complex
CI questions, they encounter difficulties when tackling BA questions and face
obvious performance fluctuations as the problem complexity changes.
Furthermore, when the names of phenomena incorporate existing terms, even if
these names are entirely novel, their generalization performance can still be
hindered by interference from familiar terms.

æè¦ï¼å¨å ææ¨è«ä¸­ï¼æ³åè½åæ¯æå¨æ°çè³æä¸å·è¡å ææ¨è«æ¹æ³ä»¥ä¼°è¨æªç¥ç¾è±¡ä¹éçå æéä¿çè½åï¼éå°æ¼æ´å±ç¥è­ççéè³ééè¦ãç ç©¶å·²ç¶è©ä¼°äºå¤§åèªè¨æ¨¡å (LLM) éæ¼å·²ç¥ç¾è±¡çå ææ¨è«è½åï¼ä½ LLM éæ¼æªç¥ç¾è±¡çæ³åè½åä»æªè¢«æ¢è¨ãå¨æ¬æä¸­ï¼æåé¸æäºååä»»åï¼å æè·¯å¾ç¼ç¾ (CP)ãå¾éèª¿æ´ (BA)ãäºå¯¦æ¨è« (FI) ååäºå¯¦æ¨è« (CI) ä½çºå ææ¨è«ä»»åçä»£è¡¨ãçºäºç¢çéæ¼æ°è³æä¸­ä»¥åæªè¦ç¾è±¡çè©ä¼°åé¡ï¼æåæåºäºåºæºçææ¡æ¶ï¼è©²æ¡æ¶æ¡ç¨é¨æ©çæçåå½¢åç¯é»åç¨±å¨åè¨­çæ°å æå ´æ¯ä¸­å¶å®åé¡ãåºæ¼æ­¤æ¡æ¶ï¼æåç·¨å¶äºä¸ååé¡è¤éç¨åº¦ä¸åçåºæºæ¸æéãæåå»£æ³æ¸¬è©¦äºäºåé åç LLM å¨ååä»»åä¸­çæ³åè½åãå¯¦é©çµæè¡¨æï¼éç¶ LLM å¨è§£æ±ºç°¡å®ç CPãFI åè¤éç CI åé¡æè¡¨ç¾åºè¯å¥½çæ³åæ§è½ï¼ä½å¨è§£æ±º BA åé¡æéå°å°é£ï¼ä¸¦ä¸é¨èåé¡è¤éæ§çè®åèé¢è¨æé¡¯çæ§è½æ³¢åãæ­¤å¤ï¼ç¶ç¾è±¡çåç¨±åå«ç¾æè¡èªæï¼å³ä½¿éäºåç¨±æ¯å®å¨æ°ç©çï¼å¶æ³åæ§è½ä»ç¶æåå°çæè¡èªçå¹²æ¾ã

##### **Enhance Graph Alignment for Large Language Models**
2410.11370v1 by Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, Yujun Zhang

Graph-structured data is prevalent in the real world. Recently, due to the
powerful emergent capabilities, Large Language Models (LLMs) have shown
promising performance in modeling graphs. The key to effectively applying LLMs
on graphs is converting graph data into a format LLMs can comprehend.
Graph-to-token approaches are popular in enabling LLMs to process graph
information. They transform graphs into sequences of tokens and align them with
text tokens through instruction tuning, where self-supervised instruction
tuning helps LLMs acquire general knowledge about graphs, and supervised
fine-tuning specializes LLMs for the downstream tasks on graphs. Despite their
initial success, we find that existing methods have a misalignment between
self-supervised tasks and supervised downstream tasks, resulting in negative
transfer from self-supervised fine-tuning to downstream tasks. To address these
issues, we propose Graph Alignment Large Language Models (GALLM) to benefit
from aligned task templates. In the self-supervised tuning stage, we introduce
a novel text matching task using templates aligned with downstream tasks. In
the task-specific tuning stage, we propose two category prompt methods that
learn supervision information from additional explanation with further aligned
templates. Experimental evaluations on four datasets demonstrate substantial
improvements in supervised learning, multi-dataset generalizability, and
particularly in zero-shot capability, highlighting the model's potential as a
graph foundation model.

æè¦ï¼åå½¢çµæ§çè³æå¨ç¾å¯¦ä¸çä¸­å¾å¸¸è¦ãæè¿ï¼ç±æ¼å¼·å¤§çæ°èè½åï¼å¤§åèªè¨æ¨¡å (LLM) å¨åå½¢å»ºæ¨¡æ¹é¢å±ç¾åºä»¤äººæ»¿æçæè½ãææå° LLM æç¨æ¼åå½¢çééµæ¯å°åå½¢è³æè½ææ LLM å¯ä»¥çè§£çæ ¼å¼ãåå½¢å°æ¨è¨çæ¹æ³å¾æµè¡ï¼è® LLM å¯ä»¥èçåå½¢è³è¨ãå®åå°åå½¢è½æææ¨è¨åºåï¼ä¸¦ééæä»¤èª¿æ´èæå­æ¨è¨å°é½ï¼å¶ä¸­èªæç£ç£çæä»¤èª¿æ´æå©æ¼ LLM ç²å¾éæ¼åå½¢çå¸¸è­ï¼èç£ç£å¾®èª¿åå°ééå°åå½¢ä¸çä¸æ¸¸ä»»åèª¿æ´ LLMãåç®¡å®åæåå¾æåï¼æåç¼ç¾ç¾ææ¹æ³å¨èªæç£ç£ä»»ååç£ç£ä¸æ¸¸ä»»åä¹éå­å¨é¯ä½ï¼å°è´èªæç£ç£å¾®èª¿å°ä¸æ¸¸ä»»åç¢çè² é¢å½±é¿ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºåå½¢å°é½å¤§åèªè¨æ¨¡å (GALLM) ä»¥å¾å°é½çä»»åç¯æ¬ä¸­åçãå¨èªæç£ç£èª¿æ´éæ®µï¼æåä½¿ç¨èä¸æ¸¸ä»»åå°é½çç¯æ¬ï¼å¼å¥ä¸åæ°ç©çæå­æ¯å°ä»»åãå¨ç¹å®ä»»åçèª¿æ´éæ®µï¼æåæåºå©ç¨®é¡å¥æç¤ºæ¹æ³ï¼å¾é²ä¸æ­¥å°é½ç¯æ¬çé¡å¤èªªæä¸­å­¸ç¿ç£ç£è³è¨ãå¨ååè³æéä¸çå¯¦é©è©ä¼°è­æäºç£ç£å¼å­¸ç¿ãå¤è³æéçæ¦æ¬æ§ï¼ç¹å¥æ¯å¨é¶æ¬¡å­¸ç¿è½åæ¹é¢æé¡¯èçé²æ­¥ï¼çªé¡¯äºè©²æ¨¡åä½çºåå½¢åºç¤æ¨¡åçæ½åã

##### **Unleashing the Power of LLMs as Multi-Modal Encoders for Text and Graph-Structured Data**
2410.11235v1 by Jiacheng Lin, Kun Qian, Haoyu Han, Nurendra Choudhary, Tianxin Wei, Zhongruo Wang, Sahika Genc, Edward W Huang, Sheng Wang, Karthik Subbian, Danai Koutra, Jimeng Sun

Graph-structured information offers rich contextual information that can
enhance language models by providing structured relationships and hierarchies,
leading to more expressive embeddings for various applications such as
retrieval, question answering, and classification. However, existing methods
for integrating graph and text embeddings, often based on Multi-layer
Perceptrons (MLPs) or shallow transformers, are limited in their ability to
fully exploit the heterogeneous nature of these modalities. To overcome this,
we propose Janus, a simple yet effective framework that leverages Large
Language Models (LLMs) to jointly encode text and graph data. Specifically,
Janus employs an MLP adapter to project graph embeddings into the same space as
text embeddings, allowing the LLM to process both modalities jointly. Unlike
prior work, we also introduce contrastive learning to align the graph and text
spaces more effectively, thereby improving the quality of learned joint
embeddings. Empirical results across six datasets spanning three tasks,
knowledge graph-contextualized question answering, graph-text pair
classification, and retrieval, demonstrate that Janus consistently outperforms
existing baselines, achieving significant improvements across multiple
datasets, with gains of up to 11.4% in QA tasks. These results highlight
Janus's effectiveness in integrating graph and text data. Ablation studies
further validate the effectiveness of our method.

æè¦ï¼åå½¢çµæ§åè³è¨æä¾è±å¯çèçµ¡è³è¨ï¼å¯ä»¥ééæä¾çµæ§åçéä¿åéå±¤ä¾å¢å¼·èªè¨æ¨¡åï¼é²èçºåç¨®æç¨ç¨å¼ï¼ä¾å¦æª¢ç´¢ãåç­ååé¡ï¼ç¢çæ´å·è¡¨ç¾åçåµå¥ãç¶èï¼ç¾æçåå½¢åæå­åµå¥æ´åæ¹æ³ï¼éå¸¸åºæ¼å¤å±¤æç¥å¨ (MLP) ææ·ºå±¤è½æå¨ï¼å¨ååå©ç¨éäºæ¨¡æçç°è³ªæ§æ¹é¢è½åæéãçºäºåæéä¸é»ï¼æåæåºäº Janusï¼ä¸åç°¡å®ä½ææçæ¡æ¶ï¼å®å©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾è¯åç·¨ç¢¼æå­ååå½¢è³æãå·é«ä¾èªªï¼Janus ä½¿ç¨ MLP é©éå¨å°åå½¢åµå¥æå½±å°èæå­åµå¥ç¸åçç©ºéï¼åè¨± LLM è¯åèçéå©ç¨®æ¨¡æãèååçç ç©¶ä¸åï¼æåéå¼å¥äºå°æ¯å­¸ç¿ï¼ä»¥æ´ææå°å°é½åå½¢åæå­ç©ºéï¼å¾èæé«å­¸ç¿å°çè¯ååµå¥çåè³ªãè·¨è¶å­åè³æéçå¯¦è­çµææ¶µèäºä¸åä»»åï¼ç¥è­åè­èçµ¡ååç­ãåå½¢æå­å°åé¡åæª¢ç´¢ï¼è­æ Janus æçºåªæ¼ç¾æåºæºï¼å¨å¤åè³æéä¸åå¾é¡¯èé²æ­¥ï¼å¨ QA ä»»åä¸­ç²å¾é«é 11.4% çæåãéäºçµæçªé¡¯äº Janus å¨æ´ååå½¢åæå­è³ææ¹é¢çæææ§ãæ¶èç ç©¶é²ä¸æ­¥é©è­äºæåæ¹æ³çæææ§ã

##### **Tree of Attributes Prompt Learning for Vision-Language Models**
2410.11201v1 by Tong Ding, Wanhua Li, Zhongqi Miao, Hanspeter Pfister

Prompt learning has proven effective in adapting vision language models for
downstream tasks. However, existing methods usually append learnable prompt
tokens solely with the category names to obtain textual features, which fails
to fully leverage the rich context indicated in the category name. To address
this issue, we propose the Tree of Attributes Prompt learning (TAP), which
first instructs LLMs to generate a tree of attributes with a "concept -
attribute - description" structure for each category, and then learn the
hierarchy with vision and text prompt tokens. Unlike existing methods that
merely augment category names with a set of unstructured descriptions, our
approach essentially distills structured knowledge graphs associated with class
names from LLMs. Furthermore, our approach introduces text and vision prompts
designed to explicitly learn the corresponding visual attributes, effectively
serving as domain experts. Additionally, the general and diverse descriptions
generated based on the class names may be wrong or absent in the specific given
images. To address this misalignment, we further introduce a vision-conditional
pooling module to extract instance-specific text features. Extensive
experimental results demonstrate that our approach outperforms state-of-the-art
methods on the zero-shot base-to-novel generalization, cross-dataset transfer,
as well as few-shot classification across 11 diverse datasets.

æè¦ï¼æç¤ºå­¸ç¿å·²è¢«è­æææå°å°è¦è¦ºèªè¨æ¨¡åé©ææ¼ä¸æ¸¸ä»»åãç¶èï¼ç¾ææ¹æ³éå¸¸åå°å¯å­¸ç¿çæç¤ºä»¤çéå å°é¡å¥åç¨±ä»¥ç²åææ¬ç¹å¾µï¼éæªè½ååå©ç¨é¡å¥åç¨±ä¸­æç¤ºçè±å¯ä¸ä¸æãçºäºè§£æ±ºéååé¡ï¼æåæåºäºå±¬æ§æç¤ºå­¸ç¿æ¨¹ (TAP)ï¼å®é¦åæç¤º LLM çºæ¯åé¡å¥çæä¸åå·æãæ¦å¿µ - å±¬æ§ - æè¿°ãçµæ§çå±¬æ§æ¨¹ï¼ç¶å¾ä½¿ç¨è¦è¦ºåææ¬æç¤ºä»¤çå­¸ç¿å±¤æ¬¡çµæ§ãèåä½¿ç¨ä¸çµéçµæ§åæè¿°ä¾æ´åé¡å¥åç¨±çç¾ææ¹æ³ä¸åï¼æåçåæ³å¯¦è³ªä¸å¾ LLM ä¸­æçåºèé¡å¥åç¨±ç¸éççµæ§åç¥è­åãæ­¤å¤ï¼æåçåæ³å¼å¥äºææ¬åè¦è¦ºæç¤ºï¼æ¨å¨æç¢ºå­¸ç¿å°æçè¦è¦ºå±¬æ§ï¼ææå°åç¶é åå°å®¶ãæ­¤å¤ï¼æ ¹æé¡å¥åç¨±çæçéç¨ä¸å¤æ¨£çæè¿°å¨çµ¦å®çç¹å®å½±åä¸­å¯è½æ¯é¯èª¤çæä¸å­å¨çãçºäºè§£æ±ºéç¨®é¯ä½ï¼æåé²ä¸æ­¥å¼å¥äºä¸åè¦è¦ºæ¢ä»¶æ± åæ¨¡çµä¾æåç¹å®æ¼å¯¦ä¾çææ¬ç¹å¾µãå»£æ³çå¯¦é©çµæè¡¨æï¼æåçåæ³å¨é¶æ¬¡å­¸ç¿åºç¤å°æ°ç©çæ¦åãè·¨è³æéå³è¼¸ä»¥å 11 åä¸åè³æéçå°æ¬¡å­¸ç¿åé¡ä¸åªæ¼æåé²çæ¹æ³ã

##### **Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs**
2410.11001v1 by Haozhen Zhang, Tao Feng, Jiaxuan You

Retrieval-augmented generation (RAG) has revitalized Large Language Models
(LLMs) by injecting non-parametric factual knowledge. Compared with
long-context LLMs, RAG is considered an effective summarization tool in a more
concise and lightweight manner, which can interact with LLMs multiple times
using diverse queries to get comprehensive responses. However, the
LLM-generated historical responses, which contain potentially insightful
information, are largely neglected and discarded by existing approaches,
leading to suboptimal results. In this paper, we propose \textit{graph of
records} (\textbf{GoR}), which leverages historical responses generated by LLMs
to enhance RAG for long-context global summarization. Inspired by the
\textit{retrieve-then-generate} paradigm of RAG, we construct a graph by
establishing an edge between the retrieved text chunks and the corresponding
LLM-generated response. To further uncover the intricate correlations between
them, GoR further features a \textit{graph neural network} and an elaborately
designed \textit{BERTScore}-based objective for self-supervised model training,
enabling seamless supervision signal backpropagation between reference
summaries and node embeddings. We comprehensively compare GoR with 12 baselines
across four long-context summarization datasets, and the results indicate that
our proposed method reaches the best performance e.g., 15\%, 8\%, and 19\%
improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP
dataset). Extensive experiments further demonstrate the effectiveness of GoR.
Code is available at https://github.com/ulab-uiuc/GoR

æè¦ï¼æ£ç´¢å¢å¼ºçæ (RAG) éè¿æ³¨å¥éåæ°äºå®ç¥è¯ï¼è®©å¤§åè¯­è¨æ¨¡å (LLM) éè·çæºãä¸é¿ææ¬ LLM ç¸æ¯ï¼RAG è¢«è§ä¸ºä¸ç§æ´ç®æ´ãè½»éçº§çæææè¦å·¥å·ï¼å®å¯ä»¥ä½¿ç¨ä¸åçæ¥è¯¢ä¸ LLM å¤æ¬¡äºå¨ï¼ä»¥è·å¾å¨é¢çååºãç¶èï¼ç°æçæ¹æ³å¨å¾å¤§ç¨åº¦ä¸å¿½ç¥å¹¶èå¼äº LLM çæçåå²ååºï¼å¶ä¸­åå«æ½å¨çæè§è§£çä¿¡æ¯ï¼ä»èå¯¼è´æ¬¡ä¼çç»æãå¨æ¬æä¸­ï¼æä»¬æåºäºãè®°å½å¾ã(**GoR**)ï¼å®å©ç¨ LLM çæçåå²ååºæ¥å¢å¼º RAGï¼ä»¥è¿è¡é¿ææ¬å¨å±æè¦ãå RAG çãåæ£ç´¢åçæãèä¾å¯åï¼æä»¬éè¿å¨æ£ç´¢å°çææ¬ååç¸åºç LLM çæçååºä¹é´å»ºç«è¾¹æ¥æå»ºå¾ãä¸ºäºè¿ä¸æ­¥æ­ç¤ºå®ä»¬ä¹é´çå¤æç¸å³æ§ï¼GoR è¿ä¸æ­¥éç¨äºãå¾ç¥ç»ç½ç»ãåç²¾å¿è®¾è®¡çåºäºãBERTScoreãçç®æ ï¼ç¨äºèªæçç£æ¨¡åè®­ç»ï¼ä»èå¨åèæè¦åèç¹åµå¥ä¹é´å®ç°æ ç¼ççç£ä¿¡å·ååä¼ æ­ãæä»¬å¯¹ GoR ä¸ 12 ä¸ªåºåè¿è¡äºå¨é¢æ¯è¾ï¼æ¶µçäºåä¸ªé¿ææ¬æè¦æ°æ®éï¼ç»æè¡¨ææä»¬æåºçæ¹æ³è¾¾å°äºæä½³æ§è½ï¼ä¾å¦ï¼å¨ WCEP æ°æ®éä¸ï¼ç¸å¯¹äºæ£ç´¢å¨ï¼Rouge-LãRouge-1 å Rouge-2 åå«æé«äº 15%ã8% å 19%ãå¹¿æ³çå®éªè¿ä¸æ­¥è¯æäº GoR çæææ§ãä»£ç å¯å¨ https://github.com/ulab-uiuc/GoR è·å¾

##### **NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models**
2410.10743v1 by Yanbiao Ji, Chang Liu, Xin Chen, Yue Ding, Dan Luo, Mei Li, Wenqing Lin, Hongtao Lu

Graphs are a fundamental data structure for representing relationships in
real-world scenarios. With the success of Large Language Models (LLMs) across
various natural language processing (NLP) tasks, there has been growing
interest in integrating LLMs for graph learning. However, applying LLMs to
graph-related tasks poses significant challenges, as these models are not
inherently designed to capture the complex structural information present in
graphs. Existing approaches address this challenge through two strategies: the
chain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the
graph structure so that LLMs are relieved from understanding spatial positions;
and Graph-to-Text Conversion, which translates graph structures into semantic
text representations that LLMs can process. Despite their progress, these
methods often struggle to fully preserve the topological information of graphs
or require extensive computational resources, limiting their practical
applicability.
  In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM),
a novel framework that efficiently encodes graph structures by selecting key
nodes as anchors and representing each node based on its relative distance to
these anchors. This position-anchored encoding effectively captures the graph
topology, enabling enhanced reasoning capabilities in LLMs over graph data.
Additionally, we implement a task-specific tuning procedure to further improve
structural understanding within LLMs. Through extensive empirical evaluations,
NT-LLM demonstrates significant performance improvements across a variety of
graph-related tasks.

æè¦ï¼åå½¢æ¯ä¸ç¨®åºæ¬è³æçµæ§ï¼ç¨æ¼è¡¨ç¤ºç¾å¯¦ä¸çå ´æ¯ä¸­çéä¿ãé¨èå¤§åèªè¨æ¨¡å (LLM) å¨åç¨®èªç¶èªè¨èç (NLP) ä»»åä¸­çæåï¼æ´å LLM ä»¥é²è¡åå½¢å­¸ç¿çèè¶£æ¥çæ¿åãç¶èï¼å° LLM æç¨æ¼èåå½¢ç¸éçä»»åæå¸¶ä¾éå¤§ææ°ï¼å çºéäºæ¨¡åä¸¦éå¤©çå°±è¨­è¨æç¨ä¾æ·ååå½¢ä¸­å­å¨çè¤éçµæ§è³è¨ãç¾ææ¹æ³ééå©ç¨®ç­ç¥ä¾æå°æ­¤ææ°ï¼ä»»åéæ¹æ³ï¼å®ä½¿ç¨åå½¢ç¥ç¶ç¶²è·¯ (GNN) ç·¨ç¢¼åå½¢çµæ§ï¼ä»¥ä¾¿æ¸è¼ LLM çè§£ç©ºéä½ç½®çè² æï¼ä»¥ååå½¢è½æå­è½æï¼å®å°åå½¢çµæ§è½ææ LLM å¯ä»¥èççèªææå­è¡¨ç¤ºãåç®¡éäºæ¹æ³åå¾äºé²å±ï¼ä½å®åéå¸¸é£ä»¥å®å¨ä¿çåå½¢çææ²è³è¨ï¼æèéè¦å¤§éçéç®è³æºï¼éå¶äºå®åçå¯¦éæç¨æ§ã
å¨æ¬æä¸­ï¼æåä»ç´¹äºå¤§åèªè¨æ¨¡åç¯é»æ¨è¨å¨ (NT-LLM)ï¼éæ¯ä¸åæ°ç©çæ¡æ¶ï¼å®ééé¸æééµç¯é»ä½çºé¨é»ï¼ä¸¦æ ¹ææ¯åç¯é»èéäºé¨é»çç¸å°è·é¢ä¾è¡¨ç¤ºæ¯åç¯é»ï¼å¾èææå°ç·¨ç¢¼åå½¢çµæ§ãéç¨®åºæ¼ä½ç½®çé¨é»ç·¨ç¢¼ææå°æ·åäºåå½¢ææ²ï¼è® LLM è½å¤ å°åå½¢è³æé²è¡å¢å¼·çæ¨çãæ­¤å¤ï¼æåå¯¦ä½äºä¸åç¹å®æ¼ä»»åçèª¿æ´ç¨åºï¼ä»¥é²ä¸æ­¥æ¹å LLM ä¸­ççµæ§çè§£ãééå»£æ³çå¯¦è­è©ä¼°ï¼NT-LLM å¨åç¨®èåå½¢ç¸éçä»»åä¸­é½å±ç¤ºåºé¡¯èçæè½æåã

##### **GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs**
2410.10329v2 by Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, Siliang Tang

Recently, research on Text-Attributed Graphs (TAGs) has gained significant
attention due to the prevalence of free-text node features in real-world
applications and the advancements in Large Language Models (LLMs) that bolster
TAG methodologies. However, current TAG approaches face two primary challenges:
(i) Heavy reliance on label information and (ii) Limited cross-domain
zero/few-shot transferability. These issues constrain the scaling of both data
and model size, owing to high labor costs and scaling laws, complicating the
development of graph foundation models with strong transferability. In this
work, we propose the GraphCLIP framework to address these challenges by
learning graph foundation models with strong cross-domain zero/few-shot
transferability through a self-supervised contrastive graph-summary pretraining
method. Specifically, we generate and curate large-scale graph-summary pair
data with the assistance of LLMs, and introduce a novel graph-summary
pretraining method, combined with invariant learning, to enhance graph
foundation models with strong cross-domain zero-shot transferability. For
few-shot learning, we propose a novel graph prompt tuning technique aligned
with our pretraining objective to mitigate catastrophic forgetting and minimize
learning costs. Extensive experiments show the superiority of GraphCLIP in both
zero-shot and few-shot settings, while evaluations across various downstream
tasks confirm the versatility of GraphCLIP. Our code is available at:
https://github.com/ZhuYun97/GraphCLIP

æè¦ï¼æè¿ï¼ææ¬å±æ§å¾ï¼TAGï¼çç ç©¶ç±äºç°å®ä¸çåºç¨ä¸­èªç±ææ¬èç¹ç¹å¾çæ®éæ§ä»¥åæ¯æ TAG æ¹æ³çå¤§è¯­è¨æ¨¡åï¼LLMï¼çè¿æ­¥èå¤åå³æ³¨ãç¶èï¼å½åç TAG æ¹æ³é¢ä¸´ä¸¤å¤§ä¸»è¦ææï¼(i) å¯¹æ ç­¾ä¿¡æ¯çä¸¥éä¾èµï¼ä»¥å (ii) è·¨åé¶/å°æ ·æ¬å¯è¿ç§»æ§çåéãç±äºé«æçäººåææ¬åè§æ¨¡åå®å¾ï¼è¿äºé®é¢éå¶äºæ°æ®åæ¨¡åè§æ¨¡çæ©å±ï¼ä½¿å¾å·æå¼ºå¤§å¯è¿ç§»æ§çå¾åºç¡æ¨¡åçå¼ååå¾å¤æãå¨è¿é¡¹å·¥ä½ä¸­ï¼æä»¬æåºäº GraphCLIP æ¡æ¶ï¼éè¿èªçç£å¯¹æ¯å¾æè¦é¢è®­ç»æ¹æ³æ¥å­¦ä¹ å·æå¼ºå¤§è·¨åé¶/å°æ ·æ¬å¯è¿ç§»æ§çå¾åºç¡æ¨¡åï¼ä»¥åºå¯¹è¿äºææãå·ä½æ¥è¯´ï¼æä»¬åå© LLM çæå¹¶æ´çäºå¤§è§æ¨¡å¾æè¦å¯¹æ°æ®ï¼å¹¶å¼å¥äºä¸ç§æ°é¢çå¾æè¦é¢è®­ç»æ¹æ³ï¼ç»åä¸åæ§å­¦ä¹ ï¼ä»¥å¢å¼ºå·æå¼ºå¤§è·¨åé¶æ ·æ¬å¯è¿ç§»æ§çå¾åºç¡æ¨¡åãå¯¹äºå°æ ·æ¬å­¦ä¹ ï¼æä»¬æåºäºä¸ç§æ°é¢çå¾æç¤ºè°æ´ææ¯ï¼è¯¥ææ¯ä¸æä»¬çé¢è®­ç»ç®æ ä¸è´ï¼ä»¥åè½»ç¾é¾æ§éå¿å¹¶æå¤§ç¨åº¦å°éä½å­¦ä¹ ææ¬ãå¤§éçå®éªè¡¨æï¼GraphCLIP å¨é¶æ ·æ¬åå°æ ·æ¬è®¾ç½®ä¸­é½å·æä¼è¶æ§ï¼åæ¶å¯¹åç§ä¸æ¸¸ä»»å¡çè¯ä¼°è¯å®äº GraphCLIP çå¤åè½æ§ãæä»¬çä»£ç å¯å¨ä»¥ä¸ä½ç½®è·å¾ï¼
https://github.com/ZhuYun97/GraphCLIP

##### **Unified Representation of Genomic and Biomedical Concepts through Multi-Task, Multi-Source Contrastive Learning**
2410.10144v1 by Hongyi Yuan, Suqi Liu, Kelly Cho, Katherine Liao, Alexandre Pereira, Tianxi Cai

We introduce GENomic Encoding REpresentation with Language Model (GENEREL), a
framework designed to bridge genetic and biomedical knowledge bases. What sets
GENEREL apart is its ability to fine-tune language models to infuse biological
knowledge behind clinical concepts such as diseases and medications. This
fine-tuning enables the model to capture complex biomedical relationships more
effectively, enriching the understanding of how genomic data connects to
clinical outcomes. By constructing a unified embedding space for biomedical
concepts and a wide range of common SNPs from sources such as patient-level
data, biomedical knowledge graphs, and GWAS summaries, GENEREL aligns the
embeddings of SNPs and clinical concepts through multi-task contrastive
learning. This allows the model to adapt to diverse natural language
representations of biomedical concepts while bypassing the limitations of
traditional code mapping systems across different data sources. Our experiments
demonstrate GENEREL's ability to effectively capture the nuanced relationships
between SNPs and clinical concepts. GENEREL also emerges to discern the degree
of relatedness, potentially allowing for a more refined identification of
concepts. This pioneering approach in constructing a unified embedding system
for both SNPs and biomedical concepts enhances the potential for data
integration and discovery in biomedical research.

æè¦ï¼<paragraph>æåä»ç´¹ GENomic Encoding REpresentation with Language Model (GENEREL)ï¼ä¸åæ¨å¨æ©æ¥éºå³åçç©é«å­¸ç¥è­åº«çæ¡æ¶ãGENEREL çç¨ç¹ä¹èå¨æ¼å®å¾®èª¿èªè¨æ¨¡åï¼ä»¥çè¼¸ç¾çåè¥ç©ç­è¨åºæ¦å¿µèå¾ççç©ç¥è­ãéç¨®å¾®èª¿ä½¿æ¨¡åè½å¤ æ´ææå°ææè¤éççç©é«å­¸éä¿ï¼è±å¯å°åºå çµæ¸æå¦ä½é£æ¥è¨åºçµæççè§£ãééæ§å»ºä¸åçµ±ä¸ççç©é«å­¸æ¦å¿µåµå¥ç©ºéåä¾èªæ£èç´å¥æ¸æãçç©é«å­¸ç¥è­åè­å GWAS ç¸½çµç­ä¾æºçå»£æ³å¸¸è¦ SNPï¼GENEREL ééå¤ä»»åå°æ¯å­¸ç¿å°é½ SNP åè¨åºæ¦å¿µçåµå¥ãéåè¨±æ¨¡åé©æçç©é«å­¸æ¦å¿µçå¤åèªç¶èªè¨è¡¨ç¤ºï¼åæç¹éä¸åæ¸ææºä¸­å³çµ±ä»£ç¢¼æ å°ç³»çµ±çéå¶ãæåçå¯¦é©è­æäº GENEREL ææææ SNP åè¨åºæ¦å¿µä¹éç´°å¾®éä¿çè½åãGENEREL ä¹åºç¾äºè¾¨å¥ç¸éç¨åº¦ï¼æ½å¨å°åè¨±æ´ç²¾ç¢ºå°è­å¥æ¦å¿µãéç¨®æ§å»º SNP åçç©é«å­¸æ¦å¿µçµ±ä¸åµå¥ç³»çµ±çåé©æ¹æ³å¢å¼·äºçç©é«å­¸ç ç©¶ä¸­æ¸ææ´ååç¼ç¾çæ½åã</paragraph>

##### **Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?**
2410.10083v2 by Yifan Feng, Chengwu Yang, Xingliang Hou, Shaoyi Du, Shihui Ying, Zongze Wu, Yue Gao

Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by
focusing mainly on pairwise relationships, overlooking the high-order
correlations found in real-world data. Hypergraphs, which can model complex
beyond-pairwise relationships, offer a more robust framework but are still
underexplored in the context of LLMs. To address this gap, we introduce
LLM4Hypergraph, the first comprehensive benchmark comprising 21,500 problems
across eight low-order, five high-order, and two isomorphism tasks, utilizing
both synthetic and real-world hypergraphs from citation networks and protein
structures. We evaluate six prominent LLMs, including GPT-4o, demonstrating our
benchmark's effectiveness in identifying model strengths and weaknesses. Our
specialized prompting framework incorporates seven hypergraph languages and
introduces two novel techniques, Hyper-BAG and Hyper-COT, which enhance
high-order reasoning and achieve an average 4% (up to 9%) performance
improvement on structure classification tasks. This work establishes a
foundational testbed for integrating hypergraph computational capabilities into
LLMs, advancing their comprehension. The source codes are at
https://github.com/iMoonLab/LLM4Hypergraph.

æè¦ï¼ç¾æç NLGraph å GraphQA ç­åºæºä¸»è¦éæ³¨æå°éä¿ï¼èå¿½ç¥äºå¨ç¾å¯¦ä¸çè³æä¸­ç¼ç¾çé«éç¸éæ§ï¼å¾èå°åå½¢ä¸­ç LLM é²è¡è©ä¼°ãè¶åå¯ä»¥å»ºæ¨¡è¤éçè¶è¶æå°éä¿ï¼æä¾æ´å¼·å¤§çæ¡æ¶ï¼ä½å¨ LLM çèæ¯ä¸ä»æªå¾å°ååæ¢ç´¢ãçºäºè§£æ±ºéåå·®è·ï¼æåå¼å¥äº LLM4Hypergraphï¼éæ¯ç¬¬ä¸åç¶ååºæºï¼åå« 21,500 ååé¡ï¼æ¶µèå«åä½éãäºåé«éåå©ååæ§ä»»åï¼å©ç¨ä¾èªå¼æç¶²è·¯åèç½è³ªçµæ§çåæåçå¯¦ä¸çè¶åãæåè©ä¼°äºå­åèåç LLMï¼åæ¬ GPT-4oï¼è­æäºæåçåºæºå¨è­å¥æ¨¡ååªå¢åå£å¢æ¹é¢çæææ§ãæåå°æ¥­çæç¤ºæ¡æ¶åå«ä¸ç¨®è¶åèªè¨ï¼ä¸¦å¼å¥äºå©ç¨®æ°æè¡ Hyper-BAG å Hyper-COTï¼å®åå¢å¼·äºé«éæ¨çï¼ä¸¦å¨çµæ§åé¡ä»»åä¸å¯¦ç¾äºå¹³å 4%ï¼æé« 9%ï¼çæ§è½æ¹é²ãéé å·¥ä½çºå°è¶åè¨ç®è½åæ´åå° LLM ä¸­å»ºç«äºä¸ååºç¤æ¸¬è©¦å¹³å°ï¼å¾èæåäºå®åççè§£åãæºä»£ç¢¼ä½æ¼ https://github.com/iMoonLab/LLM4Hypergraphã

##### **Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent Simulation**
2410.09824v1 by Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding

Graph generation is a fundamental task that has been extensively studied in
social, technological, and scientific analysis. For modeling the dynamic graph
evolution process, traditional rule-based methods struggle to capture community
structures within graphs, while deep learning methods only focus on fitting
training graphs. This limits existing graph generators to producing graphs that
adhere to predefined rules or closely resemble training datasets, achieving
poor performance in dynamic graph generation. Given that graphs are abstract
representations arising from pairwise interactions in human activities, a
realistic simulation of human-wise interaction could provide deeper insights
into the graph evolution mechanism. With the increasing recognition of large
language models (LLMs) in simulating human behavior, we introduce
GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic
graph generation. Without training or fine-tuning process of LLM, our framework
effectively replicates seven macro-level structural characteristics in
established network science theories while surpassing existing baselines in
graph expansion tasks by 31\% on specific evaluation metrics. Through node
classification task, we validate GAG effectively preserves characteristics of
real-world network for node-wise textual features in generated text-rich graph.
Furthermore, by incorporating parallel acceleration, GAG supports generating
graphs with up to nearly 100,000 nodes or 10 million edges through large-scale
LLM-based agent simulation, with a minimum speed-up of 90.4\%. The source code
is available at https://anonymous.4open.science/r/GraphAgent-2206.

æè¦ï¼åè¡¨çææ¯ä¸é åºæ¬ä»»åï¼å·²å¨ç¤¾æãæè¡åç§å­¸åæä¸­å»£æ³ç ç©¶ãå°æ¼å»ºæ¨¡åæåè¡¨æ¼åéç¨ï¼å³çµ±åºæ¼è¦åçæ¹æ³é£ä»¥ææåè¡¨ä¸­çç¤¾ç¾¤çµæ§ï¼èæ·±åº¦å­¸ç¿æ¹æ³åå°æ³¨æ¼æ¬åè¨ç·´åè¡¨ãééå¶äºç¾æçåè¡¨çæå¨ç¢çç¬¦åé å®ç¾©è¦åæèè¨ç·´è³æééå¸¸ç¸ä¼¼çåè¡¨ï¼å¨åæåè¡¨çæä¸­è¡¨ç¾ä¸ä½³ãç±æ¼åè¡¨æ¯æºèªäººé¡æ´»åä¸­æå°äºåçæ½è±¡è¡¨ç¤ºï¼å æ­¤å°äººé¡äºåçé¼çæ¨¡æ¬å¯ä»¥æä¾å°åè¡¨æ¼åæ©å¶çæ´æ·±å¥è¦è§£ãé¨èå¤§åèªè¨æ¨¡å (LLM) å¨æ¨¡æ¬äººé¡è¡çºæ¹é¢ç²å¾è¶ä¾è¶å¤çèªå¯ï¼æåå¼å¥äº GraphAgent-Generator (GAG)ï¼éæ¯ä¸åç¨æ¼åæåè¡¨çæçåµæ°åºæ¼æ¨¡æ¬çæ¡æ¶ãå¨æ²æ LLM çè¨ç·´æå¾®èª¿éç¨ä¸­ï¼æåçæ¡æ¶ææå°è¤è£½äºå·²å»ºç«çç¶²è·¯ç§å­¸çè«ä¸­çä¸åå·¨è§å±¤ç´çµæ§ç¹å¾µï¼åæå¨å·é«è©ä¼°ææ¨ä¸è¶è¶äºç¾æçåºæºï¼åè¡¨æ´åä»»åæé«äº 31%ãééç¯é»åé¡ä»»åï¼æåé©è­ GAG ææå°ä¿çäºçææå­è±å¯åè¡¨ä¸­ç¯é»æå­ç¹å¾µççå¯¦ä¸çç¶²è·¯ç¹å¾µãæ­¤å¤ï¼ééçµåä¸¦è¡å éï¼GAG æ¯æ´ééå¤§è¦æ¨¡ LLM åºæ¼ä»£ççæ¨¡æ¬çæå¤éè¿ 100,000 åç¯é»æ 1,000 è¬æ¢éçåè¡¨ï¼éåº¦æåè³å° 90.4%ãåå§ç¢¼å¯å¨ https://anonymous.4open.science/r/GraphAgent-2206 åå¾ã

##### **A Mixed-Language Multi-Document News Summarization Dataset and a Graphs-Based Extract-Generate Model**
2410.09773v1 by Shengxiang Gao, Fang nan, Yongbing Zhang, Yuxin Huang, Kaiwen Tan, Zhengtao Yu

Existing research on news summarization primarily focuses on single-language
single-document (SLSD), single-language multi-document (SLMD) or cross-language
single-document (CLSD). However, in real-world scenarios, news about a
international event often involves multiple documents in different languages,
i.e., mixed-language multi-document (MLMD). Therefore, summarizing MLMD news is
of great significance. However, the lack of datasets for MLMD news
summarization has constrained the development of research in this area. To fill
this gap, we construct a mixed-language multi-document news summarization
dataset (MLMD-news), which contains four different languages and 10,992 source
document cluster and target summary pairs. Additionally, we propose a
graph-based extract-generate model and benchmark various methods on the
MLMD-news dataset and publicly release our dataset and
code\footnote[1]{https://github.com/Southnf9/MLMD-news}, aiming to advance
research in summarization within MLMD scenarios.

æè¦ï¼ç¾ææ°èæè¦çç ç©¶ä¸»è¦éä¸­å¨å®èªè¨å®æä»¶ (SLSD)ãå®èªè¨å¤æä»¶ (SLMD) æè·¨èªè¨å®æä»¶ (CLSD)ãç¶èï¼å¨ç¾å¯¦ä¸ççå ´æ¯ä¸­ï¼åéäºä»¶çæ°èéå¸¸æ¶åä¸åèªè¨çå¤åæä»¶ï¼å³æ··åèªè¨å¤æä»¶ (MLMD)ãå æ­¤ï¼å° MLMD æ°èé²è¡æè¦å·æéå¤§æç¾©ãç¶èï¼ç¼ºä¹ MLMD æ°èæè¦çæ¸æééå¶äºéä¸é åçç ç©¶ç¼å±ãçºäºå¡«è£éä¸ç©ºç½ï¼æåæ§å»ºäºä¸åæ··åèªè¨å¤æä»¶æ°èæè¦æ¸æé (MLMD-news)ï¼å¶ä¸­åå«åç¨®ä¸åçèªè¨å 10,992 åæºæä»¶ç¾¤éåç®æ¨æè¦å°ãæ­¤å¤ï¼æåæåºäºä¸ååºæ¼åçæåçææ¨¡åï¼ä¸¦å¨ MLMD-news æ¸æéä¸å°åç¨®æ¹æ³é²è¡äºåºæºæ¸¬è©¦ï¼ä¸¦å¬éç¼å¸æåçæ¸æéåä»£ç¢¼\footnote[1]{https://github.com/Southnf9/MLMD-news}ï¼æ¨å¨æ¨é² MLMD å ´æ¯ä¸­çæè¦ç ç©¶ã

##### **Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG**
2410.09699v1 by Xinxi Chen, Li Wang, Wei Wu, Qi Tang, Yiyao Liu

Hallucination is a key roadblock for applications of Large Language Models
(LLMs), particularly for enterprise applications that are sensitive to
information accuracy. To address this issue, two general approaches have been
explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated
information as context, and fine-tuning the LLMs with new information and
desired output styles. In this paper, we propose Honest AI: a novel strategy to
fine-tune "small" language models to say "I don't know" to reduce
hallucination, along with several alternative RAG approaches. The solution
ranked 1st in Task 2 for the false premise question. The alternative approaches
include using RAG with search engine and knowledge graph results, fine-tuning
base LLMs with new information and combinations of both approaches. Although
all approaches improve the performance of the LLMs, RAG alone does not
significantly improve the performance and fine-tuning is needed for better
results. Finally, the hybrid approach achieved the highest score in the CRAG
benchmark. In addition, our approach emphasizes the use of relatively small
models with fewer than 10 billion parameters, promoting resource efficiency.

æè¦ï¼å¹»è¦ºæ¯å¤§åèªè¨æ¨¡å (LLM) æç¨ç¨å¼çä¸å¤§éç¤ï¼ç¹å¥æ¯å°è³è¨æºç¢ºåº¦ææçä¼æ¥­æç¨ç¨å¼ãçºäºè§£æ±ºæ­¤åé¡ï¼å·²æ¢è¨å©ç¨®ä¸è¬æ¹æ³ï¼æª¢ç´¢æ´åçæ (RAG) ä»¥æä¾ LLM æ´æ°çè³è¨ä½çºèæ¯ï¼ä»¥åå¾®èª¿ LLM ä»¥ç²å¾æ°çè³è¨åææçè¼¸åºæ¨£å¼ãå¨æ¬æä¸­ï¼æåæåº Honest AIï¼ä¸ç¨®æ°ç©çç­ç¥ï¼å¾®èª¿ãå°åãèªè¨æ¨¡åä»¥è¡¨éãæä¸ç¥éãä»¥æ¸å°å¹»è¦ºï¼ä»¥åå¶ä»å¹¾ç¨®æ¿ä»£ç RAG æ¹æ³ãè©²è§£æ±ºæ¹æ¡å¨èååæåé¡çä»»å 2 ä¸­æåç¬¬ 1ãæ¿ä»£æ¹æ³åæ¬ä½¿ç¨ RAG æ­éæå°å¼æåç¥è­åè­çµæãå¾®èª¿åºç¤ LLM ä»¥ç²å¾æ°çè³è¨ï¼ä»¥åçµåéå©ç¨®æ¹æ³ãéç¶æææ¹æ³é½æ¹åäº LLM çæè½ï¼ä½åä½¿ç¨ RAG ç¡æ³é¡¯èæ¹åæè½ï¼éè¦å¾®èª¿æè½ç²å¾æ´å¥½ççµæãæå¾ï¼æ··åæ¹æ³å¨ CRAG åºæºæ¸¬è©¦ä¸­ç²å¾æé«åãæ­¤å¤ï¼æåçåæ³å¼·èª¿ä½¿ç¨åæ¸å°æ¼ 100 åçå°åæ¨¡åï¼ä»¥ä¿é²è³æºæçã

##### **LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning**
2410.09541v1 by Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Jun Zhao

Large language models (LLMs) sometimes demonstrate poor performance on
knowledge-intensive tasks, commonsense reasoning is one of them. Researchers
typically address these issues by retrieving related knowledge from knowledge
graphs or employing self-enhancement methods to elicit knowledge in LLMs.
However, noisy knowledge and invalid reasoning issues hamper their ability to
answer questions accurately. To this end, we propose a novel method named
eliciting, filtering and integrating knowledge in large language model
(LINKED). In it, we design a reward model to filter out the noisy knowledge and
take the marginal consistent reasoning module to reduce invalid reasoning. With
our comprehensive experiments on two complex commonsense reasoning benchmarks,
our method outperforms SOTA baselines (up to 9.0% improvement of accuracy).
Besides, to measure the positive and negative impact of the injected knowledge,
we propose a new metric called effectiveness-preservation score for the
knowledge enhancement works. Finally, through extensive experiments, we conduct
an in-depth analysis and find many meaningful conclusions about LLMs in
commonsense reasoning tasks.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼ææå¨ç¥è­å¯éåä»»åä¸è¡¨ç¾ä¸ä½³ï¼å¸¸è­æ¨çå°±æ¯å¶ä¸­ä¹ä¸ãç ç©¶äººå¡éå¸¸ééå¾ç¥è­åè­ä¸­æª¢ç´¢ç¸éç¥è­ææ¡ç¨èªæå¢å¼·æ¹æ³ä¾å¼ç¼ LLM ä¸­çç¥è­ä¾è§£æ±ºéäºåé¡ãç¶èï¼åéçç¥è­åç¡æçæ¨çåé¡é»ç¤äºå®åæºç¢ºåç­åé¡çè½åãçºæ­¤ï¼æåæåºäºä¸ç¨®åçºå¤§åèªè¨æ¨¡åä¸­ç¥è­çå¼åºãéæ¿¾åæ´åï¼LINKEDï¼çæ°æ¹æ³ãå¨å¶ä¸­ï¼æåè¨­è¨äºä¸åçåµæ¨¡åä¾éæ¿¾æåéçç¥è­ï¼ä¸¦æ¡ç¨ééä¸è´æ¨çæ¨¡çµä¾æ¸å°ç¡ææ¨çãééæåå¨å©åè¤éçå¸¸è­æ¨çåºæºä¸çå¨é¢å¯¦é©ï¼æåçæ¨¡ååªæ¼ SOTA åºæºï¼æºç¢ºçæé«äº 9.0%ï¼ãæ­¤å¤ï¼çºäºè¡¡éæ³¨å¥ç¥è­çæ­£é¢åè² é¢å½±é¿ï¼æåæåºäºä¸ç¨®æ°çææ¨ï¼ç¨±çºç¥è­å¢å¼·å·¥ä½çæææ§ä¿çåæ¸ãæå¾ï¼ééå¤§éçå¯¦é©ï¼æåé²è¡äºæ·±å¥çåæï¼ä¸¦å¨å¸¸è­æ¨çä»»åä¸­ç¼ç¾äºè¨±å¤éæ¼ LLM çææç¾©ççµè«ã

##### **Text Classification using Graph Convolutional Networks: A Comprehensive Survey**
2410.09399v1 by Syed Mustafa Haider Rizvi, Ramsha Imran, Arif Mahmood

Text classification is a quintessential and practical problem in natural
language processing with applications in diverse domains such as sentiment
analysis, fake news detection, medical diagnosis, and document classification.
A sizable body of recent works exists where researchers have studied and
tackled text classification from different angles with varying degrees of
success. Graph convolution network (GCN)-based approaches have gained a lot of
traction in this domain over the last decade with many implementations
achieving state-of-the-art performance in more recent literature and thus,
warranting the need for an updated survey. This work aims to summarize and
categorize various GCN-based Text Classification approaches with regard to the
architecture and mode of supervision. It identifies their strengths and
limitations and compares their performance on various benchmark datasets. We
also discuss future research directions and the challenges that exist in this
domain.

æè¦ï¼ææ¬åé¡æ¯èªç¶èªè¨èçä¸­ä¸åç¶å¸ä¸å¯¦ç¨çåé¡ï¼å¨æç·åæãåæ°èåµæ¸¬ãé«çè¨ºæ·åæä»¶åé¡ç­é åä¸­é½ææç¨ãæè¿æå¤§éçç ç©¶æ¢è¨ææ¬åé¡ï¼ä¸¦å¾ä¸åçè§åº¦èæï¼ç²å¾äºä¸åç¨åº¦çæåãåå½¢å·ç©ç¶²è·¯ (GCN) æ¹æ³å¨éå»åå¹´ä¸­å¨éé åç²å¾äºè¨±å¤éæ³¨ï¼è¨±å¤å¯¦ä½å¨æè¿çæç»ä¸­éå°äºæåé²çæè½ï¼å æ­¤æå¿è¦é²è¡æ´æ°çèª¿æ¥ãéé å·¥ä½æ¨å¨éå°æ¶æ§åç£ç£æ¨¡å¼ï¼ç¸½çµååé¡åç¨®åºæ¼ GCN çææ¬åé¡æ¹æ³ãå®æ¾åºå®åçåªç¼ºé»ï¼ä¸¦æ¯è¼å®åå¨åç¨®åºæºè³æéä¸çæè½ãæåä¹è¨è«äºæªä¾ç ç©¶æ¹ååéåé åä¸­å­å¨çææ°ã

##### **Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation**
2410.09350v1 by Jinyoung Park, Minseok Joo, Joo-Kyung Kim, Hyunwoo J. Kim

Knowledge graph-grounded dialog generation requires retrieving a
dialog-relevant subgraph from the given knowledge base graph and integrating it
with the dialog history. Previous works typically represent the graph using an
external encoder, such as graph neural networks, and retrieve relevant triplets
based on the similarity between single-vector representations of triplets and
the dialog history. However, these external encoders fail to leverage the rich
knowledge of pretrained language models, and the retrieval process is also
suboptimal due to the information bottleneck caused by the single-vector
abstraction of the dialog history. In this work, we propose Dialog generation
with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant
knowledge subgraphs by directly generating their token sequences on top of
language models. For effective generative subgraph retrieval, we introduce two
key methods: (i) structure-aware knowledge graph linearization with
self-supervised graph-specific tokens and (ii) graph-constrained decoding
utilizing graph structural proximity-based entity informativeness scores for
valid and relevant generative retrieval. DialogGSR achieves state-of-the-art
performance in knowledge graph-grounded dialog generation, as demonstrated on
OpenDialKG and KOMODIS datasets.

æè¦ï¼ç¥è­åè­å°è©±çæéè¦å¾çµ¦å®çç¥è­åº«åè­ä¸­æ·åèå°è©±ç¸éçå­åï¼ä¸¦å°å¶èå°è©±è¨éæ´åãååçç ç©¶éå¸¸ä½¿ç¨å¤é¨ç·¨ç¢¼å¨ï¼ä¾å¦åå½¢ç¥ç¶ç¶²è·¯ï¼ä¾è¡¨ç¤ºåå½¢ï¼ä¸¦æ ¹æä¸åçµçå®åéè¡¨ç¤ºèå°è©±è¨éä¹éçç¸ä¼¼æ§ä¾æ·åç¸éçä¸åçµãç¶èï¼éäºå¤é¨ç·¨ç¢¼å¨ç¡æ³å©ç¨é è¨ç·´èªè¨æ¨¡åçè±å¯ç¥è­ï¼èæ·åéç¨ä¹å å°è©±è¨éçå®åéæ½è±¡åé æçè³è¨ç¶é ¸èæ¬¡æ¼æä½³ãå¨éé å·¥ä½ä¸­ï¼æåæåºå¸¶æçæå­åæ·åçå°è©±çæï¼DialogGSRï¼ï¼å®ç´æ¥å¨èªè¨æ¨¡åä¹ä¸çæå¶æ¨è¨åºåä¾æ·åç¸éçç¥è­å­åãçºäºææå°çæå­åæ·åï¼æåå¼å¥äºå©ç¨®ééµæ¹æ³ï¼ï¼ä¸ï¼å·æèªæç£ç£åå½¢ç¹å®æ¨è¨ççµæ§æç¥ç¥è­åå½¢ç·æ§åï¼ä»¥åï¼äºï¼å©ç¨åå½¢çµæ§é°è¿åº¦çºåºç¤çå¯¦é«è³è¨æ§åæ¸é²è¡åå½¢ç´æè§£ç¢¼ï¼ä»¥é²è¡ææä¸ç¸éççææ§æ·åãDialogGSR å¨ç¥è­åè­å°è©±çæä¸­å¯¦ç¾äºæåé²çæè½ï¼å¦ OpenDialKG å KOMODIS è³æéæç¤ºã

##### **Natural Language Counterfactual Explanations for Graphs Using Large Language Models**
2410.09295v1 by Flavio Giorgi, Cesare Campagnano, Fabrizio Silvestri, Gabriele Tolomei

Explainable Artificial Intelligence (XAI) has emerged as a critical area of
research to unravel the opaque inner logic of (deep) machine learning models.
Among the various XAI techniques proposed in the literature, counterfactual
explanations stand out as one of the most promising approaches. However, these
``what-if'' explanations are frequently complex and technical, making them
difficult for non-experts to understand and, more broadly, challenging for
humans to interpret. To bridge this gap, in this work, we exploit the power of
open-source Large Language Models to generate natural language explanations
when prompted with valid counterfactual instances produced by state-of-the-art
explainers for graph-based models. Experiments across several graph datasets
and counterfactual explainers show that our approach effectively produces
accurate natural language representations of counterfactual instances, as
demonstrated by key performance metrics.

æè¦ï¼å¯è§£éäººå·¥æºæ§ (XAI) å·²æçºç ç©¶é åä¸­ä¸åéè¦çé åï¼ç¨ä»¥è§£éï¼æ·±åº¦ï¼æ©å¨å­¸ç¿æ¨¡åçå§é¨éè¼¯ãå¨æç»ä¸­æåºçåç¨® XAI æè¡ä¸­ï¼åäºå¯¦è§£éè¢«èªçºæ¯ææåéçæ¹æ³ä¹ä¸ãç¶èï¼éäºãåè¨­æ§ãè§£ééå¸¸è¤éä¸æè¡æ§ï¼éä½¿å¾éå°å®¶é£ä»¥çè§£ï¼æ´å»£æ³å°èªªï¼äººé¡é£ä»¥è§£éãçºäºå½åéåå·®è·ï¼å¨éé å·¥ä½ä¸­ï¼æåå©ç¨éæºå¤§åèªè¨æ¨¡åçåéï¼å¨æç¤ºç±æåé²çåå½¢æ¨¡åè§£éå¨ç¢ççææåäºå¯¦å¯¦ä¾æï¼ç¢çèªç¶èªè¨è§£éãè·¨è¶å¹¾ååå½¢è³æéååäºå¯¦è§£éå¨çå¯¦é©è¡¨æï¼æåçåæ³ææå°ç¢çåäºå¯¦å¯¦ä¾çæºç¢ºèªç¶èªè¨è¡¨ç¤ºï¼éç±ééµæè½ææ¨æè­æã

##### **ReasonPlanner: Enhancing Autonomous Planning in Dynamic Environments with Temporal Knowledge Graphs and LLMs**
2410.09252v1 by Minh Pham Dinh, Munira Syed, Michael G Yankoski, Trenton W. Ford

Planning and performing interactive tasks, such as conducting experiments to
determine the melting point of an unknown substance, is straightforward for
humans but poses significant challenges for autonomous agents. We introduce
ReasonPlanner, a novel generalist agent designed for reflective thinking,
planning, and interactive reasoning. This agent leverages LLMs to plan
hypothetical trajectories by building a World Model based on a Temporal
Knowledge Graph. The agent interacts with the environment using a natural
language actor-critic module, where the actor translates the imagined
trajectory into a sequence of actionable steps, and the critic determines if
replanning is necessary. ReasonPlanner significantly outperforms previous
state-of-the-art prompting-based methods on the ScienceWorld benchmark by more
than 1.8 times, while being more sample-efficient and interpretable. It relies
solely on frozen weights thus requiring no gradient updates. ReasonPlanner can
be deployed and utilized without specialized knowledge of Machine Learning,
making it accessible to a wide range of users.

æè¦ï¼è¦ååå·è¡äºåä»»åï¼ä¾å¦é²è¡å¯¦é©ä»¥ç¢ºå®æªç¥ç©è³ªççé»ï¼å°äººé¡ä¾èªªå¾ç°¡å®ï¼ä½å°èªä¸»ä»£çä¾èªªå»æ§æéå¤§ææ°ãæåå¼å¥äº ReasonPlannerï¼éæ¯ä¸åæ°ç©çéæä»£çï¼å°éç¨æ¼åææ§æèãè¦ååäºåæ¨çãæ­¤ä»£çå©ç¨ LLM ééå»ºç«åºæ¼æåºç¥è­åè¡¨ç World Model ä¾è¦ååè¨­æ§è»è·¡ãä»£çä½¿ç¨èªç¶èªè¨çåä½-è©è«æ¨¡çµèç°å¢äºåï¼å¶ä¸­åä½å°æ³åçè»è·¡è½æçºä¸ç³»åå¯æä½çæ­¥é©ï¼èè©è«åç¢ºå®æ¯å¦éè¦éæ°è¦åãReasonPlanner å¨ ScienceWorld åºæºä¸å¤§å¹åªæ¼ååçæåé²æç¤ºå¼æ¹æ³ï¼åªæ¼ 1.8 åï¼åææ´å·æ¨£æ¬æçåå¯è§£éæ§ãå®åä¾è³´åçµæ¬éï¼å æ­¤ä¸éè¦æ¢¯åº¦æ´æ°ãReasonPlanner å¯ä»¥é¨ç½²åä½¿ç¨ï¼èç¡éæ©å¨å­¸ç¿çå°æ¥­ç¥è­ï¼è®å»£æ³çä½¿ç¨èé½è½ä½¿ç¨ã

##### **Towards Trustworthy Knowledge Graph Reasoning: An Uncertainty Aware Perspective**
2410.08985v1 by Bo Ni, Yu Wang, Lu Cheng, Erik Blasch, Tyler Derr

Recently, Knowledge Graphs (KGs) have been successfully coupled with Large
Language Models (LLMs) to mitigate their hallucinations and enhance their
reasoning capability, such as in KG-based retrieval-augmented frameworks.
However, current KG-LLM frameworks lack rigorous uncertainty estimation,
limiting their reliable deployment in high-stakes applications. Directly
incorporating uncertainty quantification into KG-LLM frameworks presents
challenges due to their complex architectures and the intricate interactions
between the knowledge graph and language model components. To address this gap,
we propose a new trustworthy KG-LLM framework, Uncertainty Aware
Knowledge-Graph Reasoning (UAG), which incorporates uncertainty quantification
into the KG-LLM framework. We design an uncertainty-aware multi-step reasoning
framework that leverages conformal prediction to provide a theoretical
guarantee on the prediction set. To manage the error rate of the multi-step
process, we additionally introduce an error rate control module to adjust the
error rate within the individual components. Extensive experiments show that
our proposed UAG can achieve any pre-defined coverage rate while reducing the
prediction set/interval size by 40% on average over the baselines.

æè¦ï¼æè¿ï¼ç¥è¯å¾è°± (KG) å·²æåä¸å¤§åè¯­è¨æ¨¡å (LLM) ç»åï¼ä»¥åè½»å¶å¹»è§å¹¶å¢å¼ºå¶æ¨çè½åï¼ä¾å¦åºäº KG çæ£ç´¢å¢å¼ºæ¡æ¶ã
ç¶èï¼å½åç KG-LLM æ¡æ¶ç¼ºä¹ä¸¥æ ¼çä¸ç¡®å®æ§ä¼°è®¡ï¼éå¶äºå®ä»¬å¨é«é£é©åºç¨ç¨åºä¸­çå¯é é¨ç½²ãç±äºå¶å¤æçæ¶æä»¥åç¥è¯å¾è°±ä¸è¯­è¨æ¨¡åç»ä»¶ä¹é´çå¤æäº¤äºï¼å°ä¸ç¡®å®æ§éåç´æ¥çº³å¥ KG-LLM æ¡æ¶æåºäºææãä¸ºäºè§£å³è¿ä¸å·®è·ï¼æä»¬æåºäºä¸ç§æ°çå¯ä¿¡èµ KG-LLM æ¡æ¶ï¼å³ä¸ç¡®å®æ§æç¥ç¥è¯å¾æ¨ç (UAG)ï¼å®å°ä¸ç¡®å®æ§éåçº³å¥ KG-LLM æ¡æ¶ãæä»¬è®¾è®¡äºä¸ä¸ªä¸ç¡®å®æ§æç¥å¤æ­¥éª¤æ¨çæ¡æ¶ï¼å®å©ç¨å±å½¢é¢æµä¸ºé¢æµéæä¾çè®ºä¿è¯ãä¸ºäºç®¡çå¤æ­¥éª¤è¿ç¨çéè¯¯çï¼æä»¬å¦å¤å¼å¥äºä¸ä¸ªéè¯¯çæ§å¶æ¨¡åï¼ä»¥è°æ´åä¸ªç»ä»¶åçéè¯¯çãå¤§éå®éªè¡¨æï¼æä»¬æåºç UAG å¯ä»¥è¾¾å°ä»»ä½é¢å®ä¹çè¦ççï¼åæ¶å°é¢æµé/åºé´å¤§å°å¹³ååå° 40%ï¼é«äºåºçº¿ã

##### **When Graph meets Multimodal: Benchmarking on Multimodal Attributed Graphs Learning**
2410.09132v1 by Hao Yan, Chaozhuo Li, Zhigang Yu, Jun Yin, Ruochen Liu, Peiyan Zhang, Weihao Han, Mingzheng Li, Zhengxin Zeng, Hao Sun, Weiwei Deng, Feng Sun, Qi Zhang, Senzhang Wang

Multimodal attributed graphs (MAGs) are prevalent in various real-world
scenarios and generally contain two kinds of knowledge: (a) Attribute knowledge
is mainly supported by the attributes of different modalities contained in
nodes (entities) themselves, such as texts and images. (b) Topology knowledge,
on the other hand, is provided by the complex interactions posed between nodes.
The cornerstone of MAG representation learning lies in the seamless integration
of multimodal attributes and topology. Recent advancements in Pre-trained
Language/Vision models (PLMs/PVMs) and Graph neural networks (GNNs) have
facilitated effective learning on MAGs, garnering increased research interest.
However, the absence of meaningful benchmark datasets and standardized
evaluation procedures for MAG representation learning has impeded progress in
this field. In this paper, we propose Multimodal Attribute Graph Benchmark
(MAGB)}, a comprehensive and diverse collection of challenging benchmark
datasets for MAGs. The MAGB datasets are notably large in scale and encompass a
wide range of domains, spanning from e-commerce networks to social networks. In
addition to the brand-new datasets, we conduct extensive benchmark experiments
over MAGB with various learning paradigms, ranging from GNN-based and PLM-based
methods, to explore the necessity and feasibility of integrating multimodal
attributes and graph topology. In a nutshell, we provide an overview of the MAG
datasets, standardized evaluation procedures, and present baseline experiments.
The entire MAGB project is publicly accessible at
https://github.com/sktsherlock/ATG.

æè¦ï¼<paragraph>å¤æ¨¡æå±¬æ§å (MAG) å¨åç¨®çå¯¦ä¸ççå ´æ¯ä¸­å¾å¸¸è¦ï¼éå¸¸åå«å©ç¨®é¡åçç¥è­ï¼(a) å±¬æ§ç¥è­ä¸»è¦ç±ç¯é»ï¼å¯¦é«ï¼æ¬èº«æåå«çä¸åæ¨¡æçå±¬æ§æä¾æ¯æ´ï¼ä¾å¦æå­åååã(b) å¦ä¸æ¹é¢ï¼ææ²ç¥è­åæ¯ç±ç¯é»ä¹éæåºçè¤éäºåæä¾ãMAG è¡¨ç¤ºå¼å­¸ç¿çåºç³å¨æ¼å¤æ¨¡æå±¬æ§åææ²çç¡ç¸«æ´åãé è¨ç·´èªè¨/è¦è¦ºæ¨¡å (PLM/PVM) ååå½¢ç¥ç¶ç¶²è·¯ (GNN) çææ°é²å±ä¿é²äºå° MAG çææå­¸ç¿ï¼å¼èµ·äºè¶ä¾è¶å¤çç ç©¶èè¶£ãç¶èï¼ç¼ºä¹ææç¾©çåºæºè³æéåæ¨æºåç MAG è¡¨ç¤ºå¼å­¸ç¿è©ä¼°ç¨åºé»ç¤äºè©²é åçé²å±ãå¨æ¬æä¸­ï¼æåæåºäºå¤æ¨¡æå±¬æ§ååºæº (MAGB)ï¼éæ¯éå° MAG çå¨é¢ä¸å¤æ¨£åçææ°æ§åºæºè³æééåãMAGB è³æéçè¦æ¨¡é¡¯èé¾å¤§ï¼æ¶µèäºå¾é»å­ååç¶²è·¯å°ç¤¾äº¤ç¶²è·¯çå»£æ³é åãé¤äºå¨æ°çè³æéå¤ï¼æåéä½¿ç¨åç¨®å­¸ç¿ç¯ä¾å° MAGB é²è¡äºå»£æ³çåºæºå¯¦é©ï¼å¾åºæ¼ GNN ååºæ¼ PLM çæ¹æ³ï¼ä»¥æ¢ç´¢æ´åå¤æ¨¡æå±¬æ§ååå½¢ææ²çå¿è¦æ§åå¯è¡æ§ãç°¡èè¨ä¹ï¼æåæä¾äº MAG è³æéãæ¨æºåè©ä¼°ç¨åºçæ¦è¿°ï¼ä¸¦æåºäºåºæºå¯¦é©ãæ´å MAGB å°æ¡å¯å¨ https://github.com/sktsherlock/ATG å¬éåå¾ã</paragraph>

##### **GIVE: Structured Reasoning with Knowledge Graph Inspired Veracity Extrapolation**
2410.08475v1 by Jiashu He, Mingyu Derek Ma, Jinxuan Fan, Dan Roth, Wei Wang, Alejandro Ribeiro

Existing retrieval-based reasoning approaches for large language models
(LLMs) heavily rely on the density and quality of the non-parametric knowledge
source to provide domain knowledge and explicit reasoning chain. However,
inclusive knowledge sources are expensive and sometimes infeasible to build for
scientific or corner domains. To tackle the challenges, we introduce Graph
Inspired Veracity Extrapolation (GIVE), a novel reasoning framework that
integrates the parametric and non-parametric memories to enhance both knowledge
retrieval and faithful reasoning processes on very sparse knowledge graphs. By
leveraging the external structured knowledge to inspire LLM to model the
interconnections among relevant concepts, our method facilitates a more logical
and step-wise reasoning approach akin to experts' problem-solving, rather than
gold answer retrieval. Specifically, the framework prompts LLMs to decompose
the query into crucial concepts and attributes, construct entity groups with
relevant entities, and build an augmented reasoning chain by probing potential
relationships among node pairs across these entity groups. Our method
incorporates both factual and extrapolated linkages to enable comprehensive
understanding and response generation. Extensive experiments on
reasoning-intense benchmarks on biomedical and commonsense QA demonstrate the
effectiveness of our proposed method. Specifically, GIVE enables GPT3.5-turbo
to outperform advanced models like GPT4 without any additional training cost,
thereby underscoring the efficacy of integrating structured information and
internal reasoning ability of LLMs for tackling specialized tasks with limited
external resources.

æè¦ï¼ç¾æçåºæ¼æª¢ç´¢çæ¨çæ¹æ³å°æ¼å¤§åèªè¨æ¨¡å (LLM) å´éä¾è³´éåæ¸ç¥è­ä¾æºçå¯åº¦ååè³ªï¼ä»¥æä¾é åç¥è­åæç¢ºçæ¨çéãç¶èï¼åå®¹æ§çç¥è­ä¾æºå¾æè²´ï¼ææå°æ¼ç§å­¸æè§è½é åä¾èªªï¼å»ºç«èµ·ä¾ä¹ä¸å¯è¡ãçºäºæå°éäºææ°ï¼æåå¼å¥äºåå½¢åç¼çå¯¦æ¨æ· (GIVE)ï¼éæ¯ä¸åæ°ç©çæ¨çæ¶æ§ï¼å®æ´åäºåæ¸åéåæ¸è¨æ¶é«ï¼ä»¥å¢å¼·å¨éå¸¸ç¨ççç¥è­åè­ä¸é²è¡ç¥è­æª¢ç´¢åå¿ å¯¦æ¨çéç¨ãééå©ç¨å¤é¨çµæ§åç¥è­ä¾æ¿åµ LLM æ¨¡æ¬ç¸éæ¦å¿µä¹éçç¸äºéè¯ï¼æåçæè¡ä¿é²äºä¸ç¨®æ´åä¹éè¼¯ä¸å¾ªåºæ¼¸é²çæ¨çæ¹æ³ï¼é¡ä¼¼æ¼å°å®¶çåé¡è§£æ±ºï¼èä¸æ¯é»éç­æ¡æª¢ç´¢ãå·é«ä¾èªªï¼è©²æ¶æ§æç¤º LLM å°æ¥è©¢åè§£çºééµæ¦å¿µåå±¬æ§ï¼ä½¿ç¨ç¸éå¯¦é«å»ºæ§å¯¦é«ç¾¤çµï¼ä¸¦ééæ¢æ¥éäºå¯¦é«ç¾¤çµä¸­ç¯é»å°ä¹éçæ½å¨éä¿ä¾å»ºç«å¢å¼·çæ¨çéãæåçæè¡çµåäºäºå¯¦åå¤æ¨éè¯ï¼ä»¥å¯¦ç¾å¨é¢ççè§£ååæç¢çãå¨çç©é«å­¸åå¸¸è­åç­ä¸å°æ¨çå¯éåºæºé²è¡çå»£æ³å¯¦é©è­æäºæåæåºçæ¹æ³çæææ§ãå·é«ä¾èªªï¼GIVE ä½¿ GPT3.5-turbo è½å¤ å¨æ²æä»»ä½é¡å¤è¨ç·´ææ¬çææ³ä¸åªæ¼ GPT4 ç­é²éæ¨¡åï¼å¾èå¼·èª¿äºæ´åçµæ§åè³è¨å LLM å§é¨æ¨çè½åå°æ¼èçå·ææéå¤é¨è³æºçå°æ¥­ä»»åçæè½ã

##### **Privately Learning from Graphs with Applications in Fine-tuning Large Language Models**
2410.08299v1 by Haoteng Yin, Rongzhe Wei, Eli Chien, Pan Li

Graphs offer unique insights into relationships and interactions between
entities, complementing data modalities like text, images, and videos. By
incorporating relational information from graph data, AI models can extend
their capabilities beyond traditional tasks. However, relational data in
sensitive domains such as finance and healthcare often contain private
information, making privacy preservation crucial. Existing privacy-preserving
methods, such as DP-SGD, which rely on gradient decoupling assumptions, are not
well-suited for relational learning due to the inherent dependencies between
coupled training samples. To address this challenge, we propose a
privacy-preserving relational learning pipeline that decouples dependencies in
sampled relations during training, ensuring differential privacy through a
tailored application of DP-SGD. We apply this method to fine-tune large
language models (LLMs) on sensitive graph data, and tackle the associated
computational complexities. Our approach is evaluated on LLMs of varying sizes
(e.g., BERT, Llama2) using real-world relational data from four text-attributed
graphs. The results demonstrate significant improvements in relational learning
tasks, all while maintaining robust privacy guarantees during training.
Additionally, we explore the trade-offs between privacy, utility, and
computational efficiency, offering insights into the practical deployment of
our approach. Code is available at https://github.com/Graph-COM/PvGaLM.

æè¦ï¼åè¡¨æä¾éæ¼å¯¦é«ä¹ééä¿åäºåçç¨ç¹è¦è§£ï¼è£åäºææ¬ãå½±ååå½±çç­è³ææ¨¡å¼ãééç´å¥ä¾èªåè¡¨è³æçéä¿è³è¨ï¼AI æ¨¡åå¯ä»¥å°å¶åè½å»¶ä¼¸å°å³çµ±ä»»åä¹å¤ãç¶èï¼ææé åï¼ä¾å¦éèåé«çä¿å¥ï¼ä¸­çéä¿è³æéå¸¸åå«ç§äººè³è¨ï¼å æ­¤é±ç§ä¿è­·è³ééè¦ãç¾æçé±ç§ä¿è­·æ¹æ³ï¼ä¾å¦ DP-SGDï¼ï¼ä¾è³´æ¼æ¢¯åº¦è§£è¦åè¨­ï¼ç±æ¼è¦åè¨ç·´æ¨£æ¬ä¹éçå§å¨ä¾è³´æ§ï¼ä¸¦ä¸é©åéä¿å­¸ç¿ãçºäºæå°éåææ°ï¼æåæåºä¸åé±ç§ä¿è­·éä¿å­¸ç¿ç®¡éï¼å¨è¨ç·´æéè§£è¦åæ¨£éä¿ä¸­çä¾è³´æ§ï¼ééå®¢è£½åæç¨ DP-SGD ä¾ç¢ºä¿å·®ç°é±ç§ãæåå°æ­¤æ¹æ³æç¨æ¼ææåè¡¨è³æä¸å¾®èª¿å¤§åèªè¨æ¨¡å (LLM)ï¼ä¸¦è§£æ±ºç¸éçè¨ç®è¤éæ§ãæåçæ¹æ³å¨ä¸åå¤§å°ç LLMï¼ä¾å¦ BERTãLlama2ï¼ä¸é²è¡è©ä¼°ï¼ä½¿ç¨ä¾èªååæå­å±¬æ§åè¡¨ççå¯¦ä¸çéä¿è³æãçµæè­æéä¿å­¸ç¿ä»»åæé¡¯èçé²æ­¥ï¼åæå¨è¨ç·´æéç¶­æå¼·å¤§çé±ç§ä¿è­ãæ­¤å¤ï¼æåæ¢è¨é±ç§ãæç¨åè¨ç®æçä¹éçæ¬è¡¡ï¼æä¾å°æåæ¹æ³å¯¦éé¨ç½²çè¦è§£ãç¨å¼ç¢¼å¯å¨ https://github.com/Graph-COM/PvGaLM åå¾ã

##### **Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering**
2410.08085v1 by Yuan Sui, Bryan Hooi

Recent works integrating Knowledge Graphs (KGs) have led to promising
improvements in enhancing reasoning accuracy of Large Language Models (LLMs).
However, current benchmarks mainly focus on closed tasks, leaving a gap in the
assessment of more complex, real-world scenarios. This gap has also obscured
the evaluation of KGs' potential to mitigate the problem of hallucination in
LLMs. To fill the gap, we introduce OKGQA, a new benchmark specifically
designed to assess LLMs enhanced with KGs under open-ended, real-world question
answering scenarios. OKGQA is designed to closely reflect the complexities of
practical applications using questions from different types, and incorporates
specific metrics to measure both the reduction in hallucinations and the
enhancement in reasoning capabilities. To consider the scenario in which KGs
may have varying levels of mistakes, we further propose another experiment
setting OKGQA-P to assess model performance when the semantics and structure of
KGs are deliberately perturbed and contaminated. OKGQA aims to (1) explore
whether KGs can make LLMs more trustworthy in an open-ended setting, and (2)
conduct a comparative analysis to shed light on methods and future directions
for leveraging KGs to reduce LLMs' hallucination. We believe that this study
can facilitate a more complete performance comparison and encourage continuous
improvement in integrating KGs with LLMs.

æè¦ï¼è¿æçç¥è¯å¾è°± (KG) æ´åç ç©¶ï¼å·²æåå¤§åè¯­è¨æ¨¡å (LLM) æ¨çåç¡®åº¦çè¡¨ç°ã
ç¶èï¼ç°æçåºåæµè¯ä¸»è¦çéäºå°é­å¼ä»»å¡ï¼å¨è¯ä¼°æ´å¤æãæ´å®éçåºæ¯æ¶å­å¨ç¼ºå£ãæ­¤ç¼ºå£ä¹æ¨¡ç³äºç¥è¯å¾è°±å¨åè½» LLM å¹»è§é®é¢ä¸çæ½åè¯ä¼°ãä¸ºäºå¡«è¡¥æ­¤ç¼ºå£ï¼æä»¬å¼å¥äº OKGQAï¼è¿æ¯ä¸ä¸ªä¸é¨è®¾è®¡ç¨æ¥è¯ä¼°å¨å¼æ¾å¼ãå®éé®ç­åºæ¯ä¸­ï¼å¢å¼ºäºç¥è¯å¾è°±ç LLM çæ°åºåæµè¯ãOKGQA æ¨å¨ç´§å¯åæ å®éåºç¨ä¸­çå¤ææ§ï¼ä½¿ç¨ä¸åç±»åçé¢ç®ï¼å¹¶çº³å¥ç¹å®ææ æ¥è¡¡éå¹»è§çåå°åæ¨çè½åçå¢å¼ºãä¸ºäºèèç¥è¯å¾è°±å¯è½å­å¨ä¸åç¨åº¦éè¯¯çåºæ¯ï¼æä»¬è¿ä¸æ­¥æåºäºå¦ä¸ä¸ªå®éªè®¾ç½® OKGQA-Pï¼ä»¥è¯ä¼°å½ç¥è¯å¾è°±çè¯­ä¹åç»æè¢«æææ°å¨åæ±¡ææ¶çæ¨¡åæ§è½ãOKGQA æ¨å¨ (1) æ¢ç´¢ç¥è¯å¾è°±æ¯å¦è½ä½¿ LLM å¨å¼æ¾å¼è®¾ç½®ä¸­æ´å¼å¾ä¿¡èµï¼ä»¥å (2) è¿è¡æ¯è¾åæï¼ä»¥éæå©ç¨ç¥è¯å¾è°±æ¥åå° LLM å¹»è§çæ¹æ³åæªæ¥æ¹åãæä»¬ç¸ä¿¡è¿é¡¹ç ç©¶å¯ä»¥ä¿è¿æ´å®æ´çæ§è½æ¯è¾ï¼å¹¶é¼å±æç»­æ¹è¿ç¥è¯å¾è°±ä¸ LLM çæ´åã

##### **Disease Entity Recognition and Normalization is Improved with Large Language Model Derived Synthetic Normalized Mentions**
2410.07951v1 by Kuleen Sasse, Shinjitha Vadlakonda, Richard E. Kennedy, John D. Osborne

Background: Machine learning methods for clinical named entity recognition
and entity normalization systems can utilize both labeled corpora and Knowledge
Graphs (KGs) for learning. However, infrequently occurring concepts may have
few mentions in training corpora and lack detailed descriptions or synonyms,
even in large KGs. For Disease Entity Recognition (DER) and Disease Entity
Normalization (DEN), this can result in fewer high quality training examples
relative to the number of known diseases. Large Language Model (LLM) generation
of synthetic training examples could improve performance in these information
extraction tasks.
  Methods: We fine-tuned a LLaMa-2 13B Chat LLM to generate a synthetic corpus
containing normalized mentions of concepts from the Unified Medical Language
System (UMLS) Disease Semantic Group. We measured overall and Out of
Distribution (OOD) performance for DER and DEN, with and without synthetic data
augmentation. We evaluated performance on 3 different disease corpora using 4
different data augmentation strategies, assessed using BioBERT for DER and
SapBERT and KrissBERT for DEN.
  Results: Our synthetic data yielded a substantial improvement for DEN, in all
3 training corpora the top 1 accuracy of both SapBERT and KrissBERT improved by
3-9 points in overall performance and by 20-55 points in OOD data. A small
improvement (1-2 points) was also seen for DER in overall performance, but only
one dataset showed OOD improvement.
  Conclusion: LLM generation of normalized disease mentions can improve DEN
relative to normalization approaches that do not utilize LLMs to augment data
with synthetic mentions. Ablation studies indicate that performance gains for
DEN were only partially attributable to improvements in OOD performance. The
same approach has only a limited ability to improve DER. We make our software
and dataset publicly available.

æè¦ï¼<paragraph>èæ¯ï¼è¨åºå½åå¯¦é«è­å¥çæ©å¨å­¸ç¿æ¹æ³åå¯¦é«æ­£è¦åç³»çµ±å¯ä»¥å©ç¨æ¨è¨èªæåº«åç¥è­åè­ (KG) ä¾å­¸ç¿ãç¶èï¼å¨è¨ç·´èªæåº«ä¸­å¾å°åºç¾çæ¦å¿µå¯è½åªæå°æ¸æåï¼å³ä½¿å¨å¤§åç¥è­åè­ä¸­ä¹ç¼ºä¹è©³ç´°çæè¿°æåç¾©è©ãå°æ¼ç¾çå¯¦é«è­å¥ (DER) åç¾çå¯¦é«æ­£è¦å (DEN)ï¼ç¸å°æ¼å·²ç¥ç¾ççæ¸éï¼éå¯è½æå°è´è¼å°çé«åè³ªè¨ç·´ç¯ä¾ãå¤§åèªè¨æ¨¡å (LLM) çæçåæè¨ç·´ç¯ä¾å¯ä»¥æåéäºè³è¨æ·åä»»åçæè½ã
æ¹æ³ï¼æåå¾®èª¿äºä¸å LLaMa-2 13B èå¤© LLMï¼ä»¥ç¢çä¸ååæèªæåº«ï¼å¶ä¸­åå«ä¾èªçµ±ä¸é«å­¸èªè¨ç³»çµ± (UMLS) ç¾çèªç¾©ç¾¤çæ¨æºåæ¦å¿µæåãæåè¡¡éäº DER å DEN çæ´é«ååå¸å¤ (OOD) æè½ï¼æåæ²æåæè³ææ´åãæåä½¿ç¨ 4 ç¨®ä¸åçè³ææ´åç­ç¥è©ä¼°äº 3 åä¸åç¾çèªæåº«çæè½ï¼ä½¿ç¨ BioBERT è©ä¼° DERï¼ä½¿ç¨ SapBERT å KrissBERT è©ä¼° DENã
çµæï¼æåçåæè³æå° DEN ç¢çäºé¡¯èçæ¹åï¼å¨ææ 3 åè¨ç·´èªæåº«ä¸­ï¼SapBERT å KrissBERT çå 1 åæºç¢ºçå¨æ´é«æè½ä¸æé«äº 3-9 åç¾åé»ï¼å¨ OOD è³æä¸­æé«äº 20-55 åç¾åé»ãå¨ DER çæ´é«æè½ä¸ä¹çå°äºå¾®å°çæ¹åï¼1-2 åç¾åé»ï¼ï¼ä½åªæä¸çµè³æé¡¯ç¤ºåº OOD æ¹åã
çµè«ï¼èä¸å©ç¨ LLM æ´åè³æä»¥åææåçæ­£è¦åæ¹æ³ç¸æ¯ï¼LLM çæçæ¨æºåç¾çæåå¯ä»¥æ¹å DENãæ¶èç ç©¶è¡¨æï¼DEN çæè½æååé¨åæ­¸å æ¼ OOD æè½çæ¹åãç¸åçæ¹æ³å°æ¼æ¹å DER çè½åæéãæåå¬éæåçè»é«åè³æéã</paragraph>

##### **Benchmarking Agentic Workflow Generation**
2410.07869v1 by Shuofei Qiao, Runnan Fang, Zhisong Qiu, Xiaobin Wang, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen

Large Language Models (LLMs), with their exceptional ability to handle a wide
range of tasks, have driven significant advancements in tackling reasoning and
planning tasks, wherein decomposing complex problems into executable workflows
is a crucial step in this process. Existing workflow evaluation frameworks
either focus solely on holistic performance or suffer from limitations such as
restricted scenario coverage, simplistic workflow structures, and lax
evaluation standards. To this end, we introduce WorFBench, a unified workflow
generation benchmark with multi-faceted scenarios and intricate graph workflow
structures. Additionally, we present WorFEval, a systemic evaluation protocol
utilizing subsequence and subgraph matching algorithms to accurately quantify
the LLM agent's workflow generation capabilities. Through comprehensive
evaluations across different types of LLMs, we discover distinct gaps between
the sequence planning capabilities and graph planning capabilities of LLM
agents, with even GPT-4 exhibiting a gap of around 15%. We also train two
open-source models and evaluate their generalization abilities on held-out
tasks. Furthermore, we observe that the generated workflows can enhance
downstream tasks, enabling them to achieve superior performance with less time
during inference. Code and dataset will be available at
https://github.com/zjunlp/WorFBench.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ææèçåç¨®ä»»åçéå¡è½åï¼æ¨åäºè§£æ±ºæ¨çåè¦åä»»åçé¡¯èé²å±ï¼å¶ä¸­å°è¤éåé¡åè§£çºå¯å·è¡å·¥ä½æµç¨æ¯æ­¤éç¨ä¸­è³ééè¦çä¸æ­¥ãç¾æçå·¥ä½æµç¨è©ä¼°æ¡æ¶åªå°æ³¨æ¼æ´é«æè½ï¼æåå°æå¢æ¶µèç¯ååéãå·¥ä½æµç¨çµæ§ç°¡ååè©ä¼°æ¨æºå¯¬é¬ç­éå¶ãçºæ­¤ï¼æåå¼å¥äº WorFBenchï¼ä¸åçµ±ä¸çå·¥ä½æµç¨çæåºæºï¼å·æå¤æ¹é¢çå ´æ¯åè¤éçåå½¢å·¥ä½æµç¨çµæ§ãæ­¤å¤ï¼æåæåºäº WorFEvalï¼ä¸åå©ç¨å­åºååå­åå¹éæ¼ç®æ³ä¾æºç¢ºéå LLM ä»£çå·¥ä½æµç¨çæè½åçç³»çµ±æ§è©ä¼°åå®ãééå°ä¸åé¡å LLM çå¨é¢è©ä¼°ï¼æåç¼ç¾ LLM ä»£ççåºåè¦åè½åååå½¢è¦åè½åä¹éå­å¨æé¡¯çå·®è·ï¼å³ä½¿æ¯ GPT-4 ä¹è¡¨ç¾åºç´ 15% çå·®è·ãæåéè¨ç·´äºå©åéæºæ¨¡åï¼ä¸¦è©ä¼°äºå®åå¨ä¿çä»»åä¸çæ³åè½åãæ­¤å¤ï¼æåè§å¯å°çæççå·¥ä½æµç¨å¯ä»¥å¢å¼·ä¸æ¸¸ä»»åï¼è®å®åå¨æ¨çæéä»¥æ´å°çæéç²å¾æ´å¥½çæè½ãç¨å¼ç¢¼åè³æéå°å¨ https://github.com/zjunlp/WorFBench ä¸æä¾ã

##### **KRAG Framework for Enhancing LLMs in the Legal Domain**
2410.07551v1 by Nguyen Ha Thanh, Ken Satoh

This paper introduces Knowledge Representation Augmented Generation (KRAG), a
novel framework designed to enhance the capabilities of Large Language Models
(LLMs) within domain-specific applications. KRAG points to the strategic
inclusion of critical knowledge entities and relationships that are typically
absent in standard data sets and which LLMs do not inherently learn. In the
context of legal applications, we present Soft PROLEG, an implementation model
under KRAG, which uses inference graphs to aid LLMs in delivering structured
legal reasoning, argumentation, and explanations tailored to user inquiries.
The integration of KRAG, either as a standalone framework or in tandem with
retrieval augmented generation (RAG), markedly improves the ability of language
models to navigate and solve the intricate challenges posed by legal texts and
terminologies. This paper details KRAG's methodology, its implementation
through Soft PROLEG, and potential broader applications, underscoring its
significant role in advancing natural language understanding and processing in
specialized knowledge domains.

æè¦ï¼æ¬æä»ç´¹ç¥è­è¡¨å¾µå¢å¼·çæ (KRAG)ï¼ä¸åæ°ç©çæ¶æ§ï¼æ¨å¨å¢å¼·å¤§åèªè¨æ¨¡å (LLM) å¨ç¹å®é åæç¨ä¸­çè½åãKRAG æåºç­ç¥æ§å°ç´å¥ééµç¥è­å¯¦é«åéä¿ï¼éäºå¯¦é«åéä¿éå¸¸ä¸å­å¨æ¼æ¨æºè³æéä¸­ï¼è LLM ä¹ç¡æ³åºæå°å­¸ç¿ãå¨æ³å¾æç¨æ¹é¢ï¼æåæåº Soft PROLEGï¼éæ¯ä¸åå¨ KRAG ä¸çå¯¦ä½æ¨¡åï¼å®ä½¿ç¨æ¨çåä¾åå© LLM æä¾çµæ§åçæ³å¾æ¨çãè«è­åè§£éï¼ä»¥æ»¿è¶³ä½¿ç¨èçè©¢åãæ´å KRAGï¼ç¡è«æ¯ä½çºç¨ç«æ¶æ§æèæª¢ç´¢å¢å¼·çæ (RAG) çµåä½¿ç¨ï¼é½è½é¡¯èæåèªè¨æ¨¡åå°èªåè§£æ±ºæ³å¾ææ¬åè¡èªæå¸¶ä¾çè¤éææ°çè½åãæ¬æè©³è¿° KRAG çæ¹æ³è«ãéé Soft PROLEG çå¯¦ä½ï¼ä»¥åæ½å¨çæ´å»£æ³æç¨ï¼å¼·èª¿å¶å¨æ¨é²å°æ¥­ç¥è­é åçèªç¶èªè¨çè§£åèçä¸­æ®æ¼çéè¦è§è²ã

##### **MKGL: Mastery of a Three-Word Language**
2410.07526v1 by Lingbing Guo, Zhongpu Bo, Zhuo Chen, Yichi Zhang, Jiaoyan Chen, Yarong Lan, Mengshu Sun, Zhiqiang Zhang, Yangyifei Luo, Qian Li, Qiang Zhang, Wen Zhang, Huajun Chen

Large language models (LLMs) have significantly advanced performance across a
spectrum of natural language processing (NLP) tasks. Yet, their application to
knowledge graphs (KGs), which describe facts in the form of triplets and allow
minimal hallucinations, remains an underexplored frontier. In this paper, we
investigate the integration of LLMs with KGs by introducing a specialized KG
Language (KGL), where a sentence precisely consists of an entity noun, a
relation verb, and ends with another entity noun. Despite KGL's unfamiliar
vocabulary to the LLM, we facilitate its learning through a tailored dictionary
and illustrative sentences, and enhance context understanding via real-time KG
context retrieval and KGL token embedding augmentation. Our results reveal that
LLMs can achieve fluency in KGL, drastically reducing errors compared to
conventional KG embedding methods on KG completion. Furthermore, our enhanced
LLM shows exceptional competence in generating accurate three-word sentences
from an initial entity and interpreting new unseen terms out of KGs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å¤§å¹æååç¨®èªç¶èªè¨èç (NLP) ä»»åçæè½ãç¶èï¼å®åå¨ç¥è­åè­ (KG) çæç¨ä¸ä»æå¾éç¼ï¼ç¥è­åè­ä»¥ä¸åçµå½¢å¼æè¿°äºå¯¦ï¼ä¸¦åè¨±æå°çå¹»è¦ºãå¨æ¬æä¸­ï¼æåééå¼å¥ä¸ç¨®å°æ¥­ç KG èªè¨ (KGL) ä¾æ¢è¨ LLM è KG çæ´åï¼å¶ä¸­ä¸åå¥å­ç²¾ç¢ºå°åå«ä¸åå¯¦é«åè©ãä¸åéä¿åè©ï¼ä¸¦ä»¥å¦ä¸åå¯¦é«åè©çµå°¾ãåç®¡ LLM å° KGL çè©å½ä¸çæï¼ä½æåéééèº«æé çå­å¸åèªªææ§å¥å­ä¾ä¿é²å®çå­¸ç¿ï¼ä¸¦ééå³æ KG èæ¯æ·åå KGL ä»£å¹£åµå¥å¼·åä¾æåèæ¯çè§£ãæåççµæé¡¯ç¤ºï¼LLM è½å¤ æµæ¢ä½¿ç¨ KGLï¼å¤§å¹æ¸å°é¯èª¤ï¼åªæ¼ KG å®æä¸­å³çµ±ç KG åµå¥æ¹æ³ãæ­¤å¤ï¼æåå¢å¼·ç LLM å¨å¾åå§å¯¦é«çææºç¢ºçä¸å­è©å¥å­ï¼ä»¥åå¾ KG è§£éæ°çæªè¦è¡èªæ¹é¢å±ç¾åºåè¶çè½åã

##### **InstructG2I: Synthesizing Images from Multimodal Attributed Graphs**
2410.07157v1 by Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han

In this paper, we approach an overlooked yet critical task Graph2Image:
generating images from multimodal attributed graphs (MMAGs). This task poses
significant challenges due to the explosion in graph size, dependencies among
graph entities, and the need for controllability in graph conditions. To
address these challenges, we propose a graph context-conditioned diffusion
model called InstructG2I. InstructG2I first exploits the graph structure and
multimodal information to conduct informative neighbor sampling by combining
personalized page rank and re-ranking based on vision-language features. Then,
a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary
set of graph prompts to guide the denoising process of diffusion. Finally, we
propose graph classifier-free guidance, enabling controllable generation by
varying the strength of graph guidance and multiple connected edges to a node.
Extensive experiments conducted on three datasets from different domains
demonstrate the effectiveness and controllability of our approach. The code is
available at https://github.com/PeterGriffinJin/InstructG2I.

æè¦ï¼å¨æ¬æä¸­ï¼æä»¬æ¢è®¨ä¸é¡¹è¢«å¿½è§ä½è³å³éè¦çä»»å¡ Graph2Imageï¼
ä»å¤æ¨¡æå±æ§å¾ (MMAG) çæå¾åãç±äºå¾å¤§å°æ¿å¢ãå¾å®ä½ä¹é´çä¾èµå³ç³»ä»¥åå¯¹å¾æ¡ä»¶çå¯æ§æ§éæ±ï¼æ­¤ä»»å¡å¸¦æ¥äºéå¤§ææãä¸ºäºåºå¯¹è¿äºææï¼æä»¬æåºäºä¸ç§ç§°ä¸º InstructG2I çå¾ä¸ä¸ææ¡ä»¶æ©æ£æ¨¡åãInstructG2I é¦åå©ç¨å¾ç»æåå¤æ¨¡æä¿¡æ¯ï¼éè¿ç»åä¸ªæ§åé¡µé¢æåååºäºè§è§è¯­è¨ç¹å¾çéæ°æåæ¥æ§è¡ä¿¡æ¯ä¸°å¯çé»å±éæ ·ãç¶åï¼Graph-QFormer ç¼ç å¨å°å¾èç¹èªéåºå°ç¼ç ä¸ºä¸ç»è¾å©å¾æç¤ºï¼ä»¥æå¯¼æ©æ£çå»åªè¿ç¨ãæåï¼æä»¬æåºäºæ å¾åç±»å¨æå¯¼ï¼éè¿æ¹åå¾æå¯¼çå¼ºåº¦åä¸èç¹çå¤ä¸ªè¿æ¥è¾¹æ¥å®ç°å¯æ§çæãå¨æ¥èªä¸åé¢åçä¸ç»æ°æ®éä¸è¿è¡çå¹¿æ³å®éªè¯æäºæä»¬æ¹æ³çæææ§åå¯æ§æ§ãä»£ç å¯å¨ https://github.com/PeterGriffinJin/InstructG2I è·å¾ã

##### **CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages**
2410.06944v1 by Pretam Ray, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal

Neural dependency parsing has achieved remarkable performance for low
resource morphologically rich languages. It has also been well-studied that
morphologically rich languages exhibit relatively free word order. This prompts
a fundamental investigation: Is there a way to enhance dependency parsing
performance, making the model robust to word order variations utilizing the
relatively free word order nature of morphologically rich languages? In this
work, we examine the robustness of graph-based parsing architectures on 7
relatively free word order languages. We focus on scrutinizing essential
modifications such as data augmentation and the removal of position encoding
required to adapt these architectures accordingly. To this end, we propose a
contrastive self-supervised learning method to make the model robust to word
order variations. Furthermore, our proposed modification demonstrates a
substantial average gain of 3.03/2.95 points in 7 relatively free word order
languages, as measured by the UAS/LAS Score metric when compared to the best
performing baseline.

æè¦ï¼ç¥ç¶ä¾è³´è§£æå°æ¼è³æºè¼å°çå½¢æè±å¯èªè¨å·²éå°é¡¯èçæè½ãå½¢æè±å¯èªè¨å±ç¾ç¸å°èªç±çèªåºï¼éé»ä¹å·²ç²å¾æ·±å¥æ¢è¨ãéå¼ç¼äºä¸é åºç¤èª¿æ¥ï¼æ¯å¦ææ¹æ³å¯ä»¥æåä¾è³´è§£ææè½ï¼è®æ¨¡åè½éééç¨å½¢æè±å¯èªè¨ç¸å°èªç±çèªåºç¹è³ªï¼å°èªåºè®åå·æç©©å¥æ§ï¼å¨éé å·¥ä½ä¸­ï¼æåæª¢è¦äº 7 ç¨®ç¸å°èªç±èªåºèªè¨ä¸­åºæ¼åè¡¨çè§£ææ¶æ§çç©©å¥æ§ãæåå°æ³¨æ¼å¯©è¦å¿è¦çä¿®æ¹ï¼ä¾å¦è³ææ´ååç§»é¤ä½ç½®ç·¨ç¢¼ï¼ä»¥é©ç¶å°èª¿æ´éäºæ¶æ§ãçºæ­¤ï¼æåæåºå°æ¯èªæç£ç£å­¸ç¿æ¹æ³ï¼è®æ¨¡åå°èªåºè®åå·æç©©å¥æ§ãæ­¤å¤ï¼æåæåºçä¿®æ¹å¨ 7 ç¨®ç¸å°èªç±èªåºèªè¨ä¸­å±ç¾äº 3.03/2.95 é»çé¡¯èå¹³åå¢çï¼éæ¯æ ¹æ UAS/LAS åæ¸ææ¨ï¼èæè½æä½³çåºæºç·é²è¡æ¯è¼å¾å¾åºççµæã

##### **Tree of Problems: Improving structured problem solving with compositionality**
2410.06634v1 by Armel Zebaze, BenoÃ®t Sagot, Rachel Bawden

Large Language Models (LLMs) have demonstrated remarkable performance across
multiple tasks through in-context learning. For complex reasoning tasks that
require step-by-step thinking, Chain-of-Thought (CoT) prompting has given
impressive results, especially when combined with self-consistency.
Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree
of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing
the complex problem into paths of subproblems. In this paper, we propose Tree
of Problems (ToP), a simpler version of ToT, which we hypothesise can work
better for complex tasks that can be divided into identical subtasks. Our
empirical results show that our approach outperforms ToT and GoT, and in
addition performs better than CoT on complex reasoning tasks. All code for this
paper is publicly available here:
https://github.com/ArmelRandy/tree-of-problems.

æè¦ï¼å¤§åè¯­è¨æ¨¡å (LLM) å·²éè¿æå¢å­¦ä¹ å¨å¤é¡¹ä»»å¡ä¸­å±ç¤ºåºéå¡çæ§è½ãå¯¹äºéè¦å¾ªåºæ¸è¿æèçå¤ææ¨çä»»å¡ï¼æç»´é¾ (CoT) æç¤ºå·²åå¾ä»¤äººå°è±¡æ·±å»çç»æï¼å°¤å¶æ¯å¨ä¸èªæ´½æ§ç¸ç»åæ¶ãå°½ç®¡å¦æ­¤ï¼LLM ä»ç¶é¾ä»¥è§£å³æäºä»»å¡ãæç»´æ  (ToT) åæç»´å¾ (GoT) ä½ä¸ºæ¿ä»£æ¹æ¡åºç°ï¼å°å¤æé®é¢ååä¸ºå­é®é¢çè·¯å¾ãå¨æ¬æä¸­ï¼æä»¬æåºäºæç»´æ  (ToP)ï¼å®æ¯ ToT çä¸ä¸ªæ´ç®åççæ¬ï¼æä»¬åè®¾å®å¯ä»¥æ´å¥½å°éç¨äºå¯ä»¥ååä¸ºç¸åå­ä»»å¡çå¤æä»»å¡ãæä»¬çå®è¯ç»æè¡¨æï¼æä»¬çæ¹æ³ä¼äº ToT å GoTï¼å¹¶ä¸å¨å¤ææ¨çä»»å¡ä¸çè¡¨ç°ä¹ä¼äº CoTãæ¬æçææä»£ç å¨æ­¤å¬å¼æä¾ï¼
https://github.com/ArmelRandy/tree-of-problemsã

##### **Multi-Task Program Error Repair and Explanatory Diagnosis**
2410.07271v1 by Zhenyu Xu, Victor S. Sheng

Program errors can occur in any type of programming, and can manifest in a
variety of ways, such as unexpected output, crashes, or performance issues. And
program error diagnosis can often be too abstract or technical for developers
to understand, especially for beginners. The goal of this paper is to present a
novel machine-learning approach for Multi-task Program Error Repair and
Explanatory Diagnosis (mPRED). A pre-trained language model is used to encode
the source code, and a downstream model is specifically designed to identify
and repair errors. Programs and test cases will be augmented and optimized from
several perspectives. Additionally, our approach incorporates a "chain of
thoughts" method, which enables the models to produce intermediate reasoning
explanations before providing the final correction. To aid in visualizing and
analyzing the program structure, we use a graph neural network for program
structure visualization. Overall, our approach offers a promising approach for
repairing program errors across different programming languages and providing
helpful explanations to programmers.

æè¦ï¼ç¨å¼é¯èª¤å¯è½ç¼çå¨ä»»ä½é¡åçç¨å¼è¨­è¨ä¸­ï¼ä¸¦å¯è½ä»¥åç¨®æ¹å¼åç¾ï¼ä¾å¦æå¤è¼¸åºãç¶æ©ææè½åé¡ãç¨å¼é¯èª¤è¨ºæ·éå¸¸å°éç¼äººå¡ä¾èªªéæ¼æ½è±¡ææè¡æ§ï¼ç¹å¥æ¯å°æ¼åå­¸èèè¨ãæ¬æçç®çæ¯æåºä¸åæ°ç©çå¤ä»»åç¨å¼é¯èª¤ä¿®å¾©èè§£éæ§è¨ºæ· (mPRED) æ©å¨å­¸ç¿æ¹æ³ãé åè¨ç·´çèªè¨æ¨¡åç¨æ¼ç·¨ç¢¼åå§ç¢¼ï¼èä¸æ¸¸æ¨¡ååå°éè¨­è¨ç¨æ¼è­å¥åä¿®å¾©é¯èª¤ãç¨å¼åæ¸¬è©¦æ¡ä¾å°å¾å¤åè§åº¦é²è¡æ´ååæä½³åãæ­¤å¤ï¼æåçåæ³åå«ãæèéãæ¹æ³ï¼ä½¿æ¨¡åè½å¤ å¨æä¾æçµä¿®æ­£ä¹åç¢çä¸­éæ¨çèªªæãçºäºå¹«å©è¦è¦ºåååæç¨å¼çµæ§ï¼æåä½¿ç¨åå½¢ç¥ç¶ç¶²è·¯é²è¡ç¨å¼çµæ§è¦è¦ºåãç¸½çä¾èªªï¼æåçåæ³æä¾äºä¸åæåæ¯çæ¹æ³ï¼å¯ä»¥ç¨æ¼ä¿®å¾©ä¸åç¨å¼èªè¨ä¸­çç¨å¼é¯èª¤ï¼ä¸¦åç¨å¼è¨­è¨å¸«æä¾æç¨çèªªæã

##### **Counterfactual Causal Inference in Natural Language with Large Language Models**
2410.06392v1 by GaÃ«l Gendron, JoÅ¾e M. RoÅ¾anec, Michael Witbrock, Gillian Dobbie

Causal structure discovery methods are commonly applied to structured data
where the causal variables are known and where statistical testing can be used
to assess the causal relationships. By contrast, recovering a causal structure
from unstructured natural language data such as news articles contains numerous
challenges due to the absence of known variables or counterfactual data to
estimate the causal links. Large Language Models (LLMs) have shown promising
results in this direction but also exhibit limitations. This work investigates
LLM's abilities to build causal graphs from text documents and perform
counterfactual causal inference. We propose an end-to-end causal structure
discovery and causal inference method from natural language: we first use an
LLM to extract the instantiated causal variables from text data and build a
causal graph. We merge causal graphs from multiple data sources to represent
the most exhaustive set of causes possible. We then conduct counterfactual
inference on the estimated graph. The causal graph conditioning allows
reduction of LLM biases and better represents the causal estimands. We use our
method to show that the limitations of LLMs in counterfactual causal reasoning
come from prediction errors and propose directions to mitigate them. We
demonstrate the applicability of our method on real-world news articles.

æè¦ï¼å æç»æåç°æ¹æ³éå¸¸åºç¨äºç»æåæ°æ®ï¼å¶ä¸­å æåéæ¯å·²ç¥çï¼å¹¶ä¸å¯ä»¥ä½¿ç¨ç»è®¡æ£éªæ¥è¯ä¼°å æå³ç³»ãç¸æ¯ä¹ä¸ï¼ä»æ°é»æç« ç­éç»æåçèªç¶è¯­è¨æ°æ®ä¸­æ¢å¤å æç»æç±äºç¼ºå°å·²ç¥åéæåäºå®æ°æ®æ¥ä¼°è®¡å æå³ç³»èåå«ä¼å¤ææãå¤§åè¯­è¨æ¨¡å (LLM) å¨è¿ä¸ªæ¹åä¸æ¾ç¤ºåºæå¸æçç»æï¼ä½ä¹è¡¨ç°åºå±éæ§ãè¿é¡¹å·¥ä½è°æ¥äº LLM ä»ææ¬ææ¡£æå»ºå æå¾åæ§è¡åäºå®å ææ¨ççè½åãæä»¬æåºäºä¸ç§ä»èªç¶è¯­è¨ä¸­è¿è¡ç«¯å°ç«¯å æç»æåç°åå ææ¨ççæ¹æ³ï¼æä»¬é¦åä½¿ç¨ LLM ä»ææ¬æ°æ®ä¸­æåå®ä¾åçå æåéå¹¶æå»ºå æå¾ãæä»¬åå¹¶æ¥èªå¤ä¸ªæ°æ®æºçå æå¾ï¼ä»¥è¡¨ç¤ºå¯è½çæè¯¦å°½çå æéãç¶åï¼æä»¬å¯¹ä¼°è®¡å¾è¿è¡åäºå®æ¨çãå æå¾æ¡ä»¶åè®¸åå° LLM åå·®å¹¶æ´å¥½å°è¡¨ç¤ºå æä¼°è®¡éãæä»¬ä½¿ç¨æä»¬çæ¹æ³è¡¨æ LLM å¨åäºå®å ææ¨çä¸­çå±éæ§æ¥èªé¢æµè¯¯å·®ï¼å¹¶æåºåè½»å®ä»¬çæªæ½ãæä»¬å¨ç°å®ä¸ççæ°é»æç« ä¸­å±ç¤ºäºæä»¬æ¹æ³çéç¨æ§ã

##### **Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA**
2410.06121v1 by Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella Lapata, Jeff Z. Pan

Retrieval-Augmented Generation (RAG) is widely used to inject external
non-parametric knowledge into large language models (LLMs). Recent works
suggest that Knowledge Graphs (KGs) contain valuable external knowledge for
LLMs. Retrieving information from KGs differs from extracting it from document
sets. Most existing approaches seek to directly retrieve relevant subgraphs,
thereby eliminating the need for extensive SPARQL annotations, traditionally
required by semantic parsing methods. In this paper, we model the subgraph
retrieval task as a conditional generation task handled by small language
models. Specifically, we define a subgraph identifier as a sequence of
relations, each represented as a special token stored in the language models.
Our base generative subgraph retrieval model, consisting of only 220M
parameters, achieves competitive retrieval performance compared to
state-of-the-art models relying on 7B parameters, demonstrating that small
language models are capable of performing the subgraph retrieval task.
Furthermore, our largest 3B model, when plugged with an LLM reader, sets new
SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model
and data will be made available online: https://github.com/hwy9855/GSR.

æè¦ï¼æª¢ç´¢å¢å¼·çæ (RAG) å»£æ³ç¨æ¼å°å¤é¨éåæ¸ç¥è­æ³¨å¥å¤§åèªè¨æ¨¡å (LLM)ãæè¿çç ç©¶è¡¨æï¼ç¥è­å (KG) åå«å° LLM æå¹å¼çå¤é¨ç¥è­ãå¾ KG ä¸­æ·åè³è¨èå¾æä»¶éä¸­æ·åè³è¨ä¸åãå¤§å¤æ¸ç¾ææ¹æ³å°æ±ç´æ¥æ·åç¸éå­åï¼å¾èæ¶é¤äºå°èªç¾©è§£ææ¹æ³å³çµ±ä¸æéçå»£æ³ SPARQL è¨»è§£çéæ±ãå¨æ¬æä¸­ï¼æåå°å­åæ·åä»»åå»ºæ¨¡çºç±å°åèªè¨æ¨¡åèççæ¢ä»¶çæä»»åãå·é«ä¾èªªï¼æåå°å­åè­å¥ç¬¦å®ç¾©çºéä¿åºåï¼æ¯åéä¿é½è¡¨ç¤ºçºå²å­å¨èªè¨æ¨¡åä¸­çç¹æ®æ¨è¨ãæåçåºç¤çæå¼å­åæ·åæ¨¡åååå« 220M åæ¸ï¼èä¾è³´ 7B åæ¸çææ°æ¨¡åç¸æ¯ï¼éå°äºå·æç«¶ç­åçæ·åæè½ï¼è­æå°åèªè¨æ¨¡åè½å¤ å·è¡å­åæ·åä»»åãæ­¤å¤ï¼ç¶æåæå¤§ç 3B æ¨¡åè LLM é±è®å¨çµåä½¿ç¨æï¼å¨ WebQSP å CWQ åºæºä¸è¨­å®äºæ°çç«¯å°ç«¯æè½ SOTAãæåçæ¨¡ååè³æå°å¨ç·ä¸å¬éï¼https://github.com/hwy9855/GSRã

##### **LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs**
2410.06062v2 by Vincent Emonet, Jerven Bolleman, Severine Duvaud, Tarcisio Mendes de Farias, Ana Claudia Sima

We introduce a Retrieval-Augmented Generation (RAG) system for translating
user questions into accurate federated SPARQL queries over bioinformatics
knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance
accuracy and reduce hallucinations in query generation, our system utilises
metadata from the KGs, including query examples and schema information, and
incorporates a validation step to correct generated queries. The system is
available online at chat.expasy.org.

æè¦ï¼æåå¼å¥æª¢ç´¢å¢å¼·çæ (RAG) ç³»çµ±ï¼ç¨æ¼å°ä½¿ç¨èåé¡ç¿»è­¯ææºç¢ºçè¯å SPARQL æ¥è©¢ï¼ä»¥å©ç¨å¤§åèªè¨æ¨¡å (LLM) é²è¡çç©è³è¨å­¸ç¥è­å (KG)ãçºäºå¢å¼·æºç¢ºæ§ä¸¦æ¸å°æ¥è©¢çæä¸­çå¹»è¦ºï¼æåçç³»çµ±å©ç¨ä¾èª KG çåè³æï¼åæ¬æ¥è©¢ç¯ä¾åæ¶æ§è³è¨ï¼ä¸¦çµåé©è­æ­¥é©ä¾ä¿®æ­£å·²çæçæ¥è©¢ãè©²ç³»çµ±å¯å¨ chat.expasy.org ä¸ç·ä½¿ç¨ã

##### **Jet Expansions of Residual Computation**
2410.06024v1 by Yihong Chen, Xiangxiang Xu, Yao Lu, Pontus Stenetorp, Luca Franceschi

We introduce a framework for expanding residual computational graphs using
jets, operators that generalize truncated Taylor series. Our method provides a
systematic approach to disentangle contributions of different computational
paths to model predictions. In contrast to existing techniques such as
distillation, probing, or early decoding, our expansions rely solely on the
model itself and requires no data, training, or sampling from the model. We
demonstrate how our framework grounds and subsumes logit lens, reveals a
(super-)exponential path structure in the recursive residual depth and opens up
several applications. These include sketching a transformer large language
model with $n$-gram statistics extracted from its computations, and indexing
the models' levels of toxicity knowledge. Our approach enables data-free
analysis of residual computation for model interpretability, development, and
evaluation.

æè¦ï¼æåå¼å¥äºä¸åæ¡æ¶ï¼ç¨å´å°æµæ´å±æ®å·®è¨ç®åï¼å´å°æµæ¯ä¸ç¨®å°æªæ·çæ³°åç´æ¸æ³åçéç®å­ãæåçéåæ¹æ³æä¾äºä¸åç³»çµ±åçéå¾ï¼ç¨ä¾è§£éä¸åè¨ç®è·¯å¾å°æ¨¡åé æ¸¬çè²¢ç»ãèç¾æçæè¡ï¼ä¾å¦è¸é¤¾ãæ¢æ¸¬ææ©æè§£ç¢¼ï¼ç¸åï¼æåçæ´å±åä¾è³´æ¼æ¨¡åæ¬èº«ï¼ä¸éè¦å¾æ¨¡åä¸­ç²åæ¸æãè¨ç·´æåæ¨£ãæåå±ç¤ºäºæåçæ¡æ¶å¦ä½å¥ å®åæ¦æ¬éè¼¯éé¡ï¼æ­ç¤ºäºéæ­¸æ®å·®æ·±åº¦ä¸­çï¼è¶ï¼ææ¸è·¯å¾çµæ§ï¼ä¸¦æéäºå¹¾åæç¨ãéäºæç¨åæ¬ç¨å¾å¶è¨ç®ä¸­æåç n-gram çµ±è¨æ¸æç¹ªè£½ä¸åTransformerå¤§åèªè¨æ¨¡åï¼ä¸¦ç´¢å¼æ¨¡åçæ¯æ§ç¥è­ç´å¥ãæåçéç¨®æ¹æ³è½å¤ å°æ®å·®è¨ç®é²è¡ç¡æ¸æåæï¼ç¨æ¼æ¨¡åçå¯è§£éæ§ãéç¼åè©ä¼°ã

##### **A large collection of bioinformatics question-query pairs over federated knowledge graphs: methodology and applications**
2410.06010v1 by Jerven Bolleman, Vincent Emonet, Adrian Altenhoff, Amos Bairoch, Marie-Claude Blatter, Alan Bridge, Severine Duvaud, Elisabeth Gasteiger, Dmitry Kuznetsov, Sebastien Moretti, Pierre-Andre Michel, Anne Morgat, Marco Pagni, Nicole Redaschi, Monique Zahn-Zabal, Tarcisio Mendes de Farias, Ana Claudia Sima

Background. In the last decades, several life science resources have
structured data using the same framework and made these accessible using the
same query language to facilitate interoperability. Knowledge graphs have seen
increased adoption in bioinformatics due to their advantages for representing
data in a generic graph format. For example, yummydata.org catalogs more than
60 knowledge graphs accessible through SPARQL, a technical query language.
Although SPARQL allows powerful, expressive queries, even across physically
distributed knowledge graphs, formulating such queries is a challenge for most
users. Therefore, to guide users in retrieving the relevant data, many of these
resources provide representative examples. These examples can also be an
important source of information for machine learning, if a sufficiently large
number of examples are provided and published in a common, machine-readable and
standardized format across different resources.
  Findings. We introduce a large collection of human-written natural language
questions and their corresponding SPARQL queries over federated bioinformatics
knowledge graphs (KGs) collected for several years across different research
groups at the SIB Swiss Institute of Bioinformatics. The collection comprises
more than 1000 example questions and queries, including 65 federated queries.
We propose a methodology to uniformly represent the examples with minimal
metadata, based on existing standards. Furthermore, we introduce an extensive
set of open-source applications, including query graph visualizations and smart
query editors, easily reusable by KG maintainers who adopt the proposed
methodology.
  Conclusions. We encourage the community to adopt and extend the proposed
methodology, towards richer KG metadata and improved Semantic Web services.

æè¦ï¼<paragraph>èæ¯ãå¨éå»å¹¾åå¹´ï¼è¨±å¤çå½ç§å­¸è³æºä½¿ç¨ç¸åçæ¶æ§ä¾å»ºæ§è³æï¼ä¸¦ä½¿ç¨ç¸åçæ¥è©¢èªè¨ä¾å­åéäºè³æï¼ä»¥ä¿é²äºæä½æ§ãç¥è­åè­ç±æ¼å¶ä»¥éç¨åå½¢æ ¼å¼è¡¨ç¤ºè³æçåªé»ï¼å æ­¤å¨çç©è³è¨å­¸ä¸­ç²å¾äºè¶ä¾è¶å»£æ³çæ¡ç¨ãä¾å¦ï¼yummydata.org ç·¨éäºè¶é 60 åå¯ééæè¡æ¥è©¢èªè¨ SPARQL å­åçç¥è­åè­ãåç®¡ SPARQL åè¨±é²è¡å¼·å¤§ä¸å·è¡¨éåçæ¥è©¢ï¼çè³è·¨è¶å¯¦é«åå¸çç¥è­åè­ï¼ä½å°å¤§å¤æ¸ä½¿ç¨èä¾èªªï¼å¶å®æ­¤é¡æ¥è©¢æ¯ä¸é ææ°ãå æ­¤ï¼çºäºæå°ä½¿ç¨èæ·åç¸éè³æï¼å¶ä¸­è¨±å¤è³æºæä¾äºå·ä»£è¡¨æ§çç¯ä¾ãå¦ææä¾äºè¶³å¤ å¤§éçç¯ä¾ï¼ä¸¦ä»¥è·¨ä¸åè³æºçéç¨ãæ©å¨å¯è®ä¸æ¨æºåçæ ¼å¼ç¼å¸ï¼éäºç¯ä¾ä¹å¯ä»¥æçºæ©å¨å­¸ç¿çéè¦è³è¨ä¾æºã
  ç¼ç¾ãæåå¼å¥äºå¤§éç±äººé¡æ°å¯«çèªç¶èªè¨åé¡åå¶å°æç SPARQL æ¥è©¢ï¼éäºæ¥è©¢æ¯å¤å¹´ä¾å¨ SIB çå£«çç©è³è¨å­¸ç ç©¶æçä¸åç ç©¶å°çµä¸­æ¶éçï¼æ¶µèäºè¯é¦çç©è³è¨å­¸ç¥è­åè­ (KG)ãè©²éååå« 1000 å¤åç¯ä¾åé¡åæ¥è©¢ï¼åæ¬ 65 åè¯é¦æ¥è©¢ãæåæåºäºä¸ç¨®æ¹æ³ï¼åºæ¼ç¾ææ¨æºï¼ä»¥æå°çåè³æçµ±ä¸è¡¨ç¤ºéäºç¯ä¾ãæ­¤å¤ï¼æåéå¼å¥äºä¸å¥å»£æ³çéæºæç¨ç¨å¼ï¼åæ¬æ¥è©¢åå½¢è¦è¦ºååæºæ§æ¥è©¢ç·¨è¼¯å¨ï¼éäºæç¨ç¨å¼å¾å®¹æè¢«æ¡ç¨ææåºæ¹æ³ç KG ç¶­è­·äººå¡éè¤ä½¿ç¨ã
  çµè«ãæåé¼åµç¤¾ç¾¤æ¡ç¨ä¸¦æ´åææåºçæ¹æ³ï¼ä»¥ç²å¾æ´è±å¯ç KG åè³æåæ¹åçèªæç¶²è·¯æåã</paragraph>

##### **LightRAG: Simple and Fast Retrieval-Augmented Generation**
2410.05779v1 by Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang

Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge sources, enabling more accurate and
contextually relevant responses tailored to user needs. However, existing RAG
systems have significant limitations, including reliance on flat data
representations and inadequate contextual awareness, which can lead to
fragmented answers that fail to capture complex inter-dependencies. To address
these challenges, we propose LightRAG, which incorporates graph structures into
text indexing and retrieval processes. This innovative framework employs a
dual-level retrieval system that enhances comprehensive information retrieval
from both low-level and high-level knowledge discovery. Additionally, the
integration of graph structures with vector representations facilitates
efficient retrieval of related entities and their relationships, significantly
improving response times while maintaining contextual relevance. This
capability is further enhanced by an incremental update algorithm that ensures
the timely integration of new data, allowing the system to remain effective and
responsive in rapidly changing data environments. Extensive experimental
validation demonstrates considerable improvements in retrieval accuracy and
efficiency compared to existing approaches. We have made our LightRAG
open-source and available at the link: https://github.com/HKUDS/LightRAG.

æè¦ï¼æª¢ç´¢å¢å¼·çæ (RAG) ç³»çµ±ééæ´åå¤é¨ç¥è­ä¾æºä¾å¢å¼·å¤§åèªè¨æ¨¡å (LLM)ï¼è½éå°ä½¿ç¨èéæ±æä¾æ´æºç¢ºä¸èèçµ¡ç¸éçåæãç¶èï¼ç¾æç RAG ç³»çµ±æå¾å¤§çéå¶ï¼åæ¬ä¾è³´å¹³é¢è³æè¡¨ç¤ºåä¸è¶³çèçµ¡æç¥ï¼éå¯è½æå°è´ç¡æ³ææè¤éç¸äºä¾è³´æ§ççæ®µå¼ç­æ¡ãçºäºæå°éäºææ°ï¼æåæåº LightRAGï¼å®å°åå½¢çµæ§ç´å¥æå­ç´¢å¼åæª¢ç´¢æµç¨ä¸­ãéååµæ°æ¶æ§æ¡ç¨éå±¤æª¢ç´¢ç³»çµ±ï¼è½å¾ä½å±¤ç´åé«å±¤ç´ç¥è­ç¼ç¾ä¸­å¢å¼·å¨é¢çè³è¨æª¢ç´¢ãæ­¤å¤ï¼å°åå½¢çµæ§èåéè¡¨ç¤ºæ´åï¼æå©æ¼æææª¢ç´¢ç¸éå¯¦é«åå¶éä¿ï¼å¤§å¹æ¹ååææéï¼åæç¶­æèçµ¡ç¸éæ§ãæ­¤åè½é²ä¸æ­¥ééå¢éæ´æ°æ¼ç®æ³å¢å¼·ï¼å¯ç¢ºä¿åææ´åæ°è³æï¼è®ç³»çµ±å¨å¿«éè®åçè³æç°å¢ä¸­ä¿æææä¸å³æåæãå»£æ³çå¯¦é©é©è­é¡¯ç¤ºï¼èç¾ææ¹æ³ç¸æ¯ï¼æª¢ç´¢æºç¢ºåº¦åæçé½æé¡¯èçæ¹åãæåå·²éæ¾ LightRAG åå§ç¢¼ï¼ä¸¦æä¾ä»¥ä¸é£çµï¼https://github.com/HKUDS/LightRAGã

##### **Information Discovery in e-Commerce**
2410.05763v2 by Zhaochun Ren, Xiangnan He, Dawei Yin, Maarten de Rijke

Electronic commerce, or e-commerce, is the buying and selling of goods and
services, or the transmitting of funds or data online. E-commerce platforms
come in many kinds, with global players such as Amazon, Airbnb, Alibaba, eBay
and platforms targeting specific geographic regions. Information retrieval has
a natural role to play in e-commerce, especially in connecting people to goods
and services. Information discovery in e-commerce concerns different types of
search (e.g., exploratory search vs. lookup tasks), recommender systems, and
natural language processing in e-commerce portals. The rise in popularity of
e-commerce sites has made research on information discovery in e-commerce an
increasingly active research area. This is witnessed by an increase in
publications and dedicated workshops in this space. Methods for information
discovery in e-commerce largely focus on improving the effectiveness of
e-commerce search and recommender systems, on enriching and using knowledge
graphs to support e-commerce, and on developing innovative question answering
and bot-based solutions that help to connect people to goods and services. In
this survey, an overview is given of the fundamental infrastructure,
algorithms, and technical solutions for information discovery in e-commerce.
The topics covered include user behavior and profiling, search, recommendation,
and language technology in e-commerce.

æè¦ï¼é»å­ååï¼æç¨±é»å­ååï¼æ¯è²·è³£åååæåï¼æå¨ç·ä¸å³è¼¸è³éæè³æãé»å­ååå¹³å°ç¨®é¡ç¹å¤ï¼åæ¬ AmazonãAirbnbãAlibabaãeBay ç­å¨çæ§æ¥­èï¼ä»¥åéå®ç¹å®å°çååçå¹³å°ãè³è¨æª¢ç´¢å¨é»å­ååä¸­æ®æ¼èªç¶çè§è²ï¼ç¹å¥æ¯å¨å°äººåèåååæåé£çµèµ·ä¾æ¹é¢ãé»å­ååä¸­çè³è¨ç¼ç¾æ¶åä¸åé¡åçæå°ï¼ä¾å¦æ¢ç´¢æ§æå°èæ¥è©¢ä»»åï¼ãæ¨è¦ç³»çµ±ï¼ä»¥åé»å­ååå¥å£ç¶²ç«ä¸­çèªç¶èªè¨èçãé»å­ååç¶²ç«çæ®åä½¿é»å­ååä¸­çè³è¨ç¼ç¾ç ç©¶æçºä¸åè¶ä¾è¶æ´»èºçç ç©¶é åãéé»å¯ä»¥å¾éæ¹é¢çåºçååå°éç è¨æçå¢å çåºãé»å­ååä¸­çè³è¨ç¼ç¾æ¹æ³ä¸»è¦èéæ¼æ¹åé»å­ååæå°åæ¨è¦ç³»çµ±çæè½ï¼è±å¯ä¸¦ä½¿ç¨ç¥è­åè¡¨ä¾æ¯æ´é»å­ååï¼ä»¥åéç¼åµæ°çåé¡è§£ç­åæ©å¨äººè§£æ±ºæ¹æ¡ï¼ä»¥åå©å°äººåèåååæåé£çµèµ·ä¾ãå¨éé èª¿æ¥ä¸­ï¼æ¦è¿°äºé»å­ååä¸­è³è¨ç¼ç¾çåºæ¬æ¶æ§ãæ¼ç®æ³åæè¡è§£æ±ºæ¹æ¡ãææ¶µèçä¸»é¡åæ¬é»å­ååä¸­çä½¿ç¨èè¡çºåè¨­å®æªãæå°ãæ¨è¦åèªè¨æè¡ã

##### **Vector-ICL: In-context Learning with Continuous Vector Representations**
2410.05629v1 by Yufan Zhuang, Chandan Singh, Liyuan Liu, Jingbo Shang, Jianfeng Gao

Large language models (LLMs) have shown remarkable in-context learning (ICL)
capabilities on textual data. We explore whether these capabilities can be
extended to continuous vectors from diverse domains, obtained from black-box
pretrained encoders. By aligning input data with an LLM's embedding space
through lightweight projectors, we observe that LLMs can effectively process
and learn from these projected vectors, which we term Vector-ICL. In
particular, we find that pretraining projectors with general language modeling
objectives enables Vector-ICL, while task-specific finetuning further enhances
performance. In our experiments across various tasks and modalities, including
text reconstruction, numerical function regression, text classification,
summarization, molecule captioning, time-series classification, graph
classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL
and domain-specific model or tuning. We further conduct analyses and case
studies, indicating the potential of LLMs to process vector representations
beyond traditional token-based paradigms.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨ææ¬è³æä¸å±ç¾åºé¡¯èçèªå¢å­¸ç¿ (ICL) è½åãæåæ¢è¨éäºè½åæ¯å¦å¯ä»¥æ´å±å°å¾ä¸åé ååå¾ï¼ä¸¦ç±é»ç®±é è¨ç·´ç·¨ç¢¼å¨ç²å¾çé£çºåéãééè¼éç´æå½±å¨å°è¼¸å¥è³æè LLM çåµå¥ç©ºéå°é½ï¼æåè§å¯å° LLM å¯ä»¥ææå°èçåå­¸ç¿éäºæå½±åéï¼æåç¨±ä¹çº Vector-ICLãç¹å¥æ¯ï¼æåç¼ç¾ä½¿ç¨ä¸è¬èªè¨å»ºæ¨¡ç®æ¨é è¨ç·´æå½±å¨å¯ä»¥åç¨ Vector-ICLï¼èç¹å®ä»»åçå¾®èª¿é²ä¸æ­¥æåäºæè½ãå¨æåè·¨è¶åç¨®ä»»ååæ¨¡æçå¯¦é©ä¸­ï¼åæ¬æå­éå»ºãæ¸å¼å½æ¸åæ­¸ãæå­åé¡ãæè¦ãåå­æ¨é¡ãæéåºååé¡ãåå½¢åé¡å fMRI è§£ç¢¼ï¼Vector-ICL éå¸¸é½åªæ¼å°æ¬¡æ¸ ICL åç¹å®é åæ¨¡åæèª¿æ´ãæåé²ä¸æ­¥é²è¡åæåæ¡ä¾ç ç©¶ï¼æåº LLM èçåéè¡¨ç¤ºçæ½åï¼è¶è¶å³çµ±çåºæ¼æ¨è¨çç¯ä¾ã

##### **Narrative-of-Thought: Improving Temporal Reasoning of Large Language Models via Recounted Narratives**
2410.05558v1 by Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang

Reasoning about time and temporal relations is an integral aspect of human
cognition, essential for perceiving the world and navigating our experiences.
Though large language models (LLMs) have demonstrated impressive performance in
many reasoning tasks, temporal reasoning remains challenging due to its
intrinsic complexity. In this work, we first study an essential task of
temporal reasoning -- temporal graph generation, to unveil LLMs' inherent,
global reasoning capabilities. We show that this task presents great challenges
even for the most powerful LLMs, such as GPT-3.5/4. We also notice a
significant performance gap by small models (<10B) that lag behind LLMs by 50%.
Next, we study how to close this gap with a budget constraint, e.g., not using
model finetuning. We propose a new prompting technique tailored for temporal
reasoning, Narrative-of-Thought (NoT), that first converts the events set to a
Python class, then prompts a small model to generate a temporally grounded
narrative, guiding the final generation of a temporal graph. Extensive
experiments showcase the efficacy of NoT in improving various metrics. Notably,
NoT attains the highest F1 on the Schema-11 evaluation set, while securing an
overall F1 on par with GPT-3.5. NoT also achieves the best structural
similarity across the board, even compared with GPT-3.5/4. Our code is
available at https://github.com/launchnlp/NoT.

æè¦ï¼æ¨çæéåæééä¿æ¯äººé¡èªç¥ä¸­ä¸å¯æç¼ºçä¸ç°ï¼å°æ¼æç¥ä¸çåå°èªæåçç¶é©è³ééè¦ãåç®¡å¤§åèªè¨æ¨¡å (LLM) å¨è¨±å¤æ¨çä»»åä¸­è¡¨ç¾åºè²ï¼ä½ç±æ¼æéæ¨ççå§å¨è¤éæ§ï¼å æ­¤ä»ç¶å·æææ°æ§ãå¨éé å·¥ä½ä¸­ï¼æåé¦åç ç©¶æéæ¨ççä¸é åºæ¬ä»»åââæéåè¡¨çæï¼ä»¥æ­ç¤º LLM åºæçå¨å±æ¨çè½åãæåè¡¨æï¼å³ä½¿å°æ¼åè½æå¼ºå¤§ç LLMï¼ä¾å¦ GPT-3.5/4ï¼æ­¤ä»»åä¹æåºäºå·¨å¤§çææ°ãæåéæ³¨æå°ï¼è½å¾æ¼ LLM 50% çå°åæ¨¡å (<10B) å­å¨é¡¯èçæ§è½å·®è·ãæ¥ä¸ä¾ï¼æåç ç©¶å¦ä½å¨é ç®ç´æä¸ç¸®å°éç¨®å·®è·ï¼ä¾å¦ä¸ä½¿ç¨æ¨¡åå¾®èª¿ãæåæåºäºä¸ç¨®éå°æéæ¨çéèº«å®å¶çæ°æç¤ºæè¡ï¼å³æèæäº (NoT)ï¼å®é¦åå°äºä»¶éè½æçº Python é¡ï¼ç¶å¾æç¤ºä¸åå°åæ¨¡åçæä¸åæéä¾æçæäºï¼æå°æéåè¡¨çæçµçæãå¤§éçå¯¦é©å±ç¤ºäº NoT å¨æ¹ååç¨®ææ¨æ¹é¢çåæãå¼å¾æ³¨æçæ¯ï¼NoT å¨ Schema-11 è©ä¼°éä¸­ç²å¾äºæé«ç F1ï¼åæç¢ºä¿äºè GPT-3.5 ç¸ç¶çæ´é« F1ãå³ä½¿è GPT-3.5/4 ç¸æ¯ï¼NoT ä¹å¨åæ¹é¢å¯¦ç¾äºæä½³çµæ§ç¸ä¼¼æ§ãæåçä»£ç¢¼å¯å¨ https://github.com/launchnlp/NoT ä¸­ç²å¾ã

##### **Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents**
2410.05130v1 by Yuwei Hu, Runlin Lei, Xinyi Huang, Zhewei Wei, Yongchao Liu

Recent research has explored the use of Large Language Models (LLMs) for
tackling complex graph reasoning tasks. However, due to the intricacies of
graph structures and the inherent limitations of LLMs in handling long text,
current approaches often fail to deliver satisfactory accuracy, even on
small-scale graphs and simple tasks. To address these challenges, we introduce
GraphAgent-Reasoner, a fine-tuning-free framework that utilizes a multi-agent
collaboration strategy for explicit and precise graph reasoning. Inspired by
distributed graph computation theory, our framework decomposes graph problems
into smaller, node-centric tasks that are distributed among multiple agents.
The agents collaborate to solve the overall problem, significantly reducing the
amount of information and complexity handled by a single LLM, thus enhancing
the accuracy of graph reasoning. By simply increasing the number of agents,
GraphAgent-Reasoner can efficiently scale to accommodate larger graphs with
over 1,000 nodes. Evaluated on the GraphInstruct dataset, our framework
demonstrates near-perfect accuracy on polynomial-time graph reasoning tasks,
significantly outperforming the best available models, both closed-source and
fine-tuned open-source variants. Our framework also demonstrates the capability
to handle real-world graph reasoning applications such as webpage importance
analysis.

æè¦ï¼è¿æç ç©¶å·²æ¢è¨ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾èçè¤éçåå½¢æ¨çä»»åãç¶èï¼ç±æ¼åå½¢çµæ§çè¤éæ§å LLM å¨èçé·ææ¬æåºæçéå¶ï¼ç¾ææ¹æ³éå¸¸ç¡æ³æä¾ä»¤äººæ»¿æçæºç¢ºæ§ï¼å³ä½¿æ¯å¨å°è¦æ¨¡åå½¢åç°¡å®ä»»åä¸ãçºäºæå°éäºææ°ï¼æåå¼å¥äº GraphAgent-Reasonerï¼éæ¯ä¸åç¡éå¾®èª¿çæ¡æ¶ï¼å®å©ç¨å¤ä¸»é«åä½ç­ç¥é²è¡æç¢ºä¸ç²¾ç¢ºçåå½¢æ¨çãæåçæ¡æ¶åå°åæ£å¼åå½¢è¨ç®çè«çåç¼ï¼å°åå½¢åé¡åè§£ææ´å°çä»¥ç¯é»çºä¸­å¿çä»»åï¼ä¸¦å°éäºä»»ååéçµ¦å¤åä¸»é«ãéäºä¸»é«åä½è§£æ±ºæ´é«åé¡ï¼å¤§å¹æ¸å°å®å LLM èççè³è¨éåè¤éåº¦ï¼å¾èæååå½¢æ¨ççæºç¢ºæ§ãééå®ç´å¢å ä¸»é«æ¸éï¼GraphAgent-Reasoner å¯ä»¥æææ´åä»¥å®¹ç´ç¯é»è¶é 1,000 åçå¤§ååå½¢ãå¨ GraphInstruct è³æéä¸é²è¡è©ä¼°ï¼æåçæ¡æ¶å¨å¤é å¼æéåå½¢æ¨çä»»åä¸å±ç¾åºè¿ä¹å®ç¾çæºç¢ºæ§ï¼å¤§å¹åªæ¼å¸é¢ä¸æå¥½çæ¨¡åï¼åå«éæºåå¾®èª¿éæºçæ¬ãæåçæ¡æ¶ä¹å±ç¾åºèççå¯¦ä¸çåå½¢æ¨çæç¨ç¨å¼çè½åï¼ä¾å¦ç¶²é éè¦æ§åæã

##### **Leverage Knowledge Graph and Large Language Model for Law Article Recommendation: A Case Study of Chinese Criminal Law**
2410.04949v1 by Yongming Chen, Miner Chen, Ye Zhu, Juan Pei, Siyu Chen, Yu Zhou, Yi Wang, Yifan Zhou, Hao Li, Songan Zhang

Court efficiency is vital for social stability. However, in most countries
around the world, the grassroots courts face case backlogs, with decisions
relying heavily on judicial personnel's cognitive labor, lacking intelligent
tools to improve efficiency. To address this issue, we propose an efficient law
article recommendation approach utilizing a Knowledge Graph (KG) and a Large
Language Model (LLM). Firstly, we propose a Case-Enhanced Law Article Knowledge
Graph (CLAKG) as a database to store current law statutes, historical case
information, and correspondence between law articles and historical cases.
Additionally, we introduce an automated CLAKG construction method based on LLM.
On this basis, we propose a closed-loop law article recommendation method.
Finally, through a series of experiments using judgment documents from the
website "China Judgements Online", we have improved the accuracy of law article
recommendation in cases from 0.549 to 0.694, demonstrating that our proposed
method significantly outperforms baseline approaches.

æè¦ï¼æ³é¢æçå°æ¼ç¤¾æç©©å®è³ééè¦ãç¶èï¼å¨ä¸çå¤§å¤æ¸åå®¶ä¸­ï¼åºå±¤æ³é¢é¢è¨æ¡ä»¶ç©å£ï¼å¤æ±ºå´éä¾è³´å¸æ³äººå¡çèªç¥ååï¼ç¼ºä¹æé«æççæºè½å·¥å·ãçºäºè§£æ±ºéååé¡ï¼æåæåºäºä¸åå©ç¨ç¥è­åè­ (KG) åå¤§åèªè¨æ¨¡å (LLM) çé«ææ³å¾æ¢ææ¨è¦æ¹æ³ãé¦åï¼æåæåºä¸åæ¡ä¾å¢å¼·æ³å¾æ¢æç¥è­åè­ (CLAKG) ä½çºä¸åè³æåº«ï¼ç¨æ¼å²å­ç¾è¡æ³å¾æ³è¦ãæ­·å²æ¡ä¾è³è¨åæ³å¾æ¢æèæ­·å²æ¡ä¾ä¹éçå°æéä¿ãæ­¤å¤ï¼æåå¼å¥ä¸ååºæ¼ LLM çèªåå CLAKG æ§å»ºæ¹æ³ãå¨æ­¤åºç¤ä¸ï¼æåæåºäºä¸åéç°æ³å¾æ¢ææ¨è¦æ¹æ³ãæå¾ï¼ééä¸é£ä¸²ä½¿ç¨ä¾èªç¶²ç«ãä¸­åè£å¤ææ¸ç¶²ãçè£å¤ææ¸çå¯¦é©ï¼æåå°æ¡ä»¶ä¸­æ³å¾æ¢ææ¨è¦çæºç¢ºçå¾ 0.549 æåè³ 0.694ï¼è­ææåæåºçæ¹æ³é¡¯èåªæ¼åºæºæ¹æ³ã

##### **GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA**
2410.04790v1 by Xinyu Wang, Yanzheng Xiang, Lin Gui, Yulan He

In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods.

æè¦ï¼<paragraph>å¨éå»ï¼æª¢ç´¢å¢å¼·çæ (RAG) æ¹æ³æå°æå­åå²æå¡ï¼ä»¥ä½¿èªè¨æ¨¡åè½å¤ èçé·ç¯æä»¶ãæè¿çåºæ¼æ¨¹ç RAG æ¹æ³è½å¤ å¨ä¿çæ´é«èçµ¡çåææª¢ç´¢è©³ç´°è³è¨ãç¶èï¼é¨èåè½æ´å¼·å¤§ç LLMï¼ä¾å¦ Llama 3.1ï¼çåºç¾ï¼éäº LLM æä¾äºæ´å¥½ççè§£ååå°æ´é·è¼¸å¥çæ¯æ´ï¼æåç¼ç¾å³ä½¿æ¯æè¿çåºæ¼æ¨¹ç RAG æ¹æ³ä¹æ¯ç´æ¥å°æ´åæä»¶è¼¸å¥ Llama 3.1 çè¡¨ç¾æ´å·®ï¼åç®¡ RAG æ¹æ³å¨éä½éç®ææ¬æ¹é¢ä»å·ååªå¢ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°çæª¢ç´¢æ¹æ³ï¼ç¨±çºå·æåå±¤å æ¬åç LLM å¼å°åæé²åº¦æ§å¶ (GARLIC)ï¼å®åªæ¼ååçæåé²åºæºï¼åæ¬ Llama 3.1ï¼åæä¿çäº RAG æ¹æ³çéç®æçãæåçæ¹é²æ¹æ³å¼å¥äºå¤é æ¹é²ï¼(1) æåæ²æä½¿ç¨æ¨¹ççµæ§ï¼èæ¯æ§å»ºäºä¸åå·æå¤å°å¤æè¦çåå±¤å æ¬æåç¡ç°åï¼å¶ä¸­åéç·£æºèªæ³¨æåæ©å¶ï¼æ¯åç¯é»é½å°æ³¨æ¼å®ä¸äºä»¶ææ¥µå°æ¸äºä»¶ã(2) æåå¼å¥äºä¸ç¨®æ°ç©çæª¢ç´¢æ¹æ³ï¼å®å©ç¨ LLM çæ³¨æåæ¬éï¼èä¸æ¯å¯éåµå¥ç¸ä¼¼æ§ãæåçæ¹é²æ¹æ³åè¨±æ²¿å¤åè·¯å¾æå°åï¼ä¸¦ä¸å¯ä»¥å¨ä»»ä½æ·±åº¦çµæ­¢ã(3) æåä½¿ç¨ LLM ä¾æ§å¶æª¢ç´¢éç¨ï¼ä½¿å¶è½å¤ åæèª¿æ´çºä¸åæ¥è©¢æª¢ç´¢çè³è¨éåæ·±åº¦ãå¯¦é©çµæè¡¨æï¼æåçæ¹é²æ¹æ³å¨å©åå®æä»¶åå©åå¤æä»¶åç­è³æéä¸åªæ¼ååçæåé²åºæºï¼åæ¬ Llama 3.1ï¼åæç¶­æèå³çµ± RAG æ¹æ³é¡ä¼¼çéç®è¤éåº¦ã</paragraph>

##### **Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval**
2410.04585v1 by Pengcheng Jiang, Cao Xiao, Minhao Jiang, Parminder Bhatia, Taha Kass-Hout, Jimeng Sun, Jiawei Han

Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å¨è¨åºæ±ºç­æ¯æ´ä¸­å±ç¾åºé¡¯èçæ½åãç¶èï¼LLM ä»æå¹»è¦ºä¸ç¼ºä¹ç´°ç·»çèæ¯é«çç¥è­ï¼éå¶äºå®åå¨é«é¢¨éªé«çä¿å¥æç¨ä¸­çä½¿ç¨ï¼ä¾å¦è¨åºè¨ºæ·ãå³çµ±çæª¢ç´¢å¢å¼·çæ (RAG) æ¹æ³è©¦åè§£æ±ºéäºéå¶ï¼ä½ç¶å¸¸æª¢ç´¢ç¨çæç¡éçè³è¨ï¼æå®³é æ¸¬æºç¢ºåº¦ãæåå¼å¥äº KAREï¼éæ¯ä¸åæ°ç©çæ¶æ§ï¼æ´åäºç¥è­åè­ (KG) ç¤¾ç¾¤å±¤ç´æª¢ç´¢è LLM æ¨çï¼ä»¥å¢å¼·é«çä¿å¥é æ¸¬ãKARE ééæ´åçç©é«å­¸è³æåº«ãè¨åºæç»å LLM çæçè¦è§£ï¼å»ºæ§äºä¸åå¨é¢çå¤ä¾æº KGï¼ä¸¦ä½¿ç¨éå±¤å¼åå½¢ç¤¾ç¾¤åµæ¸¬åæè¦é²è¡çµç¹ï¼ä»¥é²è¡ç²¾ç¢ºä¸èèæ¯ç¸éçè³è¨æª¢ç´¢ãæåçééµåµæ°åæ¬ï¼(1) ä¸ç¨®å¯éçé«çç¥è­çµæ§åæ¹æ³ï¼è½å¤ æºç¢ºæª¢ç´¢ç¸éè³è¨ï¼(2) ä¸ç¨®åæç¥è­æª¢ç´¢æ©å¶ï¼å®ä½¿ç¨æç¦é»ãå¤é¢åçé«çè¦è§£ä¾è±å¯æ£èèæ¯ï¼ä»¥å (3) ä¸åæ¨çå¢å¼·é æ¸¬æ¶æ§ï¼å®å©ç¨éäºè±å¯çèæ¯ä¾ç¢çæºç¢ºä¸å¯è§£éçè¨åºé æ¸¬ãå»£æ³çå¯¦é©è­æï¼KARE å¨ MIMIC-III ä¸çæ­»äº¡çååå¥é¢é æ¸¬ä¸­æ¯é åæ¨¡åé«åº 10.8-15.0%ï¼å¨ MIMIC-IV ä¸é«åº 12.6-12.7%ãé¤äºä»¤äººå°è±¡æ·±å»çé æ¸¬æºç¢ºåº¦å¤ï¼æåçæ¶æ§éå©ç¨äº LLM çæ¨çè½åï¼å¢å¼·äºè¨åºé æ¸¬çå¯ä¿¡åº¦ã

##### **Mitigating Hallucinations Using Ensemble of Knowledge Graph and Vector Store in Large Language Models to Enhance Mental Health Support**
2410.10853v1 by Abdul Muqtadir, Hafiz Syed Muhammad Bilal, Ayesha Yousaf, Hafiz Farooq Ahmed, Jamil Hussain

This research work delves into the manifestation of hallucination within
Large Language Models (LLMs) and its consequential impacts on applications
within the domain of mental health. The primary objective is to discern
effective strategies for curtailing hallucinatory occurrences, thereby
bolstering the dependability and security of LLMs in facilitating mental health
interventions such as therapy, counseling, and the dissemination of pertinent
information. Through rigorous investigation and analysis, this study seeks to
elucidate the underlying mechanisms precipitating hallucinations in LLMs and
subsequently propose targeted interventions to alleviate their occurrence. By
addressing this critical issue, the research endeavors to foster a more robust
framework for the utilization of LLMs within mental health contexts, ensuring
their efficacy and reliability in aiding therapeutic processes and delivering
accurate information to individuals seeking mental health support.

æè¦ï¼æ¬ç ç©¶æ¢è¨å¤§åèªè¨æ¨¡å (LLM) ä¸­å¹»è¦ºçè¡¨ç¾ï¼åå¶å°å¿çå¥åº·é åæç¨ç¢ççå¾çºå½±é¿ãä¸»è¦ç®æ¨æ¯è¾¨å¥éå¶å¹»è¦ºç¼ççææç­ç¥ï¼å¾èå å¼· LLM å¨ä¿é²å¿çå¥åº·å¹²é æªæ½ï¼ä¾å¦æ²»çãè«®è©¢åå³æ­ç¸éè³è¨ï¼æ¹é¢çå¯é æ§åå®å¨æ§ãééå´è¬¹çèª¿æ¥ååæï¼æ¬ç ç©¶è©¦åé¡æå°è´ LLM ç¢çå¹»è¦ºçæ½å¨æ©å¶ï¼ä¸¦é²ä¸æ­¥æåºæéå°æ§çå¹²é æªæ½ä¾æ¸è¼å¶ç¼çãééè§£æ±ºéåééµåé¡ï¼æ¬ç ç©¶è´åæ¼å»ºç«ä¸åæ´ç©©å¥çæ¶æ§ï¼ä»¥ä¾¿å¨å¿çå¥åº·æå¢ä¸­ä½¿ç¨ LLMï¼ç¢ºä¿å¶å¨åå©æ²»çéç¨ååå°æ±å¿çå¥åº·æ¯æçåäººæä¾æºç¢ºè³è¨æ¹é¢çæè½åå¯é æ§ã

##### **Leveraging Social Determinants of Health in Alzheimer's Research Using LLM-Augmented Literature Mining and Knowledge Graphs**
2410.09080v1 by Tianqi Shang, Shu Yang, Weiqing He, Tianhua Zhai, Dawei Li, Bojian Hou, Tianlong Chen, Jason H. Moore, Marylyn D. Ritchie, Li Shen

Growing evidence suggests that social determinants of health (SDoH), a set of
nonmedical factors, affect individuals' risks of developing Alzheimer's disease
(AD) and related dementias. Nevertheless, the etiological mechanisms underlying
such relationships remain largely unclear, mainly due to difficulties in
collecting relevant information. This study presents a novel, automated
framework that leverages recent advancements of large language model (LLM) and
natural language processing techniques to mine SDoH knowledge from extensive
literature and integrate it with AD-related biological entities extracted from
the general-purpose knowledge graph PrimeKG. Utilizing graph neural networks,
we performed link prediction tasks to evaluate the resultant SDoH-augmented
knowledge graph. Our framework shows promise for enhancing knowledge discovery
in AD and can be generalized to other SDoH-related research areas, offering a
new tool for exploring the impact of social determinants on health outcomes.
Our code is available at: https://github.com/hwq0726/SDoHenPKG

æè¦ï¼è¶ä¾è¶å¤çè­æè¡¨æï¼ç¤¾æå¥åº·æ±ºå®å ç´  (SDoH) æ¯ä¸çµéé«çå ç´ ï¼æå½±é¿åäººç½¹æ£é¿è²æµ·é»ç (AD) åç¸éå¤±æºççé¢¨éªãç¶èï¼éç¨®éä¿èå¾çåºæ¬æ©å¶å¨å¾å¤§ç¨åº¦ä¸ä»ä¸æ¸æ¥ï¼ä¸»è¦æ¯å çºé£ä»¥æ¶éç¸éè³è¨ãæ¬ç ç©¶æåºä¸åæ°ç©çèªååæ¶æ§ï¼å©ç¨å¤§åèªè¨æ¨¡å (LLM) åèªç¶èªè¨èçæè¡çææ°é²å±ï¼å¾å»£æ³çæç»ä¸­ææ SDoH ç¥è­ï¼ä¸¦å°å¶èå¾éç¨ç¥è­å PrimeKG ä¸­æåç AD ç¸éçç©å¯¦é«æ´åå¨ä¸èµ·ãå©ç¨åç¥ç¶ç¶²è·¯ï¼æåå·è¡é£çµé æ¸¬ä»»åï¼ä»¥è©ä¼°çæç SDoH æ´åç¥è­åãæåçæ¶æ§é¡¯ç¤ºåºå¢å¼· AD ä¸­ç¥è­ç¼ç¾çå¸æï¼ä¸¦ä¸å¯ä»¥æ¨å»£å°å¶ä»è SDoH ç¸éçç ç©¶é åï¼æä¾ä¸åæ¢ç´¢ç¤¾ææ±ºå®å ç´ å°å¥åº·çµæå½±é¿çæ°å·¥å·ãæåçç¨å¼ç¢¼å¯å¨ https://github.com/hwq0726/SDoHenPKG åå¾

##### **Empowering Domain-Specific Language Models with Graph-Oriented Databases: A Paradigm Shift in Performance and Model Maintenance**
2410.03867v1 by Ricardo Di Pasquale, Soledad Represa

In an era dominated by data, the management and utilization of
domain-specific language have emerged as critical challenges in various
application domains, particularly those with industry-specific requirements.
Our work is driven by the need to effectively manage and process large volumes
of short text documents inherent in specific application domains. By leveraging
domain-specific knowledge and expertise, our approach aims to shape factual
data within these domains, thereby facilitating enhanced utilization and
understanding by end-users. Central to our methodology is the integration of
domain-specific language models with graph-oriented databases, facilitating
seamless processing, analysis, and utilization of textual data within targeted
domains. Our work underscores the transformative potential of the partnership
of domain-specific language models and graph-oriented databases. This
cooperation aims to assist researchers and engineers in metric usage,
mitigation of latency issues, boosting explainability, enhancing debug and
improving overall model performance. Moving forward, we envision our work as a
guide AI engineers, providing valuable insights for the implementation of
domain-specific language models in conjunction with graph-oriented databases,
and additionally provide valuable experience in full-life cycle maintenance of
this kind of products.

æè¦ï¼å¨è³æç¶éçæä»£ï¼ç¹å®é åèªè¨çç®¡çåæç¨å·²æçºåæç¨é åä¸­çééµææ°ï¼å°¤å¶æ¯é£äºå·æç¢æ¥­ç¹å®éæ±çé åãæåçç ç©¶åæ©æ¯ææç®¡çåèçç¹å®æç¨é åä¸­åºæçæµ·éç°¡ç­ææ¬æä»¶ãéééç¨ç¹å®é åçç¥è­åå°æ¥­ç¥è­ï¼æåçåæ³æ¨å¨å½¢å¡éäºé åä¸­çäºå¯¦è³æï¼é²èä¿é²æçµä½¿ç¨èå¢å¼·å©ç¨åçè§£ãæåæ¹æ³çæ ¸å¿æ¯å°ç¹å®é åçèªè¨æ¨¡åèåå½¢å°åè³æåº«æ´åï¼ä¿é²ç®æ¨é åä¸­æå­è³æçç¡ç¸«èçãåæåå©ç¨ãæåçç ç©¶å¼·èª¿äºç¹å®é åèªè¨æ¨¡åèåå½¢å°åè³æåº«åä½çè½åæ½åãéç¨®åä½æ¨å¨åå©ç ç©¶äººå¡åå·¥ç¨å¸«é²è¡ææ¨ä½¿ç¨ãæ¸è¼å»¶é²åé¡ãæåå¯è§£éæ§ãå¢å¼·é¤é¯ï¼ä¸¦æ¹åæ´é«æ¨¡åæè½ãå±ææªä¾ï¼æåé ææåçç ç©¶å°æçºäººå·¥æºæ§å·¥ç¨å¸«çæåï¼æä¾æå¹å¼çè¦è§£ï¼ä¾ä»åå°ç¹å®é åèªè¨æ¨¡åèåå½¢å°åè³æåº«çµåå¯¦ä½ï¼ä¸¦é²ä¸æ­¥æä¾æ­¤é¡ç¢åå¨çå½é±æç¶­è­·çå¯¶è²´ç¶é©ã

##### **GraphRouter: A Graph-based Router for LLM Selections**
2410.03834v1 by Tao Feng, Yanzhen Shen, Jiaxuan You

The rapidly growing number and variety of Large Language Models (LLMs)
present significant challenges in efficiently selecting the appropriate LLM for
a given query, especially considering the trade-offs between performance and
computational cost. Current LLM selection methods often struggle to generalize
across new LLMs and different tasks because of their limited ability to
leverage contextual interactions among tasks, queries, and LLMs, as well as
their dependence on a transductive learning framework. To address these
shortcomings, we introduce a novel inductive graph framework, named as
GraphRouter, which fully utilizes the contextual information among tasks,
queries, and LLMs to enhance the LLM selection process. GraphRouter constructs
a heterogeneous graph comprising task, query, and LLM nodes, with interactions
represented as edges, which efficiently captures the contextual information
between the query's requirements and the LLM's capabilities. Through an
innovative edge prediction mechanism, GraphRouter is able to predict attributes
(the effect and cost of LLM response) of potential edges, allowing for
optimized recommendations that adapt to both existing and newly introduced LLMs
without requiring retraining. Comprehensive experiments across three distinct
effect-cost weight scenarios have shown that GraphRouter substantially
surpasses existing routers, delivering a minimum performance improvement of
12.3%. In addition, it achieves enhanced generalization across new LLMs
settings and supports diverse tasks with at least a 9.5% boost in effect and a
significant reduction in computational demands. This work endeavors to apply a
graph-based approach for the contextual and adaptive selection of LLMs,
offering insights for real-world applications. Our codes for GraphRouter will
soon be released at https://github.com/ulab-uiuc/GraphRouter.

æè¦ï¼<paragraph>å¤§åèªè¨æ¨¡å (LLM) çæ¸éåç¨®é¡å¿«éå¢é·ï¼å¨ææå°éå°ç¹å®æ¥è©¢é¸æé©ç¶ç LLM ææå¸¶ä¾éå¤§çææ°ï¼ç¹å¥æ¯èæ®å°æè½åéç®ææ¬ä¹éçæ¬è¡¡ãç®åç LLM é¸ææ¹æ³éå¸¸é£ä»¥æ¦æ¬å°æ°ç LLM åä¸åçä»»åï¼å çºå®åå¨å©ç¨ä»»åãæ¥è©¢å LLM ä¹éçèçµ¡äºåæ¹é¢çè½åæéï¼èä¸ä¾è³´æ¼è½å°å­¸ç¿æ¶æ§ãçºäºè§£æ±ºéäºç¼ºé»ï¼æåå¼é²äºä¸ååçº GraphRouter çæ°æ­¸ç´åå½¢æ¶æ§ï¼å®ååå©ç¨ä»»åãæ¥è©¢å LLM ä¹éçèçµ¡è³è¨ä¾å¢å¼· LLM é¸ææµç¨ãGraphRouter æ§å»ºäºä¸åç°è³ªåå½¢ï¼åå«ä»»åãæ¥è©¢å LLM ç¯é»ï¼ä¸¦å°äºåè¡¨ç¤ºçºéç·£ï¼ææå°æ·åæ¥è©¢éæ±å LLM è½åä¹éçèçµ¡è³è¨ãééåµæ°çéç·£é æ¸¬æ©å¶ï¼GraphRouter è½å¤ é æ¸¬æ½å¨éç·£çå±¬æ§ï¼LLM åæçææåææ¬ï¼ï¼åè¨±æä½³åå»ºè­°ï¼ä»¥é©æç¾æåæ°æ¨åºç LLMï¼èç¡ééæ°è¨ç·´ãå¨ä¸åä¸åçææææ¬æ¬éæå¢ä¸­é²è¡çå¨é¢å¯¦é©é¡¯ç¤ºï¼GraphRouter æé¡¯è¶è¶ç¾æçè·¯ç±å¨ï¼æè½è³å°æå 12.3%ãæ­¤å¤ï¼å®å¨æ°ç LLM è¨­å®ä¸­å¯¦ç¾äºå¢å¼·çæ¦æ¬æ§ï¼ä¸¦æ¯æ´å¤æ¨£åçä»»åï¼ææè³å°æå 9.5%ï¼ä¸¦å¤§å¹éä½éç®éæ±ãéé å·¥ä½è´åæ¼æç¨åºæ¼åå½¢çæ¹æ³ï¼ä»¥é²è¡ LLM çèçµ¡åé©ææ§é¸æï¼çºçå¯¦ä¸ççæç¨æä¾è¦è§£ãæåç GraphRouter ç¨å¼ç¢¼å°å¾å¿«å¨ https://github.com/ulab-uiuc/GraphRouter ç¼å¸ã</paragraph>

##### **Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing**
2410.03357v1 by Jeongwoo Kang, Maximin Coavoux, CÃ©dric Lopez, Didier Schwab

Cross-lingual AMR parsing is the task of predicting AMR graphs in a target
language when training data is available only in a source language. Due to the
small size of AMR training data and evaluation data, cross-lingual AMR parsing
has only been explored in a small set of languages such as English, Spanish,
German, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022),
who apply meta-learning to tackle cross-lingual syntactic parsing, we
investigate the use of meta-learning for cross-lingual AMR parsing. We evaluate
our models in $k$-shot scenarios (including 0-shot) and assess their
effectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean
and Croatian test sets are developed as part of our work, based on the existing
The Little Prince English AMR corpus, and made publicly available. We
empirically study our method by comparing it to classical joint learning. Our
findings suggest that while the meta-learning model performs slightly better in
0-shot evaluation for certain languages, the performance gain is minimal or
absent when $k$ is higher than 0.

æè¦ï¼è·¨èªè¨ AMR è§£ææ¯ä¸é ä»»åï¼å¨åå¨æºèªè¨ä¸­æä¾è¨ç·´è³ææï¼é æ¸¬ç®æ¨èªè¨ä¸­ç AMR åå½¢ãç±æ¼ AMR è¨ç·´è³æåè©ä¼°è³æçè¦æ¨¡å¾å°ï¼å æ­¤è·¨èªè¨ AMR è§£æåå¨å°æ¸èªè¨ä¸­é²è¡éæ¢ç´¢ï¼ä¾å¦è±èªãè¥¿ç­çèªãå¾·èªãä¸­æåç¾©å¤§å©èªãåå° Langedijk ç­äºº (2022) çåç¼ï¼ä»åæç¨åå­¸ç¿ä¾èçè·¨èªè¨å¥æ³è§£æï¼æåç ç©¶äºä½¿ç¨åå­¸ç¿é²è¡è·¨èªè¨ AMR è§£æãæåå¨ $k$-shot å ´æ¯ï¼åæ¬ 0-shotï¼ä¸­è©ä¼°æåçæ¨¡åï¼ä¸¦è©ä¼°å®åå¨åç¾åè¥¿äºèªãæ³¢æ¯èªãéèªãä¸­æåæ³èªä¸­çæææ§ãå¼å¾æ³¨æçæ¯ï¼éèªååç¾åè¥¿äºèªæ¸¬è©¦éæ¯æ ¹æç¾æçãå°çå­ãè±èª AMR èªæåº«éç¼çï¼ä¸¦å¬éæä¾ãæåééå°æåçæ¨¡åèå³çµ±è¯åå­¸ç¿é²è¡æ¯è¼ï¼å°æåçæ¨¡åé²è¡å¯¦è­ç ç©¶ãæåçç ç©¶çµæè¡¨æï¼éç¶åå­¸ç¿æ¨¡åå¨æäºèªè¨ç 0-shot è©ä¼°ä¸­è¡¨ç¾ç¥å¥½ï¼ä½æ¯ç¶ $k$ é«æ¼ 0 æï¼æè½æåå¾å°ææ²æã

##### **Enriching Ontologies with Disjointness Axioms using Large Language Models**
2410.03235v1 by Elias Crum, Antonio De Santis, Manon Ovide, Jiaxin Pan, Alessia Pisu, Nicolas Lazzari, Sebastian Rudolph

Ontologies often lack explicit disjointness declarations between classes,
despite their usefulness for sophisticated reasoning and consistency checking
in Knowledge Graphs. In this study, we explore the potential of Large Language
Models (LLMs) to enrich ontologies by identifying and asserting class
disjointness axioms. Our approach aims at leveraging the implicit knowledge
embedded in LLMs, using prompt engineering to elicit this knowledge for
classifying ontological disjointness. We validate our methodology on the
DBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,
when guided by effective prompt strategies, can reliably identify disjoint
class relationships, thus streamlining the process of ontology completion
without extensive manual input. For comprehensive disjointness enrichment, we
propose a process that takes logical relationships between disjointness and
subclass statements into account in order to maintain satisfiability and reduce
the number of calls to the LLM. This work provides a foundation for future
applications of LLMs in automated ontology enhancement and offers insights into
optimizing LLM performance through strategic prompt design. Our code is
publicly available on GitHub at https://github.com/n28div/llm-disjointness.

æè¦ï¼æ¬ä½è«éå¸¸ç¼ºä¹é¡å¥ä¹éæç¢ºçä¸ç¸äº¤è²æï¼åç®¡å®åå°æ¼ç¥è­åè­ä¸­çç²¾å¯æ¨çåä¸è´æ§æª¢æ¥å¾æç¨ãå¨æ¬ç ç©¶ä¸­ï¼æåæ¢è¨äºå¤§åèªè¨æ¨¡å (LLM) çæ½åï¼ééè­å¥åæ·è¨é¡å¥ä¸ç¸äº¤å¬çä¾è±å¯æ¬ä½è«ãæåçåæ³æ¨å¨å©ç¨åµå¥å¨ LLM ä¸­çé±å¼ç¥è­ï¼å©ç¨æç¤ºå·¥ç¨ä¾å¼åºéç¨®ç¥è­ä»¥åé¡æ¬ä½è«ä¸ç¸äº¤ãæåå¨ DBpedia æ¬ä½è«ä¸é©è­äºæåçæ¹æ³ï¼éé»éæ³¨éæº LLMãæåçç ç©¶çµæè¡¨æï¼LLM å¨æææç¤ºç­ç¥çæå°ä¸ï¼å¯ä»¥å¯é å°è­å¥ä¸ç¸äº¤é¡å¥éä¿ï¼å¾èç°¡åæ¬ä½è«å®æéç¨ï¼èç¡éå¤§éæåè¼¸å¥ãå°æ¼å¨é¢çä¸ç¸äº¤è±å¯ï¼æåæåºäºä¸åéç¨ï¼è©²éç¨èæ®äºä¸ç¸äº¤åå­é¡å¥é³è¿°ä¹éçéè¼¯éä¿ï¼ä»¥ç¶­æå¯æ»¿è¶³æ§ä¸¦æ¸å°å° LLM çèª¿ç¨æ¬¡æ¸ãéé å·¥ä½çº LLM å¨èªåæ¬ä½è«å¢å¼·ä¸­çæªä¾æç¨å¥ å®äºåºç¤ï¼ä¸¦æä¾äºééç­ç¥æç¤ºè¨­è¨åªå LLM æ§è½çè¦è§£ãæåçä»£ç¢¼å¨ GitHub ä¸å¬éï¼ç¶²åçº https://github.com/n28div/llm-disjointnessã

##### **How Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension**
2410.05298v1 by Xinnan Dai, Haohao Qu, Yifen Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang Tang, Caihua Shan

Benchmarking the capabilities and limitations of large language models (LLMs)
in graph-related tasks is becoming an increasingly popular and crucial area of
research. Recent studies have shown that LLMs exhibit a preliminary ability to
understand graph structures and node features. However, the potential of LLMs
in graph pattern mining remains largely unexplored. This is a key component in
fields such as computational chemistry, biology, and social network analysis.
To bridge this gap, this work introduces a comprehensive benchmark to assess
LLMs' capabilities in graph pattern tasks. We have developed a benchmark that
evaluates whether LLMs can understand graph patterns based on either
terminological or topological descriptions. Additionally, our benchmark tests
the LLMs' capacity to autonomously discover graph patterns from data. The
benchmark encompasses both synthetic and real datasets, and a variety of
models, with a total of 11 tasks and 7 models. Our experimental framework is
designed for easy expansion to accommodate new models and datasets. Our
findings reveal that: (1) LLMs have preliminary abilities to understand graph
patterns, with O1-mini outperforming in the majority of tasks; (2) Formatting
input data to align with the knowledge acquired during pretraining can enhance
performance; (3) The strategies employed by LLMs may differ from those used in
conventional algorithms.

æè¦ï¼è©éå¤§åèªè¨æ¨¡å (LLM) å¨åå½¢ç¸éä»»åä¸­çè½ååéå¶ï¼æ­£æçºä¸åè¶ä¾è¶åæ­¡è¿ä¸è³ééè¦çç ç©¶é åãæè¿çç ç©¶è¡¨æï¼LLM å±ç¾åºåæ­¥çè§£åå½¢çµæ§åç¯é»ç¹å¾µçè½åãç¶èï¼LLM å¨åå½¢æ¨¡å¼ææä¸­çæ½åä»æªè¢«å»£æ³æ¢ç´¢ãéæ¯è¨ç®åå­¸ãçç©å­¸åç¤¾äº¤ç¶²è·¯åæç­é åçééµçµæé¨åãçºäºå½åéåå·®è·ï¼éé å·¥ä½æåºäºä¸åå¨é¢çåºæºä¾è©ä¼° LLM å¨åå½¢æ¨¡å¼ä»»åä¸­çè½åãæåéç¼äºä¸ååºæºï¼ç¨ä¾è©ä¼° LLM è½å¦æ ¹æè¡èªæææ²æè¿°ä¾çè§£åå½¢æ¨¡å¼ãæ­¤å¤ï¼æåçåºæºæ¸¬è©¦ LLM å¾è³æä¸­èªä¸»ç¼ç¾åå½¢æ¨¡å¼çè½åãè©²åºæºæ¶µèäºåæåçå¯¦è³æéï¼ä»¥ååç¨®æ¨¡åï¼ç¸½å±æ 11 é ä»»åå 7 åæ¨¡åãæåçå¯¦é©æ¶æ§è¨­è¨çºææ¼æ´åï¼ä»¥å®¹ç´æ°çæ¨¡ååè³æéãæåçç ç©¶çµæé¡¯ç¤ºï¼(1) LLM å·æåæ­¥çè§£åå½¢æ¨¡å¼çè½åï¼å¶ä¸­ O1-mini å¨å¤§å¤æ¸ä»»åä¸­è¡¨ç¾åªç°ï¼(2) å°è¼¸å¥è³ææ ¼å¼åçºèé è¨ç·´æéç¿å¾çç¥è­ä¸è´ï¼å¯ä»¥å¢å¼·æè½ï¼(3) LLM ä½¿ç¨çç­ç¥å¯è½èå³çµ±æ¼ç®æ³ä¸­ä½¿ç¨çç­ç¥ä¸åã

##### **LLMCO2: Advancing Accurate Carbon Footprint Prediction for LLM Inferences**
2410.02950v1 by Zhenxiao Fu, Fan Chen, Shan Zhou, Haitong Li, Lei Jiang

Throughout its lifecycle, a large language model (LLM) generates a
substantially larger carbon footprint during inference than training. LLM
inference requests vary in batch size, prompt length, and token generation
number, while cloud providers employ different GPU types and quantities to meet
diverse service-level objectives for accuracy and latency. It is crucial for
both users and cloud providers to have a tool that quickly and accurately
estimates the carbon impact of LLM inferences based on a combination of
inference request and hardware configurations before execution. Estimating the
carbon footprint of LLM inferences is more complex than training due to lower
and highly variable model FLOPS utilization, rendering previous equation-based
models inaccurate. Additionally, existing machine learning (ML) prediction
methods either lack accuracy or demand extensive training data, as they
inadequately handle the distinct prefill and decode phases, overlook
hardware-specific features, and inefficiently sample uncommon inference
configurations. We introduce \coo, a graph neural network (GNN)-based model
that greatly improves the accuracy of LLM inference carbon footprint
predictions compared to previous methods.

æè¦ï¼å¨æ´åçå½é±æä¸­ï¼å¤§åèªè¨æ¨¡å (LLM) å¨æ¨çæéç¢ççç¢³è¶³è·¡é å¤§æ¼è¨ç·´æéãLLM æ¨çè«æ±å¨æ¹æ¬¡å¤§å°ãæç¤ºé·åº¦åæ¬æçææ¸éæ¹é¢ææä¸åï¼èé²ç«¯ä¾æåæ¡ç¨ä¸åç GPU é¡ååæ¸éä¾æ»¿è¶³æºç¢ºæ§åå»¶é²çåç¨®æåå±¤ç´ç®æ¨ãå°æ¼ä½¿ç¨èåé²ç«¯ä¾æåä¾èªªï¼å¨å·è¡åæ ¹ææ¨çè«æ±åç¡¬é«éç½®çµåå¿«éä¸æºç¢ºå°ä¼°è¨ LLM æ¨ççç¢³å½±é¿è³ééè¦ãä¼°è¨ LLM æ¨ççç¢³è¶³è·¡æ¯è¨ç·´æ´è¤éï¼å çºæ¨¡å FLOPS å©ç¨çè¼ä½ä¸è®åå¾å¤§ï¼å°è´ååçåºæ¼æ¹ç¨å¼çæ¨¡åä¸æºç¢ºãæ­¤å¤ï¼ç¾æçæ©å¨å­¸ç¿ (ML) é æ¸¬æ¹æ³è¦ä¹ç¼ºä¹æºç¢ºæ§ï¼è¦ä¹éè¦å¤§éçè¨ç·´è³æï¼å çºå®åç¡æ³ååèçä¸åçé å¡«ååè§£ç¢¼éæ®µï¼å¿½ç¥ç¡¬é«ç¹å®çåè½ï¼ä¸¦ä¸ä½æçå°åæ¨£ä¸å¸¸è¦çæ¨çéç½®ãæåå¼å¥äº \cooï¼éæ¯ä¸ååºæ¼åç¥ç¶ç¶²è·¯ (GNN) çæ¨¡åï¼èååçæ¨¡åç¸æ¯ï¼å®å¤§å¤§æé«äº LLM æ¨çç¢³è¶³è·¡é æ¸¬çæºç¢ºæ§ã

##### **Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization**
2410.02721v1 by Ryan C. Barron, Ves Grantcharov, Selma Wanna, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim Ã. Rasmussen, Cynthia Matuszek, Boian S. Alexandrov

Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ç¶éå¤§éèªæåº«çé åè¨ç·´ï¼å¨è¨±å¤ä¸è¬èªç¶èªè¨èç (NLP) ä»»åä¸­è¡¨ç¾åºè²ï¼ä¾å¦åé¡è§£ç­ (QA)ãåç®¡å®åå·æåé²çèªè¨è½åï¼ä½ LLM å¨ç¹å®é ååç¥è­å¯éåä»»åæ¹é¢æåºç¾å¹»è¦ºãç¥è­æ·å±¤åç¼ºä¹ç¥è­æ­¸å ãæ­¤å¤ï¼å¾®èª¿ LLM çå§å¨ç¥è­ä»¥é©æé«åº¦ç¹å®é åæ¯ä¸åæè²´ä¸èæçéç¨ãæª¢ç´¢å¢å¼·çæ (RAG) æµç¨æè¿å·²æçºä¸ç¨®åªå LLM åæçæ¹æ³ï¼æ¹æ³æ¯å°å®ååç§é åç¢ºå®çæ¬ä½ãç ç©¶è¡¨æï¼å°ç¥è­åè­ (KG) æ¬ä½ç¨æ¼ RAG å¯ééèæ®ä»¥çµæ§åæ¹å¼ä¿çè³è¨çç¸éå­åï¼ä¾æé« QA çæºç¢ºæ§ãå¨æ¬æä¸­ï¼æåä»ç´¹äº SMART-SLICï¼éæ¯ä¸åé«åº¦ç¹å®é åç LLM æ¡æ¶ï¼å®å° RAG è KG åä¸åå²å­äºå¯¦ç¹å®é åè³è¨çåéå²å­ (VS) æ´åå¨ä¸èµ·ãéè¦çæ¯ï¼çºäºé¿å KG ä¸­çå¹»è¦ºï¼æåå¨ä¸ä½¿ç¨ LLM çææ³ä¸å»ºç«äºéäºé«åº¦ç¹å®é åç KG å VSï¼èæ¯éé NLPãè³ææ¢ååå·æèªåæ¨¡åé¸æçéè² å¼µéåè§£ãå°æåç RAG èç¹å®é åéå°ï¼(i) KGï¼åå«çµæ§åè³è¨ï¼ï¼å (ii) VSï¼åå«éçµæ§åè³è¨ï¼è½å¤ éç¼ç¹å®é åçèå¤©æ©å¨äººï¼éäºèå¤©æ©å¨äººææ­¸å æ¼è³è¨ä¾æºãæ¸è¼å¹»è¦ºãæ¸å°å¾®èª¿çéè¦ï¼ä¸¦å¨é«åº¦ç¹å®é åçåé¡è§£ç­ä»»åä¸­è¡¨ç¾åºè²ãæåå° SMART-SLIC èæèéæç¤ºä»£çéå°ãè©²æ¡æ¶è¢«è¨­è¨æå¯æ¦æ¬ä»¥é©æä»»ä½ç¹å®æå°æ¥­é åãå¨æ¬æä¸­ï¼æåå¨æ¡æè»é«åæåç°å¸¸åµæ¸¬çç§å­¸åºçç©èªæåº«ä¸å±ç¤ºäºæåæ¡æ¶çåé¡è§£ç­è½åã

##### **A Schema-aware Logic Reformulation for Graph Reachability**
2410.02533v1 by Davide Di Pierro, Stefano Ferilli

Graph reachability is the task of understanding whether two distinct points
in a graph are interconnected by arcs to which in general a semantic is
attached. Reachability has plenty of applications, ranging from motion planning
to routing. Improving reachability requires structural knowledge of relations
so as to avoid the complexity of traditional depth-first and breadth-first
strategies, implemented in logic languages. In some contexts, graphs are
enriched with their schema definitions establishing domain and range for every
arc. The introduction of a schema-aware formalization for guiding the search
may result in a sensitive improvement by cutting out unuseful paths and
prioritising those that, in principle, reach the target earlier. In this work,
we propose a strategy to automatically exclude and sort certain graph paths by
exploiting the higher-level conceptualization of instances. The aim is to
obtain a new first-order logic reformulation of the graph reachability
scenario, capable of improving the traditional algorithms in terms of time,
space requirements, and number of backtracks. The experiments exhibit the
expected advantages of the approach in reducing the number of backtracks during
the search strategy, resulting in saving time and space as well.

æè¦ï¼åå½¢å¯éæ§æ¯äºè§£åå½¢ä¸­å©åä¸åé»æ¯å¦ç±å¼§ç·ç¸äºé£æ¥çä»»åï¼éäºå¼§ç·éå¸¸éå¸¶èªç¾©ãå¯éæ§æå¾å¤æç¨ï¼å¾éåè¦åå°è·¯ç±ãæé«å¯éæ§éè¦çµæ§éä¿ç¥è­ï¼ä»¥é¿åéè¼¯èªè¨ä¸­å¯¦ç¾çå³çµ±æ·±åº¦åªååå»£åº¦åªåç­ç¥çè¤éæ§ãå¨æäºææ³ä¸ï¼åå½¢æééå¶æ¶æ§å®ç¾©å¾å°è±å¯ï¼çºæ¯åå¼§ç·å»ºç«ååç¯åãå¼å¥æ¶æ§æç¥å½¢å¼åä»¥æå°æå°å¯è½æééåæ·ç¡ç¨çè·¯å¾ååªåèæ®ååä¸è¼æ©å°éç®æ¨çè·¯å¾èç¢çé¡¯èçæ¹é²ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸ç¨®ç­ç¥ï¼ééå©ç¨å¯¦ä¾çé«éæ¦å¿µåä¾èªåæé¤åæåºæäºåå½¢è·¯å¾ãç®çæ¯ç²å¾åå½¢å¯éæ§å ´æ¯çæ°ä¸ééè¼¯éæ°è¡¨è¿°ï¼è½å¤ å¨æéãç©ºééæ±ååæº¯æ¬¡æ¸æ¹é¢æ¹é²å³çµ±æ¼ç®æ³ãå¯¦é©å±ç¤ºäºè©²æ¹æ³å¨æ¸å°æå°ç­ç¥æéåæº¯æ¬¡æ¸æ¹é¢çé æåªé»ï¼å¾èç¯çäºæéåç©ºéã

##### **Language Models are Graph Learners**
2410.02296v1 by Zhe Xu, Kaveh Hassani, Si Zhang, Hanqing Zeng, Michihiro Yasunaga, Limei Wang, Dongqi Fu, Ning Yao, Bo Long, Hanghang Tong

Language Models (LMs) are increasingly challenging the dominance of
domain-specific models, including Graph Neural Networks (GNNs) and Graph
Transformers (GTs), in graph learning tasks. Following this trend, we propose a
novel approach that empowers off-the-shelf LMs to achieve performance
comparable to state-of-the-art GNNs on node classification tasks, without
requiring any architectural modification. By preserving the LM's original
architecture, our approach retains a key benefit of LM instruction tuning: the
ability to jointly train on diverse datasets, fostering greater flexibility and
efficiency. To achieve this, we introduce two key augmentation strategies: (1)
Enriching LMs' input using topological and semantic retrieval methods, which
provide richer contextual information, and (2) guiding the LMs' classification
process through a lightweight GNN classifier that effectively prunes class
candidates. Our experiments on real-world datasets show that backbone Flan-T5
models equipped with these augmentation strategies outperform state-of-the-art
text-output node classifiers and are comparable to top-performing vector-output
node classifiers. By bridging the gap between specialized task-specific node
classifiers and general LMs, this work paves the way for more versatile and
widely applicable graph learning models. We will open-source the code upon
publication.

æè¦ï¼èªè¨æ¨¡åï¼LMï¼æ­£æ¥çææ°ç¹å®é åæ¨¡åå¨åå½¢å­¸ç¿ä»»åä¸­çä¸»å°å°ä½ï¼åæ¬åå½¢ç¥ç¶ç¶²è·¯ï¼GNNï¼ååå½¢è½æå¨ï¼GTï¼ãéµå¾ªæ­¤è¶¨å¢ï¼æåæåºäºä¸ç¨®åµæ°æ¹æ³ï¼ä½¿ç¾æç LM è½å¤ å¨ç¯é»åé¡ä»»åä¸­å¯¦ç¾èæåé²ç GNN ç¸ç¶çæè½ï¼èç¡éä»»ä½æ¶æ§ä¿®æ¹ãééä¿ç LM çåå§æ¶æ§ï¼æåçåæ³ä¿çäº LM æä»¤èª¿æ´çä¸é ä¸»è¦åªé»ï¼è½å¤ å¨ä¸åçè³æéä¸é²è¡è¯åè¨ç·´ï¼ä¿é²æ´å¤§çéæ´»æ§åæçãçºæ­¤ï¼æåå¼å¥äºå©åä¸»è¦çæ´åç­ç¥ï¼(1) ä½¿ç¨ææ²åèªç¾©æª¢ç´¢æ¹æ³è±å¯ LM çè¼¸å¥ï¼æä¾æ´è±å¯çä¸ä¸æè³è¨ï¼ä»¥å (2) ééè¼éç´ GNN åé¡å¨å¼å° LM çåé¡éç¨ï¼ææå°ä¿®åªé¡å¥åé¸ãæåå¨çå¯¦ä¸çè³æéä¸çå¯¦é©è¡¨æï¼éåéäºæ´åç­ç¥çä¸»å¹¹ Flan-T5 æ¨¡ååªæ¼æåé²çæå­è¼¸åºç¯é»åé¡å¨ï¼ä¸¦ä¸èæè½æä½³çåéè¼¸åºç¯é»åé¡å¨ç¸ç¶ãééç¸®å°å°éçç¹å®ä»»åç¯é»åé¡å¨åä¸è¬ LM ä¹éçå·®è·ï¼éé å·¥ä½çºæ´å¤ååä¸å»£æ³é©ç¨çåå½¢å­¸ç¿æ¨¡åéªå¹³äºéè·¯ãæåå°å¨åºçå¾éæºç¨å¼ç¢¼ã

##### **GraphIC: A Graph-Based In-Context Example Retrieval Model for Multi-Step Reasoning**
2410.02203v1 by Jiale Fu, Yaqing Wang, Simeng Han, Jiaming Fan, Chen Si, Xu Yang

In-context learning (ICL) enables large language models (LLMs) to generalize
to new tasks by incorporating a few in-context examples (ICEs) directly in the
input, without updating parameters. However, the effectiveness of ICL heavily
relies on the selection of ICEs, and conventional text-based embedding methods
are often inadequate for tasks that require multi-step reasoning, such as
mathematical and logical problem solving. This is due to the bias introduced by
shallow semantic similarities that fail to capture the deeper reasoning
structures required for these tasks. We present GraphIC, a novel approach that
leverages graph-based representations of reasoning processes, coupled with
Bayesian Networks (BNs) to select ICEs. Graph structures inherently filter out
shallow semantics while preserving the core reasoning structure. Importantly,
BNs capture the dependency of a node's attributes on its parent nodes, closely
mirroring the hierarchical nature of human cognition-where each thought is
shaped by preceding ones. This makes BNs particularly well-suited for
multi-step reasoning tasks, aligning the process more closely with human-like
reasoning. Extensive experiments across three types of reasoning tasks
(mathematical reasoning, code generation, and logical reasoning) demonstrate
that GraphIC outperforms both training-free and training-based models in
selecting ICEs, excelling in terms of both effectiveness and efficiency. We
show that GraphIC enhances ICL's performance and interoperability,
significantly advancing ICE selection for multi-step reasoning tasks.

æè¦ï¼<paragraph>æå¢å­¸ç¿ (ICL) è®å¤§åèªè¨æ¨¡å (LLM) è½å¤ ééç´æ¥å¨è¼¸å¥ä¸­å å¥å°æ¸æå¢ç¯ä¾ (ICE)ï¼èç¡éæ´æ°åæ¸ï¼ä¾æ¦åå°æ°çä»»åä¸­ãç¶èï¼ICL çæææ§æ¥µåº¦ä»°è³´ ICE çé¸æï¼èå³çµ±çåºæ¼æå­çåµå¥æ¹æ³éå¸¸ä¸è¶³ä»¥æä»éè¦å¤æ­¥é©æ¨ççä»»åï¼ä¾å¦æ¸å­¸åéè¼¯åé¡è§£æ±ºãéæ¯ç±æ¼æ·ºå±¤èªæç¸ä¼¼æ§å¸¶ä¾çåå·®ï¼èéç¨®åå·®ç¡æ³ææéäºä»»åæéçæ´æ·±å¥æ¨ççµæ§ãæåæåº GraphICï¼éæ¯ä¸ç¨®åµæ°çæ¹æ³ï¼å®å©ç¨æ¨çéç¨çåå½¢åè¡¨å¾µï¼çµåè²æ°ç¶²è·¯ (BN) ä¾é¸æ ICEãåå½¢çµæ§æåºæå°æ¿¾é¤æ·ºå±¤èªæï¼åæä¿çæ ¸å¿æ¨ççµæ§ãéè¦çæ¯ï¼BN ææç¯é»å±¬æ§å°å¶ç¶ç¯é»çä¾è³´æ§ï¼ç·å¯åæ äººé¡èªç¥çéå±¤æ§è³ªï¼å¶ä¸­æ¯åæ³æ³é½æ¯ç±åä¸åæ³æ³æå½¢å¡ãéä½¿å¾ BN ç¹å¥é©åå¤æ­¥é©æ¨çä»»åï¼è®æµç¨æ´è²¼è¿é¡ä¼¼äººé¡çæ¨çãæ©«è·¨ä¸ç¨®é¡åçæ¨çä»»åï¼æ¸å­¸æ¨çãç¨å¼ç¢¼ç¢çåéè¼¯æ¨çï¼çå¤§éå¯¦é©è­æï¼GraphIC å¨é¸æ ICE æåªæ¼ç¡è¨ç·´ååºæ¼è¨ç·´çæ¨¡åï¼å¨æææ§åæçæ¹é¢é½è¡¨ç¾åºè²ãæåå±ç¤ºäº GraphIC å¢å¼·äº ICL çæè½åäºæä½æ§ï¼é¡¯èå°æ¨é²äºå¤æ­¥é©æ¨çä»»åç ICE é¸æã</paragraph>

##### **G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models**
2410.02198v1 by Zhaoning Yu, Xiangyang Xu, Hongyang Gao

We introduce G2T-LLM, a novel approach for molecule generation that uses
graph-to-tree text encoding to transform graph-based molecular structures into
a hierarchical text format optimized for large language models (LLMs). This
encoding converts complex molecular graphs into tree-structured formats, such
as JSON and XML, which LLMs are particularly adept at processing due to their
extensive pre-training on these types of data. By leveraging the flexibility of
LLMs, our approach allows for intuitive interaction using natural language
prompts, providing a more accessible interface for molecular design. Through
supervised fine-tuning, G2T-LLM generates valid and coherent chemical
structures, addressing common challenges like invalid outputs seen in
traditional graph-based methods. While LLMs are computationally intensive, they
offer superior generalization and adaptability, enabling the generation of
diverse molecular structures with minimal task-specific customization. The
proposed approach achieved comparable performances with state-of-the-art
methods on various benchmark molecular generation datasets, demonstrating its
potential as a flexible and innovative tool for AI-driven molecular design.

æè¦ï¼æåä»ç´¹ G2T-LLMï¼ä¸ç¨®éå°åå­çæçåµæ°æ¹æ³ï¼å®ä½¿ç¨åå½¢è½æ¨¹çæå­ç·¨ç¢¼ï¼å°åºæ¼åå½¢çåå­çµæ§è½æçºéå±¤å¼æå­æ ¼å¼ï¼éå°å¤§åèªè¨æ¨¡å (LLM) é²è¡æä½³åãæ­¤ç·¨ç¢¼æå°è¤éçåå­åå½¢è½æçºæ¨¹ççµæ§æ ¼å¼ï¼ä¾å¦ JSON å XMLï¼ç±æ¼ LLM å¨éäºé¡åè³æçå»£æ³é åè¨ç·´ï¼å æ­¤ç¹å¥æé·èçéäºæ ¼å¼ãééå©ç¨ LLM çéæ´»æ§ï¼æåçåæ³åè¨±ä½¿ç¨èªç¶èªè¨æç¤ºé²è¡ç´è¦ºå¼äºåï¼æä¾ä¸åæ´å®¹æå­åçåå­è¨­è¨ä»é¢ãééç£ç£å¾®èª¿ï¼G2T-LLM æç¢çææä¸é£è²«çåå­¸çµæ§ï¼è§£æ±ºå³çµ±åºæ¼åå½¢æ¹æ³ä¸­å¸¸è¦çç¡æè¼¸åºç­ææ°ãéç¶ LLM å¨è¨ç®ä¸å¾å¯éï¼ä½å®åæä¾åªç°çæ¦æ¬æ§åé©ææ§ï¼è½å¤ ç¢çå¤æ¨£åçåå­çµæ§ï¼ä¸ä»»åç¹å®èªè¨åéæ±æ¥µä½ãææåºçæ¹æ³å¨åç¨®åºæºåå­çæè³æéä¸éå°äºèæåé²æ¹æ³ç¸ç¶çæè½ï¼è­æäºå¶ä½çº AI é©ååå­è¨­è¨çéæ´»ä¸åµæ°çå·¥å·çæ½åã

##### **FLAG: Financial Long Document Classification via AMR-based GNN**
2410.02024v2 by Bolun "Namir" Xia, Mohammed J. Zaki, Aparna Gupta

The advent of large language models (LLMs) has initiated much research into
their various financial applications. However, in applying LLMs on long
documents, semantic relations are not explicitly incorporated, and a full or
arbitrarily sparse attention operation is employed. In recent years, progress
has been made in Abstract Meaning Representation (AMR), which is a graph-based
representation of text to preserve its semantic relations. Since AMR can
represent semantic relationships at a deeper level, it can be beneficially
utilized by graph neural networks (GNNs) for constructing effective
document-level graph representations built upon LLM embeddings to predict
target metrics in the financial domain. We propose FLAG: Financial Long
document classification via AMR-based GNN, an AMR graph based framework to
generate document-level embeddings for long financial document classification.
We construct document-level graphs from sentence-level AMR graphs, endow them
with specialized LLM word embeddings in the financial domain, apply a deep
learning mechanism that utilizes a GNN, and examine the efficacy of our
AMR-based approach in predicting labeled target data from long financial
documents. Extensive experiments are conducted on a dataset of quarterly
earnings calls transcripts of companies in various sectors of the economy, as
well as on a corpus of more recent earnings calls of companies in the S&P 1500
Composite Index. We find that our AMR-based approach outperforms fine-tuning
LLMs directly on text in predicting stock price movement trends at different
time horizons in both datasets. Our work also outperforms previous work
utilizing document graphs and GNNs for text classification.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çåºç¾ï¼éåäºå°å¶åç¨®è²¡åæç¨çå¤§éç ç©¶ãç¶èï¼å¨é·ç¯æä»¶ä¸­æç¨ LLM æï¼èªç¾©éä¿ä¸¦æªè¢«æç¢ºç´å¥ï¼ä¸¦ä¸æ¡ç¨äºå®å¨æä»»æç¨ççæ³¨ææä½ãè¿å¹´ä¾ï¼æ½è±¡æç¾©è¡¨ç¤º (AMR) å·²åå¾é²å±ï¼éæ¯ä¸ç¨®åºæ¼åè¡¨çæå­è¡¨ç¤ºå½¢å¼ï¼ç¨æ¼ä¿çå¶èªç¾©éä¿ãç±æ¼ AMR è½å¨æ´æ·±å±¤æ¬¡è¡¨ç¤ºèªç¾©éä¿ï¼å æ­¤å¯ä»¥ç±åå½¢ç¥ç¶ç¶²è·¯ (GNN) ææå°å©ç¨ï¼ç¨æ¼å»ºæ§å»ºç«å¨ LLM åµå¥ä¸çæææä»¶ç´åå½¢è¡¨ç¤ºï¼ä»¥é æ¸¬è²¡åé åçç®æ¨ææ¨ãæåæåº FLAGï¼ééåºæ¼ AMR ç GNN é²è¡è²¡åé·æä»¶åé¡ï¼éæ¯ä¸ååºæ¼ AMR åè¡¨çæ¶æ§ï¼ç¨æ¼çºé·ç¯è²¡åæä»¶åé¡ç¢çæä»¶ç´åµå¥ãæåå¾å¥å­ç´ AMR åè¡¨å»ºæ§æä»¶ç´åè¡¨ï¼å¨è²¡åé åè³¦äºå®åå°æ¥­ç LLM å­è©åµå¥ï¼æç¨å©ç¨ GNN çæ·±åº¦å­¸ç¿æ©å¶ï¼ä¸¦æª¢é©æååºæ¼ AMR çæ¹æ³å¨é æ¸¬é·ç¯è²¡åæä»¶æ¨è¨ç®æ¨è³ææ¹é¢çæè½ãæåå¨ä¸åè³æéä¸é²è¡äºå»£æ³çå¯¦é©ï¼è©²è³æéåå«ååç¶æ¿é¨éå¬å¸çæ¯å­£æ¶çé»è©±æè­°è¨éï¼ä»¥åæ¨æ® 1500 ç¶åææ¸å¬å¸æè¿æ¶çé»è©±æè­°çèªæåº«ãæåç¼ç¾ï¼æåçåºæ¼ AMR çæ¹æ³å¨é æ¸¬å©åè³æéä¸­çä¸åæéç¯åå§çè¡å¹è®åè¶¨å¢æ¹é¢ï¼åªæ¼ç´æ¥å°æå­é²è¡å¾®èª¿ç LLMãæåçç ç©¶ä¹åªæ¼ååå©ç¨æä»¶åè¡¨å GNN é²è¡æå­åé¡çç ç©¶ã

##### **Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in Graph Tasks**
2410.01985v1 by Hamed Firooz, Maziar Sanjabi, Wenlong Jiang, Xiaoling Zhai

Despite significant advancements, Large Language Models (LLMs) exhibit blind
spots that impair their ability to retrieve and process relevant contextual
data effectively. We demonstrate that LLM performance in graph tasks with
complexities beyond the "needle-in-a-haystack" scenario-where solving the
problem requires cross-referencing and reasoning across multiple subproblems
jointly-is influenced by the proximity of relevant information within the
context, a phenomenon we term "lost-in-distance". We examine two fundamental
graph tasks: identifying common connections between two nodes and assessing
similarity among three nodes, and show that the model's performance in these
tasks significantly depends on the relative positioning of common edges. We
evaluate three publicly available LLMs-Llama-3-8B, Llama-3-70B, and GPT-4-using
various graph encoding techniques that represent graph structures for LLM
input. We propose a formulation for the lost-in-distance phenomenon and
demonstrate that lost-in-distance and lost-in-the middle phenomenas occur
independently. Results indicate that model accuracy can decline by up to 6x as
the distance between node connections increases, independent of graph encoding
and model size.

æè¦ï¼åç®¡æé¡¯èçé²å±ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼ä»å­å¨ç²é»ï¼ææå®³å®åæææ·ååèçç¸éèçµ¡è³æçè½åãæåè­æäº LLM å¨åå½¢ä»»åä¸­çè¡¨ç¾ï¼å¶è¤éåº¦è¶åºäºãå¤§æµ·æéãæå¢ï¼å¶ä¸­è§£æ±ºåé¡éè¦è·¨å¤åå­åé¡é²è¡äº¤ååç§åæ¨çï¼ä¸¦åå°èçµ¡ä¸­ç¸éè³è¨çæ¥è¿ç¨åº¦å½±é¿ï¼æåå°æ­¤ç¾è±¡ç¨±çºãå¤±ä¹è·é¢ããæåæª¢é©äºå©ååºæ¬çåå½¢ä»»åï¼è­å¥å©åç¯é»ä¹éçå±åé£æ¥ï¼ä»¥åè©ä¼°ä¸åç¯é»ä¹éçç¸ä¼¼æ§ï¼ä¸¦é¡¯ç¤ºæ¨¡åå¨éäºä»»åä¸­çè¡¨ç¾é¡¯èå°åæ±ºæ¼å±åéç·£çç¸å°ä½ç½®ãæåè©ä¼°äºä¸åå¬éå¯ç¨ç LLMï¼åå¥çº Llama-3-8BãLlama-3-70B å GPT-4ï¼ä½¿ç¨åç¨®åå½¢ç·¨ç¢¼æè¡ï¼éäºæè¡è¡¨ç¤º LLM è¼¸å¥çåå½¢çµæ§ãæåæåºäºå¤±ä¹è·é¢ç¾è±¡çå¬å¼ï¼ä¸¦è­æäºå¤±ä¹è·é¢åå¤±ä¹æ¼ä¸­éç¾è±¡æç¨ç«ç¼çãçµæè¡¨æï¼é¨èç¯é»é£æ¥ä¹éçè·é¢å¢å ï¼æ¨¡åæºç¢ºåº¦æå¤å¯è½æä¸é 6 åï¼èåå½¢ç·¨ç¢¼åæ¨¡åå¤§å°ç¡éã

##### **LLM+KG@VLDB'24 Workshop Summary**
2410.01978v1 by Arijit Khan, Tianxing Wu, Xi Chen

The unification of large language models (LLMs) and knowledge graphs (KGs)
has emerged as a hot topic. At the LLM+KG'24 workshop, held in conjunction with
VLDB 2024 in Guangzhou, China, one of the key themes explored was important
data management challenges and opportunities due to the effective interaction
between LLMs and KGs. This report outlines the major directions and approaches
presented by various speakers during the LLM+KG'24 workshop.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) åç¥è­åè­ (KG) ççµ±ä¸å·²æçºç±éè©±é¡ãå¨ä¸­åå»£å·èè¡ç 2024 å¹´ VLDB æè­°æéèè¾¦ç LLM+KG'24 ç è¨æä¸ï¼æ¢ç´¢çä¸åééµä¸»é¡æ¯ LLM å KG ä¹éçææäºåæå¸¶ä¾çéè¦çæ¸æç®¡çææ°åæ©éãæ¬å ±åæ¦è¿°äº LLM+KG'24 ç è¨ææéåæ¼è¬èæåºçä¸»è¦æ¹ååæ¹æ³ã

##### **Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering**
2410.01660v1 by Klaus-Rudolf Kladny, Bernhard SchÃ¶lkopf, Michael Muehlebach

Generative models lack rigorous statistical guarantees for their outputs and
are therefore unreliable in safety-critical applications. In this work, we
propose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a
sequential conformal prediction method producing prediction sets that satisfy a
rigorous statistical guarantee called conformal admissibility control. This
guarantee states that with high probability, the prediction sets contain at
least one admissible (or valid) example. To this end, our method first samples
an initial set of i.i.d. examples from a black box generative model. Then, this
set is iteratively pruned via so-called greedy filters. As a consequence of the
iterative generation procedure, admissibility of the final prediction set
factorizes as a Markov chain. This factorization is crucial, because it allows
to control each factor separately, using conformal prediction. In comparison to
prior work, our method demonstrates a large reduction in the number of
admissibility evaluations during calibration. This reduction is important in
safety-critical applications, where these evaluations must be conducted
manually by domain experts and are therefore costly and time consuming. We
highlight the advantages of our method in terms of admissibility evaluations
and cardinality of the prediction sets through experiments in natural language
generation and molecular graph extension tasks.

æè¦ï¼çææ¨¡åç¼ºä¹å°å¶è¼¸åºé²è¡å´æ ¼ççµ±è¨ä¿è­ï¼å æ­¤å¨å®å¨ééµæç¨ä¸­ä¸å¯é ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºçææ¨¡åçé åºå±å½¢é æ¸¬ (SCOPE-Gen)ï¼éæ¯ä¸ç¨®é åºå±å½¢é æ¸¬æ¹æ³ï¼ç¢çæ»¿è¶³ç¨±çºå±å½¢å¯æ¡æ§æ§å¶çå´æ ¼çµ±è¨ä¿è­çé æ¸¬éãæ­¤ä¿è­è¡¨ç¤ºï¼é æ¸¬éå¨é«æ©çä¸è³å°åå«ä¸åå¯æ¡ (æææ) çç¯ä¾ãçºæ­¤ï¼æåçæ¨¡åé¦åå¾é»ççææ¨¡åä¸­æ½åä¸çµ i.i.d. ç¯ä¾ãç¶å¾ï¼ééæè¬çè²ªå©ªéæ¿¾å¨åè¦ä¿®åªæ­¤çµãä½çºåè¦çæç¨åºççµæï¼æçµé æ¸¬éçå¯æ¡æ§åè§£çºé¦¬å¯å¤«éãæ­¤åè§£è³ééè¦ï¼å çºå®åè¨±ä½¿ç¨å±å½¢é æ¸¬åå¥æ§å¶æ¯åå å­ãèååçå·¥ä½ç¸æ¯ï¼æåçæ¨¡åé¡¯ç¤ºå¨æ ¡æºéç¨ä¸­å¯å¤§å¹æ¸å°å¯æ¡æ§è©ä¼°çæ¸éãæ­¤æ¸å°å¨å®å¨ééµæç¨ä¸­å¾éè¦ï¼å¨éäºæç¨ä¸­ï¼éäºè©ä¼°å¿é ç±é åå°å®¶æåé²è¡ï¼å æ­¤ææ¬é«æä¸èæãæåééèªç¶èªè¨çæååå­åå½¢å»¶ä¼¸ä»»åä¸­çå¯¦é©ï¼çªé¡¯æåçæ¨¡åå¨å¯æ¡æ§è©ä¼°åé æ¸¬éåºæ¸æ¹é¢çåªé»ã

##### **HiReview: Hierarchical Taxonomy-Driven Automatic Literature Review Generation**
2410.03761v1 by Yuntong Hu, Zhuofeng Li, Zheng Zhang, Chen Ling, Raasikh Kanjiani, Boxin Zhao, Liang Zhao

In this work, we present HiReview, a novel framework for hierarchical
taxonomy-driven automatic literature review generation. With the exponential
growth of academic documents, manual literature reviews have become
increasingly labor-intensive and time-consuming, while traditional
summarization models struggle to generate comprehensive document reviews
effectively. Large language models (LLMs), with their powerful text processing
capabilities, offer a potential solution; however, research on incorporating
LLMs for automatic document generation remains limited. To address key
challenges in large-scale automatic literature review generation (LRG), we
propose a two-stage taxonomy-then-generation approach that combines graph-based
hierarchical clustering with retrieval-augmented LLMs. First, we retrieve the
most relevant sub-community within the citation network, then generate a
hierarchical taxonomy tree by clustering papers based on both textual content
and citation relationships. In the second stage, an LLM generates coherent and
contextually accurate summaries for clusters or topics at each hierarchical
level, ensuring comprehensive coverage and logical organization of the
literature. Extensive experiments demonstrate that HiReview significantly
outperforms state-of-the-art methods, achieving superior hierarchical
organization, content relevance, and factual accuracy in automatic literature
review generation tasks.

æè¦ï¼å¨æ¬æä¸­ï¼æåæåºäº HiReviewï¼ä¸åç¨æ¼åå±¤åé¡é©åçèªåæç»åé¡§çæçå¨æ°æ¡æ¶ãé¨èå­¸è¡æç»çææ¸ç´å¢é·ï¼æåæç»åé¡§è®å¾è¶ä¾è¶ååå¯éä¸èæï¼èå³çµ±çæè¦æ¨¡åé£ä»¥ææå°çæå¨é¢çæä»¶åé¡§ãå¤§åèªè¨æ¨¡å (LLM) æèå¶å¼·å¤§çææ¬èçè½åæä¾äºä¸åæ½å¨çè§£æ±ºæ¹æ¡ï¼ç¶èï¼å° LLM ç´å¥èªåæä»¶çæçç ç©¶ä»ç¶æéãçºäºæå°å¤§è¦æ¨¡èªåæç»åé¡§çæ (LRG) ä¸­çä¸»è¦ææ°ï¼æåæåºäºä¸ç¨®å©éæ®µçåé¡åçææ¹æ³ï¼è©²æ¹æ³å°åºæ¼åå½¢çå±¤æ¬¡èé¡èæª¢ç´¢å¢å¼·ç LLM ç¸çµåãé¦åï¼æåæª¢ç´¢å¼æç¶²è·¯ä¸­æç¸éçå­ç¤¾ç¾¤ï¼ç¶å¾æ ¹æææ¬å§å®¹åå¼æéä¿å°è«æé²è¡èé¡ï¼çæä¸ååå±¤åé¡æ¨¹ãå¨ç¬¬äºéæ®µï¼LLM çºæ¯åå±¤ç´çç¾¤éæä¸»é¡çæé£è²«ä¸å¨èªå¢ä¸æºç¢ºçæè¦ï¼ç¢ºä¿æç»çå¨é¢è¦èåéè¼¯çµç¹ãå»£æ³çå¯¦é©è¡¨æï¼HiReview æé¡¯åªæ¼æåé²çæ¹æ³ï¼å¨èªåæç»åé¡§çæä»»åä¸­å¯¦ç¾äºåºè²çå±¤æ¬¡çµç¹ãå§å®¹ç¸éæ§åäºå¯¦æºç¢ºæ§ã

##### **LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion**
2410.01506v2 by Dexuan Ding, Lei Wang, Liyun Zhu, Tom Gedeon, Piotr Koniusz

In computer vision tasks, features often come from diverse representations,
domains, and modalities, such as text, images, and videos. Effectively fusing
these features is essential for robust performance, especially with the
availability of powerful pre-trained models like vision-language models.
However, common fusion methods, such as concatenation, element-wise operations,
and non-linear techniques, often fail to capture structural relationships, deep
feature interactions, and suffer from inefficiency or misalignment of features
across domains. In this paper, we shift from high-dimensional feature space to
a lower-dimensional, interpretable graph space by constructing similarity
graphs that encode feature relationships at different levels, e.g., clip,
frame, patch, token, etc. To capture deeper interactions, we use graph power
expansions and introduce a learnable graph fusion operator to combine these
graph powers for more effective fusion. Our approach is relationship-centric,
operates in a homogeneous space, and is mathematically principled, resembling
element-wise similarity score aggregation via multilinear polynomials. We
demonstrate the effectiveness of our graph-based fusion method on video anomaly
detection, showing strong performance across multi-representational,
multi-modal, and multi-domain feature fusion tasks.

æè¦ï¼<paragraph>å¨é»è¦è¦è¦ºä»»åä¸­ï¼ç¹å¾µéå¸¸ä¾èªä¸åçè¡¨ç¤ºã
é ååæ¨¡å¼ï¼ä¾å¦æå­ãå½±ååå½±çãææèå
éäºç¹å¾µå°æ¼å¼·å¥çæè½è³ééè¦ï¼ç¹å¥æ¯å¨
å·åå¼·å¤§é è¨ç·´æ¨¡åï¼ä¾å¦è¦è¦ºèªè¨æ¨¡åï¼çææ³ä¸ã
ç¶èï¼å¸¸è¦çèåæ¹æ³ï¼ä¾å¦ä¸²æ¥ãéåç´ éç®ï¼
åéç·æ§æè¡ï¼éå¸¸ç¡æ³ææçµæ§éä¿ãæ·±åº¦
ç¹å¾µäºåï¼ä¸¦ä¸æåå°éæçæç¹å¾µå¨ä¸åé åä¸­æªå°é½çå½±é¿ãå¨æ¬æä¸­ï¼æåå¾é«ç¶­ç¹å¾µç©ºéè½ç§»å°
ä½ç¶­ãå¯è§£éçåå½¢ç©ºéï¼ééå»ºæ§ç¸ä¼¼æ§
åå½¢ä¾ç·¨ç¢¼ä¸åå±¤ç´çç¹å¾µéä¿ï¼ä¾å¦åªè¼¯ã
å½±æ ¼ãè²¼çãæ¨è¨ç­ãçºäºæææ´æ·±å¥çäºåï¼æåä½¿ç¨åå½¢åª
å±éï¼ä¸¦å¼å¥å¯å­¸ç¿çåå½¢èåéç®å­ï¼ä»¥çµåéäº
åå½¢åªï¼ä»¥å¯¦ç¾æ´ææçèåãæåçåæ³ä»¥éä¿çºä¸­å¿ï¼
å¨åè³ªç©ºéä¸­éä½ï¼ä¸¦ä¸å·ææ¸å­¸åçï¼é¡ä¼¼æ¼
ééå¤ç·æ§å¤é å¼é²è¡éåç´ ç¸ä¼¼åº¦åæ¸èåãæå
å¨å½±çç°å¸¸åµæ¸¬ä¸­å±ç¤ºäºåºæ¼åå½¢çèåæ¹æ³çæææ§ï¼å¨å¤è¡¨ç¤ºã
å¤æ¨¡å¼åå¤é åç¹å¾µèåä»»åä¸­å±ç¾å¼·å¤§çæè½ã</paragraph>

##### **Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering**
2410.01401v1 by Yu Zhang, Kehai Chen, Xuefeng Bai, zhao kang, Quanjiang Guo, Min Zhang

Knowledge graph question answering (KGQA) involves answering natural language
questions by leveraging structured information stored in a knowledge graph.
Typically, KGQA initially retrieve a targeted subgraph from a large-scale
knowledge graph, which serves as the basis for reasoning models to address
queries. However, the retrieved subgraph inevitably brings distraction
information for knowledge utilization, impeding the model's ability to perform
accurate reasoning. To address this issue, we propose a Question-guided
Knowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the
input question, thereby focusing specifically on pertinent factual knowledge.
Moreover, we introduce Knowformer, a parameter-efficient method for injecting
the re-scored knowledge graph into large language models to enhance their
ability to perform factual reasoning. Extensive experiments on multiple KGQA
benchmarks demonstrate the superiority of our method over existing systems.

æè¦ï¼ç¥è­åè¡¨åç­ (KGQA) æ¶åå©ç¨å²å­å¨ç¥è­åè¡¨ä¸­ççµæ§åè³è¨ä¾åç­èªç¶èªè¨åé¡ãéå¸¸ï¼KGQA æåæå¾å¤§è¦æ¨¡ç¥è­åè¡¨ä¸­æ·åç®æ¨å­åï¼ä½çºæ¨çæ¨¡åèçæ¥è©¢çåºç¤ãç¶èï¼æ·åçå­åé£åæå¸¶ä¾éè¨è³è¨ï¼é»ç¤æ¨¡åå·è¡ç²¾ç¢ºæ¨ççè½åãçºäºè§£æ±ºéååé¡ï¼æåæåºäºä¸ååé¡å°åç¥è­åè¡¨éæ°è©åæ¹æ³ (Q-KGR)ï¼ä»¥æ¶é¤è¼¸å¥åé¡çéè¨è·¯å¾ï¼å¾èå°æ³¨æ¼ç¸éçäºå¯¦ç¥è­ãæ­¤å¤ï¼æåå¼å¥äº Knowformerï¼éæ¯ä¸ç¨®åæ¸æçé«çæ¹æ³ï¼ç¨æ¼å°éæ°è©åçç¥è­åè¡¨æ³¨å¥å¤§åèªè¨æ¨¡åï¼ä»¥å¢å¼·å®åå·è¡äºå¯¦æ¨ççè½åãå¨å¤å KGQA åºæºä¸çå»£æ³å¯¦é©è­æäºæåçæ¹æ³åªæ¼ç¾æç³»çµ±ã

##### **Unveiling Language Skills under Circuits**
2410.01334v1 by Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang

The exploration of language skills in language models (LMs) has always been
one of the central goals in mechanistic interpretability. However, existing
circuit analyses often fall short in representing the full functional scope of
these models, primarily due to the exclusion of Feed-Forward layers.
Additionally, isolating the effect of a single language skill from a text,
which inherently involves multiple entangled skills, poses a significant
challenge. To address these gaps, we introduce a novel concept, Memory Circuit,
a minimum unit that fully and independently manipulates the memory-reading
functionality of a language model, and disentangle the transformer model
precisely into a circuit graph which is an ensemble of paths connecting
different memory circuits. Based on this disentanglement, we identify salient
circuit paths, named as skill paths, responsible for three crucial language
skills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning
(ICL) Skill, leveraging causal effect estimation through interventions and
counterfactuals. Our experiments on various datasets confirm the correspondence
between our identified skill paths and language skills, and validate three
longstanding hypotheses: 1) Language skills are identifiable through circuit
dissection; 2) Simple language skills reside in shallow layers, whereas complex
language skills are found in deeper layers; 3) Complex language skills are
formed on top of simpler language skills. Our codes are available at:
https://github.com/Zodiark-ch/Language-Skill-of-LLMs.

æè¦ï¼<paragraph>å¨èªè¨æ¨¡å (LM) ä¸­æ¢ç´¢èªè¨æè½ä¸ç´æ¯æ©æ¢°å¯è§£éæ§çæ ¸å¿ç®æ¨ä¹ä¸ãç¶èï¼ç¾æçé»è·¯åæå¾å¾ç¡æ³è¡¨ç¤ºéäºæ¨¡åçå¨é¨åè½ç¯åï¼ä¸»è¦æ¯ç±æ¼æé¤äºåé¥å±¤ãæ­¤å¤ï¼å¾ææ¬ä¸­åé¢åºå®ä¸èªè¨æè½çå½±é¿ï¼éæ¬è³ªä¸æ¶åå¤ç¨®ç³¾çºçæè½ï¼æ§æäºä¸é éå¤§ææ°ãçºäºè§£æ±ºéäºå·®è·ï¼æåå¼å¥äº Memory Circuitï¼éæ¯ä¸åæ°ç©çæ¦å¿µï¼å®æ¯ä¸åæå°å®åï¼å¯ä»¥å®æ´ä¸ç¨ç«å°æä½èªè¨æ¨¡åçè¨æ¶é«è®ååè½ï¼ä¸¦å° Transformer æ¨¡åç²¾ç¢ºå°è§£éæä¸åé»è·¯åï¼å®æ¯ä¸åé£æ¥ä¸åè¨æ¶é«é»è·¯çè·¯å¾éåãåºæ¼éç¨®è§£éï¼æåè­å¥åºé¡¯èçé»è·¯è·¯å¾ï¼ç¨±çºæè½è·¯å¾ï¼å®è² è²¬ä¸é ééµçèªè¨æè½ï¼å³åä¸åç¬¦èæè½ãæ­¸ç´æè½åèªå¢å­¸ç¿ (ICL) æè½ï¼å©ç¨å æææä¼°è¨ééå¹²é ååäºå¯¦ãæåå¨åç¨®è³æéä¸çå¯¦é©è­å¯¦äºæåè­å¥åºçæè½è·¯å¾èèªè¨æè½ä¹éçå°æéä¿ï¼ä¸¦é©è­äºä¸åé·æçåè¨­ï¼1) èªè¨æè½å¯ä»¥ééé»è·¯è§£åä¾è­å¥ï¼2) ç°¡å®çèªè¨æè½å­å¨æ¼æ·ºå±¤ä¸­ï¼èè¤éçèªè¨æè½åå­å¨æ¼æ·±å±¤ä¸­ï¼3) è¤éçèªè¨æè½å»ºç«å¨æ´ç°¡å®çèªè¨æè½ä¹ä¸ãæåçç¨å¼ç¢¼å¯å¨ https://github.com/Zodiark-ch/Language-Skill-of-LLMs åå¾ã</paragraph>

##### **From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems**
2410.01066v1 by Ali Mohammadjafari, Anthony S. Maida, Raju Gottumukkala

Since the onset of LLMs, translating natural language queries to structured
SQL commands is assuming increasing. Unlike the previous reviews, this survey
provides a comprehensive study of the evolution of LLM-based text-to-SQL
systems, from early rule-based models to advanced LLM approaches, and how LLMs
impacted this field. We discuss benchmarks, evaluation methods and evaluation
metrics. Also, we uniquely study the role of integration of knowledge graphs
for better contextual accuracy and schema linking in these systems. The current
techniques fall into two categories: in-context learning of corpus and
fine-tuning, which then leads to approaches such as zero-shot, few-shot
learning from the end, and data augmentation. Finally, we highlight key
challenges such as computational efficiency, model robustness, and data privacy
with perspectives toward their development and improvements in potential areas
for future of LLM-based text-to-SQL system.

æè¦ï¼èª LLM åºç¾ä»¥ä¾ï¼å°èªç¶èªè¨æ¥è©¢è½æçºçµæ§å SQL æä»¤æ­£è®å¾è¶ä¾è¶æ®éãèååçè©è«ä¸åï¼æ¬èª¿æ¥å°åºæ¼ LLM çæå­è½ SQL ç³»çµ±çæ¼è®é²è¡äºå¨é¢çç ç©¶ï¼å¾æ©æçåºæ¼è¦åçæ¨¡åå°åé²ç LLM æ¹æ³ï¼ä»¥å LLM å¦ä½å½±é¿éåé åãæåè¨è«äºåºæºãè©ä¼°æ¹æ³åè©ä¼°ææ¨ãæ­¤å¤ï¼æåéç¨ç¹å°ç ç©¶äºç¥è­åè­æ´åå¨éäºç³»çµ±ä¸­ç¼æ®çä½ç¨ï¼ä»¥æé«èªå¢æºç¢ºæ§åæ¨¡å¼é£çµãç®åçæè¡åçºå©é¡ï¼èªæåº«çèªå¢å­¸ç¿åå¾®èª¿ï¼éé²èå°è´äºé¶æ¬¡å­¸ç¿ãå°æ¬¡å­¸ç¿ç­æ¹æ³ï¼æå¾æ¯è³ææ´åãæå¾ï¼æåéé»ä»ç´¹äºè¨ç®æçãæ¨¡åç©©å¥æ§åè³æé±ç§ç­ééµææ°ï¼ä¸¦å±æäºå®åå¨æªä¾åºæ¼ LLM çæå­è½ SQL ç³»çµ±çç¼å±åæ¹é²çæ½å¨é åã

##### **GUNDAM: Aligning Large Language Models with Graph Understanding**
2409.20053v2 by Sheng Ouyang, Yulan Hu, Ge Chen, Yong Liu

Large Language Models (LLMs) have achieved impressive results in processing
text data, which has sparked interest in applying these models beyond textual
data, such as graphs. In the field of graph learning, there is a growing
interest in harnessing LLMs to comprehend and manipulate graph-structured data.
Existing research predominantly focuses on graphs with rich textual features,
such as knowledge graphs or text attribute graphs, leveraging LLMs' ability to
process text but inadequately addressing graph structure. This work
specifically aims to assess and enhance LLMs' abilities to comprehend and
utilize the structural knowledge inherent in graph data itself, rather than
focusing solely on graphs rich in textual content. To achieve this, we
introduce the \textbf{G}raph \textbf{U}nderstanding for \textbf{N}atural
Language \textbf{D}riven \textbf{A}nalytical \textbf{M}odel (\model). This
model adapts LLMs to better understand and engage with the structure of graph
data, enabling them to perform complex reasoning tasks by leveraging the
graph's structure itself. Our experimental evaluations on graph reasoning
benchmarks not only substantiate that \model~ outperforms the SOTA baselines
for comparisons. But also reveals key factors affecting the graph reasoning
capabilities of LLMs. Moreover, we provide a theoretical analysis illustrating
how reasoning paths can enhance LLMs' reasoning capabilities.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨èçæå­è³ææ¹é¢åå¾ä»¤äººå°è±¡æ·±å»çææï¼éæ¿ç¼äºå°éäºæ¨¡åæç¨æ¼æå­è³æä»¥å¤é åçèè¶£ï¼ä¾å¦åè¡¨ãå¨åè¡¨å­¸ç¿é åï¼å©ç¨ LLM ä¾çè§£åèçåå½¢çµæ§è³æçèè¶£èæ¥ä¿±å¢ãç¾æçç ç©¶ä¸»è¦éä¸­æ¼å·æè±å¯æå­ç¹å¾µçåè¡¨ï¼ä¾å¦ç¥è­åè¡¨ææå­å±¬æ§åè¡¨ï¼å©ç¨ LLM èçæå­çè½åï¼ä½æªè½ååè§£æ±ºåè¡¨çµæ§ãéé å·¥ä½ç¹å¥æ¨å¨è©ä¼°åå¢å¼· LLM çè§£åå©ç¨åè¡¨è³ææ¬èº«åºæçµæ§ç¥è­çè½åï¼èä¸æ¯åå°æ³¨æ¼å¯å«æå­å§å®¹çåè¡¨ãçºæ­¤ï¼æåå¼å¥äºèªç¶èªè¨é©ååææ¨¡å (\model) çåè¡¨çè§£ãæ­¤æ¨¡åèª¿æ´ LLM ä»¥ä¾¿æ´å¥½å°çè§£ååèåè¡¨è³æççµæ§ï¼ä½¿å®åè½å¤ ééå©ç¨åè¡¨ççµæ§æ¬èº«ä¾å·è¡è¤éçæ¨çä»»åãæåå¨åè¡¨æ¨çåºæºä¸çå¯¦é©è©ä¼°ä¸åè­å¯¦ \model~ åªæ¼æ¯è¼ä¸­ç SOTA åºæºãéæ­ç¤ºäºå½±é¿ LLM åè¡¨æ¨çè½åçä¸»è¦å ç´ ãæ­¤å¤ï¼æåæä¾äºçè«åæï¼èªªææ¨çè·¯å¾å¦ä½å¢å¼· LLM çæ¨çè½åã

##### **Enhancing High-order Interaction Awareness in LLM-based Recommender Model**
2409.19979v2 by Xinfeng Wang, Jin Cui, Fumiyo Fukumoto, Yoshimi Suzuki

Large language models (LLMs) have demonstrated prominent reasoning
capabilities in recommendation tasks by transforming them into text-generation
tasks. However, existing approaches either disregard or ineffectively model the
user-item high-order interactions. To this end, this paper presents an enhanced
LLM-based recommender (ELMRec). We enhance whole-word embeddings to
substantially enhance LLMs' interpretation of graph-constructed interactions
for recommendations, without requiring graph pre-training. This finding may
inspire endeavors to incorporate rich knowledge graphs into LLM-based
recommenders via whole-word embedding. We also found that LLMs often recommend
items based on users' earlier interactions rather than recent ones, and present
a reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in
both direct and sequential recommendations.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²è­æå¨æ¨è¦ä»»åä¸­å·æé¡¯èçæ¨çè½åï¼æ¹æ³æ¯å°å¶è½æçºææ¬çæä»»åãç¶èï¼ç¾ææ¹æ³ä¸æ¯å¿½ç¥ç¨æ¶é ç®é«éäºåï¼å°±æ¯å°å¶å»ºæ¨¡ææä¸ä½³ãçºæ­¤ï¼æ¬ææåºäºä¸ç¨®å¢å¼·çåºæ¼ LLM çæ¨è¦å¨ (ELMRec)ãæåå¢å¼·äºå¨è©åµå¥ï¼ä»¥å¤§å¹å¢å¼· LLM å°åå½¢æ§å»ºäºåçè§£è®ï¼ç¨æ¼æ¨è¦ï¼èä¸éè¦åå½¢é è¨ç·´ãéä¸ç¼ç¾å¯è½ææ¿åµå°è±å¯çç¥è­åè­ééå¨è©åµå¥æ´åå°åºæ¼ LLM çæ¨è¦å¨ä¸­çåªåãæåéç¼ç¾ï¼LLM éå¸¸æ ¹æç¨æ¶æ©æçäºåèéæè¿çäºåä¾æ¨è¦é ç®ï¼ä¸¦æåºäºä¸ç¨®éæ°æåºçè§£æ±ºæ¹æ¡ãæåç ELMRec å¨ç´æ¥åé åºæ¨è¦ä¸­é½åªæ¼æåé² (SOTA) æ¹æ³ã

##### **CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering**
2409.19753v2 by Yike Wu, Yi Huang, Nan Hu, Yuncheng Hua, Guilin Qi, Jiaoyan Chen, Jeff Z. Pan

Recent studies have explored the use of Large Language Models (LLMs) with
Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering
(KGQA). They typically require rewriting retrieved subgraphs into natural
language formats comprehensible to LLMs. However, when tackling complex
questions, the knowledge rewritten by existing methods may include irrelevant
information, omit crucial details, or fail to align with the question's
semantics. To address them, we propose a novel rewriting method CoTKR,
Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces
and corresponding knowledge in an interleaved manner, thereby mitigating the
limitations of single-step knowledge rewriting. Additionally, to bridge the
preference gap between the knowledge rewriter and the question answering (QA)
model, we propose a training strategy PAQAF, Preference Alignment from Question
Answering Feedback, for leveraging feedback from the QA model to further
optimize the knowledge rewriter. We conduct experiments using various LLMs
across several KGQA benchmarks. Experimental results demonstrate that, compared
with previous knowledge rewriting methods, CoTKR generates the most beneficial
knowledge representation for QA models, which significantly improves the
performance of LLMs in KGQA.

æè¦ï¼æè¿çç ç©¶æ¢ç´¢äºå°å¤§åèªè¨æ¨¡å (LLM) èæª¢ç´¢æ´å¢çæ (RAG) çµåç¨æ¼ç¥è­åè¡¨åç­ (KGQA)ãå®åéå¸¸éè¦å°æª¢ç´¢å°çå­åæ¹å¯«æ LLM å¯ä»¥çè§£çèªç¶èªè¨æ ¼å¼ãç¶èï¼å¨èçè¤éåé¡æï¼ç¾ææ¹æ³æ¹å¯«çç¥è­å¯è½åå«ä¸ç¸éçè³è¨ãéºæ¼ééµç´°ç¯ï¼æç¡æ³èåé¡çèªç¾©å°é½ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ç¨®æ°çæ¹å¯«æ¹æ³ CoTKRï¼å³åºæ¼æèéçç¥è­å¢å¼·æ¹å¯«ï¼ç¨æ¼äº¤é¯çææ¨çè»è·¡åå°æçç¥è­ï¼å¾èæ¸è¼å®æ­¥ç¥è­æ¹å¯«çéå¶ãæ­¤å¤ï¼çºäºå½åç¥è­æ¹å¯«å¨ååç­ (QA) æ¨¡åä¹éçåå¥½å·®è·ï¼æåæåºäºä¸ç¨®è¨ç·´ç­ç¥ PAQAFï¼å³åºæ¼åç­åé¥çåå¥½å°é½ï¼ç¨æ¼å©ç¨ QA æ¨¡åçåé¥é²ä¸æ­¥æä½³åç¥è­æ¹å¯«å¨ãæåä½¿ç¨åç¨® LLM å¨å¤å KGQA åºæºä¸é²è¡äºå¯¦é©ãå¯¦é©çµæè¡¨æï¼èååçç¥è­æ¹å¯«æ¹æ³ç¸æ¯ï¼CoTKR çº QA æ¨¡åçæäºææççç¥è­è¡¨ç¤ºï¼éé¡¯èæé«äº LLM å¨ KGQA ä¸­çæè½ã

##### **Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models**
2409.19667v1 by Xin Li, Weize Chen, Qizhi Chu, Haopeng Li, Zhaojun Sun, Ran Li, Chen Qian, Yiwei Wei, Zhiyuan Liu, Chuan Shi, Maosong Sun, Cheng Yang

The need to analyze graphs is ubiquitous across various fields, from social
networks to biological research and recommendation systems. Therefore, enabling
the ability of large language models (LLMs) to process graphs is an important
step toward more advanced general intelligence. However, current LLM benchmarks
on graph analysis require models to directly reason over the prompts describing
graph topology, and are thus limited to small graphs with only a few dozens of
nodes. In contrast, human experts typically write programs based on popular
libraries for task solving, and can thus handle graphs with different scales.
To this end, a question naturally arises: can LLMs analyze graphs like
professionals? In this paper, we introduce ProGraph, a manually crafted
benchmark containing 3 categories of graph tasks. The benchmark expects
solutions based on programming instead of directly reasoning over raw inputs.
Our findings reveal that the performance of current LLMs is unsatisfactory,
with the best model achieving only 36% accuracy. To bridge this gap, we propose
LLM4Graph datasets, which include crawled documents and auto-generated codes
based on 6 widely used graph libraries. By augmenting closed-source LLMs with
document retrieval and fine-tuning open-source ones on the codes, we show
11-32% absolute improvements in their accuracies. Our results underscore that
the capabilities of LLMs in handling structured data are still under-explored,
and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph
analysis. The benchmark, datasets and enhanced open-source models are available
at https://github.com/BUPT-GAMMA/ProGraph.

æè¦ï¼<paragraph>å¨å¾ç¤¾äº¤ç¶²è·¯å°çç©ç ç©¶åæ¨è¦ç³»çµ±çåç¨®é åä¸­ï¼åæåå½¢çéæ±ç¡èä¸å¨ãå æ­¤ï¼è³¦äºå¤§åèªè¨æ¨¡å (LLM) èçåå½¢çè½åï¼æ¯éåæ´åé²çéç¨æºæ§çéè¦ä¸æ­¥ãç¶èï¼ç®ååå½¢åæä¸ç LLM åºæºè¦æ±æ¨¡åç´æ¥å°æè¿°åå½¢ææ²çµæ§çæç¤ºé²è¡æ¨çï¼å æ­¤åéæ¼åªææ¸ååç¯é»çå°ååå½¢ãç¸æ¯ä¹ä¸ï¼äººé¡å°å®¶éå¸¸ææ ¹ææµè¡çå½å¼åº«æ°å¯«ç¨å¼ä¾è§£æ±ºä»»åï¼å æ­¤å¯ä»¥èçä¸åè¦æ¨¡çåå½¢ãçºæ­¤ï¼èªç¶æç¢çä¸ååé¡ï¼LLM è½åå°æ¥­äººå£«ä¸æ¨£åæåå½¢åï¼å¨æ¬æä¸­ï¼æåä»ç´¹ ProGraphï¼ä¸ååå« 3 é¡åå½¢ä»»åçæå·¥è£½ä½åºæºãè©²åºæºé æè§£æ±ºæ¹æ¡æ¯åºæ¼ç¨å¼è¨­è¨ï¼èä¸æ¯ç´æ¥å°åå§è¼¸å¥é²è¡æ¨çãæåçç¼ç¾é¡¯ç¤ºï¼ç®å LLM çæè½ä¸¦ä¸ä»¤äººæ»¿æï¼æä½³æ¨¡ååéå° 36% çæºç¢ºåº¦ãçºäºå½è£éåå·®è·ï¼æåæåºäº LLM4Graph è³æéï¼å¶ä¸­åå«æ ¹æ 6 åå»£æ³ä½¿ç¨çåå½¢å½å¼åº«ç¬åççæä»¶åèªåçæçç¨å¼ç¢¼ãééä½¿ç¨æä»¶æª¢ç´¢ä¾æ´åéæº LLMï¼ä¸¦éå°ç¨å¼ç¢¼å¾®èª¿éæº LLMï¼æåå±ç¤ºäºæºç¢ºåº¦æåäº 11-32%ãæåççµæå¼·èª¿ï¼LLM å¨èççµæ§åè³ææ¹é¢çè½åä»æªè¢«ååæ¢ç´¢ï¼ä¸¦é¡¯ç¤ºäº LLM4Graph å¨æå LLM åå½¢åæè½åæ¹é¢çæææ§ãåºæºãè³æéåå¢å¼·çéæºæ¨¡åå¯å¨ https://github.com/BUPT-GAMMA/ProGraph åå¾ã</paragraph>

##### **Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs**
2409.19401v1 by Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, Wei Shi

In the age of mobile internet, user data, often referred to as memories, is
continuously generated on personal devices. Effectively managing and utilizing
this data to deliver services to users is a compelling research topic. In this
paper, we introduce a novel task of crafting personalized agents powered by
large language models (LLMs), which utilize a user's smartphone memories to
enhance downstream applications with advanced LLM capabilities. To achieve this
goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented
Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach
is further optimized using Reinforcement Learning to address three distinct
challenges: data collection, editability, and selectability. Extensive
experiments on a real-world dataset validate the effectiveness of EMG-RAG,
achieving an improvement of approximately 10% over the best existing approach.
Additionally, the personalized agents have been transferred into a real
smartphone AI assistant, which leads to enhanced usability.

æè¦ï¼å¨è¡åç¶²è·¯æä»£ï¼ä½¿ç¨èè³æï¼å¸¸ç¨±çºè¨æ¶ï¼ææçºå¨åäººè£ç½®ä¸ç¢çãææç®¡çä¸¦å©ç¨éäºè³æï¼çºä½¿ç¨èæä¾æåï¼æ¯ä¸åå¼äººå¥åçç ç©¶ä¸»é¡ãå¨æ¬æä¸­ï¼æåä»ç´¹äºä¸é åµæ°çä»»åï¼å³å©ç¨å¤§åèªè¨æ¨¡åï¼LLMï¼æé åäººåä»£çï¼å©ç¨ä½¿ç¨èçæºæ§åææ©è¨æ¶é«ï¼ä»¥é²é LLM åè½å¼·åä¸æ¸¸æç¨ç¨å¼ãçºäºéæéåç®æ¨ï¼æåä»ç´¹äº EMG-RAGï¼ä¸ç¨®çµåæª¢ç´¢æ´åçæï¼RAGï¼æè¡èå¯ç·¨è¼¯è¨æ¶åï¼EMGï¼çè§£æ±ºæ¹æ¡ãéç¨®æ¹æ³é²ä¸æ­¥ééå¼·åå­¸ç¿é²è¡æä½³åï¼ä»¥è§£æ±ºä¸åä¸åçææ°ï¼è³ææ¶éãå¯ç·¨è¼¯æ§èå¯é¸ææ§ãå¨çå¯¦ä¸çè³æéä¸çå»£æ³å¯¦é©é©è­äº EMG-RAG çæææ§ï¼æ¯ç¾ææä½³æ¹æ³æåäºç´ 10%ãæ­¤å¤ï¼åäººåä»£çå·²è½ç§»å°çæ­£çæºæ§åææ© AI å©çä¸­ï¼éæåäºå¯ç¨æ§ã

##### **CLLMate: A Multimodal LLM for Weather and Climate Events Forecasting**
2409.19058v1 by Haobo Li, Zhaowei Wang, Jiachen Wang, Alexis Kai Hon Lau, Huamin Qu

Forecasting weather and climate events is crucial for making appropriate
measures to mitigate environmental hazards and minimize associated losses.
Previous research on environmental forecasting focuses on predicting numerical
meteorological variables related to closed-set events rather than forecasting
open-set events directly, which limits the comprehensiveness of event
forecasting. We propose Weather and Climate Event Forecasting (WCEF), a new
task that leverages meteorological raster data and textual event data to
predict potential weather and climate events. However, due to difficulties in
aligning multimodal data and the lack of sufficient supervised datasets, this
task is challenging to accomplish. Therefore, we first propose a framework to
align historical meteorological data with past weather and climate events using
the large language model (LLM). In this framework, we construct a knowledge
graph by using LLM to extract information about weather and climate events from
a corpus of over 41k highly environment-focused news articles. Subsequently, we
mapped these events with meteorological raster data, creating a supervised
dataset, which is the largest and most novel for LLM tuning on the WCEF task.
Finally, we introduced our aligned models, CLLMate (LLM for climate), a
multimodal LLM to forecast weather and climate events using meteorological
raster data. In evaluating CLLMate, we conducted extensive experiments. The
results indicate that CLLMate surpasses both the baselines and other multimodal
LLMs, showcasing the potential of utilizing LLM to align weather and climate
events with meteorological data and highlighting the promising future for
research on the WCEF task.

æè¦ï¼é æ¸¬å¤©æ°£åæ°£åäºä»¶å°æ¼æ¡åé©ç¶æªæ½æ¸è¼ç°å¢å±å®³åå°ç¸éæå¤±éè³æä½è³ééè¦ã
ååæéç°å¢é æ¸¬çç ç©¶èéæ¼é æ¸¬èå°ééäºä»¶ç¸éçæ¸å¼æ°£è±¡è®æ¸ï¼èéç´æ¥é æ¸¬éæ¾éäºä»¶ï¼ééå¶äºäºä»¶é æ¸¬çå¨é¢æ§ã
æåæåºå¤©æ°£åæ°£åäºä»¶é æ¸¬ (WCEF)ï¼éæ¯ä¸é å©ç¨æ°£è±¡æµæ ¼è³æåæå­äºä»¶è³æä¾é æ¸¬æ½å¨å¤©æ°£åæ°£åäºä»¶çæ°ä»»åã
ç¶èï¼ç±æ¼å¤æ¨¡æè³æå°é½çå°é£ä»¥åç¼ºä¹è¶³å¤ çç£ç£å¼è³æéï¼å æ­¤æ­¤ä»»åé£ä»¥éæã
å æ­¤ï¼æåé¦åæåºä¸åæ¶æ§ï¼ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) å°æ­·å²æ°£è±¡è³æèéå»çå¤©æ°£åæ°£åäºä»¶å°é½ã
å¨æ­¤æ¶æ§ä¸­ï¼æåééä½¿ç¨ LLM å¾è¶é 41,000 ç¯é«åº¦éæ³¨ç°å¢çæ°èæç« ä¸­æ·åæéå¤©æ°£åæ°£åäºä»¶çè³è¨ä¾å»ºæ§ç¥è­åè­ã
é¨å¾ï¼æåå°éäºäºä»¶å°æå°æ°£è±¡æµæ ¼è³æï¼å»ºç«ä¸åç£ç£å¼è³æéï¼éæ¯ LLM å¨ WCEF ä»»åä¸èª¿æ´ä¸­æå¤§ä¸ææ°ç©çè³æéã
æå¾ï¼æåå¼å¥äºæåçå°é½æ¨¡åï¼CLLMateï¼æ°£åç LLMï¼ï¼éæ¯ä¸åå¤æ¨¡æ LLMï¼ä½¿ç¨æ°£è±¡æµæ ¼è³æä¾é æ¸¬å¤©æ°£åæ°£åäºä»¶ã
å¨è©ä¼° CLLMate æï¼æåé²è¡äºå¤§éçå¯¦é©ã
çµæè¡¨æï¼CLLMate è¶è¶äºåºæºåå¶ä»çå¤æ¨¡æ LLMï¼å±ç¤ºäºå©ç¨ LLM å°å¤©æ°£åæ°£åäºä»¶èæ°£è±¡è³æå°é½çæ½åï¼ä¸¦å¼·èª¿äº WCEF ä»»åç ç©¶çæªä¾åæ¯ã

##### **AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow**
2409.18924v2 by Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi, Xiang Li, Wenyue Hua, Mingyu Jin, Guang Chen, Yang Zhou, Zhao Li, Trisha Gupte, Ming-Li Chen, Zahra Azizi, Yongfeng Zhang, Themistocles L. Assimes, Xin Ma, Danielle S. Bitterman, Lin Lu, Lizhou Fan

Simulated patient systems play a crucial role in modern medical education and
research, providing safe, integrative learning environments and enabling
clinical decision-making simulations. Large Language Models (LLM) could advance
simulated patient systems by replicating medical conditions and patient-doctor
interactions with high fidelity and low cost. However, ensuring the
effectiveness and trustworthiness of these systems remains a challenge, as they
require a large, diverse, and precise patient knowledgebase, along with a
robust and stable knowledge diffusion to users. Here, we developed AIPatient,
an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient
KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning
RAG) agentic workflow as the generation backbone. AIPatient KG samples data
from Electronic Health Records (EHRs) in the Medical Information Mart for
Intensive Care (MIMIC)-III database, producing a clinically diverse and
relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).
Reasoning RAG leverages six LLM powered agents spanning tasks including
retrieval, KG query generation, abstraction, checker, rewrite, and
summarization. This agentic framework reaches an overall accuracy of 94.15% in
EHR-based medical Question Answering (QA), outperforming benchmarks that use
either no agent or only partial agent integration. Our system also presents
high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade
5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value
0.782, p>0.1). The promising performance of the AIPatient system highlights its
potential to support a wide range of applications, including medical education,
model evaluation, and system integration.

æè¦ï¼æ¨¡æ¬çäººç³»çµ±å¨ç¾ä»£é«å­¸æè²åç ç©¶ä¸­æ®æ¼èè³ééè¦çè§è²ï¼æä¾å®å¨ãæ´åçå­¸ç¿ç°å¢ï¼ä¸¦è½é²è¡è¨åºæ±ºç­æ¨¡æ¬ãå¤§åèªè¨æ¨¡å (LLM) è½ééé«ä¿çåº¦åä½ææ¬è¤è£½é«ççæ³åé«çäºåï¼é²èæåæ¨¡æ¬çäººç³»çµ±ãç¶èï¼ç¢ºä¿éäºç³»çµ±çæææ§åå¯ä¿¡åº¦ä»ç¶æ¯ä¸é ææ°ï¼å çºå®åéè¦ä¸åé¾å¤§ãå¤åä¸ç²¾ç¢ºççäººç¥è­åº«ï¼ä»¥åç©©å¥ä¸ç©©å®çç¥è­å³æ­çµ¦ä½¿ç¨èãå¨æ­¤ï¼æåéç¼äº AIPatientï¼ä¸åé²éçæ¨¡æ¬çäººç³»çµ±ï¼ä»¥ AIPatient ç¥è­åè­ (AIPatient KG) ä½çºè¼¸å¥ï¼ä¸¦ä»¥æ¨çæª¢ç´¢å¢å¼·çæ (Reasoning RAG) ä»£çå·¥ä½æµç¨ä½çºçæä¸»å¹¹ãAIPatient KG å¾éçç£è­·é«å­¸è³è¨ä¸­å¿ (MIMIC)-III è³æåº«ä¸­çé»å­å¥åº·ç´é (EHR) ä¸­æ½åè³æï¼ç¢çä¸åè¨åºå¤æ¨£ä¸ç¸éç 1,495 åçæ£ç¾¤çµï¼å·æå¾é«çç¥è­åº«æåº¦ (F1 0.89)ãæ¨ç RAG æ§æ¡¿äºå­å LLM é©åçä»£çï¼è·¨è¶æª¢ç´¢ãKG æ¥è©¢ç¢çãæ½è±¡ãæª¢æ¥å¨ãéå¯«åæè¦ç­ä»»åãéåä»£çæ¡æ¶å¨åºæ¼ EHR çé«çåç­ (QA) ä¸­éå°äº 94.15% çæ´é«æºç¢ºåº¦ï¼åªæ¼ä¸ä½¿ç¨ä»£çæåé¨åä»£çæ´åçåºæºãæåçç³»çµ±éå·æå¾é«çå¯è®æ§ (Flesch é±è®ç°¡ä¾¿æ§ä¸­ä½æ¸ 77.23ï¼Flesch Kincaid ç­ç´ä¸­ä½æ¸ 5.6)ãç©©å¥æ§ (ANOVA F å¼ 0.6126ï¼p>0.1) åç©©å®æ§ (ANOVA F å¼ 0.782ï¼p>0.1)ãAIPatient ç³»çµ±çåºè²è¡¨ç¾çªé¡¯äºå®å¨æ¯æ´åç¨®æç¨ç¨å¼çæ½åï¼åæ¬é«å­¸æè²ãæ¨¡åè©ä¼°åç³»çµ±æ´åã

##### **Soft Measures for Extracting Causal Collective Intelligence**
2409.18911v1 by Maryam Berijanian, Spencer Dork, Kuldeep Singh, Michael Riley Millikan, Ashlin Riggs, Aadarsh Swaminathan, Sarah L. Gibbs, Scott E. Friedman, Nathan Brugnone

Understanding and modeling collective intelligence is essential for
addressing complex social systems. Directed graphs called fuzzy cognitive maps
(FCMs) offer a powerful tool for encoding causal mental models, but extracting
high-integrity FCMs from text is challenging. This study presents an approach
using large language models (LLMs) to automate FCM extraction. We introduce
novel graph-based similarity measures and evaluate them by correlating their
outputs with human judgments through the Elo rating system. Results show
positive correlations with human evaluations, but even the best-performing
measure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs
improves performance, but existing measures still fall short. This study
highlights the need for soft similarity measures tailored to FCM extraction,
advancing collective intelligence modeling with NLP.

æè¦ï¼äºè§£åå»ºæ¨¡éä½æºæ§å¯¹äºè§£å³å¤æçç¤¾ä¼ç³»ç»è³å³éè¦ãç§°ä¸ºæ¨¡ç³è®¤ç¥å¾ï¼FCMï¼çæåå¾æä¾äºä¸ç§å¼ºå¤§çå·¥å·æ¥ç¼ç å æå¿æºæ¨¡åï¼ä½ä»ææ¬ä¸­æåé«å®æ´æ§ç FCM å·ææææ§ãæ¬ç ç©¶æåºäºä¸ç§ä½¿ç¨å¤§åè¯­è¨æ¨¡åï¼LLMï¼æ¥èªå¨å FCM æåçæ¹æ³ãæä»¬å¼å¥äºæ°é¢çåºäºå¾çç¸ä¼¼æ§åº¦éï¼å¹¶éè¿éè¿ Elo è¯çº§ç³»ç»å°å¶è¾åºä¸äººç±»å¤æ­ç¸å³èæ¥è¯ä¼°å®ä»¬ãç»æè¡¨æä¸äººç±»è¯ä¼°åæ­£ç¸å³ï¼ä½å³ä½¿æ¯è¡¨ç°æå¥½çåº¦éå¨ææ FCM ç»å¾®å·®å«æ¹é¢ä¹è¡¨ç°åºå±éæ§ãå¾®è° LLM å¯ä»¥æé«æ§è½ï¼ä½ç°ææªæ½ä»ç¶ä¸è¶³ãæ¬ç ç©¶å¼ºè°äºéå¯¹ FCM æåéèº«å®å¶çè½¯ç¸ä¼¼æ§åº¦éçå¿è¦æ§ï¼éè¿ NLP æ¨è¿äºéä½æºè½å»ºæ¨¡ã

##### **OpenObject-NAV: Open-Vocabulary Object-Oriented Navigation Based on Dynamic Carrier-Relationship Scene Graph**
2409.18743v1 by Yujie Tang, Meiling Wang, Yinan Deng, Zibo Zheng, Jiagui Zhong, Yufeng Yue

In everyday life, frequently used objects like cups often have unfixed
positions and multiple instances within the same category, and their carriers
frequently change as well. As a result, it becomes challenging for a robot to
efficiently navigate to a specific instance. To tackle this challenge, the
robot must capture and update scene changes and plans continuously. However,
current object navigation approaches primarily focus on semantic-level and lack
the ability to dynamically update scene representation. This paper captures the
relationships between frequently used objects and their static carriers. It
constructs an open-vocabulary Carrier-Relationship Scene Graph (CRSG) and
updates the carrying status during robot navigation to reflect the dynamic
changes of the scene. Based on the CRSG, we further propose an instance
navigation strategy that models the navigation process as a Markov Decision
Process. At each step, decisions are informed by Large Language Model's
commonsense knowledge and visual-language feature similarity. We designed a
series of long-sequence navigation tasks for frequently used everyday items in
the Habitat simulator. The results demonstrate that by updating the CRSG, the
robot can efficiently navigate to moved targets. Additionally, we deployed our
algorithm on a real robot and validated its practical effectiveness.

æè¦ï¼æ¥å¸¸çæ´»ä¸­ï¼ç¶å¸¸ä½¿ç¨çç©åï¼ä¾å¦æ¯å­ï¼éå¸¸æ²æåºå®çä½ç½®ï¼èä¸åä¸é¡å¥ä¸­æå¤åå¯¦ä¾ï¼å¶æ¿è¼èä¹ç¶å¸¸è®æ´ãå æ­¤ï¼æ©å¨äººè¦ææå°å°èªå°ç¹å®å¯¦ä¾è®å¾å·æææ°æ§ãçºäºæå°éä¸ææ°ï¼æ©å¨äººå¿é ä¸æ·ææåæ´æ°å ´æ¯è®æ´åè¨ç«ãç¶èï¼ç®åçç©ä»¶å°èªæ¹æ³ä¸»è¦éä¸­å¨èªç¾©å±¤ç´ï¼ä¸¦ä¸ç¼ºä¹åææ´æ°å ´æ¯è¡¨ç¤ºçè½åãæ¬æææäºç¶å¸¸ä½¿ç¨çç©ä»¶åå¶éææ¿è¼èä¹éçéä¿ãå®æ§å»ºäºä¸åéæ¾è©å½çæ¿è¼èéä¿å ´æ¯å (CRSG)ï¼ä¸¦å¨æ©å¨äººå°èªæéæ´æ°æ¿è¼çæä»¥åæ å ´æ¯çåæè®åãåºæ¼ CRSGï¼æåé²ä¸æ­¥æåºäºä¸ç¨®å°å°èªéç¨å»ºæ¨¡çºé¦¬å¯å¤«æ±ºç­éç¨çå¯¦ä¾å°èªç­ç¥ãå¨æ¯ä¸æ­¥ä¸­ï¼æ±ºç­é½ç±å¤§åèªè¨æ¨¡åçå¸¸è­ç¥è­åè¦è¦ºèªè¨ç¹å¾µç¸ä¼¼æ§ä¾åç¥ãæåçº Habitat æ¨¡æ¬å¨ä¸­çæ¥å¸¸å¸¸ç¨ç©åè¨­è¨äºä¸ç³»åé·åºåå°èªä»»åãçµæè¡¨æï¼ééæ´æ° CRSGï¼æ©å¨äººå¯ä»¥ææå°å°èªå°ç§»åçç®æ¨ãæ­¤å¤ï¼æåå¨çå¯¦æ©å¨äººä¸é¨ç½²äºæåçæ¼ç®æ³ï¼ä¸¦é©è­äºå¶å¯¦éæè½ã

##### **Rehearsing Answers to Probable Questions with Perspective-Taking**
2409.18678v1 by Yung-Yu Shih, Ziwei Xu, Hiroya Takamura, Yun-Nung Chen, Chung-Chi Chen

Question answering (QA) has been a long-standing focus in the NLP field,
predominantly addressing reading comprehension and common sense QA. However,
scenarios involving the preparation of answers to probable questions during
professional oral presentations remain underexplored. In this paper, we pioneer
the examination of this crucial yet overlooked topic by utilizing real-world QA
conversation transcripts between company managers and professional analysts. We
explore the proposed task using three causal knowledge graphs (KGs) and three
large language models (LLMs). This work provides foundational insights into the
application of LLMs in professional QA scenarios, highlighting the importance
of causal KGs and perspective-taking in generating effective responses.

æè¦ï¼åé¡è§£ç­ (QA) ä¸ç´æ¯èªç¶èªè¨èç (NLP) é åçé·æéæ³¨éé»ï¼
ä¸»è¦è§£æ±ºé±è®çè§£åå¸¸è­åé¡è§£ç­ãç¶èï¼
å¨å°æ¥­å£é ­ç°¡å ±ä¸­æºååç­å¯è½åé¡çå ´æ¯ä»æªå¾å°ååæ¢è¨ãå¨æ¬æä¸­ï¼æåçå
å©ç¨å¬å¸ç¶çåå°æ¥­åæå¸«ä¹éççå¯¦ä¸çåç­å°è©±è¨éï¼æ¢è¨éåè³ééè¦ä½è¢«å¿½è¦çä¸»é¡ãæå
ä½¿ç¨ä¸åå æç¥è­åè­ (KG) åä¸åå¤§åèªè¨æ¨¡å (LLM) ä¾æ¢è¨æåºçä»»åãéé å·¥ä½çº LLM å¨å°æ¥­åç­å ´æ¯ä¸­çæç¨æä¾äºåºç¤è¦è§£ï¼å¼·èª¿äºå æ KG åè§é»æ¡åå¨ç¢çææåæä¸­çéè¦æ§ã

##### **LowREm: A Repository of Word Embeddings for 87 Low-Resource Languages Enhanced with Multilingual Graph Knowledge**
2409.18193v1 by Daniil Gurgurov, Rishu Kumar, Simon Ostermann

Contextualized embeddings based on large language models (LLMs) are available
for various languages, but their coverage is often limited for lower resourced
languages. Training LLMs for such languages is often difficult due to
insufficient data and high computational cost. Especially for very low resource
languages, static word embeddings thus still offer a viable alternative. There
is, however, a notable lack of comprehensive repositories with such embeddings
for diverse languages. To address this, we present LowREm, a centralized
repository of static embeddings for 87 low-resource languages. We also propose
a novel method to enhance GloVe-based embeddings by integrating multilingual
graph knowledge, utilizing another source of knowledge. We demonstrate the
superior performance of our enhanced embeddings as compared to contextualized
embeddings extracted from XLM-R on sentiment analysis. Our code and data are
publicly available under https://huggingface.co/DFKI.

æè¦ï¼åºæ¼å¤§åèªè¨æ¨¡å (LLM) çèªå¢ååµå¥å¯ä¾åç¨®èªè¨ä½¿ç¨ï¼ä½å¶æ¶µèç¯åéå¸¸åéæ¼è³æºè¼å°çèªè¨ãç±æ¼è³æä¸è¶³åé«éç®ææ¬ï¼çºæ­¤é¡èªè¨è¨ç·´ LLM éå¸¸å¾å°é£ãç¹å¥æ¯å°æ¼è³æºéå¸¸å°çèªè¨ï¼å æ­¤éæå­è©åµå¥ä»æä¾å¯è¡çæ¿ä»£æ¹æ¡ãç¶èï¼å°æ¼åç¨®èªè¨ä¾èªªï¼æ­¤é¡åµå¥ç¼ºä¹å¨é¢çå²å­åº«ãçºäºè§£æ±ºéååé¡ï¼æåæåºäº LowREmï¼ä¸åéå° 87 ç¨®ä½è³æºèªè¨çéæåµå¥éä¸­å¼å²å­åº«ãæåéæåºäºä¸ç¨®æ°æ¹æ³ï¼ééæ´åå¤èªè¨åå½¢ç¥è­ä¾å¢å¼·åºæ¼ GloVe çåµå¥ï¼å©ç¨å¦ä¸åç¥è­ä¾æºãæåå±ç¤ºäºæåå¢å¼·çåµå¥å¨æç·åæä¸åªæ¼å¾ XLM-R æåçèªå¢ååµå¥ãæåçç¨å¼ç¢¼åè³æå·²å¬éå¨ https://huggingface.co/DFKI ä¸ã

##### **A Survey of Spatio-Temporal EEG data Analysis: from Models to Applications**
2410.08224v1 by Pengfei Wang, Huanran Zheng, Silong Dai, Yiqiao Wang, Xiaotian Gu, Yuanbin Wu, Xiaoling Wang

In recent years, the field of electroencephalography (EEG) analysis has
witnessed remarkable advancements, driven by the integration of machine
learning and artificial intelligence. This survey aims to encapsulate the
latest developments, focusing on emerging methods and technologies that are
poised to transform our comprehension and interpretation of brain activity. We
delve into self-supervised learning methods that enable the robust
representation of brain signals, which are fundamental for a variety of
downstream applications. We also explore emerging discriminative methods,
including graph neural networks (GNN), foundation models, and large language
models (LLMs)-based approaches. Furthermore, we examine generative technologies
that harness EEG data to produce images or text, offering novel perspectives on
brain activity visualization and interpretation. The survey provides an
extensive overview of these cutting-edge techniques, their current
applications, and the profound implications they hold for future research and
clinical practice. The relevant literature and open-source materials have been
compiled and are consistently being refreshed at
\url{https://github.com/wpf535236337/LLMs4TS}

æè¦ï¼è¿å¹´ä¾ï¼è¦é»å (EEG) åæé åè¦è­äºé¡¯èçé²å±ï¼éè¦æ­¸åæ¼æ©å¨å­¸ç¿åäººå·¥æºè½çæ´åãæ¬èª¿æ¥æ¨å¨æ¦æ¬ææ°ç¼å±ï¼éé»éæ³¨æ°èæ¹æ³åæè¡ï¼éäºæ¹æ³åæè¡ææè½è®æåå°å¤§è¦æ´»åççè§£åè©®éãæåæ·±å¥æ¢è¨èªç£ç£å­¸ç¿æ¹æ³ï¼éäºæ¹æ³è½è®å¤§è¦ä¿¡èçè¡¨ç¤ºè®å¾ç©©å¥ï¼éå°åç¨®ä¸æ¸¸æç¨è³ééè¦ãæåéæ¢è¨æ°èçå¤å¥æ¹æ³ï¼åæ¬åç¥ç¶ç¶²è·¯ (GNN)ãåºç¤æ¨¡ååå¤§èªè¨æ¨¡å (LLM) çºåºç¤çæ¹æ³ãæ­¤å¤ï¼æåæª¢è¦å©ç¨ EEG è³æç¢çå½±åææå­ççæå¼æè¡ï¼çºå¤§è¦æ´»åè¦è¦ºååè©®éæä¾äºæ°çè§é»ãæ¬èª¿æ¥å°éäºå°ç«¯æè¡ãå®åç®åçæç¨ä»¥åå®åå°æªä¾ç ç©¶åè¨åºå¯¦åçæ·±é å½±é¿æä¾äºå»£æ³çæ¦è¿°ãç¸éæç»åéæ¾åå§ç¢¼è³æå·²ç·¨è­¯ï¼ä¸¦æçºæ´æ°æ¼
\url{https://github.com/wpf535236337/LLMs4TS}

##### **Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study**
2409.17580v1 by Zahra Sepasdar, Sushant Gautam, Cise Midoglu, Michael A. Riegler, PÃ¥l Halvorsen

Extracting meaningful insights from large and complex datasets poses
significant challenges, particularly in ensuring the accuracy and relevance of
retrieved information. Traditional data retrieval methods such as sequential
search and index-based retrieval often fail when handling intricate and
interconnected data structures, resulting in incomplete or misleading outputs.
To overcome these limitations, we introduce Structured-GraphRAG, a versatile
framework designed to enhance information retrieval across structured datasets
in natural language queries. Structured-GraphRAG utilizes multiple knowledge
graphs, which represent data in a structured format and capture complex
relationships between entities, enabling a more nuanced and comprehensive
retrieval of information. This graph-based approach reduces the risk of errors
in language model outputs by grounding responses in a structured format,
thereby enhancing the reliability of results. We demonstrate the effectiveness
of Structured-GraphRAG by comparing its performance with that of a recently
published method using traditional retrieval-augmented generation. Our findings
show that Structured-GraphRAG significantly improves query processing
efficiency and reduces response times. While our case study focuses on soccer
data, the framework's design is broadly applicable, offering a powerful tool
for data analysis and enhancing language model applications across various
structured domains.

æè¦ï¼å¾é¾å¤§ä¸è¤éçè³æéä¸­èååºææç¾©çè¦è§£æå¸¶ä¾é¡¯èçææ°ï¼ç¹å¥æ¯å¨ç¢ºä¿æ·åè³è¨çæºç¢ºæ§åç¸éæ§æ¹é¢ãå³çµ±çè³ææ·åæ¹æ³ï¼ä¾å¦é åºæå°ååºæ¼ç´¢å¼çæ·åï¼å¨èçè¤éä¸ç¸äºé£çµçè³æçµæ§æï¼å¸¸å¸¸æå¤±æï¼å°è´ä¸å®æ´æèª¤å°æ§çè¼¸åºãçºäºåæéäºéå¶ï¼æåå¼å¥äºçµæ§ååå½¢ RAGï¼éæ¯ä¸åéç¨æ¡æ¶ï¼æ¨å¨å¢å¼·èªç¶èªè¨æ¥è©¢ä¸­çµæ§åè³æéçè³è¨æ·åãçµæ§ååå½¢ RAG å©ç¨å¤åç¥è­åå½¢ï¼å®åä»¥çµæ§åæ ¼å¼è¡¨ç¤ºè³æï¼ä¸¦æ·åå¯¦é«ä¹éçè¤ééä¿ï¼å¾èå¯¦ç¾æ´ç´°ç·»ä¸å¨é¢çè³è¨æ·åãéç¨®åºæ¼åå½¢çåæ³ééä»¥çµæ§åæ ¼å¼çºåºç¤åæï¼éä½èªè¨æ¨¡åè¼¸åºä¸­åºç¾é¯èª¤çé¢¨éªï¼å¾èæé«çµæçå¯é æ§ãæåééå°çµæ§ååå½¢ RAG çæè½èæè¿ç¼è¡¨çå³çµ±æ·åå¢å¼·çææ¹æ³é²è¡æ¯è¼ï¼ä¾è­æå¶æææ§ãæåçç ç©¶çµæé¡¯ç¤ºï¼çµæ§ååå½¢ RAG å¤§å¹æåäºæ¥è©¢èçæçï¼ä¸¦ç¸®ç­äºåææéãéç¶æåçæ¡ä¾ç ç©¶å°æ³¨æ¼è¶³çè³æï¼ä½éåæ¶æ§çè¨­è¨å·æå»£æ³çé©ç¨æ§ï¼æä¾äºä¸åå¼·å¤§çè³æåæå·¥å·ï¼ä¸¦å¢å¼·äºåç¨®çµæ§åé åçèªè¨æ¨¡åæç¨ã

##### **Probing Omissions and Distortions in Transformer-based RDF-to-Text Models**
2409.16707v1 by Juliette Faille, Albert Gatt, Claire Gardent

In Natural Language Generation (NLG), important information is sometimes
omitted in the output text. To better understand and analyse how this type of
mistake arises, we focus on RDF-to-Text generation and explore two methods of
probing omissions in the encoder output of BART (Lewis et al, 2020) and of T5
(Raffel et al, 2019): (i) a novel parameter-free probing method based on the
computation of cosine similarity between embeddings of RDF graphs and of RDF
graphs in which we removed some entities and (ii) a parametric probe which
performs binary classification on the encoder embeddings to detect omitted
entities. We also extend our analysis to distorted entities, i.e. entities that
are not fully correctly mentioned in the generated text (e.g. misspelling of
entity, wrong units of measurement). We found that both omitted and distorted
entities can be probed in the encoder's output embeddings. This suggests that
the encoder emits a weaker signal for these entities and therefore is
responsible for some loss of information. This also shows that probing methods
can be used to detect mistakes in the output of NLG models.

æè¦ï¼å¨èªç¶èªè¨çæ (NLG) ä¸­ï¼éè¦è³è¨æææå¨è¼¸åºæå­ä¸­è¢«çç¥ãçºäºæ´äºè§£ä¸¦åæéé¡é¯èª¤æ¯å¦ä½ç¢ççï¼æåå°æ³¨æ¼ RDF è½æå­ççæï¼ä¸¦æ¢è¨å©ç¨®æ¢æ¸¬ BART (Lewis ç­äººï¼2020) å T5 (Raffel ç­äººï¼2019) çç·¨ç¢¼å¨è¼¸åºä¸­éºæ¼çæ¹æ³ï¼(i) ä¸ç¨®åºæ¼ RDF åå½¢åµå¥åæåç§»é¤ä¸äºå¯¦é«ç RDF åå½¢ä¹éçé¤å¼¦ç¸ä¼¼åº¦è¨ç®çæ°åç¡åæ¸æ¢æ¸¬æ¹æ³ï¼ä»¥å (ii) ä¸ç¨®å¨ç·¨ç¢¼å¨åµå¥ä¸­å·è¡äºååé¡ä»¥åµæ¸¬éºæ¼å¯¦é«çåæ¸åæ¢æ¸¬ãæåä¹å°æåçåæå»¶ä¼¸å°æ­æ²çå¯¦é«ï¼ä¹å°±æ¯å¨ç¢ççæå­ä¸­æ²æè¢«å®å¨æ­£ç¢ºæåçå¯¦é« (ä¾å¦å¯¦é«æ¼å¯«é¯èª¤ãæ¸¬éå®ä½é¯èª¤)ãæåç¼ç¾éºæ¼åæ­æ²çå¯¦é«é½å¯ä»¥è¢«æ¢æ¸¬å°å¨ç·¨ç¢¼å¨çè¼¸åºåµå¥ä¸­ãéè¡¨ç¤ºç·¨ç¢¼å¨éå°éäºå¯¦é«ç¼å°è¼å¼±çè¨èï¼å æ­¤å°è´ä¸äºè³è¨éºå¤±ãéä¹é¡¯ç¤ºæ¢æ¸¬æ¹æ³å¯ä»¥ç¨æ¼åµæ¸¬ NLG æ¨¡åè¼¸åºä¸­çé¯èª¤ã

##### **GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning**
2409.16670v1 by Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu

Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in
handling a range of graph analytical tasks across various domains, such as
e-commerce and social networks. Despite their versatility, GNNs face
significant challenges in transferability, limiting their utility in real-world
applications. Existing research in GNN transfer learning overlooks
discrepancies in distribution among various graph datasets, facing challenges
when transferring across different distributions. How to effectively adopt a
well-trained GNN to new graphs with varying feature and structural
distributions remains an under-explored problem. Taking inspiration from the
success of Low-Rank Adaptation (LoRA) in adapting large language models to
various domains, we propose GraphLoRA, an effective and parameter-efficient
method for transferring well-trained GNNs to diverse graph domains.
Specifically, we first propose a Structure-aware Maximum Mean Discrepancy
(SMMD) to align divergent node feature distributions across source and target
graphs. Moreover, we introduce low-rank adaptation by injecting a small
trainable GNN alongside the pre-trained one, effectively bridging structural
distribution gaps while mitigating the catastrophic forgetting. Additionally, a
structure-aware regularization objective is proposed to enhance the
adaptability of the pre-trained GNN to target graph with scarce supervision
labels. Extensive experiments on six real-world datasets demonstrate the
effectiveness of GraphLoRA against eleven baselines by tuning only 20% of
parameters, even across disparate graph domains. The code is available at
https://anonymous.4open.science/r/GraphLoRA.

æè¦ï¼åå½¢ç¥ç¶ç¶²è·¯ (GNN) å·²å±ç¾åºå¨åç¨®é åèçä¸ç³»ååå½¢åæä»»åçåè¶è½åï¼ä¾å¦é»å­åååç¤¾ç¾¤ç¶²è·¯ãåç®¡ GNN å·æå¤åè½æ§ï¼ä½å¨å¯è½ç§»æ§æ¹é¢ä»é¢è¨éå¤§ææ°ï¼éå¶äºå®åå¨ç¾å¯¦ä¸çæç¨ä¸­çæç¨ãç¾æç GNN è½ç§»å­¸ç¿ç ç©¶å¿½è¦äºåç¨®åå½¢è³æéä¹éçåå¸å·®ç°ï¼å¨è·¨ä¸ååå¸è½ç§»æé¢è¨ææ°ãå¦ä½ææå°å°è¨ç·´è¯å¥½ç GNN æç¨æ¼å·æä¸åç¹å¾µåçµæ§åå¸çæ°åå½¢ï¼ä»ç¶æ¯ä¸åå°æªååæ¢è¨çåé¡ãå¾ä½ç§©é©æ (LoRA) å¨å°å¤§åèªè¨æ¨¡åé©æå°åç¨®é åæ¹é¢ç²å¾çæåä¸­æ±²åéæï¼æåæåºäº GraphLoRAï¼éæ¯ä¸ç¨®ææä¸åæ¸æçé«çæ¹æ³ï¼å¯ç¨æ¼å°è¨ç·´è¯å¥½ç GNN è½ç§»å°ä¸åçåå½¢é åãå·é«ä¾èªªï¼æåé¦åæåºä¸åçµæ§æç¥æå¤§å¹³åå·®ç° (SMMD) ä¾èª¿æ´ä¾æºåç®æ¨åå½¢ä¸­çä¸åç¯é»ç¹å¾µåå¸ãæ­¤å¤ï¼æåééå¨é åè¨ç·´ç GNN æéæ³¨å¥ä¸åå°çå¯è¨ç·´ GNN ä¾å¼å¥ä½ç§©é©æï¼å¾èææå°å½åçµæ§åå¸å·®è·ï¼åææ¸è¼ç½é£æ§éºå¿ãæ­¤å¤ï¼éæåºäºçµæ§æç¥æ­£ååç®æ¨ï¼ä»¥å¢å¼·é åè¨ç·´ç GNN å°å·æç¨çç£ç£æ¨ç±¤çç®æ¨åå½¢çé©ææ§ãå¨å­åçå¯¦ä¸çè³æéä¸çå¤§éå¯¦é©è­æäº GraphLoRA çæææ§ï¼å®åèª¿æ´äº 20% çåæ¸ï¼å³ä½¿å¨ä¸åçåå½¢é åä¸­ä¹è½å¤ åéåä¸ç¨®åºæºãç¨å¼ç¢¼å¯å¨ https://anonymous.4open.science/r/GraphLoRA åå¾ã

##### **Cyber Knowledge Completion Using Large Language Models**
2409.16176v1 by Braden K Webb, Sumit Purohit, Rounak Meyur

The integration of the Internet of Things (IoT) into Cyber-Physical Systems
(CPSs) has expanded their cyber-attack surface, introducing new and
sophisticated threats with potential to exploit emerging vulnerabilities.
Assessing the risks of CPSs is increasingly difficult due to incomplete and
outdated cybersecurity knowledge. This highlights the urgent need for
better-informed risk assessments and mitigation strategies. While previous
efforts have relied on rule-based natural language processing (NLP) tools to
map vulnerabilities, weaknesses, and attack patterns, recent advancements in
Large Language Models (LLMs) present a unique opportunity to enhance
cyber-attack knowledge completion through improved reasoning, inference, and
summarization capabilities. We apply embedding models to encapsulate
information on attack patterns and adversarial techniques, generating mappings
between them using vector embeddings. Additionally, we propose a
Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained
models to create structured mappings between different taxonomies of threat
patterns. Further, we use a small hand-labeled dataset to compare the proposed
RAG-based approach to a baseline standard binary classification model. Thus,
the proposed approach provides a comprehensive framework to address the
challenge of cyber-attack knowledge graph completion.

æè¦ï¼ç©è¯ç¶² (IoT) èç¶²è·¯å¯¦é«ç³»çµ± (CPS) çæ´åæ´å¤§äºå¶ç¶²è·¯æ»æé¢ï¼å¼å¥äºæ°çåè¤éçå¨èï¼å·æå©ç¨æ°èæ¼æ´çæ½åãç±æ¼ç¶²è·¯å®å¨ç¥è­ä¸å®æ´ä¸éæï¼è©ä¼° CPS çé¢¨éªè®å¾è¶ä¾è¶å°é£ãéçªé¡¯äºè¿«åéè¦æ´å®åçé¢¨éªè©ä¼°åç·©è§£ç­ç¥ãéç¶ååçåªåä¾è³´æ¼åºæ¼è¦åçèªç¶èªè¨èç (NLP) å·¥å·ä¾ç¹ªè£½æ¼æ´ãå¼±é»åæ»ææ¨¡å¼ï¼ä½å¤§åèªè¨æ¨¡å (LLM) çææ°é²å±æä¾äºä¸åç¨ç¹çæ©æï¼å¯ä»¥ééæ¹é²çæ¨çãæ¨è«åæè¦è½åä¾å¢å¼·ç¶²è·¯æ»æç¥è­çå®æåº¦ãæåæç¨åµå¥æ¨¡åä¾å°è£æéæ»ææ¨¡å¼åå°ææè¡çè³è¨ï¼ä½¿ç¨åéåµå¥å¨å®åä¹éç¢çå°æéä¿ãæ­¤å¤ï¼æåæåºäºä¸ååºæ¼æª¢ç´¢å¢å¼·çæ (RAG) çæ¹æ³ï¼è©²æ¹æ³å©ç¨é åè¨ç·´çæ¨¡åå¨å¨èæ¨¡å¼çä¸ååé¡æ³ä¹éå»ºç«çµæ§åçå°æéä¿ãæ­¤å¤ï¼æåä½¿ç¨ä¸åå°åçæåæ¨è¨è³æéä¾æ¯è¼ææåºçåºæ¼ RAG çæ¹æ³èåºç·æ¨æºäºååé¡æ¨¡åãå æ­¤ï¼ææåºçæ¹æ³æä¾äºä¸åå¨é¢çæ¶æ§ä¾è§£æ±ºç¶²è·¯æ»æç¥è­åå®æçææ°ã

##### **Konstruktor: A Strong Baseline for Simple Knowledge Graph Question Answering**
2409.15902v1 by Maria Lysyuk, Mikhail Salnikov, Pavel Braslavski, Alexander Panchenko

While being one of the most popular question types, simple questions such as
"Who is the author of Cinderella?", are still not completely solved.
Surprisingly, even the most powerful modern Large Language Models are prone to
errors when dealing with such questions, especially when dealing with rare
entities. At the same time, as an answer may be one hop away from the question
entity, one can try to develop a method that uses structured knowledge graphs
(KGs) to answer such questions. In this paper, we introduce Konstruktor - an
efficient and robust approach that breaks down the problem into three steps:
(i) entity extraction and entity linking, (ii) relation prediction, and (iii)
querying the knowledge graph. Our approach integrates language models and
knowledge graphs, exploiting the power of the former and the interpretability
of the latter. We experiment with two named entity recognition and entity
linking methods and several relation detection techniques. We show that for
relation detection, the most challenging step of the workflow, a combination of
relation classification/generation and ranking outperforms other methods. We
report Konstruktor's strong results on four datasets.

æè¦ï¼åç®¡æ¯æå¸¸è¦çåé¡é¡åä¹ä¸ï¼ä½è«¸å¦ãç°å§å¨çä½èæ¯èª°ï¼ãéé¡ç°¡å®çåé¡ä»æªå®å¨ç²å¾è§£ç­ãä»¤äººé©è¨çæ¯ï¼å³ä½¿æ¯æå¼·å¤§çç¾ä»£å¤§åèªè¨æ¨¡åå¨èçæ­¤é¡åé¡æä¹å®¹æåºé¯ï¼ç¹å¥æ¯å¨èçç½è¦å¯¦é«æãèæ­¤åæï¼ç±æ¼ç­æ¡å¯è½è·é¢åé¡å¯¦é«åä¸æ­¥ä¹éï¼å æ­¤å¯ä»¥åè©¦éç¼ä¸ç¨®ä½¿ç¨çµæ§åç¥è­åè­ (KG) ä¾åç­æ­¤é¡åé¡çæ¹æ³ãå¨æ¬æä¸­ï¼æåä»ç´¹ Konstruktor - ä¸ç¨®é«æä¸å¼·å¤§çæ¹æ³ï¼å®å°åé¡åè§£çºä¸åæ­¥é©ï¼(i) å¯¦é«èååå¯¦é«é£çµã(ii) éä¿é æ¸¬ä»¥å (iii) æ¥è©¢ç¥è­åè­ãæåçåæ³æ´åäºèªè¨æ¨¡ååç¥è­åè­ï¼ç¼æ®äºåèçè½ååå¾èçå¯è§£éæ§ãæåå¯¦é©äºå©ç¨®å½åå¯¦é«è­å¥åå¯¦é«é£çµæ¹æ³ä»¥åå¤ç¨®éä¿åµæ¸¬æè¡ãæåè¡¨æï¼å°æ¼éä¿åµæ¸¬ï¼ä¹å°±æ¯å·¥ä½æµç¨ä¸­æå·ææ°æ§çæ­¥é©ï¼éä¿åé¡/çæåæåç¸çµåççµååªæ¼å¶ä»æ¹æ³ãæåå ±åäº Konstruktor å¨ååè³æéä¸çå¼·åææã

##### **Symmetries and Expressive Requirements for Learning General Policies**
2409.15892v1 by Dominik Drexler, Simon StÃ¥hlberg, Blai Bonet, Hector Geffner

State symmetries play an important role in planning and generalized planning.
In the first case, state symmetries can be used to reduce the size of the
search; in the second, to reduce the size of the training set. In the case of
general planning, however, it is also critical to distinguish non-symmetric
states, i.e., states that represent non-isomorphic relational structures.
However, while the language of first-order logic distinguishes non-symmetric
states, the languages and architectures used to represent and learn general
policies do not. In particular, recent approaches for learning general policies
use state features derived from description logics or learned via graph neural
networks (GNNs) that are known to be limited by the expressive power of C_2,
first-order logic with two variables and counting. In this work, we address the
problem of detecting symmetries in planning and generalized planning and use
the results to assess the expressive requirements for learning general policies
over various planning domains. For this, we map planning states to plain
graphs, run off-the-shelf algorithms to determine whether two states are
isomorphic with respect to the goal, and run coloring algorithms to determine
if C_2 features computed logically or via GNNs distinguish non-isomorphic
states. Symmetry detection results in more effective learning, while the
failure to detect non-symmetries prevents general policies from being learned
at all in certain domains.

æè¦ï¼çæå°ç¨±æ§å¨è¦ååå»£ç¾©è¦åä¸­æ®æ¼èéè¦çè§è²ã
å¨ç¬¬ä¸ç¨®ææ³ä¸­ï¼çæå°ç¨±æ§å¯ç¨æ¼ç¸®å°æå°çè¦æ¨¡ï¼å¨ç¬¬äºç¨®ææ³ä¸­ï¼å¯ç¨æ¼ç¸®å°è¨ç·´éçè¦æ¨¡ãç¶èï¼å¨å»£ç¾©è¦åçææ³ä¸­ï¼ååéå°ç¨±çæï¼å³è¡¨ç¤ºéåæ§éä¿çµæ§ççæï¼ä¹å¾éè¦ãç¶èï¼éç¶ä¸ééè¼¯çèªè¨ååäºéå°ç¨±çæï¼ä½ç¨æ¼è¡¨ç¤ºåå­¸ç¿ä¸è¬ç­ç¥çèªè¨åæ¶æ§å»æ²æãç¹å¥æ¯ï¼æè¿ç¨æ¼å­¸ç¿ä¸è¬ç­ç¥çæ¹æ³ä½¿ç¨å¾æè¿°éè¼¯ä¸­è¡çççæç¹å¾µï¼æééåç¥ç¶ç¶²è·¯ (GNN) å­¸ç¿ï¼å·²ç¥éäºç¹å¾µåå°å·æå©åè®æ¸åè¨æ¸çä¸ééè¼¯ C_2 çè¡¨éè½åéå¶ãå¨éé å·¥ä½ä¸­ï¼æåè§£æ±ºäºå¨è¦ååå»£ç¾©è¦åä¸­æª¢æ¸¬å°ç¨±æ§çåé¡ï¼ä¸¦ä½¿ç¨çµæè©ä¼°å¨åç¨®è¦åé åä¸­å­¸ç¿ä¸è¬ç­ç¥çè¡¨ééæ±ãçºæ­¤ï¼æåå°è¦åçææ å°å°å¹³é¢åå½¢ï¼å·è¡ç¾æçæ¼ç®æ³ä¾ç¢ºå®å©åçææ¯å¦ç¸å°æ¼ç®æ¨åæ§ï¼ä¸¦å·è¡èè²æ¼ç®æ³ä¾ç¢ºå®éééè¼¯æ GNN è¨ç®ç C_2 ç¹å¾µæ¯å¦ååéåæ§çæãå°ç¨±æ§æª¢æ¸¬æå¸¶ä¾æ´ææçå­¸ç¿ï¼èç¡æ³æª¢æ¸¬éå°ç¨±æ§åæå®å¨é»æ­¢å¨æäºé åä¸­å­¸ç¿ä¸è¬ç­ç¥ã

##### **GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation**
2409.15566v1 by Brendan Hogan Rappazzo, Yingheng Wang, Aaron Ferber, Carla Gomes

The ability to form, retrieve, and reason about memories in response to
stimuli serves as the cornerstone for general intelligence - shaping entities
capable of learning, adaptation, and intuitive insight. Large Language Models
(LLMs) have proven their ability, given the proper memories or context, to
reason and respond meaningfully to stimuli. However, they are still unable to
optimally encode, store, and retrieve memories - the ability to do this would
unlock their full ability to operate as AI agents, and to specialize to niche
domains. To remedy this, one promising area of research is Retrieval Augmented
Generation (RAG), which aims to augment LLMs by providing them with rich
in-context examples and information. In question-answering (QA) applications,
RAG methods embed the text of interest in chunks, and retrieve the most
relevant chunks for a prompt using text embeddings. Motivated by human memory
encoding and retrieval, we aim to improve over standard RAG methods by
generating and encoding higher-level information and tagging the chunks by
their utility to answer questions. We introduce Graphical Eigen Memories For
Retrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk
of text in a given text corpus with LLM generated ``utility'' questions,
connecting chunks in a graph based on the similarity of both their text and
utility questions, and then using the eigendecomposition of the memory graph to
build higher level summary nodes that capture the main themes of the text. We
evaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with
SBERT, and OpenAI's text encoders on two standard QA tasks, showing that
GEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also
discuss the implications of having a robust RAG system and future directions.

æè¦ï¼<paragraph>æ ¹æåºæ¿å½¢æãæª¢ç´¢åæ¨çè¨æ¶çè½åæ¯éç¨æºæ§çåºç³ï¼å¡é äºå·åå­¸ç¿ãé©æåç´è¦ºæ´å¯åçå¯¦é«ãå¤§åèªè¨æ¨¡å (LLM) å·²è­æå¶è½åï¼å¨é©ç¶çè¨æ¶æèæ¯ä¸ï¼å°åºæ¿é²è¡æ¨çåææç¾©å°åæãç¶èï¼å®åä»ç¶ç¡æ³æä½³å°ç·¨ç¢¼ãå²å­åæª¢ç´¢è¨æ¶ï¼å·è¡æ­¤æä½çè½åå°è§£éå®åä½çº AI ä»£çéä½ä¸¦å°éåçºå©åºé åçå¨é¨è½åãçºäºè£ææ­¤åé¡ï¼ä¸åæåæ¯çç ç©¶é åæ¯æª¢ç´¢å¢å¼·çæ (RAG)ï¼å¶ç®æ¨æ¯ééæä¾è±å¯çä¸ä¸æç¯ä¾åè³è¨ä¾æ´å LLMãå¨åç­ (QA) æç¨ç¨å¼ä¸­ï¼RAG æ¹æ³å°æèè¶£çæå­åå¡åµå¥ï¼ä¸¦ä½¿ç¨æå­åµå¥çºæç¤ºæª¢ç´¢æç¸éçåå¡ãåäººé¡è¨æ¶ç·¨ç¢¼åæª¢ç´¢çåç¼ï¼æåæ¨å¨ééç¢çåç·¨ç¢¼æ´é«ç´å¥çè³è¨ä¸¦æ ¹æåå¡åç­åé¡çæç¨æ¨è¨åå¡ï¼å¾èæ¹é²æ¨æº RAG æ¹æ³ãæåå¼å¥äºç¨æ¼æª¢ç´¢å¢å¼·çæçåå½¢ç¹å¾µè¨æ¶ (GEM-RAG)ãGEM-RAG çå·¥ä½åçæ¯ä½¿ç¨ LLM çæçãæç¨ãåé¡æ¨è¨çµ¦å®æå­èªæåº«ä¸­æ¯åæå­åå¡ï¼æ ¹ææå­åæç¨åé¡çç¸ä¼¼æ§å°åå¡é£æ¥å¨åå½¢ä¸­ï¼ç¶å¾ä½¿ç¨è¨æ¶åå½¢çç¹å¾µåè§£ä¾å»ºç«æ·åæå­ä¸»é¡çé«éæè¦ç¯é»ãæåä½¿ç¨ UnifiedQA å GPT-3.5 Turbo ä½çº LLMï¼ä»¥å SBERT å OpenAI çæå­ç·¨ç¢¼å¨ï¼å¨å©åæ¨æº QA ä»»åä¸­è©ä¼° GEM-RAGï¼é¡¯ç¤º GEM-RAG å¨éäºä»»åä¸­åªæ¼å¶ä»æåé²ç RAG æ¹æ³ãæåéè¨è«äºææå¼·å¤§ç RAG ç³»çµ±çå«æåæªä¾çæ¹åã</paragraph>

##### **KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems**
2409.14908v1 by Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan

Embodied AI agents responsible for executing interconnected, long-sequence
household tasks often face difficulties with in-context memory, leading to
inefficiencies and errors in task execution. To address this issue, we
introduce KARMA, an innovative memory system that integrates long-term and
short-term memory modules, enhancing large language models (LLMs) for planning
in embodied agents through memory-augmented prompting. KARMA distinguishes
between long-term and short-term memory, with long-term memory capturing
comprehensive 3D scene graphs as representations of the environment, while
short-term memory dynamically records changes in objects' positions and states.
This dual-memory structure allows agents to retrieve relevant past scene
experiences, thereby improving the accuracy and efficiency of task planning.
Short-term memory employs strategies for effective and adaptive memory
replacement, ensuring the retention of critical information while discarding
less pertinent data. Compared to state-of-the-art embodied agents enhanced with
memory, our memory-augmented embodied AI agent improves success rates by 1.3x
and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator,
respectively, and enhances task execution efficiency by 3.4x and 62.7x.
Furthermore, we demonstrate that KARMA's plug-and-play capability allows for
seamless deployment on real-world robotic systems, such as mobile manipulation
platforms.Through this plug-and-play memory system, KARMA significantly
enhances the ability of embodied agents to generate coherent and contextually
appropriate plans, making the execution of complex household tasks more
efficient. The experimental videos from the work can be found at
https://youtu.be/4BT7fnw9ehs.

æè¦ï¼è² è²¬å·è¡ç¸äºé£æ¥çé·åºåå®¶åº­ä»»åçå·èº«å AI ä»£çç¶å¸¸é¢è¨æå¢è¨æ¶çå°é£ï¼å°è´ä»»åå·è¡æçä½ä¸åé¯èª¤ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº KARMAï¼éæ¯ä¸ååµæ°çè¨æ¶ç³»çµ±ï¼å®æ´åäºé·æåç­æè¨æ¶æ¨¡çµï¼ééè¨æ¶å¢å¼·æç¤ºï¼å¢å¼·å·èº«åä»£çä¸­ç¨æ¼è¦åçå¤§èªè¨æ¨¡å (LLM)ãKARMA ååé·æåç­æè¨æ¶ï¼é·æè¨æ¶æ·åå¨é¢ç 3D å ´æ¯åå½¢ä½çºç°å¢çè¡¨ç¤ºï¼èç­æè¨æ¶ååæè¨éç©ä»¶ä½ç½®åçæçè®åãéç¨®ééè¨æ¶çµæ§åè¨±ä»£çæ·åç¸éçéå»å ´æ¯ç¶é©ï¼å¾èæé«ä»»åè¦åçæºç¢ºæ§åæçãç­æè¨æ¶æ¡ç¨ç­ç¥ä¾é²è¡ææåé©ææ§çè¨æ¶æ¿æï¼ç¢ºä¿ä¿çééµè³è¨ï¼åææ¨æ£è¼ä¸ç¸éçè³æãèå·åå¢å¼·è¨æ¶åè½çææ°å·èº«åä»£çç¸æ¯ï¼æåçè¨æ¶å¢å¼·å·èº«å AI ä»£çå¨ AI2-THOR æ¨¡æ¬å¨ä¸­çè¤åä»»ååè¤éä»»åä¸­ï¼åå¥å°æåçæé«äº 1.3 åå 2.3 åï¼ä¸¦å°ä»»åå·è¡æçæé«äº 3.4 åå 62.7 åãæ­¤å¤ï¼æåè­æäº KARMA çå³æå³ç¨åè½åè¨±å¨çå¯¦ä¸ççæ©å¨äººç³»çµ±ä¸é²è¡ç¡ç¸«é¨ç½²ï¼ä¾å¦è¡åæä½å¹³å°ãéééåå³æå³ç¨è¨æ¶ç³»çµ±ï¼KARMA å¤§å¹å¢å¼·äºå·èº«åä»£çç¢çä¸è´ä¸ç¬¦åæå¢çè¨ç«çè½åï¼ä½¿è¤éå®¶åº­ä»»åçå·è¡æ´ææçãéé å·¥ä½çå¯¦é©å½±çå¯ä»¥å¨ https://youtu.be/4BT7fnw9ehs æ¾å°ã

##### **End-to-End Graph Flattening Method for Large Language Models**
2409.14880v1 by Bin Hong, Jinze Wu, Jiayu Liu, Liang Ding, Jing Sha, Kai Zhang, Shijin Wang, Zhenya Huang

In recent years, the breakthrough of Large Language Models (LLMs) offers new
ideas for achieving universal methods on graph data. The common practice of
converting graphs into natural language for LLMs, which refers to graph
flattening, exhibits good generalizability and interpretability. However, the
poor organization of the textual format results in poor performance in
long-distance scenario understanding. Inspired by human cognitive reasoning
habits, we propose a novel method for graph flattening to fit LLMs, termed as
End-to-End DAG-Path prompting (EEDP). Experiments on real-world datasets show
that EEDP enhances the reasoning performance of LLMs in long-distance scenarios
while maintaining excellent performance in short-distance scenarios,
demonstrating good robustness in the face of distance variations.

æè¦ï¼è¿å¹´ä¾ï¼å¤§åèªè¨æ¨¡å (LLM) ççªç ´çºå¨åå½¢è³æä¸­éæéç¨æ¹æ³æä¾äºæ°æ³æ³ãå°åå½¢è½æçºèªç¶èªè¨ä»¥ä¾ LLM ä½¿ç¨çå¸¸è¦åæ³ï¼å³åå½¢æå¹³åï¼å±ç¾åºè¯å¥½çéç¨æ§åå¯è§£éæ§ãç¶èï¼ææ¬æ ¼å¼çµç¹ä¸ä½³å°è´å¨é·è·é¢å ´æ¯çè§£ä¸­è¡¨ç¾ä¸ä½³ãåå°äººé¡èªç¥æ¨çç¿æ£çåç¼ï¼æåæåºäºä¸ç¨®æ°ç©çæ¹æ³ä¾é²è¡åå½¢æå¹³åä»¥éå LLMï¼ç¨±çºç«¯å°ç«¯ DAG è·¯å¾æç¤º (EEDP)ãå¨çå¯¦ä¸çè³æéä¸çå¯¦é©è¡¨æï¼EEDP å¢å¼·äº LLM å¨é·è·é¢å ´æ¯ä¸­çæ¨çæ§è½ï¼åæå¨ç­è·é¢å ´æ¯ä¸­ä¿æäºåºè²çæ§è½ï¼å¨é¢å°è·é¢è®åæè¡¨ç¾åºè¯å¥½çé­¯æ£æ§ã

##### **RACOON: An LLM-based Framework for Retrieval-Augmented Column Type Annotation with a Knowledge Graph**
2409.14556v1 by Linxi Wei, Guorui Xiao, Magdalena Balazinska

As an important component of data exploration and integration, Column Type
Annotation (CTA) aims to label columns of a table with one or more semantic
types. With the recent development of Large Language Models (LLMs), researchers
have started to explore the possibility of using LLMs for CTA, leveraging their
strong zero-shot capabilities. In this paper, we build on this promising work
and improve on LLM-based methods for CTA by showing how to use a Knowledge
Graph (KG) to augment the context information provided to the LLM. Our
approach, called RACOON, combines both pre-trained parametric and
non-parametric knowledge during generation to improve LLMs' performance on CTA.
Our experiments show that RACOON achieves up to a 0.21 micro F-1 improvement
compared against vanilla LLM inference.

æè¦ï¼ä½çºè³ææ¢åèæ´åçéè¦çµæé¨åï¼æ¬ä½é¡åè¨»è§£ (CTA) çç®æ¨æ¯ä½¿ç¨ä¸åæå¤åèªæé¡åæ¨è¨è¡¨æ ¼æ¬ä½ãé¨èå¤§åèªè¨æ¨¡å (LLM) çè¿æç¼å±ï¼ç ç©¶äººå¡å·²éå§æ¢è¨ä½¿ç¨ LLM ä¾é²è¡ CTA çå¯è½æ§ï¼ä¸¦å©ç¨å¶å¼·å¤§çé¶æ¬¡å­¸ç¿è½åãå¨æ¬æä¸­ï¼æåå»ºç«å¨éåæåæ¯çç ç©¶ä¸ï¼ä¸¦ééå±ç¤ºå¦ä½ä½¿ç¨ç¥è­åè­ (KG) ä¾æ´åæä¾çµ¦ LLM çèçµ¡è³è¨ï¼é²èæ¹ååºæ¼ LLM ç CTA æ¹æ³ãæåçæ¹æ³ç¨±çº RACOONï¼å®å¨çæéç¨ä¸­çµåé åè¨ç·´çåæ¸å¼åéåæ¸å¼ç¥è­ï¼ä»¥æ¹å LLM å¨ CTA ä¸çæè½ãæåçå¯¦é©é¡¯ç¤ºï¼èç´ç²¹ç LLM æ¨è«ç¸æ¯ï¼RACOON å¨å¾®å F-1 ä¸çé²æ­¥é«é 0.21ã

