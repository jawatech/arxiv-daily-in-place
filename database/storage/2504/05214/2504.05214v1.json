{"2504.05214": {"publish_time": "2025-04-07", "title": "Post-Training Language Models for Continual Relation Extraction", "paper_summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.", "paper_summary_zh": "<paragraph>\u73fe\u5be6\u4e16\u754c\u4e2d\u7684\u6578\u64da\uff0c\u4f8b\u5982\u65b0\u805e\u6587\u7ae0\u3001\u793e\u7fa4\u5a92\u9ad4\u8cbc\u6587\u548c\u804a\u5929\u6a5f\u5668\u4eba\u5c0d\u8a71\uff0c\u672c\u8cea\u4e0a\u662f\u52d5\u614b\u4e14\u975e\u975c\u614b\u7684\uff0c\u9019\u5c0d\u900f\u904e\u77e5\u8b58\u5716\u8b5c (KGs) \u69cb\u5efa\u5be6\u6642\u7d50\u69cb\u5316\u8868\u793a\u5f62\u5f0f\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6230\u3002\u95dc\u4fc2\u62bd\u53d6 (RE) \u662f KG \u5275\u5efa\u7684\u57fa\u672c\u7d44\u6210\u90e8\u5206\uff0c\u7576\u50b3\u7d71\u6a21\u578b\u4f9d\u8cf4\u65bc\u975c\u614b\u3001\u904e\u6642\u7684\u6578\u64da\u96c6\u6642\uff0c\u95dc\u4fc2\u62bd\u53d6\u5f80\u5f80\u96e3\u4ee5\u9069\u61c9\u4e0d\u65b7\u8b8a\u5316\u7684\u6578\u64da\u3002\u6301\u7e8c\u95dc\u4fc2\u62bd\u53d6 (CRE) \u65b9\u6cd5\u900f\u904e\u589e\u91cf\u5b78\u7fd2\u65b0\u95dc\u4fc2\u7684\u540c\u6642\u4fdd\u7559\u5148\u524d\u7372\u5f97\u7684\u77e5\u8b58\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLMs)\uff0c\u7279\u5225\u662f\u5927\u8a9e\u8a00\u6a21\u578b (LLMs)\uff0c\u5728 CRE \u4e2d\u7684\u61c9\u7528\uff0c\u91cd\u9ede\u662f\u5229\u7528\u8a18\u61b6\u91cd\u64ad\u4f86\u89e3\u6c7a\u707d\u96e3\u6027\u907a\u5fd8\u554f\u984c\u3002\u6211\u5011\u5728 TACRED \u548c FewRel \u6578\u64da\u96c6\u4e0a\u8a55\u4f30\u4e86\u50c5\u89e3\u78bc\u5668\u6a21\u578b\uff08\u4f8b\u5982 Mistral-7B \u548c Llama2-7B\uff09\u548c\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\uff08\u4f8b\u5982 Flan-T5 Base\uff09\u3002\u5728 TACRED \u4e0a\uff0cLLM \u7684\u4efb\u52d9\u589e\u91cf\u5fae\u8abf\u5c55\u73fe\u51fa\u6bd4\u5148\u524d\u4f7f\u7528\u50cf BERT \u4e4b\u985e\u7684\u50c5\u7de8\u78bc\u5668\u6a21\u578b\u7684\u65b9\u6cd5\u66f4\u512a\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u5df2\u898b\u4efb\u52d9\u6e96\u78ba\u6027\u548c\u6574\u9ad4\u6027\u80fd\uff08\u900f\u904e\u6574\u9ad4\u548c\u5e73\u5747\u6e96\u78ba\u6027\u8861\u91cf\uff09\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528 Mistral \u548c Flan-T5 \u6a21\u578b\u6642\u3002\u5728 FewRel \u4e0a\u7684\u7d50\u679c\u540c\u6a23\u4ee4\u4eba\u632f\u596e\uff0c\u5728\u6574\u9ad4\u548c\u5e73\u5747\u6e96\u78ba\u6027\u6307\u6a19\u4e0a\u5747\u53d6\u5f97\u4e86\u7b2c\u4e8c\u540d\u7684\u4f73\u7e3e\u3002\u9019\u9805\u5de5\u4f5c\u5f37\u8abf\u4e86\u77e5\u8b58\u9077\u79fb\u3001\u8a9e\u8a00\u6a21\u578b\u67b6\u69cb\u548c KG \u5b8c\u6574\u6027\u7684\u95dc\u9375\u56e0\u7d20\uff0c\u63a8\u52d5\u4e86\u4f7f\u7528 LLM \u548c\u8a18\u61b6\u91cd\u64ad\u9032\u884c\u52d5\u614b\u3001\u5be6\u6642\u95dc\u4fc2\u62bd\u53d6\u7684 CRE \u767c\u5c55\u3002 </paragraph>\n", "author": "Sefika Efeoglu et.al.", "authors": "Sefika Efeoglu, Adrian Paschke, Sonja Schimmler", "id": "2504.05214v1", "paper_url": "http://arxiv.org/abs/2504.05214v1", "repo": "null"}}