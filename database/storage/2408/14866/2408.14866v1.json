{"2408.14866": {"publish_time": "2024-08-27", "title": "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "paper_summary": "Language Language Models (LLMs) face safety concerns due to potential misuse\nby malicious users. Recent red-teaming efforts have identified adversarial\nsuffixes capable of jailbreaking LLMs using the gradient-based search algorithm\nGreedy Coordinate Gradient (GCG). However, GCG struggles with computational\ninefficiency, limiting further investigations regarding suffix transferability\nand scalability across models and data. In this work, we bridge the connection\nbetween search efficiency and suffix transferability. We propose a two-stage\ntransfer learning framework, DeGCG, which decouples the search process into\nbehavior-agnostic pre-searching and behavior-relevant post-searching.\nSpecifically, we employ direct first target token optimization in pre-searching\nto facilitate the search process. We apply our approach to cross-model,\ncross-data, and self-transfer scenarios. Furthermore, we introduce an\ninterleaved variant of our approach, i-DeGCG, which iteratively leverages\nself-transferability to accelerate the search process. Experiments on HarmBench\ndemonstrate the efficiency of our approach across various models and domains.\nNotably, our i-DeGCG outperforms the baseline on Llama2-chat-7b with ASRs of\n$43.9$ ($+22.2$) and $39.0$ ($+19.5$) on valid and test sets, respectively.\nFurther analysis on cross-model transfer indicates the pivotal role of first\ntarget token optimization in leveraging suffix transferability for efficient\nsearching.", "paper_summary_zh": "\u8a9e\u8a00\u8a9e\u8a00\u6a21\u578b (LLM) \u56e0\u60e1\u610f\u4f7f\u7528\u8005\u53ef\u80fd\u6feb\u7528\u800c\u9762\u81e8\u5b89\u5168\u554f\u984c\u3002\u6700\u8fd1\u7684\u7d05\u968a\u884c\u52d5\u5df2\u627e\u51fa\u5c0d\u6297\u6027\u5b57\u5c3e\uff0c\u53ef\u4f7f\u7528\u57fa\u65bc\u68af\u5ea6\u7684\u641c\u5c0b\u6f14\u7b97\u6cd5\u8caa\u5a6a\u5ea7\u6a19\u68af\u5ea6 (GCG) \u7834\u89e3 LLM\u3002\u7136\u800c\uff0cGCG \u98fd\u53d7\u904b\u7b97\u6548\u7387\u4e0d\u5f70\u6240\u82e6\uff0c\u9650\u5236\u9032\u4e00\u6b65\u8abf\u67e5\u5b57\u5c3e\u7684\u53ef\u79fb\u690d\u6027\u548c\u8de8\u6a21\u578b\u53ca\u8cc7\u6599\u7684\u53ef\u64f4\u5145\u6027\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u642d\u8d77\u641c\u5c0b\u6548\u7387\u548c\u5b57\u5c3e\u53ef\u79fb\u690d\u6027\u4e4b\u9593\u7684\u6a4b\u6a11\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5169\u968e\u6bb5\u8f49\u79fb\u5b78\u7fd2\u67b6\u69cb DeGCG\uff0c\u5b83\u5c07\u641c\u5c0b\u7a0b\u5e8f\u89e3\u8026\u6210\u8207\u884c\u70ba\u7121\u95dc\u7684\u524d\u7f6e\u641c\u5c0b\u548c\u8207\u884c\u70ba\u76f8\u95dc\u7684\u5f8c\u7f6e\u641c\u5c0b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728\u9810\u5148\u641c\u5c0b\u4e2d\u63a1\u7528\u76f4\u63a5\u7684\u7b2c\u4e00\u500b\u76ee\u6a19\u7b26\u865f\u6700\u4f73\u5316\uff0c\u4ee5\u5229\u641c\u5c0b\u7a0b\u5e8f\u3002\u6211\u5011\u5c07\u6211\u5011\u7684\u505a\u6cd5\u61c9\u7528\u65bc\u8de8\u6a21\u578b\u3001\u8de8\u8cc7\u6599\u548c\u81ea\u6211\u8f49\u79fb\u5834\u666f\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86\u6211\u5011\u505a\u6cd5\u7684\u4ea4\u932f\u8b8a\u9ad4 i-DeGCG\uff0c\u5b83\u53cd\u8986\u5229\u7528\u81ea\u6211\u53ef\u8f49\u79fb\u6027\u4f86\u52a0\u901f\u641c\u5c0b\u7a0b\u5e8f\u3002\u5728 HarmBench \u4e0a\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u5404\u7a2e\u6a21\u578b\u548c\u9818\u57df\u4e2d\u7684\u6548\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684 i-DeGCG \u5728 Llama2-chat-7b \u4e0a\u512a\u65bc\u57fa\u6e96\uff0c\u5728\u9a57\u8b49\u548c\u6e2c\u8a66\u96c6\u4e0a\u7684 ASR \u5206\u5225\u70ba 43.9 (+22.2) \u548c 39.0 (+19.5)\u3002\u8de8\u6a21\u578b\u8f49\u79fb\u7684\u9032\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0c\u7b2c\u4e00\u500b\u76ee\u6a19\u7b26\u865f\u6700\u4f73\u5316\u5728\u5229\u7528\u5b57\u5c3e\u53ef\u79fb\u690d\u6027\u4ee5\u9032\u884c\u6709\u6548\u641c\u5c0b\u4e2d\u626e\u6f14\u95dc\u9375\u89d2\u8272\u3002", "author": "Hongfu Liu et.al.", "authors": "Hongfu Liu, Yuxi Xie, Ye Wang, Michael Shieh", "id": "2408.14866v1", "paper_url": "http://arxiv.org/abs/2408.14866v1", "repo": "null"}}