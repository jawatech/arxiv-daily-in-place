{"2406.03496": {"publish_time": "2024-06-05", "title": "Wings: Learning Multimodal LLMs without Text-only Forgetting", "paper_summary": "Multimodal large language models (MLLMs), initiated with a trained LLM, first\nalign images with text and then fine-tune on multimodal mixed inputs. However,\nthe MLLM catastrophically forgets the text-only instructions, which do not\ninclude images and can be addressed within the initial LLM. In this paper, we\npresent Wings, a novel MLLM that excels in both text-only dialogues and\nmultimodal comprehension. Analyzing MLLM attention in multimodal instructions\nreveals that text-only forgetting is related to the attention shifts from\npre-image to post-image text. From that, we construct extra modules that act as\nthe boosted learner to compensate for the attention shift. The complementary\nvisual and textual learners, like \"wings\" on either side, are connected in\nparallel within each layer's attention block. Initially, image and text inputs\nare aligned with visual learners operating alongside the main attention,\nbalancing focus on visual elements. Textual learners are later collaboratively\nintegrated with attention-based routing to blend the outputs of the visual and\ntextual learners. We design the Low-Rank Residual Attention (LoRRA) to\nguarantee high efficiency for learners. Our experimental results demonstrate\nthat Wings outperforms equally-scaled MLLMs in both text-only and visual\nquestion-answering tasks. On a newly constructed Interleaved Image-Text (IIT)\nbenchmark, Wings exhibits superior performance from text-only-rich to\nmultimodal-rich question-answering tasks.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM) \u4ece\u8bad\u7ec3\u8fc7\u7684 LLM \u5f00\u59cb\uff0c\u9996\u5148\u5c06\u56fe\u50cf\u4e0e\u6587\u672c\u5bf9\u9f50\uff0c\u7136\u540e\u9488\u5bf9\u591a\u6a21\u6001\u6df7\u5408\u8f93\u5165\u8fdb\u884c\u5fae\u8c03\u3002\u7136\u800c\uff0cMLLM \u707e\u96be\u6027\u5730\u5fd8\u8bb0\u4e86\u4ec5\u6587\u672c\u7684\u6307\u4ee4\uff0c\u5176\u4e2d\u4e0d\u5305\u62ec\u56fe\u50cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u521d\u59cb LLM \u4e2d\u89e3\u51b3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c55\u793a\u4e86 Wings\uff0c\u4e00\u79cd\u65b0\u9896\u7684 MLLM\uff0c\u5b83\u5728\u4ec5\u6587\u672c\u5bf9\u8bdd\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u90fd\u975e\u5e38\u51fa\u8272\u3002\u5206\u6790\u591a\u6a21\u6001\u6307\u4ee4\u4e2d\u7684 MLLM \u6ce8\u610f\u529b\u63ed\u793a\u4e86\u4ec5\u6587\u672c\u9057\u5fd8\u4e0e\u6ce8\u610f\u529b\u4ece\u56fe\u50cf\u524d\u6587\u672c\u8f6c\u79fb\u5230\u56fe\u50cf\u540e\u6587\u672c\u6709\u5173\u3002\u7531\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u5145\u5f53\u589e\u5f3a\u5b66\u4e60\u5668\u7684\u989d\u5916\u6a21\u5757\u6765\u8865\u507f\u6ce8\u610f\u529b\u8f6c\u79fb\u3002\u4e92\u8865\u7684\u89c6\u89c9\u548c\u6587\u672c\u5b66\u4e60\u5668\uff0c\u5c31\u50cf\u4e24\u4fa7\u7684\u201c\u7fc5\u8180\u201d\uff0c\u5728\u6bcf\u4e2a\u5c42\u7684\u6ce8\u610f\u529b\u5757\u5185\u5e76\u884c\u8fde\u63a5\u3002\u6700\u521d\uff0c\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\u4e0e\u89c6\u89c9\u5b66\u4e60\u5668\u5bf9\u9f50\uff0c\u4e0e\u4e3b\u8981\u6ce8\u610f\u529b\u4e00\u8d77\u64cd\u4f5c\uff0c\u5e73\u8861\u5bf9\u89c6\u89c9\u5143\u7d20\u7684\u5173\u6ce8\u3002\u6587\u672c\u5b66\u4e60\u5668\u540e\u6765\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8def\u7531\u534f\u4f5c\u96c6\u6210\uff0c\u4ee5\u6df7\u5408\u89c6\u89c9\u548c\u6587\u672c\u5b66\u4e60\u5668\u7684\u8f93\u51fa\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4f4e\u79e9\u6b8b\u5dee\u6ce8\u610f\u529b (LoRRA) \u6765\u4fdd\u8bc1\u5b66\u4e60\u5668\u7684\u6548\u7387\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWings \u5728\u4ec5\u6587\u672c\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u89c4\u6a21\u76f8\u540c\u7684 MLLM\u3002\u5728\u65b0\u5efa\u7684\u4ea4\u9519\u56fe\u50cf\u6587\u672c (IIT) \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWings \u4ece\u4ec5\u6587\u672c\u4e30\u5bcc\u7684\u95ee\u7b54\u4efb\u52a1\u5230\u591a\u6a21\u6001\u4e30\u5bcc\u7684\u95ee\u7b54\u4efb\u52a1\u90fd\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "author": "Yi-Kai Zhang et.al.", "authors": "Yi-Kai Zhang, Shiyin Lu, Yang Li, Yanqing Ma, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye", "id": "2406.03496v1", "paper_url": "http://arxiv.org/abs/2406.03496v1", "repo": "null"}}