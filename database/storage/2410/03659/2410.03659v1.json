{"2410.03659": {"publish_time": "2024-10-04", "title": "Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models", "paper_summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive\ncapabilities for capturing and reasoning over multimodal inputs. However, these\nmodels are prone to parametric knowledge conflicts, which arise from\ninconsistencies of represented knowledge between their vision and language\ncomponents. In this paper, we formally define the problem of\n$\\textbf{cross-modality parametric knowledge conflict}$ and present a\nsystematic approach to detect, interpret, and mitigate them. We introduce a\npipeline that identifies conflicts between visual and textual answers, showing\na persistently high conflict rate across modalities in recent LVLMs regardless\nof the model size. We further investigate how these conflicts interfere with\nthe inference process and propose a contrastive metric to discern the\nconflicting samples from the others. Building on these insights, we develop a\nnovel dynamic contrastive decoding method that removes undesirable logits\ninferred from the less confident modality components based on answer\nconfidence. For models that do not provide logits, we also introduce two\nprompt-based strategies to mitigate the conflicts. Our methods achieve\npromising improvements in accuracy on both the ViQuAE and InfoSeek datasets.\nSpecifically, using LLaVA-34B, our proposed dynamic contrastive decoding\nimproves an average accuracy of 2.24%.", "paper_summary_zh": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u5df2\u5c55\u793a\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u7528\u4e8e\u6355\u6349\u548c\u63a8\u7406\u591a\u6a21\u6001\u8f93\u5165\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5bb9\u6613\u51fa\u73b0\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81\uff0c\u8fd9\u662f\u7531\u4e8e\u5176\u89c6\u89c9\u548c\u8bed\u8a00\u7ec4\u4ef6\u4e4b\u95f4\u6240\u8868\u793a\u77e5\u8bc6\u7684\u4e0d\u4e00\u81f4\u6027\u9020\u6210\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6b63\u5f0f\u5b9a\u4e49\u4e86$\\textbf{\u8de8\u6a21\u6001\u53c2\u6570\u77e5\u8bc6\u51b2\u7a81}$\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u7f13\u89e3\u5b83\u4eec\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7ba1\u9053\uff0c\u7528\u4e8e\u8bc6\u522b\u89c6\u89c9\u548c\u6587\u672c\u7b54\u6848\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u663e\u793a\u4e86\u5728\u6700\u8fd1\u7684 LVLMs \u4e2d\uff0c\u65e0\u8bba\u6a21\u578b\u5927\u5c0f\u5982\u4f55\uff0c\u8de8\u6a21\u6001\u7684\u51b2\u7a81\u7387\u6301\u7eed\u5c45\u9ad8\u4e0d\u4e0b\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u8fd9\u4e9b\u51b2\u7a81\u5982\u4f55\u5e72\u6270\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5bf9\u6bd4\u5ea6\u91cf\u6765\u533a\u5206\u51b2\u7a81\u6837\u672c\u548c\u5176\u4ed6\u6837\u672c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6839\u636e\u7b54\u6848\u7f6e\u4fe1\u5ea6\uff0c\u5220\u9664\u4e86\u4ece\u4e0d\u592a\u81ea\u4fe1\u7684\u6a21\u6001\u7ec4\u4ef6\u63a8\u65ad\u51fa\u7684\u4e0d\u826f logit\u3002\u5bf9\u4e8e\u4e0d\u63d0\u4f9b logit \u7684\u6a21\u578b\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e24\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u7b56\u7565\u6765\u7f13\u89e3\u51b2\u7a81\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728 ViQuAE \u548c InfoSeek \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6709\u5e0c\u671b\u7684\u51c6\u786e\u6027\u6539\u8fdb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f7f\u7528 LLaVA-34B\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u52a8\u6001\u5bf9\u6bd4\u89e3\u7801\u5c06\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 2.24%\u3002", "author": "Tinghui Zhu et.al.", "authors": "Tinghui Zhu, Qin Liu, Fei Wang, Zhengzhong Tu, Muhao Chen", "id": "2410.03659v1", "paper_url": "http://arxiv.org/abs/2410.03659v1", "repo": "null"}}