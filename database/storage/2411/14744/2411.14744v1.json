{"2411.14744": {"publish_time": "2024-11-22", "title": "Point Cloud Understanding via Attention-Driven Contrastive Learning", "paper_summary": "Recently Transformer-based models have advanced point cloud understanding by\nleveraging self-attention mechanisms, however, these methods often overlook\nlatent information in less prominent regions, leading to increased sensitivity\nto perturbations and limited global comprehension. To solve this issue, we\nintroduce PointACL, an attention-driven contrastive learning framework designed\nto address these limitations. Our method employs an attention-driven dynamic\nmasking strategy that guides the model to focus on under-attended regions,\nenhancing the understanding of global structures within the point cloud. Then\nwe combine the original pre-training loss with a contrastive learning loss,\nimproving feature discrimination and generalization. Extensive experiments\nvalidate the effectiveness of PointACL, as it achieves state-of-the-art\nperformance across a variety of 3D understanding tasks, including object\nclassification, part segmentation, and few-shot learning. Specifically, when\nintegrated with different Transformer backbones like Point-MAE and PointGPT,\nPointACL demonstrates improved performance on datasets such as ScanObjectNN,\nModelNet40, and ShapeNetPart. This highlights its superior capability in\ncapturing both global and local features, as well as its enhanced robustness\nagainst perturbations and incomplete data.", "paper_summary_zh": "\u8fd1\u671f\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u901a\u8fc7\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u4e86\u70b9\u4e91\u7406\u89e3\uff0c\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7ecf\u5e38\u5ffd\u7565\u4e0d\u663e\u7740\u533a\u57df\u4e2d\u7684\u6f5c\u5728\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u6270\u52a8\u7684\u654f\u611f\u6027\u589e\u52a0\u4e14\u5168\u5c40\u7406\u89e3\u53d7\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 PointACL\uff0c\u4e00\u4e2a\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u4e2a\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u52a8\u6001\u63a9\u853d\u7b56\u7565\uff0c\u5f15\u5bfc\u6a21\u578b\u4e13\u6ce8\u4e8e\u672a\u88ab\u5145\u5206\u5173\u6ce8\u7684\u533a\u57df\uff0c\u589e\u5f3a\u5bf9\u70b9\u4e91\u4e2d\u5168\u5c40\u7ed3\u6784\u7684\u7406\u89e3\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u539f\u59cb\u7684\u9884\u8bad\u7ec3\u635f\u5931\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u7279\u5f81\u8fa8\u522b\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 PointACL \u7684\u6709\u6548\u6027\uff0c\u56e0\u4e3a\u5b83\u5728\u5404\u79cd 3D \u7406\u89e3\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5bf9\u8c61\u5206\u7c7b\u3001\u90e8\u5206\u5206\u5272\u548c\u5c11\u91cf\u5b66\u4e60\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f53\u4e0e\u4e0d\u540c\u7684 Transformer \u4e3b\u5e72\uff08\u5982 Point-MAE \u548c PointGPT\uff09\u96c6\u6210\u65f6\uff0cPointACL \u5728 ScanObjectNN\u3001ModelNet40 \u548c ShapeNetPart \u7b49\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u6027\u80fd\u3002\u8fd9\u7a81\u51fa\u4e86\u5176\u5728\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u5bf9\u6270\u52a8\u548c\u4e0d\u5b8c\u6574\u6570\u636e\u7684\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "author": "Yi Wang et.al.", "authors": "Yi Wang, Jiaze Wang, Ziyu Guo, Renrui Zhang, Donghao Zhou, Guangyong Chen, Anfeng Liu, Pheng-Ann Heng", "id": "2411.14744v1", "paper_url": "http://arxiv.org/abs/2411.14744v1", "repo": "null"}}