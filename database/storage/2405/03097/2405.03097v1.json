{"2405.03097": {"publish_time": "2024-05-06", "title": "To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models", "paper_summary": "LLMs have been found to memorize training textual sequences and regurgitate\nverbatim said sequences during text generation time. This fact is known to be\nthe cause of privacy and related (e.g., copyright) problems. Unlearning in LLMs\nthen takes the form of devising new algorithms that will properly deal with\nthese side-effects of memorized data, while not hurting the model's utility. We\noffer a fresh perspective towards this goal, namely, that each textual sequence\nto be forgotten should be treated differently when being unlearned based on its\ndegree of memorization within the LLM. We contribute a new metric for measuring\nunlearning quality, an adversarial attack showing that SOTA algorithms lacking\nthis perspective fail for privacy, and two new unlearning methods based on\nGradient Ascent and Task Arithmetic, respectively. A comprehensive performance\nevaluation across an extensive suite of NLP tasks then mapped the solution\nspace, identifying the best solutions under different scales in model\ncapacities and forget set sizes and quantified the gains of the new approaches.", "paper_summary_zh": "", "author": "George-Octavian Barbulescu et.al.", "authors": "George-Octavian Barbulescu,Peter Triantafillou", "id": "2405.03097v1", "paper_url": "http://arxiv.org/abs/2405.03097v1", "repo": "null"}}