{"2407.19619": {"publish_time": "2024-07-29", "title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation", "paper_summary": "The advent of large language models (LLMs) has significantly advanced the\nfield of code translation, enabling automated translation between programming\nlanguages. However, these models often struggle with complex translation tasks\ndue to inadequate contextual understanding. This paper introduces a novel\napproach that enhances code translation through Few-Shot Learning, augmented\nwith retrieval-based techniques. By leveraging a repository of existing code\ntranslations, we dynamically retrieve the most relevant examples to guide the\nmodel in translating new code segments. Our method, based on\nRetrieval-Augmented Generation (RAG), substantially improves translation\nquality by providing contextual examples from which the model can learn in\nreal-time. We selected RAG over traditional fine-tuning methods due to its\nability to utilize existing codebases or a locally stored corpus of code, which\nallows for dynamic adaptation to diverse translation tasks without extensive\nretraining. Extensive experiments on diverse datasets with open LLM models such\nas Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code\nInstruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5\nTurbo and GPT-4o, demonstrate our approach's superiority over traditional\nzero-shot methods, especially in translating between Fortran and CPP. We also\nexplored varying numbers of shots i.e. examples provided during inference,\nspecifically 1, 2, and 3 shots and different embedding models for RAG,\nincluding Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and\neffectiveness of our approach.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\u5927\u5e45\u63a8\u52d5\u4e86\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u9818\u57df\uff0c\u5be6\u73fe\u4e86\u7a0b\u5f0f\u8a9e\u8a00\u4e4b\u9593\u7684\u81ea\u52d5\u7ffb\u8b6f\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u5728\u8655\u7406\u8907\u96dc\u7684\u7ffb\u8b6f\u4efb\u52d9\u6642\u5e38\u5e38\u6703\u9047\u5230\u56f0\u96e3\uff0c\u539f\u56e0\u662f\u5c0d\u8a9e\u5883\u7684\u7406\u89e3\u4e0d\u8db3\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u900f\u904e\u5c11\u91cf\u6b21\u5b78\u7fd2\u589e\u5f37\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\uff0c\u4e26\u8f14\u4ee5\u57fa\u65bc\u6aa2\u7d22\u7684\u6280\u8853\u3002\u900f\u904e\u5229\u7528\u73fe\u6709\u7a0b\u5f0f\u78bc\u7ffb\u8b6f\u7684\u5132\u5b58\u5eab\uff0c\u6211\u5011\u52d5\u614b\u6aa2\u7d22\u6700\u76f8\u95dc\u7684\u7bc4\u4f8b\uff0c\u4ee5\u6307\u5c0e\u6a21\u578b\u7ffb\u8b6f\u65b0\u7684\u7a0b\u5f0f\u78bc\u5340\u6bb5\u3002\u6211\u5011\u7684\u9019\u9805\u65b9\u6cd5\u4ee5\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u70ba\u57fa\u790e\uff0c\u900f\u904e\u63d0\u4f9b\u6a21\u578b\u53ef\u4ee5\u5f9e\u4e2d\u5373\u6642\u5b78\u7fd2\u7684\u8a9e\u5883\u7bc4\u4f8b\uff0c\u5927\u5e45\u63d0\u5347\u7ffb\u8b6f\u54c1\u8cea\u3002\u6211\u5011\u9078\u64c7 RAG \u800c\u975e\u50b3\u7d71\u7684\u5fae\u8abf\u65b9\u6cd5\uff0c\u539f\u56e0\u5728\u65bc\u5b83\u80fd\u5920\u5229\u7528\u73fe\u6709\u7684\u7a0b\u5f0f\u78bc\u5eab\u6216\u672c\u5730\u5132\u5b58\u7684\u7a0b\u5f0f\u78bc\u8a9e\u6599\u5eab\uff0c\u9019\u8b93\u6211\u5011\u5f97\u4ee5\u52d5\u614b\u9069\u61c9\u4e0d\u540c\u7684\u7ffb\u8b6f\u4efb\u52d9\uff0c\u800c\u7121\u9700\u9032\u884c\u5927\u91cf\u7684\u91cd\u65b0\u8a13\u7df4\u3002\u5728\u4f7f\u7528 Starcoder\u3001Llama3-70B Instruct\u3001CodeLlama-34B Instruct\u3001Granite-34B Code Instruct \u548c Mixtral-8x22B \u7b49\u958b\u653e\u5f0f LLM \u6a21\u578b\uff0c\u4ee5\u53ca GPT-3.5 Turbo \u548c GPT-4o \u7b49\u5546\u696d LLM \u6a21\u578b\u7684\u5404\u7a2e\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\uff0c\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u512a\u65bc\u50b3\u7d71\u7684\u96f6\u6b21\u5b78\u7fd2\u65b9\u6cd5\uff0c\u7279\u5225\u662f\u5728 Fortran \u548c CPP \u4e4b\u9593\u7684\u7ffb\u8b6f\u3002\u6211\u5011\u9084\u63a2\u7d22\u4e86\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u63d0\u4f9b\u7684\u7bc4\u4f8b\u6578\u91cf\u7684\u8b8a\u5316\uff0c\u7279\u5225\u662f 1\u30012 \u548c 3 \u500b\u7bc4\u4f8b\uff0c\u4ee5\u53ca RAG \u7684\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\uff0c\u5305\u62ec Nomic-Embed\u3001Starencoder \u548c CodeBERT\uff0c\u4ee5\u8a55\u4f30\u6211\u5011\u65b9\u6cd5\u7684\u7a69\u5065\u6027\u548c\u6709\u6548\u6027\u3002", "author": "Manish Bhattarai et.al.", "authors": "Manish Bhattarai, Javier E. Santos, Shawn Jones, Ayan Biswas, Boian Alexandrov, Daniel O'Malley", "id": "2407.19619v1", "paper_url": "http://arxiv.org/abs/2407.19619v1", "repo": "null"}}