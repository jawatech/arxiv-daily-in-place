{"2407.03282": {"publish_time": "2024-07-03", "title": "LLM Internal States Reveal Hallucination Risk Faced With a Query", "paper_summary": "The hallucination problem of Large Language Models (LLMs) significantly\nlimits their reliability and trustworthiness. Humans have a self-awareness\nprocess that allows us to recognize what we don't know when faced with queries.\nInspired by this, our paper investigates whether LLMs can estimate their own\nhallucination risk before response generation. We analyze the internal\nmechanisms of LLMs broadly both in terms of training data sources and across 15\ndiverse Natural Language Generation (NLG) tasks, spanning over 700 datasets.\nOur empirical analysis reveals two key insights: (1) LLM internal states\nindicate whether they have seen the query in training data or not; and (2) LLM\ninternal states show they are likely to hallucinate or not regarding the query.\nOur study explores particular neurons, activation layers, and tokens that play\na crucial role in the LLM perception of uncertainty and hallucination risk. By\na probing estimator, we leverage LLM self-assessment, achieving an average\nhallucination estimation accuracy of 84.32\\% at run time.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7b\u89ba\u554f\u984c\u986f\u8457\u5730\u9650\u5236\u4e86\u5b83\u5011\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u4eba\u985e\u5177\u5099\u4e00\u7a2e\u81ea\u6211\u89ba\u5bdf\u7684\u904e\u7a0b\uff0c\u9019\u8b93\u6211\u5011\u5728\u9762\u5c0d\u554f\u984c\u6642\u80fd\u5920\u8a8d\u77e5\u5230\u81ea\u5df1\u7684\u7121\u77e5\u3002\u53d7\u5230\u6b64\u555f\u767c\uff0c\u6211\u5011\u7684\u8ad6\u6587\u63a2\u8a0e LLM \u662f\u5426\u53ef\u4ee5\u5728\u7522\u751f\u56de\u61c9\u4e4b\u524d\u8a55\u4f30\u5b83\u5011\u81ea\u5df1\u7684\u5e7b\u89ba\u98a8\u96aa\u3002\u6211\u5011\u5ee3\u6cdb\u5730\u5206\u6790\u4e86 LLM \u7684\u5167\u90e8\u6a5f\u5236\uff0c\u5305\u62ec\u8a13\u7df4\u8cc7\u6599\u4f86\u6e90\u548c\u6a6b\u8de8 15 \u500b\u591a\u5143\u7684\u81ea\u7136\u8a9e\u8a00\u751f\u6210\uff08NLG\uff09\u4efb\u52d9\uff0c\u6db5\u84cb\u8d85\u904e 700 \u500b\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7684\u5be6\u8b49\u5206\u6790\u63ed\u9732\u4e86\u5169\u500b\u95dc\u9375\u7684\u898b\u89e3\uff1a\uff081\uff09LLM \u5167\u90e8\u72c0\u614b\u986f\u793a\u5b83\u5011\u662f\u5426\u5728\u8a13\u7df4\u8cc7\u6599\u4e2d\u898b\u904e\u8a72\u554f\u984c\uff1b\u4ee5\u53ca\uff082\uff09LLM \u5167\u90e8\u72c0\u614b\u986f\u793a\u5b83\u5011\u662f\u5426\u53ef\u80fd\u5c0d\u8a72\u554f\u984c\u7522\u751f\u5e7b\u89ba\u3002\u6211\u5011\u7684\u7814\u7a76\u63a2\u8a0e\u4e86\u7279\u5b9a\u795e\u7d93\u5143\u3001\u6fc0\u6d3b\u5c64\u548c\u7b26\u865f\uff0c\u9019\u4e9b\u5728 LLM \u5c0d\u4e0d\u78ba\u5b9a\u6027\u548c\u5e7b\u89ba\u98a8\u96aa\u7684\u611f\u77e5\u4e2d\u626e\u6f14\u4e86\u95dc\u9375\u89d2\u8272\u3002\u85c9\u7531\u4e00\u500b\u63a2\u6e2c\u4f30\u8a08\u5668\uff0c\u6211\u5011\u5229\u7528 LLM \u81ea\u6211\u8a55\u4f30\uff0c\u5728\u57f7\u884c\u6642\u9054\u6210\u5e73\u5747 84.32% \u7684\u5e7b\u89ba\u4f30\u8a08\u6e96\u78ba\u5ea6\u3002", "author": "Ziwei Ji et.al.", "authors": "Ziwei Ji, Delong Chen, Etsuko Ishii, Samuel Cahyawijaya, Yejin Bang, Bryan Wilie, Pascale Fung", "id": "2407.03282v1", "paper_url": "http://arxiv.org/abs/2407.03282v1", "repo": "null"}}