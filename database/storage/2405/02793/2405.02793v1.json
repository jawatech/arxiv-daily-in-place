{"2405.02793": {"publish_time": "2024-05-05", "title": "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "paper_summary": "Despite the longstanding adage \"an image is worth a thousand words,\" creating\naccurate and hyper-detailed image descriptions for training Vision-Language\nmodels remains challenging. Current datasets typically have web-scraped\ndescriptions that are short, low-granularity, and often contain details\nunrelated to the visual content. As a result, models trained on such data\ngenerate descriptions replete with missing information, visual inconsistencies,\nand hallucinations. To address these issues, we introduce ImageInWords (IIW), a\ncarefully designed human-in-the-loop annotation framework for curating\nhyper-detailed image descriptions and a new dataset resulting from this\nprocess. We validate the framework through evaluations focused on the quality\nof the dataset and its utility for fine-tuning with considerations for\nreadability, comprehensiveness, specificity, hallucinations, and\nhuman-likeness. Our dataset significantly improves across these dimensions\ncompared to recently released datasets (+66%) and GPT-4V outputs (+48%).\nFurthermore, models fine-tuned with IIW data excel by +31% against prior work\nalong the same human evaluation dimensions. Given our fine-tuned models, we\nalso evaluate text-to-image generation and vision-language reasoning. Our\nmodel's descriptions can generate images closest to the original, as judged by\nboth automated and human metrics. We also find our model produces more\ncompositionally rich descriptions, outperforming the best baseline by up to 6%\non ARO, SVO-Probes, and Winoground datasets.", "paper_summary_zh": "", "author": "Roopal Garg et.al.", "authors": "Roopal Garg,Andrea Burns,Burcu Karagol Ayan,Yonatan Bitton,Ceslee Montgomery,Yasumasa Onoe,Andrew Bunner,Ranjay Krishna,Jason Baldridge,Radu Soricut", "id": "2405.02793v1", "paper_url": "http://arxiv.org/abs/2405.02793v1", "repo": "https://github.com/google/imageinwords"}}