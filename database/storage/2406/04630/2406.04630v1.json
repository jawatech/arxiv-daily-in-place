{"2406.04630": {"publish_time": "2024-06-07", "title": "Low-Resource Cross-Lingual Summarization through Few-Shot Learning with Large Language Models", "paper_summary": "Cross-lingual summarization (XLS) aims to generate a summary in a target\nlanguage different from the source language document. While large language\nmodels (LLMs) have shown promising zero-shot XLS performance, their few-shot\ncapabilities on this task remain unexplored, especially for low-resource\nlanguages with limited parallel data. In this paper, we investigate the\nfew-shot XLS performance of various models, including Mistral-7B-Instruct-v0.2,\nGPT-3.5, and GPT-4. Our experiments demonstrate that few-shot learning\nsignificantly improves the XLS performance of LLMs, particularly GPT-3.5 and\nGPT-4, in low-resource settings. However, the open-source model\nMistral-7B-Instruct-v0.2 struggles to adapt effectively to the XLS task with\nlimited examples. Our findings highlight the potential of few-shot learning for\nimproving XLS performance and the need for further research in designing LLM\narchitectures and pre-training objectives tailored for this task. We provide a\nfuture work direction to explore more effective few-shot learning strategies\nand to investigate the transfer learning capabilities of LLMs for cross-lingual\nsummarization.", "paper_summary_zh": "\u8de8\u8a9e\u8a00\u6458\u8981 (XLS) \u65e8\u5728\u4ee5\u8207\u539f\u59cb\u8a9e\u8a00\u6587\u4ef6\u4e0d\u540c\u7684\u76ee\u6a19\u8a9e\u8a00\u7522\u751f\u6458\u8981\u3002\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u6709\u524d\u666f\u7684\u96f6\u6b21\u5b78\u7fd2 XLS \u6548\u80fd\uff0c\u4f46\u5b83\u5011\u5728\u9019\u500b\u4efb\u52d9\u4e0a\u7684\u5c11\u91cf\u5b78\u7fd2\u80fd\u529b\u4ecd\u672a\u88ab\u63a2\u7d22\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u5e73\u884c\u8cc7\u6599\u6709\u9650\u7684\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5404\u7a2e\u6a21\u578b\u7684\u5c11\u91cf\u5b78\u7fd2 XLS \u6548\u80fd\uff0c\u5305\u62ec Mistral-7B-Instruct-v0.2\u3001GPT-3.5 \u548c GPT-4\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u5c11\u91cf\u5b78\u7fd2\u986f\u8457\u63d0\u5347\u4e86 LLM \u7684 XLS \u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u4f4e\u8cc7\u6e90\u8a2d\u5b9a\u4e2d\u7684 GPT-3.5 \u548c GPT-4\u3002\u7136\u800c\uff0c\u958b\u6e90\u6a21\u578b Mistral-7B-Instruct-v0.2 \u96e3\u4ee5\u6709\u6548\u9069\u61c9\u7bc4\u4f8b\u6709\u9650\u7684 XLS \u4efb\u52d9\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u7a81\u986f\u4e86\u5c11\u91cf\u5b78\u7fd2\u5728\u63d0\u5347 XLS \u6548\u80fd\u65b9\u9762\u7684\u6f5b\u529b\uff0c\u4ee5\u53ca\u9032\u4e00\u6b65\u7814\u7a76\u8a2d\u8a08\u91dd\u5c0d\u6b64\u4efb\u52d9\u91cf\u8eab\u6253\u9020\u7684 LLM \u67b6\u69cb\u548c\u9810\u8a13\u7df4\u76ee\u6a19\u7684\u5fc5\u8981\u6027\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u672a\u4f86\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a2\u7d22\u66f4\u6709\u6548\u7684\u5c11\u91cf\u5b78\u7fd2\u7b56\u7565\uff0c\u4e26\u63a2\u8a0e LLM \u5728\u8de8\u8a9e\u8a00\u6458\u8981\u4e2d\u7684\u8f49\u79fb\u5b78\u7fd2\u80fd\u529b\u3002", "author": "Gyutae Park et.al.", "authors": "Gyutae Park, Seojin Hwang, Hwanhee Lee", "id": "2406.04630v1", "paper_url": "http://arxiv.org/abs/2406.04630v1", "repo": "null"}}