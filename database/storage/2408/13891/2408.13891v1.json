{"2408.13891": {"publish_time": "2024-08-25", "title": "SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning", "paper_summary": "Instruction-based speech processing is becoming popular. Studies show that\ntraining with multiple tasks boosts performance, but collecting diverse,\nlarge-scale tasks and datasets is expensive. Thus, it is highly desirable to\ndesign a fundamental task that benefits other downstream tasks. This paper\nintroduces a multi-talker speaking style captioning task to enhance the\nunderstanding of speaker and prosodic information. We used large language\nmodels to generate descriptions for multi-talker speech. Then, we trained our\nmodel with pre-training on this captioning task followed by instruction tuning.\nEvaluation on Dynamic-SUPERB shows our model outperforming the baseline\npre-trained only on single-talker tasks, particularly in speaker and emotion\nrecognition. Additionally, tests on a multi-talker QA task reveal that current\nmodels struggle with attributes such as gender, pitch, and speaking rate. The\ncode and dataset are available at https://github.com/cyhuang-tw/speechcaps.", "paper_summary_zh": "\u57fa\u65bc\u6307\u4ee4\u7684\u8a9e\u97f3\u8655\u7406\u6b63\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u666e\u53ca\u3002\u7814\u7a76\u986f\u793a\uff0c\u900f\u904e\u591a\u4efb\u52d9\u8a13\u7df4\u53ef\u4ee5\u63d0\u5347\u6548\u80fd\uff0c\u4f46\u6536\u96c6\u591a\u6a23\u4e14\u5927\u898f\u6a21\u7684\u4efb\u52d9\u548c\u8cc7\u6599\u96c6\u6210\u672c\u6602\u8cb4\u3002\u56e0\u6b64\uff0c\u8a2d\u8a08\u4e00\u500b\u80fd\u8b93\u5176\u4ed6\u4e0b\u6e38\u4efb\u52d9\u53d7\u76ca\u7684\u57fa\u672c\u4efb\u52d9\u975e\u5e38\u91cd\u8981\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u9805\u591a\u8aaa\u8a71\u8005\u8aaa\u8a71\u98a8\u683c\u5b57\u5e55\u4efb\u52d9\uff0c\u4ee5\u589e\u5f37\u5c0d\u8aaa\u8a71\u8005\u548c\u97fb\u5f8b\u8cc7\u8a0a\u7684\u7406\u89e3\u3002\u6211\u5011\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u70ba\u591a\u8aaa\u8a71\u8005\u8a9e\u97f3\u7522\u751f\u63cf\u8ff0\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u5728\u9019\u500b\u5b57\u5e55\u4efb\u52d9\u4e0a\u7684\u9810\u8a13\u7df4\u8a13\u7df4\u6211\u5011\u7684\u6a21\u578b\uff0c\u63a5\u8457\u9032\u884c\u6307\u4ee4\u5fae\u8abf\u3002\u5728 Dynamic-SUPERB \u4e0a\u7684\u8a55\u4f30\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u512a\u65bc\u50c5\u5728\u55ae\u4e00\u8aaa\u8a71\u8005\u4efb\u52d9\u4e0a\u9032\u884c\u9810\u8a13\u7df4\u7684\u57fa\u6e96\uff0c\u7279\u5225\u662f\u5728\u8aaa\u8a71\u8005\u548c\u60c5\u7dd2\u8fa8\u8b58\u65b9\u9762\u3002\u6b64\u5916\uff0c\u5728\u591a\u8aaa\u8a71\u8005\u554f\u7b54\u4efb\u52d9\u4e0a\u7684\u6e2c\u8a66\u986f\u793a\uff0c\u76ee\u524d\u7684\u6a21\u578b\u5728\u6027\u5225\u3001\u97f3\u9ad8\u548c\u8aaa\u8a71\u901f\u5ea6\u7b49\u5c6c\u6027\u4e0a\u4ecd\u6709\u56f0\u96e3\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u96c6\u53ef\u5728 https://github.com/cyhuang-tw/speechcaps \u53d6\u5f97\u3002", "author": "Chien-yu Huang et.al.", "authors": "Chien-yu Huang, Min-Han Shih, Ke-Han Lu, Chi-Yuan Hsiao, Hung-yi Lee", "id": "2408.13891v1", "paper_url": "http://arxiv.org/abs/2408.13891v1", "repo": "null"}}