{"2410.23234": {"publish_time": "2024-10-30", "title": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning", "paper_summary": "This paper introduces a framework, called EMOTION, for generating expressive\nmotion sequences in humanoid robots, enhancing their ability to engage in\nhumanlike non-verbal communication. Non-verbal cues such as facial expressions,\ngestures, and body movements play a crucial role in effective interpersonal\ninteractions. Despite the advancements in robotic behaviors, existing methods\noften fall short in mimicking the diversity and subtlety of human non-verbal\ncommunication. To address this gap, our approach leverages the in-context\nlearning capability of large language models (LLMs) to dynamically generate\nsocially appropriate gesture motion sequences for human-robot interaction. We\nuse this framework to generate 10 different expressive gestures and conduct\nonline user studies comparing the naturalness and understandability of the\nmotions generated by EMOTION and its human-feedback version, EMOTION++, against\nthose by human operators. The results demonstrate that our approach either\nmatches or surpasses human performance in generating understandable and natural\nrobot motions under certain scenarios. We also provide design implications for\nfuture research to consider a set of variables when generating expressive\nrobotic gestures.", "paper_summary_zh": "\u672c\u6587\u4ecb\u7d39\u4e86\u4e00\u500b\u540d\u70ba EMOTION \u7684\u6846\u67b6\uff0c\u53ef\u7528\u65bc\u5728\u985e\u4eba\u6a5f\u5668\u4eba\u4e2d\u751f\u6210\u8868\u9054\u6027\u7684\u52d5\u4f5c\u5e8f\u5217\uff0c\u589e\u5f37\u5176\u53c3\u8207\u985e\u4eba\u975e\u8a9e\u8a00\u6e9d\u901a\u7684\u80fd\u529b\u3002\u975e\u8a9e\u8a00\u7dda\u7d22\uff08\u4f8b\u5982\u9762\u90e8\u8868\u60c5\u3001\u624b\u52e2\u548c\u8eab\u9ad4\u52d5\u4f5c\uff09\u5728\u6709\u6548\u7684\u4eba\u969b\u4e92\u52d5\u4e2d\u626e\u6f14\u8457\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\u3002\u5118\u7ba1\u6a5f\u5668\u4eba\u884c\u70ba\u6709\u9032\u6b65\uff0c\u4f46\u73fe\u6709\u65b9\u6cd5\u5728\u6a21\u4eff\u4eba\u985e\u975e\u8a9e\u8a00\u6e9d\u901a\u7684\u591a\u6a23\u6027\u548c\u5fae\u5999\u6027\u65b9\u9762\u5f80\u5f80\u505a\u5f97\u4e0d\u5920\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u60c5\u5883\u5b78\u7fd2\u80fd\u529b\uff0c\u52d5\u614b\u751f\u6210\u9069\u61c9\u793e\u4ea4\u5834\u5408\u7684\u624b\u52e2\u52d5\u4f5c\u5e8f\u5217\uff0c\u7528\u65bc\u4eba\u6a5f\u4e92\u52d5\u3002\u6211\u5011\u4f7f\u7528\u9019\u500b\u6846\u67b6\u751f\u6210\u4e86 10 \u7a2e\u4e0d\u540c\u7684\u8868\u9054\u6027\u624b\u52e2\uff0c\u4e26\u9032\u884c\u7dda\u4e0a\u4f7f\u7528\u8005\u7814\u7a76\uff0c\u6bd4\u8f03 EMOTION \u53ca\u5176\u4eba\u985e\u56de\u994b\u7248\u672c EMOTION++ \u6240\u751f\u6210\u52d5\u4f5c\u7684\u81ea\u7136\u6027\u548c\u53ef\u7406\u89e3\u6027\uff0c\u4ee5\u53ca\u4eba\u985e\u64cd\u4f5c\u54e1\u6240\u505a\u7684\u52d5\u4f5c\u3002\u7d50\u679c\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u5834\u666f\u4e0b\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u7406\u89e3\u4e14\u81ea\u7136\u7684\u6a5f\u5668\u4eba\u52d5\u4f5c\u65b9\u9762\u9054\u5230\u6216\u8d85\u904e\u4eba\u985e\u8868\u73fe\u3002\u6211\u5011\u9084\u70ba\u672a\u4f86\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u8a2d\u8a08\u542b\u7fa9\uff0c\u4ee5\u8003\u616e\u5728\u751f\u6210\u8868\u9054\u6027\u6a5f\u5668\u4eba\u624b\u52e2\u6642\u7684\u4e00\u7d44\u8b8a\u6578\u3002", "author": "Peide Huang et.al.", "authors": "Peide Huang, Yuhan Hu, Nataliya Nechyporenko, Daehwa Kim, Walter Talbott, Jian Zhang", "id": "2410.23234v1", "paper_url": "http://arxiv.org/abs/2410.23234v1", "repo": "null"}}