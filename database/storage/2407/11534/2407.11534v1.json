{"2407.11534": {"publish_time": "2024-07-16", "title": "LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices", "paper_summary": "With the commercialization of large language models (LLMs), weight-activation\nquantization has emerged to compress and accelerate LLMs, achieving high\nthroughput while reducing inference costs. However, existing post-training\nquantization (PTQ) techniques for quantizing weights and activations of LLMs\nstill suffer from non-negligible accuracy drops, especially on massive\nmultitask language understanding. To address this issue, we propose Low-Rank\nQuantization (LRQ) $-$ a simple yet effective post-training weight quantization\nmethod for LLMs that reconstructs the outputs of an intermediate Transformer\nblock by leveraging low-rank weight-scaling matrices, replacing the\nconventional full weight-scaling matrices that entail as many learnable scales\nas their associated weights. Thanks to parameter sharing via low-rank\nstructure, LRQ only needs to learn significantly fewer parameters while\nenabling the individual scaling of weights, thus boosting the generalization\ncapability of quantized LLMs. We show the superiority of LRQ over prior LLM PTQ\nworks under (i) $8$-bit weight and per-tensor activation quantization, (ii)\n$4$-bit weight and $8$-bit per-token activation quantization, and (iii) low-bit\nweight-only quantization schemes. Our code is available at\n\\url{https://github.com/onliwad101/FlexRound_LRQ} to inspire LLM researchers\nand engineers.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5546\u696d\u5316\uff0c\u6b0a\u91cd\u6d3b\u5316\u91cf\u5316\u61c9\u904b\u800c\u751f\uff0c\u7528\u65bc\u58d3\u7e2e\u548c\u52a0\u901f LLM\uff0c\u5728\u964d\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u6642\u5be6\u73fe\u9ad8\u541e\u5410\u91cf\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 LLM \u6b0a\u91cd\u548c\u6d3b\u5316\u91cf\u5316\u7684\u8a13\u7df4\u5f8c\u91cf\u5316\uff08PTQ\uff09\u6280\u8853\u4ecd\u7136\u6703\u5c0e\u81f4\u975e\u53ef\u5ffd\u7565\u7684\u6e96\u78ba\u5ea6\u4e0b\u964d\uff0c\u7279\u5225\u662f\u5728\u5927\u898f\u6a21\u591a\u4efb\u52d9\u8a9e\u8a00\u7406\u89e3\u4e0a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4f4e\u79e9\u91cf\u5316\uff08LRQ\uff09$-$\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684 LLM \u8a13\u7df4\u5f8c\u6b0a\u91cd\u91cf\u5316\u65b9\u6cd5\uff0c\u5b83\u901a\u904e\u5229\u7528\u4f4e\u79e9\u6b0a\u91cd\u7e2e\u653e\u77e9\u9663\u4f86\u91cd\u5efa\u4e2d\u9593 Transformer \u584a\u7684\u8f38\u51fa\uff0c\u53d6\u4ee3\u4e86\u5305\u542b\u8207\u5176\u95dc\u806f\u6b0a\u91cd\u4e00\u6a23\u591a\u53ef\u5b78\u7fd2\u7e2e\u653e\u7684\u50b3\u7d71\u5168\u6b0a\u91cd\u7e2e\u653e\u77e9\u9663\u3002\u7531\u65bc\u901a\u904e\u4f4e\u79e9\u7d50\u69cb\u5171\u4eab\u53c3\u6578\uff0cLRQ \u53ea\u9700\u5b78\u7fd2\u986f\u8457\u66f4\u5c11\u7684\u53c3\u6578\uff0c\u540c\u6642\u555f\u7528\u6b0a\u91cd\u7684\u500b\u5225\u7e2e\u653e\uff0c\u5f9e\u800c\u63d0\u5347\u91cf\u5316 LLM \u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u5011\u5c55\u793a\u4e86 LRQ \u5728\u4ee5\u4e0b\u60c5\u6cc1\u4e0b\u512a\u65bc\u5148\u524d\u7684 LLM PTQ \u5de5\u4f5c\uff1a(i) 8 \u4f4d\u5143\u6b0a\u91cd\u548c\u6bcf\u500b\u5f35\u91cf\u6d3b\u5316\u91cf\u5316\uff0c(ii) 4 \u4f4d\u5143\u6b0a\u91cd\u548c 8 \u4f4d\u5143\u6bcf\u500b\u7b26\u865f\u6d3b\u5316\u91cf\u5316\uff0c\u4ee5\u53ca (iii) \u4f4e\u4f4d\u5143\u50c5\u6b0a\u91cd\u91cf\u5316\u65b9\u6848\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 \\url{https://github.com/onliwad101/FlexRound_LRQ} \u53d6\u5f97\uff0c\u4ee5\u555f\u767c LLM \u7814\u7a76\u4eba\u54e1\u548c\u5de5\u7a0b\u5e2b\u3002", "author": "Jung Hyun Lee et.al.", "authors": "Jung Hyun Lee, Jeonghoon Kim, June Yong Yang, Se Jung Kwon, Eunho Yang, Kang Min Yoo, Dongsoo Lee", "id": "2407.11534v1", "paper_url": "http://arxiv.org/abs/2407.11534v1", "repo": "https://github.com/onliwad101/flexround_lrq"}}