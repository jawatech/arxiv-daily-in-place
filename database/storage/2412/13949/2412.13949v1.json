{"2412.13949": {"publish_time": "2024-12-18", "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence", "paper_summary": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5728\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLMs) \u8207\u8996\u89ba\u8f38\u5165\u6574\u5408\u65b9\u9762\u53d6\u5f97\u4e86\u5be6\u8cea\u6027\u9032\u5c55\uff0c\u5be6\u73fe\u4e86\u5148\u9032\u7684\u591a\u6a21\u614b\u63a8\u7406\u3002\u5118\u7ba1\u5b83\u5011\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u6301\u7e8c\u7684\u6311\u6230\u5728\u65bc\u5e7b\u89ba\uff0c\u5176\u4e2d\u7522\u751f\u7684\u6587\u5b57\u7121\u6cd5\u6e96\u78ba\u53cd\u6620\u8996\u89ba\u5167\u5bb9\uff0c\u5f9e\u800c\u640d\u5bb3\u4e86\u6e96\u78ba\u6027\u548c\u53ef\u9760\u6027\u3002\u73fe\u6709\u65b9\u6cd5\u5c08\u6ce8\u65bc\u5c0d\u9f4a\u8a13\u7df4\u6216\u89e3\u78bc\u512a\u5316\uff0c\u4f46\u4e3b\u8981\u89e3\u6c7a\u7522\u751f\u968e\u6bb5\u7684\u75c7\u72c0\uff0c\u800c\u6c92\u6709\u63a2\u7a76\u6839\u672c\u539f\u56e0\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86\u9a45\u52d5 LVLMs \u4e2d\u5e7b\u89ba\u7684\u5167\u90e8\u6a5f\u5236\uff0c\u91cd\u9ede\u95dc\u6ce8\u591a\u982d\u6ce8\u610f\u6a21\u7d44\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5f15\u5165\u4e86\u8996\u89ba\u611f\u77e5\u982d\u90e8\u5dee\u7570 (VHD)\uff0c\u9019\u662f\u4e00\u500b\u91cf\u5316\u6ce8\u610f\u982d\u90e8\u8f38\u51fa\u5c0d\u8996\u89ba\u74b0\u5883\u654f\u611f\u6027\u7684\u6307\u6a19\u3002\u57fa\u65bc\u6b64\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\u4e86\u5b58\u5728\u8996\u89ba\u611f\u77e5\u6ce8\u610f\u982d\u90e8\uff0c\u5b83\u5011\u66f4\u9069\u61c9\u8996\u89ba\u8cc7\u8a0a\uff1b\u7136\u800c\uff0c\u6a21\u578b\u904e\u5ea6\u4f9d\u8cf4\u5176\u5148\u524d\u7684\u8a9e\u8a00\u6a21\u5f0f\u8207\u5e7b\u89ba\u5bc6\u5207\u76f8\u95dc\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u8996\u89ba\u611f\u77e5\u982d\u90e8\u5f37\u5316 (VHR)\uff0c\u9019\u662f\u4e00\u7a2e\u7121\u9700\u8a13\u7df4\u7684\u65b9\u6cd5\uff0c\u901a\u904e\u589e\u5f37\u8996\u89ba\u611f\u77e5\u6ce8\u610f\u982d\u90e8\u7684\u4f5c\u7528\u4f86\u6e1b\u8f15\u5e7b\u89ba\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u6e1b\u8f15\u5e7b\u89ba\u7684\u6700\u65b0\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u540c\u6642\u4fdd\u6301\u9ad8\u6548\u7387\uff0c\u4e14\u5e7e\u4e4e\u6c92\u6709\u984d\u5916\u7684\u6642\u9593\u958b\u92b7\u3002", "author": "Jinghan He et.al.", "authors": "Jinghan He, Kuan Zhu, Haiyun Guo, Junfeng Fang, Zhenglin Hua, Yuheng Jia, Ming Tang, Tat-Seng Chua, Jinqiao Wang", "id": "2412.13949v1", "paper_url": "http://arxiv.org/abs/2412.13949v1", "repo": "null"}}