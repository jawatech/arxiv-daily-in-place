{"2410.18441": {"publish_time": "2024-10-24", "title": "The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI", "paper_summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.", "paper_summary_zh": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c0d\u751f\u6210\u5f0f AI \u9818\u57df\u4e2d Transformer \u6a21\u578b [33] \u7684\u4e00\u4e9b\u95dc\u9375\u7d44\u6210\u90e8\u5206\u7684\u6578\u5b78\u554f\u984c\u8868\u8ff0\u548c\u6a5f\u7387\u6700\u4f73\u5316\u63a2\u7d22\u9032\u884c\u6df1\u5165\u5206\u6790\u3002\u6211\u5011\u63a2\u8a0e\u4e26\u8a0e\u8ad6\u4e86\u5f9e\u6f14\u7b97\u6cd5\u548c\u6a5f\u7387\u6700\u4f73\u5316\u7684\u89d2\u5ea6\uff0c\u91dd\u5c0d\u751f\u6210\u5f0f AI \u6a21\u578b\u7684\u4e00\u4e9b\u95dc\u9375\u57fa\u790e\u6280\u8853\u7684\u73fe\u6709\u6280\u8853\u9032\u4e00\u6b65\u589e\u5f37\u7684\u6f5b\u529b\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u91dd\u5c0d\u5b50\u8a5e\u7de8\u78bc (SWE) \u63d0\u51fa\u4e00\u500b\u6700\u4f73\u89e3\uff0c\u5176\u57fa\u65bc\u8207 [9] \u4e2d\u7684\u4f4d\u5143\u7d44\u5c0d\u7de8\u78bc (BPE) \u6f14\u7b97\u6cd5\u985e\u4f3c\u7684\u521d\u59cb\u8a2d\u5b9a\uff0c\u4e26\u5177\u6709\u8207 [28, 31] \u4e2d\u7684 WordPiece \u65b9\u6cd5\u985e\u4f3c\u7684\u76ee\u6a19\uff0c\u4ee5\u6700\u5927\u5316\u8a13\u7df4\u8cc7\u6599\u7684\u53ef\u80fd\u6027\u3002\u6211\u5011\u4e5f\u63d0\u51fa\u4ea4\u53c9\u71b5\u6700\u4f73\u5316\u65b9\u6cd5\uff0c\u7528\u65bc\u6700\u4f73\u5316 word2vec \u6a21\u578b [17] \u7684\u8d85\u53c3\u6578\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u65cb\u8f49\u4f4d\u7f6e\u7de8\u78bc (RoPE) [32] \u548c\u5e36\u6709\u7dda\u6027\u504f\u5dee\u7684\u6ce8\u610f\u529b (ALiBi) [23] \u7684\u5206\u89e3\u7d44\u5408\uff0c\u4e26\u642d\u914d\u8ae7\u6ce2\u7d1a\u6578\u3002\u6211\u5011\u4e5f\u63d0\u51fa\u4e00\u500b\u6a5f\u7387 FlashAttention [6, 7] (PrFlashAttention) \u65b9\u6cd5\uff0c\u5176\u4e2d\u77e9\u9663\u4e2d\u7684\u5340\u584a\u8ddd\u96e2\u5177\u6709\u6a5f\u7387\u5206\u4f48\uff0c\u7528\u65bc\u6c7a\u5b9a\u54ea\u500b\u5340\u584a\u53ef\u80fd\u53c3\u8207\u7279\u5b9a\u56de\u5408\u7684\u6ce8\u610f\u529b\u904b\u7b97\uff0c\u540c\u6642\u900f\u904e\u91cd\u65b0\u8abf\u6574\u5f35\u91cf\u5f62\u72c0\uff0c\u7dad\u6301\u81ea\u8ff4\u6b78\u8a9e\u8a00\u6a21\u578b\u7684\u5f35\u91cf\u4e0b\u4e09\u89d2\u5f62\u72c0\u3002\u6700\u5f8c\uff0c\u6211\u5011\u91dd\u5c0d\u591a\u91cd\u67e5\u8a62\u6ce8\u610f\u529b (MQA) \u7684\u9375\u503c (KV) \u5feb\u53d6\u63d0\u51fa\u968e\u68af\u5f0f\u81ea\u9069\u61c9\u91cf\u5316 (SAQ)\uff0c\u5176\u57fa\u65bc [16] \u4e2d\u63d0\u51fa\u7684\u67b6\u69cb\uff0c\u4ee5\u5728\u9054\u6210\u5408\u7406\u7684\u6a21\u578b\u54c1\u8cea\u548c\u6210\u672c\u7bc0\u7701\u7684\u540c\u6642\uff0c\u9010\u6b65\u91cf\u5316\u52a3\u5316\u3002", "author": "Fulu Li et.al.", "authors": "Fulu Li", "id": "2410.18441v1", "paper_url": "http://arxiv.org/abs/2410.18441v1", "repo": "null"}}