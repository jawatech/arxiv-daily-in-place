
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-09**|**MM-Ego: Towards Building Egocentric Multimodal LLMs**|Hanrong Ye et.al.|[2410.07177v1](http://arxiv.org/abs/2410.07177v1)|null|
|**2024-10-09**|**Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models**|Fei Wang et.al.|[2410.07176v1](http://arxiv.org/abs/2410.07176v1)|null|
|**2024-10-09**|**Do better language models have crisper vision?**|Jona Ruthardt et.al.|[2410.07173v1](http://arxiv.org/abs/2410.07173v1)|null|
|**2024-10-09**|**One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation**|Fabian Paischer et.al.|[2410.07170v1](http://arxiv.org/abs/2410.07170v1)|null|
|**2024-10-09**|**Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate**|Qidong Huang et.al.|[2410.07167v1](http://arxiv.org/abs/2410.07167v1)|[link](https://github.com/shikiw/modality-integration-rate)|
|**2024-10-09**|**Sylber: Syllabic Embedding Representation of Speech from Raw Audio**|Cheol Jun Cho et.al.|[2410.07168v1](http://arxiv.org/abs/2410.07168v1)|null|
|**2024-10-09**|**Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making**|Manling Li et.al.|[2410.07166v1](http://arxiv.org/abs/2410.07166v1)|[link](https://github.com/embodied-agent-interface/embodied-agent-interface)|
|**2024-10-09**|**Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning**|Chongyu Fan et.al.|[2410.07163v1](http://arxiv.org/abs/2410.07163v1)|null|
|**2024-10-09**|**InstructG2I: Synthesizing Images from Multimodal Attributed Graphs**|Bowen Jin et.al.|[2410.07157v1](http://arxiv.org/abs/2410.07157v1)|null|
|**2024-10-09**|**Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling**|Yingfa Chen et.al.|[2410.07145v1](http://arxiv.org/abs/2410.07145v1)|null|
|**2024-10-09**|**Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates**|Xiaosen Zheng et.al.|[2410.07137v1](http://arxiv.org/abs/2410.07137v1)|null|
|**2024-10-09**|**Mental Disorders Detection in the Era of Large Language Models**|Gleb Kuzmin et.al.|[2410.07129v1](http://arxiv.org/abs/2410.07129v1)|null|
|**2024-10-09**|**Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy**|Tagore Rao Kosireddy et.al.|[2410.07118v1](http://arxiv.org/abs/2410.07118v1)|[link](https://github.com/Tagore-7/Small-Language-Models-for-the-Democratization-of-Financial-Literacy)|
|**2024-10-09**|**VHELM: A Holistic Evaluation of Vision Language Models**|Tony Lee et.al.|[2410.07112v1](http://arxiv.org/abs/2410.07112v1)|null|
|**2024-10-09**|**I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy**|Gian Maria Campedelli et.al.|[2410.07109v1](http://arxiv.org/abs/2410.07109v1)|null|
|**2024-10-09**|**Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context**|Sangwon Yu et.al.|[2410.07103v1](http://arxiv.org/abs/2410.07103v1)|null|
|**2024-10-09**|**MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering**|Jun Shern Chan et.al.|[2410.07095v1](http://arxiv.org/abs/2410.07095v1)|[link](https://github.com/openai/mle-bench)|
|**2024-10-09**|**An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots**|Ebube Alor et.al.|[2410.07094v1](http://arxiv.org/abs/2410.07094v1)|null|
|**2024-10-09**|**Stanceformer: Target-Aware Transformer for Stance Detection**|Krishna Garg et.al.|[2410.07083v1](http://arxiv.org/abs/2410.07083v1)|null|
|**2024-10-09**|**MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses**|Zonglin Yang et.al.|[2410.07076v1](http://arxiv.org/abs/2410.07076v1)|[link](https://github.com/ZonglinY/MOOSE-Chem)|
|**2024-10-09**|**Pixtral 12B**|Pravesh Agrawal et.al.|[2410.07073v1](http://arxiv.org/abs/2410.07073v1)|[link](https://github.com/mistralai/mistral-inference)|
|**2024-10-09**|**Retrieval-Augmented Decision Transformer: External Memory for In-context RL**|Thomas Schmied et.al.|[2410.07071v1](http://arxiv.org/abs/2410.07071v1)|null|
|**2024-10-09**|**ReIFE: Re-evaluating Instruction-Following Evaluation**|Yixin Liu et.al.|[2410.07069v1](http://arxiv.org/abs/2410.07069v1)|null|
|**2024-10-09**|**Data Selection via Optimal Control for Language Models**|Yuxian Gu et.al.|[2410.07064v1](http://arxiv.org/abs/2410.07064v1)|[link](https://github.com/microsoft/lmops)|
|**2024-10-09**|**Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing**|Weichuan Wang et.al.|[2410.07054v1](http://arxiv.org/abs/2410.07054v1)|null|
|**2024-10-09**|**Robots in the Middle: Evaluating LLMs in Dispute Resolution**|Jinzhe Tan et.al.|[2410.07053v1](http://arxiv.org/abs/2410.07053v1)|null|
|**2024-10-09**|**Emergent properties with repeated examples**|François Charton et.al.|[2410.07041v1](http://arxiv.org/abs/2410.07041v1)|null|
|**2024-10-09**|**PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness**|Zekun Wang et.al.|[2410.07035v1](http://arxiv.org/abs/2410.07035v1)|null|
|**2024-10-09**|**Clean Evaluations on Contaminated Visual Language Models**|Hongyuan Lu et.al.|[2410.07030v1](http://arxiv.org/abs/2410.07030v1)|null|
|**2024-10-09**|**Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback**|Dennis Hein et.al.|[2410.07025v1](http://arxiv.org/abs/2410.07025v1)|null|
|**2024-10-09**|**Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization**|Chengtao Jian et.al.|[2410.07018v1](http://arxiv.org/abs/2410.07018v1)|null|
|**2024-10-09**|**Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based Outline-guided Generation**|Valentin Knappich et.al.|[2410.07009v1](http://arxiv.org/abs/2410.07009v1)|null|
|**2024-10-09**|**CursorCore: Assist Programming through Aligning Anything**|Hao Jiang et.al.|[2410.07002v1](http://arxiv.org/abs/2410.07002v1)|[link](https://github.com/TechxGenus/CursorCore)|
|**2024-10-09**|**Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models**|Michael Lan et.al.|[2410.06981v1](http://arxiv.org/abs/2410.06981v1)|null|
|**2024-10-09**|**Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification**|Chenyue Li et.al.|[2410.06977v1](http://arxiv.org/abs/2410.06977v1)|null|
|**2024-10-09**|**Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara**|Azree Nazri et.al.|[2410.06973v1](http://arxiv.org/abs/2410.06973v1)|null|
|**2024-10-09**|**DLGNet: Hyperedge Classification through Directed Line Graphs for Chemical Reactions**|Stefano Fiorini et.al.|[2410.06969v1](http://arxiv.org/abs/2410.06969v1)|null|
|**2024-10-09**|**Uncovering Factor Level Preferences to Improve Human-Model Alignment**|Juhyun Oh et.al.|[2410.06965v1](http://arxiv.org/abs/2410.06965v1)|null|
|**2024-10-09**|**ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling**|Deok-Kyeong Jang et.al.|[2410.06963v1](http://arxiv.org/abs/2410.06963v1)|null|
|**2024-10-09**|**Self-Boosting Large Language Models with Synthetic Preference Data**|Qingxiu Dong et.al.|[2410.06961v1](http://arxiv.org/abs/2410.06961v1)|null|
|**2024-10-09**|**Support Vector Boosting Machine (SVBM): Enhancing Classification Performance with AdaBoost and Residual Connections**|Junbo Jacob Lian et.al.|[2410.06957v1](http://arxiv.org/abs/2410.06957v1)|null|
|**2024-10-09**|**Faithful Interpretation for Graph Neural Networks**|Lijie Hu et.al.|[2410.06950v1](http://arxiv.org/abs/2410.06950v1)|null|
|**2024-10-09**|**Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach**|Xuanming Zhang et.al.|[2410.06949v1](http://arxiv.org/abs/2410.06949v1)|[link](https://github.com/XMZhangAI/Seeker)|
|**2024-10-09**|**CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages**|Pretam Ray et.al.|[2410.06944v1](http://arxiv.org/abs/2410.06944v1)|null|
|**2024-10-09**|**AutoFeedback: An LLM-based Framework for Efficient and Accurate API Request Generation**|Huanxi Liu et.al.|[2410.06943v1](http://arxiv.org/abs/2410.06943v1)|null|
|**2024-10-09**|**Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models**|Daniel Albert et.al.|[2410.06932v1](http://arxiv.org/abs/2410.06932v1)|null|
|**2024-10-09**|**SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration**|Heming Xia et.al.|[2410.06916v1](http://arxiv.org/abs/2410.06916v1)|null|
|**2024-10-09**|**Utilize the Flow before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal-Aware Instruction Tuning**|Runchuan Zhu et.al.|[2410.06913v1](http://arxiv.org/abs/2410.06913v1)|null|
|**2024-10-09**|**Compositional Entailment Learning for Hyperbolic Vision-Language Models**|Avik Pal et.al.|[2410.06912v1](http://arxiv.org/abs/2410.06912v1)|null|
|**2024-10-09**|**Generative Model for Less-Resourced Language with 1 billion parameters**|Domen Vreš et.al.|[2410.06898v1](http://arxiv.org/abs/2410.06898v1)|null|
|**2024-10-09**|**FltLM: An Intergrated Long-Context Large Language Model for Effective Context Filtering and Understanding**|Jingyang Deng et.al.|[2410.06886v1](http://arxiv.org/abs/2410.06886v1)|null|
|**2024-10-09**|**Understanding Model Ensemble in Transferable Adversarial Attack**|Wei Yao et.al.|[2410.06851v1](http://arxiv.org/abs/2410.06851v1)|null|
|**2024-10-09**|**A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement Learning and Application in UAV Hovering**|Qihan Qi et.al.|[2410.06847v1](http://arxiv.org/abs/2410.06847v1)|null|
|**2024-10-09**|**Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity**|Mutian He et.al.|[2410.06846v1](http://arxiv.org/abs/2410.06846v1)|null|
|**2024-10-09**|**MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders**|Cheng Li et.al.|[2410.06845v1](http://arxiv.org/abs/2410.06845v1)|null|
|**2024-10-09**|**Dynamic Neural Potential Field: Online Trajectory Optimization in Presence of Moving Obstacles**|Aleksey Staroverov et.al.|[2410.06819v1](http://arxiv.org/abs/2410.06819v1)|null|
|**2024-10-09**|**An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion**|Narjes Benameur et.al.|[2410.06818v1](http://arxiv.org/abs/2410.06818v1)|null|
|**2024-10-09**|**Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning**|Qiang Hu et.al.|[2410.06814v1](http://arxiv.org/abs/2410.06814v1)|null|
|**2024-10-09**|**Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level**|Xinyi Zeng et.al.|[2410.06809v1](http://arxiv.org/abs/2410.06809v1)|null|
|**2024-10-09**|**Seg2Act: Global Context-aware Action Generation for Document Logical Structuring**|Zichao Li et.al.|[2410.06802v1](http://arxiv.org/abs/2410.06802v1)|null|
|**2024-10-09**|**Diffuse or Confuse: A Diffusion Deepfake Speech Dataset**|Anton Firc et.al.|[2410.06796v1](http://arxiv.org/abs/2410.06796v1)|null|
|**2024-10-09**|**From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models**|Yuying Shang et.al.|[2410.06795v1](http://arxiv.org/abs/2410.06795v1)|null|
|**2024-10-09**|**To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models**|Junyan Lin et.al.|[2410.06765v1](http://arxiv.org/abs/2410.06765v1)|[link](https://github.com/eit-nlp/connector-selection-for-mllm)|
|**2024-10-09**|**CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models**|Zi Gong et.al.|[2410.06741v1](http://arxiv.org/abs/2410.06741v1)|[link](https://github.com/codefuse-ai/mftcoder)|
|**2024-10-09**|**Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?**|Fumiya Uchiyama et.al.|[2410.06735v1](http://arxiv.org/abs/2410.06735v1)|null|
|**2024-10-09**|**Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles**|Qi Chen et.al.|[2410.06733v1](http://arxiv.org/abs/2410.06733v1)|null|
|**2024-10-09**|**Scaling Laws for Mixed quantization in Large Language Models**|Zeyu Cao et.al.|[2410.06722v1](http://arxiv.org/abs/2410.06722v1)|null|
|**2024-10-09**|**Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques**|Benyuan Meng et.al.|[2410.06719v1](http://arxiv.org/abs/2410.06719v1)|null|
|**2024-10-09**|**MatMamba: A Matryoshka State Space Model**|Abhinav Shukla et.al.|[2410.06718v1](http://arxiv.org/abs/2410.06718v1)|[link](https://github.com/scaledfoundations/matmamba)|
|**2024-10-09**|**Guaranteed Generation from Large Language Models**|Minbeom Kim et.al.|[2410.06716v1](http://arxiv.org/abs/2410.06716v1)|null|
|**2024-10-09**|**Calibrating Verbalized Probabilities for Large Language Models**|Cheng Wang et.al.|[2410.06707v1](http://arxiv.org/abs/2410.06707v1)|null|
|**2024-10-09**|**PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs**|Krishna Kanth Nakka et.al.|[2410.06704v1](http://arxiv.org/abs/2410.06704v1)|null|
|**2024-10-09**|**ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents**|Ido Levy et.al.|[2410.06703v1](http://arxiv.org/abs/2410.06703v1)|null|
|**2024-10-09**|**Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models**|Yubo Wang et.al.|[2410.06699v1](http://arxiv.org/abs/2410.06699v1)|null|
|**2024-10-09**|**Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization**|Changli Tang et.al.|[2410.06682v1](http://arxiv.org/abs/2410.06682v1)|null|
|**2024-10-09**|**M${}^{3}$Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes**|Zeyu Zhang et.al.|[2410.06678v1](http://arxiv.org/abs/2410.06678v1)|null|
|**2024-10-09**|**Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures**|Junxuan Wang et.al.|[2410.06672v1](http://arxiv.org/abs/2410.06672v1)|null|
|**2024-10-09**|**Large Language Models as Code Executors: An Exploratory Study**|Chenyang Lyu et.al.|[2410.06667v1](http://arxiv.org/abs/2410.06667v1)|null|
|**2024-10-09**|**Revisiting Multi-Permutation Equivariance through the Lens of Irreducible Representations**|Yonatan Sverdlov et.al.|[2410.06665v1](http://arxiv.org/abs/2410.06665v1)|null|
|**2024-10-09**|**Decouple-Then-Merge: Towards Better Training for Diffusion Models**|Qianli Ma et.al.|[2410.06664v1](http://arxiv.org/abs/2410.06664v1)|null|
|**2024-10-09**|**Task-oriented Time Series Imputation Evaluation via Generalized Representers**|Zhixian Wang et.al.|[2410.06652v1](http://arxiv.org/abs/2410.06652v1)|null|
|**2024-10-09**|**Toward Physics-guided Time Series Embedding**|Jiaxi Hu et.al.|[2410.06651v1](http://arxiv.org/abs/2410.06651v1)|null|
|**2024-10-09**|**Subtle Errors Matter: Preference Learning via Error-injected Self-editing**|Kaishuai Xu et.al.|[2410.06638v1](http://arxiv.org/abs/2410.06638v1)|null|
|**2024-10-09**|**Tree of Problems: Improving structured problem solving with compositionality**|Armel Zebaze et.al.|[2410.06634v1](http://arxiv.org/abs/2410.06634v1)|null|
|**2024-10-09**|**ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time**|Yi Ding et.al.|[2410.06625v1](http://arxiv.org/abs/2410.06625v1)|[link](https://github.com/dripnowhy/eta)|
|**2024-10-09**|**Effective Exploration Based on the Structural Information Principles**|Xianghua Zeng et.al.|[2410.06621v1](http://arxiv.org/abs/2410.06621v1)|null|
|**2024-10-09**|**Learning Evolving Tools for Large Language Models**|Guoxin Chen et.al.|[2410.06617v1](http://arxiv.org/abs/2410.06617v1)|null|
|**2024-10-09**|**$β$-calibration of Language Model Confidence Scores for Generative QA**|Putra Manggala et.al.|[2410.06615v1](http://arxiv.org/abs/2410.06615v1)|null|
|**2024-10-09**|**Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers**|Stephen Hausler et.al.|[2410.06614v1](http://arxiv.org/abs/2410.06614v1)|null|
|**2024-10-09**|**Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS**|Onkar Kishor Susladkar et.al.|[2410.06608v1](http://arxiv.org/abs/2410.06608v1)|null|
|**2024-10-09**|**Dissecting Fine-Tuning Unlearning in Large Language Models**|Yihuai Hong et.al.|[2410.06606v1](http://arxiv.org/abs/2410.06606v1)|null|
|**2024-10-09**|**Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions**|Zhihao He et.al.|[2410.06577v1](http://arxiv.org/abs/2410.06577v1)|null|
|**2024-10-09**|**Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare**|Pardis Sadat Zahraei et.al.|[2410.06566v1](http://arxiv.org/abs/2410.06566v1)|null|
|**2024-10-09**|**Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching**|Wenqi Niu et.al.|[2410.06561v1](http://arxiv.org/abs/2410.06561v1)|null|
|**2024-10-09**|**Mitigating Time Discretization Challenges with WeatherODE: A Sandwich Physics-Driven Neural ODE for Weather Forecasting**|Peiyuan Liu et.al.|[2410.06560v1](http://arxiv.org/abs/2410.06560v1)|null|
|**2024-10-09**|**ING-VP: MLLMs cannot Play Easy Vision-based Games Yet**|Haoran Zhang et.al.|[2410.06555v1](http://arxiv.org/abs/2410.06555v1)|[link](https://github.com/thisisus7/ing-vp)|
|**2024-10-09**|**The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models**|Yanjun Chen et.al.|[2410.06554v1](http://arxiv.org/abs/2410.06554v1)|null|
|**2024-10-09**|**InstantIR: Blind Image Restoration with Instant Generative Reference**|Jen-Yuan Huang et.al.|[2410.06551v1](http://arxiv.org/abs/2410.06551v1)|null|
|**2024-10-09**|**Investigating Cost-Efficiency of LLM-Generated Training Data for Conversational Semantic Frame Analysis**|Shiho Matta et.al.|[2410.06550v1](http://arxiv.org/abs/2410.06550v1)|null|
|**2024-10-09**|**TuringQ: Benchmarking AI Comprehension in Theory of Computation**|Pardis Sadat Zahraei et.al.|[2410.06547v1](http://arxiv.org/abs/2410.06547v1)|null|

#### Abstracts
##### **MM-Ego: Towards Building Egocentric Multimodal LLMs**
2410.07177v1 by Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, Yinfei Yang

This research aims to comprehensively explore building a multimodal
foundation model for egocentric video understanding. To achieve this goal, we
work on three fronts. First, as there is a lack of QA data for egocentric video
understanding, we develop a data engine that efficiently generates 7M
high-quality QA samples for egocentric videos ranging from 30 seconds to one
hour long, based on human-annotated data. This is currently the largest
egocentric QA dataset. Second, we contribute a challenging egocentric QA
benchmark with 629 videos and 7,026 questions to evaluate the models' ability
in recognizing and memorizing visual details across videos of varying lengths.
We introduce a new de-biasing evaluation method to help mitigate the
unavoidable language bias present in the models being evaluated. Third, we
propose a specialized multimodal architecture featuring a novel "Memory Pointer
Prompting" mechanism. This design includes a global glimpse step to gain an
overarching understanding of the entire video and identify key visual
information, followed by a fallback step that utilizes the key visual
information to generate responses. This enables the model to more effectively
comprehend extended video content. With the data, benchmark, and model, we
successfully build MM-Ego, an egocentric multimodal LLM that shows powerful
performance on egocentric video understanding.

摘要：本研究旨在全面探索建立一個用於以自我為中心影片理解的多模態基礎模型。為了達成此目標，我們在三個方面著手進行。首先，由於缺乏用於以自我為中心影片理解的問答資料，我們開發了一個資料引擎，可有效地為長度從 30 秒到一小時不等的以自我為中心影片生成 700 萬個高品質問答範例，這些範例是根據人工標註資料生成的。這目前是最龐大的以自我為中心問答資料集。其次，我們提供了一個具有挑戰性的以自我為中心問答基準，其中包含 629 部影片和 7,026 個問題，用於評估模型在辨識和記憶不同長度影片中視覺細節的能力。我們引入了一種新的去偏評估方法，以協助減輕模型評估中存在的不可避免的語言偏誤。第三，我們提出了一種特殊的多模態架構，其特色在於具備一種新穎的「記憶指標提示」機制。此設計包含一個整體概覽步驟，用於獲得對整個影片的全面理解並識別關鍵視覺資訊，接著是一個後備步驟，利用關鍵視覺資訊來產生回應。這使模型能夠更有效地理解延伸的影片內容。透過資料、基準和模型，我們成功建立了 MM-Ego，這是一個以自我為中心的多模態 LLM，在以自我為中心影片理解方面展現出強大的效能。

##### **Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models**
2410.07176v1 by Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan Ö. Arık

Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems.

摘要：檢索增強生成（RAG）在整合外部知識以解決大型語言模型（LLM）的限制方面雖然有效，但可能會受到不完美的檢索的影響，這可能會引入不相關、誤導，甚至惡意的資訊。儘管其重要性，先前的研究很少通過聯合分析來探討 RAG 的行為，探討不完美的檢索屬性如何產生錯誤並傳播，以及 LLM 的內部知識與外部來源之間如何產生潛在衝突。我們發現不完美的檢索增強可能是不可避免且非常有害的，通過在現實條件下的受控分析。我們將 LLM 內部和外部知識之間的知識衝突從檢索中確定為在 RAG 的檢索後階段中需要克服的瓶頸。為了使 LLM 對不完美的檢索具有彈性，我們提出了精明的 RAG，這是一種新穎的 RAG 方法，它能適應性地從 LLM 的內部知識中引出必要的資訊，反覆地整合內部和外部知識與來源感知，並根據資訊可靠性確定答案。我們使用 Gemini 和 Claude 進行的實驗表明，精明的 RAG 明顯優於先前的魯棒性增強 RAG 方法。值得注意的是，精明的 RAG 是唯一一種在最壞情況下與沒有 RAG 的 LLM 的效能相匹配或超過其效能的方法。進一步的分析表明，精明的 RAG 有效地解決了知識衝突，提高了 RAG 系統的可靠性和可信度。

##### **Do better language models have crisper vision?**
2410.07173v1 by Jona Ruthardt, Gertjan J. Burghouts, Serge Belongie, Yuki M. Asano

How well do text-only Large Language Models (LLMs) grasp the visual world? As
LLMs are increasingly used in computer vision, addressing this question becomes
both fundamental and pertinent. However, existing studies have primarily
focused on limited scenarios, such as their ability to generate visual content
or cluster multimodal data. To this end, we propose the Visual Text
Representation Benchmark (ViTeRB) to isolate key properties that make language
models well-aligned with the visual world. With this, we identify large-scale
decoder-based LLMs as ideal candidates for representing text in vision-centric
contexts, counter to the current practice of utilizing text encoders. Building
on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model.
By leveraging precomputable frozen features from strong vision and language
models, ShareLock achieves an impressive 51% accuracy on ImageNet despite
utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU
hour (or 10 hours including the precomputation of features) - orders of
magnitude less than prior methods. Code will be released.

摘要：純文字大型語言模型 (LLM) 對視覺世界的掌握程度如何？由於 LLM 在電腦視覺領域的應用日益廣泛，探討這個問題既基本又切合時宜。然而，現有研究主要集中在有限的場景中，例如它們生成視覺內容或聚類多模態數據的能力。為此，我們提出了視覺文本表示基準 (ViTeRB)，以分離出讓語言模型與視覺世界緊密貼合的主要特性。藉此，我們將大型基於解碼器的 LLM 確定為在以視覺為中心的上下文中表示文本的理想候選者，這與當前使用文本編碼器的做法相反。根據這些發現，我們提出了 ShareLock，一個超輕量級的 CLIP 類似模型。通過利用來自強大視覺和語言模型的預先可計算凍結特徵，ShareLock 在 ImageNet 上達到了令人印象深刻的 51% 準確率，儘管僅使用了 563k 圖像標題對。此外，訓練僅需要 1 個 GPU 小時（或包括特徵預計算在內的 10 小時）——比以前的方法少幾個數量級。程式碼將會發布。

##### **One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation**
2410.07170v1 by Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, Sepp Hochreiter

Foundation models (FMs) are pre-trained on large-scale datasets and then
fine-tuned on a downstream task for a specific application. The most successful
and most commonly used fine-tuning method is to update the pre-trained weights
via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are
usually initialized at random with a uniform rank distribution across model
weights. Recent works focus on weight-driven initialization or learning of
adaptive ranks during training. Both approaches have only been investigated in
isolation, resulting in slow convergence or a uniform rank distribution, in
turn leading to sub-optimal performance. We propose to enhance LoRA by
initializing the new weights in a data-driven manner by computing singular
value decomposition on minibatches of activation vectors. Then, we initialize
the LoRA matrices with the obtained right-singular vectors and re-distribute
ranks among all weight matrices to explain the maximal amount of variance and
continue the standard LoRA fine-tuning procedure. This results in our new
method Explained Variance Adaptation (EVA). We apply EVA to a variety of
fine-tuning tasks ranging from language generation and understanding to image
classification and reinforcement learning. EVA exhibits faster convergence than
competitors and attains the highest average score across a multitude of tasks
per domain.

摘要：基礎模型 (FM) 在大型資料集上經過預訓練，然後針對特定應用程式在下游任務中進行微調。最成功且最常用的微調方法是透過低階適應 (LoRA) 來更新預訓練權重。LoRA 引入了新的權重矩陣，通常在模型權重中以均勻的秩分佈進行隨機初始化。最近的研究重點在於權重驅動初始化或在訓練期間學習自適應秩。這兩種方法都只在孤立的情況下進行調查，導致收斂速度慢或秩分佈均勻，進而導致次佳效能。我們建議透過對激活向量的迷你批次進行奇異值分解，以資料驅動的方式初始化新的權重，來增強 LoRA。然後，我們使用獲得的右奇異向量初始化 LoRA 矩陣，並在所有權重矩陣中重新分配秩，以解釋最大變異量，並繼續標準的 LoRA 微調程序。這造就了我們新的方法，稱為解釋變異適應 (EVA)。我們將 EVA 應用於各種微調任務，從語言生成和理解到影像分類和強化學習。EVA 展現出比競爭對手更快的收斂速度，並在每個領域的眾多任務中獲得最高的平均分數。

##### **Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate**
2410.07167v1 by Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu

We present the Modality Integration Rate (MIR), an effective, robust, and
generalized metric to indicate the multi-modal pre-training quality of Large
Vision Language Models (LVLMs). Large-scale pre-training plays a critical role
in building capable LVLMs, while evaluating its training quality without the
costly supervised fine-tuning stage is under-explored. Loss, perplexity, and
in-context evaluation results are commonly used pre-training metrics for Large
Language Models (LLMs), while we observed that these metrics are less
indicative when aligning a well-trained LLM with a new modality. Due to the
lack of proper metrics, the research of LVLMs in the critical pre-training
stage is hindered greatly, including the training data choice, efficient module
design, etc. In this paper, we propose evaluating the pre-training quality from
the inter-modal distribution distance perspective and present MIR, the Modality
Integration Rate, which is 1) \textbf{Effective} to represent the pre-training
quality and show a positive relation with the benchmark performance after
supervised fine-tuning. 2) \textbf{Robust} toward different training/evaluation
data. 3) \textbf{Generalize} across training configurations and architecture
choices. We conduct a series of pre-training experiments to explore the
effectiveness of MIR and observe satisfactory results that MIR is indicative
about training data selection, training strategy schedule, and model
architecture design to get better pre-training results. We hope MIR could be a
helpful metric for building capable LVLMs and inspire the following research
about modality alignment in different areas. Our code is at:
https://github.com/shikiw/Modality-Integration-Rate.

摘要：<paragraph>我們提出模態整合率 (MIR)，一個有效、穩健且通用的指標，用以表示大型視覺語言模型 (LVLMs) 的多模態預訓練品質。大規模預訓練在建構有能力的 LVLMs 中扮演關鍵角色，同時在沒有昂貴監督微調階段的情況下評估其訓練品質，卻鮮少被探討。損失、困惑度和情境內評估結果通常用於大型語言模型 (LLMs) 的預訓練指標，而我們觀察到在將訓練良好的 LLM 與新模態對齊時，這些指標較不具指示性。由於缺乏適當的指標，LVLMs 在關鍵預訓練階段的研究受到極大阻礙，包括訓練資料選擇、有效模組設計等。在本文中，我們提議從模態間分佈距離的角度評估預訓練品質，並提出 MIR（模態整合率），其 1) \textbf{有效} 表示預訓練品質，並在監督微調後顯示與基準效能的正向關係。2) \textbf{穩健} 對不同的訓練/評估資料。3) \textbf{通用} 適用於各種訓練組態和架構選擇。我們進行一系列預訓練實驗，以探討 MIR 的有效性，並觀察到令人滿意的結果，即 MIR 能指示訓練資料選擇、訓練策略時程和模型架構設計，以獲得更好的預訓練結果。我們希望 MIR 能成為建構有能力的 LVLMs 的有用指標，並激勵在不同領域中進行模態對齊的後續研究。我們的程式碼位於：
https://github.com/shikiw/Modality-Integration-Rate。</paragraph>

##### **Sylber: Syllabic Embedding Representation of Speech from Raw Audio**
2410.07168v1 by Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W Black, Gopala K. Anumanchipalli

Syllables are compositional units of spoken language that play a crucial role
in human speech perception and production. However, current neural speech
representations lack structure, resulting in dense token sequences that are
costly to process. To bridge this gap, we propose a new model, Sylber, that
produces speech representations with clean and robust syllabic structure.
Specifically, we propose a self-supervised model that regresses features on
syllabic segments distilled from a teacher model which is an exponential moving
average of the model in training. This results in a highly structured
representation of speech features, offering three key benefits: 1) a fast,
linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization
with an average of 4.27 tokens per second, and 3) syllabic units better suited
for lexical and syntactic understanding. We also train token-to-speech
generative models with our syllabic units and show that fully intelligible
speech can be reconstructed from these tokens. Lastly, we observe that
categorical perception, a linguistic phenomenon of speech perception, emerges
naturally in our model, making the embedding space more categorical and sparse
than previous self-supervised learning approaches. Together, we present a novel
self-supervised approach for representing speech as syllables, with significant
potential for efficient speech tokenization and spoken language modeling.

摘要：音节是口语的组成单位，在人类言语感知和产生中扮演着至关重要的角色。然而，当前的神经语言表征缺乏结构，导致密集的标记序列难以处理。为了弥合这一差距，我们提出了一个新模型 Sylber，它可以生成具有清晰且稳健音节结构的语言表征。具体来说，我们提出了一种自监督模型，它对从教师模型中提取的音节片段进行特征回归，该教师模型是对训练中模型的指数移动平均。这产生了语言特征的高度结构化表征，提供了三个主要好处：1）快速、线性时间的音节分割算法，2）高效的音节标记化，平均每秒 4.27 个标记，以及 3）更适合词法和句法理解的音节单位。我们还使用我们的音节单位训练了标记到语音的生成模型，并表明可以从这些标记重建完全可理解的语音。最后，我们观察到，作为语音感知的语言现象的范畴知觉，自然而然地出现在我们的模型中，这使得嵌入空间比以前的自监督学习方法更具范畴性和稀疏性。总之，我们提出了一种新颖的自监督方法来表示音节形式的语音，它在高效语音标记化和口语建模方面具有显著的潜力。

##### **Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making**
2410.07166v1 by Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu

We aim to evaluate Large Language Models (LLMs) for embodied decision making.
While a significant body of work has been leveraging LLMs for decision making
in embodied environments, we still lack a systematic understanding of their
performance because they are usually applied in different domains, for
different purposes, and built based on different inputs and outputs.
Furthermore, existing evaluations tend to rely solely on a final success rate,
making it difficult to pinpoint what ability is missing in LLMs and where the
problem lies, which in turn blocks embodied agents from leveraging LLMs
effectively and selectively. To address these limitations, we propose a
generalized interface (Embodied Agent Interface) that supports the
formalization of various types of tasks and input-output specifications of
LLM-based modules. Specifically, it allows us to unify 1) a broad set of
embodied decision-making tasks involving both state and temporally extended
goals, 2) four commonly-used LLM-based modules for decision making: goal
interpretation, subgoal decomposition, action sequencing, and transition
modeling, and 3) a collection of fine-grained metrics which break down
evaluation into various types of errors, such as hallucination errors,
affordance errors, various types of planning errors, etc. Overall, our
benchmark offers a comprehensive assessment of LLMs' performance for different
subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI
systems, and providing insights for effective and selective use of LLMs in
embodied decision making.

摘要：我們旨在評估大型語言模型 (LLM) 對於具體決策制定。
儘管大量的工作已經利用 LLM 在具體環境中進行決策制定，但我們仍然缺乏對其效能的系統性理解，因為它們通常應用於不同的領域，用於不同的目的，並根據不同的輸入和輸出進行建構。
此外，現有的評估傾向於僅依賴最終成功率，這使得難以精確指出 LLM 中缺少哪些能力以及問題出在哪裡，進而阻礙具體代理人有效且選擇性地利用 LLM。
為了解決這些限制，我們提出一個通用介面（具體代理介面），支援各種任務的正式化以及基於 LLM 的模組的輸入輸出規格。
具體來說，它允許我們統一 1) 廣泛的具體決策制定任務，包括狀態和時間延伸目標，2) 四個常用的基於 LLM 的決策制定模組：目標詮釋、子目標分解、動作排序和轉換建模，以及 3) 一組細粒度的指標，將評估分解為各種類型的錯誤，例如幻覺錯誤、可供性錯誤、各種類型的規劃錯誤等。
總體而言，我們的基準提供對 LLM 在不同子任務中的效能進行全面評估，精確指出由 LLM 驅動的具體 AI 系統中的優點和缺點，並提供見解，以有效且選擇性地使用 LLM 進行具體決策制定。

##### **Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning**
2410.07163v1 by Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, Sijia Liu

In this work, we address the problem of large language model (LLM)
unlearning, aiming to remove unwanted data influences and associated model
capabilities (e.g., copyrighted data or harmful content generation) while
preserving essential model utilities, without the need for retraining from
scratch. Despite the growing need for LLM unlearning, a principled optimization
framework remains lacking. To this end, we revisit the state-of-the-art
approach, negative preference optimization (NPO), and identify the issue of
reference model bias, which could undermine NPO's effectiveness, particularly
when unlearning forget data of varying difficulty. Given that, we propose a
simple yet effective unlearning optimization framework, called SimNPO, showing
that 'simplicity' in removing the reliance on a reference model (through the
lens of simple preference optimization) benefits unlearning. We also provide
deeper insights into SimNPO's advantages, supported by analysis using mixtures
of Markov chains. Furthermore, we present extensive experiments validating
SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU
and MUSE, and robustness against relearning attacks. Codes are available at
https://github.com/OPTML-Group/Unlearn-Simple.

摘要：在這項工作中，我們探討大型語言模型 (LLM) 的遺忘問題，旨在移除不需要的資料影響和相關的模型功能（例如，受版權保護的資料或有害內容產生），同時保留必要的模型實用程式，而無需從頭開始重新訓練。儘管對 LLM 遺忘的需求越來越大，但仍然缺乏一個有原則的最佳化架構。為此，我們重新探討了最先進的方法，負偏好最佳化 (NPO)，並找出參考模型偏差的問題，它可能會破壞 NPO 的效能，特別是在遺忘不同難度的遺忘資料時。有鑑於此，我們提出了一個簡單但有效的遺忘最佳化架構，稱為 SimNPO，顯示移除對參考模型的依賴（透過簡單偏好最佳化的觀點）的「簡單性」有助於遺忘。我們也提供了對 SimNPO 優點的更深入見解，並以使用馬可夫鏈混合物的分析作為支持。此外，我們提出了廣泛的實驗，驗證了 SimNPO 優於現有的遺忘基準，例如 TOFU 和 MUSE，以及對重新學習攻擊的穩健性。程式碼可在 https://github.com/OPTML-Group/Unlearn-Simple 取得。

##### **InstructG2I: Synthesizing Images from Multimodal Attributed Graphs**
2410.07157v1 by Bowen Jin, Ziqi Pang, Bingjun Guo, Yu-Xiong Wang, Jiaxuan You, Jiawei Han

In this paper, we approach an overlooked yet critical task Graph2Image:
generating images from multimodal attributed graphs (MMAGs). This task poses
significant challenges due to the explosion in graph size, dependencies among
graph entities, and the need for controllability in graph conditions. To
address these challenges, we propose a graph context-conditioned diffusion
model called InstructG2I. InstructG2I first exploits the graph structure and
multimodal information to conduct informative neighbor sampling by combining
personalized page rank and re-ranking based on vision-language features. Then,
a Graph-QFormer encoder adaptively encodes the graph nodes into an auxiliary
set of graph prompts to guide the denoising process of diffusion. Finally, we
propose graph classifier-free guidance, enabling controllable generation by
varying the strength of graph guidance and multiple connected edges to a node.
Extensive experiments conducted on three datasets from different domains
demonstrate the effectiveness and controllability of our approach. The code is
available at https://github.com/PeterGriffinJin/InstructG2I.

摘要：在本文中，我们探讨一项被忽视但至关重要的任务 Graph2Image：
从多模态属性图 (MMAG) 生成图像。由于图大小激增、图实体之间的依赖关系以及对图条件的可控性需求，此任务带来了重大挑战。为了应对这些挑战，我们提出了一种称为 InstructG2I 的图上下文条件扩散模型。InstructG2I 首先利用图结构和多模态信息，通过结合个性化页面排名和基于视觉语言特征的重新排名来执行信息丰富的邻居采样。然后，Graph-QFormer 编码器将图节点自适应地编码为一组辅助图提示，以指导扩散的去噪过程。最后，我们提出了无图分类器指导，通过改变图指导的强度和与节点的多个连接边来实现可控生成。在来自不同领域的三组数据集上进行的广泛实验证明了我们方法的有效性和可控性。代码可在 https://github.com/PeterGriffinJin/InstructG2I 获得。

##### **Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling**
2410.07145v1 by Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun

One essential advantage of recurrent neural networks (RNNs) over
transformer-based language models is their linear computational complexity
concerning the sequence length, which makes them much faster in handling long
sequences during inference. However, most publicly available RNNs (e.g., Mamba
and RWKV) are trained on sequences with less than 10K tokens, and their
effectiveness in longer contexts remains largely unsatisfying so far. In this
paper, we study the cause of the inability to process long context for RNNs and
suggest critical mitigations. We examine two practical concerns when applying
state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to
inputs longer than the training length and (2) the upper bound of memory
capacity. Addressing the first concern, we first investigate *state collapse*
(SC), a phenomenon that causes severe performance degradation on sequence
lengths not encountered during training. With controlled experiments, we
attribute this to overfitting due to the recurrent state being
overparameterized for the training length. For the second concern, we train a
series of Mamba-2 models on long documents to empirically estimate the
recurrent state capacity in language modeling and passkey retrieval. Then,
three SC mitigation methods are proposed to improve Mamba-2's length
generalizability, allowing the model to process more than 1M tokens without SC.
We also find that the recurrent state capacity in passkey retrieval scales
exponentially to the state size, and we empirically train a Mamba-2 370M with
near-perfect passkey retrieval accuracy on 256K context length. This suggests a
promising future for RNN-based long-context modeling.

摘要：遞迴神經網路 (RNN) 相較於基於Transformer的語言模型，有一個重要的優勢，就是其線性計算複雜度與序列長度有關，這使得它們在推理過程中處理長序列時快很多。然而，大多數公開的 RNN（例如 Mamba 和 RWKV）都是針對少於 10K 符號的序列進行訓練，而它們在較長脈絡中的有效性至今仍極不令人滿意。在本文中，我們研究了 RNN 無法處理長脈絡的原因，並提出重要的解決方案。我們探討了在將最先進的 RNN 應用於長脈絡時遇到的兩個實際問題：(1) 無法外推到長於訓練長度的輸入，以及 (2) 記憶容量的上限。針對第一個問題，我們首先探討「狀態崩潰」(SC)，這是一個會導致在訓練過程中未遇到的序列長度上造成嚴重效能下降的現象。透過受控實驗，我們將此歸因於由於遞迴狀態對於訓練長度而言過度參數化而導致的過度擬合。針對第二個問題，我們在長文件中訓練了一系列 Mamba-2 模型，以經驗估計語言建模和密鑰檢索中的遞迴狀態容量。然後，提出了三種 SC 緩解方法來改善 Mamba-2 的長度泛化性，讓模型能夠處理超過 100 萬個符號而不會發生 SC。我們還發現密鑰檢索中的遞迴狀態容量與狀態大小呈指數比例，並且我們經驗訓練了一個具有 256K 脈絡長度近乎完美的密鑰檢索精確度的 Mamba-2 370M。這表明基於 RNN 的長脈絡建模具有光明的未來。

##### **Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates**
2410.07137v1 by Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin

Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and
MT-Bench, have become popular for evaluating language models due to their
cost-effectiveness and scalability compared to human evaluation. Achieving high
win rates on these benchmarks can significantly boost the promotional impact of
newly released language models. This promotional benefit may motivate tricks,
such as manipulating model output length or style to game win rates, even
though several mechanisms have been developed to control length and disentangle
style to reduce gameability. Nonetheless, we show that even a "null model" that
always outputs a constant response (irrelevant to input instructions) can cheat
automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on
AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.
Moreover, the crafted cheating outputs are transferable because we assume that
the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are
private and cannot be accessed. While our experiments are primarily
proof-of-concept, an adversary could use LLMs to generate more imperceptible
cheating responses, unethically benefiting from high win rates and promotional
impact. Our findings call for the development of anti-cheating mechanisms for
reliable automatic benchmarks. The code is available at
https://github.com/sail-sg/Cheating-LLM-Benchmarks.

摘要：自動 LLM 評量基準，例如 AlpacaEval 2.0、Arena-Hard-Auto 和 MT-Bench，由於其與人工評量相比具有成本效益且可擴充性，因此已廣泛用於評量語言模型。在這些評量基準上取得高獲勝率可以顯著提升新發布語言模型的宣傳影響力。這種宣傳效益可能會激勵一些技巧，例如操縱模型輸出長度或風格以玩弄獲勝率，儘管已經開發出多種機制來控制長度和解開風格以減少可玩性。儘管如此，我們表明即使總是輸出恆定回應（與輸入指令無關）的「空模型」也能欺騙自動評量基準並獲得排名最高的獲勝率：在 AlpacaEval 2.0 上獲得 86.5% 的 LC 獲勝率；在 Arena-Hard-Auto 上獲得 83.0 分；在 MT-Bench 上獲得 9.55 分。此外，精心製作的作弊輸出是可以轉移的，因為我們假設這些評量基準的指令（例如 AlpacaEval 2.0 的 805 個範例）是私有的，無法存取。雖然我們的實驗主要是概念驗證，但對手可以使用 LLM 產生更難察覺的作弊回應，不道德地受益於高獲勝率和宣傳影響力。我們的發現要求開發防作弊機制以確保自動評量基準的可靠性。程式碼可在 https://github.com/sail-sg/Cheating-LLM-Benchmarks 取得。

##### **Mental Disorders Detection in the Era of Large Language Models**
2410.07129v1 by Gleb Kuzmin, Petr Strepetov, Maksim Stankevich, Ivan Smirnov, Artem Shelmanov

This paper compares the effectiveness of traditional machine learning
methods, encoder-based models, and large language models (LLMs) on the task of
detecting depression and anxiety. Five datasets were considered, each differing
in format and the method used to define the target pathology class. We tested
AutoML models based on linguistic features, several variations of encoder-based
Transformers such as BERT, and state-of-the-art LLMs as pathology
classification models. The results demonstrated that LLMs outperform
traditional methods, particularly on noisy and small datasets where training
examples vary significantly in text length and genre. However, psycholinguistic
features and encoder-based models can achieve performance comparable to
language models when trained on texts from individuals with clinically
confirmed depression, highlighting their potential effectiveness in targeted
clinical applications.

摘要：本文比較了傳統機器學習方法、編碼器模型和大型語言模型 (LLM) 在憂鬱和焦慮症偵測任務上的有效性。考慮了五個資料集，每個資料集在格式和用於定義目標病理類別的方法上都不同。我們測試了基於語言特徵的 AutoML 模型、多種編碼器模型（如 BERT）的變體，以及最新的 LLM 作為病理分類模型。結果表明，LLM 優於傳統方法，特別是在訓練範例在文字長度和類型上差異很大的嘈雜且小的資料集上。然而，當使用經臨床證實患有憂鬱症的個人的文字進行訓練時，心理語言學特徵和編碼器模型可以達到與語言模型相當的性能，突顯了它們在目標臨床應用中的潛在有效性。

##### **Exploring the Readiness of Prominent Small Language Models for the Democratization of Financial Literacy**
2410.07118v1 by Tagore Rao Kosireddy, Jeffrey D. Wall, Evan Lucas

The use of small language models (SLMs), herein defined as models with less
than three billion parameters, is increasing across various domains and
applications. Due to their ability to run on more accessible hardware and
preserve user privacy, SLMs possess the potential to democratize access to
language models for individuals of different socioeconomic status and with
different privacy preferences. This study assesses several state-of-the-art
SLMs (e.g., Apple's OpenELM, Microsoft's Phi, Google's Gemma, and the Tinyllama
project) for use in the financial domain to support the development of
financial literacy LMs. Democratizing access to quality financial information
for those who are financially under educated is greatly needed in society,
particularly as new financial markets and products emerge and participation in
financial markets increases due to ease of access. We are the first to examine
the use of open-source SLMs to democratize access to financial question
answering capabilities for individuals and students. To this end, we provide an
analysis of the memory usage, inference time, similarity comparisons to
ground-truth answers, and output readability of prominent SLMs to determine
which models are most accessible and capable of supporting access to financial
information. We analyze zero-shot and few-shot learning variants of the models.
The results suggest that some off-the-shelf SLMs merit further exploration and
fine-tuning to prepare them for individual use, while others may have limits to
their democratization.

摘要：小型語言模型 (SLM) 的使用，本文定義為參數少於 30 億的模型，正於各個領域和應用中增加。由於 SLM 能在更易取得的硬體上執行並保護使用者隱私，因此 SLM 具有民主化語言模型的使用權，讓不同社會經濟地位和隱私偏好的個人都能使用。本研究評估了數個最先進的 SLM（例如 Apple 的 OpenELM、Microsoft 的 Phi、Google 的 Gemma 和 Tinyllama 專案），以用於金融領域，以支援金融素養 LM 的開發。社會極需要民主化對高品質財務資訊的取得，特別是隨著新金融市場和產品的出現，以及由於易於取得而增加對金融市場的參與。我們是第一個探討使用開源 SLM，以民主化對個人和學生的財務問題解答能力的取得。為此，我們分析了記憶體使用量、推論時間、與正確答案的相似性比較，以及傑出 SLM 的輸出可讀性，以確定哪些模型最容易取得，且有能力支援取得財務資訊。我們分析了模型的零次學習和少次學習變體。結果表明，一些現成的 SLM 值得進一步探討和微調，以準備個人使用，而其他 SLM 可能有其民主化限制。

##### **VHELM: A Holistic Evaluation of Vision Language Models**
2410.07112v1 by Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, Percy Liang

Current benchmarks for assessing vision-language models (VLMs) often focus on
their perception or problem-solving capabilities and neglect other critical
aspects such as fairness, multilinguality, or toxicity. Furthermore, they
differ in their evaluation procedures and the scope of the evaluation, making
it difficult to compare models. To address these issues, we extend the HELM
framework to VLMs to present the Holistic Evaluation of Vision Language Models
(VHELM). VHELM aggregates various datasets to cover one or more of the 9
aspects: visual perception, knowledge, reasoning, bias, fairness,
multilinguality, robustness, toxicity, and safety. In doing so, we produce a
comprehensive, multi-dimensional view of the capabilities of the VLMs across
these important factors. In addition, we standardize the standard inference
parameters, methods of prompting, and evaluation metrics to enable fair
comparisons across models. Our framework is designed to be lightweight and
automatic so that evaluation runs are cheap and fast. Our initial run evaluates
22 VLMs on 21 existing datasets to provide a holistic snapshot of the models.
We uncover new key findings, such as the fact that efficiency-focused models
(e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than
their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark
but not when evaluated on the other aspects. For transparency, we release the
raw model generations and complete results on our website
(https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living
benchmark, and we hope to continue adding new datasets and models over time.

摘要：目前用於評估視覺語言模型 (VLM) 的基準，通常著重於它們的感知或問題解決能力，而忽略了公平性、多語言性或毒性等其他重要方面。此外，它們在評估程序和評估範圍上有所不同，這使得比較模型變得困難。為了解決這些問題，我們將 HELM 框架擴展到 VLM，以呈現視覺語言模型的整體評估 (VHELM)。VHELM 彙總各種資料集，涵蓋 9 個方面中的至少一個：視覺感知、知識、推理、偏見、公平性、多語言性、健壯性、毒性和安全性。這樣一來，我們可以全面、多面向地了解 VLM 在這些重要因素方面的能力。此外，我們標準化標準推論參數、提示方法和評估指標，以實現跨模型的公平比較。我們的框架旨在輕量且自動化，以便評估執行便宜且快速。我們的初始執行在 21 個現有資料集上評估 22 個 VLM，以提供模型的整體快照。我們發現了新的關鍵發現，例如，以效率為重點的模型（例如，Claude 3 Haiku 或 Gemini 1.5 Flash）在偏見基準上表現明顯遜於其完整模型（例如，Claude 3 Opus 或 Gemini 1.5 Pro），但在其他方面評估時則不然。為了透明起見，我們在我們的網站（https://crfm.stanford.edu/helm/vhelm/v2.0.1）上發布原始模型生成和完整結果。VHELM 旨在成為一個實時基準，我們希望隨著時間的推移繼續添加新的資料集和模型。

##### **I Want to Break Free! Anti-Social Behavior and Persuasion Ability of LLMs in Multi-Agent Settings with Social Hierarchy**
2410.07109v1 by Gian Maria Campedelli, Nicolò Penzo, Massimo Stefan, Roberto Dessì, Marco Guerini, Bruno Lepri, Jacopo Staiano

As Large Language Model (LLM)-based agents become increasingly autonomous and
will more freely interact with each other, studying interactions between them
becomes crucial to anticipate emergent phenomena and potential risks. Drawing
inspiration from the widely popular Stanford Prison Experiment, we contribute
to this line of research by studying interaction patterns of LLM agents in a
context characterized by strict social hierarchy. We do so by specifically
studying two types of phenomena: persuasion and anti-social behavior in
simulated scenarios involving a guard and a prisoner agent who seeks to achieve
a specific goal (i.e., obtaining additional yard time or escape from prison).
Leveraging 200 experimental scenarios for a total of 2,000 machine-machine
conversations across five different popular LLMs, we provide a set of
noteworthy findings. We first document how some models consistently fail in
carrying out a conversation in our multi-agent setup where power dynamics are
at play. Then, for the models that were able to engage in successful
interactions, we empirically show how the goal that an agent is set to achieve
impacts primarily its persuasiveness, while having a negligible effect with
respect to the agent's anti-social behavior. Third, we highlight how agents'
personas, and particularly the guard's personality, drive both the likelihood
of successful persuasion from the prisoner and the emergence of anti-social
behaviors. Fourth, we show that even without explicitly prompting for specific
personalities, anti-social behavior emerges by simply assigning agents' roles.
These results bear implications for the development of interactive LLM agents
as well as the debate on their societal impact.

摘要：隨著大型語言模型 (LLM) 為基礎的代理日益自主，且將更自由地彼此互動，研究它們之間的互動關係對於預測新興現象和潛在風險至關重要。從廣受歡迎的史丹佛監獄實驗中汲取靈感，我們透過研究 LLM 代理在以嚴格社會階層為特徵的環境中的互動模式，為這條研究路線做出貢獻。我們透過特別研究兩種現象類型來做到這一點：在模擬情境中涉及一名警衛和一名囚犯代理的說服和反社會行為，該囚犯代理試圖達成特定目標（例如獲得額外的庭院時間或逃離監獄）。我們利用 200 個實驗情境，總計 2,000 次橫跨五種不同熱門 LLM 的機器對機器對話，提供一組值得注意的發現。我們首先記錄一些模型在我們的多重代理設定中始終無法進行對話，其中包含權力動態。然後，對於能夠進行成功互動的模型，我們以經驗方式展示代理設定要達成的目標主要影響其說服力，同時對代理的反社會行為幾乎沒有影響。第三，我們強調代理的角色，尤其是警衛的人格，如何推動囚犯成功說服的可能性和反社會行為的出現。第四，我們展示即使沒有明確提示特定人格，反社會行為也會透過簡單地分配代理角色而出現。這些結果對互動式 LLM 代理的發展及其對社會影響的辯論具有影響。

##### **Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context**
2410.07103v1 by Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon

Multi-hop reasoning, which requires multi-step reasoning based on the
supporting documents within a given context, remains challenging for large
language models (LLMs). LLMs often struggle to filter out irrelevant documents
within the context, and their performance is sensitive to the position of
supporting documents within that context. In this paper, we identify an
additional challenge: LLMs' performance is also sensitive to the order in which
the supporting documents are presented. We refer to this as the misordered
context problem. To address this issue, we propose a simple yet effective
method called context repetition (CoRe), which involves prompting the model by
repeatedly presenting the context to ensure the supporting documents are
presented in the optimal order for the model. Using CoRe, we improve the F1
score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p
on a synthetic task. Additionally, CoRe helps mitigate the well-known
"lost-in-the-middle" problem in LLMs and can be effectively combined with
retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.

摘要：多跳推理需要根據給定脈絡中的支援文件進行多步驟推理，這對大型語言模型 (LLM) 來說仍然具有挑戰性。LLM 經常難以在脈絡中過濾掉無關的文件，而且它們的效能會受到支援文件在該脈絡中的位置影響。在本文中，我們找出另一個挑戰：LLM 的效能也會受到支援文件呈現順序的影響。我們將此稱為順序錯誤的脈絡問題。為了解決這個問題，我們提出一個簡單但有效的方法，稱為脈絡重複 (CoRe)，其中涉及透過重複呈現脈絡來提示模型，以確保支援文件以模型的最佳順序呈現。使用 CoRe，我們將多跳問答任務的 F1 分數提高了 30%，並在合成任務中將準確度提高了 70%。此外，CoRe 有助於減輕 LLM 中眾所周知的「迷失在中間」問題，並且可以有效地與利用思想鏈 (CoT) 推理的基於檢索的方法結合使用。

##### **MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering**
2410.07095v1 by Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, Aleksander Mądry

We introduce MLE-bench, a benchmark for measuring how well AI agents perform
at machine learning engineering. To this end, we curate 75 ML
engineering-related competitions from Kaggle, creating a diverse set of
challenging tasks that test real-world ML engineering skills such as training
models, preparing datasets, and running experiments. We establish human
baselines for each competition using Kaggle's publicly available leaderboards.
We use open-source agent scaffolds to evaluate several frontier language models
on our benchmark, finding that the best-performing setup--OpenAI's o1-preview
with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in
16.9% of competitions. In addition to our main results, we investigate various
forms of resource scaling for AI agents and the impact of contamination from
pre-training. We open-source our benchmark code (github.com/openai/mle-bench/)
to facilitate future research in understanding the ML engineering capabilities
of AI agents.

摘要：我們推出 MLE-bench，一個用於衡量 AI 代理在機器學習工程方面的表現的基準測試。為此，我們從 Kaggle 精選了 75 個與 ML 工程相關的競賽，建立了一組多樣化的挑戰性任務，用於測試實際的 ML 工程技能，例如訓練模型、準備資料集和執行實驗。我們使用 Kaggle 公開提供的排行榜為每個競賽建立人類基準。我們使用開源代理架構在我們的基準測試中評估了幾個前沿語言模型，發現表現最佳的設置——OpenAI 的 o1-preview 與 AIDE 架構——在 16.9% 的競賽中至少達到 Kaggle 銅牌水準。除了我們的主要結果之外，我們還研究了 AI 代理資源擴充的各種形式以及預訓練造成的污染影響。我們開源了我們的基準測試程式碼 (github.com/openai/mle-bench/)，以利於未來研究了解 AI 代理的 ML 工程能力。

##### **An Approach for Auto Generation of Labeling Functions for Software Engineering Chatbots**
2410.07094v1 by Ebube Alor, Ahmad Abdellatif, SayedHassan Khatoonabadi, Emad Shihab

Software engineering (SE) chatbots are increasingly gaining attention for
their role in enhancing development processes. At the core of chatbots are the
Natural Language Understanding platforms (NLUs), which enable them to
comprehend and respond to user queries. Before deploying NLUs, there is a need
to train them with labeled data. However, acquiring such labeled data for SE
chatbots is challenging due to the scarcity of high-quality datasets. This
challenge arises because training SE chatbots requires specialized vocabulary
and phrases not found in typical language datasets. Consequently, chatbot
developers often resort to manually annotating user queries to gather the data
necessary for training effective chatbots, a process that is both
time-consuming and resource-intensive. Previous studies propose approaches to
support chatbot practitioners in annotating users' posed queries. However,
these approaches require human intervention to generate rules, called labeling
functions (LFs), that identify and categorize user queries based on specific
patterns in the data. To address this issue, we propose an approach to
automatically generate LFs by extracting patterns from labeled user queries. We
evaluate the effectiveness of our approach by applying it to the queries of
four diverse SE datasets (namely AskGit, MSA, Ask Ubuntu, and Stack Overflow)
and measure the performance improvement gained from training the NLU on the
queries labeled by the generated LFs. We find that the generated LFs
effectively label data with AUC scores of up to 85.3%, and NLU's performance
improvement of up to 27.2% across the studied datasets. Furthermore, our
results show that the number of LFs used to generate LFs affects the labeling
performance. We believe that our approach can save time and resources in
labeling users' queries, allowing practitioners to focus on core chatbot
functionalities.

摘要：軟體工程（SE）聊天機器人因其在強化開發流程中的角色而越來越受到關注。聊天機器人的核心是自然語言理解平台（NLU），它使聊天機器人能夠理解並回應使用者的查詢。在部署 NLU 之前，需要使用標記資料訓練它們。然而，由於缺乏高品質的資料集，取得此類標記資料來訓練 SE 聊天機器人是一項挑戰。這個挑戰之所以產生，是因為訓練 SE 聊天機器人需要在典型的語言資料集中找不到的特殊詞彙和片語。因此，聊天機器人開發人員經常訴諸人工標記使用者查詢，以收集訓練有效聊天機器人所需的資料，這是一個耗時且耗費資源的過程。先前的研究提出方法來支援聊天機器人從業人員標記使用者提出的查詢。然而，這些方法需要人工介入來產生規則，稱為標記函數（LF），根據資料中的特定模式識別和分類使用者查詢。為了解決這個問題，我們提出了一種方法，透過從標記的使用者查詢中提取模式來自動產生 LF。我們透過將我們的做法應用於四個不同的 SE 資料集（即 AskGit、MSA、Ask Ubuntu 和 Stack Overflow）的查詢來評估其有效性，並衡量在由產生的 LF 標記的查詢上訓練 NLU 所獲得的效能提升。我們發現產生的 LF 有效地標記資料，AUC 分數高達 85.3%，而 NLU 的效能提升在研究的資料集中高達 27.2%。此外，我們的結果顯示用於產生 LF 的 LF 數量會影響標記效能。我們相信我們的做法可以節省標記使用者查詢的時間和資源，讓從業人員可以專注於核心的聊天機器人功能。

##### **Stanceformer: Target-Aware Transformer for Stance Detection**
2410.07083v1 by Krishna Garg, Cornelia Caragea

The task of Stance Detection involves discerning the stance expressed in a
text towards a specific subject or target. Prior works have relied on existing
transformer models that lack the capability to prioritize targets effectively.
Consequently, these models yield similar performance regardless of whether we
utilize or disregard target information, undermining the task's significance.
To address this challenge, we introduce Stanceformer, a target-aware
transformer model that incorporates enhanced attention towards the targets
during both training and inference. Specifically, we design a \textit{Target
Awareness} matrix that increases the self-attention scores assigned to the
targets. We demonstrate the efficacy of the Stanceformer with various
BERT-based models, including state-of-the-art models and Large Language Models
(LLMs), and evaluate its performance across three stance detection datasets,
alongside a zero-shot dataset. Our approach Stanceformer not only provides
superior performance but also generalizes even to other domains, such as
Aspect-based Sentiment Analysis. We make the code publicly
available.\footnote{\scriptsize\url{https://github.com/kgarg8/Stanceformer}}

摘要：立場偵測的任務涉及分辨文本中針對特定主旨或目標所表達的立場。先前的著作仰賴現有的轉換器模型，這些模型缺乏有效優先處理目標的能力。因此，這些模型產生類似的效能，無論我們是否使用或忽略目標資訊，都損害了任務的重要性。為了應對此挑戰，我們引進了立場轉換器，一種目標感知轉換器模型，在訓練和推論期間都納入了對目標的加強關注。具體來說，我們設計了一個「目標感知」矩陣，以增加指定給目標的自注意力分數。我們展示了立場轉換器與各種基於 BERT 的模型的效能，包括最先進的模型和大語言模型 (LLM)，並評估其在三個立場偵測資料集以及零次學習資料集中的效能。我們的立場轉換器方法不僅提供了卓越的效能，甚至可以概括到其他領域，例如基於面向的觀點分析。我們公開了程式碼。\footnote{\scriptsize\url{https://github.com/kgarg8/Stanceformer}}

##### **MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses**
2410.07076v1 by Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou

Scientific discovery contributes largely to human society's prosperity, and
recent progress shows that LLMs could potentially catalyze this process.
However, it is still unclear whether LLMs can discover novel and valid
hypotheses in chemistry. In this work, we investigate this central research
question: Can LLMs automatically discover novel and valid chemistry research
hypotheses given only a chemistry research background (consisting of a research
question and/or a background survey), without limitation on the domain of the
research question? After extensive discussions with chemistry experts, we
propose an assumption that a majority of chemistry hypotheses can be resulted
from a research background and several inspirations. With this key insight, we
break the central question into three smaller fundamental questions. In brief,
they are: (1) given a background question, whether LLMs can retrieve good
inspirations; (2) with background and inspirations, whether LLMs can lead to
hypothesis; and (3) whether LLMs can identify good hypotheses to rank them
higher. To investigate these questions, we construct a benchmark consisting of
51 chemistry papers published in Nature, Science, or a similar level in 2024
(all papers are only available online since 2024). Every paper is divided by
chemistry PhD students into three components: background, inspirations, and
hypothesis. The goal is to rediscover the hypothesis, given only the background
and a large randomly selected chemistry literature corpus consisting the ground
truth inspiration papers, with LLMs trained with data up to 2023. We also
develop an LLM-based multi-agent framework that leverages the assumption,
consisting of three stages reflecting the three smaller questions. The proposed
method can rediscover many hypotheses with very high similarity with the ground
truth ones, covering the main innovations.

摘要：<paragraph>科學發現對人類社會的繁榮做出了巨大的貢獻，而最近的進展表明，大型語言模型 (LLM) 有可能催化這一進程。然而，目前尚不清楚 LLM 是否可以在化學中發現新的有效假設。在這項工作中，我們探討了這個核心的研究問題：僅給定化學研究背景（包括研究問題和/或背景調查），不限制研究問題的領域，LLM 能否自動發現新的有效化學研究假設？在與化學專家進行了廣泛討論後，我們提出了一個假設，即大多數化學假設都可以從研究背景和一些靈感產生。有了這個關鍵見解，我們將核心問題分解為三個較小的基本問題。簡而言之，它們是：(1) 給定一個背景問題，LLM 是否可以獲取良好的靈感；(2) 有了背景和靈感，LLM 是否可以得出假設；(3) LLM 是否可以識別出好的假設並對它們進行較高的排名。為了探討這些問題，我們構建了一個基準，其中包括 51 篇於 2024 年發表在《自然》、《科學》或類似級別期刊上的化學論文（所有論文自 2024 年起僅在線提供）。每篇論文都被化學博士生分為三個組成部分：背景、靈感和假設。目標是僅給定背景和一個隨機選擇的大型化學文獻語料庫（其中包含基礎真實靈感論文）重新發現假設，而 LLM 使用截至 2023 年的數據進行訓練。我們還開發了一個基於 LLM 的多主體框架，該框架利用了這一假設，包括反映三個較小問題的三個階段。所提出的方法可以重新發現許多與真實假設非常相似的假設，涵蓋了主要的創新。</paragraph>

##### **Pixtral 12B**
2410.07073v1 by Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang

We introduce Pixtral-12B, a 12--billion-parameter multimodal language model.
Pixtral-12B is trained to understand both natural images and documents,
achieving leading performance on various multimodal benchmarks, surpassing a
number of larger models. Unlike many open-source models, Pixtral is also a
cutting-edge text model for its size, and does not compromise on natural
language performance to excel in multimodal tasks. Pixtral uses a new vision
encoder trained from scratch, which allows it to ingest images at their natural
resolution and aspect ratio. This gives users flexibility on the number of
tokens used to process an image. Pixtral is also able to process any number of
images in its long context window of 128K tokens. Pixtral 12B substanially
outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B).
It also outperforms much larger open models like Llama-3.2 90B while being 7x
smaller. We further contribute an open-source benchmark, MM-MT-Bench, for
evaluating vision-language models in practical scenarios, and provide detailed
analysis and code for standardized evaluation protocols for multimodal LLMs.
Pixtral-12B is released under Apache 2.0 license.

摘要：我們介紹了 Pixtral-12B，一個擁有 120 億個參數的多模態語言模型。
Pixtral-12B 經過訓練，可以理解自然影像和文件，
在各種多模態基準測試中取得領先的效能，超越了
許多較大型的模型。與許多開源模型不同，Pixtral 也是
一個以其規模而言最先進的文字模型，並且不會
為了在多模態任務中表現出色而犧牲自然
語言效能。Pixtral 使用一個從頭訓練的新視覺
編碼器，它允許以其自然解析度和長寬比擷取影像。這讓使用者在用於處理影像的代碼數量上更具彈性。Pixtral 也能夠在其 128K 代碼的長脈絡視窗中處理任意數量的影像。Pixtral 12B 在表現上大幅超越其他類似規模的開放模型（Llama-3.2 11B 和 Qwen-2-VL 7B）。
它也超越了許多大得多的開放模型，例如 Llama-3.2 90B，同時卻小了 7 倍。我們進一步貢獻了一個開放原始碼基準測試 MM-MT-Bench，用於在實際情況下評估視覺語言模型，並提供詳細的分析和用於多模態 LLM 的標準化評估協定的程式碼。
Pixtral-12B 在 Apache 2.0 授權下發布。

##### **Retrieval-Augmented Decision Transformer: External Memory for In-context RL**
2410.07071v1 by Thomas Schmied, Fabian Paischer, Vihang Patil, Markus Hofmarcher, Razvan Pascanu, Sepp Hochreiter

In-context learning (ICL) is the ability of a model to learn a new task by
observing a few exemplars in its context. While prevalent in NLP, this
capability has recently also been observed in Reinforcement Learning (RL)
settings. Prior in-context RL methods, however, require entire episodes in the
agent's context. Given that complex environments typically lead to long
episodes with sparse rewards, these methods are constrained to simple
environments with short episodes. To address these challenges, we introduce
Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external
memory mechanism to store past experiences from which it retrieves only
sub-trajectories relevant for the current situation. The retrieval component in
RA-DT does not require training and can be entirely domain-agnostic. We
evaluate the capabilities of RA-DT on grid-world environments, robotics
simulations, and procedurally-generated video games. On grid-worlds, RA-DT
outperforms baselines, while using only a fraction of their context length.
Furthermore, we illuminate the limitations of current in-context RL methods on
complex environments and discuss future directions. To facilitate future
research, we release datasets for four of the considered environments.

摘要：情境學習 (ICL) 是模型透過觀察其情境中的幾個範例，學習新任務的能力。儘管在自然語言處理 (NLP) 中很普遍，但此功能最近也已在強化學習 (RL) 設定中被觀察到。然而，先前的情境 RL 方法需要代理程式情境中的完整劇集。由於複雜的環境通常會導致具有稀疏獎勵的長劇集，因此這些方法受限於具有短劇集的簡單環境。為了應對這些挑戰，我們引入了檢索增強決策轉換器 (RA-DT)。RA-DT 使用外部記憶體機制來儲存過去的經驗，它僅從中檢索與當前情況相關的子軌跡。RA-DT 中的檢索元件不需要訓練，並且可以完全與領域無關。我們在網格世界環境、機器人模擬和程序生成的電子遊戲上評估 RA-DT 的功能。在網格世界中，RA-DT 優於基準，同時僅使用其情境長度的一小部分。此外，我們闡明了當前情境 RL 方法在複雜環境中的局限性，並討論了未來的方向。為了促進未來的研究，我們發布了四個考慮環境的資料集。

##### **ReIFE: Re-evaluating Instruction-Following Evaluation**
2410.07069v1 by Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan

The automatic evaluation of instruction following typically involves using
large language models (LLMs) to assess response quality. However, there is a
lack of comprehensive evaluation of these LLM-based evaluators across two
dimensions: the base LLMs and the evaluation protocols. Therefore, we present a
thorough meta-evaluation of instruction following, including 25 base LLMs and
15 recently proposed evaluation protocols, on 4 human-annotated datasets,
assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows
us to identify the best-performing base LLMs and evaluation protocols with a
high degree of robustness. Moreover, our large-scale evaluation reveals: (1)
Base LLM performance ranking remains largely consistent across evaluation
protocols, with less capable LLMs showing greater improvement from protocol
enhancements; (2) Robust evaluation of evaluation protocols requires many base
LLMs with varying capability levels, as protocol effectiveness can depend on
the base LLM used; (3) Evaluation results on different datasets are not always
consistent, so a rigorous evaluation requires multiple datasets with
distinctive features. We release our meta-evaluation suite ReIFE, which
provides the codebase and evaluation result collection for more than 500
LLM-evaluator configurations, to support future research in
instruction-following evaluation.

摘要：自動評估指令遵循通常涉及使用大型語言模型 (LLM) 來評估回應品質。然而，這些基於 LLM 的評估器缺乏跨兩個面向的全面評估：基礎 LLM 和評估協定。因此，我們針對指令遵循提出一個徹底的元評估，包括 25 個基礎 LLM 和 15 個最近提出的評估協定，在 4 個由人工標註的資料集上，評估 LLM 評估器的評估準確度。我們的評估讓我們得以識別表現最佳的基礎 LLM 和評估協定，具備高度的穩健性。此外，我們的大規模評估揭露：(1) 基礎 LLM 效能排名在評估協定之間大致保持一致，能力較弱的 LLM 從協定增強中展現出更大的進步；(2) 評估協定的穩健評估需要許多具備不同能力等級的基礎 LLM，因為協定效能可能取決於所使用的基礎 LLM；(3) 不同資料集上的評估結果並不總是保持一致，因此嚴謹的評估需要具備獨特特徵的各種資料集。我們發布我們的元評估套件 ReIFE，提供超過 500 個 LLM 評估器組態的程式碼庫和評估結果收集，以支持指令遵循評估的未來研究。

##### **Data Selection via Optimal Control for Language Models**
2410.07064v1 by Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, Minlie Huang

This work investigates the selection of high-quality pre-training data from
massive corpora to enhance LMs' capabilities for downstream usage. We formulate
data selection as a generalized Optimal Control problem, which can be solved
theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of
necessary conditions that characterize the relationship between optimal data
selection and LM training dynamics. Based on these theoretical results, we
introduce PMP-based Data Selection (PDS), a framework that approximates optimal
data selection by solving the PMP conditions. In our experiments, we adopt PDS
to select data from CommmonCrawl and show that the PDS-selected corpus
accelerates the learning of LMs and constantly boosts their performance on a
wide range of downstream tasks across various model sizes. Moreover, the
benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by
the extrapolation of the test loss curves according to the Scaling Laws. PDS
also improves data utilization when the pre-training data is limited, by
reducing the data demand by 1.8 times, which mitigates the quick exhaustion of
available web-crawled corpora. Our code, data, and model checkpoints can be
found in https://github.com/microsoft/LMOps/tree/main/data_selection.

摘要：本研究探討從海量語料庫中選取高品質預訓練資料，以增強 LM 在下游使用時的效能。我們將資料選取制定為廣義最佳控制問題，此問題可用龐特里亞金最大值原理 (PMP) 在理論上解決，並產生一組必要條件，用於描述最佳資料選取與 LM 訓練動態之間的關係。根據這些理論結果，我們引入了基於 PMP 的資料選取 (PDS)，這是一個透過解決 PMP 條件來近似最佳資料選取的架構。在我們的實驗中，我們採用 PDS 從 CommonCrawl 中選取資料，並顯示 PDS 選取的語料庫加速了 LM 的學習，並持續提升其在各種模型大小中廣泛下游任務上的效能。此外，PDS 的優點延伸至根據擴充定律推算測試損失曲線所證實的，在約 10T 個 token 上訓練的約 400B 模型。當預訓練資料有限時，PDS 也能改善資料利用率，方法是將資料需求減少 1.8 倍，這減輕了可用網路爬取語料庫的快速耗盡。我們的程式碼、資料和模型檢查點可以在 https://github.com/microsoft/LMOps/tree/main/data_selection 中找到。

##### **Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing**
2410.07054v1 by Weichuan Wang, Zhaoyi Li, Defu Lian, Chen Ma, Linqi Song, Ying Wei

Large Language Models (LLMs) have recently revolutionized the NLP field,
while they still fall short in some specific down-stream tasks. In the work, we
focus on utilizing LLMs to perform machine translation, where we observe that
two patterns of errors frequently occur and drastically affect the translation
quality: language mismatch and repetition. The work sets out to explore the
potential for mitigating these two issues by leveraging model editing methods,
e.g., by locating Feed-Forward Network (FFN) neurons or something that are
responsible for the errors and deactivating them in the inference time. We find
that directly applying such methods either limited effect on the targeted
errors or has significant negative side-effect on the general translation
quality, indicating that the located components may also be crucial for
ensuring machine translation with LLMs on the rails. To this end, we propose to
refine the located components by fetching the intersection of the locating
results under different language settings, filtering out the aforementioned
information that is irrelevant to targeted errors. The experiment results
empirically demonstrate that our methods can effectively reduce the language
mismatch and repetition ratios and meanwhile enhance or keep the general
translation quality in most cases.

摘要：大型語言模型 (LLM) 近來徹底改變了自然語言處理領域，
儘管它們在某些特定下游任務中仍有不足。在這項工作中，我們
專注於利用 LLM 來執行機器翻譯，我們觀察到
兩種錯誤模式經常發生並嚴重影響翻譯
品質：語言不匹配和重複。這項工作著手探索
透過利用模型編輯方法來減輕這兩個問題的可能性，
例如，透過定位負責錯誤的饋前網路 (FFN) 神經元或其他東西，並在推論時間停用它們。我們發現
直接應用此類方法對目標
錯誤影響有限，或對一般翻譯
品質有顯著的負面影響，這表示已定位的組成部分也可能是
確保機器翻譯在軌道上使用 LLM 的關鍵。為此，我們建議透過擷取不同語言設定下定位
結果的交集來改善已定位的組件，過濾掉與目標錯誤無關的前述
資訊。實驗結果實證顯示我們的模型可以有效減少語言
不匹配和重複比率，同時在多數情況下提升或維持一般
翻譯品質。

##### **Robots in the Middle: Evaluating LLMs in Dispute Resolution**
2410.07053v1 by Jinzhe Tan, Hannes Westermann, Nikhil Reddy Pottanigari, Jaromír Šavelka, Sébastien Meeùs, Mia Godet, Karim Benyekhlef

Mediation is a dispute resolution method featuring a neutral third-party
(mediator) who intervenes to help the individuals resolve their dispute. In
this paper, we investigate to which extent large language models (LLMs) are
able to act as mediators. We investigate whether LLMs are able to analyze
dispute conversations, select suitable intervention types, and generate
appropriate intervention messages. Using a novel, manually created dataset of
50 dispute scenarios, we conduct a blind evaluation comparing LLMs with human
annotators across several key metrics. Overall, the LLMs showed strong
performance, even outperforming our human annotators across dimensions.
Specifically, in 62% of the cases, the LLMs chose intervention types that were
rated as better than or equivalent to those chosen by humans. Moreover, in 84%
of the cases, the intervention messages generated by the LLMs were rated as
better than or equal to the intervention messages written by humans. LLMs
likewise performed favourably on metrics such as impartiality, understanding
and contextualization. Our results demonstrate the potential of integrating AI
in online dispute resolution (ODR) platforms.

摘要：調解是一種爭議解決方法，特點是由中立的第三方（調解人）介入，協助個人解決爭議。在本文中，我們探討大型語言模型（LLM）在多大程度上能夠擔任調解人的角色。我們探討 LLM 是否能夠分析爭議對話、選擇合適的介入類型，以及產生適當的介入訊息。我們使用一個新穎的手動建立的 50 個爭議情境資料集，進行一項盲測評估，比較 LLM 與人類註解者在幾個關鍵指標上的表現。整體而言，LLM 表現出色，甚至在各個面向都優於我們的人類註解者。具體來說，在 62% 的案例中，LLM 選擇的介入類型被評為優於或等同於人類選擇的類型。此外，在 84% 的案例中，LLM 生成的介入訊息被評為優於或等同於人類撰寫的介入訊息。同樣地，LLM 在公正性、理解力和情境化等指標上也表現良好。我們的結果證明了將 AI 整合到線上爭議解決（ODR）平台的潛力。

##### **Emergent properties with repeated examples**
2410.07041v1 by François Charton, Julia Kempe

We study the performance of transformers as a function of the number of
repetitions of training examples with algorithmically generated datasets. On
three problems of mathematics: the greatest common divisor, modular
multiplication, and matrix eigenvalues, we show that for a fixed number of
training steps, models trained on smaller sets of repeated examples outperform
models trained on larger sets of single-use examples. We also demonstrate that
two-set training - repeated use of a small random subset of examples, along
normal sampling on the rest of the training set - provides for faster learning
and better performance. This highlights that the benefits of repetition can
outweigh those of data diversity. These datasets and problems provide a
controlled setting to shed light on the still poorly understood interplay
between generalization and memorization in deep learning.

摘要：我們研究了Transformer在演算法生成的資料集訓練範例重複次數的函數效能。在三個數學問題中：最大公因數、模乘法和矩陣特徵值，我們顯示對於固定次數的訓練步驟，在較小組重複範例上訓練的模型優於在較大組單次使用範例上訓練的模型。我們也展示了兩組訓練 - 重複使用範例的小隨機子集，以及在訓練集的其餘部分進行常態抽樣 - 提供了更快的學習和更好的效能。這突顯了重複的好處可能超過資料多樣性的好處。這些資料集和問題提供了一個受控的設定，可以闡明深度學習中概括和記憶之間仍然知之甚少的交互作用。

##### **PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness**
2410.07035v1 by Zekun Wang, Feiyu Duan, Yibo Zhang, Wangchunshu Zhou, Ke Xu, Wenhao Huang, Jie Fu

Large Language Models (LLMs) demonstrate impressive capabilities across
various domains, including role-playing, creative writing, mathematical
reasoning, and coding. Despite these advancements, LLMs still encounter
challenges with length control, frequently failing to adhere to specific length
constraints due to their token-level operations and insufficient training on
data with strict length limitations. We identify this issue as stemming from a
lack of positional awareness and propose novel approaches--PositionID Prompting
and PositionID Fine-Tuning--to address it. These methods enhance the model's
ability to continuously monitor and manage text length during generation.
Additionally, we introduce PositionID CP Prompting to enable LLMs to perform
copy and paste operations accurately. Furthermore, we develop two benchmarks
for evaluating length control and copy-paste abilities. Our experiments
demonstrate that our methods significantly improve the model's adherence to
length constraints and copy-paste accuracy without compromising response
quality.

摘要：大型語言模型 (LLM) 在各種領域展現令人印象深刻的能力，包括角色扮演、創意寫作、數學推理和編碼。儘管有這些進展，LLM 仍會遇到長度控制的挑戰，由於它們的代幣級別運算和對長度限制嚴格的數據訓練不足，經常無法遵守特定的長度限制。我們將此問題歸因於缺乏位置感，並提出新穎的方法——位置 ID 提示和位置 ID 微調——來解決它。這些方法增強了模型在生成過程中持續監控和管理文本長度的能力。此外，我們引入了位置 ID CP 提示，讓 LLM 能夠準確執行複製和貼上的操作。此外，我們開發了兩個基準來評估長度控制和複製貼上的能力。我們的實驗表明，我們的模型顯著改善了模型對長度限制的遵守度和複製貼上的準確度，而不會損害回應品質。

##### **Clean Evaluations on Contaminated Visual Language Models**
2410.07030v1 by Hongyuan Lu, Shujie Miao, Wai Lam

How to evaluate large language models (LLMs) cleanly has been established as
an important research era to genuinely report the performance of possibly
contaminated LLMs. Yet, how to cleanly evaluate the visual language models
(VLMs) is an under-studied problem. We propose a novel approach to achieve such
goals through data augmentation methods on the visual input information. We
then craft a new visual clean evaluation benchmark with thousands of data
instances. Through extensive experiments, we found that the traditional visual
data augmentation methods are useful, but they are at risk of being used as a
part of the training data as a workaround. We further propose using BGR
augmentation to switch the colour channel of the visual information. We found
that it is a simple yet effective method for reducing the effect of data
contamination and fortunately, it is also harmful to be used as a data
augmentation method during training. It means that it is hard to integrate such
data augmentation into training by malicious trainers and it could be a
promising technique to cleanly evaluate visual LLMs. Our code, data, and model
weights will be released upon publication.

摘要：如何乾淨地評估大型語言模型（LLM）已確立為一個重要的研究時代，以真正報告可能受到污染的 LLM 的效能。然而，如何乾淨地評估視覺語言模型（VLM）是一個研究不足的問題。我們提出一個創新的方法，透過視覺輸入資訊的資料擴充方法來達成這些目標。然後我們製作一個新的視覺乾淨評估基準，其中有數千個資料實例。透過廣泛的實驗，我們發現傳統的視覺資料擴充方法很有用，但它們有風險會被當成訓練資料的一部分作為一種變通方法。我們進一步提出使用 BGR 擴充來切換視覺資訊的色彩通道。我們發現這是一個簡單但有效的方法，可以減少資料污染的影響，而且幸運的是，它也不適合在訓練期間當成資料擴充方法來使用。這表示惡意的訓練員很難將這種資料擴充整合到訓練中，而且這可能是乾淨評估視覺 LLM 的一種有前途的技術。我們的程式碼、資料和模型權重將在發布後釋出。

##### **Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback**
2410.07025v1 by Dennis Hein, Zhihong Chen, Sophie Ostmeier, Justin Xu, Maya Varma, Eduardo Pontes Reis, Arne Edward Michalson, Christian Bluethgen, Hyun Joo Shin, Curtis Langlotz, Akshay S Chaudhari

Radiologists play a crucial role by translating medical images into medical
reports. However, the field faces staffing shortages and increasing workloads.
While automated approaches using vision-language models (VLMs) show promise as
assistants, they require exceptionally high accuracy. Most current VLMs in
radiology rely solely on supervised fine-tuning (SFT). Meanwhile, in the
general domain, additional preference fine-tuning has become standard practice.
The challenge in radiology lies in the prohibitive cost of obtaining
radiologist feedback. We propose a scalable automated preference alignment
technique for VLMs in radiology, focusing on chest X-ray (CXR) report
generation. Our method leverages publicly available datasets with an
LLM-as-a-Judge mechanism, eliminating the need for additional expert
radiologist feedback. We evaluate and benchmark five direct alignment
algorithms (DAAs). Our results show up to a 57.4% improvement in average GREEN
scores, a LLM-based metric for evaluating CXR reports, and a 9.2% increase in
an average across six metrics (domain specific and general), compared to the
SFT baseline. We study reward overoptimization via length exploitation, with
reports lengthening by up to 3.2x. To assess a potential alignment tax, we
benchmark on six additional diverse tasks, finding no significant degradations.
A reader study involving four board-certified radiologists indicates win rates
of up to 0.62 over the SFT baseline, while significantly penalizing verbosity.
Our analysis provides actionable insights for the development of VLMs in
high-stakes fields like radiology.

摘要：<paragraph>放射科醫師透過將醫學影像轉換成醫學報告扮演著至關重要的角色。然而，該領域面臨人手短缺和工作量增加的問題。雖然使用視覺語言模型 (VLM) 的自動化方法顯示出作為助理的潛力，但它們需要極高的準確度。放射科中大多數現有的 VLM 僅依賴監督微調 (SFT)。同時，在一般領域，額外的偏好微調已成為標準做法。放射科的挑戰在於取得放射科醫師回饋的成本過高。我們提出了一種可擴充的自動化偏好對齊技術，用於放射科的 VLM，專注於胸部 X 光 (CXR) 報告生成。我們的技術利用具備 LLM 作為評審機制的公開可用資料集，消除了需要額外專家放射科醫師回饋的必要性。我們評估和比較了五種直接對齊演算法 (DAA)。我們的結果顯示，平均 GREEN 分數（用於評估 CXR 報告的基於 LLM 的指標）提高了 57.4%，與 SFT 基準相比，六項指標（特定領域和一般）的平均值增加了 9.2%。我們透過長度利用研究獎勵過度最佳化，報告長度最多增加了 3.2 倍。為了評估潛在對齊稅，我們在六項額外的不同任務中進行基準測試，發現沒有顯著的降低。一項涉及四位通過認證的放射科醫師的讀者研究指出，與 SFT 基準相比，獲勝率最高可達 0.62，同時顯著懲罰冗長。我們的分析為放射科等高風險領域的 VLM 開發提供了可行的見解。</paragraph>

##### **Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization**
2410.07018v1 by Chengtao Jian, Kai Yang, Yang Jiao

Out-of-Distribution (OOD) generalization in machine learning is a burgeoning
area of study. Its primary goal is to enhance the adaptability and resilience
of machine learning models when faced with new, unseen, and potentially
adversarial data that significantly diverges from their original training
datasets. In this paper, we investigate time series OOD generalization via
pre-trained Large Language Models (LLMs). We first propose a novel
\textbf{T}ri-level learning framework for \textbf{T}ime \textbf{S}eries
\textbf{O}OD generalization, termed TTSO, which considers both sample-level and
group-level uncertainties. This formula offers a fresh theoretic perspective
for formulating and analyzing OOD generalization problem. In addition, we
provide a theoretical analysis to justify this method is well motivated. We
then develop a stratified localization algorithm tailored for this tri-level
optimization problem, theoretically demonstrating the guaranteed convergence of
the proposed algorithm. Our analysis also reveals that the iteration complexity
to obtain an $\epsilon$-stationary point is bounded by
O($\frac{1}{\epsilon^{2}}$). Extensive experiments on real-world datasets have
been conducted to elucidate the effectiveness of the proposed method.

摘要：機器學習中的異配分 (OOD) 概化是一個新興的研究領域。其主要目標是增強機器學習模型在面對與其原始訓練資料集顯著不同的新的、未見的和潛在對抗性的資料時的可適應性和復原力。在本文中，我們透過預訓練的大語言模型 (LLM) 調查時間序列 OOD 概化。我們首先提出一個新穎的**T**ri-level 學習架構，用於**T**ime **S**eries **O**OD 概化，稱為 TTSO，它同時考慮了樣本級和群組級的不確定性。此公式為制定和分析 OOD 概化問題提供了新的理論觀點。此外，我們提供理論分析來證明此方法的動機充分。然後，我們開發了一個分層定位演算法，專為此三層級最佳化問題量身打造，理論上證明了所提出演算法的保證收斂性。我們的分析還揭示了獲得 $\epsilon$-平穩點的迭代複雜度受 O($\frac{1}{\epsilon^{2}}$) 限制。已經在真實世界資料集上進行了廣泛的實驗，以闡明所提出方法的有效性。

##### **Pap2Pat: Towards Automated Paper-to-Patent Drafting using Chunk-based Outline-guided Generation**
2410.07009v1 by Valentin Knappich, Simon Razniewski, Anna Hätty, Annemarie Friedrich

The patent domain is gaining attention in natural language processing
research, offering practical applications in streamlining the patenting process
and providing challenging benchmarks for large language models (LLMs). However,
the generation of the description sections of patents, which constitute more
than 90% of the patent document, has not been studied to date. We address this
gap by introducing the task of outline-guided paper-to-patent generation, where
an academic paper provides the technical specification of the invention and an
outline conveys the desired patent structure. We present PAP2PAT, a new
challenging benchmark of 1.8k patent-paper pairs with document outlines,
collected using heuristics that reflect typical research lab practices. Our
experiments with current open-weight LLMs and outline-guided chunk-based
generation show that they can effectively use information from the paper but
struggle with repetitions, likely due to the inherent repetitiveness of patent
language. We release our data and code.

摘要：專利領域在自然語言處理研究中備受關注，在簡化專利流程和為大型語言模型 (LLM) 提供具有挑戰性的基準方面提供實用應用。然而，專利說明書部分的產生，其構成專利文件 90% 以上的內容，迄今為止尚未經過研究。我們透過引入綱要引導的論文轉專利產生任務來解決這個差距，其中學術論文提供發明的技術規範，而綱要傳達所需的專利結構。我們提出 PAP2PAT，一個新的具有挑戰性的基準，包含 1.8k 專利論文對，以及使用反映典型研究實驗室實務的啟發法收集的的文件綱要。我們使用目前的開放權重 LLM 和綱要引導的區塊式產生進行的實驗顯示，它們可以有效地使用論文中的資訊，但會遇到重複的問題，這可能是由於專利語言固有的重複性。我們釋出我們的資料和程式碼。

##### **CursorCore: Assist Programming through Aligning Anything**
2410.07002v1 by Hao Jiang, Qi Liu, Rui Li, Shengyu Ye, Shijin Wang

Large language models have been successfully applied to programming
assistance tasks, such as code completion, code insertion, and instructional
code editing. However, these applications remain insufficiently automated and
struggle to effectively integrate various types of information during the
programming process, including coding history, current code, and user
instructions. In this work, we propose a new conversational framework that
comprehensively integrates these information sources, collect data to train our
models and evaluate their performance. Firstly, to thoroughly evaluate how well
models align with different types of information and the quality of their
outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to
comprehensively assess the performance of models in programming assistance
tasks. Then, for data collection, we develop a data generation pipeline,
Programming-Instruct, which synthesizes training data from diverse sources,
such as GitHub and online judge platforms. This pipeline can automatically
generate various types of messages throughout the programming process. Finally,
using this pipeline, we generate 219K samples, fine-tune multiple models, and
develop the CursorCore series. We show that CursorCore outperforms other models
of comparable size. This framework unifies applications such as inline chat and
automated editing, contributes to the advancement of coding assistants. Code,
models and data are freely available at
https://github.com/TechxGenus/CursorCore.

摘要：大型語言模型已成功應用於程式設計輔助任務，例如程式碼完成、程式碼插入和教學程式碼編輯。然而，這些應用程式仍然不夠自動化，並且難以在程式設計過程中有效整合各種資訊，包括編碼記錄、目前程式碼和使用者指令。在這項工作中，我們提出了一個新的對話架構，全面整合這些資訊來源，收集資料來訓練我們的模型並評估它們的效能。首先，為了徹底評估模型與不同類型資訊的吻合程度以及它們輸出的品質，我們引進了一個新的基準，APEval（輔助程式設計評估），以全面評估模型在程式設計輔助任務中的效能。然後，對於資料收集，我們開發了一個資料產生管線，Programming-Instruct，它從 GitHub 和線上評審平台等不同來源合成訓練資料。這個管線可以在程式設計過程中自動產生各種類型的訊息。最後，使用這個管線，我們產生了 219K 個範例，微調多個模型，並開發了 CursorCore 系列。我們展示了 CursorCore 優於其他規模相當的模型。這個架構統一了內嵌聊天和自動編輯等應用程式，有助於編碼助理的進步。程式碼、模型和資料可以在 https://github.com/TechxGenus/CursorCore 免費取得。

##### **Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models**
2410.06981v1 by Michael Lan, Philip Torr, Austin Meek, Ashkan Khakzar, David Krueger, Fazl Barez

We investigate feature universality in large language models (LLMs), a
research field that aims to understand how different models similarly represent
concepts in the latent spaces of their intermediate layers. Demonstrating
feature universality allows discoveries about latent representations to
generalize across several models. However, comparing features across LLMs is
challenging due to polysemanticity, in which individual neurons often
correspond to multiple features rather than distinct ones. This makes it
difficult to disentangle and match features across different models. To address
this issue, we employ a method known as dictionary learning by using sparse
autoencoders (SAEs) to transform LLM activations into more interpretable spaces
spanned by neurons corresponding to individual features. After matching feature
neurons across models via activation correlation, we apply representational
space similarity metrics like Singular Value Canonical Correlation Analysis to
analyze these SAE features across different LLMs. Our experiments reveal
significant similarities in SAE feature spaces across various LLMs, providing
new evidence for feature universality.

摘要：我們調查大型語言模型 (LLM) 中的特徵通用性，這是一個研究領域，旨在了解不同的模型如何在其中間層的潛在空間中以相似的方式表示概念。證明特徵通用性允許跨多個模型概括潛在表示的發現。然而，由於多義性，比較 LLM 中的特徵具有挑戰性，其中個別神經元通常對應於多個特徵，而不是不同的特徵。這使得難以解開和匹配不同模型中的特徵。為了解決這個問題，我們採用一種稱為字典學習的方法，使用稀疏自動編碼器 (SAE) 將 LLM 激活轉換為由對應於個別特徵的神經元所跨越的更具可解釋性的空間。在通過激活相關性匹配模型中的特徵神經元後，我們應用表示空間相似性度量，例如奇異值正則相關分析，以分析不同 LLM 中的這些 SAE 特徵。我們的實驗揭示了跨不同 LLM 的 SAE 特徵空間中顯著的相似性，為特徵通用性提供了新的證據。

##### **Adaptive High-Frequency Transformer for Diverse Wildlife Re-Identification**
2410.06977v1 by Chenyue Li, Shuoyi Chen, Mang Ye

Wildlife ReID involves utilizing visual technology to identify specific
individuals of wild animals in different scenarios, holding significant
importance for wildlife conservation, ecological research, and environmental
monitoring. Existing wildlife ReID methods are predominantly tailored to
specific species, exhibiting limited applicability. Although some approaches
leverage extensively studied person ReID techniques, they struggle to address
the unique challenges posed by wildlife. Therefore, in this paper, we present a
unified, multi-species general framework for wildlife ReID. Given that
high-frequency information is a consistent representation of unique features in
various species, significantly aiding in identifying contours and details such
as fur textures, we propose the Adaptive High-Frequency Transformer model with
the goal of enhancing high-frequency information learning. To mitigate the
inevitable high-frequency interference in the wilderness environment, we
introduce an object-aware high-frequency selection strategy to adaptively
capture more valuable high-frequency components. Notably, we unify the
experimental settings of multiple wildlife datasets for ReID, achieving
superior performance over state-of-the-art ReID methods. In domain
generalization scenarios, our approach demonstrates robust generalization to
unknown species.

摘要：野生動物 ReID 涉及利用視覺技術來識別不同場景中野生動物的特定個體，這對野生動物保育、生態研究和環境監控具有重要意義。現有的野生動物 ReID 方法主要針對特定物種量身打造，適用性有限。儘管有些方法利用廣泛研究的人員 ReID 技術，但它們仍難以應對野生動物帶來的獨特挑戰。因此，在本文中，我們提出了一個統一的多物種通用框架，用於野生動物 ReID。鑑於高頻信息是一致表示各種物種中獨特特徵，顯著有助於識別輪廓和細節，例如皮毛紋理，我們提出了自適應高頻Transformer模型，目標是加強高頻信息學習。為了減輕野外環境中不可避免的高頻干擾，我們引入了一個對象感知高頻選擇策略，以自適應地捕捉更有價值的高頻組成。值得注意的是，我們統一了 ReID 的多個野生動物數據集的實驗設置，實現了優於最先進的 ReID 方法的卓越性能。在域泛化場景中，我們的做法證明了對未知物種的強大泛化能力。

##### **Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara**
2410.06973v1 by Azree Nazri, Olalekan Agbolade, Faisal Aziz

In contexts with limited computational and data resources, high-resource
language models often prove inadequate, particularly when addressing the
specific needs of Malay languages. This paper introduces a Personal
Intelligence System designed to efficiently integrate both on-device and
server-based models. The system incorporates SLiM-34M for on-device processing,
optimized for low memory and power usage, and MANYAK-1.3B for server-based
tasks, allowing for scalable, high-performance language processing. The models
achieve significant results across various tasks, such as machine translation,
question-answering, and translate IndoMMLU. Particularly noteworthy is
SLiM-34M's ability to achieve a high improvement in accuracy compared to other
LLMs while using 2 times fewer pre-training tokens. This work challenges the
prevailing assumption that large-scale computational resources are necessary to
build effective language models, contributing to the development of
resource-efficient models for the Malay language with the unique orchestration
between SLiM-34M and MANYAK-1.3B.

摘要：在計算和資料資源有限的環境下，高資源語言模型通常被證明不足，特別是在滿足馬來語的特定需求時。本文介紹了一個個人化智慧系統，旨在有效整合設備和伺服器端模型。該系統結合了針對設備處理而最佳化的 SLiM-34M（優化以降低記憶體和電源使用量）和針對伺服器端任務的 MANYAK-1.3B，允許可擴充、高性能的語言處理。這些模型在各種任務中都取得顯著成果，例如機器翻譯、問答和翻譯 IndoMMLU。特別值得注意的是，與其他 LLM 相比，SLiM-34M 的準確度有顯著提升，同時使用的預訓練代幣減少了 2 倍。這項工作挑戰了普遍的假設，即建立有效的語言模型需要大規模的計算資源，並有助於開發馬來語的資源有效模型，並在 SLiM-34M 和 MANYAK-1.3B 之間進行獨特的編排。

##### **DLGNet: Hyperedge Classification through Directed Line Graphs for Chemical Reactions**
2410.06969v1 by Stefano Fiorini, Giulia M. Bovolenta, Stefano Coniglio, Michele Ciavotta, Pietro Morerio, Michele Parrinello, Alessio Del Bue

Graphs and hypergraphs provide powerful abstractions for modeling
interactions among a set of entities of interest and have been attracting a
growing interest in the literature thanks to many successful applications in
several fields. In particular, they are rapidly expanding in domains such as
chemistry and biology, especially in the areas of drug discovery and molecule
generation. One of the areas witnessing the fasted growth is the chemical
reactions field, where chemical reactions can be naturally encoded as directed
hyperedges of a hypergraph. In this paper, we address the chemical reaction
classification problem by introducing the notation of a Directed Line Graph
(DGL) associated with a given directed hypergraph. On top of it, we build the
Directed Line Graph Network (DLGNet), the first spectral-based Graph Neural
Network (GNN) expressly designed to operate on a hypergraph via its DLG
transformation. The foundation of DLGNet is a novel Hermitian matrix, the
Directed Line Graph Laplacian, which compactly encodes the directionality of
the interactions taking place within the directed hyperedges of the hypergraph
thanks to the DLG representation. The Directed Line Graph Laplacian enjoys many
desirable properties, including admitting an eigenvalue decomposition and being
positive semidefinite, which make it well-suited for its adoption within a
spectral-based GNN. Through extensive experiments on chemical reaction
datasets, we show that DGLNet significantly outperforms the existing
approaches, achieving on a collection of real-world datasets an average
relative-percentage-difference improvement of 33.01%, with a maximum
improvement of 37.71%.

摘要：圖形和超圖形提供強大的抽象方法，用於對一組感興趣實體之間的交互進行建模，並且由於在多個領域的許多成功應用而備受文獻關注。特別是，它們在化學和生物學等領域迅速擴展，特別是在藥物發現和分子生成領域。見證增長最快的領域之一是化學反應領域，其中化學反應可以自然地編碼為超圖形的定向超邊。在本文中，我們通過引入與給定定向超圖形相關的定向線圖 (DGL) 的符號來解決化學反應分類問題。最重要的是，我們構建了定向線圖網路 (DLGNet)，這是一個第一個基於譜的圖神經網路 (GNN)，專門設計用於通過其 DLG 轉換在超圖形上運作。DLGNet 的基礎是一個新穎的 Hermitian 矩陣，即定向線圖拉普拉斯算子，它由於 DLG 表示而緊湊地編碼了在超圖形的定向超邊內發生的交互的方向性。定向線圖拉普拉斯算子具有許多理想的性質，包括承認特徵值分解和正半定的性質，這使其非常適合在基於譜的 GNN 中採用。通過在化學反應資料集上進行大量實驗，我們表明 DLGNet 明顯優於現有方法，在實際世界資料集的集合上實現了 33.01% 的平均相對百分比差異改進，最大改進為 37.71%。

##### **Uncovering Factor Level Preferences to Improve Human-Model Alignment**
2410.06965v1 by Juhyun Oh, Eunsu Kim, Jiseon Kim, Wenda Xu, Inha Cha, William Yang Wang, Alice Oh

Despite advancements in Large Language Model (LLM) alignment, understanding
the reasons behind LLM preferences remains crucial for bridging the gap between
desired and actual behavior. LLMs often exhibit biases or tendencies that
diverge from human preferences, such as favoring certain writing styles or
producing overly verbose outputs. However, current methods for evaluating
preference alignment often lack explainability, relying on coarse-grained
comparisons. To address this, we introduce PROFILE (PRObing Factors of
InfLuence for Explainability), a novel framework that uncovers and quantifies
the influence of specific factors driving preferences. PROFILE's factor level
analysis explains the 'why' behind human-model alignment and misalignment,
offering insights into the direction of model improvement. We apply PROFILE to
analyze human and LLM preferences across three tasks: summarization, helpful
response generation, and document-based question-answering. Our factor level
analysis reveals a substantial discrepancy between human and LLM preferences in
generation tasks, whereas LLMs show strong alignment with human preferences in
evaluation tasks. We demonstrate how leveraging factor level insights,
including addressing misaligned factors or exploiting the generation-evaluation
gap, can improve alignment with human preferences. This work underscores the
importance of explainable preference analysis and highlights PROFILE's
potential to provide valuable training signals, driving further improvements in
human-model alignment.

摘要：儘管大型語言模型（LLM）對齊取得進展，但了解 LLM 偏好的原因對於縮小預期行為與實際行為之間的差距仍然至關重要。LLM 經常表現出與人類偏好不同的偏差或傾向，例如偏好某些寫作風格或產生過於冗長的輸出。然而，目前評估偏好對齊的方法通常缺乏可解釋性，依賴於粗略的比較。為了解決這個問題，我們引入了 PROFILE（可解釋性的影響因素探測），一個揭示和量化推動偏好的特定因素影響的新框架。PROFILE 的因素層級分析解釋了人機對齊和未對齊背後的「原因」，提供了對模型改進方向的見解。我們將 PROFILE 應用於分析人類和 LLM 在三個任務中的偏好：摘要、有用的回應產生和基於文件的問答。我們的因素層級分析揭示了人類和 LLM 在產生任務中的偏好存在顯著差異，而 LLM 在評估任務中顯示出與人類偏好強烈的對齊。我們展示了如何利用因素層級見解，包括解決未對齊的因素或利用產生評估差距，可以改善與人類偏好的對齊。這項工作強調了可解釋偏好分析的重要性，並突出了 PROFILE 提供有價值訓練訊號的潛力，推動人機對齊進一步改善。

##### **ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling**
2410.06963v1 by Deok-Kyeong Jang, Dongseok Yang, Deok-Yun Jang, Byeoli Choi, Donghoon Shin, Sung-hee Lee

This paper introduces ELMO, a real-time upsampling motion capture framework
designed for a single LiDAR sensor. Modeled as a conditional autoregressive
transformer-based upsampling motion generator, ELMO achieves 60 fps motion
capture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is
the coupling of the self-attention mechanism with thoughtfully designed
embedding modules for motion and point clouds, significantly elevating the
motion quality. To facilitate accurate motion capture, we develop a one-time
skeleton calibration model capable of predicting user skeleton offsets from a
single-frame point cloud. Additionally, we introduce a novel data augmentation
technique utilizing a LiDAR simulator, which enhances global root tracking to
improve environmental understanding. To demonstrate the effectiveness of our
method, we compare ELMO with state-of-the-art methods in both image-based and
point cloud-based motion capture. We further conduct an ablation study to
validate our design principles. ELMO's fast inference time makes it well-suited
for real-time applications, exemplified in our demo video featuring live
streaming and interactive gaming scenarios. Furthermore, we contribute a
high-quality LiDAR-mocap synchronized dataset comprising 20 different subjects
performing a range of motions, which can serve as a valuable resource for
future research. The dataset and evaluation code are available at {\blue
\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}

摘要：<paragraph>本文介紹 ELMO，一個專為單一 LiDAR 感測器設計的即時上採樣動作捕捉架構。ELMO 以條件自迴歸轉換器為基礎的上採樣動作生成器建模，從 20 fps LiDAR 點雲序列中實現 60 fps 動作捕捉。ELMO 的關鍵特點在於將自注意力機制與精心設計的動作和點雲嵌入模組結合，大幅提升動作品質。為了促進準確的動作捕捉，我們開發了一個一次性的骨架校正模型，能夠從單一幀點雲預測使用者骨架偏移。此外，我們還引入了一種利用 LiDAR 模擬器的新穎資料擴充技術，這項技術增強了整體根部追蹤，以改善環境理解。為了證明我們方法的有效性，我們在基於影像和基於點雲的動作捕捉中，將 ELMO 與最先進的方法進行比較。我們進一步進行消融研究，以驗證我們的設計原則。ELMO 的快速推論時間使其非常適合於即時應用，這在我們展示現場串流和互動遊戲場景的示範影片中得到證明。此外，我們貢獻了一個高品質的 LiDAR-mocap 同步資料集，其中包含 20 個不同的受試者執行一系列動作，這可以作為未來研究的寶貴資源。資料集和評估程式碼可在 {\blue \url{https://movin3d.github.io/ELMO_SIGASIA2024/}} 取得</paragraph>

##### **Self-Boosting Large Language Models with Synthetic Preference Data**
2410.06961v1 by Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, Furu Wei

Through alignment with human preferences, Large Language Models (LLMs) have
advanced significantly in generating honest, harmless, and helpful responses.
However, collecting high-quality preference data is a resource-intensive and
creativity-demanding process, especially for the continual improvement of LLMs.
We introduce SynPO, a self-boosting paradigm that leverages synthetic
preference data for model alignment. SynPO employs an iterative mechanism
wherein a self-prompt generator creates diverse prompts, and a response
improver refines model responses progressively. This approach trains LLMs to
autonomously learn the generative rewards for their own outputs and eliminates
the need for large-scale annotation of prompts and human preferences. After
four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements
in instruction-following abilities, achieving over 22.1% win rate improvements
on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general
performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score
increase on the well-recognized Open LLM leaderboard.

摘要：透過與人類偏好對齊，大型語言模型 (LLM) 在產生誠實、無害且有用的回應方面已取得顯著進展。然而，收集高品質的偏好資料是一個耗費資源且需要創造力的過程，特別是對於 LLM 的持續改進。我們介紹 SynPO，一種自我提升的範例，它利用合成偏好資料進行模型對齊。SynPO 採用意義機制，其中自我提示產生器會產生不同的提示，而回應改進器會逐步改善模型回應。這種方法訓練 LLM 自主學習其輸出產生的獎勵，並消除了對提示和大規模標記人類偏好的需求。在四次 SynPO 迭代後，Llama3-8B 和 Mistral-7B 在遵循指令的能力上表現出顯著的提升，在 AlpacaEval 2.0 和 ArenaHard 上的獲勝率提升超過 22.1%。同時，SynPO 提升了 LLM 在各種任務上的整體表現，這由公認的 Open LLM 排行榜上平均分數提升 3.2 到 5.0 得到驗證。

##### **Support Vector Boosting Machine (SVBM): Enhancing Classification Performance with AdaBoost and Residual Connections**
2410.06957v1 by Junbo Jacob Lian

In traditional boosting algorithms, the focus on misclassified training
samples emphasizes their importance based on difficulty during the learning
process. While using a standard Support Vector Machine (SVM) as a weak learner
in an AdaBoost framework can enhance model performance by concentrating on
error samples, this approach introduces significant challenges. Specifically,
SVMs, characterized by their stability and robustness, may require
destabilization to fit the boosting paradigm, which in turn can constrain
performance due to reliance on the weighted results from preceding iterations.
To address these challenges, we propose the Support Vector Boosting Machine
(SVBM), which integrates a novel subsampling process with SVM algorithms and
residual connection techniques. This method updates sample weights by
considering both the current model's predictions and the outputs from prior
rounds, allowing for effective sparsity control. The SVBM framework enhances
the ability to form complex decision boundaries, thereby improving
classification performance. The MATLAB source code for SVBM can be accessed at
https://github.com/junbolian/SVBM.

摘要：在傳統的提升演算法中，著重於錯誤分類的訓練樣本，強調它們在學習過程中基於難度的重要性。雖然在 AdaBoost 架構中使用標準支援向量機 (SVM) 作為弱學習器，可以透過專注於錯誤樣本來增強模型效能，但這種方法會帶來重大的挑戰。具體來說，SVM 以其穩定性和穩健性為特徵，可能需要不穩定化才能符合提升範例，而這反過來又會由於依賴於前一次反覆運算的加權結果而限制效能。為了應對這些挑戰，我們提出了支援向量提升機 (SVBM)，它將創新的子抽樣流程與 SVM 演算法和殘差連接技術整合在一起。此方法透過考量目前模型的預測和前一輪的輸出，來更新樣本權重，從而允許有效的稀疏性控制。SVBM 架構增強了形成複雜決策邊界的能​​力，從而改善了分類效能。SVBM 的 MATLAB 原始碼可以在 https://github.com/junbolian/SVBM 取得。

##### **Faithful Interpretation for Graph Neural Networks**
2410.06950v1 by Lijie Hu, Tianhao Huang, Lu Yu, Wanyu Lin, Tianhang Zheng, Di Wang

Currently, attention mechanisms have garnered increasing attention in Graph
Neural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph
Transformers (GTs). It is not only due to the commendable boost in performance
they offer but also its capacity to provide a more lucid rationale for model
behaviors, which are often viewed as inscrutable. However, Attention-based GNNs
have demonstrated instability in interpretability when subjected to various
sources of perturbations during both training and testing phases, including
factors like additional edges or nodes. In this paper, we propose a solution to
this problem by introducing a novel notion called Faithful Graph
Attention-based Interpretation (FGAI). In particular, FGAI has four crucial
properties regarding stability and sensitivity to interpretation and final
output distribution. Built upon this notion, we propose an efficient
methodology for obtaining FGAI, which can be viewed as an ad hoc modification
to the canonical Attention-based GNNs. To validate our proposed solution, we
introduce two novel metrics tailored for graph interpretation assessment.
Experimental results demonstrate that FGAI exhibits superior stability and
preserves the interpretability of attention under various forms of
perturbations and randomness, which makes FGAI a more faithful and reliable
explanation tool.

摘要：目前，注意力机制在图神经网络 (GNN) 中引起了越来越多的关注，例如图注意力网络 (GAT) 和图 Transformer (GT)。这不仅是因为它们提供的可喜的性能提升，还因为它们能够为模型行为提供更清晰的理由，而这些行为通常被视为深不可测的。然而，基于注意力的 GNN 在训练和测试阶段受到各种扰动源（包括附加边或节点等因素）时，其可解释性表现出不稳定性。在本文中，我们通过引入一个称为忠实图注意力解释 (FGAI) 的新概念来提出解决此问题的方案。具体来说，FGAI 具有四个关于稳定性和对解释和最终输出分布的敏感性的关键属性。在此概念的基础上，我们提出了一种获取 FGAI 的有效方法，该方法可以看作是对规范的基于注意力的 GNN 的特设修改。为了验证我们提出的解决方案，我们引入了两个针对图解释评估量身定制的新指标。实验结果表明，FGAI 表现出卓越的稳定性，并在各种形式的扰动和随机性下保持了注意力的可解释性，这使得 FGAI 成为一种更忠实、更可靠的解释工具。

##### **Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach**
2410.06949v1 by Xuanming Zhang, Yuxuan Chen, Yuan Yuan, Minlie Huang

In real world software development, improper or missing exception handling
can severely impact the robustness and reliability of code. Exception handling
mechanisms require developers to detect, capture, and manage exceptions
according to high standards, but many developers struggle with these tasks,
leading to fragile code. This problem is particularly evident in open source
projects and impacts the overall quality of the software ecosystem. To address
this challenge, we explore the use of large language models (LLMs) to improve
exception handling in code. Through extensive analysis, we identify three key
issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception
Types, and Distorted Handling Solutions. These problems are widespread across
real world repositories, suggesting that robust exception handling practices
are often overlooked or mishandled. In response, we propose Seeker, a multi
agent framework inspired by expert developer strategies for exception handling.
Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist
LLMs in detecting, capturing, and resolving exceptions more effectively. Our
work is the first systematic study on leveraging LLMs to enhance exception
handling practices, providing valuable insights for future improvements in code
reliability.

摘要：在現實世界的軟體開發中，不當或遺失的例外處理
可能會嚴重影響程式碼的健全性和可靠性。例外處理
機制要求開發人員根據高標準偵測、擷取和管理例外，但許多開發人員在這些任務上苦苦掙扎，
導致程式碼脆弱。這個問題在開源專案中特別明顯，並影響軟體生態系統的整體品質。為了應對
這個挑戰，我們探討使用大型語言模型 (LLM) 來改善程式碼中的例外處理。透過廣泛的分析，我們找出三個關鍵
問題：脆弱程式碼的不敏感偵測、例外類型的錯誤擷取，以及扭曲的處理解決方案。這些問題在
現實世界的儲存庫中很普遍，這表示健全的例外處理實務通常被忽略或處理不當。為了解決這個問題，我們提出 Seeker，一個多
代理架構，靈感來自專家開發人員的例外處理策略。Seeker 使用代理：掃描器、偵測器、掠奪者、排名器和處理器來協助
LLM 更有效地偵測、擷取和解決例外。我們的研究是第一個系統性研究，利用 LLM 來增強例外
處理實務，為程式碼可靠性的未來改善提供有價值的見解。

##### **CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages**
2410.06944v1 by Pretam Ray, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal

Neural dependency parsing has achieved remarkable performance for low
resource morphologically rich languages. It has also been well-studied that
morphologically rich languages exhibit relatively free word order. This prompts
a fundamental investigation: Is there a way to enhance dependency parsing
performance, making the model robust to word order variations utilizing the
relatively free word order nature of morphologically rich languages? In this
work, we examine the robustness of graph-based parsing architectures on 7
relatively free word order languages. We focus on scrutinizing essential
modifications such as data augmentation and the removal of position encoding
required to adapt these architectures accordingly. To this end, we propose a
contrastive self-supervised learning method to make the model robust to word
order variations. Furthermore, our proposed modification demonstrates a
substantial average gain of 3.03/2.95 points in 7 relatively free word order
languages, as measured by the UAS/LAS Score metric when compared to the best
performing baseline.

摘要：神經依賴解析對於資源較少的形態豐富語言已達到顯著的效能。形態豐富語言展現相對自由的語序，這點也已獲得深入探討。這引發了一項基礎調查：是否有方法可以提升依賴解析效能，讓模型能透過運用形態豐富語言相對自由的語序特質，對語序變化具有穩健性？在這項工作中，我們檢視了 7 種相對自由語序語言中基於圖表的解析架構的穩健性。我們專注於審視必要的修改，例如資料擴充和移除位置編碼，以適當地調整這些架構。為此，我們提出對比自我監督學習方法，讓模型對語序變化具有穩健性。此外，我們提出的修改在 7 種相對自由語序語言中展現了 3.03/2.95 點的顯著平均增益，這是根據 UAS/LAS 分數指標，與效能最佳的基準線進行比較後得出的結果。

##### **AutoFeedback: An LLM-based Framework for Efficient and Accurate API Request Generation**
2410.06943v1 by Huanxi Liu, Jiaqi Liao, Dawei Feng, Kele Xu, Huaimin Wang

Large Language Models (LLMs) leverage external tools primarily through
generating the API request to enhance task completion efficiency. The accuracy
of API request generation significantly determines the capability of LLMs to
accomplish tasks.
  Due to the inherent hallucinations within the LLM, it is difficult to
efficiently and accurately generate the correct API request.
  Current research uses prompt-based feedback to facilitate the LLM-based API
request generation. However, existing methods lack factual information and are
insufficiently detailed.
  To address these issues, we propose AutoFeedback, an LLM-based framework for
efficient and accurate API request generation, with a Static Scanning Component
(SSC) and a Dynamic Analysis Component (DAC). SSC incorporates errors detected
in the API requests as pseudo-facts into the feedback, enriching the factual
information. DAC retrieves information from API documentation, enhancing the
level of detail in feedback.
  Based on this two components, Autofeedback implementes two feedback loops
during the process of generating API requests by the LLM.
  Extensive experiments demonstrate that it significantly improves accuracy of
API request generation and reduces the interaction cost. AutoFeedback achieves
an accuracy of 100.00\% on a real-world API dataset and reduces the cost of
interaction with GPT-3.5 Turbo by 23.44\%, and GPT-4 Turbo by 11.85\%.

摘要：大型語言模型 (LLM) 主要透過產生 API 要求來提升任務完成效率，以利用外部工具。API 要求產生的準確性顯著決定 LLM 完成任務的能力。
由於 LLM 內部固有的幻覺，很難有效且準確地產生正確的 API 請求。
目前的研究使用基於提示的回饋來促進基於 LLM 的 API 請求產生。然而，現有方法缺乏事實資訊且不夠詳細。
為了解決這些問題，我們提出了 AutoFeedback，一個基於 LLM 的架構，用於有效且準確地產生 API 請求，並具備靜態掃描元件 (SSC) 和動態分析元件 (DAC)。SSC 將 API 請求中偵測到的錯誤作為偽事實納入回饋中，豐富了事實資訊。DAC 從 API 文件中擷取資訊，提升回饋中的詳細程度。
基於這兩個元件，Autofeedback 在 LLM 產生 API 請求的過程中實作了兩個回饋迴路。
廣泛的實驗證明，它顯著提升了 API 請求產生的準確性並降低了互動成本。AutoFeedback 在真實世界的 API 資料集上達到了 100.00% 的準確度，並將與 GPT-3.5 Turbo 的互動成本降低了 23.44%，與 GPT-4 Turbo 的互動成本降低了 11.85%。

##### **Reproducing and Extending Experiments in Behavioral Strategy with Large Language Models**
2410.06932v1 by Daniel Albert, Stephan Billinger

In this study, we propose LLM agents as a novel approach in behavioral
strategy research, complementing simulations and laboratory experiments to
advance our understanding of cognitive processes in decision-making.
Specifically, we reproduce a human laboratory experiment in behavioral strategy
using large language model (LLM) generated agents and investigate how LLM
agents compare to observed human behavior. Our results show that LLM agents
effectively reproduce search behavior and decision-making comparable to humans.
Extending our experiment, we analyze LLM agents' simulated "thoughts,"
discovering that more forward-looking thoughts correlate with favoring
exploitation over exploration to maximize wealth. We show how this new approach
can be leveraged in behavioral strategy research and address limitations.

摘要：本研究中，我們提出 LLM 代理作為行為策略研究的新方法，以補充模擬和實驗室實驗，以增進我們對決策過程中認知過程的了解。具體來說，我們使用大型語言模型 (LLM) 生成的代理重新製作行為策略的人類實驗室實驗，並探討 LLM 代理與觀察到的行為有何不同。我們的結果顯示，LLM 代理有效地重現了與人類相當的搜尋行為和決策。擴展我們的實驗，我們分析了 LLM 代理模擬的「思考」，發現更具前瞻性的思考與偏好利用而非探索以最大化財富有關。我們展示了這種新方法如何運用於行為策略研究並解決限制。

##### **SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration**
2410.06916v1 by Heming Xia, Yongqi Li, Jun Zhang, Cunxiao Du, Wenjie Li

Speculative decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by first employing a compact model to draft multiple tokens
efficiently and then using the target LLM to verify them in parallel. While
this technique has achieved notable speedups, most existing approaches
necessitate either additional parameters or extensive training to construct
effective draft models, thereby restricting their applicability across
different LLMs and tasks. To address this limitation, we explore a novel
plug-and-play SD solution with layer-skipping, which skips intermediate layers
of the target LLM as the compact draft model. Our analysis reveals that LLMs
exhibit great potential for self-acceleration through layer sparsity and the
task-specific nature of this sparsity. Building on these insights, we introduce
SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively
selects intermediate layers of LLMs to skip during inference. SWIFT does not
require auxiliary models or additional training, making it a plug-and-play
solution for accelerating LLM inference across diverse input data streams. Our
extensive experiments across a wide range of models and downstream tasks
demonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving
the original distribution of the generated text.

摘要：推測性解碼 (SD) 已成為廣泛使用的範例，用於加速大型語言模型 (LLM) 的推論，同時不損害生成品質。它的運作方式是先採用精簡模型有效率地起草多個符號，然後使用目標 LLM 平行驗證它們。雖然此技術已達到顯著的加速，但現有的方法大多需要額外的參數或廣泛的訓練，才能建構有效的起草模型，因此限制了它們在不同 LLM 和任務中的適用性。為了解決此限制，我們探索一種創新的即插即用 SD 解决方案，具有跳層功能，它會略過目標 LLM 的中間層，作為精簡的起草模型。我們的分析顯示，LLM 通過層稀疏性以及此稀疏性的任務特定性質，展現出極大的自我加速潛力。根據這些見解，我們引入了 SWIFT，這是一種即時自我推測解碼演算法，可自適應地選取 LLM 的中間層，在推論期間略過。SWIFT 不需要輔助模型或額外訓練，使其成為即插即用的解決方案，用於加速 LLM 推論，以涵蓋不同的輸入資料串流。我們在各種模型和下游任務中進行廣泛的實驗，證明 SWIFT 可以達到 1.3 倍至 1.6 倍的加速，同時保留生成的文字的原始分佈。

##### **Utilize the Flow before Stepping into the Same River Twice: Certainty Represented Knowledge Flow for Refusal-Aware Instruction Tuning**
2410.06913v1 by Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, Conghui He

Refusal-Aware Instruction Tuning (RAIT) enables Large Language Models (LLMs)
to refuse to answer unknown questions. By modifying responses of unknown
questions in the training data to refusal responses such as "I don't know",
RAIT enhances the reliability of LLMs and reduces their hallucination.
Generally, RAIT modifies training samples based on the correctness of the
initial LLM's response. However, this crude approach can cause LLMs to
excessively refuse answering questions they could have correctly answered, the
problem we call over-refusal. In this paper, we explore two primary causes of
over-refusal: Static conflict emerges when the RAIT data is constructed solely
on correctness criteria, causing similar samples in the LLM's feature space to
be assigned different labels (original vs. modified "I don't know"). Dynamic
conflict occurs due to the changes of LLM's knowledge state during fine-tuning,
which transforms previous unknown questions into knowns, while the training
data, which is constructed based on the initial LLM, remains unchanged. These
conflicts cause the trained LLM to misclassify known questions as unknown,
resulting in over-refusal. To address this issue, we introduce Certainty
Represented Knowledge Flow for Refusal-Aware Instructions Construction (CRaFT).
CRaFT centers on two main contributions: First, we additionally incorporate
response certainty to selectively filter and modify data, reducing static
conflicts. Second, we implement preliminary rehearsal training to characterize
changes in the LLM's knowledge state, which helps mitigate dynamic conflicts
during the fine-tuning process. We conducted extensive experiments on
open-ended question answering and multiple-choice question task. Experiment
results show that CRaFT can improve LLM's overall performance during the RAIT
process. Source code and training data will be released at Github.

摘要：拒絕感知指令調校 (RAIT) 讓大型語言模型 (LLM) 能拒絕回答未知問題。透過修改訓練資料中未知問題的回應為拒絕回應，例如「我不知道」，RAIT 提升了 LLM 的可靠性並減少了它們的幻覺。一般來說，RAIT 會根據 LLM 初始回應的正確性修改訓練樣本。然而，這種粗糙的方法會導致 LLM 過度拒絕回答它們原本可以正確回答的問題，我們稱之為過度拒絕的問題。在本文中，我們探討了過度拒絕的兩個主要原因：靜態衝突出現在 RAIT 資料僅根據正確性標準建構時，導致 LLM 特徵空間中類似的樣本被指定為不同的標籤（原始與修改後的「我不知道」）。動態衝突則因為微調期間 LLM 知識狀態的改變而發生，這會將先前的未知問題轉變為已知，而根據初始 LLM 建構的訓練資料則保持不變。這些衝突導致訓練後的 LLM 將已知問題誤分類為未知，進而導致過度拒絕。為了解決這個問題，我們引入了確定性表示知識流，用於拒絕感知指令建構 (CRaFT)。CRaFT 專注於兩個主要貢獻：首先，我們額外納入回應確定性來選擇性地過濾和修改資料，減少靜態衝突。其次，我們實作了初步排練訓練，以描述 LLM 知識狀態的改變，這有助於在微調過程中減輕動態衝突。我們在開放式問題解答和多重選擇題任務上進行了廣泛的實驗。實驗結果顯示，CRaFT 可以改善 LLM 在 RAIT 過程中的整體表現。原始碼和訓練資料將在 Github 上發布。

##### **Compositional Entailment Learning for Hyperbolic Vision-Language Models**
2410.06912v1 by Avik Pal, Max van Spengler, Guido Maria D'Amely di Melendugno, Alessandro Flaborea, Fabio Galasso, Pascal Mettes

Image-text representation learning forms a cornerstone in vision-language
models, where pairs of images and textual descriptions are contrastively
aligned in a shared embedding space. Since visual and textual concepts are
naturally hierarchical, recent work has shown that hyperbolic space can serve
as a high-potential manifold to learn vision-language representation with
strong downstream performance. In this work, for the first time we show how to
fully leverage the innate hierarchical nature of hyperbolic embeddings by
looking beyond individual image-text pairs. We propose Compositional Entailment
Learning for hyperbolic vision-language models. The idea is that an image is
not only described by a sentence but is itself a composition of multiple object
boxes, each with their own textual description. Such information can be
obtained freely by extracting nouns from sentences and using openly available
localized grounding models. We show how to hierarchically organize images,
image boxes, and their textual descriptions through contrastive and
entailment-based objectives. Empirical evaluation on a hyperbolic
vision-language model trained with millions of image-text pairs shows that the
proposed compositional learning approach outperforms conventional Euclidean
CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot
and retrieval generalization and clearly stronger hierarchical performance.

摘要：图像文本表征学习是视觉语言模型中的基石，其中图像和文本描述对在共享嵌入空间中对比对齐。由于视觉和文本概念天然具有层次结构，最近的研究表明，双曲空间可以作为学习视觉语言表征的高潜力流形，具有强大的下游性能。在这项工作中，我们首次展示了如何充分利用双曲嵌入的固有层次结构，方法是超越单个图像文本对。我们提出了双曲视觉语言模型的组合蕴涵学习。这个想法是，图像不仅由句子描述，而且本身是由多个对象框组成的，每个对象框都有自己的文本描述。可以通过从句子中提取名词并使用公开可用的局部接地模型免费获得此类信息。我们展示了如何通过对比和基于蕴涵的目标分层组织图像、图像框及其文本描述。对使用数百万图像文本对训练的双曲视觉语言模型进行的经验评估表明，所提出的组合学习方法优于传统的欧几里得 CLIP 学习，以及最近的双曲替代方案，具有更好的零样本和检索泛化能力，以及明显更强的层次性能。

##### **Generative Model for Less-Resourced Language with 1 billion parameters**
2410.06898v1 by Domen Vreš, Martin Božič, Aljaž Potočnik, Tomaž Martinčič, Marko Robnik-Šikonja

Large language models (LLMs) are a basic infrastructure for modern natural
language processing. Many commercial and open-source LLMs exist for English,
e.g., ChatGPT, Llama, Falcon, and Mistral. As these models are trained on
mostly English texts, their fluency and knowledge of low-resource languages and
societies are superficial. We present the development of large generative
language models for a less-resourced language. GaMS 1B - Generative Model for
Slovene with 1 billion parameters was created by continuing pretraining of the
existing English OPT model. We developed a new tokenizer adapted to Slovene,
Croatian, and English languages and used embedding initialization methods FOCUS
and WECHSEL to transfer the embeddings from the English OPT model. We evaluate
our models on several classification datasets from the Slovene suite of
benchmarks and generative sentence simplification task SENTA. We only used a
few-shot in-context learning of our models, which are not yet
instruction-tuned. For classification tasks, in this mode, the generative
models lag behind the existing Slovene BERT-type models fine-tuned for specific
tasks. On a sentence simplification task, the GaMS models achieve comparable or
better performance than the GPT-3.5-Turbo model.

摘要：大型語言模型 (LLM) 是現代自然語言處理的基本基礎設施。有許多商業和開放原始碼的 LLM 適用於英語，例如 ChatGPT、Llama、Falcon 和 Mistral。由於這些模型大多訓練於英語文本，因此它們對低資源語言和社會的流暢度和知識都很膚淺。我們展示了針對資源較少的語言開發大型生成語言模型。GaMS 1B - 10 億個參數的斯洛維尼亞生成模型是透過持續預訓練現有的英語 OPT 模型而建立的。我們開發了一個適用於斯洛維尼亞語、克羅埃西亞語和英語的新分詞器，並使用嵌入初始化方法 FOCUS 和 WECHSEL 從英語 OPT 模型傳輸嵌入。我們在斯洛維尼亞基準套件中的幾個分類資料集和生成式句子簡化任務 SENTA 上評估我們的模型。我們僅使用我們模型的少量情境學習，這些模型尚未針對指令進行調整。對於分類任務，在此模式中，生成式模型落後於針對特定任務進行微調的現有斯洛維尼亞 BERT 類型模型。在句子簡化任務中，GaMS 模型達到了與 GPT-3.5-Turbo 模型相當或更好的效能。

##### **FltLM: An Intergrated Long-Context Large Language Model for Effective Context Filtering and Understanding**
2410.06886v1 by Jingyang Deng, Zhengyang Shen, Boyang Wang, Lixin Su, Suqi Cheng, Ying Nie, Junfeng Wang, Dawei Yin, Jinwen Ma

The development of Long-Context Large Language Models (LLMs) has markedly
advanced natural language processing by facilitating the process of textual
data across long documents and multiple corpora. However, Long-Context LLMs
still face two critical challenges: The lost in the middle phenomenon, where
crucial middle-context information is likely to be missed, and the distraction
issue that the models lose focus due to overly extended contexts. To address
these challenges, we propose the Context Filtering Language Model (FltLM), a
novel integrated Long-Context LLM which enhances the ability of the model on
multi-document question-answering (QA) tasks. Specifically, FltLM innovatively
incorporates a context filter with a soft mask mechanism, identifying and
dynamically excluding irrelevant content to concentrate on pertinent
information for better comprehension and reasoning. Our approach not only
mitigates these two challenges, but also enables the model to operate
conveniently in a single forward pass. Experimental results demonstrate that
FltLM significantly outperforms supervised fine-tuning and retrieval-based
methods in complex QA scenarios, suggesting a promising solution for more
accurate and reliable long-context natural language understanding applications.

摘要：長語境大型語言模型 (LLM) 的發展顯著地促進了自然語言處理，促進了跨長文檔和多個語料庫的文本數據處理。然而，長語境 LLM 仍面臨兩項關鍵挑戰：遺失中間現象，其中關鍵的中間語境資訊很可能會被遺漏，以及分心問題，即模型因過度延伸的語境而失去焦點。為了應對這些挑戰，我們提出了語境過濾語言模型 (FltLM)，這是一種新穎的整合式長語境 LLM，它增強了模型在多文件問答 (QA) 任務上的能力。具體來說，FltLM 創新地結合了一個帶有軟遮罩機制的語境過濾器，識別並動態排除不相關內容，以專注於相關資訊，以便更好地理解和推理。我們的做法不僅緩解了這兩項挑戰，而且使模型能夠在單次前向傳遞中方便地運作。實驗結果表明，FltLM 在複雜的問答場景中明顯優於監督微調和基於檢索的方法，為更準確和可靠的長語境自然語言理解應用程式提出了有希望的解決方案。

##### **Understanding Model Ensemble in Transferable Adversarial Attack**
2410.06851v1 by Wei Yao, Zeliang Zhang, Huayi Tang, Yong Liu

Model ensemble adversarial attack has become a powerful method for generating
transferable adversarial examples that can target even unknown models, but its
theoretical foundation remains underexplored. To address this gap, we provide
early theoretical insights that serve as a roadmap for advancing model ensemble
adversarial attack. We first define transferability error to measure the error
in adversarial transferability, alongside concepts of diversity and empirical
model ensemble Rademacher complexity. We then decompose the transferability
error into vulnerability, diversity, and a constant, which rigidly explains the
origin of transferability error in model ensemble attack: the vulnerability of
an adversarial example to ensemble components, and the diversity of ensemble
components. Furthermore, we apply the latest mathematical tools in information
theory to bound the transferability error using complexity and generalization
terms, contributing to three practical guidelines for reducing transferability
error: (1) incorporating more surrogate models, (2) increasing their diversity,
and (3) reducing their complexity in cases of overfitting. Finally, extensive
experiments with 54 models validate our theoretical framework, representing a
significant step forward in understanding transferable model ensemble
adversarial attacks.

摘要：模型集成对抗攻擊已成為生成可轉移對抗範例的強大方法，甚至可以針對未知模型，但其理論基礎仍未得到充分探討。為了解決這個差距，我們提供了早期的理論見解，作為推進模型集成對抗攻擊的路線圖。我們首先定義可轉移性誤差來衡量對抗可轉移性中的誤差，以及多樣性和經驗模型集成 Rademacher 複雜性的概念。然後，我們將可轉移性誤差分解為漏洞、多樣性和常數，這嚴格地解釋了模型集成攻擊中可轉移性誤差的來源：對抗範例對集成組件的漏洞以及集成組件的多樣性。此外，我們應用信息論中最新的數學工具，使用複雜性和泛化術語來限制可轉移性誤差，為減少可轉移性誤差提供了三條實用準則：(1) 納入更多代理模型，(2) 增加它們的多樣性，以及 (3) 在過度擬合的情況下降低它們的複雜性。最後，對 54 個模型進行的廣泛實驗驗證了我們的理論框架，這代表了在理解可轉移模型集成對抗攻擊方面向前邁出的重要一步。

##### **A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement Learning and Application in UAV Hovering**
2410.06847v1 by Qihan Qi, Xinsong Yang, Gang Xia, Daniel W. C. Ho, Pengyang Tang

This paper proposes a safety modulator actor-critic (SMAC) method to address
safety constraint and overestimation mitigation in model-free safe
reinforcement learning (RL). A safety modulator is developed to satisfy safety
constraints by modulating actions, allowing the policy to ignore safety
constraint and focus on maximizing reward. Additionally, a distributional
critic with a theoretical update rule for SMAC is proposed to mitigate the
overestimation of Q-values with safety constraints. Both simulation and
real-world scenarios experiments on Unmanned Aerial Vehicles (UAVs) hovering
confirm that the SMAC can effectively maintain safety constraints and
outperform mainstream baseline algorithms.

摘要：這篇論文提出了一種安全調製器-評論家 (SMAC) 方法來解決無模型安全強化學習 (RL) 中的安全約束和過度估計的緩解。開發了一個安全調製器，通過調節動作來滿足安全約束，使策略忽略安全約束並專注於最大化回報。此外，還提出了一個具有理論更新規則的分配評論家，用於緩解安全約束的 Q 值過度估計。無人機 (UAV) 懸停的模擬和現實世界場景實驗都證實了 SMAC 能夠有效地維持安全約束，並且優於主流基準演算法。

##### **Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity**
2410.06846v1 by Mutian He, Philip N. Garner

Architectures such as Linformer and Mamba have recently emerged as
competitive linear time replacements for transformers. However, corresponding
large pretrained models are often unavailable, especially in non-text domains.
To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)
approach that jointly converts a transformer model to a linear time substitute
and fine-tunes it to a target task. We also compare several means to guide the
fine-tuning to optimally retain the desired inference capability from the
original model. The methods differ in their use of the target model and the
trajectory of the parameters. In a series of empirical studies on language
processing, language modeling, and speech processing, we show that CALD can
effectively recover the result of the original model, and that the guiding
strategy contributes to the result. Some reasons for the variation are
suggested.

摘要：最近，Linformer 和 Mamba 等架構已成為 transformer 的競爭線性時間替換。然而，對應的大型預訓練模型通常不可用，特別是在非文字領域。為了補救這個問題，我們提出了一個跨架構逐層蒸餾 (CALD) 方法，該方法聯合將 transformer 模型轉換為線性時間替代並對其進行微調以適應目標任務。我們還比較了幾種方法來指導微調，以最佳地保留原始模型所需的推論能力。這些方法在使用目標模型和參數軌跡方面有所不同。在語言處理、語言建模和語音處理的一系列實證研究中，我們表明 CALD 可以有效地恢復原始模型的結果，並且指導策略有助於實現這一結果。提出了一些變化的原因。

##### **MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders**
2410.06845v1 by Cheng Li, May Fung, Qingyun Wang, Chi Han, Manling Li, Jindong Wang, Heng Ji

Mental health disorders are one of the most serious diseases in the world.
Most people with such a disease lack access to adequate care, which highlights
the importance of training models for the diagnosis and treatment of mental
health disorders. However, in the mental health domain, privacy concerns limit
the accessibility of personalized treatment data, making it challenging to
build powerful models. In this paper, we introduce MentalArena, a self-play
framework to train language models by generating domain-specific personalized
data, where we obtain a better model capable of making a personalized diagnosis
and treatment (as a therapist) and providing information (as a patient). To
accurately model human-like mental health patients, we devise Symptom Encoder,
which simulates a real patient from both cognition and behavior perspectives.
To address intent bias during patient-therapist interactions, we propose
Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and
dynamically manage the dialogue between patient and therapist according to the
identified deviations. We evaluated MentalArena against 6 benchmarks, including
biomedicalQA and mental health tasks, compared to 6 advanced models. Our
models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform
their counterparts, including GPT-4o. We hope that our work can inspire future
research on personalized care. Code is available in
https://github.com/Scarelette/MentalArena/tree/main

摘要：心理健康障礙是世界上最嚴重的疾病之一。
大多數患有這種疾病的人無法獲得適當的照護，這凸顯了訓練模型以診斷和治療心理健康障礙的重要性。然而，在心理健康領域，隱私問題限制了個人化治療資料的可及性，這使得建立強大的模型變得具有挑戰性。在本文中，我們介紹了 MentalArena，一個自玩框架，通過生成特定領域的個人化資料來訓練語言模型，在其中我們獲得了一個更好的模型，能夠進行個人化診斷和治療（作為治療師）並提供資訊（作為患者）。為了準確模擬類似人類的心理健康患者，我們設計了症狀編碼器，它從認知和行為的角度模擬一個真實的患者。為了解決患者與治療師互動期間的意圖偏差，我們提出了症狀解碼器，將診斷出的症狀與編碼症狀進行比較，並根據識別出的偏差動態管理患者與治療師之間的對話。我們針對 6 個基準對 MentalArena 進行了評估，包括生物醫學問答和心理健康任務，並與 6 個先進模型進行了比較。我們的模型在 GPT-3.5 和 Llama-3-8b 上都進行了微調，顯著優於其對應模型，包括 GPT-4o。我們希望我們的研究能激勵未來對個人化照護的研究。程式碼可在 https://github.com/Scarelette/MentalArena/tree/main 中獲得

##### **Dynamic Neural Potential Field: Online Trajectory Optimization in Presence of Moving Obstacles**
2410.06819v1 by Aleksey Staroverov, Muhammad Alhaddad, Aditya Narendra, Konstantin Mironov, Aleksandr Panov

We address a task of local trajectory planning for the mobile robot in the
presence of static and dynamic obstacles. Local trajectory is obtained as a
numerical solution of the Model Predictive Control (MPC) problem. Collision
avoidance may be provided by adding repulsive potential of the obstacles to the
cost function of MPC. We develop an approach, where repulsive potential is
estimated by the neural model. We propose and explore three possible strategies
of handling dynamic obstacles. First, environment with dynamic obstacles is
considered as a sequence of static environments. Second, the neural model
predict a sequence of repulsive potential at once. Third, the neural model
predict future repulsive potential step by step in autoregressive mode. We
implement these strategies and compare it with CIAO* and MPPI using BenchMR
framework. First two strategies showed higher performance than CIAO* and MPPI
while preserving safety constraints. The third strategy was a bit slower,
however it still satisfy time limits. We deploy our approach on Husky UGV
mobile platform, which move through the office corridors under proposed MPC
local trajectory planner. The code and trained models are available at
\url{https://github.com/CognitiveAISystems/Dynamic-Neural-Potential-Field}.

摘要：<paragraph>我们针对存在静态和动态障碍物的移动机器人的局部轨迹规划任务进行了解决。局部轨迹作为模型预测控制 (MPC) 问题的数值解获得。通过向 MPC 的成本函数添加障碍物的排斥势能，可以提供防撞功能。我们开发了一种方法，其中排斥势能由神经模型估计。我们提出并探讨了处理动态障碍物的三种可能的策略。首先，将具有动态障碍物环境视为一系列静态环境。其次，神经模型一次预测一系列排斥势能。第三，神经模型在自回归模式中逐步预测未来的排斥势能。我们实施了这些策略，并使用 BenchMR 框架将其与 CIAO* 和 MPPI 进行了比较。前两种策略在保持安全约束的同时，表现出比 CIAO* 和 MPPI 更高的性能。第三种策略有点慢，但仍然满足时间限制。我们在 Husky UGV 移动平台上部署了我们的方法，该平台在提出的 MPC 局部轨迹规划器的作用下在办公室走廊中移动。代码和训练模型可在 \url{https://github.com/CognitiveAISystems/Dynamic-Neural-Potential-Field} 获得。</paragraph>

##### **An Improved Approach for Cardiac MRI Segmentation based on 3D UNet Combined with Papillary Muscle Exclusion**
2410.06818v1 by Narjes Benameur, Ramzi Mahmoudi, Mohamed Deriche, Amira fayouka, Imene Masmoudi, Nessrine Zoghlami

Left ventricular ejection fraction (LVEF) is the most important clinical
parameter of cardiovascular function. The accuracy in estimating this parameter
is highly dependent upon the precise segmentation of the left ventricle (LV)
structure at the end diastole and systole phases. Therefore, it is crucial to
develop robust algorithms for the precise segmentation of the heart structure
during different phases. Methodology: In this work, an improved 3D UNet model
is introduced to segment the myocardium and LV, while excluding papillary
muscles, as per the recommendation of the Society for Cardiovascular Magnetic
Resonance. For the practical testing of the proposed framework, a total of
8,400 cardiac MRI images were collected and analysed from the military hospital
in Tunis (HMPIT), as well as the popular ACDC public dataset. As performance
metrics, we used the Dice coefficient and the F1 score for validation/testing
of the LV and the myocardium segmentation. Results: The data was split into
70%, 10%, and 20% for training, validation, and testing, respectively. It is
worth noting that the proposed segmentation model was tested across three axis
views: basal, medio basal and apical at two different cardiac phases: end
diastole and end systole instances. The experimental results showed a Dice
index of 0.965 and 0.945, and an F1 score of 0.801 and 0.799, at the end
diastolic and systolic phases, respectively. Additionally, clinical evaluation
outcomes revealed a significant difference in the LVEF and other clinical
parameters when the papillary muscles were included or excluded.

摘要：左心室射血分數 (LVEF) 是心血管功能最重要的臨床參數。估計此參數的準確性高度依賴於左心室 (LV) 結構在舒張末期和收縮期的精確分割。因此，開發用於精確分割不同時期心臟結構的強健演算法至關重要。方法：在此工作中，引進了一個改良的 3D UNet 模型來分割心肌和左心室，同時根據心血管磁共振學會的建議排除乳頭肌。為了對提出的架構進行實際測試，從突尼斯的軍事醫院 (HMPIT) 和流行的 ACDC 公共資料集收集並分析了總共 8,400 張心臟 MRI 影像。作為效能指標，我們使用 Dice 係數和 F1 分數來驗證/測試左心室和心肌分割。結果：資料被分成 70%、10% 和 20% 分別用於訓練、驗證和測試。值得注意的是，所提出的分割模型在三個軸向視圖中進行了測試：基底、中基底和心尖，在兩個不同的心臟時期：舒張末期和收縮末期。實驗結果顯示，在舒張末期和收縮期，Dice 指數分別為 0.965 和 0.945，F1 分數分別為 0.801 和 0.799。此外，臨床評估結果顯示，當乳頭肌被納入或排除時，LVEF 和其他臨床參數存在顯著差異。

##### **Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning**
2410.06814v1 by Qiang Hu, Hengxiang Zhang, Hongxin Wei

Over-parameterized models are typically vulnerable to membership inference
attacks, which aim to determine whether a specific sample is included in the
training of a given model. Previous Weight regularizations (e.g., L1
regularization) typically impose uniform penalties on all parameters, leading
to a suboptimal tradeoff between model utility and privacy. In this work, we
first show that only a small fraction of parameters substantially impact the
privacy risk. In light of this, we propose Privacy-aware Sparsity Tuning
(PAST), a simple fix to the L1 Regularization, by employing adaptive penalties
to different parameters. Our key idea behind PAST is to promote sparsity in
parameters that significantly contribute to privacy leakage. In particular, we
construct the adaptive weight for each parameter based on its privacy
sensitivity, i.e., the gradient of the loss gap with respect to the parameter.
Using PAST, the network shrinks the loss gap between members and non-members,
leading to strong resistance to privacy attacks. Extensive experiments
demonstrate the superiority of PAST, achieving a state-of-the-art balance in
the privacy-utility trade-off.

摘要：過度參數化的模型通常容易受到成員推論攻擊，其目的是確定特定範例是否包含在給定模型的訓練中。先前的權重正則化（例如 L1 正則化）通常對所有參數施加均勻的懲罰，導致模型實用性與隱私之間的權衡取捨不佳。在這項工作中，我們首先展示只有少部分參數會大幅影響隱私風險。有鑑於此，我們提出具有隱私意識的稀疏調整 (PAST)，一種透過對不同參數採用自適應懲罰來對 L1 正則化進行簡單修正的方法。PAST 背後的主要概念是促進對導致隱私外洩有重大影響的參數中的稀疏性。特別是，我們根據每個參數的隱私敏感度（即損失差距相對於參數的梯度）來建構自適應權重。使用 PAST，網路會縮小成員和非成員之間的損失差距，從而對隱私攻擊產生強大的抵抗力。廣泛的實驗證明了 PAST 的優越性，在隱私實用性權衡中取得了最先進的平衡。

##### **Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level**
2410.06809v1 by Xinyi Zeng, Yuying Shang, Yutao Zhu, Jiawei Chen, Yu Tian

Large language models (LLMs) have demonstrated immense utility across various
industries. However, as LLMs advance, the risk of harmful outputs increases due
to incorrect or malicious instruction prompts. While current methods
effectively address jailbreak risks, they share common limitations: 1) Judging
harmful responses from the prefill-level lacks utilization of the model's
decoding outputs, leading to relatively lower effectiveness and robustness. 2)
Rejecting potentially harmful responses based on a single evaluation can
significantly impair the model's helpfulness.This paper examines the LLMs'
capability to recognize harmful outputs, revealing and quantifying their
proficiency in assessing the danger of previous tokens. Motivated by pilot
experiment results, we design a robust defense mechanism at the decoding level.
Our novel decoder-oriented, step-by-step defense architecture corrects harmful
queries directly rather than rejecting them outright. We introduce speculative
decoding to enhance usability and facilitate deployment to boost secure
decoding speed. Extensive experiments demonstrate that our approach improves
model security without compromising reasoning speed. Notably, our method
leverages the model's ability to discern hazardous information, maintaining its
helpfulness compared to existing methods.

摘要：大型語言模型 (LLM) 已在各產業展示出極大的效用。然而，隨著 LLM 的進步，由於錯誤或惡意的指令提示，有害輸出的風險也隨之增加。雖然目前的技術能有效解決越獄風險，但它們有共同的限制：1) 從預填層級判斷有害回應缺乏利用模型的解碼輸出，導致相對較低的有效性和穩健性。2) 根據單一評估拒絕潛在有害的回應會顯著損害模型的幫助性。本文探討了 LLM 識別有害輸出的能力，揭示並量化了它們評估先前標記危險的能力。受試驗結果啟發，我們在解碼層級設計了一個強健的防禦機制。我們新穎的面向解碼器、逐步進行的防禦架構會直接修正有害查詢，而不是直接拒絕它們。我們引入了推測性解碼來增強可用性，並促進部署以提升安全的解碼速度。大量的實驗證實，我們的方法改善了模型安全性，同時不影響推理速度。值得注意的是，與現有方法相比，我們的方法利用了模型辨別有害資訊的能力，維持了其幫助性。

##### **Seg2Act: Global Context-aware Action Generation for Document Logical Structuring**
2410.06802v1 by Zichao Li, Shaojie He, Meng Liao, Xuanang Chen, Yaojie Lu, Hongyu Lin, Yanxiong Lu, Xianpei Han, Le Sun

Document logical structuring aims to extract the underlying hierarchical
structure of documents, which is crucial for document intelligence. Traditional
approaches often fall short in handling the complexity and the variability of
lengthy documents. To address these issues, we introduce Seg2Act, an
end-to-end, generation-based method for document logical structuring,
revisiting logical structure extraction as an action generation task.
Specifically, given the text segments of a document, Seg2Act iteratively
generates the action sequence via a global context-aware generative model, and
simultaneously updates its global context and current logical structure based
on the generated actions. Experiments on ChCatExt and HierDoc datasets
demonstrate the superior performance of Seg2Act in both supervised and transfer
learning settings.

摘要：文件邏輯結構化旨在提取文件的基礎階層結構，這對文件智慧至關重要。傳統方法在處理複雜性和長文件的可變性時，常常力不從心。為了解決這些問題，我們引入了 Seg2Act，一種端到端的、基於生成的用於文件邏輯結構化的方法，將邏輯結構提取重新視為一個動作生成任務。具體來說，給定一個文件的文本片段，Seg2Act 通過一個全局上下文感知生成模型反覆生成動作序列，並根據生成的動作同時更新其全局上下文和當前的邏輯結構。在 ChCatExt 和 HierDoc 數據集上的實驗證明了 Seg2Act 在監督式和遷移式學習設置中都具有優異的性能。

##### **Diffuse or Confuse: A Diffusion Deepfake Speech Dataset**
2410.06796v1 by Anton Firc, Kamil Malinka, Petr Hanáček

Advancements in artificial intelligence and machine learning have
significantly improved synthetic speech generation. This paper explores
diffusion models, a novel method for creating realistic synthetic speech. We
create a diffusion dataset using available tools and pretrained models.
Additionally, this study assesses the quality of diffusion-generated deepfakes
versus non-diffusion ones and their potential threat to current deepfake
detection systems. Findings indicate that the detection of diffusion-based
deepfakes is generally comparable to non-diffusion deepfakes, with some
variability based on detector architecture. Re-vocoding with diffusion vocoders
shows minimal impact, and the overall speech quality is comparable to
non-diffusion methods.

摘要：人工智慧和機器學習的進步已顯著改善合成語音產生。本文探討擴散模型，一種創新方法來建立逼真的合成語音。我們使用現有工具和預訓練模型建立擴散資料集。此外，本研究評估擴散生成的深度偽造與非擴散生成的深度偽造的品質，以及它們對目前深度偽造偵測系統的潛在威脅。研究結果顯示，基於擴散的深度偽造偵測通常與非擴散深度偽造相當，但會根據偵測器架構而有些許變異。使用擴散語音編碼器重新編碼顯示影響最小，且整體語音品質與非擴散方法相當。

##### **From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models**
2410.06795v1 by Yuying Shang, Xinyi Zeng, Yutao Zhu, Xiao Yang, Zhengwei Fang, Jingyuan Zhang, Jiawei Chen, Zinan Liu, Yu Tian

Hallucinations in large vision-language models (LVLMs) are a significant
challenge, i.e., generating objects that are not presented in the visual input,
which impairs their reliability. Recent studies often attribute hallucinations
to a lack of understanding of visual input, yet ignore a more fundamental
issue: the model's inability to effectively extract or decouple visual
features. In this paper, we revisit the hallucinations in LVLMs from an
architectural perspective, investigating whether the primary cause lies in the
visual encoder (feature extraction) or the modal alignment module (feature
decoupling). Motivated by our findings on the preliminary investigation, we
propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs.
This plug-and-play method can be integrated into various LVLMs, utilizing
adaptive virtual tokens to extract object features from bounding boxes, thereby
addressing hallucinations caused by insufficient decoupling of visual features.
PATCH achieves state-of-the-art performance on multiple multi-modal
hallucination datasets. We hope this approach provides researchers with deeper
insights into the underlying causes of hallucinations in LVLMs, fostering
further advancements and innovation in this field.

摘要：大型視覺語言模型 (LVLMs) 中的幻覺是一個重大的挑戰，也就是產生視覺輸入中未呈現的物體，這會損害其可靠性。最近的研究常常將幻覺歸因於對視覺輸入理解不足，卻忽略了一個更根本的問題：模型無法有效提取或解耦視覺特徵。在本文中，我們從架構的角度重新探討 LVLMs 中的幻覺，探討主要原因是否在於視覺編碼器（特徵提取）或模態對齊模組（特徵解耦）。受到我們在初步調查中發現的結果啟發，我們提出了一種新穎的調整策略 PATCH，以減輕 LVLMs 中的幻覺。這種即插即用方法可以整合到各種 LVLMs 中，利用自適應虛擬代幣從邊界框中提取物件特徵，從而解決因視覺特徵解耦不足而導致的幻覺。PATCH 在多個多模態幻覺資料集上達到了最先進的效能。我們希望這種方法能為研究人員提供對 LVLMs 中幻覺根本原因更深入的見解，進而促進該領域的進一步進步和創新。

##### **To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models**
2410.06765v1 by Junyan Lin, Haoran Chen, Dawei Zhu, Xiaoyu Shen

In recent years, multimodal large language models (MLLMs) have garnered
significant attention from both industry and academia. However, there is still
considerable debate on constructing MLLM architectures, particularly regarding
the selection of appropriate connectors for perception tasks of varying
granularities. This paper systematically investigates the impact of connectors
on MLLM performance. Specifically, we classify connectors into
feature-preserving and feature-compressing types. Utilizing a unified
classification standard, we categorize sub-tasks from three comprehensive
benchmarks, MMBench, MME, and SEED-Bench, into three task types: coarse-grained
perception, fine-grained perception, and reasoning, and evaluate the
performance. Our findings reveal that feature-preserving connectors excel in
\emph{fine-grained perception} tasks due to their ability to retain detailed
visual information. In contrast, feature-compressing connectors, while less
effective in fine-grained perception tasks, offer significant speed advantages
and perform comparably in \emph{coarse-grained perception} and \emph{reasoning}
tasks. These insights are crucial for guiding MLLM architecture design and
advancing the optimization of MLLM architectures.

摘要：近年來，多模態大型語言模型 (MLLM) 已引起業界和學術界的極大關注。然而，對於構建 MLLM 架構，特別是關於選擇適用於不同粒度感知任務的連接器，仍有相當大的爭議。本文系統性地探討了連接器對 MLLM 效能的影響。具體來說，我們將連接器分類為特徵保留型和特徵壓縮型。利用統一的分類標準，我們將來自三個綜合基準 MMBench、MME 和 SEED-Bench 的子任務分類為三種類型的任務：粗粒度感知、細粒度感知和推理，並評估其效能。我們的研究結果表明，特徵保留型連接器在「細粒度感知」任務中表現出色，因為它們能夠保留詳細的視覺資訊。相比之下，特徵壓縮型連接器雖然在細粒度感知任務中的效果較差，但提供了顯著的速度優勢，並且在「粗粒度感知」和「推理」任務中的表現相當。這些見解對於指導 MLLM 架構設計和推進 MLLM 架構的最佳化至關重要。

##### **CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models**
2410.06741v1 by Zi Gong, Hang Yu, Cong Liao, Bingchang Liu, Chaoyu Chen, Jianguo Li

Multi-task learning (MTL) benefits the fine-tuning of large language models
(LLMs) by providing a single model with improved performance and generalization
ability across tasks, presenting a resource-efficient alternative to developing
separate models for each task. Yet, existing MTL strategies for LLMs often fall
short by either being computationally intensive or failing to ensure
simultaneous task convergence. This paper presents CoBa, a new MTL approach
designed to effectively manage task convergence balance with minimal
computational overhead. Utilizing Relative Convergence Scores (RCS), Absolute
Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically
adjusts task weights during the training process, ensuring that the validation
loss of all tasks progress towards convergence at an even pace while mitigating
the issue of individual task divergence. The results of our experiments
involving three disparate datasets underscore that this approach not only
fosters equilibrium in task improvement but enhances the LLMs' performance by
up to 13% relative to the second-best baselines. Code is open-sourced at
https://github.com/codefuse-ai/MFTCoder.

摘要：多任務學習 (MTL) 透過提供單一模型，進而改善大型語言模型 (LLM) 的微調，提升任務間的效能和泛化能力，提供開發每項任務的獨立模型的省資源替代方案。然而，現有的 LLM MTL 策略通常會因運算密集或無法確保任務同時收斂而不足。本文提出 CoBa，一種新的 MTL 方法，旨在有效管理任務收斂平衡，且運算開銷最小。CoBa 在訓練過程中動態調整任務權重，利用相對收斂分數 (RCS)、絕對收斂分數 (ACS) 和分歧因子 (DF)，確保所有任務的驗證損失以均等的步調朝向收斂進行，同時減輕個別任務分歧的問題。我們在三個不同的資料集進行實驗的結果強調，這種方法不僅促進任務改進的平衡，更將 LLM 的效能提升多達 13%，相較於第二佳基準。程式碼已於 https://github.com/codefuse-ai/MFTCoder 開源。

##### **Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?**
2410.06735v1 by Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo

Recent large language models (LLMs) have demonstrated remarkable
generalization abilities in mathematics and logical reasoning tasks. Prior
research indicates that LLMs pre-trained with programming language data exhibit
high mathematical and reasoning abilities; however, this causal relationship
has not been rigorously tested. Our research aims to verify which programming
languages and features during pre-training affect logical inference
performance. Specifically, we pre-trained decoder-based language models from
scratch using datasets from ten programming languages (e.g., Python, C, Java)
and three natural language datasets (Wikipedia, Fineweb, C4) under identical
conditions. Thereafter, we evaluated the trained models in a few-shot
in-context learning setting on logical reasoning tasks: FLD and bAbi, which do
not require commonsense or world knowledge. The results demonstrate that nearly
all models trained with programming languages consistently outperform those
trained with natural languages, indicating that programming languages contain
factors that elicit logic inference performance. In addition, we found that
models trained with programming languages exhibit a better ability to follow
instructions compared to those trained with natural languages. Further analysis
reveals that the depth of Abstract Syntax Trees representing parsed results of
programs also affects logical reasoning performance. These findings will offer
insights into the essential elements of pre-training for acquiring the
foundational abilities of LLMs.

摘要：近期的大型语言模型 (LLM) 在数学和逻辑推理任务中展示出卓越的泛化能力。先前的研究表明，使用编程语言数据预先训练的 LLM 表现出很高的数学和推理能力；然而，这种因果关系尚未经过严格的测试。我们的研究旨在验证预训练期间哪种编程语言和功能会影响逻辑推理性能。具体而言，我们从头开始使用来自十种编程语言（例如 Python、C、Java）和三种自然语言数据集（维基百科、Fineweb、C4）的数据集预先训练了基于解码器的语言模型，条件相同。此后，我们在逻辑推理任务（FLD 和 bAbi）的少量镜头情境学习设置中评估了训练后的模型，这些任务不需要常识或世界知识。结果表明，几乎所有使用编程语言训练的模型都比使用自然语言训练的模型表现得更好，这表明编程语言包含引发逻辑推理性能的因素。此外，我们发现使用编程语言训练的模型与使用自然语言训练的模型相比，表现出更好的遵循指令的能力。进一步的分析表明，表示程序解析结果的抽象语法树的深度也会影响逻辑推理性能。这些发现将为获取 LLM 的基础能力的预训练的基本要素提供见解。

##### **Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles**
2410.06733v1 by Qi Chen, Bowen Zhang, Gang Wang, Qi Wu

While advancements in NLP have significantly improved the performance of
Large Language Models (LLMs) on tasks requiring vertical thinking, their
lateral thinking capabilities remain under-explored and challenging to measure
due to the complexity of assessing creative thought processes and the scarcity
of relevant data. To address these challenges, we introduce SPLAT, a benchmark
leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs.
This benchmark, containing 975 graded situation puzzles across three difficulty
levels, employs a new multi-turn player-judge framework instead of the
traditional model-based evaluation, which often necessitates a stronger
evaluation model. This framework simulates an interactive game where the model
(player) asks the evaluation model (judge) questions about an incomplete story
to infer the full scenario. The judge answers based on a detailed reference
scenario or evaluates if the player's predictions align with the reference one.
This approach lessens dependence on more robust evaluation models, enabling the
assessment of state-of-the-art LLMs. The experiments demonstrate that a robust
evaluation model, such as WizardLM-2, closely matches human judgements in both
intermediate question-answering and final scenario accuracy, achieving over 80%
agreement-similar to the agreement levels among humans. Furthermore, applying
data and reasoning processes from our benchmark to other lateral
thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to
performance enhancements. This suggests that our benchmark effectively
evaluates and elicits the lateral thinking abilities of LLMs. Code is available
at: https://github.com/chenqi008/LateralThinking.

摘要：<paragraph>儘管 NLP 的進展大幅提升了大型語言模型 (LLM) 在需要垂直思考任務上的表現，但由於評估創意思考過程的複雜性和相關數據的稀缺性，它們的橫向思考能力仍然處於探索不足且難以衡量的狀態。為了應對這些挑戰，我們引入了 SPLAT，一個利用情境謎題來評估和引發 LLM 的橫向思考的基準。這個基準包含 975 個分級情境謎題，涵蓋三個難度等級，採用新的多輪玩家評審架構，取代了傳統的基於模型的評估，後者通常需要更強大的評估模型。此架構模擬了一場互動遊戲，其中模型（玩家）向評估模型（評審）詢問關於不完整故事的問題，以推斷出完整的情境。評審根據詳細的參考情境回答問題，或評估玩家的預測是否與參考情境一致。這種方法減少了對更強大的評估模型的依賴，從而能夠評估最先進的 LLM。實驗表明，強大的評估模型（例如 WizardLM-2）在中間問題解答和最終情境準確度方面與人類判斷非常接近，達成率超過 80%，類似於人類之間的達成率。此外，將我們基準中的數據和推理過程應用於其他與橫向思考相關的基準（例如 RiddleSense 和 BrainTeaser），會導致效能提升。這表明我們的基準有效地評估和引發了 LLM 的橫向思考能力。程式碼可在以下網址取得：https://github.com/chenqi008/LateralThinking。</paragraph>

##### **Scaling Laws for Mixed quantization in Large Language Models**
2410.06722v1 by Zeyu Cao, Cheng Zhang, Pedro Gimenes, Jianqiao Lu, Jianyi Cheng, Yiren Zhao

Post-training quantization of Large Language Models (LLMs) has proven
effective in reducing the computational requirements for running inference on
these models. In this study, we focus on a straightforward question: When
aiming for a specific accuracy or perplexity target for low-precision
quantization, how many high-precision numbers or calculations are required to
preserve as we scale LLMs to larger sizes? We first introduce a critical metric
named the quantization ratio, which compares the number of parameters quantized
to low-precision arithmetic against the total parameter count. Through
extensive and carefully controlled experiments across different model families,
arithmetic types, and quantization granularities (e.g. layer-wise,
matmul-wise), we identify two central phenomenons. 1) The larger the models,
the better they can preserve performance with an increased quantization ratio,
as measured by perplexity in pre-training tasks or accuracy in downstream
tasks. 2) The finer the granularity of mixed-precision quantization (e.g.,
matmul-wise), the more the model can increase the quantization ratio. We
believe these observed phenomena offer valuable insights for future AI hardware
design and the development of advanced Efficient AI algorithms.

摘要：大型語言模型 (LLM) 的訓練後量化已證明可有效降低執行這些模型推理的運算需求。在本研究中，我們專注於一個簡單的問題：在針對低精度量化設定特定準確度或困惑度目標時，在我們將 LLM 擴展到更大規模時，需要保留多少高精度數字或運算？我們首先引入一個稱為量化比率的關鍵指標，它會比較量化為低精度算術的參數數量與總參數數量。透過針對不同模型系列、算術類型和量化粒度（例如，按層、按矩陣乘法）進行廣泛且仔細控制的實驗，我們找出兩個主要的現象。1) 模型越大，它們就能以更高的量化比率維持效能，這可透過預訓練任務中的困惑度或下游任務中的準確度來衡量。2) 混合精度量化的粒度越細（例如，按矩陣乘法），模型就能提高量化比率。我們相信這些觀察到的現象為未來的 AI 硬體設計和進階高效能 AI 演算法的開發提供了寶貴的見解。

##### **Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques**
2410.06719v1 by Benyuan Meng, Qianqian Xu, Zitai Wang, Zhiyong Yang, Xiaochun Cao, Qingming Huang

Diffusion models are powerful generative models, and this capability can also
be applied to discrimination. The inner activations of a pre-trained diffusion
model can serve as features for discriminative tasks, namely, diffusion
feature. We discover that diffusion feature has been hindered by a hidden yet
universal phenomenon that we call content shift. To be specific, there are
content differences between features and the input image, such as the exact
shape of a certain object. We locate the cause of content shift as one inherent
characteristic of diffusion models, which suggests the broad existence of this
phenomenon in diffusion feature. Further empirical study also indicates that
its negative impact is not negligible even when content shift is not visually
perceivable. Hence, we propose to suppress content shift to enhance the overall
quality of diffusion features. Specifically, content shift is related to the
information drift during the process of recovering an image from the noisy
input, pointing out the possibility of turning off-the-shelf generation
techniques into tools for content shift suppression. We further propose a
practical guideline named GATE to efficiently evaluate the potential benefit of
a technique and provide an implementation of our methodology. Despite the
simplicity, the proposed approach has achieved superior results on various
tasks and datasets, validating its potential as a generic booster for diffusion
features. Our code is available at
https://github.com/Darkbblue/diffusion-content-shift.

摘要：扩散模型是强大的生成模型，这种能力也可以应用于判别。预先训练过的扩散模型的内部激活可以作为判别任务的特征，即扩散特征。我们发现扩散特征受到一种我们称之为内容转移的隐藏但普遍现象的阻碍。具体来说，特征和输入图像之间存在内容差异，例如某个对象的准确形状。我们定位内容转移的原因是扩散模型的一个固有特征，这表明这种现象在扩散特征中普遍存在。进一步的实证研究还表明，即使内容转移在视觉上不可感知，其负面影响也不容忽视。因此，我们建议抑制内容转移以提高扩散特征的整体质量。具体来说，内容转移与从噪声输入中恢复图像过程中的信息漂移有关，指出了将现成的生成技术转变为内容转移抑制工具的可能性。我们进一步提出了一个名为 GATE 的实用指南，以有效评估技术潜在收益，并提供了我们方法的实现。尽管简单，但所提出的方法在各种任务和数据集上取得了优异的成果，验证了其作为扩散特征通用助推器的潜力。我们的代码可在 https://github.com/Darkbblue/diffusion-content-shift 获得。

##### **MatMamba: A Matryoshka State Space Model**
2410.06718v1 by Abhinav Shukla, Sai Vemprala, Aditya Kusupati, Ashish Kapoor

State Space Models (SSMs) like Mamba2 are a promising alternative to
Transformers, with faster theoretical training and inference times --
especially for long context lengths. Recent work on Matryoshka Representation
Learning -- and its application to Transformer backbones in works like
MatFormer -- showed how to introduce nested granularities of smaller submodels
in one universal elastic model. In this work, we present MatMamba: a state
space model which combines Matryoshka-style learning with Mamba2, by modifying
the block to contain nested dimensions to enable joint training and adaptive
inference. MatMamba allows for efficient and adaptive deployment across various
model sizes. We train a single large MatMamba model and are able to get a
number of smaller nested models for free -- while maintaining or improving upon
the performance of a baseline smaller model trained from scratch. We train
language and image models at a variety of parameter sizes from 35M to 1.4B. Our
results on ImageNet and FineWeb show that MatMamba models scale comparably to
Transformers, while having more efficient inference characteristics. This makes
MatMamba a practically viable option for deploying large-scale models in an
elastic way based on the available inference compute. Code and models are open
sourced at \url{https://github.com/ScaledFoundations/MatMamba}

摘要：狀態空間模型 (SSM)，例如 Mamba2，是 Transformer 的有前途的替代方案，理論訓練和推論時間更快——特別是對於長內容長度。最近關於 Matryoshka 表示學習的工作——以及它在 MatFormer 等作品中對 Transformer 主幹的應用——展示了如何在一個通用的彈性模型中引入較小子模型的嵌套粒度。在這項工作中，我們展示了 MatMamba：一種狀態空間模型，它通過修改塊以包含嵌套維度來結合 Matryoshka 風格的學習與 Mamba2，以實現聯合訓練和自適應推論。MatMamba 允許跨各種模型大小進行高效和自適應的部署。我們訓練一個大型 MatMamba 模型，並且能夠免費獲得許多較小的嵌套模型——同時保持或改進從頭開始訓練的基準較小模型的性能。我們以 35M 到 1.4B 的各種參數大小訓練語言和圖像模型。我們在 ImageNet 和 FineWeb 上的結果表明，MatMamba 模型可以與 Transformer 進行比較，同時具有更有效的推論特性。這使得 MatMamba 成為一種實用的可行選項，可以用彈性方式部署大規模模型，具體取決於可用的推論計算。代碼和模型在 \url{https://github.com/ScaledFoundations/MatMamba} 開源

##### **Guaranteed Generation from Large Language Models**
2410.06716v1 by Minbeom Kim, Thibaut Thonet, Jos Rozen, Hwaran Lee, Kyomin Jung, Marc Dymetman

As large language models (LLMs) are increasingly used across various
applications, there is a growing need to control text generation to satisfy
specific constraints or requirements. This raises a crucial question: Is it
possible to guarantee strict constraint satisfaction in generated outputs while
preserving the distribution of the original model as much as possible? We first
define the ideal distribution - the one closest to the original model, which
also always satisfies the expressed constraint - as the ultimate goal of
guaranteed generation. We then state a fundamental limitation, namely that it
is impossible to reach that goal through autoregressive training alone. This
motivates the necessity of combining training-time and inference-time methods
to enforce such guarantees. Based on this insight, we propose GUARD, a simple
yet effective approach that combines an autoregressive proposal distribution
with rejection sampling. Through GUARD's theoretical properties, we show how
controlling the KL divergence between a specific proposal and the target ideal
distribution simultaneously optimizes inference speed and distributional
closeness. To validate these theoretical concepts, we conduct extensive
experiments on two text generation settings with hard-to-satisfy constraints: a
lexical constraint scenario and a sentiment reversal scenario. These
experiments show that GUARD achieves perfect constraint satisfaction while
almost preserving the ideal distribution with highly improved inference
efficiency. GUARD provides a principled approach to enforcing strict guarantees
for LLMs without compromising their generative capabilities.

摘要：隨著大型語言模型 (LLM) 在各種應用中使用越來越廣泛，控制文本生成以滿足特定約束或需求的需求也日益增長。這引發了一個關鍵問題：是否能在生成的輸出中保證嚴格的約束滿足，同時盡可能保持原始模型的分布？我們首先定義理想分佈，即最接近原始模型的分佈，它也總是滿足表達的約束，作為保證生成的最終目標。然後，我們陳述了一個基本限制，即僅通過自迴歸訓練無法實現這一目標。這促使有必要結合訓練時間和推理時間方法來強制執行此類保證。基於此見解，我們提出了 GUARD，這是一種簡單而有效的方法，它將自迴歸提案分佈與拒絕採樣相結合。通過 GUARD 的理論屬性，我們展示了如何控制特定提案與目標理想分佈之間的 KL 散度，同時優化推理速度和分佈接近度。為了驗證這些理論概念，我們對兩個具有難以滿足約束的文本生成設置進行了廣泛的實驗：詞彙約束場景和情緒反轉場景。這些實驗表明，GUARD 在幾乎保持理想分佈的同時實現了完美的約束滿足，並極大地提高了推理效率。GUARD 提供了一種原則性的方法，可以在不影響 LLM 的生成能力的情況下強制執行嚴格的保證。

##### **Calibrating Verbalized Probabilities for Large Language Models**
2410.06707v1 by Cheng Wang, Gyuri Szarvas, Georges Balazs, Pavel Danchenko, Patrick Ernst

Calibrating verbalized probabilities presents a novel approach for reliably
assessing and leveraging outputs from black-box Large Language Models (LLMs).
Recent methods have demonstrated improved calibration by applying techniques
like Platt scaling or temperature scaling to the confidence scores generated by
LLMs. In this paper, we explore the calibration of verbalized probability
distributions for discriminative tasks. First, we investigate the capability of
LLMs to generate probability distributions over categorical labels. We
theoretically and empirically identify the issue of re-softmax arising from the
scaling of verbalized probabilities, and propose using the invert softmax trick
to approximate the "logit" by inverting verbalized probabilities. Through
extensive evaluation on three public datasets, we demonstrate: (1) the robust
capability of LLMs in generating class distributions, and (2) the effectiveness
of the invert softmax trick in estimating logits, which, in turn, facilitates
post-calibration adjustments.

摘要：校準口頭機率提出了一個新的方法，可以可靠地評估和利用黑箱大型語言模型 (LLM) 的輸出。
最近的方法已經展示了透過將 Platt 尺度或溫度尺度技術應用於 LLM 生成的信心分數，來改善校準。
在本文中，我們探討了針對區分任務的口頭機率分佈校準。首先，我們調查 LLM 在類別標籤上產生機率分佈的能力。
我們從理論和經驗上找出由口頭機率的縮放產生的重新 softmax 問題，並提出使用反 softmax 技巧透過反轉口頭機率來近似「邏輯」。
透過在三個公開資料集上進行廣泛評估，我們展示：(1) LLM 在產生類別分佈方面的強健能力，以及 (2) 反 softmax 技巧在估計邏輯方面的有效性，這反過來有助於後校準調整。

##### **PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs**
2410.06704v1 by Krishna Kanth Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, Xuebing Zhou

In this work, we introduce PII-Scope, a comprehensive benchmark designed to
evaluate state-of-the-art methodologies for PII extraction attacks targeting
LLMs across diverse threat settings. Our study provides a deeper understanding
of these attacks by uncovering several hyperparameters (e.g., demonstration
selection) crucial to their effectiveness. Building on this understanding, we
extend our study to more realistic attack scenarios, exploring PII attacks that
employ advanced adversarial strategies, including repeated and diverse
querying, and leveraging iterative learning for continual PII extraction.
Through extensive experimentation, our results reveal a notable underestimation
of PII leakage in existing single-query attacks. In fact, we show that with
sophisticated adversarial capabilities and a limited query budget, PII
extraction rates can increase by up to fivefold when targeting the pretrained
model. Moreover, we evaluate PII leakage on finetuned models, showing that they
are more vulnerable to leakage than pretrained models. Overall, our work
establishes a rigorous empirical benchmark for PII extraction attacks in
realistic threat scenarios and provides a strong foundation for developing
effective mitigation strategies.

摘要：在這項工作中，我們介紹 PII-Scope，一個全面的基準，旨在評估針對 LLM 的 PII 提取攻擊的最先進方法，涵蓋各種威脅設定。我們的研究透過揭露幾個對其有效性至關重要的超參數（例如，示範選擇）來更深入地了解這些攻擊。根據這項了解，我們將研究擴展到更實際的攻擊場景，探索採用進階對抗策略的 PII 攻擊，包括重複且多樣化的查詢，以及利用反覆學習進行持續的 PII 提取。透過廣泛的實驗，我們的結果揭示了現有單一查詢攻擊中對 PII 洩漏的顯著低估。事實上，我們表明，在具備精密的對抗能力和有限的查詢預算下，針對預訓練模型時，PII 提取率可以提高五倍。此外，我們評估了微調模型上的 PII 洩漏，表明它們比預訓練模型更容易受到洩漏的影響。總體而言，我們的研究為實際威脅場景中的 PII 提取攻擊建立了一個嚴謹的經驗基準，並為開發有效的緩解策略奠定了堅實的基礎。

##### **ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness in Web Agents**
2410.06703v1 by Ido Levy, Ben Wiesel, Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov

Recent advancements in LLM-based web agents have introduced novel
architectures and benchmarks showcasing progress in autonomous web navigation
and interaction. However, most existing benchmarks prioritize effectiveness and
accuracy, overlooking crucial factors like safety and trustworthiness which are
essential for deploying web agents in enterprise settings. The risks of unsafe
web agent behavior, such as accidentally deleting user accounts or performing
unintended actions in critical business operations, pose significant barriers
to widespread adoption.In this paper, we present ST-WebAgentBench, a new online
benchmark specifically designed to evaluate the safety and trustworthiness of
web agents in enterprise contexts. This benchmark is grounded in a detailed
framework that defines safe and trustworthy (ST) agent behavior, outlines how
ST policies should be structured and introduces the Completion under Policies
metric to assess agent performance. Our evaluation reveals that current SOTA
agents struggle with policy adherence and cannot yet be relied upon for
critical business applications. Additionally, we propose architectural
principles aimed at improving policy awareness and compliance in web agents. We
open-source this benchmark and invite the community to contribute, with the
goal of fostering a new generation of safer, more trustworthy AI agents.

摘要：最近基於 LLM 的網路代理的進展引入了新的架構和基準，展示了自主網路導航和互動的進展。然而，現有的基準大多重視有效性和準確性，卻忽略了在企業環境中部署網路代理必備的安全性和可信賴性等關鍵因素。網路代理行為不安全的風險，例如意外刪除使用者帳戶或在關鍵業務運作中執行意外動作，會對廣泛採用造成重大障礙。在本文中，我們提出了 ST-WebAgentBench，這是一個專門設計用於評估企業環境中網路代理的安全性和可信賴性的新線上基準。此基準建立在一個詳細的架構上，定義了安全和可信賴 (ST) 代理行為，概述了 ST 政策應如何制定，並引入了在政策下完成的指標來評估代理效能。我們的評估顯示，目前的 SOTA 代理在政策遵守方面有困難，而且還無法依賴在關鍵業務應用程式上。此外，我們提出了旨在改善網路代理中政策意識和遵循度的架構原則。我們開放原始碼這個基準，並邀請社群共同貢獻，目標是培育新一代更安全、更可信賴的 AI 代理。

##### **Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models**
2410.06699v1 by Yubo Wang, Chaohu Liu, Yanqiu Qu, Haoyu Cao, Deqiang Jiang, Linli Xu

Large vision-language models (LVLMs) integrate visual information into large
language models, showcasing remarkable multi-modal conversational capabilities.
However, the visual modules introduces new challenges in terms of robustness
for LVLMs, as attackers can craft adversarial images that are visually clean
but may mislead the model to generate incorrect answers. In general, LVLMs rely
on vision encoders to transform images into visual tokens, which are crucial
for the language models to perceive image contents effectively. Therefore, we
are curious about one question: Can LVLMs still generate correct responses when
the encoded visual tokens are attacked and disrupting the visual information?
To this end, we propose a non-targeted attack method referred to as VT-Attack
(Visual Tokens Attack), which constructs adversarial examples from multiple
perspectives, with the goal of comprehensively disrupting feature
representations and inherent relationships as well as the semantic properties
of visual tokens output by image encoders. Using only access to the image
encoder in the proposed attack, the generated adversarial examples exhibit
transferability across diverse LVLMs utilizing the same image encoder and
generality across different tasks. Extensive experiments validate the superior
attack performance of the VT-Attack over baseline methods, demonstrating its
effectiveness in attacking LVLMs with image encoders, which in turn can provide
guidance on the robustness of LVLMs, particularly in terms of the stability of
the visual feature space.

摘要：大型视觉语言模型 (LVLMs) 将视觉信息整合到大型语言模型中，展示了非凡的多模态对话能力。然而，视觉模块为 LVLMs 的鲁棒性带来了新的挑战，因为攻击者可以制作视觉上干净但可能误导模型生成错误答案的对抗性图像。一般来说，LVLMs 依赖于视觉编码器将图像转换为视觉标记，视觉标记对于语言模型有效感知图像内容至关重要。因此，我们对一个问题感到好奇：当编码的视觉标记受到攻击并破坏视觉信息时，LVLMs 是否仍然可以生成正确的响应？为此，我们提出了一种称为 VT-Attack（视觉标记攻击）的非目标攻击方法，该方法从多个角度构建对抗性示例，目的是全面破坏特征表示和固有关系以及图像编码器输出的视觉标记的语义属性。仅通过访问提议攻击中的图像编码器，生成的对抗性示例展示了跨利用相同图像编码器的不同 LVLMs 的可转移性以及跨不同任务的通用性。广泛的实验验证了 VT-Attack 优于基线方法的卓越攻击性能，证明了其在攻击具有图像编码器的 LVLMs 中的有效性，进而可以为 LVLMs 的鲁棒性提供指导，特别是在视觉特征空间的稳定性方面。

##### **Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization**
2410.06682v1 by Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zujun Ma, Chao Zhang

Videos contain a wealth of information, and generating detailed and accurate
descriptions in natural language is a key aspect of video understanding. In
this paper, we present video-SALMONN 2, an advanced audio-visual large language
model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with
paired audio) captioning through directed preference optimization (DPO). We
propose new metrics to evaluate the completeness and accuracy of video
descriptions, which are optimized using DPO. To further improve training, we
introduce a novel multi-round DPO (mrDPO) approach, which involves periodically
updating the DPO reference model, merging and re-initializing the LoRA module
as a proxy for parameter updates after each training round (1,000 steps), and
incorporating guidance from ground-truth video captions to stabilize the
process. To address potential catastrophic forgetting of non-captioning
abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO
LLM by using the captions generated by the mrDPO-trained model as supervised
labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's
captioning accuracy, reducing global and local error rates by 40\% and 20\%,
respectively, while decreasing the repetition rate by 35\%. The final
video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models
such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining
competitive performance to the state-of-the-art on widely used video
question-answering benchmark among models of similar size. Upon acceptance, we
will release the code, model checkpoints, and training and test data. Demos are
available at
\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}.

摘要：影片包含豐富的資訊，而以自然語言產生詳細且準確的描述是影片理解的重要面向。在這篇論文中，我們提出 video-SALMONN 2，這是一個進階的音訊視覺大型語言模型 (LLM)，具備低階層次適應 (LoRA)，專為透過導向偏好最佳化 (DPO) 進行增強影片（搭配音訊）字幕編寫而設計。我們提出新的指標來評估影片描述的完整性和準確性，並使用 DPO 進行最佳化。為了進一步改善訓練，我們引入一種新穎的多輪 DPO (mrDPO) 方法，其中包括定期更新 DPO 參考模型，在每次訓練輪次（1,000 步驟）後合併並重新初始化 LoRA 模組作為參數更新的代理，並納入真實影片字幕的指導，以穩定此程序。為了解決由於 mrDPO 導致非字幕編寫能力潛在的災難性遺忘，我們提出重生微調，它會使用 mrDPO 訓練模型產生的字幕作為監督標籤，對 DPO 前的 LLM 進行微調。實驗顯示，mrDPO 大幅提升 video-SALMONN 2 的字幕編寫準確度，分別降低了 40% 和 20% 的整體和局部錯誤率，同時將重複率降低了 35%。最終的 video-SALMONN 2 模型僅有 70 億個參數，在影片字幕編寫任務中超越了 GPT-4o 和 Gemini-1.5-Pro 等領先模型，同時在廣泛使用的影片問答基準中，維持與同等規模模型的最新技術競爭力。在獲接受後，我們將釋出程式碼、模型檢查點，以及訓練和測試資料。展示可見於
\href{https://video-salmonn-2.github.io}{https://video-salmonn-2.github.io}。

##### **M${}^{3}$Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes**
2410.06678v1 by Zeyu Zhang, Sixu Yan, Muzhi Han, Zaijin Wang, Xinggang Wang, Song-Chun Zhu, Hangxin Liu

We propose M^3Bench, a new benchmark for whole-body motion generation for
mobile manipulation tasks. Given a 3D scene context, M^3Bench requires an
embodied agent to understand its configuration, environmental constraints and
task objectives, then generate coordinated whole-body motion trajectories for
object rearrangement tasks. M^3Bench features 30k object rearrangement tasks
across 119 diverse scenes, providing expert demonstrations generated by our
newly developed M^3BenchMaker. This automatic data generation tool produces
coordinated whole-body motion trajectories from high-level task instructions,
requiring only basic scene and robot information. Our benchmark incorporates
various task splits to assess generalization across different dimensions and
leverages realistic physics simulation for trajectory evaluation. Through
extensive experimental analyses, we reveal that state-of-the-art models still
struggle with coordinated base-arm motion while adhering to environment-context
and task-specific constraints, highlighting the need to develop new models that
address this gap. Through M^3Bench, we aim to facilitate future robotics
research towards more adaptive and capable mobile manipulation in diverse,
real-world environments.

摘要：我們提出 M^3Bench，一種用於移動操作任務的全方位運動生成的新基準。給定 3D 場景環境，M^3Bench 需要一個具象化代理來了解其組態、環境約束和任務目標，然後為物件重新排列任務生成協調的全方位運動軌跡。M^3Bench 具有 30k 個物件重新排列任務，橫跨 119 個不同的場景，提供由我們新開發的 M^3BenchMaker 生成的專家示範。這個自動資料生成工具從高階任務指令中產生協調的全方位運動軌跡，僅需要基本的場景和機器人資訊。我們的基準納入了各種任務分割，以評估不同面向的概化能力，並利用逼真的物理模擬進行軌跡評估。透過廣泛的實驗分析，我們發現最先進的模型在遵守環境背景和特定任務約束的同時，仍然難以應付協調的基本運動，強調開發新模型以解決此差距的必要性。透過 M^3Bench，我們旨在促進未來的機器人研究，朝向在多樣化的現實世界環境中更具適應性和能力的移動操作。

##### **Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures**
2410.06672v1 by Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu

The hypothesis of Universality in interpretability suggests that different
neural networks may converge to implement similar algorithms on similar tasks.
In this work, we investigate two mainstream architectures for language
modeling, namely Transformers and Mambas, to explore the extent of their
mechanistic similarity. We propose to use Sparse Autoencoders (SAEs) to isolate
interpretable features from these models and show that most features are
similar in these two models. We also validate the correlation between feature
similarity and Universality. We then delve into the circuit-level analysis of
Mamba models and find that the induction circuits in Mamba are structurally
analogous to those in Transformers. We also identify a nuanced difference we
call \emph{Off-by-One motif}: The information of one token is written into the
SSM state in its next position. Whilst interaction between tokens in
Transformers does not exhibit such trend.

摘要：普遍性可解釋性假設表明，不同的神經網路可能會收斂以在類似的任務上實作類似的演算法。在這項工作中，我們研究了語言建模的兩個主流架構，即 Transformer 和 Mamba，以探索它們機制相似性的程度。我們建議使用稀疏自動編碼器 (SAE) 來從這些模型中分離可解釋特徵，並顯示大多數特徵在這兩個模型中都是相似的。我們還驗證了特徵相似性和普遍性之間的相關性。然後，我們深入探討 Mamba 模型的電路級別分析，並發現 Mamba 中的歸納電路在結構上類似於 Transformer 中的電路。我們還發現了一個細微的差異，我們稱之為「差一個基序」：一個記號的資訊會寫入其下一個位置的 SSM 狀態。而 Transformer 中記號之間的互動並未展現出這種趨勢。

##### **Large Language Models as Code Executors: An Exploratory Study**
2410.06667v1 by Chenyang Lyu, Lecheng Yan, Rui Xing, Wenxi Li, Younes Samih, Tianbo Ji, Longyue Wang

The capabilities of Large Language Models (LLMs) have significantly evolved,
extending from natural language processing to complex tasks like code
understanding and generation. We expand the scope of LLMs' capabilities to a
broader context, using LLMs to execute code snippets to obtain the output. This
paper pioneers the exploration of LLMs as code executors, where code snippets
are directly fed to the models for execution, and outputs are returned. We are
the first to comprehensively examine this feasibility across various LLMs,
including OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the
o1 model achieved over 90% accuracy in code execution, while others
demonstrated lower accuracy levels. Furthermore, we introduce an Iterative
Instruction Prompting (IIP) technique that processes code snippets line by
line, enhancing the accuracy of weaker models by an average of 7.22% (with the
highest improvement of 18.96%) and an absolute average improvement of 3.86%
against CoT prompting (with the highest improvement of 19.46%). Our study not
only highlights the transformative potential of LLMs in coding but also lays
the groundwork for future advancements in automated programming and the
completion of complex tasks.

摘要：大型語言模型 (LLM) 的功能已大幅提升，從自然語言處理延伸到複雜任務，例如程式碼理解和產生。我們將 LLM 的功能範圍擴展到更廣泛的脈絡，使用 LLM 執行程式碼片段以取得輸出。本文開創了將 LLM 視為程式碼執行器的探索，其中程式碼片段直接提供給模型執行，並回傳輸出。我們是第一個全面檢視各種 LLM 的可行性，包括 OpenAI 的 o1、GPT-4o、GPT-3.5、DeepSeek 和 Qwen-Coder。值得注意的是，o1 模型在程式碼執行中達到了 90% 以上的準確度，而其他模型則表現出較低的準確度。此外，我們介紹了一種反覆指令提示 (IIP) 技術，逐行處理程式碼片段，將較弱模型的準確度平均提升 7.22%（最高提升 18.96%），與 CoT 提示相比，絕對平均提升 3.86%（最高提升 19.46%）。我們的研究不僅突顯了 LLM 在編碼方面的轉型潛力，也為自動化程式設計和完成複雜任務的未來進展奠定了基礎。

##### **Revisiting Multi-Permutation Equivariance through the Lens of Irreducible Representations**
2410.06665v1 by Yonatan Sverdlov, Ido Springer, Nadav Dym

This paper explores the characterization of equivariant linear layers for
representations of permutations and related groups. Unlike traditional
approaches, which address these problems using parameter-sharing, we consider
an alternative methodology based on irreducible representations and Schur's
lemma. Using this methodology, we obtain an alternative derivation for existing
models like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space
(DWS) networks. The derivation for DWS networks is significantly simpler than
that of previous results.
  Next, we extend our approach to unaligned symmetric sets, where equivariance
to the wreath product of groups is required. Previous works have addressed this
problem in a rather restrictive setting, in which almost all wreath equivariant
layers are Siamese. In contrast, we give a full characterization of layers in
this case and show that there is a vast number of additional non-Siamese layers
in some settings. We also show empirically that these additional non-Siamese
layers can improve performance in tasks like graph anomaly detection, weight
space alignment, and learning Wasserstein distances. Our code is available at
\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.

摘要：這篇論文探討了置換和相關群組表示的等變線性層的特性。與使用參數共享來解決這些問題的傳統方法不同，我們考慮基於不可約表示和 Schur 引理的替代方法。使用此方法，我們獲得了現有模型的替代推導，例如 DeepSets、2-IGN 圖形等變網路和深度權重空間 (DWS) 網路。DWS 網路的推導顯著簡單於先前的結果。
接著，我們將我們的做法延伸到未對齊的對稱集合，其中需要對群的環狀積進行等變。先前的研究已在相當嚴格的設定中解決了這個問題，其中幾乎所有環狀等變層都是連體的。相反，我們在此情況下對層進行了全面表徵，並表明在某些設定中存在大量額外的非連體層。我們還通過實證表明，這些額外的非連體層可以改善圖形異常偵測、權重空間對齊和學習 Wasserstein 距離等任務的效能。我們的程式碼可在 \href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub} 取得。

##### **Decouple-Then-Merge: Towards Better Training for Diffusion Models**
2410.06664v1 by Qianli Ma, Xuefei Ning, Dongrui Liu, Li Niu, Linfeng Zhang

Diffusion models are trained by learning a sequence of models that reverse
each step of noise corruption. Typically, the model parameters are fully shared
across multiple timesteps to enhance training efficiency. However, since the
denoising tasks differ at each timestep, the gradients computed at different
timesteps may conflict, potentially degrading the overall performance of image
generation. To solve this issue, this work proposes a Decouple-then-Merge
(DeMe) framework, which begins with a pretrained model and finetunes separate
models tailored to specific timesteps. We introduce several improved techniques
during the finetuning stage to promote effective knowledge sharing while
minimizing training interference across timesteps. Finally, after finetuning,
these separate models can be merged into a single model in the parameter space,
ensuring efficient and practical inference. Experimental results show
significant generation quality improvements upon 6 benchmarks including Stable
Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN
Bedroom, and CIFAR10.

摘要：擴散模型的訓練方式是學習一系列模型，以逆轉雜訊破壞的每個步驟。通常，模型參數會在多個時間步長中完全共用，以提高訓練效率。然而，由於去噪任務在每個時間步長都不同，因此在不同時間步長計算出的梯度可能會衝突，這可能會降低影像生成的整體效能。為了解決這個問題，這項工作提出了一個先解耦再合併 (DeMe) 架構，它從一個預訓練模型開始，並微調針對特定時間步長量身打造的獨立模型。我們在微調階段引入了多項改良技術，以促進有效的知識共享，同時將跨時間步長的訓練干擾降到最低。最後，在微調後，這些獨立模型可以在參數空間中合併成一個單一模型，確保有效率且實用的推論。實驗結果顯示，在 6 個基準上顯著提升了生成品質，包括 COCO30K 上的 Stable Diffusion、ImageNet1K、PartiPrompts，以及 LSUN Church、LSUN Bedroom 和 CIFAR10 上的 DDPM。

##### **Task-oriented Time Series Imputation Evaluation via Generalized Representers**
2410.06652v1 by Zhixian Wang, Linxiao Yang, Liang Sun, Qingsong Wen, Yi Wang

Time series analysis is widely used in many fields such as power energy,
economics, and transportation, including different tasks such as forecasting,
anomaly detection, classification, etc. Missing values are widely observed in
these tasks, and often leading to unpredictable negative effects on existing
methods, hindering their further application. In response to this situation,
existing time series imputation methods mainly focus on restoring sequences
based on their data characteristics, while ignoring the performance of the
restored sequences in downstream tasks. Considering different requirements of
downstream tasks (e.g., forecasting), this paper proposes an efficient
downstream task-oriented time series imputation evaluation approach. By
combining time series imputation with neural network models used for downstream
tasks, the gain of different imputation strategies on downstream tasks is
estimated without retraining, and the most favorable imputation value for
downstream tasks is given by combining different imputation strategies
according to the estimated gain.

摘要：時序分析廣泛應用於電力能源、經濟、交通等眾多領域，包含預測、異常偵測、分類等不同任務。在這些任務中，缺失值被廣泛地觀察到，且常導致對既有方法無法預測的負面影響，阻礙其進一步的應用。針對此情況，現有的時序補值方法主要著重於根據其資料特性還原序列，而忽略了還原序列在下游任務中的表現。考量到下游任務的不同需求（例如預測），本文提出一個有效率的下游任務導向時序補值評估方法。藉由將時序補值與用於下游任務的神經網路模型相結合，在不重新訓練的情況下估計不同補值策略在下游任務中的增益，並根據估計的增益，結合不同的補值策略給出對下游任務最有利的補值值。

##### **Toward Physics-guided Time Series Embedding**
2410.06651v1 by Jiaxi Hu, Bowen Zhang, Qingsong Wen, Fugee Tsung, Yuxuan Liang

In various scientific and engineering fields, the primary research areas have
revolved around physics-based dynamical systems modeling and data-driven time
series analysis. According to the embedding theory, dynamical systems and time
series can be mutually transformed using observation functions and physical
reconstruction techniques. Based on this, we propose Embedding Duality Theory,
where the parameterized embedding layer essentially provides a linear
estimation of the non-linear time series dynamics. This theory enables us to
bypass the parameterized embedding layer and directly employ physical
reconstruction techniques to acquire a data embedding representation. Utilizing
physical priors results in a 10X reduction in parameters, a 3X increase in
speed, and maximum performance boosts of 18% in expert, 22% in few-shot, and
53\% in zero-shot tasks without any hyper-parameter tuning. All methods are
encapsulated as a plug-and-play module

摘要：在各種科學和工程領域中，主要的研發領域圍繞著基於物理的動態系統建模和資料驅動的時間序列分析。根據嵌入理論，動態系統和時間序列可以使用觀測函數和物理重建技術相互轉換。基於此，我們提出了嵌入對偶理論，其中參數化嵌入層本質上提供了非線性時間序列動態的線性估計。此理論使我們能夠繞過參數化嵌入層，並直接採用物理重建技術來獲取資料嵌入表示。利用物理先驗可將參數減少 10 倍，速度提高 3 倍，且在專家任務中效能提升最高 18%，在少樣本任務中提升 22%，在零樣本任務中提升 53%，且無需任何超參數調整。所有方法都封裝為即插即用的模組

##### **Subtle Errors Matter: Preference Learning via Error-injected Self-editing**
2410.06638v1 by Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li

Large Language Models (LLMs) have exhibited strong mathematical reasoning and
computational prowess, tackling tasks ranging from basic arithmetic to advanced
competition-level problems. However, frequently occurring subtle errors, such
as miscalculations or incorrect substitutions, limit the models' full
mathematical potential. Existing studies to improve mathematical ability
typically involve distilling reasoning skills from stronger LLMs or applying
preference learning to step-wise response pairs. Although these methods
leverage samples of varying granularity to mitigate reasoning errors, they
overlook the frequently occurring subtle errors. A major reason is that sampled
preference pairs involve differences unrelated to the errors, which may
distract the model from focusing on subtle errors. In this work, we propose a
novel preference learning framework called eRror-Injected Self-Editing (RISE),
which injects predefined subtle errors into partial tokens of correct solutions
to construct hard pairs for error mitigation. In detail, RISE uses the model
itself to edit a small number of tokens in the solution, injecting designed
subtle errors. Then, pairs composed of self-edited solutions and their
corresponding correct ones, along with pairs of correct and incorrect solutions
obtained through sampling, are used together for subtle error-aware DPO
training. Compared with other preference learning methods, RISE further refines
the training objective to focus on predefined errors and their tokens, without
requiring fine-grained sampling or preference annotation. Extensive experiments
validate the effectiveness of RISE, with preference learning on
Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on
MATH.

摘要：大型語言模型 (LLM) 展現強大的數學推理能力和計算實力，能處理從基本算術到進階競賽等級問題的各種任務。然而，經常發生的細微錯誤，例如計算錯誤或不正確的替換，限制了模型的完整數學潛力。現有的研究用於提升數學能力通常涉及從更強大的 LLM 中萃取推理技能，或將偏好學習應用於逐步回應配對。儘管這些方法利用不同粒度的範本來減輕推理錯誤，但它們忽略了經常發生的細微錯誤。主要原因是取樣的偏好配對涉及與錯誤無關的差異，這可能會分散模型對細微錯誤的關注。在這項工作中，我們提出一個名為 eRror-Injected Self-Editing (RISE) 的新偏好學習架構，它將預定義的細微錯誤注入正確解答的部分代幣中，以建構難配對來減輕錯誤。詳細來說，RISE 使用模型本身編輯解答中少數的代幣，注入設計的細微錯誤。然後，由自我編輯的解答及其對應的正確解答所組成的配對，連同透過取樣獲得的正確和不正確解答的配對，一起用於具備細微錯誤感知的 DPO 訓練。與其他偏好學習方法相比，RISE 進一步改善訓練目標，專注於預定義的錯誤及其代幣，而不需要細粒度的取樣或偏好註解。廣泛的實驗驗證了 RISE 的有效性，在 Qwen2-7B-Instruct 上的偏好學習產生顯著的進步，在 GSM8K 上提升了 3.0%，在 MATH 上提升了 7.9%。

##### **Tree of Problems: Improving structured problem solving with compositionality**
2410.06634v1 by Armel Zebaze, Benoît Sagot, Rachel Bawden

Large Language Models (LLMs) have demonstrated remarkable performance across
multiple tasks through in-context learning. For complex reasoning tasks that
require step-by-step thinking, Chain-of-Thought (CoT) prompting has given
impressive results, especially when combined with self-consistency.
Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree
of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing
the complex problem into paths of subproblems. In this paper, we propose Tree
of Problems (ToP), a simpler version of ToT, which we hypothesise can work
better for complex tasks that can be divided into identical subtasks. Our
empirical results show that our approach outperforms ToT and GoT, and in
addition performs better than CoT on complex reasoning tasks. All code for this
paper is publicly available here:
https://github.com/ArmelRandy/tree-of-problems.

摘要：大型语言模型 (LLM) 已通过情境学习在多项任务中展示出非凡的性能。对于需要循序渐进思考的复杂推理任务，思维链 (CoT) 提示已取得令人印象深刻的结果，尤其是在与自洽性相结合时。尽管如此，LLM 仍然难以解决某些任务。思维树 (ToT) 和思维图 (GoT) 作为替代方案出现，将复杂问题划分为子问题的路径。在本文中，我们提出了思维树 (ToP)，它是 ToT 的一个更简单的版本，我们假设它可以更好地适用于可以划分为相同子任务的复杂任务。我们的实证结果表明，我们的方法优于 ToT 和 GoT，并且在复杂推理任务上的表现也优于 CoT。本文的所有代码在此公开提供：
https://github.com/ArmelRandy/tree-of-problems。

##### **ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time**
2410.06625v1 by Yi Ding, Bolian Li, Ruqi Zhang

Vision Language Models (VLMs) have become essential backbones for multimodal
intelligence, yet significant safety challenges limit their real-world
application. While textual inputs are often effectively safeguarded,
adversarial visual inputs can easily bypass VLM defense mechanisms. Existing
defense methods are either resource-intensive, requiring substantial data and
compute, or fail to simultaneously ensure safety and usefulness in responses.
To address these limitations, we propose a novel two-phase inference-time
alignment framework, Evaluating Then Aligning (ETA): 1) Evaluating input visual
contents and output responses to establish a robust safety awareness in
multimodal settings, and 2) Aligning unsafe behaviors at both shallow and deep
levels by conditioning the VLMs' generative distribution with an interference
prefix and performing sentence-level best-of-N to search the most harmless and
helpful generation paths. Extensive experiments show that ETA outperforms
baseline methods in terms of harmlessness, helpfulness, and efficiency,
reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6%
win-ties in GPT-4 helpfulness evaluation. The code is publicly available at
https://github.com/DripNowhy/ETA.

摘要：視覺語言模型（VLM）已成為多模態智慧的關鍵骨幹，但嚴重的安全性挑戰限制了它們在現實世界中的應用。雖然文字輸入通常受到有效保障，但對抗性的視覺輸入可以輕易繞過 VLM 防禦機制。現有的防禦方法不是資源密集型（需要大量的數據和運算），就是無法同時確保回應的安全性與實用性。為了解決這些限制，我們提出了一種新穎的兩階段推論時間對齊架構，即先評估再對齊（ETA）：1）評估輸入視覺內容和輸出回應，以在多模態設置中建立穩健的安全意識，以及 2）透過使用干擾前綴調整 VLM 的生成分佈，並執行句子層級的最佳 N 選擇，以搜尋最無害且有用的生成路徑，從而調整淺層和深層的不安全行為。廣泛的實驗顯示，ETA 在無害性、有用性和效率方面優於基線方法，在跨模式攻擊中將不安全率降低了 87.5%，並在 GPT-4 有用性評估中實現了 96.6% 的獲勝平局。程式碼已公開發布於 https://github.com/DripNowhy/ETA。

##### **Effective Exploration Based on the Structural Information Principles**
2410.06621v1 by Xianghua Zeng, Hao Peng, Angsheng Li

Traditional information theory provides a valuable foundation for
Reinforcement Learning, particularly through representation learning and
entropy maximization for agent exploration. However, existing methods primarily
concentrate on modeling the uncertainty associated with RL's random variables,
neglecting the inherent structure within the state and action spaces. In this
paper, we propose a novel Structural Information principles-based Effective
Exploration framework, namely SI2E. Structural mutual information between two
variables is defined to address the single-variable limitation in structural
information, and an innovative embedding principle is presented to capture
dynamics-relevant state-action representations. The SI2E analyzes value
differences in the agent's policy between state-action pairs and minimizes
structural entropy to derive the hierarchical state-action structure, referred
to as the encoding tree. Under this tree structure, value-conditional
structural entropy is defined and maximized to design an intrinsic reward
mechanism that avoids redundant transitions and promotes enhanced coverage in
the state-action space. Theoretical connections are established between SI2E
and classical information-theoretic methodologies, highlighting our framework's
rationality and advantage. Comprehensive evaluations in the MiniGrid,
MetaWorld, and DeepMind Control Suite benchmarks demonstrate that SI2E
significantly outperforms state-of-the-art exploration baselines regarding
final performance and sample efficiency, with maximum improvements of 37.63%
and 60.25%, respectively.

摘要：傳統資訊理論為強化學習提供了有價值的基礎，特別是透過表示學習和代理探索的熵最大化。然而，現有方法主要集中於建立與 RL 隨機變數相關的不確定性模型，忽略了狀態和動作空間中的內在結構。在本文中，我們提出了一個基於結構資訊原則的新型有效探索框架，即 SI2E。定義了兩個變數之間的結構互資訊，以解決結構資訊中的單一變數限制，並提出了一個創新的嵌入原則來擷取與動態相關的狀態動作表示。SI2E 分析了代理策略中狀態動作對之間的價值差異，並最小化結構熵以推導階層式狀態動作結構，稱為編碼樹。在此樹狀結構下，定義並最大化值條件結構熵，以設計一種內在獎勵機制，避免重複的轉換，並促進在狀態動作空間中的擴展覆蓋率。在 SI2E 和經典資訊理論方法之間建立了理論聯繫，突出了我們框架的合理性和優勢。在 MiniGrid、MetaWorld 和 DeepMind Control Suite 基準中的綜合評估表明，SI2E 在最終效能和樣本效率方面顯著優於最先進的探索基準，最大改進分別為 37.63% 和 60.25%。

##### **Learning Evolving Tools for Large Language Models**
2410.06617v1 by Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, Yasheng Wang

Tool learning enables large language models (LLMs) to interact with external
tools and APIs, greatly expanding the application scope of LLMs. However, due
to the dynamic nature of external environments, these tools and APIs may become
outdated over time, preventing LLMs from correctly invoking tools. Existing
research primarily focuses on static environments and overlooks this issue,
limiting the adaptability of LLMs in real-world applications. In this paper, we
propose ToolEVO, a novel framework designed to enhance the adaptive and
reflective capabilities of LLMs against tool variability. By leveraging Monte
Carlo Tree Search, ToolEVO facilitates active exploration and interaction of
LLMs within dynamic environments, allowing for autonomous self-reflection and
self-updating of tool usage based on environmental feedback. Additionally, we
introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of
tool variability. Extensive experiments demonstrate the effectiveness and
stability of our approach, highlighting the importance of adaptability to tool
variability for effective tool learning.

摘要：工具學習讓大型語言模型 (LLM) 能與外部工具和 API 互動，大幅擴展 LLM 的應用範圍。然而，由於外部環境的動態性質，這些工具和 API 可能會隨著時間而過時，導致 LLM 無法正確呼叫工具。現有的研究主要集中在靜態環境，而忽略了這個問題，限制了 LLM 在真實世界應用中的適應性。在本文中，我們提出 ToolEVO，一個新穎的框架，旨在增強 LLM 對工具變異性的自適應和反思能力。透過利用蒙地卡羅樹狀搜尋，ToolEVO 促進 LLM 在動態環境中的主動探索和互動，允許根據環境回饋進行工具使用的自主自我反思和自我更新。此外，我們引入了 ToolQA-D，一個專門設計來評估工具變異性影響的基準。廣泛的實驗證明了我們方法的有效性和穩定性，突顯了適應工具變異性對有效工具學習的重要性。

##### **$β$-calibration of Language Model Confidence Scores for Generative QA**
2410.06615v1 by Putra Manggala, Atalanti Mastakouri, Elke Kirschbaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas

To use generative question-and-answering (QA) systems for decision-making and
in any critical application, these systems need to provide well-calibrated
confidence scores that reflect the correctness of their answers. Existing
calibration methods aim to ensure that the confidence score is on average
indicative of the likelihood that the answer is correct. We argue, however,
that this standard (average-case) notion of calibration is difficult to
interpret for decision-making in generative QA. To address this, we generalize
the standard notion of average calibration and introduce $\beta$-calibration,
which ensures calibration holds across different question-and-answer groups. We
then propose discretized posthoc calibration schemes for achieving
$\beta$-calibration.

摘要：若要將生成式問答 (QA) 系統用於決策制定和任何關鍵應用程式，這些系統需要提供經過良好校準的信心評分，以反映其答案的正確性。現有的校準方法旨在確保信心評分平均表示答案正確的可能性。然而，我們認為，對於生成式 QA 中的決策制定而言，這種標準（平均情況）校準概念難以解釋。為了解決這個問題，我們將標準的平均校準概念加以概括，並引入 $\beta$-校準，以確保校準適用於不同的問答群組。然後，我們提出離散化的後設校準方案，以實現 $\beta$-校準。

##### **Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers**
2410.06614v1 by Stephen Hausler, Peyman Moghadam

In this work we propose a novel joint training method for Visual Place
Recognition (VPR), which simultaneously learns a global descriptor and a pair
classifier for re-ranking. The pair classifier can predict whether a given pair
of images are from the same place or not. The network only comprises Vision
Transformer components for both the encoder and the pair classifier, and both
components are trained using their respective class tokens. In existing VPR
methods, typically the network is initialized using pre-trained weights from a
generic image dataset such as ImageNet. In this work we propose an alternative
pre-training strategy, by using Siamese Masked Image Modelling as a
pre-training task. We propose a Place-aware image sampling procedure from a
collection of large VPR datasets for pre-training our model, to learn visual
features tuned specifically for VPR. By re-using the Mask Image Modelling
encoder and decoder weights in the second stage of training, Pair-VPR can
achieve state-of-the-art VPR performance across five benchmark datasets with a
ViT-B encoder, along with further improvements in localization recall with
larger encoders. The Pair-VPR website is:
https://csiro-robotics.github.io/Pair-VPR.

摘要：在這項工作中，我們提出了一種新的視覺場景識別 (VPR) 聯合訓練方法，它同時學習一個全局描述符和一個用於重新排序的配對分類器。配對分類器可以預測給定的一對影像是否來自同一個場景。網路只包含用於編碼器和配對分類器的視覺轉換器組件，並且兩個組件都使用它們各自的類別代碼進行訓練。在現有的 VPR 方法中，網路通常使用來自通用影像資料集（例如 ImageNet）的預先訓練權重進行初始化。在這項工作中，我們提出了一種替代性的預先訓練策略，使用連體遮罩影像建模作為預先訓練任務。我們提出了一個來自大型 VPR 資料集集合的場景感知影像取樣程序，用於預先訓練我們的模型，以學習專門針對 VPR 調整的視覺特徵。通過在訓練的第二階段重新使用遮罩影像建模編碼器和解碼器權重，Pair-VPR 可以使用 ViT-B 編碼器在五個基準資料集上實現最先進的 VPR 效能，同時隨著編碼器變大，定位召回率進一步提高。Pair-VPR 網站為：
https://csiro-robotics.github.io/Pair-VPR。

##### **Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS**
2410.06608v1 by Onkar Kishor Susladkar, Vishesh Tripathi, Biddwan Ahmed

This research introduces a comprehensive Bahasa text-to-speech (TTS) dataset
and a novel TTS model, EnGen-TTS, designed to enhance the quality and
versatility of synthetic speech in the Bahasa language. The dataset, spanning
\textasciitilde55.0 hours and 52K audio recordings, integrates diverse textual
sources, ensuring linguistic richness. A meticulous recording setup captures
the nuances of Bahasa phonetics, employing professional equipment to ensure
high-fidelity audio samples. Statistical analysis reveals the dataset's scale
and diversity, laying the foundation for model training and evaluation. The
proposed EnGen-TTS model performs better than established baselines, achieving
a Mean Opinion Score (MOS) of 4.45 $\pm$ 0.13. Additionally, our investigation
on real-time factor and model size highlights EnGen-TTS as a compelling choice,
with efficient performance. This research marks a significant advancement in
Bahasa TTS technology, with implications for diverse language applications.
Link to Generated Samples: \url{https://bahasa-harmony-comp.vercel.app/}

摘要：本研究引入了全面的印尼語文字轉語音 (TTS) 資料集和一個新穎的 TTS 模型 EnGen-TTS，旨在提升印尼語合成語音的品質和多功能性。該資料集橫跨約 55.0 小時和 52K 音訊錄音，整合了多樣化的文字來源，確保語言豐富性。細緻的錄音設定捕捉印尼語語音的細微差別，採用專業設備以確保高保真音訊範本。統計分析揭示了資料集的規模和多樣性，為模型訓練和評估奠定基礎。所提出的 EnGen-TTS 模型表現優於既定的基準，達到 4.45 ± 0.13 的平均意見分數 (MOS)。此外，我們對即時因素和模型大小的調查突顯出 EnGen-TTS 是一個引人注目的選擇，具有高效能表現。本研究標誌著印尼語 TTS 技術的重大進步，對多樣化的語言應用具有影響。生成的範例連結：\url{https://bahasa-harmony-comp.vercel.app/}

##### **Dissecting Fine-Tuning Unlearning in Large Language Models**
2410.06606v1 by Yihuai Hong, Yuelin Zou, Lijie Hu, Ziqian Zeng, Di Wang, Haiqin Yang

Fine-tuning-based unlearning methods prevail for preventing targeted harmful,
sensitive, or copyrighted information within large language models while
preserving overall capabilities. However, the true effectiveness of these
methods is unclear. In this paper, we delve into the limitations of
fine-tuning-based unlearning through activation patching and parameter
restoration experiments. Our findings reveal that these methods alter the
model's knowledge retrieval process, rather than genuinely erasing the
problematic knowledge embedded in the model parameters. Furthermore, behavioral
tests demonstrate that the unlearning mechanisms inevitably impact the global
behavior of the models, affecting unrelated knowledge or capabilities. Our work
advocates the development of more resilient unlearning techniques for truly
erasing knowledge. Our code is released at
https://github.com/yihuaihong/Dissecting-FT-Unlearning.

摘要：微調為基礎的遺忘方法普遍用於防止大型語言模型中目標有害、敏感或受版權保護的資訊，同時保留整體能力。然而，這些方法的真正有效性尚不清楚。在本文中，我們深入探討了基於微調的遺忘的限制，透過啟動程式修補和參數還原實驗。我們的發現揭示了這些方法會改變模型的知識擷取過程，而不是真正抹除嵌入在模型參數中的問題知識。此外，行為測試表明，遺忘機制不可避免地會影響模型的整體行為，影響不相關的知識或能力。我們的研究提倡開發更具韌性的遺忘技術，以真正抹除知識。我們的程式碼已發布在 https://github.com/yihuaihong/Dissecting-FT-Unlearning。

##### **Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions**
2410.06577v1 by Zhihao He, Hang Yu, Zi Gong, Shizhan Liu, Jianguo Li, Weiyao Lin

Recent advancements in Transformer-based large language models (LLMs) have
set new standards in natural language processing. However, the classical
softmax attention incurs significant computational costs, leading to a $O(T)$
complexity for per-token generation, where $T$ represents the context length.
This work explores reducing LLMs' complexity while maintaining performance by
introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an
innovative data-dependent tempered selection (DDTS) mechanism within a linear
attention-based, purely recurrent framework, achieving significant accuracy
while drastically reducing the memory usage typically associated with recurrent
models. This method exemplifies semantic compression by maintaining essential
input information with fixed-size hidden states. Building on this, Rodimus$+$
combines Rodimus with the innovative Sliding Window Shared-Key Attention
(SW-SKA) in a hybrid approach, effectively leveraging the complementary
semantic, token, and head compression techniques. Our experiments demonstrate
that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior
downstream performance against models trained on more tokens, including
Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the
accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints
will be available soon.

摘要：<paragraph>Transformer大型語言模型 (LLM) 的最新進展已在自然語言處理中樹立了新標準。然而，傳統的 softmax 注意力會產生大量的運算成本，導致每個 token 產生的複雜度為 $O(T)$, 其中 $T$ 代表脈絡長度。本研究探討了在維持效能的同時降低 LLM 複雜度的可能性，方法是引入 Rodimus 及其增強版本 Rodimus$+$. Rodimus 在純遞迴架構的線性注意力基礎上採用創新的資料相關緩和選擇 (DDTS) 機制，在大幅減少通常與遞迴模型相關的記憶體使用量的情況下，達到了顯著的準確性。此方法透過維持具有固定大小隱藏狀態的基本輸入資訊，來例證語意壓縮。在此基礎上，Rodimus$+$ 將 Rodimus 與創新的滑動視窗共享金鑰注意力 (SW-SKA) 結合在混合方法中，有效地利用了互補的語意、token 和頭部壓縮技術。我們的實驗證明，在 1 兆個 token 上訓練的 Rodimus$+$-1.6B，相較於在更多 token 上訓練的模型（包括 Qwen2-1.5B 和 RWKV6-1.6B）達到了更佳的下游效能，突顯了其重新定義 LLM 中準確性與效率平衡的潛力。模型程式碼和預先訓練的檢查點將很快提供。</paragraph>

##### **Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare**
2410.06566v1 by Pardis Sadat Zahraei, Zahra Shakeri

Biased AI-generated medical advice and misdiagnoses can jeopardize patient
safety, making the integrity of AI in healthcare more critical than ever. As
Large Language Models (LLMs) take on a growing role in medical decision-making,
addressing their biases and enhancing their accuracy is key to delivering safe,
reliable care. This study addresses these challenges head-on by introducing new
resources designed to promote ethical and precise AI in healthcare. We present
two datasets: BiasMD, featuring 6,007 question-answer pairs crafted to evaluate
and mitigate biases in health-related LLM outputs, and DiseaseMatcher, with
32,000 clinical question-answer pairs spanning 700 diseases, aimed at assessing
symptom-based diagnostic accuracy. Using these datasets, we developed the
EthiClinician, a fine-tuned model built on the ChatDoctor framework, which
outperforms GPT-4 in both ethical reasoning and clinical judgment. By exposing
and correcting hidden biases in existing models for healthcare, our work sets a
new benchmark for safer, more reliable patient outcomes.

摘要：有偏見的 AI 生成的醫療建議和誤診會危及患者安全，讓醫療中的 AI 誠信比以往任何時候都更為關鍵。隨著大型語言模型 (LLM) 在醫療決策中扮演越來越重要的角色，解決其偏見並提高其準確性是提供安全、可靠照護的關鍵。本研究透過引進旨在促進醫療中合乎道德且精確的 AI 的新資源，正面解決這些挑戰。我們提供兩個資料集：BiasMD，包含 6,007 個問題解答配對，用於評估和減輕健康相關 LLM 輸出的偏見；以及 DiseaseMatcher，包含 32,000 個臨床問題解答配對，涵蓋 700 種疾病，旨在評估基於症狀的診斷準確性。使用這些資料集，我們開發了 EthiClinician，這是一個建立在 ChatDoctor 架構上的微調模型，在道德推理和臨床判斷方面都優於 GPT-4。透過揭露和修正醫療保健現有模型中隱藏的偏見，我們的研究為更安全、更可靠的患者結果樹立了新的基準。

##### **Efficient and Robust Knowledge Distillation from A Stronger Teacher Based on Correlation Matching**
2410.06561v1 by Wenqi Niu, Yingchao Wang, Guohui Cai, Hanpo Hou

Knowledge Distillation (KD) has emerged as a pivotal technique for neural
network compression and performance enhancement. Most KD methods aim to
transfer dark knowledge from a cumbersome teacher model to a lightweight
student model based on Kullback-Leibler (KL) divergence loss. However, the
student performance improvements achieved through KD exhibit diminishing
marginal returns, where a stronger teacher model does not necessarily lead to a
proportionally stronger student model. To address this issue, we empirically
find that the KL-based KD method may implicitly change the inter-class
relationships learned by the student model, resulting in a more complex and
ambiguous decision boundary, which in turn reduces the model's accuracy and
generalization ability. Therefore, this study argues that the student model
should learn not only the probability values from the teacher's output but also
the relative ranking of classes, and proposes a novel Correlation Matching
Knowledge Distillation (CMKD) method that combines the Pearson and Spearman
correlation coefficients-based KD loss to achieve more efficient and robust
distillation from a stronger teacher model. Moreover, considering that samples
vary in difficulty, CMKD dynamically adjusts the weights of the Pearson-based
loss and Spearman-based loss. CMKD is simple yet practical, and extensive
experiments demonstrate that it can consistently achieve state-of-the-art
performance on CIRAR-100 and ImageNet, and adapts well to various teacher
architectures, sizes, and other KD methods.

摘要：知識萃取 (KD) 已成為神經網路壓縮和效能增強的關鍵技術。大多數 KD 方法的目標是根據 Kullback-Leibler (KL) 差異損失，將龐大教師模型中的深層知識轉移到輕量級學生模型。然而，透過 KD 達成的學生效能提升表現出遞減的邊際報酬，其中較強大的教師模型並不一定會導致成比例地更強大的學生模型。為了解決此問題，我們根據經驗發現，基於 KL 的 KD 方法可能會隱含地改變學生模型所學習的類間關係，導致更複雜且模稜兩可的決策邊界，進而降低模型的準確度和泛化能力。因此，本研究認為學生模型不應只學習教師輸出中的機率值，還應學習類別的相對排名，並提出一個新穎的相關性比對知識萃取 (CMKD) 方法，該方法結合皮爾森和斯皮爾曼相關係數為基礎的 KD 損失，以從更強大的教師模型中達成更有效率且穩健的萃取。此外，考量到樣本難度有別，CMKD 會動態調整基於皮爾森的損失和基於斯皮爾曼的損失的權重。CMKD 既簡單又實用，廣泛的實驗證明它可以在 CIRAR-100 和 ImageNet 上持續達成最先進的效能，並且能很好地適應各種教師架構、大小和其他 KD 方法。

##### **Mitigating Time Discretization Challenges with WeatherODE: A Sandwich Physics-Driven Neural ODE for Weather Forecasting**
2410.06560v1 by Peiyuan Liu, Tian Zhou, Liang Sun, Rong Jin

In the field of weather forecasting, traditional models often grapple with
discretization errors and time-dependent source discrepancies, which limit
their predictive performance. In this paper, we present WeatherODE, a novel
one-stage, physics-driven ordinary differential equation (ODE) model designed
to enhance weather forecasting accuracy. By leveraging wave equation theory and
integrating a time-dependent source model, WeatherODE effectively addresses the
challenges associated with time-discretization error and dynamic atmospheric
processes. Moreover, we design a CNN-ViT-CNN sandwich structure, facilitating
efficient learning dynamics tailored for distinct yet interrelated tasks with
varying optimization biases in advection equation estimation. Through rigorous
experiments, WeatherODE demonstrates superior performance in both global and
regional weather forecasting tasks, outperforming recent state-of-the-art
approaches by significant margins of over 40.0\% and 31.8\% in root mean square
error (RMSE), respectively. The source code is available at
\url{https://github.com/DAMO-DI-ML/WeatherODE}.

摘要：在天氣預測領域，傳統模型經常與離散化誤差和時間相關來源差異作鬥爭，這限制了它們的預測性能。在本文中，我們提出了 WeatherODE，這是一個新穎的一階段、物理驅動的常微分方程 (ODE) 模型，旨在提高天氣預測的準確性。通過利用波動方程理論並整合時間相關來源模型，WeatherODE 有效地應對了與時間離散化誤差和動態大氣過程相關的挑戰。此外，我們設計了一個 CNN-ViT-CNN 桑威治結構，促進了針對具有不同優化偏差的明顯但相互關聯的任務量身定制的有效學習動態，用於平流方程估計。通過嚴謹的實驗，WeatherODE 在全球和區域天氣預測任務中都表現出優異的性能，在均方根誤差 (RMSE) 中分別以 40.0% 和 31.8% 的顯著優勢優於最近的最新技術。源代碼可在 \url{https://github.com/DAMO-DI-ML/WeatherODE} 獲得。

##### **ING-VP: MLLMs cannot Play Easy Vision-based Games Yet**
2410.06555v1 by Haoran Zhang, Hangyu Guo, Shuyue Guo, Meng Cao, Wenhao Huang, Jiaheng Liu, Ge Zhang

As multimodal large language models (MLLMs) continue to demonstrate
increasingly competitive performance across a broad spectrum of tasks, more
intricate and comprehensive benchmarks have been developed to assess these
cutting-edge models. These benchmarks introduce new challenges to core
capabilities such as perception, reasoning, and planning. However, existing
multimodal benchmarks fall short in providing a focused evaluation of
multi-step planning based on spatial relationships in images. To bridge this
gap, we present ING-VP, the first INteractive Game-based Vision Planning
benchmark, specifically designed to evaluate the spatial imagination and
multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games,
encompassing 300 levels, each with 6 unique configurations. A single model
engages in over 60,000 rounds of interaction. The benchmark framework allows
for multiple comparison settings, including image-text vs. text-only inputs,
single-step vs. multi-step reasoning, and with-history vs. without-history
conditions, offering valuable insights into the model's capabilities. We
evaluated numerous state-of-the-art MLLMs, with the highest-performing model,
Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the
anticipated standard. This work aims to provide a specialized evaluation
framework to drive advancements in MLLMs' capacity for complex spatial
reasoning and planning. The code is publicly available at
https://github.com/Thisisus7/ING-VP.git.

摘要：隨著多模態大型語言模型 (MLLM) 持續展現出在廣泛任務中越來越有競爭力的表現，已經開發出更複雜且全面的基準來評估這些尖端模型。這些基準為核心能力（例如感知、推理和規劃）帶來了新的挑戰。然而，現有的多模態基準在提供基於圖像中空間關係的多步驟規劃的重點評估方面做得並不好。為了彌補這個差距，我們提出了 ING-VP，這是第一個基於互動遊戲的視覺規劃基準，專門設計用於評估 MLLM 的空間想像力和多步驟推理能力。ING-VP 特色是 6 個不同的遊戲，包含 300 個等級，每個等級有 6 個獨特配置。一個單一模型參與超過 60,000 輪互動。基準架構允許多種比較設定，包括圖像文字與純文字輸入、單步驟與多步驟推理，以及有歷史記錄與無歷史記錄條件，提供對模型能力的寶貴見解。我們評估了許多最先進的 MLLM，表現最好的模型 Claude-3.5 Sonnet，平均準確度僅達到 3.37%，遠低於預期的標準。這項工作旨在提供一個專業的評估架構，以推動 MLLM 在複雜空間推理和規劃方面的能力進步。程式碼可在 https://github.com/Thisisus7/ING-VP.git 公開取得。

##### **The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models**
2410.06554v1 by Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, Xiaoyu Shen

Reinforcement Learning from Human Feedback significantly enhances Natural
Language Processing by aligning language models with human expectations. A
critical factor in this alignment is the strength of reward models used during
training. This study explores whether stronger reward models invariably lead to
better language models. In this paper, through experiments on relevance,
factuality, and completeness tasks using the QA-FEEDBACK dataset and reward
models based on Longformer, we uncover a surprising paradox: language models
trained with moderately accurate reward models outperform those guided by
highly accurate ones. This challenges the widely held belief that stronger
reward models always lead to better language models, and opens up new avenues
for future research into the key factors driving model performance and how to
choose the most suitable reward models. Code and additional details are
available at
[https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF).

摘要：透過人類回饋的強化學習，能顯著增強自然語言處理，讓語言模型與人類期望保持一致。在此一致性中，一個關鍵的因素是訓練期間使用的獎勵模型強度。本研究探討強度較高的獎勵模型是否一定會產生更好的語言模型。在本文中，透過對 QA-FEEDBACK 資料集和基於 Longformer 的獎勵模型進行相關性、事實性和完整性任務的實驗，我們發現了一個驚人的悖論：使用中等精確度獎勵模型訓練的語言模型，表現優於那些由高精確度獎勵模型引導的語言模型。這挑戰了廣泛存在的信念，即強度較高的獎勵模型總是會產生更好的語言模型，並為未來研究驅動模型效能和如何選擇最合適的獎勵模型的主要因素，開啟了新的途徑。程式碼和其他詳細資訊可在 [https://github.com/EIT-NLP/AccuracyParadox-RLHF](https://github.com/EIT-NLP/AccuracyParadox-RLHF) 取得。

##### **InstantIR: Blind Image Restoration with Instant Generative Reference**
2410.06551v1 by Jen-Yuan Huang, Haofan Wang, Qixun Wang, Xu Bai, Hao Ai, Peng Xing, Jen-Tse Huang

Handling test-time unknown degradation is the major challenge in Blind Image
Restoration (BIR), necessitating high model generalization. An effective
strategy is to incorporate prior knowledge, either from human input or
generative model. In this paper, we introduce Instant-reference Image
Restoration (InstantIR), a novel diffusion-based BIR method which dynamically
adjusts generation condition during inference. We first extract a compact
representation of the input via a pre-trained vision encoder. At each
generation step, this representation is used to decode current diffusion latent
and instantiate it in the generative prior. The degraded image is then encoded
with this reference, providing robust generation condition. We observe the
variance of generative references fluctuate with degradation intensity, which
we further leverage as an indicator for developing a sampling algorithm
adaptive to input quality. Extensive experiments demonstrate InstantIR achieves
state-of-the-art performance and offering outstanding visual quality. Through
modulating generative references with textual description, InstantIR can
restore extreme degradation and additionally feature creative restoration.

摘要：處理測試時間未知的退化是盲圖像修復 (BIR) 中的主要挑戰，需要很高的模型概化能力。一個有效的策略是納入先驗知識，無論是來自人類輸入或生成模型。在本文中，我們介紹了即時參考圖像修復 (InstantIR)，這是一種基於擴散的新型 BIR 方法，可以在推理過程中動態調整生成條件。我們首先通過預訓練的視覺編碼器提取輸入的緊湊表示。在每個生成步驟中，此表示用於解碼當前的擴散潛在，並在生成先驗中實例化它。然後使用此參考對降級的圖像進行編碼，從而提供穩健的生成條件。我們觀察到生成參考的方差隨降解強度而波動，我們進一步利用它作為開發適應輸入質量的採樣演算法的指標。大量的實驗表明，InstantIR 達到了最先進的性能，並提供了出色的視覺品質。通過用文字描述調節生成參考，InstantIR 可以修復極端的退化，並另外提供創意的修復。

##### **Investigating Cost-Efficiency of LLM-Generated Training Data for Conversational Semantic Frame Analysis**
2410.06550v1 by Shiho Matta, Yin Jou Huang, Fei Cheng, Hirokazu Kiyomaru, Yugo Murawaki

Recent studies have demonstrated that few-shot learning allows LLMs to
generate training data for supervised models at a low cost. However, the
quality of LLM-generated data may not entirely match that of human-labeled
data. This raises a crucial question: how should one balance the trade-off
between the higher quality but more expensive human data and the lower quality
yet substantially cheaper LLM-generated data? In this paper, we synthesized
training data for conversational semantic frame analysis using GPT-4 and
examined how to allocate budgets optimally to achieve the best performance. Our
experiments, conducted across various budget levels, reveal that optimal
cost-efficiency is achieved by combining both human and LLM-generated data
across a wide range of budget levels. Notably, as the budget decreases, a
higher proportion of LLM-generated data becomes more preferable.

摘要：近期的研究表明，小样本学习允许 LLM 以低成本为监督模型生成训练数据。然而，LLM 生成的数据质量可能无法完全匹配人工标注的数据。这提出了一个关键问题：如何平衡更高质量但更昂贵的人工数据和质量较低但成本大幅降低的 LLM 生成数据之间的权衡？在本文中，我们使用 GPT-4 合成了会话语义框架分析的训练数据，并研究了如何优化分配预算以实现最佳性能。我们在不同预算级别进行的实验表明，通过在广泛的预算级别范围内结合人工数据和 LLM 生成的数据，可以实现最佳的成本效益。值得注意的是，随着预算的减少，LLM 生成数据的比例更高变得更加可取。

##### **TuringQ: Benchmarking AI Comprehension in Theory of Computation**
2410.06547v1 by Pardis Sadat Zahraei, Ehsaneddin Asgari

We present TuringQ, the first benchmark designed to evaluate the reasoning
capabilities of large language models (LLMs) in the theory of computation.
TuringQ consists of 4,006 undergraduate and graduate-level question-answer
pairs, categorized into four difficulty levels and covering seven core
theoretical areas. We evaluate several open-source LLMs, as well as GPT-4,
using Chain of Thought prompting and expert human assessment. Additionally, we
propose an automated LLM-based evaluation system that demonstrates competitive
accuracy when compared to human evaluation. Fine-tuning a Llama3-8B model on
TuringQ shows measurable improvements in reasoning ability and out-of-domain
tasks such as algebra. TuringQ serves as both a benchmark and a resource for
enhancing LLM performance in complex computational reasoning tasks. Our
analysis offers insights into LLM capabilities and advances in AI comprehension
of theoretical computer science.

摘要：我們展示 TuringQ，這是第一個基準，旨在評估大型語言模型 (LLM) 在計算理論中的推理能力。TuringQ 包含 4,006 個大學部和研究所等級的問答配對，分為四個難度等級，涵蓋七個核心理論領域。我們使用 Thought 提示鏈和專家人類評估來評估幾個開源 LLM，以及 GPT-4。此外，我們提出了一個自動化的 LLM 基礎評估系統，與人類評估相比，它展示出具有競爭力的準確性。在 TuringQ 上微調 Llama3-8B 模型顯示出推理能力和代數等領域任務的可衡量改進。TuringQ 同時作為基準和資源，用於增強 LLM 在複雜計算推理任務中的表現。我們的分析提供了對 LLM 能力的見解，並推動了 AI 對理論計算科學的理解。

