{"2411.09243": {"publish_time": "2024-11-14", "title": "Towards Unified Neural Decoding of Perceived, Spoken and Imagined Speech from EEG Signals", "paper_summary": "Brain signals accompany various information relevant to human actions and\nmental imagery, making them crucial to interpreting and understanding human\nintentions. Brain-computer interface technology leverages this brain activity\nto generate external commands for controlling the environment, offering\ncritical advantages to individuals with paralysis or locked-in syndrome. Within\nthe brain-computer interface domain, brain-to-speech research has gained\nattention, focusing on the direct synthesis of audible speech from brain\nsignals. Most current studies decode speech from brain activity using invasive\ntechniques and emphasize spoken speech data. However, humans express various\nspeech states, and distinguishing these states through non-invasive approaches\nremains a significant yet challenging task. This research investigated the\neffectiveness of deep learning models for non-invasive-based neural signal\ndecoding, with an emphasis on distinguishing between different speech\nparadigms, including perceived, overt, whispered, and imagined speech, across\nmultiple frequency bands. The model utilizing the spatial conventional neural\nnetwork module demonstrated superior performance compared to other models,\nespecially in the gamma band. Additionally, imagined speech in the theta\nfrequency band, where deep learning also showed strong effects, exhibited\nstatistically significant differences compared to the other speech paradigms.", "paper_summary_zh": "\u8166\u4fe1\u865f\u4f34\u96a8\u8457\u8207\u4eba\u985e\u52d5\u4f5c\u548c\u5fc3\u667a\u610f\u8c61\u76f8\u95dc\u7684\u5404\u7a2e\u8cc7\u8a0a\uff0c\u4f7f\u5176\u5c0d\u65bc\u8a6e\u91cb\u548c\u7406\u89e3\u4eba\u985e\u610f\u5716\u81f3\u95dc\u91cd\u8981\u3002\u8166\u96fb\u8166\u4ecb\u9762\u6280\u8853\u5229\u7528\u6b64\u8166\u90e8\u6d3b\u52d5\u4f86\u7522\u751f\u63a7\u5236\u74b0\u5883\u7684\u5916\u90e8\u6307\u4ee4\uff0c\u70ba\u7671\u7613\u6216\u9589\u9396\u75c7\u5019\u7fa4\u7684\u500b\u4eba\u63d0\u4f9b\u95dc\u9375\u512a\u52e2\u3002\u5728\u8166\u96fb\u8166\u4ecb\u9762\u9818\u57df\u4e2d\uff0c\u8166\u8a9e\u97f3\u7814\u7a76\u5099\u53d7\u95dc\u6ce8\uff0c\u91cd\u9ede\u5728\u65bc\u5f9e\u8166\u4fe1\u865f\u76f4\u63a5\u5408\u6210\u53ef\u807d\u7684\u8a9e\u97f3\u3002\u5927\u591a\u6578\u73fe\u884c\u7814\u7a76\u4f7f\u7528\u4fb5\u5165\u5f0f\u6280\u8853\u5f9e\u8166\u90e8\u6d3b\u52d5\u4e2d\u89e3\u78bc\u8a9e\u97f3\uff0c\u4e26\u5f37\u8abf\u53e3\u8aaa\u8a9e\u97f3\u8cc7\u6599\u3002\u7136\u800c\uff0c\u4eba\u985e\u6703\u8868\u9054\u5404\u7a2e\u8a9e\u97f3\u72c0\u614b\uff0c\u800c\u900f\u904e\u975e\u4fb5\u5165\u5f0f\u65b9\u6cd5\u5340\u5206\u9019\u4e9b\u72c0\u614b\u4ecd\u662f\u4e00\u9805\u91cd\u8981\u4e14\u5177\u6709\u6311\u6230\u6027\u7684\u4efb\u52d9\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4e86\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u5c0d\u65bc\u975e\u4fb5\u5165\u5f0f\u795e\u7d93\u4fe1\u865f\u89e3\u78bc\u7684\u6709\u6548\u6027\uff0c\u91cd\u9ede\u5728\u65bc\u5340\u5206\u4e0d\u540c\u7684\u8a9e\u97f3\u7bc4\u4f8b\uff0c\u5305\u62ec\u611f\u77e5\u3001\u986f\u6027\u3001\u8033\u8a9e\u548c\u60f3\u50cf\u7684\u8a9e\u97f3\uff0c\u6a6b\u8de8\u591a\u500b\u983b\u7387\u983b\u6bb5\u3002\u5229\u7528\u7a7a\u9593\u50b3\u7d71\u795e\u7d93\u7db2\u8def\u6a21\u7d44\u7684\u6a21\u578b\u5c55\u73fe\u51fa\u512a\u65bc\u5176\u4ed6\u6a21\u578b\u7684\u6548\u80fd\uff0c\u7279\u5225\u662f\u5728\u4f3d\u99ac\u983b\u6bb5\u3002\u6b64\u5916\uff0c\u5728\u6df1\u5ea6\u5b78\u7fd2\u4e5f\u5c55\u73fe\u51fa\u5f37\u5927\u6548\u679c\u7684\u897f\u5854\u983b\u6bb5\u4e2d\uff0c\u60f3\u50cf\u7684\u8a9e\u97f3\u8207\u5176\u4ed6\u8a9e\u97f3\u7bc4\u4f8b\u76f8\u6bd4\uff0c\u5448\u73fe\u51fa\u5177\u6709\u7d71\u8a08\u986f\u8457\u6027\u7684\u5dee\u7570\u3002", "author": "Jung-Sun Lee et.al.", "authors": "Jung-Sun Lee, Ha-Na Jo, Seo-Hyun Lee", "id": "2411.09243v1", "paper_url": "http://arxiv.org/abs/2411.09243v1", "repo": "null"}}