{"2406.18839": {"publish_time": "2024-06-27", "title": "Disentangling Knowledge-based and Visual Reasoning by Question Decomposition in KB-VQA", "paper_summary": "We study the Knowledge-Based visual question-answering problem, for which\ngiven a question, the models need to ground it into the visual modality to find\nthe answer. Although many recent works use question-dependent captioners to\nverbalize the given image and use Large Language Models to solve the VQA\nproblem, the research results show they are not reasonably performing for\nmulti-hop questions. Our study shows that replacing a complex question with\nseveral simpler questions helps to extract more relevant information from the\nimage and provide a stronger comprehension of it. Moreover, we analyze the\ndecomposed questions to find out the modality of the information that is\nrequired to answer them and use a captioner for the visual questions and LLMs\nas a general knowledge source for the non-visual KB-based questions. Our\nresults demonstrate the positive impact of using simple questions before\nretrieving visual or non-visual information. We have provided results and\nanalysis on three well-known VQA datasets including OKVQA, A-OKVQA, and KRVQA,\nand achieved up to 2% improvement in accuracy.", "paper_summary_zh": "\u6211\u5011\u7814\u7a76\u4e86\u57fa\u65bc\u77e5\u8b58\u7684\u8996\u89ba\u554f\u7b54\u554f\u984c\uff0c\u5c0d\u65bc\u6b64\u554f\u984c\uff0c\u7d66\u5b9a\u4e00\u500b\u554f\u984c\uff0c\u6a21\u578b\u9700\u8981\u5c07\u5176\u57fa\u790e\u5316\u70ba\u8996\u89ba\u6a21\u5f0f\u4ee5\u627e\u51fa\u7b54\u6848\u3002\u5118\u7ba1\u8a31\u591a\u8fd1\u671f\u7814\u7a76\u4f7f\u7528\u4f9d\u8cf4\u554f\u984c\u7684\u6a19\u984c\u8aaa\u660e\u5668\u5c07\u7d66\u5b9a\u7684\u5f71\u50cf\u53e3\u8a9e\u5316\uff0c\u4e26\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4f86\u89e3\u6c7a VQA \u554f\u984c\uff0c\u4f46\u7814\u7a76\u7d50\u679c\u986f\u793a\u5b83\u5011\u5c0d\u65bc\u591a\u8df3\u554f\u984c\u7684\u57f7\u884c\u4e26\u975e\u5408\u7406\u3002\u6211\u5011\u7684\u7814\u7a76\u986f\u793a\uff0c\u5c07\u8907\u96dc\u554f\u984c\u66ff\u63db\u70ba\u5e7e\u500b\u8f03\u7c21\u55ae\u7684\u554f\u984c\u6709\u52a9\u65bc\u5f9e\u5f71\u50cf\u4e2d\u8403\u53d6\u51fa\u66f4\u76f8\u95dc\u7684\u8cc7\u8a0a\uff0c\u4e26\u63d0\u4f9b\u5c0d\u5176\u66f4\u5f37\u7684\u7406\u89e3\u3002\u6b64\u5916\uff0c\u6211\u5011\u5206\u6790\u5206\u89e3\u7684\u554f\u984c\uff0c\u627e\u51fa\u56de\u7b54\u5b83\u5011\u6240\u9700\u7684\u8cc7\u8a0a\u6a21\u5f0f\uff0c\u4e26\u4f7f\u7528\u6a19\u984c\u8aaa\u660e\u5668\u9032\u884c\u8996\u89ba\u554f\u984c\uff0c\u4e26\u5c07 LLM \u4f5c\u70ba\u975e\u8996\u89ba KB-based \u554f\u984c\u7684\u4e00\u822c\u77e5\u8b58\u4f86\u6e90\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\u4e86\u5728\u64f7\u53d6\u8996\u89ba\u6216\u975e\u8996\u89ba\u8cc7\u8a0a\u4e4b\u524d\u4f7f\u7528\u7c21\u55ae\u554f\u984c\u7684\u6b63\u9762\u5f71\u97ff\u3002\u6211\u5011\u5728\u4e09\u500b\u8457\u540d\u7684 VQA \u8cc7\u6599\u96c6\uff0c\u5305\u62ec OKVQA\u3001A-OKVQA \u548c KRVQA\uff0c\u4e0a\u63d0\u4f9b\u4e86\u7d50\u679c\u548c\u5206\u6790\uff0c\u4e26\u5728\u6e96\u78ba\u5ea6\u4e0a\u7372\u5f97\u4e86\u9ad8\u9054 2% \u7684\u63d0\u5347\u3002", "author": "Elham J. Barezi et.al.", "authors": "Elham J. Barezi, Parisa Kordjamshidi", "id": "2406.18839v1", "paper_url": "http://arxiv.org/abs/2406.18839v1", "repo": "null"}}