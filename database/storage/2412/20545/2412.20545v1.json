{"2412.20545": {"publish_time": "2024-12-29", "title": "The Impact of Prompt Programming on Function-Level Code Generation", "paper_summary": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. Despite this, the impact\nof different prompt techniques -- and their combinations -- on code generation\nremains underexplored. In this study, we introduce CodePromptEval, a dataset of\n7072 prompts designed to evaluate five prompt techniques (few-shot, persona,\nchain-of-thought, function signature, list of packages) and their effect on the\ncorrectness, similarity, and quality of complete functions generated by three\nLLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt\ntechniques significantly influence the generated code, combining multiple\ntechniques does not necessarily improve the outcome. Additionally, we observed\na trade-off between correctness and quality when using prompt techniques. Our\ndataset and replication package enable future research on improving\nLLM-generated code and evaluating new prompt techniques.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)  zunehmend von Software-Ingenieuren f\u00fcr die Code-Generierung verwendet. Einschr\u00e4nkungen von LLMs wie irrelevanter oder inkorrekter Code haben jedoch die Notwendigkeit einer prompten Programmierung (oder Prompt-Engineering) verdeutlicht, bei der Ingenieure spezifische Prompt-Techniken (z. B. Gedankenkette oder Eingabe-Ausgabe-Beispiele) anwenden, um den generierten Code zu verbessern. Trotzdem bleibt der Einfluss verschiedener Prompt-Techniken \u2013 und ihrer Kombinationen \u2013 auf die Codegenerierung unerforscht. In dieser Studie stellen wir CodePromptEval vor, einen Datensatz mit 7072 Prompts, die entwickelt wurden, um f\u00fcnf Prompt-Techniken (Few-Shot, Persona, Gedankenkette, Funktionssignatur, Liste von Paketen) und ihre Auswirkungen auf die Korrektheit, \u00c4hnlichkeit und Qualit\u00e4t vollst\u00e4ndiger Funktionen zu bewerten, die von drei LLMs (GPT-4o, Llama3 und Mistral) generiert wurden. Unsere Ergebnisse zeigen, dass bestimmte Prompt-Techniken den generierten Code zwar erheblich beeinflussen, die Kombination mehrerer Techniken jedoch nicht unbedingt das Ergebnis verbessert. Dar\u00fcber hinaus haben wir einen Kompromiss zwischen Korrektheit und Qualit\u00e4t bei der Verwendung von Prompt-Techniken beobachtet. Unser Datensatz und Replikationspaket erm\u00f6glichen zuk\u00fcnftige Forschungen zur Verbesserung von LLM-generiertem Code und zur Bewertung neuer Prompt-Techniken.", "author": "Ranim Khojah et.al.", "authors": "Ranim Khojah, Francisco Gomes de Oliveira Neto, Mazen Mohamad, Philipp Leitner", "id": "2412.20545v1", "paper_url": "http://arxiv.org/abs/2412.20545v1", "repo": "https://github.com/icetlab/codeprompteval"}}