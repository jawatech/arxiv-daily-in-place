{"2410.01627": {"publish_time": "2024-10-02", "title": "Intent Detection in the Age of LLMs", "paper_summary": "Intent detection is a critical component of task-oriented dialogue systems\n(TODS) which enables the identification of suitable actions to address user\nutterances at each dialog turn. Traditional approaches relied on\ncomputationally efficient supervised sentence transformer encoder models, which\nrequire substantial training data and struggle with out-of-scope (OOS)\ndetection. The emergence of generative large language models (LLMs) with\nintrinsic world knowledge presents new opportunities to address these\nchallenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context\nlearning and chain-of-thought prompting for intent detection, and compare their\nperformance with contrastively fine-tuned sentence transformer (SetFit) models\nto highlight prediction quality and latency tradeoff. We propose a hybrid\nsystem using uncertainty based routing strategy to combine the two approaches\nthat along with negative data augmentation results in achieving the best of\nboth worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To\nbetter understand LLM OOS detection capabilities, we perform controlled\nexperiments revealing that this capability is significantly influenced by the\nscope of intent labels and the size of the label space. We also introduce a\ntwo-step approach utilizing internal LLM representations, demonstrating\nempirical gains in OOS detection accuracy and F1-score by >5% for the\nMistral-7B model.", "paper_summary_zh": "\u610f\u5716\u5075\u6e2c\u662f\u4efb\u52d9\u5c0e\u5411\u5c0d\u8a71\u7cfb\u7d71 (TODS) \u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\uff0c\u5b83\u80fd\u5920\u8b58\u5225\u9069\u7576\u7684\u52d5\u4f5c\u4f86\u8655\u7406\u4f7f\u7528\u8005\u5728\u6bcf\u500b\u5c0d\u8a71\u56de\u5408\u4e2d\u7684\u8a71\u8a9e\u3002\u50b3\u7d71\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u8a08\u7b97\u6548\u7387\u9ad8\u7684\u76e3\u7763\u5f0f\u53e5\u5b50\u8f49\u63db\u5668\u7de8\u78bc\u5668\u6a21\u578b\uff0c\u9019\u9700\u8981\u5927\u91cf\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u4e26\u96e3\u4ee5\u8655\u7406\u8d85\u51fa\u7bc4\u570d (OOS) \u7684\u5075\u6e2c\u3002\u5177\u6709\u5167\u5728\u4e16\u754c\u77e5\u8b58\u7684\u751f\u6210\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\u70ba\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\u63d0\u4f9b\u4e86\u65b0\u7684\u6a5f\u6703\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u9069\u61c9\u6027\u60c5\u5883\u5167\u5b78\u7fd2\u548c\u601d\u8003\u93c8\u63d0\u793a\u4f86\u8abf\u6574 7 \u500b SOTA LLM \u4ee5\u9032\u884c\u610f\u5716\u5075\u6e2c\uff0c\u4e26\u5c07\u5b83\u5011\u7684\u6548\u80fd\u8207\u5c0d\u6bd4\u5fae\u8abf\u7684\u53e5\u5b50\u8f49\u63db\u5668 (SetFit) \u6a21\u578b\u9032\u884c\u6bd4\u8f03\uff0c\u4ee5\u7a81\u986f\u9810\u6e2c\u54c1\u8cea\u548c\u5ef6\u9072\u6b0a\u8861\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u4f7f\u7528\u57fa\u65bc\u4e0d\u78ba\u5b9a\u6027\u7684\u8def\u7531\u7b56\u7565\u7684\u6df7\u5408\u7cfb\u7d71\uff0c\u4ee5\u7d50\u5408\u9019\u5169\u7a2e\u65b9\u6cd5\uff0c\u52a0\u4e0a\u8ca0\u9762\u8cc7\u6599\u64f4\u5145\uff0c\u53ef\u4ee5\u5be6\u73fe\u5169\u5168\u5176\u7f8e\uff08\u5373\u5728\u5ef6\u9072\u6e1b\u5c11 50% \u7684\u60c5\u6cc1\u4e0b\uff0c\u9054\u5230\u539f\u751f LLM \u7cbe\u78ba\u5ea6\u7684 2% \u4ee5\u5167\uff09\u3002\u70ba\u4e86\u66f4\u597d\u5730\u4e86\u89e3 LLM OOS \u5075\u6e2c\u529f\u80fd\uff0c\u6211\u5011\u9032\u884c\u4e86\u53d7\u63a7\u5be6\u9a57\uff0c\u767c\u73fe\u9019\u7a2e\u529f\u80fd\u53d7\u5230\u610f\u5716\u6a19\u7c64\u7684\u7bc4\u570d\u548c\u6a19\u7c64\u7a7a\u9593\u5927\u5c0f\u7684\u986f\u8457\u5f71\u97ff\u3002\u6211\u5011\u9084\u5f15\u5165\u4e86\u4e00\u500b\u5229\u7528\u5167\u90e8 LLM \u8868\u793a\u7684\u5169\u6b65\u9a5f\u65b9\u6cd5\uff0c\u8b49\u660e\u4e86 Mistral-7B \u6a21\u578b\u5728 OOS \u5075\u6e2c\u6e96\u78ba\u5ea6\u548c F1 \u5206\u6578\u65b9\u9762\u7372\u5f97\u4e86 >5% \u7684\u7d93\u9a57\u6536\u76ca\u3002", "author": "Gaurav Arora et.al.", "authors": "Gaurav Arora, Shreya Jain, Srujana Merugu", "id": "2410.01627v1", "paper_url": "http://arxiv.org/abs/2410.01627v1", "repo": "null"}}