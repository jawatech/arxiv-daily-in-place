{"2405.19086": {"publish_time": "2024-05-29", "title": "MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors", "paper_summary": "Model editing aims to efficiently alter the behavior of Large Language Models\n(LLMs) within a desired scope, while ensuring no adverse impact on other\ninputs. Recent years have witnessed various model editing methods been\nproposed. However, these methods either exhibit poor overall performance or\nstruggle to strike a balance between generalization and locality. We propose\nMOMoE, a model editing adapter utilizing a Mixture of Experts (MoE)\narchitecture with a knowledge anchor routing strategy. MOMoE updates knowledge\nusing a bypass MoE structure, keeping the original parameters unchanged to\npreserve the general ability of LLMs. And, the knowledge anchor routing ensures\nthat inputs requiring similar knowledge are routed to the same expert, thereby\nenhancing the generalization of the updated knowledge. Experimental results\nshow the superiority of our approach over both batch editing and sequential\nbatch editing tasks, exhibiting exceptional overall performance alongside\noutstanding balance between generalization and locality. Our code will be\navailable.", "paper_summary_zh": "\u6a21\u578b\u7de8\u8f2f\u65e8\u5728\u6709\u6548\u5730\u6539\u8b8a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6240\u9700\u7bc4\u570d\u5167\u7684\u884c\u70ba\uff0c\u540c\u6642\u78ba\u4fdd\u4e0d\u6703\u5c0d\u5176\u4ed6\u8f38\u5165\u7522\u751f\u4e0d\u5229\u5f71\u97ff\u3002\u8fd1\u5e74\u4f86\u898b\u8b49\u4e86\u5404\u7a2e\u6a21\u578b\u7de8\u8f2f\u65b9\u6cd5\u7684\u63d0\u51fa\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u8981\u4e48\u8868\u73fe\u51fa\u6574\u9ad4\u6027\u80fd\u4e0d\u4f73\uff0c\u8981\u4e48\u96e3\u4ee5\u5728\u6cdb\u5316\u548c\u5c40\u90e8\u6027\u4e4b\u9593\u53d6\u5f97\u5e73\u8861\u3002\u6211\u5011\u63d0\u51fa MOMoE\uff0c\u4e00\u7a2e\u5229\u7528\u5c08\u5bb6\u6df7\u5408 (MoE) \u67b6\u69cb\u548c\u77e5\u8b58\u9328\u5b9a\u8def\u7531\u7b56\u7565\u7684\u6a21\u578b\u7de8\u8f2f\u9069\u914d\u5668\u3002MOMoE \u4f7f\u7528\u65c1\u8def MoE \u7d50\u69cb\u66f4\u65b0\u77e5\u8b58\uff0c\u4fdd\u6301\u539f\u59cb\u53c3\u6578\u4e0d\u8b8a\u4ee5\u4fdd\u7559 LLM \u7684\u4e00\u822c\u80fd\u529b\u3002\u800c\u4e14\uff0c\u77e5\u8b58\u9328\u5b9a\u8def\u7531\u78ba\u4fdd\u9700\u8981\u985e\u4f3c\u77e5\u8b58\u7684\u8f38\u5165\u88ab\u8def\u7531\u5230\u540c\u4e00\u500b\u5c08\u5bb6\uff0c\u5f9e\u800c\u589e\u5f37\u66f4\u65b0\u77e5\u8b58\u7684\u6cdb\u5316\u6027\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u512a\u65bc\u6279\u6b21\u7de8\u8f2f\u548c\u9806\u5e8f\u6279\u6b21\u7de8\u8f2f\u4efb\u52d9\uff0c\u8868\u73fe\u51fa\u5353\u8d8a\u7684\u6574\u9ad4\u6027\u80fd\u4ee5\u53ca\u6cdb\u5316\u548c\u5c40\u90e8\u6027\u4e4b\u9593\u7684\u51fa\u8272\u5e73\u8861\u3002\u6211\u5011\u7684\u4ee3\u78bc\u5c07\u6703\u63d0\u4f9b\u3002", "author": "Renzhi Wang et.al.", "authors": "Renzhi Wang, Piji Li", "id": "2405.19086v1", "paper_url": "http://arxiv.org/abs/2405.19086v1", "repo": "null"}}