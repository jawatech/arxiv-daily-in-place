{"2412.11952": {"publish_time": "2024-12-16", "title": "Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided Self-Supervised Learning", "paper_summary": "Image Aesthetic Assessment (IAA) is a vital and intricate task that entails\nanalyzing and assessing an image's aesthetic values, and identifying its\nhighlights and areas for improvement. Traditional methods of IAA often\nconcentrate on a single aesthetic task and suffer from inadequate labeled\ndatasets, thus impairing in-depth aesthetic comprehension. Despite efforts to\novercome this challenge through the application of Multi-modal Large Language\nModels (MLLMs), such models remain underdeveloped for IAA purposes. To address\nthis, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic\ninsight. Central to our approach is an innovative multi-scale text-guided\nself-supervised learning technique. This technique features a multi-scale\nfeature alignment module and capitalizes on a wealth of unlabeled data in a\nself-supervised manner to structurally and functionally enhance aesthetic\nability. The empirical evidence indicates that accompanied with extensive\ninstruct-tuning, our model sets new state-of-the-art benchmarks across multiple\ntasks, including aesthetic scoring, aesthetic commenting, and personalized\nimage aesthetic assessment. Remarkably, it also demonstrates zero-shot learning\ncapabilities in the emerging task of aesthetic suggesting. Furthermore, for\npersonalized image aesthetic assessment, we harness the potential of in-context\nlearning and showcase its inherent advantages.", "paper_summary_zh": "\u5f71\u50cf\u7f8e\u5b78\u8a55\u4f30 (IAA) \u662f\u4e00\u9805\u91cd\u8981\u4e14\u8907\u96dc\u7684\u4efb\u52d9\uff0c\u9700\u8981\u5206\u6790\u548c\u8a55\u4f30\u5f71\u50cf\u7684\u7f8e\u5b78\u50f9\u503c\uff0c\u4e26\u627e\u51fa\u5176\u4eae\u9ede\u548c\u9700\u8981\u6539\u9032\u7684\u5730\u65b9\u3002\u50b3\u7d71\u7684 IAA \u65b9\u6cd5\u901a\u5e38\u5c08\u6ce8\u65bc\u55ae\u4e00\u7f8e\u5b78\u4efb\u52d9\uff0c\u4e14\u53d7\u9650\u65bc\u6a19\u8a3b\u8cc7\u6599\u96c6\u4e0d\u8db3\uff0c\u56e0\u6b64\u5f71\u97ff\u6df1\u5165\u7684\u7f8e\u5b78\u7406\u89e3\u3002\u5118\u7ba1\u900f\u904e\u61c9\u7528\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4f86\u514b\u670d\u9019\u9805\u6311\u6230\uff0c\u4f46\u6b64\u985e\u6a21\u578b\u5728 IAA \u7528\u9014\u4e0a\u4ecd\u672a\u767c\u5c55\u6210\u719f\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5168\u9762\u7684\u7f8e\u5b78 MLLM\uff0c\u5177\u5099\u7d30\u7dfb\u5165\u5fae\u7684\u7f8e\u5b78\u898b\u89e3\u3002\u6211\u5011\u7684\u65b9\u6cd5\u6838\u5fc3\u662f\u4e00\u500b\u5275\u65b0\u7684\u591a\u5c3a\u5ea6\u6587\u5b57\u5f15\u5c0e\u5f0f\u81ea\u76e3\u7763\u5b78\u7fd2\u6280\u8853\u3002\u6b64\u6280\u8853\u5177\u5099\u591a\u5c3a\u5ea6\u7279\u5fb5\u5c0d\u9f4a\u6a21\u7d44\uff0c\u4e26\u5229\u7528\u5927\u91cf\u672a\u6a19\u8a3b\u8cc7\u6599\u4ee5\u81ea\u76e3\u7763\u7684\u65b9\u5f0f\uff0c\u5728\u7d50\u69cb\u548c\u529f\u80fd\u4e0a\u63d0\u5347\u7f8e\u5b78\u80fd\u529b\u3002\u5be6\u8b49\u8b49\u64da\u986f\u793a\uff0c\u5728\u5ee3\u6cdb\u7684\u6307\u4ee4\u5fae\u8abf\u4e0b\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u591a\u9805\u4efb\u52d9\u4e2d\u8a2d\u5b9a\u65b0\u7684\u6700\u5148\u9032\u57fa\u6e96\uff0c\u5305\u62ec\u7f8e\u5b78\u8a55\u5206\u3001\u7f8e\u5b78\u8a55\u8ad6\u548c\u500b\u4eba\u5316\u5f71\u50cf\u7f8e\u5b78\u8a55\u4f30\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u9084\u5728\u7f8e\u5b78\u5efa\u8b70\u7684\u65b0\u8208\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5c0d\u65bc\u500b\u4eba\u5316\u5f71\u50cf\u7f8e\u5b78\u8a55\u4f30\uff0c\u6211\u5011\u5229\u7528\u60c5\u5883\u5b78\u7fd2\u7684\u6f5b\u529b\uff0c\u4e26\u5c55\u793a\u5176\u56fa\u6709\u512a\u52e2\u3002", "author": "Yuti Liu et.al.", "authors": "Yuti Liu, Shice Liu, Junyuan Gao, Pengtao Jiang, Hao Zhang, Jinwei Chen, Bo Li", "id": "2412.11952v1", "paper_url": "http://arxiv.org/abs/2412.11952v1", "repo": "null"}}