{"2411.05195": {"publish_time": "2024-11-07", "title": "On Erroneous Agreements of CLIP Image Embeddings", "paper_summary": "Recent research suggests that the failures of Vision-Language Models (VLMs)\nat visual reasoning often stem from erroneous agreements -- when semantically\ndistinct images are ambiguously encoded by the CLIP image encoder into\nembeddings with high cosine similarity. In this paper, we show that erroneous\nagreements are not always the main culprit, as Multimodal Large Language Models\n(MLLMs) can still extract distinct information from them. For instance, when\ndistinguishing objects on the left vs right in the What'sUp benchmark, the CLIP\nimage embeddings of the left/right pairs have an average cosine similarity\n$>0.99$, and CLIP performs at random chance; but LLaVA-1.5-7B, which uses the\nsame CLIP image encoder, achieves nearly $100\\%$ accuracy. We find that the\nextractable information in CLIP image embeddings is likely obscured by CLIP's\ninadequate vision-language alignment: Its matching score learned by the\ncontrastive objective might not capture all diverse image-text correspondences.\nWe also study the MMVP benchmark, on which prior work has shown that LLaVA-1.5\ncannot distinguish image pairs with high cosine similarity. We observe a\nperformance gain brought by attending more to visual input through an\nalternative decoding algorithm. Further, the accuracy significantly increases\nif the model can take both images as input to emphasize their nuanced\ndifferences. Both findings indicate that LLaVA-1.5 did not utilize extracted\nvisual information sufficiently. In conclusion, our findings suggest that while\nimproving image encoders could benefit VLMs, there is still room to enhance\nmodels with a fixed image encoder by applying better strategies for extracting\nand utilizing visual information.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u5931\u8d25\u901a\u5e38\u6e90\u4e8e\u9519\u8bef\u7684\u534f\u8bae\u2014\u2014\u5f53\u8bed\u4e49\u4e0a\u4e0d\u540c\u7684\u56fe\u50cf\u88ab CLIP \u56fe\u50cf\u7f16\u7801\u5668\u6a21\u7cca\u5730\u7f16\u7801\u5230\u5177\u6709\u9ad8\u4f59\u5f26\u76f8\u4f3c\u6027\u7684\u5d4c\u5165\u4e2d\u65f6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8868\u660e\u9519\u8bef\u7684\u534f\u8bae\u5e76\u4e0d\u603b\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u56e0\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM) \u4ecd\u7136\u53ef\u4ee5\u4ece\u4e2d\u63d0\u53d6\u4e0d\u540c\u7684\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5728 What'sUp \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u533a\u5206\u5de6\u53f3\u7269\u4f53\u65f6\uff0c\u5de6\u53f3\u5bf9\u7684 CLIP \u56fe\u50cf\u5d4c\u5165\u7684\u5e73\u5747\u4f59\u5f26\u76f8\u4f3c\u6027$>0.99$\uff0c\u800c CLIP \u4ee5\u968f\u673a\u673a\u4f1a\u6267\u884c\uff1b\u4f46\u4f7f\u7528\u76f8\u540c CLIP \u56fe\u50cf\u7f16\u7801\u5668\u7684 LLaVA-1.5-7B \u5b9e\u73b0\u4e86\u63a5\u8fd1 $100\\%$ \u7684\u51c6\u786e\u7387\u3002\u6211\u4eec\u53d1\u73b0 CLIP \u56fe\u50cf\u5d4c\u5165\u4e2d\u53ef\u63d0\u53d6\u7684\u4fe1\u606f\u5f88\u53ef\u80fd\u88ab CLIP \u4e0d\u5145\u5206\u7684\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u6240\u63a9\u76d6\uff1a\u5b83\u901a\u8fc7\u5bf9\u6bd4\u76ee\u6807\u5b66\u4e60\u7684\u5339\u914d\u5206\u6570\u53ef\u80fd\u65e0\u6cd5\u6355\u83b7\u6240\u6709\u4e0d\u540c\u7684\u56fe\u50cf\u6587\u672c\u5bf9\u5e94\u5173\u7cfb\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86 MMVP \u57fa\u51c6\uff0c\u4e4b\u524d\u7684\u5de5\u4f5c\u8868\u660e LLaVA-1.5 \u65e0\u6cd5\u533a\u5206\u5177\u6709\u9ad8\u4f59\u5f26\u76f8\u4f3c\u6027\u7684\u56fe\u50cf\u5bf9\u3002\u6211\u4eec\u89c2\u5bdf\u5230\u901a\u8fc7\u66ff\u4ee3\u89e3\u7801\u7b97\u6cd5\u66f4\u591a\u5730\u5173\u6ce8\u89c6\u89c9\u8f93\u5165\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u5982\u679c\u6a21\u578b\u53ef\u4ee5\u5c06\u4e24\u5e45\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u4ee5\u5f3a\u8c03\u5176\u7ec6\u5fae\u5dee\u522b\uff0c\u5219\u51c6\u786e\u6027\u4f1a\u663e\u7740\u63d0\u9ad8\u3002\u8fd9\u4e24\u9879\u53d1\u73b0\u90fd\u8868\u660e LLaVA-1.5 \u6ca1\u6709\u5145\u5206\u5229\u7528\u63d0\u53d6\u7684\u89c6\u89c9\u4fe1\u606f\u3002\u603b\u4e4b\uff0c\u6211\u4eec\u7684\u53d1\u73b0\u8868\u660e\uff0c\u867d\u7136\u6539\u8fdb\u56fe\u50cf\u7f16\u7801\u5668\u53ef\u4ee5\u4f7f VLM \u53d7\u76ca\uff0c\u4f46\u4ecd\u6709\u7a7a\u95f4\u901a\u8fc7\u5e94\u7528\u66f4\u597d\u7684\u7b56\u7565\u6765\u63d0\u53d6\u548c\u5229\u7528\u89c6\u89c9\u4fe1\u606f\u6765\u589e\u5f3a\u5177\u6709\u56fa\u5b9a\u56fe\u50cf\u7f16\u7801\u5668\u7684\u6a21\u578b\u3002", "author": "Siting Li et.al.", "authors": "Siting Li, Pang Wei Koh, Simon Shaolei Du", "id": "2411.05195v1", "paper_url": "http://arxiv.org/abs/2411.05195v1", "repo": "null"}}