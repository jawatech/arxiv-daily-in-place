{"2406.02301": {"publish_time": "2024-06-04", "title": "mCoT: Multilingual Instruction Tuning for Reasoning Consistency in Language Models", "paper_summary": "Large language models (LLMs) with Chain-of-thought (CoT) have recently\nemerged as a powerful technique for eliciting reasoning to improve various\ndownstream tasks. As most research mainly focuses on English, with few\nexplorations in a multilingual context, the question of how reliable this\nreasoning capability is in different languages is still open. To address it\ndirectly, we study multilingual reasoning consistency across multiple\nlanguages, using popular open-source LLMs. First, we compile the first\nlarge-scale multilingual math reasoning dataset, mCoT-MATH, covering eleven\ndiverse languages. Then, we introduce multilingual CoT instruction tuning to\nboost reasoning capability across languages, thereby improving model\nconsistency. While existing LLMs show substantial variation across the\nlanguages we consider, and especially low performance for lesser resourced\nlanguages, our 7B parameter model mCoT achieves impressive consistency across\nlanguages, and superior or comparable performance to close- and open-source\nmodels even of much larger sizes.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u642d\u914d\u601d\u8003\u93c8 (CoT) \u6700\u8fd1\u5df2\u6210\u70ba\u5f15\u767c\u63a8\u7406\u4ee5\u6539\u5584\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u7684\u5f37\u5927\u6280\u8853\u3002\u7531\u65bc\u5927\u591a\u6578\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8a9e\uff0c\u4e14\u5728\u591a\u8a9e\u8a00\u74b0\u5883\u4e2d\u63a2\u7d22\u8f03\u5c11\uff0c\u56e0\u6b64\u5728\u4e0d\u540c\u8a9e\u8a00\u4e2d\u9019\u7a2e\u63a8\u7406\u80fd\u529b\u7684\u53ef\u9760\u6027\u554f\u984c\u4ecd\u672a\u89e3\u6c7a\u3002\u70ba\u4e86\u76f4\u63a5\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u4f7f\u7528\u6d41\u884c\u7684\u958b\u653e\u539f\u59cb\u78bc LLM \u7814\u7a76\u591a\u8a9e\u8a00\u63a8\u7406\u4e00\u81f4\u6027\u3002\u9996\u5148\uff0c\u6211\u5011\u7de8\u8b6f\u7b2c\u4e00\u500b\u5927\u578b\u591a\u8a9e\u8a00\u6578\u5b78\u63a8\u7406\u8cc7\u6599\u96c6 mCoT-MATH\uff0c\u6db5\u84cb 11 \u7a2e\u4e0d\u540c\u7684\u8a9e\u8a00\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5f15\u5165\u591a\u8a9e\u8a00 CoT \u6307\u4ee4\u8abf\u6574\uff0c\u4ee5\u63d0\u5347\u8de8\u8a9e\u8a00\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9032\u800c\u6539\u5584\u6a21\u578b\u4e00\u81f4\u6027\u3002\u96d6\u7136\u73fe\u6709\u7684 LLM \u5728\u6211\u5011\u8003\u616e\u7684\u8a9e\u8a00\u4e2d\u986f\u793a\u51fa\u986f\u8457\u7684\u5dee\u7570\uff0c\u4e26\u4e14\u8cc7\u6e90\u8f03\u5c11\u7684\u8a9e\u8a00\u8868\u73fe\u7279\u5225\u4f4e\u843d\uff0c\u4f46\u6211\u5011\u7684 7B \u53c3\u6578\u6a21\u578b mCoT \u5728\u4e0d\u540c\u8a9e\u8a00\u4e2d\u9054\u5230\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u4e00\u81f4\u6027\uff0c\u4e26\u4e14\u8868\u73fe\u512a\u65bc\u6216\u7b49\u65bc\u751a\u81f3\u5927\u5f97\u591a\u7684\u9589\u6e90\u548c\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\u3002", "author": "Huiyuan Lai et.al.", "authors": "Huiyuan Lai, Malvina Nissim", "id": "2406.02301v1", "paper_url": "http://arxiv.org/abs/2406.02301v1", "repo": "null"}}