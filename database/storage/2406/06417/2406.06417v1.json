{"2406.06417": {"publish_time": "2024-06-10", "title": "Explainable Graph Neural Networks Under Fire", "paper_summary": "Predictions made by graph neural networks (GNNs) usually lack\ninterpretability due to their complex computational behavior and the abstract\nnature of graphs. In an attempt to tackle this, many GNN explanation methods\nhave emerged. Their goal is to explain a model's predictions and thereby obtain\ntrust when GNN models are deployed in decision critical applications. Most GNN\nexplanation methods work in a post-hoc manner and provide explanations in the\nform of a small subset of important edges and/or nodes. In this paper we\ndemonstrate that these explanations can unfortunately not be trusted, as common\nGNN explanation methods turn out to be highly susceptible to adversarial\nperturbations. That is, even small perturbations of the original graph\nstructure that preserve the model's predictions may yield drastically different\nexplanations. This calls into question the trustworthiness and practical\nutility of post-hoc explanation methods for GNNs. To be able to attack GNN\nexplanation models, we devise a novel attack method dubbed \\textit{GXAttack},\nthe first \\textit{optimization-based} adversarial attack method for post-hoc\nGNN explanations under such settings. Due to the devastating effectiveness of\nour attack, we call for an adversarial evaluation of future GNN explainers to\ndemonstrate their robustness.", "paper_summary_zh": "\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u6240\u505a\u7684\u9810\u6e2c\u901a\u5e38\u7f3a\u4e4f\u53ef\u89e3\u91cb\u6027\uff0c\u9019\u662f\u56e0\u70ba\u5b83\u5011\u7684\u904b\u7b97\u884c\u70ba\u5f88\u8907\u96dc\uff0c\u800c\u4e14\u5716\u5f62\u7684\u6027\u8cea\u5f88\u62bd\u8c61\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u5df2\u7d93\u51fa\u73fe\u4e86\u8a31\u591a GNN \u89e3\u91cb\u65b9\u6cd5\u3002\u5b83\u5011\u7684\u76ee\u6a19\u662f\u89e3\u91cb\u6a21\u578b\u7684\u9810\u6e2c\uff0c\u5f9e\u800c\u7576 GNN \u6a21\u578b\u90e8\u7f72\u5728\u6c7a\u7b56\u95dc\u9375\u61c9\u7528\u7a0b\u5f0f\u4e2d\u6642\uff0c\u53ef\u4ee5\u7372\u5f97\u4fe1\u4efb\u3002\u5927\u591a\u6578 GNN \u89e3\u91cb\u65b9\u6cd5\u90fd\u662f\u4e8b\u5f8c\u904b\u4f5c\uff0c\u4e26\u4ee5\u4e00\u5c0f\u90e8\u5206\u91cd\u8981\u908a\u7de3\u548c/\u6216\u7bc0\u9ede\u7684\u5f62\u5f0f\u63d0\u4f9b\u89e3\u91cb\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u4e86\u9019\u4e9b\u89e3\u91cb\u4e0d\u5e78\u5730\u4e0d\u53ef\u9760\uff0c\u56e0\u70ba\u5e38\u898b\u7684 GNN \u89e3\u91cb\u65b9\u6cd5\u5f88\u5bb9\u6613\u53d7\u5230\u5c0d\u6297\u6027\u64fe\u52d5\u7684\u5f71\u97ff\u3002\u4e5f\u5c31\u662f\u8aaa\uff0c\u5373\u4f7f\u662f\u4fdd\u7559\u6a21\u578b\u9810\u6e2c\u7684\u539f\u59cb\u5716\u5f62\u7d50\u69cb\u7684\u5c0f\u64fe\u52d5\uff0c\u4e5f\u53ef\u80fd\u6703\u7522\u751f\u622a\u7136\u4e0d\u540c\u7684\u89e3\u91cb\u3002\u9019\u5c0d GNN \u7684\u4e8b\u5f8c\u89e3\u91cb\u65b9\u6cd5\u7684\u53ef\u4fe1\u5ea6\u548c\u5be6\u7528\u6027\u63d0\u51fa\u4e86\u8cea\u7591\u3002\u70ba\u4e86\u80fd\u5920\u653b\u64ca GNN \u89e3\u91cb\u6a21\u578b\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u653b\u64ca\u65b9\u6cd5\uff0c\u7a31\u70ba \\textit{GXAttack}\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u5728\u9019\u7a2e\u8a2d\u5b9a\u4e0b\u91dd\u5c0d\u4e8b\u5f8c GNN \u89e3\u91cb\u7684\\textit{\u57fa\u65bc\u6700\u4f73\u5316}\u5c0d\u6297\u653b\u64ca\u65b9\u6cd5\u3002\u7531\u65bc\u6211\u5011\u7684\u653b\u64ca\u5177\u6709\u6bc0\u6ec5\u6027\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u547c\u7c72\u5c0d\u672a\u4f86\u7684 GNN \u89e3\u91cb\u5668\u9032\u884c\u5c0d\u6297\u6027\u8a55\u4f30\uff0c\u4ee5\u8b49\u660e\u5b83\u5011\u7684\u7a69\u5065\u6027\u3002", "author": "Zhong Li et.al.", "authors": "Zhong Li, Simon Geisler, Yuhang Wang, Stephan G\u00fcnnemann, Matthijs van Leeuwen", "id": "2406.06417v1", "paper_url": "http://arxiv.org/abs/2406.06417v1", "repo": "null"}}