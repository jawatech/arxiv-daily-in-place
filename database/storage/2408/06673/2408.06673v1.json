{"2408.06673": {"publish_time": "2024-08-13", "title": "Pragmatic inference of scalar implicature by LLMs", "paper_summary": "This study investigates how Large Language Models (LLMs), particularly BERT\n(Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic\ninference of scalar implicature, such as some. Two sets of experiments were\nconducted using cosine similarity and next sentence/token prediction as\nexperimental methods. The results in experiment 1 showed that, both models\ninterpret some as pragmatic implicature not all in the absence of context,\naligning with human language processing. In experiment 2, in which Question\nUnder Discussion (QUD) was presented as a contextual cue, BERT showed\nconsistent performance regardless of types of QUDs, while GPT-2 encountered\nprocessing difficulties since a certain type of QUD required pragmatic\ninference for implicature. The findings revealed that, in terms of theoretical\napproaches, BERT inherently incorporates pragmatic implicature not all within\nthe term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2\nseems to encounter processing difficulties in inferring pragmatic implicature\nwithin context, consistent with Context-driven model (Sperber and Wilson,\n2002).", "paper_summary_zh": "\u672c\u7814\u7a76\u63a2\u8a0e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u7279\u5225\u662f BERT (Devlin \u7b49\u4eba\uff0c2019) \u548c GPT-2 (Radford \u7b49\u4eba\uff0c2019) \u5982\u4f55\u9032\u884c\u6a19\u91cf\u6697\u793a\u7684\u8a9e\u7528\u63a8\u7406\uff0c\u4f8b\u5982 some\u3002\u4f7f\u7528\u9918\u5f26\u76f8\u4f3c\u6027\u548c\u4e0b\u4e00\u500b\u53e5\u5b50/\u7b26\u865f\u9810\u6e2c\u4f5c\u70ba\u5be6\u9a57\u65b9\u6cd5\uff0c\u9032\u884c\u4e86\u5169\u7d44\u5be6\u9a57\u3002\u5be6\u9a57 1 \u7684\u7d50\u679c\u8868\u660e\uff0c\u9019\u5169\u500b\u6a21\u578b\u5728\u6c92\u6709\u8a9e\u5883\u7684\u60c5\u6cc1\u4e0b\uff0c\u90fd\u5c07 some \u89e3\u91cb\u70ba\u8a9e\u7528\u6697\u793a not all\uff0c\u9019\u8207\u4eba\u985e\u8a9e\u8a00\u8655\u7406\u4e00\u81f4\u3002\u5728\u5be6\u9a57 2 \u4e2d\uff0c\u5176\u4e2d\u554f\u984c\u8a0e\u8ad6 (QUD) \u88ab\u5448\u73fe\u70ba\u8a9e\u5883\u7dda\u7d22\uff0cBERT \u986f\u793a\u51fa\u7121\u8ad6 QUD \u985e\u578b\u5982\u4f55\uff0c\u90fd\u5177\u6709\u7a69\u5b9a\u7684\u8868\u73fe\uff0c\u800c GPT-2 \u9047\u5230\u8655\u7406\u56f0\u96e3\uff0c\u56e0\u70ba\u67d0\u7a2e\u985e\u578b\u7684 QUD \u9700\u8981\u8a9e\u7528\u63a8\u7406\u624d\u80fd\u6697\u793a\u3002\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u5728\u7406\u8ad6\u65b9\u6cd5\u65b9\u9762\uff0cBERT \u672c\u8cea\u4e0a\u5c07\u8a9e\u7528\u6697\u793a not all \u7d0d\u5165\u8853\u8a9e some \u4e2d\uff0c\u9075\u5faa\u9ed8\u8a8d\u6a21\u578b (Levinson\uff0c2000)\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cGPT-2 \u4f3c\u4e4e\u5728\u63a8\u8ad6\u8a9e\u5883\u4e2d\u7684\u8a9e\u7528\u6697\u793a\u6642\u9047\u5230\u8655\u7406\u56f0\u96e3\uff0c\u9019\u8207\u8a9e\u5883\u9a45\u52d5\u6a21\u578b (Sperber \u548c Wilson\uff0c2002) \u4e00\u81f4\u3002", "author": "Ye-eun Cho et.al.", "authors": "Ye-eun Cho, Seong mook Kim", "id": "2408.06673v1", "paper_url": "http://arxiv.org/abs/2408.06673v1", "repo": "null"}}