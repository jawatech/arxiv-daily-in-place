{"2405.11582": {"publish_time": "2024-05-19", "title": "SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization", "paper_summary": "Transformers have become foundational architectures for both natural language\nand computer vision tasks. However, the high computational cost makes it quite\nchallenging to deploy on resource-constraint devices. This paper investigates\nthe computational bottleneck modules of efficient transformer, i.e.,\nnormalization layers and attention modules. LayerNorm is commonly used in\ntransformer architectures but is not computational friendly due to statistic\ncalculation during inference. However, replacing LayerNorm with more efficient\nBatchNorm in transformer often leads to inferior performance and collapse in\ntraining. To address this problem, we propose a novel method named PRepBN to\nprogressively replace LayerNorm with re-parameterized BatchNorm in training.\nMoreover, we propose a simplified linear attention (SLA) module that is simple\nyet effective to achieve strong performance. Extensive experiments on image\nclassification as well as object detection demonstrate the effectiveness of our\nproposed method. For example, our SLAB-Swin obtains $83.6\\%$ top-1 accuracy on\nImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of\nFlatten-Swin with $0.1\\%$ higher accuracy. We also evaluated our method for\nlanguage modeling task and obtain comparable performance and lower\nlatency.Codes are publicly available at https://github.com/xinghaochen/SLAB and\nhttps://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB.", "paper_summary_zh": "Transformer \u5df2\u6210\u4e3a\u81ea\u7136\u8a9e\u8a00\u548c\u96fb\u8166\u8996\u89ba\u4efb\u52d9\u7684\u57fa\u790e\u67b6\u69cb\u3002\u7136\u800c\uff0c\u9ad8\u904b\u7b97\u6210\u672c\u4f7f\u5176\u5728\u8cc7\u6e90\u53d7\u9650\u88dd\u7f6e\u4e0a\u90e8\u7f72\u6975\u5177\u6311\u6230\u6027\u3002\u672c\u6587\u63a2\u8a0e\u4e86\u9ad8\u6548 Transformer \u7684\u904b\u7b97\u74f6\u9838\u6a21\u7d44\uff0c\u5373\u6b63\u898f\u5316\u5c64\u548c\u6ce8\u610f\u529b\u6a21\u7d44\u3002LayerNorm \u5e38\u7528\u65bc Transformer \u67b6\u69cb\u4e2d\uff0c\u4f46\u7531\u65bc\u63a8\u8ad6\u671f\u9593\u7684\u7d71\u8a08\u8a08\u7b97\uff0c\u5b83\u4e26\u975e\u904b\u7b97\u53cb\u5584\u3002\u7136\u800c\uff0c\u5728 Transformer \u4e2d\u7528\u66f4\u6709\u6548\u7684 BatchNorm \u53d6\u4ee3 LayerNorm \u5f80\u5f80\u6703\u5c0e\u81f4\u6548\u80fd\u4f4e\u843d\u548c\u8a13\u7df4\u5d29\u6f70\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba PRepBN \u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u8a13\u7df4\u4e2d\u9010\u6b65\u7528\u91cd\u65b0\u53c3\u6578\u5316\u7684 BatchNorm \u53d6\u4ee3 LayerNorm\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u5316\u7684\u7dda\u6027\u6ce8\u610f\u529b (SLA) \u6a21\u7d44\uff0c\u5b83\u7c21\u55ae\u4f46\u6709\u6548\uff0c\u53ef\u5be6\u73fe\u5f37\u5927\u7684\u6548\u80fd\u3002\u5728\u5f71\u50cf\u5206\u985e\u548c\u7269\u4ef6\u5075\u6e2c\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4f8b\u5982\uff0c\u6211\u5011\u7684 SLAB-Swin \u5728 ImageNet-1K \u4e0a\u7372\u5f97 83.6% \u7684 top-1 \u7cbe\u78ba\u5ea6\uff0c\u5ef6\u9072\u6642\u9593\u70ba 16.2 \u6beb\u79d2\uff0c\u6bd4 Flatten-Swin \u5c11 2.4 \u6beb\u79d2\uff0c\u4f46\u7cbe\u78ba\u5ea6\u9ad8\u51fa 0.1%\u3002\u6211\u5011\u4e5f\u8a55\u4f30\u4e86\u6211\u5011\u5728\u8a9e\u8a00\u6a21\u578b\u4efb\u52d9\u4e2d\u7684\u65b9\u6cd5\uff0c\u4e26\u7372\u5f97\u53ef\u6bd4\u8f03\u7684\u6548\u80fd\u548c\u66f4\u4f4e\u7684\u5ef6\u9072\u6642\u9593\u3002\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u65bc https://github.com/xinghaochen/SLAB \u548c https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB\u3002", "author": "Jialong Guo et.al.", "authors": "Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang", "id": "2405.11582v1", "paper_url": "http://arxiv.org/abs/2405.11582v1", "repo": "https://github.com/xinghaochen/slab"}}