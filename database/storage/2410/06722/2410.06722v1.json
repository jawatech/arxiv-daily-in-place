{"2410.06722": {"publish_time": "2024-10-09", "title": "Scaling Laws for Mixed quantization in Large Language Models", "paper_summary": "Post-training quantization of Large Language Models (LLMs) has proven\neffective in reducing the computational requirements for running inference on\nthese models. In this study, we focus on a straightforward question: When\naiming for a specific accuracy or perplexity target for low-precision\nquantization, how many high-precision numbers or calculations are required to\npreserve as we scale LLMs to larger sizes? We first introduce a critical metric\nnamed the quantization ratio, which compares the number of parameters quantized\nto low-precision arithmetic against the total parameter count. Through\nextensive and carefully controlled experiments across different model families,\narithmetic types, and quantization granularities (e.g. layer-wise,\nmatmul-wise), we identify two central phenomenons. 1) The larger the models,\nthe better they can preserve performance with an increased quantization ratio,\nas measured by perplexity in pre-training tasks or accuracy in downstream\ntasks. 2) The finer the granularity of mixed-precision quantization (e.g.,\nmatmul-wise), the more the model can increase the quantization ratio. We\nbelieve these observed phenomena offer valuable insights for future AI hardware\ndesign and the development of advanced Efficient AI algorithms.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8a13\u7df4\u5f8c\u91cf\u5316\u5df2\u8b49\u660e\u53ef\u6709\u6548\u964d\u4f4e\u57f7\u884c\u9019\u4e9b\u6a21\u578b\u63a8\u7406\u7684\u904b\u7b97\u9700\u6c42\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u4e00\u500b\u7c21\u55ae\u7684\u554f\u984c\uff1a\u5728\u91dd\u5c0d\u4f4e\u7cbe\u5ea6\u91cf\u5316\u8a2d\u5b9a\u7279\u5b9a\u6e96\u78ba\u5ea6\u6216\u56f0\u60d1\u5ea6\u76ee\u6a19\u6642\uff0c\u5728\u6211\u5011\u5c07 LLM \u64f4\u5c55\u5230\u66f4\u5927\u898f\u6a21\u6642\uff0c\u9700\u8981\u4fdd\u7559\u591a\u5c11\u9ad8\u7cbe\u5ea6\u6578\u5b57\u6216\u904b\u7b97\uff1f\u6211\u5011\u9996\u5148\u5f15\u5165\u4e00\u500b\u7a31\u70ba\u91cf\u5316\u6bd4\u7387\u7684\u95dc\u9375\u6307\u6a19\uff0c\u5b83\u6703\u6bd4\u8f03\u91cf\u5316\u70ba\u4f4e\u7cbe\u5ea6\u7b97\u8853\u7684\u53c3\u6578\u6578\u91cf\u8207\u7e3d\u53c3\u6578\u6578\u91cf\u3002\u900f\u904e\u91dd\u5c0d\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u3001\u7b97\u8853\u985e\u578b\u548c\u91cf\u5316\u7c92\u5ea6\uff08\u4f8b\u5982\uff0c\u6309\u5c64\u3001\u6309\u77e9\u9663\u4e58\u6cd5\uff09\u9032\u884c\u5ee3\u6cdb\u4e14\u4ed4\u7d30\u63a7\u5236\u7684\u5be6\u9a57\uff0c\u6211\u5011\u627e\u51fa\u5169\u500b\u4e3b\u8981\u7684\u73fe\u8c61\u30021) \u6a21\u578b\u8d8a\u5927\uff0c\u5b83\u5011\u5c31\u80fd\u4ee5\u66f4\u9ad8\u7684\u91cf\u5316\u6bd4\u7387\u7dad\u6301\u6548\u80fd\uff0c\u9019\u53ef\u900f\u904e\u9810\u8a13\u7df4\u4efb\u52d9\u4e2d\u7684\u56f0\u60d1\u5ea6\u6216\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684\u6e96\u78ba\u5ea6\u4f86\u8861\u91cf\u30022) \u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7684\u7c92\u5ea6\u8d8a\u7d30\uff08\u4f8b\u5982\uff0c\u6309\u77e9\u9663\u4e58\u6cd5\uff09\uff0c\u6a21\u578b\u5c31\u80fd\u63d0\u9ad8\u91cf\u5316\u6bd4\u7387\u3002\u6211\u5011\u76f8\u4fe1\u9019\u4e9b\u89c0\u5bdf\u5230\u7684\u73fe\u8c61\u70ba\u672a\u4f86\u7684 AI \u786c\u9ad4\u8a2d\u8a08\u548c\u9032\u968e\u9ad8\u6548\u80fd AI \u6f14\u7b97\u6cd5\u7684\u958b\u767c\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\u3002", "author": "Zeyu Cao et.al.", "authors": "Zeyu Cao, Cheng Zhang, Pedro Gimenes, Jianqiao Lu, Jianyi Cheng, Yiren Zhao", "id": "2410.06722v1", "paper_url": "http://arxiv.org/abs/2410.06722v1", "repo": "null"}}