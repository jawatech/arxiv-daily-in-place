{"2405.14507": {"publish_time": "2024-05-23", "title": "Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by Self-Contrast", "paper_summary": "Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling\nmodel size while maintaining computational efficiency. In MoE, each token in\nthe input sequence activates a different subset of experts determined by a\nrouting mechanism. However, the unchosen experts in MoE models do not\ncontribute to the output, potentially leading to underutilization of the\nmodel's capacity. In this work, we first conduct exploratory studies to\ndemonstrate that increasing the number of activated experts does not\nnecessarily improve and can even degrade the output quality. Then, we show that\noutput distributions from an MoE model using different routing strategies\nsubstantially differ, indicating that different experts do not always act\nsynergistically. Motivated by these findings, we propose Self-Contrast\nMixture-of-Experts (SCMoE), a training-free strategy that utilizes unchosen\nexperts in a self-contrast manner during inference. In SCMoE, the next-token\nprobabilities are determined by contrasting the outputs from strong and weak\nactivation using the same MoE model. Our method is conceptually simple and\ncomputationally lightweight, as it incurs minimal latency compared to greedy\ndecoding. Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and\nHumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B's\nreasoning capability across various domains. For example, it improves the\naccuracy on GSM8K from 61.79 to 66.94. Moreover, combining SCMoE with\nself-consistency yields additional gains, increasing major@20 accuracy from\n75.59 to 78.31.", "paper_summary_zh": "\u6df7\u5408\u5c08\u5bb6 (MoE) \u5df2\u6210\u70ba\u4e00\u7a2e\u91cd\u8981\u7684\u67b6\u69cb\uff0c\u7528\u65bc\u64f4\u5c55\u6a21\u578b\u5927\u5c0f\uff0c\u540c\u6642\u4fdd\u6301\u904b\u7b97\u6548\u7387\u3002\u5728 MoE \u4e2d\uff0c\u8f38\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u500b\u7b26\u865f\u6703\u555f\u52d5\u7531\u8def\u7531\u6a5f\u5236\u6c7a\u5b9a\u7684\u4e0d\u540c\u5c08\u5bb6\u5b50\u96c6\u3002\u7136\u800c\uff0cMoE \u6a21\u578b\u4e2d\u672a\u9078\u64c7\u7684\u5c08\u5bb6\u4e0d\u6703\u5c0d\u8f38\u51fa\u505a\u51fa\u8ca2\u737b\uff0c\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u6a21\u578b\u5bb9\u91cf\u5229\u7528\u4e0d\u8db3\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u9032\u884c\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u4ee5\u8b49\u660e\u589e\u52a0\u5df2\u555f\u52d5\u5c08\u5bb6\u7684\u6578\u91cf\u4e0d\u4e00\u5b9a\u6703\u6539\u5584\uff0c\u751a\u81f3\u53ef\u80fd\u964d\u4f4e\u8f38\u51fa\u54c1\u8cea\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8b49\u660e\u4f7f\u7528\u4e0d\u540c\u8def\u7531\u7b56\u7565\u7684 MoE \u6a21\u578b\u7684\u8f38\u51fa\u5206\u4f48\u6709\u5f88\u5927\u4e0d\u540c\uff0c\u9019\u8868\u660e\u4e0d\u540c\u7684\u5c08\u5bb6\u4e26\u4e0d\u7e3d\u662f\u5354\u540c\u4f5c\u7528\u3002\u53d7\u5230\u9019\u4e9b\u767c\u73fe\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u81ea\u5c0d\u6bd4\u6df7\u5408\u5c08\u5bb6 (SCMoE)\uff0c\u9019\u662f\u4e00\u7a2e\u7121\u9700\u8a13\u7df4\u7684\u7b56\u7565\uff0c\u5b83\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u4ee5\u81ea\u5c0d\u6bd4\u7684\u65b9\u5f0f\u5229\u7528\u672a\u9078\u64c7\u7684\u5c08\u5bb6\u3002\u5728 SCMoE \u4e2d\uff0c\u4e0b\u4e00\u500b\u7b26\u865f\u7684\u6a5f\u7387\u662f\u7531\u4f7f\u7528\u76f8\u540c MoE \u6a21\u578b\u5c0d\u6bd4\u5f37\u5f31\u6fc0\u6d3b\u7684\u8f38\u51fa\u6c7a\u5b9a\u7684\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u5728\u6982\u5ff5\u4e0a\u5f88\u7c21\u55ae\uff0c\u4e14\u904b\u7b97\u91cf\u5f88\u8f15\uff0c\u56e0\u70ba\u8207\u8caa\u5a6a\u89e3\u78bc\u76f8\u6bd4\uff0c\u5b83\u7522\u751f\u7684\u5ef6\u9072\u6700\u5c11\u3002\u5728\u5e7e\u500b\u57fa\u6e96\u6e2c\u8a66\uff08GSM8K\u3001StrategyQA\u3001MBPP \u548c HumanEval\uff09\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0cSCMoE \u53ef\u4ee5\u6301\u7e8c\u589e\u5f37 Mixtral 8x7B \u5728\u5404\u7a2e\u9818\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u5b83\u5c07 GSM8K \u7684\u6e96\u78ba\u5ea6\u5f9e 61.79 \u63d0\u9ad8\u5230 66.94\u3002\u6b64\u5916\uff0c\u5c07 SCMoE \u8207\u81ea\u4e00\u81f4\u6027\u7d50\u5408\u4f7f\u7528\u6703\u7522\u751f\u984d\u5916\u7684\u6536\u76ca\uff0c\u5c07 major@20 \u6e96\u78ba\u5ea6\u5f9e 75.59 \u63d0\u9ad8\u5230 78.31\u3002", "author": "Chufan Shi et.al.", "authors": "Chufan Shi, Cheng Yang, Xinyu Zhu, Jiahao Wang, Taiqiang Wu, Siheng Li, Deng Cai, Yujiu Yang, Yu Meng", "id": "2405.14507v1", "paper_url": "http://arxiv.org/abs/2405.14507v1", "repo": "null"}}