{"2411.13757": {"publish_time": "2024-11-21", "title": "AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking Vulnerabilities in LLMs through Bit-Flip Attacks", "paper_summary": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP), excelling in tasks like text generation and summarization. However,\ntheir increasing adoption in mission-critical applications raises concerns\nabout hardware-based threats, particularly bit-flip attacks (BFAs). BFAs,\nenabled by fault injection methods such as Rowhammer, target model parameters\nin memory, compromising both integrity and performance. Identifying critical\nparameters for BFAs in the vast parameter space of LLMs poses significant\nchallenges. While prior research suggests transformer-based architectures are\ninherently more robust to BFAs compared to traditional deep neural networks, we\nchallenge this assumption. For the first time, we demonstrate that as few as\nthree bit-flips can cause catastrophic performance degradation in an LLM with\nbillions of parameters. Current BFA techniques are inadequate for exploiting\nthis vulnerability due to the difficulty of efficiently identifying critical\nparameters within the immense parameter space. To address this, we propose\nAttentionBreaker, a novel framework tailored for LLMs that enables efficient\ntraversal of the parameter space to identify critical parameters. Additionally,\nwe introduce GenBFA, an evolutionary optimization strategy designed to refine\nthe search further, isolating the most critical bits for an efficient and\neffective attack. Empirical results reveal the profound vulnerability of LLMs\nto AttentionBreaker. For example, merely three bit-flips (4.129 x 10^-9% of\ntotal parameters) in the LLaMA3-8B-Instruct 8-bit quantized (W8) model result\nin a complete performance collapse: accuracy on MMLU tasks drops from 67.3% to\n0%, and Wikitext perplexity skyrockets from 12.6 to 4.72 x 10^5. These findings\nunderscore the effectiveness of AttentionBreaker in uncovering and exploiting\ncritical vulnerabilities within LLM architectures.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fb9\u5e95\u6539\u8b8a\u4e86\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP)\uff0c\u5728\u6587\u672c\u751f\u6210\u548c\u6458\u8981\u7b49\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u4efb\u52d9\u95dc\u9375\u578b\u61c9\u7528\u4e2d\u7684\u63a1\u7528\u8d8a\u4f86\u8d8a\u591a\uff0c\u9019\u5f15\u8d77\u4e86\u5c0d\u57fa\u65bc\u786c\u9ad4\u7684\u5a01\u8105\u7684\u64d4\u6182\uff0c\u7279\u5225\u662f\u4f4d\u5143\u7ffb\u8f49\u653b\u64ca (BFA)\u3002BFA \u7531\u6545\u969c\u6ce8\u5165\u65b9\u6cd5\uff08\u4f8b\u5982 Rowhammer\uff09\u555f\u7528\uff0c\u91dd\u5c0d\u8a18\u61b6\u9ad4\u4e2d\u7684\u6a21\u578b\u53c3\u6578\uff0c\u640d\u5bb3\u5b8c\u6574\u6027\u548c\u6548\u80fd\u3002\u5728 LLM \u5de8\u5927\u53c3\u6578\u7a7a\u9593\u4e2d\u8b58\u5225 BFA \u7684\u95dc\u9375\u53c3\u6578\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6230\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u8207\u50b3\u7d71\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u76f8\u6bd4\uff0c\u57fa\u65bcTransformer\u7684\u67b6\u69cb\u5929\u751f\u5c0d BFA \u66f4\u5177\u9b6f\u68d2\u6027\uff0c\u4f46\u6211\u5011\u6311\u6230\u4e86\u9019\u4e00\u5047\u8a2d\u3002\u6211\u5011\u9996\u6b21\u8b49\u660e\uff0c\u50c5\u4e09\u8655\u4f4d\u5143\u7ffb\u8f49\u5373\u53ef\u5c0e\u81f4\u5177\u6709\u6578\u5341\u5104\u500b\u53c3\u6578\u7684 LLM \u767c\u751f\u707d\u96e3\u6027\u7684\u6548\u80fd\u4e0b\u964d\u3002\u7531\u65bc\u96e3\u4ee5\u5728\u5de8\u5927\u7684\u53c3\u6578\u7a7a\u9593\u4e2d\u6709\u6548\u8b58\u5225\u95dc\u9375\u53c3\u6578\uff0c\u76ee\u524d\u7684 BFA \u6280\u8853\u4e0d\u8db3\u4ee5\u5229\u7528\u6b64\u6f0f\u6d1e\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 AttentionBreaker\uff0c\u9019\u662f\u4e00\u500b\u5c08\u9580\u91dd\u5c0d LLM \u7684\u65b0\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u904d\u6b77\u53c3\u6578\u7a7a\u9593\u4ee5\u8b58\u5225\u95dc\u9375\u53c3\u6578\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86 GenBFA\uff0c\u9019\u662f\u4e00\u7a2e\u9032\u5316\u512a\u5316\u7b56\u7565\uff0c\u65e8\u5728\u9032\u4e00\u6b65\u512a\u5316\u641c\u5c0b\uff0c\u9694\u96e2\u6700\u95dc\u9375\u7684\u4f4d\u5143\uff0c\u4ee5\u9032\u884c\u6709\u6548\u7387\u4e14\u6709\u6548\u7684\u653b\u64ca\u3002\u7d93\u9a57\u7d50\u679c\u63ed\u793a\u4e86 LLM \u5c0d AttentionBreaker \u7684\u56b4\u91cd\u6f0f\u6d1e\u3002\u4f8b\u5982\uff0cLLaMA3-8B-Instruct 8 \u4f4d\u5143\u91cf\u5316 (W8) \u6a21\u578b\u4e2d\u50c5\u4e09\u8655\u4f4d\u5143\u7ffb\u8f49\uff08\u7e3d\u53c3\u6578\u7684 4.129 x 10^-9%\uff09\u5c31\u6703\u5c0e\u81f4\u6548\u80fd\u5b8c\u5168\u5d29\u6f70\uff1aMMLU \u4efb\u52d9\u7684\u6e96\u78ba\u5ea6\u5f9e 67.3% \u964d\u81f3 0%\uff0c\u800c Wikitext \u7684\u56f0\u60d1\u5ea6\u5f9e 12.6 \u98c6\u5347\u81f3 4.72 x 10^5\u3002\u9019\u4e9b\u767c\u73fe\u5f37\u8abf\u4e86 AttentionBreaker \u5728\u63ed\u9732\u548c\u5229\u7528 LLM \u67b6\u69cb\u4e2d\u7684\u95dc\u9375\u6f0f\u6d1e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Sanjay Das et.al.", "authors": "Sanjay Das, Swastik Bhattacharya, Souvik Kundu, Shamik Kundu, Anand Menon, Arnab Raha, Kanad Basu", "id": "2411.13757v1", "paper_url": "http://arxiv.org/abs/2411.13757v1", "repo": "null"}}