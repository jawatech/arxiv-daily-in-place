{"2405.14314": {"publish_time": "2024-05-23", "title": "Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration", "paper_summary": "Grounding the reasoning ability of large language models (LLMs) for embodied\ntasks is challenging due to the complexity of the physical world. Especially,\nLLM planning for multi-agent collaboration requires communication of agents or\ncredit assignment as the feedback to re-adjust the proposed plans and achieve\neffective coordination. However, existing methods that overly rely on physical\nverification or self-reflection suffer from excessive and inefficient querying\nof LLMs. In this paper, we propose a novel framework for multi-agent\ncollaboration that introduces Reinforced Advantage feedback (ReAd) for\nefficient self-refinement of plans. Specifically, we perform critic regression\nto learn a sequential advantage function from LLM-planned data, and then treat\nthe LLM planner as an optimizer to generate actions that maximize the advantage\nfunction. It endows the LLM with the foresight to discern whether the action\ncontributes to accomplishing the final task. We provide theoretical analysis by\nextending advantage-weighted regression in reinforcement learning to\nmulti-agent systems. Experiments on Overcooked-AI and a difficult variant of\nRoCoBench show that ReAd surpasses baselines in success rate, and also\nsignificantly decreases the interaction steps of agents and query rounds of\nLLMs, demonstrating its high efficiency for grounding LLMs. More results are\ngiven at \\url{https://read-llm.github.io/}.", "paper_summary_zh": "\u7531\u65bc\u7269\u7406\u4e16\u754c\u7684\u8907\u96dc\u6027\uff0c\u8981\u5960\u5b9a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\u4ee5\u9032\u884c\u5177\u9ad4\u4efb\u52d9\u662f\u4e00\u9805\u6311\u6230\u3002\u5c24\u5176\u662f\uff0cLLM \u898f\u5283\u591a\u91cd\u4ee3\u7406\u5354\u4f5c\u9700\u8981\u4ee3\u7406\u4e4b\u9593\u7684\u6e9d\u901a\u6216\u4fe1\u7528\u5206\u914d\uff0c\u4f5c\u70ba\u91cd\u65b0\u8abf\u6574\u63d0\u8b70\u8a08\u756b\u548c\u5be6\u73fe\u6709\u6548\u5354\u8abf\u7684\u56de\u994b\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u904e\u5ea6\u4f9d\u8cf4\u7269\u7406\u9a57\u8b49\u6216\u81ea\u6211\u53cd\u7701\uff0c\u6703\u9020\u6210 LLM \u904e\u5ea6\u4e14\u4f4e\u6548\u7387\u7684\u67e5\u8a62\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u591a\u91cd\u4ee3\u7406\u5354\u4f5c\u7684\u65b0\u7a4e\u67b6\u69cb\uff0c\u5c0e\u5165\u5f37\u5316\u512a\u52e2\u56de\u994b (ReAd) \u4ee5\u6709\u6548\u81ea\u6211\u6539\u5584\u8a08\u756b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u57f7\u884c\u6279\u8a55\u8ff4\u6b78\uff0c\u5f9e LLM \u898f\u5283\u7684\u6578\u64da\u4e2d\u5b78\u7fd2\u9806\u5e8f\u512a\u52e2\u51fd\u6578\uff0c\u7136\u5f8c\u5c07 LLM \u898f\u5283\u5668\u8996\u70ba\u4e00\u500b\u6700\u4f73\u5316\u5668\uff0c\u4ee5\u7522\u751f\u6700\u5927\u5316\u512a\u52e2\u51fd\u6578\u7684\u52d5\u4f5c\u3002\u5b83\u8ce6\u4e88 LLM \u9810\u898b\u529b\uff0c\u4ee5\u8fa8\u5225\u52d5\u4f5c\u662f\u5426\u6709\u52a9\u65bc\u5b8c\u6210\u6700\u7d42\u4efb\u52d9\u3002\u6211\u5011\u900f\u904e\u5c07\u5f37\u5316\u5b78\u7fd2\u4e2d\u7684\u512a\u52e2\u52a0\u6b0a\u8ff4\u6b78\u5ef6\u4f38\u5230\u591a\u91cd\u4ee3\u7406\u7cfb\u7d71\uff0c\u63d0\u4f9b\u7406\u8ad6\u5206\u6790\u3002\u5728 Overcooked-AI \u548c RoCoBench \u7684\u56f0\u96e3\u8b8a\u9ad4\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u986f\u793a\uff0cReAd \u5728\u6210\u529f\u7387\u4e0a\u8d85\u8d8a\u57fa\u6e96\uff0c\u4e26\u4e14\u986f\u8457\u6e1b\u5c11\u4ee3\u7406\u7684\u4e92\u52d5\u6b65\u9a5f\u548c LLM \u7684\u67e5\u8a62\u56de\u5408\uff0c\u8b49\u660e\u5176\u5728\u5960\u5b9a LLM \u4e0a\u7684\u9ad8\u6548\u7387\u3002\u66f4\u591a\u7d50\u679c\u8acb\u53c3\u95b1 \\url{https://read-llm.github.io/}\u3002", "author": "Yang Zhang et.al.", "authors": "Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong Li, Zhen Wang", "id": "2405.14314v1", "paper_url": "http://arxiv.org/abs/2405.14314v1", "repo": "null"}}