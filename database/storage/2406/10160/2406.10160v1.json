{"2406.10160": {"publish_time": "2024-06-14", "title": "One-pass Multiple Conformer and Foundation Speech Systems Compression and Quantization Using An All-in-one Neural Model", "paper_summary": "We propose a novel one-pass multiple ASR systems joint compression and\nquantization approach using an all-in-one neural model. A single compression\ncycle allows multiple nested systems with varying Encoder depths, widths, and\nquantization precision settings to be simultaneously constructed without the\nneed to train and store individual target systems separately. Experiments\nconsistently demonstrate the multiple ASR systems compressed in a single\nall-in-one model produced a word error rate (WER) comparable to, or lower by up\nto 1.01\\% absolute (6.98\\% relative) than individually trained systems of equal\ncomplexity. A 3.4x overall system compression and training time speed-up was\nachieved. Maximum model size compression ratios of 12.8x and 3.93x were\nobtained over the baseline Switchboard-300hr Conformer and LibriSpeech-100hr\nfine-tuned wav2vec2.0 models, respectively, incurring no statistically\nsignificant WER increase.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u4e00\u904d\u5f0f\u591a\u91cd ASR \u7cfb\u7d71\u806f\u5408\u58d3\u7e2e\u548c\u91cf\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e00\u500b\u591a\u5408\u4e00\u7684\u985e\u795e\u7d93\u6a21\u578b\u3002\u55ae\u4e00\u58d3\u7e2e\u9031\u671f\u5141\u8a31\u540c\u6642\u69cb\u9020\u5177\u6709\u4e0d\u540c\u7de8\u78bc\u5668\u6df1\u5ea6\u3001\u5bec\u5ea6\u548c\u91cf\u5316\u7cbe\u5ea6\u8a2d\u5b9a\u7684\u591a\u91cd\u5d4c\u5957\u7cfb\u7d71\uff0c\u800c\u7121\u9700\u5206\u5225\u8a13\u7df4\u548c\u5132\u5b58\u500b\u5225\u7684\u76ee\u6a19\u7cfb\u7d71\u3002\u5be6\u9a57\u6301\u7e8c\u8b49\u660e\u58d3\u7e2e\u5728\u55ae\u4e00\u591a\u5408\u4e00\u6a21\u578b\u4e2d\u7684\u591a\u91cd ASR \u7cfb\u7d71\u7522\u751f\u4e00\u500b\u8a5e\u5f59\u932f\u8aa4\u7387 (WER)\uff0c\u8207\u500b\u5225\u8a13\u7df4\u7684\u76f8\u540c\u8907\u96dc\u5ea6\u7cfb\u7d71\u76f8\u7576\uff0c\u6216\u4f4e\u81f3 1.01% \u7d55\u5c0d\u503c (6.98% \u76f8\u5c0d\u503c)\u3002\u5be6\u73fe\u4e86 3.4 \u500d\u7684\u6574\u9ad4\u7cfb\u7d71\u58d3\u7e2e\u548c\u8a13\u7df4\u6642\u9593\u52a0\u901f\u3002\u8207\u57fa\u6e96 Switchboard-300hr Conformer \u548c LibriSpeech-100hr \u5fae\u8abf wav2vec2.0 \u6a21\u578b\u76f8\u6bd4\uff0c\u5206\u5225\u7372\u5f97\u4e86 12.8 \u500d\u548c 3.93 \u500d\u7684\u6700\u5927\u6a21\u578b\u5927\u5c0f\u58d3\u7e2e\u6bd4\uff0c\u4e0d\u6703\u9020\u6210\u7d71\u8a08\u4e0a\u986f\u8457\u7684 WER \u589e\u52a0\u3002", "author": "Zhaoqing Li et.al.", "authors": "Zhaoqing Li, Haoning Xu, Tianzi Wang, Shoukang Hu, Zengrui Jin, Shujie Hu, Jiajun Deng, Mingyu Cui, Mengzhe Geng, Xunying Liu", "id": "2406.10160v1", "paper_url": "http://arxiv.org/abs/2406.10160v1", "repo": "null"}}