{"2405.17871": {"publish_time": "2024-05-28", "title": "Seeing the Image: Prioritizing Visual Correlation by Contrastive Alignment", "paper_summary": "Existing image-text modality alignment in Vision Language Models (VLMs)\ntreats each text token equally in an autoregressive manner. Despite being\nsimple and effective, this method results in sub-optimal cross-modal alignment\nby over-emphasizing the text tokens that are less correlated with or even\ncontradictory with the input images. In this paper, we advocate for assigning\ndistinct contributions for each text token based on its visual correlation.\nSpecifically, we present by contrasting image inputs, the difference in\nprediction logits on each text token provides strong guidance of visual\ncorrelation. We therefore introduce Contrastive ALignment (CAL), a simple yet\neffective re-weighting strategy that prioritizes training visually correlated\ntokens. Our experimental results demonstrate that CAL consistently improves\ndifferent types of VLMs across different resolutions and model sizes on various\nbenchmark datasets. Importantly, our method incurs minimal additional\ncomputational overhead, rendering it highly efficient compared to alternative\ndata scaling strategies. Codes are available at\nhttps://github.com/foundation-multimodal-models/CAL.", "paper_summary_zh": "\u73fe\u6709\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4e2d\u7684\u5f71\u50cf\u6587\u5b57\u6a21\u614b\u5c0d\u9f4a\uff0c\u4ee5\u81ea\u8ff4\u6b78\u7684\u65b9\u5f0f\u5e73\u5747\u8655\u7406\u6bcf\u500b\u6587\u5b57\u7b26\u865f\u3002\u5118\u7ba1\u9019\u7a2e\u65b9\u6cd5\u7c21\u55ae\u4e14\u6709\u6548\uff0c\u4f46\u5b83\u6703\u904e\u5ea6\u5f37\u8abf\u8207\u8f38\u5165\u5f71\u50cf\u76f8\u95dc\u6027\u8f03\u4f4e\uff0c\u751a\u81f3\u8207\u5176\u77db\u76fe\u7684\u6587\u5b57\u7b26\u865f\uff0c\u9032\u800c\u5c0e\u81f4\u6b21\u4f73\u7684\u8de8\u6a21\u614b\u5c0d\u9f4a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4e3b\u5f35\u6839\u64da\u6bcf\u500b\u6587\u5b57\u7b26\u865f\u7684\u8996\u89ba\u76f8\u95dc\u6027\uff0c\u70ba\u5176\u5206\u914d\u4e0d\u540c\u7684\u8ca2\u737b\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u900f\u904e\u5c0d\u6bd4\u5f71\u50cf\u8f38\u5165\uff0c\u63d0\u4f9b\u6bcf\u500b\u6587\u5b57\u7b26\u865f\u7684\u9810\u6e2c\u908f\u8f2f\u65af\u8ff4\u6b78\u4e2d\u7684\u5dee\u7570\uff0c\u4ee5\u63d0\u4f9b\u8996\u89ba\u76f8\u95dc\u6027\u7684\u5f37\u6709\u529b\u6307\u5c0e\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5c0d\u6bd4\u5c0d\u9f4a (CAL)\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u91cd\u65b0\u52a0\u6b0a\u7b56\u7565\uff0c\u53ef\u512a\u5148\u8a13\u7df4\u8996\u89ba\u76f8\u95dc\u7684\u7b26\u865f\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cCAL \u5728\u5404\u7a2e\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\uff0c\u6301\u7e8c\u6539\u5584\u4e0d\u540c\u89e3\u6790\u5ea6\u548c\u6a21\u578b\u5927\u5c0f\u7684\u5404\u7a2e VLM\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u6703\u7522\u751f\u6700\u5c0f\u7684\u984d\u5916\u904b\u7b97\u8ca0\u64d4\uff0c\u8207\u5176\u4ed6\u8cc7\u6599\u64f4\u5145\u7b56\u7565\u76f8\u6bd4\uff0c\u6548\u7387\u6975\u9ad8\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/foundation-multimodal-models/CAL \u53d6\u5f97\u3002", "author": "Xin Xiao et.al.", "authors": "Xin Xiao, Bohong Wu, Jiacong Wang, Chunyuan Li, Xun Zhou, Haoyuan Guo", "id": "2405.17871v1", "paper_url": "http://arxiv.org/abs/2405.17871v1", "repo": "null"}}