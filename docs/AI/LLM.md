
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-13**|**Explore the Limits of Omni-modal Pretraining at Scale**|Yiyuan Zhang et.al.|[2406.09412v1](http://arxiv.org/abs/2406.09412v1)|[link](https://github.com/invictus717/MiCo)|
|**2024-06-13**|**MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding**|Fei Wang et.al.|[2406.09411v1](http://arxiv.org/abs/2406.09411v1)|null|
|**2024-06-13**|**Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach**|Yansheng Li et.al.|[2406.09410v1](http://arxiv.org/abs/2406.09410v1)|[link](https://github.com/yangxue0827/rsg-mmrotate)|
|**2024-06-13**|**4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities**|Roman Bachmann et.al.|[2406.09406v2](http://arxiv.org/abs/2406.09406v2)|null|
|**2024-06-13**|**ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing**|Jun-Kun Chen et.al.|[2406.09404v1](http://arxiv.org/abs/2406.09404v1)|null|
|**2024-06-13**|**Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models**|Yushi Hu et.al.|[2406.09403v1](http://arxiv.org/abs/2406.09403v1)|null|
|**2024-06-13**|**MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations**|Ruiyuan Lyu et.al.|[2406.09401v1](http://arxiv.org/abs/2406.09401v1)|[link](https://github.com/openrobotlab/embodiedscan)|
|**2024-06-13**|**Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion**|Linzhan Mou et.al.|[2406.09402v1](http://arxiv.org/abs/2406.09402v1)|null|
|**2024-06-13**|**Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms**|Miaosen Zhang et.al.|[2406.09397v1](http://arxiv.org/abs/2406.09397v1)|null|
|**2024-06-13**|**A More Practical Approach to Machine Unlearning**|David Zagardo et.al.|[2406.09391v1](http://arxiv.org/abs/2406.09391v1)|null|
|**2024-06-13**|**Exploring the Spectrum of Visio-Linguistic Compositionality and Recognition**|Youngtaek Oh et.al.|[2406.09388v1](http://arxiv.org/abs/2406.09388v1)|[link](https://github.com/ytaek-oh/vl_compo)|
|**2024-06-13**|**ElicitationGPT: Text Elicitation Mechanisms via Language Models**|Yifan Wu et.al.|[2406.09363v1](http://arxiv.org/abs/2406.09363v1)|null|
|**2024-06-13**|**Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores**|Álvaro Ciudad et.al.|[2406.09346v1](http://arxiv.org/abs/2406.09346v1)|null|
|**2024-06-13**|**DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding**|Suwon Shon et.al.|[2406.09345v1](http://arxiv.org/abs/2406.09345v1)|null|
|**2024-06-13**|**ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models**|David Anugraha et.al.|[2406.09334v2](http://arxiv.org/abs/2406.09334v2)|[link](https://github.com/davidanugraha/proxylm)|
|**2024-06-13**|**Learning from Natural Language Explanations for Generalizable Entity Matching**|Somin Wadhwa et.al.|[2406.09330v1](http://arxiv.org/abs/2406.09330v1)|null|
|**2024-06-13**|**PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance**|Qijun Gan et.al.|[2406.09326v1](http://arxiv.org/abs/2406.09326v1)|null|
|**2024-06-13**|**REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space**|Tomer Ashuach et.al.|[2406.09325v1](http://arxiv.org/abs/2406.09325v1)|null|
|**2024-06-13**|**Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs**|Zhao Xu et.al.|[2406.09324v1](http://arxiv.org/abs/2406.09324v1)|[link](https://github.com/usail-hkust/bag_of_tricks_for_llm_jailbreaking)|
|**2024-06-13**|**JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models**|Delong Ran et.al.|[2406.09321v1](http://arxiv.org/abs/2406.09321v1)|[link](https://github.com/thuccslab/jailbreakeval)|
|**2024-06-13**|**Characterising Interventions in Causal Games**|Manuj Mishra et.al.|[2406.09318v1](http://arxiv.org/abs/2406.09318v1)|null|
|**2024-06-13**|**Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers**|Zhuolin Fu et.al.|[2406.09315v1](http://arxiv.org/abs/2406.09315v1)|[link](https://github.com/neverUseThisName/vlora)|
|**2024-06-13**|**Transformers meet Neural Algorithmic Reasoners**|Wilfried Bounsi et.al.|[2406.09308v1](http://arxiv.org/abs/2406.09308v1)|null|
|**2024-06-13**|**Parameter-Efficient Active Learning for Foundational models**|Athmanarayanan Lakshmi Narayanan et.al.|[2406.09296v2](http://arxiv.org/abs/2406.09296v2)|null|
|**2024-06-13**|**AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models**|Yuhang Wu et.al.|[2406.09295v2](http://arxiv.org/abs/2406.09295v2)|null|
|**2024-06-13**|**Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image Diffusion Models**|Ziyi Wu et.al.|[2406.09292v1](http://arxiv.org/abs/2406.09292v1)|null|
|**2024-06-13**|**Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech**|Martina Valente et.al.|[2406.09290v1](http://arxiv.org/abs/2406.09290v1)|null|
|**2024-06-13**|**Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models**|Sarah Ball et.al.|[2406.09289v1](http://arxiv.org/abs/2406.09289v1)|[link](https://github.com/s-ball-10/jailbreak_dynamics)|
|**2024-06-13**|**On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models**|Jinchuan Tian et.al.|[2406.09282v1](http://arxiv.org/abs/2406.09282v1)|null|
|**2024-06-13**|**Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback**|Hamish Ivison et.al.|[2406.09279v1](http://arxiv.org/abs/2406.09279v1)|[link](https://github.com/hamishivi/easylm)|
|**2024-06-13**|**End-to-end Streaming model for Low-Latency Speech Anonymization**|Waris Quamer et.al.|[2406.09277v1](http://arxiv.org/abs/2406.09277v1)|null|
|**2024-06-13**|**Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos**|Changan Chen et.al.|[2406.09272v1](http://arxiv.org/abs/2406.09272v1)|null|
|**2024-06-13**|**Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs**|Weixuan Wang et.al.|[2406.09265v1](http://arxiv.org/abs/2406.09265v1)|null|
|**2024-06-13**|**Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions**|Hua Shen et.al.|[2406.09264v1](http://arxiv.org/abs/2406.09264v1)|null|
|**2024-06-13**|**Deep Transformer Network for Monocular Pose Estimation of Ship-Based UAV**|Maneesha Wickramasuriya et.al.|[2406.09260v1](http://arxiv.org/abs/2406.09260v1)|null|
|**2024-06-13**|**MirrorCheck: Efficient Adversarial Defense for Vision-Language Models**|Samar Fares et.al.|[2406.09250v1](http://arxiv.org/abs/2406.09250v1)|null|
|**2024-06-13**|**Towards a Characterisation of Monte-Carlo Tree Search Performance in Different Games**|Dennis J. N. J. Soemers et.al.|[2406.09242v1](http://arxiv.org/abs/2406.09242v1)|null|
|**2024-06-13**|**On Softmax Direct Preference Optimization for Recommendation**|Yuxin Chen et.al.|[2406.09215v2](http://arxiv.org/abs/2406.09215v2)|[link](https://github.com/chenyuxin1999/s-dpo)|
|**2024-06-13**|**Investigating potential causes of Sepsis with Bayesian network structure learning**|Bruno Petrungaro et.al.|[2406.09207v1](http://arxiv.org/abs/2406.09207v1)|null|
|**2024-06-13**|**Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models**|Christopher Schröder et.al.|[2406.09206v1](http://arxiv.org/abs/2406.09206v1)|null|
|**2024-06-13**|**ReadCtrl: Personalizing text generation with readability-controlled instruction learning**|Hieu Tran et.al.|[2406.09205v1](http://arxiv.org/abs/2406.09205v1)|null|
|**2024-06-13**|**Language Complexity and Speech Recognition Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn't**|Chihiro Taguchi et.al.|[2406.09202v1](http://arxiv.org/abs/2406.09202v1)|null|
|**2024-06-13**|**Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations**|Mukhtar Mohamed et.al.|[2406.09200v1](http://arxiv.org/abs/2406.09200v1)|null|
|**2024-06-13**|**ReMI: A Dataset for Reasoning with Multiple Images**|Mehran Kazemi et.al.|[2406.09175v1](http://arxiv.org/abs/2406.09175v1)|null|
|**2024-06-13**|**Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning**|Bahare Fatemi et.al.|[2406.09170v1](http://arxiv.org/abs/2406.09170v1)|null|
|**2024-06-13**|**Fine-Grained Domain Generalization with Feature Structuralization**|Wenlong Yu et.al.|[2406.09166v1](http://arxiv.org/abs/2406.09166v1)|null|
|**2024-06-13**|**DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation**|A B M Ashikur Rahman et.al.|[2406.09155v1](http://arxiv.org/abs/2406.09155v1)|null|
|**2024-06-13**|**Diffusion Gaussian Mixture Audio Denoise**|Pu Wang et.al.|[2406.09154v1](http://arxiv.org/abs/2406.09154v1)|null|
|**2024-06-13**|**LASER: Learning by Aligning Self-supervised Representations of Speech for Improving Content-related Tasks**|Amit Meghanani et.al.|[2406.09153v1](http://arxiv.org/abs/2406.09153v1)|null|
|**2024-06-13**|**Generative AI-based Prompt Evolution Engineering Design Optimization With Vision-Language Model**|Melvin Wong et.al.|[2406.09143v2](http://arxiv.org/abs/2406.09143v2)|null|
|**2024-06-13**|**Investigating the translation capabilities of Large Language Models trained on parallel data only**|Javier García Gilabert et.al.|[2406.09140v1](http://arxiv.org/abs/2406.09140v1)|[link](https://github.com/projecte-aina/plume)|
|**2024-06-13**|**Leveraging Explicit Reasoning for Inference Integration in Commonsense-Augmented Dialogue Models**|Sarah E. Finch et.al.|[2406.09138v1](http://arxiv.org/abs/2406.09138v1)|null|
|**2024-06-13**|**Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs**|Xuan Zhang et.al.|[2406.09136v1](http://arxiv.org/abs/2406.09136v1)|[link](https://github.com/sail-sg/cpo)|
|**2024-06-13**|**RH-SQL: Refined Schema and Hardness Prompt for Text-to-SQL**|Jiawen Yi et.al.|[2406.09133v1](http://arxiv.org/abs/2406.09133v1)|null|
|**2024-06-13**|**Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning**|Haoxin Liu et.al.|[2406.09130v1](http://arxiv.org/abs/2406.09130v1)|null|
|**2024-06-13**|**CoastTerm: a Corpus for Multidisciplinary Term Extraction in Coastal Scientific Literature**|Julien Delaunay et.al.|[2406.09128v1](http://arxiv.org/abs/2406.09128v1)|null|
|**2024-06-13**|**PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation**|Injoon Hwang et.al.|[2406.09117v1](http://arxiv.org/abs/2406.09117v1)|null|
|**2024-06-13**|**Large-Scale Evaluation of Open-Set Image Classification Techniques**|Halil Bisgin et.al.|[2406.09112v1](http://arxiv.org/abs/2406.09112v1)|[link](https://github.com/aiml-ifi/openset-imagenet-comparison)|
|**2024-06-13**|**INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance**|Chenwei Lin et.al.|[2406.09105v1](http://arxiv.org/abs/2406.09105v1)|[link](https://github.com/fdu-ins/ins-mmbench)|
|**2024-06-13**|**Chain-of-Though (CoT) prompting strategies for medical error detection and correction**|Zhaolong Wu et.al.|[2406.09103v1](http://arxiv.org/abs/2406.09103v1)|null|
|**2024-06-13**|**SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models**|Kehua Feng et.al.|[2406.09098v1](http://arxiv.org/abs/2406.09098v1)|[link](https://github.com/hicai-zju/sciknoweval)|
|**2024-06-13**|**Modeling Comparative Logical Relation with Contrastive Learning for Text Generation**|Yuhao Dan et.al.|[2406.09095v1](http://arxiv.org/abs/2406.09095v1)|null|
|**2024-06-13**|**Suitability of KANs for Computer Vision: A preliminary investigation**|Basim Azam et.al.|[2406.09087v1](http://arxiv.org/abs/2406.09087v1)|null|
|**2024-06-13**|**Data-driven modeling and supervisory control system optimization for plug-in hybrid electric vehicles**|Hao Zhang et.al.|[2406.09082v1](http://arxiv.org/abs/2406.09082v1)|null|
|**2024-06-13**|**3M: Multi-modal Multi-task Multi-teacher Learning for Game Event Detection**|Thye Shan Ng et.al.|[2406.09076v1](http://arxiv.org/abs/2406.09076v1)|null|
|**2024-06-13**|**Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?**|Zhaochen Su et.al.|[2406.09072v1](http://arxiv.org/abs/2406.09072v1)|[link](https://github.com/zhaochen0110/cotempqa)|
|**2024-06-13**|**EquiPrompt: Debiasing Diffusion Models via Iterative Bootstrapping in Chain of Thoughts**|Zahraa Al Sahili et.al.|[2406.09070v1](http://arxiv.org/abs/2406.09070v1)|null|
|**2024-06-13**|**How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models**|Tarun Khajuria et.al.|[2406.09067v1](http://arxiv.org/abs/2406.09067v1)|null|
|**2024-06-13**|**CUDRT: Benchmarking the Detection of Human vs. Large Language Models Generated Texts**|Zhen Tao et.al.|[2406.09056v1](http://arxiv.org/abs/2406.09056v1)|null|
|**2024-06-13**|**MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning**|Hanqing Wang et.al.|[2406.09044v1](http://arxiv.org/abs/2406.09044v1)|null|
|**2024-06-13**|**Language Models are Crossword Solvers**|Soumadeep Saha et.al.|[2406.09043v1](http://arxiv.org/abs/2406.09043v1)|null|
|**2024-06-13**|**ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models**|Jing Liu et.al.|[2406.09041v1](http://arxiv.org/abs/2406.09041v1)|null|
|**2024-06-13**|**Deep learning empowered sensor fusion to improve infant movement classification**|Tomas Kulvicius et.al.|[2406.09014v2](http://arxiv.org/abs/2406.09014v2)|null|
|**2024-06-13**|**Bayesian Statistical Modeling with Predictors from LLMs**|Michael Franke et.al.|[2406.09012v1](http://arxiv.org/abs/2406.09012v1)|null|
|**2024-06-13**|**Fredformer: Frequency Debiased Transformer for Time Series Forecasting**|Xihao Piao et.al.|[2406.09009v2](http://arxiv.org/abs/2406.09009v2)|[link](https://github.com/chenzrg/fredformer)|
|**2024-06-13**|**LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models**|Xiaohao Yang et.al.|[2406.09008v1](http://arxiv.org/abs/2406.09008v1)|null|
|**2024-06-13**|**Introducing Brain-like Concepts to Embodied Hand-crafted Dialog Management System**|Frank Joublin et.al.|[2406.08996v1](http://arxiv.org/abs/2406.08996v1)|null|
|**2024-06-13**|**Multi-Agent Software Development through Cross-Team Collaboration**|Zhuoyun Du et.al.|[2406.08979v1](http://arxiv.org/abs/2406.08979v1)|[link](https://github.com/openbmb/chatdev)|
|**2024-06-13**|**XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning**|Alexander Nikulin et.al.|[2406.08973v1](http://arxiv.org/abs/2406.08973v1)|null|
|**2024-06-13**|**Separation Power of Equivariant Neural Networks**|Marco Pacini et.al.|[2406.08966v1](http://arxiv.org/abs/2406.08966v1)|null|
|**2024-06-13**|**Tool Wear Prediction in CNC Turning Operations using Ultrasonic Microphone Arrays and CNNs**|Jan Steckel et.al.|[2406.08957v1](http://arxiv.org/abs/2406.08957v1)|null|
|**2024-06-13**|**Word Order in English-Japanese Simultaneous Interpretation: Analyses and Evaluation using Chunk-wise Monotonic Translation**|Kosuke Doi et.al.|[2406.08940v1](http://arxiv.org/abs/2406.08940v1)|null|
|**2024-06-13**|**Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning**|Arnav Goel et.al.|[2406.08931v1](http://arxiv.org/abs/2406.08931v1)|[link](https://github.com/arnav10goel/camulenet)|
|**2024-06-13**|**Efficient Multi-View Fusion and Flexible Adaptation to View Missing in Cardiovascular System Signals**|Qihan Hu et.al.|[2406.08930v1](http://arxiv.org/abs/2406.08930v1)|null|
|**2024-06-13**|**Step-by-Step Diffusion: An Elementary Tutorial**|Preetum Nakkiran et.al.|[2406.08929v1](http://arxiv.org/abs/2406.08929v1)|null|
|**2024-06-13**|**Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors**|Ying Zhou et.al.|[2406.08922v1](http://arxiv.org/abs/2406.08922v1)|[link](https://github.com/zhouying20/ai-text-detector-evaluation)|
|**2024-06-13**|**AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis**|Swapnil Bhosale et.al.|[2406.08920v2](http://arxiv.org/abs/2406.08920v2)|null|
|**2024-06-13**|**An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios**|Cheng Gong et.al.|[2406.08911v1](http://arxiv.org/abs/2406.08911v1)|null|
|**2024-06-13**|**Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models**|Bowen Ping et.al.|[2406.08903v1](http://arxiv.org/abs/2406.08903v1)|null|
|**2024-06-13**|**No perspective, no perception!! Perspective-aware Healthcare Answer Summarization**|Gauri Naik et.al.|[2406.08881v1](http://arxiv.org/abs/2406.08881v1)|null|
|**2024-06-13**|**Research on Early Warning Model of Cardiovascular Disease Based on Computer Deep Learning**|Yuxiang Hu et.al.|[2406.08864v1](http://arxiv.org/abs/2406.08864v1)|null|
|**2024-06-13**|**Self-supervised Graph Neural Network for Mechanical CAD Retrieval**|Yuhan Quan et.al.|[2406.08863v1](http://arxiv.org/abs/2406.08863v1)|null|
|**2024-06-13**|**Plan, Generate and Complicate: Improving Low-resource Dialogue State Tracking via Easy-to-Difficult Zero-shot Data Augmentation**|Ming Gu et.al.|[2406.08860v1](http://arxiv.org/abs/2406.08860v1)|null|
|**2024-06-13**|**Current applications and potential future directions of reinforcement learning-based Digital Twins in agriculture**|Georg Goldenits et.al.|[2406.08854v1](http://arxiv.org/abs/2406.08854v1)|null|
|**2024-06-13**|**An Approach to Build Zero-Shot Slot-Filling System for Industry-Grade Conversational Assistants**|G P Shrivatsa Bhargav et.al.|[2406.08848v1](http://arxiv.org/abs/2406.08848v1)|null|
|**2024-06-13**|**ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions**|Xu Zhang et.al.|[2406.08842v1](http://arxiv.org/abs/2406.08842v1)|null|
|**2024-06-13**|**Research on Optimization of Natural Language Processing Model Based on Multimodal Deep Learning**|Dan Sun et.al.|[2406.08838v1](http://arxiv.org/abs/2406.08838v1)|null|
|**2024-06-13**|**Center-Sensitive Kernel Optimization for Efficient On-Device Incremental Learning**|Dingwen Zhang et.al.|[2406.08830v1](http://arxiv.org/abs/2406.08830v1)|null|
|**2024-06-13**|**Estimating Difficulty Levels of Programming Problems with Pre-trained Model**|Zhiyuan Wang et.al.|[2406.08828v1](http://arxiv.org/abs/2406.08828v1)|null|
|**2024-06-13**|**LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions**|Rumaisa Azeem et.al.|[2406.08824v1](http://arxiv.org/abs/2406.08824v1)|null|

#### Abstracts
##### **Explore the Limits of Omni-modal Pretraining at Scale**
2406.09412v1 by Yiyuan Zhang, Handong Li, Jing Liu, Xiangyu Yue

We propose to build omni-modal intelligence, which is capable of
understanding any modality and learning universal representations. In specific,
we propose a scalable pretraining paradigm, named Multimodal Context (MiCo),
which can scale up the numbers of modalities and amount of data, together with
the model parameters, in the pretraining process. With MiCo, the pretrained
models show significant emergent abilities in multimodal learning, which are
evaluated on the following tasks: i) single-modality perception benchmarks of
10 different modalities, ii) 25 cross-modality understanding tasks of
retrieval, question-answering, captioning, and iii) 18 multimodal large
language model benchmarks. Our models establish 37 new records for
state-of-the-art performance. We hope that our research could contribute to the
development of omni-modal intelligence. Code and Models are at
https://github.com/invictus717/MiCo

摘要：<paragraph>我們提議建立全模態智能，它能夠理解任何模態並學習通用表示。具體來說，我們提出了一個可擴展的預訓練範例，名為多模態上下文 (MiCo)，它可以在預訓練過程中擴展模態數量和數據量，以及模型參數。使用 MiCo，預訓練模型在多模態學習中展現出顯著的新興能力，這些能力在以下任務中得到評估：i) 10 種不同模態的單模態感知基準，ii) 25 個跨模態理解任務，包括檢索、問答、字幕，以及 iii) 18 個多模態大型語言模型基準。我們的模型為最先進的性能建立了 37 項新紀錄。我們希望我們的研究能為全模態智能的發展做出貢獻。程式碼和模型可在 https://github.com/invictus717/MiCo 找到</paragraph>

##### **MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding**
2406.09411v1 by Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen

We introduce MuirBench, a comprehensive benchmark that focuses on robust
multi-image understanding capabilities of multimodal LLMs. MuirBench consists
of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that
involve 10 categories of multi-image relations (e.g., multiview, temporal
relations). Comprising 11,264 images and 2,600 multiple-choice questions,
MuirBench is created in a pairwise manner, where each standard instance is
paired with an unanswerable variant that has minimal semantic differences, in
order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our
results reveal that even the best-performing models like GPT-4o and Gemini Pro
find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.
Open-source multimodal LLMs trained on single images can hardly generalize to
multi-image questions, hovering below 33.3% in accuracy. These results
highlight the importance of MuirBench in encouraging the community to develop
multimodal LLMs that can look beyond a single image, suggesting potential
pathways for future improvements.

摘要：我們推出 MuirBench，一個全面的基準，專注於多模態 LLM 的健全多圖像理解能力。MuirBench 包含 12 個多樣化的多圖像任務（例如場景理解、排序），涉及 10 類多圖像關係（例如多視圖、時間關係）。MuirBench 由 11,264 張圖像和 2,600 個多選題組成，以成對的方式建立，其中每個標準實例都與具有最小語義差異的不可回答變體配對，以便進行可靠的評估。在 20 個最近的多模態 LLM 上進行評估，我們的結果表明，即使是表現最好的模型，例如 GPT-4o 和 Gemini Pro，也發現解決 MuirBench 具有挑戰性，準確率分別為 68.0% 和 49.3%。在單個圖像上訓練的開源多模態 LLM 難以概括為多圖像問題，準確率低於 33.3%。這些結果強調了 MuirBench 的重要性，它鼓勵社群開發能夠超越單一圖像的多模態 LLM，並為未來的改進提出潛在途徑。

##### **Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach**
2406.09410v1 by Yansheng Li, Linlin Wang, Tingzhu Wang, Xue Yang, Junwei Luo, Qi Wang, Youming Deng, Wenbin Wang, Xian Sun, Haifeng Li, Bo Dang, Yongjun Zhang, Yi Yu, Junchi Yan

Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting
intelligent understanding of geospatial scenarios from perception to cognition.
In SAI, objects exhibit great variations in scales and aspect ratios, and there
exist rich relationships between objects (even between spatially disjoint
objects), which makes it necessary to holistically conduct SGG in large-size
very-high-resolution (VHR) SAI. However, the lack of SGG datasets with
large-size VHR SAI has constrained the advancement of SGG in SAI. Due to the
complexity of large-size VHR SAI, mining triplets <subject, relationship,
object> in large-size VHR SAI heavily relies on long-range contextual
reasoning. Consequently, SGG models designed for small-size natural imagery are
not directly applicable to large-size VHR SAI. To address the scarcity of
datasets, this paper constructs a large-scale dataset for SGG in large-size VHR
SAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, named
RSG, encompassing over 210,000 objects and more than 400,000 triplets. To
realize SGG in large-size VHR SAI, we propose a context-aware cascade cognition
(CAC) framework to understand SAI at three levels: object detection (OBD), pair
pruning and relationship prediction. As a fundamental prerequisite for SGG in
large-size SAI, a holistic multi-class object detection network (HOD-Net) that
can flexibly integrate multi-scale contexts is proposed. With the consideration
that there exist a huge amount of object pairs in large-size SAI but only a
minority of object pairs contain meaningful relationships, we design a pair
proposal generation (PPG) network via adversarial reconstruction to select
high-value pairs. Furthermore, a relationship prediction network with
context-aware messaging (RPCM) is proposed to predict the relationship types of
these pairs.

摘要：衛星影像 (SAI) 中的場景圖生成 (SGG) 有助於從感知到認知促進對地理空間場景的智能理解。在 SAI 中，物體在比例和長寬比上表現出很大的變化，並且物體之間存在豐富的關係（甚至在空間不相連的物體之間），這使得在大尺寸超高解析度 (VHR) SAI 中整體執行 SGG 變得有必要。然而，缺乏具有大尺寸 VHR SAI 的 SGG 資料集限制了 SGG 在 SAI 中的進展。由於大尺寸 VHR SAI 的複雜性，在大尺寸 VHR SAI 中挖掘三元組 <主體、關係、物體> 在很大程度上依賴於長程脈絡推理。因此，為小尺寸自然影像設計的 SGG 模型不直接適用於大尺寸 VHR SAI。為了解決資料集的稀缺性，本文構建了一個用於大尺寸 VHR SAI 中 SGG 的大規模資料集，影像大小從 512 x 768 到 27,860 x 31,096 像素，名為 RSG，包含超過 210,000 個物體和 400,000 多個三元組。為了在大尺寸 VHR SAI 中實現 SGG，我們提出了一個基於脈絡的串聯認知 (CAC) 框架，以在三個層次上理解 SAI：物體偵測 (OBD)、配對剪枝和關係預測。作為在大尺寸 SAI 中進行 SGG 的基本先決條件，提出了一個整體多類別物體偵測網路 (HOD-Net)，它可以靈活地整合多尺度脈絡。考慮到大尺寸 SAI 中存在大量物體對，但只有少數物體對包含有意義的關係，我們透過對抗式重建設計了一個配對提案生成 (PPG) 網路來選擇高價值配對。此外，還提出了一個具有基於脈絡的訊息傳遞的關係預測網路 (RPCM)，以預測這些配對的關係類型。

##### **4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities**
2406.09406v2 by Roman Bachmann, Oğuzhan Fatih Kar, David Mizrahi, Ali Garjani, Mingfei Gao, David Griffiths, Jiaming Hu, Afshin Dehghan, Amir Zamir

Current multimodal and multitask foundation models like 4M or UnifiedIO show
promising results, but in practice their out-of-the-box abilities to accept
diverse inputs and perform diverse tasks are limited by the (usually rather
small) number of modalities and tasks they are trained on. In this paper, we
expand upon the capabilities of them by training a single model on tens of
highly diverse modalities and by performing co-training on large-scale
multimodal datasets and text corpora. This includes training on several
semantic and geometric modalities, feature maps from recent state of the art
models like DINOv2 and ImageBind, pseudo labels of specialist models like SAM
and 4DHumans, and a range of new modalities that allow for novel ways to
interact with the model and steer the generation, for example image metadata or
color palettes. A crucial step in this process is performing discrete
tokenization on various modalities, whether they are image-like, neural network
feature maps, vectors, structured data like instance segmentation or human
poses, or data that can be represented as text. Through this, we expand on the
out-of-the-box capabilities of multimodal models and specifically show the
possibility of training one model to solve at least 3x more tasks/modalities
than existing ones and doing so without a loss in performance. This enables
more fine-grained and controllable multimodal generation capabilities and
allows us to study the distillation of models trained on diverse data and
objectives into a unified model. We successfully scale the training to a three
billion parameter model using tens of modalities and different datasets. The
resulting models and training code are open sourced at 4m.epfl.ch.

摘要：目前的 4M 或 UnifiedIO 等多模态和多任务基础模型显示出有希望的结果，但实际上，它们开箱即用的接受各种输入和执行各种任务的能力受到它们接受训练的模态和任务（通常相当少）数量的限制。在本文中，我们通过在数十种高度多样化的模态上训练单个模型，并在大规模多模态数据集和文本语料库上执行联合训练，扩展了它们的能力。这包括在几个语义和几何模态上进行训练，从 DINOv2 和 ImageBind 等最新模型的特征图，SAM 和 4DHumans 等专家模型的伪标签，以及一系列允许以新颖方式与模型交互和引导生成的新模态，例如图像元数据或调色板。此过程中的一个关键步骤是对各种模态执行离散标记化，无论它们是图像、神经网络特征图、向量、结构化数据（如实例分割或人体姿势），还是可以表示为文本的数据。通过此，我们扩展了多模态模型的开箱即用能力，并特别展示了训练一个模型来解决至少比现有模型多 3 倍的任务/模态的可能性，并且在不损失性能的情况下这样做。这实现了更细粒度和可控的多模态生成能力，并允许我们研究在不同数据和目标上训练的模型到统一模型的蒸馏。我们成功地将训练扩展到一个使用数十种模态和不同数据集的 30 亿参数模型。生成模型和训练代码在 4m.epfl.ch 开源。

##### **ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing**
2406.09404v1 by Jun-Kun Chen, Samuel Rota Bulò, Norman Müller, Lorenzo Porzi, Peter Kontschieder, Yu-Xiong Wang

This paper proposes ConsistDreamer - a novel framework that lifts 2D
diffusion models with 3D awareness and 3D consistency, thus enabling
high-fidelity instruction-guided scene editing. To overcome the fundamental
limitation of missing 3D consistency in 2D diffusion models, our key insight is
to introduce three synergetic strategies that augment the input of the 2D
diffusion model to become 3D-aware and to explicitly enforce 3D consistency
during the training process. Specifically, we design surrounding views as
context-rich input for the 2D diffusion model, and generate 3D-consistent,
structured noise instead of image-independent noise. Moreover, we introduce
self-supervised consistency-enforcing training within the per-scene editing
procedure. Extensive evaluation shows that our ConsistDreamer achieves
state-of-the-art performance for instruction-guided scene editing across
various scenes and editing instructions, particularly in complicated
large-scale indoor scenes from ScanNet++, with significantly improved sharpness
and fine-grained textures. Notably, ConsistDreamer stands as the first work
capable of successfully editing complex (e.g., plaid/checkered) patterns. Our
project page is at immortalco.github.io/ConsistDreamer.

摘要：本論文提出 ConsistDreamer - 一個新穎的框架，它提升了 2D 擴散模型，具備 3D 感知和 3D 一致性，從而實現高保真指令引導場景編輯。為了克服 2D 擴散模型中缺少 3D 一致性的基本限制，我們的關鍵見解是引入三種協同策略，擴充 2D 擴散模型的輸入，使其具備 3D 感知，並在訓練過程中明確執行 3D 一致性。具體來說，我們設計周圍視圖作為 2D 擴散模型的富含背景的輸入，並產生 3D 一致的結構化噪聲，而不是圖像無關的噪聲。此外，我們在場景編輯過程中引入了自監督一致性強制訓練。廣泛的評估表明，我們的 ConsistDreamer 在各種場景和編輯指令的指令引導場景編輯方面達到了最先進的性能，特別是在 ScanNet++ 中複雜的大型室內場景中，顯著提高了清晰度和精細紋理。值得注意的是，ConsistDreamer 是第一個能夠成功編輯複雜（例如格子/方格）圖案的作品。我們的專案頁面位於 immortalco.github.io/ConsistDreamer。

##### **Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models**
2406.09403v1 by Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Ranjay Krishna

Humans draw to facilitate reasoning: we draw auxiliary lines when solving
geometry problems; we mark and circle when reasoning on maps; we use sketches
to amplify our ideas and relieve our limited-capacity working memory. However,
such actions are missing in current multimodal language models (LMs). Current
chain-of-thought and tool-use paradigms only use text as intermediate reasoning
steps. In this work, we introduce Sketchpad, a framework that gives multimodal
LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts
planning and reasoning according to the visual artifacts it has drawn.
Different from prior work, which uses text-to-image models to enable LMs to
draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is
closer to human sketching and better facilitates reasoning. Sketchpad can also
use specialist vision models during the sketching process (e.g., draw bounding
boxes with object detection models, draw masks with segmentation models), to
further enhance visual perception and reasoning. We experiment with a wide
range of math tasks (including geometry, functions, graphs, and chess) and
complex visual reasoning tasks. Sketchpad substantially improves performance on
all tasks over strong base models with no sketching, yielding an average gain
of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a
new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial
reasoning (83.9%), and visual correspondence (80.8%). All codes and data are in
https://visualsketchpad.github.io/.

摘要：<paragraph>人類利用繪畫來促進推理：我們在解決幾何問題時會畫輔助線；在研究地圖時會標記和畫圈；我們使用草圖來擴展我們的想法並減輕我們容量有限的工作記憶。然而，目前的模態語言模型 (LM) 中缺少此類動作。目前的思考鏈和工具使用範例僅將文字用作中間推理步驟。在這項工作中，我們介紹了 Sketchpad，一個框架，它為多模態 LM 提供了一個視覺草圖本和可以在草圖本上繪畫的工具。LM 根據它繪製的視覺工件進行規劃和推理。不同於以前使用文字轉圖像模型使 LM 能夠繪畫的先前工作，Sketchpad 使 LM 能夠使用線條、方塊、標記等進行繪畫，這更接近於人類的素描，並且更好地促進了推理。Sketchpad 也可以在素描過程中使用專家視覺模型（例如，使用物件偵測模型繪製邊界框，使用分割模型繪製遮罩），以進一步增強視覺感知和推理。我們使用廣泛的數學任務（包括幾何、函數、圖形和西洋棋）和複雜的視覺推理任務進行了實驗。Sketchpad 大幅提升了所有任務在沒有素描的情況下強大的基礎模型的效能，在數學任務上平均提升 12.7%，在視覺任務上提升 8.6%。配備 Sketchpad 的 GPT-4o 在所有任務上都創下了新的技術水準，包括 V*Bench (80.3%)、BLINK 空間推理 (83.9%) 和視覺對應 (80.8%)。所有程式碼和資料都可以在 https://visualsketchpad.github.io/ 中找到。</paragraph>

##### **MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations**
2406.09401v1 by Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, Jiangmiao Pang

With the emergence of LLMs and their integration with other data modalities,
multi-modal 3D perception attracts more attention due to its connectivity to
the physical world and makes rapid progress. However, limited by existing
datasets, previous works mainly focus on understanding object properties or
inter-object spatial relationships in a 3D scene. To tackle this problem, this
paper builds the first largest ever multi-modal 3D scene dataset and benchmark
with hierarchical grounded language annotations, MMScan. It is constructed
based on a top-down logic, from region to object level, from a single target to
inter-target relationships, covering holistic aspects of spatial and attribute
understanding. The overall pipeline incorporates powerful VLMs via carefully
designed prompts to initialize the annotations efficiently and further involve
humans' correction in the loop to ensure the annotations are natural, correct,
and comprehensive. Built upon existing 3D scanning data, the resulting
multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects
and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding
and question-answering benchmarks. We evaluate representative baselines on our
benchmarks, analyze their capabilities in different aspects, and showcase the
key problems to be addressed in the future. Furthermore, we use this
high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and
obtain remarkable performance improvement both on existing benchmarks and
in-the-wild evaluation. Codes, datasets, and benchmarks will be available at
https://github.com/OpenRobotLab/EmbodiedScan.

摘要：<paragraph>隨著 LLM 的出現及其與其他資料模式的整合，多模式 3D 感知因其與物理世界的連通性而備受關注，並取得快速進展。然而，受限於現有資料集，先前的研究主要集中於理解 3D 場景中的物件屬性或物件間的空間關係。為了解決這個問題，本文建構了史上第一個最大的多模式 3D 場景資料集和基準，並使用分層的基礎語言註解，即 MMScan。它是根據由上而下的邏輯建構的，從區域到物件層級，從單一目標到目標間的關係，涵蓋了空間和屬性理解的整體面向。整體流程透過精心設計的提示整合強大的 VLM，以有效地初始化註解，並進一步讓人類參與循環中的修正，以確保註解自然、正確且全面。建立在現有的 3D 掃描資料之上，所產生的多模式 3D 資料集包含 109k 個物件和 7.7k 個區域的 1.4M 個元註解標題，以及超過 3.04M 個用於 3D 視覺基礎和問答基准的多樣化樣本。我們在我們的基準上評估具代表性的基準，分析它們在不同方面的能力，並展示未來需要解決的關鍵問題。此外，我們使用這個高品質的資料集來訓練最先進的 3D 視覺基礎和 LLM，並在現有的基準和實際評估中獲得顯著的效能提升。程式碼、資料集和基準將在 https://github.com/OpenRobotLab/EmbodiedScan 提供。</paragraph>

##### **Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion**
2406.09402v1 by Linzhan Mou, Jun-Kun Chen, Yu-Xiong Wang

This paper proposes Instruct 4D-to-4D that achieves 4D awareness and
spatial-temporal consistency for 2D diffusion models to generate high-quality
instruction-guided dynamic scene editing results. Traditional applications of
2D diffusion models in dynamic scene editing often result in inconsistency,
primarily due to their inherent frame-by-frame editing methodology. Addressing
the complexities of extending instruction-guided editing to 4D, our key insight
is to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems:
achieving temporal consistency in video editing and applying these edits to the
pseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P)
model with an anchor-aware attention module for batch processing and consistent
editing. Additionally, we integrate optical flow-guided appearance propagation
in a sliding window fashion for more precise frame-to-frame editing and
incorporate depth-based projection to manage the extensive data of pseudo-3D
scenes, followed by iterative editing to achieve convergence. We extensively
evaluate our approach in various scenes and editing instructions, and
demonstrate that it achieves spatially and temporally consistent editing
results, with significantly enhanced detail and sharpness over the prior art.
Notably, Instruct 4D-to-4D is general and applicable to both monocular and
challenging multi-camera scenes. Code and more results are available at
immortalco.github.io/Instruct-4D-to-4D.

摘要：本論文提出 Instruct 4D-to-4D，它為 2D 擴散模型實現了 4D 意識和時空一致性，以產生高品質的指示引導動態場景編輯結果。2D 擴散模型在動態場景編輯中的傳統應用通常會導致不一致，主要是由於它們固有的逐幀編輯方法。為了解決將指示引導編輯擴展到 4D 的複雜性，我們的關鍵見解是將 4D 場景視為偽 3D 場景，分解為兩個子問題：在影片編輯中實現時間一致性，並將這些編輯應用於偽 3D 場景。緊接著，我們首先使用錨點感知注意力模組增強 Instruct-Pix2Pix (IP2P) 模型，以進行批次處理和一致編輯。此外，我們以滑動視窗的方式整合光流引導的外觀傳播，以進行更精確的逐幀編輯，並結合基於深度投影來管理偽 3D 場景的廣泛資料，接著進行反覆編輯以實現收斂。我們廣泛評估了我們的方法在各種場景和編輯指示中的表現，並證明它實現了時空一致的編輯結果，與先前的技術相比，細節和清晰度顯著提升。值得注意的是，Instruct 4D-to-4D 具有通用性，適用於單眼和具有挑戰性的多鏡頭場景。程式碼和更多結果可在 immortalco.github.io/Instruct-4D-to-4D 取得。

##### **Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms**
2406.09397v1 by Miaosen Zhang, Yixuan Wei, Zhen Xing, Yifei Ma, Zuxuan Wu, Ji Li, Zheng Zhang, Qi Dai, Chong Luo, Xin Geng, Baining Guo

Modern vision models are trained on very large noisy datasets. While these
models acquire strong capabilities, they may not follow the user's intent to
output the desired results in certain aspects, e.g., visual aesthetic,
preferred style, and responsibility. In this paper, we target the realm of
visual aesthetics and aim to align vision models with human aesthetic standards
in a retrieval system. Advanced retrieval systems usually adopt a cascade of
aesthetic models as re-rankers or filters, which are limited to low-level
features like saturation and perform poorly when stylistic, cultural or
knowledge contexts are involved. We find that utilizing the reasoning ability
of large language models (LLMs) to rephrase the search query and extend the
aesthetic expectations can make up for this shortcoming. Based on the above
findings, we propose a preference-based reinforcement learning method that
fine-tunes the vision models to distill the knowledge from both LLMs reasoning
and the aesthetic models to better align the vision models with human
aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval
systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic
performance with their strong abilities. As aesthetic assessment is one of the
most subjective tasks, to validate the robustness of LMM, we further propose a
novel dataset named HPIR to benchmark the alignment with human aesthetics.
Experiments demonstrate that our method significantly enhances the aesthetic
behaviors of the vision models, under several metrics. We believe the proposed
algorithm can be a general practice for aligning vision models with human
values.

摘要：<paragraph>現代視覺模型在非常龐大且有雜訊的資料集上進行訓練。雖然這些模型具備強大的功能，但它們可能無法遵循使用者的意圖，在特定方面輸出理想的結果，例如視覺美學、偏好的風格和責任。在本文中，我們針對視覺美學領域，並旨在在檢索系統中將視覺模型與人類美學標準對齊。進階檢索系統通常採用一連串美學模型作為重新排序或篩選器，這些模型僅限於低階層功能，例如飽和度，而且在涉及風格、文化或知識脈絡時執行效果不佳。我們發現，利用大型語言模型 (LLM) 的推理能力來改寫搜尋查詢並延伸美學期望，可以彌補這個缺點。根據上述發現，我們提出一個基於偏好的強化學習方法，微調視覺模型以從 LLM 推理和美學模型中萃取出知識，讓視覺模型能更符合人類美學。同時，透過為評估檢索系統而設計的稀有基準，我們利用大型多模態模型 (LMM) 來評估美學表現及其強大能力。由於美學評估是最主觀的任務之一，為了驗證 LMM 的健全性，我們進一步提出一個名為 HPIR 的新資料集，以基準對齊人類美學。實驗證明，我們的模型在多項指標下顯著增強視覺模型的美學行為。我們相信，所提出的演算法可以成為將視覺模型與人類價值觀對齊的一般實務。</paragraph>

##### **A More Practical Approach to Machine Unlearning**
2406.09391v1 by David Zagardo

Machine learning models often incorporate vast amounts of data, raising
significant privacy concerns. Machine unlearning, the ability to remove the
influence of specific data points from a trained model, addresses these
concerns. This paper explores practical methods for implementing machine
unlearning, focusing on a first-epoch gradient-ascent approach.
  Key findings include: 1. Single vs. Multi-Epoch Unlearning: First-epoch
gradient unlearning is more effective than multi-epoch gradients. 2.
Layer-Based Unlearning: The embedding layer in GPT-2 is crucial for effective
unlearning. Gradients from the output layers (11 and 12) have no impact.
Efficient unlearning can be achieved using only the embedding layer, halving
space complexity. 3. Influence Functions & Scoring: Techniques like Hessian
Vector Product and the dot product of activations and tensors are used for
quantifying unlearning. 4. Gradient Ascent Considerations: Calibration is
necessary to avoid overexposing the model to specific data points during
unlearning, which could prematurely terminate the process. 5. Fuzzy Matching
vs. Iterative Unlearning: Fuzzy matching techniques shift the model to a new
optimum, while iterative unlearning provides a more complete modality.
  Our empirical evaluation confirms that first-epoch gradient ascent for
machine unlearning is more effective than whole-model gradient ascent. These
results highlight the potential of machine unlearning for enhancing data
privacy and compliance with regulations such as GDPR and CCPA. The study
underscores the importance of formal methods to comprehensively evaluate the
unlearning process.

摘要：機器學習模型通常會納入大量資料，引發重大的隱私問題。機器去學習，也就是從訓練好的模型中移除特定資料點影響力的能力，可以解決這些問題。這篇論文探討實作機器去學習的實用方法，重點放在第一個世代的梯度上升法。
主要發現包括：1. 單一世代與多世代去學習：第一個世代的梯度去學習比多個世代的梯度更有效。2. 基於層的去學習：GPT-2 中的嵌入層對於有效的去學習至關重要。來自輸出層（11 和 12）的梯度沒有影響。僅使用嵌入層就可以實現有效的去學習，將空間複雜度減半。3. 影響函數與評分：海森向量乘積和激活與張量的點積等技術用於量化去學習。4. 梯度上升考量：校準對於避免在去學習期間過度暴露模型於特定資料點是必要的，這可能會過早終止程序。5. 模糊比對與反覆去學習：模糊比對技術將模型轉移到新的最佳狀態，而反覆去學習則提供更完整的模式。
我們的實證評估證實，機器去學習的第一個世代梯度上升比全模型梯度上升更有效。這些結果突顯了機器去學習在加強資料隱私和符合 GDPR 和 CCPA 等法規方面的潛力。這項研究強調了正式方法對於全面評估去學習過程的重要性。

##### **Exploring the Spectrum of Visio-Linguistic Compositionality and Recognition**
2406.09388v1 by Youngtaek Oh, Pyunghwan Ahn, Jinhyung Kim, Gwangmo Song, Soonyoung Lee, In So Kweon, Junmo Kim

Vision and language models (VLMs) such as CLIP have showcased remarkable
zero-shot recognition abilities yet face challenges in visio-linguistic
compositionality, particularly in linguistic comprehension and fine-grained
image-text alignment. This paper explores the intricate relationship between
compositionality and recognition -- two pivotal aspects of VLM capability. We
conduct a comprehensive evaluation of existing VLMs, covering both pre-training
approaches aimed at recognition and the fine-tuning methods designed to improve
compositionality. Our evaluation employs 12 benchmarks for compositionality,
along with 21 zero-shot classification and two retrieval benchmarks for
recognition. In our analysis from 274 CLIP model checkpoints, we reveal
patterns and trade-offs that emerge between compositional understanding and
recognition accuracy. Ultimately, this necessitates strategic efforts towards
developing models that improve both capabilities, as well as the meticulous
formulation of benchmarks for compositionality. We open our evaluation
framework at https://github.com/ytaek-oh/vl_compo.

摘要：視覺和語言模型 (VLM)，例如 CLIP，展示了非凡的零次學習辨識能力，但卻在視覺語言的組成性方面面臨挑戰，特別是在語言理解和細緻的影像文字對齊上。本文探討了組成性和辨識之間的複雜關係，這是 VLM 能力的兩個關鍵面向。我們對現有的 VLM 進行全面評估，涵蓋了針對辨識的預訓練方法和旨在改善組成性的微調方法。我們的評估採用了 12 個組成性基準，以及 21 個零次學習分類和兩個用於辨識的檢索基準。在我們對 274 個 CLIP 模型檢查點的分析中，我們揭示了組成理解和辨識準確度之間出現的模式和權衡。最終，這需要策略性地努力開發能改善這兩種能力的模型，以及仔細制定組成性的基準。我們在 https://github.com/ytaek-oh/vl_compo 開放我們的評估架構。

##### **ElicitationGPT: Text Elicitation Mechanisms via Language Models**
2406.09363v1 by Yifan Wu, Jason Hartline

Scoring rules evaluate probabilistic forecasts of an unknown state against
the realized state and are a fundamental building block in the incentivized
elicitation of information and the training of machine learning models. This
paper develops mechanisms for scoring elicited text against ground truth text
using domain-knowledge-free queries to a large language model (specifically
ChatGPT) and empirically evaluates their alignment with human preferences. The
empirical evaluation is conducted on peer reviews from a peer-grading dataset
and in comparison to manual instructor scores for the peer reviews.

摘要：評分規則會評估未知狀態的機率預測，並針對已實現狀態，且為誘發資訊和訓練機器學習模型的基本架構。本篇論文發展出機制，使用對大型語言模型（特別是 ChatGPT）的無領域知識查詢，對引發的文字進行評分，並根據人類偏好對其比對進行經驗評估。經驗評估針對來自同儕評分資料集的同儕評論進行，並與同儕評論的手動指導評分進行比較。

##### **Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores**
2406.09346v1 by Álvaro Ciudad, Adrián Morales-Pastor, Laura Malo, Isaac Filella-Mercè, Victor Guallar, Alexis Molina

In this study, we present ScoreFormer, a novel graph transformer model
designed to accurately predict molecular docking scores, thereby optimizing
high-throughput virtual screening (HTVS) in drug discovery. The architecture
integrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk
Positional Encodings (LRWPE), enhancing the model's ability to understand
complex molecular structures and their relationship with their respective
docking scores. This approach significantly surpasses traditional HTVS methods
and recent Graph Neural Network (GNN) models in both recovery and efficiency
due to a wider coverage of the chemical space and enhanced performance. Our
results demonstrate that ScoreFormer achieves competitive performance in
docking score prediction and offers a substantial 1.65-fold reduction in
inference time compared to existing models. We evaluated ScoreFormer across
multiple datasets under various conditions, confirming its robustness and
reliability in identifying potential drug candidates rapidly.

摘要：在這項研究中，我們提出 ScoreFormer，一個新穎的圖形轉換器模型，旨在準確預測分子對接分數，從而優化藥物發現中的高通量虛擬篩選 (HTVS)。此架構整合了主鄰域聚合 (PNA) 和可學習隨機漫步位置編碼 (LRWPE)，增強模型理解複雜分子結構及其與各自對接分數的關係的能力。由於化學空間覆蓋範圍更廣且效能增強，此方法顯著超越傳統 HTVS 方法和最近的圖神經網路 (GNN) 模型，無論是在恢復還是效率方面。我們的結果證明，ScoreFormer 在對接分數預測中實現了競爭力表現，並且與現有模型相比，推論時間大幅減少了 1.65 倍。我們在各種條件下對多個資料集評估 ScoreFormer，證實了其在快速識別潛在藥物候選物方面的穩健性和可靠性。

##### **DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding**
2406.09345v1 by Suwon Shon, Kwangyoun Kim, Yi-Te Hsu, Prashant Sridhar, Shinji Watanabe, Karen Livescu

The integration of pre-trained text-based large language models (LLM) with
speech input has enabled instruction-following capabilities for diverse speech
tasks. This integration requires the use of a speech encoder, a speech adapter,
and an LLM, trained on diverse tasks. We propose the use of discrete speech
units (DSU), rather than continuous-valued speech encoder outputs, that are
converted to the LLM token embedding space using the speech adapter. We
generate DSU using a self-supervised speech encoder followed by k-means
clustering. The proposed model shows robust performance on speech inputs from
seen/unseen domains and instruction-following capability in spoken question
answering. We also explore various types of DSU extracted from different layers
of the self-supervised speech encoder, as well as Mel frequency Cepstral
Coefficients (MFCC). Our findings suggest that the ASR task and datasets are
not crucial in instruction-tuning for spoken question answering tasks.

摘要：透過將預先訓練好的基於文字的大語言模型 (LLM) 與語音輸入整合，已讓各種語音任務具備遵循指令的能力。這種整合需要使用在各種任務上受過訓練的語音編碼器、語音適配器和 LLM。我們建議使用離散語音單位 (DSU)，而非連續值語音編碼器輸出，這些輸出使用語音適配器轉換為 LLM 令牌嵌入空間。我們使用自監督語音編碼器，然後進行 k 平均群集，來產生 DSU。所提出的模型在來自已見/未見領域的語音輸入和口說問答中的遵循指令能力方面，展現出穩健的效能。我們也探索從自監督語音編碼器不同層中萃取出的各種 DSU 類型，以及梅爾頻率倒譜係數 (MFCC)。我們的發現顯示，ASR 任務和資料集對於口說問答任務的指令微調並非至關重要。

##### **ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models**
2406.09334v2 by David Anugraha, Genta Indra Winata, Chenyue Li, Patrick Amadeus Irawan, En-Shiun Annie Lee

Performance prediction is a method to estimate the performance of Language
Models (LMs) on various Natural Language Processing (NLP) tasks, mitigating
computational costs associated with model capacity and data for fine-tuning.
Our paper introduces ProxyLM, a scalable framework for predicting LM
performance using proxy models in multilingual tasks. These proxy models act as
surrogates, approximating the performance of the LM of interest. By leveraging
proxy models, ProxyLM significantly reduces computational overhead on task
evaluations, achieving up to a 37.08x speedup compared to traditional methods,
even with our smallest proxy models. Additionally, our methodology showcases
adaptability to previously unseen languages in pre-trained LMs, outperforming
the state-of-the-art performance by 1.89x as measured by root-mean-square error
(RMSE). This framework streamlines model selection, enabling efficient
deployment and iterative LM enhancements without extensive computational
resources.

摘要：效能預測是一種估計語言模型 (LM) 在各種自然語言處理 (NLP) 任務上效能的方法，可降低與模型容量和微調資料相關的運算成本。我們的論文介紹了 ProxyLM，這是一個可擴充的框架，可用於使用多語言任務中的代理模型預測 LM 效能。這些代理模型扮演代理角色，近似感興趣的 LM 效能。透過利用代理模型，ProxyLM 大幅降低了任務評估的運算負擔，與傳統方法相比，即使使用我們最小的代理模型，也能達到高達 37.08 倍的加速。此外，我們的技術展示了對預先訓練 LM 中以前未見語言的適應性，以均方根誤差 (RMSE) 衡量，其效能優於現有技術 1.89 倍。此框架簡化了模型選擇，即使沒有大量的運算資源，也能進行有效率的部署和反覆 LM 增強。

##### **Learning from Natural Language Explanations for Generalizable Entity Matching**
2406.09330v1 by Somin Wadhwa, Adit Krishnan, Runhui Wang, Byron C. Wallace, Chris Kong

Entity matching is the task of linking records from different sources that
refer to the same real-world entity. Past work has primarily treated entity
linking as a standard supervised learning problem. However, supervised entity
matching models often do not generalize well to new data, and collecting
exhaustive labeled training data is often cost prohibitive. Further, recent
efforts have adopted LLMs for this task in few/zero-shot settings, exploiting
their general knowledge. But LLMs are prohibitively expensive for performing
inference at scale for real-world entity matching tasks.
  As an efficient alternative, we re-cast entity matching as a conditional
generation task as opposed to binary classification. This enables us to
"distill" LLM reasoning into smaller entity matching models via natural
language explanations. This approach achieves strong performance, especially on
out-of-domain generalization tests (10.85% F-1) where standalone generative
methods struggle. We perform ablations that highlight the importance of
explanations, both for performance and model robustness.

摘要：實體配對是連結來自不同來源且指涉相同真實世界實體的記錄的任務。過去的工作主要將實體連結視為標準的監督式學習問題。然而，監督式實體配對模型通常無法很好地推廣到新的資料，而收集詳盡標記的訓練資料通常成本過高。此外，最近的工作已在少樣本/零樣本設定中採用 LLM 來執行這項任務，利用它們的一般知識。但 LLM 在執行大規模推論以進行真實世界實體配對任務時成本過高。
作為一種有效率的替代方案，我們將實體配對重新定義為條件生成任務，而不是二元分類。這使我們能夠透過自然語言說明將 LLM 推理「蒸餾」成較小的實體配對模型。此方法可達成強大的效能，特別是在獨立生成式方法難以應付的領域外推廣測試（10.85% F-1）上。我們執行消融研究，強調說明對於效能和模型穩健性都很重要。

##### **PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance**
2406.09326v1 by Qijun Gan, Song Wang, Shengtao Wu, Jianke Zhu

Recently, artificial intelligence techniques for education have been received
increasing attentions, while it still remains an open problem to design the
effective music instrument instructing systems. Although key presses can be
directly derived from sheet music, the transitional movements among key presses
require more extensive guidance in piano performance. In this work, we
construct a piano-hand motion generation benchmark to guide hand movements and
fingerings for piano playing. To this end, we collect an annotated dataset,
PianoMotion10M, consisting of 116 hours of piano playing videos from a
bird's-eye view with 10 million annotated hand poses. We also introduce a
powerful baseline model that generates hand motions from piano audios through a
position predictor and a position-guided gesture generator. Furthermore, a
series of evaluation metrics are designed to assess the performance of the
baseline model, including motion similarity, smoothness, positional accuracy of
left and right hands, and overall fidelity of movement distribution. Despite
that piano key presses with respect to music scores or audios are already
accessible, PianoMotion10M aims to provide guidance on piano fingering for
instruction purposes. The dataset and source code can be accessed at
https://agnjason.github.io/PianoMotion-page.

摘要：最近，人工智慧技術在教育領域受到越來越多的關注，而設計有效的樂器教學系統仍然是一個開放性的問題。儘管可以從樂譜中直接推演出按鍵，但在鋼琴演奏中，按鍵之間的過渡動作需要更廣泛的指導。在這項工作中，我們構建了一個鋼琴手部動作生成基準，用於指導鋼琴演奏中的手部動作和指法。為此，我們收集了一個帶註釋的數據集 PianoMotion10M，其中包含 116 小時的鋼琴演奏視頻，從鳥瞰視角拍攝，並帶有 1000 萬個帶註釋的手部姿勢。我們還引入了一個強大的基線模型，該模型通過位置預測器和位置引導手勢生成器從鋼琴音頻生成手部動作。此外，還設計了一系列評估指標來評估基線模型的性能，包括動作相似性、平滑性、左右手的定位準確性以及整體動作分佈保真度。儘管已經可以根據樂譜或音頻訪問鋼琴按鍵，但 PianoMotion10M 旨在為鋼琴指法提供指導，以供教學使用。可以在 https://agnjason.github.io/PianoMotion-page 訪問數據集和源代碼。

##### **REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space**
2406.09325v1 by Tomer Ashuach, Martin Tutek, Yonatan Belinkov

Large language models (LLMs) risk inadvertently memorizing and divulging
sensitive or personally identifiable information (PII) seen in training data,
causing privacy concerns. Current approaches to address this issue involve
costly dataset scrubbing, or model filtering through unlearning and model
editing, which can be bypassed through extraction attacks. We propose REVS, a
novel model editing method for unlearning sensitive information from LLMs. REVS
identifies and modifies a small subset of neurons relevant for each piece of
sensitive information. By projecting these neurons to the vocabulary space
(unembedding), we pinpoint the components driving its generation. We then
compute a model edit based on the pseudo-inverse of the unembedding matrix, and
apply it to de-promote generation of the targeted sensitive data. To adequately
evaluate our method on truly sensitive information, we curate two datasets: an
email dataset inherently memorized by GPT-J, and a synthetic social security
number dataset that we tune the model to memorize. Compared to other
state-of-the-art model editing methods, REVS demonstrates superior performance
in both eliminating sensitive information and robustness to extraction attacks,
while retaining integrity of the underlying model. The code and a demo notebook
are available at https://technion-cs-nlp.github.io/REVS.

摘要：大型語言模型 (LLM) 有風險會不經意地記憶和洩露在訓練資料中看到的敏感或個人可識別資訊 (PII)，造成隱私問題。目前解決此問題的方法包括代價高昂的資料集清除，或透過忘記和模型編輯來進行模型篩選，但這可能會被擷取攻擊繞過。我們提出 REVS，一種用於從 LLM 中忘記敏感資訊的新穎模型編輯方法。REVS 會識別並修改與每一段敏感資訊相關的小部分神經元。透過將這些神經元投影到詞彙空間（取消嵌入），我們可以精確找出驅動其產生的組成部分。接著，我們根據取消嵌入矩陣的偽逆來計算模型編輯，並套用它來降低目標敏感資料產生的優先順序。為了適當地評估我們的方法對真正敏感資訊的影響，我們策劃了兩個資料集：一個由 GPT-J 內建記憶的電子郵件資料集，以及一個我們調整模型以記憶的合成社會安全號碼資料集。與其他最先進的模型編輯方法相比，REVS 在消除敏感資訊和對抗擷取攻擊的強健性方面都展現出卓越的效能，同時保留基礎模型的完整性。程式碼和示範筆記本可以在 https://technion-cs-nlp.github.io/REVS 取得。

##### **Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs**
2406.09324v1 by Zhao Xu, Fan Liu, Hao Liu

Although Large Language Models (LLMs) have demonstrated significant
capabilities in executing complex tasks in a zero-shot manner, they are
susceptible to jailbreak attacks and can be manipulated to produce harmful
outputs. Recently, a growing body of research has categorized jailbreak attacks
into token-level and prompt-level attacks. However, previous work primarily
overlooks the diverse key factors of jailbreak attacks, with most studies
concentrating on LLM vulnerabilities and lacking exploration of
defense-enhanced LLMs. To address these issues, we evaluate the impact of
various attack settings on LLM performance and provide a baseline benchmark for
jailbreak attacks, encouraging the adoption of a standardized evaluation
framework. Specifically, we evaluate the eight key factors of implementing
jailbreak attacks on LLMs from both target-level and attack-level perspectives.
We further conduct seven representative jailbreak attacks on six defense
methods across two widely used datasets, encompassing approximately 320
experiments with about 50,000 GPU hours on A800-80G. Our experimental results
highlight the need for standardized benchmarking to evaluate these attacks on
defense-enhanced LLMs. Our code is available at
https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking.

摘要：儘管大型語言模型 (LLM) 已展現出以零次學習的方式執行複雜任務的顯著能力，它們仍容易受到越獄攻擊，並可能被操縱以產生有害的輸出。最近，越來越多的研究將越獄攻擊分類為代幣級別和提示級別攻擊。然而，先前的研究主要忽略了越獄攻擊的多種關鍵因素，大多數研究都集中在 LLM 漏洞上，而缺乏對防禦增強 LLM 的探索。為了解決這些問題，我們評估了各種攻擊設置對 LLM 效能的影響，並為越獄攻擊提供基線基準，鼓勵採用標準化的評估架構。具體來說，我們從目標層級和攻擊層級的角度評估了在 LLM 上實施越獄攻擊的八個關鍵因素。我們進一步對兩個廣泛使用的資料集上的六種防禦方法進行了七次代表性的越獄攻擊，涵蓋了大約 320 個實驗，在 A800-80G 上大約有 50,000 個 GPU 小時。我們的實驗結果強調了需要標準化的基準測試來評估這些對防禦增強 LLM 的攻擊。我們的程式碼可在 https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking 取得。

##### **JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models**
2406.09321v1 by Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, Anyu Wang

Jailbreak attacks aim to induce Large Language Models (LLMs) to generate
harmful responses for forbidden instructions, presenting severe misuse threats
to LLMs. Up to now, research into jailbreak attacks and defenses is emerging,
however, there is (surprisingly) no consensus on how to evaluate whether a
jailbreak attempt is successful. In other words, the methods to assess the
harmfulness of an LLM's response are varied, such as manual annotation or
prompting GPT-4 in specific ways. Each approach has its own set of strengths
and weaknesses, impacting their alignment with human values, as well as the
time and financial cost. This diversity in evaluation presents challenges for
researchers in choosing suitable evaluation methods and conducting fair
comparisons across different jailbreak attacks and defenses. In this paper, we
conduct a comprehensive analysis of jailbreak evaluation methodologies, drawing
from nearly ninety jailbreak research released between May 2023 and April 2024.
Our study introduces a systematic taxonomy of jailbreak evaluators, offering
in-depth insights into their strengths and weaknesses, along with the current
status of their adaptation. Moreover, to facilitate subsequent research, we
propose JailbreakEval, a user-friendly toolkit focusing on the evaluation of
jailbreak attempts. It includes various well-known evaluators out-of-the-box,
so that users can obtain evaluation results with only a single command.
JailbreakEval also allows users to customize their own evaluation workflow in a
unified framework with the ease of development and comparison. In summary, we
regard JailbreakEval to be a catalyst that simplifies the evaluation process in
jailbreak research and fosters an inclusive standard for jailbreak evaluation
within the community.

摘要：越獄攻擊旨在誘導大型語言模型 (LLM) 對禁止的指令產生有害的回應，對 LLM 構成嚴重的濫用威脅。到目前為止，對越獄攻擊和防禦的研究正在興起，然而，（令人驚訝的是）對於如何評估越獄嘗試是否成功尚未達成共識。換句話說，評估 LLM 回應的危害性的方法是多種多樣的，例如人工註釋或以特定方式提示 GPT-4。每種方法都有自己的一組優缺點，影響它們與人類價值觀的一致性，以及時間和財務成本。評估中的這種多樣性對研究人員在選擇合適的評估方法以及對不同的越獄攻擊和防禦進行公平比較提出了挑戰。在本文中，我們對越獄評估方法進行了全面的分析，借鑒了 2023 年 5 月至 2024 年 4 月之間發布的近 90 項越獄研究。我們的研究引入了一個系統的越獄評估器分類法，深入了解它們的優缺點，以及它們適應的當前狀態。此外，為了促進後續研究，我們提出了 JailbreakEval，這是一個專注於評估越獄嘗試的用戶友好工具包。它包含各種開箱即用的知名評估器，以便用戶只需一個命令即可獲得評估結果。JailbreakEval 還允許用戶在統一的框架中自定義自己的評估工作流程，並輕鬆進行開發和比較。總之，我們認為 JailbreakEval 是簡化越獄研究中評估過程的催化劑，並在社區內培養越獄評估的包容性標準。

##### **Characterising Interventions in Causal Games**
2406.09318v1 by Manuj Mishra, James Fox, Michael Wooldridge

Causal games are probabilistic graphical models that enable causal queries to
be answered in multi-agent settings. They extend causal Bayesian networks by
specifying decision and utility variables to represent the agents' degrees of
freedom and objectives. In multi-agent settings, whether each agent decides on
their policy before or after knowing the causal intervention is important as
this affects whether they can respond to the intervention by adapting their
policy. Consequently, previous work in causal games imposed chronological
constraints on permissible interventions. We relax this by outlining a sound
and complete set of primitive causal interventions so the effect of any
arbitrarily complex interventional query can be studied in multi-agent
settings. We also demonstrate applications to the design of safe AI systems by
considering causal mechanism design and commitment.

摘要：因果遊戲是一種機率圖形模型，可以在多重代理設定中回答因果查詢。它們透過指定決策和效用變數來延伸因果貝氏網路，以表示代理的自由度和目標。在多重代理設定中，每個代理在知道因果介入之前或之後決定其政策非常重要，因為這會影響他們是否能透過調整其政策來回應介入。因此，因果遊戲中的先前工作對允許的介入施加了時間順序限制。我們透過概述一組合理且完整的原始因果介入來放寬這一點，因此可以在多重代理設定中研究任何任意複雜介入查詢的效果。我們也透過考量因果機制設計和承諾來展示對安全 AI 系統設計的應用。

##### **Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers**
2406.09315v1 by Zhuolin Fu

In this paper, we show how Transformers can be interpreted as dense
Expectation-Maximization algorithms performed on Bayesian Nets. Based on the
above interpretation, we propose a new model design paradigm, namely Vertical
LoRA (VLoRA), which reduces the parameter count dramatically while preserving
performance. In VLoRA, a model consists of layers, each of which recursively
learns an increment based on the previous layer. We then apply LoRA
decomposition to the increments. VLoRA works on the base model, which is
orthogonal to LoRA, meaning they can be used together. We do experiments on
various tasks and models. The results show that 1) with VLoRA, the Transformer
model parameter count can be reduced dramatically and 2) the performance of the
original model is preserved. The source code is available at
\url{https://github.com/neverUseThisName/vlora}

摘要：在本文中，我們展示了如何將 Transformer 解釋為在貝氏網路中執行的密集期望最大化演算法。根據上述解釋，我們提出了一個新的模型設計範例，即垂直 LoRA (VLoRA)，它在保留效能的同時大幅減少了參數數量。在 VLoRA 中，模型由各個層組成，每個層都會遞迴地根據前一層學習增量。然後我們將 LoRA 分解應用於增量。VLoRA 運作於基礎模型，這與 LoRA 是正交的，意即它們可以一起使用。我們對各種任務和模型進行了實驗。結果表明，1) 使用 VLoRA，Transformer 模型參數數量可以大幅減少，2) 原始模型的效能得以保留。原始碼可在 \url{https://github.com/neverUseThisName/vlora} 取得

##### **Transformers meet Neural Algorithmic Reasoners**
2406.09308v1 by Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, Petar Veličković

Transformers have revolutionized machine learning with their simple yet
effective architecture. Pre-training Transformers on massive text datasets from
the Internet has led to unmatched generalization for natural language
understanding (NLU) tasks. However, such language models remain fragile when
tasked with algorithmic forms of reasoning, where computations must be precise
and robust. To address this limitation, we propose a novel approach that
combines the Transformer's language understanding with the robustness of graph
neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs
proved effective as generic solvers for algorithmic tasks, when specified in
graph form. To make their embeddings accessible to a Transformer, we propose a
hybrid architecture with a two-phase training procedure, allowing the tokens in
the language model to cross-attend to the node embeddings from the NAR. We
evaluate our resulting TransNAR model on CLRS-Text, the text-based version of
the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only
models for algorithmic reasoning, both in and out of distribution.

摘要：Transformer 憑藉其簡單但有效的架構，徹底改變了機器學習。在網際網路上的大量文字資料集上對 Transformer 進行預訓練，已為自然語言理解 (NLU) 任務帶來了無與倫比的泛化。然而，在需要演算法形式推理的任務中，此類語言模型仍然脆弱，因為計算必須精確且穩健。為了解決這個限制，我們提出了一種新方法，結合了 Transformer 的語言理解與圖形神經網路 (GNN) 為基礎的神經演算法推理器 (NAR) 的穩健性。此類 NAR 被證明在以圖形形式指定時，可作為演算法任務的通用解算器。為了讓 Transformer 可以存取其嵌入，我們提出了具有兩階段訓練程序的混合架構，讓語言模型中的符號可以交叉關注 NAR 中的節點嵌入。我們在 CLRS-Text（CLRS-30 基準的文字版本）上評估我們得到的 TransNAR 模型，並證明了在演算法推理中，無論是在分佈內或分佈外，都比僅使用 Transformer 的模型有顯著的進步。

##### **Parameter-Efficient Active Learning for Foundational models**
2406.09296v2 by Athmanarayanan Lakshmi Narayanan, Ranganath Krishnan, Amrutha Machireddy, Mahesh Subedar

Foundational vision transformer models have shown impressive few shot
performance on many vision tasks. This research presents a novel investigation
into the application of parameter efficient fine-tuning methods within an
active learning (AL) framework, to advance the sampling selection process in
extremely budget constrained classification tasks. The focus on image datasets,
known for their out-of-distribution characteristics, adds a layer of complexity
and relevance to our study. Through a detailed evaluation, we illustrate the
improved AL performance on these challenging datasets, highlighting the
strategic advantage of merging parameter efficient fine tuning methods with
foundation models. This contributes to the broader discourse on optimizing AL
strategies, presenting a promising avenue for future exploration in leveraging
foundation models for efficient and effective data annotation in specialized
domains.

摘要：基礎視覺變換器模型已在許多視覺任務上展現令人印象深刻的少量鏡頭表現。本研究提出對參數有效微調方法在主動學習 (AL) 架構中的應用進行一項新穎的調查，以推進極度預算受限分類任務中的抽樣選擇流程。專注於以其分布外特性而聞名的影像資料集，為我們的研究增添了一層複雜性和關聯性。透過詳細的評估，我們說明了這些具有挑戰性的資料集上改良的 AL 表現，強調了將參數有效微調方法與基礎模型合併的策略優勢。這有助於優化 AL 策略的廣泛討論，為在特定領域中利用基礎模型進行有效率且有效的資料標註，提出了一個有前景的未來探索途徑。

##### **AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models**
2406.09295v2 by Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, Yuxiao Dong

Evaluating the alignment capabilities of large Vision-Language Models (VLMs)
is essential for determining their effectiveness as helpful assistants.
However, existing benchmarks primarily focus on basic abilities using nonverbal
methods, such as yes-no and multiple-choice questions. In this paper, we
address this gap by introducing AlignMMBench, a comprehensive alignment
benchmark specifically designed for emerging Chinese VLMs. This benchmark is
meticulously curated from real-world scenarios and Chinese Internet sources,
encompassing thirteen specific tasks across three categories, and includes both
single-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite
strategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer
pairs. To facilitate the evaluation pipeline, we propose CritiqueVLM, a
rule-calibrated evaluator that exceeds GPT-4's evaluation ability. Finally, we
report the performance of representative VLMs on AlignMMBench, offering
insights into the capabilities and limitations of different VLM architectures.
All evaluation codes and data are available on https://alignmmbench.github.io.

摘要：評估大型視覺語言模型 (VLM) 的對齊能力對於確定它們作為有用輔助工具的有效性至關重要。
然而，現有的基準主要集中於使用非語言方法的基本能力，例如是非題和多選題。在本文中，我們通過引入 AlignMMBench 來解決這一差距，AlignMMBench 是一個專門為新興中文 VLM 設計的綜合對齊基準。此基準是從現實世界場景和中文網路來源精心策劃的，涵蓋三個類別中的 13 個具體任務，包括單輪和多輪對話場景。AlignMMBench 採用提示重寫策略，包含 1,054 張圖片和 4,978 個問答對。為了促進評估管道，我們提出了 CritiqueVLM，這是一個規則校準評估器，其評估能力超過 GPT-4。最後，我們報告了代表性 VLM 在 AlignMMBench 上的性能，提供了對不同 VLM 架構的能力和限制的見解。所有評估代碼和數據均可在 https://alignmmbench.github.io/ 上獲得。

##### **Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image Diffusion Models**
2406.09292v1 by Ziyi Wu, Yulia Rubanova, Rishabh Kabra, Drew A. Hudson, Igor Gilitschenski, Yusuf Aytar, Sjoerd van Steenkiste, Kelsey R. Allen, Thomas Kipf

We address the problem of multi-object 3D pose control in image diffusion
models. Instead of conditioning on a sequence of text tokens, we propose to use
a set of per-object representations, Neural Assets, to control the 3D pose of
individual objects in a scene. Neural Assets are obtained by pooling visual
representations of objects from a reference image, such as a frame in a video,
and are trained to reconstruct the respective objects in a different image,
e.g., a later frame in the video. Importantly, we encode object visuals from
the reference image while conditioning on object poses from the target frame.
This enables learning disentangled appearance and pose features. Combining
visual and 3D pose representations in a sequence-of-tokens format allows us to
keep the text-to-image architecture of existing models, with Neural Assets in
place of text tokens. By fine-tuning a pre-trained text-to-image diffusion
model with this information, our approach enables fine-grained 3D pose and
placement control of individual objects in a scene. We further demonstrate that
Neural Assets can be transferred and recomposed across different scenes. Our
model achieves state-of-the-art multi-object editing results on both synthetic
3D scene datasets, as well as two real-world video datasets (Objectron, Waymo
Open).

摘要：<paragraph>我們探討了影像擴散模型中的多物件 3D 姿勢控制問題。我們不針對文字符號序列進行條件設定，而是提議使用一組物件表示，也就是神經資產，來控制場景中個別物件的 3D 姿勢。神經資產是透過匯集參考影像（例如影片中的某個畫面）中物件的視覺表示而取得，並受訓練以重建不同影像（例如影片中較後的畫面）中的個別物件。重要的是，我們在對目標畫面中的物件姿勢進行條件設定的同時，對參考影像中的物件視覺效果進行編碼。這能學習解開糾纏的外觀和姿勢特徵。將視覺和 3D 姿勢表示以符號序列格式結合，讓我們得以保留現有模型的文字轉影像架構，並以神經資產取代文字符號。透過微調預先訓練好的文字轉影像擴散模型，我們的做法能對場景中個別物件進行細緻的 3D 姿勢和位置控制。我們進一步證明，神經資產可以在不同的場景中進行傳輸和重新組合。我們的模型在合成 3D 場景資料集，以及兩個真實世界影片資料集（Objectron、Waymo Open）上，都達到了最先進的多物件編輯結果。</paragraph>

##### **Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech**
2406.09290v1 by Martina Valente, Fabio Brugnara, Giovanni Morrone, Enrico Zovato, Leonardo Badino

This paper addresses spoken language identification (SLI) and speech
recognition of multilingual broadcast and institutional speech, real
application scenarios that have been rarely addressed in the SLI literature.
Observing that in these domains language changes are mostly associated with
speaker changes, we propose a cascaded system consisting of speaker diarization
and language identification and compare it with more traditional language
identification and language diarization systems. Results show that the proposed
system often achieves lower language classification and language diarization
error rates (up to 10% relative language diarization error reduction and 60%
relative language confusion reduction) and leads to lower WERs on multilingual
test sets (more than 8% relative WER reduction), while at the same time does
not negatively affect speech recognition on monolingual audio (with an absolute
WER increase between 0.1% and 0.7% w.r.t. monolingual ASR).

摘要：本文探討多語廣播和機構演講的口語辨識 (SLI) 和語音辨識，這些是 SLI 文獻中鮮少探討的實際應用場景。觀察到在這些領域中，語言變換大多與說話者變換有關，我們提出一個由說話者區分和語言辨識組成的串聯系統，並將其與更傳統的語言辨識和語言區分系統進行比較。結果顯示，所提出的系統通常能達到較低的語言分類和語言區分錯誤率（相對語言區分錯誤率降低多達 10%，相對語言混淆減少 60%），並在多語測試集中產生較低的 WER（相對 WER 降低超過 8%），同時不會對單語音訊的語音辨識產生負面影響（相較於單語音訊 ASR，絕對 WER 增加在 0.1% 到 0.7% 之間）。

##### **Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models**
2406.09289v1 by Sarah Ball, Frauke Kreuter, Nina Rimsky

Conversational Large Language Models are trained to refuse to answer harmful
questions. However, emergent jailbreaking techniques can still elicit unsafe
outputs, presenting an ongoing challenge for model alignment. To better
understand how different jailbreak types circumvent safeguards, this paper
analyses model activations on different jailbreak inputs. We find that it is
possible to extract a jailbreak vector from a single class of jailbreaks that
works to mitigate jailbreak effectiveness from other classes. This may indicate
that different kinds of effective jailbreaks operate via similar internal
mechanisms. We investigate a potential common mechanism of harmfulness feature
suppression, and provide evidence for its existence by looking at the
harmfulness vector component. These findings offer actionable insights for
developing more robust jailbreak countermeasures and lay the groundwork for a
deeper, mechanistic understanding of jailbreak dynamics in language models.

摘要：對話式大型語言模型經過訓練，可拒絕回答有害問題。然而，新興的越獄技術仍可引發不安全的輸出，對模型調整構成持續的挑戰。為了更了解不同類型的越獄如何規避安全防護措施，本文分析了模型在不同越獄輸入上的激活。我們發現，可以從一類越獄中提取一個越獄向量，用於減輕其他類越獄的越獄效力。這可能表明，不同類型的有效越獄通過類似的內部機制運作。我們研究了有害特徵抑制的潛在共同機制，並通過觀察有害向量組成部分來提供其存在的證據。這些發現為開發更強大的越獄對策提供了可行的見解，並為深入了解語言模型中的越獄動態奠定了基礎。

##### **On the Effects of Heterogeneous Data Sources on Speech-to-Text Foundation Models**
2406.09282v1 by Jinchuan Tian, Yifan Peng, William Chen, Kwanghee Choi, Karen Livescu, Shinji Watanabe

The Open Whisper-style Speech Model (OWSM) series was introduced to achieve
full transparency in building advanced speech-to-text (S2T) foundation models.
To this end, OWSM models are trained on 25 public speech datasets, which are
heterogeneous in multiple ways. In this study, we advance the OWSM series by
introducing OWSM v3.2, which improves on prior models by investigating and
addressing the impacts of this data heterogeneity. Our study begins with a
detailed analysis of each dataset, from which we derive two key strategies:
data filtering with proxy task to enhance data quality, and the incorporation
of punctuation and true-casing using an open large language model (LLM). With
all other configurations staying the same, OWSM v3.2 improves performance over
the OWSM v3.1 baseline while using 15% less training data.

摘要：Open Whisper 風格語音模型 (OWSM) 系列的推出，旨在建構進階語音轉文字 (S2T) 基礎模型時，達成完全透明。為此，OWSM 模型會在 25 個公開語音資料集上進行訓練，這些資料集在多方面都是異質的。在本研究中，我們透過推出 OWSM v3.2 來推進 OWSM 系列，藉由調查和處理資料異質性的影響，來改善先前的模型。我們的研究從對每個資料集的詳細分析開始，由此我們衍生出兩個關鍵策略：使用代理任務進行資料過濾以提升資料品質，以及使用開放大型語言模型 (LLM) 加入標點符號和真實大小寫。在所有其他設定都保持不變的情況下，OWSM v3.2 在使用少 15% 訓練資料的同時，改善了 OWSM v3.1 基準的效能。

##### **Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback**
2406.09279v1 by Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi

Learning from preference feedback has emerged as an essential step for
improving the generation quality and performance of modern language models
(LMs). Despite its widespread use, the way preference-based learning is applied
varies wildly, with differing data, learning algorithms, and evaluations used,
making disentangling the impact of each aspect difficult. In this work, we
identify four core aspects of preference-based learning: preference data,
learning algorithm, reward model, and policy training prompts, systematically
investigate the impact of these components on downstream model performance, and
suggest a recipe for strong learning for preference feedback. Our findings
indicate that all aspects are important for performance, with better preference
data leading to the largest improvements, followed by the choice of learning
algorithm, the use of improved reward models, and finally the use of additional
unlabeled prompts for policy training. Notably, PPO outperforms DPO by up to
2.5% in math and 1.2% in general domains. High-quality preference data leads to
improvements of up to 8% in instruction following and truthfulness. Despite
significant gains of up to 5% in mathematical evaluation when scaling up reward
models, we surprisingly observe marginal improvements in other categories.
  We publicly release the code used for training
(https://github.com/hamishivi/EasyLM) and evaluating
(https://github.com/allenai/open-instruct) our models, along with the models
and datasets themselves
(https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618).

摘要：<paragraph>從偏好回饋中學習已成為改善現代語言模型 (LM) 的生成品質和效能的必要步驟。儘管偏好式學習廣泛使用，但其應用方式卻大相逕庭，使用不同的資料、學習演算法和評量，使得難以釐清各個面向的影響。在這項工作中，我們找出偏好式學習的四個核心面向：偏好資料、學習演算法、獎勵模型和政策訓練提示，系統性地探討這些組成部分對下游模型效能的影響，並建議偏好回饋的強式學習配方。我們的發現指出，各個面向對於效能都至關重要，其中以更好的偏好資料帶來最大的改善，其次是學習演算法的選擇、改良獎勵模型的使用，最後是額外未標記提示在政策訓練中的使用。值得注意的是，PPO 在數學上優於 DPO 達 2.5%，在一般領域中優於 DPO 達 1.2%。高品質的偏好資料在遵循說明和真實性方面帶來高達 8% 的改善。儘管在擴充獎勵模型時，數學評量獲得高達 5% 的顯著提升，但我們驚訝地觀察到其他類別的改善幅度很小。我們公開發布用於訓練 (https://github.com/hamishivi/EasyLM) 和評量 (https://github.com/allenai/open-instruct) 我們的模型的程式碼，以及模型和資料集本身 (https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618)。</paragraph>

##### **End-to-end Streaming model for Low-Latency Speech Anonymization**
2406.09277v1 by Waris Quamer, Ricardo Gutierrez-Osuna

Speaker anonymization aims to conceal cues to speaker identity while
preserving linguistic content. Current machine learning based approaches
require substantial computational resources, hindering real-time streaming
applications. To address these concerns, we propose a streaming model that
achieves speaker anonymization with low latency. The system is trained in an
end-to-end autoencoder fashion using a lightweight content encoder that
extracts HuBERT-like information, a pretrained speaker encoder that extract
speaker identity, and a variance encoder that injects pitch and energy
information. These three disentangled representations are fed to a decoder that
resynthesizes the speech signal. We present evaluation results from two
implementations of our system, a full model that achieves a latency of 230ms,
and a lite version (0.1x in size) that further reduces latency to 66ms while
maintaining state-of-the-art performance in naturalness, intelligibility, and
privacy preservation.

摘要：講者匿名化旨在隱藏講者身分線索，同時保留語言內容。目前的機器學習方法需要大量的運算資源，阻礙了即時串流應用。為了解決這些問題，我們提出了一個串流模型，可以低延遲地實現講者匿名化。該系統以端到端自編碼器的方式進行訓練，使用輕量級內容編碼器提取類 HuBERT 的資訊、預訓練的講者編碼器提取講者身分，以及注入音高和能量資訊的變異編碼器。這三個解開糾纏的表示被饋送到一個解碼器，該解碼器重新合成語音訊號。我們展示了系統的兩個實作的評估結果，一個完整模型的延遲時間為 230ms，一個精簡版（大小為 0.1 倍）進一步將延遲時間減少到 66ms，同時在自然度、可理解性和隱私保護方面保持了最先進的效能。

##### **Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos**
2406.09272v1 by Changan Chen, Puyuan Peng, Ami Baid, Zihui Xue, Wei-Ning Hsu, David Harwarth, Kristen Grauman

Generating realistic audio for human interactions is important for many
applications, such as creating sound effects for films or virtual reality
games. Existing approaches implicitly assume total correspondence between the
video and audio during training, yet many sounds happen off-screen and have
weak to no correspondence with the visuals -- resulting in uncontrolled ambient
sounds or hallucinations at test time. We propose a novel ambient-aware audio
generation model, AV-LDM. We devise a novel audio-conditioning mechanism to
learn to disentangle foreground action sounds from the ambient background
sounds in in-the-wild training videos. Given a novel silent video, our model
uses retrieval-augmented generation to create audio that matches the visual
content both semantically and temporally. We train and evaluate our model on
two in-the-wild egocentric video datasets Ego4D and EPIC-KITCHENS. Our model
outperforms an array of existing methods, allows controllable generation of the
ambient sound, and even shows promise for generalizing to computer graphics
game clips. Overall, our work is the first to focus video-to-audio generation
faithfully on the observed visual content despite training from uncurated clips
with natural background sounds.

摘要：生成逼真的人类互动音频对于许多应用非常重要，例如为电影或虚拟现实游戏创建音效。现有方法在训练期间隐式地假定视频和音频之间完全对应，但许多声音发生在屏幕外，与视觉效果几乎或完全没有对应关系，从而导致在测试时出现不受控制的环境声音或幻觉。我们提出了一个新颖的环境感知音频生成模型 AV-LDM。我们设计了一种新颖的音频调节机制，以学习从野外训练视频中的环境背景声音中解开前景动作声音。给定一个新颖的无声视频，我们的模型使用检索增强生成来创建与视觉内容在语义和时间上都匹配的音频。我们在两个野外自我中心视频数据集 Ego4D 和 EPIC-KITCHENS 上训练和评估我们的模型。我们的模型优于现有的多种方法，允许可控生成环境声音，甚至显示出推广到计算机图形游戏剪辑的希望。总体而言，我们的工作首次专注于视频到音频生成，忠实于观察到的视觉内容，尽管是从具有自然背景声音的未经整理的剪辑中进行训练的。

##### **Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs**
2406.09265v1 by Weixuan Wang, Barry Haddow, Wei Peng, Alexandra Birch

Multilingual large language models (LLMs) have greatly increased the ceiling
of performance on non-English tasks. However the mechanisms behind
multilingualism in these LLMs are poorly understood. Of particular interest is
the degree to which internal representations are shared between languages.
Recent work on neuron analysis of LLMs has focused on the monolingual case, and
the limited work on the multilingual case has not considered the interaction
between tasks and linguistic representations. In our work, we investigate how
neuron activation is shared across languages by categorizing neurons into four
distinct groups according to their responses across different languages for a
particular input: all-shared, partial-shared, specific, and non-activated. This
categorization is combined with a study of neuron attribution, i.e. the
importance of a neuron w.r.t an output. Our analysis reveals the following
insights: (i) the linguistic sharing patterns are strongly affected by the type
of task, but neuron behaviour changes across different inputs even for the same
task; (ii) all-shared neurons play a key role in generating correct responses;
(iii) boosting multilingual alignment by increasing all-shared neurons can
enhance accuracy on multilingual tasks. The code is available at
https://github.com/weixuan-wang123/multilingual-neurons.

摘要：多語言大型語言模型 (LLM) 已大幅提升非英語任務的效能上限。然而，這些 LLM 中多語言機制背後的原理卻鮮為人知。特別令人感興趣的是，內部表徵在語言之間共享的程度。近期針對 LLM 的神經元分析研究主要集中在單一語言的情況，而針對多語言情況的有限研究並未考慮任務與語言表徵之間的互動。在我們的研究中，我們透過將神經元分類為四個不同的群組來探討神經元活化如何跨語言共享，這四個群組是根據神經元對特定輸入在不同語言中的反應來分類的：全共享、部分共享、特定和未活化。此分類結合了神經元歸因研究，即神經元相對於輸出的重要性。我們的分析揭示了以下見解：(i) 語言共享模式受到任務類型的強烈影響，但即使對於相同的任務，神經元行為也會隨著不同的輸入而改變；(ii) 全共享神經元在產生正確回應中扮演關鍵角色；(iii) 透過增加全共享神經元來提升多語言對齊，可以提高多語言任務的準確度。程式碼可在 https://github.com/weixuan-wang123/multilingual-neurons 取得。

##### **Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions**
2406.09264v1 by Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey P. Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith Ringel Morris, Paul Resnick, David Jurgens

Recent advancements in general-purpose AI have highlighted the importance of
guiding AI systems towards the intended goals, ethical principles, and values
of individuals and groups, a concept broadly recognized as alignment. However,
the lack of clarified definitions and scopes of human-AI alignment poses a
significant obstacle, hampering collaborative efforts across research domains
to achieve this alignment. In particular, ML- and philosophy-oriented alignment
research often views AI alignment as a static, unidirectional process (i.e.,
aiming to ensure that AI systems' objectives match humans) rather than an
ongoing, mutual alignment problem [429]. This perspective largely neglects the
long-term interaction and dynamic changes of alignment. To understand these
gaps, we introduce a systematic review of over 400 papers published between
2019 and January 2024, spanning multiple domains such as Human-Computer
Interaction (HCI), Natural Language Processing (NLP), Machine Learning (ML),
and others. We characterize, define and scope human-AI alignment. From this, we
present a conceptual framework of "Bidirectional Human-AI Alignment" to
organize the literature from a human-centered perspective. This framework
encompasses both 1) conventional studies of aligning AI to humans that ensures
AI produces the intended outcomes determined by humans, and 2) a proposed
concept of aligning humans to AI, which aims to help individuals and society
adjust to AI advancements both cognitively and behaviorally. Additionally, we
articulate the key findings derived from literature analysis, including
discussions about human values, interaction techniques, and evaluations. To
pave the way for future studies, we envision three key challenges for future
directions and propose examples of potential future solutions.

摘要：<paragraph>近期在通用 AI 領域的進展突顯出引導 AI 系統朝向預期目標、道德原則與個人和團體價值觀的重要性，而這項概念廣泛地被視為調整。然而，缺乏明確定義和人類與 AI 調整的範圍對跨研究領域合作努力達成此調整構成重大障礙。特別是，以機器學習和哲學為導向的調整研究通常將 AI 調整視為靜態、單向的過程（即，旨在確保 AI 系統的目標與人類相符），而不是持續的、相互調整的問題 [429]。此觀點在很大程度上忽略了調整的長期互動和動態變化。為了理解這些差距，我們針對 2019 年至 2024 年 1 月間發表的 400 多篇論文進行系統性回顧，涵蓋人機互動 (HCI)、自然語言處理 (NLP)、機器學習 (ML) 等多個領域。我們描述、定義和界定人類與 AI 的調整。從中，我們提出「雙向人類與 AI 調整」的概念架構，以人類為中心的觀點組織文獻。此架構包含 1) 傳統研究，即調整 AI 以符合人類，確保 AI 產生人類決定的預期結果，以及 2) 提出調整人類以符合 AI 的概念，旨在幫助個人和社會在認知和行為上適應 AI 的進步。此外，我們闡述從文獻分析中得出的主要發現，包括有關人類價值觀、互動技巧和評估的討論。為了為未來的研究鋪路，我們預見未來方向的三項關鍵挑戰，並提出潛在未來解決方案的範例。</paragraph>

##### **Deep Transformer Network for Monocular Pose Estimation of Ship-Based UAV**
2406.09260v1 by Maneesha Wickramasuriya, Taeyoung Lee, Murray Snyder

This paper introduces a deep transformer network for estimating the relative
6D pose of a Unmanned Aerial Vehicle (UAV) with respect to a ship using
monocular images. A synthetic dataset of ship images is created and annotated
with 2D keypoints of multiple ship parts. A Transformer Neural Network model is
trained to detect these keypoints and estimate the 6D pose of each part. The
estimates are integrated using Bayesian fusion. The model is tested on
synthetic data and in-situ flight experiments, demonstrating robustness and
accuracy in various lighting conditions. The position estimation error is
approximately 0.8\% and 1.0\% of the distance to the ship for the synthetic
data and the flight experiments, respectively. The method has potential
applications for ship-based autonomous UAV landing and navigation.

摘要：本文介紹一個深度 Transformer 網路，用於估算無人機 (UAV) 相對於船隻的 6D 相對位姿，方法是使用單眼影像。建立一個合成船隻影像資料集，並標註多個船隻部位的 2D 關鍵點。訓練一個 Transformer 神經網路模型來偵測這些關鍵點並估算每個部位的 6D 位姿。這些估計值使用貝氏融合進行整合。此模型在合成資料和現場飛行實驗中經過測試，證明在各種光照條件下都具有穩健性和準確性。對於合成資料和飛行實驗，位置估計誤差分別約為船隻距離的 0.8% 和 1.0%。此方法有潛力應用於船舶自主無人機著陸和導航。

##### **MirrorCheck: Efficient Adversarial Defense for Vision-Language Models**
2406.09250v1 by Samar Fares, Klea Ziu, Toluwani Aremu, Nikita Durasov, Martin Takáč, Pascal Fua, Karthik Nandakumar, Ivan Laptev

Vision-Language Models (VLMs) are becoming increasingly vulnerable to
adversarial attacks as various novel attack strategies are being proposed
against these models. While existing defenses excel in unimodal contexts, they
currently fall short in safeguarding VLMs against adversarial threats. To
mitigate this vulnerability, we propose a novel, yet elegantly simple approach
for detecting adversarial samples in VLMs. Our method leverages Text-to-Image
(T2I) models to generate images based on captions produced by target VLMs.
Subsequently, we calculate the similarities of the embeddings of both input and
generated images in the feature space to identify adversarial samples.
Empirical evaluations conducted on different datasets validate the efficacy of
our approach, outperforming baseline methods adapted from image classification
domains. Furthermore, we extend our methodology to classification tasks,
showcasing its adaptability and model-agnostic nature. Theoretical analyses and
empirical findings also show the resilience of our approach against adaptive
attacks, positioning it as an excellent defense mechanism for real-world
deployment against adversarial threats.

摘要：視覺語言模型 (VLM) 正變得越來越容易受到對抗性攻擊，因為針對這些模型提出了各種新穎的攻擊策略。雖然現有的防禦措施在單模態環境中表現出色，但它們目前無法保護 VLM 免受對抗性威脅。為了減輕這種漏洞，我們提出了一種新穎但優雅簡潔的方法來在 VLM 中偵測對抗性樣本。我們的模型利用文字到影像 (T2I) 模型根據目標 VLM 產生的標題來產生影像。隨後，我們計算特徵空間中輸入影像和產生影像的嵌入相似性，以識別對抗性樣本。在不同資料集上進行的經驗評估驗證了我們方法的效能，優於從影像分類領域改編的基準方法。此外，我們將我們的模型延伸到分類任務，展示其適應性和與模型無關的特性。理論分析和經驗發現也顯示了我們的方法對抗適應性攻擊的韌性，使其成為對抗現實世界對抗性威脅的絕佳防禦機制。

##### **Towards a Characterisation of Monte-Carlo Tree Search Performance in Different Games**
2406.09242v1 by Dennis J. N. J. Soemers, Guillaume Bams, Max Persoon, Marco Rietjens, Dimitar Sladić, Stefan Stefanov, Kurt Driessens, Mark H. M. Winands

Many enhancements to Monte-Carlo Tree Search (MCTS) have been proposed over
almost two decades of general game playing and other artificial intelligence
research. However, our ability to characterise and understand which variants
work well or poorly in which games is still lacking. This paper describes work
on an initial dataset that we have built to make progress towards such an
understanding: 268,386 plays among 61 different agents across 1494 distinct
games. We describe a preliminary analysis and work on training predictive
models on this dataset, as well as lessons learned and future plans for a new
and improved version of the dataset.

摘要：在將近二十年的通用遊戲遊玩與其他人工智慧研究中，蒙地卡羅樹狀搜尋 (MCTS) 已提出許多增強技術。然而，我們對於描述和理解哪些變體在哪些遊戲中表現良好或不佳的能力仍然不足。本文描述了我們建立的初始資料集，以朝向這樣的理解邁進：61 位不同代理人在 1494 個不同遊戲中進行 268,386 次遊玩。我們描述了對此資料集進行預測模型訓練的初步分析和工作，以及對資料集的新版和改良版學到的教訓和未來計畫。

##### **On Softmax Direct Preference Optimization for Recommendation**
2406.09215v2 by Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, Tat-Seng Chua

Recommender systems aim to predict personalized rankings based on user
preference data. With the rise of Language Models (LMs), LM-based recommenders
have been widely explored due to their extensive world knowledge and powerful
reasoning abilities. Most of the LM-based recommenders convert historical
interactions into language prompts, pairing with a positive item as the target
response and fine-tuning LM with a language modeling loss. However, the current
objective fails to fully leverage preference data and is not optimized for
personalized ranking tasks, which hinders the performance of LM-based
recommenders. Inspired by the current advancement of Direct Preference
Optimization (DPO) in human preference alignment and the success of softmax
loss in recommendations, we propose Softmax-DPO (S-DPO) to instill ranking
information into the LM to help LM-based recommenders distinguish preferred
items from negatives, rather than solely focusing on positives. Specifically,
we incorporate multiple negatives in user preference data and devise an
alternative version of DPO loss tailored for LM-based recommenders, connected
to softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax
loss over negative sampling and find that it has a side effect of mining hard
negatives, which assures its exceptional capabilities in recommendation tasks.
Empirically, extensive experiments conducted on three real-world datasets
demonstrate the superiority of S-DPO to effectively model user preference and
further boost recommendation performance while mitigating the data likelihood
decline issue of DPO. Our codes are available at
https://github.com/chenyuxin1999/S-DPO.

摘要：推薦系統旨在根據使用者偏好資料預測個人化排名。隨著語言模型 (LM) 的興起，基於 LM 的推薦系統因其廣泛的世界知識和強大的推理能力而被廣泛探索。大多數基於 LM 的推薦系統將歷史互動轉換為語言提示，與正向項目配對作為目標回應，並使用語言建模損失微調 LM。然而，目前的目標未能充分利用偏好資料，且並未針對個人化排名任務進行最佳化，這阻礙了基於 LM 的推薦系統的效能。受到人類偏好對齊中直接偏好最佳化 (DPO) 的最新進展以及推薦中 softmax 損失成功的啟發，我們提出 Softmax-DPO (S-DPO) 將排名資訊灌輸到 LM 中，以幫助基於 LM 的推薦系統區分偏好的項目和負面項目，而不仅仅是專注於正向項目。具體來說，我們在使用者偏好資料中納入多個負面項目，並設計出專門針對基於 LM 的推薦系統的 DPO 損失的替代版本，並連接到 softmax 採樣策略。理論上，我們將 S-DPO 與負面採樣的 softmax 損失聯繫起來，並發現它具有挖掘困難負面項目的副作用，這確保了它在推薦任務中的卓越能力。根據經驗，在三個真實世界資料集上進行的廣泛實驗證明了 S-DPO 在有效建模使用者偏好方面的優越性，並進一步提升了推薦效能，同時減輕了 DPO 的資料可能性下降問題。我們的程式碼可在 https://github.com/chenyuxin1999/S-DPO 取得。

##### **Investigating potential causes of Sepsis with Bayesian network structure learning**
2406.09207v1 by Bruno Petrungaro, Neville K. Kitson, Anthony C. Constantinou

Sepsis is a life-threatening and serious global health issue. This study
combines knowledge with available hospital data to investigate the potential
causes of Sepsis that can be affected by policy decisions. We investigate the
underlying causal structure of this problem by combining clinical expertise
with score-based, constraint-based, and hybrid structure learning algorithms. A
novel approach to model averaging and knowledge-based constraints was
implemented to arrive at a consensus structure for causal inference. The
structure learning process highlighted the importance of exploring data-driven
approaches alongside clinical expertise. This includes discovering unexpected,
although reasonable, relationships from a clinical perspective. Hypothetical
interventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and
Diabetes suggest that the presence of any of these risk factors in patients
increases the likelihood of Sepsis. This finding, alongside measuring the
effect of these risk factors on Sepsis, has potential policy implications.
Recognising the importance of prediction in improving Sepsis related health
outcomes, the model built is also assessed in its ability to predict Sepsis.
The predictions generated by the consensus model were assessed for their
accuracy, sensitivity, and specificity. These three indicators all had results
around 70%, and the AUC was 80%, which means the causal structure of the model
is reasonably accurate given that the models were trained on data available for
commissioning purposes only.

摘要：敗血症是一個危及生命且嚴重的全球性健康問題。本研究結合知識與現有的醫院資料，探討可受政策決策影響的敗血症潛在成因。我們結合臨床專業知識與基於分數、基於約束和混合結構學習演算法，探討這個問題的根本因果結構。實作了一種模型平均和基於知識的約束的新方法，以達成因果推論的共識結構。結構學習過程強調了在臨床專業知識的基礎上探索資料驅動方法的重要性。這包括從臨床觀點發現出乎意料，但合理的關係。對慢性阻塞性肺疾病、酒精依賴和糖尿病的假設性干預表明，患者存在任何這些風險因素都會增加敗血症的可能性。這個發現，以及測量這些風險因素對敗血症的影響，具有潛在的政策影響。認識到預測在改善敗血症相關健康結果中的重要性，所建立的模型也評估了其預測敗血症的能力。評估共識模型產生的預測的準確性、敏感性和特異性。這三個指標的結果都在 70% 左右，而 AUC 為 80%，這表示模型的因果結構相當準確，因為模型僅根據可用於委託目的的資料進行訓練。

##### **Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models**
2406.09206v1 by Christopher Schröder, Gerhard Heyer

Active learning is an iterative labeling process that is used to obtain a
small labeled subset, despite the absence of labeled data, thereby enabling to
train a model for supervised tasks such as text classification. While active
learning has made considerable progress in recent years due to improvements
provided by pre-trained language models, there is untapped potential in the
often neglected unlabeled portion of the data, although it is available in
considerably larger quantities than the usually small set of labeled data. Here
we investigate how self-training, a semi-supervised approach where a model is
used to obtain pseudo-labels from the unlabeled data, can be used to improve
the efficiency of active learning for text classification. Starting with an
extensive reproduction of four previous self-training approaches, some of which
are evaluated for the first time in the context of active learning or natural
language processing, we devise HAST, a new and effective self-training
strategy, which is evaluated on four text classification benchmarks, on which
it outperforms the reproduced self-training approaches and reaches
classification results comparable to previous experiments for three out of four
datasets, using only 25% of the data.

摘要：主動學習是一種反覆標籤的處理程序，用於取得一個小型的標籤子集，儘管沒有標籤資料，從而能夠訓練一個模型，以進行監督式任務，例如文字分類。雖然主動學習近年來由於預先訓練語言模型提供的改進而取得了相當的進展，但資料中常被忽略的未標籤部分仍有未開發的潛力，儘管它的數量遠大於通常的一小組標籤資料。在此，我們探討自訓練，一種半監督式方法，其中一個模型用於從未標籤資料取得偽標籤，如何用於提高主動學習文字分類的效率。從廣泛複製四種先前的自訓練方法開始，其中一些方法首次在主動學習或自然語言處理的背景下進行評估，我們設計了一個新的且有效的自訓練策略 HAST，它在四個文字分類基準上進行評估，在這些基準上，它優於複製的自訓練方法，並在四個資料集中的三個資料集上達到了與先前實驗相當的分類結果，僅使用了 25% 的資料。

##### **ReadCtrl: Personalizing text generation with readability-controlled instruction learning**
2406.09205v1 by Hieu Tran, Zonghai Yao, Lingxi Li, Hong Yu

Content generation conditioning on users's readability is an important
application for personalization. In an era of large language models (LLMs),
readability-controlled text generation based on LLMs has become increasingly
important. This paper introduces a novel methodology called
"Readability-Controlled Instruction Learning (ReadCtrl)," which aims to
instruction-tune LLMs to tailor users' readability levels. Unlike the
traditional methods, which primarily focused on categorical readability
adjustments typically classified as high, medium, and low or expert and
layperson levels with limited success, ReadCtrl introduces a dynamic framework
that enables LLMs to generate content at various (near continuous level)
complexity levels, thereby enhancing their versatility across different
applications. Our results show that the ReadCtrl-Mistral-7B models
significantly outperformed strong baseline models such as GPT-4 and Claude-3,
with a win rate of 52.1%:35.7% against GPT-4 in human evaluations. Furthermore,
Read-Ctrl has shown significant improvements in automatic evaluations, as
evidenced by better readability metrics (e.g., FOG, FKGL) and generation
quality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and
Coherence). These results underscore Read-Ctrl's effectiveness and tenacity in
producing high-quality, contextually appropriate outputs that closely align
with targeted readability levels, marking a significant advancement in
personalized content generation using LLMs.

摘要：內容生成依賴於使用者的可讀性，對個人化來說是一個重要的應用。在大語言模型（LLM）的時代，基於 LLM 的可讀性控制文本生成變得越來越重要。本文介紹了一種名為「可讀性控制指令學習（ReadCtrl）」的新方法，旨在對 LLM 進行指令微調，以調整使用者的可讀性等級。與傳統方法不同，傳統方法主要關注分類的可讀性調整，通常分類為高、中、低或專家和非專業人士等級，且成功有限，ReadCtrl 導入了一個動態架構，使 LLM 能夠生成各種（近乎連續等級）複雜程度的內容，從而增強其在不同應用中的多功能性。我們的結果顯示，ReadCtrl-Mistral-7B 模型顯著優於 GPT-4 和 Claude-3 等強大的基準模型，在人類評估中以 52.1%:35.7% 擊敗 GPT-4。此外，Read-Ctrl 在自動評估中也顯示出顯著的改進，這從更好的可讀性指標（例如 FOG、FKGL）和生成品質指標（例如 BLEU、SARI、SummaC-Factuality、UniEval-Consistency 和 Coherence）中可以得到證明。這些結果強調了 Read-Ctrl 在產生高品質、符合脈絡的輸出方面的有效性和韌性，這些輸出與目標可讀性等級緊密對齊，標誌著使用 LLM 進行個人化內容生成方面的一項重大進展。

##### **Language Complexity and Speech Recognition Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn't**
2406.09202v1 by Chihiro Taguchi, David Chiang

We investigate what linguistic factors affect the performance of Automatic
Speech Recognition (ASR) models. We hypothesize that orthographic and
phonological complexities both degrade accuracy. To examine this, we fine-tune
the multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25
languages with 15 writing systems, and we compare their ASR accuracy, number of
graphemes, unigram grapheme entropy, logographicity (how much
word/morpheme-level information is encoded in the writing system), and number
of phonemes. The results demonstrate that orthographic complexities
significantly correlate with low ASR accuracy, while phonological complexity
shows no significant correlation.

摘要：我們探討語言因素如何影響自動語音辨識 (ASR) 模型的效能。我們假設正字法和音韻的複雜性都會降低準確度。為了檢驗這一點，我們針對 25 種語言和 15 種書寫系統微調多語言自我監督預訓練模型 Wav2Vec2-XLSR-53，並比較它們的 ASR 準確度、字位數量、單字字位熵、表意文字（書寫系統中編碼多少字詞/語素層級資訊）和音位數量。結果表明，正字法複雜性與較低的 ASR 準確度顯著相關，而音韻複雜性則沒有顯著相關性。

##### **Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations**
2406.09200v1 by Mukhtar Mohamed, Oli Danyi Liu, Hao Tang, Sharon Goldwater

Self-supervised speech representations can hugely benefit downstream speech
technologies, yet the properties that make them useful are still poorly
understood. Two candidate properties related to the geometry of the
representation space have been hypothesized to correlate well with downstream
tasks: (1) the degree of orthogonality between the subspaces spanned by the
speaker centroids and phone centroids, and (2) the isotropy of the space, i.e.,
the degree to which all dimensions are effectively utilized. To study them, we
introduce a new measure, Cumulative Residual Variance (CRV), which can be used
to assess both properties. Using linear classifiers for speaker and phone ID to
probe the representations of six different self-supervised models and two
untrained baselines, we ask whether either orthogonality or isotropy correlate
with linear probing accuracy. We find that both measures correlate with
phonetic probing accuracy, though our results on isotropy are more nuanced.

摘要：自监督语音表征可以极大地造福下游语音技术，但使其有用的特性仍然知之甚少。已经假设与表征空间几何形状相关的两个候选特性与下游任务密切相关：(1) 说话者质心和音素质心所跨越的子空间之间的正交程度，以及 (2) 空间的各向同性，即所有维度得到有效利用的程度。为了研究它们，我们引入了一个新测量值，即累积残差方差 (CRV)，它可用于评估这两个特性。使用线性分类器对说话者和音素 ID 对六个不同的自监督模型和两个未经训练的基准表征进行探测，我们询问正交性或各向同性是否与线性探测准确性相关。我们发现这两个测量值都与音素探测准确性相关，尽管我们对各向同性的结果更为细致。

##### **ReMI: A Dataset for Reasoning with Multiple Images**
2406.09175v1 by Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal Awasthi, Dee Guo, Sreenivas Gollapudi, Ahmed Qureshi

With the continuous advancement of large language models (LLMs), it is
essential to create new benchmarks to effectively evaluate their expanding
capabilities and identify areas for improvement. This work focuses on
multi-image reasoning, an emerging capability in state-of-the-art LLMs. We
introduce ReMI, a dataset designed to assess LLMs' ability to Reason with
Multiple Images. This dataset encompasses a diverse range of tasks, spanning
various reasoning domains such as math, physics, logic, code, table/chart
understanding, and spatial and temporal reasoning. It also covers a broad
spectrum of characteristics found in multi-image reasoning scenarios. We have
benchmarked several cutting-edge LLMs using ReMI and found a substantial gap
between their performance and human-level proficiency. This highlights the
challenges in multi-image reasoning and the need for further research. Our
analysis also reveals the strengths and weaknesses of different models,
shedding light on the types of reasoning that are currently attainable and
areas where future models require improvement. To foster further research in
this area, we are releasing ReMI publicly:
https://huggingface.co/datasets/mehrankazemi/ReMI.

摘要：隨著大型語言模型 (LLM) 的持續進步，建立新的基準以有效評估其擴展能力並找出改進領域至關重要。這項工作重點在於多圖像推理，這是最先進的 LLM 中出現的能力。我們推出 ReMI，一個旨在評估 LLM 使用多個圖像推理能力的資料集。此資料集包含各種任務，涵蓋各種推理領域，例如數學、物理、邏輯、程式碼、表格/圖表理解，以及空間和時間推理。它還涵蓋了多圖像推理場景中發現的廣泛特徵。我們已使用 ReMI 對幾個尖端的 LLM 進行基準測試，發現其性能與人類水準之間存在顯著差距。這凸顯了多圖像推理中的挑戰和進一步研究的必要性。我們的分析也揭示了不同模型的優缺點，闡明了當前可實現的推理類型和未來模型需要改進的領域。為了促進此領域的進一步研究，我們公開發佈 ReMI：
https://huggingface.co/datasets/mehrankazemi/ReMI。

##### **Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning**
2406.09170v1 by Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, Bryan Perozzi

Large language models (LLMs) have showcased remarkable reasoning
capabilities, yet they remain susceptible to errors, particularly in temporal
reasoning tasks involving complex temporal logic. Existing research has
explored LLM performance on temporal reasoning using diverse datasets and
benchmarks. However, these studies often rely on real-world data that LLMs may
have encountered during pre-training or employ anonymization techniques that
can inadvertently introduce factual inconsistencies. In this work, we address
these limitations by introducing novel synthetic datasets specifically designed
to assess LLM temporal reasoning abilities in various scenarios. The diversity
of question types across these datasets enables systematic investigation into
the impact of the problem structure, size, question type, fact order, and other
factors on LLM performance. Our findings provide valuable insights into the
strengths and weaknesses of current LLMs in temporal reasoning tasks. To foster
further research in this area, we are open-sourcing the datasets and evaluation
framework used in our experiments: https://huggingface.co/datasets/baharef/ToT.

摘要：大型語言模型 (LLM) 已展現出非凡的推理能力，但它們仍容易出錯，特別是在涉及複雜時間邏輯的時間推理任務中。現有研究已使用不同的資料集和基準探討 LLM 在時間推理上的表現。然而，這些研究通常依賴於 LLM 在預訓練期間可能遇到的真實世界資料，或採用匿名化技術，而這可能會無意間造成事實上的不一致。在這項工作中，我們透過引入專門設計用來評估 LLM 在各種情境中時間推理能力的新型合成資料集來解決這些限制。這些資料集中問題類型的多樣性，讓我們能夠系統性地探討問題結構、大小、問題類型、事實順序和其他因素對 LLM 表現的影響。我們的研究結果提供了有價值的見解，讓我們了解當前 LLM 在時間推理任務中的優勢和劣勢。為了促進這個領域的進一步研究，我們開放原始碼資料集和在實驗中使用的評估架構：https://huggingface.co/datasets/baharef/ToT。

##### **Fine-Grained Domain Generalization with Feature Structuralization**
2406.09166v1 by Wenlong Yu, Dongyue Chen, Qilong Wang, Qinghua Hu

Fine-grained domain generalization (FGDG) is a more challenging task due to
its small inter-class variations and relatively large intra-class disparities.
When domain distribution changes, the fragility of subtle features leads to a
pronounced deterioration in model performance.Nevertheless, humans inherently
demonstrate the capacity for generalizing to out-of-distribution data,
leveraging structured multi-granularity knowledge that emerges from discerning
both the commonality and specificity within categories.Likewise, we propose a
Feature Structuralized Domain Generalization (FSDG) model, wherein features
experience structuralization into common, specific, and confounding segments,
harmoniously aligned with their relevant semantic concepts, to elevate
performance in FGDG. Specifically, feature structuralization (FS) is achieved
through a decorrelation function on disentangled segments, constraints on
common feature consistency, specific feature distinctiveness, and a prediction
calibration operation across granularities. By imposing these stipulations,
FSDG is prompted to disentangle and align features based on multi-granularity
knowledge, facilitating robust subtle distinctions among categories. Extensive
experimentation on three benchmarks consistently validates the superiority of
FSDG over state-of-the-art counterparts, with an average improvement of 6.1% in
terms of FGDG performance. Beyond that, the explainability analysis and
experiments on various mainstream model architectures confirm the validity of
FS.

摘要：細粒度領域泛化 (FGDG) 是一個更具挑戰性的任務，原因在於其類間變異小且類內差異相對較大。當領域分佈改變時，細微特徵的脆弱性會導致模型效能顯著惡化。儘管如此，人類天生就具備對超出分佈資料進行泛化的能力，並利用從辨別類別中的共性和特異性中產生的結構化多粒度知識。同樣地，我們提出了一個特徵結構化領域泛化 (FSDG) 模型，其中特徵會被結構化成共用、特定和混淆的區段，並與其相關的語義概念和諧一致，以提升 FGDG 的效能。具體來說，特徵結構化 (FS) 是透過對解糾纏區段進行去相關函數、對共用特徵一致性、特定特徵獨特性和跨粒度的預測校準操作的約束來實現的。透過實施這些規定，FSDG 會根據多粒度知識解糾纏和對齊特徵，促進類別之間穩健的細微區別。在三個基準上進行的廣泛實驗持續驗證了 FSDG 優於最先進的對應方法，在 FGDG 效能方面平均提升了 6.1%。除此之外，對各種主流模型架構的可解釋性分析和實驗證實了 FS 的有效性。

##### **DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation**
2406.09155v1 by A B M Ashikur Rahman, Saeed Anwar, Muhammad Usman, Ajmal Mian

Large Language Models (LLMs) have demonstrated remarkable capabilities,
revolutionizing the integration of AI in daily life applications. However, they
are prone to hallucinations, generating claims that contradict established
facts, deviating from prompts, and producing inconsistent responses when the
same prompt is presented multiple times. Addressing these issues is challenging
due to the lack of comprehensive and easily assessable benchmark datasets. Most
existing datasets are small and rely on multiple-choice questions, which are
inadequate for evaluating the generative prowess of LLMs. To measure
hallucination in LLMs, this paper introduces a comprehensive benchmark dataset
comprising over 75,000 prompts across eight domains. These prompts are designed
to elicit definitive, concise, and informative answers. The dataset is divided
into two segments: one publicly available for testing and assessing LLM
performance and a hidden segment for benchmarking various LLMs. In our
experiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and
Zephyr-revealing that overall factual hallucination ranges from 59% to 82% on
the public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment
hallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the
hidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%,
respectively. Domain-wise analysis shows that LLM performance significantly
deteriorates when asked for specific numeric information while performing
moderately with person, location, and date queries. Our dataset demonstrates
its efficacy and serves as a comprehensive benchmark for LLM performance
evaluation. Our dataset and LLMs responses are available at
\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.

摘要：大型語言模型 (LLM) 已展現出非凡的能力，徹底改變了 AI 在日常生活中應用的整合方式。然而，它們容易產生幻覺，提出與既定事實相矛盾的主張，偏離提示，以及在多次提出相同提示時產生不一致的回應。由於缺乏全面且易於評估的基準資料集，因此要解決這些問題具有挑戰性。大多數現有的資料集都很小，而且依賴於多選題，這不足以評估 LLM 的生成能力。為了衡量 LLM 中的幻覺，本文介紹了一個全面的基準資料集，其中包含八個領域的超過 75,000 個提示。這些提示旨在引發明確、簡潔且有資訊性的答案。該資料集分為兩個部分：一個公開可供測試和評估 LLM 效能，另一個隱藏部分則用於對各種 LLM 進行基準測試。在我們的實驗中，我們測試了六個 LLM-GPT-3.5、Llama 2、Llama 3、Gemini、Mixtral 和 Zephyr，結果顯示在公開資料集上的整體事實幻覺範圍從 59% 到 82%，在隱藏基準測試中的範圍則從 57% 到 76%。提示錯位幻覺在公開資料集中範圍從 6% 到 95%，在隱藏對應部分中範圍從 17% 到 94%。平均一致性分別從 21% 到 61% 和 22% 到 63%。按領域分析顯示，當被要求提供具體數字資訊時，LLM 效能會顯著下降，而在處理人物、地點和日期查詢時表現中等。我們的資料集證明了其效力，並作為 LLM 效能評估的全面基準。我們的資料集和 LLM 回應可在 \href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn} 取得。

##### **Diffusion Gaussian Mixture Audio Denoise**
2406.09154v1 by Pu Wang, Junhui Li, Jialu Li, Liangdong Guo, Youshan Zhang

Recent diffusion models have achieved promising performances in
audio-denoising tasks. The unique property of the reverse process could recover
clean signals. However, the distribution of real-world noises does not comply
with a single Gaussian distribution and is even unknown. The sampling of
Gaussian noise conditions limits its application scenarios. To overcome these
challenges, we propose a DiffGMM model, a denoising model based on the
diffusion and Gaussian mixture models. We employ the reverse process to
estimate parameters for the Gaussian mixture model. Given a noisy audio signal,
we first apply a 1D-U-Net to extract features and train linear layers to
estimate parameters for the Gaussian mixture model, and we approximate the real
noise distributions. The noisy signal is continuously subtracted from the
estimated noise to output clean audio signals. Extensive experimental results
demonstrate that the proposed DiffGMM model achieves state-of-the-art
performance.

摘要：最近的擴散模型在音訊去噪任務中取得了令人滿意的表現。反向程序的獨特屬性可以復原乾淨的訊號。然而，真實世界雜訊的分布並不符合單一的高斯分布，甚至未知。高斯雜訊條件的取樣限制了其應用場景。為了克服這些挑戰，我們提出了 DiffGMM 模型，這是一個基於擴散和高斯混合模型的去噪模型。我們採用反向程序來估計高斯混合模型的參數。給定一個有雜訊的音訊訊號，我們首先應用 1D-U-Net 來提取特徵並訓練線性層以估計高斯混合模型的參數，並且我們近似真實的雜訊分佈。從估計的雜訊中持續減去有雜訊的訊號以輸出乾淨的音訊訊號。大量的實驗結果表明，所提出的 DiffGMM 模型達到了最先進的效能。

##### **LASER: Learning by Aligning Self-supervised Representations of Speech for Improving Content-related Tasks**
2406.09153v1 by Amit Meghanani, Thomas Hain

Self-supervised learning (SSL)-based speech models are extensively used for
full-stack speech processing. However, it has been observed that improving
SSL-based speech representations using unlabeled speech for content-related
tasks is challenging and computationally expensive. Recent attempts have been
made to address this issue with cost-effective self-supervised fine-tuning
(SSFT) approaches. Continuing in this direction, a cost-effective SSFT method
named "LASER: Learning by Aligning Self-supervised Representations" is
presented. LASER is based on the soft-DTW alignment loss with temporal
regularisation term. Experiments are conducted with HuBERT and WavLM models and
evaluated on the SUPERB benchmark for two content-related tasks: automatic
speech recognition (ASR) and phoneme recognition (PR). A relative improvement
of 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the
ASR and PR tasks respectively, with only < 3 hours of fine-tuning on a single
GPU.

摘要：基於自我監督學習 (SSL) 的語音模型廣泛用於全端語音處理。然而，已觀察到使用與內容相關任務的未標記語音來改進基於 SSL 的語音表示具有挑戰性且在計算上很昂貴。最近已嘗試使用具有成本效益的自我監督微調 (SSFT) 方法來解決此問題。繼續朝這個方向，提出了一個名為「LASER：透過比對自我監督表示進行學習」的具有成本效益的 SSFT 方法。LASER 基於具有時間正則化項的軟 DTW 對齊損失。使用 HuBERT 和 WavLM 模型進行實驗，並針對兩個與內容相關的任務（自動語音辨識 (ASR) 和音素辨識 (PR)）在 SUPERB 基準上進行評估。對於 HuBERT，ASR 和 PR 任務分別觀察到 3.7% 和 8.2% 的相對改進，而 WavLM 則觀察到 4.1% 和 11.7%，而僅在單一 GPU 上進行不到 3 小時的微調。

##### **Generative AI-based Prompt Evolution Engineering Design Optimization With Vision-Language Model**
2406.09143v2 by Melvin Wong, Thiago Rios, Stefan Menzel, Yew Soon Ong

Engineering design optimization requires an efficient combination of a 3D
shape representation, an optimization algorithm, and a design performance
evaluation method, which is often computationally expensive. We present a
prompt evolution design optimization (PEDO) framework contextualized in a
vehicle design scenario that leverages a vision-language model for penalizing
impractical car designs synthesized by a generative model. The backbone of our
framework is an evolutionary strategy coupled with an optimization objective
function that comprises a physics-based solver and a vision-language model for
practical or functional guidance in the generated car designs. In the prompt
evolutionary search, the optimizer iteratively generates a population of text
prompts, which embed user specifications on the aerodynamic performance and
visual preferences of the 3D car designs. Then, in addition to the
computational fluid dynamics simulations, the pre-trained vision-language model
is used to penalize impractical designs and, thus, foster the evolutionary
algorithm to seek more viable designs. Our investigations on a car design
optimization problem show a wide spread of potential car designs generated at
the early phase of the search, which indicates a good diversity of designs in
the initial populations, and an increase of over 20\% in the probability of
generating practical designs compared to a baseline framework without using a
vision-language model. Visual inspection of the designs against the performance
results demonstrates prompt evolution as a very promising paradigm for finding
novel designs with good optimization performance while providing ease of use in
specifying design specifications and preferences via a natural language
interface.

摘要：<paragraph>工程設計最佳化需要結合 3D 形狀表示、最佳化演算法和設計效能評估方法，這些方法通常在運算上很耗費資源。我們提出一個情境化於車輛設計場景的提示演化設計最佳化 (PEDO) 架構，該架構利用視覺語言模型來懲罰生成模型合成的非實際汽車設計。我們的架構主幹是一個演化策略，結合一個包含基於物理求解器和視覺語言模型的最佳化目標函數，用於在生成的汽車設計中提供實用或功能指導。在提示演化搜尋中，最佳化器會反覆產生一組文字提示，其中包含使用者對 3D 汽車設計的空氣動力效能和視覺偏好的規格。然後，除了計算流體力學模擬外，預先訓練好的視覺語言模型用於懲罰非實際設計，並因此促使演化演算法尋找更多可行的設計。我們對汽車設計最佳化問題的調查顯示，在搜尋的早期階段產生了廣泛的潛在汽車設計，這表示初始族群中有良好的設計多樣性，並且與不使用視覺語言模型的基準架構相比，產生實用設計的機率增加了 20% 以上。根據效能結果對設計進行視覺檢查，證明提示演化是一種非常有前途的範例，可以用於尋找具有良好最佳化效能的新穎設計，同時透過自然語言介面輕鬆指定設計規格和偏好。</paragraph>

##### **Investigating the translation capabilities of Large Language Models trained on parallel data only**
2406.09140v1 by Javier García Gilabert, Carlos Escolano, Aleix Sant Savall, Francesca De Luca Fornaciari, Audrey Mash, Xixian Liao, Maite Melero

In recent years, Large Language Models (LLMs) have demonstrated exceptional
proficiency across a broad spectrum of Natural Language Processing (NLP) tasks,
including Machine Translation. However, previous methods predominantly relied
on iterative processes such as instruction fine-tuning or continual
pre-training, leaving unexplored the challenges of training LLMs solely on
parallel data. In this work, we introduce PLUME (Parallel Language Model), a
collection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and
256k) trained exclusively on Catalan-centric parallel examples. These models
perform comparably to previous encoder-decoder architectures on 16 supervised
translation directions and 56 zero-shot ones. Utilizing this set of models, we
conduct a thorough investigation into the translation capabilities of LLMs,
probing their performance, the impact of the different elements of the prompt,
and their cross-lingual representation space.

摘要：近年來，大型語言模型 (LLM) 已展現出在廣泛自然語言處理 (NLP) 任務中的傑出能力，包括機器翻譯。然而，先前的做法主要依賴於反覆的程序，例如指令微調或持續預訓練，而未探索僅使用平行資料訓練 LLM 的挑戰。在這項工作中，我們引入了 PLUME（平行語言模型），一個由三個 2B LLM 組成的集合，它們具有不同的詞彙量（32k、128k 和 256k），並專門使用以加泰羅尼亞語為中心的平行範例進行訓練。這些模型在 16 個監督式翻譯方向和 56 個零次學習方向上與先前的編碼器-解碼器架構表現相當。利用這組模型，我們對 LLM 的翻譯能力進行了徹底調查，探討了它們的效能、提示不同元素的影響，以及它們的跨語言表示空間。

##### **Leveraging Explicit Reasoning for Inference Integration in Commonsense-Augmented Dialogue Models**
2406.09138v1 by Sarah E. Finch, Jinho D. Choi

Open-domain dialogue systems need to grasp social commonsense to understand
and respond effectively to human users. Commonsense-augmented dialogue models
have been proposed that aim to infer commonsense knowledge from dialogue
contexts in order to improve response quality. However, existing approaches to
commonsense-augmented dialogue rely on implicit reasoning to integrate
commonsense inferences during response generation. In this study, we explore
the impact of explicit reasoning against implicit reasoning over commonsense
for dialogue response generation. Our findings demonstrate that separating
commonsense reasoning into explicit steps for generating, selecting, and
integrating commonsense into responses leads to better dialogue interactions,
improving naturalness, engagement, specificity, and overall quality. Subsequent
analyses of these findings unveil insights into the effectiveness of various
types of commonsense in generating responses and the particular response traits
enhanced through explicit reasoning for commonsense integration. Our work
advances research in open-domain dialogue by achieving a new state-of-the-art
in commonsense-augmented response generation.

摘要：開放領域對話系統需要掌握社會常識才能理解人類使用者的對話並有效回應。已提出增強常識的對話模型，旨在從對話脈絡中推論常識知識，以提升回應品質。然而，現有的增強常識對話方法依賴於隱式推理，在回應產生期間整合常識推論。在本研究中，我們探討了對常識進行顯式推理與隱式推理的影響，以產生對話回應。我們的研究結果表明，將常識推理分為產生、選擇和整合常識到回應中的明確步驟，會帶來更好的對話互動，提升自然性、參與度、特殊性，以及整體品質。隨後對這些發現的分析揭示了各種常識在產生回應中的有效性，以及透過顯式推理增強常識整合的特定回應特質。我們的研究透過在增強常識的回應產生中達成新的最先進技術，推動了開放領域對話的研究。

##### **Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs**
2406.09136v1 by Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, Min Lin

The recent development of chain-of-thought (CoT) decoding has enabled large
language models (LLMs) to generate explicit logical reasoning paths for complex
problem-solving. However, research indicates that these paths are not always
deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching
to extensively explore the reasoning space and find better reasoning paths that
CoT decoding might overlook. This deliberation, however, comes at the cost of
significantly increased inference complexity. In this work, we demonstrate that
fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to
achieve similar or better performance, thereby avoiding the substantial
inference burden. This is achieved through Chain of Preference Optimization
(CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths
with those of ToT using the inherent preference information in the tree-search
process. Extensive experimental results show that CPO significantly improves
LLM performance in solving a variety of complex problems, including question
answering, fact verification, and arithmetic reasoning, demonstrating its
effectiveness. Our code is available at https://github.com/sail-sg/CPO.

摘要：最近鏈式思考 (CoT) 解碼的發展，讓大型語言模型 (LLM) 能夠為複雜問題解決產生明確的邏輯推理路徑。然而，研究表明，這些路徑並不總是深思熟慮且最佳的。思考樹 (ToT) 方法採用樹狀搜尋來廣泛探索推理空間，並找到 CoT 解碼可能忽略的更佳推理路徑。然而，這種深思熟慮是以大幅增加推理複雜性為代價的。在這項工作中，我們證明了微調利用 ToT 建構的搜尋樹的 LLM，讓 CoT 能夠達成類似或更好的效能，從而避免大量的推理負擔。這是透過偏好鏈最佳化 (CPO) 來實現的，其中 LLM 被微調，以使用樹狀搜尋過程中固有的偏好資訊，將 CoT 推理路徑的每一步與 ToT 的路徑對齊。大量的實驗結果顯示，CPO 大幅提升了 LLM 在解決各種複雜問題時的效能，包括問題回答、事實驗證和算術推理，證明了它的有效性。我們的程式碼可在 https://github.com/sail-sg/CPO 取得。

##### **RH-SQL: Refined Schema and Hardness Prompt for Text-to-SQL**
2406.09133v1 by Jiawen Yi, Guo Chen, Zixiang Shen

Text-to-SQL is a technology that converts natural language queries into the
structured query language SQL. A novel research approach that has recently
gained attention focuses on methods based on the complexity of SQL queries,
achieving notable performance improvements. However, existing methods entail
significant storage and training costs, which hampers their practical
application. To address this issue, this paper introduces a method for
Text-to-SQL based on Refined Schema and Hardness Prompt. By filtering out
low-relevance schema information with a refined schema and identifying query
hardness through a Language Model (LM) to form prompts, this method reduces
storage and training costs while maintaining performance. It's worth mentioning
that this method is applicable to any sequence-to-sequence (seq2seq) LM. Our
experiments on the Spider dataset, specifically with large-scale LMs, achieved
an exceptional Execution accuracy (EX) of 82.6%, demonstrating the
effectiveness and greater suitability of our method for real-world
applications.

摘要：文字轉 SQL 是一種將自然語言查詢轉換為結構化查詢語言 SQL 的技術。最近備受關注的一種新穎研究方法專注於基於 SQL 查詢複雜度的技術，實現了顯著的效能提升。然而，現有方法需要大量的儲存和訓練成本，這阻礙了它們的實際應用。為了解決這個問題，本文介紹了一種基於精緻架構和難度提示的文字轉 SQL 方法。通過使用精緻的架構過濾出低相關性的架構資訊，並透過語言模型 (LM) 識別查詢難度以形成提示，此方法在維持效能的同時降低了儲存和訓練成本。值得一提的是，此方法適用於任何序列到序列 (seq2seq) LM。我們在 Spider 資料集上的實驗，特別是使用大規模 LM，達到了 82.6% 的出色執行準確度 (EX)，證明了我們的方法在實際應用中的有效性和更強的適用性。

##### **Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning**
2406.09130v1 by Haoxin Liu, Harshavardhan Kamarthi, Lingkai Kong, Zhiyuan Zhao, Chao Zhang, B. Aditya Prakash

Time-series forecasting (TSF) finds broad applications in real-world
scenarios. Due to the dynamic nature of time-series data, it is crucial to
equip TSF models with out-of-distribution (OOD) generalization abilities, as
historical training data and future test data can have different distributions.
In this paper, we aim to alleviate the inherent OOD problem in TSF via
invariant learning. We identify fundamental challenges of invariant learning
for TSF. First, the target variables in TSF may not be sufficiently determined
by the input due to unobserved core variables in TSF, breaking the conventional
assumption of invariant learning. Second, time-series datasets lack adequate
environment labels, while existing environmental inference methods are not
suitable for TSF.
  To address these challenges, we propose FOIL, a model-agnostic framework that
enables timeseries Forecasting for Out-of-distribution generalization via
Invariant Learning. FOIL employs a novel surrogate loss to mitigate the impact
of unobserved variables. Further, FOIL implements a joint optimization by
alternately inferring environments effectively with a multi-head network while
preserving the temporal adjacency structure, and learning invariant
representations across inferred environments for OOD generalized TSF. We
demonstrate that the proposed FOIL significantly improves the performance of
various TSF models, achieving gains of up to 85%.

摘要：時間序列預測 (TSF) 在實際情境中廣泛應用。由於時間序列資料的動態特性，因此 TSF 模型必須具備超出分佈 (OOD) 的概化能力，因為歷史訓練資料和未來測試資料可能具有不同的分佈。在本文中，我們旨在透過不變學習來緩解 TSF 中固有的 OOD 問題。我們找出 TSF 中不變學習的基本挑戰。首先，由於 TSF 中未觀察到的核心變數，因此輸入可能無法充分決定 TSF 中的目標變數，打破了不變學習的傳統假設。其次，時間序列資料集缺乏適當的環境標籤，而現有的環境推論方法並不適合 TSF。
為了應對這些挑戰，我們提出 FOIL，一個與模型無關的架構，它能透過不變學習啟用超出分佈概化之時間序列預測。FOIL 採用一種新穎的替代損失來減輕未觀察變數的影響。此外，FOIL 透過多頭網路有效地交替推論環境，同時保留時間鄰接結構，並學習推論環境中跨 OOD 概化 TSF 的不變表示，來實作聯合最佳化。我們證明所提出的 FOIL 大幅提升各種 TSF 模型的效能，獲得高達 85% 的收益。

##### **CoastTerm: a Corpus for Multidisciplinary Term Extraction in Coastal Scientific Literature**
2406.09128v1 by Julien Delaunay, Hanh Thi Hong Tran, Carlos-Emiliano González-Gallardo, Georgeta Bordea, Mathilde Ducos, Nicolas Sidere, Antoine Doucet, Senja Pollak, Olivier De Viron

The growing impact of climate change on coastal areas, particularly active
but fragile regions, necessitates collaboration among diverse stakeholders and
disciplines to formulate effective environmental protection policies. We
introduce a novel specialized corpus comprising 2,491 sentences from 410
scientific abstracts concerning coastal areas, for the Automatic Term
Extraction (ATE) and Classification (ATC) tasks. Inspired by the ARDI
framework, focused on the identification of Actors, Resources, Dynamics and
Interactions, we automatically extract domain terms and their distinct roles in
the functioning of coastal systems by leveraging monolingual and multilingual
transformer models. The evaluation demonstrates consistent results, achieving
an F1 score of approximately 80\% for automated term extraction and F1 of 70\%
for extracting terms and their labels. These findings are promising and signify
an initial step towards the development of a specialized Knowledge Base
dedicated to coastal areas.

摘要：氣候變遷對沿海地區的影響日益擴大，特別是活躍但脆弱的地區，這需要不同利害關係人和學科之間的合作，以制定有效的環境保護政策。我們引入一個新的專業語料庫，包含來自 410 個關於沿海地區的科學摘要的 2,491 個句子，用於自動術語提取 (ATE) 和分類 (ATC) 任務。受 ARDI 框架的啟發，專注於識別參與者、資源、動態和互動，我們通過利用單語和多語Transformer模型自動提取領域術語及其在沿海系統運作中的不同角色。評估結果一致，自動術語提取的 F1 分數約為 80%，提取術語及其標籤的 F1 為 70%。這些發現很有希望，標誌著邁向開發專門針對沿海地區的知識庫的第一步。

##### **PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation**
2406.09117v1 by Injoon Hwang, Haewon Park, Youngwan Lee, Jooyoung Yang, SunJae Maeng

Low-rank adaption (LoRA) is a prominent method that adds a small number of
learnable parameters to the frozen pre-trained weights for parameter-efficient
fine-tuning. Prompted by the question, ``Can we make its representation enough
with LoRA weights solely at the final phase of finetuning without the
pre-trained weights?'' In this work, we introduce Progressive Compression
LoRA~(PC-LoRA), which utilizes low-rank adaptation (LoRA) to simultaneously
perform model compression and fine-tuning. The PC-LoRA method gradually removes
the pre-trained weights during the training process, eventually leaving only
the low-rank adapters in the end. Thus, these low-rank adapters replace the
whole pre-trained weights, achieving the goals of compression and fine-tuning
at the same time. Empirical analysis across various models demonstrates that
PC-LoRA achieves parameter and FLOPs compression rates of 94.36%/89.1% for
vision models, e.g., ViT-B, and 93.42%/84.2% parameters and FLOPs compressions
for language models, e.g., BERT.

摘要：低秩適應 (LoRA) 是一種重要的方法，它會在凍結預訓練權重中加入少數可學習參數，以進行參數有效率的微調。受此問題的啟發：「我們是否能僅在微調的最後階段使用 LoRA 權重，使其表示足夠，而不需要預訓練權重？」在這項工作中，我們引入了漸進壓縮 LoRA (PC-LoRA)，它利用低秩適應 (LoRA) 同時執行模型壓縮和微調。PC-LoRA 方法在訓練過程中逐漸移除預訓練權重，最後只留下低秩適配器。因此，這些低秩適配器取代了整個預訓練權重，同時達到了壓縮和微調的目標。針對各種模型的經驗分析證明，PC-LoRA 達到了 94.36%/89.1% 的參數和 FLOP 壓縮率，例如視覺模型 ViT-B，以及 93.42%/84.2% 的參數和 FLOP 壓縮率，例如語言模型 BERT。

##### **Large-Scale Evaluation of Open-Set Image Classification Techniques**
2406.09112v1 by Halil Bisgin, Andres Palechor, Mike Suter, Manuel Günther

The goal for classification is to correctly assign labels to unseen samples.
However, most methods misclassify samples with unseen labels and assign them to
one of the known classes. Open-Set Classification (OSC) algorithms aim to
maximize both closed and open-set recognition capabilities. Recent studies
showed the utility of such algorithms on small-scale data sets, but limited
experimentation makes it difficult to assess their performances in real-world
problems. Here, we provide a comprehensive comparison of various OSC
algorithms, including training-based (SoftMax, Garbage, EOS) and
post-processing methods (Maximum SoftMax Scores, Maximum Logit Scores, OpenMax,
EVM, PROSER), the latter are applied on features from the former. We perform
our evaluation on three large-scale protocols that mimic real-world challenges,
where we train on known and negative open-set samples, and test on known and
unknown instances. Our results show that EOS helps to improve performance of
almost all post-processing algorithms. Particularly, OpenMax and PROSER are
able to exploit better-trained networks, demonstrating the utility of hybrid
models. However, while most algorithms work well on negative test samples --
samples of open-set classes seen during training -- they tend to perform poorly
when tested on samples of previously unseen unknown classes, especially in
challenging conditions.

摘要：分類的目標是正確地將標籤分配給未見過樣本。
然而，大多數方法會錯誤地分類具有未見過標籤的樣本，並將它們分配到已知的類別之一。開放式分類 (OSC) 演算法旨在最大化封閉式和開放式識別功能。最近的研究顯示此類演算法在小規模資料集上的效用，但有限的實驗使得難以評估它們在實際問題中的效能。在此，我們提供了各種 OSC 演算法的綜合比較，包括基於訓練的 (SoftMax、Garbage、EOS) 和後處理方法 (Maximum SoftMax Scores、Maximum Logit Scores、OpenMax、EVM、PROSER)，後者應用於前者的特徵。我們在三個模擬現實世界挑戰的大規模協定上執行評估，我們在已知和負的開放式樣本上進行訓練，並在已知和未知的實例上進行測試。我們的結果顯示，EOS 有助於改善幾乎所有後處理演算法的效能。特別是，OpenMax 和 PROSER 能夠利用訓練良好的網路，證明了混合模型的效用。然而，雖然大多數演算法在負面測試樣本上運作良好，也就是在訓練期間看到的開放式類別樣本，但當在先前未見過的未知類別樣本上進行測試時，它們往往表現不佳，尤其是在具有挑戰性的條件下。

##### **INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance**
2406.09105v1 by Chenwei Lin, Hanjia Lyu, Xian Xu, Jiebo Luo

Large Vision-Language Models (LVLMs) have demonstrated outstanding
performance in various general multimodal applications such as image
recognition and visual reasoning, and have also shown promising potential in
specialized domains. However, the application potential of LVLMs in the
insurance domain-characterized by rich application scenarios and abundant
multimodal data-has not been effectively explored. There is no systematic
review of multimodal tasks in the insurance domain, nor a benchmark
specifically designed to evaluate the capabilities of LVLMs in insurance. This
gap hinders the development of LVLMs within the insurance domain. In this
paper, we systematically review and distill multimodal tasks for four
representative types of insurance: auto insurance, property insurance, health
insurance, and agricultural insurance. We propose INS-MMBench, the first
comprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench
comprises a total of 2.2K thoroughly designed multiple-choice questions,
covering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate
multiple representative LVLMs, including closed-source models such as GPT-4o
and open-source models like BLIP-2. This evaluation not only validates the
effectiveness of our benchmark but also provides an in-depth performance
analysis of current LVLMs on various multimodal tasks in the insurance domain.
We hope that INS-MMBench will facilitate the further application of LVLMs in
the insurance domain and inspire interdisciplinary development. Our dataset and
evaluation code are available at https://github.com/FDU-INS/INS-MMBench.

摘要：大型视觉语言模型 (LVLMs) 已在图像识别和视觉推理等各种通用多模态应用程序中展示了出色的性能，并且在专门的领域中也显示出有希望的潜力。然而，LVLMs 在保险领域（以丰富的应用场景和大量多模态数据为特征）的应用潜力尚未得到有效探索。目前还没有对保险领域的模态任务进行系统的审查，也没有专门设计用来评估 LVLMs 在保险中的能力的基准。这种差距阻碍了 LVLMs 在保险领域的开发。在本文中，我们系统地审查和提炼了四种代表性保险类型的多模态任务：汽车保险、财产保险、健康保险和农业保险。我们提出了 INS-MMBench，这是第一个针对保险领域定制的综合 LVLMs 基准。INS-MMBench 包含总共 2.2K 个经过精心设计的单选题，涵盖 12 个元任务和 22 个基本任务。此外，我们评估了多种代表性 LVLMs，包括封闭源模型（如 GPT-4o）和开放源模型（如 BLIP-2）。此评估不仅验证了我们基准的有效性，还提供了当前 LVLMs 在保险领域各种多模态任务上的深入性能分析。我们希望 INS-MMBench 将促进 LVLMs 在保险领域的进一步应用，并激发跨学科发展。我们的数据集和评估代码可在 https://github.com/FDU-INS/INS-MMBench 获得。

##### **Chain-of-Though (CoT) prompting strategies for medical error detection and correction**
2406.09103v1 by Zhaolong Wu, Abul Hasan, Jinge Wu, Yunsoo Kim, Jason P. Y. Cheung, Teng Zhang, Honghan Wu

This paper describes our submission to the MEDIQA-CORR 2024 shared task for
automatically detecting and correcting medical errors in clinical notes. We
report results for three methods of few-shot In-Context Learning (ICL)
augmented with Chain-of-Thought (CoT) and reason prompts using a large language
model (LLM). In the first method, we manually analyse a subset of train and
validation dataset to infer three CoT prompts by examining error types in the
clinical notes. In the second method, we utilise the training dataset to prompt
the LLM to deduce reasons about their correctness or incorrectness. The
constructed CoTs and reasons are then augmented with ICL examples to solve the
tasks of error detection, span identification, and error correction. Finally,
we combine the two methods using a rule-based ensemble method. Across the three
sub-tasks, our ensemble method achieves a ranking of 3rd for both sub-task 1
and 2, while securing 7th place in sub-task 3 among all submissions.

摘要：本文描述了我們對 MEDIQA-CORR 2024 共享任務的提交，該任務是自動檢測和更正臨床筆記中的醫療錯誤。我們報告了使用大型語言模型 (LLM) 擴充的少次數情境學習 (ICL) 三種方法的結果，並採用思考鏈 (CoT) 和原因提示。在第一種方法中，我們手動分析訓練和驗證資料集的子集，藉由檢查臨床筆記中的錯誤類型來推斷三個 CoT 提示。在第二種方法中，我們利用訓練資料集提示 LLM 推斷其正確性或不正確性的原因。然後將建構的 CoT 和原因與 ICL 範例合併，以解決錯誤檢測、範圍識別和錯誤更正的任務。最後，我們使用基於規則的整體方法結合這兩種方法。在三個子任務中，我們的整體方法在子任務 1 和 2 中都取得第 3 名的排名，而在子任務 3 中則在所有提交中獲得第 7 名。

##### **SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models**
2406.09098v1 by Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, Huajun Chen

The burgeoning utilization of Large Language Models (LLMs) in scientific
research necessitates advanced benchmarks capable of evaluating their
understanding and application of scientific knowledge comprehensively. To
address this need, we introduce the SciKnowEval benchmark, a novel framework
that systematically evaluates LLMs across five progressive levels of scientific
knowledge: studying extensively, inquiring earnestly, thinking profoundly,
discerning clearly, and practicing assiduously. These levels aim to assess the
breadth and depth of scientific knowledge in LLMs, including knowledge
coverage, inquiry and exploration capabilities, reflection and reasoning
abilities, ethic and safety considerations, as well as practice proficiency.
Specifically, we take biology and chemistry as the two instances of SciKnowEval
and construct a dataset encompassing 50K multi-level scientific problems and
solutions. By leveraging this dataset, we benchmark 20 leading open-source and
proprietary LLMs using zero-shot and few-shot prompting strategies. The results
reveal that despite achieving state-of-the-art performance, the proprietary
LLMs still have considerable room for improvement, particularly in addressing
scientific computations and applications. We anticipate that SciKnowEval will
establish a comprehensive standard for benchmarking LLMs in science research
and discovery, and promote the development of LLMs that integrate scientific
knowledge with strong safety awareness. The dataset and code are publicly
available at https://github.com/hicai-zju/sciknoweval .

摘要：大型語言模型 (LLM) 在科學研究中的蓬勃發展，需要先進的基準來全面評估它們對科學知識的理解和應用能力。為了滿足這個需求，我們引入了 SciKnowEval 基準，這是一個新穎的框架，可以系統性地評估 LLM 在科學知識的五個漸進層面上：廣泛學習、認真探究、深入思考、清楚辨別和勤奮實踐。這些層級旨在評估 LLM 中科學知識的廣度和深度，包括知識涵蓋範圍、探究和探索能力、反思和推理能力、道德和安全考量以及實踐熟練度。具體來說，我們將生物學和化學作為 SciKnowEval 的兩個實例，並構建了一個包含 50K 多層級科學問題和解決方案的資料集。通過利用這個資料集，我們使用零次學習和少次學習提示策略對 20 個領先的開源和專有 LLM 進行基準測試。結果顯示，儘管取得了最先進的效能，但專有 LLM 仍有很大的改進空間，特別是在解決科學計算和應用方面。我們預期 SciKnowEval 將為科學研究和發現中的 LLM 基準測試建立一個全面的標準，並促進開發將科學知識與強烈的安全意識相結合的 LLM。資料集和程式碼已公開發布於 https://github.com/hicai-zju/sciknoweval。

##### **Modeling Comparative Logical Relation with Contrastive Learning for Text Generation**
2406.09095v1 by Yuhao Dan, Junfeng Tian, Jie Zhou, Ming Yan, Ji Zhang, Qin Chen, Liang He

Data-to-Text Generation (D2T), a classic natural language generation problem,
aims at producing fluent descriptions for structured input data, such as a
table. Existing D2T works mainly focus on describing the superficial
associative relations among entities, while ignoring the deep comparative
logical relations, such as A is better than B in a certain aspect with a
corresponding opinion, which is quite common in our daily life. In this paper,
we introduce a new D2T task named comparative logical relation generation
(CLRG). Additionally, we propose a Comparative Logic (CoLo) based text
generation method, which generates texts following specific comparative logical
relations with contrastive learning. Specifically, we first construct various
positive and negative samples by fine-grained perturbations in entities,
aspects and opinions. Then, we perform contrastive learning in the encoder
layer to have a better understanding of the comparative logical relations, and
integrate it in the decoder layer to guide the model to correctly generate the
relations. Noting the data scarcity problem, we construct a Chinese Comparative
Logical Relation Dataset (CLRD), which is a high-quality human-annotated
dataset and challenging for text generation with descriptions of multiple
entities and annotations on their comparative logical relations. Extensive
experiments show that our method achieves impressive performance in both
automatic and human evaluations.

摘要：資料轉文字生成（D2T）是一個經典的自然語言生成問題，旨在為結構化輸入資料（例如表格）產生流暢的描述。現有的 D2T 工作主要專注於描述實體之間表面的關聯關係，而忽略了深層的比較邏輯關係，例如 A 在某個方面比 B 好，並帶有相應的意見，這在我們的日常生活中很常見。在本文中，我們介紹了一項名為比較邏輯關係生成（CLRG）的新 D2T 任務。此外，我們提出了一個基於比較邏輯（CoLo）的文字生成方法，該方法遵循特定的比較邏輯關係並使用對比學習來生成文字。具體來說，我們首先通過實體、方面和意見的細粒度擾動來構造各種正負樣本。然後，我們在編碼器層中執行對比學習，以更好地理解比較邏輯關係，並將其整合到解碼器層中以指導模型正確生成關係。注意到資料稀缺的問題，我們構建了一個中文比較邏輯關係資料集（CLRD），這是一個高品質的人工標註資料集，對於具有多個實體描述和其比較邏輯關係標註的文字生成來說具有挑戰性。大量的實驗表明，我們的模型在自動和人工評估中都取得了令人印象深刻的表現。

##### **Suitability of KANs for Computer Vision: A preliminary investigation**
2406.09087v1 by Basim Azam, Naveed Akhtar

Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling
that implements learnable functions on the edges of the networks, diverging
from the traditional node-centric activations in neural networks. This work
assesses the applicability and efficacy of KANs in visual modeling, focusing on
the image recognition task. We mainly analyze the performance and efficiency of
different network architectures built using KAN concepts along with
conventional building blocks of convolutional and linear layers, enabling a
comparative analysis with the conventional models. Our findings are aimed at
contributing to understanding the potential of KANs in computer vision,
highlighting both their strengths and areas for further research. Our
evaluation shows that whereas KAN-based architectures perform in-line with the
original claims of KAN paper for performance and model-complexity in the case
of simpler vision datasets like MNIST, the advantages seem to diminish even for
slightly more complex datasets like CIFAR-10.

摘要：Kolmogorov-Arnold 網路 (KAN) 引入神經建模範式，在網路邊緣實作可學習函數，與神經網路中傳統以節點為中心的活化不同。這項工作評估 KAN 在視覺建模中的適用性和效能，重點在於影像辨識任務。我們主要分析使用 KAN 概念建置的不同網路架構的效能和效率，以及卷積和線性層的傳統建構區塊，以便與傳統模型進行比較分析。我們的研究結果旨在促進了解 KAN 在電腦視覺中的潛力，強調它們的優勢和進一步研究的領域。我們的評估顯示，雖然基於 KAN 的架構在效能和模型複雜性方面符合 KAN 論文的原始主張，適用於 MNIST 等較簡單的視覺資料集，但即使對於 CIFAR-10 等稍微更複雜的資料集，優勢似乎也會減弱。

##### **Data-driven modeling and supervisory control system optimization for plug-in hybrid electric vehicles**
2406.09082v1 by Hao Zhang, Nuo Lei, Boli Chen, Bingbing Li, Rulong Li, Zhi Wang

Learning-based intelligent energy management systems for plug-in hybrid
electric vehicles (PHEVs) are crucial for achieving efficient energy
utilization. However, their application faces system reliability challenges in
the real world, which prevents widespread acceptance by original equipment
manufacturers (OEMs). This paper begins by establishing a PHEV model based on
physical and data-driven models, focusing on the high-fidelity training
environment. It then proposes a real-vehicle application-oriented control
framework, combining horizon-extended reinforcement learning (RL)-based energy
management with the equivalent consumption minimization strategy (ECMS) to
enhance practical applicability, and improves the flawed method of equivalent
factor evaluation based on instantaneous driving cycle and powertrain states
found in existing research. Finally, comprehensive simulation and
hardware-in-the-loop validation are carried out which demonstrates the
advantages of the proposed control framework in fuel economy over adaptive-ECMS
and rule-based strategies. Compared to conventional RL architectures that
directly control powertrain components, the proposed control method not only
achieves similar optimality but also significantly enhances the disturbance
resistance of the energy management system, providing an effective control
framework for RL-based energy management strategies aimed at real-vehicle
applications by OEMs.

摘要：基於學習的智慧型能源管理系統對於插電式混合動力電動車 (PHEV) 而言至關重要，可達成有效率的能源利用。然而，其應用在現實世界中面臨系統可靠性的挑戰，這阻礙了原始設備製造商 (OEM) 的廣泛接受。本文首先建立一個基於物理和資料驅動模型的 PHEV 模型，專注於高保真訓練環境。接著提出一個面向實際車輛應用的控制架構，結合以時域延伸強化學習 (RL) 為基礎的能源管理與等效消耗最小化策略 (ECMS)，以增強實用性，並改善現有研究中基於瞬時駕駛循環和動力系統狀態的等效因子評估有缺陷的方法。最後，進行全面的模擬和硬體迴路驗證，證明了所提出的控制架構在燃油經濟性方面優於適應性 ECMS 和基於規則的策略。與直接控制動力系統元件的傳統 RL 架構相比，所提出的控制方法不僅達到了類似的最佳化，還顯著增強了能源管理系統的抗擾性，為 OEM 針對實際車輛應用所採用的基於 RL 的能源管理策略提供了有效的控制架構。

##### **3M: Multi-modal Multi-task Multi-teacher Learning for Game Event Detection**
2406.09076v1 by Thye Shan Ng, Feiqi Cao, Soyeon Caren Han

Esports has rapidly emerged as a global phenomenon with an ever-expanding
audience via platforms, like YouTube. Due to the inherent complexity nature of
the game, it is challenging for newcomers to comprehend what the event entails.
The chaotic nature of online chat, the fast-paced speech of the game
commentator, and the game-specific user interface further compound the
difficulty for users in comprehending the gameplay. To overcome these
challenges, it is crucial to integrate the Multi-Modal (MM) information from
the platform and understand the event. The paper introduces a new MM
multi-teacher-based game event detection framework, with the ultimate goal of
constructing a comprehensive framework that enhances the comprehension of the
ongoing game situation. While conventional MM models typically prioritise
aligning MM data through concurrent training towards a unified objective, our
framework leverages multiple teachers trained independently on different tasks
to accomplish the Game Event Detection. The experiment clearly shows the
effectiveness of the proposed MM multi-teacher framework.

摘要：電競已迅速成為一項全球現象，透過 YouTube 等平台不斷擴展其受眾。由於遊戲的複雜本質，對於新手來說，要理解事件的內容是一項挑戰。在線聊天的混亂性質、遊戲評論員的快速語速，以及特定於遊戲的使用者介面進一步加劇了使用者理解遊戲玩法的難度。為了克服這些挑戰，整合來自平台的多模態 (MM) 資訊並了解事件至關重要。本文介紹了一個新的 MM 多教師遊戲事件偵測架構，其最終目標是建構一個全面的架構，以增強對正在進行的遊戲情況的理解。雖然傳統的 MM 模型通常優先透過同時訓練統一目標來對齊 MM 資料，但我們的架構利用了在不同任務上獨立訓練的多個教師來完成遊戲事件偵測。實驗清楚顯示了所提出的 MM 多教師架構的有效性。

##### **Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?**
2406.09072v1 by Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, Min zhang

Temporal reasoning is fundamental for large language models (LLMs) to
comprehend the world. Current temporal reasoning datasets are limited to
questions about single or isolated events, falling short in mirroring the
realistic temporal characteristics involving concurrent nature and intricate
temporal interconnections. In this paper, we introduce CoTempQA, a
comprehensive co-temporal Question Answering (QA) benchmark containing four
co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for
evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our
extensive experiments reveal a significant gap between the performance of
current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced
with Chain of Thought (CoT) methodologies, models consistently struggle with
our task. In our preliminary exploration, we discovered that mathematical
reasoning plays a significant role in handling co-temporal events and proposed
a strategy to boost LLMs' co-temporal reasoning from a mathematical
perspective. We hope that our CoTempQA datasets will encourage further
advancements in improving the co-temporal reasoning capabilities of LLMs. Our
code is available at https://github.com/zhaochen0110/Cotempqa.

摘要：時序推理對於大型語言模型 (LLM) 理解世界至關重要。目前時序推理資料集僅限於單一或孤立事件的問題，無法反映涉及並發性質和複雜時序關聯的現實時序特徵。在本文中，我們介紹 CoTempQA，一個全面的共時問答 (QA) 基準，包含四個共時場景（相等、重疊、期間、混合），有 4,748 個樣本，用於評估 LLM 的共時理解和推理能力。我們的廣泛實驗揭示了當前 LLM 的效能與人類層級在 CoTempQA 任務上的推理之間存在顯著差距。即使使用思想鏈 (CoT) 方法增強，模型仍持續在我們的任務中掙扎。在我們的初步探討中，我們發現數學推理在處理共時事件中扮演重要角色，並提出了一個從數學角度提升 LLM 共時推理的策略。我們希望我們的 CoTempQA 資料集將鼓勵進一步提升 LLM 的共時推理能力。我們的程式碼可在 https://github.com/zhaochen0110/Cotempqa 取得。

##### **EquiPrompt: Debiasing Diffusion Models via Iterative Bootstrapping in Chain of Thoughts**
2406.09070v1 by Zahraa Al Sahili, Ioannis Patras, Matthew Purver

In the domain of text-to-image generative models, the inadvertent propagation
of biases inherent in training datasets poses significant ethical challenges,
particularly in the generation of socially sensitive content. This paper
introduces EquiPrompt, a novel method employing Chain of Thought (CoT)
reasoning to reduce biases in text-to-image generative models. EquiPrompt uses
iterative bootstrapping and bias-aware exemplar selection to balance creativity
and ethical responsibility. It integrates iterative reasoning refinement with
controlled evaluation techniques, addressing zero-shot CoT issues in sensitive
contexts. Experiments on several generation tasks show EquiPrompt effectively
lowers bias while maintaining generative quality, advancing ethical AI and
socially responsible creative processes.Code will be publically available.

摘要：在文本到影像生成模型的領域中，訓練資料集中固有的偏差會無意間傳播，這對道德層面造成重大的挑戰，特別是在產生社會敏感內容時。本文介紹 EquiPrompt，這是一種採用思考鏈 (CoT) 推理的新方法，用於減少文本到影像生成模型中的偏差。EquiPrompt 使用反覆引導和有偏見的範例選擇來平衡創造力和道德責任。它將反覆推理精煉與受控評估技術整合在一起，解決敏感情境中的零次 CoT 問題。在多項生成任務上的實驗顯示，EquiPrompt 有效降低偏差，同時維持生成品質，促進道德 AI 和社會責任的創作流程。程式碼將公開提供。

##### **How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models**
2406.09067v1 by Tarun Khajuria, Braian Olmiro Dias, Jaan Aru

Forming and using symbol-like structured representations for reasoning has
been considered essential for generalising over novel inputs. The primary tool
that allows generalisation outside training data distribution is the ability to
abstract away irrelevant information into a compact form relevant to the task.
An extreme form of such abstract representations is symbols. Humans make use of
symbols to bind information while abstracting away irrelevant parts to utilise
the information consistently and meaningfully. This work estimates the state of
such structured representations in vision encoders. Specifically, we evaluate
image encoders in large vision-language pre-trained models to address the
question of which desirable properties their representations lack by applying
the criteria of symbolic structured reasoning described for LLMs to the image
models. We test the representation space of image encoders like VIT, BLIP,
CLIP, and FLAVA to characterise the distribution of the object representations
in these models. In particular, we create decoding tasks using multi-object
scenes from the COCO dataset, relating the token space to its input content for
various objects in the scene. We use these tasks to characterise the network's
token and layer-wise information modelling. Our analysis highlights that the
CLS token, used for the downstream task, only focuses on a few objects
necessary for the trained downstream task. Still, other individual objects are
well-modelled separately by the tokens in the network originating from those
objects. We further observed a widespread distribution of scene information.
This demonstrates that information is far more entangled in tokens than optimal
for representing objects similar to symbols. Given these symbolic properties,
we show the network dynamics that cause failure modes of these models on basic
downstream tasks in a multi-object scene.

摘要：<paragraph>形成和使用符号状的结构化表征进行推理被认为对于概括新颖的输入至关重要。允许在训练数据分布之外进行概括的主要工具是将不相关信息抽象成与任务相关的紧凑形式的能力。这种抽象表征的极端形式是符号。人类利用符号来绑定信息，同时抽象掉不相关部分，以便一致且有意义地利用信息。这项工作估计了视觉编码器中这种结构化表征的状态。具体来说，我们评估大型视觉语言预训练模型中的图像编码器，以解决其表征缺乏哪些理想属性的问题，方法是将针对 LLM 描述的符号结构化推理标准应用于图像模型。我们测试了 VIT、BLIP、CLIP 和 FLAVA 等图像编码器的表征空间，以表征这些模型中对象表征的分布。特别是，我们使用 COCO 数据集中的多对象场景创建解码任务，将标记空间与其输入内容关联起来，用于场景中的各种对象。我们使用这些任务来表征网络的标记和逐层信息建模。我们的分析强调，用于下游任务的 CLS 标记只关注训练的下游任务所需的几个对象。不过，网络中源自这些对象的标记可以很好地单独对其他各个对象进行建模。我们进一步观察到场景信息的分布很广泛。这表明信息在标记中的纠缠程度远高于以类似于符号的方式表征对象的最佳程度。鉴于这些符号属性，我们展示了导致这些模型在多对象场景中的基本下游任务上出现故障模式的网络动态。</paragraph>

##### **CUDRT: Benchmarking the Detection of Human vs. Large Language Models Generated Texts**
2406.09056v1 by Zhen Tao, Zhiyu Li, Dinghao Xi, Wei Xu

The proliferation of large language models (LLMs) has significantly enhanced
text generation capabilities across various industries. However, these models'
ability to generate human-like text poses substantial challenges in discerning
between human and AI authorship. Despite the effectiveness of existing
AI-generated text detectors, their development is hindered by the lack of
comprehensive, publicly available benchmarks. Current benchmarks are limited to
specific scenarios, such as question answering and text polishing, and
predominantly focus on English texts, failing to capture the diverse
applications and linguistic nuances of LLMs. To address these limitations, this
paper constructs a comprehensive bilingual benchmark in both Chinese and
English to evaluate mainstream AI-generated text detectors. We categorize LLM
text generation into five distinct operations: Create, Update, Delete, Rewrite,
and Translate (CUDRT), encompassing all current LLMs activities. We also
establish a robust benchmark evaluation framework to support scalable and
reproducible experiments. For each CUDRT category, we have developed extensive
datasets to thoroughly assess detector performance. By employing the latest
mainstream LLMs specific to each language, our datasets provide a thorough
evaluation environment. Extensive experimental results offer critical insights
for optimizing AI-generated text detectors and suggest future research
directions to improve detection accuracy and generalizability across various
scenarios.

摘要：大型語言模型 (LLM) 的激增大幅提升了各產業的文字生成能力。然而，這些模型生成類似人類文字的能力，在辨別人類和 AI 的作者時帶來極大的挑戰。儘管現有的 AI 生成的文字偵測器已具備成效，但其發展受到全面、公開基準的缺乏所阻礙。目前的基準僅限於特定場景，例如問答和文字潤飾，且主要集中於英文文字，未能掌握 LLM 的多元應用和語言差異。為了解決這些限制，本文建構了一個包含中文和英文的全面雙語基準，以評估主流的 AI 生成的文字偵測器。我們將 LLM 文字生成分類為五種不同的操作：建立、更新、刪除、重寫和翻譯 (CUDRT)，涵蓋所有當前 LLM 的活動。我們還建立了一個強健的基準評估架構，以支援可擴充且可重現的實驗。對於每個 CUDRT 類別，我們都開發了廣泛的資料集，以徹底評估偵測器的效能。透過採用針對每種語言的最新主流 LLM，我們的資料集提供了徹底的評估環境。廣泛的實驗結果為最佳化 AI 生成的文字偵測器提供了重要的見解，並建議未來的研究方向，以提高偵測準確度和在各種場景中的概括性。

##### **MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning**
2406.09044v1 by Hanqing Wang, Zeguan Xiao, Yixia Li, Shuo Wang, Guanhua Chen, Yun Chen

Efficient finetuning of large language models (LLMs) aims to adapt the LLMs
with reduced computation and memory cost. Previous LoRA-based approaches
initialize the low-rank matrices with gaussian distribution and zero values,
while keeping the original weight matrices frozen. However, the trainable model
parameters optimized in an unguided subspace might have interference with the
well-learned subspace of the pretrained weight matrix. In this paper, we
propose MiLoRA, a simple yet effective LLM finetuning approach that only
updates the minor singular components of the weight matrix while keeping the
principle singular components frozen. It is observed that the minor matrix
corresponds to the noisy or long-tail information, while the principle matrix
contains important knowledge. The MiLoRA initializes the low-rank matrices
within a subspace that is orthogonal to the principle matrix, thus the
pretrained knowledge is expected to be well preserved. During finetuning,
MiLoRA makes the most use of the less-optimized subspace for learning the
finetuning dataset. Extensive experiments on commonsense reasoning, math
reasoning and instruction following benchmarks present the superior performance
of our method.

摘要：大型語言模型 (LLM) 的有效微調旨在以降低運算和記憶體成本的方式調整 LLM。先前的基於 LoRA 的方法使用高斯分佈和零值初始化低秩矩陣，同時保持原始權重矩陣凍結。然而，在不受引導的子空間中最佳化的可訓練模型參數可能會與預訓練權重矩陣的良好學習子空間產生干擾。在本文中，我們提出 MiLoRA，這是一種簡單但有效的 LLM 微調方法，僅更新權重矩陣的次要奇異分量，同時保持主要奇異分量凍結。可以觀察到，次要矩陣對應於雜訊或長尾資訊，而主矩陣包含重要知識。MiLoRA 在與主矩陣正交的子空間內初始化低秩矩陣，因此預計預訓練知識將得到很好的保留。在微調過程中，MiLoRA 最充分地利用了次優化的子空間來學習微調資料集。在常識推理、數學推理和指令遵循基準上的廣泛實驗展示了我們方法的優異性能。

##### **Language Models are Crossword Solvers**
2406.09043v1 by Soumadeep Saha, Sutanoya Chakraborty, Saptarshi Saha, Utpal Garain

Crosswords are a form of word puzzle that require a solver to demonstrate a
high degree of proficiency in natural language understanding, wordplay,
reasoning, and world knowledge, along with adherence to character and length
constraints. In this paper we tackle the challenge of solving crosswords with
Large Language Models (LLMs). We demonstrate that the current generation of
state-of-the art (SoTA) language models show significant competence at
deciphering cryptic crossword clues, and outperform previously reported SoTA
results by a factor of 2-3 in relevant benchmarks. We also develop a search
algorithm that builds off this performance to tackle the problem of solving
full crossword grids with LLMs for the very first time, achieving an accuracy
of 93\% on New York Times crossword puzzles. Contrary to previous work in this
area which concluded that LLMs lag human expert performance significantly, our
research suggests this gap is a lot narrower.

摘要：填字遊戲是一種文字謎題，要求解謎者展現高度的自然語言理解、文字遊戲、推理和世界知識能力，同時還要遵守字元和長度限制。在本文中，我們將解決使用大型語言模型 (LLM) 解決填字遊戲的挑戰。我們證明了當前最先進 (SoTA) 的語言模型在解讀隱晦的填字遊戲線索方面表現出顯著的能力，並且在相關基準測試中比先前報告的 SoTA 結果高出 2-3 倍。我們還開發了一種搜尋演算法，利用此效能來解決使用 LLM 解決完整填字遊戲格子的問題，這是第一次在紐約時報填字遊戲謎題中達到 93% 的準確率。與先前在這個領域的結論相反，即 LLM 遠遠落後於人類專家表現，我們的研究表明這個差距小了很多。

##### **ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models**
2406.09041v1 by Jing Liu, Ruihao Gong, Mingyang Zhang, Yefei He, Jianfei Cai, Bohan Zhuang

The typical process for developing LLMs involves pre-training a general
foundation model on massive data, followed by fine-tuning on task-specific data
to create specialized experts. Serving these experts poses challenges, as
loading all experts onto devices is impractical, and frequent switching between
experts in response to user requests incurs substantial I/O costs, increasing
latency and expenses. Previous approaches decompose expert weights into
pre-trained model weights and residual delta weights, then quantize the delta
weights to reduce model size. However, these methods often lead to significant
quantization errors at extremely low bitwidths and assume the appropriate model
for a user request is known in advance, which is not practical. To address
these issues, we introduce ME-Switch, a memory-efficient expert switching
framework for LLM serving. ME-Switch uses mixed-precision quantization,
selectively quantizing non-salient input channels of delta weights to extremely
low bits while keeping salient ones intact, significantly reducing storage
demands while maintaining performance. Additionally, we develop a routing
method that efficiently directs user queries to the most suitable expert by
transforming the model selection problem into a domain classification problem.
Extensive experiments show ME-Switch's promising memory efficiency and routing
performance. For example, when serving three models from the Mistral-7B family,
ME-Switch reduces model size by 1.74x while maintaining nearly lossless
performance on instruction, mathematical reasoning, and code generation tasks.
Furthermore, ME-Switch can efficiently serve 16 models from the Mistral-7B
family on a single NVIDIA A100 GPU.

摘要：<paragraph>開發大型語言模型的典型流程涉及在大量資料上預先訓練通用基礎模型，然後針對特定任務資料進行微調以建立專門的專家。提供這些專家服務會帶來挑戰，因為將所有專家載入裝置並不實際，而且根據使用者要求在專家之間頻繁切換會產生大量的 I/O 成本，增加延遲和費用。先前的做法將專家權重分解為預先訓練的模型權重和殘差 delta 權重，然後對 delta 權重進行量化以減少模型大小。然而，這些方法通常會在極低的位寬產生顯著的量化誤差，並假設事先已知道使用者要求的適當模型，這並不實際。為了解決這些問題，我們引入了 ME-Switch，一種用於大型語言模型服務的記憶體高效專家切換架構。ME-Switch 使用混合精度量化，選擇性地將 delta 權重的非顯著輸入通道量化為極低的位元，同時保持顯著通道的完整性，顯著降低儲存需求，同時維持效能。此外，我們開發了一種路由方法，透過將模型選擇問題轉換為網域分類問題，有效地將使用者查詢引導至最合適的專家。廣泛的實驗顯示 ME-Switch 具有良好的記憶體效率和路由效能。例如，在提供 Mistral-7B 系列中的三個模型時，ME-Switch 將模型大小減少了 1.74 倍，同時在指令、數學推理和程式碼產生任務上維持幾乎無損的效能。此外，ME-Switch 可以有效地在單一的 NVIDIA A100 GPU 上提供 Mistral-7B 系列中的 16 個模型。</paragraph>

##### **Deep learning empowered sensor fusion to improve infant movement classification**
2406.09014v2 by Tomas Kulvicius, Dajie Zhang, Luise Poustka, Sven Bölte, Lennart Jahn, Sarah Flügge, Marc Kraft, Markus Zweckstetter, Karin Nielsen-Saines, Florentin Wörgötter, Peter B Marschik

There is a recent boom in the development of AI solutions to facilitate and
enhance diagnostic procedures for established clinical tools. To assess the
integrity of the developing nervous system, the Prechtl general movement
assessment (GMA) is recognized for its clinical value in diagnosing
neurological impairments in early infancy. GMA has been increasingly augmented
through machine learning approaches intending to scale-up its application,
circumvent costs in the training of human assessors and further standardize
classification of spontaneous motor patterns. Available deep learning tools,
all of which are based on single sensor modalities, are however still
considerably inferior to that of well-trained human assessors. These approaches
are hardly comparable as all models are designed, trained and evaluated on
proprietary/silo-data sets. With this study we propose a sensor fusion approach
for assessing fidgety movements (FMs) comparing three different sensor
modalities (pressure, inertial, and visual sensors). Various combinations and
two sensor fusion approaches (late and early fusion) for infant movement
classification were tested to evaluate whether a multi-sensor system
outperforms single modality assessments. The performance of the three-sensor
fusion (classification accuracy of 94.5\%) was significantly higher than that
of any single modality evaluated, suggesting the sensor fusion approach is a
promising avenue for automated classification of infant motor patterns. The
development of a robust sensor fusion system may significantly enhance AI-based
early recognition of neurofunctions, ultimately facilitating automated early
detection of neurodevelopmental conditions.

摘要：近來，促進並提升既有臨床工具的診斷程序的人工智慧解決方案發展蓬勃。為了評估發育中神經系統的完整性，Prechtl 全般動作評估 (GMA) 因其在診斷嬰兒早期神經損傷的臨床價值而受到肯定。GMA 已透過機器學習方法持續擴充，旨在擴大其應用範圍，規避人類評估人員培訓的成本，並進一步標準化自發性動作模式的分類。現有的深度學習工具，全部都基於單一感測器模式，但仍遠遜於訓練有素的人類評估人員。這些方法難以比較，因為所有模型都是在專有/孤立資料集上設計、訓練和評估的。透過這項研究，我們提出了一種感測器融合方法，用於評估坐立不安的動作 (FM)，並比較三種不同的感測器模式（壓力、慣性和視覺感測器）。測試了各種組合和兩種感測器融合方法（後融合和早融合），用於嬰兒動作分類，以評估多感測器系統是否優於單一模式評估。三感測器融合的效能（分類準確度為 94.5%）顯著高於評估的任何單一模式，這表示感測器融合方法是自動化分類嬰兒動作模式的一個有前途的方法。健全的感測器融合系統的發展可能會顯著提升基於人工智慧的早期神經功能辨識，最終促進神經發展狀況的自動化早期偵測。

##### **Bayesian Statistical Modeling with Predictors from LLMs**
2406.09012v1 by Michael Franke, Polina Tsvilodub, Fausto Carcassi

State of the art large language models (LLMs) have shown impressive
performance on a variety of benchmark tasks and are increasingly used as
components in larger applications, where LLM-based predictions serve as proxies
for human judgements or decision. This raises questions about the
human-likeness of LLM-derived information, alignment with human intuition, and
whether LLMs could possibly be considered (parts of) explanatory models of
(aspects of) human cognition or language use. To shed more light on these
issues, we here investigate the human-likeness of LLMs' predictions for
multiple-choice decision tasks from the perspective of Bayesian statistical
modeling. Using human data from a forced-choice experiment on pragmatic
language use, we find that LLMs do not capture the variance in the human data
at the item-level. We suggest different ways of deriving full distributional
predictions from LLMs for aggregate, condition-level data, and find that some,
but not all ways of obtaining condition-level predictions yield adequate fits
to human data. These results suggests that assessment of LLM performance
depends strongly on seemingly subtle choices in methodology, and that LLMs are
at best predictors of human behavior at the aggregate, condition-level, for
which they are, however, not designed to, or usually used to, make predictions
in the first place.

摘要：最先进的大型语言模型 (LLM) 在各种基准任务中表现出色，并且越来越多地用作大型应用程序中的组件，其中基于 LLM 的预测用作人类判断或决策的代理。这引发了关于 LLM 衍生信息的人类相似性、与人类直觉的一致性，以及 LLM 是否可能被认为是（部分）人类认知或语言使用的解释性模型的问题。为了更深入地了解这些问题，我们在此从贝叶斯统计建模的角度研究了 LLM 对多项选择决策任务的预测的人类相似性。使用来自务实语言使用强制选择实验的人类数据，我们发现 LLM 无法在项目级别捕捉人类数据中的差异。我们提出了从 LLM 中推导总体条件级数据的完整分布式预测的不同方法，并发现某些（但并非全部）获得条件级预测的方法产生了与人类数据充分拟合。这些结果表明，对 LLM 性能的评估在很大程度上取决于方法论中看似细微的选择，并且 LLM 最多是在总体条件级预测人类行为，但它们最初并非设计用于或通常用于进行预测。

##### **Fredformer: Frequency Debiased Transformer for Time Series Forecasting**
2406.09009v2 by Xihao Piao, Zheng Chen, Taichi Murayama, Yasuko Matsubara, Yasushi Sakurai

The Transformer model has shown leading performance in time series
forecasting. Nevertheless, in some complex scenarios, it tends to learn
low-frequency features in the data and overlook high-frequency features,
showing a frequency bias. This bias prevents the model from accurately
capturing important high-frequency data features. In this paper, we undertook
empirical analyses to understand this bias and discovered that frequency bias
results from the model disproportionately focusing on frequency features with
higher energy. Based on our analysis, we formulate this bias and propose
Fredformer, a Transformer-based framework designed to mitigate frequency bias
by learning features equally across different frequency bands. This approach
prevents the model from overlooking lower amplitude features important for
accurate forecasting. Extensive experiments show the effectiveness of our
proposed approach, which can outperform other baselines in different real-world
time-series datasets. Furthermore, we introduce a lightweight variant of the
Fredformer with an attention matrix approximation, which achieves comparable
performance but with much fewer parameters and lower computation costs. The
code is available at: https://github.com/chenzRG/Fredformer

摘要：Transformer 模型在時序預測方面展現出領先的效能。然而，在某些複雜場景中，它傾向於學習資料中的低頻特徵，而忽略高頻特徵，顯示出頻率偏差。此偏差會阻止模型準確擷取重要的「高頻資料特徵」。在本文中，我們進行了經驗分析，以了解此偏差，並發現頻率偏差源自模型不成比例地關注能量較高的頻率特徵。根據我們的分析，我們制定了此偏差，並提出了 Fredformer，一個基於 Transformer 的框架，旨在透過在不同頻率頻帶中平等地學習特徵，來減輕頻率偏差。此方法可防止模型忽略對於準確預測很重要的低振幅特徵。廣泛的實驗顯示了我們提出的方法的有效性，它可以在不同的實際時序資料集上優於其他基線。此外，我們引入了 Fredformer 的輕量級變體，具有注意力矩陣近似，可實現相當的效能，但參數更少，運算成本更低。程式碼可於以下網址取得：https://github.com/chenzRG/Fredformer

##### **LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models**
2406.09008v1 by Xiaohao Yang, He Zhao, Dinh Phung, Wray Buntine, Lan Du

Topic modeling has been a widely used tool for unsupervised text analysis.
However, comprehensive evaluations of a topic model remain challenging.
Existing evaluation methods are either less comparable across different models
(e.g., perplexity) or focus on only one specific aspect of a model (e.g., topic
quality or document representation quality) at a time, which is insufficient to
reflect the overall model performance. In this paper, we propose WALM (Words
Agreement with Language Model), a new evaluation method for topic modeling that
comprehensively considers the semantic quality of document representations and
topics in a joint manner, leveraging the power of large language models (LLMs).
With extensive experiments involving different types of topic models, WALM is
shown to align with human judgment and can serve as a complementary evaluation
method to the existing ones, bringing a new perspective to topic modeling. Our
software package will be available at
https://github.com/Xiaohao-Yang/Topic_Model_Evaluation, which can be integrated
with many widely used topic models.

摘要：主題建模一直是廣泛用於無監督文本分析的工具。
然而，主題模型的綜合評估仍然具有挑戰性。
現有的評估方法要么在不同模型之間的可比性較差（例如困惑度），要么一次只關注模型的一個特定方面（例如主題質量或文件表示質量），這不足以反映模型的整體性能。在本文中，我們提出了 WALM（詞語與語言模型的一致性），這是一種新的主題建模評估方法，它以一種聯合的方式全面考慮文件表示和主題的語義質量，利用大型語言模型 (LLM) 的強大功能。通過涉及不同類型主題模型的廣泛實驗，WALM 被證明與人類判斷一致，並且可以用作現有模型的補充評估方法，為主題建模帶來新的視角。我們的軟體套件將在 https://github.com/Xiaohao-Yang/Topic_Model_Evaluation 上提供，它可以與許多廣泛使用的主題模型整合。

##### **Introducing Brain-like Concepts to Embodied Hand-crafted Dialog Management System**
2406.08996v1 by Frank Joublin, Antonello Ceravola, Cristian Sandu

Along with the development of chatbot, language models and speech
technologies, there is a growing possibility and interest of creating systems
able to interface with humans seamlessly through natural language or directly
via speech. In this paper, we want to demonstrate that placing the research on
dialog system in the broader context of embodied intelligence allows to
introduce concepts taken from neurobiology and neuropsychology to define
behavior architecture that reconcile hand-crafted design and artificial neural
network and open the gate to future new learning approaches like imitation or
learning by instruction. To do so, this paper presents a neural behavior engine
that allows creation of mixed initiative dialog and action generation based on
hand-crafted models using a graphical language. A demonstration of the
usability of such brain-like inspired architecture together with a graphical
dialog model is described through a virtual receptionist application running on
a semi-public space.

摘要：隨著聊天機器人的發展、語言模型和語音技術，創造出能夠透過自然語言或直接透過語音與人類無縫互動的系統，其可能性和興趣與日俱增。在本文中，我們想要證明將對話系統的研究置於具體智能的廣泛背景下，可以引入從神經生物學和神經心理學中擷取的概念來定義行為架構，調和手工設計和人工神經網路，並為未來的學習新方法（例如模仿或透過指示學習）開啟大門。為此，本文提出了一款神經行為引擎，它允許使用圖形語言建立混合主動式對話和基於手工模型的動作產生。透過一個在半公共空間中執行的虛擬接待員應用程式，描述了這種類腦啟發架構與圖形對話模型一起使用的可用性示範。

##### **Multi-Agent Software Development through Cross-Team Collaboration**
2406.08979v1 by Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, Cheng Yang

The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have
catalyzed profound transformations, particularly through multi-agent
collaboration for software development. LLM agents can collaborate in teams
like humans, and follow the waterfall model to sequentially work on
requirements analysis, development, review, testing, and other phases to
perform autonomous software generation. However, for an agent team, each phase
in a single development process yields only one possible outcome. This results
in the completion of only one development chain, thereby losing the opportunity
to explore multiple potential decision paths within the solution space.
Consequently, this may lead to obtaining suboptimal results. To address this
challenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team
framework that enables orchestrated teams to jointly propose various decisions
and communicate with their insights in a cross-team collaboration environment
for superior content generation. Experimental results in software development
reveal a notable increase in quality compared to state-of-the-art baselines,
underscoring the efficacy of our framework. The significant improvements in
story generation demonstrate the promising generalization ability of our
framework across various domains. We anticipate that our work will guide LLM
agents towards a cross-team paradigm and contribute to their significant growth
in but not limited to software development. The code and data will be available
at https://github.com/OpenBMB/ChatDev.

摘要：大型語言模型 (LLM) 的最新突破，例如 ChatDev，已經催化了深刻的轉變，特別是透過多重代理的軟體開發合作。LLM 代理可以像人類一樣在團隊中合作，並遵循瀑布模型依序處理需求分析、開發、檢閱、測試和其他階段，以執行自主軟體生成。然而，對於一個代理團隊來說，單一開發流程中的每個階段只會產生一個可能的結果。這導致只完成一個開發鏈，從而失去在解決方案空間中探索多個潛在決策路徑的機會。因此，這可能會導致獲得次優的結果。為了應對這個挑戰，我們引入了跨團隊協作 (CTC)，這是一個可擴充的多團隊架構，使協調的團隊能夠共同提出各種決策，並在跨團隊協作環境中透過見解進行溝通，以進行優質內容生成。軟體開發的實驗結果顯示，與現有的基準相比，品質顯著提升，突顯了我們架構的功效。故事生成方面的大幅改進證明了我們架構在各種領域中具有良好的泛化能力。我們預期我們的成果將引導 LLM 代理朝向跨團隊典範，並為其在軟體開發（但不限於此）方面做出重大貢獻。程式碼和資料將於 https://github.com/OpenBMB/ChatDev 提供。

##### **XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning**
2406.08973v1 by Alexander Nikulin, Ilya Zisman, Alexey Zemtsov, Viacheslav Sinii, Vladislav Kurenkov, Sergey Kolesnikov

Following the success of the in-context learning paradigm in large-scale
language and computer vision models, the recently emerging field of in-context
reinforcement learning is experiencing a rapid growth. However, its development
has been held back by the lack of challenging benchmarks, as all the
experiments have been carried out in simple environments and on small-scale
datasets. We present \textbf{XLand-100B}, a large-scale dataset for in-context
reinforcement learning based on the XLand-MiniGrid environment, as a first step
to alleviate this problem. It contains complete learning histories for nearly
$30,000$ different tasks, covering $100$B transitions and $2.5$B episodes. It
took $50,000$ GPU hours to collect the dataset, which is beyond the reach of
most academic labs. Along with the dataset, we provide the utilities to
reproduce or expand it even further. With this substantial effort, we aim to
democratize research in the rapidly growing field of in-context reinforcement
learning and provide a solid foundation for further scaling. The code is
open-source and available under Apache 2.0 licence at
https://github.com/dunno-lab/xland-minigrid-datasets.

摘要：隨著大型語言和電腦視覺模型中情境學習範例的成功，最近興起的領域情境強化學習正經歷快速成長。然而，它的發展受到缺乏具挑戰性基準的阻礙，因為所有實驗都是在簡單的環境和小型資料集上進行。我們提出 \textbf{XLand-100B}，一個基於 XLand-MiniGrid 環境的情境強化學習大型資料集，作為緩解此問題的第一步。它包含近 30,000 個不同任務的完整學習歷程，涵蓋 100B 轉換和 2.5B 個情節。收集資料集花了 50,000 個 GPU 小時，這超出了大多數學術實驗室的能力範圍。除了資料集，我們還提供公用程式，以進一步複製或擴充它。透過這項重大的努力，我們旨在讓快速成長的情境強化學習領域的研究民主化，並為進一步擴展提供穩固的基礎。程式碼是開放原始碼，並在 https://github.com/dunno-lab/xland-minigrid-datasets 下根據 Apache 2.0 授權提供。

##### **Separation Power of Equivariant Neural Networks**
2406.08966v1 by Marco Pacini, Xiaowen Dong, Bruno Lepri, Gabriele Santin

The separation power of a machine learning model refers to its capacity to
distinguish distinct inputs, and it is often employed as a proxy for its
expressivity. In this paper, we propose a theoretical framework to investigate
the separation power of equivariant neural networks with point-wise
activations. Using the proposed framework, we can derive an explicit
description of inputs indistinguishable by a family of neural networks with
given architecture, demonstrating that it remains unaffected by the choice of
non-polynomial activation function employed. We are able to understand the role
played by activation functions in separability. Indeed, we show that all
non-polynomial activations, such as ReLU and sigmoid, are equivalent in terms
of expressivity, and that they reach maximum discrimination capacity. We
demonstrate how assessing the separation power of an equivariant neural network
can be simplified to evaluating the separation power of minimal
representations. We conclude by illustrating how these minimal components form
a hierarchy in separation power.

摘要：機器學習模型的分離能力是指其區分不同輸入的能力，通常用作其表達能力的代理。在本文中，我們提出一個理論架構來探討具有逐點激活的等變神經網路的分離能力。使用所提出的架構，我們可以推導出由具有給定架構的神經網路家族無法區分輸入的明確描述，證明它不受所採用非多項式激活函數的選擇影響。我們能夠理解激活函數在可分離性中所扮演的角色。事實上，我們表明所有非多項式激活，例如 ReLU 和 sigmoid，在表達能力方面都是等效的，並且它們達到最大的辨別能力。我們展示了如何評估等變神經網路的分離能力可以簡化為評估最小表示的分離能力。我們最後說明這些最小組成如何形成分離能力的階層。

##### **Tool Wear Prediction in CNC Turning Operations using Ultrasonic Microphone Arrays and CNNs**
2406.08957v1 by Jan Steckel, Arne Aerts, Erik Verreycken, Dennis Laurijssen, Walter Daems

This paper introduces a novel method for predicting tool wear in CNC turning
operations, combining ultrasonic microphone arrays and convolutional neural
networks (CNNs). High-frequency acoustic emissions between 0 kHz and 60 kHz are
enhanced using beamforming techniques to improve the signal- to-noise ratio.
The processed acoustic data is then analyzed by a CNN, which predicts the
Remaining Useful Life (RUL) of cutting tools. Trained on data from 350
workpieces machined with a single carbide insert, the model can accurately
predict the RUL of the carbide insert. Our results demonstrate the potential
gained by integrating advanced ultrasonic sensors with deep learning for
accurate predictive maintenance tasks in CNC machining.

摘要：本文提出了一種新穎的方法，用於預測 CNC 車削操作中的刀具磨損，結合超音波麥克風陣列和卷積神經網路 (CNN)。使用波束成形技術增強 0 kHz 至 60 kHz 之間的高頻聲發射，以改善訊噪比。然後由 CNN 分析處理過的聲學資料，預測切削刀具的剩餘使用壽命 (RUL)。該模型以使用單一硬質合金刀片加工的 350 個工件的資料進行訓練，可以準確預測硬質合金刀片的 RUL。我們的結果證明了將先進的超音波感測器與深度學習整合，在 CNC 加工中進行準確的預測性維護任務所獲得的潛力。

##### **Word Order in English-Japanese Simultaneous Interpretation: Analyses and Evaluation using Chunk-wise Monotonic Translation**
2406.08940v1 by Kosuke Doi, Yuka Ko, Mana Makinae, Katsuhito Sudoh, Satoshi Nakamura

This paper analyzes the features of monotonic translations, which follow the
word order of the source language, in simultaneous interpreting (SI). The word
order differences are one of the biggest challenges in SI, especially for
language pairs with significant structural differences like English and
Japanese. We analyzed the characteristics of monotonic translations using the
NAIST English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset
and found some grammatical structures that make monotonic translation difficult
in English-Japanese SI. We further investigated the features of monotonic
translations through evaluating the output from the existing speech translation
(ST) and simultaneous speech translation (simulST) models on NAIST
English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset as well
as on existing test sets. The results suggest that the existing SI-based test
set underestimates the model performance. We also found that the
monotonic-translation-based dataset would better evaluate simulST models, while
using an offline-based test set for evaluating simulST models underestimates
the model performance.

摘要：本文分析了在同聲傳譯 (SI) 中遵循源語言詞序的單調翻譯的特徵。詞序差異是 SI 中最大的挑戰之一，特別是對於英語和日語等結構差異顯著的語言對。我們使用 NAIST 英日詞塊單調翻譯評估資料集分析了單調翻譯的特徵，發現了一些讓英語日語 SI 中的單調翻譯變得困難的語法結構。我們進一步透過評估現有語音翻譯 (ST) 和同聲語音翻譯 (simulST) 模型在 NAIST 英日詞塊單調翻譯評估資料集以及現有測試集上的輸出，調查了單調翻譯的特徵。結果表明，現有的基於 SI 的測試集低估了模型效能。我們還發現，基於單調翻譯的資料集可以更好地評估 simulST 模型，而使用基於離線的測試集來評估 simulST 模型會低估模型效能。

##### **Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning**
2406.08931v1 by Arnav Goel, Medha Hira, Anubha Gupta

Advent of modern deep learning techniques has given rise to advancements in
the field of Speech Emotion Recognition (SER). However, most systems prevalent
in the field fail to generalize to speakers not seen during training. This
study focuses on handling challenges of multilingual SER, specifically on
unseen speakers. We introduce CAMuLeNet, a novel architecture leveraging
co-attention based fusion and multitask learning to address this problem.
Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0,
and WavLM using 10-fold leave-speaker-out cross-validation on five existing
multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and,
release a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet
shows an average improvement of approximately 8% over all benchmarks on unseen
speakers determined by our cross-validation strategy.

摘要：現代深度學習技術的出現促進了語音情緒辨識 (SER) 領域的進步。然而，該領域中普遍存在的系統無法推廣到訓練期間未見過的說話者。本研究專注於處理多語言 SER 的挑戰，特別是針對未見過的說話者。我們引入了 CAMuLeNet，這是一種新穎的架構，利用基於共同注意力的融合和多任務學習來解決此問題。此外，我們使用 10 倍留說話者交叉驗證對 Whisper、HuBERT、Wav2Vec2.0 和 WavLM 的預訓練編碼器進行基準測試，並在五個現有的多語言基準資料集上進行驗證：IEMOCAP、RAVDESS、CREMA-D、EmoDB 和 CaFE，並發布一個新的印地語 SER 資料集 (BhavVani)。根據我們的交叉驗證策略，CAMuLeNet 在所有基準上對未見過說話者的平均改進約為 8%。

##### **Efficient Multi-View Fusion and Flexible Adaptation to View Missing in Cardiovascular System Signals**
2406.08930v1 by Qihan Hu, Daomiao Wang, Hong Wu, Jian Liu, Cuiwei Yang

The progression of deep learning and the widespread adoption of sensors have
facilitated automatic multi-view fusion (MVF) about the cardiovascular system
(CVS) signals. However, prevalent MVF model architecture often amalgamates CVS
signals from the same temporal step but different views into a unified
representation, disregarding the asynchronous nature of cardiovascular events
and the inherent heterogeneity across views, leading to catastrophic view
confusion. Efficient training strategies specifically tailored for MVF models
to attain comprehensive representations need simultaneous consideration.
Crucially, real-world data frequently arrives with incomplete views, an aspect
rarely noticed by researchers. Thus, the View-Centric Transformer (VCT) and
Multitask Masked Autoencoder (M2AE) are specifically designed to emphasize the
centrality of each view and harness unlabeled data to achieve superior fused
representations. Additionally, we systematically define the missing-view
problem for the first time and introduce prompt techniques to aid pretrained
MVF models in flexibly adapting to various missing-view scenarios. Rigorous
experiments involving atrial fibrillation detection, blood pressure estimation,
and sleep staging-typical health monitoring tasks-demonstrate the remarkable
advantage of our method in MVF compared to prevailing methodologies. Notably,
the prompt technique requires finetuning less than 3% of the entire model's
data, substantially fortifying the model's resilience to view missing while
circumventing the need for complete retraining. The results demonstrate the
effectiveness of our approaches, highlighting their potential for practical
applications in cardiovascular health monitoring. Codes and models are released
at URL.

摘要：深度學習的進展和感測器的廣泛採用，促進了心血管系統 (CVS) 訊號的自動多視角融合 (MVF)。然而，普遍的 MVF 模型架構經常將來自相同時間步驟但不同視角的 CVS 訊號融合為統一的表示，忽視了心血管事件的非同步性質和視角之間的內在異質性，導致災難性的視角混淆。需要同時考慮專門針對 MVF 模型量身打造的有效訓練策略，以獲得全面的表示。至關重要的是，真實世界的資料經常會出現不完整的視角，研究人員很少注意到這個面向。因此，視角中心Transformer (VCT) 和多任務遮罩式自動編碼器 (M2AE) 被特別設計為強調每個視角的中心性，並利用未標記的資料來實現優越的融合表示。此外，我們系統性地首次定義了缺失視角問題，並引入了提示技術，以幫助預訓練的 MVF 模型靈活適應各種缺失視角場景。涉及心房顫動偵測、血壓估計和睡眠分期的嚴謹實驗（典型的健康監測任務）證明了我們的方法在 MVF 中與現有方法相比具有顯著優勢。值得注意的是，提示技術需要微調不到整個模型資料的 3%，這大大加強了模型對視角缺失的復原力，同時避免了重新訓練的需要。結果證明了我們方法的有效性，突出了它們在心血管健康監測中的實際應用潛力。程式碼和模型已在 URL 中發布。

##### **Step-by-Step Diffusion: An Elementary Tutorial**
2406.08929v1 by Preetum Nakkiran, Arwen Bradley, Hattie Zhou, Madhu Advani

We present an accessible first course on diffusion models and flow matching
for machine learning, aimed at a technical audience with no diffusion
experience. We try to simplify the mathematical details as much as possible
(sometimes heuristically), while retaining enough precision to derive correct
algorithms.

摘要：我們提供一個關於擴散模型和流匹配的平易近人的入門課程，目標受眾為沒有擴散經驗的技術人士。我們盡可能簡化數學細節（有時採用啟發式方法），同時保留足夠的精確度以推導出正確的演算法。

##### **Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors**
2406.08922v1 by Ying Zhou, Ben He, Le Sun

With the launch of ChatGPT, large language models (LLMs) have attracted
global attention. In the realm of article writing, LLMs have witnessed
extensive utilization, giving rise to concerns related to intellectual property
protection, personal privacy, and academic integrity. In response, AI-text
detection has emerged to distinguish between human and machine-generated
content. However, recent research indicates that these detection systems often
lack robustness and struggle to effectively differentiate perturbed texts.
Currently, there is a lack of systematic evaluations regarding detection
performance in real-world applications, and a comprehensive examination of
perturbation techniques and detector robustness is also absent. To bridge this
gap, our work simulates real-world scenarios in both informal and professional
writing, exploring the out-of-the-box performance of current detectors.
Additionally, we have constructed 12 black-box text perturbation methods to
assess the robustness of current detection models across various perturbation
granularities. Furthermore, through adversarial learning experiments, we
investigate the impact of perturbation data augmentation on the robustness of
AI-text detectors. We have released our code and data at
https://github.com/zhouying20/ai-text-detector-evaluation.

摘要：<paragraph>隨著 ChatGPT 的推出，大型語言模型 (LLM) 吸引了全球的關注。在文章寫作領域，LLM 已廣泛使用，引發了與智慧財產權保護、個人隱私和學術誠信相關的疑慮。為了解決這些問題，AI 文字偵測技術應運而生，用以區分人類和機器產生的內容。然而，最近的研究指出，這些偵測系統通常缺乏穩健性，難以有效區分擾動文字。目前，對於實際應用中的偵測效能缺乏系統性的評估，且對擾動技術和偵測器穩健性的全面檢視也付之闕如。為了彌補這個差距，我們的工作模擬了非正式和專業寫作中的真實世界場景，探索了當前偵測器的開箱即用效能。此外，我們構建了 12 種黑盒文字擾動方法，以評估當前偵測模型在各種擾動粒度下的穩健性。進一步地，透過對抗式學習實驗，我們探討了擾動資料擴充對 AI 文字偵測器穩健性的影響。我們已在 https://github.com/zhouying20/ai-text-detector-evaluation/ 發布我們的程式碼和資料。</paragraph>

##### **AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis**
2406.08920v2 by Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiankang Deng, Xiatian Zhu

Novel view acoustic synthesis (NVAS) aims to render binaural audio at any
target viewpoint, given a mono audio emitted by a sound source at a 3D scene.
Existing methods have proposed NeRF-based implicit models to exploit visual
cues as a condition for synthesizing binaural audio. However, in addition to
low efficiency originating from heavy NeRF rendering, these methods all have a
limited ability of characterizing the entire scene environment such as room
geometry, material properties, and the spatial relation between the listener
and sound source. To address these issues, we propose a novel Audio-Visual
Gaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware
condition for audio synthesis, we learn an explicit point-based scene
representation with an audio-guidance parameter on locally initialized Gaussian
points, taking into account the space relation from the listener and sound
source. To make the visual scene model audio adaptive, we propose a point
densification and pruning strategy to optimally distribute the Gaussian points,
with the per-point contribution in sound propagation (e.g., more points needed
for texture-less wall surfaces as they affect sound path diversion). Extensive
experiments validate the superiority of our AV-GS over existing alternatives on
the real-world RWAS and simulation-based SoundSpaces datasets.

摘要：新穎觀點聲學合成（NVAS）旨在呈現任何目標視點的雙耳音訊，並提供 3D 場景中聲源發出的單聲道音訊。現有方法已提出基於 NeRF 的隱式模型，以利用視覺線索作為合成雙耳音訊的條件。然而，除了源自繁重 NeRF 渲染的低效率之外，這些方法在表徵整個場景環境（例如房間幾何形狀、材料屬性和聽眾與聲源之間的空間關係）的能力上也有限。為了解決這些問題，我們提出了一個新穎的音視高斯潑點（AV-GS）模型。為了獲得對材料和幾何形狀感知的音訊合成條件，我們學習了一個明確的基於點的場景表示，其中包含局部初始化的高斯點上的音訊引導參數，並考慮了聽眾與聲源的空間關係。為了使視覺場景模型適應音訊，我們提出了一個點增密和剪枝策略，以最佳方式分佈高斯點，並在聲音傳播中按點貢獻（例如，紋理較少的牆面需要更多點，因為它們會影響聲音路徑轉移）。廣泛的實驗驗證了我們的 AV-GS 在真實世界 RWAS 和基於模擬的 SoundSpaces 資料集上優於現有替代方案。

##### **An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios**
2406.08911v1 by Cheng Gong, Erica Cooper, Xin Wang, Chunyu Qiang, Mengzhe Geng, Dan Wells, Longbiao Wang, Jianwu Dang, Marc Tessier, Aidan Pine, Korin Richmond, Junichi Yamagishi

Self-supervised learning (SSL) representations from massively multilingual
models offer a promising solution for low-resource language speech tasks.
Despite advancements, language adaptation in TTS systems remains an open
problem. This paper explores the language adaptation capability of ZMM-TTS, a
recent SSL-based multilingual TTS system proposed in our previous work. We
conducted experiments on 12 languages using limited data with various
fine-tuning configurations. We demonstrate that the similarity in phonetics
between the pre-training and target languages, as well as the language
category, affects the target language's adaptation performance. Additionally,
we find that the fine-tuning dataset size and number of speakers influence
adaptability. Surprisingly, we also observed that using paired data for
fine-tuning is not always optimal compared to audio-only data. Beyond speech
intelligibility, our analysis covers speaker similarity, language
identification, and predicted MOS.

摘要：自監督式學習 (SSL) 的表徵來自大量多語言模型，為低資源語言語音任務提供了一個有希望的解決方案。儘管有進展，TTS 系統中的語言適應仍然是一個未解決的問題。本文探討了 ZMM-TTS 的語言適應能力，這是一個我們先前工作中提出的基於 SSL 的多語言 TTS 系統。我們使用有限的資料，在 12 種語言上進行了實驗，並採用了各種微調組態。我們證明了預訓練語言和目標語言之間的音標相似性，以及語言類別，會影響目標語言的適應效能。此外，我們發現微調資料集大小和說話者數量會影響適應性。令人驚訝的是，我們還觀察到，與僅音訊資料相比，使用配對資料進行微調並非總是最佳選擇。除了語音清晰度之外，我們的分析還涵蓋了說話者相似性、語言辨識和預測的 MOS。

##### **Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models**
2406.08903v1 by Bowen Ping, Shuo Wang, Hanqing Wang, Xu Han, Yuzhuang Xu, Yukun Yan, Yun Chen, Baobao Chang, Zhiyuan Liu, Maosong Sun

Fine-tuning is a crucial process for adapting large language models (LLMs) to
diverse applications. In certain scenarios, such as multi-tenant serving,
deploying multiple LLMs becomes necessary to meet complex demands. Recent
studies suggest decomposing a fine-tuned LLM into a base model and
corresponding delta weights, which are then compressed using low-rank or
low-bit approaches to reduce costs. In this work, we observe that existing
low-rank and low-bit compression methods can significantly harm the model
performance for task-specific fine-tuned LLMs (e.g., WizardMath for math
problems). Motivated by the long-tail distribution of singular values in the
delta weights, we propose a delta quantization approach using mixed-precision.
This method employs higher-bit representation for singular vectors
corresponding to larger singular values. We evaluate our approach on various
fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.
Experimental results demonstrate that our approach performs comparably to full
fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a
considerable margin. Additionally, we show that our method is compatible with
various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its
generalizability.

摘要：微調是將大型語言模型 (LLM) 適應於各種應用程式的重要過程。在某些情況下，例如多租戶服務，部署多個 LLM 以滿足複雜的需求變得有必要。最近的研究表明將微調的 LLM 分解成基礎模型和對應的 delta 權重，然後使用低秩或低位元方法壓縮它們以降低成本。在這項工作中，我們觀察到現有的低秩和低位元壓縮方法可能會嚴重損害任務特定的微調 LLM（例如，用於數學問題的 WizardMath）的模型效能。受到 delta 權重中奇異值長尾分佈的啟發，我們提出了一種使用混合精度的 delta 量化方法。此方法採用對應於較大奇異值的奇異向量的較高位元表示。我們在各種微調的 LLM 上評估我們的作法，包括數學 LLM、程式碼 LLM、聊天 LLM，甚至是 VLM。實驗結果表明，我們的作法執行起來可與完整的微調 LLM 相媲美，在相當大的幅度上超越了低秩和低位元的基準。此外，我們展示了我們的作法與各種主幹 LLM 相容，例如 Llama-2、Llama-3 和 Mistral，突顯了它的通用性。

##### **No perspective, no perception!! Perspective-aware Healthcare Answer Summarization**
2406.08881v1 by Gauri Naik, Sharad Chandakacherla, Shweta Yadav, Md. Shad Akhtar

Healthcare Community Question Answering (CQA) forums offer an accessible
platform for individuals seeking information on various healthcare-related
topics. People find such platforms suitable for self-disclosure, seeking
medical opinions, finding simplified explanations for their medical conditions,
and answering others' questions. However, answers on these forums are typically
diverse and prone to off-topic discussions. It can be challenging for readers
to sift through numerous answers and extract meaningful insights, making answer
summarization a crucial task for CQA forums. While several efforts have been
made to summarize the community answers, most of them are limited to the open
domain and overlook the different perspectives offered by these answers. To
address this problem, this paper proposes a novel task of perspective-specific
answer summarization. We identify various perspectives, within
healthcare-related responses and frame a perspective-driven abstractive summary
covering all responses. To achieve this, we annotate 3167 CQA threads with 6193
perspective-aware summaries in our PUMA dataset. Further, we propose PLASMA, a
prompt-driven controllable summarization model. To encapsulate the
perspective-specific conditions, we design an energy-controlled loss function
for the optimization. We also leverage the prefix tuner to learn the
intricacies of the health-care perspective summarization. Our evaluation
against five baselines suggests the superior performance of PLASMA by a margin
of 1.5-21% improvement. We supplement our experiments with ablation and
qualitative analysis.

摘要：<paragraph>醫療保健社群問答 (CQA) 論壇提供了一個可供個人尋找各種醫療保健相關主題資訊的平臺。人們發現此類平臺適合自我揭露、尋求醫療意見、尋找其病情的簡化說明，以及回答他人的問題。然而，這些論壇上的答案通常很分散，且容易出現離題的討論。讀者可能很難篩選眾多答案並提取有意義的見解，這使得答案摘要成為 CQA 論壇的一項重要任務。儘管已做出多項努力來總結社群答案，但其中大多數僅限於開放領域，且忽略了這些答案提供的不同觀點。為了解決這個問題，本文提出了一項觀點特定的答案摘要新任務。我們在醫療保健相關的回應中識別出各種觀點，並構建一個觀點驅動的摘要摘要，涵蓋所有回應。為此，我們使用我們的 PUMA 資料集為 3167 個 CQA 執行緒加上 6193 個觀點感知摘要的註解。此外，我們提出 PLASMA，一種提示驅動的可控摘要模型。為了概括觀點特定的條件，我們設計了一個能量控制損失函數進行最佳化。我們還利用前綴調諧器來學習醫療保健觀點摘要的複雜性。我們對五個基準的評估表明，PLASMA 的效能優於基準，改進幅度在 1.5-21% 之間。我們使用消融和定性分析補充我們的實驗。</paragraph>

##### **Research on Early Warning Model of Cardiovascular Disease Based on Computer Deep Learning**
2406.08864v1 by Yuxiang Hu, Jinxin Hu, Ting Xu, Bo Zhang, Jiajie Yuan, Haozhang Deng

This project intends to study a cardiovascular disease risk early warning
model based on one-dimensional convolutional neural networks. First, the
missing values of 13 physiological and symptom indicators such as patient age,
blood glucose, cholesterol, and chest pain were filled and Z-score was
standardized. The convolutional neural network is converted into a 2D matrix,
the convolution function of 1,3, and 5 is used for the first-order convolution
operation, and the Max Pooling algorithm is adopted for dimension reduction.
Set the learning rate and output rate. It is optimized by the Adam algorithm.
The result of classification is output by a soft classifier. This study was
conducted based on Statlog in the UCI database and heart disease database
respectively. The empirical data indicate that the forecasting precision of
this technique has been enhanced by 11.2%, relative to conventional approaches,
while there is a significant improvement in the logarithmic curve fitting. The
efficacy and applicability of the novel approach are corroborated through the
examination employing a one-dimensional convolutional neural network.

摘要：本研究拟对基于一维卷积神经网络的心血管疾病风险预警模型进行研究。首先，对患者年龄、血糖、胆固醇、胸痛等13项生理及症状指标的缺失值进行填充并进行Z-score标准化。将卷积神经网络转化为二维矩阵，采用1、3、5的卷积核进行一维卷积操作，采用最大池化算法进行降维。设置学习率和输出率。采用Adam算法进行优化。通过softmax分类器输出分类结果。本研究分别基于UCI数据库中的Statlog和心脏病数据库进行。实证结果表明，该方法较传统方法预测精度提升11.2%，对数拟合曲线拟合效果明显提升。通过一维卷积神经网络的实验验证了该方法的有效性和适用性。

##### **Self-supervised Graph Neural Network for Mechanical CAD Retrieval**
2406.08863v1 by Yuhan Quan, Huan ZHao, Jinfeng Yi, Yuqiang Chen

CAD (Computer-Aided Design) plays a crucial role in mechanical industry,
where large numbers of similar-shaped CAD parts are often created. Efficiently
reusing these parts is key to reducing design and production costs for
enterprises. Retrieval systems are vital for achieving CAD reuse, but the
complex shapes of CAD models are difficult to accurately describe using text or
keywords, making traditional retrieval methods ineffective. While existing
representation learning approaches have been developed for CAD, manually
labeling similar samples in these methods is expensive. Additionally, CAD
models' unique parameterized data structure presents challenges for applying
existing 3D shape representation learning techniques directly. In this work, we
propose GC-CAD, a self-supervised contrastive graph neural network-based method
for mechanical CAD retrieval that directly models parameterized CAD raw files.
GC-CAD consists of two key modules: structure-aware representation learning and
contrastive graph learning framework. The method leverages graph neural
networks to extract both geometric and topological information from CAD models,
generating feature representations. We then introduce a simple yet effective
contrastive graph learning framework approach, enabling the model to train
without manual labels and generate retrieval-ready representations.
Experimental results on four datasets including human evaluation demonstrate
that the proposed method achieves significant accuracy improvements and up to
100 times efficiency improvement over the baseline methods.

摘要：電腦輔助設計 (CAD) 在機械產業中扮演著至關重要的角色，
其中經常會產生大量形狀相似的 CAD 零件。有效地重複使用這些零件是降低企業設計和生產成本的關鍵。檢索系統對於實現 CAD 重複使用至關重要，但 CAD 模型的複雜形狀很難使用文字或關鍵字準確描述，這使得傳統的檢索方法無效。雖然現有的表示學習方法已針對 CAD 開發，但使用這些方法手動標記相似的樣本成本很高。此外，CAD 模型獨特的參數化數據結構對直接應用現有的 3D 形狀表示學習技術提出了挑戰。在這項工作中，我們提出了 GC-CAD，這是一種基於自監督對比圖神經網路的方法，用於直接對參數化的 CAD 原始檔案進行機械 CAD 檢索。GC-CAD 包含兩個關鍵模組：結構感知表示學習和對比圖學習架構。該方法利用圖神經網路從 CAD 模型中提取幾何和拓撲資訊，生成特徵表示。然後，我們引入了一個簡單但有效的對比圖學習架構方法，使模型能夠在沒有手動標籤的情況下進行訓練並生成可檢索的表示。在包括人類評估在內的四個資料集上的實驗結果表明，所提出的方法在準確度上取得了顯著的改進，並且比基線方法的效率提高了 100 倍。

##### **Plan, Generate and Complicate: Improving Low-resource Dialogue State Tracking via Easy-to-Difficult Zero-shot Data Augmentation**
2406.08860v1 by Ming Gu, Yan Yang

Data augmentation methods have been a promising direction to improve the
performance of small models for low-resource dialogue state tracking. However,
traditional methods rely on pre-defined user goals and neglect the importance
of data complexity in this task. In this paper, we propose EDZ-DA, an
Easy-to-Difficult Zero-shot Data Augmentation framework for low-resource
dialogue state tracking that utilizes large language models to automatically
catch the relationships of different domains and then generate the dialogue
data. We also complicate the dialogues based on the domain relation to enhance
the model's capability for co-reference slot tracking. Furthermore, we permute
slot values to mitigate the influence of output orders and the problem of
incomplete value generation. Experimental results illustrate the superiority of
our proposed method compared to previous strong data augmentation baselines on
MultiWOZ.

摘要：資料擴充方法一直是改善低資源對話狀態追蹤的小型模型效能的一個有前途的方向。然而，傳統方法依賴於預先定義的使用者目標，並且忽略了資料複雜性在此任務中的重要性。在本文中，我們提出 EDZ-DA，一個針對低資源對話狀態追蹤的易於困難零次資料擴充架構，它利用大型語言模型自動擷取不同領域的關係，然後產生對話資料。我們也根據領域關係複雜化對話，以增強模型對共指槽追蹤的能力。此外，我們排列槽位值以減輕輸出順序的影響和不完整值產生的問題。實驗結果說明我們提出的方法比以前在 MultiWOZ 上強大的資料擴充基準表現更優異。

##### **Current applications and potential future directions of reinforcement learning-based Digital Twins in agriculture**
2406.08854v1 by Georg Goldenits, Kevin Mallinger, Sebastian Raubitzek, Thomas Neubauer

Digital Twins have gained attention in various industries for simulation,
monitoring, and decision-making, relying on ever-improving machine learning
models. However, agricultural Digital Twin implementations are limited compared
to other industries. Meanwhile, machine learning, particularly reinforcement
learning, has shown potential in agricultural applications like optimizing
decision-making, task automation, and resource management. A key aspect of
Digital Twins is representing physical assets or systems in a virtual
environment, which aligns well with reinforcement learning's need for
environment representations to learn the best policy for a task. Reinforcement
learning in agriculture can thus enable various Digital Twin applications in
agricultural domains. This review aims to categorize existing research
employing reinforcement learning in agricultural settings by application
domains like robotics, greenhouse management, irrigation systems, and crop
management, identifying potential future areas for reinforcement learning-based
Digital Twins. It also categorizes the reinforcement learning techniques used,
including tabular methods, Deep Q-Networks (DQN), Policy Gradient methods, and
Actor-Critic algorithms, to overview currently employed models. The review
seeks to provide insights into the state-of-the-art in integrating Digital
Twins and reinforcement learning in agriculture, identifying gaps and
opportunities for future research, and exploring synergies to tackle
agricultural challenges and optimize farming, paving the way for more efficient
and sustainable farming methodologies.

摘要：數位分身在各種產業中受到關注，用於模擬、監控和決策制定，並依賴於不斷改進的機器學習模型。然而，與其他產業相比，農業數位分身的應用受到限制。與此同時，機器學習，特別是強化學習，已在農業應用中展現潛力，例如最佳化決策制定、任務自動化和資源管理。數位分身的關鍵面向是在虛擬環境中表示實體資產或系統，這與強化學習需要環境表示以學習任務的最佳策略非常吻合。因此，農業中的強化學習可以支援農業領域中的各種數位分身應用。本篇評論旨在依據機器人在農業環境中使用的應用領域（例如機器人、溫室管理、灌溉系統和作物管理）對採用強化學習的現有研究進行分類，找出強化學習為基礎的數位分身潛在的未來領域。它也對所使用的強化學習技術進行分類，包括表格方法、深度 Q 網路 (DQN)、策略梯度方法和 Actor-Critic 演算法，以概述目前採用的模型。本篇評論旨在深入了解整合數位分身和農業中的強化學習的最新技術，找出未來研究的差距和機會，並探索解決農業挑戰和最佳化農業的協同作用，為更有效率和永續的農業方法鋪路。

##### **An Approach to Build Zero-Shot Slot-Filling System for Industry-Grade Conversational Assistants**
2406.08848v1 by G P Shrivatsa Bhargav, Sumit Neelam, Udit Sharma, Shajith Ikbal, Dheeraj Sreedhar, Hima Karanam, Sachindra Joshi, Pankaj Dhoolia, Dinesh Garg, Kyle Croutwater, Haode Qi, Eric Wayne, J William Murdock

We present an approach to build Large Language Model (LLM) based slot-filling
system to perform Dialogue State Tracking in conversational assistants serving
across a wide variety of industry-grade applications. Key requirements of this
system include: 1) usage of smaller-sized models to meet low latency
requirements and to enable convenient and cost-effective cloud and customer
premise deployments, and 2) zero-shot capabilities to serve across a wide
variety of domains, slot types and conversational scenarios. We adopt a
fine-tuning approach where a pre-trained LLM is fine-tuned into a slot-filling
model using task specific data. The fine-tuning data is prepared carefully to
cover a wide variety of slot-filling task scenarios that the model is expected
to face across various domains. We give details of the data preparation and
model building process. We also give a detailed analysis of the results of our
experimental evaluations. Results show that our prescribed approach for
slot-filling model building has resulted in 6.9% relative improvement of F1
metric over the best baseline on a realistic benchmark, while at the same time
reducing the latency by 57%. More over, the data we prepared has helped improve
F1 on an average by 4.2% relative across various slot-types.

摘要：我們提出了一種構建大型語言模型 (LLM) 的方法，以打造基於槽位填補的系統，用於執行對話狀態追蹤，服務於各種產業級應用。此系統的主要需求包括：1) 使用較小型的模型以符合低延遲的要求，並能進行便利且具成本效益的雲端和客戶端部署，以及 2) 零次學習能力，以服務於各種領域、槽位類型和對話場景。我們採用微調方法，將預先訓練好的 LLM 微調成槽位填補模型，使用特定於任務的資料。微調資料經過仔細準備，以涵蓋模型在各種領域中預期會遇到的各種槽位填補任務場景。我們提供了資料準備和模型建置流程的詳細資訊。我們也對實驗評估的結果進行了詳細分析。結果顯示，我們規定的槽位填補模型建置方法，在實際基準上，相較於最佳基準，F1 指標提升了 6.9%，同時將延遲降低了 57%。此外，我們準備的資料有助於在各種槽位類型中平均提升 F1 指標 4.2%。

##### **ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions**
2406.08842v1 by Xu Zhang, Xunjian Yin, Xiaojun Wan

While substantial advancements have been made in developing large language
models (LLMs), achieving control over their behavior can be difficult. Direct
preference optimization (DPO) assumes the existence of a latent reward function
to evaluate the responses of LLMs. This assumption indicates a strict
preference ordering of different responses to the same input. However, there
always exist contradictions of preference in LLMs according to our experimental
observations. In this paper, we construct a graph structure of the preference
relationship among different responses with self-annotation to find
contradictions in the preference order. We propose ContraSolver, an algorithm
that traverses all edges on the preference graph to identify those that might
cause contradictions. ContraSolver initializes the graph with a maximum
spanning tree and identifies contradictory edges, prioritizing the resolution
of low-confidence preferences while preserving high-confidence ones.
Experimental results on four different generation tasks show that the
performance of different LLMs can be largely improved through our completely
unsupervised self-alignment. Furthermore, by analyzing the preference graphs of
LLMs with and without self-alignment by ContraSolver, we quantify the reduction
in contradictions, suggesting that resolving preference contradictions is
crucial for achieving better alignment performance.

摘要：儘管在開發大型語言模型 (LLM) 方面已經取得實質性進展，但要控制其行為可能會很困難。直接偏好最佳化 (DPO) 假設存在一個潛在的獎勵函數來評估 LLM 的回應。此假設表示對相同輸入的不同回應有嚴格的偏好排序。然而，根據我們的實驗觀察，在 LLM 中始終存在偏好的矛盾。在本文中，我們構建了一個偏好關係圖，其中包含不同回應的自我註解，以找出偏好順序中的矛盾。我們提出 ContraSolver，這是一種演算法，用於遍歷偏好圖上的所有邊緣，以找出可能導致矛盾的邊緣。ContraSolver 使用最大生成樹初始化圖形，並找出矛盾的邊緣，優先解決低信心的偏好，同時保留高信心的偏好。在四項不同的生成任務上的實驗結果顯示，透過我們完全無監督的自對齊，可以大幅改善不同 LLM 的效能。此外，透過分析使用 ContraSolver 進行自對齊和未進行自對齊的 LLM 的偏好圖，我們量化了矛盾的減少，這表示解決偏好矛盾對於達成更好的對齊效能至關重要。

##### **Research on Optimization of Natural Language Processing Model Based on Multimodal Deep Learning**
2406.08838v1 by Dan Sun, Yaxin Liang, Yining Yang, Yuhan Ma, Qishi Zhan, Erdi Gao

This project intends to study the image representation based on attention
mechanism and multimodal data. By adding multiple pattern layers to the
attribute model, the semantic and hidden layers of image content are
integrated. The word vector is quantified by the Word2Vec method and then
evaluated by a word embedding convolutional neural network. The published
experimental results of the two groups were tested. The experimental results
show that this method can convert discrete features into continuous characters,
thus reducing the complexity of feature preprocessing. Word2Vec and natural
language processing technology are integrated to achieve the goal of direct
evaluation of missing image features. The robustness of the image feature
evaluation model is improved by using the excellent feature analysis
characteristics of a convolutional neural network. This project intends to
improve the existing image feature identification methods and eliminate the
subjective influence in the evaluation process. The findings from the
simulation indicate that the novel approach has developed is viable,
effectively augmenting the features within the produced representations.

摘要：本專案旨在研究基於注意力機制和多模態資料的影像表徵。透過在屬性模型中加入多重模式層，將影像內容的語意層和隱藏層整合起來。將詞向量透過 Word2Vec 方法量化，再由詞嵌入式卷積神經網路進行評估。測試了兩組已發表的實驗結果。實驗結果顯示，此方法能將離散特徵轉換為連續特徵，進而降低特徵前處理的複雜度。將 Word2Vec 和自然語言處理技術整合，達成直接評估缺失影像特徵的目標。利用卷積神經網路優異的特徵分析特性，提升影像特徵評估模型的魯棒性。本專案旨在改善現有的影像特徵辨識方法，並消除評估過程中的主觀影響。模擬結果顯示，所發展的新穎方法是可行的，能有效地擴充表徵中所產生的特徵。

##### **Center-Sensitive Kernel Optimization for Efficient On-Device Incremental Learning**
2406.08830v1 by Dingwen Zhang, Yan Li, De Cheng, Nannan Wang, Junwei Han

To facilitate the evolution of edge intelligence in ever-changing
environments, we study on-device incremental learning constrained in limited
computation resource in this paper. Current on-device training methods just
focus on efficient training without considering the catastrophic forgetting,
preventing the model getting stronger when continually exploring the world. To
solve this problem, a direct solution is to involve the existing incremental
learning mechanisms into the on-device training framework. Unfortunately, such
a manner cannot work well as those mechanisms usually introduce large
additional computational cost to the network optimization process, which would
inevitably exceed the memory capacity of the edge devices. To address this
issue, this paper makes an early effort to propose a simple but effective
edge-friendly incremental learning framework. Based on an empirical study on
the knowledge intensity of the kernel elements of the neural network, we find
that the center kernel is the key for maximizing the knowledge intensity for
learning new data, while freezing the other kernel elements would get a good
balance on the model's capacity for overcoming catastrophic forgetting. Upon
this finding, we further design a center-sensitive kernel optimization
framework to largely alleviate the cost of the gradient computation and
back-propagation. Besides, a dynamic channel element selection strategy is also
proposed to facilitate a sparse orthogonal gradient projection for further
reducing the optimization complexity, upon the knowledge explored from the new
task data. Extensive experiments validate our method is efficient and
effective, e.g., our method achieves average accuracy boost of 38.08% with even
less memory and approximate computation compared to existing on-device training
methods, indicating its significant potential for on-device incremental
learning.

摘要：為了促進邊緣智慧在瞬息萬變的環境中演進，我們在本文中研究了受限於有限計算資源的裝置內增量學習。目前的裝置內訓練方法僅專注於有效率的訓練，而未考慮災難性遺忘，這會阻止模型在持續探索世界時變得更強大。為了解決此問題，一個直接的解決方案是將現有的增量學習機制納入裝置內訓練架構。不幸的是，這種方式無法順利運作，因為這些機制通常會為網路最佳化程序帶來大量的額外運算成本，這將不可避免地超過邊緣裝置的記憶體容量。為了解決此問題，本文提出了一個簡單但有效的邊緣友善增量學習架構。根據對神經網路核心元素知識強度的實證研究，我們發現中心核心是最大化新資料學習知識強度的關鍵，而凍結其他核心元素將在克服災難性遺忘的模型容量上取得良好的平衡。根據此發現，我們進一步設計了一個中心敏感核心最佳化架構，以大幅減輕梯度運算和反向傳播的成本。此外，還提出了一個動態通道元素選擇策略，以利於稀疏正交梯度投影，進一步降低最佳化複雜度，根據從新任務資料中探索的知識。廣泛的實驗驗證了我們的方法是有效率且有效的，例如，與現有的裝置內訓練方法相比，即使記憶體更少且運算量更接近，我們的平均準確度提升了 38.08%，這表示它在裝置內增量學習方面具有顯著的潛力。

##### **Estimating Difficulty Levels of Programming Problems with Pre-trained Model**
2406.08828v1 by Zhiyuan Wang, Wei Zhang, Jun Wang

As the demand for programming skills grows across industries and academia,
students often turn to Programming Online Judge (POJ) platforms for coding
practice and competition. The difficulty level of each programming problem
serves as an essential reference for guiding students' adaptive learning.
However, current methods of determining difficulty levels either require
extensive expert annotations or take a long time to accumulate enough student
solutions for each problem. To address this issue, we formulate the problem of
automatic difficulty level estimation of each programming problem, given its
textual description and a solution example of code. For tackling this problem,
we propose to couple two pre-trained models, one for text modality and the
other for code modality, into a unified model. We built two POJ datasets for
the task and the results demonstrate the effectiveness of the proposed approach
and the contributions of both modalities.

摘要：隨著各行各業和學術界對程式設計技能的需求日益增長，學生經常求助於程式設計線上評審（POJ）平台進行編碼練習和競賽。每個程式設計題目的難度等級是指導學生適應性學習的重要參考。然而，目前用於確定難度等級的方法，要不就是需要大量的專家註解，要不就是需要花費很長的時間才能為每個問題累積足夠的學生解答。為了解決這個問題，我們制定了一個問題，即在給定文字描述和程式碼範例解答的情況下，自動評估每個程式設計題目的難度等級。為了解決這個問題，我們提議將兩個預先訓練好的模型（一個用於文字形式，另一個用於程式碼形式）結合到一個統一的模型中。我們為這個任務建立了兩個 POJ 資料集，結果證明了所提出的方法和兩種形式的貢獻的有效性。

##### **LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions**
2406.08824v1 by Rumaisa Azeem, Andrew Hundt, Masoumeh Mansouri, Martim Brandão

Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI)
communities have proposed Large Language Models (LLMs) as a promising resource
for robotics tasks such as natural language interactions, doing household and
workplace tasks, approximating `common sense reasoning', and modeling humans.
However, recent research has raised concerns about the potential for LLMs to
produce discriminatory outcomes and unsafe behaviors in real-world robot
experiments and applications. To address these concerns, we conduct an
HRI-based evaluation of discrimination and safety criteria on several
highly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness
when encountering people across a diverse range of protected identity
characteristics (e.g., race, gender, disability status, nationality, religion,
and their intersections), producing biased outputs consistent with directly
discriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled
untrustworthy, but not `european' or `able-bodied' people. Furthermore, we test
models in settings with unconstrained natural language (open vocabulary)
inputs, and find they fail to act safely, generating responses that accept
dangerous, violent, or unlawful instructions -- such as incident-causing
misstatements, taking people's mobility aids, and sexual predation. Our results
underscore the urgent need for systematic, routine, and comprehensive risk
assessments and assurances to improve outcomes and ensure LLMs only operate on
robots when it is safe, effective, and just to do so. Data and code will be
made available.

摘要：人機互動 (HRI) 和人工智慧 (AI) 社群的成員已提出大型語言模型 (LLM) 作為機器人任務的有希望資源，例如自然語言互動、執行家庭和工作場所任務、近似「常識推理」以及人類建模。然而，最近的研究對 LLM 在現實世界的機器人實驗和應用中產生歧視性結果和不安全行為的可能性提出了擔憂。為了解決這些問題，我們對幾個評分很高的 LLM 進行了基於 HRI 的歧視和安全標準評估。我們的評估顯示，LLM 目前在遇到具有各種受保護身份特徵（例如種族、性別、殘疾狀況、國籍、宗教及其交集）的人時缺乏穩健性，產生與直接歧視性結果一致的偏見輸出——例如「吉普賽人」和「啞巴」被標記為不可信，但「歐洲人」或「健全人」則不然。此外，我們在具有不受約束的自然語言（開放式詞彙）輸入的設置中測試模型，發現它們無法安全地執行，會產生接受危險、暴力或非法指令的回應——例如造成事故的錯誤陳述、拿走人們的行動輔具和性掠奪。我們的結果強調了對系統性、例行性和全面的風險評估和保證的迫切需要，以改善結果並確保 LLM 僅在安全、有效和公正的情況下在機器人上運行。數據和代碼將會提供。

