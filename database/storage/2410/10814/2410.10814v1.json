{"2410.10814": {"publish_time": "2024-10-14", "title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free", "paper_summary": "While large language models (LLMs) excel on generation tasks, their\ndecoder-only architecture often limits their potential as embedding models if\nno further representation finetuning is applied. Does this contradict their\nclaim of generalists? To answer the question, we take a closer look at\nMixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE\nLLMs can serve as an off-the-shelf embedding model with promising performance\non a diverse class of embedding-focused tasks, without requiring any\nfinetuning. Moreover, our extensive analysis shows that the MoE routing weights\n(RW) is complementary to the hidden state (HS) of LLMs, a widely-used\nembedding. Compared to HS, we find that RW is more robust to the choice of\nprompts and focuses on high-level semantics. Motivated by the analysis, we\npropose MoEE combining RW and HS, which achieves better performance than using\neither separately. Our exploration of their combination and prompting strategy\nshed several novel insights, e.g., a weighted sum of RW and HS similarities\noutperforms the similarity on their concatenation. Our experiments are\nconducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding\nBenchmark (MTEB). The results demonstrate the significant improvement brought\nby MoEE to LLM-based embedding without further finetuning.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u751f\u6210\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u5176\u50c5\u89e3\u78bc\u5668\u7684\u67b6\u69cb\u901a\u5e38\u6703\u9650\u5236\u5176\u4f5c\u70ba\u5d4c\u5165\u6a21\u578b\u7684\u6f5b\u529b\uff0c\u9664\u975e\u61c9\u7528\u9032\u4e00\u6b65\u7684\u8868\u793a\u5fae\u8abf\u3002\u9019\u662f\u5426\u8207\u5176\u901a\u624d\u7684\u8aaa\u6cd5\u76f8\u77db\u76fe\uff1f\u70ba\u4e86\u56de\u7b54\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u4ed4\u7d30\u7814\u7a76\u4e86\u5c08\u5bb6\u6df7\u5408 (MoE) LLM\u3002\u6211\u5011\u7684\u7814\u7a76\u8868\u660e\uff0cMoE LLM \u4e2d\u7684\u5c08\u5bb6\u8def\u7531\u5668\u53ef\u4ee5\u7528\u4f5c\u73fe\u6210\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u5728\u5404\u7a2e\u4ee5\u5d4c\u5165\u70ba\u4e2d\u5fc3\u7684\u4efb\u52d9\u4e0a\u5177\u6709\u4ee4\u4eba\u6eff\u610f\u7684\u6548\u80fd\uff0c\u800c\u7121\u9700\u4efb\u4f55\u5fae\u8abf\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u5ee3\u6cdb\u5206\u6790\u8868\u660e\uff0cMoE \u8def\u7531\u6b0a\u91cd (RW) \u8207 LLM \u7684\u96b1\u85cf\u72c0\u614b (HS)\uff08\u4e00\u7a2e\u5ee3\u6cdb\u4f7f\u7528\u7684\u5d4c\u5165\uff09\u662f\u4e92\u88dc\u7684\u3002\u8207 HS \u76f8\u6bd4\uff0c\u6211\u5011\u767c\u73fe RW \u5c0d\u63d0\u793a\u7684\u9078\u64c7\u66f4\u5177\u9b6f\u68d2\u6027\uff0c\u4e26\u4e14\u5c08\u6ce8\u65bc\u9ad8\u5c64\u6b21\u8a9e\u7fa9\u3002\u53d7\u5206\u6790\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7d50\u5408 RW \u548c HS \u7684 MoEE\uff0c\u5176\u6548\u80fd\u512a\u65bc\u55ae\u7368\u4f7f\u7528\u4efb\u4e00\u65b9\u6cd5\u3002\u6211\u5011\u5c0d\u5176\u7d44\u5408\u548c\u63d0\u793a\u7b56\u7565\u7684\u63a2\u7d22\u63ed\u793a\u4e86\u5e7e\u500b\u65b0\u7a4e\u7684\u898b\u89e3\uff0c\u4f8b\u5982\uff0cRW \u548c HS \u76f8\u4f3c\u6027\u7684\u52a0\u6b0a\u548c\u512a\u65bc\u5b83\u5011\u4e32\u806f\u7684\u76f8\u4f3c\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u662f\u5728\u5927\u898f\u6a21\u6587\u5b57\u5d4c\u5165\u57fa\u6e96 (MTEB) \u4e2d\u7684 20 \u500b\u8cc7\u6599\u96c6\u4e0a\u7684 6 \u500b\u5d4c\u5165\u4efb\u52d9\u4e2d\u9032\u884c\u7684\u3002\u7d50\u679c\u8b49\u660e\u4e86 MoEE \u5728\u6c92\u6709\u9032\u4e00\u6b65\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\u70ba\u57fa\u65bc LLM \u7684\u5d4c\u5165\u5e36\u4f86\u7684\u986f\u8457\u6539\u9032\u3002", "author": "Ziyue Li et.al.", "authors": "Ziyue Li, Tianyi Zhou", "id": "2410.10814v1", "paper_url": "http://arxiv.org/abs/2410.10814v1", "repo": "null"}}