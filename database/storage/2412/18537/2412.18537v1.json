{"2412.18537": {"publish_time": "2024-12-24", "title": "Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation", "paper_summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet\nstruggle with hallucination and outdated knowledge when tasked with complex\nknowledge reasoning, resulting in factually incorrect outputs. Previous studies\nhave attempted to mitigate it by retrieving factual knowledge from large-scale\nknowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of\nanswers. However, this kind of approach often introduces noise and irrelevant\ndata, especially in situations with extensive context from multiple knowledge\naspects. In this way, LLM attention can be potentially mislead from question\nand relevant information. In our study, we introduce an Adaptive Multi-Aspect\nRetrieval-augmented over KGs (Amar) framework. This method retrieves knowledge\nincluding entities, relations, and subgraphs, and converts each piece of\nretrieved text into prompt embeddings. The Amar framework comprises two key\nsub-components: 1) a self-alignment module that aligns commonalities among\nentities, relations, and subgraphs to enhance retrieved text, thereby reducing\nnoise interference; 2) a relevance gating module that employs a soft gate to\nlearn the relevance score between question and multi-aspect retrieved data, to\ndetermine which information should be used to enhance LLMs' output, or even\nfiltered altogether. Our method has achieved state-of-the-art performance on\ntwo common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy\nover its best competitor and a 6.6\\% improvement in logical form generation\nover a method that directly uses retrieved text as context prompts. These\nresults demonstrate the effectiveness of Amar in improving the reasoning of\nLLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u793a\u4e86\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u7576\u5b83\u5011\u88ab\u8ce6\u4e88\u8907\u96dc\u7684\u77e5\u8b58\u63a8\u7406\u4efb\u52d9\u6642\uff0c\u537b\u6703\u9677\u5165\u5e7b\u89ba\u548c\u904e\u6642\u77e5\u8b58\u7684\u56f0\u5883\uff0c\u5c0e\u81f4\u4e8b\u5be6\u4e0a\u4e0d\u6b63\u78ba\u7684\u8f38\u51fa\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u5617\u8a66\u901a\u904e\u5f9e\u5927\u898f\u6a21\u77e5\u8b58\u5716\u8b5c (KG) \u4e2d\u64f7\u53d6\u4e8b\u5be6\u77e5\u8b58\u4f86\u6e1b\u8f15\u9019\u7a2e\u60c5\u6cc1\uff0c\u4ee5\u5354\u52a9 LLM \u9032\u884c\u908f\u8f2f\u63a8\u7406\u548c\u7b54\u6848\u9810\u6e2c\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u901a\u5e38\u6703\u5f15\u5165\u96dc\u8a0a\u548c\u7121\u95dc\u8cc7\u6599\uff0c\u7279\u5225\u662f\u5728\u5177\u6709\u4f86\u81ea\u591a\u500b\u77e5\u8b58\u9762\u5411\u7684\u5ee3\u6cdb\u80cc\u666f\u7684\u60c5\u6cc1\u4e0b\u3002\u901a\u904e\u9019\u7a2e\u65b9\u5f0f\uff0cLLM \u6ce8\u610f\u529b\u53ef\u80fd\u6703\u88ab\u554f\u984c\u548c\u76f8\u95dc\u8cc7\u8a0a\u8aa4\u5c0e\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u81ea\u9069\u61c9\u591a\u9762\u5411\u6aa2\u7d22\u589e\u5f37\u7684\u77e5\u8b58\u5716\u8b5c (Amar) \u67b6\u69cb\u3002\u6b64\u65b9\u6cd5\u64f7\u53d6\u5305\u62ec\u5be6\u9ad4\u3001\u95dc\u4fc2\u548c\u5b50\u5716\u7684\u77e5\u8b58\uff0c\u4e26\u5c07\u6bcf\u500b\u64f7\u53d6\u7684\u6587\u5b57\u8f49\u63db\u70ba\u63d0\u793a\u5d4c\u5165\u3002Amar \u67b6\u69cb\u5305\u542b\u5169\u500b\u95dc\u9375\u5b50\u5143\u4ef6\uff1a1) \u4e00\u500b\u81ea\u6211\u5c0d\u9f4a\u6a21\u7d44\uff0c\u5b83\u5c0d\u9f4a\u5be6\u9ad4\u3001\u95dc\u4fc2\u548c\u5b50\u5716\u4e4b\u9593\u7684\u5171\u6027\u4ee5\u589e\u5f37\u64f7\u53d6\u7684\u6587\u5b57\uff0c\u5f9e\u800c\u6e1b\u5c11\u96dc\u8a0a\u5e72\u64fe\uff1b2) \u4e00\u500b\u76f8\u95dc\u6027\u9598\u9580\u6a21\u7d44\uff0c\u5b83\u63a1\u7528\u8edf\u9598\u9580\u4f86\u5b78\u7fd2\u554f\u984c\u8207\u591a\u9762\u5411\u64f7\u53d6\u8cc7\u6599\u4e4b\u9593\u7684\u76f8\u5173\u6027\u5206\u6578\uff0c\u4ee5\u78ba\u5b9a\u54ea\u4e9b\u8cc7\u8a0a\u61c9\u88ab\u7528\u4f86\u589e\u5f37 LLM \u7684\u8f38\u51fa\uff0c\u751a\u81f3\u5b8c\u5168\u904e\u6ffe\u6389\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u5169\u500b\u5e38\u898b\u7684\u8cc7\u6599\u96c6 WebQSP \u548c CWQ \u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u8207\u6700\u4f73\u7af6\u722d\u8005\u76f8\u6bd4\uff0c\u6e96\u78ba\u5ea6\u63d0\u5347\u4e86 1.9%\uff0c\u8207\u76f4\u63a5\u4f7f\u7528\u64f7\u53d6\u6587\u5b57\u4f5c\u70ba\u80cc\u666f\u63d0\u793a\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u908f\u8f2f\u5f62\u5f0f\u7522\u751f\u7684\u6539\u9032\u70ba 6.6%\u3002\u9019\u4e9b\u7d50\u679c\u8b49\u660e\u4e86 Amar \u5728\u6539\u5584 LLM \u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "author": "Derong Xu Xinhang Li et.al.", "authors": "Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, Zhihong Zhu, Zhi Zheng, Xian Wu, Xiangyu Zhao, Tong Xu, Enhong Chen", "id": "2412.18537v1", "paper_url": "http://arxiv.org/abs/2412.18537v1", "repo": "null"}}