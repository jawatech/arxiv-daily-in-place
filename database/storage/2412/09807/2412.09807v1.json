{"2412.09807": {"publish_time": "2024-12-13", "title": "LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering", "paper_summary": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA.", "paper_summary_zh": "\u591a\u9078\u984c\u554f\u7b54 (MCQA) \u662f\u4e00\u500b\u91cd\u8981\u7684\u554f\u984c\uff0c\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u6709\u8a31\u591a\u61c9\u7528\uff0c\u4f8b\u5982\u91ab\u5b78\u3001\u6cd5\u5f8b\u548c\u6559\u80b2\u3002\u5efa\u7acb MCQA \u8cc7\u6599\u96c6\u7684\u6210\u672c\u5f88\u9ad8\uff0c\u9019\u4f7f\u5f97\u5c0f\u6a23\u672c\u5b78\u7fd2\u5728\u9019\u500b\u9818\u57df\u81f3\u95dc\u91cd\u8981\u3002\u5118\u7ba1\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u652f\u63f4\u5c0f\u6a23\u672c\u5b78\u7fd2\uff0c\u4f46\u5b83\u5011\u5728\u5be6\u969b\u5834\u666f\u4e2d\u7684\u76f4\u63a5\u61c9\u7528\u5e38\u5e38\u53d7\u5230\u5176\u9ad8\u6602\u7684\u904b\u7b97\u6210\u672c\u6240\u963b\u7919\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u4f7f\u7528 LLM \u4f86\u9032\u884c\u8cc7\u6599\u7522\u751f\u548c\u8a55\u5206\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528 LLM \u4f86\u5efa\u7acb\u5305\u542b\u554f\u984c\u548c\u9078\u9805\u7684 MCQA \u8cc7\u6599\uff0c\u4e26\u70ba\u7522\u751f\u7684\u9078\u9805\u5206\u914d\u6a5f\u7387\u5206\u6578\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u7522\u751f\u7684\u8cc7\u6599\u548c LLM \u5206\u914d\u7684\u5206\u6578\uff0c\u85c9\u7531\u5229\u7528\u84b8\u993e\u640d\u5931\u4f86\u5fae\u8abf\u4e00\u500b\u66f4\u5c0f\u4e14\u66f4\u6709\u6548\u7387\u7684\u7de8\u78bc\u5668\u5c08\u7528\u6a21\u578b DeBERTa-v3-base\u3002\u5728 Massive Multitask Language Understanding (MMLU) \u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5c07\u6e96\u78ba\u7387\u5f9e 28.9% \u63d0\u5347\u81f3 39.3%\uff0c\u8207\u76f4\u63a5\u5fae\u8abf\u5728 5 \u6b21\u5617\u8a66\u7684\u7bc4\u4f8b\u4e0a\u76f8\u6bd4\uff0c\u589e\u52a0\u4e86\u8d85\u904e 10%\u3002\u9019\u986f\u793a\u4e86 LLM \u9a45\u52d5\u7684\u8cc7\u6599\u7522\u751f\u548c\u77e5\u8b58\u84b8\u993e\u5728\u5c0f\u6a23\u672c MCQA \u4e2d\u7684\u6709\u6548\u6027\u3002", "author": "Patrick Sutanto et.al.", "authors": "Patrick Sutanto, Joan Santoso", "id": "2412.09807v1", "paper_url": "http://arxiv.org/abs/2412.09807v1", "repo": "null"}}