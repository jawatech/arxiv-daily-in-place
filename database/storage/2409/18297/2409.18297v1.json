{"2409.18297": {"publish_time": "2024-09-26", "title": "Flat'n'Fold: A Diverse Multi-Modal Dataset for Garment Perception and Manipulation", "paper_summary": "We present Flat'n'Fold, a novel large-scale dataset for garment manipulation\nthat addresses critical gaps in existing datasets. Comprising 1,212 human and\n887 robot demonstrations of flattening and folding 44 unique garments across 8\ncategories, Flat'n'Fold surpasses prior datasets in size, scope, and diversity.\nOur dataset uniquely captures the entire manipulation process from crumpled to\nfolded states, providing synchronized multi-view RGB-D images, point clouds,\nand action data, including hand or gripper positions and rotations. We quantify\nthe dataset's diversity and complexity compared to existing benchmarks and show\nthat our dataset features natural and diverse manipulations of real-world\ndemonstrations of human and robot demonstrations in terms of visual and action\ninformation. To showcase Flat'n'Fold's utility, we establish new benchmarks for\ngrasping point prediction and subtask decomposition. Our evaluation of\nstate-of-the-art models on these tasks reveals significant room for\nimprovement. This underscores Flat'n'Fold's potential to drive advances in\nrobotic perception and manipulation of deformable objects. Our dataset can be\ndownloaded at https://cvas-ug.github.io/flat-n-fold", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa Flat'n'Fold\uff0c\u9019\u662f\u4e00\u500b\u7528\u65bc\u670d\u88dd\u64cd\u4f5c\u7684\u65b0\u578b\u5927\u578b\u6578\u64da\u96c6\uff0c\u5b83\u89e3\u6c7a\u4e86\u73fe\u6709\u6578\u64da\u96c6\u4e2d\u7684\u95dc\u9375\u5dee\u8ddd\u3002Flat'n'Fold \u5305\u542b 1,212 \u500b\u771f\u4eba\u793a\u7bc4\u548c 887 \u500b\u6a5f\u5668\u4eba\u793a\u7bc4\uff0c\u5c55\u793a\u4e86 8 \u500b\u985e\u5225\u4e2d 44 \u4ef6\u7368\u7279\u670d\u88dd\u7684\u6524\u5e73\u548c\u647a\u758a\u904e\u7a0b\uff0c\u5728\u898f\u6a21\u3001\u7bc4\u570d\u548c\u591a\u6a23\u6027\u65b9\u9762\u90fd\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6578\u64da\u96c6\u3002\u6211\u5011\u7684\u6578\u64da\u96c6\u7368\u7279\u5730\u6355\u6349\u4e86\u5f9e\u76ba\u5df4\u5df4\u5230\u647a\u758a\u72c0\u614b\u7684\u6574\u500b\u64cd\u4f5c\u904e\u7a0b\uff0c\u63d0\u4f9b\u4e86\u540c\u6b65\u7684\u591a\u8996\u5716 RGB-D \u5f71\u50cf\u3001\u9ede\u96f2\u548c\u52d5\u4f5c\u6578\u64da\uff0c\u5305\u62ec\u624b\u90e8\u6216\u593e\u5177\u7684\u4f4d\u7f6e\u548c\u65cb\u8f49\u3002\u6211\u5011\u91cf\u5316\u4e86\u6578\u64da\u96c6\u7684\u591a\u6a23\u6027\u548c\u8907\u96dc\u6027\uff0c\u4e26\u8207\u73fe\u6709\u7684\u57fa\u6e96\u9032\u884c\u6bd4\u8f03\uff0c\u7d50\u679c\u986f\u793a\u6211\u5011\u7684\u6578\u64da\u96c6\u5728\u8996\u89ba\u548c\u52d5\u4f5c\u8cc7\u8a0a\u65b9\u9762\u5177\u6709\u81ea\u7136\u4e14\u591a\u6a23\u5316\u7684\u771f\u4eba\u548c\u6a5f\u5668\u4eba\u793a\u7bc4\u64cd\u4f5c\u3002\u70ba\u4e86\u5c55\u793a Flat'n'Fold \u7684\u6548\u7528\uff0c\u6211\u5011\u70ba\u6293\u53d6\u9ede\u9810\u6e2c\u548c\u5b50\u4efb\u52d9\u5206\u89e3\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u6e96\u3002\u6211\u5011\u5c0d\u9019\u4e9b\u4efb\u52d9\u4e2d\u6700\u5148\u9032\u6a21\u578b\u7684\u8a55\u4f30\u986f\u793a\u51fa\u6709\u986f\u8457\u7684\u6539\u9032\u7a7a\u9593\u3002\u9019\u7a81\u986f\u4e86 Flat'n'Fold \u5728\u63a8\u52d5\u53ef\u8b8a\u5f62\u7269\u9ad4\u7684\u6a5f\u5668\u4eba\u611f\u77e5\u548c\u64cd\u4f5c\u65b9\u9762\u53d6\u5f97\u9032\u5c55\u7684\u6f5b\u529b\u3002\u6211\u5011\u7684\u6578\u64da\u96c6\u53ef\u4ee5\u5728 https://cvas-ug.github.io/flat-n-fold \u4e0b\u8f09</paragraph>", "author": "Lipeng Zhuang et.al.", "authors": "Lipeng Zhuang, Shiyu Fan, Yingdong Ru, Florent Audonnet, Paul Henderson, Gerardo Aragon-Camarasa", "id": "2409.18297v1", "paper_url": "http://arxiv.org/abs/2409.18297v1", "repo": "null"}}