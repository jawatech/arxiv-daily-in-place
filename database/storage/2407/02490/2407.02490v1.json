{"2407.02490": {"publish_time": "2024-07-02", "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention", "paper_summary": "The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u63a8\u8ad6\u7684\u904b\u7b97\u6311\u6230\u4ecd\u7136\u662f\u5176\u5ee3\u6cdb\u90e8\u7f72\u7684\u91cd\u5927\u969c\u7919\uff0c\u7279\u5225\u662f\u56e0\u70ba\u63d0\u793a\u9577\u5ea6\u6301\u7e8c\u589e\u52a0\u3002\u7531\u65bc\u6ce8\u610f\u529b\u904b\u7b97\u7684\u4e8c\u6b21\u8907\u96dc\u5ea6\uff0c8B LLM \u8655\u7406 1M \u500b token \u7684\u63d0\u793a\uff08\u5373\u9810\u586b\u5165\u968e\u6bb5\uff09\u9700\u8981\u5728\u55ae\u500b A100 GPU \u4e0a\u82b1\u8cbb 30 \u5206\u9418\u3002\u73fe\u6709\u7684\u52a0\u901f\u9810\u586b\u5165\u65b9\u6cd5\u5728\u61c9\u7528\u65bc\u9577\u8a9e\u5883 LLM \u6642\uff0c\u901a\u5e38\u7121\u6cd5\u7dad\u6301\u53ef\u63a5\u53d7\u7684\u6e96\u78ba\u5ea6\u6216\u6548\u7387\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 MInference\uff08\u767e\u842c token \u63a8\u8ad6\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u7a00\u758f\u8a08\u7b97\u65b9\u6cd5\uff0c\u65e8\u5728\u52a0\u901f\u9577\u5e8f\u5217\u8655\u7406\u7684\u9810\u586b\u5165\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728\u9577\u8a9e\u5883\u6ce8\u610f\u529b\u77e9\u9663\u4e2d\u8b58\u5225\u51fa\u4e09\u7a2e\u7368\u7279\u7684\u6a21\u5f0f\u2014\u2014A \u5f62\u3001\u5782\u76f4\u659c\u7dda\u548c\u5340\u584a\u7a00\u758f\uff0c\u9019\u4e9b\u6a21\u5f0f\u53ef\u4ee5\u5229\u7528 GPU \u4e0a\u7684\u6709\u6548\u7a00\u758f\u8a08\u7b97\u3002\u6211\u5011\u96e2\u7dda\u78ba\u5b9a\u6bcf\u500b\u6ce8\u610f\u529b\u982d\u7684\u6700\u4f73\u6a21\u5f0f\uff0c\u4e26\u5728\u63a8\u8ad6\u671f\u9593\u6839\u64da\u6307\u5b9a\u7684\u6a21\u5f0f\u52d5\u614b\u69cb\u5efa\u7a00\u758f\u7d22\u5f15\u3002\u6709\u4e86\u6a21\u5f0f\u548c\u7a00\u758f\u7d22\u5f15\uff0c\u6211\u5011\u901a\u904e\u512a\u5316\u7684 GPU \u6838\u5fc3\u57f7\u884c\u6709\u6548\u7684\u7a00\u758f\u6ce8\u610f\u529b\u8a08\u7b97\uff0c\u4ee5\u986f\u8457\u6e1b\u5c11\u9577\u8a9e\u5883 LLM \u9810\u586b\u5165\u968e\u6bb5\u7684\u5ef6\u9072\u3002\u6211\u5011\u63d0\u51fa\u7684\u6280\u8853\u53ef\u4ee5\u76f4\u63a5\u61c9\u7528\u65bc\u73fe\u6709\u7684 LLM\uff0c\u800c\u7121\u9700\u5c0d\u9810\u8a13\u7df4\u8a2d\u5b9a\u6216\u984d\u5916\u5fae\u8abf\u9032\u884c\u4efb\u4f55\u4fee\u6539\u3002\u901a\u904e\u5728\u5ee3\u6cdb\u7684\u4e0b\u6e38\u4efb\u52d9\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u5305\u62ec InfiniteBench\u3001RULER\u3001PG-19 \u548c Needle In A Haystack\uff0c\u4ee5\u53ca LLaMA-3-1M\u3001GLM4-1M\u3001Yi-200K\u3001Phi-3-128K \u548c Qwen2-128K \u7b49\u6a21\u578b\uff0c\u6211\u5011\u8b49\u660e\u4e86 MInference \u6709\u6548\u5730\u5c07\u9810\u586b\u5165\u7684\u63a8\u8ad6\u5ef6\u9072\u964d\u4f4e\u4e86 10 \u500d\uff0c\u540c\u6642\u4fdd\u6301\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://aka.ms/MInference \u53d6\u5f97\u3002", "author": "Huiqiang Jiang et.al.", "authors": "Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu", "id": "2407.02490v1", "paper_url": "http://arxiv.org/abs/2407.02490v1", "repo": "https://github.com/microsoft/MInference"}}