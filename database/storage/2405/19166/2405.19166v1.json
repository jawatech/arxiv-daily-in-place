{"2405.19166": {"publish_time": "2024-05-29", "title": "Transformers as Neural Operators for Solutions of Differential Equations with Finite Regularity", "paper_summary": "Neural operator learning models have emerged as very effective surrogates in\ndata-driven methods for partial differential equations (PDEs) across different\napplications from computational science and engineering. Such operator learning\nmodels not only predict particular instances of a physical or biological system\nin real-time but also forecast classes of solutions corresponding to a\ndistribution of initial and boundary conditions or forcing terms. % DeepONet is\nthe first neural operator model and has been tested extensively for a broad\nclass of solutions, including Riemann problems. Transformers have not been used\nin that capacity, and specifically, they have not been tested for solutions of\nPDEs with low regularity. %\n  In this work, we first establish the theoretical groundwork that transformers\npossess the universal approximation property as operator learning models.\n  We then apply transformers to forecast solutions of diverse dynamical systems\nwith solutions of finite regularity for a plurality of initial conditions and\nforcing terms. In particular, we consider three examples: the Izhikevich neuron\nmodel, the tempered fractional-order Leaky Integrate-and-Fire (LIF) model, and\nthe one-dimensional Euler equation Riemann problem. For the latter problem, we\nalso compare with variants of DeepONet, and we find that transformers\noutperform DeepONet in accuracy but they are computationally more expensive.", "paper_summary_zh": "\u795e\u7d93\u7b97\u5b50\u5b78\u7fd2\u6a21\u578b\u5df2\u6210\u70ba\u8de8\u8d8a\u8a08\u7b97\u79d1\u5b78\u548c\u5de5\u7a0b\u4e0d\u540c\u61c9\u7528\u4e2d\u504f\u5fae\u5206\u65b9\u7a0b\u5f0f (PDE) \u7684\u6578\u64da\u9a45\u52d5\u65b9\u6cd5\u4e2d\u975e\u5e38\u6709\u6548\u7684\u66ff\u4ee3\u54c1\u3002\u6b64\u985e\u7b97\u5b50\u5b78\u7fd2\u6a21\u578b\u4e0d\u50c5\u53ef\u4ee5\u5be6\u6642\u9810\u6e2c\u7269\u7406\u6216\u751f\u7269\u7cfb\u7d71\u7684\u7279\u5b9a\u5be6\u4f8b\uff0c\u9084\u80fd\u9810\u6e2c\u5c0d\u61c9\u65bc\u521d\u59cb\u548c\u908a\u754c\u689d\u4ef6\u6216\u5f37\u8feb\u9805\u5206\u4f48\u7684\u89e3\u985e\u5225\u3002DeepONet \u662f\u7b2c\u4e00\u500b\u795e\u7d93\u7b97\u5b50\u6a21\u578b\uff0c\u4e26\u4e14\u5df2\u91dd\u5c0d\u5ee3\u6cdb\u7684\u89e3\u985e\u5225\uff08\u5305\u62ec\u9ece\u66fc\u554f\u984c\uff09\u9032\u884c\u4e86\u5ee3\u6cdb\u6e2c\u8a66\u3002Transformer\u5c1a\u672a\u7528\u65bc\u8a72\u5bb9\u91cf\uff0c\u7279\u5225\u662f\u5b83\u5011\u5c1a\u672a\u91dd\u5c0d\u4f4e\u6b63\u5247\u6027\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u5f0f\u89e3\u9032\u884c\u6e2c\u8a66\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u5efa\u7acb\u4e86Transformer\u4f5c\u70ba\u7b97\u5b50\u5b78\u7fd2\u6a21\u578b\u5177\u5099\u901a\u7528\u903c\u8fd1\u7279\u6027\u7684\u7406\u8ad6\u57fa\u790e\u3002\u7136\u5f8c\uff0c\u6211\u5011\u61c9\u7528Transformer\u4f86\u9810\u6e2c\u5177\u6709\u6709\u9650\u6b63\u5247\u6027\u89e3\u7684\u591a\u500b\u521d\u59cb\u689d\u4ef6\u548c\u5f37\u8feb\u9805\u7684\u5404\u7a2e\u52d5\u614b\u7cfb\u7d71\u7684\u89e3\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u8003\u616e\u4e09\u500b\u7bc4\u4f8b\uff1aIzhikevich \u795e\u7d93\u5143\u6a21\u578b\u3001\u7de9\u548c\u5206\u6578\u968e Leaky Integrate-and-Fire (LIF) \u6a21\u578b\u548c\u4e00\u7dad\u6b50\u62c9\u65b9\u7a0b\u9ece\u66fc\u554f\u984c\u3002\u5c0d\u65bc\u5f8c\u4e00\u500b\u554f\u984c\uff0c\u6211\u5011\u9084\u8207 DeepONet \u7684\u8b8a\u9ad4\u9032\u884c\u4e86\u6bd4\u8f03\uff0c\u6211\u5011\u767c\u73feTransformer\u5728\u6e96\u78ba\u6027\u65b9\u9762\u512a\u65bc DeepONet\uff0c\u4f46\u5b83\u5011\u5728\u8a08\u7b97\u4e0a\u66f4\u6602\u8cb4\u3002", "author": "Benjamin Shih et.al.", "authors": "Benjamin Shih, Ahmad Peyvan, Zhongqiang Zhang, George Em Karniadakis", "id": "2405.19166v1", "paper_url": "http://arxiv.org/abs/2405.19166v1", "repo": "null"}}