{"2501.03832": {"publish_time": "2025-01-07", "title": "Three-dimensional attention Transformer for state evaluation in real-time strategy games", "paper_summary": "Situation assessment in Real-Time Strategy (RTS) games is crucial for\nunderstanding decision-making in complex adversarial environments. However,\nexisting methods remain limited in processing multi-dimensional feature\ninformation and temporal dependencies. Here we propose a tri-dimensional\nSpace-Time-Feature Transformer (TSTF Transformer) architecture, which\nefficiently models battlefield situations through three independent but\ncascaded modules: spatial attention, temporal attention, and feature attention.\nOn a dataset comprising 3,150 adversarial experiments, the 8-layer TSTF\nTransformer demonstrates superior performance: achieving 58.7% accuracy in the\nearly game (~4% progress), significantly outperforming the conventional\nTimesformer's 41.8%; reaching 97.6% accuracy in the mid-game (~40% progress)\nwhile maintaining low performance variation (standard deviation 0.114).\nMeanwhile, this architecture requires fewer parameters (4.75M) compared to the\nbaseline model (5.54M). Our study not only provides new insights into situation\nassessment in RTS games but also presents an innovative paradigm for\nTransformer-based multi-dimensional temporal modeling.", "paper_summary_zh": "\u5728\u5373\u6642\u6230\u7565 (RTS) \u904a\u6232\u4e2d\uff0c\u60c5\u5883\u8a55\u4f30\u5c0d\u65bc\u7406\u89e3\u5728\u8907\u96dc\u5c0d\u6297\u74b0\u5883\u4e2d\u7684\u6c7a\u7b56\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u5728\u8655\u7406\u591a\u7dad\u7279\u5fb5\u8cc7\u8a0a\u548c\u6642\u9593\u4f9d\u8cf4\u6027\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002\u5728\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u4e09\u7dad\u6642\u7a7a\u7279\u5fb5\u8f49\u63db\u5668 (TSTF Transformer) \u67b6\u69cb\uff0c\u5b83\u900f\u904e\u4e09\u500b\u7368\u7acb\u4f46\u4e32\u806f\u7684\u6a21\u7d44\u6709\u6548\u5730\u6a21\u64ec\u6230\u5834\u60c5\u5883\uff1a\u7a7a\u9593\u6ce8\u610f\u529b\u3001\u6642\u9593\u6ce8\u610f\u529b\u548c\u7279\u5fb5\u6ce8\u610f\u529b\u3002\u5728\u5305\u542b 3,150 \u500b\u5c0d\u6297\u5be6\u9a57\u7684\u8cc7\u6599\u96c6\u4e0a\uff0c8 \u5c64 TSTF Transformer \u5c55\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\uff1a\u5728\u904a\u6232\u521d\u671f\u7372\u5f97 58.7% \u7684\u6e96\u78ba\u5ea6\uff08\u9032\u6b65\u7d04 4%\uff09\uff0c\u660e\u986f\u512a\u65bc\u50b3\u7d71 Timesformer \u7684 41.8%\uff1b\u5728\u904a\u6232\u4e2d\u671f\u9054\u5230 97.6% \u7684\u6e96\u78ba\u5ea6\uff08\u9032\u6b65\u7d04 40%\uff09\uff0c\u540c\u6642\u7dad\u6301\u4f4e\u6548\u80fd\u8b8a\u7570\uff08\u6a19\u6e96\u5dee 0.114\uff09\u3002\u540c\u6642\uff0c\u8207\u57fa\u7dda\u6a21\u578b\uff085.54M\uff09\u76f8\u6bd4\uff0c\u6b64\u67b6\u69cb\u9700\u8981\u7684\u53c3\u6578\u66f4\u5c11\uff084.75M\uff09\u3002\u6211\u5011\u7684\u7814\u7a76\u4e0d\u50c5\u70ba RTS \u904a\u6232\u4e2d\u7684\u60c5\u5883\u8a55\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\uff0c\u4e5f\u70ba\u57fa\u65bc Transformer \u7684\u591a\u7dad\u6642\u9593\u5efa\u6a21\u63d0\u51fa\u4e86\u5275\u65b0\u7684\u5178\u7bc4\u3002", "author": "Yanqing Ye et.al.", "authors": "Yanqing Ye, Weilong Yang, Kai Qiu, Jie Zhang", "id": "2501.03832v1", "paper_url": "http://arxiv.org/abs/2501.03832v1", "repo": "null"}}