{"2411.18203": {"publish_time": "2024-11-27", "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning", "paper_summary": "Vision-language models~(VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.", "paper_summary_zh": "<paragraph>\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5728\u591a\u6a21\u614b\u63a8\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u986f\u8457\u7684\u9032\u6b65\u3002\u7136\u800c\uff0c\u7531\u65bc\u5e7b\u89ba\u5f71\u50cf\u7406\u89e3\u6216\u672a\u7d93\u63d0\u7149\u7684\u63a8\u7406\u8def\u5f91\u7b49\u554f\u984c\uff0c\u5b83\u5011\u4ecd\u7136\u7d93\u5e38\u7522\u751f\u4e0d\u6e96\u78ba\u6216\u4e0d\u76f8\u95dc\u7684\u56de\u61c9\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 Critic-V\uff0c\u9019\u662f\u4e00\u500b\u53d7 Actor-Critic \u5178\u7bc4\u555f\u767c\u7684\u65b0\u7a4e\u67b6\u69cb\uff0c\u7528\u65bc\u63d0\u5347 VLM \u7684\u63a8\u7406\u80fd\u529b\u3002\u6b64\u67b6\u69cb\u900f\u904e\u6574\u5408\u5169\u500b\u7368\u7acb\u7684\u7d44\u4ef6\u4f86\u89e3\u8026\u63a8\u7406\u7a0b\u5e8f\u548c\u6279\u8a55\u7a0b\u5e8f\uff1a\u63a8\u7406\u5668\uff0c\u5b83\u6839\u64da\u8996\u89ba\u548c\u6587\u5b57\u8f38\u5165\u7522\u751f\u63a8\u7406\u8def\u5f91\uff0c\u4ee5\u53ca\u6279\u8a55\u8005\uff0c\u5b83\u63d0\u4f9b\u5efa\u8a2d\u6027\u7684\u6279\u8a55\u4f86\u512a\u5316\u9019\u4e9b\u8def\u5f91\u3002\u5728\u6b64\u65b9\u6cd5\u4e2d\uff0c\u63a8\u7406\u5668\u6839\u64da\u6587\u5b57\u63d0\u793a\u7522\u751f\u63a8\u7406\u56de\u61c9\uff0c\u9019\u4e9b\u56de\u61c9\u53ef\u4ee5\u6839\u64da\u6279\u8a55\u8005\u7684\u56de\u994b\u4f5c\u70ba\u653f\u7b56\u53cd\u8986\u6f14\u9032\u3002\u6b64\u4e92\u52d5\u7a0b\u5e8f\u5728\u7406\u8ad6\u4e0a\u662f\u7531\u5f37\u5316\u5b78\u7fd2\u67b6\u69cb\u63a8\u52d5\u7684\uff0c\u5176\u4e2d\u6279\u8a55\u8005\u63d0\u4f9b\u81ea\u7136\u8a9e\u8a00\u6279\u8a55\uff0c\u800c\u4e0d\u662f\u6a19\u91cf\u734e\u52f5\uff0c\u5f9e\u800c\u80fd\u5920\u63d0\u4f9b\u66f4\u591a\u7d30\u5fae\u7684\u56de\u994b\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u5668\u5728\u8907\u96dc\u63a8\u7406\u4efb\u52d9\u4e2d\u7684\u80fd\u529b\u3002\u6279\u8a55\u8005\u6a21\u578b\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u9032\u884c\u8a13\u7df4\uff0c\u5229\u7528\u57fa\u65bc\u898f\u5247\u7684\u734e\u52f5 (RBR) \u6392\u540d\u7684\u6279\u8a55\u504f\u597d\u8cc7\u6599\u96c6\u4f86\u589e\u5f37\u5176\u6279\u8a55\u80fd\u529b\u3002\u8a55\u4f30\u7d50\u679c\u986f\u793a\uff0cCritic-V \u67b6\u69cb\u5728 8 \u500b\u57fa\u6e96\u4e2d\u7684 5 \u500b\u57fa\u6e96\u4e0a\u986f\u8457\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u5305\u62ec GPT-4V\uff0c\u7279\u5225\u662f\u5728\u63a8\u7406\u6e96\u78ba\u6027\u548c\u6548\u7387\u65b9\u9762\u3002\u7d50\u5408\u63a8\u7406\u5668\u7684\u52d5\u614b\u57fa\u65bc\u6587\u5b57\u7684\u653f\u7b56\u548c\u504f\u597d\u6700\u4f73\u5316\u6279\u8a55\u8005\u7684\u5efa\u8a2d\u6027\u56de\u994b\uff0c\u53ef\u4ee5\u5be6\u73fe\u66f4\u53ef\u9760\u4e14\u5c0d\u60c5\u5883\u654f\u611f\u7684\u591a\u6a21\u614b\u63a8\u7406\u7a0b\u5e8f\u3002\u6211\u5011\u7684\u505a\u6cd5\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u589e\u5f37 VLM \u7684\u53ef\u9760\u6027\uff0c\u6539\u5584\u5b83\u5011\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u4ee5\u63a8\u7406\u70ba\u4e3b\u7684\u591a\u6a21\u614b\u61c9\u7528\u7a0b\u5f0f\uff08\u4f8b\u5982\u81ea\u52d5\u99d5\u99db\u548c\u5177\u8eab\u667a\u80fd\uff09\u4e2d\u7684\u6027\u80fd\u3002</paragraph>", "author": "Di Zhang et.al.", "authors": "Di Zhang, Jingdi Lei, Junxian Li, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou", "id": "2411.18203v1", "paper_url": "http://arxiv.org/abs/2411.18203v1", "repo": "null"}}