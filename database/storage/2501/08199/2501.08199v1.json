{"2501.08199": {"publish_time": "2025-01-14", "title": "EmoNeXt: an Adapted ConvNeXt for Facial Emotion Recognition", "paper_summary": "Facial expressions play a crucial role in human communication serving as a\npowerful and impactful means to express a wide range of emotions. With\nadvancements in artificial intelligence and computer vision, deep neural\nnetworks have emerged as effective tools for facial emotion recognition. In\nthis paper, we propose EmoNeXt, a novel deep learning framework for facial\nexpression recognition based on an adapted ConvNeXt architecture network. We\nintegrate a Spatial Transformer Network (STN) to focus on feature-rich regions\nof the face and Squeeze-and-Excitation blocks to capture channel-wise\ndependencies. Moreover, we introduce a self-attention regularization term,\nencouraging the model to generate compact feature vectors. We demonstrate the\nsuperiority of our model over existing state-of-the-art deep learning models on\nthe FER2013 dataset regarding emotion classification accuracy.", "paper_summary_zh": "\u9762\u90e8\u8868\u60c5\u5728\u4eba\u7c7b\u6c9f\u901a\u4e2d\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\uff0c\u4f5c\u4e3a\u8868\u8fbe\u5e7f\u6cdb\u60c5\u7eea\u7684\u4e00\u79cd\u6709\u529b\u4e14\u6709\u5f71\u54cd\u529b\u7684\u65b9\u5f0f\u3002\u968f\u7740\u4eba\u5de5\u667a\u80fd\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u8fdb\u6b65\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5df2\u6210\u4e3a\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u6709\u6548\u7684\u5de5\u5177\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 EmoNeXt\uff0c\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94 ConvNeXt \u67b6\u6784\u7f51\u7edc\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002\u6211\u4eec\u96c6\u6210\u4e86\u7a7a\u95f4\u53d8\u6362\u7f51\u7edc (STN) \u6765\u5173\u6ce8\u9762\u90e8\u4e2d\u5bcc\u542b\u7279\u5f81\u7684\u533a\u57df\u548c Squeeze-and-Excitation \u5757\u6765\u6355\u83b7\u901a\u9053\u4f9d\u8d56\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u6ce8\u610f\u529b\u6b63\u5219\u5316\u9879\uff0c\u9f13\u52b1\u6a21\u578b\u751f\u6210\u7d27\u51d1\u7684\u7279\u5f81\u5411\u91cf\u3002\u6211\u4eec\u5728 FER2013 \u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u60c5\u7eea\u5206\u7c7b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "author": "Yassine El Boudouri et.al.", "authors": "Yassine El Boudouri, Amine Bohi", "id": "2501.08199v1", "paper_url": "http://arxiv.org/abs/2501.08199v1", "repo": "https://github.com/yelboudouri/EmoNeXt"}}