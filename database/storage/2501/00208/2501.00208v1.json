{"2501.00208": {"publish_time": "2024-12-31", "title": "An Empirical Evaluation of Large Language Models on Consumer Health Questions", "paper_summary": "This study evaluates the performance of several Large Language Models (LLMs)\non MedRedQA, a dataset of consumer-based medical questions and answers by\nverified experts extracted from the AskDocs subreddit. While LLMs have shown\nproficiency in clinical question answering (QA) benchmarks, their effectiveness\non real-world, consumer-based, medical questions remains less understood.\nMedRedQA presents unique challenges, such as informal language and the need for\nprecise responses suited to non-specialist queries. To assess model\nperformance, responses were generated using five LLMs: GPT-4o mini, Llama 3.1:\n70B, Mistral-123B, Mistral-7B, and Gemini-Flash. A cross-evaluation method was\nused, where each model evaluated its responses as well as those of others to\nminimize bias. The results indicated that GPT-4o mini achieved the highest\nalignment with expert responses according to four out of the five models'\njudges, while Mistral-7B scored lowest according to three out of five models'\njudges. This study highlights the potential and limitations of current LLMs for\nconsumer health medical question answering, indicating avenues for further\ndevelopment.", "paper_summary_zh": "\u672c\u7814\u7a76\u8a55\u4f30\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728 MedRedQA \u4e0a\u7684\u6548\u80fd\uff0cMedRedQA \u662f\u4e00\u7d44\u6d88\u8cbb\u8005\u91ab\u7642\u554f\u984c\u8207\u7b54\u6848\u7684\u8cc7\u6599\u96c6\uff0c\u7531 AskDocs \u5b50\u7248\u584a\u4e2d\u7d93\u904e\u9a57\u8b49\u7684\u5c08\u5bb6\u6240\u63d0\u51fa\u3002\u5118\u7ba1 LLM \u5df2\u5728\u81e8\u5e8a\u554f\u984c\u89e3\u7b54 (QA) \u57fa\u6e96\u4e2d\u5c55\u73fe\u51fa\u5c08\u696d\u77e5\u8b58\uff0c\u4f46\u5b83\u5011\u5728\u73fe\u5be6\u4e16\u754c\u3001\u6d88\u8cbb\u8005\u70ba\u57fa\u790e\u7684\u91ab\u7642\u554f\u984c\u4e0a\u7684\u6709\u6548\u6027\u4ecd\u8f03\u4e0d\u660e\u78ba\u3002MedRedQA \u63d0\u51fa\u7368\u7279\u7684\u6311\u6230\uff0c\u4f8b\u5982\u975e\u6b63\u5f0f\u8a9e\u8a00\u548c\u5c0d\u975e\u5c08\u5bb6\u67e5\u8a62\u63d0\u4f9b\u7cbe\u78ba\u56de\u61c9\u7684\u9700\u6c42\u3002\u70ba\u4e86\u8a55\u4f30\u6a21\u578b\u6548\u80fd\uff0c\u4f7f\u7528\u4e94\u500b LLM \u751f\u6210\u4e86\u56de\u61c9\uff1aGPT-4o mini\u3001Llama 3.1\uff1a70B\u3001Mistral-123B\u3001Mistral-7B \u548c Gemini-Flash\u3002\u4f7f\u7528\u4e86\u4ea4\u53c9\u8a55\u4f30\u65b9\u6cd5\uff0c\u5176\u4e2d\u6bcf\u500b\u6a21\u578b\u8a55\u4f30\u81ea\u5df1\u7684\u56de\u61c9\u4ee5\u53ca\u5176\u4ed6\u6a21\u578b\u7684\u56de\u61c9\uff0c\u4ee5\u6700\u5c0f\u5316\u504f\u5dee\u3002\u7d50\u679c\u986f\u793a\uff0c\u6839\u64da\u4e94\u500b\u6a21\u578b\u4e2d\u7684\u56db\u500b\u6a21\u578b\u8a55\u5be9\uff0cGPT-4o mini \u8207\u5c08\u5bb6\u56de\u61c9\u7684\u4e00\u81f4\u6027\u6700\u9ad8\uff0c\u800c\u6839\u64da\u4e94\u500b\u6a21\u578b\u4e2d\u7684\u4e09\u500b\u6a21\u578b\u8a55\u5be9\uff0cMistral-7B \u7684\u5206\u6578\u6700\u4f4e\u3002\u672c\u7814\u7a76\u5f37\u8abf\u4e86\u76ee\u524d LLM \u5728\u6d88\u8cbb\u8005\u5065\u5eb7\u91ab\u7642\u554f\u984c\u89e3\u7b54\u65b9\u9762\u7684\u6f5b\u529b\u548c\u9650\u5236\uff0c\u4e26\u6307\u51fa\u9032\u4e00\u6b65\u767c\u5c55\u7684\u9014\u5f91\u3002", "author": "Moaiz Abrar et.al.", "authors": "Moaiz Abrar, Yusuf Sermet, Ibrahim Demir", "id": "2501.00208v1", "paper_url": "http://arxiv.org/abs/2501.00208v1", "repo": "null"}}