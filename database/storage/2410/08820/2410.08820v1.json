{"2410.08820": {"publish_time": "2024-10-11", "title": "Which Demographics do LLMs Default to During Annotation?", "paper_summary": "Demographics and cultural background of annotators influence the labels they\nassign in text annotation -- for instance, an elderly woman might find it\noffensive to read a message addressed to a \"bro\", but a male teenager might\nfind it appropriate. It is therefore important to acknowledge label variations\nto not under-represent members of a society. Two research directions developed\nout of this observation in the context of using large language models (LLM) for\ndata annotations, namely (1) studying biases and inherent knowledge of LLMs and\n(2) injecting diversity in the output by manipulating the prompt with\ndemographic information. We combine these two strands of research and ask the\nquestion to which demographics an LLM resorts to when no demographics is given.\nTo answer this question, we evaluate which attributes of human annotators LLMs\ninherently mimic. Furthermore, we compare non-demographic conditioned prompts\nand placebo-conditioned prompts (e.g., \"you are an annotator who lives in house\nnumber 5\") to demographics-conditioned prompts (\"You are a 45 year old man and\nan expert on politeness annotation. How do you rate {instance}\"). We study\nthese questions for politeness and offensiveness annotations on the POPQUORN\ndata set, a corpus created in a controlled manner to investigate human label\nvariations based on demographics which has not been used for LLM-based analyses\nso far. We observe notable influences related to gender, race, and age in\ndemographic prompting, which contrasts with previous studies that found no such\neffects.", "paper_summary_zh": "\u6a19\u8a18\u54e1\u7684\u4eba\u53e3\u7d71\u8a08\u548c\u6587\u5316\u80cc\u666f\u6703\u5f71\u97ff\u4ed6\u5011\u5728\u6587\u5b57\u6a19\u8a18\u4e2d\u6307\u5b9a\u7684\u6a19\u7c64\u2014\u2014\u4f8b\u5982\uff0c\u4e00\u4f4d\u5e74\u9577\u5973\u6027\u53ef\u80fd\u6703\u89ba\u5f97\u6536\u5230\u4e00\u5247\u6a19\u984c\u70ba\u300c\u5144\u5f1f\u300d\u7684\u8a0a\u606f\u5f88\u5192\u72af\uff0c\u4f46\u4e00\u4f4d\u7537\u6027\u9752\u5c11\u5e74\u53ef\u80fd\u6703\u89ba\u5f97\u5f88\u5408\u9069\u3002\u56e0\u6b64\uff0c\u627f\u8a8d\u6a19\u7c64\u7684\u5dee\u7570\u6027\u5f88\u91cd\u8981\uff0c\u4ee5\u907f\u514d\u4f4e\u4f30\u793e\u6703\u6210\u54e1\u3002\u5728\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9032\u884c\u8cc7\u6599\u6a19\u8a18\u7684\u8108\u7d61\u4e2d\uff0c\u5f9e\u6b64\u89c0\u5bdf\u4e2d\u767c\u5c55\u51fa\u5169\u500b\u7814\u7a76\u65b9\u5411\uff0c\u5373 (1) \u7814\u7a76 LLM \u7684\u504f\u898b\u548c\u5167\u5728\u77e5\u8b58\uff0c\u4ee5\u53ca (2) \u900f\u904e\u4f7f\u7528\u4eba\u53e3\u7d71\u8a08\u8cc7\u8a0a\u64cd\u7e31\u63d0\u793a\u4f86\u6ce8\u5165\u8f38\u51fa\u4e2d\u7684\u591a\u6a23\u6027\u3002\u6211\u5011\u7d50\u5408\u9019\u5169\u500b\u7814\u7a76\u65b9\u5411\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u554f\u984c\uff0c\u7576\u6c92\u6709\u63d0\u4f9b\u4eba\u53e3\u7d71\u8a08\u8cc7\u8a0a\u6642\uff0cLLM \u6703\u8a34\u8af8\u54ea\u4e9b\u4eba\u53e3\u7d71\u8a08\u8cc7\u8a0a\u3002\u70ba\u4e86\u56de\u7b54\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u8a55\u4f30 LLM \u5167\u5728\u6a21\u4eff\u54ea\u4e9b\u4eba\u985e\u6a19\u8a18\u54e1\u7684\u5c6c\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u6bd4\u8f03\u975e\u4eba\u53e3\u7d71\u8a08\u689d\u4ef6\u63d0\u793a\u548c\u5b89\u6170\u5291\u689d\u4ef6\u63d0\u793a\uff08\u4f8b\u5982\uff0c\u300c\u4f60\u662f\u4e00\u4f4d\u4f4f\u5728 5 \u865f\u5c4b\u7684\u6a19\u8a18\u54e1\u300d\uff09\u8207\u4eba\u53e3\u7d71\u8a08\u689d\u4ef6\u63d0\u793a\uff08\u300c\u4f60\u662f\u4e00\u4f4d 45 \u6b72\u7684\u7537\u6027\uff0c\u4e5f\u662f\u79ae\u8c8c\u6a19\u8a18\u65b9\u9762\u7684\u5c08\u5bb6\u3002\u4f60\u5982\u4f55\u8a55\u5206 {\u5be6\u4f8b}\u300d\uff09\u3002\u6211\u5011\u91dd\u5c0d POPQUORN \u8cc7\u6599\u96c6\u4e0a\u7684\u79ae\u8c8c\u548c\u5192\u72af\u6027\u6a19\u8a18\u7814\u7a76\u9019\u4e9b\u554f\u984c\uff0c\u9019\u500b\u8a9e\u6599\u5eab\u662f\u4ee5\u53d7\u63a7\u65b9\u5f0f\u5efa\u7acb\u7684\uff0c\u76ee\u7684\u662f\u8abf\u67e5\u57fa\u65bc\u4eba\u53e3\u7d71\u8a08\u8cc7\u6599\u7684\u4eba\u985e\u6a19\u7c64\u5dee\u7570\uff0c\u800c\u9019\u5c1a\u672a\u7528\u65bc\u57fa\u65bc LLM \u7684\u5206\u6790\u3002\u6211\u5011\u89c0\u5bdf\u5230\u8207\u5148\u524d\u767c\u73fe\u6c92\u6709\u6b64\u985e\u5f71\u97ff\u7684\u7814\u7a76\u5f62\u6210\u5c0d\u6bd4\uff0c\u5728\u4eba\u53e3\u7d71\u8a08\u63d0\u793a\u4e2d\u8207\u6027\u5225\u3001\u7a2e\u65cf\u548c\u5e74\u9f61\u76f8\u95dc\u7684\u986f\u8457\u5f71\u97ff\u3002", "author": "Christopher Bagdon et.al.", "authors": "Christopher Bagdon, Aidan Combs, Lynn Greschner, Roman Klinger, Jiahui Li, Sean Papay, Nadine Probol, Yarik Menchaca Resendiz, Johannes Sch\u00e4fer, Aswathy Velutharambath, Sabine Weber, Amelie W\u00fchrl", "id": "2410.08820v1", "paper_url": "http://arxiv.org/abs/2410.08820v1", "repo": "null"}}