{"2405.14297": {"publish_time": "2024-05-23", "title": "Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models", "paper_summary": "The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the\nefficiency of training and inference for Transformer-based foundational models,\nyielding promising results. However, the performance of SMoE heavily depends on\nthe choice of hyper-parameters, such as the number of experts and the number of\nexperts to be activated (referred to as top-k), resulting in significant\ncomputational overhead due to the extensive model training by searching over\nvarious hyper-parameter configurations. As a remedy, we introduce the Dynamic\nMixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating\nmethod that enables each token to automatically determine the number of experts\nto activate. (2) An adaptive process automatically adjusts the number of\nexperts during training. Extensive numerical results across Vision, Language,\nand Vision-Language tasks demonstrate the effectiveness of our approach to\nachieve competitive performance compared to GMoE for vision and language tasks,\nand MoE-LLaVA for vision-language tasks, while maintaining efficiency by\nactivating fewer parameters. Our code is available at\nhttps://github.com/LINs-lab/DynMoE.", "paper_summary_zh": "\u7a00\u758f\u4e13\u5bb6\u6df7\u5408 (SMoE) \u5df2\u5e7f\u6cdb\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e Transformer \u7684\u57fa\u7840\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u53d6\u5f97\u4e86\u53ef\u559c\u7684\u6210\u679c\u3002\u7136\u800c\uff0cSMoE \u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u8d85\u53c2\u6570\u7684\u9009\u62e9\uff0c\u4f8b\u5982\u4e13\u5bb6\u6570\u91cf\u548c\u8981\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\uff08\u79f0\u4e3a top-k\uff09\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u7531\u4e8e\u641c\u7d22\u5404\u79cd\u8d85\u53c2\u6570\u914d\u7f6e\u800c\u8fdb\u884c\u7684\u5e7f\u6cdb\u6a21\u578b\u8bad\u7ec3\uff0c\u4ece\u800c\u4ea7\u751f\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u52a8\u6001\u4e13\u5bb6\u6df7\u5408 (DynMoE) \u6280\u672f\u3002DynMoE \u878d\u5408\u4e86 (1) \u4e00\u79cd\u65b0\u9896\u7684\u95e8\u63a7\u65b9\u6cd5\uff0c\u4f7f\u6bcf\u4e2a\u6807\u8bb0\u80fd\u591f\u81ea\u52a8\u786e\u5b9a\u8981\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\u3002(2) \u4e00\u4e2a\u81ea\u9002\u5e94\u8fc7\u7a0b\u5728\u8bad\u7ec3\u671f\u95f4\u81ea\u52a8\u8c03\u6574\u4e13\u5bb6\u6570\u91cf\u3002\u8de8\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u5927\u91cf\u6570\u503c\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5b9e\u73b0\u4e0e\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u7684 GMoE \u4ee5\u53ca\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684 MoE-LLaVA \u76f8\u6bd4\u7684\u7ade\u4e89\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u6fc0\u6d3b\u66f4\u5c11\u7684\u53c2\u6570\u6765\u4fdd\u6301\u6548\u7387\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728 https://github.com/LINs-lab/DynMoE \u83b7\u5f97\u3002", "author": "Yongxin Guo et.al.", "authors": "Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Tao Lin", "id": "2405.14297v1", "paper_url": "http://arxiv.org/abs/2405.14297v1", "repo": "null"}}