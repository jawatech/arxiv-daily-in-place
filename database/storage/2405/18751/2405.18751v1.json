{"2405.18751": {"publish_time": "2024-05-29", "title": "On the Limits of Multi-modal Meta-Learning with Auxiliary Task Modulation Using Conditional Batch Normalization", "paper_summary": "Few-shot learning aims to learn representations that can tackle novel tasks\ngiven a small number of examples. Recent studies show that cross-modal learning\ncan improve representations for few-shot classification. More specifically,\nlanguage is a rich modality that can be used to guide visual learning. In this\nwork, we experiment with a multi-modal architecture for few-shot learning that\nconsists of three components: a classifier, an auxiliary network, and a bridge\nnetwork. While the classifier performs the main classification task, the\nauxiliary network learns to predict language representations from the same\ninput, and the bridge network transforms high-level features of the auxiliary\nnetwork into modulation parameters for layers of the few-shot classifier using\nconditional batch normalization. The bridge should encourage a form of\nlightweight semantic alignment between language and vision which could be\nuseful for the classifier. However, after evaluating the proposed approach on\ntwo popular few-shot classification benchmarks we find that a) the improvements\ndo not reproduce across benchmarks, and b) when they do, the improvements are\ndue to the additional compute and parameters introduced by the bridge network.\nWe contribute insights and recommendations for future work in multi-modal\nmeta-learning, especially when using language representations.", "paper_summary_zh": "\u5c0f\u6837\u672c\u5b78\u7fd2\u65e8\u5728\u5b78\u7fd2\u53ef\u4ee5\u8655\u7406\u65b0\u4efb\u52d9\u7684\u8868\u793a\uff0c\u7d66\u5b9a\u5c11\u6578\u7bc4\u4f8b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u8de8\u6a21\u614b\u5b78\u7fd2\u53ef\u4ee5\u6539\u5584\u5c0f\u6a23\u672c\u5206\u985e\u7684\u8868\u793a\u3002\u66f4\u5177\u9ad4\u5730\u8aaa\uff0c\u8a9e\u8a00\u662f\u4e00\u7a2e\u8c50\u5bcc\u7684\u6a21\u614b\uff0c\u53ef\u7528\u65bc\u6307\u5c0e\u8996\u89ba\u5b78\u7fd2\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a66\u9a57\u4e86\u4e00\u500b\u591a\u6a21\u614b\u67b6\u69cb\uff0c\u7528\u65bc\u5c0f\u6a23\u672c\u5b78\u7fd2\uff0c\u5b83\u5305\u542b\u4e09\u500b\u7d44\u6210\u90e8\u5206\uff1a\u4e00\u500b\u5206\u985e\u5668\u3001\u4e00\u500b\u8f14\u52a9\u7db2\u8def\u548c\u4e00\u500b\u6a4b\u63a5\u7db2\u8def\u3002\u96d6\u7136\u5206\u985e\u5668\u57f7\u884c\u4e3b\u8981\u7684\u5206\u985e\u4efb\u52d9\uff0c\u4f46\u8f14\u52a9\u7db2\u8def\u5b78\u7fd2\u5f9e\u540c\u4e00\u500b\u8f38\u5165\u9810\u6e2c\u8a9e\u8a00\u8868\u793a\uff0c\u800c\u6a4b\u63a5\u7db2\u8def\u5c07\u8f14\u52a9\u7db2\u8def\u7684\u9ad8\u968e\u7279\u5fb5\u8f49\u63db\u70ba\u5c0f\u6a23\u672c\u5206\u985e\u5668\u5c64\u7684\u8abf\u88fd\u53c3\u6578\uff0c\u4f7f\u7528\u689d\u4ef6\u6279\u6b21\u6b63\u898f\u5316\u3002\u6a4b\u63a5\u61c9\u9f13\u52f5\u8a9e\u8a00\u548c\u8996\u89ba\u4e4b\u9593\u7684\u4e00\u7a2e\u5f62\u5f0f\u7684\u8f15\u91cf\u7d1a\u8a9e\u7fa9\u5c0d\u9f4a\uff0c\u9019\u5c0d\u65bc\u5206\u985e\u5668\u53ef\u80fd\u662f\u6709\u7528\u7684\u3002\u7136\u800c\uff0c\u5728\u5169\u500b\u6d41\u884c\u7684\u5c0f\u6a23\u672c\u5206\u985e\u57fa\u6e96\u4e0a\u8a55\u4f30\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5f8c\uff0c\u6211\u5011\u767c\u73fe a) \u6539\u9032\u4e0d\u6703\u5728\u57fa\u6e96\u4e4b\u9593\u91cd\u73fe\uff0c\u4ee5\u53ca b) \u7576\u5b83\u5011\u9019\u6a23\u505a\u6642\uff0c\u6539\u9032\u662f\u7531\u65bc\u6a4b\u63a5\u7db2\u8def\u5f15\u5165\u7684\u984d\u5916\u8a08\u7b97\u548c\u53c3\u6578\u3002\u6211\u5011\u70ba\u591a\u6a21\u614b\u5143\u5b78\u7fd2\u7684\u672a\u4f86\u5de5\u4f5c\u63d0\u4f9b\u898b\u89e3\u548c\u5efa\u8b70\uff0c\u7279\u5225\u662f\u5728\u4f7f\u7528\u8a9e\u8a00\u8868\u793a\u6642\u3002", "author": "Jordi Armengol-Estap\u00e9 et.al.", "authors": "Jordi Armengol-Estap\u00e9, Vincent Michalski, Ramnath Kumar, Pierre-Luc St-Charles, Doina Precup, Samira Ebrahimi Kahou", "id": "2405.18751v1", "paper_url": "http://arxiv.org/abs/2405.18751v1", "repo": "null"}}