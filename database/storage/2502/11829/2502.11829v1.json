{"2502.11829": {"publish_time": "2025-02-17", "title": "Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities", "paper_summary": "This paper introduces Code-Vision, a benchmark designed to evaluate the\nlogical understanding and code generation capabilities of Multimodal Large\nLanguage Models (MLLMs). It challenges MLLMs to generate a correct program that\nfulfills specific functionality requirements based on a given flowchart, which\nvisually represents the desired algorithm or process. Code-Vision comprises\nthree subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding\nabilities across basic programming, algorithmic, and mathematical\nproblem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.\nExperimental results demonstrate that there is a large performance difference\nbetween proprietary and open-source models. On Hard problems, GPT-4o can\nachieve 79.3% pass@1, but the best open-source model only achieves 15%. Further\nexperiments reveal that Code-Vision can pose unique challenges compared to\nother multimodal reasoning benchmarks MMCode and MathVista. We also explore the\nreason for the poor performance of the open-source models. All data and codes\nare available at https://github.com/wanghanbinpanda/CodeVision.", "paper_summary_zh": "\u672c\u6587\u4ecb\u7ecd Code-Vision\uff0c\u6b64\u57fa\u51c6\u6d4b\u8bd5\u65e8\u5728\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u903b\u8f91\u7406\u89e3\u548c\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002\u5b83\u8981\u6c42 MLLM \u6839\u636e\u7ed9\u5b9a\u7684\u6d41\u7a0b\u56fe\u751f\u6210\u4e00\u4e2a\u6b63\u786e\u7684\u7a0b\u5e8f\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7684\u529f\u80fd\u9700\u6c42\uff0c\u800c\u6d41\u7a0b\u56fe\u76f4\u89c2\u5730\u8868\u793a\u6240\u9700\u7684\u7b97\u6cd5\u6216\u6d41\u7a0b\u3002Code-Vision \u5305\u542b\u4e09\u4e2a\u5b50\u96c6\uff1aHumanEval-V\u3001Algorithm \u548c MATH\uff0c\u5b83\u4eec\u8bc4\u4f30 MLLM \u5728\u57fa\u672c\u7f16\u7a0b\u3001\u7b97\u6cd5\u548c\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u57df\u4e2d\u7684\u7f16\u7801\u80fd\u529b\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u5bf9 Code-Vision \u4e0a\u7684 12 \u4e2a MLLM \u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e13\u6709\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u5f88\u5927\u3002\u5728\u56f0\u96be\u95ee\u9898\u4e0a\uff0cGPT-4o \u53ef\u4ee5\u8fbe\u5230 79.3% \u7684 pass@1\uff0c\u4f46\u6700\u597d\u7684\u5f00\u6e90\u6a21\u578b\u53ea\u80fd\u8fbe\u5230 15%\u3002\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6 MMCode \u548c MathVista \u76f8\u6bd4\uff0cCode-Vision \u53ef\u80fd\u4f1a\u5e26\u6765\u72ec\u7279\u7684\u6311\u6218\u3002\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u7684\u539f\u56e0\u3002\u6240\u6709\u6570\u636e\u548c\u4ee3\u7801\u5747\u53ef\u5728 https://github.com/wanghanbinpanda/CodeVision \u4e2d\u83b7\u5f97\u3002", "author": "Hanbin Wang et.al.", "authors": "Hanbin Wang, Xiaoxuan Zhou, Zhipeng Xu, Keyuan Cheng, Yuxin Zuo, Kai Tian, Jingwei Song, Junting Lu, Wenhui Hu, Xueyang Liu", "id": "2502.11829v1", "paper_url": "http://arxiv.org/abs/2502.11829v1", "repo": "null"}}