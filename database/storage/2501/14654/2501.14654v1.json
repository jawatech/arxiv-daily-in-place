{"2501.14654": {"publish_time": "2025-01-24", "title": "MedAgentBench: Dataset for Benchmarking LLMs as Agents in Medical Applications", "paper_summary": "Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 100 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(GPT-4o) achieves a success rate of 72%. However, there is still substantial\nspace for improvement to give the community a next direction to optimize.\nFurthermore, there is significant variation in performance across task\ncategories. MedAgentBench establishes this and is publicly available at\nhttps://github.com/stanfordmlgroup/MedAgentBench , offering a valuable\nframework for model developers to track progress and drive continuous\nimprovements in the agent capabilities of large language models within the\nmedical domain.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u793a\u51fa\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u7279\u522b\u662f\u5728\u5176\u4f5c\u4e3a\u4ee3\u7406\u7684\u80fd\u529b\u65b9\u9762\uff0c\u4ece\u800c\u8d85\u8d8a\u4e86\u5176\u4f5c\u4e3a\u804a\u5929\u673a\u5668\u4eba\u7684\u4f20\u7edf\u89d2\u8272\u3002\u8fd9\u4e9b\u4ee3\u7406\u53ef\u4ee5\u5229\u7528\u5176\u89c4\u5212\u548c\u5de5\u5177\u5229\u7528\u80fd\u529b\u6765\u89e3\u51b3\u5728\u9ad8\u5c42\u6307\u5b9a\u7684\u4efb\u52a1\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u5bf9\u533b\u7597\u5e94\u7528\u4e2d LLM \u7684\u4ee3\u7406\u80fd\u529b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u7684\u6807\u51c6\u5316\u6570\u636e\u96c6\uff0c\u8fd9\u4f7f\u5f97\u5728\u4ea4\u4e92\u5f0f\u533b\u7597\u4fdd\u5065\u73af\u5883\u4e2d\u5bf9 LLM \u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 MedAgentBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5e7f\u6cdb\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u8bb0\u5f55\u80cc\u666f\u4e0b\u7684\u4ee3\u7406\u80fd\u529b\u3002MedAgentBench \u5305\u542b 100 \u4e2a\u7531\u4eba\u7c7b\u533b\u751f\u7f16\u5199\u7684\u6765\u81ea 10 \u4e2a\u7c7b\u522b\u7684\u7279\u5b9a\u4e8e\u60a3\u8005\u7684\u4e34\u5e8a\u4efb\u52a1\u3001100 \u4e2a\u60a3\u8005\u7684\u771f\u5b9e\u4e2a\u4eba\u8d44\u6599\uff08\u5305\u542b\u8d85\u8fc7 700,000 \u4e2a\u6570\u636e\u5143\u7d20\uff09\u3001\u4e00\u4e2a\u7b26\u5408 FHIR \u7684\u4ea4\u4e92\u5f0f\u73af\u5883\u4ee5\u53ca\u4e00\u4e2a\u914d\u5957\u7684\u4ee3\u7801\u5e93\u3002\u8be5\u73af\u5883\u4f7f\u7528\u73b0\u4ee3 EMR \u7cfb\u7edf\u4e2d\u4f7f\u7528\u7684\u6807\u51c6 API \u548c\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\uff0c\u56e0\u6b64\u53ef\u4ee5\u8f7b\u677e\u5730\u8fc1\u79fb\u5230\u5b9e\u65f6 EMR \u7cfb\u7edf\u4e2d\u3002MedAgentBench \u5448\u73b0\u4e86\u4e00\u4e2a\u672a\u9971\u548c\u7684\u4ee5\u4ee3\u7406\u4e3a\u5bfc\u5411\u7684\u57fa\u51c6\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684 LLM \u8868\u73b0\u51fa\u4e00\u5b9a\u7a0b\u5ea6\u7684\u6210\u529f\u80fd\u529b\u3002\u6700\u597d\u7684\u6a21\u578b (GPT-4o) \u7684\u6210\u529f\u7387\u8fbe\u5230 72%\u3002\u7136\u800c\uff0c\u4ecd\u7136\u6709\u5f88\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u53ef\u4ee5\u4e3a\u793e\u533a\u63d0\u4f9b\u4f18\u5316\u65b9\u5411\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u4efb\u52a1\u7c7b\u522b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u5f88\u5927\u3002MedAgentBench \u5efa\u7acb\u4e86\u8fd9\u4e00\u70b9\uff0c\u5e76\u5728 https://github.com/stanfordmlgroup/MedAgentBench \u516c\u5f00\u63d0\u4f9b\uff0c\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8ddf\u8e2a\u8fdb\u5ea6\u5e76\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u4ee3\u7406\u80fd\u529b\u7684\u6301\u7eed\u6539\u8fdb\u3002</paragraph>", "author": "Yixing Jiang et.al.", "authors": "Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, Andrew Y. Ng, Jonathan H. Chen", "id": "2501.14654v1", "paper_url": "http://arxiv.org/abs/2501.14654v1", "repo": "https://github.com/stanfordmlgroup/medagentbench"}}