{"2412.07220": {"publish_time": "2024-12-10", "title": "Comateformer: Combined Attention Transformer for Semantic Sentence Matching", "paper_summary": "The Transformer-based model have made significant strides in semantic\nmatching tasks by capturing connections between phrase pairs. However, to\nassess the relevance of sentence pairs, it is insufficient to just examine the\ngeneral similarity between the sentences. It is crucial to also consider the\ntiny subtleties that differentiate them from each other. Regrettably, attention\nsoftmax operations in transformers tend to miss these subtle differences. To\nthis end, in this work, we propose a novel semantic sentence matching model\nnamed Combined Attention Network based on Transformer model (Comateformer). In\nComateformer model, we design a novel transformer-based quasi-attention\nmechanism with compositional properties. Unlike traditional attention\nmechanisms that merely adjust the weights of input tokens, our proposed method\nlearns how to combine, subtract, or resize specific vectors when building a\nrepresentation. Moreover, our proposed approach builds on the intuition of\nsimilarity and dissimilarity (negative affinity) when calculating dual affinity\nscores. This allows for a more meaningful representation of relationships\nbetween sentences. To evaluate the performance of our proposed model, we\nconducted extensive experiments on ten public real-world datasets and\nrobustness testing. Experimental results show that our method achieves\nconsistent improvements.", "paper_summary_zh": "\u57fa\u65bc Transformer \u6a21\u578b\u7684 Transformer \u6a21\u578b\u5728\u8a9e\u7fa9\u914d\u5c0d\u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86\u91cd\u5927\u9032\u5c55\uff0c\u65b9\u6cd5\u662f\u64f7\u53d6\u8a5e\u7d44\u5c0d\u4e4b\u9593\u7684\u95dc\u806f\u3002\u7136\u800c\uff0c\u8981\u8a55\u4f30\u53e5\u5b50\u5c0d\u7684\u76f8\u95dc\u6027\uff0c\u50c5\u6aa2\u67e5\u53e5\u5b50\u4e4b\u9593\u7684\u4e00\u822c\u76f8\u4f3c\u6027\u662f\u4e0d\u5920\u7684\u3002\u8003\u616e\u4f7f\u5b83\u5011\u5f7c\u6b64\u5340\u5225\u958b\u4f86\u7684\u7d30\u5fae\u5dee\u5225\u4e5f\u81f3\u95dc\u91cd\u8981\u3002\u907a\u61be\u7684\u662f\uff0cTransformer\u4e2d\u7684\u6ce8\u610f\u529b softmax \u904b\u7b97\u5f80\u5f80\u6703\u907a\u6f0f\u9019\u4e9b\u7d30\u5fae\u7684\u5dee\u7570\u3002\u70ba\u6b64\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u540d\u70ba\u57fa\u65bc Transformer \u6a21\u578b\u7684\u7d44\u5408\u6ce8\u610f\u529b\u7db2\u8def\uff08Comateformer\uff09\u7684\u65b0\u578b\u8a9e\u7fa9\u53e5\u5b50\u5339\u914d\u6a21\u578b\u3002\u5728 Comateformer \u6a21\u578b\u4e2d\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u57fa\u65bc Transformer \u7684\u6e96\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u5177\u6709\u7d44\u5408\u7279\u6027\u3002\u8207\u50c5\u8abf\u6574\u8f38\u5165\u6a19\u8a18\u6b0a\u91cd\u7684\u50b3\u7d71\u6ce8\u610f\u529b\u6a5f\u5236\u4e0d\u540c\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5b78\u7fd2\u5982\u4f55\u5728\u5efa\u7acb\u8868\u793a\u6642\u7d44\u5408\u3001\u6e1b\u53bb\u6216\u8abf\u6574\u7279\u5b9a\u5411\u91cf\u7684\u5927\u5c0f\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u5efa\u7acb\u5728\u8a08\u7b97\u96d9\u91cd\u89aa\u548c\u5206\u6578\u6642\u76f8\u4f3c\u6027\u548c\u76f8\u7570\u6027\uff08\u8ca0\u89aa\u548c\u6027\uff09\u7684\u76f4\u89ba\u4e0a\u3002\u9019\u5141\u8a31\u5c0d\u53e5\u5b50\u4e4b\u9593\u7684\u95dc\u4fc2\u9032\u884c\u66f4\u6709\u610f\u7fa9\u7684\u8868\u793a\u3002\u70ba\u4e86\u8a55\u4f30\u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b\u7684\u6548\u80fd\uff0c\u6211\u5011\u5c0d\u5341\u500b\u516c\u5171\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u548c\u7a69\u5065\u6027\u6e2c\u8a66\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u9032\u6b65\u3002", "author": "Bo Li et.al.", "authors": "Bo Li, Di Liang, Zixin Zhang", "id": "2412.07220v1", "paper_url": "http://arxiv.org/abs/2412.07220v1", "repo": "null"}}