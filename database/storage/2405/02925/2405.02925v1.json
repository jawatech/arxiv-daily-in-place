{"2405.02925": {"publish_time": "2024-05-05", "title": "A Two-Stage Prediction-Aware Contrastive Learning Framework for Multi-Intent NLU", "paper_summary": "Multi-intent natural language understanding (NLU) presents a formidable\nchallenge due to the model confusion arising from multiple intents within a\nsingle utterance. While previous works train the model contrastively to\nincrease the margin between different multi-intent labels, they are less suited\nto the nuances of multi-intent NLU. They ignore the rich information between\nthe shared intents, which is beneficial to constructing a better embedding\nspace, especially in low-data scenarios. We introduce a two-stage\nPrediction-Aware Contrastive Learning (PACL) framework for multi-intent NLU to\nharness this valuable knowledge. Our approach capitalizes on shared intent\ninformation by integrating word-level pre-training and prediction-aware\ncontrastive fine-tuning. We construct a pre-training dataset using a word-level\ndata augmentation strategy. Subsequently, our framework dynamically assigns\nroles to instances during contrastive fine-tuning while introducing a\nprediction-aware contrastive loss to maximize the impact of contrastive\nlearning. We present experimental results and empirical analysis conducted on\nthree widely used datasets, demonstrating that our method surpasses the\nperformance of three prominent baselines on both low-data and full-data\nscenarios.", "paper_summary_zh": "", "author": "Guanhua Chen et.al.", "authors": "Guanhua Chen,Yutong Yao,Derek F. Wong,Lidia S. Chao", "id": "2405.02925v1", "paper_url": "http://arxiv.org/abs/2405.02925v1", "repo": "null"}}