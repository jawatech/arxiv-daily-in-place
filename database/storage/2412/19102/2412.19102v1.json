{"2412.19102": {"publish_time": "2024-12-26", "title": "\"I've Heard of You!\": Generate Spoken Named Entity Recognition Data for Unseen Entities", "paper_summary": "Spoken named entity recognition (NER) aims to identify named entities from\nspeech, playing an important role in speech processing. New named entities\nappear every day, however, annotating their Spoken NER data is costly. In this\npaper, we demonstrate that existing Spoken NER systems perform poorly when\ndealing with previously unseen named entities. To tackle this challenge, we\npropose a method for generating Spoken NER data based on a named entity\ndictionary (NED) to reduce costs. Specifically, we first use a large language\nmodel (LLM) to generate sentences from the sampled named entities and then use\na text-to-speech (TTS) system to generate the speech. Furthermore, we introduce\na noise metric to filter out noisy data. To evaluate our approach, we release a\nnovel Spoken NER benchmark along with a corresponding NED containing 8,853\nentities. Experiment results show that our method achieves state-of-the-art\n(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully\nzero-shot settings. Our data will be available at\nhttps://github.com/DeepLearnXMU/HeardU.", "paper_summary_zh": "\u53e3\u8a9e\u5316\u547d\u540d\u5be6\u9ad4\u8b58\u5225 (NER) \u65e8\u5728\u5f9e\u8a9e\u97f3\u4e2d\u8b58\u5225\u547d\u540d\u5be6\u9ad4\uff0c\u5728\u8a9e\u97f3\u8655\u7406\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\u3002\u6bcf\u5929\u90fd\u6703\u51fa\u73fe\u65b0\u7684\u547d\u540d\u5be6\u9ad4\uff0c\u7136\u800c\u5c0d\u5176\u53e3\u8a9e\u5316 NER \u8cc7\u6599\u9032\u884c\u8a3b\u89e3\u7684\u6210\u672c\u5f88\u9ad8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u73fe\u6709\u7684\u53e3\u8a9e\u5316 NER \u7cfb\u7d71\u5728\u8655\u7406\u4ee5\u524d\u672a\u898b\u904e\u7684\u547d\u540d\u5be6\u9ad4\u6642\u8868\u73fe\u4e0d\u4f73\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u547d\u540d\u5be6\u9ad4\u5b57\u5178 (NED) \u4f86\u7522\u751f\u53e3\u8a9e\u5316 NER \u8cc7\u6599\u7684\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5f9e\u53d6\u6a23\u7684\u547d\u540d\u5be6\u9ad4\u4e2d\u7522\u751f\u53e5\u5b50\uff0c\u7136\u5f8c\u4f7f\u7528\u6587\u5b57\u8f49\u8a9e\u97f3 (TTS) \u7cfb\u7d71\u7522\u751f\u8a9e\u97f3\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u96dc\u8a0a\u6307\u6a19\u4f86\u904e\u6ffe\u96dc\u8a0a\u8cc7\u6599\u3002\u70ba\u4e86\u8a55\u4f30\u6211\u5011\u7684\u505a\u6cd5\uff0c\u6211\u5011\u767c\u5e03\u4e86\u4e00\u500b\u65b0\u7684\u53e3\u8a9e\u5316 NER \u57fa\u6e96\uff0c\u4ee5\u53ca\u4e00\u500b\u5305\u542b 8,853 \u500b\u5be6\u9ad4\u7684\u5c0d\u61c9 NED\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u7279\u5b9a\u9818\u57df\u3001\u96f6\u6b21\u5b78\u7fd2\u9818\u57df\u9069\u61c9\u548c\u5b8c\u5168\u96f6\u6b21\u5b78\u7fd2\u8a2d\u5b9a\u4e2d\u5be6\u73fe\u4e86\u6700\u5148\u9032 (SOTA) \u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u8cc7\u6599\u5c07\u5728 https://github.com/DeepLearnXMU/HeardU \u4e0a\u63d0\u4f9b\u3002", "author": "Jiawei Yu et.al.", "authors": "Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su", "id": "2412.19102v1", "paper_url": "http://arxiv.org/abs/2412.19102v1", "repo": "null"}}