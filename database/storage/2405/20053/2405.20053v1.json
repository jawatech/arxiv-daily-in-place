{"2405.20053": {"publish_time": "2024-05-30", "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads", "paper_summary": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context\nlearning capabilities; however, their behaviors are often difficult to control.\nBy utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible\nto fine-tune unsupervised LMs to follow instructions and produce outputs that\nreflect human preferences. Despite its benefits, RLHF has been shown to\npotentially harm a language model's reasoning capabilities and introduce\nartifacts such as hallucinations where the model may fabricate facts. To\naddress this issue we introduce Direct Preference Heads (DPH), a fine-tuning\nframework that enables LMs to learn human preference signals through an\nauxiliary reward head without directly affecting the output distribution of the\nlanguage modeling head. We perform a theoretical analysis of our objective\nfunction and find strong ties to Conservative Direct Preference Optimization\n(cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All\nevaluation suite and demonstrate that our method produces models which achieve\nhigher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct\nPreference Optimization (DPO) alone.", "paper_summary_zh": "\u9810\u5148\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b (LM) \u5c55\u73fe\u5f37\u5927\u7684\u96f6\u6b21\u5b78\u7fd2\u548c\u60c5\u5883\u5b78\u7fd2\u80fd\u529b\uff1b\u7136\u800c\uff0c\u5b83\u5011\u7684\u884c\u70ba\u901a\u5e38\u96e3\u4ee5\u63a7\u5236\u3002\u85c9\u7531\u5229\u7528\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF)\uff0c\u53ef\u4ee5\u5fae\u8abf\u7121\u76e3\u7763\u7684 LM \u4ee5\u9075\u5faa\u6307\u793a\u4e26\u7522\u751f\u53cd\u6620\u4eba\u985e\u504f\u597d\u7684\u8f38\u51fa\u3002\u5118\u7ba1\u6709\u9019\u4e9b\u597d\u8655\uff0c\u4f46 RLHF \u5df2\u88ab\u8b49\u660e\u53ef\u80fd\u6703\u640d\u5bb3\u8a9e\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e26\u5f15\u5165\u5e7b\u89ba\u7b49\u4eba\u5de5\u88fd\u54c1\uff0c\u5176\u4e2d\u6a21\u578b\u53ef\u80fd\u6703\u634f\u9020\u4e8b\u5be6\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u76f4\u63a5\u504f\u597d\u982d (DPH)\uff0c\u9019\u662f\u4e00\u500b\u5fae\u8abf\u6846\u67b6\uff0c\u4f7f LM \u80fd\u5920\u900f\u904e\u8f14\u52a9\u734e\u52f5\u982d\u5b78\u7fd2\u4eba\u985e\u504f\u597d\u8a0a\u865f\uff0c\u800c\u4e0d\u6703\u76f4\u63a5\u5f71\u97ff\u8a9e\u8a00\u5efa\u6a21\u982d\u7684\u8f38\u51fa\u5206\u4f48\u3002\u6211\u5011\u5c0d\u6211\u5011\u7684\u76ee\u6a19\u51fd\u6578\u9032\u884c\u7406\u8ad6\u5206\u6790\uff0c\u4e26\u767c\u73fe\u8207\u4fdd\u5b88\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (cDPO) \u6709\u5bc6\u5207\u95dc\u806f\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728 GLUE\u3001RACE \u548c GPT4All \u8a55\u4f30\u5957\u4ef6\u4e0a\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\uff0c\u4e26\u8b49\u660e\u6211\u5011\u7684\u6a21\u578b\u7522\u751f\u4e86\u6bd4\u50c5\u4f7f\u7528\u76e3\u7763\u5fae\u8abf (SFT) \u6216\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u5fae\u8abf\u7684\u6a21\u578b\u7372\u5f97\u66f4\u9ad8\u5206\u6578\u3002", "author": "Avelina Asada Hadji-Kyriacou et.al.", "authors": "Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic", "id": "2405.20053v1", "paper_url": "http://arxiv.org/abs/2405.20053v1", "repo": "null"}}