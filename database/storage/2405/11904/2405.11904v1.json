{"2405.11904": {"publish_time": "2024-05-20", "title": "A Constraint-Enforcing Reward for Adversarial Attacks on Text Classifiers", "paper_summary": "Text classifiers are vulnerable to adversarial examples --\ncorrectly-classified examples that are deliberately transformed to be\nmisclassified while satisfying acceptability constraints. The conventional\napproach to finding adversarial examples is to define and solve a combinatorial\noptimisation problem over a space of allowable transformations. While\neffective, this approach is slow and limited by the choice of transformations.\nAn alternate approach is to directly generate adversarial examples by\nfine-tuning a pre-trained language model, as is commonly done for other\ntext-to-text tasks. This approach promises to be much quicker and more\nexpressive, but is relatively unexplored. For this reason, in this work we\ntrain an encoder-decoder paraphrase model to generate a diverse range of\nadversarial examples. For training, we adopt a reinforcement learning algorithm\nand propose a constraint-enforcing reward that promotes the generation of valid\nadversarial examples. Experimental results over two text classification\ndatasets show that our model has achieved a higher success rate than the\noriginal paraphrase model, and overall has proved more effective than other\ncompetitive attacks. Finally, we show how key design choices impact the\ngenerated examples and discuss the strengths and weaknesses of the proposed\napproach.", "paper_summary_zh": "\u6587\u672c\u5206\u7c7b\u5668\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u5f71\u54cd\u2014\u2014\u7ecf\u8fc7\u84c4\u610f\u8f6c\u6362\u4ee5\u88ab\u9519\u8bef\u5206\u7c7b\uff0c\u540c\u65f6\u6ee1\u8db3\u53ef\u63a5\u53d7\u6027\u7ea6\u675f\u7684\u6b63\u786e\u5206\u7c7b\u793a\u4f8b\u3002\u5bfb\u627e\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u4f20\u7edf\u65b9\u6cd5\u662f\u5728\u5141\u8bb8\u7684\u8f6c\u6362\u7a7a\u95f4\u4e0a\u5b9a\u4e49\u548c\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u867d\u7136\u6709\u6548\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5f88\u6162\uff0c\u5e76\u4e14\u53d7\u5230\u8f6c\u6362\u9009\u62e9\u7684\u5f71\u54cd\u3002\u53e6\u4e00\u79cd\u65b9\u6cd5\u662f\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u6765\u76f4\u63a5\u751f\u6210\u5bf9\u6297\u6027\u793a\u4f8b\uff0c\u5c31\u50cf\u901a\u5e38\u5bf9\u5176\u4ed6\u6587\u672c\u5230\u6587\u672c\u4efb\u52a1\u6240\u505a\u7684\u90a3\u6837\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u671b\u66f4\u5feb\u3001\u66f4\u5177\u8868\u73b0\u529b\uff0c\u4f46\u76f8\u5bf9\u6765\u8bf4\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002\u51fa\u4e8e\u8fd9\u4e2a\u539f\u56e0\uff0c\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u91ca\u4e49\u6a21\u578b\u6765\u751f\u6210\u5404\u79cd\u5bf9\u6297\u6027\u793a\u4f8b\u3002\u5728\u8bad\u7ec3\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5236\u7ea6\u675f\u7684\u5956\u52b1\uff0c\u4ee5\u4fc3\u8fdb\u751f\u6210\u6709\u6548\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u3002\u5728\u4e24\u4e2a\u6587\u672c\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u6bd4\u539f\u59cb\u91ca\u4e49\u6a21\u578b\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5e76\u4e14\u603b\u4f53\u4e0a\u6bd4\u5176\u4ed6\u7ade\u4e89\u6027\u653b\u51fb\u66f4\u6709\u6548\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u751f\u6210\u7684\u793a\u4f8b\uff0c\u5e76\u8ba8\u8bba\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u4f18\u70b9\u548c\u7f3a\u70b9\u3002", "author": "Tom Roth et.al.", "authors": "Tom Roth, Inigo Jauregi Unanue, Alsharif Abuadbba, Massimo Piccardi", "id": "2405.11904v1", "paper_url": "http://arxiv.org/abs/2405.11904v1", "repo": "null"}}