{"2410.22179": {"publish_time": "2024-10-29", "title": "Very Attentive Tacotron: Robust and Unbounded Length Generalization in Autoregressive Transformer-Based Text-to-Speech", "paper_summary": "Autoregressive (AR) Transformer-based sequence models are known to have\ndifficulty generalizing to sequences longer than those seen during training.\nWhen applied to text-to-speech (TTS), these models tend to drop or repeat words\nor produce erratic output, especially for longer utterances. In this paper, we\nintroduce enhancements aimed at AR Transformer-based encoder-decoder TTS\nsystems that address these robustness and length generalization issues. Our\napproach uses an alignment mechanism to provide cross-attention operations with\nrelative location information. The associated alignment position is learned as\na latent property of the model via backprop and requires no external alignment\ninformation during training. While the approach is tailored to the monotonic\nnature of TTS input-output alignment, it is still able to benefit from the\nflexible modeling power of interleaved multi-head self- and cross-attention\noperations. A system incorporating these improvements, which we call Very\nAttentive Tacotron, matches the naturalness and expressiveness of a baseline\nT5-based TTS system, while eliminating problems with repeated or dropped words\nand enabling generalization to any practical utterance length.", "paper_summary_zh": "\u81ea\u56de\u5f52 (AR) Transformer \u4e3a\u57fa\u7840\u7684\u5e8f\u5217\u6a21\u578b\u5df2\u77e5\u5728\u63a8\u5e7f\u5230\u6bd4\u8bad\u7ec3\u671f\u95f4\u6240\u89c1\u7684\u5e8f\u5217\u66f4\u957f\u7684\u5e8f\u5217\u65f6\u6709\u56f0\u96be\u3002\u5f53\u5e94\u7528\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3 (TTS) \u65f6\uff0c\u8fd9\u4e9b\u6a21\u578b\u503e\u5411\u4e8e\u4e22\u5f03\u6216\u91cd\u590d\u5355\u8bcd\u6216\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u8f93\u51fa\uff0c\u5c24\u5176\u5bf9\u4e8e\u8f83\u957f\u7684\u8bed\u97f3\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u9488\u5bf9 AR Transformer \u4e3a\u57fa\u7840\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668 TTS \u7cfb\u7edf\u7684\u589e\u5f3a\u529f\u80fd\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u9c81\u68d2\u6027\u548c\u957f\u5ea6\u63a8\u5e7f\u95ee\u9898\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u5bf9\u9f50\u673a\u5236\u6765\u63d0\u4f9b\u5177\u6709\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u7684\u4ea4\u53c9\u6ce8\u610f\u64cd\u4f5c\u3002\u5173\u8054\u7684\u5bf9\u9f50\u4f4d\u7f6e\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u4f5c\u4e3a\u6a21\u578b\u7684\u6f5c\u5728\u5c5e\u6027\u5b66\u4e60\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u671f\u95f4\u4e0d\u9700\u8981\u5916\u90e8\u5bf9\u9f50\u4fe1\u606f\u3002\u867d\u7136\u8be5\u65b9\u6cd5\u9488\u5bf9 TTS \u8f93\u5165\u8f93\u51fa\u5bf9\u9f50\u7684\u5355\u8c03\u7279\u6027\u91cf\u8eab\u5b9a\u5236\uff0c\u4f46\u5b83\u4ecd\u7136\u80fd\u591f\u53d7\u76ca\u4e8e\u4ea4\u9519\u591a\u5934\u81ea\u6ce8\u610f\u548c\u4ea4\u53c9\u6ce8\u610f\u64cd\u4f5c\u7684\u7075\u6d3b\u5efa\u6a21\u80fd\u529b\u3002\u5305\u542b\u8fd9\u4e9b\u6539\u8fdb\u7684\u7cfb\u7edf\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u975e\u5e38\u6ce8\u610f\u7684 Tacotron\uff0c\u5339\u914d\u4e86\u57fa\u4e8e T5 \u7684\u57fa\u7ebf TTS \u7cfb\u7edf\u7684\u81ea\u7136\u6027\u548c\u8868\u73b0\u529b\uff0c\u540c\u65f6\u6d88\u9664\u4e86\u91cd\u590d\u6216\u4e22\u5f03\u5355\u8bcd\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u4efb\u4f55\u5b9e\u9645\u8bed\u97f3\u957f\u5ea6\u7684\u63a8\u5e7f\u3002", "author": "Eric Battenberg et.al.", "authors": "Eric Battenberg, RJ Skerry-Ryan, Daisy Stanton, Soroosh Mariooryad, Matt Shannon, Julian Salazar, David Kao", "id": "2410.22179v1", "paper_url": "http://arxiv.org/abs/2410.22179v1", "repo": "null"}}