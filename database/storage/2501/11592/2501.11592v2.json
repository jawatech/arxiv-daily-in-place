{"2501.11592": {"publish_time": "2025-01-20", "title": "Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing", "paper_summary": "Pre-trained large models attract widespread attention in recent years, but\nthey face challenges in applications that require high interpretability or have\nlimited resources, such as physical sensing, medical imaging, and\nbioinformatics. Compressed Sensing (CS) is a well-proved theory that drives\nmany recent breakthroughs in these applications. However, as a typical\nunder-determined linear system, CS suffers from excessively long sparse\nreconstruction times when using traditional iterative methods, particularly\nwith large-scale data. Current AI methods like deep unfolding fail to\nsubstitute them because pre-trained models exhibit poor generality beyond their\ntraining conditions and dataset distributions, or lack interpretability.\nInstead of following the big model fervor, this paper proposes ultra-small\nartificial neural models called coefficients learning (CL), enabling\ntraining-free and rapid sparse reconstruction while perfectly inheriting the\ngenerality and interpretability of traditional iterative methods, bringing new\nfeature of incorporating prior knowledges. In CL, a signal of length $n$ only\nneeds a minimal of $n$ trainable parameters. A case study model called CLOMP is\nimplemented for evaluation. Experiments are conducted on both synthetic and\nreal one-dimensional and two-dimensional signals, demonstrating significant\nimprovements in efficiency and accuracy. Compared to representative iterative\nmethods, CLOMP improves efficiency by 100 to 1000 folds for large-scale data.\nTest results on eight diverse image datasets indicate that CLOMP improves\nstructural similarity index by 292%, 98%, 45% for sampling rates of 0.1, 0.3,\n0.5, respectively. We believe this method can truly usher CS reconstruction\ninto the AI era, benefiting countless under-determined linear systems that rely\non sparse solution.", "paper_summary_zh": "<paragraph>\u9810\u8a13\u7df4\u5927\u578b\u6a21\u578b\u8fd1\u5e74\u4f86\u5ee3\u53d7\u95dc\u6ce8\uff0c\u4f46\u5b83\u5011\u5728\u9700\u8981\u9ad8\u53ef\u89e3\u91cb\u6027\u6216\u8cc7\u6e90\u53d7\u9650\u7684\u61c9\u7528\u4e2d\u9762\u81e8\u6311\u6230\uff0c\u4f8b\u5982\u7269\u7406\u611f\u6e2c\u3001\u91ab\u5b78\u5f71\u50cf\u548c\u751f\u7269\u8cc7\u8a0a\u5b78\u3002\u58d3\u7e2e\u611f\u6e2c (CS) \u662f\u4e00\u500b\u7d93\u904e\u9a57\u8b49\u7684\u7406\u8ad6\uff0c\u63a8\u52d5\u4e86\u9019\u4e9b\u61c9\u7528\u4e2d\u7684\u8a31\u591a\u8fd1\u671f\u7a81\u7834\u3002\u7136\u800c\uff0c\u4f5c\u70ba\u4e00\u500b\u5178\u578b\u7684\u6b20\u5b9a\u7dda\u6027\u7cfb\u7d71\uff0cCS \u5728\u4f7f\u7528\u50b3\u7d71\u8fed\u4ee3\u65b9\u6cd5\u6642\u6703\u5c0e\u81f4\u904e\u9577\u7684\u7a00\u758f\u91cd\u5efa\u6642\u9593\uff0c\u7279\u5225\u662f\u5728\u5927\u898f\u6a21\u8cc7\u6599\u7684\u60c5\u6cc1\u4e0b\u3002\u50cf\u6df1\u5ea6\u5c55\u958b\u7b49\u7576\u524d AI \u65b9\u6cd5\u7121\u6cd5\u53d6\u4ee3\u5b83\u5011\uff0c\u56e0\u70ba\u9810\u8a13\u7df4\u6a21\u578b\u5728\u8a13\u7df4\u689d\u4ef6\u548c\u8cc7\u6599\u96c6\u5206\u4f48\u4e4b\u5916\u8868\u73fe\u51fa\u8f03\u5dee\u7684\u6982\u62ec\u6027\uff0c\u6216\u7f3a\u4e4f\u53ef\u89e3\u91cb\u6027\u3002\u672c\u8ad6\u6587\u6c92\u6709\u8ffd\u96a8\u5927\u578b\u6a21\u578b\u71b1\u6f6e\uff0c\u800c\u662f\u63d0\u51fa\u4e86\u7a31\u70ba\u4fc2\u6578\u5b78\u7fd2 (CL) \u7684\u8d85\u5c0f\u578b\u4eba\u5de5\u795e\u7d93\u7db2\u8def\u6a21\u578b\uff0c\u5be6\u73fe\u7121\u8a13\u7df4\u4e14\u5feb\u901f\u7684\u7a00\u758f\u91cd\u5efa\uff0c\u540c\u6642\u5b8c\u7f8e\u7e7c\u627f\u50b3\u7d71\u8fed\u4ee3\u65b9\u6cd5\u7684\u6982\u62ec\u6027\u548c\u53ef\u89e3\u91cb\u6027\uff0c\u5e36\u4f86\u7d50\u5408\u5148\u9a57\u77e5\u8b58\u7684\u65b0\u7279\u9ede\u3002\u5728 CL \u4e2d\uff0c\u9577\u5ea6\u70ba $n$ \u7684\u4fe1\u865f\u53ea\u9700\u8981\u6700\u5c11 $n$ \u500b\u53ef\u8a13\u7df4\u53c3\u6578\u3002\u5be6\u4f5c\u4e86\u4e00\u500b\u7a31\u70ba CLOMP \u7684\u6848\u4f8b\u7814\u7a76\u6a21\u578b\u9032\u884c\u8a55\u4f30\u3002\u5728\u5408\u6210\u548c\u771f\u5be6\u7684\u4e00\u7dad\u548c\u4e8c\u7dad\u4fe1\u865f\u4e0a\u9032\u884c\u4e86\u5be6\u9a57\uff0c\u8b49\u660e\u4e86\u6548\u7387\u548c\u6e96\u78ba\u6027\u7684\u986f\u8457\u63d0\u5347\u3002\u8207\u5177\u4ee3\u8868\u6027\u7684\u8fed\u4ee3\u65b9\u6cd5\u76f8\u6bd4\uff0cCLOMP \u5c07\u5927\u578b\u8cc7\u6599\u7684\u6548\u7387\u63d0\u5347\u4e86 100 \u5230 1000 \u500d\u3002\u5728\u516b\u500b\u4e0d\u540c\u7684\u5f71\u50cf\u8cc7\u6599\u96c6\u4e0a\u7684\u6e2c\u8a66\u7d50\u679c\u8868\u660e\uff0cCLOMP \u5206\u5225\u5c07\u63a1\u6a23\u7387\u70ba 0.1\u30010.3\u30010.5 \u7684\u7d50\u69cb\u76f8\u4f3c\u6027\u6307\u6a19\u63d0\u5347\u4e86 292%\u300198%\u300145%\u3002\u6211\u5011\u76f8\u4fe1\u9019\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u771f\u6b63\u5c07 CS \u91cd\u5efa\u5e36\u5165 AI \u6642\u4ee3\uff0c\u4f7f\u4f9d\u8cf4\u7a00\u758f\u89e3\u7684\u7121\u6578\u6b20\u5b9a\u7dda\u6027\u7cfb\u7d71\u53d7\u76ca\u3002</paragraph>", "author": "Chaoqing Tang et.al.", "authors": "Chaoqing Tang, Huanze Zhuang, Guiyun Tian, Zhenli Zeng, Yi Ding, Wenzhong Liu, Xiang Bai", "id": "2501.11592v2", "paper_url": "http://arxiv.org/abs/2501.11592v2", "repo": "https://github.com/billttzqgbt/cscoefficientslearning"}}