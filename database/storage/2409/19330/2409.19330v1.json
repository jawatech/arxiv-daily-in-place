{"2409.19330": {"publish_time": "2024-09-28", "title": "3D-CT-GPT: Generating 3D Radiology Reports through Integration of Large Vision-Language Models", "paper_summary": "Medical image analysis is crucial in modern radiological diagnostics,\nespecially given the exponential growth in medical imaging data. The demand for\nautomated report generation systems has become increasingly urgent. While prior\nresearch has mainly focused on using machine learning and multimodal language\nmodels for 2D medical images, the generation of reports for 3D medical images\nhas been less explored due to data scarcity and computational complexities.\nThis paper introduces 3D-CT-GPT, a Visual Question Answering (VQA)-based\nmedical visual language model specifically designed for generating radiology\nreports from 3D CT scans, particularly chest CTs. Extensive experiments on both\npublic and private datasets demonstrate that 3D-CT-GPT significantly\noutperforms existing methods in terms of report accuracy and quality. Although\ncurrent methods are few, including the partially open-source CT2Rep and the\nopen-source M3D, we ensured fair comparison through appropriate data conversion\nand evaluation methodologies. Experimental results indicate that 3D-CT-GPT\nenhances diagnostic accuracy and report coherence, establishing itself as a\nrobust solution for clinical radiology report generation. Future work will\nfocus on expanding the dataset and further optimizing the model to enhance its\nperformance and applicability.", "paper_summary_zh": "\u91ab\u7642\u5f71\u50cf\u5206\u6790\u5728\u73fe\u4ee3\u653e\u5c04\u8a3a\u65b7\u4e2d\u81f3\u95dc\u91cd\u8981\uff0c\u7279\u5225\u662f\u8003\u616e\u5230\u91ab\u5b78\u5f71\u50cf\u8cc7\u6599\u7684\u6307\u6578\u6210\u9577\u3002\u5c0d\u81ea\u52d5\u5316\u5831\u544a\u7522\u751f\u7cfb\u7d71\u7684\u9700\u6c42\u5df2\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u8feb\u5207\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u4f7f\u7528\u6a5f\u5668\u5b78\u7fd2\u548c\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b\u9032\u884c 2D \u91ab\u7642\u5f71\u50cf\uff0c\u4f46\u7531\u65bc\u8cc7\u6599\u7a00\u5c11\u548c\u8a08\u7b97\u8907\u96dc\u5ea6\uff0c3D \u91ab\u7642\u5f71\u50cf\u7684\u5831\u544a\u7522\u751f\u8f03\u5c11\u88ab\u63a2\u8a0e\u3002\u672c\u6587\u4ecb\u7d39 3D-CT-GPT\uff0c\u4e00\u7a2e\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u5f9e 3D CT \u6383\u63cf\uff08\u7279\u5225\u662f\u80f8\u90e8 CT\uff09\u7522\u751f\u653e\u5c04\u79d1\u5831\u544a\u7684\u57fa\u65bc\u8996\u89ba\u554f\u7b54 (VQA) \u7684\u91ab\u5b78\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u3002\u5728\u516c\u5171\u548c\u79c1\u4eba\u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0c3D-CT-GPT \u5728\u5831\u544a\u6e96\u78ba\u6027\u548c\u54c1\u8cea\u65b9\u9762\u986f\u8457\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002\u96d6\u7136\u76ee\u524d\u7684\u65b9\u6cd5\u5f88\u5c11\uff0c\u5305\u62ec\u90e8\u5206\u958b\u6e90\u7684 CT2Rep \u548c\u958b\u6e90\u7684 M3D\uff0c\u4f46\u6211\u5011\u900f\u904e\u9069\u7576\u7684\u8cc7\u6599\u8f49\u63db\u548c\u8a55\u4f30\u65b9\u6cd5\u78ba\u4fdd\u4e86\u516c\u5e73\u7684\u6bd4\u8f03\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c3D-CT-GPT \u589e\u5f37\u4e86\u8a3a\u65b7\u6e96\u78ba\u6027\u548c\u5831\u544a\u4e00\u81f4\u6027\uff0c\u78ba\u7acb\u4e86\u5176\u4f5c\u70ba\u81e8\u5e8a\u653e\u5c04\u79d1\u5831\u544a\u7522\u751f\u7684\u5f37\u5927\u89e3\u6c7a\u65b9\u6848\u3002\u672a\u4f86\u7684\u7814\u7a76\u5c07\u5c08\u6ce8\u65bc\u64f4\u5145\u8cc7\u6599\u96c6\u548c\u9032\u4e00\u6b65\u6700\u4f73\u5316\u6a21\u578b\uff0c\u4ee5\u589e\u5f37\u5176\u6548\u80fd\u548c\u9069\u7528\u6027\u3002", "author": "Hao Chen et.al.", "authors": "Hao Chen, Wei Zhao, Yingli Li, Tianyang Zhong, Yisong Wang, Youlan Shang, Lei Guo, Junwei Han, Tianming Liu, Jun Liu, Tuo Zhang", "id": "2409.19330v1", "paper_url": "http://arxiv.org/abs/2409.19330v1", "repo": "null"}}