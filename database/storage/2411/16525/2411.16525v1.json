{"2411.16525": {"publish_time": "2024-11-25", "title": "Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency", "paper_summary": "We investigate the statistical and computational limits of prompt tuning for\ntransformer-based foundation models. Our key contributions are prompt tuning on\n\\textit{single-head} transformers with only a \\textit{single} self-attention\nlayer: (i) is universal, and (ii) supports efficient (even almost-linear time)\nalgorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,\nwe prove that prompt tuning on such simplest possible transformers are\nuniversal approximators for sequence-to-sequence Lipschitz functions. In\naddition, we provide an exponential-in-$dL$ and -in-$(1/\\epsilon)$ lower bound\non the required soft-prompt tokens for prompt tuning to memorize any dataset\nwith 1-layer, 1-head transformers. Computationally, we identify a phase\ntransition in the efficiency of prompt tuning, determined by the norm of the\n\\textit{soft-prompt-induced} keys and queries, and provide an upper bound\ncriterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for\nprompt tuning exists under SETH. Within this criterion, we showcase our theory\nby proving the existence of almost-linear time prompt tuning inference\nalgorithms. These fundamental limits provide important necessary conditions for\ndesigning expressive and efficient prompt tuning methods for practitioners.", "paper_summary_zh": "\u6211\u5011\u63a2\u8a0e\u4e86\u57fa\u65bcTransformer\u7684\u57fa\u790e\u6a21\u578b\u63d0\u793a\u8abf\u6574\u7684\u7d71\u8a08\u548c\u8a08\u7b97\u9650\u5236\u3002\u6211\u5011\u7684\u95dc\u9375\u8ca2\u737b\u662f\u55ae\u982dTransformer\u4e0a\u7684\u63d0\u793a\u8abf\u6574\uff0c\u50c5\u6709\u4e00\u500b\u55ae\u4e00\u7684\u81ea\u6ce8\u610f\u529b\u5c64\uff1a(i) \u662f\u901a\u7528\u7684\uff0c\u4e26\u4e14 (ii) \u5728\u5f37\u6307\u6578\u6642\u9593\u5047\u8a2d (SETH) \u4e0b\u652f\u6301\u9ad8\u6548\uff08\u751a\u81f3\u5e7e\u4e4e\u7dda\u6027\u6642\u9593\uff09\u6f14\u7b97\u6cd5\u3002\u5728\u7d71\u8a08\u4e0a\uff0c\u6211\u5011\u8b49\u660e\u4e86\u5728\u9019\u6a23\u6700\u7c21\u55ae\u7684Transformer\u4e0a\u9032\u884c\u63d0\u793a\u8abf\u6574\u662f\u5e8f\u5217\u5230\u5e8f\u5217 Lipschitz \u51fd\u6578\u7684\u901a\u7528\u903c\u8fd1\u5668\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u5728 $dL$ \u548c -in-$(1/\\epsilon)$ \u4e2d\u5448\u6307\u6578\u7d1a\u7684\u8f03\u4f4e\u908a\u754c\uff0c\u7528\u65bc\u63d0\u793a\u8abf\u6574\u6240\u9700\u7684\u8edf\u63d0\u793a\u7b26\u865f\uff0c\u4ee5\u8a18\u61b6\u5177\u6709 1 \u5c64\u30011 \u982dTransformer\u7684\u4efb\u4f55\u8cc7\u6599\u96c6\u3002\u5728\u8a08\u7b97\u4e0a\uff0c\u6211\u5011\u5728\u63d0\u793a\u8abf\u6574\u7684\u6548\u7387\u4e2d\u767c\u73fe\u4e86\u4e00\u500b\u76f8\u8b8a\uff0c\u7531\u8edf\u63d0\u793a\u8a98\u5c0e\u7684\u9375\u548c\u67e5\u8a62\u7684\u7bc4\u6578\u6c7a\u5b9a\uff0c\u4e26\u63d0\u4f9b\u4e86\u4e00\u500b\u4e0a\u9650\u6e96\u5247\u3002\u5728\u6b64\u6e96\u5247\u4e4b\u5916\uff0c\u5728 SETH \u4e0b\u4e0d\u5b58\u5728\u4efb\u4f55\u6b21\u4e8c\u6b21\uff08\u9ad8\u6548\uff09\u7684\u63d0\u793a\u8abf\u6574\u6f14\u7b97\u6cd5\u3002\u5728\u6b64\u6e96\u5247\u5167\uff0c\u6211\u5011\u901a\u904e\u8b49\u660e\u5e7e\u4e4e\u7dda\u6027\u6642\u9593\u63d0\u793a\u8abf\u6574\u63a8\u8ad6\u6f14\u7b97\u6cd5\u7684\u5b58\u5728\u4f86\u5c55\u793a\u6211\u5011\u7684\u7406\u8ad6\u3002\u9019\u4e9b\u57fa\u672c\u9650\u5236\u70ba\u5be6\u52d9\u8005\u8a2d\u8a08\u5177\u6709\u8868\u9054\u529b\u548c\u9ad8\u6548\u7684\u63d0\u793a\u8abf\u6574\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5fc5\u8981\u689d\u4ef6\u3002", "author": "Jerry Yao-Chieh Hu et.al.", "authors": "Jerry Yao-Chieh Hu, Wei-Po Wang, Ammar Gilani, Chenyang Li, Zhao Song, Han Liu", "id": "2411.16525v1", "paper_url": "http://arxiv.org/abs/2411.16525v1", "repo": "null"}}