{"2411.11465": {"publish_time": "2024-11-18", "title": "Re-examining learning linear functions in context", "paper_summary": "In context learning (ICL) is an attractive method of solving a wide range of\nproblems. Inspired by Garg et al. (2022), we look closely at ICL in a variety\nof train and test settings for several transformer models of different sizes\ntrained from scratch. Our study complements prior work by pointing out several\nsystematic failures of these models to generalize to data not in the training\ndistribution, thereby showing some limitations of ICL. We find that models\nadopt a strategy for this task that is very different from standard solutions.", "paper_summary_zh": "\u60c5\u5883\u5b78\u7fd2 (ICL) \u662f\u4e00\u7a2e\u89e3\u6c7a\u5ee3\u6cdb\u554f\u984c\u7684\u8a98\u4eba\u65b9\u6cd5\u3002\u53d7 Garg \u7b49\u4eba (2022) \u7684\u555f\u767c\uff0c\u6211\u5011\u4ed4\u7d30\u7814\u7a76\u4e86 ICL \u5728\u5404\u7a2e\u8a13\u7df4\u548c\u6e2c\u8a66\u8a2d\u5b9a\u4e2d\uff0c\u91dd\u5c0d\u591a\u500b\u4e0d\u540c\u5927\u5c0f\u7684Transformer\u6a21\u578b\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u3002\u6211\u5011\u7684\u7814\u7a76\u88dc\u5145\u4e86\u5148\u524d\u7684\u7814\u7a76\uff0c\u6307\u51fa\u4e86\u9019\u4e9b\u6a21\u578b\u5728\u5c0d\u8a13\u7df4\u5206\u4f48\u4e2d\u6c92\u6709\u7684\u8cc7\u6599\u9032\u884c\u6cdb\u5316\u7684\u5e7e\u500b\u7cfb\u7d71\u6027\u5931\u6557\uff0c\u5f9e\u800c\u986f\u793a\u4e86 ICL \u7684\u4e00\u4e9b\u9650\u5236\u3002\u6211\u5011\u767c\u73fe\u6a21\u578b\u63a1\u7528\u4e86\u4e00\u7a2e\u8207\u6a19\u6e96\u89e3\u6c7a\u65b9\u6848\u622a\u7136\u4e0d\u540c\u7684\u7b56\u7565\u4f86\u57f7\u884c\u9019\u9805\u4efb\u52d9\u3002", "author": "Omar Naim et.al.", "authors": "Omar Naim, Guilhem Fouilh\u00e9, Nicholas Asher", "id": "2411.11465v1", "paper_url": "http://arxiv.org/abs/2411.11465v1", "repo": "null"}}