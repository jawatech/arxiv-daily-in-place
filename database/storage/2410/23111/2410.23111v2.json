{"2410.23111": {"publish_time": "2024-10-30", "title": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in Federated Fine-Tuning of Large Language Models", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore is a more effective\nalternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA\nacross both text and image modalities. While privacy remains paramount in FL\ndiscourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u9818\u57df\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u7279\u5225\u662f\u5728\u6587\u672c\u548c\u8996\u89ba\u8cc7\u6599\u7684\u4efb\u52d9\u6982\u62ec\u65b9\u9762\u3002\u96d6\u7136\u5fae\u8abf\u9019\u4e9b\u6a21\u578b\u53ef\u4ee5\u986f\u8457\u63d0\u5347\u5176\u5728\u7279\u5b9a\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684\u6548\u80fd\uff0c\u4f46\u9019\u901a\u5e38\u9700\u8981\u9ad8\u54c1\u8cea\u7684\u8cc7\u6599\uff0c\u800c\u9019\u4e9b\u8cc7\u6599\u53ef\u80fd\u7121\u6cd5\u56e0\u96b1\u79c1\u8003\u91cf\u800c\u5171\u7528\u3002\u806f\u5408\u5b78\u7fd2 (FL) \u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u53ef\u4ee5\u9032\u884c\u5354\u4f5c\u8a13\u7df4\uff0c\u800c\u7121\u9700\u76f4\u63a5\u5171\u7528\u8cc7\u6599\u3002\u7136\u800c\uff0c\u8a31\u591a\u91dd\u5c0d LLM \u5728 FL \u4e2d\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf\u7b56\u7565\uff0c\u7279\u5225\u662f\u90a3\u4e9b\u57fa\u65bc\u4f4e\u79e9\u9069\u61c9 (LoRA) \u7684\u7b56\u7565\uff0c\u90fd\u9762\u81e8\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6279\u5224\u6027\u5730\u5206\u6790\u4e86\u4f7f\u7528 LoRA \u7684\u71b1\u9580 FL \u67b6\u69cb\u7684\u6536\u6582\u6027\u548c\u6548\u80fd\u4fdd\u8b49\uff0c\u4e26\u5f37\u8abf\u5176\u6b21\u4f73\u6027\u8cea\uff0c\u539f\u56e0\u5728\u65bc\u4f4e\u79e9\u77e9\u9663\u7684\u53d7\u9650\u5b50\u7a7a\u9593\u5b78\u7fd2\u3002\u6b64\u9650\u5236\u963b\u7919\u4e86\u5728\u806f\u5408\u8a2d\u5b9a\u4e2d\u5c0d LLM \u9032\u884c\u6709\u6548\u7684\u5fae\u8abf\u3002\u900f\u904e\u56b4\u8b39\u7684\u5206\u6790\u548c\u7d93\u9a57\u8a55\u4f30\uff0c\u6211\u5011\u8b49\u660e\u76f4\u63a5\u6b0a\u91cd\u5e73\u5747\u512a\u65bc\u57fa\u65bc LoRA \u7684\u7b56\u7565\uff0c\u5f9e\u800c\u63d0\u5347\u5fae\u8abf\u6a21\u578b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u5168\u9762\u6bd4\u8f03\u63ed\u9732\u4e86 LoRA \u65b9\u6cd5\u7684\u4f4e\u6548\u7387\uff0c\u4e26\u5f37\u8abf\u4e86\u76f4\u63a5\u6b0a\u91cd\u805a\u5408\u7684\u512a\u9ede\u3002\u6211\u5011\u5c07\u5206\u6790\u5ef6\u4f38\u81f3\u4f4e\u79e9\u57fa\u65bc\u68af\u5ea6\u7684\u6700\u4f73\u5316\u5668\uff0c\u4f8b\u5982 GaLore\uff0c\u9019\u4e9b\u6700\u4f73\u5316\u5668\u7528\u65bc\u5c40\u90e8\u8a13\u7df4\u6b65\u9a5f\u3002\u6211\u5011\u7684\u767c\u73fe\u986f\u793a\uff0cGaLore \u662f\u4e00\u7a2e\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u6587\u672c\u548c\u5f71\u50cf\u6a21\u5f0f\u4e2d\u90fd\u512a\u65bc\u806f\u5408 LoRA \u65b9\u6cd5\uff0c\u4f8b\u5982 FlexLoRA \u548c FFA-LoRA\u3002\u96d6\u7136\u96b1\u79c1\u5728 FL \u8ad6\u8ff0\u4e2d\u4ecd\u7136\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u6211\u5011\u7684\u91cd\u9ede\u5728\u65bc\u8a55\u4f30\u806f\u5408\u5fae\u8abf\u6a21\u578b\u7684\u6548\u80fd\u7d50\u679c\uff0c\u4e26\u5f9e\u7406\u8ad6\u548c\u7d93\u9a57\u7684\u89d2\u5ea6\u8a55\u4f30\u5404\u7a2e FL \u67b6\u69cb\u3002\u6211\u5011\u7684\u767c\u73fe\u4e3b\u5f35\u91cd\u65b0\u8a55\u4f30\u5728 FL \u80cc\u666f\u4e2d\u5c0d LoRA \u7684\u4f9d\u8cf4\uff0c\u70ba\u66f4\u6709\u6548\u7684\u8a13\u7df4\u65b9\u6cd5\u92ea\u8def\u3002", "author": "Navyansh Mahla et.al.", "authors": "Navyansh Mahla, Ganesh Ramakrishnan", "id": "2410.23111v2", "paper_url": "http://arxiv.org/abs/2410.23111v2", "repo": "null"}}