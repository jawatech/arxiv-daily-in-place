{"2410.02451": {"publish_time": "2024-10-03", "title": "Strong Preferences Affect the Robustness of Value Alignment", "paper_summary": "Value alignment, which aims to ensure that large language models (LLMs) and\nother AI agents behave in accordance with human values, is critical for\nensuring safety and trustworthiness of these systems. A key component of value\nalignment is the modeling of human preferences as a representation of human\nvalues. In this paper, we investigate the robustness of value alignment by\nexamining the sensitivity of preference models. Specifically, we ask: how do\nchanges in the probabilities of some preferences affect the predictions of\nthese models for other preferences? To answer this question, we theoretically\nanalyze the robustness of widely used preference models by examining their\nsensitivities to minor changes in preferences they model. Our findings reveal\nthat, in the Bradley-Terry and the Placket-Luce model, the probability of a\npreference can change significantly as other preferences change, especially\nwhen these preferences are dominant (i.e., with probabilities near 0 or 1). We\nidentify specific conditions where this sensitivity becomes significant for\nthese models and discuss the practical implications for the robustness and\nsafety of value alignment in AI systems.", "paper_summary_zh": "\u50f9\u503c\u5c0d\u9f4a\u65e8\u5728\u78ba\u4fdd\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u5176\u4ed6 AI \u4ee3\u7406\u6839\u64da\u4eba\u985e\u50f9\u503c\u89c0\u884c\u4e8b\uff0c\u9019\u5c0d\u65bc\u78ba\u4fdd\u9019\u4e9b\u7cfb\u7d71\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u81f3\u95dc\u91cd\u8981\u3002\u50f9\u503c\u5c0d\u9f4a\u7684\u4e00\u500b\u95dc\u9375\u7d44\u6210\u90e8\u5206\u662f\u5c07\u4eba\u985e\u504f\u597d\u5efa\u6a21\u70ba\u4eba\u985e\u50f9\u503c\u89c0\u7684\u8868\u73fe\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u6aa2\u9a57\u504f\u597d\u6a21\u578b\u7684\u654f\u611f\u6027\u4f86\u63a2\u8a0e\u50f9\u503c\u5c0d\u9f4a\u7684\u7a69\u5065\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8a62\u554f\uff1a\u67d0\u4e9b\u504f\u597d\u6a5f\u7387\u7684\u8b8a\u5316\u5982\u4f55\u5f71\u97ff\u9019\u4e9b\u6a21\u578b\u5c0d\u5176\u4ed6\u504f\u597d\u7684\u9810\u6e2c\uff1f\u70ba\u4e86\u56de\u7b54\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u900f\u904e\u6aa2\u9a57\u5ee3\u6cdb\u4f7f\u7528\u7684\u504f\u597d\u6a21\u578b\u5c0d\u5176\u6240\u5efa\u6a21\u504f\u597d\u4e2d\u7684\u5fae\u5c0f\u8b8a\u5316\u7684\u654f\u611f\u6027\uff0c\u5f9e\u7406\u8ad6\u4e0a\u5206\u6790\u5176\u7a69\u5065\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u63ed\u793a\uff0c\u5728 Bradley-Terry \u548c Placket-Luce \u6a21\u578b\u4e2d\uff0c\u96a8\u8457\u5176\u4ed6\u504f\u597d\u7684\u6539\u8b8a\uff0c\u504f\u597d\u7684\u6a5f\u7387\u53ef\u80fd\u6703\u767c\u751f\u986f\u8457\u8b8a\u5316\uff0c\u7279\u5225\u662f\u5728\u9019\u4e9b\u504f\u597d\u4f54\u512a\u52e2\u6642\uff08\u5373\u6a5f\u7387\u63a5\u8fd1 0 \u6216 1\uff09\u3002\u6211\u5011\u627e\u51fa\u9019\u4e9b\u6a21\u578b\u7684\u654f\u611f\u6027\u5728\u54ea\u4e9b\u7279\u5b9a\u689d\u4ef6\u4e0b\u8b8a\u5f97\u986f\u8457\uff0c\u4e26\u8a0e\u8ad6\u5c0d AI \u7cfb\u7d71\u4e2d\u50f9\u503c\u5c0d\u9f4a\u7684\u7a69\u5065\u6027\u548c\u5b89\u5168\u6027\u9020\u6210\u7684\u5be6\u969b\u5f71\u97ff\u3002", "author": "Ziwei Xu et.al.", "authors": "Ziwei Xu, Mohan Kankanhalli", "id": "2410.02451v1", "paper_url": "http://arxiv.org/abs/2410.02451v1", "repo": "null"}}