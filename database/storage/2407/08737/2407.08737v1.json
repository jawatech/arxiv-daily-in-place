{"2407.08737": {"publish_time": "2024-07-11", "title": "Video Diffusion Alignment via Reward Gradients", "paper_summary": "We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.", "paper_summary_zh": "\u6211\u5011\u5728\u5efa\u7acb\u57fa\u790e\u5f71\u7247\u64f4\u6563\u6a21\u578b\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u9032\u5c55\u3002\u7531\u65bc\u9019\u4e9b\u6a21\u578b\u4f7f\u7528\u5927\u898f\u6a21\u7121\u76e3\u7763\u8cc7\u6599\u9032\u884c\u8a13\u7df4\uff0c\u56e0\u6b64\u5fc5\u9808\u5c07\u9019\u4e9b\u6a21\u578b\u8abf\u6574\u5230\u7279\u5b9a\u4e0b\u6e38\u4efb\u52d9\u3002\u900f\u904e\u76e3\u7763\u5fae\u8abf\u8abf\u6574\u9019\u4e9b\u6a21\u578b\u9700\u8981\u6536\u96c6\u5f71\u7247\u7684\u76ee\u6a19\u8cc7\u6599\u96c6\uff0c\u9019\u5177\u6709\u6311\u6230\u6027\u4e14\u7e41\u7463\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5229\u7528\u9810\u8a13\u7df4\u7684\u734e\u52f5\u6a21\u578b\uff0c\u9019\u4e9b\u6a21\u578b\u662f\u900f\u904e\u504f\u597d\u5728\u5f37\u5927\u7684\u8996\u89ba\u8fa8\u5225\u6a21\u578b\u4e0a\u5b78\u7fd2\uff0c\u4ee5\u8abf\u6574\u5f71\u7247\u64f4\u6563\u6a21\u578b\u3002\u9019\u4e9b\u6a21\u578b\u5305\u542b\u95dc\u65bc\u751f\u6210 RGB \u50cf\u7d20\u7684\u5bc6\u96c6\u68af\u5ea6\u8cc7\u8a0a\uff0c\u9019\u5c0d\u65bc\u5728\u8907\u96dc\u7684\u641c\u5c0b\u7a7a\u9593\uff08\u4f8b\u5982\u5f71\u7247\uff09\u4e2d\u9032\u884c\u6709\u6548\u7387\u7684\u5b78\u7fd2\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u5c55\u793a\u4e86\u5f9e\u9019\u4e9b\u734e\u52f5\u6a21\u578b\u53cd\u5411\u50b3\u64ad\u68af\u5ea6\u5230\u5f71\u7247\u64f4\u6563\u6a21\u578b\u53ef\u4ee5\u5141\u8a31\u8a08\u7b97\u548c\u6a23\u672c\u6709\u6548\u5730\u5c0d\u9f4a\u5f71\u7247\u64f4\u6563\u6a21\u578b\u3002\u6211\u5011\u5c55\u793a\u4e86\u5404\u7a2e\u734e\u52f5\u6a21\u578b\u548c\u5f71\u7247\u64f4\u6563\u6a21\u578b\u7684\u7d50\u679c\uff0c\u8b49\u660e\u6211\u5011\u7684\u505a\u6cd5\u5728\u734e\u52f5\u67e5\u8a62\u548c\u8a08\u7b97\u65b9\u9762\u53ef\u4ee5\u6bd4\u5148\u524d\u7684\u7121\u68af\u5ea6\u65b9\u6cd5\u5b78\u7fd2\u66f4\u6709\u6548\u7387\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3001\u6a21\u578b\u6b0a\u91cd\u548c\u66f4\u591a\u8996\u89ba\u5316\u8cc7\u8a0a\u53ef\u5728 https://vader-vid.github.io/ \u53d6\u5f97\u3002", "author": "Mihir Prabhudesai et.al.", "authors": "Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, Deepak Pathak", "id": "2407.08737v1", "paper_url": "http://arxiv.org/abs/2407.08737v1", "repo": "https://github.com/mihirp1998/vader"}}