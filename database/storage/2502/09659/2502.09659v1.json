{"2502.09659": {"publish_time": "2025-02-12", "title": "Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large Language Models", "paper_summary": "Motivation: An adjuvant is a chemical incorporated into vaccines that\nenhances their efficacy by improving the immune response. Identifying adjuvant\nnames from cancer vaccine studies is essential for furthering research and\nenhancing immunotherapies. However, the manual curation from the constantly\nexpanding biomedical literature poses significant challenges. This study\nexplores the automated recognition of vaccine adjuvant names using Large\nLanguage Models (LLMs), specifically Generative Pretrained Transformers (GPT)\nand Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97\nclinical trial records from AdjuvareDB and 290 abstracts annotated with the\nVaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in\nzero-shot and few-shot learning paradigms with up to four examples per prompt.\nPrompts explicitly targeted adjuvant names, testing the impact of contextual\ninformation such as substances or interventions. Outputs underwent automated\nand manual validation for accuracy and consistency. Results: GPT-4o attained\n100% Precision across all situations while exhibiting notable improve in Recall\nand F1-scores, particularly with incorporating interventions. On the VAC\ndataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,\nsurpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o\nreached an F1-score of 81.67% for three-shot prompting with interventions,\nsurpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings\ndemonstrate that LLMs excel at identifying adjuvant names, including rare\nvariations of naming representation. This study emphasizes the capability of\nLLMs to enhance cancer vaccine development by efficiently extracting insights.\nFuture work aims to broaden the framework to encompass various biomedical\nliterature and enhance model generalizability across various vaccines and\nadjuvants.", "paper_summary_zh": "<paragraph>\u52d5\u6a5f\uff1a\u4f50\u5291\u662f\u4e00\u7a2e\u52a0\u5165\u75ab\u82d7\u7684\u5316\u5b78\u7269\u8cea\uff0c\u80fd\u85c9\u7531\u6539\u5584\u514d\u75ab\u53cd\u61c9\u4f86\u63d0\u5347\u75ab\u82d7\u7684\u6548\u529b\u3002\u5f9e\u764c\u75c7\u75ab\u82d7\u7814\u7a76\u4e2d\u627e\u51fa\u4f50\u5291\u540d\u7a31\u5c0d\u65bc\u63a8\u9032\u7814\u7a76\u548c\u6539\u5584\u514d\u75ab\u7642\u6cd5\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f9e\u4e0d\u65b7\u64f4\u5c55\u7684\u751f\u7269\u91ab\u5b78\u6587\u737b\u4e2d\u624b\u52d5\u6574\u7406\u6703\u9020\u6210\u91cd\u5927\u6311\u6230\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u7279\u5225\u662f\u751f\u6210\u5f0f\u9810\u8a13\u7df4Transformer (GPT) \u548c\u5927\u578b\u8a9e\u8a00\u6a21\u578b Meta AI (Llama) \u4f86\u81ea\u52d5\u8fa8\u8b58\u75ab\u82d7\u4f50\u5291\u540d\u7a31\u3002\u65b9\u6cd5\uff1a\u6211\u5011\u4f7f\u7528\u5169\u500b\u8cc7\u6599\u96c6\uff1a\u4f86\u81ea AdjuvareDB \u7684 97 \u4efd\u81e8\u5e8a\u8a66\u9a57\u8a18\u9304\u548c 290 \u7bc7\u6a19\u8a3b\u4e86\u75ab\u82d7\u4f50\u5291\u5f59\u7de8 (VAC) \u7684\u6458\u8981\u3002GPT-4o \u548c Llama 3.2 \u88ab\u7528\u65bc\u96f6\u6b21\u5b78\u7fd2\u548c\u5c11\u91cf\u5b78\u7fd2\u7bc4\u4f8b\uff0c\u6bcf\u500b\u63d0\u793a\u6700\u591a\u6709\u56db\u500b\u7bc4\u4f8b\u3002\u63d0\u793a\u660e\u78ba\u9396\u5b9a\u4f50\u5291\u540d\u7a31\uff0c\u6e2c\u8a66\u7269\u8cea\u6216\u4ecb\u5165\u63aa\u65bd\u7b49\u80cc\u666f\u8cc7\u8a0a\u7684\u5f71\u97ff\u3002\u8f38\u51fa\u7d93\u904e\u81ea\u52d5\u548c\u624b\u52d5\u9a57\u8b49\uff0c\u4ee5\u78ba\u4fdd\u6e96\u78ba\u6027\u548c\u4e00\u81f4\u6027\u3002\u7d50\u679c\uff1aGPT-4o \u5728\u6240\u6709\u60c5\u6cc1\u4e0b\u90fd\u9054\u5230 100% \u7684\u6e96\u78ba\u7387\uff0c\u540c\u6642\u5728\u53ec\u56de\u7387\u548c F1 \u5206\u6578\u4e0a\u8868\u73fe\u51fa\u986f\u8457\u7684\u9032\u6b65\uff0c\u7279\u5225\u662f\u5728\u7d0d\u5165\u4ecb\u5165\u63aa\u65bd\u7684\u60c5\u6cc1\u4e0b\u3002\u5728 VAC \u8cc7\u6599\u96c6\u4e0a\uff0cGPT-4o \u5728\u6709\u4ecb\u5165\u63aa\u65bd\u7684\u60c5\u6cc1\u4e0b\u9054\u5230 77.32% \u7684\u6700\u9ad8 F1 \u5206\u6578\uff0c\u6bd4 Llama-3.2-3B \u9ad8\u51fa\u7d04 2%\u3002\u5728 AdjuvareDB \u8cc7\u6599\u96c6\u4e0a\uff0cGPT-4o \u5728\u6709\u4ecb\u5165\u63aa\u65bd\u7684\u4e09\u6b21\u63d0\u793a\u4e2d\u9054\u5230 81.67% \u7684 F1 \u5206\u6578\uff0c\u8d85\u904e Llama-3.2-3 B \u7684\u6700\u9ad8 F1 \u5206\u6578 65.62%\u3002\u7d50\u8ad6\uff1a\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0cLLM \u5728\u8fa8\u8b58\u4f50\u5291\u540d\u7a31\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u5305\u62ec\u547d\u540d\u8868\u793a\u7684\u7f55\u898b\u8b8a\u7570\u3002\u672c\u7814\u7a76\u5f37\u8abf\u4e86 LLM \u5728\u6709\u6548\u63d0\u53d6\u898b\u89e3\u65b9\u9762\u589e\u5f37\u764c\u75c7\u75ab\u82d7\u958b\u767c\u7684\u80fd\u529b\u3002\u672a\u4f86\u7684\u7814\u7a76\u5de5\u4f5c\u65e8\u5728\u64f4\u5927\u67b6\u69cb\uff0c\u6db5\u84cb\u5404\u7a2e\u751f\u7269\u91ab\u5b78\u6587\u737b\uff0c\u4e26\u589e\u5f37\u6a21\u578b\u5728\u5404\u7a2e\u75ab\u82d7\u548c\u4f50\u5291\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002</paragraph>", "author": "Hasin Rehana et.al.", "authors": "Hasin Rehana, Jie Zheng, Leo Yeh, Benu Bansal, Nur Bengisu \u00c7am, Christianah Jemiyo, Brett McGregor, Arzucan \u00d6zg\u00fcr, Yongqun He, Junguk Hur", "id": "2502.09659v1", "paper_url": "http://arxiv.org/abs/2502.09659v1", "repo": "null"}}