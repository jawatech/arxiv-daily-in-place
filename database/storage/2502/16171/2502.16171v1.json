{"2502.16171": {"publish_time": "2025-02-22", "title": "EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering", "paper_summary": "Due to the remarkable reasoning ability, Large language models (LLMs) have\ndemonstrated impressive performance in knowledge graph question answering\n(KGQA) tasks, which find answers to natural language questions over knowledge\ngraphs (KGs). To alleviate the hallucinations and lack of knowledge issues of\nLLMs, existing methods often retrieve the question-related information from KGs\nto enrich the input context. However, most methods focus on retrieving the\nrelevant information while ignoring the importance of different types of\nknowledge in reasoning, which degrades their performance. To this end, this\npaper reformulates the KGQA problem as a graphical model and proposes a\nthree-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM)\nfor KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a\nsubgraph related to the question from the original knowledge graph. In the\nsecond stage, EPERM filters out the evidence paths that faithfully support the\nreasoning of the questions, and score their importance in reasoning. Finally,\nEPERM uses the weighted evidence paths to reason the final answer. Since\nconsidering the importance of different structural information in KGs for\nreasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks.\nExtensive experiments on benchmark datasets demonstrate that EPERM achieves\nsuperior performances in KGQA tasks.", "paper_summary_zh": "\u7531\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5177\u6709\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u5728\u77e5\u8b58\u5716\u8b5c\u554f\u7b54 (KGQA) \u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u53ef\u4ee5\u5728\u77e5\u8b58\u5716\u8b5c (KG) \u4e2d\u627e\u5230\u81ea\u7136\u8a9e\u8a00\u554f\u984c\u7684\u7b54\u6848\u3002\u70ba\u4e86\u7de9\u89e3 LLM \u7684\u5e7b\u89ba\u548c\u7f3a\u4e4f\u77e5\u8b58\u554f\u984c\uff0c\u73fe\u6709\u65b9\u6cd5\u901a\u5e38\u5f9e KG \u4e2d\u6aa2\u7d22\u8207\u554f\u984c\u76f8\u95dc\u7684\u8cc7\u8a0a\uff0c\u4ee5\u8c50\u5bcc\u8f38\u5165\u5167\u5bb9\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u65b9\u6cd5\u5c08\u6ce8\u65bc\u6aa2\u7d22\u76f8\u95dc\u8cc7\u8a0a\uff0c\u800c\u5ffd\u7565\u4e86\u4e0d\u540c\u985e\u578b\u77e5\u8b58\u5728\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u9019\u6703\u964d\u4f4e\u5176\u6548\u80fd\u3002\u70ba\u6b64\uff0c\u672c\u6587\u5c07 KGQA \u554f\u984c\u91cd\u65b0\u8868\u8ff0\u70ba\u5716\u5f62\u6a21\u578b\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u540d\u70ba\u8b49\u64da\u8def\u5f91\u589e\u5f37\u63a8\u7406\u6a21\u578b (EPERM) \u7684\u4e09\u968e\u6bb5\u67b6\u69cb\uff0c\u7528\u65bc KGQA\u3002\u5728\u7b2c\u4e00\u968e\u6bb5\uff0cEPERM \u4f7f\u7528\u5fae\u8abf\u5f8c\u7684 LLM \u5f9e\u539f\u59cb\u77e5\u8b58\u5716\u8b5c\u4e2d\u6aa2\u7d22\u8207\u554f\u984c\u76f8\u95dc\u7684\u5b50\u5716\u3002\u5728\u7b2c\u4e8c\u968e\u6bb5\uff0cEPERM \u7be9\u9078\u51fa\u5fe0\u5be6\u652f\u6301\u554f\u984c\u63a8\u7406\u7684\u8b49\u64da\u8def\u5f91\uff0c\u4e26\u8a55\u5206\u5176\u5728\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\u3002\u6700\u5f8c\uff0cEPERM \u4f7f\u7528\u52a0\u6b0a\u8b49\u64da\u8def\u5f91\u63a8\u7406\u51fa\u6700\u7d42\u7b54\u6848\u3002\u7531\u65bc\u8003\u616e\u4e86 KG \u4e2d\u4e0d\u540c\u7d50\u69cb\u8cc7\u8a0a\u5728\u63a8\u7406\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64 EPERM \u53ef\u4ee5\u63d0\u5347 LLM \u5728 KGQA \u4efb\u52d9\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u5728\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cEPERM \u5728 KGQA \u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86\u512a\u7570\u7684\u6548\u80fd\u3002", "author": "Xiao Long et.al.", "authors": "Xiao Long, Liansheng Zhuang, Aodi Li, Minghong Yao, Shafei Wang", "id": "2502.16171v1", "paper_url": "http://arxiv.org/abs/2502.16171v1", "repo": "null"}}