{"2409.17656": {"publish_time": "2024-09-26", "title": "Prototype based Masked Audio Model for Self-Supervised Learning of Sound Event Detection", "paper_summary": "A significant challenge in sound event detection (SED) is the effective\nutilization of unlabeled data, given the limited availability of labeled data\ndue to high annotation costs. Semi-supervised algorithms rely on labeled data\nto learn from unlabeled data, and the performance is constrained by the quality\nand size of the former. In this paper, we introduce the Prototype based Masked\nAudio Model~(PMAM) algorithm for self-supervised representation learning in\nSED, to better exploit unlabeled data. Specifically, semantically rich\nframe-level pseudo labels are constructed from a Gaussian mixture model (GMM)\nbased prototypical distribution modeling. These pseudo labels supervise the\nlearning of a Transformer-based masked audio model, in which binary\ncross-entropy loss is employed instead of the widely used InfoNCE loss, to\nprovide independent loss contributions from different prototypes, which is\nimportant in real scenarios in which multiple labels may apply to unsupervised\ndata frames. A final stage of fine-tuning with just a small amount of labeled\ndata yields a very high performing SED model. On like-for-like tests using the\nDESED task, our method achieves a PSDS1 score of 62.5\\%, surpassing current\nstate-of-the-art models and demonstrating the superiority of the proposed\ntechnique.", "paper_summary_zh": "\u8072\u97f3\u4e8b\u4ef6\u5075\u6e2c (SED) \u4e2d\u7684\u4e00\u9805\u91cd\u5927\u6311\u6230\u662f\u6709\u6548\u5229\u7528\u672a\u6a19\u8a3b\u8cc7\u6599\uff0c\u56e0\u70ba\u6a19\u8a3b\u8cc7\u6599\u7684\u53d6\u5f97\u53d7\u5230\u6a19\u8a3b\u6210\u672c\u9ad8\u6602\u7684\u9650\u5236\u3002\u534a\u76e3\u7763\u6f14\u7b97\u6cd5\u4f9d\u8cf4\u6a19\u8a3b\u8cc7\u6599\u5f9e\u672a\u6a19\u8a3b\u8cc7\u6599\u4e2d\u5b78\u7fd2\uff0c\u800c\u5176\u6548\u80fd\u53d7\u5230\u524d\u8005\u7684\u54c1\u8cea\u8207\u898f\u6a21\u6240\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u57fa\u65bc\u539f\u578b\u906e\u7f69\u97f3\u8a0a\u6a21\u578b (PMAM) \u7684\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc SED \u4e2d\u7684\u81ea\u6211\u76e3\u7763\u8868\u5fb5\u5b78\u7fd2\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u672a\u6a19\u8a3b\u8cc7\u6599\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u8a9e\u7fa9\u8c50\u5bcc\u7684\u5e40\u7d1a\u507d\u6a19\u7c64\u662f\u7531\u57fa\u65bc\u9ad8\u65af\u6df7\u5408\u6a21\u578b (GMM) \u7684\u539f\u578b\u5206\u4f48\u5efa\u6a21\u6240\u5efa\u69cb\u7684\u3002\u9019\u4e9b\u507d\u6a19\u7c64\u76e3\u7763\u57fa\u65bc Transformer \u7684\u906e\u7f69\u97f3\u8a0a\u6a21\u578b\u7684\u5b78\u7fd2\uff0c\u5176\u4e2d\u63a1\u7528\u4e8c\u5143\u4ea4\u53c9\u71b5\u640d\u5931\uff0c\u800c\u975e\u5ee3\u6cdb\u4f7f\u7528\u7684 InfoNCE \u640d\u5931\uff0c\u4ee5\u63d0\u4f9b\u4f86\u81ea\u4e0d\u540c\u539f\u578b\u7684\u7368\u7acb\u640d\u5931\u8ca2\u737b\uff0c\u9019\u5728\u591a\u500b\u6a19\u7c64\u53ef\u80fd\u5957\u7528\u65bc\u672a\u76e3\u7763\u8cc7\u6599\u5e40\u7684\u5be6\u969b\u5834\u666f\u4e2d\u975e\u5e38\u91cd\u8981\u3002\u6700\u5f8c\u4e00\u500b\u5fae\u8abf\u968e\u6bb5\u53ea\u4f7f\u7528\u5c11\u91cf\u6a19\u8a3b\u8cc7\u6599\uff0c\u5c31\u80fd\u7522\u751f\u6548\u80fd\u975e\u5e38\u9ad8\u7684 SED \u6a21\u578b\u3002\u5728\u4f7f\u7528 DESED \u4efb\u52d9\u9032\u884c\u7684\u76f8\u540c\u6e2c\u8a66\u4e2d\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 PSDS1 \u5f97\u5206\u9054\u5230 62.5%\uff0c\u8d85\u8d8a\u4e86\u73fe\u884c\u7684\u6700\u5148\u9032\u6a21\u578b\uff0c\u4e26\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u6280\u8853\u7684\u512a\u8d8a\u6027\u3002", "author": "Pengfei Cai et.al.", "authors": "Pengfei Cai, Yan Song, Nan Jiang, Qing Gu, Ian McLoughlin", "id": "2409.17656v1", "paper_url": "http://arxiv.org/abs/2409.17656v1", "repo": "https://github.com/cai525/transformer4sed"}}