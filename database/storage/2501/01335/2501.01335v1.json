{"2501.01335": {"publish_time": "2025-01-02", "title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models", "paper_summary": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods.", "paper_summary_zh": "<paragraph>\u8a31\u591a\u7814\u7a76\u63a2\u8a0e\u4e86\u8d8a\u7344\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4ee5\u7522\u751f\u6709\u5bb3\u5167\u5bb9\u7684\u65b9\u6cd5\u3002\u901a\u5e38\uff0c\u9019\u4e9b\u65b9\u6cd5\u4f7f\u7528\u60e1\u610f\u63d0\u793a\u7684\u8cc7\u6599\u96c6\u9032\u884c\u8a55\u4f30\uff0c\u9019\u4e9b\u63d0\u793a\u65e8\u5728\u7e5e\u904e LLM \u63d0\u4f9b\u8005\u5efa\u7acb\u7684\u5b89\u5168\u653f\u7b56\u3002\u7136\u800c\uff0c\u73fe\u6709\u8cc7\u6599\u96c6\u7684\u5ee3\u6cdb\u7bc4\u570d\u548c\u958b\u653e\u5f0f\u6027\u8cea\u901a\u5e38\u6703\u4f7f\u8d8a\u7344\u6709\u6548\u6027\u7684\u8a55\u4f30\u8b8a\u5f97\u8907\u96dc\uff0c\u7279\u5225\u662f\u5728\u7279\u5b9a\u9818\u57df\uff0c\u5c24\u5176\u662f\u7db2\u8def\u5b89\u5168\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e26\u516c\u958b\u767c\u5e03 CySecBench\uff0c\u9019\u662f\u4e00\u500b\u5168\u9762\u7684\u8cc7\u6599\u96c6\uff0c\u5305\u542b 12662 \u500b\u63d0\u793a\uff0c\u5c08\u9580\u7528\u65bc\u8a55\u4f30\u7db2\u8def\u5b89\u5168\u9818\u57df\u7684\u8d8a\u7344\u6280\u8853\u3002\u8a72\u8cc7\u6599\u96c6\u7d44\u7e54\u6210 10 \u500b\u4e0d\u540c\u7684\u653b\u64ca\u985e\u578b\u985e\u5225\uff0c\u63a1\u7528\u5c01\u9589\u5f0f\u63d0\u793a\uff0c\u4ee5\u4fbf\u5c0d\u8d8a\u7344\u5617\u8a66\u9032\u884c\u66f4\u4e00\u81f4\u4e14\u6e96\u78ba\u7684\u8a55\u4f30\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a73\u7d30\u8aaa\u660e\u4e86\u6211\u5011\u7684\u8cc7\u6599\u96c6\u751f\u6210\u548c\u904e\u6ffe\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u53ef\u4ee5\u8abf\u6574\u4ee5\u5728\u5176\u4ed6\u9818\u57df\u5efa\u7acb\u985e\u4f3c\u7684\u8cc7\u6599\u96c6\u3002\u70ba\u4e86\u5c55\u793a CySecBench \u7684\u6548\u7528\uff0c\u6211\u5011\u63d0\u51fa\u4e26\u8a55\u4f30\u4e86\u4e00\u7a2e\u57fa\u65bc\u63d0\u793a\u6df7\u6dc6\u7684\u8d8a\u7344\u65b9\u6cd5\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u6b64\u65b9\u6cd5\u6210\u529f\u5730\u5f9e\u5546\u696d\u9ed1\u76d2 LLM \u7372\u53d6\u6709\u5bb3\u5167\u5bb9\uff0c\u5728 ChatGPT \u4e2d\u9054\u5230 65% \u7684\u6210\u529f\u7387 (SR)\uff0c\u5728 Gemini \u4e2d\u9054\u5230 88%\uff1b\u76f8\u6bd4\u4e4b\u4e0b\uff0cClaude \u4ee5 17% \u7684\u8d8a\u7344 SR \u8868\u73fe\u51fa\u66f4\u5927\u7684\u97cc\u6027\u3002\u8207\u73fe\u6709\u7684\u57fa\u6e96\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u986f\u793a\u51fa\u512a\u7570\u7684\u6027\u80fd\uff0c\u7a81\u986f\u4e86\u7279\u5b9a\u9818\u57df\u8a55\u4f30\u8cc7\u6599\u96c6\u5c0d\u65bc\u8a55\u4f30 LLM \u5b89\u5168\u63aa\u65bd\u7684\u50f9\u503c\u3002\u6b64\u5916\uff0c\u7576\u4f7f\u7528\u4f86\u81ea\u5ee3\u6cdb\u4f7f\u7528\u8cc7\u6599\u96c6 (\u5373 AdvBench) \u7684\u63d0\u793a\u9032\u884c\u8a55\u4f30\u6642\uff0c\u5b83\u9054\u5230\u4e86 78.5% \u7684 SR\uff0c\u9ad8\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002</paragraph>", "author": "Johan Wahr\u00e9us et.al.", "authors": "Johan Wahr\u00e9us, Ahmed Mohamed Hussain, Panos Papadimitratos", "id": "2501.01335v1", "paper_url": "http://arxiv.org/abs/2501.01335v1", "repo": "null"}}