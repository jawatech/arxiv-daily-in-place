{"2406.14909": {"publish_time": "2024-06-21", "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression", "paper_summary": "Sparse attention can effectively mitigate the significant memory and\nthroughput demands of Large Language Models (LLMs) in long contexts. Existing\nmethods typically employ a uniform sparse attention mask, applying the same\nsparse pattern across different attention heads and input lengths. However,\nthis uniform approach fails to capture the diverse attention patterns inherent\nin LLMs, ignoring their distinct accuracy-latency trade-offs. To address this\nchallenge, we propose the Mixture of Attention (MoA), which automatically\ntailors distinct sparse attention configurations to different heads and layers.\nMoA constructs and navigates a search space of various attention patterns and\ntheir scaling rules relative to input sequence lengths. It profiles the model,\nevaluates potential configurations, and pinpoints the optimal sparse attention\ncompression plan. MoA adapts to varying input sizes, revealing that some\nattention heads expand their focus to accommodate longer sequences, while other\nheads consistently concentrate on fixed-length local contexts. Experiments show\nthat MoA increases the effective context length by $3.9\\times$ with the same\naverage attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the\nuniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models.\nMoreover, MoA narrows the capability gaps between sparse and dense models,\nreducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$\nacross two long-context understanding benchmarks. MoA achieves a\n$1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7\n\\times$ for 7B and 13B dense models on a single GPU, with minimal impact on\nperformance.", "paper_summary_zh": "<paragraph>\u7a00\u758f\u6ce8\u610f\u529b\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u957f\u6587\u672c\u4e2d\u5bf9\u5185\u5b58\u548c\u541e\u5410\u91cf\u7684\u5de8\u5927\u9700\u6c42\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u7edf\u4e00\u7684\u7a00\u758f\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u5bf9\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u5934\u548c\u8f93\u5165\u957f\u5ea6\u5e94\u7528\u76f8\u540c\u7684\u7a00\u758f\u6a21\u5f0f\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u5230 LLM \u4e2d\u56fa\u6709\u7684\u591a\u6837\u5316\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5ffd\u89c6\u4e86\u5b83\u4eec\u4e0d\u540c\u7684\u51c6\u786e\u6027-\u5ef6\u8fdf\u6743\u8861\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6ce8\u610f\u529b\u6df7\u5408 (MoA)\uff0c\u5b83\u53ef\u4ee5\u81ea\u52a8\u4e3a\u4e0d\u540c\u7684\u5934\u548c\u5c42\u5b9a\u5236\u4e0d\u540c\u7684\u7a00\u758f\u6ce8\u610f\u529b\u914d\u7f6e\u3002MoA \u6784\u5efa\u5e76\u904d\u5386\u4e86\u5404\u79cd\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4ee5\u53ca\u5b83\u4eec\u76f8\u5bf9\u4e8e\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u7684\u7f29\u653e\u89c4\u5219\u3002\u5b83\u5bf9\u6a21\u578b\u8fdb\u884c\u5206\u6790\uff0c\u8bc4\u4f30\u6f5c\u5728\u914d\u7f6e\uff0c\u5e76\u627e\u51fa\u6700\u4f73\u7684\u7a00\u758f\u6ce8\u610f\u529b\u538b\u7f29\u8ba1\u5212\u3002MoA \u9002\u5e94\u4e0d\u540c\u7684\u8f93\u5165\u5927\u5c0f\uff0c\u63ed\u793a\u4e86\u4e00\u4e9b\u6ce8\u610f\u529b\u5934\u4f1a\u6269\u5c55\u5b83\u4eec\u7684\u7126\u70b9\u4ee5\u9002\u5e94\u66f4\u957f\u7684\u5e8f\u5217\uff0c\u800c\u5176\u4ed6\u6ce8\u610f\u529b\u5934\u5219\u59cb\u7ec8\u96c6\u4e2d\u5728\u56fa\u5b9a\u957f\u5ea6\u7684\u5c40\u90e8\u4e0a\u4e0b\u6587\u4e2d\u3002\u5b9e\u9a8c\u8868\u660e\uff0cMoA \u5c06\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u4e86 $3.9\\times$\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u5e73\u5747\u6ce8\u610f\u529b\u8de8\u5ea6\uff0c\u5728 Vicuna-7B\u3001Vicuna-13B \u548c Llama3-8B \u6a21\u578b\u4e0a\uff0c\u68c0\u7d22\u51c6\u786e\u5ea6\u6bd4\u7edf\u4e00\u6ce8\u610f\u529b\u57fa\u7ebf\u63d0\u9ad8\u4e86 $1.5-7.1\\times$\u3002\u6b64\u5916\uff0cMoA \u7f29\u5c0f\u4e86\u7a00\u758f\u6a21\u578b\u548c\u7a20\u5bc6\u6a21\u578b\u4e4b\u95f4\u7684\u80fd\u529b\u5dee\u8ddd\uff0c\u5c06\u4e24\u4e2a\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u57fa\u51c6\u7684\u6700\u5927\u76f8\u5bf9\u6027\u80fd\u4e0b\u964d\u4ece $9\\%-36\\%$ \u964d\u4f4e\u5230 $5\\%$ \u4ee5\u5185\u3002MoA \u5728\u5355\u4e2a GPU \u4e0a\u4e3a 7B \u548c 13B \u7a20\u5bc6\u6a21\u578b\u5b9e\u73b0\u4e86 $1.2-1.4\\times$ \u7684 GPU \u5185\u5b58\u51cf\u5c11\uff0c\u5e76\u5c06\u89e3\u7801\u541e\u5410\u91cf\u63d0\u9ad8\u4e86 $5.5-6.7\\times$\uff0c\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u6700\u5c0f\u3002</paragraph>", "author": "Tianyu Fu et.al.", "authors": "Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang", "id": "2406.14909v1", "paper_url": "http://arxiv.org/abs/2406.14909v1", "repo": "https://github.com/thu-nics/moa"}}