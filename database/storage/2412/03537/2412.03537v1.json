{"2412.03537": {"publish_time": "2024-12-04", "title": "Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models", "paper_summary": "Large language models (LLMs) are increasingly being adapted to achieve\ntask-specificity for deployment in real-world decision systems. Several\nprevious works have investigated the bias transfer hypothesis (BTH) by studying\nthe effect of the fine-tuning adaptation strategy on model fairness to find\nthat fairness in pre-trained masked language models have limited effect on the\nfairness of models when adapted using fine-tuning. In this work, we expand the\nstudy of BTH to causal models under prompt adaptations, as prompting is an\naccessible, and compute-efficient way to deploy models in real-world systems.\nIn contrast to previous works, we establish that intrinsic biases in\npre-trained Mistral, Falcon and Llama models are strongly correlated (rho >=\n0.94) with biases when the same models are zero- and few-shot prompted, using a\npronoun co-reference resolution task. Further, we find that bias transfer\nremains strongly correlated even when LLMs are specifically prompted to exhibit\nfair or biased behavior (rho >= 0.92), and few-shot length and stereotypical\ncomposition are varied (rho >= 0.97). Our findings highlight the importance of\nensuring fairness in pre-trained LLMs, especially when they are later used to\nperform downstream tasks via prompt adaptation.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u8c03\u6574\u4e3a\u5b9e\u73b0\u4efb\u52a1\u7279\u5f02\u6027\uff0c\u4ee5\u4fbf\u90e8\u7f72\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u51b3\u7b56\u7cfb\u7edf\u4e2d\u3002\u4e00\u4e9b\u4ee5\u524d\u7684\u4f5c\u54c1\u901a\u8fc7\u7814\u7a76\u5fae\u8c03\u8c03\u6574\u7b56\u7565\u5bf9\u6a21\u578b\u516c\u5e73\u6027\u7684\u5f71\u54cd\u6765\u8c03\u67e5\u504f\u5dee\u8f6c\u79fb\u5047\u8bbe (BTH)\uff0c\u53d1\u73b0\u9884\u5148\u8bad\u7ec3\u7684\u63a9\u853d\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u516c\u5e73\u6027\u5bf9\u4f7f\u7528\u5fae\u8c03\u8c03\u6574\u540e\u7684\u6a21\u578b\u516c\u5e73\u6027\u5f71\u54cd\u6709\u9650\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06 BTH \u7684\u7814\u7a76\u6269\u5c55\u5230\u63d0\u793a\u8c03\u6574\u4e0b\u7684\u56e0\u679c\u6a21\u578b\uff0c\u56e0\u4e3a\u63d0\u793a\u662f\u4e00\u79cd\u53ef\u8bbf\u95ee\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u6a21\u578b\u90e8\u7f72\u5230\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\u3002\u4e0e\u4ee5\u524d\u7684\u4f5c\u54c1\u76f8\u53cd\uff0c\u6211\u4eec\u786e\u5b9a\u9884\u5148\u8bad\u7ec3\u7684 Mistral\u3001Falcon \u548c Llama \u6a21\u578b\u4e2d\u7684\u5185\u5728\u504f\u5dee\u4e0e\u4f7f\u7528\u4ee3\u8bcd\u5171\u6307\u89e3\u6790\u4efb\u52a1\u5bf9\u76f8\u540c\u6a21\u578b\u8fdb\u884c\u96f6\u6b21\u548c\u5c11\u6b21\u63d0\u793a\u65f6\u7684\u504f\u5dee\u5bc6\u5207\u76f8\u5173\uff08rho >= 0.94\uff09\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5373\u4f7f LLM \u88ab\u660e\u786e\u63d0\u793a\u8868\u73b0\u51fa\u516c\u5e73\u6216\u6709\u504f\u5dee\u7684\u884c\u4e3a\uff08rho >= 0.92\uff09\uff0c\u5e76\u4e14\u5c11\u6b21\u63d0\u793a\u957f\u5ea6\u548c\u523b\u677f\u5370\u8c61\u6210\u5206\u6709\u6240\u4e0d\u540c\uff08rho >= 0.97\uff09\uff0c\u504f\u5dee\u8f6c\u79fb\u4ecd\u7136\u5bc6\u5207\u76f8\u5173\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u786e\u4fdd\u9884\u5148\u8bad\u7ec3\u7684 LLM \u516c\u5e73\u6027\u7684\u91cd\u8981\u6027\uff0c\u5c24\u5176\u662f\u5728\u5b83\u4eec\u540e\u6765\u901a\u8fc7\u63d0\u793a\u8c03\u6574\u7528\u4e8e\u6267\u884c\u4e0b\u6e38\u4efb\u52a1\u65f6\u3002", "author": "Natalie Mackraz et.al.", "authors": "Natalie Mackraz, Nivedha Sivakumar, Samira Khorshidi, Krishna Patel, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff", "id": "2412.03537v1", "paper_url": "http://arxiv.org/abs/2412.03537v1", "repo": "null"}}