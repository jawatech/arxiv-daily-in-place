{"2409.18313": {"publish_time": "2024-09-26", "title": "Embodied-RAG: General non-parametric Embodied Memory for Retrieval and Generation", "paper_summary": "There is no limit to how much a robot might explore and learn, but all of\nthat knowledge needs to be searchable and actionable. Within language research,\nretrieval augmented generation (RAG) has become the workhouse of large-scale\nnon-parametric knowledge, however existing techniques do not directly transfer\nto the embodied domain, which is multimodal, data is highly correlated, and\nperception requires abstraction.\n  To address these challenges, we introduce Embodied-RAG, a framework that\nenhances the foundational model of an embodied agent with a non-parametric\nmemory system capable of autonomously constructing hierarchical knowledge for\nboth navigation and language generation. Embodied-RAG handles a full range of\nspatial and semantic resolutions across diverse environments and query types,\nwhether for a specific object or a holistic description of ambiance. At its\ncore, Embodied-RAG's memory is structured as a semantic forest, storing\nlanguage descriptions at varying levels of detail. This hierarchical\norganization allows the system to efficiently generate context-sensitive\noutputs across different robotic platforms. We demonstrate that Embodied-RAG\neffectively bridges RAG to the robotics domain, successfully handling over 200\nexplanation and navigation queries across 19 environments, highlighting its\npromise for general-purpose non-parametric system for embodied agents.", "paper_summary_zh": "\u6a5f\u5668\u4eba\u7684\u63a2\u7d22\u548c\u5b78\u7fd2\u6c92\u6709\u9650\u5236\uff0c\u4f46\u6240\u6709\u9019\u4e9b\u77e5\u8b58\u90fd\u9700\u8981\u53ef\u641c\u5c0b\u4e14\u53ef\u64cd\u4f5c\u3002\u5728\u8a9e\u8a00\u7814\u7a76\u4e2d\uff0c\u6aa2\u7d22\u64f4\u589e\u751f\u6210 (RAG) \u5df2\u6210\u70ba\u5927\u898f\u6a21\u975e\u53c3\u6578\u77e5\u8b58\u7684\u57fa\u790e\uff0c\u4f46\u73fe\u6709\u6280\u8853\u4e26\u672a\u76f4\u63a5\u8f49\u79fb\u5230\u5177\u591a\u6a21\u614b\u3001\u8cc7\u6599\u9ad8\u5ea6\u76f8\u95dc\u4e14\u611f\u77e5\u9700\u8981\u62bd\u8c61\u7684\u5177\u8eab\u9818\u57df\u3002\n\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86 Embodied-RAG\uff0c\u4e00\u500b\u589e\u5f37\u4e86\u5177\u8eab\u4ee3\u7406\u57fa\u790e\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5177\u5099\u975e\u53c3\u6578\u8a18\u61b6\u7cfb\u7d71\uff0c\u80fd\u5920\u81ea\u4e3b\u5efa\u69cb\u968e\u5c64\u5f0f\u77e5\u8b58\uff0c\u4ee5\u9032\u884c\u5c0e\u822a\u548c\u8a9e\u8a00\u751f\u6210\u3002Embodied-RAG \u8655\u7406\u5404\u7a2e\u74b0\u5883\u548c\u67e5\u8a62\u985e\u578b\u4e2d\u7684\u5b8c\u6574\u7a7a\u9593\u548c\u8a9e\u7fa9\u89e3\u6790\uff0c\u7121\u8ad6\u662f\u91dd\u5c0d\u7279\u5b9a\u7269\u4ef6\u6216\u74b0\u5883\u7684\u6574\u9ad4\u63cf\u8ff0\u3002\u5728\u6838\u5fc3\u90e8\u5206\uff0cEmbodied-RAG \u7684\u8a18\u61b6\u88ab\u7d50\u69cb\u5316\u70ba\u8a9e\u7fa9\u68ee\u6797\uff0c\u5132\u5b58\u4e0d\u540c\u8a73\u7d30\u7a0b\u5ea6\u7684\u8a9e\u8a00\u63cf\u8ff0\u3002\u9019\u7a2e\u968e\u5c64\u7d44\u7e54\u8b93\u7cfb\u7d71\u80fd\u5920\u5728\u4e0d\u540c\u7684\u6a5f\u5668\u4eba\u5e73\u53f0\u4e0a\u6709\u6548\u7522\u751f\u8207\u60c5\u5883\u76f8\u95dc\u7684\u8f38\u51fa\u3002\u6211\u5011\u8b49\u660e\u4e86 Embodied-RAG \u6709\u6548\u5730\u5c07 RAG \u6a4b\u63a5\u5230\u6a5f\u5668\u4eba\u9818\u57df\uff0c\u6210\u529f\u8655\u7406\u4e86 19 \u500b\u74b0\u5883\u4e2d\u8d85\u904e 200 \u500b\u89e3\u91cb\u548c\u5c0e\u822a\u67e5\u8a62\uff0c\u7a81\u986f\u4e86\u5b83\u5c0d\u5177\u8eab\u4ee3\u7406\u7684\u4e00\u822c\u7528\u9014\u975e\u53c3\u6578\u7cfb\u7d71\u7684\u627f\u8afe\u3002", "author": "Quanting Xie et.al.", "authors": "Quanting Xie, So Yeon Min, Tianyi Zhang, Aarav Bajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson, Yonatan Bisk", "id": "2409.18313v1", "paper_url": "http://arxiv.org/abs/2409.18313v1", "repo": "null"}}