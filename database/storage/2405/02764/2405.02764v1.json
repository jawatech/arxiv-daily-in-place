{"2405.02764": {"publish_time": "2024-05-04", "title": "Assessing Adversarial Robustness of Large Language Models: An Empirical Study", "paper_summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their robustness against adversarial attacks remains a critical concern. We\npresents a novel white-box style attack approach that exposes vulnerabilities\nin leading open-source LLMs, including Llama, OPT, and T5. We assess the impact\nof model size, structure, and fine-tuning strategies on their resistance to\nadversarial perturbations. Our comprehensive evaluation across five diverse\ntext classification tasks establishes a new benchmark for LLM robustness. The\nfindings of this study have far-reaching implications for the reliable\ndeployment of LLMs in real-world applications and contribute to the advancement\nof trustworthy AI systems.", "paper_summary_zh": "", "author": "Zeyu Yang et.al.", "authors": "Zeyu Yang,Zhao Meng,Xiaochen Zheng,Roger Wattenhofer", "id": "2405.02764v1", "paper_url": "http://arxiv.org/abs/2405.02764v1", "repo": "null"}}