
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-10-04**|**Estimating Body and Hand Motion in an Ego-sensed World**|Brent Yi et.al.|[2410.03665v1](http://arxiv.org/abs/2410.03665v1)|null|
|**2024-10-04**|**Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models**|Zhuochun Li et.al.|[2410.03663v1](http://arxiv.org/abs/2410.03663v1)|null|
|**2024-10-04**|**System 2 reasoning capabilities are nigh**|Scott C. Lowe et.al.|[2410.03662v1](http://arxiv.org/abs/2410.03662v1)|null|
|**2024-10-04**|**Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models**|Tinghui Zhu et.al.|[2410.03659v1](http://arxiv.org/abs/2410.03659v1)|null|
|**2024-10-04**|**RAFT: Realistic Attacks to Fool Text Detectors**|James Wang et.al.|[2410.03658v1](http://arxiv.org/abs/2410.03658v1)|null|
|**2024-10-04**|**Geometric Representation Condition Improves Equivariant Molecule Generation**|Zian Li et.al.|[2410.03655v1](http://arxiv.org/abs/2410.03655v1)|null|
|**2024-10-04**|**GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs**|Pu Hua et.al.|[2410.03645v1](http://arxiv.org/abs/2410.03645v1)|null|
|**2024-10-04**|**Aligning LLMs with Individual Preferences via Interaction**|Shujin Wu et.al.|[2410.03642v1](http://arxiv.org/abs/2410.03642v1)|[link](https://github.com/shujinwu-0814/aloe)|
|**2024-10-04**|**What Matters for Model Merging at Scale?**|Prateek Yadav et.al.|[2410.03617v1](http://arxiv.org/abs/2410.03617v1)|null|
|**2024-10-04**|**TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation**|Jonathan Cook et.al.|[2410.03608v1](http://arxiv.org/abs/2410.03608v1)|null|
|**2024-10-04**|**Efficiently Identifying Watermarked Segments in Mixed-Source Texts**|Xuandong Zhao et.al.|[2410.03600v1](http://arxiv.org/abs/2410.03600v1)|null|
|**2024-10-04**|**Understanding Reasoning in Chain-of-Thought from the Hopfieldian View**|Lijie Hu et.al.|[2410.03595v1](http://arxiv.org/abs/2410.03595v1)|null|
|**2024-10-04**|**Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments**|Omar Sharif et.al.|[2410.03594v1](http://arxiv.org/abs/2410.03594v1)|null|
|**2024-10-04**|**Variational Bayes Gaussian Splatting**|Toon Van de Maele et.al.|[2410.03592v1](http://arxiv.org/abs/2410.03592v1)|[link](https://github.com/versestech/vbgs)|
|**2024-10-04**|**A Multi-model Approach for Video Data Retrieval in Autonomous Vehicle Development**|Jesper Knapp et.al.|[2410.03580v1](http://arxiv.org/abs/2410.03580v1)|null|
|**2024-10-04**|**Table Question Answering for Low-resourced Indic Languages**|Vaishali Pal et.al.|[2410.03576v1](http://arxiv.org/abs/2410.03576v1)|[link](https://github.com/kolk/low-resource-tableqa-indic-languages)|
|**2024-10-04**|**Towards Linguistically-Aware and Language-Independent Tokenization for Large Language Models (LLMs)**|Abrar Rahman et.al.|[2410.03568v1](http://arxiv.org/abs/2410.03568v1)|null|
|**2024-10-04**|**Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features**|Benyuan Meng et.al.|[2410.03558v1](http://arxiv.org/abs/2410.03558v1)|[link](https://github.com/darkbblue/generic-diffusion-feature)|
|**2024-10-04**|**Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding**|Wei Wu et.al.|[2410.03553v1](http://arxiv.org/abs/2410.03553v1)|null|
|**2024-10-04**|**Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research**|Yida Mu et.al.|[2410.03545v1](http://arxiv.org/abs/2410.03545v1)|null|
|**2024-10-04**|**Re-examining Sexism and Misogyny Classification with Annotator Attitudes**|Aiqi Jiang et.al.|[2410.03543v1](http://arxiv.org/abs/2410.03543v1)|null|
|**2024-10-04**|**Ward: Provable RAG Dataset Inference via LLM Watermarks**|Nikola Jovanović et.al.|[2410.03537v1](http://arxiv.org/abs/2410.03537v1)|null|
|**2024-10-04**|**MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction**|Han Jiang et.al.|[2410.03531v1](http://arxiv.org/abs/2410.03531v1)|null|
|**2024-10-04**|**No Need to Talk: Asynchronous Mixture of Language Models**|Anastasiia Filippova et.al.|[2410.03529v1](http://arxiv.org/abs/2410.03529v1)|null|
|**2024-10-04**|**Steering Large Language Models between Code Execution and Textual Reasoning**|Yongchao Chen et.al.|[2410.03524v1](http://arxiv.org/abs/2410.03524v1)|null|
|**2024-10-04**|**A Probabilistic Perspective on Unlearning and Alignment for Large Language Models**|Yan Scholten et.al.|[2410.03523v1](http://arxiv.org/abs/2410.03523v1)|null|
|**2024-10-04**|**CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios**|Zetian Ouyang et.al.|[2410.03502v1](http://arxiv.org/abs/2410.03502v1)|null|
|**2024-10-04**|**FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator**|Sunny Gupta et.al.|[2410.03499v1](http://arxiv.org/abs/2410.03499v1)|[link](https://github.com/sunnyinAI/FedStein)|
|**2024-10-04**|**Generative Artificial Intelligence for Navigating Synthesizable Chemical Space**|Wenhao Gao et.al.|[2410.03494v1](http://arxiv.org/abs/2410.03494v1)|[link](https://github.com/wenhao-gao/synformer)|
|**2024-10-04**|**Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark Scores**|Robert E. Blackwell et.al.|[2410.03492v1](http://arxiv.org/abs/2410.03492v1)|null|
|**2024-10-04**|**Gradient-based Jailbreak Images for Multimodal Fusion Models**|Javier Rando et.al.|[2410.03489v1](http://arxiv.org/abs/2410.03489v1)|null|
|**2024-10-04**|**A Multimodal Framework for Deepfake Detection**|Kashish Gandhi et.al.|[2410.03487v1](http://arxiv.org/abs/2410.03487v1)|null|
|**2024-10-04**|**Group Fairness in Peer Review**|Haris Aziz et.al.|[2410.03474v1](http://arxiv.org/abs/2410.03474v1)|null|
|**2024-10-04**|**Vulnerability Detection via Topological Analysis of Attention Maps**|Pavel Snopov et.al.|[2410.03470v1](http://arxiv.org/abs/2410.03470v1)|null|
|**2024-10-04**|**Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering**|Helena Bonaldi et.al.|[2410.03466v1](http://arxiv.org/abs/2410.03466v1)|null|
|**2024-10-04**|**Diffusion State-Guided Projected Gradient for Inverse Problems**|Rayhan Zirvi et.al.|[2410.03463v1](http://arxiv.org/abs/2410.03463v1)|null|
|**2024-10-04**|**Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation**|Tobias Leemann et.al.|[2410.03461v1](http://arxiv.org/abs/2410.03461v1)|null|
|**2024-10-04**|**Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges**|Nguyen Van Dinh et.al.|[2410.03458v1](http://arxiv.org/abs/2410.03458v1)|null|
|**2024-10-04**|**CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds**|Min-Hsuan Yeh et.al.|[2410.03457v1](http://arxiv.org/abs/2410.03457v1)|null|
|**2024-10-04**|**How Toxicity Classifiers and Large Language Models Respond to Ableism**|Mahika Phutane et.al.|[2410.03448v1](http://arxiv.org/abs/2410.03448v1)|null|
|**2024-10-04**|**How Language Models Prioritize Contextual Grammatical Cues?**|Hamidreza Amirzadeh et.al.|[2410.03447v1](http://arxiv.org/abs/2410.03447v1)|null|
|**2024-10-04**|**On Uncertainty In Natural Language Processing**|Dennis Ulmer et.al.|[2410.03446v1](http://arxiv.org/abs/2410.03446v1)|[link](https://github.com/kaleidophon/nlp-uncertainty-zoo)|
|**2024-10-04**|**Exploring the Benefit of Activation Sparsity in Pre-training**|Zhengyan Zhang et.al.|[2410.03440v1](http://arxiv.org/abs/2410.03440v1)|[link](https://github.com/thunlp/moefication)|
|**2024-10-04**|**ToolGen: Unified Tool Retrieval and Calling via Generation**|Renxi Wang et.al.|[2410.03439v1](http://arxiv.org/abs/2410.03439v1)|null|
|**2024-10-04**|**A General Framework for Producing Interpretable Semantic Text Embeddings**|Yiqun Sun et.al.|[2410.03435v1](http://arxiv.org/abs/2410.03435v1)|null|
|**2024-10-04**|**Self-supervised Spatio-Temporal Graph Mask-Passing Attention Network for Perceptual Importance Prediction of Multi-point Tactility**|Dazhong He et.al.|[2410.03434v1](http://arxiv.org/abs/2410.03434v1)|null|
|**2024-10-04**|**Images Speak Volumes: User-Centric Assessment of Image Generation for Accessible Communication**|Miriam Anschütz et.al.|[2410.03430v1](http://arxiv.org/abs/2410.03430v1)|null|
|**2024-10-04**|**How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics**|Adrian Cosma et.al.|[2410.03429v1](http://arxiv.org/abs/2410.03429v1)|null|
|**2024-10-04**|**Cayley Graph Propagation**|JJ Wilson et.al.|[2410.03424v1](http://arxiv.org/abs/2410.03424v1)|[link](https://github.com/josephjwilson/cayley_graph_propagation)|
|**2024-10-04**|**One2set + Large Language Model: Best Partners for Keyphrase Generation**|Liangying Shao et.al.|[2410.03421v1](http://arxiv.org/abs/2410.03421v1)|null|
|**2024-10-04**|**Towards Real-time Intrahepatic Vessel Identification in Intraoperative Ultrasound-Guided Liver Surgery**|Karl-Philippe Beaudet et.al.|[2410.03420v1](http://arxiv.org/abs/2410.03420v1)|null|
|**2024-10-04**|**Surgical, Cheap, and Flexible: Mitigating False Refusal in Language Models via Single Vector Ablation**|Xinpeng Wang et.al.|[2410.03415v1](http://arxiv.org/abs/2410.03415v1)|null|
|**2024-10-04**|**Team MTS @ AutoMin 2021: An Overview of Existing Summarization Approaches and Comparison to Unsupervised Summarization Techniques**|Olga Iakovenko et.al.|[2410.03412v1](http://arxiv.org/abs/2410.03412v1)|null|
|**2024-10-04**|**Comparative study of regression vs pairwise models for surrogate-based heuristic optimisation**|Pablo S. Naharro et.al.|[2410.03409v1](http://arxiv.org/abs/2410.03409v1)|null|
|**2024-10-04**|**EBES: Easy Benchmarking for Event Sequences**|Dmitry Osin et.al.|[2410.03399v1](http://arxiv.org/abs/2410.03399v1)|null|
|**2024-10-04**|**GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction**|Shijin Duan et.al.|[2410.03396v1](http://arxiv.org/abs/2410.03396v1)|[link](https://github.com/sjduan/graphcroc)|
|**2024-10-04**|**Killing Two Flies with One Stone: An Attempt to Break LLMs Using English->Icelandic Idioms and Proper Names**|Bjarki Ármannsson et.al.|[2410.03394v1](http://arxiv.org/abs/2410.03394v1)|null|
|**2024-10-04**|**Cogs in a Machine, Doing What They're Meant to Do -- The AMI Submission to the WMT24 General Translation Task**|Atli Jasonarson et.al.|[2410.03381v1](http://arxiv.org/abs/2410.03381v1)|null|
|**2024-10-04**|**Predicting perturbation targets with causal differential networks**|Menghua Wu et.al.|[2410.03380v1](http://arxiv.org/abs/2410.03380v1)|null|
|**2024-10-04**|**An Enhanced Harmonic Densely Connected Hybrid Transformer Network Architecture for Chronic Wound Segmentation Utilising Multi-Colour Space Tensor Merging**|Bill Cassidy et.al.|[2410.03359v1](http://arxiv.org/abs/2410.03359v1)|null|
|**2024-10-04**|**Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing**|Jeongwoo Kang et.al.|[2410.03357v1](http://arxiv.org/abs/2410.03357v1)|null|
|**2024-10-04**|**LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding**|Doohyuk Jang et.al.|[2410.03355v1](http://arxiv.org/abs/2410.03355v1)|null|
|**2024-10-04**|**Generating Equivalent Representations of Code By A Self-Reflection Approach**|Jia Li et.al.|[2410.03351v1](http://arxiv.org/abs/2410.03351v1)|null|
|**2024-10-04**|**Zero-Shot Fact Verification via Natural Logic and Large Language Models**|Marek Strong et.al.|[2410.03341v1](http://arxiv.org/abs/2410.03341v1)|null|
|**2024-10-04**|**An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation**|Ahmed Abdulaal et.al.|[2410.03334v1](http://arxiv.org/abs/2410.03334v1)|null|
|**2024-10-04**|**Comparative Analysis and Ensemble Enhancement of Leading CNN Architectures for Breast Cancer Classification**|Gary Murphy et.al.|[2410.03333v1](http://arxiv.org/abs/2410.03333v1)|null|
|**2024-10-04**|**Influence-oriented Personalized Federated Learning**|Yue Tan et.al.|[2410.03315v1](http://arxiv.org/abs/2410.03315v1)|null|
|**2024-10-04**|**Context and System Fusion in Post-ASR Emotion Recognition with Large Language Models**|Pavel Stepachev et.al.|[2410.03312v1](http://arxiv.org/abs/2410.03312v1)|[link](https://github.com/rggdmonk/GenSEC-Task-3)|
|**2024-10-04**|**Comparing zero-shot self-explanations with human rationales in multilingual text classification**|Stephanie Brandl et.al.|[2410.03296v1](http://arxiv.org/abs/2410.03296v1)|null|
|**2024-10-04**|**Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis**|Nirmalya Thakur et.al.|[2410.03293v1](http://arxiv.org/abs/2410.03293v1)|null|
|**2024-10-04**|**Enhanced Transformer architecture for in-context learning of dynamical systems**|Matteo Rufolo et.al.|[2410.03291v1](http://arxiv.org/abs/2410.03291v1)|null|
|**2024-10-04**|**Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models**|Haibo Wang et.al.|[2410.03290v1](http://arxiv.org/abs/2410.03290v1)|[link](https://github.com/whb139426/grounded-video-llm)|
|**2024-10-04**|**What do Large Language Models Need for Machine Translation Evaluation?**|Shenbin Qian et.al.|[2410.03278v1](http://arxiv.org/abs/2410.03278v1)|null|
|**2024-10-04**|**Test-time Adaptation for Regression by Subspace Alignment**|Kazuki Adachi et.al.|[2410.03263v1](http://arxiv.org/abs/2410.03263v1)|null|
|**2024-10-04**|**Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models**|Gunjan Balde et.al.|[2410.03258v1](http://arxiv.org/abs/2410.03258v1)|[link](https://github.com/gb-kgp/adaptbpe)|
|**2024-10-04**|**Towards a Benchmark for Large Language Models for Business Process Management Tasks**|Kiran Busch et.al.|[2410.03255v1](http://arxiv.org/abs/2410.03255v1)|null|
|**2024-10-04**|**Are Expert-Level Language Models Expert-Level Annotators?**|Yu-Min Tseng et.al.|[2410.03254v1](http://arxiv.org/abs/2410.03254v1)|null|
|**2024-10-04**|**How much can we forget about Data Contamination?**|Sebastian Bordt et.al.|[2410.03249v1](http://arxiv.org/abs/2410.03249v1)|null|
|**2024-10-04**|**Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken Vocabulary?**|Adam Nohejl et.al.|[2410.03240v1](http://arxiv.org/abs/2410.03240v1)|[link](https://github.com/naist-nlp/tubelex)|
|**2024-10-04**|**Enriching Ontologies with Disjointness Axioms using Large Language Models**|Elias Crum et.al.|[2410.03235v1](http://arxiv.org/abs/2410.03235v1)|[link](https://github.com/n28div/llm-disjointness)|
|**2024-10-04**|**Showing LLM-Generated Code Selectively Based on Confidence of LLMs**|Jia Li et.al.|[2410.03234v1](http://arxiv.org/abs/2410.03234v1)|null|
|**2024-10-04**|**ALR$^2$: A Retrieve-then-Reason Framework for Long-context Question Answering**|Huayang Li et.al.|[2410.03227v1](http://arxiv.org/abs/2410.03227v1)|null|
|**2024-10-04**|**Frame-Voyager: Learning to Query Frames for Video Large Language Models**|Sicheng Yu et.al.|[2410.03226v1](http://arxiv.org/abs/2410.03226v1)|null|
|**2024-10-04**|**AutoPenBench: Benchmarking Generative Agents for Penetration Testing**|Luca Gioacchini et.al.|[2410.03225v1](http://arxiv.org/abs/2410.03225v1)|[link](https://github.com/lucagioacchini/genai-pentest-paper)|
|**2024-10-04**|**Consultation on Industrial Machine Faults with Large language Models**|Apiradee Boonmee et.al.|[2410.03223v1](http://arxiv.org/abs/2410.03223v1)|null|
|**2024-10-04**|**NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task**|Pramit Sahoo et.al.|[2410.03215v1](http://arxiv.org/abs/2410.03215v1)|null|
|**2024-10-04**|**Learning Semantic Structure through First-Order-Logic Translation**|Akshay Chaturvedi et.al.|[2410.03203v1](http://arxiv.org/abs/2410.03203v1)|null|
|**2024-10-04**|**PersoBench: Benchmarking Personalized Response Generation in Large Language Models**|Saleh Afzoon et.al.|[2410.03198v1](http://arxiv.org/abs/2410.03198v1)|null|
|**2024-10-04**|**Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages**|Seonjeong Hwang et.al.|[2410.03197v1](http://arxiv.org/abs/2410.03197v1)|null|
|**2024-10-04**|**Parallel Corpus Augmentation using Masked Language Models**|Vibhuti Kumari et.al.|[2410.03194v1](http://arxiv.org/abs/2410.03194v1)|null|
|**2024-10-04**|**MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech**|Taejun Bak et.al.|[2410.03192v1](http://arxiv.org/abs/2410.03192v1)|null|
|**2024-10-04**|**Looking into Concept Explanation Methods for Diabetic Retinopathy Classification**|Andrea M. Storås et.al.|[2410.03188v1](http://arxiv.org/abs/2410.03188v1)|[link](https://github.com/andreastoraas/conceptexplanations_dr_grading)|
|**2024-10-04**|**EXAQ: Exponent Aware Quantization For LLMs Acceleration**|Moran Shkolnik et.al.|[2410.03185v1](http://arxiv.org/abs/2410.03185v1)|[link](https://github.com/anonymous1252022/exaq)|
|**2024-10-04**|**Generating bilingual example sentences with large language models as lexicography assistants**|Raphael Merx et.al.|[2410.03182v1](http://arxiv.org/abs/2410.03182v1)|null|
|**2024-10-04**|**Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas**|Seungjong Sun et.al.|[2410.03181v1](http://arxiv.org/abs/2410.03181v1)|null|
|**2024-10-04**|**Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models**|Yufang Liu et.al.|[2410.03176v1](http://arxiv.org/abs/2410.03176v1)|[link](https://github.com/yufang-liu/clip_hallucination)|
|**2024-10-04**|**Autoregressive Large Language Models are Computationally Universal**|Dale Schuurmans et.al.|[2410.03170v1](http://arxiv.org/abs/2410.03170v1)|null|
|**2024-10-04**|**Can Watermarked LLMs be Identified by Users via Crafted Prompts?**|Aiwei Liu et.al.|[2410.03168v1](http://arxiv.org/abs/2410.03168v1)|null|
|**2024-10-04**|**Adaptive Masking Enhances Visual Grounding**|Sen Jia et.al.|[2410.03161v1](http://arxiv.org/abs/2410.03161v1)|null|
|**2024-10-04**|**Autoregressive Moving-average Attention Mechanism for Time Series Forecasting**|Jiecheng Lu et.al.|[2410.03159v1](http://arxiv.org/abs/2410.03159v1)|null|

#### Abstracts
##### **Estimating Body and Hand Motion in an Ego-sensed World**
2410.03665v1 by Brent Yi, Vickie Ye, Maya Zheng, Lea Müller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa

We present EgoAllo, a system for human motion estimation from a head-mounted
device. Using only egocentric SLAM poses and images, EgoAllo guides sampling
from a conditional diffusion model to estimate 3D body pose, height, and hand
parameters that capture the wearer's actions in the allocentric coordinate
frame of the scene. To achieve this, our key insight is in representation: we
propose spatial and temporal invariance criteria for improving model
performance, from which we derive a head motion conditioning parameterization
that improves estimation by up to 18%. We also show how the bodies estimated by
our system can improve the hands: the resulting kinematic and temporal
constraints result in over 40% lower hand estimation errors compared to noisy
monocular estimates. Project page: https://egoallo.github.io/

摘要：我們提出 EgoAllo，一種從頭戴式裝置估計人體動作的系統。僅使用自我中心 SLAM 姿勢和影像，EgoAllo 引導從條件擴散模型中取樣，以估計 3D 身體姿勢、高度和手部參數，捕捉穿戴者在場景世界座標系中的動作。為達成此目的，我們的關鍵見解在於表示：我們提出空間和時間不變性準則以改善模型效能，由此我們衍生出頭部動作條件參數化，可將估計值改善多達 18%。我們也展示我們的系統估計的身體如何改善手部：與有雜訊的單眼估計值相比，產生的運動學和時間約束可將手部估計誤差降低超過 40%。專案頁面：https://egoallo.github.io/

##### **Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models**
2410.03663v1 by Zhuochun Li, Yuelyu Ji, Rui Meng, Daqing He

Large language models (LLMs) have exhibited complex reasoning abilities by
generating question rationales and demonstrated exceptional performance in
natural language processing (NLP) tasks. However, these reasoning capabilities
generally emerge in models with tens of billions of parameters, creating
significant computational challenges for real-world deployment. Recent research
has concentrated on improving open-source smaller models through knowledge
distillation (KD) from commercial LLMs. Nevertheless, most of these studies
rely solely on the responses from one single LLM as the gold rationale for
training. In this paper, we introduce a novel Mistake-Aware Peer-Review
Distillation (MAPD) approach: 1) Instead of merely obtaining gold rationales
from teachers, our method asks teachers to identify and explain the student's
mistakes, providing customized instruction learning data. 2) We design a
simulated peer-review process between teacher LLMs, which selects only the
generated rationales above the acceptance threshold. This reduces the chance of
teachers guessing correctly with flawed rationale, improving instructional data
quality. Comprehensive experiments and analysis on mathematical, commonsense,
and logical reasoning tasks demonstrate the effectiveness of our method.

摘要：大型語言模型 (LLM) 已展現出複雜的推理能力，可產生問題依據，並在自然語言處理 (NLP) 任務中展現出色的效能。然而，這些推理能力通常出現在數十億個參數的模型中，為實際部署創造了重大的運算挑戰。最近的研究集中於透過商業 LLM 的知識提煉 (KD) 來改善開源的小型模型。儘管如此，這些研究大多僅依賴單一 LLM 的回應作為訓練的黃金依據。在本文中，我們介紹了一種新穎的錯誤感知同儕審查提煉 (MAPD) 方法：1) 我們的模型並非僅從教師那裡取得黃金依據，而是要求教師找出並說明學生的錯誤，提供客製化的教學學習資料。2) 我們設計了一個在教師 LLM 之間模擬的同儕審查流程，僅選擇高於接受門檻的產生依據。這降低了教師以有缺陷的依據正確猜測的機率，並改善了教學資料的品質。在數學、常識和邏輯推理任務上的全面實驗和分析證明了我們方法的有效性。

##### **System 2 reasoning capabilities are nigh**
2410.03662v1 by Scott C. Lowe

In recent years, machine learning models have made strides towards human-like
reasoning capabilities from several directions. In this work, we review the
current state of the literature and describe the remaining steps to achieve a
neural model which can perform System 2 reasoning analogous to a human. We
argue that if current models are insufficient to be classed as performing
reasoning, there remains very little additional progress needed to attain that
goal.

摘要：近年來，機器學習模型從幾個方向朝向類人推理能力邁進。在這項工作中，我們回顧了文獻的現狀，並描述了實現神經模型以執行類似於人類的系統 2 推理的剩餘步驟。我們認為，如果當前模型不足以歸類為執行推理，那麼實現該目標所需的額外進展非常少。

##### **Unraveling Cross-Modality Knowledge Conflict in Large Vision-Language Models**
2410.03659v1 by Tinghui Zhu, Qin Liu, Fei Wang, Zhengzhong Tu, Muhao Chen

Large Vision-Language Models (LVLMs) have demonstrated impressive
capabilities for capturing and reasoning over multimodal inputs. However, these
models are prone to parametric knowledge conflicts, which arise from
inconsistencies of represented knowledge between their vision and language
components. In this paper, we formally define the problem of
$\textbf{cross-modality parametric knowledge conflict}$ and present a
systematic approach to detect, interpret, and mitigate them. We introduce a
pipeline that identifies conflicts between visual and textual answers, showing
a persistently high conflict rate across modalities in recent LVLMs regardless
of the model size. We further investigate how these conflicts interfere with
the inference process and propose a contrastive metric to discern the
conflicting samples from the others. Building on these insights, we develop a
novel dynamic contrastive decoding method that removes undesirable logits
inferred from the less confident modality components based on answer
confidence. For models that do not provide logits, we also introduce two
prompt-based strategies to mitigate the conflicts. Our methods achieve
promising improvements in accuracy on both the ViQuAE and InfoSeek datasets.
Specifically, using LLaVA-34B, our proposed dynamic contrastive decoding
improves an average accuracy of 2.24%.

摘要：大型视觉语言模型 (LVLMs) 已展示出令人印象深刻的能力，用于捕捉和推理多模态输入。然而，这些模型容易出现参数知识冲突，这是由于其视觉和语言组件之间所表示知识的不一致性造成的。在本文中，我们正式定义了$\textbf{跨模态参数知识冲突}$的问题，并提出了一种系统的方法来检测、解释和缓解它们。我们引入了一个管道，用于识别视觉和文本答案之间的冲突，显示了在最近的 LVLMs 中，无论模型大小如何，跨模态的冲突率持续居高不下。我们进一步研究了这些冲突如何干扰推理过程，并提出了一个对比度量来区分冲突样本和其他样本。基于这些见解，我们开发了一种新颖的动态对比解码方法，该方法根据答案置信度，删除了从不太自信的模态组件推断出的不良 logit。对于不提供 logit 的模型，我们还引入了两种基于提示的策略来缓解冲突。我们的方法在 ViQuAE 和 InfoSeek 数据集上实现了有希望的准确性改进。具体来说，使用 LLaVA-34B，我们提出的动态对比解码将平均准确率提高了 2.24%。

##### **RAFT: Realistic Attacks to Fool Text Detectors**
2410.03658v1 by James Wang, Ran Li, Junfeng Yang, Chengzhi Mao

Large language models (LLMs) have exhibited remarkable fluency across various
tasks. However, their unethical applications, such as disseminating
disinformation, have become a growing concern. Although recent works have
proposed a number of LLM detection methods, their robustness and reliability
remain unclear. In this paper, we present RAFT: a grammar error-free black-box
attack against existing LLM detectors. In contrast to previous attacks for
language models, our method exploits the transferability of LLM embeddings at
the word-level while preserving the original text quality. We leverage an
auxiliary embedding to greedily select candidate words to perturb against the
target detector. Experiments reveal that our attack effectively compromises all
detectors in the study across various domains by up to 99%, and are
transferable across source models. Manual human evaluation studies show our
attacks are realistic and indistinguishable from original human-written text.
We also show that examples generated by RAFT can be used to train adversarially
robust detectors. Our work shows that current LLM detectors are not
adversarially robust, underscoring the urgent need for more resilient detection
mechanisms.

摘要：大型語言模型 (LLM) 在各種任務中表現出卓越的流暢性。然而，它們不道德的應用，例如散佈錯誤訊息，已成為日益嚴重的問題。儘管最近的研究提出了一些 LLM 檢測方法，但它們的穩健性和可靠性仍不清楚。在本文中，我們提出了 RAFT：一個針對現有 LLM 檢測器的語法無錯誤黑盒攻擊。與之前針對語言模型的攻擊不同，我們的技術利用了 LLM 嵌入在詞彙層面的可傳遞性，同時保留原始文本品質。我們利用輔助嵌入貪婪地選擇候選詞，以針對目標檢測器進行擾動。實驗表明，我們的攻擊有效地損害了研究中所有檢測器，在各個領域中高達 99%，並且可以在來源模型之間進行傳遞。手動人類評估研究表明，我們的攻擊是真實的，並且與原始人類撰寫的文字無法區分。我們還表明，由 RAFT 生成的範例可以用於訓練對抗性的穩健檢測器。我們的研究表明，目前的 LLM 檢測器在對抗攻擊中並不穩健，強調了對更具復原力的檢測機制迫切的需求。

##### **Geometric Representation Condition Improves Equivariant Molecule Generation**
2410.03655v1 by Zian Li, Cai Zhou, Xiyuan Wang, Xingang Peng, Muhan Zhang

Recent advancements in molecular generative models have demonstrated
substantial potential in accelerating scientific discovery, particularly in
drug design. However, these models often face challenges in generating
high-quality molecules, especially in conditional scenarios where specific
molecular properties must be satisfied. In this work, we introduce GeoRCG, a
general framework to enhance the performance of molecular generative models by
integrating geometric representation conditions. We decompose the molecule
generation process into two stages: first, generating an informative geometric
representation; second, generating a molecule conditioned on the
representation. Compared to directly generating a molecule, the relatively
easy-to-generate representation in the first-stage guides the second-stage
generation to reach a high-quality molecule in a more goal-oriented and much
faster way. Leveraging EDM as the base generator, we observe significant
quality improvements in unconditional molecule generation on the widely-used
QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional
molecular generation task, our framework achieves an average 31\% performance
improvement over state-of-the-art approaches, highlighting the superiority of
conditioning on semantically rich geometric representations over conditioning
on individual property values as in previous approaches. Furthermore, we show
that, with such representation guidance, the number of diffusion steps can be
reduced to as small as 100 while maintaining superior generation quality than
that achieved with 1,000 steps, thereby significantly accelerating the
generation process.

摘要：<paragraph>分子生成模型的最新進展已證明在加速科學發現方面具有相當大的潛力，特別是在藥物設計中。然而，這些模型在生成高品質分子時經常面臨挑戰，特別是在必須滿足特定分子特性的條件場景中。在這項工作中，我們介紹了 GeoRCG，一個透過整合幾何表示條件來增強分子生成模型效能的通用框架。我們將分子生成過程分解為兩個階段：首先，生成一個有資訊性的幾何表示；其次，生成一個以表示為條件的分子。與直接生成分子相比，第一階段中相對容易生成的表示引導第二階段的生成以更具目標導向且更快速的方式達到高品質分子。利用 EDM 作為基礎生成器，我們觀察到廣泛使用的 QM9 和 GEOM-DRUG 資料集上的無條件分子生成的顯著品質提升。更值得注意的是，在具有挑戰性的條件式分子生成任務中，我們的框架比最先進的方法平均提升了 31% 的效能，突顯了以語義豐富的幾何表示為條件的優越性，而不是像以前的方法那樣以個別屬性值為條件。此外，我們展示了在這種表示指導下，擴散步驟的數量可以減少到僅 100，同時保持優於使用 1,000 步所實現的生成品質，從而顯著加速生成過程。</paragraph>

##### **GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs**
2410.03645v1 by Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, Lirui Wang

Robotic simulation today remains challenging to scale up due to the human
efforts required to create diverse simulation tasks and scenes.
Simulation-trained policies also face scalability issues as many sim-to-real
methods focus on a single task. To address these challenges, this work proposes
GenSim2, a scalable framework that leverages coding LLMs with multi-modal and
reasoning capabilities for complex and realistic simulation task creation,
including long-horizon tasks with articulated objects. To automatically
generate demonstration data for these tasks at scale, we propose planning and
RL solvers that generalize within object categories. The pipeline can generate
data for up to 100 articulated tasks with 200 objects and reduce the required
human efforts. To utilize such data, we propose an effective multi-task
language-conditioned policy architecture, dubbed proprioceptive point-cloud
transformer (PPT), that learns from the generated demonstrations and exhibits
strong sim-to-real zero-shot transfer. Combining the proposed pipeline and the
policy architecture, we show a promising usage of GenSim2 that the generated
data can be used for zero-shot transfer or co-train with real-world collected
data, which enhances the policy performance by 20% compared with training
exclusively on limited real data.

摘要：機器人模擬至今仍難以擴展，原因在於創造多樣化的模擬任務和場景需要大量人力。
經過模擬訓練的策略也面臨可擴展性問題，因為許多模擬到真實的方法都專注於單一任務。
為了應對這些挑戰，本研究提出 GenSim2，一個可擴展的框架，該框架利用具備多模態和推理能力的編碼 LLM 來進行複雜且逼真的模擬任務建立，包括具有關節物體的長時程任務。
為了自動生成這些任務的大規模示範資料，我們提出在物件類別中進行概括的規劃和 RL 求解器。
該管道可以生成多達 100 個具有 200 個物件的關節任務的資料，並減少所需的人力。
為了利用此類資料，我們提出一個有效的多任務語言條件策略架構，稱為 proprioceptive point-cloud transformer (PPT)，該架構從生成的示範中學習，並展現出強大的模擬到真實零次學習轉移。
結合所提出的管道和策略架構，我們展示了 GenSim2 的一個有前途的用途，即生成的資料可用於零次學習轉移或與現實世界收集的資料共同訓練，與僅在有限的真實資料上進行訓練相比，這將策略效能提升了 20%。

##### **Aligning LLMs with Individual Preferences via Interaction**
2410.03642v1 by Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, Heng Ji

As large language models (LLMs) demonstrate increasingly advanced
capabilities, aligning their behaviors with human values and preferences
becomes crucial for their wide adoption. While previous research focuses on
general alignment to principles such as helpfulness, harmlessness, and honesty,
the need to account for individual and diverse preferences has been largely
overlooked, potentially undermining customized human experiences. To address
this gap, we train LLMs that can ''interact to align'', essentially cultivating
the meta-skill of LLMs to implicitly infer the unspoken personalized
preferences of the current user through multi-turn conversations, and then
dynamically align their following behaviors and responses to these inferred
preferences. Our approach involves establishing a diverse pool of 3,310
distinct user personas by initially creating seed examples, which are then
expanded through iterative self-generation and filtering. Guided by distinct
user personas, we leverage multi-LLM collaboration to develop a multi-turn
preference dataset containing 3K+ multi-turn conversations in tree structures.
Finally, we apply supervised fine-tuning and reinforcement learning to enhance
LLMs using this dataset. For evaluation, we establish the ALOE (ALign With
CustOmized PrEferences) benchmark, consisting of 100 carefully selected
examples and well-designed metrics to measure the customized alignment
performance during conversations. Experimental results demonstrate the
effectiveness of our method in enabling dynamic, personalized alignment via
interaction.

摘要：隨著大型語言模型 (LLM) 展現出越來越先進的能力，讓它們的行為與人類的價值觀和偏好保持一致，對於它們的廣泛採用至關重要。雖然先前的研究專注於對諸如樂於助人、無害和誠實等原則的一般性對齊，但對個人和不同偏好的需求在很大程度上被忽視了，這可能會破壞客製化的人類體驗。為了解決這個差距，我們訓練了能夠「互動對齊」的 LLM，本質上培養了 LLM 的元技能，透過多輪對話隱含推斷當前使用者的未說出口的個人化偏好，然後動態對齊其後續行為和回應，以符合這些推斷出的偏好。我們的做法包括建立一個由 3,310 個不同的使用者角色組成的多元化池，最初透過建立種子範例，然後透過反覆自我產生和過濾來擴展。在不同的使用者角色的指導下，我們利用多 LLM 協作來開發一個多輪偏好資料集，其中包含樹狀結構中的 3K 多個多輪對話。最後，我們應用監督微調和強化學習，使用此資料集來增強 LLM。為了評估，我們建立了 ALOE（與客製化偏好對齊）基準，其中包含 100 個經過仔細挑選的範例和精心設計的指標，用於衡量對話期間的客製化對齊效能。實驗結果證明了我們的方法在透過互動實現動態、個人化對齊方面的有效性。

##### **What Matters for Model Merging at Scale?**
2410.03617v1 by Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, Tsendsuren Munkhdalai

Model merging aims to combine multiple expert models into a more capable
single model, offering benefits such as reduced storage and serving costs,
improved generalization, and support for decentralized model development.
Despite its promise, previous studies have primarily focused on merging a few
small models. This leaves many unanswered questions about the effect of scaling
model size and how it interplays with other key factors -- like the base model
quality and number of expert models -- , to affect the merged model's
performance. This work systematically evaluates the utility of model merging at
scale, examining the impact of these different factors. We experiment with
merging fully fine-tuned models using 4 popular merging methods -- Averaging,
Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B
parameters and merging up to 8 different expert models. We evaluate the merged
models on both held-in tasks, i.e., the expert's training tasks, and zero-shot
generalization to unseen held-out tasks. Our experiments provide several new
insights about model merging at scale and the interplay between different
factors. First, we find that merging is more effective when experts are created
from strong base models, i.e., models with good zero-shot performance. Second,
larger models facilitate easier merging. Third merging consistently improves
generalization capabilities. Notably, when merging 8 large expert models, the
merged models often generalize better compared to the multitask trained models.
Fourth, we can better merge more expert models when working with larger models.
Fifth, different merging methods behave very similarly at larger scales.
Overall, our findings shed light on some interesting properties of model
merging while also highlighting some limitations. We hope that this study will
serve as a reference point on large-scale merging for upcoming research.

摘要：模型合併旨在將多個專家模型合併成一個更強大的單一模型，提供諸如降低儲存和服務成本、改善泛化以及支援分散式模型開發等好處。儘管有其承諾，但先前的研究主要集中在合併少數小型模型。這留下了許多關於擴充模型規模的影響以及它如何與其他關鍵因素（例如基本模型品質和專家模型數量）相互作用，以影響合併模型效能的未解問題。這項工作系統性地評估了模型合併在規模上的效用，檢視了這些不同因素的影響。我們使用 4 種流行的合併方法（平均、Task~Arithmetic、Dare 和 TIES）對經過充分微調的模型進行合併實驗，模型規模範圍從 1B-64B 參數，並合併多達 8 個不同的專家模型。我們在兩種保留任務（即專家的訓練任務）上評估合併模型，以及對未見保留任務進行零次學習泛化。我們的實驗提供了關於規模化模型合併和不同因素之間相互作用的幾個新見解。首先，我們發現當專家是由強大的基本模型（即具有良好零次學習效能的模型）建立時，合併更有效。其次，較大的模型有助於更輕鬆地合併。第三，合併始終改善泛化能力。值得注意的是，在合併 8 個大型專家模型時，合併模型通常比多任務訓練模型泛化得更好。第四，在處理較大型模型時，我們可以更好地合併更多專家模型。第五，不同的合併方法在較大規模時表現非常相似。總體而言，我們的研究結果揭示了模型合併的一些有趣特性，同時也突出了某些限制。我們希望這項研究將作為即將進行的研究中大規模合併的參考點。

##### **TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation**
2410.03608v1 by Jonathan Cook, Tim Rocktäschel, Jakob Foerster, Dennis Aumiller, Alex Wang

Given the widespread adoption and usage of Large Language Models (LLMs), it
is crucial to have flexible and interpretable evaluations of their
instruction-following ability. Preference judgments between model outputs have
become the de facto evaluation standard, despite distilling complex,
multi-faceted preferences into a single ranking. Furthermore, as human
annotation is slow and costly, LLMs are increasingly used to make these
judgments, at the expense of reliability and interpretability. In this work, we
propose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,
interpretable evaluation protocol that structures evaluations with
LLM-generated, instruction-specific checklists. We first show that, given an
instruction, LLMs can reliably produce high-quality, tailored evaluation
checklists that decompose the instruction into a series of YES/NO questions.
Each question asks whether a candidate response meets a specific requirement of
the instruction. We demonstrate that using TICK leads to a significant increase
(46.4% $\to$ 52.2%) in the frequency of exact agreements between LLM judgements
and human preferences, as compared to having an LLM directly score an output.
We then show that STICK (Self-TICK) can be used to improve generation quality
across multiple benchmarks via self-refinement and Best-of-N selection. STICK
self-refinement on LiveBench reasoning tasks leads to an absolute gain of
$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute
improvement on the real-world instruction dataset, WildBench. In light of this,
structured, multi-faceted self-improvement is shown to be a promising way to
further advance LLM capabilities. Finally, by providing LLM-generated
checklists to human evaluators tasked with directly scoring LLM responses to
WildBench instructions, we notably increase inter-annotator agreement (0.194
$\to$ 0.256).

摘要：<paragraph>鉴于大型语言模型 (LLM) 的广泛采用和使用，对它们的指令遵循能力进行灵活且可解释的评估至关重要。模型输出之间的偏好判断已成为事实上的评估标准，尽管将复杂、多方面的偏好提炼成单一排名。此外，由于人工注释缓慢且成本高昂，LLM 正越来越多地用于做出这些判断，但牺牲了可靠性和可解释性。在这项工作中，我们提出了 TICK（有针对性的指令评估与清单），这是一种完全自动化的、可解释的评估协议，它使用 LLM 生成的、特定于指令的清单来构建评估。我们首先表明，给定一个指令，LLM 可以可靠地生成高质量的、定制的评估清单，将指令分解为一系列是/否问题。每个问题询问候选响应是否满足指令的特定要求。我们证明使用 TICK 导致 LLM 判断和人类偏好之间完全一致的频率显着增加（46.4% $ \to $ 52.2%），与让 LLM 直接对输出进行评分相比。然后我们表明，STICK（自 TICK）可以通过自优化和 N 选优来提高多个基准测试的生成质量。LiveBench 推理任务上的 STICK 自优化导致绝对增益为 +$7.8%，而 STICK 上的 N 选优在现实世界指令数据集 WildBench 上获得了 +$6.3% 的绝对改进。鉴于此，结构化、多方面的自我改进被证明是进一步提升 LLM 能力的一种有前途的方法。最后，通过向负责直接对 LLM 对 WildBench 指令的响应进行评分的人类评估者提供 LLM 生成的清单，我们显着提高了注释者间的一致性（0.194 $ \to $ 0.256）。</paragraph>

##### **Efficiently Identifying Watermarked Segments in Mixed-Source Texts**
2410.03600v1 by Xuandong Zhao, Chenwen Liao, Yu-Xiang Wang, Lei Li

Text watermarks in large language models (LLMs) are increasingly used to
detect synthetic text, mitigating misuse cases like fake news and academic
dishonesty. While existing watermarking detection techniques primarily focus on
classifying entire documents as watermarked or not, they often neglect the
common scenario of identifying individual watermark segments within longer,
mixed-source documents. Drawing inspiration from plagiarism detection systems,
we propose two novel methods for partial watermark detection. First, we develop
a geometry cover detection framework aimed at determining whether there is a
watermark segment in long text. Second, we introduce an adaptive online
learning algorithm to pinpoint the precise location of watermark segments
within the text. Evaluated on three popular watermarking techniques
(KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves
high accuracy, significantly outperforming baseline methods. Moreover, our
framework is adaptable to other watermarking techniques, offering new insights
for precise watermark detection.

摘要：大型語言模型 (LLM) 中的文字浮水印日益用於檢測合成文字，緩解假新聞和學術不誠實等濫用案例。雖然現有的浮水印檢測技術主要集中於將整個文件分類為有浮水印或沒有浮水印，但它們通常忽略了在較長、混合來源文件內識別個別浮水印區段的常見場景。從抄襲檢測系統中汲取靈感，我們提出了兩種用於部分浮水印檢測的新穎方法。首先，我們開發了一個幾何覆蓋檢測框架，旨在確定長文本中是否存在浮水印區段。其次，我們引入了一個自適應線上學習演算法，以精確找出文字中浮水印區段的位置。在三種流行的浮水印技術（KGW-Watermark、Unigram-Watermark 和 Gumbel-Watermark）上進行評估，我們的做法達到了很高的準確度，顯著優於基線方法。此外，我們的框架可以適應其他浮水印技術，為精確的浮水印檢測提供新的見解。

##### **Understanding Reasoning in Chain-of-Thought from the Hopfieldian View**
2410.03595v1 by Lijie Hu, Liang Liu, Shu Yang, Xin Chen, Zhen Tan, Muhammad Asif Ali, Mengdi Li, Di Wang

Large Language Models have demonstrated remarkable abilities across various
tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to
enhance reasoning capabilities. However, existing research primarily focuses on
improving performance, lacking a comprehensive framework to explain and
understand the fundamental factors behind CoT's success. To bridge this gap, we
introduce a novel perspective grounded in the Hopfieldian view of cognition in
cognitive neuroscience. We establish a connection between CoT reasoning and key
cognitive elements such as stimuli, actions, neural populations, and
representation spaces. From our view, we can understand the reasoning process
as the movement between these representation spaces. Building on this insight,
we develop a method for localizing reasoning errors in the response of CoTs.
Moreover, we propose the Representation-of-Thought (RoT) framework, which
leverages the robustness of low-dimensional representation spaces to enhance
the robustness of the reasoning process in CoTs. Experimental results
demonstrate that RoT improves the robustness and interpretability of CoT
reasoning while offering fine-grained control over the reasoning process.

摘要：大型語言模型已在各種任務中展現出非凡的能力，其中以思想鏈 (CoT) 提示作為增強推理能力的一項關鍵技術而備受矚目。然而，現有研究主要專注於提升效能，卻缺乏一個全面的架構來解釋和理解 CoT 成功背後的基本因素。為了填補這個鴻溝，我們引進一個新的觀點，它奠基於認知神經科學中霍普菲爾德認知觀點。我們建立了 CoT 推理與關鍵認知元素（例如刺激、動作、神經元族群和表徵空間）之間的聯繫。從我們的觀點來看，我們可以將推理過程理解為在這些表徵空間之間的移動。根據這個見解，我們開發了一種方法來定位 CoT 回應中的推理錯誤。此外，我們提出了思想表徵 (RoT) 架構，它利用低維表徵空間的穩健性來增強 CoT 中推理過程的穩健性。實驗結果表明，RoT 改善了 CoT 推理的穩健性和可解釋性，同時對推理過程提供了細緻的控制。

##### **Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments**
2410.03594v1 by Omar Sharif, Joseph Gatto, Madhusudan Basak, Sarah M. Preum

Prior works formulate the extraction of event-specific arguments as a span
extraction problem, where event arguments are explicit -- i.e. assumed to be
contiguous spans of text in a document. In this study, we revisit this
definition of Event Extraction (EE) by introducing two key argument types that
cannot be modeled by existing EE frameworks. First, implicit arguments are
event arguments which are not explicitly mentioned in the text, but can be
inferred through context. Second, scattered arguments are event arguments that
are composed of information scattered throughout the text. These two argument
types are crucial to elicit the full breadth of information required for proper
event modeling.
  To support the extraction of explicit, implicit, and scattered arguments, we
develop a novel dataset, DiscourseEE, which includes 7,464 argument annotations
from online health discourse. Notably, 51.2% of the arguments are implicit, and
17.4% are scattered, making DiscourseEE a unique corpus for complex event
extraction. Additionally, we formulate argument extraction as a text generation
problem to facilitate the extraction of complex argument types. We provide a
comprehensive evaluation of state-of-the-art models and highlight critical open
challenges in generative event extraction. Our data and codebase are available
at https://omar-sharif03.github.io/DiscourseEE.

摘要：先前的研究將事件特定論證的提取制定為跨度提取問題，其中事件論證是明確的——即假設為文件中連續的文本跨度。在本研究中，我們通過引入兩個現有 EE 框架無法建模的關鍵論證類型來重新審視事件提取 (EE) 的定義。首先，隱含論證是文中未明確提及的事件論證，但可透過上下文推論。其次，分散論證是事件論證，由散布在文本中的資訊組成。這兩種論證類型對於引出適當事件建模所需的完整資訊至關重要。
為了支援顯式、隱含和分散論證的提取，我們開發了一個新穎的資料集 DiscourseEE，其中包含來自線上健康論述的 7,464 個論證註解。值得注意的是，51.2% 的論證是隱含的，17.4% 是分散的，這使得 DiscourseEE 成為複雜事件提取的獨特語料庫。此外，我們將論證提取制定為文字生成問題，以利於複雜論證類型的提取。我們提供了對最先進模型的全面評估，並強調了生成式事件提取中的關鍵開放挑戰。我們的資料和程式碼庫可在 https://omar-sharif03.github.io/DiscourseEE 取得。

##### **Variational Bayes Gaussian Splatting**
2410.03592v1 by Toon Van de Maele, Ozan Catal, Alexander Tschantz, Christopher L. Buckley, Tim Verbelen

Recently, 3D Gaussian Splatting has emerged as a promising approach for
modeling 3D scenes using mixtures of Gaussians. The predominant optimization
method for these models relies on backpropagating gradients through a
differentiable rendering pipeline, which struggles with catastrophic forgetting
when dealing with continuous streams of data. To address this limitation, we
propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that
frames training a Gaussian splat as variational inference over model
parameters. By leveraging the conjugacy properties of multivariate Gaussians,
we derive a closed-form variational update rule, allowing efficient updates
from partial, sequential observations without the need for replay buffers. Our
experiments show that VBGS not only matches state-of-the-art performance on
static datasets, but also enables continual learning from sequentially streamed
2D and 3D data, drastically improving performance in this setting.

摘要：最近，3D 高斯潑濺已成為一種前景看好的方法，用於使用高斯混合對 3D 場景進行建模。這些模型的主要最佳化方法依賴於通過可微分渲染管線反向傳播梯度，在處理連續數據串流時會遇到災難性遺忘。為了解決這個限制，我們提出了變分貝氏高斯潑濺 (VBGS)，這是一種新方法，將訓練高斯潑濺構建為模型參數上的變分推論。通過利用多元高斯的共軛性質，我們推導出一個閉合形式的變分更新規則，允許從部分、順序觀測中進行有效更新，而無需重播緩衝區。我們的實驗表明，VBGS 不僅在靜態數據集上匹配了最先進的效能，而且還能從順序串流的 2D 和 3D 數據中進行持續學習，從而大幅提升了在這種設定下的效能。

##### **A Multi-model Approach for Video Data Retrieval in Autonomous Vehicle Development**
2410.03580v1 by Jesper Knapp, Klas Moberg, Yuchuan Jin, Simin Sun, Miroslaw Staron

Autonomous driving software generates enormous amounts of data every second,
which software development organizations save for future analysis and testing
in the form of logs. However, given the vast size of this data, locating
specific scenarios within a collection of vehicle logs can be challenging.
Writing the correct SQL queries to find these scenarios requires engineers to
have a strong background in SQL and the specific databases in question, further
complicating the search process. This paper presents and evaluates a pipeline
that allows searching for specific scenarios in log collections using natural
language descriptions instead of SQL. The generated descriptions were evaluated
by engineers working with vehicle logs at the Zenseact on a scale from 1 to 5.
Our approach achieved a mean score of 3.3, demonstrating the potential of using
a multi-model architecture to improve the software development workflow. We
also present an interface that can visualize the query process and visualize
the results.

摘要：自動駕駛軟體每秒產生大量的資料，
軟體開發組織將這些資料儲存起來以供將來的分析和測試
以記錄檔的形式。然而，由於資料量龐大，在車輛記錄檔的集合中找到
特定情境可能具有挑戰性。撰寫正確的 SQL 查詢以找到這些情境需要工程師
具備 SQL 和特定資料庫的紮實背景，這進一步複雜化了搜尋程序。本文提出並評估一個管線，
它允許使用自然語言描述而不是 SQL 在記錄檔集合中搜尋特定情境。產生的描述由在 Zenseact 使用車輛記錄檔的工程師評估，評分範圍為 1 到 5。
我們的做法達到平均分數 3.3，證明了使用多模型架構來改善軟體開發工作流程的可能性。我們
還提供了一個介面，它可以視覺化查詢程序和視覺化結果。

##### **Table Question Answering for Low-resourced Indic Languages**
2410.03576v1 by Vaishali Pal, Evangelos Kanoulas, Andrew Yates, Maarten de Rijke

TableQA is the task of answering questions over tables of structured
information, returning individual cells or tables as output. TableQA research
has focused primarily on high-resource languages, leaving medium- and
low-resource languages with little progress due to scarcity of annotated data
and neural models. We address this gap by introducing a fully automatic
large-scale tableQA data generation process for low-resource languages with
limited budget. We incorporate our data generation method on two Indic
languages, Bengali and Hindi, which have no tableQA datasets or models. TableQA
models trained on our large-scale datasets outperform state-of-the-art LLMs. We
further study the trained models on different aspects, including mathematical
reasoning capabilities and zero-shot cross-lingual transfer. Our work is the
first on low-resource tableQA focusing on scalable data generation and
evaluation procedures. Our proposed data generation method can be applied to
any low-resource language with a web presence. We release datasets, models, and
code (https://github.com/kolk/Low-Resource-TableQA-Indic-languages).

摘要：TableQA 是一項任務，回答有關結構化資訊表格的問題，回傳個別儲存格或表格作為輸出。TableQA 研究主要集中在高資源語言，由於標註資料和神經模型的稀少，導致中低資源語言進展甚微。我們透過引入一個完全自動化的大規模 TableQA 資料產生程序來解決這個差距，適用於預算有限的低資源語言。我們將資料產生方法納入兩種印度語言，孟加拉語和印地語，這兩種語言沒有 TableQA 資料集或模型。在我們的大規模資料集上訓練的 TableQA 模型優於最先進的 LLM。我們進一步研究訓練模型的不同面向，包括數學推理能力和零次學習的跨語言轉移。我們的研究是第一個針對低資源 TableQA，專注於可擴充資料產生和評估程序的研究。我們提出的資料產生方法可以應用於任何有網路存在的低資源語言。我們發布了資料集、模型和程式碼 (https://github.com/kolk/Low-Resource-TableQA-Indic-languages)。

##### **Towards Linguistically-Aware and Language-Independent Tokenization for Large Language Models (LLMs)**
2410.03568v1 by Abrar Rahman, Garry Bowlin, Binit Mohanty, Sean McGunigal

This paper presents a comprehensive study on the tokenization techniques
employed by state-of-the-art large language models (LLMs) and their
implications on the cost and availability of services across different
languages, especially low resource languages. The analysis considers multiple
LLMs, including GPT-4 (using cl100k_base embeddings), GPT-3 (with p50k_base
embeddings), and DaVinci (employing r50k_base embeddings), as well as the
widely used BERT base tokenizer. The study evaluates the tokenization
variability observed across these models and investigates the challenges of
linguistic representation in subword tokenization. The research underscores the
importance of fostering linguistically-aware development practices, especially
for languages that are traditionally under-resourced. Moreover, this paper
introduces case studies that highlight the real-world implications of
tokenization choices, particularly in the context of electronic health record
(EHR) systems. This research aims to promote generalizable Internationalization
(I18N) practices in the development of AI services in this domain and beyond,
with a strong emphasis on inclusivity, particularly for languages traditionally
underrepresented in AI applications.

摘要：本文對最先進大型語言模型 (LLM) 所採用的標記化技術進行了全面研究，並探討了這些技術對不同語言（尤其是低資源語言）服務的成本和可用性的影響。分析考量了多個 LLM，包括 GPT-4（使用 cl100k_base 嵌入）、GPT-3（使用 p50k_base 嵌入）和 DaVinci（使用 r50k_base 嵌入），以及廣泛使用的 BERT 基礎標記器。研究評估了這些模型中觀察到的標記化變異性，並探討了詞彙化標記化中語言表示的挑戰。研究強調了培養語言感知開發實務的重要性，特別是對於傳統上資源不足的語言。此外，本文介紹了案例研究，重點說明標記化選擇的實際影響，特別是在電子健康紀錄 (EHR) 系統的背景下。本研究旨在推廣此領域（及其他領域）AI 服務開發中可概化的國際化 (I18N) 實務，並特別強調包容性，特別是對於傳統上在 AI 應用中代表性不足的語言。

##### **Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features**
2410.03558v1 by Benyuan Meng, Qianqian Xu, Zitai Wang, Xiaochun Cao, Qingming Huang

Diffusion models are initially designed for image generation. Recent research
shows that the internal signals within their backbones, named activations, can
also serve as dense features for various discriminative tasks such as semantic
segmentation. Given numerous activations, selecting a small yet effective
subset poses a fundamental problem. To this end, the early study of this field
performs a large-scale quantitative comparison of the discriminative ability of
the activations. However, we find that many potential activations have not been
evaluated, such as the queries and keys used to compute attention scores.
Moreover, recent advancements in diffusion architectures bring many new
activations, such as those within embedded ViT modules. Both combined,
activation selection remains unresolved but overlooked. To tackle this issue,
this paper takes a further step with a much broader range of activations
evaluated. Considering the significant increase in activations, a full-scale
quantitative comparison is no longer operational. Instead, we seek to
understand the properties of these activations, such that the activations that
are clearly inferior can be filtered out in advance via simple qualitative
evaluation. After careful analysis, we discover three properties universal
among diffusion models, enabling this study to go beyond specific models. On
top of this, we present effective feature selection solutions for several
popular diffusion models. Finally, the experiments across multiple
discriminative tasks validate the superiority of our method over the SOTA
competitors. Our code is available at
https://github.com/Darkbblue/generic-diffusion-feature.

摘要：擴散模型最初是針對影像生成而設計的。最近的研究顯示，其主幹中的內部訊號（稱為啟動）也可作為各種判別式任務（例如語意分割）的密集特徵。在給定許多啟動的情況下，選擇一個小但有效的子集會構成一個基本問題。為此，該領域的早期研究對啟動的判別能力進行了大規模的定量比較。然而，我們發現許多潛在的啟動尚未經過評估，例如用於計算注意力分數的查詢和鍵。此外，擴散架構的最新進展帶來了許多新的啟動，例如嵌入式 ViT 模組中的那些。兩者相結合，啟動選擇仍然未解決且被忽視。為了解決這個問題，本文更進一步，評估了更廣泛的啟動範圍。考慮到啟動的大幅增加，全面定量比較不再可行。相反，我們試圖了解這些啟動的屬性，以便可以透過簡單的定性評估事先篩選出明顯較差的啟動。經過仔細分析，我們發現了擴散模型中普遍存在的三個屬性，使本研究能夠超越特定模型。除此之外，我們還為幾個流行的擴散模型提供了有效的特徵選擇解決方案。最後，跨多個判別式任務的實驗驗證了我們的方法優於 SOTA 競爭對手。我們的程式碼可在 https://github.com/Darkbblue/generic-diffusion-feature 取得。

##### **Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding**
2410.03553v1 by Wei Wu, Chao Wang, Liyi Chen, Mingze Yin, Yiheng Zhu, Kun Fu, Jieping Ye, Hui Xiong, Zheng Wang

Proteins, as essential biomolecules, play a central role in biological
processes, including metabolic reactions and DNA replication. Accurate
prediction of their properties and functions is crucial in biological
applications. Recent development of protein language models (pLMs) with
supervised fine tuning provides a promising solution to this problem. However,
the fine-tuned model is tailored for particular downstream prediction task, and
achieving general-purpose protein understanding remains a challenge. In this
paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT)
framework to bridge this gap. Our approach integrates a noval structure-aware
module into pLMs to inform them with structural knowledge, and then connects
these enhanced pLMs to large language models (LLMs) to generate understanding
of proteins. In this framework, we propose a novel two-stage instruction tuning
pipeline that first establishes a basic understanding of proteins through
caption-based instructions and then refines this understanding using a mixture
of experts (MoEs) to learn more complex properties and functional information
with the same amount of activated parameters. Moreover, we construct the
largest and most comprehensive protein instruction dataset to date, which
allows us to train and evaluate the general-purpose protein understanding
model. Extensive experimental results on open-ended generation and closed-set
answer tasks demonstrate the superior performance of SEPIT over both
closed-source general LLMs and open-source LLMs trained with protein knowledge.

摘要：蛋白質作為重要的生物分子，在生物過程中扮演著核心角色，包括代謝反應和 DNA 複製。準確預測其特性和功能在生物應用中至關重要。最近開發的蛋白質語言模型 (pLM)，透過監督微調提供了解決此問題的絕佳方案。然而，微調模型是針對特定下游預測任務量身打造，而達成通用的蛋白質理解仍是一項挑戰。在本文中，我們介紹了結構增強蛋白質指令微調 (SEPIT) 框架來彌補這個差距。我們的做法將一個新穎的結構感知模組整合到 pLM 中，以結構知識告知它們，然後將這些增強的 pLM 連接到大型語言模型 (LLM) 以產生對蛋白質的理解。在這個框架中，我們提出了一個新穎的兩階段指令微調管道，它首先透過基於標題的指令建立對蛋白質的基本理解，然後使用專家混合 (MoE) 來完善此理解，以使用相同數量的已啟動參數來學習更複雜的特性和功能資訊。此外，我們建構了迄今為止最大且最全面的蛋白質指令資料集，這讓我們能夠訓練和評估通用蛋白質理解模型。在開放式生成和封閉式回答任務上的廣泛實驗結果證明了 SEPIT 優於封閉原始碼通用 LLM 和使用蛋白質知識訓練的開放原始碼 LLM。

##### **Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research**
2410.03545v1 by Yida Mu, Mali Jin, Xingyi Song, Nikolaos Aletras

Research in natural language processing (NLP) for Computational Social
Science (CSS) heavily relies on data from social media platforms. This data
plays a crucial role in the development of models for analysing
socio-linguistic phenomena within online communities. In this work, we conduct
an in-depth examination of 20 datasets extensively used in NLP for CSS to
comprehensively examine data quality. Our analysis reveals that social media
datasets exhibit varying levels of data duplication. Consequently, this gives
rise to challenges like label inconsistencies and data leakage, compromising
the reliability of models. Our findings also suggest that data duplication has
an impact on the current claims of state-of-the-art performance, potentially
leading to an overestimation of model effectiveness in real-world scenarios.
Finally, we propose new protocols and best practices for improving dataset
development from social media data and its usage.

摘要：自然語言處理 (NLP) 在計算社會科學 (CSS) 中的研究，非常依賴於社群媒體平台的資料。這些資料在社群中分析社會語言現象的模型開發中，扮演至關重要的角色。在本研究中，我們對 NLP 中廣泛用於 CSS 的 20 個資料集進行深入探討，以全面檢查資料品質。我們的分析顯示，社群媒體資料集呈現出不同程度的資料重複。因此，這會導致標籤不一致和資料外洩等挑戰，進而損害模型的可靠性。我們的研究結果也顯示，資料重複會影響當前最先進的效能聲明，可能會導致在實際情況中高估模型的有效性。最後，我們提出新的協定和最佳實務，以改善社群媒體資料的資料集開發及其使用方式。

##### **Re-examining Sexism and Misogyny Classification with Annotator Attitudes**
2410.03543v1 by Aiqi Jiang, Nikolas Vitsakis, Tanvi Dinkar, Gavin Abercrombie, Ioannis Konstas

Gender-Based Violence (GBV) is an increasing problem online, but existing
datasets fail to capture the plurality of possible annotator perspectives or
ensure the representation of affected groups. We revisit two important stages
in the moderation pipeline for GBV: (1) manual data labelling; and (2)
automated classification. For (1), we examine two datasets to investigate the
relationship between annotator identities and attitudes and the responses they
give to two GBV labelling tasks. To this end, we collect demographic and
attitudinal information from crowd-sourced annotators using three validated
surveys from Social Psychology. We find that higher Right Wing Authoritarianism
scores are associated with a higher propensity to label text as sexist, while
for Social Dominance Orientation and Neosexist Attitudes, higher scores are
associated with a negative tendency to do so. For (2), we conduct
classification experiments using Large Language Models and five prompting
strategies, including infusing prompts with annotator information. We find: (i)
annotator attitudes affect the ability of classifiers to predict their labels;
(ii) including attitudinal information can boost performance when we use
well-structured brief annotator descriptions; and (iii) models struggle to
reflect the increased complexity and imbalanced classes of the new label sets.

摘要：性別暴力 (GBV) 在網路上是一個日益嚴重的問題，但現有的資料集無法掌握註解者可能觀點的多樣性，也無法確保受影響群體的代表性。我們重新探討 GBV 審核流程中的兩個重要階段：(1) 手動資料標記；和 (2) 自動化分類。對於 (1)，我們探討兩個資料集，以調查註解者身分和態度與他們對兩個 GBV 標記任務的回應之間的關係。為此，我們使用社會心理學的三項驗證調查，從群眾外包的註解者收集人口統計和態度資訊。我們發現，較高的右翼威權主義得分與將文字標記為性別歧視的傾向較高有關，而對於社會支配傾向和新性別主義態度，較高的得分與這樣做的負面傾向有關。對於 (2)，我們使用大型語言模型和五種提示策略進行分類實驗，包括在提示中注入註解者資訊。我們發現：(i) 註解者態度影響分類器預測其標籤的能力；(ii) 在我們使用結構良好的簡要註解者描述時，包含態度資訊可以提升效能；以及 (iii) 模型難以反映新標籤集的複雜性和不平衡類別。

##### **Ward: Provable RAG Dataset Inference via LLM Watermarks**
2410.03537v1 by Nikola Jovanović, Robin Staab, Maximilian Baader, Martin Vechev

Retrieval-Augmented Generation (RAG) improves LLMs by enabling them to
incorporate external data during generation. This raises concerns for data
owners regarding unauthorized use of their content in RAG systems. Despite its
importance, the challenge of detecting such unauthorized usage remains
underexplored, with existing datasets and methodologies from adjacent fields
being ill-suited for its study. In this work, we take several steps to bridge
this gap. First, we formalize this problem as (black-box) RAG Dataset Inference
(RAG-DI). To facilitate research on this challenge, we further introduce a
novel dataset specifically designed for benchmarking RAG-DI methods under
realistic conditions, and propose a set of baseline approaches. Building on
this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks
that enables data owners to obtain rigorous statistical guarantees regarding
the usage of their dataset in a RAG system. In our experimental evaluation, we
show that Ward consistently outperforms all baselines across many challenging
settings, achieving higher accuracy, superior query efficiency and robustness.
Our work provides a foundation for future studies of RAG-DI and highlights LLM
watermarks as a promising approach to this problem.

摘要：檢索增強生成（RAG）透過讓大型語言模型在生成期間納入外部資料，進而改善大型語言模型。這引發了資料擁有者對於其內容在 RAG 系統中遭到未經授權使用之疑慮。儘管這點很重要，但偵測此類未經授權使用之挑戰仍未獲得充分探討，且現有資料集和來自相關領域的方法論並不適合用於研究。在這項工作中，我們採取了多項步驟來彌合此差距。首先，我們將此問題形式化為（黑盒）RAG 資料集推論（RAG-DI）。為促進對此挑戰的研究，我們進一步引入一個新穎的資料集，特別設計用於在實際情況下對 RAG-DI 方法進行基準測試，並提出了一組基準方法。在此基礎上，我們引入了 Ward，這是一種基於 LLM 浮水印的 RAG-DI 方法，讓資料擁有者能夠獲得嚴謹的統計保證，以了解其資料集在 RAG 系統中的使用情況。在我們的實驗評估中，我們證明 Ward 在許多具挑戰性的設定中始終優於所有基準，達到了更高的準確度、優異的查詢效率和穩健性。我們的研究為 RAG-DI 的後續研究奠定了基礎，並強調 LLM 浮水印作為解決此問題的一種有前途的方法。

##### **MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction**
2410.03531v1 by Han Jiang, Junwen Duan, Zhe Qu, Jianxin Wang

Unsupervised rationale extraction aims to extract text snippets to support
model predictions without explicit rationale annotation. Researchers have made
many efforts to solve this task. Previous works often encode each aspect
independently, which may limit their ability to capture meaningful internal
correlations between aspects. While there has been significant work on
mitigating spurious correlations, our approach focuses on leveraging the
beneficial internal correlations to improve multi-aspect rationale extraction.
In this paper, we propose a Multi-Aspect Rationale Extractor (MARE) to explain
and predict multiple aspects simultaneously. Concretely, we propose a
Multi-Aspect Multi-Head Attention (MAMHA) mechanism based on hard deletion to
encode multiple text chunks simultaneously. Furthermore, multiple special
tokens are prepended in front of the text with each corresponding to one
certain aspect. Finally, multi-task training is deployed to reduce the training
overhead. Experimental results on two unsupervised rationale extraction
benchmarks show that MARE achieves state-of-the-art performance. Ablation
studies further demonstrate the effectiveness of our method. Our codes have
been available at https://github.com/CSU-NLP-Group/MARE.

摘要：無監督基礎抽取旨在抽取文字片段以支援模型預測，而無需明確的基礎標註。研究人員已為了解決此任務做出了許多努力。先前的作品通常獨立編碼每個面向，這可能會限制它們捕捉面向之間有意義的內部關聯性的能力。儘管已經有大量工作在減輕虛假關聯性上，但我們的做法著重於利用有益的內部關聯性來改善多面向基礎抽取。在本文中，我們提出了一個多面向基礎抽取器 (MARE) 來同時解釋和預測多個面向。具體來說，我們提出了一個基於硬刪除的多面向多頭注意力 (MAMHA) 機制，以同時編碼多個文字塊。此外，多個特殊標記會預先加在文字前面，每個標記都對應一個特定面向。最後，部署多任務訓練以減少訓練開銷。兩個無監督基礎抽取基準上的實驗結果表明，MARE 達到了最先進的效能。消融研究進一步證明了我們方法的有效性。我們的程式碼已在 https://github.com/CSU-NLP-Group/MARE 中提供。

##### **No Need to Talk: Asynchronous Mixture of Language Models**
2410.03529v1 by Anastasiia Filippova, Angelos Katharopoulos, David Grangier, Ronan Collobert

We introduce SmallTalk LM, an innovative method for training a mixture of
language models in an almost asynchronous manner. Each model of the mixture
specializes in distinct parts of the data distribution, without the need of
high-bandwidth communication between the nodes training each model. At
inference, a lightweight router directs a given sequence to a single expert,
according to a short prefix. This inference scheme naturally uses a fraction of
the parameters from the overall mixture model. Our experiments on language
modeling demonstrate tha SmallTalk LM achieves significantly lower perplexity
than dense model baselines for the same total training FLOPs and an almost
identical inference cost. Finally, in our downstream evaluations we outperform
the dense baseline on $75\%$ of the tasks.

摘要：我們介紹 SmallTalk LM，一種創新的方法，用於以幾乎非同步的方式訓練混合語言模型。混合中的每個模型專注於資料分佈的不同部分，無需在訓練每個模型的節點之間進行高頻寬通訊。在推論時，輕量級路由器根據簡短的前綴將給定的序列導向單個專家。這種推論方案自然會使用來自整體混合模型的一小部分參數。我們在語言建模上的實驗表明，SmallTalk LM 的困惑度顯著低於相同總訓練 FLOP 的密集模型基線，並且推理成本幾乎相同。最後，在我們的下游評估中，我們在 $75\%$ 的任務上優於密集基線。

##### **Steering Large Language Models between Code Execution and Textual Reasoning**
2410.03524v1 by Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, Chi Wang

While a lot of recent research focuses on enhancing the textual reasoning
capabilities of Large Language Models (LLMs) by optimizing the multi-agent
framework or reasoning chains, several benchmark tasks can be solved with 100%
success through direct coding, which is more scalable and avoids the
computational overhead associated with textual iterating and searching. Textual
reasoning has inherent limitations in solving tasks with challenges in math,
logics, optimization, and searching, which is unlikely to be solved by simply
scaling up the model and data size. The recently released OpenAI GPT Code
Interpreter and multi-agent frameworks such as AutoGen have demonstrated
remarkable proficiency of integrating code generation and execution to solve
complex tasks using LLMs. However, based on our experiments on 7 existing
popular methods for steering code/text generation in both single- and
multi-turn settings with 14 tasks and 6 types of LLMs (including the new
O1-preview), currently there is no optimal method to correctly steer LLMs to
write code when needed. We discover some interesting patterns on when models
use code vs. textual reasoning with the evolution to task complexity and model
sizes, which even result in an astonishingly inverse scaling law. We also
discover that results from LLM written code are not always better than using
textual reasoning, even if the task could be solved through code. To mitigate
the above issues, we propose three methods to better steer LLM code/text
generation and achieve a notable improvement. The costs of token lengths and
runtime are thoroughly discussed for all the methods. We believe the problem of
steering LLM code/text generation is critical for future research and has much
space for further improvement. Project Page, Datasets, and Codes are available
at https://yongchao98.github.io/CodeSteer/.

摘要：<paragraph>儘管許多近期研究專注於透過最佳化多重代理架構或推理鏈來增強大型語言模型 (LLM) 的文字推理能力，但幾個基準任務可透過直接編碼解決，並獲得 100% 的成功率，這更具可擴充性，並避免與文字反覆運算及搜尋相關的運算開銷。文字推理在解決數學、邏輯、最佳化和搜尋等挑戰任務時有其固有限制，這不太可能透過單純擴充模型和資料大小來解決。最近發布的 OpenAI GPT 程式碼解譯器和 AutoGen 等多重代理架構已展示出整合程式碼產生和執行以使用 LLM 解決複雜任務的顯著能力。然而，根據我們對 7 種現有流行方法的實驗，這些方法用於在單回合和多回合設定中引導程式碼/文字產生，並使用 14 項任務和 6 種類型的 LLM（包括新的 O1-preview），目前沒有最佳方法可以正確引導 LLM 在需要時撰寫程式碼。我們發現了一些有趣的模式，說明模型何時使用程式碼相對於文字推理，以及任務複雜度和模型大小的演變，這甚至導致了一個驚人的反向擴充法則。我們還發現，即使任務可以透過程式碼解決，LLM 編寫程式碼的結果並不總是優於使用文字推理。為了減輕上述問題，我們提出了三種方法來更好地引導 LLM 程式碼/文字產生，並獲得顯著的改進。所有方法的權杖長度和執行時間成本都經過徹底討論。我們相信引導 LLM 程式碼/文字產生的問題對於未來的研究至關重要，並且有很大的改進空間。專案頁面、資料集和程式碼可在 https://yongchao98.github.io/CodeSteer/ 取得。</paragraph>

##### **A Probabilistic Perspective on Unlearning and Alignment for Large Language Models**
2410.03523v1 by Yan Scholten, Stephan Günnemann, Leo Schwinn

Comprehensive evaluation of Large Language Models (LLMs) is an open research
problem. Existing evaluations rely on deterministic point estimates generated
via greedy decoding. However, we find that deterministic evaluations fail to
capture the whole output distribution of a model, yielding inaccurate
estimations of model capabilities. This is particularly problematic in critical
contexts such as unlearning and alignment, where precise model evaluations are
crucial. To remedy this, we introduce the first formal probabilistic evaluation
framework in LLMs. Namely, we derive novel metrics with high-probability
guarantees concerning the output distribution of a model. Our metrics are
application-independent and allow practitioners to make more reliable estimates
about model capabilities before deployment. Through a case study focused on
unlearning, we reveal that deterministic evaluations falsely indicate
successful unlearning, whereas our probabilistic evaluations demonstrate that
most if not all of the supposedly unlearned information remains accessible in
these models. Additionally, we propose a novel unlearning loss based on entropy
optimization and adaptive temperature scaling, which significantly improves
unlearning in probabilistic settings on recent benchmarks. Our proposed shift
from point estimates to probabilistic evaluations of output distributions
represents an important step toward comprehensive evaluations of LLMs.
https://github.com/yascho/probabilistic-unlearning

摘要：大型語言模型 (LLM) 的全面評估是一個開放的研究問題。現有的評估依賴於通過貪婪解碼產生的確定性點估計。然而，我們發現確定性評估無法捕捉模型的整個輸出分佈，從而對模型能力產生不準確的估計。這在關鍵語境（例如取消學習和對齊）中尤為成問題，在這些語境中，精確的模型評估至關重要。為了補救這一問題，我們引入了 LLM 中的第一個正式機率評估框架。具體而言，我們推導出具有關於模型輸出分佈的高機率保證的新穎指標。我們的指標與應用無關，並允許實務工作者在部署之前對模型能力做出更可靠的估計。通過專注於取消學習的案例研究，我們揭示確定性評估錯誤地表明取消學習成功，而我們的機率評估表明，這些模型中大部分（如果不是全部）假設取消學習的資訊仍然可以存取。此外，我們提出了一種基於熵最佳化和自適應溫度調整的新穎取消學習損失，這顯著改善了最近基準測試中機率設定中的取消學習。我們提出的從點估計到輸出分佈的機率評估的轉變，代表了朝著 LLM 全面評估邁出的重要一步。
https://github.com/yascho/probabilistic-unlearning

##### **CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios**
2410.03502v1 by Zetian Ouyang, Yishuai Qiu, Linlin Wang, Gerard de Melo, Ya Zhang, Yanfeng Wang, Liang He

With the proliferation of Large Language Models (LLMs) in diverse domains,
there is a particular need for unified evaluation standards in clinical medical
scenarios, where models need to be examined very thoroughly. We present
CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical
scenarios specifically designed to assess the medical ability of LLMs across 7
pivot dimensions. It comprises 33,735 questions derived from real-world medical
reports of top-tier tertiary hospitals and authentic examination exercises. The
reliability of this benchmark has been confirmed in several ways. Subsequent
experiments with existing LLMs have led to the following findings: (i) Chinese
medical LLMs underperform on this benchmark, especially where medical reasoning
and factual consistency are vital, underscoring the need for advances in
clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs
demonstrate substantial potential in medical clinics, while the limited input
capacity of many medical LLMs hinders their practical use. These findings
reveal both the strengths and limitations of LLMs in clinical scenarios and
offer critical insights for medical research.

摘要：隨著大型語言模型 (LLM) 在不同領域的普及，在需要非常徹底地檢查模型的臨床醫療場景中，特別需要統一的評估標準。我們提出 CliMedBench，這是一個全面的基準，包含 14 個專家指導的核心臨床場景，專門用於評估 LLM 在 7 個樞紐維度中的醫療能力。它包含 33,735 個問題，這些問題來自頂級三級醫院的真實醫療報告和真實的考試練習。此基準的可靠性已通過多種方式得到確認。隨後對現有 LLM 進行的實驗得出以下發現：(i) 中文醫療 LLM 在此基準上的表現不佳，尤其是在醫療推理和事實一致性至關重要的情況下，這強調了對臨床知識和診斷準確性進步的需求。(ii) 幾個通用領域的 LLM 在醫療診所中表現出巨大的潛力，而許多醫療 LLM 的輸入容量有限，阻礙了它們的實際使用。這些發現揭示了 LLM 在臨床場景中的優點和局限性，並為醫學研究提供了重要的見解。

##### **FedStein: Enhancing Multi-Domain Federated Learning Through James-Stein Estimator**
2410.03499v1 by Sunny Gupta, Nikita Jangid, Amit Sethi

Federated Learning (FL) facilitates data privacy by enabling collaborative
in-situ training across decentralized clients. Despite its inherent advantages,
FL faces significant challenges of performance and convergence when dealing
with data that is not independently and identically distributed (non-i.i.d.).
While previous research has primarily addressed the issue of skewed label
distribution across clients, this study focuses on the less explored challenge
of multi-domain FL, where client data originates from distinct domains with
varying feature distributions. We introduce a novel method designed to address
these challenges FedStein: Enhancing Multi-Domain Federated Learning Through
the James-Stein Estimator. FedStein uniquely shares only the James-Stein (JS)
estimates of batch normalization (BN) statistics across clients, while
maintaining local BN parameters. The non-BN layer parameters are exchanged via
standard FL techniques. Extensive experiments conducted across three datasets
and multiple models demonstrate that FedStein surpasses existing methods such
as FedAvg and FedBN, with accuracy improvements exceeding 14% in certain
domains leading to enhanced domain generalization. The code is available at
https://github.com/sunnyinAI/FedStein

摘要：联邦学习 (FL) 透过在分散式客户端中启用协作原位训练，促进数据隐私。尽管有其固有优势，FL 在处理非独立同分布 (non-i.i.d.) 数据时，面临着性能和收敛方面的重大挑战。虽然先前的研究主要解决了客户端间标签分布偏差的问题，但本研究专注于探索较少的挑战，即多领域 FL，其中客户端数据源自具有不同特征分布的不同领域。我们引入了一种新方法，旨在解决这些挑战 FedStein：通过 James-Stein 估计器增强多领域联邦学习。FedStein 独特地仅在客户端间共享批次规范化 (BN) 统计的 James-Stein (JS) 估计，同时维护本地 BN 参数。非 BN 层参数通过标准 FL 技术交换。在三个数据集和多个模型上进行的广泛实验表明，FedStein 超越了现有的方法，例如 FedAvg 和 FedBN，在某些领域的准确性提高超过 14%，从而增强了领域泛化。代码可在 https://github.com/sunnyinAI/FedStein 获得

##### **Generative Artificial Intelligence for Navigating Synthesizable Chemical Space**
2410.03494v1 by Wenhao Gao, Shitong Luo, Connor W. Coley

We introduce SynFormer, a generative modeling framework designed to
efficiently explore and navigate synthesizable chemical space. Unlike
traditional molecular generation approaches, we generate synthetic pathways for
molecules to ensure that designs are synthetically tractable. By incorporating
a scalable transformer architecture and a diffusion module for building block
selection, SynFormer surpasses existing models in synthesizable molecular
design. We demonstrate SynFormer's effectiveness in two key applications: (1)
local chemical space exploration, where the model generates synthesizable
analogs of a reference molecule, and (2) global chemical space exploration,
where the model aims to identify optimal molecules according to a black-box
property prediction oracle. Additionally, we demonstrate the scalability of our
approach via the improvement in performance as more computational resources
become available. With our code and trained models openly available, we hope
that SynFormer will find use across applications in drug discovery and
materials science.

摘要：我們介紹 SynFormer，這是一個生成模型架構，旨在有效探索和導航可合成化學空間。與傳統分子生成方法不同，我們為分子生成合成路徑，以確保設計在合成上可行。透過整合可擴充的Transformer架構和用於構件選擇的擴散模組，SynFormer 在可合成分子設計中超越現有模型。我們在兩個關鍵應用中展示了 SynFormer 的效能：(1) 局部化學空間探索，其中模型會生成參考分子的可合成類比物，以及 (2) 全域化學空間探索，其中模型旨在根據黑盒屬性預測神諭來識別最佳分子。此外，我們透過在更多運算資源可用時效能的提升，展示了我們方法的可擴充性。我們的程式碼和訓練好的模型公開可用，我們希望 SynFormer 能在藥物發現和材料科學的各種應用中發揮作用。

##### **Towards Reproducible LLM Evaluation: Quantifying Uncertainty in LLM Benchmark Scores**
2410.03492v1 by Robert E. Blackwell, Jon Barry, Anthony G. Cohn

Large language models (LLMs) are stochastic, and not all models give
deterministic answers, even when setting temperature to zero with a fixed
random seed. However, few benchmark studies attempt to quantify uncertainty,
partly due to the time and cost of repeated experiments. We use benchmarks
designed for testing LLMs' capacity to reason about cardinal directions to
explore the impact of experimental repeats on mean score and prediction
interval. We suggest a simple method for cost-effectively quantifying the
uncertainty of a benchmark score and make recommendations concerning
reproducible LLM evaluation.

摘要：大型語言模型 (LLM) 是隨機的，並非所有模型都會給出確定性的答案，即使在固定隨機種子時將溫度設定為零。然而，很少有基準研究嘗試量化不確定性，部分原因是重複實驗的時間和成本。我們使用專門用於測試 LLM 推論基數方向能力的基準來探討實驗重複對平均分數和預測區間的影響。我們提出了一種簡單的方法來經濟有效地量化基準分數的不確定性，並就可重複的 LLM 評估提出建議。

##### **Gradient-based Jailbreak Images for Multimodal Fusion Models**
2410.03489v1 by Javier Rando, Hannah Korevaar, Erik Brinkman, Ivan Evtimov, Florian Tramèr

Augmenting language models with image inputs may enable more effective
jailbreak attacks through continuous optimization, unlike text inputs that
require discrete optimization. However, new multimodal fusion models tokenize
all input modalities using non-differentiable functions, which hinders
straightforward attacks. In this work, we introduce the notion of a tokenizer
shortcut that approximates tokenization with a continuous function and enables
continuous optimization. We use tokenizer shortcuts to create the first
end-to-end gradient image attacks against multimodal fusion models. We evaluate
our attacks on Chameleon models and obtain jailbreak images that elicit harmful
information for 72.5% of prompts. Jailbreak images outperform text jailbreaks
optimized with the same objective and require 3x lower compute budget to
optimize 50x more input tokens. Finally, we find that representation
engineering defenses, like Circuit Breakers, trained only on text attacks can
effectively transfer to adversarial image inputs.

摘要：透過連續最佳化，使用影像輸入擴增語言模型可能讓監獄破解攻擊更有效率，這與需要離散最佳化的文字輸入不同。然而，新的多模態融合模型使用不可微分的函數對所有輸入模式進行標記化，這阻礙了直接的攻擊。在這項工作中，我們引入了標記化捷徑的概念，它使用連續函數近似標記化，並啟用了連續最佳化。我們使用標記化捷徑，針對多模態融合模型建立了第一個端對端梯度影像攻擊。我們在變色龍模型上評估我們的攻擊，並取得了對 72.5% 的提示引發有害資訊的監獄破解影像。監獄破解影像優於使用相同目標最佳化的文字監獄破解，且最佳化 50 倍更多的輸入標記只需要 3 倍較低的運算預算。最後，我們發現僅針對文字攻擊訓練的表示工程防禦措施（例如電路中斷器）可以有效轉移到對抗性的影像輸入。

##### **A Multimodal Framework for Deepfake Detection**
2410.03487v1 by Kashish Gandhi, Prutha Kulkarni, Taran Shah, Piyush Chaudhari, Meera Narvekar, Kranti Ghag

The rapid advancement of deepfake technology poses a significant threat to
digital media integrity. Deepfakes, synthetic media created using AI, can
convincingly alter videos and audio to misrepresent reality. This creates risks
of misinformation, fraud, and severe implications for personal privacy and
security. Our research addresses the critical issue of deepfakes through an
innovative multimodal approach, targeting both visual and auditory elements.
This comprehensive strategy recognizes that human perception integrates
multiple sensory inputs, particularly visual and auditory information, to form
a complete understanding of media content. For visual analysis, a model that
employs advanced feature extraction techniques was developed, extracting nine
distinct facial characteristics and then applying various machine learning and
deep learning models. For auditory analysis, our model leverages
mel-spectrogram analysis for feature extraction and then applies various
machine learning and deep learningmodels. To achieve a combined analysis, real
and deepfake audio in the original dataset were swapped for testing purposes
and ensured balanced samples. Using our proposed models for video and audio
classification i.e. Artificial Neural Network and VGG19, the overall sample is
classified as deepfake if either component is identified as such. Our
multimodal framework combines visual and auditory analyses, yielding an
accuracy of 94%.

摘要：深度偽造技術的快速進展對數位媒體的真實性構成重大威脅。深度偽造，使用 AI 建立的合成媒體，可以令人信服地改變影片和音訊，以曲解事實。這會造成錯誤資訊、詐騙的風險，並對個人隱私和安全造成嚴重影響。我們的研究透過創新的多模式方法來解決深度偽造的關鍵問題，針對視覺和聽覺元素。這個全面的策略承認人類感知會整合多重感官輸入，尤其是視覺和聽覺資訊，以形成對媒體內容的完整理解。對於視覺分析，開發了一個採用先進特徵萃取技術的模型，萃取出九種不同的臉部特徵，然後應用各種機器學習和深度學習模型。對於聽覺分析，我們的模型利用梅爾頻譜圖分析進行特徵萃取，然後應用各種機器學習和深度學習模型。為了達成結合分析，將原始資料集中真實的和深度偽造的音訊交換進行測試，並確保樣本平衡。使用我們提出的影片和音訊分類模型，即人工神經網路和 VGG19，如果任一組成被識別為深度偽造，則整體樣本被分類為深度偽造。我們的多模式架構結合視覺和聽覺分析，產生 94% 的準確度。

##### **Group Fairness in Peer Review**
2410.03474v1 by Haris Aziz, Evi Micha, Nisarg Shah

Large conferences such as NeurIPS and AAAI serve as crossroads of various AI
fields, since they attract submissions from a vast number of communities.
However, in some cases, this has resulted in a poor reviewing experience for
some communities, whose submissions get assigned to less qualified reviewers
outside of their communities. An often-advocated solution is to break up any
such large conference into smaller conferences, but this can lead to isolation
of communities and harm interdisciplinary research. We tackle this challenge by
introducing a notion of group fairness, called the core, which requires that
every possible community (subset of researchers) to be treated in a way that
prevents them from unilaterally benefiting by withdrawing from a large
conference.
  We study a simple peer review model, prove that it always admits a reviewing
assignment in the core, and design an efficient algorithm to find one such
assignment. We use real data from CVPR and ICLR conferences to compare our
algorithm to existing reviewing assignment algorithms on a number of metrics.

摘要：大型會議，例如 NeurIPS 和 AAAI，是各種 AI 領域的交叉點，因為它們吸引了來自大量社群的投稿。
然而，在某些情況下，這導致某些社群的審查體驗不佳，其投稿被分配給社群外較不合格的審查者。
經常提倡的解決方案是將任何此類大型會議分拆為較小的會議，但這可能導致社群孤立並損害跨領域研究。
我們透過引入稱為核心的小組公平性概念來應對此挑戰，該概念要求每個可能的社群（研究人員子集）都受到公平對待，以防止他們因退出大型會議而單方面受益。
我們研究了一個簡單的同儕審查模型，證明它總是在核心部分承認審查指派，並設計了一個有效的演算法來找到一個此類指派。
我們使用來自 CVPR 和 ICLR 會議的真實資料，根據多項指標將我們的演算法與現有的審查指派演算法進行比較。

##### **Vulnerability Detection via Topological Analysis of Attention Maps**
2410.03470v1 by Pavel Snopov, Andrey Nikolaevich Golubinskiy

Recently, deep learning (DL) approaches to vulnerability detection have
gained significant traction. These methods demonstrate promising results, often
surpassing traditional static code analysis tools in effectiveness.
  In this study, we explore a novel approach to vulnerability detection
utilizing the tools from topological data analysis (TDA) on the attention
matrices of the BERT model. Our findings reveal that traditional machine
learning (ML) techniques, when trained on the topological features extracted
from these attention matrices, can perform competitively with pre-trained
language models (LLMs) such as CodeBERTa. This suggests that TDA tools,
including persistent homology, are capable of effectively capturing semantic
information critical for identifying vulnerabilities.

摘要：近来，针对漏洞侦测的深度学习 (DL) 方法获得了显著的进展。这些方法展示了有希望的结果，在有效性上通常超越了传统的静态代码分析工具。
在这项研究中，我们探索了一种新颖的漏洞侦测方法，利用拓扑数据分析 (TDA) 中的工具对 BERT 模型的注意力矩阵进行分析。我们的发现显示，当在从这些注意力矩阵中提取的拓扑特征上进行训练时，传统的机器学习 (ML) 技术可以与预训练语言模型 (LLM)，例如 CodeBERTa，进行竞争。这表明 TDA 工具，包括持久同调，能够有效地捕捉识别漏洞至关重要的语义信息。

##### **Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering**
2410.03466v1 by Helena Bonaldi, Greta Damo, Nicolás Benjamín Ocampo, Elena Cabrio, Serena Villata, Marco Guerini

The potential effectiveness of counterspeech as a hate speech mitigation
strategy is attracting increasing interest in the NLG research community,
particularly towards the task of automatically producing it. However,
automatically generated responses often lack the argumentative richness which
characterises expert-produced counterspeech. In this work, we focus on two
aspects of counterspeech generation to produce more cogent responses. First, by
investigating the tension between helpfulness and harmlessness of LLMs, we test
whether the presence of safety guardrails hinders the quality of the
generations. Secondly, we assess whether attacking a specific component of the
hate speech results in a more effective argumentative strategy to fight online
hate. By conducting an extensive human and automatic evaluation, we show how
the presence of safety guardrails can be detrimental also to a task that
inherently aims at fostering positive social interactions. Moreover, our
results show that attacking a specific component of the hate speech, and in
particular its implicit negative stereotype and its hateful parts, leads to
higher-quality generations.

摘要：反制言論作為仇恨言論緩解策略的潛在效用，在自然語言處理研究社群中引起越來越大的興趣，特別是在自動產生反制言論的任務上。然而，自動產生的回應通常缺乏論證的豐富性，而這是專家產生的反制言論的特徵。在這項工作中，我們專注於反制言論產生的兩個面向，以產生更有說服力的回應。首先，透過探討大型語言模型（LLM）的有用性和無害性之間的緊張關係，我們測試了安全防護措施的存在是否會妨礙產生的品質。其次，我們評估攻擊仇恨言論的特定組成部分是否會產生更有效的論證策略，以對抗網路仇恨。透過進行廣泛的人工和自動評估，我們展示了安全防護措施的存在如何也會對本質上旨在促進正面社交互動的任務產生不利影響。此外，我們的結果顯示，攻擊仇恨言論的特定組成部分，特別是其隱含的負面刻板印象及其可憎的部分，會產生更高品質的回應。

##### **Diffusion State-Guided Projected Gradient for Inverse Problems**
2410.03463v1 by Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar

Recent advancements in diffusion models have been effective in learning data
priors for solving inverse problems. They leverage diffusion sampling steps for
inducing a data prior while using a measurement guidance gradient at each step
to impose data consistency. For general inverse problems, approximations are
needed when an unconditionally trained diffusion model is used since the
measurement likelihood is intractable, leading to inaccurate posterior
sampling. In other words, due to their approximations, these methods fail to
preserve the generation process on the data manifold defined by the diffusion
prior, leading to artifacts in applications such as image restoration. To
enhance the performance and robustness of diffusion models in solving inverse
problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad),
which projects the measurement gradient onto a subspace that is a low-rank
approximation of an intermediate state of the diffusion process. DiffStateGrad,
as a module, can be added to a wide range of diffusion-based inverse solvers to
improve the preservation of the diffusion process on the prior manifold and
filter out artifact-inducing components. We highlight that DiffStateGrad
improves the robustness of diffusion models in terms of the choice of
measurement guidance step size and noise while improving the worst-case
performance. Finally, we demonstrate that DiffStateGrad improves upon the
state-of-the-art on linear and nonlinear image restoration inverse problems.

摘要：擴散模型的最新進展在學習資料先驗以解決反問題方面非常有效。它們利用擴散採樣步驟來誘導資料先驗，同時在每個步驟中使用量測引導梯度來強加資料一致性。對於一般的反問題，在使用無條件訓練的擴散模型時需要近似，因為量測似然函數難以處理，導致後驗採樣不準確。換句話說，由於這些方法的近似，它們無法在由擴散先驗定義的資料流形上保留生成過程，導致在影像復原等應用中產生人工製品。為了增強擴散模型在解決反問題時的效能和穩健性，我們提出了擴散狀態引導投影梯度（DiffStateGrad），它將量測梯度投影到一個子空間上，該子空間是擴散過程的中間狀態的低秩近似。DiffStateGrad 作為一個模組，可以新增到各種基於擴散的反問題求解器中，以改善在先驗流形上保留擴散過程並過濾掉產生人工製品的組成部分。我們強調，DiffStateGrad 在量測引導步長和雜訊的選擇方面改善了擴散模型的穩健性，同時改善了最差情況下的效能。最後，我們證明了 DiffStateGrad 在線性和非線性影像復原反問題上改進了最先進的技術。

##### **Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation**
2410.03461v1 by Tobias Leemann, Periklis Petridis, Giuseppe Vietri, Dionysis Manousakas, Aaron Roth, Sergul Aydore

While retrieval augmented generation (RAG) has been shown to enhance
factuality of large language model (LLM) outputs, LLMs still suffer from
hallucination, generating incorrect or irrelevant information. One common
detection strategy involves prompting the LLM again to assess whether its
response is grounded in the retrieved evidence, but this approach is costly.
Alternatively, lightweight natural language inference (NLI) models for
efficient grounding verification can be used at inference time. While existing
pre-trained NLI models offer potential solutions, their performance remains
subpar compared to larger models on realistic RAG inputs. RAG inputs are more
complex than most datasets used for training NLI models and have
characteristics specific to the underlying knowledge base, requiring adaptation
of the NLI models to a specific target domain. Additionally, the lack of
labeled instances in the target domain makes supervised domain adaptation,
e.g., through fine-tuning, infeasible. To address these challenges, we
introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework
enables unsupervised domain adaptation through synthetic data generation.
Unlike previous methods that rely on handcrafted filtering and augmentation
strategies, Auto-GDA employs an iterative process to continuously improve the
quality of generated samples using weak labels from less efficient teacher
models and discrete optimization to select the most promising augmented
samples. Experimental results demonstrate the effectiveness of our approach,
with models fine-tuned on synthetic data using Auto-GDA often surpassing the
performance of the teacher model and reaching the performance level of LLMs at
10 % of their computational cost.

摘要：<paragraph>雖然檢索增強生成 (RAG) 已被證明可以增強大型語言模型 (LLM) 輸出的真實性，但 LLM 仍然會出現幻覺，產生不正確或不相關的資訊。一種常見的偵測策略涉及再次提示 LLM 以評估其回應是否建立在檢索到的證據上，但這種方法成本很高。或者，可以在推理時使用輕量級自然語言推理 (NLI) 模型進行有效的基礎驗證。雖然現有的預訓練 NLI 模型提供了潛在的解決方案，但與針對實際 RAG 輸入的較大型模型相比，它們的效能仍然較差。RAG 輸入比用於訓練 NLI 模型的大多數資料集更為複雜，並且具有特定於基礎知識庫的特徵，需要將 NLI 模型調整到特定的目標網域。此外，目標網域中缺乏標籤實例會使監督網域適應（例如透過微調）變得不可行。為了應對這些挑戰，我們引入了自動生成網域適應 (Auto-GDA)。我們的架構透過合成資料生成啟用無監督網域適應。與依賴手工過濾和增強策略的先前方法不同，Auto-GDA 採用反覆運算的程序，以持續改善生成樣本的品質，方法是使用效率較低的教師模型中的弱標籤，並透過離散最佳化來選擇最有希望的增強樣本。實驗結果證明了我們方法的有效性，使用 Auto-GDA 對合成資料進行微調的模型通常優於教師模型的效能，並達到 LLM 效能水準，且計算成本僅為其 10%。</paragraph>

##### **Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges**
2410.03458v1 by Nguyen Van Dinh, Thanh Chi Dang, Luan Thanh Nguyen, Kiet Van Nguyen

Vietnamese, a low-resource language, is typically categorized into three
primary dialect groups that belong to Northern, Central, and Southern Vietnam.
However, each province within these regions exhibits its own distinct
pronunciation variations. Despite the existence of various speech recognition
datasets, none of them has provided a fine-grained classification of the 63
dialects specific to individual provinces of Vietnam. To address this gap, we
introduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive
dataset capturing the rich diversity of 63 provincial dialects spoken across
Vietnam. Our dataset comprises 102.56 hours of audio, consisting of
approximately 19,000 utterances, and the associated transcripts contain over
1.2 million words. To provide benchmarks and simultaneously demonstrate the
challenges of our dataset, we fine-tune state-of-the-art pre-trained models for
two downstream tasks: (1) Dialect identification and (2) Speech recognition.
The empirical results suggest two implications including the influence of
geographical factors on dialects, and the constraints of current approaches in
speech recognition tasks involving multi-dialect speech data. Our dataset is
available for research purposes.

摘要：越南語是一種低資源語言，通常分為三個主要方言組，分別屬於越南北部、中部和南部。然而，這些地區內的每個省份都展現出自己獨特的發音變化。儘管存在各種語音辨識資料集，但沒有一個資料集對越南各省的 63 種方言提供細緻的分類。為了解決這個差距，我們引入了越南多方言 (ViMD) 資料集，這是一個新穎的綜合資料集，捕捉了越南各地 63 種省級方言的豐富多樣性。我們的資料集包含 102.56 小時的音訊，包含大約 19,000 個語句，相關的轉錄包含超過 120 萬個字。為了提供基準，並同時展示我們資料集的挑戰，我們微調了最先進的預訓練模型，以執行兩個下游任務：(1) 方言辨識和 (2) 語音辨識。實證結果顯示了兩個含意，包括地理因素對方言的影響，以及當前方法在涉及多方言語音資料的語音辨識任務中的限制。我們的資料集可供研究用途。

##### **CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds**
2410.03457v1 by Min-Hsuan Yeh, Ruyuan Wan, Ting-Hao 'Kenneth' Huang

Detecting logical fallacies in texts can help users spot argument flaws, but
automating this detection is not easy. Manually annotating fallacies in
large-scale, real-world text data to create datasets for developing and
validating detection models is costly. This paper introduces CoCoLoFa, the
largest known logical fallacy dataset, containing 7,706 comments for 648 news
articles, with each comment labeled for fallacy presence and type. We recruited
143 crowd workers to write comments embodying specific fallacy types (e.g.,
slippery slope) in response to news articles. Recognizing the complexity of
this writing task, we built an LLM-powered assistant into the workers'
interface to aid in drafting and refining their comments. Experts rated the
writing quality and labeling validity of CoCoLoFa as high and reliable.
BERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy
detection (F1=0.86) and classification (F1=0.87) performance on its test set,
outperforming the state-of-the-art LLMs. Our work shows that combining
crowdsourcing and LLMs enables us to more effectively construct datasets for
complex linguistic phenomena that crowd workers find challenging to produce on
their own.

摘要：透過在文本中偵測邏輯謬誤，可以協助使用者找出論點中的缺陷，但自動化偵測並不容易。手動註記大規模、真實世界的文字資料中的謬誤，以建立用於開發和驗證偵測模型的資料集，成本很高。本文介紹了 CoCoLoFa，這是已知最大的邏輯謬誤資料集，包含了 648 篇新聞文章的 7,706 則留言，每則留言都標記了謬誤的存在和類型。我們招募了 143 位群眾工作者，針對新聞文章撰寫體現特定謬誤類型（例如滑坡謬誤）的留言。我們了解到這項寫作任務的複雜性，因此在工作者的介面中建置了一個由 LLM 驅動的助理，協助起草和修改他們的留言。專家評估 CoCoLoFa 的寫作品質和標記有效性，認為其品質高且可靠。使用 CoCoLoFa 微調的 BERT 基礎模型在其測試集上達到了最高的謬誤偵測 (F1=0.86) 和分類 (F1=0.87) 效能，優於現有最先進的 LLM。我們的研究顯示，結合群眾外包和 LLM 能讓我們更有效地建構出資料集，以處理群眾工作者自己難以產出的複雜語言現象。

##### **How Toxicity Classifiers and Large Language Models Respond to Ableism**
2410.03448v1 by Mahika Phutane, Ananya Seelam, Aditya Vashistha

People with disabilities (PwD) regularly encounter ableist hate and
microaggressions online. While online platforms use machine learning models to
moderate online harm, there is little research investigating how these models
interact with ableism. In this paper, we curated a dataset of 100 social media
comments targeted towards PwD, and recruited 160 participants to rate and
explain how toxic and ableist these comments were. We then prompted
state-of-the art toxicity classifiers (TCs) and large language models (LLMs) to
rate and explain the harm. Our analysis revealed that TCs and LLMs rated
toxicity significantly lower than PwD, but LLMs rated ableism generally on par
with PwD. However, ableism explanations by LLMs overlooked emotional harm, and
lacked specificity and acknowledgement of context, important facets of PwD
explanations. Going forward, we discuss challenges in designing
disability-aware toxicity classifiers, and advocate for the shift from ableism
detection to ableism interpretation and explanation.

摘要：身障者（PwD）經常在網路上遭遇歧視身障者的仇恨言論和微歧視。儘管網路平台使用機器學習模型來緩解網路上的危害，但鮮少有研究探討這些模型如何與歧視身障者產生互動。在本文中，我們策劃了一組針對身障者的 100 則社群媒體留言資料集，並招募了 160 位參與者來評分並說明這些留言有多麼具有毒性和歧視性。然後，我們提示最先進的毒性分類器 (TC) 和大型語言模型 (LLM) 來評分並說明這些危害。我們的分析顯示，TC 和 LLM 評定的毒性顯著低於身障者，但 LLM 評定的歧視性通常與身障者相當。然而，LLM 對歧視性的解釋忽視了情緒傷害，並且缺乏對脈絡的具體說明和認知，這是身障者解釋中的重要面向。展望未來，我們將討論設計身障者感知毒性分類器的挑戰，並倡導從歧視身障者偵測轉變為歧視身障者詮釋和說明。

##### **How Language Models Prioritize Contextual Grammatical Cues?**
2410.03447v1 by Hamidreza Amirzadeh, Afra Alishahi, Hosein Mohebbi

Transformer-based language models have shown an excellent ability to
effectively capture and utilize contextual information. Although various
analysis techniques have been used to quantify and trace the contribution of
single contextual cues to a target task such as subject-verb agreement or
coreference resolution, scenarios in which multiple relevant cues are available
in the context remain underexplored. In this paper, we investigate how language
models handle gender agreement when multiple gender cue words are present, each
capable of independently disambiguating a target gender pronoun. We analyze two
widely used Transformer-based models: BERT, an encoder-based, and GPT-2, a
decoder-based model. Our analysis employs two complementary approaches: context
mixing analysis, which tracks information flow within the model, and a variant
of activation patching, which measures the impact of cues on the model's
prediction. We find that BERT tends to prioritize the first cue in the context
to form both the target word representations and the model's prediction, while
GPT-2 relies more on the final cue. Our findings reveal striking differences in
how encoder-based and decoder-based models prioritize and use contextual
information for their predictions.

摘要：基於 Transformer 的語言模型已展現出極佳的能力，能有效擷取和利用脈絡資訊。儘管已使用各種分析技術來量化和追蹤單一脈絡線索對目標任務（例如主詞動詞一致或共同參照解析）的貢獻，但對於在脈絡中有多個相關線索可用的場景仍未充分探討。在本文中，我們探討語言模型在存在多個性別線索字詞時如何處理性別一致，每個線索字詞都能獨立消除目標性別代名詞的歧義。我們分析兩個廣泛使用的基於 Transformer 的模型：BERT（基於編碼器）和 GPT-2（基於解碼器）模型。我們的分析採用兩種互補方法：脈絡混合分析（追蹤模型內的資訊流動）和一種活化修補程式（測量線索對模型預測的影響）。我們發現 BERT 傾向於優先考慮脈絡中的第一個線索，以形成目標字詞表徵和模型預測，而 GPT-2 則更多依賴最後一個線索。我們的研究結果揭示了基於編碼器和基於解碼器的模型在優先順序和使用脈絡資訊進行預測方面存在顯著差異。

##### **On Uncertainty In Natural Language Processing**
2410.03446v1 by Dennis Ulmer

The last decade in deep learning has brought on increasingly capable systems
that are deployed on a wide variety of applications. In natural language
processing, the field has been transformed by a number of breakthroughs
including large language models, which are used in increasingly many
user-facing applications. In order to reap the benefits of this technology and
reduce potential harms, it is important to quantify the reliability of model
predictions and the uncertainties that shroud their development.
  This thesis studies how uncertainty in natural language processing can be
characterized from a linguistic, statistical and neural perspective, and how it
can be reduced and quantified through the design of the experimental pipeline.
We further explore uncertainty quantification in modeling by theoretically and
empirically investigating the effect of inductive model biases in text
classification tasks. The corresponding experiments include data for three
different languages (Danish, English and Finnish) and tasks as well as a large
set of different uncertainty quantification approaches. Additionally, we
propose a method for calibrated sampling in natural language generation based
on non-exchangeable conformal prediction, which provides tighter token sets
with better coverage of the actual continuation. Lastly, we develop an approach
to quantify confidence in large black-box language models using auxiliary
predictors, where the confidence is predicted from the input to and generated
output text of the target model alone.

摘要：深度學習的最後十年帶來了越來越有能力的系統，這些系統被部署在各種應用程式上。在自然語言處理中，這個領域已經被許多突破所改變，包括大型語言模型，這些模型被用於越來越多的面向使用者的應用程式中。為了獲取這項技術的好處並減少潛在的危害，量化模型預測的可靠性和籠罩其開發的不確定性非常重要。本論文研究了自然語言處理中的不確定性如何從語言學、統計學和神經網路的角度來表徵，以及如何通過實驗管線的設計來降低和量化不確定性。我們進一步探討了建模中的不確定性量化，通過理論和經驗研究了歸納模型偏差在文本分類任務中的影響。對應的實驗包括三種不同語言（丹麥語、英語和芬蘭語）的資料和任務，以及一大組不同的不確定性量化方法。此外，我們提出了一種基於不可交換一致性預測的校準抽樣方法，該方法提供了更嚴格的標記集，並更好地涵蓋了實際的延續。最後，我們開發了一種方法來量化大型黑盒語言模型中的信心，使用輔助預測器，其中信心僅從目標模型的輸入和生成的輸出文本中預測。

##### **Exploring the Benefit of Activation Sparsity in Pre-training**
2410.03440v1 by Zhengyan Zhang, Chaojun Xiao, Qiujieli Qin, Yankai Lin, Zhiyuan Zeng, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Jie Zhou

Pre-trained Transformers inherently possess the characteristic of sparse
activation, where only a small fraction of the neurons are activated for each
token. While sparse activation has been explored through post-training methods,
its potential in pre-training remains untapped. In this work, we first study
how activation properties change during pre-training. Our examination reveals
that Transformers exhibit sparse activation throughout the majority of the
pre-training process while the activation correlation keeps evolving as
training progresses. Leveraging this observation, we propose Switchable
Sparse-Dense Learning (SSD). SSD adaptively switches between the
Mixtures-of-Experts (MoE) based sparse training and the conventional dense
training during the pre-training process, leveraging the efficiency of sparse
training and avoiding the static activation correlation of sparse training.
Compared to dense training, SSD achieves comparable performance with identical
model size and reduces pre-training costs. Moreover, the models trained with
SSD can be directly used as MoE models for sparse inference and achieve the
same performance as dense models with up to $2\times$ faster inference speed.
Codes are available at https://github.com/thunlp/moefication.

摘要：預訓練轉換器本質上具有稀疏激活的特徵，其中只會為每個標記啟用一小部分神經元。儘管已透過後訓練方法探索稀疏激活，但其在預訓練中的潛力仍未開發。在這項工作中，我們首先研究在預訓練期間激活屬性如何改變。我們的檢查顯示，轉換器在預訓練過程的大部分時間中都表現出稀疏激活，而激活關聯會隨著訓練進度不斷演變。利用這一觀察結果，我們提出了可切換稀疏密集學習 (SSD)。SSD 在預訓練過程中自適應地在基於專家混合 (MoE) 的稀疏訓練和傳統密集訓練之間切換，利用稀疏訓練的效率並避免稀疏訓練的靜態激活關聯。與密集訓練相比，SSD 在相同的模型大小下實現了相當的效能，並降低了預訓練成本。此外，使用 SSD 訓練的模型可以直接用作 MoE 模型進行稀疏推論，並以高達 2 倍的推論速度實現與密集模型相同的效能。程式碼可在 https://github.com/thunlp/moefication 取得。

##### **ToolGen: Unified Tool Retrieval and Calling via Generation**
2410.03439v1 by Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, Haonan Li

As large language models (LLMs) advance, their inability to autonomously
execute tasks by directly interacting with external tools remains a critical
limitation. Traditional methods rely on inputting tool descriptions as context,
which is constrained by context length and requires separate, often
inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that
integrates tool knowledge directly into the LLM's parameters by representing
each tool as a unique token. This enables the LLM to generate tool calls and
arguments as part of its next token prediction capabilities, seamlessly
blending tool invocation with language generation. Our framework allows the LLM
to access and utilize a vast amount of tools with no additional retrieval step,
significantly enhancing both performance and scalability. Experimental results
with over 47,000 tools show that ToolGen not only achieves superior results in
both tool retrieval and autonomous task completion but also sets the stage for
a new era of AI agents that can adapt to tools across diverse domains. By
fundamentally transforming tool retrieval into a generative process, ToolGen
paves the way for more versatile, efficient, and autonomous AI systems. ToolGen
enables end-to-end tool learning and opens opportunities for integration with
other advanced techniques such as chain-of-thought and reinforcement learning,
thereby expanding the practical capabilities of LLMs.

摘要：隨著大型語言模型 (LLM) 的進步，它們無法透過直接與外部工具互動來自主執行任務，這仍然是一個嚴重的限制。傳統方法依賴於將工具描述作為上下文輸入，這受到上下文長度的限制，並且需要單獨的、通常低效的檢索機制。我們引入了 ToolGen，這是一個範式轉移，它透過將每個工具表示為一個獨特的符號，將工具知識直接整合到 LLM 的參數中。這使 LLM 能夠生成工具呼叫和參數，作為其下一個符號預測能力的一部分，將工具呼叫與語言生成無縫融合。我們的架構允許 LLM 訪問和使用大量的工具，而無需額外的檢索步驟，從而顯著提高效能和可擴充性。超過 47,000 個工具的實驗結果表明，ToolGen 不僅在工具檢索和自主任務完成方面取得了優異的成果，而且還為新時代的人工智慧代理奠定了基礎，這些代理可以適應不同領域的工具。透過從根本上將工具檢索轉變為一個生成過程，ToolGen 為更靈活、更有效率和更自主的人工智慧系統鋪平了道路。ToolGen 能夠進行端到端的工具學習，並開啟了與其他先進技術（例如思考鏈和強化學習）整合的機會，從而擴展了 LLM 的實用能力。

##### **A General Framework for Producing Interpretable Semantic Text Embeddings**
2410.03435v1 by Yiqun Sun, Qiang Huang, Yixuan Tang, Anthony K. H. Tung, Jun Yu

Semantic text embedding is essential to many tasks in Natural Language
Processing (NLP). While black-box models are capable of generating high-quality
embeddings, their lack of interpretability limits their use in tasks that
demand transparency. Recent approaches have improved interpretability by
leveraging domain-expert-crafted or LLM-generated questions, but these methods
rely heavily on expert input or well-prompt design, which restricts their
generalizability and ability to generate discriminative questions across a wide
range of tasks. To address these challenges, we introduce \algo{CQG-MBQA}
(Contrastive Question Generation - Multi-task Binary Question Answering), a
general framework for producing interpretable semantic text embeddings across
diverse tasks. Our framework systematically generates highly discriminative,
low cognitive load yes/no questions through the \algo{CQG} method and answers
them efficiently with the \algo{MBQA} model, resulting in interpretable
embeddings in a cost-effective manner. We validate the effectiveness and
interpretability of \algo{CQG-MBQA} through extensive experiments and ablation
studies, demonstrating that it delivers embedding quality comparable to many
advanced black-box models while maintaining inherently interpretability.
Additionally, \algo{CQG-MBQA} outperforms other interpretable text embedding
methods across various downstream tasks.

摘要：語意文字嵌入對於自然語言處理 (NLP) 中的許多任務至關重要。雖然黑箱模型能夠產生高品質的嵌入，但其缺乏可解釋性限制了它們在需要透明度的任務中的使用。最近的方法通過利用領域專家精心製作或 LLM 生成的問題改進了解釋性，但這些方法在很大程度上依賴於專家輸入或良好的提示設計，這限制了它們在廣泛任務中產生區別性問題的泛化性和能力。為了應對這些挑戰，我們引入了 \algo{CQG-MBQA}（對比問題生成 - 多任務二元問題解答），一個用於跨不同任務產生可解釋語意文字嵌入的通用框架。我們的框架通過 \algo{CQG} 方法系統性地生成高度區別性、低認知負載的是/否問題，並使用 \algo{MBQA} 模型有效地回答這些問題，從而以經濟高效的方式產生可解釋的嵌入。我們通過廣泛的實驗和消融研究驗證了 \algo{CQG-MBQA} 的有效性和可解釋性，證明了它提供了與許多先進黑箱模型相當的嵌入品質，同時保持了固有的可解釋性。此外，\algo{CQG-MBQA} 在各種下游任務中優於其他可解釋的文字嵌入方法。

##### **Self-supervised Spatio-Temporal Graph Mask-Passing Attention Network for Perceptual Importance Prediction of Multi-point Tactility**
2410.03434v1 by Dazhong He, Qian Liu

While visual and auditory information are prevalent in modern multimedia
systems, haptic interaction, e.g., tactile and kinesthetic interaction,
provides a unique form of human perception. However, multimedia technology for
contact interaction is less mature than non-contact multimedia technologies and
requires further development. Specialized haptic media technologies, requiring
low latency and bitrates, are essential to enable haptic interaction,
necessitating haptic information compression. Existing vibrotactile signal
compression methods, based on the perceptual model, do not consider the
characteristics of fused tactile perception at multiple spatially distributed
interaction points. In fact, differences in tactile perceptual importance are
not limited to conventional frequency and time domains, but also encompass
differences in the spatial locations on the skin unique to tactile perception.
For the most frequently used tactile information, vibrotactile texture
perception, we have developed a model to predict its perceptual importance at
multiple points, based on self-supervised learning and Spatio-Temporal Graph
Neural Network. Current experimental results indicate that this model can
effectively predict the perceptual importance of various points in multi-point
tactile perception scenarios.

摘要：在现代多媒体系统中，视觉和听觉信息无处不在，触觉交互（例如触觉和本体感觉交互）提供了一种独特的人类感知形式。然而，接触交互的多媒体技术不如非接触多媒体技术成熟，需要进一步开发。专门的触觉媒体技术需要低延迟和比特率，对于实现触觉交互至关重要，需要触觉信息压缩。现有的基于感知模型的振动触觉信号压缩方法没有考虑多个空间分布交互点上融合触觉感知的特征。事实上，触觉感知重要性的差异不仅限于传统的频率域和时域，还包括触觉感知特有的皮肤上空间位置的差异。对于最常用的触觉信息，振动触觉纹理感知，我们开发了一个模型来预测其在多个点上的感知重要性，该模型基于自监督学习和时空图神经网络。目前的实验结果表明，该模型可以有效预测多点触觉感知场景中各个点的感知重要性。

##### **Images Speak Volumes: User-Centric Assessment of Image Generation for Accessible Communication**
2410.03430v1 by Miriam Anschütz, Tringa Sylaj, Georg Groh

Explanatory images play a pivotal role in accessible and easy-to-read (E2R)
texts. However, the images available in online databases are not tailored
toward the respective texts, and the creation of customized images is
expensive. In this large-scale study, we investigated whether text-to-image
generation models can close this gap by providing customizable images quickly
and easily. We benchmarked seven, four open- and three closed-source, image
generation models and provide an extensive evaluation of the resulting images.
In addition, we performed a user study with people from the E2R target group to
examine whether the images met their requirements. We find that some of the
models show remarkable performance, but none of the models are ready to be used
at a larger scale without human supervision. Our research is an important step
toward facilitating the creation of accessible information for E2R creators and
tailoring accessible images to the target group's needs.

摘要：說明性圖片在無障礙且易於閱讀 (E2R) 的文字中扮演著關鍵角色。然而，線上資料庫中提供的圖片並非針對特定文字量身打造，而自訂圖片的製作成本又很高。在此項大規模研究中，我們調查文字轉圖片生成模型是否能快速且輕鬆地提供自訂圖片，以填補此一缺口。我們對七種圖片生成模型進行評比，其中四種為開放原始碼，三種為閉源，並對產出的圖片進行廣泛評估。此外，我們針對 E2R 目標族群進行使用者研究，以檢視圖片是否符合他們的需求。我們發現某些模型表現傑出，但若沒有人工監督，所有模型都無法大規模使用。我們的研究對於協助 E2R 創作人員製作無障礙資訊，並根據目標族群的需求量身打造無障礙圖片，是一個重要的進展。

##### **How Hard is this Test Set? NLI Characterization by Exploiting Training Dynamics**
2410.03429v1 by Adrian Cosma, Stefan Ruseti, Mihai Dascalu, Cornelia Caragea

Natural Language Inference (NLI) evaluation is crucial for assessing language
understanding models; however, popular datasets suffer from systematic spurious
correlations that artificially inflate actual model performance. To address
this, we propose a method for the automated creation of a challenging test set
without relying on the manual construction of artificial and unrealistic
examples. We categorize the test set of popular NLI datasets into three
difficulty levels by leveraging methods that exploit training dynamics. This
categorization significantly reduces spurious correlation measures, with
examples labeled as having the highest difficulty showing markedly decreased
performance and encompassing more realistic and diverse linguistic phenomena.
When our characterization method is applied to the training set, models trained
with only a fraction of the data achieve comparable performance to those
trained on the full dataset, surpassing other dataset characterization
techniques. Our research addresses limitations in NLI dataset construction,
providing a more authentic evaluation of model performance with implications
for diverse NLU applications.

摘要：自然語言推論 (NLI) 評估對於評估語言理解模型至關重要；然而，熱門資料集存在系統性的虛假相關性，這會人為地提升實際模型效能。為了解決這個問題，我們提出一個自動建立具挑戰性測試集的方法，而不需要人工建構不自然且不切實際的範例。我們透過利用訓練動態的方法，將熱門 NLI 資料集的測試集分類成三個難度等級。這種分類大幅降低虛假相關性測量，標記為難度最高級別的範例顯示出效能顯著下降，且包含更多現實且多樣的語言現象。當我們的特徵化方法套用於訓練集時，僅使用部分資料訓練的模型就能達到與使用完整資料集訓練的模型相近的效能，超越其他資料集特徵化技術。我們的研究解決了 NLI 資料集建構的限制，提供更真實的模型效能評估，並對各種 NLU 應用產生影響。

##### **Cayley Graph Propagation**
2410.03424v1 by JJ Wilson, Maya Bechler-Speicher, Petar Veličković

In spite of the plethora of success stories with graph neural networks (GNNs)
on modelling graph-structured data, they are notoriously vulnerable to
over-squashing, whereby tasks necessitate the mixing of information between
distance pairs of nodes. To address this problem, prior work suggests rewiring
the graph structure to improve information flow. Alternatively, a significant
body of research has dedicated itself to discovering and precomputing
bottleneck-free graph structures to ameliorate over-squashing. One well
regarded family of bottleneck-free graphs within the mathematical community are
expander graphs, with prior work$\unicode{x2014}$Expander Graph Propagation
(EGP)$\unicode{x2014}$proposing the use of a well-known expander graph
family$\unicode{x2014}$the Cayley graphs of the $\mathrm{SL}(2,\mathbb{Z}_n)$
special linear group$\unicode{x2014}$as a computational template for GNNs.
However, in EGP the computational graphs used are truncated to align with a
given input graph. In this work, we show that truncation is detrimental to the
coveted expansion properties. Instead, we propose CGP, a method to propagate
information over a complete Cayley graph structure, thereby ensuring it is
bottleneck-free to better alleviate over-squashing. Our empirical evidence
across several real-world datasets not only shows that CGP recovers significant
improvements as compared to EGP, but it is also akin to or outperforms
computationally complex graph rewiring techniques.

摘要：儘管圖形神經網路（GNN）在建模圖形結構資料方面有許多成功案例，但它們出了名容易發生過度壓縮，這會導致任務需要在距離成對的節點之間混合資訊。為了解決這個問題，先前的研究建議重新連接圖形結構以改善資訊流。或者，大量的研究致力於發現並預先計算無瓶頸的圖形結構以改善過度壓縮。在數學界中，一個備受推崇的無瓶頸圖形家族是膨脹圖，先前的研究$\unicode{x2014}$膨脹圖傳播 (EGP)$\unicode{x2014}$提出使用一個著名的膨脹圖家族$\unicode{x2014}$即 $\mathrm{SL}(2,\mathbb{Z}_n)$ 特殊線性群的凱萊圖$\unicode{x2014}$作為 GNN 的計算範本。然而，在 EGP 中，所使用的計算圖形被截斷以與給定的輸入圖形對齊。在這項工作中，我們表明截斷對令人垂涎的擴充屬性有害。相反，我們提出 CGP，這是一種在完整的凱萊圖形結構上傳播資訊的方法，從而確保它是無瓶頸的，以更好地緩解過度壓縮。我們在幾個真實世界資料集中的實證證據不僅表明 CGP 與 EGP 相比恢復了顯著的改進，而且還類似於或優於計算複雜的圖形重新連接技術。

##### **One2set + Large Language Model: Best Partners for Keyphrase Generation**
2410.03421v1 by Liangying Shao, Liang Zhang, Minlong Peng, Guoqi Ma, Hao Yue, Mingming Sun, Jinsong Su

Keyphrase generation (KPG) aims to automatically generate a collection of
phrases representing the core concepts of a given document. The dominant
paradigms in KPG include one2seq and one2set. Recently, there has been
increasing interest in applying large language models (LLMs) to KPG. Our
preliminary experiments reveal that it is challenging for a single model to
excel in both recall and precision. Further analysis shows that: 1) the one2set
paradigm owns the advantage of high recall, but suffers from improper
assignments of supervision signals during training; 2) LLMs are powerful in
keyphrase selection, but existing selection methods often make redundant
selections. Given these observations, we introduce a generate-then-select
framework decomposing KPG into two steps, where we adopt a one2set-based model
as generator to produce candidates and then use an LLM as selector to select
keyphrases from these candidates. Particularly, we make two important
improvements on our generator and selector: 1) we design an Optimal
Transport-based assignment strategy to address the above improper assignments;
2) we model the keyphrase selection as a sequence labeling task to alleviate
redundant selections. Experimental results on multiple benchmark datasets show
that our framework significantly surpasses state-of-the-art models, especially
in absent keyphrase prediction.

摘要：關鍵字生成 (KPG) 旨在自動生成一組代表給定文件核心概念的短語。KPG 中的主要範例包括 one2seq 和 one2set。最近，將大型語言模型 (LLM) 應用於 KPG 的興趣與日俱增。我們的初步實驗表明，單一模型在召回率和準確度方面都很難表現出色。進一步的分析表明：1) one2set 範例擁有高召回率的優勢，但訓練期間監督信號的分配不當；2) LLM 在關鍵字選擇方面很強大，但現有的選擇方法通常會做出重複的選擇。根據這些觀察，我們引入了一個生成然後選擇的框架，將 KPG 分解為兩個步驟，其中我們採用基於 one2set 的模型作為生成器來產生候選項，然後使用 LLM 作為選擇器從這些候選項中選擇關鍵字。特別是，我們對生成器和選擇器進行了兩項重要的改進：1) 我們設計了一種基於最佳傳輸的分配策略來解決上述不當分配；2) 我們將關鍵字選擇建模為一個序列標籤任務，以減輕重複的選擇。在多個基準資料集上的實驗結果表明，我們的框架顯著優於最先進的模型，特別是在沒有關鍵字預測的情況下。

##### **Towards Real-time Intrahepatic Vessel Identification in Intraoperative Ultrasound-Guided Liver Surgery**
2410.03420v1 by Karl-Philippe Beaudet, Alexandros Karargyris, Sidaty El Hadramy, Stéphane Cotin, Jean-Paul Mazellier, Nicolas Padoy, Juan Verde

While laparoscopic liver resection is less prone to complications and
maintains patient outcomes compared to traditional open surgery, its complexity
hinders widespread adoption due to challenges in representing the liver's
internal structure. Laparoscopic intraoperative ultrasound offers efficient,
cost-effective and radiation-free guidance. Our objective is to aid physicians
in identifying internal liver structures using laparoscopic intraoperative
ultrasound. We propose a patient-specific approach using preoperative 3D
ultrasound liver volume to train a deep learning model for real-time
identification of portal tree and branch structures. Our personalized AI model,
validated on ex vivo swine livers, achieved superior precision (0.95) and
recall (0.93) compared to surgeons, laying groundwork for precise vessel
identification in ultrasound-based liver resection. Its adaptability and
potential clinical impact promise to advance surgical interventions and improve
patient care.

摘要：腹腔鏡肝切除術與傳統的開放手術相比，較不容易產生併發症，且能維持患者的治療成效，但由於其複雜性，在呈現肝臟內部結構時會遭遇挑戰，因此阻礙了它的廣泛採用。腹腔鏡術中超音波提供了有效率、經濟且無輻射的引導。我們的目標是協助醫師使用腹腔鏡術中超音波來辨識肝臟內部結構。我們提出一個使用術前 3D 超音波肝臟體積來訓練深度學習模型的患者專屬方法，以利於即時辨識門靜脈樹和分支結構。我們的個人化 AI 模型已在離體豬肝臟上驗證，與外科醫師相比，達到了更高的精確度 (0.95) 和召回率 (0.93)，這為基於超音波的肝切除術中精確的血管辨識奠定了基礎。它的適應性和潛在的臨床影響有望推進外科手術，並改善患者照護。

##### **Surgical, Cheap, and Flexible: Mitigating False Refusal in Language Models via Single Vector Ablation**
2410.03415v1 by Xinpeng Wang, Chengzhi Hu, Paul Röttger, Barbara Plank

Training a language model to be both helpful and harmless requires careful
calibration of refusal behaviours: Models should refuse to follow malicious
instructions or give harmful advice (e.g. "how do I kill someone?"), but they
should not refuse safe requests, even if they superficially resemble unsafe
ones (e.g. "how do I kill a Python process?"). Avoiding such false refusal, as
prior work has shown, is challenging even for highly-capable language models.
In this paper, we propose a simple and surgical method for mitigating false
refusal in language models via single vector ablation. For a given model, we
extract a false refusal vector and show that ablating this vector reduces false
refusal rate without negatively impacting model safety and general model
capabilities. We also show that our approach can be used for fine-grained
calibration of model safety. Our approach is training-free and model-agnostic,
making it useful for mitigating the problem of false refusal in current and
future language models.

摘要：訓練語言模型既有幫助又無害，需要仔細校準拒絕行為：模型應拒絕遵循惡意指令或提供有害建議（例如「我如何殺死某人？」），但它們不應拒絕安全請求，即使它們表面上類似不安全的請求（例如「我如何終止 Python 程序？」）。避免這種錯誤拒絕，正如先前的工作所示，即使對於能力很強的語言模型來說也是具有挑戰性的。在本文中，我們提出了一種簡單且有力的方法，透過單一向量消融來減輕語言模型中的錯誤拒絕。對於給定的模型，我們提取一個錯誤拒絕向量，並顯示消除此向量會降低錯誤拒絕率，而不會對模型安全性與一般模型能力產生負面影響。我們也顯示我們的做法可用於模型安全性的細緻校準。我們的做法無需訓練且與模型無關，這使得它對於減輕當前和未來語言模型中錯誤拒絕的問題很有用。

##### **Team MTS @ AutoMin 2021: An Overview of Existing Summarization Approaches and Comparison to Unsupervised Summarization Techniques**
2410.03412v1 by Olga Iakovenko, Anna Andreeva, Anna Lapidus, Liana Mikaelyan

Remote communication through video or audio conferences has become more
popular than ever because of the worldwide pandemic. These events, therefore,
have provoked the development of systems for automatic minuting of spoken
language leading to AutoMin 2021 challenge. The following paper illustrates the
results of the research that team MTS has carried out while participating in
the Automatic Minutes challenge. In particular, in this paper we analyze
existing approaches to text and speech summarization, propose an unsupervised
summarization technique based on clustering and provide a pipeline that
includes an adapted automatic speech recognition block able to run on real-life
recordings. The proposed unsupervised technique outperforms pre-trained
summarization models on the automatic minuting task with Rouge 1, Rouge 2 and
Rouge L values of 0.21, 0.02 and 0.2 on the dev set, with Rouge 1, Rouge 2,
Rouge L, Adequacy, Grammatical correctness and Fluency values of 0.180, 0.035,
0.098, 1.857, 2.304, 1.911 on the test set accordingly

摘要：由於全球疫情，透過視訊或音訊會議進行遠端溝通變得比以往更加普及。因此，這些活動促使了自動化口語會議記錄系統的開發，進而產生了 AutoMin 2021 挑戰。以下論文說明了 MTS 團隊在參與自動化會議記錄挑戰時進行的研究成果。特別是在這篇論文中，我們分析了現有的文字和語音摘要方法，提出了一個基於分群的非監督摘要技術，並提供了一個管道，其中包含一個適應性的自動語音辨識區塊，能夠在真實錄音中執行。所提出的非監督技術在自動化會議記錄任務中優於預先訓練的摘要模型，在開發組上的 Rouge 1、Rouge 2 和 Rouge L 值分別為 0.21、0.02 和 0.2，在測試組上 Rouge 1、Rouge 2、Rouge L、充足性、語法正確性和流暢性值分別為 0.180、0.035、0.098、1.857、2.304、1.911

##### **Comparative study of regression vs pairwise models for surrogate-based heuristic optimisation**
2410.03409v1 by Pablo S. Naharro, Pablo Toharia, Antonio LaTorre, José-María Peña

Heuristic optimisation algorithms explore the search space by sampling
solutions, evaluating their fitness, and biasing the search in the direction of
promising solutions. However, in many cases, this fitness function involves
executing expensive computational calculations, drastically reducing the
reasonable number of evaluations. In this context, surrogate models have
emerged as an excellent alternative to alleviate these computational problems.
This paper addresses the formulation of surrogate problems as both regression
models that approximate fitness (surface surrogate models) and a novel way to
connect classification models (pairwise surrogate models). The pairwise
approach can be directly exploited by some algorithms, such as Differential
Evolution, in which the fitness value is not actually needed to drive the
search, and it is sufficient to know whether a solution is better than another
one or not. Based on these modelling approaches, we have conducted a
multidimensional analysis of surrogate models under different configurations:
different machine learning algorithms (regularised regression, neural networks,
decision trees, boosting methods, and random forests), different surrogate
strategies (encouraging diversity or relaxing prediction thresholds), and
compare them for both surface and pairwise surrogate models. The experimental
part of the article includes the benchmark problems already proposed for the
SOCO2011 competition in continuous optimisation and a simulation problem
included in the recent GECCO2021 Industrial Challenge. This paper shows that
the performance of the overall search, when using online machine learning-based
surrogate models, depends not only on the accuracy of the predictive model but
also on both the kind of bias towards positive or negative cases and how the
optimisation uses those predictions to decide whether to execute the actual
fitness function.

摘要：启发式优化算法通过对解决方案进行抽样、评估其适应度，并按有前景的解决方案的方向偏置搜索来探索搜索空间。然而，在许多情况下，此适应度函数涉及执行昂贵的计算计算，从而大幅减少合理的评估数量。在此上下文中，代理模型已成为缓解这些计算问题的极佳替代方案。本文解决了将代理问题表述为近似适应度的回归模型（表面代理模型）和连接分类模型（成对代理模型）的新方法。成对方法可被一些算法直接利用，例如差分进化，其中实际上不需要适应度值来驱动搜索，并且只需知道一个解决方案是否优于另一个解决方案即可。基于这些建模方法，我们对不同配置下的代理模型进行了多维分析：不同的机器学习算法（正则化回归、神经网络、决策树、提升方法和随机森林）、不同的代理策略（鼓励多样性或放宽预测阈值），并将其与表面和成对代理模型进行比较。本文的实验部分包括已针对 SOCO2011 连续优化竞赛提出的基准问题和最近的 GECCO2021 工业挑战中包含的仿真问题。本文表明，在使用基于在线机器学习的代理模型时，整体搜索的性能不仅取决于预测模型的准确性，还取决于对正例或负例的偏置类型以及优化如何使用这些预测来决定是否执行实际适应度函数。

##### **EBES: Easy Benchmarking for Event Sequences**
2410.03399v1 by Dmitry Osin, Igor Udovichenko, Viktor Moskvoretskii, Egor Shvetsov, Evgeny Burnaev

Event sequences, characterized by irregular sampling intervals and a mix of
categorical and numerical features, are common data structures in various
real-world domains such as healthcare, finance, and user interaction logs.
Despite advances in temporal data modeling techniques, there is no standardized
benchmarks for evaluating their performance on event sequences. This
complicates result comparison across different papers due to varying evaluation
protocols, potentially misleading progress in this field. We introduce EBES, a
comprehensive benchmarking tool with standardized evaluation scenarios and
protocols, focusing on regression and classification problems with
sequence-level targets. Our library simplifies benchmarking, dataset addition,
and method integration through a unified interface. It includes a novel
synthetic dataset and provides preprocessed real-world datasets, including the
largest publicly available banking dataset. Our results provide an in-depth
analysis of datasets, identifying some as unsuitable for model comparison. We
investigate the importance of modeling temporal and sequential components, as
well as the robustness and scaling properties of the models. These findings
highlight potential directions for future research. Our benchmark aim is to
facilitate reproducible research, expediting progress and increasing real-world
impacts.

摘要：事件序列的特征是不规则的采样间隔和类别与数值特征的混合，在医疗保健、金融和用户交互日志等各种现实世界领域中是常见的资料结构。尽管时间资料建模技术不断进步，但尚未有标准化的基准来评估其在事件序列上的效能。这使得不同论文之间的结果比较变得复杂，因为评估协议不同，可能会误导该领域的进展。我们引入了 EBES，这是一个全面的基准测试工具，具有标准化的评估场景和协议，重点关注序列级别目标的回归和分类问题。我们的资料库通过统一的界面简化了基准测试、数据集添加和方法整合。它包括一个新颖的合成数据集，并提供预处理的现实世界数据集，包括最大的公开银行数据集。我们的结果提供了对数据集的深入分析，指出一些数据集不适合模型比较。我们调查了对时间和顺序组件进行建模的重要性，以及模型的鲁棒性和缩放特性。这些发现突出了未来研究的潜在方向。我们的基准测试的目标是促进可重复的研究，加快进度并增加现实世界的影响。

##### **GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction**
2410.03396v1 by Shijin Duan, Ruyi Ding, Jiaxing He, Aidong Adam Ding, Yunsi Fei, Xiaolin Xu

Graph-structured data is integral to many applications, prompting the
development of various graph representation methods. Graph autoencoders (GAEs),
in particular, reconstruct graph structures from node embeddings. Current GAE
models primarily utilize self-correlation to represent graph structures and
focus on node-level tasks, often overlooking multi-graph scenarios. Our
theoretical analysis indicates that self-correlation generally falls short in
accurately representing specific graph features such as islands, symmetrical
structures, and directional edges, particularly in smaller or multiple graph
contexts. To address these limitations, we introduce a cross-correlation
mechanism that significantly enhances the GAE representational capabilities.
Additionally, we propose GraphCroc, a new GAE that supports flexible encoder
architectures tailored for various downstream tasks and ensures robust
structural reconstruction, through a mirrored encoding-decoding process. This
model also tackles the challenge of representation bias during optimization by
implementing a loss-balancing strategy. Both theoretical analysis and numerical
evaluations demonstrate that our methodology significantly outperforms existing
self-correlation-based GAEs in graph structure reconstruction.

摘要：圖形結構化資料是許多應用程式中不可或缺的一部分，促使各種圖形表示方法的發展。特別是圖形自動編碼器 (GAE) 從節點嵌入中重建圖形結構。目前的 GAE 模型主要利用自我相關性來表示圖形結構，並專注於節點層級的任務，經常忽略多圖形場景。我們的理論分析表明，自我相關性通常無法準確表示特定的圖形特徵，例如孤立點、對稱結構和有向邊緣，特別是在較小或多個圖形環境中。為了解決這些限制，我們引入了一個交叉相關機制，可以顯著增強 GAE 的表示能力。此外，我們提出了 GraphCroc，這是一個新的 GAE，它支援針對各種下游任務量身打造的彈性編碼器架構，並透過鏡像編碼解碼過程確保穩健的結構重建。此模型也透過實作損失平衡策略來解決最佳化過程中表示偏差的挑戰。理論分析和數值評估都證明，我們的技術在圖形結構重建方面顯著優於現有的基於自我相關性的 GAE。

##### **Killing Two Flies with One Stone: An Attempt to Break LLMs Using English->Icelandic Idioms and Proper Names**
2410.03394v1 by Bjarki Ármannsson, Hinrik Hafsteinsson, Atli Jasonarson, Steinþór Steingrímsson

This paper presents the submission of the \'Arni Magn\'usson Institute's team
to the WMT24 test suite subtask, focusing on idiomatic expressions and proper
names for the English->Icelandic translation direction.
  Intuitively and empirically, idioms and proper names are known to be a
significant challenge for modern translation models. We create two different
test suites. The first evaluates the competency of MT systems in translating
common English idiomatic expressions, as well as testing whether systems can
distinguish between those expressions and the same phrases when used in a
literal context. The second test suite consists of place names that should be
translated into their Icelandic exonyms (and correctly inflected) and pairs of
Icelandic names that share a surface form between the male and female variants,
so that incorrect translations impact meaning as well as readability.
  The scores reported are relatively low, especially for idiomatic expressions
and place names, and indicate considerable room for improvement.

摘要：本文介紹了「Arni Magn'usson 研究所」團隊提交至 WMT24 測試套件子任務的內容，重點在於英語到冰島語翻譯方向的慣用語和專有名詞。
直覺上和經驗上，慣用語和專有名詞是現代翻譯模型的一大挑戰。我們建立了兩個不同的測試套件。第一個評估機器翻譯系統翻譯常見英語慣用語的能力，以及測試系統是否能區分這些慣用語和在字面語境中使用的相同片語。第二個測試套件包含應翻譯成其冰島語外來語（並正確變格）的地名，以及表面形式在男性和女性變體之間相同的冰島語姓名，因此錯誤的翻譯會影響意義和可讀性。
報告的分數相對較低，特別是慣用語和地名，這表示有很大的改進空間。

##### **Cogs in a Machine, Doing What They're Meant to Do -- The AMI Submission to the WMT24 General Translation Task**
2410.03381v1 by Atli Jasonarson, Hinrik Hafsteinsson, Bjarki Ármannsson, Steinþór Steingrímsson

This paper presents the submission of the \'Arni Magnusson Institute's team
to the WMT24 General translation task. We work on the English->Icelandic
translation direction. Our system comprises four translation models and a
grammar correction model. For training our models we carefully curate our
datasets, aggressively filtering out sentence pairs that may detrimentally
affect the quality of our system's output. Some of our data are collected from
human translations and some are synthetically generated. A part of the
synthetic data is generated using an LLM, and we find that it increases the
translation capability of our system significantly.

摘要：這篇論文展示了「Árni Magnússon 研究所」團隊在 WMT24 一般翻譯任務中的提交內容。我們處理英語到冰島語的翻譯方向。我們的系統包含四個翻譯模型和一個語法校正模型。為了訓練我們的模型，我們仔細策劃我們的資料集，積極過濾掉可能對我們系統輸出的品質造成負面影響的句子對。我們的部分資料是從人類翻譯中收集的，有些則是合成產生的。合成資料的一部分是使用 LLM 產生的，我們發現它大幅提升了我們系統的翻譯能力。

##### **Predicting perturbation targets with causal differential networks**
2410.03380v1 by Menghua Wu, Umesh Padia, Sean H. Murphy, Regina Barzilay, Tommi Jaakkola

Rationally identifying variables responsible for changes to a biological
system can enable myriad applications in disease understanding and cell
engineering. From a causality perspective, we are given two datasets generated
by the same causal model, one observational (control) and one interventional
(perturbed). The goal is to isolate the subset of measured variables (e.g.
genes) that were the targets of the intervention, i.e. those whose conditional
independencies have changed. Knowing the causal graph would limit the search
space, allowing us to efficiently pinpoint these variables. However, current
algorithms that infer causal graphs in the presence of unknown intervention
targets scale poorly to the hundreds or thousands of variables in biological
data, as they must jointly search the combinatorial spaces of graphs and
consistent intervention targets. In this work, we propose a causality-inspired
approach for predicting perturbation targets that decouples the two search
steps. First, we use an amortized causal discovery model to separately infer
causal graphs from the observational and interventional datasets. Then, we
learn to map these paired graphs to the sets of variables that were intervened
upon, in a supervised learning framework. This approach consistently
outperforms baselines for perturbation modeling on seven single-cell
transcriptomics datasets, each with thousands of measured variables. We also
demonstrate significant improvements over six causal discovery algorithms in
predicting intervention targets across a variety of tractable, synthetic
datasets.

摘要：透過理性地找出負責生物系統變化的變數，我們可以找出許多應用在疾病理解和細胞工程上。從因果關係的角度來看，我們會得到兩個由同一個因果模型產生的資料集，一個是觀察性的（控制），另一個是介入性的（擾動）。目標是找出測量變數的子集（例如基因），這些變數是介入的目標，也就是那些條件獨立性已經改變的變數。知道因果圖表會限制搜尋空間，讓我們可以有效地找出這些變數。然而，目前在未知介入目標的情況下推論因果圖表的演算法，對於生物資料中數百或數千個變數來說擴充性很差，因為它們必須共同搜尋圖表和一致介入目標的組合空間。在這項工作中，我們提出一個因果靈感的介入目標預測方法，它將兩個搜尋步驟分開。首先，我們使用攤銷因果發現模型，分別從觀察性和介入性資料集推論因果圖表。然後，我們在一個監督學習架構中學習將這些配對圖表對應到被介入的變數集合。這種方法在七個單細胞轉錄組學資料集的擾動建模中，始終優於基線，每個資料集都有數千個測量變數。我們也展示出在各種易於處理的合成資料集中，相較於六個因果發現演算法，在預測介入目標方面有顯著的改進。

##### **An Enhanced Harmonic Densely Connected Hybrid Transformer Network Architecture for Chronic Wound Segmentation Utilising Multi-Colour Space Tensor Merging**
2410.03359v1 by Bill Cassidy, Christian Mcbride, Connah Kendrick, Neil D. Reeves, Joseph M. Pappachan, Cornelius J. Fernandez, Elias Chacko, Raphael Brüngel, Christoph M. Friedrich, Metib Alotaibi, Abdullah Abdulaziz AlWabel, Mohammad Alderwish, Kuan-Ying Lai, Moi Hoon Yap

Chronic wounds and associated complications present ever growing burdens for
clinics and hospitals world wide. Venous, arterial, diabetic, and pressure
wounds are becoming increasingly common globally. These conditions can result
in highly debilitating repercussions for those affected, with limb amputations
and increased mortality risk resulting from infection becoming more common. New
methods to assist clinicians in chronic wound care are therefore vital to
maintain high quality care standards. This paper presents an improved HarDNet
segmentation architecture which integrates a contrast-eliminating component in
the initial layers of the network to enhance feature learning. We also utilise
a multi-colour space tensor merging process and adjust the harmonic shape of
the convolution blocks to facilitate these additional features. We train our
proposed model using wound images from light-skinned patients and test the
model on two test sets (one set with ground truth, and one without) comprising
only darker-skinned cases. Subjective ratings are obtained from clinical wound
experts with intraclass correlation coefficient used to determine inter-rater
reliability. For the dark-skin tone test set with ground truth, we demonstrate
improvements in terms of Dice similarity coefficient (+0.1221) and intersection
over union (+0.1274). Qualitative analysis showed high expert ratings, with
improvements of >3% demonstrated when comparing the baseline model with the
proposed model. This paper presents the first study to focus on darker-skin
tones for chronic wound segmentation using models trained only on wound images
exhibiting lighter skin. Diabetes is highly prevalent in countries where
patients have darker skin tones, highlighting the need for a greater focus on
such cases. Additionally, we conduct the largest qualitative study to date for
chronic wound segmentation.

摘要：<paragraph>慢性傷口及其併發症對全球診所和醫院而言，帶來日益沉重的負擔。靜脈性、動脈性、糖尿病性和壓瘡在全球愈來愈普遍。這些疾病會對患者造成高度衰弱的影響，截肢和因感染而導致的死亡風險也日益普遍。因此，新的方法協助臨床醫生進行慢性傷口照護至關重要，以維持高品質的照護標準。本文提出一個改良的 HarDNet 分割架構，將對比消除元件整合到網路的初始層，以增強特徵學習。我們也利用多色彩空間張量合併程序，並調整卷積區塊的諧波形狀，以利於這些額外特徵。我們使用淺膚色患者的傷口影像訓練我們提出的模型，並在兩個測試組（一個組有基本事實，一個組沒有）上測試模型，這些組僅包含較深膚色的病例。從臨床傷口專家取得主觀評分，並使用類內相關係數來確定評分者間信賴度。對於有基本事實的深膚色測試組，我們展示了骰子相似係數 (+0.1221) 和聯集比交集 (+0.1274) 的改進。定性分析顯示專家評分很高，與基準模型相比，提出的模型顯示出 >3% 的改進。本文提出第一個研究，專注於使用僅在表現出較淺膚色的傷口影像上訓練的模型，進行深膚色慢性傷口分割。糖尿病在患者膚色較深的國家非常普遍，強調需要更多關注此類病例。此外，我們進行了迄今為止最大的慢性傷口分割定性研究。</paragraph>

##### **Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing**
2410.03357v1 by Jeongwoo Kang, Maximin Coavoux, Cédric Lopez, Didier Schwab

Cross-lingual AMR parsing is the task of predicting AMR graphs in a target
language when training data is available only in a source language. Due to the
small size of AMR training data and evaluation data, cross-lingual AMR parsing
has only been explored in a small set of languages such as English, Spanish,
German, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022),
who apply meta-learning to tackle cross-lingual syntactic parsing, we
investigate the use of meta-learning for cross-lingual AMR parsing. We evaluate
our models in $k$-shot scenarios (including 0-shot) and assess their
effectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean
and Croatian test sets are developed as part of our work, based on the existing
The Little Prince English AMR corpus, and made publicly available. We
empirically study our method by comparing it to classical joint learning. Our
findings suggest that while the meta-learning model performs slightly better in
0-shot evaluation for certain languages, the performance gain is minimal or
absent when $k$ is higher than 0.

摘要：跨語言 AMR 解析是一項任務，在僅在源語言中提供訓練資料時，預測目標語言中的 AMR 圖形。由於 AMR 訓練資料和評估資料的規模很小，因此跨語言 AMR 解析僅在少數語言中進行過探索，例如英語、西班牙語、德語、中文和義大利語。受到 Langedijk 等人 (2022) 的啟發，他們應用元學習來處理跨語言句法解析，我們研究了使用元學習進行跨語言 AMR 解析。我們在 $k$-shot 場景（包括 0-shot）中評估我們的模型，並評估它們在克羅埃西亞語、波斯語、韓語、中文和法語中的有效性。值得注意的是，韓語和克羅埃西亞語測試集是根據現有的《小王子》英語 AMR 語料庫開發的，並公開提供。我們通過將我們的模型與傳統聯合學習進行比較，對我們的模型進行實證研究。我們的研究結果表明，雖然元學習模型在某些語言的 0-shot 評估中表現略好，但是當 $k$ 高於 0 時，效能提升很小或沒有。

##### **LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding**
2410.03355v1 by Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang

Auto-Regressive (AR) models have recently gained prominence in image
generation, often matching or even surpassing the performance of diffusion
models. However, one major limitation of AR models is their sequential nature,
which processes tokens one at a time, slowing down generation compared to
models like GANs or diffusion-based methods that operate more efficiently.
While speculative decoding has proven effective for accelerating LLMs by
generating multiple tokens in a single forward, its application in visual AR
models remains largely unexplored. In this work, we identify a challenge in
this setting, which we term \textit{token selection ambiguity}, wherein visual
AR models frequently assign uniformly low probabilities to tokens, hampering
the performance of speculative decoding. To overcome this challenge, we propose
a relaxed acceptance condition referred to as LANTERN that leverages the
interchangeability of tokens in latent space. This relaxation restores the
effectiveness of speculative decoding in visual AR models by enabling more
flexible use of candidate tokens that would otherwise be prematurely rejected.
Furthermore, by incorporating a total variation distance bound, we ensure that
these speed gains are achieved without significantly compromising image quality
or semantic coherence. Experimental results demonstrate the efficacy of our
method in providing a substantial speed-up over speculative decoding. In
specific, compared to a na\"ive application of the state-of-the-art speculative
decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and
$\mathbf{1.76}\times$, as compared to greedy decoding and random sampling,
respectively, when applied to LlamaGen, a contemporary visual AR model.

摘要：<paragraph>自迴歸 (AR) 模型最近在影像生成中獲得顯著地位，其效能經常與擴散模型相匹配，甚至超越擴散模型。然而，AR 模型的一項主要限制是其序列性質，它一次處理一個符號，與運作更有效率的 GAN 或基於擴散的方法等模型相比，減緩了生成速度。雖然推測解碼已被證明可透過在單一前饋中生成多個符號來加速 LLM，但它在視覺 AR 模型中的應用仍未廣泛探索。在這項工作中，我們找出此設定中的挑戰，我們稱其為「符號選擇模糊性」，其中視覺 AR 模型經常將均勻的低機率分配給符號，阻礙推測解碼的效能。為了克服此挑戰，我們提出一個放寬的接受條件，稱為 LANTERN，它利用潛在空間中符號的可互換性。此放寬恢復視覺 AR 模型中推測解碼的效能，讓候選符號能更靈活地使用，否則這些符號將會過早被拒絕。此外，透過納入總變異距離邊界，我們確保這些速度提升並未顯著損害影像品質或語意一致性。實驗結果證明了我們的方法在提供大幅加速效果方面的效能，優於推測解碼。具體來說，與當代視覺 AR 模型 LlamaGen 應用於最先進的推測解碼的純真應用相比，LANTERN 分別將速度提升增加了 $\mathbf{1.75}\times$ 和 $\mathbf{1.76}\times$，與貪婪解碼和隨機抽樣相比。</paragraph>

##### **Generating Equivalent Representations of Code By A Self-Reflection Approach**
2410.03351v1 by Jia Li, Ge Li, Lecheng Wang, Hao Zhu, Zhi Jin

Equivalent Representations (ERs) of code are textual representations that
preserve the same semantics as the code itself, e.g., natural language comments
and pseudocode. ERs play a critical role in software development and
maintenance. However, how to automatically generate ERs of code remains an open
challenge. In this paper, we propose a self-reflection approach to generating
ERs of code. It enables two Large Language Models (LLMs) to work mutually and
produce an ER through a reflection process. Depending on whether constraints on
ERs are applied, our approach generates ERs in both open and constrained
settings. We conduct a empirical study to generate ERs in two settings and
obtain eight findings. (1) Generating ERs in the open setting. In the open
setting, we allow LLMs to represent code without any constraints, analyzing the
resulting ERs and uncovering five key findings. These findings shed light on
how LLMs comprehend syntactic structures, APIs, and numerical computations in
code. (2) Generating ERs in the constrained setting. In the constrained
setting, we impose constraints on ERs, such as natural language comments,
pseudocode, and flowcharts. This allows our approach to address a range of
software engineering tasks. Based on our experiments, we have three findings
demonstrating that our approach can effectively generate ERs that adhere to
specific constraints, thus supporting various software engineering tasks. (3)
Future directions. We also discuss potential future research directions, such
as deriving intermediate languages for code generation, exploring LLM-friendly
requirement descriptions, and further supporting software engineering tasks. We
believe that this paper will spark discussions in research communities and
inspire many follow-up studies.

摘要：<paragraph>代碼的等效表示 (ER) 是文字表示，它保留與代碼本身相同的語義，例如自然語言註解和偽代碼。ER 在軟體開發和維護中扮演至關重要的角色。然而，如何自動產生代碼的 ER 仍然是一個未解決的挑戰。在這篇論文中，我們提出一個自省方法來產生代碼的 ER。它讓兩個大型語言模型 (LLM) 互相合作，並透過自省過程產生 ER。取決於是否套用 ER 的限制，我們的做法在開放和受限的設定中產生 ER。我們進行一項實證研究，在兩個設定中產生 ER，並獲得八項發現。(1) 在開放設定中產生 ER。在開放設定中，我們允許 LLM 在沒有任何限制的情況下表示代碼，分析產生的 ER 並找出五項關鍵發現。這些發現闡明 LLM 如何理解代碼中的語法結構、API 和數值運算。(2) 在受限設定中產生 ER。在受限設定中，我們對 ER 施加限制，例如自然語言註解、偽代碼和流程圖。這讓我們的做法能夠處理一系列軟體工程任務。根據我們的實驗，我們有三個發現，證明我們的做法可以有效產生符合特定限制的 ER，進而支援各種軟體工程任務。(3) 未來方向。我們也討論潛在的未來研究方向，例如推導用於產生代碼的中間語言、探索 LLM 友善的需求描述，以及進一步支援軟體工程任務。我們相信這篇論文將在研究社群中引發討論，並激勵許多後續研究。</paragraph>

##### **Zero-Shot Fact Verification via Natural Logic and Large Language Models**
2410.03341v1 by Marek Strong, Rami Aly, Andreas Vlachos

The recent development of fact verification systems with natural logic has
enhanced their explainability by aligning claims with evidence through
set-theoretic operators, providing faithful justifications. Despite these
advancements, such systems often rely on a large amount of training data
annotated with natural logic. To address this issue, we propose a zero-shot
method that utilizes the generalization capabilities of instruction-tuned large
language models. To comprehensively assess the zero-shot capabilities of our
method and other fact verification systems, we evaluate all models on both
artificial and real-world claims, including multilingual datasets. We also
compare our method against other fact verification systems in two setups.
First, in the zero-shot generalization setup, we demonstrate that our approach
outperforms other systems that were not specifically trained on natural logic
data, achieving an average accuracy improvement of 8.96 points over the
best-performing baseline. Second, in the zero-shot transfer setup, we show that
current systems trained on natural logic data do not generalize well to other
domains, and our method outperforms these systems across all datasets with
real-world claims.

摘要：最近基於自然邏輯的事實驗證系統開發，透過集合論運算符將聲明與證據對齊，提供忠實的證據，進而提升了解釋能力。儘管有這些進展，此類系統通常依賴大量標記有自然邏輯的訓練資料。為了解決這個問題，我們提出一個零次學習方法，利用指令調整大型語言模型的概化能力。為了全面評估我們的方法和其他事實驗證系統的零次學習能力，我們在人工和真實世界聲明（包括多語言資料集）上評估所有模型。我們還針對兩種設定比較我們的方法與其他事實驗證系統。首先，在零次學習概化設定中，我們證明我們的做法優於未針對自然邏輯資料進行特別訓練的其他系統，在最佳效能基準上達到平均準確度提升 8.96 點。其次，在零次學習傳輸設定中，我們證明當前針對自然邏輯資料訓練的系統無法很好地概化到其他領域，而且我們的方法在所有包含真實世界聲明的資料集上都優於這些系統。

##### **An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation**
2410.03334v1 by Ahmed Abdulaal, Hugo Fry, Nina Montaña-Brown, Ayodeji Ijishakin, Jack Gao, Stephanie Hyland, Daniel C. Alexander, Daniel C. Castro

Radiological services are experiencing unprecedented demand, leading to
increased interest in automating radiology report generation. Existing
Vision-Language Models (VLMs) suffer from hallucinations, lack
interpretability, and require expensive fine-tuning. We introduce SAE-Rad,
which uses sparse autoencoders (SAEs) to decompose latent representations from
a pre-trained vision transformer into human-interpretable features. Our hybrid
architecture combines state-of-the-art SAE advancements, achieving accurate
latent reconstructions while maintaining sparsity. Using an off-the-shelf
language model, we distil ground-truth reports into radiological descriptions
for each SAE feature, which we then compile into a full report for each image,
eliminating the need for fine-tuning large models for this task. To the best of
our knowledge, SAE-Rad represents the first instance of using mechanistic
interpretability techniques explicitly for a downstream multi-modal reasoning
task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific
metrics compared to state-of-the-art models while using significantly fewer
computational resources for training. Qualitative analysis reveals that SAE-Rad
learns meaningful visual concepts and generates reports aligning closely with
expert interpretations. Our results suggest that SAEs can enhance multimodal
reasoning in healthcare, providing a more interpretable alternative to existing
VLMs.

摘要：放射服務正經歷前所未有的需求，導致自動化放射報告產生的興趣增加。現有的視覺語言模型 (VLM) 存在幻覺、缺乏可解釋性，且需要昂貴的微調。我們引入了 SAE-Rad，它使用稀疏自動編碼器 (SAE) 將預先訓練的視覺轉換器中的潛在表示分解為人類可解釋的功能。我們的混合架構結合了最先進的 SAE 進展，實現了準確的潛在重建，同時保持稀疏性。使用現成的語言模型，我們將真實報告提煉為每個 SAE 特徵的放射描述，然後將其編譯成每個影像的完整報告，消除了針對此任務微調大型模型的需要。據我們所知，SAE-Rad 代表了首次明確使用機械可解釋性技術進行下游多模式推理任務的範例。在 MIMIC-CXR 資料集上，SAE-Rad 達到了與最先進模型相比具有競爭力的放射專用指標，同時在訓練中使用了明顯更少的計算資源。定性分析顯示，SAE-Rad 學習了有意義的視覺概念，並產生與專家解釋緊密對齊的報告。我們的結果表明，SAE 可以增強醫療保健中的多模式推理，為現有的 VLM 提供更具可解釋性的替代方案。

##### **Comparative Analysis and Ensemble Enhancement of Leading CNN Architectures for Breast Cancer Classification**
2410.03333v1 by Gary Murphy, Raghubir Singh

This study introduces a novel and accurate approach to breast cancer
classification using histopathology images. It systematically compares leading
Convolutional Neural Network (CNN) models across varying image datasets,
identifies their optimal hyperparameters, and ranks them based on
classification efficacy. To maximize classification accuracy for each model we
explore, the effects of data augmentation, alternative fully-connected layers,
model training hyperparameter settings, and, the advantages of retraining
models versus using pre-trained weights. Our methodology includes several
original concepts, including serializing generated datasets to ensure
consistent data conditions across training runs and significantly reducing
training duration. Combined with automated curation of results, this enabled
the exploration of over 2,000 training permutations -- such a comprehensive
comparison is as yet unprecedented. Our findings establish the settings
required to achieve exceptional classification accuracy for standalone CNN
models and rank them by model efficacy. Based on these results, we propose
ensemble architectures that stack three high-performing standalone CNN models
together with diverse classifiers, resulting in improved classification
accuracy. The ability to systematically run so many model permutations to get
the best outcomes gives rise to very high quality results, including 99.75% for
BreakHis x40 and BreakHis x200 and 95.18% for the Bach datasets when split into
train, validation and test datasets. The Bach Online blind challenge, yielded
89% using this approach. Whilst this study is based on breast cancer
histopathology image datasets, the methodology is equally applicable to other
medical image datasets.

摘要：本研究引入了一種新穎且準確的方法，使用組織病理學影像來對乳癌進行分類。它系統性地比較了在不同影像資料集中的領先卷積神經網路 (CNN) 模型，找出它們最佳的超參數，並根據分類效能對它們進行排名。為了最大化我們探索的每個模型的分類準確度，我們探討了資料擴充、替代全連接層、模型訓練超參數設定，以及重新訓練模型與使用預訓練權重的優點。我們的做法包含了幾個原始概念，包括序列化產生的資料集，以確保在訓練過程中資料條件一致，並大幅縮短訓練時間。結合自動化結果整理，這使得我們能夠探索超過 2,000 個訓練排列組合——如此全面的比較在目前為止是前所未有的。我們的發現建立了達成傑出分類準確度所需的設定，並根據模型效能對獨立的 CNN 模型進行排名。根據這些結果，我們提出了將三個高性能獨立 CNN 模型與不同的分類器堆疊在一起的整體架構，進而提升了分類準確度。系統性地執行這麼多模型排列組合以獲得最佳結果的能力，產生了非常高品質的結果，包括將 BreakHis x40 和 BreakHis x200 分割成訓練、驗證和測試資料集時，準確度達到 99.75%，而 Bach 資料集的準確度達到 95.18%。使用這種方法，Bach Online 盲測的準確度達到 89%。雖然本研究是基於乳癌組織病理學影像資料集，但此方法同樣適用於其他醫學影像資料集。

##### **Influence-oriented Personalized Federated Learning**
2410.03315v1 by Yue Tan, Guodong Long, Jing Jiang, Chengqi Zhang

Traditional federated learning (FL) methods often rely on fixed weighting for
parameter aggregation, neglecting the mutual influence by others. Hence, their
effectiveness in heterogeneous data contexts is limited. To address this
problem, we propose an influence-oriented federated learning framework, namely
FedC^2I, which quantitatively measures Client-level and Class-level Influence
to realize adaptive parameter aggregation for each client. Our core idea is to
explicitly model the inter-client influence within an FL system via the
well-crafted influence vector and influence matrix. The influence vector
quantifies client-level influence, enables clients to selectively acquire
knowledge from others, and guides the aggregation of feature representation
layers. Meanwhile, the influence matrix captures class-level influence in a
more fine-grained manner to achieve personalized classifier aggregation. We
evaluate the performance of FedC^2I against existing federated learning methods
under non-IID settings and the results demonstrate the superiority of our
method.

摘要：傳統的聯合學習 (FL) 方法通常依賴於固定權重來進行參數聚合，忽略了其他參數的相互影響。因此，它們在異質數據環境中的有效性受到限制。為了解決這個問題，我們提出了一個以影響為導向的聯合學習框架，稱為 FedC^2I，它定量測量客戶端級別和類別級別的影響，以實現每個客戶端的自適應參數聚合。我們的核心思想是通過精心製作的影響向量和影響矩陣，明確地對 FL 系統內的客戶端間影響進行建模。影響向量量化客戶端級別的影響，使客戶端能夠有選擇地從其他客戶端獲取知識，並指導特徵表示層的聚合。同時，影響矩陣以更細粒度的形式捕捉類別級別的影響，以實現個性化分類器聚合。我們在非 IID 設置下評估了 FedC^2I 與現有聯合學習方法的性能，結果證明了我們方法的優越性。

##### **Context and System Fusion in Post-ASR Emotion Recognition with Large Language Models**
2410.03312v1 by Pavel Stepachev, Pinzhen Chen, Barry Haddow

Large language models (LLMs) have started to play a vital role in modelling
speech and text. To explore the best use of context and multiple systems'
outputs for post-ASR speech emotion prediction, we study LLM prompting on a
recent task named GenSEC. Our techniques include ASR transcript ranking,
variable conversation context, and system output fusion. We show that the
conversation context has diminishing returns and the metric used to select the
transcript for prediction is crucial. Finally, our best submission surpasses
the provided baseline by 20% in absolute accuracy.

摘要：大型語言模型 (LLM) 已開始在建模語言和文字中扮演至關重要的角色。為了探索在 ASR 後語音情緒預測中最佳使用情境和多個系統的輸出，我們在一個名為 GenSEC 的最新任務上研究 LLM 提示。我們的技術包括 ASR 轉錄排名、可變對話情境和系統輸出融合。我們顯示對話情境具有遞減回報，而用於選擇預測轉錄的指標至關重要。最後，我們最好的提交在絕對準確度上超過了提供的基準 20%。

##### **Comparing zero-shot self-explanations with human rationales in multilingual text classification**
2410.03296v1 by Stephanie Brandl, Oliver Eberle

Instruction-tuned LLMs are able to provide an explanation about their output
to users by generating self-explanations that do not require gradient
computations or the application of possibly complex XAI methods. In this paper,
we analyse whether this ability results in a good explanation by evaluating
self-explanations in the form of input rationales with respect to their
plausibility to humans as well as their faithfulness to models. For this, we
apply two text classification tasks: sentiment classification and forced labour
detection. Next to English, we further include Danish and Italian translations
of the sentiment classification task and compare self-explanations to human
annotations for all samples. To allow for direct comparisons, we also compute
post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and
apply this pipeline to 4 LLMs (Llama2, Llama3, Mistral and Mixtral). Our
results show that self-explanations align more closely with human annotations
compared to LRP, while maintaining a comparable level of faithfulness.

摘要：經過指令微調的 LLM 可以透過產生自我解釋來對其輸出提供說明給使用者，而這些自我解釋不需要梯度運算或應用可能很複雜的 XAI 方法。在本文中，我們分析這種能力是否能產生良好的解釋，方法是評估輸入依據形式的自我解釋，並考量其對人類的合理性以及對模型的忠實度。為此，我們應用兩個文字分類任務：情緒分類和強迫勞動偵測。除了英文之外，我們還納入了情緒分類任務的丹麥文和義大利文翻譯，並將自我解釋與所有範例的人類註解進行比較。為了進行直接比較，我們也計算事後特徵歸因，也就是逐層相關性傳播 (LRP)，並將此管道應用於 4 個 LLM（Llama2、Llama3、Mistral 和 Mixtral）。我們的結果顯示，與 LRP 相比，自我解釋與人類註解更為一致，同時維持相當程度的忠實度。

##### **Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis**
2410.03293v1 by Nirmalya Thakur

The work presented in this paper makes three scientific contributions with a
specific focus on mining and analysis of COVID-19-related posts on Instagram.
First, it presents a multilingual dataset of 500,153 Instagram posts about
COVID-19 published between January 2020 and September 2024. This dataset,
available at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in
161 different languages as well as 535,021 distinct hashtags. After the
development of this dataset, multilingual sentiment analysis was performed,
which involved classifying each post as positive, negative, or neutral. The
results of sentiment analysis are presented as a separate attribute in this
dataset. Second, it presents the results of performing sentiment analysis per
year from 2020 to 2024. The findings revealed the trends in sentiment related
to COVID-19 on Instagram since the beginning of the pandemic. For instance,
between 2020 and 2024, the sentiment trends show a notable shift, with positive
sentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from
44.19% to 58.34%. Finally, the paper also presents findings of
language-specific sentiment analysis. This analysis highlighted similar and
contrasting trends of sentiment across posts published in different languages
on Instagram. For instance, out of all English posts, 49.68% were positive,
14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts,
4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting
distinct differences in the sentiment distribution between these two languages.

摘要：<paragraph>本文提出的工作在開採和分析 Instagram 上與 COVID-19 相關的貼文方面做出了三項科學貢獻。
首先，它展示了一個多語言資料集，其中包含 2020 年 1 月至 2024 年 9 月間發布的 500,153 篇關於 COVID-19 的 Instagram 貼文。此資料集可於 https://dx.doi.org/10.21227/d46p-v480 取得，其中包含 161 種不同語言的 Instagram 貼文，以及 535,021 個不同的標籤。在開發此資料集後，執行了多語言情緒分析，其中包括將每則貼文分類為正面、負面或中立。情緒分析的結果以一個獨立的屬性呈現於此資料集中。其次，它展示了從 2020 年到 2024 年每年執行情緒分析的結果。這些發現揭露了自疫情爆發以來，Instagram 上與 COVID-19 相關的情緒趨勢。例如，在 2020 年至 2024 年間，情緒趨勢顯示出顯著的變化，正面情緒從 38.35% 下降至 28.69%，而中立情緒從 44.19% 上升至 58.34%。最後，本文也展示了特定語言情緒分析的發現。此分析突顯了在 Instagram 上以不同語言發布的貼文中，情緒的相似和對比趨勢。例如，在所有英文貼文中，49.68% 為正面，14.84% 為負面，35.48% 為中立。相比之下，在印地語貼文中，4.40% 為正面，57.04% 為負面，38.56% 為中立，反映出這兩種語言在情緒分佈上的顯著差異。</paragraph>

##### **Enhanced Transformer architecture for in-context learning of dynamical systems**
2410.03291v1 by Matteo Rufolo, Dario Piga, Gabriele Maroni, Marco Forgione

Recently introduced by some of the authors, the in-context identification
paradigm aims at estimating, offline and based on synthetic data, a meta-model
that describes the behavior of a whole class of systems. Once trained, this
meta-model is fed with an observed input/output sequence (context) generated by
a real system to predict its behavior in a zero-shot learning fashion. In this
paper, we enhance the original meta-modeling framework through three key
innovations: by formulating the learning task within a probabilistic framework;
by managing non-contiguous context and query windows; and by adopting recurrent
patching to effectively handle long context sequences. The efficacy of these
modifications is demonstrated through a numerical example focusing on the
Wiener-Hammerstein system class, highlighting the model's enhanced performance
and scalability.

摘要：一些作者最近提出的情境识别范例旨在根据合成数据离线估计描述整个系统类别行为的元模型。经过训练后，此元模型将馈入由真实系统生成的观察到的输入/输出序列（上下文），以零次学习方式预测其行为。在本文中，我们通过三个关键创新增强了原始元建模框架：通过在概率框架内制定学习任务；通过管理非连续上下文和查询窗口；以及通过采用循环修补来有效地处理长上下文序列。通过关注 Wiener-Hammerstein 系统类的数值示例，证明了这些修改的有效性，突出了模型增强的性能和可扩展性。

##### **Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models**
2410.03290v1 by Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, Lifu Huang

Video Large Language Models (Video-LLMs) have demonstrated remarkable
capabilities in coarse-grained video understanding, however, they struggle with
fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM,
a novel Video-LLM adept at perceiving and reasoning over specific video moments
in a fine-grained manner. We identify that current Video-LLMs have limitations
for fine-grained video understanding since they lack effective temporal
modeling and timestamp representation. In light of this, we sharpen our model
by incorporating (1) an additional temporal stream to encode the relationships
between frames and (2) discrete temporal tokens enriched with specific time
knowledge to represent timestamps. To optimize the training of
Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with
simple video-captioning tasks and progressively introducing video temporal
grounding tasks of increasing complexity. To further enhance
Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded
VideoQA dataset by an automatic annotation pipeline. Extensive experiments
demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding
tasks such as temporal sentence grounding, dense video captioning, and grounded
VideoQA, but also shows great potential as a versatile video assistant for
general video understanding.

摘要：影片大型語言模型 (Video-LLM) 已展現出在粗略影片理解方面的非凡能力，然而，它們在細緻的時間依據方面卻面臨挑戰。在本文中，我們引入了 Grounded-VideoLLM，這是一種新穎的 Video-LLM，擅長以細緻的方式感知和推理特定影片時刻。我們發現目前的 Video-LLM 在細緻影片理解方面存在限制，因為它們缺乏有效的時間建模和時間戳表示。有鑑於此，我們透過整合 (1) 一個額外的時間串流來編碼幀之間的關係，以及 (2) 具有特定時間知識的離散時間標記來表示時間戳，進而強化我們的模型。為了最佳化 Grounded-VideoLLM 的訓練，我們採用多階段訓練架構，從簡單的影片字幕任務開始，並逐步引入越來越複雜的影片時間依據任務。為了進一步增強 Grounded-VideoLLM 的時間推理能力，我們還透過自動註解管道整理了一個依據的 VideoQA 資料集。廣泛的實驗證明，Grounded-VideoLLM 不僅在細緻依據任務中表現出色，例如時間句子依據、密集影片字幕和依據 VideoQA，而且還展現出作為通用影片助理在一般影片理解方面具有極大的潛力。

##### **What do Large Language Models Need for Machine Translation Evaluation?**
2410.03278v1 by Shenbin Qian, Archchana Sindhujan, Minnie Kabra, Diptesh Kanojia, Constantin Orăsan, Tharindu Ranasinghe, Frédéric Blain

Leveraging large language models (LLMs) for various natural language
processing tasks has led to superlative claims about their performance. For the
evaluation of machine translation (MT), existing research shows that LLMs are
able to achieve results comparable to fine-tuned multilingual pre-trained
language models. In this paper, we explore what translation information, such
as the source, reference, translation errors and annotation guidelines, is
needed for LLMs to evaluate MT quality. In addition, we investigate prompting
techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for
eight language pairs covering high-, medium- and low-resource languages,
leveraging varying LLM variants. Our findings indicate the importance of
reference translations for an LLM-based evaluation. While larger models do not
necessarily fare better, they tend to benefit more from CoT prompting, than
smaller models. We also observe that LLMs do not always provide a numerical
score when generating evaluations, which poses a question on their reliability
for the task. Our work presents a comprehensive analysis for
resource-constrained and training-less LLM-based evaluation of machine
translation. We release the accrued prompt templates, code and data publicly
for reproducibility.

摘要：<paragraph>利用大型語言模型 (LLM) 執行各種自然語言處理任務，對其效能已產生極佳的評價。針對機器翻譯 (MT) 的評估，現有研究顯示，LLM 能夠達成與微調多語言預訓練語言模型相當的結果。在這篇論文中，我們探討 LLM 評估 MT 品質所需的翻譯資訊，例如來源、參考、翻譯錯誤和註解指南。此外，我們研究提示技術，例如零次學習、思考鏈 (CoT) 和少次學習提示，涵蓋高、中和低資源語言的八種語言對，並利用各種 LLM 變體。我們的研究結果顯示參考翻譯對基於 LLM 的評估非常重要。雖然較大的模型不見得表現較好，但它們往往比較小的模型更能受益於 CoT 提示。我們也觀察到，LLM 在產生評估時並不總是提供數字分數，這對其任務可靠性提出了疑問。我們的研究針對資源受限和無需訓練的基於 LLM 的機器翻譯評估，提出全面的分析。我們公開發布累積的提示範本、程式碼和資料，以利重現。</paragraph>

##### **Test-time Adaptation for Regression by Subspace Alignment**
2410.03263v1 by Kazuki Adachi, Shin'ya Yamaguchi, Atsutoshi Kumagai, Tomoki Hamagami

This paper investigates test-time adaptation (TTA) for regression, where a
regression model pre-trained in a source domain is adapted to an unknown target
distribution with unlabeled target data. Although regression is one of the
fundamental tasks in machine learning, most of the existing TTA methods have
classification-specific designs, which assume that models output
class-categorical predictions, whereas regression models typically output only
single scalar values. To enable TTA for regression, we adopt a feature
alignment approach, which aligns the feature distributions between the source
and target domains to mitigate the domain gap. However, we found that naive
feature alignment employed in existing TTA methods for classification is
ineffective or even worse for regression because the features are distributed
in a small subspace and many of the raw feature dimensions have little
significance to the output. For an effective feature alignment in TTA for
regression, we propose Significant-subspace Alignment (SSA). SSA consists of
two components: subspace detection and dimension weighting. Subspace detection
finds the feature subspace that is representative and significant to the
output. Then, the feature alignment is performed in the subspace during TTA.
Meanwhile, dimension weighting raises the importance of the dimensions of the
feature subspace that have greater significance to the output. We
experimentally show that SSA outperforms various baselines on real-world
datasets.

摘要：本文研究了回归的测试时自适应 (TTA)，其中在源域中预训练的回归模型被调整到具有未标记目标数据的未知目标分布。虽然回归是机器学习中的基本任务之一，但大多数现有的 TTA 方法都有特定于分类的设计，它们假设模型输出类别分类预测，而回归模型通常仅输出单个标量值。为了实现回归的 TTA，我们采用了一种特征对齐方法，该方法对齐了源域和目标域之间的特征分布以减轻域差距。然而，我们发现用于分类的现有 TTA 方法中采用的朴素特征对齐对于回归无效甚至更糟，因为特征分布在一个小子空间中，并且许多原始特征维度对输出意义不大。为了在回归的 TTA 中进行有效的特征对齐，我们提出了显著子空间对齐 (SSA)。SSA 由两个部分组成：子空间检测和维度加权。子空间检测找到对输出具有代表性和重要性的特征子空间。然后，在 TTA 期间在子空间中执行特征对齐。同时，维度加权提高了对输出具有更大重要性的特征子空间的维度的重要性。我们通过实验表明，SSA 在现实世界的数据集上优于各种基线。

##### **Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models**
2410.03258v1 by Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly

In this work, we show a fundamental limitation in vocabulary adaptation
approaches that use Byte-Pair Encoding (BPE) tokenization scheme for
fine-tuning pretrained language models (PLMs) to expert domains. Current
approaches trivially append the target domain-specific vocabulary at the end of
the PLM vocabulary. This approach leads to a lower priority score and causes
sub-optimal tokenization in BPE that iteratively uses merge rules to tokenize a
given text. To mitigate this issue, we propose AdaptBPE where the BPE
tokenization initialization phase is modified to first perform the longest
string matching on the added (target) vocabulary before tokenizing at the
character level. We perform an extensive evaluation of AdaptBPE versus the
standard BPE over various classification and summarization tasks; AdaptBPE
improves by 3.57% (in terms of accuracy) and 1.87% (in terms of Rouge-L),
respectively. AdaptBPE for MEDVOC works particularly well when reference
summaries have high OOV concentration or are longer in length. We also conduct
a human evaluation, revealing that AdaptBPE generates more relevant and more
faithful summaries as compared to MEDVOC. We make our codebase publicly
available at https://github.com/gb-kgp/adaptbpe.

摘要：在這項工作中，我們展示了詞彙適應方法的基本限制，該方法使用 Byte-Pair 編碼 (BPE) 標記化方案對預訓練語言模型 (PLM) 進行微調以適應專業領域。目前的做法是將目標領域特定詞彙瑣碎地附加在 PLM 詞彙的結尾。這種做法導致較低的優先級分數，並導致 BPE 中的次優標記化，而 BPE 迭代使用合併規則對給定文字進行標記化。為了減輕這個問題，我們提出了 AdaptBPE，其中 BPE 標記化初始化階段被修改為在字元層級標記化之前，首先對新增的（目標）詞彙執行最長字串比對。我們對 AdaptBPE 與各種分類和摘要任務上的標準 BPE 進行了廣泛評估；AdaptBPE 分別在準確度方面提升了 3.57%，在 Rouge-L 方面提升了 1.87%。當參考摘要具有高 OOV 集中度或長度較長時，MEDVOC 的 AdaptBPE 效果特別好。我們還進行了人工評估，結果顯示與 MEDVOC 相比，AdaptBPE 生成的摘要更相關、更忠實。我們將我們的程式碼庫公開在 https://github.com/gb-kgp/adaptbpe。

##### **Towards a Benchmark for Large Language Models for Business Process Management Tasks**
2410.03255v1 by Kiran Busch, Henrik Leopold

An increasing number of organizations are deploying Large Language Models
(LLMs) for a wide range of tasks. Despite their general utility, LLMs are prone
to errors, ranging from inaccuracies to hallucinations. To objectively assess
the capabilities of existing LLMs, performance benchmarks are conducted.
However, these benchmarks often do not translate to more specific real-world
tasks. This paper addresses the gap in benchmarking LLM performance in the
Business Process Management (BPM) domain. Currently, no BPM-specific benchmarks
exist, creating uncertainty about the suitability of different LLMs for BPM
tasks. This paper systematically compares LLM performance on four BPM tasks
focusing on small open-source models. The analysis aims to identify
task-specific performance variations, compare the effectiveness of open-source
versus commercial models, and assess the impact of model size on BPM task
performance. This paper provides insights into the practical applications of
LLMs in BPM, guiding organizations in selecting appropriate models for their
specific needs.

摘要：越來越多組織正在部署大型語言模型 (LLM) 來執行各種任務。儘管 LLM 具有普遍的效用，但它們容易發生錯誤，從不準確到幻覺。為了客觀評估現有 LLM 的能力，會進行效能基準測試。但是，這些基準測試通常無法轉換為更具體的實際任務。本文探討了在業務流程管理 (BPM) 領域中對 LLM 效能進行基準測試的差距。目前，沒有專門針對 BPM 的基準測試，這使得不同 LLM 是否適合 BPM 任務產生不確定性。本文系統性地比較了 LLM 在四個 BPM 任務上的效能，重點放在小型開源模型上。分析的目的是找出特定任務的效能變化，比較開源模型與商業模型的有效性，並評估模型大小對 BPM 任務效能的影響。本文提供了 LLM 在 BPM 中實際應用的見解，指導組織為其特定需求選擇適當的模型。

##### **Are Expert-Level Language Models Expert-Level Annotators?**
2410.03254v1 by Yu-Min Tseng, Wei-Lin Chen, Chung-Chi Chen, Hsin-Hsi Chen

Data annotation refers to the labeling or tagging of textual data with
relevant information. A large body of works have reported positive results on
leveraging LLMs as an alternative to human annotators. However, existing
studies focus on classic NLP tasks, and the extent to which LLMs as data
annotators perform in domains requiring expert knowledge remains underexplored.
In this work, we investigate comprehensive approaches across three highly
specialized domains and discuss practical suggestions from a cost-effectiveness
perspective. To the best of our knowledge, we present the first systematic
evaluation of LLMs as expert-level data annotators.

摘要：資料標註是指使用相關資訊標記或標籤文字資料。大量作品已報告利用 LLM 作為人工標註者的替代方案獲得正面成果。然而，現有研究著重於傳統的 NLP 任務，而 LLM 作為資料標註者在需要專業知識的領域中的執行程度仍未充分探討。在這項工作中，我們探討了三個高度專業化領域的全面方法，並從成本效益的角度討論實用建議。據我們所知，我們提出了 LLM 作為專家級資料標註者的首次系統性評估。

##### **How much can we forget about Data Contamination?**
2410.03249v1 by Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg

The leakage of benchmark data into the training data has emerged as a
significant challenge for evaluating the capabilities of large language models
(LLMs). In this work, we use experimental evidence and theoretical estimates to
challenge the common assumption that small-scale contamination renders
benchmark evaluations invalid. First, we experimentally quantify the magnitude
of benchmark overfitting based on scaling along three dimensions: The number of
model parameters (up to 1.6B), the number of times an example is seen (up to
144), and the number of training tokens (up to 40B). We find that if model and
data follow the Chinchilla scaling laws, minor contamination indeed leads to
overfitting. At the same time, even 144 times of contamination can be forgotten
if the training data is scaled beyond five times Chinchilla, a regime
characteristic of many modern LLMs. We then derive a simple theory of example
forgetting via cumulative weight decay. It allows us to bound the number of
gradient steps required to forget past data for any training run where we know
the hyperparameters of AdamW. This indicates that many LLMs, including Llama 3,
have forgotten the data seen at the beginning of training. Experimentally, we
demonstrate that forgetting occurs faster than what is predicted by our bounds.
Taken together, our results suggest that moderate amounts of contamination can
be forgotten at the end of realistically scaled training runs.

摘要：基準資料外洩到訓練資料中，已成為評估大型語言模型 (LLM) 能力的一項重大挑戰。在這項工作中，我們使用實驗證據和理論估計，來挑戰小規模污染會讓基準評估失效的普遍假設。首先，我們根據三個面向的規模化，實驗量化基準過度擬合的幅度：模型參數數量 (最高 1.6B)、範例被看見的次數 (最高 144) 和訓練權杖數量 (最高 40B)。我們發現，如果模型和資料遵循 Chinchilla 的規模化法則，輕微的污染確實會導致過度擬合。同時，如果訓練資料規模化超過 Chinchilla 的五倍，即使 144 倍的污染也能被遺忘，而這正是許多現代 LLM 的特徵。然後，我們透過累積權重衰減，推導出一個簡單的範例遺忘理論。這讓我們能夠限制忘記過去資料所需的梯度步驟數量，適用於我們已知 AdamW 超參數的任何訓練執行。這表示許多 LLM，包括 Llama 3，都已忘記在訓練開始時看到的資料。透過實驗，我們證明遺忘的速度比我們設定的界限預測得更快。綜合我們的結果，建議在實際規模的訓練執行結束時，可以遺忘適度的污染量。

##### **Beyond Film Subtitles: Is YouTube the Best Approximation of Spoken Vocabulary?**
2410.03240v1 by Adam Nohejl, Frederikus Hudi, Eunike Andriani Kardinata, Shintaro Ozaki, Maria Angelica Riera Machin, Hongyu Sun, Justin Vasselli, Taro Watanabe

Word frequency is a key variable in psycholinguistics, useful for modeling
human familiarity with words even in the era of large language models (LLMs).
Frequency in film subtitles has proved to be a particularly good approximation
of everyday language exposure. For many languages, however, film subtitles are
not easily available, or are overwhelmingly translated from English. We
demonstrate that frequencies extracted from carefully processed YouTube
subtitles provide an approximation comparable to, and often better than, the
best currently available resources. Moreover, they are available for languages
for which a high-quality subtitle or speech corpus does not exist. We use
YouTube subtitles to construct frequency norms for five diverse languages,
Chinese, English, Indonesian, Japanese, and Spanish, and evaluate their
correlation with lexical decision time, word familiarity, and lexical
complexity. In addition to being strongly correlated with two psycholinguistic
variables, a simple linear regression on the new frequencies achieves a new
high score on a lexical complexity prediction task in English and Japanese,
surpassing both models trained on film subtitle frequencies and the LLM GPT-4.
Our code, the frequency lists, fastText word embeddings, and statistical
language models are freely available at https://github.com/naist-nlp/tubelex.

摘要：字詞頻率是心理語言學中的關鍵變數，即使在大語言模型（LLM）的時代，對於建模人類對字詞的熟悉度也很有用。電影字幕中的頻率已被證明是日常語言接觸的特別好的近似值。然而，對於許多語言來說，電影字幕不容易取得，或者絕大部分都是從英文翻譯過來的。我們證明，從經過仔細處理的 YouTube 字幕中提取的頻率提供了可與目前最好的可用資源相媲美，甚至更好的近似值。此外，它們可供沒有高品質字幕或語料庫的語言使用。我們使用 YouTube 字幕為五種不同的語言（中文、英文、印尼文、日文和西班牙文）建立頻率規範，並評估它們與詞彙決策時間、字詞熟悉度和詞彙複雜度的相關性。除了與兩個心理語言學變數密切相關之外，對新頻率進行簡單的線性迴歸，在英文和日文的詞彙複雜度預測任務中取得了新的高分，超越了使用電影字幕頻率和 LLM GPT-4 訓練的模型。我們的程式碼、頻率清單、fastText 字詞嵌入和統計語言模型可在 https://github.com/naist-nlp/tubelex 免費取得。

##### **Enriching Ontologies with Disjointness Axioms using Large Language Models**
2410.03235v1 by Elias Crum, Antonio De Santis, Manon Ovide, Jiaxin Pan, Alessia Pisu, Nicolas Lazzari, Sebastian Rudolph

Ontologies often lack explicit disjointness declarations between classes,
despite their usefulness for sophisticated reasoning and consistency checking
in Knowledge Graphs. In this study, we explore the potential of Large Language
Models (LLMs) to enrich ontologies by identifying and asserting class
disjointness axioms. Our approach aims at leveraging the implicit knowledge
embedded in LLMs, using prompt engineering to elicit this knowledge for
classifying ontological disjointness. We validate our methodology on the
DBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs,
when guided by effective prompt strategies, can reliably identify disjoint
class relationships, thus streamlining the process of ontology completion
without extensive manual input. For comprehensive disjointness enrichment, we
propose a process that takes logical relationships between disjointness and
subclass statements into account in order to maintain satisfiability and reduce
the number of calls to the LLM. This work provides a foundation for future
applications of LLMs in automated ontology enhancement and offers insights into
optimizing LLM performance through strategic prompt design. Our code is
publicly available on GitHub at https://github.com/n28div/llm-disjointness.

摘要：本体論通常缺乏類別之間明確的不相交聲明，儘管它們對於知識圖譜中的精密推理和一致性檢查很有用。在本研究中，我們探討了大型語言模型 (LLM) 的潛力，通過識別和斷言類別不相交公理來豐富本体論。我們的做法旨在利用嵌入在 LLM 中的隱式知識，利用提示工程來引出這種知識以分類本体論不相交。我們在 DBpedia 本体論上驗證了我們的方法，重點關注開源 LLM。我們的研究結果表明，LLM 在有效提示策略的指導下，可以可靠地識別不相交類別關係，從而簡化本体論完成過程，而無需大量手動輸入。對於全面的不相交豐富，我們提出了一個過程，該過程考慮了不相交和子類別陳述之間的邏輯關係，以維持可滿足性並減少對 LLM 的調用次數。這項工作為 LLM 在自動本体論增強中的未來應用奠定了基礎，並提供了通過策略提示設計優化 LLM 性能的見解。我們的代碼在 GitHub 上公開，網址為 https://github.com/n28div/llm-disjointness。

##### **Showing LLM-Generated Code Selectively Based on Confidence of LLMs**
2410.03234v1 by Jia Li, Yuqi Zhu, Yongmin Li, Ge Li, Zhi Jin

Large Language Models (LLMs) have shown impressive abilities in code
generation, but they may generate erroneous programs. Reading a program takes
ten times longer than writing it. Showing these erroneous programs to
developers will waste developers' energies and introduce security risks to
software.
  To address the above limitations, we propose HonestCoder, a novel LLM-based
code generation approach. HonestCoder selectively shows the generated programs
to developers based on LLMs' confidence. The confidence provides valuable
insights into the correctness of generated programs. To achieve this goal, we
propose a novel approach to estimate LLMs' confidence in code generation. It
estimates confidence by measuring the multi-modal similarity between
LLMs-generated programs.
  We collect and release a multilingual benchmark named TruthCodeBench, which
consists of 2,265 samples and covers two popular programming languages (i.e.,
Python and Java). We apply HonestCoder to four popular LLMs (e.g.,
DeepSeek-Coder and Code Llama) and evaluate it on TruthCodeBench. Based on the
experiments, we obtain the following insights. (1) HonestCoder can effectively
estimate LLMs' confidence and accurately determine the correctness of generated
programs. For example, HonestCoder outperforms the state-of-the-art baseline by
27.79% in AUROC and 63.74% in AUCPR. (2) HonestCoder can decrease the number of
erroneous programs shown to developers. Compared to eight baselines, it can
show more correct programs and fewer erroneous programs to developers. (3)
Compared to showing code indiscriminately, HonestCoder only adds slight time
overhead (approximately 0.4 seconds per requirement). (4) We discuss future
directions to facilitate the application of LLMs in software development. We
hope this work can motivate broad discussions about measuring the reliability
of LLMs' outputs in performing code-related tasks.

摘要：大型語言模型 (LLM) 在程式碼生成方面展現驚人的能力，但它們可能會產生錯誤的程式。閱讀程式碼的時間是撰寫程式碼的十倍。將這些有錯誤的程式碼顯示給開發人員會浪費開發人員的精力，並對軟體引入安全風險。
為了解決上述限制，我們提出 HonestCoder，一種新的基於 LLM 的程式碼生成方法。HonestCoder 根據 LLM 的信心有選擇地向開發人員顯示生成的程式碼。信心提供對生成程式碼正確性的寶貴見解。為了實現這個目標，我們提出了一種新的方法來估計 LLM 在程式碼生成中的信心。它透過測量 LLM 生成的程式碼之間的多模式相似性來估計信心。
我們收集並發布了一個名為 TruthCodeBench 的多語言基準，其中包含 2,265 個範例，涵蓋兩種流行的程式語言（即 Python 和 Java）。我們將 HonestCoder 應用於四個流行的 LLM（例如 DeepSeek-Coder 和 Code Llama），並在 TruthCodeBench 上對其進行評估。根據實驗，我們獲得以下見解。（1）HonestCoder 可以有效估計 LLM 的信心，並準確確定生成程式碼的正確性。例如，HonestCoder 在 AUROC 中比最先進的基準高出 27.79%，在 AUCPR 中高出 63.74%。（2）HonestCoder 可以減少顯示給開發人員的錯誤程式碼數量。與八個基準相比，它可以向開發人員顯示更多正確的程式碼和更少的錯誤程式碼。（3）與不加選擇地顯示程式碼相比，HonestCoder 只會增加輕微的時間開銷（每個需求大約 0.4 秒）。（4）我們討論了促進 LLM 在軟體開發中應用的未來方向。我們希望這項工作可以激勵廣泛討論，以衡量 LLM 在執行與程式碼相關任務時的輸出可靠性。

##### **ALR$^2$: A Retrieve-then-Reason Framework for Long-context Question Answering**
2410.03227v1 by Huayang Li, Pat Verga, Priyanka Sen, Bowen Yang, Vijay Viswanathan, Patrick Lewis, Taro Watanabe, Yixuan Su

The context window of large language models (LLMs) has been extended
significantly in recent years. However, while the context length that the LLM
can process has grown, the capability of the model to accurately reason over
that context degrades noticeably. This occurs because modern LLMs often become
overwhelmed by the vast amount of information in the context; when answering
questions, the model must identify and reason over relevant evidence sparsely
distributed throughout the text. To alleviate the challenge of long-context
reasoning, we develop a retrieve-then-reason framework, enabling LLMs to reason
over relevant evidence collected during an intermediate retrieval step. We find
that modern LLMs struggle to accurately retrieve relevant facts and instead,
often hallucinate "retrieved facts", resulting in flawed reasoning and the
production of incorrect answers. To address these issues, we introduce ALR$^2$,
a method that augments the long-context reasoning capability of LLMs via an
explicit two-stage procedure, i.e., aligning LLMs with the objectives of both
retrieval and reasoning. We demonstrate the efficacy of ALR$^2$ for mitigating
performance degradation in long-context reasoning tasks. Through extensive
experiments on long-context QA benchmarks, we find our method to outperform
competitive baselines by large margins, achieving at least 8.4 and 7.9 EM gains
on the long-context versions of HotpotQA and SQuAD datasets, respectively.

摘要：近年來，大型語言模型 (LLM) 的上下文視窗已顯著擴展。然而，儘管 LLM 可處理的上下文長度已增加，但模型準確推理該上下文的效能卻明顯下降。這是因為現代 LLM 經常被大量上下文資訊淹沒；在回答問題時，模型必須識別並推論分散在整個文本中的相關證據。為了緩解長上下文推理的挑戰，我們開發了一個先擷取再推理的框架，使 LLM 能夠對在中間擷取步驟中收集到的相關證據進行推理。我們發現，現代 LLM 難以準確擷取相關事實，而經常會出現「擷取事實」的幻覺，導致推理有缺陷並產生不正確的答案。為了解決這些問題，我們引入了 ALR$^2$，這是一種透過明確的兩階段程序來增強 LLM 長上下文推理能力的方法，即讓 LLM 與擷取和推理的目標保持一致。我們展示了 ALR$^2$ 在減輕長上下文推理任務中效能下降方面的效力。透過對長上下文 QA 基準進行廣泛的實驗，我們發現我們的模型大幅優於競爭基準，分別在 HotpotQA 和 SQuAD 資料集的長上下文版本中獲得至少 8.4 和 7.9 EM 的增益。

##### **Frame-Voyager: Learning to Query Frames for Video Large Language Models**
2410.03226v1 by Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xioalei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, Qianru Sun

Video Large Language Models (Video-LLMs) have made remarkable progress in
video understanding tasks. However, they are constrained by the maximum length
of input tokens, making it impractical to input entire videos. Existing frame
selection approaches, such as uniform frame sampling and text-frame retrieval,
fail to account for the information density variations in the videos or the
complex instructions in the tasks, leading to sub-optimal performance. In this
paper, we propose Frame-Voyager that learns to query informative frame
combinations, based on the given textual queries in the task. To train
Frame-Voyager, we introduce a new data collection and labeling pipeline, by
ranking frame combinations using a pre-trained Video-LLM. Given a video of M
frames, we traverse its T-frame combinations, feed them into a Video-LLM, and
rank them based on Video-LLM's prediction losses. Using this ranking as
supervision, we train Frame-Voyager to query the frame combinations with lower
losses. In experiments, we evaluate Frame-Voyager on four Video Question
Answering benchmarks by plugging it into two different Video-LLMs. The
experimental results demonstrate that Frame-Voyager achieves impressive results
in all settings, highlighting its potential as a plug-and-play solution for
Video-LLMs.

摘要：影片大型語言模型（Video-LLM）在影片理解任務中取得顯著進展。然而，它們受到輸入令牌最大長度的限制，使得輸入整個影片變得不切實際。現有的影格選擇方法，例如均勻影格取樣和文字影格檢索，無法考量影片中的資訊密度變化或任務中的複雜指令，導致次佳效能。在本文中，我們提出 Frame-Voyager，它會根據任務中給定的文字查詢，學習查詢有資訊性的影格組合。為了訓練 Frame-Voyager，我們透過使用預先訓練的 Video-LLM 對影格組合進行排名，引進新的資料收集和標記管道。給定 M 個影格的影片，我們會遍歷其 T 個影格組合，將它們輸入 Video-LLM，並根據 Video-LLM 的預測損失對它們進行排名。使用此排名作為監督，我們訓練 Frame-Voyager 查詢損失較低的影格組合。在實驗中，我們透過將 Frame-Voyager 插入兩個不同的 Video-LLM，對其在四個影片問答基準進行評估。實驗結果證明，Frame-Voyager 在所有設定中均取得令人印象深刻的結果，突顯其作為 Video-LLM 即插即用解決方案的潛力。

##### **AutoPenBench: Benchmarking Generative Agents for Penetration Testing**
2410.03225v1 by Luca Gioacchini, Marco Mellia, Idilio Drago, Alexander Delsanto, Giuseppe Siracusano, Roberto Bifulco

Generative AI agents, software systems powered by Large Language Models
(LLMs), are emerging as a promising approach to automate cybersecurity tasks.
Among the others, penetration testing is a challenging field due to the task
complexity and the diverse strategies to simulate cyber-attacks. Despite
growing interest and initial studies in automating penetration testing with
generative agents, there remains a significant gap in the form of a
comprehensive and standard framework for their evaluation and development. This
paper introduces AutoPenBench, an open benchmark for evaluating generative
agents in automated penetration testing. We present a comprehensive framework
that includes 33 tasks, each representing a vulnerable system that the agent
has to attack. Tasks are of increasing difficulty levels, including in-vitro
and real-world scenarios. We assess the agent performance with generic and
specific milestones that allow us to compare results in a standardised manner
and understand the limits of the agent under test. We show the benefits of
AutoPenBench by testing two agent architectures: a fully autonomous and a
semi-autonomous supporting human interaction. We compare their performance and
limitations. For example, the fully autonomous agent performs unsatisfactorily
achieving a 21% Success Rate (SR) across the benchmark, solving 27% of the
simple tasks and only one real-world task. In contrast, the assisted agent
demonstrates substantial improvements, with 64% of SR. AutoPenBench allows us
also to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability
of the agents to complete the tasks. We believe that our benchmark fills the
gap with a standard and flexible framework to compare penetration testing
agents on a common ground. We hope to extend AutoPenBench along with the
research community by making it available under
https://github.com/lucagioacchini/auto-pen-bench.

摘要：生成式 AI 代理，由大型语言模型 (LLM) 驱动的软件系统，正成为一种很有前途的自动化网络安全任务的方法。在其他任务中，渗透测试由于任务复杂性和模拟网络攻击的不同策略而成为一项具有挑战性的领域。尽管人们对使用生成式代理自动化渗透测试越来越感兴趣并进行了初步研究，但仍存在一个重大差距，即缺乏用于评估和开发的全面且标准的框架。本文介绍了 AutoPenBench，这是一个用于评估生成式代理在自动化渗透测试中的开放基准。我们提出了一个全面的框架，其中包括 33 个任务，每个任务都代表代理必须攻击的易受攻击系统。任务的难度级别不断增加，包括体外和真实场景。我们使用通用和特定里程碑来评估代理性能，这使我们能够以标准化方式比较结果并了解受测代理的局限性。我们通过测试两种代理架构展示了 AutoPenBench 的优势：完全自主的架构和支持人机交互的半自主架构。我们比较了它们的性能和局限性。例如，完全自主代理的性能不令人满意，在基准测试中实现了 21% 的成功率 (SR)，解决了 27% 的简单任务和仅一个真实世界任务。相比之下，辅助代理表现出显着改善，SR 为 64%。AutoPenBench 也让我们观察到 GPT-4o 或 OpenAI o1 等不同的 LLM 如何影响代理完成任务的能力。我们相信我们的基准测试通过一个标准且灵活的框架填补了空白，可以在一个共同的基础上比较渗透测试代理。我们希望通过在 https://github.com/lucagioacchini/auto-pen-bench 下提供 AutoPenBench，与研究社区一起对其进行扩展。

##### **Consultation on Industrial Machine Faults with Large language Models**
2410.03223v1 by Apiradee Boonmee, Kritsada Wongsuwan, Pimchanok Sukjai

Industrial machine fault diagnosis is a critical component of operational
efficiency and safety in manufacturing environments. Traditional methods rely
heavily on expert knowledge and specific machine learning models, which can be
limited in their adaptability and require extensive labeled data. This paper
introduces a novel approach leveraging Large Language Models (LLMs),
specifically through a structured multi-round prompting technique, to improve
fault diagnosis accuracy. By dynamically crafting prompts, our method enhances
the model's ability to synthesize information from diverse data sources,
leading to improved contextual understanding and actionable recommendations.
Experimental results demonstrate that our approach outperforms baseline models,
achieving an accuracy of 91% in diagnosing various fault types. The findings
underscore the potential of LLMs in revolutionizing industrial fault
consultation practices, paving the way for more effective maintenance
strategies in complex environments.

摘要：工業機器的故障診斷是製造環境中營運效率和安全性的關鍵組成部分。傳統方法高度依賴專家知識和特定的機器學習模型，這些模型的適應性可能受到限制，並且需要大量的標籤資料。本文介紹了一種利用大型語言模型 (LLM) 的新方法，特別是透過結構化的多輪提示技術，以提高故障診斷的準確性。透過動態建立提示，我們的模型可以從不同的資料來源中綜合資訊，進而改善脈絡理解和可行的建議。實驗結果證明，我們的模型優於基準模型，在診斷各種故障類型時達到 91% 的準確性。這些發現強調了 LLM 在革新工業故障諮詢實務上的潛力，為在複雜環境中制定更有效的維護策略鋪路。

##### **NLIP_Lab-IITH Low-Resource MT System for WMT24 Indic MT Shared Task**
2410.03215v1 by Pramit Sahoo, Maharaj Brahma, Maunendra Sankar Desarkar

In this paper, we describe our system for the WMT 24 shared task of
Low-Resource Indic Language Translation. We consider eng $\leftrightarrow$ {as,
kha, lus, mni} as participating language pairs. In this shared task, we explore
the finetuning of a pre-trained model motivated by the pre-trained objective of
aligning embeddings closer by alignment augmentation \cite{lin-etal-2020-pre}
for 22 scheduled Indian languages. Our primary system is based on
language-specific finetuning on a pre-trained model. We achieve chrF2 scores of
50.6, 42.3, 54.9, and 66.3 on the official public test set for
eng$\rightarrow$as, eng$\rightarrow$kha, eng$\rightarrow$lus,
eng$\rightarrow$mni respectively. We also explore multilingual training
with/without language grouping and layer-freezing. Our code, models, and
generated translations are available here:
https://github.com/pramitsahoo/WMT2024-LRILT.

摘要：在本文中，我們描述了我們在 WMT 24 低資源印度語言翻譯共享任務中的系統。我們將 eng $\leftrightarrow$ {as, kha, lus, mni} 視為參與語言對。在此共享任務中，我們探討了預訓練模型的微調，其動機是預訓練目標，即通過對齊擴充來更緊密地對齊嵌入 \cite{lin-etal-2020-pre}，適用於 22 種預定的印度語言。我們的首要系統基於對預訓練模型的語言特定微調。我們在 eng$\rightarrow$as、eng$\rightarrow$kha、eng$\rightarrow$lus、eng$\rightarrow$mni 的官方公開測試集中實現了 chrF2 分數 50.6、42.3、54.9 和 66.3。我們還探討了有/無語言分組和層凍結的多語言訓練。我們的程式碼、模型和產生的翻譯可以在這裡找到：
https://github.com/pramitsahoo/WMT2024-LRILT。

##### **Learning Semantic Structure through First-Order-Logic Translation**
2410.03203v1 by Akshay Chaturvedi, Nicholas Asher

In this paper, we study whether transformer-based language models can extract
predicate argument structure from simple sentences. We firstly show that
language models sometimes confuse which predicates apply to which objects. To
mitigate this, we explore two tasks: question answering (Q/A), and first order
logic (FOL) translation, and two regimes, prompting and finetuning. In FOL
translation, we finetune several large language models on synthetic datasets
designed to gauge their generalization abilities. For Q/A, we finetune encoder
models like BERT and RoBERTa and use prompting for LLMs. The results show that
FOL translation for LLMs is better suited to learn predicate argument
structure.

摘要：在本文中，我們研究基於 Transformer 的語言模型是否能從簡單句子中提取謂詞論元結構。我們首先展示語言模型有時會混淆哪些謂詞適用於哪些物件。為了減輕這種情況，我們探討兩種任務：問答 (Q/A) 和一階邏輯 (FOL) 翻譯，以及兩種模式：提示和微調。在 FOL 翻譯中，我們在合成資料集上微調了數個大型語言模型，以評估它們的泛化能力。對於問答，我們微調了 BERT 和 RoBERTa 等編碼器模型，並使用提示來進行 LLM。結果顯示，對於 LLM 而言，FOL 翻譯更適合學習謂詞論元結構。

##### **PersoBench: Benchmarking Personalized Response Generation in Large Language Models**
2410.03198v1 by Saleh Afzoon, Usman Naseem, Amin Beheshti, Zahra Jamali

While large language models (LLMs) have exhibited impressive conversational
capabilities, their proficiency in delivering personalized responses remains
unclear. Although recent benchmarks automatically evaluate persona consistency
in role-playing contexts using LLM-based judgment, the evaluation of
personalization in response generation remains underexplored. To address this
gap, we present a new benchmark, PersoBench, to evaluate the personalization
ability of LLMs in persona-aware dialogue generation within a zero-shot
setting. We assess the performance of three open-source and three closed-source
LLMs using well-known datasets and a range of metrics. Our analysis, conducted
on three well-known persona-aware datasets, evaluates multiple dimensions of
response quality, including fluency, diversity, coherence, and personalization,
across both standard and chain-of-thought prompting methods. Our findings
reveal that while LLMs excel at generating fluent and diverse responses, they
are far from satisfactory in delivering personalized and coherent responses
considering both the conversation context and the provided personas. Our
benchmark implementation is available at
https://github.com/salehafzoon/PersoBench.

摘要：儘管大型語言模型 (LLM) 已展現令人印象深刻的對話能力，但它們提供個人化回應的熟練度仍不明確。儘管最近的基準使用基於 LLM 的判斷在角色扮演情境中自動評估角色一致性，但回應生成中的個人化評估仍未被充分探討。為了解決這個差距，我們提出一個新的基準 PersoBench，以評估 LLM 在零次學習設定中以角色為意識的對話生成中的個人化能力。我們使用知名資料集和一系列指標評估三個開源和三個閉源 LLM 的效能。我們的分析在三個知名角色意識資料集上進行，評估回應品質的多個面向，包括流暢度、多樣性、一致性和個人化，並採用標準和思考鏈提示方法。我們的研究結果顯示，儘管 LLM 在生成流暢且多樣化的回應方面表現出色，但在提供個人化和一致的回應方面，考慮到對話情境和提供的角色時，它們遠未令人滿意。我們的基準實作可在 https://github.com/salehafzoon/PersoBench 取得。

##### **Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages**
2410.03197v1 by Seonjeong Hwang, Yunsu Kim, Gary Geunbae Lee

Automatic question generation (QG) serves a wide range of purposes, such as
augmenting question-answering (QA) corpora, enhancing chatbot systems, and
developing educational materials. Despite its importance, most existing
datasets predominantly focus on English, resulting in a considerable gap in
data availability for other languages. Cross-lingual transfer for QG (XLT-QG)
addresses this limitation by allowing models trained on high-resource language
datasets to generate questions in low-resource languages. In this paper, we
propose a simple and efficient XLT-QG method that operates without the need for
monolingual, parallel, or labeled data in the target language, utilizing a
small language model. Our model, trained solely on English QA datasets, learns
interrogative structures from a limited set of question exemplars, which are
then applied to generate questions in the target language. Experimental results
show that our method outperforms several XLT-QG baselines and achieves
performance comparable to GPT-3.5-turbo across different languages.
Additionally, the synthetic data generated by our model proves beneficial for
training multilingual QA models. With significantly fewer parameters than large
language models and without requiring additional training for target languages,
our approach offers an effective solution for QG and QA tasks across various
languages.

摘要：自動問題生成 (QG) 可用於各種目的，例如擴充問答 (QA) 資料庫、增強聊天機器人系統，以及開發教育材料。儘管其重要性，但現有的資料集大多以英語為主，導致其他語言的資料可用性存在相當大的差距。跨語言轉移 QG (XLT-QG) 透過允許在高資源語言資料集上訓練的模型來產生低資源語言的問題，來解決這個限制。在本文中，我們提出一個簡單且有效率的 XLT-QG 方法，該方法無需在目標語言中使用單語、平行或標記資料，而是利用一個小型語言模型。我們的模型僅在英語 QA 資料集上訓練，從一組有限的問題範例中學習疑問結構，然後將其應用於產生目標語言中的問題。實驗結果表明，我們的模型優於多個 XLT-QG 基準，並且在不同語言中實現了與 GPT-3.5-turbo 相當的效能。此外，我們的模型產生的合成資料被證明對訓練多語言 QA 模型有益。與大型語言模型相比，我們的模型參數明顯較少，並且不需要對目標語言進行額外訓練，因此我們的模型為各種語言的 QG 和 QA 任務提供了一個有效的解決方案。

##### **Parallel Corpus Augmentation using Masked Language Models**
2410.03194v1 by Vibhuti Kumari, Narayana Murthy Kavi

In this paper we propose a novel method of augmenting parallel text corpora
which promises good quality and is also capable of producing many fold larger
corpora than the seed corpus we start with. We do not need any additional
monolingual corpora. We use Multi-Lingual Masked Language Model to mask and
predict alternative words in context and we use Sentence Embeddings to check
and select sentence pairs which are likely to be translations of each other. We
cross check our method using metrics for MT Quality Estimation. We believe this
method can greatly alleviate the data scarcity problem for all language pairs
for which a reasonable seed corpus is available.

摘要：在本文中，我們提出了一種擴充平行文本語料庫的新方法，該方法有望獲得良好的品質，並且能夠產生比我們開始使用的種子語料庫大得多的語料庫。我們不需要任何額外的單語語料庫。我們使用多語言掩蔽語言模型來掩蔽和預測上下文中的替代詞，並且我們使用句子嵌入來檢查和選擇可能彼此翻譯的句子對。我們使用機器翻譯品質評估指標交叉檢查我們的模型。我們相信這種方法可以大大緩解所有可獲得合理種子語料庫的語言對的資料稀缺問題。

##### **MultiVerse: Efficient and Expressive Zero-Shot Multi-Task Text-to-Speech**
2410.03192v1 by Taejun Bak, Youngsik Eom, SeungJae Choi, Young-Sun Joo

Text-to-speech (TTS) systems that scale up the amount of training data have
achieved significant improvements in zero-shot speech synthesis. However, these
systems have certain limitations: they require a large amount of training data,
which increases costs, and often overlook prosody similarity. To address these
issues, we propose MultiVerse, a zero-shot multi-task TTS system that is able
to perform TTS or speech style transfer in zero-shot and cross-lingual
conditions. MultiVerse requires much less training data than traditional
data-driven approaches. To ensure zero-shot performance even with limited data,
we leverage source-filter theory-based disentanglement, utilizing the prompt
for modeling filter-related and source-related representations. Additionally,
to further enhance prosody similarity, we adopt a prosody modeling approach
combining prompt-based autoregressive and non-autoregressive methods.
Evaluations demonstrate the remarkable zero-shot multi-task TTS performance of
MultiVerse and show that MultiVerse not only achieves zero-shot TTS performance
comparable to data-driven TTS systems with much less data, but also
significantly outperforms other zero-shot TTS systems trained with the same
small amount of data. In particular, our novel prosody modeling technique
significantly contributes to MultiVerse's ability to generate speech with high
prosody similarity to the given prompts. Our samples are available at
https://nc-ai.github.io/speech/publications/multiverse/index.html

摘要：文字轉語音（TTS）系統擴大了訓練資料的數量，在零次學習語音合成方面取得了顯著的進步。然而，這些系統存在一定的限制：它們需要大量的訓練資料，這會增加成本，而且常常忽略音調相似性。為了解決這些問題，我們提出 MultiVerse，這是一個零次學習的多任務 TTS 系統，能夠在零次學習和跨語言條件下執行 TTS 或語音風格轉換。與傳統的數據驅動方法相比，MultiVerse 所需的訓練資料要少得多。為了確保即使在資料有限的情況下也能進行零次學習，我們利用基於源濾波器理論的解耦，利用提示對與濾波器相關和與源相關的表示進行建模。此外，為了進一步增強音調相似性，我們採用了結合基於提示的自迴歸和非自迴歸方法的音調建模方法。評估證明了 MultiVerse 的零次學習多任務 TTS 性能，並表明 MultiVerse 不僅實現了與數據驅動 TTS 系統相當的零次學習 TTS 性能，而且還顯著優於使用相同少量資料訓練的其他零次學習 TTS 系統。特別是，我們新穎的音調建模技術顯著提升了 MultiVerse 生成與給定提示具有高音調相似性的語音的能力。我們的範例可以在 https://nc-ai.github.io/speech/publications/multiverse/index.html 獲得

##### **Looking into Concept Explanation Methods for Diabetic Retinopathy Classification**
2410.03188v1 by Andrea M. Storås, Josefine V. Sundgaard

Diabetic retinopathy is a common complication of diabetes, and monitoring the
progression of retinal abnormalities using fundus imaging is crucial. Because
the images must be interpreted by a medical expert, it is infeasible to screen
all individuals with diabetes for diabetic retinopathy. Deep learning has shown
impressive results for automatic analysis and grading of fundus images. One
drawback is, however, the lack of interpretability, which hampers the
implementation of such systems in the clinic. Explainable artificial
intelligence methods can be applied to explain the deep neural networks.
Explanations based on concepts have shown to be intuitive for humans to
understand, but have not yet been explored in detail for diabetic retinopathy
grading. This work investigates and compares two concept-based explanation
techniques for explaining deep neural networks developed for automatic
diagnosis of diabetic retinopathy: Quantitative Testing with Concept Activation
Vectors and Concept Bottleneck Models. We found that both methods have
strengths and weaknesses, and choice of method should take the available data
and the end user's preferences into account.

摘要：糖尿病視網膜病變是糖尿病的常見併發症，使用眼底成像監控視網膜異常的進展至關重要。由於影像必須由醫療專家解釋，因此不可能篩選出所有患有糖尿病視網膜病變的糖尿病患者。深度學習已在眼底影像的自動分析和分級方面展現出令人印象深刻的成果。然而，其中一個缺點是缺乏可解釋性，這阻礙了此類系統在臨床上的實施。可解釋的人工智慧方法可應用於解釋深度神經網路。基於概念的解釋已被證明對人類來說直觀易懂，但尚未詳細探討糖尿病視網膜病變分級。這項工作探討並比較了兩種基於概念的解釋技術，用於解釋為糖尿病視網膜病變自動診斷而開發的深度神經網路：使用概念激活向量進行的定量測試和概念瓶頸模型。我們發現這兩種方法各有優缺點，方法的選擇應考慮可用的資料和最終使用者的偏好。

##### **EXAQ: Exponent Aware Quantization For LLMs Acceleration**
2410.03185v1 by Moran Shkolnik, Maxim Fishman, Brian Chmiel, Hilla Ben-Yaacov, Ron Banner, Kfir Yehuda Levy

Quantization has established itself as the primary approach for decreasing
the computational and storage expenses associated with Large Language Models
(LLMs) inference. The majority of current research emphasizes quantizing
weights and activations to enable low-bit general-matrix-multiply (GEMM)
operations, with the remaining non-linear operations executed at higher
precision. In our study, we discovered that following the application of these
techniques, the primary bottleneck in LLMs inference lies in the softmax layer.
The softmax operation comprises three phases: exponent calculation,
accumulation, and normalization, Our work focuses on optimizing the first two
phases. We propose an analytical approach to determine the optimal clipping
value for the input to the softmax function, enabling sub-4-bit quantization
for LLMs inference. This method accelerates the calculations of both $e^x$ and
$\sum(e^x)$ with minimal to no accuracy degradation. For example, in
LLaMA1-30B, we achieve baseline performance with 2-bit quantization on the
well-known "Physical Interaction: Question Answering" (PIQA) dataset
evaluation. This ultra-low bit quantization allows, for the first time, an
acceleration of approximately 4x in the accumulation phase. The combination of
accelerating both $e^x$ and $\sum(e^x)$ results in a 36.9% acceleration in the
softmax operation.

摘要：量化已成為減少與大語言模型 (LLM) 推論相關的運算和儲存開銷的主要方法。目前大多數的研究都強調量化權重和啟用，以實現低位元通用矩陣乘法 (GEMM) 運算，而其餘非線性運算則以較高精度執行。在我們的研究中，我們發現，在應用這些技術後，LLM 推論中的主要瓶頸在於 softmax 層。softmax 運算包含三個階段：指數計算、累積和正規化，我們的研究重點在於最佳化前兩個階段。我們提出了一種分析方法來確定 softmax 函數輸入的最佳裁剪值，從而實現 LLM 推論的 4 位元以下量化。此方法可加速計算 $e^x$ 和 $\sum(e^x)$，同時將精確度降低到最小甚至不降低。例如，在 LLaMA1-30B 中，我們在著名的「物理互動：問答」(PIQA) 資料集評估中，以 2 位元量化達到了基準效能。這種超低位元量化首次允許在累積階段加速約 4 倍。加速 $e^x$ 和 $\sum(e^x)$ 的結合，使 softmax 運算加速了 36.9%。

##### **Generating bilingual example sentences with large language models as lexicography assistants**
2410.03182v1 by Raphael Merx, Ekaterina Vylomova, Kemal Kurniawan

We present a study of LLMs' performance in generating and rating example
sentences for bilingual dictionaries across languages with varying resource
levels: French (high-resource), Indonesian (mid-resource), and Tetun
(low-resource), with English as the target language. We evaluate the quality of
LLM-generated examples against the GDEX (Good Dictionary EXample) criteria:
typicality, informativeness, and intelligibility. Our findings reveal that
while LLMs can generate reasonably good dictionary examples, their performance
degrades significantly for lower-resourced languages. We also observe high
variability in human preferences for example quality, reflected in low
inter-annotator agreement rates. To address this, we demonstrate that
in-context learning can successfully align LLMs with individual annotator
preferences. Additionally, we explore the use of pre-trained language models
for automated rating of examples, finding that sentence perplexity serves as a
good proxy for typicality and intelligibility in higher-resourced languages.
Our study also contributes a novel dataset of 600 ratings for LLM-generated
sentence pairs, and provides insights into the potential of LLMs in reducing
the cost of lexicographic work, particularly for low-resource languages.

摘要：<paragraph>我們提出針對不同資源水準語言的雙語字典進行範例句產生和評分的 LLM 效能研究：法語（高資源）、印尼語（中資源）和德頓語（低資源），以英語為目標語言。我們根據 GDEX（良好字典範例）標準評估 LLM 產生的範例品質：典型性、資訊性和可理解性。我們的研究結果顯示，儘管 LLM 能產生相當好的字典範例，但其效能會隨著資源水準較低的語言而大幅降低。我們也觀察到，人類對範例品質的偏好具有高度變異性，這反映在低標註者間一致性評分中。為了解決這個問題，我們證明情境學習能成功地讓 LLM 與個別標註者的偏好一致。此外，我們探討使用預先訓練的語言模型進行範例自動評分，發現句子困惑度可用作高資源語言中典型性和可理解性的良好指標。我們的研究還提供了一個包含 600 個 LLM 產生的句子對評分的全新資料集，並深入探討 LLM 在降低詞彙學工作成本方面的潛力，特別是對於低資源語言。</paragraph>

##### **Kiss up, Kick down: Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas**
2410.03181v1 by Seungjong Sun, Eungu Lee, Seo Yeon Baek, Seunghyun Hwang, Wonbyung Lee, Dongyan Nan, Bernard J. Jansen, Jang Hyun Kim

This study is the first to explore whether multi-modal large language models
(LLMs) can align their behaviors with visual personas, addressing a significant
gap in the literature that predominantly focuses on text-based personas. We
developed a novel dataset of 5K fictional avatar images for assignment as
visual personas to LLMs, and analyzed their negotiation behaviors based on the
visual traits depicted in these images, with a particular focus on
aggressiveness. The results indicate that LLMs assess the aggressiveness of
images in a manner similar to humans and output more aggressive negotiation
behaviors when prompted with an aggressive visual persona. Interestingly, the
LLM exhibited more aggressive negotiation behaviors when the opponent's image
appeared less aggressive than their own, and less aggressive behaviors when the
opponents image appeared more aggressive.

摘要：本研究首次探討多模態大型語言模型 (LLM) 是否能將其行為與視覺角色對齊，填補了文獻中主要關注基於文字的角色的重大空白。我們開發了一個包含 5,000 個虛構頭像圖片的新穎資料集，以將視覺角色分配給 LLM，並根據這些圖片中描繪的視覺特徵分析他們的協商行為，特別關注侵略性。結果表明，LLM 以類似於人類的方式評估圖片的侵略性，並且在以侵略性視覺角色提示時輸出更具侵略性的協商行為。有趣的是，當對手的圖片看起來比自己的圖片不那麼具有侵略性時，LLM 表現出更具侵略性的協商行為；而當對手的圖片看起來更具侵略性時，則表現出不那麼具有侵略性的行為。

##### **Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models**
2410.03176v1 by Yufang Liu, Tao Ji, Changzhi Sun, Yuanbin Wu, Aimin Zhou

Large Vision-Language Models (LVLMs) have achieved impressive performance,
yet research has pointed out a serious issue with object hallucinations within
these models. However, there is no clear conclusion as to which part of the
model these hallucinations originate from. In this paper, we present an
in-depth investigation into the object hallucination problem specifically
within the CLIP model, which serves as the backbone for many state-of-the-art
vision-language systems. We unveil that even in isolation, the CLIP model is
prone to object hallucinations, suggesting that the hallucination problem is
not solely due to the interaction between vision and language modalities. To
address this, we propose a counterfactual data augmentation method by creating
negative samples with a variety of hallucination issues. We demonstrate that
our method can effectively mitigate object hallucinations for CLIP model, and
we show the the enhanced model can be employed as a visual encoder, effectively
alleviating the object hallucination issue in LVLMs.

摘要：大型視覺語言模型 (LVLMs) 已取得令人印象深刻的表現，但研究指出這些模型中存在嚴重的物件幻覺問題。然而，目前尚不清楚這些幻覺源自模型的哪個部分。在本文中，我們對 CLIP 模型中的物件幻覺問題進行深入探討，該模型是許多最先進的視覺語言系統的骨幹。我們揭示，即使在孤立的情況下，CLIP 模型也容易產生物件幻覺，這表明幻覺問題並非僅僅是由於視覺和語言模組之間的互動所致。為了解決這個問題，我們提出了一種反事實資料擴充方法，通過建立具有各種幻覺問題的負面樣本。我們證明了我們的方法可以有效減輕 CLIP 模型的物件幻覺，並且我們展示了增強後的模型可以用作視覺編碼器，有效緩解 LVLMs 中的物件幻覺問題。

##### **Autoregressive Large Language Models are Computationally Universal**
2410.03170v1 by Dale Schuurmans, Hanjun Dai, Francesco Zanini

We show that autoregressive decoding of a transformer-based language model
can realize universal computation, without external intervention or
modification of the model's weights. Establishing this result requires
understanding how a language model can process arbitrarily long inputs using a
bounded context. For this purpose, we consider a generalization of
autoregressive decoding where, given a long input, emitted tokens are appended
to the end of the sequence as the context window advances. We first show that
the resulting system corresponds to a classical model of computation, a Lag
system, that has long been known to be computationally universal. By leveraging
a new proof, we show that a universal Turing machine can be simulated by a Lag
system with 2027 production rules. We then investigate whether an existing
large language model can simulate the behaviour of such a universal Lag system.
We give an affirmative answer by showing that a single system-prompt can be
developed for gemini-1.5-pro-001 that drives the model, under deterministic
(greedy) decoding, to correctly apply each of the 2027 production rules. We
conclude that, by the Church-Turing thesis, prompted gemini-1.5-pro-001 with
extended autoregressive (greedy) decoding is a general purpose computer.

摘要：我們證明Transformer語言模型的自動回歸解碼
可以在沒有外部介入或修改模型權重的狀況下
實現通用運算。建立此結果需要
了解語言模型如何使用
有界上下文處理任意長的輸入。為此目的，我們考慮
自動回歸解碼的概括，其中，給定一個長輸入，發射的符號
會附加到序列的結尾，因為上下文視窗會前進。我們首先展示
結果系統對應於一個經典運算模型，一個 Lag
系統，它早已被知道是計算通用的。透過利用
新的證明，我們展示一個通用圖靈機可以被一個 Lag
系統模擬，有 2027 個產生規則。然後我們調查一個現有的
大型語言模型是否可以模擬這樣一個通用 Lag 系統的行為。
我們給出肯定的答案，展示一個單一的系統提示可以
為 gemini-1.5-pro-001 開發，驅動模型，在確定性的
（貪婪的）解碼下，正確地應用每個 2027 個產生規則。我們
得出結論，透過邱奇-圖靈論文，提示 gemini-1.5-pro-001 有
延伸的自動回歸（貪婪的）解碼是一個通用電腦。

##### **Can Watermarked LLMs be Identified by Users via Crafted Prompts?**
2410.03168v1 by Aiwei Liu, Sheng Guan, Yiming Liu, Leyi Pan, Yifei Zhang, Liancheng Fang, Lijie Wen, Philip S. Yu, Xuming Hu

Text watermarking for Large Language Models (LLMs) has made significant
progress in detecting LLM outputs and preventing misuse. Current watermarking
techniques offer high detectability, minimal impact on text quality, and
robustness to text editing. However, current researches lack investigation into
the imperceptibility of watermarking techniques in LLM services. This is
crucial as LLM providers may not want to disclose the presence of watermarks in
real-world scenarios, as it could reduce user willingness to use the service
and make watermarks more vulnerable to attacks. This work is the first to
investigate the imperceptibility of watermarked LLMs. We design an
identification algorithm called Water-Probe that detects watermarks through
well-designed prompts to the LLM. Our key motivation is that current
watermarked LLMs expose consistent biases under the same watermark key,
resulting in similar differences across prompts under different watermark keys.
Experiments show that almost all mainstream watermarking algorithms are easily
identified with our well-designed prompts, while Water-Probe demonstrates a
minimal false positive rate for non-watermarked LLMs. Finally, we propose that
the key to enhancing the imperceptibility of watermarked LLMs is to increase
the randomness of watermark key selection. Based on this, we introduce the
Water-Bag strategy, which significantly improves watermark imperceptibility by
merging multiple watermark keys.

摘要：大型語言模型 (LLM) 的文字浮水印在偵測 LLM 輸出和防止濫用方面取得顯著進展。目前的浮水印技術提供高偵測率、對文字品質的影響極小，且對於文字編輯具有穩健性。然而，目前的研究缺乏對 LLM 服務中浮水印技術的難以察覺性的調查。這一點至關重要，因為 LLM 供應商可能不願意在現實場景中公開浮水印的存在，因為這可能會降低使用者使用該服務的意願，並使浮水印更容易受到攻擊。這項研究首次探討了浮水印 LLM 的難以察覺性。我們設計了一種稱為 Water-Probe 的識別演算法，透過精心設計的提示來偵測 LLM 中的浮水印。我們的關鍵動機是，目前的浮水印 LLM 在相同的浮水印金鑰下會表現出一致的偏差，導致在不同的浮水印金鑰下的提示產生類似的差異。實驗顯示，幾乎所有主流的浮水印演算法都可以透過我們精心設計的提示輕鬆識別，而 Water-Probe 對於非浮水印 LLM 則表現出極低的誤報率。最後，我們提出增強浮水印 LLM 難以察覺性的關鍵在於增加浮水印金鑰選擇的隨機性。基於此，我們引入了 Water-Bag 策略，透過合併多個浮水印金鑰，大幅提升浮水印的難以察覺性。

##### **Adaptive Masking Enhances Visual Grounding**
2410.03161v1 by Sen Jia, Lei Li

In recent years, zero-shot and few-shot learning in visual grounding have
garnered considerable attention, largely due to the success of large-scale
vision-language pre-training on expansive datasets such as LAION-5B and
DataComp-1B. However, the continuous expansion of these datasets presents
significant challenges, particularly with respect to data availability and
computational overhead, thus creating a bottleneck in the advancement of
low-shot learning capabilities. In this paper, we propose IMAGE, Interpretative
MAsking with Gaussian radiation modEling, aimed at enhancing vocabulary
grounding in low-shot learning scenarios without necessitating an increase in
dataset size. Drawing inspiration from cognitive science and the recent success
of masked autoencoders (MAE), our method leverages adaptive masking on salient
regions of the feature maps generated by the vision backbone. This enables the
model to learn robust, generalized representations through the reconstruction
of occluded information, thereby facilitating effective attention to both local
and global features. We evaluate the efficacy of our approach on benchmark
datasets, including COCO and ODinW, demonstrating its superior performance in
zero-shot and few-shot tasks. Experimental results consistently show that IMAGE
outperforms baseline models, achieving enhanced generalization and improved
performance in low-shot scenarios. These findings highlight the potential of
adaptive feature manipulation through attention mechanisms and Gaussian
modeling as a promising alternative to approaches that rely on the continual
scaling of dataset sizes for the advancement of zero-shot and few-shot
learning. Our code is publicly available at https://github.com/git-lenny/IMAGE.

摘要：近年来，视觉基础中的零样本和少样本学习备受关注，这在很大程度上归功于在 LAION-5B 和 DataComp-1B 等扩展数据集上进行大规模视觉语言预训练的成功。然而，这些数据集的持续扩展带来了重大挑战，特别是在数据可用性和计算开销方面，从而在低样本学习能力的提升方面造成了瓶颈。在本文中，我们提出了 IMAGE，即使用高斯辐射建模的解释性掩蔽，旨在增强低样本学习场景中的词汇基础，而无需增加数据集大小。从认知科学和掩码自动编码器 (MAE) 的最新成功中汲取灵感，我们的方法利用了对视觉骨干网络生成的特征图的显着区域的自适应掩蔽。这使模型能够通过重建被遮挡的信息来学习鲁棒的、泛化的表示，从而促进对局部和全局特征的有效关注。我们在基准数据集（包括 COCO 和 ODinW）上评估了我们方法的有效性，展示了其在零样本和少样本任务中的卓越性能。实验结果一致表明，IMAGE 优于基线模型，在低样本场景中实现了增强的泛化能力和改进的性能。这些发现突出了自适应特征操作的潜力，通过注意机制和高斯建模作为一种有前途的替代方案，以取代依赖于数据集大小的持续扩展来推进零样本和少样本学习的方法。我们的代码已公开发布，网址为 https://github.com/git-lenny/IMAGE。

##### **Autoregressive Moving-average Attention Mechanism for Time Series Forecasting**
2410.03159v1 by Jiecheng Lu, Xu Han, Yan Sun, Shihao Yang

We propose an Autoregressive (AR) Moving-average (MA) attention structure
that can adapt to various linear attention mechanisms, enhancing their ability
to capture long-range and local temporal patterns in time series. In this
paper, we first demonstrate that, for the time series forecasting (TSF) task,
the previously overlooked decoder-only autoregressive Transformer model can
achieve results comparable to the best baselines when appropriate tokenization
and training methods are applied. Moreover, inspired by the ARMA model from
statistics and recent advances in linear attention, we introduce the full ARMA
structure into existing autoregressive attention mechanisms. By using an
indirect MA weight generation method, we incorporate the MA term while
maintaining the time complexity and parameter size of the underlying efficient
attention models. We further explore how indirect parameter generation can
produce implicit MA weights that align with the modeling requirements for local
temporal impacts. Experimental results show that incorporating the ARMA
structure consistently improves the performance of various AR attentions on TSF
tasks, achieving state-of-the-art results.

摘要：我們提出一個自迴歸 (AR) 移動平均 (MA) 注意力結構，它可以適應各種線性注意力機制，增強它們捕捉時間序列中長距離和局部時間模式的能力。在本文中，我們首先證明，對於時間序列預測 (TSF) 任務，以前被忽視的僅解碼器自迴歸 Transformer 模型可以在應用適當的標記化和訓練方法時，達到與最佳基線相當的結果。此外，受到統計學中的 ARMA 模型和線性注意力的最新進展的啟發，我們將完整的 ARMA 結構引入現有的自迴歸注意力機制。通過使用間接 MA 權重生成方法，我們在保持底層高效注意力模型的時間複雜度和參數大小的同時，納入了 MA 項。我們進一步探討了間接參數生成如何產生與局部時間影響的建模需求一致的隱式 MA 權重。實驗結果表明，納入 ARMA 結構持續改善各種 AR 注意力在 TSF 任務上的性能，取得了最先進的結果。

