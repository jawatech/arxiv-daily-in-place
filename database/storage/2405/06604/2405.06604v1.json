{"2405.06604": {"publish_time": "2024-05-10", "title": "Explaining Text Similarity in Transformer Models", "paper_summary": "As Transformers have become state-of-the-art models for natural language\nprocessing (NLP) tasks, the need to understand and explain their predictions is\nincreasingly apparent. Especially in unsupervised applications, such as\ninformation retrieval tasks, similarity models built on top of foundation model\nrepresentations have been widely applied. However, their inner prediction\nmechanisms have mostly remained opaque. Recent advances in explainable AI have\nmade it possible to mitigate these limitations by leveraging improved\nexplanations for Transformers through layer-wise relevance propagation (LRP).\nUsing BiLRP, an extension developed for computing second-order explanations in\nbilinear similarity models, we investigate which feature interactions drive\nsimilarity in NLP models. We validate the resulting explanations and\ndemonstrate their utility in three corpus-level use cases, analyzing\ngrammatical interactions, multilingual semantics, and biomedical text\nretrieval. Our findings contribute to a deeper understanding of different\nsemantic similarity tasks and models, highlighting how novel explainable AI\nmethods enable in-depth analyses and corpus-level insights.", "paper_summary_zh": "\u96a8\u8457 Transformer \u6210\u70ba\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u4efb\u52d9\u7684\u6700\u65b0\u6280\u8853\uff0c\u4e86\u89e3\u548c\u89e3\u91cb\u5176\u9810\u6e2c\u7684\u9700\u6c42\u65e5\u76ca\u660e\u986f\u3002\u7279\u5225\u662f\u5728\u7121\u76e3\u7763\u61c9\u7528\u7a0b\u5f0f\u4e2d\uff0c\u4f8b\u5982\u8cc7\u8a0a\u6aa2\u7d22\u4efb\u52d9\uff0c\u5efa\u7acb\u5728\u57fa\u790e\u6a21\u578b\u8868\u793a\u4e0a\u7684\u76f8\u4f3c\u6027\u6a21\u578b\u5df2\u88ab\u5ee3\u6cdb\u61c9\u7528\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u5167\u90e8\u9810\u6e2c\u6a5f\u5236\u5927\u591a\u4ecd\u7136\u4e0d\u900f\u660e\u3002\u53ef\u89e3\u91cb AI \u7684\u6700\u65b0\u9032\u5c55\u4f7f\u5f97\u900f\u904e\u5206\u5c64\u76f8\u95dc\u6027\u50b3\u64ad (LRP) \u70ba Transformer \u63d0\u4f9b\u66f4\u597d\u7684\u89e3\u91cb\uff0c\u5f9e\u800c\u6e1b\u8f15\u9019\u4e9b\u9650\u5236\u3002\u4f7f\u7528 BiLRP\uff0c\u4e00\u7a2e\u70ba\u8a08\u7b97\u96d9\u7dda\u6027\u76f8\u4f3c\u6027\u6a21\u578b\u4e2d\u7684\u4e8c\u968e\u89e3\u91cb\u800c\u958b\u767c\u7684\u5ef6\u4f38\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u54ea\u4e9b\u7279\u5fb5\u4e92\u52d5\u9a45\u52d5\u4e86 NLP \u6a21\u578b\u4e2d\u7684\u76f8\u4f3c\u6027\u3002\u6211\u5011\u9a57\u8b49\u4e86\u7522\u751f\u7684\u89e3\u91cb\uff0c\u4e26\u5c55\u793a\u4e86\u5b83\u5011\u5728\u4e09\u500b\u8a9e\u6599\u5eab\u7d1a\u5225\u7528\u4f8b\u4e2d\u7684\u6548\u7528\uff0c\u5206\u6790\u8a9e\u6cd5\u4e92\u52d5\u3001\u591a\u8a9e\u8a00\u8a9e\u7fa9\u548c\u751f\u7269\u91ab\u5b78\u6587\u5b57\u6aa2\u7d22\u3002\u6211\u5011\u7684\u767c\u73fe\u6709\u52a9\u65bc\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u4e0d\u540c\u7684\u8a9e\u7fa9\u76f8\u4f3c\u6027\u4efb\u52d9\u548c\u6a21\u578b\uff0c\u5f37\u8abf\u4e86\u65b0\u7a4e\u7684\u53ef\u89e3\u91cb AI \u65b9\u6cd5\u5982\u4f55\u5be6\u73fe\u6df1\u5165\u5206\u6790\u548c\u8a9e\u6599\u5eab\u7d1a\u5225\u898b\u89e3\u3002", "author": "Alexandros Vasileiou et.al.", "authors": "Alexandros Vasileiou, Oliver Eberle", "id": "2405.06604v1", "paper_url": "http://arxiv.org/abs/2405.06604v1", "repo": "null"}}