{"2410.23822": {"publish_time": "2024-10-31", "title": "Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding", "paper_summary": "Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7ee7\u627f\u4e86 LLM \u4f18\u8d8a\u7684\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5c06\u8fd9\u4e9b\u80fd\u529b\u6269\u5c55\u5230\u591a\u6a21\u6001\u573a\u666f\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u7684\u901a\u7528\u9886\u57df\u4e2d\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u5728\u533b\u5b66\u9886\u57df\uff0c\u5927\u91cf\u7684\u8bad\u7ec3\u6210\u672c\u548c\u5bf9\u5e7f\u6cdb\u533b\u5b66\u6570\u636e\u7684\u9700\u6c42\u5bf9\u533b\u5b66 MLLM \u7684\u53d1\u5c55\u6784\u6210\u4e86\u6311\u6218\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u7b54\u6848\u7684\u81ea\u7531\u6587\u672c\u5f62\u5f0f\uff0c\u9700\u8981\u4ee5\u89c4\u5b9a\u5f62\u5f0f\u751f\u6210\u8f93\u51fa\u7684\u4efb\u52a1\uff08\u4f8b\u5982\u89c6\u89c9\u57fa\u7840\uff09\u5bf9\u4e8e MLLM \u6765\u8bf4\u53d8\u5f97\u56f0\u96be\u3002\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u8fd8\u6ca1\u6709\u533b\u5b66 MLLM \u5728\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u9886\u57df\u5de5\u4f5c\u3002\u5bf9\u4e8e\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u4efb\u52a1\uff0c\u5b83\u6d89\u53ca\u6839\u636e\u7b80\u77ed\u7684\u6587\u672c\u63cf\u8ff0\u8bc6\u522b\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u4f4d\u7f6e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7528\u4e8e\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u533b\u5b66\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (PFMVG)\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6211\u4eec\u5728\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u7684\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u5176\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b83\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u660e\u663e\u4f18\u4e8e GPT-4v\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5c06\u5728\u540c\u884c\u8bc4\u5ba1\u540e\u5f00\u6e90\u3002", "author": "Jinlong He et.al.", "authors": "Jinlong He, Pengfei Li, Gang Liu, Shenjun Zhong", "id": "2410.23822v1", "paper_url": "http://arxiv.org/abs/2410.23822v1", "repo": "null"}}