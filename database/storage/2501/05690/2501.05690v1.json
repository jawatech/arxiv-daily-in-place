{"2501.05690": {"publish_time": "2025-01-10", "title": "Overcoming Language Priors for Visual Question Answering Based on Knowledge Distillation", "paper_summary": "Previous studies have pointed out that visual question answering (VQA) models\nare prone to relying on language priors for answer predictions. In this\ncontext, predictions often depend on linguistic shortcuts rather than a\ncomprehensive grasp of multimodal knowledge, which diminishes their\ngeneralization ability. In this paper, we propose a novel method, namely, KDAR,\nleveraging knowledge distillation to address the prior-dependency dilemmas\nwithin the VQA task. Specifically, the regularization effect facilitated by\nsoft labels from a well-trained teacher is employed to penalize overfitting to\nthe most common answers. The soft labels, which serve a regularization role,\nalso provide semantic guidance that narrows the range of candidate answers.\nAdditionally, we design an adaptive sample-wise reweighting learning strategy\nto further mitigate bias by dynamically adjusting the importance of each\nsample. Experimental results demonstrate that our method enhances performance\nin both OOD and IID settings. Our method achieves state-of-the-art performance\non the VQA-CPv2 out-of-distribution (OOD) benchmark, significantly\noutperforming previous state-of-the-art approaches.", "paper_summary_zh": "\u5148\u524d\u7684\u7814\u7a76\u6307\u51fa\uff0c\u8996\u89ba\u554f\u7b54 (VQA) \u6a21\u578b\u5bb9\u6613\u4f9d\u8cf4\u8a9e\u8a00\u5148\u9a57\u4f86\u9810\u6e2c\u7b54\u6848\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u9810\u6e2c\u901a\u5e38\u4f9d\u8cf4\u65bc\u8a9e\u8a00\u6377\u5f91\uff0c\u800c\u4e0d\u662f\u5c0d\u591a\u6a21\u614b\u77e5\u8b58\u7684\u5168\u9762\u638c\u63e1\uff0c\u9019\u964d\u4f4e\u4e86\u5b83\u5011\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u5373 KDAR\uff0c\u5229\u7528\u77e5\u8b58\u84b8\u993e\u4f86\u89e3\u6c7a VQA \u4efb\u52d9\u4e2d\u7684\u5148\u9a57\u4f9d\u8cf4\u56f0\u5883\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5229\u7528\u8a13\u7df4\u6709\u7d20\u7684\u6559\u5e2b\u63d0\u4f9b\u7684\u8edf\u6a19\u7c64\u6240\u4fc3\u9032\u7684\u6b63\u5247\u5316\u6548\u61c9\u4f86\u61f2\u7f70\u5c0d\u6700\u5e38\u898b\u7b54\u6848\u7684\u904e\u5ea6\u64ec\u5408\u3002\u5145\u7576\u6b63\u5247\u5316\u89d2\u8272\u7684\u8edf\u6a19\u7c64\u9084\u63d0\u4f9b\u4e86\u8a9e\u7fa9\u6307\u5c0e\uff0c\u7e2e\u5c0f\u4e86\u5019\u9078\u7b54\u6848\u7684\u7bc4\u570d\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u7a2e\u81ea\u9069\u61c9\u7684\u9010\u6a23\u672c\u91cd\u65b0\u52a0\u6b0a\u5b78\u7fd2\u7b56\u7565\uff0c\u901a\u904e\u52d5\u614b\u8abf\u6574\u6bcf\u500b\u6a23\u672c\u7684\u91cd\u8981\u6027\u4f86\u9032\u4e00\u6b65\u6e1b\u8f15\u504f\u5dee\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728 OOD \u548c IID \u8a2d\u7f6e\u4e2d\u90fd\u589e\u5f37\u4e86\u6027\u80fd\u3002\u6211\u5011\u7684\u6a21\u578b\u5728 VQA-CPv2 \u7570\u69cb\u5206\u4f48 (OOD) \u57fa\u6e96\u4e0a\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684\u6027\u80fd\uff0c\u986f\u8457\u512a\u65bc\u5148\u524d\u7684\u6700\u5148\u9032\u65b9\u6cd5\u3002", "author": "Daowan Peng et.al.", "authors": "Daowan Peng, Wei Wei", "id": "2501.05690v1", "paper_url": "http://arxiv.org/abs/2501.05690v1", "repo": "null"}}