{"2502.15654": {"publish_time": "2025-02-21", "title": "Machine-generated text detection prevents language model collapse", "paper_summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since web data is the\nprimary resource for LLM pretraining, future models will be trained on an\nunknown portion of synthetic data. This will lead to model collapse, a\ndegenerative process which causes models to reinforce their own errors and\nexperience a drop in model performance. In this study, we investigate the\nimpact of decoding strategy on model collapse, where we analyse the\ncharacteristics of the generated data during recursive training, its similarity\nto human references and the resulting model performance. Using the decoding\nstrategies that lead to the most significant model degradation, we tackle the\nquestion: how to avoid model collapse when the origin (human or synthetic) of\nthe training data is unknown. We design a novel methodology based on resampling\nthe data distribution using importance weights from our machine-generated text\ndetector. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on\nthe open-ended text generation task, demonstrating that we can successfully\nprevent model collapse and when there is enough human-authored data in the\ntraining dataset, our method improves model performance.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8b8a\u5f97\u8d8a\u4f86\u8d8a\u666e\u904d\uff0c\u5b83\u5011\u7522\u751f\u7684\u8f38\u51fa\u5728\u7db2\u8def\u4e2d\u5927\u91cf\u589e\u52a0\uff0c\u5192\u8457\u672a\u4f86\u6a5f\u5668\u7522\u751f\u7684\u5167\u5bb9\u7a00\u91cb\u4eba\u985e\u64b0\u5beb\u6587\u5b57\u7684\u98a8\u96aa\u3002\u7531\u65bc\u7db2\u8def\u8cc7\u6599\u662f LLM \u9810\u8a13\u7df4\u7684\u4e3b\u8981\u8cc7\u6e90\uff0c\u672a\u4f86\u7684\u6a21\u578b\u5c07\u5728\u672a\u77e5\u90e8\u5206\u7684\u5408\u6210\u8cc7\u6599\u4e0a\u9032\u884c\u8a13\u7df4\u3002\u9019\u5c07\u5c0e\u81f4\u6a21\u578b\u5d29\u6f70\uff0c\u4e00\u7a2e\u9000\u5316\u904e\u7a0b\uff0c\u5c0e\u81f4\u6a21\u578b\u5f37\u5316\u5b83\u5011\u81ea\u5df1\u7684\u932f\u8aa4\u4e26\u7d93\u6b77\u6a21\u578b\u6548\u80fd\u4e0b\u964d\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u89e3\u78bc\u7b56\u7565\u5c0d\u6a21\u578b\u5d29\u6f70\u7684\u5f71\u97ff\uff0c\u6211\u5011\u5728\u905e\u8ff4\u8a13\u7df4\u904e\u7a0b\u4e2d\u5206\u6790\u751f\u6210\u8cc7\u6599\u7684\u7279\u5fb5\u3001\u5b83\u8207\u4eba\u985e\u53c3\u8003\u7684\u76f8\u4f3c\u6027\u4ee5\u53ca\u7522\u751f\u7684\u6a21\u578b\u6548\u80fd\u3002\u4f7f\u7528\u5c0e\u81f4\u6700\u986f\u8457\u6a21\u578b\u9000\u5316\u7684\u89e3\u78bc\u7b56\u7565\uff0c\u6211\u5011\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff1a\u7576\u8a13\u7df4\u8cc7\u6599\u7684\u4f86\u6e90\uff08\u4eba\u985e\u6216\u5408\u6210\uff09\u672a\u77e5\u6642\uff0c\u5982\u4f55\u907f\u514d\u6a21\u578b\u5d29\u6f70\u3002\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\uff0c\u57fa\u65bc\u4f7f\u7528\u6211\u5011\u6a5f\u5668\u7522\u751f\u7684\u6587\u5b57\u5075\u6e2c\u5668\u7684\u91cd\u8981\u6027\u6b0a\u91cd\u5c0d\u8cc7\u6599\u5206\u4f48\u9032\u884c\u91cd\u65b0\u53d6\u6a23\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u5169\u500b LLM \u8b8a\u9ad4\uff08GPT-2 \u548c SmolLM2\uff09\u4e0a\u91dd\u5c0d\u958b\u653e\u5f0f\u6587\u5b57\u751f\u6210\u4efb\u52d9\u9032\u884c\u9a57\u8b49\uff0c\u8b49\u660e\u6211\u5011\u53ef\u4ee5\u6210\u529f\u9632\u6b62\u6a21\u578b\u5d29\u6f70\uff0c\u4e26\u4e14\u7576\u8a13\u7df4\u8cc7\u6599\u96c6\u4e2d\u6709\u8db3\u5920\u7684\u4eba\u985e\u64b0\u5beb\u8cc7\u6599\u6642\uff0c\u6211\u5011\u7684\u6a21\u578b\u53ef\u4ee5\u6539\u5584\u6a21\u578b\u6548\u80fd\u3002", "author": "George Drayson et.al.", "authors": "George Drayson, Vasileios Lampos", "id": "2502.15654v1", "paper_url": "http://arxiv.org/abs/2502.15654v1", "repo": "null"}}