{"2410.07069": {"publish_time": "2024-10-09", "title": "ReIFE: Re-evaluating Instruction-Following Evaluation", "paper_summary": "The automatic evaluation of instruction following typically involves using\nlarge language models (LLMs) to assess response quality. However, there is a\nlack of comprehensive evaluation of these LLM-based evaluators across two\ndimensions: the base LLMs and the evaluation protocols. Therefore, we present a\nthorough meta-evaluation of instruction following, including 25 base LLMs and\n15 recently proposed evaluation protocols, on 4 human-annotated datasets,\nassessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows\nus to identify the best-performing base LLMs and evaluation protocols with a\nhigh degree of robustness. Moreover, our large-scale evaluation reveals: (1)\nBase LLM performance ranking remains largely consistent across evaluation\nprotocols, with less capable LLMs showing greater improvement from protocol\nenhancements; (2) Robust evaluation of evaluation protocols requires many base\nLLMs with varying capability levels, as protocol effectiveness can depend on\nthe base LLM used; (3) Evaluation results on different datasets are not always\nconsistent, so a rigorous evaluation requires multiple datasets with\ndistinctive features. We release our meta-evaluation suite ReIFE, which\nprovides the codebase and evaluation result collection for more than 500\nLLM-evaluator configurations, to support future research in\ninstruction-following evaluation.", "paper_summary_zh": "\u81ea\u52d5\u8a55\u4f30\u6307\u4ee4\u9075\u5faa\u901a\u5e38\u6d89\u53ca\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u8a55\u4f30\u56de\u61c9\u54c1\u8cea\u3002\u7136\u800c\uff0c\u9019\u4e9b\u57fa\u65bc LLM \u7684\u8a55\u4f30\u5668\u7f3a\u4e4f\u8de8\u5169\u500b\u9762\u5411\u7684\u5168\u9762\u8a55\u4f30\uff1a\u57fa\u790e LLM \u548c\u8a55\u4f30\u5354\u5b9a\u3002\u56e0\u6b64\uff0c\u6211\u5011\u91dd\u5c0d\u6307\u4ee4\u9075\u5faa\u63d0\u51fa\u4e00\u500b\u5fb9\u5e95\u7684\u5143\u8a55\u4f30\uff0c\u5305\u62ec 25 \u500b\u57fa\u790e LLM \u548c 15 \u500b\u6700\u8fd1\u63d0\u51fa\u7684\u8a55\u4f30\u5354\u5b9a\uff0c\u5728 4 \u500b\u7531\u4eba\u5de5\u6a19\u8a3b\u7684\u8cc7\u6599\u96c6\u4e0a\uff0c\u8a55\u4f30 LLM \u8a55\u4f30\u5668\u7684\u8a55\u4f30\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u7684\u8a55\u4f30\u8b93\u6211\u5011\u5f97\u4ee5\u8b58\u5225\u8868\u73fe\u6700\u4f73\u7684\u57fa\u790e LLM \u548c\u8a55\u4f30\u5354\u5b9a\uff0c\u5177\u5099\u9ad8\u5ea6\u7684\u7a69\u5065\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u5927\u898f\u6a21\u8a55\u4f30\u63ed\u9732\uff1a(1) \u57fa\u790e LLM \u6548\u80fd\u6392\u540d\u5728\u8a55\u4f30\u5354\u5b9a\u4e4b\u9593\u5927\u81f4\u4fdd\u6301\u4e00\u81f4\uff0c\u80fd\u529b\u8f03\u5f31\u7684 LLM \u5f9e\u5354\u5b9a\u589e\u5f37\u4e2d\u5c55\u73fe\u51fa\u66f4\u5927\u7684\u9032\u6b65\uff1b(2) \u8a55\u4f30\u5354\u5b9a\u7684\u7a69\u5065\u8a55\u4f30\u9700\u8981\u8a31\u591a\u5177\u5099\u4e0d\u540c\u80fd\u529b\u7b49\u7d1a\u7684\u57fa\u790e LLM\uff0c\u56e0\u70ba\u5354\u5b9a\u6548\u80fd\u53ef\u80fd\u53d6\u6c7a\u65bc\u6240\u4f7f\u7528\u7684\u57fa\u790e LLM\uff1b(3) \u4e0d\u540c\u8cc7\u6599\u96c6\u4e0a\u7684\u8a55\u4f30\u7d50\u679c\u4e26\u4e0d\u7e3d\u662f\u4fdd\u6301\u4e00\u81f4\uff0c\u56e0\u6b64\u56b4\u8b39\u7684\u8a55\u4f30\u9700\u8981\u5177\u5099\u7368\u7279\u7279\u5fb5\u7684\u5404\u7a2e\u8cc7\u6599\u96c6\u3002\u6211\u5011\u767c\u5e03\u6211\u5011\u7684\u5143\u8a55\u4f30\u5957\u4ef6 ReIFE\uff0c\u63d0\u4f9b\u8d85\u904e 500 \u500b LLM \u8a55\u4f30\u5668\u7d44\u614b\u7684\u7a0b\u5f0f\u78bc\u5eab\u548c\u8a55\u4f30\u7d50\u679c\u6536\u96c6\uff0c\u4ee5\u652f\u6301\u6307\u4ee4\u9075\u5faa\u8a55\u4f30\u7684\u672a\u4f86\u7814\u7a76\u3002", "author": "Yixin Liu et.al.", "authors": "Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan", "id": "2410.07069v1", "paper_url": "http://arxiv.org/abs/2410.07069v1", "repo": "null"}}