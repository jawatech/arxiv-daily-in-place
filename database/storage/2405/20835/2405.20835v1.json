{"2405.20835": {"publish_time": "2024-05-31", "title": "Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs", "paper_summary": "Post-Training Quantization (PTQ) enhances the efficiency of Large Language\nModels (LLMs) by enabling faster operation and compatibility with more\naccessible hardware through reduced memory usage, at the cost of small\nperformance drops. We explore the role of calibration sets in PTQ, specifically\ntheir effect on hidden activations in various notable open-source LLMs.\nCalibration sets are crucial for evaluating activation magnitudes and\nidentifying outliers, which can distort the quantization range and negatively\nimpact performance. Our analysis reveals a marked contrast in quantization\neffectiveness across models. The older OPT model, which much of the\nquantization literature is based on, shows significant performance\ndeterioration and high susceptibility to outliers with varying calibration\nsets. In contrast, newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and\nMistral 7B demonstrate strong robustness, with Mistral 7B showing near-immunity\nto outliers and stable activations. These findings suggest a shift in PTQ\nstrategies might be needed. As advancements in pre-training methods reduce the\nrelevance of outliers, there is an emerging need to reassess the fundamentals\nof current quantization literature. The emphasis should pivot towards\noptimizing inference speed, rather than primarily focusing on outlier\npreservation, to align with the evolving characteristics of state-of-the-art\nLLMs.", "paper_summary_zh": "\u5f8c\u8a13\u7df4\u91cf\u5316 (PTQ) \u900f\u904e\u964d\u4f4e\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u4f86\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6548\u7387\uff0c\u9032\u800c\u5be6\u73fe\u66f4\u5feb\u901f\u7684\u904b\u4f5c\uff0c\u4e26\u8207\u66f4\u591a\u53ef\u5b58\u53d6\u7684\u786c\u9ad4\u76f8\u5bb9\uff0c\u4f46\u4ee3\u50f9\u662f\u6703\u7a0d\u5fae\u964d\u4f4e\u6548\u80fd\u3002\u6211\u5011\u63a2\u8a0e\u6821\u6b63\u96c6\u5728 PTQ \u4e2d\u7684\u89d2\u8272\uff0c\u7279\u5225\u662f\u5b83\u5011\u5c0d\u5404\u7a2e\u8457\u540d\u7684\u958b\u653e\u539f\u59cb\u78bc LLM \u4e2d\u7684\u96b1\u85cf\u6fc0\u6d3b\u7684\u5f71\u97ff\u3002\u6821\u6b63\u96c6\u5c0d\u65bc\u8a55\u4f30\u6fc0\u6d3b\u5e45\u5ea6\u548c\u627e\u51fa\u7570\u5e38\u503c\u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u7570\u5e38\u503c\u53ef\u80fd\u6703\u626d\u66f2\u91cf\u5316\u7bc4\u570d\u4e26\u5c0d\u6548\u80fd\u9020\u6210\u8ca0\u9762\u5f71\u97ff\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u91cf\u5316\u6709\u6548\u6027\u4e0a\u6709\u986f\u8457\u7684\u5dee\u7570\u3002\u8a31\u591a\u91cf\u5316\u6587\u737b\u6240\u4f9d\u64da\u7684\u8f03\u820a OPT \u6a21\u578b\uff0c\u986f\u793a\u51fa\u986f\u8457\u7684\u6548\u80fd\u60e1\u5316\uff0c\u4e14\u5c0d\u4e0d\u540c\u6821\u6b63\u96c6\u7684\u7570\u5e38\u503c\u9ad8\u5ea6\u654f\u611f\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u66f4\u65b0\u7684\u6a21\u578b\uff0c\u4f8b\u5982 Llama-2 7B\u3001Llama-3 8B\u3001Command-R 35B \u548c Mistral 7B\uff0c\u5247\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u7a69\u5065\u6027\uff0c\u5176\u4e2d Mistral 7B \u986f\u793a\u51fa\u5c0d\u7570\u5e38\u503c\u7684\u8fd1\u4e4e\u514d\u75ab\u6027\u548c\u7a69\u5b9a\u7684\u6fc0\u6d3b\u3002\u9019\u4e9b\u767c\u73fe\u8868\u660e\uff0cPTQ \u7b56\u7565\u53ef\u80fd\u9700\u8981\u8f49\u8b8a\u3002\u96a8\u8457\u9810\u8a13\u7df4\u65b9\u6cd5\u7684\u9032\u6b65\u964d\u4f4e\u7570\u5e38\u503c\u7684\u91cd\u8981\u6027\uff0c\u91cd\u65b0\u8a55\u4f30\u7576\u524d\u91cf\u5316\u6587\u737b\u7684\u57fa\u672c\u539f\u7406\u5df2\u6210\u70ba\u65b0\u8208\u9700\u6c42\u3002\u91cd\u9ede\u61c9\u8f49\u5411\u6700\u4f73\u5316\u63a8\u8ad6\u901f\u5ea6\uff0c\u800c\u4e0d\u662f\u4e3b\u8981\u95dc\u6ce8\u7570\u5e38\u503c\u4fdd\u7559\uff0c\u4ee5\u7b26\u5408\u6700\u5148\u9032 LLM \u7684\u6f14\u5316\u7279\u6027\u3002", "author": "Davide Paglieri et.al.", "authors": "Davide Paglieri, Saurabh Dash, Tim Rockt\u00e4schel, Jack Parker-Holder", "id": "2405.20835v1", "paper_url": "http://arxiv.org/abs/2405.20835v1", "repo": "null"}}