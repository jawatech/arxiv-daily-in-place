{"2405.09266": {"publish_time": "2024-05-15", "title": "Dance Any Beat: Blending Beats with Visuals in Dance Video Generation", "paper_summary": "The task of generating dance from music is crucial, yet current methods,\nwhich mainly produce joint sequences, lead to outputs that lack intuitiveness\nand complicate data collection due to the necessity for precise joint\nannotations. We introduce a Dance Any Beat Diffusion model, namely DabFusion,\nthat employs music as a conditional input to directly create dance videos from\nstill images, utilizing conditional image-to-video generation principles. This\napproach pioneers the use of music as a conditioning factor in image-to-video\nsynthesis. Our method unfolds in two stages: training an auto-encoder to\npredict latent optical flow between reference and driving frames, eliminating\nthe need for joint annotation, and training a U-Net-based diffusion model to\nproduce these latent optical flows guided by music rhythm encoded by CLAP.\nAlthough capable of producing high-quality dance videos, the baseline model\nstruggles with rhythm alignment. We enhance the model by adding beat\ninformation, improving synchronization. We introduce a 2D motion-music\nalignment score (2D-MM Align) for quantitative assessment. Evaluated on the\nAIST++ dataset, our enhanced model shows marked improvements in 2D-MM Align\nscore and established metrics. Video results can be found on our project page:\nhttps://DabFusion.github.io.", "paper_summary_zh": "\u5f9e\u97f3\u6a02\u4e2d\u7522\u751f\u821e\u8e48\u7684\u4efb\u52d9\u81f3\u95dc\u91cd\u8981\uff0c\u7136\u800c\uff0c\u76ee\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u662f\u7522\u751f\u95dc\u7bc0\u5e8f\u5217\uff0c\u5c0e\u81f4\u7522\u51fa\u7684\u7d50\u679c\u7f3a\u4e4f\u76f4\u89ba\u6027\uff0c\u4e26\u4e14\u7531\u65bc\u9700\u8981\u7cbe\u78ba\u7684\u95dc\u7bc0\u8a3b\u89e3\u800c\u4f7f\u8cc7\u6599\u6536\u96c6\u8b8a\u5f97\u8907\u96dc\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u821e\u8e48\u4efb\u610f\u7bc0\u62cd\u64f4\u6563\u6a21\u578b\uff0c\u5373 DabFusion\uff0c\u5b83\u4f7f\u7528\u97f3\u6a02\u4f5c\u70ba\u689d\u4ef6\u8f38\u5165\uff0c\u76f4\u63a5\u5f9e\u975c\u614b\u5f71\u50cf\u5efa\u7acb\u821e\u8e48\u5f71\u7247\uff0c\u5229\u7528\u689d\u4ef6\u5f71\u50cf\u5230\u5f71\u7247\u7684\u7522\u751f\u539f\u5247\u3002\u9019\u7a2e\u65b9\u6cd5\u958b\u5275\u4e86\u4f7f\u7528\u97f3\u6a02\u4f5c\u70ba\u5f71\u50cf\u5230\u5f71\u7247\u5408\u6210\u4e2d\u689d\u4ef6\u56e0\u5b50\u7684\u5148\u6cb3\u3002\u6211\u5011\u7684\u505a\u6cd5\u5206\u70ba\u5169\u500b\u968e\u6bb5\uff1a\u8a13\u7df4\u4e00\u500b\u81ea\u52d5\u7de8\u78bc\u5668\u4f86\u9810\u6e2c\u53c3\u8003\u548c\u9a45\u52d5\u5e40\u4e4b\u9593\u7684\u6f5b\u5728\u5149\u6d41\uff0c\u6d88\u9664\u5c0d\u95dc\u7bc0\u8a3b\u89e3\u7684\u9700\u6c42\uff0c\u4e26\u8a13\u7df4\u4e00\u500b\u57fa\u65bc U-Net \u7684\u64f4\u6563\u6a21\u578b\uff0c\u4ee5\u7522\u751f\u7531 CLAP \u7de8\u78bc\u7684\u97f3\u6a02\u7bc0\u594f\u5f15\u5c0e\u7684\u9019\u4e9b\u6f5b\u5728\u5149\u6d41\u3002\u5118\u7ba1\u80fd\u5920\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u821e\u8e48\u5f71\u7247\uff0c\u4f46\u57fa\u6e96\u6a21\u578b\u5728\u7bc0\u594f\u5c0d\u9f4a\u65b9\u9762\u4ecd\u6709\u56f0\u96e3\u3002\u6211\u5011\u900f\u904e\u52a0\u5165\u7bc0\u62cd\u8cc7\u8a0a\u4f86\u589e\u5f37\u6a21\u578b\uff0c\u9032\u800c\u6539\u5584\u540c\u6b65\u3002\u6211\u5011\u5f15\u5165\u4e00\u500b 2D \u904b\u52d5\u97f3\u6a02\u5c0d\u9f4a\u5206\u6578 (2D-MM Align) \u4f86\u9032\u884c\u91cf\u5316\u8a55\u4f30\u3002\u5728 AIST++ \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a55\u4f30\uff0c\u6211\u5011\u589e\u5f37\u7684\u6a21\u578b\u5728 2D-MM Align \u5206\u6578\u548c\u5df2\u5efa\u7acb\u7684\u6307\u6a19\u4e0a\u986f\u793a\u51fa\u986f\u8457\u7684\u6539\u9032\u3002\u5f71\u7247\u7d50\u679c\u53ef\u4ee5\u5728\u6211\u5011\u7684\u5c08\u6848\u9801\u9762\u627e\u5230\uff1ahttps://DabFusion.github.io\u3002", "author": "Xuanchen Wang et.al.", "authors": "Xuanchen Wang, Heng Wang, Dongnan Liu, Weidong Cai", "id": "2405.09266v1", "paper_url": "http://arxiv.org/abs/2405.09266v1", "repo": "null"}}