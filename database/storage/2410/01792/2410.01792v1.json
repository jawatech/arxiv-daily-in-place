{"2410.01792": {"publish_time": "2024-10-02", "title": "When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1", "paper_summary": "In \"Embers of Autoregression\" (McCoy et al., 2023), we showed that several\nlarge language models (LLMs) have some important limitations that are\nattributable to their origins in next-word prediction. Here we investigate\nwhether these issues persist with o1, a new system from OpenAI that differs\nfrom previous LLMs in that it is optimized for reasoning. We find that o1\nsubstantially outperforms previous LLMs in many cases, with particularly large\nimprovements on rare variants of common tasks (e.g., forming acronyms from the\nsecond letter of each word in a list, rather than the first letter). Despite\nthese quantitative improvements, however, o1 still displays the same\nqualitative trends that we observed in previous systems. Specifically, o1 -\nlike previous LLMs - is sensitive to the probability of examples and tasks,\nperforming better and requiring fewer \"thinking tokens\" in high-probability\nsettings than in low-probability ones. These results show that optimizing a\nlanguage model for reasoning can mitigate but might not fully overcome the\nlanguage model's probability sensitivity.", "paper_summary_zh": "\u5728\u300c\u81ea\u8ff4\u6b78\u7684\u9918\u71fc\u300d\uff08McCoy \u7b49\u4eba\uff0c2023 \u5e74\uff09\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5e7e\u500b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u6709\u4e00\u4e9b\u91cd\u8981\u7684\u9650\u5236\uff0c\u9019\u4e9b\u9650\u5236\u53ef\u6b78\u56e0\u65bc\u5b83\u5011\u6e90\u81ea\u4e0b\u4e00\u500b\u55ae\u5b57\u9810\u6e2c\u3002\u5728\u6b64\uff0c\u6211\u5011\u63a2\u8a0e\u9019\u4e9b\u554f\u984c\u662f\u5426\u6301\u7e8c\u5b58\u5728\u65bc o1\uff0c\u9019\u662f OpenAI \u7684\u4e00\u500b\u65b0\u7cfb\u7d71\uff0c\u5b83\u8207\u5148\u524d\u7684 LLM \u4e0d\u540c\uff0c\u56e0\u70ba\u5b83\u91dd\u5c0d\u63a8\u7406\u9032\u884c\u4e86\u6700\u4f73\u5316\u3002\u6211\u5011\u767c\u73fe\uff0c\u5728\u8a31\u591a\u60c5\u6cc1\u4e0b\uff0co1 \u90fd\u5927\u5e45\u512a\u65bc\u5148\u524d\u7684 LLM\uff0c\u7279\u5225\u662f\u5728\u5e38\u898b\u4efb\u52d9\u7684\u7f55\u898b\u8b8a\u9ad4\u4e0a\u6709\u4e86\u5927\u5e45\u9032\u6b65\uff08\u4f8b\u5982\uff0c\u6839\u64da\u6e05\u55ae\u4e2d\u6bcf\u500b\u55ae\u5b57\u7684\u7b2c\u4e8c\u500b\u5b57\u6bcd\u7d44\u6210\u7e2e\u5beb\uff0c\u800c\u4e0d\u662f\u7b2c\u4e00\u500b\u5b57\u6bcd\uff09\u3002\u7136\u800c\uff0c\u5118\u7ba1\u6709\u9019\u4e9b\u5b9a\u91cf\u7684\u6539\u9032\uff0co1 \u4ecd\u7136\u986f\u793a\u51fa\u6211\u5011\u5728\u5148\u524d\u7684\u7cfb\u7d71\u4e2d\u89c0\u5bdf\u5230\u7684\u76f8\u540c\u5b9a\u6027\u8da8\u52e2\u3002\u5177\u9ad4\u4f86\u8aaa\uff0co1 - \u8207\u5148\u524d\u7684 LLM \u4e00\u6a23 - \u5c0d\u7bc4\u4f8b\u548c\u4efb\u52d9\u7684\u6a5f\u7387\u5f88\u654f\u611f\uff0c\u5728\u9ad8\u6a5f\u7387\u8a2d\u5b9a\u4e2d\u6bd4\u5728\u4f4e\u6a5f\u7387\u8a2d\u5b9a\u4e2d\u8868\u73fe\u5f97\u66f4\u597d\uff0c\u800c\u4e14\u9700\u8981\u7684\u300c\u601d\u8003\u7b26\u865f\u300d\u66f4\u5c11\u3002\u9019\u4e9b\u7d50\u679c\u986f\u793a\uff0c\u91dd\u5c0d\u63a8\u7406\u6700\u4f73\u5316\u8a9e\u8a00\u6a21\u578b\u53ef\u4ee5\u6e1b\u8f15\u8a9e\u8a00\u6a21\u578b\u7684\u6a5f\u7387\u654f\u611f\u6027\uff0c\u4f46\u53ef\u80fd\u7121\u6cd5\u5b8c\u5168\u514b\u670d\u5b83\u3002", "author": "R. Thomas McCoy et.al.", "authors": "R. Thomas McCoy, Shunyu Yao, Dan Friedman, Mathew D. Hardy, Thomas L. Griffiths", "id": "2410.01792v1", "paper_url": "http://arxiv.org/abs/2410.01792v1", "repo": "null"}}