{"2407.16142": {"publish_time": "2024-07-23", "title": "Diffusion Models as Optimizers for Efficient Planning in Offline RL", "paper_summary": "Diffusion models have shown strong competitiveness in offline reinforcement\nlearning tasks by formulating decision-making as sequential generation.\nHowever, the practicality of these methods is limited due to the lengthy\ninference processes they require. In this paper, we address this problem by\ndecomposing the sampling process of diffusion models into two decoupled\nsubprocesses: 1) generating a feasible trajectory, which is a time-consuming\nprocess, and 2) optimizing the trajectory. With this decomposition approach, we\nare able to partially separate efficiency and quality factors, enabling us to\nsimultaneously gain efficiency advantages and ensure quality assurance. We\npropose the Trajectory Diffuser, which utilizes a faster autoregressive model\nto handle the generation of feasible trajectories while retaining the\ntrajectory optimization process of diffusion models. This allows us to achieve\nmore efficient planning without sacrificing capability. To evaluate the\neffectiveness and efficiency of the Trajectory Diffuser, we conduct experiments\non the D4RL benchmarks. The results demonstrate that our method achieves $\\it\n3$-$\\it 10 \\times$ faster inference speed compared to previous sequence\nmodeling methods, while also outperforming them in terms of overall\nperformance. https://github.com/RenMing-Huang/TrajectoryDiffuser\n  Keywords: Reinforcement Learning and Efficient Planning and Diffusion Model", "paper_summary_zh": "\u64f4\u6563\u6a21\u578b\u900f\u904e\u5c07\u6c7a\u7b56\u5236\u5b9a\u8868\u8ff0\u70ba\u9806\u5e8f\u751f\u6210\uff0c\u5728\u96e2\u7dda\u5f37\u5316\u5b78\u7fd2\u4efb\u52d9\u4e2d\u5c55\u73fe\u5f37\u52c1\u7684\u7af6\u722d\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u7684\u5be6\u7528\u6027\u53d7\u5230\u5176\u6240\u9700\u7684\u5197\u9577\u63a8\u8ad6\u904e\u7a0b\u6240\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5c07\u64f4\u6563\u6a21\u578b\u7684\u63a1\u6a23\u904e\u7a0b\u5206\u89e3\u70ba\u5169\u500b\u89e3\u8026\u7684\u5b50\u7a0b\u5e8f\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\uff1a1) \u751f\u6210\u53ef\u884c\u7684\u8ecc\u8de1\uff0c\u9019\u662f\u4e00\u500b\u8017\u6642\u8cbb\u529b\u7684\u904e\u7a0b\uff0c\u4ee5\u53ca 2) \u6700\u4f73\u5316\u8ecc\u8de1\u3002\u900f\u904e\u9019\u7a2e\u5206\u89e3\u65b9\u6cd5\uff0c\u6211\u5011\u80fd\u5920\u90e8\u5206\u5730\u5c07\u6548\u7387\u548c\u54c1\u8cea\u56e0\u7d20\u5206\u958b\uff0c\u8b93\u6211\u5011\u80fd\u5920\u540c\u6642\u7372\u5f97\u6548\u7387\u512a\u52e2\u4e26\u78ba\u4fdd\u54c1\u8cea\u4fdd\u8b49\u3002\u6211\u5011\u63d0\u51fa\u8ecc\u8de1\u64f4\u6563\u5668\uff0c\u5b83\u5229\u7528\u66f4\u5feb\u901f\u7684\u81ea\u52d5\u8ff4\u6b78\u6a21\u578b\u4f86\u8655\u7406\u53ef\u884c\u8ecc\u8de1\u7684\u751f\u6210\uff0c\u540c\u6642\u4fdd\u7559\u64f4\u6563\u6a21\u578b\u7684\u8ecc\u8de1\u6700\u4f73\u5316\u904e\u7a0b\u3002\u9019\u8b93\u6211\u5011\u80fd\u5920\u5728\u4e0d\u72a7\u7272\u80fd\u529b\u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u66f4\u6709\u6548\u7684\u898f\u5283\u3002\u70ba\u4e86\u8a55\u4f30\u8ecc\u8de1\u64f4\u6563\u5668\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u6211\u5011\u5728 D4RL \u57fa\u6e96\u4e0a\u9032\u884c\u5be6\u9a57\u3002\u7d50\u679c\u8868\u660e\uff0c\u8207\u5148\u524d\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5be6\u73fe\u4e86\u5feb $\\it 3$-$\\it 10 \\times$ \u500d\u7684\u63a8\u8ad6\u901f\u5ea6\uff0c\u540c\u6642\u5728\u6574\u9ad4\u6548\u80fd\u65b9\u9762\u4e5f\u512a\u65bc\u5b83\u5011\u3002https://github.com/RenMing-Huang/TrajectoryDiffuser\n\u95dc\u9375\u5b57\uff1a\u5f37\u5316\u5b78\u7fd2\u8207\u9ad8\u6548\u898f\u5283\u8207\u64f4\u6563\u6a21\u578b", "author": "Renming Huang et.al.", "authors": "Renming Huang, Yunqiang Pei, Guoqing Wang, Yangming Zhang, Yang Yang, Peng Wang, Hengtao Shen", "id": "2407.16142v1", "paper_url": "http://arxiv.org/abs/2407.16142v1", "repo": "https://github.com/renming-huang/trajectorydiffuser"}}