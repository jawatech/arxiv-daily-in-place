{"2409.11673": {"publish_time": "2024-09-18", "title": "RUIE: Retrieval-based Unified Information Extraction using Large Language Model", "paper_summary": "Unified information extraction (UIE) aims to complete all information\nextraction tasks using a single model or framework. While previous work has\nprimarily focused on instruction-tuning large language models (LLMs) with\nconstructed datasets, these methods require significant computational resources\nand struggle to generalize to unseen tasks. To address these limitations, we\npropose RUIE (Retrieval-based Unified Information Extraction), a framework that\nleverages in-context learning to enable rapid generalization while reducing\ncomputational costs. The key challenge in RUIE is selecting the most beneficial\ndemonstrations for LLMs to effectively handle diverse IE tasks. To achieve\nthis, we integrate LLM preferences for ranking candidate demonstrations and\ndesign a keyword-enhanced reward model to capture fine-grained relationships\nbetween queries and demonstrations. We then train a bi-encoder retriever for\nUIE through contrastive learning and knowledge distillation. To the best of our\nknowledge, RUIE is the first trainable retrieval framework for UIE.\nExperimental results on 8 held-out datasets demonstrate RUIE's effectiveness in\ngeneralizing to unseen tasks, with average F1-score improvements of 19.22 and\n3.13 compared to instruction-tuning methods and other retrievers, respectively.\nFurther analysis confirms RUIE's adaptability to LLMs of varying sizes and the\nimportance of its key components.", "paper_summary_zh": "\u7d71\u4e00\u8cc7\u8a0a\u64f7\u53d6 (UIE) \u65e8\u5728\u4f7f\u7528\u55ae\u4e00\u6a21\u578b\u6216\u67b6\u69cb\u5b8c\u6210\u6240\u6709\u8cc7\u8a0a\u64f7\u53d6\u4efb\u52d9\u3002\u96d6\u7136\u5148\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u4f7f\u7528\u5efa\u69cb\u8cc7\u6599\u96c6\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9032\u884c\u6307\u4ee4\u5fae\u8abf\uff0c\u4f46\u9019\u4e9b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u4e14\u96e3\u4ee5\u63a8\u5ee3\u5230\u672a\u898b\u4efb\u52d9\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86 RUIE (\u57fa\u65bc\u6aa2\u7d22\u7684\u7d71\u4e00\u8cc7\u8a0a\u64f7\u53d6)\uff0c\u9019\u662f\u4e00\u500b\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u60c5\u5883\u5b78\u7fd2\u4f86\u5be6\u73fe\u5feb\u901f\u63a8\u5ee3\uff0c\u540c\u6642\u964d\u4f4e\u904b\u7b97\u6210\u672c\u3002RUIE \u4e2d\u7684\u4e3b\u8981\u6311\u6230\u662f\u9078\u64c7\u5c0d LLM \u6700\u6709\u76ca\u7684\u793a\u7bc4\uff0c\u4ee5\u6709\u6548\u8655\u7406\u4e0d\u540c\u7684 IE \u4efb\u52d9\u3002\u70ba\u4e86\u9054\u6210\u6b64\u76ee\u7684\uff0c\u6211\u5011\u6574\u5408\u4e86 LLM \u504f\u597d\uff0c\u7528\u65bc\u5c0d\u5019\u9078\u793a\u7bc4\u9032\u884c\u6392\u540d\uff0c\u4e26\u8a2d\u8a08\u4e86\u4e00\u500b\u95dc\u9375\u5b57\u589e\u5f37\u734e\u52f5\u6a21\u578b\uff0c\u4ee5\u6355\u6349\u67e5\u8a62\u548c\u793a\u7bc4\u4e4b\u9593\u7684\u7d30\u5fae\u95dc\u4fc2\u3002\u7136\u5f8c\uff0c\u6211\u5011\u900f\u904e\u5c0d\u6bd4\u5b78\u7fd2\u548c\u77e5\u8b58\u8403\u53d6\u8a13\u7df4\u4e00\u500b UIE \u7684\u96d9\u7de8\u78bc\u6aa2\u7d22\u5668\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cRUIE \u662f\u7b2c\u4e00\u500b\u53ef\u8a13\u7df4\u7684 UIE \u6aa2\u7d22\u67b6\u69cb\u30028 \u500b\u7559\u5b58\u8cc7\u6599\u96c6\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 RUIE \u5728\u63a8\u5ee3\u5230\u672a\u898b\u4efb\u52d9\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u8207\u6307\u4ee4\u5fae\u8abf\u65b9\u6cd5\u548c\u5176\u4ed6\u6aa2\u7d22\u5668\u76f8\u6bd4\uff0c\u5e73\u5747 F1 \u5206\u6578\u5206\u5225\u63d0\u9ad8\u4e86 19.22 \u548c 3.13\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u8b49\u5be6\u4e86 RUIE \u5c0d\u4e0d\u540c\u5927\u5c0f LLM \u7684\u9069\u61c9\u6027\uff0c\u4ee5\u53ca\u5176\u95dc\u9375\u7d44\u6210\u7684\u91cd\u8981\u6027\u3002", "author": "Xincheng Liao et.al.", "authors": "Xincheng Liao, Junwen Duan, Yixi Huang, Jianxin Wang", "id": "2409.11673v1", "paper_url": "http://arxiv.org/abs/2409.11673v1", "repo": "null"}}