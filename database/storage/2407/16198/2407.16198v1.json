{"2407.16198": {"publish_time": "2024-07-23", "title": "INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model", "paper_summary": "With advancements in data availability and computing resources, Multimodal\nLarge Language Models (MLLMs) have showcased capabilities across various\nfields. However, the quadratic complexity of the vision encoder in MLLMs\nconstrains the resolution of input images. Most current approaches mitigate\nthis issue by cropping high-resolution images into smaller sub-images, which\nare then processed independently by the vision encoder. Despite capturing\nsufficient local details, these sub-images lack global context and fail to\ninteract with one another. To address this limitation, we propose a novel MLLM,\nINF-LLaVA, designed for effective high-resolution image perception. INF-LLaVA\nincorporates two innovative components. First, we introduce a Dual-perspective\nCropping Module (DCM), which ensures that each sub-image contains continuous\ndetails from a local perspective and comprehensive information from a global\nperspective. Second, we introduce Dual-perspective Enhancement Module (DEM) to\nenable the mutual enhancement of global and local features, allowing INF-LLaVA\nto effectively process high-resolution images by simultaneously capturing\ndetailed local information and comprehensive global context. Extensive ablation\nstudies validate the effectiveness of these components, and experiments on a\ndiverse set of benchmarks demonstrate that INF-LLaVA outperforms existing\nMLLMs. Code and pretrained model are available at\nhttps://github.com/WeihuangLin/INF-LLaVA.", "paper_summary_zh": "\u96a8\u8457\u6578\u64da\u53ef\u7528\u6027\u548c\u904b\u7b97\u8cc7\u6e90\u7684\u9032\u6b65\uff0c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5df2\u5728\u5404\u500b\u9818\u57df\u5c55\u793a\u5176\u80fd\u529b\u3002\u7136\u800c\uff0cMLLM \u4e2d\u8996\u89ba\u7de8\u78bc\u5668\u7684\u4e8c\u6b21\u8907\u96dc\u5ea6\u9650\u5236\u4e86\u8f38\u5165\u5f71\u50cf\u89e3\u6790\u5ea6\u3002\u76ee\u524d\u5927\u591a\u6578\u65b9\u6cd5\u900f\u904e\u5c07\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u88c1\u5207\u6210\u8f03\u5c0f\u7684\u5b50\u5f71\u50cf\u4f86\u7de9\u89e3\u6b64\u554f\u984c\uff0c\u7136\u5f8c\u7531\u8996\u89ba\u7de8\u78bc\u5668\u7368\u7acb\u8655\u7406\u3002\u5118\u7ba1\u6355\u6349\u5230\u8db3\u5920\u7684\u5c40\u90e8\u7d30\u7bc0\uff0c\u4f46\u9019\u4e9b\u5b50\u5f71\u50cf\u7f3a\u4e4f\u5168\u5c40\u8108\u7d61\uff0c\u4e14\u7121\u6cd5\u5f7c\u6b64\u4e92\u52d5\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684 MLLM\uff0cINF-LLaVA\uff0c\u65e8\u5728\u6709\u6548\u611f\u77e5\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u3002INF-LLaVA \u7d50\u5408\u4e86\u5169\u500b\u5275\u65b0\u7684\u7d44\u6210\u90e8\u5206\u3002\u9996\u5148\uff0c\u6211\u5011\u5f15\u5165\u4e00\u500b\u96d9\u8996\u89d2\u88c1\u5207\u6a21\u7d44 (DCM)\uff0c\u78ba\u4fdd\u6bcf\u500b\u5b50\u5f71\u50cf\u5f9e\u5c40\u90e8\u8996\u89d2\u5305\u542b\u9023\u7e8c\u7684\u7d30\u7bc0\uff0c\u4e26\u5f9e\u5168\u5c40\u8996\u89d2\u5305\u542b\u5168\u9762\u7684\u8cc7\u8a0a\u3002\u5176\u6b21\uff0c\u6211\u5011\u5f15\u5165\u96d9\u8996\u89d2\u589e\u5f37\u6a21\u7d44 (DEM)\uff0c\u4ee5\u5be6\u73fe\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5fb5\u7684\u76f8\u4e92\u589e\u5f37\uff0c\u8b93 INF-LLaVA \u80fd\u5920\u540c\u6642\u6355\u6349\u8a73\u7d30\u7684\u5c40\u90e8\u8cc7\u8a0a\u548c\u5168\u9762\u7684\u5168\u5c40\u8108\u7d61\uff0c\u6709\u6548\u8655\u7406\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u3002\u5ee3\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u9a57\u8b49\u4e86\u9019\u4e9b\u7d44\u6210\u90e8\u5206\u7684\u6709\u6548\u6027\uff0c\u4e14\u5728\u5404\u7a2e\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u5be6\u9a57\u8b49\u660e INF-LLaVA \u512a\u65bc\u73fe\u6709\u7684 MLLM\u3002\u7a0b\u5f0f\u78bc\u548c\u9810\u8a13\u7df4\u6a21\u578b\u53ef\u5728 https://github.com/WeihuangLin/INF-LLaVA \u53d6\u5f97\u3002", "author": "Yiwei Ma et.al.", "authors": "Yiwei Ma, Zhibin Wang, Xiaoshuai Sun, Weihuang Lin, Qiang Zhou, Jiayi Ji, Rongrong Ji", "id": "2407.16198v1", "paper_url": "http://arxiv.org/abs/2407.16198v1", "repo": "https://github.com/weihuanglin/inf-llava"}}