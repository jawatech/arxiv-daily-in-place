
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-13**|**SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**|Paloma Rabaey et.al.|[2409.08936v1](http://arxiv.org/abs/2409.08936v1)|[link](https://github.com/prabaey/synsum)|
|**2024-09-13**|**A BERT-Based Summarization approach for depression detection**|Hossein Salahshoor Gavalan et.al.|[2409.08483v1](http://arxiv.org/abs/2409.08483v1)|null|
|**2024-09-12**|**Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**|Fuchen Zheng et.al.|[2409.07793v1](http://arxiv.org/abs/2409.07793v1)|[link](https://github.com/lzeeorno/lagrange-duality-and-cmaformer)|
|**2024-09-12**|**ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**|Fuchen Zheng et.al.|[2409.07779v1](http://arxiv.org/abs/2409.07779v1)|[link](https://github.com/lzeeorno/assnet)|
|**2024-09-11**|**SoK: Security and Privacy Risks of Medical AI**|Yuanhaur Chang et.al.|[2409.07415v1](http://arxiv.org/abs/2409.07415v1)|null|
|**2024-09-11**|**Federated Impression for Learning with Distributed Heterogeneous Data**|Sana Ayromlou et.al.|[2409.07351v1](http://arxiv.org/abs/2409.07351v1)|null|
|**2024-09-11**|**MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**|Praveen K Kanithi et.al.|[2409.07314v1](http://arxiv.org/abs/2409.07314v1)|null|
|**2024-09-11**|**Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**|Sheng Chen et.al.|[2409.07186v1](http://arxiv.org/abs/2409.07186v1)|null|
|**2024-09-11**|**CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**|Feiyang Jia et.al.|[2409.07092v1](http://arxiv.org/abs/2409.07092v1)|null|
|**2024-09-11**|**Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**|Daeun Kyung et.al.|[2409.07012v1](http://arxiv.org/abs/2409.07012v1)|null|
|**2024-09-11**|**Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**|Jianmei Jiang et.al.|[2409.06928v1](http://arxiv.org/abs/2409.06928v1)|[link](https://github.com/jjm1589/dstct)|
|**2024-09-10**|**Bifurcation Identification for Ultrasound-driven Robotic Cannulation**|Cecilia G. Morales et.al.|[2409.06817v1](http://arxiv.org/abs/2409.06817v1)|null|
|**2024-09-10**|**Personalized Federated Learning Techniques: Empirical Analysis**|Azal Ahmad Khan et.al.|[2409.06805v1](http://arxiv.org/abs/2409.06805v1)|null|
|**2024-09-10**|**Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**|Cristian Trout et.al.|[2409.06672v1](http://arxiv.org/abs/2409.06672v1)|null|
|**2024-09-10**|**EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**|Danli Shi et.al.|[2409.06644v2](http://arxiv.org/abs/2409.06644v2)|null|
|**2024-09-10**|**Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**|Zoe Hancox et.al.|[2409.06585v1](http://arxiv.org/abs/2409.06585v1)|[link](https://github.com/zoehancox/sparse_tgcnn)|
|**2024-09-10**|**MAGDA: Multi-agent guideline-driven diagnostic assistance**|David Bani-Harouni et.al.|[2409.06351v1](http://arxiv.org/abs/2409.06351v1)|null|
|**2024-09-10**|**Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**|Xin Zhang et.al.|[2409.06209v1](http://arxiv.org/abs/2409.06209v1)|[link](https://github.com/xinz0419/unisurv)|
|**2024-09-10**|**Can Large Language Models Unlock Novel Scientific Research Ideas?**|Sandeep Kumar et.al.|[2409.06185v1](http://arxiv.org/abs/2409.06185v1)|null|
|**2024-09-10**|**Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**|Georgios Chochlakis et.al.|[2409.06173v1](http://arxiv.org/abs/2409.06173v1)|[link](https://github.com/gchochla/cot-priors)|
|**2024-09-10**|**Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**|Dong Han et.al.|[2409.06147v1](http://arxiv.org/abs/2409.06147v1)|null|
|**2024-09-09**|**ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**|Zhaoyue Sun et.al.|[2409.05592v1](http://arxiv.org/abs/2409.05592v1)|null|
|**2024-09-09**|**Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**|Camilo Thorne et.al.|[2409.05486v1](http://arxiv.org/abs/2409.05486v1)|null|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-09**|**Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**|Javad Hassannataj Joloudari et.al.|[2409.07493v1](http://arxiv.org/abs/2409.07493v1)|null|
|**2024-09-09**|**Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**|Nirmalya Thakur et.al.|[2409.05292v2](http://arxiv.org/abs/2409.05292v2)|null|
|**2024-09-09**|**RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**|Quoc-Bao Nguyen-Le et.al.|[2409.05280v1](http://arxiv.org/abs/2409.05280v1)|[link](https://github.com/kyle-paul/RotCAtt-TransUNet-plusplus)|
|**2024-09-07**|**Activation Function Optimization Scheme for Image Classification**|Abdur Rahman et.al.|[2409.04915v1](http://arxiv.org/abs/2409.04915v1)|[link](https://github.com/abdurrahman1828/afos)|
|**2024-09-07**|**LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**|Yongxin Deng et.al.|[2409.04744v1](http://arxiv.org/abs/2409.04744v1)|null|
|**2024-09-07**|**NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**|Debaditya Shome et.al.|[2409.04723v1](http://arxiv.org/abs/2409.04723v1)|null|
|**2024-09-07**|**A Comprehensive Survey on Evidential Deep Learning and Its Applications**|Junyu Gao et.al.|[2409.04720v1](http://arxiv.org/abs/2409.04720v1)|[link](https://github.com/mengyuanchen21/awesome-evidential-deep-learning)|
|**2024-09-07**|**A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**|Cheng Wan et.al.|[2409.04704v1](http://arxiv.org/abs/2409.04704v1)|null|
|**2024-09-06**|**The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**|Gregory Szumel et.al.|[2409.04368v1](http://arxiv.org/abs/2409.04368v1)|null|
|**2024-09-06**|**CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**|William Knottenbelt et.al.|[2409.04290v1](http://arxiv.org/abs/2409.04290v1)|[link](https://github.com/knottwill/CoxKAN)|
|**2024-09-06**|**Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**|Daniel J. Tan et.al.|[2409.04224v1](http://arxiv.org/abs/2409.04224v1)|null|
|**2024-09-06**|**Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**|Yizhen Zheng et.al.|[2409.04481v1](http://arxiv.org/abs/2409.04481v1)|null|
|**2024-09-06**|**FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**|Kai Shu et.al.|[2409.03947v1](http://arxiv.org/abs/2409.03947v1)|null|
|**2024-09-05**|**A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**|Esther Lagemann et.al.|[2409.03933v1](http://arxiv.org/abs/2409.03933v1)|null|
|**2024-09-05**|**Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**|Yucong Zhang et.al.|[2409.03597v1](http://arxiv.org/abs/2409.03597v1)|null|
|**2024-09-05**|**Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**|Prerak Mody et.al.|[2409.03470v1](http://arxiv.org/abs/2409.03470v1)|[link](https://github.com/prerakmody/bayesuncertainty-error-correspondence)|
|**2024-09-05**|**Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**|Francisco de Arriba-PÃ©rez et.al.|[2409.03375v1](http://arxiv.org/abs/2409.03375v1)|null|
|**2024-09-05**|**Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**|Juan A. Berrios Moya et.al.|[2409.03147v1](http://arxiv.org/abs/2409.03147v1)|null|
|**2024-09-04**|**MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**|Shehan Perera et.al.|[2409.03062v1](http://arxiv.org/abs/2409.03062v1)|[link](https://github.com/osupcvlab/mobileunetr)|
|**2024-09-04**|**Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**|Junyoung Park et.al.|[2409.02883v1](http://arxiv.org/abs/2409.02883v1)|null|
|**2024-09-04**|**Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**|Ramon Tavares et.al.|[2409.02681v1](http://arxiv.org/abs/2409.02681v1)|null|
|**2024-09-04**|**SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**|Wenwu Guo et.al.|[2409.02598v1](http://arxiv.org/abs/2409.02598v1)|[link](https://github.com/wenwucode/surgtrack)|
|**2024-09-04**|**Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**|Chih-Yuan Li et.al.|[2409.02530v1](http://arxiv.org/abs/2409.02530v1)|null|
|**2024-09-03**|**Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**|Deepak Raina et.al.|[2409.02337v1](http://arxiv.org/abs/2409.02337v1)|null|
|**2024-09-03**|**Action-Based ADHD Diagnosis in Video**|Yichun Li et.al.|[2409.02261v1](http://arxiv.org/abs/2409.02261v1)|null|
|**2024-09-03**|**A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**|Anna L. Trella et.al.|[2409.02069v1](http://arxiv.org/abs/2409.02069v1)|[link](https://github.com/StatisticalReinforcementLearningLab/oralytics-post-deployment-analysis)|
|**2024-09-03**|**TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**|Bobby Azad et.al.|[2409.02018v1](http://arxiv.org/abs/2409.02018v1)|null|
|**2024-09-03**|**A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**|Abdelmalek Mouazer et.al.|[2409.01903v1](http://arxiv.org/abs/2409.01903v1)|null|
|**2024-09-03**|**Training on the Benchmark Is Not All You Need**|Shiwen Ni et.al.|[2409.01790v1](http://arxiv.org/abs/2409.01790v1)|[link](https://github.com/nishiwen1214/Benchmark-leakage-detection)|
|**2024-09-03**|**Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**|Wenyang Hu et.al.|[2409.01676v1](http://arxiv.org/abs/2409.01676v1)|null|
|**2024-09-03**|**A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**|Zekang Yang et.al.|[2409.02145v1](http://arxiv.org/abs/2409.02145v1)|[link](https://github.com/yang-ze-kang/MOC)|
|**2024-09-03**|**A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**|Ruben D. Fonnegra et.al.|[2409.01596v1](http://arxiv.org/abs/2409.01596v1)|null|
|**2024-09-02**|**Kvasir-VQA: A Text-Image Pair GI Tract Dataset**|Sushant Gautam et.al.|[2409.01437v1](http://arxiv.org/abs/2409.01437v1)|[link](https://github.com/simula/Kvasir-VQA)|
|**2024-09-02**|**EEG-Language Modeling for Pathology Detection**|Sam Gijsen et.al.|[2409.07480v1](http://arxiv.org/abs/2409.07480v1)|null|
|**2024-09-02**|**SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**|Mevan Ekanayake et.al.|[2409.01013v1](http://arxiv.org/abs/2409.01013v1)|null|
|**2024-09-01**|**Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**|Sajib Acharjee Dip et.al.|[2409.00873v1](http://arxiv.org/abs/2409.00873v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**|Mahsa Khosravi et.al.|[2409.00735v1](http://arxiv.org/abs/2409.00735v1)|[link](https://github.com/scslabisu/aggym)|
|**2024-09-01**|**LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**|Zhaojie Fang et.al.|[2409.00726v1](http://arxiv.org/abs/2409.00726v1)|[link](https://github.com/Tinysqua/LPUWF-LDM)|
|**2024-09-01**|**BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**|Shams Nafisa Ali et.al.|[2409.00724v1](http://arxiv.org/abs/2409.00724v1)|[link](https://github.com/sani002/HS-Dataset)|
|**2024-09-01**|**Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**|Pragya Gupta et.al.|[2409.00718v1](http://arxiv.org/abs/2409.00718v1)|null|
|**2024-09-01**|**Curriculum Prompting Foundation Models for Medical Image Segmentation**|Xiuqi Zheng et.al.|[2409.00695v1](http://arxiv.org/abs/2409.00695v1)|[link](https://github.com/annazzz-zxq/curriculum-prompting)|
|**2024-08-31**|**Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**|Jacqueline Lammert et.al.|[2409.00544v1](http://arxiv.org/abs/2409.00544v1)|[link](https://github.com/LammertJ/RGT-Digital-Twin)|
|**2024-08-31**|**Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**|Georgios Ioannides et.al.|[2409.00391v1](http://arxiv.org/abs/2409.00391v1)|null|
|**2024-08-31**|**Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**|Mikhail Borisenkov et.al.|[2409.00310v1](http://arxiv.org/abs/2409.00310v1)|null|
|**2024-08-30**|**Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**|Antonio Rago et.al.|[2408.17401v1](http://arxiv.org/abs/2408.17401v1)|null|
|**2024-08-30**|**Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**|Yuhan Zheng et.al.|[2409.00163v1](http://arxiv.org/abs/2409.00163v1)|null|
|**2024-08-30**|**NDP: Next Distribution Prediction as a More Broad Target**|Junhao Ruan et.al.|[2408.17377v1](http://arxiv.org/abs/2408.17377v1)|null|
|**2024-08-30**|**Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**|Jutika Borah et.al.|[2408.17011v2](http://arxiv.org/abs/2408.17011v2)|null|
|**2024-08-29**|**A Survey for Large Language Models in Biomedicine**|Chong Wang et.al.|[2409.00133v1](http://arxiv.org/abs/2409.00133v1)|null|
|**2024-08-29**|**Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**|Yifei Chen et.al.|[2408.16343v1](http://arxiv.org/abs/2408.16343v1)|[link](https://github.com/justlfc03/mstnet)|
|**2024-08-29**|**Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**|Xia Jiang et.al.|[2408.16256v1](http://arxiv.org/abs/2408.16256v1)|null|
|**2024-08-29**|**M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**|Jonggwon Park et.al.|[2408.16213v1](http://arxiv.org/abs/2408.16213v1)|null|
|**2024-08-28**|**A Survey on Evaluation of Multimodal Large Language Models**|Jiaxing Huang et.al.|[2408.15769v1](http://arxiv.org/abs/2408.15769v1)|null|
|**2024-08-28**|**Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**|Yijun Zhou et.al.|[2408.15498v1](http://arxiv.org/abs/2408.15498v1)|null|
|**2024-08-27**|**What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**|Jana Schaich Borg et.al.|[2408.15354v1](http://arxiv.org/abs/2408.15354v1)|null|
|**2024-08-27**|**Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**|Weiyi Zhang et.al.|[2408.15217v1](http://arxiv.org/abs/2408.15217v1)|null|
|**2024-08-27**|**Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**|Daniil Filienko et.al.|[2409.00112v1](http://arxiv.org/abs/2409.00112v1)|null|
|**2024-08-27**|**Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**|Francesco Sovrano et.al.|[2408.15121v1](http://arxiv.org/abs/2408.15121v1)|null|
|**2024-08-27**|**MiWaves Reinforcement Learning Algorithm**|Susobhan Ghosh et.al.|[2408.15076v1](http://arxiv.org/abs/2408.15076v1)|[link](https://github.com/statisticalreinforcementlearninglab/miwaves_rl_service)|
|**2024-08-27**|**Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**|Yuqi Zhang et.al.|[2408.15032v1](http://arxiv.org/abs/2408.15032v1)|[link](https://github.com/yuqizhang-buaa/mamba2mil)|
|**2024-08-27**|**Sequence-aware Pre-training for Echocardiography Probe Guidance**|Haojun Jiang et.al.|[2408.15026v1](http://arxiv.org/abs/2408.15026v1)|null|
|**2024-08-27**|**Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**|Christos Theodoropoulos et.al.|[2408.15294v2](http://arxiv.org/abs/2408.15294v2)|null|
|**2024-08-27**|**Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**|Qiaoxin Li et.al.|[2408.14754v1](http://arxiv.org/abs/2408.14754v1)|null|
|**2024-08-27**|**Large Language Models for Disease Diagnosis: A Scoping Review**|Shuang Zhou et.al.|[2409.00097v1](http://arxiv.org/abs/2409.00097v1)|null|
|**2024-08-26**|**Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI**|Yukyeong Song et.al.|[2409.06723v1](http://arxiv.org/abs/2409.06723v1)|null|
|**2024-08-26**|**Improving Clinical Note Generation from Complex Doctor-Patient Conversation**|Yizhan Li et.al.|[2408.14568v1](http://arxiv.org/abs/2408.14568v1)|null|
|**2024-08-26**|**Temporal Ensemble Logic**|Guo-Qiang Zhang et.al.|[2408.14443v2](http://arxiv.org/abs/2408.14443v2)|null|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418v2](http://arxiv.org/abs/2408.14418v2)|null|
|**2024-08-26**|**Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**|Xiaoman Zhang et.al.|[2408.14397v1](http://arxiv.org/abs/2408.14397v1)|[link](https://github.com/rajpurkarlab/rexkg)|
|**2024-08-26**|**Foundation Models for Music: A Survey**|Yinghao Ma et.al.|[2408.14340v3](http://arxiv.org/abs/2408.14340v3)|[link](https://github.com/nicolaus625/fm4music)|
|**2024-08-26**|**Uncertainties of Latent Representations in Computer Vision**|Michael Kirchhof et.al.|[2408.14281v1](http://arxiv.org/abs/2408.14281v1)|null|
|**2024-08-26**|**Automatic Medical Report Generation: Methods and Applications**|Li Guo et.al.|[2408.13988v1](http://arxiv.org/abs/2408.13988v1)|null|
|**2024-08-25**|**Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**|Seyed Amir Ahmad Safavi-Naini et.al.|[2409.00084v2](http://arxiv.org/abs/2409.00084v2)|[link](https://github.com/sdamirsa/llm-vlm-in-gastroenterology)|
|**2024-08-25**|**PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**|Zifan Chen et.al.|[2408.13836v1](http://arxiv.org/abs/2408.13836v1)|null|

#### Abstracts
##### **SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**
2409.08936v1 by Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester

We present the SynSUM benchmark, a synthetic dataset linking unstructured
clinical notes to structured background variables. The dataset consists of
10,000 artificial patient records containing tabular variables (like symptoms,
diagnoses and underlying conditions) and related notes describing the fictional
patient encounter in the domain of respiratory diseases. The tabular portion of
the data is generated through a Bayesian network, where both the causal
structure between the variables and the conditional probabilities are proposed
by an expert based on domain knowledge. We then prompt a large language model
(GPT-4o) to generate a clinical note related to this patient encounter,
describing the patient symptoms and additional context. The SynSUM dataset is
primarily designed to facilitate research on clinical information extraction in
the presence of tabular background variables, which can be linked through
domain knowledge to concepts of interest to be extracted from the text - the
symptoms, in the case of SynSUM. Secondary uses include research on the
automation of clinical reasoning over both tabular data and text, causal effect
estimation in the presence of tabular and/or textual confounders, and
multi-modal synthetic data generation. The dataset can be downloaded from
https://github.com/prabaey/SynSUM.

æè¦ï¼æåæåº SynSUM åºæºï¼ä¸åå°éçµæ§åè¨åºè¨éé£çµå°çµæ§åèæ¯è®æ¸çåæè³æéãè©²è³æéåå« 10,000 åäººå·¥çæ­·ï¼å¶ä¸­åå«è¡¨æ ¼è®æ¸ï¼ä¾å¦ççãè¨ºæ·åæ½å¨çæ³ï¼åç¸éè¨éï¼æè¿°äºå¼å¸ç³»çµ±ç¾çé åä¸­çèæ§æ£èé­éãè³æçè¡¨æ ¼é¨åæ¯ééè²æ°ç¶²è·¯ç¢ççï¼å¶ä¸­è®æ¸ä¹éçå æçµæ§åæ¢ä»¶æ©çé½æ¯ç±å°å®¶æ ¹æé åç¥è­æåºçãç¶å¾ï¼æåæç¤ºä¸åå¤§åèªè¨æ¨¡å (GPT-4o) ç¢çèæ­¤æ£èé­éç¸éçè¨åºè¨éï¼æè¿°æ£èççåé¡å¤èæ¯ãSynSUM è³æéä¸»è¦æ¯çºäºä¿é²å¨è¡¨æ ¼èæ¯è®æ¸å­å¨çææ³ä¸é²è¡è¨åºè³è¨èåçç ç©¶ï¼éäºè®æ¸å¯ä»¥ééé åç¥è­é£çµå°å¾ææ¬ä¸­èåçç®æ¨æ¦å¿µ - å¨ SynSUM çæ¡ä¾ä¸­ï¼æ¯ççãæ¬¡è¦ç¨éåæ¬ç ç©¶è¡¨æ ¼è³æåææ¬çè¨åºæ¨çèªååãå¨è¡¨æ ¼å/æææ¬æ··æ·å å­å­å¨çææ³ä¸é²è¡å æææä¼°è¨ï¼ä»¥åå¤æ¨¡å¼åæè³æçæãæ­¤è³æéå¯å¾ https://github.com/prabaey/SynSUM ä¸è¼ã

##### **A BERT-Based Summarization approach for depression detection**
2409.08483v1 by Hossein Salahshoor Gavalan, Mohmmad Naim Rastgoo, Bahareh Nakisa

Depression is a globally prevalent mental disorder with potentially severe
repercussions if not addressed, especially in individuals with recurrent
episodes. Prior research has shown that early intervention has the potential to
mitigate or alleviate symptoms of depression. However, implementing such
interventions in a real-world setting may pose considerable challenges. A
promising strategy involves leveraging machine learning and artificial
intelligence to autonomously detect depression indicators from diverse data
sources. One of the most widely available and informative data sources is text,
which can reveal a person's mood, thoughts, and feelings. In this context,
virtual agents programmed to conduct interviews using clinically validated
questionnaires, such as those found in the DAIC-WOZ dataset, offer a robust
means for depression detection through linguistic analysis. Utilizing
BERT-based models, which are powerful and versatile yet use fewer resources
than contemporary large language models, to convert text into numerical
representations significantly enhances the precision of depression diagnosis.
These models adeptly capture complex semantic and syntactic nuances, improving
the detection accuracy of depressive symptoms. Given the inherent limitations
of these models concerning text length, our study proposes text summarization
as a preprocessing technique to diminish the length and intricacies of input
texts. Implementing this method within our uniquely developed framework for
feature extraction and classification yielded an F1-score of 0.67 on the test
set surpassing all prior benchmarks and 0.81 on the validation set exceeding
most previous results on the DAIC-WOZ dataset. Furthermore, we have devised a
depression lexicon to assess summary quality and relevance. This lexicon
constitutes a valuable asset for ongoing research in depression detection.

æè¦ï¼æé¬±çæ¯ä¸ç¨®å¨çæ®éå­å¨çå¿çç¾çï¼å¦æä¸å ä»¥è§£æ±ºï¼å¯è½æé æå´éçå¾æï¼å°¤å¶æ¯å°æå¾©ç¼æ§ç¼ä½çäººãååçç ç©¶è¡¨æï¼æ©æä»å¥æå¯è½æ¸è¼æç·©è§£æé¬±ççãç¶èï¼å¨ç¾å¯¦ç°å¢ä¸­å¯¦æ½æ­¤é¡å¹²é æªæ½å¯è½æå¸¶ä¾ç¸ç¶å¤§çææ°ãä¸åæåéçç­ç¥åæ¬å©ç¨æ©å¨å­¸ç¿åäººå·¥æºæ§ï¼å¾ä¸åçæ¸æä¾æºä¸­èªåæª¢æ¸¬æé¬±çææ¨ãæå»£æ³å¯ç¨ä¸ææè³è¨çæ¸æä¾æºä¹ä¸æ¯æå­ï¼å®å¯ä»¥æ­ç¤ºä¸åäººçæç·ãæ³æ³åæåãå¨æ­¤èçµ¡ä¸­ï¼ä½¿ç¨è¨åºé©è­åå·ï¼ä¾å¦å¨ DAIC-WOZ è³æéä¸­æ¾å°çåå·ï¼ç·¨å¯«ç¨å¼é²è¡è¨ªè«çèæ¬ä»£çäººï¼æä¾äºä¸ç¨®ééèªè¨åæé²è¡æé¬±çæª¢æ¸¬çå¼·å¤§æ¹æ³ãå©ç¨ BERT åºç¤æ¨¡åï¼åè½å¼·å¤§ä¸ç¨éå»£æ³ï¼ä½ä½¿ç¨çè³æºæ¯ç¶ä»£å¤§åèªè¨æ¨¡åå°ï¼å°æå­è½æçºæ¸å¼è¡¨ç¤ºï¼å¯é¡¯èæé«æé¬±çè¨ºæ·çæºç¢ºæ§ãéäºæ¨¡åå·§å¦å°ææè¤éçèªç¾©åå¥æ³ç´°å¾®å·®å¥ï¼æé«æé¬±çççæª¢æ¸¬æºç¢ºæ§ãéæ¼éäºæ¨¡åå¨æå­é·åº¦æ¹é¢å­å¨åºæç¼ºé·ï¼æåçç ç©¶æåºæå­æè¦ä½çºé èçæè¡ï¼ä»¥æ¸å°è¼¸å¥æå­çé·åº¦åè¤éæ§ãå¨æåç¨èªéç¼çåè½æåååé¡æ¡æ¶ä¸­å¯¦æ½æ­¤æ¹æ³ï¼å¨æ¸¬è©¦éä¸­ç¢ç 0.67 ç F1 åæ¸ï¼è¶è¶ææååçåºæºï¼å¨é©è­éä¸­ç¢ç 0.81 ç F1 åæ¸ï¼è¶é DAIC-WOZ è³æéä¸å¤§å¤æ¸ååççµæãæ­¤å¤ï¼æåè¨­è¨äºä¸åæé¬±çè©å½è¡¨ï¼ç¨æ¼è©ä¼°æè¦åè³ªåç¸éæ§ãæ­¤è©å½è¡¨æ§ææé¬±çæª¢æ¸¬æçºç ç©¶çå¯¶è²´è³ç¢ã

##### **Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**
2409.07793v1 by Fuchen Zheng, Quanjun Li, Weixuan Li, Xuhang Chen, Yihang Dong, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a critical application of semantic segmentation
in healthcare, has seen significant advancements through specialized computer
vision techniques. While deep learning-based medical image segmentation is
essential for assisting in medical diagnosis, the lack of diverse training data
causes the long-tail problem. Moreover, most previous hybrid CNN-ViT
architectures have limited ability to combine various attentions in different
layers of the Convolutional Neural Network. To address these issues, we propose
a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware
Contrastive Loss, as the overall training objective for semi-supervised
learning to mitigate the long-tail problem. Additionally, we introduce
CMAformer, a novel network that synergizes the strengths of ResUNet and
Transformer. The cross-attention block in CMAformer effectively integrates
spatial attention and channel attention for multi-scale feature fusion.
Overall, our results indicate that CMAformer, combined with the feature fusion
framework and the new consistency loss, demonstrates strong complementarity in
semi-supervised learning ensembles. We achieve state-of-the-art results on
multiple public medical image datasets. Example code are available at:
\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.

æè¦ï¼é«å­¸å½±ååå²æ¯èªæåå²å¨é«çä¿å¥é åä¸­çä¸é éè¦æç¨ï¼å·²ééå°æ¥­çé»è¦è¦è¦ºæè¡ç²å¾é¡¯èé²å±ãéç¶åºæ¼æ·±åº¦å­¸ç¿çé«å­¸å½±ååå²å°æ¼åå©é«çè¨ºæ·è³ééè¦ï¼ä½ç¼ºä¹å¤æ¨£åçè¨ç·´è³ææå°è´é·å°¾åé¡ãæ­¤å¤ï¼å¤§å¤æ¸ååçæ··åå¼ CNN-ViT æ¶æ§å¨çµåå·ç©ç¥ç¶ç¶²è·¯ä¸åå±¤ä¸­çåç¨®æ³¨æåæ¹é¢è½åæéãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ç¨®ææ ¼ææ¥å°å¶ä¸è´æ§ (LDC) æå¤±ï¼ä¸¦èéçæç¥å°æ¯æå¤±æ´åï¼ä½çºåç£ç£å¼å­¸ç¿çæ´é«è¨ç·´ç®æ¨ï¼ä»¥æ¸è¼é·å°¾åé¡ãæ­¤å¤ï¼æåä»ç´¹äº CMAformerï¼éæ¯ä¸åæ°ç©çç¶²è·¯ï¼å®ååäº ResUNet å Transformer çåªé»ãCMAformer ä¸­çäº¤åæ³¨æååå¡ææå°æ´åäºç©ºéæ³¨æååééæ³¨æåï¼ä»¥é²è¡å¤å°ºåº¦ç¹å¾µèåãç¸½çä¾èªªï¼æåççµæè¡¨æï¼CMAformer çµåç¹å¾µèåæ¶æ§åæ°çç¨ å¯æå¤±ï¼å¨åç£ç£å¼å­¸ç¿éåä¸­å±ç¾åºå¼·å¤§çäºè£æ§ãæåå¨å¤åå¬éé«å­¸å½±åè³æéä¸åå¾äºæåé²çææãç¯ä¾ç¨å¼ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}ã

##### **ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**
2409.07779v1 by Fuchen Zheng, Xinyi Chen, Xuhang Chen, Haolun Li, Xiaojiao Guo, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a crucial task in computer vision, facilitates
the automated delineation of anatomical structures and pathologies, supporting
clinicians in diagnosis, treatment planning, and disease monitoring. Notably,
transformers employing shifted window-based self-attention have demonstrated
exceptional performance. However, their reliance on local window attention
limits the fusion of local and global contextual information, crucial for
segmenting microtumors and miniature organs. To address this limitation, we
propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer
architecture that effectively integrates local and global features for precise
medical image segmentation. ASSNet comprises a transformer-based U-shaped
encoder-decoder network. The encoder utilizes shifted window self-attention
across five resolutions to extract multi-scale features, which are then
propagated to the decoder through skip connections. We introduce an augmented
multi-layer perceptron within the encoder to explicitly model long-range
dependencies during feature extraction. Recognizing the constraints of
conventional symmetrical encoder-decoder designs, we propose an Adaptive
Feature Fusion (AFF) decoder to complement our encoder. This decoder
incorporates three key components: the Long Range Dependencies (LRD) block, the
Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)
block. These components synergistically facilitate the effective fusion of
multi-scale features extracted by the decoder while capturing long-range
dependencies and refining object boundaries. Comprehensive experiments on
diverse medical image segmentation tasks, including multi-organ, liver tumor,
and bladder tumor segmentation, demonstrate that ASSNet achieves
state-of-the-art results. Code and models are available at:
\url{https://github.com/lzeeorno/ASSNet}.

æè¦ï¼<paragraph>é«å­¸å½±ååå²æ¯é»è¦è¦è¦ºä¸­ä¸é éè¦çä»»åï¼æå©æ¼èªåæç¹ªè§£åçµæ§åççï¼åå©è¨åºé«å¸«é²è¡è¨ºæ·ãæ²»çè¨ç«åç¾çç£æ§ãå¼å¾æ³¨æçæ¯ï¼æ¡ç¨ä½ç§»è¦çªèªæ³¨æåæ©å¶çTransformerå±ç¾åºéå¡çæè½ãç¶èï¼å®åä¾è³´æ¼ååè¦çªæ³¨æåï¼ééå¶äºåååå¨åèçµ¡è³è¨çèåï¼èéå°æ¼åå²å¾®å°è«ç¤åå¾®åå¨å®è³ééè¦ãçºäºè§£æ±ºéåéå¶ï¼æåæåºäºèªé©æèªç¾©åå²ç¶²è·¯ (ASSNet)ï¼éæ¯ä¸åTransformeræ¶æ§ï¼å¯ä»¥æææ´ååååå¨åç¹å¾µï¼ä»¥é²è¡ç²¾ç¢ºçé«å­¸å½±ååå²ãASSNet åå«ä¸ååºæ¼Transformerç U åç·¨ç¢¼å¨-è§£ç¢¼å¨ç¶²è·¯ãç·¨ç¢¼å¨å©ç¨äºåè§£æåº¦çä½ç§»è¦çªèªæ³¨æåä¾èåå¤å°ºåº¦ç¹å¾µï¼ç¶å¾ééè·³èºé£ç·å°éäºç¹å¾µå³æ­å°è§£ç¢¼å¨ãæåå¨ç·¨ç¢¼å¨ä¸­å¼å¥äºæ´å¢çå¤å±¤æç¥å¨ï¼ä»¥ä¾¿å¨ç¹å¾µèåæéæç¢ºå°å»ºæ¨¡é·ç¨ä¾è³´æ§ãéæ¼å³çµ±å°ç¨±ç·¨ç¢¼å¨-è§£ç¢¼å¨è¨­è¨çéå¶ï¼æåæåºäºä¸åèªé©æç¹å¾µèå (AFF) è§£ç¢¼å¨ä¾è£åæåçç·¨ç¢¼å¨ãæ­¤è§£ç¢¼å¨åå«ä¸åééµçµæé¨åï¼é·ç¨ä¾è³´æ§ (LRD) åå¡ãå¤å°ºåº¦ç¹å¾µèå (MFF) åå¡åèªé©æèªç¾©ä¸­å¿ (ASC) åå¡ãéäºçµæé¨åç¸äºéåï¼ä¿æè§£ç¢¼å¨èåçå¤å°ºåº¦ç¹å¾µææèåï¼åæææé·ç¨ä¾è³´æ§ä¸¦å¾®èª¿ç©ä»¶éçãå¨å¤å¨å®ãèèè«ç¤åèè±è«ç¤åå²ç­åç¨®é«å­¸å½±ååå²ä»»åä¸çå¨é¢å¯¦é©è­æï¼ASSNet éå°äºæåé²çææãç¨å¼ç¢¼åæ¨¡åå¯æ¼ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/lzeeorno/ASSNet}ã</paragraph>

##### **SoK: Security and Privacy Risks of Medical AI**
2409.07415v1 by Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang

The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.

æè¦ï¼ç§æèé«ççæ´åéåäºä¸åæ°ç´åï¼ç±äººå·¥æºæ§åæ©å¨å­¸ç¿é©åçè»é«ç³»çµ±å·²æçºé«çç¢ååæåçå¿è¦çµæé¨åãéç¶éäºé²æ­¥å°æ¹åæ£èç§è­·åé«çä¿å¥æä¾æçæå¾å¤§çå¹«å©ï¼ä½å®åä¹è®ææçé«çè³æåç³»çµ±å®æ´æ§é¢è¨æ½å¨çç¶²è·¯æ»æé¢¨éªãæ¬ææ¢è¨äºäººå·¥æºæ§/æ©å¨å­¸ç¿æç¨å¨é«çä¿å¥ä¸­å¸¶ä¾çå®å¨æ§åé±ç§å¨èãééå¾¹åºæª¢è¦åé é«çé åç¾æçç ç©¶ï¼æåç¼ç¾äºå¨äºè§£éå°é«çäººå·¥æºæ§ç³»çµ±çå°ææ§æ»ææ¹é¢æé¡¯èçå·®è·ãééæ¦è¿°é«çç°å¢çç¹å®å°ææ§å¨èæ¨¡åä¸¦æ¾åºå®¹æåæ»æçæç¨é åï¼æåçºæªä¾ç ç©¶å¥ å®åºç¤ï¼æ¢è¨äººå·¥æºæ§é©åé«çç³»çµ±çå®å¨æ§èå¾©ååãééåæä¸åçå¨èæ¨¡ååéå°ä¸åé«çé åçå°ææ§æ»æå¯è¡æ§ç ç©¶ï¼æåå°äººå·¥æºæ§é«çä¿å¥æè¡å¿«éç¼å±é åä¸­ç¶²è·¯å®å¨ç ç©¶çè¿«åéæ±æä¾äºä»¤äººä¿¡æçè¦è§£ã

##### **Federated Impression for Learning with Distributed Heterogeneous Data**
2409.07351v1 by Sana Ayromlou, Atrin Arya, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li

Standard deep learning-based classification approaches may not always be
practical in real-world clinical applications, as they require a centralized
collection of all samples. Federated learning (FL) provides a paradigm that can
learn from distributed datasets across clients without requiring them to share
data, which can help mitigate privacy and data ownership issues. In FL,
sub-optimal convergence caused by data heterogeneity is common among data from
different health centers due to the variety in data collection protocols and
patient demographics across centers. Through experimentation in this study, we
show that data heterogeneity leads to the phenomenon of catastrophic forgetting
during local training. We propose FedImpres which alleviates catastrophic
forgetting by restoring synthetic data that represents the global information
as federated impression. To achieve this, we distill the global model resulting
from each communication round. Subsequently, we use the synthetic data
alongside the local data to enhance the generalization of local training.
Extensive experiments show that the proposed method achieves state-of-the-art
performance on both the BloodMNIST and Retina datasets, which contain label
imbalance and domain shift, with an improvement in classification accuracy of
up to 20%.

æè¦ï¼æ¨æºçæ·±åº¦å­¸ç¿åé¡æ¹æ³å¨å¯¦éçè¨åºæç¨ä¸­å¯è½ä¸¦ä¸ç¸½æ¯å¯¦ç¨çï¼å çºå®åéè¦éä¸­æ¶éæææ¨£æ¬ãè¯é¦å­¸ç¿ (FL) æä¾äºä¸åç¯ä¾ï¼å¯ä»¥å¨ä¸è®å®¢æ¶ç«¯åäº«æ¸æçææ³ä¸å¾åå¸å¼æ¸æéå­¸ç¿ï¼éæå©æ¼æ¸è¼é±ç§åæ¸ææææ¬åé¡ãå¨ FL ä¸­ï¼ç±æ¼ä¸åé«çä¸­å¿çæ¸ææ¶éåå®åæ£èäººå£çµ±è¨è³æçå·®ç°ï¼ä¾èªä¸åé«çä¸­å¿çæ¸æä¹éå¸¸è¦çæ¸æç°è³ªæ§æå°è´æ¬¡æä½³æ¶æãééæ¬ç ç©¶ä¸­çå¯¦é©ï¼æåè¡¨ææ¸æç°è³ªæ§æå°è´å±é¨è¨ç·´æéç¼çç½é£æ§éºå¿ç¾è±¡ãæåæåº FedImpresï¼å®éééåè¡¨ç¤ºå¨çè³è¨çåæè³æä½çºè¯é¦å°è±¡ä¾æ¸è¼ç½é£æ§éºå¿ãçºæ­¤ï¼æåæçåºæ¯ä¸è¼ªéè¨æç¢ççå¨çæ¨¡åãé¨å¾ï¼æåä½¿ç¨åæè³æåæ¬å°è³æä¾å¢å¼·æ¬å°è¨ç·´çæ¦æ¬æ§ãå»£æ³çå¯¦é©è¡¨æï¼ææåºçæ¹æ³å¨ BloodMNIST å Retina æ¸æéä¸é½éå°äºæåé²çæè½ï¼éäºæ¸æéåå«æ¨ç±¤ä¸å¹³è¡¡åé åè½ç§»ï¼åé¡æºç¢ºåº¦æé«äº 20%ã

##### **MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**
2409.07314v1 by Praveen K Kanithi, ClÃ©ment Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan

The rapid development of Large Language Models (LLMs) for healthcare
applications has spurred calls for holistic evaluation beyond frequently-cited
benchmarks like USMLE, to better reflect real-world performance. While
real-world assessments are valuable indicators of utility, they often lag
behind the pace of LLM evolution, likely rendering findings obsolete upon
deployment. This temporal disconnect necessitates a comprehensive upfront
evaluation that can guide model selection for specific clinical applications.
We introduce MEDIC, a framework assessing LLMs across five critical dimensions
of clinical competence: medical reasoning, ethics and bias, data and language
understanding, in-context learning, and clinical safety. MEDIC features a novel
cross-examination framework quantifying LLM performance across areas like
coverage and hallucination detection, without requiring reference outputs. We
apply MEDIC to evaluate LLMs on medical question-answering, safety,
summarization, note generation, and other tasks. Our results show performance
disparities across model sizes, baseline vs medically finetuned models, and
have implications on model selection for applications requiring specific model
strengths, such as low hallucination or lower cost of inference. MEDIC's
multifaceted evaluation reveals these performance trade-offs, bridging the gap
between theoretical capabilities and practical implementation in healthcare
settings, ensuring that the most promising models are identified and adapted
for diverse healthcare applications.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨é«çä¿å¥æç¨æ¹é¢çå¿«éç¼å±ï¼ä¿ä½¿äººåå¼ç±²é²è¡æ´é«è©ä¼°ï¼è¶è¶ç¶å¸¸å¼ç¨çåºæºï¼ä¾å¦ USMLEï¼ï¼ä»¥æ´å¥½å°åæ å¯¦éæè½ãåç®¡å¯¦éè©ä¼°æ¯å¯¦ç¨æ§çå¯¶è²´ææ¨ï¼ä½å®åéå¸¸è½å¾æ¼ LLM æ¼åçéåº¦ï¼å¨é¨ç½²å¾å¯è½æä½¿ç ç©¶çµæéæãéç¨®æéä¸çè«ç¯éè¦é²è¡å¨é¢çåæè©ä¼°ï¼ä»¥æå°ç¹å®è¨åºæç¨ç¨å¼çæ¨¡åé¸æãæåå¼é² MEDICï¼ä¸åè©ä¼° LLM è·¨è¶è¨åºè½åçäºåééµé¢åçæ¶æ§ï¼é«çæ¨çãå«çååå·®ãè³æåèªè¨çè§£ãæå¢å­¸ç¿åè¨åºå®å¨æ§ãMEDIC æ¡ç¨ä¸ç¨®æ°ç©çäº¤äºå¼æª¢æ¥æ¶æ§ï¼éå LLM å¨æ¶µèç¯ååå¹»è¦ºåµæ¸¬ç­é åçæè½ï¼èä¸éè¦åèè¼¸åºãæåä½¿ç¨ MEDIC ä¾è©ä¼° LLM å¨é«çåé¡è§£ç­ãå®å¨æ§ãæè¦ãç­è¨ç¢çåå¶ä»ä»»åä¸çè¡¨ç¾ãæåççµæé¡¯ç¤ºï¼ä¸åæ¨¡åå¤§å°ãåºæºèç¶éé«çå¾®èª¿çæ¨¡åä¹éçæè½å·®ç°ï¼ä¸¦å°éè¦ç¹å®æ¨¡ååªå¢çæç¨ç¨å¼ï¼ä¾å¦ä½å¹»è¦ºæè¼ä½çæ¨è«ææ¬ï¼çæ¨¡åé¸æç¢çå½±é¿ãMEDIC çå¤é¢åè©ä¼°æ­ç¤ºäºéäºæè½æ¬è¡¡ï¼ç¸®å°äºçè«è½åèé«çä¿å¥ç°å¢ä¸­çå¯¦éå¯¦ä½ä¹éçå·®è·ï¼ç¢ºä¿æ¾åºææå¸æçæ¨¡åï¼ä¸¦éå°ä¸åçé«çä¿å¥æç¨ç¨å¼é²è¡èª¿æ´ã

##### **Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**
2409.07186v1 by Sheng Chen, Zihao Tang, Mariano Cabezas, Xinyi Wang, Arkiev D'Souza, Michael Barnett, Fernando Calamante, Weidong Cai, Chenyu Wang

Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging
(MRI) technique sensitised to the diffusivity of water molecules, offering the
capability to inspect tissue microstructures and is the only in-vivo method to
reconstruct white matter fiber tracts non-invasively. The DWI signal can be
analysed with the diffusion tensor imaging (DTI) model to estimate the
directionality of water diffusion within voxels. Several scalar metrics,
including axial diffusivity (AD), mean diffusivity (MD), radial diffusivity
(RD), and fractional anisotropy (FA), can be further derived from DTI to
quantitatively summarise the microstructural integrity of brain tissue. These
scalar metrics have played an important role in understanding the organisation
and health of brain tissue at a microscopic level in clinical studies. However,
reliable DTI metrics rely on DWI acquisitions with high gradient directions,
which often go beyond the commonly used clinical protocols. To enhance the
utility of clinically acquired DWI and save scanning time for robust DTI
analysis, this work proposes DirGeo-DTI, a deep learning-based method to
estimate reliable DTI metrics even from a set of DWIs acquired with the minimum
theoretical number (6) of gradient directions. DirGeo-DTI leverages directional
encoding and geometric constraints to facilitate the training process. Two
public DWI datasets were used for evaluation, demonstrating the effectiveness
of the proposed method. Extensive experimental results show that the proposed
method achieves the best performance compared to existing DTI enhancement
methods and potentially reveals further clinical insights with routine clinical
DWI scans.

æè¦ï¼æ´æ£å æ¬å½±åï¼DWIï¼æ¯ä¸ç¨®ç£æ¯é å½±ï¼MRIï¼æè¡ï¼å°æ°´åå­æ´æ£ææï¼è½æª¢æ¸¬çµç¹å¾®çµæ§ï¼æ¯å¯ä¸éä¾µå¥æ§éå»ºç½è³ªçºç¶­æçé«å§æ¹æ³ãDWI è¨èå¯ç¨æ´æ£å¼µéå½±åï¼DTIï¼æ¨¡ååæï¼ä»¥ä¼°è¨é«ç´ å§æ°´æ´æ£çæ¹åæ§ãå¾ DTI å¯é²ä¸æ­¥è¡çåºæ¸åæ¨ééæ¸¬ï¼åæ¬è»¸åæ´æ£çï¼ADï¼ãå¹³åæ´æ£çï¼MDï¼ãå¾åæ´æ£çï¼RDï¼ååæ¸ååç°æ§ï¼FAï¼ï¼ä»¥éåç¸½çµè¦çµç¹çå¾®çµæ§å®æ´æ§ãéäºæ¨ééæ¸¬å¨è¨åºç ç©¶ä¸­å°æ¼äºè§£è¦çµç¹å¨å¾®è§å±¤é¢ççµç¹åå¥åº·æ®æ¼éè¦è§è²ãç¶èï¼å¯é ç DTI éæ¸¬ä»°è³´å·æé«æ¢¯åº¦æ¹åç DWI æ·åï¼ééå¸¸è¶åºè¨åºä¸å¸¸ç¨çåå®ãçºäºæåè¨åºæ·å DWI çæç¨ï¼ä¸¦çºç©©å¥ç DTI åæç¯çæææéï¼æ¬ç ç©¶æåº DirGeo-DTIï¼ä¸ç¨®åºæ¼æ·±åº¦å­¸ç¿çæ¹æ³ï¼å³ä½¿å¾å·åæå°çè«æ¸éï¼6ï¼çæ¢¯åº¦æ¹åæ·åç DWI çµä¹è½ä¼°è¨å¯é ç DTI éæ¸¬ãDirGeo-DTI å©ç¨æ¹åç·¨ç¢¼åå¹¾ä½ç´æä¾ä¿é²è¨ç·´éç¨ãä½¿ç¨å©åå¬éç DWI è³æéé²è¡è©ä¼°ï¼è­æäºææåºæ¹æ³çæææ§ãå»£æ³çå¯¦é©çµæé¡¯ç¤ºï¼èç¾æç DTI å¢å¼·æ¹æ³ç¸æ¯ï¼ææåºçæ¹æ³åå¾æä½³æè½ï¼ä¸¦æ½å¨æ­é²ä¾è¡è¨åº DWI ææçé²ä¸æ­¥è¨åºè¦è§£ã

##### **CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**
2409.07092v1 by Feiyang Jia, Zhineng Chen, Ziying Song, Lin Liu, Caiyan Jia

Super-resolution (SR) aims to enhance the quality of low-resolution images
and has been widely applied in medical imaging. We found that the design
principles of most existing methods are influenced by SR tasks based on
real-world images and do not take into account the significance of the
multi-level structure in pathological images, even if they can achieve
respectable objective metric evaluations. In this work, we delve into two
super-resolution working paradigms and propose a novel network called CWT-Net,
which leverages cross-scale image wavelet transform and Transformer
architecture. Our network consists of two branches: one dedicated to learning
super-resolution and the other to high-frequency wavelet features. To generate
high-resolution histopathology images, the Transformer module shares and fuses
features from both branches at various stages. Notably, we have designed a
specialized wavelet reconstruction module to effectively enhance the wavelet
domain features and enable the network to operate in different modes, allowing
for the introduction of additional relevant information from cross-scale
images. Our experimental results demonstrate that our model significantly
outperforms state-of-the-art methods in both performance and visualization
evaluations and can substantially boost the accuracy of image diagnostic
networks.

æè¦ï¼è¶è§£æåº¦ (SR) æ¨å¨æåä½è§£æåº¦å½±åçåè³ªï¼ä¸¦å·²å»£æ³æç¨æ¼é«å­¸å½±åãæåç¼ç¾ç¾ææ¹æ³çå¤§é¨åè¨­è¨ååé½åå°åºæ¼çå¯¦å½±åç SR ä»»åå½±é¿ï¼èä¸å³ä½¿å®åè½éå°å¯è§çå®¢è§ææ¨è©ä¼°ï¼ä¹ä¸æèæ®ççå½±åä¸­å¤å±¤ç´çµæ§çéè¦æ§ãå¨éé å·¥ä½ä¸­ï¼æåæ·±å¥æ¢è¨å©ç¨®è¶è§£æåº¦å·¥ä½ç¯ä¾ï¼ä¸¦æåºä¸ååçº CWT-Net çæ°åç¶²è·¯ï¼å®å©ç¨è·¨å°ºåº¦å½±åå°æ³¢è½æå Transformer æ¶æ§ãæåçç¶²è·¯åå«å©ååæ¯ï¼ä¸åå°éç¨æ¼å­¸ç¿è¶è§£æåº¦ï¼å¦ä¸ååç¨æ¼é«é »å°æ³¢ç¹å¾µãçºäºç¢çé«è§£æåº¦çµç¹ççå­¸å½±åï¼Transformer æ¨¡çµæå¨ä¸åéæ®µåäº«åèåä¾èªå©ååæ¯çç¹å¾µãå¼å¾æ³¨æçæ¯ï¼æåè¨­è¨äºä¸åå°éçå°æ³¢éå»ºæ¨¡çµï¼ä»¥ææå¢å¼·å°æ³¢åç¹å¾µï¼ä¸¦è®ç¶²è·¯è½å¤ å¨ä¸åæ¨¡å¼ä¸éä½ï¼åè¨±å¾è·¨å°ºåº¦å½±åä¸­å¼å¥å¶ä»ç¸éè³è¨ãæåçå¯¦é©çµæè­æï¼æåçæ¨¡åå¨æè½åè¦è¦ºåè©ä¼°æ¹é¢é½å¤§å¹åªæ¼ç¾ææè¡ï¼èä¸è½å¤§å¹æåå½±åè¨ºæ·ç¶²è·¯çæºç¢ºåº¦ã

##### **Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**
2409.07012v1 by Daeun Kyung, Junu Kim, Tackeun Kim, Edward Choi

Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals
to assess patient conditions and monitor changes over time. Generative models,
specifically diffusion-based models, have shown promise in generating realistic
synthetic X-rays. However, these models mainly focus on conditional generation
using single-time-point data, i.e., typically CXRs taken at a specific time
with their corresponding reports, limiting their clinical utility, particularly
for capturing temporal changes. To address this limitation, we propose a novel
framework, EHRXDiff, which predicts future CXR images by integrating previous
CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc.
Our framework dynamically tracks and predicts disease progression based on a
latent diffusion model, conditioned on the previous CXR image and a history of
medical events. We comprehensively evaluate the performance of our framework
across three key aspects, including clinical consistency, demographic
consistency, and visual realism. We demonstrate that our framework generates
high-quality, realistic future images that capture potential temporal changes,
suggesting its potential for further development as a clinical simulation tool.
This could offer valuable insights for patient monitoring and treatment
planning in the medical field.

æè¦ï¼è¸é¨ X åå½±åï¼CXRï¼æ¯ä¸ç¨®éè¦çè¨ºæ·å·¥å·ï¼ç¨æ¼é«é¢è©ä¼°çæ£çæ³ä¸¦ç£æ§å¶é¨èæéçè®åãçææ¨¡åï¼ç¹å¥æ¯åºæ¼æ´æ£çæ¨¡åï¼å·²å¨çæé¼ççåæ X åå½±åæ¹é¢å±ç¾åºæ½åãç¶èï¼éäºæ¨¡åä¸»è¦å°æ³¨æ¼ä½¿ç¨å®ä¸æéé»è³æé²è¡æ¢ä»¶çæï¼å³éå¸¸å¨ç¹å®æéé»ææç CXR åå¶å°æå ±åï¼ééå¶äºå¶è¨åºæç¨ï¼ç¹å¥æ¯å°æ¼æææéè®åãçºäºè§£æ±ºæ­¤éå¶ï¼æåæåºäºä¸åæ°çæ¡æ¶ EHRXDiffï¼å®ééæ´åååç CXR èå¾çºçé«çäºä»¶ï¼ä¾å¦èæ¹ãå¯¦é©å®¤æª¢æ¸¬ç­ï¼ä¾é æ¸¬æªä¾ç CXR å½±åãæåçæ¡æ¶åºæ¼æ½å¨æ´æ£æ¨¡ååæè¿½è¹¤ä¸¦é æ¸¬ç¾çé²å±ï¼æ¢ä»¶åæ±ºæ¼ååç CXR å½±ååé«çäºä»¶çæ­·å²è¨éãæåå¨é¢è©ä¼°äºæåæ¡æ¶å¨ä¸åééµæ¹é¢çæè½ï¼åæ¬è¨åºä¸è´æ§ãäººå£çµ±è¨ä¸è´æ§åè¦è¦ºé¼çåº¦ãæåè­ææåçæ¡æ¶çæäºé«åè³ªãé¼ççæªä¾å½±åï¼ææäºæ½å¨çæéè®åï¼éè¡¨æå¶é²ä¸æ­¥ç¼å±çºè¨åºæ¨¡æ¬å·¥å·çæ½åãéå¯ä»¥çºé«çé åççæ£ç£æ§åæ²»çè¦åæä¾æå¹å¼çè¦è§£ã

##### **Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**
2409.06928v1 by Jianmei Jiang, Huijin Wang, Jieyun Bai, Shun Long, Shuangping Chen, Victor M. Campello, Karim Lekadir

The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a
pivotal step in monitoring labor progression and identifying potential delivery
complications. Despite the advances in deep learning, the lack of annotated
medical images hinders the training of segmentation. Traditional
semi-supervised learning approaches primarily utilize a unified network model
based on Convolutional Neural Networks (CNNs) and apply consistency
regularization to mitigate the reliance on extensive annotated data. However,
these methods often fall short in capturing the discriminative features of
unlabeled data and in delineating the long-range dependencies inherent in the
ambiguous boundaries of PSFH within ultrasound images. To address these
limitations, we introduce a novel framework, the Dual-Student and Teacher
Combining CNN and Transformer (DSTCT), which synergistically integrates the
capabilities of CNNs and Transformers. Our framework comprises a Vision
Transformer (ViT) as the teacher and two student mod ls one ViT and one CNN.
This dual-student setup enables mutual supervision through the generation of
both hard and soft pseudo-labels, with the consistency in their predictions
being refined by minimizing the classifier determinacy discrepancy. The teacher
model further reinforces learning within this architecture through the
imposition of consistency regularization constraints. To augment the
generalization abilities of our approach, we employ a blend of data and model
perturbation techniques. Comprehensive evaluations on the benchmark dataset of
the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT
framework outperformed ten contemporary semi-supervised segmentation methods.
Code available at https://github.com/jjm1589/DSTCT.

æè¦ï¼æ¥éª¨è¯ååèé ­ï¼PSFHï¼çåå²æ¯ç£æ¸¬ç¢ç¨é²åº¦åè­å¥æ½å¨åå¨©ä½µç¼ççééµæ­¥é©ãåç®¡æ·±åº¦å­¸ç¿åå¾é²å±ï¼ä½æ¨è¨»é«å­¸å½±åçç¼ºä¹é»ç¤äºåå²çè¨ç·´ãå³çµ±çåç£ç£å¼å­¸ç¿æ¹æ³ä¸»è¦å©ç¨åºæ¼å·ç©ç¥ç¶ç¶²è·¯ï¼CNNï¼ççµ±ä¸ç¶²è·¯æ¨¡åï¼ä¸¦æç¨ä¸è´æ§æ­£ååä¾æ¸è¼å°å¤§éæ¨è¨»æ¸æçä¾è³´ãç¶èï¼éäºæ¹æ³éå¸¸ç¡æ³æææªæ¨è¨»æ¸æçåå¥æ§ç¹å¾µï¼ä¹ç¡æ³æç¹ªè¶é³æ³¢å½±åä¸­ PSFH æ¨¡ç³éçä¸­åºæçé·ç¨ä¾è³´æ§ãçºäºè§£æ±ºéäºéå¶ï¼æåå¼å¥äºä¸åæ°çæ¡æ¶ï¼å³éå­¸çåæå¸«çµå CNN å Transformerï¼DSTCTï¼ï¼å®ååæ´åäº CNN å Transformer çåè½ãæåçæ¡æ¶åå«ä¸åè¦è¦º Transformerï¼ViTï¼ä½çºæå¸«åå©åå­¸çæ¨¡åï¼ä¸å ViT åä¸å CNNãéç¨®éå­¸çè¨­ç½®ééçæç¡¬å½æ¨ç±¤åè»å½æ¨ç±¤å¯¦ç¾ç¸äºç£ç£ï¼ä¸¦ééæå°ååé¡å¨ç¢ºå®æ§å·®ç°ä¾åªåå¶é æ¸¬çä¸è´æ§ãæå¸«æ¨¡åééæ½å ä¸è´æ§æ­£ååç´æé²ä¸æ­¥å å¼·äºæ­¤æ¶æ§ä¸­çå­¸ç¿ãçºäºå¢å¼·æåæ¹æ³çæ³åè½åï¼æåæ¡ç¨äºæ¸æåæ¨¡åæ¾åæè¡çæ··åãå¨ MICCAI 2023 ç PSFH åå²å¤§ææ°åºæºæ¸æéä¸çç¶åè©ä¼°è¡¨æï¼æåç DSTCT æ¡æ¶åªæ¼åç¨®ç¶ä»£åç£ç£å¼åå²æ¹æ³ãç¨å¼ç¢¼å¯å¨ https://github.com/jjm1589/DSTCT åå¾ã

##### **Bifurcation Identification for Ultrasound-driven Robotic Cannulation**
2409.06817v1 by Cecilia G. Morales, Dhruv Srikanth, Jack H. Good, Keith A. Dufendach, Artur Dubrawski

In trauma and critical care settings, rapid and precise intravascular access
is key to patients' survival. Our research aims at ensuring this access, even
when skilled medical personnel are not readily available. Vessel bifurcations
are anatomical landmarks that can guide the safe placement of catheters or
needles during medical procedures. Although ultrasound is advantageous in
navigating anatomical landmarks in emergency scenarios due to its portability
and safety, to our knowledge no existing algorithm can autonomously extract
vessel bifurcations using ultrasound images. This is primarily due to the
limited availability of ground truth data, in particular, data from live
subjects, needed for training and validating reliable models. Researchers often
resort to using data from anatomical phantoms or simulations. We introduce
BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a
novel algorithm that identifies vessel bifurcations and provides optimal needle
insertion sites for an autonomous robotic cannulation system. BIFURC integrates
expert knowledge with deep learning techniques to efficiently detect vessel
bifurcations within the femoral region and can be trained on a limited amount
of in-vivo data. We evaluated our algorithm using a medical phantom as well as
real-world experiments involving live pigs. In all cases, BIFURC consistently
identified bifurcation points and needle insertion locations in alignment with
those identified by expert clinicians.

æè¦ï¼å¨åµå·åéçç§è­·ç°å¢ä¸­ï¼å¿«éä¸ç²¾ç¢ºçè¡ç®¡å§éè·¯æ¯æ£èå­æ´»çééµãæåçç ç©¶æ¨å¨ç¢ºä¿éç¨®éè·¯ï¼å³ä½¿å¨çç·´çé«çäººå¡ç¡æ³ç«å³ç²å¾çææ³ä¸ãè¡ç®¡ååæ¯è§£åæ¨èªï¼å¯ä»¥æå°å¨é«çéç¨ä¸­å®å¨æ¾ç½®å°ç®¡æéé ­ãåç®¡è¶é³æ³¢ç±æ¼å¶å¯ææ§åå®å¨æ§èå¨ç·æ¥ææ³ä¸å°èªè§£åæ¨èªå·æåªå¢ï¼ä½ææåæç¥ï¼æ²æç¾ææ¼ç®æ³å¯ä»¥ä½¿ç¨è¶é³æ³¢å½±åèªåæåè¡ç®¡ååãéä¸»è¦æ¯ç±æ¼å°é¢å¯¦æ³è³æçå¯ç¨æ§æéï¼ç¹å¥æ¯ä¾èªæ´»é«åè©¦èçè³æï¼èéå°æ¼è¨ç·´åé©è­å¯é æ¨¡åæ¯å¿éçãç ç©¶äººå¡ç¶å¸¸æ±å©æ¼ä½¿ç¨è§£åæ¨¡åææ¨¡æ¬çè³æãæåå¼å¥äº BIFURCï¼å³è¶é³æ³¢é©åæ©å¨äººæç®¡çååè­å¥ï¼éæ¯ä¸ç¨®æ°ç©çæ¼ç®æ³ï¼å¯ä»¥è­å¥è¡ç®¡ååï¼ä¸¦çºèªåæ©å¨äººæç®¡ç³»çµ±æä¾æä½³éé ­æå¥ä½ç½®ãBIFURC å°å°å®¶ç¥è­èæ·±åº¦å­¸ç¿æè¡ç¸çµåï¼ä»¥æææª¢æ¸¬è¡éª¨ååå§çè¡ç®¡ååï¼ä¸¦ä¸å¯ä»¥å¨æéçé«å§è³æä¸é²è¡è¨ç·´ãæåä½¿ç¨é«ç¨æ¨¡åä»¥åæ¶åæ´»é«è±¬ççå¯¦ä¸çå¯¦é©è©ä¼°äºæåçæ¼ç®æ³ãå¨ææææ³ä¸ï¼BIFURC é½ä¸è´å°è­å¥åºååé»åéé ­æå¥ä½ç½®ï¼èå°å®¶è¨åºé«çè­å¥çä½ç½®ä¸è´ã

##### **Personalized Federated Learning Techniques: Empirical Analysis**
2409.06805v1 by Azal Ahmad Khan, Ahmad Faraz Khan, Haider Ali, Ali Anwar

Personalized Federated Learning (pFL) holds immense promise for tailoring
machine learning models to individual users while preserving data privacy.
However, achieving optimal performance in pFL often requires a careful
balancing act between memory overhead costs and model accuracy. This paper
delves into the trade-offs inherent in pFL, offering valuable insights for
selecting the right algorithms for diverse real-world scenarios. We empirically
evaluate ten prominent pFL techniques across various datasets and data splits,
uncovering significant differences in their performance. Our study reveals
interesting insights into how pFL methods that utilize personalized (local)
aggregation exhibit the fastest convergence due to their efficiency in
communication and computation. Conversely, fine-tuning methods face limitations
in handling data heterogeneity and potential adversarial attacks while
multi-objective learning methods achieve higher accuracy at the cost of
additional training and resource consumption. Our study emphasizes the critical
role of communication efficiency in scaling pFL, demonstrating how it can
significantly affect resource usage in real-world deployments.

æè¦ï¼åäººåè¯åå­¸ç¿ (pFL) å¨ç¶­è­·è³æé±ç§çåæï¼çºå®¢è£½åæ©å¨å­¸ç¿æ¨¡åçµ¦åå¥ä½¿ç¨èå¸¶ä¾æ¥µå¤§çå¸æãç¶èï¼è¦éæ pFL çæä½³æè½ï¼éå¸¸éè¦å¨è¨æ¶é«éé·ææ¬åæ¨¡åæºç¢ºåº¦ä¹éåå¾ä»ç´°çå¹³è¡¡ãæ¬ææ·±å¥æ¢è¨ pFL ä¸­åºæçæ¬è¡¡åæ¨ï¼çºå¨åç¨®å¯¦éå ´æ¯ä¸­é¸ææ­£ç¢ºçæ¼ç®æ³æä¾å¯¶è²´çè¦è§£ãæåæ ¹æåç¨®è³æéåè³æåå²ï¼å°åç¨®ååºç pFL æè¡é²è¡å¯¦è­è©ä¼°ï¼æ­é²å¶æè½çé¡¯èå·®ç°ãæåçç ç©¶æ­é²äºæè¶£çè¦è§£ï¼èªªæå©ç¨åäººå (å±é¨) èåç pFL æ¹æ³ï¼ç±æ¼å¶å¨éè¨åéç®æ¹é¢çæçï¼å±ç¾åºæå¿«çæ¶æéåº¦ãç¸åå°ï¼å¾®èª¿æ¹æ³å¨èçè³æç°è³ªæ§åæ½å¨å°ææ»ææ¹é¢é¢è¨éå¶ï¼èå¤ç®æ¨å­¸ç¿æ¹æ³åä»¥é¡å¤çè¨ç·´åè³æºæ¶èçºä»£å¹ï¼éå°æ´é«çæºç¢ºåº¦ãæåçç ç©¶å¼·èª¿äºéè¨æçå¨æ´å pFL ä¸­çééµè§è²ï¼å±ç¤ºå®å¦ä½å¨å¯¦éé¨ç½²ä¸­é¡¯èå½±é¿è³æºä½¿ç¨ã

##### **Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**
2409.06672v1 by Cristian Trout

Many experts believe that AI systems will sooner or later pose uninsurable
risks, including existential risks. This creates an extreme judgment-proof
problem: few if any parties can be held accountable ex post in the event of
such a catastrophe. This paper proposes a novel solution: a
government-provided, mandatory indemnification program for AI developers. The
program uses risk-priced indemnity fees to induce socially optimal levels of
care. Risk-estimates are determined by surveying experts, including indemnified
developers. The Bayesian Truth Serum mechanism is employed to incent honest and
effortful responses. Compared to alternatives, this approach arguably better
leverages all private information, and provides a clearer signal to indemnified
developers regarding what risks they must mitigate to lower their fees. It's
recommended that collected fees be used to help fund the safety research
developers need, employing a fund matching mechanism (Quadratic Financing) to
induce an optimal supply of this public good. Under Quadratic Financing, safety
research projects would compete for private contributions from developers,
signaling how much each is to be supplemented with public funds.

æè¦ï¼è¨±å¤å°å®¶ç¸ä¿¡ AI ç³»çµ±é²æ©æé æç¡æ³æ¿ä¿çé¢¨éªï¼åæ¬çå­é¢¨éªãéæé ææ¥µç«¯çç¡æ³è¿½ç©¶è²¬ä»»åé¡ï¼å¨ç¼çæ­¤é¡ç½é£æï¼å¹¾ä¹æ²æä»»ä½ä¸æ¹å¯ä»¥äºå¾è¢«è¿½ç©¶è²¬ä»»ãæ¬ææåºäºä¸ååµæ°çè§£æ±ºæ¹æ¡ï¼æ¿åºæä¾ç AI éç¼äººå¡å¼·å¶æ§è£åè¨ç«ãè©²è¨ç«ä½¿ç¨é¢¨éªå®å¹çè£åè²»ç¨ä¾èªä½¿éå°ç¤¾ææé©ç¨åº¦çç§è­·ãé¢¨éªä¼°è¨å¼æ¯ç±èª¿æ¥å°å®¶ï¼åæ¬ç²å¾è£åçéç¼äººå¡ï¼ä¾æ±ºå®ãè²æ°çè©±è¡æ¸æ©å¶è¢«ç¨ä¾æ¿åµèª å¯¦ä¸åªåçåæãèå¶ä»æ¹æ³ç¸æ¯ï¼éç¨®æ¹æ³å¯ä»¥èªªè½æ´å¥½å°å©ç¨ææç§äººè³è¨ï¼ä¸¦åç²å¾è£åçéç¼äººå¡æä¾æ´æç¢ºçè¨èï¼èªªæä»åå¿é æ¸è¼åªäºé¢¨éªæè½éä½è²»ç¨ãå»ºè­°å°æ¶åçè²»ç¨ç¨æ¼è³å©å®å¨ç ç©¶éç¼äººå¡æéçç ç©¶ï¼ä¸¦æ¡ç¨åºééå°æ©å¶ï¼äºæ¬¡æ¹èè³ï¼ä¾èªä½¿æä¾éç¨®å¬å±è²¡çæä½³ä¾æãå¨äºæ¬¡æ¹èè³ä¸ï¼å®å¨ç ç©¶è¨ç«å°ç«¶ç­éç¼äººå¡çç§äººææ¬¾ï¼ä¸¦è¡¨ç¤ºå¶ä¸­æå¤å°å°ç±å¬å±è³éè£åã

##### **EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**
2409.06644v2 by Danli Shi, Weiyi Zhang, Jiancheng Yang, Siyu Huang, Xiaolan Chen, Mayinuer Yusufu, Kai Jin, Shan Lin, Shunming Liu, Qing Zhang, Mingguang He

Early detection of eye diseases like glaucoma, macular degeneration, and
diabetic retinopathy is crucial for preventing vision loss. While artificial
intelligence (AI) foundation models hold significant promise for addressing
these challenges, existing ophthalmic foundation models primarily focus on a
single modality, whereas diagnosing eye diseases requires multiple modalities.
A critical yet often overlooked aspect is harnessing the multi-view information
across various modalities for the same patient. Additionally, due to the
long-tail nature of ophthalmic diseases, standard fully supervised or
unsupervised learning approaches often struggle. Therefore, it is essential to
integrate clinical text to capture a broader spectrum of diseases. We propose
EyeCLIP, a visual-language foundation model developed using over 2.77 million
multi-modal ophthalmology images with partial text data. To fully leverage the
large multi-modal unlabeled and labeled data, we introduced a pretraining
strategy that combines self-supervised reconstructions, multi-modal image
contrastive learning, and image-text contrastive learning to learn a shared
representation of multiple modalities. Through evaluation using 14 benchmark
datasets, EyeCLIP can be transferred to a wide range of downstream tasks
involving ocular and systemic diseases, achieving state-of-the-art performance
in disease classification, visual question answering, and cross-modal
retrieval. EyeCLIP represents a significant advancement over previous methods,
especially showcasing few-shot, even zero-shot capabilities in real-world
long-tail scenarios.

æè¦ï¼æ©æåµæ¸¬éåç¼ãé»æé¨çè®åç³å°¿çè¦ç¶²èçè®ç­ç¼ç¾å°æ¼é é²è¦ååªå¤±è³ééè¦ãåç®¡äººå·¥æºæ§ (AI) åºç¤æ¨¡åå¨æå°éäºææ°æ¹é¢æ¥µå·åæ¯ï¼ä½ç¾æçç¼ç§åºç¤æ¨¡åä¸»è¦éæ³¨æ¼å®ä¸æ¨¡å¼ï¼èè¨ºæ·ç¼ç¾éè¦å¤ç¨®æ¨¡å¼ãä¸åéè¦ä½ç¶å¸¸è¢«å¿½è¦çæ¹é¢æ¯å©ç¨åä¸æ£èä¸åæ¨¡å¼çå¤è¦åè³è¨ãæ­¤å¤ï¼ç±æ¼ç¼ç§ç¾ççé·å°¾æ§è³ªï¼æ¨æºçå¨ç£ç£æç¡ç£ç£å­¸ç¿æ¹æ³éå¸¸é£ä»¥æä»ãå æ­¤ï¼æ´åè¨åºææ¬ä»¥æ¶µèæ´å»£æ³çç¾çè­ç³»è³ééè¦ãæåæåº EyeCLIPï¼éæ¯ä¸åè¦è¦ºèªè¨åºç¤æ¨¡åï¼ä½¿ç¨è¶é 277 è¬å¼µå·æé¨åæå­è³æçå¤æ¨¡å¼ç¼ç§å½±åéç¼èæãçºäºååå©ç¨å¤§éçå¤æ¨¡å¼æªæ¨è¨åæ¨è¨è³æï¼æåå¼å¥äºä¸ç¨®é è¨ç·´ç­ç¥ï¼çµåäºèªæç£ç£éå»ºãå¤æ¨¡å¼å½±åå°æ¯å­¸ç¿åå½±åæå­å°æ¯å­¸ç¿ï¼ä»¥å­¸ç¿å¤ç¨®æ¨¡å¼çå±äº«è¡¨å¾µãééä½¿ç¨ 14 ååºæºè³æéé²è¡è©ä¼°ï¼EyeCLIP å¯ä»¥è½ç§»å°æ¶åç¼é¨åå¨èº«ç¾ççå»£æ³ä¸æ¸¸ä»»åï¼å¨ç¾çåé¡ãè¦è¦ºåé¡è§£ç­åè·¨æ¨¡å¼æª¢ç´¢ä¸­å¯¦ç¾æåé²çæè½ãEyeCLIP ä»£è¡¨äºå°ååæ¹æ³çéå¤§é²å±ï¼ç¹å¥æ¯å¨ç¾å¯¦ä¸çé·å°¾å ´æ¯ä¸­å±ç¤ºäºå°æ¨£æ¬ï¼çè³é¶æ¨£æ¬çè½åã

##### **Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**
2409.06585v1 by Zoe Hancox, Sarah R. Kingsbury, Andrew Clegg, Philip G. Conaghan, Samuel D. Relton

Background: Hip replacement procedures improve patient lives by relieving
pain and restoring mobility. Predicting hip replacement in advance could reduce
pain by enabling timely interventions, prioritising individuals for surgery or
rehabilitation, and utilising physiotherapy to potentially delay the need for
joint replacement. This study predicts hip replacement a year in advance to
enhance quality of life and health service efficiency. Methods: Adapting
previous work using Temporal Graph Convolutional Neural Network (TG-CNN)
models, we construct temporal graphs from primary care medical event codes,
sourced from ResearchOne EHRs of 40-75-year-old patients, to predict hip
replacement risk. We match hip replacement cases to controls by age, sex, and
Index of Multiple Deprivation. The model, trained on 9,187 cases and 9,187
controls, predicts hip replacement one year in advance. We validate the model
on two unseen datasets, recalibrating for class imbalance. Additionally, we
conduct an ablation study and compare against four baseline models. Results:
Our best model predicts hip replacement risk one year in advance with an AUROC
of 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209),
achieving a calibration slope of 1.107 (95% CI: 1.074-1.139) after
recalibration. Conclusions: The TG-CNN model effectively predicts hip
replacement risk by identifying patterns in patient trajectories, potentially
improving understanding and management of hip-related conditions.

æè¦ï¼èæ¯ï¼é«éç¯ç½®ææè¡å¯æ¸è¼ç¼çä¸¦æ¢å¾©è¡åè½åï¼é²èæ¹åæ£èçæ´»ãé æ¸¬é«éç¯ç½®ææè¡æå©æ¼åæä»å¥ãåªåå®æåäººé²è¡æè¡æå¾©å¥ï¼ä¸¦å©ç¨ç©çæ²»çä¾å»¶ç·©éç¯ç½®ææè¡çå¿è¦æ§ï¼é²èæ¸å°ç¼çãæ¬ç ç©¶é æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡ï¼ä»¥æåçæ´»åè³ªåé«çæåæçãæ¹æ³ï¼æ¡ç¨æéåå½¢å·ç©ç¥ç¶ç¶²è·¯ (TG-CNN) æ¨¡åæ¹ç·¨ååçç ç©¶ï¼æåå¾ ResearchOne EHR 40-75 æ­²æ£èçä¸»è¦ç§è­·é«çäºä»¶ä»£ç¢¼å»ºæ§æéåå½¢ï¼ä»¥é æ¸¬é«éç¯ç½®ææè¡é¢¨éªãæåæ ¹æå¹´é½¡ãæ§å¥åå¤éåå¥ªææ¸ï¼å°é«éç¯ç½®ææè¡çä¾èå°ç§çµé²è¡éå°ãè©²æ¨¡åéå° 9,187 åçä¾å 9,187 åå°ç§çµé²è¡è¨ç·´ï¼é æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡ãæåå¨å©åæªè¦æ¸æéé©è­æ¨¡åï¼ä¸¦éæ°æ ¡æºä»¥è§£æ±ºé¡å¥ä¸å¹³è¡¡åé¡ãæ­¤å¤ï¼æåé²è¡æ¶èç ç©¶ï¼ä¸¦èåååºæºæ¨¡åé²è¡æ¯è¼ãçµæï¼æåæä½³çæ¨¡åé æ¸¬ä¸å¹´å¾çé«éç¯ç½®ææè¡é¢¨éªï¼AUROC çº 0.724 (95% CIï¼0.715-0.733)ï¼AUPRC çº 0.185 (95% CIï¼0.160-0.209)ï¼éæ°æ ¡æºå¾æ ¡æºæççº 1.107 (95% CIï¼1.074-1.139)ãçµè«ï¼TG-CNN æ¨¡åå¯ææé æ¸¬é«éç¯ç½®ææè¡é¢¨éªï¼æ¹æ³æ¯æ¾åºæ£èè»è·¡ä¸­çæ¨¡å¼ï¼é²èæ½å¨æ¹åå°é«éç¯ç¸éç¾ççäºè§£åç®¡çã

##### **MAGDA: Multi-agent guideline-driven diagnostic assistance**
2409.06351v1 by David Bani-Harouni, Nassir Navab, Matthias Keicher

In emergency departments, rural hospitals, or clinics in less developed
regions, clinicians often lack fast image analysis by trained radiologists,
which can have a detrimental effect on patients' healthcare. Large Language
Models (LLMs) have the potential to alleviate some pressure from these
clinicians by providing insights that can help them in their decision-making.
While these LLMs achieve high test results on medical exams showcasing their
great theoretical medical knowledge, they tend not to follow medical
guidelines. In this work, we introduce a new approach for zero-shot
guideline-driven decision support. We model a system of multiple LLM agents
augmented with a contrastive vision-language model that collaborate to reach a
patient diagnosis. After providing the agents with simple diagnostic
guidelines, they will synthesize prompts and screen the image for findings
following these guidelines. Finally, they provide understandable
chain-of-thought reasoning for their diagnosis, which is then self-refined to
consider inter-dependencies between diseases. As our method is zero-shot, it is
adaptable to settings with rare diseases, where training data is limited, but
expert-crafted disease descriptions are available. We evaluate our method on
two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing
performance improvement over existing zero-shot methods and generalizability to
rare diseases.

æè¦ï¼å¨æ¥è¨ºå®¤ãéæé«é¢ææ¬ ç¼éå°åçè¨ºæï¼è¨åºé«å¸«å¸¸å¸¸ç¼ºä¹åéè¨ç·´çæ¾å°ç§é«å¸«é²è¡å¿«éçå½±ååæï¼éå¯è½æå°çæ£çé«çä¿å¥é æä¸å©å½±é¿ãå¤§åèªè¨æ¨¡å (LLM) ææ½åæ¸è¼éäºè¨åºé«å¸«çä¸äºå£åï¼æ¹æ³æ¯æä¾è¦è§£ï¼åå©ä»åé²è¡æ±ºç­ãåç®¡éäº LLM å¨å±ç¤ºå¶è±å¯ççè«é«å­¸ç¥è­çé«å­¸èè©¦ä¸­ç²å¾äºå¾é«çæ¸¬è©¦çµæï¼ä½å®åå¾å¾ä¸éµå¾ªé«çæåãå¨éé å·¥ä½ä¸­ï¼æåä»ç´¹äºä¸ç¨®æ°çé¶æ¬¡å­¸ç¿æå°æ¹éé©åæ±ºç­æ¯æ´æ¹æ³ãæåæ¨¡æ¬äºä¸åå¤å LLM ä»£çç³»çµ±ï¼ä¸¦å¢å¼·äºä¸åå°æ¯è¦è¦ºèªè¨æ¨¡åï¼è©²æ¨¡ååä½ä»¥éæçæ£è¨ºæ·ãå¨çºä»£çæä¾ç°¡å®çè¨ºæ·æåå¾ï¼å®åå°ç¶åæç¤ºä¸¦æ ¹æéäºæåç¯©é¸å½±åä»¥æ¾åºç¼ç¾ãæå¾ï¼å®åçºå¶è¨ºæ·æä¾å¯ä»¥çè§£çæè·¯æ¨çï¼ç¶å¾èªæç²¾é²ä»¥èéç¾çä¹éçç¸äºä¾å­éä¿ãç±æ¼æåçæ¨¡åæ¯é¶æ¬¡å­¸ç¿ï¼å æ­¤å®å¯ä»¥é©æç½è¦ç¾ççè¨­å®ï¼å¨éç¨®è¨­å®ä¸­ï¼è¨ç·´è³ææéï¼ä½æå°å®¶è£½ä½çç¾çæè¿°å¯ç¨ãæåå¨å©åè¸é¨ X åçè³æéï¼CheXpert å ChestX-ray 14 Longtailï¼è©ä¼°æåçæ¨¡åï¼å±ç¤ºäºç¸è¼æ¼ç¾æçé¶æ¬¡å­¸ç¿æ¹æ³çæè½æåï¼ä»¥åå°ç½è¦ç¾ççæ¦æ¬æ§ã

##### **Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**
2409.06209v1 by Xin Zhang, Deval Mehta, Yanan Hu, Chao Zhu, David Darby, Zhen Yu, Daniel Merlo, Melissa Gresle, Anneke Van Der Walt, Helmut Butzkueven, Zongyuan Ge

Survival analysis holds a crucial role across diverse disciplines, such as
economics, engineering and healthcare. It empowers researchers to analyze both
time-invariant and time-varying data, encompassing phenomena like customer
churn, material degradation and various medical outcomes. Given the complexity
and heterogeneity of such data, recent endeavors have demonstrated successful
integration of deep learning methodologies to address limitations in
conventional statistical approaches. However, current methods typically involve
cluttered probability distribution function (PDF), have lower sensitivity in
censoring prediction, only model static datasets, or only rely on recurrent
neural networks for dynamic modelling. In this paper, we propose a novel
survival regression method capable of producing high-quality unimodal PDFs
without any prior distribution assumption, by optimizing novel
Margin-Mean-Variance loss and leveraging the flexibility of Transformer to
handle both temporal and non-temporal data, coined UniSurv. Extensive
experiments on several datasets demonstrate that UniSurv places a significantly
higher emphasis on censoring compared to other methods.

æè¦ï¼å­æ´»åæå¨ç¶æ¿ãå·¥ç¨åé«çä¿å¥ç­ä¸åå­¸ç§ä¸­æ®æ¼èè³ééè¦çè§è²ãå®è®ç ç©¶äººå¡è½å¤ åææä¸è®åæè®æ¸æï¼åå«å®¢æ¶æµå¤±ãææéè§£ååç¨®é«ççµæç­ç¾è±¡ãéæ¼æ­¤é¡æ¸æçè¤éæ§åç°è³ªæ§ï¼æè¿çåªåå·²è­ææåæ´åæ·±åº¦å­¸ç¿æ¹æ³ä»¥è§£æ±ºå³çµ±çµ±è¨æ¹æ³çéå¶ãç¶èï¼ç®åçæ¹æ³éå¸¸æ¶åéäºçæ©çåä½å½æ¸ (PDF)ï¼å¨å¯©æ¥é æ¸¬ä¸­å·æè¼ä½çæææ§ï¼åå°éææ¸æéé²è¡å»ºæ¨¡ï¼æåä¾è³´éè¿´ç¥ç¶ç¶²è·¯é²è¡åæå»ºæ¨¡ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°ç©çå­æ´»è¿´æ­¸æ¹æ³ï¼è½å¤ å¨æ²æä»»ä½åé©åä½åè¨­çææ³ä¸ç¢çé«åè³ªçå®å³° PDFï¼èç±æä½³åæ°ç©çééå¹³åå¼è®ç°æå¤±ï¼ä¸¦å©ç¨ Transformer çéæ´»æ§ä¾èçæéåéæéæ¸æï¼ç¨±çº UniSurvãå¨å¹¾åæ¸æéä¸çå»£æ³å¯¦é©è­æï¼èå¶ä»æ¹æ³ç¸æ¯ï¼UniSurv å°å¯©æ¥çéè¦ç¨åº¦é¡¯èæé«ã

##### **Can Large Language Models Unlock Novel Scientific Research Ideas?**
2409.06185v1 by Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal

"An idea is nothing more nor less than a new combination of old elements"
(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and
publicly available ChatGPT have marked a significant turning point in the
integration of Artificial Intelligence (AI) into people's everyday lives. This
study explores the capability of LLMs in generating novel research ideas based
on information from research papers. We conduct a thorough examination of 4
LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and
Physics). We found that the future research ideas generated by Claude-2 and
GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.
We also found that Claude-2 generates more diverse future research ideas than
GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the
novelty, relevancy, and feasibility of the generated future research ideas.
This investigation offers insights into the evolving role of LLMs in idea
generation, highlighting both its capability and limitations. Our work
contributes to the ongoing efforts in evaluating and utilizing language models
for generating future research ideas. We make our datasets and codes publicly
available.

æè¦ï¼ãä¸åæ³æ³ä¸éå°±æ¯èåç´ çæ°çµåèå·²ã
(Young, J.W.)ãå¤§åèªè¨æ¨¡å (LLM) åå¬éç ChatGPT å»£æ³æ¡ç¨ï¼æ¨èªèäººå·¥æºè½ (AI) æ´åå°äººåæ¥å¸¸çæ´»ä¸­çéè¦è½æé»ãæ¬ç ç©¶æ¢è¨äº LLM å¨æ ¹æç ç©¶è«æè³è¨ç¢çæ°ç ç©¶æ³æ³æ¹é¢çè½åãæåå°äºåé åï¼ä¾å¦åå­¸ãé»è¦ãç¶æ¿ãé«å­¸åç©çï¼ä¸­ç 4 å LLM é²è¡äºå¾¹åºæª¢æ¥ãæåç¼ç¾ Claude-2 å GPT-4 ç¢ççæªä¾ç ç©¶æ³æ³æ¯ GPT-3.5 å Gemini æ´ç¬¦åä½èçè§é»ãæåéç¼ç¾ï¼Claude-2 ç¢ççæªä¾ç ç©¶æ³æ³æ¯ GPT-4ãGPT-3.5 å Gemini 1.0 æ´çºå¤æ¨£åãæåé²ä¸æ­¥å°ç¢ççæªä¾ç ç©¶æ³æ³çæ°ç©æ§ãç¸éæ§åå¯è¡æ§é²è¡äºäººå·¥è©ä¼°ãæ¬èª¿æ¥æä¾äºå° LLM å¨ç¢çæ³æ³ä¸­ä¸æ·æ¼è®çè§è²çè¦è§£ï¼çªåºäºå¶è½ååéå¶ãæåçç ç©¶æå©æ¼è©ä¼°åå©ç¨èªè¨æ¨¡åä¾ç¢çæªä¾ç ç©¶æ³æ³çæçºåªåãæåå¬éæä¾æåçæ¸æéåç¨å¼ç¢¼ã

##### **Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**
2409.06173v1 by Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan

In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the
dominant technique for performing natural language tasks, as it does not
require updating the model parameters with gradient-based methods. ICL promises
to "adapt" the LLM to perform the present task at a competitive or
state-of-the-art level at a fraction of the computational cost. ICL can be
augmented by incorporating the reasoning process to arrive at the final label
explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.
However, recent work has found that ICL relies mostly on the retrieval of task
priors and less so on "learning" to perform tasks, especially for complex
subjective domains like emotion and morality, where priors ossify posterior
predictions. In this work, we examine whether "enabling" reasoning also creates
the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors
that remain relatively unchanged despite the evidence in the prompt. We find
that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL
for larger language models. Code is avalaible at
https://github.com/gchochla/cot-priors.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ä¸­çèªå¢å­¸ç¿ (ICL) å·²æçºå·è¡èªç¶èªè¨ä»»åçä¸»æµæè¡ï¼å çºå®ä¸éè¦ä½¿ç¨åºæ¼æ¢¯åº¦çæ¨¡ååæ¸ä¾æ´æ°ãICL æ¿è«¾ä»¥æ¥µä½çè¨ç®ææ¬ãèª¿æ´ãLLMï¼ä»¥å¨ç«¶ç­ææåé²çå±¤ç´å·è¡ç¶åä»»åãICL å¯ä»¥ééå¨æç¤ºä¸­æç¢ºå°ç´å¥æ¨çéç¨ä¾æ´åï¼ä»¥å¾åºæçµæ¨ç±¤ï¼éé æè¡ç¨±çºæèé (CoT) æç¤ºãç¶èï¼æè¿çç ç©¶ç¼ç¾ï¼ICL ä¸»è¦ä¾è³´æ¼ä»»ååé©çæª¢ç´¢ï¼è¼å°ä¾è³´æ¼ãå­¸ç¿ãå·è¡ä»»åï¼ç¹å¥æ¯å°æ¼æç·åéå¾·ç­è¤éçä¸»è§é åï¼å¶ä¸­åé©æåµåå¾é©é æ¸¬ãå¨éé ç ç©¶ä¸­ï¼æåæ¢è¨ãåç¨ãæ¨çæ¯å¦ä¹æå¨ LLM ä¸­ç¢çç¸åçè¡çºï¼å¶ä¸­ CoT çæ ¼å¼ææª¢ç´¢æ¨çåé©ï¼åç®¡æç¤ºä¸­çè­æä¸åï¼ä½éäºåé©ä»ç¶ç¸å°ä¸è®ãæåç¼ç¾ï¼ä»¤äººé©è¨çæ¯ï¼å°æ¼è¼å¤§çèªè¨æ¨¡åï¼CoT ç¢ºå¯¦æè ICL ç¼çç¸åçå¾é©å´©æ½°ãç¨å¼ç¢¼å¯æ¼ https://github.com/gchochla/cot-priors åå¾ã

##### **Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**
2409.06147v1 by Dong Han, Jihye Moon, LuÃ­s Roberto Mercado DÃ­az, Darren Chen, Devan Williams, Eric Y. Ding, Khanh-Van Tran, David D. McManus, Ki H. Chon

Most deep learning models of multiclass arrhythmia classification are tested
on fingertip photoplethysmographic (PPG) data, which has higher signal-to-noise
ratios compared to smartwatch-derived PPG, and the best reported sensitivity
value for premature atrial/ventricular contraction (PAC/PVC) detection is only
75%. To improve upon PAC/PVC detection sensitivity while maintaining high AF
detection, we use multi-modal data which incorporates 1D PPG, accelerometers,
and heart rate data as the inputs to a computationally efficient 1D
bi-directional Gated Recurrent Unit (1D-Bi-GRU) model to detect three
arrhythmia classes. We used motion-artifact prone smartwatch PPG data from the
NIH-funded Pulsewatch clinical trial. Our multimodal model tested on 72
subjects achieved an unprecedented 83% sensitivity for PAC/PVC detection while
maintaining a high accuracy of 97.31% for AF detection. These results
outperformed the best state-of-the-art model by 20.81% for PAC/PVC and 2.55%
for AF detection even while our model was computationally more efficient (14
times lighter and 2.7 faster).

æè¦ï¼å¤§å¤æ¸å¤é¡å¿å¾ä¸æ´åé¡çæ·±åº¦å­¸ç¿æ¨¡åé½æ¯å¨æå°åé»å®¹ç©æè¨æ³ (PPG) è³æä¸é²è¡æ¸¬è©¦ï¼èæºæ§æé¶è¡çç PPG ç¸æ¯ï¼å¶è¨èéè¨æ¯æ´é«ï¼èå°æ¼æåå¿æ¿/å¿å®¤æ¶ç¸® (PAC/PVC) åµæ¸¬æå ±åçæä½³ææåº¦å¼åçº 75%ãçºäºå¨ç¶­æé«æ¿é¡«åµæ¸¬çåææé« PAC/PVC åµæ¸¬ææåº¦ï¼æåä½¿ç¨å¤æ¨¡å¼è³æï¼å° 1D PPGãå éåº¦è¨åå¿çè³æä½çºè¨ç®æçé«ç 1D éåéæ§éè¿´å®å (1D-Bi-GRU) æ¨¡åçè¼¸å¥ï¼ä»¥åµæ¸¬ä¸é¡å¿å¾ä¸æ´ãæåä½¿ç¨äºç¾ååå®¶è¡çç ç©¶é¢è³å©ç Pulsewatch è¨åºè©¦é©ä¸­çéåå½å½±æææºæ§æé¶ PPG è³æãæåå¨ 72 ååè©¦èèº«ä¸æ¸¬è©¦çå¤æ¨¡å¼æ¨¡åï¼å°æ¼ PAC/PVC åµæ¸¬éå°äºåææªæç 83% ææåº¦ï¼åæå°æ¼æ¿é¡«åµæ¸¬ç¶­æäº 97.31% çé«æºç¢ºåº¦ãå³ä½¿æåçæ¨¡åå¨è¨ç®ä¸æ´ææçï¼è¼ 14 åï¼å¿« 2.7 åï¼ï¼éäºçµæä»æ¯æåé²çæ¨¡åå¨ PAC/PVC åµæ¸¬ä¸é«åº 20.81%ï¼å¨æ¿é¡«åµæ¸¬ä¸é«åº 2.55%ã

##### **ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**
2409.05592v1 by Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, Yulan He

Predicting unknown drug-drug interactions (DDIs) is crucial for improving
medication safety. Previous efforts in DDI prediction have typically focused on
binary classification or predicting DDI categories, with the absence of
explanatory insights that could enhance trust in these predictions. In this
work, we propose to generate natural language explanations for DDI predictions,
enabling the model to reveal the underlying pharmacodynamics and
pharmacokinetics mechanisms simultaneously as making the prediction. To do
this, we have collected DDI explanations from DDInter and DrugBank and
developed various models for extensive experiments and analysis. Our models can
provide accurate explanations for unknown DDIs between known drugs. This paper
contributes new tools to the field of DDI prediction and lays a solid
foundation for further research on generating explanations for DDI predictions.

æè¦ï¼é æ¸¬æªç¥çè¥ç©äº¤äºä½ç¨ (DDI) å°æ¼æ¹åè¥ç©å®å¨è³ééè¦ãååå¨ DDI é æ¸¬æ¹é¢æåçåªåéå¸¸éä¸­æ¼äºååé¡æé æ¸¬ DDI é¡å¥ï¼èç¼ºä¹è½å¤ å¢å¼·éäºé æ¸¬çå¯ä¿¡åº¦çè§£éæ§è¦è§£ãå¨éé å·¥ä½ä¸­ï¼æåå»ºè­°çº DDI é æ¸¬ç¢çèªç¶èªè¨è§£éï¼ä½¿æ¨¡åè½å¤ åææ­ç¤ºè¥æå­¸åè¥ç©ååå­¸æ©å¶ï¼ä¸¦é²è¡é æ¸¬ãçºæ­¤ï¼æåå¾ DDInter å DrugBank æ¶éäº DDI è§£éï¼ä¸¦éç¼äºåç¨®æ¨¡åé²è¡å»£æ³çå¯¦é©ååæãæåçæ¨¡åå¯ä»¥çºå·²ç¥è¥ç©ä¹éæªç¥ç DDI æä¾æºç¢ºçè§£éãæ¬æçº DDI é æ¸¬é åè²¢ç»äºæ°çå·¥å·ï¼ä¸¦çºé²ä¸æ­¥ç ç©¶ DDI é æ¸¬çè§£éçæå¥ å®äºå å¯¦çåºç¤ã

##### **Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**
2409.05486v1 by Camilo Thorne, Christian Druckenbrodt, Kinga Szarkowska, Deepika Goyal, Pranita Marajan, Vijay Somanath, Corey Harper, Mao Yan, Tony Scerri

The quality and capabilities of large language models cannot be currently
fully assessed with automated, benchmark evaluations. Instead, human
evaluations that expand on traditional qualitative techniques from natural
language generation literature are required. One recent best-practice consists
in using A/B-testing frameworks, which capture preferences of human evaluators
for specific models. In this paper we describe a human evaluation experiment
focused on the biomedical domain (health, biology, chemistry/pharmacology)
carried out at Elsevier. In it a large but not massive (8.8B parameter)
decoder-only foundational transformer trained on a relatively small (135B
tokens) but highly curated collection of Elsevier datasets is compared to
OpenAI's GPT-3.5-turbo and Meta's foundational 7B parameter Llama 2 model
against multiple criteria. Results indicate -- even if IRR scores were
generally low -- a preference towards GPT-3.5-turbo, and hence towards models
that possess conversational abilities, are very large and were trained on very
large datasets. But at the same time, indicate that for less massive models
training on smaller but well-curated training sets can potentially give rise to
viable alternatives in the biomedical domain.

æè¦ï¼å¤§åèªè¨æ¨¡åçåè³ªåè½åç®åç¡æ³ä½¿ç¨èªåååºæºè©ä¼°ä¾å®å¨è©ä¼°ãç¸åï¼éè¦ä½¿ç¨æ´å±èªç¶èªè¨çææç»ä¸­å³çµ±å®æ§æè¡çäººé¡è©ä¼°ãä¸åæè¿çæä½³å¯¦åæ¯ä½¿ç¨ A/B æ¸¬è©¦æ¡æ¶ï¼è©²æ¡æ¶ææ·åäººé¡è©ä¼°èå°ç¹å®æ¨¡åçåå¥½ãå¨æ¬æä¸­ï¼æåæè¿°äºå°æ³¨æ¼çç©é«å­¸é åï¼å¥åº·ãçç©å­¸ãåå­¸/è¥çå­¸ï¼çäººé¡è©ä¼°å¯¦é©ï¼è©²å¯¦é©å¨ Elsevier é²è¡ãå¶ä¸­ï¼ä¸åå¤§åä½ä¸¦éé¾å¤§ï¼8.8B åæ¸ï¼åè§£ç¢¼å¨åºç¤Transformerå¨ç¸å°è¼å°ï¼135B ä»¤çï¼ä½ç¶éé«åº¦ç­å±ç Elsevier è³æéä¸è¨ç·´ï¼è OpenAI ç GPT-3.5-turbo å Meta çåºç¤ 7B åæ¸ Llama 2 æ¨¡åé²è¡æ¯è¼ï¼éå°å¤åæ¨æºãçµæè¡¨æââå³ä½¿ IRR åæ¸æ®éè¼ä½ââåå¥½ GPT-3.5-turboï¼å æ­¤åå¥½å·åå°è©±è½åãéå¸¸é¾å¤§ä¸å¨éå¸¸é¾å¤§çè³æéä¸è¨ç·´çæ¨¡åãä½åæï¼è¡¨æå°æ¼è¼å°è¦æ¨¡çæ¨¡åï¼å¨è¼å°ä½ç¶éè¯å¥½ç­å±çè¨ç·´éä¸è¨ç·´ï¼æå¯è½å¨çç©é«å­¸é åç¢çå¯è¡çæ¿ä»£æ¹æ¡ã

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

æè¦ï¼<paragraph>å©ç¨å¤§åèªè¨æ¨¡å (LLM) å¼·å¤§çåè½ï¼é²è¡æäºçæãéè¼¯æ¨çåå¸¸è­ç¥è­æ´åï¼æ¬ç ç©¶æ·±å¥æ¢è¨å©ç¨ LLM ä¾å¢å¼·èªååæ¾å°å ±åçæ (R2Gen)ãåç®¡ LLM ææè±å¯çç¥è­ï¼ä½è¦ææè§¸ç¼éäºå¤§åæ¨¡åä¸­èç¹å®ä»»åï¼å¦ R2Genï¼ç¸éçç¥è­ï¼æ¯ä¸åéè¦çç ç©¶ææ°ãæ¬ææåºäº KARGENï¼ä¸ååºæ¼ LLM çç¥è­å¢å¼·èªååæ¾å°å ±åçææ¡æ¶ãå©ç¨åçµç LLM ä¾çæå ±åï¼è©²æ¡æ¶æ´åäºä¸åç¥è­åè­ï¼ä»¥è§£é LLM ä¸­èè¸é¨ç¾çç¸éçç¥è­ï¼ä»¥å¢å¼·çæå ±åçè¨åºæç¨ãéæ¯ééå©ç¨ç¥è­åè­ä»¥è¨­è¨çæ¹å¼æåèç¾çç¸éçç¹å¾µä¾å¯¦ç¾çãç±æ¼æ¾å°å ±ååå«æ­£å¸¸åç¾çç¸éçç¼ç¾ï¼å æ­¤æåçåå½¢å¢å¼·ç¾çç¸éç¹å¾µèååå½±åç¹å¾µæ´åï¼å¼é¡§å©åæ¹é¢ãæåæ¢ç´¢äºå©ç¨®èåæ¹æ³ï¼ä»¥èªååªåæåºåé¸ææç¸éçç¹å¾µãèåçç¹å¾µç± LLM ä½¿ç¨ï¼ä»¥çæå°ç¾çæ´ææä¸åè³ªæ´é«çå ±åãæåçåæ³å¨ MIMIC-CXR å IU-Xray è³æéä¸å±ç¤ºäºæå¸æççµæã</paragraph>

##### **Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**
2409.07493v1 by Javad Hassannataj Joloudari, Mohammad Maftoun, Bahareh Nakisa, Roohallah Alizadehsani, Meisam Yadollahzadeh-Tabari

The Complex Emotion Recognition System (CERS) deciphers complex emotional
states by examining combinations of basic emotions expressed, their
interconnections, and the dynamic variations. Through the utilization of
advanced algorithms, CERS provides profound insights into emotional dynamics,
facilitating a nuanced understanding and customized responses. The attainment
of such a level of emotional recognition in machines necessitates the knowledge
distillation and the comprehension of novel concepts akin to human cognition.
The development of AI systems for discerning complex emotions poses a
substantial challenge with significant implications for affective computing.
Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task
due to the intricacies involved in capturing subtle emotions, necessitating
specialized methods for data collection and processing. Incorporating
physiological signals such as Electrocardiogram (ECG) and Electroencephalogram
(EEG) can notably enhance CERS by furnishing valuable insights into the user's
emotional state, enhancing the quality of datasets, and fortifying system
dependability. A comprehensive literature review was conducted in this study to
assess the efficacy of machine learning, deep learning, and meta-learning
approaches in both basic and complex emotion recognition utilizing EEG, ECG
signals, and facial expression datasets. The chosen research papers offer
perspectives on potential applications, clinical implications, and results of
CERSs, with the objective of promoting their acceptance and integration into
clinical decision-making processes. This study highlights research gaps and
challenges in understanding CERSs, encouraging further investigation by
relevant studies and organizations. Lastly, the significance of meta-learning
approaches in improving CERS performance and guiding future research endeavors
is underscored.

æè¦ï¼è¤éæç·è¾¨è­ç³»çµ± (CERS) ééæª¢é©è¡¨éçåºæ¬æç·çµåãå®åçç¸äºé£çµï¼ä»¥ååæè®åä¾è§£ç¢¼è¤éçæç·çæãééä½¿ç¨é²éæ¼ç®æ³ï¼CERS æä¾äºå°æç·åæçæ·±å¥è¦è§£ï¼ä¿é²ç´°ç·»ççè§£åå®¢è£½åçåæãå¨æ©å¨ä¸­éæéç¨®ç¨åº¦çæç·è¾¨è­éè¦ç¥è­æçåçè§£é¡ä¼¼æ¼äººé¡èªç¥çæ°æ¦å¿µãç¼å±ç¨æ¼è¾¨å¥è¤éæç·çäººå·¥æºæ§ç³»çµ±å°ææéç®ä¾èªªæ¯ä¸åéå¤§çææ°ï¼ä¸¦å·æéè¦çå½±é¿ãæ­¤å¤ï¼ç±æ¼ææå¾®å¦æç·ææ¶åçè¤éæ§ï¼åå¾ CERS çå¤§éè³æéè¢«è­ææ¯ä¸é è±éçä»»åï¼å æ­¤éè¦æ¡ç¨ç¹æ®çæ¹æ³ä¾æ¶éåèçè³æãç´å¥ççè¨èï¼ä¾å¦å¿é»å (ECG) åè¦é»å (EEG)ï¼å¯ä»¥ééæä¾å°ä½¿ç¨èæç·çæçå¯¶è²´è¦è§£ãæåè³æéçåè³ªä»¥åå¼·åç³»çµ±çå¯é æ§ï¼ä¾é¡¯èå¢å¼· CERSãæ¬ç ç©¶é²è¡äºä¸é å¨é¢çæç»æ¢è¨ï¼ä»¥è©ä¼°æ©å¨å­¸ç¿ãæ·±åº¦å­¸ç¿ååå­¸ç¿æ¹æ³å¨å©ç¨è¦é»åãå¿é»åè¨èåé¢é¨è¡¨æè³æéé²è¡åºæ¬åè¤éæç·è¾¨è­æ¹é¢çæè½ãæé¸çç ç©¶è«ææä¾äºéæ¼ CERS çæ½å¨æç¨ãè¨åºå½±é¿åçµæçè§é»ï¼ç®çæ¯ä¿é²å®åè¢«æ¥åä¸¦æ´åå°è¨åºæ±ºç­å¶å®éç¨ä¸­ãæ¬ç ç©¶çªåºäºçè§£ CERS çç ç©¶å·®è·åææ°ï¼é¼åµç¸éç ç©¶åçµç¹é²ä¸æ­¥èª¿æ¥ãæå¾ï¼å¼·èª¿äºåå­¸ç¿æ¹æ³å¨æ¹å CERS æè½åæå°æªä¾ç ç©¶å·¥ä½ä¸­çéè¦æ§ã

##### **Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**
2409.05292v2 by Nirmalya Thakur

The world is currently experiencing an outbreak of mpox, which has been
declared a Public Health Emergency of International Concern by WHO. No prior
work related to social media mining has focused on the development of a dataset
of Instagram posts about the mpox outbreak. The work presented in this paper
aims to address this research gap and makes two scientific contributions to
this field. First, it presents a multilingual dataset of 60,127 Instagram posts
about mpox, published between July 23, 2022, and September 5, 2024. The
dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram
posts about mpox in 52 languages. For each of these posts, the Post ID, Post
Description, Date of publication, language, and translated version of the post
(translation to English was performed using the Google Translate API) are
presented as separate attributes in the dataset. After developing this dataset,
sentiment analysis, hate speech detection, and anxiety or stress detection were
performed. This process included classifying each post into (i) one of the
sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or
neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no
anxiety/stress detected. These results are presented as separate attributes in
the dataset. Second, this paper presents the results of performing sentiment
analysis, hate speech analysis, and anxiety or stress analysis. The variation
of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and
neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and
50.64%, respectively. In terms of hate speech detection, 95.75% of the posts
did not contain hate and the remaining 4.25% of the posts contained hate.
Finally, 72.05% of the posts did not indicate any anxiety/stress, and the
remaining 27.95% of the posts represented some form of anxiety/stress.

æè¦ï¼ä¸çç®åæ­£å¨ç»åç´çç«æï¼ä¸çå«çç»ç»å·²å®£å¸ç´çç«æä¸ºå½éå³æ³¨ççªåå¬å±å«çäºä»¶ãæ­¤åæ²¡æä¸ç¤¾äº¤åªä½ææç¸å³çç ç©¶éä¸­äºå¼åæå³ç´çç«æç Instagram å¸å­çæ°æ®éãæ¬æä»ç»çç ç©¶æ¨å¨è§£å³è¿ä¸ç ç©¶ç©ºç½ï¼å¹¶å¯¹è¯¥é¢åååºä¸¤é¡¹ç§å­¦è´¡ç®ãé¦åï¼å®æä¾äº 60,127 æ¡æå³ç´çç Instagram å¸å­çå¤è¯­è¨æ°æ®éï¼è¿äºå¸å­åå¸äº 2022 å¹´ 7 æ 23 æ¥è³ 2024 å¹´ 9 æ 5 æ¥ä¹é´ãè¯¥æ°æ®éå¯å¨ https://dx.doi.org/10.21227/7fvc-y093 å¤è·å¾ï¼å¶ä¸­åå« 52 ç§è¯­è¨çæå³ç´çç Instagram å¸å­ãå¯¹äºå¶ä¸­æ¯ç¯å¸å­ï¼å¸å­ IDãå¸å­æè¿°ãåå¸æ¥æãè¯­è¨åå¸å­çç¿»è¯çæ¬ï¼ä½¿ç¨ Google ç¿»è¯ API ç¿»è¯æè±æï¼ä½ä¸ºåç¬çå±æ§æ¾ç¤ºå¨æ°æ®éä¸­ãå¨å¼åæ­¤æ°æ®éåï¼è¿è¡äºææåæãä»æ¨è¨è®ºæ£æµä»¥åç¦èæååæ£æµãæ­¤è¿ç¨åæ¬å°æ¯ç¯å¸å­åç±»ä¸º (i) ææç±»å«ä¹ä¸ï¼å³ææ§ãæè®¶ãå¿«ä¹ãæ²ä¼¤ãæ¤æãåæ¶æä¸­ç«ï¼(ii) ä»æ¨æéä»æ¨ï¼ä»¥å (iii) æ£æµå°ç¦è/ååææªæ£æµå°ç¦è/ååãè¿äºç»æä½ä¸ºåç¬çå±æ§æ¾ç¤ºå¨æ°æ®éä¸­ãå¶æ¬¡ï¼æ¬æä»ç»äºæ§è¡ææåæãä»æ¨è¨è®ºåæåç¦èæåååæçç»æãè§å¯å°ææç±»å«çååââææ§ãæè®¶ãå¿«ä¹ãæ²ä¼¤ãæ¤æãåæ¶åä¸­ç«åå«ä¸º 27.95%ã2.57%ã8.69%ã5.94%ã2.69%ã1.53% å 50.64%ãå¨ä»æ¨è¨è®ºæ£æµæ¹é¢ï¼95.75% çå¸å­ä¸åå«ä»æ¨ï¼å¶ä½ 4.25% çå¸å­åå«ä»æ¨ãæåï¼72.05% çå¸å­æ²¡æè¡¨ç°åºä»»ä½ç¦è/ååï¼å¶ä½ 27.95% çå¸å­ä»£è¡¨æç§å½¢å¼çç¦è/ååã

##### **RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**
2409.05280v1 by Quoc-Bao Nguyen-Le, Tuan-Hy Le, Anh-Triet Do, Quoc-Huy Trinh

Cardiovascular disease is a major global health concern, contributing
significantly to global mortality. Accurately segmenting cardiac medical
imaging data is crucial for reducing fatality rates associated with these
conditions. However, current state-of-the-art (SOTA) neural networks, including
CNN-based and Transformer-based approaches, face challenges in capturing both
inter-slice connections and intra-slice details, especially in datasets
featuring intricate, long-range details along the z-axis like coronary
arteries. Existing methods also struggle with differentiating non-cardiac
components from the myocardium, resulting in segmentation inaccuracies and the
"spraying" phenomenon. To address these issues, we introduce
RotCAtt-TransUNet++, a novel architecture designed for robust segmentation of
intricate cardiac structures. Our approach enhances global context modeling
through multiscale feature aggregation and nested skip connections in the
encoder. Transformer layers facilitate capturing intra-slice interactions,
while a rotatory attention mechanism handles inter-slice connectivity. A
channel-wise cross-attention gate integrates multiscale information and decoder
features, effectively bridging semantic gaps. Experimental results across
multiple datasets demonstrate superior performance over current methods,
achieving near-perfect annotation of coronary arteries and myocardium. Ablation
studies confirm that our rotatory attention mechanism significantly improves
segmentation accuracy by transforming embedded vectorized patches in semantic
dimensional space.

æè¦ï¼å¿è¡ç®¡ç¾çæ¯å¨çä¸»è¦çå¥åº·åé¡ï¼å°å¨çæ­»äº¡çæé¡¯èçå½±é¿ãæºç¢ºåå²å¿èé«å­¸å½±åè³æå°æ¼éä½éäºç¾çç¸éçæ­»äº¡çè³ééè¦ãç¶èï¼ç®åçåé²ç¥ç¶ç¶²è·¯ï¼åæ¬åºæ¼ CNN ååºæ¼ Transformer çæ¹æ³ï¼å¨æ·åå±¤éé£æ¥åå±¤å§ç´°ç¯æ¹é¢é¢è¨ææ°ï¼ç¹å¥æ¯å¨å·ææ²¿è z è»¸çè¤éãé·ç¨ç´°ç¯çè³æéï¼ä¾å¦å çåèãç¾ææ¹æ³ä¹é£ä»¥ååéå¿èæååå¿èï¼å°è´åå²ä¸æºç¢ºåãå´çãç¾è±¡ãçºäºè§£æ±ºéäºåé¡ï¼æåå¼å¥äº RotCAtt-TransUNet++ï¼ä¸ç¨®å°çºè¤éå¿èçµæ§çç©©å¥åå²èè¨­è¨çæ°ç©æ¶æ§ãæåçåæ³ééç·¨ç¢¼å¨ä¸­çå¤å°ºåº¦ç¹å¾µèååå·¢çè·³èºé£æ¥å¢å¼·äºå¨å±èæ¯å»ºæ¨¡ãTransformer å±¤ä¿é²æ·åå±¤å§äº¤äºä½ç¨ï¼èæè½æ³¨ææ©å¶åèçå±¤éé£æ¥ãééå¼äº¤åæ³¨æééæ´åäºå¤å°ºåº¦è³è¨åè§£ç¢¼å¨ç¹å¾µï¼ææå°å½åäºèªç¾©å·®è·ãè·¨å¤åè³æéçå¯¦é©çµæè­æäºå¶åªæ¼ç®åæ¹æ³çæè½ï¼å¯¦ç¾äºå çåèåå¿èçè¿ä¹å®ç¾çè¨»è§£ãæ¶èç ç©¶è­å¯¦ï¼æåçæè½æ³¨ææ©å¶ééè½æèªç¾©ç¶­åº¦ç©ºéä¸­çåµå¥åéåè£ä¸ï¼é¡¯èå°æé«äºåå²æºç¢ºåº¦ã

##### **Activation Function Optimization Scheme for Image Classification**
2409.04915v1 by Abdur Rahman, Lu He, Haifeng Wang

Activation function has a significant impact on the dynamics, convergence,
and performance of deep neural networks. The search for a consistent and
high-performing activation function has always been a pursuit during deep
learning model development. Existing state-of-the-art activation functions are
manually designed with human expertise except for Swish. Swish was developed
using a reinforcement learning-based search strategy. In this study, we propose
an evolutionary approach for optimizing activation functions specifically for
image classification tasks, aiming to discover functions that outperform
current state-of-the-art options. Through this optimization framework, we
obtain a series of high-performing activation functions denoted as Exponential
Error Linear Unit (EELU). The developed activation functions are evaluated for
image classification tasks from two perspectives: (1) five state-of-the-art
neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and
Compact Convolutional Transformer which cover computationally heavy to light
neural networks, and (2) eight standard datasets, including CIFAR10,
Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15,
and TinyImageNet which cover from typical machine vision benchmark,
agricultural image applications to medical image applications. Finally, we
statistically investigate the generalization of the resultant activation
functions developed through the optimization scheme. With a Friedman test, we
conclude that the optimization scheme is able to generate activation functions
that outperform the existing standard ones in 92.8% cases among 28 different
cases studied, and $-x\cdot erf(e^{-x})$ is found to be the best activation
function for image classification generated by the optimization scheme.

æè¦ï¼<paragraph>æ¿æ´»å½æ¸å°æ·±åº¦ç¥ç¶ç¶²è·¯çåæãæ¶æåæè½æé¡¯èçå½±é¿ãå¨æ·±åº¦å­¸ç¿æ¨¡åéç¼éç¨ä¸­ï¼ä¸ç´è´åæ¼å°æ¾ä¸è´ä¸æè½é«çæ¿æ´»å½æ¸ãç¾æçæåé²æ¿æ´»å½æ¸ï¼é¤äº Swish ä¹å¤ï¼é½æ¯ç±äººé¡å°å®¶æåè¨­è¨çãSwish æ¯ä½¿ç¨åºæ¼å¼·åå­¸ç¿çæå°ç­ç¥éç¼çãå¨æ¬ç ç©¶ä¸­ï¼æåæåºäºä¸ç¨®æ¼åæ¹æ³ï¼å°ééå°åååé¡ä»»åæä½³åæ¿æ´»å½æ¸ï¼æ¨å¨ç¼ç¾æè½åªæ¼ç¾ææåé²é¸é çå½æ¸ãéééåæä½³åæ¶æ§ï¼æåç²å¾äºä¸ç³»åæè½é«çæ¿æ´»å½æ¸ï¼è¡¨ç¤ºçºææ¸èª¤å·®ç·æ§å®å (EELU)ãå·²éå°å©åè§é»è©ä¼°å·²éç¼çæ¿æ´»å½æ¸ï¼ç¨æ¼åååé¡ä»»åï¼(1) äºç¨®æåé²çç¥ç¶ç¶²è·¯æ¶æ§ï¼ä¾å¦ ResNet50ãAlexNetãVGG16ãMobileNet å Compact Convolutional Transformerï¼æ¶µèå¾è¨ç®ééçå°è¼éçç¶²è·¯ï¼(2) å«åæ¨æºè³æéï¼åæ¬ CIFAR10ãImagenetteãMNISTãFashion MNISTãBeansãColorectal HistologyãCottonWeedID15 å TinyImageNetï¼æ¶µèå¾å¸åçæ©å¨è¦è¦ºåºæºãè¾²æ¥­å½±åæç¨å°é«å­¸å½±åæç¨ãæå¾ï¼æåçµ±è¨èª¿æ¥äºééæä½³åæ¹æ¡éç¼ççµææ¿æ´»å½æ¸çæ¦åãéé Friedman æª¢å®ï¼æåå¾åºçµè«ï¼æä½³åæ¹æ¡è½å¤ ç¢çå¨ 28 åä¸åçç ç©¶æ¡ä¾ä¸­ï¼æ 92.8% çæ¡ä¾æè½åªæ¼ç¾ææ¨æºå½æ¸ï¼ä¸¦ä¸ç¼ç¾ $-x\cdot erf(e^{-x})$ æ¯æä½³åæ¹æ¡ç¢ççæä½³å½±ååé¡æ¿æ´»å½æ¸ã</paragraph>

##### **LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**
2409.04744v1 by Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Wei Chu, Yinghui Xu

The uncertainty inherent in the environmental transition model of
Reinforcement Learning (RL) necessitates a careful balance between exploration
and exploitation to optimize the use of computational resources for accurately
estimating an agent's expected reward. Achieving balance in control systems is
particularly challenging in scenarios with sparse rewards. However, given the
extensive prior knowledge available for many environments, it is redundant to
begin learning from scratch in such settings. To address this, we introduce
\textbf{L}anguage \textbf{M}odel \textbf{G}uided \textbf{T}rade-offs (i.e.,
\textbf{LMGT}), a novel, sample-efficient framework that leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their adeptness at processing non-standard data forms, such as wiki tutorials.
LMGT proficiently manages the exploration-exploitation trade-off by employing
reward shifts guided by LLMs, which direct agents' exploration endeavors,
thereby improving sample efficiency. We have thoroughly tested LMGT across
various RL tasks and deployed it in industrial-grade RL recommendation systems,
where it consistently outperforms baseline methods. The results indicate that
our framework can significantly reduce the time cost required during the
training phase in RL.

æè¦ï¼å¨å¼·åå­¸ç¿ï¼RLï¼çç°å¢è½ææ¨¡åä¸­ï¼åºæçä¸ç¢ºå®æ§éè¦å¨æ¢ç´¢åå©ç¨ä¹éåå¾ä»ç´°çå¹³è¡¡ï¼ä»¥æä½³åè¨ç®è³æºçä½¿ç¨ï¼ä»¥ç²¾æºä¼°è¨ä»£çé æççåµãå¨æ§å¶ç³»çµ±ä¸­åå¾å¹³è¡¡å¨çåµç¨ççææ³ä¸ç¹å¥å·æææ°æ§ãç¶èï¼ç±æ¼è¨±å¤ç°å¢é½æå»£æ³çåé©ç¥è­ï¼å æ­¤å¨éç¨®è¨­å®ä¸­å¾é ­éå§å­¸ç¿æ¯å¤é¤çãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº**L**anguage **M**odel **G**uided **T**rade-offsï¼å³**LMGT**ï¼ï¼éæ¯ä¸åæ°ç©ä¸æ¨£æ¬æçé«çæ¶æ§ï¼å®å©ç¨äºå¤§åèªè¨æ¨¡åï¼LLMï¼ä¸­åµå¥çå¨é¢åé©ç¥è­ï¼ä»¥åå®åèçéæ¨æºæ¸æå½¢å¼ï¼ä¾å¦ wiki æç¨ï¼çéæ´»æ§ãLMGT ééæ¡ç¨ç± LLM å¼å°ççåµè½ç§»ä¾çç·´å°ç®¡çæ¢ç´¢-å©ç¨æ¬è¡¡ï¼æå°ä»£ççæ¢ç´¢å·¥ä½ï¼å¾èæé«æ¨£æ¬æçãæåå·²ç¶å¾¹åºæ¸¬è©¦äº LMGT å¨åç¨® RL ä»»åä¸­çè¡¨ç¾ï¼ä¸¦å°å¶é¨ç½²å¨å·¥æ¥­ç´ RL æ¨è¦ç³»çµ±ä¸­ï¼å¨éäºç³»çµ±ä¸­ï¼å®å§çµåªæ¼åºç·æ¹æ³ãçµæè¡¨æï¼æåçæ¶æ§å¯ä»¥é¡¯èæ¸å° RL è¨ç·´éæ®µæéçæéææ¬ã

##### **NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**
2409.04723v1 by Debaditya Shome, Nasim Montazeri Ghahjaverestan, Ali Etemad

Sleep is known to be a key factor in emotional regulation and overall mental
health. In this study, we explore the integration of sleep measures from the
previous night into wearable-based mood recognition. To this end, we propose
NapTune, a novel prompt-tuning framework that utilizes sleep-related measures
as additional inputs to a frozen pre-trained wearable time-series encoder by
adding and training lightweight prompt parameters to each Transformer layer.
Through rigorous empirical evaluation, we demonstrate that the inclusion of
sleep data using NapTune not only improves mood recognition performance across
different wearable time-series namely ECG, PPG, and EDA, but also makes it more
sample-efficient. Our method demonstrates significant improvements over the
best baselines and unimodal variants. Furthermore, we analyze the impact of
adding sleep-related measures on recognizing different moods as well as the
influence of individual sleep-related measures.

æè¦ï¼ç¡ç å·²ç¥æ¯æç·èª¿ç¯åæ´é«å¿çå¥åº·ä¸­çééµå ç´ ãå¨æ¬ç ç©¶ä¸­ï¼æåæ¢è¨å°åä¸æçç¡ç æ¸¬éæ´åå°å¯ç©¿æ´å¼æç·è¾¨è­ä¸­ãçºæ­¤ï¼æåæåºäº NapTuneï¼éæ¯ä¸åæ°ç©çæç¤ºèª¿æ´æ¡æ¶ï¼å®å©ç¨èç¡ç ç¸éçæ¸¬éä½çºåçµé è¨ç·´å¯ç©¿æ´æéåºåç·¨ç¢¼å¨çéå è¼¸å¥ï¼æ¹æ³æ¯å°è¼éç´æç¤ºåæ¸æ°å¢ä¸¦è¨ç·´å°æ¯å Transformer å±¤ãééå´è¬¹çç¶é©è©ä¼°ï¼æåè­æä½¿ç¨ NapTune ç´å¥ç¡ç æ¸æä¸åæ¹åäºä¸åå¯ç©¿æ´æéåºåï¼å³å¿é»åãåé»å®¹ç©æè¨åç®é»æ´»åï¼çæç·è¾¨è­æè½ï¼éè®å®æ´å·æ¨£æ¬æçãæåçæ¨¡åè­æäºç¸è¼æ¼æä½³åºç·åå®æ¨¡æè®ç°ï¼æé¡¯èçæ¹åãæ­¤å¤ï¼æååæäºæ°å¢èç¡ç ç¸éçæ¸¬éå°è¾¨è­ä¸åæç·çå½±é¿ï¼ä»¥ååå¥èç¡ç ç¸éçæ¸¬éçå½±é¿ã

##### **A Comprehensive Survey on Evidential Deep Learning and Its Applications**
2409.04720v1 by Junyu Gao, Mengyuan Chen, Liangyu Xiang, Changsheng Xu

Reliable uncertainty estimation has become a crucial requirement for the
industrial deployment of deep learning algorithms, particularly in high-risk
applications such as autonomous driving and medical diagnosis. However,
mainstream uncertainty estimation methods, based on deep ensembling or Bayesian
neural networks, generally impose substantial computational overhead. To
address this challenge, a novel paradigm called Evidential Deep Learning (EDL)
has emerged, providing reliable uncertainty estimation with minimal additional
computation in a single forward pass. This survey provides a comprehensive
overview of the current research on EDL, designed to offer readers a broad
introduction to the field without assuming prior knowledge. Specifically, we
first delve into the theoretical foundation of EDL, the subjective logic
theory, and discuss its distinctions from other uncertainty estimation
frameworks. We further present existing theoretical advancements in EDL from
four perspectives: reformulating the evidence collection process, improving
uncertainty estimation via OOD samples, delving into various training
strategies, and evidential regression networks. Thereafter, we elaborate on its
extensive applications across various machine learning paradigms and downstream
tasks. In the end, an outlook on future directions for better performances and
broader adoption of EDL is provided, highlighting potential research avenues.

æè¦ï¼å¯é çä¸ç¢ºå®æ§ä¼°è¨å·²æçºæ·±åº¦å­¸ç¿æ¼ç®æ³ç¢æ¥­é¨ç½²çééµéæ±ï¼ç¹å¥æ¯å¨é«é¢¨éªæç¨ä¸­ï¼ä¾å¦èªåé§é§åé«çè¨ºæ·ãç¶èï¼åºæ¼æ·±åº¦éææè²æ°ç¥ç¶ç¶²è·¯çä¸»æµä¸ç¢ºå®æ§ä¼°è¨æ¹æ³éå¸¸æé æå¤§éçè¨ç®è² æãçºäºæå°éé ææ°ï¼ä¸ç¨®ç¨±çºè­ææ·±åº¦å­¸ç¿ (EDL) çæ°ç¯ä¾æéèçï¼å®å¨å®æ¬¡ååå³éä¸­ä»¥æå°çé¡å¤éç®æä¾å¯é çä¸ç¢ºå®æ§ä¼°è¨ãéé èª¿æ¥å° EDL çç¾æç ç©¶æä¾å¨é¢çæ¦è¿°ï¼æ¨å¨çºè®èæä¾è©²é åçå»£æ³ä»ç´¹ï¼èç¡éåè¨­ååç¥è­ãå·é«ä¾èªªï¼æåé¦åæ·±å¥æ¢è¨ EDL ççè«åºç¤ï¼å³ä¸»è§éè¼¯çè«ï¼ä¸¦è¨è«å¶èå¶ä»ä¸ç¢ºå®æ§ä¼°è¨æ¶æ§çåå¥ãæåé²ä¸æ­¥å¾ååè§åº¦ä»ç´¹ EDL ä¸­ç¾æççè«é²å±ï¼éæ°å¶å®è­ææ¶ééç¨ãéé OOD æ¨£æ¬æ¹åä¸ç¢ºå®æ§ä¼°è¨ãæ·±å¥æ¢è¨åç¨®è¨ç·´ç­ç¥ä»¥åè­æåæ­¸ç¶²è·¯ãæ­¤å¾ï¼æåè©³ç´°èªªæå®å¨åç¨®æ©å¨å­¸ç¿ç¯ä¾åä¸æ¸¸ä»»åä¸­çå»£æ³æç¨ãæå¾ï¼æåæä¾äºå°æªä¾æ¹åçå±æï¼ä»¥æç²å¾æ´å¥½çæè½åæ´å»£æ³å°æ¡ç¨ EDLï¼ä¸¦éé»ä»ç´¹æ½å¨çç ç©¶éå¾ã

##### **A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**
2409.04704v1 by Cheng Wan, Chenjie Xie, Longfei Liu, Dan Wu, Ye Li

Continuous blood pressure (BP) monitoring is essential for timely diagnosis
and intervention in critical care settings. However, BP varies significantly
across individuals, this inter-patient variability motivates the development of
personalized models tailored to each patient's physiology. In this work, we
propose a personalized BP forecasting model mainly using electrocardiogram
(ECG) and photoplethysmogram (PPG) signals. This time-series model incorporates
2D representation learning to capture complex physiological relationships.
Experiments are conducted on datasets collected from three diverse scenarios
with BP measurements from 60 subjects total. Results demonstrate that the model
achieves accurate and robust BP forecasts across scenarios within the
Association for the Advancement of Medical Instrumentation (AAMI) standard
criteria. This reliable early detection of abnormal fluctuations in BP is
crucial for at-risk patients undergoing surgery or intensive care. The proposed
model provides a valuable addition for continuous BP tracking to reduce
mortality and improve prognosis.

æè¦ï¼æçºçè¡å£ (BP) ç£æ§å°æ¼éçç£è­·ç°å¢ä¸­çåæè¨ºæ·åå¹²é è³ééè¦ãç¶èï¼BP å äººèç°ï¼éç¨®æ£èéè®ç°æ§ä¿ä½¿éç¼éå°æ¯ä½æ£èçççæ³éèº«æé çåäººåæ¨¡åãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸ç¨®åäººå BP é æ¸¬æ¨¡åï¼ä¸»è¦ä½¿ç¨å¿é»å (ECG) ååé»å®¹ç©æè¨æ³ (PPG) ä¿¡èãæ­¤æéåºåæ¨¡åçµåäº 2D è¡¨å¾µå­¸ç¿ä»¥ææè¤éçççéä¿ãå¯¦é©æ¯å¨å¾ä¸ç¨®ä¸åæå¢æ¶éçè³æéä¸é²è¡ï¼ç¸½å±ä¾èª 60 ä½åè©¦èç BP æ¸¬éãçµæè¡¨æï¼è©²æ¨¡åå¨é«å­¸åå¨ä¿é²åæ (AAMI) æ¨æºæ¨æºå§å¯¦ç¾äºè·¨æå¢çæºç¢ºä¸ç©©å¥ç BP é æ¸¬ãå°æ¼æ¥åæè¡æéçç£è­·çé«é¢¨éªæ£èèè¨ï¼éç¨®å° BP ç°å¸¸æ³¢åçå¯é æ©ææª¢æ¸¬è³ééè¦ãææåºçæ¨¡åçºæçº BP è¿½è¹¤æä¾äºæå¹å¼çè£åï¼ä»¥éä½æ­»äº¡çä¸¦æ¹åé å¾ã

##### **The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**
2409.04368v1 by Gregory Szumel, Brian Guo, Darui Lu, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski

Purpose: Medical images acquired using different scanners and protocols can
differ substantially in their appearance. This phenomenon, scanner domain
shift, can result in a drop in the performance of deep neural networks which
are trained on data acquired by one scanner and tested on another. This
significant practical issue is well-acknowledged, however, no systematic study
of the issue is available across different modalities and diagnostic tasks.
Materials and Methods: In this paper, we present a broad experimental study
evaluating the impact of scanner domain shift on convolutional neural network
performance for different automated diagnostic tasks. We evaluate this
phenomenon in common radiological modalities, including X-ray, CT, and MRI.
Results: We find that network performance on data from a different scanner is
almost always worse than on same-scanner data, and we quantify the degree of
performance drop across different datasets. Notably, we find that this drop is
most severe for MRI, moderate for X-ray, and quite small for CT, on average,
which we attribute to the standardized nature of CT acquisition systems which
is not present in MRI or X-ray. We also study how injecting varying amounts of
target domain data into the training set, as well as adding noise to the
training data, helps with generalization. Conclusion: Our results provide
extensive experimental evidence and quantification of the extent of performance
drop caused by scanner domain shift in deep learning across different
modalities, with the goal of guiding the future development of robust deep
learning models for medical image analysis.

æè¦ï¼<paragraph>ç®çï¼ä½¿ç¨ä¸åææåååå®åå¾çé«å­¸å½±åï¼å¨å½±åå¤è§ä¸å¯è½ææé¡¯èå·®ç°ãéç¨®ç¾è±¡ç¨±çºææåé ååç§»ï¼å¯è½æå°è´æ·±åº¦ç¥ç¶ç¶²è·¯çæè½ä¸éï¼èéäºç¶²è·¯æ¯éå°ç±ä¸ç¨®ææååå¾çè³æé²è¡è¨ç·´ï¼ä¸¦å¨å¦ä¸ç¨®ææåä¸é²è¡æ¸¬è©¦ãéåéè¦çå¯¦éåé¡å·²ç²å¾å»£æ³èªå¯ï¼ä½ç®åå°æªéå°ä¸åå½¢å¼åè¨ºæ·ä»»åé²è¡ç³»çµ±æ§ç ç©¶ãææåæ¹æ³ï¼å¨æ¬æä¸­ï¼æåæåºäºä¸é å»£æ³çå¯¦é©ç ç©¶ï¼è©ä¼°ææåé ååç§»å°ä¸åèªååè¨ºæ·ä»»åçå·ç©ç¥ç¶ç¶²è·¯æè½çå½±é¿ãæåå¨å¸¸è¦çæ¾å°å­¸å½¢å¼ä¸­è©ä¼°éç¨®ç¾è±¡ï¼åæ¬ X åãé»è¦æ·å±¤ææåç£æ¯é å½±ãçµæï¼æåç¼ç¾ï¼ä¾èªä¸åææåçè³æå¨ç¶²è·¯ä¸çæè½å¹¾ä¹ç¸½æ¯æ¯ä¾èªç¸åææåçè³æå·®ï¼æåéåäºä¸åè³æéæè½ä¸éçç¨åº¦ãå¼å¾æ³¨æçæ¯ï¼æåç¼ç¾éç¨®ä¸éå¨ç£æ¯é å½±ä¸­æä¸ºå´éï¼å¨ X åä¸­çºä¸­ç­ï¼å¨é»è¦æ·å±¤ææä¸­ç¸ç¶å°ï¼å¹³åèè¨ï¼æåå°å¶æ­¸å æ¼é»è¦æ·å±¤ææåå¾ç³»çµ±çæ¨æºåæ§è³ªï¼èç£æ¯é å½±æ X åä¸­ä¸å­å¨éç¨®æ§è³ªãæåéç ç©¶äºå°ä¸åæ¸éçç®æ¨é åè³ææ³¨å¥è¨ç·´éï¼ä»¥ååè¨ç·´è³æå å¥éè¨ï¼å¦ä½æå©æ¼æ³åãçµè«ï¼æåççµææä¾äºå»£æ³çå¯¦é©è­æï¼ä¸¦éåäºæ·±åº¦å­¸ç¿ä¸­ç±ææåé ååç§»é æçæè½ä¸éç¨åº¦ï¼ç®æ¨æ¯å¼å°æªä¾éå°é«å­¸å½±ååæçå¼·å¥æ·±åº¦å­¸ç¿æ¨¡åçç¼å±ã</paragraph>

##### **CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**
2409.04290v1 by William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhidong Zhang, Jiashuai Liu, Mireia Crispin-Ortuzar

Survival analysis is a branch of statistics used for modeling the time until
a specific event occurs and is widely used in medicine, engineering, finance,
and many other fields. When choosing survival models, there is typically a
trade-off between performance and interpretability, where the highest
performance is achieved by black-box models based on deep learning. This is a
major problem in fields such as medicine where practitioners are reluctant to
blindly trust black-box models to make important patient decisions.
Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable
and accurate alternative to multi-layer perceptrons (MLPs). We introduce
CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,
high-performance survival analysis. We evaluate the proposed CoxKAN on 4
synthetic datasets and 9 real medical datasets. The synthetic experiments
demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for
the hazard function, and effectively performs automatic feature selection.
Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the
Cox proportional hazards model and achieves performance that is superior or
comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies
complex interactions between predictor variables that would be extremely
difficult to recognise using existing survival methods, and automatically finds
symbolic formulae which uncover the precise effect of important biomarkers on
patient risk.

æè¦ï¼çå­åææ¯çµ±è¨å­¸çä¸ååæ¯ï¼ç¨æ¼å»ºæ¨¡ç¹å®äºä»¶ç¼ççæéï¼ä¸¦å»£æ³ç¨æ¼é«å­¸ãå·¥ç¨ãéèåè¨±å¤å¶ä»é åãå¨é¸æçå­æ¨¡åæï¼éå¸¸å¨æ§è½åå¯è§£éæ§ä¹éé²è¡æ¬è¡¡ï¼å¶ä¸­æé«æ§è½æ¯ç±åºæ¼æ·±åº¦å­¸ç¿çé»çæ¨¡åå¯¦ç¾çãéå¨é«å­¸ç­é åæ¯ä¸åä¸»è¦åé¡ï¼å çºå¾æ¥­èä¸é¡æç²ç®ä¿¡ä»»é»çæ¨¡åä¾ååºéè¦çæ£èæ±ºç­ãKolmogorov-é¿è«¾å¾·ç¶²çµ¡ (KAN) æè¿è¢«æè­°ä½çºå¤å±¤æç¥å¨ (MLP) çå¯è§£éä¸æºç¢ºçæ¿ä»£æ¹æ¡ãæåå¼å¥äº CoxKANï¼éæ¯ä¸åç¨æ¼å¯è§£éãé«æ§è½çå­åæç Cox æ¯ä¾é¢¨éª Kolmogorov-Arnold ç¶²çµ¡ãæåå¨ 4 ååææ¸æéå 9 åçå¯¦é«çæ¸æéä¸è©ä¼°äºææåºç CoxKANãåæå¯¦é©è¡¨æï¼CoxKAN æºç¢ºå°æ¢å¾©äºé¢¨éªå½æ¸çå¯è§£éç¬¦èå¬å¼ï¼ä¸¦ææå°å·è¡èªåç¹å¾µé¸æãå° 9 åçå¯¦æ¸æéçè©ä¼°è¡¨æï¼CoxKAN å§çµåªæ¼ Cox æ¯ä¾é¢¨éªæ¨¡åï¼ä¸¦ä¸éå°äºåªæ¼æèèª¿æ´å¾ç MLP ç¸ç¶çæ§è½ãæ­¤å¤ï¼æåç¼ç¾ CoxKAN è­å¥äºé æ¸¬è®éä¹éçè¤éäº¤äºä½ç¨ï¼éäºäº¤äºä½ç¨ä½¿ç¨ç¾æççå­æ¹æ³æ¥µé£è­å¥ï¼ä¸¦èªåæ¾å°æ­ç¤ºéè¦çç©æ¨èªç©å°æ£èé¢¨éªçæºç¢ºå½±é¿çç¬¦èå¬å¼ã

##### **Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**
2409.04224v1 by Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng

Multi-organ diseases present significant challenges due to their simultaneous
impact on multiple organ systems, necessitating complex and adaptive treatment
strategies. Despite recent advancements in AI-powered healthcare decision
support systems, existing solutions are limited to individual organ systems.
They often ignore the intricate dependencies between organ system and thereby
fails to provide holistic treatment recommendations that are useful in
practice. We propose a novel hierarchical multi-agent reinforcement learning
(HMARL) framework to address these challenges. This framework uses dedicated
agents for each organ system, and model dynamic through explicit inter-agent
communication channels, enabling coordinated treatment strategies across
organs. Furthermore, we introduce a dual-layer state representation technique
to contextualize patient conditions at various hierarchical levels, enhancing
the treatment accuracy and relevance. Through extensive qualitative and
quantitative evaluations in managing sepsis (a complex multi-organ disease),
our approach demonstrates its ability to learn effective treatment policies
that significantly improve patient survival rates. This framework marks a
substantial advancement in clinical decision support systems, pioneering a
comprehensive approach for multi-organ treatment recommendations.

æè¦ï¼å¤å¨å®ç¾çç±æ¼åæå½±é¿å¤åå¨å®ç³»çµ±ï¼å æ­¤æå¸¶ä¾éå¤§çææ°ï¼éè¦è¤éä¸å·æé©ææ§çæ²»çç­ç¥ãåç®¡ AI é©åçé«çä¿å¥æ±ºç­æ¯æ´ç³»çµ±æè¿æé²å±ï¼ä½ç¾æè§£æ±ºæ¹æ¡åéæ¼åå¥å¨å®ç³»çµ±ãå®åå¸¸å¸¸å¿½ç¥å¨å®ç³»çµ±ä¹éçè¤éä¾è³´æ§ï¼å æ­¤ç¡æ³æä¾å¯¦åä¸æç¨çæ´é«æ²»çå»ºè­°ãæåæåºä¸åæ°ç©çåå±¤å¤æºè½é«å¼·åå­¸ç¿ (HMARL) æ¶æ§ä¾è§£æ±ºéäºææ°ãæ­¤æ¶æ§çºæ¯åå¨å®ç³»çµ±ä½¿ç¨å°ç¨æºè½é«ï¼ä¸¦ééæç¢ºçæºè½é«ééè¨ç®¡éå»ºæ¨¡åæï¼è®ä¸åå¨å®ä¹éçæ²»çç­ç¥è½å¤ åèª¿ãæ­¤å¤ï¼æåå¼å¥éå±¤çæè¡¨ç¤ºæè¡ï¼å¨åç¨®å±¤ç´èªå¢åçæ£çæ³ï¼ä»¥æåæ²»çæºç¢ºæ§åç¸éæ§ãééå¨æè¡çï¼ä¸ç¨®è¤éçå¤å¨å®ç¾çï¼ç®¡çä¸­é²è¡å»£æ³çå®æ§åå®éè©ä¼°ï¼æåçåæ³å±ç¤ºäºå®å­¸ç¿æææ²»çæ¿ç­çè½åï¼å¯é¡¯èæ¹åçæ£å­æ´»çãæ­¤æ¶æ§æ¨èªèè¨åºæ±ºç­æ¯æ´ç³»çµ±çä¸å¤§é²æ­¥ï¼éåµäºå¤å¨å®æ²»çå»ºè­°çå¨é¢æ§æ¹æ³ã

##### **Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**
2409.04481v1 by Yizhen Zheng, Huan Yee Koh, Maddie Yang, Li Li, Lauren T. May, Geoffrey I. Webb, Shirui Pan, George Church

The integration of Large Language Models (LLMs) into the drug discovery and
development field marks a significant paradigm shift, offering novel
methodologies for understanding disease mechanisms, facilitating drug
discovery, and optimizing clinical trial processes. This review highlights the
expanding role of LLMs in revolutionizing various stages of the drug
development pipeline. We investigate how these advanced computational models
can uncover target-disease linkage, interpret complex biomedical data, enhance
drug molecule design, predict drug efficacy and safety profiles, and facilitate
clinical trial processes. Our paper aims to provide a comprehensive overview
for researchers and practitioners in computational biology, pharmacology, and
AI4Science by offering insights into the potential transformative impact of
LLMs on drug discovery and development.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼æ´åå°è¥ç©ç¼ç¾åéç¼é åæ¨èªèéå¤§çå¸ç¯è½ç§»ï¼æä¾äºè§£ç¾çæ©å¶ãä¿é²è¥ç©ç¼ç¾ååªåè¨åºè©¦é©æµç¨çæ°æ¹æ³ãæ¬ç¶è¿°éé»ä»ç´¹äº LLM å¨é©æ°è¥ç©éç¼ç®¡ç·ååéæ®µä¸­æ¥çéè¦çä½ç¨ãæåæ¢è¨äºéäºåé²çè¨ç®æ¨¡åå¦ä½æ­ç¤ºé¶é»ç¾çéè¯æ§ãè§£éè¤éççç©é«å­¸æ¸æãå¢å¼·è¥ç©åå­è¨­è¨ãé æ¸¬è¥ç©çæåå®å¨æ§ï¼ä»¥åä¿é²è¨åºè©¦é©æµç¨ãæåçè«ææ¨å¨çºè¨ç®çç©å­¸ãè¥çå­¸å AI4Science çç ç©¶äººå¡åå¾æ¥­èæä¾å¨é¢çæ¦è¿°ï¼æ·±å¥äºè§£ LLM å°è¥ç©ç¼ç¾åéç¼çæ½å¨è®é©æ§å½±é¿ã

##### **FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**
2409.03947v1 by Kai Shu, Yuzhuo Jia, Ziyang Zhang, Jiechao Gao

Automatic Medical Imaging Narrative generation aims to alleviate the workload
of radiologists by producing accurate clinical descriptions directly from
radiological images. However, the subtle visual nuances and domain-specific
terminology in medical images pose significant challenges compared to generic
image captioning tasks. Existing approaches often neglect the vital distinction
between normal and abnormal findings, leading to suboptimal performance. In
this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive
Partitioning Graph framework that addresses these limitations through
domain-adaptive learning. FODA-PG constructs a granular graphical
representation of radiological findings by separating disease-related
attributes into distinct "disease-specific" and "disease-free" categories based
on their clinical significance and location. This adaptive partitioning enables
our model to capture the nuanced differences between normal and pathological
states, mitigating the impact of data biases. By integrating this fine-grained
semantic knowledge into a powerful transformer-based architecture and providing
rigorous mathematical justifications for its effectiveness, FODA-PG generates
precise and clinically coherent reports with enhanced generalization
capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks
demonstrate the superiority of our approach over state-of-the-art methods,
highlighting the importance of domain adaptation in medical report generation.

æè¦ï¼èªåé«å­¸å½±åæè¿°çææ¨å¨ééç´æ¥å¾æ¾å°å½±åç¢çç²¾ç¢ºçè¨åºæè¿°ï¼æ¸è¼æ¾å°ç§é«å¸«çå·¥ä½è² æãç¶èï¼èä¸è¬å½±åæ¨é¡ä»»åç¸æ¯ï¼é«å­¸å½±åä¸­çç´°å¾®è¦è¦ºå·®ç°åç¹å®é åè¡èªæå¸¶ä¾éå¤§ææ°ãç¾ææ¹æ³å¸¸å¸¸å¿½ç¥æ­£å¸¸èç°å¸¸ç¼ç¾ä¹éçéè¦åå¥ï¼å°è´æ¬¡ä½³æè½ãå¨éé å·¥ä½ä¸­ï¼æåæåº FODA-PGï¼éæ¯ä¸åæ°ç©çç´°ç²åº¦å¨å®ç¾çèªé©æåå²åå½¢æ¶æ§ï¼ééé åèªé©æå­¸ç¿ä¾è§£æ±ºéäºéå¶ãFODA-PG ééå°ç¾çç¸éå±¬æ§ä¾æå¶è¨åºéè¦æ§åä½ç½®åçºä¸åçãç¹å®ç¾çãåãç¡ç¾çãé¡å¥ï¼ä¾å»ºæ§æ¾å°å­¸ç¼ç¾çç´°ç²åº¦åå½¢è¡¨ç¤ºãéç¨®èªé©æåå²ä½¿æåçæ¨¡åè½å¤ æææ­£å¸¸èçççæä¹éçç´°å¾®å·®ç°ï¼æ¸è¼è³æåå·®çå½±é¿ãééå°éç¨®ç´°ç²åº¦èªç¾©ç¥è­æ´åå°å¼·å¤§çåºæ¼è½æå¨çæ¶æ§ä¸­ï¼ä¸¦æä¾å¶æææ§çå´è¬¹æ¸å­¸è­æï¼FODA-PG è½å¤ çæç²¾ç¢ºä¸è¨åºä¸é£è²«çå ±åï¼ä¸¦å·åå¢å¼·çæ¦æ¬è½åãå¨ IU-Xray å MIMIC-CXR åºæºä¸çå»£æ³å¯¦é©è­æäºæåçæ¹æ³åªæ¼æåé²çæ¹æ³ï¼çªé¡¯äºé åé©æå¨é«å­¸å ±åçæä¸­çéè¦æ§ã

##### **A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**
2409.03933v1 by Esther Lagemann, Julia Roeb, Steven L. Brunton, Christian Lagemann

The accurate quantification of wall-shear stress dynamics is of substantial
importance for various applications in fundamental and applied research,
spanning areas from human health to aircraft design and optimization. Despite
significant progress in experimental measurement techniques and post-processing
algorithms, temporally resolved wall-shear stress dynamics with adequate
spatial resolution and within a suitable spatial domain remain an elusive goal.
To address this gap, we introduce a deep learning architecture that ingests
wall-parallel velocity fields from the logarithmic layer of turbulent
wall-bounded flows and outputs the corresponding 2D wall-shear stress fields
with identical spatial resolution and domain size. From a physical perspective,
our framework acts as a surrogate model encapsulating the various mechanisms
through which highly energetic outer-layer flow structures influence the
governing wall-shear stress dynamics. The network is trained in a supervised
fashion on a unified dataset comprising direct numerical simulations of
statistically 1D turbulent channel and spatially developing turbulent boundary
layer flows at friction Reynolds numbers ranging from 390 to 1,500. We
demonstrate a zero-shot applicability to experimental velocity fields obtained
from Particle-Image Velocimetry measurements and verify the physical accuracy
of the wall-shear stress estimates with synchronized wall-shear stress
measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up
to 2,000. In summary, the presented framework lays the groundwork for
extracting inaccessible experimental wall-shear stress information from readily
available velocity measurements and thus, facilitates advancements in a variety
of experimental applications.

æè¦ï¼<paragraph>æºç¢ºéåå£é¢åªæååæå°æ¼åºç¤åæç¨ç ç©¶ä¸­çåç¨®æç¨å·æå¯¦è³ªæ§çéè¦æ§ï¼æ¶µèå¾äººé¡å¥åº·å°é£æ©è¨­è¨ååªåçé åãåç®¡å¨å¯¦é©æ¸¬éæè¡åå¾èçæ¼ç®æ³æ¹é¢åå¾äºé¡¯èé²å±ï¼ä½æéè§£æå£é¢åªæååæä»å·æè¶³å¤ çç©ºéè§£æåº¦åå¨åé©çç©ºéåä¸­ä»ç¶æ¯ä¸åé£ä»¥ææ¸çç®æ¨ãçºäºè§£æ±ºéåå·®è·ï¼æåå¼å¥äºä¸åæ·±åº¦å­¸ç¿æ¶æ§ï¼å®å¾æ¹æµå£é¢ç´ææµçå°æ¸å±¤ä¸­æåå£é¢å¹³è¡éåº¦å ´ï¼ä¸¦è¼¸åºç¸æç 2D å£é¢åªæåå ´ï¼å·æç¸åçç©ºéè§£æåº¦ååå¤§å°ãå¾ç©çè§åº¦ä¾çï¼æåçæ¡æ¶åç¶ä¸åä»£çæ¨¡åï¼æ¦æ¬äºé«è½éå¤å±¤æµçµæ§å½±é¿æ§å¶å£é¢åªæååæçåç¨®æ©å¶ãè©²ç¶²è·¯ä»¥ç£ç£æ¹å¼å¨ä¸åçµ±ä¸çæ¸æéä¸é²è¡è¨ç·´ï¼è©²æ¸æéåå«çµ±è¨ 1D æ¹æµééçç´æ¥æ¸å¼æ¨¡æ¬åç©ºéç¼å±çæ¹æµéçå±¤æµï¼æ©æ¦é·è«¾æ¸ç¯åå¾ 390 å° 1,500ãæåå±ç¤ºäºå°å¾ç²å­å½±åæ¸¬éæ¸¬éä¸­ç²å¾çå¯¦é©éåº¦å ´çé¶æ¬¡æç¨ï¼ä¸¦ä½¿ç¨å¾®æ±åªæåææ¸¬å¨å°é·è«¾æ¸æé« 2,000 çåæ­¥å£é¢åªæåæ¸¬éé©è­äºå£é¢åªæåä¼°è¨çç©çæºç¢ºæ§ãç¸½ä¹ï¼ææåºçæ¡æ¶çºå¾å®¹æç²å¾çéåº¦æ¸¬éä¸­æåç¡æ³ç²å¾çå¯¦é©å£é¢åªæåè³è¨å¥ å®äºåºç¤ï¼å¾èä¿é²äºåç¨®å¯¦é©æç¨ä¸­çé²å±ã</paragraph>

##### **Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**
2409.03597v1 by Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Faya Liang, Ming Li

This paper presents the Multimodal Analyzing System for Laryngoscope (MASL),
a system that combines audio and video data to automatically extract key
segments and metrics from laryngeal videostroboscopic videos for clinical
assessment. MASL integrates glottis detection with keyword spotting to analyze
patient vocalizations and refine video highlights for better inspection of
vocal cord movements. The system includes a strobing video extraction module
that identifies frames by analyzing hue, saturation, and value fluctuations.
MASL also provides effective metrics for vocal cord paralysis detection,
employing a two-stage glottis segmentation process using U-Net followed by
diffusion-based refinement to reduce false positives. Instead of glottal area
waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis
masks, evaluating both left and right vocal cords to detect unilateral vocal
cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between
left and right paralysis. Ablation studies and experiments on public and
real-world datasets validate MASL's segmentation module and demonstrate its
ability to provide reliable metrics for UVFP diagnosis.

æè¦ï¼æ¬ææåºäºåéå¤æ¨¡æåæç³»ç» (MASL)ï¼
è¯¥ç³»ç»ç»åé³é¢åè§é¢æ°æ®ï¼èªå¨ä»åé¨è§é¢é¢éªéè§é¢ä¸­æåå³é®
çæ®µåææ ï¼ç¨äºä¸´åºè¯ä¼°ãMASL å°å£°é¨æ£æµä¸å³é®è¯è¯å«ç¸ç»åï¼ä»¥åæ
æ£èåå£°å¹¶ç»åè§é¢éç¹ï¼ä»¥ä¾¿æ´å¥½å°æ£æ¥å£°å¸¦è¿å¨ãè¯¥ç³»ç»åæ¬ä¸ä¸ªé¢éªè§é¢æåæ¨¡åï¼
è¯¥æ¨¡åéè¿åæè²ç¸ãé¥±ååº¦åå¼æ³¢å¨æ¥è¯å«å¸§ã
MASL è¿ä¸ºå£°å¸¦éº»ç¹æ£æµæä¾äºææçææ ï¼
éç¨ä¸¤é¶æ®µå£°é¨åå²è¿ç¨ï¼ä½¿ç¨ U-Netï¼ç¶åè¿è¡åºäºæ©æ£çç»åä»¥åå°è¯¯æ¥ãMASL ä¸ä½¿ç¨å£°é¨é¢ç§¯æ³¢å½¢ï¼èæ¯ä»å£°é¨æ©æ¨¡ä¸­ä¼°è®¡åå£°é¨è§æ³¢å½¢ (AGAW)ï¼è¯ä¼°å·¦å³å£°å¸¦ä»¥æ£æµåä¾§å£°å¸¦éº»ç¹ (UVFP)ãéè¿æ¯è¾ AGAW æ¹å·®ï¼MASL åºåå·¦å³éº»ç¹ãæ¶èç ç©¶åå¯¹å¬å±åçå®ä¸çæ°æ®éçå®éªéªè¯äº MASL çåå²æ¨¡åï¼å¹¶è¯æäºå¶æä¾å¯é ç UVFP è¯æ­ææ çè½åã

##### **Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**
2409.03470v1 by Prerak Mody, Nicolas F. Chaves-de-Plaza, Chinmay Rao, Eleftheria Astrenidou, Mischa de Ridder, Nienke Hoekstra, Klaus Hildebrandt, Marius Staring

Increased usage of automated tools like deep learning in medical image
segmentation has alleviated the bottleneck of manual contouring. This has
shifted manual labour to quality assessment (QA) of automated contours which
involves detecting errors and correcting them. A potential solution to
semi-automated QA is to use deep Bayesian uncertainty to recommend potentially
erroneous regions, thus reducing time spent on error detection. Previous work
has investigated the correspondence between uncertainty and error, however, no
work has been done on improving the "utility" of Bayesian uncertainty maps such
that it is only present in inaccurate regions and not in the accurate ones. Our
work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which
promotes uncertainty to be present only in inaccurate regions. We apply this
method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and
prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated
against voxel inaccuracies using Receiver Operating Characteristic (ROC) and
Precision-Recall (PR) curves. Numerical results show that when compared to the
Bayesian baseline the proposed method successfully suppresses uncertainty for
accurate voxels, with similar presence of uncertainty for inaccurate voxels.
Code to reproduce experiments is available at
https://github.com/prerakmody/bayesuncertainty-error-correspondence

æè¦ï¼æ·±åº¦å­¸ç¿ç­èªååå·¥å·å¨é«å­¸å½±ååå²ä¸­ä½¿ç¨çæåï¼æ¸è¼äºæåè¼ªå»æç¹ªçç¶é ¸ãéå·²å°æåååè½ç§»å°èªåè¼ªå»çåè³ªè©ä¼° (QA)ï¼å¶ä¸­åå«åµæ¸¬é¯èª¤ä¸¦ä¿®æ­£å®åãåèªåå QA çæ½å¨è§£æ±ºæ¹æ¡æ¯ä½¿ç¨æ·±åº¦è²æ°ä¸ç¢ºå®æ§ä¾å»ºè­°æ½å¨çé¯èª¤ååï¼å¾èæ¸å°è±è²»å¨é¯èª¤åµæ¸¬ä¸çæéãååçç ç©¶å·²èª¿æ¥ä¸ç¢ºå®æ§åé¯èª¤ä¹éçå°æéä¿ï¼ç¶èï¼å°æªå°æ¹åè²æ°ä¸ç¢ºå®æ§å°åçãæç¨ãé²è¡ç ç©¶ï¼ä»¥ä½¿å¶ååºç¾å¨ä¸æºç¢ºååï¼èä¸åºç¾å¨æºç¢ºååãæåçç ç©¶ä½¿ç¨æºç¢ºåº¦å°æä¸ç¢ºå®æ§ (AvU) æå¤±ä¾è¨ç·´ FlipOut æ¨¡åï¼éæä¿ä½¿ä¸ç¢ºå®æ§ååºç¾å¨ä¸æºç¢ºååãæåå°æ­¤æ¹æ³æç¨æ¼å©åæ¾å°æ²»çé¨ä½çè³æéï¼å³é ­é ¸é¨é»è¦æ·å±¤ææåååèºæ ¸ç£å±æ¯ææãä½¿ç¨æ¥æ¶å¨æä½ç¹æ§ (ROC) åç²¾ç¢ºåº¦å¬åç (PR) æ²ç·ï¼éå°é«ç´ ä¸æºç¢ºæ§è©ä¼°ä¸ç¢ºå®æ§ç±åï¼å³é æ¸¬çµï¼ãæ¸å¼çµæé¡¯ç¤ºï¼èè²æ°åºæºç¸æ¯ï¼ææåºçæ¹æ³æåå°æå¶æºç¢ºé«ç´ çä¸ç¢ºå®æ§ï¼å°æ¼ä¸æºç¢ºé«ç´ çä¸ç¢ºå®æ§å­å¨é¡ä¼¼ææ³ãå¯å¨ https://github.com/prerakmody/bayesuncertainty-error-correspondence åå¾éç¾å¯¦é©çç¨å¼ç¢¼

##### **Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**
2409.03375v1 by Francisco de Arriba-PÃ©rez, Silvia GarcÃ­a-MÃ©ndez

Based on official estimates, 50 million people worldwide are affected by
dementia, and this number increases by 10 million new patients every year.
Without a cure, clinical prognostication and early intervention represent the
most effective ways to delay its progression. To this end, Artificial
Intelligence and computational linguistics can be exploited for natural
language analysis, personalized assessment, monitoring, and treatment. However,
traditional approaches need more semantic knowledge management and
explicability capabilities. Moreover, using Large Language Models (LLMs) for
cognitive decline diagnosis is still scarce, even though these models represent
the most advanced way for clinical-patient communication using intelligent
systems. Consequently, we leverage an LLM using the latest Natural Language
Processing (NLP) techniques in a chatbot solution to provide interpretable
Machine Learning prediction of cognitive decline in real-time.
Linguistic-conceptual features are exploited for appropriate natural language
analysis. Through explainability, we aim to fight potential biases of the
models and improve their potential to help clinical workers in their diagnosis
decisions. More in detail, the proposed pipeline is composed of (i) data
extraction employing NLP-based prompt engineering; (ii) stream-based data
processing including feature engineering, analysis, and selection; (iii)
real-time classification; and (iv) the explainability dashboard to provide
visual and natural language descriptions of the prediction outcome.
Classification results exceed 80 % in all evaluation metrics, with a recall
value for the mental deterioration class about 85 %. To sum up, we contribute
with an affordable, flexible, non-invasive, personalized diagnostic system to
this work.

æè¦ï¼<paragraph>æ ¹æå®æ¹çä¼°è¨ï¼å¨çç´æ 5000 è¬äººç½¹æ£å¤±æºçï¼ä¸éåæ¸å­æ¯å¹´å¢å  1000 è¬åæ°æ£èãå¨æ²ææ²»çæ¹æ³çææ³ä¸ï¼è¨åºé å¾åæ©æä»å¥æ¯å»¶ç·©å¶æ¡åçææææ¹æ³ãçºæ­¤ï¼äººå·¥æºæ§åè¨ç®èªè¨å­¸å¯è¢«ç¨æ¼èªç¶èªè¨åæãåäººåè©ä¼°ãç£æ§åæ²»çãç¶èï¼å³çµ±æ¹æ³éè¦æ´å¤èªç¾©ç¥è­ç®¡çåå¯è§£éæ§è½åãæ­¤å¤ï¼åç®¡éäºæ¨¡åä»£è¡¨äºä½¿ç¨æºæ§ç³»çµ±é²è¡è¨åºæ£èæºéçæåé²æ¹å¼ï¼ä½å°å¤§åèªè¨æ¨¡å (LLM) ç¨æ¼èªç¥è½åä¸éè¨ºæ·ä»ç¶å¾å°è¦ãå æ­¤ï¼æåå©ç¨èå¤©æ©å¨äººè§£æ±ºæ¹æ¡ä¸­ä½¿ç¨ææ°èªç¶èªè¨èç (NLP) æè¡ç LLMï¼ä»¥æä¾å°èªç¥è½åä¸éçæ©å¨å­¸ç¿é æ¸¬ãèªè¨æ¦å¿µç¹å¾µè¢«ç¨æ¼é©ç¶çèªç¶èªè¨åæãééå¯è§£éæ§ï¼æåæ¨å¨æ¶é¤æ¨¡åçæ½å¨åå·®ï¼ä¸¦æé«å¶å¨è¨ºæ·æ±ºç­ä¸­åå©è¨åºå·¥ä½èçæ½åãæ´è©³ç´°å°èªªï¼ææåºçç®¡éåæ¬ï¼(i) ä½¿ç¨åºæ¼ NLP çæç¤ºå·¥ç¨é²è¡è³æèåï¼(ii) ä¸²æµå¼è³æèçï¼åæ¬ç¹å¾µå·¥ç¨ãåæåé¸æï¼(iii) å³æåé¡ï¼ä»¥å (iv) å¯è§£éæ§åè¡¨æ¿ï¼ä»¥æä¾é æ¸¬çµæçå¯è¦ååèªç¶èªè¨æè¿°ãåé¡çµæå¨ææè©ä¼°ææ¨ä¸­é½è¶é 80%ï¼å¿æºéåé¡å¥çå¬åçç´çº 85%ãç¸½èè¨ä¹ï¼æåçºéé å·¥ä½è²¢ç»äºä¸åç¶æ¿å¯¦æ ãéæ´»ãéä¾µå¥æ§ãåäººåçè¨ºæ·ç³»çµ±ã</paragraph>

##### **Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**
2409.03147v1 by Juan A. Berrios Moya

The rapid global aging trend has led to an increase in dementia cases,
including Alzheimer's disease, underscoring the urgent need for early and
accurate diagnostic methods. Traditional diagnostic techniques, such as
cognitive tests, neuroimaging, and biomarker analysis, face significant
limitations in sensitivity, accessibility, and cost, particularly in the early
stages. This study explores the potential of machine learning (ML) as a
transformative approach to enhance early dementia detection by leveraging ML
models to analyze and integrate complex multimodal datasets, including
cognitive assessments, neuroimaging, and genetic information. A comprehensive
review of existing literature was conducted to evaluate various ML models,
including supervised learning, deep learning, and advanced techniques such as
ensemble learning and transformer models, assessing their accuracy,
interpretability, and potential for clinical integration. The findings indicate
that while ML models show significant promise in improving diagnostic precision
and enabling earlier interventions, challenges remain in their
generalizability, interpretability, and ethical deployment. This research
concludes by outlining future directions aimed at enhancing the clinical
utility of ML models in dementia detection, emphasizing interdisciplinary
collaboration and ethically sound frameworks to improve early detection and
intervention strategies for Alzheimer's disease and other forms of dementia.

æè¦ï¼å¨çäººå£å¿«éèåè¶¨å¢å°è´å¤±æºççä¾å¢å ï¼åæ¬é¿è²æµ·é»çï¼çªé¡¯åºæ©æä¸æºç¢ºçè¨ºæ·æ¹æ³çè¿«åéæ±ãå³çµ±çè¨ºæ·æè¡ï¼ä¾å¦èªç¥æ¸¬é©ãç¥ç¶å½±ååçç©æ¨è¨åæï¼å¨æææ§ãå¯åæ§åææ¬æ¹é¢é¢è¨éå¤§éå¶ï¼ç¹å¥æ¯å¨æ©æéæ®µãæ¬ç ç©¶æ¢è¨æ©å¨å­¸ç¿ (ML) ä½çºä¸ç¨®è®é©æ§æ¹æ³çæ½åï¼ééå©ç¨ ML æ¨¡ååæåæ´åè¤éçå¤æ¨¡å¼æ¸æéï¼åæ¬èªç¥è©ä¼°ãç¥ç¶å½±ååéºå³ä¿¡æ¯ï¼ä¾å¢å¼·æ©æå¤±æºçæª¢æ¸¬ãå°ç¾ææç»é²è¡äºå¨é¢åé¡§ï¼ä»¥è©ä¼°åç¨® ML æ¨¡åï¼åæ¬ç£ç£å­¸ç¿ãæ·±åº¦å­¸ç¿ååé²æè¡ï¼ä¾å¦éæå­¸ç¿åTransformeræ¨¡åï¼è©ä¼°å¶æºç¢ºæ§ãå¯è§£éæ§åè¨åºæ´åçæ½åãç ç©¶çµæè¡¨æï¼åç®¡ ML æ¨¡åå¨æé«è¨ºæ·ç²¾åº¦åå¯¦ç¾æ©æå¹²é æ¹é¢é¡¯ç¤ºåºé¡¯èçå¸æï¼ä½å¶å¯æ¦åæ§ãå¯è§£éæ§åéå¾·é¨ç½²ä»ç¶å­å¨ææ°ãæ¬ç ç©¶æå¾æ¦è¿°äºæ¨å¨å¢å¼· ML æ¨¡åå¨å¤±æºçæª¢æ¸¬ä¸­çè¨åºæç¨çæªä¾æ¹åï¼å¼·èª¿è·¨å­¸ç§åä½åéå¾·å¥å¨çæ¡æ¶ï¼ä»¥æ¹åé¿è²æµ·é»çåå¶ä»å½¢å¼å¤±æºççæ©ææª¢æ¸¬åå¹²é ç­ç¥ã

##### **MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**
2409.03062v1 by Shehan Perera, Yunus Erzurumlu, Deepak Gulati, Alper Yilmaz

Skin cancer segmentation poses a significant challenge in medical image
analysis. Numerous existing solutions, predominantly CNN-based, face issues
related to a lack of global contextual understanding. Alternatively, some
approaches resort to large-scale Transformer models to bridge the global
contextual gaps, but at the expense of model size and computational complexity.
Finally many Transformer based approaches rely primarily on CNN based decoders
overlooking the benefits of Transformer based decoding models. Recognizing
these limitations, we address the need efficient lightweight solutions by
introducing MobileUNETR, which aims to overcome the performance constraints
associated with both CNNs and Transformers while minimizing model size,
presenting a promising stride towards efficient image segmentation. MobileUNETR
has 3 main features. 1) MobileUNETR comprises of a lightweight hybrid
CNN-Transformer encoder to help balance local and global contextual feature
extraction in an efficient manner; 2) A novel hybrid decoder that
simultaneously utilizes low-level and global features at different resolutions
within the decoding stage for accurate mask generation; 3) surpassing large and
complex architectures, MobileUNETR achieves superior performance with 3 million
parameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x
reduction in parameters and FLOPS, respectively. Extensive experiments have
been conducted to validate the effectiveness of our proposed method on four
publicly available skin lesion segmentation datasets, including ISIC 2016, ISIC
2017, ISIC 2018, and PH2 datasets. The code will be publicly available at:
https://github.com/OSUPCVLab/MobileUNETR.git

æè¦ï¼ç®èçåå²å¨é«å­¸å½±ååæä¸­æ§æä¸é éå¤§ææ°ãç¾æè¨±å¤è§£æ±ºæ¹æ¡ï¼ä¸»è¦æ¯åºæ¼ CNNï¼é¢è¨ç¼ºä¹æ´é«èæ¯çè§£çåé¡ãæèï¼ä¸äºæ¹æ³è¨´è«¸æ¼å¤§è¦æ¨¡ Transformer æ¨¡åä¾å½åæ´é«èæ¯å·®è·ï¼ä½ç§ç²äºæ¨¡åå¤§å°åè¨ç®è¤éåº¦ãæå¾ï¼è¨±å¤åºæ¼ Transformer çæ¹æ³ä¸»è¦ä¾è³´æ¼åºæ¼ CNN çè§£ç¢¼å¨ï¼èå¿½è¦äºåºæ¼ Transformer çè§£ç¢¼æ¨¡åçåªé»ãèªè­å°éäºéå¶ï¼æåééå¼å¥ MobileUNETR ä¾è§£æ±ºå°é«æè¼éç´è§£æ±ºæ¹æ¡çéæ±ï¼å¶ç®æ¨æ¯åæè CNN å Transformer ç¸éçæè½éå¶ï¼åææå°åæ¨¡åå¤§å°ï¼çºé«æå½±ååå²éåºæå¸æçä¸æ­¥ãMobileUNETR æ 3 åä¸»è¦ç¹é»ã1) MobileUNETR åå«ä¸åè¼éç´æ··å CNN-Transformer ç·¨ç¢¼å¨ï¼ä»¥ææçæ¹å¼å¹«å©å¹³è¡¡å±é¨åæ´é«èæ¯ç¹å¾µæåï¼2) ä¸åæ°ç©çæ··åè§£ç¢¼å¨ï¼å¨è§£ç¢¼éæ®µåæå©ç¨ä¸åè§£æåº¦ä¸çä½éåæ´é«ç¹å¾µï¼ä»¥é²è¡ç²¾ç¢ºçé®ç½©çæï¼3) è¶è¶å¤§åèè¤éçæ¶æ§ï¼MobileUNETR ä»¥ 300 è¬ååæ¸å 1.3 GFLOP çè¨ç®è¤éåº¦å¯¦ç¾äºåè¶çæè½ï¼åå¥æ¸å°äº 10 åå 23 åçåæ¸å FLOPãå·²ç¶é²è¡äºå»£æ³çå¯¦é©ï¼ä»¥é©è­æåæåºçæ¹æ³å¨ååå¬éå¯ç¨çç®èçè®åå²è³æéï¼åæ¬ ISIC 2016ãISIC 2017ãISIC 2018 å PH2 è³æéï¼ä¸çæææ§ãç¨å¼ç¢¼å°å¬éæ¼ï¼https://github.com/OSUPCVLab/MobileUNETR.git

##### **Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**
2409.02883v1 by Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won

Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to
assess cognitive functions such as visuospatial skills and memory, making them
valuable tools for detecting mild cognitive impairment (MCI). Despite their
utility, existing predictive models based on these tests often suffer from
limitations like small sample sizes and lack of external validation, which
undermine their reliability. We developed a multi-stream deep learning
framework that integrates two distinct processing streams: a multi-head
self-attention based spatial stream using raw RCFT images and a scoring stream
employing a previously developed automated scoring system. Our model was
trained on data from 1,740 subjects in the Korean cohort and validated on an
external hospital dataset of 222 subjects from Korea. The proposed multi-stream
model demonstrated superior performance over baseline models (AUC = 0.872,
Accuracy = 0.781) in external validation. The integration of both spatial and
scoring streams enables the model to capture intricate visual details from the
raw images while also incorporating structured scoring data, which together
enhance its ability to detect subtle cognitive impairments. This dual approach
not only improves predictive accuracy but also increases the robustness of the
model, making it more reliable in diverse clinical settings. Our model has
practical implications for clinical settings, where it could serve as a
cost-effective tool for early MCI screening.

æè¦ï¼é·æ°è¤éåå½¢æ¸¬é© (RCFT) ç­ç¹ªç«æ¸¬é©å»£æ³ç¨æ¼è©ä¼°è¦è¦ºç©ºéæè½åè¨æ¶åç­èªç¥åè½ï¼ä½¿å¶æçºæª¢æ¸¬è¼åº¦èªç¥éç¤ (MCI) çå¯¶è²´å·¥å·ãåç®¡å®åå¾æç¨ï¼ä½åºæ¼éäºæ¸¬é©çç¾æé æ¸¬æ¨¡åéå¸¸æåå°æ¨£æ¬éå°åç¼ºä¹å¤é¨é©è­ç­éå¶ï¼éææå®³å¶å¯é æ§ãæåéç¼äºä¸åå¤ä¸²æµæ·±åº¦å­¸ç¿æ¡æ¶ï¼å®æ´åäºå©åä¸åçèçä¸²æµï¼ä¸ååºæ¼å¤é ­èªæ³¨æåï¼ä½¿ç¨åå§ RCFT å½±åçç©ºéä¸²æµï¼ä»¥åä¸åæ¡ç¨ååéç¼çèªåè©åç³»çµ±çè©åä¸²æµãæåçæ¨¡åå¨éåç¾¤çµä¸­ 1,740 ååè©¦èçè³æä¸é²è¡è¨ç·´ï¼ä¸¦å¨ä¾èªéåç 222 ååè©¦èçå¤é¨é«é¢è³æéä¸é²è¡é©è­ãææåºçå¤ä¸²æµæ¨¡åå¨å¤é¨é©è­ä¸­è¡¨ç¾åºåªæ¼åºæºæ¨¡åçæè½ (AUC = 0.872ï¼æºç¢ºç = 0.781)ãç©ºéåè©åä¸²æµçæ´åä½¿æ¨¡åè½å¤ å¾åå§å½±åæ·åè¤éçè¦è¦ºç´°ç¯ï¼åæä¹è½ç´å¥çµæ§åçè©åè³æï¼éå±åå¢å¼·äºå®æª¢æ¸¬ç´°å¾®èªç¥éç¤çè½åãéç¨®ééæ¹æ³ä¸åæé«äºé æ¸¬æºç¢ºæ§ï¼ä¹å¢å äºæ¨¡åçç©©å¥æ§ï¼ä½¿å¶å¨ä¸åçè¨åºç°å¢ä¸­æ´å¯é ãæåçæ¨¡åå°è¨åºç°å¢æå¯¦éçæç¾©ï¼å®å¯ä»¥å¨å¶ä¸­ä½çºæ©æ MCI ç¯©æª¢çå·ææ¬æççå·¥å·ã

##### **Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**
2409.02681v1 by Ramon Tavares

This study presents a comprehensive methodology for modeling and forecasting
the historical time series of fire spots detected by the AQUA_M-T satellite in
the Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network
(RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit
(GRU) architectures to predict monthly accumulations of daily detected fire
spots. A summary of the data revealed a consistent seasonality over time, with
annual maximum and minimum fire spot values tending to repeat at the same
periods each year. The primary objective is to verify whether the forecasts
capture this inherent seasonality through rigorous statistical analysis. The
methodology involved careful data preparation, model configuration, and
training using cross-validation with two seeds, ensuring that the data
generalizes well to the test and validation sets, and confirming the
convergence of the model parameters. The results indicate that the mixed LSTM
and GRU model offers improved accuracy in forecasting 12 months ahead,
demonstrating its effectiveness in capturing complex temporal patterns and
modeling the observed time series. This research significantly contributes to
the application of deep learning techniques in environmental monitoring,
specifically in fire spot forecasting. In addition to improving forecast
accuracy, the proposed approach highlights the potential for adaptation to
other time series forecasting challenges, opening new avenues for research and
development in machine learning and natural phenomenon prediction. Keywords:
Time Series Forecasting, Recurrent Neural Networks, Deep Learning.

æè¦ï¼æ¬ç ç©¶æåºäºä¸åå¨é¢çæ¹æ³ï¼ç¨æ¼å»ºæ¨¡åé æ¸¬å·´è¥¿äºé¦¬éå°åç± AQUA_M-T è¡æåµæ¸¬å°çæ­·å²ç«ç½é»æéåºåãè©²æ¹æ³æ¡ç¨æ··åéè¿´ç¥ç¶ç¶²è·¯ (RNN) æ¨¡åï¼çµåé·ç­æè¨æ¶ (LSTM) åéæ§éè¿´å®å (GRU) æ¶æ§ï¼ä»¥é æ¸¬æ¯æ¥åµæ¸¬ç«ç½é»çæç´¯è¨å¼ãå°è³æçæè¦é¡¯ç¤ºåºé¨èæéæ¨ç§»èåºç¾çä¸è´å­£ç¯æ§ï¼æ¯å¹´çå¹´åº¦æå¤§åæå°ç«ç½é»å¼å¾åæ¼å¨åä¸ææéè¤åºç¾ãä¸»è¦ç®æ¨æ¯ééå´è¬¹ççµ±è¨åæé©è­é æ¸¬æ¯å¦ææå°éç¨®åºæçå­£ç¯æ§ãè©²æ¹æ³æ¶åä»ç´°çè³ææºåãæ¨¡åéç½®ï¼ä»¥åä½¿ç¨å©åç¨®å­çäº¤åé©è­é²è¡è¨ç·´ï¼ç¢ºä¿è³æè½å¾å¥½å°æ¨å»£å°æ¸¬è©¦åé©è­éï¼ä¸¦ç¢ºèªæ¨¡ååæ¸çæ¶ææ§ãçµæè¡¨æï¼æ··å LSTM å GRU æ¨¡åå¨é æ¸¬ 12 åæå¾æä¾äºæ´é«çæºç¢ºåº¦ï¼è­æäºå¶å¨ææè¤éæéæ¨¡å¼åå»ºæ¨¡è§æ¸¬æéåºåæ¹é¢çæææ§ãæ¬ç ç©¶é¡¯èå°ä¿è¿äºæ·±åº¦å­¸ç¿æè¡å¨ç°å¢ç£æ¸¬ä¸­çæç¨ï¼ç¹å¥æ¯å¨ç«ç½é»é æ¸¬æ¹é¢ãé¤äºæé«é æ¸¬æºç¢ºåº¦å¤ï¼ææåºçæ¹æ³éå¼·èª¿äºé©æå¶ä»æéåºåé æ¸¬ææ°çæ½åï¼çºæ©å¨å­¸ç¿åèªç¶ç¾è±¡é æ¸¬çç ç©¶åéç¼éé¢äºæ°çéå¾ãééµå­ï¼æéåºåé æ¸¬ãéè¿´ç¥ç¶ç¶²è·¯ãæ·±åº¦å­¸ç¿ã

##### **SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**
2409.02598v1 by Wenwu Guo, Jinlin Wu, Zhen Chen, Qingxiang Zhao, Miao Xu, Zhen Lei, Hongbin Liu

Vision-based surgical navigation has received increasing attention due to its
non-invasive, cost-effective, and flexible advantages. In particular, a
critical element of the vision-based navigation system is tracking surgical
instruments. Compared with 2D instrument tracking methods, 3D instrument
tracking has broader value in clinical practice, but is also more challenging
due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models
for 3D registration. To solve these challenges, we propose the SurgTrack, a
two-stage 3D instrument tracking method for CAD-free and robust real-world
applications. In the first registration stage, we incorporate an Instrument
Signed Distance Field (SDF) modeling the 3D representation of instruments,
achieving CAD-freed 3D registration. Due to this, we can obtain the location
and orientation of instruments in the 3D space by matching the video stream
with the registered SDF model. In the second tracking stage, we devise a
posture graph optimization module, leveraging the historical tracking results
of the posture memory pool to optimize the tracking results and improve the
occlusion robustness. Furthermore, we collect the Instrument3D dataset to
comprehensively evaluate the 3D tracking of surgical instruments. The extensive
experiments validate the superiority and scalability of our SurgTrack, by
outperforming the state-of-the-arts with a remarkable improvement. The code and
dataset are available at https://github.com/wenwucode/SurgTrack.

æè¦ï¼<paragraph>åºæ¼è¦è¦ºçå¤ç§å°èªç±æ¼å¶éä¾µå¥æ§ãææ¬æçåéæ´»æ§åªå¢èåå°è¶ä¾è¶å¤çéæ³¨ãç¹å¥æ¯ï¼åºæ¼è¦è¦ºçå°èªç³»çµ±çä¸åééµåç´ æ¯è¿½è¹¤æè¡å¨æ¢°ãè 2D å¨æ¢°è¿½è¹¤æ¹æ³ç¸æ¯ï¼3D å¨æ¢°è¿½è¹¤å¨è¨åºå¯¦åä¸­å·ææ´å»£æ³çå¹å¼ï¼ä½ç±æ¼ç´çå¼±ãé®æåç¼ºä¹ç¨æ¼ 3D éæºçé»è¦è¼å©è¨­è¨ (CAD) æ¨¡åï¼å æ­¤ä¹æ´å·ææ°æ§ãçºäºè§£æ±ºéäºææ°ï¼æåæåº SurgTrackï¼ä¸ç¨®é©ç¨æ¼ç¡ CAD åç©©å¥ççå¯¦ä¸çæç¨ç¨å¼çå©éæ®µ 3D å¨æ¢°è¿½è¹¤æ¹æ³ãå¨ç¬¬ä¸åéæºéæ®µï¼æåæ´åä¸åå¨æ¢°ç°½ç½²è·é¢å ´ (SDF)ï¼å°å¨æ¢°ç 3D è¡¨å¾µé²è¡å»ºæ¨¡ï¼å¯¦ç¾ç¡ CAD ç 3D éæºãå æ­¤ï¼æåå¯ä»¥ééå°è¦è¨ä¸²æµèå·²éæºç SDF æ¨¡åé²è¡å¹éï¼åå¾å¨æ¢°å¨ 3D ç©ºéä¸­çä½ç½®åæ¹åãå¨ç¬¬äºåè¿½è¹¤éæ®µï¼æåè¨­è¨ä¸åå§¿å¢åæä½³åæ¨¡çµï¼å©ç¨å§¿å¢è¨æ¶æ± çæ­·å²è¿½è¹¤çµæä¾æä½³åè¿½è¹¤çµæä¸¦æ¹åé®æçç©©å¥æ§ãæ­¤å¤ï¼æåæ¶é Instrument3D è³æéï¼ä»¥å¨é¢è©ä¼°æè¡å¨æ¢°ç 3D è¿½è¹¤ãå»£æ³çå¯¦é©é©è­äºæå SurgTrack çåªè¶æ§åå¯æ´åæ§ï¼ä»¥é¡¯èçæ¹é²åªæ¼ç¾ææè¡ãç¨å¼ç¢¼åè³æéå¯å¨ https://github.com/wenwucode/SurgTrack åå¾ã</paragraph>

##### **Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**
2409.02530v1 by Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of
kidney function in clinical practice. Although traditional equations and
Machine Learning (ML) models using clinical and laboratory data can estimate
eGFR, accurately predicting future eGFR levels remains a significant challenge
for nephrologists and ML researchers. Recent advances demonstrate that Large
Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust
foundation models for diverse applications. This study investigates the
potential of LMMs to predict future eGFR levels with a dataset consisting of
laboratory and clinical values from 50 patients. By integrating various
prompting techniques and ensembles of LMMs, our findings suggest that these
models, when combined with precise prompts and visual representations of eGFR
trajectories, offer predictive performance comparable to existing ML models.
This research extends the application of foundation models and suggests avenues
for future studies to harness these models in addressing complex medical
forecasting challenges.

æè¦ï¼ä¼°è¨çèå°çéæ¿¾ç (eGFR) æ¯è¨åºå¯¦åä¸­èèåè½çéè¦ææ¨ãéç¶å³çµ±æ¹ç¨å¼åä½¿ç¨è¨åºèå¯¦é©å®¤è³æçæ©å¨å­¸ç¿ (ML) æ¨¡åå¯ä»¥ä¼°è¨ eGFRï¼ä½æºç¢ºé æ¸¬æªä¾ eGFR æ°´å¹³ä»ç¶æ¯èèç§é«å¸«å ML ç ç©¶äººå¡çä¸å¤§ææ°ãæè¿çç ç©¶é²å±é¡¯ç¤ºï¼å¤§åèªè¨æ¨¡å (LLM) åå¤§åå¤æ¨¡ææ¨¡å (LMM) å¯ä»¥ä½çºåç¨®æç¨ç¨å¼çå¼·å¥åºç¤æ¨¡åãæ¬ç ç©¶æ¢è¨ LMM é æ¸¬æªä¾ eGFR æ°´å¹³çæ½åï¼å¶è³æéåå« 50 ä½çæ£çå¯¦é©å®¤åè¨åºæ¸å¼ãééæ´ååç¨®æç¤ºæè¡å LMM çåå¥ï¼æåçç ç©¶çµæé¡¯ç¤ºï¼éäºæ¨¡åå¨çµåç²¾ç¢ºæç¤ºå eGFR è»è·¡çè¦è¦ºåè¡¨ç¤ºæï¼å¯æä¾èç¾æ ML æ¨¡åç¸è¿çé æ¸¬æè½ãéé ç ç©¶æ´å±äºåºç¤æ¨¡åçæç¨ï¼ä¸¦çºæªä¾ç ç©¶å©ç¨éäºæ¨¡åä¾æå°è¤éçé«çé æ¸¬ææ°æä¾äºéå¾ã

##### **Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**
2409.02337v1 by Deepak Raina, Mythra V. Balakuntala, Byung Wook Kim, Juan Wachs, Richard Voyles

Ultrasound is widely employed for clinical intervention and diagnosis, due to
its advantages of offering non-invasive, radiation-free, and real-time imaging.
However, the accessibility of this dexterous procedure is limited due to the
substantial training and expertise required of operators. The robotic
ultrasound (RUS) offers a viable solution to address this limitation;
nonetheless, achieving human-level proficiency remains challenging. Learning
from demonstrations (LfD) methods have been explored in RUS, which learns the
policy prior from a dataset of offline demonstrations to encode the mental
model of the expert sonographer. However, active engagement of experts, i.e.
Coaching, during the training of RUS has not been explored thus far. Coaching
is known for enhancing efficiency and performance in human training. This paper
proposes a coaching framework for RUS to amplify its performance. The framework
combines DRL (self-supervised practice) with sparse expert's feedback through
coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a
reward based on image quality rating. The coaching by experts is modeled as a
Partially Observable Markov Decision Process (POMDP), which updates the policy
parameters based on the correction by the expert. The validation study on
phantoms showed that coaching increases the learning rate by $25\%$ and the
number of high-quality image acquisition by $74.5\%$.

æè¦ï¼è¶é³æ³¢å å¶æä¾éä¾µå¥æ§ãç¡è¼»å°ä¸å³æå½±åçåªé»ï¼èå»£æ³ç¨æ¼è¨åºä»å¥åè¨ºæ·ã
ç¶èï¼ç±æ¼æä½å¡éè¦å¤§éçè¨ç·´åå°æ¥­ç¥è­ï¼éå¶äºæ­¤éæ´»ç¨åºçå¯åæ§ãæ©å¨äººè¶é³æ³¢ (RUS) æä¾äºä¸åå¯è¡çè§£æ±ºæ¹æ¡ä¾è§£æ±ºæ­¤éå¶ï¼
åç®¡å¦æ­¤ï¼è¦éå°äººé¡ç­ç´ççç·´åº¦ä»ç¶å·æææ°æ§ãå­¸ç¿ç¤ºç¯ (LfD) æ¹æ³å·²å¨ RUS ä¸­é²è¡æ¢è¨ï¼å®å¾é¢ç·ç¤ºç¯çè³æéå­¸ç¿åé©ç­ç¥ï¼ä»¥ç·¨ç¢¼å°å®¶è¶é³æ³¢æª¢æ¥å¡çå¿æºæ¨¡åãç¶èï¼è¿ä»å°æªæ¢è¨å°å®¶å¨ RUS è¨ç·´æéçç©æ¥µåèï¼å³æå°ãæå°å·²ç¥å¯ä»¥æé«äººé¡è¨ç·´çæçåç¸¾æãæ¬ææåºäºä¸å RUS æå°æ¶æ§ï¼ä»¥æåå¶ç¸¾æãæ­¤æ¶æ§çµåäº DRLï¼èªæç£ç£å¯¦åï¼èééæå°æä¾çå°å®¶ç¨çåé¥ãDRL ä½¿ç¨é¢ç·ç­ç¥è»æ§åä½-è©è« (SAC) ç¶²è·¯ï¼ä¸¦æ ¹æå½±ååè³ªè©åçµ¦äºçåµãå°å®¶çæå°è¢«å»ºæ¨¡çºé¨åå¯è§å¯é¦¬å¯å¤«æ±ºç­éç¨ (POMDP)ï¼å®æ ¹æå°å®¶çä¿®æ­£ä¾æ´æ°ç­ç¥åæ¸ãå¨æ¨¡æ¬äººé«æ¨¡åä¸çé©è­ç ç©¶é¡¯ç¤ºï¼æå°å°å­¸ç¿çæé«äº $25\%$ï¼é«åè³ªå½±åæ·åæ¸éæé«äº $74.5\%$ã

##### **Action-Based ADHD Diagnosis in Video**
2409.02261v1 by Yichun Li, Yuxing Yang, Syed Nohsen Naqvi

Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment
in various domains. Early diagnosis of ADHD and treatment could significantly
improve the quality of life and functioning. Recently, machine learning methods
have improved the accuracy and efficiency of the ADHD diagnosis process.
However, the cost of the equipment and trained staff required by the existing
methods are generally huge. Therefore, we introduce the video-based frame-level
action recognition network to ADHD diagnosis for the first time. We also record
a real multi-modal ADHD dataset and extract three action classes from the video
modality for ADHD diagnosis. The whole process data have been reported to
CNTW-NHS Foundation Trust, which would be reviewed by medical
consultants/professionals and will be made public in due course.

æè¦ï¼æ³¨æåç¼ºé·éåç (ADHD) æå¨åç¨®é åé æé¡¯èçæå®³ãææ©è¨ºæ· ADHD ä¸¦æ¥åæ²»çå¯ä»¥å¤§å¹æ¹åçæ´»åè³ªååè½ãæè¿ï¼æ©å¨å­¸ç¿æ¹æ³å·²ç¶æåäº ADHD è¨ºæ·ç¨åºçæºç¢ºåº¦åæçãç¶èï¼ç¾ææ¹æ³æéçè¨­ååè¨ç·´æç´ çäººå¡ææ¬éå¸¸å¾é«ãå æ­¤ï¼æåé¦æ¬¡å°åºæ¼å½±ççå¹ç´åä½è¾¨è­ç¶²è·¯å¼å¥ ADHD è¨ºæ·ãæåä¹è¨éäºä¸åçæ­£çå¤æ¨¡å¼ ADHD è³æéï¼ä¸¦å¾å½±çæ¨¡å¼ä¸­èååºä¸ååä½é¡å¥ä»¥é²è¡ ADHD è¨ºæ·ãæ´åæµç¨çè³æå·²ç¶åå ±çµ¦ CNTW-NHS åºéæï¼å°ç±é«çé¡§å/å°æ¥­äººå£«å¯©æ¥ï¼ä¸¦å°é©æå¬éã

##### **A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**
2409.02069v1 by Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy

Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized individuals at risk for dental disease.
Oralytics incorporates an online reinforcement learning algorithm to determine
optimal times to deliver intervention prompts that encourage oral self-care
behaviors. We have deployed Oralytics in a registered clinical trial. The
deployment required careful design to manage challenges specific to the
clinical trials setting in the U.S. In this paper, we (1) highlight key design
decisions of the RL algorithm that address these challenges and (2) conduct a
re-sampling analysis to evaluate algorithm design decisions. A second phase
(randomized control trial) of Oralytics is planned to start in spring 2025.

æè¦ï¼çç§ç¾çæ¯ä¸ç¨®æ®éçæ¢æ§ç¾çï¼èå¤§éçç¶æ¿è² æãåäººçè¦åå¢å çå¨èº«ç¾çé¢¨éªæéãåç®¡æ®éå»ºè­°æ¯å¤©å·çå©æ¬¡ï¼ä½ç±æ¼å¥å¿åè«é¢ç­å ç´ ï¼å°å»ºè­°çå£èèªæä¿å¥è¡çºçä¾å¾æ§ä»ç¶ä½æ¼æä½³æ°´å¹³ãçºäºè§£æ±ºéååé¡ï¼æåéç¼äº Oralyticsï¼ä¸å mHealth ä»å¥ç³»çµ±ï¼æ¨å¨è£åè¨åºé«çæä¾çé é²ä¿å¥ï¼ä»¥é é²æçç§ç¾çé¢¨éªçéç·£ååäººãOralytics çµåäºä¸åå¨ç·å¼·åå­¸ç¿æ¼ç®æ³ï¼ä»¥ç¢ºå®æä¾ä»å¥æç¤ºçæä½³æéï¼éäºæç¤ºé¼åµå£èèªæä¿å¥è¡çºãæåå·²å¨è¨»åçè¨åºè©¦é©ä¸­é¨ç½²äº Oralyticsãè©²é¨ç½²éè¦ä»ç´°çè¨­è¨ä¾ç®¡çç¾åè¨åºè©¦é©è¨­ç½®ä¸­å·é«çææ°ãå¨æ¬æä¸­ï¼æåï¼1ï¼éé»ä»ç´¹äºè§£æ±ºéäºææ°ç RL æ¼ç®æ³çééµè¨­è¨æ±ºç­ï¼ä»¥åï¼2ï¼é²è¡éæ°æ½æ¨£åæä»¥è©ä¼°æ¼ç®æ³è¨­è¨æ±ºç­ãOralytics çç¬¬äºéæ®µï¼é¨æ©å°ç§è©¦é©ï¼è¨åæ¼ 2025 å¹´æ¥å­£éå§ã

##### **TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**
2409.02018v1 by Bobby Azad, Pourya Adibfar, Kaiqun Fu

In healthcare, medical image segmentation is crucial for accurate disease
diagnosis and the development of effective treatment strategies. Early
detection can significantly aid in managing diseases and potentially prevent
their progression. Machine learning, particularly deep convolutional neural
networks, has emerged as a promising approach to addressing segmentation
challenges. Traditional methods like U-Net use encoding blocks for local
representation modeling and decoding blocks to uncover semantic relationships.
However, these models often struggle with multi-scale objects exhibiting
significant variations in texture and shape, and they frequently fail to
capture long-range dependencies in the input data. Transformers designed for
sequence-to-sequence predictions have been proposed as alternatives, utilizing
global self-attention mechanisms. Yet, they can sometimes lack precise
localization due to insufficient granular details. To overcome these
limitations, we introduce TransDAE: a novel approach that reimagines the
self-attention mechanism to include both spatial and channel-wise associations
across the entire feature space, while maintaining computational efficiency.
Additionally, TransDAE enhances the skip connection pathway with an inter-scale
interaction module, promoting feature reuse and improving localization
accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on
the Synaps multi-organ dataset, even without relying on pre-trained weights.

æè¦ï¼å¨å»çä¿å¥é¢åï¼å»å­¦å½±ååå²å¯¹äºåç¡®çç¾çè¯æ­åæææ²»çç­ç¥çå¼åè³å³éè¦ãæ©ææ£æµå¯ä»¥æå¤§å°å¸®å©æ§å¶ç¾çï¼å¹¶å¯è½é²æ­¢ç¾çè¿å±ãæºå¨å­¦ä¹ ï¼å°¤å¶æ¯æ·±åº¦å·ç§¯ç¥ç»ç½ç»ï¼å·²æä¸ºè§£å³åå²ææçä¸ç§æåéçæ¹æ³ãU-Net ç­ä¼ ç»æ¹æ³ä½¿ç¨ç¼ç åè¿è¡å±é¨è¡¨ç¤ºå»ºæ¨¡åè§£ç åæ¥æ­ç¤ºè¯­ä¹å³ç³»ãç¶èï¼è¿äºæ¨¡åéå¸¸é¾ä»¥å¤çå¨çº¹çåå½¢ç¶ä¸è¡¨ç°åºæ¾çååçå¤å°ºåº¦å¯¹è±¡ï¼å¹¶ä¸å®ä»¬ç»å¸¸æ æ³æè·è¾å¥æ°æ®ä¸­çè¿ç¨ä¾èµå³ç³»ãä¸ä¸ºåºåå°åºåé¢æµèè®¾è®¡ç Transformer å·²è¢«æåºä½ä¸ºæ¿ä»£æ¹æ¡ï¼å©ç¨å¨å±èªæ³¨æåæºå¶ãç¶èï¼ç±äºç²åº¦ç»èä¸è¶³ï¼å®ä»¬ææ¶å¯è½ç¼ºä¹ç²¾ç¡®çå®ä½ãä¸ºäºåæè¿äºéå¶ï¼æä»¬å¼å¥äº TransDAEï¼ä¸ç§æ°é¢çæ¹æ³ï¼å®éæ°ææ³äºèªæ³¨æåæºå¶ï¼ä»¥åå«æ´ä¸ªç¹å¾ç©ºé´ä¸­çç©ºé´åééå³èï¼åæ¶ä¿æè®¡ç®æçãæ­¤å¤ï¼TransDAE éè¿å°ºåº¦é´äº¤äºæ¨¡åå¢å¼ºäºè·³è·è¿æ¥è·¯å¾ï¼ä¿è¿äºç¹å¾éç¨å¹¶æé«äºå®ä½ç²¾åº¦ãå¼å¾æ³¨æçæ¯ï¼å³ä½¿ä¸ä¾èµé¢è®­ç»æéï¼TransDAE å¨ Synaps å¤å¨å®æ°æ®éä¸ä¹ä¼äºç°æçæåè¿æ¹æ³ã

##### **A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**
2409.01903v1 by Abdelmalek Mouazer, Sophie Dubois, Romain LÃ©guillon, Nada Boudegzdame, Thibaud Levrard, Yoann Le Bars, Christian Simon, Brigitte SÃ©roussi, Julien Grosjean, Romain Lelong, Catherine Letord, StÃ©fan Darmoni, Karima Sedki, Pierre Meneton, Rosy Tsopra, Hector Falcoff, Jean-Baptiste Lamy

Background: Medication review is a structured interview of the patient,
performed by the pharmacist and aimed at optimizing drug treatments. In
practice, medication review is a long and cognitively-demanding task that
requires specific knowledge. Clinical practice guidelines have been proposed,
but their application is tedious. Methods: We designed ABiMed, a clinical
decision support system for medication reviews, based on the implementation of
the STOPP/START v2 guidelines and on the visual presentation of aggregated drug
knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39
community pharmacists during a randomized simulation trial, each pharmacist
performing a medication review for two fictitious patients without ABiMed, and
two others with ABiMed. We recorded the problems identified by the pharmacists,
the interventions proposed, the response time, the perceived usability and the
comments. Pharmacists' medication reviews were compared to an expert-designed
gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant
drug-related problems during the medication review (p=1.1e-12) and proposed
better interventions (p=9.8e-9), without needing more time (p=0.56). The System
Usability Scale score is 82.7, which is ranked "excellent". In their comments,
pharmacists appreciated the visual aspect of ABiMed and its ability to compare
the current treatment with the proposed one. A multifactor analysis showed no
difference in the support offered by ABiMed according to the pharmacist's age
or sex, in terms of percentage of problems identified or quality of the
proposed interventions. Conclusions: The use of an intelligent and visual
clinical decision support system can help pharmacists when they perform
medication reviews. Our main perspective is the validation of the system in
clinical conditions.

æè¦ï¼<paragraph>èæ¯ï¼ç¨è¥å¯©æ¥æ¯ç±è¥å¸«å·è¡çä¸ç¨®çµæ§åæ£èè¨ªè«ï¼ç®çå¨æ¼åªåè¥ç©æ²»çãå¨å¯¦åä¸ï¼ç¨è¥å¯©æ¥æ¯ä¸é åé·ä¸èªç¥éæ±é«çä»»åï¼éè¦å·åç¹å®ç¥è­ãéç¶å·²æåºè¨åºå¯¦åæå¼ï¼ä½å¶æç¨å¾ç¹ç£ãæ¹æ³ï¼æåæ ¹æ STOPP/START v2 æå¼çå¯¦ä½ï¼ä¸¦ä½¿ç¨è¡¨æ ¼ãåè¡¨åè±å½¢ç¬¦èè¦è¦ºååç¾å½æ´çè¥ç©ç¥è­ï¼è¨­è¨äºä¸å¥ç¨è¥å¯©æ¥çè¨åºæ±ºç­æ¯æ´ç³»çµ± ABiMedãæåå¨é¨æ©æ¨¡æ¬è©¦é©ä¸­ï¼è® 39 ä½ç¤¾åè¥å¸«è©ä¼° ABiMedï¼æ¯ä½è¥å¸«éå°å©ä½èæ§æ£èå·è¡ç¨è¥å¯©æ¥ï¼å©æ¬¡æ²æä½¿ç¨ ABiMedï¼å©æ¬¡ä½¿ç¨ ABiMedãæåè¨éäºè¥å¸«è­å¥åºçåé¡ãå»ºè­°çä»å¥æªæ½ãåææéãæç¥å¯ç¨æ§åè©è«ãå°è¥å¸«çç¨è¥å¯©æ¥èå°å®¶è¨­è¨çéæ¨æºé²è¡æ¯è¼ãçµæï¼ä½¿ç¨ ABiMed å¾ï¼è¥å¸«å¨ç¨è¥å¯©æ¥æéç¼ç¾äºå¤ 1.6 åç¸éçè¥ç©ç¸éåé¡ï¼p=1.1e-12ï¼ï¼ä¸¦æåºæ´å¥½çä»å¥æªæ½ï¼p=9.8e-9ï¼ï¼èç¡éè±è²»æ´å¤æéï¼p=0.56ï¼ãç³»çµ±å¯ç¨æ§è©åçº 82.7ï¼è¢«è©çºãåªè¯ããå¨ä»åçè©è«ä¸­ï¼è¥å¸«è®è³ ABiMed çè¦è¦ºåé¢åï¼ä»¥åå®æ¯è¼ç®åæ²»çèå»ºè­°æ²»ççè½åãå¤å ç´ åæé¡¯ç¤ºï¼ABiMed æä¾çæ¯æ´å¨è¥å¸«çå¹´é½¡ææ§å¥æ¹é¢æ²æå·®ç°ï¼å°±è­å¥åºçåé¡ç¾åæ¯æå»ºè­°ä»å¥æªæ½çåè³ªèè¨ãçµè«ï¼ä½¿ç¨æºæ§ä¸è¦è¦ºåçè¨åºæ±ºç­æ¯æ´ç³»çµ±ï¼å¯ä»¥åå©è¥å¸«å·è¡ç¨è¥å¯©æ¥ãæåçè§é»ä¸»è¦æ¯é©è­ç³»çµ±å¨è¨åºæ¢ä»¶ä¸çæåº¦ã</paragraph>

##### **Training on the Benchmark Is Not All You Need**
2409.01790v1 by Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang

The success of Large Language Models (LLMs) relies heavily on the huge amount
of pre-training data learned in the pre-training phase. The opacity of the
pre-training process and the training data causes the results of many benchmark
tests to become unreliable. If any model has been trained on a benchmark test
set, it can seriously hinder the health of the field. In order to automate and
efficiently test the capabilities of large language models, numerous mainstream
benchmarks adopt a multiple-choice format. As the swapping of the contents of
multiple-choice options does not affect the meaning of the question itself, we
propose a simple and effective data leakage detection method based on this
property. Specifically, we shuffle the contents of the options in the data to
generate the corresponding derived data sets, and then detect data leakage
based on the model's log probability distribution over the derived data sets.
If there is a maximum and outlier in the set of log probabilities, it indicates
that the data is leaked. Our method is able to work under black-box conditions
without access to model training data or weights, effectively identifying data
leakage from benchmark test sets in model pre-training data, including both
normal scenarios and complex scenarios where options may have been shuffled
intentionally or unintentionally. Through experiments based on two LLMs and
benchmark designs, we demonstrate the effectiveness of our method. In addition,
we evaluate the degree of data leakage of 31 mainstream open-source LLMs on
four benchmark datasets and give a ranking of the leaked LLMs for each
benchmark, and we find that the Qwen family of LLMs has the highest degree of
data leakage.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çæåå¨å¾å¤§ç¨åº¦ä¸åæ±ºæ¼é è¨ç·´éæ®µä¸­å­¸ç¿å°çæµ·éé è¨ç·´æ¸æãé è¨ç·´éç¨åè¨ç·´æ¸æçä¸éææ§å°è´è¨±å¤åºæºæ¸¬è©¦ççµæè®å¾ä¸å¯é ãå¦æä»»ä½æ¨¡åå·²å¨åºæºæ¸¬è©¦éä¸­é²è¡è¨ç·´ï¼åå¯è½æå´éé»ç¤è©²é åçç¼å±ãçºäºèªååä¸ææå°æ¸¬è©¦å¤§åèªè¨æ¨¡åçè½åï¼è¨±å¤ä¸»æµåºæºæ¡ç¨å¤é¸é¡æ ¼å¼ãç±æ¼å¤é¸é¡é¸é å§å®¹çäºæä¸å½±é¿åé¡æ¬èº«çå«ç¾©ï¼å æ­¤æåæåºäºä¸ç¨®åºæ¼æ­¤å±¬æ§çç°¡å®ä¸ææçæ°æ®æ´©æ¼æª¢æ¸¬æ¹æ³ãå·é«ä¾èªªï¼æåå°æ¸æä¸­é¸é çå§å®¹é¨æ©æåä»¥çæå°æçæ´¾çæ¸æéï¼ç¶å¾æ ¹ææ¨¡åå¨æ´¾çæ¸æéä¸çå°æ¸æ¦çåä½æª¢æ¸¬æ¸ææ´©æ¼ãå¦æå°æ¸æ¦çéä¸­å­å¨æå¤§å¼åç°å¸¸å¼ï¼åè¡¨ç¤ºæ¸æå·²æ´©æ¼ãæåçæ¹æ³è½å¤ å¨ä¸è¨ªåæ¨¡åè¨ç·´æ¸æææ¬éçé»çæ¢ä»¶ä¸å·¥ä½ï¼ææå°è­å¥æ¨¡åé è¨ç·´æ¸æä¸­åºæºæ¸¬è©¦éçæ¸ææ´©æ¼ï¼åæ¬é¸é å¯è½å·²æææç¡æå°è¢«æäºçæ­£å¸¸å ´æ¯åè¤éå ´æ¯ãééåºæ¼å©å LLM ååºæºè¨­è¨çå¯¦é©ï¼æåè­æäºæåæ¹æ³çæææ§ãæ­¤å¤ï¼æåè©ä¼°äº 31 åä¸»æµéæº LLM å¨åååºæºæ¸æéä¸çæ¸ææ´©æ¼ç¨åº¦ï¼ä¸¦å°æ¯ååºæºçæ´©æ¼ LLM é²è¡äºæåï¼æåç¼ç¾ Qwen å®¶æç LLM å·ææé«çæ¸ææ´©æ¼ç¨åº¦ã

##### **Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**
2409.01676v1 by Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink

Deriving health indicators of rotating machines is crucial for their
maintenance. However, this process is challenging for the prevalent adopted
intelligent methods since they may take the whole data distributions, not only
introducing noise interference but also lacking the explainability. To address
these issues, we propose a diffusion-based weakly-supervised approach for
deriving health indicators of rotating machines, enabling early fault detection
and continuous monitoring of condition evolution. This approach relies on a
classifier-free diffusion model trained using healthy samples and a few
anomalies. This model generates healthy samples. and by comparing the
differences between the original samples and the generated ones in the envelope
spectrum, we construct an anomaly map that clearly identifies faults. Health
indicators are then derived, which can explain the fault types and mitigate
noise interference. Comparative studies on two cases demonstrate that the
proposed method offers superior health monitoring effectiveness and robustness
compared to baseline models.

æè¦ï¼æ¨å°æè½æ©å¨çå¥åº·ææ¨å°æ¼å¶ç¶­è­·è³ééè¦ãç¶èï¼éåéç¨å°æ®éæ¡ç¨çæºè½æ¹æ³ä¾èªªå·æææ°æ§ï¼å çºå®åå¯è½ææ¡ç¨æ´åè³æåä½ï¼ä¸åæå¼å¥éè¨å¹²æ¾ï¼èä¸ç¼ºä¹å¯è§£éæ§ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ç¨®åºæ¼æ´æ£çå¼±ç£ç£å¼æ¹æ³ï¼ç¨æ¼æ¨å°æè½æ©å¨çå¥åº·ææ¨ï¼å¯¦ç¾æ©ææéæª¢æ¸¬åçææ¼è®çæçºç£æ§ãéç¨®æ¹æ³ä¾è³´æ¼ä½¿ç¨å¥åº·æ¨£æ¬åä¸äºç°å¸¸å¼è¨ç·´çç¡åé¡å¨æ´æ£æ¨¡åãéåæ¨¡åæç¢çå¥åº·æ¨£æ¬ãä¸¦ä¸ééæ¯è¼å°å¥è­ä¸­åå§æ¨£æ¬åçææ¨£æ¬ä¹éçå·®ç°ï¼æåæ§å»ºäºä¸åç°å¸¸åï¼å¯ä»¥æ¸æ¥å°è­å¥æéãç¶å¾æ¨å°åºå¥åº·ææ¨ï¼å¯ä»¥è§£éæéé¡åä¸¦æ¸è¼éè¨å¹²æ¾ãå°å©åæ¡ä¾çæ¯è¼ç ç©¶è¡¨æï¼èåºæºæ¨¡åç¸æ¯ï¼ææåºçæ¹æ³æä¾äºåè¶çå¥åº·ç£æ§æææ§åé­¯æ£æ§ã

##### **A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**
2409.02145v1 by Zekang Yang, Hong Liu, Xiangdong Wang

Computer-aided cancer survival risk prediction plays an important role in the
timely treatment of patients. This is a challenging weakly supervised ordinal
regression task associated with multiple clinical factors involved such as
pathological images, genomic data and etc. In this paper, we propose a new
training method, multimodal object-level contrast learning, for cancer survival
risk prediction. First, we construct contrast learning pairs based on the
survival risk relationship among the samples in the training sample set. Then
we introduce the object-level contrast learning method to train the survival
risk predictor. We further extend it to the multimodal scenario by applying
cross-modal constrast. Considering the heterogeneity of pathological images and
genomics data, we construct a multimodal survival risk predictor employing
attention-based and self-normalizing based nerural network respectively.
Finally, the survival risk predictor trained by our proposed method outperforms
state-of-the-art methods on two public multimodal cancer datasets for survival
risk prediction.

æè¦ï¼é»è¦è¼å©ççå­æ´»é¢¨éªé æ¸¬å¨çæ£çåææ²»çä¸­æ®æ¼èéè¦çè§è²ãéæ¯ä¸åå°é£çå¼±ç£ç£åºæ¸åæ­¸ä»»åï¼èå¤éè¨åºå ç´ æéï¼ä¾å¦ççååãåºå çµæ¸æç­ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°çè¨ç·´æ¹æ³ï¼å¤æ¨¡æç©ä»¶å±¤ç´å°æ¯å­¸ç¿ï¼ç¨æ¼ççå­æ´»é¢¨éªé æ¸¬ãé¦åï¼æåæ ¹æè¨ç·´æ¨£æ¬éä¸­æ¨£æ¬ä¹éçå­æ´»é¢¨éªéä¿å»ºç«å°æ¯å­¸ç¿å°ãæ¥èï¼æåå¼å¥ç©ä»¶å±¤ç´å°æ¯å­¸ç¿æ¹æ³ä¾è¨ç·´å­æ´»é¢¨éªé æ¸¬å¨ãæåé²ä¸æ­¥å°å¶å»¶ä¼¸è³å¤æ¨¡æå ´æ¯ï¼ééæç¨è·¨æ¨¡æå°æ¯ãèéå°ççååååºå é«æ¸æçç°è³ªæ§ï¼æååå¥æ¡ç¨åºæ¼æ³¨æåçåèªæ¨æºåçç¥ç¶ç¶²è·¯ä¾å»ºæ§å¤æ¨¡æå­æ´»é¢¨éªé æ¸¬å¨ãæå¾ï¼æåæåºçæ¹æ³æè¨ç·´çå­æ´»é¢¨éªé æ¸¬å¨å¨å©åå¬éçå¤æ¨¡æççè³æéä¸ï¼å¨å­æ´»é¢¨éªé æ¸¬æ¹é¢åªæ¼æåé²çæ¹æ³ã

##### **A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**
2409.01596v1 by Ruben D. Fonnegra, Maria Liliana HernÃ¡ndez, Juan C. Caicedo, Gloria M. DÃ­az

Contrast-enhancement pattern analysis is critical in breast magnetic
resonance imaging (MRI) to distinguish benign from probably malignant tumors.
However, contrast-enhanced image acquisitions are time-consuming and very
expensive. As an alternative to physical acquisition, this paper proposes a
comprehensive pipeline for the generation of accurate long-term (late)
contrast-enhanced breast MRI from the early counterpart. The proposed strategy
focuses on preserving the contrast agent pattern in the enhanced regions while
maintaining visual properties in the entire synthesized images. To that end, a
novel loss function that leverages the biological behavior of contrast agent
(CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed
to optimize a pixel-attention based generative model. In addition, unlike
traditional normalization and standardization methods, we developed a new
normalization strategy that maintains the contrast enhancement pattern across
the image sequences at multiple timestamps. This ensures the prevalence of the
CA pattern after image preprocessing, unlike conventional approaches.
Furthermore, in order to objectively evaluate the clinical quality of the
synthesized images, two metrics are also introduced to measure the differences
between the TI curves of enhanced regions of the acquired and synthesized
images. The experimental results showed that the proposed strategy generates
images that significantly outperform diagnostic quality in contrast-enhanced
regions while maintaining the spatial features of the entire image. This
results suggest a potential use of synthetic late enhanced images generated via
deep learning in clinical scenarios.

æè¦ï¼å°æ¯å¢å¼·æ¨¡å¼åæå¨ä¹³æ¿ç£å±æ¯å½±å (MRI) ä¸­è³ééè¦ï¼å¯ç¨æ¼ååè¯æ§è«ç¤åå¯è½æ¯æ¡æ§è«ç¤ã
ç¶èï¼å°æ¯å¢å¼·å½±åçæ·åéå¸¸èæä¸æè²´ãä½çºç©çæ·åçæ¿ä»£æ¹æ¡ï¼æ¬ææåºäºä¸åå¨é¢çç®¡éï¼ç¨æ¼å¾æ©æå°æç©çææºç¢ºçé·æï¼ææï¼å°æ¯å¢å¼·ä¹³æ¿ MRIãææåºçç­ç¥èéæ¼å¨å¢å¼·ååä¸­ä¿çå°æ¯åæ¨¡å¼ï¼åæå¨æ´ååæå½±åä¸­ç¶­æè¦è¦ºå±¬æ§ãçºæ­¤ï¼æåºäºä¸ç¨®æ°ç©çæå¤±å½æ¸ï¼å©ç¨å°æ¯å (CA) å¨çµç¹ä¸­ççç©è¡çºï¼ç±æéå¼·åº¦ (TI) å¢å¼·æ²ç·çµ¦åºï¼ï¼ä»¥æä½³ååºæ¼åç´ æ³¨æåççææ¨¡åãæ­¤å¤ï¼èå³çµ±çæ­£è¦ååæ¨æºåæ¹æ³ä¸åï¼æåéç¼äºä¸ç¨®æ°çæ­£è¦åç­ç¥ï¼å¯å¨å¤åæéæ³çå½±ååºåä¸­ç¶­æå°æ¯å¢å¼·æ¨¡å¼ãéç¢ºä¿äºå½±ååèçå¾ CA æ¨¡å¼çæ®éæ§ï¼éèå³çµ±æ¹æ³ä¸åãæ­¤å¤ï¼çºäºå®¢è§è©ä¼°åæå½±åçè¨åºåè³ªï¼éå¼å¥äºå©åææ¨ä¾æ¸¬éæ·åååæå½±åçå¢å¼·ååç TI æ²ç·ä¹éçå·®ç°ãå¯¦é©çµæé¡¯ç¤ºï¼ææåºçç­ç¥ç¢ççå½±åå¨å°æ¯å¢å¼·ååä¸­çè¨ºæ·åè³ªæé¡¯åªæ¼å¶ä»å½±åï¼åæç¶­æäºæ´åå½±åçç©ºéç¹å¾µãéäºçµæè¡¨æï¼å¨è¨åºå ´æ¯ä¸­ï¼ééæ·±åº¦å­¸ç¿çæçåæææå¢å¼·å½±åå·ææ½å¨ç¨éã

##### **Kvasir-VQA: A Text-Image Pair GI Tract Dataset**
2409.01437v1 by Sushant Gautam, Andrea StorÃ¥s, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, PÃ¥l Halvorsen, Michael A. Riegler

We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and
Kvasir-Instrument datasets, augmented with question-and-answer annotations to
facilitate advanced machine learning tasks in Gastrointestinal (GI)
diagnostics. This dataset comprises 6,500 annotated images spanning various GI
tract conditions and surgical instruments, and it supports multiple question
types including yes/no, choice, location, and numerical count. The dataset is
intended for applications such as image captioning, Visual Question Answering
(VQA), text-based generation of synthetic medical images, object detection, and
classification. Our experiments demonstrate the dataset's effectiveness in
training models for three selected tasks, showcasing significant applications
in medical image analysis and diagnostics. We also present evaluation metrics
for each task, highlighting the usability and versatility of our dataset. The
dataset and supporting artifacts are available at
https://datasets.simula.no/kvasir-vqa.

æè¦ï¼æåå¼é² Kvasir-VQAï¼ä¸åç± HyperKvasir å Kvasir-Instrument è³æéè¡ççå»¶ä¼¸è³æéï¼ä¸¦å å¥åé¡èè§£ç­è¨»è§£ï¼ä»¥ä¿é²å¨èè¸ (GI) è¨ºæ·ä¸­çé²éæ©å¨å­¸ç¿ä»»åãæ­¤è³æéåå« 6,500 åè¨»è§£å½±åï¼æ¶µèåç¨® GI éçæ³åæè¡å¨æ¢°ï¼ä¸¦ä¸æ¯æ´åæ¬æ¯éé¡ãé¸æé¡ãä½ç½®åæ¸å­è¨æ¸ç­å¤ç¨®é¡åçåé¡ãæ­¤è³æéé©ç¨æ¼å½±åæ¨é¡ãè¦è¦ºåç­ (VQA)ãåæé«å­¸å½±åçæå­çæãç©ä»¶åµæ¸¬ååé¡ç­æç¨ç¨å¼ãæåçå¯¦é©è­ææ­¤è³æéå¨è¨ç·´ä¸åé¸å®ä»»åçæ¨¡åä¸­å·æææï¼å±ç¤ºäºå¨é«å­¸å½±ååæåè¨ºæ·ä¸­éè¦çæç¨ãæåä¹çºæ¯åä»»åæä¾è©ä¼°ææ¨ï¼çªé¡¯æåè³æéçå¯ç¨æ§åå¤åè½æ§ãæ­¤è³æéåæ¯æ´å·¥ä»¶å¯æ¼ https://datasets.simula.no/kvasir-vqa åå¾ã

##### **EEG-Language Modeling for Pathology Detection**
2409.07480v1 by Sam Gijsen, Kerstin Ritter

Multimodal language modeling constitutes a recent breakthrough which
leverages advances in large language models to pretrain capable multimodal
models. The integration of natural language during pretraining has been shown
to significantly improve learned representations, particularly in computer
vision. However, the efficacy of multimodal language modeling in the realm of
functional brain data, specifically for advancing pathology detection, remains
unexplored. This study pioneers EEG-language models trained on clinical reports
and 15000 EEGs. We extend methods for multimodal alignment to this novel domain
and investigate which textual information in reports is useful for training
EEG-language models. Our results indicate that models learn richer
representations from being exposed to a variety of report segments, including
the patient's clinical history, description of the EEG, and the physician's
interpretation. Compared to models exposed to narrower clinical text
information, we find such models to retrieve EEGs based on clinical reports
(and vice versa) with substantially higher accuracy. Yet, this is only observed
when using a contrastive learning approach. Particularly in regimes with few
annotations, we observe that representations of EEG-language models can
significantly improve pathology detection compared to those of EEG-only models,
as demonstrated by both zero-shot classification and linear probes. In sum,
these results highlight the potential of integrating brain activity data with
clinical text, suggesting that EEG-language models represent significant
progress for clinical applications.

æè¦ï¼å¤æ¨¡æè¯­è¨å»ºæ¨¡æ¯ä¸é¡¹æè¿ççªç ´ï¼å®å©ç¨å¤§åè¯­è¨æ¨¡åçè¿æ­¥æ¥é¢è®­ç»æè½åçå¤æ¨¡ææ¨¡åãäºå®è¯æï¼å¨é¢è®­ç»æé´æ´åèªç¶è¯­è¨å¯ä»¥æ¾èæ¹åå­¦ä¹ å°çè¡¨å¾ï¼ç¹å«æ¯å¨è®¡ç®æºè§è§ä¸­ãç¶èï¼å¤æ¨¡æè¯­è¨å»ºæ¨¡å¨åè½æ§èæ°æ®é¢åä¸­çåæï¼ç¹å«æ¯å¯¹äºæ¨è¿ççæ£æµï¼ä»ç¶æªå¾å°æ¢ç´¢ãæ¬ç ç©¶å¼åäºå¨ä¸´åºæ¥åå 15000 ä¸ªèçµå¾ä¸è®­ç»çèçµå¾è¯­è¨æ¨¡åãæä»¬å°å¤æ¨¡æå¯¹é½çæ¹æ³æ©å±å°è¿ä¸ªæ°é¢åï¼å¹¶ç ç©¶æ¥åä¸­çåªäºææ¬ä¿¡æ¯å¯¹äºè®­ç»èçµå¾è¯­è¨æ¨¡åæ¯æç¨çãæä»¬çç»æè¡¨æï¼éè¿æ¥è§¦åç§æ¥åçæ®µï¼åæ¬æ£èççå²ãèçµå¾æè¿°åå»ççè§£éï¼ï¼æ¨¡åå¯ä»¥å­¦ä¹ å°æ´ä¸°å¯çè¡¨å¾ãä¸æ¥è§¦å°è¾çªçä¸´åºææ¬ä¿¡æ¯çæ¨¡åç¸æ¯ï¼æä»¬åç°æ­¤ç±»æ¨¡åå¯ä»¥æ ¹æ®ä¸´åºæ¥åï¼åä¹äº¦ç¶ï¼æ£ç´¢èçµå¾ï¼å¶åç¡®æ§å¤§å¤§æé«ãç¶èï¼åªæå¨ä½¿ç¨å¯¹æ¯å­¦ä¹ æ¹æ³æ¶æä¼è§å¯å°è¿ä¸ç¹ãç¹å«æ¯å¨æ³¨éè¾å°çæ¹æ¡ä¸­ï¼æä»¬è§å¯å°èçµå¾è¯­è¨æ¨¡åçè¡¨å¾å¯ä»¥æ¾èæ¹åççæ£æµï¼ä¸ä»èçµå¾æ¨¡åç¸æ¯ï¼é¶æ ·æ¬åç±»åçº¿æ§æ¢éé½è¯æäºè¿ä¸ç¹ãæ»ä¹ï¼è¿äºç»æçªåºäºå°èæ´»å¨æ°æ®ä¸ä¸´åºææ¬ç¸ç»åçæ½åï¼è¡¨æèçµå¾è¯­è¨æ¨¡åä»£è¡¨äºä¸´åºåºç¨çéå¤§è¿å±ã

##### **SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**
2409.01013v1 by Mevan Ekanayake, Zhifeng Chen, Gary Egan, Mehrtash Harandi, Zhaolin Chen

Implicit Neural Representations (INRs) have recently advanced the field of
deep learning due to their ability to learn continuous representations of
signals without the need for large training datasets. Although INR methods have
been studied for medical image super-resolution, their adaptability to
localized priors in medical images has not been extensively explored. Medical
images contain rich anatomical divisions that could provide valuable local
prior information to enhance the accuracy and robustness of INRs. In this work,
we propose a novel framework, referred to as the Semantically Conditioned INR
(SeCo-INR), that conditions an INR using local priors from a medical image,
enabling accurate model fitting and interpolation capabilities to achieve
super-resolution. Our framework learns a continuous representation of the
semantic segmentation features of a medical image and utilizes it to derive the
optimal INR for each semantic region of the image. We tested our framework
using several medical imaging modalities and achieved higher quantitative
scores and more realistic super-resolution outputs compared to state-of-the-art
methods.

æè¦ï¼é±å¼ç¥ç¶è¡¨å¾µ (INR) è¿æç±æ¼å¶ç¡éå¤§éè¨ç·´è³æéå°±è½å­¸ç¿è¨èçé£çºè¡¨å¾µçè½åï¼èæ¨åäºæ·±åº¦å­¸ç¿é åçé²å±ãåç®¡ INR æ¹æ³å·²è¢«ç ç©¶ç¨æ¼é«å­¸å½±åè¶è§£æåº¦ï¼ä½å¶å°æ¼é«å­¸å½±åä¸­å±é¨åé©çé©ææ§å°æªè¢«å»£æ³æ¢è¨ãé«å­¸å½±ååå«è±å¯çè§£åå­¸ååï¼éäºååå¯ä»¥æä¾æå¹å¼çå±é¨åé©è³è¨ï¼ä»¥å¢å¼· INR çæºç¢ºæ§åç©©å¥æ§ãå¨éé å·¥ä½ä¸­ï¼æåæåºäºä¸åæ°ç©çæ¶æ§ï¼ç¨±çºèªç¾©æ¢ä»¶ INR (SeCo-INR)ï¼å®ä½¿ç¨é«å­¸å½±åä¸­çå±é¨åé©ä¾èª¿æ´ INRï¼å¯¦ç¾æºç¢ºçæ¨¡åæ¬ååæå¼è½åï¼ä»¥å¯¦ç¾è¶è§£æåº¦ãæåçæ¶æ§å­¸ç¿é«å­¸å½±åçèªæåå²ç¹å¾µçé£çºè¡¨å¾µï¼ä¸¦å©ç¨å®çºå½±åçæ¯åèªæååæ¨å°æä½³ INRãæåä½¿ç¨å¤ç¨®é«å­¸å½±åæ¹å¼æ¸¬è©¦æåçæ¶æ§ï¼ä¸¦èæåé²çæ¹æ³ç¸æ¯ï¼éå°äºæ´é«çéåè©ååæ´é¼ççè¶è§£æåº¦è¼¸åºã

##### **Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**
2409.00873v1 by Sajib Acharjee Dip, Kazi Hasan Ibn Arif, Uddip Acharjee Shuvo, Ishtiaque Ahmed Khan, Na Meng

In the realm of dermatology, the complexity of diagnosing skin conditions
manually necessitates the expertise of dermatologists. Accurate identification
of various skin ailments, ranging from cancer to inflammatory diseases, is
paramount. However, existing artificial intelligence (AI) models in dermatology
face challenges, particularly in accurately diagnosing diseases across diverse
skin tones, with a notable performance gap in darker skin. Additionally, the
scarcity of publicly available, unbiased datasets hampers the development of
inclusive AI diagnostic tools. To tackle the challenges in accurately
predicting skin conditions across diverse skin tones, we employ a
transfer-learning approach that capitalizes on the rich, transferable knowledge
from various image domains. Our method integrates multiple pre-trained models
from a wide range of sources, including general and specific medical images, to
improve the robustness and inclusiveness of the skin condition predictions. We
rigorously evaluated the effectiveness of these models using the Diverse
Dermatology Images (DDI) dataset, which uniquely encompasses both
underrepresented and common skin tones, making it an ideal benchmark for
assessing our approach. Among all methods, Med-ViT emerged as the top performer
due to its comprehensive feature representation learned from diverse image
sources. To further enhance performance, we conducted domain adaptation using
additional skin image datasets such as HAM10000. This adaptation significantly
improved model performance across all models.

æè¦ï¼<paragraph>å¨ç®è¤çå­¦é¢åï¼äººå·¥è¯æ­ç®è¤ç¶åµçå¤ææ§éè¦ç®è¤ç§å»å¸çä¸ä¸ç¥è¯ãä»ççå°ççæ§ç¾çï¼å¯¹åç§ç®è¤ç¾ççåç¡®è¯å«è³å³éè¦ãç¶èï¼ç°æçç®è¤çå­¦äººå·¥æºè½ (AI) æ¨¡åé¢ä¸´ææï¼å°¤å¶æ¯å¨åç¡®è¯æ­ä¸åè¤è²çç¾çæ¶ï¼å¨è¾æ·±çè¤è²ä¸å­å¨ææ¾çæ§è½å·®è·ãæ­¤å¤ï¼å¬å¼å¯ç¨çæ åæ°æ®éçç¨ç¼ºæ§é»ç¢äºåå®¹æ§ AI è¯æ­å·¥å·çå¼åãä¸ºäºåºå¯¹åç¡®é¢æµä¸åè¤è²ç®è¤ç¶åµçææï¼æä»¬éç¨äºä¸ç§è¿ç§»å­¦ä¹ æ¹æ³ï¼è¯¥æ¹æ³å©ç¨äºæ¥èªåç§å¾ååçä¸°å¯å¯è½¬ç§»ç¥è¯ãæä»¬çæ¹æ³éæäºæ¥èªå¹¿æ³æ¥æºçå¤ä¸ªé¢è®­ç»æ¨¡åï¼åæ¬ä¸è¬åç¹å®çå»å­¦å¾åï¼ä»¥æé«ç®è¤ç¶åµé¢æµçç¨³å¥æ§ååå®¹æ§ãæä»¬ä½¿ç¨ Diverse Dermatology Images (DDI) æ°æ®éä¸¥æ ¼è¯ä¼°äºè¿äºæ¨¡åçæææ§ï¼è¯¥æ°æ®éç¬ç¹å°åå«äºä»£è¡¨æ§ä¸è¶³åå¸¸è§çè¤è²ï¼ä½¿å¶æä¸ºè¯ä¼°æä»¬æ¹æ³ççæ³åºåãå¨æææ¹æ³ä¸­ï¼Med-ViT ç±äºå¶ä»åç§å¾åæ¥æºä¸­å­¦å°çç»¼åç¹å¾è¡¨ç¤ºèæä¸ºè¡¨ç°æå¥½çæ¹æ³ãä¸ºäºè¿ä¸æ­¥æé«æ§è½ï¼æä»¬ä½¿ç¨ HAM10000 ç­å¶ä»ç®è¤å¾åæ°æ®éè¿è¡äºåéåºãè¿ç§éåºæ¾çæé«äºæææ¨¡åçæ¨¡åæ§è½ã</paragraph>

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ç¶å¸¸ç¼ºä¹ç¹å®é åçç¥è­ï¼å³ä½¿ç¶éå¾®èª¿çæ¨¡åä¹å®¹æç¢çå¹»è¦ºãå æ­¤ï¼éè¦æ´å¤å¯é çæ¨¡åä¾ç´å¥å¤é¨ç¥è­ãæåæåºäºä¸åæµç¨ 4StepFocusï¼ç¹å¥æ¯é èçæ­¥é©ï¼å¯ä»¥å¤§å¹æ¹å LLM çç­æ¡ãéæ¯ééæä¾åå¼å°çå¤é¨ç¥è­å­åï¼å©ç¨æ¨¡åèªè¡æ·åéè¯æ§èçµ¡åé²è¡åºæ¬æ¨ççè½åä¾å¯¦ç¾çãæ­¤æ¹æ³ééå¨åçµæ§åç¥è­åº«ä¸­é²è¡åºæ¼ä¸åçµçæå°ï¼ä»¥ç´æ¥ä¸å¯è¿½è¹¤çæ¹å¼ç¸®å°æ½å¨æ­£ç¢ºç­æ¡çç¯åï¼ç¶å¾ååæå°æ½å¨è¡¨å¾µï¼æ ¹æéçµæ§åè³æå°éäºåé¸ç­æ¡é²è¡æåãéèç´ç²¹åºæ¼æ½å¨è¡¨å¾µçç¸éæ¹æ³ææåå¥ã4StepFocus åå«ä»¥ä¸æ­¥é©ï¼1) ç± LLM é²è¡ä¸åçµç¢çä»¥æ·åéè¯è³æï¼2) å¨éäºä¸åçµä¸­æ¿æè®æ¸ï¼ä»¥æ¡ç¨ç¥è­åè¡¨ç¸®å°ç­æ¡åé¸ç¯åï¼3) ä½¿ç¨æ¶åéè¯éçµæ§åè³æçåéç¸ä¼¼æ§æå°å°å©é¤åé¸ç­æ¡é²è¡æåºï¼4) ç± LLM éæ°å°æä½³åé¸ç­æ¡é²è¡æåï¼ä¸¦æä¾èæ¯è³æãå¨é«çãç¢åæ¨è¦åå­¸è¡è«ææå°æ¸¬è©¦éä¸­é²è¡çå¯¦é©è­æï¼éç¨®æ¹æ³ç¢ºå¯¦æ¯ä¸ç¨®å¼·å¤§çæ´åãå®ä¸åå¢å äºä¾èªè³è¨æª¢ç´¢çç¸å³å¯è¿½è¹¤èæ¯è³è¨ï¼èä¸èæåé²çæ¹æ³ç¸æ¯ï¼ä¹å¤§å¹æåäºæè½ãæ¬ææåºäºä¸åæ°ç©ä¸é®®å°æ¢ç´¢çæ¹åï¼å æ­¤æä¾äºå»£æ³çæªä¾å·¥ä½æ©æãä½¿ç¨çåå§ç¢¼å¯å¨ https://github.com/kramerlab/4StepFocus åå¾ã

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

æè¦ï¼æ¬ææåºäºä¸åç¥è­å·¥ç¨åå¤èªè¨èªç¾©æ¨çæè¡çæ¬ä½è¨­è¨ï¼ç¨æ¼å»ºç«ä¸åèªååç³»çµ±ï¼ä»¥ç¥è­åè­çå½¢å¼å¸æ¶å°åº¦æççç¹é£ªè³è¨ãéé»å¨æ¼è¨­è¨æºæ§æ¹æ³ï¼ä»¥æ¨å°æ¬ä½è¨­è¨ï¼ä¸¦å¨é¢æ·åéæ¼é£ç©ãé£è­ãé£æãç¹é£ªç¹æ§ï¼ä»¥åæéè¦ççé¤çç¥è­ï¼ä¸¦æ´å¤§è¦æ¨¡ãæåå¨éåç è¨æè«æä¸­ä»ç´¹äºæåæ­£å¨é²è¡çå·¥ä½ï¼è©³ç´°æè¿°äºæ´çå°åº¦æçç¥è­ç¸éçææ°ï¼ä¸¦æåºäºæåçé«éæ¬ä½è¨­è¨ãæåä¹æåºäºä¸ç¨®æ°çå·¥ä½æµç¨ï¼å®ä½¿ç¨ AIãLLM åèªè¨æè¡ï¼å¾å¬å±é åçé£è­é¨è½æ ¼ç¶²ç«ä¸­æ´çè³è¨ï¼ä»¥å»ºç«å°åº¦æççç¥è­åè­ãæ¬ææåºçç¥è­æ´çæ¹æ³æ¯éç¨çï¼å¯ä»¥è¤è£½å°ä»»ä½é åãè¨­è¨èæç¨ç¡éï¼å¯ç¨æ¼ AI é©åçæºæ§åæãå»ºç«åäººåæ¸ä½å¥åº·æ¨è¦ç³»çµ±ï¼ä»¥åä½¿ç¨ä½¿ç¨èè³è¨ãé£ç©çç©åå­¸ãå°çè³è¨ãè¾²æ¥­è³è¨ç­èçµ¡è³è¨ï¼ä¾è£åå°åº¦æççç¥è­åè­ã

##### **AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**
2409.00735v1 by Mahsa Khosravi, Matthew Carroll, Kai Liang Tan, Liza Van der Laan, Joscif Raigne, Daren S. Mueller, Arti Singh, Aditya Balu, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar

Agricultural production requires careful management of inputs such as
fungicides, insecticides, and herbicides to ensure a successful crop that is
high-yielding, profitable, and of superior seed quality. Current
state-of-the-art field crop management relies on coarse-scale crop management
strategies, where entire fields are sprayed with pest and disease-controlling
chemicals, leading to increased cost and sub-optimal soil and crop management.
To overcome these challenges and optimize crop production, we utilize machine
learning tools within a virtual field environment to generate localized
management plans for farmers to manage biotic threats while maximizing profits.
Specifically, we present AgGym, a modular, crop and stress agnostic simulation
framework to model the spread of biotic stresses in a field and estimate yield
losses with and without chemical treatments. Our validation with real data
shows that AgGym can be customized with limited data to simulate yield outcomes
under various biotic stress conditions. We further demonstrate that deep
reinforcement learning (RL) policies can be trained using AgGym for designing
ultra-precise biotic stress mitigation strategies with potential to increase
yield recovery with less chemicals and lower cost. Our proposed framework
enables personalized decision support that can transform biotic stress
management from being schedule based and reactive to opportunistic and
prescriptive. We also release the AgGym software implementation as a community
resource and invite experts to contribute to this open-sourced publicly
available modular environment framework. The source code can be accessed at:
https://github.com/SCSLabISU/AgGym.

æè¦ï¼è¾²æ¥­çç¢éè¦å°å¿ç®¡çè¼¸å¥ï¼ä¾å¦æ®ºèåãæ®ºè²ååé¤èåï¼ä»¥ç¢ºä¿ä½ç©æåãé«ç¢ãæå©å¯åä¸å·æåªè¯çç¨®å­åè³ªãç®åæåé²çç°éä½ç©ç®¡çä¾è³´æ¼ç²ç¥çä½ç©ç®¡çç­ç¥ï¼å¶ä¸­æ´åç°å°é½å´çäºæ§å¶çè²å®³çåå­¸ç©è³ªï¼å°è´ææ¬å¢å ååå£¤åä½ç©ç®¡çä¸ä½³ãçºäºåæéäºææ°ä¸¦åªåä½ç©çç¢ï¼æåå¨èæ¬ç°éç°å¢ä¸­å©ç¨æ©å¨å­¸ç¿å·¥å·çºè¾²æ°çæå±é¨ç®¡çè¨ç«ï¼ä»¥ç®¡ççç©å¨èä¸¦åææå¤§åå©æ½¤ãå·é«ä¾èªªï¼æåæåºäº AgGymï¼ä¸åæ¨¡çµåãä½ç©åå£åä¸å¯ç¥çæ¨¡æ¬æ¶æ§ï¼ç¨æ¼æ¨¡æ¬ç°éçç©å£åçæ´æ£ï¼ä¸¦ä¼°ç®æåæ²æåå­¸èççç¢éæå¤±ãæåä½¿ç¨çå¯¦æ¸æé²è¡é©è­ï¼é¡¯ç¤º AgGym å¯ä»¥ä½¿ç¨æéçæ¸æé²è¡èªè¨ï¼ä»¥æ¨¡æ¬åç¨®çç©å£åæ¢ä»¶ä¸çç¢éçµæãæåé²ä¸æ­¥è­æï¼æ·±åº¦å¼·åå­¸ç¿ (RL) æ¿ç­å¯ä»¥ä½¿ç¨ AgGym é²è¡è¨ç·´ï¼ä»¥è¨­è¨è¶ç²¾ç¢ºççç©å£åç·©è§£ç­ç¥ï¼ä¸¦æå¯è½ä»¥æ´å°çåå­¸ç©è³ªåæ´ä½çææ¬å¢å ç¢éæ¢å¾©ãæåæåºçæ¶æ§åç¨äºåäººåæ±ºç­æ¯æ´ï¼å¯ä»¥å°çç©å£åç®¡çå¾åºæ¼æéè¡¨åè¢«åè½è®çºæ©æä¸»ç¾©åè¦ç¯æ§ãæåéå° AgGym è»é«å¯¦ä½ä½çºç¤¾åè³æºéåºï¼ä¸¦éè«å°å®¶çºéåéæ¾åå§ç¢¼ä¸å¬éå¯ç¨çæ¨¡çµåç°å¢æ¶æ§ååºè²¢ç»ãå¯ä»¥å¨ä»¥ä¸ä½ç½®åå¾åå§ç¢¼ï¼https://github.com/SCSLabISU/AgGymã

##### **LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**
2409.00726v1 by Zhaojie Fang, Xiao Yu, Guanyu Zhou, Ke Zhuang, Yifei Chen, Ruiquan Ge, Changmiao Wang, Gangyong Jia, Qing Wu, Juan Ye, Maimaiti Nuliqiman, Peifang Xu, Ahmed Elazab

Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise
identification of ocular diseases using sodium fluorescein, which can be
potentially harmful. Existing research has developed methods to generate UWF-FA
from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the
adverse reactions associated with injections. However, these methods have been
less effective in producing high-quality late-phase UWF-FA, particularly in
lesion areas and fine details. Two primary challenges hinder the generation of
high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and
early/late-phase UWF-FA datasets, and the need for realistic generation at
lesion sites and potential blood leakage regions. This study introduces an
improved latent diffusion model framework to generate high-quality late-phase
UWF-FA from limited paired UWF images. To address the challenges as mentioned
earlier, our approach employs a module utilizing Cross-temporal Regional
Difference Loss, which encourages the model to focus on the differences between
early and late phases. Additionally, we introduce a low-frequency enhanced
noise strategy in the diffusion forward process to improve the realism of
medical images. To further enhance the mapping capability of the variational
autoencoder module, especially with limited datasets, we implement a Gated
Convolutional Encoder to extract additional information from conditional
images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein
Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase
UWF-FA and achieves state-of-the-art results compared to other existing methods
when working with limited datasets. Our source code is available at:
https://github.com/Tinysqua/****.

æè¦ï¼è¶å»£è§è¢åè¡ç®¡é å½±ï¼UWF-FAï¼ä½¿ç¨å¯è½å·ææ½å¨å±å®³çéè¢åç´ ï¼å¯ç²¾ç¢ºè­å¥ç¼ç¾ãç¾æç ç©¶å·²éç¼åºå¾è¶å»£è§ææé·å°ç¼ç§é¡ï¼UWF-SLOï¼ç¢ç UWF-FA çæ¹æ³ï¼ä»¥æ¸å°èæ³¨å°ç¸éçä¸è¯åæãç¶èï¼éäºæ¹æ³å¨ç¢çé«åè³ªçå¾æ UWF-FA æ¹é¢ææè¼å·®ï¼ç¹å¥æ¯å¨çç¶åååç²¾ç´°ç´°ç¯æ¹é¢ãç¢çé«åè³ªå¾æ UWF-FA é¢è¨å©é ä¸»è¦ææ°ï¼éå°ç UWF-SLO åæ©æ/å¾æ UWF-FA è³æéç¨å°ï¼ä»¥åéè¦å¨çç¶é¨ä½åæ½å¨åºè¡ååé²è¡é¼ççç¢çãæ¬ç ç©¶å¼é²ä¸ç¨®æ¹è¯çæ½å¨æ´æ£æ¨¡åæ¶æ§ï¼å¾æééå°ç UWF å½±åç¢çé«åè³ªçå¾æ UWF-FAãçºäºæå°åé¢æå°çææ°ï¼æåçæ¹æ³æ¡ç¨ä¸åæ¨¡çµï¼å©ç¨è·¨æéååå·®ç°æå¤±ï¼é¼åµæ¨¡åå°æ³¨æ¼æ©æåå¾æä¹éçå·®ç°ãæ­¤å¤ï¼æåå¨æ´æ£ååéç¨ä¸­å¼é²ä¸ç¨®ä½é »å¢å¼·éè¨ç­ç¥ï¼ä»¥æ¹åé«å­¸å½±åççå¯¦æ§ãçºäºé²ä¸æ­¥å¢å¼·è®ç°èªåç·¨ç¢¼å¨æ¨¡çµçå°æè½åï¼ç¹å¥æ¯å¨è³æéæéçææ³ä¸ï¼æåå¯¦ä½ä¸åéæ§å·ç©ç·¨ç¢¼å¨ï¼å¾æ¢ä»¶å½±åä¸­èåé¡å¤è³è¨ãæåéå°è¶å»£è§å¾æè¢åè¡ç®¡é å½±ï¼LPUWF-LDMï¼çæ½å¨æ´æ£æ¨¡åææéå»ºå¾æ UWF-FA ä¸­çç²¾ç´°ç´°ç¯ï¼ä¸¦å¨ä½¿ç¨æéè³æéæï¼èå¶ä»ç¾ææ¹æ³ç¸æ¯ï¼éå°æåé²ççµæãæåçåå§ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼
https://github.com/Tinysqua/****ã

##### **BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**
2409.00724v1 by Shams Nafisa Ali, Afia Zahin, Samiul Based Shuvo, Nusrat Binta Nizam, Shoyad Ibn Sabur Khan Nuhash, Sayeed Sajjad Razin, S. M. Sakeef Sani, Farihin Rahman, Nawshad Binta Nizam, Farhat Binte Azam, Rakib Hossen, Sumaiya Ohab, Nawsabah Noor, Taufiq Hasan

Cardiac auscultation, an integral tool in diagnosing cardiovascular diseases
(CVDs), often relies on the subjective interpretation of clinicians, presenting
a limitation in consistency and accuracy. Addressing this, we introduce the
BUET Multi-disease Heart Sound (BMD-HS) dataset - a comprehensive and
meticulously curated collection of heart sound recordings. This dataset,
encompassing 864 recordings across five distinct classes of common heart
sounds, represents a broad spectrum of valvular heart diseases, with a focus on
diagnostically challenging cases. The standout feature of the BMD-HS dataset is
its innovative multi-label annotation system, which captures a diverse range of
diseases and unique disease states. This system significantly enhances the
dataset's utility for developing advanced machine learning models in automated
heart sound classification and diagnosis. By bridging the gap between
traditional auscultation practices and contemporary data-driven diagnostic
methods, the BMD-HS dataset is poised to revolutionize CVD diagnosis and
management, providing an invaluable resource for the advancement of cardiac
health research. The dataset is publicly available at this link:
https://github.com/mHealthBuet/BMD-HS-Dataset.

æè¦ï¼å¿èè½è¨ºæ¯è¨ºæ·å¿è¡ç®¡ç¾ç (CVD) çä¸é æ´åå·¥å·ï¼éå¸¸ä¾è³´æ¼è¨åºé«å¸«çä¸»è§è©®éï¼å¨ä¸è´æ§åæºç¢ºæ§æ¹é¢å­å¨éå¶ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äº BUET å¤éç¾çå¿é³ (BMD-HS) è³æéï¼éæ¯ä¸åå¨é¢ä¸ç¶éç²¾å¿ç­åçå¿é³éé³è³æéãæ­¤è³æéåå«äºç¨®å¸¸è¦å¿é³ç 864 åéé³ï¼ä»£è¡¨äºå»£æ³çå¿ç£èç¾çï¼éé»å¨æ¼è¨ºæ·å°é£ççä¾ãBMD-HS è³æéççªåºç¹é»æ¯å¶åµæ°çå¤æ¨ç±¤è¨»è§£ç³»çµ±ï¼å®æ¶µèäºåç¨®ç¾çåç¨ç¹çç¾ççæãéåç³»çµ±é¡¯èå¢å¼·äºè³æéå¨éç¼èªåå¿é³åé¡åè¨ºæ·ä¸­é²éæ©å¨å­¸ç¿æ¨¡åçæç¨ãééå½åå³çµ±è½è¨ºå¯¦åèç¶ä»£è³æé©åè¨ºæ·æ¹æ³ä¹éçå·®è·ï¼BMD-HS è³æéæºåå¥½é©æ°å¿è¡ç®¡ç¾ççè¨ºæ·åç®¡çï¼çºå¿èå¥åº·ç ç©¶çé²å±æä¾å¯¶è²´çè³æºãæ­¤è³æéå¯ééä»¥ä¸é£çµå¬éåå¾ï¼https://github.com/mHealthBuet/BMD-HS-Datasetã

##### **Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**
2409.00718v1 by Pragya Gupta, Subhamoy Mandal, Debashree Guha, Debjani Chakraborty

Automatic diagnosis techniques have evolved to identify age-related macular
degeneration (AMD) by employing single modality Fundus images or optical
coherence tomography (OCT). To classify ocular diseases, fundus and OCT images
are the most crucial imaging modalities used in the clinical setting. Most deep
learning-based techniques are established on a single imaging modality, which
contemplates the ocular disorders to a specific extent and disregards other
modality that comprises exhaustive information among distinct imaging
modalities. This paper proposes a modality-specific multiscale color space
embedding integrated with the attention mechanism based on transfer learning
for classification (MCGAEc), which can efficiently extract the distinct
modality information at various scales using the distinct color spaces. In this
work, we first introduce the modality-specific multiscale color space encoder
model, which includes diverse feature representations by integrating distinct
characteristic color spaces on a multiscale into a unified framework. The
extracted features from the prior encoder module are incorporated with the
attention mechanism to extract the global features representation, which is
integrated with the prior extracted features and transferred to the random
forest classifier for the classification of AMD. To analyze the performance of
the proposed MCGAEc method, a publicly available multi-modality dataset from
Project Macula for AMD is utilized and compared with the existing models.

æè¦ï¼èªåè¨ºæ·æè¡å·²æ¼é²å°è½ééä½¿ç¨å®ä¸æ¨¡å¼ç¼åºå½±åæåå­¸ç¸å¹²æ·å±¤ææ (OCT) ä¾è¾¨è­å¹´é½¡ç¸éæ§é»æé¨çè® (AMD)ãçºäºåé¡ç¼ç¾ï¼ç¼åºå OCT å½±åæ¯è¨åºç°å¢ä¸­ä½¿ç¨æééµçå½±åæ¨¡å¼ãå¤§å¤æ¸åºæ¼æ·±åº¦å­¸ç¿çæè¡å»ºç«å¨å®ä¸å½±åæ¨¡å¼ä¸ï¼å®å¨ä¸å®ç¨åº¦ä¸èéäºç¼ç¾ï¼å»å¿½ç¥äºå¶ä»æ¨¡å¼ï¼èå¶ä»æ¨¡å¼åå«äºä¸åå½±åæ¨¡å¼ä¹éçè©³ç¡è³è¨ãæ¬ææåºäºä¸ç¨®æ¨¡å¼ç¹å®çå¤å°ºåº¦è²å½©ç©ºéåµå¥æ´åï¼ä¸¦åºæ¼ç¨æ¼åé¡çè½ç§»å­¸ç¿çæ³¨æåæ©å¶ (MCGAEc)ï¼å®è½ä½¿ç¨ä¸åçè²å½©ç©ºéå¨ä¸åçå°ºåº¦ä¸æææåä¸åçæ¨¡å¼è³è¨ãå¨éé å·¥ä½ä¸­ï¼æåé¦åä»ç´¹äºæ¨¡å¼ç¹å®çå¤å°ºåº¦è²å½©ç©ºéç·¨ç¢¼å¨æ¨¡åï¼å®ééå°ä¸åçç¹å¾µè²å½©ç©ºéæ´åå°å¤å°ºåº¦ä¸­ï¼ä¾ç´å¥ä¸åçç¹å¾µè¡¨å¾µå°ä¸åçµ±ä¸çæ¶æ§ä¸­ãå¾ååçç·¨ç¢¼å¨æ¨¡çµä¸­æåçç¹å¾µèæ³¨æåæ©å¶çµåï¼ä»¥æåå¨åç¹å¾µè¡¨å¾µï¼å®èååæåçç¹å¾µæ´åï¼ä¸¦è½ç§»å°é¨æ©æ£®æåé¡å¨ï¼ä»¥é²è¡ AMD åé¡ãçºäºåæææåºç MCGAEc æ¹æ³çæè½ï¼æåå©ç¨äºä¾èª Project Macula for AMD çå¬éå¤æ¨¡å¼è³æéï¼ä¸¦èç¾ææ¨¡åé²è¡æ¯è¼ã

##### **Curriculum Prompting Foundation Models for Medical Image Segmentation**
2409.00695v1 by Xiuqi Zheng, Yuhang Zhang, Haoran Zhang, Hongrui Liang, Xueqi Bao, Zhuqing Jiang, Qicheng Lao

Adapting large pre-trained foundation models, e.g., SAM, for medical image
segmentation remains a significant challenge. A crucial step involves the
formulation of a series of specialized prompts that incorporate specific
clinical instructions. Past works have been heavily reliant on a singular type
of prompt for each instance, necessitating manual input of an ideally correct
prompt, which is less efficient. To tackle this issue, we propose to utilize
prompts of different granularity, which are sourced from original images to
provide a broader scope of clinical insights. However, combining prompts of
varying types can pose a challenge due to potential conflicts. In response, we
have designed a coarse-to-fine mechanism, referred to as curriculum prompting,
that progressively integrates prompts of different types. Through extensive
experiments on three public medical datasets across various modalities, we
demonstrate the effectiveness of our proposed approach, which not only
automates the prompt generation process but also yields superior performance
compared to other SAM-based medical image segmentation methods. Code is
available at: https://github.com/AnnaZzz-zxq/Curriculum-Prompting.

æè¦ï¼èª¿æ´å¤§åé è¨ç·´åºç¤æ¨¡åï¼ä¾å¦ SAMï¼ä»¥é²è¡é«å­¸å½±ååå²ä»æ¯ä¸é éå¤§ææ°ãééµæ­¥é©æ¶åå¶å®ä¸ç³»ååå«ç¹å®è¨åºèªªæçå°éæç¤ºãéå»çå·¥ä½å¨å¾å¤§ç¨åº¦ä¸ä¾è³´æ¼æ¯åä¾é çå®ä¸æç¤ºé¡åï¼ééè¦æåè¼¸å¥çæ³çæ­£ç¢ºæç¤ºï¼æçè¼ä½ãçºäºè§£æ±ºéååé¡ï¼æåå»ºè­°å©ç¨ä¸åç²åº¦çæç¤ºï¼éäºæç¤ºä¾èªåå§å½±åï¼ä»¥æä¾æ´å»£æ³çè¨åºè¦è§£ãç¶èï¼ç±æ¼æ½å¨è¡çªï¼çµåä¸åé¡åçæç¤ºå¯è½ææ§æææ°ãçºäºè§£æ±ºéååé¡ï¼æåè¨­è¨äºä¸ç¨®ç±ç²å°ç´°çæ©å¶ï¼ç¨±çºèª²ç¨æç¤ºï¼å®éæ­¥æ´åä¸åé¡åçæç¤ºãééå°åç¨®æ¨¡å¼ä¸çä¸åå¬å±é«å­¸è³æéé²è¡å»£æ³çå¯¦é©ï¼æåè­æäºæåæåºçæ¹æ³çæææ§ï¼å®ä¸åèªååæç¤ºçæéç¨ï¼èä¸èå¶ä»åºæ¼ SAM çé«å­¸å½±ååå²æ¹æ³ç¸æ¯ï¼éç¢çäºæ´å¥½çæè½ãç¨å¼ç¢¼å¯å¨ä»¥ä¸ä½ç½®åå¾ï¼https://github.com/AnnaZzz-zxq/Curriculum-Promptingã

##### **Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**
2409.00544v1 by Jacqueline Lammert, Nicole Pfarr, Leonid Kuligin, Sonja Mathes, Tobias Dreyer, Luise Modersohn, Patrick Metzger, Dyke Ferber, Jakob Nikolas Kather, Daniel Truhn, Lisa Christine Adams, Keno Kyrill Bressem, Sebastian Lange, Kristina Schwamborn, Martin Boeker, Marion Kiechle, Ulrich A. Schatz, Holger Bronger, Maximilian Tschochohei

Rare gynecological tumors (RGTs) present major clinical challenges due to
their low incidence and heterogeneity. The lack of clear guidelines leads to
suboptimal management and poor prognosis. Molecular tumor boards accelerate
access to effective therapies by tailoring treatment based on biomarkers,
beyond cancer type. Unstructured data that requires manual curation hinders
efficient use of biomarker profiling for therapy matching. This study explores
the use of large language models (LLMs) to construct digital twins for
precision medicine in RGTs.
  Our proof-of-concept digital twin system integrates clinical and biomarker
data from institutional and published cases (n=21) and literature-derived data
(n=655 publications with n=404,265 patients) to create tailored treatment plans
for metastatic uterine carcinosarcoma, identifying options potentially missed
by traditional, single-source analysis. LLM-enabled digital twins efficiently
model individual patient trajectories. Shifting to a biology-based rather than
organ-based tumor definition enables personalized care that could advance RGT
management and thus enhance patient outcomes.

æè¦ï¼ç½è¦å©¦ç§è«ç¤ (RGT) ç±æ¼å¶ä½ç¼ççåç°è³ªæ§ï¼å°è¨åºå¸¶ä¾éå¤§ææ°ãç¼ºä¹æç¢ºçæå¼å°è´æ¬¡ä½³ç®¡çåä¸è¯é å¾ãåå­è«ç¤å§å¡æééæ ¹æçç©æ¨è¨å®¢è£½åæ²»çï¼å éåå¾ææçæ³ï¼è¶è¶ççé¡åãéè¦æåæ´ççéçµæ§åè³æé»ç¤äºçç©æ¨è¨åæå¨çæ³éå°ä¸­çææä½¿ç¨ãæ¬ç ç©¶æ¢è¨ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) çº RGT çç²¾æºé«çå»ºæ§æ¸ä½éèèã
æåçæ¦å¿µé©è­æ¸ä½éèèç³»çµ±æ´åäºä¾èªæ©æ§åå·²ç¼è¡¨çæ¡ä¾ (n=21) çè¨åºåçç©æ¨è¨è³æï¼ä»¥åä¾èªæç»çè³æ (n=655 ç¯åºçç©ï¼n=404,265 åæ£è)ï¼çºè½ç§»æ§å­å®®èç¤çå¶å®å®¢è£½åæ²»çè¨ç«ï¼æ¾åºå³çµ±å®ä¸ä¾æºåæå¯è½éºæ¼çé¸é ãLLM åç¨çæ¸ä½éèèææå°æ¨¡æ¬åå¥æ£èçè»è·¡ãå¾åºæ¼å¨å®çè«ç¤å®ç¾©è½è®çºåºæ¼çç©å­¸çå®ç¾©ï¼è½å¯¦ç¾åäººåç§è­·ï¼é²èæå RGT ç®¡çä¸¦æ¹åæ£èé å¾ã

##### **Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**
2409.00391v1 by Georgios Ioannides, Adrian Kieback, Aman Chadha, Aaron Elkins

Speech-based depression detection poses significant challenges for automated
detection due to its unique manifestation across individuals and data scarcity.
Addressing these challenges, we introduce DAAMAudioCNNLSTM and
DAAMAudioTransformer, two parameter efficient and explainable models for audio
feature extraction and depression detection. DAAMAudioCNNLSTM features a novel
CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),
focusing dynamically on informative speech segments. DAAMAudioTransformer,
leveraging a transformer encoder in place of the CNN-LSTM architecture,
incorporates the same DAAM module for enhanced attention and interpretability.
These approaches not only enhance detection robustness and interpretability but
also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro
score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the
DAIC-WOZ dataset, without reliance on supplementary information such as vowel
positions and speaker information during training/validation as in previous
approaches. Both models' significant explainability and efficiency in
leveraging speech signals for depression detection represent a leap towards
more reliable, clinically useful diagnostic tools, promising advancements in
speech and mental health care. To foster further research in this domain, we
make our code publicly available.

æè¦ï¼èªé³åæé¬±æª¢æ¸¬å°èªååæª¢æ¸¬ä¾èªªæ¯ä¸å¤§ææ°ï¼å çºå®å¨ä¸ååé«éçè¡¨ç¾ç¨ç¹ï¼ä¸è³æç¨å°ãçºäºæå°éäºææ°ï¼æåå¼å¥äº DAAMAudioCNNLSTM å DAAMAudioTransformerï¼éå©ååæ¸ææä¸å¯è§£éçæ¨¡åï¼ç¨æ¼é³è¨ç¹å¾µèååæé¬±æª¢æ¸¬ãDAAMAudioCNNLSTM æ¡ç¨åµæ°ç CNN-LSTM æ¶æ§ï¼æ­éå¤é ­å¯åº¦èªé©ææ³¨æåæ©å¶ (DAAM)ï¼åæéæ³¨æ¼ææç¾©çèªé³åæ®µãDAAMAudioTransformer å©ç¨Transformerç·¨ç¢¼å¨åä»£ CNN-LSTM æ¶æ§ï¼ä¸¦ç´å¥ç¸åç DAAM æ¨¡çµï¼ä»¥å¢å¼·æ³¨æååå¯è§£éæ§ãéäºæ¹æ³ä¸åå¢å¼·äºæª¢æ¸¬çç©©å¥æ§åå¯è§£éæ§ï¼ééå°äºæåé²çæè½ï¼DAAMAudioCNNLSTM ç F1 å·¨è§åæ¸çº 0.702ï¼DAAMAudioTransformer å¨ DAIC-WOZ è³æéä¸ç F1 å·¨è§åæ¸çº 0.72ï¼å¨è¨ç·´/é©è­æéä¸ä¾è³´æ¼è¼å©è³è¨ï¼ä¾å¦æ¯é³ä½ç½®åèªªè©±èè³è¨ï¼éèååçåæ³ä¸åãéå©åæ¨¡åå¨å©ç¨èªé³è¨èé²è¡æé¬±æª¢æ¸¬æ¹é¢å·æé¡¯èçå¯è§£éæ§åæçï¼ä»£è¡¨èæåæ´å¯é ãè¨åºä¸æç¨çè¨ºæ·å·¥å·éé²äºä¸å¤§æ­¥ï¼ä¸¦é ç¤ºèèªé³åå¿çä¿å¥çé²æ­¥ãçºäºä¿é²éåé åçé²ä¸æ­¥ç ç©¶ï¼æåå¬éäºæåçç¨å¼ç¢¼ã

##### **Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**
2409.00310v1 by Mikhail Borisenkov, Andrei Velichko, Maksim Belyaev, Dmitry Korzun, Tatyana Tserne, Larisa Bakutova, Denis Gubin

This study investigates machine learning algorithms to identify objective
features for diagnosing food addiction (FA) and assessing confirmed symptoms
(SC). Data were collected from 81 participants (mean age: 21.5 years, range:
18-61 years, women: 77.8%) whose FA and SC were measured using the Yale Food
Addiction Scale (YFAS). Participants provided demographic and anthropometric
data, completed the YFAS, the Zung Self-Rating Depression Scale, and the Dutch
Eating Behavior Questionnaire, and wore an actimeter on the non-dominant wrist
for a week to record motor activity. Analysis of the actimetric data identified
significant statistical and entropy-based features that accurately predicted FA
and SC using ML. The Matthews correlation coefficient (MCC) was the primary
metric. Activity-related features were more effective for FA prediction
(MCC=0.88) than rest-related features (MCC=0.68). For SC, activity segments
yielded MCC=0.47, rest segments MCC=0.38, and their combination MCC=0.51.
Significant correlations were also found between actimetric features related to
FA, emotional, and restrained eating behaviors, supporting the model's
validity. Our results support the concept of a human bionic suite composed of
IoT devices and ML sensors, which implements health digital assistance with
real-time monitoring and analysis of physiological indicators related to FA and
SC.

æè¦ï¼æ¬ç ç©¶èª¿æ¥æ©å¨å­¸ç¿æ¼ç®æ³ï¼ä»¥è­å¥è¨ºæ·é£ç©æç® (FA) åè©ä¼°å·²ç¢ºèªçç (SC) çå®¢è§ç¹å¾µãè³æä¾èª 81 ä½åèèï¼å¹³åå¹´é½¡ï¼21.5 æ­²ï¼ç¯åï¼18-61 æ­²ï¼å¥³æ§ï¼77.8%ï¼ï¼å¶ FA å SC æ¯ä½¿ç¨è¶é­¯é£ç©æç®éè¡¨ (YFAS) æ¸¬éçãåèèæä¾äºäººå£çµ±è¨åäººé¡æ¸¬éè³æï¼å®æäº YFASãZung èªæè©éæé¬±éè¡¨åè·è­é£²é£è¡çºåå·ï¼ä¸¦å¨éæ£ç¨æèä¸ä½©æ´æ´»åè¨ä¸é±ä»¥è¨ééåæ´»åãå°æ´»åè¨è³æçåæè­å¥åºéè¦ççµ±è¨ååºæ¼çµçç¹å¾µï¼éäºç¹å¾µä½¿ç¨æ©å¨å­¸ç¿æºç¢ºé æ¸¬äº FA å SCãé¦¬ä¿®æ¯ç¸éä¿æ¸ (MCC) æ¯ä¸»è¦ææ¨ãèä¼æ¯ç¸éçç¹å¾µï¼MCC=0.68ï¼ç¸æ¯ï¼èæ´»åç¸éçç¹å¾µå°æ¼ FA é æ¸¬æ´ææï¼MCC=0.88ï¼ãå°æ¼ SCï¼æ´»ååæ®µç¢çç MCC=0.47ï¼ä¼æ¯åæ®µ MCC=0.38ï¼çµåå¾ MCC=0.51ãéç¼ç¾è FAãæç·ååéé£²é£è¡çºç¸éçæ´»åè¨ç¹å¾µä¹éå­å¨é¡¯èç¸éæ§ï¼éæ¯æäºæ¨¡åçæææ§ãæåççµææ¯æç±ç©è¯ç¶²è£ç½®åæ©å¨å­¸ç¿ææ¸¬å¨çµæçäººé«ä»¿çå¥ä»¶çæ¦å¿µï¼è©²å¥ä»¶å¯¦ä½äºå¥åº·æ¸ä½åå©ï¼ä¸¦å°è FA å SC ç¸éçççææ¨é²è¡å³æç£æ§ååæã

##### **Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**
2408.17401v1 by Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni

In recent years, various methods have been introduced for explaining the
outputs of "black-box" AI models. However, it is not well understood whether
users actually comprehend and trust these explanations. In this paper, we focus
on explanations for a regression tool for assessing cancer risk and examine the
effect of the explanations' content and format on the user-centric metrics of
comprehension and trust. Regarding content, we experiment with two explanation
methods: the popular SHAP, based on game-theoretic notions and thus potentially
complex for everyday users to comprehend, and occlusion-1, based on feature
occlusion which may be more comprehensible. Regarding format, we present SHAP
explanations as charts (SC), as is conventional, and occlusion-1 explanations
as charts (OC) as well as text (OT), to which their simpler nature also lends
itself. The experiments amount to user studies questioning participants, with
two different levels of expertise (the general population and those with some
medical training), on their subjective and objective comprehension of and trust
in explanations for the outputs of the regression tool. In both studies we
found a clear preference in terms of subjective comprehension and trust for
occlusion-1 over SHAP explanations in general, when comparing based on content.
However, direct comparisons of explanations when controlling for format only
revealed evidence for OT over SC explanations in most cases, suggesting that
the dominance of occlusion-1 over SHAP explanations may be driven by a
preference for text over charts as explanations. Finally, we found no evidence
of a difference between the explanation types in terms of objective
comprehension. Thus overall, the choice of the content and format of
explanations needs careful attention, since in some contexts format, rather
than content, may play the critical role in improving user experience.

æè¦ï¼<paragraph>è¿å¹´ä¾ï¼å·²ç¶å¼é²åç¨®æ¹æ³ä¾è§£éãé»ç®±ãAI æ¨¡åçè¼¸åºãç¶èï¼ç®åä¸¦ä¸æ¸æ¥ä½¿ç¨èæ¯å¦å¯¦éçè§£åä¿¡ä»»éäºè§£éãå¨æ¬æä¸­ï¼æåå°æ³¨æ¼è©ä¼°ççé¢¨éªçåæ­¸å·¥å·çè§£éï¼ä¸¦æ¢è¨è§£éçå§å®¹åæ ¼å¼å°ä»¥ä½¿ç¨èçºä¸­å¿ççè§£åä¿¡ä»»ææ¨çå½±é¿ãéæ¼å§å®¹ï¼æåå¯¦é©äºå©ç¨®è§£éæ¹æ³ï¼æµè¡ç SHAPï¼åºæ¼åå¼è«æ¦å¿µï¼å æ­¤å°æ¼æ¥å¸¸ä½¿ç¨èä¾èªªå¯è½å¾è¤éï¼ä»¥ååºæ¼ç¹å¾µé®è½ç occlusion-1ï¼å¯è½æ´ææ¼çè§£ãéæ¼æ ¼å¼ï¼æåå° SHAP è§£éåç¾çºåè¡¨ (SC)ï¼éæ¯æ£ä¾ï¼èå° occlusion-1 è§£éåç¾çºåè¡¨ (OC) ä»¥åæå­ (OT)ï¼å¶è¼çºç°¡å®çæ§è³ªä¹é©ç¨æ¼æ­¤ãéäºå¯¦é©ç­åæ¼ä½¿ç¨èç ç©¶ï¼è©¢ååèèï¼å·æå©ç¨®ä¸åç¨åº¦çå°æ¥­ç¥è­ï¼ä¸è¬æ°ç¾åå·åä¸äºé«å­¸è¨ç·´çäººï¼ï¼ä»åå°åæ­¸å·¥å·è¼¸åºè§£éçä¸»è§åå®¢è§çè§£åä¿¡ä»»ãå¨å©é ç ç©¶ä¸­ï¼æåç¼ç¾ï¼å¨åºæ¼å§å®¹é²è¡æ¯è¼æï¼ä¸è¬ä¾èªªï¼occlusion-1 åªæ¼ SHAP è§£éï¼å¨ä¸»è§çè§£åä¿¡ä»»æ¹é¢ææé¡¯çåå¥½ãç¶èï¼å¨åæ§å¶æ ¼å¼çææ³ä¸ç´æ¥æ¯è¼è§£éï¼å¨å¤§å¤æ¸ææ³ä¸åªé¡¯ç¤º OT åªæ¼ SC è§£éçè­æï¼éè¡¨æ occlusion-1 åªæ¼ SHAP è§£éçä¸»å°å°ä½å¯è½æ¯ç±åå¥½æå­èéåè¡¨ä½çºè§£éæé©åçãæå¾ï¼æåæ²æç¼ç¾è§£éé¡åå¨å®¢è§çè§£æ¹é¢çå·®ç°è­æãå æ­¤ï¼ç¸½é«èè¨ï¼å°è§£éçå§å®¹åæ ¼å¼çé¸æéè¦ä»ç´°æ³¨æï¼å çºå¨æäºææ³ä¸ï¼æ ¼å¼èéå§å®¹ï¼å¯è½å¨æ¹åä½¿ç¨èé«é©æ¹é¢ç¼æ®ééµä½ç¨ã</paragraph>

##### **Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**
2409.00163v1 by Yuhan Zheng, Jessie A Elliott, John V Reynolds, Sheraz R Markar, BartÅomiej W. PapieÅ¼, ENSURE study group

Esophageal cancer is a major cause of cancer-related mortality
internationally, with high recurrence rates and poor survival even among
patients treated with curative-intent surgery. Investigating relevant
prognostic factors and predicting prognosis can enhance post-operative clinical
decision-making and potentially improve patients' outcomes. In this work, we
assessed prognostic factor identification and discriminative performances of
three models for Disease-Free Survival (DFS) and Overall Survival (OS) using a
large multicenter international dataset from ENSURE study. We first employed
Cox Proportional Hazards (CoxPH) model to assess the impact of each feature on
outcomes. Subsequently, we utilised CoxPH and two deep neural network
(DNN)-based models, DeepSurv and DeepHit, to predict DFS and OS. The
significant prognostic factors identified by our models were consistent with
clinical literature, with post-operative pathologic features showing higher
significance than clinical stage features. DeepSurv and DeepHit demonstrated
comparable discriminative accuracy to CoxPH, with DeepSurv slightly
outperforming in both DFS and OS prediction tasks, achieving C-index of 0.735
and 0.74, respectively. While these results suggested the potential of DNNs as
prognostic tools for improving predictive accuracy and providing personalised
guidance with respect to risk stratification, CoxPH still remains an adequately
good prediction model, with the data used in this study.

æè¦ï¼é£éçæ¯åééççç¸éæ­»äº¡çä¸»è¦åå ï¼å³ä½¿æ¥åæ ¹æ²»æ§æè¡çæ£èï¼å¾©ç¼çé«ä¸å­æ´»çä½ãæ¢è¨ç¸éé å¾å å­ä¸¦é æ¸¬é å¾ï¼å¯ä»¥å¢é²è¡å¾è¨åºæ±ºç­å¶å®ï¼ä¸¦å¯è½æ¹åæ£èççµæãå¨éé å·¥ä½ä¸­ï¼æåè©ä¼°äºä¸ç¨®æ¨¡åçé å¾å å­è­å¥åå¤å¥æè½ï¼åå¥éå°ç¡ç¾çå­æ´»æ (DFS) åæ´é«å­æ´»æ (OS)ï¼ä½¿ç¨ ENSURE ç ç©¶çå¤§åå¤ä¸­å¿åéæ¸æéãæåé¦åæ¡ç¨ Cox æ¯ä¾é¢¨éª (CoxPH) æ¨¡åä¾è©ä¼°æ¯åç¹å¾µå°çµæçå½±é¿ãé¨å¾ï¼æåå©ç¨ CoxPH åå©ååºæ¼æ·±åº¦ç¥ç¶ç¶²è·¯ (DNN) çæ¨¡å DeepSurv å DeepHit ä¾é æ¸¬ DFS å OSãæåçæ¨¡åè­å¥åºçé¡¯èé å¾å å­èè¨åºæç»ä¸è´ï¼è¡å¾ççç¹å¾µé¡¯ç¤ºåºæ¯è¨åºåæç¹å¾µæ´é«çé¡¯èæ§ãDeepSurv å DeepHit å±ç¾åºè CoxPH ç¸ç¶çå¤å¥æºç¢ºåº¦ï¼DeepSurv å¨ DFS å OS é æ¸¬ä»»åä¸­è¡¨ç¾ç¥åä¸ç±ï¼åå¥éå° C ææ¸ 0.735 å 0.74ãéç¶éäºçµæè¡¨æ DNN ä½çºé å¾å·¥å·çæ½åï¼å¯ä»¥æé«é æ¸¬æºç¢ºåº¦ä¸¦éå°é¢¨éªåå±¤æä¾åäººåæå°ï¼ä½ CoxPH ä»ç¶æ¯è¶³å¤ å¥½çé æ¸¬æ¨¡åï¼ä½¿ç¨æ¬ç ç©¶ä¸­çæ¸æã

##### **NDP: Next Distribution Prediction as a More Broad Target**
2408.17377v1 by Junhao Ruan, Abudukeyumu Abudula, Xinyu Liu, Bei Li, Yinqiao Li, Chenglong Wang, Yuchun Fan, Yuan Ge, Tong Xiao, Jingbo Zhu

Large language models (LLMs) trained on next-token prediction (NTP) paradigm
have demonstrated powerful capabilities. However, the existing NTP paradigm
contains several limitations, particularly related to planned task
complications and error propagation during inference. In our work, we extend
the critique of NTP, highlighting its limitation also due to training with a
narrow objective: the prediction of a sub-optimal one-hot distribution. To
support this critique, we conducted a pre-experiment treating the output
distribution from powerful LLMs as efficient world data compression. By
evaluating the similarity between the $n$-gram distribution and the one-hot
distribution with LLMs, we observed that the $n$-gram distributions align more
closely with the output distribution of LLMs. Based on this insight, we
introduce Next Distribution Prediction (NDP), which uses $n$-gram distributions
to replace the one-hot targets, enhancing learning without extra online
training time. We conducted experiments across translation, general task,
language transfer, and medical domain adaptation. Compared to NTP, NDP can
achieve up to +2.97 COMET improvement in translation tasks, +0.61 average
improvement in general tasks, and incredible +10.75 average improvement in the
medical domain. This demonstrates the concrete benefits of addressing the
target narrowing problem, pointing to a new direction for future work on
improving NTP.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) æ ¹æä¸ä¸åç¬¦èé æ¸¬ (NTP) ç¯ä¾é²è¡è¨ç·´ï¼å·²å±ç¾å¼·å¤§çåè½ãç¶èï¼ç¾æç NTP ç¯ä¾åå«äºå¹¾åéå¶ï¼ç¹å¥æ¯èè¨ç«ä»»åè¤éæ§åæ¨è«æéçé¯èª¤å³æ­æéãå¨æåçç ç©¶ä¸­ï¼æåæ´å±äº NTP çæ¹å¤ï¼å¼·èª¿å¶éå¶ä¹ç±æ¼ä½¿ç¨ç¹éçç®æ¨é²è¡è¨ç·´ï¼é æ¸¬æ¬¡ä½³çä¸ç±åä½ãçºäºæ¯æéé æ¹å¤ï¼æåé²è¡äºä¸ååç½®å¯¦é©ï¼å°å¼·å¤§ LLM çè¼¸åºåä½è¦çºææçä¸çè³æå£ç¸®ãééè©ä¼° $n$-gram åä½è LLM çä¸ç±åä½ä¹éçç¸ä¼¼æ§ï¼æåè§å¯å° $n$-gram åä½è LLM çè¼¸åºåä½æ´çºæ¥è¿ãåºæ¼éåè¦è§£ï¼æåå¼å¥äºä¸ä¸ååä½é æ¸¬ (NDP)ï¼å®ä½¿ç¨ $n$-gram åä½ä¾åä»£ä¸ç±ç®æ¨ï¼å¨æ²æé¡å¤ç·ä¸è¨ç·´æéçææ³ä¸ï¼å å¼·å­¸ç¿ãæåé²è¡äºç¿»è­¯ãä¸è¬ä»»åãèªè¨è½ç§»åé«å­¸é åé©æçå¯¦é©ãè NTP ç¸æ¯ï¼NDP å¨ç¿»è­¯ä»»åä¸­å¯ä»¥éå° +2.97 COMET æ¹é²ï¼å¨ä¸è¬ä»»åä¸­å¹³åæ¹é² +0.61ï¼å¨é«å­¸é åä¸­ä»¤äººé£ä»¥ç½®ä¿¡çå¹³åæ¹é² +10.75ãéè­æäºè§£æ±ºç®æ¨æ¶çªåé¡çå·é«å¥½èï¼æåºäºæ¹é² NTP æªä¾å·¥ä½çå¨æ°æ¹åã

##### **Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**
2408.17011v2 by Jutika Borah, Kumaresh Sarmah, Hidam Kumarjit Singh

Imaging techniques such as Chest X-rays, whole slide images, and optical
coherence tomography serve as the initial screening and detection for a wide
variety of medical pulmonary and ophthalmic conditions respectively. This paper
investigates the intricacies of using pretrained deep convolutional neural
networks with transfer learning across diverse medical imaging datasets with
varying modalities for binary and multiclass classification. We conducted a
comprehensive performance analysis with ten network architectures and model
families each with pretraining and random initialization. Our finding showed
that the use of pretrained models as fixed feature extractors yields poor
performance irrespective of the datasets. Contrary, histopathology microscopy
whole slide images have better performance. It is also found that deeper and
more complex architectures did not necessarily result in the best performance.
This observation implies that the improvements in ImageNet are not parallel to
the medical imaging tasks. Within a medical domain, the performance of the
network architectures varies within model families with shifts in datasets.
This indicates that the performance of models within a specific modality may
not be conclusive for another modality within the same domain. This study
provides a deeper understanding of the applications of deep learning techniques
in medical imaging and highlights the impact of pretrained networks across
different medical imaging datasets under five different experimental settings.

æè¦ï¼å½±åæè¡ï¼ä¾å¦è¸é¨ X åãå¨åçå½±åååå­¸ç¸å¹²æ·å±¤ææï¼åå¥ä½çºåç¨®é«å­¸èºé¨åç¼ç§ç¾ççåæ­¥ç¯©æª¢ååµæ¸¬ãæ¬ææ¢è¨äºä½¿ç¨é è¨ç·´æ·±åº¦å·ç©ç¥ç¶ç¶²è·¯æ­éé·ç§»å­¸ç¿ï¼æ©«è·¨ä¸åé«çå½±åè³æéï¼ä»¥é²è¡äºååå¤é¡å¥åé¡çè¤éæ§ãæåå°åç¨®ç¶²è·¯æ¶æ§åæ¨¡åç³»åé²è¡äºå¨é¢çæè½åæï¼æ¯åæ¶æ§åç³»åé½ç¶éé è¨ç·´åé¨æ©åå§åãæåçç¼ç¾é¡¯ç¤ºï¼å°é è¨ç·´æ¨¡åç¨ä½åºå®ç¹å¾µèåå¨æç¢çä¸ä½³çæè½ï¼èè³æéç¡éãç¸åå°ï¼çµç¹ççå­¸é¡¯å¾®é¡å¨åçå½±åæè¼å¥½çæè½ãæåä¹ç¼ç¾ï¼è¼æ·±ä¸è¤éçæ¶æ§ä¸¦éä¸å®æç¢çæä½³æè½ãæ­¤è§å¯çµææå³è ImageNet çæ¹è¯ä¸¦æªèé«çå½±åä»»åå¹³è¡ãå¨é«çé åå§ï¼ç¶²è·¯æ¶æ§çæè½æé¨èè³æéçè½æèæ¹è®æ¨¡åç³»åãéè¡¨ç¤ºå¨ç¹å®æ¨¡å¼ä¸­æ¨¡åçæè½å¯è½ç¡æ³æ±ºå®å¨åä¸åé åä¸­å¦ä¸ç¨®æ¨¡å¼çæè½ãæ¬ç ç©¶æä¾äºå°æ·±åº¦å­¸ç¿æè¡å¨é«çå½±åä¸­çæç¨æ´æ·±å¥ççè§£ï¼ä¸¦å¼·èª¿äºé è¨ç·´ç¶²è·¯å¨äºç¨®ä¸åå¯¦é©è¨­å®ä¸è·¨ä¸åé«çå½±åè³æéçå½±é¿ã

##### **A Survey for Large Language Models in Biomedicine**
2409.00133v1 by Chong Wang, Mengyao Li, Junjun He, Zhongruo Wang, Erfan Darzi, Zan Chen, Jin Ye, Tianbin Li, Yanzhou Su, Jing Ke, Kaili Qu, Shuxin Li, Yi Yu, Pietro LiÃ², Tianyun Wang, Yu Guang Wang, Yiqing Shen

Recent breakthroughs in large language models (LLMs) offer unprecedented
natural language understanding and generation capabilities. However, existing
surveys on LLMs in biomedicine often focus on specific applications or model
architectures, lacking a comprehensive analysis that integrates the latest
advancements across various biomedical domains. This review, based on an
analysis of 484 publications sourced from databases including PubMed, Web of
Science, and arXiv, provides an in-depth examination of the current landscape,
applications, challenges, and prospects of LLMs in biomedicine, distinguishing
itself by focusing on the practical implications of these models in real-world
biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot
learning across a broad spectrum of biomedical tasks, including diagnostic
assistance, drug discovery, and personalized medicine, among others, with
insights drawn from 137 key studies. Then, we discuss adaptation strategies of
LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to
enhance their performance in specialized biomedical contexts where zero-shot
fails to achieve, such as medical question answering and efficient processing
of biomedical literature. Finally, we discuss the challenges that LLMs face in
the biomedicine domain including data privacy concerns, limited model
interpretability, issues with dataset quality, and ethics due to the sensitive
nature of biomedical data, the need for highly reliable model outputs, and the
ethical implications of deploying AI in healthcare. To address these
challenges, we also identify future research directions of LLM in biomedicine
including federated learning methods to preserve data privacy and integrating
explainable AI methodologies to enhance the transparency of LLMs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çææ°çªç ´æä¾äºåææªæçèªç¶èªè¨çè§£åçæè½åãç¶èï¼ç¾æéæ¼çç©é«å­¸ä¸­ LLM çèª¿æ¥éå¸¸å°æ³¨æ¼ç¹å®æç¨ææ¨¡åæ¶æ§ï¼ç¼ºä¹æ´ååç¨®çç©é«å­¸é åææ°é²å±çå¨é¢åæãæ¬ç¶è¿°åºæ¼å°ä¾èª PubMedãWeb of Science å arXiv ç­æ¸æåº«ç 484 ç¯åºçç©çåæï¼æ·±å¥æ¢è¨äºçç©é«å­¸ä¸­ LLM çç¶åç¾æ³ãæç¨ãææ°ååæ¯ï¼å¶ç¹é»æ¯éæ³¨éäºæ¨¡åå¨ç¾å¯¦ä¸ççç©é«å­¸èæ¯ä¸­çå¯¦éæç¨ãé¦åï¼æåæ¢è¨äº LLM å¨å»£æ³ççç©é«å­¸ä»»åä¸­çé¶æ¬¡å­¸ç¿è½åï¼åæ¬è¨ºæ·è¼å©ãè¥ç©ç¼ç¾ååæ§åé«çç­ï¼ä¸¦å¾ 137 é ééµç ç©¶ä¸­æ±²åè¦è§£ãç¶å¾ï¼æåè¨è«äº LLM çé©æç­ç¥ï¼åæ¬å®æ¨¡æåå¤æ¨¡æ LLM çå¾®èª¿æ¹æ³ï¼ä»¥å¢å¼·å®åå¨é¶æ¬¡å­¸ç¿ç¡æ³å¯¦ç¾çå°æ¥­çç©é«å­¸èæ¯ä¸­çæ§è½ï¼ä¾å¦é«çåé¡è§£ç­åçç©é«å­¸æç»çææèçãæå¾ï¼æåè¨è«äº LLM å¨çç©é«å­¸é åé¢è¨çææ°ï¼åæ¬æ¸æé±ç§åé¡ãæ¨¡åå¯è§£éæ§æéãæ¸æéè³ªéåé¡ä»¥åç±æ¼çç©é«å­¸æ¸æçæææ§ãå°é«åº¦å¯é æ¨¡åè¼¸åºçéæ±ä»¥åå¨é«çä¿å¥ä¸­é¨ç½² AI çå«çå½±é¿èç¢ççå«çåé¡ãçºäºæå°éäºææ°ï¼æåéç¢ºå®äºçç©é«å­¸ä¸­ LLM æªä¾çç ç©¶æ¹åï¼åæ¬ç¨æ¼ä¿è­·æ¸æé±ç§çè¯åå­¸ç¿æ¹æ³ä»¥åæ´åå¯è§£é AI æ¹æ³ä»¥å¢å¼· LLM çéæåº¦ã

##### **Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**
2408.16343v1 by Yifei Chen, Shenghao Zhu, Zhaojie Fang, Chang Liu, Binfeng Zou, Yuhe Wang, Shuo Chang, Fan Jia, Feiwei Qin, Jin Fan, Yong Peng, Changmiao Wang

Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by
memory loss, executive dysfunction, and personality changes. Early diagnosis is
challenging due to subtle symptoms and varied presentations, often leading to
misdiagnosis with traditional unimodal diagnostic methods due to their limited
scope. This study introduces an advanced multimodal classification model that
integrates clinical, cognitive, neuroimaging, and EEG data to enhance
diagnostic accuracy. The model incorporates a feature tagger with a tabular
data coding architecture and utilizes the TimesBlock module to capture
intricate temporal patterns in Electroencephalograms (EEG) data. By employing
Cross-modal Attention Aggregation module, the model effectively fuses Magnetic
Resonance Imaging (MRI) spatial information with EEG temporal data,
significantly improving the distinction between AD, Mild Cognitive Impairment,
and Normal Cognition. Simultaneously, we have constructed the first AD
classification dataset that includes three modalities: EEG, MRI, and tabular
data. Our innovative approach aims to facilitate early diagnosis and
intervention, potentially slowing the progression of AD. The source code and
our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.

æè¦ï¼é¿è²æµ·é»ç (AD) æ¯ä¸ç¨®è¤éçç¥ç¶éåæ§ç¾çï¼ç¹å¾µæ¯è¨æ¶ååªå¤±ãå·è¡åè½éç¤åäººæ ¼æ¹è®ãç±æ¼ççå¾®å¦ä¸è¡¨ç¾å½¢å¼å¤æ¨£ï¼æ©æè¨ºæ·å·æææ°æ§ï¼éå¸¸ç±æ¼å³çµ±å®æ¨¡æè¨ºæ·æ¹æ³çç¯åæéèå°è´èª¤è¨ºãæ¬ç ç©¶å¼å¥äºä¸ååé²çå¤æ¨¡æåé¡æ¨¡åï¼å®æ´åäºè¨åºãèªç¥ãç¥ç¶å½±ååè¦é»åæ¸æï¼ä»¥æé«è¨ºæ·æºç¢ºæ§ãè©²æ¨¡åçµåäºä¸åå·æè¡¨æ ¼æ¸æç·¨ç¢¼æ¶æ§çç¹å¾µæ¨ç±¤å¨ï¼ä¸¦å©ç¨ TimesBlock æ¨¡çµä¾ææè¦é»å (EEG) æ¸æä¸­çè¤éæéæ¨¡å¼ãééæ¡ç¨è·¨æ¨¡ææ³¨æåèåæ¨¡çµï¼è©²æ¨¡åææå°èåäºç£å±æ¯æå (MRI) ç©ºéè³è¨åè¦é»åæéæ¸æï¼é¡¯èæ¹åäº ADãè¼åº¦èªç¥éç¤åæ­£å¸¸èªç¥ä¹éçåå¥ãåæï¼æåæ§å»ºäºç¬¬ä¸å AD åé¡æ¸æéï¼å¶ä¸­åå«ä¸ç¨®æ¨¡æï¼è¦é»åãç£å±æ¯æååè¡¨æ ¼æ¸æãæåçåµæ°æ¹æ³æ¨å¨ä¿é²æ©æè¨ºæ·åå¹²é ï¼æ½å¨å°æ¸ç·© AD çé²å±ãåå§ç¢¼åæåçç§äºº ADMC æ¸æéå¯å¨ https://github.com/JustlfC03/MSTNet ç²å¾ã

##### **Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**
2408.16256v1 by Xia Jiang, Yijun Zhou, Alan Wells, Adam Brufsky

Breast cancer is one of the two cancers responsible for the most deaths in
women, with about 42,000 deaths each year in the US. That there are over
300,000 breast cancers newly diagnosed each year suggests that only a fraction
of the cancers result in mortality. Thus, most of the women undergo seemingly
curative treatment for localized cancers, but a significant later succumb to
metastatic disease for which current treatments are only temporizing for the
vast majority. The current prognostic metrics are of little actionable value
for 4 of the 5 women seemingly cured after local treatment, and many women are
exposed to morbid and even mortal adjuvant therapies unnecessarily, with these
adjuvant therapies reducing metastatic recurrence by only a third. Thus, there
is a need for better prognostics to target aggressive treatment at those who
are likely to relapse and spare those who were actually cured. While there is a
plethora of molecular and tumor-marker assays in use and under-development to
detect recurrence early, these are time consuming, expensive and still often
un-validated as to actionable prognostic utility. A different approach would
use large data techniques to determine clinical and histopathological
parameters that would provide accurate prognostics using existing data. Herein,
we report on machine learning, together with grid search and Bayesian Networks
to develop algorithms that present a AUC of up to 0.9 in ROC analyses, using
only extant data. Such algorithms could be rapidly translated to clinical
management as they do not require testing beyond routine tumor evaluations.

æè¦ï¼ä¹³çæ¯é æå¥³æ§æ­»äº¡äººæ¸æå¤çå©ç¨®ççä¹ä¸ï¼æ¯å¹´ç´æ 42,000 åå¥³æ§æ­»æ¼ä¹³çãæ¯å¹´æè¶é 300,000 ä¾ä¹³çæ°ç¢ºè¨ºï¼éè¡¨ç¤ºåªæå°é¨åççæå°è´æ­»äº¡ãå æ­¤ï¼å¤§å¤æ¸å¥³æ§æ¥åå±é¨çççæ ¹æ²»æ§æ²»çï¼ä½è¨±å¤äººå¾ä¾ä»ææ­»æ¼è½ç§»æ§ç¾çï¼èç®åçæ²»çæ¹æ³å°çµå¤§å¤æ¸æ£èä¾èªªåªæ¯æ«æçãç®åçé å¾ææ¨å°æ¼ 5 åæ¥åå±é¨æ²»çå¾çä¼¼æ²»ççå¥³æ§ä¸­ï¼æ 4 åå¹¾ä¹æ²æå¯¦éå¹å¼ï¼è¨±å¤å¥³æ§ä¸å¿è¦å°æ¥åçæçè³è´å½çè¼å©çæ³ï¼èéäºè¼å©çæ³åè½å°è½ç§»æ§å¾©ç¼çéä½ä¸åä¹ä¸ãå æ­¤ï¼éè¦æ´å¥½çé å¾ææ¨ï¼æè½éå°é£äºå¯è½å¾©ç¼çäººé²è¡ç©æ¥µæ²»çï¼ä¸¦é¿åé£äºå¯¦éä¸å·²ç¶æ²»ççäººæ¥åæ²»çãéç¶æè¨±å¤åå­åè«ç¤æ¨è¨æª¢æ¸¬æ¹æ³æ­£å¨ä½¿ç¨åéç¼ä¸­ï¼å¯ä»¥åæ©ç¼ç¾å¾©ç¼ï¼ä½éäºæ¹æ³èæãæè²´ï¼èä¸ä½çºå¯æä½çé å¾å·¥å·ï¼å¶æç¨ä»ç¶å¸¸å¸¸æªç¶é©è­ãå¦ä¸ç¨®æ¹æ³æä½¿ç¨å¤§éçè³ææè¡ï¼ä¾ç¢ºå®è¨åºåçµç¹ççå­¸åæ¸ï¼ä¸¦ä½¿ç¨ç¾æè³ææä¾æºç¢ºçé å¾ææ¨ãå¨æ­¤ï¼æåå ±åäºæ©å¨å­¸ç¿ï¼ä»¥åç¶²æ ¼æå°åè²æ°ç¶²è·¯ï¼ç¨ä¾éç¼æ¼ç®æ³ï¼å¨ ROC åæä¸­æä¾é«é 0.9 ç AUCï¼åä½¿ç¨ç¾æè³æãæ­¤é¡æ¼ç®æ³å¯ä»¥å¿«éè½æçºè¨åºç®¡çï¼å çºå®åä¸éè¦é²è¡å¸¸è¦è«ç¤è©ä¼°ä»¥å¤çæ¸¬è©¦ã

##### **M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**
2408.16213v1 by Jonggwon Park, Soobum Kim, Byungmu Yoon, Jihun Hyun, Kyoyun Choi

The rapid evolution of artificial intelligence, especially in large language
models (LLMs), has significantly impacted various domains, including
healthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,
but with limitations: either underutilizing the multi-tasking capabilities of
LLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM
designed to enhance CXR interpretation. The model is trained on a visual
instruction-following dataset that integrates various task-specific datasets in
a conversational format. As a result, the model supports multiple tasks such as
medical report generation (MRG), visual grounding, and visual question
answering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by
employing a chain-of-thought prompting strategy, in which it identifies
findings in CXR images and subsequently generates corresponding reports. The
model is adaptable to various MRG scenarios depending on the available inputs,
such as single-image, multi-image, and multi-study contexts. In addition to
MRG, M4CXR performs visual grounding at a level comparable to specialized
models and also demonstrates outstanding performance in VQA. Both quantitative
and qualitative assessments reveal M4CXR's versatility in MRG, visual
grounding, and VQA, while consistently maintaining clinical accuracy.

æè¦ï¼äººå·¥æºæ§çå¿«éç¼å±ï¼ç¹å¥æ¯å¨å¤§åèªè¨æ¨¡å (LLM) ä¸­ï¼å·²å°åæ¬é«çä¿å¥å¨å§çååé åç¢çéå¤§å½±é¿ãå¨è¸é¨ X å (CXR) åæä¸­ï¼ååçç ç©¶å·²æ¡ç¨ LLMï¼ä½æå¶éå¶ï¼ä¸æ¯æªè½ååå©ç¨ LLM çå¤ä»»åèçè½åï¼å°±æ¯ç¼ºä¹è¨åºæºç¢ºæ§ãæ¬ææåº M4CXRï¼ä¸ç¨®å¤æ¨¡æ LLMï¼æ¨å¨å¢å¼· CXR è§£éãè©²æ¨¡åè¨ç·´æ¼è¦è¦ºæä»¤éµå¾ªè³æéï¼å¶ä¸­ä»¥å°è©±æ ¼å¼æ´ååç¨®ç¹å®ä»»åè³æéãå æ­¤ï¼è©²æ¨¡åæ¯æ´å¤é ä»»åï¼ä¾å¦é«çå ±åç¢ç (MRG)ãè¦è¦ºåºç¤åè¦è¦ºåé¡åç­ (VQA)ãM4CXR ééæ¡ç¨æèéæç¤ºç­ç¥ï¼å¨ MRG ä¸­éææåé²çè¨åºæºç¢ºæ§ï¼å¶ä¸­å®æè­å¥ CXR å½±åä¸­çç¼ç¾ï¼ä¸¦é¨å¾ç¢çå°æçå ±åãè©²æ¨¡åå¯æ ¹æå¯ç¨è¼¸å¥ï¼ä¾å¦å®ä¸å½±åãå¤éå½±ååå¤éç ç©¶èçµ¡ï¼é©æåç¨® MRG æå¢ãé¤äº MRG ä¹å¤ï¼M4CXR ä»¥èå°éæ¨¡åç¸ç¶çå±¤ç´å·è¡è¦è¦ºåºç¤ï¼ä¸¦å¨ VQA ä¸­å±ç¾åºè²çæè½ãå®éåå®æ§è©ä¼°åé¡¯ç¤ºåº M4CXR å¨ MRGãè¦è¦ºåºç¤å VQA ä¸­çå¤åè½æ§ï¼åææçºç¶­æè¨åºæºç¢ºæ§ã

##### **A Survey on Evaluation of Multimodal Large Language Models**
2408.15769v1 by Jiaxing Huang, Jingyi Zhang

Multimodal Large Language Models (MLLMs) mimic human perception and reasoning
system by integrating powerful Large Language Models (LLMs) with various
modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and
various modality encoders as sensory organs. This framework endows MLLMs with
human-like capabilities, and suggests a potential pathway towards achieving
artificial general intelligence (AGI). With the emergence of all-round MLLMs
like GPT-4V and Gemini, a multitude of evaluation methods have been developed
to assess their capabilities across different dimensions. This paper presents a
systematic and comprehensive review of MLLM evaluation methods, covering the
following key aspects: (1) the background of MLLMs and their evaluation; (2)
"what to evaluate" that reviews and categorizes existing MLLM evaluation tasks
based on the capabilities assessed, including general multimodal recognition,
perception, reasoning and trustworthiness, and domain-specific applications
such as socioeconomic, natural sciences and engineering, medical usage, AI
agent, remote sensing, video and audio processing, 3D point cloud analysis, and
others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into
general and specific benchmarks; (4) "how to evaluate" that reviews and
illustrates MLLM evaluation steps and metrics; Our overarching goal is to
provide valuable insights for researchers in the field of MLLM evaluation,
thereby facilitating the development of more capable and reliable MLLMs. We
emphasize that evaluation should be regarded as a critical discipline,
essential for advancing the field of MLLMs.

æè¦ï¼å¤æ¨¡æå¤§åèªè¨æ¨¡å (MLLM) ééæ´åå¼·å¤§çå¤§åèªè¨æ¨¡å (LLM) èåç¨®æ¨¡æç·¨ç¢¼å¨ï¼ä¾å¦è¦è¦ºãé³è¨ï¼ï¼æ¨¡æ¬äººé¡çæç¥åæ¨çç³»çµ±ï¼å° LLM å®ä½çºãå¤§è¦ãï¼èå°åç¨®æ¨¡æç·¨ç¢¼å¨å®ä½çºæå®å¨å®ãæ­¤æ¶æ§è³¦äº MLLM é¡ä¼¼äººé¡çè½åï¼ä¸¦æåºå¯¦ç¾äººå·¥éç¨æºæ§ (AGI) çæ½å¨éå¾ãé¨è GPT-4V å Gemini ç­å¨æ¹ä½ MLLM çåºç¾ï¼å·²ç¶éç¼åºå¤ç¨®è©ä¼°æ¹æ³ä¾è©ä¼°å®åå¨ä¸åç¶­åº¦ä¸çè½åãæ¬æå° MLLM è©ä¼°æ¹æ³é²è¡äºç³»çµ±ä¸å¨é¢çåé¡§ï¼æ¶µèä»¥ä¸å¹¾åééµé¢åï¼(1) MLLM åå¶è©ä¼°çèæ¯ï¼(2)ãè¦è©ä¼°ä»éº¼ãæ ¹æè©ä¼°çè½åï¼åé¡§ä¸¦åé¡ç¾æç MLLM è©ä¼°ä»»åï¼åæ¬ä¸è¬å¤æ¨¡æè¾¨è­ãæç¥ãæ¨çåå¯ä¿¡åº¦ï¼ä»¥åç¹å®é åçæç¨ï¼ä¾å¦ç¤¾æç¶æ¿ãèªç¶ç§å­¸åå·¥ç¨ãé«çç¨éãAI ä»£çãéæ¸¬ãå½±çåé³è¨èçã3D é»é²åæç­ï¼(3)ãå¨åªè£¡è©ä¼°ãå° MLLM è©ä¼°åºæºç¸½çµçºä¸è¬åºæºåç¹å®åºæºï¼(4)ãå¦ä½è©ä¼°ãåé¡§ä¸¦èªªæ MLLM è©ä¼°æ­¥é©åææ¨ãæåçé¦è¦ç®æ¨æ¯çº MLLM è©ä¼°é åçç ç©¶äººå¡æä¾æå¹å¼çè¦è§£ï¼å¾èä¿é²æ´å¼·å¤§ä¸å¯é ç MLLM çéç¼ãæåå¼·èª¿è©ä¼°æè¢«è¦çºä¸é ééµçå­¸ç§ï¼å°æ¼æ¨é² MLLM é åè³ééè¦ã

##### **Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**
2408.15498v1 by Yijun Zhou, Om Arora-Jain, Xia Jiang

While machine learning has advanced in medicine, its widespread use in
clinical applications, especially in predicting breast cancer metastasis, is
still limited. We have been dedicated to constructing a DFNN model to predict
breast cancer metastasis n years in advance. However, the challenge lies in
efficiently identifying optimal hyperparameter values through grid search,
given the constraints of time and resources. Issues such as the infinite
possibilities for continuous hyperparameters like l1 and l2, as well as the
time-consuming and costly process, further complicate the task. To address
these challenges, we developed Single Hyperparameter Grid Search (SHGS)
strategy, serving as a preselection method before grid search. Our experiments
with SHGS applied to DFNN models for breast cancer metastasis prediction focus
on analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,
learning rate, decay, and momentum. We created three figures, each depicting
the experiment results obtained from three LSM-I-10-Plus-year datasets. These
figures illustrate the relationship between model performance and the target
hyperparameter values. For each hyperparameter, we analyzed whether changes in
this hyperparameter would affect model performance, examined if there were
specific patterns, and explored how to choose values for the particular
hyperparameter. Our experimental findings reveal that the optimal value of a
hyperparameter is not only dependent on the dataset but is also significantly
influenced by the settings of other hyperparameters. Additionally, our
experiments suggested some reduced range of values for a target hyperparameter,
which may be helpful for low-budget grid search. This approach serves as a
prior experience and foundation for subsequent use of grid search to enhance
model performance.

æè¦ï¼åç®¡æ©å¨å­¸ç¿å¨é«å­¸é åå·²ææé²å±ï¼ä½å¶å¨è¨åºæç¨ä¸­çå»£æ³ä½¿ç¨ï¼ç¹å¥æ¯å¨é æ¸¬ä¹³çè½ç§»æ¹é¢ï¼ä»æå¶éå¶ãæåè´åæ¼å»ºæ§ DFNN æ¨¡åï¼ä»¥é æ¸¬ä¹³çè½ç§» n å¹´ãç¶èï¼ææ°å¨æ¼ééç¶²æ ¼æå°ææçå°æ¾åºæä½³è¶åæ¸å¼ï¼éåå°æéåè³æºçéå¶ãè«¸å¦ l1 å l2 ç­é£çºè¶åæ¸çå¯è½æ§ç¡çª®ï¼ä»¥åèæä¸æè²´çéç¨ç­åé¡ï¼æ´è®éé ä»»åè®å¾è¤éãçºäºæå°éäºææ°ï¼æåéç¼äºå®ä¸è¶åæ¸ç¶²æ ¼æå° (SHGS) ç­ç¥ï¼ä½çºç¶²æ ¼æå°åçé é¸æ¹æ³ãæåéå°ä¹³çè½ç§»é æ¸¬æç¨ç DFNN æ¨¡åé²è¡ SHGS å¯¦é©ï¼éé»åæå«åç®æ¨è¶åæ¸ï¼epoch æ¬¡æ¸ãæ¹æ¬¡å¤§å°ãä¸­æ·ãL1ãL2ãå­¸ç¿çãè¡°æ¸ååéãæåè£½ä½äºä¸å¹åï¼æ¯å¹åé½æç¹ªäºå¾ä¸å LSM-I-10-Plus-year è³æéç²å¾çå¯¦é©çµæãéäºåè¡¨èªªæäºæ¨¡åæè½èç®æ¨è¶åæ¸å¼ä¹éçéä¿ãå°æ¼æ¯åè¶åæ¸ï¼æååæäºè¶åæ¸çè®åæ¯å¦æå½±é¿æ¨¡åæè½ï¼ä¸¦æª¢è¦æ¯å¦æç¹å®æ¨¡å¼ï¼ä»¥åå¦ä½éå°ç¹å®è¶åæ¸é¸æå¼ãæåçå¯¦é©çµæé¡¯ç¤ºï¼è¶åæ¸çæä½³å¼ä¸ååæ±ºæ¼è³æéï¼ä¹åå°å¶ä»è¶åæ¸è¨­å®çé¡¯èå½±é¿ãæ­¤å¤ï¼æåçå¯¦é©å»ºè­°ç¸®å°ç®æ¨è¶åæ¸å¼çç¯åï¼éå¯è½æå©æ¼ä½é ç®çç¶²æ ¼æå°ãæ­¤æ¹æ³å¯ä½çºå¾çºä½¿ç¨ç¶²æ ¼æå°ä»¥å¢å¼·æ¨¡åæè½çååç¶é©ååºç¤ã

##### **What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**
2408.15354v1 by Jana Schaich Borg, Hannah Read

Interest is growing in artificial empathy, but so is confusion about what
artificial empathy is or needs to be. This confusion makes it challenging to
navigate the technical and ethical issues that accompany empathic AI
development. Here, we outline a framework for thinking about empathic AI based
on the premise that different constellations of capabilities associated with
empathy are important for different empathic AI applications. We describe
distinctions of capabilities that we argue belong under the empathy umbrella,
and show how three medical empathic AI use cases require different sets of
these capabilities. We conclude by discussing why appreciation of the diverse
capabilities under the empathy umbrella is important for both AI creators and
users.

æè¦ï¼å°äººå·¥åçå¿è¶ä¾è¶æèè¶£ï¼ä½å°æ¼äººå·¥åçå¿æ¯ä»éº¼æéè¦ä»éº¼ä¹è¶ä¾è¶å°æãéç¨®æ··æ·ä½¿å¾é£ä»¥è§£æ±ºä¼´é¨åçå¿ AI éç¼èä¾çæè¡åå«çåé¡ãå¨æ­¤ï¼æåæ¦è¿°äºä¸åæèåçå¿ AI çæ¶æ§ï¼å¶åºæ¼éæ¨£ä¸ååæï¼èåçå¿ç¸éçä¸åè½åçµåå°æ¼ä¸åçåçå¿ AI æç¨å¾éè¦ãæåæè¿°äºæåèªçºå±¬æ¼åçå¿ç¯ççè½ååå¥ï¼ä¸¦å±ç¤ºäºä¸åé«çåçå¿ AI ä½¿ç¨æ¡ä¾éè¦éäºè½åçä¸åçµåãæåæå¾è¨è«äºçºä»éº¼æ¬£è³åçå¿ç¯çä¸çåç¨®è½åå°æ¼ AI åµé èåä½¿ç¨èé½å¾éè¦çåå ã

##### **Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**
2408.15217v1 by Weiyi Zhang, Siyu Huang, Jiancheng Yang, Ruoyu Chen, Zongyuan Ge, Yingfeng Zheng, Danli Shi, Mingguang He

Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal
vascular dynamics and aiding in the diagnosis of eye diseases. However, its
invasive nature and less accessibility compared to Color Fundus (CF) images
pose significant challenges. Current CF to FFA translation methods are limited
to static generation. In this work, we pioneer dynamic FFA video generation
from static CF images. We introduce an autoregressive GAN for smooth,
memory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic
lesion changes in FFA regions, we design a knowledge mask based on clinical
experience. Leveraging this mask, our approach integrates innovative knowledge
mask-guided techniques, including knowledge-boosted attention, knowledge-aware
discriminators, and mask-enhanced patchNCE loss, aimed at refining generation
in critical areas and addressing the pixel misalignment challenge. Our method
achieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common
video generation approaches. Human assessment by an ophthalmologist confirms
its high generation quality. Notably, our knowledge mask surpasses supervised
lesion segmentation masks, offering a promising non-invasive alternative to
traditional FFA for research and clinical applications. The code is available
at https://github.com/Michi-3000/Fundus2Video.

æè¦ï¼ç¼åºè¢åè¡ç®¡æå½± (FFA) æ¯è©ä¼°è¦ç¶²èè¡ç®¡ååå­¸ååå©è¨ºæ·ç¼ç¾çéè¦å·¥å·ãç¶èï¼èå½©è²ç¼åº (CF) å½±åç¸æ¯ï¼å¶ä¾µå¥æ§è¼é«ä¸åå¾ä¸æï¼å æ­¤é æéå¤§ææ°ãç®å CF è½ææ FFA çç¿»è­¯æ¹æ³åéæ¼éæç¢çãå¨éé å·¥ä½ä¸­ï¼æåçåå¾éæ CF å½±åç¢çåæ FFA å½±çãæåå¼å¥ä¸åèªè¿´æ­¸ GANï¼ä»¥é²è¡æµæ¢ä¸ç¯çè¨æ¶é«çéå¹ FFA åæãçºäºå å¼·å° FFA ååä¸­åæçç¶è®åçéæ³¨ï¼æåæ ¹æè¨åºç¶é©è¨­è¨äºä¸åç¥è­é®ç½©ãééå©ç¨éåé®ç½©ï¼æåçåæ³æ´åäºåµæ°çç¥è­é®ç½©å¼å°æè¡ï¼åæ¬ç¥è­å¢å¼·çæ³¨æåãç¥è­æç¥çè¾¨å¥å¨ä»¥åé®ç½©å¢å¼·ç patchNCE æå¤±ï¼æ¨å¨æ¹åééµååççæä¸¦è§£æ±ºåç´ æªå°é½çææ°ãèå¶ä»å¸¸è¦çå½±ççææ¹æ³ç¸æ¯ï¼æåçåæ³éå°äºæä½³ç FVD 1503.21 å PSNR 11.81ãç¼ç§é«å¸«çäººçºè©ä¼°è­å¯¦äºå¶çæåè³ªå¾é«ãå¼å¾æ³¨æçæ¯ï¼æåçç¥è­é®ç½©è¶è¶äºæç£ç£ççç¶åå²é®ç½©ï¼çºå³çµ± FFA æä¾äºä¸åæåéçéä¾µå¥æ§æ¿ä»£æ¹æ¡ï¼å¯ç¨æ¼ç ç©¶åè¨åºæç¨ãç¨å¼ç¢¼å¯å¨ https://github.com/Michi-3000/Fundus2Video åå¾ã

##### **Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**
2409.00112v1 by Daniil Filienko, Yinzhou Wang, Caroline El Jazmi, Serena Xie, Trevor Cohen, Martine De Cock, Weichao Yuwen

While Large Language Models (LLMs) are being quickly adapted to many domains,
including healthcare, their strengths and pitfalls remain under-explored. In
our study, we examine the effects of prompt engineering to guide Large Language
Models (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session
via text, particularly during the symptom identification and assessment phase
for personalized goal setting. We present evaluation results of the models'
performances by automatic metrics and experienced medical professionals. We
demonstrate that the models' capability to deliver protocolized therapy can be
improved with the proper use of prompt engineering methods, albeit with
limitations. To our knowledge, this study is among the first to assess the
effects of various prompting techniques in enhancing a generalist model's
ability to deliver psychotherapy, focusing on overall quality, consistency, and
empathy. Exploring LLMs' potential in delivering psychotherapy holds promise
with the current shortage of mental health professionals amid significant
needs, enhancing the potential utility of AI-based and AI-enhanced care
services.

æè¦ï¼é¨èå¤§åèªè¨æ¨¡åï¼LLMï¼å¿«éé©ææ¼è¨±å¤é åï¼åæ¬é«çä¿å¥ï¼å®åçåªå¢åç¼ºé·ä»æªå¾å°ååæ¢ç´¢ãå¨æåçç ç©¶ä¸­ï¼æåæ¢è¨äºæç¤ºå·¥ç¨å¨å¼å°å¤§åèªè¨æ¨¡åï¼LLMï¼ééæå­æä¾åé¡è§£æ±ºçæ³ï¼PSTï¼ç°ç¯çé¨åå§å®¹çææï¼ç¹å¥æ¯å¨ççè­å¥åè©ä¼°éæ®µï¼ç¨æ¼åæ§åç®æ¨è¨­å®ãæåééèªååææ¨åç¶é©è±å¯çé«çå°æ¥­äººå¡å±ç¤ºäºæ¨¡åæ§è½çè©ä¼°çµæãæåè­æäºæ¨¡åæä¾ç¨å¼åæ²»ççè½åå¯ä»¥ä½¿ç¨æç¤ºå·¥ç¨æ¹æ³é©ç¶ä½¿ç¨ä¾æ¹é²ï¼åç®¡æå±éæ§ãææåæç¥ï¼éé ç ç©¶æ¯ç¬¬ä¸æ¹è©ä¼°åç¨®æç¤ºæè¡å°å¢å¼·éææ¨¡åæä¾å¿çæ²»çè½åçå½±é¿çç ç©¶ä¹ä¸ï¼éé»éæ³¨æ´é«åè³ªãä¸è´æ§ååçå¿ãå¨å¿çå¥åº·å°æ¥­äººå¡å´éç­ç¼ºä¸éæ±å·¨å¤§çææ³ä¸ï¼æ¢ç´¢ LLM å¨æä¾å¿çæ²»çæ¹é¢çæ½åå¾æåæ¯ï¼å¢å¼·äºåºæ¼ AI å AI å¢å¼·çè­·çæåçæ½å¨æç¨ã

##### **Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**
2408.15121v1 by Francesco Sovrano, Michael Lognoul, Giulia Vilone

Significant investment and development have gone into integrating Artificial
Intelligence (AI) in medical and healthcare applications, leading to advanced
control systems in medical technology. However, the opacity of AI systems
raises concerns about essential characteristics needed in such sensitive
applications, like transparency and trustworthiness. Our study addresses these
concerns by investigating a process for selecting the most adequate Explainable
AI (XAI) methods to comply with the explanation requirements of key EU
regulations in the context of smart bioelectronics for medical devices. The
adopted methodology starts with categorising smart devices by their control
mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving
into their technology. Then, we analyse these regulations to define their
explainability requirements for the various devices and related goals.
Simultaneously, we classify XAI methods by their explanatory objectives. This
allows for matching legal explainability requirements with XAI explanatory
goals and determining the suitable XAI algorithms for achieving them. Our
findings provide a nuanced understanding of which XAI algorithms align better
with EU regulations for different types of medical devices. We demonstrate this
through practical case studies on different neural implants, from chronic
disease management to advanced prosthetics. This study fills a crucial gap in
aligning XAI applications in bioelectronics with stringent provisions of EU
regulations. It provides a practical framework for developers and researchers,
ensuring their AI innovations advance healthcare technology and adhere to legal
and ethical standards.

æè¦ï¼äººå·¥æºæ§ï¼AIï¼å¨é«çåä¿å¥æç¨ä¸­æå¥äºå¤§éçæè³åéç¼ï¼é²èå°è´é«çæè¡ä¸­çåé²æ§å¶ç³»çµ±ãç¶èï¼AI ç³»çµ±çä¸éææ§å¼ç¼äºå°æ­¤é¡æææç¨ä¸­æéåºæ¬ç¹æ§çææï¼ä¾å¦éæåº¦åå¯ä¿¡åº¦ãæåçç ç©¶ééèª¿æ¥ä¸åç¨åºä¾è§£æ±ºéäºåé¡ï¼ç¨æ¼é¸ææååçå¯è§£é AIï¼XAIï¼æ¹æ³ï¼ä»¥ç¬¦åæ­çæ³è¦å¨é«çå¨æçæºæ§åçç©é»å­å­¸ä¸­çèªªæè¦æ±ãæ¡ç¨çæ¹æ³å¾ééå¶æ§å¶æ©å¶ï¼éè¿´è·¯ãéè¿´è·¯ååéè¿´è·¯ç³»çµ±ï¼å°æºæ§åè£ç½®é²è¡åé¡ï¼ä¸¦æ·±å¥æ¢è¨å¶æè¡éå§ãç¶å¾ï¼æååæéäºæ³è¦ä»¥å®ç¾©å¶å°åç¨®è£ç½®åç¸éç®æ¨çå¯è§£éæ§è¦æ±ãåæï¼æåééå¶èªªæç®æ¨å° XAI æ¹æ³é²è¡åé¡ãéåè¨±å°æ³å¾å¯è§£éæ§è¦æ±è XAI èªªæç®æ¨ç¸å¹éï¼ä¸¦ç¢ºå®é©ç¶ç XAI æ¼ç®æ³ä¾éæå®åãæåçç ç©¶çµææä¾äºå°åªäº XAI æ¼ç®æ³æ´ç¬¦åæ­çæ³è¦ä»¥é©ç¨æ¼ä¸åé¡åçé«çå¨æçç´°ç·»çè§£ãæåééä¸åç¥ç¶æ¤å¥ç©çå¯¦éæ¡ä¾ç ç©¶ä¾è­æéä¸é»ï¼å¾æ¢æ§ç¾çç®¡çå°åé²çç¾©è¢ãéé ç ç©¶å¡«è£äºå°çç©é»å­å­¸ä¸­ç XAI æç¨èæ­çæ³è¦çå´æ ¼è¦å®ç¸ç¬¦çéè¦ç©ºç½ãå®çºéç¼äººå¡åç ç©¶äººå¡æä¾äºä¸åå¯¦ç¨çæ¶æ§ï¼ç¢ºä¿å¶ AI åµæ°è½ä¿é²é«çæè¡ä¸¦éµå®æ³å¾åéå¾·æ¨æºã

##### **MiWaves Reinforcement Learning Algorithm**
2408.15076v1 by Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy

The escalating prevalence of cannabis use poses a significant public health
challenge globally. In the U.S., cannabis use is more prevalent among emerging
adults (EAs) (ages 18-25) than any other age group, with legalization in the
multiple states contributing to a public perception that cannabis is less risky
than in prior decades. To address this growing concern, we developed MiWaves, a
reinforcement learning (RL) algorithm designed to optimize the delivery of
personalized intervention prompts to reduce cannabis use among EAs. MiWaves
leverages domain expertise and prior data to tailor the likelihood of delivery
of intervention messages. This paper presents a comprehensive overview of the
algorithm's design, including key decisions and experimental outcomes. The
finalized MiWaves RL algorithm was deployed in a clinical trial from March to
May 2024.

æè¦ï¼å¤§éº»ä½¿ç¨çä¸æ·ä¸åï¼å°å¨çå¬å±è¡çæ§æéå¤§ææ°ãå¨ç¾åï¼å¤§éº»ä½¿ç¨çå¨å¹´è¼æå¹´äººï¼EAï¼ï¼18-25 æ­²ï¼ä¸­æ¯ä»»ä½å¶ä»å¹´é½¡çµé½è¦æ®éï¼å¤åå·çåæ³åå°è´å¬ç¾èªçºå¤§éº»æ¯éå»å¹¾åå¹´é¢¨éªè¼ä½ãçºäºè§£æ±ºéåæ¥çå´éçåé¡ï¼æåéç¼äº MiWavesï¼éæ¯ä¸ç¨®å¢å¼·å­¸ç¿ (RL) æ¼ç®æ³ï¼æ¨å¨åªååæ§åå¹²é æç¤ºçå³éï¼ä»¥æ¸å° EA ä¸­çå¤§éº»ä½¿ç¨ãMiWaves å©ç¨é åå°æ¥­ç¥è­åååçæ¸æä¾èª¿æ´å¹²é è¨æ¯å³éçå¯è½æ§ãæ¬æå¨é¢æ¦è¿°äºæ¼ç®æ³çè¨­è¨ï¼åæ¬ééµæ±ºç­åå¯¦é©çµæãæçµç MiWaves RL æ¼ç®æ³å·²æ¼ 2024 å¹´ 3 æè³ 5 æå¨è¨åºè©¦é©ä¸­é¨ç½²ã

##### **Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**
2408.15032v1 by Yuqi Zhang, Xiaoqian Zhang, Jiakai Wang, Yuancheng Yang, Taiying Peng, Chao Tong

Computational pathology (CPath) has significantly advanced the clinical
practice of pathology. Despite the progress made, Multiple Instance Learning
(MIL), a promising paradigm within CPath, continues to face challenges,
particularly related to incomplete information utilization. Existing
frameworks, such as those based on Convolutional Neural Networks (CNNs),
attention, and selective scan space state sequential model (SSM), lack
sufficient flexibility and scalability in fusing diverse features, and cannot
effectively fuse diverse features. Additionally, current approaches do not
adequately exploit order-related and order-independent features, resulting in
suboptimal utilization of sequence information. To address these limitations,
we propose a novel MIL framework called Mamba2MIL. Our framework utilizes the
state space duality model (SSD) to model long sequences of patches of whole
slide images (WSIs), which, combined with weighted feature selection, supports
the fusion processing of more branching features and can be extended according
to specific application needs. Moreover, we introduce a sequence transformation
method tailored to varying WSI sizes, which enhances sequence-independent
features while preserving local sequence information, thereby improving
sequence information utilization. Extensive experiments demonstrate that
Mamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive
experiments across multiple datasets, achieving improvements in nearly all
performance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a
binary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the
BRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an
accuracy of 0.4981. The code is available at
https://github.com/YuqiZhang-Buaa/Mamba2MIL.

æè¦ï¼<paragraph>è¨ç®ççå­¸ (CPath) å·²é¡¯èæåççå­¸çè¨åºå¯¦åãåç®¡å·²æé²å±ï¼ä½çº CPath ä¸­ä¸åæåéçç¯ä¾ï¼å¤éå¯¦ä¾å­¸ç¿ (MIL) æçºé¢è¨ææ°ï¼ç¹å¥æ¯èä¸å®æ´è³è¨ä½¿ç¨æéãç¾æçæ¶æ§ï¼ä¾å¦åºæ¼å·ç©ç¥ç¶ç¶²è·¯ (CNN)ãæ³¨æååé¸ææ§ææç©ºéçæåºåæ¨¡å (SSM) çæ¶æ§ï¼å¨èååç¨®ç¹å¾µæç¼ºä¹è¶³å¤ çå½æ§åå¯æ´åæ§ï¼ä¸ç¡æ³ææèååç¨®ç¹å¾µãæ­¤å¤ï¼ç®åçä½æ³ä¸¦æªååå©ç¨èé åºç¸éåèé åºç¡éçç¹å¾µï¼å°è´åºåè³è¨ä½¿ç¨çä¸ä½³ãçºäºè§£æ±ºéäºéå¶ï¼æåæåºä¸ååçº Mamba2MIL çæ° MIL æ¶æ§ãæåçæ¶æ§å©ç¨çæç©ºéå°å¶æ¨¡å (SSD) ä¾å»ºæ¨¡å¨å¹»ççå½±å (WSI) çé·åºåè²¼çï¼éèå æ¬ç¹å¾µé¸åçµåä½¿ç¨ï¼æ¯æ´æ´å¤åæ¯ç¹å¾µçèåèçï¼ä¸å¯æ ¹æç¹å®æç¨éæ±é²è¡å»¶ä¼¸ãæ­¤å¤ï¼æåå¼å¥ä¸ç¨®éå°ä¸å WSI å¤§å°éèº«æé çåºåè½ææ¹æ³ï¼éå¢å¼·äºèåºåç¡éçç¹å¾µï¼åæä¿çäºå±é¨åºåè³è¨ï¼é²èæ¹ååºåè³è¨ä½¿ç¨çãå»£æ³çå¯¦é©è­æ Mamba2MIL è¶è¶äºæåé²ç MIL æ¹æ³ãæåå¨å¤åè³æéä¸é²è¡å»£æ³çå¯¦é©ï¼å¨å¹¾ä¹æææè½ææ¨ä¸åç²å¾æ¹åãç¹å¥æ¯å¨ NSCLC è³æéä¸ï¼Mamba2MIL éå° 0.9533 çäºåè«ç¤åé¡ AUC å 0.8794 çæºç¢ºåº¦ãå¨ BRACS è³æéä¸ï¼å®éå° 0.7986 çå¤é¡å¥åé¡ AUC å 0.4981 çæºç¢ºåº¦ãç¨å¼ç¢¼å¯å¨ https://github.com/YuqiZhang-Buaa/Mamba2MIL åå¾ã</paragraph>

##### **Sequence-aware Pre-training for Echocardiography Probe Guidance**
2408.15026v1 by Haojun Jiang, Zhenguo Sun, Yu Sun, Ning Jia, Meng Li, Shaqi Luo, Shiji Song, Gao Huang

Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe
pose to obtain high-quality sectional images. Cardiac ultrasound faces two
major challenges: (1) the inherently complex structure of the heart, and (2)
significant individual variations. Previous works have only learned the
population-averaged 2D and 3D structures of the heart rather than personalized
cardiac structural features, leading to a performance bottleneck. Clinically,
we observed that sonographers adjust their understanding of a patient's cardiac
structure based on prior scanning sequences, thereby modifying their scanning
strategies. Inspired by this, we propose a sequence-aware self-supervised
pre-training method. Specifically, our approach learns personalized 2D and 3D
cardiac structural features by predicting the masked-out images and actions in
a scanning sequence. We hypothesize that if the model can predict the missing
content it has acquired a good understanding of the personalized cardiac
structure. In the downstream probe guidance task, we also introduced a sequence
modeling approach that models individual cardiac structural information based
on the images and actions from historical scan data, enabling more accurate
navigation decisions. Experiments on a large-scale dataset with 1.36 million
samples demonstrated that our proposed sequence-aware paradigm can
significantly reduce navigation errors, with translation errors decreasing by
15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared
to state-of-the-art methods.

æè¦ï¼<paragraph>å¿èè¶é³æ³¢æ¢é ­å¼å°æ¨å¨å¹«å©æ°æèª¿æ´ 6-DOF æ¢é ­å§¿å¢ï¼ä»¥åå¾é«åè³ªçæ·é¢å½±åãå¿èè¶é³æ³¢é¢è¨å©é ä¸»è¦ææ°ï¼(1) å¿èçµæ§è¤éä¸åºæï¼ä»¥å (2) åé«å·®ç°é¡¯èãååçç ç©¶åå­¸ç¿äºæ´é«å¹³åç 2D å 3D å¿èçµæ§ï¼èéåäººåçè§£åç¹å¾µï¼å°è´æè½ç¶é ¸ãè¨åºä¸ï¼æåè§å¯å°è¶é³æ³¢æå¸«ææ ¹æååçææåºåèª¿æ´ä»åå°æ£èå¿èçµæ§ççè§£ï¼é²èä¿®æ¹ä»åçææç­ç¥ãåå°æ­¤åç¼ï¼æåæåºä¸åå·åºåæç¥çèªç£ç£é è¨ç·´æ¹æ³ãå·é«ä¾èªªï¼æåçåæ³ééé æ¸¬ææåºåä¸­é®ç½©çå½±åååä½ï¼ä¾å­¸ç¿åäººåç 2D å 3D å¿èè§£åç¹å¾µãæååè¨­ï¼å¦ææ¨¡åå¯ä»¥é æ¸¬éºæ¼çå§å®¹ï¼å®ä¾¿å°åäººåçè§£åçµæ§æäºè¯å¥½ççè§£ãå¨ä¸æ¸¸çæ¢é ­å¼å°ä»»åä¸­ï¼æåä¹å°å¥ä¸ååºåå»ºæ¨¡æ¹æ³ï¼è©²æ¹æ³æ ¹ææ­·å²ææè³æä¸­çå½±åååä½ï¼æ¨¡æ¬åå¥å¿èè§£åè³è¨ï¼é²èååºæ´ç²¾ç¢ºçå°èªæ±ºç­ãå¨æ 136 è¬åæ¨£æ¬çå¤§è¦æ¨¡è³æéä¸é²è¡çå¯¦é©è­æï¼æåæåºçå·åºåæç¥çå¸ç¯å¯ä»¥å¤§å¹æ¸å°å°èªé¯èª¤ï¼å¶ä¸­å¹³ç§»é¯èª¤æ¸å°äº 15.90% è³ 36.87%ï¼æè½é¯èª¤æ¸å°äº 11.13% è³ 20.77%ï¼èæåé²çæ¹æ³ç¸æ¯ã</paragraph>

##### **Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**
2408.15294v2 by Christos Theodoropoulos, Natasha Mulligan, Joao Bettencourt-Silva

Developing novel predictive models with complex biomedical information is
challenging due to various idiosyncrasies related to heterogeneity,
standardization or sparseness of the data. We previously introduced a
person-centric ontology to organize information about individual patients, and
a representation learning framework to extract person-centric knowledge graphs
(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a
systematic approach to examine the results of GNN models trained with both
structured and unstructured information from the MIMIC-III dataset. Through
ablation studies on different clinical, demographic, and social data, we show
the robustness of this approach in identifying predictive features in PKGs for
the task of readmission prediction.

æè¦ï¼éç¼å·æè¤éçç©é«å­¸è³è¨çæ°ç©é æ¸¬æ¨¡åï¼ç±æ¼è³æçç°è³ªæ§ãæ¨æºåæç¨çæ§ï¼å æ­¤å·æææ°æ§ãæåååä»ç´¹äºä¸åä»¥äººçºä¸­å¿çæ¬é«ï¼ç¨æ¼çµç¹æéåå¥æ£èçè³è¨ï¼ä»¥åä¸åè¡¨ç¤ºå­¸ç¿æ¶æ§ï¼ç¨æ¼æåä»¥äººçºä¸­å¿çç¥è­åè­ (PKG) åè¨ç·´åå½¢ç¥ç¶ç¶²è·¯ (GNN)ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®ç³»çµ±æ§çæ¹æ³ä¾æª¢é©ä½¿ç¨ MIMIC-III è³æéä¸­ççµæ§ååéçµæ§åè³è¨è¨ç·´ç GNN æ¨¡åççµæãééå°ä¸åçè¨åºãäººå£çµ±è¨åç¤¾æè³æé²è¡æ¶èç ç©¶ï¼æåå±ç¤ºäºéç¨®æ¹æ³å¨è­å¥ PKG ä¸­çé æ¸¬ç¹å¾µä»¥é²è¡åå¥é¢é æ¸¬ä»»åæçç©©å¥æ§ã

##### **Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**
2408.14754v1 by Qiaoxin Li, Ruifeng Chen, Peng Wang, Guotao Quan, Yanfeng Du, Dong Liang, Yinsheng Li

Dual-energy computed tomography (DECT) has been widely used to obtain
quantitative elemental composition of imaged subjects for personalized and
precise medical diagnosis. Compared with DECT leveraging advanced X-ray source
and/or detector technologies, the use of the sequential-scanning data
acquisition scheme to implement DECT may make a broader impact on clinical
practice because this scheme requires no specialized hardware designs and can
be directly implemented into conventional CT systems. However, since the
concentration of iodinated contrast agent in the imaged subject varies over
time, sequentially scanned data sets acquired at two tube potentials are
temporally inconsistent. As existing material basis image reconstruction
approaches assume that the data sets acquired at two tube potentials are
temporally consistent, the violation of this assumption results in inaccurate
quantification of material concentration. In this work, we developed
sequential-scanning DECT imaging using high temporal resolution image
reconstruction and error-compensated material basis image generation,
ACCELERATION in short, to address the technical challenge induced by temporal
inconsistency of sequentially scanned data sets and improve quantification
accuracy of material concentration in sequential-scanning DECT. ACCELERATION
has been validated and evaluated using numerical simulation data sets generated
from clinical human subject exams and experimental human subject studies.
Results demonstrated the improvement of quantification accuracy and image
quality using ACCELERATION.

æè¦ï¼éè½éé»è¦æ·å±¤ææ (DECT) å·²å»£æ³ç¨æ¼åå¾å½±åååè©¦èçå®éåç´ çµæï¼ä»¥é²è¡åäººåä¸ç²¾ç¢ºçé«çè¨ºæ·ãèä½¿ç¨é²é X åæºå/æåµæ¸¬å¨æè¡ç DECT ç¸æ¯ï¼ä½¿ç¨é£çºææè³ææ·åæ¹æ¡ä¾å¯¦ä½ DECT å¯è½å°è¨åºå¯¦åç¢çæ´å»£æ³çå½±é¿ï¼å çºæ­¤æ¹æ¡ä¸éè¦å°éçç¡¬é«è¨­è¨ï¼ä¸å¯ç´æ¥å¯¦ä½å°å³çµ± CT ç³»çµ±ä¸­ãç¶èï¼ç±æ¼å½±åååè©¦èä¸­ç¢åå°æ¯åçæ¿åº¦æé¨æéèè®åï¼å æ­¤å¨å©åç®¡é»ä½ä¸æ·åçé£çºææè³æéå¨æéä¸ä¸¦ä¸ä¸è´ãç±æ¼ç¾æçææåºç¤å½±åéå»ºæ¹æ³åè¨­å¨å©åç®¡é»ä½ä¸æ·åçè³æéå¨æéä¸æ¯ä¸è´çï¼å æ­¤éåæ­¤åè¨­æå°è´æææ¿åº¦çéåä¸æºç¢ºãå¨éé å·¥ä½ä¸­ï¼æåéç¼äºä½¿ç¨é«æéè§£æåº¦å½±åéå»ºåèª¤å·®è£åææåºç¤å½±åç¢ççé£çºææ DECT å½±åï¼ç°¡ç¨± ACCELERATIONï¼ä»¥è§£æ±ºé£çºææè³æéæéä¸ä¸è´æå¼ç¼çæè¡ææ°ï¼ä¸¦æ¹åé£çºææ DECT ä¸­æææ¿åº¦çéåæºç¢ºåº¦ãACCELERATION å·²ä½¿ç¨å¾è¨åºäººé«åè©¦èæª¢æ¥åå¯¦é©äººé«åè©¦èç ç©¶ç¢ççæ¸å¼æ¨¡æ¬è³æéé²è¡é©è­åè©ä¼°ãçµæè­æä½¿ç¨ ACCELERATION å¯æ¹åéåæºç¢ºåº¦åå½±ååè³ªã

##### **Large Language Models for Disease Diagnosis: A Scoping Review**
2409.00097v1 by Shuang Zhou, Zidu Xu, Mian Zhang, Chunpu Xu, Yawen Guo, Zaifu Zhan, Sirui Ding, Jiashuo Wang, Kaishuai Xu, Yi Fang, Liqiao Xia, Jeremy Yeung, Daochen Zha, Mingquan Lin, Rui Zhang

Automatic disease diagnosis has become increasingly valuable in clinical
practice. The advent of large language models (LLMs) has catalyzed a paradigm
shift in artificial intelligence, with growing evidence supporting the efficacy
of LLMs in diagnostic tasks. Despite the growing attention in this field, many
critical research questions remain under-explored. For instance, what diseases
and LLM techniques have been investigated for diagnostic tasks? How can
suitable LLM techniques and evaluation methods be selected for clinical
decision-making? To answer these questions, we performed a comprehensive
analysis of LLM-based methods for disease diagnosis. This scoping review
examined the types of diseases, associated organ systems, relevant clinical
data, LLM techniques, and evaluation methods reported in existing studies.
Furthermore, we offered guidelines for data preprocessing and the selection of
appropriate LLM techniques and evaluation strategies for diagnostic tasks. We
also assessed the limitations of current research and delineated the challenges
and future directions in this research field. In summary, our review outlined a
blueprint for LLM-based disease diagnosis, helping to streamline and guide
future research endeavors.

æè¦ï¼èªåç¾çè¨ºæ·å¨è¨åºå¯¦åä¸­è®å¾è¶ä¾è¶æå¹å¼ãå¤§èªè¨æ¨¡å (LLM) çåºç¾å¬åäºäººå·¥æºè½çå¸ç¯è½ç§»ï¼è¶ä¾è¶å¤è­ææ¯æ LLM å¨è¨ºæ·ä»»åä¸­çæè½ãåç®¡éåé ååå°è¶ä¾è¶å¤çéæ³¨ï¼ä½è¨±å¤ééµçç ç©¶åé¡ä»æªå¾å°ååæ¢è¨ãä¾å¦ï¼åªäºç¾çå LLM æè¡å·²è¢«èª¿æ¥ç¨æ¼è¨ºæ·ä»»åï¼å¦ä½çºè¨åºæ±ºç­å¶å®é¸æåé©ç LLM æè¡åè©ä¼°æ¹æ³ï¼çºäºåç­éäºåé¡ï¼æåå°åºæ¼ LLM çç¾çè¨ºæ·æ¹æ³é²è¡äºå¨é¢çåæãéé ç¯åæ¢è¨åé¡§äºç¾æç ç©¶ä¸­å ±åçç¾çé¡åãç¸éå¨å®ç³»çµ±ãç¸éè¨åºè³æãLLM æè¡åè©ä¼°æ¹æ³ãæ­¤å¤ï¼æåæä¾äºè³æåèçåé¸æé©ç¶ç LLM æè¡åè©ä¼°ç­ç¥ä»¥é²è¡è¨ºæ·ä»»åçæåãæåéè©ä¼°äºç¶åç ç©¶çéå¶ï¼ä¸¦æè¿°äºéåç ç©¶é åçææ°åæªä¾æ¹åãç¸½ä¹ï¼æåçåé¡§æ¦è¿°äºåºæ¼ LLM çç¾çè¨ºæ·èåï¼æå©æ¼ç°¡ååæå°æªä¾çç ç©¶å·¥ä½ã

##### **Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI**
2409.06723v1 by Yukyeong Song, Jinhee Kim, Wanli Xing, Zifeng Liu, Chenglu Li, Hyunju Oh

While mathematical creative writing can potentially engage students in
expressing mathematical ideas in an imaginative way, some elementary school-age
students struggle in this process. Generative AI (GenAI) offers possibilities
for supporting creative writing activities, such as providing story generation.
However, the design of GenAI-powered learning technologies requires careful
consideration of the technology reception in the actual classrooms. This study
explores students' and teachers' perceptions of creative mathematical writing
with the developed GenAI-powered technology. The study adopted a qualitative
thematic analysis of the interviews, triangulated with open-ended survey
responses and classroom observation of 79 elementary school students, resulting
in six themes and 19 subthemes. This study contributes by investigating the
lived experience of GenAI-supported learning and the design considerations for
GenAI-powered learning technologies and instructions.

æè¦ï¼<paragraph>éç¶æ¸å­¸åµæå¯«ä½æå¯è½è®å­¸çä»¥å¯ææ³ååçæ¹å¼è¡¨éæ¸å­¸æ¦å¿µï¼ä½æäºå°å­¸å¹´é½¡çå­¸çå¨éåéç¨ä¸­æéå°å°é£ãçæå¼ AI (GenAI) æä¾äºæ¯æåµæå¯«ä½æ´»åçå¯è½æ§ï¼ä¾å¦æä¾æäºçæãç¶èï¼GenAI é©åçå­¸ç¿æè¡çè¨­è¨éè¦ä»ç´°èæ®æè¡å¨å¯¦éæå®¤ä¸­çæ¥ååº¦ãæ¬ç ç©¶æ¢è¨äºå­¸çåæå¸«å°ä½¿ç¨å·²éç¼ç GenAI é©åæè¡é²è¡åµææ¸å­¸å¯«ä½ççæ³ãæ¬ç ç©¶æ¡ç¨äºè¨ªè«çå®æ§ä¸»é¡åæï¼ä¸¦è 79 åå°å­¸ççéæ¾å¼èª¿æ¥åæåæå®¤è§å¯é²è¡ä¸è§é©è­ï¼ç¢çäºå­åä¸»é¡å 19 åå­ä¸»é¡ãæ¬ç ç©¶ééèª¿æ¥ GenAI æ¯æçå­¸ç¿ççå¯¦é«é©ä»¥å GenAI é©åçå­¸ç¿æè¡åèªªæçè¨­è¨èéï¼ååºäºè²¢ç»ã</paragraph>

##### **Improving Clinical Note Generation from Complex Doctor-Patient Conversation**
2408.14568v1 by Yizhan Li, Sifan Wu, Christopher Smith, Thomas Lo, Bang Liu

Writing clinical notes and documenting medical exams is a critical task for
healthcare professionals, serving as a vital component of patient care
documentation. However, manually writing these notes is time-consuming and can
impact the amount of time clinicians can spend on direct patient interaction
and other tasks. Consequently, the development of automated clinical note
generation systems has emerged as a clinically meaningful area of research
within AI for health. In this paper, we present three key contributions to the
field of clinical note generation using large language models (LLMs). First, we
introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex
doctor-patient conversations paired with their full clinical notes. This
dataset, created and curated by medical experts with the help of modern neural
networks, provides a valuable resource for training and evaluating models in
clinical note generation tasks. Second, we propose the K-SOAP (Keyword,
Subjective, Objective, Assessment, and Plan) note format, which enhances
traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and
Plan) notes by adding a keyword section at the top, allowing for quick
identification of essential information. Third, we develop an automatic
pipeline to generate K-SOAP notes from doctor-patient conversations and
benchmark various modern LLMs using various metrics. Our results demonstrate
significant improvements in efficiency and performance compared to standard LLM
finetuning methods.

æè¦ï¼æ°å¯«è¨åºç­è¨åè¨éé«çæª¢æ¥æ¯é«çä¿å¥å°æ¥­äººå¡çä¸é éè¦ä»»åï¼æ¯æ£èç§è­·æä»¶ä¸­çéè¦çµæé¨åãç¶èï¼æåæ°å¯«éäºç­è¨å¾èæï¼ä¸¦ä¸æå½±é¿è¨åºé«çè±å¨ç´æ¥æ£èäºååå¶ä»ä»»åä¸çæéãå æ­¤ï¼èªååè¨åºç­è¨çæç³»çµ±çéç¼å·²æçº AI å¨å¥åº·é åä¸­å·æè¨åºæç¾©çç ç©¶é åãå¨æ¬æä¸­ï¼æåæåºäºä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) é²è¡è¨åºç­è¨çæçé åç 3 é ééµè²¢ç»ãé¦åï¼æåä»ç´¹äº CliniKnoteï¼éæ¯ä¸åç¶åæ§æ¸æéï¼åå« 1,200 åè¤éçé«æ£å°è©±åå¶å®æ´çè¨åºç­è¨ãæ­¤æ¸æéç±é«å­¸å°å®¶å¨ç¾ä»£ç¥ç¶ç¶²è·¯çå¹«å©ä¸åµå»ºåç­åï¼çºè¨åºç­è¨çæä»»åä¸­çæ¨¡åè¨ç·´åè©ä¼°æä¾äºå¯¶è²´çè³æºãå¶æ¬¡ï¼æåæåºäº K-SOAPï¼ééµå­ãä¸»è§ãå®¢è§ãè©ä¼°åè¨ç«ï¼ç­è¨æ ¼å¼ï¼å®ééå¨é é¨æ·»å ä¸åééµå­é¨åä¾å¢å¼·å³çµ±ç SOAP~\cite{podder2023soap}ï¼ä¸»è§ãå®¢è§ãè©ä¼°åè¨ç«ï¼ç­è¨ï¼ä»¥ä¾¿å¿«éè­å¥åºæ¬è³è¨ãç¬¬ä¸ï¼æåéç¼äºä¸åèªååç®¡éï¼å¾é«æ£å°è©±ä¸­çæ K-SOAP ç­è¨ï¼ä¸¦ä½¿ç¨åç¨®ææ¨å°åç¨®ç¾ä»£ LLM é²è¡åºæºæ¸¬è©¦ãæåççµæè¡¨æï¼èæ¨æº LLM å¾®èª¿æ¹æ³ç¸æ¯ï¼æçåæ§è½æäºé¡¯èçæåã

##### **Temporal Ensemble Logic**
2408.14443v2 by Guo-Qiang Zhang

We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal
logic for linear-time temporal reasoning. TEL includes primitive temporal
constructs such as ``always up to $t$ time later'' ($\Box_t$), ``sometimes
before $t$ time in the future'' ($\Diamond_t$), and ``$t$-time later''
$\varphi_t$. TEL has been motivated from the requirement for rigor and
reproducibility for cohort specification and discovery in clinical and
population health research, to fill a gap in formalizing temporal reasoning in
biomedicine. Existing logical frameworks such as linear temporal logic are too
restrictive to express temporal and sequential properties in biomedicine, or
too permissive in semantic constructs, such as in Halpern-Shoham logic, to
serve this purpose. In this paper, we first introduce TEL in a general set up,
with discrete and dense time as special cases. We then focus on the theoretical
development of discrete TEL on the temporal domain of positive integers
$\mathbb{N}^+$, denoted as ${\rm TEL}_{\mathbb{N}^+}$. ${\rm
TEL}_{\mathbb{N}^+}$ is strictly more expressive than the standard monadic
second order logic, characterized by B\"{u}chi automata. We present its formal
semantics, a proof system, and provide a proof for the undecidability of the
satisfiability of ${\rm TEL}_{\mathbb{N}^+}$. We also include initial results
on expressiveness and decidability fragments for ${\rm TEL}_{\mathbb{N}^+}$,
followed by application outlook and discussions.

æè¦ï¼<paragraph>æåä»ç´¹æééåéè¼¯ (TEL)ï¼ä¸ç¨®ç¨æ¼ç·æ§æéæææ¨ççä¸éå®å­æ¨¡æéè¼¯ãTEL åå«åå§ææçµæ§ï¼ä¾å¦ãå§çµå¨ $t$ æéå¾ã($\Box_t$)ããææå¨æªä¾ $t$ æéåã($\Diamond_t$) åã$t$ æéå¾ã$\varphi_t$ãTEL çåæ©æ¯çºäºè¨åºåäººå£å¥åº·ç ç©¶ä¸­ç¾¤çµè¦ç¯åç¼ç¾çå´è¬¹æ§åå¯åç¾æ§ï¼ä»¥å¡«è£çç©é«å­¸ä¸­æææ¨çå½¢å¼åçç©ºç½ãç¾æçéè¼¯æ¡æ¶ï¼ä¾å¦ç·æ§ææéè¼¯ï¼å°æ¼è¡¨éçç©é«å­¸ä¸­çææåé åºå±¬æ§éæ¼å´æ ¼ï¼æèå¨èªç¾©çµæ§ä¸éæ¼å¯¬é¬ï¼ä¾å¦å¨ Halpern-Shoham éè¼¯ä¸­ï¼ï¼ç¡æ³éå°æ­¤ç®çãå¨æ¬æä¸­ï¼æåé¦åå¨ä¸è¬è¨­ç½®ä¸­ä»ç´¹ TELï¼å¶ä¸­é¢æ£æéåç¨ å¯æéçºç¹æ®ææ³ãç¶å¾ï¼æåå°æ³¨æ¼æ­£æ´æ¸æéå $\mathbb{N}^+$ ä¸é¢æ£ TEL ççè«ç¼å±ï¼è¡¨ç¤ºçº ${\rm TEL}_{\mathbb{N}^+}$. ${\rm TEL}_{\mathbb{N}^+}$ æ¯æ¨æºå®å­äºééè¼¯æ´å·è¡¨éåï¼å¶ç¹å¾µå¨æ¼ B\"{u}chi èªåæ©ãæåå±ç¤ºå¶å½¢å¼èªç¾©ãè­æç³»çµ±ï¼ä¸¦æä¾ ${\rm TEL}_{\mathbb{N}^+}$ å¯æ»¿è¶³æ§çä¸å¯å¤å®æ§çè­æãæåéåæ¬ ${\rm TEL}_{\mathbb{N}^+}$ çè¡¨éååå¯å¤å®çæ®µçåæ­¥çµæï¼ç¶å¾æ¯æç¨åæ¯åè¨è«ã</paragraph>

##### **MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**
2408.14418v2 by Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler

Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech
into text, yet the errors they introduce can significantly degrade the
performance of downstream tasks like summarization. This issue is particularly
pronounced in clinical dialogue summarization, a low-resource domain where
supervised data for fine-tuning is scarce, necessitating the use of ASR models
as black-box solutions. Employing conventional data augmentation for enhancing
the noise robustness of summarization models is not feasible either due to the
unavailability of sufficient medical dialogue audio recordings and
corresponding ASR transcripts. To address this challenge, we propose MEDSAGE,
an approach for generating synthetic samples for data augmentation using Large
Language Models (LLMs). Specifically, we leverage the in-context learning
capabilities of LLMs and instruct them to generate ASR-like errors based on a
few available medical dialogue examples with audio recordings. Experimental
results show that LLMs can effectively model ASR noise, and incorporating this
noisy data into the training process significantly improves the robustness and
accuracy of medical dialogue summarization systems. This approach addresses the
challenges of noisy ASR outputs in critical applications, offering a robust
solution to enhance the reliability of clinical dialogue summarization.

æè¦ï¼èªåèªé³è¾¨è­ (ASR) ç³»çµ±å¨å°èªé³è½éææå­æ¹é¢è³ééè¦ï¼ä½å®åæç¢ççé¯èª¤å¯è½æå¤§å¹éä½æè¦ç­ä¸æ¸¸ä»»åçæè½ãéååé¡å¨è¨åºå°è©±æè¦ä¸­ç¹å¥æé¡¯ï¼éæ¯ä¸åä½è³æºçé åï¼å¶ä¸­ç¨æ¼å¾®èª¿çç£ç£è³æå¾ç¨å°ï¼å æ­¤å¿é ä½¿ç¨ ASR æ¨¡åä½çºé»çè§£æ±ºæ¹æ¡ãç±æ¼ç¼ºä¹è¶³å¤ çé«çå°è©±é³è¨éé³åå°æç ASR è½éï¼æ¡ç¨å³çµ±è³ææ´åä¾å¢å¼·æè¦æ¨¡åçæåªæ§ä¹ä¸å¯è¡ãçºäºæå°éåææ°ï¼æåæåºäº MEDSAGEï¼éæ¯ä¸ç¨®ä½¿ç¨å¤§åèªè¨æ¨¡å (LLM) ç¢çåææ¨£æ¬é²è¡è³ææ´åçæ¹æ³ãå·é«ä¾èªªï¼æåå©ç¨ LLM çæå¢å­¸ç¿è½åï¼ä¸¦æç¤ºå®åæ ¹æå°æ¸å¸¶æé³è¨éé³çå¯ç¨é«çå°è©±ç¯ä¾ç¢çé¡ä¼¼ç ASR é¯èª¤ãå¯¦é©çµæé¡¯ç¤ºï¼LLM å¯ä»¥ææå°æ¨¡æ¬ ASR éè¨ï¼èå°éäºéè¨è³æç´å¥è¨ç·´éç¨ä¸­å¯ä»¥é¡¯èæé«é«çå°è©±æè¦ç³»çµ±çç©©å¥æ§åæºç¢ºæ§ãéç¨®æ¹æ³æå°äºééµæç¨ä¸­ ASR è¼¸åºéè¨çåé¡ï¼æä¾äºä¸åç©©å¥çè§£æ±ºæ¹æ¡ä¾å¢å¼·è¨åºå°è©±æè¦çå¯é æ§ã

##### **Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**
2408.14397v1 by Xiaoman Zhang, JuliÃ¡n N. Acosta, Hong-Yu Zhou, Pranav Rajpurkar

Recent advancements in artificial intelligence have significantly improved
the automatic generation of radiology reports. However, existing evaluation
methods fail to reveal the models' understanding of radiological images and
their capacity to achieve human-level granularity in descriptions. To bridge
this gap, we introduce a system, named ReXKG, which extracts structured
information from processed reports to construct a comprehensive radiology
knowledge graph. We then propose three metrics to evaluate the similarity of
nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs
(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative
analysis of AI-generated and human-written radiology reports, assessing the
performance of both specialist and generalist models. Our study provides a
deeper understanding of the capabilities and limitations of current AI models
in radiology report generation, offering valuable insights for improving model
performance and clinical applicability.

æè¦ï¼è¿æäººå·¥æºè½çé²å±é¡¯èæ¹åäºæ¾å°å ±åçèªåçæãç¶èï¼ç¾æçè©ä¼°æ¹æ³ç¡æ³æ­ç¤ºæ¨¡åå°æ¾å°å½±åççè§£ï¼ä»¥åå®åå¨æè¿°ä¸­éå°äººé¡å±¤ç´ç²¾ç´°åº¦çè½åãçºäºå½è£éåå·®è·ï¼æåå¼é²ä¸ååçº ReXKG çç³»çµ±ï¼å®å¾èçéçå ±åä¸­èååºçµæ§åçè³è¨ï¼ä»¥å»ºæ§ä¸åå¨é¢çæ¾å°ç¥è­åè­ãæ¥èï¼æåæåºä¸åææ¨ä¾è©ä¼°åç¨®ç¥è­åè­ä¸­ç¯é»çç¸ä¼¼æ§ (ReXKG-NSC)ãéç·£çåå¸ (ReXKG-AMS) åå­åçæ¶µèç¯å (ReXKG-SCS)ãæåå° AI çæçåäººé¡æ°å¯«çæ¾å°å ±åé²è¡æ·±å¥çæ¯è¼åæï¼è©ä¼°å°å®¶åéææ¨¡åçæè½ãæåçç ç©¶æä¾å°ç®å AI æ¨¡åå¨æ¾å°å ±åçæä¸­çè½ååéå¶æ´æ·±å¥ççè§£ï¼ä¸¦æä¾æå¹å¼çè¦è§£ä¾æ¹åæ¨¡åæè½åè¨åºæç¨ã

##### **Foundation Models for Music: A Survey**
2408.14340v3 by Yinghao Ma, Anders Ãland, Anton Ragni, Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elona Shatri, Fabio Morreale, Ge Zhang, GyÃ¶rgy Fazekas, Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wenhao Huang, Xingjian Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang

In recent years, foundation models (FMs) such as large language models (LLMs)
and latent diffusion models (LDMs) have profoundly impacted diverse sectors,
including music. This comprehensive review examines state-of-the-art (SOTA)
pre-trained models and foundation models in music, spanning from representation
learning, generative learning and multimodal learning. We first contextualise
the significance of music in various industries and trace the evolution of AI
in music. By delineating the modalities targeted by foundation models, we
discover many of the music representations are underexplored in FM development.
Then, emphasis is placed on the lack of versatility of previous methods on
diverse music applications, along with the potential of FMs in music
understanding, generation and medical application. By comprehensively exploring
the details of the model pre-training paradigm, architectural choices,
tokenisation, finetuning methodologies and controllability, we emphasise the
important topics that should have been well explored, like instruction tuning
and in-context learning, scaling law and emergent ability, as well as
long-sequence modelling etc. A dedicated section presents insights into music
agents, accompanied by a thorough analysis of datasets and evaluations
essential for pre-training and downstream tasks. Finally, by underscoring the
vital importance of ethical considerations, we advocate that following research
on FM for music should focus more on such issues as interpretability,
transparency, human responsibility, and copyright issues. The paper offers
insights into future challenges and trends on FMs for music, aiming to shape
the trajectory of human-AI collaboration in the music realm.

æè¦ï¼è¿å¹´ä¾ï¼åºç¤æ¨¡å (FM)ï¼ä¾å¦å¤§åèªè¨æ¨¡å (LLM) åæ½å¨æ´æ£æ¨¡å (LDM)ï¼å·²å°åæ¬é³æ¨å¨å§çä¸åç¢æ¥­ç¢çæ·±é å½±é¿ãéç¯å¨é¢æ§çè©è«æ¢è¨äºé³æ¨é åä¸­æåé² (SOTA) çé è¨ç·´æ¨¡åååºç¤æ¨¡åï¼æ¶µèäºè¡¨å¾µå­¸ç¿ãçæå¼å­¸ç¿åå¤æ¨¡æå­¸ç¿ãæåé¦åå°é³æ¨å¨åç¢æ¥­çéè¦æ§èçµ¡åï¼ä¸¦è¿½æº¯ AI å¨é³æ¨ä¸­çæ¼é²ãééæç¹ªåºç¤æ¨¡åæéå°çæ¨¡æï¼æåç¼ç¾è¨±å¤é³æ¨è¡¨å¾µå¨ FM éç¼ä¸­å°æªè¢«ååæ¢ç´¢ãæ¥èï¼æåå¼·èª¿ååæ¹æ³å¨ä¸åé³æ¨æç¨ä¸­ç¼ºä¹å¤æ¨£æ§ï¼ä»¥å FM å¨é³æ¨çè§£ãçæåé«çæç¨ä¸­çæ½åãééå¨é¢æ¢è¨æ¨¡åé è¨ç·´å¸ç¯ãæ¶æ§é¸æãæ¨è¨åãå¾®èª¿æ¹æ³åå¯æ§æ§çç´°ç¯ï¼æåå¼·èª¿äºææ·±å¥æ¢è¨çéè¦ä¸»é¡ï¼ä¾å¦æä»¤å¾®èª¿åæå¢å­¸ç¿ãè¦æ¨¡å®å¾åæ°èè½åï¼ä»¥åé·åºåå»ºæ¨¡ç­ãå°éçç« ç¯æä¾äºå°é³æ¨ä»£ççè¦è§£ï¼ä¸¦éæå°é è¨ç·´åä¸æ¸¸ä»»åè³ééè¦çè³æéåè©ä¼°çæ·±å¥åæãæå¾ï¼ééå¼·èª¿å«çèéçè³ééè¦æ§ï¼æåä¸»å¼µå¾çºéæ¼é³æ¨ FM çç ç©¶ææ´å°æ³¨æ¼å¯è§£éæ§ãéæåº¦ãäººé¡è²¬ä»»åçæ¬åé¡ç­è­°é¡ãæ¬ææä¾äºå°é³æ¨ FM æªä¾ææ°åè¶¨å¢çè¦è§£ï¼æ¨å¨å½¢å¡äººé¡è AI å¨é³æ¨é åä¸­åä½çè»è·¡ã

##### **Uncertainties of Latent Representations in Computer Vision**
2408.14281v1 by Michael Kirchhof

Uncertainty quantification is a key pillar of trustworthy machine learning.
It enables safe reactions under unsafe inputs, like predicting only when the
machine learning model detects sufficient evidence, discarding anomalous data,
or emitting warnings when an error is likely to be inbound. This is
particularly crucial in safety-critical areas like medical image classification
or self-driving cars. Despite the plethora of proposed uncertainty
quantification methods achieving increasingly higher scores on performance
benchmarks, uncertainty estimates are often shied away from in practice. Many
machine learning projects start from pretrained latent representations that
come without uncertainty estimates. Uncertainties would need to be trained by
practitioners on their own, which is notoriously difficult and
resource-intense.
  This thesis makes uncertainty estimates easily accessible by adding them to
the latent representation vectors of pretrained computer vision models. Besides
proposing approaches rooted in probability and decision theory, such as
Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both
theoretical and empirical questions. We show that these unobservable
uncertainties about unobservable latent representations are indeed provably
correct. We also provide an uncertainty-aware representation learning (URL)
benchmark to compare these unobservables against observable ground-truths.
Finally, we compile our findings to pretrain lightweight representation
uncertainties on large-scale computer vision models that transfer to unseen
datasets in a zero-shot manner.
  Our findings do not only advance the current theoretical understanding of
uncertainties over latent variables, but also facilitate the access to
uncertainty quantification for future researchers inside and outside the field,
enabling straightforward but trustworthy machine learning.

æè¦ï¼ä¸ç¢ºå®éåæ¯å¼å¾ä¿¡è³´æ©å¨å­¸ç¿çä¸å¤§æ¯æ±ã
å®è½è®æ©å¨å­¸ç¿æ¨¡åå¨ä¸å®å¨çè¼¸å¥ä¸ååºå®å¨çåæï¼ä¾å¦åªå¨æ©å¨å­¸ç¿æ¨¡ååµæ¸¬å°è¶³å¤ è­æææé²è¡é æ¸¬ãæ¨æ£ç°å¸¸è³æï¼ææ¯å¨å¯è½ç¼çé¯èª¤æç¼åºè­¦åãéå¨é«çå½±ååé¡æèªé§è»ç­å®å¨ééµé åä¸­ç¹å¥éè¦ãåç®¡æè¨±å¤å·²æåºçä¸ç¢ºå®éåæ¹æ³å¨æè½åºæºä¸åå¾è¶ä¾è¶é«çåæ¸ï¼ä½å¨å¯¦åä¸å»å¸¸å¸¸è¿´é¿ä¸ç¢ºå®æ§ä¼°è¨ãè¨±å¤æ©å¨å­¸ç¿å°æ¡å¾é è¨ç·´çæ½å¨è¡¨å¾µéå§ï¼èéäºè¡¨å¾µæ²æä¸ç¢ºå®æ§ä¼°è¨ãå¯¦åå·¥ä½èéè¦èªè¡è¨ç·´ä¸ç¢ºå®æ§ï¼éåºäºåçå°é£ä¸èè²»è³æºã
æ¬è«æééå°ä¸ç¢ºå®æ§ä¼°è¨æ°å¢å°é è¨ç·´é»è¦è¦è¦ºæ¨¡åçæ½å¨è¡¨å¾µåéä¸­ï¼è®ä¸ç¢ºå®æ§ä¼°è¨ææ¼åå¾ãé¤äºæåºæ¤åºæ¼æ©çåæ±ºç­çè«çæ¹æ³ï¼ä¾å¦èå°å¡ç¾è³è¨å°æ¯ä¼°è¨ (MCInfoNCE) åæå¤±é æ¸¬ä¹å¤ï¼æåéæ·±å¥æ¢è¨çè«åå¯¦è­åé¡ãæåè­æéäºéæ¼ä¸å¯è§å¯æ½å¨è¡¨å¾µçä¸å¯è§å¯ä¸ç¢ºå®æ§ç¢ºå¯¦å¯ä»¥è­ææ¯æ­£ç¢ºçãæåéæä¾ä¸åä¸ç¢ºå®æ§æç¥è¡¨å¾µå­¸ç¿ (URL) åºæºï¼ç¨ä¾æ¯è¼éäºä¸å¯è§å¯çä¸ç¢ºå®æ§èå¯è§å¯ççå¯¦å¼ãæå¾ï¼æåå°æåçç¼ç¾å½æ´èµ·ä¾ï¼å¨å¤§åé»è¦è¦è¦ºæ¨¡åä¸é è¨ç·´è¼éç´è¡¨å¾µä¸ç¢ºå®æ§ï¼ä¸¦ä»¥é¶æ¬¡å­¸ç¿çæ¹å¼è½ç§»å°æªè¦éçè³æéã
æåçç¼ç¾ä¸åæåäºç¶åå°æ½å¨è®æ¸ä¸ç¢ºå®æ§ççè«çè§£ï¼éä¿é²äºæªä¾ç ç©¶äººå¡å¨è©²é åå§å¤åå¾ä¸ç¢ºå®éåï¼é²èå¯¦ç¾ç´æ¥ä½å¼å¾ä¿¡è³´çæ©å¨å­¸ç¿ã

##### **Automatic Medical Report Generation: Methods and Applications**
2408.13988v1 by Li Guo, Anas M. Tahir, Dong Zhang, Z. Jane Wang, Rabab K. Ward

The increasing demand for medical imaging has surpassed the capacity of
available radiologists, leading to diagnostic delays and potential
misdiagnoses. Artificial intelligence (AI) techniques, particularly in
automatic medical report generation (AMRG), offer a promising solution to this
dilemma. This review comprehensively examines AMRG methods from 2021 to 2024.
It (i) presents solutions to primary challenges in this field, (ii) explores
AMRG applications across various imaging modalities, (iii) introduces publicly
available datasets, (iv) outlines evaluation metrics, (v) identifies techniques
that significantly enhance model performance, and (vi) discusses unresolved
issues and potential future research directions. This paper aims to provide a
comprehensive understanding of the existing literature and inspire valuable
future research.

æè¦ï¼ç±æ¼å°é«å­¸å½±åçéæ±æ¥çå¢é·ï¼å·²ç¶è¶éäºç¾ææ¾å°ç§é«å¸«çè½åï¼å°è´è¨ºæ·å»¶èª¤åæ½å¨çèª¤è¨ºãäººå·¥æºæ§ (AI) æè¡ï¼ç¹å¥æ¯å¨èªåé«çå ±åçæ (AMRG) æ¹é¢ï¼çºæ­¤å°å¢æä¾äºæå¸æçè§£æ±ºæ¹æ¡ãæ¬ç¯è©è«å¨é¢æ¢è¨äº 2021 å¹´è³ 2024 å¹´ç AMRG æ¹æ³ãå® (i) æåºè§£æ±ºæ­¤é åä¸­ä¸»è¦ææ°çæ¹æ¡ï¼(ii) æ¢è¨ AMRG å¨åç¨®å½±åæ¨¡å¼ä¸­çæç¨ï¼(iii) ä»ç´¹å¬éå¯ç¨çè³æéï¼(iv) æ¦è¿°è©ä¼°ææ¨ï¼(v) æ¾åºé¡¯èæåæ¨¡åæè½çæè¡ï¼ä»¥å (vi) è¨è«å°æªè§£æ±ºçåé¡åæ½å¨çæªä¾ç ç©¶æ¹åãæ¬ææ¨å¨æä¾å°ç¾ææç»çå¨é¢äºè§£ï¼ä¸¦æ¿ç¼æå¹å¼çæªä¾ç ç©¶ã

##### **Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**
2409.00084v2 by Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush

Background and Aims: This study evaluates the medical reasoning performance
of large language models (LLMs) and vision language models (VLMs) in
gastroenterology.
  Methods: We used 300 gastroenterology board exam-style multiple-choice
questions, 138 of which contain images to systematically assess the impact of
model configurations and parameters and prompt engineering strategies utilizing
GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs
(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),
Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces
(web and API), computing environments (cloud and local), and model precisions
(with and without quantization). Finally, we assessed accuracy using a
semiautomated pipeline.
  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet
(74.0%) achieved the highest accuracy, outperforming the top open-source
models: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).
Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)
performed best. The scores of the quantized models were comparable to those of
the full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM
performance on image-containing questions did not improve when the images were
provided and worsened when LLM-generated captions were provided. In contrast, a
10% increase in accuracy was observed when images were accompanied by
human-crafted image descriptions.
  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in
medical reasoning, the integration of visual data remains a challenge for VLMs.
Effective deployment involves carefully determining optimal model
configurations, encouraging users to consider either the high performance of
proprietary models or the flexible adaptability of open-source models.

æè¦ï¼<paragraph>èæ¯èç®æ¨ï¼æ¬ç ç©¶è©ä¼°å¤§åèªè¨æ¨¡å (LLM) åè¦è¦ºèªè¨æ¨¡å (VLM) å¨è¸èçå­¸ä¸­çé«çæ¨çè¡¨ç¾ã
æ¹æ³ï¼æåä½¿ç¨ 300 åè¸èçå­¸å°ç§èè©¦é¢¨æ ¼çå¤é¸é¡ï¼å¶ä¸­ 138 ååå«å½±åï¼ä»¥ç³»çµ±æ§å°è©ä¼°æ¨¡åéç½®ååæ¸ä»¥åå©ç¨ GPT-3.5 çæç¤ºå·¥ç¨ç­ç¥çå½±é¿ãæ¥ä¸ä¾ï¼æåè©ä¼°å°æåéæº LLMï¼çæ¬ï¼çè¡¨ç¾ï¼åæ¬ GPTï¼3.5ã4ã4oã4ominiï¼ãClaudeï¼3ã3.5ï¼ãGeminiï¼1.0ï¼ãMistralãLlamaï¼2ã3ã3.1ï¼ãMixtral å Phiï¼3ï¼ï¼è·¨ä¸åä»é¢ï¼ç¶²è·¯å APIï¼ãéç®ç°å¢ï¼é²ç«¯åæ¬å°ï¼åæ¨¡åç²¾ç¢ºåº¦ï¼æåæ²æéåï¼ãæå¾ï¼æåä½¿ç¨åèªååç®¡éè©ä¼°æºç¢ºåº¦ã
çµæï¼å¨å°ææ¨¡åä¸­ï¼GPT-4oï¼73.7%ï¼å Claude3.5-Sonnetï¼74.0%ï¼éå°æé«æºç¢ºåº¦ï¼åªæ¼é å°çéæºæ¨¡åï¼Llama3.1-405bï¼64%ï¼ãLlama3.1-70bï¼58.3%ï¼å Mixtral-8x7bï¼54.3%ï¼ãå¨éåçéæºæ¨¡åä¸­ï¼6 ä½åéåç Phi3-14bï¼48.7%ï¼è¡¨ç¾æä½³ãéåæ¨¡åçåæ¸èå¨ç²¾åº¦æ¨¡å Llama2-7bãLlama2--13b å Gemma2-9b ç¸ç¶ãå¼å¾æ³¨æçæ¯ï¼ç¶æä¾å½±åæï¼VLM å¨åå«å½±åçåé¡ä¸çè¡¨ç¾ä¸¦æªæ¹åï¼èå¨æä¾ LLM çæçæ¨é¡æè¡¨ç¾æ¡åãç¸åå°ï¼ç¶å½±åéæäººå·¥è£½ä½çå½±åæè¿°æï¼æºç¢ºåº¦è§å¯å°å¢å äº 10%ã
çµè«ï¼çµè«èè¨ï¼éç¶ LLM å¨é«çæ¨çä¸­è¡¨ç¾åºå¼·å¥çé¶æ¬¡å­¸ç¿è¡¨ç¾ï¼ä½è¦è¦ºè³æçæ´åä»ç¶æ¯ VLM çä¸é ææ°ãææçé¨ç½²æ¶åä»ç´°ç¢ºå®æä½³æ¨¡åéç½®ï¼é¼åµä½¿ç¨èèæ®å°ææ¨¡åçé«æè½æéæºæ¨¡åçéæ´»é©ææ§ã</paragraph>

##### **PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**
2408.13836v1 by Zifan Chen, Xinyu Nan, Jiazheng Li, Jie Zhao, Haifeng Li, Zilin Lin, Haoshen Li, Heyun Chen, Yiting Liu, Bin Dong, Li Zhang, Lei Tang

Volumetric segmentation is crucial for medical imaging but is often
constrained by labor-intensive manual annotations and the need for
scenario-specific model training. Furthermore, existing general segmentation
models are inefficient due to their design and inferential approaches.
Addressing this clinical demand, we introduce PropSAM, a propagation-based
segmentation model that optimizes the use of 3D medical structure information.
PropSAM integrates a CNN-based UNet for intra-slice processing with a
Transformer-based module for inter-slice propagation, focusing on structural
and semantic continuities to enhance segmentation across various modalities.
Distinctively, PropSAM operates on a one-view prompt, such as a 2D bounding box
or sketch mask, unlike conventional models that require two-view prompts. It
has demonstrated superior performance, significantly improving the Dice
Similarity Coefficient (DSC) across 44 medical datasets and various imaging
modalities, outperforming models like MedSAM and SegVol with an average DSC
improvement of 18.1%. PropSAM also maintains stable predictions despite prompt
deviations and varying propagation configurations, confirmed by one-way ANOVA
tests with P>0.5985 and P>0.6131, respectively. Moreover, PropSAM's efficient
architecture enables faster inference speeds (Wilcoxon rank-sum test, P<0.001)
and reduces user interaction time by 37.8% compared to two-view prompt models.
Its ability to handle irregular and complex objects with robust performance
further demonstrates its potential in clinical settings, facilitating more
automated and reliable medical imaging analyses with minimal retraining.

æè¦ï¼é«ç©åå²å°æ¼é«å­¸å½±åè³ééè¦ï¼ä½éå¸¸åå°èè²»å¤§éäººåçæ¨è¨»åç¹å®å ´æ¯æ¨¡åè¨ç·´éæ±çéå¶ãæ­¤å¤ï¼ç¾æçéç¨åå²æ¨¡åç±æ¼å¶è¨­è¨åæ¨è«æ¹æ³èæçä½ä¸ãçºäºæ»¿è¶³éé è¨åºéæ±ï¼æåå¼å¥äº PropSAMï¼éæ¯ä¸ç¨®åºæ¼å³æ­çåå²æ¨¡åï¼åªåäº 3D é«å­¸çµæ§è³è¨çä½¿ç¨ãPropSAM æ´åäºä¸ååºæ¼ CNN ç UNetï¼ç¨æ¼åçå§èçï¼ä»¥åä¸ååºæ¼ Transformer çæ¨¡çµï¼ç¨æ¼åçéå³æ­ï¼éé»éæ³¨çµæ§åèªç¾©é£çºæ§ï¼ä»¥å¢å¼·åç¨®æ¨¡å¼ä¸çåå²ãèéè¦å©è¦æç¤ºçå³çµ±æ¨¡åä¸åï¼PropSAM ç¨ç¹å°éä½æ¼å®è¦æç¤ºä¸ï¼ä¾å¦ 2D éçæ¡æèåé®ç½©ãå®å·²è­æå·æåªç°çæè½ï¼é¡¯èæ¹åäº 44 åé«å­¸è³æéååç¨®å½±åæ¨¡å¼ä¸çéª°å­ç¸ä¼¼ä¿æ¸ (DSC)ï¼åªæ¼ MedSAM å SegVol ç­æ¨¡åï¼å¹³å DSC æåäº 18.1%ãåç®¡æç¤ºåå·®åå³æ­éç½®ä¸åï¼PropSAM ä»è½ç¶­æç©©å®çé æ¸¬ï¼éå·²ééå®å ANOVA æ¸¬è©¦å¾å°è­å¯¦ï¼åå¥çº P>0.5985 å P>0.6131ãæ­¤å¤ï¼PropSAM çé«ææ¶æ§è½å¯¦ç¾æ´å¿«çæ¨è«éåº¦ï¼Wilcoxon ç­ç´åç¸½åæª¢å®ï¼P<0.001ï¼ï¼ä¸¦å°ä½¿ç¨èäºåæéæ¸å°äº 37.8%ï¼åªæ¼å©è¦æç¤ºæ¨¡åãå®å¨èçä¸è¦ååè¤éç©ä»¶æè½å±ç¾åºç©©å¥çæè½ï¼é²ä¸æ­¥è­æäºå¶å¨è¨åºç°å¢ä¸­çæ½åï¼æå©æ¼ä»¥æå°çéæ°è¨ç·´é²è¡æ´èªåååå¯é çé«å­¸å½±ååæã

