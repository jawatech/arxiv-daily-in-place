{"2405.18832": {"publish_time": "2024-05-29", "title": "MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models", "paper_summary": "Mixture-of-Experts (MoE) large language models (LLM) have memory requirements\nthat often exceed the GPU memory capacity, requiring costly parameter movement\nfrom secondary memories to the GPU for expert computation. In this work, we\npresent Mixture of Near-Data Experts (MoNDE), a near-data computing solution\nthat efficiently enables MoE LLM inference. MoNDE reduces the volume of MoE\nparameter movement by transferring only the $\\textit{hot}$ experts to the GPU,\nwhile computing the remaining $\\textit{cold}$ experts inside the host memory\ndevice. By replacing the transfers of massive expert parameters with the ones\nof small activations, MoNDE enables far more communication-efficient MoE\ninference, thereby resulting in substantial speedups over the existing\nparameter offloading frameworks for both encoder and decoder operations.", "paper_summary_zh": "\u6df7\u5408\u5c08\u5bb6 (MoE) \u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5177\u6709\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u901a\u5e38\u6703\u8d85\u904e GPU \u8a18\u61b6\u9ad4\u5bb9\u91cf\uff0c\u9700\u8981\u5f9e\u6b21\u8981\u8a18\u61b6\u9ad4\u5c07\u6602\u8cb4\u7684\u53c3\u6578\u79fb\u52d5\u5230 GPU \u9032\u884c\u5c08\u5bb6\u904b\u7b97\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u8fd1\u8cc7\u6599\u5c08\u5bb6\u6df7\u5408 (MoNDE)\uff0c\u4e00\u7a2e\u8fd1\u8cc7\u6599\u904b\u7b97\u89e3\u6c7a\u65b9\u6848\uff0c\u53ef\u6709\u6548\u555f\u7528 MoE LLM \u63a8\u8ad6\u3002MoNDE \u900f\u904e\u50c5\u5c07\u300c\u71b1\u9580\u300d\u5c08\u5bb6\u50b3\u8f38\u5230 GPU \u4f86\u6e1b\u5c11 MoE \u53c3\u6578\u79fb\u52d5\u7684\u91cf\uff0c\u540c\u6642\u5728\u4e3b\u6a5f\u8a18\u61b6\u9ad4\u88dd\u7f6e\u5167\u904b\u7b97\u5176\u9918\u7684\u300c\u51b7\u9580\u300d\u5c08\u5bb6\u3002\u900f\u904e\u5c07\u5927\u91cf\u5c08\u5bb6\u53c3\u6578\u7684\u50b3\u8f38\u66ff\u63db\u70ba\u5c11\u91cf\u6d3b\u5316\u7684\u50b3\u8f38\uff0cMoNDE \u80fd\u5920\u5be6\u73fe\u66f4\u5177\u901a\u8a0a\u6548\u7387\u7684 MoE \u63a8\u8ad6\uff0c\u5f9e\u800c\u5927\u5e45\u63d0\u5347\u7de8\u78bc\u5668\u548c\u89e3\u78bc\u5668\u64cd\u4f5c\u73fe\u6709\u53c3\u6578\u5378\u8f09\u67b6\u69cb\u7684\u901f\u5ea6\u3002", "author": "Taehyun Kim et.al.", "authors": "Taehyun Kim, Kwanseok Choi, Youngmock Cho, Jaehoon Cho, Hyuk-Jae Lee, Jaewoong Sim", "id": "2405.18832v1", "paper_url": "http://arxiv.org/abs/2405.18832v1", "repo": "null"}}