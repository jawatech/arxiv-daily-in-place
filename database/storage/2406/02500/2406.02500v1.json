{"2406.02500": {"publish_time": "2024-06-04", "title": "Demystifying the Compression of Mixture-of-Experts Through a Unified Framework", "paper_summary": "Scaling large language models has revolutionized the performance across\ndiverse domains, yet the continual growth in model size poses significant\nchallenges for real-world deployment. The Mixture of Experts (MoE) approach\naddresses this by dynamically selecting and activating only a subset of\nexperts, significantly reducing computational costs while maintaining high\nperformance. However, MoE introduces potential redundancy (e.g., parameters)\nand extra costs (e.g., communication overhead). Despite numerous compression\ntechniques developed for mitigating the redundancy in dense models, the\ncompression of MoE remains under-explored. We first bridge this gap with a\ncutting-edge unified framework that not only seamlessly integrates mainstream\ncompression methods but also helps systematically understand MoE compression.\nThis framework approaches compression from two perspectives: Expert Slimming\nwhich compresses individual experts and Expert Trimming which removes\nstructured modules. Within this framework, we explore the optimization space\nunexplored by existing methods,and further introduce aggressive Expert Trimming\ntechniques, i.e., Layer Drop and Block Drop, to eliminate redundancy at larger\nscales. Based on these insights,we present a comprehensive recipe to guide\npractitioners in compressing MoE effectively. Extensive experimental results\ndemonstrate the effectiveness of the compression methods under our framework\nand the proposed recipe, achieving a 6.05x speedup and only 20.0GB memory usage\nwhile maintaining over 92% of performance on Mixtral-8x7B.", "paper_summary_zh": "<paragraph>\u5927\u898f\u6a21\u8a9e\u8a00\u6a21\u578b\u7684\u64f4\u5c55\u5fb9\u5e95\u6539\u8b8a\u4e86\u5404\u500b\u9818\u57df\u7684\u6548\u80fd\uff0c\u7136\u800c\u6a21\u578b\u898f\u6a21\u7684\u6301\u7e8c\u64f4\u5927\u5c0d\u5be6\u969b\u90e8\u7f72\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6230\u3002\u5c08\u5bb6\u6df7\u5408 (MoE) \u65b9\u6cd5\u900f\u904e\u52d5\u614b\u9078\u53d6\u4e26\u50c5\u555f\u7528\u5c08\u5bb6\u5b50\u96c6\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u5927\u5e45\u964d\u4f4e\u904b\u7b97\u6210\u672c\uff0c\u540c\u6642\u7dad\u6301\u9ad8\u6548\u80fd\u3002\u7136\u800c\uff0cMoE \u5f15\u5165\u4e86\u6f5b\u5728\u7684\u5197\u9918\uff08\u4f8b\u5982\u53c3\u6578\uff09\u548c\u984d\u5916\u6210\u672c\uff08\u4f8b\u5982\u901a\u8a0a\u8ca0\u64d4\uff09\u3002\u5118\u7ba1\u5df2\u958b\u767c\u51fa\u8a31\u591a\u58d3\u7e2e\u6280\u8853\u4f86\u6e1b\u8f15\u5bc6\u96c6\u6a21\u578b\u4e2d\u7684\u5197\u9918\uff0c\u4f46 MoE \u7684\u58d3\u7e2e\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u6211\u5011\u9996\u5148\u900f\u904e\u4e00\u500b\u5c16\u7aef\u7684\u7d71\u4e00\u67b6\u69cb\u4f86\u5f4c\u88dc\u6b64\u5dee\u8ddd\uff0c\u4e0d\u50c5\u80fd\u7121\u7e2b\u6574\u5408\u4e3b\u6d41\u58d3\u7e2e\u65b9\u6cd5\uff0c\u9084\u80fd\u5e6b\u52a9\u7cfb\u7d71\u6027\u5730\u4e86\u89e3 MoE \u58d3\u7e2e\u3002\u6b64\u67b6\u69cb\u5f9e\u5169\u500b\u89d2\u5ea6\u4f86\u63a2\u8a0e\u58d3\u7e2e\uff1a\u5c08\u5bb6\u7cbe\u7c21\uff08\u58d3\u7e2e\u500b\u5225\u5c08\u5bb6\uff09\u548c\u5c08\u5bb6\u4fee\u526a\uff08\u79fb\u9664\u7d50\u69cb\u5316\u6a21\u7d44\uff09\u3002\u5728\u6b64\u67b6\u69cb\u5167\uff0c\u6211\u5011\u63a2\u7d22\u4e86\u73fe\u6709\u65b9\u6cd5\u672a\u63a2\u7d22\u7684\u6700\u4f73\u5316\u7a7a\u9593\uff0c\u4e26\u9032\u4e00\u6b65\u5f15\u5165\u4e86\u7a4d\u6975\u7684\u5c08\u5bb6\u4fee\u526a\u6280\u8853\uff0c\u5373\u5c64\u6b21\u4e2d\u65b7\u548c\u5340\u584a\u4e2d\u65b7\uff0c\u4ee5\u6d88\u9664\u66f4\u5927\u898f\u6a21\u7684\u5197\u9918\u3002\u6839\u64da\u9019\u4e9b\u898b\u89e3\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5168\u9762\u7684\u914d\u65b9\uff0c\u4ee5\u6307\u5c0e\u5f9e\u696d\u4eba\u54e1\u6709\u6548\u58d3\u7e2e MoE\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u67b6\u69cb\u548c\u5efa\u8b70\u914d\u65b9\u4e2d\u58d3\u7e2e\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728 Mixtral-8x7B \u4e0a\u5be6\u73fe\u4e86 6.05 \u500d\u52a0\u901f\u548c\u50c5 20.0GB \u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff0c\u540c\u6642\u7dad\u6301\u8d85\u904e 92% \u7684\u6548\u80fd\u3002</paragraph>", "author": "Shwai He et.al.", "authors": "Shwai He, Daize Dong, Liang Ding, Ang Li", "id": "2406.02500v1", "paper_url": "http://arxiv.org/abs/2406.02500v1", "repo": "null"}}