{"2412.14905": {"publish_time": "2024-12-19", "title": "Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation", "paper_summary": "Large language models (LLMs) are susceptible to generating hallucinated\ninformation, despite the integration of retrieval-augmented generation (RAG).\nParallel context extension (PCE) is a line of research attempting to\neffectively integrating parallel (unordered) contexts, while it still suffers\nfrom hallucinations when adapted to RAG scenarios. In this paper, we propose\nDePaC (Dehallucinating Parallel Context Extension), which alleviates the\nhallucination problem with context-aware negative training and\ninformation-calibrated aggregation. DePaC is designed to alleviate two types of\nin-context hallucination: fact fabrication (i.e., LLMs present claims that are\nnot supported by the contexts) and fact omission (i.e., LLMs fail to present\nclaims that can be supported by the contexts). Specifically, (1) for fact\nfabrication, we apply the context-aware negative training that fine-tunes the\nLLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to\nanswer when contexts are not related to questions; (2) for fact omission, we\npropose the information-calibrated aggregation which prioritizes context\nwindows with higher information increment from their contexts. The experimental\nresults on nine RAG tasks demonstrate that DePaC significantly alleviates the\ntwo types of hallucination and consistently achieves better performances on\nthese tasks.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u4fe1\u606f\uff0c\u5c3d\u7ba1\u96c6\u6210\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG)\u3002\n\u5e73\u884c\u4e0a\u4e0b\u6587\u6269\u5c55 (PCE) \u662f\u4e00\u6761\u5c1d\u8bd5\u6709\u6548\u96c6\u6210\u5e73\u884c\uff08\u65e0\u5e8f\uff09\u4e0a\u4e0b\u6587\u7684\u7684\u7814\u7a76\u7ebf\uff0c\u4f46\u5f53\u5b83\u9002\u5e94 RAG \u573a\u666f\u65f6\uff0c\u5b83\u4ecd\u7136\u4f1a\u4ea7\u751f\u5e7b\u89c9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 DePaC\uff08\u53bb\u5e7b\u89c9\u5e73\u884c\u4e0a\u4e0b\u6587\u6269\u5c55\uff09\uff0c\u5b83\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8d1f\u8bad\u7ec3\u548c\u4fe1\u606f\u6821\u51c6\u805a\u5408\u6765\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002DePaC \u65e8\u5728\u7f13\u89e3\u4e24\u79cd\u7c7b\u578b\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9\uff1a\u4e8b\u5b9e\u634f\u9020\uff08\u5373\uff0cLLM \u63d0\u51fa\u4e0d\u53d7\u4e0a\u4e0b\u6587\u652f\u6301\u7684\u4e3b\u5f20\uff09\u548c\u4e8b\u5b9e\u9057\u6f0f\uff08\u5373\uff0cLLM \u65e0\u6cd5\u63d0\u51fa\u53ef\u4ee5\u7531\u4e0a\u4e0b\u6587\u652f\u6301\u7684\u4e3b\u5f20\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\uff081\uff09\u5bf9\u4e8e\u4e8b\u5b9e\u634f\u9020\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8d1f\u8bad\u7ec3\uff0c\u8be5\u8bad\u7ec3\u4f7f\u7528\u8d1f\u76d1\u7763\u5bf9 LLM \u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u660e\u786e\u6307\u5bfc LLM \u5728\u4e0a\u4e0b\u6587\u4e0e\u95ee\u9898\u65e0\u5173\u65f6\u62d2\u7edd\u56de\u7b54\uff1b\uff082\uff09\u5bf9\u4e8e\u4e8b\u5b9e\u9057\u6f0f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4fe1\u606f\u6821\u51c6\u805a\u5408\uff0c\u8be5\u805a\u5408\u4f18\u5148\u8003\u8651\u4ece\u5176\u4e0a\u4e0b\u6587\u4e2d\u5177\u6709\u8f83\u9ad8\u4fe1\u606f\u589e\u91cf\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002\u5728\u4e5d\u4e2a RAG \u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDePaC \u663e\u7740\u7f13\u89e3\u4e86\u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u5e7b\u89c9\uff0c\u5e76\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u6301\u7eed\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "author": "Zexiong Ma et.al.", "authors": "Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie", "id": "2412.14905v1", "paper_url": "http://arxiv.org/abs/2412.14905v1", "repo": "null"}}