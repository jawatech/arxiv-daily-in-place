{"2407.04615": {"publish_time": "2024-07-05", "title": "ARM: Efficient Guided Decoding with Autoregressive Reward Models", "paper_summary": "Language models trained on large amounts of data require careful tuning to be\nsafely deployed in real world. We revisit the guided decoding paradigm, where\nthe goal is to augment the logits of the base language model using the scores\nfrom a task-specific reward model. We propose a simple but efficient\nparameterization of the autoregressive reward model enabling fast and effective\nguided decoding. On detoxification and sentiment control tasks, we show that\nour efficient parameterization performs on par with RAD, a strong but less\nefficient guided decoding approach.", "paper_summary_zh": "\u5728\u5927\u91cf\u8cc7\u6599\u4e0a\u8a13\u7df4\u7684\u8a9e\u8a00\u6a21\u578b\u9700\u8981\u5c0f\u5fc3\u8abf\u6574\uff0c\u624d\u80fd\u5b89\u5168\u5730\u90e8\u7f72\u5728\u73fe\u5be6\u4e16\u754c\u3002\u6211\u5011\u91cd\u65b0\u63a2\u8a0e\u5f15\u5c0e\u5f0f\u89e3\u78bc\u7bc4\u4f8b\uff0c\u5176\u4e2d\u76ee\u6a19\u662f\u4f7f\u7528\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u734e\u52f5\u6a21\u578b\u7684\u5206\u6578\u4f86\u64f4\u5145\u57fa\u790e\u8a9e\u8a00\u6a21\u578b\u7684 logit\u3002\u6211\u5011\u63d0\u51fa\u81ea\u8ff4\u6b78\u734e\u52f5\u6a21\u578b\u7684\u7c21\u55ae\u4f46\u6709\u6548\u7684\u53c3\u6578\u5316\uff0c\u4f7f\u5f15\u5c0e\u5f0f\u89e3\u78bc\u5feb\u901f\u4e14\u6709\u6548\u3002\u5728\u89e3\u6bd2\u548c\u60c5\u7dd2\u63a7\u5236\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u6211\u5011\u7684\u6709\u6548\u53c3\u6578\u5316\u8868\u73fe\u8207 RAD \u76f8\u7576\uff0cRAD \u662f\u4e00\u7a2e\u5f37\u5927\u4f46\u6548\u7387\u8f03\u4f4e\u7684\u5f15\u5c0e\u5f0f\u89e3\u78bc\u65b9\u6cd5\u3002", "author": "Sergey Troshin et.al.", "authors": "Sergey Troshin, Vlad Niculae, Antske Fokkens", "id": "2407.04615v1", "paper_url": "http://arxiv.org/abs/2407.04615v1", "repo": "null"}}