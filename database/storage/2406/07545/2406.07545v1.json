{"2406.07545": {"publish_time": "2024-06-11", "title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena", "paper_summary": "Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.", "paper_summary_zh": "\u591a\u9078\u984c (MCQ) \u5e38\u7528\u65bc\u8a55\u91cf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u901a\u5e38\uff0cLLM \u6703\u6536\u5230\u4e00\u500b\u554f\u984c\uff0c\u4e26\u5728\u91dd\u5c0d\u9577\u5ea6\u7b49\u56e0\u7d20\u9032\u884c\u8abf\u6574\u5f8c\uff0c\u9078\u64c7\u6700\u6709\u53ef\u80fd\u7684\u7b54\u6848\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u7531\u65bc\u5148\u9a57\u4e0d\u5e73\u8861\u6a5f\u7387\u7684\u5167\u5728\u504f\u8aa4\uff0cLLM \u53ef\u80fd\u5929\u751f\u504f\u597d\u67d0\u4e9b\u7b54\u6848\u9078\u9805 ID\uff0c\u4f8b\u5982 A/B/C/D\uff0c\u9019\u6703\u5f71\u97ff\u6839\u64da\u9019\u4e9b ID \u9810\u6e2c\u7b54\u6848\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u5f15\u5165\u65b9\u6cd5\uff0c\u900f\u904e\u50c5\u5728\u5e7e\u500b\u6e2c\u8a66\u6a23\u672c\u4e0a\u6392\u5217\u9078\u9805\u4e26\u5957\u7528\u5230\u65b0\u7684\u6a23\u672c\uff0c\u4f86\u6e1b\u5c11\u9019\u7a2e\u300c\u9078\u64c7\u504f\u8aa4\u300d\u3002MCQ \u7684\u53e6\u4e00\u500b\u554f\u984c\u662f\u300c\u96a8\u6a5f\u731c\u6e2c\u300d\u7684\u6a02\u900f\u9078\u9805\u3002LLM \u6c92\u6709\u5b78\u7fd2\u7279\u5b9a\u77e5\u8b58\uff0c\u4f46\u9078\u9805\u731c\u5c0d\u4e86\u3002\u5c0d\u65bc\u90a3\u4e9b\u5c0f\u898f\u6a21\u7684 LLM \u4f86\u8aaa\uff0c\u9019\u7a2e\u60c5\u6cc1\u5c24\u5176\u56b4\u91cd\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u66f4\u5fb9\u5e95\u7684\u65b9\u6cd5\u6d89\u53ca\u5f9e MCQ \u8f49\u63db\u70ba\u958b\u653e\u5f0f\u554f\u984c\uff0c\u9019\u53ef\u4ee5\u5f9e\u6839\u672c\u4e0a\u6d88\u9664\u9078\u64c7\u504f\u8aa4\u548c\u96a8\u6a5f\u731c\u6e2c\u7684\u554f\u984c\u3002\u7136\u800c\uff0c\u8f49\u63db\u6703\u5728 (1) \u627e\u51fa\u5408\u9069\u7684\u958b\u653e\u5f0f\u554f\u984c\u548c (2) \u9a57\u8b49 LLM \u958b\u653e\u5f0f\u56de\u61c9\u7684\u6b63\u78ba\u6027\u662f\u5426\u7b26\u5408\u4eba\u5de5\u6a19\u8a18\u7684\u771f\u5be6\u60c5\u6cc1\u65b9\u9762\uff0c\u9020\u6210\u5176\u81ea\u8eab\u7684\u6311\u6230\u3002\u9019\u9805\u5de5\u4f5c\u65e8\u5728\u89e3\u6c7a\u9019\u4e9b\u91cd\u5927\u7684\u56f0\u96e3\uff0c\u4e26\u900f\u904e\u5b8c\u5168\u958b\u653e\u5f0f\u554f\u984c\u5efa\u7acb\u65b0\u7684 LLM \u8a55\u91cf\u57fa\u6e96\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 Open-LLM-Leaderboard \u4f86\u8ffd\u8e64\u5404\u7a2e LLM \u7684\u6548\u80fd\uff0c\u4e26\u53cd\u6620\u5b83\u5011\u7684\u771f\u5be6\u80fd\u529b\uff0c\u4f8b\u5982 GPT-4o/4/3.5\u3001Claude 3\u3001Gemini \u7b49\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u96c6\u53ef\u5728 https://github.com/VILA-Lab/Open-LLM-Leaderboard \u53d6\u5f97\u3002", "author": "Aidar Myrzakhan et.al.", "authors": "Aidar Myrzakhan, Sondos Mahmoud Bsharat, Zhiqiang Shen", "id": "2406.07545v1", "paper_url": "http://arxiv.org/abs/2406.07545v1", "repo": "https://github.com/vila-lab/open-llm-leaderboard"}}