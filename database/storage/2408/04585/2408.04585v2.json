{"2408.04585": {"publish_time": "2024-08-08", "title": "Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency, Performance, and Adversarial Robustness", "paper_summary": "With the increasing demand for practical applications of Large Language\nModels (LLMs), many attention-efficient models have been developed to balance\nperformance and computational cost. However, the adversarial robustness of\nthese models remains under-explored. In this work, we design a framework to\ninvestigate the trade-off between efficiency, performance, and adversarial\nrobustness of LLMs by comparing three prominent models with varying levels of\ncomplexity and efficiency -- Transformer++, Gated Linear Attention (GLA)\nTransformer, and MatMul-Free LM -- utilizing the GLUE and AdvGLUE datasets. The\nAdvGLUE dataset extends the GLUE dataset with adversarial samples designed to\nchallenge model robustness. Our results show that while the GLA Transformer and\nMatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate\nhigher efficiency and either superior or comparative robustness on AdvGLUE\ntasks compared to Transformer++ across different attack levels. These findings\nhighlight the potential of simplified architectures to achieve a compelling\nbalance between efficiency, performance, and adversarial robustness, offering\nvaluable insights for applications where resource constraints and resilience to\nadversarial attacks are critical.", "paper_summary_zh": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u52a0\uff0c\u8bb8\u591a\u6ce8\u91cd\u6548\u7387\u7684\u6a21\u578b\u5df2\u88ab\u5f00\u53d1\u51fa\u6765\uff0c\u4ee5\u5e73\u8861\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e09\u4e2a\u5177\u6709\u4e0d\u540c\u590d\u6742\u6027\u548c\u6548\u7387\u6c34\u5e73\u7684\u7a81\u51fa\u6a21\u578b\u2014\u2014Transformer++\u3001\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\uff08GLA\uff09Transformer \u548c\u65e0 MatMul LM\u2014\u2014\u5229\u7528 GLUE \u548c AdvGLUE \u6570\u636e\u96c6\uff0c\u6765\u7814\u7a76 LLM \u7684\u6548\u7387\u3001\u6027\u80fd\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002AdvGLUE \u6570\u636e\u96c6\u6269\u5c55\u4e86 GLUE \u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u65e8\u5728\u6311\u6218\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5bf9\u6297\u6837\u672c\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136 GLA Transformer \u548c\u65e0 MatMul LM \u5728 GLUE \u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u5ea6\u7565\u4f4e\uff0c\u4f46\u5b83\u4eec\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u653b\u51fb\u7ea7\u522b\u4e0a\u4e0e Transformer++ \u76f8\u6bd4\uff0c\u5728 AdvGLUE \u4efb\u52a1\u4e0a\u5177\u6709\u66f4\u9ad8\u6216\u76f8\u5f53\u7684\u9c81\u68d2\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u51fa\u4e86\u7b80\u5316\u67b6\u6784\u5728\u5b9e\u73b0\u6548\u7387\u3001\u6027\u80fd\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u4e4b\u95f4\u5f15\u4eba\u6ce8\u76ee\u7684\u5e73\u8861\u7684\u6f5c\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u548c\u5bf9\u6297\u653b\u51fb\u5f39\u6027\u81f3\u5173\u91cd\u8981\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "author": "Xiaojing Fan et.al.", "authors": "Xiaojing Fan, Chunliang Tao", "id": "2408.04585v2", "paper_url": "http://arxiv.org/abs/2408.04585v2", "repo": "null"}}