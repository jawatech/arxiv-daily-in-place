{"2409.04168": {"publish_time": "2024-09-06", "title": "From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks", "paper_summary": "To reduce the need for human annotations, large language models (LLMs) have\nbeen proposed as judges of the quality of other candidate models. LLM judges\nare typically evaluated by measuring the correlation with human judgments on\ngeneration tasks such as summarization or machine translation. In contrast, we\nstudy LLM judges on mathematical reasoning tasks. These tasks require\nmulti-step reasoning, and the correctness of their solutions is verifiable,\nenabling a more objective evaluation. We perform a detailed performance\nanalysis and find that the used judges are mostly unable to improve task\nperformance but are able to pick the better model. Our analysis uncovers a\nstrong correlation between judgment performance and the candidate model task\nperformance. We observe that judges tend to choose the model of higher quality\neven if its answer is incorrect. Further, we show that it is possible to use\nstatistics, such as the task performances of the individual models, to predict\njudgment performance. In an ablation, we either swap or mask the candidate\nanswers and observe that judges often keep the original judgment, providing\nevidence that judges incorporate writing style in their judgments. In summary,\nwe find that regularities in the judgments are quantifiable using statistical\nmeasures and provide various angles on exploiting them.", "paper_summary_zh": "<paragraph>\u70ba\u4e86\u6e1b\u5c11\u4eba\u5de5\u6a19\u8a3b\u7684\u9700\u6c42\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u88ab\u63d0\u8b70\u7528\u4f5c\u5176\u4ed6\u5019\u9078\u6a21\u578b\u54c1\u8cea\u7684\u8a55\u5be9\u3002LLM \u8a55\u5be9\u901a\u5e38\u900f\u904e\u8861\u91cf\u5728\u751f\u6210\u4efb\u52d9\uff08\u4f8b\u5982\u6458\u8981\u6216\u6a5f\u5668\u7ffb\u8b6f\uff09\u4e0a\u8207\u4eba\u985e\u5224\u65b7\u76f8\u95dc\u6027\u4f86\u8a55\u4f30\u3002\u76f8\u53cd\u5730\uff0c\u6211\u5011\u5728\u6578\u5b78\u63a8\u7406\u4efb\u52d9\u4e0a\u7814\u7a76 LLM \u8a55\u5be9\u3002\u9019\u4e9b\u4efb\u52d9\u9700\u8981\u591a\u6b65\u9a5f\u63a8\u7406\uff0c\u4e14\u5176\u89e3\u7684\u6b63\u78ba\u6027\u53ef\u9a57\u8b49\uff0c\u80fd\u9032\u884c\u66f4\u5ba2\u89c0\u7684\u8a55\u4f30\u3002\u6211\u5011\u57f7\u884c\u8a73\u7d30\u7684\u6548\u80fd\u5206\u6790\uff0c\u767c\u73fe\u6240\u4f7f\u7528\u7684\u8a55\u5be9\u5927\u591a\u7121\u6cd5\u6539\u5584\u4efb\u52d9\u6548\u80fd\uff0c\u4f46\u80fd\u5920\u6311\u9078\u51fa\u66f4\u597d\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u5206\u6790\u63ed\u9732\u5224\u65b7\u6548\u80fd\u8207\u5019\u9078\u6a21\u578b\u4efb\u52d9\u6548\u80fd\u4e4b\u9593\u6709\u5f88\u5f37\u7684\u95dc\u806f\u6027\u3002\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u8a55\u5be9\u50be\u5411\u9078\u64c7\u54c1\u8cea\u8f03\u9ad8\u7684\u6a21\u578b\uff0c\u5373\u4f7f\u5176\u7b54\u6848\u4e0d\u6b63\u78ba\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c55\u793a\u4e86\u53ef\u4ee5\u4f7f\u7528\u7d71\u8a08\u8cc7\u6599\uff08\u4f8b\u5982\u500b\u5225\u6a21\u578b\u7684\u4efb\u52d9\u6548\u80fd\uff09\u4f86\u9810\u6e2c\u5224\u65b7\u6548\u80fd\u3002\u5728\u6d88\u878d\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u8abf\u63db\u6216\u906e\u853d\u5019\u9078\u7b54\u6848\uff0c\u4e26\u89c0\u5bdf\u5230\u8a55\u5be9\u7d93\u5e38\u4fdd\u7559\u539f\u59cb\u5224\u65b7\uff0c\u9019\u63d0\u4f9b\u4e86\u8a55\u5be9\u5728\u5224\u65b7\u4e2d\u7d0d\u5165\u5beb\u4f5c\u98a8\u683c\u7684\u8b49\u64da\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u767c\u73fe\u5224\u65b7\u4e2d\u7684\u898f\u5f8b\u6027\u53ef\u4ee5\u4f7f\u7528\u7d71\u8a08\u91cf\u5316\uff0c\u4e26\u63d0\u4f9b\u5404\u7a2e\u89d2\u5ea6\u4f86\u5229\u7528\u5b83\u5011\u3002</paragraph>", "author": "Andreas Stephan et.al.", "authors": "Andreas Stephan, Dawei Zhu, Matthias A\u00dfenmacher, Xiaoyu Shen, Benjamin Roth", "id": "2409.04168v1", "paper_url": "http://arxiv.org/abs/2409.04168v1", "repo": "null"}}