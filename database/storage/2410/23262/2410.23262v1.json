{"2410.23262": {"publish_time": "2024-10-30", "title": "EMMA: End-to-End Multimodal Model for Autonomous Driving", "paper_summary": "We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4ecb\u7d39 EMMA\uff0c\u4e00\u7a2e\u7528\u65bc\u81ea\u52d5\u99d5\u99db\u7684\u7aef\u5230\u7aef\u591a\u6a21\u614b\u6a21\u578b\u3002\n\u5efa\u7acb\u5728\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u57fa\u790e\u4e0a\uff0cEMMA \u76f4\u63a5\u5c07\u539f\u59cb\n\u76f8\u6a5f\u611f\u6e2c\u5668\u8cc7\u6599\u5c0d\u61c9\u5230\u5404\u7a2e\u7279\u5b9a\u65bc\u99d5\u99db\u7684\u8f38\u51fa\uff0c\u5305\u62ec\u898f\u5283\u5668\n\u8ecc\u8de1\u3001\u611f\u77e5\u7269\u4ef6\u548c\u9053\u8def\u5716\u5f62\u5143\u7d20\u3002EMMA \u900f\u904e\n\u5c07\u6240\u6709\u975e\u611f\u6e2c\u5668\u8f38\u5165\uff08\u4f8b\u5982\u5c0e\u822a\u6307\u793a\u548c\u81ea\u6211\n\u8eca\u8f1b\u72c0\u614b\uff09\u548c\u8f38\u51fa\uff08\u4f8b\u5982\u8ecc\u8de1\u548c 3D \u4f4d\u7f6e\uff09\u8868\u793a\u70ba\u81ea\u7136\n\u8a9e\u8a00\u6587\u5b57\uff0c\u6700\u5927\u5316\u9810\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u4e16\u754c\u77e5\u8b58\u6548\u7528\u3002\u9019\u7a2e\u65b9\u6cd5\u8b93 EMMA \u80fd\u5920\u5728\u7d71\u4e00\u7684\u8a9e\u8a00\u7a7a\u9593\u4e2d\u5171\u540c\u8655\u7406\u5404\u7a2e\u99d5\u99db\n\u4efb\u52d9\uff0c\u4e26\u4f7f\u7528\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u63d0\u793a\u7522\u751f\u6bcf\u500b\u4efb\u52d9\u7684\u8f38\u51fa\u3002\u6839\u64da\u7d93\u9a57\uff0c\u6211\u5011\u900f\u904e\n\u5728 nuScenes \u4e0a\u5be6\u73fe\u904b\u52d5\u898f\u5283\u7684\u6700\u65b0\u6548\u80fd\uff0c\u4ee5\u53ca\u5728 Waymo Open Motion Dataset (WOMD) \u4e0a\u7372\u5f97\u7af6\u722d\u529b\u7d50\u679c\uff0c\u4f86\u8b49\u660e EMMA \u7684\u6709\u6548\u6027\u3002EMMA \u4e5f\n\u5728 Waymo Open Dataset (WOD) \u4e0a\u7522\u751f\u4e86\u76f8\u6a5f\u512a\u5148 3D \u7269\u4ef6\u5075\u6e2c\u7684\u7af6\u722d\u529b\u7d50\u679c\u3002\u6211\u5011\u5c55\u793a\u4e86\u4f7f\u7528\u898f\u5283\u5668\u8ecc\u8de1\u3001\n\u7269\u4ef6\u5075\u6e2c\u548c\u9053\u8def\u5716\u5f62\u4efb\u52d9\u5171\u540c\u8a13\u7df4 EMMA\uff0c\u6703\u5728\u6240\u6709\u4e09\u500b\n\u9818\u57df\u4e2d\u7522\u751f\u6539\u9032\uff0c\u7a81\u986f\u4e86 EMMA \u4f5c\u70ba\u81ea\u52d5\u99d5\u99db\u61c9\u7528\u7a0b\u5f0f\u901a\u7528\u6a21\u578b\u7684\u6f5b\u529b\u3002\u7136\u800c\uff0cEMMA \u4e5f\u5c55\u73fe\u51fa\u67d0\u4e9b\u9650\u5236\uff1a\u5b83\u53ea\u80fd\n\u8655\u7406\u5c11\u91cf\u5f71\u50cf\u5e40\uff0c\u4e0d\u6574\u5408\u6e96\u78ba\u7684 3D \u611f\u6e2c\u6a21\u5f0f\uff08\u4f8b\u5982 LiDAR \u6216\u96f7\u9054\uff09\uff0c\u800c\u4e14\u8a08\u7b97\u6210\u672c\u9ad8\u6602\u3002\u6211\u5011\n\u5e0c\u671b\u6211\u5011\u7684\u7d50\u679c\u80fd\u6fc0\u52f5\u9032\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u4ee5\u6e1b\u8f15\u9019\u4e9b\u554f\u984c\uff0c\u4e26\u9032\u4e00\u6b65\u767c\u5c55\u81ea\u52d5\u99d5\u99db\u6a21\u578b\n\u67b6\u69cb\u7684\u6700\u65b0\u6280\u8853\u3002</paragraph>", "author": "Jyh-Jing Hwang et.al.", "authors": "Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, James Guo, Dragomir Anguelov, Mingxing Tan", "id": "2410.23262v1", "paper_url": "http://arxiv.org/abs/2410.23262v1", "repo": "null"}}