{"2409.08250": {"publish_time": "2024-09-12", "title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering", "paper_summary": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nmostly only support retrieving individual pieces of information like certain\nobjects in photos and struggle with answering more complex queries that involve\ninterpreting interconnected memories like event sequences. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories, retrieves relevant memories, and uses a large language model (LLM) to\ncomprehensive answers. In human evaluations, we show the effectiveness of\nOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG\nsystem, winning or tying in 74.5% of the time.", "paper_summary_zh": "\u4eba\u5011\u7d93\u5e38\u900f\u904e\u7167\u7247\u3001\u87a2\u5e55\u622a\u5716\u548c\u5f71\u7247\u4f86\u6355\u6349\u56de\u61b6\u3002\u73fe\u6709\u7684\u57fa\u65bc\u4eba\u5de5\u667a\u6167\u7684\u5de5\u5177\uff0c\u96d6\u7136\u80fd\u4f7f\u7528\u81ea\u7136\u8a9e\u8a00\u4f86\u67e5\u8a62\u9019\u4e9b\u8cc7\u6599\uff0c\u4f46\u5b83\u5011\u5927\u591a\u53ea\u652f\u63f4\u64f7\u53d6\u500b\u5225\u8cc7\u8a0a\uff0c\u4f8b\u5982\u7167\u7247\u4e2d\u7684\u7279\u5b9a\u7269\u4ef6\uff0c\u800c\u4e14\u5f88\u96e3\u56de\u7b54\u6d89\u53ca\u89e3\u8b80\u76f8\u4e92\u9023\u7d50\u56de\u61b6\uff08\u4f8b\u5982\u4e8b\u4ef6\u9806\u5e8f\uff09\u7684\u66f4\u8907\u96dc\u67e5\u8a62\u3002\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u70ba\u671f\u4e00\u500b\u6708\u7684\u65e5\u8a18\u7814\u7a76\uff0c\u4ee5\u6536\u96c6\u5be6\u969b\u7684\u4f7f\u7528\u8005\u67e5\u8a62\uff0c\u4e26\u7522\u751f\u4e86\u4e00\u500b\u5fc5\u8981\u7684\u8108\u7d61\u8cc7\u8a0a\u5206\u985e\u6cd5\uff0c\u7528\u65bc\u8207\u64f7\u53d6\u7684\u56de\u61b6\u6574\u5408\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4ecb\u7d39 OmniQuery\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u7cfb\u7d71\uff0c\u80fd\u5920\u56de\u7b54\u8907\u96dc\u7684\u500b\u4eba\u8a18\u61b6\u76f8\u95dc\u554f\u984c\uff0c\u9700\u8981\u64f7\u53d6\u548c\u63a8\u65b7\u8108\u7d61\u8cc7\u8a0a\u3002OmniQuery \u900f\u904e\u6574\u5408\u4f86\u81ea\u591a\u500b\u76f8\u4e92\u9023\u7d50\u56de\u61b6\u7684\u96f6\u6563\u8108\u7d61\u8cc7\u8a0a\uff0c\u4f86\u64f4\u5145\u55ae\u4e00\u7684\u64f7\u53d6\u56de\u61b6\uff0c\u64f7\u53d6\u76f8\u95dc\u56de\u61b6\uff0c\u4e26\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u63d0\u4f9b\u5168\u9762\u7684\u7b54\u6848\u3002\u5728\u4eba\u985e\u8a55\u91cf\u4e2d\uff0c\u6211\u5011\u4ee5 71.5% \u7684\u6e96\u78ba\u5ea6\u5c55\u793a\u4e86 OmniQuery \u7684\u6709\u6548\u6027\uff0c\u4e26\u4e14\u5b83\u512a\u65bc\u50b3\u7d71\u7684 RAG \u7cfb\u7d71\uff0c\u5728 74.5% \u7684\u6642\u9593\u4e2d\u7372\u52dd\u6216\u6253\u5e73\u3002", "author": "Jiahao Nick Li et.al.", "authors": "Jiahao Nick Li, Zhuohao, Zhang, Jiaju Ma", "id": "2409.08250v1", "paper_url": "http://arxiv.org/abs/2409.08250v1", "repo": "null"}}