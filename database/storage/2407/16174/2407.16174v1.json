{"2407.16174": {"publish_time": "2024-07-23", "title": "Pixel Embedding: Fully Quantized Convolutional Neural Network with Differentiable Lookup Table", "paper_summary": "By quantizing network weights and activations to low bitwidth, we can obtain\nhardware-friendly and energy-efficient networks. However, existing quantization\ntechniques utilizing the straight-through estimator and piecewise constant\nfunctions face the issue of how to represent originally high-bit input data\nwith low-bit values. To fully quantize deep neural networks, we propose pixel\nembedding, which replaces each float-valued input pixel with a vector of\nquantized values by using a lookup table. The lookup table or low-bit\nrepresentation of pixels is differentiable and trainable by backpropagation.\nSuch replacement of inputs with vectors is similar to word embedding in the\nnatural language processing field. Experiments on ImageNet and CIFAR-100 show\nthat pixel embedding reduces the top-5 error gap caused by quantizing the\nfloating points at the first layer to only 1% for the ImageNet dataset, and the\ntop-1 error gap caused by quantizing first and last layers to slightly over 1%\nfor the CIFAR-100 dataset. The usefulness of pixel embedding is further\ndemonstrated by inference time measurements, which demonstrate over 1.7 times\nspeedup compared to floating point precision first layer.", "paper_summary_zh": "\u900f\u904e\u5c07\u7db2\u8def\u6b0a\u91cd\u548c\u555f\u7528\u91cf\u5316\u70ba\u4f4e\u4f4d\u5143\u5bec\u5ea6\uff0c\u6211\u5011\u53ef\u4ee5\u7372\u5f97\u786c\u9ad4\u53cb\u5584\u4e14\u7bc0\u80fd\u7684\u7db2\u8def\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u91cf\u5316\u6280\u8853\u5229\u7528\u76f4\u901a\u4f30\u8a08\u5668\u548c\u5206\u6bb5\u5e38\u6578\u51fd\u6578\uff0c\u9762\u81e8\u5982\u4f55\u7528\u4f4e\u4f4d\u5143\u503c\u8868\u793a\u539f\u672c\u9ad8\u4f4d\u5143\u8f38\u5165\u8cc7\u6599\u7684\u554f\u984c\u3002\u70ba\u4e86\u5b8c\u5168\u91cf\u5316\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\uff0c\u6211\u5011\u63d0\u51fa\u50cf\u7d20\u5d4c\u5165\uff0c\u5b83\u7528\u67e5\u627e\u8868\u5c07\u6bcf\u500b\u6d6e\u9ede\u503c\u8f38\u5165\u50cf\u7d20\u66ff\u63db\u70ba\u91cf\u5316\u503c\u7684\u5411\u91cf\u3002\u67e5\u627e\u8868\u6216\u50cf\u7d20\u7684\u4f4e\u4f4d\u5143\u8868\u793a\u662f\u53ef\u5fae\u5206\u7684\uff0c\u4e26\u53ef\u900f\u904e\u53cd\u5411\u50b3\u64ad\u9032\u884c\u8a13\u7df4\u3002\u9019\u7a2e\u7528\u5411\u91cf\u66ff\u63db\u8f38\u5165\u7684\u65b9\u5f0f\u985e\u4f3c\u65bc\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u9818\u57df\u4e2d\u7684\u8a5e\u5d4c\u5165\u3002\u5728 ImageNet \u548c CIFAR-100 \u4e0a\u7684\u5be6\u9a57\u986f\u793a\uff0c\u50cf\u7d20\u5d4c\u5165\u5c07 ImageNet \u8cc7\u6599\u96c6\u7b2c\u4e00\u5c64\u6d6e\u9ede\u6578\u4f4d\u91cf\u5316\u9020\u6210\u7684 top-5 \u932f\u8aa4\u5dee\u8ddd\u6e1b\u5c11\u5230\u50c5 1%\uff0c\u800c\u5c0d\u65bc CIFAR-100 \u8cc7\u6599\u96c6\uff0c\u5c07\u7b2c\u4e00\u5c64\u548c\u6700\u5f8c\u4e00\u5c64\u91cf\u5316\u9020\u6210\u7684 top-1 \u932f\u8aa4\u5dee\u8ddd\u6e1b\u5c11\u5230\u7565\u9ad8\u65bc 1%\u3002\u50cf\u7d20\u5d4c\u5165\u7684\u6548\u7528\u9032\u4e00\u6b65\u900f\u904e\u63a8\u8ad6\u6642\u9593\u6e2c\u91cf\u5f97\u5230\u8b49\u660e\uff0c\u8207\u6d6e\u9ede\u7cbe\u5ea6\u7b2c\u4e00\u5c64\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u5347\u8d85\u904e 1.7 \u500d\u3002", "author": "Hiroyuki Tokunaga et.al.", "authors": "Hiroyuki Tokunaga, Joel Nicholls, Daria Vazhenina, Atsunori Kanemura", "id": "2407.16174v1", "paper_url": "http://arxiv.org/abs/2407.16174v1", "repo": "null"}}