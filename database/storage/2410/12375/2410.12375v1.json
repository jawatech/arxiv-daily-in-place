{"2410.12375": {"publish_time": "2024-10-16", "title": "PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking", "paper_summary": "PRefLexOR (Preference-based Recursive Language Modeling for Exploratory\nOptimization of Reasoning) combines preference optimization with concepts from\nReinforcement Learning to enable models to self-teach through iterative\nreasoning improvements. We propose a recursive learning approach that engages\nthe model in multi-step reasoning, revisiting, and refining intermediate steps\nbefore producing a final output in training and inference phases. Through\nmultiple training stages, the model first learns to align its reasoning with\naccurate decision paths by optimizing the log odds between preferred and\nnon-preferred responses. During this process, PRefLexOR builds a dynamic\nknowledge graph by generating questions from random text chunks and\nretrieval-augmentation to contextualize relevant details from the entire\ntraining corpus. In the second stage, preference optimization enhances model\nperformance by using rejection sampling to fine-tune reasoning quality by\ncontinually producing in-situ training data while masking the reasoning steps.\nRecursive optimization within a thinking token framework introduces iterative\nfeedback loops, where the model refines reasoning, achieving deeper coherence,\nconsistency, and adaptability. Implemented in small language models with only 3\nbillion parameters, we should that even tiny models can iteratively teach\nthemselves to reason with greater depth and reflectivity. Our implementation is\nstraightforward and can be incorporated into any existing pretrained LLM. We\nfocus our examples on applications in biological materials science and\ndemonstrate the method in a variety of case studies that range from in-domain\nto cross-domain applications. Using reasoning strategies that include thinking\nand reflection modalities we build a multi-agent recursive self-improving\ninference approach to successively improve responses via repeated sampling in\ninference time.", "paper_summary_zh": "PRefLexOR\uff08\u7528\u65bc\u63a2\u7d22\u6027\u63a8\u7406\u512a\u5316\u7684\u57fa\u65bc\u504f\u597d\u7684\u905e\u8ff4\u8a9e\u8a00\u5efa\u6a21\uff09\u5c07\u504f\u597d\u512a\u5316\u8207\u5f37\u5316\u5b78\u7fd2\u4e2d\u7684\u6982\u5ff5\u76f8\u7d50\u5408\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u901a\u904e\u53cd\u8986\u63a8\u7406\u6539\u9032\u4f86\u81ea\u6211\u6559\u5b78\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u905e\u8ff4\u5b78\u7fd2\u65b9\u6cd5\uff0c\u8b93\u6a21\u578b\u53c3\u8207\u591a\u6b65\u9a5f\u63a8\u7406\u3001\u91cd\u65b0\u5be9\u8996\u548c\u6539\u9032\u4e2d\u9593\u6b65\u9a5f\uff0c\u7136\u5f8c\u5728\u8a13\u7df4\u548c\u63a8\u7406\u968e\u6bb5\u7522\u751f\u6700\u7d42\u8f38\u51fa\u3002\u901a\u904e\u591a\u500b\u8a13\u7df4\u968e\u6bb5\uff0c\u6a21\u578b\u9996\u5148\u5b78\u7fd2\u901a\u904e\u512a\u5316\u9996\u9078\u548c\u975e\u9996\u9078\u97ff\u61c9\u4e4b\u9593\u7684\u5c0d\u6578\u5e7e\u7387\uff0c\u4f7f\u5176\u63a8\u7406\u8207\u6e96\u78ba\u7684\u6c7a\u7b56\u8def\u5f91\u4fdd\u6301\u4e00\u81f4\u3002\u5728\u6b64\u904e\u7a0b\u4e2d\uff0cPRefLexOR \u901a\u904e\u5f9e\u96a8\u6a5f\u6587\u672c\u584a\u751f\u6210\u554f\u984c\u548c\u6aa2\u7d22\u589e\u5f37\u4f86\u69cb\u5efa\u4e00\u500b\u52d5\u614b\u77e5\u8b58\u5716\uff0c\u5f9e\u6574\u500b\u8a13\u7df4\u8a9e\u6599\u5eab\u4e2d\u63d0\u53d6\u76f8\u95dc\u7d30\u7bc0\u4ee5\u9032\u884c\u8a9e\u5883\u5316\u3002\u5728\u7b2c\u4e8c\u968e\u6bb5\uff0c\u504f\u597d\u512a\u5316\u901a\u904e\u4f7f\u7528\u62d2\u7d55\u63a1\u6a23\u4f86\u5fae\u8abf\u63a8\u7406\u8cea\u91cf\uff0c\u5f9e\u800c\u589e\u5f37\u6a21\u578b\u6027\u80fd\uff0c\u540c\u6642\u9023\u7e8c\u7522\u751f\u539f\u4f4d\u8a13\u7df4\u6578\u64da\uff0c\u540c\u6642\u63a9\u84cb\u63a8\u7406\u6b65\u9a5f\u3002\u5728\u601d\u8003\u4ee4\u724c\u6846\u67b6\u5167\u9032\u884c\u905e\u8ff4\u512a\u5316\u6703\u5f15\u5165\u8fed\u4ee3\u53cd\u994b\u8ff4\u8def\uff0c\u5176\u4e2d\u6a21\u578b\u6703\u6539\u9032\u63a8\u7406\uff0c\u5f9e\u800c\u5be6\u73fe\u66f4\u6df1\u5165\u7684\u9023\u8cab\u6027\u3001\u4e00\u81f4\u6027\u548c\u9069\u61c9\u6027\u3002\u5728\u53ea\u6709 30 \u5104\u500b\u53c3\u6578\u7684\u5c0f\u8a9e\u8a00\u6a21\u578b\u4e2d\u5be6\u73fe\uff0c\u6211\u5011\u61c9\u8a72\u8b93\u5373\u4f7f\u662f\u5f88\u5c0f\u7684\u6a21\u578b\u4e5f\u80fd\u901a\u904e\u8fed\u4ee3\u7684\u65b9\u5f0f\u6559\u6703\u81ea\u5df1\u4ee5\u66f4\u5927\u7684\u6df1\u5ea6\u548c\u53cd\u601d\u80fd\u529b\u9032\u884c\u63a8\u7406\u3002\u6211\u5011\u7684\u5be6\u73fe\u975e\u5e38\u76f4\u63a5\uff0c\u53ef\u4ee5\u6574\u5408\u5230\u4efb\u4f55\u73fe\u6709\u7684\u9810\u8a13\u7df4 LLM \u4e2d\u3002\u6211\u5011\u5c07\u6211\u5011\u7684\u793a\u4f8b\u91cd\u9ede\u653e\u5728\u751f\u7269\u6750\u6599\u79d1\u5b78\u61c9\u7528\u4e0a\uff0c\u4e26\u5728\u5f9e\u57df\u5167\u5230\u8de8\u57df\u61c9\u7528\u7b49\u5404\u7a2e\u6848\u4f8b\u7814\u7a76\u4e2d\u6f14\u793a\u4e86\u8a72\u65b9\u6cd5\u3002\u4f7f\u7528\u5305\u62ec\u601d\u8003\u548c\u53cd\u601d\u6a21\u5f0f\u5728\u5167\u7684\u63a8\u7406\u7b56\u7565\uff0c\u6211\u5011\u69cb\u5efa\u4e86\u4e00\u500b\u591a\u4ee3\u7406\u905e\u8ff4\u81ea\u6211\u6539\u9032\u63a8\u7406\u65b9\u6cd5\uff0c\u4ee5\u901a\u904e\u5728\u63a8\u7406\u6642\u9593\u91cd\u8907\u63a1\u6a23\u4f86\u9023\u7e8c\u6539\u9032\u97ff\u61c9\u3002", "author": "Markus J. Buehler et.al.", "authors": "Markus J. Buehler", "id": "2410.12375v1", "paper_url": "http://arxiv.org/abs/2410.12375v1", "repo": "null"}}