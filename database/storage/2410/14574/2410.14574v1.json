{"2410.14574": {"publish_time": "2024-10-18", "title": "MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts", "paper_summary": "Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.", "paper_summary_zh": "\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\uff08SMoE\uff09\u5df2\u6210\u4e3a\u89e3\u9501\u6df1\u5ea6\u5b66\u4e60\u4e2d\u65e0\u4e0e\u4f26\u6bd4\u7684\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u3002SMoE \u6709\u53ef\u80fd\u5448\u6307\u6570\u7ea7\u589e\u52a0\u53c2\u6570\u8ba1\u6570\uff0c\u540c\u65f6\u4ec5\u901a\u8fc7\u6fc0\u6d3b\u7ed9\u5b9a\u6837\u672c\u4e2d\u8fd9\u4e9b\u53c2\u6570\u7684\u4e00\u4e2a\u5c0f\u5b50\u96c6\u6765\u7ef4\u6301\u6a21\u578b\u7684\u6548\u7387\u3002\u7136\u800c\uff0c\u636e\u89c2\u5bdf\uff0cSMoE \u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u5e76\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u7684\u5206\u5e03\uff0c\u5bfc\u81f4\u6a21\u578b\u5bf9\u6570\u636e\u6c61\u67d3\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u9996\u5148\u5728 SMoE \u4e2d\u4e13\u5bb6\u8868\u793a\u7684\u52a8\u6001\u4e0e\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u7684\u68af\u5ea6\u4e0b\u964d\u4e4b\u95f4\u5efa\u7acb\u4e86\u8054\u7cfb\u3002\u5229\u7528\u6211\u4eec\u7684\u6846\u67b6\uff0c\u6211\u4eec\u968f\u540e\u5c06\u52a8\u91cf\u6574\u5408\u5230 SMoE \u4e2d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u540d\u4e3a MomentumSMoE \u7684\u65b0 SMoE\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u5e76\u901a\u8fc7\u6570\u5b57\u6f14\u793a\u4e86 MomentumSMoE \u6bd4 SMoE \u66f4\u7a33\u5b9a\u3001\u66f4\u7a33\u5065\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u5728\u5404\u79cd\u5b9e\u9645\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86 MomentumSMoE \u76f8\u5bf9\u4e8e SMoE \u7684\u4f18\u52bf\uff0c\u5305\u62ec ImageNet-1K \u5bf9\u8c61\u8bc6\u522b\u548c WikiText-103 \u8bed\u8a00\u5efa\u6a21\u3002\u6211\u4eec\u5c55\u793a\u4e86 MomentumSMoE \u5bf9\u591a\u79cd\u7c7b\u578b SMoE \u6a21\u578b\u7684\u9002\u7528\u6027\uff0c\u5305\u62ec\u89c6\u89c9\u7a00\u758f MoE \u6a21\u578b (V-MoE) \u548c\u901a\u7528\u8bed\u8a00\u6a21\u578b (GLaM)\u3002\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u5176\u4ed6\u57fa\u4e8e\u52a8\u91cf\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4f8b\u5982 Adam\uff0c\u53ef\u4ee5\u8f7b\u677e\u5730\u6574\u5408\u5230 MomentumSMoE \u6846\u67b6\u4e2d\uff0c\u7528\u4e8e\u8bbe\u8ba1\u5177\u6709\u66f4\u597d\u6027\u80fd\u3001\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u7684\u989d\u5916\u8ba1\u7b97\u6210\u672c\u548c\u7b80\u5355\u5b9e\u73b0\u7684\u65b0 SMoE \u6a21\u578b\u3002", "author": "Rachel S. Y. Teo et.al.", "authors": "Rachel S. Y. Teo, Tan M. Nguyen", "id": "2410.14574v1", "paper_url": "http://arxiv.org/abs/2410.14574v1", "repo": "https://github.com/rachtsy/momentumsmoe"}}