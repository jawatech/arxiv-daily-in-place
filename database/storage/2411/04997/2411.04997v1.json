{"2411.04997": {"publish_time": "2024-11-07", "title": "LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation", "paper_summary": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks.", "paper_summary_zh": "CLIP \u662f\u7576\u4eca\u6700\u91cd\u8981\u7684\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u4e4b\u4e00\u3002\u662f\u4ec0\u9ebc\u8ce6\u4e88\u4e86 CLIP \u7684\u80fd\u529b\uff1f\u81ea\u7136\u8a9e\u8a00\u63d0\u4f9b\u7684\u8c50\u5bcc\u76e3\u7763\u8a0a\u865f\uff0c\u4eba\u985e\u77e5\u8b58\u7684\u8f09\u9ad4\uff0c\u5851\u9020\u4e86\u4e00\u500b\u5f37\u5927\u7684\u8de8\u6a21\u614b\u8868\u793a\u7a7a\u9593\u3002\u7136\u800c\uff0c\u96a8\u8457 GPT-4 \u548c LLaMA \u7b49\u5927\u578b\u8a9e\u8a00\u6a21\u578b LLM \u7684\u5feb\u901f\u9032\u5c55\uff0c\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u7684\u754c\u9650\u4e0d\u65b7\u88ab\u63a8\u52d5\u3002\u9019\u5f15\u767c\u4e86\u4e00\u500b\u6709\u8da3\u7684\u554f\u984c\uff1aLLM \u7684\u80fd\u529b\u662f\u5426\u53ef\u4ee5\u88ab\u5229\u7528\u4f86\u9032\u4e00\u6b65\u6539\u9032\u591a\u6a21\u614b\u8868\u793a\u5b78\u7fd2\uff1f\u5c07 LLM \u7d0d\u5165 CLIP \u7684\u6f5b\u5728\u597d\u8655\u5f88\u660e\u986f\u3002LLM \u5f37\u5927\u7684\u6587\u672c\u7406\u89e3\u529b\u53ef\u4ee5\u5f9e\u6839\u672c\u4e0a\u63d0\u9ad8 CLIP \u8655\u7406\u5716\u50cf\u6a19\u984c\u7684\u80fd\u529b\uff0c\u5927\u5e45\u589e\u5f37\u5176\u8655\u7406\u9577\u800c\u8907\u96dc\u6587\u672c\u7684\u80fd\u529b\uff0c\u9019\u662f\u9999\u8349 CLIP \u7684\u4e00\u500b\u773e\u6240\u5468\u77e5\u9650\u5236\u3002\u6b64\u5916\uff0cLLM \u662f\u5728\u5927\u91cf\u7684\u6587\u672c\u8a9e\u6599\u5eab\u4e0a\u8a13\u7df4\u7684\uff0c\u64c1\u6709\u958b\u653e\u4e16\u754c\u7684\u77e5\u8b58\u3002\u9019\u4f7f\u4ed6\u5011\u80fd\u5920\u5728\u8a13\u7df4\u671f\u9593\u64f4\u5c55\u6a19\u984c\u4fe1\u606f\uff0c\u5f9e\u800c\u63d0\u9ad8\u5b78\u7fd2\u904e\u7a0b\u7684\u6548\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LLM2CLIP\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528 LLM \u7684\u529b\u91cf\u4f86\u91cb\u653e CLIP \u7684\u6f5b\u529b\u3002\u901a\u904e\u5728\u5c0d\u6bd4\u5b78\u7fd2\u7684\u6a19\u984c\u7a7a\u9593\u4e2d\u5fae\u8abf LLM\uff0c\u6211\u5011\u5c07\u5176\u6587\u672c\u80fd\u529b\u63d0\u53d6\u5230\u8f38\u51fa\u5d4c\u5165\u4e2d\uff0c\u986f\u8457\u63d0\u9ad8\u4e86\u8f38\u51fa\u5c64\u7684\u6587\u672c\u53ef\u5340\u5206\u6027\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u9ad8\u6548\u7684\u8a13\u7df4\u904e\u7a0b\uff0c\u5176\u4e2d\u5fae\u8abf\u5f8c\u7684 LLM \u5145\u7576 CLIP \u8996\u89ba\u7de8\u78bc\u5668\u7684\u5f37\u5927\u6559\u5e2b\u3002\u7531\u65bc LLM \u7684\u5b58\u5728\uff0c\u6211\u5011\u73fe\u5728\u53ef\u4ee5\u7d0d\u5165\u66f4\u9577\u3001\u66f4\u8907\u96dc\u7684\u6a19\u984c\uff0c\u800c\u4e0d\u6703\u53d7\u5230\u9999\u8349 CLIP \u7684\u6587\u672c\u7de8\u78bc\u5668\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u80fd\u529b\u9650\u5236\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u9019\u7a2e\u65b9\u6cd5\u5728\u8de8\u6a21\u614b\u4efb\u52d9\u4e2d\u5e36\u4f86\u4e86\u986f\u8457\u7684\u6539\u9032\u3002", "author": "Weiquan Huang et.al.", "authors": "Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu", "id": "2411.04997v1", "paper_url": "http://arxiv.org/abs/2411.04997v1", "repo": "https://github.com/microsoft/LLM2CLIP"}}