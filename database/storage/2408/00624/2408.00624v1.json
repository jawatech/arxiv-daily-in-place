{"2408.00624": {"publish_time": "2024-08-01", "title": "SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data", "paper_summary": "In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.", "paper_summary_zh": "\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 SynesLM\uff0c\u4e00\u500b\u7d71\u4e00\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u57f7\u884c\u4e09\u9805\u591a\u6a21\u614b\u8a9e\u8a00\u7406\u89e3\u4efb\u52d9\uff1a\u97f3\u8a0a\u8996\u89ba\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (AV-ASR) \u548c\u8996\u89ba\u8f14\u52a9\u8a9e\u97f3/\u6a5f\u5668\u7ffb\u8b6f (VST/VMT)\u3002\u8207\u4ee5\u5f80\u5c08\u6ce8\u65bc\u5507\u90e8\u52d5\u4f5c\u4f5c\u70ba\u8a9e\u97f3\u8a0a\u865f\u8996\u89ba\u7dda\u7d22\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u5011\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u6574\u500b\u756b\u9762\u4e2d\u66f4\u901a\u7528\u7684\u8996\u89ba\u8cc7\u8a0a\uff0c\u4f8b\u5982\u7269\u4ef6\u548c\u52d5\u4f5c\u3002\u6b64\u5916\uff0c\u6211\u5011\u4f7f\u7528\u5408\u6210\u5f71\u50cf\u8cc7\u6599\u4f86\u589e\u5f37\u5f71\u50cf\u548c\u8a9e\u97f3\u8cc7\u6599\u4e4b\u9593\u7684\u95dc\u806f\u6027\u3002\u6211\u5011\u4ee5 How2 \u8cc7\u6599\u96c6\u4f86\u8a55\u91cf SynesLM\uff0c\u8b49\u660e\u5176\u6548\u80fd\u8207\u5c08\u9580\u7528\u65bc AV-ASR \u7684\u6700\u65b0 (SOTA) \u6a21\u578b\u76f8\u7576\uff0c\u540c\u6642\u7dad\u6301\u6211\u5011\u7684\u591a\u4efb\u52d9\u67b6\u69cb\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5c0d\u65bc\u96f6\u6b21\u5b78\u7fd2 AV-ASR\uff0cSynesLM \u5728 VisSpeech \u8cc7\u6599\u96c6\u4e0a\u5c07\u5b57\u5143\u932f\u8aa4\u7387 (WER) \u5f9e 43.4% \u964d\u4f4e\u5230 39.4%\uff0c\u9054\u5230\u4e86 SOTA \u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728 VST \u548c VMT \u4e2d\u7684\u7d50\u679c\u512a\u65bc\u5148\u524d\u7684\u7d50\u679c\uff0c\u5c07 VST \u7684 BLEU \u5206\u6578\u5f9e 37.2 \u63d0\u5347\u5230 43.5\uff0c\u5c07 VMT \u7684 BLEU \u5206\u6578\u5f9e 54.4 \u63d0\u5347\u5230 54.8\u3002", "author": "Yichen Lu et.al.", "authors": "Yichen Lu, Jiaqi Song, Xuankai Chang, Hengwei Bian, Soumi Maiti, Shinji Watanabe", "id": "2408.00624v1", "paper_url": "http://arxiv.org/abs/2408.00624v1", "repo": "null"}}