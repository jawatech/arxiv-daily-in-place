{"2411.17309": {"publish_time": "2024-11-26", "title": "PIM-AI: A Novel Architecture for High-Efficiency LLM Inference", "paper_summary": "Large Language Models (LLMs) have become essential in a variety of\napplications due to their advanced language understanding and generation\ncapabilities. However, their computational and memory requirements pose\nsignificant challenges to traditional hardware architectures.\nProcessing-in-Memory (PIM), which integrates computational units directly into\nmemory chips, offers several advantages for LLM inference, including reduced\ndata transfer bottlenecks and improved power efficiency.\n  This paper introduces PIM-AI, a novel DDR5/LPDDR5 PIM architecture designed\nfor LLM inference without modifying the memory controller or DDR/LPDDR memory\nPHY. We have developed a simulator to evaluate the performance of PIM-AI in\nvarious scenarios and demonstrate its significant advantages over conventional\narchitectures. In cloud-based scenarios, PIM-AI reduces the 3-year TCO per\nqueries-per-second by up to 6.94x compared to state-of-the-art GPUs, depending\non the LLM model used. In mobile scenarios, PIM-AI achieves a 10- to 20-fold\nreduction in energy per token compared to state-of-the-art mobile SoCs,\nresulting in 25 to 45~\\% more queries per second and 6.9x to 13.4x less energy\nper query, extending battery life and enabling more inferences per charge.\n  These results highlight PIM-AI's potential to revolutionize LLM deployments,\nmaking them more efficient, scalable, and sustainable.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7531\u65bc\u5176\u5148\u9032\u7684\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u5df2\u6210\u70ba\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\u9700\u6c42\u5c0d\u50b3\u7d71\u786c\u9ad4\u67b6\u69cb\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u8655\u7406\u8a18\u61b6\u9ad4 (PIM) \u5c07\u904b\u7b97\u55ae\u5143\u76f4\u63a5\u6574\u5408\u5230\u8a18\u61b6\u9ad4\u6676\u7247\u4e2d\uff0c\u70ba LLM \u63a8\u8ad6\u63d0\u4f9b\u4e86\u591a\u9805\u512a\u9ede\uff0c\u5305\u62ec\u6e1b\u5c11\u8cc7\u6599\u50b3\u8f38\u74f6\u9838\u548c\u63d0\u5347\u96fb\u529b\u6548\u7387\u3002\u9019\u7bc7\u8ad6\u6587\u4ecb\u7d39\u4e86 PIM-AI\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684 DDR5/LPDDR5 PIM \u67b6\u69cb\uff0c\u8a2d\u8a08\u7528\u65bc LLM \u63a8\u8ad6\uff0c\u4e14\u4e0d\u4fee\u6539\u8a18\u61b6\u9ad4\u63a7\u5236\u5668\u6216 DDR/LPDDR \u8a18\u61b6\u9ad4 PHY\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u6a21\u64ec\u5668\u4f86\u8a55\u4f30 PIM-AI \u5728\u5404\u7a2e\u5834\u666f\u4e2d\u7684\u6548\u80fd\uff0c\u4e26\u5c55\u793a\u5176\u76f8\u8f03\u65bc\u50b3\u7d71\u67b6\u69cb\u7684\u986f\u8457\u512a\u52e2\u3002\u5728\u96f2\u7aef\u5834\u666f\u4e2d\uff0cPIM-AI \u5c07\u6bcf\u79d2\u67e5\u8a62\u7684 3 \u5e74 TCO \u964d\u4f4e\u4e86\u591a\u9054 6.94 \u500d\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u6240\u4f7f\u7528\u7684 LLM \u6a21\u578b\uff0c\u8207\u6700\u5148\u9032\u7684 GPU \u76f8\u6bd4\u3002\u5728\u884c\u52d5\u5834\u666f\u4e2d\uff0cPIM-AI \u6bcf\u500b\u4ee3\u5e63\u7684\u80fd\u8017\u964d\u4f4e\u4e86 10 \u5230 20 \u500d\uff0c\u8207\u6700\u5148\u9032\u7684\u884c\u52d5 SoC \u76f8\u6bd4\uff0c\u6bcf\u79d2\u67e5\u8a62\u6578\u91cf\u589e\u52a0\u4e86 25% \u5230 45%\uff0c\u6bcf\u9805\u67e5\u8a62\u7684\u80fd\u8017\u964d\u4f4e\u4e86 6.9 \u500d\u5230 13.4 \u500d\uff0c\u5ef6\u9577\u4e86\u96fb\u6c60\u7e8c\u822a\u529b\uff0c\u4e26\u5728\u6bcf\u6b21\u5145\u96fb\u6642\u80fd\u9032\u884c\u66f4\u591a\u63a8\u8ad6\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86 PIM-AI \u5fb9\u5e95\u8b8a\u9769 LLM \u90e8\u7f72\u7684\u6f5b\u529b\uff0c\u4f7f\u5176\u66f4\u6709\u6548\u7387\u3001\u66f4\u5177\u53ef\u64f4\u5145\u6027\u548c\u6c38\u7e8c\u6027\u3002", "author": "Cristobal Ortega et.al.", "authors": "Cristobal Ortega, Yann Falevoz, Renaud Ayrignac", "id": "2411.17309v1", "paper_url": "http://arxiv.org/abs/2411.17309v1", "repo": "null"}}