{"2407.13035": {"publish_time": "2024-07-17", "title": "Pre-Trained Foundation Model representations to uncover Breathing patterns in Speech", "paper_summary": "The process of human speech production involves coordinated respiratory\naction to elicit acoustic speech signals. Typically, speech is produced when\nair is forced from the lungs and is modulated by the vocal tract, where such\nactions are interspersed by moments of breathing in air (inhalation) to refill\nthe lungs again. Respiratory rate (RR) is a vital metric that is used to assess\nthe overall health, fitness, and general well-being of an individual. Existing\napproaches to measure RR (number of breaths one takes in a minute) are\nperformed using specialized equipment or training. Studies have demonstrated\nthat machine learning algorithms can be used to estimate RR using bio-sensor\nsignals as input. Speech-based estimation of RR can offer an effective approach\nto measure the vital metric without requiring any specialized equipment or\nsensors. This work investigates a machine learning based approach to estimate\nRR from speech segments obtained from subjects speaking to a close-talking\nmicrophone device. Data were collected from N=26 individuals, where the\ngroundtruth RR was obtained through commercial grade chest-belts and then\nmanually corrected for any errors. A convolutional long-short term memory\nnetwork (Conv-LSTM) is proposed to estimate respiration time-series data from\nthe speech signal. We demonstrate that the use of pre-trained representations\nobtained from a foundation model, such as Wav2Vec2, can be used to estimate\nrespiration-time-series with low root-mean-squared error and high correlation\ncoefficient, when compared with the baseline. The model-driven time series can\nbe used to estimate $RR$ with a low mean absolute error (MAE) ~ 1.6\nbreaths/min.", "paper_summary_zh": "\u4eba\u985e\u8aaa\u8a71\u7684\u904e\u7a0b\u6d89\u53ca\u5354\u8abf\u7684\u547c\u5438\u4f5c\u7528\u4ee5\u5f15\u767c\u8072\u5b78\u8a9e\u97f3\u8a0a\u865f\u3002\u901a\u5e38\uff0c\u7576\u7a7a\u6c23\u5f9e\u80ba\u90e8\u6392\u51fa\u4e26\u7531\u8072\u9053\u8abf\u7bc0\u6642\u6703\u7522\u751f\u8a9e\u97f3\uff0c\u5176\u4e2d\u9019\u4e9b\u52d5\u4f5c\u6703\u7a7f\u63d2\u5438\u5165\u7a7a\u6c23\uff08\u5438\u6c23\uff09\u7684\u6642\u523b\uff0c\u4ee5\u518d\u6b21\u586b\u6eff\u80ba\u90e8\u3002\u547c\u5438\u7387 (RR) \u662f\u4e00\u9805\u91cd\u8981\u7684\u6307\u6a19\uff0c\u7528\u65bc\u8a55\u4f30\u500b\u4eba\u7684\u6574\u9ad4\u5065\u5eb7\u3001\u9ad4\u80fd\u548c\u4e00\u822c\u5e78\u798f\u611f\u3002\u73fe\u6709\u7684\u6e2c\u91cf RR\uff08\u6bcf\u5206\u9418\u547c\u5438\u6b21\u6578\uff09\u65b9\u6cd5\u662f\u4f7f\u7528\u5c08\u696d\u8a2d\u5099\u6216\u8a13\u7df4\u4f86\u57f7\u884c\u3002\u7814\u7a76\u8868\u660e\uff0c\u6a5f\u5668\u5b78\u7fd2\u6f14\u7b97\u6cd5\u53ef\u7528\u65bc\u4f7f\u7528\u751f\u7269\u611f\u6e2c\u5668\u8a0a\u865f\u4f5c\u70ba\u8f38\u5165\u4f86\u4f30\u8a08 RR\u3002\u57fa\u65bc\u8a9e\u97f3\u7684 RR \u4f30\u8a08\u53ef\u4ee5\u63d0\u4f9b\u4e00\u7a2e\u6709\u6548\u7684\u6e2c\u91cf\u65b9\u6cd5\uff0c\u7121\u9700\u4efb\u4f55\u5c08\u696d\u8a2d\u5099\u6216\u611f\u6e2c\u5668\u3002\u9019\u9805\u5de5\u4f5c\u63a2\u8a0e\u4e86\u4e00\u7a2e\u57fa\u65bc\u6a5f\u5668\u5b78\u7fd2\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u5f9e\u53d7\u8a66\u8005\u5c0d\u8457\u8fd1\u8b1b\u9ea5\u514b\u98a8\u88dd\u7f6e\u8aaa\u8a71\u6240\u7372\u5f97\u7684\u8a9e\u97f3\u7247\u6bb5\u4e2d\u4f30\u8a08 RR\u3002\u8cc7\u6599\u662f\u5f9e N=26 \u500b\u4eba\u6536\u96c6\u7684\uff0c\u5176\u4e2d\u900f\u904e\u5546\u7528\u7b49\u7d1a\u80f8\u5e36\u53d6\u5f97\u57fa\u6e96 RR\uff0c\u7136\u5f8c\u624b\u52d5\u66f4\u6b63\u4efb\u4f55\u932f\u8aa4\u3002\u63d0\u51fa\u4e86\u4e00\u500b\u5377\u7a4d\u9577\u77ed\u671f\u8a18\u61b6\u7db2\u8def (Conv-LSTM) \u4f86\u5f9e\u8a9e\u97f3\u8a0a\u865f\u4f30\u8a08\u547c\u5438\u6642\u9593\u5e8f\u5217\u8cc7\u6599\u3002\u6211\u5011\u8b49\u660e\uff0c\u8207\u57fa\u6e96\u76f8\u6bd4\uff0c\u4f7f\u7528\u5f9e\u57fa\u790e\u6a21\u578b\uff08\u4f8b\u5982 Wav2Vec2\uff09\u7372\u5f97\u7684\u9810\u8a13\u7df4\u8868\u793a\u53ef\u4ee5\u4f30\u8a08\u5177\u6709\u4f4e\u5747\u65b9\u6839\u8aa4\u5dee\u548c\u9ad8\u76f8\u95dc\u4fc2\u6578\u7684\u547c\u5438\u6642\u9593\u5e8f\u5217\u3002\u6a21\u578b\u9a45\u52d5\u7684\u6642\u9593\u5e8f\u5217\u53ef\u7528\u65bc\u4f30\u8a08\u5e73\u5747\u7d55\u5c0d\u8aa4\u5dee (MAE) \u7d04\u70ba 1.6 \u6b21/\u5206\u9418\u7684\u4f4e $RR$\u3002", "author": "Vikramjit Mitra et.al.", "authors": "Vikramjit Mitra, Anirban Chatterjee, Ke Zhai, Helen Weng, Ayuko Hill, Nicole Hay, Christopher Webb, Jamie Cheng, Erdrin Azemi", "id": "2407.13035v1", "paper_url": "http://arxiv.org/abs/2407.13035v1", "repo": "null"}}