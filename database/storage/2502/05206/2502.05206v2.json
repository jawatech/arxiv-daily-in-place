{"2502.05206": {"publish_time": "2025-02-02", "title": "Safety at Scale: A Comprehensive Survey of Large Model Safety", "paper_summary": "The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.", "paper_summary_zh": "<paragraph>\u5927\u578b\u6a21\u578b\u7684\u5feb\u901f\u9032\u5c55\uff0c\u5f97\u76ca\u65bc\u5b83\u5011\u5728\u901a\u904e\u5927\u898f\u6a21\u9810\u8a13\u7df4\u9032\u884c\u5b78\u7fd2\u548c\u6982\u62ec\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5df2\u7d93\u91cd\u5851\u4e86\u4eba\u5de5\u667a\u80fd (AI) \u7684\u683c\u5c40\u3002\u9019\u4e9b\u6a21\u578b\u73fe\u5728\u662f\u5ee3\u6cdb\u61c9\u7528\u7a0b\u5f0f\uff08\u5305\u62ec\u5c0d\u8a71\u5f0f AI\u3001\u63a8\u85a6\u7cfb\u7d71\u3001\u81ea\u52d5\u99d5\u99db\u3001\u5167\u5bb9\u751f\u6210\u3001\u91ab\u7642\u8a3a\u65b7\u548c\u79d1\u5b78\u767c\u73fe\uff09\u7684\u57fa\u790e\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u5ee3\u6cdb\u90e8\u7f72\u4e5f\u4f7f\u5b83\u5011\u9762\u81e8\u91cd\u5927\u7684\u5b89\u5168\u98a8\u96aa\uff0c\u5f15\u767c\u4e86\u5c0d\u7a69\u5065\u6027\u3001\u53ef\u9760\u6027\u548c\u502b\u7406\u5f71\u97ff\u7684\u64d4\u6182\u3002\u672c\u8abf\u67e5\u63d0\u4f9b\u4e86\u5c0d\u5927\u578b\u6a21\u578b\u7576\u524d\u5b89\u5168\u7814\u7a76\u7684\u7cfb\u7d71\u6027\u56de\u9867\uff0c\u6db5\u84cb\u8996\u89ba\u57fa\u790e\u6a21\u578b (VFM)\u3001\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3001\u8996\u89ba\u8a9e\u8a00\u9810\u8a13\u7df4 (VLP) \u6a21\u578b\u3001\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\u3001\u64f4\u6563\u6a21\u578b (DM) \u548c\u57fa\u65bc\u5927\u578b\u6a21\u578b\u7684\u4ee3\u7406\u3002\u6211\u5011\u7684\u8ca2\u737b\u7e3d\u7d50\u5982\u4e0b\uff1a(1) \u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u91dd\u5c0d\u9019\u4e9b\u6a21\u578b\u7684\u5b89\u5168\u5a01\u8105\u7684\u5168\u9762\u5206\u985e\uff0c\u5305\u62ec\u5c0d\u6297\u6027\u653b\u64ca\u3001\u8cc7\u6599\u4e2d\u6bd2\u3001\u5f8c\u9580\u653b\u64ca\u3001\u8d8a\u7344\u548c\u63d0\u793a\u6ce8\u5165\u653b\u64ca\u3001\u80fd\u91cf\u5ef6\u9072\u653b\u64ca\u3001\u8cc7\u6599\u548c\u6a21\u578b\u63d0\u53d6\u653b\u64ca\u4ee5\u53ca\u65b0\u8208\u7684\u7279\u5b9a\u4ee3\u7406\u5a01\u8105\u3002(2) \u6211\u5011\u6aa2\u8996\u4e86\u91dd\u5c0d\u6bcf\u7a2e\u985e\u578b\u653b\u64ca\u63d0\u51fa\u7684\u9632\u79a6\u7b56\u7565\uff08\u5982\u679c\u6709\u7684\u8a71\uff09\uff0c\u4e26\u7e3d\u7d50\u4e86\u5b89\u5168\u7814\u7a76\u4e2d\u5e38\u7528\u7684\u8cc7\u6599\u96c6\u548c\u57fa\u6e96\u3002(3) \u57fa\u65bc\u6b64\uff0c\u6211\u5011\u627e\u51fa\u4e26\u8a0e\u8ad6\u4e86\u5927\u578b\u6a21\u578b\u5b89\u5168\u4e2d\u7684\u958b\u653e\u6027\u6311\u6230\uff0c\u5f37\u8abf\u4e86\u5c0d\u5168\u9762\u5b89\u5168\u8a55\u4f30\u3001\u53ef\u64f4\u5145\u4e14\u6709\u6548\u7684\u9632\u79a6\u6a5f\u5236\u4ee5\u53ca\u6c38\u7e8c\u8cc7\u6599\u5be6\u52d9\u7684\u9700\u6c42\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u5f37\u8abf\u4e86\u7814\u7a76\u793e\u7fa4\u548c\u570b\u969b\u5408\u4f5c\u5171\u540c\u52aa\u529b\u7684\u5fc5\u8981\u6027\u3002\u6211\u5011\u7684\u7814\u7a76\u53ef\u4f5c\u70ba\u7814\u7a76\u4eba\u54e1\u548c\u5f9e\u696d\u4eba\u54e1\u7684\u6709\u7528\u53c3\u8003\uff0c\u4fc3\u9032\u5168\u9762\u9632\u79a6\u7cfb\u7d71\u548c\u5e73\u53f0\u7684\u6301\u7e8c\u767c\u5c55\uff0c\u4ee5\u4fdd\u8b77 AI \u6a21\u578b\u3002</paragraph>", "author": "Xingjun Ma et.al.", "authors": "Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang", "id": "2502.05206v2", "paper_url": "http://arxiv.org/abs/2502.05206v2", "repo": "null"}}