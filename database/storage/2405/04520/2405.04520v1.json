{"2405.04520": {"publish_time": "2024-05-07", "title": "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts", "paper_summary": "Large language models (LLMs) have manifested strong ability to generate codes\nfor productive activities. However, current benchmarks for code synthesis, such\nas HumanEval, MBPP, and DS-1000, are predominantly oriented towards\nintroductory tasks on algorithm and data science, insufficiently satisfying\nchallenging requirements prevalent in real-world coding. To fill this gap, we\npropose NaturalCodeBench (NCB), a challenging code benchmark designed to mirror\nthe complexity and variety of scenarios in real coding tasks. NCB comprises 402\nhigh-quality problems in Python and Java, meticulously selected from natural\nuser queries from online coding services, covering 6 different domains. Noting\nthe extraordinary difficulty in creating testing cases for real-world queries,\nwe also introduce a semi-automated pipeline to enhance the efficiency of test\ncase construction. Comparing with manual solutions, it achieves an efficiency\nincrease of more than 4 times. Our systematic experiments on 39 LLMs find that\nperformance gaps on NCB between models with close HumanEval scores could still\nbe significant, indicating a lack of focus on practical code synthesis\nscenarios or over-specified optimization on HumanEval. On the other hand, even\nthe best-performing GPT-4 is still far from satisfying on NCB. The evaluation\ntoolkit and development set are available at\nhttps://github.com/THUDM/NaturalCodeBench.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u7522\u751f\u4ee3\u78bc\u4ee5\u9032\u884c\u751f\u7522\u6d3b\u52d5\u7684\u5f37\u5927\u80fd\u529b\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u7a0b\u5f0f\u78bc\u5408\u6210\u57fa\u6e96\uff0c\u4f8b\u5982 HumanEval\u3001MBPP \u548c DS-1000\uff0c\u4e3b\u8981\u91dd\u5c0d\u6f14\u7b97\u6cd5\u548c\u8cc7\u6599\u79d1\u5b78\u7684\u5165\u9580\u4efb\u52d9\uff0c\u7121\u6cd5\u5145\u5206\u6eff\u8db3\u73fe\u5be6\u4e16\u754c\u7de8\u78bc\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6311\u6230\u6027\u9700\u6c42\u3002\u70ba\u4e86\u586b\u88dc\u9019\u500b\u7a7a\u767d\uff0c\u6211\u5011\u63d0\u51fa NaturalCodeBench (NCB)\uff0c\u4e00\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u7a0b\u5f0f\u78bc\u57fa\u6e96\uff0c\u65e8\u5728\u53cd\u6620\u771f\u5be6\u7de8\u78bc\u4efb\u52d9\u4e2d\u5404\u7a2e\u5834\u666f\u7684\u8907\u96dc\u6027\u548c\u591a\u6a23\u6027\u3002NCB \u5305\u542b 402 \u500b\u9ad8\u54c1\u8cea\u7684 Python \u548c Java \u554f\u984c\uff0c\u5f9e\u7dda\u4e0a\u7de8\u78bc\u670d\u52d9\u7684\u81ea\u7136\u4f7f\u7528\u8005\u67e5\u8a62\u4e2d\u4ed4\u7d30\u6311\u9078\uff0c\u6db5\u84cb 6 \u500b\u4e0d\u540c\u7684\u9818\u57df\u3002\u6ce8\u610f\u5230\u70ba\u73fe\u5be6\u4e16\u754c\u7684\u67e5\u8a62\u5efa\u7acb\u6e2c\u8a66\u6848\u4f8b\u7684\u96e3\u5ea6\u6975\u9ad8\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86\u4e00\u500b\u534a\u81ea\u52d5\u5316\u7ba1\u9053\u4f86\u63d0\u9ad8\u6e2c\u8a66\u6848\u4f8b\u5efa\u69cb\u7684\u6548\u7387\u3002\u8207\u624b\u52d5\u89e3\u6c7a\u65b9\u6848\u76f8\u6bd4\uff0c\u5b83\u7684\u6548\u7387\u63d0\u5347\u4e86 4 \u500d\u4ee5\u4e0a\u3002\u6211\u5011\u5c0d 39 \u500b LLM \u9032\u884c\u7684\u7cfb\u7d71\u6027\u5be6\u9a57\u767c\u73fe\uff0c\u5728 HumanEval \u5f97\u5206\u63a5\u8fd1\u7684\u6a21\u578b\u4e4b\u9593\uff0cNCB \u4e0a\u7684\u6548\u80fd\u5dee\u8ddd\u4ecd\u7136\u53ef\u80fd\u5f88\u5927\uff0c\u9019\u8868\u793a\u7f3a\u4e4f\u5c0d\u5be6\u969b\u7a0b\u5f0f\u78bc\u5408\u6210\u5834\u666f\u7684\u95dc\u6ce8\u6216\u5c0d HumanEval \u7684\u904e\u5ea6\u7279\u5b9a\u6700\u4f73\u5316\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5373\u4f7f\u6548\u80fd\u6700\u4f73\u7684 GPT-4 \u5728 NCB \u4e0a\u4ecd\u9060\u672a\u4ee4\u4eba\u6eff\u610f\u3002\u8a55\u4f30\u5de5\u5177\u5305\u548c\u958b\u767c\u7d44\u53ef\u4ee5\u5728 https://github.com/THUDM/NaturalCodeBench \u53d6\u5f97\u3002", "author": "Shudan Zhang et.al.", "authors": "Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong, Jie Tang", "id": "2405.04520v1", "paper_url": "http://arxiv.org/abs/2405.04520v1", "repo": "null"}}