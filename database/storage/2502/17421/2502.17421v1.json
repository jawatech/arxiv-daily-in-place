{"2502.17421": {"publish_time": "2025-02-24", "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and Verification", "paper_summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.", "paper_summary_zh": "\u63a8\u6e2c\u6027\u89e3\u78bc\u5df2\u6210\u70ba\u4e00\u7a2e\u6709\u524d\u9014\u7684\u6280\u8853\uff0c\u7528\u65bc\u6e1b\u8f15\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u81ea\u8ff4\u6b78\u89e3\u78bc\u7684\u9ad8\u63a8\u8ad6\u5ef6\u9072\u3002\n\u5118\u7ba1\u6709\u524d\u666f\uff0c\u63a8\u6e2c\u6027\u89e3\u78bc\u5728 LLM \u4e2d\u7684\u6709\u6548\u61c9\u7528\u4ecd\u9762\u81e8\u4e09\u5927\u6311\u6230\uff1a\u8349\u7a3f\u6a21\u578b\u4e0d\u65b7\u589e\u52a0\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\u3001\u77ed\u8a13\u7df4\u8a9e\u6599\u5eab\u548c\u9577\u8a9e\u5883\u63a8\u8ad6\u4e4b\u9593\u7684\u5206\u5e03\u8f49\u79fb\uff0c\u4ee5\u53ca\u6ce8\u610f\u529b\u5be6\u4f5c\u7684\u4f4e\u6548\u7387\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u900f\u904e\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\u4f86\u589e\u5f37\u63a8\u6e2c\u6027\u89e3\u78bc\u5728\u9577\u8a9e\u5883\u8a2d\u5b9a\u4e2d\u7684\u6548\u80fd\u3002\u9996\u5148\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u8a18\u61b6\u9ad4\u9ad8\u6548\u7684\u8349\u7a3f\u6a21\u578b\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u500b\u5927\u5c0f\u56fa\u5b9a\u7684\u9375\u503c (KV) \u5feb\u53d6\u3002\u5176\u6b21\uff0c\u6211\u5011\u70ba\u77ed\u8a13\u7df4\u8cc7\u6599\u5f15\u5165\u65b0\u7684\u4f4d\u7f6e\u7d22\u5f15\uff0c\u8b93\u5f9e\u77ed\u8a9e\u5883\u8a13\u7df4\u5230\u9577\u8a9e\u5883\u63a8\u8ad6\u7684\u7121\u7e2b\u9069\u61c9\u6210\u70ba\u53ef\u80fd\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5275\u65b0\u7684\u6ce8\u610f\u529b\u805a\u5408\u65b9\u6cd5\uff0c\u7d50\u5408\u7528\u65bc\u524d\u7db4\u8a08\u7b97\u7684\u5feb\u901f\u5be6\u4f5c\u8207\u7528\u65bc\u6a39\u72c0\u906e\u7f69\u8655\u7406\u7684\u6a19\u6e96\u6ce8\u610f\u529b\uff0c\u6709\u6548\u89e3\u6c7a\u6a39\u72c0\u89e3\u78bc\u7684\u5ef6\u9072\u548c\u8a18\u61b6\u9ad4\u4f4e\u6548\u7387\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\u5404\u7a2e\u9577\u8a9e\u5883\u4efb\u52d9\u4e2d\u7372\u5f97\u5f37\u52c1\u7684\u7d50\u679c\uff0c\u5305\u62ec\u5132\u5b58\u5eab\u5c64\u7d1a\u7684\u7a0b\u5f0f\u78bc\u5b8c\u6210\u3001\u9577\u8a9e\u5883\u6458\u8981\uff0c\u4ee5\u53ca\u985e\u4f3c O1 \u7684\u9577\u63a8\u7406\u4efb\u52d9\uff0c\u8b49\u660e\u5728\u964d\u4f4e\u5ef6\u9072\u65b9\u9762\u6709\u986f\u8457\u7684\u9032\u6b65\u3002\u7a0b\u5f0f\u78bc\u53ef\u65bc https://github.com/sail-sg/LongSpec \u53d6\u5f97\u3002", "author": "Penghui Yang et.al.", "authors": "Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An", "id": "2502.17421v1", "paper_url": "http://arxiv.org/abs/2502.17421v1", "repo": "null"}}