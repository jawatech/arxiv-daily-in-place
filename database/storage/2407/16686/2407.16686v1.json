{"2407.16686": {"publish_time": "2024-07-23", "title": "Can Large Language Models Automatically Jailbreak GPT-4V?", "paper_summary": "GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity.", "paper_summary_zh": "GPT-4V \u56e0\u5176\u6574\u5408\u548c\u8655\u7406\u591a\u6a21\u614b\u8cc7\u8a0a\u7684\u975e\u51e1\u80fd\u529b\u800c\u5099\u53d7\u95dc\u6ce8\u3002\u540c\u6642\uff0c\u5176\u4eba\u81c9\u8b58\u5225\u80fd\u529b\u4e5f\u5f15\u767c\u4e86\u65b0\u7684\u96b1\u79c1\u6d29\u9732\u5b89\u5168\u554f\u984c\u3002\u5118\u7ba1\u7814\u7a76\u4eba\u54e1\u900f\u904e RLHF \u6216\u9810\u8655\u7406\u904e\u6ffe\u5668\u5728\u5b89\u5168\u8abf\u6574\u65b9\u9762\u505a\u51fa\u4e86\u52aa\u529b\uff0c\u4f46\u6f0f\u6d1e\u4ecd\u53ef\u80fd\u88ab\u5229\u7528\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 AutoJailbreak\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u81ea\u52d5\u8d8a\u7344\u6280\u8853\uff0c\u9748\u611f\u4f86\u81ea\u63d0\u793a\u6700\u4f73\u5316\u3002\u6211\u5011\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9032\u884c\u7d05\u968a\u6f14\u7df4\uff0c\u4ee5\u512a\u5316\u8d8a\u7344\u63d0\u793a\uff0c\u4e26\u4f7f\u7528\u5f31\u5230\u5f37\u7684\u4e0a\u4e0b\u6587\u5b78\u7fd2\u63d0\u793a\u4f86\u63d0\u9ad8\u6548\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7d50\u5408\u4e86\u65e9\u671f\u505c\u6b62\u4ee5\u6700\u5c0f\u5316\u6700\u4f73\u5316\u6642\u9593\u548c\u7b26\u865f\u652f\u51fa\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cAutoJailbreak \u660e\u986f\u512a\u65bc\u50b3\u7d71\u65b9\u6cd5\uff0c\u653b\u64ca\u6210\u529f\u7387 (ASR) \u8d85\u904e 95.3%\u3002\u9019\u9805\u7814\u7a76\u6709\u52a9\u65bc\u52a0\u5f37 GPT-4V \u5b89\u5168\u6027\uff0c\u5f37\u8abf\u4e86 LLM \u5728\u7834\u58de GPT-4V \u5b8c\u6574\u6027\u65b9\u9762\u88ab\u5229\u7528\u7684\u6f5b\u529b\u3002", "author": "Yuanwei Wu et.al.", "authors": "Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun", "id": "2407.16686v1", "paper_url": "http://arxiv.org/abs/2407.16686v1", "repo": "null"}}