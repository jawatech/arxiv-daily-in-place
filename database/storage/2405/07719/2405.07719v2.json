{"2405.07719": {"publish_time": "2024-05-13", "title": "A Unified Sequence Parallelism Approach for Long Context Generative AI", "paper_summary": "Sequence parallelism (SP), which divides the sequence dimension of input\ntensors across multiple computational devices, is becoming key to unlocking the\nlong-context capabilities of generative AI models. This paper investigates the\nstate-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and\nproposes a unified SP approach, which is more robust to transformer model\narchitectures and network hardware topology. This paper compares the\ncommunication and memory cost of SP and existing parallelism, including\ndata/tensor/zero/expert/pipeline parallelism, and discusses the best practices\nfor designing hybrid 4D parallelism involving SP. We achieved 86\\% MFU on two\n8xA800 nodes using SP for sequence length 208K for the LLAMA3-8B model. Our\ncode is publicly available on\n\\url{https://github.com/feifeibear/long-context-attention}.", "paper_summary_zh": "\u5e8f\u5217\u5e73\u884c\uff08SP\uff09\u5c07\u8f38\u5165\u5f35\u91cf\u7684\u5e8f\u5217\u7dad\u5ea6\u5283\u5206\u5230\u591a\u500b\u8a08\u7b97\u88dd\u7f6e\u4e0a\uff0c\u6b63\u6210\u70ba\u89e3\u9396\u751f\u6210\u5f0f AI \u6a21\u578b\u7684\u9577\u8a9e\u5883\u80fd\u529b\u7684\u95dc\u9375\u3002\u672c\u6587\u7814\u7a76\u4e86\u6700\u5148\u9032\u7684 SP \u65b9\u6cd5\uff0c\u5373 DeepSpeed-Ulysses \u548c Ring-Attention\uff0c\u4e26\u63d0\u51fa\u4e86\u4e00\u7a2e\u7d71\u4e00\u7684 SP \u65b9\u6cd5\uff0c\u5b83\u5c0dTransformer\u6a21\u578b\u67b6\u69cb\u548c\u7db2\u8def\u786c\u9ad4\u62d3\u64b2\u66f4\u5177\u9b6f\u68d2\u6027\u3002\u672c\u6587\u6bd4\u8f03\u4e86 SP \u548c\u73fe\u6709\u4e26\u884c\u5316\u7684\u901a\u8a0a\u548c\u8a18\u61b6\u9ad4\u6210\u672c\uff0c\u5305\u62ec\u8cc7\u6599/\u5f35\u91cf/\u96f6/\u5c08\u5bb6/\u7ba1\u7dda\u4e26\u884c\u5316\uff0c\u4e26\u8a0e\u8ad6\u4e86\u6d89\u53ca SP \u7684\u6df7\u5408 4D \u4e26\u884c\u5316\u7684\u6700\u4f73\u5be6\u52d9\u3002\u6211\u5011\u5728\u5169\u500b 8xA800 \u7bc0\u9ede\u4e0a\u4f7f\u7528 SP \u70ba LLAMA3-8B \u6a21\u578b\u7684\u5e8f\u5217\u9577\u5ea6 208K \u9054\u5230\u4e86 86% \u7684 MFU\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u767c\u5e03\u5728\n\\url{https://github.com/feifeibear/long-context-attention}\u3002", "author": "Jiarui Fang et.al.", "authors": "Jiarui Fang, Shangchun Zhao", "id": "2405.07719v2", "paper_url": "http://arxiv.org/abs/2405.07719v2", "repo": "https://github.com/feifeibear/long-context-attention"}}