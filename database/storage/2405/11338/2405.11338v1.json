{"2405.11338": {"publish_time": "2024-05-18", "title": "EyeFound: A Multimodal Generalist Foundation Model for Ophthalmic Imaging", "paper_summary": "Artificial intelligence (AI) is vital in ophthalmology, tackling tasks like\ndiagnosis, classification, and visual question answering (VQA). However,\nexisting AI models in this domain often require extensive annotation and are\ntask-specific, limiting their clinical utility. While recent developments have\nbrought about foundation models for ophthalmology, they are limited by the need\nto train separate weights for each imaging modality, preventing a comprehensive\nrepresentation of multi-modal features. This highlights the need for versatile\nfoundation models capable of handling various tasks and modalities in\nophthalmology. To address this gap, we present EyeFound, a multimodal\nfoundation model for ophthalmic images. Unlike existing models, EyeFound learns\ngeneralizable representations from unlabeled multimodal retinal images,\nenabling efficient model adaptation across multiple applications. Trained on\n2.78 million images from 227 hospitals across 11 ophthalmic modalities,\nEyeFound facilitates generalist representations and diverse multimodal\ndownstream tasks, even for detecting challenging rare diseases. It outperforms\nprevious work RETFound in diagnosing eye diseases, predicting systemic disease\nincidents, and zero-shot multimodal VQA. EyeFound provides a generalizable\nsolution to improve model performance and lessen the annotation burden on\nexperts, facilitating widespread clinical AI applications from retinal imaging.", "paper_summary_zh": "\u4eba\u5de5\u667a\u6167\uff08AI\uff09\u5c0d\u65bc\u773c\u79d1\u81f3\u95dc\u91cd\u8981\uff0c\u5b83\u80fd\u8655\u7406\u8a3a\u65b7\u3001\u5206\u985e\u548c\u8996\u89ba\u554f\u984c\u89e3\u7b54\uff08VQA\uff09\u7b49\u4efb\u52d9\u3002\u7136\u800c\uff0c\u9019\u500b\u9818\u57df\u73fe\u6709\u7684 AI \u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8a3b\u89e3\uff0c\u800c\u4e14\u7279\u5b9a\u65bc\u4efb\u52d9\uff0c\u9019\u9650\u5236\u4e86\u5b83\u5011\u7684\u81e8\u5e8a\u6548\u7528\u3002\u96d6\u7136\u6700\u8fd1\u7684\u767c\u5c55\u70ba\u773c\u79d1\u5e36\u4f86\u4e86\u57fa\u790e\u6a21\u578b\uff0c\u4f46\u5b83\u5011\u53d7\u5230\u70ba\u6bcf\u500b\u5f71\u50cf\u6a21\u5f0f\u8a13\u7df4\u7368\u7acb\u6b0a\u91cd\u7684\u9700\u6c42\u9650\u5236\uff0c\u9019\u963b\u7919\u4e86\u591a\u6a21\u5f0f\u7279\u5fb5\u7684\u5168\u9762\u8868\u793a\u3002\u9019\u51f8\u986f\u4e86\u9700\u8981\u80fd\u5920\u8655\u7406\u773c\u79d1\u4e2d\u5404\u7a2e\u4efb\u52d9\u548c\u6a21\u5f0f\u7684\u591a\u529f\u80fd\u57fa\u790e\u6a21\u578b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e86 EyeFound\uff0c\u4e00\u500b\u7528\u65bc\u773c\u79d1\u5f71\u50cf\u7684\u591a\u6a21\u5f0f\u57fa\u790e\u6a21\u578b\u3002\u8207\u73fe\u6709\u6a21\u578b\u4e0d\u540c\uff0cEyeFound \u5f9e\u672a\u6a19\u8a18\u7684\u591a\u6a21\u5f0f\u8996\u7db2\u819c\u5f71\u50cf\u4e2d\u5b78\u7fd2\u53ef\u6982\u62ec\u7684\u8868\u793a\uff0c\u5f9e\u800c\u80fd\u5920\u8de8\u591a\u500b\u61c9\u7528\u7a0b\u5f0f\u6709\u6548\u5730\u8abf\u6574\u6a21\u578b\u3002EyeFound \u8a13\u7df4\u4e86\u4f86\u81ea 11 \u7a2e\u773c\u79d1\u6a21\u5f0f\u7684 227 \u5bb6\u91ab\u9662\u7684 278 \u842c\u5f35\u5f71\u50cf\uff0c\u5b83\u4fc3\u9032\u4e86\u901a\u624d\u8868\u793a\u548c\u591a\u6a23\u5316\u7684\u591a\u6a21\u5f0f\u4e0b\u6e38\u4efb\u52d9\uff0c\u751a\u81f3\u53ef\u4ee5\u5075\u6e2c\u5177\u6709\u6311\u6230\u6027\u7684\u7f55\u898b\u75be\u75c5\u3002\u5b83\u5728\u8a3a\u65b7\u773c\u75be\u3001\u9810\u6e2c\u5168\u8eab\u75be\u75c5\u4e8b\u4ef6\u548c\u96f6\u6b21\u5b78\u7fd2\u591a\u6a21\u5f0f VQA \u65b9\u9762\u512a\u65bc\u5148\u524d\u7684 RETFound\u3002EyeFound \u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u6982\u62ec\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4ee5\u6539\u5584\u6a21\u578b\u6548\u80fd\u4e26\u6e1b\u8f15\u5c08\u5bb6\u7684\u8a3b\u89e3\u8ca0\u64d4\uff0c\u5f9e\u800c\u4fc3\u9032\u8996\u7db2\u819c\u5f71\u50cf\u7684\u5ee3\u6cdb\u81e8\u5e8a AI \u61c9\u7528\u3002", "author": "Danli Shi et.al.", "authors": "Danli Shi, Weiyi Zhang, Xiaolan Chen, Yexin Liu, Jianchen Yang, Siyu Huang, Yih Chung Tham, Yingfeng Zheng, Mingguang He", "id": "2405.11338v1", "paper_url": "http://arxiv.org/abs/2405.11338v1", "repo": "null"}}