{"2502.15434": {"publish_time": "2025-02-21", "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation", "paper_summary": "Model merging integrates the parameters of multiple models into a unified\nmodel, combining their diverse capabilities. Existing model merging methods are\noften constrained by fixed parameter merging ratios. In this study, we propose\nMixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data\naugmentation technique. This method merges the parameters of two large language\nmodels (LLMs) by randomly generating linear interpolation ratios, allowing for\na more flexible and comprehensive exploration of the parameter space. Extensive\nexperiments demonstrate the superiority of our proposed M$^3$ method in merging\nfine-tuned LLMs: (1) it significantly improves performance across multiple\ntasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and\nadversarial robustness, (3) it achieves superior results when combined with\nsparsification techniques such as DARE, and (4) it offers a simple yet\nefficient solution that does not require additional computational resources. In\nconclusion, M$^3$ is a simple yet effective model merging method that\nsignificantly enhances the performance of the merged model by randomly\ngenerating contribution ratios for two fine-tuned LLMs. The code is available\nat https://github.com/MLGroupJLU/MixupModelMerge.", "paper_summary_zh": "\u6a21\u578b\u5408\u4f75\u5c07\u591a\u500b\u6a21\u578b\u7684\u53c3\u6578\u6574\u5408\u5230\u4e00\u500b\u7d71\u4e00\u7684\u6a21\u578b\u4e2d\uff0c\u7d50\u5408\u5b83\u5011\u591a\u6a23\u5316\u7684\u529f\u80fd\u3002\u73fe\u6709\u7684\u6a21\u578b\u5408\u4f75\u65b9\u6cd5\u901a\u5e38\u53d7\u5230\u56fa\u5b9a\u53c3\u6578\u5408\u4f75\u6bd4\u7387\u7684\u9650\u5236\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Mixup \u6a21\u578b\u5408\u4f75 (M$^3$)\uff0c\u9019\u662f\u4e00\u7a2e\u5275\u65b0\u7684\u65b9\u6cd5\uff0c\u9748\u611f\u4f86\u81ea Mixup \u8cc7\u6599\u64f4\u5145\u6280\u8853\u3002\u6b64\u65b9\u6cd5\u900f\u904e\u96a8\u6a5f\u7522\u751f\u7dda\u6027\u63d2\u503c\u6bd4\u7387\u4f86\u5408\u4f75\u5169\u500b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u53c3\u6578\uff0c\u5141\u8a31\u66f4\u9748\u6d3b\u4e14\u5168\u9762\u5730\u63a2\u7d22\u53c3\u6578\u7a7a\u9593\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684 M$^3$ \u65b9\u6cd5\u5728\u5408\u4f75\u5fae\u8abf LLM \u6642\u7684\u512a\u8d8a\u6027\uff1a(1) \u5b83\u986f\u8457\u6539\u5584\u4e86\u591a\u9805\u4efb\u52d9\u7684\u6548\u80fd\uff0c(2) \u5b83\u589e\u5f37\u4e86 LLM \u7684\u5206\u5e03\u5916 (OOD) \u7a69\u5065\u6027\u548c\u5c0d\u6297\u7a69\u5065\u6027\uff0c(3) \u5b83\u5728\u8207\u7a00\u758f\u5316\u6280\u8853\uff08\u4f8b\u5982 DARE\uff09\u7d50\u5408\u6642\u53d6\u5f97\u4e86\u66f4\u4f73\u7684\u7d50\u679c\uff0c(4) \u5b83\u63d0\u4f9b\u4e86\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4e0d\u9700\u8981\u984d\u5916\u7684\u904b\u7b97\u8cc7\u6e90\u3002\u7e3d\u4e4b\uff0cM$^3$ \u662f\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u6a21\u578b\u5408\u4f75\u65b9\u6cd5\uff0c\u900f\u904e\u96a8\u6a5f\u7522\u751f\u5169\u500b\u5fae\u8abf LLM \u7684\u8ca2\u737b\u6bd4\u7387\uff0c\u986f\u8457\u63d0\u5347\u5408\u4f75\u6a21\u578b\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/MLGroupJLU/MixupModelMerge \u53d6\u5f97\u3002", "author": "Yue Zhou et.al.", "authors": "Yue Zhou, Yi Chang, Yuan Wu", "id": "2502.15434v1", "paper_url": "http://arxiv.org/abs/2502.15434v1", "repo": "null"}}