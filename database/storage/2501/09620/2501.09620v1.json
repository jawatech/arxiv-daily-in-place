{"2501.09620": {"publish_time": "2025-01-16", "title": "Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment", "paper_summary": "Recent advances in large language models (LLMs) have demonstrated significant\nprogress in performing complex tasks. While Reinforcement Learning from Human\nFeedback (RLHF) has been effective in aligning LLMs with human preferences, it\nis susceptible to spurious correlations in reward modeling. Consequently, it\noften introduces biases-such as length bias, sycophancy, conceptual bias, and\ndiscrimination that hinder the model's ability to capture true causal\nrelationships. To address this, we propose a novel causal reward modeling\napproach that integrates causal inference to mitigate these spurious\ncorrelations. Our method enforces counterfactual invariance, ensuring reward\npredictions remain consistent when irrelevant variables are altered. Through\nexperiments on both synthetic and real-world datasets, we show that our\napproach mitigates various types of spurious correlations effectively,\nresulting in more reliable and fair alignment of LLMs with human preferences.\nAs a drop-in enhancement to the existing RLHF workflow, our causal reward\nmodeling provides a practical way to improve the trustworthiness and fairness\nof LLM finetuning.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u8b49\u660e\u5728\u57f7\u884c\u8907\u96dc\u4efb\u52d9\u65b9\u9762\u53d6\u5f97\u986f\u8457\u9032\u5c55\u3002\u96d6\u7136\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5df2\u6709\u6548\u5730\u5c07 LLM \u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u4f46\u5b83\u5bb9\u6613\u53d7\u5230\u734e\u52f5\u5efa\u6a21\u4e2d\u7684\u865b\u5047\u76f8\u95dc\u6027\u5f71\u97ff\u3002\u56e0\u6b64\uff0c\u5b83\u5e38\u5e38\u6703\u5f15\u5165\u504f\u5dee\uff0c\u4f8b\u5982\u9577\u5ea6\u504f\u5dee\u3001\u963f\u8adb\u5949\u627f\u3001\u6982\u5ff5\u504f\u5dee\u548c\u6b67\u8996\uff0c\u9019\u4e9b\u504f\u5dee\u6703\u963b\u7919\u6a21\u578b\u6355\u6349\u771f\u5be6\u56e0\u679c\u95dc\u4fc2\u7684\u80fd\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u56e0\u679c\u734e\u52f5\u5efa\u6a21\u65b9\u6cd5\uff0c\u5b83\u6574\u5408\u4e86\u56e0\u679c\u63a8\u7406\u4f86\u6e1b\u8f15\u9019\u4e9b\u865b\u5047\u76f8\u95dc\u6027\u3002\u6211\u5011\u7684\u6a21\u578b\u5f37\u5236\u57f7\u884c\u53cd\u4e8b\u5be6\u4e0d\u8b8a\u6027\uff0c\u78ba\u4fdd\u5728\u6539\u8b8a\u7121\u95dc\u8b8a\u6578\u6642\u734e\u52f5\u9810\u6e2c\u4fdd\u6301\u4e00\u81f4\u3002\u900f\u904e\u5728\u5408\u6210\u548c\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u6a21\u578b\u6709\u6548\u6e1b\u8f15\u4e86\u5404\u7a2e\u985e\u578b\u7684\u865b\u5047\u76f8\u95dc\u6027\uff0c\u5f9e\u800c\u66f4\u53ef\u9760\u3001\u66f4\u516c\u5e73\u5730\u5c07 LLM \u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u4f5c\u70ba\u5c0d\u73fe\u6709 RLHF \u5de5\u4f5c\u6d41\u7a0b\u7684\u76f4\u63a5\u589e\u5f37\uff0c\u6211\u5011\u7684\u56e0\u679c\u734e\u52f5\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u7a2e\u5be6\u7528\u7684\u65b9\u6cd5\u4f86\u63d0\u5347 LLM \u5fae\u8abf\u7684\u53ef\u4fe1\u5ea6\u548c\u516c\u5e73\u6027\u3002", "author": "Chaoqi Wang et.al.", "authors": "Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang", "id": "2501.09620v1", "paper_url": "http://arxiv.org/abs/2501.09620v1", "repo": "null"}}