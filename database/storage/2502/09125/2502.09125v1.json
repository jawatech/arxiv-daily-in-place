{"2502.09125": {"publish_time": "2025-02-13", "title": "Automatic Pruning via Structured Lasso with Class-wise Information", "paper_summary": "Most pruning methods concentrate on unimportant filters of neural networks.\nHowever, they face the loss of statistical information due to a lack of\nconsideration for class-wise data. In this paper, from the perspective of\nleveraging precise class-wise information for model pruning, we utilize\nstructured lasso with guidance from Information Bottleneck theory. Our approach\nensures that statistical information is retained during the pruning process.\nWith these techniques, we introduce two innovative adaptive network pruning\nschemes: sparse graph-structured lasso pruning with Information Bottleneck\n(\\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information\nBottleneck (\\textbf{sTLP-IB}). The key aspect is pruning model filters using\nsGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to\nmultiple state-of-the-art methods, our approaches demonstrate superior\nperformance across three datasets and six model architectures in extensive\nexperiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we\nachieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain\nan accuracy of 94.10% (0.14% higher than the original model); we reduce the\nparameters by 55% with the accuracy at 76.12% using the ResNet architecture on\nImageNet (only drops 0.03%). In summary, we successfully reduce model size and\ncomputational resource usage while maintaining accuracy. Our codes are at\nhttps://anonymous.4open.science/r/IJCAI-8104.", "paper_summary_zh": "\u5927\u591a\u6578\u526a\u679d\u65b9\u6cd5\u90fd\u96c6\u4e2d\u5728\u795e\u7d93\u7db2\u8def\u4e2d\u4e0d\u91cd\u8981\u7684\u6ffe\u6ce2\u5668\u4e0a\u3002\n\u7136\u800c\uff0c\u7531\u65bc\u7f3a\u4e4f\u5c0d\u985e\u5225\u8cc7\u6599\u7684\u8003\u91cf\uff0c\u5b83\u5011\u9762\u81e8\u7d71\u8a08\u8cc7\u8a0a\u7684\u907a\u5931\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f9e\u5229\u7528\u7cbe\u78ba\u985e\u5225\u8cc7\u8a0a\u9032\u884c\u6a21\u578b\u526a\u679d\u7684\u89d2\u5ea6\uff0c\u5229\u7528\u7d50\u69cb\u5316\u5957\u7d22\u642d\u914d\u8cc7\u8a0a\u74f6\u9838\u7406\u8ad6\u7684\u6307\u5c0e\u3002\u6211\u5011\u7684\u505a\u6cd5\u78ba\u4fdd\u5728\u526a\u679d\u904e\u7a0b\u4e2d\u4fdd\u7559\u7d71\u8a08\u8cc7\u8a0a\u3002\u85c9\u7531\u9019\u4e9b\u6280\u8853\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5169\u500b\u5275\u65b0\u7684\u81ea\u9069\u61c9\u7db2\u8def\u526a\u679d\u65b9\u6848\uff1a\u5e36\u6709\u8cc7\u8a0a\u74f6\u9838\u7684\u7a00\u758f\u5716\u5f62\u7d50\u69cb\u5957\u7d22\u526a\u679d\uff08sGLP-IB\uff09\u548c\u5e36\u6709\u8cc7\u8a0a\u74f6\u9838\u7684\u7a00\u758f\u6a39\u5c0e\u5f15\u5957\u7d22\u526a\u679d\uff08sTLP-IB\uff09\u3002\u95dc\u9375\u65b9\u9762\u662f\u4f7f\u7528 sGLP-IB \u548c sTLP-IB \u526a\u679d\u6a21\u578b\u6ffe\u6ce2\u5668\uff0c\u4ee5\u66f4\u597d\u5730\u64f7\u53d6\u985e\u5225\u95dc\u806f\u6027\u3002\u8207\u591a\u7a2e\u6700\u5148\u9032\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u5ee3\u6cdb\u7684\u5be6\u9a57\u4e2d\u5c55\u73fe\u51fa\u8de8\u4e09\u500b\u8cc7\u6599\u96c6\u548c\u516d\u500b\u6a21\u578b\u67b6\u69cb\u7684\u5353\u8d8a\u6548\u80fd\u3002\u4f8b\u5982\uff0c\u5728 CIFAR-10 \u8cc7\u6599\u96c6\u4e0a\u4f7f\u7528 VGG16 \u6a21\u578b\uff0c\u6211\u5011\u9054\u5230\u4e86 85% \u7684\u53c3\u6578\u6e1b\u5c11\u300161% \u7684 FLOP \u6e1b\u5c11\uff0c\u4e26\u7dad\u6301 94.10% \u7684\u6e96\u78ba\u5ea6\uff08\u6bd4\u539f\u59cb\u6a21\u578b\u9ad8 0.14%\uff09\uff1b\u6211\u5011\u5728 ImageNet \u4e0a\u4f7f\u7528 ResNet \u67b6\u69cb\u5c07\u53c3\u6578\u6e1b\u5c11\u4e86 55%\uff0c\u6e96\u78ba\u5ea6\u70ba 76.12%\uff08\u50c5\u4e0b\u964d 0.03%\uff09\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u6210\u529f\u5730\u6e1b\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u8a08\u7b97\u8cc7\u6e90\u4f7f\u7528\uff0c\u540c\u6642\u7dad\u6301\u6e96\u78ba\u5ea6\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u4f4d\u65bc https://anonymous.4open.science/r/IJCAI-8104\u3002", "author": "Xiang Liu et.al.", "authors": "Xiang Liu, Mingchen Li, Xia Li, Leigang Qu, Zifan Peng, Yijun Song, Zemin Liu, Linshan Jiang, Jialin Li", "id": "2502.09125v1", "paper_url": "http://arxiv.org/abs/2502.09125v1", "repo": "null"}}