{"2501.19201": {"publish_time": "2025-01-31", "title": "Efficient Reasoning with Hidden Thinking", "paper_summary": "Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.", "paper_summary_zh": "\u93c8\u5f0f\u601d\u8003 (CoT) \u63a8\u7406\u5df2\u6210\u70ba\u4e00\u7a2e\u5f37\u5927\u7684\u6846\u67b6\uff0c\u7528\u65bc\u6539\u5584\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4e2d\u7684\u8907\u96dc\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u3002\u7136\u800c\uff0c\u6587\u672c\u63a8\u7406\u7684\u5197\u9577\u6027\u8cea\u6703\u9020\u6210\u986f\u8457\u7684\u4f4e\u6548\u7387\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 $\\textbf{Heima}$\uff08\u4f5c\u70ba\u96b1\u85cf\u7684\u7f8a\u99dd\uff09\uff0c\u9019\u662f\u4e00\u500b\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u5229\u7528\u96b1\u85cf\u6f5b\u5728\u7a7a\u9593\u4e2d\u7684\u63a8\u7406 CoT\u3002\u6211\u5011\u8a2d\u8a08\u4e86 Heima \u7de8\u78bc\u5668\uff0c\u4f7f\u7528\u55ae\u4e00\u7684\u601d\u8003\u6a19\u8a18\u5c07\u6bcf\u500b\u4e2d\u9593 CoT \u58d3\u7e2e\u6210\u4e00\u500b\u7dca\u6e4a\u3001\u66f4\u9ad8\u7d1a\u5225\u7684\u96b1\u85cf\u8868\u793a\uff0c\u6709\u6548\u5730\u6700\u5c0f\u5316\u5197\u9577\u4e26\u6e1b\u5c11\u63a8\u7406\u904e\u7a0b\u4e2d\u6240\u9700\u7684\u7e3d\u6a19\u8a18\u6578\u3002\u540c\u6642\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u5c0d\u61c9\u7684 Heima \u89e3\u78bc\u5668\u8207\u50b3\u7d71\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e00\u8d77\uff0c\u81ea\u9069\u61c9\u5730\u5c07\u96b1\u85cf\u8868\u793a\u89e3\u91cb\u70ba\u53ef\u8b8a\u9577\u5ea6\u7684\u6587\u672c\u5e8f\u5217\uff0c\u91cd\u5efa\u8207\u539f\u59cb CoT \u975e\u5e38\u76f8\u4f3c\u7684\u63a8\u7406\u904e\u7a0b\u3002\u8de8\u4e0d\u540c\u63a8\u7406 MLLM \u57fa\u6e96\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cHeima \u6a21\u578b\u5728\u4fdd\u6301\u6216\u751a\u81f3\u63d0\u9ad8\u96f6\u6b21\u4efb\u52d9\u6e96\u78ba\u6027\u7684\u540c\u6642\uff0c\u5be6\u73fe\u4e86\u66f4\u9ad8\u7684\u751f\u6210\u6548\u7387\u3002\u6b64\u5916\uff0c\u4f7f\u7528 Heima \u89e3\u78bc\u5668\u6709\u6548\u91cd\u5efa\u591a\u6a21\u614b\u63a8\u7406\u904e\u7a0b\u9a57\u8b49\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u9b6f\u68d2\u6027\u548c\u53ef\u89e3\u91cb\u6027\u3002", "author": "Xuan Shen et.al.", "authors": "Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu", "id": "2501.19201v1", "paper_url": "http://arxiv.org/abs/2501.19201v1", "repo": "https://github.com/shawnricecake/heima"}}