
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-02-18**|**SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation**|Zekun Qi et.al.|[2502.13143v1](http://arxiv.org/abs/2502.13143v1)|null|
|**2025-02-18**|**Pre-training Auto-regressive Robotic Models with 4D Representations**|Dantong Niu et.al.|[2502.13142v1](http://arxiv.org/abs/2502.13142v1)|null|
|**2025-02-18**|**UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models**|Huawei Lin et.al.|[2502.13141v1](http://arxiv.org/abs/2502.13141v1)|null|
|**2025-02-18**|**AIDE: AI-Driven Exploration in the Space of Code**|Zhengyao Jiang et.al.|[2502.13138v1](http://arxiv.org/abs/2502.13138v1)|null|
|**2025-02-18**|**Theorem Prover as a Judge for Synthetic Data Generation**|Joshua Ong Jun Leang et.al.|[2502.13137v1](http://arxiv.org/abs/2502.13137v1)|null|
|**2025-02-18**|**Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions**|Taedong Yun et.al.|[2502.13135v1](http://arxiv.org/abs/2502.13135v1)|null|
|**2025-02-18**|**Learning to Defer for Causal Discovery with Imperfect Experts**|Oscar Clivio et.al.|[2502.13132v1](http://arxiv.org/abs/2502.13132v1)|null|
|**2025-02-18**|**Rethinking Diverse Human Preference Learning through Principal Component Analysis**|Feng Luo et.al.|[2502.13131v1](http://arxiv.org/abs/2502.13131v1)|null|
|**2025-02-18**|**Magma: A Foundation Model for Multimodal AI Agents**|Jianwei Yang et.al.|[2502.13130v1](http://arxiv.org/abs/2502.13130v1)|null|
|**2025-02-18**|**SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation**|Zihan Liu et.al.|[2502.13128v1](http://arxiv.org/abs/2502.13128v1)|null|
|**2025-02-18**|**Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning**|Jingyang Lin et.al.|[2502.13127v1](http://arxiv.org/abs/2502.13127v1)|null|
|**2025-02-18**|**RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises**|Zenan Zhai et.al.|[2502.13125v1](http://arxiv.org/abs/2502.13125v1)|null|
|**2025-02-18**|**NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions**|Weizhe Yuan et.al.|[2502.13124v1](http://arxiv.org/abs/2502.13124v1)|null|
|**2025-02-18**|**Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context**|Marion Bartl et.al.|[2502.13120v1](http://arxiv.org/abs/2502.13120v1)|null|
|**2025-02-18**|**STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models**|Narun Raman et.al.|[2502.13119v1](http://arxiv.org/abs/2502.13119v1)|null|
|**2025-02-18**|**Performance Evaluation of Large Language Models in Statistical Programming**|Xinyi Song et.al.|[2502.13117v1](http://arxiv.org/abs/2502.13117v1)|null|
|**2025-02-18**|**Near-Optimal Private Learning in Linear Contextual Bandits**|Fan Chen et.al.|[2502.13115v1](http://arxiv.org/abs/2502.13115v1)|null|
|**2025-02-18**|**The influence of motion features in temporal perception**|Rosa Illan Castillo et.al.|[2502.13114v1](http://arxiv.org/abs/2502.13114v1)|null|
|**2025-02-18**|**Improving Clinical Question Answering with Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization**|Priyaranjan Pattnayak et.al.|[2502.13108v1](http://arxiv.org/abs/2502.13108v1)|null|
|**2025-02-18**|**MatterChat: A Multi-Modal LLM for Material Science**|Yingheng Tang et.al.|[2502.13107v1](http://arxiv.org/abs/2502.13107v1)|null|
|**2025-02-18**|**Understanding and Rectifying Safety Perception Distortion in VLMs**|Xiaohan Zou et.al.|[2502.13095v1](http://arxiv.org/abs/2502.13095v1)|null|
|**2025-02-18**|**Text2World: Benchmarking Large Language Models for Symbolic World Model Generation**|Mengkang Hu et.al.|[2502.13092v1](http://arxiv.org/abs/2502.13092v1)|null|
|**2025-02-18**|**KAPPA: A Generic Patent Analysis Framework with Keyphrase-Based Portraits**|Xin Xia et.al.|[2502.13076v1](http://arxiv.org/abs/2502.13076v1)|null|
|**2025-02-18**|**Interactive Agents to Overcome Ambiguity in Software Engineering**|Sanidhya Vijayvargiya et.al.|[2502.13069v1](http://arxiv.org/abs/2502.13069v1)|null|
|**2025-02-18**|**Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity**|Yuri Kuratov et.al.|[2502.13063v1](http://arxiv.org/abs/2502.13063v1)|null|
|**2025-02-18**|**AI-Assisted Decision Making with Human Learning**|Gali Noti et.al.|[2502.13062v1](http://arxiv.org/abs/2502.13062v1)|null|
|**2025-02-18**|**Improved Fine-Tuning of Large Multimodal Models for Hateful Meme Detection**|Jingbiao Mei et.al.|[2502.13061v1](http://arxiv.org/abs/2502.13061v1)|null|
|**2025-02-18**|**SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models**|Xianfu Cheng et.al.|[2502.13059v1](http://arxiv.org/abs/2502.13059v1)|null|
|**2025-02-18**|**LAMD: Context-driven Android Malware Detection and Classification with LLMs**|Xingzhi Qian et.al.|[2502.13055v1](http://arxiv.org/abs/2502.13055v1)|null|
|**2025-02-18**|**Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction**|Nils Constantin Hellwig et.al.|[2502.13044v1](http://arxiv.org/abs/2502.13044v1)|null|
|**2025-02-18**|**Natural Language Generation from Visual Sequences: Challenges and Future Directions**|Aditya K Surikuchi et.al.|[2502.13034v1](http://arxiv.org/abs/2502.13034v1)|null|
|**2025-02-18**|**HPSS: Heuristic Prompting Strategy Search for LLM Evaluators**|Bosi Wen et.al.|[2502.13031v1](http://arxiv.org/abs/2502.13031v1)|null|
|**2025-02-18**|**Whose story is it? Personalizing story generation by inferring author styles**|Nischal Ashok Kumar et.al.|[2502.13028v1](http://arxiv.org/abs/2502.13028v1)|null|
|**2025-02-18**|**Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks**|Markus J. Buehler et.al.|[2502.13025v1](http://arxiv.org/abs/2502.13025v1)|null|
|**2025-02-18**|**Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation**|Sha Li et.al.|[2502.13019v1](http://arxiv.org/abs/2502.13019v1)|null|
|**2025-02-18**|**LLM-Powered Proactive Data Systems**|Sepanta Zeighami et.al.|[2502.13016v1](http://arxiv.org/abs/2502.13016v1)|null|
|**2025-02-18**|**Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents**|Chaoran Chen et.al.|[2502.13012v1](http://arxiv.org/abs/2502.13012v1)|null|
|**2025-02-18**|**Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge**|Mohammad Reza Rezaei et.al.|[2502.13010v1](http://arxiv.org/abs/2502.13010v1)|null|
|**2025-02-18**|**Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks**|Yarin Benyamin et.al.|[2502.13006v1](http://arxiv.org/abs/2502.13006v1)|null|
|**2025-02-18**|**Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer Architectures for Speech Quality Estimation**|Wafaa Wardah et.al.|[2502.13004v1](http://arxiv.org/abs/2502.13004v1)|null|
|**2025-02-18**|**You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations**|Frederic Kirstein et.al.|[2502.13001v1](http://arxiv.org/abs/2502.13001v1)|null|
|**2025-02-18**|**Personalized Top-k Set Queries Over Predicted Scores**|Sohrab Namazi Nia et.al.|[2502.12998v1](http://arxiv.org/abs/2502.12998v1)|null|
|**2025-02-18**|**Eager Updates For Overlapped Communication and Computation in DiLoCo**|Satyen Kale et.al.|[2502.12996v1](http://arxiv.org/abs/2502.12996v1)|null|
|**2025-02-18**|**Free Argumentative Exchanges for Explaining Image Classifiers**|Avinash Kori et.al.|[2502.12995v1](http://arxiv.org/abs/2502.12995v1)|null|
|**2025-02-18**|**B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability**|Yifan Wang et.al.|[2502.12992v1](http://arxiv.org/abs/2502.12992v1)|null|
|**2025-02-18**|**Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs**|Zixiao Wang et.al.|[2502.12988v1](http://arxiv.org/abs/2502.12988v1)|null|
|**2025-02-18**|**PartSDF: Part-Based Implicit Neural Representation for Composite 3D Shape Parametrization and Optimization**|Nicolas Talabot et.al.|[2502.12985v1](http://arxiv.org/abs/2502.12985v1)|null|
|**2025-02-18**|**Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs**|Longxu Dou et.al.|[2502.12982v1](http://arxiv.org/abs/2502.12982v1)|null|
|**2025-02-18**|**Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking**|Junda Zhu et.al.|[2502.12970v1](http://arxiv.org/abs/2502.12970v1)|null|
|**2025-02-18**|**A Survey of Text Classification Under Class Distribution Shift**|Adriana Valentina Costache et.al.|[2502.12965v1](http://arxiv.org/abs/2502.12965v1)|null|
|**2025-02-18**|**Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs**|Adi Simhi et.al.|[2502.12964v1](http://arxiv.org/abs/2502.12964v1)|null|
|**2025-02-18**|**Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing**|Xiaoju Ye et.al.|[2502.12962v1](http://arxiv.org/abs/2502.12962v1)|null|
|**2025-02-18**|**Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger**|Wenjun Li et.al.|[2502.12961v1](http://arxiv.org/abs/2502.12961v1)|null|
|**2025-02-18**|**AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages**|Steve Bakos et.al.|[2502.12959v1](http://arxiv.org/abs/2502.12959v1)|null|
|**2025-02-18**|**Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text**|Andrei Jarca et.al.|[2502.12953v1](http://arxiv.org/abs/2502.12953v1)|null|
|**2025-02-18**|**Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for Improved Text-Based Learning for LGE Detection**|Athira J Jacob et.al.|[2502.12948v1](http://arxiv.org/abs/2502.12948v1)|null|
|**2025-02-18**|**Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models**|Gyeongman Kim et.al.|[2502.12947v1](http://arxiv.org/abs/2502.12947v1)|null|
|**2025-02-18**|**LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation**|Junchen Fu et.al.|[2502.12945v1](http://arxiv.org/abs/2502.12945v1)|null|
|**2025-02-18**|**Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning in Low-Resource Languages**|Salsabila Zahirah Pranida et.al.|[2502.12932v1](http://arxiv.org/abs/2502.12932v1)|null|
|**2025-02-18**|**Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options**|Lakshmi Nair et.al.|[2502.12929v1](http://arxiv.org/abs/2502.12929v1)|null|
|**2025-02-18**|**Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts**|Leiyu Pan et.al.|[2502.12928v1](http://arxiv.org/abs/2502.12928v1)|null|
|**2025-02-18**|**SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems**|Mike Zhang et.al.|[2502.12927v1](http://arxiv.org/abs/2502.12927v1)|null|
|**2025-02-18**|**Towards more Contextual Agents: An extractor-Generator Optimization Framework**|Mourad Aouini et.al.|[2502.12926v1](http://arxiv.org/abs/2502.12926v1)|null|
|**2025-02-18**|**Keep what you need : extracting efficient subnetworks from large audio representation models**|David Genova et.al.|[2502.12925v1](http://arxiv.org/abs/2502.12925v1)|null|
|**2025-02-18**|**Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded in Naturally Occurring Data**|Maite Heredia et.al.|[2502.12924v1](http://arxiv.org/abs/2502.12924v1)|null|
|**2025-02-18**|**On-Device LLMs for Home Assistant: Dual Role in Intent Detection and Response Generation**|Rune Birkmose et.al.|[2502.12923v1](http://arxiv.org/abs/2502.12923v1)|null|
|**2025-02-18**|**Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison**|George-Kirollos Saad et.al.|[2502.12921v1](http://arxiv.org/abs/2502.12921v1)|null|
|**2025-02-18**|**GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning**|Sifan Zhou et.al.|[2502.12913v1](http://arxiv.org/abs/2502.12913v1)|null|
|**2025-02-18**|**Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation**|Zheng Yuan et.al.|[2502.12911v1](http://arxiv.org/abs/2502.12911v1)|null|
|**2025-02-18**|**Graph Neural Networks for Databases: A Survey**|Ziming Li et.al.|[2502.12908v1](http://arxiv.org/abs/2502.12908v1)|null|
|**2025-02-18**|**Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements**|Shu Yang et.al.|[2502.12904v1](http://arxiv.org/abs/2502.12904v1)|null|
|**2025-02-18**|**Soundwave: Less is More for Speech-Text Alignment in LLMs**|Yuhao Zhang et.al.|[2502.12900v1](http://arxiv.org/abs/2502.12900v1)|null|
|**2025-02-18**|**None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks**|Eva Sánchez Salido et.al.|[2502.12896v1](http://arxiv.org/abs/2502.12896v1)|null|
|**2025-02-18**|**Multilingual European Language Models: Benchmarking Approaches and Challenges**|Fabio Barth et.al.|[2502.12895v1](http://arxiv.org/abs/2502.12895v1)|null|
|**2025-02-18**|**H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking**|Martin Kuo et.al.|[2502.12893v1](http://arxiv.org/abs/2502.12893v1)|null|
|**2025-02-18**|**Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?**|Georg Rehm et.al.|[2502.12886v1](http://arxiv.org/abs/2502.12886v1)|null|
|**2025-02-18**|**How desirable is alignment between LLMs and linguistically diverse human users?**|Pia Knoeferle et.al.|[2502.12884v1](http://arxiv.org/abs/2502.12884v1)|null|
|**2025-02-18**|**Continuous Learning Conversational AI: A Personalized Agent Framework via A2C Reinforcement Learning**|Nandakishor M et.al.|[2502.12876v1](http://arxiv.org/abs/2502.12876v1)|null|
|**2025-02-18**|**PAFT: Prompt-Agnostic Fine-Tuning**|Chenxing Wei et.al.|[2502.12859v1](http://arxiv.org/abs/2502.12859v1)|null|
|**2025-02-18**|**Rejected Dialects: Biases Against African American Language in Reward Models**|Joel Mire et.al.|[2502.12858v1](http://arxiv.org/abs/2502.12858v1)|null|
|**2025-02-18**|**Integrating Arithmetic Learning Improves Mathematical Reasoning in Smaller Models**|Neeraj Gangwar et.al.|[2502.12855v1](http://arxiv.org/abs/2502.12855v1)|null|
|**2025-02-18**|**S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning**|Ruotian Ma et.al.|[2502.12853v1](http://arxiv.org/abs/2502.12853v1)|null|
|**2025-02-18**|**MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching**|Fabian David Schmidt et.al.|[2502.12852v1](http://arxiv.org/abs/2502.12852v1)|null|
|**2025-02-18**|**MeMo: Towards Language Models with Associative Memory Mechanisms**|Fabio Massimo Zanzotto et.al.|[2502.12851v1](http://arxiv.org/abs/2502.12851v1)|null|
|**2025-02-18**|**Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols**|Kathrin Seßler et.al.|[2502.12842v1](http://arxiv.org/abs/2502.12842v1)|null|
|**2025-02-18**|**Towards Equitable AI: Detecting Bias in Using Large Language Models for Marketing**|Berk Yilmaz et.al.|[2502.12838v1](http://arxiv.org/abs/2502.12838v1)|null|
|**2025-02-18**|**An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation**|Mohammad Feli et.al.|[2502.12836v1](http://arxiv.org/abs/2502.12836v1)|null|
|**2025-02-18**|**Subword models struggle with word learning, but surprisal hides it**|Bastian Bunzeck et.al.|[2502.12835v1](http://arxiv.org/abs/2502.12835v1)|null|
|**2025-02-18**|**KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan**|Mukhammed Togmanov et.al.|[2502.12829v1](http://arxiv.org/abs/2502.12829v1)|null|
|**2025-02-18**|**Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models**|Rubing Lu et.al.|[2502.12825v1](http://arxiv.org/abs/2502.12825v1)|null|
|**2025-02-18**|**Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models**|Elena Stringli et.al.|[2502.12821v1](http://arxiv.org/abs/2502.12821v1)|null|
|**2025-02-18**|**Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models**|Adnan Ahmad et.al.|[2502.12813v1](http://arxiv.org/abs/2502.12813v1)|null|
|**2025-02-18**|**Towards Text-Image Interleaved Retrieval**|Xin Zhang et.al.|[2502.12799v1](http://arxiv.org/abs/2502.12799v1)|null|
|**2025-02-18**|**Envious Explore and Exploit**|Omer Ben-Porat et.al.|[2502.12798v1](http://arxiv.org/abs/2502.12798v1)|null|
|**2025-02-18**|**Commonsense Reasoning in Arab Culture**|Abdelrahman Sadallah et.al.|[2502.12788v1](http://arxiv.org/abs/2502.12788v1)|null|
|**2025-02-18**|**VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation**|Xinlong Chen et.al.|[2502.12782v1](http://arxiv.org/abs/2502.12782v1)|null|
|**2025-02-18**|**Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models**|Daiki Chijiwa et.al.|[2502.12776v1](http://arxiv.org/abs/2502.12776v1)|null|
|**2025-02-18**|**Mind the Gap: Aligning the Brain with Language Models Requires a Nonlinear and Multimodal Approach**|Danny Dongyeop Han et.al.|[2502.12771v1](http://arxiv.org/abs/2502.12771v1)|null|
|**2025-02-18**|**How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild**|Saad Obaid ul Islam et.al.|[2502.12769v1](http://arxiv.org/abs/2502.12769v1)|null|
|**2025-02-18**|**R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs**|Sumin Jo et.al.|[2502.12767v1](http://arxiv.org/abs/2502.12767v1)|null|

#### Abstracts
##### **SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation**
2502.13143v1 by Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma, Zhizheng Zhang, He Wang, Li Yi

Spatial intelligence is a critical component of embodied AI, promoting robots
to understand and interact with their environments. While recent advances have
enhanced the ability of VLMs to perceive object locations and positional
relationships, they still lack the capability to precisely understand object
orientations-a key requirement for tasks involving fine-grained manipulations.
Addressing this limitation not only requires geometric reasoning but also an
expressive and intuitive way to represent orientation. In this context, we
propose that natural language offers a more flexible representation space than
canonical frames, making it particularly suitable for instruction-following
robotic systems. In this paper, we introduce the concept of semantic
orientation, which defines object orientations using natural language in a
reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the
''handle'' direction of a knife). To support this, we construct OrienText300K,
a large-scale dataset of 3D models annotated with semantic orientations that
link geometric understanding to functional semantics. By integrating semantic
orientation into a VLM system, we enable robots to generate manipulation
actions with both positional and orientational constraints. Extensive
experiments in simulation and real world demonstrate that our approach
significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy
on Open6DOR and 74.9% accuracy on SIMPLER.

摘要：空間智能是具象 AI 的關鍵組成部分，促使機器人了解其環境並與之互動。雖然最近的進展增強了 VLM 感知物件位置和位置關係的能力，但它們仍然缺乏精確理解物件方向的能力，這對於涉及細微操作的任務來說是一項關鍵要求。解決這個限制不僅需要幾何推理，還需要一種表達性和直觀的方式來表示方向。在此背景下，我們提出自然語言提供了一個比標準框架更靈活的表示空間，使其特別適合於遵循指令的機器人系統。在本文中，我們介紹了語義方向的概念，它使用自然語言以無參考框架的方式定義物件方向（例如，USB 的「插入」方向或刀子的「握柄」方向）。為了支持這一點，我們構建了 OrienText300K，這是一個大型 3D 模型數據集，其中註釋了語義方向，將幾何理解與功能語義聯繫起來。通過將語義方向整合到 VLM 系統中，我們使機器人能夠生成同時具有位置和方向約束的操作動作。在模擬和現實世界中進行的廣泛實驗表明，我們的做法顯著增強了機器人的操作能力，例如，Open6DOR 的準確率為 48.7%，SIMPLER 的準確率為 74.9%。

##### **Pre-training Auto-regressive Robotic Models with 4D Representations**
2502.13142v1 by Dantong Niu, Yuvan Sharma, Haoru Xue, Giscard Biamby, Junyi Zhang, Ziteng Ji, Trevor Darrell, Roei Herzig

Foundation models pre-trained on massive unlabeled datasets have
revolutionized natural language and computer vision, exhibiting remarkable
generalization capabilities, thus highlighting the importance of pre-training.
Yet, efforts in robotics have struggled to achieve similar success, limited by
either the need for costly robotic annotations or the lack of representations
that effectively model the physical world. In this paper, we introduce ARM4R,
an Auto-regressive Robotic Model that leverages low-level 4D Representations
learned from human video data to yield a better pre-trained robotic model.
Specifically, we focus on utilizing 3D point tracking representations from
videos derived by lifting 2D representations into 3D space via monocular depth
estimation across time. These 4D representations maintain a shared geometric
structure between the points and robot state representations up to a linear
transformation, enabling efficient transfer learning from human video data to
low-level robotic control. Our experiments show that ARM4R can transfer
efficiently from human video data to robotics and consistently improves
performance on tasks across various robot environments and configurations.

摘要：預先在大量未標記資料集上訓練好的基礎模型已經徹底改變了自然語言和電腦視覺，展現出非凡的概化能力，因此突顯了預先訓練的重要性。然而，機器人領域的努力一直難以取得類似的成功，受到昂貴的機器人標註需求或缺乏有效建模物理世界的表徵的限制。在本文中，我們介紹了 ARM4R，一種自迴歸機器人模型，它利用從人類影片資料中學習到的低階 4D 表徵，以產生更好的預先訓練機器人模型。具體來說，我們專注於利用從影片中獲得的 3D 點追蹤表徵，這些表徵是透過單眼深度估計跨時間將 2D 表徵提升到 3D 空間而導出的。這些 4D 表徵在點和機器人狀態表徵之間保持一個共用的幾何結構，直到一個線性轉換，這使得從人類影片資料到低階機器人控制的有效遷移學習成為可能。我們的實驗表明，ARM4R 可以有效地從人類影片資料轉移到機器人技術，並持續改善各種機器人環境和組態中的任務效能。

##### **UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models**
2502.13141v1 by Huawei Lin, Yingjie Lao, Tong Geng, Tan Yu, Weijie Zhao

Large Language Models (LLMs) are vulnerable to attacks like prompt injection,
backdoor attacks, and adversarial attacks, which manipulate prompts or models
to generate harmful outputs. In this paper, departing from traditional deep
learning attack paradigms, we explore their intrinsic relationship and
collectively term them Prompt Trigger Attacks (PTA). This raises a key
question: Can we determine if a prompt is benign or poisoned? To address this,
we propose UniGuardian, the first unified defense mechanism designed to detect
prompt injection, backdoor attacks, and adversarial attacks in LLMs.
Additionally, we introduce a single-forward strategy to optimize the detection
pipeline, enabling simultaneous attack detection and text generation within a
single forward pass. Our experiments confirm that UniGuardian accurately and
efficiently identifies malicious prompts in LLMs.

摘要：大型語言模型 (LLM) 容易受到提示注入、後門攻擊和對抗性攻擊等攻擊，這些攻擊會操縱提示或模型以產生有害的輸出。在本文中，我們跳脫傳統深度學習攻擊範例，探討它們的內在關係，並將它們統稱為提示觸發攻擊 (PTA)。這引發了一個關鍵問題：我們能確定一個提示是良性的還是惡意的嗎？為了解決這個問題，我們提出了 UniGuardian，這是一種旨在偵測 LLM 中的提示注入、後門攻擊和對抗性攻擊的第一個統一防禦機制。此外，我們引入了一個單一前向策略來最佳化偵測管道，在單一前向傳遞中同時進行攻擊偵測和文字生成。我們的實驗證實，UniGuardian 能準確且有效地識別 LLM 中的惡意提示。

##### **AIDE: AI-Driven Exploration in the Space of Code**
2502.13138v1 by Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu

Machine learning, the foundation of modern artificial intelligence, has
driven innovations that have fundamentally transformed the world. Yet, behind
advancements lies a complex and often tedious process requiring labor and
compute intensive iteration and experimentation. Engineers and scientists
developing machine learning models spend much of their time on trial-and-error
tasks instead of conceptualizing innovative solutions or research hypotheses.
To address this challenge, we introduce AI-Driven Exploration (AIDE), a machine
learning engineering agent powered by large language models (LLMs). AIDE frames
machine learning engineering as a code optimization problem, and formulates
trial-and-error as a tree search in the space of potential solutions. By
strategically reusing and refining promising solutions, AIDE effectively trades
computational resources for enhanced performance, achieving state-of-the-art
results on multiple machine learning engineering benchmarks, including our
Kaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.

摘要：機器學習，現代人工智慧的基礎，已經推動了根本性地改變世界的創新。然而，進步的背後是一個複雜且經常繁瑣的過程，需要人工和計算密集的迭代和實驗。開發機器學習模型的工程師和科學家將大部分時間花在試錯任務上，而不是構思創新的解決方案或研究假設。為了應對這一挑戰，我們引入了 AI 驅動探索 (AIDE)，這是一種由大型語言模型 (LLM) 驅動的機器學習工程代理。AIDE 將機器學習工程構建為一個程式碼最佳化問題，並將試錯表述為在潛在解決方案空間中的樹狀搜尋。透過策略性地重複使用和改進有希望的解決方案，AIDE 有效地將計算資源轉換為增強的效能，在多個機器學習工程基準上取得了最先進的成果，包括我們的 Kaggle 評估、OpenAI MLE-Bench 和 METRs RE-Bench。

##### **Theorem Prover as a Judge for Synthetic Data Generation**
2502.13137v1 by Joshua Ong Jun Leang, Giwon Hong, Wenda Li, Shay B. Cohen

The demand for synthetic data in mathematical reasoning has increased due to
its potential to enhance the mathematical capabilities of large language models
(LLMs). However, ensuring the validity of intermediate reasoning steps remains
a significant challenge, affecting data quality. While formal verification via
theorem provers effectively validates LLM reasoning, the autoformalisation of
mathematical proofs remains error-prone. In response, we introduce iterative
autoformalisation, an approach that iteratively refines theorem prover
formalisation to mitigate errors, thereby increasing the execution rate on the
Lean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as
a Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to
rigorously assess LLM intermediate reasoning, effectively integrating
autoformalisation with synthetic data generation. Finally, we present
Reinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that
replaces human annotation with theorem prover feedback in Reinforcement
Learning from Human Feedback (RLHF). Across multiple LLMs, applying
TP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving
5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for
SVAMP, and 3.55% on Llama-3.1-8B for AQUA.

摘要：<paragraph>由於合成資料在數學推理中具有增強大型語言模型 (LLM) 數學能力的潛力，對合成資料的需求已增加。然而，確保中間推理步驟的有效性仍然是一項重大的挑戰，影響資料品質。雖然透過定理證明器進行形式驗證可有效驗證 LLM 推理，但數學證明自動形式化仍然容易出錯。為了解決這個問題，我們引入了迭代自動形式化，這是一種迭代優化定理證明器形式化以減少錯誤的方法，從而將 Lean 證明器的執行率從 60% 提高到 87%。在此基礎上，我們引入了定理證明器作為評審 (TP-as-a-Judge)，這是一種採用定理證明器形式化來嚴格評估 LLM 中間推理的方法，有效地將自動形式化與合成資料產生整合。最後，我們提出了定理證明器回饋強化學習 (RLTPF)，這是一個框架，用定理證明器回饋取代人類標註，以進行人類回饋強化學習 (RLHF)。在多個 LLM 中，應用 TP-as-a-Judge 和 RLTPF 可透過僅 3,508 個樣本改善基準，在 MultiArith 上獲得 5.56% 的準確度提升，在 SVAMP 上獲得 Llama-2-7B 的 6.00% 提升，在 AQUA 上獲得 Llama-3.1-8B 的 3.55% 提升。</paragraph>

##### **Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions**
2502.13135v1 by Taedong Yun, Eric Yang, Mustafa Safdari, Jong Ha Lee, Vaishnavi Vinod Kumar, S. Sara Mahdavi, Jonathan Amar, Derek Peyton, Reut Aharony, Andreas Michaelides, Logan Schneider, Isaac Galatzer-Levy, Yugang Jia, John Canny, Arthur Gretton, Maja Matarić

We present an end-to-end framework for generating synthetic users for
evaluating interactive agents designed to encourage positive behavior changes,
such as in health and lifestyle coaching. The synthetic users are grounded in
health and lifestyle conditions, specifically sleep and diabetes management in
this study, to ensure realistic interactions with the health coaching agent.
Synthetic users are created in two stages: first, structured data are generated
grounded in real-world health and lifestyle factors in addition to basic
demographics and behavioral attributes; second, full profiles of the synthetic
users are developed conditioned on the structured data. Interactions between
synthetic users and the coaching agent are simulated using generative
agent-based models such as Concordia, or directly by prompting a language
model. Using two independently-developed agents for sleep and diabetes coaching
as case studies, the validity of this framework is demonstrated by analyzing
the coaching agent's understanding of the synthetic users' needs and
challenges. Finally, through multiple blinded evaluations of user-coach
interactions by human experts, we demonstrate that our synthetic users with
health and behavioral attributes more accurately portray real human users with
the same attributes, compared to generic synthetic users not grounded in such
attributes. The proposed framework lays the foundation for efficient
development of conversational agents through extensive, realistic, and grounded
simulated interactions.

摘要：<paragraph>我們提供了一個端到端的架構，用於為評估互動式代理生成合成使用者，這些代理旨在鼓勵正向行為改變，例如健康和生活方式指導。合成使用者以健康和生活方式狀況為基礎，特別是本研究中的睡眠和糖尿病管理，以確保與健康指導代理的互動具有真實性。合成使用者分兩個階段建立：首先，除了基本人口統計資料和行為屬性外，還會產生以現實世界的健康和生活方式因素為基礎的結構化資料；其次，會根據結構化資料開發合成使用者的完整個人資料。合成使用者和指導代理之間的互動是使用生成式基於代理的模型（例如 Concordia）模擬的，或者直接通過提示語言模型來模擬。使用兩個獨立開發的睡眠和糖尿病指導代理作為案例研究，通過分析指導代理對合成使用者需求和挑戰的理解，證明了此架構的有效性。最後，通過人類專家對使用者指導互動進行多重盲測評估，我們證明了與未以這些屬性為基礎的通用合成使用者相比，具有健康和行為屬性的合成使用者更準確地描繪了具有相同屬性的真實人類使用者。所提出的架構為通過廣泛、真實且有根據的模擬互動，為對話代理的有效開發奠定了基礎。</paragraph>

##### **Learning to Defer for Causal Discovery with Imperfect Experts**
2502.13132v1 by Oscar Clivio, Divyat Mahajan, Perouz Taslakian, Sara Magliacane, Ioannis Mitliagkas, Valentina Zantedeschi, Alexandre Drouin

Integrating expert knowledge, e.g. from large language models, into causal
discovery algorithms can be challenging when the knowledge is not guaranteed to
be correct. Expert recommendations may contradict data-driven results, and
their reliability can vary significantly depending on the domain or specific
query. Existing methods based on soft constraints or inconsistencies in
predicted causal relationships fail to account for these variations in
expertise. To remedy this, we propose L2D-CD, a method for gauging the
correctness of expert recommendations and optimally combining them with
data-driven causal discovery results. By adapting learning-to-defer (L2D)
algorithms for pairwise causal discovery (CD), we learn a deferral function
that selects whether to rely on classical causal discovery methods using
numerical data or expert recommendations based on textual meta-data. We
evaluate L2D-CD on the canonical T\"ubingen pairs dataset and demonstrate its
superior performance compared to both the causal discovery method and the
expert used in isolation. Moreover, our approach identifies domains where the
expert's performance is strong or weak. Finally, we outline a strategy for
generalizing this approach to causal discovery on graphs with more than two
variables, paving the way for further research in this area.

摘要：整合专家知識，例如從大型語言模型中整合到因果發現演算法中，當知識無法保證正確時會很有挑戰性。專家建議可能會與資料驅動的結果相矛盾，而且他們的可靠性可能會根據領域或特定查詢而有顯著差異。現有的基於軟約束或預測因果關係中不一致的方法無法說明專業知識中的這些變化。為了補救這一點，我們提出了 L2D-CD，一種用於評估專家建議的正確性並將其與資料驅動的因果發現結果最佳結合的方法。透過調整學習延遲 (L2D) 演算法以進行成對因果發現 (CD)，我們學習了一個延遲函數，用於選擇依賴使用數值資料的傳統因果發現方法或基於文字元資料的專家建議。我們在經典的 T\"ubingen 對資料集上評估 L2D-CD，並證明其與單獨使用的因果發現方法和專家相比具有優越的效能。此外，我們的做法識別出專家表現強或弱的領域。最後，我們概述了一種將此方法推廣到具有兩個以上變數的圖表上進行因果發現的策略，為此領域的進一步研究鋪平了道路。

##### **Rethinking Diverse Human Preference Learning through Principal Component Analysis**
2502.13131v1 by Feng Luo, Rui Yang, Hao Sun, Chunyuan Deng, Jiarui Yao, Jingyan Shen, Huan Zhang, Hanjie Chen

Understanding human preferences is crucial for improving foundation models
and building personalized AI systems. However, preferences are inherently
diverse and complex, making it difficult for traditional reward models to
capture their full range. While fine-grained preference data can help,
collecting it is expensive and hard to scale. In this paper, we introduce
Decomposed Reward Models (DRMs), a novel approach that extracts diverse human
preferences from binary comparisons without requiring fine-grained annotations.
Our key insight is to represent human preferences as vectors and analyze them
using Principal Component Analysis (PCA). By constructing a dataset of
embedding differences between preferred and rejected responses, DRMs identify
orthogonal basis vectors that capture distinct aspects of preference. These
decomposed rewards can be flexibly combined to align with different user needs,
offering an interpretable and scalable alternative to traditional reward
models. We demonstrate that DRMs effectively extract meaningful preference
dimensions (e.g., helpfulness, safety, humor) and adapt to new users without
additional training. Our results highlight DRMs as a powerful framework for
personalized and interpretable LLM alignment.

摘要：理解人類偏好對於改進基礎模型和建構個人化 AI 系統至關重要。然而，偏好本質上是多樣且複雜的，這使得傳統的獎勵模型難以捕捉其全部範圍。雖然細緻的偏好數據可能有所幫助，但收集這些數據既昂貴又難以擴展。在本文中，我們介紹了解構獎勵模型 (DRM)，這是一種新穎的方法，它可以從二元比較中提取多樣化的人類偏好，而不需要細緻的註解。我們的關鍵見解是將人類偏好表示為向量，並使用主成分分析 (PCA) 對其進行分析。透過建構偏好和拒絕回應之間嵌入差異的數據集，DRM 識別出正交基向量，這些向量捕捉偏好的不同面向。這些解構的獎勵可以靈活地結合在一起，以符合不同的使用者需求，提供一種可解釋且可擴展的傳統獎勵模型替代方案。我們證明了 DRM 可以有效地提取有意義的偏好維度（例如，有用性、安全性、幽默感），並在不需要額外訓練的情況下適應新的使用者。我們的結果突顯了 DRM 作為個人化且可解釋的 LLM 對齊強大架構。

##### **Magma: A Foundation Model for Multimodal AI Agents**
2502.13130v1 by Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, Jianfeng Gao

We present Magma, a foundation model that serves multimodal AI agentic tasks
in both the digital and physical worlds. Magma is a significant extension of
vision-language (VL) models in that it not only retains the VL understanding
ability (verbal intelligence) of the latter, but is also equipped with the
ability to plan and act in the visual-spatial world (spatial-temporal
intelligence) and complete agentic tasks ranging from UI navigation to robot
manipulation. To endow the agentic capabilities, Magma is pretrained on large
amounts of heterogeneous datasets spanning from images, videos to robotics
data, where the actionable visual objects (e.g., clickable buttons in GUI) in
images are labeled by Set-of-Mark (SoM) for action grounding, and the object
movements (e.g., the trace of human hands or robotic arms) in videos are
labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show
that SoM and ToM reach great synergy and facilitate the acquisition of
spatial-temporal intelligence for our Magma model, which is fundamental to a
wide range of tasks as shown in Fig.1. In particular, Magma creates new
state-of-the-art results on UI navigation and robotic manipulation tasks,
outperforming previous models that are specifically tailored to these tasks. On
image and video-related multimodal tasks, Magma also compares favorably to
popular large multimodal models that are trained on much larger datasets. We
make our model and code public for reproducibility at
https://microsoft.github.io/Magma.

摘要：<paragraph>我們提出 Magma，這是一個基礎模型，用於服務數位和物理世界中的多模態 AI 代理任務。Magma 是視覺語言 (VL) 模型的重大延伸，它不僅保留了後者的 VL 理解能力（語言智能），還具備在視覺空間世界中規劃和行動的能力（時空智能），並完成從 UI 導航到機器人操作的代理任務。為了賦予代理能力，Magma 在從影像、影片到機器人資料的大量異質資料集上進行預訓練，其中影像中的可操作視覺物件（例如 GUI 中的可點擊按鈕）由動作接地 Set-of-Mark (SoM) 標記，影片中的物件動作（例如人手或機器手臂的軌跡）由動作規劃 Trace-of-Mark (ToM) 標記。廣泛的實驗表明，SoM 和 ToM 達到了極大的協同作用，並促進了我們 Magma 模型的時空智能的獲取，這對於圖 1 中所示的各種任務至關重要。特別是，Magma 在 UI 導航和機器人操作任務上創造了新的最先進的結果，優於專門針對這些任務的先前模型。在影像和影片相關的多模態任務上，Magma 也與在更大資料集上訓練的流行大型多模態模型相比，表現得很好。我們公開我們的模型和程式碼，以便在 https://microsoft.github.io/Magma 上重現。</paragraph>

##### **SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation**
2502.13128v1 by Zihan Liu, Shuangrui Ding, Zhixiong Zhang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang

Text-to-song generation, the task of creating vocals and accompaniment from
textual inputs, poses significant challenges due to domain complexity and data
scarcity. Existing approaches often employ multi-stage generation procedures,
resulting in cumbersome training and inference pipelines. In this paper, we
propose SongGen, a fully open-source, single-stage auto-regressive transformer
designed for controllable song generation. The proposed model facilitates
fine-grained control over diverse musical attributes, including lyrics and
textual descriptions of instrumentation, genre, mood, and timbre, while also
offering an optional three-second reference clip for voice cloning. Within a
unified auto-regressive framework, SongGen supports two output modes: mixed
mode, which generates a mixture of vocals and accompaniment directly, and
dual-track mode, which synthesizes them separately for greater flexibility in
downstream applications. We explore diverse token pattern strategies for each
mode, leading to notable improvements and valuable insights. Furthermore, we
design an automated data preprocessing pipeline with effective quality control.
To foster community engagement and future research, we will release our model
weights, training code, annotated data, and preprocessing pipeline. The
generated samples are showcased on our project page at
https://liuzh-19.github.io/SongGen/ , and the code will be available at
https://github.com/LiuZH-19/SongGen .

摘要：文字轉歌曲生成，從文字輸入建立人聲和伴奏的任務，由於領域複雜性和資料稀少性，因此構成重大挑戰。現有方法通常採用多階段生成程序，導致訓練和推論管道繁瑣。在本文中，我們提出 SongGen，一個完全開源的單階段自迴歸轉換器，專為可控歌曲生成而設計。所提出的模型促進對各種音樂屬性的細粒度控制，包括歌詞和樂器、類型、情緒和音色的文字描述，同時還提供可選的三秒參考片段以進行語音複製。在統一的自迴歸框架內，SongGen 支援兩種輸出模式：混合模式，直接生成人聲和伴奏的混合，以及雙軌模式，將它們分開合成以提高下游應用程式的靈活性。我們探索每種模式的不同代幣模式策略，從而帶來顯著的改進和有價值的見解。此外，我們設計了一個自動化資料預處理管道，具備有效的品質控制。為了促進社區參與和未來的研究，我們將釋出我們的模型權重、訓練程式碼、註解資料和預處理管道。生成的範例展示在我們的專案頁面 https://liuzh-19.github.io/SongGen/，程式碼將在 https://github.com/LiuZH-19/SongGen 中提供。

##### **Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning**
2502.13127v1 by Jingyang Lin, Andy Wong, Tian Xia, Shenghua He, Hui Wei, Mei Han, Jiebo Luo

Recent advances in Large Language Models (LLMs) have enabled them to process
increasingly longer sequences, ranging from 2K to 2M tokens and even beyond.
However, simply extending the input sequence length does not necessarily lead
to effective long-context understanding. In this study, we integrate
Chain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate
effective long-context understanding. To achieve this, we introduce
LongFinanceQA, a synthetic dataset in the financial domain designed to improve
long-context reasoning. Unlike existing long-context synthetic data,
LongFinanceQA includes intermediate CoT reasoning before the final conclusion,
which encourages LLMs to perform explicit reasoning, improving accuracy and
interpretability in long-context understanding. To generate synthetic CoT
reasoning, we propose Property-driven Agentic Inference (PAI), an agentic
framework that simulates human-like reasoning steps, including property
extraction, retrieval, and summarization. We evaluate PAI's reasoning
capabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark,
outperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune
LLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's
financial subset.

摘要：大型語言模型 (LLM) 的最新進展讓它們能夠處理越來越長的序列，範圍從 2K 到 2M 個符號，甚至更長。
然而，僅僅延長輸入序列長度並不會必然導致有效的長語境理解。在本研究中，我們以監督的方式將思考鏈 (CoT) 推理整合到 LLM 中，以促進有效的長語境理解。為此，我們引入了 LongFinanceQA，這是一個在金融領域中的合成數據集，旨在改進長語境推理。與現有的長語境合成數據不同，LongFinanceQA 在最終結論之前包含了中間的 CoT 推理，這鼓勵 LLM 執行明確的推理，從而提高長語境理解的準確性和可解釋性。為了生成合成的 CoT 推理，我們提出了基於屬性的主體推理 (PAI)，這是一個模擬類人推理步驟的主體框架，包括屬性提取、檢索和總結。我們通過評估搭載 PAI 的 GPT-4o-mini 在 Loong 基準上的推理能力，使其比標準的 GPT-4o-mini 高出 20.0%，來評估 PAI 的推理能力。此外，我們對 LLaMA-3.1-8B-Instruct 進行了微調，在 Loong 的金融子集中實現了 24.6% 的增益。

##### **RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises**
2502.13125v1 by Zenan Zhai, Hao Li, Xudong Han, Zhenxuan Zhang, Yixuan Zhang, Timothy Baldwin, Haonan Li

Recent advances in large language models (LLMs) have shown that they can
answer questions requiring complex reasoning. However, their ability to
identify and respond to text containing logical fallacies or deliberately
misleading premises remains less studied. To address this gap, we introduce
RuozhiBench, a bilingual dataset comprising 677 carefully curated questions
that contain various forms of deceptive reasoning, meticulously crafted through
extensive human effort and expert review. In a comprehensive evaluation of 17
LLMs from 5 Series over RuozhiBench using both open-ended and two-choice
formats, we conduct extensive analyses on evaluation protocols and result
patterns. Despite their high scores on conventional benchmarks, these models
showed limited ability to detect and reason correctly about logical fallacies,
with even the best-performing model, Claude-3-haiku, achieving only 62%
accuracy compared to the human of more than 90%.

摘要：大型語言模型 (LLM) 的最新進展顯示，它們可以回答需要複雜推理的問題。然而，它們識別和回應包含邏輯謬誤或故意誤導前提的文本的能力仍未得到充分研究。為了解決這個差距，我們引入了 RuozhiBench，這是一個雙語資料集，包含 677 個經過仔細策劃的問題，其中包含各種形式的欺騙性推理，並透過廣泛的人力投入和專家審查精心製作。在使用開放式和二選一格式對來自 5 個系列的 17 個 LLM 進行 RuozhiBench 的全面評估中，我們對評估協定和結果模式進行了廣泛的分析。儘管它們在傳統基準測試中獲得了高分，但這些模型在檢測和正確推理邏輯謬誤方面表現出的能力有限，即使是效能最好的模型 Claude-3-haiku，與人類的 90% 以上相比，也只達到了 62% 的準確度。

##### **NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions**
2502.13124v1 by Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason E Weston, Xian Li

Scaling reasoning capabilities beyond traditional domains such as math and
coding is hindered by the lack of diverse and high-quality questions. To
overcome this limitation, we introduce a scalable approach for generating
diverse and challenging reasoning questions, accompanied by reference answers.
We present NaturalReasoning, a comprehensive dataset comprising 2.8 million
questions that span multiple domains, including STEM fields (e.g., Physics,
Computer Science), Economics, Social Sciences, and more. We demonstrate the
utility of the questions in NaturalReasoning through knowledge distillation
experiments which show that NaturalReasoning can effectively elicit and
transfer reasoning capabilities from a strong teacher model. Furthermore, we
demonstrate that NaturalReasoning is also effective for unsupervised
self-training using external reward models or self-rewarding.

摘要：透過超越傳統領域（例如數學和編碼）來擴充推理能力，受到缺乏多元且高品質問題的阻礙。為了克服這個限制，我們引入一個可擴充的方法，用於產生多元且具挑戰性的推理問題，並附上參考答案。我們提出 NaturalReasoning，這是一個包含 280 萬個問題的綜合資料集，涵蓋多個領域，包括 STEM 領域（例如物理、電腦科學）、經濟學、社會科學等等。我們透過知識蒸餾實驗，展示 NaturalReasoning 中問題的實用性，這些實驗顯示 NaturalReasoning 能有效地引發和轉移強大教師模型的推理能力。此外，我們展示 NaturalReasoning 也適用於使用外部獎勵模型或自我獎勵的無監督自我訓練。

##### **Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context**
2502.13120v1 by Marion Bartl, Thomas Brendan Murphy, Susan Leavy

Gender-inclusive language is often used with the aim of ensuring that all
individuals, regardless of gender, can be associated with certain concepts.
While psycholinguistic studies have examined its effects in relation to human
cognition, it remains unclear how Large Language Models (LLMs) process
gender-inclusive language. Given that commercial LLMs are gaining an
increasingly strong foothold in everyday applications, it is crucial to examine
whether LLMs in fact interpret gender-inclusive language neutrally, because the
language they generate has the potential to influence the language of their
users. This study examines whether LLM-generated coreferent terms align with a
given gender expression or reflect model biases. Adapting psycholinguistic
methods from French to English and German, we find that in English, LLMs
generally maintain the antecedent's gender but exhibit underlying masculine
bias. In German, this bias is much stronger, overriding all tested
gender-neutralization strategies.

摘要：性別包容性語言通常用於確保所有個人，無論性別如何，都能與某些概念聯繫在一起。雖然心理語言學研究已經檢視了它對人類認知的影響，但大型語言模型 (LLM) 如何處理性別包容性語言仍然不清楚。鑑於商業 LLM 在日常應用中越來越站穩腳步，因此至關重要的是要檢查 LLM 是否實際上中立地解釋性別包容性語言，因為它們產生的語言有可能影響其使用者的語言。本研究探討了 LLM 生成的共指術語是否與給定的性別表達一致或反映模型偏見。我們採用法語到英語和德語的心理語言學方法，發現英語中，LLM 通常會保持先行詞的性別，但表現出潛在的男性偏見。在德語中，這種偏見強得多，凌駕於所有經過測試的性別中立化策略。

##### **STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models**
2502.13119v1 by Narun Raman, Taylor Lundy, Thiago Amin, Jesse Perla, Kevin-Leyton Brown

How should one judge whether a given large language model (LLM) can reliably
perform economic reasoning? Most existing LLM benchmarks focus on specific
applications and fail to present the model with a rich variety of economic
tasks. A notable exception is Raman et al. [2024], who offer an approach for
comprehensively benchmarking strategic decision-making; however, this approach
fails to address the non-strategic settings prevalent in microeconomics, such
as supply-and-demand analysis. We address this gap by taxonomizing
microeconomic reasoning into $58$ distinct elements, focusing on the logic of
supply and demand, each grounded in up to $10$ distinct domains, $5$
perspectives, and $3$ types. The generation of benchmark data across this
combinatorial space is powered by a novel LLM-assisted data generation protocol
that we dub auto-STEER, which generates a set of questions by adapting
handwritten templates to target new domains and perspectives. Because it offers
an automated way of generating fresh questions, auto-STEER mitigates the risk
that LLMs will be trained to over-fit evaluation benchmarks; we thus hope that
it will serve as a useful tool both for evaluating and fine-tuning models for
years to come. We demonstrate the usefulness of our benchmark via a case study
on $27$ LLMs, ranging from small open-source models to the current state of the
art. We examined each model's ability to solve microeconomic problems across
our whole taxonomy and present the results across a range of prompting
strategies and scoring metrics.

摘要：<paragraph>如何判斷一個給定的大型語言模型 (LLM) 能否可靠地進行經濟推理？現有的 LLM 基準測試大多專注於特定應用，未能為模型提供豐富多樣的經濟任務。一個值得注意的例外是 Raman 等人 [2024]，他們提供了一種全面評估策略決策制定方法；然而，這種方法無法解決微觀經濟學中普遍存在的非策略性設定，例如供需分析。我們透過將微觀經濟推理分類為 58 個不同的元素來解決這個差距，重點放在供需邏輯上，每個元素都基於多達 10 個不同的領域、5 個觀點和 3 種類型。在這個組合空間中產生基準數據是由一種新穎的 LLM 輔助數據生成協議（我們稱之為 auto-STEER）推動的，它通過調整手寫模板來針對新的領域和觀點來生成一組問題。由於它提供了一種生成新問題的自動化方式，auto-STEER 減輕了 LLM 將被訓練過度配合評估基準測試的風險；因此，我們希望它將成為未來幾年評估和微調模型的有用工具。我們通過一個案例研究展示了我們基準測試的效用，該案例研究涵蓋了 27 個 LLM，從小型開源模型到當前技術狀態。我們檢查了每個模型在我們的整個分類法中解決微觀經濟問題的能力，並在各種提示策略和評分指標中展示了結果。</paragraph>

##### **Performance Evaluation of Large Language Models in Statistical Programming**
2502.13117v1 by Xinyi Song, Kexin Xie, Lina Lee, Ruizhe Chen, Jared M. Clark, Hao He, Haoran He, Jie Min, Xinlei Zhang, Simin Zheng, Zhiyang Zhang, Xinwei Deng, Yili Hong

The programming capabilities of large language models (LLMs) have
revolutionized automatic code generation and opened new avenues for automatic
statistical analysis. However, the validity and quality of these generated
codes need to be systematically evaluated before they can be widely adopted.
Despite their growing prominence, a comprehensive evaluation of statistical
code generated by LLMs remains scarce in the literature. In this paper, we
assess the performance of LLMs, including two versions of ChatGPT and one
version of Llama, in the domain of SAS programming for statistical analysis.
Our study utilizes a set of statistical analysis tasks encompassing diverse
statistical topics and datasets. Each task includes a problem description,
dataset information, and human-verified SAS code. We conduct a comprehensive
assessment of the quality of SAS code generated by LLMs through human expert
evaluation based on correctness, effectiveness, readability, executability, and
the accuracy of output results. The analysis of rating scores reveals that
while LLMs demonstrate usefulness in generating syntactically correct code,
they struggle with tasks requiring deep domain understanding and may produce
redundant or incorrect results. This study offers valuable insights into the
capabilities and limitations of LLMs in statistical programming, providing
guidance for future advancements in AI-assisted coding systems for statistical
analysis.

摘要：大型語言模型 (LLM) 的程式設計功能徹底改變了自動程式碼生成，並為自動統計分析開啟了新途徑。然而，在廣泛採用這些產生的程式碼之前，需要系統性地評估其有效性和品質。儘管其重要性日益提升，但文獻中對於 LLM 產生的統計程式碼的全面評估仍然稀少。在本文中，我們評估了 LLM 的效能，包括兩個版本的 ChatGPT 和一個版本的 Llama，在統計分析的 SAS 程式設計領域。我們的研究利用了一組涵蓋各種統計主題和資料集的統計分析任務。每個任務都包含問題說明、資料集資訊和經過人工驗證的 SAS 程式碼。我們透過基於正確性、有效性、可讀性、可執行性和輸出結果精確度的專家評估，對 LLM 產生的 SAS 程式碼品質進行全面評估。評分結果的分析顯示，儘管 LLM 在產生語法正確的程式碼方面表現出其效用，但它們在需要深入領域理解的任務中會遇到困難，並且可能會產生冗餘或不正確的結果。本研究提供了 LLM 在統計程式設計中能力和限制的寶貴見解，為統計分析的 AI 輔助編碼系統的未來進展提供指導。

##### **Near-Optimal Private Learning in Linear Contextual Bandits**
2502.13115v1 by Fan Chen, Jiachun Li, Alexander Rakhlin, David Simchi-Levi

We analyze the problem of private learning in generalized linear contextual
bandits. Our approach is based on a novel method of re-weighted regression,
yielding an efficient algorithm with regret of order
$\sqrt{T}+\frac{1}{\alpha}$ and $\sqrt{T}/\alpha$ in the joint and local model
of $\alpha$-privacy, respectively. Further, we provide near-optimal private
procedures that achieve dimension-independent rates in private linear models
and linear contextual bandits. In particular, our results imply that joint
privacy is almost "for free" in all the settings we consider, partially
addressing the open problem posed by Azize and Basu (2024).

摘要：我們分析廣義線性情境強盜中私人學習的問題。我們的做法基於重新加權回歸的新方法，產生一種有效率的演算法，其後悔值分別為
$\sqrt{T}+\frac{1}{\alpha}$ 和 $\sqrt{T}/\alpha$ 在 $\alpha$-隱私的聯合和局部模型中。此外，我們提供近乎最佳的私人程序，在私人線性模型和線性情境強盜中實現與維度無關的比率。特別是，我們的結果表明，在我們考慮的所有設定中，聯合隱私幾乎是「免費」的，部分解決了 Azize 和 Basu (2024) 提出的開放性問題。

##### **The influence of motion features in temporal perception**
2502.13114v1 by Rosa Illan Castillo, Javier Valenzuela

This paper examines the role of manner-of-motion verbs in shaping subjective
temporal perception and emotional resonance. Through four complementary
studies, we explore how these verbs influence the conceptualization of time,
examining their use in literal and metaphorical (temporal) contexts. Our
findings reveal that faster verbs (e.g., fly, zoom) evoke dynamic and engaging
temporal experiences, often linked to positive emotions and greater agency. In
contrast, slower verbs (e.g., crawl, drag) convey passivity, monotony, and
negative emotions, reflecting tedious or constrained experiences of time. These
effects are amplified in metaphorical contexts, where manner verbs encode
emotional and experiential nuances that transcend their literal meanings. We
also find that participants prefer manner verbs over path verbs (e.g., go,
pass) in emotionally charged temporal contexts, as manner verbs capture the
experiential and emotional qualities of time more effectively. These findings
highlight the interplay between language, motion, and emotion in shaping
temporal perception, offering insights into how linguistic framing influences
subjective experiences of time.

摘要：本文探討動作方式動詞在形塑主觀時間感知和情緒共鳴中所扮演的角色。透過四項互補的研究，我們探討這些動詞如何影響時間的概念化，並檢視它們在字面和隱喻（時間）語境中的用法。我們的研究結果顯示，較快的動詞（例如飛、飆）會引起動態且引人入勝的時間體驗，通常與正面情緒和較大的自主性有關。相反地，較慢的動詞（例如爬、拖）傳達了被動、單調和負面情緒，反映出乏味或受限的時間體驗。這些效應在隱喻語境中會被放大，其中動作動詞編碼了超越其字面意義的情緒和體驗細微差別。我們還發現，在充滿情緒的時間語境中，參與者偏好動作動詞而非路徑動詞（例如走、經過），因為動作動詞更有效地捕捉了時間的體驗和情緒品質。這些研究結果突顯了語言、動作和情緒之間在形塑時間感知中的交互作用，並提供了語言框架如何影響主觀時間體驗的見解。

##### **Improving Clinical Question Answering with Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization**
2502.13108v1 by Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit Agarwal, Bhargava Kumar, Srikant Panda, Tejaswini Kumar

Clinical Question Answering (CQA) plays a crucial role in medical
decision-making, enabling physicians to extract relevant information from
Electronic Medical Records (EMRs). While transformer-based models such as BERT,
BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in
CQA, existing models lack the ability to categorize extracted answers, which is
critical for structured retrieval, content filtering, and medical decision
support.
  To address this limitation, we introduce a Multi-Task Learning (MTL)
framework that jointly trains CQA models for both answer extraction and medical
categorization. In addition to predicting answer spans, our model classifies
responses into five standardized medical categories: Diagnosis, Medication,
Symptoms, Procedure, and Lab Reports. This categorization enables more
structured and interpretable outputs, making clinical QA models more useful in
real-world healthcare settings.
  We evaluate our approach on emrQA, a large-scale dataset for medical question
answering. Results show that MTL improves F1-score by 2.2% compared to standard
fine-tuning, while achieving 90.7% accuracy in answer categorization. These
findings suggest that MTL not only enhances CQA performance but also introduces
an effective mechanism for categorization and structured medical information
retrieval.

摘要：<paragraph>臨床問答 (CQA) 在醫療決策中扮演著至關重要的角色，讓醫師能夠從電子病歷 (EMR) 中擷取相關資訊。儘管 BERT、BioBERT 和 ClinicalBERT 等基於轉換器的模型已在 CQA 中展現出最先進的效能，但現有的模型缺乏分類擷取答案的能力，這對於結構化檢索、內容過濾和醫療決策支援至關重要。
  為了解決這個限制，我們引進了一個多任務學習 (MTL) 架構，它同時訓練 CQA 模型用於答案擷取和醫療分類。除了預測答案範圍，我們的模型將回應分類為五個標準化醫療類別：診斷、藥物、症狀、程序和實驗室報告。這種分類能產生更結構化且易於理解的輸出，讓臨床問答模型在真實世界的醫療保健環境中更實用。
  我們在 emrQA 上評估我們的做法，emrQA 是用於醫療問題解答的大規模資料集。結果顯示，與標準微調相比，MTL 將 F1 分數提高了 2.2%，同時在答案分類中達到 90.7% 的準確度。這些發現表明，MTL 不僅增強了 CQA 的效能，還引入了一種分類和結構化醫療資訊檢索的有效機制。</paragraph>

##### **MatterChat: A Multi-Modal LLM for Material Science**
2502.13107v1 by Yingheng Tang, Wenbin Xu, Jie Cao, Jianzhu Ma, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael W. Mahoney, Andy Nonaka, Zhi Yao

Understanding and predicting the properties of inorganic materials is crucial
for accelerating advancements in materials science and driving applications in
energy, electronics, and beyond. Integrating material structure data with
language-based information through multi-modal large language models (LLMs)
offers great potential to support these efforts by enhancing human-AI
interaction. However, a key challenge lies in integrating atomic structures at
full resolution into LLMs. In this work, we introduce MatterChat, a versatile
structure-aware multi-modal LLM that unifies material structural data and
textual inputs into a single cohesive model. MatterChat employs a bridging
module to effectively align a pretrained machine learning interatomic potential
with a pretrained LLM, reducing training costs and enhancing flexibility. Our
results demonstrate that MatterChat significantly improves performance in
material property prediction and human-AI interaction, surpassing
general-purpose LLMs such as GPT-4. We also demonstrate its usefulness in
applications such as more advanced scientific reasoning and step-by-step
material synthesis.

摘要：了解和預測無機材料的特性對於加速材料科學的進步和推動能源、電子等方面的應用至關重要。透過多模態大型語言模型 (LLM) 將材料結構數據與基於語言的資訊整合，可以極大程度地支持這些工作，藉此增強人類與 AI 的互動。然而，一個關鍵挑戰在於將原子結構以完整解析度整合到 LLM 中。在這項工作中，我們引入了 MatterChat，這是一個通用的結構感知多模態 LLM，它將材料結構數據和文字輸入統一到一個單一的內聚模型中。MatterChat 採用橋接模組，將預先訓練好的機器學習原子間電位與預先訓練好的 LLM 有效地對齊，從而降低訓練成本並增強靈活性。我們的結果表明，MatterChat 大幅提升了材料特性預測和人類與 AI 互動的效能，超越了 GPT-4 等通用 LLM。我們也展示了它在更進階的科學推理和逐步材料合成等應用中的效用。

##### **Understanding and Rectifying Safety Perception Distortion in VLMs**
2502.13095v1 by Xiaohan Zou, Jian Kang, George Kesidis, Lu Lin

Recent studies reveal that vision-language models (VLMs) become more
susceptible to harmful requests and jailbreak attacks after integrating the
vision modality, exhibiting greater vulnerability than their text-only LLM
backbones. To uncover the root cause of this phenomenon, we conduct an in-depth
analysis and identify a key issue: multimodal inputs introduce an
modality-induced activation shift toward a "safer" direction compared to their
text-only counterparts, leading VLMs to systematically overestimate the safety
of harmful inputs. We refer to this issue as safety perception distortion. To
mitigate such distortion, we propose Activation Shift Disentanglement and
Calibration (ShiftDC), a training-free method that decomposes and calibrates
the modality-induced activation shift to reduce the impact of modality on
safety. By isolating and removing the safety-relevant component, ShiftDC
restores the inherent safety alignment of the LLM backbone while preserving the
vision-language capabilities of VLMs. Empirical results demonstrate that
ShiftDC significantly enhances alignment performance on safety benchmarks
without impairing model utility.

摘要：最近的研究表明，在整合了视觉模态后，视觉语言模型 (VLM) 更容易受到有害请求和越狱攻击，表现出比其仅文本的 LLM 主干更大的漏洞。为了揭示这种现象的根本原因，我们进行了深入分析，并确定了一个关键问题：与仅文本的对应物相比，多模态输入引入了朝“更安全”方向的模态诱导激活转移，导致 VLM 系统性地高估有害输入的安全性。我们将此问题称为安全感知扭曲。为了减轻这种扭曲，我们提出了激活转移解耦和校准 (ShiftDC)，这是一种无训练方法，用于分解和校准模态诱导的激活转移，以减少模态对安全性的影响。通过隔离和移除与安全性相关的组件，ShiftDC 恢复了 LLM 主干的固有安全性对齐，同时保留了 VLM 的视觉语言能力。实证结果表明，ShiftDC 在不损害模型效用的情况下，显著增强了安全基准上的对齐性能。

##### **Text2World: Benchmarking Large Language Models for Symbolic World Model Generation**
2502.13092v1 by Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Hongyuan Zhang, Wenqi Shao, Ping Luo

Recently, there has been growing interest in leveraging large language models
(LLMs) to generate symbolic world models from textual descriptions. Although
LLMs have been extensively explored in the context of world modeling, prior
studies encountered several challenges, including evaluation randomness,
dependence on indirect metrics, and a limited domain scope. To address these
limitations, we introduce a novel benchmark, Text2World, based on planning
domain definition language (PDDL), featuring hundreds of diverse domains and
employing multi-criteria, execution-based metrics for a more robust evaluation.
We benchmark current LLMs using Text2World and find that reasoning models
trained with large-scale reinforcement learning outperform others. However,
even the best-performing model still demonstrates limited capabilities in world
modeling. Building on these insights, we examine several promising strategies
to enhance the world modeling capabilities of LLMs, including test-time
scaling, agent training, and more. We hope that Text2World can serve as a
crucial resource, laying the groundwork for future research in leveraging LLMs
as world models. The project page is available at
https://text-to-world.github.io/.

摘要：最近，人们越来越有兴趣利用大型语言模型（LLM）从文本描述中生成符号世界模型。尽管 LLM 已在世界建模的背景下得到广泛探索，但先前的研究遇到了若干挑战，包括评估随机性、对间接指标的依赖以及有限的领域范围。为了解决这些限制，我们引入了基于规划域定义语言（PDDL）的新基准 Text2World，该基准包含数百个不同的域，并采用基于执行的多标准指标来进行更稳健的评估。我们使用 Text2World 对当前的 LLM 进行了基准测试，发现使用大规模强化学习训练的推理模型优于其他模型。然而，即使是性能最佳的模型在世界建模方面仍然表现出有限的能力。基于这些见解，我们研究了几种有希望的策略来增强 LLM 的世界建模能力，包括测试时缩放、代理训练等等。我们希望 Text2World 能够作为一项至关重要的资源，为未来利用 LLM 作为世界模型的研究奠定基础。项目页面可在 https://text-to-world.github.io/ 获得。

##### **KAPPA: A Generic Patent Analysis Framework with Keyphrase-Based Portraits**
2502.13076v1 by Xin Xia, Yujin Wang, Jun Zhou, Guisheng Zhong, Linning Cai, Chen Zhang

Patent analysis highly relies on concise and interpretable document
representations, referred to as patent portraits. Keyphrases, both present and
absent, are ideal candidates for patent portraits due to their brevity,
representativeness, and clarity. In this paper, we introduce KAPPA, an
integrated framework designed to construct keyphrase-based patent portraits and
enhance patent analysis. KAPPA operates in two phases: patent portrait
construction and portrait-based analysis. To ensure effective portrait
construction, we propose a semantic-calibrated keyphrase generation paradigm
that integrates pre-trained language models with a prompt-based hierarchical
decoding strategy to leverage the multi-level structural characteristics of
patents. For portrait-based analysis, we develop a comprehensive framework that
employs keyphrase-based patent portraits to enable efficient and accurate
patent analysis. Extensive experiments on benchmark datasets of keyphrase
generation, the proposed model achieves significant improvements compared to
state-of-the-art baselines. Further experiments conducted on real-world patent
applications demonstrate that our keyphrase-based portraits effectively capture
domain-specific knowledge and enrich semantic representation for patent
analysis tasks.

摘要：專利分析高度依賴簡潔且可解讀的文件表示，稱為專利描述。關鍵字組，無論是存在的還是不存在的，都是專利描述的理想候選者，因為它們簡潔、具有代表性且清晰。在本文中，我們介紹了 KAPPA，一個用於建構基於關鍵字組的專利描述和增強專利分析的整合式架構。KAPPA 分為兩個階段執行：專利描述建構和基於描述的分析。為確保有效的描述建構，我們提出了一個語義校準關鍵字組生成範例，它將預先訓練的語言模型與基於提示的分層解碼策略整合在一起，以利用專利的多分層結構特性。對於基於描述的分析，我們開發了一個全面的架構，它採用基於關鍵字組的專利描述，以實現高效且準確的專利分析。在關鍵字組生成基準資料集上進行的廣泛實驗中，與最先進的基準線相比，所提出的模型取得了顯著的改進。在真實世界專利申請上進行的進一步實驗表明，我們基於關鍵字組的描述有效地擷取了特定領域的知識，並豐富了專利分析任務的語義表示。

##### **Interactive Agents to Overcome Ambiguity in Software Engineering**
2502.13069v1 by Sanidhya Vijayvargiya, Xuhui Zhou, Akhila Yerukola, Maarten Sap, Graham Neubig

AI agents are increasingly being deployed to automate tasks, often based on
ambiguous and underspecified user instructions. Making unwarranted assumptions
and failing to ask clarifying questions can lead to suboptimal outcomes, safety
risks due to tool misuse, and wasted computational resources. In this work, we
study the ability of LLM agents to handle ambiguous instructions in interactive
code generation settings by evaluating proprietary and open-weight models on
their performance across three key steps: (a) leveraging interactivity to
improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c)
asking targeted questions. Our findings reveal that models struggle to
distinguish between well-specified and underspecified instructions. However,
when models interact for underspecified inputs, they effectively obtain vital
information from the user, leading to significant improvements in performance
and underscoring the value of effective interaction. Our study highlights
critical gaps in how current state-of-the-art models handle ambiguity in
complex software engineering tasks and structures the evaluation into distinct
steps to enable targeted improvements.

摘要：人工智能代理正越來越多地被部署用於自動化任務，通常基於模棱兩可且未明確規定的使用者指令。做出不合理的假設且未能提出澄清問題，可能導致次佳結果、因工具誤用而產生的安全風險，以及浪費運算資源。在這項工作中，我們研究了 LLM 代理在互動式程式碼生成設定中處理模棱兩可指令的能力，方法是在三個關鍵步驟中評估專有和開放權重的模型： (a) 利用互動性來提升在模棱兩可場景中的效能、(b) 偵測模糊性，以及 (c) 提出目標問題。我們的研究結果顯示，模型難以區分明確規範的指令和未明確規範的指令。然而，當模型針對未明確規範的輸入進行互動時，它們會有效地從使用者取得重要資訊，進而大幅提升效能，並強調有效互動的價值。我們的研究突顯了目前最先進的模型在處理複雜軟體工程任務中的模糊性時存在哪些關鍵差距，並將評估架構為不同的步驟，以促成有目標的改善。

##### **Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity**
2502.13063v1 by Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev

A range of recent works addresses the problem of compression of sequence of
tokens into a shorter sequence of real-valued vectors to be used as inputs
instead of token embeddings or key-value cache. These approaches allow to
reduce the amount of compute in existing language models. Despite relying on
powerful models as encoders, the maximum attainable lossless compression ratio
is typically not higher than x10. This fact is highly intriguing because, in
theory, the maximum information capacity of large real-valued vectors is far
beyond the presented rates even for 16-bit precision and a modest vector size.
In this work, we explore the limits of compression by replacing the encoder
with a per-sample optimization procedure. We show that vectors with compression
ratios up to x1500 exist, which highlights two orders of magnitude gap between
existing and practically attainable solutions. Furthermore, we empirically show
that the compression limits are determined not by the length of the input but
by the amount of uncertainty to be reduced, namely, the cross-entropy loss on
this sequence without any conditioning. The obtained limits highlight the
substantial gap between the theoretical capacity of input embeddings and their
practical utilization, suggesting significant room for optimization in model
design.

摘要：一系列近期作品探讨了将序列标记压缩成较短的实值向量序列的问题，以用作输入，而不是标记嵌入或键值缓存。这些方法允许减少现有语言模型中的计算量。尽管依赖于强大的模型作为编码器，但最大可达到的无损压缩比通常不高于 x10。这一事实非常有趣，因为理论上，即使对于 16 位精度和适中的向量大小，大型实值向量的最大信息容量也远远超出了所呈现的速率。在这项工作中，我们通过用按样本优化程序替换编码器来探索压缩的极限。我们表明，存在压缩比高达 x1500 的向量，这突出了现有解决方案和实际可实现解决方案之间两个数量级的差距。此外，我们凭经验表明，压缩极限不是由输入的长度决定的，而是由要减少的不确定性量决定的，即在此序列上的交叉熵损失，没有任何条件。获得的极限突出了输入嵌入的理论容量与其实际利用之间的巨大差距，表明模型设计中有很大的优化空间。

##### **AI-Assisted Decision Making with Human Learning**
2502.13062v1 by Gali Noti, Kate Donahue, Jon Kleinberg, Sigal Oren

AI systems increasingly support human decision-making. In many cases, despite
the algorithm's superior performance, the final decision remains in human
hands. For example, an AI may assist doctors in determining which diagnostic
tests to run, but the doctor ultimately makes the diagnosis. This paper studies
such AI-assisted decision-making settings, where the human learns through
repeated interactions with the algorithm. In our framework, the algorithm --
designed to maximize decision accuracy according to its own model -- determines
which features the human can consider. The human then makes a prediction based
on their own less accurate model. We observe that the discrepancy between the
algorithm's model and the human's model creates a fundamental tradeoff. Should
the algorithm prioritize recommending more informative features, encouraging
the human to recognize their importance, even if it results in less accurate
predictions in the short term until learning occurs? Or is it preferable to
forgo educating the human and instead select features that align more closely
with their existing understanding, minimizing the immediate cost of learning?
This tradeoff is shaped by the algorithm's time-discounted objective and the
human's learning ability. Our results show that optimal feature selection has a
surprisingly clean combinatorial characterization, reducible to a stationary
sequence of feature subsets that is tractable to compute. As the algorithm
becomes more "patient" or the human's learning improves, the algorithm
increasingly selects more informative features, enhancing both prediction
accuracy and the human's understanding. Notably, early investment in learning
leads to the selection of more informative features than a later investment. We
complement our analysis by showing that the impact of errors in the algorithm's
knowledge is limited as it does not make the prediction directly.

摘要：人工智慧系統日益支援人類決策。在許多情況下，儘管演算法的效能優異，最終決策仍掌握在人類手中。例如，人工智慧可能會協助醫生決定要執行哪些診斷測試，但最終下診斷的是醫生。本文探討此類人工智慧輔助決策設定，其中人類透過與演算法重複互動而學習。在我們的架構中，演算法（旨在根據其自身模型最大化決策準確度）會決定人類可以考量的特徵。然後，人類根據其自身較不準確的模型做出預測。我們觀察到，演算法模型與人類模型之間的差異會產生基本的權衡。演算法是否應優先推薦更多資訊性特徵，鼓勵人類認識其重要性，即使短期內會導致準確度較低的預測，直到學習發生？或者，是否較好放棄教育人類，而選擇與其現有理解更緊密對齊的特徵，將學習的立即成本降至最低？這種權衡取決於演算法的時間折現目標和人類的學習能力。我們的結果表明，最佳特徵選擇具有令人驚訝的乾淨組合特徵，可簡化為可計算的固定特徵子集序列。隨著演算法變得更「有耐心」或人類的學習進步，演算法會越來越多地選擇更多資訊性特徵，增強預測準確度和人類的理解。值得注意的是，早期投資於學習會導致選擇比後期投資更多資訊性特徵。我們透過顯示演算法知識中錯誤的影響是有限的，因為它不會直接做出預測，來補充我們的分析。

##### **Improved Fine-Tuning of Large Multimodal Models for Hateful Meme Detection**
2502.13061v1 by Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne

Hateful memes have become a significant concern on the Internet,
necessitating robust automated detection systems. While large multimodal models
have shown strong generalization across various tasks, they exhibit poor
generalization to hateful meme detection due to the dynamic nature of memes
tied to emerging social trends and breaking news. Recent work further
highlights the limitations of conventional supervised fine-tuning for large
multimodal models in this context. To address these challenges, we propose
Large Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a
novel two-stage fine-tuning framework designed to improve both in-domain
accuracy and cross-domain generalization. Experimental results on six widely
used meme classification datasets demonstrate that LMM-RGCL achieves
state-of-the-art performance, outperforming agent-based systems such as
VPD-PALI-X-55B. Furthermore, our method effectively generalizes to
out-of-domain memes under low-resource settings, surpassing models like GPT-4o.

摘要：網路上的仇恨迷因已成為一大隱憂，因此需要強大的自動化偵測系統。雖然大型多模態模型已在各種任務中展現出強大的泛化能力，但由於迷因與新興社會趨勢和突發新聞息息相關，因此在仇恨迷因偵測方面表現不佳。最近的研究進一步強調了在這種情況下，傳統監督微調對大型多模態模型的限制。為了應對這些挑戰，我們提出了大型多模態模型檢索引導對比學習 (LMM-RGCL)，這是一種新穎的兩階段微調架構，旨在提高領域內準確度和跨領域泛化能力。在六個廣泛使用的迷因分類資料集上的實驗結果表明，LMM-RGCL 達到了最先進的效能，優於基於代理的系統，例如 VPD-PALI-X-55B。此外，我們的模型在低資源設定下有效泛化到領域外迷因，超越了 GPT-4o 等模型。

##### **SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models**
2502.13059v1 by Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, Yutao Zeng, Zhoufutu Wen, Ke Jin, Baorui Wang, Weixiao Zhou, Yunhong Lu, Tongliang Li, Wenhao Huang, Zhoujun Li

The increasing application of multi-modal large language models (MLLMs)
across various sectors have spotlighted the essence of their output reliability
and accuracy, particularly their ability to produce content grounded in factual
information (e.g. common and domain-specific knowledge). In this work, we
introduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate
the factuality ability of MLLMs to answer natural language short questions.
SimpleVQA is characterized by six key features: it covers multiple tasks and
multiple scenarios, ensures high quality and challenging queries, maintains
static and timeless reference answers, and is straightforward to evaluate. Our
approach involves categorizing visual question-answering items into 9 different
tasks around objective events or common knowledge and situating these within 9
topics. Rigorous quality control processes are implemented to guarantee
high-quality, concise, and clear answers, facilitating evaluation with minimal
variance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a
comprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into
their image comprehension and text generation abilities by identifying and
analyzing error cases.

摘要：隨著多模態大型語言模型 (MLLM) 在各個領域的應用日益普及，其輸出結果的可靠性和準確性已備受關注，特別是其根據事實資訊（例如一般知識和特定領域知識）產生內容的能力。在本文中，我們介紹 SimpleVQA，這是第一個用於評估 MLLM 回答自然語言簡短問題的事實能力的綜合多模態基準。SimpleVQA 有六個主要特徵：涵蓋多項任務和多種情境、確保高品質且具挑戰性的查詢、維護靜態且永恆的參考答案，而且評估起來很簡單。我們的做法是將視覺問答項目分類為 9 個不同的任務，圍繞客觀事件或常識，並將它們置於 9 個主題中。我們實施嚴格的品質控管流程，以保證答案的高品質、簡潔和清晰，並透過 LLM 作為評分系統，以最小的差異進行評估。我們使用 SimpleVQA 對 18 個主要的 MLLM 和 8 個純文字 LLM 進行全面評估，透過找出和分析錯誤案例，深入探討它們的影像理解和文字生成能力。

##### **LAMD: Context-driven Android Malware Detection and Classification with LLMs**
2502.13055v1 by Xingzhi Qian, Xinran Zheng, Yiling He, Shuo Yang, Lorenzo Cavallaro

The rapid growth of mobile applications has escalated Android malware
threats. Although there are numerous detection methods, they often struggle
with evolving attacks, dataset biases, and limited explainability. Large
Language Models (LLMs) offer a promising alternative with their zero-shot
inference and reasoning capabilities. However, applying LLMs to Android malware
detection presents two key challenges: (1)the extensive support code in Android
applications, often spanning thousands of classes, exceeds LLMs' context limits
and obscures malicious behavior within benign functionality; (2)the structural
complexity and interdependencies of Android applications surpass LLMs'
sequence-based reasoning, fragmenting code analysis and hindering malicious
intent inference. To address these challenges, we propose LAMD, a practical
context-driven framework to enable LLM-based Android malware detection. LAMD
integrates key context extraction to isolate security-critical code regions and
construct program structures, then applies tier-wise code reasoning to analyze
application behavior progressively, from low-level instructions to high-level
semantics, providing final prediction and explanation. A well-designed factual
consistency verification mechanism is equipped to mitigate LLM hallucinations
from the first tier. Evaluation in real-world settings demonstrates LAMD's
effectiveness over conventional detectors, establishing a feasible basis for
LLM-driven malware analysis in dynamic threat landscapes.

摘要：隨著行動應用程式快速成長，Android 惡意軟體威脅也隨之升級。雖然有許多偵測方法，但它們經常難以應付不斷演進的攻擊、資料集偏差和有限的可解釋性。大型語言模型 (LLM) 提供了一個有前途的替代方案，具備零次學習推理和推理能力。然而，將 LLM 應用於 Android 惡意軟體偵測會出現兩個主要挑戰：(1) Android 應用程式中大量的支援程式碼，通常橫跨數千個類別，超過 LLM 的上下文限制，並模糊了良性功能中的惡意行為；(2) Android 應用程式的結構複雜性和相互依賴性超過 LLM 的基於序列的推理，會造成程式碼分析破碎，並阻礙惡意意圖推論。為了應對這些挑戰，我們提出了 LAMD，一個實用的脈絡驅動架構，以支援基於 LLM 的 Android 惡意軟體偵測。LAMD 整合了關鍵脈絡萃取，以隔離與安全性至關重要的程式碼區域並建構程式結構，然後套用分層式程式碼推理，逐步分析應用程式行為，從低階指令到高階語意，提供最終預測和說明。一個設計良好的事實一致性驗證機制具備減輕 LLM 從第一層產生的幻覺的能力。在真實環境中的評估顯示，LAMD 優於傳統偵測器，為動態威脅環境中的 LLM 驅動惡意軟體分析建立了一個可行的基礎。

##### **Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction**
2502.13044v1 by Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff

Aspect sentiment quadruple prediction (ASQP) facilitates a detailed
understanding of opinions expressed in a text by identifying the opinion term,
aspect term, aspect category and sentiment polarity for each opinion. However,
annotating a full set of training examples to fine-tune models for ASQP is a
resource-intensive process. In this study, we explore the capabilities of large
language models (LLMs) for zero- and few-shot learning on the ASQP task across
five diverse datasets. We report F1 scores slightly below those obtained with
state-of-the-art fine-tuned models but exceeding previously reported zero- and
few-shot performance. In the 40-shot setting on the Rest16 restaurant domain
dataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the
best-performing fine-tuned method MVP. Additionally, we report the performance
of LLMs in target aspect sentiment detection (TASD), where the F1 scores were
also close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot
setting, compared to 72.76 with MVP. While human annotators remain essential
for achieving optimal performance, LLMs can reduce the need for extensive
manual annotation in ASQP tasks.

摘要：面向觀點的四元預測 (ASQP) 透過辨識各個觀點的觀點詞彙、面向詞彙、面向類別和觀點極性，協助詳細了解文字中表達的意見。然而，標註一組完整的訓練範例以微調 ASQP 模型是一個耗費資源的過程。在這項研究中，我們探討大型語言模型 (LLM) 在 ASQP 任務中進行零次和少量學習的能力，橫跨五個不同的資料集。我們報告的 F1 分數略低於使用最先進的微調模型獲得的分數，但超過先前報告的零次和少量學習表現。在 Rest16 餐廳領域資料集的 40 次學習設定中，LLM 達到了 52.46 的 F1 分數，而效能最佳的微調方法 MVP 則為 60.39。此外，我們報告了 LLM 在目標面向觀點偵測 (TASD) 中的表現，其中 F1 分數也接近微調模型，在 40 次學習設定中於 Rest16 達到 66.03，而 MVP 則為 72.76。儘管人類標註員對於達成最佳效能仍然至關重要，但 LLM 可以減少 ASQP 任務中廣泛手動標註的需求。

##### **Natural Language Generation from Visual Sequences: Challenges and Future Directions**
2502.13034v1 by Aditya K Surikuchi, Raquel Fernández, Sandro Pezzelle

The ability to use natural language to talk about visual content is at the
core of human intelligence and a crucial feature of any artificial intelligence
system. Various studies have focused on generating text for single images. In
contrast, comparatively little attention has been paid to exhaustively
analyzing and advancing work on multiple-image vision-to-text settings. In this
position paper, we claim that any task dealing with temporally ordered
sequences of multiple images or frames is an instance of a broader, more
general problem involving the understanding of intricate relationships between
the visual content and the corresponding text. We comprehensively analyze five
tasks that are instances of this problem and argue that they pose a common set
of challenges and share similarities in terms of modeling and evaluation
approaches. Based on the insights from these various aspects and stages of
multi-image-to-text generation, we highlight several open questions and suggest
future research directions. We believe that these directions can advance the
understanding of complex phenomena in this domain and the development of better
models.

摘要：使用自然語言來談論視覺內容的能力是人類智慧的核心，也是任何人工智慧系統的一項關鍵功能。各種研究都專注於為單一影像產生文字。相較之下，對於詳盡分析和推進多重影像視覺轉文字設定的工作，關注較少。在此立場文件中，我們聲稱任何處理多重影像或畫格的時間順序序列的任務，都是一個更廣泛、更普遍問題的範例，涉及理解視覺內容和對應文字之間的複雜關係。我們全面分析了此問題的五個範例任務，並論證它們提出了一組常見的挑戰，且在建模和評估方法方面有相似之處。根據多重影像轉文字生成的這些不同面向和階段的見解，我們突出了幾個開放性問題，並建議未來的研究方向。我們相信這些方向可以推進對此領域中複雜現象的理解，以及開發出更好的模型。

##### **HPSS: Heuristic Prompting Strategy Search for LLM Evaluators**
2502.13031v1 by Bosi Wen, Pei Ke, Yufei Sun, Cunxiang Wang, Xiaotao Gu, Jinfeng Zhou, Jie Tang, Hongning Wang, Minlie Huang

Since the adoption of large language models (LLMs) for text evaluation has
become increasingly prevalent in the field of natural language processing
(NLP), a series of existing works attempt to optimize the prompts for LLM
evaluators to improve their alignment with human judgment. However, their
efforts are limited to optimizing individual factors of evaluation prompts,
such as evaluation criteria or output formats, neglecting the combinatorial
impact of multiple factors, which leads to insufficient optimization of the
evaluation pipeline. Nevertheless, identifying well-behaved prompting
strategies for adjusting multiple factors requires extensive enumeration. To
this end, we comprehensively integrate 8 key factors for evaluation prompts and
propose a novel automatic prompting strategy optimization method called
Heuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,
HPSS conducts an iterative search to find well-behaved prompting strategies for
LLM evaluators. A heuristic function is employed to guide the search process,
enhancing the performance of our algorithm. Extensive experiments across four
evaluation tasks demonstrate the effectiveness of HPSS, consistently
outperforming both human-designed evaluation prompts and existing automatic
prompt optimization methods.

摘要：隨著自然語言處理（NLP）領域中採用大型語言模型（LLM）進行文本評估變得越來越普遍，一系列現有工作嘗試優化 LLM 評估器的提示，以改善它們與人類判斷的一致性。然而，他們的努力僅限於優化評估提示的個別因素，例如評估準則或輸出格式，而忽略了多種因素的組合影響，這導致評估管道優化不足。儘管如此，找出調整多種因素的良好提示策略需要廣泛的枚舉。為此，我們全面整合了評估提示的 8 個關鍵因素，並提出了一種名為啟發式提示策略搜索（HPSS）的新型自動提示策略優化方法。在遺傳演算法的啟發下，HPSS 進行反覆搜索以找出 LLM 評估器的良好提示策略。採用啟發式函數來指導搜索過程，增強了我們演算法的效能。在四項評估任務中進行的廣泛實驗證明了 HPSS 的有效性，始終優於人類設計的評估提示和現有的自動提示優化方法。

##### **Whose story is it? Personalizing story generation by inferring author styles**
2502.13028v1 by Nischal Ashok Kumar, Chau Minh Pham, Mohit Iyyer, Andrew Lan

Personalization has become essential for improving user experience in
interactive writing and educational applications, yet its potential in story
generation remains largely unexplored. In this work, we propose a novel
two-stage pipeline for personalized story generation. Our approach first infers
an author's implicit story-writing characteristics from their past work and
organizes them into an Author Writing Sheet, inspired by narrative theory. The
second stage uses this sheet to simulate the author's persona through tailored
persona descriptions and personalized story writing rules. To enable and
validate our approach, we construct Mythos, a dataset of 590 stories from 64
authors across five distinct sources that reflect diverse story-writing
settings. A head-to-head comparison with a non-personalized baseline
demonstrates our pipeline's effectiveness in generating high-quality
personalized stories. Our personalized stories achieve a 75 percent win rate
(versus 14 percent for the baseline and 11 percent ties) in capturing authors'
writing style based on their past works. Human evaluation highlights the high
quality of our Author Writing Sheet and provides valuable insights into the
personalized story generation task. Notable takeaways are that writings from
certain sources, such as Reddit, are easier to personalize than others, like
AO3, while narrative aspects, like Creativity and Language Use, are easier to
personalize than others, like Plot.

摘要：個人化已成為改善互動式寫作和教育應用程式中使用者體驗的必要手段，然而其在故事生成中的潛力仍未被廣泛探索。在這項工作中，我們提出了一個創新的兩階段流程，用於個人化故事生成。我們的做法首先從作者過去的作品中推論出作者隱含的故事寫作特徵，並根據敘事理論將它們組織成作者寫作表。第二階段使用此表透過量身打造的角色描述和個人化故事寫作規則來模擬作者的角色。為了啟用和驗證我們的做法，我們建構了 Mythos，一個包含來自 64 位作者、橫跨五個不同來源的 590 個故事的資料集，這些故事反映了多樣化的故事寫作設定。與非個人化基準進行一對一的比較，證明了我們的流程在生成高品質個人化故事方面的有效性。我們的個人化故事以 75% 的獲勝率（相較於基準的 14% 和 11% 平手）捕捉到作者基於其過去作品的寫作風格。人類評估突顯了我們作者寫作表的優良品質，並提供了對個人化故事生成任務的寶貴見解。值得注意的是，來自某些來源（例如 Reddit）的作品比其他來源（例如 AO3）更容易個人化，而敘事層面（例如創造力和語言使用）比其他層面（例如情節）更容易個人化。

##### **Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks**
2502.13025v1 by Markus J. Buehler

We present an agentic, autonomous graph expansion framework that iteratively
structures and refines knowledge in situ. Unlike conventional knowledge graph
construction methods relying on static extraction or single-pass learning, our
approach couples a reasoning-native large language model with a continually
updated graph representation. At each step, the system actively generates new
concepts and relationships, merges them into a global graph, and formulates
subsequent prompts based on its evolving structure. Through this
feedback-driven loop, the model organizes information into a scale-free network
characterized by hub formation, stable modularity, and bridging nodes that link
disparate knowledge clusters. Over hundreds of iterations, new nodes and edges
continue to appear without saturating, while centrality measures and shortest
path distributions evolve to yield increasingly distributed connectivity. Our
analysis reveals emergent patterns, such as the rise of highly connected 'hub'
concepts and the shifting influence of 'bridge' nodes, indicating that agentic,
self-reinforcing graph construction can yield open-ended, coherent knowledge
structures. Applied to materials design problems, we present compositional
reasoning experiments by extracting node-specific and synergy-level principles
to foster genuinely novel knowledge synthesis, yielding cross-domain ideas that
transcend rote summarization and strengthen the framework's potential for
open-ended scientific discovery. We discuss other applications in scientific
discovery and outline future directions for enhancing scalability and
interpretability.

摘要：<paragraph>我們提出一個能動的、自主的圖形擴展框架，它反覆地建構和精煉原位知識。與依賴靜態提取或單次學習的傳統知識圖形建構方法不同，我們的做法將一個推理原生的大語言模型與一個持續更新的圖形表示結合起來。在每一步中，系統主動產生新的概念和關係，將它們合併到一個全域圖形中，並根據其不斷演化的結構制定後續提示。透過這個回饋驅動的迴圈，模型將資訊組織成一個無標度網路，其特徵是樞紐形成、穩定的模組化以及連結不同知識群集的橋接節點。在數百次反覆運算中，新的節點和邊緣會持續出現，而不會飽和，同時中心性測量和最短路徑分佈會演化為產生越來越分散的連通性。我們的分析揭示了新興模式，例如高度連接的「樞紐」概念的興起和「橋樑」節點影響力的轉移，這表明能動的、自我強化的圖形建構可以產生開放式、連貫的知識結構。應用於材料設計問題，我們提出組合推理實驗，透過提取特定於節點的原則和協同效應層級原則，以促進真正新穎的知識綜合，產生超越死背式摘要並強化框架在開放式科學發現中潛力的跨領域想法。我們討論了在科學發現中的其他應用，並概述了增強可擴充性和可解釋性的未來方向。</paragraph>

##### **Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation**
2502.13019v1 by Sha Li, Naren Ramarkrishnan

Despite the remarkable capabilities of Large Language Models (LLMs) in
various NLP tasks, they remain vulnerable to hallucinations due to their
limited parametric knowledge and lack of domain-specific expertise.
Retrieval-Augmented Generation (RAG) addresses this challenge by incorporating
external document retrieval to augment the knowledge base of LLMs. In this
approach, RAG retrieves document chunks from an external corpus in response to
a query, which are then used as context for the downstream language model to
generate an answer. However, these retrieved knowledge sources often include
irrelevant or erroneous information, undermining the effectiveness of RAG in
downstream tasks. To overcome this limitation, we introduce a compact,
efficient, and pluggable module designed to refine external knowledge sources
before feeding them to the generator. The module reconstructs retrieved content
by extracting the most relevant and supportive information and reorganising it
into a concise, query-specific format. Through a three-stage training paradigm
- comprising supervised fine-tuning, contrastive multi-task learning, and
reinforcement learning-based alignment - it prioritises critical knowledge and
aligns it with the generator's preferences. This method enables LLMs to produce
outputs that are more accurate, reliable, and contextually appropriate.

摘要：儘管大型語言模型 (LLM) 在各種自然語言處理任務中具備卓越的能力，但由於其參數知識有限且缺乏特定領域的專業知識，因此它們仍然容易出現幻覺。檢索增強式生成 (RAG) 透過納入外部文件檢索來擴充 LLM 的知識庫，以應對此項挑戰。在此方法中，RAG 會根據查詢檢索外部語料庫中的文件區塊，然後將其用作下游語言模型的背景，以產生答案。然而，這些檢索到的知識來源通常包含不相關或錯誤的資訊，因而損害了 RAG 在下游任務中的效能。為了克服此項限制，我們引入了一個精簡、有效率且可插入的模組，用於在將外部知識來源提供給生成器之前對其進行精煉。此模組透過提取最相關且有用的資訊並將其重新組織成簡潔且特定於查詢的格式，來重建檢索到的內容。透過三階段訓練範例 - 包含監督微調、對比多任務學習以及基於強化學習的比對 - 它優先考量關鍵知識，並使其與生成器的偏好相符。此方法可讓 LLM 產生更準確、可靠且在語境上更適當的輸出。

##### **LLM-Powered Proactive Data Systems**
2502.13016v1 by Sepanta Zeighami, Yiming Lin, Shreya Shankar, Aditya Parameswaran

With the power of LLMs, we now have the ability to query data that was
previously impossible to query, including text, images, and video. However,
despite this enormous potential, most present-day data systems that leverage
LLMs are reactive, reflecting our community's desire to map LLMs to known
abstractions. Most data systems treat LLMs as an opaque black box that operates
on user inputs and data as is, optimizing them much like any other approximate,
expensive UDFs, in conjunction with other relational operators. Such data
systems do as they are told, but fail to understand and leverage what the LLM
is being asked to do (i.e. the underlying operations, which may be
error-prone), the data the LLM is operating on (e.g., long, complex documents),
or what the user really needs. They don't take advantage of the characteristics
of the operations and/or the data at hand, or ensure correctness of results
when there are imprecisions and ambiguities. We argue that data systems instead
need to be proactive: they need to be given more agency -- armed with the power
of LLMs -- to understand and rework the user inputs and the data and to make
decisions on how the operations and the data should be represented and
processed. By allowing the data system to parse, rewrite, and decompose user
inputs and data, or to interact with the user in ways that go beyond the
standard single-shot query-result paradigm, the data system is able to address
user needs more efficiently and effectively. These new capabilities lead to a
rich design space where the data system takes more initiative: they are
empowered to perform optimization based on the transformation operations, data
characteristics, and user intent. We discuss various successful examples of how
this framework has been and can be applied in real-world tasks, and present
future directions for this ambitious research agenda.

摘要：<paragraph>透過 LLM 的強大功能，我們現在能夠查詢過去無法查詢的資料，包括文字、圖片和影片。然而，儘管有如此龐大的潛力，但現今大多數利用 LLM 的資料系統都是被動的，反映出我們的社群希望將 LLM 映射到已知的抽象化。大多數資料系統將 LLM 視為一個不透明的黑盒子，以使用者輸入和資料為基礎進行運作，並像其他近似、昂貴的 UDF 一樣最佳化它們，並與其他關聯運算子結合使用。這些資料系統會照著指示執行，但無法理解並運用 LLM 被要求執行的任務（例如可能容易出錯的基本運算）、LLM 正在運算的資料（例如冗長、複雜的文件），或使用者真正需要的是什麼。它們不會利用運算和/或手邊資料的特性，或在有誤差和歧義時確保結果的正確性。我們認為資料系統應該改為主動：它們需要被賦予更多自主權，並具備 LLM 的強大功能，以了解並重新處理使用者輸入和資料，並就運算和資料的表示和處理方式做出決策。透過允許資料系統解析、改寫和分解使用者輸入和資料，或以超越標準單次查詢結果模式的方式與使用者互動，資料系統能夠更有效率且有效地滿足使用者的需求。這些新功能會帶來一個豐富的設計空間，讓資料系統發揮更多主導性：它們有能力根據轉換運算、資料特性和使用者意圖進行最佳化。我們將討論這個架構如何應用於實際任務，並提出這個雄心勃勃的研究議程的未來方向。</paragraph>

##### **Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents**
2502.13012v1 by Chaoran Chen, Bingsheng Yao, Ruishi Zou, Wenyue Hua, Weimin Lyu, Toby Jia-Jun Li, Dakuo Wang

Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that
simulates human-like behaviors in a variety of tasks. However, evaluating RPAs
is challenging due to diverse task requirements and agent designs. This paper
proposes an evidence-based, actionable, and generalizable evaluation design
guideline for LLM-based RPA by systematically reviewing 1,676 papers published
between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,
seven task attributes, and seven evaluation metrics from existing literature.
Based on these findings, we present an RPA evaluation design guideline to help
researchers develop more systematic and consistent evaluation methods.

摘要：角色扮演代理（RPA）是一種越來越流行的 LLM 代理，它能模擬人類在各種任務中的行為。然而，由於任務需求和代理設計的多樣性，評估 RPA 具有挑戰性。本文通過系統地審查 2021 年 1 月至 2024 年 12 月期間發表的 1,676 篇論文，提出了基於證據、可操作且可推廣的 LLM 基於 RPA 的評估設計指南。我們的分析從現有文獻中識別出六個代理屬性、七個任務屬性和七個評估指標。根據這些發現，我們提出了 RPA 評估設計指南，以幫助研究人員開發更系統化和一致的評估方法。

##### **Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge**
2502.13010v1 by Mohammad Reza Rezaei, Reza Saadati Fard, Jayson Parker, Rahul G. Krishnan, Milad Lankarany

Large Language Models (LLMs) have significantly advanced medical
question-answering by leveraging extensive clinical data and medical
literature. However, the rapid evolution of medical knowledge and the
labor-intensive process of manually updating domain-specific resources pose
challenges to the reliability of these systems. To address this, we introduce
Adaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates
the construction and continuous updating of medical knowledge graphs,
integrates reasoning, and retrieves current external evidence, such as PubMed
and WikiSearch. By dynamically linking new findings and complex medical
concepts, AMG-RAG not only improves accuracy but also enhances interpretability
in medical queries.
  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness
of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of
66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to
100 times larger. Notably, these improvements are achieved without increasing
computational overhead, highlighting the critical role of automated knowledge
graph generation and external evidence retrieval in delivering up-to-date,
trustworthy medical insights.

摘要：大型語言模型 (LLM) 透過利用廣泛的臨床資料和醫學文獻，大幅提升了醫療問題解答的進步。然而，醫療知識的快速演進和手動更新特定領域資源的繁複程序，對這些系統的可靠性構成挑戰。為了解決這個問題，我們引入了適應性醫療圖表 RAG (AMG-RAG)，這是一個自動化建構和持續更新醫療知識圖表的綜合架構，整合推理並擷取 PubMed 和 WikiSearch 等最新的外部證據。透過動態連結新的發現和複雜的醫療概念，AMG-RAG 不僅提升了準確性，也增強了醫療查詢的可解釋性。在 MEDQA 和 MEDMCQA 基準上的評量證明了 AMG-RAG 的有效性，在 MEDQA 上達到了 74.1% 的 F1 分數，在 MEDMCQA 上達到了 66.34% 的準確度，優於其他同類模型以及那些大 10 到 100 倍的模型。值得注意的是，這些改進是在不增加運算負擔的情況下實現的，突顯了自動化知識圖表生成和外部證據擷取在提供最新、可信賴的醫療見解中扮演的重要角色。

##### **Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks**
2502.13006v1 by Yarin Benyamin, Argaman Mordoch, Shahaf S. Shperberg, Roni Stern

Automated Planning algorithms require a model of the domain that specifies
the preconditions and effects of each action. Obtaining such a domain model is
notoriously hard. Algorithms for learning domain models exist, yet it remains
unclear whether learning a domain model and planning is an effective approach
for numeric planning environments, i.e., where states include discrete and
numeric state variables. In this work, we explore the benefits of learning a
numeric domain model and compare it with alternative model-free solutions. As a
case study, we use two tasks in Minecraft, a popular sandbox game that has been
used as an AI challenge. First, we consider an offline learning setting, where
a set of expert trajectories are available to learn from. This is the standard
setting for learning domain models. We used the Numeric Safe Action Model
Learning (NSAM) algorithm to learn a numeric domain model and solve new
problems with the learned domain model and a numeric planner. We call this
model-based solution NSAM_(+p), and compare it to several model-free Imitation
Learning (IL) and Offline Reinforcement Learning (RL) algorithms. Empirical
results show that some IL algorithms can learn faster to solve simple tasks,
while NSAM_(+p) allows solving tasks that require long-term planning and
enables generalizing to solve problems in larger environments. Then, we
consider an online learning setting, where learning is done by moving an agent
in the environment. For this setting, we introduce RAMP. In RAMP, observations
collected during the agent's execution are used to simultaneously train an RL
policy and learn a planning domain action model. This forms a positive feedback
loop between the RL policy and the learned domain model. We demonstrate
experimentally the benefits of using RAMP, showing that it finds more efficient
plans and solves more problems than several RL baselines.

摘要：<paragraph>自動化規劃演算法需要一個網域模型，來指定每個動作的前提條件和效果。取得這樣的網域模型出了名的困難。學習網域模型的演算法確實存在，但學習網域模型和規劃是否為數值規劃環境的有效方法仍然不清楚，也就是說，其中狀態包含離散和數值狀態變數。在這項工作中，我們探討學習數值網域模型的優點，並將其與替代的無模型解決方案進行比較。作為一個案例研究，我們使用 Minecraft 中的兩個任務，Minecraft 是一個流行的沙盒遊戲，已被用作 AI 挑戰。首先，我們考慮離線學習設定，其中有一組專家軌跡可供學習。這是學習網域模型的標準設定。我們使用數值安全動作模型學習 (NSAM) 演算法來學習數值網域模型，並使用已學習的網域模型和數值規劃器解決新問題。我們稱此模型為基礎的解決方案 NSAM_(+p)，並將其與多種無模型模仿學習 (IL) 和離線強化學習 (RL) 演算法進行比較。經驗結果顯示，一些 IL 演算法可以更快地學習解決簡單任務，而 NSAM_(+p) 允許解決需要長期規劃的任務，並能夠推廣到在更大環境中解決問題。然後，我們考慮線上學習設定，其中學習是透過在環境中移動代理來完成的。對於此設定，我們引入了 RAMP。在 RAMP 中，在代理執行期間收集的觀察結果用於同時訓練 RL 政策和學習規劃網域動作模型。這在 RL 政策和已學習的網域模型之間形成了一個正向回饋迴路。我們透過實驗證明了使用 RAMP 的好處，顯示它比多個 RL 基準找到了更有效的計畫，並解決了更多問題。</paragraph>

##### **Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer Architectures for Speech Quality Estimation**
2502.13004v1 by Wafaa Wardah, Tuğçe Melike Koçak Büyüktaş, Kirill Shchegelskiy, Sebastian Möller, Robert P. Spang

Objective speech quality models aim to predict human-perceived speech quality
using automated methods. However, cross-lingual generalization remains a major
challenge, as Mean Opinion Scores (MOS) vary across languages due to
linguistic, perceptual, and dataset-specific differences. A model trained
primarily on English data may struggle to generalize to languages with
different phonetic, tonal, and prosodic characteristics, leading to
inconsistencies in objective assessments. This study investigates the
cross-lingual performance of two speech quality models: NISQA, a CNN-based
model, and a Transformer-based Audio Spectrogram Transformer (AST) model. Both
models were trained exclusively on English datasets containing over 49,000
speech samples and subsequently evaluated on speech in German, French,
Mandarin, Swedish, and Dutch. We analyze model performance using Pearson
Correlation Coefficient (PCC) and Root Mean Square Error (RMSE) across five
speech quality dimensions: coloration, discontinuity, loudness, noise, and MOS.
Our findings show that while AST achieves a more stable cross-lingual
performance, both models exhibit noticeable biases. Notably, Mandarin speech
quality predictions correlate highly with human MOS scores, whereas Swedish and
Dutch present greater prediction challenges. Discontinuities remain difficult
to model across all languages. These results highlight the need for more
balanced multilingual datasets and architecture-specific adaptations to improve
cross-lingual generalization.

摘要：客觀語音品質模型旨在使用自動化方法預測人類感知的語音品質。然而，跨語言的概化仍然是一項重大挑戰，因為平均意見分數 (MOS) 會因語言的不同而有所不同，這是由於語言、感知和特定於資料集的差異所致。主要使用英語資料訓練的模型可能會難以概化到具有不同語音、聲調和韻律特徵的語言，導致客觀評估不一致。本研究探討了兩種語音品質模型的跨語言效能：基於 CNN 的 NISQA 模型和基於 Transformer 的音訊光譜 Transformer (AST) 模型。這兩種模型都僅使用包含超過 49,000 個語音範例的英語資料集進行訓練，然後在德語、法語、普通話、瑞典語和荷蘭語的語音上進行評估。我們使用皮爾森相關係數 (PCC) 和均方根誤差 (RMSE) 分析五個語音品質維度的模型效能：色彩、不連續性、響度、雜訊和 MOS。我們的研究結果顯示，儘管 AST 達到了更穩定的跨語言效能，但這兩種模型都表現出明顯的偏差。值得注意的是，普通話語音品質預測與人類 MOS 分數高度相關，而瑞典語和荷蘭語則呈現出更大的預測挑戰。不連續性在所有語言中仍然難以建模。這些結果凸顯了對更平衡的多語言資料集和特定於架構的調整的需求，以改善跨語言的概化。

##### **You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations**
2502.13001v1 by Frederic Kirstein, Muneeb Khan, Jan Philip Wahle, Terry Ruas, Bela Gipp

Meeting summarization suffers from limited high-quality data, mainly due to
privacy restrictions and expensive collection processes. We address this gap
with FAME, a dataset of 500 meetings in English and 300 in German produced by
MIMIC, our new multi-agent meeting synthesis framework that generates meeting
transcripts on a given knowledge source by defining psychologically grounded
participant profiles, outlining the conversation, and orchestrating a large
language model (LLM) debate. A modular post-processing step refines these
outputs, mitigating potential repetitiveness and overly formal tones, ensuring
coherent, credible dialogues at scale. We also propose a psychologically
grounded evaluation framework assessing naturalness, social behavior
authenticity, and transcript difficulties. Human assessments show that FAME
approximates real-meeting spontaneity (4.5/5 in naturalness), preserves
speaker-centric challenges (3/5 in spoken language), and introduces richer
information-oriented difficulty (4/5 in difficulty). These findings highlight
that FAME is a good and scalable proxy for real-world meeting conditions. It
enables new test scenarios for meeting summarization research and other
conversation-centric applications in tasks requiring conversation data or
simulating social scenarios under behavioral constraints.

摘要：會議摘要因缺乏高品質資料而受限，主要是由於隱私限制和昂貴的收集程序。我們透過 FAME 來解決這個差距，FAME 是 MIMIC 製作的 500 場英文會議和 300 場德文會議的資料集，MIMIC 是我們新的多重代理會議合成架構，透過定義心理基礎的參與者設定檔、概述對話，並協調大型語言模型 (LLM) 辯論，在給定的知識來源上產生會議記錄。模組化後處理步驟會改善這些輸出，減輕潛在的重複性和過於正式的語氣，確保大規模的對話連貫且可信。我們也提出一個心理基礎的評估架構，評估自然性、社交行為真實性，以及記錄難度。人類評估顯示，FAME 近似於真實會議的即興性（自然性 4.5/5），保留以講者為中心的挑戰（口語 3/5），並引入更豐富的資訊導向難度（難度 4/5）。這些發現強調 FAME 是真實世界會議條件的良好且可擴充的代理。它能為會議摘要研究和其他對話為中心的應用程式啟用新的測試情境，在需要對話資料或在行為限制下模擬社交情境的任務中。

##### **Personalized Top-k Set Queries Over Predicted Scores**
2502.12998v1 by Sohrab Namazi Nia, Subhodeep Ghosh, Senjuti Basu Roy, Sihem Amer-Yahia

This work studies the applicability of expensive external oracles such as
large language models in answering top-k queries over predicted scores. Such
scores are incurred by user-defined functions to answer personalized queries
over multi-modal data. We propose a generic computational framework that
handles arbitrary set-based scoring functions, as long as the functions could
be decomposed into constructs, each of which sent to an oracle (in our case an
LLM) to predict partial scores. At a given point in time, the framework assumes
a set of responses and their partial predicted scores, and it maintains a
collection of possible sets that are likely to be the true top-k. Since calling
oracles is costly, our framework judiciously identifies the next construct,
i.e., the next best question to ask the oracle so as to maximize the likelihood
of identifying the true top-k. We present a principled probabilistic model that
quantifies that likelihood. We study efficiency opportunities in designing
algorithms. We run an evaluation with three large scale datasets, scoring
functions, and baselines. Experiments indicate the efficacy of our framework,
as it achieves an order of magnitude improvement over baselines in requiring
LLM calls while ensuring result accuracy. Scalability experiments further
indicate that our framework could be used in large-scale applications.

摘要：本研究探討在預測分數中回答前 k 個查詢時，昂貴的外部預言（例如大型語言模型）的適用性。此類分數是由使用者定義的函式產生，用於回答多模態資料中的個人化查詢。我們提出一個通用的運算框架，用於處理任意基於集合的計分函式，只要這些函式可以分解為建構區塊，然後將每個建構區塊傳送給預言（在本例中為 LLM）以預測部分分數。在特定時間點，此框架假設一組回應及其部分預測分數，並維護一組可能成為真實前 k 個的集合。由於呼叫預言的成本很高，因此我們的框架會明智地找出下一個建構區塊，亦即下一個最佳問題，以詢問預言，以便最大化找出真實前 k 個的可能性。我們提出一個基於原理的機率模型，用於量化此可能性。我們研究設計演算法時的效率機會。我們針對三個大型資料集、計分函式和基準執行評估。實驗結果指出我們框架的效能，因為它在需要 LLM 呼叫的同時確保結果準確性，比基準進步了一個數量級。可擴充性實驗進一步指出我們的框架可用於大型應用程式。

##### **Eager Updates For Overlapped Communication and Computation in DiLoCo**
2502.12996v1 by Satyen Kale, Arthur Douillard, Yanislav Donchev

Distributed optimization methods such as DiLoCo have been shown to be
effective in training very large models across multiple distributed workers,
such as datacenters. These methods split updates into two parts: an inner
optimization phase, where the workers independently execute multiple
optimization steps on their own local data, and an outer optimization step,
where the inner updates are synchronized. While such approaches require orders
of magnitude less communication than standard data-parallel training, in
settings where the workers are datacenters, even the limited communication
requirements of these approaches can still cause significant slow downs due to
the blocking necessary at each outer optimization step. In this paper, we
investigate techniques to mitigate this issue by overlapping communication with
computation in a manner that allows the outer optimization step to fully
overlap with the inner optimization phase. We show that a particular variant,
dubbed eager updates, provides competitive performance with standard DiLoCo in
settings with low bandwidth between workers.

摘要：分散式優化方法（例如 DiLoCo）已被證明可有效訓練橫跨多個分散式工作者的超大型模型，例如資料中心。這些方法將更新拆分為兩部分：內部最佳化階段，其中工作者獨立地在自己的本地資料上執行多個最佳化步驟，以及外部最佳化步驟，其中內部更新會同步。雖然此類方法所需的通訊量比標準資料平行訓練少幾個數量級，但在工作者為資料中心的情況下，即使這些方法有限的通訊需求仍可能由於每個外部最佳化步驟所需的封鎖而導致顯著的減速。在本文中，我們探討了透過以允許外部最佳化步驟與內部最佳化階段完全重疊的方式將通訊與運算重疊，來減輕此問題的技術。我們展示了一個特定變體，稱為即時更新，在工作者之間頻寬較低的情況下，可提供與標準 DiLoCo 相當的效能。

##### **Free Argumentative Exchanges for Explaining Image Classifiers**
2502.12995v1 by Avinash Kori, Antonio Rago, Francesca Toni

Deep learning models are powerful image classifiers but their opacity hinders
their trustworthiness. Explanation methods for capturing the reasoning process
within these classifiers faithfully and in a clear manner are scarce, due to
their sheer complexity and size. We provide a solution for this problem by
defining a novel method for explaining the outputs of image classifiers with
debates between two agents, each arguing for a particular class. We obtain
these debates as concrete instances of Free Argumentative eXchanges (FAXs), a
novel argumentation-based multi-agent framework allowing agents to internalise
opinions by other agents differently than originally stated. We define two
metrics (consensus and persuasion rate) to assess the usefulness of FAXs as
argumentative explanations for image classifiers. We then conduct a number of
empirical experiments showing that FAXs perform well along these metrics as
well as being more faithful to the image classifiers than conventional,
non-argumentative explanation methods. All our implementations can be found at
https://github.com/koriavinash1/FAX.

摘要：深度學習模型是強大的影像分類器，但其不透明性阻礙了其可信度。由於其極高的複雜性和規模，忠實且清楚地捕捉這些分類器內部推理過程的解釋方法很少見。我們透過定義一種新穎的方法來解決這個問題，該方法透過兩個代理之間的辯論來解釋影像分類器的輸出，每個代理都主張一個特定類別。我們將這些辯論作為自由論證交換 (FAX) 的具體實例，這是一個新穎的基於論證的多代理架構，允許代理以不同於原始陳述的方式內化其他代理的意見。我們定義了兩個指標（共識率和說服率）來評估 FAX 作為影像分類器論證解釋的有用性。然後，我們進行了多項實證實驗，表明 FAX 在這些指標上表現良好，並且比傳統的非論證解釋方法更忠實於影像分類器。我們所有的實作都可以在 https://github.com/koriavinash1/FAX 中找到。

##### **B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability**
2502.12992v1 by Yifan Wang, Sukrut Rao, Ji-Ung Lee, Mayank Jobanputra, Vera Demberg

Post-hoc explanation methods for black-box models often struggle with
faithfulness and human interpretability due to the lack of explainability in
current neural models. Meanwhile, B-cos networks have been introduced to
improve model explainability through architectural and computational
adaptations, but their application has so far been limited to computer vision
models and their associated training pipelines. In this work, we introduce
B-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly
transforms pre-trained language models into B-cos LMs by combining B-cos
conversion and task fine-tuning, improving efficiency compared to previous
B-cos methods. Our automatic and human evaluation results demonstrate that
B-cos LMs produce more faithful and human interpretable explanations than post
hoc methods, while maintaining task performance comparable to conventional
fine-tuning. Our in-depth analysis explores how B-cos LMs differ from
conventionally fine-tuned models in their learning processes and explanation
patterns. Finally, we provide practical guidelines for effectively building
B-cos LMs based on our findings. Our code is available at
https://anonymous.4open.science/r/bcos_lm.

摘要：黑盒模型的事后解释方法通常会因为当前神经模型缺乏可解释性而难以做到忠实和人类可解释。与此同时，B-cos 网络已被引入，以通过架构和计算改编来提高模型的可解释性，但到目前为止，它们的应用仅限于计算机视觉模型及其相关的训练管道。在这项工作中，我们引入了 B-cos LM，即针对 NLP 任务增强的 B-cos 网络。我们的方法通过结合 B-cos 转换和任务微调，将预训练的语言模型直接转换为 B-cos LM，与以前 B-cos 方法相比，提高了效率。我们的自动和人工评估结果表明，与事后方法相比，B-cos LM 产生了更忠实和人类可解释的解释，同时保持与传统微调相当的任务性能。我们的深入分析探讨了 B-cos LM 在其学习过程和解释模式中与传统微调模型有何不同。最后，我们根据我们的发现提供了有效构建 B-cos LM 的实用指南。我们的代码可在 https://anonymous.4open.science/r/bcos_lm 获得。

##### **Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs**
2502.12988v1 by Zixiao Wang, Duzhen Zhang, Ishita Agrawal, Shen Gao, Le Song, Xiuying Chen

Previous approaches to persona simulation large language models (LLMs) have
typically relied on learning basic biographical information, or using limited
role-play dialogue datasets to capture a character's responses. However, a
holistic representation of an individual goes beyond surface-level facts or
conversations to deeper thoughts and thinking. In this work, we introduce
CharacterBot, a model designed to replicate both the linguistic patterns and
distinctive thought processes of a character. Using Lu Xun, a renowned Chinese
writer, as a case study, we propose four training tasks derived from his 17
essay collections. These include a pre-training task focused on mastering
external linguistic structures and knowledge, as well as three fine-tuning
tasks: multiple-choice question answering, generative question answering, and
style transfer, each aligning the LLM with Lu Xun's internal ideation and
writing style. To optimize learning across these tasks, we introduce a CharLoRA
parameter updating mechanism, where a general linguistic style expert
collaborates with other task-specific experts to better study both the language
style and the understanding of deeper thoughts. We evaluate CharacterBot on
three tasks for linguistic accuracy and opinion comprehension, demonstrating
that it significantly outperforms the baselines on our adapted metrics. We hope
that this work inspires future research on deep character persona simulation
LLM.

摘要：<paragraph>以前對角色模擬大型語言模型 (LLM) 的方法通常依賴於學習基本傳記資訊，或使用有限的角色扮演對話資料集來捕捉角色的反應。然而，對個人的整體表徵超越了表面層面的事實或對話，深入到更深層的想法和思考。在這項工作中，我們引入了 CharacterBot，一個旨在複製角色的語言模式和獨特思考過程的模型。以著名的中國作家魯迅為案例研究，我們提出了四個從他的 17 篇散文集中衍生的訓練任務。其中包括一個預訓練任務，專注於掌握外部語言結構和知識，以及三個微調任務：多選題回答、生成式問答和風格轉移，每個任務都將 LLM 與魯迅的內部觀念和寫作風格相結合。為了優化這些任務的學習，我們引入了一個 CharLoRA 參數更新機制，其中一位通曉語言風格的專家與其他特定任務專家合作，以更好地研究語言風格和對深層思想的理解。我們在三項任務上評估了 CharacterBot 的語言準確性和意見理解，證明它在我們調整的指標上顯著優於基準。我們希望這項工作能激勵未來對深度角色角色模擬 LLM 的研究。</paragraph>

##### **PartSDF: Part-Based Implicit Neural Representation for Composite 3D Shape Parametrization and Optimization**
2502.12985v1 by Nicolas Talabot, Olivier Clerc, Arda Cinar Demirtas, Doruk Oner, Pascal Fua

Accurate 3D shape representation is essential in engineering applications
such as design, optimization, and simulation. In practice, engineering
workflows require structured, part-aware representations, as objects are
inherently designed as assemblies of distinct components. However, most
existing methods either model shapes holistically or decompose them without
predefined part structures, limiting their applicability in real-world design
tasks. We propose PartSDF, a supervised implicit representation framework that
explicitly models composite shapes with independent, controllable parts while
maintaining shape consistency. Despite its simple single-decoder architecture,
PartSDF outperforms both supervised and unsupervised baselines in
reconstruction and generation tasks. We further demonstrate its effectiveness
as a structured shape prior for engineering applications, enabling precise
control over individual components while preserving overall coherence. Code
available at https://github.com/cvlab-epfl/PartSDF.

摘要：精確的 3D 形狀表示在工程應用中至關重要，例如設計、最佳化和模擬。實際上，工程工作流程需要結構化、零件感知的表示，因為物體本質上是設計為不同元件的組件。然而，大多數現有方法不是整體建模形狀，就是將其分解，而沒有預先定義的零件結構，這限制了它們在實際設計任務中的適用性。我們提出 PartSDF，一個監督式的隱式表示框架，它明確地使用獨立、可控的零件對複合形狀進行建模，同時保持形狀一致性。儘管其單一的解碼器架構很簡單，但 PartSDF 在重建和生成任務中都優於監督式和非監督式基準。我們進一步證明了其作為工程應用結構化形狀先驗的有效性，能夠精確控制各個元件，同時保持整體一致性。程式碼可在 https://github.com/cvlab-epfl/PartSDF 取得。

##### **Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs**
2502.12982v1 by Longxu Dou, Qian Liu, Fan Zhou, Changyu Chen, Zili Wang, Ziqi Jin, Zichen Liu, Tongyao Zhu, Cunxiao Du, Penghui Yang, Haonan Wang, Jiaheng Liu, Yongchi Zhao, Xiachong Feng, Xin Mao, Man Tsung Yeung, Kunat Pipatanakul, Fajri Koto, Min Si Thu, Hynek Kydlíček, Zeyi Liu, Qunshu Lin, Sittipong Sripaisarnmongkol, Kridtaphad Sae-Khow, Nirattisai Thongchim, Taechawat Konkaew, Narong Borijindargoon, Anh Dao, Matichon Maneegard, Phakphum Artkaew, Zheng-Xin Yong, Quan Nguyen, Wannaphong Phatthiyaphaibun, Hoang H. Tran, Mike Zhang, Shiqi Chen, Tianyu Pang, Chao Du, Xinyi Wan, Wei Lu, Min Lin

Sailor2 is a family of cutting-edge multilingual language models for
South-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit
diverse applications. Building on Qwen2.5, Sailor2 undergoes continuous
pre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to
support 13 SEA languages while retaining proficiency in Chinese and English.
Sailor2-20B model achieves a 50-50 win rate against GPT-4o across SEA
languages. We also deliver a comprehensive cookbook on how to develop the
multilingual model in an efficient manner, including five key aspects: data
curation, pre-training, post-training, model customization and evaluation. We
hope that Sailor2 model (Apache 2.0 license) will drive language development in
the SEA region, and Sailor2 cookbook will inspire researchers to build more
inclusive LLMs for other under-served languages.

摘要：Sailor2 是一系列針對東南亞 (SEA) 語言的尖端多語言語言模型，備有 1B、8B 和 20B 大小，以適應各種應用。在 Qwen2.5 的基礎上，Sailor2 持續進行 500B 代幣（400B SEA 專用和 100B 重播代幣）的預訓練，以支援 13 種 SEA 語言，同時保留中文和英文的熟練度。Sailor2-20B 模型在 SEA 語言中對抗 GPT-4o 時，達到 50-50 的獲勝率。我們還提供一本全面的食譜，說明如何以有效的方式開發多語言模型，包括五個關鍵方面：資料策展、預訓練、後訓練、模型自訂和評估。我們希望 Sailor2 模型（Apache 2.0 授權）將推動 SEA 地區的語言發展，而 Sailor2 食譜將激勵研究人員為其他服務不足的語言建立更具包容性的 LLM。

##### **Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking**
2502.12970v1 by Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha

The reasoning abilities of Large Language Models (LLMs) have demonstrated
remarkable advancement and exceptional performance across diverse domains.
However, leveraging these reasoning capabilities to enhance LLM safety against
adversarial attacks and jailbreak queries remains largely unexplored. To bridge
this gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that
integrates safety reflections of queries and responses into LLMs' generation
process, unlocking a safety-aware reasoning mechanism. This approach enables
self-evaluation at each reasoning step to create safety pivot tokens as
indicators of the response's safety status. Furthermore, in order to improve
the learning efficiency of pivot token prediction, we propose Contrastive Pivot
Optimization(CPO), which enhances the model's ability to perceive the safety
status of dialogues. Through this mechanism, LLMs dynamically adjust their
response strategies during reasoning, significantly enhancing their defense
capabilities against jailbreak attacks. Extensive experimental results
demonstrate that R2D effectively mitigates various attacks and improves overall
safety, highlighting the substantial potential of safety-aware reasoning in
strengthening LLMs' robustness against jailbreaks.

摘要：大型語言模型 (LLM) 的推理能力已展現出顯著的進步，並在不同的領域中表現出色。然而，利用這些推理能力來增強 LLM 對抗攻擊和越獄查詢的安全性仍然是未開發的領域。為了彌補這個差距，我們提出了推理防禦 (R2D)，這是一種新穎的訓練範例，它將查詢和回應的安全考量整合到 LLM 的生成過程中，開啟了一個安全感知推理機制。此方法可以在每個推理步驟中進行自我評估，以建立安全樞紐標記，作為回應安全狀態的指標。此外，為了提高樞紐標記預測的學習效率，我們提出了對比樞紐最佳化 (CPO)，它增強了模型感知對話安全狀態的能力。透過此機制，LLM 在推理過程中動態調整其回應策略，大幅增強其對抗越獄攻擊的防禦能力。廣泛的實驗結果證明，R2D 有效地減輕了各種攻擊，並改善了整體安全性，突顯了安全感知推理在加強 LLM 對抗越獄的穩健性方面的潛力。

##### **A Survey of Text Classification Under Class Distribution Shift**
2502.12965v1 by Adriana Valentina Costache, Silviu Florin Gheorghe, Eduard Gabriel Poesina, Paul Irofti, Radu Tudor Ionescu

The basic underlying assumption of machine learning (ML) models is that the
training and test data are sampled from the same distribution. However, in
daily practice, this assumption is often broken, i.e.~the distribution of the
test data changes over time, which hinders the application of conventional ML
models. One domain where the distribution shift naturally occurs is text
classification, since people always find new topics to discuss. To this end, we
survey research articles studying open-set text classification and related
tasks. We divide the methods in this area based on the constraints that define
the kind of distribution shift and the corresponding problem formulation,
i.e.~learning with the Universum, zero-shot learning, and open-set learning. We
next discuss the predominant mitigation approaches for each problem setup.
Finally, we identify several future work directions, aiming to push the
boundaries beyond the state of the art. Interestingly, we find that continual
learning can solve many of the issues caused by the shifting class
distribution. We maintain a list of relevant papers at
https://github.com/Eduard6421/Open-Set-Survey.

摘要：機器學習 (ML) 模型的基本假設是訓練資料和測試資料取樣自同一個分佈。然而，在日常實務中，這個假設經常被打破，也就是說測試資料的分布會隨著時間改變，這會阻礙傳統 ML 模型的應用。分佈轉移自然發生的其中一個領域是文字分類，因為人們總能找到新的主題來討論。為此，我們調查研究開放集文字分類和相關任務的研究文章。我們根據定義分佈轉移的類型和對應問題公式的限制，將這個領域的方法分為：使用 Universum 學習、零次學習和開放集學習。接下來，我們討論每個問題設定的主要緩解方法。最後，我們找出幾個未來的研究方向，目標是將界線推展到現有技術的極限之外。有趣的是，我們發現持續學習可以解決許多由類別分佈轉移所造成的議題。我們在 https://github.com/Eduard6421/Open-Set-Survey 維護一份相關論文清單。

##### **Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs**
2502.12964v1 by Adi Simhi, Itay Itzhak, Fazl Barez, Gabriel Stanovsky, Yonatan Belinkov

Large Language Models (LLMs) often generate outputs that lack grounding in
real-world facts, a phenomenon known as hallucinations. Prior research has
associated hallucinations with model uncertainty, leveraging this relationship
for hallucination detection and mitigation. In this paper, we challenge the
underlying assumption that all hallucinations are associated with uncertainty.
Using knowledge detection and uncertainty measurement methods, we demonstrate
that models can hallucinate with high certainty even when they have the correct
knowledge. We further show that high-certainty hallucinations are consistent
across models and datasets, distinctive enough to be singled out, and challenge
existing mitigation methods. Our findings reveal an overlooked aspect of
hallucinations, emphasizing the need to understand their origins and improve
mitigation strategies to enhance LLM safety. The code is available at
https://github.com/technion-cs-nlp/Trust_me_Im_wrong .

摘要：大型語言模型 (LLM) 經常產生缺乏真實世界事實根據的輸出，這種現象稱為幻覺。先前的研究已將幻覺與模型不確定性聯繫起來，利用這種關係進行幻覺偵測和緩解。在本文中，我們挑戰所有幻覺都與不確定性相關的基本假設。使用知識偵測和不確定性測量方法，我們證明模型即使擁有正確的知識，也能以高度確定性產生幻覺。我們進一步表明，高確定性幻覺在模型和資料集之間是一致的，足夠獨特以至於可以單獨挑選出來，並挑戰現有的緩解方法。我們的研究結果揭示了幻覺的一個被忽視的方面，強調需要了解其起源並改進緩解策略以增強 LLM 安全性。可以在 https://github.com/technion-cs-nlp/Trust_me_Im_wrong 找到程式碼。

##### **Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing**
2502.12962v1 by Xiaoju Ye, Zhichun Wang, Jingyuan Wang

Limited by the context window size of Large Language Models(LLMs), handling
various tasks with input tokens exceeding the upper limit has been challenging,
whether it is a simple direct retrieval task or a complex multi-hop reasoning
task. Although various methods have been proposed to enhance the long-context
processing capabilities of LLMs, they either incur substantial post-training
costs, or require additional tool modules(e.g.,RAG), or have not shown
significant improvement in realistic tasks. Our work observes the correlation
between the attention distribution and generated answers across each layer, and
establishes the attention allocation aligns with retrieval-augmented
capabilities through experiments. Drawing on the above insights, we propose a
novel method InfiniRetri that leverages the LLMs's own attention information to
enable accurate retrieval across inputs of infinitely length. Our evaluations
indicate that InfiniRetri achieves 100% accuracy in the
Needle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model,
surpassing other method or larger models and setting a new
state-of-the-art(SOTA). Moreover, our method achieves significant performance
improvements on real-world benchmarks, with a maximum 288% improvement. In
addition, InfiniRetri can be applied to any Transformer-based LLMs without
additional training and substantially reduces inference latency and compute
overhead in long texts. In summary, our comprehensive studies show
InfiniRetri's potential for practical applications and creates a paradigm for
retrievaling information using LLMs own capabilities under infinite-length
tokens. Code will be released in link.

摘要：受限于大型语言模型 (LLM) 的上下文窗口大小，处理超出上限的输入标记的各种任务一直具有挑战性，无论是简单的直接检索任务还是复杂的多跳推理任务。虽然已经提出了各种方法来增强 LLM 的长上下文处理能力，但它们要么产生大量的后训练成本，要么需要额外的工具模块（例如，RAG），要么在实际任务中没有显示出显着的改进。我们的工作观察了每层注意力分布和生成答案之间的相关性，并通过实验建立了注意力分配与检索增强能力保持一致。根据上述见解，我们提出了一种新方法 InfiniRetri，该方法利用 LLM 自身的注意力信息来实现对无限长度输入的准确检索。我们的评估表明，InfiniRetri 在使用 0.5B 参数模型对超过 100 万个标记的针头干草堆 (NIH) 测试中实现了 100% 的准确率，超越了其他方法或更大的模型，并创造了新的最先进 (SOTA)。此外，我们的方法在实际基准上实现了显著的性能提升，最大提升了 288%。此外，InfiniRetri 可以应用于任何基于 Transformer 的 LLM，而无需额外的训练，并且可以大幅减少推理延迟和长文本中的计算开销。总之，我们的综合研究表明了 InfiniRetri 在实际应用中的潜力，并为使用 LLM 自身能力在无限长度标记下检索信息创造了一个范例。代码将在链接中发布。

##### **Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger**
2502.12961v1 by Wenjun Li, Dexun Li, Kuicai Dong, Cong Zhang, Hao Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Liu

Large language models (LLMs) have shown remarkable emergent capabilities,
transforming the execution of functional tasks by leveraging external tools for
complex problems that require specialized processing or real-time data. While
existing research expands LLMs access to diverse tools (e.g., program
interpreters, search engines, weather/map apps), the necessity of using these
tools is often overlooked, leading to indiscriminate tool invocation. This
naive approach raises two key issues:(1) increased delays due to unnecessary
tool calls, and (2) potential errors resulting from faulty interactions with
external tools. In this paper, we introduce meta-cognition as a proxy for LLMs
self-assessment of their capabilities, representing the model's awareness of
its own limitations. Based on this, we propose MeCo, an adaptive
decision-making strategy for external tool use. MeCo quantifies metacognitive
scores by capturing high-level cognitive signals in the representation space,
guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs
minimal cost. Our experiments show that MeCo accurately detects LLMs' internal
cognitive signals and significantly improves tool-use decision-making across
multiple base models and benchmarks.

摘要：大型語言模型 (LLM) 已展現出顯著的新興能力，透過運用外部工具來執行功能任務，解決需要專業處理或即時資料的複雜問題，從而轉變任務的執行方式。儘管現有研究擴展了 LLM 對各種工具的存取（例如程式碼詮釋器、搜尋引擎、天氣/地圖應用程式），但使用這些工具的必要性往往被忽略，導致不加選擇地呼叫工具。這種天真的方法提出了兩個關鍵問題：(1) 由於不必要的工具呼叫而導致延遲增加，以及 (2) 由於與外部工具互動錯誤而導致的潛在錯誤。在本文中，我們將元認知引入作為 LLM 自我評估其能力的代理，代表模型意識到其自身的限制。基於此，我們提出了 MeCo，一種用於外部工具使用的適應性決策制定策略。MeCo 透過擷取表徵空間中的高階認知訊號來量化元認知分數，指導何時呼叫工具。值得注意的是，MeCo 是免微調的，而且成本極低。我們的實驗表明，MeCo 能夠準確地偵測 LLM 的內部認知訊號，並大幅改善跨多個基本模型和基準的工具使用決策制定。

##### **AlignFreeze: Navigating the Impact of Realignment on the Layers of Multilingual Models Across Diverse Languages**
2502.12959v1 by Steve Bakos, Félix Gaschi, David Guzmán, Riddhi More, Kelly Chutong Li, En-Shiun Annie Lee

Realignment techniques are often employed to enhance cross-lingual transfer
in multilingual language models, still, they can sometimes degrade performance
in languages that differ significantly from the fine-tuned source language.
This paper introduces AlignFreeze, a method that freezes either the layers'
lower half or upper half during realignment. Through controlled experiments on
4 tasks, 3 models, and in 35 languages, we find that realignment affects all
the layers but can be the most detrimental to the lower ones. Freezing the
lower layers can prevent performance degradation. Particularly, AlignFreeze
improves Part-of-Speech (PoS) tagging performances in languages where full
realignment fails: with XLM-R, it provides improvements of more than one
standard deviation in accuracy in seven more languages than full realignment.

摘要：重新對齊技術通常用於增強多語言語言模型中的跨語言轉移，然而，它們有時會降低與微調源語言顯著不同的語言的效能。本文介紹了 AlignFreeze，一種在重新對齊期間凍結層的下半部或上半部的的方法。透過 4 項任務、3 個模型和 35 種語言的受控實驗，我們發現重新對齊會影響所有層，但對較低層的影響最大。凍結較低層可以防止效能下降。特別是，AlignFreeze 改善了在完全重新對齊失敗的語言中的詞性 (PoS) 標記效能：使用 XLM-R，它比完全重新對齊在七種語言中提供了超過一個標準差的準確度改進。

##### **Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text**
2502.12953v1 by Andrei Jarca, Florinel Alin Croitoru, Radu Tudor Ionescu

Masked language modeling has become a widely adopted unsupervised technique
to pre-train language models. However, the process of selecting tokens for
masking is random, and the percentage of masked tokens is typically fixed for
the entire training process. In this paper, we propose to adjust the masking
ratio and to decide which tokens to mask based on a novel task-informed
anti-curriculum learning scheme. First, we harness task-specific knowledge
about useful and harmful tokens in order to determine which tokens to mask.
Second, we propose a cyclic decaying masking ratio, which corresponds to an
anti-curriculum schedule (from hard to easy). We exemplify our novel
task-informed anti-curriculum by masking (TIACBM) approach across three diverse
downstream tasks: sentiment analysis, text classification by topic, and
authorship attribution. Our findings suggest that TIACBM enhances the ability
of the model to focus on key task-relevant features, contributing to
statistically significant performance gains across tasks. We release our code
at https://github.com/JarcaAndrei/TIACBM.

摘要：遮蔽語言模型已成為一種廣泛採用的無監督技術，用於預先訓練語言模型。然而，選擇用於遮蔽的詞彙的過程是隨機的，且遮蔽詞彙的百分比通常在整個訓練過程中是固定的。在本文中，我們建議調整遮蔽率，並根據一種新穎的任務資訊反課程學習方案來決定要遮蔽哪些詞彙。首先，我們利用任務特定的知識，了解有用的和有害的詞彙，以確定要遮蔽哪些詞彙。其次，我們提出一個循環遞減遮蔽率，這對應於一個反課程表（從難到易）。我們以三項不同的下游任務為例，說明我們新穎的任務資訊反課程遮蔽（TIACBM）方法：情緒分析、按主題分類文字，以及作者歸屬。我們的研究結果表明，TIACBM 增強了模型專注於關鍵任務相關特徵的能力，有助於在各項任務中獲得具有統計意義的效能提升。我們在 https://github.com/JarcaAndrei/TIACBM 釋出我們的程式碼。

##### **Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for Improved Text-Based Learning for LGE Detection**
2502.12948v1 by Athira J Jacob, Puneet Sharma, Daniel Rueckert

Detection of hyperenhancement from cardiac LGE MRI images is a complex task
requiring significant clinical expertise. Although deep learning-based models
have shown promising results for the task, they require large amounts of data
with fine-grained annotations. Clinical reports generated for cardiac MR
studies contain rich, clinically relevant information, including the location,
extent and etiology of any scars present. Although recently developed
CLIP-based training enables pretraining models with image-text pairs, it
requires large amounts of data and further finetuning strategies on downstream
tasks. In this study, we use various strategies rooted in domain knowledge to
train a model for LGE detection solely using text from clinical reports, on a
relatively small clinical cohort of 965 patients. We improve performance
through the use of synthetic data augmentation, by systematically creating scar
images and associated text. In addition, we standardize the orientation of the
images in an anatomy-informed way to enable better alignment of spatial and
text features. We also use a captioning loss to enable fine-grained supervision
and explore the effect of pretraining of the vision encoder on performance.
Finally, ablation studies are carried out to elucidate the contributions of
each design component to the overall performance of the model.

摘要：從心臟 LGE MRI 影像偵測出過度增強是一項複雜的任務，需要顯著的臨床專業知識。儘管基於深度學習的模型已顯示出對這項任務有前景的結果，但它們需要大量具有細緻註解的資料。為心臟 MR 研究產生的臨床報告包含豐富且臨床上相關的資訊，包括任何疤痕的位置、範圍和病因。儘管最近開發的基於 CLIP 的訓練能使用影像文字對預訓練模型，但它需要大量資料和進一步微調下游任務的策略。在這項研究中，我們使用植基於領域知識的各種策略，僅使用來自臨床報告的文字，在一個相對較小的 965 名患者臨床群體中訓練一個 LGE 偵測模型。我們透過使用合成資料擴充來改善效能，系統性地建立疤痕影像和相關文字。此外，我們以解剖學告知的方式標準化影像方向，以使空間和文字特徵能更好地對齊。我們也使用標題損失來啟用細緻的監督，並探討視覺編碼器的預訓練對效能的影響。最後，進行消融研究以闡明每個設計元件對模型整體效能的貢獻。

##### **Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models**
2502.12947v1 by Gyeongman Kim, Gyouk Chu, Eunho Yang

With the emergence of Mixture-of-Experts (MoE), the efficient scaling of
model size has accelerated the development of large language models in recent
years. However, their high memory requirements prevent their use in
resource-constrained environments. While knowledge distillation (KD) has been a
proven method for model compression, its application to MoE teacher models
remains underexplored. Through our investigation, we discover that
non-activated experts in MoE models possess valuable knowledge that benefits
student models. We further demonstrate that existing KD methods are not optimal
for compressing MoE models, as they fail to leverage this knowledge
effectively. To address this, we propose two intuitive MoE-specific KD methods
for the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR),
both designed to effectively extract knowledge from all experts. Specifically,
KA augments knowledge by sampling experts multiple times, while SAR uses all
experts and adjusts the expert weights through router training to provide
optimal knowledge. Extensive experiments show that our methods outperform
conventional KD methods, demonstrating their effectiveness for MoE teacher
models.

摘要：隨著 Mixture-of-Experts (MoE) 的出現，模型規模的有效擴展加速了近年來大型語言模型的發展。然而，它們的高記憶體需求會阻礙它們在資源受限的環境中使用。雖然知識蒸餾 (KD) 已被證明是一種模型壓縮的方法，但它在 MoE 教師模型中的應用仍未被充分探索。透過我們的調查，我們發現 MoE 模型中未被啟用的專家擁有有價值的知識，這些知識對學生模型有益。我們進一步證明，現有的 KD 方法並非壓縮 MoE 模型的最佳方法，因為它們無法有效利用這些知識。為了解決這個問題，我們首次提出兩種直觀的 MoE 專用 KD 方法：知識擴充 (KA) 和學生感知路由器 (SAR)，兩者都旨在從所有專家有效提取知識。具體來說，KA 透過多次抽樣專家來擴充知識，而 SAR 使用所有專家並透過路由器訓練調整專家權重以提供最佳知識。廣泛的實驗表明，我們的模型優於傳統的 KD 模型，證明了它們對 MoE 教師模型的有效性。

##### **LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation**
2502.12945v1 by Junchen Fu, Xuri Ge, Kaiwen Zheng, Ioannis Arapakis, Xin Xin, Joemon M. Jose

Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold
significant commercial value. The rise of high-quality AI-generated content has
spurred interest in AI-driven micro-video creation. However, despite the
advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek
in text generation and reasoning, their potential to assist the creation of
popular micro-videos remains largely unexplored.
  In this paper, we conduct an empirical study on LLM-assisted popular
micro-video generation (LLMPopcorn). Specifically, we investigate the following
research questions: (i) How can LLMs be effectively utilized to assist popular
micro-video generation? (ii) To what extent can prompt-based enhancements
optimize the LLM-generated content for higher popularity? (iii) How well do
various LLMs and video generators perform in the popular micro-video generation
task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3
enable micro-video generation to achieve popularity comparable to human-created
content. Prompt enhancements further boost popularity, and benchmarking
highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and
HunyuanVideo lead in video generation. This pioneering work advances
AI-assisted micro-video creation, uncovering new research opportunities. We
will release the code and datasets to support future studies.

摘要：<paragraph>在 TikTok 和 YouTube 等平台上流行的微影片具有
重要的商业价值。高质量 AI 生成的内容的兴起
激发了人们对 AI 驱动的微影片创作的兴趣。然而，尽管大型语言模型 (LLM) 如 ChatGPT 和 DeepSeek
在文本生成和推理方面的能力很强，但它们在辅助创建
流行微影片方面的潜力在很大程度上仍未得到探索。
  在本文中，我们对 LLM 辅助的流行
微影片生成 (LLMPopcorn) 进行了实证研究。具体来说，我们调查了以下
研究问题：(i) 如何有效利用 LLM 来辅助流行
微影片生成？(ii) 基于提示的增强在多大程度上可以
优化 LLM 生成的内容以获得更高的流行度？(iii) 各种 LLM 和视频生成器在流行的微视频生成中表现如何
任务？通过探索这些问题，我们表明了像 DeepSeek-V3 这样的高级 LLM
使微视频生成能够达到与人类创作的内容相当的流行度。提示增强进一步提高了受欢迎程度，并且基准测试突出了 LLM 中的 DeepSeek-V3 和 DeepSeek-R1，而 LTX-Video 和
HunyuanVideo 在视频生成中领先。这项开创性的工作推进了
人工智能辅助的微视频创作，发现了新的研究机会。我们将发布代码和数据集以支持未来的研究。</paragraph>

##### **Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning in Low-Resource Languages**
2502.12932v1 by Salsabila Zahirah Pranida, Rifo Ahmad Genadi, Fajri Koto

Quantifying reasoning capability in low-resource languages remains a
challenge in NLP due to data scarcity and limited access to annotators. While
LLM-assisted dataset construction has proven useful for medium- and
high-resource languages, its effectiveness in low-resource languages,
particularly for commonsense reasoning, is still unclear. In this paper, we
compare three dataset creation strategies: (1) LLM-assisted dataset generation,
(2) machine translation, and (3) human-written data by native speakers, to
build a culturally nuanced story comprehension dataset. We focus on Javanese
and Sundanese, two major local languages in Indonesia, and evaluate the
effectiveness of open-weight and closed-weight LLMs in assisting dataset
creation through extensive manual validation. To assess the utility of
synthetic data, we fine-tune language models on classification and generation
tasks using this data and evaluate performance on a human-written test set. Our
findings indicate that LLM-assisted data creation outperforms machine
translation.

摘要：由於資料稀少且標註者有限，量化低資源語言中的推理能力在自然語言處理中仍然是一項挑戰。雖然 LLM 輔助的資料集建構已被證明對中高資源語言有用，但其在低資源語言中的有效性，特別是對於常識推理，仍然不清楚。在本文中，我們比較了三種資料集建立策略：(1) LLM 輔助的資料集生成，(2) 機器翻譯，以及 (3) 母語人士撰寫的人工資料，以建立具有文化細微差的故事理解資料集。我們專注於爪哇語和巽他語，這兩種印尼的主要地方語言，並透過廣泛的手動驗證評估開放權重和封閉權重 LLM 在協助資料集建立中的有效性。為了評估合成資料的效用，我們使用這些資料對分類和生成任務進行語言模型微調，並在人工撰寫的測試集上評估效能。我們的研究結果表明，LLM 輔助的資料建立優於機器翻譯。

##### **Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options**
2502.12929v1 by Lakshmi Nair, Ian Trase, Mark Kim

We present a novel reasoning approach called Flow-of-Options (FoO), designed
to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs
to systematically explore a diverse range of possibilities in their reasoning,
as demonstrated by an FoO-based agentic system for autonomously solving Machine
Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines,
achieving improvements of 38.2% - 69.2% on standard data science tasks, and
37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost
under $1 per task, our framework is well-suited for cost-sensitive
applications. Beyond classification and regression, we illustrate the broader
applicability of our FoO-based agentic system to tasks such as reinforcement
learning and image generation. Our framework presents significant advancements
compared to current state-of-the-art agentic systems for AutoML, due to the
benefits of FoO in enforcing diversity in LLM solutions through compressed,
explainable representations that also support long-term memory when combined
with case-based reasoning.

摘要：我們提出了一種稱為選項流 (FoO) 的新推理方法，旨在解決大型語言模型 (LLM) 中的內在偏差。FoO 使 LLM 能系統性地探索其推理中的各種可能性，這由一個基於 FoO 的代理系統展示，該系統可自主解決機器學習任務 (AutoML)。我們的框架優於最先進的基準，在標準數據科學任務上取得了 38.2% - 69.2% 的改進，在治療化學任務上取得了 37.4% - 47.9% 的改進。由於每個任務的整體運營成本低於 1 美元，因此我們的框架非常適合對成本敏感的應用。除了分類和回歸之外，我們還說明了基於 FoO 的代理系統在強化學習和圖像生成等任務中的更廣泛適用性。我們的框架與當前最先進的 AutoML 代理系統相比具有顯著的進步，這是因為 FoO 在通過壓縮、可解釋的表示強制 LLM 解決方案的多樣性方面具有優勢，這些表示與基於案例的推理結合時還支持長期記憶。

##### **Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts**
2502.12928v1 by Leiyu Pan, Zhenpeng Su, Minxuan Lv, Yizhe Xiong, Xiangwen Zhang, Zijia Lin, Hui Chen, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Deyi Xiong

Large language models have demonstrated exceptional performance across a wide
range of tasks. However, dense models usually suffer from sparse activation,
where many activation values tend towards zero (i.e., being inactivated). We
argue that this could restrict the efficient exploration of model
representation space. To mitigate this issue, we propose Finedeep, a
deep-layered fine-grained expert architecture for dense models. Our framework
partitions the feed-forward neural network layers of traditional dense models
into small experts, arranges them across multiple sub-layers. A novel routing
mechanism is proposed to determine each expert's contribution. We conduct
extensive experiments across various model sizes, demonstrating that our
approach significantly outperforms traditional dense architectures in terms of
perplexity and benchmark performance while maintaining a comparable number of
parameters and floating-point operations. Moreover, we find that Finedeep
achieves optimal results when balancing depth and width, specifically by
adjusting the number of expert sub-layers and the number of experts per
sub-layer. Empirical results confirm that Finedeep effectively alleviates
sparse activation and efficiently utilizes representation capacity in dense
models.

摘要：大型語言模型在各種任務中展現出非凡的效能。然而，密集模型通常會出現稀疏激活，其中許多激活值趨近於零（即處於非激活狀態）。我們認為這可能會限制模型表示空間的有效探索。為了減輕這個問題，我們提出 Finedeep，這是一種針對密集模型的深度分層細粒度專家架構。我們的框架將傳統密集模型的前饋神經網路層分割成小型專家，並將它們排列在多個子層中。我們提出了一種新穎的路由機制來確定每個專家的貢獻。我們針對各種模型大小進行了廣泛的實驗，證明我們的做法在困惑度和基準效能方面顯著優於傳統的密集架構，同時保持了相當數量的參數和浮點運算。此外，我們發現 Finedeep 在平衡深度和廣度時可以達到最佳結果，特別是透過調整專家子層的數量和每個子層的專家數量。實證結果證實，Finedeep 有效地減輕了稀疏激活，並有效利用了密集模型中的表示能力。

##### **SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems**
2502.12927v1 by Mike Zhang, Amalie Pernille Dilling, Léon Gondelman, Niels Erik Ruan Lyngdorf, Euan D. Lindsay, Johannes Bjerva

Providing high-quality feedback is crucial for student success but is
constrained by time, cost, and limited data availability. We introduce
Synthetic Educational Feedback Loops (SEFL), a novel framework designed to
deliver immediate, on-demand feedback at scale without relying on extensive,
real-world student data. In SEFL, two large language models (LLMs) operate in
teacher--student roles to simulate assignment completion and formative
feedback, generating abundant synthetic pairs of student work and corresponding
critiques. We then fine-tune smaller, more computationally efficient LLMs on
these synthetic pairs, enabling them to replicate key features of high-quality,
goal-oriented feedback. Unlike personalized tutoring approaches that offer
multi-turn, individualized instruction, SEFL specifically focuses on
replicating the teacher-->student feedback loop for diverse assignments.
Through both LLM-as-a-judge and human evaluations, we demonstrate that
SEFL-tuned models outperform their non-tuned counterparts in feedback quality,
clarity, and timeliness. These findings reveal SEFL's potential to transform
feedback processes for higher education and beyond, offering an ethical and
scalable alternative to conventional manual feedback cycles.

摘要：提供高品質的回饋對於學生的成功至關重要，但受到時間、成本和資料取得有限的限制。我們引入了合成教育回饋迴圈 (SEFL)，這是一個新穎的架構，旨在提供立即且依需求的回饋，且無需仰賴大量的真實世界學生資料。在 SEFL 中，兩個大型語言模型 (LLM) 以師生角色運作，模擬作業完成和形成性回饋，產生大量的合成學生作業和對應的評論。然後我們針對這些合成配對微調較小、計算效率較高的 LLM，讓它們能夠複製高品質、目標導向回饋的主要特徵。與提供多回合、個別化教學的個人化輔導方法不同，SEFL 特別專注於複製適用於各種作業的教師-->學生回饋迴圈。透過 LLM 作為評審和人類評估，我們證明了 SEFL 微調模型在回饋品質、清晰度和時效性方面優於未微調的模型。這些發現揭示了 SEFL 轉變高等教育及其他領域回饋流程的潛力，提供了一個符合道德且可擴充的替代方案，取代傳統的手動回饋週期。

##### **Towards more Contextual Agents: An extractor-Generator Optimization Framework**
2502.12926v1 by Mourad Aouini, Jinan Loubani

Large Language Model (LLM)-based agents have demonstrated remarkable success
in solving complex tasks across a wide range of general-purpose applications.
However, their performance often degrades in context-specific scenarios, such
as specialized industries or research domains, where the absence of
domain-relevant knowledge leads to imprecise or suboptimal outcomes. To address
this challenge, our work introduces a systematic approach to enhance the
contextual adaptability of LLM-based agents by optimizing their underlying
prompts-critical components that govern agent behavior, roles, and
interactions. Manually crafting optimized prompts for context-specific tasks is
labor-intensive, error-prone, and lacks scalability. In this work, we introduce
an Extractor-Generator framework designed to automate the optimization of
contextual LLM-based agents. Our method operates through two key stages: (i)
feature extraction from a dataset of gold-standard input-output examples, and
(ii) prompt generation via a high-level optimization strategy that iteratively
identifies underperforming cases and applies self-improvement techniques. This
framework substantially improves prompt adaptability by enabling more precise
generalization across diverse inputs, particularly in context-specific tasks
where maintaining semantic consistency and minimizing error propagation are
critical for reliable performance. Although developed with single-stage
workflows in mind, the approach naturally extends to multi-stage workflows,
offering broad applicability across various agent-based systems. Empirical
evaluations demonstrate that our framework significantly enhances the
performance of prompt-optimized agents, providing a structured and efficient
approach to contextual LLM-based agents.

摘要：大型語言模型 (LLM) 為基礎的代理已展現出非凡的成功，
能解決廣泛一般用途應用程式的複雜任務。
然而，它們的效能通常會在特定情境中下降，例如專門產業或研究領域，
其中缺乏與領域相關知識會導致不精確或次佳的結果。為了解決
這項挑戰，我們的研究引進了一種系統化的方法來增強 LLM 為基礎的代理的
情境適應性，方法是最佳化它們的基礎提示，這些提示是決定代理行為、角色和
互動的重要組成部分。手動製作最佳化的提示以應對特定情境的任務既費時又容易出錯，而且缺乏可擴充性。在這項研究中，我們引進
一個萃取產生器架構，旨在自動化情境 LLM 為基礎代理的最佳化。我們的
方法透過兩個關鍵階段運作：(i) 從黃金標準輸入輸出範例的資料集萃取特徵，以及
(ii) 透過高階最佳化策略產生提示，此策略會反覆找出表現不佳的案例並套用自我改善技術。此
架構大幅改善了提示適應性，讓它能針對不同的輸入進行更精確的概括，特別是在情境特定任務中，在這些任務中，維持語意一致性和將錯誤傳播降至最低對於可靠的效能至關重要。儘管是針對單階段工作流程開發，但此方法自然能延伸至多階段工作流程，在各種基於代理的系統中提供廣泛的適用性。實證評估顯示，我們的架構大幅增強了提示最佳化代理的效能，為基於情境的 LLM 代理提供了一個結構化且有效率的方法。

##### **Keep what you need : extracting efficient subnetworks from large audio representation models**
2502.12925v1 by David Genova, Philippe Esling, Tom Hurlin

Recently, research on audio foundation models has witnessed notable advances,
as illustrated by the ever improving results on complex downstream tasks.
Subsequently, those pretrained networks have quickly been used for various
audio applications. These improvements have however resulted in a considerable
increase both in size and complexity of these models. Along the environmental
concerns this issue raises, this prevents the deployment of such networks on
consumer-level devices, and precludes their use for real-time applications.
Moreover, this appears contradictory with the specificity of the tasks for
which these models are used, which are often simpler compared to extracting a
rich, multi-purpose representation from any type of audio data. In this paper,
we address this issue with a simple, yet effective method to extract
lightweight specialist subnetworks from large foundation models. Specifically,
we introduce learnable binary masks in-between the layers of a pretrained
representation model. When training the end-to-end model on a downstream task,
we add a sparsity-inducing loss to the overall objective, hence learning a
compact subnetwork specialized on a single task. Importantly, the weights of
the foundation model are kept frozen, resulting into low additional training
costs. Once trained, the masked computational units can then be removed from
the network, implying significant performance gains. We assess our method on
three widespread audio foundation models, each based on a different backbone
architecture, and illustrate its effectiveness on common audio representation
evaluation tasks, as well as its versatility on both speech, music, and general
audio. Code for reproducing the results and supporting webpage are available at
https://github.com/gnvIRCAM/Audio-representation-trimming

摘要：<paragraph>近期，音频基础模型的研究取得了显著进展，
复杂的下游任务上不断提升的结果证明了这一点。
随后，这些预训练网络已迅速用于各种
音频应用程序。然而，这些改进导致了这些模型的尺寸和复杂性都大幅
增加。除了由此产生的环境问题外，这也阻止了此类网络在
消费者级设备上的部署，并排除了它们在实时应用程序中的使用。
此外，这似乎与这些模型的使用任务的特殊性相矛盾，与从任何类型的音频数据中提取丰富的多用途表示相比，这些任务通常更简单。在本文中，
我们通过一种简单但有效的方法来解决此问题，从大型基础模型中提取轻量级专家子网络。具体来说，
我们在预训练表示模型的层之间引入了可学习的二进制掩码。当在某个下游任务上训练端到端模型时，
我们在总体目标中添加了稀疏性诱导损失，从而学习到专门用于单个任务的紧凑型子网络。重要的是，
基础模型的权重保持冻结，从而导致额外的训练成本低。一旦训练完成，就可以从网络中移除掩码的计算单元，这意味着性能将大幅提升。我们对三个广泛使用的音频基础模型评估了我们的方法，每个模型都基于不同的骨干架构，并说明了其在常见音频表示评估任务上的有效性，以及其在语音、音乐和通用音频上的多功能性。用于重现结果的代码和支持网页可在
https://github.com/gnvIRCAM/Audio-representation-trimming 获得</paragraph>

##### **Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded in Naturally Occurring Data**
2502.12924v1 by Maite Heredia, Gorka Labaka, Jeremy Barnes, Aitor Soroa

Code-switching (CS) is still a critical challenge in Natural Language
Processing (NLP). Current Large Language Models (LLMs) struggle to interpret
and generate code-switched text, primarily due to the scarcity of large-scale
CS datasets for training. This paper presents a novel methodology to generate
CS data using LLMs, and test it on the English-Spanish language pair. We
propose back-translating natural CS sentences into monolingual English, and
using the resulting parallel corpus to fine-tune LLMs to turn monolingual
sentences into CS. Unlike previous approaches to CS generation, our methodology
uses natural CS data as a starting point, allowing models to learn its natural
distribution beyond grammatical patterns. We thoroughly analyse the models'
performance through a study on human preferences, a qualitative error analysis
and an evaluation with popular automatic metrics. Results show that our
methodology generates fluent code-switched text, expanding research
opportunities in CS communication, and that traditional metrics do not
correlate with human judgement when assessing the quality of the generated CS
data. We release our code and generated dataset under a CC-BY-NC-SA license.

摘要：代碼轉換（CS）在自然語言處理（NLP）中仍是一個嚴峻的挑戰。目前的巨量語言模型（LLM）難以解讀和生成代碼轉換文字，主要是因為缺乏用於訓練的大規模 CS 資料集。本文提出了一種使用 LLM 生成 CS 資料的新方法，並在英語-西班牙語語言對上進行測試。我們建議將自然 CS 句子反向翻譯成單語英語，並使用產生的平行語料庫微調 LLM，將單語句子轉換為 CS。與先前的 CS 生成方法不同，我們的技術使用自然 CS 資料作為起點，讓模型能夠學習其超越語法模式的自然分佈。我們透過研究人類偏好、定性錯誤分析和使用流行的自動化指標進行評估，徹底分析模型的效能。結果顯示，我們的技術可以生成流利的代碼轉換文字，擴展 CS 溝通的研究機會，而且在評估生成的 CS 資料品質時，傳統指標與人類判斷無關。我們在 CC-BY-NC-SA 授權下釋出我們的程式碼和生成的資料集。

##### **On-Device LLMs for Home Assistant: Dual Role in Intent Detection and Response Generation**
2502.12923v1 by Rune Birkmose, Nathan Mørkeberg Reece, Esben Hofstedt Norvin, Johannes Bjerva, Mike Zhang

This paper investigates whether Large Language Models (LLMs), fine-tuned on
synthetic but domain-representative data, can perform the twofold task of (i)
slot and intent detection and (ii) natural language response generation for a
smart home assistant, while running solely on resource-limited, CPU-only edge
hardware. We fine-tune LLMs to produce both JSON action calls and text
responses. Our experiments show that 16-bit and 8-bit quantized variants
preserve high accuracy on slot and intent detection and maintain strong
semantic coherence in generated text, while the 4-bit model, while retaining
generative fluency, suffers a noticeable drop in device-service classification
accuracy. Further evaluations on noisy human (non-synthetic) prompts and
out-of-domain intents confirm the models' generalization ability, obtaining
around 80--86\% accuracy. While the average inference time is 5--6 seconds per
query -- acceptable for one-shot commands but suboptimal for multi-turn
dialogue -- our results affirm that an on-device LLM can effectively unify
command interpretation and flexible response generation for home automation
without relying on specialized hardware.

摘要：本文探討微調於合成但具領域代表性的資料上的大型語言模型 (LLM)，是否能執行 (i) 槽位和意圖偵測，以及 (ii) 自然語言回應產生的雙重任務，同時僅在資源受限、僅 CPU 的邊緣硬體上執行。我們微調 LLM 以產生 JSON 動作呼叫和文字回應。我們的實驗顯示，16 位元和 8 位元量化的變體在槽位和意圖偵測上保持高準確度，並在產生的文字中維持強大的語意一致性，而 4 位元模型雖然保有生成流暢度，但在裝置服務分類準確度上卻有明顯下降。進一步對有雜訊的人類 (非合成) 提示和領域外意圖的評估，證實了模型的泛化能力，獲得約 80--86% 的準確度。雖然平均推論時間為每個查詢 5--6 秒，對於一次性命令來說是可以接受的，但對於多輪對話來說並不理想，但我們的結果證實，裝置上的 LLM 可以有效地統一命令解譯和彈性回應產生，以進行家庭自動化，而無需依賴專用硬體。

##### **Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison**
2502.12921v1 by George-Kirollos Saad, Scott Sanner

Query-driven recommendation with unknown items poses a challenge for users to
understand why certain items are appropriate for their needs. Query-driven
Contrastive Summarization (QCS) is a methodology designed to address this issue
by leveraging language-based item descriptions to clarify contrasts between
them. However, existing state-of-the-art contrastive summarization methods such
as STRUM-LLM fall short of this goal. To overcome these limitations, we
introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs
debate-style prompting to generate focused and contrastive summarizations of
item aspects relevant to a query. Leveraging modern large language models
(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides
enhanced contrastive summaries. Experiments across three datasets demonstrate
that Q-STRUM Debate yields significant performance improvements over existing
methods on key contrastive summarization criteria, thus introducing a novel and
performant debate prompting methodology for QCS.

摘要：以未知項目進行的查詢驅動推薦對使用者來說是一項挑戰，他們難以理解為何某些項目適合自己的需求。查詢驅動對比摘要 (QCS) 是一種方法，旨在透過利用基於語言的項目描述來釐清項目之間的對比，以解決這個問題。然而，現有的最先進對比摘要方法（例如 STRUM-LLM）並未達成此目標。為了克服這些限制，我們引進 Q-STRUM Debate，一種 STRUM-LLM 的新延伸，它採用辯論式提示來產生與查詢相關的項目面向的重點式對比摘要。透過利用現代大型語言模型 (LLM) 作為產生辯論的強大工具，Q-STRUM Debate 提供增強的對比摘要。透過三個資料集的實驗證明，Q-STRUM Debate 在關鍵的對比摘要標準上，比現有方法有顯著的效能改善，因此為 QCS 引進一種新穎且高性能的辯論提示方法。

##### **GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning**
2502.12913v1 by Sifan Zhou, Shuo Wang, Zhihang Yuan, Mingjia Shi, Yuzhang Shang, Dawei Yang

Large Language Models (LLMs) fine-tuning technologies have achieved
remarkable results. However, traditional LLM fine-tuning approaches face
significant challenges: they require large Floating Point (FP) computation,
raising privacy concerns when handling sensitive data, and are impractical for
resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)
techniques reduce trainable parameters, their reliance on floating-point
arithmetic creates fundamental incompatibilities with edge hardware. In this
work, we introduce a novel framework for on-device LLM fine-tuning that
eliminates the need for floating-point operations in both inference and
training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer
format, which efficiently represents model parameters in integer format using
shared exponents among parameter groups. When combined with LoRA-like adapters,
this enables fully integer-based fine-tuning that is both memory and compute
efficient. We demonstrate that our approach achieves accuracy comparable to
FP16-based fine-tuning while significantly reducing memory usage (50%).
Moreover, compared to FP8, our method can reduce 5x power consumption and 11x
chip area with same performance, making large-scale model adaptation feasible
on edge devices.

摘要：大型语言模型 (LLM) 微调技术已取得显著成果。然而，传统的 LLM 微调方法面临着严峻的挑战：它们需要大量的浮点 (FP) 计算，在处理敏感数据时会引发隐私问题，并且对于资源受限的边缘设备而言不切实际。虽然参数高效微调 (PEFT) 技术减少了可训练参数，但它们对浮点运算的依赖与边缘硬件产生了根本上的不兼容性。在这项工作中，我们引入了一个用于设备上 LLM 微调的新框架，该框架消除了推理和训练中对浮点运算的需求，名为 GSQ-Tuning。其核心是组共享指数整数格式，该格式使用参数组之间的共享指数以整数格式有效地表示模型参数。当与类似 LoRA 的适配器相结合时，这实现了完全基于整数的微调，既节省内存又节省计算。我们证明了我们的方法实现了与基于 FP16 的微调相当的准确性，同时显著减少了内存使用量 (50%)。此外，与 FP8 相比，我们的方法可以在相同的性能下减少 5 倍的功耗和 11 倍的芯片面积，从而使大规模模型适应在边缘设备上成为可能。

##### **Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation**
2502.12911v1 by Zheng Yuan, Hao Chen, Zijin Hong, Qinggang Zhang, Feiran Huang, Xiao Huang

Generating SQLs from user queries is a long-standing challenge, where the
accuracy of initial schema linking significantly impacts subsequent SQL
generation performance. However, current schema linking models still struggle
with missing relevant schema elements or an excess of redundant ones. A crucial
reason for this is that commonly used metrics, recall and precision, fail to
capture relevant element missing and thus cannot reflect actual schema linking
performance. Motivated by this, we propose an enhanced schema linking metric by
introducing a restricted missing indicator. Accordingly, we introduce Knapsack
optimization-based Schema Linking Agent (KaSLA), a plug-in schema linking agent
designed to prevent the missing of relevant schema elements while minimizing
the inclusion of redundant ones. KaSLA employs a hierarchical linking strategy
that first identifies the optimal table linking and subsequently links columns
within the selected table to reduce linking candidate space. In each linking
process, it utilize a knapsack optimization approach to link potentially
relevant elements while accounting for a limited tolerance of potential
redundant ones.With this optimization, KaSLA-1.6B achieves superior schema
linking results compared to large-scale LLMs, including deepseek-v3 with
state-of-the-art (SOTA) schema linking method. Extensive experiments on Spider
and BIRD benchmarks verify that KaSLA can significantly improve the SQL
generation performance of SOTA text-to-SQL models by substituting their schema
linking processes.

摘要：從使用者查詢中產生 SQL 是個長期的挑戰，其中初始架構連結的準確性會顯著影響後續 SQL 產生效能。然而，目前的架構連結模型仍難以處理遺漏相關架構元素或過多重複元素的問題。造成此問題的一個關鍵原因是，常用的指標召回率和精確度無法捕捉遺漏相關元素，因此無法反映實際的架構連結效能。有鑑於此，我們提出一個增強的架構連結指標，透過引入受限遺漏指標。因此，我們介紹基於背包最佳化的架構連結代理 (KaSLA)，這是一個外掛式架構連結代理，旨在防止遺漏相關架構元素，同時將重複元素的納入降至最低。KaSLA 採用分層連結策略，首先找出最佳的表格連結，然後連結所選表格中的欄位，以減少連結候選空間。在每個連結過程中，它利用背包最佳化方法連結潛在相關元素，同時考量對潛在重複元素的容忍度。透過此最佳化，KaSLA-1.6B 達到優於大規模 LLM 的架構連結結果，包括採用最先進 (SOTA) 架構連結方法的 deepseek-v3。在 Spider 和 BIRD 基準上的廣泛實驗驗證，KaSLA 可透過取代其架構連結流程，大幅提升 SOTA 文字轉 SQL 模型的 SQL 產生效能。

##### **Graph Neural Networks for Databases: A Survey**
2502.12908v1 by Ziming Li, Youhuan Li, Yuyu Luo, Guoliang Li, Chuxu Zhang

Graph neural networks (GNNs) are powerful deep learning models for
graph-structured data, demonstrating remarkable success across diverse domains.
Recently, the database (DB) community has increasingly recognized the
potentiality of GNNs, prompting a surge of researches focusing on improving
database systems through GNN-based approaches. However, despite notable
advances, There is a lack of a comprehensive review and understanding of how
GNNs could improve DB systems. Therefore, this survey aims to bridge this gap
by providing a structured and in-depth overview of GNNs for DB systems.
Specifically, we propose a new taxonomy that classifies existing methods into
two key categories: (1) Relational Databases, which includes tasks like
performance prediction, query optimization, and text-to-SQL, and (2) Graph
Databases, addressing challenges like efficient graph query processing and
graph similarity computation. We systematically review key methods in each
category, highlighting their contributions and practical implications. Finally,
we suggest promising avenues for integrating GNNs into Database systems.

摘要：圖形神經網路 (GNN) 是用於圖形結構資料的強大深度學習模型，在各種領域中展現出顯著的成功。最近，資料庫 (DB) 社群越來越認識到 GNN 的潛力，促使大量研究專注於透過基於 GNN 的方法來改善資料庫系統。然而，儘管有顯著的進展，但對於 GNN 如何改善資料庫系統，仍然缺乏全面的回顧和理解。因此，本調查旨在透過提供 GNN 在資料庫系統中的結構化且深入的概觀來彌補這個差距。具體來說，我們提出了一個新的分類法，將現有方法分類為兩個主要類別：(1) 關係資料庫，其中包括效能預測、查詢最佳化和文字轉 SQL 等任務，以及 (2) 圖形資料庫，用於處理高效圖形查詢處理和圖形相似度計算等挑戰。我們系統性地回顧了每個類別中的關鍵方法，重點說明其貢獻和實務意涵。最後，我們建議將 GNN 整合到資料庫系統中的有希望途徑。

##### **Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements**
2502.12904v1 by Shu Yang, Shenzhe Zhu, Zeyu Wu, Keyu Wang, Junchi Yao, Junchao Wu, Lijie Hu, Mengdi Li, Derek F. Wong, Di Wang

We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to
defend against internet fraud and phishing in dynamic, real-world scenarios.
Fraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job
postings, social media, and news, categorized into 5 major fraud types. Unlike
previous benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to
assess LLMs' resistance to fraud at different stages, including credibility
building, urgency creation, and emotional manipulation. Furthermore, we
evaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM
provides general decision-making assistance, and 2. Role-play, where the model
assumes a specific persona, widely used in real-world agent-based interactions.
Our evaluation reveals the significant challenges in defending against fraud
and phishing inducement, especially in role-play settings and fake job
postings. Additionally, we observe a substantial performance gap between
Chinese and English, underscoring the need for improved multilingual fraud
detection capabilities.

摘要：我們推出 Fraud-R1，一個基準，旨在評估 LLM 在動態、真實世界場景中防範網路詐騙和網路釣魚的能力。Fraud-R1 包含 8,564 起詐騙案例，來源包括網路釣魚詐騙、虛假職缺、社群媒體和新聞，分類為 5 種類型的主要詐騙手法。與先前的基準不同，Fraud-R1 引入多輪評估管道，以評估 LLM 在不同階段對詐騙的抵抗力，包括建立信譽、製造急迫感和情感操縱。此外，我們在兩種設定下評估 15 個 LLM：1. 協助助理，其中 LLM 提供一般決策協助，以及 2. 角色扮演，其中模型假設特定角色，廣泛用於現實世界中基於代理的互動。我們的評估揭示了在防範詐騙和網路釣魚誘導方面面臨的重大挑戰，尤其是在角色扮演設定和虛假職缺中。此外，我們觀察到中文和英文之間有顯著的效能差距，這凸顯了改進多語言詐騙偵測功能的必要性。

##### **Soundwave: Less is More for Speech-Text Alignment in LLMs**
2502.12900v1 by Yuhao Zhang, Zhiheng Liu, Fan Bu, Ruiyu Zhang, Benyou Wang, Haizhou Li

Existing end-to-end speech large language models (LLMs) usually rely on
large-scale annotated data for training, while data-efficient training has not
been discussed in depth. We focus on two fundamental problems between speech
and text: the representation space gap and sequence length inconsistency. We
propose Soundwave, which utilizes an efficient training strategy and a novel
architecture to address these issues. Results show that Soundwave outperforms
the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks,
using only one-fiftieth of the training data. Further analysis shows that
Soundwave still retains its intelligence during conversation. The project is
available at https://github.com/FreedomIntelligence/Soundwave.

摘要：現有的端對端語音大型語言模型 (LLM) 通常依賴於大規模註釋資料進行訓練，而資料有效率的訓練尚未深入探討。我們專注於語音和文字之間的兩個基本問題：表示空間差距和序列長度不一致。我們提出 Soundwave，它利用高效的訓練策略和新穎的架構來解決這些問題。結果顯示，Soundwave 在語音翻譯和 AIR-Bench 語音任務中優於進階的 Qwen2-Audio，僅使用五十分之一的訓練資料。進一步的分析顯示，Soundwave 在對話中仍能保持其智慧。專案可於 https://github.com/FreedomIntelligence/Soundwave 取得。

##### **None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks**
2502.12896v1 by Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco

In LLM evaluations, reasoning is often distinguished from recall/memorization
by performing numerical variations to math-oriented questions. Here we
introduce a general variation method for multiple-choice questions that
completely dissociates the correct answer from previously seen tokens or
concepts, requiring LLMs to understand and reason (rather than memorizing) in
order to answer correctly. Using this method, we evaluate state-of-the-art
proprietary and open-source LLMs on two datasets available in English and
Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.
Results show that all models experience remarkable accuracy drops under our
proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access
2024, ranging from 10% to 93% across models. Notably, the most accurate model
in our experimentation (OpenAI-o3-mini) is not the most robust
(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may
not be the ones with better reasoning capabilities. Also, we see larger
accuracy drops in public (vs private) datasets and questions posed in their
original language (vs a manual translation), which are signs of contamination
and also point to a relevant role of recall/memorization in current LLMs'
answers.

摘要：在 LLM 評估中，推理通常透過對數學導向問題進行數值變異來區別於回憶/記憶。在此，我們引入一種通用變異方法，適用於多選題，它將正確答案與先前看到的代幣或概念完全區分開來，要求 LLM 理解和推理（而不是記憶），以便正確回答。使用此方法，我們在英語和西班牙語中評估了兩種數據集中的最先進的專有和開源 LLM：公共 MMLU 基準和私有 UNED-Access 2024 數據集。結果表明，在我們提出的變異下，所有模型的準確度都出現顯著下降，在 MMLU 上平均損失 57%，在 UNED-Access 2024 上平均損失 50%，在不同模型中範圍從 10% 到 93%。值得注意的是，我們實驗中最準確的模型（OpenAI-o3-mini）並不是最穩健的模型（DeepSeek-R1-70B），這表明標準評估中最好的模型可能不是推理能力最強的模型。此外，我們看到公共（相對於私有）數據集和以原始語言提出的問題（相對於人工翻譯）的準確度下降幅度更大，這是汙染的跡象，也表明回憶/記憶在當前 LLM 的答案中發揮著相關作用。

##### **Multilingual European Language Models: Benchmarking Approaches and Challenges**
2502.12895v1 by Fabio Barth, Georg Rehm

The breakthrough of generative large language models (LLMs) that can solve
different tasks through chat interaction has led to a significant increase in
the use of general benchmarks to assess the quality or performance of these
models beyond individual applications. There is also a need for better methods
to evaluate and also to compare models due to the ever increasing number of new
models published. However, most of the established benchmarks revolve around
the English language. This paper analyses the benefits and limitations of
current evaluation datasets, focusing on multilingual European benchmarks. We
analyse seven multilingual benchmarks and identify four major challenges.
Furthermore, we discuss potential solutions to enhance translation quality and
mitigate cultural biases, including human-in-the-loop verification and
iterative translation ranking. Our analysis highlights the need for culturally
aware and rigorously validated benchmarks to assess the reasoning and
question-answering capabilities of multilingual LLMs accurately.

摘要：生成式大型語言模型 (LLM) 的突破，它能透過聊天互動解決不同任務，這導致使用一般基準來評估這些模型在個別應用程式以外的品質或效能大幅增加。由於已發布的新模型數量不斷增加，因此也有必要採用更好的方法來評估模型並進行比較。然而，大多數已建立的基準都圍繞著英語。本文分析了目前評估資料集的優點和限制，重點放在多語言歐洲基準。我們分析了七個多語言基準，並找出四個主要的挑戰。此外，我們討論了增強翻譯品質和減輕文化偏見的潛在解決方案，包括人為迴圈驗證和反覆翻譯排名。我們的分析突顯了對文化意識和嚴格驗證的基準的需求，以準確評估多語言 LLM 的推理和問答能力。

##### **H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking**
2502.12893v1 by Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Da-Cheng Juan, Hai Li, Yiran Chen

Large Reasoning Models (LRMs) have recently extended their powerful reasoning
capabilities to safety checks-using chain-of-thought reasoning to decide
whether a request should be answered. While this new approach offers a
promising route for balancing model utility and safety, its robustness remains
underexplored. To address this gap, we introduce Malicious-Educator, a
benchmark that disguises extremely dangerous or malicious requests beneath
seemingly legitimate educational prompts. Our experiments reveal severe
security flaws in popular commercial-grade LRMs, including OpenAI o1/o3,
DeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1
model initially maintains a high refusal rate of about 98%, subsequent model
updates significantly compromise its safety; and attackers can easily extract
criminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any
additional tricks. To further highlight these vulnerabilities, we propose
Hijacking Chain-of-Thought (H-CoT), a universal and transferable attack method
that leverages the model's own displayed intermediate reasoning to jailbreak
its safety reasoning mechanism. Under H-CoT, refusal rates sharply
decline-dropping from 98% to below 2%-and, in some instances, even transform
initially cautious tones into ones that are willing to provide harmful content.
We hope these findings underscore the urgent need for more robust safety
mechanisms to preserve the benefits of advanced reasoning capabilities without
compromising ethical standards.

摘要：大型推理模型 (LRM) 最近將其強大的推理能力擴展到安全檢查，使用思維鏈推理來決定是否應回答請求。雖然這種新方法為平衡模型實用性和安全性提供了一條有希望的途徑，但其穩健性仍未得到充分探索。為了解決這一差距，我們引入了 Malicious-Educator，這是一個基準，它將極其危險或惡意的請求偽裝在看似合法的教育提示之下。我們的實驗揭示了流行的商業級 LRM 中嚴重的安全缺陷，包括 OpenAI o1/o3、DeepSeek-R1 和 Gemini 2.0 Flash Thinking。例如，儘管 OpenAI 的 o1 模型最初保持約 98% 的高拒絕率，但後續的模型更新顯著損害了其安全性；攻擊者可以輕鬆地從 DeepSeek-R1 和 Gemini 2.0 Flash Thinking 中提取犯罪策略，而無需任何額外的技巧。為了進一步強調這些漏洞，我們提出了劫持思維鏈 (H-CoT)，這是一種通用且可轉移的攻擊方法，它利用模型自己顯示的中間推理來越獄其安全推理機制。在 H-CoT 下，拒絕率急劇下降，從 98% 降至 2% 以下，在某些情況下，甚至將最初謹慎的語氣轉變為願意提供有害內容的語氣。我們希望這些發現強調了對更強大的安全機制的迫切需要，以保留先進推理能力的好處，同時不損害道德標準。

##### **Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?**
2502.12886v1 by Georg Rehm, Annika Grützner-Zahn, Fabio Barth

Large language models (LLMs) demonstrate unprecedented capabilities and
define the state of the art for almost all natural language processing (NLP)
tasks and also for essentially all Language Technology (LT) applications. LLMs
can only be trained for languages for which a sufficient amount of pre-training
data is available, effectively excluding many languages that are typically
characterised as under-resourced. However, there is both circumstantial and
empirical evidence that multilingual LLMs, which have been trained using data
sets that cover multiple languages (including under-resourced ones), do exhibit
strong capabilities for some of these under-resourced languages. Eventually,
this approach may have the potential to be a technological off-ramp for those
under-resourced languages for which "native" LLMs, and LLM-based technologies,
cannot be developed due to a lack of training data. This paper, which
concentrates on European languages, examines this idea, analyses the current
situation in terms of technology support and summarises related work. The
article concludes by focusing on the key open questions that need to be
answered for the approach to be put into practice in a systematic way.

摘要：大型語言模型 (LLM) 展現前所未有的能力，並定義了幾乎所有自然語言處理 (NLP) 任務以及所有語言技術 (LT) 應用的最新技術。LLM 只能針對有足夠預訓練資料可用的語言進行訓練，實際上排除了許多通常被歸類為資源不足的語言。然而，有環境和經驗證據顯示，多語言 LLM 已使用涵蓋多種語言（包括資源不足的語言）的資料集進行訓練，確實對其中一些資源不足的語言展現出強大的能力。最終，這種方法可能具有成為那些由於缺乏訓練資料而無法開發「原生」LLM 和基於 LLM 的技術的資源不足語言的技術跳板的潛力。本文專注於歐洲語言，探討這個想法，分析技術支援方面的現狀，並總結相關工作。本文最後專注於必須回答的主要開放性問題，以便系統性地實踐這種方法。

##### **How desirable is alignment between LLMs and linguistically diverse human users?**
2502.12884v1 by Pia Knoeferle, Sebastian Möller, Dorothea Kolossa, Veronika Solopova, Georg Rehm

We discuss how desirable it is that Large Language Models (LLMs) be able to
adapt or align their language behavior with users who may be diverse in their
language use. User diversity may come about among others due to i) age
differences; ii) gender characteristics, and/or iii) multilingual experience,
and associated differences in language processing and use. We consider
potential consequences for usability, communication, and LLM development.

摘要：我們探討大型語言模型 (LLM) 能夠適應或調整其語言行為，以適應語言使用可能多樣化的使用者，這有多麼可取。使用者多樣性可能出於以下原因而產生：i) 年齡差異；ii) 性別特徵，和/或 iii) 多語言經驗，以及語言處理和使用上的相關差異。我們考慮對可用性、溝通和 LLM 開發的潛在後果。

##### **Continuous Learning Conversational AI: A Personalized Agent Framework via A2C Reinforcement Learning**
2502.12876v1 by Nandakishor M, Anjali M

Creating personalized and adaptable conversational AI remains a key
challenge. This paper introduces a Continuous Learning Conversational AI (CLCA)
approach, implemented using A2C reinforcement learning, to move beyond static
Large Language Models (LLMs). We use simulated sales dialogues, generated by
LLMs, to train an A2C agent. This agent learns to optimize conversation
strategies for personalization, focusing on engagement and delivering value.
Our system architecture integrates reinforcement learning with LLMs for both
data creation and response selection. This method offers a practical way to
build personalized AI companions that evolve through continuous learning,
advancing beyond traditional static LLM techniques.

摘要：建立個人化且適應性強的對話式 AI 仍然是一項關鍵挑戰。本文介紹了一種持續學習對話式 AI (CLCA) 方法，透過 A2C 強化學習實作，以超越靜態大型語言模型 (LLM)。我們使用 LLM 生成的模擬銷售對話來訓練 A2C 代理。此代理會學習最佳化對話策略以實現個人化，並專注於參與和提供價值。我們的系統架構將強化學習與 LLM 整合，用於資料建立和回應選取。此方法提供了一種實用的方式來建立個人化 AI 伴侶，這些伴侶會透過持續學習而演進，超越傳統的靜態 LLM 技術。

##### **PAFT: Prompt-Agnostic Fine-Tuning**
2502.12859v1 by Chenxing Wei, Yao Shu, Mingwen Ou, Ying Tiffany He, Fei Richard Yu

While Large Language Models (LLMs) adapt well to downstream tasks after
fine-tuning, this adaptability often compromises prompt robustness, as even
minor prompt variations can significantly degrade performance. To address this,
we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach
that dynamically adjusts prompts during fine-tuning. This encourages the model
to learn underlying task principles rather than overfitting to specific prompt
formulations. PAFT operates in two stages: First, a diverse set of meaningful,
synthetic candidate prompts is constructed. Second, during fine-tuning, prompts
are randomly sampled from this set to create dynamic training inputs. Extensive
experiments across diverse datasets and LLMs demonstrate that models trained
with PAFT exhibit strong robustness and generalization across a wide range of
prompts, including unseen ones. This enhanced robustness improves both model
performance and inference speed while maintaining training efficiency. Ablation
studies further confirm the effectiveness of PAFT.

摘要：儘管大型語言模型 (LLM) 在微調後能很好地適應下游任務，但這種適應性通常會損害提示的穩健性，因為即使微小的提示變異也會大幅降低效能。為了解決這個問題，我們提出提示不可知微調 (PAFT)，這是一種簡單卻有效的方法，可以在微調期間動態調整提示。這鼓勵模型學習底層任務原則，而不是過度擬合特定的提示表述。PAFT 分為兩個階段運作：首先，構建一組多樣化、有意義的合成候選提示。其次，在微調期間，從此集合中隨機抽取提示以建立動態訓練輸入。針對各種資料集和 LLM 進行的廣泛實驗表明，使用 PAFT 訓練的模型在各種提示中表現出強大的穩健性和概括性，包括未見過的提示。這種增強的穩健性同時改善了模型效能和推理速度，同時維持訓練效率。消融研究進一步證實了 PAFT 的有效性。

##### **Rejected Dialects: Biases Against African American Language in Reward Models**
2502.12858v1 by Joel Mire, Zubin Trivadi Aysola, Daniel Chechelnitsky, Nicholas Deas, Chrysoula Zerva, Maarten Sap

Preference alignment via reward models helps build safe, helpful, and
reliable large language models (LLMs). However, subjectivity in preference
judgments and the lack of representative sampling in preference data collection
can introduce new biases, hindering reward models' fairness and equity. In this
work, we introduce a framework for evaluating dialect biases in reward models
and conduct a case study on biases against African American Language (AAL)
through several experiments comparing reward model preferences and behavior on
paired White Mainstream English (WME) and both machine-translated and
human-written AAL corpora. We show that reward models are less aligned with
human preferences when processing AAL texts vs. WME ones (-4\% accuracy on
average), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and
steer conversations toward WME, even when prompted with AAL texts. Our findings
provide a targeted analysis of anti-AAL biases at a relatively understudied
stage in LLM development, highlighting representational harms and ethical
questions about the desired behavior of LLMs concerning AAL.

摘要：透過獎勵模型進行偏好比對有助於建立安全、有用的可靠大型語言模型 (LLM)。然而，偏好判斷的主觀性，以及偏好資料收集中缺乏代表性抽樣，可能會引進新的偏誤，阻礙獎勵模型的公平性和公正性。在這項工作中，我們引進一個用於評估獎勵模型中方言偏誤的架構，並透過數個實驗進行案例研究，探討針對非裔美國人語言 (AAL) 的偏誤，這些實驗比較了獎勵模型偏好和行為，比較成對的白人主流英語 (WME) 與機器翻譯和人類撰寫的 AAL 語料庫。我們顯示，與處理 WME 文字相比，獎勵模型在處理 AAL 文字時與人類偏好較不一致（平均準確度降低 4%），經常不偏好與 AAL 一致的文字，而偏好與 WME 一致的文字，並將對話導向 WME，即使提示的是 AAL 文字。我們的發現針對 LLM 開發中相對未受重視的階段，提供針對反 AAL 偏誤的目標分析，強調與表徵相關的危害和關於 LLM 對 AAL 的期望行為的倫理問題。

##### **Integrating Arithmetic Learning Improves Mathematical Reasoning in Smaller Models**
2502.12855v1 by Neeraj Gangwar, Suma P Bhat, Nickvash Kani

While large models pre-trained on high-quality data exhibit excellent
performance across various reasoning tasks, including mathematical reasoning
(e.g. GSM8k, MultiArith), specializing smaller models to excel at mathematical
reasoning remains a challenging problem. Common approaches to address this
challenge include knowledge distillation, where smaller student models learn
from large pre-trained teacher models, and data augmentation, such as
rephrasing questions. Despite these efforts, smaller models struggle with
arithmetic computations, leading to errors in mathematical reasoning. In this
work, we focus on leveraging a programmatically generated arithmetic dataset to
enhance the reasoning capabilities of smaller models. We investigate two key
approaches to incorporate this dataset -- (1) intermediate fine-tuning, where a
model is fine-tuned on the arithmetic dataset before being trained on a
reasoning dataset, and (2) integrating the arithmetic dataset into the
instruction-tuning mixture, allowing the model to learn arithmetic skills
alongside general instruction-following abilities. Our experiments on multiple
reasoning benchmarks demonstrate that incorporating an arithmetic dataset,
whether through targeted fine-tuning or within the instruction-tuning mixture,
enhances the models' arithmetic capabilities, which in turn improves their
mathematical reasoning performance.

摘要：大型模型经过针对高质量数据的预训练，在各种推理任务中表现出色，包括数学推理（例如 GSM8k、MultiArith），但专门化小型模型以擅长数学推理仍然是一个具有挑战性的问题。解决这一挑战的常见方法包括知识蒸馏，其中较小的学生模型从经过预训练的大型教师模型中学习，以及数据增强，例如重新表述问题。尽管做出了这些努力，较小的模型在算术计算中仍然存在困难，从而导致数学推理错误。在这项工作中，我们专注于利用程序化生成的算术数据集来增强较小模型的推理能力。我们研究了两种关键方法来合并此数据集——（1）中间微调，其中模型在算术数据集上进行微调，然后在推理数据集上进行训练，以及（2）将算术数据集集成到指令微调混合中，允许模型学习算术技能以及一般的指令遵循能力。我们在多个推理基准上的实验表明，通过有针对性的微调或在指令微调混合中合并算术数据集，增强了模型的算术能力，进而提高了它们的数学推理性能。

##### **S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning**
2502.12853v1 by Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, Jia Li

Recent studies have demonstrated the effectiveness of LLM test-time scaling.
However, existing approaches to incentivize LLMs' deep thinking abilities
generally require large-scale data or significant training efforts. Meanwhile,
it remains unclear how to improve the thinking abilities of less powerful base
models. In this work, we introduce S$^2$R, an efficient framework that enhances
LLM reasoning by teaching models to self-verify and self-correct during
inference. Specifically, we first initialize LLMs with iterative
self-verification and self-correction behaviors through supervised fine-tuning
on carefully curated data. The self-verification and self-correction skills are
then further strengthened by both outcome-level and process-level reinforcement
learning, with minimized resource requirements, enabling the model to
adaptively refine its reasoning process during inference. Our results
demonstrate that, with only 3.1k self-verifying and self-correcting behavior
initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from
51.0\% to 81.6\%, outperforming models trained on an equivalent amount of
long-CoT distilled data. Extensive experiments and analysis based on three base
models across both in-domain and out-of-domain benchmarks validate the
effectiveness of S$^2$R. Our code and data are available at
https://github.com/NineAbyss/S2R.

摘要：<paragraph>最近的研究表明了 LLM 测试时间扩展的有效性。
然而，现有激励 LLM 深度思考能力的方法
通常需要大规模数据或大量的训练工作。同时，
如何提高较弱基础模型的思考能力仍然不清楚。在这项工作中，我们引入了 S$^2$R，一个通过教导模型在
推理过程中进行自我验证和自我纠正来增强 LLM 推理的有效框架。具体来说，我们首先通过监督微调对精心整理的数据来初始化具有迭代自我验证和自我纠正行为的 LLM。然后通过结果级别和过程级别的强化
学习进一步加强自我验证和自我纠正技能，同时最大程度地减少资源需求，使模型能够
在推理过程中自适应地优化其推理过程。我们的结果
表明，仅使用 3.1k 个自我验证和自我纠正行为
初始化样本，Qwen2.5-math-7B 的准确率从
51.0% 提高到 81.6%，优于在等量长 CoT 蒸馏数据上训练的模型。基于三个基础模型在域内和域外基准上的广泛实验和分析验证了
S$^2$R 的有效性。我们的代码和数据可以在
https://github.com/NineAbyss/S2R 获得。</paragraph>

##### **MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching**
2502.12852v1 by Fabian David Schmidt, Florian Schneider, Chris Biemann, Goran Glavaš

Existing multilingual vision-language (VL) benchmarks often only cover a
handful of languages. Consequently, evaluations of large vision-language models
(LVLMs) predominantly target high-resource languages, underscoring the need for
evaluation data for low-resource languages. To address this limitation, we
introduce MVL-SIB, a massively multilingual vision-language benchmark that
evaluates both cross-modal and text-only topical matching across 205 languages
-- over 100 more than the most multilingual existing VL benchmarks encompass.
We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini)
on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic
matching in lower-resource languages, performing no better than chance on
languages like N'Koo. Our analysis further reveals that VL support in LVLMs
declines disproportionately relative to textual support for lower-resource
languages, as evidenced by comparison of cross-modal and text-only topical
matching performance. We further observe that open-weight LVLMs do not benefit
from representing a topic with more than one image, suggesting that these
models are not yet fully effective at handling multi-image tasks. By
correlating performance on MVL-SIB with other multilingual VL benchmarks, we
highlight that MVL-SIB serves as a comprehensive probe of multilingual VL
understanding in LVLMs.

摘要：現有的多語言視覺語言 (VL) 基準通常只涵蓋少數語言。因此，大型視覺語言模型 (LVLMs) 的評估主要針對資源豐富的語言，強調了對資源匱乏語言的評估資料的需求。為了解決此限制，我們引入了 MVL-SIB，一個大規模的多語言視覺語言基準，它評估了 205 種語言的跨模態和純文字主題匹配，比現有的多語言 VL 基準涵蓋的語言多出 100 多種。然後，我們在 MVL-SIB 上對一系列開放權重的 LVLMs 與 GPT-4o(-mini) 進行了基準測試。我們的結果表明，LVLMs 在資源較少的語言中難以進行跨模態主題匹配，在 N'Koo 等語言上的表現不比隨機好。我們的分析進一步表明，LVLMs 中的 VL 支援相對於資源較少的語言的文字支援下降得不成比例，這從跨模態和純文字主題匹配效能的比較中可以看出。我們進一步觀察到，開放權重的 LVLMs 無法從用多於一張影像來表示主題中受益，這表明這些模型在處理多影像任務方面尚未完全有效。通過將 MVL-SIB 上的效能與其他多語言 VL 基準相關聯，我們強調 MVL-SIB 可作為 LVLMs 中多語言 VL 理解的綜合探測。

##### **MeMo: Towards Language Models with Associative Memory Mechanisms**
2502.12851v1 by Fabio Massimo Zanzotto, Elena Sofia Ruzzetti, Giancarlo A. Xompero, Leonardo Ranaldi, Davide Venditti, Federico Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli

Memorization is a fundamental ability of Transformer-based Large Language
Models, achieved through learning. In this paper, we propose a paradigm shift
by designing an architecture to memorize text directly, bearing in mind the
principle that memorization precedes learning. We introduce MeMo, a novel
architecture for language modeling that explicitly memorizes sequences of
tokens in layered associative memories. By design, MeMo offers transparency and
the possibility of model editing, including forgetting texts. We experimented
with the MeMo architecture, showing the memorization power of the one-layer and
the multi-layer configurations.

摘要：記憶是 Transformer 大型語言模型的基本能力，可透過學習達成。在本文中，我們提出一個典範轉移，透過設計一個架構來直接記憶文字，並牢記記憶先於學習的原則。我們導入 MeMo，一個新穎的語言建模架構，可明確地記憶分層關聯式記憶中的代幣序列。透過設計，MeMo 提供透明度和模型編輯的可能性，包括遺忘文字。我們實驗了 MeMo 架構，展示了單層和多層組態的記憶力。

##### **Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols**
2502.12842v1 by Kathrin Seßler, Arne Bewersdorff, Claudia Nerdel, Enkelejda Kasneci

Effective feedback is essential for fostering students' success in scientific
inquiry. With advancements in artificial intelligence, large language models
(LLMs) offer new possibilities for delivering instant and adaptive feedback.
However, this feedback often lacks the pedagogical validation provided by
real-world practitioners. To address this limitation, our study evaluates and
compares the feedback quality of LLM agents with that of human teachers and
science education experts on student-written experimentation protocols. Four
blinded raters, all professionals in scientific inquiry and science education,
evaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and
3) the science education experts using a five-point Likert scale based on six
criteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive
Tone, Linguistic Clarity, and Technical Terminology. Our results indicate that
LLM-generated feedback shows no significant difference to that of teachers and
experts in overall quality. However, the LLM agent's performance lags in the
Feed Back dimension, which involves identifying and explaining errors within
the student's work context. Qualitative analysis highlighted the LLM agent's
limitations in contextual understanding and in the clear communication of
specific errors. Our findings suggest that combining LLM-generated feedback
with human expertise can enhance educational practices by leveraging the
efficiency of LLMs and the nuanced understanding of educators.

摘要：有效的回饋對於培養學生在科學探究中的成功至關重要。隨著人工智慧的進步，大型語言模型 (LLM) 為提供即時且適應性的回饋提供了新的可能性。然而，此回饋通常缺乏實際從業者提供的教學驗證。為了解決此限制，我們的研究評估並比較了 LLM 代理與人類教師和科學教育專家在學生撰寫的實驗協定上的回饋品質。四位盲評者，皆為科學探究和科學教育專業人士，使用基於六個有效回饋準則的五點李克特量表評估由 1) LLM 代理、2) 教師和 3) 科學教育專家產生的回饋文字：鼓勵、回饋、前饋、建設性語氣、語言清晰度和技術術語。我們的結果表明，LLM 產生的回饋在整體品質上與教師和專家產生的回饋沒有顯著差異。然而，LLM 代理的表現落後於回饋面向，這涉及在學生的作業背景中識別和解釋錯誤。定性分析突顯了 LLM 代理在情境理解和明確傳達特定錯誤方面的限制。我們的研究結果表明，將 LLM 產生的回饋與人類專業知識相結合，可以透過利用 LLM 的效率和教育者的細緻理解來提升教育實務。

##### **Towards Equitable AI: Detecting Bias in Using Large Language Models for Marketing**
2502.12838v1 by Berk Yilmaz, Huthaifa I. Ashqar

The recent advances in large language models (LLMs) have revolutionized
industries such as finance, marketing, and customer service by enabling
sophisticated natural language processing tasks. However, the broad adoption of
LLMs brings significant challenges, particularly in the form of social biases
that can be embedded within their outputs. Biases related to gender, age, and
other sensitive attributes can lead to unfair treatment, raising ethical
concerns and risking both company reputation and customer trust. This study
examined bias in finance-related marketing slogans generated by LLMs (i.e.,
ChatGPT) by prompting tailored ads targeting five demographic categories:
gender, marital status, age, income level, and education level. A total of
1,700 slogans were generated for 17 unique demographic groups, and key terms
were categorized into four thematic groups: empowerment, financial, benefits
and features, and personalization. Bias was systematically assessed using
relative bias calculations and statistically tested with the Kolmogorov-Smirnov
(KS) test against general slogans generated for any individual. Results
revealed that marketing slogans are not neutral; rather, they emphasize
different themes based on demographic factors. Women, younger individuals,
low-income earners, and those with lower education levels receive more distinct
messaging compared to older, higher-income, and highly educated individuals.
This underscores the need to consider demographic-based biases in AI-generated
marketing strategies and their broader societal implications. The findings of
this study provide a roadmap for developing more equitable AI systems,
highlighting the need for ongoing bias detection and mitigation efforts in
LLMs.

摘要：大型語言模型 (LLM) 的最新進展徹底改變了金融、行銷和客戶服務等產業，因為它能執行複雜的自然語言處理任務。然而，LLM 的廣泛採用帶來重大的挑戰，特別是潛藏在其輸出結果中的社會偏見形式。與性別、年齡和其他敏感屬性相關的偏見可能導致不公平的待遇，引發道德問題，並危及公司聲譽和客戶信任。本研究探討了 LLM（即 ChatGPT）產生的與金融相關的行銷標語中的偏見，方法是針對五個人口統計類別：性別、婚姻狀況、年齡、收入水準和教育水準，提示量身打造的廣告。總共為 17 個獨特的人口統計群組產生了 1,700 個標語，並且關鍵詞被分類為四個主題群組：賦權、財務、好處和功能，以及個人化。偏見使用相對偏見計算進行系統性評估，並使用科爾莫哥洛夫-史米諾夫 (KS) 檢定與針對任何個人產生的通用標語進行統計檢定。結果顯示行銷標語並非中立；相反地，它們根據人口統計因素強調不同的主題。與年紀較大、收入較高和受教育程度較高的個人相比，女性、年輕人、低收入者和教育程度較低者接收到的訊息更為不同。這強調了在 AI 生成的行銷策略中考量基於人口統計的偏見及其更廣泛的社會影響的必要性。本研究的發現提供了開發更公平 AI 系統的路線圖，突顯了在 LLM 中持續進行偏見偵測和緩解工作的重要性。

##### **An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation**
2502.12836v1 by Mohammad Feli, Iman Azimi, Pasi Liljeberg, Amir M. Rahmani

Large language models (LLMs) are revolutionizing healthcare by improving
diagnosis, patient care, and decision support through interactive
communication. More recently, they have been applied to analyzing physiological
time-series like wearable data for health insight extraction. Existing methods
embed raw numerical sequences directly into prompts, which exceeds token limits
and increases computational costs. Additionally, some studies integrated
features extracted from time-series in textual prompts or applied multimodal
approaches. However, these methods often produce generic and unreliable outputs
due to LLMs' limited analytical rigor and inefficiency in interpreting
continuous waveforms. In this paper, we develop an LLM-powered agent for
physiological time-series analysis aimed to bridge the gap in integrating LLMs
with well-established analytical tools. Built on the OpenCHA, an open-source
LLM-powered framework, our agent features an orchestrator that integrates user
interaction, data sources, and analytical tools to generate accurate health
insights. To evaluate its effectiveness, we implement a case study on heart
rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of
PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study.
The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o,
with ECG serving as the gold standard for HR estimation. Results demonstrate
that our agent significantly outperforms benchmark models by achieving lower
error rates and more reliable HR estimations. The agent implementation is
publicly available on GitHub.

摘要：大型語言模型 (LLM) 透過互動式溝通，改善診斷、病人照護和決策支援，進而革新醫療保健。最近，它們已應用於分析生理時間序列，例如可穿戴式裝置的資料，以萃取健康見解。現有方法會將原始數值序列直接嵌入提示中，這會超過權杖限制並增加運算成本。此外，一些研究將從時間序列中萃取的特徵整合到文字提示中，或應用多模態方法。然而，由於 LLM 在解譯連續波形時分析嚴謹度有限且效率不彰，這些方法經常產生通用且不可靠的輸出。在本文中，我們開發了一個由 LLM 驅動的代理，用於生理時間序列分析，旨在彌合將 LLM 與既有分析工具整合的差距。我們的代理建立在 OpenCHA（一個由 LLM 驅動的開源架構）之上，具備一個整合使用者互動、資料來源和分析工具的協調器，以產生準確的健康見解。為了評估其有效性，我們實作了一個案例研究，從遠距健康監測研究中的一組光電容積描記圖 (PPG) 和心電圖 (ECG) 記錄中估算心率 (HR)。該代理的效能與 OpenAI GPT-4o-mini 和 GPT-4o 進行基準測試，其中 ECG 作為 HR 估算的金標準。結果顯示，我們的代理透過達成較低的錯誤率和更可靠的 HR 估算，顯著優於基準模型。該代理實作已公開在 GitHub 上。

##### **Subword models struggle with word learning, but surprisal hides it**
2502.12835v1 by Bastian Bunzeck, Sina Zarrieß

We study word learning in subword and character language models with the
psycholinguistic lexical decision task. While subword LMs struggle to discern
words and non-words with high accuracy, character LMs solve this task easily
and consistently. Furthermore, when comparing word learning and syntactic
learning, both processes are separable in character LM where word learning
predates syntactic learning, whereas these processes are simultaneous in
subword LM. This raises questions about the adequacy of subword LMs for
modeling language acquisition and positions character LMs as a viable
alternative.

摘要：我們使用心理語言學的詞彙決策任務研究在子詞和字元語言模型中的詞彙學習。儘管子詞語言模型難以區分單詞和非單詞，但字元語言模型可以輕鬆且一致地解決此任務。此外，在比較單詞學習和句法學習時，這兩個過程在字元語言模型中是可分離的，其中單詞學習先於句法學習，而這些過程在子詞語言模型中是同時發生的。這引發了關於子詞語言模型對語言習得建模的充分性的問題，並將字元語言模型定位為可行的替代方案。

##### **KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan**
2502.12829v1 by Mukhammed Togmanov, Nurdaulet Mukhituly, Diana Turmakhan, Jonibek Mansurov, Maiya Goloburda, Akhmed Sakip, Zhuohan Xie, Yuxia Wang, Bekassyl Syzdykov, Nurkhan Laiyk, Alham Fikri Aji, Ekaterina Kochmar, Preslav Nakov, Fajri Koto

Despite having a population of twenty million, Kazakhstan's culture and
language remain underrepresented in the field of natural language processing.
Although large language models (LLMs) continue to advance worldwide, progress
in Kazakh language has been limited, as seen in the scarcity of dedicated
models and benchmark evaluations. To address this gap, we introduce KazMMLU,
the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU
comprises 23,000 questions that cover various educational levels, including
STEM, humanities, and social sciences, sourced from authentic educational
materials and manually validated by native speakers and educators. The dataset
includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting
Kazakhstan's bilingual education system and rich local context. Our evaluation
of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4,
and DeepSeek V3) demonstrates substantial room for improvement, as even the
best-performing models struggle to achieve competitive performance in Kazakh
and Russian. These findings underscore significant performance gaps compared to
high-resource languages. We hope that our dataset will enable further research
and development of Kazakh-centric LLMs. Data and code will be made available
upon acceptance.

摘要：儘管哈薩克人口達兩千萬，但哈薩克的文化和語言在自然語言處理領域仍未得到充分的重視。儘管大型語言模型 (LLM) 在全球持續進步，但哈薩克語的進展卻十分有限，這從專用模型和基準評估的稀缺性中可見一斑。為了解決這個差距，我們引入了 KazMMLU，這是第一個專門為哈薩克語設計的 MMLU 風格資料集。KazMMLU 包含 23,000 個問題，涵蓋各種教育層級，包括 STEM、人文學科和社會科學，這些問題來自真實的教育材料，並由母語人士和教育工作者手動驗證。該資料集包含 10,969 個哈薩克語問題和 12,031 個俄語問題，反映了哈薩克的雙語教育體系和豐富的在地脈絡。我們對幾個最先進的多語言模型（Llama-3.1、Qwen-2.5、GPT-4 和 DeepSeek V3）的評估顯示，仍有很大的改進空間，因為即使是效能最好的模型，也很難在哈薩克語和俄語中達到有競爭力的效能。這些發現強調了與資源豐富的語言相比，存在顯著的效能差距。我們希望我們的資料集能促進以哈薩克語為中心的 LLM 的進一步研究和開發。資料和程式碼將在獲得接受後提供。

##### **Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models**
2502.12825v1 by Rubing Lu, João Sedoc, Arun Sundararajan

When encountering increasingly frequent performance improvements or cost
reductions from a new large language model (LLM), developers of applications
leveraging LLMs must decide whether to take advantage of these improvements or
stay with older tried-and-tested models. Low perceived switching frictions can
lead to choices that do not consider more subtle behavior changes that the
transition may induce. Our experiments use a popular game-theoretic behavioral
economics model of trust to show stark differences in the trusting behavior of
OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust
behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing
and risk-seeking with future returns from trust, and contrast it with
DeepSeek's more sophisticated and profitable trusting behavior that stems from
an ability to incorporate deeper concepts like forward planning and
theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our
results highlight the perils of relying on LLM performance benchmarks that are
too narrowly defined and suggest that careful analysis of their hidden fault
lines should be part of any organization's AI strategy.

摘要：當遇到越來越頻繁的效能提升或來自於新的大型語言模型 (LLM) 的成本降低時，利用 LLM 的應用程式開發人員必須決定是否要利用這些提升或維持較舊且經過測試的模型。低感知切換摩擦可能會導致選擇不考慮轉換可能誘發的更細微的行為改變。我們的實驗使用信任的流行博弈論行為經濟模型來顯示 OpenAI 和 DeepSeek 模型在信任行為上的顯著差異。我們強調 o1-mini 和 o3-mini 模型的經濟信任行為崩潰，因為它們調和了利潤最大化和風險尋求與來自信任的未來回報，並將其與 DeepSeek 更複雜且有利可圖的信任行為進行對比，這種信任行為源於整合更深層的概念，例如前瞻性規劃和心智理論。由於 LLM 構成高風險商業系統的基礎，我們的結果突顯了依賴定義過於狹窄的 LLM 效能基準的危險性，並建議仔細分析其隱藏的斷層線應該是任何組織的 AI 策略的一部分。

##### **Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models**
2502.12821v1 by Elena Stringli, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou

Inverse tasks can uncover potential reasoning gaps as Large Language Models
(LLMs) scale up. In this work, we explore the redefinition task, in which we
assign alternative values to well-known physical constants and units of
measure, prompting LLMs to respond accordingly. Our findings show that not only
does model performance degrade with scale, but its false confidence also rises.
Moreover, while factors such as prompting strategies or response formatting are
influential, they do not preclude LLMs from anchoring to memorized values.

摘要：逆向任務可以揭示大型語言模型 (LLM) 擴展時潛在的推理差距。在本文中，我們探討重新定義任務，其中我們將替換值指定給著名的物理常數和測量單位，促使 LLM 做出相應回應。我們的研究結果表明，模型效能不僅會隨著規模而下降，其虛假信心也會上升。此外，儘管提示策略或回應格式等因素具有影響力，但它們並不妨礙 LLM 錨定在記憶值上。

##### **Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models**
2502.12813v1 by Adnan Ahmad, Stefan Hillmann, Sebastian Möller

In this study, we explore the application of Large Language Models (LLMs) for
generating synthetic users and simulating user conversations with a
task-oriented dialogue system and present detailed results and their analysis.
We propose a comprehensive novel approach to user simulation technique that
uses LLMs to create diverse user profiles, set goals, engage in multi-turn
dialogues, and evaluate the conversation success. We employ two proprietary
LLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a
heterogeneous base of user profiles, characterized by varied demographics,
multiple user goals, different conversational styles, initial knowledge levels,
interests, and conversational objectives. We perform a detailed analysis of the
user profiles generated by LLMs to assess the diversity, consistency, and
potential biases inherent in these LLM-generated user simulations. We find that
GPT-o1 generates more heterogeneous user distribution across most user
attributes, while GPT-4o generates more skewed user attributes. The generated
set of user profiles are then utilized to simulate dialogue sessions by
interacting with a task-oriented dialogue system.

摘要：在這項研究中，我們探討大型語言模型 (LLM) 在生成合成使用者和模擬使用者對話，並使用任務導向對話系統進行對話的應用，並提出詳細的結果及其分析。我們提出了一種全面的使用者模擬技術新方法，利用 LLM 建立多樣化的使用者概況、設定目標、參與多輪對話，並評估對話的成功性。我們採用了兩個專有的 LLM，即 GPT-4o 和 GPT-o1 (Achiam 等人，2023 年)，以生成一個異質的使用者概況基礎，其特徵在於不同的人口統計資料、多個使用者目標、不同的對話風格、初始知識水準、興趣和對話目標。我們對 LLM 生成的使用者概況進行了詳細分析，以評估這些 LLM 生成的使用者模擬中固有的多樣性、一致性和潛在偏差。我們發現 GPT-o1 在大多數使用者屬性中產生更異質的使用者分佈，而 GPT-4o 則產生更偏斜的使用者屬性。然後利用生成的使用者概況集，透過與任務導向對話系統互動來模擬對話會話。

##### **Towards Text-Image Interleaved Retrieval**
2502.12799v1 by Xin Zhang, Ziqi Dai, Yongqi Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Jun Yu, Wenjie Li, Min Zhang

Current multimodal information retrieval studies mainly focus on single-image
inputs, which limits real-world applications involving multiple images and
text-image interleaved content. In this work, we introduce the text-image
interleaved retrieval (TIIR) task, where the query and document are interleaved
text-image sequences, and the model is required to understand the semantics
from the interleaved context for effective retrieval. We construct a TIIR
benchmark based on naturally interleaved wikiHow tutorials, where a specific
pipeline is designed to generate interleaved queries. To explore the task, we
adapt several off-the-shelf retrievers and build a dense baseline by
interleaved multimodal large language model (MLLM). We then propose a novel
Matryoshka Multimodal Embedder (MME), which compresses the number of visual
tokens at different granularity, to address the challenge of excessive visual
tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption
of existing models does not consistently yield effective results. Our MME
achieves significant improvements over the baseline by substantially fewer
visual tokens. We provide extensive analysis and will release the dataset and
code to facilitate future research.

摘要：目前的多模態資訊檢索研究主要集中在單一影像輸入，這限制了涉及多個影像和文字影像交錯內容的實際應用。在這項工作中，我們引入了文字影像交錯檢索 (TIIR) 任務，其中查詢和文件是交錯的文字影像序列，並且模型需要理解交錯內容的語意以進行有效檢索。我們根據自然交錯的 wikiHow 教學課程建構了一個 TIIR 基準，其中設計了一個特定的管線來產生交錯查詢。為了探索這個任務，我們調整了幾個現成的檢索器，並透過交錯的多模態大型語言模型 (MLLM) 建立了一個密集的基準。然後，我們提出了一個新穎的 Matryoshka 多模態嵌入器 (MME)，它壓縮了不同粒度視覺符號的數量，以解決基於 MLLM 的 TIIR 模型中過多視覺符號的挑戰。實驗表明，對現有模型的簡單調整並未持續產生有效結果。我們的 MME 透過大幅減少視覺符號，達到了比基準顯著的改進。我們提供了廣泛的分析，並將釋出資料集和程式碼以促進未來的研究。

##### **Envious Explore and Exploit**
2502.12798v1 by Omer Ben-Porat, Yotam Gafni, Or Markovetzki

Explore-and-exploit tradeoffs play a key role in recommendation systems
(RSs), aiming at serving users better by learning from previous interactions.
Despite their commercial success, the societal effects of explore-and-exploit
mechanisms are not well understood, especially regarding the utility
discrepancy they generate between different users. In this work, we measure
such discrepancy using the economic notion of envy. We present a multi-armed
bandit-like model in which every round consists of several sessions, and
rewards are realized once per round. We call the latter property reward
consistency, and show that the RS can leverage this property for better
societal outcomes. On the downside, doing so also generates envy, as
late-to-arrive users enjoy the information gathered by early-to-arrive users.
We examine the generated envy under several arrival order mechanisms and
virtually any anonymous algorithm, i.e., any algorithm that treats all similar
users similarly without leveraging their identities. We provide tight envy
bounds on uniform arrival and upper bound the envy for nudged arrival, in which
the RS can affect the order of arrival by nudging its users. Furthermore, we
study the efficiency-fairness trade-off by devising an algorithm that allows
constant envy and approximates the optimal welfare in restricted settings.
Finally, we validate our theoretical results empirically using simulations.

摘要：探索與開發的取捨在推薦系統 (RS) 中扮演著關鍵角色，旨在透過學習先前的互動來為使用者提供更好的服務。儘管在商業上獲得成功，但探索與開發機制的社會效應仍未被充分理解，特別是關於它們在不同使用者之間產生的效用差異。在這項工作中，我們使用經濟學中的嫉妒概念來衡量這種差異。我們提出了一個多臂老虎機模型，其中每一輪都包含多個回合，並且每回合只會實現一次獎勵。我們將後者的特性稱為獎勵一致性，並證明 RS 可以利用此特性來獲得更好的社會成果。不利的是，這麼做也會產生嫉妒，因為較晚加入的使用者可以享受較早加入的使用者所收集的資訊。我們在多種到達順序機制和幾乎任何匿名演算法（即任何演算法都以類似的方式對待所有類似的使用者，而不利用他們的身份）下檢驗產生的嫉妒。我們對均勻到達提供嚴格的嫉妒界線，並對推動到達的上限進行嫉妒界線，其中 RS 可以透過推動其使用者來影響到達順序。此外，我們透過設計一種演算法來研究效率公平權衡，該演算法允許恆定的嫉妒，並在受限設定中近似最佳福利。最後，我們使用模擬對我們的理論結果進行經驗驗證。

##### **Commonsense Reasoning in Arab Culture**
2502.12788v1 by Abdelrahman Sadallah, Junior Cedric Tonga, Khalid Almubarak, Saeed Almheiri, Farah Atif, Chatrine Qwaider, Karima Kadaoui, Sara Shatnawi, Yaser Alesh, Fajri Koto

Despite progress in Arabic large language models, such as Jais and AceGPT,
their evaluation on commonsense reasoning has largely relied on
machine-translated datasets, which lack cultural depth and may introduce
Anglocentric biases. Commonsense reasoning is shaped by geographical and
cultural contexts, and existing English datasets fail to capture the diversity
of the Arab world. To address this, we introduce \datasetname, a commonsense
reasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13
countries across the Gulf, Levant, North Africa, and the Nile Valley. The
dataset was built from scratch by engaging native speakers to write and
validate culturally relevant questions for their respective countries.
\datasetname spans 12 daily life domains with 54 fine-grained subtopics,
reflecting various aspects of social norms, traditions, and everyday
experiences. Zero-shot evaluations show that open-weight language models with
up to 32B parameters struggle to comprehend diverse Arab cultures, with
performance varying across regions. These findings highlight the need for more
culturally aware models and datasets tailored to the Arabic-speaking world.

摘要：儘管阿拉伯語大型語言模型（例如 Jais 和 AceGPT）已有進展，
但它們在常識推理上的評估在很大程度上依賴於
機器翻譯的資料集，這些資料集缺乏文化深度，可能會引入
以英語為中心的偏見。常識推理受地理和
文化背景影響，現有的英文資料集無法捕捉阿拉伯世界的多樣性。為了解決這個問題，我們引入了 \datasetname，一個現代標準阿拉伯語 (MSA) 的常識推理資料集，涵蓋海灣地區、黎凡特地區、北非和尼羅河谷 13 個國家的文化。此資料集是從頭開始建立的，由母語人士參與編寫和驗證他們各自國家的文化相關問題。\datasetname 涵蓋 12 個日常生活領域，包含 54 個細緻的主題，反映社會規範、傳統和日常經驗的各個方面。零次學習評估顯示，具有高達 32B 參數的開放式權重語言模型難以理解不同的阿拉伯文化，且各區域的表現不一。這些發現突顯了對更具文化意識的模型和專為阿拉伯語系世界量身打造的資料集的需求。

##### **VidCapBench: A Comprehensive Benchmark of Video Captioning for Controllable Text-to-Video Generation**
2502.12782v1 by Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, Tieniu Tan

The training of controllable text-to-video (T2V) models relies heavily on the
alignment between videos and captions, yet little existing research connects
video caption evaluation with T2V generation assessment. This paper introduces
VidCapBench, a video caption evaluation scheme specifically designed for T2V
generation, agnostic to any particular caption format. VidCapBench employs a
data annotation pipeline, combining expert model labeling and human refinement,
to associate each collected video with key information spanning video
aesthetics, content, motion, and physical laws. VidCapBench then partitions
these key information attributes into automatically assessable and manually
assessable subsets, catering to both the rapid evaluation needs of agile
development and the accuracy requirements of thorough validation. By evaluating
numerous state-of-the-art captioning models, we demonstrate the superior
stability and comprehensiveness of VidCapBench compared to existing video
captioning evaluation approaches. Verification with off-the-shelf T2V models
reveals a significant positive correlation between scores on VidCapBench and
the T2V quality evaluation metrics, indicating that VidCapBench can provide
valuable guidance for training T2V models. The project is available at
https://github.com/VidCapBench/VidCapBench.

摘要：可控制文本到影片 (T2V) 模型的訓練極度仰賴影片和字幕之間的對齊，但現有研究鮮少將影片字幕評估與 T2V 生成評估連結起來。本文介紹 VidCapBench，這是一種專門為 T2V 生成設計的影片字幕評估架構，與任何特定的字幕格式無關。VidCapBench 採用資料標註流程，結合專家模型標記和人工微調，將每個收集到的影片與涵蓋影片美學、內容、動作和物理定律等關鍵資訊關聯起來。VidCapBench 接著將這些關鍵資訊屬性分割成可自動評估和可手動評估的子集，以滿足敏捷開發的快速評估需求和全面驗證的準確性要求。透過評估許多最先進的字幕模型，我們證明了 VidCapBench 與現有的影片字幕評估方法相比，具有優異的穩定性和全面性。使用現成的 T2V 模型驗證顯示，VidCapBench 得分與 T2V 品質評估指標之間存在顯著的正相關，這表示 VidCapBench 可以為訓練 T2V 模型提供有價值的指導。專案可於 https://github.com/VidCapBench/VidCapBench 取得。

##### **Portable Reward Tuning: Towards Reusable Fine-Tuning across Different Pretrained Models**
2502.12776v1 by Daiki Chijiwa, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito, Susumu Takeuchi

While foundation models have been exploited for various expert tasks through
fine-tuning, any foundation model will become outdated due to its old knowledge
or limited capability. Thus the underlying foundation model should be
eventually replaced by new ones, which leads to repeated cost of fine-tuning
these new models. Existing work addresses this problem by inference-time
tuning, i.e., modifying the output probabilities from the new foundation model
with the outputs from the old foundation model and its fine-tuned model, which
involves an additional overhead in inference by the latter two models. In this
paper, we propose a new fine-tuning principle, Portable Reward Tuning (PRT),
that reduces the inference overhead by its nature, based on the reformulation
of fine-tuning as the reward maximization. Specifically, instead of fine-tuning
parameters of the foundation models, PRT trains the reward model explicitly
through the same loss function as in fine-tuning. During inference, the reward
model can be used with any foundation model (with the same set of vocabularies
or labels) through the formulation of reward maximization. Experimental
results, covering both vision and language models, demonstrate that the
PRT-trained model can achieve comparable accuracy to the existing work of
inference-time tuning, with less inference cost.

摘要：儘管基礎模型已透過微調用於各種專家任務，任何基礎模型都將因其舊知識或有限功能而過時。因此，基礎模型最終應由新模型取代，這導致重複微調這些新模型的成本。現有工作透過推論時間調整來解決這個問題，即使用舊基礎模型及其微調模型的輸出修改新基礎模型的輸出機率，這涉及後兩個模型在推論中的額外開銷。在本文中，我們提出一個新的微調原則，可攜式獎勵調整 (PRT)，它本質上會減少推論開銷，基於將微調重新表述為獎勵最大化。具體來說，PRT 不是微調基礎模型的參數，而是透過與微調中相同的損失函數明確訓練獎勵模型。在推論期間，獎勵模型可透過獎勵最大化的公式與任何基礎模型（具有相同的詞彙或標籤組）一起使用。涵蓋視覺和語言模型的實驗結果證明，PRT 訓練的模型可以達到與現有推論時間調整工作相當的準確度，且推論成本較低。

##### **Mind the Gap: Aligning the Brain with Language Models Requires a Nonlinear and Multimodal Approach**
2502.12771v1 by Danny Dongyeop Han, Yunju Cho, Jiook Cha, Jay-Yoon Lee

Self-supervised language and audio models effectively predict brain responses
to speech. However, traditional prediction models rely on linear mappings from
unimodal features, despite the complex integration of auditory signals with
linguistic and semantic information across widespread brain networks during
speech comprehension. Here, we introduce a nonlinear, multimodal prediction
model that combines audio and linguistic features from pre-trained models
(e.g., LLAMA, Whisper). Our approach achieves a 17.2% and 17.9% improvement in
prediction performance (unnormalized and normalized correlation) over
traditional unimodal linear models, as well as a 7.7% and 14.4% improvement,
respectively, over prior state-of-the-art models. These improvements represent
a major step towards future robust in-silico testing and improved decoding
performance. They also reveal how auditory and semantic information are fused
in motor, somatosensory, and higher-level semantic regions, aligning with
existing neurolinguistic theories. Overall, our work highlights the often
neglected potential of nonlinear and multimodal approaches to brain modeling,
paving the way for future studies to embrace these strategies in naturalistic
neurolinguistics research.

摘要：自我監督的語言和音訊模型有效預測大腦對語言的反應。然而，傳統的預測模型依賴於單模態特徵的線性映射，儘管在語言理解過程中，聽覺信號與語言和語義資訊在廣泛的腦網路中進行複雜的整合。在此，我們引入一個非線性、多模態預測模型，結合預先訓練模型（例如，LLAMA、Whisper）中的音訊和語言特徵。我們的做法在預測效能上（未正規化和正規化相關性）分別比傳統的單模態線性模型提升了 17.2% 和 17.9%，分別比先前的最先進模型提升了 7.7% 和 14.4%。這些改進代表了未來穩健的電腦模擬測試和改進的解碼效能邁出了一大步。它們也揭示了聽覺和語義資訊如何在運動、體感和更高層次的語義區域中融合，與現有的神經語言學理論一致。總的來說，我們的研究突出了非線性和多模態大腦建模方法經常被忽略的潛力，為未來研究在自然主義神經語言學研究中採用這些策略鋪平了道路。

##### **How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild**
2502.12769v1 by Saad Obaid ul Islam, Anne Lauscher, Goran Glavaš

In the age of misinformation, hallucination -- the tendency of Large Language
Models (LLMs) to generate non-factual or unfaithful responses -- represents the
main risk for their global utility. Despite LLMs becoming increasingly
multilingual, the vast majority of research on detecting and quantifying LLM
hallucination are (a) English-centric and (b) focus on machine translation (MT)
and summarization, tasks that are less common ``in the wild'' than open
information seeking. In contrast, we aim to quantify the extent of LLM
hallucination across languages in knowledge-intensive long-form question
answering. To this end, we train a multilingual hallucination detection model
and conduct a large-scale study across 30 languages and 6 open-source LLM
families. We start from an English hallucination detection dataset and rely on
MT to generate (noisy) training data in other languages. We also manually
annotate gold data for five high-resource languages; we then demonstrate, for
these languages, that the estimates of hallucination rates are similar between
silver (LLM-generated) and gold test sets, validating the use of silver data
for estimating hallucination rates for other languages. For the final rates
estimation, we build a knowledge-intensive QA dataset for 30 languages with
LLM-generated prompts and Wikipedia articles as references. We find that, while
LLMs generate longer responses with more hallucinated tokens for
higher-resource languages, there is no correlation between length-normalized
hallucination rates of languages and their digital representation. Further, we
find that smaller LLMs exhibit larger hallucination rates than larger models.

摘要：<paragraph>在错误訊息的時代，幻覺——大型語言模型 (LLM) 產生非事實或不忠實回應的傾向——代表其全球效用的主要風險。儘管 LLM 變得越來越多元化，但絕大多數關於偵測和量化 LLM 幻覺的研究都是 (a) 以英語為中心，(b) 專注於機器翻譯 (MT) 和摘要，這些任務在「野外」中不如開放式資訊搜尋常見。相反地，我們旨在量化 LLM 在知識密集型長篇問答中跨語言的幻覺程度。為此，我們訓練了一個多語言幻覺偵測模型，並針對 30 種語言和 6 個開放原始碼 LLM 家族進行大規模研究。我們從一個英語幻覺偵測資料集開始，並依賴 MT 在其他語言中產生（有雜訊的）訓練資料。我們還手動為五種高資源語言註解黃金資料；然後我們證明，對於這些語言，幻覺率的估計值在白銀（LLM 產生）和黃金測試集之間是相似的，驗證了使用白銀資料來估計其他語言的幻覺率。對於最終的比率估計，我們建立了一個知識密集型問答資料集，其中包含 30 種語言，並以 LLM 產生的提示和維基百科文章作為參考。我們發現，儘管 LLM 為資源較多的語言產生了更長的回應和更多幻覺的代幣，但語言的長度正規化幻覺率與其數位表示之間沒有相關性。此外，我們發現較小的 LLM 表現出比較大的模型更大的幻覺率。</paragraph>

##### **R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs**
2502.12767v1 by Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi

Recent studies have combined Large Language Models (LLMs) with Knowledge
Graphs (KGs) to enhance reasoning, improving inference accuracy without
additional training while mitigating hallucination. However, existing
frameworks are often rigid, struggling to adapt to KG or task changes. They
also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.
To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that
separates reasoning into two roles: an Operator (a low-capacity LLM) that
gathers evidence and a Supervisor (a high-capacity LLM) that makes final
judgments. This design is cost-efficient for LLM inference while still
maintaining strong reasoning accuracy. Additionally, R2-KG employs an
Abstention mechanism, generating answers only when sufficient evidence is
collected from KG, which significantly enhances reliability. Experiments across
multiple KG-based reasoning tasks show that R2-KG consistently outperforms
baselines in both accuracy and reliability, regardless of the inherent
capability of LLMs used as the Operator. Further experiments reveal that the
single-agent version of R2-KG, equipped with a strict self-consistency
strategy, achieves significantly higher-than-baseline reliability while
reducing inference cost. However, it also leads to a higher abstention rate in
complex KGs. Our findings establish R2-KG as a flexible and cost-effective
solution for KG-based reasoning. It reduces reliance on high-capacity LLMs
while ensuring trustworthy inference.

摘要：<paragraph>最近的研究结合了大型语言模型 (LLM) 与知识图谱 (KG) 以增强推理，在不额外训练的情况下提高推理准确性，同时减轻幻觉。然而，现有的框架通常很僵化，难以适应知识图谱或任务的变化。它们还严重依赖强大的 LLM 来进行可靠（即值得信赖）的推理。为了解决这个问题，我们引入了 R2-KG，这是一个即插即用、双代理框架，它将推理分为两个角色：一个收集证据的操作员（低容量 LLM）和一个做出最终判断的监督员（高容量 LLM）。这种设计在 LLM 推理方面具有成本效益，同时仍保持强大的推理准确性。此外，R2-KG 采用弃权机制，仅在从知识图谱收集到足够证据时才生成答案，这显著提高了可靠性。跨多个基于知识图谱的推理任务的实验表明，R2-KG 在准确性和可靠性方面始终优于基线，而与用作操作员的 LLM 的固有能力无关。进一步的实验表明，R2-KG 的单代理版本配备了严格的自一致性策略，实现了明显高于基线的可靠性，同时降低了推理成本。然而，它也导致了复杂知识图谱中更高的弃权率。我们的发现将 R2-KG 确立为一种灵活且经济高效的基于知识图谱的推理解决方案。它减少了对高容量 LLM 的依赖，同时确保了可信的推理。</paragraph>

