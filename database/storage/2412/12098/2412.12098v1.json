{"2412.12098": {"publish_time": "2024-12-16", "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization", "paper_summary": "Reinforcement learning (RL) algorithms aim to balance exploiting the current\nbest strategy with exploring new options that could lead to higher rewards.\nMost common RL algorithms use undirected exploration, i.e., select random\nsequences of actions. Exploration can also be directed using intrinsic rewards,\nsuch as curiosity or model epistemic uncertainty. However, effectively\nbalancing task and intrinsic rewards is challenging and often task-dependent.\nIn this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and\nextrinsic exploration. MaxInfoRL steers exploration towards informative\ntransitions, by maximizing intrinsic rewards such as the information gain about\nthe underlying task. When combined with Boltzmann exploration, this approach\nnaturally trades off maximization of the value function with that of the\nentropy over states, rewards, and actions. We show that our approach achieves\nsublinear regret in the simplified setting of multi-armed bandits. We then\napply this general formulation to a variety of off-policy model-free RL methods\nfor continuous state-action spaces, yielding novel algorithms that achieve\nsuperior performance across hard exploration problems and complex scenarios\nsuch as visual control tasks.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u6f14\u7b97\u6cd5\u65e8\u5728\u5e73\u8861\u5229\u7528\u76ee\u524d\u6700\u4f73\u7b56\u7565\u8207\u63a2\u7d22\u53ef\u80fd\u5e36\u4f86\u66f4\u9ad8\u56de\u994b\u7684\u65b0\u9078\u9805\u3002\n\u6700\u5e38\u898b\u7684 RL \u6f14\u7b97\u6cd5\u4f7f\u7528\u975e\u5c0e\u5411\u63a2\u7d22\uff0c\u5373\u9078\u64c7\u96a8\u6a5f\u52d5\u4f5c\u5e8f\u5217\u3002\u63a2\u7d22\u4e5f\u53ef\u4ee5\u4f7f\u7528\u5167\u5728\u56de\u994b\u9032\u884c\u5f15\u5c0e\uff0c\u4f8b\u5982\u597d\u5947\u5fc3\u6216\u6a21\u578b\u8a8d\u8b58\u8ad6\u7684\u4e0d\u78ba\u5b9a\u6027\u3002\u7136\u800c\uff0c\u6709\u6548\u5e73\u8861\u4efb\u52d9\u548c\u5167\u5728\u56de\u994b\u5177\u6709\u6311\u6230\u6027\uff0c\u800c\u4e14\u901a\u5e38\u53d6\u6c7a\u65bc\u4efb\u52d9\u3002\n\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u9032\u4e86\u4e00\u500b\u6846\u67b6 MaxInfoRL\uff0c\u7528\u65bc\u5e73\u8861\u5167\u5728\u548c\u5916\u5728\u63a2\u7d22\u3002MaxInfoRL \u900f\u904e\u6700\u5927\u5316\u5167\u5728\u56de\u994b\uff08\u4f8b\u5982\u95dc\u65bc\u57fa\u790e\u4efb\u52d9\u7684\u8cc7\u8a0a\u7372\u53d6\uff09\u4f86\u5f15\u5c0e\u63a2\u7d22\uff0c\u671d\u5411\u5177\u6709\u8cc7\u8a0a\u6027\u7684\u8f49\u63db\u3002\u7576\u8207 Boltzmann \u63a2\u7d22\u7d50\u5408\u6642\uff0c\u6b64\u65b9\u6cd5\u81ea\u7136\u6703\u4ee5\u72c0\u614b\u3001\u56de\u994b\u548c\u52d5\u4f5c\u7684\u71b5\u6700\u5927\u5316\u4f86\u4ea4\u63db\u50f9\u503c\u51fd\u6578\u7684\u6700\u5927\u5316\u3002\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u591a\u91cd\u62c9\u9738\u6a5f\u7684\u7c21\u5316\u8a2d\u5b9a\u4e2d\u9054\u5230\u4e86\u6b21\u7dda\u6027\u5f8c\u6094\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07\u6b64\u4e00\u822c\u516c\u5f0f\u61c9\u7528\u65bc\u5404\u7a2e\u975e\u7b56\u7565\u6a21\u578b\u514d\u8cbb RL \u65b9\u6cd5\uff0c\u7528\u65bc\u9023\u7e8c\u72c0\u614b\u52d5\u4f5c\u7a7a\u9593\uff0c\u7522\u751f\u65b0\u7a4e\u7684\u6f14\u7b97\u6cd5\uff0c\u5728\u56f0\u96e3\u7684\u63a2\u7d22\u554f\u984c\u548c\u8907\u96dc\u5834\u666f\uff08\u4f8b\u5982\u8996\u89ba\u63a7\u5236\u4efb\u52d9\uff09\u4e2d\u5be6\u73fe\u512a\u7570\u7684\u6548\u80fd\u3002", "author": "Bhavya Sukhija et.al.", "authors": "Bhavya Sukhija, Stelian Coros, Andreas Krause, Pieter Abbeel, Carmelo Sferrazza", "id": "2412.12098v1", "paper_url": "http://arxiv.org/abs/2412.12098v1", "repo": "null"}}