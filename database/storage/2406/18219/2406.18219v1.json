{"2406.18219": {"publish_time": "2024-06-26", "title": "A Closer Look into Mixture-of-Experts in Large Language Models", "paper_summary": "Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree recent MoE-based models and reveal some intriguing observations,\nincluding (1) Neurons act like fine-grained experts. (2) The router of MoE\nusually selects experts with larger output norms. (3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier. Based on\nthe observations, we also provide suggestions for a broad spectrum of MoE\npractitioners, such as router design and expert allocation. We hope this work\ncould shed light on future research on the MoE framework and other modular\narchitectures. Code is available at\nhttps://github.com/kamanphoebe/Look-into-MoEs.", "paper_summary_zh": "\u6df7\u5408\u4e13\u5bb6 (MoE) \u56e0\u5176\u72ec\u7279\u7684\u7279\u6027\u548c\u5353\u8d8a\u7684\u6027\u80fd\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u5c24\u5176\u662f\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u3002\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u6bcf\u4e2a\u6807\u8bb0\u7684\u5b50\u96c6\u53c2\u6570\uff0cMoE \u67b6\u6784\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u8ba1\u7b97\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u589e\u52a0\u6a21\u578b\u5927\u5c0f\uff0c\u4ece\u800c\u5728\u6027\u80fd\u548c\u8bad\u7ec3\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\u3002\u7136\u800c\uff0cMoE \u7684\u5e95\u5c42\u673a\u5236\u4ecd\u7136\u7f3a\u4e4f\u8fdb\u4e00\u6b65\u7684\u63a2\u7d22\uff0c\u5176\u6a21\u5757\u5316\u7a0b\u5ea6\u4e5f\u503c\u5f97\u5546\u69b7\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5c1d\u8bd5\u521d\u6b65\u4e86\u89e3\u57fa\u4e8e MoE \u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8fd0\u4f5c\u65b9\u5f0f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5168\u9762\u7814\u7a76\u4e86\u4e09\u4e2a\u8fd1\u671f\u57fa\u4e8e MoE \u7684\u6a21\u578b\u7684\u53c2\u6570\u548c\u884c\u4e3a\u7279\u5f81\uff0c\u5e76\u63ed\u793a\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u5305\u62ec (1) \u795e\u7ecf\u5143\u5145\u5f53\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u3002 (2) MoE \u7684\u8def\u7531\u5668\u901a\u5e38\u9009\u62e9\u5177\u6709\u8f83\u5927\u8f93\u51fa\u8303\u6570\u7684\u4e13\u5bb6\u3002 (3) \u4e13\u5bb6\u591a\u6837\u6027\u968f\u7740\u5c42\u6570\u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u800c\u6700\u540e\u4e00\u5c42\u662f\u4e00\u4e2a\u5f02\u5e38\u503c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c2\u5bdf\u7ed3\u679c\uff0c\u6211\u4eec\u8fd8\u4e3a\u5e7f\u6cdb\u7684 MoE \u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5efa\u8bae\uff0c\u4f8b\u5982\u8def\u7531\u5668\u8bbe\u8ba1\u548c\u4e13\u5bb6\u5206\u914d\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u53ef\u4ee5\u4e3a\u672a\u6765\u5bf9 MoE \u6846\u67b6\u548c\u5176\u4ed6\u6a21\u5757\u5316\u67b6\u6784\u7684\u7814\u7a76\u63d0\u4f9b\u542f\u793a\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/kamanphoebe/Look-into-MoEs \u83b7\u5f97\u3002", "author": "Ka Man Lo et.al.", "authors": "Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu", "id": "2406.18219v1", "paper_url": "http://arxiv.org/abs/2406.18219v1", "repo": "https://github.com/kamanphoebe/look-into-moes"}}