{"2409.18084": {"publish_time": "2024-09-26", "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal Model", "paper_summary": "As the number of service robots and autonomous vehicles in human-centered\nenvironments grows, their requirements go beyond simply navigating to a\ndestination. They must also take into account dynamic social contexts and\nensure respect and comfort for others in shared spaces, which poses significant\nchallenges for perception and planning. In this paper, we present a group-based\nsocial navigation framework GSON to enable mobile robots to perceive and\nexploit the social group of their surroundings by leveling the visual reasoning\ncapability of the Large Multimodal Model (LMM). For perception, we apply visual\nprompting techniques to zero-shot extract the social relationship among\npedestrians and combine the result with a robust pedestrian detection and\ntracking pipeline to alleviate the problem of low inference speed of the LMM.\nGiven the perception result, the planning system is designed to avoid\ndisrupting the current social structure. We adopt a social structure-based\nmid-level planner as a bridge between global path planning and local motion\nplanning to preserve the global context and reactive response. The proposed\nmethod is validated on real-world mobile robot navigation tasks involving\ncomplex social structure understanding and reasoning. Experimental results\ndemonstrate the effectiveness of the system in these scenarios compared with\nseveral baselines.", "paper_summary_zh": "\u96a8\u8457\u4ee5\u4eba\u70ba\u4e2d\u5fc3\u7684\u74b0\u5883\u4e2d\u670d\u52d9\u6a5f\u5668\u4eba\u548c\u81ea\u52d5\u99d5\u99db\u8eca\u8f1b\u7684\u6578\u91cf\u589e\u52a0\uff0c\u5b83\u5011\u7684\u8981\u6c42\u5df2\u8d85\u8d8a\u4e86\u55ae\u7d14\u5c0e\u822a\u5230\u76ee\u7684\u5730\u3002\u5b83\u5011\u9084\u5fc5\u9808\u8003\u616e\u52d5\u614b\u7684\u793e\u4ea4\u80cc\u666f\uff0c\u4e26\u78ba\u4fdd\u5728\u5171\u4eab\u7a7a\u9593\u4e2d\u5c0a\u91cd\u548c\u5b89\u6170\u4ed6\u4eba\uff0c\u9019\u5c0d\u611f\u77e5\u548c\u898f\u5283\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u7fa4\u7d44\u7684\u793e\u4ea4\u5c0e\u822a\u6846\u67b6 GSON\uff0c\u4ee5\u4f7f\u79fb\u52d5\u6a5f\u5668\u4eba\u80fd\u5920\u901a\u904e\u63d0\u5347\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u7684\u8996\u89ba\u63a8\u7406\u80fd\u529b\u4f86\u611f\u77e5\u548c\u5229\u7528\u5468\u570d\u74b0\u5883\u4e2d\u7684\u793e\u4ea4\u7fa4\u7d44\u3002\u5c0d\u65bc\u611f\u77e5\uff0c\u6211\u5011\u5c07\u8996\u89ba\u63d0\u793a\u6280\u8853\u61c9\u7528\u65bc\u96f6\u6b21\u5b78\u7fd2\uff0c\u4ee5\u63d0\u53d6\u884c\u4eba\u4e4b\u9593\u7684\u793e\u4ea4\u95dc\u4fc2\uff0c\u4e26\u5c07\u7d50\u679c\u8207\u5f37\u5927\u7684\u884c\u4eba\u6aa2\u6e2c\u548c\u8ffd\u8e64\u7ba1\u9053\u76f8\u7d50\u5408\uff0c\u4ee5\u7de9\u89e3 LMM \u63a8\u8ad6\u901f\u5ea6\u4f4e\u7684\u554f\u984c\u3002\u6839\u64da\u611f\u77e5\u7d50\u679c\uff0c\u898f\u5283\u7cfb\u7d71\u88ab\u8a2d\u8a08\u70ba\u907f\u514d\u7834\u58de\u7576\u524d\u7684\u793e\u4ea4\u7d50\u69cb\u3002\u6211\u5011\u63a1\u7528\u57fa\u65bc\u793e\u6703\u7d50\u69cb\u7684\u4e2d\u7d1a\u898f\u5283\u5668\u4f5c\u70ba\u5168\u5c40\u8def\u5f91\u898f\u5283\u548c\u5c40\u90e8\u904b\u52d5\u898f\u5283\u4e4b\u9593\u7684\u6a4b\u6a11\uff0c\u4ee5\u4fdd\u7559\u5168\u5c40\u80cc\u666f\u548c\u53cd\u61c9\u6027\u97ff\u61c9\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6d89\u53ca\u8907\u96dc\u793e\u6703\u7d50\u69cb\u7406\u89e3\u548c\u63a8\u7406\u7684\u771f\u5be6\u4e16\u754c\u79fb\u52d5\u6a5f\u5668\u4eba\u5c0e\u822a\u4efb\u52d9\u4e2d\u5f97\u5230\u9a57\u8b49\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u8a72\u7cfb\u7d71\u5728\u9019\u4e9b\u5834\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e26\u8207\u5e7e\u500b\u57fa\u7dda\u9032\u884c\u4e86\u6bd4\u8f03\u3002", "author": "Shangyi Luo et.al.", "authors": "Shangyi Luo, Ji Zhu, Peng Sun, Yuhong Deng, Cunjun Yu, Anxing Xiao, Xueqian Wang", "id": "2409.18084v1", "paper_url": "http://arxiv.org/abs/2409.18084v1", "repo": "null"}}