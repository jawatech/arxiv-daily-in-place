{"2502.15672": {"publish_time": "2025-02-21", "title": "VaViM and VaVAM: Autonomous Driving through Video Generative Modeling", "paper_summary": "We explore the potential of large-scale generative video models for\nautonomous driving, introducing an open-source auto-regressive video model\n(VaViM) and its companion video-action model (VaVAM) to investigate how video\npre-training transfers to real-world driving. VaViM is a simple auto-regressive\nvideo model that predicts frames using spatio-temporal token sequences. We show\nthat it captures the semantics and dynamics of driving scenes. VaVAM, the\nvideo-action model, leverages the learned representations of VaViM to generate\ndriving trajectories through imitation learning. Together, the models form a\ncomplete perception-to-action pipeline. We evaluate our models in open- and\nclosed-loop driving scenarios, revealing that video-based pre-training holds\npromise for autonomous driving. Key insights include the semantic richness of\nthe learned representations, the benefits of scaling for video synthesis, and\nthe complex relationship between model size, data, and safety metrics in\nclosed-loop evaluations. We release code and model weights at\nhttps://github.com/valeoai/VideoActionModel", "paper_summary_zh": "\u6211\u5011\u63a2\u8a0e\u4e86\u5927\u578b\u751f\u6210\u5f71\u7247\u6a21\u578b\u5728\u81ea\u52d5\u99d5\u99db\u65b9\u9762\u7684\u6f5b\u529b\uff0c\u5f15\u5165\u4e86\u958b\u6e90\u81ea\u8ff4\u6b78\u5f71\u7247\u6a21\u578b (VaViM) \u53ca\u5176\u914d\u5957\u5f71\u7247\u52d5\u4f5c\u6a21\u578b (VaVAM)\uff0c\u4ee5\u63a2\u8a0e\u5f71\u7247\u9810\u8a13\u7df4\u5982\u4f55\u8f49\u79fb\u5230\u5be6\u969b\u99d5\u99db\u3002VaViM \u662f\u4e00\u500b\u7c21\u55ae\u7684\u81ea\u8ff4\u6b78\u5f71\u7247\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u6642\u7a7a\u6a19\u8a18\u5e8f\u5217\u9810\u6e2c\u5e40\u3002\u6211\u5011\u5c55\u793a\u4e86\u5b83\u6355\u6349\u4e86\u99d5\u99db\u5834\u666f\u7684\u8a9e\u7fa9\u548c\u52d5\u614b\u3002\u5f71\u7247\u52d5\u4f5c\u6a21\u578b VaVAM \u5229\u7528 VaViM \u7684\u5b78\u7fd2\u8868\u793a\uff0c\u900f\u904e\u6a21\u4eff\u5b78\u7fd2\u7522\u751f\u99d5\u99db\u8ecc\u8de1\u3002\u9019\u4e9b\u6a21\u578b\u5171\u540c\u5f62\u6210\u4e86\u5b8c\u6574\u7684\u611f\u77e5\u5230\u52d5\u4f5c\u7684\u7ba1\u9053\u3002\u6211\u5011\u5728\u958b\u653e\u548c\u9589\u74b0\u99d5\u99db\u5834\u666f\u4e2d\u8a55\u4f30\u6a21\u578b\uff0c\u7d50\u679c\u986f\u793a\u57fa\u65bc\u5f71\u7247\u7684\u9810\u8a13\u7df4\u5c0d\u81ea\u52d5\u99d5\u99db\u5f88\u6709\u524d\u666f\u3002\u95dc\u9375\u898b\u89e3\u5305\u62ec\u5b78\u7fd2\u8868\u793a\u7684\u8a9e\u7fa9\u8c50\u5bcc\u6027\u3001\u5f71\u7247\u5408\u6210\u64f4\u5145\u7684\u597d\u8655\uff0c\u4ee5\u53ca\u5728\u9589\u74b0\u8a55\u4f30\u4e2d\u6a21\u578b\u5927\u5c0f\u3001\u8cc7\u6599\u548c\u5b89\u5168\u6307\u6a19\u4e4b\u9593\u7684\u8907\u96dc\u95dc\u4fc2\u3002\u6211\u5011\u5728 https://github.com/valeoai/VideoActionModel \u4e0a\u91cb\u51fa\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u6b0a\u91cd", "author": "Florent Bartoccioni et.al.", "authors": "Florent Bartoccioni, Elias Ramzi, Victor Besnier, Shashanka Venkataramanan, Tuan-Hung Vu, Yihong Xu, Loick Chambon, Spyros Gidaris, Serkan Odabas, David Hurych, Renaud Marlet, Alexandre Boulch, Mickael Chen, \u00c9loi Zablocki, Andrei Bursuc, Eduardo Valle, Matthieu Cord", "id": "2502.15672v1", "paper_url": "http://arxiv.org/abs/2502.15672v1", "repo": "https://github.com/valeoai/VideoActionModel"}}