{"2408.06906": {"publish_time": "2024-08-13", "title": "VNet: A GAN-based Multi-Tier Discriminator Network for Speech Synthesis Vocoders", "paper_summary": "Since the introduction of Generative Adversarial Networks (GANs) in speech\nsynthesis, remarkable achievements have been attained. In a thorough\nexploration of vocoders, it has been discovered that audio waveforms can be\ngenerated at speeds exceeding real-time while maintaining high fidelity,\nachieved through the utilization of GAN-based models. Typically, the inputs to\nthe vocoder consist of band-limited spectral information, which inevitably\nsacrifices high-frequency details. To address this, we adopt the full-band Mel\nspectrogram information as input, aiming to provide the vocoder with the most\ncomprehensive information possible. However, previous studies have revealed\nthat the use of full-band spectral information as input can result in the issue\nof over-smoothing, compromising the naturalness of the synthesized speech. To\ntackle this challenge, we propose VNet, a GAN-based neural vocoder network that\nincorporates full-band spectral information and introduces a Multi-Tier\nDiscriminator (MTD) comprising multiple sub-discriminators to generate\nhigh-resolution signals. Additionally, we introduce an asymptotically\nconstrained method that modifies the adversarial loss of the generator and\ndiscriminator, enhancing the stability of the training process. Through\nrigorous experiments, we demonstrate that the VNet model is capable of\ngenerating high-fidelity speech and significantly improving the performance of\nthe vocoder.", "paper_summary_zh": "\u81ea\u751f\u6210\u5bf9\u6297\u7db2\u8def (GAN) \u5f15\u5165\u8a9e\u97f3\u5408\u6210\u4ee5\u4f86\uff0c\u5df2\u53d6\u5f97\u986f\u8457\u7684\u6210\u5c31\u3002\u5728\u5c0d\u8a9e\u97f3\u7de8\u78bc\u5668\u9032\u884c\u5fb9\u5e95\u63a2\u8a0e\u5f8c\uff0c\u767c\u73fe\u53ef\u4ee5\u4ee5\u8d85\u904e\u5be6\u6642\u7684\u901f\u5ea6\u751f\u6210\u97f3\u8a0a\u6ce2\u5f62\uff0c\u540c\u6642\u900f\u904e\u5229\u7528\u57fa\u65bc GAN \u7684\u6a21\u578b\u7dad\u6301\u9ad8\u4fdd\u771f\u5ea6\u3002\u901a\u5e38\uff0c\u8a9e\u97f3\u7de8\u78bc\u5668\u7684\u8f38\u5165\u5305\u542b\u983b\u5e36\u9650\u5236\u7684\u983b\u8b5c\u8cc7\u8a0a\uff0c\u9019\u4e0d\u53ef\u907f\u514d\u5730\u6703\u72a7\u7272\u9ad8\u983b\u7d30\u7bc0\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63a1\u7528\u5168\u983b\u6bb5 Mel \u983b\u8b5c\u5716\u8cc7\u8a0a\u4f5c\u70ba\u8f38\u5165\uff0c\u76ee\u6a19\u662f\u70ba\u8a9e\u97f3\u7de8\u78bc\u5668\u63d0\u4f9b\u6700\u5168\u9762\u7684\u8cc7\u8a0a\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u7814\u7a76\u986f\u793a\uff0c\u4f7f\u7528\u5168\u983b\u6bb5\u983b\u8b5c\u8cc7\u8a0a\u4f5c\u70ba\u8f38\u5165\u53ef\u80fd\u6703\u5c0e\u81f4\u904e\u5ea6\u5e73\u6ed1\u7684\u554f\u984c\uff0c\u640d\u5bb3\u5408\u6210\u8a9e\u97f3\u7684\u81ea\u7136\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa VNet\uff0c\u4e00\u500b\u57fa\u65bc GAN \u7684\u795e\u7d93\u8a9e\u97f3\u7de8\u78bc\u5668\u7db2\u8def\uff0c\u5b83\u7d50\u5408\u4e86\u5168\u983b\u6bb5\u983b\u8b5c\u8cc7\u8a0a\uff0c\u4e26\u5f15\u5165\u4e86\u5305\u542b\u591a\u500b\u5b50\u8fa8\u5225\u5668\u7684\u591a\u5c64\u8fa8\u5225\u5668 (MTD) \u4f86\u751f\u6210\u9ad8\u89e3\u6790\u5ea6\u8a0a\u865f\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u6f38\u8fd1\u7d04\u675f\u65b9\u6cd5\uff0c\u4fee\u6539\u4e86\u751f\u6210\u5668\u548c\u8fa8\u5225\u5668\u7684\u5c0d\u6297\u640d\u5931\uff0c\u589e\u5f37\u4e86\u8a13\u7df4\u904e\u7a0b\u7684\u7a69\u5b9a\u6027\u3002\u900f\u904e\u56b4\u8b39\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e VNet \u6a21\u578b\u80fd\u5920\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u8a9e\u97f3\uff0c\u4e26\u986f\u8457\u63d0\u5347\u8a9e\u97f3\u7de8\u78bc\u5668\u7684\u6548\u80fd\u3002", "author": "Yubing Cao et.al.", "authors": "Yubing Cao, Yongming Li, Liejun Wang, Yinfeng Yu", "id": "2408.06906v1", "paper_url": "http://arxiv.org/abs/2408.06906v1", "repo": "null"}}