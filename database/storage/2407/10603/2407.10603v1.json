{"2407.10603": {"publish_time": "2024-07-15", "title": "Leave No Knowledge Behind During Knowledge Distillation: Towards Practical and Effective Knowledge Distillation for Code-Switching ASR Using Realistic Data", "paper_summary": "Recent advances in automatic speech recognition (ASR) often rely on large\nspeech foundation models for generating high-quality transcriptions. However,\nthese models can be impractical due to limited computing resources. The\nsituation is even more severe in terms of more realistic or difficult\nscenarios, such as code-switching ASR (CS-ASR). To address this, we present a\nframework for developing more efficient models for CS-ASR through knowledge\ndistillation using realistic speech-only data. Our proposed method, Leave No\nKnowledge Behind During Knowledge Distillation (K$^2$D), leverages both the\nteacher model's knowledge and additional insights from a small auxiliary model.\nWe evaluate our approach on two in-domain and two out-domain datasets,\ndemonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled\nrealistic data, we have successfully obtained a 2-time smaller model with\n5-time faster generation speed while outperforming the baseline methods and the\nteacher model on all the testing sets. We have made our model publicly\navailable on Hugging Face\n(https://huggingface.co/andybi7676/k2d-whisper.zh-en).", "paper_summary_zh": "\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u7684\u6700\u65b0\u9032\u5c55\u901a\u5e38\u4ef0\u8cf4\u5927\u578b\u8a9e\u97f3\u57fa\u790e\u6a21\u578b\u4f86\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u8f49\u9304\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u7531\u65bc\u904b\u7b97\u8cc7\u6e90\u6709\u9650\uff0c\u53ef\u80fd\u4e0d\u5207\u5be6\u969b\u3002\u5728\u66f4\u903c\u771f\u6216\u56f0\u96e3\u7684\u5834\u666f\u4e2d\uff0c\u4f8b\u5982\u7a0b\u5f0f\u78bc\u8f49\u63db ASR (CS-ASR)\uff0c\u60c5\u6cc1\u66f4\u70ba\u56b4\u91cd\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u67b6\u69cb\uff0c\u900f\u904e\u4f7f\u7528\u903c\u771f\u7684\u8a9e\u97f3\u8cc7\u6599\u9032\u884c\u77e5\u8b58\u8403\u53d6\uff0c\u4f86\u958b\u767c\u66f4\u6709\u6548\u7387\u7684 CS-ASR \u6a21\u578b\u3002\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5728\u77e5\u8b58\u8403\u53d6\u904e\u7a0b\u4e2d\u4e0d\u907a\u6f0f\u4efb\u4f55\u77e5\u8b58 (K$^2$D)\uff0c\u540c\u6642\u5229\u7528\u6559\u5e2b\u6a21\u578b\u7684\u77e5\u8b58\u548c\u4f86\u81ea\u5c0f\u578b\u8f14\u52a9\u6a21\u578b\u7684\u984d\u5916\u898b\u89e3\u3002\u6211\u5011\u5728\u5169\u500b\u9818\u57df\u5167\u548c\u5169\u500b\u9818\u57df\u5916\u7684\u8cc7\u6599\u96c6\u4e0a\u8a55\u4f30\u6211\u5011\u7684\u65b9\u6cd5\uff0c\u8b49\u660e K$^2$D \u662f\u6709\u6548\u7684\u3002\u900f\u904e\u5728\u672a\u6a19\u8a18\u7684\u903c\u771f\u8cc7\u6599\u4e0a\u57f7\u884c K$^2$D\uff0c\u6211\u5011\u6210\u529f\u5730\u7372\u5f97\u4e00\u500b\u6a21\u578b\u5927\u5c0f\u7e2e\u5c0f 2 \u500d\uff0c\u7522\u751f\u901f\u5ea6\u5feb 5 \u500d\uff0c\u540c\u6642\u5728\u6240\u6709\u6e2c\u8a66\u96c6\u4e2d\u512a\u65bc\u57fa\u6e96\u65b9\u6cd5\u548c\u6559\u5e2b\u6a21\u578b\u3002\u6211\u5011\u5df2\u5c07\u6211\u5011\u7684\u6a21\u578b\u516c\u958b\u5728 Hugging Face\uff08https://huggingface.co/andybi7676/k2d-whisper.zh-en\uff09\u4e0a\u3002", "author": "Liang-Hsuan Tseng et.al.", "authors": "Liang-Hsuan Tseng, Zih-Ching Chen, Wei-Shun Chang, Cheng-Kuang Lee, Tsung-Ren Huang, Hung-yi Lee", "id": "2407.10603v1", "paper_url": "http://arxiv.org/abs/2407.10603v1", "repo": "null"}}