{"2406.12052": {"publish_time": "2024-06-17", "title": "UniGLM: Training One Unified Language Model for Text-Attributed Graphs", "paper_summary": "Representation learning on text-attributed graphs (TAGs), where nodes are\nrepresented by textual descriptions, is crucial for textual and relational\nknowledge systems and recommendation systems. Currently, state-of-the-art\nembedding methods for TAGs primarily focus on fine-tuning language models\n(e.g., BERT) using structure-aware training signals. While effective, these\nmethods are tailored for individual TAG and cannot generalize across various\ngraph scenarios. Given the shared textual space, leveraging multiple TAGs for\njoint fine-tuning, aligning text and graph structure from different aspects,\nwould be more beneficial. Motivated by this, we introduce a novel Unified Graph\nLanguage Model (UniGLM) framework, the first graph embedding model that\ngeneralizes well to both in-domain and cross-domain TAGs. Specifically, UniGLM\nis trained over multiple TAGs with different domains and scales using\nself-supervised contrastive learning. UniGLM includes an adaptive positive\nsample selection technique for identifying structurally similar nodes and a\nlazy contrastive module that is devised to accelerate training by minimizing\nrepetitive encoding calculations. Extensive empirical results across 9\nbenchmark TAGs demonstrate UniGLM's efficacy against leading embedding\nbaselines in terms of generalization (various downstream tasks and backbones)\nand transfer learning (in and out of domain scenarios). The code is available\nat https://github.com/NYUSHCS/UniGLM.", "paper_summary_zh": "\u5728\u6587\u5b57\u5c5e\u6027\u56fe (TAG) \u4e0a\u7684\u8868\u5f81\u5b66\u4e60\uff0c\u5176\u4e2d\u8282\u70b9\u7531\u6587\u5b57\u63cf\u8ff0\u8868\u793a\uff0c\u5bf9\u4e8e\u6587\u5b57\u548c\u5173\u7cfb\u77e5\u8bc6\u7cfb\u7edf\u4ee5\u53ca\u63a8\u8350\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\uff0cTAG \u7684\u6700\u5148\u8fdb\u5d4c\u5165\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4f7f\u7528\u7ed3\u6784\u611f\u77e5\u8bad\u7ec3\u4fe1\u53f7\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff08\u4f8b\u5982\uff0cBERT\uff09\u3002\u867d\u7136\u6709\u6548\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u662f\u9488\u5bf9\u5355\u4e2a TAG \u91cf\u8eab\u5b9a\u5236\u7684\uff0c\u5e76\u4e14\u65e0\u6cd5\u6982\u62ec\u5230\u5404\u79cd\u56fe\u573a\u666f\u3002\u9274\u4e8e\u5171\u4eab\u7684\u6587\u672c\u7a7a\u95f4\uff0c\u5229\u7528\u591a\u4e2a TAG \u8fdb\u884c\u8054\u5408\u5fae\u8c03\uff0c\u4ece\u4e0d\u540c\u65b9\u9762\u8c03\u6574\u6587\u672c\u548c\u56fe\u7ed3\u6784\uff0c\u5c06\u66f4\u6709\u76ca\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u4e00\u56fe\u8bed\u8a00\u6a21\u578b (UniGLM) \u6846\u67b6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5728\u57df\u5185\u548c\u8de8\u57df TAG \u4e2d\u90fd\u80fd\u5f88\u597d\u5730\u6982\u62ec\u7684\u56fe\u5d4c\u5165\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0cUniGLM \u4f7f\u7528\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u5728\u5177\u6709\u4e0d\u540c\u57df\u548c\u89c4\u6a21\u7684\u591a\u4e2a TAG \u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002UniGLM \u5305\u62ec\u4e00\u79cd\u81ea\u9002\u5e94\u6b63\u6837\u672c\u9009\u62e9\u6280\u672f\uff0c\u7528\u4e8e\u8bc6\u522b\u7ed3\u6784\u76f8\u4f3c\u7684\u8282\u70b9\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5ef6\u8fdf\u5bf9\u6bd4\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u65e8\u5728\u901a\u8fc7\u6700\u5c0f\u5316\u91cd\u590d\u7f16\u7801\u8ba1\u7b97\u6765\u52a0\u901f\u8bad\u7ec3\u3002\u8de8 9 \u4e2a\u57fa\u51c6 TAG \u7684\u5e7f\u6cdb\u5b9e\u8bc1\u7ed3\u679c\u8bc1\u660e\u4e86 UniGLM \u5728\u6cdb\u5316\uff08\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u548c\u4e3b\u5e72\uff09\u548c\u8fc1\u79fb\u5b66\u4e60\uff08\u57df\u5185\u548c\u57df\u5916\u573a\u666f\uff09\u65b9\u9762\u76f8\u5bf9\u4e8e\u9886\u5148\u5d4c\u5165\u57fa\u51c6\u7684\u529f\u6548\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/NYUSHCS/UniGLM \u83b7\u5f97\u3002", "author": "Yi Fang et.al.", "authors": "Yi Fang, Dongzhe Fan, Sirui Ding, Ninghao Liu, Qiaoyu Tan", "id": "2406.12052v1", "paper_url": "http://arxiv.org/abs/2406.12052v1", "repo": "null"}}