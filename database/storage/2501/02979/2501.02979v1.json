{"2501.02979": {"publish_time": "2025-01-06", "title": "Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation", "paper_summary": "The multilingual neural machine translation (MNMT) enables arbitrary\ntranslations across multiple languages by training a model with limited\nparameters using parallel data only. However, the performance of such MNMT\nmodels still lags behind that of large language models (LLMs), limiting their\npracticality. In this work, we address this limitation by introducing\nregistering to achieve the new state-of-the-art of decoder-only MNMT models.\nSpecifically, we insert a set of artificial tokens specifying the target\nlanguage, called registers, into the input sequence between the source and\ntarget tokens. By modifying the attention mask, the target token generation\nonly pays attention to the activation of registers, representing the source\ntokens in the target language space. Experiments on EC-40, a large-scale\nbenchmark, show that our method outperforms related methods driven by\noptimizing multilingual representations. We further scale up and collect 9.3\nbillion sentence pairs across 24 languages from public datasets to pre-train\ntwo models, namely MITRE (multilingual translation with registers). One of\nthem, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with\ncommercial LLMs, and shows strong adaptability in fine-tuning. Finally, we\nopen-source our models to facilitate further research and development in MNMT:\nhttps://github.com/zhiqu22/mitre.", "paper_summary_zh": "\u591a\u8a9e\u8a00\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f (MNMT) \u50c5\u4f7f\u7528\u5e73\u884c\u6578\u64da\u8a13\u7df4\u5177\u6709\u6709\u9650\u53c3\u6578\u7684\u6a21\u578b\uff0c\u5373\u53ef\u5728\u591a\u7a2e\u8a9e\u8a00\u4e4b\u9593\u9032\u884c\u4efb\u610f\u7ffb\u8b6f\u3002\u7136\u800c\uff0c\u6b64\u985e MNMT \u6a21\u578b\u7684\u6548\u80fd\u4ecd\u843d\u5f8c\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u9650\u5236\u4e86\u5176\u5be6\u7528\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5f15\u9032\u8a3b\u518a\u4f86\u89e3\u6c7a\u6b64\u9650\u5236\uff0c\u4ee5\u9054\u6210\u50c5\u89e3\u78bc\u5668 MNMT \u6a21\u578b\u7684\u6700\u65b0\u6280\u8853\u6c34\u6e96\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5728\u4f86\u6e90\u548c\u76ee\u6a19\u6a19\u8a18\u4e4b\u9593\u7684\u8f38\u5165\u5e8f\u5217\u4e2d\u63d2\u5165\u4e00\u7d44\u6307\u5b9a\u76ee\u6a19\u8a9e\u8a00\u7684\u4eba\u5de5\u6a19\u8a18\uff0c\u7a31\u70ba\u8a3b\u518a\u3002\u900f\u904e\u4fee\u6539\u6ce8\u610f\u529b\u906e\u7f69\uff0c\u76ee\u6a19\u6a19\u8a18\u7522\u751f\u53ea\u6703\u6ce8\u610f\u8a3b\u518a\u7684\u555f\u52d5\uff0c\u8868\u793a\u76ee\u6a19\u8a9e\u8a00\u7a7a\u9593\u4e2d\u7684\u4f86\u6e90\u6a19\u8a18\u3002\u5728\u5927\u578b\u57fa\u6e96 EC-40 \u4e0a\u9032\u884c\u7684\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u512a\u65bc\u7531\u6700\u4f73\u5316\u591a\u8a9e\u8a00\u8868\u793a\u9a45\u52d5\u7684\u76f8\u95dc\u65b9\u6cd5\u3002\u6211\u5011\u9032\u4e00\u6b65\u64f4\u5c55\u898f\u6a21\uff0c\u4e26\u5f9e\u516c\u5171\u8cc7\u6599\u96c6\u4e2d\u6536\u96c6\u4e86 24 \u7a2e\u8a9e\u8a00\u7684 93 \u5104\u500b\u53e5\u5b50\u5c0d\uff0c\u4ee5\u9810\u5148\u8a13\u7df4\u5169\u500b\u6a21\u578b\uff0c\u5373 MITRE\uff08\u5177\u8a3b\u518a\u7684\u591a\u8a9e\u8a00\u7ffb\u8b6f\uff09\u3002\u5176\u4e2d\u4e4b\u4e00\uff0cMITRE-913M\uff0c\u512a\u65bc NLLB-3.3B\uff0c\u9054\u5230\u8207\u5546\u7528 LLM \u76f8\u7576\u7684\u6548\u80fd\uff0c\u4e26\u5728\u5fae\u8abf\u4e2d\u5c55\u73fe\u5f37\u5927\u7684\u9069\u61c9\u6027\u3002\u6700\u5f8c\uff0c\u6211\u5011\u958b\u653e\u539f\u59cb\u78bc\u6a21\u578b\uff0c\u4ee5\u5229\u65bc\u9032\u4e00\u6b65\u7814\u7a76\u548c\u958b\u767c MNMT\uff1a\nhttps://github.com/zhiqu22/mitre\u3002", "author": "Zhi Qu et.al.", "authors": "Zhi Qu, Yiran Wang, Jiannan Mao, Chenchen Ding, Hideki Tanaka, Masao Utiyama, Taro Watanabe", "id": "2501.02979v1", "paper_url": "http://arxiv.org/abs/2501.02979v1", "repo": "https://github.com/zhiqu22/mitre"}}