{"2410.24200": {"publish_time": "2024-10-31", "title": "Length-Induced Embedding Collapse in Transformer-based Models", "paper_summary": "Text embeddings enable various applications, but their performance\ndeteriorates on longer texts. In this paper, we find that the performance\ndegradation is due to a phenomenon called Length Collapse, where longer text\nembeddings collapse into a narrow space. This collapse results in a\ndistributional inconsistency between embeddings of different text lengths,\nultimately hurting the performance of downstream tasks. Theoretically, by\nconsidering the self-attention mechanism inherently functions as a low-pass\nfilter, we prove that long sequences increase the attenuation rate of the\nlow-pass filter effect of the self-attention mechanism. With layers going\ndeeper, excessive low-pass filtering causes the token signals to retain only\ntheir Direct-Current (DC) component, which means the input token feature maps\nwill collapse into a narrow space, especially in long texts. Based on the above\nanalysis, we propose to mitigate the undesirable length collapse limitation by\nintroducing a temperature in softmax(), which achieves a higher low-filter\nattenuation rate. The tuning-free method, called TempScale, can be plugged into\nmultiple transformer-based embedding models. Empirically, we demonstrate that\nTempScale can improve existing embedding models, especially on long text\ninputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text\nEmbedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from\nLongEmbed, which specifically focuses on long context retrieval.", "paper_summary_zh": "\u6587\u5b57\u5d4c\u5165\u53ef\u555f\u7528\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\uff0c\u4f46\u5176\u6548\u80fd\u6703\u5728\u8f03\u9577\u7684\u6587\u5b57\u4e2d\u4e0b\u964d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u6548\u80fd\u4e0b\u964d\u662f\u56e0\u7232\u4e00\u500b\u7a31\u70ba\u9577\u5ea6\u5d29\u6f70\u7684\u73fe\u8c61\uff0c\u5176\u4e2d\u8f03\u9577\u7684\u6587\u5b57\u5d4c\u5165\u6703\u5d29\u6f70\u6210\u4e00\u500b\u72f9\u7a84\u7684\u7a7a\u9593\u3002\u6b64\u5d29\u6f70\u5c0e\u81f4\u4e0d\u540c\u6587\u5b57\u9577\u5ea6\u7684\u5d4c\u5165\u4e4b\u9593\u7684\u5206\u914d\u4e0d\u4e00\u81f4\uff0c\u6700\u7d42\u640d\u5bb3\u4e0b\u6e38\u4efb\u52d9\u7684\u6548\u80fd\u3002\u7406\u8ad6\u4e0a\uff0c\u900f\u904e\u8003\u616e\u81ea\u6211\u6ce8\u610f\u6a5f\u5236\u672c\u8cea\u4e0a\u4f5c\u70ba\u4f4e\u901a\u6ffe\u6ce2\u5668\u904b\u4f5c\uff0c\u6211\u5011\u8b49\u660e\u9577\u5e8f\u5217\u6703\u589e\u52a0\u81ea\u6211\u6ce8\u610f\u6a5f\u5236\u7684\u4f4e\u901a\u6ffe\u6ce2\u5668\u6548\u61c9\u7684\u8870\u6e1b\u7387\u3002\u96a8\u8457\u5c64\u6b21\u66f4\u6df1\u5165\uff0c\u904e\u5ea6\u7684\u4f4e\u901a\u6ffe\u6ce2\u6703\u5c0e\u81f4\u6b0a\u6756\u8a0a\u865f\u50c5\u4fdd\u7559\u5176\u76f4\u6d41 (DC) \u6210\u5206\uff0c\u9019\u8868\u793a\u8f38\u5165\u6b0a\u6756\u7279\u5fb5\u5716\u6703\u5d29\u6f70\u6210\u4e00\u500b\u72f9\u7a84\u7684\u7a7a\u9593\uff0c\u5c24\u5176\u662f\u5728\u9577\u6587\u5b57\u4e2d\u3002\u6839\u64da\u4ee5\u4e0a\u7684\u5206\u6790\uff0c\u6211\u5011\u63d0\u8b70\u900f\u904e\u5728 softmax() \u4e2d\u5f15\u5165\u6eab\u5ea6\u4f86\u6e1b\u8f15\u4e0d\u5e0c\u671b\u7684\u9577\u5ea6\u5d29\u6f70\u9650\u5236\uff0c\u9019\u6703\u9054\u6210\u66f4\u9ad8\u7684\u4f4e\u6ffe\u6ce2\u5668\u8870\u6e1b\u7387\u3002\u7a31\u70ba TempScale \u7684\u7121\u8abf\u6821\u65b9\u6cd5\u53ef\u4ee5\u63d2\u5165\u591a\u500b\u57fa\u65bcTransformer\u7684\u5d4c\u5165\u6a21\u578b\u4e2d\u3002\u6839\u64da\u7d93\u9a57\uff0c\u6211\u5011\u8b49\u660e TempScale \u53ef\u4ee5\u6539\u5584\u73fe\u6709\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u9577\u6587\u5b57\u8f38\u5165\u4e0a\uff0c\u5728 Massive Text Embedding Benchmark (MTEB) \u7684 40 \u500b\u8cc7\u6599\u96c6\u4e0a\u5e36\u4f86\u9ad8\u9054 0.53% \u7684\u6548\u80fd\u63d0\u5347\uff0c\u4ee5\u53ca\u5728 LongEmbed \u7684 4 \u500b\u8cc7\u6599\u96c6\u4e0a\u5e36\u4f86 0.82% \u7684\u6548\u80fd\u63d0\u5347\uff0cLongEmbed \u7279\u5225\u5c08\u6ce8\u65bc\u9577\u8108\u7d61\u6aa2\u7d22\u3002", "author": "Yuqi Zhou et.al.", "authors": "Yuqi Zhou, Sunhao Dai, Zhanshuo Cao, Xiao Zhang, Jun Xu", "id": "2410.24200v1", "paper_url": "http://arxiv.org/abs/2410.24200v1", "repo": "null"}}