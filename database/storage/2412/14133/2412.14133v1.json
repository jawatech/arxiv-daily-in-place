{"2412.14133": {"publish_time": "2024-12-18", "title": "Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models", "paper_summary": "Vision-language models (VLMs) excel at extracting and reasoning about\ninformation from images. Yet, their capacity to leverage internal knowledge\nabout specific entities remains underexplored. This work investigates the\ndisparity in model performance when answering factual questions about an entity\ndescribed in text versus depicted in an image. Our results reveal a significant\naccuracy drop --averaging 19%-- when the entity is presented visually instead\nof textually. We hypothesize that this decline arises from limitations in how\ninformation flows from image tokens to query tokens. We use mechanistic\ninterpretability tools to reveal that, although image tokens are preprocessed\nby the vision encoder, meaningful information flow from these tokens occurs\nonly in the much deeper layers. Furthermore, critical image processing happens\nin the language model's middle layers, allowing few layers for consecutive\nreasoning, highlighting a potential inefficiency in how the model utilizes its\nlayers for reasoning. These insights shed light on the internal mechanics of\nVLMs and offer pathways for enhancing their reasoning capabilities.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u64c5\u9577\u5f9e\u5f71\u50cf\u4e2d\u8403\u53d6\u548c\u63a8\u7406\u8cc7\u8a0a\u3002\u7136\u800c\uff0c\u5b83\u5011\u5229\u7528\u95dc\u65bc\u7279\u5b9a\u5be6\u9ad4\u7684\u5167\u90e8\u77e5\u8b58\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9019\u9805\u5de5\u4f5c\u63a2\u8a0e\u4e86\u5728\u56de\u7b54\u95dc\u65bc\u6587\u672c\u4e2d\u63cf\u8ff0\u6216\u5f71\u50cf\u4e2d\u63cf\u7e6a\u7684\u5be6\u9ad4\u7684\u4e8b\u5be6\u554f\u984c\u6642\uff0c\u6a21\u578b\u6548\u80fd\u7684\u5dee\u7570\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u7576\u5be6\u9ad4\u4ee5\u8996\u89ba\u65b9\u5f0f\u800c\u975e\u6587\u5b57\u65b9\u5f0f\u5448\u73fe\u6642\uff0c\u6e96\u78ba\u5ea6\u6703\u986f\u8457\u4e0b\u964d\u2014\u2014\u5e73\u5747\u4e0b\u964d 19%\u3002\u6211\u5011\u5047\u8a2d\u9019\u7a2e\u4e0b\u964d\u662f\u7531\u65bc\u5f71\u50cf\u6a19\u8a18\u5982\u4f55\u6d41\u5411\u67e5\u8a62\u6a19\u8a18\u7684\u9650\u5236\u3002\u6211\u5011\u4f7f\u7528\u6a5f\u5236\u53ef\u89e3\u91cb\u6027\u5de5\u5177\u4f86\u63ed\u793a\uff0c\u5118\u7ba1\u5f71\u50cf\u6a19\u8a18\u662f\u7531\u8996\u89ba\u7de8\u78bc\u5668\u9810\u5148\u8655\u7406\u7684\uff0c\u4f46\u5f9e\u9019\u4e9b\u6a19\u8a18\u7684\u6709\u610f\u7fa9\u7684\u8cc7\u8a0a\u6d41\u50c5\u767c\u751f\u5728\u66f4\u6df1\u7684\u5c64\u7d1a\u4e2d\u3002\u6b64\u5916\uff0c\u91cd\u8981\u7684\u5f71\u50cf\u8655\u7406\u767c\u751f\u5728\u8a9e\u8a00\u6a21\u578b\u7684\u4e2d\u9593\u5c64\uff0c\u5141\u8a31\u5c11\u6578\u5c64\u9032\u884c\u9023\u7e8c\u63a8\u7406\uff0c\u7a81\u986f\u4e86\u6a21\u578b\u5982\u4f55\u5229\u7528\u5176\u5c64\u9032\u884c\u63a8\u7406\u7684\u6f5b\u5728\u4f4e\u6548\u7387\u3002\u9019\u4e9b\u898b\u89e3\u95e1\u660e\u4e86 VLM \u7684\u5167\u90e8\u6a5f\u5236\uff0c\u4e26\u63d0\u4f9b\u4e86\u589e\u5f37\u5176\u63a8\u7406\u80fd\u529b\u7684\u9014\u5f91\u3002", "author": "Ido Cohen et.al.", "authors": "Ido Cohen, Daniela Gottesman, Mor Geva, Raja Giryes", "id": "2412.14133v1", "paper_url": "http://arxiv.org/abs/2412.14133v1", "repo": "https://github.com/ido-co/vlm-modality-gap"}}