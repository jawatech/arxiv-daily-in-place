{"2411.09009": {"publish_time": "2024-11-13", "title": "Cut Your Losses in Large-Vocabulary Language Models", "paper_summary": "As language models grow ever larger, so do their vocabularies. This has\nshifted the memory footprint of LLMs during training disproportionately to one\nsingle layer: the cross-entropy in the loss computation. Cross-entropy builds\nup a logit matrix with entries for each pair of input tokens and vocabulary\nitems and, for small models, consumes an order of magnitude more memory than\nthe rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that\ncomputes the cross-entropy loss without materializing the logits for all tokens\ninto global memory. Rather, CCE only computes the logit for the correct token\nand evaluates the log-sum-exp over all logits on the fly. We implement a custom\nkernel that performs the matrix multiplications and the log-sum-exp reduction\nover the vocabulary in flash memory, making global memory consumption for the\ncross-entropy computation negligible. This has a dramatic effect. Taking the\nGemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss\ncomputation from 24 GB to 1 MB, and the total training-time memory consumption\nof the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we\nleverage the inherent sparsity of softmax and propose to skip elements of the\ngradient computation that have a negligible (i.e., below numerical precision)\ncontribution to the gradient. Experiments demonstrate that the dramatic\nreduction in memory consumption is accomplished without sacrificing training\nspeed or convergence.", "paper_summary_zh": "\u96a8\u8457\u8a9e\u8a00\u6a21\u578b\u8d8a\u4f86\u8d8a\u9f90\u5927\uff0c\u5176\u8a5e\u5f59\u91cf\u4e5f\u96a8\u4e4b\u589e\u52a0\u3002\u9019\u4f7f\u5f97\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u8a13\u7df4\u671f\u9593\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u4e0d\u6210\u6bd4\u4f8b\u5730\u8f49\u79fb\u5230\u55ae\u4e00\u5c64\uff1a\u640d\u5931\u8a08\u7b97\u4e2d\u7684\u4ea4\u53c9\u71b5\u3002\u4ea4\u53c9\u71b5\u5efa\u7acb\u4e00\u500b\u5c0d\u6578\u6a5f\u7387\u77e9\u9663\uff0c\u5176\u4e2d\u5305\u542b\u8f38\u5165 token \u548c\u8a5e\u5f59\u9805\u76ee\u6bcf\u5c0d\u7684\u689d\u76ee\uff0c\u5c0d\u65bc\u5c0f\u578b\u6a21\u578b\uff0c\u5176\u6d88\u8017\u7684\u8a18\u61b6\u9ad4\u6578\u91cf\u7d1a\u9ad8\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5176\u4ed6\u90e8\u5206\u7684\u7e3d\u548c\u3002\u6211\u5011\u63d0\u51fa\u5207\u65b7\u4ea4\u53c9\u71b5 (CCE)\uff0c\u9019\u662f\u4e00\u7a2e\u5728\u4e0d\u5c07\u6240\u6709 token \u7684\u5c0d\u6578\u6a5f\u7387\u5be6\u9ad4\u5316\u5230\u5168\u57df\u8a18\u61b6\u9ad4\u4e2d\u60c5\u6cc1\u4e0b\u8a08\u7b97\u4ea4\u53c9\u71b5\u640d\u5931\u7684\u65b9\u6cd5\u3002\u76f8\u53cd\uff0cCCE \u50c5\u8a08\u7b97\u6b63\u78ba token \u7684\u5c0d\u6578\u6a5f\u7387\uff0c\u4e26\u5373\u6642\u8a55\u4f30\u6240\u6709\u5c0d\u6578\u6a5f\u7387\u7684\u5c0d\u6578\u548c\u6307\u6578\u3002\u6211\u5011\u5be6\u4f5c\u4e00\u500b\u81ea\u8a02\u6838\uff0c\u5728\u5feb\u9583\u8a18\u61b6\u9ad4\u4e2d\u57f7\u884c\u77e9\u9663\u4e58\u6cd5\u548c\u5c0d\u6578\u548c\u6307\u6578\u7d04\u7c21\uff0c\u4f7f\u4ea4\u53c9\u71b5\u8a08\u7b97\u7684\u5168\u57df\u8a18\u61b6\u9ad4\u6d88\u8017\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\u3002\u9019\u6709\u986f\u8457\u7684\u6548\u679c\u3002\u4ee5 Gemma 2 (2B) \u6a21\u578b\u70ba\u4f8b\uff0cCCE \u5c07\u640d\u5931\u8a08\u7b97\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u5f9e 24 GB \u6e1b\u5c11\u5230 1 MB\uff0c\u4e26\u5c07\u5206\u985e\u5668\u982d\u7684\u7e3d\u8a13\u7df4\u6642\u9593\u8a18\u61b6\u9ad4\u6d88\u8017\u5f9e 28 GB \u6e1b\u5c11\u5230 1 GB\u3002\u70ba\u4e86\u63d0\u9ad8 CCE \u7684\u8655\u7406\u91cf\uff0c\u6211\u5011\u5229\u7528 softmax \u7684\u5167\u5728\u7a00\u758f\u6027\uff0c\u4e26\u5efa\u8b70\u8df3\u904e\u5c0d\u68af\u5ea6\u8ca2\u737b\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\uff08\u5373\u4f4e\u65bc\u6578\u503c\u7cbe\u5ea6\uff09\u7684\u68af\u5ea6\u8a08\u7b97\u5143\u7d20\u3002\u5be6\u9a57\u8b49\u660e\uff0c\u5728\u4e0d\u72a7\u7272\u8a13\u7df4\u901f\u5ea6\u6216\u6536\u6582\u6027\u7684\u60c5\u6cc1\u4e0b\uff0c\u53ef\u4ee5\u5927\u5e45\u6e1b\u5c11\u8a18\u61b6\u9ad4\u6d88\u8017\u3002", "author": "Erik Wijmans et.al.", "authors": "Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, Philipp Kr\u00e4henb\u00fchl", "id": "2411.09009v1", "paper_url": "http://arxiv.org/abs/2411.09009v1", "repo": "https://github.com/apple/ml-cross-entropy"}}