{"2409.06579": {"publish_time": "2024-09-10", "title": "Quantifying and Enabling the Interpretability of CLIP-like Models", "paper_summary": "CLIP is one of the most popular foundational models and is heavily used for\nmany vision-language tasks. However, little is known about the inner workings\nof CLIP. To bridge this gap we propose a study to quantify the interpretability\nin CLIP like models. We conduct this study on six different CLIP models from\nOpenAI and OpenCLIP which vary by size, type of pre-training data and patch\nsize. Our approach begins with using the TEXTSPAN algorithm and in-context\nlearning to break down individual attention heads into specific properties. We\nthen evaluate how easily these heads can be interpreted using new metrics which\nmeasure property consistency within heads and property disentanglement across\nheads. Our findings reveal that larger CLIP models are generally more\ninterpretable than their smaller counterparts. To further assist users in\nunderstanding the inner workings of CLIP models, we introduce CLIP-InterpreT, a\ntool designed for interpretability analysis. CLIP-InterpreT offers five types\nof analyses: property-based nearest neighbor search, per-head topic\nsegmentation, contrastive segmentation, per-head nearest neighbors of an image,\nand per-head nearest neighbors of text.", "paper_summary_zh": "CLIP \u662f\u6700\u6d41\u884c\u7684\u57fa\u7840\u6a21\u578b\u4e4b\u4e00\uff0c\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bb8\u591a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u3002\u7136\u800c\uff0c\u5173\u4e8e CLIP \u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u77e5\u4e4b\u751a\u5c11\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u9879\u7814\u7a76\u6765\u91cf\u5316 CLIP \u7b49\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u5bf9 OpenAI \u548c OpenCLIP \u7684\u516d\u4e2a\u4e0d\u540c\u7684 CLIP \u6a21\u578b\u8fdb\u884c\u4e86\u8fd9\u9879\u7814\u7a76\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u5927\u5c0f\u3001\u9884\u8bad\u7ec3\u6570\u636e\u7c7b\u578b\u548c\u8865\u4e01\u5927\u5c0f\u5404\u4e0d\u76f8\u540c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9996\u5148\u4f7f\u7528 TEXTSPAN \u7b97\u6cd5\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c06\u5404\u4e2a\u6ce8\u610f\u529b\u5934\u5206\u89e3\u4e3a\u7279\u5b9a\u5c5e\u6027\u3002\u7136\u540e\u6211\u4eec\u8bc4\u4f30\u4f7f\u7528\u65b0\u6307\u6807\u89e3\u91ca\u8fd9\u4e9b\u5934\u7684\u5bb9\u6613\u7a0b\u5ea6\uff0c\u8fd9\u4e9b\u6307\u6807\u6d4b\u91cf\u5934\u5185\u5c5e\u6027\u4e00\u81f4\u6027\u548c\u5934\u95f4\u5c5e\u6027\u89e3\u8026\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u5927\u7684 CLIP \u6a21\u578b\u901a\u5e38\u6bd4\u8f83\u5c0f\u7684\u6a21\u578b\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u5e2e\u52a9\u7528\u6237\u4e86\u89e3 CLIP \u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\uff0c\u6211\u4eec\u5f15\u5165\u4e86 CLIP-InterpreT\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u53ef\u89e3\u91ca\u6027\u5206\u6790\u800c\u8bbe\u8ba1\u7684\u5de5\u5177\u3002CLIP-InterpreT \u63d0\u4f9b\u4e94\u79cd\u7c7b\u578b\u7684\u5206\u6790\uff1a\u57fa\u4e8e\u5c5e\u6027\u7684\u6700\u8fd1\u90bb\u641c\u7d22\u3001\u6bcf\u4e2a\u5934\u7684\u4e3b\u9898\u5206\u5272\u3001\u5bf9\u6bd4\u5206\u5272\u3001\u56fe\u50cf\u7684\u6bcf\u4e2a\u5934\u7684\u6700\u8fd1\u90bb\u548c\u6587\u672c\u7684\u6bcf\u4e2a\u5934\u7684\u6700\u8fd1\u90bb\u3002", "author": "Avinash Madasu et.al.", "authors": "Avinash Madasu, Yossi Gandelsman, Vasudev Lal, Phillip Howard", "id": "2409.06579v1", "paper_url": "http://arxiv.org/abs/2409.06579v1", "repo": "null"}}