{"2406.08100": {"publish_time": "2024-06-12", "title": "Multimodal Table Understanding", "paper_summary": "Although great progress has been made by previous table understanding methods\nincluding recent approaches based on large language models (LLMs), they rely\nheavily on the premise that given tables must be converted into a certain text\nsequence (such as Markdown or HTML) to serve as model input. However, it is\ndifficult to access such high-quality textual table representations in some\nreal-world scenarios, and table images are much more accessible. Therefore, how\nto directly understand tables using intuitive visual information is a crucial\nand urgent challenge for developing more practical applications. In this paper,\nwe propose a new problem, multimodal table understanding, where the model needs\nto generate correct responses to various table-related requests based on the\ngiven table image. To facilitate both the model training and evaluation, we\nconstruct a large-scale dataset named MMTab, which covers a wide spectrum of\ntable images, instructions and tasks. On this basis, we develop Table-LLaVA, a\ngeneralist tabular multimodal large language model (MLLM), which significantly\noutperforms recent open-source MLLM baselines on 23 benchmarks under held-in\nand held-out settings. The code and data is available at this\nhttps://github.com/SpursGoZmy/Table-LLaVA", "paper_summary_zh": "\u5118\u7ba1\u5148\u524d\u7684\u8868\u683c\u7406\u89e3\u65b9\u6cd5\u5df2\u53d6\u5f97\u91cd\u5927\u9032\u5c55\uff0c\u5305\u62ec\u57fa\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u4f46\u5b83\u5011\u56b4\u91cd\u4f9d\u8cf4\u65bc\u4ee5\u4e0b\u524d\u63d0\uff1a\u7d66\u5b9a\u7684\u8868\u683c\u5fc5\u9808\u8f49\u63db\u70ba\u67d0\u500b\u6587\u5b57\u5e8f\u5217\uff08\u4f8b\u5982 Markdown \u6216 HTML\uff09\u624d\u80fd\u4f5c\u70ba\u6a21\u578b\u8f38\u5165\u3002\u7136\u800c\uff0c\u5728\u67d0\u4e9b\u5be6\u969b\u5834\u666f\u4e2d\u96e3\u4ee5\u5b58\u53d6\u6b64\u985e\u9ad8\u54c1\u8cea\u7684\u6587\u5b57\u8868\u683c\u8868\u793a\uff0c\u800c\u8868\u683c\u5f71\u50cf\u5247\u66f4\u5bb9\u6613\u5b58\u53d6\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u76f4\u63a5\u4f7f\u7528\u76f4\u89ba\u7684\u8996\u89ba\u8cc7\u8a0a\u7406\u89e3\u8868\u683c\uff0c\u662f\u958b\u767c\u66f4\u591a\u5be6\u7528\u61c9\u7528\u7a0b\u5f0f\u7684\u4e00\u9805\u95dc\u9375\u4e14\u8feb\u5207\u7684\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684\u554f\u984c\uff0c\u591a\u6a21\u614b\u8868\u683c\u7406\u89e3\uff0c\u5176\u4e2d\u6a21\u578b\u9700\u8981\u6839\u64da\u7d66\u5b9a\u7684\u8868\u683c\u5f71\u50cf\uff0c\u5c0d\u5404\u7a2e\u8207\u8868\u683c\u76f8\u95dc\u7684\u8acb\u6c42\u7522\u751f\u6b63\u78ba\u7684\u56de\u61c9\u3002\u70ba\u4e86\u4fc3\u9032\u6a21\u578b\u8a13\u7df4\u548c\u8a55\u4f30\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u540d\u70ba MMTab \u7684\u5927\u578b\u8cc7\u6599\u96c6\uff0c\u6db5\u84cb\u4e86\u5ee3\u6cdb\u7684\u8868\u683c\u5f71\u50cf\u3001\u8aaa\u660e\u548c\u4efb\u52d9\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u958b\u767c\u4e86 Table-LLaVA\uff0c\u4e00\u500b\u901a\u7528\u7684\u8868\u683c\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM)\uff0c\u5728 23 \u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0c\u5728\u5167\u90e8\u548c\u5916\u90e8\u8a2d\u5b9a\u4e0b\uff0c\u5176\u6548\u80fd\u986f\u8457\u512a\u65bc\u6700\u8fd1\u7684\u958b\u6e90 MLLM \u57fa\u6e96\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728\u6b64\u53d6\u5f97\uff1ahttps://github.com/SpursGoZmy/Table-LLaVA", "author": "Mingyu Zheng et.al.", "authors": "Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, Weiping Wang", "id": "2406.08100v1", "paper_url": "http://arxiv.org/abs/2406.08100v1", "repo": "https://github.com/spursgozmy/table-llava"}}