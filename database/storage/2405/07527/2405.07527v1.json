{"2405.07527": {"publish_time": "2024-05-13", "title": "Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models", "paper_summary": "Despite their prevalence in deep-learning communities, over-parameterized\nmodels convey high demands of computational costs for proper training. This\nwork studies the fine-grained, modular-level learning dynamics of\nover-parameterized models to attain a more efficient and fruitful training\nstrategy. Empirical evidence reveals that when scaling down into network\nmodules, such as heads in self-attention models, we can observe varying\nlearning patterns implicitly associated with each module's trainability. To\ndescribe such modular-level learning capabilities, we introduce a novel concept\ndubbed modular neural tangent kernel (mNTK), and we demonstrate that the\nquality of a module's learning is tightly associated with its mNTK's principal\neigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module\nlearns features with better convergence, while those miniature ones may impact\ngeneralization negatively. Inspired by the discovery, we propose a novel\ntraining strategy termed Modular Adaptive Training (MAT) to update those\nmodules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively,\nconcentrating the model on learning common features and ignoring those\ninconsistent ones. Unlike most existing training schemes with a complete BP\ncycle across all network modules, MAT can significantly save computations by\nits partially-updating strategy and can further improve performance.\nExperiments show that MAT nearly halves the computational cost of model\ntraining and outperforms the accuracy of baselines.", "paper_summary_zh": "\u5118\u7ba1\u5728\u6df1\u5ea6\u5b78\u7fd2\u793e\u7fa4\u4e2d\u5f88\u666e\u904d\uff0c\u4f46\u904e\u5ea6\u53c3\u6578\u5316\u7684\u6a21\u578b\u5c0d\u9069\u7576\u8a13\u7df4\u63d0\u51fa\u4e86\u9ad8\u8a08\u7b97\u6210\u672c\u9700\u6c42\u3002\u9019\u9805\u7814\u7a76\u63a2\u8a0e\u904e\u5ea6\u53c3\u6578\u5316\u6a21\u578b\u7684\u7d30\u7c92\u5ea6\u3001\u6a21\u7d44\u5c64\u7d1a\u5b78\u7fd2\u52d5\u614b\uff0c\u4ee5\u9054\u6210\u66f4\u6709\u6548\u7387\u4e14\u6709\u6210\u6548\u7684\u8a13\u7df4\u7b56\u7565\u3002\u5be6\u8b49\u8b49\u64da\u986f\u793a\uff0c\u7576\u7e2e\u5c0f\u5230\u7db2\u8def\u6a21\u7d44\uff08\u4f8b\u5982\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u4e2d\u7684 heads\uff09\u6642\uff0c\u6211\u5011\u53ef\u4ee5\u89c0\u5bdf\u5230\u8207\u6bcf\u500b\u6a21\u7d44\u7684\u53ef\u8a13\u7df4\u6027\u96b1\u542b\u95dc\u806f\u7684\u4e0d\u540c\u5b78\u7fd2\u6a21\u5f0f\u3002\u70ba\u4e86\u63cf\u8ff0\u9019\u7a2e\u6a21\u7d44\u5c64\u7d1a\u5b78\u7fd2\u80fd\u529b\uff0c\u6211\u5011\u5f15\u9032\u4e00\u500b\u65b0\u6982\u5ff5\uff0c\u7a31\u70ba\u6a21\u7d44\u795e\u7d93\u5207\u7dda\u6838\uff08mNTK\uff09\uff0c\u4e26\u8b49\u660e\u6a21\u7d44\u5b78\u7fd2\u54c1\u8cea\u8207\u5176 mNTK \u7684\u4e3b\u7279\u5fb5\u503c $\\lambda_{\\max}$ \u7dca\u5bc6\u76f8\u95dc\u3002\u5927\u7684 $\\lambda_{\\max}$ \u8868\u793a\u6a21\u7d44\u5b78\u7fd2\u5177\u6709\u8f03\u4f73\u6536\u6582\u6027\u7684\u7279\u5fb5\uff0c\u800c\u90a3\u4e9b\u5fae\u5c0f\u7684\u7279\u5fb5\u53ef\u80fd\u6703\u5c0d\u6cdb\u5316\u7522\u751f\u8ca0\u9762\u5f71\u97ff\u3002\u53d7\u5230\u9019\u500b\u767c\u73fe\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u8a13\u7df4\u7b56\u7565\uff0c\u7a31\u70ba\u6a21\u7d44\u9069\u61c9\u8a13\u7df4\uff08MAT\uff09\uff0c\u4ee5\u9078\u64c7\u6027\u5730\u66f4\u65b0\u90a3\u4e9b $\\lambda_{\\max}$ \u8d85\u904e\u52d5\u614b\u95be\u503c\u7684\u6a21\u7d44\uff0c\u5c07\u6a21\u578b\u96c6\u4e2d\u5728\u5b78\u7fd2\u5e38\u898b\u7279\u5fb5\u4e0a\uff0c\u4e26\u5ffd\u7565\u90a3\u4e9b\u4e0d\u4e00\u81f4\u7684\u7279\u5fb5\u3002\u8207\u5927\u591a\u6578\u73fe\u6709\u8a13\u7df4\u65b9\u6848\u5728\u6240\u6709\u7db2\u8def\u6a21\u7d44\u4e2d\u9032\u884c\u5b8c\u6574 BP \u5faa\u74b0\u4e0d\u540c\uff0cMAT \u53ef\u4ee5\u900f\u904e\u5176\u90e8\u5206\u66f4\u65b0\u7b56\u7565\u5927\u5e45\u7bc0\u7701\u904b\u7b97\uff0c\u4e26\u9032\u4e00\u6b65\u63d0\u5347\u6548\u80fd\u3002\u5be6\u9a57\u986f\u793a\uff0cMAT \u5e7e\u4e4e\u5c07\u6a21\u578b\u8a13\u7df4\u7684\u8a08\u7b97\u6210\u672c\u6e1b\u534a\uff0c\u4e26\u512a\u65bc\u57fa\u6e96\u7684\u6e96\u78ba\u5ea6\u3002", "author": "Yubin Shi et.al.", "authors": "Yubin Shi, Yixuan Chen, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Yujiang Wang, Robert P. Dick, Qin Lv, Yingying Zhao, Fan Yang, Tun Lu, Ning Gu, Li Shang", "id": "2405.07527v1", "paper_url": "http://arxiv.org/abs/2405.07527v1", "repo": "null"}}