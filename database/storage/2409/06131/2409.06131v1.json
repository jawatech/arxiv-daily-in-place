{"2409.06131": {"publish_time": "2024-09-10", "title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review", "paper_summary": "Large Language Model (LLM) pretraining traditionally relies on autoregressive\nlanguage modeling on randomly sampled data blocks from web-scale datasets. We\ntake inspiration from human learning techniques like spaced repetition to\nhypothesize that random data sampling for LLMs leads to high training cost and\nlow quality models which tend to forget data. In order to effectively commit\nweb-scale information to long-term memory, we propose the LFR (Learn, Focus,\nand Review) pedagogy, a new dynamic training paradigm which focuses and\nrepeatedly reviews complex data blocks at systematic intervals based on the\nmodel's learning pace and progress. LFR records the model perplexities for\ndifferent data blocks and frequently revisits blocks with higher perplexity\nwhich are more likely to be forgotten. We pretrain the GPT-2 models (124M -\n1.5B) from scratch on the OpenWebText dataset using LFR. We test on downstream\ntasks from the language modeling, question answering, translation, and problem\nsolving domains to achieve consistently lower perplexity and higher accuracy\nthan the baseline OpenAI models, while obtaining a 20x pretraining speed-up.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9810\u8a13\u7df4\u50b3\u7d71\u4e0a\u4f9d\u8cf4\u65bc\u96a8\u6a5f\u53d6\u6a23\u8cc7\u6599\u5340\u584a\u7684\u81ea\u52d5\u8ff4\u6b78\u8a9e\u8a00\u6a21\u578b\u5316\uff0c\u9019\u4e9b\u8cc7\u6599\u5340\u584a\u4f86\u81ea\u7db2\u8def\u898f\u6a21\u7684\u8cc7\u6599\u96c6\u3002\u6211\u5011\u5f9e\u4eba\u985e\u5b78\u7fd2\u6280\u5de7\uff08\u4f8b\u5982\u9593\u9694\u91cd\u8907\uff09\u4e2d\u7372\u5f97\u9748\u611f\uff0c\u5047\u8a2d LLM \u7684\u96a8\u6a5f\u8cc7\u6599\u53d6\u6a23\u6703\u5c0e\u81f4\u9ad8\u8a13\u7df4\u6210\u672c\u548c\u4f4e\u54c1\u8cea\u6a21\u578b\uff0c\u800c\u9019\u4e9b\u6a21\u578b\u5f80\u5f80\u6703\u907a\u5fd8\u8cc7\u6599\u3002\u70ba\u4e86\u6709\u6548\u5730\u5c07\u7db2\u8def\u898f\u6a21\u7684\u8cc7\u8a0a\u63d0\u4ea4\u81f3\u9577\u671f\u8a18\u61b6\u9ad4\uff0c\u6211\u5011\u63d0\u51fa LFR\uff08\u5b78\u7fd2\u3001\u5c08\u6ce8\u548c\u8907\u7fd2\uff09\u6559\u5b78\u6cd5\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u52d5\u614b\u8a13\u7df4\u7bc4\u4f8b\uff0c\u6703\u6839\u64da\u6a21\u578b\u7684\u5b78\u7fd2\u6b65\u8abf\u548c\u9032\u5ea6\uff0c\u5728\u7cfb\u7d71\u6027\u7684\u9593\u9694\u4e2d\u5c08\u6ce8\u65bc\u8907\u96dc\u7684\u8cc7\u6599\u5340\u584a\u4e26\u53cd\u8986\u8907\u7fd2\u3002LFR \u6703\u8a18\u9304\u6a21\u578b\u5c0d\u65bc\u4e0d\u540c\u8cc7\u6599\u5340\u584a\u7684\u56f0\u60d1\u5ea6\uff0c\u4e26\u983b\u7e41\u5730\u91cd\u65b0\u6aa2\u8996\u56f0\u60d1\u5ea6\u8f03\u9ad8\u7684\u5340\u584a\uff0c\u56e0\u70ba\u9019\u4e9b\u5340\u584a\u8f03\u5bb9\u6613\u88ab\u907a\u5fd8\u3002\u6211\u5011\u5f9e\u982d\u958b\u59cb\u4f7f\u7528 LFR \u5728 OpenWebText \u8cc7\u6599\u96c6\u4e0a\u9810\u8a13\u7df4 GPT-2 \u6a21\u578b\uff08124M - 1.5B\uff09\u3002\u6211\u5011\u5728\u8a9e\u8a00\u6a21\u578b\u5316\u3001\u554f\u984c\u89e3\u7b54\u3001\u7ffb\u8b6f\u548c\u554f\u984c\u89e3\u6c7a\u9818\u57df\u7684\u4e0b\u6e38\u4efb\u52d9\u4e0a\u9032\u884c\u6e2c\u8a66\uff0c\u8207\u57fa\u6e96 OpenAI \u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u59cb\u7d42\u7372\u5f97\u8f03\u4f4e\u7684\u56f0\u60d1\u5ea6\u548c\u8f03\u9ad8\u7684\u6e96\u78ba\u5ea6\uff0c\u540c\u6642\u7372\u5f97 20 \u500d\u7684\u9810\u8a13\u7df4\u52a0\u901f\u3002", "author": "Neha Prakriya et.al.", "authors": "Neha Prakriya, Jui-Nan Yen, Cho-Jui Hsieh, Jason Cong", "id": "2409.06131v1", "paper_url": "http://arxiv.org/abs/2409.06131v1", "repo": "null"}}