{"2408.16725": {"publish_time": "2024-08-29", "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming", "paper_summary": "Recent advances in language models have achieved significant progress.\nGPT-4o, as a new milestone, has enabled real-time conversations with humans,\ndemonstrating near-human natural fluency. Such human-computer interaction\nnecessitates models with the capability to perform reasoning directly with the\naudio modality and generate output in streaming. However, this remains beyond\nthe reach of current academic models, as they typically depend on extra TTS\nsystems for speech synthesis, resulting in undesirable latency. This paper\nintroduces the Mini-Omni, an audio-based end-to-end conversational model,\ncapable of real-time speech interaction. To achieve this capability, we propose\na text-instructed speech generation method, along with batch-parallel\nstrategies during inference to further boost the performance. Our method also\nhelps to retain the original model's language capabilities with minimal\ndegradation, enabling other works to establish real-time interaction\ncapabilities. We call this training method \"Any Model Can Talk\". We also\nintroduce the VoiceAssistant-400K dataset to fine-tune models optimized for\nspeech output. To our best knowledge, Mini-Omni is the first fully end-to-end,\nopen-source model for real-time speech interaction, offering valuable potential\nfor future research.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u7684\u6700\u65b0\u9032\u5c55\u53d6\u5f97\u91cd\u5927\u9032\u5c55\u3002\nGPT-4o \u4f5c\u70ba\u4e00\u500b\u65b0\u7684\u91cc\u7a0b\u7891\uff0c\u5be6\u73fe\u4e86\u8207\u4eba\u985e\u7684\u5373\u6642\u5c0d\u8a71\uff0c\n\u5c55\u793a\u4e86\u63a5\u8fd1\u4eba\u985e\u7684\u81ea\u7136\u6d41\u66a2\u5ea6\u3002\u9019\u7a2e\u4eba\u6a5f\u4e92\u52d5\n\u9700\u8981\u6a21\u578b\u5177\u5099\u76f4\u63a5\u4f7f\u7528\u97f3\u8a0a\u6a21\u5f0f\u9032\u884c\u63a8\u7406\u4e26\u751f\u6210\u4e32\u6d41\u8f38\u51fa\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4ecd\u7136\u8d85\u51fa\u4e86\u7576\u524d\u5b78\u8853\u6a21\u578b\u7684\u7bc4\u570d\uff0c\u56e0\u70ba\u5b83\u5011\u901a\u5e38\u4f9d\u8cf4\u65bc\u984d\u5916\u7684 TTS \u7cfb\u7d71\u9032\u884c\u8a9e\u97f3\u5408\u6210\uff0c\u5f9e\u800c\u5c0e\u81f4\u4e0d\u5e0c\u671b\u7684\u5ef6\u9072\u3002\u672c\u6587\u4ecb\u7d39\u4e86 Mini-Omni\uff0c\u4e00\u500b\u57fa\u65bc\u97f3\u8a0a\u7684\u7aef\u5230\u7aef\u5c0d\u8a71\u6a21\u578b\uff0c\u80fd\u5920\u9032\u884c\u5be6\u6642\u8a9e\u97f3\u4e92\u52d5\u3002\u70ba\u4e86\u5be6\u73fe\u9019\u4e00\u80fd\u529b\uff0c\u6211\u5011\u63d0\u51fa\n\u4e00\u7a2e\u6587\u5b57\u6307\u5c0e\u7684\u8a9e\u97f3\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u63a1\u7528\u6279\u6b21\u4e26\u884c\u7b56\u7565\u4ee5\u9032\u4e00\u6b65\u63d0\u5347\u6548\u80fd\u3002\u6211\u5011\u7684\u6280\u8853\u4e5f\u6709\u52a9\u65bc\u4fdd\u7559\u539f\u59cb\u6a21\u578b\u7684\u8a9e\u8a00\u80fd\u529b\uff0c\u540c\u6642\u5c07\u9000\u5316\u964d\u81f3\u6700\u4f4e\uff0c\u4f7f\u5176\u4ed6\u4f5c\u54c1\u80fd\u5920\u5efa\u7acb\u5be6\u6642\u4e92\u52d5\u80fd\u529b\u3002\u6211\u5011\u5c07\u9019\u7a2e\u8a13\u7df4\u65b9\u6cd5\u7a31\u70ba\u300c\u4efb\u4f55\u6a21\u578b\u90fd\u80fd\u8aaa\u8a71\u300d\u3002\u6211\u5011\u9084\u5f15\u5165\u4e86 VoiceAssistant-400K \u8cc7\u6599\u96c6\uff0c\u4ee5\u5fae\u8abf\u91dd\u5c0d\u8a9e\u97f3\u8f38\u51fa\u6700\u4f73\u5316\u7684\u6a21\u578b\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cMini-Omni \u662f\u7b2c\u4e00\u500b\u5b8c\u5168\u7aef\u5230\u7aef\u3001\u958b\u653e\u539f\u59cb\u78bc\u7684\u5be6\u6642\u8a9e\u97f3\u4e92\u52d5\u6a21\u578b\uff0c\u70ba\u672a\u4f86\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u6f5b\u529b\u3002", "author": "Zhifei Xie et.al.", "authors": "Zhifei Xie, Changqiao Wu", "id": "2408.16725v2", "paper_url": "http://arxiv.org/abs/2408.16725v2", "repo": "https://github.com/gpt-omni/mini-omni"}}