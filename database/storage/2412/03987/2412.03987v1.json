{"2412.03987": {"publish_time": "2024-12-05", "title": "MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM", "paper_summary": "Large language models (LLMs) have shown limitations in tasks requiring\ncomplex logical reasoning and multi-step problem-solving. To address these\nchallenges, researchers have employed carefully designed prompts and\nflowcharts, simulating human cognitive processes to enhance LLM performance,\nsuch as the Chain of Thought approach. In this paper, we introduce MTMT\n(Multi-thinking Modes Tree), a novel method that interacts with LLMs to\nconstruct a thought tree, simulating various advanced cognitive processes,\nincluding but not limited to association, counterfactual thinking, task\ndecomposition, and comparison. By breaking down the original complex task into\nsimpler sub-questions, MTMT facilitates easier problem-solving for LLMs,\nenabling more effective utilization of the latent knowledge within LLMs. We\nevaluate the performance of MTMT under different parameter configurations,\nusing GPT-4o mini as the base model. Our results demonstrate that integrating\nmultiple modes of thinking significantly enhances the ability of LLMs to handle\ncomplex tasks.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9700\u8981\u8907\u96dc\u908f\u8f2f\u63a8\u7406\u548c\u591a\u6b65\u9a5f\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u7684\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u9650\u5236\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u7814\u7a76\u4eba\u54e1\u63a1\u7528\u7cbe\u5fc3\u8a2d\u8a08\u7684\u63d0\u793a\u548c\u6d41\u7a0b\u5716\uff0c\u6a21\u64ec\u4eba\u985e\u8a8d\u77e5\u904e\u7a0b\u4ee5\u589e\u5f37 LLM \u7684\u6548\u80fd\uff0c\u4f8b\u5982\u601d\u8003\u93c8\u65b9\u6cd5\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 MTMT\uff08\u591a\u601d\u8003\u6a21\u5f0f\u6a39\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u8207 LLM \u4e92\u52d5\u4ee5\u5efa\u69cb\u601d\u8003\u6a39\u7684\u65b0\u65b9\u6cd5\uff0c\u6a21\u64ec\u5404\u7a2e\u5148\u9032\u7684\u8a8d\u77e5\u904e\u7a0b\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u65bc\u806f\u60f3\u3001\u53cd\u4e8b\u5be6\u601d\u8003\u3001\u4efb\u52d9\u5206\u89e3\u548c\u6bd4\u8f03\u3002\u901a\u904e\u5c07\u539f\u672c\u8907\u96dc\u7684\u4efb\u52d9\u5206\u89e3\u6210\u66f4\u7c21\u55ae\u7684\u5b50\u554f\u984c\uff0cMTMT \u4fc3\u9032 LLM \u66f4\u5bb9\u6613\u89e3\u6c7a\u554f\u984c\uff0c\u8b93 LLM \u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u5176\u6f5b\u5728\u77e5\u8b58\u3002\u6211\u5011\u4f7f\u7528 GPT-4o mini \u4f5c\u70ba\u57fa\u790e\u6a21\u578b\uff0c\u8a55\u4f30 MTMT \u5728\u4e0d\u540c\u53c3\u6578\u914d\u7f6e\u4e0b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u6574\u5408\u591a\u7a2e\u601d\u8003\u6a21\u5f0f\u986f\u8457\u589e\u5f37\u4e86 LLM \u8655\u7406\u8907\u96dc\u4efb\u52d9\u7684\u80fd\u529b\u3002", "author": "Changcheng Li et.al.", "authors": "Changcheng Li, Xiangyu Wang, Qiuju Chen, Xiren Zhou, Huanhuan Chen", "id": "2412.03987v1", "paper_url": "http://arxiv.org/abs/2412.03987v1", "repo": "null"}}