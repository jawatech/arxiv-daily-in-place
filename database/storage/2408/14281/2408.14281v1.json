{"2408.14281": {"publish_time": "2024-08-26", "title": "Uncertainties of Latent Representations in Computer Vision", "paper_summary": "Uncertainty quantification is a key pillar of trustworthy machine learning.\nIt enables safe reactions under unsafe inputs, like predicting only when the\nmachine learning model detects sufficient evidence, discarding anomalous data,\nor emitting warnings when an error is likely to be inbound. This is\nparticularly crucial in safety-critical areas like medical image classification\nor self-driving cars. Despite the plethora of proposed uncertainty\nquantification methods achieving increasingly higher scores on performance\nbenchmarks, uncertainty estimates are often shied away from in practice. Many\nmachine learning projects start from pretrained latent representations that\ncome without uncertainty estimates. Uncertainties would need to be trained by\npractitioners on their own, which is notoriously difficult and\nresource-intense.\n  This thesis makes uncertainty estimates easily accessible by adding them to\nthe latent representation vectors of pretrained computer vision models. Besides\nproposing approaches rooted in probability and decision theory, such as\nMonte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both\ntheoretical and empirical questions. We show that these unobservable\nuncertainties about unobservable latent representations are indeed provably\ncorrect. We also provide an uncertainty-aware representation learning (URL)\nbenchmark to compare these unobservables against observable ground-truths.\nFinally, we compile our findings to pretrain lightweight representation\nuncertainties on large-scale computer vision models that transfer to unseen\ndatasets in a zero-shot manner.\n  Our findings do not only advance the current theoretical understanding of\nuncertainties over latent variables, but also facilitate the access to\nuncertainty quantification for future researchers inside and outside the field,\nenabling straightforward but trustworthy machine learning.", "paper_summary_zh": "\u4e0d\u78ba\u5b9a\u91cf\u5316\u662f\u503c\u5f97\u4fe1\u8cf4\u6a5f\u5668\u5b78\u7fd2\u7684\u4e00\u5927\u652f\u67f1\u3002\n\u5b83\u80fd\u8b93\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u5728\u4e0d\u5b89\u5168\u7684\u8f38\u5165\u4e0b\u505a\u51fa\u5b89\u5168\u7684\u53cd\u61c9\uff0c\u4f8b\u5982\u53ea\u5728\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u5075\u6e2c\u5230\u8db3\u5920\u8b49\u64da\u6642\u624d\u9032\u884c\u9810\u6e2c\u3001\u6368\u68c4\u7570\u5e38\u8cc7\u6599\uff0c\u6216\u662f\u5728\u53ef\u80fd\u767c\u751f\u932f\u8aa4\u6642\u767c\u51fa\u8b66\u544a\u3002\u9019\u5728\u91ab\u7642\u5f71\u50cf\u5206\u985e\u6216\u81ea\u99d5\u8eca\u7b49\u5b89\u5168\u95dc\u9375\u9818\u57df\u4e2d\u7279\u5225\u91cd\u8981\u3002\u5118\u7ba1\u6709\u8a31\u591a\u5df2\u63d0\u51fa\u7684\u4e0d\u78ba\u5b9a\u91cf\u5316\u65b9\u6cd5\u5728\u6548\u80fd\u57fa\u6e96\u4e0a\u53d6\u5f97\u8d8a\u4f86\u8d8a\u9ad8\u7684\u5206\u6578\uff0c\u4f46\u5728\u5be6\u52d9\u4e0a\u537b\u5e38\u5e38\u8ff4\u907f\u4e0d\u78ba\u5b9a\u6027\u4f30\u8a08\u3002\u8a31\u591a\u6a5f\u5668\u5b78\u7fd2\u5c08\u6848\u5f9e\u9810\u8a13\u7df4\u7684\u6f5b\u5728\u8868\u5fb5\u958b\u59cb\uff0c\u800c\u9019\u4e9b\u8868\u5fb5\u6c92\u6709\u4e0d\u78ba\u5b9a\u6027\u4f30\u8a08\u3002\u5be6\u52d9\u5de5\u4f5c\u8005\u9700\u8981\u81ea\u884c\u8a13\u7df4\u4e0d\u78ba\u5b9a\u6027\uff0c\u9019\u51fa\u4e86\u540d\u7684\u56f0\u96e3\u4e14\u8017\u8cbb\u8cc7\u6e90\u3002\n\u672c\u8ad6\u6587\u900f\u904e\u5c07\u4e0d\u78ba\u5b9a\u6027\u4f30\u8a08\u65b0\u589e\u5230\u9810\u8a13\u7df4\u96fb\u8166\u8996\u89ba\u6a21\u578b\u7684\u6f5b\u5728\u8868\u5fb5\u5411\u91cf\u4e2d\uff0c\u8b93\u4e0d\u78ba\u5b9a\u6027\u4f30\u8a08\u6613\u65bc\u53d6\u5f97\u3002\u9664\u4e86\u63d0\u51fa\u690d\u57fa\u65bc\u6a5f\u7387\u548c\u6c7a\u7b56\u7406\u8ad6\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u8499\u5730\u5361\u7f85\u8cc7\u8a0a\u5c0d\u6bd4\u4f30\u8a08 (MCInfoNCE) \u548c\u640d\u5931\u9810\u6e2c\u4e4b\u5916\uff0c\u6211\u5011\u9084\u6df1\u5165\u63a2\u8a0e\u7406\u8ad6\u548c\u5be6\u8b49\u554f\u984c\u3002\u6211\u5011\u8b49\u660e\u9019\u4e9b\u95dc\u65bc\u4e0d\u53ef\u89c0\u5bdf\u6f5b\u5728\u8868\u5fb5\u7684\u4e0d\u53ef\u89c0\u5bdf\u4e0d\u78ba\u5b9a\u6027\u78ba\u5be6\u53ef\u4ee5\u8b49\u660e\u662f\u6b63\u78ba\u7684\u3002\u6211\u5011\u9084\u63d0\u4f9b\u4e00\u500b\u4e0d\u78ba\u5b9a\u6027\u611f\u77e5\u8868\u5fb5\u5b78\u7fd2 (URL) \u57fa\u6e96\uff0c\u7528\u4f86\u6bd4\u8f03\u9019\u4e9b\u4e0d\u53ef\u89c0\u5bdf\u7684\u4e0d\u78ba\u5b9a\u6027\u8207\u53ef\u89c0\u5bdf\u7684\u771f\u5be6\u503c\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5c07\u6211\u5011\u7684\u767c\u73fe\u5f59\u6574\u8d77\u4f86\uff0c\u5728\u5927\u578b\u96fb\u8166\u8996\u89ba\u6a21\u578b\u4e0a\u9810\u8a13\u7df4\u8f15\u91cf\u7d1a\u8868\u5fb5\u4e0d\u78ba\u5b9a\u6027\uff0c\u4e26\u4ee5\u96f6\u6b21\u5b78\u7fd2\u7684\u65b9\u5f0f\u8f49\u79fb\u5230\u672a\u898b\u904e\u7684\u8cc7\u6599\u96c6\u3002\n\u6211\u5011\u7684\u767c\u73fe\u4e0d\u50c5\u63d0\u5347\u4e86\u7576\u524d\u5c0d\u6f5b\u5728\u8b8a\u6578\u4e0d\u78ba\u5b9a\u6027\u7684\u7406\u8ad6\u7406\u89e3\uff0c\u9084\u4fc3\u9032\u4e86\u672a\u4f86\u7814\u7a76\u4eba\u54e1\u5728\u8a72\u9818\u57df\u5167\u5916\u53d6\u5f97\u4e0d\u78ba\u5b9a\u91cf\u5316\uff0c\u9032\u800c\u5be6\u73fe\u76f4\u63a5\u4f46\u503c\u5f97\u4fe1\u8cf4\u7684\u6a5f\u5668\u5b78\u7fd2\u3002", "author": "Michael Kirchhof et.al.", "authors": "Michael Kirchhof", "id": "2408.14281v1", "paper_url": "http://arxiv.org/abs/2408.14281v1", "repo": "null"}}