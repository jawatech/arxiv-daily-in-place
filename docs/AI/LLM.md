
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-17**|**mDPO: Conditional Preference Optimization for Multimodal Large Language Models**|Fei Wang et.al.|[2406.11839v1](http://arxiv.org/abs/2406.11839v1)|null|
|**2024-06-17**|**MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs**|Ziyu Liu et.al.|[2406.11833v1](http://arxiv.org/abs/2406.11833v1)|[link](https://github.com/liuziyu77/mmdu)|
|**2024-06-17**|**Language Modeling with Editable External Knowledge**|Belinda Z. Li et.al.|[2406.11830v1](http://arxiv.org/abs/2406.11830v1)|[link](https://github.com/belindal/erase)|
|**2024-06-17**|**WPO: Enhancing RLHF with Weighted Preference Optimization**|Wenxuan Zhou et.al.|[2406.11827v1](http://arxiv.org/abs/2406.11827v1)|[link](https://github.com/wzhouad/wpo)|
|**2024-06-17**|**On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning**|Geewook Kim et.al.|[2406.11823v1](http://arxiv.org/abs/2406.11823v1)|[link](https://github.com/naver-ai/elva)|
|**2024-06-17**|**Embodied Instruction Following in Unknown Environments**|Zhenyu Wu et.al.|[2406.11818v1](http://arxiv.org/abs/2406.11818v1)|null|
|**2024-06-17**|**Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level**|Jie Liu et.al.|[2406.11817v1](http://arxiv.org/abs/2406.11817v1)|null|
|**2024-06-17**|**How Do Large Language Models Acquire Factual Knowledge During Pretraining?**|Hoyeon Chang et.al.|[2406.11813v1](http://arxiv.org/abs/2406.11813v1)|null|
|**2024-06-17**|**RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content**|Joao Monteiro et.al.|[2406.11811v1](http://arxiv.org/abs/2406.11811v1)|null|
|**2024-06-17**|**Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations**|Rima Hazra et.al.|[2406.11801v1](http://arxiv.org/abs/2406.11801v1)|[link](https://github.com/declare-lab/safety-arithmetic)|
|**2024-06-17**|**DataComp-LM: In search of the next generation of training sets for language models**|Jeffrey Li et.al.|[2406.11794v1](http://arxiv.org/abs/2406.11794v1)|null|
|**2024-06-17**|**A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping**|Abhi Kamboj et.al.|[2406.11786v1](http://arxiv.org/abs/2406.11786v1)|null|
|**2024-06-17**|**CELL your Model: Contrastive Explanation Methods for Large Language Models**|Ronny Luss et.al.|[2406.11785v1](http://arxiv.org/abs/2406.11785v1)|null|
|**2024-06-17**|**MDCR: A Dataset for Multi-Document Conditional Reasoning**|Peter Baile Chen et.al.|[2406.11784v1](http://arxiv.org/abs/2406.11784v1)|null|
|**2024-06-17**|**Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs**|Swanand Ravindra Kadhe et.al.|[2406.11780v1](http://arxiv.org/abs/2406.11780v1)|null|
|**2024-06-17**|**Improving Multi-Agent Debate with Sparse Communication Topology**|Yunxuan Li et.al.|[2406.11776v1](http://arxiv.org/abs/2406.11776v1)|null|
|**2024-06-17**|**Task Me Anything**|Jieyu Zhang et.al.|[2406.11775v1](http://arxiv.org/abs/2406.11775v1)|[link](https://github.com/jieyuz2/taskmeanything)|
|**2024-06-17**|**GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities**|Sreyan Ghosh et.al.|[2406.11768v1](http://arxiv.org/abs/2406.11768v1)|null|
|**2024-06-17**|**STAR: SocioTechnical Approach to Red Teaming Language Models**|Laura Weidinger et.al.|[2406.11757v1](http://arxiv.org/abs/2406.11757v1)|null|
|**2024-06-17**|**DustNet: skillful neural network predictions of Saharan dust**|Trish E. Nowak et.al.|[2406.11754v1](http://arxiv.org/abs/2406.11754v1)|null|
|**2024-06-17**|**A Semantic-based Layer Freezing Approach to Efficient Fine-Tuning of Language Models**|Jian Gu et.al.|[2406.11753v1](http://arxiv.org/abs/2406.11753v1)|null|
|**2024-06-17**|**Multi-Layer Ranking with Large Language Models for News Source Recommendation**|Wenjia Zhang et.al.|[2406.11745v1](http://arxiv.org/abs/2406.11745v1)|null|
|**2024-06-17**|**Transcendence: Generative Models Can Outperform The Experts That Train Them**|Edwin Zhang et.al.|[2406.11741v1](http://arxiv.org/abs/2406.11741v1)|null|
|**2024-06-17**|**Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies**|Haojie Huang et.al.|[2406.11740v1](http://arxiv.org/abs/2406.11740v1)|null|
|**2024-06-17**|**Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models**|Fangzhi Xu et.al.|[2406.11736v1](http://arxiv.org/abs/2406.11736v1)|null|
|**2024-06-17**|**1000 African Voices: Advancing inclusive multi-speaker multi-accent speech synthesis**|Sewade Ogun et.al.|[2406.11727v1](http://arxiv.org/abs/2406.11727v1)|null|
|**2024-06-17**|**Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity**|Bingxiang He et.al.|[2406.11721v1](http://arxiv.org/abs/2406.11721v1)|[link](https://github.com/hbx-hbx/dynamics_of_zero-shot_generalization)|
|**2024-06-17**|**Refusal in Language Models Is Mediated by a Single Direction**|Andy Arditi et.al.|[2406.11717v1](http://arxiv.org/abs/2406.11717v1)|null|
|**2024-06-17**|**Measuring memorization in RLHF for code completion**|Aneesh Pappu et.al.|[2406.11715v1](http://arxiv.org/abs/2406.11715v1)|null|
|**2024-06-17**|**Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging**|Priyanka Kargupta et.al.|[2406.11709v1](http://arxiv.org/abs/2406.11709v1)|null|
|**2024-06-17**|**Prompts as Auto-Optimized Training Hyperparameters: Training Best-in-Class IR Models from Scratch with 10 Gold Labels**|Jasper Xian et.al.|[2406.11706v1](http://arxiv.org/abs/2406.11706v1)|null|
|**2024-06-17**|**Nemotron-4 340B Technical Report**|Nvidia et.al.|[2406.11704v1](http://arxiv.org/abs/2406.11704v1)|null|
|**2024-06-17**|**Meta Reasoning for Large Language Models**|Peizhong Gao et.al.|[2406.11698v1](http://arxiv.org/abs/2406.11698v1)|null|
|**2024-06-17**|**Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs**|Krista Opsahl-Ong et.al.|[2406.11695v1](http://arxiv.org/abs/2406.11695v1)|[link](https://github.com/stanfordnlp/dspy)|
|**2024-06-17**|**Tokenization Falling Short: The Curse of Tokenization**|Yekun Chai et.al.|[2406.11687v1](http://arxiv.org/abs/2406.11687v1)|null|
|**2024-06-17**|**HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing**|Jing Chen et.al.|[2406.11683v1](http://arxiv.org/abs/2406.11683v1)|null|
|**2024-06-17**|**Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack**|Shangqing Tu et.al.|[2406.11682v1](http://arxiv.org/abs/2406.11682v1)|[link](https://github.com/thu-keg/knowledge-to-jailbreak)|
|**2024-06-17**|**R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models**|Shangqing Tu et.al.|[2406.11681v1](http://arxiv.org/abs/2406.11681v1)|[link](https://github.com/thu-keg/r-eval)|
|**2024-06-17**|**TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy**|Yiqun Chen et.al.|[2406.11678v1](http://arxiv.org/abs/2406.11678v1)|null|
|**2024-06-17**|**BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models**|Yibin Wang et.al.|[2406.11675v1](http://arxiv.org/abs/2406.11675v1)|null|
|**2024-06-17**|**Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference**|Donghyeon Joo et.al.|[2406.11674v1](http://arxiv.org/abs/2406.11674v1)|null|
|**2024-06-17**|**Benchmarking of LLM Detection: Comparing Two Competing Approaches**|Thorsten Pröhl et.al.|[2406.11670v1](http://arxiv.org/abs/2406.11670v1)|null|
|**2024-06-17**|**"Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak**|Lingrui Mei et.al.|[2406.11668v1](http://arxiv.org/abs/2406.11668v1)|null|
|**2024-06-17**|**See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding**|Amith Ananthram et.al.|[2406.11665v1](http://arxiv.org/abs/2406.11665v1)|[link](https://github.com/amith-ananthram/see-it-from-my-perspective)|
|**2024-06-17**|**Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting**|Sagnik Mukherjee et.al.|[2406.11661v1](http://arxiv.org/abs/2406.11661v1)|null|
|**2024-06-17**|**Can LLM be a Personalized Judge?**|Yijiang River Dong et.al.|[2406.11657v1](http://arxiv.org/abs/2406.11657v1)|[link](https://github.com/dong-river/personalized-judge)|
|**2024-06-17**|**A Two-dimensional Zero-shot Dialogue State Tracking Evaluation Method using GPT-4**|Ming Gu et.al.|[2406.11651v1](http://arxiv.org/abs/2406.11651v1)|null|
|**2024-06-17**|**YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection**|Tamara R. Lenhard et.al.|[2406.11641v1](http://arxiv.org/abs/2406.11641v1)|null|
|**2024-06-17**|**Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning with Few Actions**|Noah Golowich et.al.|[2406.11640v1](http://arxiv.org/abs/2406.11640v1)|null|
|**2024-06-17**|**MASAI: Modular Architecture for Software-engineering AI Agents**|Daman Arora et.al.|[2406.11638v1](http://arxiv.org/abs/2406.11638v1)|null|
|**2024-06-17**|**The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance**|Kyle Moore et.al.|[2406.11634v1](http://arxiv.org/abs/2406.11634v1)|null|
|**2024-06-17**|**Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation**|Boxuan Lyu et.al.|[2406.11632v1](http://arxiv.org/abs/2406.11632v1)|null|
|**2024-06-17**|**Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!**|Mingyang Song et.al.|[2406.11629v1](http://arxiv.org/abs/2406.11629v1)|null|
|**2024-06-17**|**Words in Motion: Representation Engineering for Motion Forecasting**|Omer Sahin Tas et.al.|[2406.11624v1](http://arxiv.org/abs/2406.11624v1)|[link](https://github.com/kit-mrt/future-motion)|
|**2024-06-17**|**Building Knowledge-Guided Lexica to Model Cultural Variation**|Shreya Havaldar et.al.|[2406.11622v1](http://arxiv.org/abs/2406.11622v1)|null|
|**2024-06-17**|**DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling**|Pala Tej Deep et.al.|[2406.11617v1](http://arxiv.org/abs/2406.11617v1)|[link](https://github.com/declare-lab/della)|
|**2024-06-17**|**Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces**|Yihuai Hong et.al.|[2406.11614v1](http://arxiv.org/abs/2406.11614v1)|[link](https://github.com/yihuaihong/conceptvectors)|
|**2024-06-17**|**Long Code Arena: a Set of Benchmarks for Long-Context Code Models**|Egor Bogomolov et.al.|[2406.11612v1](http://arxiv.org/abs/2406.11612v1)|null|
|**2024-06-17**|**Understanding "Democratization" in NLP and ML Research**|Arjun Subramonian et.al.|[2406.11598v1](http://arxiv.org/abs/2406.11598v1)|null|
|**2024-06-17**|**CoSQA+: Enhancing Code Search Dataset with Matching Code**|Jing Gong et.al.|[2406.11589v1](http://arxiv.org/abs/2406.11589v1)|[link](https://github.com/DeepSoftwareAnalytics/CoSQA_Plus)|
|**2024-06-17**|**Style Transfer with Multi-iteration Preference Optimization**|Shuai Liu et.al.|[2406.11581v1](http://arxiv.org/abs/2406.11581v1)|null|
|**2024-06-17**|**Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation**|Tom Kocmi et.al.|[2406.11580v1](http://arxiv.org/abs/2406.11580v1)|null|
|**2024-06-17**|**Mathematical Entities: Corpora and Benchmarks**|Jacob Collard et.al.|[2406.11577v1](http://arxiv.org/abs/2406.11577v1)|null|
|**2024-06-17**|**Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models**|Sheng Feng et.al.|[2406.11568v1](http://arxiv.org/abs/2406.11568v1)|[link](https://github.com/fsfrancis15/brainllm)|
|**2024-06-17**|**Quaternion Generative Adversarial Neural Networks and Applications to Color Image Inpainting**|Duan Wang et.al.|[2406.11567v1](http://arxiv.org/abs/2406.11567v1)|null|
|**2024-06-17**|**MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked Low-Rank Adaptation**|Jiakuan Xie et.al.|[2406.11566v1](http://arxiv.org/abs/2406.11566v1)|null|
|**2024-06-17**|**Extrinsic Evaluation of Cultural Competence in Large Language Models**|Shaily Bhatt et.al.|[2406.11565v1](http://arxiv.org/abs/2406.11565v1)|null|
|**2024-06-17**|**Input Conditioned Graph Generation for Language Agents**|Lukas Vierling et.al.|[2406.11555v1](http://arxiv.org/abs/2406.11555v1)|[link](https://github.com/lukasvierling/dynamicgptswarm)|
|**2024-06-17**|**AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation**|Chuyan Xiong et.al.|[2406.11548v1](http://arxiv.org/abs/2406.11548v1)|null|
|**2024-06-17**|**GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations**|Rick Wilming et.al.|[2406.11547v1](http://arxiv.org/abs/2406.11547v1)|[link](https://github.com/braindatalab/gecobench)|
|**2024-06-17**|**GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement**|Yifan Yang et.al.|[2406.11546v1](http://arxiv.org/abs/2406.11546v1)|null|
|**2024-06-17**|**Do Parameters Reveal More than Loss for Membership Inference?**|Anshuman Suri et.al.|[2406.11544v1](http://arxiv.org/abs/2406.11544v1)|[link](https://github.com/iamgroot42/iha_hild)|
|**2024-06-17**|**Improving Quality Control of Whole Slide Images by Explicit Artifact Augmentation**|Artur Jurgas et.al.|[2406.11538v1](http://arxiv.org/abs/2406.11538v1)|null|
|**2024-06-17**|**Explainable Artificial Intelligence and Multicollinearity : A Mini Review of Current Approaches**|Ahmed M Salih et.al.|[2406.11524v1](http://arxiv.org/abs/2406.11524v1)|null|
|**2024-06-17**|**FullCert: Deterministic End-to-End Certification for Training and Inference of Neural Networks**|Tobias Lorenz et.al.|[2406.11522v1](http://arxiv.org/abs/2406.11522v1)|null|
|**2024-06-17**|**Revisiting Spurious Correlation in Domain Generalization**|Bin Qin et.al.|[2406.11517v1](http://arxiv.org/abs/2406.11517v1)|null|
|**2024-06-17**|**Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs**|Yi Fang et.al.|[2406.11514v1](http://arxiv.org/abs/2406.11514v1)|null|
|**2024-06-17**|**On the Feasibility of Fidelity$^-$ for Graph Pruning**|Yong-Min Shin et.al.|[2406.11504v1](http://arxiv.org/abs/2406.11504v1)|null|
|**2024-06-17**|**GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation**|Shihao Cai et.al.|[2406.11503v1](http://arxiv.org/abs/2406.11503v1)|null|
|**2024-06-17**|**Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality**|Jiangmeng Li et.al.|[2406.11501v1](http://arxiv.org/abs/2406.11501v1)|null|
|**2024-06-17**|**CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG**|Boyi Deng et.al.|[2406.11497v1](http://arxiv.org/abs/2406.11497v1)|null|
|**2024-06-17**|**Analysing zero-shot temporal relation extraction on clinical notes using temporal consistency**|Vasiliki Kougia et.al.|[2406.11486v1](http://arxiv.org/abs/2406.11486v1)|null|
|**2024-06-17**|**Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms**|Vaneet Aggarwal et.al.|[2406.11481v1](http://arxiv.org/abs/2406.11481v1)|null|
|**2024-06-17**|**Vocabulary Expansion for Low-resource Cross-lingual Transfer**|Atsuki Yamaguchi et.al.|[2406.11477v1](http://arxiv.org/abs/2406.11477v1)|null|
|**2024-06-17**|**How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment**|Heyan Huang et.al.|[2406.11474v1](http://arxiv.org/abs/2406.11474v1)|null|
|**2024-06-17**|**Promises, Outlooks and Challenges of Diffusion Language Modeling**|Justin Deschenaux et.al.|[2406.11473v1](http://arxiv.org/abs/2406.11473v1)|null|
|**2024-06-17**|**Automating Easy Read Text Segmentation**|Jesús Calleja et.al.|[2406.11464v1](http://arxiv.org/abs/2406.11464v1)|null|
|**2024-06-17**|**TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation**|Jinyuan Fang et.al.|[2406.11460v1](http://arxiv.org/abs/2406.11460v1)|[link](https://github.com/jyfang6/trace)|
|**2024-06-17**|**Adaptive Reinforcement Learning Planning: Harnessing Large Language Models for Complex Information Extraction**|Zepeng Ding et.al.|[2406.11455v1](http://arxiv.org/abs/2406.11455v1)|null|
|**2024-06-17**|**GPT-Powered Elicitation Interview Script Generator for Requirements Engineering Training**|Binnur Görer et.al.|[2406.11439v1](http://arxiv.org/abs/2406.11439v1)|null|
|**2024-06-17**|**Analysing the Behaviour of Tree-Based Neural Networks in Regression Tasks**|Peter Samoaa et.al.|[2406.11437v1](http://arxiv.org/abs/2406.11437v1)|[link](https://github.com/petersamoaa/tree_based_nn_error_analysis)|
|**2024-06-17**|**AnyTrans: Translate AnyText in the Image with Large Scale Models**|Zhipeng Qian et.al.|[2406.11432v1](http://arxiv.org/abs/2406.11432v1)|null|
|**2024-06-17**|**Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization**|Wenkai Yang et.al.|[2406.11431v1](http://arxiv.org/abs/2406.11431v1)|[link](https://github.com/keven980716/weak-to-strong-deception)|
|**2024-06-17**|**A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression**|Alessio Devoto et.al.|[2406.11430v1](http://arxiv.org/abs/2406.11430v1)|null|
|**2024-06-17**|**DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer**|Keon Lee et.al.|[2406.11427v1](http://arxiv.org/abs/2406.11427v1)|null|
|**2024-06-17**|**Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability**|Gautam B et.al.|[2406.11424v1](http://arxiv.org/abs/2406.11424v1)|[link](https://github.com/amaze18/RAGbot-OpenSource-Comparison/blob/main/Hybrid_Topk_similarity_expt.ipynb)|
|**2024-06-17**|**Dredge Word, Social Media, and Webgraph Networks for Unreliable Website Classification and Identification**|Evan M. Williams et.al.|[2406.11423v1](http://arxiv.org/abs/2406.11423v1)|null|
|**2024-06-17**|**BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pretraining of BabyLM**|Zhewen Shen et.al.|[2406.11418v1](http://arxiv.org/abs/2406.11418v1)|null|
|**2024-06-17**|**Formally Certified Approximate Model Counting**|Yong Kiam Tan et.al.|[2406.11414v1](http://arxiv.org/abs/2406.11414v1)|null|
|**2024-06-17**|**HARE: HumAn pRiors, a key to small language model Efficiency**|Lingyun Zhang et.al.|[2406.11410v1](http://arxiv.org/abs/2406.11410v1)|[link](https://github.com/liteai-team/hare)|

#### Abstracts
##### **mDPO: Conditional Preference Optimization for Multimodal Large Language Models**
2406.11839v1 by Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen

Direct preference optimization (DPO) has shown to be an effective method for
large language model (LLM) alignment. Recent works have attempted to apply DPO
to multimodal scenarios but have found it challenging to achieve consistent
improvement. Through a comparative experiment, we identify the unconditional
preference problem in multimodal preference optimization, where the model
overlooks the image condition. To address this problem, we propose mDPO, a
multimodal DPO objective that prevents the over-prioritization of language-only
preferences by also optimizing image preference. Moreover, we introduce a
reward anchor that forces the reward to be positive for chosen responses,
thereby avoiding the decrease in their likelihood -- an intrinsic problem of
relative preference optimization. Experiments on two multimodal LLMs of
different sizes and three widely used benchmarks demonstrate that mDPO
effectively addresses the unconditional preference problem in multimodal
preference optimization and significantly improves model performance,
particularly in reducing hallucination.

摘要：直接偏好最佳化 (DPO) 已被證明是大型語言模型 (LLM) 對齊的有效方法。最近的研究已嘗試將 DPO 應用於多模態場景，但發現難以達成一致的改善。透過比較實驗，我們找出多模態偏好最佳化中的無條件偏好問題，其中模型忽略了影像條件。為了解決此問題，我們提出 mDPO，一種多模態 DPO 目標，透過最佳化影像偏好來防止過度優先考慮僅語言的偏好。此外，我們引入了獎勵錨點，強制獎勵對所選回應為正值，從而避免其可能性降低——這是相對偏好最佳化的內在問題。在不同大小的兩個多模態 LLM 和三個廣泛使用的基準上的實驗證明，mDPO 有效地解決了多模態偏好最佳化中的無條件偏好問題，並顯著改善了模型效能，特別是在減少幻覺方面。

##### **MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs**
2406.11833v1 by Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, Jiaqi Wang

Generating natural and meaningful responses to communicate with multi-modal
human inputs is a fundamental capability of Large Vision-Language
Models(LVLMs). While current open-source LVLMs demonstrate promising
performance in simplified scenarios such as single-turn single-image input,
they fall short in real-world conversation scenarios such as following
instructions in a long context history with multi-turn and multi-images.
Existing LVLM benchmarks primarily focus on single-choice questions or
short-form responses, which do not adequately assess the capabilities of LVLMs
in real-world human-AI interaction applications. Therefore, we introduce MMDU,
a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning
dataset, designed to evaluate and improve LVLMs' abilities in multi-turn and
multi-image conversations. We employ the clustering algorithm to ffnd the
relevant images and textual descriptions from the open-source Wikipedia and
construct the question-answer pairs by human annotators with the assistance of
the GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and
27 turns, which is at least 5x longer than previous benchmarks and poses
challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs
using MMDU reveals that open-source LVLMs lag behind closed-source counterparts
due to limited conversational instruction tuning data. We demonstrate that
ffne-tuning open-source LVLMs on MMDU-45k signiffcantly address this gap,
generating longer and more accurate conversations, and improving scores on MMDU
and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:+1.2%). Our
contributions pave the way for bridging the gap between current LVLM models and
real-world application demands. This project is available at
https://github.com/Liuziyu77/MMDU.

摘要：生成自然且有意義的回應，與多模態人類輸入進行溝通，是大型視覺語言模型 (LVLMs) 的基本能力。雖然目前的開源 LVLMs 在簡化的場景中展現出有希望的表現，例如單次輪換單一影像輸入，但它們在現實世界的對話場景中卻表現不佳，例如在具有多輪換和多影像的長語境歷史中遵循指示。現有的 LVLM 基準主要關注單選題或短篇回應，這並不能充分評估 LVLMs 在現實世界的人工智慧互動應用中的能力。因此，我們引入了 MMDU，一個全面的基準測試，以及 MMDU-45k，一個大規模的指令調整資料集，旨在評估和提升 LVLMs 在多輪換和多影像對話中的能力。我們採用群集演算法從開源維基百科中找出相關的影像和文字描述，並在 GPT-4o 模型的協助下，由人工註解者建構問答配對。MMDU 最多有 18k 個影像 + 文字符號、20 個影像和 27 個輪換，這至少比先前的基準測試長 5 倍，並對目前的 LVLMs 構成挑戰。我們使用 MMDU 對 15 個具代表性的 LVLMs 進行深入分析，發現開源 LVLMs 由於對話式指令調整資料有限，而落後於閉源對應程式。我們證明了在 MMDU-45k 上微調開源 LVLMs 可以顯著解決此差距，產生更長且更準確的對話，並提升 MMDU 和現有基準測試的分數 (MMStar：+1.1%，MathVista：+1.5%，ChartQA：+1.2%)。我們的貢獻為縮小目前 LVLM 模型與現實世界應用需求之間的差距鋪路。這個專案可在 https://github.com/Liuziyu77/MMDU 取得。

##### **Language Modeling with Editable External Knowledge**
2406.11830v1 by Belinda Z. Li, Emmy Liu, Alexis Ross, Abbas Zeitoun, Graham Neubig, Jacob Andreas

When the world changes, so does the text that humans write about it. How do
we build language models that can be easily updated to reflect these changes?
One popular approach is retrieval-augmented generation, in which new documents
are inserted into a knowledge base and retrieved during prediction for
downstream tasks. Most prior work on these systems have focused on improving
behavior during prediction through better retrieval or reasoning. This paper
introduces ERASE, which instead improves model behavior when new documents are
acquired, by incrementally deleting or rewriting other entries in the knowledge
base each time a document is added. In two new benchmark datasets evaluating
models' ability to answer questions about a stream of news articles or
conversations, ERASE improves accuracy relative to conventional
retrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B)
absolute. Code and data are available at https://github.com/belindal/ERASE

摘要：當世界改變，人類對它的文字書寫也跟著改變。我們如何建構可以輕易更新以反映這些改變的語言模型？一種廣受歡迎的方法是檢索增強生成，其中新的文件會插入到知識庫中，並在預測中檢索以進行下游任務。這些系統的大部分先前工作都專注於透過更好的檢索或推理來改善預測期間的行為。本文介紹 ERASE，它在取得新文件時改善模型行為，透過每次新增文件時遞增刪除或改寫知識庫中的其他條目。在兩個新的基準資料集評估模型回答新聞文章或對話串流問題的能力，ERASE 相較於傳統的檢索增強生成，準確性提升 7-13%（Mixtral-8x7B）和 6-10%（Llama-3-8B）絕對值。程式碼和資料可於 https://github.com/belindal/ERASE 取得

##### **WPO: Enhancing RLHF with Weighted Preference Optimization**
2406.11827v1 by Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, Chenguang Zhu

Reinforcement learning from human feedback (RLHF) is a promising solution to
align large language models (LLMs) more closely with human values. Off-policy
preference optimization, where the preference data is obtained from other
models, is widely adopted due to its cost efficiency and scalability. However,
off-policy preference optimization often suffers from a distributional gap
between the policy used for data collection and the target policy, leading to
suboptimal optimization. In this paper, we propose a novel strategy to mitigate
this problem by simulating on-policy learning with off-policy preference data.
Our Weighted Preference Optimization (WPO) method adapts off-policy data to
resemble on-policy data more closely by reweighting preference pairs according
to their probability under the current policy. This method not only addresses
the distributional gap problem but also enhances the optimization process
without incurring additional costs. We validate our method on instruction
following benchmarks including Alpaca Eval 2 and MT-bench. WPO not only
outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2
but also establishes a remarkable length-controlled winning rate against
GPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8B
model on the leaderboard. We will release the code and models at
https://github.com/wzhouad/WPO.

摘要：人類回饋強化學習 (RLHF) 是讓大型語言模型 (LLM) 更符合人類價值觀的有前途的解決方案。離線偏好最佳化（偏好數據從其他模型取得）因其成本效益和可擴充性而廣泛採用。然而，離線偏好最佳化經常會因用於數據收集的政策和目標政策之間的分配差距而受苦，導致次佳最佳化。在本文中，我們提出了一種新的策略，透過模擬在線政策學習與離線偏好數據來減輕此問題。我們的加權偏好最佳化 (WPO) 方法會調整離線數據，使其更接近在線政策數據，方法是根據當前政策的機率重新加權偏好配對。此方法不僅解決了分配差距問題，還增強了最佳化流程，而不會產生額外成本。我們在包括 Alpaca Eval 2 和 MT-bench 在內的指令遵循基準上驗證了我們的模型。WPO 不僅在 Alpaca Eval 2 上比直接偏好最佳化 (DPO) 高出 5.6%，還根據 Llama-3-8B-Instruct 建立了與 GPT-4-turbo 相抗衡的 48.6% 長度控制獲勝率，使其成為排行榜上最強大的 8B 模型。我們將在 https://github.com/wzhouad/WPO 釋出程式碼和模型。

##### **On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning**
2406.11823v1 by Geewook Kim, Minjoon Seo

Recent advancements in language and vision assistants have showcased
impressive capabilities but suffer from a lack of transparency, limiting
broader research and reproducibility. While open-source models handle general
image tasks effectively, they face challenges with the high computational
demands of complex visually-situated text understanding. Such tasks often
require increased token inputs and large vision modules to harness
high-resolution information. Striking a balance between model size and data
importance remains an open question. This study aims to redefine the design of
vision-language models by identifying key components and creating efficient
models with constrained inference costs. By strategically formulating datasets,
optimizing vision modules, and enhancing supervision techniques, we achieve
significant improvements in inference throughput while maintaining high
performance. Extensive experiments across models ranging from 160M to 13B
parameters offer insights into model optimization. We will fully open-source
our codebase, models, and datasets at https://github.com/naver-ai/elva .

摘要：語言和視覺助理的最新進展展示了令人印象深刻的能力，但缺乏透明度，限制了更廣泛的研究和可複製性。雖然開源模型有效地處理一般影像任務，但它們在複雜的視覺情境文本理解的高計算需求方面面臨挑戰。此類任務通常需要增加的標記輸入和大型視覺模組，以利用高解析度資訊。在模型大小和資料重要性之間取得平衡仍然是一個未解決的問題。本研究旨在透過識別關鍵組成部分和建立具有受限推論成本的有效率模型，重新定義視覺語言模型的設計。透過策略性地制定資料集、最佳化視覺模組和增強監督技術，我們在維持高性能的同時，顯著改善了推論處理量。從 160M 到 13B 參數的模型的廣泛實驗提供了模型最佳化的見解。我們將在 https://github.com/naver-ai/elva 完全開放我們的程式碼庫、模型和資料集。

##### **Embodied Instruction Following in Unknown Environments**
2406.11818v1 by Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan

Enabling embodied agents to complete complex human instructions from natural
language is crucial to autonomous systems in household services. Conventional
methods can only accomplish human instructions in the known environment where
all interactive objects are provided to the embodied agent, and directly
deploying the existing approaches for the unknown environment usually generates
infeasible plans that manipulate non-existing objects. On the contrary, we
propose an embodied instruction following (EIF) method for complex tasks in the
unknown environment, where the agent efficiently explores the unknown
environment to generate feasible plans with existing objects to accomplish
abstract instructions. Specifically, we build a hierarchical embodied
instruction following framework including the high-level task planner and the
low-level exploration controller with multimodal large language models. We then
construct a semantic representation map of the scene with dynamic region
attention to demonstrate the known visual clues, where the goal of task
planning and scene exploration is aligned for human instruction. For the task
planner, we generate the feasible step-by-step plans for human goal
accomplishment according to the task completion process and the known visual
clues. For the exploration controller, the optimal navigation or object
interaction policy is predicted based on the generated step-wise plans and the
known visual clues. The experimental results demonstrate that our method can
achieve 45.09% success rate in 204 complex human instructions such as making
breakfast and tidying rooms in large house-level scenes.

摘要：讓具身代理人完成自然語言中的複雜人類指示對於家庭服務中的自主系統至關重要。傳統方法只能在已知環境中完成人類指示，其中所有互動對象都提供給具身代理人，並且直接部署現有方法來應對未知環境通常會產生不可行的計畫，這些計畫會操作不存在的對象。相反，我們提出了一種在未知環境中執行複雜任務的具身指令遵循 (EIF) 方法，其中代理人有效地探索未知環境以生成可行的計畫，並使用現有對象來完成抽象指令。具體來說，我們構建了一個分層具身指令遵循架構，包括高級任務規劃器和具有多模態大型語言模型的低級探索控制器。然後，我們構建一個場景的語義表示圖，並使用動態區域注意來展示已知的視覺線索，其中任務規劃和場景探索的目標與人類指令保持一致。對於任務規劃器，我們根據任務完成過程和已知的視覺線索來生成可行的逐步計畫，以完成人類目標。對於探索控制器，最佳導航或對象互動策略是根據生成的逐步計畫和已知的視覺線索來預測的。實驗結果表明，我們的模型可以在 204 條複雜的人類指令中實現 45.09% 的成功率，例如在大型房屋場景中製作早餐和整理房間。

##### **Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level**
2406.11817v1 by Jie Liu, Zhanhui Zhou, Jiaheng Liu, Xingyuan Bu, Chao Yang, Han-Sen Zhong, Wanli Ouyang

Direct Preference Optimization (DPO), a standard method for aligning language
models with human preferences, is traditionally applied to offline preferences.
Recent studies show that DPO benefits from iterative training with online
preferences labeled by a trained reward model. In this work, we identify a
pitfall of vanilla iterative DPO - improved response quality can lead to
increased verbosity. To address this, we introduce iterative length-regularized
DPO (iLR-DPO) to penalize response length. Our empirical results show that
iLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing
verbosity. Specifically, our 7B model achieves a $50.5\%$ length-controlled win
rate against $\texttt{GPT-4 Preview}$ on AlpacaEval 2.0, and excels across
standard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.
These results demonstrate the effectiveness of iterative DPO in aligning
language models with human feedback.

摘要：直接偏好最佳化（DPO）是一種將語言模型與人類偏好對齊的標準方法，傳統上應用於離線偏好。最近的研究表明，DPO 受益於使用受過訓練的獎勵模型標記的線上偏好的反覆訓練。在這項工作中，我們發現了香草反覆 DPO 的一個陷阱 - 改善的回應品質可能導致冗長。為了解決這個問題，我們引入了反覆長度正規化的 DPO (iLR-DPO) 來懲罰回應長度。我們的實證結果表明，iLR-DPO 可以增強 7B 模型，使其在不增加冗長的情況下與 GPT-4 相媲美。具體來說，我們的 7B 模型在 AlpacaEval 2.0 上實現了對 $\texttt{GPT-4 預覽}$ 的 $50.5\%$ 長度控制勝率，並在包括 MT-Bench、Arena-Hard 和 OpenLLM 排行榜在內的標準基準中表現出色。這些結果證明了反覆 DPO 在將語言模型與人類回饋對齊方面的有效性。

##### **How Do Large Language Models Acquire Factual Knowledge During Pretraining?**
2406.11813v1 by Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo

Despite the recent observation that large language models (LLMs) can store
substantial factual knowledge, there is a limited understanding of the
mechanisms of how they acquire factual knowledge through pretraining. This work
addresses this gap by studying how LLMs acquire factual knowledge during
pretraining. The findings reveal several important insights into the dynamics
of factual knowledge acquisition during pretraining. First, counterintuitively,
we observe that pretraining on more data shows no significant improvement in
the model's capability to acquire and maintain factual knowledge. Next, there
is a power-law relationship between training steps and forgetting of
memorization and generalization of factual knowledge, and LLMs trained with
duplicated training data exhibit faster forgetting. Third, training LLMs with
larger batch sizes can enhance the models' robustness to forgetting. Overall,
our observations suggest that factual knowledge acquisition in LLM pretraining
occurs by progressively increasing the probability of factual knowledge
presented in the pretraining data at each step. However, this increase is
diluted by subsequent forgetting. Based on this interpretation, we demonstrate
that we can provide plausible explanations for recently observed behaviors of
LLMs, such as the poor performance of LLMs on long-tail knowledge and the
benefits of deduplicating the pretraining corpus.

摘要：儘管最近觀察到大型語言模型 (LLM) 能儲存大量的實際知識，但對於 LLM 在預訓練過程中如何獲取實際知識的機制，理解仍然有限。這項研究透過探討 LLM 在預訓練期間如何獲取實際知識，來探討這個知識缺口。研究結果揭露了幾個重要的見解，說明在預訓練期間獲取實際知識的動態。首先，與直覺相反，我們觀察到使用更多資料進行預訓練並未顯著提升模型獲取和維護實際知識的能力。其次，訓練步驟與遺忘、記憶和實際知識的概化之間存在冪律關係，而使用重複訓練資料訓練的 LLM 遺忘得更快。第三，使用較大的批次大小訓練 LLM 能夠提升模型對遺忘的穩健性。總的來說，我們的觀察結果顯示，LLM 預訓練中的實際知識獲取是透過在每個步驟逐步增加預訓練資料中呈現的實際知識機率而發生的。然而，這種增加會被隨後的遺忘所稀釋。根據這個解釋，我們證明我們可以對最近觀察到的 LLM 行為提供合理的解釋，例如 LLM 在長尾知識上的表現不佳，以及對預訓練語料庫進行重複資料刪除的好處。

##### **RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content**
2406.11811v1 by Joao Monteiro, Pierre-Andre Noel, Etienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, David Vazquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian

Large Language Models (LLMs) are trained on vast amounts of data, most of
which is automatically scraped from the internet. This data includes
encyclopedic documents that harbor a vast amount of general knowledge (e.g.,
Wikipedia) but also potentially overlap with benchmark datasets used for
evaluating LLMs. Consequently, evaluating models on test splits that might have
leaked into the training set is prone to misleading conclusions. To foster
sound evaluation of language models, we introduce a new test dataset named
RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a
collection of five splits of test sets, four of which have not been released to
the internet or exposed to LLM APIs prior to this publication. Each sample in
RepLiQA comprises (1) a reference document crafted by a human annotator and
depicting an imaginary scenario (e.g., a news article) absent from the
internet; (2) a question about the document's topic; (3) a ground-truth answer
derived directly from the information in the document; and (4) the paragraph
extracted from the reference document containing the answer. As such, accurate
answers can only be generated if a model can find relevant content within the
provided document. We run a large-scale benchmark comprising several
state-of-the-art LLMs to uncover differences in performance across models of
various types and sizes in a context-conditional language modeling setting.
Released splits of RepLiQA can be found here:
https://huggingface.co/datasets/ServiceNow/repliqa.

摘要：大型語言模型 (LLM) 是在大量的資料上訓練的，其中大部分是從網路上自動抓取的。這些資料包括包含大量一般知識的百科全書文件（例如維基百科），但也可能與用於評估 LLM 的基準資料集重疊。因此，在可能已洩漏到訓練集中的測試分割上評估模型容易導致誤導性的結論。為了促進語言模型的健全評估，我們引入了名為 RepLiQA 的新測試資料集，適用於問答和主題檢索任務。RepLiQA 是由五個測試集分割組成的集合，其中四個在本次發布之前尚未發布到網路上或公開給 LLM API。RepLiQA 中的每個範例包含 (1) 由人類註解者編寫的參考文件，描述一個不存在於網路上、虛構的情境（例如新聞文章）；(2) 關於文件主題的問題；(3) 直接從文件中的資訊衍生的基本事實答案；以及 (4) 從包含答案的參考文件中摘錄的段落。因此，只有當模型可以在提供的文件中找到相關內容時，才能產生準確的答案。我們執行了一項大規模基準測試，其中包含多個最先進的 LLM，以在情境條件語言建模設定中揭示各種類型和規模的模型之間的效能差異。RepLiQA 的已發布分割可以在此處找到：https://huggingface.co/datasets/ServiceNow/repliqa。

##### **Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations**
2406.11801v1 by Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria

Ensuring the safe alignment of large language models (LLMs) with human values
is critical as they become integral to applications like translation and
question answering. Current alignment methods struggle with dynamic user
intentions and complex objectives, making models vulnerable to generating
harmful content. We propose Safety Arithmetic, a training-free framework
enhancing LLM safety across different scenarios: Base models, Supervised
fine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm
Direction Removal to avoid harmful content and Safety Alignment to promote safe
responses. Additionally, we present NoIntentEdit, a dataset highlighting edit
instances that could compromise model safety if used unintentionally. Our
experiments show that Safety Arithmetic significantly improves safety measures,
reduces over-safety, and maintains model utility, outperforming existing
methods in ensuring safe content generation.

摘要：確保大型語言模型 (LLM) 與人類價值觀安全一致，對於它們成為翻譯和問答等應用程式不可或缺的一部分至關重要。目前的對齊方式方法難以應對動態使用者意圖和複雜目標，使得模型容易產生有害內容。我們提出安全算術，這是一個免訓練框架，可增強 LLM 在不同場景中的安全性：基礎模型、監督微調模型 (SFT) 和編輯模型。安全算術涉及危害方向移除，以避免有害內容，以及安全對齊，以促進安全回應。此外，我們提出 NoIntentEdit，這是一個資料集，重點標示出如果無意中使用，可能會損害模型安全性的編輯實例。我們的實驗表明，安全算術顯著改善了安全措施，減少了過度安全性，並維持模型效用，在確保安全內容生成方面優於現有方法。

##### **DataComp-LM: In search of the next generation of training sets for language models**
2406.11794v1 by Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muenninghoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldani, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, Vaishaal Shankar

We introduce DataComp for Language Models (DCLM), a testbed for controlled
dataset experiments with the goal of improving language models. As part of
DCLM, we provide a standardized corpus of 240T tokens extracted from Common
Crawl, effective pretraining recipes based on the OpenLM framework, and a broad
suite of 53 downstream evaluations. Participants in the DCLM benchmark can
experiment with data curation strategies such as deduplication, filtering, and
data mixing at model scales ranging from 412M to 7B parameters. As a baseline
for DCLM, we conduct extensive experiments and find that model-based filtering
is key to assembling a high-quality training set. The resulting dataset,
DCLM-Baseline enables training a 7B parameter language model from scratch to
64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the
previous state-of-the-art in open-data language models, DCLM-Baseline
represents a 6.6 percentage point improvement on MMLU while being trained with
40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and
Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53
natural language understanding tasks while being trained with 6.6x less compute
than Llama 3 8B. Our results highlight the importance of dataset design for
training language models and offer a starting point for further research on
data curation.

摘要：<paragraph>我們為語言模型 (DCLM) 引入 DataComp，一個用於控制資料集實驗的測試平台，目標是改進語言模型。作為 DCLM 的一部分，我們提供一個標準語料庫，包含從 Common Crawl 中提取的 240T 個符號，基於 OpenLM 框架的有效預訓練配方，以及廣泛的 53 個下游評估。DCLM 基準測試的參與者可以嘗試資料策展策略，例如去重、過濾和資料混合，模型規模從 412M 到 7B 個參數。作為 DCLM 的基準，我們進行了廣泛的實驗，發現基於模型的過濾是組裝高品質訓練集的關鍵。由此產生的資料集 DCLM-Baseline 能夠從頭開始訓練一個 7B 參數語言模型，在 MMLU 上以 2.6T 個訓練符號達到 64% 的 5 次準確度。與 MAP-Neo 相比，DCLM-Baseline 是開放資料語言模型中先前的最新技術，在 MMLU 上提高了 6.6 個百分點，同時訓練時運算量減少了 40%。我們的基準模型也與 MMLU 上的 Mistral-7B-v0.3 和 Llama 3 8B 相當（63% 和 66%），並且在平均 53 個自然語言理解任務上的表現類似，同時訓練時運算量比 Llama 3 8B 少 6.6 倍。我們的結果強調了資料集設計對訓練語言模型的重要性，並為資料策展的進一步研究提供了一個起點。</paragraph>

##### **A Brief Survey on Leveraging Large Scale Vision Models for Enhanced Robot Grasping**
2406.11786v1 by Abhi Kamboj, Katherine Driggs-Campbell

Robotic grasping presents a difficult motor task in real-world scenarios,
constituting a major hurdle to the deployment of capable robots across various
industries. Notably, the scarcity of data makes grasping particularly
challenging for learned models. Recent advancements in computer vision have
witnessed a growth of successful unsupervised training mechanisms predicated on
massive amounts of data sourced from the Internet, and now nearly all prominent
models leverage pretrained backbone networks. Against this backdrop, we begin
to investigate the potential benefits of large-scale visual pretraining in
enhancing robot grasping performance. This preliminary literature review sheds
light on critical challenges and delineates prospective directions for future
research in visual pretraining for robotic manipulation.

摘要：機器人抓握在現實世界場景中呈現出困難的運動任務，
構成機器人在各個產業中部署的重大障礙。值得注意的是，數據的稀缺性使得抓握對學習模型特別具有挑戰性。最近在電腦視覺方面的進展見證了成功無監督訓練機制的成長，這些機制建立在從網際網路上取得的大量數據，而現在幾乎所有著名的模型都利用預先訓練好的主幹網路。在此背景下，我們開始探討大規模視覺預訓練在提升機器人抓握效能方面的潛在好處。這篇初步文獻回顧闡明了關鍵挑戰，並勾勒出機器人操作視覺預訓練未來研究的預期方向。

##### **CELL your Model: Contrastive Explanation Methods for Large Language Models**
2406.11785v1 by Ronny Luss, Erik Miehling, Amit Dhurandhar

The advent of black-box deep neural network classification models has sparked
the need to explain their decisions. However, in the case of generative AI such
as large language models (LLMs), there is no class prediction to explain.
Rather, one can ask why an LLM output a particular response to a given prompt.
In this paper, we answer this question by proposing, to the best of our
knowledge, the first contrastive explanation methods requiring simply
black-box/query access. Our explanations suggest that an LLM outputs a reply to
a given prompt because if the prompt was slightly modified, the LLM would have
given a different response that is either less preferable or contradicts the
original response. The key insight is that contrastive explanations simply
require a distance function that has meaning to the user and not necessarily a
real valued representation of a specific response (viz. class label). We offer
two algorithms for finding contrastive explanations: i) A myopic algorithm,
which although effective in creating contrasts, requires many model calls and
ii) A budgeted algorithm, our main algorithmic contribution, which
intelligently creates contrasts adhering to a query budget, necessary for
longer contexts. We show the efficacy of these methods on diverse natural
language tasks such as open-text generation, automated red teaming, and
explaining conversational degradation.

摘要：隨著黑箱深度神經網路分類模型的出現，解釋其決策的需求也隨之產生。然而，在生成式 AI（例如大型語言模型 (LLM)）的情況下，沒有類別預測可以解釋。相反，可以詢問 LLM 為何對特定提示輸出特定回應。在本文中，我們透過提出（據我們所知）第一個只要求黑箱/查詢存取權的對比解釋方法來回答這個問題。我們的解釋表明，LLM 對特定提示輸出的回覆是因為，如果提示稍作修改，LLM 會給出不同的回覆，而該回覆較不理想或與原始回覆相矛盾。關鍵見解在於，對比解釋僅需要對使用者有意義的距離函數，而不一定需要特定回覆的實值表示（即類別標籤）。我們提供了兩種尋找對比解釋的演算法：i) 一種近視演算法，雖然在建立對比方面有效，但需要許多模型呼叫，以及 ii) 一種預算演算法，這是我們主要的演算法貢獻，它可以智慧地建立符合查詢預算的對比，這對於較長的脈絡是必要的。我們在各種自然語言任務上展示了這些方法的效能，例如開放文字生成、自動化紅隊測試，以及解釋對話式退化。

##### **MDCR: A Dataset for Multi-Document Conditional Reasoning**
2406.11784v1 by Peter Baile Chen, Yi Zhang, Chunwei Liu, Sejal Gupta, Yoon Kim, Michael Cafarella

The same real-life questions posed to different individuals may lead to
different answers based on their unique situations. For instance, whether a
student is eligible for a scholarship depends on eligibility conditions, such
as major or degree required. ConditionalQA was proposed to evaluate models'
capability of reading a document and answering eligibility questions,
considering unmentioned conditions. However, it is limited to questions on
single documents, neglecting harder cases that may require cross-document
reasoning and optimization, for example, "What is the maximum number of
scholarships attainable?" Such questions over multiple documents are not only
more challenging due to more context having to understand, but also because the
model has to (1) explore all possible combinations of unmentioned conditions
and (2) understand the relationship between conditions across documents, to
reason about the optimal outcome. To evaluate models' capability of answering
such questions, we propose a new dataset MDCR, which can reflect real-world
challenges and serve as a new test bed for complex conditional reasoning that
requires optimization. We evaluate this dataset using the most recent LLMs and
demonstrate their limitations in solving this task. We believe this dataset
will facilitate future research in answering optimization questions with
unknown conditions.

摘要：相同的真實生活問題向不同的人提出時，可能會根據他們獨特的情況而導致不同的答案。例如，學生是否有資格獲得獎學金取決於資格條件，例如所需的專業或學位。ConditionalQA 被提議用於評估模型閱讀文件和回答資格問題的能力，考慮未提到的條件。然而，它僅限於單一文件的問題，忽視了可能需要跨文件推理和優化的更困難的情況，例如，「可以獲得的最大獎學金數量是多少？」此類跨多個文件的問題不僅由於必須理解的內容更多而更具挑戰性，還因為模型必須 (1) 探索未提及條件的所有可能組合，以及 (2) 了解跨文件條件之間的關係，才能對最佳結果進行推理。為了評估模型回答此類問題的能力，我們提出了一個新的資料集 MDCR，它可以反映現實世界的挑戰，並作為複雜條件推理的新測試平台，需要進行優化。我們使用最新的 LLM 評估此資料集，並展示它們在解決此任務時的限制。我們相信此資料集將促進未來在回答具有未知條件的優化問題方面的研究。

##### **Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs**
2406.11780v1 by Swanand Ravindra Kadhe, Farhan Ahmed, Dennis Wei, Nathalie Baracaldo, Inkit Padhi

Large language models (LLMs) have shown to pose social and ethical risks such
as generating toxic language or facilitating malicious use of hazardous
knowledge. Machine unlearning is a promising approach to improve LLM safety by
directly removing harmful behaviors and knowledge. In this paper, we propose
"SPlit, UNlearn, MerGE" (SPUNGE), a framework that can be used with any
unlearning method to amplify its effectiveness. SPUNGE leverages data
attributes during unlearning by splitting unlearning data into subsets based on
specific attribute values, unlearning each subset separately, and merging the
unlearned models. We empirically demonstrate that SPUNGE significantly improves
the performance of two recent unlearning methods on state-of-the-art LLMs while
maintaining their general capabilities on standard academic benchmarks.

摘要：大型語言模型 (LLM) 已顯示會造成社會和道德風險，例如產生有毒語言或促進惡意使用危險知識。機器遺忘是一種有前途的方法，可透過直接移除有害行為和知識來改善 LLM 的安全性。在本文中，我們提出「SPlit, UNlearn, MerGE」（SPUNGE），一個可與任何遺忘方法一起使用以擴大其有效性的框架。SPUNGE 在遺忘過程中利用資料屬性，透過根據特定屬性值將遺忘資料分割成子集、個別遺忘每個子集，然後合併已遺忘的模型。我們透過實證證明，SPUNGE 在最先進的 LLM 上顯著改善了兩種最新遺忘方法的效能，同時在標準學術基準上維持其一般能力。

##### **Improving Multi-Agent Debate with Sparse Communication Topology**
2406.11776v1 by Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, Eugene Ie

Multi-agent debate has proven effective in improving large language models
quality for reasoning and factuality tasks. While various role-playing
strategies in multi-agent debates have been explored, in terms of the
communication among agents, existing approaches adopt a brute force algorithm
-- each agent can communicate with all other agents. In this paper, we
systematically investigate the effect of communication connectivity in
multi-agent systems. Our experiments on GPT and Mistral models reveal that
multi-agent debates leveraging sparse communication topology can achieve
comparable or superior performance while significantly reducing computational
costs. Furthermore, we extend the multi-agent debate framework to multimodal
reasoning and alignment labeling tasks, showcasing its broad applicability and
effectiveness. Our findings underscore the importance of communication
connectivity on enhancing the efficiency and effectiveness of the "society of
minds" approach.

摘要：多主體辯論已被證實能有效提升大型語言模型在推理和事實性任務中的品質。儘管已探討過多主體辯論中各種角色扮演策略，但在主體間的溝通方面，現有方法採用蠻力演算法——每個主體都可以與所有其他主體溝通。在本文中，我們系統性地探討了溝通連通性在多主體系統中的影響。我們在 GPT 和 Mistral 模型上的實驗揭示，利用稀疏溝通拓撲的多主體辯論可以在大幅降低運算成本的同時，達到可比較或更佳的效能。此外，我們將多主體辯論架構延伸到多模態推理和比對標記任務，展示其廣泛的適用性和有效性。我們的研究結果強調了溝通連通性對於提升「心智社會」方法的效率和有效性的重要性。

##### **Task Me Anything**
2406.11775v1 by Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, Ranjay Krishna

Benchmarks for large multimodal language models (MLMs) now serve to
simultaneously assess the general capabilities of models instead of evaluating
for a specific capability. As a result, when a developer wants to identify
which models to use for their application, they are overwhelmed by the number
of benchmarks and remain uncertain about which benchmark's results are most
reflective of their specific use case. This paper introduces Task-Me-Anything,
a benchmark generation engine which produces a benchmark tailored to a user's
needs. Task-Me-Anything maintains an extendable taxonomy of visual assets and
can programmatically generate a vast number of task instances. Additionally, it
algorithmically addresses user queries regarding MLM performance efficiently
within a computational budget. It contains 113K images, 10K videos, 2K 3D
object assets, over 365 object categories, 655 attributes, and 335
relationships. It can generate 750M image/video question-answering pairs, which
focus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals
critical insights: open-source MLMs excel in object and attribute recognition
but lack spatial and temporal understanding; each model exhibits unique
strengths and weaknesses; larger models generally perform better, though
exceptions exist; and GPT4o demonstrates challenges in recognizing
rotating/moving objects and distinguishing colors.

摘要：大型多模态语言模型 (MLM) 的基准现在可用于同时评估模型的通用功能，而不是评估特定功能。因此，当开发人员想要确定为其应用程序使用哪些模型时，他们会被基准数量所淹没，并且仍然不确定哪个基准的结果最能反映其特定用例。本文介绍了 Task-Me-Anything，这是一个基准生成引擎，可以根据用户的需求生成定制基准。Task-Me-Anything 维护了一个可扩展的可视资产分类法，并且可以以编程方式生成大量任务实例。此外，它在计算预算内通过算法高效地处理用户关于 MLM 性能的查询。它包含 113K 张图像、10K 个视频、2K 个 3D 对象资产、超过 365 个对象类别、655 个属性和 335 个关系。它可以生成 750M 个图像/视频问答对，重点是评估 MLM 感知能力。Task-Me-Anything 揭示了关键见解：开源 MLM 在对象和属性识别方面表现出色，但缺乏空间和时间理解；每个模型都表现出独特的优势和劣势；较大的模型通常表现得更好，尽管存在例外；GPT4o 在识别旋转/移动物体和区分颜色方面表现出挑战。

##### **GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities**
2406.11768v1 by Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, S Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha

Perceiving and understanding non-speech sounds and non-verbal speech is
essential to making decisions that help us interact with our surroundings. In
this paper, we propose GAMA, a novel General-purpose Large Audio-Language Model
(LALM) with Advanced Audio Understanding and Complex Reasoning Abilities. We
build GAMA by integrating an LLM with multiple types of audio representations,
including features from a custom Audio Q-Former, a multi-layer aggregator that
aggregates features from multiple layers of an audio encoder. We fine-tune GAMA
on a large-scale audio-language dataset, which augments it with audio
understanding capabilities. Next, we propose CompA-R (Instruction-Tuning for
Complex Audio Reasoning), a synthetically generated instruction-tuning (IT)
dataset with instructions that require the model to perform complex reasoning
on the input audio. We instruction-tune GAMA with CompA-R to endow it with
complex reasoning abilities, where we further add a soft prompt as input with
high-level semantic evidence by leveraging event tags of the input audio.
Finally, we also propose CompA-R-test, a human-labeled evaluation dataset for
evaluating the capabilities of LALMs on open-ended audio question-answering
that requires complex reasoning. Through automated and expert human
evaluations, we show that GAMA outperforms all other LALMs in literature on
diverse audio understanding tasks by margins of 1%-84%. Further, GAMA IT-ed on
CompA-R proves to be superior in its complex reasoning and instruction
following capabilities.

摘要：<paragraph>感知和理解非語言聲音和非語言言語對於做出有助於我們與周圍環境互動的決策至關重要。在本文中，我們提出 GAMA，一種新穎的通用大型音訊語言模型 (LALM)，具備先進的音訊理解和複雜推理能力。我們透過將 LLM 與多種類型的音訊表徵整合在一起來建構 GAMA，包括來自自訂音訊 Q-Former 的特徵，這是一個多層聚合器，可以聚合音訊編碼器的多層特徵。我們在一個大規模的音訊語言資料集上微調 GAMA，這賦予它音訊理解能力。接下來，我們提出 CompA-R（複雜音訊推理的指令微調），一個合成產生的指令微調 (IT) 資料集，其中包含需要模型對輸入音訊執行複雜推理的指令。我們使用 CompA-R 對 GAMA 進行指令微調，賦予它複雜的推理能力，我們進一步添加一個軟提示作為輸入，並透過利用輸入音訊的事件標籤來獲取高層級語義證據。最後，我們還提出了 CompA-R-test，一個人工標記的評估資料集，用於評估 LALM 在需要複雜推理的開放式音訊問答上的能力。透過自動化和專家人工評估，我們表明 GAMA 在各種音訊理解任務上優於文獻中的所有其他 LALM，幅度為 1%-84%。此外，在 CompA-R 上進行 IT 的 GAMA 證明其在複雜推理和指令遵循能力方面具有優越性。</paragraph>

##### **STAR: SocioTechnical Approach to Red Teaming Language Models**
2406.11757v1 by Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac

This research introduces STAR, a sociotechnical framework that improves on
current best practices for red teaming safety of large language models. STAR
makes two key contributions: it enhances steerability by generating
parameterised instructions for human red teamers, leading to improved coverage
of the risk surface. Parameterised instructions also provide more detailed
insights into model failures at no increased cost. Second, STAR improves signal
quality by matching demographics to assess harms for specific groups, resulting
in more sensitive annotations. STAR further employs a novel step of arbitration
to leverage diverse viewpoints and improve label reliability, treating
disagreement not as noise but as a valuable contribution to signal quality.

摘要：本研究引入了 STAR，一種社會技術框架，改進了大型語言模型紅隊安全性的現有最佳實務。STAR 做出兩項主要貢獻：它透過為人類紅隊員產生參數化指令來增強可導向性，進而改善風險表面的覆蓋範圍。參數化指令也以無增加成本的方式提供更詳細的模型失敗見解。其次，STAR 透過比對人口統計資料來評估特定群體的危害，進而改善信號品質，產生更靈敏的註解。STAR 進一步採用一種新穎的仲裁步驟，以利用不同的觀點並改善標籤可靠性，將分歧視為信號品質的寶貴貢獻，而不是雜訊。

##### **DustNet: skillful neural network predictions of Saharan dust**
2406.11754v1 by Trish E. Nowak, Andy T. Augousti, Benno I. Simmons, Stefan Siegert

Suspended in the atmosphere are millions of tonnes of mineral dust which
interacts with weather and climate. Accurate representation of mineral dust in
weather models is vital, yet remains challenging. Large scale weather models
use high power supercomputers and take hours to complete the forecast. Such
computational burden allows them to only include monthly climatological means
of mineral dust as input states inhibiting their forecasting accuracy. Here, we
introduce DustNet a simple, accurate and super fast forecasting model for
24-hours ahead predictions of aerosol optical depth AOD. DustNet trains in less
than 8 minutes and creates predictions in 2 seconds on a desktop computer.
Created by DustNet predictions outperform the state-of-the-art physics-based
model on coarse 1 x 1 degree resolution at 95% of grid locations when compared
to ground truth satellite data. Our results show DustNet has a potential for
fast and accurate AOD forecasting which could transform our understanding of
dust impacts on weather patterns.

摘要：懸浮在大氣中的礦物塵有數百萬噸，會與天氣和氣候相互作用。在天氣模型中準確呈現礦物塵至關重要，但仍然具有挑戰性。大規模天氣模型使用高功率超級電腦，需要數小時才能完成預測。這種計算負擔只能讓它們將礦物塵的每月氣候平均值作為輸入狀態，從而抑制了預測準確度。在此，我們介紹 DustNet，這是一個簡單、準確且超級快速的預測模型，用於預測 24 小時後的氣溶膠光學厚度 AOD。DustNet 的訓練時間不到 8 分鐘，並可以在桌上型電腦上於 2 秒內建立預測。與地面真實衛星數據相比，DustNet 預測在與最先進的基於物理模型的粗略 1 x 1 度解析度中，於 95% 的網格位置表現優異。我們的結果顯示，DustNet 具有快速且準確的 AOD 預測潛力，這可以改變我們對灰塵對天氣模式影響的理解。

##### **A Semantic-based Layer Freezing Approach to Efficient Fine-Tuning of Language Models**
2406.11753v1 by Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang

Finetuning language models (LMs) is crucial for adapting the models to
downstream data and tasks. However, full finetuning is usually costly. Existing
work, such as parameter-efficient finetuning (PEFT), often focuses on
\textit{how to finetune} but neglects the issue of \textit{where to finetune}.
As a pioneering work on answering where to finetune (at the layer level), we
conduct a semantic analysis of the LM inference process. We first propose a
virtual transition of the latent representation and then trace its factual
transition. Based on the deviation in transitions, we estimate the gain of
finetuning each model layer, and further, narrow down the scope for finetuning.
We perform extensive experiments across well-known LMs and datasets. The
results show that our approach is effective and efficient, and outperforms the
existing baselines. Our approach is orthogonal to existing efficient
techniques, such as PEFT methods, offering practical values on LM finetuning.

摘要：微調語言模型 (LM) 對於調整模型以適應下游資料和任務至關重要。然而，完整的微調通常成本很高。現有的工作，例如參數高效微調 (PEFT)，通常側重於「如何微調」，但忽略了「在哪裡微調」的問題。作為回答在哪裡微調（在層級）的開創性工作，我們對 LM 推論過程進行了語義分析。我們首先提出潛在表徵的虛擬轉換，然後追蹤其事實轉換。根據轉換中的偏差，我們估計微調每個模型層的增益，並進一步縮小微調的範圍。我們對著名的 LM 和資料集進行了廣泛的實驗。結果表明，我們的做法有效且高效，並且優於現有的基準。我們的做法與現有的高效技術（例如 PEFT 方法）正交，為 LM 微調提供了實用的價值。

##### **Multi-Layer Ranking with Large Language Models for News Source Recommendation**
2406.11745v1 by Wenjia Zhang, Lin Gui, Rob Procter, Yulan He

To seek reliable information sources for news events, we introduce a novel
task of expert recommendation, which aims to identify trustworthy sources based
on their previously quoted statements. To achieve this, we built a novel
dataset, called NewsQuote, consisting of 23,571 quote-speaker pairs sourced
from a collection of news articles. We formulate the recommendation task as the
retrieval of experts based on their likelihood of being associated with a given
query. We also propose a multi-layer ranking framework employing Large Language
Models to improve the recommendation performance. Our results show that
employing an in-context learning based LLM ranker and a multi-layer
ranking-based filter significantly improve both the predictive quality and
behavioural quality of the recommender system.

摘要：為了尋找新聞事件的可靠資訊來源，我們提出了一項專家推薦的新任務，目標是根據他們之前引用的聲明來識別可信賴的來源。為了實現此目的，我們建立了一個名為 NewsQuote 的新資料集，其中包含從新聞文章集合中取得的 23,571 對引述-發言者。我們將推薦任務制定為根據專家與特定查詢相關的可能性來檢索專家。我們還提出了一個多層級排名架構，採用大型語言模型來改善推薦效能。我們的結果顯示，採用基於情境學習的 LLM 排名器和基於多層級排名的篩選器，可以顯著改善推薦系統的預測品質和行為品質。

##### **Transcendence: Generative Models Can Outperform The Experts That Train Them**
2406.11741v1 by Edwin Zhang, Vincent Zhu, Naomi Saphra, Anat Kleiman, Benjamin L. Edelman, Milind Tambe, Sham M. Kakade, Eran Malach

Generative models are trained with the simple objective of imitating the
conditional probability distribution induced by the data they are trained on.
Therefore, when trained on data generated by humans, we may not expect the
artificial model to outperform the humans on their original objectives. In this
work, we study the phenomenon of transcendence: when a generative model
achieves capabilities that surpass the abilities of the experts generating its
data. We demonstrate transcendence by training an autoregressive transformer to
play chess from game transcripts, and show that the trained model can sometimes
achieve better performance than all players in the dataset. We theoretically
prove that transcendence is enabled by low-temperature sampling, and rigorously
assess this experimentally. Finally, we discuss other sources of transcendence,
laying the groundwork for future investigation of this phenomenon in a broader
setting.

摘要：生成模型的訓練目標很簡單，就是模仿訓練資料所引發的條件機率分佈。因此，當我們使用人類產生的資料進行訓練時，我們可能不會期望人工模型在原本的目標上超越人類。在這項研究中，我們探討超越現象：當生成模型達到的能力超越產生其資料的專家能力。我們透過訓練一個自迴歸Transformer從遊戲記錄中下棋來證明超越，並顯示訓練出來的模型有時可以在資料集中達成比所有玩家更好的表現。我們在理論上證明了超越是由低溫取樣所促成，並嚴格地以實驗評估這一點。最後，我們討論超越的其他來源，為未來在更廣泛的設定中探討此現象奠定基礎。

##### **Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies**
2406.11740v1 by Haojie Huang, Karl Schmeckpeper, Dian Wang, Ondrej Biza, Yaoyao Qian, Haotian Liu, Mingxi Jia, Robert Platt, Robin Walters

Humans can imagine goal states during planning and perform actions to match
those goals. In this work, we propose Imagination Policy, a novel multi-task
key-frame policy network for solving high-precision pick and place tasks.
Instead of learning actions directly, Imagination Policy generates point clouds
to imagine desired states which are then translated to actions using rigid
action estimation. This transforms action inference into a local generative
task. We leverage pick and place symmetries underlying the tasks in the
generation process and achieve extremely high sample efficiency and
generalizability to unseen configurations. Finally, we demonstrate
state-of-the-art performance across various tasks on the RLbench benchmark
compared with several strong baselines.

摘要：人類可以在規劃期間想像目標狀態，並執行動作以符合這些目標。在這項工作中，我們提出想像策略，一種新穎的多任務關鍵影格策略網路，用於解決高精度的取放任務。想像策略並非直接學習動作，而是產生點雲以想像期望的狀態，然後使用剛性動作估計將其轉換為動作。這將動作推論轉變為一個局部生成任務。我們利用取放對稱性在生成過程中作為任務的基礎，並實現極高的樣本效率和對未見配置的一般化能力。最後，我們展示了在 RLbench 基準上各種任務的最新效能，並與幾個強大的基準進行比較。

##### **Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models**
2406.11736v1 by Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, Zhiyong Wu

One of the primary driving forces contributing to the superior performance of
Large Language Models (LLMs) is the extensive availability of human-annotated
natural language data, which is used for alignment fine-tuning. This inspired
researchers to investigate self-training methods to mitigate the extensive
reliance on human annotations. However, the current success of self-training
has been primarily observed in natural language scenarios, rather than in the
increasingly important neural-symbolic scenarios. To this end, we propose an
environment-guided neural-symbolic self-training framework named ENVISIONS. It
aims to overcome two main challenges: (1) the scarcity of symbolic data, and
(2) the limited proficiency of LLMs in processing symbolic language. Extensive
evaluations conducted on three distinct domains demonstrate the effectiveness
of our approach. Additionally, we have conducted a comprehensive analysis to
uncover the factors contributing to ENVISIONS's success, thereby offering
valuable insights for future research in this area. Code will be available at
\url{https://github.com/xufangzhi/ENVISIONS}.

摘要：大型語言模型 (LLM) 效能卓越，其中一項主要驅動力在於大量可供取得的人工標註自然語言資料，這些資料用於比對微調。這啟發研究人員探究自訓練方法，以減輕對人工標註的廣泛依賴。然而，自訓練目前的成功主要見於自然語言場景，而非日益重要的神經符號場景。有鑑於此，我們提出一個名為 ENVISIONS 的環境引導神經符號自訓練架構。其目標是克服兩項主要挑戰：(1) 符號資料的稀少性，以及 (2) LLM 處理符號語言的熟練度有限。在三個不同領域進行的廣泛評估證明了我們方法的有效性。此外，我們進行了全面分析，以找出促成 ENVISIONS 成功的原因，進而為此領域的未來研究提供寶貴見解。程式碼將於\url{https://github.com/xufangzhi/ENVISIONS} 提供。

##### **1000 African Voices: Advancing inclusive multi-speaker multi-accent speech synthesis**
2406.11727v1 by Sewade Ogun, Abraham T. Owodunni, Tobi Olatunji, Eniola Alese, Babatunde Oladimeji, Tejumade Afonja, Kayode Olaleye, Naome A. Etori, Tosin Adewumi

Recent advances in speech synthesis have enabled many useful applications
like audio directions in Google Maps, screen readers, and automated content
generation on platforms like TikTok. However, these systems are mostly
dominated by voices sourced from data-rich geographies with personas
representative of their source data. Although 3000 of the world's languages are
domiciled in Africa, African voices and personas are under-represented in these
systems. As speech synthesis becomes increasingly democratized, it is desirable
to increase the representation of African English accents. We present Afro-TTS,
the first pan-African accented English speech synthesis system able to generate
speech in 86 African accents, with 1000 personas representing the rich
phonological diversity across the continent for downstream application in
Education, Public Health, and Automated Content Creation. Speaker interpolation
retains naturalness and accentedness, enabling the creation of new voices.

摘要：近期在語音合成方面的進展已啟用許多有用的應用程式，
例如 Google 地圖中的語音指示、螢幕閱讀器，以及 TikTok 等平台上的自動化內容產生。然而，這些系統大多由來自資料豐富地區的聲音主導，這些角色代表其原始資料。儘管世界上有 3000 種語言在非洲使用，但這些系統中非洲的聲音和角色卻鮮少被代表。隨著語音合成日益民主化，增加非洲英語口音的代表性是值得期待的。我們提出 Afro-TTS，這是第一個泛非洲口音英語語音合成系統，能夠以 86 種非洲口音產生語音，並有 1000 個角色代表整個大陸豐富的音韻多樣性，以應用於教育、公共衛生和自動化內容建立的下游應用程式。說話者內插保留了自然性和口音，能夠創造出新的聲音。

##### **Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity**
2406.11721v1 by Bingxiang He, Ning Ding, Cheng Qian, Jia Deng, Ganqu Cui, Lifan Yuan, Huan-ang Gao, Huimin Chen, Zhiyuan Liu, Maosong Sun

Understanding alignment techniques begins with comprehending zero-shot
generalization brought by instruction tuning, but little of the mechanism has
been understood. Existing work has largely been confined to the task level,
without considering that tasks are artificially defined and, to LLMs, merely
consist of tokens and representations. This line of research has been limited
to examining transfer between tasks from a task-pair perspective, with few
studies focusing on understanding zero-shot generalization from the perspective
of the data itself. To bridge this gap, we first demonstrate through multiple
metrics that zero-shot generalization during instruction tuning happens very
early. Next, we investigate the facilitation of zero-shot generalization from
both data similarity and granularity perspectives, confirming that encountering
highly similar and fine-grained training data earlier during instruction
tuning, without the constraints of defined "tasks", enables better
generalization. Finally, we propose a more grounded training data arrangement
method, Test-centric Multi-turn Arrangement, and show its effectiveness in
promoting continual learning and further loss reduction. For the first time, we
show that zero-shot generalization during instruction tuning is a form of
similarity-based generalization between training and test data at the instance
level. We hope our analysis will advance the understanding of zero-shot
generalization during instruction tuning and contribute to the development of
more aligned LLMs. Our code is released at
https://github.com/HBX-hbx/dynamics_of_zero-shot_generalization.

摘要：理解對齊技術始於理解指令調整帶來的零次學習泛化，但對機制了解甚少。現有工作在很大程度上局限於任務層面，而沒有考慮到任務是人工定義的，而且對於 LLM 來說，僅僅由標記和表示組成。這條研究路線僅限於從任務對的角度審視任務之間的轉移，很少有研究專注於從數據本身的角度理解零次學習泛化。為了彌補這一差距，我們首先通過多項指標證明，在指令調整期間，零次學習泛化發生得很早。接下來，我們從數據相似性和粒度角度研究零次學習泛化的促進作用，確認在指令調整期間更早地遇到高度相似且細粒度的訓練數據，不受定義的「任務」約束，可以實現更好的泛化。最後，我們提出了一種更紮實的訓練數據排列方法，即以測試為中心的多次排列，並展示了其在促進持續學習和進一步損失減少方面的有效性。我們首次表明，在指令調整期間，零次學習泛化是在實例層面上訓練數據和測試數據之間基於相似性的泛化形式。我們希望我們的分析將促進對指令調整期間零次學習泛化的理解，並有助於開發更多對齊的 LLM。我們的代碼發布在 https://github.com/HBX-hbx/dynamics_of_zero-shot_generalization。

##### **Refusal in Language Models Is Mediated by a Single Direction**
2406.11717v1 by Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, Neel Nanda

Conversational large language models are fine-tuned for both
instruction-following and safety, resulting in models that obey benign requests
but refuse harmful ones. While this refusal behavior is widespread across chat
models, its underlying mechanisms remain poorly understood. In this work, we
show that refusal is mediated by a one-dimensional subspace, across 13 popular
open-source chat models up to 72B parameters in size. Specifically, for each
model, we find a single direction such that erasing this direction from the
model's residual stream activations prevents it from refusing harmful
instructions, while adding this direction elicits refusal on even harmless
instructions. Leveraging this insight, we propose a novel white-box jailbreak
method that surgically disables refusal with minimal effect on other
capabilities. Finally, we mechanistically analyze how adversarial suffixes
suppress propagation of the refusal-mediating direction. Our findings
underscore the brittleness of current safety fine-tuning methods. More broadly,
our work showcases how an understanding of model internals can be leveraged to
develop practical methods for controlling model behavior.

摘要：對話式大型語言模型經過微調，同時具備遵循指示和安全性，因此產生的模型會遵守良性要求，但拒絕有害要求。雖然這種拒絕行為在聊天模型中很普遍，但其底層機制仍鮮為人知。在這項工作中，我們展示拒絕是由一維子空間調解的，跨越 13 個流行的開源聊天模型，參數規模高達 72B。具體來說，對於每個模型，我們找到一個單一方向，從模型的殘差流激活中擦除此方向可防止它拒絕有害指令，而添加此方向會引發拒絕，即使是無害指令。利用此見解，我們提出了一種新穎的白盒越獄方法，該方法通過手術禁用拒絕，對其他功能的影響最小。最後，我們機械地分析了對抗性後綴如何抑制拒絕調解方向的傳播。我們的發現強調了當前安全微調方法的脆弱性。更廣泛地說，我們的研究展示了如何利用對模型內部的理解來開發控制模型行為的實用方法。

##### **Measuring memorization in RLHF for code completion**
2406.11715v1 by Aneesh Pappu, Billy Porter, Ilia Shumailov, Jamie Hayes

Reinforcement learning with human feedback (RLHF) has become the dominant
method to align large models to user preferences. Unlike fine-tuning, for which
there are many studies regarding training data memorization, it is not clear
how memorization is affected by or introduced in the RLHF alignment process.
Understanding this relationship is important as real user data may be collected
and used to align large models; if user data is memorized during RLHF and later
regurgitated, this could raise privacy concerns. In this work, we analyze how
training data memorization can surface and propagate through each phase of
RLHF. We focus our study on code completion models, as code completion is one
of the most popular use cases for large language models. We find that RLHF
significantly decreases the chance that data used for reward modeling and
reinforcement learning is memorized, in comparison to aligning via directly
fine-tuning on this data, but that examples already memorized during the
fine-tuning stage of RLHF, will, in the majority of cases, remain memorized
after RLHF.

摘要：人類回饋增強學習（RLHF）已成為調整大型模型以符合使用者偏好的主要方法。與微調不同，對於微調有許多關於訓練資料記憶的研究，但尚不清楚記憶是如何受到 RLHF 調整程序影響或引入的。了解這種關係很重要，因為可能會收集真實使用者資料並用於調整大型模型；如果使用者資料在 RLHF 期間被記憶下來並稍後再重複，這可能會引發隱私問題。在這項工作中，我們分析訓練資料記憶如何在 RLHF 的每個階段浮現並傳播。我們將研究重點放在程式碼完成模型上，因為程式碼完成是大語言模型最受歡迎的用例之一。我們發現，與直接對此資料進行微調的調整方式相比，RLHF 會顯著降低用於獎勵建模和增強學習的資料被記憶的機率，但 RLHF 微調階段中已記憶的範例在大部分情況下，RLHF 之後仍會保持記憶。

##### **Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging**
2406.11709v1 by Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han

Socratic questioning is an effective teaching strategy, encouraging critical
thinking and problem-solving. The conversational capabilities of large language
models (LLMs) show great potential for providing scalable, real-time student
guidance. However, current LLMs often give away solutions directly, making them
ineffective instructors. We tackle this issue in the code debugging domain with
TreeInstruct, an Instructor agent guided by a novel state space-based planning
algorithm. TreeInstruct asks probing questions to help students independently
identify and resolve errors. It estimates a student's conceptual and
syntactical knowledge to dynamically construct a question tree based on their
responses and current knowledge state, effectively addressing both independent
and dependent mistakes concurrently in a multi-turn interaction setting. In
addition to using an existing single-bug debugging benchmark, we construct a
more challenging multi-bug dataset of 150 coding problems, incorrect solutions,
and bug fixes -- all carefully constructed and annotated by experts. Extensive
evaluation shows TreeInstruct's state-of-the-art performance on both datasets,
proving it to be a more effective instructor than baselines. Furthermore, a
real-world case study with five students of varying skill levels further
demonstrates TreeInstruct's ability to guide students to debug their code
efficiently with minimal turns and highly Socratic questioning.

摘要：蘇格拉底式提問是一種有效的教學策略，鼓勵批判性思考和問題解決。大型語言模型 (LLM) 的對話能力顯示出提供可擴充、即時的學生指導的巨大潛力。然而，目前的 LLM 經常直接提供解決方案，這使得它們成為無效的指導者。我們在程式碼偵錯領域中使用 TreeInstruct 來解決這個問題，TreeInstruct 是一個指導代理，由一個新穎的基於狀態空間的規劃演算法所引導。TreeInstruct 會提出探測性問題，以幫助學生獨立識別和解決錯誤。它會估計學生的概念性和句法知識，以根據他們的回應和當前的知識狀態動態構建一個問題樹，有效地同時在多輪互動設定中解決獨立和依賴的錯誤。除了使用現有的單一錯誤偵錯基準之外，我們還構建了一個更具挑戰性的 150 個編碼問題、不正確的解決方案和錯誤修正的多錯誤資料集，這些資料集都是由專家仔細構建和註解的。廣泛的評估顯示 TreeInstruct 在兩個資料集上的效能都達到最先進的水平，證明它比基準更有效的指導者。此外，一個包含五名不同技能等級學生的真實案例研究進一步證明了 TreeInstruct 能夠指導學生以最少的輪次和高度蘇格拉底式的提問有效地偵錯他們的程式碼。

##### **Prompts as Auto-Optimized Training Hyperparameters: Training Best-in-Class IR Models from Scratch with 10 Gold Labels**
2406.11706v1 by Jasper Xian, Saron Samuel, Faraz Khoubsirat, Ronak Pradeep, Md Arafat Sultan, Radu Florian, Salim Roukos, Avirup Sil, Christopher Potts, Omar Khattab

We develop a method for training small-scale (under 100M parameter) neural
information retrieval models with as few as 10 gold relevance labels. The
method depends on generating synthetic queries for documents using a language
model (LM), and the key step is that we automatically optimize the LM prompt
that is used to generate these queries based on training quality. In
experiments with the BIRCO benchmark, we find that models trained with our
method outperform RankZephyr and are competitive with RankLLama, both of which
are 7B parameter models trained on over 100K labels. These findings point to
the power of automatic prompt optimization for synthetic dataset generation.

摘要：我們開發了一種方法，可以用少至 10 個黃金相關標籤來訓練小規模（小於 100M 參數）神經資訊檢索模型。
該方法依賴於使用語言模型 (LM) 為文件生成合成查詢，關鍵步驟是我們根據訓練品質自動最佳化用於生成這些查詢的 LM 提示。
在使用 BIRCO 基準進行的實驗中，我們發現使用我們的方法訓練的模型優於 RankZephyr，並且與 RankLLama 競爭，而 RankZephyr 和 RankLLama 都是基於超過 100K 個標籤訓練的 7B 參數模型。
這些發現指出了自動提示最佳化對於合成資料集生成的力量。

##### **Nemotron-4 340B Technical Report**
2406.11704v1 by Nvidia, :, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu

We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,
Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open
access under the NVIDIA Open Model License Agreement, a permissive model
license that allows distribution, modification, and use of the models and its
outputs. These models perform competitively to open access models on a wide
range of evaluation benchmarks, and were sized to fit on a single DGX H100 with
8 GPUs when deployed in FP8 precision. We believe that the community can
benefit from these models in various research studies and commercial
applications, especially for generating synthetic data to train smaller
language models. Notably, over 98% of data used in our model alignment process
is synthetically generated, showcasing the effectiveness of these models in
generating synthetic data. To further support open research and facilitate
model development, we are also open-sourcing the synthetic data generation
pipeline used in our model alignment process.

摘要：我們發布 Nemotron-4 340B 模型系列，包括 Nemotron-4-340B-Base、
Nemotron-4-340B-Instruct 和 Nemotron-4-340B-Reward。我們的模型在 NVIDIA 開放模型授權協議下開放取用，這是一個寬鬆的模型授權，允許分發、修改和使用模型及其輸出。這些模型在廣泛的評估基準上執行與開放取用模型競爭，並且在部署 FP8 精度時大小適合單個配備 8 個 GPU 的 DGX H100。我們相信社群可以在各種研究和商業應用中受益於這些模型，特別是為了產生用於訓練較小語言模型的合成資料。值得注意的是，我們模型比對過程中使用的資料有超過 98% 是合成產生的，展示了這些模型在產生合成資料方面的效能。為了進一步支援開放研究和促進模型開發，我們也開放原始碼，在我們的模型比對過程中使用合成資料產生管道。

##### **Meta Reasoning for Large Language Models**
2406.11698v1 by Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, Furu Wei

We introduce Meta-Reasoning Prompting (MRP), a novel and efficient system
prompting method for large language models (LLMs) inspired by human
meta-reasoning. Traditional in-context learning-based reasoning techniques,
such as Tree-of-Thoughts, show promise but lack consistent state-of-the-art
performance across diverse tasks due to their specialized nature. MRP addresses
this limitation by guiding LLMs to dynamically select and apply different
reasoning methods based on the specific requirements of each task, optimizing
both performance and computational efficiency. With MRP, LLM reasoning operates
in two phases. Initially, the LLM identifies the most appropriate reasoning
method using task input cues and objective descriptions of available methods.
Subsequently, it applies the chosen method to complete the task. This dynamic
strategy mirrors human meta-reasoning, allowing the model to excel in a wide
range of problem domains. We evaluate the effectiveness of MRP through
comprehensive benchmarks. The results demonstrate that MRP achieves or
approaches state-of-the-art performance across diverse tasks. MRP represents a
significant advancement in enabling LLMs to identify cognitive challenges
across problems and leverage benefits across different reasoning approaches,
enhancing their ability to handle diverse and complex problem domains
efficiently. Every LLM deserves a Meta-Reasoning Prompting to unlock its full
potential and ensure adaptability in an ever-evolving landscape of challenges
and applications.

摘要：<paragraph>我們引入了元推理提示（MRP），這是一種新穎且有效率的系統提示方法，適用於大型語言模型（LLM），其靈感來自人類的元推理。傳統的基於情境學習的推理技術，例如思想樹，顯示出一定的潛力，但由於其專業性質，在不同的任務中缺乏一致的最新技術效能。MRP 透過引導 LLM 根據每個任務的特定需求動態選擇並應用不同的推理方法來解決這個限制，同時最佳化效能和運算效率。透過 MRP，LLM 推理分為兩個階段。最初，LLM 使用任務輸入提示和可用方法的客觀描述來識別最合適的推理方法。隨後，它應用所選方法來完成任務。這種動態策略反映了人類的元推理，讓模型能夠在廣泛的問題領域中表現出色。我們透過全面的基準測試來評估 MRP 的有效性。結果表明，MRP 在不同的任務中達到或接近最新技術效能。MRP 代表了讓 LLM 能夠識別不同問題的認知挑戰，並利用不同推理方法的優點的重大進展，進而增強它們有效處理不同且複雜問題領域的能力。每個 LLM 都應具備元推理提示，以發揮其全部潛力，並確保在不斷變化的挑戰和應用環境中適應。

##### **Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs**
2406.11695v1 by Krista Opsahl-Ong, Michael J Ryan, Josh Purtell, David Broman, Christopher Potts, Matei Zaharia, Omar Khattab

Language Model Programs, i.e. sophisticated pipelines of modular language
model (LM) calls, are increasingly advancing NLP tasks, but they require
crafting prompts that are jointly effective for all modules. We study prompt
optimization for LM programs, i.e. how to update these prompts to maximize a
downstream metric without access to module-level labels or gradients. To make
this tractable, we factorize our problem into optimizing the free-form
instructions and few-shot demonstrations of every module and introduce several
strategies to craft task-grounded instructions and navigate credit assignment
across modules. Our strategies include (i) program- and data-aware techniques
for proposing effective instructions, (ii) a stochastic mini-batch evaluation
function for learning a surrogate model of our objective, and (iii) a
meta-optimization procedure in which we refine how LMs construct proposals over
time. Using these insights we develop MIPRO, a novel optimizer that outperforms
baselines on five of six diverse LM programs using a best-in-class open-source
model (Llama-3-8B), by as high as 12.9% accuracy. We will release our new
optimizers and benchmark in DSPy at https://github.com/stanfordnlp/dspy

摘要：語言模型程式，即模組化語言模型 (LM) 呼叫的複雜管線，正日益提升 NLP 任務，但它們需要製作對所有模組都共同有效的提示。我們研究 LM 程式的提示最佳化，即如何在沒有模組層級標籤或梯度的狀況下更新這些提示，以最大化下游指標。為了使這個問題可處理，我們將問題分解為最佳化每一個模組的自由形式說明和少量示範，並引入多種策略來製作以任務為基礎的說明，並在模組間導航信用分配。我們的策略包括：(i) 提出有效說明的程式和資料感知技術，(ii) 學習目標替代模型的隨機小批次評估函數，以及 (iii) 我們在其中精進 LM 隨著時間推移建構提案方式的元最佳化程序。利用這些見解，我們開發了 MIPRO，一種新的最佳化器，它在使用最佳開放原始碼模型 (Llama-3-8B) 的六個不同的 LM 程式中的五個程式中優於基準，準確度提高了 12.9%。我們將在 https://github.com/stanfordnlp/dspy 上發布我們的新的最佳化器和 DSPy 基準

##### **Tokenization Falling Short: The Curse of Tokenization**
2406.11687v1 by Yekun Chai, Yewei Fang, Qiwei Peng, Xuhong Li

Language models typically tokenize raw text into sequences of subword
identifiers from a predefined vocabulary, a process inherently sensitive to
typographical errors, length variations, and largely oblivious to the internal
structure of tokens-issues we term the curse of tokenization. In this study, we
delve into these drawbacks and demonstrate that large language models (LLMs)
remain susceptible to these problems. This study systematically investigates
these challenges and their impact on LLMs through three critical research
questions: (1) complex problem solving, (2) token structure probing, and (3)
resilience to typographical variation. Our findings reveal that scaling model
parameters can mitigate the issue of tokenization; however, LLMs still suffer
from biases induced by typos and other text format variations. Our experiments
show that subword regularization such as BPE-dropout can mitigate this issue.
We will release our code and data to facilitate further research.

摘要：語言模型通常將原始文字分詞成預先定義詞彙表中子字詞的序列，這個過程對印刷錯誤、長度變化很敏感，而且在很大程度上忽略了詞彙的內部結構，我們稱這個問題為分詞的詛咒。在這項研究中，我們深入探討了這些缺點，並證明大型語言模型 (LLM) 仍然容易受到這些問題的影響。本研究系統性地探討了這些挑戰及其對 LLM 的影響，透過三個重要的研究問題：(1) 複雜問題解決，(2) 詞彙結構探測，以及 (3) 對印刷變異的韌性。我們的發現顯示，調整模型參數可以減輕分詞的問題；然而，LLM 仍然會受到印刷錯誤和其他文字格式變異所產生的偏差影響。我們的實驗顯示，子字詞正規化（例如 BPE-dropout）可以減輕這個問題。我們將釋出我們的程式碼和資料，以利進一步研究。

##### **HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing**
2406.11683v1 by Jing Chen, Xinyu Zhu, Cheng Yang, Chufan Shi, Yadong Xi, Yuxiang Zhang, Junjie Wang, Jiashu Pu, Rongsheng Zhang, Yujiu Yang, Tian Feng

Generative AI has demonstrated unprecedented creativity in the field of
computer vision, yet such phenomena have not been observed in natural language
processing. In particular, large language models (LLMs) can hardly produce
written works at the level of human experts due to the extremely high
complexity of literature writing. In this paper, we present HoLLMwood, an
automated framework for unleashing the creativity of LLMs and exploring their
potential in screenwriting, which is a highly demanding task. Mimicking the
human creative process, we assign LLMs to different roles involved in the
real-world scenario. In addition to the common practice of treating LLMs as
${Writer}$, we also apply LLMs as ${Editor}$, who is responsible for providing
feedback and revision advice to ${Writer}$. Besides, to enrich the characters
and deepen the plots, we introduce a role-playing mechanism and adopt LLMs as
${Actors}$ that can communicate and interact with each other. Evaluations on
automatically generated screenplays show that HoLLMwood substantially
outperforms strong baselines in terms of coherence, relevance, interestingness
and overall quality.

摘要：生成式 AI 已在電腦視覺領域展現出前所未有的創造力，但自然語言處理中尚未觀察到這種現象。特別是，由於文學寫作極高的複雜性，大型語言模型 (LLM) 難以產生人類專家等級的書面作品。在本文中，我們提出 HoLLMwood，一個自動化框架，用於釋放 LLM 的創造力並探索它們在編劇中的潛力，這是一項要求很高的任務。模仿人類的創造過程，我們將 LLM 分配到現實世界場景中涉及的不同角色。除了將 LLM 視為 ${Writer}$ 的常見做法外，我們還將 LLM 用作 ${Editor}，負責向 ${Writer}$ 提供回饋和修改建議。此外，為了豐富角色並加深劇情，我們引入角色扮演機制，並採用 LLM 作為 ${Actors}，它們可以相互溝通和互動。對自動生成的劇本進行評估表明，HoLLMwood 在連貫性、相關性、趣味性和整體品質方面明顯優於強大的基準。

##### **Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack**
2406.11682v1 by Shangqing Tu, Zhuoran Pan, Wenxuan Wang, Zhexin Zhang, Yuliang Sun, Jifan Yu, Hongning Wang, Lei Hou, Juanzi Li

Large language models (LLMs) have been increasingly applied to various
domains, which triggers increasing concerns about LLMs' safety on specialized
domains, e.g. medicine. However, testing the domain-specific safety of LLMs is
challenging due to the lack of domain knowledge-driven attacks in existing
benchmarks. To bridge this gap, we propose a new task, knowledge-to-jailbreak,
which aims to generate jailbreaks from domain knowledge to evaluate the safety
of LLMs when applied to those domains. We collect a large-scale dataset with
12,974 knowledge-jailbreak pairs and fine-tune a large language model as
jailbreak-generator, to produce domain knowledge-specific jailbreaks.
Experiments on 13 domains and 8 target LLMs demonstrate the effectiveness of
jailbreak-generator in generating jailbreaks that are both relevant to the
given knowledge and harmful to the target LLMs. We also apply our method to an
out-of-domain knowledge base, showing that jailbreak-generator can generate
jailbreaks that are comparable in harmfulness to those crafted by human
experts. Data and code: https://github.com/THU-KEG/Knowledge-to-Jailbreak/.

摘要：大型语言模型（LLM）已越来越多地应用于各种领域，这引发了人们对 LLM 在专业领域（例如医学）上的安全性日益担忧。然而，由于现有基准缺乏领域知识驱动的攻击，因此测试 LLM 的特定领域安全性具有挑战性。为了弥合这一差距，我们提出了一个新任务，即知识到越狱，其目的是从领域知识中生成越狱，以评估 LLM 应用于这些领域时的安全性。我们收集了一个包含 12,974 个知识越狱对的大规模数据集，并微调了一个大型语言模型作为越狱生成器，以产生特定于领域知识的越狱。在 13 个领域和 8 个目标 LLM 上的实验表明了越狱生成器在生成与给定知识相关且对目标 LLM 有害的越狱方面的有效性。我们还将我们的方法应用于域外知识库，表明越狱生成器可以生成与人类专家制作的越狱在危害性方面相当的越狱。数据和代码：https://github.com/THU-KEG/Knowledge-to-Jailbreak/。

##### **R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models**
2406.11681v1 by Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li

Large language models have achieved remarkable success on general NLP tasks,
but they may fall short for domain-specific problems. Recently, various
Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this
shortcoming. However, existing evaluation tools only provide a few baselines
and evaluate them on various domains without mining the depth of domain
knowledge. In this paper, we address the challenges of evaluating RALLMs by
introducing the R-Eval toolkit, a Python toolkit designed to streamline the
evaluation of different RAG workflows in conjunction with LLMs. Our toolkit,
which supports popular built-in RAG workflows and allows for the incorporation
of customized testing data on the specific domain, is designed to be
user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs
across three task levels and two representative domains, revealing significant
variations in the effectiveness of RALLMs across different tasks and domains.
Our analysis emphasizes the importance of considering both task and domain
requirements when choosing a RAG workflow and LLM combination. We are committed
to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval
to facilitate both the industry and the researchers.

摘要：大型語言模型在通用的 NLP 任務上取得了顯著的成功，
但它們在特定領域的問題上可能表現不佳。最近，提出了各種
檢索增強大型語言模型 (RALLM) 來解決這個
缺點。然而，現有的評估工具只提供了一些基準，
並在各種領域對它們進行評估，而沒有挖掘領域
知識的深度。在本文中，我們解決了評估 RALLM 的挑戰，
引入了 R-Eval 工具包，這是一個 Python 工具包，旨在簡化
評估不同的 RAG 工作流程以及 LLM。我們的工具包，
它支持流行的內建 RAG 工作流程，並允許在特定領域整合
自訂的測試數據，旨在成為使用者友善、模組化和可擴充的。我們對 21 個 RALLM
進行了評估，涵蓋三個任務層級和兩個代表性領域，揭示了 RALLM
在不同任務和領域中的有效性有顯著差異。
我們的分析強調了在選擇 RAG 工作流程和 LLM 組合時考慮任務和領域
需求的重要性。我們致力於持續維護我們的平台，網址為 https://github.com/THU-KEG/R-Eval，
以促進產業和研究人員。

##### **TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy**
2406.11678v1 by Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Daiting Shi, Jiaxin Mao, Dawei Yin

Large Language Models (LLMs) are increasingly employed in zero-shot documents
ranking, yielding commendable results. However, several significant challenges
still persist in LLMs for ranking: (1) LLMs are constrained by limited input
length, precluding them from processing a large number of documents
simultaneously; (2) The output document sequence is influenced by the input
order of documents, resulting in inconsistent ranking outcomes; (3) Achieving a
balance between cost and ranking performance is quite challenging. To tackle
these issues, we introduce a novel documents ranking method called TourRank,
which is inspired by the tournament mechanism. This approach alleviates the
impact of LLM's limited input length through intelligent grouping, while the
tournament-like points system ensures robust ranking, mitigating the influence
of the document input sequence. We test TourRank with different LLMs on the
TREC DL datasets and the BEIR benchmark. Experimental results show that
TourRank achieves state-of-the-art performance at a reasonable cost.

摘要：大型語言模型 (LLM)  zunehmend in Zero-Shot-Dokumenten eingesetzt
Ranking, was zu lobenswerten Ergebnissen führt. Allerdings bestehen bei LLMs für das Ranking immer noch mehrere erhebliche Herausforderungen: (1) LLMs sind durch eine begrenzte Eingabelänge eingeschränkt, was sie daran hindert, eine große Anzahl von Dokumenten gleichzeitig zu verarbeiten; (2) Die Ausgabe-Dokumentsequenz wird durch die Eingabereihenfolge der Dokumente beeinflusst, was zu inkonsistenten Ranking-Ergebnissen führt; (3) Es ist eine ziemliche Herausforderung, ein Gleichgewicht zwischen Kosten und Ranking-Leistung zu erreichen. Um diese Probleme anzugehen, stellen wir eine neuartige Dokumenten-Ranking-Methode namens TourRank vor, die vom Turniermechanismus inspiriert ist. Dieser Ansatz mildert die Auswirkungen der begrenzten Eingabelänge von LLM durch intelligente Gruppierung, während das turnierähnliche Punktesystem ein robustes Ranking gewährleistet und den Einfluss der Dokumenteneingabesequenz abschwächt. Wir testen TourRank mit verschiedenen LLMs auf den TREC DL-Datensätzen und dem BEIR-Benchmark. Experimentelle Ergebnisse zeigen, dass TourRank zu einem vernünftigen Preis eine hochmoderne Leistung erzielt.

##### **BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models**
2406.11675v1 by Yibin Wang, Haizhou Shi, Ligong Han, Dimitris Metaxas, Hao Wang

Large Language Models (LLMs) often suffer from overconfidence during
inference, particularly when adapted to downstream domain-specific tasks with
limited data. Previous work addresses this issue by employing approximate
Bayesian estimation after the LLMs are trained, enabling them to quantify
uncertainty. However, such post-training approaches' performance is severely
limited by the parameters learned during training. In this paper, we go beyond
post-training Bayesianization and propose Bayesian Low-Rank Adaptation by
Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both
the mean and covariance of LLM parameters throughout the whole fine-tuning
process. Our empirical results verify the effectiveness of BLoB in terms of
generalization and uncertainty estimation, when evaluated on both
in-distribution and out-of-distribution data.

摘要：大型語言模型 (LLM) 在推論期間經常過於自信，特別是在適應具有有限數據的下游特定領域任務時。先前的研究透過在 LLM 訓練後採用近似貝氏估計來解決此問題，使其能夠量化不確定性。然而，此類訓練後方法的效能受到訓練期間所學習參數的嚴重限制。在本文中，我們超越了訓練後的貝氏化，並提出透過反向傳播的貝氏低階適應 (BLoB)，這是一種演算法，可在整個微調過程中持續且同時調整 LLM 參數的平均值和共變異數。我們的實證結果驗證了 BLoB 在泛化和不確定性估計方面的效能，在分佈內和分佈外數據上進行評估時皆能驗證。

##### **Endor: Hardware-Friendly Sparse Format for Offloaded LLM Inference**
2406.11674v1 by Donghyeon Joo, Ramyad Hadidi, Soheil Feizi, Bahar Asgari

The increasing size of large language models (LLMs) challenges their usage on
resource-constrained platforms. For example, memory on modern GPUs is
insufficient to hold LLMs that are hundreds of Gigabytes in size. Offloading is
a popular method to escape this constraint by storing weights of an LLM model
to host CPU memory and SSD, then loading each weight to GPU before every use.
In our case study of offloaded inference, we found that due to the low
bandwidth between storage devices and GPU, the latency of transferring large
model weights from its offloaded location to GPU memory becomes the critical
bottleneck with actual compute taking nearly 0% of runtime. To effectively
reduce the weight transfer latency, we propose a novel sparse format that
compresses the unstructured sparse pattern of pruned LLM weights to non-zero
values with high compression ratio and low decompression overhead. Endor
achieves this by expressing the positions of non-zero elements with a bitmap.
Compared to offloaded inference using the popular Huggingface Accelerate,
applying Endor accelerates OPT-66B by 1.70x and Llama2-70B by 1.78x. When
direct weight transfer from SSD to GPU is leveraged, Endor achieves 2.25x
speedup on OPT-66B and 2.37x speedup on Llama2-70B.

摘要：大型语言模型 (LLM) 的尺寸越来越大，对资源受限平台上的使用构成挑战。例如，现代 GPU 上的内存不足以容纳数百 GB 大小的 LLM。卸载是一种流行的方法，可以通过将 LLM 模型的权重存储到主机 CPU 内存和 SSD 中来摆脱此限制，然后在每次使用之前将每个权重加载到 GPU 中。在我们的卸载推理案例研究中，我们发现由于存储设备和 GPU 之间的带宽较低，将大型模型权重从其卸载位置传输到 GPU 内存的延迟成为关键瓶颈，实际计算仅占运行时的 0%。为了有效减少权重传输延迟，我们提出了一种新颖的稀疏格式，该格式将修剪的 LLM 权重的非结构化稀疏模式压缩为非零值，具有高压缩比和低解压缩开销。Endor 通过使用位图表示非零元素的位置来实现这一点。与使用流行的 Huggingface Accelerate 进行卸载推理相比，应用 Endor 可将 OPT-66B 加速 1.70 倍，将 Llama2-70B 加速 1.78 倍。当利用从 SSD 到 GPU 的直接权重传输时，Endor 在 OPT-66B 上实现了 2.25 倍的加速，在 Llama2-70B 上实现了 2.37 倍的加速。

##### **Benchmarking of LLM Detection: Comparing Two Competing Approaches**
2406.11670v1 by Thorsten Pröhl, Erik Putzier, Rüdiger Zarnekow

This article gives an overview of the field of LLM text recognition.
Different approaches and implemented detectors for the recognition of
LLM-generated text are presented. In addition to discussing the
implementations, the article focuses on benchmarking the detectors. Although
there are numerous software products for the recognition of LLM-generated text,
with a focus on ChatGPT-like LLMs, the quality of the recognition (recognition
rate) is not clear. Furthermore, while it can be seen that scientific
contributions presenting their novel approaches strive for some kind of
comparison with other approaches, the construction and independence of the
evaluation dataset is often not comprehensible. As a result, discrepancies in
the performance evaluation of LLM detectors are often visible due to the
different benchmarking datasets. This article describes the creation of an
evaluation dataset and uses this dataset to investigate the different
detectors. The selected detectors are benchmarked against each other.

摘要：本文概述了 LLM 文本识别的领域。
介绍了用于识别 LLM 生成的文本的不同方法和已实现的检测器。除了讨论
实现之外，本文重点关注检测器的基准测试。虽然
有许多用于识别 LLM 生成的文本的软件产品，
专注于类似 ChatGPT 的 LLM，但识别质量（识别
率）尚不清楚。此外，虽然可以看出科学
贡献者在提出其新方法时努力与其他方法进行某种
比较，但评估数据集的构建和独立性通常难以理解。因此，由于
不同的基准数据集，LLM 检测器的性能评估中经常会出现差异。本文介绍了评估数据集的创建，并使用此数据集来调查不同的
检测器。选定的检测器相互进行基准测试。

##### **"Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak**
2406.11668v1 by Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, Xueqi Cheng

"Jailbreak" is a major safety concern of Large Language Models (LLMs), which
occurs when malicious prompts lead LLMs to produce harmful outputs, raising
issues about the reliability and safety of LLMs. Therefore, an effective
evaluation of jailbreaks is very crucial to develop its mitigation strategies.
However, our research reveals that many jailbreaks identified by current
evaluations may actually be hallucinations-erroneous outputs that are mistaken
for genuine safety breaches. This finding suggests that some perceived
vulnerabilities might not represent actual threats, indicating a need for more
precise red teaming benchmarks. To address this problem, we propose the
$\textbf{B}$enchmark for reli$\textbf{AB}$ilit$\textbf{Y}$ and
jail$\textbf{B}$reak ha$\textbf{L}$l$\textbf{U}$cination $\textbf{E}$valuation
(BabyBLUE). BabyBLUE introduces a specialized validation framework including
various evaluators to enhance existing jailbreak benchmarks, ensuring outputs
are useful malicious instructions. Additionally, BabyBLUE presents a new
dataset as an augmentation to the existing red teaming benchmarks, specifically
addressing hallucinations in jailbreaks, aiming to evaluate the true potential
of jailbroken LLM outputs to cause harm to human society.

摘要：「越獄」是大型語言模型（LLM）的主要安全問題，它發生在惡意提示導致 LLM 產生有害輸出時，引發了關於 LLM 的可靠性和安全性問題。因此，對越獄進行有效的評估對於制定其緩解策略至關重要。然而，我們的研究表明，當前評估中識別出的許多越獄實際上可能是幻覺——錯誤的輸出，被誤認為真正的安全漏洞。這一發現表明，一些感知到的漏洞可能並不能代表實際威脅，這表明需要更精確的紅隊基準。為了解決這個問題，我們提出了$\textbf{B}$enchmark for reli$\textbf{AB}$ilit$\textbf{Y}$ and jail$\textbf{B}$reak ha$\textbf{L}$l$\textbf{U}$cination $\textbf{E}$valuation（BabyBLUE）。BabyBLUE 引入了一個專門的驗證框架，包括各種評估器，以增強現有的越獄基準，確保輸出是有用的惡意指令。此外，BabyBLUE 還提供了一個新數據集作為對現有紅隊基準的擴充，特別針對越獄中的幻覺，旨在評估越獄 LLM 輸出對人類社會造成傷害的真正潛力。

##### **See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding**
2406.11665v1 by Amith Ananthram, Elias Stengel-Eskin, Carl Vondrick, Mohit Bansal, Kathleen McKeown

Vision-language models (VLMs) can respond to queries about images in many
languages. However, beyond language, culture affects how we see things. For
example, individuals from Western cultures focus more on the central figure in
an image while individuals from Eastern cultures attend more to scene context.
In this work, we present a novel investigation that demonstrates and localizes
VLMs' Western bias in image understanding. We evaluate large VLMs across
subjective and objective visual tasks with culturally diverse images and
annotations. We find that VLMs perform better on the Western subset than the
Eastern subset of each task. Controlled experimentation tracing the source of
this bias highlights the importance of a diverse language mix in text-only
pre-training for building equitable VLMs, even when inference is performed in
English. Moreover, while prompting in the language of a target culture can lead
to reductions in bias, it is not a substitute for building AI more
representative of the world's languages.

摘要：視覺語言模型 (VLM) 能以多種語言回答關於影像的查詢。然而，除了語言之外，文化也會影響我們看待事物的方式。例如，來自西方文化的人會更專注於影像中的中央人物，而來自東方文化的人則會更關注場景背景。在這項研究中，我們提出了一項新穎的研究，展示並定位了 VLM 在影像理解中的西方偏見。我們在具有文化多元性影像和註解的主觀和客觀視覺任務中評估了大型 VLM。我們發現，VLM 在每個任務的西方子集上表現優於東方子集。追蹤此偏見來源的受控實驗強調了在僅文字預訓練中使用多樣化語言組合對於建構公平的 VLM 的重要性，即使是在英語中執行推論時也是如此。此外，儘管使用目標文化的語言提示可以減少偏見，但它並不能取代建構更能代表世界語言的 AI。

##### **Cultural Conditioning or Placebo? On the Effectiveness of Socio-Demographic Prompting**
2406.11661v1 by Sagnik Mukherjee, Muhammad Farid Adilazuarda, Sunayana Sitaram, Kalika Bali, Alham Fikri Aji, Monojit Choudhury

Socio-demographic prompting is a commonly employed approach to study cultural
biases in LLMs as well as for aligning models to certain cultures. In this
paper, we systematically probe four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo
and GPT-4) with prompts that are conditioned on culturally sensitive and
non-sensitive cues, on datasets that are supposed to be culturally sensitive
(EtiCor and CALI) or neutral (MMLU and ETHICS). We observe that all models
except GPT-4 show significant variations in their responses on both kinds of
datasets for both kinds of prompts, casting doubt on the robustness of the
culturally-conditioned prompting as a method for eliciting cultural bias in
models or as an alignment strategy. The work also calls rethinking the control
experiment design to tease apart the cultural conditioning of responses from
"placebo effect", i.e., random perturbations of model responses due to
arbitrary tokens in the prompt.

摘要：社會人口提示是一種廣泛採用的方法，用於研究 LLM 中的文化偏見，以及將模型與特定文化對齊。在本文中，我們系統性地探查了四個 LLM（Llama 3、Mistral v0.2、GPT-3.5 Turbo 和 GPT-4），提示它們以文化敏感和不敏感的線索為條件，在假設為文化敏感（EtiCor 和 CALI）或中立（MMLU 和 ETHICS）的數據集上。我們觀察到除了 GPT-4 之外，所有模型在兩種提示的兩種數據集上的回應都出現顯著差異，這對文化條件提示作為引發模型中文化偏見的方法或作為對齊策略的穩健性提出質疑。這項工作也要求重新思考對照實驗設計，以區分回應的文化條件和「安慰劑效應」，即提示中任意符號導致模型回應的隨機擾動。

##### **Can LLM be a Personalized Judge?**
2406.11657v1 by Yijiang River Dong, Tiancheng Hu, Nigel Collier

Ensuring that large language models (LLMs) reflect diverse user values and
preferences is crucial as their user bases expand globally. It is therefore
encouraging to see the growing interest in LLM personalization within the
research community. However, current works often rely on the LLM-as-a-Judge
approach for evaluation without thoroughly examining its validity. In this
paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking
LLMs to judge user preferences based on personas. Our findings suggest that
directly applying LLM-as-a-Personalized-Judge is less reliable than previously
assumed, showing low and inconsistent agreement with human ground truth. The
personas typically used are often overly simplistic, resulting in low
predictive power. To address these issues, we introduce verbal uncertainty
estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to
express low confidence on uncertain judgments. This adjustment leads to much
higher agreement (above 80%) on high-certainty samples for binary tasks.
Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves
comparable performance to third-party humans evaluation and even surpasses
human performance on high-certainty samples. Our work indicates that
certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for
developing more reliable and scalable methods for evaluating LLM
personalization.

摘要：確保大型語言模型 (LLM) 能反映多樣化的使用者價值觀和偏好，隨著其使用者基礎在全球擴展，這點至關重要。因此，看到研究社群對 LLM 個人化越來越感興趣，令人感到振奮。然而，目前的著作常依賴 LLM 作為評估的評判者，而未徹底檢視其有效性。在本論文中，我們調查了 LLM 作為個人化評判者的可靠性，要求 LLM 根據角色來判斷使用者的偏好。我們的研究結果顯示，直接應用 LLM 作為個人化評判者，其可靠性低於先前的假設，與人類的真實情況顯示出低且不一致的相符性。通常所使用的角色往往過於簡化，導致預測能力低。為了解決這些問題，我們在 LLM 作為個人化評判者的管線中引入了口頭不確定性評估，讓模型能夠對不確定的判斷表達低信心。此調整導致二元任務中對高確定性樣本的相符性大幅提升 (超過 80%)。透過人為評估，我們發現 LLM 作為個人化評判者達到了與第三方人類評估相當的表現，甚至在高確定性樣本上超越了人類表現。我們的研究指出，確定性增強的 LLM 作為個人化評判者為開發更可靠且可擴充的方法來評估 LLM 個人化提供了一個有前途的方向。

##### **A Two-dimensional Zero-shot Dialogue State Tracking Evaluation Method using GPT-4**
2406.11651v1 by Ming Gu, Yan Yang

Dialogue state tracking (DST) is evaluated by exact matching methods, which
rely on large amounts of labeled data and ignore semantic consistency, leading
to over-evaluation. Currently, leveraging large language models (LLM) in
evaluating natural language processing tasks has achieved promising results.
However, using LLM for DST evaluation is still under explored. In this paper,
we propose a two-dimensional zero-shot evaluation method for DST using GPT-4,
which divides the evaluation into two dimensions: accuracy and completeness.
Furthermore, we also design two manual reasoning paths in prompting to further
improve the accuracy of evaluation. Experimental results show that our method
achieves better performance compared to the baselines, and is consistent with
traditional exact matching based methods.

摘要：對話狀態追蹤 (DST) 是由精確比對方法評估，它依賴大量的標籤資料，並忽略語意一致性，這導致評估過高。目前，利用大型語言模型 (LLM) 來評估自然語言處理任務已取得令人滿意的結果。然而，使用 LLM 進行 DST 評估仍處於探索階段。在本文中，我們提出了一個使用 GPT-4 的二維零次評估方法，將評估分為兩個面向：準確性和完整性。此外，我們還設計了兩個手動推理路徑，以進一步提高評估的準確性。實驗結果表明，與基線相比，我們的評估方法表現更好，並且與傳統的基於精確比對的方法一致。

##### **YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection**
2406.11641v1 by Tamara R. Lenhard, Andreas Weinmann, Stefan Jäger, Tobias Koch

Predominant methods for image-based drone detection frequently rely on
employing generic object detection algorithms like YOLOv5. While proficient in
identifying drones against homogeneous backgrounds, these algorithms often
struggle in complex, highly textured environments. In such scenarios, drones
seamlessly integrate into the background, creating camouflage effects that
adversely affect the detection quality. To address this issue, we introduce a
novel deep learning architecture called YOLO-FEDER FusionNet. Unlike
conventional approaches, YOLO-FEDER FusionNet combines generic object detection
methods with the specialized strength of camouflage object detection techniques
to enhance drone detection capabilities. Comprehensive evaluations of
YOLO-FEDER FusionNet show the efficiency of the proposed model and demonstrate
substantial improvements in both reducing missed detections and false alarms.

摘要：基於影像的無人機偵測的常見方法經常依賴使用 YOLOv5 等通用物件偵測演算法。這些演算法雖然精於在均勻背景中辨識無人機，但在複雜、紋理豐富的環境中卻常常力不從心。在這種情況下，無人機能無縫融入背景，產生偽裝效果，對偵測品質造成負面影響。為了解決這個問題，我們引進一種稱為 YOLO-FEDER FusionNet 的創新深度學習架構。與傳統方法不同，YOLO-FEDER FusionNet 結合了通用物件偵測方法與偽裝物件偵測技術的專門優勢，以增強無人機偵測能力。對 YOLO-FEDER FusionNet 的全面評估顯示了所提議模型的效率，並證明在減少漏偵測和誤報方面都有顯著的改善。

##### **Linear Bellman Completeness Suffices for Efficient Online Reinforcement Learning with Few Actions**
2406.11640v1 by Noah Golowich, Ankur Moitra

One of the most natural approaches to reinforcement learning (RL) with
function approximation is value iteration, which inductively generates
approximations to the optimal value function by solving a sequence of
regression problems. To ensure the success of value iteration, it is typically
assumed that Bellman completeness holds, which ensures that these regression
problems are well-specified. We study the problem of learning an optimal policy
under Bellman completeness in the online model of RL with linear function
approximation. In the linear setting, while statistically efficient algorithms
are known under Bellman completeness (e.g., Jiang et al. (2017); Zanette et al.
(2020)), these algorithms all rely on the principle of global optimism which
requires solving a nonconvex optimization problem. In particular, it has
remained open as to whether computationally efficient algorithms exist. In this
paper we give the first polynomial-time algorithm for RL under linear Bellman
completeness when the number of actions is any constant.

摘要：在使用函數逼近進行強化學習 (RL) 時，最自然的其中一種方法是價值迭代，它透過解一系列回歸問題，以歸納方式產生對最佳價值函數的逼近。為了確保價值迭代的成功，通常假設貝爾曼完備性成立，這確保了這些回歸問題有明確的規範。我們研究在線 RL 模型中，在貝爾曼完備性下學習最佳策略的問題，並使用線性函數逼近。在線性設定中，雖然在貝爾曼完備性下已知統計上有效率的演算法（例如，Jiang 等人 (2017)；Zanette 等人 (2020)），但這些演算法都依賴於整體樂觀原則，這需要解一個非凸最佳化問題。特別是，計算上有效率的演算法是否存在，仍然是一個未解的問題。在本文中，我們給出了在線性貝爾曼完備性下 RL 的第一個多項式時間演算法，其中動作數量為任意常數。

##### **MASAI: Modular Architecture for Software-engineering AI Agents**
2406.11638v1 by Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, Nagarajan Natarajan

A common method to solve complex problems in software engineering, is to
divide the problem into multiple sub-problems. Inspired by this, we propose a
Modular Architecture for Software-engineering AI (MASAI) agents, where
different LLM-powered sub-agents are instantiated with well-defined objectives
and strategies tuned to achieve those objectives. Our modular architecture
offers several advantages: (1) employing and tuning different problem-solving
strategies across sub-agents, (2) enabling sub-agents to gather information
from different sources scattered throughout a repository, and (3) avoiding
unnecessarily long trajectories which inflate costs and add extraneous context.
MASAI enabled us to achieve the highest performance (28.33% resolution rate) on
the popular and highly challenging SWE-bench Lite dataset consisting of 300
GitHub issues from 11 Python repositories. We conduct a comprehensive
evaluation of MASAI relative to other agentic methods and analyze the effects
of our design decisions and their contribution to the success of MASAI.

摘要：在軟體工程中，解決複雜問題的一種常見方法，是將問題分解成多個子問題。受到此啟發，我們提出軟體工程 AI（MASAI）代理模組化架構，其中不同的 LLM 驅動子代理會以明確的目標和策略實例化，以達成這些目標。我們的模組化架構提供了多項優點：(1) 採用和調整跨子代理的不同問題解決策略，(2) 讓子代理從散佈在整個儲存庫的不同來源收集資訊，以及 (3) 避免不必要的長軌跡，這會增加成本並增加無關的內容。MASAI 讓我們在由來自 11 個 Python 儲存庫的 300 個 GitHub 問題組成的熱門且極具挑戰性的 SWE-bench Lite 資料集上，達到最高效能（28.33% 解析率）。我們對 MASAI 相對於其他代理方法進行全面評估，並分析我們的設計決策及其對 MASAI 成功所做的貢獻。

##### **The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance**
2406.11634v1 by Kyle Moore, Jesse Roberts, Thao Pham, Oseremhen Ewaleifoh, Doug Fisher

Cloze testing is a common method for measuring the behavior of large language
models on a number of benchmark tasks. Using the MMLU dataset, we show that the
base-rate probability (BRP) differences across answer tokens are significant
and affect task performance ie. guess A if uncertain. We find that
counterfactual prompting does sufficiently mitigate the BRP effect. The BRP
effect is found to have a similar effect to test taking strategies employed by
humans leading to the conflation of task performance and test-taking ability.
We propose the Nvr-X-MMLU task, a variation of MMLU, which helps to
disambiguate test-taking ability from task performance and reports the latter.

摘要：完形填空測試是一種常見的方法，用於衡量大型語言模型在許多基準任務上的行為。使用 MMLU 資料集，我們展示了答案符號之間的基礎機率 (BRP) 差異具有顯著意義，並影響任務執行，例如在不確定時猜測 A。我們發現反事實提示確實可以充分減輕 BRP 效應。發現 BRP 效應對人類採用的應試策略具有類似的效應，導致任務執行和應試能力混淆。我們提出了 Nvr-X-MMLU 任務，這是 MMLU 的一種變體，有助於消除應試能力與任務執行之間的歧義，並報告後者。

##### **Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation**
2406.11632v1 by Boxuan Lyu, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura

Maximum a posteriori decoding, a commonly used method for neural machine
translation (NMT), aims to maximize the estimated posterior probability.
However, high estimated probability does not always lead to high translation
quality. Minimum Bayes Risk (MBR) decoding offers an alternative by seeking
hypotheses with the highest expected utility.
  In this work, we show that Quality Estimation (QE) reranking, which uses a QE
model as a reranker, can be viewed as a variant of MBR. Inspired by this, we
propose source-based MBR (sMBR) decoding, a novel approach that utilizes
synthetic sources generated by backward translation as ``support hypotheses''
and a reference-free quality estimation metric as the utility function, marking
the first work to solely use sources in MBR decoding. Experiments show that
sMBR significantly outperforms QE reranking and is competitive with standard
MBR decoding. Furthermore, sMBR calls the utility function fewer times compared
to MBR. Our findings suggest that sMBR is a promising approach for high-quality
NMT decoding.

摘要：最大後驗解碼是神經機器翻譯（NMT）中常用的方法，旨在最大化估計後驗機率。
然而，高估計機率並不總是能帶來高翻譯品質。最小貝氏風險（MBR）解碼提供了一種替代方案，方法是尋找具有最高預期效用的假設。
在這項工作中，我們說明品質評估（QE）重新排序（使用 QE 模型作為重新排序器）可以視為 MBR 的一種變體。受到此啟發，我們提出基於來源的 MBR（sMBR）解碼，一種新穎的方法，利用反向翻譯產生的合成來源作為「支援假設」，並使用無參考品質評估指標作為效用函數，標示出僅在 MBR 解碼中使用來源的第一項工作。實驗顯示，sMBR 明顯優於 QE 重新排序，並且與標準 MBR 解碼具有競爭力。此外，與 MBR 相比，sMBR 呼叫效用函數的次數較少。我們的發現表明，sMBR 是一種有望用於高品質 NMT 解碼的方法。

##### **Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!**
2406.11629v1 by Mingyang Song, Mao Zheng, Xuan Luo

Leveraging Large Language Models (LLMs) as judges for evaluating the
performance of LLMs has recently garnered attention. Nonetheless, this type of
approach concurrently introduces potential biases from LLMs, raising concerns
about the reliability of the evaluation results. To mitigate this issue, we
propose and study two versions of many-shot in-context prompts, Reinforced and
Unsupervised ICL, for helping GPT-4o-as-a-Judge in single answer grading. Based
on the designed prompts, we investigate the impact of scaling the number of
in-context examples on the agreement and quality of the evaluation.
Furthermore, we first reveal the symbol bias in GPT-4o-as-a-Judge for pairwise
comparison and then propose a simple yet effective approach to mitigate it.
Experimental results show that advanced long-context LLMs, such as GPT-4o,
perform better in the many-shot regime than in the zero-shot regime. Meanwhile,
the experimental results further verify the effectiveness of the symbol bias
mitigation approach.

摘要：利用大型语言模型 (LLM) 作为评委来评估 LLM 的性能最近引起了关注。尽管如此，这种类型的评估同时引入了 LLM 的潜在偏差，引起了人们对评估结果可靠性的担忧。为了缓解这个问题，我们提出了两种版本的多镜头上下文提示，强化和无监督 ICL，以帮助 GPT-4o-as-a-Judge 进行单一答案评分。基于设计的提示，我们研究了扩展上下文示例数量对评估的一致性和质量的影响。此外，我们首先揭示了 GPT-4o-as-a-Judge 在成对比较中的符号偏差，然后提出了一种简单但有效的方法来缓解它。实验结果表明，先进的长上下文 LLM，例如 GPT-4o，在多镜头模式下的表现优于零镜头模式。同时，实验结果进一步验证了符号偏差缓解方法的有效性。

##### **Words in Motion: Representation Engineering for Motion Forecasting**
2406.11624v1 by Omer Sahin Tas, Royden Wagner

Motion forecasting transforms sequences of past movements and environment
context into future motion. Recent methods rely on learned representations,
resulting in hidden states that are difficult to interpret. In this work, we
use natural language to quantize motion features in a human-interpretable way,
and measure the degree to which they are embedded in hidden states. Our
experiments reveal that hidden states of motion sequences are arranged with
respect to our discrete sets of motion features. Following these insights, we
fit control vectors to motion features, which allow for controlling motion
forecasts at inference. Consequently, our method enables controlling
transformer-based motion forecasting models with textual inputs, providing a
unique interface to interact with and understand these models. Our
implementation is available at https://github.com/kit-mrt/future-motion

摘要：動作預測將過去動作和環境背景序列轉換為未來動作。最近的方法依賴於學習表徵，導致難以解釋的隱藏狀態。在這項工作中，我們使用自然語言以人類可解釋的方式量化動作特徵，並衡量它們嵌入隱藏狀態的程度。我們的實驗表明，動作序列的隱藏狀態根據我們離散的動作特徵集進行排列。根據這些見解，我們將控制向量擬合到動作特徵，這允許在推理時控制動作預測。因此，我們的方使我們能夠使用文本輸入控制基於轉換器的動作預測模型，提供了一個與這些模型互動和理解的獨特介面。我們的實作可在 https://github.com/kit-mrt/future-motion 取得

##### **Building Knowledge-Guided Lexica to Model Cultural Variation**
2406.11622v1 by Shreya Havaldar, Salvatore Giorgi, Sunny Rai, Thomas Talhelm, Sharath Chandra Guntuku, Lyle Ungar

Cultural variation exists between nations (e.g., the United States vs.
China), but also within regions (e.g., California vs. Texas, Los Angeles vs.
San Francisco). Measuring this regional cultural variation can illuminate how
and why people think and behave differently. Historically, it has been
difficult to computationally model cultural variation due to a lack of training
data and scalability constraints. In this work, we introduce a new research
problem for the NLP community: How do we measure variation in cultural
constructs across regions using language? We then provide a scalable solution:
building knowledge-guided lexica to model cultural variation, encouraging
future work at the intersection of NLP and cultural understanding. We also
highlight modern LLMs' failure to measure cultural variation or generate
culturally varied language.

摘要：文化差異存在於國家之間（例如美國與中國），也存在於地區之間（例如加州與德州，洛杉磯與舊金山）。衡量這種區域文化差異可以闡明人們思考和行為方式不同的原因和方式。由於缺乏訓練數據和可擴充性限制，在歷史上，計算文化差異一直很困難。在這項工作中，我們為 NLP 社群引入了一個新的研究問題：我們如何使用語言來衡量不同地區文化建構的差異？然後我們提供一個可擴充的解決方案：建構知識引導的詞彙來建模文化差異，鼓勵在 NLP 和文化理解的交集中進行未來的研究。我們也強調現代 LLM 無法衡量文化差異或產生文化多樣化的語言。

##### **DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling**
2406.11617v1 by Pala Tej Deep, Rishabh Bhardwaj, Soujanya Poria

With the proliferation of domain-specific models, model merging has emerged
as a set of techniques that combine the capabilities of multiple models into
one that can multitask without the cost of additional training. In this paper,
we propose a new model merging technique, Drop and rEscaLe via sampLing with
mAgnitude (DELLA-Merging), that employs a novel pruning technique, MAGPRUNE,
which shows significant advantages over DARE and TIES. MAGPRUNE first ranks the
parameters in order of their magnitude and assigns higher dropout probabilities
(p) to parameters with lower ranks corresponding to lower magnitudes. To
approximate the original embeddings, MAGPRUNE employs a rescaling operation on
the parameters that survive the random dropping by 1/(1 - p). On three
different expert models considered for merging (LM, Math, Code) and
corresponding benchmark datasets (AlpacaEval, GSM8K, MBPP), DELLA shows an
average improvement of 2.4 points over baseline methods employing delta
parameter pruning (an improvement of 3.6 points over TIES, 1.2 points over
DARE), and 11.1 points over the no-pruning baseline (TA). We release the source
code at: https://github.com/declare-lab/della.

摘要：隨著特定領域模型的激增，模型合併已成為一組技術，它將多個模型的功能結合到一個模型中，而無需額外訓練成本即可執行多任務。在本文中，我們提出了一種新的模型合併技術，即通過具有幅度的採樣進行刪除和縮放 (DELLA-Merging)，它採用了一種新穎的剪枝技術 MAGPRUNE，與 DARE 和 TIES 相比，它顯示出顯著的優勢。MAGPRUNE 首先按幅度對參數進行排序，並將較高的中斷機率 (p) 分配給幅度較低、對應於較低幅度的參數。為了近似原始嵌入，MAGPRUNE 對通過 1/(1 - p) 隨機刪除而存活的參數採用縮放操作。對於合併考慮的三個不同的專家模型 (LM、Math、Code) 和對應的基準資料集 (AlpacaEval、GSM8K、MBPP)，DELLA 顯示比採用 delta 參數剪枝的基本方法平均提高 2.4 分（比 TIES 提高 3.6 分，比 DARE 提高 1.2 分），並且比不剪枝的基本線 (TA) 高 11.1 分。我們在以下位置釋出原始碼：https://github.com/declare-lab/della。

##### **Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces**
2406.11614v1 by Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang, Mor Geva

The task of "unlearning" certain concepts in large language models (LLMs) has
attracted immense attention recently, due to its importance for mitigating
undesirable model behaviours, such as the generation of harmful, private, or
incorrect information. Current protocols to evaluate unlearning methods largely
rely on behavioral tests, without monitoring the presence of unlearned
knowledge within the model's parameters. This residual knowledge can be
adversarially exploited to recover the erased information post-unlearning. We
argue that unlearning should also be evaluated internally, by considering
changes in the parametric knowledge traces of the unlearned concepts. To this
end, we propose a general methodology for eliciting directions in the parameter
space (termed "concept vectors") that encode concrete concepts, and construct
ConceptVectors, a benchmark dataset containing hundreds of common concepts and
their parametric knowledge traces within two open-source LLMs. Evaluation on
ConceptVectors shows that existing unlearning methods minimally impact concept
vectors, while directly ablating these vectors demonstrably removes the
associated knowledge from the LLMs and significantly reduces their
susceptibility to adversarial manipulation. Our results highlight limitations
in behavioral-based unlearning evaluations and call for future work to include
parametric-based evaluations. To support this, we release our code and
benchmark at https://github.com/yihuaihong/ConceptVectors.

摘要：「取消學習」大型語言模型 (LLM) 中某些概念的任務最近備受關注，因為這對於減輕不良的模型行為（例如產生有害、私人或不正確的資訊）非常重要。目前用於評估取消學習方法的協定主要依賴行為測試，而不會監控模型參數中未學習知識的存在。這種殘留知識可以被對手利用，以便在取消學習後恢復已刪除的資訊。我們認為取消學習也應該在內部進行評估，方法是考慮未學習概念的參數化知識軌跡的變化。為此，我們提出了一種通用方法，用於在參數空間中引出編碼具體概念的方向（稱為「概念向量」），並建構 ConceptVectors，這是一個基準資料集，包含數百個常見概念及其在兩個開源 LLM 中的參數化知識軌跡。對 ConceptVectors 的評估顯示，現有的取消學習方法對概念向量影響最小，而直接消融這些向量則明顯從 LLM 中移除了相關知識，並大幅降低了它們對對手操縱的敏感性。我們的結果突出了基於行為的取消學習評估的限制，並呼籲未來的研究納入基於參數的評估。為了支持這一點，我們在 https://github.com/yihuaihong/ConceptVectors 上釋出我們的程式碼和基準。

##### **Long Code Arena: a Set of Benchmarks for Long-Context Code Models**
2406.11612v1 by Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, Timofey Bryksin

Nowadays, the fields of code and natural language processing are evolving
rapidly. In particular, models become better at processing long context windows
- supported context sizes have increased by orders of magnitude over the last
few years. However, there is a shortage of benchmarks for code processing that
go beyond a single file of context, while the most popular ones are limited to
a single method. With this work, we aim to close this gap by introducing Long
Code Arena, a suite of six benchmarks for code processing tasks that require
project-wide context. These tasks cover different aspects of code processing:
library-based code generation, CI builds repair, project-level code completion,
commit message generation, bug localization, and module summarization. For each
task, we provide a manually verified dataset for testing, an evaluation suite,
and open-source baseline solutions based on popular LLMs to showcase the usage
of the dataset and to simplify adoption by other researchers. We publish the
benchmark page on HuggingFace Spaces with the leaderboard, links to HuggingFace
Hub for all the datasets, and link to the GitHub repository with baselines:
https://huggingface.co/spaces/JetBrains-Research/long-code-arena.

摘要：如今，代码和自然语言处理领域正在快速发展。尤其是，模型在处理长上下文窗口方面变得更好，在过去几年中，支持的上下文大小已经增加了几个数量级。然而，对于超出单个上下文文件的代码处理基准却很匮乏，而最流行的基准仅限于单个方法。通过这项工作，我们旨在通过引入 Long Code Arena 来弥补这一差距，这是一套针对需要项目范围上下文的代码处理任务的六个基准。这些任务涵盖了代码处理的不同方面：基于库的代码生成、CI 构建修复、项目级代码完成、提交消息生成、错误定位和模块摘要。对于每个任务，我们提供一个手动验证的数据集用于测试、一个评估套件和基于流行 LLM 的开源基线解决方案，以展示数据集的使用并简化其他研究人员的采用。我们在 HuggingFace Spaces 上发布基准页面，其中包含排行榜、指向所有数据集的 HuggingFace Hub 的链接，以及指向带有基线的 GitHub 存储库的链接：https://huggingface.co/spaces/JetBrains-Research/long-code-arena。

##### **Understanding "Democratization" in NLP and ML Research**
2406.11598v1 by Arjun Subramonian, Vagrant Gautam, Dietrich Klakow, Zeerak Talat

Recent improvements in natural language processing (NLP) and machine learning
(ML) and increased mainstream adoption have led to researchers frequently
discussing the "democratization" of artificial intelligence. In this paper, we
seek to clarify how democratization is understood in NLP and ML publications,
through large-scale mixed-methods analyses of papers using the keyword
"democra*" published in NLP and adjacent venues. We find that democratization
is most frequently used to convey (ease of) access to or use of technologies,
without meaningfully engaging with theories of democratization, while research
using other invocations of "democra*" tends to be grounded in theories of
deliberation and debate. Based on our findings, we call for researchers to
enrich their use of the term democratization with appropriate theory, towards
democratic technologies beyond superficial access.

摘要：<paragraph>自然語言處理 (NLP) 和機器學習 (ML) 近期的進展，以及主流應用的增加，已導致研究人員頻繁地討論人工智慧的「民主化」。在本文中，我們試圖釐清在 NLP 和 ML 出版物中，民主化是如何被理解的，透過使用關鍵字「democra*」在 NLP 和相關領域發表的論文進行大規模的混合方法分析。我們發現民主化最常被用來傳達（容易）取得或使用技術，而沒有有意義地探討民主化理論，而使用其他「democra*」的呼籲的研究則傾向於以審議和辯論的理論為基礎。根據我們的發現，我們呼籲研究人員以適當的理論豐富他們對民主化一詞的使用，朝向超越表面取得的民主技術。</paragraph>

##### **CoSQA+: Enhancing Code Search Dataset with Matching Code**
2406.11589v1 by Jing Gong, Yanghui Wu, Linxi Liang, Zibin Zheng, Yanlin Wang

Semantic code search, retrieving code that matches a given natural language
query, is an important task to improve productivity in software engineering.
Existing code search datasets are problematic: either using unrealistic
queries, or with mismatched codes, and typically using one-to-one query-code
pairing, which fails to reflect the reality that a query might have multiple
valid code matches. This paper introduces CoSQA+, pairing high-quality queries
(reused from CoSQA) with multiple suitable codes. We collect code candidates
from diverse sources and form candidate pairs by pairing queries with these
codes. Utilizing the power of large language models (LLMs), we automate pair
annotation, filtering, and code generation for queries without suitable
matches. Through extensive experiments, CoSQA+ has demonstrated superior
quality over CoSQA. Models trained on CoSQA+ exhibit improved performance.
Furthermore, we propose a new metric Mean Multi-choice Reciprocal Rank (MMRR),
to assess one-to-N code search performance. We provide the code and data at
https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.

摘要：語意程式碼搜尋，擷取與給定的自然語言查詢相符的程式碼，是提升軟體工程生產力的重要任務。
現有的程式碼搜尋資料集存在問題：不是使用不切實際的查詢，就是程式碼不匹配，而且通常使用一對一的查詢程式碼配對，無法反映查詢可能有多個有效程式碼配對的現實。本文介紹 CoSQA+，將高品質查詢（重新使用自 CoSQA）與多個合適的程式碼配對。我們從不同的來源收集程式碼候選項，並透過將查詢與這些程式碼配對來形成候選配對。利用大型語言模型 (LLM) 的功能，我們自動執行配對註解、篩選，以及為沒有合適配對的查詢產生程式碼。透過廣泛的實驗，CoSQA+ 已證明其品質優於 CoSQA。在 CoSQA+ 上訓練的模型表現出更好的效能。此外，我們提出一個新的指標平均多選倒數排名 (MMRR)，用於評估一對多程式碼搜尋效能。我們在 https://github.com/DeepSoftwareAnalytics/CoSQA_Plus 提供程式碼和資料。

##### **Style Transfer with Multi-iteration Preference Optimization**
2406.11581v1 by Shuai Liu, Jonathan May

Numerous recent techniques for text style transfer characterize their
approaches as variants of reinforcement learning and preference optimization.
In this work, we consider the relationship between these approaches and a class
of optimization approaches developed primarily for (non-neural) statistical
machine translation, formerly known as 'tuning'. Inspired by these techniques
from the past, we improve upon established preference optimization approaches,
incorporating multiple iterations of exploration and optimization, and choosing
contrastive examples by following a 'hope' vs 'fear' sampling strategy.
Cognizant of the difference between machine translation and style transfer,
however, we further tailor our framework with a new pseudo-parallel generation
method and a dynamic weighted reward aggregation method to tackle the lack of
parallel data and the need for a multi-objective reward. We evaluate our model
on two commonly used text style transfer datasets. Through automatic and human
evaluation results we show the effectiveness and the superiority of our model
compared to state-of-the-art baselines.

摘要：最近許多文本樣式轉移技術將其方法描述為強化學習和偏好最佳化的變體。在這項工作中，我們考慮這些方法與主要為（非神經）統計機器翻譯開發的一類最佳化方法之間的關係，以前稱為「調整」。受到過去這些技術的啟發，我們改進既有的偏好最佳化方法，納入多重探索和最佳化的反覆運算，並遵循「希望」與「恐懼」取樣策略來選擇對比範例。然而，我們認知到機器翻譯與樣式轉移之間的差異，進一步透過新的偽平行產生方法和動態加權回饋彙總方法調整我們的架構，以解決缺乏平行資料和需要多目標回饋的問題。我們在兩個常用的文本樣式轉移資料集上評估我們的模型。透過自動和人工評估結果，我們展示了我們的模型與最先進的基準相比的有效性和優越性。

##### **Error Span Annotation: A Balanced Approach for Human Evaluation of Machine Translation**
2406.11580v1 by Tom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popović, Mrinmaya Sachan, Mariya Shmatova

High-quality Machine Translation (MT) evaluation relies heavily on human
judgments. Comprehensive error classification methods, such as Multidimensional
Quality Metrics (MQM), are expensive as they are time-consuming and can only be
done by experts, whose availability may be limited especially for low-resource
languages. On the other hand, just assigning overall scores, like Direct
Assessment (DA), is simpler and faster and can be done by translators of any
level, but are less reliable. In this paper, we introduce Error Span Annotation
(ESA), a human evaluation protocol which combines the continuous rating of DA
with the high-level error severity span marking of MQM. We validate ESA by
comparing it to MQM and DA for 12 MT systems and one human reference
translation (English to German) from WMT23. The results show that ESA offers
faster and cheaper annotations than MQM at the same quality level, without the
requirement of expensive MQM experts.

摘要：高品質機器翻譯 (MT) 評量高度依賴於人類的判斷。全面的錯誤分類方法（例如多維度品質指標 (MQM)）成本高昂，因為它們耗時且只能由專家執行，而專家的可用性可能有限，特別是對於低資源語言。另一方面，僅指定整體分數（例如直接評量 (DA)）較為簡單且快速，且可由任何層級的翻譯人員執行，但可靠性較低。在本文中，我們介紹錯誤範圍標註 (ESA)，這是一種人類評量協定，結合 DA 的連續評分與 MQM 的高層級錯誤嚴重性範圍標記。我們透過比較 ESA 與 MQM 和 DA 對 12 個機器翻譯系統和一個人類參考翻譯（英譯德）從 WMT23 驗證 ESA。結果顯示，ESA 提供比 MQM 更快速且更便宜的標註，品質水準相同，且不需要昂貴的 MQM 專家。

##### **Mathematical Entities: Corpora and Benchmarks**
2406.11577v1 by Jacob Collard, Valeria de Paiva, Eswaran Subrahmanian

Mathematics is a highly specialized domain with its own unique set of
challenges. Despite this, there has been relatively little research on natural
language processing for mathematical texts, and there are few mathematical
language resources aimed at NLP. In this paper, we aim to provide annotated
corpora that can be used to study the language of mathematics in different
contexts, ranging from fundamental concepts found in textbooks to advanced
research mathematics. We preprocess the corpora with a neural parsing model and
some manual intervention to provide part-of-speech tags, lemmas, and dependency
trees. In total, we provide 182397 sentences across three corpora. We then aim
to test and evaluate several noteworthy natural language processing models
using these corpora, to show how well they can adapt to the domain of
mathematics and provide useful tools for exploring mathematical language. We
evaluate several neural and symbolic models against benchmarks that we extract
from the corpus metadata to show that terminology extraction and definition
extraction do not easily generalize to mathematics, and that additional work is
needed to achieve good performance on these metrics. Finally, we provide a
learning assistant that grants access to the content of these corpora in a
context-sensitive manner, utilizing text search and entity linking. Though our
corpora and benchmarks provide useful metrics for evaluating mathematical
language processing, further work is necessary to adapt models to mathematics
in order to provide more effective learning assistants and apply NLP methods to
different mathematical domains.

摘要：數學是一個高度專業的領域，有其獨特的一組挑戰。儘管如此，針對數學文本的自然語言處理的研究相對較少，而且針對 NLP 的數學語言資源也很少。在本文中，我們旨在提供註解語料庫，可用于研究不同脈絡中的數學語言，從教科書中發現的基本概念到先進的研究數學。我們使用神經句法分析模型和一些手動干預對語料庫進行預處理，以提供詞性標記、詞幹和依賴樹。總的來說，我們在三個語料庫中提供了 182397 個句子。然後，我們旨在使用這些語料庫測試和評估幾個值得注意的自然語言處理模型，以展示它們如何適應數學領域並提供探索數學語言的有用工具。我們根據從語料庫元數據中提取的基準評估了幾個神經和符號模型，以表明術語提取和定義提取不易於推廣到數學，並且需要額外的研究才能在這些指標上取得良好的表現。最後，我們提供了一個學習助理，它以上下文敏感的方式授予訪問這些語料庫內容的權限，利用文本搜索和實體連結。儘管我們的語料庫和基準為評估數學語言處理提供了有用的指標，但適應模型以適應數學以提供更有效的學習助理並將 NLP 方法應用於不同的數學領域仍然需要進一步的研究。

##### **Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models**
2406.11568v1 by Sheng Feng, Heyang Liu, Yu Wang, Yanfeng Wang

In this paper, we introduce a groundbreaking end-to-end (E2E) framework for
decoding invasive brain signals, marking a significant advancement in the field
of speech neuroprosthesis. Our methodology leverages the comprehensive
reasoning abilities of large language models (LLMs) to facilitate direct
decoding. By fully integrating LLMs, we achieve results comparable to the
state-of-the-art cascade models. Our findings underscore the immense potential
of E2E frameworks in speech neuroprosthesis, particularly as the technology
behind brain-computer interfaces (BCIs) and the availability of relevant
datasets continue to evolve. This work not only showcases the efficacy of
combining LLMs with E2E decoding for enhancing speech neuroprosthesis but also
sets a new direction for future research in BCI applications, underscoring the
impact of LLMs in decoding complex neural signals for communication
restoration. Code will be made available at
https://github.com/FsFrancis15/BrainLLM.

摘要：在本文中，我們介紹了一個突破性的端到端 (E2E) 框架，用於解碼侵入性腦信號，標誌著語音神經假體領域的重大進展。我們的技術利用大型語言模型 (LLM) 的全面推理能力，以促進直接解碼。通過完全整合 LLM，我們取得了與最先進的串聯模型相當的結果。我們的研究結果強調了 E2E 框架在語音神經假體中的巨大潛力，特別是隨著腦機介面 (BCI) 技術和相關資料集可用性的持續發展。這項工作不僅展示了將 LLM 與 E2E 解碼相結合以增強語音神經假體的功效，而且也為 BCI 應用中的未來研究設定了一個新方向，強調了 LLM 在解碼複雜神經信號以恢復溝通方面的影響力。程式碼將在 https://github.com/FsFrancis15/BrainLLM 上提供。

##### **Quaternion Generative Adversarial Neural Networks and Applications to Color Image Inpainting**
2406.11567v1 by Duan Wang, Dandan Zhu, Meixiang Zhao, Zhigang Jia

Color image inpainting is a challenging task in imaging science. The existing
method is based on real operation, and the red, green and blue channels of the
color image are processed separately, ignoring the correlation between each
channel. In order to make full use of the correlation between each channel,
this paper proposes a Quaternion Generative Adversarial Neural Network (QGAN)
model and related theory, and applies it to solve the problem of color image
inpainting with large area missing. Firstly, the definition of quaternion
deconvolution is given and the quaternion batch normalization is proposed.
Secondly, the above two innovative modules are applied to generate adversarial
networks to improve stability. Finally, QGAN is applied to color image
inpainting and compared with other state-of-the-art algorithms. The
experimental results show that QGAN has superiority in color image inpainting
with large area missing.

摘要：彩色影像修復在影像科學中是一項具有挑戰性的任務。現有的方法基於實數運算，並且分別處理彩色影像的紅、綠、藍通道，忽略各通道之間的關聯性。為了充分利用各通道之間的關聯性，本文提出四元數生成對抗網路 (QGAN) 模型及相關理論，並應用於解決大面積缺失的彩色影像修復問題。首先，給出四元數反摺積的定義，並提出四元數批次正規化。其次，將上述兩個創新的模組應用於生成對抗網路，以提升穩定性。最後，將 QGAN 應用於彩色影像修復，並與其他最先進的演算法進行比較。實驗結果表明，QGAN 在大面積缺失的彩色影像修復中具有優越性。

##### **MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked Low-Rank Adaptation**
2406.11566v1 by Jiakuan Xie, Pengfei Cao, Yuheng Chen, Yubo Chen, Kang Liu, Jun Zhao

Knowledge editing aims to adjust the knowledge within large language models
(LLMs) to prevent their responses from becoming obsolete or inaccurate.
However, existing works on knowledge editing are primarily conducted in a
single language, which is inadequate for multilingual language models. In this
paper, we focus on multilingual knowledge editing (MKE), which requires
propagating updates across multiple languages. This necessity poses a
significant challenge for the task. Furthermore, the limited availability of a
comprehensive dataset for MKE exacerbates this challenge, hindering progress in
this area. Hence, we introduce the Multilingual Knowledge Editing Benchmark
(MKEB), a novel dataset comprising 12 languages and providing a complete
evaluation framework. Additionally, we propose a method that enhances
Multilingual knowledge Editing with neuron-Masked Low-Rank Adaptation (MEMLA).
Specifically, we identify two categories of knowledge neurons to improve
editing precision. Moreover, we perform LoRA-based editing with neuron masks to
efficiently modify parameters and facilitate the propagation of updates across
multiple languages. Experiments demonstrate that our method outperforms
existing baselines and significantly enhances the multi-hop reasoning
capability of the edited model, with minimal impact on its downstream task
performance. The dataset and code will be made publicly available.

摘要：知識編輯旨在調整大型語言模型 (LLM) 中的知識，以防止其回應過時或不準確。然而，現有的知識編輯工作主要以單一語言進行，這對於多語言語言模型來說是不夠的。在本文中，我們專注於多語言知識編輯 (MKE)，這需要跨多種語言傳播更新。這種必要性對這項任務提出了重大挑戰。此外，綜合 MKE 數據集的可用性有限，加劇了這一挑戰，阻礙了該領域的進展。因此，我們引入了多語言知識編輯基準 (MKEB)，這是一個包含 12 種語言的新穎數據集，並提供了一個完整的評估框架。此外，我們提出了一種方法，該方法使用神經元掩碼低秩適應 (MEMLA) 增強了多語言知識編輯。具體來說，我們識別出兩類知識神經元以提高編輯精度。此外，我們使用神經元掩碼執行基於 LoRA 的編輯，以有效修改參數並促進跨多種語言的更新傳播。實驗表明，我們的方法優於現有的基準，並顯著增強了已編輯模型的多跳推理能力，同時對其下游任務性能的影響很小。該數據集和代碼將公開發布。

##### **Extrinsic Evaluation of Cultural Competence in Large Language Models**
2406.11565v1 by Shaily Bhatt, Fernando Diaz

Productive interactions between diverse users and language technologies
require outputs from the latter to be culturally relevant and sensitive. Prior
works have evaluated models' knowledge of cultural norms, values, and
artifacts, without considering how this knowledge manifests in downstream
applications. In this work, we focus on extrinsic evaluation of cultural
competence in two text generation tasks, open-ended question answering and
story generation. We quantitatively and qualitatively evaluate model outputs
when an explicit cue of culture, specifically nationality, is perturbed in the
prompts. Although we find that model outputs do vary when varying nationalities
and feature culturally relevant words, we also find weak correlations between
text similarity of outputs for different countries and the cultural values of
these countries. Finally, we discuss important considerations in designing
comprehensive evaluation of cultural competence in user-facing tasks.

摘要：生產性的互動，存在於多元的使用者和語言科技之間，需要後者的輸出在文化上相關且敏感。先前的研究已經評估了模型對文化規範、價值觀和人工製品的知識，而沒有考慮到這些知識如何表現在下游應用程式中。在這項研究中，我們專注於文化能力的外部評估，在兩個文本生成任務中，開放式問答和故事生成。我們在提示中擾動了文化的明確線索，特別是國籍，並對模型輸出進行量化和質化評估。儘管我們發現，當國籍不同且具有文化相關詞彙時，模型輸出確實會有所不同，但我們也發現不同國家的輸出文本相似度與這些國家的文化價值觀之間的相關性較弱。最後，我們討論了在使用者面對的任務中設計文化能力全面評估時的重要考量因素。

##### **Input Conditioned Graph Generation for Language Agents**
2406.11555v1 by Lukas Vierling, Jie Fu, Kai Chen

Recent progress in Large Language Models (LLMs) and language agents has
demonstrated significant promise for various future applications across
multiple disciplines. While traditional approaches to language agents often
rely on fixed, handcrafted designs, our research aims to develop both learnable
and dynamic agents. Our method uses an existing framework that abstracts
language agents as graphs. Within this graph framework, we aim to learn a model
that can generate edges for every given input to the language agent. This
allows us to generate edges that represent the flow of communication within the
graph based on the given input, thereby adjusting the internal communication of
a language agent. We learn to generate these edges using a pretrained LLM that
is fine-tuned with reinforcement learning. This LLM can be fine-tuned on
several datasets simultaneously, and we hypothesize that the model learns to
adapt to these different domains during training, achieving good overall
performance when encountering data from different domains during deployment. We
demonstrate that our approach surpasses the previous static approach by nearly
6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when
trained with a sparsity-inducing loss. It also performs superior in additional
experiments conducted with the MMLU and Mini Crossword Puzzles datasets. The
code is available at https://github.com/lukasVierling/DynamicGPTSwarm.

摘要：大型语言模型 (LLM) 和语言代理最近的进展已展示出对跨多个学科的各种未来应用的重大前景。虽然传统的语言代理方法通常依赖于固定的手工设计，但我们的研究旨在开发可学习和动态的代理。我们的方法使用了一个现有的框架，将语言代理抽象为图。在这个图框架内，我们旨在学习一个模型，该模型可以为语言代理的每个给定输入生成边。这使我们能够生成代表图中基于给定输入的通信流的边，从而调整语言代理的内部通信。我们学习使用经过强化学习微调的预训练 LLM 来生成这些边。该 LLM 可以同时在多个数据集上进行微调，我们假设该模型在训练期间学习适应这些不同的域，在部署期间遇到来自不同域的数据时实现良好的整体性能。我们证明，我们的方法在 MMLU 和 CMMLU 的组合数据集上比先前的静态方法高出近 6% 的准确度，并且在使用稀疏性诱导损失进行训练时高出 10% 以上。它还在使用 MMLU 和迷你填字游戏数据集进行的其他实验中表现出色。代码可在 https://github.com/lukasVierling/DynamicGPTSwarm 获得。

##### **AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation**
2406.11548v1 by Chuyan Xiong, Chengyu Shen, Xiaoqi Li, Kaichen Zhou, Jiaming Liu, Ruiping Wang, Hao Dong

The ability to reflect on and correct failures is crucial for robotic systems
to interact stably with real-life objects.Observing the generalization and
reasoning capabilities of Multimodal Large Language Models (MLLMs), previous
approaches have aimed to utilize these models to enhance robotic systems
accordingly.However, these methods typically focus on high-level planning
corrections using an additional MLLM, with limited utilization of failed
samples to correct low-level contact poses. To address this gap, we propose an
Autonomous Interactive Correction (AIC) MLLM, which makes use of previous
low-level interaction experiences to correct SE(3) pose predictions.
Specifically, AIC MLLM is initially fine-tuned to acquire both pose prediction
and feedback prompt comprehension abilities.We carefully design two types of
prompt instructions through interactions with objects: 1) visual masks to
highlight unmovable parts for position correction, and 2)textual descriptions
to indicate potential directions for rotation correction.During inference, a
Feedback Information Extraction module is introduced to recognize the failure
cause, allowing AIC MLLM to adaptively correct the pose prediction using the
corresponding prompts.To further enhance manipulation stability, we devise a
Test Time Adaptation strategy that enables AIC MLLM to better adapt to the
current scene configuration.Finally, extensive experiments are conducted in
both simulated and real-world environments to evaluate the proposed method. The
results demonstrate that our AIC MLLM can efficiently correct failure samples
by leveraging interaction experience prompts.Real-world demonstration can be
found at https://sites.google.com/view/aic-mllm

摘要：機器人系統若要與現實生活中的物體穩定互動，必須具備反省和修正失敗的能力。觀察多模態大型語言模型 (MLLM) 的概化和推理能力，先前的做法旨在利用這些模型來相應地強化機器人系統。然而，這些方法通常著重於使用額外的 MLLM 進行高層級規劃修正，而僅有限地利用失敗樣本來修正低層級接觸姿勢。為了解決這個差距，我們提出自治互動修正 (AIC) MLLM，它利用先前的低層級互動經驗來修正 SE(3) 姿勢預測。具體來說，AIC MLLM 最初經過微調，以獲得姿勢預測和回饋提示理解能力。我們透過與物體互動，仔細設計兩種類型的提示指令：1) 視覺遮罩，用於突出不可移動的部分以進行位置修正，以及 2) 文字描述，用於指示旋轉修正的潛在方向。在推理期間，引入了回饋資訊提取模組，以識別失敗原因，讓 AIC MLLM 能夠使用對應的提示自適應地修正姿勢預測。為了進一步增強操作穩定性，我們設計了一個測試時間適應策略，讓 AIC MLLM 能夠更好地適應當前的場景組態。最後，在模擬和真實世界環境中進行了廣泛的實驗，以評估所提出的方法。結果表明，我們的 AIC MLLM 能夠透過利用互動經驗提示有效地修正失敗樣本。可以在 https://sites.google.com/view/aic-mllm 找到實際操作示範

##### **GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations**
2406.11547v1 by Rick Wilming, Artur Dox, Hjalmar Schulz, Marta Oliveira, Benedict Clark, Stefan Haufe

Large pre-trained language models have become popular for many applications
and form an important backbone of many downstream tasks in natural language
processing (NLP). Applying 'explainable artificial intelligence' (XAI)
techniques to enrich such models' outputs is considered crucial for assuring
their quality and shedding light on their inner workings. However, large
language models are trained on a plethora of data containing a variety of
biases, such as gender biases, affecting model weights and, potentially,
behavior. Currently, it is unclear to what extent such biases also impact model
explanations in possibly unfavorable ways. We create a gender-controlled text
dataset, GECO, in which otherwise identical sentences appear in male and female
forms. This gives rise to ground-truth 'world explanations' for gender
classification tasks, enabling the objective evaluation of the correctness of
XAI methods. We also provide GECOBench, a rigorous quantitative evaluation
framework benchmarking popular XAI methods, applying them to pre-trained
language models fine-tuned to different degrees. This allows us to investigate
how pre-training induces undesirable bias in model explanations and to what
extent fine-tuning can mitigate such explanation bias. We show a clear
dependency between explanation performance and the number of fine-tuned layers,
where XAI methods are observed to particularly benefit from fine-tuning or
complete retraining of embedding layers. Remarkably, this relationship holds
for models achieving similar classification performance on the same task. With
that, we highlight the utility of the proposed gender-controlled dataset and
novel benchmarking approach for research and development of novel XAI methods.
All code including dataset generation, model training, evaluation and
visualization is available at: https://github.com/braindatalab/gecobench

摘要：<paragraph>大型預訓練語言模型已廣泛應用於許多應用程式中，並構成自然語言處理 (NLP) 中許多下游任務的重要骨幹。將「可解釋人工智慧」(XAI) 技術應用於豐富此類模型的輸出，被認為對於確保其品質並闡明其內部運作至關重要。然而，大型語言模型是在包含各種偏差的龐大資料上訓練的，例如性別偏差，會影響模型權重和潛在行為。目前尚不清楚此類偏差在何種程度上也會以可能不利的影響模型解釋。我們建立了一個性別控制文字資料集 GECO，其中原本相同的句子以男性和女性形式出現。這產生了性別分類任務的真實「世界解釋」，使我們能夠客觀評估 XAI 方法的正確性。我們還提供了 GECOBench，這是一個嚴謹的量化評估架構，用於對流行的 XAI 方法進行基準測試，將它們應用於經過不同程度微調的預訓練語言模型。這使我們能夠研究預訓練如何導致模型解釋中出現不良偏差，以及微調可以在多大程度上減輕這種解釋偏差。我們展示了解釋效能與微調層數之間的明顯依賴關係，其中觀察到 XAI 方法特別受益於微調或嵌入層的完整重新訓練。值得注意的是，這種關係適用於在同一任務上實現類似分類效能的模型。有了這些，我們強調了所提出的性別控制資料集和新基準測試方法對新 XAI 方法的研究和開發的效用。所有程式碼，包括資料集生成、模型訓練、評估和視覺化，都可以在以下位置取得：https://github.com/braindatalab/gecobench</paragraph>

##### **GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement**
2406.11546v1 by Yifan Yang, Zheshu Song, Jianheng Zhuo, Mingyu Cui, Jinpeng Li, Bo Yang, Yexing Du, Ziyang Ma, Xunying Liu, Ziyuan Wang, Ke Li, Shuai Fan, Kai Yu, Wei-Qiang Zhang, Guoguo Chen, Xie Chen

The evolution of speech technology has been spurred by the rapid increase in
dataset sizes. Traditional speech models generally depend on a large amount of
labeled training data, which is scarce for low-resource languages. This paper
presents GigaSpeech 2, a large-scale, multi-domain, multilingual speech
recognition corpus. It is designed for low-resource languages and does not rely
on paired speech and text data. GigaSpeech 2 comprises about 30,000 hours of
automatically transcribed speech, including Thai, Indonesian, and Vietnamese,
gathered from unlabeled YouTube videos. We also introduce an automated pipeline
for data crawling, transcription, and label refinement. Specifically, this
pipeline uses Whisper for initial transcription and TorchAudio for forced
alignment, combined with multi-dimensional filtering for data quality
assurance. A modified Noisy Student Training is developed to further refine
flawed pseudo labels iteratively, thus enhancing model performance.
Experimental results on our manually transcribed evaluation set and two public
test sets from Common Voice and FLEURS confirm our corpus's high quality and
broad applicability. Notably, ASR models trained on GigaSpeech 2 can reduce the
word error rate for Thai, Indonesian, and Vietnamese on our challenging and
realistic YouTube test set by 25% to 40% compared to the Whisper large-v3
model, with merely 10% model parameters. Furthermore, our ASR models trained on
Gigaspeech 2 yield superior performance compared to commercial services. We
believe that our newly introduced corpus and pipeline will open a new avenue
for low-resource speech recognition and significantly facilitate research in
this area.

摘要：<paragraph>語音技術的演進受到資料集規模快速增加的刺激。傳統的語音模型通常依賴於大量的標記訓練資料，而這對低資源語言來說很稀少。本文提出了 GigaSpeech 2，一個大型、多領域、多語言的語音辨識語料庫。它是為低資源語言設計的，且不依賴成對的語音和文字資料。GigaSpeech 2 包含約 30,000 小時的自動轉錄語音，包括泰語、印尼語和越南語，這些語音是從未標記的 YouTube 影片中收集來的。我們還導入了一個用於資料爬取、轉錄和標籤精煉的自動化管道。具體來說，這個管道使用 Whisper 進行初始轉錄，並使用 TorchAudio 進行強制比對，結合多維度過濾來確保資料品質。開發了一個改良的 Noisy Student Training，用於進一步反覆精煉有缺陷的偽標籤，從而增強模型效能。我們手動轉錄的評估集和來自 Common Voice 和 FLEURS 的兩個公開測試集的實驗結果證實了我們語料庫的高品質和廣泛的適用性。值得注意的是，在 GigaSpeech 2 上訓練的 ASR 模型可以將我們具有挑戰性和實際性的 YouTube 測試集上泰語、印尼語和越南語的字元錯誤率降低 25% 到 40%，而 Whisper large-v3 模型僅有 10% 的模型參數。此外，我們在 Gigaspeech 2 上訓練的 ASR 模型與商業服務相比，表現出更優異的效能。我們相信我們新推出的語料庫和管道將為低資源語音辨識開啟一條新途徑，並顯著促進這方面的研究。</paragraph>

##### **Do Parameters Reveal More than Loss for Membership Inference?**
2406.11544v1 by Anshuman Suri, Xiao Zhang, David Evans

Membership inference attacks aim to infer whether an individual record was
used to train a model, serving as a key tool for disclosure auditing. While
such evaluations are useful to demonstrate risk, they are computationally
expensive and often make strong assumptions about potential adversaries' access
to models and training environments, and thus do not provide very tight bounds
on leakage from potential attacks. We show how prior claims around black-box
access being sufficient for optimal membership inference do not hold for most
useful settings such as stochastic gradient descent, and that optimal
membership inference indeed requires white-box access. We validate our findings
with a new white-box inference attack IHA (Inverse Hessian Attack) that
explicitly uses model parameters by taking advantage of computing
inverse-Hessian vector products. Our results show that both audits and
adversaries may be able to benefit from access to model parameters, and we
advocate for further research into white-box methods for membership privacy
auditing.

摘要：成員推論攻擊旨在推論個別記錄是否用於訓練模型，作為揭露稽核的關鍵工具。雖然此類評估有助於證明風險，但它們在計算上很昂貴，而且通常對潛在對手訪問模型和訓練環境做出強有力的假設，因此無法提供非常嚴格的潛在攻擊洩漏範圍。我們展示了有關黑盒存取足以進行最佳成員推論的先前說法並不適用於大多數有用的設定，例如隨機梯度下降，以及最佳成員推論確實需要白盒存取。我們使用新的白盒推論攻擊 IHA（逆海森攻擊）驗證我們的發現，該攻擊透過利用計算逆海森向量積，明確使用模型參數。我們的結果顯示，稽核和對手都可以從訪問模型參數中受益，我們主張進一步研究白盒方法以進行成員隱私稽核。

##### **Improving Quality Control of Whole Slide Images by Explicit Artifact Augmentation**
2406.11538v1 by Artur Jurgas, Marek Wodzinski, Marina D'Amato, Jeroen van der Laak, Manfredo Atzori, Henning Müller

The problem of artifacts in whole slide image acquisition, prevalent in both
clinical workflows and research-oriented settings, necessitates human
intervention and re-scanning. Overcoming this challenge requires developing
quality control algorithms, that are hindered by the limited availability of
relevant annotated data in histopathology. The manual annotation of
ground-truth for artifact detection methods is expensive and time-consuming.
This work addresses the issue by proposing a method dedicated to augmenting
whole slide images with artifacts. The tool seamlessly generates and blends
artifacts from an external library to a given histopathology dataset. The
augmented datasets are then utilized to train artifact classification methods.
The evaluation shows their usefulness in classification of the artifacts, where
they show an improvement from 0.10 to 0.01 AUROC depending on the artifact
type. The framework, model, weights, and ground-truth annotations are freely
released to facilitate open science and reproducible research.

摘要：在臨床上或研究中，全幻燈片影像擷取時產生的偽像問題，需要人為介入和重新掃描。克服這個挑戰需要開發品質控管演算法，但組織病理學中相關註解資料有限，阻礙了演算法的發展。人工註解偽像偵測方法的真實情況既昂貴又費時。本研究提出一個方法來解決這個問題，這個方法專門用來增加全幻燈片影像中的偽像。這個工具可以無縫地從外部資料庫產生並混合偽像到給定的組織病理學資料集。然後使用擴充後的資料集來訓練偽像分類方法。評估顯示出它們在偽像分類中的效用，在不同的偽像類型中，它們的 AUROC 從 0.10 進步到 0.01。這個架構、模型、權重和真實註解是免費釋出的，以利於開放科學和可重製的研究。

##### **Explainable Artificial Intelligence and Multicollinearity : A Mini Review of Current Approaches**
2406.11524v1 by Ahmed M Salih

Explainable Artificial Intelligence (XAI) methods help to understand the
internal mechanism of machine learning models and how they reach a specific
decision or made a specific action. The list of informative features is one of
the most common output of XAI methods. Multicollinearity is one of the big
issue that should be considered when XAI generates the explanation in terms of
the most informative features in an AI system. No review has been dedicated to
investigate the current approaches to handle such significant issue. In this
paper, we provide a review of the current state-of-the-art approaches in
relation to the XAI in the context of recent advances in dealing with the
multicollinearity issue. To do so, we searched in three repositories that are:
Web of Science, Scopus and IEEE Xplore to find pertinent published papers.
After excluding irrelevant papers, seven papers were considered in the review.
In addition, we discuss the current XAI methods and their limitations in
dealing with the multicollinearity and suggest future directions.

摘要：可解釋人工智慧（XAI）方法有助於理解機器學習模型的內部機制，以及它們如何達成特定決策或採取特定行動。資訊性特徵清單是 XAI 方法最常見的輸出之一。多重共線性是 XAI 在 AI 系統中根據資訊性特徵產生解釋時應考慮的主要問題之一。目前還沒有任何評論專門探討處理此類重大問題的現有方法。在本文中，我們回顧了與 XAI 相關的現有最先進方法，以及在處理多重共線性問題方面取得的最新進展。為此，我們在 Web of Science、Scopus 和 IEEE Xplore 三個資料庫中搜尋相關已發表的論文。在排除無關論文後，我們在評論中考慮了七篇論文。此外，我們討論了現有的 XAI 方法及其在處理多重共線性方面的限制，並提出了未來的方向。

##### **FullCert: Deterministic End-to-End Certification for Training and Inference of Neural Networks**
2406.11522v1 by Tobias Lorenz, Marta Kwiatkowska, Mario Fritz

Modern machine learning models are sensitive to the manipulation of both the
training data (poisoning attacks) and inference data (adversarial examples).
Recognizing this issue, the community has developed many empirical defenses
against both attacks and, more recently, provable certification methods against
inference-time attacks. However, such guarantees are still largely lacking for
training-time attacks. In this work, we present FullCert, the first end-to-end
certifier with sound, deterministic bounds, which proves robustness against
both training-time and inference-time attacks. We first bound all possible
perturbations an adversary can make to the training data under the considered
threat model. Using these constraints, we bound the perturbations' influence on
the model's parameters. Finally, we bound the impact of these parameter changes
on the model's prediction, resulting in joint robustness guarantees against
poisoning and adversarial examples. To facilitate this novel certification
paradigm, we combine our theoretical work with a new open-source library
BoundFlow, which enables model training on bounded datasets. We experimentally
demonstrate FullCert's feasibility on two different datasets.

摘要：現代機器學習模型對於訓練數據（中毒攻擊）和推論數據（對抗範例）的操縱很敏感。認識到這個問題，社群已經針對這兩種攻擊開發出許多經驗防禦措施，而且最近還針對推論時間攻擊開發出可證明認證方法。然而，對於訓練時間攻擊，此類擔保仍然嚴重不足。在此研究中，我們提出 FullCert，這是第一個具有健全、確定性界線的端對端認證器，它證明了對訓練時間和推論時間攻擊的穩健性。我們首先界定在所考慮的威脅模型下，對手對訓練數據可以進行的所有擾動。使用這些約束條件，我們界定擾動對模型參數的影響。最後，我們界定這些參數變更對模型預測的影響，從而針對中毒和對抗範例產生聯合穩健性擔保。為了促進這種新穎的認證範例，我們將我們的理論工作與一個新的開放原始碼函式庫 BoundFlow 結合起來，它可以在有界資料集上進行模型訓練。我們在兩個不同的資料集上以實驗方式證明了 FullCert 的可行性。

##### **Revisiting Spurious Correlation in Domain Generalization**
2406.11517v1 by Bin Qin, Jiangmeng Li, Yi Li, Xuesong Wu, Yupeng Wang, Wenwen Qiang, Jianwen Cao

Without loss of generality, existing machine learning techniques may learn
spurious correlation dependent on the domain, which exacerbates the
generalization of models in out-of-distribution (OOD) scenarios. To address
this issue, recent works build a structural causal model (SCM) to describe the
causality within data generation process, thereby motivating methods to avoid
the learning of spurious correlation by models. However, from the machine
learning viewpoint, such a theoretical analysis omits the nuanced difference
between the data generation process and representation learning process,
resulting in that the causal analysis based on the former cannot well adapt to
the latter. To this end, we explore to build a SCM for representation learning
process and further conduct a thorough analysis of the mechanisms underlying
spurious correlation. We underscore that adjusting erroneous covariates
introduces bias, thus necessitating the correct selection of spurious
correlation mechanisms based on practical application scenarios. In this
regard, we substantiate the correctness of the proposed SCM and further propose
to control confounding bias in OOD generalization by introducing a propensity
score weighted estimator, which can be integrated into any existing OOD method
as a plug-and-play module. The empirical results comprehensively demonstrate
the effectiveness of our method on synthetic and large-scale real OOD datasets.

摘要：在不失一般性的前提下，现有的机器学习技术可能会学习
依赖于领域的虚假相关性，这会加剧模型在分布外 (OOD) 场景中的泛化。为了解决
这个问题，最近的工作构建了一个结构因果模型 (SCM) 来描述
数据生成过程中的因果关系，从而激发方法来避免
模型学习虚假相关性。然而，从机器的角度来看
学习观点，这种理论分析忽略了数据生成过程和表示学习过程之间的细微差别，
导致基于前者的因果分析不能很好地适应后者。为此，我们探索构建一个 SCM 以进行表示学习
过程并进一步对虚假相关性背后的机制进行彻底分析。我们强调调整错误的协变量
会引入偏差，因此需要根据实际应用场景正确选择虚假
相关机制。在这方面，我们证实了所提出的 SCM 的正确性，并进一步提出
通过引入倾向得分加权估计量来控制 OOD 泛化中的混杂偏差，该估计量可以集成到任何现有的 OOD 方法中
作为一个即插即用的模块。经验结果全面证明了
我们方法在合成和大型真实 OOD 数据集上的有效性。

##### **Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs**
2406.11514v1 by Yi Fang, Moxin Li, Wenjie Wang, Hui Lin, Fuli Feng

Large Language Models (LLMs) excel in various natural language processing
tasks but struggle with hallucination issues. Existing solutions have
considered utilizing LLMs' inherent reasoning abilities to alleviate
hallucination, such as self-correction and diverse sampling methods. However,
these methods often overtrust LLMs' initial answers due to inherent biases. The
key to alleviating this issue lies in overriding LLMs' inherent biases for
answer inspection. To this end, we propose a CounterFactual Multi-Agent Debate
(CFMAD) framework. CFMAD presets the stances of LLMs to override their inherent
biases by compelling LLMs to generate justifications for a predetermined
answer's correctness. The LLMs with different predetermined stances are engaged
with a skeptical critic for counterfactual debate on the rationality of
generated justifications. Finally, the debate process is evaluated by a
third-party judge to determine the final answer. Extensive experiments on four
datasets of three tasks demonstrate the superiority of CFMAD over existing
methods.

摘要：大型語言模型 (LLM) 在各種自然語言處理任務中表現出色，但卻飽受幻覺問題所苦。現有的解決方案已考慮利用 LLM 內在的推理能力來減輕幻覺，例如自我修正和多樣化抽樣方法。然而，這些方法由於內在偏見，常常過度信任 LLM 的初始答案。解決此問題的關鍵在於覆寫 LLM 固有的偏見以進行答案檢查。為此，我們提出了一個反事實多主體辯論 (CFMAD) 框架。CFMAD 預設 LLM 的立場以覆寫其內在偏見，強迫 LLM 為預先確定的答案正確性產生依據。具有不同預先確定立場的 LLM 與一位懷疑論者進行反事實辯論，針對已產生依據的合理性進行辯論。最後，辯論過程由第三方評審進行評估，以確定最終答案。在三個任務的四個資料集上進行的廣泛實驗證明了 CFMAD 優於現有方法。

##### **On the Feasibility of Fidelity$^-$ for Graph Pruning**
2406.11504v1 by Yong-Min Shin, Won-Yong Shin

As one of popular quantitative metrics to assess the quality of explanation
of graph neural networks (GNNs), fidelity measures the output difference after
removing unimportant parts of the input graph. Fidelity has been widely used
due to its straightforward interpretation that the underlying model should
produce similar predictions when features deemed unimportant from the
explanation are removed. This raises a natural question: "Does fidelity induce
a global (soft) mask for graph pruning?" To solve this, we aim to explore the
potential of the fidelity measure to be used for graph pruning, eventually
enhancing the GNN models for better efficiency. To this end, we propose
Fidelity$^-$-inspired Pruning (FiP), an effective framework to construct global
edge masks from local explanations. Our empirical observations using 7 edge
attribution methods demonstrate that, surprisingly, general eXplainable AI
methods outperform methods tailored to GNNs in terms of graph pruning
performance.

摘要：作為評估圖神經網路 (GNN) 解釋品質的熱門定量指標之一，保真度衡量移除輸入圖形中不重要部分後輸出的差異。保真度廣泛使用，因為其直觀的解釋是，當從解釋中移除被認為不重要的特徵時，底層模型應產生類似的預測。這引發了一個自然的問題：「保真度是否會為圖形剪枝誘發一個全局 (軟) 掩碼？」為了解決這個問題，我們旨在探索保真度測量在圖形剪枝中的潛力，最終增強 GNN 模型以提高效率。為此，我們提出了保真度$^-$- 啟發剪枝 (FiP)，一個從局部解釋構造全局邊緣掩碼的有效框架。我們使用 7 種邊緣歸因方法的經驗觀察表明，令人驚訝的是，一般可解釋的 AI 方法在圖形剪枝效能方面優於針對 GNN 量身打造的方法。

##### **GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation**
2406.11503v1 by Shihao Cai, Keqin Bao, Hangyu Guo, Jizhi Zhang, Jun Song, Bo Zheng

Large language models have seen widespread adoption in math problem-solving.
However, in geometry problems that usually require visual aids for better
understanding, even the most advanced multi-modal models currently still face
challenges in effectively using image information. High-quality data is crucial
for enhancing the geometric capabilities of multi-modal models, yet existing
open-source datasets and related efforts are either too challenging for direct
model learning or suffer from misalignment between text and images. To overcome
this issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to
generate relatively basic geometry problems with aligned text and images,
facilitating model learning. We have produced a dataset of 4.9K geometry
problems and combined it with 19K open-source data to form our GeoGPT4V
dataset. Experimental results demonstrate that the GeoGPT4V dataset
significantly improves the geometry performance of various models on the
MathVista and MathVision benchmarks. The code is available at
https://github.com/Lanyu0303/GeoGPT4V_Project

摘要：大型語言模型已被廣泛採用於數學問題求解。
然而，在通常需要視覺輔助工具才能更佳理解的幾何問題中，即使是最先進的多模態模型目前仍面臨有效使用影像資訊的挑戰。高品質資料對於提升多模態模型的幾何能力至關重要，但現有的開源資料集和相關工作，要不就是對模型學習而言過於困難，要不就是文本和影像之間存在錯位問題。為了克服這個問題，我們提出一個新穎的管道，利用 GPT-4 和 GPT-4V 來產生具有對齊文本和影像的相對基礎的幾何問題，促進模型學習。我們已經產生了一個包含 4.9K 個幾何問題的資料集，並將其與 19K 個開源資料結合，以形成我們的 GeoGPT4V 資料集。實驗結果表明，GeoGPT4V 資料集顯著提升了各種模型在 MathVista 和 MathVision 基準上的幾何效能。程式碼可在 https://github.com/Lanyu0303/GeoGPT4V_Project 取得

##### **Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality**
2406.11501v1 by Jiangmeng Li, Bin Qin, Qirui Ji, Yi Li, Wenwen Qiang, Jianwen Cao, Fanjiang Xu

Leveraging the development of structural causal model (SCM), researchers can
establish graphical models for exploring the causal mechanisms behind machine
learning techniques. As the complexity of machine learning applications rises,
single-world interventionism causal analysis encounters theoretical adaptation
limitations. Accordingly, cross-world counterfactual approach extends our
understanding of causality beyond observed data, enabling hypothetical
reasoning about alternative scenarios. However, the joint involvement of
cross-world variables, encompassing counterfactual variables and real-world
variables, challenges the construction of the graphical model. Twin network is
a subtle attempt, establishing a symbiotic relationship, to bridge the gap
between graphical modeling and the introduction of counterfactuals albeit with
room for improvement in generalization. In this regard, we demonstrate the
theoretical breakdowns of twin networks in certain cross-world counterfactual
scenarios. To this end, we propose a novel teleporter theory to establish a
general and simple graphical representation of counterfactuals, which provides
criteria for determining teleporter variables to connect multiple worlds. In
theoretical application, we determine that introducing the proposed teleporter
theory can directly obtain the conditional independence between counterfactual
variables and real-world variables from the cross-world SCM without requiring
complex algebraic derivations. Accordingly, we can further identify
counterfactual causal effects through cross-world symbolic derivation. We
demonstrate the generality of the teleporter theory to the practical
application. Adhering to the proposed theory, we build a plug-and-play module,
and the effectiveness of which are substantiated by experiments on benchmarks.

摘要：<paragraph>藉由結構因果模型 (SCM) 的發展，研究人員可以建立圖形模型，以探索機器學習技術背後的因果機制。隨著機器學習應用程式的複雜性增加，單一世界介入主義因果分析遭遇了理論適應上的限制。因此，跨世界反事實方法擴展了我們對因果關係的理解，超越了觀察到的資料，能夠對替代情境進行假設性推理。然而，跨世界變數的共同參與，包含反事實變數和真實世界變數，對圖形模型的建構提出了挑戰。雙生網路是一個微妙的嘗試，建立共生關係，以彌合圖形建模和反事實引入之間的差距，儘管在概括方面仍有改進的空間。有鑑於此，我們展示了雙生網路在特定跨世界反事實情境中的理論分解。為此，我們提出了一個新穎的傳送器理論，以建立反事實的一般且簡單的圖形表示，這提供了確定傳送器變數以連接多個世界的準則。在理論應用中，我們確定引入所提出的傳送器理論可以直接從跨世界 SCM 中獲得反事實變數和真實世界變數之間的條件獨立性，而不需要複雜的代數推導。因此，我們可以進一步透過跨世界符號推導來識別反事實因果效應。我們展示了傳送器理論在實際應用中的普遍性。遵循所提出的理論，我們建立了一個即插即用的模組，其有效性已通過基準測試中的實驗得到證實。</paragraph>

##### **CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG**
2406.11497v1 by Boyi Deng, Wenjie Wang, Fengbin Zhu, Qifan Wang, Fuli Feng

Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large
Language Models (LLMs) by referencing external documents. However, the
misinformation in external documents may mislead LLMs' generation. To address
this issue, we explore the task of "credibility-aware RAG", in which LLMs
automatically adjust the influence of retrieved documents based on their
credibility scores to counteract misinformation. To this end, we introduce a
plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention
$\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in
LLMs and adjusts their attention scores based on the credibility of the
documents, thereby reducing the impact of low-credibility documents.
Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and
Qwen-7B show that CrAM improves the RAG performance of LLMs against
misinformation pollution by over 20%, even surpassing supervised fine-tuning
methods.

摘要：檢索增強生成 (RAG) 能透過參照外部文件減輕大型語言模型 (LLM) 的幻覺。然而，外部文件中的錯誤資訊可能會誤導 LLM 的生成。為了解決這個問題，我們探討了「可信度感知 RAG」的任務，其中 LLM 會根據檢索文件的可信度評分自動調整檢索文件的影響力，以對抗錯誤資訊。為此，我們引入了稱為 $\textbf{Cr}$edibility-aware $\textbf{A}$ttention $\textbf{M}$odification (CrAM) 的即插即用方法。CrAM 會找出 LLM 中有影響力的注意力頭部，並根據文件的可信度調整其注意力評分，從而降低低可信度文件的影響。在使用 Llama2-13B、Llama3-8B 和 Qwen-7B 進行的 Natual Questions 和 TriviaQA 實驗中，結果顯示 CrAM 將 LLM 在對抗錯誤資訊污染方面的 RAG 效能提升了 20% 以上，甚至超越了監督微調方法。

##### **Analysing zero-shot temporal relation extraction on clinical notes using temporal consistency**
2406.11486v1 by Vasiliki Kougia, Anastasiia Sedova, Andreas Stephan, Klim Zaporojets, Benjamin Roth

This paper presents the first study for temporal relation extraction in a
zero-shot setting focusing on biomedical text. We employ two types of prompts
and five LLMs (GPT-3.5, Mixtral, Llama 2, Gemma, and PMC-LLaMA) to obtain
responses about the temporal relations between two events. Our experiments
demonstrate that LLMs struggle in the zero-shot setting performing worse than
fine-tuned specialized models in terms of F1 score, showing that this is a
challenging task for LLMs. We further contribute a novel comprehensive temporal
analysis by calculating consistency scores for each LLM. Our findings reveal
that LLMs face challenges in providing responses consistent to the temporal
properties of uniqueness and transitivity. Moreover, we study the relation
between the temporal consistency of an LLM and its accuracy and whether the
latter can be improved by solving temporal inconsistencies. Our analysis shows
that even when temporal consistency is achieved, the predictions can remain
inaccurate.

摘要：本文提出第一個專注於生物醫學文本的零次學習中時間關係萃取研究。我們採用兩種提示和五種 LLM（GPT-3.5、Mixtral、Llama 2、Gemma 和 PMC-LLaMA）來取得關於兩個事件之間時間關係的回應。我們的實驗證明，LLM 在零次學習中表現不佳，在 F1 分數方面表現比微調的專業模型差，這表示這對 LLM 來說是一項具有挑戰性的任務。我們進一步貢獻了一種新穎的全面時間分析，藉由計算每個 LLM 的一致性分數。我們的研究結果顯示，LLM 在提供與唯一性和遞移性時間屬性一致的回應時面臨挑戰。此外，我們研究了 LLM 的時間一致性與其準確性之間的關係，以及後者是否能藉由解決時間不一致性而得到改善。我們的分析顯示，即使達到了時間一致性，預測仍可能不準確。

##### **Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms**
2406.11481v1 by Vaneet Aggarwal, Washim Uddin Mondal, Qinbo Bai

Reinforcement Learning (RL) serves as a versatile framework for sequential
decision-making, finding applications across diverse domains such as robotics,
autonomous driving, recommendation systems, supply chain optimization, biology,
mechanics, and finance. The primary objective in these applications is to
maximize the average reward. Real-world scenarios often necessitate adherence
to specific constraints during the learning process.
  This monograph focuses on the exploration of various model-based and
model-free approaches for Constrained RL within the context of average reward
Markov Decision Processes (MDPs). The investigation commences with an
examination of model-based strategies, delving into two foundational methods -
optimism in the face of uncertainty and posterior sampling. Subsequently, the
discussion transitions to parametrized model-free approaches, where the
primal-dual policy gradient-based algorithm is explored as a solution for
constrained MDPs. The monograph provides regret guarantees and analyzes
constraint violation for each of the discussed setups.
  For the above exploration, we assume the underlying MDP to be ergodic.
Further, this monograph extends its discussion to encompass results tailored
for weakly communicating MDPs, thereby broadening the scope of its findings and
their relevance to a wider range of practical scenarios.

摘要：強化學習 (RL) 是一個通用的框架，可用於順序決策制定，並在機器人技術、自動駕駛、推薦系統、供應鏈最佳化、生物學、力學和金融等不同領域中找到應用。這些應用中的主要目標是最大化平均獎勵。現實世界的場景通常需要在學習過程中遵守特定的約束。
本專題側重於探索各種基於模型和無模型的方法，以在平均獎勵馬可夫決策過程 (MDP) 的背景下進行約束 RL。調查從對基於模型的策略的檢查開始，深入探討兩種基礎方法——面對不確定性和後驗抽樣的樂觀主義。隨後，討論過渡到參數化的無模型方法，其中基於原始對偶策略梯度的演算法被探索為約束 MDP 的解決方案。本專題提供了後悔保證，並分析了每個討論設定的約束違規。
對於上述探索，我們假設基礎 MDP 是 ergodic。此外，本專題將其討論擴展到包含針對弱通信 MDP 量身打造的結果，從而擴大了其發現的範圍及其與更廣泛實際場景相關性。

##### **Vocabulary Expansion for Low-resource Cross-lingual Transfer**
2406.11477v1 by Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras

Large language models (LLMs) have shown remarkable capabilities in many
languages beyond English. Yet, LLMs require more inference steps when
generating non-English text due to their reliance on English-centric
tokenizers, vocabulary, and pre-training data, resulting in higher usage costs
to non-English speakers. Vocabulary expansion with target language tokens is a
widely used cross-lingual vocabulary adaptation approach to remedy this issue.
Despite its effectiveness in inference speedup, the majority of previous work
has focused on high-resource settings assuming access to a substantial amount
of target language data to effectively initialize the embeddings of the new
tokens and adapt the LLM to the target language. However, vocabulary expansion
for LLMs in low-resource settings (i.e. languages and compute) has yet to be
explored. In this paper, we investigate sample-efficient adaptation strategies
from different angles, including target vocabulary size and initialization
methods, and the amount of target data available for adaptation. Extensive
experiments across typologically diverse languages, tasks and models show that
simpler heuristic-based embedding initialization is more efficient and robust
to changes in target vocabulary size and adaptation data in low-resource
settings, outperforming a popular random initialization and a more
sophisticated state-of-the-art approach that relies on external data and model.

摘要：大型語言模型 (LLM) 已在英語以外的許多語言中展現出卓越的能力。然而，由於依賴以英語為中心的標記化器、詞彙和預訓練資料，LLM 在產生非英語文本時需要更多的推論步驟，導致非英語使用者使用成本較高。使用目標語言標記擴充詞彙是一種廣泛使用的跨語言詞彙適應方法，用來解決此問題。儘管其在推論加速方面很有效，但大多數先前的研究都集中在假設可以存取大量目標語言資料的高資源設定，以有效初始化新標記的嵌入並將 LLM 適應到目標語言。然而，在低資源設定（例如語言和運算）中針對 LLM 擴充詞彙尚未被探討。在本文中，我們從不同的角度探討樣本有效適應策略，包括目標詞彙大小和初始化方法，以及可用於適應的目標資料量。跨型態多樣語言、任務和模型的廣泛實驗顯示，在低資源設定中，基於啟發式的嵌入初始化較為有效且強健，且能因應目標詞彙大小和適應資料的變化，優於流行的隨機初始化和依賴外部資料和模型的更精密的最新方法。

##### **How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment**
2406.11474v1 by Heyan Huang, Yinghao Li, Huashan Sun, Yu Bai, Yang Gao

Recent studies have demonstrated that In-Context Learning (ICL), through the
use of specific demonstrations, can align Large Language Models (LLMs) with
human preferences known as In-Context Alignment (ICA), indicating that models
can comprehend human instructions without requiring parameter adjustments.
However, the exploration of the mechanism and applicability of ICA remains
limited. In this paper, we begin by dividing the context text used in ICA into
three categories: format, system prompt, and example. Through ablation
experiments, we investigate the effectiveness of each part in enabling ICA to
function effectively. We then examine how variants in these parts impact the
model's alignment performance. Our findings indicate that the example part is
crucial for enhancing the model's alignment capabilities, with changes in
examples significantly affecting alignment performance. We also conduct a
comprehensive evaluation of ICA's zero-shot capabilities in various alignment
tasks. The results indicate that compared to parameter fine-tuning methods, ICA
demonstrates superior performance in knowledge-based tasks and tool-use tasks.
However, it still exhibits certain limitations in areas such as multi-turn
dialogues and instruction following.

摘要：最近的研究表明，通过使用特定演示的上下文学习 (ICL) 可以将大型语言模型 (LLM) 与称为上下文对齐 (ICA) 的人类偏好相结合，表明模型可以理解人类指令而无需参数调整。
然而，对 ICA 机制和适用性的探索仍然有限。在本文中，我们首先将 ICA 中使用的上下文文本分为三类：格式、系统提示和示例。通过消融实验，我们研究了每部分在使 ICA 有效运行中的有效性。然后我们研究这些部分中的变体如何影响模型的对齐性能。我们的研究结果表明，示例部分对于增强模型的对齐能力至关重要，示例中的变化会显着影响对齐性能。我们还对 ICA 在各种对齐任务中的零样本能力进行了全面评估。结果表明，与参数微调方法相比，ICA 在基于知识的任务和工具使用任务中表现出优异的性能。
然而，它在多轮对话和指令遵循等领域仍然表现出一定的局限性。

##### **Promises, Outlooks and Challenges of Diffusion Language Modeling**
2406.11473v1 by Justin Deschenaux, Caglar Gulcehre

The modern autoregressive Large Language Models (LLMs) have achieved
outstanding performance on NLP benchmarks, and they are deployed in the real
world. However, they still suffer from limitations of the autoregressive
training paradigm. For example, autoregressive token generation is notably slow
and can be prone to \textit{exposure bias}. The diffusion-based language models
were proposed as an alternative to autoregressive generation to address some of
these limitations. We evaluate the recently proposed Score Entropy Discrete
Diffusion (SEDD) approach and show it is a promising alternative to
autoregressive generation but it has some short-comings too. We empirically
demonstrate the advantages and challenges of SEDD, and observe that SEDD
generally matches autoregressive models in perplexity and on benchmarks such as
HellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference
latency, SEDD can be up to 4.5$\times$ more efficient than GPT-2. While SEDD
allows conditioning on tokens at abitrary positions, SEDD appears slightly
weaker than GPT-2 for conditional generation given short prompts. Finally, we
reproduced the main results from the original SEDD paper.

摘要：<paragraph>現代的自迴歸大型語言模型 (LLM) 在 NLP 基準上取得了傑出的表現，並已部署在現實世界中。然而，它們仍然受到自迴歸訓練範例的限制。例如，自迴歸權杖生成明顯緩慢，且可能容易出現「曝光偏差」。基於擴散的語言模型被提出作為自迴歸生成的替代方案，以解決其中一些限制。我們評估了最近提出的分數熵離散擴散 (SEDD) 方法，並展示它是一種很有前途的自迴歸生成替代方案，但它也有一些缺點。我們以經驗方式證明了 SEDD 的優點和挑戰，並觀察到 SEDD 通常在困惑度和 HellaSwag、Arc 或 WinoGrande 等基準上與自迴歸模型相匹配。此外，我們表明在推理延遲方面，SEDD 的效率可以比 GPT-2 高達 4.5 倍。雖然 SEDD 允許在任意位置對權杖進行條件設定，但對於給定簡短提示的條件生成，SEDD 似乎比 GPT-2 稍弱。最後，我們重現了原始 SEDD 論文中的主要結果。</paragraph>

##### **Automating Easy Read Text Segmentation**
2406.11464v1 by Jesús Calleja, Thierry Etchegoyhen, David Ponce

Easy Read text is one of the main forms of access to information for people
with reading difficulties. One of the key characteristics of this type of text
is the requirement to split sentences into smaller grammatical segments, to
facilitate reading. Automated segmentation methods could foster the creation of
Easy Read content, but their viability has yet to be addressed. In this work,
we study novel methods for the task, leveraging masked and generative language
models, along with constituent parsing. We conduct comprehensive automatic and
human evaluations in three languages, analysing the strengths and weaknesses of
the proposed alternatives, under scarce resource limitations. Our results
highlight the viability of automated ER segmentation and remaining deficiencies
compared to expert-driven human segmentation.

摘要：簡易閱讀文本是閱讀困難者獲取資訊的主要形式之一。此類文本的一個關鍵特徵是必須將句子拆分成較小的語法區塊，以利閱讀。自動化分段方法有助於建立簡易閱讀內容，但其可行性尚未獲得探討。在這項工作中，我們研究此任務的新方法，利用遮蔽式和生成式語言模型，以及成分分析。我們在三種語言中進行全面的自動化和人工評估，分析所提出替代方案在資源有限情況下的優點和缺點。我們的結果突顯了自動化 ER 分段的可行性，以及與專家主導的人工分段相比仍存在的不足。

##### **TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation**
2406.11460v1 by Jinyuan Fang, Zaiqiao Meng, Craig Macdonald

Retrieval-augmented generation (RAG) offers an effective approach for
addressing question answering (QA) tasks. However, the imperfections of the
retrievers in RAG models often result in the retrieval of irrelevant
information, which could introduce noises and degrade the performance,
especially when handling multi-hop questions that require multiple steps of
reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose
TRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series
of logically connected knowledge triples, to identify and integrate supporting
evidence from the retrieved documents for answering questions. Specifically,
TRACE employs a KG Generator to create a knowledge graph (KG) from the
retrieved documents, and then uses an Autoregressive Reasoning Chain
Constructor to build reasoning chains. Experimental results on three multi-hop
QA datasets show that TRACE achieves an average performance improvement of up
to 14.03% compared to using all the retrieved documents. Moreover, the results
indicate that using reasoning chains as context, rather than the entire
documents, is often sufficient to correctly answer questions.

摘要：检索增强生成 (RAG) 提供了一种解决问题解答 (QA) 任务的有效方法。然而，RAG 模型中的检索器的不完善通常会导致检索到不相关的讯息，这可能会引入杂讯并降低效能，尤其是在处理需要多步骤推理的多跳问题时。为了增强 RAG 模型的多跳推理能力，我们提出了 TRACE。TRACE 构建了以知识为基础的推理链，这是一系列逻辑连接的知识三元组，以识别和整合来自检索到的文件以回答问题的支持证据。具体来说，TRACE 使用 KG 生成器从检索到的文件中创建知识图 (KG)，然后使用自回归推理链构造器构建推理链。在三个多跳 QA 数据集上的实验结果表明，与使用所有检索到的文件相比，TRACE 的平均性能提升高达 14.03%。此外，结果表明，使用推理链作为上下文，而不是整个文件，通常足以正确回答问题。

##### **Adaptive Reinforcement Learning Planning: Harnessing Large Language Models for Complex Information Extraction**
2406.11455v1 by Zepeng Ding, Ruiyang Ke, Wenhao Huang, Guochao Jiang, Yanda Li, Deqing Yang, Yanghua Xiao, Jiaqing Liang

Existing research on large language models (LLMs) shows that they can solve
information extraction tasks through multi-step planning. However, their
extraction behavior on complex sentences and tasks is unstable, emerging issues
such as false positives and missing elements. We observe that decomposing
complex extraction tasks and extracting them step by step can effectively
improve LLMs' performance, and the extraction orders of entities significantly
affect the final results of LLMs. This paper proposes a two-stage multi-step
method for LLM-based information extraction and adopts the RL framework to
execute the multi-step planning. We regard sequential extraction as a Markov
decision process, build an LLM-based extraction environment, design a decision
module to adaptively provide the optimal order for sequential entity extraction
on different sentences, and utilize the DDQN algorithm to train the decision
model. We also design the rewards and evaluation metrics suitable for the
extraction results of LLMs. We conduct extensive experiments on multiple public
datasets to demonstrate the effectiveness of our method in improving the
information extraction capabilities of LLMs.

摘要：現有的大語言模型（LLM）研究表明，它們可以透過多步驟規劃來解決資訊萃取任務。然而，它們在複雜句子和任務上的萃取行為並不穩定，出現了諸如假陽性和遺漏元素等問題。我們觀察到，將複雜的萃取任務分解並逐步萃取可以有效地提升 LLM 的效能，而實體的萃取順序會顯著影響 LLM 的最終結果。本文提出了一個基於 LLM 的資訊萃取的兩階段多步驟方法，並採用 RL 框架來執行多步驟規劃。我們將順序萃取視為一個馬可夫決策過程，建立一個基於 LLM 的萃取環境，設計一個決策模組，以自適應的方式提供不同句子中順序實體萃取的最佳順序，並利用 DDQN 演算法來訓練決策模型。我們還設計了適合 LLM 萃取結果的獎勵和評估指標。我們在多個公開資料集上進行了廣泛的實驗，以證明我們的方法在提升 LLM 的資訊萃取能力方面的有效性。

##### **GPT-Powered Elicitation Interview Script Generator for Requirements Engineering Training**
2406.11439v1 by Binnur Görer, Fatma Başak Aydemir

Elicitation interviews are the most common requirements elicitation
technique, and proficiency in conducting these interviews is crucial for
requirements elicitation. Traditional training methods, typically limited to
textbook learning, may not sufficiently address the practical complexities of
interviewing techniques. Practical training with various interview scenarios is
important for understanding how to apply theoretical knowledge in real-world
contexts. However, there is a shortage of educational interview material, as
creating interview scripts requires both technical expertise and creativity. To
address this issue, we develop a specialized GPT agent for auto-generating
interview scripts. The GPT agent is equipped with a dedicated knowledge base
tailored to the guidelines and best practices of requirements elicitation
interview procedures. We employ a prompt chaining approach to mitigate the
output length constraint of GPT to be able to generate thorough and detailed
interview scripts. This involves dividing the interview into sections and
crafting distinct prompts for each, allowing for the generation of complete
content for each section. The generated scripts are assessed through standard
natural language generation evaluation metrics and an expert judgment study,
confirming their applicability in requirements engineering training.

摘要：引出式訪談是最常見的需求引出技術，而精通進行這些訪談對於需求引出至關重要。傳統的培訓方法通常僅限於教科書學習，可能無法充分解決訪談技巧的實際複雜性。透過各種訪談場景的實際培訓對於理解如何在現實世界中應用理論知識非常重要。然而，教育訪談資料卻很缺乏，因為建立訪談腳本需要技術專業知識和創造力。為了解決這個問題，我們開發了一種專門的 GPT 代理，用於自動產生訪談腳本。GPT 代理配備了一個專門的知識庫，專門針對需求引出訪談程序的準則和最佳實務。我們採用提示串接方法來減輕 GPT 的輸出長度限制，以便能夠產生全面且詳細的訪談腳本。這包括將訪談分為幾個部分，並為每個部分製作不同的提示，以便為每個部分產生完整的內容。產生的腳本通過標準自然語言生成評估指標和專家判斷研究進行評估，證實了它們在需求工程培訓中的適用性。

##### **Analysing the Behaviour of Tree-Based Neural Networks in Regression Tasks**
2406.11437v1 by Peter Samoaa, Mehrdad Farahani, Antonio Longa, Philipp Leitner, Morteza Haghir Chehreghani

The landscape of deep learning has vastly expanded the frontiers of source
code analysis, particularly through the utilization of structural
representations such as Abstract Syntax Trees (ASTs). While these methodologies
have demonstrated effectiveness in classification tasks, their efficacy in
regression applications, such as execution time prediction from source code,
remains underexplored. This paper endeavours to decode the behaviour of
tree-based neural network models in the context of such regression challenges.
We extend the application of established models--tree-based Convolutional
Neural Networks (CNNs), Code2Vec, and Transformer-based methods--to predict the
execution time of source code by parsing it to an AST. Our comparative analysis
reveals that while these models are benchmarks in code representation, they
exhibit limitations when tasked with regression. To address these deficiencies,
we propose a novel dual-transformer approach that operates on both source code
tokens and AST representations, employing cross-attention mechanisms to enhance
interpretability between the two domains. Furthermore, we explore the
adaptation of Graph Neural Networks (GNNs) to this tree-based problem,
theorizing the inherent compatibility due to the graphical nature of ASTs.
Empirical evaluations on real-world datasets showcase that our dual-transformer
model outperforms all other tree-based neural networks and the GNN-based
models. Moreover, our proposed dual transformer demonstrates remarkable
adaptability and robust performance across diverse datasets.

摘要：深度學習的領域已經大幅擴展了原始碼分析的疆界，特別是透過使用結構化表示法，例如抽象語法樹 (AST)。儘管這些方法已證明在分類任務中有效，但它們在回歸應用中的效能，例如從原始碼中預測執行時間，仍未被充分探討。本文試圖解碼樹狀神經網路模型在這種回歸挑戰中的行為。我們擴展了既定模型的應用——基於樹狀的卷積神經網路 (CNN)、Code2Vec 和基於 Transformer 的方法——透過將原始碼解析成 AST 來預測其執行時間。我們的比較分析顯示，儘管這些模型是程式碼表示法的基準，但它們在執行回歸任務時表現出限制。為了解決這些缺陷，我們提出了一種新穎的雙 Transformer 方法，它同時作用於原始碼代碼和 AST 表示法，並採用跨注意力機制來增強兩個領域之間的可解釋性。此外，我們探討了將圖神經網路 (GNN) 適應到這個基於樹狀的問題，並根據 AST 的圖形性質，理論化其內在相容性。在真實世界資料集上的實證評估顯示，我們的雙 Transformer 模型優於所有其他基於樹狀的神經網路和基於 GNN 的模型。此外，我們提出的雙 Transformer 在不同的資料集上展現出顯著的適應性和穩健的效能。

##### **AnyTrans: Translate AnyText in the Image with Large Scale Models**
2406.11432v1 by Zhipeng Qian, Pei Zhang, Baosong Yang, Kai Fan, Yiwei Ma, Derek F. Wong, Xiaoshuai Sun, Rongrong Ji

This paper introduces AnyTrans, an all-encompassing framework for the
task-Translate AnyText in the Image (TATI), which includes multilingual text
translation and text fusion within images. Our framework leverages the
strengths of large-scale models, such as Large Language Models (LLMs) and
text-guided diffusion models, to incorporate contextual cues from both textual
and visual elements during translation. The few-shot learning capability of
LLMs allows for the translation of fragmented texts by considering the overall
context. Meanwhile, the advanced inpainting and editing abilities of diffusion
models make it possible to fuse translated text seamlessly into the original
image while preserving its style and realism. Additionally, our framework can
be constructed entirely using open-source models and requires no training,
making it highly accessible and easily expandable. To encourage advancement in
the TATI task, we have meticulously compiled a test dataset called MTIT6, which
consists of multilingual text image translation data from six language pairs.

摘要：本文介紹了 AnyTrans，一種用於「圖像中的任何文字翻譯 (TATI)」任務的全面性架構，其中包括多語言文字翻譯和圖像中的文字融合。我們的架構利用了大型語言模型 (LLM) 和文字引導擴散模型等大規模模型的優勢，在翻譯過程中納入了文字和視覺元素的背景線索。LLM 的少量學習能力允許透過考慮整體背景來翻譯片段文字。同時，擴散模型的先進修復和編輯能力讓翻譯後的文字能夠無縫融合到原始圖像中，同時保留其風格和真實性。此外，我們的架構可以使用完全開放原始碼的模型建構，並且不需要訓練，這讓它高度易於存取且容易擴充。為了鼓勵 TATI 任務的進展，我們精心編制了一個名為 MTIT6 的測試資料集，其中包含來自六種語言配對的多語言文字圖像翻譯資料。

##### **Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization**
2406.11431v1 by Wenkai Yang, Shiqi Shen, Guangyao Shen, Zhi Gong, Yankai Lin

Superalignment, where humans are weak supervisors of superhuman models, has
become an important and widely discussed issue in the current era of rapid
development of Large Language Models (LLMs). The recent work preliminarily
studies this problem by using weak models to supervise strong models. It
discovers that weakly supervised strong students can consistently outperform
weak teachers towards the alignment target, leading to a weak-to-strong
generalization phenomenon. However, we are concerned that behind such a
promising phenomenon, whether there exists an issue of weak-to-strong
deception, where strong models may deceive weak models by exhibiting
well-aligned in areas known to weak models but producing misaligned behaviors
in cases weak models do not know. We then take an initial step towards
exploring this security issue in a specific but realistic multi-objective
alignment case, where there may be some alignment targets conflicting with each
other (e.g., helpfulness v.s. harmlessness). Such a conflict is likely to cause
strong models to deceive weak models in one alignment dimension to gain high
reward in other alignment dimension. Our experiments on both the reward
modeling task and the preference optimization scenario indicate: (1) the
weak-to-strong deception exists; (2) the deception phenomenon may intensify as
the capability gap between weak and strong models increases. We also discuss
potential solutions and find bootstrapping with an intermediate model can
mitigate the deception to some extent. Our work highlights the urgent need to
pay more attention to the true reliability of superalignment.

摘要：超對齊（人類是超人類模型的弱監督者）已成為大型語言模型 (LLM) 快速發展的當前時代中一個重要且廣泛討論的問題。最近的研究初步透過使用弱模型監督強模型來探討這個問題。它發現弱監督的強學生可以持續在對齊目標上優於弱教師，導致弱到強的概化現象。然而，我們擔心在這樣一個有希望的現象背後，是否存在弱到強的欺騙問題，其中強模型可能透過在弱模型已知的領域表現出良好對齊，但在弱模型不知道的情況下產生失衡行為來欺騙弱模型。然後，我們採取初步步驟在特定但實際的多目標對齊案例中探討這個安全問題，其中可能有一些對齊目標相互衝突（例如，有益性 v.s. 無害性）。這種衝突可能會導致強模型在一個對齊維度欺騙弱模型，以在其他對齊維度獲得高回報。我們在回報建模任務和偏好最佳化情境中的實驗表明：(1) 弱到強的欺騙存在；(2) 隨著弱模型和強模型之間能力差距的增加，欺騙現象可能會加劇。我們也討論潛在的解決方案，並發現使用中介模型進行自舉可在某種程度上減輕欺騙。我們的研究強調迫切需要更注意超對齊的真實可靠性。

##### **A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression**
2406.11430v1 by Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini

The deployment of large language models (LLMs) is often hindered by the
extensive memory requirements of the Key-Value (KV) cache, especially as
context lengths increase. Existing approaches to reduce the KV cache size
involve either fine-tuning the model to learn a compression strategy or
leveraging attention scores to reduce the sequence length. We analyse the
attention distributions in decoder-only Transformers-based models and observe
that attention allocation patterns stay consistent across most layers.
Surprisingly, we find a clear correlation between the $L_2$ and the attention
scores over cached KV pairs, where a low $L_2$ of a key embedding usually leads
to a high attention score during decoding. This finding indicates that the
influence of a KV pair is potentially determined by the key embedding itself
before being queried. Based on this observation, we compress the KV cache based
on the $L_2$ of key embeddings. Our experimental results show that this simple
strategy can reduce the KV cache size by 50% on language modelling and
needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing
accuracy.

摘要：大型語言模型 (LLM) 的部署通常受到 Key-Value (KV) 快取廣泛的記憶體需求所阻礙，特別是在脈絡長度增加時。現有的減少 KV 快取大小的方法包括微調模型以學習壓縮策略或利用注意力分數來減少序列長度。我們分析了僅解碼器 Transformer 基礎模型中的注意力分佈，並觀察到注意力分配模式在大部分層中保持一致。令人驚訝的是，我們發現 $L_2$ 和快取 KV 對的注意力分數之間存在明顯的關聯性，其中鍵嵌入的低 $L_2$ 通常會導致解碼期間的高注意力分數。這一發現表明，KV 對的影響可能在查詢之前就由鍵嵌入本身決定。基於這一觀察，我們根據鍵嵌入的 $L_2$ 壓縮 KV 快取。我們的實驗結果表明，這種簡單的策略可以在語言建模和大海撈針任務中將 KV 快取大小減少 50%，在密鑰檢索任務中減少 90%，而不會降低準確度。

##### **DiTTo-TTS: Efficient and Scalable Zero-Shot Text-to-Speech with Diffusion Transformer**
2406.11427v1 by Keon Lee, Dong Won Kim, Jaehyeon Kim, Jaewoong Cho

Large-scale diffusion models have shown outstanding generative abilities
across multiple modalities including images, videos, and audio. However,
text-to-speech (TTS) systems typically involve domain-specific modeling factors
(e.g., phonemes and phoneme-level durations) to ensure precise temporal
alignments between text and speech, which hinders the efficiency and
scalability of diffusion models for TTS. In this work, we present an efficient
and scalable Diffusion Transformer (DiT) that utilizes off-the-shelf
pre-trained text and speech encoders. Our approach addresses the challenge of
text-speech alignment via cross-attention mechanisms with the prediction of the
total length of speech representations. To achieve this, we enhance the DiT
architecture to suit TTS and improve the alignment by incorporating semantic
guidance into the latent space of speech. We scale the training dataset and the
model size to 82K hours and 790M parameters, respectively. Our extensive
experiments demonstrate that the large-scale diffusion model for TTS without
domain-specific modeling not only simplifies the training pipeline but also
yields superior or comparable zero-shot performance to state-of-the-art TTS
models in terms of naturalness, intelligibility, and speaker similarity. Our
speech samples are available at https://ditto-tts.github.io.

摘要：大型擴散模型已展現出跨越多種模態（包含影像、影片和音訊）的出色生成能力。然而，文字轉語音（TTS）系統通常包含特定於領域的建模因子（例如音素和音素層級的持續時間），以確保文字與語音之間精確的時間對齊，這會阻礙擴散模型在 TTS 中的效率和可擴充性。在這項工作中，我們提出了一個有效且可擴充的擴散Transformer（DiT），它利用現成的預訓練文字和語音編碼器。我們的做法透過交叉注意力機制以及預測語音表徵的總長度，來解決文字語音對齊的挑戰。為達成此目的，我們增強了 DiT 架構以適用於 TTS，並透過將語意引導納入語音的潛在空間中來改善對齊。我們將訓練資料集和模型大小分別擴充到 82K 小時和 790M 參數。我們廣泛的實驗證明，大型擴散模型用於 TTS，即使沒有特定於領域的建模，不僅簡化了訓練流程，還能產生優於或媲美現有 TTS 模型的零次學習效能，在自然度、清晰度和說話者相似性方面皆是如此。我們的語音範例可於 https://ditto-tts.github.io/ 取得。

##### **Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability**
2406.11424v1 by Gautam B, Anupam Purwar

This paper presents an analysis of open-source large language models (LLMs)
and their application in Retrieval-Augmented Generation (RAG) tasks, specific
for enterprise-specific data sets scraped from their websites. With the
increasing reliance on LLMs in natural language processing, it is crucial to
evaluate their performance, accessibility, and integration within specific
organizational contexts. This study examines various open-source LLMs, explores
their integration into RAG frameworks using enterprise-specific data, and
assesses the performance of different open-source embeddings in enhancing the
retrieval and generation process. Our findings indicate that open-source LLMs,
combined with effective embedding techniques, can significantly improve the
accuracy and efficiency of RAG systems, offering a viable alternative to
proprietary solutions for enterprises.

摘要：本文分析了開源大型語言模型 (LLM) 及其在檢索增強生成 (RAG) 任務中的應用，特別是針對從其網站中擷取的特定企業數據集。隨著自然語言處理對 LLM 的依賴性越來越高，評估其在特定組織環境中的效能、可及性和整合至關重要。本研究檢視了各種開源 LLM，探討它們使用特定企業數據整合到 RAG 架構中，並評估不同開源嵌入對加強檢索和生成過程的效能。我們的研究結果表明，開源 LLM 與有效的嵌入技術相結合，可以顯著提高 RAG 系統的準確性和效率，為企業提供可行的專有解決方案替代方案。

##### **Dredge Word, Social Media, and Webgraph Networks for Unreliable Website Classification and Identification**
2406.11423v1 by Evan M. Williams, Peter Carragher, Kathleen M. Carley

In an attempt to mimic the complex paths through which unreliable content
spreads between search engines and social media, we explore the impact of
incorporating both webgraph and large-scale social media contexts into website
credibility classification and discovery systems. We further explore the usage
of what we define as \textit{dredge words} on social media -- terms or phrases
for which unreliable domains rank highly. Through comprehensive graph neural
network ablations, we demonstrate that curriculum-based heterogeneous graph
models that leverage context from both webgraphs and social media data
outperform homogeneous and single-mode approaches. We further demonstrate that
the incorporation of dredge words into our model strongly associates unreliable
websites with social media and online commerce platforms. Finally, we show our
heterogeneous model greatly outperforms competing systems in the top-k
identification of unlabeled unreliable websites. We demonstrate the strong
unreliability signals present in the diverse paths that users follow to uncover
unreliable content, and we release a novel dataset of dredge words.

摘要：為了模擬不可靠內容在搜尋引擎和社群媒體之間傳播的複雜路徑，我們探討了將網路圖和大型社群媒體背景納入網站可信度分類和發現系統的影響。我們進一步探討了我們定義為社群媒體上的「撈耙子詞彙」的使用——不可靠網域排名很高的詞彙或片語。透過全面的圖形神經網路消融，我們證明了利用網路圖和社群媒體資料的背景的課程式異質圖形模型，優於同質和單一模式方法。我們進一步證明，將撈耙子詞彙納入我們的模型，將不可靠網站與社群媒體和線上商務平台緊密聯繫在一起。最後，我們展示我們的異質模型在未標記不可靠網站的 top-k 識別中，大幅優於競爭系統。我們展示了使用者追蹤以揭露不可靠內容時，各種路徑中存在強烈的不可靠信號，並釋出一個新穎的撈耙子詞彙資料集。

##### **BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pretraining of BabyLM**
2406.11418v1 by Zhewen Shen, Aditya Joshi, Ruey-Cheng Chen

Children from bilingual backgrounds benefit from interactions with parents
and teachers to re-acquire their heritage language. In this paper, we
investigate how this insight from behavioral study can be incorporated into the
learning of small-scale language models. We introduce BAMBINO-LM, a continual
pretraining strategy for BabyLM that uses a novel combination of alternation
and PPO-based perplexity reward induced from a parent Italian model. Upon
evaluation on zero-shot classification tasks for English and Italian,
BAMBINO-LM improves the Italian language capability of a BabyLM baseline. Our
ablation analysis demonstrates that employing both the alternation strategy and
PPO-based modeling is key to this effectiveness gain. We also show that, as a
side effect, the proposed method leads to similar degradation in L1
effectiveness as human children would have had in an equivalent learning
scenario.

摘要：<paragraph>來自雙語背景的兒童從與父母和老師的互動中受益，以重新習得他們的傳統語言。在本文中，我們探討如何將行為研究中的這一見解融入小規模語言模型的學習中。我們介紹了 BAMBINO-LM，這是一種針對 BabyLM 的持續預訓練策略，它使用了一種新穎的交替組合和基於 PPO 的困惑獎勵，這些獎勵來自於一個意大利語父代模型。在對英語和意大利語的零次分類任務進行評估後，BAMBINO-LM 提升了 BabyLM 基準的意大利語能力。我們的消融分析表明，採用交替策略和基於 PPO 的建模對於這種有效性提升至關重要。我們還表明，作為一個副作用，所提出的方法導致了與人類兒童在等效學習場景中會出現的類似的 L1 有效性下降。</paragraph>

##### **Formally Certified Approximate Model Counting**
2406.11414v1 by Yong Kiam Tan, Jiong Yang, Mate Soos, Magnus O. Myreen, Kuldeep S. Meel

Approximate model counting is the task of approximating the number of
solutions to an input Boolean formula. The state-of-the-art approximate model
counter for formulas in conjunctive normal form (CNF), ApproxMC, provides a
scalable means of obtaining model counts with probably approximately correct
(PAC)-style guarantees. Nevertheless, the validity of ApproxMC's approximation
relies on a careful theoretical analysis of its randomized algorithm and the
correctness of its highly optimized implementation, especially the latter's
stateful interactions with an incremental CNF satisfiability solver capable of
natively handling parity (XOR) constraints.
  We present the first certification framework for approximate model counting
with formally verified guarantees on the quality of its output approximation.
Our approach combines: (i) a static, once-off, formal proof of the algorithm's
PAC guarantee in the Isabelle/HOL proof assistant; and (ii) dynamic, per-run,
verification of ApproxMC's calls to an external CNF-XOR solver using proof
certificates. We detail our general approach to establish a rigorous connection
between these two parts of the verification, including our blueprint for
turning the formalized, randomized algorithm into a verified proof checker, and
our design of proof certificates for both ApproxMC and its internal CNF-XOR
solving steps. Experimentally, we show that certificate generation adds little
overhead to an approximate counter implementation, and that our certificate
checker is able to fully certify $84.7\%$ of instances with generated
certificates when given the same time and memory limits as the counter.

摘要：近似模型計數是近似輸入布林公式的解的數量的工作。聯集範式 (CNF) 中公式的最新近似模型計數器 ApproxMC 提供了一個可擴充的方法，用於取得模型計數，並具有可能近似正確 (PAC) 式的保證。儘管如此，ApproxMC 近似的有效性依賴於其隨機演算法的仔細理論分析，以及其高度最佳化實作的正確性，特別是後者與原生處理奇偶校驗 (XOR) 約束的增量 CNF 可滿足性求解器的有狀態互動。
我們提出了一個近似模型計數的第一個認證架構，其輸出近似的品質具有正式驗證的保證。我們的做法結合：(i) 在 Isabelle/HOL 證明輔助工具中演算法的 PAC 保證的靜態、一次性的正式證明；以及 (ii) ApproxMC 對外部 CNF-XOR 求解器呼叫的動態、每次執行驗證，使用證明證書。我們詳細說明了我們建立驗證這兩個部分之間嚴謹連接的通用做法，包括我們將形式化隨機演算法轉變成驗證證明檢查器的藍圖，以及我們為 ApproxMC 及其內部 CNF-XOR 求解步驟設計的證明證書。在實驗中，我們顯示證書產生為近似計數器實作增加很少的開銷，而且我們的證書檢查器能夠在與計數器給予相同時間和記憶體限制時，完全認證 $84.7\%$ 個具有產生證書的實例。

##### **HARE: HumAn pRiors, a key to small language model Efficiency**
2406.11410v1 by Lingyun Zhang, Bin jin, Gaojian Ge, Lunhui Liu, Xuewen Shen, Mingyong Wu, Houqian Zhang, Yongneng Jiang, Shiqi Chen, Shi Pu

Human priors play a crucial role in efficiently utilizing data in deep
learning. However, with the development of large language models (LLMs), there
is an increasing emphasis on scaling both model size and data volume, which
often diminishes the importance of human priors in data construction.
Influenced by these trends, existing Small Language Models (SLMs) mainly rely
on web-scraped large-scale training data, neglecting the proper incorporation
of human priors. This oversight limits the training efficiency of language
models in resource-constrained settings. In this paper, we propose a principle
to leverage human priors for data construction. This principle emphasizes
achieving high-performance SLMs by training on a concise dataset that
accommodates both semantic diversity and data quality consistency, while
avoiding benchmark data leakage. Following this principle, we train an SLM
named HARE-1.1B. Extensive experiments on large-scale benchmark datasets
demonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs,
validating the effectiveness of the proposed principle. Additionally, this
provides new insights into efficient language model training in
resource-constrained environments from the view of human priors.

摘要：人類先驗在有效利用深度學習中的資料扮演著至關重要的角色。然而，隨著大型語言模型（LLM）的發展，越來越重視擴展模型規模和資料量，這常常降低了人類先驗在資料建構中的重要性。受到這些趨勢的影響，現有的小型語言模型（SLM）主要依賴網路爬取的大規模訓練資料，忽略了適當納入人類先驗。這種疏忽限制了語言模型在資源受限環境中的訓練效率。在本文中，我們提出了一個利用人類先驗進行資料建構的原則。此原則強調透過訓練一個簡潔的資料集來達成高性能 SLM，該資料集包含語義多樣性和資料品質一致性，同時避免基準資料外洩。遵循此原則，我們訓練了一個名為 HARE-1.1B 的 SLM。在大型基準資料集上的廣泛實驗證明，HARE-1.1B 相對於最先進的 SLM 表現良好，驗證了所提出原則的有效性。此外，這從人類先驗的角度提供了對資源受限環境中有效語言模型訓練的新見解。

