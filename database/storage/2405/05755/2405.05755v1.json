{"2405.05755": {"publish_time": "2024-05-09", "title": "CSA-Net: Channel-wise Spatially Autocorrelated Attention Networks", "paper_summary": "In recent years, convolutional neural networks (CNNs) with channel-wise\nfeature refining mechanisms have brought noticeable benefits to modelling\nchannel dependencies. However, current attention paradigms fail to infer an\noptimal channel descriptor capable of simultaneously exploiting statistical and\nspatial relationships among feature maps. In this paper, to overcome this\nshortcoming, we present a novel channel-wise spatially autocorrelated (CSA)\nattention mechanism. Inspired by geographical analysis, the proposed CSA\nexploits the spatial relationships between channels of feature maps to produce\nan effective channel descriptor. To the best of our knowledge, this is the f\nirst time that the concept of geographical spatial analysis is utilized in deep\nCNNs. The proposed CSA imposes negligible learning parameters and light\ncomputational overhead to the deep model, making it a powerful yet efficient\nattention module of choice. We validate the effectiveness of the proposed CSA\nnetworks (CSA-Nets) through extensive experiments and analysis on ImageNet, and\nMS COCO benchmark datasets for image classification, object detection, and\ninstance segmentation. The experimental results demonstrate that CSA-Nets are\nable to consistently achieve competitive performance and superior\ngeneralization than several state-of-the-art attention-based CNNs over\ndifferent benchmark tasks and datasets.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5177\u6709\u901a\u9053\u7d1a\u7279\u5fb5\u7cbe\u7149\u6a5f\u5236\u7684\u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u70ba\u901a\u9053\u4f9d\u8cf4\u95dc\u4fc2\u5efa\u6a21\u5e36\u4f86\u4e86\u986f\u8457\u7684\u597d\u8655\u3002\u7136\u800c\uff0c\u7576\u524d\u7684\u6ce8\u610f\u529b\u7bc4\u4f8b\u7121\u6cd5\u63a8\u65b7\u51fa\u540c\u6642\u5229\u7528\u7279\u5fb5\u5716\u4e2d\u7d71\u8a08\u548c\u7a7a\u9593\u95dc\u4fc2\u7684\u6700\u4f73\u901a\u9053\u63cf\u8ff0\u7b26\u3002\u5728\u672c\u6587\u4e2d\uff0c\u70ba\u4e86\u514b\u670d\u9019\u500b\u7f3a\u9ede\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u901a\u9053\u7d1a\u7a7a\u9593\u81ea\u76f8\u95dc (CSA) \u6ce8\u610f\u529b\u6a5f\u5236\u3002\u53d7\u5730\u7406\u5206\u6790\u7684\u555f\u767c\uff0c\u6240\u63d0\u51fa\u7684 CSA \u5229\u7528\u7279\u5fb5\u5716\u901a\u9053\u4e4b\u9593\u7684\u7a7a\u9593\u95dc\u4fc2\u4f86\u7522\u751f\u6709\u6548\u7684\u901a\u9053\u63cf\u8ff0\u7b26\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u662f\u5730\u7406\u7a7a\u9593\u5206\u6790\u7684\u6982\u5ff5\u9996\u6b21\u7528\u65bc\u6df1\u5ea6 CNN\u3002\u6240\u63d0\u51fa\u7684 CSA \u5c0d\u6df1\u5ea6\u6a21\u578b\u65bd\u52a0\u4e86\u53ef\u5ffd\u7565\u7684\u5b78\u7fd2\u53c3\u6578\u548c\u8f15\u91cf\u7d1a\u7684\u904b\u7b97\u958b\u92b7\uff0c\u4f7f\u5176\u6210\u70ba\u529f\u80fd\u5f37\u5927\u4e14\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u6a21\u7d44\u3002\u6211\u5011\u900f\u904e\u5728 ImageNet \u548c MS COCO \u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\u548c\u5206\u6790\uff0c\u9a57\u8b49\u4e86\u6240\u63d0\u51fa\u7684 CSA \u7db2\u8def (CSA-Nets) \u7684\u6709\u6548\u6027\uff0c\u7528\u65bc\u5f71\u50cf\u5206\u985e\u3001\u7269\u4ef6\u5075\u6e2c\u548c\u5be6\u4f8b\u5206\u5272\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207\u591a\u7a2e\u73fe\u6709\u6ce8\u610f\u529b\u5f0f CNN \u76f8\u6bd4\uff0cCSA-Nets \u80fd\u5728\u4e0d\u540c\u7684\u57fa\u6e96\u4efb\u52d9\u548c\u8cc7\u6599\u96c6\u4e0a\u6301\u7e8c\u9054\u6210\u5177\u7af6\u722d\u529b\u7684\u6548\u80fd\u548c\u512a\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "author": "Nick et.al.", "authors": "Nick, Nikzad, Yongsheng Gao, Jun Zhou", "id": "2405.05755v1", "paper_url": "http://arxiv.org/abs/2405.05755v1", "repo": "null"}}