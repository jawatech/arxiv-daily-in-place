{"2408.08869": {"publish_time": "2024-08-16", "title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars", "paper_summary": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable gains in accuracy for Large\nLanguage Models (LLMs). However, such techniques depend on the availability of\nan accurate answer extraction process to aggregate across multiple outputs.\nMoreover, they acquire higher inference cost, in comparison to Greedy Decoding,\ndue to generation of relatively higher number of output tokens. Research has\nshown that the free form text outputs from Self-Consistency can be aggregated\nreliably using LLMs to produce the final output. Additionally, recent\nadvancements in LLM inference have demonstrated that usage of diverse exemplars\nin prompts have the ability to induce diversity in the LLM outputs. Such proven\ntechniques can be easily extended to self-ensembling based approaches to\nachieve enhanced results in text generation. In this paper, we introduce PEDAL\n(Prompts based on Exemplar Diversity Aggregated using LLMs), a hybrid\nself-ensembling approach, that combines the strengths of diverse exemplar based\nprompts and LLM based aggregation to achieve improvement in overall\nperformance. On the publicly available SVAMP and ARC datasets, our experiments\nreveal that PEDAL can achieve better accuracy than Greedy Decoding based\nstrategies with lower inference cost compared to Self Consistency based\napproaches.", "paper_summary_zh": "\u5229\u7528\u4e0d\u540c\u63a8\u7406\u8def\u5f91\uff08\u4f8b\u5982\u81ea\u4e00\u81f4\u6027\uff09\u7684\u81ea\u7d44\u88dd\u6280\u8853\u5df2\u8b49\u660e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6e96\u78ba\u6027\u6709\u986f\u8457\u7684\u63d0\u5347\u3002\u7136\u800c\uff0c\u6b64\u985e\u6280\u8853\u4f9d\u8cf4\u65bc\u6e96\u78ba\u7684\u7b54\u6848\u8403\u53d6\u7a0b\u5e8f\uff0c\u624d\u80fd\u5f59\u7e3d\u591a\u500b\u8f38\u51fa\u3002\u6b64\u5916\uff0c\u8207\u8caa\u5a6a\u89e3\u78bc\u76f8\u6bd4\uff0c\u5b83\u5011\u6703\u7522\u751f\u8f03\u9ad8\u6578\u91cf\u7684\u8f38\u51fa\u6a19\u8a18\uff0c\u56e0\u6b64\u6703\u7522\u751f\u8f03\u9ad8\u7684\u63a8\u8ad6\u6210\u672c\u3002\u7814\u7a76\u986f\u793a\uff0c\u53ef\u4ee5\u4f7f\u7528 LLM \u53ef\u9760\u5730\u5f59\u7e3d\u4f86\u81ea\u81ea\u4e00\u81f4\u6027\u7684\u81ea\u7531\u5f62\u5f0f\u6587\u5b57\u8f38\u51fa\uff0c\u4ee5\u7522\u751f\u6700\u7d42\u8f38\u51fa\u3002\u6b64\u5916\uff0cLLM \u63a8\u8ad6\u7684\u6700\u65b0\u9032\u5c55\u5df2\u8b49\u660e\uff0c\u5728\u63d0\u793a\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684\u7bc4\u4f8b\u80fd\u5920\u8a98\u5c0e LLM \u8f38\u51fa\u7684\u591a\u6a23\u6027\u3002\u6b64\u985e\u5df2\u9a57\u8b49\u7684\u6280\u8853\u53ef\u4ee5\u8f15\u9b06\u5730\u64f4\u5c55\u5230\u57fa\u65bc\u81ea\u7d44\u88dd\u7684\u65b9\u6cd5\uff0c\u4ee5\u5728\u6587\u672c\u751f\u6210\u4e2d\u7372\u5f97\u589e\u5f37\u7684\u7d50\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 PEDAL\uff08\u57fa\u65bc\u7bc4\u4f8b\u591a\u6a23\u6027\uff0c\u4e26\u4f7f\u7528 LLM \u5f59\u7e3d\u7684\u63d0\u793a\uff09\uff0c\u9019\u662f\u4e00\u7a2e\u6df7\u5408\u5f0f\u81ea\u7d44\u88dd\u65b9\u6cd5\uff0c\u5b83\u7d50\u5408\u4e86\u57fa\u65bc\u4e0d\u540c\u7bc4\u4f8b\u7684\u63d0\u793a\u548c\u57fa\u65bc LLM \u7684\u5f59\u7e3d\u7684\u512a\u9ede\uff0c\u4ee5\u63d0\u9ad8\u6574\u9ad4\u6548\u80fd\u3002\u5728\u516c\u958b\u63d0\u4f9b\u7684 SVAMP \u548c ARC \u8cc7\u6599\u96c6\u4e0a\uff0c\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u8207\u57fa\u65bc\u81ea\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cPEDAL \u53ef\u4ee5\u6bd4\u57fa\u65bc\u8caa\u5a6a\u89e3\u78bc\u7684\u7b56\u7565\u7372\u5f97\u66f4\u597d\u7684\u6e96\u78ba\u6027\uff0c\u540c\u6642\u5177\u6709\u8f03\u4f4e\u7684\u63a8\u8ad6\u6210\u672c\u3002", "author": "Sumanth Prabhu et.al.", "authors": "Sumanth Prabhu", "id": "2408.08869v1", "paper_url": "http://arxiv.org/abs/2408.08869v1", "repo": "null"}}