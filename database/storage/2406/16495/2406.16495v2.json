{"2406.16495": {"publish_time": "2024-06-24", "title": "OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser", "paper_summary": "Recent research has shown that combining Mamba with Transformer architecture,\nwhich has selective state space and quadratic self-attention mechanism,\noutperforms using Mamba or Transformer architecture alone in language modeling\ntasks. The quadratic self-attention mechanism effectively alleviates the\nshortcomings of selective state space in handling long-term dependencies of any\nelement in the sequence. We propose a position information injection method\nthat connects the selective state space model with the quadratic attention, and\nintegrates these two architectures with hybrid experts with cross-sharing\ndomains, so that we can enjoy the advantages of both. We design a new\narchitecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser\n(OTCE), which can compete with well-known medium-scale open-source language\nmodels on a small scale in language modeling tasks.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c06 Mamba \u4e0e Transformer \u67b6\u6784\u76f8\u7ed3\u5408\uff0c\n\u5b83\u5177\u6709\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u548c\u4e8c\u6b21\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\n\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528 Mamba \u6216 Transformer \u67b6\u6784\u3002\n\u4e8c\u6b21\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u5730\u7f13\u89e3\u4e86\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u5728\u5904\u7406\u5e8f\u5217\u4e2d\u4efb\u4f55\u5143\u7d20\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u7684\u7f3a\u70b9\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4d\u7f6e\u4fe1\u606f\u6ce8\u5165\u65b9\u6cd5\uff0c\u5c06\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e0e\u4e8c\u6b21\u6ce8\u610f\u529b\u76f8\u8fde\u63a5\uff0c\u5e76\u5c06\u8fd9\u4e24\u4e2a\u67b6\u6784\u4e0e\u5177\u6709\u4ea4\u53c9\u5171\u4eab\u57df\u7684\u6df7\u5408\u4e13\u5bb6\u76f8\u96c6\u6210\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u540c\u65f6\u4eab\u53d7\u4e24\u8005\u7684\u4f18\u52bf\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5177\u6709\u66f4\u4eff\u751f\u601d\u60f3\u7684\u65b0\u67b6\u6784\uff1a\u89c2\u5bdf\u8005-\u601d\u8003\u8005-\u6784\u601d\u8005-\u8868\u8fbe\u8005 (OTCE)\uff0c\u5b83\u53ef\u4ee5\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u4e0e\u4f17\u6240\u5468\u77e5\u7684\u4e2d\u7b49\u89c4\u6a21\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u4e0a\u7ade\u4e89\u3002", "author": "Jingze Shi et.al.", "authors": "Jingze Shi, Ting Xie, Bingheng Wu, Chunjun Zheng, Kai Wang", "id": "2406.16495v2", "paper_url": "http://arxiv.org/abs/2406.16495v2", "repo": "https://github.com/LoserCheems/OTCE"}}