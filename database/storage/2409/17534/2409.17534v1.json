{"2409.17534": {"publish_time": "2024-09-26", "title": "Just say what you want: only-prompting self-rewarding online preference optimization", "paper_summary": "We address the challenge of online Reinforcement Learning from Human Feedback\n(RLHF) with a focus on self-rewarding alignment methods. In online RLHF,\nobtaining feedback requires interaction with the environment, which can be\ncostly when using additional reward models or the GPT-4 API. Current\nself-rewarding approaches rely heavily on the discriminator's judgment\ncapabilities, which are effective for large-scale models but challenging to\ntransfer to smaller ones. To address these limitations, we propose a novel,\nonly-prompting self-rewarding online algorithm that generates preference\ndatasets without relying on judgment capabilities. Additionally, we employ\nfine-grained arithmetic control over the optimality gap between positive and\nnegative examples, generating more hard negatives in the later stages of\ntraining to help the model better capture subtle human preferences. Finally, we\nconduct extensive experiments on two base models, Mistral-7B and\nMistral-Instruct-7B, which significantly bootstrap the performance of the\nreference model, achieving 34.5% in the Length-controlled Win Rates of\nAlpacaEval 2.0.", "paper_summary_zh": "\u6211\u5011\u900f\u904e\u5c08\u6ce8\u65bc\u81ea\u734e\u52f5\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u4f86\u89e3\u6c7a\u4eba\u985e\u56de\u994b\u7dda\u4e0a\u5f37\u5316\u5b78\u7fd2 (RLHF) \u7684\u6311\u6230\u3002\u5728\u7dda\u4e0a RLHF \u4e2d\uff0c\u53d6\u5f97\u56de\u994b\u9700\u8981\u8207\u74b0\u5883\u4e92\u52d5\uff0c\u9019\u5728\u4f7f\u7528\u984d\u5916\u7684\u734e\u52f5\u6a21\u578b\u6216 GPT-4 API \u6642\u53ef\u80fd\u6703\u5f88\u6602\u8cb4\u3002\u76ee\u524d\u7684\u81ea\u734e\u52f5\u65b9\u6cd5\u56b4\u91cd\u4f9d\u8cf4\u5224\u5225\u5668\u7684\u5224\u65b7\u80fd\u529b\uff0c\u9019\u5c0d\u65bc\u5927\u578b\u6a21\u578b\u4f86\u8aaa\u5f88\u6709\u6548\uff0c\u4f46\u5c0d\u65bc\u8f03\u5c0f\u7684\u6a21\u578b\u4f86\u8aaa\u537b\u5f88\u96e3\u8f49\u79fb\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u3001\u50c5\u63d0\u793a\u81ea\u734e\u52f5\u7dda\u4e0a\u6f14\u7b97\u6cd5\uff0c\u5b83\u6703\u7522\u751f\u504f\u597d\u8cc7\u6599\u96c6\uff0c\u800c\u4e0d\u4f9d\u8cf4\u5224\u65b7\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a1\u7528\u7d30\u7dfb\u7684\u7b97\u8853\u63a7\u5236\u4f86\u63a7\u5236\u6b63\u8ca0\u7bc4\u4f8b\u4e4b\u9593\u7684\u6700\u4f73\u5316\u5dee\u8ddd\uff0c\u5728\u8a13\u7df4\u7684\u5f8c\u7e8c\u968e\u6bb5\u7522\u751f\u66f4\u591a\u56f0\u96e3\u7684\u8ca0\u9762\u7bc4\u4f8b\uff0c\u4ee5\u5e6b\u52a9\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u7d30\u5fae\u7684\u4eba\u985e\u504f\u597d\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728\u5169\u500b\u57fa\u790e\u6a21\u578b Mistral-7B \u548c Mistral-Instruct-7B \u4e0a\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u9019\u986f\u8457\u5730\u5f15\u5c0e\u4e86\u53c3\u8003\u6a21\u578b\u7684\u6548\u80fd\uff0c\u5728 AlpacaEval 2.0 \u7684\u9577\u5ea6\u63a7\u5236\u52dd\u7387\u4e2d\u9054\u5230 34.5%\u3002", "author": "Ruijie Xu et.al.", "authors": "Ruijie Xu, Zhihan Liu, Yongfei Liu, Shipeng Yan, Zhaoran Wang, Zhi Zhang, Xuming He", "id": "2409.17534v1", "paper_url": "http://arxiv.org/abs/2409.17534v1", "repo": "null"}}