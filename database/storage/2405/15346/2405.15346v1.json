{"2405.15346": {"publish_time": "2024-05-24", "title": "BiSup: Bidirectional Quantization Error Suppression for Large Language Models", "paper_summary": "As the size and context length of Large Language Models (LLMs) grow,\nweight-activation quantization has emerged as a crucial technique for efficient\ndeployment of LLMs. Compared to weight-only quantization, weight-activation\nquantization presents greater challenges due to the presence of outliers in\nactivations. Existing methods have made significant progress by exploring\nmixed-precision quantization and outlier suppression. However, these methods\nprimarily focus on optimizing the results of single matrix multiplication,\nneglecting the bidirectional propagation of quantization errors in LLMs.\nSpecifically, errors accumulate vertically within the same token through\nlayers, and diffuse horizontally across different tokens due to self-attention\nmechanisms. To address this issue, we introduce BiSup, a Bidirectional\nquantization error Suppression method. By constructing appropriate optimizable\nparameter spaces, BiSup utilizes a small amount of data for quantization-aware\nparameter-efficient fine-tuning to suppress the error vertical accumulation.\nBesides, BiSup employs prompt mixed-precision quantization strategy, which\npreserves high precision for the key-value cache of system prompts, to mitigate\nthe error horizontal diffusion. Extensive experiments on Llama and Qwen\nfamilies demonstrate that BiSup can improve performance over two\nstate-of-the-art methods (the average WikiText2 perplexity decreases from 13.26\nto 9.41 for Atom and from 14.33 to 7.85 for QuaRot under the W3A3-g128\nconfiguration), further facilitating the practical applications of low-bit\nweight-activation quantization.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u898f\u6a21\u548c\u5167\u5bb9\u9577\u5ea6\u4e0d\u65b7\u589e\u9577\uff0c\n\u6b0a\u91cd\u6fc0\u6d3b\u91cf\u5316\u5df2\u6210\u70ba LLM \u6709\u6548\u90e8\u7f72\u7684\u4e00\u9805\u95dc\u9375\u6280\u8853\u3002\u8207\u50c5\u6b0a\u91cd\u91cf\u5316\u76f8\u6bd4\uff0c\u6b0a\u91cd\u6fc0\u6d3b\u91cf\u5316\u7531\u65bc\u6fc0\u6d3b\u503c\u4e2d\u5b58\u5728\u7570\u5e38\u503c\u800c\u5e36\u4f86\u4e86\u66f4\u5927\u7684\u6311\u6230\u3002\u73fe\u6709\u65b9\u6cd5\u901a\u904e\u63a2\u7d22\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u548c\u7570\u5e38\u503c\u6291\u5236\u53d6\u5f97\u4e86\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u4e3b\u8981\u8457\u91cd\u65bc\u512a\u5316\u55ae\u4e00\u77e9\u9663\u4e58\u6cd5\u7684\u7d50\u679c\uff0c\u800c\u5ffd\u8996\u4e86 LLM \u4e2d\u91cf\u5316\u8aa4\u5dee\u7684\u96d9\u5411\u50b3\u64ad\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u8aa4\u5dee\u5728\u540c\u4e00\u4ee4\u724c\u5167\u901a\u904e\u5c64\u5782\u76f4\u7d2f\u7a4d\uff0c\u4e26\u7531\u65bc\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u800c\u6a6b\u5411\u64f4\u6563\u5230\u4e0d\u540c\u4ee4\u724c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 BiSup\uff0c\u4e00\u7a2e\u96d9\u5411\u91cf\u5316\u8aa4\u5dee\u6291\u5236\u65b9\u6cd5\u3002\u901a\u904e\u69cb\u5efa\u9069\u7576\u7684\u53ef\u512a\u5316\u53c3\u6578\u7a7a\u9593\uff0cBiSup \u5229\u7528\u5c11\u91cf\u6578\u64da\u9032\u884c\u91cf\u5316\u611f\u77e5\u7684\u53c3\u6578\u9ad8\u6548\u5fae\u8abf\uff0c\u4ee5\u6291\u5236\u8aa4\u5dee\u5782\u76f4\u7d2f\u7a4d\u3002\u6b64\u5916\uff0cBiSup \u63a1\u7528\u63d0\u793a\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7b56\u7565\uff0c\u8a72\u7b56\u7565\u4fdd\u7559\u4e86\u7cfb\u7d71\u63d0\u793a\u7684\u9375\u503c\u5feb\u53d6\u7684\u9ad8\u7cbe\u5ea6\uff0c\u4ee5\u6e1b\u8f15\u8aa4\u5dee\u6c34\u5e73\u64f4\u6563\u3002\u5728 Llama \u548c Qwen \u5bb6\u65cf\u4e0a\u7684\u5927\u91cf\u5be6\u9a57\u8868\u660e\uff0cBiSup \u53ef\u4ee5\u512a\u65bc\u5169\u7a2e\u6700\u5148\u9032\u7684\u65b9\u6cd5\uff08\u5728 W3A3-g128 \u914d\u7f6e\u4e0b\uff0cAtom \u7684\u5e73\u5747 WikiText2 \u56f0\u60d1\u5ea6\u5f9e 13.26 \u6e1b\u5c11\u5230 9.41\uff0cQuaRot \u7684\u56f0\u60d1\u5ea6\u5f9e 14.33 \u6e1b\u5c11\u5230 7.85\uff09\uff0c\u9032\u4e00\u6b65\u4fc3\u9032\u4e86\u4f4e\u4f4d\u5143\u6b0a\u91cd\u6fc0\u6d3b\u91cf\u5316\u7684\u5be6\u969b\u61c9\u7528\u3002", "author": "Minghui Zou et.al.", "authors": "Minghui Zou, Ronghui Guo, Sai Zhang, Xiaowang Zhang, Zhiyong Feng", "id": "2405.15346v1", "paper_url": "http://arxiv.org/abs/2405.15346v1", "repo": "null"}}