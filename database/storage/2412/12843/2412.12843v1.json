{"2412.12843": {"publish_time": "2024-12-17", "title": "Efficient Event-based Semantic Segmentation with Spike-driven Lightweight Transformer-based Networks", "paper_summary": "Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at least 7.30% and 3.30% mIoU,\nrespectively, with extremely 5.48x lower energy consumption and 1.14x faster\ninference speed.", "paper_summary_zh": "\u57fa\u65bc\u4e8b\u4ef6\u7684\u8a9e\u7fa9\u5206\u5272\u5728\u81ea\u52d5\u99d5\u99db\u548c\u6a5f\u5668\u4eba\u6280\u8853\u4e2d\u5177\u6709\u5de8\u5927\u7684\u6f5b\u529b\uff0c\u9019\u662f\u56e0\u70ba\u4e8b\u4ef6\u76f8\u6a5f\u5177\u6709\u9ad8\u52d5\u614b\u7bc4\u570d\u3001\u4f4e\u5ef6\u9072\u548c\u4f4e\u529f\u8017\u6210\u672c\u7b49\u512a\u52e2\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u7576\u524d\u7684\u57fa\u65bc\u4eba\u5de5\u795e\u7d93\u7db2\u8def (ANN) \u7684\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u8a08\u7b97\u9700\u6c42\u9ad8\u3001\u5c0d\u5f71\u50cf\u5e40\u7684\u8981\u6c42\u4ee5\u53ca\u5927\u91cf\u8017\u80fd\u7b49\u554f\u984c\uff0c\u9650\u5236\u4e86\u5b83\u5011\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u908a\u7de3/\u884c\u52d5\u5e73\u53f0\u4e0a\u7684\u6548\u7387\u548c\u61c9\u7528\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 SLTNet\uff0c\u9019\u662f\u4e00\u500b\u4ee5\u8108\u885d\u9a45\u52d5\u7684\u8f15\u91cf\u7d1aTransformer\u70ba\u57fa\u790e\u7684\u7db2\u8def\uff0c\u5c08\u70ba\u57fa\u65bc\u4e8b\u4ef6\u7684\u8a9e\u7fa9\u5206\u5272\u800c\u8a2d\u8a08\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cSLTNet \u5efa\u7acb\u5728\u9ad8\u6548\u7684\u8108\u885d\u9a45\u52d5\u5377\u7a4d\u5340\u584a (SCB) \u4e0a\uff0c\u4ee5\u63d0\u53d6\u8c50\u5bcc\u7684\u8a9e\u7fa9\u7279\u5fb5\uff0c\u540c\u6642\u6e1b\u5c11\u6a21\u578b\u7684\u53c3\u6578\u3002\u7136\u5f8c\uff0c\u70ba\u4e86\u589e\u5f37\u9577\u7a0b\u8108\u7d61\u7279\u5fb5\u4ea4\u4e92\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7d50\u5408\u4e8c\u9032\u4f4d\u906e\u7f69\u904b\u7b97\u7684\u65b0\u578b\u8108\u885d\u9a45\u52d5Transformer\u5340\u584a (STB)\u3002\u57fa\u65bc\u9019\u4e9b\u57fa\u672c\u5340\u584a\uff0cSLTNet \u63a1\u7528\u9ad8\u6548\u7387\u7684\u55ae\u5206\u652f\u67b6\u69cb\uff0c\u540c\u6642\u4fdd\u6301\u8108\u885d\u795e\u7d93\u7db2\u8def (SNN) \u7684\u4f4e\u80fd\u8017\u3002\u6700\u5f8c\uff0c\u5728 DDD17 \u548c DSEC-Semantic \u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cSLTNet \u5728 mIoU \u4e0a\u5206\u5225\u512a\u65bc\u6700\u5148\u9032 (SOTA) \u7684\u57fa\u65bc SNN \u7684\u65b9\u6cd5\u81f3\u5c11 7.30% \u548c 3.30%\uff0c\u540c\u6642\u80fd\u8017\u6975\u4f4e\uff0c\u70ba 5.48 \u500d\uff0c\u63a8\u8ad6\u901f\u5ea6\u70ba 1.14 \u500d\u3002", "author": "Xiaxin Zhu et.al.", "authors": "Xiaxin Zhu, Fangming Guo, Xianlei Long, Qingyi Gu, Chao Chen, Fuqiang Gu", "id": "2412.12843v1", "paper_url": "http://arxiv.org/abs/2412.12843v1", "repo": "null"}}