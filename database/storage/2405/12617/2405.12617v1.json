{"2405.12617": {"publish_time": "2024-05-21", "title": "Quantifying Emergence in Large Language Models", "paper_summary": "Emergence, broadly conceptualized as the ``intelligent'' behaviors of LLMs,\nhas recently been studied and proved challenging to quantify due to the lack of\na measurable definition. Most commonly, it has been estimated statistically\nthrough model performances across extensive datasets and tasks, which consumes\nsignificant resources. In addition, such estimation is difficult to interpret\nand may not accurately reflect the models' intrinsic emergence. In this work,\nwe propose a quantifiable solution for estimating emergence. Inspired by\nemergentism in dynamics, we quantify the strength of emergence by comparing the\nentropy reduction of the macroscopic (semantic) level with that of the\nmicroscopic (token) level, both of which are derived from the representations\nwithin the transformer block. Using a low-cost estimator, our quantification\nmethod demonstrates consistent behaviors across a suite of LMs (GPT-2, GEMMA,\netc.) under both in-context learning (ICL) and natural sentences. Empirical\nresults show that (1) our method gives consistent measurements which align with\nexisting observations based on performance metrics, validating the\neffectiveness of our emergence quantification; (2) our proposed metric uncovers\nnovel emergence patterns such as the correlations between the variance of our\nmetric and the number of ``shots'' in ICL, which further suggests a new way of\ninterpreting hallucinations in LLMs; (3) we offer a potential solution towards\nestimating the emergence of larger and closed-resource LMs via smaller LMs like\nGPT-2. Our codes are available at:\nhttps://github.com/Zodiark-ch/Emergence-of-LLMs/.", "paper_summary_zh": "<paragraph>\u51fa\u73fe\uff0c\u5ee3\u6cdb\u6982\u5ff5\u5316\u70ba LLM \u7684\u300c\u667a\u6167\u300d\u884c\u70ba\uff0c\n\u6700\u8fd1\u5df2\u88ab\u7814\u7a76\u4e26\u8b49\u5be6\u96e3\u4ee5\u91cf\u5316\uff0c\u56e0\u70ba\u7f3a\u4e4f\u53ef\u8861\u91cf\u7684\u5b9a\u7fa9\u3002\u6700\u5e38\u898b\u7684\u662f\uff0c\u5b83\u5df2\u900f\u904e\u6a21\u578b\u5728\u5ee3\u6cdb\u7684\u8cc7\u6599\u96c6\u548c\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u7d71\u8a08\u4f30\u8a08\uff0c\u9019\u6d88\u8017\u4e86\u5927\u91cf\u7684\u8cc7\u6e90\u3002\u6b64\u5916\uff0c\u9019\u7a2e\u4f30\u8a08\u96e3\u4ee5\u89e3\u91cb\uff0c\u53ef\u80fd\u7121\u6cd5\u6e96\u78ba\u53cd\u6620\u6a21\u578b\u7684\u5167\u5728\u51fa\u73fe\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4f30\u8a08\u51fa\u73fe\u7684\u53ef\u91cf\u5316\u89e3\u6c7a\u65b9\u6848\u3002\u53d7\u5230\u52d5\u529b\u5b78\u51fa\u73fe\u4e3b\u7fa9\u7684\u555f\u767c\uff0c\u6211\u5011\u900f\u904e\u6bd4\u8f03\u5de8\u89c0\uff08\u8a9e\u7fa9\uff09\u5c64\u7d1a\u548c\u5fae\u89c0\uff08\u7b26\u865f\uff09\u5c64\u7d1a\u7684\u71b5\u6e1b\u5c11\u4f86\u91cf\u5316\u51fa\u73fe\u7684\u5f37\u5ea6\uff0c\u5169\u8005\u90fd\u4f86\u81ea\u65bcTransformer\u5340\u584a\u4e2d\u7684\u8868\u793a\u3002\u4f7f\u7528\u4f4e\u6210\u672c\u4f30\u8a08\u5668\uff0c\u6211\u5011\u7684\u91cf\u5316\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u60c5\u5883\u5b78\u7fd2 (ICL) \u548c\u81ea\u7136\u53e5\u5b50\u7684\u60c5\u6cc1\u4e0b\uff0c\u5728 LMs\uff08GPT-2\u3001GEMMA \u7b49\uff09\u4e2d\u7684\u4e00\u81f4\u884c\u70ba\u3002\u5be6\u8b49\u7d50\u679c\u986f\u793a\uff0c\uff081\uff09\u6211\u5011\u7684\u91cf\u5316\u65b9\u6cd5\u8207\u57fa\u65bc\u6548\u80fd\u6307\u6a19\u7684\u73fe\u6709\u89c0\u5bdf\u7d50\u679c\u4e00\u81f4\uff0c\u9a57\u8b49\u4e86\u6211\u5011\u51fa\u73fe\u91cf\u5316\u7684\u6709\u6548\u6027\uff1b\uff082\uff09\u6211\u5011\u63d0\u51fa\u7684\u6307\u6a19\u63ed\u793a\u4e86\u65b0\u7684\u51fa\u73fe\u6a21\u5f0f\uff0c\u4f8b\u5982\u6211\u5011\u7684\u6307\u6a19\u7684\u8b8a\u7570\u6578\u8207 ICL \u4e2d\u300c\u6b21\u6578\u300d\u4e4b\u9593\u7684\u76f8\u95dc\u6027\uff0c\u9032\u4e00\u6b65\u63d0\u51fa\u4e86\u89e3\u91cb LLM \u4e2d\u5e7b\u89ba\u7684\u65b0\u65b9\u6cd5\uff1b\uff083\uff09\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u6f5b\u5728\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u900f\u904e\u8f03\u5c0f\u7684 LLM\uff08\u4f8b\u5982 GPT-2\uff09\u4f86\u4f30\u8a08\u8f03\u5927\u4e14\u5c01\u9589\u8cc7\u6e90\u7684 LLM \u7684\u51fa\u73fe\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u53d6\u5f97\uff1a\nhttps://github.com/Zodiark-ch/Emergence-of-LLMs/.</paragraph>", "author": "Hang Chen et.al.", "authors": "Hang Chen, Xinyu Yang, Jiaying Zhu, Wenya Wang", "id": "2405.12617v1", "paper_url": "http://arxiv.org/abs/2405.12617v1", "repo": "https://github.com/zodiark-ch/emergence-of-llms"}}