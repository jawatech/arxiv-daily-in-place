{"2412.08585": {"publish_time": "2024-12-11", "title": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs LLMs", "paper_summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u63a8\u8ad6\u9700\u8981\u5927\u91cf\u7684\u904b\u7b97\u548c\u8a18\u61b6\u9ad4\uff0c\u7279\u5225\u662f\u5728\u95dc\u9375\u7684\u6ce8\u610f\u529b\u6a5f\u5236\u4e2d\u3002\u96d6\u7136\u91cf\u5316\u548c\u52a0\u901f\u6f14\u7b97\u6cd5\u7b49\u6280\u8853\uff0c\u4f8b\u5982 FlashAttention\uff0c\u5df2\u6539\u5584\u6574\u9ad4\u63a8\u8ad6\u6548\u7387\uff0c\u4f46\u5b83\u5011\u89e3\u6c7a\u4e86\u554f\u984c\u7684\u4e0d\u540c\u9762\u5411\uff1a\u91cf\u5316\u5c08\u6ce8\u65bc\u6b0a\u91cd\u555f\u52d5\u64cd\u4f5c\uff0c\u800c FlashAttention \u5247\u6539\u5584\u57f7\u884c\uff0c\u4f46\u9700\u8981\u9ad8\u7cbe\u5ea6\u683c\u5f0f\u3002\u6700\u8fd1\u7684 Key-value (KV) \u5feb\u53d6\u91cf\u5316\u6e1b\u5c11\u4e86\u8a18\u61b6\u9ad4\u983b\u5bec\uff0c\u4f46\u4ecd\u9700\u8981\u6d6e\u9ede\u53bb\u91cf\u5316\u4ee5\u9032\u884c\u6ce8\u610f\u529b\u64cd\u4f5c\u3002\n\u6211\u5011\u63d0\u51fa TurboAttention\uff0c\u9019\u662f\u4e00\u7a2e\u5168\u9762\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u555f\u7528\u6ce8\u610f\u529b\u7684\u91cf\u5316\u57f7\u884c\uff0c\u540c\u6642\u8655\u7406\u8a18\u61b6\u9ad4\u548c\u904b\u7b97\u6548\u7387\u3002\u6211\u5011\u7684\u89e3\u6c7a\u65b9\u6848\u5f15\u5165\u4e86\u5169\u9805\u95dc\u9375\u5275\u65b0\uff1aFlashQ\uff0c\u4e00\u7a2e\u982d\u90e8\u6ce8\u610f\u529b\u91cf\u5316\u6280\u8853\uff0c\u53ef\u540c\u6642\u58d3\u7e2e KV \u5feb\u53d6\u548c\u555f\u7528\u555f\u52d5\u4e58\u6cd5\u7684\u91cf\u5316\u57f7\u884c\uff0c\u4ee5\u53ca\u57fa\u65bc\u7a00\u758f\u6027\u7684 Softmax \u8fd1\u4f3c (SAS)\uff0c\u5b83\u6d88\u9664\u4e86\u5728\u6ce8\u610f\u529b\u4e2d\u6307\u6578\u904b\u7b97\u671f\u9593\u5c0d FP32 \u53bb\u91cf\u5316\u7684\u9700\u6c42\u3002\n\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cTurboAttention \u5728\u6ce8\u610f\u529b\u65b9\u9762\u5be6\u73fe\u4e86 1.2-1.8 \u500d\u7684\u52a0\u901f\uff0c\u5c07 KV \u5feb\u53d6\u5927\u5c0f\u6e1b\u5c11\u4e86 4.4 \u500d\u4ee5\u4e0a\uff0c\u4e26\u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u548c\u6a21\u578b\u4e0a\u512a\u65bc\u6700\u5148\u9032\u7684\u91cf\u5316\u548c\u58d3\u7e2e\u6280\u8853\uff0c\u540c\u6642\u555f\u7528\u4e86\u6bd4 FP16 \u57fa\u7dda\u9ad8\u9054 2.37 \u500d\u7684\u6700\u5927\u541e\u5410\u91cf\u3002", "author": "Hao Kang et.al.", "authors": "Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan", "id": "2412.08585v1", "paper_url": "http://arxiv.org/abs/2412.08585v1", "repo": "null"}}