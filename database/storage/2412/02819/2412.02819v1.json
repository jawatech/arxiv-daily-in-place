{"2412.02819": {"publish_time": "2024-12-03", "title": "CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels", "paper_summary": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering\n16k\\textasciitilde128k, 695 samples in total, the annotations are human-driven.\nWe evaluate commercial and open-source models on CNNSum and conduct a detailed\nanalysis. Based on the observations, we further conduct fine-tuning exploration\nwith short-context summary data. In our study: (1) GPT-4o underperformed, due\nto excessive subjective commentary. (2) Currently, long-context summarization\nmainly relies on memory ability, small LLMs with stable longer context lengths\nare the most cost-effective. Using long data concatenated from short-context\nsummaries makes a significant improvement. (3) Prompt templates may cause a\nlarge performance gap but can be mitigated through fine-tuning. (4) Fine-tuned\nChat or Instruction versions may harm the Base model and further fine-tuning\ncannot bridge performance gap. (5) while models with RoPE base scaling exhibit\nstrong extrapolation potential, their performance may vary significantly when\ncombined with other interpolation methods and need careful selection. (6)\nCNNSum provides more reliable and insightful evaluation results than other\nbenchmarks. We release CNNSum to advance research in this field.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u8a31\u591a\u9577\u8a9e\u5883\u4efb\u52d9\u4e2d\u7372\u5f97\u5145\u5206\u7814\u7a76\u3002\u7136\u800c\uff0c\u7531\u65bc\u6a19\u8a3b\u6210\u672c\u9ad8\u6602\uff0c\u7528\u65bc\u8a13\u7df4\u6216\u8a55\u4f30\u7684\u9ad8\u54c1\u8cea\u9577\u8a9e\u5883\u6458\u8981\u8cc7\u6599\u96c6\u7a00\u5c11\uff0c\u9650\u5236\u4e86\u9032\u4e00\u6b65\u7684\u7814\u7a76\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 CNNSum\uff0c\u4e00\u500b\u65b0\u7684\u591a\u5c3a\u5ea6\u4e2d\u6587\u9577\u8a9e\u5883\u5c0f\u8aaa\u6458\u8981\u57fa\u6e96\uff0c\u5305\u62ec\u56db\u500b\u5b50\u96c6\uff0c\u9577\u5ea6\u6db5\u84cb 16k\\textasciitilde128k\uff0c\u7e3d\u5171 695 \u500b\u6a23\u672c\uff0c\u6a19\u8a3b\u662f\u7531\u4eba\u5de5\u9a45\u52d5\u7684\u3002\u6211\u5011\u8a55\u4f30\u4e86 CNNSum \u4e0a\u7684\u5546\u696d\u548c\u958b\u6e90\u6a21\u578b\uff0c\u4e26\u9032\u884c\u4e86\u8a73\u7d30\u7684\u5206\u6790\u3002\u6839\u64da\u89c0\u5bdf\u7d50\u679c\uff0c\u6211\u5011\u9032\u4e00\u6b65\u4f7f\u7528\u77ed\u8a9e\u5883\u6458\u8981\u8cc7\u6599\u9032\u884c\u5fae\u8abf\u63a2\u7d22\u3002\u5728\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff1a(1) GPT-4o \u8868\u73fe\u4e0d\u4f73\uff0c\u56e0\u70ba\u904e\u5ea6\u7684\u4e3b\u89c0\u8a55\u8ad6\u3002(2) \u76ee\u524d\uff0c\u9577\u8a9e\u5883\u6458\u8981\u4e3b\u8981\u4f9d\u8cf4\u8a18\u61b6\u80fd\u529b\uff0c\u5177\u6709\u7a69\u5b9a\u8f03\u9577\u8a9e\u5883\u9577\u5ea6\u7684\u5c0f\u578b LLM \u6700\u5177\u6210\u672c\u6548\u76ca\u3002\u4f7f\u7528\u5f9e\u77ed\u8a9e\u5883\u6458\u8981\u4e32\u63a5\u800c\u6210\u7684\u9577\u8cc7\u6599\u53ef\u4ee5\u986f\u8457\u63d0\u5347\u3002(3) \u63d0\u793a\u7bc4\u672c\u53ef\u80fd\u6703\u5c0e\u81f4\u5f88\u5927\u7684\u6548\u80fd\u5dee\u8ddd\uff0c\u4f46\u53ef\u4ee5\u900f\u904e\u5fae\u8abf\u4f86\u6e1b\u8f15\u3002(4) \u5fae\u8abf\u7684\u804a\u5929\u6216\u6307\u4ee4\u7248\u672c\u53ef\u80fd\u6703\u640d\u5bb3\u57fa\u790e\u6a21\u578b\uff0c\u9032\u4e00\u6b65\u7684\u5fae\u8abf\u7121\u6cd5\u5f4c\u5408\u6548\u80fd\u5dee\u8ddd\u3002(5) \u96d6\u7136\u5177\u6709 RoPE \u57fa\u790e\u7e2e\u653e\u7684\u6a21\u578b\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u5916\u63a8\u6f5b\u529b\uff0c\u4f46\u5b83\u5011\u8207\u5176\u4ed6\u5167\u63d2\u65b9\u6cd5\u7d50\u5408\u4f7f\u7528\u6642\uff0c\u6548\u80fd\u53ef\u80fd\u6703\u986f\u8457\u8b8a\u5316\uff0c\u9700\u8981\u4ed4\u7d30\u9078\u64c7\u3002(6) CNNSum \u63d0\u4f9b\u6bd4\u5176\u4ed6\u57fa\u6e96\u66f4\u53ef\u9760\u4e14\u6709\u898b\u5730\u7684\u8a55\u4f30\u7d50\u679c\u3002\u6211\u5011\u767c\u5e03 CNNSum \u4ee5\u63a8\u52d5\u6b64\u9818\u57df\u7684\u7814\u7a76\u3002</paragraph>", "author": "Lingxiao Wei et.al.", "authors": "Lingxiao Wei, He Yan, Xiangju Lu, Junmin Zhu, Jun Wang, Wei Zhang", "id": "2412.02819v1", "paper_url": "http://arxiv.org/abs/2412.02819v1", "repo": "null"}}