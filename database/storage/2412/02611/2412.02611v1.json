{"2412.02611": {"publish_time": "2024-12-03", "title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?", "paper_summary": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini\n1.5 Pro, and Reka Core, have expanded their capabilities to include vision and\naudio modalities. While these models demonstrate impressive performance across\na wide range of audio-visual applications, our proposed DeafTest reveals that\nMLLMs often struggle with simple tasks humans find trivial: 1) determining\nwhich of two sounds is louder, and 2) determining which of two sounds has a\nhigher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a\ncomprehensive audio-visual benchmark designed to assess whether those MLLMs can\ntruly understand the audio-visual information. This benchmark encompasses 4,555\ncarefully crafted problems, each incorporating text, visual, and audio\ncomponents. To successfully infer answers, models must effectively leverage\nclues from both visual and audio inputs. To ensure precise and objective\nevaluation of MLLM responses, we have structured the questions as\nmultiple-choice, eliminating the need for human evaluation or LLM-assisted\nassessment. We benchmark a series of closed-source and open-source models and\nsummarize the observations. By revealing the limitations of current models, we\naim to provide useful insight for future dataset collection and model\ndevelopment.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM)\uff0c\u4f8b\u5982 GPT-4o\u3001Gemini 1.5 Pro \u548c Reka Core\uff0c\u5df2\u6269\u5c55\u5176\u529f\u80fd\uff0c\u5305\u62ec\u89c6\u89c9\u548c\u97f3\u9891\u6a21\u6001\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u5728\u5e7f\u6cdb\u7684\u89c6\u542c\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u4f46\u6211\u4eec\u63d0\u51fa\u7684 DeafTest \u8868\u660e\uff0cMLLM \u901a\u5e38\u96be\u4ee5\u5b8c\u6210\u4eba\u7c7b\u8ba4\u4e3a\u5fae\u4e0d\u8db3\u9053\u7684\u7b80\u5355\u4efb\u52a1\uff1a1) \u786e\u5b9a\u4e24\u4e2a\u58f0\u97f3\u4e2d\u54ea\u4e2a\u58f0\u97f3\u66f4\u54cd\u4eae\uff0c\u4ee5\u53ca 2) \u786e\u5b9a\u4e24\u4e2a\u58f0\u97f3\u4e2d\u54ea\u4e2a\u58f0\u97f3\u66f4\u9ad8\u3002\u53d7\u8fd9\u4e9b\u89c2\u5bdf\u7ed3\u679c\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86 AV-Odyssey Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u89c6\u542c\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b MLLM \u662f\u5426\u771f\u6b63\u80fd\u591f\u7406\u89e3\u89c6\u542c\u4fe1\u606f\u3002\u6b64\u57fa\u51c6\u5305\u542b 4,555 \u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u5305\u542b\u6587\u672c\u3001\u89c6\u89c9\u548c\u97f3\u9891\u7ec4\u4ef6\u3002\u4e3a\u4e86\u6210\u529f\u63a8\u65ad\u7b54\u6848\uff0c\u6a21\u578b\u5fc5\u987b\u6709\u6548\u5229\u7528\u89c6\u89c9\u548c\u97f3\u9891\u8f93\u5165\u7684\u7ebf\u7d22\u3002\u4e3a\u4e86\u786e\u4fdd\u5bf9 MLLM \u54cd\u5e94\u8fdb\u884c\u7cbe\u786e\u548c\u5ba2\u89c2\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u5df2\u5c06\u95ee\u9898\u6784\u5efa\u4e3a\u591a\u9879\u9009\u62e9\u9898\uff0c\u4ece\u800c\u65e0\u9700\u4eba\u5de5\u8bc4\u4f30\u6216 LLM \u8f85\u52a9\u8bc4\u4f30\u3002\u6211\u4eec\u5bf9\u4e00\u7cfb\u5217\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u603b\u7ed3\u4e86\u89c2\u5bdf\u7ed3\u679c\u3002\u901a\u8fc7\u63ed\u793a\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u65e8\u5728\u4e3a\u672a\u6765\u7684\u6570\u636e\u96c6\u6536\u96c6\u548c\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u6709\u7528\u7684\u89c1\u89e3\u3002", "author": "Kaixiong Gong et.al.", "authors": "Kaixiong Gong, Kaituo Feng, Bohao Li, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, Xiangyu Yue", "id": "2412.02611v1", "paper_url": "http://arxiv.org/abs/2412.02611v1", "repo": "null"}}