{"2409.18892": {"publish_time": "2024-09-27", "title": "IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation", "paper_summary": "As Large Language Models (LLMs) grow increasingly adept at managing complex\ntasks, the evaluation set must keep pace with these advancements to ensure it\nremains sufficiently discriminative. Item Discrimination (ID) theory, which is\nwidely used in educational assessment, measures the ability of individual test\nitems to differentiate between high and low performers. Inspired by this\ntheory, we propose an ID-induced prompt synthesis framework for evaluating LLMs\nto ensure the evaluation set can continually update and refine according to\nmodel abilities. Our data synthesis framework prioritizes both breadth and\nspecificity. It can generate prompts that comprehensively evaluate the\ncapabilities of LLMs while revealing meaningful performance differences between\nmodels, allowing for effective discrimination of their relative strengths and\nweaknesses across various tasks and domains. To produce high-quality data, we\nincorporate a self-correct mechanism into our generalization framework, and\ndevelop two models to predict prompt discrimination and difficulty score to\nfacilitate our data synthesis framework, contributing valuable tools to\nevaluation data synthesis research. We apply our generated data to evaluate\nfive SOTA models. Our data achieves an average score of 51.92, accompanied by a\nvariance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and\nWizardLM) obtain an average score exceeding 67, with a variance below 3.2. The\nresults demonstrate that the data generated by our framework is more\nchallenging and discriminative compared to previous works. We will release a\ndataset of over 3,000 carefully crafted prompts to facilitate evaluation\nresearch of LLMs.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8655\u7406\u8907\u96dc\u4efb\u52d9\u65b9\u9762\u65e5\u76ca\u5f97\u5fc3\u61c9\u624b\uff0c\u8a55\u4f30\u96c6\u5fc5\u9808\u8207\u9019\u4e9b\u9032\u5c55\u4fdd\u6301\u540c\u6b65\uff0c\u4ee5\u78ba\u4fdd\u5b83\u4ecd\u7136\u5177\u6709\u8db3\u5920\u7684\u5340\u5206\u5ea6\u3002\u5ee3\u6cdb\u7528\u65bc\u6559\u80b2\u8a55\u4f30\u7684\u9805\u76ee\u5340\u5206\uff08ID\uff09\u7406\u8ad6\uff0c\u8861\u91cf\u500b\u5225\u6e2c\u9a57\u9805\u76ee\u5340\u5206\u9ad8\u4f4e\u8868\u73fe\u8005\u7684\u80fd\u529b\u3002\u53d7\u6b64\u7406\u8ad6\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b ID \u8a98\u5c0e\u63d0\u793a\u5408\u6210\u67b6\u69cb\uff0c\u7528\u65bc\u8a55\u4f30 LLM\uff0c\u4ee5\u78ba\u4fdd\u8a55\u4f30\u96c6\u53ef\u4ee5\u6839\u64da\u6a21\u578b\u80fd\u529b\u6301\u7e8c\u66f4\u65b0\u548c\u512a\u5316\u3002\u6211\u5011\u7684\u6578\u64da\u5408\u6210\u67b6\u69cb\u512a\u5148\u8003\u616e\u5ee3\u5ea6\u548c\u5177\u9ad4\u6027\u3002\u5b83\u53ef\u4ee5\u751f\u6210\u5168\u9762\u8a55\u4f30 LLM \u80fd\u529b\u7684\u63d0\u793a\uff0c\u540c\u6642\u63ed\u793a\u6a21\u578b\u4e4b\u9593\u6709\u610f\u7fa9\u7684\u6548\u80fd\u5dee\u7570\uff0c\u5141\u8a31\u6709\u6548\u5340\u5206\u5b83\u5011\u5728\u5404\u7a2e\u4efb\u52d9\u548c\u9818\u57df\u4e2d\u7684\u76f8\u5c0d\u512a\u7f3a\u9ede\u3002\u70ba\u4e86\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u6578\u64da\uff0c\u6211\u5011\u5728\u6211\u5011\u7684\u6982\u5316\u6846\u67b6\u4e2d\u7d0d\u5165\u4e00\u500b\u81ea\u6211\u4fee\u6b63\u6a5f\u5236\uff0c\u4e26\u958b\u767c\u5169\u500b\u6a21\u578b\u4f86\u9810\u6e2c\u63d0\u793a\u5340\u5206\u548c\u96e3\u5ea6\u5206\u6578\uff0c\u4ee5\u4fc3\u9032\u6211\u5011\u7684\u6578\u64da\u5408\u6210\u67b6\u69cb\uff0c\u70ba\u8a55\u4f30\u6578\u64da\u5408\u6210\u7814\u7a76\u505a\u51fa\u5bf6\u8cb4\u7684\u8ca2\u737b\u3002\u6211\u5011\u5c07\u6211\u5011\u7522\u751f\u7684\u6578\u64da\u61c9\u7528\u65bc\u8a55\u4f30\u4e94\u500b SOTA \u6a21\u578b\u3002\u6211\u5011\u7684\u6578\u64da\u9054\u5230 51.92 \u7684\u5e73\u5747\u5206\uff0c\u4e26\u4f34\u96a8\u8457 10.06 \u7684\u8b8a\u7570\u6578\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5148\u524d\u7684\u7814\u7a76\uff08\u5373 SELF-INSTRUCT \u548c WizardLM\uff09\u7372\u5f97\u7684\u5e73\u5747\u5206\u8d85\u904e 67\uff0c\u8b8a\u7570\u6578\u4f4e\u65bc 3.2\u3002\u7d50\u679c\u8868\u660e\uff0c\u8207\u5148\u524d\u7684\u7814\u7a76\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6846\u67b6\u7522\u751f\u7684\u6578\u64da\u66f4\u5177\u6311\u6230\u6027\u548c\u5340\u5206\u5ea6\u3002\u6211\u5011\u5c07\u767c\u5e03\u4e00\u500b\u5305\u542b 3,000 \u591a\u500b\u7cbe\u5fc3\u88fd\u4f5c\u7684\u63d0\u793a\u7684\u6578\u64da\u96c6\uff0c\u4ee5\u4fc3\u9032 LLM \u7684\u8a55\u4f30\u7814\u7a76\u3002</paragraph>", "author": "Fan Lin et.al.", "authors": "Fan Lin, Shuyi Xie, Yong Dai, Wenlin Yao, Tianjiao Lang, Zishan Xu, Zhichao Hu, Xiao Xiao, Yuhong Liu, Yu Zhang", "id": "2409.18892v1", "paper_url": "http://arxiv.org/abs/2409.18892v1", "repo": "null"}}