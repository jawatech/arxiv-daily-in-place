{"2410.16006": {"publish_time": "2024-10-21", "title": "Exploring Continual Fine-Tuning for Enhancing Language Ability in Large Language Model", "paper_summary": "A common challenge towards the adaptability of Large Language Models (LLMs)\nis their ability to learn new languages over time without hampering the model's\nperformance on languages in which the model is already proficient (usually\nEnglish). Continual fine-tuning (CFT) is the process of sequentially\nfine-tuning an LLM to enable the model to adapt to downstream tasks with\nvarying data distributions and time shifts. This paper focuses on the language\nadaptability of LLMs through CFT. We study a two-phase CFT process in which an\nEnglish-only end-to-end fine-tuned LLM from Phase 1 (predominantly Task\nAbility) is sequentially fine-tuned on a multilingual dataset -- comprising\ntask data in new languages -- in Phase 2 (predominantly Language Ability). We\nobserve that the ``similarity'' of Phase 2 tasks with Phase 1 determines the\nLLM's adaptability. For similar phase-wise datasets, the LLM after Phase 2 does\nnot show deterioration in task ability. In contrast, when the phase-wise\ndatasets are not similar, the LLM's task ability deteriorates. We test our\nhypothesis on the open-source \\mis\\ and \\llm\\ models with multiple phase-wise\ndataset pairs. To address the deterioration, we analyze tailored variants of\ntwo CFT methods: layer freezing and generative replay. Our findings demonstrate\ntheir effectiveness in enhancing the language ability of LLMs while preserving\ntask performance, in comparison to relevant baselines.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9069\u61c9\u6027\u7684\u5e38\u898b\u6311\u6230\u662f\u5b83\u5011\u5728\u4e0d\u5f71\u97ff\u6a21\u578b\u5728\u5df2\u7d93\u719f\u7df4\u7684\u8a9e\u8a00\uff08\u901a\u5e38\u662f\u82f1\u8a9e\uff09\u4e0a\u7684\u8868\u73fe\u7684\u60c5\u6cc1\u4e0b\uff0c\u96a8\u8457\u6642\u9593\u63a8\u79fb\u5b78\u7fd2\u65b0\u8a9e\u8a00\u7684\u80fd\u529b\u3002\u6301\u7e8c\u5fae\u8abf (CFT) \u662f\u6309\u9806\u5e8f\u5fae\u8abf LLM \u7684\u904e\u7a0b\uff0c\u4f7f\u6a21\u578b\u80fd\u5920\u9069\u61c9\u5177\u6709\u4e0d\u540c\u8cc7\u6599\u5206\u4f48\u548c\u6642\u9593\u8b8a\u5316\u7684\u4e0b\u6e38\u4efb\u52d9\u3002\u672c\u6587\u91cd\u9ede\u95dc\u6ce8 LLM \u901a\u904e CFT \u7684\u8a9e\u8a00\u9069\u61c9\u6027\u3002\u6211\u5011\u7814\u7a76\u4e86\u4e00\u500b\u5169\u968e\u6bb5 CFT \u904e\u7a0b\uff0c\u5176\u4e2d\u4e00\u500b\u50c5\u9650\u82f1\u8a9e\u7684\u7aef\u5230\u7aef\u5fae\u8abf LLM\uff08\u7b2c 1 \u968e\u6bb5\uff0c\u4e3b\u8981\u662f\u4efb\u52d9\u80fd\u529b\uff09\u5728\u7b2c 2 \u968e\u6bb5\uff08\u4e3b\u8981\u662f\u8a9e\u8a00\u80fd\u529b\uff09\u4e2d\u5728\u4e00\u500b\u591a\u8a9e\u8a00\u8cc7\u6599\u96c6\uff08\u5305\u62ec\u65b0\u8a9e\u8a00\u7684\u4efb\u52d9\u8cc7\u6599\uff09\u4e0a\u6309\u9806\u5e8f\u5fae\u8abf\u3002\u6211\u5011\u89c0\u5bdf\u5230\u7b2c 2 \u968e\u6bb5\u4efb\u52d9\u8207\u7b2c 1 \u968e\u6bb5\u7684\u300c\u76f8\u4f3c\u6027\u300d\u6c7a\u5b9a\u4e86 LLM \u7684\u9069\u61c9\u6027\u3002\u5c0d\u65bc\u76f8\u4f3c\u7684\u968e\u6bb5\u8cc7\u6599\u96c6\uff0c\u7b2c 2 \u968e\u6bb5\u5f8c\u7684 LLM \u4efb\u52d9\u80fd\u529b\u6c92\u6709\u4e0b\u964d\u3002\u76f8\u53cd\uff0c\u7576\u968e\u6bb5\u8cc7\u6599\u96c6\u4e0d\u76f8\u4f3c\u7684\uff0cLLM \u7684\u4efb\u52d9\u80fd\u529b\u6703\u4e0b\u964d\u3002\u6211\u5011\u4f7f\u7528\u591a\u500b\u968e\u6bb5\u8cc7\u6599\u96c6\u5c0d\u5728\u958b\u6e90 \\mis\\ \u548c \\llm\\ \u6a21\u578b\u4e0a\u6e2c\u8a66\u6211\u5011\u7684\u5047\u8a2d\u3002\u70ba\u4e86\u89e3\u6c7a\u60e1\u5316\u554f\u984c\uff0c\u6211\u5011\u5206\u6790\u4e86\u5169\u7a2e CFT \u65b9\u6cd5\u7684\u5b9a\u5236\u8b8a\u9ad4\uff1a\u5c64\u51cd\u7d50\u548c\u751f\u6210\u5f0f\u91cd\u64ad\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8b49\u660e\u4e86\u5b83\u5011\u5728\u589e\u5f37 LLM \u7684\u8a9e\u8a00\u80fd\u529b\u7684\u540c\u6642\u4fdd\u6301\u4efb\u52d9\u8868\u73fe\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u8207\u76f8\u95dc\u57fa\u6e96\u76f8\u6bd4\u3002</paragraph>", "author": "Divyanshu Aggarwal et.al.", "authors": "Divyanshu Aggarwal, Sankarshan Damle, Navin Goyal, Satya Lokam, Sunayana Sitaram", "id": "2410.16006v1", "paper_url": "http://arxiv.org/abs/2410.16006v1", "repo": "null"}}