{"2502.13913": {"publish_time": "2025-02-19", "title": "How Do LLMs Perform Two-Hop Reasoning in Context?", "paper_summary": "\"Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\"\nThis classical example demonstrates two-hop reasoning, where a conclusion\nlogically follows from two connected premises. While transformer-based Large\nLanguage Models (LLMs) can make two-hop reasoning, they tend to collapse to\nrandom guessing when faced with distracting premises. To understand the\nunderlying mechanism, we train a three-layer transformer on synthetic two-hop\nreasoning tasks. The training dynamics show two stages: a slow learning phase,\nwhere the 3-layer transformer performs random guessing like LLMs, followed by\nan abrupt phase transitions, where the 3-layer transformer suddenly reaches\n$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms\nfor how models learn to randomly guess between distractions initially, and how\nthey learn to ignore distractions eventually. We further propose a\nthree-parameter model that supports the causal claims for the mechanisms to the\ntraining dynamics of the transformer. Finally, experiments on LLMs suggest that\nthe discovered mechanisms generalize across scales. Our methodologies provide\nnew perspectives for scientific understandings of LLMs and our findings provide\nnew insights into how reasoning emerges during training.", "paper_summary_zh": "\u300c\u8607\u683c\u62c9\u5e95\u662f\u4eba\u3002\u6240\u6709\u4eba\u985e\u90fd\u662f\u6703\u6b7b\u7684\u3002\u56e0\u6b64\uff0c\u8607\u683c\u62c9\u5e95\u6703\u6b7b\u3002\u300d\n\u9019\u500b\u7d93\u5178\u7bc4\u4f8b\u5c55\u793a\u4e86\u5169\u6b65\u63a8\u7406\uff0c\u7d50\u8ad6\u908f\u8f2f\u5730\u5f9e\u5169\u500b\u76f8\u95dc\u7684\u524d\u63d0\u4e2d\u63a8\u5c0e\u51fa\u4f86\u3002\u96d6\u7136\u57fa\u65bc\u8f49\u63db\u5668\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u9032\u884c\u5169\u6b65\u63a8\u7406\uff0c\u4f46\u5b83\u5011\u5728\u9762\u5c0d\u5206\u6563\u6ce8\u610f\u529b\u7684\u524d\u63d0\u6642\u5f80\u5f80\u6703\u9677\u5165\u96a8\u6a5f\u731c\u6e2c\u3002\u70ba\u4e86\u4e86\u89e3\u5176\u5e95\u5c64\u6a5f\u5236\uff0c\u6211\u5011\u5728\u5408\u6210\u5169\u6b65\u63a8\u7406\u4efb\u52d9\u4e0a\u8a13\u7df4\u4e86\u4e00\u500b\u4e09\u5c64\u8f49\u63db\u5668\u3002\u8a13\u7df4\u52d5\u614b\u986f\u793a\u4e86\u5169\u500b\u968e\u6bb5\uff1a\u4e00\u500b\u7de9\u6162\u7684\u5b78\u7fd2\u968e\u6bb5\uff0c\u5176\u4e2d 3 \u5c64\u8f49\u63db\u5668\u50cf LLM \u4e00\u6a23\u9032\u884c\u96a8\u6a5f\u731c\u6e2c\uff0c\u7136\u5f8c\u662f\u4e00\u500b\u7a81\u7136\u7684\u968e\u6bb5\u8f49\u63db\uff0c\u5176\u4e2d 3 \u5c64\u8f49\u63db\u5668\u7a81\u7136\u9054\u5230 100% \u7684\u6e96\u78ba\u5ea6\u3002\u901a\u904e\u9006\u5411\u5de5\u7a0b\uff0c\u6211\u5011\u89e3\u91cb\u4e86\u6a21\u578b\u6700\u521d\u5982\u4f55\u5728\u5206\u5fc3\u4e2d\u96a8\u6a5f\u731c\u6e2c\u7684\u5167\u90e8\u6a5f\u5236\uff0c\u4ee5\u53ca\u5b83\u5011\u5982\u4f55\u6700\u7d42\u5b78\u6703\u5ffd\u7565\u5206\u5fc3\u7684\u3002\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e86\u652f\u6301\u8f49\u63db\u5668\u8a13\u7df4\u52d5\u614b\u6a5f\u5236\u7684\u56e0\u679c\u4e3b\u5f35\u7684\u4e09\u53c3\u6578\u6a21\u578b\u3002\u6700\u5f8c\uff0c\u91dd\u5c0d LLM \u7684\u5be6\u9a57\u8868\u660e\uff0c\u767c\u73fe\u7684\u6a5f\u5236\u5728\u4e0d\u540c\u898f\u6a21\u4e0a\u662f\u901a\u7528\u7684\u3002\u6211\u5011\u7684\u6280\u8853\u70ba\u79d1\u5b78\u7406\u89e3 LLM \u63d0\u4f9b\u4e86\u65b0\u7684\u89c0\u9ede\uff0c\u6211\u5011\u7684\u767c\u73fe\u70ba\u63a8\u7406\u5982\u4f55\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u51fa\u73fe\u63d0\u4f9b\u4e86\u65b0\u7684\u898b\u89e3\u3002", "author": "Tianyu Guo et.al.", "authors": "Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell", "id": "2502.13913v1", "paper_url": "http://arxiv.org/abs/2502.13913v1", "repo": "null"}}