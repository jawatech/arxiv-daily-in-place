{"2411.01738": {"publish_time": "2024-11-04", "title": "xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism", "paper_summary": "Diffusion models are pivotal for generating high-quality images and videos.\nInspired by the success of OpenAI's Sora, the backbone of diffusion models is\nevolving from U-Net to Transformer, known as Diffusion Transformers (DiTs).\nHowever, generating high-quality content necessitates longer sequence lengths,\nexponentially increasing the computation required for the attention mechanism,\nand escalating DiTs inference latency. Parallel inference is essential for\nreal-time DiTs deployments, but relying on a single parallel method is\nimpractical due to poor scalability at large scales. This paper introduces\nxDiT, a comprehensive parallel inference engine for DiTs. After thoroughly\ninvestigating existing DiTs parallel approaches, xDiT chooses Sequence Parallel\n(SP) and PipeFusion, a novel Patch-level Pipeline Parallel method, as\nintra-image parallel strategies, alongside CFG parallel for inter-image\nparallelism. xDiT can flexibly combine these parallel approaches in a hybrid\nmanner, offering a robust and scalable solution. Experimental results on two\n8xL40 GPUs (PCIe) nodes interconnected by Ethernet and an 8xA100 (NVLink) node\nshowcase xDiT's exceptional scalability across five state-of-the-art DiTs.\nNotably, we are the first to demonstrate DiTs scalability on Ethernet-connected\nGPU clusters. xDiT is available at https://github.com/xdit-project/xDiT.", "paper_summary_zh": "\u64f4\u6563\u6a21\u578b\u5c0d\u65bc\u751f\u6210\u9ad8\u54c1\u8cea\u5f71\u50cf\u548c\u5f71\u7247\u81f3\u95dc\u91cd\u8981\u3002\n\u53d7 OpenAI \u7684 Sora \u6210\u529f\u555f\u767c\uff0c\u64f4\u6563\u6a21\u578b\u7684\u9aa8\u5e79\u5f9e U-Net \u6f14\u9032\u5230 Transformer\uff0c\u7a31\u70ba\u64f4\u6563 Transformer\uff08DiT\uff09\u3002\n\u7136\u800c\uff0c\u751f\u6210\u9ad8\u54c1\u8cea\u5167\u5bb9\u9700\u8981\u66f4\u9577\u7684\u5e8f\u5217\u9577\u5ea6\uff0c\u9019\u6703\u4f7f\u6ce8\u610f\u529b\u6a5f\u5236\u6240\u9700\u7684\u904b\u7b97\u5448\u6307\u6578\u6210\u9577\uff0c\u4e26\u589e\u52a0 DiT \u63a8\u8ad6\u5ef6\u9072\u3002\u4e26\u884c\u63a8\u8ad6\u5c0d\u65bc DiT \u7684\u5373\u6642\u90e8\u7f72\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u7531\u65bc\u5728\u5927\u578b\u898f\u6a21\u4e0b\u53ef\u64f4\u5145\u6027\u4e0d\u4f73\uff0c\u56e0\u6b64\u4f9d\u8cf4\u55ae\u4e00\u4e26\u884c\u65b9\u6cd5\u4e26\u4e0d\u5be6\u969b\u3002\u672c\u6587\u4ecb\u7d39 xDiT\uff0c\u9019\u662f\u4e00\u500b DiT \u7684\u5168\u9762\u4e26\u884c\u63a8\u8ad6\u5f15\u64ce\u3002\u5728\u5fb9\u5e95\u8abf\u67e5\u73fe\u6709\u7684 DiT \u4e26\u884c\u65b9\u6cd5\u5f8c\uff0cxDiT \u9078\u64c7\u5e8f\u5217\u4e26\u884c\uff08SP\uff09\u548c PipeFusion\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u5340\u584a\u7d1a\u7ba1\u9053\u4e26\u884c\u65b9\u6cd5\uff0c\u4f5c\u70ba\u5f71\u50cf\u5167\u4e26\u884c\u7b56\u7565\uff0c\u4ee5\u53ca CFG \u4e26\u884c\u4f5c\u70ba\u5f71\u50cf\u9593\u4e26\u884c\u3002xDiT \u53ef\u4ee5\u9748\u6d3b\u5730\u4ee5\u6df7\u5408\u65b9\u5f0f\u7d50\u5408\u9019\u4e9b\u4e26\u884c\u65b9\u6cd5\uff0c\u63d0\u4f9b\u5f37\u5065\u4e14\u53ef\u64f4\u5145\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u5728\u5169\u500b\u900f\u904e\u4e59\u592a\u7db2\u8def\u4e92\u9023\u7684 8xL40 GPU\uff08PCIe\uff09\u7bc0\u9ede\u548c\u4e00\u500b 8xA100\uff08NVLink\uff09\u7bc0\u9ede\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u5c55\u793a\u4e86 xDiT \u5728\u4e94\u500b\u6700\u5148\u9032\u7684 DiT \u4e0a\u7684\u51fa\u8272\u53ef\u64f4\u5145\u6027\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u662f\u7b2c\u4e00\u500b\u5c55\u793a DiT \u5728\u4e59\u592a\u7db2\u8def\u9023\u63a5\u7684 GPU \u96c6\u7fa4\u4e0a\u53ef\u64f4\u5145\u6027\u7684\u4eba\u3002xDiT \u53ef\u5728 https://github.com/xdit-project/xDiT \u53d6\u5f97\u3002", "author": "Jiarui Fang et.al.", "authors": "Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, Jiannan Wang", "id": "2411.01738v1", "paper_url": "http://arxiv.org/abs/2411.01738v1", "repo": "https://github.com/xdit-project/xdit"}}