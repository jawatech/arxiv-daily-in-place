
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-25**|**EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data**|Jesse Zhang et.al.|[2406.17768v1](http://arxiv.org/abs/2406.17768v1)|null|
|**2024-06-25**|**BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning**|Ercong Nie et.al.|[2406.17764v1](http://arxiv.org/abs/2406.17764v1)|null|
|**2024-06-25**|**DiffusionPDE: Generative PDE-Solving Under Partial Observation**|Jiahe Huang et.al.|[2406.17763v1](http://arxiv.org/abs/2406.17763v1)|null|
|**2024-06-25**|**CaLMQA: Exploring culturally specific long-form question answering across 23 languages**|Shane Arora et.al.|[2406.17761v1](http://arxiv.org/abs/2406.17761v1)|[link](https://github.com/2015aroras/calmqa)|
|**2024-06-25**|**Accelerating Clinical Evidence Synthesis with Large Language Models**|Zifeng Wang et.al.|[2406.17755v1](http://arxiv.org/abs/2406.17755v1)|null|
|**2024-06-25**|**Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language**|Amalie Brogaard Pauli et.al.|[2406.17753v1](http://arxiv.org/abs/2406.17753v1)|null|
|**2024-06-25**|**Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon**|USVSN Sai Prashanth et.al.|[2406.17746v1](http://arxiv.org/abs/2406.17746v1)|null|
|**2024-06-25**|**Following Length Constraints in Instructions**|Weizhe Yuan et.al.|[2406.17744v1](http://arxiv.org/abs/2406.17744v1)|null|
|**2024-06-25**|**Point-SAM: Promptable 3D Segmentation Model for Point Clouds**|Yuchen Zhou et.al.|[2406.17741v1](http://arxiv.org/abs/2406.17741v1)|[link](https://github.com/zyc00/point-sam)|
|**2024-06-25**|**Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning**|Arijit Sehanobish et.al.|[2406.17740v1](http://arxiv.org/abs/2406.17740v1)|null|
|**2024-06-25**|**Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model**|Fei Xia et.al.|[2406.17739v1](http://arxiv.org/abs/2406.17739v1)|null|
|**2024-06-25**|**LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users**|Elinor Poole-Dayan et.al.|[2406.17737v1](http://arxiv.org/abs/2406.17737v1)|null|
|**2024-06-25**|**ViANLI: Adversarial Natural Language Inference for Vietnamese**|Tin Van Huynh et.al.|[2406.17716v1](http://arxiv.org/abs/2406.17716v1)|null|
|**2024-06-25**|**Compositional Models for Estimating Causal Effects**|Purva Pruthi et.al.|[2406.17714v1](http://arxiv.org/abs/2406.17714v1)|null|
|**2024-06-25**|**Data curation via joint example selection further accelerates multimodal learning**|Talfan Evans et.al.|[2406.17711v1](http://arxiv.org/abs/2406.17711v1)|null|
|**2024-06-25**|**FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model**|Feijie Wu et.al.|[2406.17706v1](http://arxiv.org/abs/2406.17706v1)|null|
|**2024-06-25**|**HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction**|Xi Xiao et.al.|[2406.17697v1](http://arxiv.org/abs/2406.17697v1)|null|
|**2024-06-25**|**From Distributional to Overton Pluralism: Investigating Large Language Model Alignment**|Thom Lake et.al.|[2406.17692v1](http://arxiv.org/abs/2406.17692v1)|[link](https://github.com/thomlake/investigating-alignment)|
|**2024-06-25**|**Unified Auto-Encoding with Masked Diffusion**|Philippe Hansen-Estruch et.al.|[2406.17688v1](http://arxiv.org/abs/2406.17688v1)|null|
|**2024-06-25**|**VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation**|Kun Qian et.al.|[2406.17681v2](http://arxiv.org/abs/2406.17681v2)|[link](https://github.com/qbetterk/VarBench)|
|**2024-06-25**|**Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models**|Yuan Li et.al.|[2406.17675v1](http://arxiv.org/abs/2406.17675v1)|null|
|**2024-06-25**|**This Paper Had the Smartest Reviewers -- Flattery Detection Utilising an Audio-Textual Transformer-Based Approach**|Lukas Christ et.al.|[2406.17667v1](http://arxiv.org/abs/2406.17667v1)|null|
|**2024-06-25**|**LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic**|Aditya Kalyanpur et.al.|[2406.17663v1](http://arxiv.org/abs/2406.17663v1)|null|
|**2024-06-25**|**DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning**|Xiaohan Zhang et.al.|[2406.17659v1](http://arxiv.org/abs/2406.17659v1)|null|
|**2024-06-25**|**MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for Multi-View 3D Object Detection**|Michelle Adeline et.al.|[2406.17654v1](http://arxiv.org/abs/2406.17654v1)|null|
|**2024-06-25**|**Leveraging Large Language Models for Software Model Completion: Results from Industrial and Public Datasets**|Christof Tinnes et.al.|[2406.17651v1](http://arxiv.org/abs/2406.17651v1)|null|
|**2024-06-25**|**ELIZA Reinterpreted: The world's first chatbot was not intended as a chatbot at all**|Jeff Shrager et.al.|[2406.17650v1](http://arxiv.org/abs/2406.17650v1)|null|
|**2024-06-25**|**Variationist: Exploring Multifaceted Variation and Bias in Written Language Data**|Alan Ramponi et.al.|[2406.17647v1](http://arxiv.org/abs/2406.17647v1)|null|
|**2024-06-25**|**Banishing LLM Hallucinations Requires Rethinking Generalization**|Johnny Li et.al.|[2406.17642v1](http://arxiv.org/abs/2406.17642v1)|null|
|**2024-06-25**|**BayTTA: Uncertainty-aware medical image classification with optimized test-time augmentation using Bayesian model averaging**|Zeinab Sherkatghanad et.al.|[2406.17640v1](http://arxiv.org/abs/2406.17640v1)|null|
|**2024-06-25**|**Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP**|Sedigheh Eslami et.al.|[2406.17639v2](http://arxiv.org/abs/2406.17639v2)|null|
|**2024-06-25**|**Aligning Diffusion Models with Noise-Conditioned Perception**|Alexander Gambashidze et.al.|[2406.17636v1](http://arxiv.org/abs/2406.17636v1)|null|
|**2024-06-25**|**Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels**|Nicholas Pangakis et.al.|[2406.17633v1](http://arxiv.org/abs/2406.17633v1)|null|
|**2024-06-25**|**CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference**|Erxin Yu et.al.|[2406.17626v1](http://arxiv.org/abs/2406.17626v1)|null|
|**2024-06-25**|**Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models**|Zhiyuan Wen et.al.|[2406.17624v1](http://arxiv.org/abs/2406.17624v1)|null|
|**2024-06-25**|**Towards Building an End-to-End Multilingual Automatic Lyrics Transcription Model**|Jiawen Huang et.al.|[2406.17618v1](http://arxiv.org/abs/2406.17618v1)|null|
|**2024-06-25**|**Aligning Programming Language and Natural Language: Exploring Design Choices in Multi-Modal Transformer-Based Embedding for Bug Localization**|Partha Chakraborty et.al.|[2406.17615v1](http://arxiv.org/abs/2406.17615v1)|[link](https://zenodo.org/record/10519746)|
|**2024-06-25**|**Diffusion-based Adversarial Purification for Intrusion Detection**|Mohamed Amine Merzouk et.al.|[2406.17606v1](http://arxiv.org/abs/2406.17606v1)|null|
|**2024-06-25**|**"Seeing the Big through the Small": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?**|Beiduo Chen et.al.|[2406.17600v1](http://arxiv.org/abs/2406.17600v1)|null|
|**2024-06-25**|**LongIns: A Challenging Long-context Instruction-based Exam for LLMs**|Shawn Gavin et.al.|[2406.17588v2](http://arxiv.org/abs/2406.17588v2)|null|
|**2024-06-25**|**Towards Compositional Interpretability for XAI**|Sean Tull et.al.|[2406.17583v1](http://arxiv.org/abs/2406.17583v1)|null|
|**2024-06-25**|**Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for Querying and Classifying IoT Threats**|Ryan Pavlich et.al.|[2406.17574v1](http://arxiv.org/abs/2406.17574v1)|null|
|**2024-06-25**|**FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating Toxicity in French Texts**|Caroline Brun et.al.|[2406.17566v1](http://arxiv.org/abs/2406.17566v1)|null|
|**2024-06-25**|**Multi-property Steering of Large Language Models with Dynamic Activation Composition**|Daniel Scalena et.al.|[2406.17563v1](http://arxiv.org/abs/2406.17563v1)|null|
|**2024-06-25**|**The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale**|Guilherme Penedo et.al.|[2406.17557v1](http://arxiv.org/abs/2406.17557v1)|null|
|**2024-06-25**|**Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft**|Chalamalasetti Kranti et.al.|[2406.17553v1](http://arxiv.org/abs/2406.17553v1)|null|
|**2024-06-25**|**CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained Models using Greedy Coordinate Descent**|Pranav Ajit Nair et.al.|[2406.17542v2](http://arxiv.org/abs/2406.17542v2)|null|
|**2024-06-25**|**SincVAE: a New Approach to Improve Anomaly Detection on EEG Data Using SincNet and Variational Autoencoder**|Andrea Pollastro et.al.|[2406.17537v1](http://arxiv.org/abs/2406.17537v1)|null|
|**2024-06-25**|**Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark**|Fabio Mercorio et.al.|[2406.17535v1](http://arxiv.org/abs/2406.17535v1)|null|
|**2024-06-25**|**Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification**|Huiyao Chen et.al.|[2406.17534v1](http://arxiv.org/abs/2406.17534v1)|null|
|**2024-06-25**|**Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study**|Keyu Wang et.al.|[2406.17532v1](http://arxiv.org/abs/2406.17532v1)|null|
|**2024-06-25**|**Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness**|Lucrezia Grassi et.al.|[2406.17531v1](http://arxiv.org/abs/2406.17531v1)|null|
|**2024-06-25**|**LumberChunker: Long-Form Narrative Document Segmentation**|André V. Duarte et.al.|[2406.17526v1](http://arxiv.org/abs/2406.17526v1)|[link](https://github.com/joaodsmarques/lumberchunker)|
|**2024-06-25**|**Entropy-Based Decoding for Retrieval-Augmented Large Language Models**|Zexuan Qiu et.al.|[2406.17519v1](http://arxiv.org/abs/2406.17519v1)|null|
|**2024-06-25**|**Benchmarking Mental State Representations in Language Models**|Matteo Bortoletto et.al.|[2406.17513v1](http://arxiv.org/abs/2406.17513v1)|null|
|**2024-06-25**|**MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation**|Yusheng Liao et.al.|[2406.17484v1](http://arxiv.org/abs/2406.17484v1)|null|
|**2024-06-25**|**Transformer-based Named Entity Recognition with Combined Data Representation**|Michał Marcińczuk et.al.|[2406.17474v1](http://arxiv.org/abs/2406.17474v1)|null|
|**2024-06-25**|**TSynD: Targeted Synthetic Data Generation for Enhanced Medical Image Classification**|Joshua Niemeijer et.al.|[2406.17473v1](http://arxiv.org/abs/2406.17473v1)|null|
|**2024-06-25**|**Dynamic Scheduling for Vehicle-to-Vehicle Communications Enhanced Federated Learning**|Jintao Yan et.al.|[2406.17470v1](http://arxiv.org/abs/2406.17470v1)|null|
|**2024-06-25**|**Enhancing Tool Retrieval with Iterative Feedback from Large Language Models**|Qiancheng Xu et.al.|[2406.17465v1](http://arxiv.org/abs/2406.17465v1)|null|
|**2024-06-25**|**The Tree of Diffusion Life: Evolutionary Embeddings to Understand the Generation Process of Diffusion Models**|Vidya Prasad et.al.|[2406.17462v1](http://arxiv.org/abs/2406.17462v1)|null|
|**2024-06-25**|**Improving Grammatical Error Correction via Contextual Data Augmentation**|Yixuan Wang et.al.|[2406.17456v1](http://arxiv.org/abs/2406.17456v1)|null|
|**2024-06-25**|**Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain**|Davide Mazzaccara et.al.|[2406.17453v1](http://arxiv.org/abs/2406.17453v1)|null|
|**2024-06-25**|**Pseudo Labelling for Enhanced Masked Autoencoders**|Srinivasa Rao Nandam et.al.|[2406.17450v1](http://arxiv.org/abs/2406.17450v1)|null|
|**2024-06-25**|**Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights**|Hao Yang et.al.|[2406.17430v1](http://arxiv.org/abs/2406.17430v1)|null|
|**2024-06-25**|**CuDA2: An approach for Incorporating Traitor Agents into Cooperative Multi-Agent Systems**|Zhen Chen et.al.|[2406.17425v1](http://arxiv.org/abs/2406.17425v1)|null|
|**2024-06-25**|**Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA**|Minzheng Wang et.al.|[2406.17419v1](http://arxiv.org/abs/2406.17419v1)|[link](https://github.com/mozerwang/loong)|
|**2024-06-25**|**Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels**|Razvan-Gabriel Dumitru et.al.|[2406.17415v2](http://arxiv.org/abs/2406.17415v2)|null|
|**2024-06-25**|**Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training**|Yixuan Wang et.al.|[2406.17404v1](http://arxiv.org/abs/2406.17404v1)|null|
|**2024-06-25**|**Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance**|Manon Reusens et.al.|[2406.17385v1](http://arxiv.org/abs/2406.17385v1)|null|
|**2024-06-25**|**A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens**|Zhijie Nie et.al.|[2406.17378v1](http://arxiv.org/abs/2406.17378v1)|null|
|**2024-06-25**|**A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual LLMs**|Vaibhav Singh et.al.|[2406.17377v1](http://arxiv.org/abs/2406.17377v1)|null|
|**2024-06-25**|**Temporal-Channel Modeling in Multi-head Self-Attention for Synthetic Speech Detection**|Duc-Tuan Truong et.al.|[2406.17376v1](http://arxiv.org/abs/2406.17376v1)|[link](https://github.com/ductuantruong/tcm_add)|
|**2024-06-25**|**An Empirical Study on the Characteristics of Bias upon Context Length Variation for Bangla**|Jayanta Sadhu et.al.|[2406.17375v1](http://arxiv.org/abs/2406.17375v1)|null|
|**2024-06-25**|**Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech Translation**|Yasmin Moslem et.al.|[2406.17363v1](http://arxiv.org/abs/2406.17363v1)|null|
|**2024-06-25**|**Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers**|Lei Chen et.al.|[2406.17343v1](http://arxiv.org/abs/2406.17343v1)|null|
|**2024-06-25**|**Masked Generative Extractor for Synergistic Representation and 3D Generation of Point Clouds**|Hongliang Zeng et.al.|[2406.17342v1](http://arxiv.org/abs/2406.17342v1)|null|
|**2024-06-25**|**Joint Admission Control and Resource Allocation of Virtual Network Embedding via Hierarchical Deep Reinforcement Learning**|Tianfu Wang et.al.|[2406.17334v1](http://arxiv.org/abs/2406.17334v1)|[link](https://github.com/geminilight/hrl-acra)|
|**2024-06-25**|**Dual-Space Knowledge Distillation for Large Language Models**|Songming Zhang et.al.|[2406.17328v1](http://arxiv.org/abs/2406.17328v1)|[link](https://github.com/songmzhang/dskd)|
|**2024-06-25**|**Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy**|Simone Astarita et.al.|[2406.17324v1](http://arxiv.org/abs/2406.17324v1)|null|
|**2024-06-25**|**Retrieval Augmented Instruction Tuning for Open NER with Large Language Models**|Tingyu Xie et.al.|[2406.17305v1](http://arxiv.org/abs/2406.17305v1)|[link](https://github.com/emma1066/retrieval-augmented-it-openner)|
|**2024-06-25**|**Leveraging LLMs for Dialogue Quality Measurement**|Jinghan Jia et.al.|[2406.17304v1](http://arxiv.org/abs/2406.17304v1)|null|
|**2024-06-25**|**Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models**|Wenhao Shi et.al.|[2406.17294v2](http://arxiv.org/abs/2406.17294v2)|null|
|**2024-06-25**|**Hyperbolic Knowledge Transfer in Cross-Domain Recommendation System**|Xin Yang et.al.|[2406.17289v1](http://arxiv.org/abs/2406.17289v1)|null|
|**2024-06-25**|**Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models**|Yang Yan et.al.|[2406.17287v1](http://arxiv.org/abs/2406.17287v1)|[link](https://github.com/kuri-leo/bigfive-llm-predictor)|
|**2024-06-25**|**SetBERT: Enhancing Retrieval Performance for Boolean Logic and Set Operation Queries**|Quan Mai et.al.|[2406.17282v2](http://arxiv.org/abs/2406.17282v2)|null|
|**2024-06-25**|**OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure**|Jikai Wang et.al.|[2406.17276v1](http://arxiv.org/abs/2406.17276v1)|[link](https://github.com/jikai0wang/opt-tree)|
|**2024-06-25**|**Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?**|Jianfeng He et.al.|[2406.17274v1](http://arxiv.org/abs/2406.17274v1)|null|
|**2024-06-25**|**DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph**|Zhehao Zhang et.al.|[2406.17271v1](http://arxiv.org/abs/2406.17271v1)|[link](https://github.com/salt-nlp/darg)|
|**2024-06-25**|**AG-LSEC: Audio Grounded Lexical Speaker Error Correction**|Rohit Paturi et.al.|[2406.17266v1](http://arxiv.org/abs/2406.17266v1)|null|
|**2024-06-25**|**D2LLM: Decomposed and Distilled Large Language Models for Semantic Search**|Zihan Liao et.al.|[2406.17262v1](http://arxiv.org/abs/2406.17262v1)|null|
|**2024-06-25**|**TRAWL: Tensor Reduced and Approximated Weights for Large Language Models**|Yiran Luo et.al.|[2406.17261v1](http://arxiv.org/abs/2406.17261v1)|null|
|**2024-06-25**|**Mitigating Hallucination in Fictional Character Role-Play**|Nafis Sadeq et.al.|[2406.17260v1](http://arxiv.org/abs/2406.17260v1)|null|
|**2024-06-25**|**Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual Text-to-Speech Adaptation**|Yingting Li et.al.|[2406.17257v1](http://arxiv.org/abs/2406.17257v1)|null|
|**2024-06-25**|**MPCODER: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning**|Zhenlong Dai et.al.|[2406.17255v1](http://arxiv.org/abs/2406.17255v1)|null|
|**2024-06-25**|**How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?**|Huaizhi Ge et.al.|[2406.17253v1](http://arxiv.org/abs/2406.17253v1)|null|
|**2024-06-25**|**TopoGCL: Topological Graph Contrastive Learning**|Yuzhou Chen et.al.|[2406.17251v1](http://arxiv.org/abs/2406.17251v1)|[link](https://github.com/topogclaaai24/topogcl)|
|**2024-06-25**|**Beyond Silence: Bias Analysis through Loss and Asymmetric Approach in Audio Anti-Spoofing**|Hye-jin Shim et.al.|[2406.17246v1](http://arxiv.org/abs/2406.17246v1)|null|
|**2024-06-25**|**Unlocking Continual Learning Abilities in Language Models**|Wenyu Du et.al.|[2406.17245v1](http://arxiv.org/abs/2406.17245v1)|[link](https://github.com/wenyudu/migu)|
|**2024-06-25**|**What Do the Circuits Mean? A Knowledge Edit View**|Huaizhi Ge et.al.|[2406.17241v1](http://arxiv.org/abs/2406.17241v1)|null|

#### Abstracts
##### **EXTRACT: Efficient Policy Learning by Extracting Transferrable Robot Skills from Offline Data**
2406.17768v1 by Jesse Zhang, Minho Heo, Zuxin Liu, Erdem Biyik, Joseph J Lim, Yao Liu, Rasool Fakoor

Most reinforcement learning (RL) methods focus on learning optimal policies
over low-level action spaces. While these methods can perform well in their
training environments, they lack the flexibility to transfer to new tasks.
Instead, RL agents that can act over useful, temporally extended skills rather
than low-level actions can learn new tasks more easily. Prior work in
skill-based RL either requires expert supervision to define useful skills,
which is hard to scale, or learns a skill-space from offline data with
heuristics that limit the adaptability of the skills, making them difficult to
transfer during downstream RL. Our approach, EXTRACT, instead utilizes
pre-trained vision language models to extract a discrete set of semantically
meaningful skills from offline data, each of which is parameterized by
continuous arguments, without human supervision. This skill parameterization
allows robots to learn new tasks by only needing to learn when to select a
specific skill and how to modify its arguments for the specific task. We
demonstrate through experiments in sparse-reward, image-based, robot
manipulation environments that EXTRACT can more quickly learn new tasks than
prior works, with major gains in sample efficiency and performance over prior
skill-based RL. Website at https://www.jessezhang.net/projects/extract/.

摘要：大多数强化学习 (RL) 方法专注于学习低级动作空间上的最优策略。虽然这些方法可以在其训练环境中表现良好，但它们缺乏转移到新任务的灵活性。相反，能够作用于有用、时间延长的技能而不是低级动作的 RL 代理可以更容易地学习新任务。基于技能的 RL 中的先前工作要么需要专家监督来定义有用的技能（很难扩展），要么从离线数据中学习技能空间，而启发式方法限制了技能的适应性，使其难以在后端 RL 期间转移。我们的方法 EXTRACT 则利用预训练的视觉语言模型从离线数据中提取一组离散的语义有意义的技能，每个技能都由连续参数参数化，无需人工监督。这种技能参数化允许机器人通过仅需学习何时选择特定技能以及如何修改其参数以适应特定任务来学习新任务。我们通过在稀疏奖励、基于图像的机器人操作环境中的实验表明，EXTRACT 可以比以前的工作更快地学习新任务，在样本效率和性能方面取得了比以前基于技能的 RL 的重大收益。网站位于 https://www.jessezhang.net/projects/extract/。

##### **BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning**
2406.17764v1 by Ercong Nie, Bo Shao, Zifeng Ding, Mingyang Wang, Helmut Schmid, Hinrich Schütze

Large language models (LLMs) possess extensive parametric knowledge, but this
knowledge is difficult to update with new information because retraining is
very expensive and infeasible for closed-source models. Knowledge editing (KE)
has emerged as a viable solution for updating the knowledge of LLMs without
compromising their overall performance. On-the-fly KE methods, inspired by
in-context learning (ICL), have shown great promise and allow LLMs to be
treated as black boxes. In the past, KE was primarily employed in English
contexts, whereas the potential for cross-lingual KE in current English-centric
LLMs has not been fully explored. To foster more research in this direction, we
introduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse
languages across three KE task types. We also propose a gradient-free KE method
called Multilingual In-context Knowledge Editing (MIKE) and evaluate it on
BMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms
of reliability, generality, locality, and portability, offering valuable
insights and a framework for future research in cross-lingual KE. Our code and
data are publicly accessible via the anonymous repository at
https://anonymous.4open.science/r/MIKE.

摘要：大型語言模型 (LLM) 擁有廣泛的參數化知識，但由於重新訓練非常昂貴且對閉源模型不可行，因此難以使用新資訊更新此知識。知識編輯 (KE) 已成為在不影響 LLM 整體效能的情況下更新其知識的可行解決方案。受情境學習 (ICL) 啟發的即時 KE 方法已展現極大潛力，並允許將 LLM 視為黑盒子。過去，KE 主要用於英語情境，而目前以英語為中心的 LLM 中跨語言 KE 的潛力尚未得到充分探索。為了促進更多這方面的研究，我們引入了 BMIKE-53 基準，用於評估 53 種不同語言在三種 KE 任務類型中的跨語言 KE。我們還提出了一種稱為多語言情境知識編輯 (MIKE) 的無梯度 KE 方法，並在 BMIKE-53 上對其進行評估。我們的評估重點在於跨語言知識轉移的可靠性、普遍性、局部性和可移植性，為跨語言 KE 的未來研究提供有價值的見解和框架。我們的程式碼和資料可透過 https://anonymous.4open.science/r/MIKE 上的匿名儲存庫公開存取。

##### **DiffusionPDE: Generative PDE-Solving Under Partial Observation**
2406.17763v1 by Jiahe Huang, Guandao Yang, Zichen Wang, Jeong Joon Park

We introduce a general framework for solving partial differential equations
(PDEs) using generative diffusion models. In particular, we focus on the
scenarios where we do not have the full knowledge of the scene necessary to
apply classical solvers. Most existing forward or inverse PDE approaches
perform poorly when the observations on the data or the underlying coefficients
are incomplete, which is a common assumption for real-world measurements. In
this work, we propose DiffusionPDE that can simultaneously fill in the missing
information and solve a PDE by modeling the joint distribution of the solution
and coefficient spaces. We show that the learned generative priors lead to a
versatile framework for accurately solving a wide range of PDEs under partial
observation, significantly outperforming the state-of-the-art methods for both
forward and inverse directions.

摘要：我們提出一個通用架構，用生成擴散模型解決偏微分方程式 (PDE)。特別是，我們專注於我們沒有場景的完整知識，無法應用經典求解器的情況。當資料或基礎係數的觀測不完整時，現有的正向或反向 PDE 方法大多表現不佳，這是現實世界測量的一個常見假設。在這項工作中，我們提出 DiffusionPDE，它可以同時填補遺失的資訊並透過對解和係數空間的聯合分佈建模來求解 PDE。我們展示了學習到的生成先驗導致了一個通用的框架，用於在部分觀測下準確求解各種 PDE，顯著優於正向和反向方向的最新方法。

##### **CaLMQA: Exploring culturally specific long-form question answering across 23 languages**
2406.17761v1 by Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi

Large language models (LLMs) are commonly used for long-form question
answering, which requires them to generate paragraph-length answers to complex
questions. While long-form QA has been well-studied in English via many
different datasets and evaluation metrics, this research has not been extended
to cover most other languages. To bridge this gap, we introduce CaLMQA, a
collection of 2.6K complex questions spanning 23 languages, including
under-resourced, rarely-studied languages such as Fijian and Kirundi. Our
dataset includes both naturally-occurring questions collected from community
web forums as well as questions written by native speakers, whom we hire for
this purpose. Our process yields diverse, complex questions that reflect
cultural topics (e.g. traditions, laws, news) and the language usage of native
speakers. We conduct automatic evaluation across a suite of open- and
closed-source models using our novel metric CaLMScore, which detects incorrect
language and token repetitions in answers, and observe that the quality of
LLM-generated answers degrades significantly for some low-resource languages.
We perform human evaluation on a subset of models and see that model
performance is significantly worse for culturally specific questions than for
culturally agnostic questions. Our findings highlight the need for further
research in LLM multilingual capabilities and non-English LFQA evaluation.

摘要：大型語言模型 (LLM) 常用於長篇問答，這需要它們針對複雜問題產生段落長度的答案。雖然長篇問答已透過許多不同的資料集和評估指標在英文中得到充分研究，但此研究尚未擴展到涵蓋大多數其他語言。為了彌補這個差距，我們引入了 CaLMQA，這是一個包含 2.6K 個複雜問題的集合，涵蓋 23 種語言，包括斐濟語和基隆迪語等資源不足、鮮少研究的語言。我們的資料集包含從社群網路論壇收集到的自然發生問題，以及我們為此目的聘請母語人士撰寫的問題。我們的流程產生了多元、複雜的問題，反映了文化主題（例如傳統、法律、新聞）和母語人士的語言使用。我們使用我們的新穎指標 CaLMScore 對一系列開放和閉源模型進行自動評估，該指標會偵測答案中的不正確語言和重複記號，並觀察到 LLM 生成的答案品質對於某些低資源語言會顯著下降。我們對模型的子集進行人工評估，並發現針對特定文化問題的模型效能顯著低於針對文化不可知問題的模型效能。我們的研究結果突顯了進一步研究 LLM 多語言能力和非英文 LFQA 評估的必要性。

##### **Accelerating Clinical Evidence Synthesis with Large Language Models**
2406.17755v1 by Zifeng Wang, Lang Cao, Benjamin Danek, Yichi Zhang, Qiao Jin, Zhiyong Lu, Jimeng Sun

Automatic medical discovery by AI is a dream of many. One step toward that
goal is to create an AI model to understand clinical studies and synthesize
clinical evidence from the literature. Clinical evidence synthesis currently
relies on systematic reviews of clinical trials and retrospective analyses from
medical literature. However, the rapid expansion of publications presents
challenges in efficiently identifying, summarizing, and updating evidence. We
introduce TrialMind, a generative AI-based pipeline for conducting medical
systematic reviews, encompassing study search, screening, and data extraction
phases. We utilize large language models (LLMs) to drive each pipeline
component while incorporating human expert oversight to minimize errors. To
facilitate evaluation, we also create a benchmark dataset TrialReviewBench, a
custom dataset with 870 annotated clinical studies from 25 meta-analysis papers
across various medical treatments. Our results demonstrate that TrialMind
significantly improves the literature review process, achieving high recall
rates (0.897-1.000) in study searching from over 20 million PubMed studies and
outperforming traditional language model embeddings-based methods in screening
(Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses
direct GPT-4 performance in result extraction, with accuracy ranging from 0.65
to 0.84. We also support clinical evidence synthesis in forest plots, as
validated by eight human annotators who preferred TrialMind over the GPT-4
baseline with a winning rate of 62.5%-100% across the involved reviews. Our
findings suggest that an LLM-based clinical evidence synthesis approach, such
as TrialMind, can enable reliable and high-quality clinical evidence synthesis
to improve clinical research efficiency.

摘要：<paragraph>許多人夢想著由 AI 自動進行醫學發現。朝此目標邁進的一步是建立一個 AI 模型來理解臨床研究，並從文獻中綜合臨床證據。臨床證據綜合目前依賴於臨床試驗的系統性回顧和醫學文獻的回溯性分析。然而，出版物的快速擴張在有效地識別、總結和更新證據方面提出了挑戰。我們介紹了 TrialMind，這是一個基於生成式 AI 的管道，用於進行醫學系統性回顧，包括研究搜尋、篩選和數據萃取階段。我們利用大型語言模型 (LLM) 來驅動每個管道組件，同時結合人類專家監督以最大程度地減少錯誤。為了促進評估，我們還建立了一個基準數據集 TrialReviewBench，這是一個自訂數據集，其中包含來自 25 篇關於各種醫療治療的元分析論文的 870 項註解臨床研究。我們的結果表明，TrialMind 大幅改善了文獻回顧流程，在超過 2000 萬篇 PubMed 研究中達到了很高的召回率 (0.897-1.000)，並在篩選方面優於基於傳統語言模型嵌入的方法（召回率 @20 為 0.227-0.246，相較於 0.000-0.102）。此外，我們的做法在結果萃取方面超越了直接的 GPT-4 效能，準確度範圍從 0.65 到 0.84。我們還支援森林圖中的臨床證據綜合，這點已獲得八位人類註解者的驗證，他們在相關回顧中以 62.5%-100% 的獲勝率偏好 TrialMind，勝過 GPT-4 基準。我們的研究結果表明，基於 LLM 的臨床證據綜合方法（例如 TrialMind）可以實現可靠且高品質的臨床證據綜合，以提高臨床研究效率。</paragraph>

##### **Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language**
2406.17753v1 by Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent

We are exposed to much information trying to influence us, such as teaser
messages, debates, politically framed news, and propaganda - all of which use
persuasive language. With the recent interest in Large Language Models (LLMs),
we study the ability of LLMs to produce persuasive text. As opposed to prior
work which focuses on particular domains or types of persuasion, we conduct a
general study across various domains to measure and benchmark to what degree
LLMs produce persuasive text - both when explicitly instructed to rewrite text
to be more or less persuasive and when only instructed to paraphrase. To this
end, we construct a new dataset, Persuasive-Pairs, of pairs each consisting of
a short text and of a text rewritten by an LLM to amplify or diminish
persuasive language. We multi-annotate the pairs on a relative scale for
persuasive language. This data is not only a valuable resource in itself, but
we also show that it can be used to train a regression model to predict a score
of persuasive language between text pairs. This model can score and benchmark
new LLMs across domains, thereby facilitating the comparison of different LLMs.
Finally, we discuss effects observed for different system prompts. Notably, we
find that different 'personas' in the system prompt of LLaMA3 change the
persuasive language in the text substantially, even when only instructed to
paraphrase. These findings underscore the importance of investigating
persuasive language in LLM generated text.

摘要：<paragraph>我們會接觸到許多試圖影響我們的資訊，例如預告訊息、辯論、政治框架新聞和宣傳，所有這些都使用具說服力的語言。隨著最近對大型語言模型 (LLM) 的興趣，我們研究了 LLM 產生具說服力文字的能力。與專注於特定領域或說服類型的先前研究相反，我們在各種領域進行了一項一般性研究，以衡量和基準 LLM 產生具說服力文字的程度，無論是在明確指示改寫文字以使其更具說服力或更不具說服力時，或僅指示進行同義改寫時。為此，我們構建了一個新資料集 Persuasive-Pairs，其中每一對都包含一段簡短文字和一段由 LLM 改寫的文字，以擴大或減少具說服力的語言。我們以相對量表對這些對進行多重註解，以表示具說服力的語言。這些資料本身不僅是一個有價值的資源，我們還展示了它可用於訓練回歸模型，以預測文本對之間具說服力的語言分數。此模型可以在各個領域對新的 LLM 進行評分和基準測試，從而促進對不同 LLM 的比較。最後，我們討論了對不同系統提示觀察到的影響。值得注意的是，我們發現 LLaMA3 系統提示中的不同「角色」會大幅改變文字中的具說服力語言，即使僅指示進行同義改寫時也是如此。這些發現強調了調查 LLM 生成的文字中具說服力語言的重要性。</paragraph>

##### **Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon**
2406.17746v1 by USVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, Jyothir S V, Mohammad Aflah Khan, Jaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy Ke, Katherine Lee, Naomi Saphra

Memorization in language models is typically treated as a homogenous
phenomenon, neglecting the specifics of the memorized data. We instead model
memorization as the effect of a set of complex factors that describe each
sample and relate it to the model and corpus. To build intuition around these
factors, we break memorization down into a taxonomy: recitation of highly
duplicated sequences, reconstruction of inherently predictable sequences, and
recollection of sequences that are neither. We demonstrate the usefulness of
our taxonomy by using it to construct a predictive model for memorization. By
analyzing dependencies and inspecting the weights of the predictive model, we
find that different factors influence the likelihood of memorization
differently depending on the taxonomic category.

摘要：語言模型中的記憶通常被視為一種同質現象，忽略了記憶數據的具體情況。我們反過來將記憶建模為一組複雜因素的效應，這些因素描述每個範例並將其與模型和語料庫聯繫起來。為了建立對這些因素的直覺，我們將記憶分解為一個分類法：高度重複序列的背誦、固有可預測序列的重建以及既不是也不是的序列的回憶。我們通過使用它來構建記憶的預測模型來證明我們的分類法的有用性。通過分析依賴關係並檢查預測模型的權重，我們發現不同的因素會根據分類類別不同地影響記憶的可能性。

##### **Following Length Constraints in Instructions**
2406.17744v1 by Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, Jing Xu

Aligned instruction following models can better fulfill user requests than
their unaligned counterparts. However, it has been shown that there is a length
bias in evaluation of such models, and that training algorithms tend to exploit
this bias by learning longer responses. In this work we show how to train
models that can be controlled at inference time with instructions containing
desired length constraints. Such models are superior in length instructed
evaluations, outperforming standard instruction following models such as GPT4,
Llama 3 and Mixtral.

摘要：對齊指令遵循模型比其未對齊的模型能更好地滿足用戶的要求。然而，研究表明，此類模型的評估存在長度偏差，而且訓練演算法會利用此偏差，學習更長的回應。在此研究中，我們展示如何訓練模型，使其能在推論時間使用包含所需長度約束的指令進行控制。此類模型在長度指導評估中表現優異，優於 GPT4、Llama 3 和 Mixtral 等標準指令遵循模型。

##### **Point-SAM: Promptable 3D Segmentation Model for Point Clouds**
2406.17741v1 by Yuchen Zhou, Jiayuan Gu, Tung Yen Chiang, Fanbo Xiang, Hao Su

The development of 2D foundation models for image segmentation has been
significantly advanced by the Segment Anything Model (SAM). However, achieving
similar success in 3D models remains a challenge due to issues such as
non-unified data formats, lightweight models, and the scarcity of labeled data
with diverse masks. To this end, we propose a 3D promptable segmentation model
(Point-SAM) focusing on point clouds. Our approach utilizes a transformer-based
method, extending SAM to the 3D domain. We leverage part-level and object-level
annotations and introduce a data engine to generate pseudo labels from SAM,
thereby distilling 2D knowledge into our 3D model. Our model outperforms
state-of-the-art models on several indoor and outdoor benchmarks and
demonstrates a variety of applications, such as 3D annotation. Codes and demo
can be found at https://github.com/zyc00/Point-SAM.

摘要：2D 影像分割基礎模型的發展已因 Segment Anything Model (SAM) 而大幅進展。然而，由於非統一資料格式、輕量級模型，以及標籤資料缺乏多樣化遮罩等問題，在 3D 模型中取得類似的成功仍是一項挑戰。為此，我們提出一個專注於點雲的 3D 可提示式分割模型 (Point-SAM)。我們的方法利用基於 Transformer 的方法，將 SAM 擴展到 3D 領域。我們利用部分級和物件級註解，並引入一個資料引擎從 SAM 產生偽標籤，從而將 2D 知識提煉到我們的 3D 模型中。我們的模型在多個室內和室外基準上優於最先進的模型，並展示了各種應用，例如 3D 註解。程式碼和示範可以在 https://github.com/zyc00/Point-SAM 中找到。

##### **Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning**
2406.17740v1 by Arijit Sehanobish, Avinava Dubey, Krzysztof Choromanski, Somnath Basu Roy Chowdhury, Deepali Jain, Vikas Sindhwani, Snigdha Chaturvedi

Recent efforts to scale Transformer models have demonstrated rapid progress
across a wide range of tasks (Wei et al., 2022). However, fine-tuning these
models for downstream tasks is expensive due to their large parameter counts.
Parameter-efficient fine-tuning (PEFT) approaches have emerged as a viable
alternative by allowing us to fine-tune models by updating only a small number
of parameters. In this work, we propose a general framework for parameter
efficient fine-tuning (PEFT), based on structured unrestricted-rank matrices
(SURM) which can serve as a drop-in replacement for popular approaches such as
Adapters and LoRA. Unlike other methods like LoRA, SURMs provides more
flexibility in finding the right balance between compactness and
expressiveness. This is achieved by using low displacement rank matrices
(LDRMs), which hasn't been used in this context before. SURMs remain
competitive with baselines, often providing significant quality improvements
while using a smaller parameter budget. SURMs achieve 5-7% accuracy gains on
various image classification tasks while replacing low-rank matrices in LoRA.
It also results in up to 12x reduction of the number of parameters in adapters
(with virtually no loss in quality) on the GLUE benchmark.

摘要：最近对 Transformer 模型进行扩展的尝试展示了各种任务的快速进展（Wei 等人，2022 年）。然而，由于这些模型的参数数量庞大，因此为下游任务进行微调非常昂贵。参数高效微调 (PEFT) 方法已经成为一种可行的替代方案，它允许我们仅通过更新少量参数来微调模型。在这项工作中，我们提出了一个基于结构化无限制秩矩阵 (SURM) 的参数高效微调 (PEFT) 的通用框架，该框架可以作为流行方法（例如适配器和 LoRA）的替代方案。与 LoRA 等其他方法不同，SURM 在寻找紧凑性和表现力之间的适当平衡方面提供了更大的灵活性。这是通过使用低置换秩矩阵 (LDRM) 实现的，这在以前从未在该上下文中使用过。SURM 仍然具有与基线相当的竞争力，通常在使用较小参数预算的同时提供显著的质量改进。SURM 在各种图像分类任务中实现了 5-7% 的准确度提升，同时替换了 LoRA 中的低秩矩阵。在 GLUE 基准测试中，它还导致适配器中参数数量减少了 12 倍（几乎没有质量损失）。

##### **Find Parent then Label Children: A Two-stage Taxonomy Completion Method with Pre-trained Language Model**
2406.17739v1 by Fei Xia, Yixuan Weng, Shizhu He, Kang Liu, Jun Zhao

Taxonomies, which organize domain concepts into hierarchical structures, are
crucial for building knowledge systems and downstream applications. As domain
knowledge evolves, taxonomies need to be continuously updated to include new
concepts. Previous approaches have mainly focused on adding concepts to the
leaf nodes of the existing hierarchical tree, which does not fully utilize the
taxonomy's knowledge and is unable to update the original taxonomy structure
(usually involving non-leaf nodes). In this paper, we propose a two-stage
method called ATTEMPT for taxonomy completion. Our method inserts new concepts
into the correct position by finding a parent node and labeling child nodes.
Specifically, by combining local nodes with prompts to generate natural
sentences, we take advantage of pre-trained language models for
hypernym/hyponymy recognition. Experimental results on two public datasets
(including six domains) show that ATTEMPT performs best on both taxonomy
completion and extension tasks, surpassing existing methods.

摘要：分類法會將領域概念組織成階層結構，對於建構知識系統和下游應用程式至關重要。隨著領域知識的演進，分類法需要持續更新才能納入新概念。先前的做法主要集中於將概念新增至現有階層樹狀結構的葉節點，這並未充分利用分類法的知識，也無法更新原始分類法結構（通常涉及非葉節點）。在本文中，我們提出了一種稱為 ATTEMPT 的兩階段方法，用於分類法完成。我們的做法透過尋找父節點和標記子節點，將新概念插入正確位置。具體來說，我們透過將局部節點與提示結合以產生自然句子，利用預先訓練的語言模型來進行上位詞/下位詞辨識。在兩個公開資料集（包括六個領域）上的實驗結果顯示，ATTEMPT 在分類法完成和擴充任務上均表現最佳，超越了現有方法。

##### **LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users**
2406.17737v1 by Elinor Poole-Dayan, Deb Roy, Jad Kabbara

While state-of-the-art Large Language Models (LLMs) have shown impressive
performance on many tasks, there has been extensive research on undesirable
model behavior such as hallucinations and bias. In this work, we investigate
how the quality of LLM responses changes in terms of information accuracy,
truthfulness, and refusals depending on three user traits: English proficiency,
education level, and country of origin. We present extensive experimentation on
three state-of-the-art LLMs and two different datasets targeting truthfulness
and factuality. Our findings suggest that undesirable behaviors in
state-of-the-art LLMs occur disproportionately more for users with lower
English proficiency, of lower education status, and originating from outside
the US, rendering these models unreliable sources of information towards their
most vulnerable users.

摘要：儘管最先進的大語言模型 (LLM) 在許多任務上展現令人印象深刻的表現，但對於幻覺和偏見等不良模型行為，已有廣泛的研究。在本文中，我們探討 LLM 回應的品質如何根據三項使用者特質而改變，包括資訊準確性、真實性和拒絕：英語能力、教育程度和原籍國。我們針對三種最先進的 LLM 進行廣泛的實驗，以及兩個針對真實性和事實性的不同資料集。我們的研究結果顯示，最先進的 LLM 中的不良行為對英語能力較低、教育程度較低以及原籍國不在美國的使用者來說，發生機率不成比例地較高，這使得這些模型對其最弱勢的使用者來說成為不可靠的資訊來源。

##### **ViANLI: Adversarial Natural Language Inference for Vietnamese**
2406.17716v1 by Tin Van Huynh, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen

The development of Natural Language Processing (NLI) datasets and models has
been inspired by innovations in annotation design. With the rapid development
of machine learning models today, the performance of existing machine learning
models has quickly reached state-of-the-art results on a variety of tasks
related to natural language processing, including natural language inference
tasks. By using a pre-trained model during the annotation process, it is
possible to challenge current NLI models by having humans produce
premise-hypothesis combinations that the machine model cannot correctly
predict. To remain attractive and challenging in the research of natural
language inference for Vietnamese, in this paper, we introduce the adversarial
NLI dataset to the NLP research community with the name ViANLI. This data set
contains more than 10K premise-hypothesis pairs and is built by a continuously
adjusting process to obtain the most out of the patterns generated by the
annotators. ViANLI dataset has brought many difficulties to many current SOTA
models when the accuracy of the most powerful model on the test set only
reached 48.4%. Additionally, the experimental results show that the models
trained on our dataset have significantly improved the results on other
Vietnamese NLI datasets.

摘要：自然語言處理 (NLI) 資料集和模型的開發靈感來自於標註設計的創新。隨著當今機器學習模型的快速發展，現有的機器學習模型的效能已迅速在各種與自然語言處理相關的任務上達到最先進的成果，包括自然語言推論任務。透過在標註過程中使用預先訓練的模型，可以透過讓人們產生機器模型無法正確預測的前提假設組合來挑戰當前的 NLI 模型。為了在越南語自然語言推論的研究中保持吸引力和挑戰性，在本文中，我們向 NLP 研究社群介紹對抗式 NLI 資料集，並將其命名為 ViANLI。此資料集包含超過 10K 個前提假設對，並透過持續調整的流程建置，以充分利用標註員產生的模式。ViANLI 資料集為許多現有的 SOTA 模型帶來了許多困難，因為在測試集上最強大模型的準確度僅達到 48.4%。此外，實驗結果顯示，在我們的資料集上訓練的模型已顯著改善其他越南語 NLI 資料集的結果。

##### **Compositional Models for Estimating Causal Effects**
2406.17714v1 by Purva Pruthi, David Jensen

Many real-world systems can be represented as sets of interacting components.
Examples of such systems include computational systems such as query
processors, natural systems such as cells, and social systems such as families.
Many approaches have been proposed in traditional (associational) machine
learning to model such structured systems, including statistical relational
models and graph neural networks. Despite this prior work, existing approaches
to estimating causal effects typically treat such systems as single units,
represent them with a fixed set of variables and assume a homogeneous
data-generating process. We study a compositional approach for estimating
individual treatment effects (ITE) in structured systems, where each unit is
represented by the composition of multiple heterogeneous components. This
approach uses a modular architecture to model potential outcomes at each
component and aggregates component-level potential outcomes to obtain the
unit-level potential outcomes. We discover novel benefits of the compositional
approach in causal inference - systematic generalization to estimate
counterfactual outcomes of unseen combinations of components and improved
overlap guarantees between treatment and control groups compared to the
classical methods for causal effect estimation. We also introduce a set of
novel environments for empirically evaluating the compositional approach and
demonstrate the effectiveness of our approach using both simulated and
real-world data.

摘要：許多真實世界的系統都可以表示為互動組件的集合。
此類系統的範例包括運算系統（例如查詢處理器）、自然系統（例如細胞）和社會系統（例如家庭）。
傳統（關聯式）機器學習中已提出許多方法來建模此類結構化系統，包括統計關係模型和圖神經網路。儘管有這些先前的工作，現有的因果關係估計方法通常將此類系統視為單一單位，使用一組固定的變數表示它們，並假設資料產生過程是同質的。我們研究一種組合式方法，用於估計結構化系統中的個別處理效果 (ITE)，其中每個單位都由多個異質組件的組合表示。此方法使用模組化架構來建模每個組件的潛在結果，並彙總組件層級的潛在結果以獲得單位層級的潛在結果。我們發現組合式方法在因果推論中具有新穎的優點 - 系統化概化以估計未見組件組合的反事實結果，以及與因果關係估計的傳統方法相比，處理組和對照組之間的重疊保證得到改善。我們還引入一組新穎的環境，用於經驗評估組合式方法，並使用模擬和真實世界資料證明我們方法的有效性。

##### **Data curation via joint example selection further accelerates multimodal learning**
2406.17711v1 by Talfan Evans, Nikhil Parthasarathy, Hamza Merzic, Olivier J. Henaff

Data curation is an essential component of large-scale pretraining. In this
work, we demonstrate that jointly selecting batches of data is more effective
for learning than selecting examples independently. Multimodal contrastive
objectives expose the dependencies between data and thus naturally yield
criteria for measuring the joint learnability of a batch. We derive a simple
and tractable algorithm for selecting such batches, which significantly
accelerate training beyond individually-prioritized data points. As performance
improves by selecting from larger super-batches, we also leverage recent
advances in model approximation to reduce the associated computational
overhead. As a result, our approach--multimodal contrastive learning with joint
example selection (JEST)--surpasses state-of-the-art models with up to
13$\times$ fewer iterations and 10$\times$ less computation. Essential to the
performance of JEST is the ability to steer the data selection process towards
the distribution of smaller, well-curated datasets via pretrained reference
models, exposing the level of data curation as a new dimension for neural
scaling laws.

摘要：數據策展是大規模預訓練的必要組成部分。在這項工作中，我們證明了聯合選擇數據批次比獨立選擇範例更有效地進行學習。多模態對比目標揭露了數據之間的依賴性，因此自然而然地產生了衡量批次聯合可學習性的標準。我們推導出一個簡單且易於應用的演算法來選擇這些批次，這顯著地加速了訓練，超越了個別優先化的數據點。隨著從更大的超級批次中進行選擇而提高效能，我們也利用了模型近似中的最新進展來減少相關的運算開銷。因此，我們的做法——具有聯合範例選擇 (JEST) 的多模態對比學習——超越了最先進的模型，迭代次數減少了 13 倍，運算量減少了 10 倍。對於 JEST 的效能至關重要的是，能夠透過預訓練的參考模型將數據選擇過程引導到較小、策展良好的資料集的分布，將數據策展的層級揭露為神經擴充定律的新面向。

##### **FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model**
2406.17706v1 by Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, Jing Gao

Large language models (LLMs) show amazing performance on many domain-specific
tasks after fine-tuning with some appropriate data. However, many
domain-specific data are privately distributed across multiple owners. Thus,
this dilemma raises the interest in how to perform LLM fine-tuning in federated
learning (FL). However, confronted with limited computation and communication
capacities, FL clients struggle to fine-tune an LLM effectively. To this end,
we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL.
Specifically, our method involves the server generating a compressed LLM and
aligning its performance with the full model. Subsequently, the clients
fine-tune a lightweight yet important part of the compressed model, referred to
as an adapter. Notice that as the server has no access to the private data
owned by the clients, the data used for alignment by the server has a different
distribution from the one used for fine-tuning by clients. We formulate the
problem into a bi-level optimization problem to minimize the negative effect of
data discrepancy and derive the updating rules for the server and clients. We
conduct extensive experiments on LLaMA-2, empirically showing that the adapter
has exceptional performance when reintegrated into the global LLM. The results
also indicate that the proposed FedBiOT significantly reduces resource
consumption compared to existing benchmarks, all while achieving comparable
performance levels.

摘要：大型語言模型 (LLM) 在微調一些適當數據後，在許多特定領域任務上表現出色。然而，許多特定領域數據在多個所有者之間私下分發。因此，這個兩難局面引起了人們對如何在聯合學習 (FL) 中執行 LLM 微調的興趣。然而，面對有限的計算和通信能力，FL 客戶端難以有效地微調 LLM。為此，我們引入了 FedBiOT，一種適用於 FL 的資源高效 LLM 微調方法。具體來說，我們的模型涉及伺服器生成壓縮的 LLM，並使其性能與完整模型保持一致。隨後，客戶端微調壓縮模型中輕量但重要的部分，稱為適配器。請注意，由於伺服器無法訪問客戶端擁有的私人數據，因此伺服器用於對齊的數據與客戶端用於微調的數據分佈不同。我們將問題表述為一個雙層優化問題，以最小化數據差異的負面影響，並推導出伺服器和客戶端的更新規則。我們對 LLaMA-2 進行了廣泛的實驗，實證表明，將適配器重新整合到全局 LLM 中時，其性能異常出色。結果還表明，與現有基準相比，所提出的 FedBiOT 大大降低了資源消耗，同時達到了相當的性能水平。

##### **HGTDP-DTA: Hybrid Graph-Transformer with Dynamic Prompt for Drug-Target Binding Affinity Prediction**
2406.17697v1 by Xi Xiao, Wentao Wang, Jiacheng Xie, Lijing Zhu, Gaofei Chen, Zhengji Li, Tianyang Wang, Min Xu

Drug target binding affinity (DTA) is a key criterion for drug screening.
Existing experimental methods are time-consuming and rely on limited structural
and domain information. While learning-based methods can model sequence and
structural information, they struggle to integrate contextual data and often
lack comprehensive modeling of drug-target interactions. In this study, we
propose a novel DTA prediction method, termed HGTDP-DTA, which utilizes dynamic
prompts within a hybrid Graph-Transformer framework. Our method generates
context-specific prompts for each drug-target pair, enhancing the model's
ability to capture unique interactions. The introduction of prompt tuning
further optimizes the prediction process by filtering out irrelevant noise and
emphasizing task-relevant information, dynamically adjusting the input features
of the molecular graph. The proposed hybrid Graph-Transformer architecture
combines structural information from Graph Convolutional Networks (GCNs) with
sequence information captured by Transformers, facilitating the interaction
between global and local information. Additionally, we adopted the multi-view
feature fusion method to project molecular graph views and affinity subgraph
views into a common feature space, effectively combining structural and
contextual information. Experiments on two widely used public datasets, Davis
and KIBA, show that HGTDP-DTA outperforms state-of-the-art DTA prediction
methods in both prediction performance and generalization ability.

摘要：藥物標靶結合親和力 (DTA) 是藥物篩選的一個關鍵標準。
現有的實驗方法耗時，且依賴有限的結構和結構域資訊。雖然基於學習的方法可以建模序列和結構資訊，但它們難以整合脈絡資料，且往往缺乏對藥物標靶交互作用的全面建模。在此研究中，我們提出一個新的 DTA 預測方法，稱為 HGTDP-DTA，它在混合圖形轉換器框架中使用動態提示。我們的模型為每對藥物標靶產生特定於脈絡的提示，增強模型捕捉獨特交互作用的能力。提示調整的引入進一步最佳化預測流程，透過濾除無關雜訊和強調與任務相關的資訊，動態調整分子圖形的輸入特徵。所提出的混合圖形轉換器架構結合了來自圖形卷積網路 (GCN) 的結構資訊和轉換器捕捉的序列資訊，促進了全域和局部資訊之間的交互作用。此外，我們採用多視圖特徵融合方法，將分子圖形視圖和親和力子圖視圖投影到一個共同的特徵空間，有效地結合了結構和脈絡資訊。在兩個廣泛使用的公共資料集 Davis 和 KIBA 上的實驗表明，HGTDP-DTA 在預測效能和泛化能力方面都優於最先進的 DTA 預測方法。

##### **From Distributional to Overton Pluralism: Investigating Large Language Model Alignment**
2406.17692v1 by Thom Lake, Eunsol Choi, Greg Durrett

The alignment process changes several properties of a large language model's
(LLM's) output distribution. We analyze two aspects of post-alignment
distributional shift of LLM responses. First, we re-examine previously reported
reductions in response diversity post-alignment. Our analysis suggests that an
apparent drop in the diversity of responses is largely explained by quality
control and information aggregation. Alignment suppresses irrelevant and
unhelpful content while shifting the output distribution toward longer
responses that cover information spanning several responses from the base LLM,
essentially presenting diverse information in a single response. Finding little
evidence that alignment suppresses useful information, it is natural to ask the
opposite question: do aligned models surface information that cannot be
recovered from base models? Our second investigation shows this is not the case
and the behavior of aligned models is recoverable from base models without
fine-tuning. A combination of in-context examples and lower-resolution semantic
hints about response content can elicit responses from base LLMs that are as
similar to alignment-tuned LLM responses as alignment-tuned LLM responses are
to each other. Taken together, these results indicate that current alignment
techniques capture but do not extend the useful subset of assistant-like base
LLM behavior, providing further evidence for the Superficial Alignment
Hypothesis. They also show that in-context alignment can go surprisingly far as
a strategy for imitating aligned LLMs without fine-tuning. Our code and data is
available at https://github.com/thomlake/investigating-alignment.

摘要：大型語言模型 (LLM) 的輸出分佈會因為對齊程序而產生變動。我們分析 LLM 回應在對齊後分佈轉移的兩個面向。首先，我們重新檢視先前報告的回應多樣性在對齊後下降。我們的分析顯示，回應多樣性明顯下降，這在很大程度上可歸因於品質控管和資訊彙整。對齊會抑制不相關且無益的內容，同時將輸出分佈轉移至較長的回應，涵蓋了 LLM 基礎模型中多個回應的資訊，本質上是在單一回應中呈現多樣化的資訊。由於幾乎沒有證據顯示對齊會抑制有用的資訊，因此自然會問相反的問題：對齊的模型是否會浮現基礎模型無法復原的資訊？我們的第二次調查顯示並非如此，而且對齊模型的行為可以從基礎模型中復原，而無需微調。結合情境中的範例和關於回應內容的較低解析度語意提示，可以引發基礎 LLM 的回應，這些回應與對齊微調的 LLM 回應一樣類似，而對齊微調的 LLM 回應彼此之間也一樣類似。綜合來看，這些結果顯示目前的對齊技術捕捉了類助理基礎 LLM 行為的有用子集，但並未延伸該子集，進一步證實了表面對齊假說。它們也顯示情境中對齊可以成為一種令人驚訝的策略，用於模仿對齊的 LLM，而無需微調。我們的程式碼和資料可於 https://github.com/thomlake/investigating-alignment 取得。

##### **Unified Auto-Encoding with Masked Diffusion**
2406.17688v1 by Philippe Hansen-Estruch, Sriram Vishwanath, Amy Zhang, Manan Tomar

At the core of both successful generative and self-supervised representation
learning models there is a reconstruction objective that incorporates some form
of image corruption. Diffusion models implement this approach through a
scheduled Gaussian corruption process, while masked auto-encoder models do so
by masking patches of the image. Despite their different approaches, the
underlying similarity in their methodologies suggests a promising avenue for an
auto-encoder capable of both de-noising tasks. We propose a unified
self-supervised objective, dubbed Unified Masked Diffusion (UMD), that combines
patch-based and noise-based corruption techniques within a single auto-encoding
framework. Specifically, UMD modifies the diffusion transformer (DiT) training
process by introducing an additional noise-free, high masking representation
step in the diffusion noising schedule, and utilizes a mixed masked and noised
image for subsequent timesteps. By integrating features useful for diffusion
modeling and for predicting masked patch tokens, UMD achieves strong
performance in downstream generative and representation learning tasks,
including linear probing and class-conditional generation. This is achieved
without the need for heavy data augmentations, multiple views, or additional
encoders. Furthermore, UMD improves over the computational efficiency of prior
diffusion based methods in total training time. We release our code at
https://github.com/philippe-eecs/small-vision.

摘要：在成功的生成式和自监督表示学习模型的核心，存在一个重建目标，其中包含某种形式的图像损坏。扩散模型通过预定的高斯损坏过程实现此方法，而掩码自动编码器模型通过掩盖图像的块来实现此方法。尽管它们的方法不同，但它们的方法论中的潜在相似性表明了一种有希望的途径，即能够执行去噪任务的自动编码器。我们提出了一个统一的自监督目标，称为统一掩码扩散 (UMD)，它在单个自动编码框架内结合了基于块和基于噪声的损坏技术。具体来说，UMD 通过在扩散噪声计划中引入额外的无噪声、高掩码表示步骤来修改扩散变换器 (DiT) 训练过程，并利用混合掩码和噪声图像进行后续时间步长。通过整合对扩散建模和预测掩码块标记有用的特征，UMD 在下游生成式和表示学习任务中实现了强大的性能，包括线性探测和类条件生成。这是在不需要大量数据增强、多个视图或其他编码器的情况下实现的。此外，UMD 提高了基于扩散的先前方法在总训练时间内的计算效率。我们在 https://github.com/philippe-eecs/small-vision 上发布了我们的代码。

##### **VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation**
2406.17681v2 by Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu

As large language models achieve impressive scores on traditional benchmarks,
an increasing number of researchers are becoming concerned about benchmark data
leakage during pre-training, commonly known as the data contamination problem.
To ensure fair evaluation, recent benchmarks release only the training and
validation sets, keeping the test set labels closed-source. They require anyone
wishing to evaluate his language model to submit the model's predictions for
centralized processing and then publish the model's result on their
leaderboard. However, this submission process is inefficient and prevents
effective error analysis. To address this issue, we propose to variabilize
benchmarks and evaluate language models dynamically. Specifically, we extract
variables from each test case and define a value range for each variable. For
each evaluation, we sample new values from these value ranges to create unique
test cases, thus ensuring a fresh evaluation each time. We applied this
variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and
TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our
experimental results demonstrate that this approach provides a more accurate
assessment of the true capabilities of language models, effectively mitigating
the contamination problem.

摘要：隨著大型語言模型在傳統基準測試中獲得驚人的分數，
越來越多的研究人員開始擔心預訓練期間的基準資料
外洩，這通常稱為資料污染問題。
為了確保公平評量，最近的基準測試只釋出訓練和
驗證集，並將測試集標籤保持為封閉原始碼。他們要求任何
希望評估其語言模型的人提交模型的預測，以進行
集中處理，然後在他們的排行榜上公布模型的結果。然而，這個提交過程效率低下，且無法進行
有效的錯誤分析。為了解決這個問題，我們建議將
基準測試變數化，並動態評估語言模型。具體而言，我們從每個測試案例中擷取
變數，並為每個變數定義一個值域。對於
每個評估，我們從這些值域中抽樣新的值，以建立獨特的
測試案例，從而每次都能確保進行新的評估。我們將這個
變數擾動方法應用於四個資料集：GSM8K、ARC、CommonsenseQA 和
TruthfulQA，這些資料集涵蓋數學生成和多選題任務。我們的
實驗結果證明，這種方法可以更準確地評估語言模型的真正能力，有效地減輕
污染問題。

##### **Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models**
2406.17675v1 by Yuan Li, Yue Huang, Hongyi Wang, Xiangliang Zhang, James Zou, Lichao Sun

Large Language Models (LLMs) have demonstrated exceptional task-solving
capabilities, increasingly adopting roles akin to human-like assistants. The
broader integration of LLMs into society has sparked interest in whether they
manifest psychological attributes, and whether these attributes are
stable-inquiries that could deepen the understanding of their behaviors.
Inspired by psychometrics, this paper presents a framework for investigating
psychology in LLMs, including psychological dimension identification,
assessment dataset curation, and assessment with results validation. Following
this framework, we introduce a comprehensive psychometrics benchmark for LLMs
that covers six psychological dimensions: personality, values, emotion, theory
of mind, motivation, and intelligence. This benchmark includes thirteen
datasets featuring diverse scenarios and item types. Our findings indicate that
LLMs manifest a broad spectrum of psychological attributes. We also uncover
discrepancies between LLMs' self-reported traits and their behaviors in
real-world scenarios. This paper demonstrates a thorough psychometric
assessment of LLMs, providing insights into reliable evaluation and potential
applications in AI and social sciences.

摘要：大型語言模型 (LLM) 已展現出卓越的任務解決能力，並逐漸扮演類似人類助理的角色。 LLM 更廣泛地整合到社會中，已引起人們對其是否表現出心理屬性，以及這些屬性是否穩定，這些探討可能有助於深入了解其行為。受到心理測量的啟發，本文提出了一個用於調查 LLM 心理學的架構，包括心理維度識別、評估資料集策展，以及透過驗證結果進行評估。遵循此架構，我們為 LLM 引入了一個全面的心理測量基準，涵蓋六個心理維度：人格、價值觀、情緒、心智理論、動機和智力。此基準包含 13 個資料集，涵蓋各種情境和項目類型。我們的研究結果表明，LLM 表現出廣泛的心理屬性。我們還發現 LLM 自我報告的特質與其在真實世界情境中的行為之間存在差異。本文展示了對 LLM 進行徹底的心理測量評估，提供了對 AI 和社會科學中可靠評估和潛在應用見解。

##### **This Paper Had the Smartest Reviewers -- Flattery Detection Utilising an Audio-Textual Transformer-Based Approach**
2406.17667v1 by Lukas Christ, Shahin Amiriparian, Friederike Hawighorst, Ann-Kathrin Schill, Angelo Boutalikakis, Lorenz Graf-Vlachy, Andreas König, Björn W. Schuller

Flattery is an important aspect of human communication that facilitates
social bonding, shapes perceptions, and influences behavior through strategic
compliments and praise, leveraging the power of speech to build rapport
effectively. Its automatic detection can thus enhance the naturalness of
human-AI interactions. To meet this need, we present a novel audio textual
dataset comprising 20 hours of speech and train machine learning models for
automatic flattery detection. In particular, we employ pretrained AST,
Wav2Vec2, and Whisper models for the speech modality, and Whisper TTS models
combined with a RoBERTa text classifier for the textual modality. Subsequently,
we build a multimodal classifier by combining text and audio representations.
Evaluation on unseen test data demonstrates promising results, with Unweighted
Average Recall scores reaching 82.46% in audio-only experiments, 85.97% in
text-only experiments, and 87.16% using a multimodal approach.

摘要：奉承是人类沟通中一个重要的方面，它通过策略性的赞美和表扬来促进社交联系、塑造认知并影响行为，利用言语的力量有效地建立融洽关系。因此，对其进行自动检测可以增强人机交互的自然性。为了满足这一需求，我们提供了一个新颖的音视频文本数据集，其中包含 20 小时的语音，并训练机器学习模型以进行自动奉承检测。特别是，我们为语音模式采用了预训练的 AST、Wav2Vec2 和 Whisper 模型，并为文本模式结合了 Whisper TTS 模型和 RoBERTa 文本分类器。随后，我们通过组合文本和音频表示来构建一个多模态分类器。对未见测试数据的评估显示出有希望的结果，在仅音频实验中，未加权平均召回率得分达到 82.46%，在仅文本实验中达到 85.97%，在使用多模态方法时达到 87.16%。

##### **LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic**
2406.17663v1 by Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, Jennifer Chu-Carroll, David Melville, David Ferrucci

We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the
logical reasoning capabilities of Large Language Models (LLMs), by combining
them with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic
method where the LLM Actor generates declarative logic programs along with
tests for semantic correctness, while the Automated Reasoning Critic evaluates
the code, runs the tests and provides feedback on test failures for iterative
refinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a
new state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests
complex logical reasoning capabilities. Our experiments demonstrate significant
improvements over LLM-only baselines, highlighting the importance of logic test
generation and iterative self-refinement. We achieve our best result using a
fully automated self-supervised training loop where the Actor is trained on
end-to-end dialog traces with Critic feedback. We discuss potential
enhancements and provide a detailed error analysis, showcasing the robustness
and efficacy of LLM-ARC for complex natural language reasoning tasks.

摘要：我們引入了 LLM-ARC，這是一個神經符號框架，旨在透過將大型語言模型 (LLM) 與自動推理批評器 (ARC) 結合起來，來增強大型語言模型的邏輯推理能力。LLM-ARC 採用 Actor-Critic 方法，其中 LLM Actor 會產生宣告式邏輯程式，並針對語意正確性進行測試，而自動推理批評器會評估程式碼、執行測試，並針對測試失敗提供回饋，以進行反覆改進。LLM-ARC 使用 Answer Set Programming (ASP) 實作，在 FOLIO 基準測試中達到了 88.32% 的最新準確度，該基準測試會測試複雜的邏輯推理能力。我們的實驗證明了 LLM-only 基準的顯著改進，突顯了邏輯測試產生與反覆自我改進的重要性。我們使用全自動自我監督訓練迴圈達到了最佳結果，其中 Actor 會在端對端對話追蹤中使用批評者的回饋進行訓練。我們討論了潛在的增強功能，並提供了詳細的錯誤分析，展示了 LLM-ARC 在複雜自然語言推理任務中的強健性和有效性。

##### **DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning**
2406.17659v1 by Xiaohan Zhang, Zainab Altaweel, Yohei Hayamizu, Yan Ding, Saeid Amiri, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang

Vision-language models (VLMs) have been applied to robot task planning
problems, where the robot receives a task in natural language and generates
plans based on visual inputs. While current VLMs have demonstrated strong
vision-language understanding capabilities, their performance is still far from
being satisfactory in planning tasks. At the same time, although classical task
planners, such as PDDL-based, are strong in planning for long-horizon tasks,
they do not work well in open worlds where unforeseen situations are common. In
this paper, we propose a novel task planning and execution framework, called
DKPROMPT, which automates VLM prompting using domain knowledge in PDDL for
classical planning in open worlds. Results from quantitative experiments show
that DKPROMPT outperforms classical planning, pure VLM-based and a few other
competitive baselines in task completion rate.

摘要：視覺語言模型 (VLM) 已應用於機器人任務規劃問題，其中機器人會以自然語言接收任務，並根據視覺輸入產生計畫。雖然目前的 VLM 已展現出強大的視覺語言理解能力，但其在規劃任務中的表現仍遠未令人滿意。同時，儘管基於 PDDL 等的傳統任務規劃器在規劃長期任務方面很強大，但在不可預見的情況很常見的開放世界中，它們無法發揮作用。在本文中，我們提出一個名為 DKPROMPT 的新任務規劃和執行架構，它使用 PDDL 中的領域知識自動化 VLM 提示，以進行開放世界中的傳統規劃。定量實驗結果顯示，DKPROMPT 在任務完成率方面優於傳統規劃、純粹基於 VLM 的規劃以及其他一些具有競爭力的基線。

##### **MDHA: Multi-Scale Deformable Transformer with Hybrid Anchors for Multi-View 3D Object Detection**
2406.17654v1 by Michelle Adeline, Junn Yong Loo, Vishnu Monn Baskaran

Multi-view 3D object detection is a crucial component of autonomous driving
systems. Contemporary query-based methods primarily depend either on
dataset-specific initialization of 3D anchors, introducing bias, or utilize
dense attention mechanisms, which are computationally inefficient and
unscalable. To overcome these issues, we present MDHA, a novel sparse
query-based framework, which constructs adaptive 3D output proposals using
hybrid anchors from multi-view, multi-scale input. Fixed 2D anchors are
combined with depth predictions to form 2.5D anchors, which are projected to
obtain 3D proposals. To ensure high efficiency, our proposed Anchor Encoder
performs sparse refinement and selects the top-k anchors and features.
Moreover, while existing multi-view attention mechanisms rely on projecting
reference points to multiple images, our novel Circular Deformable Attention
mechanism only projects to a single image but allows reference points to
seamlessly attend to adjacent images, improving efficiency without compromising
on performance. On the nuScenes val set, it achieves 46.4% mAP and 55.0% NDS
with a ResNet101 backbone. MDHA significantly outperforms the baseline, where
anchor proposals are modelled as learnable embeddings.

摘要：多視圖 3D 物件偵測是自動駕駛系統的關鍵組成部分。當代基於查詢的方法主要依賴於 3D 錨點的特定資料集初始化，引入偏差，或利用密集注意力機制，這在計算上效率低下且無法擴展。為了克服這些問題，我們提出了 MDHA，一個創新的稀疏基於查詢的框架，它使用來自多視圖、多尺度輸入的混合錨點構建自適應 3D 輸出建議。固定的 2D 錨點與深度預測相結合，形成 2.5D 錨點，並投影以獲得 3D 建議。為了確保高效率，我們提出的 Anchor Encoder 執行稀疏調整，並選擇前 k 個錨點和特徵。此外，雖然現有的多視圖注意力機制依賴於將參考點投影到多個影像，但我們創新的 Circular Deformable Attention 機制僅投影到單一影像，但允許參考點無縫地關注相鄰影像，從而在不影響效能的情況下提高效率。在 nuScenes val 設定中，它使用 ResNet101 主幹達到了 46.4% mAP 和 55.0% NDS。MDHA 明顯優於基線，其中錨點建議被建模為可學習的嵌入。

##### **Leveraging Large Language Models for Software Model Completion: Results from Industrial and Public Datasets**
2406.17651v1 by Christof Tinnes, Alisa Welter, Sven Apel

Modeling structure and behavior of software systems plays a crucial role in
the industrial practice of software engineering. As with other software
engineering artifacts, software models are subject to evolution. Supporting
modelers in evolving software models with recommendations for model completions
is still an open problem, though. In this paper, we explore the potential of
large language models for this task. In particular, we propose an approach,
retrieval-augmented generation, leveraging large language models, model
histories, and retrieval-augmented generation for model completion. Through
experiments on three datasets, including an industrial application, one public
open-source community dataset, and one controlled collection of simulated model
repositories, we evaluate the potential of large language models for model
completion with retrieval-augmented generation. We found that large language
models are indeed a promising technology for supporting software model
evolution (62.30% semantically correct completions on real-world industrial
data and up to 86.19% type-correct completions). The general inference
capabilities of large language models are particularly useful when dealing with
concepts for which there are few, noisy, or no examples at all.

摘要：在軟體工程的產業實務中，軟體系統的結構和行為建模扮演著至關重要的角色。與其他軟體工程產出物一樣，軟體模型也會隨著時間演化。然而，支援建模人員使用模型完成建議來演化軟體模型仍是一個尚未解決的問題。在本文中，我們探討大型語言模型在這個任務中的潛力。具體來說，我們提出了一種方法，稱為檢索增強式生成，它利用大型語言模型、模型歷史記錄和檢索增強式生成來完成模型。透過在三個資料集上進行實驗，包括一個產業應用程式、一個公開的開源社群資料集，以及一個模擬模型儲存庫的受控集合，我們評估了大型語言模型在使用檢索增強式生成進行模型完成的潛力。我們發現大型語言模型確實是一種有前途的技術，可以用來支援軟體模型演化（在真實世界的產業資料上，有 62.30% 的語義正確完成，類型正確完成的比例最高可達 86.19%）。大型語言模型的一般推論能力在處理幾乎沒有、有雜訊或完全沒有範例的概念時特別有用。

##### **ELIZA Reinterpreted: The world's first chatbot was not intended as a chatbot at all**
2406.17650v1 by Jeff Shrager

ELIZA, often considered the world's first chatbot, was written by Joseph
Weizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot,
but rather to build a platform for research into human-machine conversation and
the important cognitive processes of interpretation and misinterpretation. His
purpose was obscured by ELIZA's fame, resulting in large part from the
fortuitous timing of it's creation, and it's escape into the wild. In this
paper I provide a rich historical context for ELIZA's creation, demonstrating
that ELIZA arose from the intersection of some of the central threads in the
technical history of AI. I also briefly discuss how ELIZA escaped into the
world, and how its accidental escape, along with several coincidental turns of
the programming language screws, led both to the misapprehension that ELIZA was
intended as a chatbot, and to the loss of the original ELIZA to history for
over 50 years.

摘要：ELIZA，常被認為是世界上第一個聊天機器人，由約瑟夫·魏森鮑姆在 1960 年代初期編寫。魏森鮑姆並非有意發明聊天機器人，而是為了建立一個研究人機對話以及解釋和誤解的重要認知過程的平台。他的目的被 ELIZA 的名聲所掩蓋，這在很大程度上是由於其創作的時機巧合，以及它逃逸到野外。在本文中，我為 ELIZA 的創作提供了一個豐富的歷史背景，證明 ELIZA 來自於 AI 技術史中一些中心線索的交匯處。我也簡要地討論了 ELIZA 如何逃逸到世界，以及它如何意外逃逸，以及編程語言螺絲的幾個巧合轉折，既導致了 ELIZA 被誤解為聊天機器人，也導致了原始 ELIZA 在歷史上遺失了 50 多年。

##### **Variationist: Exploring Multifaceted Variation and Bias in Written Language Data**
2406.17647v1 by Alan Ramponi, Camilla Casula, Stefano Menini

Exploring and understanding language data is a fundamental stage in all areas
dealing with human language. It allows NLP practitioners to uncover quality
concerns and harmful biases in data before training, and helps linguists and
social scientists to gain insight into language use and human behavior. Yet,
there is currently a lack of a unified, customizable tool to seamlessly inspect
and visualize language variation and bias across multiple variables, language
units, and diverse metrics that go beyond descriptive statistics. In this
paper, we introduce Variationist, a highly-modular, extensible, and
task-agnostic tool that fills this gap. Variationist handles at once a
potentially unlimited combination of variable types and semantics across
diversity and association metrics with regards to the language unit of choice,
and orchestrates the creation of up to five-dimensional interactive charts for
over 30 variable type-semantics combinations. Through our case studies on
computational dialectology, human label variation, and text generation, we show
how Variationist enables researchers from different disciplines to effortlessly
answer specific research questions or unveil undesired associations in language
data. A Python library, code, documentation, and tutorials are made publicly
available to the research community.

摘要：探索和理解語言資料是所有處理人類語言領域的基本階段。它允許 NLP 從業人員在訓練前發現資料中的品質問題和有害偏見，並幫助語言學家和社會科學家深入了解語言使用和人類行為。然而，目前缺乏一個統一的、可自訂的工具來無縫地檢查和視覺化跨多個變數、語言單位和超越描述性統計的各種指標的語言變異和偏見。在本文中，我們介紹了 Variationist，這是一個高度模組化、可擴充且與任務無關的工具，填補了這一空白。Variationist 同時處理變數類型和語義的潛在無限組合，跨越與所選語言單位的相關性和關聯性指標，並協調建立多達五維互動圖表，以涵蓋 30 多個變數類型語義組合。透過我們在計算方言學、人類標籤變異和文本生成的案例研究，我們展示了 Variationist 如何使不同領域的研究人員能夠毫不費力地回答具體的研究問題或揭示語言資料中不需要的關聯性。Python 庫、程式碼、文件和教學課程已公開提供給研究社群。

##### **Banishing LLM Hallucinations Requires Rethinking Generalization**
2406.17642v1 by Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, Gregory Diamos

Despite their powerful chat, coding, and reasoning abilities, Large Language
Models (LLMs) frequently hallucinate. Conventional wisdom suggests that
hallucinations are a consequence of a balance between creativity and
factuality, which can be mitigated, but not eliminated, by grounding the LLM in
external knowledge sources. Through extensive systematic experiments, we show
that these traditional approaches fail to explain why LLMs hallucinate in
practice. Specifically, we show that LLMs augmented with a massive Mixture of
Memory Experts (MoME) can easily memorize large datasets of random numbers. We
corroborate these experimental findings with a theoretical construction showing
that simple neural networks trained to predict the next token hallucinate when
the training loss is above a threshold as it usually does in practice when
training on internet scale data. We interpret our findings by comparing against
traditional retrieval methods for mitigating hallucinations. We use our
findings to design a first generation model for removing hallucinations --
Lamini-1 -- that stores facts in a massive mixture of millions of memory
experts that are retrieved dynamically.

摘要：儘管擁有強大的聊天、編碼和推理能力，大型語言模型 (LLM) 經常會出現幻覺。傳統智慧認為幻覺是創造力和事實性之間平衡的結果，這種平衡可以透過將 LLM 基於外部知識來源來減輕，但無法消除。透過廣泛的系統性實驗，我們證明這些傳統方法無法解釋為什麼 LLM 在實務中會出現幻覺。具體來說，我們證明了透過大量記憶專家混合 (MoME) 增強的 LLM 可以輕鬆記住大量隨機數字的資料集。我們用一個理論建構來證實這些實驗發現，證明訓練來預測下一個標記的簡單神經網路會在訓練損失高於閾值時出現幻覺，這通常會在訓練網路規模資料時發生。我們透過與傳統的擷取方法進行比較來詮釋我們的發現，以減輕幻覺。我們利用我們的發現設計了一個第一代模型來移除幻覺 -- Lamini-1 -- 它會將事實儲存在數百萬個記憶專家的龐大混合中，並會動態擷取這些事實。

##### **BayTTA: Uncertainty-aware medical image classification with optimized test-time augmentation using Bayesian model averaging**
2406.17640v1 by Zeinab Sherkatghanad, Moloud Abdar, Mohammadreza Bakhtyari, Vladimir Makarenkov

Test-time augmentation (TTA) is a well-known technique employed during the
testing phase of computer vision tasks. It involves aggregating multiple
augmented versions of input data. Combining predictions using a simple average
formulation is a common and straightforward approach after performing TTA. This
paper introduces a novel framework for optimizing TTA, called BayTTA
(Bayesian-based TTA), which is based on Bayesian Model Averaging (BMA). First,
we generate a model list associated with different variations of the input data
created through TTA. Then, we use BMA to combine model predictions weighted by
their respective posterior probabilities. Such an approach allows one to take
into account model uncertainty, and thus to enhance the predictive performance
of the related machine learning or deep learning model. We evaluate the
performance of BayTTA on various public data, including three medical image
datasets comprising skin cancer, breast cancer, and chest X-ray images and two
well-known gene editing datasets, CRISPOR and GUIDE-seq. Our experimental
results indicate that BayTTA can be effectively integrated into
state-of-the-art deep learning models used in medical image analysis as well as
into some popular pre-trained CNN models such as VGG-16, MobileNetV2,
DenseNet201, ResNet152V2, and InceptionRes-NetV2, leading to the enhancement in
their accuracy and robustness performance.

摘要：測試時間擴充 (TTA) 是一種在電腦視覺任務的測試階段中廣泛使用的技術。它涉及聚合輸入資料的許多擴充版本。在執行 TTA 之後，使用簡單平均公式組合預測是一種常見且直接的方法。本文介紹了一個用於最佳化 TTA 的新框架，稱為 BayTTA（基於貝氏的 TTA），它基於貝氏模型平均 (BMA)。首先，我們產生一個與輸入資料的不同變異相關的模型清單，這些變異是透過 TTA 建立的。然後，我們使用 BMA 來組合模型預測，其權重由它們各自的後驗機率決定。這種方法允許考慮模型的不確定性，從而增強相關機器學習或深度學習模型的預測性能。我們在各種公開資料上評估 BayTTA 的性能，包括三個醫學影像資料集，其中包含皮膚癌、乳癌和胸部 X 光影像，以及兩個著名的基因編輯資料集，CRISPOR 和 GUIDE-seq。我們的實驗結果表明，BayTTA 可以有效整合到用於醫學影像分析的最新深度學習模型中，以及一些流行的預訓練 CNN 模型中，例如 VGG-16、MobileNetV2、DenseNet201、ResNet152V2 和 InceptionRes-NetV2，從而提升它們的準確度和健壯性表現。

##### **Mitigate the Gap: Investigating Approaches for Improving Cross-Modal Alignment in CLIP**
2406.17639v2 by Sedigheh Eslami, Gerard de Melo

Contrastive Language--Image Pre-training (CLIP) has manifested remarkable
improvements in zero-shot classification and cross-modal vision-language tasks.
Yet, from a geometrical point of view, the CLIP embedding space has been found
to have a pronounced modality gap. This gap renders the embedding space overly
sparse and disconnected, with different modalities being densely distributed in
distinct subregions of the hypersphere. In this work, we aim at answering two
main questions: 1. Does sharing the parameter space between the multi-modal
encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart
the uni-modal embeddings via intra-modality separation? We design AlignCLIP, in
order to answer these questions and show that answers to both questions are
positive. Through extensive experiments, we show that AlignCLIP achieves
noticeable enhancements in the cross-modal alignment of the embeddings, and
thereby, reduces the modality gap, while maintaining the performance across
several downstream evaluations, such as zero-shot image classification,
zero-shot multi-modal retrieval and zero-shot semantic text similarity.

摘要：對比語言影像預訓練 (CLIP) 已展現出在零次分類和跨模態視覺語言任務中顯著的改進。然而，從幾何觀點來看，發現 CLIP 嵌入空間具有明顯的模態差距。此差距使嵌入空間過於稀疏且不連續，且不同模態密集分佈在超球體的不同子區域中。在這項工作中，我們旨在回答兩個主要問題：1. 在多模態編碼器之間共享參數空間是否會縮小模態差距？2. 是否可以透過透過模態內分隔將單模態嵌入分開來縮小差距？我們設計了 AlignCLIP，以回答這些問題，並表明對這兩個問題的答案都是肯定的。透過廣泛的實驗，我們表明 AlignCLIP 在嵌入的跨模態對齊中獲得顯著的增強，從而縮小了模態差距，同時維持了多個下游評估的效能，例如零次影像分類、零次多模態檢索和零次語意文字相似性。

##### **Aligning Diffusion Models with Noise-Conditioned Perception**
2406.17636v1 by Alexander Gambashidze, Anton Kulikov, Yuriy Sosnin, Ilya Makarov

Recent advancements in human preference optimization, initially developed for
Language Models (LMs), have shown promise for text-to-image Diffusion Models,
enhancing prompt alignment, visual appeal, and user preference. Unlike LMs,
Diffusion Models typically optimize in pixel or VAE space, which does not align
well with human perception, leading to slower and less efficient training
during the preference alignment stage. We propose using a perceptual objective
in the U-Net embedding space of the diffusion model to address these issues.
Our approach involves fine-tuning Stable Diffusion 1.5 and XL using Direct
Preference Optimization (DPO), Contrastive Preference Optimization (CPO), and
supervised fine-tuning (SFT) within this embedding space. This method
significantly outperforms standard latent-space implementations across various
metrics, including quality and computational cost. For SDXL, our approach
provides 60.8\% general preference, 62.2\% visual appeal, and 52.1\% prompt
following against original open-sourced SDXL-DPO on the PartiPrompts dataset,
while significantly reducing compute. Our approach not only improves the
efficiency and quality of human preference alignment for diffusion models but
is also easily integrable with other optimization techniques. The training code
and LoRA weights will be available here:
https://huggingface.co/alexgambashidze/SDXL\_NCP-DPO\_v0.1

摘要：最近在人類偏好最佳化方面的進展，最初是為語言模型 (LM) 開發的，已顯示出對文字到影像的擴散模型很有前景，增強提示對齊、視覺吸引力和用戶偏好。與 LM 不同，擴散模型通常在像素或 VAE 空間中進行最佳化，這與人類感知不符，導致在偏好對齊階段訓練更慢且效率更低。我們建議在擴散模型的 U-Net 嵌入空間中使用感知目標來解決這些問題。我們的做法包括使用直接偏好最佳化 (DPO)、對比偏好最佳化 (CPO) 和在這個嵌入空間內的監督式微調 (SFT) 來微調 Stable Diffusion 1.5 和 XL。這種方法在各種指標上明顯優於標準的潛在空間實作，包括品質和運算成本。對於 SDXL，我們的方法在 PartiPrompts 資料集上提供 60.8% 的一般偏好、62.2% 的視覺吸引力和 52.1% 的提示遵循，同時顯著減少運算。我們的做法不僅提高了擴散模型的人類偏好對齊的效率和品質，而且很容易與其他最佳化技術整合。訓練程式碼和 LoRA 權重將在此處提供：
https://huggingface.co/alexgambashidze/SDXL\_NCP-DPO\_v0.1

##### **Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels**
2406.17633v1 by Nicholas Pangakis, Samuel Wolken

Computational social science (CSS) practitioners often rely on human-labeled
data to fine-tune supervised text classifiers. We assess the potential for
researchers to augment or replace human-generated training data with surrogate
training labels from generative large language models (LLMs). We introduce a
recommended workflow and test this LLM application by replicating 14
classification tasks and measuring performance. We employ a novel corpus of
English-language text classification data sets from recent CSS articles in
high-impact journals. Because these data sets are stored in password-protected
archives, our analyses are less prone to issues of contamination. For each
task, we compare supervised classifiers fine-tuned using GPT-4 labels against
classifiers fine-tuned with human annotations and against labels from GPT-4 and
Mistral-7B with few-shot in-context learning. Our findings indicate that
supervised classification models fine-tuned on LLM-generated labels perform
comparably to models fine-tuned with labels from human annotators. Fine-tuning
models using LLM-generated labels can be a fast, efficient and cost-effective
method of building supervised text classifiers.

摘要：計算社會科學 (CSS) 實務工作者常依賴人工標籤資料來微調監督式文字分類器。我們評估研究人員利用生成式大型語言模型 (LLM) 的替代訓練標籤來擴充或取代人工產生的訓練資料的潛力。我們介紹一個建議的工作流程，並透過複製 14 個分類任務和衡量效能來測試這個 LLM 應用程式。我們採用一個新穎的英文文字分類資料集語料庫，該語料庫取自高影響力期刊近期的 CSS 文章。由於這些資料集儲存在受密碼保護的檔案中，我們的分析較不容易受到汙染問題的影響。對於每個任務，我們比較使用 GPT-4 標籤微調的監督式分類器與使用人工標註微調的分類器，以及使用 GPT-4 和 Mistral-7B 進行少量次數情境學習的標籤。我們的研究結果顯示，使用 LLM 產生的標籤微調的監督式分類模型，其效能與使用人工標註者標籤微調的模型相當。使用 LLM 產生的標籤微調模型可能是建構監督式文字分類器的一種快速、有效率且具成本效益的方法。

##### **CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference**
2406.17626v1 by Erxin Yu, Jing Li, Ming Liao, Siqi Wang, Zuchen Gao, Fei Mi, Lanqing Hong

As large language models (LLMs) constantly evolve, ensuring their safety
remains a critical research problem. Previous red-teaming approaches for LLM
safety have primarily focused on single prompt attacks or goal hijacking. To
the best of our knowledge, we are the first to study LLM safety in multi-turn
dialogue coreference. We created a dataset of 1,400 questions across 14
categories, each featuring multi-turn coreference safety attacks. We then
conducted detailed evaluations on five widely used open-source LLMs. The
results indicated that under multi-turn coreference safety attacks, the highest
attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was
13.9% with the Mistral-7B-Instruct model. These findings highlight the safety
vulnerabilities in LLMs during dialogue coreference interactions.

摘要：随着大型语言模型 (LLM) 不断发展，确保其安全性仍然是一个关键的研究问题。以前针对 LLM 安全性的红队方法主要集中在单一提示攻击或目标劫持上。据我们所知，我们是第一个在多轮对话共指中研究 LLM 安全性的人。我们创建了一个包含 14 个类别的 1,400 个问题的数据集，每个问题都包含多轮共指安全攻击。然后，我们对五种广泛使用的开源 LLM 进行了详细的评估。结果表明，在多轮共指安全攻击下，攻击成功率最高的是 LLaMA2-Chat-7b 模型，为 56%，而最低的是 Mistral-7B-Instruct 模型，为 13.9%。这些发现突出了 LLM 在对话共指交互过程中的安全漏洞。

##### **Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models**
2406.17624v1 by Zhiyuan Wen, Yu Yang, Jiannong Cao, Haoming Sun, Ruosong Yang, Shuaiqi Liu

As large language models (LLMs) appear to behave increasingly human-like in
text-based interactions, more and more researchers become interested in
investigating personality in LLMs. However, the diversity of psychological
personality research and the rapid development of LLMs have led to a broad yet
fragmented landscape of studies in this interdisciplinary field. Extensive
studies across different research focuses, different personality psychometrics,
and different LLMs make it challenging to have a holistic overview and further
pose difficulties in applying findings to real-world applications. In this
paper, we present a comprehensive review by categorizing current studies into
three research problems: self-assessment, exhibition, and recognition, based on
the intrinsic characteristics and external manifestations of personality in
LLMs. For each problem, we provide a thorough analysis and conduct in-depth
comparisons of their corresponding solutions. Besides, we summarize research
findings and open challenges from current studies and further discuss their
underlying causes. We also collect extensive publicly available resources to
facilitate interested researchers and developers. Lastly, we discuss the
potential future research directions and application scenarios. Our paper is
the first comprehensive survey of up-to-date literature on personality in LLMs.
By presenting a clear taxonomy, in-depth analysis, promising future directions,
and extensive resource collections, we aim to provide a better understanding
and facilitate further advancements in this emerging field.

摘要：隨著大型語言模型 (LLM) 在基於文字的互動中表現出越來越像人類的行為，越來越多的研究人員開始有興趣探討 LLM 中的人格特質。然而，心理人格研究的多樣性和 LLM 的快速發展，導致這個跨領域領域的研究呈現出廣泛但支離破碎的樣貌。橫跨不同研究重點、不同人格心理測量法和不同 LLM 的廣泛研究，使得難以全面了解，並進一步造成將研究結果應用於實際應用上的困難。在本文中，我們透過將目前的研究分類為三個研究問題：自我評量、展現和辨識，基於 LLM 中人格特質的內在特徵和外在表現，提出了一份全面的回顧。對於每個問題，我們提供了徹底的分析，並對其對應的解決方案進行深入比較。此外，我們總結了目前研究的研究結果和開放性挑戰，並進一步探討其潛在原因。我們也蒐集了廣泛的公開資源，以協助有興趣的研究人員和開發人員。最後，我們討論了潛在的未來研究方向和應用情境。我們的論文是第一份關於 LLM 中人格特質的最新文獻的全面調查。透過提出明確的分類法、深入的分析、有前景的未來方向和廣泛的資源蒐集，我們旨在提供更好的理解，並促進這個新興領域的進一步發展。

##### **Towards Building an End-to-End Multilingual Automatic Lyrics Transcription Model**
2406.17618v1 by Jiawen Huang, Emmanouil Benetos

Multilingual automatic lyrics transcription (ALT) is a challenging task due
to the limited availability of labelled data and the challenges introduced by
singing, compared to multilingual automatic speech recognition. Although some
multilingual singing datasets have been released recently, English continues to
dominate these collections. Multilingual ALT remains underexplored due to the
scale of data and annotation quality. In this paper, we aim to create a
multilingual ALT system with available datasets. Inspired by architectures that
have been proven effective for English ALT, we adapt these techniques to the
multilingual scenario by expanding the target vocabulary set. We then evaluate
the performance of the multilingual model in comparison to its monolingual
counterparts. Additionally, we explore various conditioning methods to
incorporate language information into the model. We apply analysis by language
and combine it with the language classification performance. Our findings
reveal that the multilingual model performs consistently better than the
monolingual models trained on the language subsets. Furthermore, we demonstrate
that incorporating language information significantly enhances performance.

摘要：多語言自動歌詞轉錄 (ALT) 是一項具有挑戰性的任務，原因在於標籤資料的可用性有限，以及與多語言自動語音辨識相比，唱歌所帶來的挑戰。儘管最近已發布一些多語言歌唱資料集，但英語在這些資料集中仍佔主導地位。由於資料規模和標註品質，多語言 ALT 仍未被充分探索。在本文中，我們旨在使用現有資料集建立一個多語言 ALT 系統。受到已被證明對英語 ALT 有效的架構啟發，我們透過擴充目標詞彙集，將這些技術調整到多語言場景。然後，我們評估多語言模型的效能，並與其單語言對應模型進行比較。此外，我們探索各種條件化方法，以將語言資訊納入模型中。我們按語言進行分析，並將其與語言分類效能結合。我們的發現顯示，多語言模型的效能始終優於以語言子集訓練的單語言模型。此外，我們證明納入語言資訊可顯著提升效能。

##### **Aligning Programming Language and Natural Language: Exploring Design Choices in Multi-Modal Transformer-Based Embedding for Bug Localization**
2406.17615v1 by Partha Chakraborty, Venkatraman Arumugam, Meiyappan Nagappan

Bug localization refers to the identification of source code files which is
in a programming language and also responsible for the unexpected behavior of
software using the bug report, which is a natural language. As bug localization
is labor-intensive, bug localization models are employed to assist software
developers. Due to the domain difference between source code files and bug
reports, modern bug-localization systems, based on deep learning models, rely
heavily on embedding techniques that project bug reports and source code files
into a shared vector space. The creation of an embedding involves several
design choices, but the impact of these choices on the quality of embedding and
the performance of bug localization models remains unexplained in current
research.
  To address this gap, our study evaluated 14 distinct embedding models to gain
insights into the effects of various design choices. Subsequently, we developed
bug localization models utilizing these embedding models to assess the
influence of these choices on the performance of the localization models. Our
findings indicate that the pre-training strategies significantly affect the
quality of the embedding. Moreover, we discovered that the familiarity of the
embedding models with the data has a notable impact on the bug localization
model's performance. Notably, when the training and testing data are collected
from different projects, the performance of the bug localization models
exhibits substantial fluctuations.

摘要：錯誤定位是指識別程式碼檔案，這些檔案使用錯誤報告（一種自然語言）以程式語言撰寫，並負責軟體的異常行為。由於錯誤定位需要大量人力，因此採用錯誤定位模型來協助軟體開發人員。由於原始碼檔案和錯誤報告之間的領域差異，基於深度學習模型的現代錯誤定位系統，極度依賴將錯誤報告和原始碼檔案投射到共享向量空間的嵌入技術。建立嵌入涉及多項設計選擇，但這些選擇對嵌入品質和錯誤定位模型效能的影響，在目前的研究所中仍未獲得解釋。
為了解決這個差距，我們的研究評估了 14 個不同的嵌入模型，以深入了解各種設計選擇的影響。隨後，我們開發了利用這些嵌入模型的錯誤定位模型，以評估這些選擇對定位模型效能的影響。我們的研究結果指出，預訓練策略會顯著影響嵌入品質。此外，我們發現嵌入模型對資料的熟悉度，對錯誤定位模型的效能有顯著影響。值得注意的是，當訓練和測試資料從不同的專案收集時，錯誤定位模型的效能會出現大幅波動。

##### **Diffusion-based Adversarial Purification for Intrusion Detection**
2406.17606v1 by Mohamed Amine Merzouk, Erwan Beurier, Reda Yaich, Nora Boulahia-Cuppens, Frédéric Cuppens

The escalating sophistication of cyberattacks has encouraged the integration
of machine learning techniques in intrusion detection systems, but the rise of
adversarial examples presents a significant challenge. These crafted
perturbations mislead ML models, enabling attackers to evade detection or
trigger false alerts. As a reaction, adversarial purification has emerged as a
compelling solution, particularly with diffusion models showing promising
results. However, their purification potential remains unexplored in the
context of intrusion detection. This paper demonstrates the effectiveness of
diffusion models in purifying adversarial examples in network intrusion
detection. Through a comprehensive analysis of the diffusion parameters, we
identify optimal configurations maximizing adversarial robustness with minimal
impact on normal performance. Importantly, this study reveals insights into the
relationship between diffusion noise and diffusion steps, representing a novel
contribution to the field. Our experiments are carried out on two datasets and
against 5 adversarial attacks. The implementation code is publicly available.

摘要：網路攻擊的日益複雜化促使機器學習技術整合到入侵偵測系統中，但對抗範例的興起帶來重大挑戰。這些精心製作的擾動會誤導機器學習模型，讓攻擊者得以規避偵測或觸發錯誤警報。作為應對，對抗淨化已成為一個引人注目的解決方案，特別是擴散模型顯示出有希望的結果。然而，它們在入侵偵測中的淨化潛力仍未被探索。本文展示了擴散模型在網路入侵偵測中淨化對抗範例的有效性。透過對擴散參數進行全面分析，我們找出最佳組態，最大化對抗彈性，同時將對正常效能的影響降至最低。重要的是，這項研究揭示了擴散雜訊和擴散步驟之間的關係，這對該領域來說是一項創新的貢獻。我們的實驗在兩個資料集上進行，並針對 5 種對抗攻擊。實作程式碼已公開。

##### **"Seeing the Big through the Small": Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?**
2406.17600v1 by Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert Litschko, Anna Korhonen, Barbara Plank

Human label variation (HLV) is a valuable source of information that arises
when multiple human annotators provide different labels for valid reasons. In
Natural Language Inference (NLI) earlier approaches to capturing HLV involve
either collecting annotations from many crowd workers to represent human
judgment distribution (HJD) or use expert linguists to provide detailed
explanations for their chosen labels. While the former method provides denser
HJD information, obtaining it is resource-intensive. In contrast, the latter
offers richer textual information but it is challenging to scale up to many
human judges. Besides, large language models (LLMs) are increasingly used as
evaluators (``LLM judges'') but with mixed results, and few works aim to study
HJDs. This study proposes to exploit LLMs to approximate HJDs using a small
number of expert labels and explanations. Our experiments show that a few
explanations significantly improve LLMs' ability to approximate HJDs with and
without explicit labels, thereby providing a solution to scale up annotations
for HJD. However, fine-tuning smaller soft-label aware models with the
LLM-generated model judgment distributions (MJDs) presents partially
inconsistent results: while similar in distance, their resulting fine-tuned
models and visualized distributions differ substantially. We show the
importance of complementing instance-level distance measures with a
global-level shape metric and visualization to more effectively evaluate MJDs
against human judgment distributions.

摘要：人類標籤變異 (HLV) 是有價值的訊息來源，當多個人類註解者出於有效原因提供不同的標籤時就會出現。在自然語言推論 (NLI) 中，擷取 HLV 的早期方法包括從許多群眾工作者收集註解以表示人類判斷分佈 (HJD)，或使用專家語言學家提供其所選標籤的詳細說明。雖然前一種方法提供了更密集的 HJD 資訊，但取得它需要大量資源。相比之下，後者提供了更豐富的文字資訊，但要擴展到許多人類評審員具有挑戰性。此外，大型語言模型 (LLM) 越來越多地被用作評估者（``LLM 評審員''），但結果好壞參半，而且很少有研究旨在研究 HJD。本研究建議利用 LLM 使用少數專家標籤和說明來近似 HJD。我們的實驗表明，一些說明顯著提高了 LLM 近似 HJD 的能力，無論是否有明確標籤，從而為 HJD 的註解擴展提供了一個解決方案。然而，使用 LLM 生成的模型判斷分佈 (MJD) 微調較小的軟標籤感知模型會產生部分不一致的結果：雖然距離相似，但其產生的微調模型和可視化分佈有很大不同。我們展示了使用全局形狀指標和視覺化補充例項級距離測量的重要性，以便更有效地針對人類判斷分佈評估 MJD。

##### **LongIns: A Challenging Long-context Instruction-based Exam for LLMs**
2406.17588v2 by Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, Ge Zhang

The long-context capabilities of large language models (LLMs) have been a hot
topic in recent years. To evaluate the performance of LLMs in different
scenarios, various assessment benchmarks have emerged. However, as most of
these benchmarks focus on identifying key information to answer questions,
which mainly requires the retrieval ability of LLMs, these benchmarks can
partially represent the reasoning performance of LLMs from large amounts of
information. Meanwhile, although LLMs often claim to have context windows of
32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual
supported length of these LLMs. To address these issues, we propose the LongIns
benchmark dataset, a challenging long-context instruction-based exam for LLMs,
which is built based on the existing instruction datasets. Specifically, in our
LongIns, we introduce three evaluation settings: Global Instruction & Single
Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction &
Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations
on existing LLMs and have the following important findings: (1). The
top-performing GPT-4 with 128k context length performs poorly on the evaluation
context window of 16k in our LongIns. (2). For the multi-hop reasoning ability
of many existing LLMs, significant efforts are still needed under short context
windows (less than 4k).

摘要：近年來，大型語言模型 (LLM) 的長文本處理能力一直是熱門話題。為了評估 LLM 在不同場景中的表現，出現了各種評估基準。然而，由於大多數這些基準都專注於識別關鍵資訊以回答問題，這主要需要 LLM 的檢索能力，因此這些基準只能部分代表 LLM 從大量資訊中進行推理的表現。同時，儘管 LLM 經常聲稱具有 32k、128k、200k 甚至更長的上下文視窗，但這些基準未能揭示這些 LLM 的實際支援長度。為了解決這些問題，我們提出了 LongIns 基準資料集，這是一個基於指令的具有挑戰性的 LLM 長文本考試，它是根據現有的指令資料集建立的。具體來說，在我們的 LongIns 中，我們引入了三種評估設定：全局指令和單一任務 (GIST)、局部指令和單一任務 (LIST) 以及局部指令和多個任務 (LIMT)。根據 LongIns，我們對現有的 LLM 進行了全面的評估，並有了以下重要的發現：(1)。在我們的 LongIns 中，表現最佳的 GPT-4，其上下文長度為 128k，在 16k 的評估上下文視窗中表現不佳。(2)。對於許多現有 LLM 的多跳推理能力，在短上下文視窗（小於 4k）下仍需要大量的努力。

##### **Towards Compositional Interpretability for XAI**
2406.17583v1 by Sean Tull, Robin Lorenz, Stephen Clark, Ilyas Khan, Bob Coecke

Artificial intelligence (AI) is currently based largely on black-box machine
learning models which lack interpretability. The field of eXplainable AI (XAI)
strives to address this major concern, being critical in high-stakes areas such
as the finance, legal and health sectors.
  We present an approach to defining AI models and their interpretability based
on category theory. For this we employ the notion of a compositional model,
which sees a model in terms of formal string diagrams which capture its
abstract structure together with its concrete implementation. This
comprehensive view incorporates deterministic, probabilistic and quantum
models. We compare a wide range of AI models as compositional models, including
linear and rule-based models, (recurrent) neural networks, transformers, VAEs,
and causal and DisCoCirc models.
  Next we give a definition of interpretation of a model in terms of its
compositional structure, demonstrating how to analyse the interpretability of a
model, and using this to clarify common themes in XAI. We find that what makes
the standard 'intrinsically interpretable' models so transparent is brought out
most clearly diagrammatically. This leads us to the more general notion of
compositionally-interpretable (CI) models, which additionally include, for
instance, causal, conceptual space, and DisCoCirc models.
  We next demonstrate the explainability benefits of CI models. Firstly, their
compositional structure may allow the computation of other quantities of
interest, and may facilitate inference from the model to the modelled
phenomenon by matching its structure. Secondly, they allow for diagrammatic
explanations for their behaviour, based on influence constraints, diagram
surgery and rewrite explanations. Finally, we discuss many future directions
for the approach, raising the question of how to learn such meaningfully
structured models in practice.

摘要：<paragraph>人工智慧（AI）目前在很大程度上依賴於缺乏可解釋性的黑盒機器學習模型。可解釋性人工智慧（XAI）領域致力於解決這個主要問題，這在金融、法律和健康等高風險領域至關重要。
我們提出了一種基於範疇論定義 AI 模型及其可解釋性的方法。為此，我們採用組合模型的概念，它以形式弦圖的形式看待模型，這些弦圖捕獲了模型的抽象結構及其具體實現。這種綜合觀點包含了確定性、概率性和量子模型。我們將各種 AI 模型作為組合模型進行比較，包括線性和基於規則的模型、（遞迴）神經網路、Transformer、VAE，以及因果和 DisCoCirc 模型。
接下來，我們根據模型的組合結構給出模型解釋的定義，展示如何分析模型的可解釋性，並使用它來澄清 XAI 中的常見主題。我們發現，讓標準的「內在可解釋」模型如此透明的原因在圖表中表現得最為清楚。這引導我們得出更一般的組合可解釋（CI）模型概念，它另外還包括因果、概念空間和 DisCoCirc 模型。
接下來，我們展示了 CI 模型的可解釋性優勢。首先，它們的組合結構允許計算其他感興趣的量，並可能通過匹配模型的結構來促進從模型到被建模現象的推理。其次，它們允許對其行為進行圖解說明，這些說明基於影響約束、圖解手術和重寫說明。最後，我們討論了這種方法的許多未來方向，提出了如何在實踐中學習這種有意義的結構化模型的問題。</paragraph>

##### **Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for Querying and Classifying IoT Threats**
2406.17574v1 by Ryan Pavlich, Nima Ebadi, Richard Tarbell, Billy Linares, Adrian Tan, Rachael Humphreys, Jayanta Kumar Das, Rambod Ghandiparsi, Hannah Haley, Jerris George, Rocky Slavin, Kim-Kwang Raymond Choo, Glenn Dietrich, Anthony Rios

Recognizing the promise of natural language interfaces to databases, prior
studies have emphasized the development of text-to-SQL systems. While
substantial progress has been made in this field, existing research has
concentrated on generating SQL statements from text queries. The broader
challenge, however, lies in inferring new information about the returned data.
Our research makes two major contributions to address this gap. First, we
introduce a novel Internet-of-Things (IoT) text-to-SQL dataset comprising
10,985 text-SQL pairs and 239,398 rows of network traffic activity. The dataset
contains additional query types limited in prior text-to-SQL datasets, notably
temporal-related queries. Our dataset is sourced from a smart building's IoT
ecosystem exploring sensor read and network traffic data. Second, our dataset
allows two-stage processing, where the returned data (network traffic) from a
generated SQL can be categorized as malicious or not. Our results show that
joint training to query and infer information about the data can improve
overall text-to-SQL performance, nearly matching substantially larger models.
We also show that current large language models (e.g., GPT3.5) struggle to
infer new information about returned data, thus our dataset provides a novel
test bed for integrating complex domain-specific reasoning into LLMs.

摘要：認識到自然語言介面對資料庫的承諾，先前的研究強調了文本到 SQL 系統的開發。雖然這個領域已經取得了實質進展，但現有的研究集中在從文字查詢產生 SQL 陳述。然而，更廣泛的挑戰在於推論出關於返回資料的新資訊。我們的研究對解決這個差距做出了兩項重大貢獻。首先，我們引入了一個新穎的物聯網 (IoT) 文本到 SQL 資料集，包含 10,985 個文本到 SQL 對和 239,398 列的網路流量活動。該資料集包含先前文本到 SQL 資料集中受限的額外查詢類型，特別是與時間相關的查詢。我們的資料集來自智慧建築的物聯網生態系統，探索感測器讀取和網路流量資料。其次，我們的資料集允許兩階段處理，其中從產生的 SQL 返回的資料（網路流量）可以分類為惡意或非惡意。我們的結果表明，聯合訓練查詢和推論資料資訊可以改善整體文本到 SQL 的效能，幾乎與大幅更大的模型相匹配。我們也表明，目前的大語言模型（例如 GPT3.5）難以推論出關於返回資料的新資訊，因此我們的資料集提供了一個新穎的測試平台，用於將複雜的特定領域推理整合到 LLM 中。

##### **FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating Toxicity in French Texts**
2406.17566v1 by Caroline Brun, Vassilina Nikoulina

Large language models (LLMs) are increasingly popular but are also prone to
generating bias, toxic or harmful language, which can have detrimental effects
on individuals and communities. Although most efforts is put to assess and
mitigate toxicity in generated content, it is primarily concentrated on
English, while it's essential to consider other languages as well. For
addressing this issue, we create and release FrenchToxicityPrompts, a dataset
of 50K naturally occurring French prompts and their continuations, annotated
with toxicity scores from a widely used toxicity classifier. We evaluate 14
different models from four prevalent open-sourced families of LLMs against our
dataset to assess their potential toxicity across various dimensions. We hope
that our contribution will foster future research on toxicity detection and
mitigation beyond Englis

摘要：大型語言模型（LLM）越來越受歡迎，但也很容易產生有偏見、有毒或有害的語言，這可能會對個人和社群產生負面影響。雖然大多數的努力都放在評估和減輕生成內容中的毒性上，但主要集中在英語上，同時也必須考慮其他語言。為了解決此問題，我們建立並發布 FrenchToxicityPrompts，這是一個包含 50K 個自然發生的法語提示及其延續的資料集，並使用廣泛使用的毒性分類器標記毒性分數。我們根據資料集評估來自四個流行的 LLM 開源系列的 14 個不同模型，以評估它們在不同面向的潛在毒性。我們希望我們的貢獻將促進未來對英語以外的毒性偵測和減緩的研究。

##### **Multi-property Steering of Large Language Models with Dynamic Activation Composition**
2406.17563v1 by Daniel Scalena, Gabriele Sarti, Malvina Nissim

Activation steering methods were shown to be effective in conditioning
language model generation by additively intervening over models' intermediate
representations. However, the evaluation of these techniques has so far been
limited to single conditioning properties and synthetic settings. In this work,
we conduct a comprehensive evaluation of various activation steering
strategies, highlighting the property-dependent nature of optimal parameters to
ensure a robust effect throughout generation. To address this issue, we propose
Dynamic Activation Composition, an information-theoretic approach to modulate
the steering intensity of one or more properties throughout generation. Our
experiments on multi-property steering show that our method successfully
maintains high conditioning while minimizing the impact of conditioning on
generation fluency.

摘要：激活转向方法被證明在通過對模型的中間表示進行加性干預來調節語言模型生成方面是有效的。然而，到目前為止，對這些技術的評估僅限於單一調節屬性和合成設置。在這項工作中，我們對各種激活轉向策略進行了全面評估，強調了最佳參數的屬性依賴性，以確保在整個生成過程中產生穩健的影響。為了解決這個問題，我們提出了動態激活組合，這是一種信息論方法，用於在整個生成過程中調節一個或多個屬性的轉向強度。我們對多屬性轉向的實驗表明，我們的模型在最大程度地降低調節對生成流暢性的影響的同時，成功地保持了高調節。

##### **The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale**
2406.17557v1 by Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf

The performance of a large language model (LLM) depends heavily on the
quality and size of its pretraining dataset. However, the pretraining datasets
for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly
available and very little is known about how they were created. In this work,
we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl
snapshots that produces better-performing LLMs than other open pretraining
datasets. To advance the understanding of how best to curate high-quality
pretraining datasets, we carefully document and ablate all of the design
choices used in FineWeb, including in-depth investigations of deduplication and
filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion
token collection of educational text filtered from FineWeb. LLMs pretrained on
FineWeb-Edu exhibit dramatically better performance on knowledge- and
reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we
publicly release our data curation codebase and all of the models trained
during our ablation experiments.

摘要：大型語言模型 (LLM) 的效能取決於其預訓練資料集的品質和規模。然而，最先進的開放 LLM（例如 Llama 3 和 Mixtral）的預訓練資料集並未公開，而且我們對於它們的建立方式所知甚少。在這項工作中，我們引入了 FineWeb，這是一個由 96 個 Common Crawl 快照衍生的 15 兆個代幣的資料集，它產生的 LLM 效能優於其他開放預訓練資料集。為了增進我們對於如何最佳策劃高品質預訓練資料集的理解，我們仔細記錄並消融了 FineWeb 中使用的所有設計選擇，包括對重複資料刪除和過濾策略的深入探討。此外，我們引入了 FineWeb-Edu，這是一個從 FineWeb 中過濾出的 1.3 兆個代幣的教育文本集合。在 FineWeb-Edu 上預訓練的 LLM 在知識和推理密集的基準（例如 MMLU 和 ARC）上展現出顯著更好的效能。除了我們的資料集之外，我們還公開發布了我們的資料策劃程式碼庫以及在我們的消融實驗中訓練的所有模型。

##### **Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft**
2406.17553v1 by Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen

In the Minecraft Collaborative Building Task, two players collaborate: an
Architect (A) provides instructions to a Builder (B) to assemble a specified
structure using 3D blocks. In this work, we investigate the use of large
language models (LLMs) to predict the sequence of actions taken by the Builder.
Leveraging LLMs' in-context learning abilities, we use few-shot prompting
techniques, that significantly improve performance over baseline methods.
Additionally, we present a detailed analysis of the gaps in performance for
future work

摘要：在 Minecraft 協作建造任務中，兩位玩家協作：一名建築師 (A) 向一名建造者 (B) 提供指示，使用 3D 方塊組裝一個指定的結構。在這項工作中，我們研究使用大型語言模型 (LLM) 來預測建造者採取的動作順序。利用 LLM 的情境學習能力，我們使用少次提示技術，大幅提升了基準方法的效能。此外，我們針對效能差距提出詳細的分析，以利於後續工作

##### **CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained Models using Greedy Coordinate Descent**
2406.17542v2 by Pranav Ajit Nair, Arun Sai Suggala

Large language models (LLMs) have recently demonstrated remarkable
performance across diverse language tasks. But their deployment is often
constrained by their substantial computational and storage requirements.
Quantization has emerged as a key technique for addressing this challenge,
enabling the compression of large models with minimal impact on performance.
The recent GPTQ algorithm, a post-training quantization (PTQ) method, has
proven highly effective for compressing LLMs, sparking a wave of research that
leverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the
PTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ
with improved performance. CDQuant uses coordinate descent to minimize the
layer-wise reconstruction loss to achieve high-quality quantized weights. Our
algorithm is easy to implement and scales efficiently to models with hundreds
of billions of parameters. Through extensive evaluation on the PaLM2 model
family, we demonstrate that CDQuant consistently outperforms GPTQ across
diverse model sizes and quantization levels. In particular, for INT2
quantization of PaLM2-Otter, CDQuant achieves a 10% reduction in perplexity
compared to GPTQ.

摘要：大型語言模型 (LLM) 近期在各種語言任務中展現出顯著的效能。但其部署通常受到其龐大的運算和儲存需求所限制。量化已成為解決此挑戰的一項關鍵技術，能壓縮大型模型，且對效能的影響極小。最近的 GPTQ 演算法，一種訓練後量化 (PTQ) 方法，已被證明對於壓縮 LLM 非常有效，引發了一波以 GPTQ 為核心元件的研究浪潮。鑑於 GPTQ 在 PTQ 領域中的關鍵角色，我們引入了 CDQuant，一種簡單且可擴充的 GPTQ 替代方案，且效能有所提升。CDQuant 使用座標下降來最小化層級重建損失，以達成高品質的量化權重。我們的演算法易於實作，且可有效擴充至具有數千億個參數的模型。透過對 PaLM2 模型系列進行廣泛評估，我們證明 CDQuant 在各種模型大小和量化層級中始終優於 GPTQ。特別是，對於 PaLM2-Otter 的 INT2 量化，CDQuant 與 GPTQ 相比，困惑度降低了 10%。

##### **SincVAE: a New Approach to Improve Anomaly Detection on EEG Data Using SincNet and Variational Autoencoder**
2406.17537v1 by Andrea Pollastro, Francesco Isgrò, Roberto Prevete

Over the past few decades, electroencephalography (EEG) monitoring has become
a pivotal tool for diagnosing neurological disorders, particularly for
detecting seizures. Epilepsy, one of the most prevalent neurological diseases
worldwide, affects approximately the 1 \% of the population. These patients
face significant risks, underscoring the need for reliable, continuous seizure
monitoring in daily life. Most of the techniques discussed in the literature
rely on supervised Machine Learning (ML) methods. However, the challenge of
accurately labeling variations in epileptic EEG waveforms complicates the use
of these approaches. Additionally, the rarity of ictal events introduces an
high imbalancing within the data, which could lead to poor prediction
performance in supervised learning approaches. Instead, a semi-supervised
approach allows to train the model only on data not containing seizures, thus
avoiding the issues related to the data imbalancing. This work proposes a
semi-supervised approach for detecting epileptic seizures from EEG data,
utilizing a novel Deep Learning-based method called SincVAE. This proposal
incorporates the learning of an ad-hoc array of bandpass filter as a first
layer of a Variational Autoencoder (VAE), potentially eliminating the
preprocessing stage where informative band frequencies are identified and
isolated. Results indicate that SincVAE improves seizure detection in EEG data
and is capable of identifying early seizures during the preictal stage as well
as monitoring patients throughout the postictal stage.

摘要：在過去的幾十年中，腦電圖 (EEG) 監控已成為診斷神經疾病的重要工具，特別是對於檢測癲癇發作。癲癇是全球最普遍的神經疾病之一，影響著大約 1% 的人口。這些患者面臨著重大的風險，這凸顯了在日常生活中進行可靠、持續的癲癇發作監控的必要性。文獻中討論的大多數技術都依賴於監督式機器學習 (ML) 方法。然而，準確標記癲癇 EEG 波形變化的挑戰使得這些方法的使用變得複雜。此外，發作事件的罕見性導致數據中存在嚴重的失衡，這可能會導致監督式學習方法的預測效果不佳。相反，半監督式方法允許僅使用不包含癲癇發作的數據來訓練模型，從而避免與數據失衡相關的問題。這項工作提出了一種半監督式方法，用於從 EEG 數據中檢測癲癇發作，利用一種稱為 SincVAE 的新穎深度學習方法。此提案將學習一組特別設計的帶通濾波器作為變分自編碼器 (VAE) 的第一層，從而潛在地消除了識別和分離信息頻帶的預處理階段。結果表明，SincVAE 改善了 EEG 數據中的癲癇發作檢測，並且能夠在發作前階段識別早期癲癇發作，並在發作後階段監控患者。

##### **Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark**
2406.17535v1 by Fabio Mercorio, Mario Mezzanzanica, Daniele Potertì, Antonio Serino, Andrea Seveso

Recent advancements in Large Language Models (LLMs) have significantly
enhanced their ability to generate and manipulate human language, highlighting
their potential across various applications. Evaluating LLMs in languages other
than English is crucial for ensuring their linguistic versatility, cultural
relevance, and applicability in diverse global contexts, thus broadening their
usability and effectiveness. We tackle this challenge by introducing a
structured benchmark using the INVALSI tests, a set of well-established
assessments designed to measure educational competencies across Italy. Our
study makes three primary contributions: Firstly, we adapt the INVALSI
benchmark for automated LLM evaluation, which involves rigorous adaptation of
the test format to suit automated processing while retaining the essence of the
original tests. Secondly, we provide a detailed assessment of current LLMs,
offering a crucial reference point for the academic community. Finally, we
visually compare the performance of these models against human results.
Additionally, researchers are invited to submit their models for ongoing
evaluation, ensuring the benchmark remains a current and valuable resource.

摘要：大型語言模型 (LLM) 的最新進展顯著增強了它們產生和處理人類語言的能力，突顯了它們在各種應用中的潛力。評估除英語以外語言中的 LLM，對於確保它們的語言通用性、文化相關性和在不同全球背景下的適用性至關重要，從而擴展它們的可用性和有效性。我們通過使用 INVALSI 測試引入結構化基準來應對這一挑戰，INVALSI 測試是一套完善的評估，旨在衡量義大利各地的教育能力。我們的研究做出了三項主要貢獻：首先，我們調整 INVALSI 基準以進行自動化 LLM 評估，其中涉及嚴格調整測試格式以適應自動化處理，同時保留原始測試的精髓。其次，我們對目前的 LLM 提供詳細評估，為學術界提供了一個重要的參考點。最後，我們將這些模型的性能與人類的結果進行視覺比較。此外，我們邀請研究人員提交他們的模型進行持續評估，確保基準仍然是一個最新且有價值的資源。

##### **Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification**
2406.17534v1 by Huiyao Chen, Yu Zhao, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang, Min Zhang

Hierarchical text classification (HTC) is an important task with broad
applications, while few-shot HTC has gained increasing interest recently. While
in-context learning (ICL) with large language models (LLMs) has achieved
significant success in few-shot learning, it is not as effective for HTC
because of the expansive hierarchical label sets and extremely-ambiguous
labels. In this work, we introduce the first ICL-based framework with LLM for
few-shot HTC. We exploit a retrieval database to identify relevant
demonstrations, and an iterative policy to manage multi-layer hierarchical
labels. Particularly, we equip the retrieval database with HTC label-aware
representations for the input texts, which is achieved by continual training on
a pretrained language model with masked language modeling (MLM), layer-wise
classification (CLS, specifically for HTC), and a novel divergent contrastive
learning (DCL, mainly for adjacent semantically-similar labels) objective.
Experimental results on three benchmark datasets demonstrate superior
performance of our method, and we can achieve state-of-the-art results in
few-shot HTC.

摘要：層級文本分類 (HTC) 是一項重要的任務，具有廣泛的應用，而少樣本 HTC 近來備受關注。雖然具備大型語言模型 (LLM) 的情境內學習 (ICL) 已在少樣本學習中獲得顯著成功，但由於龐大的層級標籤組和極其模糊的標籤，它對 HTC 而言並非那麼有效。在這項工作中，我們引入了第一個基於 ICL 的 LLM 框架，用於少樣本 HTC。我們利用檢索資料庫來識別相關示範，並採用反覆運算政策來管理多層層級標籤。特別是，我們使用 HTC 標籤感知表徵來為檢索資料庫配備輸入文本，這可透過持續在預訓練語言模型上進行遮罩語言建模 (MLM)、層級分類 (CLS，特別針對 HTC) 和一種新穎的發散對比學習 (DCL，主要針對相鄰的語義相似標籤) 目標來達成。在三個基準資料集上的實驗結果證明了我們方法的優異效能，而且我們可以在少樣本 HTC 中獲得最先進的結果。

##### **Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study**
2406.17532v1 by Keyu Wang, Guilin Qi, Jiaqi Li, Songlin Zhai

Large language models (LLMs) have shown significant achievements in solving a
wide range of tasks. Recently, LLMs' capability to store, retrieve and infer
with symbolic knowledge has drawn a great deal of attention, showing their
potential to understand structured information. However, it is not yet known
whether LLMs can understand Description Logic (DL) ontologies. In this work, we
empirically analyze the LLMs' capability of understanding DL-Lite ontologies
covering 6 representative tasks from syntactic and semantic aspects. With
extensive experiments, we demonstrate both the effectiveness and limitations of
LLMs in understanding DL-Lite ontologies. We find that LLMs can understand
formal syntax and model-theoretic semantics of concepts and roles. However,
LLMs struggle with understanding TBox NI transitivity and handling ontologies
with large ABoxes. We hope that our experiments and analyses provide more
insights into LLMs and inspire to build more faithful knowledge engineering
solutions.

摘要：大型語言模型 (LLM) 在解決廣泛的任務方面已展現出顯著的成就。最近，LLM 儲存、擷取和推論符號知識的能力引起了極大的關注，顯示出它們理解結構化資訊的潛力。然而，目前尚不清楚 LLM 是否能理解描述邏輯 (DL) 知識庫。在這項工作中，我們從句法和語義層面實證分析 LLM 理解 DL-Lite 知識庫的能力，涵蓋 6 項代表性任務。透過廣泛的實驗，我們展示了 LLM 在理解 DL-Lite 知識庫方面的效能和限制。我們發現 LLM 能理解概念和角色的形式句法和模型論語義。然而，LLM 在理解 TBox NI 傳遞性和處理具有大型 ABox 的知識庫方面有困難。我們希望我們的實驗和分析能為 LLM 提供更多見解，並激勵建構更忠實的知識工程解決方案。

##### **Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness**
2406.17531v1 by Lucrezia Grassi, Carmine Tommaso Recchiuto, Antonio Sgorbissa

This paper presents a system for diversity-aware autonomous conversation
leveraging the capabilities of large language models (LLMs). The system adapts
to diverse populations and individuals, considering factors like background,
personality, age, gender, and culture. The conversation flow is guided by the
structure of the system's pre-established knowledge base, while LLMs are tasked
with various functions, including generating diversity-aware sentences.
Achieving diversity-awareness involves providing carefully crafted prompts to
the models, incorporating comprehensive information about users, conversation
history, contextual details, and specific guidelines. To assess the system's
performance, we conducted both controlled and real-world experiments, measuring
a wide range of performance indicators.

摘要：本文提出了一個系統，用於多元化的自主對話，利用大型語言模型 (LLM) 的能力。該系統適應不同的族群和個人，考慮背景、個性、年齡、性別和文化等因素。對話流程由系統預先建立的知識庫結構引導，而 LLM 則負責各種功能，包括生成多元化的句子。實現多元化意識涉及向模型提供精心製作的提示，納入有關使用者、對話記錄、上下文詳細資料和具體指南的全面資訊。為了評估系統的效能，我們進行了受控和真實世界的實驗，衡量了廣泛的效能指標。

##### **LumberChunker: Long-Form Narrative Document Segmentation**
2406.17526v1 by André V. Duarte, João Marques, Miguel Graça, Miguel Freire, Lei Li, Arlindo L. Oliveira

Modern NLP tasks increasingly rely on dense retrieval methods to access
up-to-date and relevant contextual information. We are motivated by the premise
that retrieval benefits from segments that can vary in size such that a
content's semantic independence is better captured. We propose LumberChunker, a
method leveraging an LLM to dynamically segment documents, which iteratively
prompts the LLM to identify the point within a group of sequential passages
where the content begins to shift. To evaluate our method, we introduce
GutenQA, a benchmark with 3000 "needle in a haystack" type of question-answer
pairs derived from 100 public domain narrative books available on Project
Gutenberg. Our experiments show that LumberChunker not only outperforms the
most competitive baseline by 7.37% in retrieval performance (DCG@20) but also
that, when integrated into a RAG pipeline, LumberChunker proves to be more
effective than other chunking methods and competitive baselines, such as the
Gemini 1.5M Pro. Our Code and Data are available at
https://github.com/joaodsmarques/LumberChunker

摘要：現代 NLP 任務越來越依賴密集檢索方法來存取最新且相關的脈絡資訊。我們的動機是基於檢索受益於大小可變的區段，以便更好地擷取內容的語意獨立性。我們提出 LumberChunker，一種利用 LLM 動態區隔文件的方法，它會反覆提示 LLM 找出連續段落群組中內容開始轉移的點。為了評估我們的這項方法，我們引入了 GutenQA，一個基準，其中包含 3000 個「大海撈針」類型的問答對，這些對來自專案古騰堡上 100 本可公開取得的敘事書籍。我們的實驗顯示，LumberChunker 不僅在檢索效能 (DCG@20) 上優於競爭最激烈的基準 7.37%，而且，當整合到 RAG 管線中時，LumberChunker 證明比其他區塊方法和競爭基準更有效，例如 Gemini 1.5M Pro。我們的程式碼和資料可在 https://github.com/joaodsmarques/LumberChunker 取得

##### **Entropy-Based Decoding for Retrieval-Augmented Large Language Models**
2406.17519v1 by Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King

Augmenting Large Language Models (LLMs) with retrieved external knowledge has
proven effective for improving the factual accuracy of generated responses.
Despite their success, retrieval-augmented LLMs still face the distractibility
issue, where the generated responses are negatively influenced by noise from
both external and internal knowledge sources. In this paper, we introduce a
novel, training-free decoding method guided by entropy considerations to
mitigate this issue. Our approach utilizes entropy-based document-parallel
ensemble decoding to prioritize low-entropy distributions from retrieved
documents, thereby enhancing the extraction of relevant information of context.
Additionally, it incorporates a contrastive decoding mechanism that contrasts
the obtained low-entropy ensemble distribution with the high-entropy
distribution derived from the model's internal knowledge across layers, which
ensures a greater emphasis on reliable external information. Extensive
experiments on open-domain question answering datasets demonstrate the
superiority of our method.

摘要：擴充大型語言模型 (LLM) 與擷取外部知識已被證實可有效提升回應事實的準確性。儘管它們很成功，但擷取擴充的 LLM 仍面臨分心問題，其中產生的回應受到來自外部和內部知識來源雜訊的負面影響。在本文中，我們介紹一種新穎的、免訓練的解碼方法，以熵考量為導向來減輕此問題。我們的做法利用基於熵的文件平行整體解碼，以優先處理擷取文件中的低熵分佈，從而增強對脈絡相關資訊的提取。此外，它結合對比解碼機制，將取得的低熵整體分佈與模型從各層中衍生的高熵分佈進行對比，這確保了更強調可靠的外部資訊。在開放領域問答資料集上的廣泛實驗證明了我們方法的優越性。

##### **Benchmarking Mental State Representations in Language Models**
2406.17513v1 by Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling

While numerous works have assessed the generative performance of language
models (LMs) on tasks requiring Theory of Mind reasoning, research into the
models' internal representation of mental states remains limited. Recent work
has used probing to demonstrate that LMs can represent beliefs of themselves
and others. However, these claims are accompanied by limited evaluation, making
it difficult to assess how mental state representations are affected by model
design and training choices. We report an extensive benchmark with various LM
types with different model sizes, fine-tuning approaches, and prompt designs to
study the robustness of mental state representations and memorisation issues
within the probes. Our results show that the quality of models' internal
representations of the beliefs of others increases with model size and, more
crucially, with fine-tuning. We are the first to study how prompt variations
impact probing performance on theory of mind tasks. We demonstrate that models'
representations are sensitive to prompt variations, even when such variations
should be beneficial. Finally, we complement previous activation editing
experiments on Theory of Mind tasks and show that it is possible to improve
models' reasoning performance by steering their activations without the need to
train any probe.

摘要：儘管許多作品已經評估了語言模型 (LM) 在需要心智理論推理的任務上的生成效能，但對於心智狀態模型的內部表徵的研究仍然有限。最近的研究已使用探測來證明 LM 可以表徵自己和別人的信念。然而，這些說法伴隨著有限的評估，這使得難以評估心智狀態表徵如何受到模型設計和訓練選擇的影響。我們報告了一個廣泛的基準，其中包含具有不同模型大小、微調方法和提示設計的各種 LM 類型，以研究探測中的心智狀態表徵和記憶問題的穩健性。我們的結果表明，模型對他人信念的內部表徵的品質會隨著模型大小的增加而增加，更重要的是，隨著微調而增加。我們是第一個研究提示變異如何影響心智理論任務探測效能的人。我們證明了模型的表徵對提示變異很敏感，即使這些變異應該是好的。最後，我們補充了之前在心智理論任務上進行的激活編輯實驗，並表明可以透過引導模型的激活來改善模型的推理效能，而無需訓練任何探測。

##### **MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation**
2406.17484v1 by Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang

Large language models (LLMs) have shown substantial progress in natural
language understanding and generation, proving valuable especially in the
medical field. Despite advancements, challenges persist due to the complexity
and diversity inherent in medical tasks, which can be categorized as
knowledge-intensive tasks and alignment-required tasks. Previous approaches
either ignore the latter task or focus on a minority of tasks and hence lose
generalization. To address these drawbacks, we propose a progressive
fine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise
aggregator to encode diverse knowledge in the first stage and filter out
detrimental information. In the second stage, we drop the Noise Aggregator to
avoid the interference of suboptimal representation and leverage an additional
alignment module optimized towards an orthogonal direction to the knowledge
space to mitigate knowledge forgetting. Based on this two-stage paradigm, we
proposed a Medical LLM through decoupling Clinical Alignment and Knowledge
Aggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)
performance on over 20 medical tasks, as well as SOTA results on specific
medical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all
demonstrate significant improvements over existing models with similar model
sizes.

摘要：大型語言模型 (LLM) 已在自然語言理解和生成方面取得重大進展，特別是在醫療領域證明了其價值。儘管有進展，但由於醫療任務固有的複雜性和多樣性，挑戰依然存在，這些任務可歸類為知識密集型任務和需要對齊的任務。先前的做法要么忽略後者任務，要么專注於少數任務，因此失去了概括性。為了解決這些缺點，我們提出了一種漸進式微調管道。此管道在第一階段採用知識聚合器和噪音聚合器對不同的知識進行編碼，並過濾掉有害信息。在第二階段，我們放棄噪音聚合器，以避免次優表示的干擾，並利用額外對齊模組，該模組針對與知識空間正交的方向進行最佳化，以減輕知識遺忘。基於這個兩階段範例，我們通過解耦臨床對齊和知識聚合 (MedCare) 提出了一個醫療 LLM，其設計旨在在 20 多項醫療任務上實現最先進 (SOTA) 效能，以及在特定醫療對齊任務上實現 SOTA 結果。MedCare 的各種模型大小（1.8B、7B、14B）均顯示出比具有類似模型大小的現有模型有顯著的改進。

##### **Transformer-based Named Entity Recognition with Combined Data Representation**
2406.17474v1 by Michał Marcińczuk

This study examines transformer-based models and their effectiveness in named
entity recognition tasks. The study investigates data representation
strategies, including single, merged, and context, which respectively use one
sentence, multiple sentences, and sentences joined with attention to context
per vector. Analysis shows that training models with a single strategy may lead
to poor performance on different data representations. To address this
limitation, the study proposes a combined training procedure that utilizes all
three strategies to improve model stability and adaptability. The results of
this approach are presented and discussed for four languages (English, Polish,
Czech, and German) across various datasets, demonstrating the effectiveness of
the combined strategy.

摘要：本研究探討基於 Transformer 的模型及其在命名實體辨識任務中的效能。這項研究調查資料表示策略，包括單一、合併和語境，分別使用一個句子、多個句子和與語境注意力結合的句子作為每個向量的輸入。分析顯示，使用單一策略訓練模型可能會在不同的資料表示上導致效能不佳。為了解決此限制，本研究提出一個結合訓練程序，利用所有三種策略來提升模型的穩定性和適應性。此方法的結果針對四種語言（英語、波蘭語、捷克語和德語）在各種資料集上進行呈現和討論，證明了結合策略的效能。

##### **TSynD: Targeted Synthetic Data Generation for Enhanced Medical Image Classification**
2406.17473v1 by Joshua Niemeijer, Jan Ehrhardt, Hristina Uzunova, Heinz Handels

The usage of medical image data for the training of large-scale machine
learning approaches is particularly challenging due to its scarce availability
and the costly generation of data annotations, typically requiring the
engagement of medical professionals. The rapid development of generative models
allows towards tackling this problem by leveraging large amounts of realistic
synthetically generated data for the training process. However, randomly
choosing synthetic samples, might not be an optimal strategy.
  In this work, we investigate the targeted generation of synthetic training
data, in order to improve the accuracy and robustness of image classification.
Therefore, our approach aims to guide the generative model to synthesize data
with high epistemic uncertainty, since large measures of epistemic uncertainty
indicate underrepresented data points in the training set. During the image
generation we feed images reconstructed by an auto encoder into the classifier
and compute the mutual information over the class-probability distribution as a
measure for uncertainty.We alter the feature space of the autoencoder through
an optimization process with the objective of maximizing the classifier
uncertainty on the decoded image. By training on such data we improve the
performance and robustness against test time data augmentations and adversarial
attacks on several classifications tasks.

摘要：由於醫療影像資料的取得不易，且資料標註的產生成本高昂，通常需要醫療專業人員參與，因此使用醫療影像資料來訓練大型機器學習方法特別具有挑戰性。生成式模型的快速發展，允許透過利用大量逼真的合成資料來訓練流程，以解決此問題。然而，隨機選擇合成樣本可能不是最佳策略。
  在這項工作中，我們研究合成訓練資料的目標生成，以提高影像分類的準確性和穩健性。因此，我們的方法旨在引導生成式模型合成具有高認識論不確定性的資料，因為認識論不確定性的高指標表示訓練集中代表性不足的資料點。在影像生成過程中，我們將自動編碼器重建的影像輸入分類器，並計算類別機率分佈的互資訊作為不確定性的指標。我們透過優化流程來改變自動編碼器的特徵空間，目標是最大化解碼影像上分類器的未確定性。透過訓練此類資料，我們改善了在多項分類任務中對測試時間資料擴充和對抗攻擊的效能和穩健性。

##### **Dynamic Scheduling for Vehicle-to-Vehicle Communications Enhanced Federated Learning**
2406.17470v1 by Jintao Yan, Tan Chen, Yuxuan Sun, Zhaojun Nan, Sheng Zhou, Zhisheng Niu

Leveraging the computing and sensing capabilities of vehicles, vehicular
federated learning (VFL) has been applied to edge training for connected
vehicles. The dynamic and interconnected nature of vehicular networks presents
unique opportunities to harness direct vehicle-to-vehicle (V2V) communications,
enhancing VFL training efficiency. In this paper, we formulate a stochastic
optimization problem to optimize the VFL training performance, considering the
energy constraints and mobility of vehicles, and propose a V2V-enhanced dynamic
scheduling (VEDS) algorithm to solve it. The model aggregation requirements of
VFL and the limited transmission time due to mobility result in a stepwise
objective function, which presents challenges in solving the problem. We thus
propose a derivative-based drift-plus-penalty method to convert the long-term
stochastic optimization problem to an online mixed integer nonlinear
programming (MINLP) problem, and provide a theoretical analysis to bound the
performance gap between the online solution and the offline optimal solution.
Further analysis of the scheduling priority reduces the original problem into a
set of convex optimization problems, which are efficiently solved using the
interior-point method. Experimental results demonstrate that compared with the
state-of-the-art benchmarks, the proposed algorithm enhances the image
classification accuracy on the CIFAR-10 dataset by 3.18% and reduces the
average displacement errors on the Argoverse trajectory prediction dataset by
10.21%.

摘要：<paragraph>利用車輛的計算和感測能力，車輛聯邦學習 (VFL) 已應用於連網車輛的邊緣訓練。車輛網路的動態和互連特性提供了利用車對車 (V2V) 直接通訊的獨特機會，進而提升 VFL 訓練效率。在本文中，我們制定了一個隨機最佳化問題，以最佳化 VFL 訓練效能，考量車輛的能源限制和流動性，並提出一個 V2V 增強動態排程 (VEDS) 演算法來解決它。VFL 的模型聚合需求和流動性造成的傳輸時間有限，導致一個階梯式目標函數，這在解決問題時帶來挑戰。因此，我們提出一個基於導數的漂移加懲罰方法，將長期隨機最佳化問題轉換為一個線上混合整數非線性規劃 (MINLP) 問題，並提供一個理論分析來界定線上解與離線最佳解之間的效能差距。進一步分析排程優先順序，將原始問題簡化為一組凸最佳化問題，使用內點法有效地解決這些問題。實驗結果顯示，與最先進的基準相比，所提出的演算法將 CIFAR-10 資料集上的影像分類準確度提升了 3.18%，並將 Argoverse 軌跡預測資料集上的平均位移誤差降低了 10.21%。</paragraph>

##### **Enhancing Tool Retrieval with Iterative Feedback from Large Language Models**
2406.17465v1 by Qiancheng Xu, Yongqi Li, Heming Xia, Wenjie Li

Tool learning aims to enhance and expand large language models' (LLMs)
capabilities with external tools, which has gained significant attention
recently. Current methods have shown that LLMs can effectively handle a certain
amount of tools through in-context learning or fine-tuning. However, in
real-world scenarios, the number of tools is typically extensive and
irregularly updated, emphasizing the necessity for a dedicated tool retrieval
component. Tool retrieval is nontrivial due to the following challenges: 1)
complex user instructions and tool descriptions; 2) misalignment between tool
retrieval and tool usage models. To address the above issues, we propose to
enhance tool retrieval with iterative feedback from the large language model.
Specifically, we prompt the tool usage model, i.e., the LLM, to provide
feedback for the tool retriever model in multi-round, which could progressively
improve the tool retriever's understanding of instructions and tools and reduce
the gap between the two standalone components. We build a unified and
comprehensive benchmark to evaluate tool retrieval models. The extensive
experiments indicate that our proposed approach achieves advanced performance
in both in-domain evaluation and out-of-domain evaluation.

摘要：工具学习旨在通过外部工具增强和扩展大型语言模型 (LLM) 的功能，这最近引起了极大的关注。当前的方法表明，LLM 可以通过上下文学习或微调来有效处理一定数量的工具。然而，在实际场景中，工具的数量通常很大且更新不规律，强调了专门的工具检索组件的必要性。由于以下挑战，工具检索并非易事：1) 复杂的用户指令和工具描述；2) 工具检索和工具使用模型之间的不一致。为了解决上述问题，我们建议使用来自大型语言模型的迭代反馈来增强工具检索。具体来说，我们提示工具使用模型，即 LLM，在多轮中为工具检索器模型提供反馈，这可以逐步提高工具检索器对指令和工具的理解，并缩小两个独立组件之间的差距。我们构建了一个统一且全面的基准来评估工具检索模型。大量的实验表明，我们提出的方法在域内评估和域外评估中都取得了先进的性能。

##### **The Tree of Diffusion Life: Evolutionary Embeddings to Understand the Generation Process of Diffusion Models**
2406.17462v1 by Vidya Prasad, Hans van Gorp, Christina Humer, Anna Vilanova, Nicola Pezzotti

Diffusion models generate high-quality samples by corrupting data with
Gaussian noise and iteratively reconstructing it with deep learning, slowly
transforming noisy images into refined outputs. Understanding this data
evolution is important for interpretability but is complex due to its
high-dimensional evolutionary nature. While traditional dimensionality
reduction methods like t-distributed stochastic neighborhood embedding (t-SNE)
aid in understanding high-dimensional spaces, they neglect evolutionary
structure preservation. Hence, we propose Tree of Diffusion Life (TDL), a
method to understand data evolution in the generative process of diffusion
models. TDL samples a diffusion model's generative space via instances with
varying prompts and employs image encoders to extract semantic meaning from
these samples, projecting them to an intermediate space. It employs a novel
evolutionary embedding algorithm that explicitly encodes the iterations while
preserving the high-dimensional relations, facilitating the visualization of
data evolution. This embedding leverages three metrics: a standard t-SNE loss
to group semantically similar elements, a displacement loss to group elements
from the same iteration step, and an instance alignment loss to align elements
of the same instance across iterations. We present rectilinear and radial
layouts to represent iterations, enabling comprehensive exploration. We assess
various feature extractors and highlight TDL's potential with prominent
diffusion models like GLIDE and Stable Diffusion with different prompt sets.
TDL simplifies understanding data evolution within diffusion models, offering
valuable insights into their functioning.

摘要：擴散模型透過加入高斯雜訊到資料中並用深度學習反覆重建資料，產生高品質的樣本，並慢慢將有雜訊的圖片轉換成精緻的輸出。了解這種資料演化對於可解釋性很重要，但由於其高維度的演化性質，這很複雜。雖然傳統的降維方法，例如 t 分佈隨機鄰域嵌入 (t-SNE)，有助於理解高維度空間，但它們忽略了演化結構的保存。因此，我們提出擴散生命樹 (TDL)，一種了解擴散模型生成過程中資料演化的方法。TDL 透過具有不同提示的實例取樣擴散模型的生成空間，並使用影像編碼器從這些樣本中提取語義意義，將它們投影到中間空間。它採用一種新穎的演化嵌入演算法，在保留高維度關係的同時明確編碼迭代，促進資料演化的視覺化。此嵌入利用三個指標：標準 t-SNE 損失函數來將語義相似的元素分組、位移損失函數來將來自相同迭代步驟的元素分組，以及實例對齊損失函數來對齊同一實例中跨迭代的元素。我們提出直線和徑向佈局來表示迭代，實現全面的探索。我們評估各種特徵萃取器，並強調 TDL 在具有不同提示集的 GLIDE 和 Stable Diffusion 等顯著擴散模型中的潛力。TDL 簡化了對擴散模型中資料演化的理解，提供了對其運作的寶貴見解。

##### **Improving Grammatical Error Correction via Contextual Data Augmentation**
2406.17456v1 by Yixuan Wang, Baoxin Wang, Yijun Liu, Qingfu Zhu, Dayong Wu, Wanxiang Che

Nowadays, data augmentation through synthetic data has been widely used in
the field of Grammatical Error Correction (GEC) to alleviate the problem of
data scarcity. However, these synthetic data are mainly used in the
pre-training phase rather than the data-limited fine-tuning phase due to
inconsistent error distribution and noisy labels. In this paper, we propose a
synthetic data construction method based on contextual augmentation, which can
ensure an efficient augmentation of the original data with a more consistent
error distribution. Specifically, we combine rule-based substitution with
model-based generation, using the generative model to generate a richer context
for the extracted error patterns. Besides, we also propose a relabeling-based
data cleaning method to mitigate the effects of noisy labels in synthetic data.
Experiments on CoNLL14 and BEA19-Test show that our proposed augmentation
method consistently and substantially outperforms strong baselines and achieves
the state-of-the-art level with only a few synthetic data.

摘要：如今，透過合成資料進行資料擴增已廣泛用於文法錯誤修正 (GEC) 領域，以減輕資料稀少的問題。然而，這些合成資料主要用於預訓練階段，而非資料有限的微調階段，原因在於錯誤分佈不一致且標籤有雜訊。在本文中，我們提出一個基於脈絡擴增的合成資料建構方法，這可以確保以更一致的錯誤分佈有效擴增原始資料。具體來說，我們將基於規則的替換與基於模型的生成結合起來，使用生成模型來為提取的錯誤模式生成更豐富的脈絡。此外，我們還提出一個基於重新標記的資料清理方法，以減輕合成資料中雜訊標籤的影響。在 CoNLL14 和 BEA19-Test 上的實驗顯示，我們提出的擴增方法始終且大幅優於強大的基準，且僅使用少量的合成資料就達到了最先進的水平。

##### **Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain**
2406.17453v1 by Davide Mazzaccara, Alberto Testoni, Raffaella Bernardi

Questions are essential tools for acquiring the necessary information to
complete information-seeking tasks. However, large language models (LLMs),
especially open-source models, often perform poorly in generating informative
questions, as measured by expected information gain (EIG). In this paper, we
propose a method to enhance the informativeness of LLM-generated questions in
20-question game dialogues. We sample multiple questions from the same model
(LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG
questions to apply a Direct Preference Optimization (DPO) algorithm. Our
results show that this method produces more effective questions (in terms of
EIG), even in domains different from those used to train the DPO model.

摘要：問題是獲取完成資訊尋求任務所需資訊的基本工具。然而，大型語言模型 (LLM)，特別是開源模型，在產生資訊性問題方面往往表現不佳，而資訊增益預期值 (EIG) 是衡量標準。在本文中，我們提出了一種方法來增強 LLM 生成的問題在 20 題問答對話中的資訊性。我們從同一個模型（LLAMA 2-CHAT 7B）為每個遊戲抽樣多個問題，並建立低 EIG 和高 EIG 問題對，以應用直接偏好最佳化 (DPO) 演算法。我們的結果顯示，此方法產生了更有效的問題（根據 EIG），即使在與用於訓練 DPO 模型不同的領域中也是如此。

##### **Pseudo Labelling for Enhanced Masked Autoencoders**
2406.17450v1 by Srinivasa Rao Nandam, Sara Atito, Zhenhua Feng, Josef Kittler, Muhammad Awais

Masked Image Modeling (MIM)-based models, such as SdAE, CAE, GreenMIM, and
MixAE, have explored different strategies to enhance the performance of Masked
Autoencoders (MAE) by modifying prediction, loss functions, or incorporating
additional architectural components. In this paper, we propose an enhanced
approach that boosts MAE performance by integrating pseudo labelling for both
class and data tokens, alongside replacing the traditional pixel-level
reconstruction with token-level reconstruction. This strategy uses cluster
assignments as pseudo labels to promote instance-level discrimination within
the network, while token reconstruction requires generation of discrete tokens
encapturing local context. The targets for pseudo labelling and reconstruction
needs to be generated by a teacher network. To disentangle the generation of
target pseudo labels and the reconstruction of the token features, we decouple
the teacher into two distinct models, where one serves as a labelling teacher
and the other as a reconstruction teacher. This separation proves empirically
superior to a single teacher, while having negligible impact on throughput and
memory consumption. Incorporating pseudo-labelling as an auxiliary task has
demonstrated notable improvements in ImageNet-1K and other downstream tasks,
including classification, semantic segmentation, and detection.

摘要：基於遮罩圖像建模 (MIM) 的模型，例如 SdAE、CAE、GreenMIM 和 MixAE，已探索不同的策略，藉由修改預測、損失函數或整合額外的架構元件來提升遮罩自動編碼器 (MAE) 的效能。在本文中，我們提出了一種增強方法，透過整合類別和資料代碼的偽標籤，以及用代碼層級重建取代傳統的像素層級重建，來提升 MAE 效能。此策略使用叢集分配作為偽標籤，以促進網路內的實例層級辨別，而代碼重建則需要產生擷取局部內容的離散代碼。偽標籤和重建的目標需要由教師網路產生。為了解開目標偽標籤的產生和代碼特徵的重建，我們將教師解耦成兩個不同的模型，其中一個作為標籤教師，另一個作為重建教師。此分離經過實證證明優於單一教師，同時對吞吐量和記憶體消耗的影響可以忽略不計。將偽標籤納入輔助任務已證實顯著提升 ImageNet-1K 和其他下游任務，包括分類、語意分割和偵測。

##### **Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights**
2406.17430v1 by Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari

Large Multimodal Models (LMMs) have achieved great success recently,
demonstrating a strong capability to understand multimodal information and to
interact with human users. Despite the progress made, the challenge of
detecting high-risk interactions in multimodal settings, and in particular in
speech modality, remains largely unexplored. Conventional research on risk for
speech modality primarily emphasises the content (e.g., what is captured as
transcription). However, in speech-based interactions, paralinguistic cues in
audio can significantly alter the intended meaning behind utterances. In this
work, we propose a speech-specific risk taxonomy, covering 8 risk categories
under hostility (malicious sarcasm and threats), malicious imitation (age,
gender, ethnicity), and stereotypical biases (age, gender, ethnicity). Based on
the taxonomy, we create a small-scale dataset for evaluating current LMMs
capability in detecting these categories of risk. We observe even the latest
models remain ineffective to detect various paralinguistic-specific risks in
speech (e.g., Gemini 1.5 Pro is performing only slightly above random
baseline). Warning: this paper contains biased and offensive examples.

摘要：大型多模态模型 (LMM) 近来取得巨大成功，
展示出理解多模态信息和与人类用户交互的强大能力。尽管取得了进展，
在多模态设置中检测高风险交互的挑战，尤其是在
语音模态中，在很大程度上仍未得到探索。针对
语音模态的风险的传统研究主要强调内容（例如，捕获为
转录的内容）。然而，在基于语音的交互中，
音频中的副语言线索可以显著改变话语背后的预期含义。在这
项工作中，我们提出了一个特定于语音的风险分类法，涵盖了敌意（恶意讽刺和威胁）、恶意模仿（年龄、
性别、种族）和刻板偏见（年龄、性别、种族）下的 8 个风险类别。基于
分类法，我们创建了一个小规模数据集，用于评估当前 LMM
检测这些风险类别的能力。我们观察到，即使是最新的
模型在检测语音中的各种特定于副语言的风险方面仍然无效
（例如，Gemini 1.5 Pro 的表现仅略高于随机
基线）。警告：本文包含有偏见和冒犯性的示例。

##### **CuDA2: An approach for Incorporating Traitor Agents into Cooperative Multi-Agent Systems**
2406.17425v1 by Zhen Chen, Yong Liao, Youpeng Zhao, Zipeng Dai, Jian Zhao

Cooperative Multi-Agent Reinforcement Learning (CMARL) strategies are well
known to be vulnerable to adversarial perturbations. Previous works on
adversarial attacks have primarily focused on white-box attacks that directly
perturb the states or actions of victim agents, often in scenarios with a
limited number of attacks. However, gaining complete access to victim agents in
real-world environments is exceedingly difficult. To create more realistic
adversarial attacks, we introduce a novel method that involves injecting
traitor agents into the CMARL system. We model this problem as a Traitor Markov
Decision Process (TMDP), where traitors cannot directly attack the victim
agents but can influence their formation or positioning through collisions. In
TMDP, traitors are trained using the same MARL algorithm as the victim agents,
with their reward function set as the negative of the victim agents' reward.
Despite this, the training efficiency for traitors remains low because it is
challenging for them to directly associate their actions with the victim
agents' rewards. To address this issue, we propose the Curiosity-Driven
Adversarial Attack (CuDA2) framework. CuDA2 enhances the efficiency and
aggressiveness of attacks on the specified victim agents' policies while
maintaining the optimal policy invariance of the traitors. Specifically, we
employ a pre-trained Random Network Distillation (RND) module, where the extra
reward generated by the RND module encourages traitors to explore states
unencountered by the victim agents. Extensive experiments on various scenarios
from SMAC demonstrate that our CuDA2 framework offers comparable or superior
adversarial attack capabilities compared to other baselines.

摘要：合作多智能體強化學習 (CMARL) 策略已廣為人知，容易受到對抗性擾動的影響。先前對抗性攻擊的研究主要集中在直接擾動受害者智能體的狀態或動作的白盒攻擊，通常在攻擊次數有限的場景中。然而，在現實環境中完全存取受害者智能體極為困難。為了創造更逼真的對抗性攻擊，我們引入了一種新方法，包括將叛徒智能體注入 CMARL 系統。我們將此問題建模為叛徒馬可夫決策過程 (TMDP)，其中叛徒無法直接攻擊受害者智能體，但可以透過碰撞影響其形成或定位。在 TMDP 中，叛徒使用與受害者智能體相同的 MARL 演算法進行訓練，其獎勵函數設定為受害者智能體獎勵的負值。儘管如此，叛徒的訓練效率仍然很低，因為他們難以將自己的動作直接與受害者智能體的獎勵聯繫起來。為了解決這個問題，我們提出了好奇心驅動對抗性攻擊 (CuDA2) 框架。CuDA2 提高了對指定受害者智能體策略的攻擊效率和侵略性，同時維持叛徒的最佳策略不變性。具體來說，我們採用預先訓練的隨機網路蒸餾 (RND) 模組，其中 RND 模組產生的額外獎勵鼓勵叛徒探索受害者智能體未遭遇的狀態。在 SMAC 的各種場景中進行的廣泛實驗表明，與其他基準相比，我們的 CuDA2 框架提供了相當或更優越的對抗性攻擊能力。

##### **Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA**
2406.17419v1 by Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, Yongbin Li

Long-context modeling capabilities have garnered widespread attention,
leading to the emergence of Large Language Models (LLMs) with ultra-context
windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually
catching up. However, existing benchmarks employ irrelevant noise texts to
artificially extend the length of test cases, diverging from the real-world
scenarios of long-context applications. To bridge this gap, we propose a novel
long-context benchmark, Loong, aligning with realistic scenarios through
extended multi-document question answering (QA). Unlike typical document QA, in
Loong's test cases, each document is relevant to the final answer, ignoring any
document will lead to the failure of the answer. Furthermore, Loong introduces
four types of tasks with a range of context lengths: Spotlight Locating,
Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic
and comprehensive evaluation of long-context understanding. Extensive
experiments indicate that existing long-context language models still exhibit
considerable potential for enhancement. Retrieval augmented generation (RAG)
achieves poor performance, demonstrating that Loong can reliably assess the
model's long-context modeling capabilities.

摘要：長語境建模能力備受關注，促使具備超長語境視窗的大型語言模型 (LLM) 應運而生。與此同時，用於評估長語境 LLM 的基準也逐漸跟上腳步。然而，現有的基準採用無關的雜訊文字來人工延長測試案例的長度，偏離了長語境應用程式的真實場景。為了彌合這一差距，我們提出了一個新穎的長語境基準 Loong，透過擴展的多文件問答 (QA) 與現實場景保持一致。與典型的文件問答不同，在 Loong 的測試案例中，每個文件都與最終答案相關，忽略任何文件都會導致答案失敗。此外，Loong 導入了四種類型的任務，涵蓋範圍廣泛的語境長度：焦點定位、比較、分群和推理鏈，以利於對長語境理解進行更現實且全面的評估。大量的實驗表明，現有的長語境語言模型仍有很大的增強潛力。檢索增強生成 (RAG) 的表現不佳，證明 Loong 能夠可靠地評估模型的長語境建模能力。

##### **Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels**
2406.17415v2 by Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu

We present a simple variable quantization approach that quantizes different
layers of a large language model (LLM) at different bit levels. Specifically,
we quantize the most important layers to higher bit precision and less
important layers to lower bits to achieve floating point quantization levels.
We propose two effective strategies to measure the importance of layers within
LLMs: the first measures the importance of a layer based on how different its
output embeddings are from the input embeddings (the higher the better); the
second estimates the importance of a layer using the number of layer weights
that are much larger than average (the smaller the better). We show that
quantizing different layers at varying bits according to our importance scores
results in minimal performance drop with a far more compressed model size.
Finally, we present several practical key takeaways from our variable
layer-wise quantization experiments: (a) LLM performance under variable
quantization remains close to the original model until 25-50% of layers are
moved in lower quantization using our proposed ordering but only until 5-10% if
moved using no specific ordering; (b) Quantizing LLMs to lower bits performs
substantially better than pruning unless extreme quantization (2-bit) is used;
and (c) Layer-wise quantization to lower bits works better in the case of
larger LLMs with more layers compared to smaller LLMs with fewer layers. The
code used to run the experiments is available at:
https://github.com/RazvanDu/LayerwiseQuant.

摘要：<paragraph>我們提出一個簡單的變數量化方法，以不同的位元級別量化大型語言模型 (LLM) 的不同層。具體來說，我們將最重要的層量化為更高的位元精度，而將較不重要的層量化為較低的位元，以實現浮點量化級別。我們提出了兩種有效的策略來衡量 LLM 內層的重要性：第一個策略根據層的輸出嵌入與輸入嵌入的差異來衡量層的重要性（差異越大越好）；第二個策略使用遠大於平均值的層權重的數量來估計層的重要性（越小越好）。我們表明，根據我們的重要性分數，以不同的位元量化不同的層，會導致效能下降極小，但模型大小卻大幅壓縮。最後，我們從可變層級量化實驗中提出幾個實用的關鍵結論：(a) 在可變量化下，LLM 效能接近原始模型，直到 25-50% 的層使用我們建議的排序移至較低量化，但如果使用沒有特定排序，則只會接近 5-10%；(b) 將 LLM 量化為較低的位元比修剪執行得更好，除非使用極端量化 (2 位元)；(c) 與具有較少層的小型 LLM 相比，層級量化到較低位元在具有較多層的大型 LLM 中效果更好。用於執行實驗的程式碼可在以下位置取得：https://github.com/RazvanDu/LayerwiseQuant。</paragraph>

##### **Make Some Noise: Unlocking Language Model Parallel Inference Capability through Noisy Training**
2406.17404v1 by Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che

Existing speculative decoding methods typically require additional model
structure and training processes to assist the model for draft token
generation. This makes the migration of acceleration methods to the new model
more costly and more demanding on device memory. To address this problem, we
propose the Make Some Noise (MSN) training framework as a replacement for the
supervised fine-tuning stage of the large language model. The training method
simply introduces some noise at the input for the model to learn the denoising
task. It significantly enhances the parallel decoding capability of the model
without affecting the original task capability. In addition, we propose a
tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further
improve the inference speed of MSN models. Experiments in both the general and
code domains have shown that MSN can improve inference speed by 2.3-2.7x times
without compromising model performance. The MSN model also achieves comparable
acceleration ratios to the SOTA model with additional model structure on
Spec-Bench.

摘要：現有的推測性解碼方法通常需要額外的模型結構和訓練流程，以協助模型進行草稿令牌生成。這使得加速方法遷移到新模型的成本更高，並且對裝置記憶體的要求也更高。為了解決這個問題，我們提出將 Make Some Noise (MSN) 訓練架構作為大型語言模型監督微調階段的替代方案。訓練方法僅在輸入端引入一些雜訊，讓模型學習去雜訊任務。它顯著增強了模型的並行解碼能力，而不會影響原始任務能力。此外，我們提出一個基於樹的檢索增強 Jacobi (TR-Jacobi) 解碼策略，以進一步提高 MSN 模型的推理速度。一般和程式碼領域的實驗表明，MSN 可以將推理速度提高 2.3-2.7 倍，而不會損害模型效能。MSN 模型在 Spec-Bench 上也達到了與 SOTA 模型相當的加速比，並具備額外的模型結構。

##### **Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance**
2406.17385v1 by Manon Reusens, Philipp Borchert, Jochen De Weerdt, Bart Baesens

Large Language Models (LLMs) excel at providing information acquired during
pretraining on large-scale corpora and following instructions through user
prompts. This study investigates whether the quality of LLM responses varies
depending on the demographic profile of users. Considering English as the
global lingua franca, along with the diversity of its dialects among speakers
of different native languages, we explore whether non-native English speakers
receive lower-quality or even factually incorrect responses from LLMs more
frequently. Our results show that performance discrepancies occur when LLMs are
prompted by native versus non-native English speakers and persist when
comparing native speakers from Western countries with others. Additionally, we
find a strong anchoring effect when the model recognizes or is made aware of
the user's nativeness, which further degrades the response quality when
interacting with non-native speakers. Our analysis is based on a newly
collected dataset with over 12,000 unique annotations from 124 annotators,
including information on their native language and English proficiency.

摘要：大型語言模型（LLM）擅長提供在大型語料庫上預訓練期間獲得的資訊，並遵循使用者提示的指令。本研究調查 LLM 回應的品質是否會因使用者的個人資料而異。考量到英語作為全球通用語，以及不同母語使用者之間方言的多樣性，我們探討非英語母語使用者是否更常從 LLM 收到品質較差甚至事實錯誤的回應。我們的結果顯示，當 LLM 由英語母語和非母語使用者提示時，效能差異就會發生，且在將西方國家的母語使用者與其他國家進行比較時，這種差異仍然存在。此外，我們發現當模型辨識或得知使用者的母語時，會產生強烈的錨定效應，這會進一步降低與非母語使用者互動時的回應品質。我們的分析基於一個新收集的資料集，其中包含來自 124 位註解者的 12,000 多個獨特註解，包括他們母語和英語能力的資訊。

##### **A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens**
2406.17378v1 by Zhijie Nie, Richong Zhang, Zhanyu Wu

Text embeddings from large language models (LLMs) have achieved excellent
results in tasks such as information retrieval, semantic textual similarity,
etc. In this work, we show an interesting finding: when feeding a text into the
embedding LLMs, the obtained text embedding will be able to be aligned with the
key tokens in the input text. We first fully analyze this phenomenon on eight
embedding LLMs and show that this phenomenon is universal and is not affected
by model architecture, training strategy, and embedding method. With a deeper
analysis, we then find that the main change in embedding space between the
embedding LLMs and their original generative LLMs is in the first principal
component. By adjusting the first principal component, we can align text
embedding with the key tokens. Finally, we give several examples to demonstrate
the vast application potential of this finding: (1) we propose a simple and
practical sparse retrieval method based on the aligned tokens, which can
achieve 80\% of the dense retrieval effect of the same model while reducing the
computation significantly; (2) we show that our findings provide a fresh
perspective to help understand fuzzy concepts (e.g., semantic relatedness vs.
semantic similarity) and emerging technologies (e.g., instruction-following
embedding) in this field.

摘要：大型語言模型 (LLM) 的文字嵌入在資訊檢索、語義文字相似度等任務中獲得了極佳的結果。在本文中，我們展示了一個有趣的發現：當將文字輸入嵌入式 LLM 時，獲得的文字嵌入將能夠與輸入文字中的關鍵標記對齊。我們首先在八個嵌入式 LLM 上全面分析了此現象，並表明此現象是普遍的，不受模型架構、訓練策略和嵌入方法的影響。透過更深入的分析，我們發現嵌入式 LLM 與其原始生成式 LLM 之間嵌入空間的主要變化在於第一主成分。透過調整第一主成分，我們可以將文字嵌入與關鍵標記對齊。最後，我們舉了幾個例子來說明此發現的廣泛應用潛力：(1) 我們提出一個基於對齊標記的簡單且實用的稀疏檢索方法，它可以在顯著降低運算量的同時，達到相同模型 80% 的稠密檢索效果；(2) 我們展示我們的發現提供了一個新的觀點，有助於理解此領域中的模糊概念（例如，語義相關性與語義相似性）和新興技術（例如，遵循指令的嵌入）。

##### **A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual LLMs**
2406.17377v1 by Vaibhav Singh, Amrith Krishna, Karthika NJ, Ganesh Ramakrishnan

Low-resource languages, by its very definition, tend to be under represented
in the pre-training corpora of Large Language Models. In this work, we
investigate three low-resource cross-lingual approaches that enable an LLM
adapt to tasks in previously unseen languages. Llama-2 is an LLM where Indic
languages, among many other language families, contribute to less than
$0.005\%$ of the total $2$ trillion token pre-training corpora. In this work,
we experiment with the English-dominated Llama-2 for cross-lingual transfer to
three Indic languages, Bengali, Hindi, and Tamil as target languages. We study
three approaches for cross-lingual transfer, under ICL and fine-tuning. One, we
find that adding additional supervisory signals via a dominant language in the
LLM, leads to improvements, both under in-context learning and fine-tuning.
Two, adapting the target languages to word reordering may be beneficial under
ICL, but its impact diminishes with fine tuning. Finally, continued
pre-training in one low-resource language can improve model performance for
other related low-resource languages.

摘要：低資源語言，顧名思義，在大型語言模型的預訓練語料庫中往往代表性不足。在這項工作中，我們探討了三種低資源跨語言方法，這些方法使 LLM 能夠適應以前未見語言中的任務。Llama-2 是一種 LLM，其中印度語言（以及許多其他語言系列）在總計 2 兆個 token 的預訓練語料庫中所佔比例不到 0.005%。在這項工作中，我們嘗試使用以英語為主的 Llama-2 進行跨語言轉移，目標語言為三種印度語言：孟加拉語、印地語和泰米爾語。我們研究了跨語言轉移的三種方法，包括 ICL 和微調。一、我們發現通過 LLM 中的主導語言添加額外的監督信號，無論是在情境學習還是微調下，都能帶來改進。二、在 ICL 下，將目標語言適應詞序調整可能是有益的，但其影響會隨著微調而減弱。最後，在一個低資源語言中持續預訓練可以提高模型對其他相關低資源語言的性能。

##### **Temporal-Channel Modeling in Multi-head Self-Attention for Synthetic Speech Detection**
2406.17376v1 by Duc-Tuan Truong, Ruijie Tao, Tuan Nguyen, Hieu-Thi Luong, Kong Aik Lee, Eng Siong Chng

Recent synthetic speech detectors leveraging the Transformer model have
superior performance compared to the convolutional neural network counterparts.
This improvement could be due to the powerful modeling ability of the
multi-head self-attention (MHSA) in the Transformer model, which learns the
temporal relationship of each input token. However, artifacts of synthetic
speech can be located in specific regions of both frequency channels and
temporal segments, while MHSA neglects this temporal-channel dependency of the
input sequence. In this work, we proposed a Temporal-Channel Modeling (TCM)
module to enhance MHSA's capability for capturing temporal-channel
dependencies. Experimental results on the ASVspoof 2021 show that with only
0.03M additional parameters, the TCM module can outperform the state-of-the-art
system by 9.25% in EER. Further ablation study reveals that utilizing both
temporal and channel information yields the most improvement for detecting
synthetic speech.

摘要：最近利用 Transformer 模型的合成語音偵測器與卷積神經網路對應項相比具有優異的效能。這個進步可能是由於 Transformer 模型中多頭自我注意 (MHSA) 的強大建模能力，它學習每個輸入標記的時間關係。然而，合成語音的偽影可能位於頻率通道和時間區段的特定區域，而 MHSA 忽略了輸入序列的這種時間通道依賴性。在這項工作中，我們提出了一個時間通道建模 (TCM) 模組，以增強 MHSA 擷取時間通道依賴性的能力。ASVspoof 2021 的實驗結果顯示，僅使用 0.03M 的額外參數，TCM 模組就能在 EER 中以 9.25% 的表現優於最先進的系統。進一步的消融研究表明，同時利用時間和通道資訊可以為合成語音偵測帶來最大的進步。

##### **An Empirical Study on the Characteristics of Bias upon Context Length Variation for Bangla**
2406.17375v1 by Jayanta Sadhu, Ayan Antik Khan, Abhik Bhattacharjee, Rifat Shahriyar

Pretrained language models inherently exhibit various social biases,
prompting a crucial examination of their social impact across various
linguistic contexts due to their widespread usage. Previous studies have
provided numerous methods for intrinsic bias measurements, predominantly
focused on high-resource languages. In this work, we aim to extend these
investigations to Bangla, a low-resource language. Specifically, in this study,
we (1) create a dataset for intrinsic gender bias measurement in Bangla, (2)
discuss necessary adaptations to apply existing bias measurement methods for
Bangla, and (3) examine the impact of context length variation on bias
measurement, a factor that has been overlooked in previous studies. Through our
experiments, we demonstrate a clear dependency of bias metrics on context
length, highlighting the need for nuanced considerations in Bangla bias
analysis. We consider our work as a stepping stone for bias measurement in the
Bangla Language and make all of our resources publicly available to support
future research.

摘要：預訓練語言模型本質上展現出各種社會偏見，
促使對其在各種語言環境中的社會影響進行關鍵審查，因為它們被廣泛使用。先前的研究
提供了許多內在偏見測量方法，主要集中在高資源語言上。在這項工作中，我們旨在將這些
調查擴展到孟加拉語，一種低資源語言。具體來說，在這項研究中，我們 (1) 為孟加拉語中的內在性別偏見測量建立一個數據集，(2)
討論為孟加拉語應用現有的偏見測量方法所需的適應，以及 (3) 檢查上下文長度變化對偏見
測量的影響，這是一個先前研究中被忽視的因素。透過我們的
實驗，我們展示了偏見指標對上下文長度的明顯依賴性，強調了在孟加拉語偏見
分析中需要細緻的考量。我們將我們的研究視為孟加拉語中偏見測量的墊腳石，並公開我們所有的資源以支持
未來的研究。

##### **Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech Translation**
2406.17363v1 by Yasmin Moslem

This paper describes our system submission to the International Conference on
Spoken Language Translation (IWSLT 2024) for Irish-to-English speech
translation. We built end-to-end systems based on Whisper, and employed a
number of data augmentation techniques, such as speech back-translation and
noise augmentation. We investigate the effect of using synthetic audio data and
discuss several methods for enriching signal diversity.

摘要：本文描述了我們在國際語音語言翻譯會議 (IWSLT 2024) 的愛爾蘭語到英語語音翻譯系統提交。我們建立了基於 Whisper 的端到端系統，並採用了多種數據擴充技術，例如語音反向翻譯和噪聲擴充。我們探討了使用合成音頻數據的影響，並討論了多種豐富信號多樣性的方法。

##### **Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers**
2406.17343v1 by Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, Wenwu Zhu

Recent advancements in diffusion models, particularly the trend of
architectural transformation from UNet-based Diffusion to Diffusion Transformer
(DiT), have significantly improved the quality and scalability of image
synthesis. Despite the incredible generative quality, the large computational
requirements of these large-scale models significantly hinder the deployments
in real-world scenarios. Post-training Quantization (PTQ) offers a promising
solution by compressing model sizes and speeding up inference for the
pretrained models while eliminating model retraining. However, we have observed
the existing PTQ frameworks exclusively designed for both ViT and conventional
Diffusion models fall into biased quantization and result in remarkable
performance degradation. In this paper, we find that the DiTs typically exhibit
considerable variance in terms of both weight and activation, which easily runs
out of the limited numerical representations. To address this issue, we devise
Q-DiT, which seamlessly integrates three techniques: fine-grained quantization
to manage substantial variance across input channels of weights and
activations, an automatic search strategy to optimize the quantization
granularity and mitigate redundancies, and dynamic activation quantization to
capture the activation changes across timesteps. Extensive experiments on the
ImageNet dataset demonstrate the effectiveness of the proposed Q-DiT.
Specifically, when quantizing DiT-XL/2 to W8A8 on ImageNet 256x256, Q-DiT
achieves a remarkable reduction in FID by 1.26 compared to the baseline. Under
a W4A8 setting, it maintains high fidelity in image generation, showcasing only
a marginal increase in FID and setting a new benchmark for efficient,
high-quality quantization in diffusion transformers. Code is available at
\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}.

摘要：<paragraph>擴散模型的最新進展，特別是從基於 UNet 的擴散到擴散變換器 (DiT) 的架構轉換趨勢，大幅提升了影像合成的品質和可擴充性。儘管生成品質令人驚嘆，但這些大型模型龐大的運算需求，大幅阻礙了在實際場景中的部署。訓練後量化 (PTQ) 提供了一個有前景的解決方案，透過壓縮模型大小並加速預訓練模型的推論，同時消除模型重新訓練。然而，我們觀察到現有的 PTQ 框架專門設計給 ViT 和傳統擴散模型，會陷入有偏差的量化，並導致顯著的效能下降。在本文中，我們發現 DiT 通常在權重和激活方面表現出相當大的差異，這很容易超出有限的數值表示範圍。為了解決這個問題，我們設計了 Q-DiT，它無縫整合了三種技術：細粒度量化以管理權重和激活輸入通道之間的實質差異、自動搜尋策略以最佳化量化粒度並減輕冗餘，以及動態激活量化以捕捉時間步長之間的激活變化。在 ImageNet 資料集上的廣泛實驗證明了所提出的 Q-DiT 的有效性。具體來說，當將 DiT-XL/2 量化為 ImageNet 256x256 上的 W8A8 時，與基線相比，Q-DiT 在 FID 上實現了顯著的 1.26 減少。在 W4A8 設定下，它在影像生成中維持高保真度，僅展示 FID 的邊際增加，並為擴散變換器中的高效、高品質量化設定了新的基準。程式碼可在\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}取得。</paragraph>

##### **Masked Generative Extractor for Synergistic Representation and 3D Generation of Point Clouds**
2406.17342v1 by Hongliang Zeng, Ping Zhang, Fang Li, Jiahua Wang, Tingyu Ye, Pengteng Guo

In the field of 2D image generation modeling and representation learning,
Masked Generative Encoder (MAGE) has demonstrated the synergistic potential
between generative modeling and representation learning. Inspired by this, we
propose Point-MAGE to extend this concept to point cloud data. Specifically,
this framework first utilizes a Vector Quantized Variational Autoencoder
(VQVAE) to reconstruct a neural field representation of 3D shapes, thereby
learning discrete semantic features of point patches. Subsequently, by
combining the masking model with variable masking ratios, we achieve
synchronous training for both generation and representation learning.
Furthermore, our framework seamlessly integrates with existing point cloud
self-supervised learning (SSL) models, thereby enhancing their performance. We
extensively evaluate the representation learning and generation capabilities of
Point-MAGE. In shape classification tasks, Point-MAGE achieved an accuracy of
94.2% on the ModelNet40 dataset and 92.9% (+1.3%) on the ScanObjectNN dataset.
Additionally, it achieved new state-of-the-art performance in few-shot learning
and part segmentation tasks. Experimental results also confirmed that
Point-MAGE can generate detailed and high-quality 3D shapes in both
unconditional and conditional settings.

摘要：在 2D 影像生成建模和表徵學習領域中，
遮罩生成編碼器 (MAGE) 已證明生成建模和表徵學習之間的協同潛力。受此啟發，我們
提出點 MAGE 將此概念延伸至點雲資料。具體來說，
此框架首先利用向量量化變分自編碼器 (VQVAE) 來重建 3D 形狀的神經場表徵，從而
學習點狀圖案的離散語義特徵。隨後，通過將遮罩模型與可變遮罩比率結合，我們實現
同時訓練生成和表徵學習。此外，我們的框架與現有的點雲
自我監督學習 (SSL) 模型無縫整合，從而增強其效能。我們
廣泛評估了點 MAGE 的表徵學習和生成能力。在形狀分類任務中，點 MAGE 在 ModelNet40 資料集上達到了
94.2% 的準確度，在 ScanObjectNN 資料集上達到了 92.9%（+1.3%）。
此外，它在小樣本學習和部分分割任務中達到了新的最先進效能。實驗結果也證實了
點 MAGE 能在無條件和有條件設定下生成詳細且高品質的 3D 形狀。

##### **Joint Admission Control and Resource Allocation of Virtual Network Embedding via Hierarchical Deep Reinforcement Learning**
2406.17334v1 by Tianfu Wang, Li Shen, Qilin Fan, Tong Xu, Tongliang Liu, Hui Xiong

As an essential resource management problem in network virtualization,
virtual network embedding (VNE) aims to allocate the finite resources of
physical network to sequentially arriving virtual network requests (VNRs) with
different resource demands. Since this is an NP-hard combinatorial optimization
problem, many efforts have been made to provide viable solutions. However, most
existing approaches have either ignored the admission control of VNRs, which
has a potential impact on long-term performances, or not fully exploited the
temporal and topological features of the physical network and VNRs. In this
paper, we propose a deep Hierarchical Reinforcement Learning approach to learn
a joint Admission Control and Resource Allocation policy for VNE, named
HRL-ACRA. Specifically, the whole VNE process is decomposed into an upper-level
policy for deciding whether to admit the arriving VNR or not and a lower-level
policy for allocating resources of the physical network to meet the requirement
of VNR through the HRL approach. Considering the proximal policy optimization
as the basic training algorithm, we also adopt the average reward method to
address the infinite horizon problem of the upper-level agent and design a
customized multi-objective intrinsic reward to alleviate the sparse reward
issue of the lower-level agent. Moreover, we develop a deep feature-aware graph
neural network to capture the features of VNR and physical network and exploit
a sequence-to-sequence model to generate embedding actions iteratively.
Finally, extensive experiments are conducted in various settings, and show that
HRL-ACRA outperforms state-of-the-art baselines in terms of both the acceptance
ratio and long-term average revenue. Our code is available at
\url{https://github.com/GeminiLight/hrl-acra}.

摘要：<paragraph>作為網路虛擬化中一項重要的資源管理問題，
虛擬網路嵌入（VNE）旨在將實體網路的有限資源分配給依序抵達的虛擬網路請求（VNR），其資源需求各不相同。由於這是一個 NP 難組合最佳化問題，因此已投入許多心力來提供可行的解決方案。然而，大多數現有方法都忽略了 VNR 的接納控制，這可能會對長期效能造成影響，或者沒有充分利用實體網路和 VNR 的時間和拓撲特徵。在本文中，我們提出了一種深度階層式強化學習方法來學習 VNE 的聯合接納控制和資源分配政策，稱為 HRL-ACRA。具體來說，整個 VNE 程序被分解為一個上層政策，用於決定是否接納抵達的 VNR，以及一個下層政策，用於透過 HRL 方法分配實體網路的資源以滿足 VNR 的需求。考慮到近端政策最佳化作為基本的訓練演算法，我們也採用平均獎勵法來解決上層代理的無限時間軸問題，並設計一個自訂的多目標內在獎勵來緩解下層代理的稀疏獎勵問題。此外，我們開發了一個深度特徵感知圖神經網路來擷取 VNR 和實體網路的特徵，並利用序列到序列模型來反覆產生嵌入動作。最後，在各種設定中進行了廣泛的實驗，結果顯示 HRL-ACRA 在接受率和長期平均收益方面都優於最先進的基準。我們的程式碼可在
\url{https://github.com/GeminiLight/hrl-acra} 取得。</paragraph>

##### **Dual-Space Knowledge Distillation for Large Language Models**
2406.17328v1 by Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, Jinan Xu

Knowledge distillation (KD) is known as a promising solution to compress
large language models (LLMs) via transferring their knowledge to smaller
models. During this process, white-box KD methods usually minimize the distance
between the output distributions of the two models so that more knowledge can
be transferred. However, in the current white-box KD framework, the output
distributions are from the respective output spaces of the two models, using
their own prediction heads. We argue that the space discrepancy will lead to
low similarity between the teacher model and the student model on both
representation and distribution levels. Furthermore, this discrepancy also
hinders the KD process between models with different vocabularies, which is
common for current LLMs. To address these issues, we propose a dual-space
knowledge distillation (DSKD) framework that unifies the output spaces of the
two models for KD. On the basis of DSKD, we further develop a cross-model
attention mechanism, which can automatically align the representations of the
two models with different vocabularies. Thus, our framework is not only
compatible with various distance functions for KD (e.g., KL divergence) like
the current framework, but also supports KD between any two LLMs regardless of
their vocabularies. Experiments on task-agnostic instruction-following
benchmarks show that DSKD significantly outperforms the current white-box KD
framework with various distance functions, and also surpasses existing KD
methods for LLMs with different vocabularies.

摘要：知识蒸馏 (KD) 是一种有前途的解决方案，可通过将大语言模型 (LLM) 的知识转移到较小的模型中来压缩大语言模型 (LLM)。在此过程中，白盒 KD 方法通常会最小化两个模型输出分布之间的距离，以便可以转移更多知识。然而，在当前的白盒 KD 框架中，输出分布来自两个模型各自的输出空间，并使用它们自己的预测头。我们认为，这种空间差异会导致教师模型和学生模型在表示和分布层面上相似度较低。此外，这种差异还阻碍了具有不同词汇表的模型之间的 KD 过程，这对于当前的 LLM 来说很常见。为了解决这些问题，我们提出了一个双空间知识蒸馏 (DSKD) 框架，该框架统一了两个模型的输出空间以进行 KD。在 DSKD 的基础上，我们进一步开发了一种跨模型注意机制，它可以自动对齐具有不同词汇表的两个模型的表示。因此，我们的框架不仅与 KD 的各种距离函数（例如 KL 散度）兼容（如当前框架），而且还支持在任何两个 LLM 之间进行 KD，而不管它们的词汇表如何。在与任务无关的指令遵循基准上的实验表明，DSKD 在各种距离函数下明显优于当前的白盒 KD 框架，并且还超越了现有的针对具有不同词汇表的 LLM 的 KD 方法。

##### **Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy**
2406.17324v1 by Simone Astarita, Sandor Kruk, Jan Reerink, Pablo Gómez

Rapid progress in the capabilities of machine learning approaches in natural
language processing has culminated in the rise of large language models over
the last two years. Recent works have shown unprecedented adoption of these for
academic writing, especially in some fields, but their pervasiveness in
astronomy has not been studied sufficiently. To remedy this, we extract words
that ChatGPT uses more often than humans when generating academic text and
search a total of 1 million articles for them. This way, we assess the
frequency of word occurrence in published works in astronomy tracked by the
NASA Astrophysics Data System since 2000. We then perform a statistical
analysis of the occurrences. We identify a list of words favoured by ChatGPT
and find a statistically significant increase for these words against a control
group in 2024, which matches the trend in other disciplines. These results
suggest a widespread adoption of these models in the writing of astronomy
papers. We encourage organisations, publishers, and researchers to work
together to identify ethical and pragmatic guidelines to maximise the benefits
of these systems while maintaining scientific rigour.

摘要：機器學習方法在自然語言處理能力方面的快速進展，在過去兩年中達到了大型語言模型的興起。最近的研究表明，這些模型在學術寫作中得到了前所未有的採用，特別是在某些領域，但它們在天文方面的普及程度尚未得到充分的研究。為了彌補這一點，我們提取了 ChatGPT 在生成學術文本時比人類更常使用的詞彙，並在總計 100 萬篇文章中搜尋這些詞彙。通過這種方式，我們評估了自 2000 年以來 NASA 天體物理數據系統追蹤的天文學已發表作品中詞彙出現的頻率。然後我們對出現的頻率進行統計分析。我們找出 ChatGPT 偏好的詞彙清單，並發現這些詞彙在 2024 年相對於對照組有統計顯著的增加，這與其他學科的趨勢相符。這些結果表明這些模型在撰寫天文學論文方面得到了廣泛採用。我們鼓勵組織、出版商和研究人員共同努力，找出合乎道德和實用的準則，以最大化這些系統的益處，同時保持科學的嚴謹性。

##### **Retrieval Augmented Instruction Tuning for Open NER with Large Language Models**
2406.17305v1 by Tingyu Xie, Jian Zhang, Yan Zhang, Yuanyuan Liang, Qi Li, Hongwei Wang

The strong capability of large language models (LLMs) has been applied to
information extraction (IE) through either retrieval augmented prompting or
instruction tuning (IT). However, the best way to incorporate information with
LLMs for IE remains an open question. In this paper, we explore Retrieval
Augmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named
entity recognition (NER). Specifically, for each training sample, we retrieve
semantically similar examples from the training dataset as the context and
prepend them to the input of the original instruction. To evaluate our RA-IT
approach more thoroughly, we construct a Chinese IT dataset for open NER and
evaluate RA-IT in both English and Chinese scenarios. Experimental results
verify the effectiveness of RA-IT across various data sizes and in both English
and Chinese scenarios. We also conduct thorough studies to explore the impacts
of various retrieval strategies in the proposed RA-IT framework. Code and data
are available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER

摘要：大型語言模型 (LLM) 的強大功能已透過檢索擴充提示或指令微調 (IT) 應用於資訊萃取 (IE)。然而，將資訊與 LLM 整合以進行 IE 的最佳方式仍是一個開放性的問題。在本文中，我們探討了用於 IE 的檢索擴充指令微調 (RA-IT)，重點放在開放式命名實體辨識 (NER) 的任務上。具體來說，對於每個訓練範例，我們從訓練資料集中擷取語義上相似的範例作為背景，並將它們加到原始指令的輸入之前。為了更徹底地評估我們的 RA-IT 方法，我們建構了一個用於開放式 NER 的中文 IT 資料集，並在英文和中文場景中評估 RA-IT。實驗結果驗證了 RA-IT 在各種資料大小以及英文和中文場景中的有效性。我們也進行了徹底的研究，以探討在提議的 RA-IT 架構中各種檢索策略的影響。程式碼和資料可在 https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER 取得

##### **Leveraging LLMs for Dialogue Quality Measurement**
2406.17304v1 by Jinghan Jia, Abi Komma, Timothy Leffel, Xujun Peng, Ajay Nagesh, Tamer Soliman, Aram Galstyan, Anoop Kumar

In task-oriented conversational AI evaluation, unsupervised methods poorly
correlate with human judgments, and supervised approaches lack generalization.
Recent advances in large language models (LLMs) show robust zeroshot and
few-shot capabilities across NLP tasks. This paper explores using LLMs for
automated dialogue quality evaluation, experimenting with various
configurations on public and proprietary datasets. Manipulating factors such as
model size, in-context examples, and selection techniques, we examine
"chain-of-thought" (CoT) reasoning and label extraction procedures. Our results
show that (1) larger models yield more accurate dialogue labels; (2)
algorithmic selection of in-context examples outperforms random selection; (3)
CoT reasoning where an LLM is asked to provide justifications before outputting
final labels improves performance; and (4) fine-tuned LLMs outperform
out-of-the-box ones. Our results indicate that LLMs that are suitably
fine-tuned and have sufficient reasoning capabilities can be leveraged for
automated dialogue evaluation.

摘要：在面向任務的對話式 AI 評估中，非監督式方法與人類判斷關聯性低，而監督式方法缺乏概括性。大型語言模型 (LLM) 的最新進展在各種 NLP 任務中展現出強大的零次學習和少次學習能力。本文探討使用 LLM 進行自動對話品質評估，並在公開和專有資料集上嘗試各種設定。我們透過調整模型大小、脈絡範例和選擇技術等因素，檢視「思維鏈」(CoT) 推理和標籤萃取程序。我們的結果顯示：(1) 較大的模型產生更準確的對話標籤；(2) 脈絡範例的演算法選擇優於隨機選擇；(3) CoT 推理，其中要求 LLM 在輸出最終標籤前提供依據，可改善效能；(4) 微調後的 LLM 優於開箱即用的 LLM。我們的結果表明，適當地微調且具備足夠推理能力的 LLM 可用於自動對話評估。

##### **Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models**
2406.17294v2 by Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee

Large language models (LLMs) have demonstrated impressive reasoning
capabilities, particularly in textual mathematical problem-solving. However,
existing open-source image instruction fine-tuning datasets, containing limited
question-answer pairs per image, do not fully exploit visual information to
enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs
(MLLMs). To bridge this gap, we address the lack of high-quality, diverse
multimodal mathematical datasets by collecting 40K high-quality images with
question-answer pairs from 24 existing datasets and synthesizing 320K new
pairs, creating the MathV360K dataset, which enhances both the breadth and
depth of multimodal mathematical questions. We introduce Math-LLaVA, a
LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach
significantly improves the multimodal mathematical reasoning capabilities of
LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V
on MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced
generalizability, showing substantial improvements on the MMMU benchmark. Our
research highlights the importance of dataset diversity and synthesis in
advancing MLLMs' mathematical reasoning abilities. The code and data are
available at: \url{https://github.com/HZQ950419/Math-LLaVA}.

摘要：大型語言模型 (LLM) 已展現出令人印象深刻的推理能力，尤其在文字數學問題求解方面。然而，現有的開放原始碼影像指令微調資料集包含每個影像有限的問題解答配對，並未充分利用視覺資訊來增強多模態 LLM (MLLM) 的多模態數學推理能力。為了彌補這個差距，我們透過從 24 個現有資料集中收集 40K 個附有問題解答配對的高品質影像，並合成 320K 個新配對，解決了缺乏高品質、多樣化的多模態數學資料集的問題，進而建立了 MathV360K 資料集，該資料集擴展了多模態數學問題的廣度和深度。我們引入了 Math-LLaVA，這是一個以 MathV360K 微調的 LLaVA-1.5 基礎模型。這種新方法大幅改善了 LLaVA-1.5 的多模態數學推理能力，在 MathVista 的迷你測試分割中獲得了 19 分的提升，並達到與 GPT-4V 相當的效能。此外，Math-LLaVA 展現出增強的泛化能力，在 MMMU 評量基準上獲得了顯著的改善。我們的研究突顯了資料集多樣性和合成在提升 MLLM 數學推理能力方面的重要性。程式碼和資料可於以下網址取得：\url{https://github.com/HZQ950419/Math-LLaVA}。

##### **Hyperbolic Knowledge Transfer in Cross-Domain Recommendation System**
2406.17289v1 by Xin Yang, Heng Chang, Zhijian La, Jinze Yang, Xingrun Li, Yu Lu, Shuaiqiang Wang, Dawei Yin, Erxue Min

Cross-Domain Recommendation (CDR) seeks to utilize knowledge from different
domains to alleviate the problem of data sparsity in the target recommendation
domain, and it has been gaining more attention in recent years. Although there
have been notable advancements in this area, most current methods represent
users and items in Euclidean space, which is not ideal for handling long-tail
distributed data in recommendation systems. Additionally, adding data from
other domains can worsen the long-tail characteristics of the entire dataset,
making it harder to train CDR models effectively. Recent studies have shown
that hyperbolic methods are particularly suitable for modeling long-tail
distributions, which has led us to explore hyperbolic representations for users
and items in CDR scenarios. However, due to the distinct characteristics of the
different domains, applying hyperbolic representation learning to CDR tasks is
quite challenging. In this paper, we introduce a new framework called
Hyperbolic Contrastive Learning (HCTS), designed to capture the unique features
of each domain while enabling efficient knowledge transfer between domains. We
achieve this by embedding users and items from each domain separately and
mapping them onto distinct hyperbolic manifolds with adjustable curvatures for
prediction. To improve the representations of users and items in the target
domain, we develop a hyperbolic contrastive learning module for knowledge
transfer. Extensive experiments on real-world datasets demonstrate that
hyperbolic manifolds are a promising alternative to Euclidean space for CDR
tasks.

摘要：跨域推薦（CDR）旨在利用不同領域的知識來緩解目標推薦領域中資料稀疏的問題，並且近年來備受關注。儘管在此領域已取得顯著進展，但大多數當前方法都在歐幾里得空間中表示使用者和項目，這並不適合處理推薦系統中的長尾分佈資料。此外，從其他領域新增資料可能會惡化整個資料集的長尾特性，這使得更難有效訓練 CDR 模型。最近的研究表明，雙曲方法特別適合於對長尾分佈進行建模，這促使我們探索 CDR 場景中使用者和項目的雙曲表示。然而，由於不同領域的獨特特性，將雙曲表示學習應用於 CDR 任務非常具有挑戰性。在本文中，我們介紹了一個稱為雙曲對比學習（HCTS）的新框架，旨在擷取每個領域的獨特特徵，同時實現領域之間的有效知識轉移。我們透過分別嵌入每個領域的使用者和項目，並將它們映射到具有可調整曲率的不同雙曲流形上以進行預測，來達成此目標。為了改善目標領域中使用者和項目的表示，我們開發了一個雙曲對比學習模組，用於知識轉移。在真實世界資料集上進行的大量實驗表明，雙曲流形是 CDR 任務中歐幾里得空間的有希望的替代方案。

##### **Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models**
2406.17287v1 by Yang Yan, Lizhi Ma, Anqi Li, Jingsong Ma, Zhenzhong Lan

Accurate assessment of personality traits is crucial for effective
psycho-counseling, yet traditional methods like self-report questionnaires are
time-consuming and biased. This study exams whether Large Language Models
(LLMs) can predict the Big Five personality traits directly from counseling
dialogues and introduces an innovative framework to perform the task. Our
framework applies role-play and questionnaire-based prompting to condition LLMs
on counseling sessions, simulating client responses to the Big Five Inventory.
We evaluated our framework on 853 real-world counseling sessions, finding a
significant correlation between LLM-predicted and actual Big Five traits,
proving the validity of framework. Moreover, ablation studies highlight the
importance of role-play simulations and task simplification via questionnaires
in enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model,
utilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves
a 130.95\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\%
in personality prediction validity. In conclusion, LLMs can predict personality
based on counseling dialogues. Our code and model are publicly available at
\url{https://github.com/kuri-leo/BigFive-LLM-Predictor}, providing a valuable
tool for future research in computational psychometrics.

摘要：準確評估人格特質對於有效的心理諮商至關重要，但傳統方法（例如自我報告問卷）既耗時又存在偏見。本研究探討大型語言模型 (LLM) 是否能直接從諮商對話中預測大五人格特質，並提出一個創新的架構來執行這項任務。我們的架構應用角色扮演和基於問卷的提示，在諮商會談中對 LLM 進行條件化，模擬客戶對大五人格量表的回應。我們對 853 個真實世界的諮商會談評估了我們的架構，發現 LLM 預測的大五特質與實際大五特質之間存在顯著相關性，證明了架構的有效性。此外，消融研究強調了角色扮演模擬和透過問卷簡化任務對於提高預測準確性的重要性。同時，我們微調後的 Llama3-8B 模型，利用帶有監督微調的直接偏好最佳化，在人格預測有效性上取得了 130.95% 的改進，比最先進的 Qwen1.5-110B 高出 36.94%。總之，LLM 可以根據諮商對話預測人格。我們的程式碼和模型已公開發布在 \url{https://github.com/kuri-leo/BigFive-LLM-Predictor}，為計算心理測量學的未來研究提供了有價值的工具。

##### **SetBERT: Enhancing Retrieval Performance for Boolean Logic and Set Operation Queries**
2406.17282v2 by Quan Mai, Susan Gauch, Douglas Adams

We introduce SetBERT, a fine-tuned BERT-based model designed to enhance query
embeddings for set operations and Boolean logic queries, such as Intersection
(AND), Difference (NOT), and Union (OR). SetBERT significantly improves
retrieval performance for logic-structured queries, an area where both
traditional and neural retrieval methods typically underperform. We propose an
innovative use of inversed-contrastive loss, focusing on identifying the
negative sentence, and fine-tuning BERT with a dataset generated via prompt
GPT. Furthermore, we demonstrate that, unlike other BERT-based models,
fine-tuning with triplet loss actually degrades performance for this specific
task. Our experiments reveal that SetBERT-base not only significantly
outperforms BERT-base (up to a 63% improvement in Recall) but also achieves
performance comparable to the much larger BERT-large model, despite being only
one-third the size.

摘要：我們引入了 SetBERT，一個經過微調的 BERT 模型，旨在增強集合運算和布林邏輯查詢（例如交集 (AND)、差集 (NOT) 和聯集 (OR)）的查詢嵌入。SetBERT 大幅提升了邏輯結構查詢的檢索效能，這是傳統和神經網路檢索方法通常表現不佳的領域。我們提出了一種創新的反向對比損失使用方式，專注於識別負向句子，並使用透過提示 GPT 生成的資料集微調 BERT。此外，我們證明了與其他 BERT 模型不同，使用三重損失進行微調實際上會降低此特定任務的效能。我們的實驗顯示，SetBERT-base 不僅顯著優於 BERT-base（召回率提升達 63%），而且儘管只有三分之一的大小，但效能可與大得多的 BERT-large 模型相媲美。

##### **OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure**
2406.17276v1 by Jikai Wang, Yi Su, Juntao Li, Qinrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang

Autoregressive language models demonstrate excellent performance in various
scenarios. However, the inference efficiency is limited by its
one-step-one-word generation mode, which has become a pressing problem recently
as the models become increasingly larger. Speculative decoding employs a "draft
and then verify" mechanism to allow multiple tokens to be generated in one
step, realizing lossless acceleration. Existing methods mainly adopt fixed
heuristic draft structures, which fail to adapt to different situations to
maximize the acceptance length during verification. To alleviate this dilemma,
we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft
trees. It searches the optimal tree structure that maximizes the mathematical
expectation of the acceptance length in each decoding step. Experimental
results reveal that OPT-Tree outperforms the existing draft structures and
achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.
If the draft model is powerful enough and the node budget is sufficient, it can
generate more than ten tokens in a single step. Our code is available at
https://github.com/Jikai0Wang/OPT-Tree.

摘要：自回归语言模型在各种场景中表现出优异的性能。然而，其推理效率受到其一步一词的生成模式的限制，随着模型变得越来越大，这已成为最近的一个紧迫问题。推测解码采用“先起草再验证”的机制，允许一步生成多个标记，实现无损加速。现有方法主要采用固定的启发式草稿结构，无法适应不同的情况以最大化验证期间的接受长度。为了缓解这一困境，我们提出了 OPT-Tree，一种构建自适应且可扩展的草稿树的算法。它搜索最优树结构，以最大化每个解码步骤中接受长度的数学期望。实验结果表明，OPT-Tree 优于现有的草稿结构，与自回归解码相比，实现了高达 3.2 的加速比。如果草稿模型足够强大，并且节点预算充足，它可以在一步中生成十多个标记。我们的代码可以在 https://github.com/Jikai0Wang/OPT-Tree 中获得。

##### **Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?**
2406.17274v1 by Jianfeng He, Runing Yang, Linlin Yu, Changbin Li, Ruoxi Jia, Feng Chen, Ming Jin, Chang-Tien Lu

Text summarization, a key natural language generation (NLG) task, is vital in
various domains. However, the high cost of inaccurate summaries in
risk-critical applications, particularly those involving human-in-the-loop
decision-making, raises concerns about the reliability of uncertainty
estimation on text summarization (UE-TS) evaluation methods. This concern stems
from the dependency of uncertainty model metrics on diverse and potentially
conflicting NLG metrics. To address this issue, we introduce a comprehensive
UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The
benchmark evaluates the uncertainty estimation capabilities of two large
language models and one pre-trained language model on three datasets, with
human-annotation analysis incorporated where applicable. We also assess the
performance of 14 common uncertainty estimation methods within this benchmark.
Our findings emphasize the importance of considering multiple uncorrelated NLG
metrics and diverse uncertainty estimation methods to ensure reliable and
efficient evaluation of UE-TS techniques.

摘要：文本摘要，一種關鍵的自然語言生成 (NLG) 任務，在各種領域中至關重要。然而，在風險關鍵應用中不準確摘要的高成本，特別是那些涉及人類參與決策的應用，引起了人們對文本摘要不確定性估計 (UE-TS) 評估方法可靠性的擔憂。這種擔憂源於不確定性模型指標對多樣且潛在衝突的 NLG 指標的依賴性。為了解決這個問題，我們引入了綜合 UE-TS 基準，包含了四個維度中的 31 個 NLG 指標。該基準評估了兩個大型語言模型和一個預訓練語言模型在三個數據集上的不確定性估計能力，並在適用的情況下納入了人工註釋分析。我們還在這個基準中評估了 14 種常見的不確定性估計方法的性能。我們的研究結果強調了考慮多個不相關的 NLG 指標和多樣的不確定性估計方法以確保 UE-TS 技術的可靠且有效的評估的重要性。

##### **DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph**
2406.17271v1 by Zhehao Zhang, Jiaao Chen, Diyi Yang

The current paradigm of evaluating Large Language Models (LLMs) through
static benchmarks comes with significant limitations, such as vulnerability to
data contamination and a lack of adaptability to the evolving capabilities of
LLMs. Therefore, evaluation methods that can adapt and generate evaluation data
with controlled complexity are urgently needed. In this work, we introduce
Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to
dynamically extend current benchmarks with controlled complexity and diversity.
Specifically, we first extract the reasoning graphs of data points in current
benchmarks and then perturb the reasoning graphs to generate novel testing
data. Such newly generated test samples can have different levels of complexity
while maintaining linguistic diversity similar to the original benchmarks. We
further use a code-augmented LLM to ensure the label correctness of newly
generated data. We apply our DARG framework to diverse reasoning tasks in four
domains with 15 state-of-the-art LLMs. Experimental results show that almost
all LLMs experience a performance decrease with increased complexity and
certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit
more biases when being evaluated via the data generated by DARG with higher
complexity levels. These observations provide useful insights into how to
dynamically and adaptively evaluate LLMs. The code is available at
https://github.com/SALT-NLP/DARG.

摘要：目前透過靜態基準評估大型語言模型 (LLM) 的範例伴隨著顯著的限制，例如容易受到資料污染，以及缺乏適應 LLM 不斷演進的能力。因此，迫切需要能夠適應並產生具有受控複雜性的評估資料的評估方法。在這項工作中，我們透過自適應推理圖形演化 (DARG) 引入 LLM 的動態評估，以動態延伸目前具有受控複雜性和多樣性的基準。具體來說，我們首先擷取目前基準中資料點的推理圖形，然後擾動推理圖形以產生新的測試資料。這些新產生的測試樣本可以有不同的複雜性層級，同時維持與原始基準類似的語言多樣性。我們進一步使用程式碼增強的 LLM 來確保新產生資料的標籤正確性。我們將 DARG 架構套用於四個領域中的各種推理任務，並使用 15 個最先進的 LLM。實驗結果顯示，幾乎所有 LLM 在複雜性增加的情況下都會出現效能下降，而某些 LLM 則表現出顯著的下降。此外，我們發現 LLM 在透過 DARG 產生具有較高複雜性層級的資料進行評估時，會表現出更多偏差。這些觀察結果提供了有用的見解，說明如何動態且自適應地評估 LLM。程式碼可在 https://github.com/SALT-NLP/DARG 取得。

##### **AG-LSEC: Audio Grounded Lexical Speaker Error Correction**
2406.17266v1 by Rohit Paturi, Xiang Li, Sundararajan Srinivasan

Speaker Diarization (SD) systems are typically audio-based and operate
independently of the ASR system in traditional speech transcription pipelines
and can have speaker errors due to SD and/or ASR reconciliation, especially
around speaker turns and regions of speech overlap. To reduce these errors, a
Lexical Speaker Error Correction (LSEC), in which an external language model
provides lexical information to correct the speaker errors, was recently
proposed. Though the approach achieves good Word Diarization error rate (WDER)
improvements, it does not use any additional acoustic information and is prone
to miscorrections. In this paper, we propose to enhance and acoustically ground
the LSEC system with speaker scores directly derived from the existing SD
pipeline. This approach achieves significant relative WDER reductions in the
range of 25-40% over the audio-based SD, ASR system and beats the LSEC system
by 15-25% relative on RT03-CTS, Callhome American English and Fisher datasets.

摘要：說話者日記化 (SD) 系統通常是基於音訊，並獨立於傳統語音轉錄管線中的 ASR 系統運作，且可能因 SD 和/或 ASR 調整而產生說話者錯誤，特別是在說話者輪流發言和語音重疊區域附近。為了減少這些錯誤，最近提出了一個詞彙說話者錯誤修正 (LSEC)，其中外部語言模型提供詞彙資訊來修正說話者錯誤。儘管此方法可有效改善詞彙日記化錯誤率 (WDER)，但它並未使用任何額外的音訊資訊，且容易發生錯誤修正。在本文中，我們提議使用直接從現有 SD 管線衍生的說話者分數來增強和建立 LSEC 系統的音訊基礎。這種方法在音訊基礎 SD、ASR 系統上實現了 25-40% 的顯著相對 WDER 降低，並在 RT03-CTS、Callhome 美式英語和 Fisher 資料集上比 LSEC 系統高出 15-25%。

##### **D2LLM: Decomposed and Distilled Large Language Models for Semantic Search**
2406.17262v1 by Zihan Liao, Hang Yu, Jianguo Li, Jun Wang, Wei Zhang

The key challenge in semantic search is to create models that are both
accurate and efficient in pinpointing relevant sentences for queries. While
BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they
often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with
cross-encoder designs capture these nuances but are computationally intensive,
hindering real-time applications. In this paper, we present D2LLMs-Decomposed
and Distilled LLMs for semantic search-that combines the best of both worlds.
We decompose a cross-encoder into an efficient bi-encoder integrated with
Pooling by Multihead Attention and an Interaction Emulation Module, achieving
nuanced understanding and pre-computability. Knowledge from the LLM is
distilled into this model using contrastive, rank, and feature imitation
techniques. Our experiments show that D2LLM surpasses five leading baselines in
terms of all metrics across three tasks, particularly improving NLI task
performance by at least 6.45%. The source code is available at
https://github.com/codefuse-ai/D2LLM.

摘要：語意搜尋的主要挑戰在於建立精準且有效率的模型，以找出與查詢相關的句子。雖然 BERT 式雙編碼器在預先計算的嵌入中表現出效率，但它們在搜尋任務中常常會遺漏細微的差異。相反地，具有交叉編碼器設計的 GPT 式 LLM 可以捕捉到這些差異，但計算量很大，會阻礙實時應用。在本文中，我們提出 D2LLM，即分解和萃取的 LLM，用於語意搜尋，結合兩全其美的優點。我們將交叉編碼器分解成一個有效率的雙編碼器，整合多頭注意力池化和互動模擬模組，實現細微的理解和預先計算。我們使用對比、排序和特徵模擬技術，將 LLM 的知識萃取到這個模型中。我們的實驗顯示，D2LLM 在三個任務的所有指標中都超越了五個主要的基準，特別是將 NLI 任務的效能提升了至少 6.45%。原始碼可在 https://github.com/codefuse-ai/D2LLM 取得。

##### **TRAWL: Tensor Reduced and Approximated Weights for Large Language Models**
2406.17261v1 by Yiran Luo, Het Patel, Yu Fu, Dawon Ahn, Jia Chen, Yue Dong, Evangelos E. Papalexakis

Large language models (LLMs) have fundamentally transformed artificial
intelligence, catalyzing recent advancements while imposing substantial
environmental and computational burdens. We introduce TRAWL (Tensor Reduced and
Approximated Weights for Large Language Models), a novel methodology for
optimizing LLMs through tensor decomposition. TRAWL leverages diverse
strategies to exploit matrices within transformer-based architectures,
realizing notable performance enhancements without necessitating retraining.
The most significant improvements were observed through a layer-by-layer
intervention strategy, particularly when applied to fully connected weights of
the final layers, yielding up to 16% enhancement in accuracy without the need
for additional data or fine-tuning. These results underscore the importance of
targeted and adaptive techniques in increasing the efficiency and effectiveness
of large language model optimization, thereby promoting the development of more
sustainable and accessible AI systems.

摘要：大型語言模型 (LLM) 從根本上改變了人工智慧，催化了近期的進展，同時也帶來巨大的環境和運算負擔。我們引入了 TRAWL（大型語言模型的張量簡化和近似權重），這是一種透過張量分解來最佳化 LLM 的新方法。TRAWL 採用不同的策略來利用基於Transformer的架構中的矩陣，實現顯著的效能提升，而無需重新訓練。最顯著的改進是透過逐層介入策略觀察到的，特別是應用於最後一層的全連接權重時，在無需額外資料或微調的情況下，準確度提升了 16%。這些結果強調了有針對性和自適應技術在提高大型語言模型最佳化的效率和效能方面的重要性，從而促進更永續且可存取的人工智慧系統的發展。

##### **Mitigating Hallucination in Fictional Character Role-Play**
2406.17260v1 by Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley

Role-playing has wide-ranging applications in customer support, embodied
agents, computational social science, etc. The influence of parametric world
knowledge of large language models (LLMs) often causes role-playing characters
to act out of character and hallucinate about things outside the scope of their
knowledge. In this work, we focus on the evaluation and mitigation of
hallucination in fictional character role-play. We introduce a dataset with
more than 2,000 characters and 72,000 interviews, including 18,000 adversarial
questions. We propose RoleFact, a role-playing method that mitigates
hallucination by modulating the influence of parametric knowledge using a
pre-calibrated confidence threshold. Experiments show that the proposed method
improves the factual precision of generated responses by 18% for adversarial
questions with a 44% reduction in temporal hallucination for time-sensitive
interviews. The code and the dataset will be available at
https://github.com/NafisSadeq/rolefact.git.

摘要：角色扮演在客服、具象代理、计算社会科学等方面有着广泛的应用。大型语言模型（LLM）的参数化世界知识的影响常常会导致角色扮演角色的行为脱离角色，并且对超出其知识范围的事物产生幻觉。在这项工作中，我们专注于评估和减轻虚构角色角色扮演中的幻觉。我们引入了一个数据集，其中包含 2,000 多个角色和 72,000 次采访，包括 18,000 个对抗性问题。我们提出了 RoleFact，这是一种通过使用预先校准的置信度阈值来调节参数化知识的影响来减轻幻觉的角色扮演方法。实验表明，对于对抗性问题，所提出的方法将生成响应的事实准确性提高了 18%，而对于时间敏感的采访，时间幻觉减少了 44%。代码和数据集将在 https://github.com/NafisSadeq/rolefact.git 上提供。

##### **Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual Text-to-Speech Adaptation**
2406.17257v1 by Yingting Li, Ambuj Mehrish, Bryan Chew, Bo Cheng, Soujanya Poria

Different languages have distinct phonetic systems and vary in their prosodic
features making it challenging to develop a Text-to-Speech (TTS) model that can
effectively synthesise speech in multilingual settings. Furthermore, TTS
architecture needs to be both efficient enough to capture nuances in multiple
languages and efficient enough to be practical for deployment. The standard
approach is to build transformer based model such as SpeechT5 and train it on
large multilingual dataset. As the size of these models grow the conventional
fine-tuning for adapting these model becomes impractical due to heavy
computational cost. In this paper, we proposes to integrate parameter-efficient
transfer learning (PETL) methods such as adapters and hypernetwork with TTS
architecture for multilingual speech synthesis. Notably, in our experiments
PETL methods able to achieve comparable or even better performance compared to
full fine-tuning with only $\sim$2.5\% tunable parameters.The code and samples
are available at: https://anonymous.4open.science/r/multilingualTTS-BA4C.

摘要：不同的語言具有不同的音系系統，且在韻律特徵上有所不同，這使得開發出能在多語言環境中有效合成語音的文字轉語音 (TTS) 模型變得極具挑戰性。此外，TTS 架構必須既能有效捕捉多種語言的細微差別，又足夠有效率以利於實際部署。標準做法是建立基於轉換器的模型（例如 SpeechT5），並在大型多語言資料集上訓練它。隨著這些模型的規模越來越大，傳統的微調以調整這些模型變得不切實際，因為計算成本過於龐大。在本文中，我們建議將參數有效率的遷移學習 (PETL) 方法（例如適配器和超網路）與 TTS 架構整合，以進行多語言語音合成。值得注意的是，在我們的實驗中，PETL 方法能夠達到與完整微調相當甚至更好的效能，而可調整的參數僅約為 2.5%。程式碼和範例可於以下網址取得：https://anonymous.4open.science/r/multilingualTTS-BA4C。

##### **MPCODER: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning**
2406.17255v1 by Zhenlong Dai, Chang Yao, WenKang Han, Ying Yuan, Zhipeng Gao, Jingyuan Chen

Large Language Models (LLMs) have demonstrated great potential for assisting
developers in their daily development. However, most research focuses on
generating correct code, how to use LLMs to generate personalized code has
seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user
Personalized Code Generator) to generate personalized code for multiple users.
To better learn coding style features, we utilize explicit coding style
residual learning to capture the syntax code style standards and implicit style
learning to capture the semantic code style conventions. We train a multi-user
style adapter to better differentiate the implicit feature representations of
different users through contrastive learning, ultimately enabling personalized
code generation for multiple users. We further propose a novel evaluation
metric for estimating similarities between codes of different coding styles.
The experimental results show the effectiveness of our approach for this novel
task.

摘要：大型語言模型 (LLM) 已展現出在協助開發人員日常開發方面具有巨大潛力。然而，大多數研究都專注於產生正確的程式碼，而鮮少探討如何使用 LLM 來產生個人化的程式碼。為了填補這項空白，我們提出了 MPCoder（多使用者個人化程式碼產生器）來為多位使用者產生個人化的程式碼。為了更深入地學習程式碼風格特徵，我們利用明確的程式碼風格殘差學習來擷取語法程式碼風格標準，並利用隱含風格學習來擷取語義程式碼風格慣例。我們訓練了一個多使用者風格適配器，以透過對比學習來更好地區分不同使用者的隱含特徵表示，最終實現針對多位使用者的個人化程式碼產生。我們進一步提出一個新的評估指標，用於估計不同程式碼風格的程式碼之間的相似性。實驗結果顯示我們的做法對於這項新穎任務的有效性。

##### **How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?**
2406.17253v1 by Huaizhi Ge, Frank Rudzicz, Zining Zhu

As large language models (LLMs) are widely deployed, targeted editing of
their knowledge has become a critical challenge. Recently, advancements in
model editing techniques, such as Rank-One Model Editing (ROME), have paved the
way for updating LLMs with new knowledge. However, the efficacy of these
methods varies across different types of knowledge. This study investigates the
capability of knowledge editing methods to incorporate new knowledge with
varying degrees of "perplexingness", a term we use to describe the initial
difficulty LLMs have in understanding new concepts. We begin by quantifying the
"perplexingness" of target knowledge using pre-edit conditional probabilities,
and assess the efficacy of edits through post-edit conditional probabilities.
Utilizing the widely-used CounterFact dataset, we find significant negative
correlations between the "perplexingness" of the new knowledge and the edit
efficacy across all 12 scenarios. To dive deeper into this phenomenon, we
introduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym
pairs across diverse categories. Our analysis reveal that more abstract
concepts (hypernyms) tend to be more perplexing than their specific
counterparts (hyponyms). Further exploration into the influence of knowledge
hierarchy on editing outcomes indicates that knowledge positioned at higher
hierarchical levels is more challenging to modify in some scenarios. Our
research highlights a previously overlooked aspect of LLM editing: the variable
efficacy of editing methods in handling perplexing knowledge. By revealing how
hierarchical relationships can influence editing outcomes, our findings offer
new insights into the challenges of updating LLMs and pave the way for more
nuanced approaches to model editing in the future.

摘要：<paragraph>隨著大型語言模型 (LLM) 廣泛部署，有針對性地編輯其知識已成為一項關鍵挑戰。最近，模型編輯技術的進展，例如秩一模型編輯 (ROME)，為使用新知識更新 LLM 鋪平了道路。然而，這些方法的功效因不同類型的知識而異。本研究探討了知識編輯方法將具有不同程度「困惑性」的新知識納入其中的能力，我們使用這個術語來描述 LLM 在理解新概念時的初始困難。我們首先使用預編輯條件機率對目標知識的「困惑性」進行量化，並透過後編輯條件機率評估編輯的功效。利用廣泛使用的 CounterFact 資料集，我們發現新知識的「困惑性」與所有 12 種情況下的編輯功效之間存在顯著的負相關。為了更深入探討這個現象，我們引入了新的資料集 HierarchyData，其中包含 99 個跨類別的低階詞-高階詞對。我們的分析顯示，更抽象的概念（高階詞）往往比它們具體的對應概念（低階詞）更令人困惑。進一步探討知識層級對編輯結果的影響表明，在某些情況下，位於較高層級的知識更難以修改。我們的研究強調了 LLM 編輯中先前被忽略的一個面向：編輯方法在處理令人困惑的知識時功效的可變性。透過揭示層級關係如何影響編輯結果，我們的發現為更新 LLM 的挑戰提供了新的見解，並為未來更細緻的模型編輯方法鋪平了道路。</paragraph>

##### **TopoGCL: Topological Graph Contrastive Learning**
2406.17251v1 by Yuzhou Chen, Jose Frias, Yulia R. Gel

Graph contrastive learning (GCL) has recently emerged as a new concept which
allows for capitalizing on the strengths of graph neural networks (GNNs) to
learn rich representations in a wide variety of applications which involve
abundant unlabeled information. However, existing GCL approaches largely tend
to overlook the important latent information on higher-order graph
substructures. We address this limitation by introducing the concepts of
topological invariance and extended persistence on graphs to GCL. In
particular, we propose a new contrastive mode which targets topological
representations of the two augmented views from the same graph, yielded by
extracting latent shape properties of the graph at multiple resolutions. Along
with the extended topological layer, we introduce a new extended persistence
summary, namely, extended persistence landscapes (EPL) and derive its
theoretical stability guarantees. Our extensive numerical results on
biological, chemical, and social interaction graphs show that the new
Topological Graph Contrastive Learning (TopoGCL) model delivers significant
performance gains in unsupervised graph classification for 11 out of 12
considered datasets and also exhibits robustness under noisy scenarios.

摘要：圖形對比學習 (GCL) 近期浮現為一種新概念，它利用圖形神經網路 (GNN) 的優勢，在各種應用中學習豐富的表徵，其中包含大量未標記資訊。然而，現有的 GCL 方法大多傾向於忽略高階圖形子結構中重要的潛在資訊。我們透過在 GCL 中引入拓撲不變性和圖形上的擴充持久性概念來解決這個限制。特別是，我們提出一個新的對比模式，它針對來自同一個圖形的兩個擴充檢視的拓撲表徵，藉由提取圖形在多個解析度下的潛在形狀屬性來產生。隨著擴充拓撲層，我們引入一個新的擴充持久性摘要，即擴充持久性景觀 (EPL)，並推導出其理論穩定性保證。我們在生物、化學和社交互動圖形上的廣泛數值結果顯示，新的拓撲圖形對比學習 (TopoGCL) 模型在 12 個考慮中的資料集中有 11 個資料集的無監督圖形分類中提供了顯著的效能提升，並且在有雜訊的情況下也展現出穩健性。

##### **Beyond Silence: Bias Analysis through Loss and Asymmetric Approach in Audio Anti-Spoofing**
2406.17246v1 by Hye-jin Shim, Md Sahidullah, Jee-weon Jung, Shinji Watanabe, Tomi Kinnunen

Current trends in audio anti-spoofing detection research strive to improve
models' ability to generalize across unseen attacks by learning to identify a
variety of spoofing artifacts. This emphasis has primarily focused on the spoof
class. Recently, several studies have noted that the distribution of silence
differs between the two classes, which can serve as a shortcut. In this paper,
we extend class-wise interpretations beyond silence. We employ loss analysis
and asymmetric methodologies to move away from traditional attack-focused and
result-oriented evaluations towards a deeper examination of model behaviors.
Our investigations highlight the significant differences in training dynamics
between the two classes, emphasizing the need for future research to focus on
robust modeling of the bonafide class.

摘要：當前音訊防偽偵測的研究趨勢致力於提升模型對未見攻擊的概化能力，方法是學習識別各種偽造人工製品。這種重點主要集中在偽造類別上。最近，多項研究指出，兩種類別之間的靜默分佈不同，這可以作為捷徑。在本文中，我們將類別解釋延伸到靜默之外。我們採用損失分析和非對稱方法，從傳統以攻擊為重點和以結果為導向的評估轉向更深入地探討模型行為。我們的調查強調了兩個類別之間訓練動態的顯著差異，強調未來研究需要專注於真實類別的穩健建模。

##### **Unlocking Continual Learning Abilities in Language Models**
2406.17245v1 by Wenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold Cheng, Jie Fu

Language models (LMs) exhibit impressive performance and generalization
capabilities. However, LMs struggle with the persistent challenge of
catastrophic forgetting, which undermines their long-term sustainability in
continual learning (CL). Existing approaches usually address the issue by
incorporating old task data or task-wise inductive bias into LMs. However, old
data and accurate task information are often unavailable or costly to collect,
hindering the availability of current CL approaches for LMs. To address this
limitation, we introduce $\textbf{MIGU}$ ($\textbf{M}$agn$\textbf{I}$tude-based
$\textbf{G}$radient $\textbf{U}$pdating for continual learning), a
rehearsal-free and task-label-free method that only updates the model
parameters with large magnitudes of output in LMs' linear layers. MIGU is based
on our observation that the L1-normalized magnitude distribution of the output
in LMs' linear layers is different when the LM models deal with different task
data. By imposing this simple constraint on the gradient update process, we can
leverage the inherent behaviors of LMs, thereby unlocking their innate CL
abilities. Our experiments demonstrate that MIGU is universally applicable to
all three LM architectures (T5, RoBERTa, and Llama2), delivering
state-of-the-art or on-par performance across continual finetuning and
continual pre-training settings on four CL benchmarks. For example, MIGU brings
a 15.2% average accuracy improvement over conventional parameter-efficient
finetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly
integrate with all three existing CL types to further enhance performance. Code
is available at \href{https://github.com/wenyudu/MIGU}{this https URL}.

摘要：<paragraph>語言模型 (LM) 展現出令人印象深刻的效能和概化能力。然而，LM 面臨持續的災難性遺忘挑戰，這會損害其在持續學習 (CL) 中的長期永續性。現有方法通常透過將舊任務資料或任務明智的歸納偏差納入 LM 來解決這個問題。然而，舊資料和準確的任務資訊通常無法取得或收集成本高昂，這阻礙了目前 LM 的 CL 方法的可用性。為了解決這個限制，我們引入了 $\textbf{MIGU}$（$\textbf{M}$agn$\textbf{I}$tude-based $\textbf{G}$radient $\textbf{U}$pdating for continual learning），一種無需複習和任務標籤的方法，它只更新 LM 線性層中輸出幅度大的模型參數。MIGU 基於我們的觀察，即當 LM 模型處理不同的任務資料時，LM 線性層中輸出的 L1 標準化幅度分佈是不同的。透過對梯度更新過程施加這個簡單的約束，我們可以利用 LM 的固有行為，從而解鎖其天生的 CL 能力。我們的實驗證明，MIGU 普遍適用於所有三種 LM 架構（T5、RoBERTa 和 Llama2），在四個 CL 基準上提供最先進或同等的效能，包括持續微調和持續預訓練設定。例如，在 15 個任務的 CL 基準中，MIGU 比傳統的參數有效微調基準提高了 15.2% 的平均準確度。MIGU 也可以無縫整合所有三種類型的現有 CL，以進一步提升效能。程式碼可在 \href{https://github.com/wenyudu/MIGU}{這個 https URL} 取得。</paragraph>

##### **What Do the Circuits Mean? A Knowledge Edit View**
2406.17241v1 by Huaizhi Ge, Frank Rudzicz, Zining Zhu

In the field of language model interpretability, circuit discovery is gaining
popularity. Despite this, the true meaning of these circuits remain largely
unanswered. We introduce a novel method to learn their meanings as a holistic
object through the lens of knowledge editing. We extract circuits in the
GPT2-XL model using diverse text classification datasets, and use hierarchical
relations datasets to explore knowledge editing in the circuits. Our findings
indicate that these circuits contain entity knowledge but resist new knowledge
more than complementary circuits during knowledge editing. Additionally, we
examine the impact of circuit size, discovering that an ideal "theoretical
circuit" where essential knowledge is concentrated likely incorporates more
than 5% but less than 50% of the model's parameters. We also assess the overlap
between circuits from different datasets, finding moderate similarities. What
constitutes these circuits, then? We find that up to 60% of the circuits
consist of layer normalization modules rather than attention or MLP modules,
adding evidence to the ongoing debates regarding knowledge localization. In
summary, our findings offer new insights into the functions of the circuits,
and introduce research directions for further interpretability and safety
research of language models.

摘要：在語言模型可解釋性的領域中，電路發現正獲得
普及。儘管如此，這些電路的真正含義在很大程度上仍未得到解答。我們引入一種新方法，通過知識編輯的視角將它們的含義作為一個整體對象進行學習。我們使用多樣化的文本分類數據集提取 GPT2-XL 模型中的電路，並使用分層關係數據集探索電路中的知識編輯。我們的研究結果表明，這些電路包含實體知識，但在知識編輯過程中比互補電路更抗拒新知識。此外，我們檢查了電路大小的影響，發現了一個理想的「理論電路」，其中集中了必要的知識，可能包含超過 5% 但少於模型參數的 50%。我們還評估了來自不同數據集的電路之間的重疊，發現有中等相似性。那麼，這些電路由什麼組成呢？我們發現，高達 60% 的電路由層歸一化模組組成，而不是注意力或 MLP 模組，這為關於知識本地化的持續爭論提供了證據。總之，我們的研究結果為電路的功能提供了新的見解，並為語言模型的進一步可解釋性和安全性研究引入了研究方向。

