{"2406.08474": {"publish_time": "2024-06-12", "title": "Real2Code: Reconstruct Articulated Objects via Code Generation", "paper_summary": "We present Real2Code, a novel approach to reconstructing articulated objects\nvia code generation. Given visual observations of an object, we first\nreconstruct its part geometry using an image segmentation model and a shape\ncompletion model. We then represent the object parts with oriented bounding\nboxes, which are input to a fine-tuned large language model (LLM) to predict\njoint articulation as code. By leveraging pre-trained vision and language\nmodels, our approach scales elegantly with the number of articulated parts, and\ngeneralizes from synthetic training data to real world objects in unstructured\nenvironments. Experimental results demonstrate that Real2Code significantly\noutperforms previous state-of-the-art in reconstruction accuracy, and is the\nfirst approach to extrapolate beyond objects' structural complexity in the\ntraining set, and reconstructs objects with up to 10 articulated parts. When\nincorporated with a stereo reconstruction model, Real2Code also generalizes to\nreal world objects from a handful of multi-view RGB images, without the need\nfor depth or camera information.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa Real2Code\uff0c\u4e00\u7a2e\u900f\u904e\u7522\u751f\u7a0b\u5f0f\u78bc\u4f86\u91cd\u5efa\u95dc\u7bc0\u7269\u4ef6\u7684\u65b0\u7a4e\u65b9\u6cd5\u3002\u7d66\u5b9a\u7269\u4ef6\u7684\u8996\u89ba\u89c0\u5bdf\u7d50\u679c\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528\u5f71\u50cf\u5206\u5272\u6a21\u578b\u548c\u5f62\u72c0\u5b8c\u6210\u6a21\u578b\u4f86\u91cd\u5efa\u5176\u90e8\u5206\u5e7e\u4f55\u3002\u7136\u5f8c\u6211\u5011\u7528\u6709\u5411\u908a\u754c\u6846\u8868\u793a\u7269\u4ef6\u90e8\u5206\uff0c\u4f5c\u70ba\u8f38\u5165\u5230\u5fae\u8abf\u904e\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\uff0c\u4ee5\u9810\u6e2c\u4f5c\u70ba\u7a0b\u5f0f\u78bc\u7684\u95dc\u7bc0\u95dc\u7bc0\u3002\u900f\u904e\u5229\u7528\u9810\u5148\u8a13\u7df4\u597d\u7684\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b\uff0c\u6211\u5011\u7684\u505a\u6cd5\u53ef\u4ee5\u512a\u96c5\u5730\u96a8\u8457\u95dc\u7bc0\u90e8\u5206\u6578\u91cf\u7684\u589e\u52a0\u800c\u64f4\u5c55\uff0c\u4e26\u4e14\u53ef\u4ee5\u5f9e\u5408\u6210\u8a13\u7df4\u8cc7\u6599\u63a8\u5ee3\u5230\u975e\u7d50\u69cb\u5316\u74b0\u5883\u4e2d\u7684\u771f\u5be6\u4e16\u754c\u7269\u4ef6\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cReal2Code \u5728\u91cd\u5efa\u6e96\u78ba\u5ea6\u65b9\u9762\u986f\u8457\u512a\u65bc\u5148\u524d\u7684\u6700\u65b0\u6280\u8853\uff0c\u4e26\u4e14\u662f\u7b2c\u4e00\u500b\u8d85\u8d8a\u8a13\u7df4\u96c6\u4e2d\u7269\u4ef6\u7d50\u69cb\u8907\u96dc\u6027\u7684\u5916\u63a8\u65b9\u6cd5\uff0c\u4e26\u4e14\u91cd\u5efa\u5177\u6709\u591a\u9054 10 \u500b\u95dc\u7bc0\u90e8\u5206\u7684\u7269\u4ef6\u3002\u7576\u8207\u7acb\u9ad4\u91cd\u5efa\u6a21\u578b\u7d50\u5408\u4f7f\u7528\u6642\uff0cReal2Code \u4e5f\u80fd\u63a8\u5ee3\u5230\u771f\u5be6\u4e16\u754c\u7684\u7269\u4ef6\uff0c\u9019\u4e9b\u7269\u4ef6\u4f86\u81ea\u5c11\u6578\u591a\u8996\u89d2 RGB \u5f71\u50cf\uff0c\u800c\u4e0d\u9700\u8981\u6df1\u5ea6\u6216\u76f8\u6a5f\u8cc7\u8a0a\u3002", "author": "Zhao Mandi et.al.", "authors": "Zhao Mandi, Yijia Weng, Dominik Bauer, Shuran Song", "id": "2406.08474v1", "paper_url": "http://arxiv.org/abs/2406.08474v1", "repo": "null"}}