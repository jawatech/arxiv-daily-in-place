{"2411.07191": {"publish_time": "2024-11-11", "title": "The Super Weight in Large Language Models", "paper_summary": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u7814\u7a76\u663e\u793a\u4e86\u4e00\u4e2a\u4ee4\u4eba\u60ca\u8bb6\u7684\u7ed3\u679c\uff1a\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u53c2\u6570\u5f02\u5e38\u503c\u7684\u4e00\u5c0f\u90e8\u5206\u5bf9\u4e8e\u6a21\u578b\u7684\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002LLM \u5305\u542b\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\uff0c\u56e0\u6b64\u8fd9\u4e9b\u5c0f\u90e8\u5206\uff0c\u4f8b\u5982 0.01%\uff0c\u8f6c\u6362\u4e3a\u6570\u5341\u4e07\u4e2a\u53c2\u6570\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u4ee4\u4eba\u60ca\u8bb6\u7684\u53d1\u73b0\uff1a\u4fee\u526a\u4e00\u4e2a\u53c2\u6570\u5c31\u53ef\u4ee5\u7834\u574f LLM \u751f\u6210\u6587\u672c\u7684\u80fd\u529b\u2014\u2014\u56f0\u60d1\u5ea6\u589e\u52a0\u4e86 3 \u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u5c06\u96f6\u6b21\u7cbe\u5ea6\u964d\u4f4e\u5230\u731c\u6d4b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6570\u636e\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u8fd9\u4e9b\u53c2\u6570\uff0c\u79f0\u4e3a\u8d85\u7ea7\u6743\u91cd\uff0c\u4f7f\u7528\u5355\u4e2a\u524d\u5411\u4f20\u9012\u901a\u8fc7\u6a21\u578b\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u8fd9\u4e9b\u8d85\u7ea7\u6743\u91cd\u8bf1\u5bfc\u4e86\u76f8\u5e94\u7f55\u89c1\u4e14\u5927\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\uff0c\u79f0\u4e3a\u8d85\u7ea7\u6fc0\u6d3b\u3002\u5f53\u4ee5\u9ad8\u7cbe\u5ea6\u4fdd\u7559\u65f6\uff0c\u8d85\u7ea7\u6fc0\u6d3b\u53ef\u4ee5\u6539\u5584\u7b80\u5355\u7684\u56db\u820d\u4e94\u5165\u91cf\u5316\uff0c\u4ee5\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u7ade\u4e89\u3002\u5bf9\u4e8e\u6743\u91cd\u91cf\u5316\uff0c\u6211\u4eec\u540c\u6837\u53d1\u73b0\uff0c\u901a\u8fc7\u4fdd\u7559\u8d85\u7ea7\u6743\u91cd\u5e76\u88c1\u526a\u5176\u4ed6\u6743\u91cd\u5f02\u5e38\u503c\uff0c\u56db\u820d\u4e94\u5165\u91cf\u5316\u53ef\u4ee5\u6269\u5c55\u5230\u6bd4\u4ee5\u524d\u8003\u8651\u7684\u66f4\u5927\u7684\u5757\u5927\u5c0f\u3002\u4e3a\u4e86\u4fc3\u8fdb\u5bf9\u8d85\u7ea7\u6743\u91cd\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d85\u7ea7\u6743\u91cd\u5750\u6807\u7d22\u5f15\uff0c\u7528\u4e8e\u5e38\u89c1\u4e14\u516c\u5f00\u53ef\u7528\u7684 LLM\u3002</paragraph>", "author": "Mengxia Yu et.al.", "authors": "Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan", "id": "2411.07191v1", "paper_url": "http://arxiv.org/abs/2411.07191v1", "repo": "https://github.com/mengxiayu/llmsuperweight"}}