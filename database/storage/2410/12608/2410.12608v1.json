{"2410.12608": {"publish_time": "2024-10-16", "title": "Not All Votes Count! Programs as Verifiers Improve Self-Consistency of Language Models for Math Reasoning", "paper_summary": "Large language models (LLMs) have shown increasing proficiency in solving\nmathematical reasoning problems. However, many current open-source LLMs often\nstill make calculation and semantic understanding errors in their intermediate\nreasoning steps. In this work, we propose PROVE, a simple yet effective\nframework that uses program-based verification as a heuristic to filter out\npotentially incorrect reasoning paths before aggregating the final answers.\nInstead of relying on vanilla majority voting, our approach rejects solutions\nwhose corresponding program outputs are inconsistent with the generated\nsolution, aggregating only those validated by Python programs. We conducted\nextensive experiments on 13 open-source LLMs from various model families and\nsizes, ranging from 0.5B to 13B parameters, across seven math benchmarks. We\ndemonstrate that PROVE consistently outperforms vanilla majority voting as a\nheuristic for solving mathematical reasoning tasks across all datasets and\nmodel sizes. Notably, PROVE increases accuracy on the GSM8K benchmark from\n48.85% to 53.83% for Qwen2-0.5B-Instruct, from 65.66% to 73.01% for\nLlama-3.2-1B-Instruct, from 73.39% to 79.61% for Gemma-2-2b-it, and from 41.32%\nto 59.51% for Llama-2-7B-chat. Our codes are available at\nhttps://github.com/declare-lab/prove.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u89e3\u6c7a\u6578\u5b78\u63a8\u7406\u554f\u984c\u65b9\u9762\u5df2\u5c55\u73fe\u51fa\u8d8a\u4f86\u8d8a\u9ad8\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8a31\u591a\u7576\u524d\u958b\u653e\u539f\u59cb\u78bc\u7684 LLM \u4ecd\u7d93\u5e38\u5728\u5176\u4e2d\u9593\u63a8\u7406\u6b65\u9a5f\u4e2d\u7522\u751f\u8a08\u7b97\u548c\u8a9e\u7fa9\u7406\u89e3\u932f\u8aa4\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa PROVE\uff0c\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684\u67b6\u69cb\uff0c\u5b83\u4f7f\u7528\u57fa\u65bc\u7a0b\u5f0f\u7684\u9a57\u8b49\u4f5c\u70ba\u4e00\u7a2e\u555f\u767c\u5f0f\u65b9\u6cd5\uff0c\u5728\u532f\u7e3d\u6700\u7d42\u7b54\u6848\u4e4b\u524d\u904e\u6ffe\u6389\u6f5b\u5728\u4e0d\u6b63\u78ba\u7684\u63a8\u7406\u8def\u5f91\u3002\u6211\u5011\u7684\u505a\u6cd5\u4e26\u975e\u4f9d\u8cf4\u65bc\u9999\u8349\u591a\u6578\u6c7a\uff0c\u800c\u662f\u62d2\u7d55\u5176\u5c0d\u61c9\u7a0b\u5f0f\u8f38\u51fa\u8207\u7522\u751f\u89e3\u4e0d\u76f8\u7b26\u7684\u89e3\uff0c\u50c5\u532f\u7e3d\u7531 Python \u7a0b\u5f0f\u9a57\u8b49\u7684\u89e3\u3002\u6211\u5011\u5c0d\u4f86\u81ea\u5404\u7a2e\u6a21\u578b\u7cfb\u5217\u548c\u898f\u6a21\u7684 13 \u500b\u958b\u653e\u539f\u59cb\u78bc LLM \u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6a21\u578b\u53c3\u6578\u7bc4\u570d\u5f9e 0.5B \u5230 13B\uff0c\u6db5\u84cb\u4e03\u500b\u6578\u5b78\u57fa\u6e96\u3002\u6211\u5011\u8b49\u660e PROVE \u5728\u6240\u6709\u8cc7\u6599\u96c6\u548c\u6a21\u578b\u898f\u6a21\u4e2d\u59cb\u7d42\u512a\u65bc\u9999\u8349\u591a\u6578\u6c7a\uff0c\u4f5c\u70ba\u89e3\u6c7a\u6578\u5b78\u63a8\u7406\u4efb\u52d9\u7684\u555f\u767c\u5f0f\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cPROVE \u5c07 Qwen2-0.5B-Instruct \u7684 GSM8K \u57fa\u6e96\u6e96\u78ba\u7387\u5f9e 48.85% \u63d0\u5347\u81f3 53.83%\uff0c\u5c07 Llama-3.2-1B-Instruct \u7684\u6e96\u78ba\u7387\u5f9e 65.66% \u63d0\u5347\u81f3 73.01%\uff0c\u5c07 Gemma-2-2b-it \u7684\u6e96\u78ba\u7387\u5f9e 73.39% \u63d0\u5347\u81f3 79.61%\uff0c\u5c07 Llama-2-7B-chat \u7684\u6e96\u78ba\u7387\u5f9e 41.32% \u63d0\u5347\u81f3 59.51%\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/declare-lab/prove \u4e2d\u53d6\u5f97\u3002", "author": "Vernon Y. H. Toh et.al.", "authors": "Vernon Y. H. Toh, Deepanway Ghosal, Soujanya Poria", "id": "2410.12608v1", "paper_url": "http://arxiv.org/abs/2410.12608v1", "repo": "null"}}