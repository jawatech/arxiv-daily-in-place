{"2412.09278": {"publish_time": "2024-12-12", "title": "Towards a Multimodal Large Language Model with Pixel-Level Insight for Biomedicine", "paper_summary": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB.", "paper_summary_zh": "<paragraph>\u8fd1\u5e74\u6765\uff0c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e86\u5f00\u53d1\u667a\u80fd\u751f\u7269\u533b\u5b66\u52a9\u7406\u7684\u53ef\u884c\u6027\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u751f\u7269\u533b\u5b66 MLLM \u4e3b\u8981\u4e13\u6ce8\u4e8e\u56fe\u50cf\u7ea7\u7406\u89e3\uff0c\u5e76\u5c06\u4ea4\u4e92\u9650\u5236\u5728\u6587\u672c\u547d\u4ee4\u4e2d\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u80fd\u529b\u8fb9\u754c\u548c\u4f7f\u7528\u7075\u6d3b\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u5168\u65b0\u7aef\u5230\u7aef\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u540d\u4e3a MedPLIB\uff0c\u5b83\u5177\u6709\u50cf\u7d20\u7ea7\u7406\u89e3\u80fd\u529b\u3002\u4ee4\u4eba\u5174\u594b\u7684\u662f\uff0c\u5b83\u652f\u6301\u89c6\u89c9\u95ee\u7b54 (VQA)\u3001\u4efb\u610f\u50cf\u7d20\u7ea7\u63d0\u793a\uff08\u70b9\u3001\u8fb9\u754c\u6846\u548c\u81ea\u7531\u5f62\u5f0f\u5f62\u72b6\uff09\u4ee5\u53ca\u50cf\u7d20\u7ea7\u63a5\u5730\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e13\u5bb6\u6df7\u5408 (MoE) \u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5c06 MoE \u5206\u4e3a\u89c6\u89c9\u8bed\u8a00\u4e13\u5bb6\u6a21\u578b\u548c\u50cf\u7d20\u63a5\u5730\u4e13\u5bb6\u6a21\u578b\u7684\u5355\u72ec\u8bad\u7ec3\u9636\u6bb5\uff0c\u7136\u540e\u4f7f\u7528 MoE \u8fdb\u884c\u5fae\u8c03\u3002\u8be5\u7b56\u7565\u6709\u6548\u5730\u534f\u8c03\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u6210\u672c\u4fdd\u6301\u5728\u4e0e\u5355\u4e2a\u4e13\u5bb6\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\u3002\u4e3a\u4e86\u63a8\u8fdb\u751f\u7269\u533b\u5b66 MLLM \u7684\u7814\u7a76\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u533b\u5b66\u590d\u6742\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6 (MeCoVQA)\uff0c\u5b83\u5305\u542b\u4e00\u7cfb\u5217 8 \u79cd\u7528\u4e8e\u590d\u6742\u533b\u5b66\u5f71\u50cf\u95ee\u7b54\u548c\u56fe\u50cf\u533a\u57df\u7406\u89e3\u7684\u6a21\u6001\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMedPLIB \u5728\u591a\u4e2a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5728\u50cf\u7d20\u63a5\u5730\u4efb\u52a1\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\uff0cMedPLIB \u5728 mDice \u6307\u6807\u4e0a\u5206\u522b\u4ee5 19.7 \u548c 15.6 \u7684\u4f18\u52bf\u9886\u5148\u4e8e\u6700\u597d\u7684\u5c0f\u578b\u548c\u5927\u578b\u6a21\u578b\u3002\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u5c06\u5728 https://github.com/ShawnHuang497/MedPLIB \u4e0a\u516c\u5f00\u3002\n</paragraph>", "author": "Xiaoshuang Huang et.al.", "authors": "Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang", "id": "2412.09278v1", "paper_url": "http://arxiv.org/abs/2412.09278v1", "repo": "https://github.com/shawnhuang497/medplib"}}