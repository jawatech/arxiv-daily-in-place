{"2502.03884": {"publish_time": "2025-02-06", "title": "Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning", "paper_summary": "Large language models (LLMs) have demonstrated remarkable success across\nvarious tasks, accompanied by a continuous increase in their parameter size.\nParameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), address the challenges of fine-tuning LLMs by significantly reducing\nthe number of trainable parameters. Recent studies have integrated LoRA with\nMixture of Experts (MoE) architectures, leveraging multiple adapter experts and\ngating mechanisms to further improve fine-tuning performance. However, existing\napproaches primarily focus on adjusting the allocations of adapter experts per\nlayer to optimize the introduced trainable parameter size, while neglecting a\ncritical factor of adapters' rank. To this end, we propose a hierarchical\nscheme for expert allocation and rank configuration, HILO, which dynamically\nadjusts the number and rank of adapter experts across layers, matching the\nvarying representational complexity of model layers in adapter-granularity.\nExtensive experiments on multiple benchmark tasks demonstrate that HILO\noutperforms existing methods in accuracy while introducing fewer trainable\nparameters, providing an efficient and practical solution for fine-tuning LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u986f\u8457\u7684\u6210\u529f\uff0c\u540c\u6642\u5176\u53c3\u6578\u898f\u6a21\u4e5f\u4e0d\u65b7\u589e\u52a0\u3002\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\uff0c\u4f8b\u5982\u4f4e\u968e\u9069\u61c9 (LoRA)\uff0c\u900f\u904e\u5927\u5e45\u6e1b\u5c11\u53ef\u8a13\u7df4\u53c3\u6578\u6578\u91cf\u4f86\u89e3\u6c7a\u5fae\u8abf LLM \u7684\u6311\u6230\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u5c07 LoRA \u8207\u5c08\u5bb6\u6df7\u5408 (MoE) \u67b6\u69cb\u6574\u5408\uff0c\u5229\u7528\u591a\u500b\u9069\u914d\u5668\u5c08\u5bb6\u548c\u9598\u63a7\u6a5f\u5236\u9032\u4e00\u6b65\u63d0\u5347\u5fae\u8abf\u6548\u80fd\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u4e3b\u8981\u8457\u91cd\u65bc\u8abf\u6574\u6bcf\u5c64\u9069\u914d\u5668\u5c08\u5bb6\u7684\u914d\u7f6e\u4ee5\u6700\u4f73\u5316\u5f15\u5165\u7684\u53ef\u8a13\u7df4\u53c3\u6578\u898f\u6a21\uff0c\u540c\u6642\u5ffd\u7565\u9069\u914d\u5668\u7b49\u7d1a\u7684\u95dc\u9375\u56e0\u7d20\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5206\u5c64\u5c08\u5bb6\u914d\u7f6e\u548c\u7b49\u7d1a\u8a2d\u5b9a\u65b9\u6848\uff0cHILO\uff0c\u5b83\u6703\u52d5\u614b\u8abf\u6574\u5404\u5c64\u9069\u914d\u5668\u5c08\u5bb6\u7684\u6578\u91cf\u548c\u7b49\u7d1a\uff0c\u5728\u9069\u914d\u5668\u7c92\u5ea6\u4e2d\u7b26\u5408\u6a21\u578b\u5c64\u8b8a\u5316\u7684\u8868\u793a\u8907\u96dc\u5ea6\u3002\u900f\u904e\u591a\u500b\u57fa\u6e96\u4efb\u52d9\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0cHILO \u5728\u6e96\u78ba\u5ea6\u65b9\u9762\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u540c\u6642\u5f15\u5165\u8f03\u5c11\u7684\u53ef\u8a13\u7df4\u53c3\u6578\uff0c\u70ba\u5fae\u8abf LLM \u63d0\u4f9b\u9ad8\u6548\u4e14\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Peizhuang Cong et.al.", "authors": "Peizhuang Cong, Wenpu Liu, Wenhan Yu, Haochen Zhao, Tong Yang", "id": "2502.03884v1", "paper_url": "http://arxiv.org/abs/2502.03884v1", "repo": "null"}}