{"2406.16694": {"publish_time": "2024-06-24", "title": "Task Oriented In-Domain Data Augmentation", "paper_summary": "Large Language Models (LLMs) have shown superior performance in various\napplications and fields. To achieve better performance on specialized domains\nsuch as law and advertisement, LLMs are often continue pre-trained on in-domain\ndata. However, existing approaches suffer from two major issues. First,\nin-domain data are scarce compared with general domain-agnostic data. Second,\ndata used for continual pre-training are not task-aware, such that they may not\nbe helpful to downstream applications. We propose TRAIT, a task-oriented\nin-domain data augmentation framework. Our framework is divided into two parts:\nin-domain data selection and task-oriented synthetic passage generation. The\ndata selection strategy identifies and selects a large amount of in-domain data\nfrom general corpora, and thus significantly enriches domain knowledge in the\ncontinual pre-training data. The synthetic passages contain guidance on how to\nuse domain knowledge to answer questions about downstream tasks. By training on\nsuch passages, the model aligns with the need of downstream applications. We\nadapt LLMs to two domains: advertisement and math. On average, TRAIT improves\nLLM performance by 8% in the advertisement domain and 7.5% in the math domain.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u548c\u9818\u57df\u4e2d\u5c55\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\u3002\u70ba\u4e86\u5728\u6cd5\u5f8b\u548c\u5ee3\u544a\u7b49\u7279\u5b9a\u9818\u57df\u4e2d\u7372\u5f97\u66f4\u597d\u7684\u6548\u80fd\uff0cLLM \u7d93\u5e38\u6301\u7e8c\u5728\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\u4e2d\u9032\u884c\u9810\u5148\u8a13\u7df4\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u65b9\u6cd5\u6709\u5169\u500b\u4e3b\u8981\u554f\u984c\u3002\u9996\u5148\uff0c\u8207\u4e00\u822c\u9818\u57df\u7121\u95dc\u7684\u8cc7\u6599\u76f8\u6bd4\uff0c\u7279\u5b9a\u9818\u57df\u7684\u8cc7\u6599\u8f03\u70ba\u7a00\u5c11\u3002\u5176\u6b21\uff0c\u7528\u65bc\u6301\u7e8c\u9810\u5148\u8a13\u7df4\u7684\u8cc7\u6599\u4e26\u975e\u4efb\u52d9\u5c0e\u5411\uff0c\u56e0\u6b64\u53ef\u80fd\u5c0d\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u6c92\u6709\u5e6b\u52a9\u3002\u6211\u5011\u63d0\u51fa TRAIT\uff0c\u4e00\u500b\u4efb\u52d9\u5c0e\u5411\u7684\u7279\u5b9a\u9818\u57df\u8cc7\u6599\u64f4\u5145\u67b6\u69cb\u3002\u6211\u5011\u7684\u67b6\u69cb\u5206\u70ba\u5169\u500b\u90e8\u5206\uff1a\u7279\u5b9a\u9818\u57df\u8cc7\u6599\u9078\u53d6\u548c\u4efb\u52d9\u5c0e\u5411\u5408\u6210\u6bb5\u843d\u7522\u751f\u3002\u8cc7\u6599\u9078\u53d6\u7b56\u7565\u5f9e\u4e00\u822c\u8a9e\u6599\u5eab\u4e2d\u8b58\u5225\u4e26\u9078\u53d6\u5927\u91cf\u7684\u7279\u5b9a\u9818\u57df\u8cc7\u6599\uff0c\u5f9e\u800c\u986f\u8457\u8c50\u5bcc\u4e86\u6301\u7e8c\u9810\u5148\u8a13\u7df4\u8cc7\u6599\u4e2d\u7684\u9818\u57df\u77e5\u8b58\u3002\u5408\u6210\u6bb5\u843d\u5305\u542b\u6709\u95dc\u5982\u4f55\u4f7f\u7528\u9818\u57df\u77e5\u8b58\u4f86\u56de\u7b54\u4e0b\u6e38\u4efb\u52d9\u554f\u984c\u7684\u6307\u5c0e\u65b9\u91dd\u3002\u900f\u904e\u8a13\u7df4\u9019\u4e9b\u6bb5\u843d\uff0c\u6a21\u578b\u8207\u4e0b\u6e38\u61c9\u7528\u7a0b\u5f0f\u7684\u9700\u6c42\u4fdd\u6301\u4e00\u81f4\u3002\u6211\u5011\u5c07 LLM \u8abf\u6574\u5230\u5169\u500b\u9818\u57df\uff1a\u5ee3\u544a\u548c\u6578\u5b78\u3002\u5e73\u5747\u800c\u8a00\uff0cTRAIT \u5c07\u5ee3\u544a\u9818\u57df\u7684 LLM \u6548\u80fd\u63d0\u5347\u4e86 8%\uff0c\u6578\u5b78\u9818\u57df\u7684\u6548\u80fd\u63d0\u5347\u4e86 7.5%\u3002", "author": "Xiao Liang et.al.", "authors": "Xiao Liang, Xinyu Hu, Simiao Zuo, Yeyun Gong, Qiang Lou, Yi Liu, Shao-Lun Huang, Jian Jiao", "id": "2406.16694v1", "paper_url": "http://arxiv.org/abs/2406.16694v1", "repo": "null"}}