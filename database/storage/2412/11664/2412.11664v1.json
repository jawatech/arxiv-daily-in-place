{"2412.11664": {"publish_time": "2024-12-16", "title": "C3oT: Generating Shorter Chain-of-Thought without Compromising Effectiveness", "paper_summary": "Generating Chain-of-Thought (CoT) before deriving the answer can effectively\nimprove the reasoning capabilities of large language models (LLMs) and\nsignificantly improve the accuracy of the generated answer. However, in most\ncases, the length of the generated CoT is much longer than the desired final\nanswer, which results in additional decoding costs. Furthermore, existing\nresearch has discovered that shortening the reasoning steps in CoT, even while\npreserving the key information, diminishes LLMs' abilities. These phenomena\nmake it difficult to use LLMs and CoT in many real-world applications that only\nrequire the final answer and are sensitive to latency, such as search and\nrecommendation. To reduce the costs of model decoding and shorten the length of\nthe generated CoT, this paper presents $\\textbf{C}$onditioned\n$\\textbf{C}$ompressed $\\textbf{C}$hain-of-$\\textbf{T}$hought (C3oT), a CoT\ncompression framework that involves a compressor to compress an original longer\nCoT into a shorter CoT while maintaining key information and interpretability,\na conditioned training method to train LLMs with both longer CoT and shorter\nCoT simultaneously to learn the corresponding relationships between them, and a\nconditioned inference method to gain the reasoning ability learned from longer\nCoT by generating shorter CoT. We conduct experiments over four datasets from\narithmetic and commonsense scenarios, showing that the proposed method is\ncapable of compressing the length of generated CoT by up to more than 50%\nwithout compromising its effectiveness.", "paper_summary_zh": "<paragraph>\u5728\u63a8\u5c0e\u51fa\u7b54\u6848\u4e4b\u524d\u751f\u6210\u601d\u7dad\u93c8\uff08CoT\uff09\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e26\u986f\u8457\u63d0\u5347\u751f\u6210\u7b54\u6848\u7684\u6e96\u78ba\u5ea6\u3002\u7136\u800c\uff0c\u5728\u591a\u6578\u60c5\u6cc1\u4e0b\uff0c\u751f\u6210\u7684 CoT \u9577\u5ea6\u9060\u5927\u65bc\u671f\u671b\u7684\u6700\u7d42\u7b54\u6848\uff0c\u9019\u6703\u9020\u6210\u984d\u5916\u7684\u89e3\u78bc\u6210\u672c\u3002\u6b64\u5916\uff0c\u73fe\u6709\u7814\u7a76\u767c\u73fe\uff0c\u7e2e\u77ed CoT \u4e2d\u7684\u63a8\u7406\u6b65\u9a5f\uff0c\u5373\u4f7f\u4fdd\u7559\u95dc\u9375\u8cc7\u8a0a\uff0c\u4e5f\u6703\u964d\u4f4e LLM \u7684\u80fd\u529b\u3002\u9019\u4e9b\u73fe\u8c61\u4f7f\u5f97\u5728\u8a31\u591a\u50c5\u9700\u8981\u6700\u7d42\u7b54\u6848\u4e14\u5c0d\u5ef6\u9072\u654f\u611f\u7684\u5be6\u969b\u61c9\u7528\u4e2d\uff0c\u96e3\u4ee5\u4f7f\u7528 LLM \u548c CoT\uff0c\u4f8b\u5982\u641c\u5c0b\u548c\u63a8\u85a6\u3002\u70ba\u4e86\u964d\u4f4e\u6a21\u578b\u89e3\u78bc\u7684\u6210\u672c\u4e26\u7e2e\u77ed\u751f\u6210\u7684 CoT \u9577\u5ea6\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u689d\u4ef6\u58d3\u7e2e\u601d\u7dad\u93c8\uff08C3oT\uff09\uff0c\u9019\u662f\u4e00\u500b CoT \u58d3\u7e2e\u67b6\u69cb\uff0c\u5305\u542b\u4e00\u500b\u58d3\u7e2e\u5668\uff0c\u7528\u65bc\u5c07\u539f\u59cb\u8f03\u9577\u7684 CoT \u58d3\u7e2e\u6210\u8f03\u77ed\u7684 CoT\uff0c\u540c\u6642\u4fdd\u7559\u95dc\u9375\u8cc7\u8a0a\u548c\u53ef\u89e3\u91cb\u6027\uff0c\u4e00\u500b\u689d\u4ef6\u8a13\u7df4\u65b9\u6cd5\uff0c\u7528\u65bc\u540c\u6642\u8a13\u7df4\u5177\u6709\u8f03\u9577 CoT \u548c\u8f03\u77ed CoT \u7684 LLM\uff0c\u4ee5\u5b78\u7fd2\u5b83\u5011\u4e4b\u9593\u7684\u5c0d\u61c9\u95dc\u4fc2\uff0c\u4ee5\u53ca\u4e00\u500b\u689d\u4ef6\u63a8\u8ad6\u65b9\u6cd5\uff0c\u7528\u65bc\u900f\u904e\u751f\u6210\u8f03\u77ed\u7684 CoT\uff0c\u7372\u5f97\u5f9e\u8f03\u9577\u7684 CoT \u5b78\u5230\u7684\u63a8\u7406\u80fd\u529b\u3002\u6211\u5011\u91dd\u5c0d\u56db\u500b\u4f86\u81ea\u7b97\u8853\u548c\u5e38\u8b58\u5834\u666f\u7684\u8cc7\u6599\u96c6\u9032\u884c\u5be6\u9a57\uff0c\u7d50\u679c\u986f\u793a\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u5920\u5c07\u751f\u6210\u7684 CoT \u9577\u5ea6\u58d3\u7e2e\u8d85\u904e 50%\uff0c\u540c\u6642\u4e0d\u640d\u53ca\u5b83\u7684\u6709\u6548\u6027\u3002</paragraph>", "author": "Yu Kang et.al.", "authors": "Yu Kang, Xianghui Sun, Liangyu Chen, Wei Zou", "id": "2412.11664v1", "paper_url": "http://arxiv.org/abs/2412.11664v1", "repo": "null"}}