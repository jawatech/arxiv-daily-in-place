{"2412.09036": {"publish_time": "2024-12-12", "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty", "paper_summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u7814\u7a76\u71b1\u9ede\u3002\u70ba\u4e86\u52a0\u901f LLM \u7684\u63a8\u8ad6\uff0c\u5c07\u8a08\u7b97\u597d\u7684\u5feb\u53d6\u5132\u5b58\u5728\u8a18\u61b6\u9ad4\u4e2d\u5df2\u6210\u70ba\u6a19\u6e96\u6280\u8853\u3002\u7136\u800c\uff0c\u96a8\u8457\u63a8\u8ad6\u9577\u5ea6\u7684\u589e\u52a0\uff0c\u4e0d\u65b7\u589e\u9577\u7684 KV \u5feb\u53d6\u53ef\u80fd\u6703\u5c0e\u81f4\u8a18\u61b6\u9ad4\u4e0d\u8db3\u7684\u554f\u984c\u3002\u8a31\u591a\u73fe\u6709\u65b9\u6cd5\u900f\u904e KV \u5feb\u53d6\u58d3\u7e2e\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u4e3b\u8981\u662f\u900f\u904e\u5728\u6240\u6709\u5c64\u4e2d\u4fdd\u7559\u95dc\u9375\u4ee3\u78bc\uff0c\u4ee5\u6e1b\u5c11\u8cc7\u8a0a\u907a\u5931\u3002\u5b83\u5011\u5927\u591a\u6578\u6703\u70ba\u6bcf\u500b\u5c64\u5206\u914d\u4e00\u500b\u5747\u52fb\u7684\u9810\u7b97\u5927\u5c0f\u4f86\u4fdd\u7559\u3002\u7136\u800c\uff0c\u6211\u5011\u89c0\u5bdf\u5230\uff0c\u57fa\u65bc\u6ce8\u610f\u529b\u548c\u96b1\u85cf\u72c0\u614b\u8f38\u51fa\u7684\u89c0\u9ede\uff0c\u4fdd\u7559\u5fc5\u8981\u8cc7\u8a0a\u6240\u9700\u7684\u6700\u5c0f\u9810\u7b97\u5927\u5c0f\u6703\u56e0\u5c64\u548c\u6a21\u578b\u800c\u7570\u3002\u57fa\u65bc\u6b64\u89c0\u5bdf\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u4f46\u6709\u6548\u7684 KV \u5feb\u53d6\u58d3\u7e2e\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u5229\u7528\u5c64\u4e0d\u78ba\u5b9a\u6027\u70ba\u6bcf\u500b\u5c64\u5206\u914d\u9810\u7b97\u5927\u5c0f\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u8207\u5b8c\u6574 KV \u63a8\u8ad6\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c07 KV \u5feb\u53d6\u7684\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u6e1b\u5c11\u5230\u50c5\u7d04 20%\uff0c\u540c\u6642\u5be6\u73fe\u5e7e\u4e4e\u7121\u640d\u5931\u7684\u6548\u80fd\u3002", "author": "Meizhi Zhong et.al.", "authors": "Meizhi Zhong, Xikai Liu, Chen Zhang, Yikun Lei, Yan Gao, Yao Hu, Kehai Chen, Min Zhang", "id": "2412.09036v1", "paper_url": "http://arxiv.org/abs/2412.09036v1", "repo": "null"}}