{"2409.03463": {"publish_time": "2024-09-05", "title": "Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks", "paper_summary": "Graph Neural Networks (GNNs) have become increasingly popular for effectively\nmodeling data with graph structures. Recently, attention mechanisms have been\nintegrated into GNNs to improve their ability to capture complex patterns. This\npaper presents the first comprehensive study revealing a critical, unexplored\nconsequence of this integration: the emergence of Massive Activations (MAs)\nwithin attention layers. We introduce a novel method for detecting and\nanalyzing MAs, focusing on edge features in different graph transformer\narchitectures. Our study assesses various GNN models using benchmark datasets,\nincluding ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing\nthe direct link between attention mechanisms and MAs generation in GNNs, (2)\ndeveloping a robust definition and detection method for MAs based on activation\nratio distributions, (3) introducing the Explicit Bias Term (EBT) as a\npotential countermeasure and exploring it as an adversarial framework to assess\nmodels robustness based on the presence or absence of MAs. Our findings\nhighlight the prevalence and impact of attention-induced MAs across different\narchitectures, such as GraphTransformer, GraphiT, and SAN. The study reveals\nthe complex interplay between attention mechanisms, model architecture, dataset\ncharacteristics, and MAs emergence, providing crucial insights for developing\nmore robust and reliable graph models.", "paper_summary_zh": "\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN) \u5df2\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u53d7\u6b61\u8fce\uff0c\u53ef\u6709\u6548\u5efa\u6a21\u5177\u6709\u5716\u5f62\u7d50\u69cb\u7684\u8cc7\u6599\u3002\u6700\u8fd1\uff0c\u6ce8\u610f\u529b\u6a5f\u5236\u5df2\u88ab\u6574\u5408\u5230 GNN \u4e2d\uff0c\u4ee5\u63d0\u5347\u5176\u64f7\u53d6\u8907\u96dc\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\u7b2c\u4e00\u500b\u5168\u9762\u7684\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u9019\u7a2e\u6574\u5408\u7684\u4e00\u500b\u95dc\u9375\u3001\u672a\u63a2\u7d22\u7684\u5f8c\u679c\uff1a\u6ce8\u610f\u529b\u5c64\u4e2d\u51fa\u73fe\u5927\u91cf\u6d3b\u5316 (MA)\u3002\u6211\u5011\u5f15\u5165\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\u4f86\u5075\u6e2c\u548c\u5206\u6790 MA\uff0c\u91cd\u9ede\u653e\u5728\u4e0d\u540c\u5716\u5f62Transformer\u67b6\u69cb\u4e2d\u7684\u908a\u7de3\u7279\u5fb5\u3002\u6211\u5011\u7684\u7814\u7a76\u4f7f\u7528\u57fa\u6e96\u8cc7\u6599\u96c6\u8a55\u4f30\u5404\u7a2e GNN \u6a21\u578b\uff0c\u5305\u62ec ZINC\u3001TOX21 \u548c PROTEINS\u3002\u4e3b\u8981\u8ca2\u737b\u5305\u62ec\uff1a(1) \u5efa\u7acb\u6ce8\u610f\u529b\u6a5f\u5236\u548c GNN \u4e2d MA \u7522\u751f\u7684\u76f4\u63a5\u9023\u7d50\uff0c(2) \u6839\u64da\u6d3b\u5316\u7387\u5206\u4f48\u958b\u767c\u4e00\u500b\u7a69\u5065\u7684\u5b9a\u7fa9\u548c\u5075\u6e2c MA \u7684\u65b9\u6cd5\uff0c(3) \u5f15\u5165\u986f\u5f0f\u504f\u5dee\u9805 (EBT) \u4f5c\u70ba\u4e00\u500b\u6f5b\u5728\u7684\u5c0d\u7b56\uff0c\u4e26\u5c07\u5176\u4f5c\u70ba\u4e00\u500b\u5c0d\u6297\u6846\u67b6\u4f86\u63a2\u7d22\u6a21\u578b\u7684\u7a69\u5065\u6027\uff0c\u6839\u64da MA \u7684\u5b58\u5728\u6216\u4e0d\u5b58\u5728\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u7a81\u986f\u4e86\u6ce8\u610f\u529b\u8a98\u5c0e\u7684 MA \u5728\u4e0d\u540c\u67b6\u69cb\uff08\u4f8b\u5982 GraphTransformer\u3001GraphiT \u548c SAN\uff09\u4e2d\u7684\u666e\u904d\u6027\u548c\u5f71\u97ff\u3002\u8a72\u7814\u7a76\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u6a5f\u5236\u3001\u6a21\u578b\u67b6\u69cb\u3001\u8cc7\u6599\u96c6\u7279\u5fb5\u548c MA \u51fa\u73fe\u4e4b\u9593\u7684\u8907\u96dc\u4ea4\u4e92\u4f5c\u7528\uff0c\u70ba\u958b\u767c\u66f4\u7a69\u5065\u3001\u66f4\u53ef\u9760\u7684\u5716\u5f62\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u898b\u89e3\u3002", "author": "Lorenzo Bini et.al.", "authors": "Lorenzo Bini, Marco Sorbi, Stephane Marchand-Maillet", "id": "2409.03463v1", "paper_url": "http://arxiv.org/abs/2409.03463v1", "repo": "https://github.com/msorbi/gnn-ma"}}