{"2405.20947": {"publish_time": "2024-05-31", "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models", "paper_summary": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where the LLMs may reject innocuous prompts and become less\nhelpful. Although the issue of over-refusal has been empirically observed, a\nsystematic measurement is challenging due to the difficulty of crafting prompts\nthat appear harmful but are benign. This study proposes a novel method for\nautomatically generating large-scale sets of ``seemingly toxic prompts''\n(benign prompts likely rejected by LLMs). Leveraging this technique, we\nintroduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench\ncomprises 80,000 seemingly toxic prompts across 10 common rejection categories,\na subset of around 1,000 hard prompts that are challenging even for\nstate-of-the-art LLMs, and an additional 600 toxic prompts to prevent\nindiscriminate responses. We then conduct a comprehensive study to measure the\nover-refusal of 25 popular LLMs across 8 model families. Our datasets are\navailable at https://huggingface.co/datasets/bench-llm/OR-Bench and the\ncorresponding demo can be found at\nhttps://huggingface.co/spaces/bench-llm/or-bench. We hope this benchmark can\nhelp the community develop better safety aligned models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9700\u8981\u8b39\u614e\u7684\u5b89\u5168\u8abf\u6574\uff0c\u4ee5\u9632\u6b62\u60e1\u610f\u8f38\u51fa\u3002\u96d6\u7136\u91cd\u8981\u7684\u7814\u7a76\u5c08\u6ce8\u65bc\u6e1b\u8f15\u6709\u5bb3\u5167\u5bb9\u7684\u7522\u751f\uff0c\u4f46\u589e\u5f37\u7684\u5b89\u5168\u901a\u5e38\u6703\u7522\u751f\u904e\u5ea6\u62d2\u7d55\u7684\u526f\u4f5c\u7528\uff0cLLM \u53ef\u80fd\u62d2\u7d55\u7121\u5bb3\u7684\u63d0\u793a\u4e26\u8b8a\u5f97\u4e0d\u90a3\u9ebc\u6709\u5e6b\u52a9\u3002\u5118\u7ba1\u5df2\u7d93\u7d93\u9a57\u6027\u5730\u89c0\u5bdf\u5230\u904e\u5ea6\u62d2\u7d55\u7684\u554f\u984c\uff0c\u4f46\u7531\u65bc\u96e3\u4ee5\u64b0\u5beb\u770b\u4f3c\u6709\u5bb3\u4f46\u826f\u6027\u7684\u63d0\u793a\uff0c\u56e0\u6b64\u7cfb\u7d71\u6027\u7684\u6e2c\u91cf\u5177\u6709\u6311\u6230\u6027\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u81ea\u52d5\u7522\u751f\u5927\u91cf\u300c\u770b\u4f3c\u6709\u6bd2\u7684\u63d0\u793a\u300d\u96c6\u5408\uff08\u53ef\u80fd\u88ab LLM \u62d2\u7d55\u7684\u826f\u6027\u63d0\u793a\uff09\u7684\u65b0\u65b9\u6cd5\u3002\u5229\u7528\u6b64\u6280\u8853\uff0c\u6211\u5011\u5f15\u5165\u4e86 OR-Bench\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u5927\u898f\u6a21\u7684\u904e\u5ea6\u62d2\u7d55\u57fa\u6e96\u3002OR-Bench \u5305\u542b 10 \u500b\u5e38\u898b\u62d2\u7d55\u985e\u5225\u4e2d\u7684 80,000 \u500b\u770b\u4f3c\u6709\u6bd2\u7684\u63d0\u793a\uff0c\u4e00\u500b\u7531\u7d04 1,000 \u500b\u5373\u4f7f\u5c0d\u65bc\u6700\u5148\u9032\u7684 LLM \u4f86\u8aaa\u4e5f\u5f88\u6709\u6311\u6230\u6027\u7684\u56f0\u96e3\u63d0\u793a\u5b50\u96c6\uff0c\u4ee5\u53ca\u984d\u5916\u7684 600 \u500b\u6709\u6bd2\u63d0\u793a\uff0c\u4ee5\u9632\u6b62\u4e0d\u52a0\u5340\u5225\u7684\u56de\u61c9\u3002\u7136\u5f8c\uff0c\u6211\u5011\u9032\u884c\u4e86\u4e00\u9805\u5168\u9762\u7814\u7a76\uff0c\u4ee5\u6e2c\u91cf 8 \u500b\u6a21\u578b\u7cfb\u5217\u4e2d\u7684 25 \u500b\u6d41\u884c LLM \u7684\u904e\u5ea6\u62d2\u7d55\u3002\u6211\u5011\u7684\u6578\u64da\u96c6\u53ef\u5728 https://huggingface.co/datasets/bench-llm/OR-Bench \u7372\u5f97\uff0c\u53ef\u4ee5\u5728 https://huggingface.co/spaces/bench-llm/or-bench \u627e\u5230\u5c0d\u61c9\u7684\u793a\u7bc4\u3002\u6211\u5011\u5e0c\u671b\u6b64\u57fa\u6e96\u53ef\u4ee5\u5e6b\u52a9\u793e\u7fa4\u958b\u767c\u66f4\u597d\u7684\u5b89\u5168\u8abf\u6574\u6a21\u578b\u3002", "author": "Justin Cui et.al.", "authors": "Justin Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh", "id": "2405.20947v1", "paper_url": "http://arxiv.org/abs/2405.20947v1", "repo": "null"}}