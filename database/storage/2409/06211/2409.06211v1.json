{"2409.06211": {"publish_time": "2024-09-10", "title": "STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning", "paper_summary": "Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available.", "paper_summary_zh": "<paragraph>\u6df7\u5408\u4e13\u5bb6 (MoE) \u5df2\u88ab\u91c7\u7528\u4ee5\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u4e13\u5bb6\u6765\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002\u5c3d\u7ba1\u6709\u6b64\u964d\u4f4e\uff0c\u4f46 MoE \u4e2d\u5927\u91cf\u7684\u4e13\u5bb6\u4ecd\u7136\u4f7f\u5176\u670d\u52a1\u6210\u672c\u9ad8\u6602\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u4fee\u526a MoE \u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5728\u4fee\u526a\u65b9\u6cd5\u4e2d\uff0c\u4e0e\u7ed3\u6784\u5316\u4fee\u526a\u76f8\u6bd4\uff0c\u975e\u7ed3\u6784\u5316\u4fee\u526a\u5df2\u77e5\u53ef\u4ee5\u9488\u5bf9\u7ed9\u5b9a\u7684\u4fee\u526a\u6bd4\u7387\u5b9e\u73b0\u6700\u9ad8\u6027\u80fd\uff0c\u56e0\u4e3a\u540e\u8005\u5bf9\u7a00\u758f\u5316\u7ed3\u6784\u65bd\u52a0\u4e86\u7ea6\u675f\u3002\u8fd9\u662f\u76f4\u89c2\u7684\uff0c\u56e0\u4e3a\u975e\u7ed3\u6784\u5316\u4fee\u526a\u7684\u89e3\u7a7a\u95f4\u5305\u542b\u4e86\u7ed3\u6784\u5316\u4fee\u526a\u7684\u89e3\u7a7a\u95f4\u3002\u7136\u800c\uff0c\u6211\u4eec\u7684\u53cd\u76f4\u89c9\u53d1\u73b0\u8868\u660e\uff0c\u4e13\u5bb6\u4fee\u526a\uff08\u4e00\u79cd\u7ed3\u6784\u5316\u4fee\u526a\u5f62\u5f0f\uff09\u5b9e\u9645\u4e0a\u53ef\u4ee5\u5148\u4e8e\u975e\u7ed3\u6784\u5316\u4fee\u526a\uff0c\u4ee5\u4f18\u4e8e\u4ec5\u975e\u7ed3\u6784\u5316\u4fee\u526a\u7684\u6027\u80fd\u3002\u7531\u4e8e\u73b0\u6709\u7684\u4e13\u5bb6\u4fee\u526a\u9700\u8981\u9488\u5bf9 n \u4e2a\u4e13\u5bb6\u8fdb\u884c O($\\frac{k^n}{\\sqrt{n}}$) \u524d\u5411\u4f20\u9012\uff0c\u56e0\u6b64\u65e0\u6cd5\u6269\u5c55\u5230\u6700\u8fd1\u7684 MoE\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709 O(1) \u590d\u6742\u5ea6\u4e14\u4f18\u4e8e\u66f4\u6602\u8d35\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\u3002\u5173\u952e\u601d\u60f3\u662f\u5229\u7528\u4e13\u5bb6\u4e4b\u95f4\u7684\u6f5c\u5728\u7ed3\u6784\uff08\u57fa\u4e8e\u884c\u4e3a\u76f8\u4f3c\u6027\uff09\uff0c\u4ee5\u4fbf\u8d2a\u5a6a\u5730\u51b3\u5b9a\u662f\u5426\u4fee\u526a\u53ef\u4ee5\u7d27\u5bc6\u6355\u6349\u8054\u5408\u4fee\u526a\u6548\u679c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u975e\u5e38\u6709\u6548\u2014\u2014\u5bf9\u4e8e Snowflake Arctic\uff08\u4e00\u4e2a\u5177\u6709 128 \u4e2a\u4e13\u5bb6\u7684 480B \u5927\u5c0f\u7684 MoE\uff09\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ea\u9700\u8981\u4e00\u4e2a H100 \u548c\u4e24\u4e2a\u5c0f\u65f6\u5373\u53ef\u5728\u751f\u6210\u6027\u4efb\u52a1\uff08\u4f8b\u5982 GSM8K\uff09\u4e2d\u5b9e\u73b0\u51e0\u4e4e\u6ca1\u6709\u6027\u80fd\u635f\u5931\u4e14\u5177\u6709 40% \u7684\u7a00\u758f\u6027\uff0c\u800c\u6700\u5148\u8fdb\u7684\u975e\u7ed3\u6784\u5316\u4fee\u526a\u65e0\u6cd5\u505a\u5230\u8fd9\u4e00\u70b9\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u63d0\u4f9b\u3002</paragraph>", "author": "Jaeseong Lee et.al.", "authors": "Jaeseong Lee, seung-won hwang, Aurick Qiao, Daniel F Campos, Zhewei Yao, Yuxiong He", "id": "2409.06211v1", "paper_url": "http://arxiv.org/abs/2409.06211v1", "repo": "null"}}