{"2505.00662": {"publish_time": "2025-05-01", "title": "DeepCritic: Deliberate Critique with Large Language Models", "paper_summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5feb\u901f\u767c\u5c55\uff0c\u91dd\u5c0d\u5176\u8f38\u51fa\u7d50\u679c\u63d0\u4f9b\u6e96\u78ba\u7684\u53cd\u994b\u548c\u53ef\u64f4\u5c55\u7684\u76e3\u7763\uff0c\u5df2\u6210\u70ba\u4e00\u500b\u8feb\u5207\u4e14\u95dc\u9375\u7684\u554f\u984c\u3002\u5229\u7528 LLM \u4f5c\u70ba\u8a55\u8ad6\u6a21\u578b\u4f86\u5be6\u73fe\u81ea\u52d5\u5316\u76e3\u7763\u662f\u4e00\u500b\u5f88\u6709\u524d\u666f\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u7814\u7a76\u548c\u589e\u5f37 LLM \u7684\u6578\u5b78\u8a55\u8ad6\u80fd\u529b\u3002\u76ee\u524d\u7684 LLM \u8a55\u8ad6\u6a21\u578b\u63d0\u4f9b\u7684\u8a55\u8ad6\u5728\u6bcf\u500b\u6b65\u9a5f\u4e0a\u90fd\u904e\u65bc\u6dfa\u986f\u548c\u8868\u9762\uff0c\u5c0e\u81f4\u5224\u65b7\u6e96\u78ba\u6027\u4f4e\uff0c\u4e26\u4e14\u96e3\u4ee5\u63d0\u4f9b\u8db3\u5920\u7684\u53cd\u994b\u8b93 LLM \u751f\u6210\u5668\u7cfe\u6b63\u932f\u8aa4\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u4e14\u6709\u6548\u7684\u5169\u968e\u6bb5\u6846\u67b6\u4f86\u958b\u767c LLM \u8a55\u8ad6\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u5920\u4ed4\u7d30\u8a55\u8ad6\u6578\u5b78\u89e3\u984c\u904e\u7a0b\u4e2d\u7684\u6bcf\u500b\u63a8\u7406\u6b65\u9a5f\u3002\u5728\u7b2c\u4e00\u968e\u6bb5\uff0c\u6211\u5011\u5229\u7528 Qwen2.5-72B-Instruct \u751f\u6210 4.5K \u9577\u7bc7\u8a55\u8ad6\u4f5c\u70ba\u76e3\u7763\u5f0f\u5fae\u8abf\u7684\u7a2e\u5b50\u6578\u64da\u3002\u6bcf\u500b\u7a2e\u5b50\u8a55\u8ad6\u90fd\u5305\u542b\u6df1\u601d\u719f\u616e\u7684\u9010\u6b65\u8a55\u8ad6\uff0c\u5176\u4e2d\u5305\u62ec\u591a\u89d2\u5ea6\u9a57\u8b49\u4ee5\u53ca\u5c0d\u6bcf\u500b\u63a8\u7406\u6b65\u9a5f\u7684\u521d\u59cb\u8a55\u8ad6\u7684\u6df1\u5165\u8a55\u8ad6\u3002\u7136\u5f8c\uff0c\u6211\u5011\u4f7f\u7528\u4f86\u81ea PRM800K \u7684\u73fe\u6709\u4eba\u5de5\u6a19\u8a18\u6578\u64da\u6216\u6211\u5011\u901a\u904e\u57fa\u65bc\u8499\u7279\u5361\u6d1b\u62bd\u6a23\u7684\u6b63\u78ba\u6027\u4f30\u8a08\u7372\u5f97\u7684\u81ea\u52d5\u6a19\u8a18\u6578\u64da\uff0c\u5c0d\u5fae\u8abf\u5f8c\u7684\u6a21\u578b\u9032\u884c\u5f37\u5316\u5b78\u7fd2\uff0c\u4ee5\u9032\u4e00\u6b65\u6fc0\u52f5\u5176\u8a55\u8ad6\u80fd\u529b\u3002\u6211\u5011\u57fa\u65bc Qwen2.5-7B-Instruct \u958b\u767c\u7684\u8a55\u8ad6\u6a21\u578b\uff0c\u4e0d\u50c5\u5728\u5404\u7a2e\u932f\u8aa4\u8b58\u5225\u57fa\u6e96\u6e2c\u8a66\u4e2d\u986f\u8457\u512a\u65bc\u73fe\u6709\u7684 LLM \u8a55\u8ad6\u6a21\u578b\uff08\u5305\u62ec\u76f8\u540c\u5927\u5c0f\u7684 DeepSeek-R1-distill \u6a21\u578b\u548c GPT-4o\uff09\uff0c\u800c\u4e14\u9084\u80fd\u901a\u904e\u66f4\u8a73\u7d30\u7684\u53cd\u994b\u66f4\u6709\u6548\u5730\u5e6b\u52a9 LLM \u751f\u6210\u5668\u6539\u9032\u932f\u8aa4\u6b65\u9a5f\u3002</paragraph>\n", "author": "Wenkai Yang et.al.", "authors": "Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen", "id": "2505.00662v1", "paper_url": "http://arxiv.org/abs/2505.00662v1", "repo": "null"}}