
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2025-02-06**|**Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment**|Zuyan Liu et.al.|[2502.04328v1](http://arxiv.org/abs/2502.04328v1)|null|
|**2025-02-06**|**WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs**|Jack Hong et.al.|[2502.04326v1](http://arxiv.org/abs/2502.04326v1)|null|
|**2025-02-06**|**Can Grammarly and ChatGPT accelerate language change? AI-powered technologies and their impact on the English language: wordiness vs. conciseness**|Karolina Rudnicka et.al.|[2502.04324v1](http://arxiv.org/abs/2502.04324v1)|null|
|**2025-02-06**|**Variation of sentence length across time and genre**|Karolina Rudnicka et.al.|[2502.04321v1](http://arxiv.org/abs/2502.04321v1)|null|
|**2025-02-06**|**Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions**|Yik Siu Chan et.al.|[2502.04322v1](http://arxiv.org/abs/2502.04322v1)|null|
|**2025-02-06**|**ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters**|Kamer Ali Yuksel et.al.|[2502.04315v1](http://arxiv.org/abs/2502.04315v1)|null|
|**2025-02-06**|**BOUQuET: dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation**|The Omnilingual MT Team et.al.|[2502.04314v1](http://arxiv.org/abs/2502.04314v1)|null|
|**2025-02-06**|**Great Models Think Alike and this Undermines AI Oversight**|Shashwat Goel et.al.|[2502.04313v1](http://arxiv.org/abs/2502.04313v1)|[link](https://github.com/model-similarity/lm-similarity/tree/main/applications)|
|**2025-02-06**|**HOG-Diff: Higher-Order Guided Diffusion for Graph Generation**|Yiming Huang et.al.|[2502.04308v1](http://arxiv.org/abs/2502.04308v1)|null|
|**2025-02-06**|**ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization**|Yinjie Wang et.al.|[2502.04306v1](http://arxiv.org/abs/2502.04306v1)|null|
|**2025-02-06**|**Strong Equivalence in Answer Set Programming with Constraints**|Pedro Cabalar et.al.|[2502.04302v1](http://arxiv.org/abs/2502.04302v1)|null|
|**2025-02-06**|**Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization**|Yuanye Liu et.al.|[2502.04295v1](http://arxiv.org/abs/2502.04295v1)|null|
|**2025-02-06**|**A Methodology for Studying Linguistic and Cultural Change in China, 1900-1950**|Spencer Dean Stewart et.al.|[2502.04286v1](http://arxiv.org/abs/2502.04286v1)|null|
|**2025-02-06**|**How does a Multilingual LM Handle Multiple Languages?**|Santhosh Kakarla et.al.|[2502.04269v1](http://arxiv.org/abs/2502.04269v1)|null|
|**2025-02-06**|**Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion**|Marco Mistretta et.al.|[2502.04263v1](http://arxiv.org/abs/2502.04263v1)|null|
|**2025-02-06**|**Free Energy Risk Metrics for Systemically Safe AI: Gatekeeping Multi-Agent Study**|Michael Walters et.al.|[2502.04249v1](http://arxiv.org/abs/2502.04249v1)|null|
|**2025-02-06**|**TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi**|Mohammed Amaan Dhamaskar et.al.|[2502.04245v1](http://arxiv.org/abs/2502.04245v1)|null|
|**2025-02-06**|**A Theoretical Framework for Data Efficient Multi-Source Transfer Learning Based on Cramér-Rao Bound**|Qingyue Zhang et.al.|[2502.04242v1](http://arxiv.org/abs/2502.04242v1)|null|
|**2025-02-06**|**MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion**|Xintong Hao et.al.|[2502.04235v1](http://arxiv.org/abs/2502.04235v1)|null|
|**2025-02-06**|**A Classification System Approach in Predicting Chinese Censorship**|Matt Prodani et.al.|[2502.04234v1](http://arxiv.org/abs/2502.04234v1)|null|
|**2025-02-06**|**Dark Distillation: Backdooring Distilled Datasets without Accessing Raw Data**|Ziyuan Yang et.al.|[2502.04229v1](http://arxiv.org/abs/2502.04229v1)|null|
|**2025-02-06**|**NLP-Based .NET CLR Event Logs Analyzer**|Maxim Stavtsev et.al.|[2502.04219v1](http://arxiv.org/abs/2502.04219v1)|null|
|**2025-02-06**|**Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data**|Laura Biester et.al.|[2502.04218v1](http://arxiv.org/abs/2502.04218v1)|null|
|**2025-02-06**|**Algorithmic causal structure emerging through compression**|Liang Wendong et.al.|[2502.04210v1](http://arxiv.org/abs/2502.04210v1)|null|
|**2025-02-06**|**The Best Instruction-Tuning Data are Those That Fit**|Dylan Zhang et.al.|[2502.04194v1](http://arxiv.org/abs/2502.04194v1)|null|
|**2025-02-06**|**Multi-agent Architecture Search via Agentic Supernet**|Guibin Zhang et.al.|[2502.04180v1](http://arxiv.org/abs/2502.04180v1)|null|
|**2025-02-06**|**Lexical Substitution is not Synonym Substitution: On the Importance of Producing Contextually Relevant Word Substitutes**|Juraj Vladika et.al.|[2502.04173v1](http://arxiv.org/abs/2502.04173v1)|null|
|**2025-02-06**|**UltraIF: Advancing Instruction Following from the Wild**|Kaikai An et.al.|[2502.04153v1](http://arxiv.org/abs/2502.04153v1)|null|
|**2025-02-06**|**Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using PDEs**|Jost Arndt et.al.|[2502.04140v1](http://arxiv.org/abs/2502.04140v1)|null|
|**2025-02-06**|**The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs**|Bryan Guan et.al.|[2502.04134v1](http://arxiv.org/abs/2502.04134v1)|null|
|**2025-02-06**|**Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis**|Zhen Ye et.al.|[2502.04128v1](http://arxiv.org/abs/2502.04128v1)|null|
|**2025-02-06**|**VTutor: An Open-Source SDK for Generative AI-Powered Animated Pedagogical Agents with Multi-Media Output**|Eason Chen et.al.|[2502.04103v1](http://arxiv.org/abs/2502.04103v1)|null|
|**2025-02-06**|**Efficient Few-Shot Continual Learning in Vision-Language Models**|Aristeidis Panos et.al.|[2502.04098v1](http://arxiv.org/abs/2502.04098v1)|null|
|**2025-02-06**|**LLMs to Support a Domain Specific Knowledge Assistant**|Maria-Flavia Lovin et.al.|[2502.04095v1](http://arxiv.org/abs/2502.04095v1)|null|
|**2025-02-06**|**Automatic quantification of breast cancer biomarkers from multiple 18F-FDG PET image segmentation**|Tewele W. Tareke et.al.|[2502.04083v1](http://arxiv.org/abs/2502.04083v1)|null|
|**2025-02-06**|**AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference**|Qingyue Yang et.al.|[2502.04077v1](http://arxiv.org/abs/2502.04077v1)|null|
|**2025-02-06**|**Controllable Emotion Generation with Emotion Vectors**|Yurui Dong et.al.|[2502.04075v1](http://arxiv.org/abs/2502.04075v1)|null|
|**2025-02-06**|**Predicting Large Language Model Capabilities on Closed-Book QA Tasks Using Only Information Available Prior to Training**|Changhao Jiang et.al.|[2502.04066v1](http://arxiv.org/abs/2502.04066v1)|null|
|**2025-02-06**|**Strategic Learning with Local Explanations as Feedback**|Kiet Q. H. Vo et.al.|[2502.04058v1](http://arxiv.org/abs/2502.04058v1)|null|
|**2025-02-06**|**Probe-Free Low-Rank Activation Intervention**|Chonghe Jiang et.al.|[2502.04043v1](http://arxiv.org/abs/2502.04043v1)|null|
|**2025-02-06**|**Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment**|Haoyu Wang et.al.|[2502.04040v1](http://arxiv.org/abs/2502.04040v1)|null|
|**2025-02-06**|**Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents**|Yuchen Lian et.al.|[2502.04038v1](http://arxiv.org/abs/2502.04038v1)|null|
|**2025-02-06**|**Exploring Imbalanced Annotations for Effective In-Context Learning**|Hongfu Gao et.al.|[2502.04037v1](http://arxiv.org/abs/2502.04037v1)|null|
|**2025-02-06**|**Generalize Drug Response Prediction by Latent Independent Projection for Asymmetric Constrained Domain Generalization**|Ran Song et.al.|[2502.04034v1](http://arxiv.org/abs/2502.04034v1)|null|
|**2025-02-06**|**Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated Model Merging**|Guinan Su et.al.|[2502.04030v1](http://arxiv.org/abs/2502.04030v1)|null|
|**2025-02-06**|**Quantification of Biodiversity from Historical Survey Text with LLM-based Best-Worst Scaling**|Thomas Haider et.al.|[2502.04022v1](http://arxiv.org/abs/2502.04022v1)|null|
|**2025-02-06**|**Automating a Complete Software Test Process Using LLMs: An Automotive Case Study**|Shuai Wang et.al.|[2502.04008v1](http://arxiv.org/abs/2502.04008v1)|null|
|**2025-02-06**|**Online Learning of Counter Categories and Ratings in PvP Games**|Chiu-Chou Lin et.al.|[2502.03998v1](http://arxiv.org/abs/2502.03998v1)|null|
|**2025-02-06**|**Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering**|Longquan Jiang et.al.|[2502.03992v1](http://arxiv.org/abs/2502.03992v1)|null|
|**2025-02-06**|**PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation**|Hyemin Lim et.al.|[2502.03984v1](http://arxiv.org/abs/2502.03984v1)|null|
|**2025-02-06**|**Towards Unified Music Emotion Recognition across Dimensional and Categorical Models**|Jaeyong Kang et.al.|[2502.03979v1](http://arxiv.org/abs/2502.03979v1)|null|
|**2025-02-06**|**MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation**|YoonJe Kang et.al.|[2502.03966v1](http://arxiv.org/abs/2502.03966v1)|null|
|**2025-02-06**|**Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples**|Konstantinos Tsigos et.al.|[2502.03957v1](http://arxiv.org/abs/2502.03957v1)|null|
|**2025-02-06**|**MAQInstruct: Instruction-based Unified Event Relation Extraction**|Jun Xu et.al.|[2502.03954v1](http://arxiv.org/abs/2502.03954v1)|null|
|**2025-02-06**|**Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond**|Mardhiyah Sanni et.al.|[2502.03945v1](http://arxiv.org/abs/2502.03945v1)|null|
|**2025-02-06**|**DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation**|Dongya Jia et.al.|[2502.03930v1](http://arxiv.org/abs/2502.03930v1)|null|
|**2025-02-06**|**Adaptation of Task Goal States from Prior Knowledge**|Andrei Costinescu et.al.|[2502.03918v1](http://arxiv.org/abs/2502.03918v1)|null|
|**2025-02-06**|**Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software**|Andreas Baumann et.al.|[2502.03916v1](http://arxiv.org/abs/2502.03916v1)|null|
|**2025-02-06**|**Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning**|Peizhuang Cong et.al.|[2502.03884v1](http://arxiv.org/abs/2502.03884v1)|null|
|**2025-02-06**|**BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation**|Bo Pang et.al.|[2502.03860v1](http://arxiv.org/abs/2502.03860v1)|null|
|**2025-02-06**|**Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount**|Yanbiao Ma et.al.|[2502.03852v1](http://arxiv.org/abs/2502.03852v1)|null|
|**2025-02-06**|**Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis**|Lin Yuan et.al.|[2502.03843v1](http://arxiv.org/abs/2502.03843v1)|null|
|**2025-02-06**|**A comprehensive survey of contemporary Arabic sentiment analysis: Methods, Challenges, and Future Directions**|Zhiqiang Shi et.al.|[2502.03827v1](http://arxiv.org/abs/2502.03827v1)|null|
|**2025-02-06**|**Syntriever: How to Train Your Retriever with Synthetic Data from LLMs**|Minsang Kim et.al.|[2502.03824v1](http://arxiv.org/abs/2502.03824v1)|null|
|**2025-02-06**|**PsyPlay: Personality-Infused Role-Playing Conversational Agents**|Tao Yang et.al.|[2502.03821v1](http://arxiv.org/abs/2502.03821v1)|null|
|**2025-02-06**|**Large Language Models for Multi-Robot Systems: A Survey**|Peihan Li et.al.|[2502.03814v1](http://arxiv.org/abs/2502.03814v1)|null|
|**2025-02-06**|**Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective**|Yuan Feng et.al.|[2502.03805v1](http://arxiv.org/abs/2502.03805v1)|null|
|**2025-02-06**|**Understanding and Supporting Formal Email Exchange by Answering AI-Generated Questions**|Yusuke Miura et.al.|[2502.03804v1](http://arxiv.org/abs/2502.03804v1)|null|
|**2025-02-06**|**SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning**|Heyi Zhang et.al.|[2502.03801v1](http://arxiv.org/abs/2502.03801v1)|null|
|**2025-02-06**|**Enhancing Hallucination Detection through Noise Injection**|Litian Liu et.al.|[2502.03799v1](http://arxiv.org/abs/2502.03799v1)|null|
|**2025-02-06**|**It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers**|Benjamin Clavié et.al.|[2502.03793v1](http://arxiv.org/abs/2502.03793v1)|null|
|**2025-02-06**|**ExpProof : Operationalizing Explanations for Confidential Models with ZKPs**|Chhavi Yadav et.al.|[2502.03773v1](http://arxiv.org/abs/2502.03773v1)|null|
|**2025-02-06**|**A Retrospective Systematic Study on Hierarchical Sparse Query Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma**|Chaoyin She et.al.|[2502.03772v1](http://arxiv.org/abs/2502.03772v1)|[link](https://github.com/Asunatan/HSQformer)|
|**2025-02-06**|**Adaptive Semantic Prompt Caching with VectorQ**|Luis Gaspar Schroeder et.al.|[2502.03771v1](http://arxiv.org/abs/2502.03771v1)|null|
|**2025-02-06**|**Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models**|Meiquan Dong et.al.|[2502.03766v1](http://arxiv.org/abs/2502.03766v1)|null|
|**2025-02-06**|**Principal Curvatures Estimation with Applications to Single Cell Data**|Yanlei Zhang et.al.|[2502.03750v1](http://arxiv.org/abs/2502.03750v1)|null|
|**2025-02-06**|**Rethinking the Residual Distribution of Locate-then-Editing Methods in Model Editing**|Xiaopeng Li et.al.|[2502.03748v1](http://arxiv.org/abs/2502.03748v1)|null|
|**2025-02-06**|**Action-Free Reasoning for Policy Generalization**|Jaden Clark et.al.|[2502.03729v1](http://arxiv.org/abs/2502.03729v1)|null|
|**2025-02-06**|**MD-BERT: Action Recognition in Dark Videos via Dynamic Multi-Stream Fusion and Temporal Modeling**|Sharana Dharshikgan Suresh Dass et.al.|[2502.03724v1](http://arxiv.org/abs/2502.03724v1)|null|
|**2025-02-06**|**Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning**|Jaden Clark et.al.|[2502.03717v1](http://arxiv.org/abs/2502.03717v1)|null|
|**2025-02-06**|**Boosting Knowledge Graph-based Recommendations through Confidence-Aware Augmentation with Large Language Models**|Rui Cai et.al.|[2502.03715v1](http://arxiv.org/abs/2502.03715v1)|null|
|**2025-02-06**|**MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers**|Nicole Cho et.al.|[2502.03711v1](http://arxiv.org/abs/2502.03711v1)|null|
|**2025-02-06**|**Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers**|Daniel Beaglehole et.al.|[2502.03708v1](http://arxiv.org/abs/2502.03708v1)|null|
|**2025-02-06**|**LLM Alignment as Retriever Optimization: An Information Retrieval Perspective**|Bowen Jin et.al.|[2502.03699v1](http://arxiv.org/abs/2502.03699v1)|null|
|**2025-02-06**|**DocMIA: Document-Level Membership Inference Attacks against DocVQA Models**|Khanh Nguyen et.al.|[2502.03692v1](http://arxiv.org/abs/2502.03692v1)|null|
|**2025-02-06**|**A Comparison of DeepSeek and Other LLMs**|Tianchen Gao et.al.|[2502.03688v1](http://arxiv.org/abs/2502.03688v1)|null|
|**2025-02-06**|**Variational Control for Guidance in Diffusion Models**|Kushagra Pandey et.al.|[2502.03686v1](http://arxiv.org/abs/2502.03686v1)|null|
|**2025-02-06**|**Controlled LLM Decoding via Discrete Auto-regressive Biasing**|Patrick Pynadath et.al.|[2502.03685v1](http://arxiv.org/abs/2502.03685v1)|null|
|**2025-02-05**|**Reflection-Window Decoding: Text Generation with Selective Refinement**|Zeyu Tang et.al.|[2502.03678v1](http://arxiv.org/abs/2502.03678v1)|null|
|**2025-02-05**|**Advancing Reasoning in Large Language Models: Promising Methods and Approaches**|Avinash Patil et.al.|[2502.03671v1](http://arxiv.org/abs/2502.03671v1)|null|
|**2025-02-05**|**Unrealized Expectations: Comparing AI Methods vs Classical Algorithms for Maximum Independent Set**|Yikai Wu et.al.|[2502.03669v1](http://arxiv.org/abs/2502.03669v1)|null|
|**2025-02-05**|**Energy & Force Regression on DFT Trajectories is Not Enough for Universal Machine Learning Interatomic Potentials**|Santiago Miret et.al.|[2502.03660v1](http://arxiv.org/abs/2502.03660v1)|null|
|**2025-02-05**|**Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics**|Indrashis Das et.al.|[2502.03654v1](http://arxiv.org/abs/2502.03654v1)|null|
|**2025-02-05**|**Looking for the Inner Music: Probing LLMs' Understanding of Literary Style**|Rebecca M. M. Hicke et.al.|[2502.03647v1](http://arxiv.org/abs/2502.03647v1)|null|
|**2025-02-05**|**Context-Preserving Gradient Modulation for Large Language Models: A Novel Approach to Semantic Consistency in Long-Form Text Generation**|Nirola Kobanov et.al.|[2502.03643v1](http://arxiv.org/abs/2502.03643v1)|null|
|**2025-02-05**|**REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations**|Peter Sushko et.al.|[2502.03629v1](http://arxiv.org/abs/2502.03629v1)|null|
|**2025-02-05**|**The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering**|Zhuowei Li et.al.|[2502.03628v1](http://arxiv.org/abs/2502.03628v1)|null|
|**2025-02-05**|**Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database**|Maxime Holmberg Sainte-Marie et.al.|[2502.03627v1](http://arxiv.org/abs/2502.03627v1)|null|
|**2025-02-05**|**AdaPhish: AI-Powered Adaptive Defense and Education Resource Against Deceptive Emails**|Rei Meguro et.al.|[2502.03622v1](http://arxiv.org/abs/2502.03622v1)|null|
|**2025-02-05**|**A Novel Zero-Touch, Zero-Trust, AI/ML Enablement Framework for IoT Network Security**|Sushil Shakya et.al.|[2502.03614v1](http://arxiv.org/abs/2502.03614v1)|null|

#### Abstracts
##### **Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment**
2502.04328v1 by Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao

Recent advances in large language models, particularly following GPT-4o, have
sparked increasing interest in developing omni-modal models capable of
understanding more modalities. While some open-source alternatives have
emerged, there is still a notable lag behind specialized single-modality models
in performance. In this paper, we present Ola, an Omni-modal language model
that achieves competitive performance across image, video, and audio
understanding compared to specialized counterparts. The core design of Ola lies
in its progressive modality alignment strategy that extends the supporting
modality of the language model progressively. Our training pipeline begins with
the most distinct modalities: image and text, then gradually expands the skill
sets of the model using speech data that connects language and audio knowledge,
and video data that connects all modalities. The progressive learning pipeline
also enables us to maintain a relatively small size of the cross-modal
alignment data, making developing omni-modal from existing vision-language
models easy and less costly. Moreover, to unlock an advanced interactive
experience like GPT-4o, we further design a sentence-wise decoding solution for
streaming speech generation. Extensive experiments demonstrate that Ola
surpasses existing open omni-modal LLMs across all modalities while achieving
highly competitive performance compared to state-of-the-art specialized models
of similar sizes. We aim to make Ola a fully open omni-modal understanding
solution to advance future research in this emerging field. Model weights,
code, and data are open-sourced at https://github.com/Ola-Omni/Ola.

摘要：<paragraph>近期大型語言模型的進展，特別是在 GPT-4o 之後，激發了人們對開發全模態模型的興趣，這種模型能夠理解更多模態。雖然已經出現了一些開源替代方案，但在效能上仍顯著落後於專門的單模態模型。在本文中，我們提出 Ola，這是一個全模態語言模型，在影像、影片和音訊理解方面，與專門的對應模型相比，達到了具有競爭力的效能。Ola 的核心設計在於其漸進式模態對齊策略，該策略逐漸擴展語言模型的支援模態。我們的訓練流程從最不同的模態開始：影像和文字，然後使用連接語言和音訊知識的語音資料，以及連接所有模態的影片資料，逐步擴展模型的技能組。漸進式學習流程也讓我們能夠維持相對較小的跨模態對齊資料大小，讓從現有的視覺語言模型開發全模態模型變得容易且成本較低。此外，為了解鎖類似 GPT-4o 的進階互動體驗，我們進一步設計了一個逐句解碼解決方案，用於串流語音生成。廣泛的實驗證明，Ola 在所有模態上都超越了現有的開放全模態 LLM，同時與類似規模的最新專門模型相比，達到了極具競爭力的效能。我們希望讓 Ola 成為一個完全開放的全模態理解解決方案，以推動這個新興領域的未來研究。模型權重、程式碼和資料已在 https://github.com/Ola-Omni/Ola 開源。</paragraph>

##### **WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs**
2502.04326v1 by Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, Weidi Xie

In this paper, we introduce WorldSense, the first benchmark to assess the
multi-modal video understanding, that simultaneously encompasses visual, audio,
and text inputs. In contrast to existing benchmarks, our WorldSense has several
features: (i) collaboration of omni-modality, we design the evaluation tasks to
feature a strong coupling of audio and video, requiring models to effectively
utilize the synergistic perception of omni-modality; (ii) diversity of videos
and tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual
synchronised videos, systematically categorized into 8 primary domains and 67
fine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice
QA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii)
high-quality annotations, all the QA pairs are manually labeled by 80 expert
annotators with multiple rounds of correction to ensure quality. Based on our
WorldSense, we extensively evaluate various state-of-the-art models. The
experimental results indicate that existing models face significant challenges
in understanding real-world scenarios (48.0% best accuracy). We hope our
WorldSense can provide a platform for evaluating the ability in constructing
and understanding coherent contexts from omni-modality.

摘要：在本文中，我們介紹了 WorldSense，這是第一個評估多模態影片理解的基準，同時包含視覺、音訊和文字輸入。與現有的基準相比，我們的 WorldSense 有幾個特點：(i) 全模態協作，我們設計評估任務以呈現音訊和視訊的強耦合，要求模型有效利用全模態的協同感知；(ii) 影片和任務的多樣性，WorldSense 涵蓋了 1,662 個音訊視訊同步影片的多樣化集合，系統地分類為 8 個主要領域和 67 個細粒度的子類別，以涵蓋廣泛的場景，以及 26 個不同任務中的 3,172 個多選題問答對，以進行全面的評估；(iii) 高品質註解，所有問答對均由 80 位專家註解者手動標記，並經過多輪校正以確保品質。根據我們的 WorldSense，我們廣泛評估了各種最先進的模型。實驗結果表明，現有模型在理解真實世界場景方面面臨重大挑戰（最佳準確度為 48.0%）。我們希望我們的 WorldSense 能夠提供一個平台，用於評估從全模態建構和理解連貫脈絡的能力。

##### **Can Grammarly and ChatGPT accelerate language change? AI-powered technologies and their impact on the English language: wordiness vs. conciseness**
2502.04324v1 by Karolina Rudnicka

The proliferation of NLP-powered language technologies, AI-based natural
language generation models, and English as a mainstream means of communication
among both native and non-native speakers make the output of AI-powered tools
especially intriguing to linguists. This paper investigates how Grammarly and
ChatGPT affect the English language regarding wordiness vs. conciseness. A case
study focusing on the purpose subordinator in order to is presented to
illustrate the way in which Grammarly and ChatGPT recommend shorter grammatical
structures instead of longer and more elaborate ones. Although the analysed
sentences were produced by native speakers, are perfectly correct, and were
extracted from a language corpus of contemporary English, both Grammarly and
ChatGPT suggest more conciseness and less verbosity, even for relatively short
sentences. The present article argues that technologies such as Grammarly not
only mirror language change but also have the potential to facilitate or
accelerate it.

摘要：自然語言處理技術、基於人工智慧的自然語言生成模型的普及，以及英語作為母語和非母語人士之間的主要溝通方式，使得人工智慧工具的輸出對語言學家來說特別有趣。本文探討了 Grammarly 和 ChatGPT 如何影響英語中的冗長與簡潔。本研究以目的從屬連接詞 in order to 為例，說明 Grammarly 和 ChatGPT 如何建議使用較短的語法結構，而不是較長且較複雜的結構。儘管分析的句子是由母語人士產生，完全正確，且摘自當代英語的語言語料庫，但 Grammarly 和 ChatGPT 即使對於相對較短的句子，也建議更簡潔、更少冗餘。本文認為，像 Grammarly 這樣的技術不僅反映了語言的變化，還有可能促進或加速這種變化。

##### **Variation of sentence length across time and genre**
2502.04321v1 by Karolina Rudnicka

The goal of this paper is threefold: i) to present some practical aspects of
using full-text version of Corpus of Historical American English (COHA), the
largest diachronic multi-genre corpus of the English language, in the
investigation of a linguistic trend of change; ii) to test a widely held
assumption that sentence length in written English has been steadily decreasing
over the past few centuries; iii) to point to a possible link between the
changes in sentence length and changes in the English syntactic usage. The
empirical proof of concept for iii) is provided by the decline in the frequency
of the non-finite purpose subordinator in order to. Sentence length, genre and
the likelihood of occurrence of in order to are shown to be interrelated.

摘要：這篇論文的三個目標是：i) 提出使用語料庫歷史美國英語（COHA）全文版本的一些實際面向，COHA 是英語最大的歷時多體裁語料庫，用於調查語言趨勢的變化；ii) 驗證一個廣泛存在的假設，即書面英語的句子長度在過去幾個世紀以來持續減少；iii) 指出句子長度變化和英語句法用法變化之間可能的關聯。iii) 的概念實證證明是由非限定目的從屬子句 in order to 的頻率下降所提供的。句子長度、體裁和 in order to 出現的可能性被證明是相互關聯的。

##### **Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions**
2502.04322v1 by Yik Siu Chan, Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi

Despite extensive safety alignment efforts, large language models (LLMs)
remain vulnerable to jailbreak attacks that elicit harmful behavior. While
existing studies predominantly focus on attack methods that require technical
expertise, two critical questions remain underexplored: (1) Are jailbroken
responses truly useful in enabling average users to carry out harmful actions?
(2) Do safety vulnerabilities exist in more common, simple human-LLM
interactions? In this paper, we demonstrate that LLM responses most effectively
facilitate harmful actions when they are both actionable and informative--two
attributes easily elicited in multi-step, multilingual interactions. Using this
insight, we propose HarmScore, a jailbreak metric that measures how effectively
an LLM response enables harmful actions, and Speak Easy, a simple multi-step,
multilingual attack framework. Notably, by incorporating Speak Easy into direct
request and jailbreak baselines, we see an average absolute increase of 0.319
in Attack Success Rate and 0.426 in HarmScore in both open-source and
proprietary LLMs across four safety benchmarks. Our work reveals a critical yet
often overlooked vulnerability: Malicious users can easily exploit common
interaction patterns for harmful intentions.

摘要：儘管安全調整工作廣泛，大型語言模型 (LLM) 仍容易受到引發有害行為的越獄攻擊。現有研究主要關注需要技術專業知識的攻擊方法，但仍有兩個關鍵問題未得到充分探討：(1) 越獄回應是否真的有助於一般使用者執行有害行為？(2) 在更常見、簡單的人類-LLM 互動中是否存在安全漏洞？在本文中，我們證明 LLM 回應在具有可操作性和資訊性時，最能有效地促進有害行為——這兩個屬性很容易在多步驟、多語言互動中引發。利用這個見解，我們提出 HarmScore，這是一個越獄指標，用於衡量 LLM 回應如何有效地促成有害行為，以及 Speak Easy，一個簡單的多步驟、多語言攻擊架構。值得注意的是，透過將 Speak Easy 納入直接請求和越獄基準，我們看到開源和專有 LLM 在四個安全基準中的攻擊成功率平均絕對增加 0.319，HarmScore 增加 0.426。我們的研究揭示了一個關鍵但經常被忽視的漏洞：惡意使用者可以輕鬆利用常見的互動模式進行有害意圖。

##### **ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters**
2502.04315v1 by Kamer Ali Yuksel, Hassan Sawaf

Recent advances in large language models (LLMs) have shown remarkable
performance across diverse tasks. However, these models are typically deployed
with fixed weights, which limits their ability to adapt dynamically to the
variability inherent in real-world data during inference. This paper introduces
ChamaleonLLM, a novel framework that enables inference-time adaptation of LLMs
by leveraging batch-aware clustering and on-the-fly generation of low-rank
updates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation
(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable
masks), our method dynamically generates adaptive modifications to the decoder
weights based on the aggregated statistics of clustered batches. By
intelligently grouping similar inputs and computing context-aware low-rank
updates via a hyper-network, ChamaleonLLM achieves significant performance
gains, outperforming conventional LoRA methods while eliminating the overhead
of maintaining multiple expert models. Our experiments highlight the potential
of our approach to serve as a versatile and highly adaptive solution for
language model inference. ChamaleonLLM is open-sourced to ensure the
reproducibility of our experiments:
https://anonymous.4open.science/r/ChamaleonLLM/

摘要：大型語言模型 (LLM) 的最新進展已在各種任務中展現出卓越的效能。然而，這些模型通常會以固定的權重進行部署，這限制了它們在推理過程中動態適應現實世界資料中固有變異性的能力。本文介紹了 ChamaleonLLM，一個創新的架構，它透過利用批次感知分群和即時生成低秩更新，實現 LLM 的推理時間適應。與傳統的微調方法（例如低秩適應 (LoRA) 或依賴於固定預先學習的均勻集 (可變遮罩) 的方法）不同，我們的模型會根據分群批次的匯總統計資料，動態生成對解碼器權重的適應性修改。透過智慧地將類似的輸入分組，並透過超網路計算與脈絡相關的低秩更新，ChamaleonLLM 達到了顯著的效能提升，優於傳統的 LoRA 方法，同時消除了維護多個專家模型的開銷。我們的實驗突顯了我們的方法作為語言模型推理的通用且高度適應性解決方案的潛力。ChamaleonLLM 是開源的，以確保我們實驗的可複製性：
https://anonymous.4open.science/r/ChamaleonLLM/

##### **BOUQuET: dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation**
2502.04314v1 by The Omnilingual MT Team, Pierre Andrews, Mikel Artetxe, Mariano Coria Meglioli, Marta R. Costa-jussà, Joe Chuang, David Dale, Cynthia Gao, Jean Maillard, Alex Mourachko, Christophe Ropers, Safiyyah Saleem, Eduardo Sánchez, Ioannis Tsiamas, Arina Turkatenko, Albert Ventayol-Boada, Shireen Yates

This paper presents BOUQuET, a multicentric and multi-register/domain dataset
and benchmark, and its broader collaborative extension initiative. This dataset
is handcrafted in non-English languages first, each of these source languages
being represented among the 23 languages commonly used by half of the world's
population and therefore having the potential to serve as pivot languages that
will enable more accurate translations. The dataset is specially designed to
avoid contamination and be multicentric, so as to enforce representation of
multilingual language features. In addition, the dataset goes beyond the
sentence level, as it is organized in paragraphs of various lengths. Compared
with related machine translation (MT) datasets, we show that BOUQuET has a
broader representation of domains while simplifying the translation task for
non-experts. Therefore, BOUQuET is specially suitable for the open initiative
and call for translation participation that we are launching to extend it to a
multi-way parallel corpus to any written language.

摘要：本論文提出 BOUQuET，這是一個多中心、多註冊/網域資料集和基準，以及其更廣泛的協作延伸計畫。此資料集首先以非英語語言手工製作，這些原始語言中的每一種語言都代表著世界一半人口常用的 23 種語言之一，因此有可能作為樞紐語言，能實現更準確的翻譯。此資料集經過特別設計，可避免污染並成為多中心，以便強制執行多語言語言特徵的表示。此外，資料集超越句子層級，因為它是以不同長度的段落組織的。與相關機器翻譯 (MT) 資料集相比，我們顯示 BOUQuET 具有更廣泛的網域表示，同時簡化了非專家的翻譯任務。因此，BOUQuET 特別適合開放計畫，並呼籲翻譯參與，我們正在啟動它，將其擴充為任何書面語言的多向平行語料庫。

##### **Great Models Think Alike and this Undermines AI Oversight**
2502.04313v1 by Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna K Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Prabhu, Matthias Bethge, Jonas Geiping

As Language Model (LM) capabilities advance, evaluating and supervising them
at scale is getting harder for humans. There is hope that other language models
can automate both these tasks, which we refer to as "AI Oversight". We study
how model similarity affects both aspects of AI oversight by proposing a
probabilistic metric for LM similarity based on overlap in model mistakes.
Using this metric, we first show that LLM-as-a-judge scores favor models
similar to the judge, generalizing recent self-preference results. Then, we
study training on LM annotations, and find complementary knowledge between the
weak supervisor and strong student model plays a crucial role in gains from
"weak-to-strong generalization". As model capabilities increase, it becomes
harder to find their mistakes, and we might defer more to AI oversight.
However, we observe a concerning trend -- model mistakes are becoming more
similar with increasing capabilities, pointing to risks from correlated
failures. Our work underscores the importance of reporting and correcting for
model similarity, especially in the emerging paradigm of AI oversight.

摘要：隨著語言模型 (LM) 能力的進步，人類越來越難以大規模評估和監督它們。我們期望其他語言模型可以自動執行這兩項任務，而我們稱之為「AI 監督」。我們研究模型相似度如何影響 AI 監督的兩個方面，方法是根據模型錯誤中的重疊提出一個用於 LM 相似度的機率指標。使用此指標，我們首先表明，LLM 作為評審的分數偏好與評審相似的模型，概括了最近的自我偏好結果。然後，我們研究 LM 標註的訓練，並發現弱監督者和強學生模型之間的互補知識在「弱到強概括」的增益中發揮了關鍵作用。隨著模型能力的提高，越來越難找到它們的錯誤，我們可能會更多地依賴 AI 監督。然而，我們觀察到一個令人擔憂的趨勢——隨著能力的提高，模型錯誤變得越來越相似，這表明相關故障存在風險。我們的研究強調了報告和更正模型相似性（特別是在新興的 AI 監督範例中）的重要性。

##### **HOG-Diff: Higher-Order Guided Diffusion for Graph Generation**
2502.04308v1 by Yiming Huang, Tolga Birdal

Graph generation is a critical yet challenging task as empirical analyses
require a deep understanding of complex, non-Euclidean structures. Although
diffusion models have recently made significant achievements in graph
generation, these models typically adapt from the frameworks designed for image
generation, making them ill-suited for capturing the topological properties of
graphs. In this work, we propose a novel Higher-order Guided Diffusion
(HOG-Diff) model that follows a coarse-to-fine generation curriculum and is
guided by higher-order information, enabling the progressive generation of
plausible graphs with inherent topological structures. We further prove that
our model exhibits a stronger theoretical guarantee than classical diffusion
frameworks. Extensive experiments on both molecular and generic graph
generation tasks demonstrate that our method consistently outperforms or
remains competitive with state-of-the-art baselines. Our code is available at
https://github.com/Yiminghh/HOG-Diff.

摘要：圖形生成是一項至關重要的任務，但由於經驗分析需要對複雜的非歐幾何結構有深入的了解，因此具有挑戰性。儘管擴散模型最近在圖形生成方面取得了顯著進展，但這些模型通常會根據為圖像生成設計的框架進行調整，這使得它們不適合捕捉圖形的拓撲屬性。在這項工作中，我們提出了一個新的高階引導擴散 (HOG-Diff) 模型，它遵循從粗到精的生成課程，並由高階資訊引導，能夠逐步生成具有內在拓撲結構的合理圖形。我們進一步證明，我們的模型比經典擴散框架具有更強的理論保證。在分子和通用圖形生成任務上的大量實驗表明，我們的模型始終優於或與最先進的基準保持競爭力。我們的程式碼可在 https://github.com/Yiminghh/HOG-Diff 取得。

##### **ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization**
2502.04306v1 by Yinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, Bryon Aragam

Recent research has leveraged large language model multi-agent systems for
complex problem-solving while trying to reduce the manual effort required to
build them, driving the development of automated agent workflow optimization
methods. However, existing methods remain inflexible due to representational
limitations, a lack of adaptability, and poor scalability when relying on
discrete optimization techniques. We address these challenges with ScoreFlow, a
simple yet high-performance framework that leverages efficient gradient-based
optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel
variant of the direct preference optimization method that accounts for
quantitative feedback. Across six benchmarks spanning question answering,
coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over
existing baselines. Moreover, it empowers smaller models to outperform larger
ones with lower inference costs. Project:
https://github.com/Gen-Verse/ScoreFlow

摘要：近期研究已利用大型語言模型多代理系統來解決複雜問題，同時試圖減少建置它們所需的手動工作，推動自動化代理工作流程最佳化方法的發展。然而，現有方法由於表示限制、缺乏適應性以及在依賴離散最佳化技術時可擴充性不佳，因此仍然缺乏彈性。我們透過 ScoreFlow 來因應這些挑戰，ScoreFlow 是個簡單但效能高的架構，它在連續空間中利用高效的基於梯度的最佳化。ScoreFlow 整合了 Score-DPO，這是一種新穎的直接偏好最佳化方法變體，它會考量量化回饋。在涵蓋問答、編碼和數學推理的六個基準測試中，ScoreFlow 較現有的基準線改善了 8.2%。此外，它能讓較小的模型以較低的推論成本勝過較大的模型。專案：
https://github.com/Gen-Verse/ScoreFlow

##### **Strong Equivalence in Answer Set Programming with Constraints**
2502.04302v1 by Pedro Cabalar, Jorge Fandinno, Torsten Schaub, Philipp Wanko

We investigate the concept of strong equivalence within the extended
framework of Answer Set Programming with constraints. Two groups of rules are
considered strongly equivalent if, informally speaking, they have the same
meaning in any context. We demonstrate that, under certain assumptions, strong
equivalence between rule sets in this extended setting can be precisely
characterized by their equivalence in the logic of Here-and-There with
constraints. Furthermore, we present a translation from the language of several
clingo-based answer set solvers that handle constraints into the language of
Here-and-There with constraints. This translation enables us to leverage the
logic of Here-and-There to reason about strong equivalence within the context
of these solvers. We also explore the computational complexity of determining
strong equivalence in this context.

摘要：我們在帶有約束的擴充回答設定程式設計架構中調查強等價的概念。兩組規則被認為是強等價的，如果非正式地說，它們在任何情況下都有相同的含義。我們證明，在某些假設下，此擴充設定中規則集之間的強等價可以透過它們在帶有約束的此處和彼處邏輯中的等價性來精確描述。此外，我們提供了一個從處理約束的幾個基於 Clingo 的答案集求解器的語言到帶有約束的此處和彼處語言的翻譯。此翻譯使我們能夠利用此處和彼處的邏輯在這些求解器的背景下推論強等價。我們還探討了在此背景下確定強等價的計算複雜度。

##### **Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization**
2502.04295v1 by Yuanye Liu, Jiahang Xu, Li Lyna Zhang, Qi Chen, Xuan Feng, Yang Chen, Zhongxin Guo, Yuqing Yang, Cheng Peng

Large Language Models (LLMs) have shown significant capability across various
tasks, with their real-world effectiveness often driven by prompt design. While
recent research has focused on optimizing prompt content, the role of prompt
formatting, a critical but often overlooked dimension, has received limited
systematic investigation. In this paper, we introduce Content-Format Integrated
Prompt Optimization (CFPO), an innovative methodology that jointly optimizes
both prompt content and formatting through an iterative refinement process.
CFPO leverages natural language mutations to explore content variations and
employs a dynamic format exploration strategy that systematically evaluates
diverse format options. Our extensive evaluations across multiple tasks and
open-source LLMs demonstrate that CFPO demonstrates measurable performance
improvements compared to content-only optimization methods. This highlights the
importance of integrated content-format optimization and offers a practical,
model-agnostic approach to enhancing LLM performance. Code will be available at
https://github.com/HenryLau7/CFPO.

摘要：大型語言模型 (LLM) 已展現出各種任務的顯著能力，其在現實世界中的有效性通常取決於提示設計。儘管最近的研究專注於最佳化提示內容，但提示格式化的角色是一個關鍵但經常被忽視的層面，尚未獲得系統性的研究。在本文中，我們介紹了內容格式整合提示最佳化 (CFPO)，這是一種創新的方法，可透過反覆改善程序，同時最佳化提示內容和格式。CFPO 利用自然語言變異來探索內容變化，並採用動態格式探索策略，系統性地評估各種格式選項。我們在多項任務和開源 LLM 中進行的廣泛評估證明，與僅內容最佳化方法相比，CFPO 表現出可衡量的效能改善。這突顯了整合內容格式最佳化的重要性，並提供了一種實用的、與模型無關的方法來提升 LLM 效能。程式碼將於 https://github.com/HenryLau7/CFPO 提供。

##### **A Methodology for Studying Linguistic and Cultural Change in China, 1900-1950**
2502.04286v1 by Spencer Dean Stewart

This paper presents a quantitative approach to studying linguistic and
cultural change in China during the first half of the twentieth century, a
period that remains understudied in computational humanities research. The
dramatic changes in Chinese language and culture during this time call for
greater reflection on the tools and methods used for text analysis. This
preliminary study offers a framework for analyzing Chinese texts from the late
nineteenth and twentieth centuries, demonstrating how established methods such
as word counts and word embeddings can provide new historical insights into the
complex negotiations between Western modernity and Chinese cultural discourse.

摘要：本文提出了一種量化方法，用於研究二十世紀上半葉中國的語言和文化變遷，這段時期在計算人文研究中仍未得到充分研究。這段時間中國語言和文化的劇烈變化，要求我們對用於文本分析的工具和方法進行更深入的思考。這項初步研究提供了一個分析十九世紀末和二十世紀中國文本的框架，展示了詞彙計數和詞嵌入等既有方法如何為西方現代性和中國文化話語之間的複雜協商提供新的歷史見解。

##### **How does a Multilingual LM Handle Multiple Languages?**
2502.04269v1 by Santhosh Kakarla, Gautama Shastry Bulusu Venkata, Aishwarya Gaddam

Multilingual language models have significantly advanced due to rapid
progress in natural language processing. Models like BLOOM 1.7B, trained on
diverse multilingual datasets, aim to bridge linguistic gaps. However, their
effectiveness in capturing linguistic knowledge, particularly for low-resource
languages, remains an open question. This study critically examines MLMs
capabilities in multilingual understanding, semantic representation, and
cross-lingual knowledge transfer. While these models perform well for
high-resource languages, they struggle with less-represented ones.
Additionally, traditional evaluation methods often overlook their internal
syntactic and semantic encoding.
  This research addresses key limitations through three objectives. First, it
assesses semantic similarity by analyzing multilingual word embeddings for
consistency using cosine similarity. Second, it examines BLOOM-1.7B and Qwen2
through Named Entity Recognition and sentence similarity tasks to understand
their linguistic structures. Third, it explores cross-lingual knowledge
transfer by evaluating generalization from high-resource to low-resource
languages in sentiment analysis and text classification.
  By leveraging linguistic probing, performance metrics, and visualizations,
this study provides insights into the strengths and limitations of MLMs. The
findings aim to enhance multilingual NLP models, ensuring better support for
both high- and low-resource languages, thereby promoting inclusivity in
language technologies.

摘要：多語言語言模型由於自然語言處理的快速進展而顯著提升。BLOOM 1.7B 等模型在多樣化多語言資料集上受訓，旨在彌合語言差距。然而，它們在擷取語言知識的效能，特別是對於低資源語言，仍然是一個開放性的問題。本研究批判性地探討多語言理解、語義表徵和跨語言知識轉移中的多語言模型能力。雖然這些模型在高資源語言中表現良好，但它們在低資源語言中卻表現不佳。此外，傳統的評估方法通常會忽略它們的內部句法和語義編碼。
本研究透過三個目標來解決關鍵的限制。首先，它透過使用餘弦相似度分析多語言詞嵌入的一致性來評估語義相似性。其次，它透過命名實體辨識和句子相似度任務來探討 BLOOM-1.7B 和 Qwen2，以了解它們的語言結構。第三，它透過評估從高資源語言到低資源語言的情感分析和文字分類中的概化，來探討跨語言知識轉移。
本研究透過利用語言探測、效能指標和視覺化，深入了解多語言模型的優點和限制。這些發現旨在增強多語言自然語言處理模型，確保對高資源和低資源語言提供更好的支援，進而促進語言技術的包容性。

##### **Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion**
2502.04263v1 by Marco Mistretta, Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, Andrew D. Bagdanov

Pre-trained multi-modal Vision-Language Models like CLIP are widely used
off-the-shelf for a variety of applications. In this paper, we show that the
common practice of individually exploiting the text or image encoders of these
powerful multi-modal models is highly suboptimal for intra-modal tasks like
image-to-image retrieval. We argue that this is inherently due to the
CLIP-style inter-modal contrastive loss that does not enforce any intra-modal
constraints, leading to what we call intra-modal misalignment. To demonstrate
this, we leverage two optimization-based modality inversion techniques that map
representations from their input modality to the complementary one without any
need for auxiliary data or additional trained adapters. We empirically show
that, in the intra-modal tasks of image-to-image and text-to-text retrieval,
approaching these tasks inter-modally significantly improves performance with
respect to intra-modal baselines on more than fifteen datasets. Additionally,
we demonstrate that approaching a native inter-modal task (e.g. zero-shot image
classification) intra-modally decreases performance, further validating our
findings. Finally, we show that incorporating an intra-modal term in the
pre-training objective or narrowing the modality gap between the text and image
feature embedding spaces helps reduce the intra-modal misalignment. The code is
publicly available at: https://github.com/miccunifi/Cross-the-Gap.

摘要：預訓練的多模態視覺語言模型，例如 CLIP，廣泛用於各種應用程式中。在本文中，我們展示了單獨利用這些強大多模態模型的文字或影像編碼器的常見做法，對於影像到影像擷取等模態內任務而言，是非常次佳的。我們認為這本質上是基於 CLIP 風格的模態間對比損失所致，它不會強制執行任何模態內約束，導致我們稱之為模態內錯位。為了證明這一點，我們利用了兩種基於最佳化的模態反轉技術，這些技術將表示從其輸入模態映射到互補模態，而無需任何輔助資料或額外訓練的適配器。我們憑經驗表明，在影像到影像和文字到文字擷取的模態內任務中，以模態間方式處理這些任務會顯著改善在超過 15 個資料集上的模態內基準的效能。此外，我們證明以模態內方式處理原生模態間任務（例如零次學習影像分類）會降低效能，這進一步驗證了我們的發現。最後，我們表明在預訓練目標中納入模態內術語或縮小文字和影像特徵嵌入空間之間的模態差距有助於減少模態內錯位。程式碼公開於：https://github.com/miccunifi/Cross-the-Gap。

##### **Free Energy Risk Metrics for Systemically Safe AI: Gatekeeping Multi-Agent Study**
2502.04249v1 by Michael Walters, Rafael Kaufmann, Justice Sefas, Thomas Kopinski

We investigate the Free Energy Principle as a foundation for measuring risk
in agentic and multi-agent systems. From these principles we introduce a
Cumulative Risk Exposure metric that is flexible to differing contexts and
needs. We contrast this to other popular theories for safe AI that hinge on
massive amounts of data or describing arbitrarily complex world models. In our
framework, stakeholders need only specify their preferences over system
outcomes, providing straightforward and transparent decision rules for risk
governance and mitigation. This framework naturally accounts for uncertainty in
both world model and preference model, allowing for decision-making that is
epistemically and axiologically humble, parsimonious, and future-proof. We
demonstrate this novel approach in a simplified autonomous vehicle environment
with multi-agent vehicles whose driving policies are mediated by gatekeepers
that evaluate, in an online fashion, the risk to the collective safety in their
neighborhood, and intervene through each vehicle's policy when appropriate. We
show that the introduction of gatekeepers in an AV fleet, even at low
penetration, can generate significant positive externalities in terms of
increased system safety.

摘要：我們探討自由能原理作為衡量代理系統和多代理系統中風險的基礎。根據這些原理，我們引入了一個累積風險暴露指標，它可以靈活地適應不同的背景和需求。我們將其與其他流行的安全 AI 理論進行對比，這些理論依賴於大量數據或描述任意複雜的世界模型。在我們的框架中，利益相關者只需指定他們對系統結果的偏好，從而為風險治理和緩解提供直接且透明的決策規則。這個框架自然地考慮了世界模型和偏好模型中的不確定性，允許進行認識論和價值論上謙虛、簡潔且具有前瞻性的決策。我們在一個簡化的自動駕駛車環境中展示了這種新穎的方法，其中多代理車輛的駕駛策略由閘門員調解，閘門員以在線方式評估其鄰域中對集體安全的風險，並在適當時通過每輛車的策略進行干預。我們表明，即使在低滲透率下，在自動駕駛車隊中引入閘門員也能產生顯著的正外部性，從而提高系統安全性。

##### **TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi**
2502.04245v1 by Mohammed Amaan Dhamaskar, Rasika Ransing

India's rich cultural and linguistic diversity poses various challenges in
the domain of Natural Language Processing (NLP), particularly in Named Entity
Recognition (NER). NER is a NLP task that aims to identify and classify tokens
into different entity groups like Person, Location, Organization, Number, etc.
This makes NER very useful for downstream tasks like context-aware
anonymization. This paper details our work to build a multilingual NER model
for the three most spoken languages in India - Hindi, Bengali & Marathi. We
train a custom transformer model and fine tune a few pretrained models,
achieving an F1 Score of 92.11 for a total of 6 entity groups. Through this
paper, we aim to introduce a single model to perform NER and significantly
reduce the inconsistencies in entity groups and tag names, across the three
languages.

摘要：印度豐富的文化和語言多樣性在自然語言處理 (NLP) 領域中帶來各種挑戰，特別是在命名實體識別 (NER) 中。NER 是一項 NLP 任務，旨在識別並將詞彙分類到不同的實體群組中，例如人名、地點、組織、數字等。這使得 NER 對於下游任務（例如基於上下文的匿名化）非常有用。本文詳細介紹了我們為印度三種最廣泛使用的語言（印地語、孟加拉語和馬拉地語）建立多語言 NER 模型的工作。我們訓練了一個自訂的轉換器模型，並微調了幾個預訓練模型，針對總共 6 個實體群組達到了 92.11 的 F1 分數。透過本文，我們旨在引入一個單一模型來執行 NER，並顯著減少三種語言中實體群組和標籤名稱的不一致性。

##### **A Theoretical Framework for Data Efficient Multi-Source Transfer Learning Based on Cramér-Rao Bound**
2502.04242v1 by Qingyue Zhang, Haohao Fu, Guanbo Huang, Yaoyuan Liang, Chang Chu, Tianren Peng, Yanru Wu, Qi Li, Yang Li, Shao-Lun Huang

Multi-source transfer learning provides an effective solution to data
scarcity in real-world supervised learning scenarios by leveraging multiple
source tasks. In this field, existing works typically use all available samples
from sources in training, which constrains their training efficiency and may
lead to suboptimal results. To address this, we propose a theoretical framework
that answers the question: what is the optimal quantity of source samples
needed from each source task to jointly train the target model? Specifically,
we introduce a generalization error measure that aligns with cross-entropy
loss, and minimize it based on the Cram\'er-Rao Bound to determine the optimal
transfer quantity for each source task. Additionally, we develop an
architecture-agnostic and data-efficient algorithm OTQMS to implement our
theoretical results for training deep multi-source transfer learning models.
Experimental studies on diverse architectures and two real-world benchmark
datasets show that our proposed algorithm significantly outperforms
state-of-the-art approaches in both accuracy and data efficiency. The code and
supplementary materials are available in
https://anonymous.4open.science/r/Materials.

摘要：多源迁移学习通过利用多源任务为现实世界中的监督学习场景中的数据稀缺性提供了有效的解决方案。在这个领域，现有工作通常在训练中使用来自源的所有可用样本，这限制了它们的训练效率，并且可能导致次优结果。为了解决这个问题，我们提出了一个理论框架来回答以下问题：联合训练目标模型时，需要从每个源任务中获取多少最优数量的源样本？具体来说，我们引入了一个与交叉熵损失一致的泛化误差度量，并基于 Cramér-Rao 界对其进行最小化，以确定每个源任务的最优传输量。此外，我们开发了一个与架构无关且数据高效的算法 OTQMS，以实现我们的理论结果，用于训练深度多源迁移学习模型。在各种架构和两个真实世界基准数据集上的实验研究表明，我们提出的算法在准确性和数据效率方面都明显优于最先进的方法。代码和补充材料可在 https://anonymous.4open.science/r/Materials 中获得。

##### **MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion**
2502.04235v1 by Xintong Hao, Ke Shen, Chenggang Li

Despite the remarkable capabilities of large language models across various
tasks, their continued scaling faces a critical challenge: the scarcity of
high-quality pretraining data. While model architectures continue to evolve,
the natural language data struggles to scale up. To tackle this bottleneck, we
propose \textbf{MA}ssive \textbf{G}enre-\textbf{A}udience~(MAGA) reformulation
method, which systematic synthesizes diverse, contextually-rich pretraining
data from existing corpus. This work makes three main contributions: (1) We
propose MAGA reformulation method, a lightweight and scalable approach for
pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We
evaluate MAGACorpus with different data budget scaling strategies,
demonstrating consistent improvements across various model sizes (134M-13B),
establishing the necessity for next-generation large-scale synthetic
pretraining language models. (3) Through comprehensive analysis, we investigate
prompt engineering's impact on synthetic training collapse and reveal
limitations in conventional collapse detection metrics using validation losses.
Our work shows that MAGA can substantially expand training datasets while
maintaining quality, offering a reliably pathway for scaling models beyond data
limitations.

摘要：儘管大型語言模型在各種任務中展現出卓越的能力，但持續擴充卻面臨一項嚴峻的挑戰：缺乏高品質的預訓練資料。雖然模型架構持續演進，但自然語言資料卻難以擴充。為了解決這個瓶頸，我們提出**M**assive **G**enre-**A**udience~(MAGA) 改寫方法，有系統地綜合來自現有語料庫的多元且脈絡豐富的預訓練資料。這項工作有三個主要貢獻：(1) 我們提出 MAGA 改寫方法，這是一種輕量且可擴充的預訓練語料庫擴充方法，並建立一個 770B 個 token 的 MAGACorpus。(2) 我們使用不同的資料預算擴充策略評估 MAGACorpus，證明各種模型大小(134M-13B) 都持續提升，確立了下一代大規模合成預訓練語言模型的必要性。(3) 透過全面的分析，我們探討提示工程對合成訓練崩潰的影響，並揭露使用驗證損失進行傳統崩潰檢測指標的限制。我們的研究顯示，MAGA 能夠大幅擴充訓練資料集，同時維持品質，提供一種可靠的路徑，讓模型在超越資料限制的同時進行擴充。

##### **A Classification System Approach in Predicting Chinese Censorship**
2502.04234v1 by Matt Prodani, Tianchu Ze, Yushen Hu

This paper is dedicated to using a classifier to predict whether a Weibo post
would be censored under the Chinese internet. Through randomized sampling from
\citeauthor{Fu2021} and Chinese tokenizing strategies, we constructed a cleaned
Chinese phrase dataset with binary censorship markings. Utilizing various
probability-based information retrieval methods on the data, we were able to
derive 4 logistic regression models for classification. Furthermore, we
experimented with pre-trained transformers to perform similar classification
tasks. After evaluating both the macro-F1 and ROC-AUC metrics, we concluded
that the Fined-Tuned BERT model exceeds other strategies in performance.

摘要：本文致力于使用分类器来预测微博帖子是否会在中国的互联网上被审查。通过从\citeauthor{Fu2021}和中文标记化策略中随机抽样，我们构建了一个带有二进制审查标记的干净中文短语数据集。利用各种基于概率的信息检索方法对数据进行处理，我们能够导出 4 个用于分类的逻辑回归模型。此外，我们对预训练的转换器进行了实验，以执行类似的分类任务。在评估了宏观 F1 和 ROC-AUC 指标后，我们得出结论，微调后的 BERT 模型在性能上优于其他策略。

##### **Dark Distillation: Backdooring Distilled Datasets without Accessing Raw Data**
2502.04229v1 by Ziyuan Yang, Ming Yan, Yi Zhang, Joey Tianyi Zhou

Dataset distillation (DD) enhances training efficiency and reduces bandwidth
by condensing large datasets into smaller synthetic ones. It enables models to
achieve performance comparable to those trained on the raw full dataset and has
become a widely adopted method for data sharing. However, security concerns in
DD remain underexplored. Existing studies typically assume that malicious
behavior originates from dataset owners during the initial distillation
process, where backdoors are injected into raw datasets. In contrast, this work
is the first to address a more realistic and concerning threat: attackers may
intercept the dataset distribution process, inject backdoors into the distilled
datasets, and redistribute them to users. While distilled datasets were
previously considered resistant to backdoor attacks, we demonstrate that they
remain vulnerable to such attacks. Furthermore, we show that attackers do not
even require access to any raw data to inject the backdoors successfully.
Specifically, our approach reconstructs conceptual archetypes for each class
from the model trained on the distilled dataset. Backdoors are then injected
into these archetypes to update the distilled dataset. Moreover, we ensure the
updated dataset not only retains the backdoor but also preserves the original
optimization trajectory, thus maintaining the knowledge of the raw dataset. To
achieve this, a hybrid loss is designed to integrate backdoor information along
the benign optimization trajectory, ensuring that previously learned
information is not forgotten. Extensive experiments demonstrate that distilled
datasets are highly vulnerable to backdoor attacks, with risks pervasive across
various raw datasets, distillation methods, and downstream training strategies.
Moreover, our attack method is efficient, capable of synthesizing a malicious
distilled dataset in under one minute in certain cases.

摘要：<paragraph>資料集蒸餾 (DD) 透過將大型資料集濃縮成較小的合成資料集，來提升訓練效率並減少頻寬。它讓模型能夠達到與在原始完整資料集上訓練相媲美的效能，並已成為資料共享的廣泛採用方法。然而，DD 中的安全疑慮仍未獲得充分探討。現有研究通常假設惡意行為源自於資料集擁有者在最初的蒸餾過程中，將後門注入原始資料集。相反地，這項工作首次探討一個更實際且令人擔憂的威脅：攻擊者可能攔截資料集分發流程，將後門注入蒸餾資料集，並將其重新分發給使用者。儘管蒸餾資料集先前被認為能抵抗後門攻擊，但我們證明它們仍然容易受到此類攻擊。此外，我們表明攻擊者甚至不需要存取任何原始資料就能成功注入後門。具體來說，我們的做法是根據在蒸餾資料集上訓練的模型，為每個類別重建概念原型。然後將後門注入這些原型，以更新蒸餾資料集。此外，我們確保更新後的資料集不僅保留後門，還保留原始最佳化軌跡，從而維護原始資料集的知識。為達成此目的，我們設計了一個混合損失函數，以將後門資訊整合到良性最佳化軌跡中，確保先前學習的資訊不會被遺忘。大量的實驗證明蒸餾資料集極易受到後門攻擊，風險普遍存在於各種原始資料集、蒸餾方法和下游訓練策略中。此外，我們的攻擊方法很有效率，在某些情況下能夠在不到一分鐘的時間內合成惡意的蒸餾資料集。</paragraph>

##### **NLP-Based .NET CLR Event Logs Analyzer**
2502.04219v1 by Maxim Stavtsev, Sergey Shershakov

In this paper, we present a tool for analyzing .NET CLR event logs based on a
novel method inspired by Natural Language Processing (NLP) approach. Our
research addresses the growing need for effective monitoring and optimization
of software systems through detailed event log analysis. We utilize a
BERT-based architecture with an enhanced tokenization process customized to
event logs. The tool, developed using Python, its libraries, and an SQLite
database, allows both conducting experiments for academic purposes and
efficiently solving industry-emerging tasks. Our experiments demonstrate the
efficacy of our approach in compressing event sequences, detecting recurring
patterns, and identifying anomalies. The trained model shows promising results,
with a high accuracy rate in anomaly detection, which demonstrates the
potential of NLP methods to improve the reliability and stability of software
systems.

摘要：在本文中，我们展示了一個基於自然語言處理 (NLP) 方法的創新方法，用於分析 .NET CLR 事件日誌的工具。我們的研究針對通過詳細事件日誌分析對軟體系統進行有效監控和最佳化的日益增長的需求。我們利用了一個基於 BERT 的架構，並使用了一個針對事件日誌進行了客製化的增強型分詞處理程序。這個工具使用 Python、其函式庫和一個 SQLite 資料庫開發，允許進行學術目的的實驗，並有效解決新興的產業任務。我們的實驗證明了我們的方法在壓縮事件序列、偵測重複模式和識別異常方面的效力。訓練好的模型顯示出有希望的結果，在異常偵測中具有很高的準確度，這證明了 NLP 方法改善軟體系統可靠性和穩定性的潛力。

##### **Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data**
2502.04218v1 by Laura Biester

Large Language Models (LLMs) have been shown to be biased in prior work, as
they generate text that is in line with stereotypical views of the world or
that is not representative of the viewpoints and values of historically
marginalized demographic groups. In this work, we propose using data from
parallel men's and women's events at the Olympic Games to investigate different
forms of gender bias in language models. We define three metrics to measure
bias, and find that models are consistently biased against women when the
gender is ambiguous in the prompt. In this case, the model frequently retrieves
only the results of the men's event with or without acknowledging them as such,
revealing pervasive gender bias in LLMs in the context of athletics.

摘要：大型語言模型 (LLM) 已被證明在先前的研究中存在偏差，因為它們產生的文字符合對世界的刻板印象，或者不能代表歷史上被邊緣化的群體的觀點和價值觀。在這項研究中，我們提議使用奧運會男子和女子平行賽事的數據來探討語言模型中不同形式的性別偏見。我們定義了三個指標來衡量偏見，並發現當提示中的性別不明確時，模型始終對女性有偏見。在這種情況下，模型經常只檢索男子賽事的結果，而不管是否承認它們，揭示了 LLM 在田徑背景下的普遍性別偏見。

##### **Algorithmic causal structure emerging through compression**
2502.04210v1 by Liang Wendong, Simon Buchholz, Bernhard Schölkopf

We explore the relationship between causality, symmetry, and compression. We
build on and generalize the known connection between learning and compression
to a setting where causal models are not identifiable. We propose a framework
where causality emerges as a consequence of compressing data across multiple
environments. We define algorithmic causality as an alternative definition of
causality when traditional assumptions for causal identifiability do not hold.
We demonstrate how algorithmic causal and symmetric structures can emerge from
minimizing upper bounds on Kolmogorov complexity, without knowledge of
intervention targets. We hypothesize that these insights may also provide a
novel perspective on the emergence of causality in machine learning models,
such as large language models, where causal relationships may not be explicitly
identifiable.

摘要：我們探討因果關係、對稱性與壓縮之間的關係。我們建立並概括了學習與壓縮之間已知的關聯，並運用於因果模型不可識別的設定中。我們提出一個架構，其中因果關係會隨著跨多個環境壓縮資料而浮現。當因果可識別性的傳統假設不成立時，我們將演算法因果關係定義為因果關係的替代定義。我們展示演算法因果關係與對稱結構如何從最小化 Kolmogorov 複雜度上限中浮現，而無需知道介入目標。我們假設這些見解也可能提供關於機器學習模型中因果關係浮現的新觀點，例如大型語言模型，其中因果關係可能無法明確識別。

##### **The Best Instruction-Tuning Data are Those That Fit**
2502.04194v1 by Dylan Zhang, Qirun Dai, Hao Peng

High-quality supervised fine-tuning (SFT) data are crucial for eliciting
strong capabilities from pretrained large language models (LLMs). Typically,
instructions are paired with multiple responses sampled from other LLMs, which
are often out of the distribution of the target model to be fine-tuned. This,
at scale, can lead to diminishing returns and even hurt the models' performance
and robustness. We propose **GRAPE**, a novel SFT framework that accounts for
the unique characteristics of the target model. For each instruction, it
gathers responses from various LLMs and selects the one with the highest
probability measured by the target model, indicating that it aligns most
closely with the target model's pretrained distribution; it then proceeds with
standard SFT training.
  We first evaluate GRAPE with a controlled experiment, where we sample various
solutions for each question in UltraInteract from multiple models and fine-tune
commonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on
GRAPE-selected data. GRAPE significantly outperforms strong baselines,
including distilling from the strongest model with an absolute gain of up to
13.8%, averaged across benchmarks, and training on 3x more data with a maximum
performance improvement of 17.3%. GRAPE's strong performance generalizes to
realistic settings. We experiment with the post-training data used for Tulu3
and Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data
by 6.1% and a state-of-the-art data selection approach by 3% on average
performance. Remarkably, using 1/3 of the data and half the number of epochs,
GRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.

摘要：高品質監督式微調 (SFT) 資料對於引發預訓練大型語言模型 (LLM) 的強大功能至關重要。通常，指令會與從其他 LLM 採樣的各種回應配對，這些回應通常超出要微調的目標模型的分布。從規模來看，這可能會導致報酬遞減，甚至損害模型的效能和穩健性。我們提出 **GRAPE**，一個新的 SFT 框架，它考慮了目標模型的獨特特徵。對於每個指令，它會從各種 LLM 收集回應，並選擇目標模型測量機率最高的那個，表示它與目標模型的預訓練分布最接近；然後進行標準的 SFT 訓練。
我們首先使用受控實驗評估 GRAPE，在該實驗中，我們從多個模型中為 UltraInteract 中的每個問題採樣各種解，並在 GRAPE 選擇的資料上微調常用的 LLM，例如 LLaMA3.1-8B、Mistral-7B 和 Qwen2.5-7B。GRAPE 明顯優於強大的基準，包括從最強的模型中萃取，絕對增益高達 13.8%，在基準上平均計算，並在資料多 3 倍的情況下進行訓練，效能最高提升 17.3%。GRAPE 的強大效能推廣到實際設定。我們實驗了用於 Tulu3 和 Olmo-2 的訓練後資料。GRAPE 優於強大的基準，這些基準在資料多 4.5 倍的情況下訓練，平均效能高出 6.1%，而最先進的資料選擇方法高出 3%。值得注意的是，使用 1/3 的資料和一半的時代，GRAPE 使 LLaMA3.1-8B 超越了 Tulu3-SFT 的效能，高出 3.5%。

##### **Multi-agent Architecture Search via Agentic Supernet**
2502.04180v1 by Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, Xiang Wang

Large Language Model (LLM)-empowered multi-agent systems extend the cognitive
boundaries of individual agents through disciplined collaboration and
interaction, while constructing these systems often requires labor-intensive
manual designs. Despite the availability of methods to automate the design of
agentic workflows, they typically seek to identify a static, complex,
one-size-fits-all system, which, however, fails to dynamically allocate
inference resources based on the difficulty and domain of each query. To
address this challenge, we shift away from the pursuit of a monolithic agentic
system, instead optimizing the \textbf{agentic supernet}, a probabilistic and
continuous distribution of agentic architectures. We introduce MaAS, an
automated framework that samples query-dependent agentic systems from the
supernet, delivering high-quality solutions and tailored resource allocation
(\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation
across six benchmarks demonstrates that MaAS \textbf{(I)} requires only
$6\sim45\%$ of the inference costs of existing handcrafted or automated
multi-agent systems, \textbf{(II)} surpasses them by $0.54\%\sim11.82\%$, and
\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone
transferability.

摘要：大型語言模型（LLM）賦能的多主體系統透過有紀律的協作和互動來擴展個別主體的認知界限，而建構這些系統通常需要大量人工設計。儘管有自動化主體工作流程設計的方法，但它們通常尋求識別一個靜態、複雜、一體適用的系統，然而，這無法根據每個查詢的難度和領域動態分配推論資源。為了應對這個挑戰，我們不再追求單一的主體系統，而是最佳化「主體超網路」，一個機率性的主體架構連續分佈。我們引進 MaAS，一個從超網路中抽樣與查詢相關的主體系統的自動化架構，提供高品質的解決方案和客製化的資源分配（例如，LLM 呼叫、工具呼叫、代幣成本）。在六個基準上的全面評估證明，MaAS （一）僅需現有手工或自動化多主體系統 $6\sim45\%$ 的推論成本，（二）超越它們 $0.54\%\sim11.82\%$，以及（三）享有優異的跨資料集和跨 LLM 骨幹的可移植性。

##### **Lexical Substitution is not Synonym Substitution: On the Importance of Producing Contextually Relevant Word Substitutes**
2502.04173v1 by Juraj Vladika, Stephen Meisenbacher, Florian Matthes

Lexical Substitution is the task of replacing a single word in a sentence
with a similar one. This should ideally be one that is not necessarily only
synonymous, but also fits well into the surrounding context of the target word,
while preserving the sentence's grammatical structure. Recent advances in
Lexical Substitution have leveraged the masked token prediction task of
Pre-trained Language Models to generate replacements for a given word in a
sentence. With this technique, we introduce ConCat, a simple augmented approach
which utilizes the original sentence to bolster contextual information sent to
the model. Compared to existing approaches, it proves to be very effective in
guiding the model to make contextually relevant predictions for the target
word. Our study includes a quantitative evaluation, measured via sentence
similarity and task performance. In addition, we conduct a qualitative human
analysis to validate that users prefer the substitutions proposed by our
method, as opposed to previous methods. Finally, we test our approach on the
prevailing benchmark for Lexical Substitution, CoInCo, revealing potential
pitfalls of the benchmark. These insights serve as the foundation for a
critical discussion on the way in which Lexical Substitution is evaluated.

摘要：詞彙替換是將句子中的一個單字替換為一個類似的單字。理想情況下，這個單字不一定是同義詞，但也能很好地融入目標單字的周圍語境，同時保留句子的語法結構。詞彙替換的最新進展利用了預訓練語言模型的遮罩符號預測任務，為句子中的特定單字產生替換。使用此技術，我們引入了 ConCat，這是一種簡單的擴充方法，利用原始句子來加強傳送到模型的上下文資訊。與現有方法相比，它被證明在引導模型對目標單字進行與上下文相關的預測方面非常有效。我們的研究包括定量評估，透過句子相似度和任務效能來衡量。此外，我們進行了定性的人類分析，以驗證使用者偏好我們的方法提出的替換，而不是以前的方法。最後，我們在詞彙替換的流行基準 CoInCo 上測試了我們的方法，揭示了基準的潛在缺陷。這些見解作為對詞彙替換評估方式進行批判性討論的基礎。

##### **UltraIF: Advancing Instruction Following from the Wild**
2502.04153v1 by Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, Baobao Chang

Instruction-following made modern large language models (LLMs) helpful
assistants. However, the key to taming LLMs on complex instructions remains
mysterious, for that there are huge gaps between models trained by open-source
community and those trained by leading companies. To bridge the gap, we propose
a simple and scalable approach UltraIF for building LLMs that can follow
complex instructions with open-source data. UltraIF first decomposes real-world
user prompts into simpler queries, constraints, and corresponding evaluation
questions for the constraints. Then, we train an UltraComposer to compose
constraint-associated prompts with evaluation questions. This prompt composer
allows us to synthesize complicated instructions as well as filter responses
with evaluation questions. In our experiment, for the first time, we
successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5
instruction-following benchmarks without any benchmark information, using only
8B model as response generator and evaluator. The aligned model also achieved
competitive scores on other benchmarks. Moreover, we also show that UltraIF
could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating
broader use cases for the method. Our code will be available at
https://github.com/kkk-an/UltraIF.

摘要：<paragraph>遵循指令使现代大型语言模型（LLM）成为有用的助手。然而，在复杂指令上驯服 LLM 的关键仍然是个谜，因为开源社区训练的模型和领先公司训练的模型之间存在巨大差距。为了弥合这一差距，我们提出了一种简单且可扩展的方法 UltraIF，用于构建能够遵循开源数据复杂指令的 LLM。UltraIF 首先将现实世界中的用户提示分解为更简单的查询、约束和针对约束的相应评估问题。然后，我们训练一个 UltraComposer 来编写带有评估问题的约束相关提示。这个提示编写器使我们能够综合复杂的指令以及使用评估问题过滤响应。在我们的实验中，我们首次成功调整 LLaMA-3.1-8B-Base，使其在 5 个指令遵循基准上赶上其指令版本，而无需任何基准信息，仅使用 8B 模型作为响应生成器和评估器。调整后的模型在其他基准上也取得了有竞争力的分数。此外，我们还表明 UltraIF 可以通过自调整进一步改进 LLaMA-3.1-8B-Instruct，从而激发该方法的更广泛用例。我们的代码将在 https://github.com/kkk-an/UltraIF 上提供。</paragraph>

##### **Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using PDEs**
2502.04140v1 by Jost Arndt, Utku Isil, Michael Detzel, Wojciech Samek, Jackie Ma

Many physical processes can be expressed through partial differential
equations (PDEs). Real-world measurements of such processes are often collected
at irregularly distributed points in space, which can be effectively
represented as graphs; however, there are currently only a few existing
datasets. Our work aims to make advancements in the field of PDE-modeling
accessible to the temporal graph machine learning community, while addressing
the data scarcity problem, by creating and utilizing datasets based on PDEs. In
this work, we create and use synthetic datasets based on PDEs to support
spatio-temporal graph modeling in machine learning for different applications.
More precisely, we showcase three equations to model different types of
disasters and hazards in the fields of epidemiology, atmospheric particles, and
tsunami waves. Further, we show how such created datasets can be used by
benchmarking several machine learning models on the epidemiological dataset.
Additionally, we show how pre-training on this dataset can improve model
performance on real-world epidemiological data. The presented methods enable
others to create datasets and benchmarks customized to individual requirements.
The source code for our methodology and the three created datasets can be found
on https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs.

摘要：許多物理程序可以用偏微分方程式 (PDE) 表示。此類程序的真實世界測量通常收集於空間中不規則分布的點，這可以有效地表示為圖形；然而，目前僅有少數現有資料集。我們的研究旨在讓時序圖形機器學習社群能夠進展 PDE 建模領域，同時透過建立並使用基於 PDE 的資料集來解決資料稀少的問題。在這項研究中，我們建立並使用基於 PDE 的合成資料集，以支援機器學習中不同應用程式的時空圖形建模。更精確地說，我們展示了三個方程式，以模擬流行病學、大氣粒子與海嘯波等領域中不同類型的災難與危害。此外，我們展示了如何透過在流行病學資料集上對多個機器學習模型進行基準測試，來使用此類建立的資料集。此外，我們展示了如何透過在此資料集上進行預訓練，來改善模型在真實世界流行病學資料上的效能。所提出的方法讓其他人能夠建立自訂於個別需求的資料集與基準。我們的方法與三個建立的資料集的原始程式碼可以在 https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs 找到。

##### **The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs**
2502.04134v1 by Bryan Guan, Tanya Roosta, Peyman Passban, Mehdi Rezagholizadeh

As large language models (LLMs) become integral to diverse applications,
ensuring their reliability under varying input conditions is crucial. One key
issue affecting this reliability is order sensitivity, wherein slight
variations in input arrangement can lead to inconsistent or biased outputs.
Although recent advances have reduced this sensitivity, the problem remains
unresolved. This paper investigates the extent of order sensitivity in
closed-source LLMs by conducting experiments across multiple tasks, including
paraphrasing, relevance judgment, and multiple-choice questions. Our results
show that input order significantly affects performance across tasks, with
shuffled inputs leading to measurable declines in output accuracy. Few-shot
prompting demonstrates mixed effectiveness and offers partial mitigation,
however, fails to fully resolve the problem. These findings highlight
persistent risks, particularly in high-stakes applications, and point to the
need for more robust LLMs or improved input-handling techniques in future
development.

摘要：隨著大型語言模型 (LLM) 成為各種應用程式中不可或缺的一部分，確保它們在不同的輸入條件下都能可靠運作至關重要。影響這個可靠性的主要問題之一是順序敏感性，其中輸入排列的微小變化可能導致不一致或有偏差的輸出。儘管最近的進展已經降低了這種敏感性，但這個問題仍未得到解決。本文透過在多項任務中進行實驗來探討閉源 LLM 中順序敏感性的程度，包括改寫、相關性判斷和多選題。我們的結果顯示，輸入順序會顯著影響各項任務的效能，輸入順序打亂會導致輸出準確度明顯下降。少次提示展示出不同的效果，並提供部分緩解，但無法完全解決問題。這些發現突顯了持續存在的風險，特別是在高風險應用中，並指出未來開發中需要更強大的 LLM 或改進的輸入處理技術。

##### **Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis**
2502.04128v1 by Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi DAI, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue

Recent advances in text-based large language models (LLMs), particularly in
the GPT series and the o1 model, have demonstrated the effectiveness of scaling
both training-time and inference-time compute. However, current
state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring
separate models (e.g., diffusion models after LLM), complicating the decision
of whether to scale a particular model during training or testing. This work
makes the following contributions: First, we explore the scaling of train-time
and inference-time compute for speech synthesis. Second, we propose a simple
framework Llasa for speech synthesis that employs a single-layer vector
quantizer (VQ) codec and a single Transformer architecture to fully align with
standard LLMs such as Llama. Our experiments reveal that scaling train-time
compute for Llasa consistently improves the naturalness of synthesized speech
and enables the generation of more complex and accurate prosody patterns.
Furthermore, from the perspective of scaling inference-time compute, we employ
speech understanding models as verifiers during the search, finding that
scaling inference-time compute shifts the sampling modes toward the preferences
of specific verifiers, thereby improving emotional expressiveness, timbre
consistency, and content accuracy. In addition, we released the checkpoint and
training code for our TTS model (1B, 3B, 8B) and codec model publicly
available.

摘要：最近在以 GPT 系列和 o1 模型為首的文本大型語言模型 (LLM) 上的進展，已證明了擴充訓練時間和推論時間運算的有效性。然而，目前利用 LLM 的最先進 TTS 系統通常是多階段的，需要個別的模型（例如，LLM 之後的擴散模型），這使得在訓練或測試期間擴充特定模型的決定變得複雜。本研究做出了以下貢獻：首先，我們探討了語音合成的訓練時間和推論時間運算的擴充。其次，我們提出了 Llasa 的簡單架構，用於語音合成，採用單層向量量化器 (VQ) 編解碼器和單一 Transformer 架構，以完全符合 Llama 等標準 LLM。我們的實驗表明，擴充 Llasa 的訓練時間運算持續改善了合成語音的自然性，並能產生更複雜且準確的韻律模式。此外，從擴充推論時間運算的角度來看，我們在搜尋期間採用語音理解模型作為驗證器，發現擴充推論時間運算將抽樣模式轉移到特定驗證器的偏好，從而改善情緒表達力、音色一致性和內容準確性。此外，我們公開發布了我們的 TTS 模型 (1B、3B、8B) 和編解碼器模型的檢查點和訓練程式碼。

##### **VTutor: An Open-Source SDK for Generative AI-Powered Animated Pedagogical Agents with Multi-Media Output**
2502.04103v1 by Eason Chen, Chengyu Lin, Xinyi Tang, Aprille Xi, Canwen Wang, Jionghao Lin, Kenneth R Koedinger

The rapid evolution of large language models (LLMs) has transformed
human-computer interaction (HCI), but the interaction with LLMs is currently
mainly focused on text-based interactions, while other multi-model approaches
remain under-explored. This paper introduces VTutor, an open-source Software
Development Kit (SDK) that combines generative AI with advanced animation
technologies to create engaging, adaptable, and realistic APAs for human-AI
multi-media interactions. VTutor leverages LLMs for real-time personalized
feedback, advanced lip synchronization for natural speech alignment, and WebGL
rendering for seamless web integration. Supporting various 2D and 3D character
models, VTutor enables researchers and developers to design emotionally
resonant, contextually adaptive learning agents. This toolkit enhances learner
engagement, feedback receptivity, and human-AI interaction while promoting
trustworthy AI principles in education. VTutor sets a new standard for
next-generation APAs, offering an accessible, scalable solution for fostering
meaningful and immersive human-AI interaction experiences. The VTutor project
is open-sourced and welcomes community-driven contributions and showcases.

摘要：大型語言模型 (LLM) 的快速演進轉變了人機互動 (HCI)，但與 LLM 的互動目前主要集中在基於文字的互動上，而其他多模式方法仍未得到充分探索。本文介紹了 VTutor，這是一個開源軟體開發套件 (SDK)，它將生成式 AI 與先進的動畫技術相結合，為人機多媒體互動創造了引人入勝、適應性強且逼真的 APA。VTutor 利用 LLM 進行即時個性化回饋、先進的唇形同步以實現自然的語音對齊，以及 WebGL 渲染以實現無縫的網路整合。VTutor 支援各種 2D 和 3D 角色模型，使研究人員和開發人員能夠設計出情感共鳴且適應脈絡的學習代理。此工具包增強了學習者的參與度、回饋接受度和人機互動，同時在教育中宣導值得信賴的 AI 原則。VTutor 為下一代 APA 設定了新標準，提供了一個可存取、可擴充的解決方案，以促進有意義且身臨其境的 AI 人機互動體驗。VTutor 這個專案是開源的，歡迎社群驅動的貢獻和展示。

##### **Efficient Few-Shot Continual Learning in Vision-Language Models**
2502.04098v1 by Aristeidis Panos, Rahaf Aljundi, Daniel Olmeda Reino, Richard E. Turner

Vision-language models (VLMs) excel in tasks such as visual question
answering and image captioning. However, VLMs are often limited by their use of
pretrained image encoders, like CLIP, leading to image understanding errors
that hinder overall performance. On top of that, real-world applications often
require the model to be continuously adapted as new and often limited data
continuously arrive. To address this, we propose LoRSU (Low-Rank Adaptation
with Structured Updates), a robust and computationally efficient method for
selectively updating image encoders within VLMs. LoRSU introduces structured
and localized parameter updates, effectively correcting performance on
previously error-prone data while preserving the model's general robustness.
Our approach leverages theoretical insights to identify and update only the
most critical parameters, achieving significant resource efficiency.
Specifically, we demonstrate that LoRSU reduces computational overhead by over
25x compared to full VLM updates, without sacrificing performance. Experimental
results on VQA tasks in the few-shot continual learning setting, validate
LoRSU's scalability, efficiency, and effectiveness, making it a compelling
solution for image encoder adaptation in resource-constrained environments.

摘要：視覺語言模型 (VLM) 在視覺問題解答和影像標題等任務中表現出色。然而，VLM 經常受到其使用預訓練影像編碼器（例如 CLIP）的限制，導致影像理解錯誤，進而阻礙整體效能。最重要的是，現實世界的應用程式通常要求模型持續適應，因為新的且經常有限的資料會持續湧入。為了解決這個問題，我們提出 LoRSU（結構化更新的低秩適應），這是一種強健且運算效率高的方式，用於選擇性地更新 VLM 中的影像編碼器。LoRSU 引入結構化且局部化的參數更新，有效地修正先前容易出錯資料的效能，同時保留模型的整體強健性。我們的做法利用理論見解來識別和更新最重要的參數，進而達成顯著的資源效率。具體來說，我們證明 LoRSU 將運算開銷減少了 25 倍以上（與完整的 VLM 更新相比），而不會犧牲效能。在少次學習設定中的 VQA 任務上進行的實驗結果驗證了 LoRSU 的可擴充性、效率和有效性，使其成為資源受限環境中影像編碼器適應的引人注目的解決方案。

##### **LLMs to Support a Domain Specific Knowledge Assistant**
2502.04095v1 by Maria-Flavia Lovin

This work presents a custom approach to developing a domain specific
knowledge assistant for sustainability reporting using the International
Financial Reporting Standards (IFRS). In this domain, there is no publicly
available question-answer dataset, which has impeded the development of a
high-quality chatbot to support companies with IFRS reporting. The two key
contributions of this project therefore are:
  (1) A high-quality synthetic question-answer (QA) dataset based on IFRS
sustainability standards, created using a novel generation and evaluation
pipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse
QA pairs that address a wide spectrum of potential user queries in
sustainability reporting. Various LLM-based techniques are employed to create
the dataset, including chain-of-thought reasoning and few-shot prompting. A
custom evaluation framework is developed to assess question and answer quality
across multiple dimensions, including faithfulness, relevance, and domain
specificity. The dataset averages a score range of 8.16 out of 10 on these
metrics.
  (2) Two architectures for question-answering in the sustainability reporting
domain - a RAG pipeline and a fully LLM-based pipeline. The architectures are
developed by experimenting, fine-tuning, and training on the QA dataset. The
final pipelines feature an LLM fine-tuned on domain specific data and an
industry classification component to improve the handling of complex queries.
The RAG architecture achieves an accuracy of 85.32% on single-industry and
72.15% on cross-industry multiple-choice questions, outperforming the baseline
approach by 4.67 and 19.21 percentage points, respectively. The LLM-based
pipeline achieves an accuracy of 93.45% on single-industry and 80.30% on
cross-industry multiple-choice questions, an improvement of 12.80 and 27.36
percentage points over the baseline, respectively.

摘要：<paragraph>這項工作展示了一種自訂方法，使用國際財務報導準則 (IFRS) 來開發永續報導的特定領域知識助理。在這個領域中，沒有公開可用的問答資料集，這阻礙了開發高品質的聊天機器人來支援公司進行 IFRS 報導。因此，此專案的兩個主要貢獻為：
  (1) 一個基於 IFRS 永續性標準的高品質合成問答 (QA) 資料集，利用大型語言模型 (LLM) 的創新生成和評估管道來建立。這包含 1,063 個不同的 QA 對，可解決永續報導中廣泛的潛在使用者查詢。採用各種基於 LLM 的技術來建立資料集，包括思考鏈推理和少次提示。開發了一個自訂評估架構來評估問題和答案品質，涵蓋多個面向，包括忠實度、相關性和領域特定性。資料集在這些指標上的平均分數範圍為 10 分中的 8.16 分。
  (2) 永續報導領域中問答的兩個架構 - RAG 管道和完全基於 LLM 的管道。這些架構是透過在 QA 資料集上進行實驗、微調和訓練來開發的。最終的管道具有一個針對特定領域資料微調的 LLM，以及一個產業分類元件，用於改善複雜查詢的處理。RAG 架構在單一產業上達到 85.32% 的準確度，在跨產業多重選擇題上達到 72.15% 的準確度，分別比基準方法高出 4.67 和 19.21 個百分點。基於 LLM 的管道在單一產業上達到 93.45% 的準確度，在跨產業多重選擇題上達到 80.30% 的準確度，分別比基準方法提高了 12.80 和 27.36 個百分點。</paragraph>

##### **Automatic quantification of breast cancer biomarkers from multiple 18F-FDG PET image segmentation**
2502.04083v1 by Tewele W. Tareke, Neree Payan, Alexandre Cochet, Laurent Arnould, Benoit Presles, Jean-Marc Vrigneaud, Fabrice Meriaudeau, Alain Lalande

Neoadjuvant chemotherapy (NAC) has become a standard clinical practice for
tumor downsizing in breast cancer with 18F-FDG Positron Emission Tomography
(PET). Our work aims to leverage PET imaging for the segmentation of breast
lesions. The focus is on developing an automated system that accurately
segments primary tumor regions and extracts key biomarkers from these areas to
provide insights into the evolution of breast cancer following the first course
of NAC. 243 baseline 18F-FDG PET scans (PET_Bl) and 180 follow-up 18F-FDG PET
scans (PET_Fu) were acquired before and after the first course of NAC,
respectively. Firstly, a deep learning-based breast tumor segmentation method
was developed. The optimal baseline model (model trained on baseline exams) was
fine-tuned on 15 follow-up exams and adapted using active learning to segment
tumor areas in PET_Fu. The pipeline computes biomarkers such as maximum
standardized uptake value (SUVmax), metabolic tumor volume (MTV), and total
lesion glycolysis (TLG) to evaluate tumor evolution between PET_Fu and PET_Bl.
Quality control measures were employed to exclude aberrant outliers. The nnUNet
deep learning model outperformed in tumor segmentation on PET_Bl, achieved a
Dice similarity coefficient (DSC) of 0.89 and a Hausdorff distance (HD) of 3.52
mm. After fine-tuning, the model demonstrated a DSC of 0.78 and a HD of 4.95 mm
on PET_Fu exams. Biomarkers analysis revealed very strong correlations whatever
the biomarker between manually segmented and automatically predicted regions.
The significant average decrease of SUVmax, MTV and TLG were 5.22, 11.79 cm3
and 19.23 cm3, respectively. The presented approach demonstrates an automated
system for breast tumor segmentation from 18F-FDG PET. Thanks to the extracted
biomarkers, our method enables the automatic assessment of cancer progression.

摘要：新辅助化疗 (NAC) 已成为乳腺癌中采用 18F-FDG 正电子发射断层扫描 (PET) 进行肿瘤缩小的标准临床实践。我们的工作旨在利用 PET 影像分割乳腺病变。重点在于开发一个自动系统，该系统可以准确分割原发性肿瘤区域并从这些区域提取关键生物标记，以深入了解乳腺癌在第一疗程 NAC 后的演变。分别在第一疗程 NAC 之前和之后采集了 243 例基线 18F-FDG PET 扫描 (PET_Bl) 和 180 例随访 18F-FDG PET 扫描 (PET_Fu)。首先，开发了一种基于深度学习的乳腺肿瘤分割方法。对 15 例随访检查对最优基线模型（在基线检查中训练的模型）进行了微调，并使用主动学习对 PET_Fu 中的肿瘤区域进行了分割。该管道计算诸如最大标准摄取值 (SUVmax)、代谢肿瘤体积 (MTV) 和总病灶糖酵解 (TLG) 等生物标记，以评估 PET_Fu 和 PET_Bl 之间的肿瘤演变。采用质量控制措施来排除异常值。nnUNet 深度学习模型在 PET_Bl 上的肿瘤分割方面表现出色，达到 0.89 的 Dice 相似性系数 (DSC) 和 3.52 毫米的 Hausdorff 距离 (HD)。微调后，该模型在 PET_Fu 检查中显示出 0.78 的 DSC 和 4.95 毫米的 HD。无论手动分割区域和自动预测区域之间的生物标记如何，生物标记分析都显示出非常强的相关性。SUVmax、MTV 和 TLG 的平均显着下降分别为 5.22、11.79 cm3 和 19.23 cm3。所提出的方法展示了一个用于从 18F-FDG PET 分割乳腺肿瘤的自动化系统。由于提取了生物标记，我们的方法能够自动评估癌症进展。

##### **AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference**
2502.04077v1 by Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, Bin Li

With the development of large language models (LLMs), efficient inference
through Key-Value (KV) cache compression has attracted considerable attention,
especially for long-context generation. To compress the KV cache, recent
methods identify critical KV tokens through heuristic ranking with attention
scores. However, these methods often struggle to accurately determine critical
tokens as they neglect the \textit{temporal patterns} in attention scores,
resulting in a noticeable degradation in LLM performance. To address this
challenge, we propose AttentionPredictor, which is the first learning-based
critical token identification approach. Specifically, AttentionPredictor learns
a lightweight convolution model to capture spatiotemporal patterns and predict
the next-token attention score. An appealing feature of AttentionPredictor is
that it accurately predicts the attention score while consuming negligible
memory. Moreover, we propose a cross-token critical cache prefetching framework
that hides the token estimation time overhead to accelerate the decoding stage.
By retaining most of the attention information, AttentionPredictor achieves
16$\times$ KV cache compression with comparable LLM performance, significantly
outperforming the state-of-the-art.

摘要：隨著大型語言模型（LLM）的發展，透過 Key-Value（KV）快取壓縮進行的有效推論備受關注，特別是長語境生成。為了壓縮 KV 快取，近期方法透過注意力分數進行啟發式排序，來識別關鍵 KV 標記。然而，這些方法通常難以準確地判斷關鍵標記，因為它們忽略了注意力分數中的「時間模式」，導致 LLM 效能顯著下降。為了應對此挑戰，我們提出 AttentionPredictor，這是第一個基於學習的關鍵標記識別方法。具體來說，AttentionPredictor 學習一個輕量級卷積模型來擷取時空模式，並預測下一個標記的注意力分數。AttentionPredictor 的一個吸引人特點是它在消耗極少記憶體的情況下準確預測注意力分數。此外，我們提出一個跨標記關鍵快取預取架構，它隱藏了標記估計時間開銷，以加速解碼階段。透過保留大部分注意力資訊，AttentionPredictor 達到 16 倍 KV 快取壓縮，並具有相當的 LLM 效能，顯著優於現有技術。

##### **Controllable Emotion Generation with Emotion Vectors**
2502.04075v1 by Yurui Dong, Luozhijie Jin, Yao Yang, Bingjie Lu, Jiaxi Yang, Zhi Liu

In recent years, technologies based on large-scale language models (LLMs)
have made remarkable progress in many fields, especially in customer service,
content creation, and embodied intelligence, showing broad application
potential. However, The LLM's ability to express emotions with proper tone,
timing, and in both direct and indirect forms is still insufficient but
significant. Few works have studied on how to build the controlable emotional
expression capability of LLMs. In this work, we propose a method for emotion
expression output by LLMs, which is universal, highly flexible, and well
controllable proved with the extensive experiments and verifications. This
method has broad application prospects in fields involving emotions output by
LLMs, such as intelligent customer service, literary creation, and home
companion robots. The extensive experiments on various LLMs with different
model-scales and architectures prove the versatility and the effectiveness of
the proposed method.

摘要：近年來，基於大型語言模型 (LLM) 的技術在許多領域取得顯著進展，特別是在客服、內容創作和具身智慧方面，展現廣泛的應用潛力。然而，LLM 以適當語氣、時機，直接和間接表達情緒的能力仍有不足，但卻至關重要。鮮少有研究探討如何建構 LLM 可控的情緒表達能力。在本文中，我們提出一個 LLM 情緒表達輸出的方法，此方法通用、高度靈活且可控性佳，並透過廣泛的實驗和驗證加以證明。此方法在涉及 LLM 情緒輸出的領域，例如智慧客服、文學創作和居家陪伴機器人，具有廣泛的應用前景。針對不同模型規模和架構的各類 LLM 進行的廣泛實驗，證明了所提方法的多功能性和有效性。

##### **Predicting Large Language Model Capabilities on Closed-Book QA Tasks Using Only Information Available Prior to Training**
2502.04066v1 by Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang

The GPT-4 technical report from OpenAI suggests that model performance on
specific tasks can be predicted prior to training, though methodologies remain
unspecified. This approach is crucial for optimizing resource allocation and
ensuring data alignment with target tasks. To achieve this vision, we focus on
predicting performance on Closed-book Question Answering (CBQA) tasks, which
are closely tied to pre-training data and knowledge retention. We address three
major challenges: 1) mastering the entire pre-training process, especially data
construction; 2) evaluating a model's knowledge retention; and 3) predicting
task-specific knowledge retention using only information available prior to
training. To tackle these challenges, we pre-train three large language models
(i.e., 1.6B, 7B, and 13B) using 560k dollars and 520k GPU hours. We analyze the
pre-training data with knowledge triples and assess knowledge retention using
established methods. Additionally, we introduce the SMI metric, an
information-theoretic measure that quantifies the relationship between
pre-training data, model size, and task-specific knowledge retention. Our
experiments reveal a strong linear correlation ($\text{R}^2 > 0.84$) between
the SMI metric and the model's accuracy on CBQA tasks across models of varying
sizes (i.e., 1.1B, 1.6B, 7B, and 13B). The dataset, model, and code are
available at https://github.com/yuhui1038/SMI.

摘要：OpenAI 的 GPT-4 技術報告指出，儘管方法仍未明確，但可以在訓練前預測模型在特定任務上的表現。這種方法對於優化資源配置和確保資料與目標任務的一致性至關重要。為了實現這個願景，我們專注於預測閉卷式問答 (CBQA) 任務的表現，這些任務與預訓練資料和知識保留密切相關。我們解決了三大挑戰：1) 掌握整個預訓練過程，特別是資料建構；2) 評估模型的知識保留；以及 3) 僅使用訓練前可得的資訊預測特定任務的知識保留。為了應對這些挑戰，我們使用 56 萬美元和 52 萬 GPU 小時預訓練了三個大型語言模型（即 1.6B、7B 和 13B）。我們使用知識三元組分析預訓練資料，並使用既定方法評估知識保留。此外，我們引入了 SMI 指標，這是一種資訊理論測量，用於量化預訓練資料、模型大小和特定任務知識保留之間的關係。我們的實驗揭示了一個強烈的線性相關性（$\text{R}^2 > 0.84$），在不同大小的模型（即 1.1B、1.6B、7B 和 13B）中，SMI 指標和模型在 CBQA 任務上的準確性之間。資料集、模型和程式碼可在 https://github.com/yuhui1038/SMI 取得。

##### **Strategic Learning with Local Explanations as Feedback**
2502.04058v1 by Kiet Q. H. Vo, Siu Lun Chau, Masahiro Kato, Yixin Wang, Krikamol Muandet

We investigate algorithmic decision problems where agents can respond
strategically to the decision maker's (DM) models. The demand for clear and
actionable explanations from DMs to (potentially strategic) agents continues to
rise. While prior work often treats explanations as full model disclosures,
explanations in practice might convey only partial information, which can lead
to misinterpretations and harmful responses. When full disclosure of the
predictive model is neither feasible nor desirable, a key open question is how
DMs can use explanations to maximise their utility without compromising agent
welfare. In this work, we explore well-known local and global explanation
methods, and establish a necessary condition to prevent explanations from
misleading agents into self-harming actions. Moreover, with conditional
homogeneity, we establish that action recommendation (AR)-based explanations
are sufficient for non-harmful responses, akin to the revelation principle in
information design. To operationalise AR-based explanations, we propose a
simple algorithm to jointly optimise the predictive model and AR policy to
balance DM outcomes with agent welfare. Our empirical results demonstrate the
benefits of this approach as a more refined strategy for safe and effective
partial model disclosure in algorithmic decision-making.

摘要：我們研究演算法決策問題，其中代理人可以對決策者 (DM) 的模型做出策略性回應。DM 對（潛在策略性）代理人的明確且可操作的解釋需求持續增加。雖然先前的研究通常將解釋視為完整的模型揭露，但實際上的解釋可能只傳達部分資訊，這可能導致誤解和有害的回應。當預測模型的完全揭露既不可行也不可取時，一個關鍵的開放性問題是 DM 如何使用解釋來最大化其效用，而不會損害代理人的福利。在這項工作中，我們探討了著名的局部和全局解釋方法，並建立了一個必要條件，以防止解釋誤導代理人採取自殘行為。此外，透過條件同質性，我們確立基於動作建議 (AR) 的解釋足以產生無害的回應，類似於資訊設計中的揭露原則。為了操作基於 AR 的解釋，我們提出了一個簡單的演算法，以共同最佳化預測模型和 AR 策略，以平衡 DM 結果與代理人福利。我們的實證結果證明了這種方法的好處，作為演算法決策中安全且有效的局部模型揭露的更精緻策略。

##### **Probe-Free Low-Rank Activation Intervention**
2502.04043v1 by Chonghe Jiang, Bao Nguyen, Anthony Man-Cho So, Viet Anh Nguyen

Language models (LMs) can produce texts that appear accurate and coherent but
contain untruthful or toxic content. Inference-time interventions that edit the
hidden activations have shown promising results in steering the LMs towards
desirable generations. Existing activation intervention methods often comprise
an activation probe to detect undesirable generation, triggering the activation
modification to steer subsequent generation. This paper proposes a probe-free
intervention method FLORAIN for all attention heads in a specific activation
layer. It eliminates the need to train classifiers for probing purposes. The
intervention function is parametrized by a sample-wise nonlinear low-rank
mapping, which is trained by minimizing the distance between the modified
activations and their projection onto the manifold of desirable content. Under
specific constructions of the manifold and projection distance, we show that
the intervention strategy can be computed efficiently by solving a smooth
optimization problem. The empirical results, benchmarked on multiple base
models, demonstrate that FLORAIN consistently outperforms several baseline
methods in enhancing model truthfulness and quality across generation and
multiple-choice tasks.

摘要：語言模型 (LM) 可以產生看起來準確且連貫的文字，但其中包含不真實或有毒的內容。編輯隱藏激活的推理時間介入已在引導 LM 朝向理想生成方面顯示出有希望的結果。現有的激活介入方法通常包含一個激活探測器，用於偵測不良生成，觸發激活修改以引導後續生成。本文提出了一種針對特定激活層中所有注意力頭部的無探測介入方法 FLORAIN。它消除了為探測目的訓練分類器的需要。介入函數由樣本級非線性低秩映射參數化，該映射通過最小化修改後的激活與其在理想內容流形上的投影之間的距離來訓練。在流形和投影距離的特定構造下，我們表明介入策略可以通過解決平滑優化問題來有效計算。在多個基礎模型上進行基準測試的經驗結果表明，FLORAIN 在增強模型真實性和生成和多項選擇任務中的質量方面始終優於多種基線方法。

##### **Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment**
2502.04040v1 by Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Minhao Cheng, Dacheng Tao

Training safe LLMs is one of the most critical research challenge. However,
the commonly used method, Refusal Training (RT), struggles to generalize
against various OOD jailbreaking attacks. Many safety training methods have
been proposed to address this issue. While they offer valuable insights, we aim
to complement this line of research by investigating whether OOD attacks truly
exceed the capability of RT model. Conducting evaluation with BoN, we observe
significant improvements on generalization as N increases. This underscores
that the model possesses sufficient safety-related latent knowledge, but RT
fails to consistently elicit this knowledge when addressing OOD attacks.
Further analysis based on domain adaptation reveals that training with direct
refusal causes model to rely on superficial shortcuts, resulting in learning of
non-robust representation mappings. Based on our findings, we propose training
model to perform safety reasoning for each query. Reasoning supervision
encourages model to perform more computations, explicitly eliciting and using
latent knowledge through reasoning. To achieve this, we synthesize reasoning
supervision based on pre-guidelines, training the model to reason in alignment
with them, thereby effectively eliciting and utilizing latent knowledge from
diverse perspectives. Extensive experiments show that our method significantly
improves generalization performance against OOD attacks.

摘要：訓練安全的 LLM 是最重要的研究挑戰之一。然而，
常用的方法，拒絕訓練 (RT)，難以概括
各種 OOD 越獄攻擊。許多安全訓練方法已
被提議來解決這個問題。雖然它們提供了有價值的見解，我們旨在
通過調查 OOD 攻擊是否真正超過 RT 模型的能力來補充這條研究線。使用 BoN 進行評估，我們觀察
隨著 N 的增加，泛化能力顯著提高。這強調
該模型具備足夠的安全相關潛在知識，但 RT
在處理 OOD 攻擊時無法始終如一地引出這種知識。
基於域適應的進一步分析表明，直接訓練
拒絕導致模型依賴於膚淺的捷徑，導致學習
非穩健表示映射。根據我們的發現，我們建議訓練
模型為每個查詢執行安全推理。推理監督
鼓勵模型執行更多計算，通過推理明確引出並使用
潛在知識。為此，我們根據預指南合成推理
監督，訓練模型與它們保持一致，從而有效地引出和利用
來自不同觀點的潛在知識。大量的實驗表明，我們的
方法顯著提高了對 OOD 攻擊的泛化性能。

##### **Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents**
2502.04038v1 by Yuchen Lian, Arianna Bisazza, Tessa Verhoef

Differential Case Marking (DCM) refers to the phenomenon where grammatical
case marking is applied selectively based on semantic, pragmatic, or other
factors. The emergence of DCM has been studied in artificial language learning
experiments with human participants, which were specifically aimed at
disentangling the effects of learning from those of communication (Smith &
Culbertson, 2020). Multi-agent reinforcement learning frameworks based on
neural networks have gained significant interest to simulate the emergence of
human-like linguistic phenomena. In this study, we employ such a framework in
which agents first acquire an artificial language before engaging in
communicative interactions, enabling direct comparisons to human result. Using
a very generic communication optimization algorithm and neural-network learners
that have no prior experience with language or semantic preferences, our
results demonstrate that learning alone does not lead to DCM, but when agents
communicate, differential use of markers arises. This supports Smith and
Culbertson (2020)'s findings that highlight the critical role of communication
in shaping DCM and showcases the potential of neural-agent models to complement
experimental research on language evolution.

摘要：差異化格位標記 (DCM) 指語法格位標記基於語義、語用或其他因素有選擇性地應用的現象。DCM 的出現已在針對人類參與者的人工語言學習實驗中獲得研究，這些實驗特別旨在區分學習與溝通的影響（Smith & Culbertson，2020）。基於神經網路的多主體強化學習架構已獲得顯著興趣，用於模擬類似人類的語言現象的出現。在本研究中，我們採用這樣的架構，其中主體在參與溝通互動之前先習得一種人工語言，從而能直接與人類結果進行比較。利用非常通用的溝通最佳化演算法和沒有語言或語義偏好先驗經驗的神經網路學習者，我們的結果證明，學習本身並不會導致 DCM，但當主體進行溝通時，就會出現標記的不同用法。這支持 Smith 和 Culbertson (2020) 的發現，強調了溝通在塑造 DCM 中的關鍵作用，並展示了神經主體模型在補充語言演化實驗研究方面的潛力。

##### **Exploring Imbalanced Annotations for Effective In-Context Learning**
2502.04037v1 by Hongfu Gao, Feipeng Zhang, Hao Zeng, Deyu Meng, Bingyi Jing, Hongxin Wei

Large language models (LLMs) have shown impressive performance on downstream
tasks through in-context learning (ICL), which heavily relies on the
demonstrations selected from annotated datasets. Existing selection methods may
hinge on the distribution of annotated datasets, which can often be long-tailed
in real-world scenarios. In this work, we show that imbalanced class
distributions in annotated datasets significantly degrade the performance of
ICL across various tasks and selection methods. Moreover, traditional rebalance
methods fail to ameliorate the issue of class imbalance in ICL. Our method is
motivated by decomposing the distributional differences between annotated and
test datasets into two-component weights: class-wise weights and conditional
bias. The key idea behind our method is to estimate the conditional bias by
minimizing the empirical error on a balanced validation dataset and to employ
the two-component weights to modify the original scoring functions during
selection. Our approach can prevent selecting too many demonstrations from a
single class while preserving the effectiveness of the original selection
methods. Extensive experiments demonstrate the effectiveness of our method,
improving the average accuracy by up to 5.46 on common benchmarks with
imbalanced datasets.

摘要：大型語言模型 (LLM) 透過情境學習 (ICL) 在下游任務中展現驚人的效能，而這項技術仰賴從標註資料集中選取的示範範例。現有的選取方法可能取決於標註資料集的分布，而在現實世界情境中，這些資料集通常會呈現長尾分布。在這項研究中，我們顯示標註資料集中不平衡的類別分布會顯著降低 ICL 在各種任務和選取方法中的效能。此外，傳統的再平衡方法無法改善 ICL 中類別不平衡的問題。我們的模型靈感來自於將標註資料集與測試資料集之間的分布差異分解成兩部分權重：類別權重與條件偏差。我們模型背後的主要概念是透過最小化平衡驗證資料集上的經驗誤差來估計條件偏差，並使用這兩部分權重來修改選取期間的原始評分函數。我們的做法可以避免從單一類別中選取過多示範範例，同時保留原始選取方法的有效性。廣泛的實驗證明了我們模型的有效性，在不平衡資料集的常見基準上，將平均準確度提升了 5.46%。

##### **Generalize Drug Response Prediction by Latent Independent Projection for Asymmetric Constrained Domain Generalization**
2502.04034v1 by Ran Song, Yinpu Bai, Hui Liu

The accurate prediction of drug responses remains a formidable challenge,
particularly at the single-cell level and in clinical treatment contexts. Some
studies employ transfer learning techniques to predict drug responses in
individual cells and patients, but they require access to target-domain data
during training, which is often unavailable or only obtainable in future. In
this study, we propose a novel domain generalization framework, termed
panCancerDR, to address this challenge. We conceptualize each cancer type as a
distinct source domain, with its cell lines serving as domain-specific samples.
Our primary objective is to extract domain-invariant features from the
expression profiles of cell lines across diverse cancer types, thereby
generalize the predictive capacity to out-of-distribution samples. To enhance
robustness, we introduce a latent independence projection (LIP) module that
encourages the encoder to extract informative yet non-redundant features. Also,
we propose an asymmetric adaptive clustering constraint, which clusters
drug-sensitive samples into a compact group while drives resistant samples
dispersed across separate clusters in the latent space. Our empirical
experiments demonstrate that panCancerDR effectively learns task-relevant
features from diverse source domains, and achieves accurate predictions of drug
response for unseen cancer type during training. Furthermore, when evaluated on
single-cell and patient-level prediction tasks, our model-trained solely on in
vitro cell line data without access to target-domain information-consistently
outperforms and matched current state-of-the-art methods. These findings
highlights the potential of our method for real-world clinical applications.

摘要：<paragraph>準確預測藥物反應仍然是一項艱鉅的挑戰，特別是在單細胞層級和臨床治療背景中。一些研究採用遷移學習技術來預測個別細胞和患者的藥物反應，但它們需要在訓練期間存取目標網域資料，而這些資料通常無法取得，或只能在未來取得。在這項研究中，我們提出一個新穎的網域概化架構，稱為 panCancerDR，以應對這項挑戰。我們將每種類型的癌症概念化為一個不同的來源網域，其細胞株作為特定網域的樣本。我們的首要目標是從不同癌症類型的細胞株表現特徵中萃取網域不變特徵，從而將預測能力概化到分布外的樣本。為了增強穩健性，我們引入一個潛在獨立投影 (LIP) 模組，鼓勵編碼器萃取有資訊但非冗餘的特徵。此外，我們提出一個非對稱自適應聚類約束，將對藥物敏感的樣本聚類到一個緊湊的群組中，同時驅動抗藥性樣本分散在潛在空間中的不同群組中。我們的實證實驗證明，panCancerDR 有效地從不同的來源網域學習與任務相關的特徵，並在訓練期間對未見的癌症類型實現準確的藥物反應預測。此外，當在單細胞和患者層級預測任務中進行評估時，我們的模型僅在體外細胞株資料上訓練，而沒有存取目標網域資訊，始終優於並符合當前的最新方法。這些發現突顯了我們的方法在實際臨床應用中的潛力。</paragraph>

##### **Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated Model Merging**
2502.04030v1 by Guinan Su, Jonas Geiping

Reasoning capabilities represent a critical frontier for large language
models (LLMs), but developing them requires extensive proprietary datasets and
computational resources. One way to efficiently supplement capabilities with is
by model merging, which offers a promising alternative by combining multiple
models without retraining. However, current merging approaches rely on
manually-designed strategies for merging hyperparameters, limiting the
exploration of potential model combinations and requiring significant human
effort. We propose an Automated Model Merging Framework that enables
fine-grained exploration of merging strategies while reducing costs through
multi-fidelity approximations. We support both single and multi-objective
optimization and introduce two novel search spaces: layerwise fusion (LFS) and
depth-wise integration (DIS). Evaluating across a number of benchmarks, we find
that the search autonomously finds 1) Merges that further boost
single-objective performance, even on tasks the model has already been
finetuned on, and 2) Merges that optimize multi-objective frontiers across
tasks. Effective merges are found with limited compute, e.g. within less than
500 search steps.

摘要：推理能力代表大型語言模型 (LLM) 的關鍵前沿，但開發它們需要廣泛的專有數據集和計算資源。有效補充功能的一種方法是模型合併，它提供了一個有前途的替代方案，可以合併多個模型而無需重新訓練。然而，當前合併方法依賴於人工設計的超參數合併策略，限制了潛在模型組合的探索，並需要大量人力。我們提出了一個自動化模型合併框架，它能夠精細地探索合併策略，同時通過多保真近似降低成本。我們支援單一和多目標最佳化，並引入了兩個新穎的搜索空間：逐層融合 (LFS) 和深度整合 (DIS)。在多個基準上進行評估，我們發現搜索會自動找到 1) 進一步提升單一目標效能的合併，即使在模型已經進行微調的任務上，以及 2) 在任務中最佳化多目標前沿的合併。在有限的運算下，例如在不到 500 個搜索步驟內，就能找到有效的合併。

##### **Quantification of Biodiversity from Historical Survey Text with LLM-based Best-Worst Scaling**
2502.04022v1 by Thomas Haider, Tobias Perschl, Malte Rehbein

In this study, we evaluate methods to determine the frequency of species via
quantity estimation from historical survey text. To that end, we formulate
classification tasks and finally show that this problem can be adequately
framed as a regression task using Best-Worst Scaling (BWS) with Large Language
Models (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the
latter two have reasonable agreement with humans and each other. We conclude
that this approach is more cost-effective and similarly robust compared to a
fine-grained multi-class approach, allowing automated quantity estimation
across species.

摘要：在本研究中，我們評估了透過歷史調查文字中的數量估計來確定物種頻率的方法。為此，我們制定了分類任務，最後表明這個問題可以用大型語言模型 (LLM) 的最佳最差縮放 (BWS) 作為回歸任務來適當地構建。我們測試了 Ministral-8B、DeepSeek-V3 和 GPT-4，發現後兩者與人類和彼此都有合理的共識。我們得出結論，與細粒度的多類別方法相比，這種方法更具成本效益且同樣穩健，允許跨物種進行自動數量估計。

##### **Automating a Complete Software Test Process Using LLMs: An Automotive Case Study**
2502.04008v1 by Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy

Vehicle API testing verifies whether the interactions between a vehicle's
internal systems and external applications meet expectations, ensuring that
users can access and control various vehicle functions and data. However, this
task is inherently complex, requiring the alignment and coordination of API
systems, communication protocols, and even vehicle simulation systems to
develop valid test cases. In practical industrial scenarios, inconsistencies,
ambiguities, and interdependencies across various documents and system
specifications pose significant challenges. This paper presents a system
designed for the automated testing of in-vehicle APIs. By clearly defining and
segmenting the testing process, we enable Large Language Models (LLMs) to focus
on specific tasks, ensuring a stable and controlled testing workflow.
Experiments conducted on over 100 APIs demonstrate that our system effectively
automates vehicle API testing. The results also confirm that LLMs can
efficiently handle mundane tasks requiring human judgment, making them suitable
for complete automation in similar industrial contexts.

摘要：車輛 API 測試驗證車輛內部系統與外部應用程式之間的互動是否符合預期，確保使用者可以存取和控制各種車輛功能和資料。然而，這項任務本質上很複雜，需要 API 系統、通訊協定，甚至車輛模擬系統的對齊和協調，才能開發出有效的測試案例。在實際的產業情境中，各個文件和系統規格之間的不一致性、模糊性，以及相互依賴性構成了重大的挑戰。本文提出了一套專門用於車載 API 自動化測試的系統。透過清楚定義和區隔測試流程，我們讓大型語言模型 (LLM) 能專注於特定任務，確保測試工作流程穩定且受控。在超過 100 個 API 上進行的實驗證明，我們的系統有效地自動化了車輛 API 測試。結果也證實，LLM 能有效率地處理需要人類判斷的例行工作，使其適合於類似產業情境中的完整自動化。

##### **Online Learning of Counter Categories and Ratings in PvP Games**
2502.03998v1 by Chiu-Chou Lin, I-Chen Wu

In competitive games, strength ratings like Elo are widely used to quantify
player skill and support matchmaking by accounting for skill disparities better
than simple win rate statistics. However, scalar ratings cannot handle complex
intransitive relationships, such as counter strategies seen in
Rock-Paper-Scissors. To address this, recent work introduced Neural Rating
Table and Neural Counter Table, which combine scalar ratings with discrete
counter categories to model intransitivity. While effective, these methods rely
on neural network training and cannot perform real-time updates. In this paper,
we propose an online update algorithm that extends Elo principles to
incorporate real-time learning of counter categories. Our method dynamically
adjusts both ratings and counter relationships after each match, preserving the
explainability of scalar ratings while addressing intransitivity. Experiments
on zero-sum competitive games demonstrate its practicality, particularly in
scenarios without complex team compositions.

摘要：在競爭遊戲中，像 Elo 這樣的強度評分被廣泛用於量化玩家技巧和支援配對機制，藉由考量技巧差異，表現優於單純的勝率統計。然而，標量評分無法處理複雜的非遞移關係，例如在剪刀石頭布中看到的反制策略。為了解決這個問題，最近的研究引入了神經評分表和神經反制表，它們將標量評分與離散反制類別結合，以建模非遞移性。儘管有效，這些方法依賴於神經網路訓練，無法執行即時更新。在本文中，我們提出了一種線上更新演算法，將 Elo 原則延伸到納入反制類別的即時學習。我們的演算法在每次比賽後動態調整評分和反制關係，在處理非遞移性的同時，保留標量評分的可解釋性。在零和競爭遊戲中的實驗證明了它的實用性，特別是在沒有複雜團隊組成的場景中。

##### **Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering**
2502.03992v1 by Longquan Jiang, Junbo Huang, Cedric Möller, Ricardo Usbeck

Most existing Knowledge Graph Question Answering (KGQA) approaches are
designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the
heterogeneity of the underlying graph schema, topology and assertions, most
KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without
resource-intensive training data. We present OntoSCPrompt, a novel Large
Language Model (LLM)-based KGQA approach with a two-stage architecture that
separates semantic parsing from KG-dependent interactions. OntoSCPrompt first
generates a SPARQL query structure (including SPARQL keywords such as SELECT,
ASK, WHERE and placeholders for missing tokens) and then fills them with
KG-specific information. To enhance the understanding of the underlying KG, we
present an ontology-guided, hybrid prompt learning strategy that integrates KG
ontology into the learning process of hybrid prompts (e.g., discrete and
continuous vectors). We also present several task-specific decoding strategies
to ensure the correctness and executability of generated SPARQL queries in both
stages. Experimental results demonstrate that OntoSCPrompt performs as well as
SOTA approaches without retraining on a number of KGQA datasets such as CWQ,
WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well
to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:
\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}

摘要：現有的知識圖譜問答（KGQA）方法大多是為特定 KG 而設計的，例如 Wikidata、DBpedia 或 Freebase。由於底層圖形模式、拓撲和斷言的異質性，大多數 KGQA 系統無法在沒有資源密集型訓練資料的情況下轉移到未見過的知識圖譜（KG）。我們提出 OntoSCPrompt，這是一種基於大型語言模型（LLM）的新型 KGQA 方法，採用兩階段架構，將語義解析與依賴 KG 的互動分開。OntoSCPrompt 首先生成 SPARQL 查詢結構（包括 SPARQL 關鍵字，例如 SELECT、ASK、WHERE 和缺失令牌的佔位符），然後用 KG 特定的資訊填寫它們。為了增強對底層 KG 的理解，我們提出了一種由本体指導的混合提示學習策略，將 KG 本体整合到混合提示（例如，離散和連續向量）的學習過程中。我們還提出了多種特定任務的解碼策略，以確保在兩個階段中生成的 SPARQL 查詢的正確性和可執行性。實驗結果表明，OntoSCPrompt 在 CWQ、WebQSP 和 LC-QuAD 1.0 等多個 KGQA 資料集上執行時，效能與 SOTA 方法一樣好，且資源使用效率高，並且可以很好地概括到未見過的特定領域 KG，例如 DBLP-QuAD 和 CoyPu KG Code：
\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}

##### **PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation**
2502.03984v1 by Hyemin Lim, Jaeyeon Lee, Dong-Wan Choi

Large pretrained language models such as BERT suffer from slow inference and
high memory usage, due to their huge size. Recent approaches to compressing
BERT rely on iterative pruning and knowledge distillation, which, however, are
often too complicated and computationally intensive. This paper proposes a
novel semi-structured one-shot pruning method for BERT, called
$\textit{Permutation and Grouping for BERT}$ (PGB), which achieves high
compression efficiency and sparsity while preserving accuracy. To this end, PGB
identifies important groups of individual weights by permutation and prunes all
other weights as a structure in both multi-head attention and feed-forward
layers. Furthermore, if no important group is formed in a particular layer, PGB
drops the entire layer to produce an even more compact model. Our experimental
results on BERT$_{\text{BASE}}$ demonstrate that PGB outperforms the
state-of-the-art structured pruning methods in terms of computational cost and
accuracy preservation.

摘要：大型預訓練語言模型，例如 BERT，由於其龐大規模，會造成推論速度慢且記憶體使用量高的問題。最近壓縮 BERT 的方法依賴於反覆剪枝和知識蒸餾，然而，這些方法通常過於複雜且計算密集。本文提出了一種新的半結構化一次性剪枝方法，稱為 BERT 的排列和分組 (PGB)，它在保持準確性的同時，達到了高壓縮效率和稀疏性。為此，PGB 透過排列識別出重要群組的個別權重，並將所有其他權重剪枝為多頭注意力和前饋層中的結構。此外，如果在特定層中沒有形成重要群組，PGB 會刪除整個層以產生更緊湊的模型。我們在 BERT$_{\text{BASE}}$ 上的實驗結果表明，PGB 在計算成本和準確性保持方面優於最先進的結構化剪枝方法。

##### **Towards Unified Music Emotion Recognition across Dimensional and Categorical Models**
2502.03979v1 by Jaeyong Kang, Dorien Herremans

One of the most significant challenges in Music Emotion Recognition (MER)
comes from the fact that emotion labels can be heterogeneous across datasets
with regard to the emotion representation, including categorical (e.g., happy,
sad) versus dimensional labels (e.g., valence-arousal). In this paper, we
present a unified multitask learning framework that combines these two types of
labels and is thus able to be trained on multiple datasets. This framework uses
an effective input representation that combines musical features (i.e., key and
chords) and MERT embeddings. Moreover, knowledge distillation is employed to
transfer the knowledge of teacher models trained on individual datasets to a
student model, enhancing its ability to generalize across multiple tasks. To
validate our proposed framework, we conducted extensive experiments on a
variety of datasets, including MTG-Jamendo, DEAM, PMEmo, and EmoMusic.
According to our experimental results, the inclusion of musical features,
multitask learning, and knowledge distillation significantly enhances
performance. In particular, our model outperforms the state-of-the-art models,
including the best-performing model from the MediaEval 2021 competition on the
MTG-Jamendo dataset. Our work makes a significant contribution to MER by
allowing the combination of categorical and dimensional emotion labels in one
unified framework, thus enabling training across datasets.

摘要：音樂情緒辨識 (MER) 中最顯著的挑戰之一是，情緒標籤在資料集之間可能不盡相同，這取決於情緒表徵，包括分類標籤（例如：快樂、悲傷）與維度標籤（例如：效價-喚醒）。在本文中，我們提出一個統一的多任務學習架構，結合這兩種標籤類型，因此能夠在多個資料集上進行訓練。此架構使用一種有效的輸入表徵，結合音樂特徵（即，音調和弦）和 MERT 嵌入。此外，知識蒸餾用於將在個別資料集上訓練的教師模型的知識傳輸給學生模型，增強其跨多個任務進行概括的能力。為了驗證我們提出的架構，我們對各種資料集進行了廣泛的實驗，包括 MTG-Jamendo、DEAM、PMEmo 和 EmoMusic。根據我們的實驗結果，音樂特徵、多任務學習和知識蒸餾的加入顯著提升了效能。特別是，我們的模型優於最先進的模型，包括 MediaEval 2021 競賽中在 MTG-Jamendo 資料集上表現最佳的模型。我們的研究透過允許在一個統一的架構中結合分類和維度情緒標籤，對 MER 做出重大貢獻，從而能夠跨資料集進行訓練。

##### **MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation**
2502.03966v1 by YoonJe Kang, Yonghoon Jung, Wonseop Shin, Bumsoo Kim, Sanghyun Seo

In this paper, we present synthetic data generation framework for flood
hazard detection system. For high fidelity and quality, we characterize several
real-world properties into virtual world and simulate the flood situation by
controlling them. For the sake of efficiency, recent generative models in
image-to-3D and urban city synthesis are leveraged to easily composite flood
environments so that we avoid data bias due to the hand-crafted manner. Based
on our framework, we build the flood synthetic dataset with 5 levels, dubbed
MultiFloodSynth which contains rich annotation types like normal map,
segmentation, 3D bounding box for a variety of downstream task. In experiments,
our dataset demonstrate the enhanced performance of flood hazard detection with
on-par realism compared with real dataset.

摘要：在本文中，我們提出合成資料產生架構，用於洪水災害偵測系統。為了高保真和品質，我們將數個真實世界屬性特徵化到虛擬世界中，並透過控制它們來模擬洪水情況。為了效率，我們利用最近的影像轉 3D 和城市合成中的生成模型，輕鬆合成洪水環境，這樣我們就能避免由於手工製作而產生的資料偏差。根據我們的架構，我們建立了 5 個等級的洪水合成資料集，稱為 MultiFloodSynth，其中包含豐富的註解類型，例如法線貼圖、分割、3D 邊界框，可用於各種下游任務。在實驗中，我們的資料集展示了洪水災害偵測的增強效能，與真實資料集相比具有同等的真實感。

##### **Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples**
2502.03957v1 by Konstantinos Tsigos, Evlampios Apostolidis, Vasileios Mezaris

In this paper, we introduce the idea of using adversarially-generated samples
of the input images that were classified as deepfakes by a detector, to form
perturbation masks for inferring the importance of different input features and
produce visual explanations. We generate these samples based on Natural
Evolution Strategies, aiming to flip the original deepfake detector's decision
and classify these samples as real. We apply this idea to four
perturbation-based explanation methods (LIME, SHAP, SOBOL and RISE) and
evaluate the performance of the resulting modified methods using a SOTA
deepfake detection model, a benchmarking dataset (FaceForensics++) and a
corresponding explanation evaluation framework. Our quantitative assessments
document the mostly positive contribution of the proposed perturbation approach
in the performance of explanation methods. Our qualitative analysis shows the
capacity of the modified explanation methods to demarcate the manipulated image
regions more accurately, and thus to provide more useful explanations.

摘要：在本文中，我們提出了一個想法，使用對抗生成樣本，這些樣本是通過一個檢測器將輸入圖像分類為深度偽造而形成的，以形成擾動遮罩，用於推斷不同輸入特徵的重要性並產生視覺解釋。我們基於自然演化策略生成這些樣本，旨在改變原始深度偽造檢測器的決策，並將這些樣本分類為真實的。我們將這個想法應用於四種基於擾動的解釋方法（LIME、SHAP、SOBOL 和 RISE），並使用 SOTA 深度偽造檢測模型、基準數據集（FaceForensics++）和相應的解釋評估框架評估所產生的修改方法的性能。我們的定量評估記錄了所提出的擾動方法在解釋方法的性能中大多數的積極貢獻。我們的定性分析顯示了修改後的解釋方法更準確地劃定被操縱的圖像區域的能力，從而提供更有用的解釋。

##### **MAQInstruct: Instruction-based Unified Event Relation Extraction**
2502.03954v1 by Jun Xu, Mengshu Sun, Zhiqiang Zhang, Jun Zhou

Extracting event relations that deviate from known schemas has proven
challenging for previous methods based on multi-class classification, MASK
prediction, or prototype matching. Recent advancements in large language models
have shown impressive performance through instruction tuning. Nevertheless, in
the task of event relation extraction, instruction-based methods face several
challenges: there are a vast number of inference samples, and the relations
between events are non-sequential. To tackle these challenges, we present an
improved instruction-based event relation extraction framework named
MAQInstruct. Firstly, we transform the task from extracting event relations
using given event-event instructions to selecting events using given
event-relation instructions, which reduces the number of samples required for
inference. Then, by incorporating a bipartite matching loss, we reduce the
dependency of the instruction-based method on the generation sequence. Our
experimental results demonstrate that MAQInstruct significantly improves the
performance of event relation extraction across multiple LLMs.

摘要：提取偏離已知模式的事件關係，已證明對基於多類分類、MASK 預測或原型匹配的先前方法構成挑戰。大型語言模型的最新進展已透過指令調整展現出令人印象深刻的效能。不過，在事件關係萃取任務中，基於指令的方法面臨多項挑戰：有大量的推論範例，而且事件之間的關係是非順序的。為了應對這些挑戰，我們提出一個名為 MAQInstruct 的改良式基於指令的事件關係萃取架構。首先，我們將任務從使用既定的事件-事件指令萃取事件關係，轉換為使用既定的事件-關係指令選取事件，這減少了推論所需的範例數量。接著，透過納入雙分匹配損失，我們降低基於指令的方法對於生成順序的依賴性。我們的實驗結果證明，MAQInstruct 大幅改善了多個 LLM 的事件關係萃取效能。

##### **Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond**
2502.03945v1 by Mardhiyah Sanni, Tassallah Abdullahi, Devendra D. Kayande, Emmanuel Ayodele, Naome A. Etori, Michael S. Mollel, Moshood Yekini, Chibuzor Okocha, Lukman E. Ismaila, Folafunmi Omofoye, Boluwatife A. Adewale, Tobi Olatunji

Speech technologies are transforming interactions across various sectors,
from healthcare to call centers and robots, yet their performance on
African-accented conversations remains underexplored. We introduce
Afrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical
African-accented English conversations, designed to evaluate automatic speech
recognition (ASR) and related technologies. We assess state-of-the-art (SOTA)
speaker diarization and ASR systems on long-form, accented speech, comparing
their performance with native accents and discover a 10%+ performance
degradation. Additionally, we explore medical conversation summarization
capabilities of large language models (LLMs) to demonstrate the impact of ASR
errors on downstream medical summaries, providing insights into the challenges
and opportunities for speech technologies in the Global South. Our work
highlights the need for more inclusive datasets to advance conversational AI in
low-resource settings.

摘要：語音技術正在轉變各個領域的互動，
從醫療保健到客服中心和機器人，但它們在
非洲口音對話中的表現仍未得到充分探索。我們推出
Afrispeech-Dialog，一個由 50 個模擬醫療和非醫療
非洲口音英語對話組成的基準資料集，旨在評估自動語音
辨識 (ASR) 和相關技術。我們評估最先進 (SOTA)
在長篇、有口音的語音上進行說話人區分和 ASR 系統，比較
它們與母語口音的表現，並發現有 10% 以上的表現
下降。此外，我們探討大型語言模型 (LLM) 的醫療對話摘要
功能，以展示 ASR 錯誤對下游醫療摘要的影響，並提供對挑戰的見解
以及全球南方語音技術的機遇。我們的研究
強調需要更多包容性的資料集來推動對話式 AI 在
資源不足的環境中。

##### **DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation**
2502.03930v1 by Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang

Several recent studies have attempted to autoregressively generate continuous
speech representations without discrete speech tokens by combining diffusion
and autoregressive models, yet they often face challenges with excessive
computational loads or suboptimal outcomes. In this work, we propose Diffusion
Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive
framework combining a language model with a diffusion transformer. This
approach significantly enhances the efficacy of autoregressive models for
continuous tokens and reduces computational demands. DiTAR utilizes a
divide-and-conquer strategy for patch generation, where the language model
processes aggregated patch embeddings and the diffusion transformer
subsequently generates the next patch based on the output of the language
model. For inference, we propose defining temperature as the time point of
introducing noise during the reverse diffusion ODE to balance diversity and
determinism. We also show in the extensive scaling analysis that DiTAR has
superb scalability. In zero-shot speech generation, DiTAR achieves
state-of-the-art performance in robustness, speaker similarity, and
naturalness.

摘要：多項近期研究嘗試結合擴散和自迴歸模型，自迴歸地產生連續語音表徵，而不使用離散語音符號，但它們經常面臨計算負載過大或結果次佳的挑戰。在這項工作中，我們提出擴散轉換器自迴歸模型 (DiTAR)，一個基於區塊的自迴歸架構，結合語言模型與擴散轉換器。這種方法大幅增強自迴歸模型對連續符號的效能，並減少計算需求。DiTAR 利用分而治之策略進行區塊產生，其中語言模型處理聚集的區塊嵌入，而擴散轉換器隨後根據語言模型的輸出產生下一個區塊。對於推論，我們建議將溫度定義為在反向擴散 ODE 中引入雜訊的時間點，以平衡多樣性和確定性。我們也在廣泛的縮放分析中顯示，DiTAR 具有極佳的可擴充性。在零次學習語音產生中，DiTAR 在穩健性、說話者相似性和自然度方面達到最先進的效能。

##### **Adaptation of Task Goal States from Prior Knowledge**
2502.03918v1 by Andrei Costinescu, Darius Burschka

This paper presents a framework to define a task with freedom and variability
in its goal state. A robot could use this to observe the execution of a task
and target a different goal from the observed one; a goal that is still
compatible with the task description but would be easier for the robot to
execute. We define the model of an environment state and an environment
variation, and present experiments on how to interactively create the variation
from a single task demonstration and how to use this variation to create an
execution plan for bringing any environment into the goal state.

摘要：本文提出了一個架構，用於定義一個任務，其目標狀態具有自由度和可變性。機器人可以使用此架構來觀察任務的執行，並針對與觀察到的目標不同的目標；一個仍然與任務描述相容，但對機器人來說更容易執行的目標。我們定義了環境狀態和環境變化的模型，並展示了如何從單一任務示範中互動式地創建變化，以及如何使用此變化為將任何環境帶入目標狀態創建執行計畫。

##### **Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software**
2502.03916v1 by Andreas Baumann, Peter Eberhard

Large Language Models (LLMs) are increasingly helpful in text generation,
even writing code in programming languages based on user prompts written in
natural language. They are even applied to generate simulation models for
multibody systems from natural language. Research results suggest that LLMs
surpass the mere replication of existing code examples, where some LLMs have
been trained on an open-source multibody simulation code. However, for
closed-source simulation software, such results are not to be expected as their
ideas and concepts might differ from other publicly available ones. LLMs can
hallucinate for knowledge-intensive tasks, such as model creation, which can
lead to wrong responses. This is especially the case for the LLM unknown
closed-source simulation software. The same applies to other internal knowledge
kept private to protect intellectual property or data privacy. The
Retrieval-Augmented Generation (RAG) approach might yield a solution for these
knowledge-intensive tasks. This paper explores the application of RAG to
closed-source simulation software and presents first experiments. After a brief
introduction to LLMs, the RAG approach, and the simulation method applied by
the close-source simulation software, several examples are provided to test
LLMs' knowledge of the simulation software and the creation of simulation
models using two RAG systems. The examples show promising results indicating
the benefits of applying RAG systems to closed-source simulation software,
helping to access their knowledge. Nevertheless, they also reveal gaps in the
applied information and open questions for further research.

摘要：大型語言模型 (LLM) 在文本生成方面越來越有幫助，甚至可以根據以自然語言寫成的使用者提示，撰寫程式語言的程式碼。它們甚至被應用於從自然語言產生多體系統的模擬模型。研究結果表明，LLM 超越了現有程式碼範例的單純複製，其中一些 LLM 已接受開源多體模擬程式碼的訓練。然而，對於閉源模擬軟體，由於它們的想法和概念可能與其他公開可用的想法和概念不同，因此無法預期有這樣的結果。LLM 可以對知識密集型任務（例如模型建立）產生幻覺，這可能導致錯誤的回應。對於 LLM 未知的閉源模擬軟體，情況尤其如此。這也適用於其他內部知識，這些知識會被保密以保護智慧財產權或資料隱私。檢索增強生成 (RAG) 方法可能為這些知識密集型任務提供了解決方案。本文探討了 RAG 在閉源模擬軟體中的應用，並提出了初步實驗。在簡要介紹 LLM、RAG 方法和閉源模擬軟體所應用的模擬方法後，提供了幾個範例來測試 LLM 對模擬軟體的了解，以及使用兩個 RAG 系統建立模擬模型。這些範例顯示了有希望的結果，表明將 RAG 系統應用於閉源模擬軟體的好處，有助於存取它們的知識。儘管如此，它們也揭露了應用資訊中的差距，並提出了進一步研究的開放性問題。

##### **Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning**
2502.03884v1 by Peizhuang Cong, Wenpu Liu, Wenhan Yu, Haochen Zhao, Tong Yang

Large language models (LLMs) have demonstrated remarkable success across
various tasks, accompanied by a continuous increase in their parameter size.
Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), address the challenges of fine-tuning LLMs by significantly reducing
the number of trainable parameters. Recent studies have integrated LoRA with
Mixture of Experts (MoE) architectures, leveraging multiple adapter experts and
gating mechanisms to further improve fine-tuning performance. However, existing
approaches primarily focus on adjusting the allocations of adapter experts per
layer to optimize the introduced trainable parameter size, while neglecting a
critical factor of adapters' rank. To this end, we propose a hierarchical
scheme for expert allocation and rank configuration, HILO, which dynamically
adjusts the number and rank of adapter experts across layers, matching the
varying representational complexity of model layers in adapter-granularity.
Extensive experiments on multiple benchmark tasks demonstrate that HILO
outperforms existing methods in accuracy while introducing fewer trainable
parameters, providing an efficient and practical solution for fine-tuning LLMs.

摘要：大型語言模型 (LLM) 已在各種任務中展現出顯著的成功，同時其參數規模也不斷增加。參數高效微調 (PEFT) 方法，例如低階適應 (LoRA)，透過大幅減少可訓練參數數量來解決微調 LLM 的挑戰。最近的研究已將 LoRA 與專家混合 (MoE) 架構整合，利用多個適配器專家和閘控機制進一步提升微調效能。然而，現有方法主要著重於調整每層適配器專家的配置以最佳化引入的可訓練參數規模，同時忽略適配器等級的關鍵因素。為此，我們提出一個分層專家配置和等級設定方案，HILO，它會動態調整各層適配器專家的數量和等級，在適配器粒度中符合模型層變化的表示複雜度。透過多個基準任務的廣泛實驗證明，HILO 在準確度方面優於現有方法，同時引入較少的可訓練參數，為微調 LLM 提供高效且實用的解決方案。

##### **BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation**
2502.03860v1 by Bo Pang, Hanze Dong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou, Caiming Xiong

Large language models (LLMs), such as o1 from OpenAI, have demonstrated
remarkable reasoning capabilities. o1 generates a long chain-of-thought
(LongCoT) before answering a question. LongCoT allows LLMs to analyze problems,
devise plans, reflect, and backtrack effectively. These actions empower LLM to
solve complex problems. After the release of o1, many teams have attempted to
replicate its LongCoT and reasoning capabilities. In terms of methods, they
primarily rely on knowledge distillation with data from existing models with
LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving
significant uncertainties on systematically developing such reasoning
abilities. In terms of data domains, these works focus narrowly on math while a
few others include coding, limiting their generalizability. This paper
introduces a novel approach to enable LLM's LongCoT capacity without
distillation from o1-like models or expensive human annotations, where we
bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three
stages: 1) LongCoT data bootstrapping with in-context learning on a standard
instruct model; 2) LongCoT supervised finetuning; 3) online training to further
refine LongCoT capacities. In BOLT, only a few in-context examples need to be
constructed during the bootstrapping stage; in our experiments, we created 10
examples, demonstrating the feasibility of this approach. We use
Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various
model scales (7B, 8B, 70B). We achieve impressive performance on a variety of
benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which
evaluate diverse task-solving and reasoning capabilities.

摘要：大型語言模型 (LLM)，例如 OpenAI 的 o1，已經展示出非凡的推理能力。o1 在回答問題之前會產生一個長鏈的想法 (LongCoT)。LongCoT 允許 LLM 分析問題、制定計劃、反思和有效回溯。這些動作賦能 LLM 解決複雜的問題。在 o1 發布後，許多團隊都嘗試複製其 LongCoT 和推理能力。在方法方面，他們主要依賴於具有 LongCoT 能力的現有模型的數據進行知識蒸餾（例如，OpenAI-o1、Qwen-QwQ、DeepSeek-R1-Preview），在系統性地開發這種推理能力方面留下重大的不確定性。在數據領域方面，這些工作狹隘地集中在數學上，而其他一些則包括編碼，限制了它們的普遍性。本文介紹了一種新穎的方法，可以在不從類似 o1 的模型或昂貴的人工註釋中進行蒸餾的情況下實現 LLM 的 LongCoT 能力，我們從標準指令模型中引導 LongCoT (BOLT)。BOLT 涉及三個階段：1) 在標準指令模型上通過語境學習引導 LongCoT 數據；2) LongCoT 監督微調；3) 在線訓練以進一步完善 LongCoT 能力。在 BOLT 中，在引導階段只需要構造幾個語境範例；在我們的實驗中，我們創建了 10 個範例，證明了這種方法的可行性。我們使用 Llama-3.1-70B-Instruct 引導 LongCoT，並將我們的模型應用於各種模型規模（7B、8B、70B）。我們在各種基準測試中取得了令人印象深刻的性能，Arena-Hard、MT-Bench、WildBench、ZebraLogic、MATH500，這些測試評估了不同的任務解決和推理能力。

##### **Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount**
2502.03852v1 by Yanbiao Ma, Wei Dai, Jiayi Chen

In object detection, the instance count is typically used to define whether a
dataset exhibits a long-tail distribution, implicitly assuming that models will
underperform on categories with fewer instances. This assumption has led to
extensive research on category bias in datasets with imbalanced instance
counts. However, models still exhibit category bias even in datasets where
instance counts are relatively balanced, clearly indicating that instance count
alone cannot explain this phenomenon. In this work, we first introduce the
concept and measurement of category information amount. We observe a
significant negative correlation between category information amount and
accuracy, suggesting that category information amount more accurately reflects
the learning difficulty of a category. Based on this observation, we propose
Information Amount-Guided Angular Margin (IGAM) Loss. The core idea of IGAM is
to dynamically adjust the decision space of each category based on its
information amount, thereby reducing category bias in long-tail datasets. IGAM
Loss not only performs well on long-tailed benchmark datasets such as LVIS v1.0
and COCO-LT but also shows significant improvement for underrepresented
categories in the non-long-tailed dataset Pascal VOC. Comprehensive experiments
demonstrate the potential of category information amount as a tool and the
generality of our proposed method.

摘要：在目標偵測中，實例計數通常用於定義資料集是否展現長尾分佈，並隱含假設模型在實例較少的類別上表現不佳。這個假設導致大量針對不平衡實例計數資料集中類別偏誤的研究。然而，即使在實例計數相對平衡的資料集中，模型仍會展現類別偏誤，這清楚地表明實例計數本身無法解釋此現象。在此研究中，我們首先介紹類別資訊量的概念和度量。我們觀察到類別資訊量與準確度之間存在顯著的負相關，這表明類別資訊量更準確地反映了類別的學習難度。基於此觀察，我們提出資訊量引導角裕度（IGAM）損失。IGAM 的核心思想是根據每個類別的資訊量動態調整其決策空間，從而減少長尾資料集中的類別偏誤。IGAM 損失不僅在長尾基準資料集（例如 LVIS v1.0 和 COCO-LT）上表現良好，而且在非長尾資料集 Pascal VOC 中的代表性不足類別上也展現出顯著的改善。全面的實驗證明了類別資訊量作為工具的潛力，以及我們提出的方法的普遍性。

##### **Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis**
2502.03843v1 by Lin Yuan, Jun Xu, Honghao Gui, Mengshu Sun, Zhiqiang Zhang, Lei Liang, Jun Zhou

High-quality, large-scale instructions are crucial for aligning large
language models (LLMs), however, there is a severe shortage of instruction in
the field of natural language understanding (NLU). Previous works on
constructing NLU instructions mainly focus on information extraction (IE),
neglecting tasks such as machine reading comprehension, question answering, and
text classification. Furthermore, the lack of diversity in the data has led to
a decreased generalization ability of trained LLMs in other NLU tasks and a
noticeable decline in the fundamental model's general capabilities. To address
this issue, we propose Hum, a large-scale, high-quality synthetic instruction
corpus for NLU tasks, designed to enhance the NLU capabilities of LLMs.
Specifically, Hum includes IE (either close IE or open IE), machine reading
comprehension, text classification, and instruction generalist tasks, thereby
enriching task diversity. Additionally, we introduce a human-LLMs collaborative
mechanism to synthesize instructions, which enriches instruction diversity by
incorporating guidelines, preference rules, and format variants. We conduct
extensive experiments on 5 NLU tasks and 28 general capability evaluation
datasets for LLMs. Experimental results show that Hum enhances the NLU
capabilities of six LLMs by an average of 3.1\%, with no significant decline
observed in other general capabilities.

摘要：高品質、大規模的指示對於調整大型語言模型 (LLM) 至關重要，然而，在自然語言理解 (NLU) 領域中嚴重缺乏指示。先前關於建構 NLU 指示的研究主要側重於資訊萃取 (IE)，忽略了機器閱讀理解、問題回答和文字分類等任務。此外，資料缺乏多樣性導致訓練後的 LLM 在其他 NLU 任務中的概化能力下降，以及基礎模型的整體能力顯著下降。為了解決此問題，我們提出 Hum，這是一個針對 NLU 任務的大規模、高品質合成指示語料庫，旨在增強 LLM 的 NLU 能力。具體來說，Hum 包含 IE（封閉式 IE 或開放式 IE）、機器閱讀理解、文字分類和指示通才任務，從而豐富任務多樣性。此外，我們引入一種人機協作機制來合成指示，透過納入準則、偏好規則和格式變體，豐富指示的多樣性。我們針對 5 個 NLU 任務和 28 個 LLM 的一般能力評估資料集進行廣泛的實驗。實驗結果顯示，Hum 將六個 LLM 的 NLU 能力平均提升了 3.1%，而其他一般能力則沒有顯著下降。

##### **A comprehensive survey of contemporary Arabic sentiment analysis: Methods, Challenges, and Future Directions**
2502.03827v1 by Zhiqiang Shi, Ruchit Agrawal

Sentiment Analysis, a popular subtask of Natural Language Processing, employs
computational methods to extract sentiment, opinions, and other subjective
aspects from linguistic data. Given its crucial role in understanding human
sentiment, research in sentiment analysis has witnessed significant growth in
the recent years. However, the majority of approaches are aimed at the English
language, and research towards Arabic sentiment analysis remains relatively
unexplored. This paper presents a comprehensive and contemporary survey of
Arabic Sentiment Analysis, identifies the challenges and limitations of
existing literature in this field and presents avenues for future research. We
present a systematic review of Arabic sentiment analysis methods, focusing
specifically on research utilizing deep learning. We then situate Arabic
Sentiment Analysis within the broader context, highlighting research gaps in
Arabic sentiment analysis as compared to general sentiment analysis. Finally,
we outline the main challenges and promising future directions for research in
Arabic sentiment analysis.

摘要：情感分析是自然語言處理中一個熱門的子任務，它採用計算方法從語言數據中提取情感、觀點和其他主觀方面。鑑於它在理解人類情感中至關重要的作用，近幾年來情感分析的研究見證了顯著的增長。然而，大多數方法都是針對英語的，而對阿拉伯語情感分析的研究仍然相對未被探索。本文對阿拉伯語情感分析進行了全面而當代的調查，找出該領域現有文獻的挑戰和局限性，並提出了未來研究的途徑。我們對阿拉伯語情感分析方法進行了系統的回顧，特別關注利用深度學習的研究。然後，我們將阿拉伯語情感分析置於更廣泛的背景中，強調阿拉伯語情感分析與一般情感分析相比的研究差距。最後，我們概述了阿拉伯語情感分析研究的主要挑戰和有希望的未來方向。

##### **Syntriever: How to Train Your Retriever with Synthetic Data from LLMs**
2502.03824v1 by Minsang Kim, Seungjun Baek

LLMs have boosted progress in many AI applications. Recently, there were
attempts to distill the vast knowledge of LLMs into information retrieval
systems. Those distillation methods mostly use output probabilities of LLMs
which are unavailable in the latest black-box LLMs. We propose Syntriever, a
training framework for retrievers using synthetic data from black-box LLMs.
Syntriever consists of two stages. Firstly in the distillation stage, we
synthesize relevant and plausibly irrelevant passages and augmented queries
using chain-of-thoughts for the given queries. LLM is asked to self-verify the
synthetic data for possible hallucinations, after which retrievers are trained
with a loss designed to cluster the embeddings of relevant passages. Secondly
in the alignment stage, we align the retriever with the preferences of LLMs. We
propose a preference modeling called partial Plackett-Luce ranking to learn LLM
preferences with regularization which prevents the model from deviating
excessively from that trained in the distillation stage. Experiments show that
Syntriever achieves state-of-the-art performances on benchmark datasets from
various domains in nDCG@$K$. The code is available at
\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.

摘要：大型語言模型 (LLM) 促进了許多 AI 應用程式的進展。最近，有人嘗試將 LLM 的龐大知識提煉到資訊檢索系統中。這些提煉方法大多使用 LLM 的輸出機率，而這些機率在最新的黑盒 LLM 中並不可用。我們提出了 Syntriever，一個使用來自黑盒 LLM 的合成資料訓練檢索器的框架。Syntriever 包含兩個階段。首先，在提煉階段，我們使用思想鏈為給定的查詢合成相關且看似不相關的段落和擴充查詢。LLM 會被要求自行驗證合成資料是否有可能的幻覺，然後使用旨在將相關段落的嵌入式分群的損失函數訓練檢索器。其次，在對齊階段，我們將檢索器與 LLM 的偏好對齊。我們提出了一種偏好模型，稱為部分 Plackett-Luce 排名，以學習 LLM 偏好，並使用正則化來防止模型過度偏離在提煉階段訓練的模型。實驗表明，Syntriever 在來自各種網域的基準資料集上達到了 nDCG@$K$ 的最先進效能。程式碼可在
\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever} 取得。

##### **PsyPlay: Personality-Infused Role-Playing Conversational Agents**
2502.03821v1 by Tao Yang, Yuhua Zhu, Xiaojun Quan, Cong Liu, Qifan Wang

The current research on Role-Playing Conversational Agents (RPCAs) with Large
Language Models (LLMs) primarily focuses on imitating specific speaking styles
and utilizing character backgrounds, neglecting the depiction of deeper
personality traits.~In this study, we introduce personality-infused
role-playing for LLM agents, which encourages agents to accurately portray
their designated personality traits during dialogues. We then propose PsyPlay,
a dialogue generation framework that facilitates the expression of rich
personalities among multiple LLM agents. Specifically, PsyPlay enables agents
to assume roles with distinct personality traits and engage in discussions
centered around specific topics, consistently exhibiting their designated
personality traits throughout the interactions. Validation on generated
dialogue data demonstrates that PsyPlay can accurately portray the intended
personality traits, achieving an overall success rate of 80.31% on GPT-3.5.
Notably, we observe that LLMs aligned with positive values are more successful
in portraying positive personality roles compared to negative ones. Moreover,
we construct a dialogue corpus for personality-infused role-playing, called
PsyPlay-Bench. The corpus, which consists of 4745 instances of correctly
portrayed dialogues using PsyPlay, aims to further facilitate research in
personalized role-playing and dialogue personality detection.

摘要：目前關於具備大型語言模型 (LLM) 的角色扮演對話代理 (RPCA) 的研究，主要集中於模仿特定的說話風格和利用角色背景，卻忽略了對更深層人格特質的描繪。在這項研究中，我們引入了為 LLM 代理注入人格的角色扮演，鼓勵代理在對話中準確地描繪他們指定的人格特質。然後我們提出了 PsyPlay，這是一個對話生成架構，可以促進多個 LLM 代理之間豐富人格的表達。具體來說，PsyPlay 能讓代理扮演具有不同人格特質的角色，並參與以特定主題為中心的討論，在整個互動過程中始終展現他們指定的人格特質。對生成的對話數據的驗證表明，PsyPlay 可以準確地描繪預期的人格特質，在 GPT-3.5 上實現了 80.31% 的整體成功率。值得注意的是，我們觀察到與正面價值觀一致的 LLM 在描繪正面人格角色方面比描繪負面人格角色更成功。此外，我們構建了一個名為 PsyPlay-Bench 的用於注入人格的角色扮演的對話語料庫。該語料庫由使用 PsyPlay 正確描繪對話的 4745 個實例組成，旨在進一步促進個性化角色扮演和對話人格檢測的研究。

##### **Large Language Models for Multi-Robot Systems: A Survey**
2502.03814v1 by Peihan Li, Zijian An, Shams Abrar, Lifeng Zhou

The rapid advancement of Large Language Models (LLMs) has opened new
possibilities in Multi-Robot Systems (MRS), enabling enhanced communication,
task planning, and human-robot interaction. Unlike traditional single-robot and
multi-agent systems, MRS poses unique challenges, including coordination,
scalability, and real-world adaptability. This survey provides the first
comprehensive exploration of LLM integration into MRS. It systematically
categorizes their applications across high-level task allocation, mid-level
motion planning, low-level action generation, and human intervention. We
highlight key applications in diverse domains, such as household robotics,
construction, formation control, target tracking, and robot games, showcasing
the versatility and transformative potential of LLMs in MRS. Furthermore, we
examine the challenges that limit adapting LLMs in MRS, including mathematical
reasoning limitations, hallucination, latency issues, and the need for robust
benchmarking systems. Finally, we outline opportunities for future research,
emphasizing advancements in fine-tuning, reasoning techniques, and
task-specific models. This survey aims to guide researchers in the intelligence
and real-world deployment of MRS powered by LLMs. Based on the fast-evolving
nature of research in the field, we keep updating the papers in the open-source
Github repository.

摘要：大型語言模型 (LLM) 的快速進展為多機器人系統 (MRS) 開啟了新的可能性，實現了增強的通信、任務規劃和人機交互。與傳統的單機器人和多智能體系統不同，MRS 提出了一些獨特的挑戰，包括協調、可擴展性和現實世界的適應性。這項調查首次全面探討了 LLM 與 MRS 的整合。它系統性地分類了它們在高層次任務分配、中層次運動規劃、低層次動作生成和人類干預中的應用。我們重點介紹了家庭機器人、建築、編隊控制、目標跟蹤和機器人遊戲等不同領域的關鍵應用，展示了 LLM 在 MRS 中的多功能性和變革潛力。此外，我們探討了限制 LLM 在 MRS 中適應的挑戰，包括數學推理限制、幻覺、延遲問題和對健壯基準系統的需求。最後，我們概述了未來研究的機會，強調微調、推理技術和特定任務模型的進展。這項調查旨在指導研究人員在 LLM 驅動的 MRS 的智能和現實世界部署中。基於該領域研究的快速發展，我們會持續更新開源 Github 儲存庫中的論文。

##### **Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective**
2502.03805v1 by Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S Kevin Zhou

Large language models have revolutionized natural language processing but
face significant challenges of high storage and runtime costs, due to the
transformer architecture's reliance on self-attention, particularly the large
Key-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV
cache size by pruning less critical entries based on attention weights remain
empirical and lack formal grounding. This paper presents a formal study on
identifying critical KV cache entries by analyzing attention output
perturbation. Our analysis reveals that, beyond attention weights, the value
states within KV entries and pretrained parameter matrices are also crucial.
Based on this, we propose a perturbation-constrained selection algorithm that
optimizes the worst-case output perturbation to identify critical entries.
Evaluations on the Needle-in-a-Haystack test and Longbench benchmark show our
algorithm enhances state-of-the-art cache eviction methods. Further empirical
analysis confirms that our algorithm achieves lower output perturbations in
over 92% attention heads in Llama model, thereby providing a significant
improvement over existing methods.

摘要：大型語言模型已經徹底改變了自然語言處理，但由於Transformer架構依賴於自我注意，特別是長序列推論的大型鍵值 (KV) 快取，因此面臨著儲存和執行時間成本高的重大挑戰。最近透過根據注意力權重來修剪較不重要的條目以減少 KV 快取大小的努力仍然是經驗性的，並且缺乏正式的依據。本文提出了一項正式的研究，透過分析注意力輸出擾動來識別重要的 KV 快取條目。我們的分析顯示，除了注意力權重之外，KV 條目中的值狀態和預訓練參數矩陣也至關重要。基於此，我們提出了一種擾動約束選擇演算法，它最佳化最差情況的輸出擾動以識別重要的條目。在 Needle-in-a-Haystack 測試和 Longbench 基準上的評估顯示，我們的演算法增強了最先進的快取驅逐方法。進一步的經驗分析證實，我們的演算法在 Llama 模型中超過 92% 的注意力頭中達到了較低的輸出擾動，從而提供了對現有方法的顯著改進。

##### **Understanding and Supporting Formal Email Exchange by Answering AI-Generated Questions**
2502.03804v1 by Yusuke Miura, Chi-Lan Yang, Masaki Kuribayashi, Keigo Matsumoto, Hideaki Kuzuoka, Shigeo Morishima

Replying to formal emails is time-consuming and cognitively demanding, as it
requires polite phrasing and ensuring an adequate response to the sender's
demands. Although systems with Large Language Models (LLM) were designed to
simplify the email replying process, users still needed to provide detailed
prompts to obtain the expected output. Therefore, we proposed and evaluated an
LLM-powered question-and-answer (QA)-based approach for users to reply to
emails by answering a set of simple and short questions generated from the
incoming email. We developed a prototype system, ResQ, and conducted controlled
and field experiments with 12 and 8 participants. Our results demonstrated that
QA-based approach improves the efficiency of replying to emails and reduces
workload while maintaining email quality compared to a conventional
prompt-based approach that requires users to craft appropriate prompts to
obtain email drafts. We discuss how QA-based approach influences the email
reply process and interpersonal relationship dynamics, as well as the
opportunities and challenges associated with using a QA-based approach in
AI-mediated communication.

摘要：回覆正式的電子郵件會耗時且在認知上要求很高，因為它需要有禮貌的措辭並確保對發件人的要求做出適當的回應。儘管具備大型語言模型 (LLM) 的系統被設計用於簡化電子郵件回覆程序，但使用者仍然需要提供詳細的提示才能獲得預期的輸出。因此，我們提出並評估了一種由 LLM 驅動、基於問答 (QA) 的方法，讓使用者透過回答從收到的電子郵件中產生的簡單且簡短的問題來回覆電子郵件。我們開發了一個原型系統 ResQ，並與 12 和 8 位參與者進行了受控的和現場實驗。我們的結果表明，與需要使用者製作適當的提示來取得電子郵件草稿的傳統基於提示的方法相比，基於 QA 的方法提高了回覆電子郵件的效率，並減少了工作量，同時維持電子郵件品質。我們討論了基於 QA 的方法如何影響電子郵件回覆程序和人際關係動態，以及在 AI 媒介的溝通中使用基於 QA 的方法相關的機會和挑戰。

##### **SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning**
2502.03801v1 by Heyi Zhang, Yule Liu, Xinlei He, Jun Wu, Tianshuo Cong, Xinyi Huang

Federated learning (FL) enables collaborative model training while preserving
data privacy, but its decentralized nature exposes it to client-side data
poisoning attacks (DPAs) and model poisoning attacks (MPAs) that degrade global
model performance. While numerous proposed defenses claim substantial
effectiveness, their evaluation is typically done in isolation with limited
attack strategies, raising concerns about their validity. Additionally,
existing studies overlook the mutual effectiveness of defenses against both
DPAs and MPAs, causing fragmentation in this field. This paper aims to provide
a unified benchmark and analysis of defenses against DPAs and MPAs, clarifying
the distinction between these two similar but slightly distinct domains. We
present a systematic taxonomy of poisoning attacks and defense strategies,
outlining their design, strengths, and limitations. Then, a unified comparative
evaluation across FL algorithms and data heterogeneity is conducted to validate
their individual and mutual effectiveness and derive key insights for design
principles and future research. Along with the analysis, we frame our work to a
unified benchmark, FLPoison, with high modularity and scalability to evaluate
15 representative poisoning attacks and 17 defense strategies, facilitating
future research in this domain. Code is available at
https://github.com/vio1etus/FLPoison.

摘要：联邦学习 (FL) 可以在保护数据隐私的同时进行协作模型训练，但其分散的特性使其面临客户端数据中毒攻击 (DPA) 和模型中毒攻击 (MPA)，这些攻击会降低全局模型性能。虽然许多提议的防御措施声称具有实质性的有效性，但它们的评估通常是在隔离的情况下进行的，攻击策略有限，这引发了对其有效性的担忧。此外，现有研究忽略了防御措施对 DPA 和 MPA 的相互有效性，导致该领域的碎片化。本文旨在提供针对 DPA 和 MPA 的防御措施的统一基准和分析，阐明这两个相似但略有不同的领域之间的区别。我们提出了中毒攻击和防御策略的系统分类法，概述了它们的设计、优势和局限性。然后，对跨 FL 算法和数据异构性的统一比较评估进行了验证，以验证它们的个体和相互有效性，并得出设计原则和未来研究的关键见解。随着分析，我们将我们的工作框架化为一个统一的基准 FLPoison，它具有高度的模块化和可扩展性，以评估 15 种代表性中毒攻击和 17 种防御策略，促进该领域的未来研究。代码可在 https://github.com/vio1etus/FLPoison 获得。

##### **Enhancing Hallucination Detection through Noise Injection**
2502.03799v1 by Litian Liu, Reza Pourreza, Sunny Panchal, Apratim Bhattacharyya, Yao Qin, Roland Memisevic

Large Language Models (LLMs) are prone to generating plausible yet incorrect
responses, known as hallucinations. Effectively detecting hallucinations is
therefore crucial for the safe deployment of LLMs. Recent research has linked
hallucinations to model uncertainty, suggesting that hallucinations can be
detected by measuring dispersion over answer distributions obtained from a set
of samples drawn from a model. While drawing from the distribution over tokens
defined by the model is a natural way to obtain samples, in this work, we argue
that it is sub-optimal for the purpose of detecting hallucinations. We show
that detection can be improved significantly by taking into account model
uncertainty in the Bayesian sense. To this end, we propose a very simple and
efficient approach that perturbs an appropriate subset of model parameters, or
equivalently hidden unit activations, during sampling. We demonstrate its
effectiveness across a wide range of datasets and model architectures.

摘要：大型語言模型 (LLM) 容易產生看似合理但錯誤的回應，稱為幻覺。因此，有效地偵測幻覺對於 LLM 的安全部署至關重要。最近的研究已將幻覺與模型不確定性連結起來，這表示幻覺可以透過測量從模型抽取的一組樣本獲得的答案分佈中的離散度來偵測。雖然從模型定義的符號分佈中抽取是取得樣本的自然方式，但在這項工作中，我們主張這對於偵測幻覺來說並非最佳選擇。我們顯示，透過考量貝氏意義中的模型不確定性，可以顯著改善偵測。為此，我們提出了一個非常簡單且有效率的方法，在取樣過程中擾動模型參數或等效的隱藏單元活化的適當子集。我們展示了它在各種資料集和模型架構中的有效性。

##### **It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers**
2502.03793v1 by Benjamin Clavié, Nathan Cooper, Benjamin Warner

While encoder-only models such as BERT and ModernBERT are ubiquitous in
real-world NLP applications, their conventional reliance on task-specific
classification heads can limit their applicability compared to decoder-based
large language models (LLMs). In this work, we introduce
ModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its
masked language modelling (MLM) head for generative classification. Our
approach employs an intentionally simple training loop and inference mechanism
that requires no heavy pre-processing, heavily engineered prompting, or
architectural modifications. ModernBERT-Large-Instruct exhibits strong
zero-shot performance on both classification and knowledge-based tasks,
outperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's
MMLU performance with 60% less parameters. We also demonstrate that, when
fine-tuned, the generative approach using the MLM head matches or even
surpasses traditional classification-head methods across diverse NLU tasks.This
capability emerges specifically in models trained on contemporary, diverse data
mixes, with models trained on lower volume, less-diverse data yielding
considerably weaker performance. Although preliminary, these results
demonstrate the potential of using the original generative masked language
modelling head over traditional task-specific heads for downstream tasks. Our
work suggests that further exploration into this area is warranted,
highlighting many avenues for future improvements.

摘要：<paragraph>雖然只有編碼器的模型，例如 BERT 和 ModernBERT，在真實世界的自然語言處理應用中無所不在，但它們傳統上依賴於特定任務分類標題，與基於解碼器的大型語言模型 (LLM) 相比，它們的適用性可能會受到限制。在這項工作中，我們引入了 ModernBERT-Large-Instruct，一個 0.4B 參數編碼器模型，它利用其遮罩語言建模 (MLM) 標題進行生成分類。我們的做法採用了一個故意簡單的訓練迴圈和推理機制，不需要繁重的預處理、大量設計提示或架構修改。ModernBERT-Large-Instruct 在分類和基於知識的任務上都表現出強大的零次學習性能，在 MMLU 上優於類似大小的 LLM，並以少 60% 的參數實現了 Llama3-1B 的 93% MMLU 性能。我們還證明，在微調時，使用 MLM 標題的生成方法在不同的 NLU 任務中與傳統的分類標題方法相匹配甚至超越它們。這種能力特別出現在訓練於當代、多樣化數據混合的模型中，而訓練於較低容量、較不多樣化的數據的模型則產生相當弱的性能。儘管是初步的，但這些結果證明了在下游任務中使用原始生成遮罩語言建模標題而不是傳統的特定任務標題的潛力。我們的研究表明，有必要進一步探索這個領域，並強調了未來改進的許多途徑。</paragraph>

##### **ExpProof : Operationalizing Explanations for Confidential Models with ZKPs**
2502.03773v1 by Chhavi Yadav, Evan Monroe Laufer, Dan Boneh, Kamalika Chaudhuri

In principle, explanations are intended as a way to increase trust in machine
learning models and are often obligated by regulations. However, many
circumstances where these are demanded are adversarial in nature, meaning the
involved parties have misaligned interests and are incentivized to manipulate
explanations for their purpose. As a result, explainability methods fail to be
operational in such settings despite the demand \cite{bordt2022post}. In this
paper, we take a step towards operationalizing explanations in adversarial
scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive.
Specifically we explore ZKP-amenable versions of the popular explainability
algorithm LIME and evaluate their performance on Neural Networks and Random
Forests.

摘要：原則上，解釋旨在作為一種增加對機器學習模型信任的方式，且通常受到法規約束。然而，許多需要解釋的環境本質上都是對抗性的，這意味著相關各方利益不一致，並有誘因操縱解釋以符合其目的。因此，儘管有需求，但可解釋性方法在這種環境中無法運作 \cite{bordt2022post}。在本文中，我們採取措施，利用零知識證明 (ZKPs)（一種密碼學原語）在對抗場景中實現解釋運作。具體來說，我們探討了廣泛使用的可解釋性演算法 LIME 的 ZKP 可用版本，並評估其在神經網路和隨機森林上的效能。

##### **A Retrospective Systematic Study on Hierarchical Sparse Query Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma**
2502.03772v1 by Chaoyin She, Ruifang Lu, Danni He, Jiayi Lv, Yadan Lin, Meiqing Cheng, Hui Huang, Lida Chen, Wei Wang, Qinghua Huang

Hepatocellular carcinoma (HCC) ranks as the third leading cause of
cancer-related mortality worldwide, with early detection being crucial for
improving patient survival rates. However, early screening for HCC using
ultrasound suffers from insufficient sensitivity and is highly dependent on the
expertise of radiologists for interpretation. Leveraging the latest
advancements in artificial intelligence (AI) in medical imaging, this study
proposes an innovative Hierarchical Sparse Query Transformer (HSQformer) model
that combines the strengths of Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs) to enhance the accuracy of HCC diagnosis in ultrasound
screening. The HSQformer leverages sparse latent space representations to
capture hierarchical details at various granularities without the need for
complex adjustments, and adopts a modular, plug-and-play design philosophy,
ensuring the model's versatility and ease of use. The HSQformer's performance
was rigorously tested across three distinct clinical scenarios: single-center,
multi-center, and high-risk patient testing. In each of these settings, it
consistently outperformed existing state-of-the-art models, such as ConvNext
and SwinTransformer. Notably, the HSQformer even matched the diagnostic
capabilities of senior radiologists and comprehensively surpassed those of
junior radiologists. The experimental results from this study strongly
demonstrate the effectiveness and clinical potential of AI-assisted tools in
HCC screening. The full code is available at
https://github.com/Asunatan/HSQformer.

摘要：肝細胞癌（HCC）是全球第三大癌症相關死亡原因，早期檢測對於提高患者存活率至關重要。然而，使用超音波進行 HCC 早期篩檢的靈敏度不足，且高度依賴放射科醫師的專業知識進行判讀。本研究利用醫學影像中人工智慧（AI）的最新進展，提出了一種創新的分層稀疏查詢Transformer（HSQformer）模型，結合了卷積神經網路（CNN）和視覺Transformer（ViT）的優點，以提高超音波篩檢中 HCC 診斷的準確性。HSQformer 利用稀疏潛在空間表示，在不需要複雜調整的情況下擷取各種粒度層級的細節，並採用模組化、即插即用的設計理念，確保模型的多功能性和易用性。HSQformer 的效能經過三個不同的臨床場景的嚴格測試：單中心、多中心和高風險患者測試。在這些設定中，它始終優於現有的最先進模型，例如 ConvNext 和 SwinTransformer。值得注意的是，HSQformer 甚至匹配了資深放射科醫師的診斷能力，並全面超越了初級放射科醫師的診斷能力。本研究的實驗結果有力地證明了 AI 輔助工具在 HCC 篩檢中的有效性和臨床潛力。完整程式碼可在 https://github.com/Asunatan/HSQformer 取得。

##### **Adaptive Semantic Prompt Caching with VectorQ**
2502.03771v1 by Luis Gaspar Schroeder, Shu Liu, Alejandro Cuadron, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez

Semantic prompt caches reduce the latency and cost of large language model
(LLM) inference by reusing cached LLM-generated responses for semantically
similar prompts. Vector similarity metrics assign a numerical score to quantify
the similarity between an embedded prompt and its nearest neighbor in the
cache. Existing systems rely on a static threshold to classify whether the
similarity score is sufficiently high to result in a cache hit. We show that
this one-size-fits-all threshold is insufficient across different prompts. We
propose VectorQ, a framework to learn embedding-specific threshold regions that
adapt to the complexity and uncertainty of an embedding. Through evaluations on
a combination of four diverse datasets, we show that VectorQ consistently
outperforms state-of-the-art systems across all static thresholds, achieving up
to 12x increases in cache hit rate and error rate reductions up to 92%.

摘要：語意提示快取透過重複使用快取的 LLM 生成的回應來降低大型語言模型 (LLM) 推論的延遲和成本，以取得語意上相似的提示。向量相似度量數會指派一個數值分數，以量化嵌入式提示與快取中最近鄰居之間的相似度。現有的系統依賴靜態閾值來分類相似度分數是否足夠高以產生快取命中。我們顯示這個一體適用的閾值不足以涵蓋不同的提示。我們提出 VectorQ，一個架構來學習適應嵌入式複雜性和不確定性的嵌入式特定閾值區域。透過評估四個不同資料集的組合，我們顯示 VectorQ 在所有靜態閾值上持續優於最先進的系統，快取命中率增加多達 12 倍，錯誤率降低多達 92%。

##### **Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models**
2502.03766v1 by Meiquan Dong, Haoran Liu, Yan Huang, Zixuan Feng, Jianhong Tang, Ruoxi Wang

The organization of latent token representations plays a crucial role in
determining the stability, generalization, and contextual consistency of
language models, yet conventional approaches to embedding refinement often rely
on parameter modifications that introduce additional computational overhead. A
hierarchical alignment method was introduced to restructure token embeddings
without altering core model weights, ensuring that representational
distributions maintained coherence across different linguistic contexts.
Experimental evaluations demonstrated improvements in rare token retrieval,
adversarial robustness, and long-range dependency tracking, highlighting the
advantages of hierarchical structuring in mitigating inconsistencies in latent
space organization. The comparative analysis against conventional fine-tuning
and embedding perturbation methods revealed that hierarchical restructuring
maintained computational efficiency while achieving measurable gains in
representation quality. Structural refinements introduced through the alignment
process resulted in improved contextual stability across varied linguistic
tasks, reducing inconsistencies in token proximity relationships and enhancing
interpretability in language generation. A detailed computational assessment
confirmed that the realignment process introduced minimal inference overhead,
ensuring that representational improvements did not compromise model
efficiency. The findings reinforced the broader significance of structured
representation learning, illustrating that hierarchical embedding modifications
could serve as an effective strategy for refining latent space distributions
while preserving pre-learned semantic associations.

摘要：潛在符號表徵的組織在決定語言模型的穩定性、概括性與脈絡一致性上扮演著至關重要的角色，然而，對嵌入式細緻化的傳統方法通常依賴於引入額外運算負擔的參數修改。引進了一種階層對齊方法來重新建構符號嵌入，而不會改變核心模型權重，確保表徵分佈在不同的語言脈絡中保持一致性。實驗評估顯示，在罕見符號檢索、對抗魯棒性與長距離依賴追蹤方面都有所改善，突顯了階層結構在減輕潛在空間組織不一致性方面的優勢。針對傳統微調與嵌入式擾動方法的比較分析顯示，階層重組維持了運算效率，同時在表徵品質方面獲得可衡量的提升。透過對齊程序引入的結構細緻化，改善了各種語言任務的脈絡穩定性，減少了符號接近關係的不一致性，並增強了語言生成的詮釋性。詳細的運算評估證實，重新對齊程序引入了最小的推論負擔，確保表徵改善不會損害模型效率。這些發現強化了結構化表徵學習的更廣泛意義，說明階層式嵌入修改可以作為細緻化潛在空間分佈的有效策略，同時保留預先學習的語義關聯。

##### **Principal Curvatures Estimation with Applications to Single Cell Data**
2502.03750v1 by Yanlei Zhang, Lydia Mezrag, Xingzhi Sun, Charles Xu, Kincaid Macdonald, Dhananjay Bhaskar, Smita Krishnaswamy, Guy Wolf, Bastian Rieck

The rapidly growing field of single-cell transcriptomic sequencing (scRNAseq)
presents challenges for data analysis due to its massive datasets. A common
method in manifold learning consists in hypothesizing that datasets lie on a
lower dimensional manifold. This allows to study the geometry of point clouds
by extracting meaningful descriptors like curvature. In this work, we will
present Adaptive Local PCA (AdaL-PCA), a data-driven method for accurately
estimating various notions of intrinsic curvature on data manifolds, in
particular principal curvatures for surfaces. The model relies on local PCA to
estimate the tangent spaces. The evaluation of AdaL-PCA on sampled surfaces
shows state-of-the-art results. Combined with a PHATE embedding, the model
applied to single-cell RNA sequencing data allows us to identify key variations
in the cellular differentiation.

摘要：單細胞轉錄組定序 (scRNAseq) 領域快速成長，由於其資料集龐大，為資料分析帶來挑戰。流形學習中一個常見的方法在於假設資料集位於較低維度的流形上。這允許透過擷取有意義的描述符（例如曲率）來研究點雲的幾何形狀。在這項工作中，我們將介紹自適應局部 PCA (AdaL-PCA)，一種資料驅動的方法，用於準確估計資料流形上內在曲率的不同概念，特別是曲面的主曲率。此模型依賴於局部 PCA 來估計切空間。AdaL-PCA 在取樣曲面上的評估顯示最先進的結果。與 PHATE 嵌入相結合，應用於單細胞 RNA 定序資料的模型使我們能夠識別細胞分化中的關鍵變化。

##### **Rethinking the Residual Distribution of Locate-then-Editing Methods in Model Editing**
2502.03748v1 by Xiaopeng Li, Shanwen Wang, Shasha Li, Shezheng Song, Bin Ji, Jun Ma, Jie Yu

Model editing is a powerful technique for updating the knowledge of Large
Language Models (LLMs). Locate-then-edit methods are a popular class of
approaches that first identify the critical layers storing knowledge, then
compute the residual of the last critical layer based on the edited knowledge,
and finally perform multi-layer updates using a least-squares solution by
evenly distributing the residual from the first critical layer to the last.
Although these methods achieve promising results, they have been shown to
degrade the original knowledge of LLMs. We argue that residual distribution
leads to this issue. To explore this, we conduct a comprehensive analysis of
residual distribution in locate-then-edit methods from both empirical and
theoretical perspectives, revealing that residual distribution introduces
editing errors, leading to inaccurate edits. To address this issue, we propose
the Boundary Layer UpdatE (BLUE) strategy to enhance locate-then-edit methods.
Sequential batch editing experiments on three LLMs and two datasets demonstrate
that BLUE not only delivers an average performance improvement of 35.59\%,
significantly advancing the state of the art in model editing, but also
enhances the preservation of LLMs' general capabilities. Our code is available
at https://github.com/xpq-tech/BLUE.

摘要：模型編輯是一種更新大型語言模型 (LLM) 知識的強大技術。定位再編輯方法是一類流行的方法，它會先找出儲存知識的關鍵層，然後根據編輯過的知識計算最後一個關鍵層的殘差，最後使用最小平方解法執行多層更新，將殘差從第一個關鍵層均勻分配到最後一個關鍵層。儘管這些方法取得了有希望的結果，但它們已被證明會降低 LLM 的原始知識。我們認為殘差分配會導致這個問題。為了探討這一點，我們從經驗和理論的角度對定位再編輯方法中的殘差分配進行了全面的分析，揭示殘差分配會引入編輯錯誤，導致編輯不準確。為了解決這個問題，我們提出了邊界層更新 (BLUE) 策略來增強定位再編輯方法。在三個 LLM 和兩個資料集上進行的順序批次編輯實驗證明，BLUE 不僅提供了 35.59% 的平均效能提升，顯著提升了模型編輯的最新技術，還增強了 LLM 一般功能的保留。我們的程式碼可在 https://github.com/xpq-tech/BLUE 取得。

##### **Action-Free Reasoning for Policy Generalization**
2502.03729v1 by Jaden Clark, Suvir Mirchandani, Dorsa Sadigh, Suneel Belkhale

End-to-end imitation learning offers a promising approach for training robot
policies. However, generalizing to new settings remains a significant
challenge. Although large-scale robot demonstration datasets have shown
potential for inducing generalization, they are resource-intensive to scale. In
contrast, human video data is abundant and diverse, presenting an attractive
alternative. Yet, these human-video datasets lack action labels, complicating
their use in imitation learning. Existing methods attempt to extract grounded
action representations (e.g., hand poses), but resulting policies struggle to
bridge the embodiment gap between human and robot actions. We propose an
alternative approach: leveraging language-based reasoning from human
videos-essential for guiding robot actions-to train generalizable robot
policies. Building on recent advances in reasoning-based policy architectures,
we introduce Reasoning through Action-free Data (RAD). RAD learns from both
robot demonstration data (with reasoning and action labels) and action-free
human video data (with only reasoning labels). The robot data teaches the model
to map reasoning to low-level actions, while the action-free data enhances
reasoning capabilities. Additionally, we will release a new dataset of 3,377
human-hand demonstrations with reasoning annotations compatible with the Bridge
V2 benchmark and aimed at facilitating future research on reasoning-driven
robot learning. Our experiments show that RAD enables effective transfer across
the embodiment gap, allowing robots to perform tasks seen only in action-free
data. Furthermore, scaling up action-free reasoning data significantly improves
policy performance and generalization to novel tasks. These results highlight
the promise of reasoning-driven learning from action-free datasets for
advancing generalizable robot control. Project page:
https://rad-generalization.github.io

摘要：端對端模仿學習提供了一種訓練機器人政策的有前景的方法。然而，推廣到新的設定仍然是一個重大的挑戰。儘管大規模機器人示範數據集已顯示出誘導概括的潛力，但它們在擴展方面需要大量資源。相比之下，人類視頻數據豐富且多樣，提供了一個有吸引力的替代方案。然而，這些人類視頻數據集缺乏動作標籤，使它們在模仿學習中的使用變得複雜。現有方法嘗試提取接地的動作表示（例如，手部姿勢），但由此產生的策略難以彌合人類和機器人動作之間的具體化差距。我們提出了一種替代方法：利用人類視頻中基於語言的推理——指導機器人動作的必要條件——來訓練可概括的機器人策略。在基於推理的策略架構的最新進展的基礎上，我們引入了無動作數據推理 (RAD)。RAD 從機器人示範數據（帶有推理和動作標籤）和無動作人類視頻數據（僅帶有推理標籤）中學習。機器人數據教導模型將推理映射到低級動作，而無動作數據增強了推理能力。此外，我們將發布一個新的數據集，其中包含 3,377 個帶有推理註釋的人手示範，這些註釋與 Bridge V2 基準兼容，旨在促進未來對基於推理的機器人學習的研究。我們的實驗表明，RAD 能夠有效地跨越具體化差距，讓機器人能夠執行僅在無動作數據中看到的任務。此外，擴展無動作推理數據顯著提高了策略性能和對新任務的概括。這些結果突出了無動作數據集的基於推理的學習在推進可概括機器人控制方面的前景。項目頁面：https://rad-generalization.github.io

##### **MD-BERT: Action Recognition in Dark Videos via Dynamic Multi-Stream Fusion and Temporal Modeling**
2502.03724v1 by Sharana Dharshikgan Suresh Dass, Hrishav Bakul Barua, Ganesh Krishnasamy, Raveendran Paramesran, Raphael C. -W. Phan

Action recognition in dark, low-light (under-exposed) or noisy videos is a
challenging task due to visibility degradation, which can hinder critical
spatiotemporal details. This paper proposes MD-BERT, a novel multi-stream
approach that integrates complementary pre-processing techniques such as gamma
correction and histogram equalization alongside raw dark frames to address
these challenges. We introduce the Dynamic Feature Fusion (DFF) module,
extending existing attentional fusion methods to a three-stream setting,
thereby capturing fine-grained and global contextual information across
different brightness and contrast enhancements. The fused spatiotemporal
features are then processed by a BERT-based temporal model, which leverages its
bidirectional self-attention to effectively capture long-range dependencies and
contextual relationships across frames. Extensive experiments on the ARID V1.0
and ARID V1.5 dark video datasets show that MD-BERT outperforms existing
methods, establishing a new state-of-the-art performance. Ablation studies
further highlight the individual contributions of each input stream and the
effectiveness of the proposed DFF and BERT modules. The official website of
this work is available at: https://github.com/HrishavBakulBarua/DarkBERT

摘要：在黑暗、低光（曝光不足）或有雜訊的影片中進行動作辨識是一項具有挑戰性的任務，因為能見度降低，可能會阻礙關鍵的時空細節。這篇論文提出 MD-BERT，這是一種新穎的多串流方法，它整合了補充性的前處理技術，例如伽瑪校正和直方圖等化，以及原始的黑暗影像，以應對這些挑戰。我們引入了動態特徵融合 (DFF) 模組，將現有的注意力融合方法擴充套件到三串流設定，從而跨越不同的亮度和對比度增強來捕捉細粒度和全域性的脈絡資訊。接著，融合的時空特徵會由基於 BERT 的時序模型處理，它利用其雙向自我注意力來有效捕捉長距離的依存關係和影像之間的脈絡關係。在 ARID V1.0 和 ARID V1.5 黑暗影片資料集上進行的廣泛實驗顯示，MD-BERT 優於現有的方法，樹立了新的最先進效能。消融研究進一步突顯了每個輸入串流的個別貢獻，以及所提出的 DFF 和 BERT 模組的效能。這項工作的官方網站可在以下網址取得：https://github.com/HrishavBakulBarua/DarkBERT

##### **Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning**
2502.03717v1 by Jaden Clark, Joey Hejna, Dorsa Sadigh

Expressive robotic behavior is essential for the widespread acceptance of
robots in social environments. Recent advancements in learned legged locomotion
controllers have enabled more dynamic and versatile robot behaviors. However,
determining the optimal behavior for interactions with different users across
varied scenarios remains a challenge. Current methods either rely on natural
language input, which is efficient but low-resolution, or learn from human
preferences, which, although high-resolution, is sample inefficient. This paper
introduces a novel approach that leverages priors generated by pre-trained LLMs
alongside the precision of preference learning. Our method, termed
Language-Guided Preference Learning (LGPL), uses LLMs to generate initial
behavior samples, which are then refined through preference-based feedback to
learn behaviors that closely align with human expectations. Our core insight is
that LLMs can guide the sampling process for preference learning, leading to a
substantial improvement in sample efficiency. We demonstrate that LGPL can
quickly learn accurate and expressive behaviors with as few as four queries,
outperforming both purely language-parameterized models and traditional
preference learning approaches. Website with videos:
https://lgpl-gaits.github.io/

摘要：表達性的機器人行為對於機器人在社交環境中的廣泛接受至關重要。最近在學習的腿部運動控制器的進步已經實現了更動態和多樣化的機器人行為。然而，確定與不同使用者在不同場景中互動的最佳行為仍然是一個挑戰。當前的辦法依賴於自然語言輸入，這種輸入效率高但解析度低，或者從人類偏好中學習，這種偏好雖然解析度高，但樣本效率低。本文介紹了一種新方法，該方法利用預先訓練的 LLM 生成的先驗以及偏好學習的精確性。我們的辦法稱為語言引導偏好學習 (LGPL)，它使用 LLM 生成初始行為樣本，然後通過基於偏好的反饋對其進行優化，以學習與人類期望緊密一致的行為。我們的核心見解是，LLM 可以指導偏好學習的採樣過程，從而大幅提高樣本效率。我們證明 LGPL 可以通過僅四次查詢快速學習準確且富有表現力的行為，從而優於純語言參數化模型和傳統偏好學習方法。包含影片的網站：https://lgpl-gaits.github.io/

##### **Boosting Knowledge Graph-based Recommendations through Confidence-Aware Augmentation with Large Language Models**
2502.03715v1 by Rui Cai, Chao Wang, Qianyi Cai, Dazhong Shen, Hui Xiong

Knowledge Graph-based recommendations have gained significant attention due
to their ability to leverage rich semantic relationships. However, constructing
and maintaining Knowledge Graphs (KGs) is resource-intensive, and the accuracy
of KGs can suffer from noisy, outdated, or irrelevant triplets. Recent
advancements in Large Language Models (LLMs) offer a promising way to improve
the quality and relevance of KGs for recommendation tasks. Despite this,
integrating LLMs into KG-based systems presents challenges, such as efficiently
augmenting KGs, addressing hallucinations, and developing effective joint
learning methods. In this paper, we propose the Confidence-aware KG-based
Recommendation Framework with LLM Augmentation (CKG-LLMA), a novel framework
that combines KGs and LLMs for recommendation task. The framework includes: (1)
an LLM-based subgraph augmenter for enriching KGs with high-quality
information, (2) a confidence-aware message propagation mechanism to filter
noisy triplets, and (3) a dual-view contrastive learning method to integrate
user-item interactions and KG data. Additionally, we employ a confidence-aware
explanation generation process to guide LLMs in producing realistic
explanations for recommendations. Finally, extensive experiments demonstrate
the effectiveness of CKG-LLMA across multiple public datasets.

摘要：基於知識圖譜的推薦因其利用豐富語義關係的能力而備受關注。然而，構建和維護知識圖譜 (KG) 是一項資源密集型任務，而 KG 的準確性可能會受到雜訊、過時或無關的三元組的影響。大型語言模型 (LLM) 的最新進展為提高 KG 在推薦任務中的品質和相關性提供了一種有前途的方法。儘管如此，將 LLM 整合到基於 KG 的系統中會帶來挑戰，例如有效擴充 KG、處理幻覺，以及開發有效的聯合學習方法。在本文中，我們提出具有 LLM 擴充的信心感知型基於 KG 的推薦框架 (CKG-LLMA)，這是一個結合 KG 和 LLM 進行推薦任務的新穎框架。該框架包括：(1) 一個基於 LLM 的子圖擴充器，用於使用高品質資訊豐富 KG，(2) 一個信心感知型訊息傳播機制，用於過濾雜訊三元組，以及 (3) 一個雙視圖對比學習方法，用於整合使用者-項目互動和 KG 資料。此外，我們採用一個信心感知型解釋產生程序，以引導 LLM 為推薦產生逼真的解釋。最後，大量的實驗證明了 CKG-LLMA 在多個公開資料集中的有效性。

##### **MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers**
2502.03711v1 by Nicole Cho, William Watson

One critical challenge in the institutional adoption journey of Large
Language Models (LLMs) stems from their propensity to hallucinate in generated
responses. To address this, we propose MultiQ&A, a systematic approach for
evaluating the robustness and consistency of LLM-generated answers. We
demonstrate MultiQ&A's ability to crowdsource question perturbations and their
respective answers through independent LLM agents at scale. Our experiments
culminated in the examination of 1.9 million question perturbations and 2.3
million answers. Furthermore, MultiQ&A shows that ensembled LLMs, such as
gpt-3.5-turbo, remain relatively robust and consistent under perturbations.
MultiQ&A provides clarity in the response generation space, offering an
effective method for inspecting disagreements and variability. Therefore, our
system offers a potential framework for institutional LLM adoption with the
ability to measure confidence, consistency, and the quantification of
hallucinations.

摘要：在大型語言模型 (LLM) 的機構採用過程中，一個關鍵挑戰來自於它們在產生的回應中容易出現幻覺。為了解決這個問題，我們提出了 MultiQ&A，一種用於評估 LLM 產生的答案的健壯性和一致性的系統性方法。我們展示了 MultiQ&A 能夠大規模地透過獨立的 LLM 代理來眾包問題擾動及其各自的答案。我們的實驗最終檢驗了 190 萬個問題擾動和 230 萬個答案。此外，MultiQ&A 顯示，集合的 LLM（例如 gpt-3.5-turbo）在擾動下仍然相對健壯且一致。MultiQ&A 在回應產生空間中提供了清晰度，提供了一種檢查分歧和變異性的有效方法。因此，我們的系統提供了一個潛在的機構 LLM 採用架構，能夠衡量信心、一致性和幻覺的量化。

##### **Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers**
2502.03708v1 by Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Mikhail Belkin

A trained Large Language Model (LLM) contains much of human knowledge. Yet,
it is difficult to gauge the extent or accuracy of that knowledge, as LLMs do
not always ``know what they know'' and may even be actively misleading. In this
work, we give a general method for detecting semantic concepts in the internal
activations of LLMs. Furthermore, we show that our methodology can be easily
adapted to steer LLMs toward desirable outputs. Our innovations are the
following: (1) we use a nonlinear feature learning method to identify important
linear directions for predicting concepts from each layer; (2) we aggregate
features across layers to build powerful concept detectors and steering
mechanisms. We showcase the power of our approach by attaining state-of-the-art
results for detecting hallucinations, harmfulness, toxicity, and untruthful
content on seven benchmarks. We highlight the generality of our approach by
steering LLMs towards new concepts that, to the best of our knowledge, have not
been previously considered in the literature, including: semantic
disambiguation, human languages, programming languages, hallucinated responses,
science subjects, poetic/Shakespearean English, and even multiple concepts
simultaneously. Moreover, our method can steer concepts with numerical
attributes such as product reviews. We provide our code (including a simple API
for our methods) at https://github.com/dmbeaglehole/neural_controllers .

摘要：經過訓練的大型語言模型 (LLM) 包含許多人類知識。然而，要衡量這些知識的廣度或準確性很困難，因為 LLM 並不總是「知道自己知道什麼」，甚至可能積極誤導。在這項工作中，我們提供了一種通用方法，用於在 LLM 的內部激活中偵測語義概念。此外，我們展示了我們的技術可以輕鬆地調整，以引導 LLM 朝向理想的輸出。我們的創新如下：(1) 我們使用非線性特徵學習方法來識別重要的線性方向，以從每一層預測概念；(2) 我們匯總各層的特徵，以建立強大的概念偵測器和導引機制。我們透過在七個基準上獲得偵測幻覺、有害性、毒性和虛假內容的最新結果，展示了我們方法的力量。我們透過引導 LLM 朝向新的概念，突顯了我們方法的普遍性，這些概念據我們所知，以前尚未在文獻中被考慮過，包括：語義消歧、人類語言、程式語言、幻覺反應、科學主題、詩意/莎士比亞英語，甚至同時包含多個概念。此外，我們的模型可以使用數值屬性（例如產品評論）來引導概念。我們在 https://github.com/dmbeaglehole/neural_controllers 提供我們的程式碼（包括我們方法的簡單 API）。

##### **LLM Alignment as Retriever Optimization: An Information Retrieval Perspective**
2502.03699v1 by Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik

Large Language Models (LLMs) have revolutionized artificial intelligence with
capabilities in reasoning, coding, and communication, driving innovation across
industries. Their true potential depends on effective alignment to ensure
correct, trustworthy and ethical behavior, addressing challenges like
misinformation, hallucinations, bias and misuse. While existing Reinforcement
Learning (RL)-based alignment methods are notoriously complex, direct
optimization approaches offer a simpler alternative. In this work, we introduce
a novel direct optimization approach for LLM alignment by drawing on
established Information Retrieval (IR) principles. We present a systematic
framework that bridges LLM alignment and IR methodologies, mapping LLM
generation and reward models to IR's retriever-reranker paradigm. Building on
this foundation, we propose LLM Alignment as Retriever Preference Optimization
(LarPO), a new alignment method that enhances overall alignment quality.
Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %
averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work
opens new avenues for advancing LLM alignment by integrating IR foundations,
offering a promising direction for future research.

摘要：大型語言模型 (LLM) 透過在推理、編碼和溝通方面的能力，徹底革新了人工智慧，並推動各產業的創新。其真正的潛力取決於有效對齊，以確保正確、值得信賴且符合道德的行為，並解決錯誤資訊、幻覺、偏見和誤用等挑戰。雖然現有的基於強化學習 (RL) 的對齊方法出了名的複雜，但直接最佳化方法提供了一個更簡單的替代方案。在這項工作中，我們透過採用既定的資訊檢索 (IR) 原則，介紹了一種新的 LLM 對齊直接最佳化方法。我們提出了一個系統化的架構，將 LLM 對齊和 IR 方法論聯繫起來，將 LLM 生成和獎勵模型對應到 IR 的檢索器重新排序範例。在此基礎上，我們提出 LLM 對齊作為檢索器偏好最佳化 (LarPO)，這是一種新的對齊方法，可增強整體對齊品質。大量的實驗驗證了 LarPO 的有效性，在 AlpacaEval2 和 MixEval-Hard 分別平均改善了 38.9% 和 13.7%。我們的研究透過整合 IR 基礎，為推進 LLM 對齊開闢了新的途徑，並為未來的研究提供了有希望的方向。

##### **DocMIA: Document-Level Membership Inference Attacks against DocVQA Models**
2502.03692v1 by Khanh Nguyen, Raouf Kerkouche, Mario Fritz, Dimosthenis Karatzas

Document Visual Question Answering (DocVQA) has introduced a new paradigm for
end-to-end document understanding, and quickly became one of the standard
benchmarks for multimodal LLMs. Automating document processing workflows,
driven by DocVQA models, presents significant potential for many business
sectors. However, documents tend to contain highly sensitive information,
raising concerns about privacy risks associated with training such DocVQA
models. One significant privacy vulnerability, exploited by the membership
inference attack, is the possibility for an adversary to determine if a
particular record was part of the model's training data. In this paper, we
introduce two novel membership inference attacks tailored specifically to
DocVQA models. These attacks are designed for two different adversarial
scenarios: a white-box setting, where the attacker has full access to the model
architecture and parameters, and a black-box setting, where only the model's
outputs are available. Notably, our attacks assume the adversary lacks access
to auxiliary datasets, which is more realistic in practice but also more
challenging. Our unsupervised methods outperform existing state-of-the-art
membership inference attacks across a variety of DocVQA models and datasets,
demonstrating their effectiveness and highlighting the privacy risks in this
domain.

摘要：文件視覺問答 (DocVQA) 引進了一種新的範例，用於端對端文件理解，並迅速成為多模態 LLM 的標準基準之一。由 DocVQA 模型驅動的自動化文件處理工作流程，對許多商業領域具有顯著的潛力。然而，文件往往包含高度敏感的資訊，引發了與訓練此類 DocVQA 模型相關的隱私風險的疑慮。一個重大的隱私漏洞，由成員資格推論攻擊所利用，是對手有可能確定特定記錄是否為模型訓練資料的一部分。在本文中，我們介紹了兩個針對 DocVQA 模型量身打造的新穎成員資格推論攻擊。這些攻擊被設計用於兩種不同的對抗場景：白盒設定，其中攻擊者可以完全存取模型架構和參數，以及黑盒設定，其中只能使用模型的輸出。值得注意的是，我們的攻擊假設對手無法存取輔助資料集，這在實務上更為實際，但挑戰性也更高。我們的無監督方法在各種 DocVQA 模型和資料集上勝過現有的最先進成員資格推論攻擊，證明了它們的有效性，並突顯了此領域的隱私風險。

##### **A Comparison of DeepSeek and Other LLMs**
2502.03688v1 by Tianchen Gao, Jiashun Jin, Zheng Tracy Ke, Gabriel Moryoussef

Recently, DeepSeek has been the focus of attention in and beyond the AI
community. An interesting problem is how DeepSeek compares to other large
language models (LLMs). There are many tasks an LLM can do, and in this paper,
we use the task of predicting an outcome using a short text for comparison. We
consider two settings, an authorship classification setting and a citation
classification setting. In the first one, the goal is to determine whether a
short text is written by human or AI. In the second one, the goal is to
classify a citation to one of four types using the textual content. For each
experiment, we compare DeepSeek with $4$ popular LLMs: Claude, Gemini, GPT, and
Llama.
  We find that, in terms of classification accuracy, DeepSeek outperforms
Gemini, GPT, and Llama in most cases, but underperforms Claude. We also find
that DeepSeek is comparably slower than others but with a low cost to use,
while Claude is much more expensive than all the others. Finally, we find that
in terms of similarity, the output of DeepSeek is most similar to those of
Gemini and Claude (and among all $5$ LLMs, Claude and Gemini have the most
similar outputs).
  In this paper, we also present a fully-labeled dataset collected by
ourselves, and propose a recipe where we can use the LLMs and a recent data
set, MADStat, to generate new data sets. The datasets in our paper can be used
as benchmarks for future study on LLMs.

摘要：<paragraph>最近，DeepSeek一直是人工智能社群内外关注的焦点。一个有趣的问题是DeepSeek与其他大型语言模型（LLM）相比如何。LLM可以完成许多任务，在本文中，我们使用使用简短文本预测结果的任务进行比较。我们考虑两种设置，一种是作者分类设置，另一种是引用分类设置。在第一个设置中，目标是确定简短文本是由人还是人工智能编写的。在第二个设置中，目标是使用文本内容将引用分类为四种类型之一。对于每个实验，我们将DeepSeek与4个流行的LLM进行比较：Claude、Gemini、GPT和Llama。
我们发现，在分类准确性方面，DeepSeek在大多数情况下优于Gemini、GPT和Llama，但在Claude面前表现不佳。我们还发现，DeepSeek比其他模型慢，但使用成本低，而Claude比其他模型贵得多。最后，我们发现，在相似性方面，DeepSeek的输出与Gemini和Claude的输出最相似（在所有5个LLM中，Claude和Gemini的输出最相似）。
在本文中，我们还展示了一个由我们自己收集的完全标记的数据集，并提出了一个配方，我们可以使用LLM和最近的数据集MADStat来生成新的数据集。我们论文中的数据集可以用作未来对LLM的研究基准。</paragraph>

##### **Variational Control for Guidance in Diffusion Models**
2502.03686v1 by Kushagra Pandey, Farrin Marouf Sofian, Felix Draxler, Theofanis Karaletsos, Stephan Mandt

Diffusion models exhibit excellent sample quality, but existing guidance
methods often require additional model training or are limited to specific
tasks. We revisit guidance in diffusion models from the perspective of
variational inference and control, introducing Diffusion Trajectory Matching
(DTM) that enables guiding pretrained diffusion trajectories to satisfy a
terminal cost. DTM unifies a broad class of guidance methods and enables novel
instantiations. We introduce a new method within this framework that achieves
state-of-the-art results on several linear and (blind) non-linear inverse
problems without requiring additional model training or modifications. For
instance, in ImageNet non-linear deblurring, our model achieves an FID score of
34.31, significantly improving over the best pretrained-method baseline (FID
78.07). We will make the code available in a future update.

摘要：擴散模型展現出極佳的樣本品質，但現有的引導方法通常需要額外的模型訓練，或僅限於特定任務。我們從變異推論和控制的角度重新探討擴散模型中的引導，並引入擴散軌跡匹配 (DTM)，讓預訓練的擴散軌跡能夠滿足終端成本。DTM 統一了廣泛的引導方法，並能實現新穎的實例化。我們在此架構中引入一種新方法，在數個線性和（盲）非線性反問題中達成最先進的結果，而不需要額外的模型訓練或修改。例如，在 ImageNet 非線性去模糊中，我們的模型達成 34.31 的 FID 分數，大幅優於最佳的預訓練方法基準（FID 78.07）。我們將在未來的更新中提供程式碼。

##### **Controlled LLM Decoding via Discrete Auto-regressive Biasing**
2502.03685v1 by Patrick Pynadath, Ruqi Zhang

Controlled text generation allows for enforcing user-defined constraints on
large language model outputs, an increasingly important field as LLMs become
more prevalent in everyday life. One common approach uses energy-based
decoding, which defines a target distribution through an energy function that
combines multiple constraints into a weighted average. However, these methods
often struggle to balance fluency with constraint satisfaction, even with
extensive tuning of the energy function's coefficients. In this paper, we
identify that this suboptimal balance arises from sampling in continuous space
rather than the natural discrete space of text tokens. To address this, we
propose Discrete Auto-regressive Biasing, a controlled decoding algorithm that
leverages gradients while operating entirely in the discrete text domain.
Specifically, we introduce a new formulation for controlled text generation by
defining a joint distribution over the generated sequence and an auxiliary bias
sequence. To efficiently sample from this joint distribution, we propose a
Langevin-within-Gibbs sampling algorithm using gradient-based discrete MCMC.
Our method significantly improves constraint satisfaction while maintaining
comparable or better fluency, all with even lower computational costs. We
demonstrate the advantages of our controlled decoding method on sentiment
control, language detoxification, and keyword-guided generation.

摘要：受控文本生成允许對大型語言模型輸出執行使用者定義的約束，隨著大型語言模型在日常生活中變得越來越普遍，這是一個越來越重要的領域。一種常見的方法使用基於能量的解碼，它通過將多個約束組合成加權平均值來定義目標分佈。然而，這些方法通常難以在流暢性和約束滿足之間取得平衡，即使對能量函數的係數進行廣泛調整也是如此。在本文中，我們發現這種次優平衡源於在連續空間中進行採樣，而不是在文本符號的自然離散空間中進行採樣。為了解決這個問題，我們提出了離散自回歸偏差，這是一種受控解碼演算法，它在完全在離散文本域中運作的同時利用梯度。具體來說，我們通過定義生成序列和輔助偏差序列上的聯合分佈來引入受控文本生成的全新公式。為了有效地從這個聯合分佈中進行採樣，我們提出了一個使用基於梯度的離散 MCMC 的 Gibbs 內 Langevin 採樣演算法。我們的模型顯著改善了約束滿足，同時維持了相當或更好的流暢性，而且計算成本更低。我們在情緒控制、語言解毒和關鍵字引導生成方面展示了我們的受控解碼方法的優點。

##### **Reflection-Window Decoding: Text Generation with Selective Refinement**
2502.03678v1 by Zeyu Tang, Zhenhao Chen, Loka Li, Xiangchen Song, Yunlong Deng, Yifan Shen, Guangyi Chen, Peter Spirtes, Kun Zhang

The autoregressive decoding for text generation in large language models
(LLMs), while widely used, is inherently suboptimal due to the lack of a
built-in mechanism to perform refinement and/or correction of the generated
content. In this paper, we consider optimality in terms of the joint
probability over the generated response, when jointly considering all tokens at
the same time. We theoretically characterize the potential deviation of the
autoregressively generated response from its globally optimal counterpart that
is of the same length. Our analysis suggests that we need to be cautious when
noticeable uncertainty arises during text generation, which may signal the
sub-optimality of the generation history. To address the pitfall of
autoregressive decoding for text generation, we propose an approach that
incorporates a sliding reflection window and a pausing criterion, such that
refinement and generation can be carried out interchangeably as the decoding
proceeds. Our selective refinement framework strikes a balance between
efficiency and optimality, and our extensive experimental results demonstrate
the effectiveness of our approach.

摘要：在大型語言模型 (LLM) 中用於文本生成的自動迴歸解碼，雖然廣泛使用，但由於缺乏內建機制來對生成的內容進行精煉和/或修正，因此本質上並非最佳。在本文中，我們在同時考慮所有標記時，根據生成的回應的聯合機率來考慮最優性。我們從理論上描述了自動迴歸生成的回應與長度相同的全局最優對應項之間的潛在偏差。我們的分析表明，當在文本生成過程中出現明顯的不確定性時，我們需要謹慎，這可能會表明生成歷史的次優性。為了解決自動迴歸解碼在文本生成中的缺陷，我們提出了一種方法，該方法結合了一個滑動反射視窗和一個暫停準則，這樣可以在解碼過程中交替進行精煉和生成。我們的選擇性精煉框架在效率和最優性之間取得了平衡，我們的廣泛實驗結果證明了我們方法的有效性。

##### **Advancing Reasoning in Large Language Models: Promising Methods and Approaches**
2502.03671v1 by Avinash Patil

Large Language Models (LLMs) have succeeded remarkably in various natural
language processing (NLP) tasks, yet their reasoning capabilities remain a
fundamental challenge. While LLMs exhibit impressive fluency and factual
recall, their ability to perform complex reasoning-spanning logical deduction,
mathematical problem-solving, commonsense inference, and multi-step
reasoning-often falls short of human expectations. This survey provides a
comprehensive review of emerging techniques enhancing reasoning in LLMs. We
categorize existing methods into key approaches, including prompting strategies
(e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought
reasoning), architectural innovations (e.g., retrieval-augmented models,
modular reasoning networks, and neuro-symbolic integration), and learning
paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement
learning, and self-supervised reasoning objectives). Additionally, we explore
evaluation frameworks used to assess reasoning in LLMs and highlight open
challenges, such as hallucinations, robustness, and reasoning generalization
across diverse tasks. By synthesizing recent advancements, this survey aims to
provide insights into promising directions for future research and practical
applications of reasoning-augmented LLMs.

摘要：大型語言模型 (LLM) 在各種自然語言處理 (NLP) 任務中取得了顯著的成功，但其推理能力仍然是一項基本挑戰。儘管 LLM 表現出令人印象深刻的流暢性和事實回憶，但它們執行複雜推理（包括邏輯推論、數學問題解決、常識推理和多步驟推理）的能力往往低於人類的期望。這項調查提供了對增強 LLM 中推理的新興技術的全面回顧。我們將現有方法分類為關鍵方法，包括提示策略（例如，思想鏈推理、自洽性和思想樹推理）、架構創新（例如，檢索增強模型、模組化推理網路和神經符號整合）以及學習範例（例如，使用特定於推理的資料集進行微調、強化學習和自我監督推理目標）。此外，我們探討了用於評估 LLM 中推理的評估框架，並強調了開放式挑戰，例如幻覺、穩健性和跨不同任務的推理概化。通過綜合最近的進展，這項調查旨在為未來研究和推理增強 LLM 的實際應用提供有希望的方向的見解。

##### **Unrealized Expectations: Comparing AI Methods vs Classical Algorithms for Maximum Independent Set**
2502.03669v1 by Yikai Wu, Haoyu Zhao, Sanjeev Arora

AI methods, such as generative models and reinforcement learning, have
recently been applied to combinatorial optimization (CO) problems, especially
NP-hard ones. This paper compares such GPU-based methods with classical
CPU-based methods on Maximum Independent Set (MIS). Experiments on standard
graph families show that AI-based algorithms fail to outperform and, in many
cases, to match the solution quality of the state-of-art classical solver KaMIS
running on a single CPU. Some GPU-based methods even perform similarly to the
simplest heuristic, degree-based greedy. Even with post-processing techniques
like local search, AI-based methods still perform worse than CPU-based solvers.
  We develop a new mode of analysis to reveal that non-backtracking AI methods,
e.g. LTFT (which is based on GFlowNets), end up reasoning similarly to the
simplest degree-based greedy approach, and thus worse than KaMIS. We also find
that CPU-based algorithms, notably KaMIS, have strong performance on sparse
random graphs, which appears to refute a well-known conjectured upper bound for
efficient algorithms from Coja-Oghlan & Efthymiou (2015).

摘要：AI 方法，例如生成模型和强化学习，最近已应用于组合优化 (CO) 问题，尤其是 NP 难问题。本文将此类基于 GPU 的方法与经典的基于 CPU 的方法在最大独立集 (MIS) 上进行了比较。对标准图族的实验表明，基于 AI 的算法未能胜过，并且在许多情况下，未能匹配在单个 CPU 上运行的最先进的经典求解器 KaMIS 的解决方案质量。一些基于 GPU 的方法甚至表现得类似于最简单的启发式，基于度的贪婪算法。即使使用局部搜索等后处理技术，基于 AI 的方法仍然比基于 CPU 的求解器表现得更差。我们开发了一种新的分析模式来揭示非回溯 AI 方法，例如 LTFT（基于 GFlowNets），最终推理类似于最简单的基于度的贪婪方法，因此比 KaMIS 更差。我们还发现，基于 CPU 的算法，特别是 KaMIS，在稀疏随机图上具有很强的性能，这似乎驳斥了 Coja-Oghlan 和 Efthymiou (2015) 提出的一个众所周知的有效算法的上界猜想。

##### **Energy & Force Regression on DFT Trajectories is Not Enough for Universal Machine Learning Interatomic Potentials**
2502.03660v1 by Santiago Miret, Kin Long Kelvin Lee, Carmelo Gonzales, Sajid Mannan, N. M. Anoop Krishnan

Universal Machine Learning Interactomic Potentials (MLIPs) enable accelerated
simulations for materials discovery. However, current research efforts fail to
impactfully utilize MLIPs due to: 1. Overreliance on Density Functional Theory
(DFT) for MLIP training data creation; 2. MLIPs' inability to reliably and
accurately perform large-scale molecular dynamics (MD) simulations for diverse
materials; 3. Limited understanding of MLIPs' underlying capabilities. To
address these shortcomings, we aargue that MLIP research efforts should
prioritize: 1. Employing more accurate simulation methods for large-scale MLIP
training data creation (e.g. Coupled Cluster Theory) that cover a wide range of
materials design spaces; 2. Creating MLIP metrology tools that leverage
large-scale benchmarking, visualization, and interpretability analyses to
provide a deeper understanding of MLIPs' inner workings; 3. Developing
computationally efficient MLIPs to execute MD simulations that accurately model
a broad set of materials properties. Together, these interdisciplinary research
directions can help further the real-world application of MLIPs to accurately
model complex materials at device scale.

摘要：通用机器学习交互势能 (MLIP) 可加速材料发现模拟。然而，目前的研究工作未能有效利用 MLIP，原因如下：1. 过度依赖密度泛函理论 (DFT) 来创建 MLIP 训练数据；2. MLIP 无法可靠且准确地执行不同材料的大规模分子动力学 (MD) 模拟；3. 对 MLIP 的底层功能了解有限。为了解决这些缺点，我们认为 MLIP 研究工作应优先考虑：1. 采用更准确的模拟方法来创建大规模 MLIP 训练数据（例如耦合簇理论），涵盖广泛的材料设计空间；2. 创建 MLIP 计量工具，利用大规模基准测试、可视化和可解释性分析，以更深入地了解 MLIP 的内部运作方式；3. 开发计算高效的 MLIP 来执行 MD 模拟，以准确建模广泛的材料属性。总之，这些跨学科研究方向可以帮助进一步促进 MLIP 的实际应用，以准确地模拟设备规模的复杂材料。

##### **Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics**
2502.03654v1 by Indrashis Das, Mahmoud Safari, Steven Adriaensen, Frank Hutter

Activation functions are fundamental elements of deep learning architectures
as they significantly influence training dynamics. ReLU, while widely used, is
prone to the dying neuron problem, which has been mitigated by variants such as
LeakyReLU, PReLU, and ELU that better handle negative neuron outputs. Recently,
self-gated activations like GELU and Swish have emerged as state-of-the-art
alternatives, leveraging their smoothness to ensure stable gradient flow and
prevent neuron inactivity. In this work, we introduce the Gompertz Linear Unit
(GoLU), a novel self-gated activation function defined as $\mathrm{GoLU}(x) = x
\, \mathrm{Gompertz}(x)$, where $\mathrm{Gompertz}(x) = e^{-e^{-x}}$. The GoLU
activation leverages the asymmetry in the Gompertz function to reduce variance
in the latent space more effectively compared to GELU and Swish, while
preserving robust gradient flow. Extensive experiments across diverse tasks,
including Image Classification, Language Modeling, Semantic Segmentation,
Object Detection, Instance Segmentation, and Diffusion, highlight GoLU's
superior performance relative to state-of-the-art activation functions,
establishing GoLU as a robust alternative to existing activation functions.

摘要：激活函數是深度學習架構的基本元素，因為它們會顯著影響訓練動態。ReLU 雖然廣泛使用，但容易發生神經元死亡問題，而 LeakyReLU、PReLU 和 ELU 等變體已減輕了此問題，它們能更好地處理負神經元輸出。最近，像 GELU 和 Swish 這樣的自門控激活函數已成為最先進的替代方案，利用它們的平滑性來確保穩定的梯度流動並防止神經元不活躍。在這項工作中，我們引入了 Gompertz 線性單元 (GoLU)，這是一種新穎的自門控激活函數，定義為 $\mathrm{GoLU}(x) = x \, \mathrm{Gompertz}(x)$，其中 $\mathrm{Gompertz}(x) = e^{-e^{-x}}$。GoLU 激活函數利用 Gompertz 函數中的不對稱性，與 GELU 和 Swish 相比，更有效地減少潛在空間中的變異，同時保留穩健的梯度流動。在各種任務中的廣泛實驗，包括影像分類、語言建模、語義分割、物件偵測、實例分割和擴散，突顯了 GoLU 相對於最先進的激活函數的優異效能，確立了 GoLU 作為現有激活函數的穩健替代方案。

##### **Looking for the Inner Music: Probing LLMs' Understanding of Literary Style**
2502.03647v1 by Rebecca M. M. Hicke, David Mimno

Recent work has demonstrated that language models can be trained to identify
the author of much shorter literary passages than has been thought feasible for
traditional stylometry. We replicate these results for authorship and extend
them to a new dataset measuring novel genre. We find that LLMs are able to
distinguish authorship and genre, but they do so in different ways. Some models
seem to rely more on memorization, while others benefit more from training to
learn author/genre characteristics. We then use three methods to probe one
high-performing LLM for features that define style. These include direct
syntactic ablations to input text as well as two methods that look at model
internals. We find that authorial style is easier to define than genre-level
style and is more impacted by minor syntactic decisions and contextual word
usage. However, some traits like pronoun usage and word order prove significant
for defining both kinds of literary style.

摘要：近期的研究顯示，語言模型可以接受訓練，以識別比傳統文體測量法認為可行的更短文學段的作者。我們複製了這些作者身分結果，並將其延伸到測量新小說類型的資料集。我們發現 LLM 能夠區分作者身分和類型，但它們以不同的方式做到這一點。有些模型似乎更依賴於記憶，而有些模型則從訓練中受益更多，以學習作者/類型的特徵。然後，我們使用三種方法來探查一個高性能 LLM 以找出定義風格的特徵。這些方法包括對輸入文字的直接句法消融，以及查看模型內部的兩種方法。我們發現，作者風格比類型層級風格更容易定義，而且受到較小的句法決策和語境詞彙使用的影響更大。然而，某些特徵（如代名詞使用和詞序）對於定義這兩種文學風格都非常重要。

##### **Context-Preserving Gradient Modulation for Large Language Models: A Novel Approach to Semantic Consistency in Long-Form Text Generation**
2502.03643v1 by Nirola Kobanov, Edmund Weatherstone, Zachary Vanderpoel, Orlando Wetherby

Maintaining semantic consistency over extended text sequences remains a
fundamental challenge in long-form text generation, where conventional training
methodologies often struggle to prevent contextual drift and coherence
degradation. A novel gradient modulation approach is introduced, designed to
adjust parameter updates dynamically in response to contextual relevance,
ensuring that generated text remains aligned with prior discourse. By
integrating a modulation function that selectively amplifies or attenuates
gradients based on learned contextual dependencies, the proposed method
enhances the stability of model-generated narratives without imposing
significant computational overhead. Comparative evaluations against baseline
models reveal improvements in coherence, contextual retention, and long-range
dependency tracking, demonstrating the effectiveness of modifying the learning
process at the gradient level. The results indicate that sentence structure
variability and lexical diversity benefit from this approach, mitigating
repetitive phrasing and improving adaptability across diverse linguistic
contexts. Statistical validation of coherence metrics further substantiates the
observed enhancements, with a significant reduction in inconsistencies emerging
as a direct consequence of the modulation mechanism. Computational efficiency
assessments confirm that the framework achieves these gains without requiring
substantial modifications to the underlying architecture, ensuring
compatibility with existing optimization workflows.

摘要：在長篇文本生成中，維持語義一致性在延伸文本序列中仍然是一個基本挑戰，其中傳統的訓練方法通常難以防止情境漂移和相干性降低。引入了一種新穎的梯度調製方法，旨在根據情境相關性動態調整參數更新，確保生成的文本與先前的語篇保持一致。通過整合一個調製函數，根據學習到的情境依賴關係選擇性地放大或衰減梯度，所提出的方法增強了模型生成的敘事的穩定性，而不會造成顯著的計算開銷。與基準模型的比較評估顯示出相干性、情境保留和長程依賴性追蹤的改進，證明了在梯度層級修改學習過程的有效性。結果表明，句子結構變異性和詞彙多樣性從這種方法中受益，減輕了重複的表述，並提高了在不同語言情境中的適應性。相干性指標的統計驗證進一步證實了觀察到的增強，由於調製機制的直接後果，不一致性顯著減少。計算效率評估確認，該框架在不需對底層架構進行大幅修改的情況下實現了這些增益，確保與現有最佳化工作流程的相容性。

##### **REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations**
2502.03629v1 by Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, Ranjay Krishna

Existing image editing models struggle to meet real-world demands. Despite
excelling in academic benchmarks, they have yet to be widely adopted for real
user needs. Datasets that power these models use artificial edits, lacking the
scale and ecological validity necessary to address the true diversity of user
requests. We introduce REALEDIT, a large-scale image editing dataset with
authentic user requests and human-made edits sourced from Reddit. REALEDIT
includes a test set of 9300 examples to evaluate models on real user requests.
Our results show that existing models fall short on these tasks, highlighting
the need for realistic training data. To address this, we introduce 48K
training examples and train our REALEDIT model, achieving substantial gains -
outperforming competitors by up to 165 Elo points in human judgment and 92
percent relative improvement on the automated VIEScore metric. We deploy our
model on Reddit, testing it on new requests, and receive positive feedback.
Beyond image editing, we explore REALEDIT's potential in detecting edited
images by partnering with a deepfake detection non-profit. Finetuning their
model on REALEDIT data improves its F1-score by 14 percentage points,
underscoring the dataset's value for broad applications.

摘要：現有的影像編輯模型難以滿足現實世界的需求。儘管在學術基準上表現優異，但它們尚未廣泛用於滿足實際的使用者需求。驅動這些模型的資料集使用人工編輯，缺乏處理使用者要求真實多樣性的規模和生態效度。我們推出 REALEDIT，一個具有真實使用者要求和來自 Reddit 的人為編輯的大規模影像編輯資料集。REALEDIT 包含一個 9300 個範例的測試集，用於評估模型對真實使用者要求的表現。我們的結果顯示，現有的模型在這些任務上表現不佳，突顯了對真實訓練資料的需求。為了解決這個問題，我們引入了 48K 個訓練範例，並訓練我們的 REALEDIT 模型，取得了顯著的進展 - 在人類判斷中領先競爭對手多達 165 Elo 點，在自動化 VIEScore 指標上相對改善了 92%。我們在 Reddit 上部署我們的模型，對新的要求進行測試，並收到正面的回饋。除了影像編輯之外，我們還探索了 REALEDIT 在偵測編輯過影像的潛力，並與一家非營利的深度偽造偵測公司合作。在 REALEDIT 資料集上微調他們的模型，將其 F1 分數提高了 14 個百分點，突顯了該資料集對廣泛應用程式的價值。

##### **The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering**
2502.03628v1 by Zhuowei Li, Haizhou Shi, Yunhe Gao, Di Liu, Zhenting Wang, Yuxiao Chen, Ting Liu, Long Zhao, Hao Wang, Dimitris N. Metaxas

Large Vision-Language Models (LVLMs) can reason effectively over both textual
and visual inputs, but they tend to hallucinate syntactically coherent yet
visually ungrounded contents. In this paper, we investigate the internal
dynamics of hallucination by examining the tokens logits rankings throughout
the generation process, revealing three key patterns in how LVLMs process
information: (1) gradual visual information loss -- visually grounded tokens
gradually become less favored throughout generation, and (2) early excitation
-- semantically meaningful tokens achieve peak activation in the layers earlier
than the final layer. (3) hidden genuine information -- visually grounded
tokens though not being eventually decided still retain relatively high
rankings at inference. Based on these insights, we propose VISTA (Visual
Information Steering with Token-logit Augmentation), a training-free
inference-time intervention framework that reduces hallucination while
promoting genuine information. VISTA works by combining two complementary
approaches: reinforcing visual information in activation space and leveraging
early layer activations to promote semantically meaningful decoding. Compared
to existing methods, VISTA requires no external supervision and is applicable
to various decoding strategies. Extensive experiments show that VISTA on
average reduces hallucination by abount 40% on evaluated open-ended generation
task, and it consistently outperforms existing methods on four benchmarks
across four architectures under three decoding strategies.

摘要：大型視覺語言模型 (LVLMs) 可以有效地對文本和視覺輸入進行推理，但它們傾向於產生語法上連貫但視覺上沒有根據的內容。在本文中，我們通過檢查整個生成過程中的代幣對數概率排名，探討了幻覺的內部動態，揭示了 LVLMs 處理資訊的三個關鍵模式：(1) 逐漸的視覺資訊損失——視覺上紮實的代幣在整個生成過程中逐漸變得不那麼受歡迎，以及 (2) 早期激勵——語義上有意義的代幣在比最後一層更早的層中達到峰值激活。(3) 隱藏的真實資訊——視覺上紮實的代幣儘管最終沒有被決定，但仍保留了相對較高的推論排名。根據這些見解，我們提出了 VISTA（使用代幣對數概率增強的視覺資訊導向），這是一個免訓練的推論時間介入框架，可減少幻覺，同時促進真實資訊。VISTA 的工作原理是結合兩種互補的方法：在激活空間中加強視覺資訊並利用早期層激活來促進語義上有意義的解碼。與現有方法相比，VISTA 不需要外部監督，並且適用於各種解碼策略。大量的實驗表明，VISTA 平均可將評估的開放式生成任務的幻覺減少約 40%，並且在四種架構下的四個基準測試中，在三種解碼策略下始終優於現有方法。

##### **Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database**
2502.03627v1 by Maxime Holmberg Sainte-Marie, Diego Kozlowski, Lucía Céspedes, Vincent Larivière

Following a recent study on the quality of OpenAlex linguistic metadata
(C\'espedes et al., 2025), the present paper aims to optimize the latter
through the design, use, and evaluation of various linguistic classification
procedures based on the latest and most efficient automatic language detection
algorithms. Starting from a multilingual set of manually-annotated samples of
articles indexed in the database, different classification procedures are then
designed, based on the application of a set of language detection algorithms on
a series of corpora generated from different combinations of textual metadata
of indexed articles. At sample level first, the performance of these different
procedures for each of the main languages in the database is evaluated in terms
of precision, recall, and processing time. Then, overall procedure performance
is estimated at the database level by means of a probabilistic simulation of
harmonically aggregated and weighted scores. Results show that procedure
performance strongly depends on the importance given to each of the measures
implemented: for contexts where precision is preferred, using the LangID
algorithm on article titles, abstracts as well as journal names gives the best
results; however, for all cases where recall is considered at least slightly
more important than precision or as soon as processing times are given any kind
of consideration, use of the FastSpell algorithm on article titles only
outperforms all other alternatives. Given the lack of truly multilingual,
large-scale bibliographic databases, it is hoped that these results help
confirm and foster the unparalleled potential of the OpenAlex database for
cross-linguistic, bibliometric-based research and analysis.

摘要：<paragraph>根據 OpenAlex 語言元資料品質的最新研究 (C\'espedes 等人，2025)，本文旨在透過設計、使用和評估各種語言分類程序，以及根據最新且最有效率的自動語言偵測演算法，來最佳化後者。從資料庫中索引的文章手動標註的多語言範例開始，接著設計不同的分類程序，根據一系列語料庫應用一組語言偵測演算法，這些語料庫是由索引文章的文字元資料的不同組合產生。首先在範例層級，評估這些不同程序在資料庫中每種主要語言的效能，包括準確度、召回率和處理時間。接著，透過對調和彙總和加權分數進行機率模擬，估計在資料庫層級的整體程序效能。結果顯示，程序效能高度仰賴所實施的每個指標所賦予的重要性：在優先考慮準確度的脈絡中，對文章標題、摘要和期刊名稱使用 LangID 演算法會產生最佳結果；然而，在所有召回率被認為至少比準確度稍重要，或在任何考量處理時間的情況下，僅對文章標題使用 FastSpell 演算法會優於所有其他替代方案。由於缺乏真正多語言、大規模的書目資料庫，我們希望這些結果有助於確認和促進 OpenAlex 資料庫在跨語言、以書目計量為基礎的研究和分析方面的無與倫比潛力。</paragraph>

##### **AdaPhish: AI-Powered Adaptive Defense and Education Resource Against Deceptive Emails**
2502.03622v1 by Rei Meguro, Ng S. T. Chong

Phishing attacks remain a significant threat in the digital age, yet
organizations lack effective methods to tackle phishing attacks without leaking
sensitive information. Phish bowl initiatives are a vital part of cybersecurity
efforts against these attacks. However, traditional phish bowls require manual
anonymization and are often limited to internal use. To overcome these
limitations, we introduce AdaPhish, an AI-powered phish bowl platform that
automatically anonymizes and analyzes phishing emails using large language
models (LLMs) and vector databases. AdaPhish achieves real-time detection and
adaptation to new phishing tactics while enabling long-term tracking of
phishing trends. Through automated reporting, adaptive analysis, and real-time
alerts, AdaPhish presents a scalable, collaborative solution for phishing
detection and cybersecurity education.

摘要：網釣攻擊在數位時代仍是一個重大的威脅，但組織缺乏有效的方法來處理網釣攻擊，而不會洩露敏感資訊。網釣信箱計畫是對抗這些攻擊的網路安全措施中至關重要的一部分。然而，傳統的網釣信箱需要手動匿名化，而且通常僅限於內部使用。為了克服這些限制，我們引入了 AdaPhish，一個由 AI 驅動的網釣信箱平台，它使用大型語言模型 (LLM) 和向量資料庫自動匿名化並分析網釣電子郵件。AdaPhish 可即時偵測並適應新的網釣策略，同時能長期追蹤網釣趨勢。透過自動化報告、適應性分析和即時警示，AdaPhish 提出了一個可擴充、協作的解決方案，用於網釣偵測和網路安全教育。

##### **A Novel Zero-Touch, Zero-Trust, AI/ML Enablement Framework for IoT Network Security**
2502.03614v1 by Sushil Shakya, Robert Abbas, Sasa Maric

The IoT facilitates a connected, intelligent, and sustainable society;
therefore, it is imperative to protect the IoT ecosystem. The IoT-based 5G and
6G will leverage the use of machine learning and artificial intelligence
(ML/AI) more to pave the way for autonomous and collaborative secure IoT
networks. Zero-touch, zero-trust IoT security with AI and machine learning (ML)
enablement frameworks offers a powerful approach to securing the expanding
landscape of Internet of Things (IoT) devices. This paper presents a novel
framework based on the integration of Zero Trust, Zero Touch, and AI/ML powered
for the detection, mitigation, and prevention of DDoS attacks in modern IoT
ecosystems. The focus will be on the new integrated framework by establishing
zero trust for all IoT traffic, fixed and mobile 5G/6G IoT network traffic, and
data security (quarantine-zero touch and dynamic policy enforcement). We
perform a comparative analysis of five machine learning models, namely,
XGBoost, Random Forest, K-Nearest Neighbors, Stochastic Gradient Descent, and
Native Bayes, by comparing these models based on accuracy, precision, recall,
F1-score, and ROC-AUC. Results show that the best performance in detecting and
mitigating different DDoS vectors comes from the ensemble-based approaches.

摘要：物聯網促進一個互聯、智慧且永續的社會；因此，保護物聯網生態系統至關重要。基於物聯網的 5G 和 6G 將更多地利用機器學習和人工智慧 (ML/AI)，為自主且協作的安全物聯網網路鋪路。零接觸、零信任的物聯網安全性，具備 AI 和機器學習 (ML) 啟用架構，提供一種強大的方法來保護不斷擴展的物聯網 (IoT) 裝置領域。本文提出一個新穎的架構，基於零信任、零接觸和 AI/ML 的整合，用於偵測、減輕和預防現代物聯網生態系統中的 DDoS 攻擊。重點將放在新的整合架構上，為所有物聯網流量、固網和行動 5G/6G 物聯網網路流量以及資料安全性（隔離零接觸和動態政策強制執行）建立零信任。我們對五種機器學習模型進行比較分析，即 XGBoost、隨機森林、K 最近鄰、隨機梯度下降和原生貝氏，並根據準確度、精確度、召回率、F1 分數和 ROC-AUC 比較這些模型。結果顯示，在偵測和減輕不同 DDoS 向量方面，表現最佳的是基於整體的方法。

