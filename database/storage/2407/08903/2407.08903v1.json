{"2407.08903": {"publish_time": "2024-07-12", "title": "TensorTEE: Unifying Heterogeneous TEE Granularity for Efficient Secure Collaborative Tensor Computing", "paper_summary": "Heterogeneous collaborative computing with NPU and CPU has received\nwidespread attention due to its substantial performance benefits. To ensure\ndata confidentiality and integrity during computing, Trusted Execution\nEnvironments (TEE) is considered a promising solution because of its\ncomparatively lower overhead. However, existing heterogeneous TEE designs are\ninefficient for collaborative computing due to fine and different memory\ngranularities between CPU and NPU. 1) The cacheline granularity of CPU TEE\nintensifies memory pressure due to its extra memory access, and 2) the\ncacheline granularity MAC of NPU escalates the pressure on the limited memory\nstorage. 3) Data transfer across heterogeneous enclaves relies on the transit\nof non-secure regions, resulting in cumbersome re-encryption and scheduling.\n  To address these issues, we propose TensorTEE, a unified tensor-granularity\nheterogeneous TEE for efficient secure collaborative tensor computing. First,\nwe virtually support tensor granularity in CPU TEE to eliminate the off-chip\nmetadata access by detecting and maintaining tensor structures on-chip. Second,\nwe propose tensor-granularity MAC management with predictive execution to avoid\ncomputational stalls while eliminating off-chip MAC storage and access.\nMoreover, based on the unified granularity, we enable direct data transfer\nwithout re-encryption and scheduling dilemmas. Our evaluation is built on\nenhanced Gem5 and a cycle-accurate NPU simulator. The results show that\nTensorTEE improves the performance of Large Language Model (LLM) training\nworkloads by 4.0x compared to existing work and incurs only 2.1% overhead\ncompared to non-secure training, offering a practical security assurance for\nLLM training.", "paper_summary_zh": "\u7570\u8cea\u5354\u4f5c\u904b\u7b97\u7d50\u5408 NPU \u548c CPU\uff0c\u56e0\u5176\u986f\u8457\u7684\u6548\u80fd\u512a\u52e2\u800c\u5ee3\u53d7\u77da\u76ee\u3002\u70ba\u4e86\u78ba\u4fdd\u904b\u7b97\u671f\u9593\u7684\u8cc7\u6599\u6a5f\u5bc6\u6027\u548c\u5b8c\u6574\u6027\uff0c\u53ef\u4fe1\u57f7\u884c\u74b0\u5883 (TEE) \u7531\u65bc\u5176\u76f8\u5c0d\u8f03\u4f4e\u7684\u958b\u92b7\u800c\u88ab\u8996\u70ba\u4e00\u500b\u6709\u524d\u9014\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u7570\u8cea TEE \u8a2d\u8a08\u5c0d\u65bc\u5354\u4f5c\u904b\u7b97\u800c\u8a00\u6548\u7387\u4f4e\u4e0b\uff0c\u9019\u662f\u56e0\u70ba CPU \u548c NPU \u4e4b\u9593\u7684\u8a18\u61b6\u9ad4\u9846\u7c92\u5ea6\u7cbe\u7d30\u4e14\u4e0d\u540c\u30021) CPU TEE \u7684\u5feb\u53d6\u884c\u9846\u7c92\u5ea6\u6703\u56e0\u5176\u984d\u5916\u7684\u8a18\u61b6\u9ad4\u5b58\u53d6\u800c\u52a0\u5287\u8a18\u61b6\u9ad4\u58d3\u529b\uff0c2) NPU \u7684\u5feb\u53d6\u884c\u9846\u7c92\u5ea6 MAC \u6703\u52a0\u5287\u5c0d\u6709\u9650\u8a18\u61b6\u9ad4\u5132\u5b58\u7684\u58d3\u529b\u30023) \u7570\u8cea\u98db\u5730\u7684\u8cc7\u6599\u50b3\u8f38\u4f9d\u8cf4\u65bc\u975e\u5b89\u5168\u5340\u57df\u7684\u4e2d\u8f49\uff0c\u5c0e\u81f4\u7e41\u7463\u7684\u91cd\u65b0\u52a0\u5bc6\u548c\u6392\u7a0b\u3002\n\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa TensorTEE\uff0c\u4e00\u500b\u7528\u65bc\u9ad8\u6548\u5b89\u5168\u5354\u4f5c\u5f35\u91cf\u904b\u7b97\u7684\u7d71\u4e00\u5f35\u91cf\u9846\u7c92\u5ea6\u7570\u8cea TEE\u3002\u9996\u5148\uff0c\u6211\u5011\u5728 CPU TEE \u4e2d\u865b\u64ec\u652f\u63f4\u5f35\u91cf\u9846\u7c92\u5ea6\uff0c\u4ee5\u900f\u904e\u5728\u6676\u7247\u4e0a\u5075\u6e2c\u548c\u7dad\u8b77\u5f35\u91cf\u7d50\u69cb\u4f86\u6d88\u9664\u6676\u7247\u5916\u5143\u8cc7\u6599\u5b58\u53d6\u3002\u5176\u6b21\uff0c\u6211\u5011\u63d0\u51fa\u5177\u6709\u9810\u6e2c\u57f7\u884c\u7684\u5f35\u91cf\u9846\u7c92\u5ea6 MAC \u7ba1\u7406\uff0c\u4ee5\u907f\u514d\u8a08\u7b97\u505c\u9813\uff0c\u540c\u6642\u6d88\u9664\u6676\u7247\u5916 MAC \u5132\u5b58\u548c\u5b58\u53d6\u3002\u6b64\u5916\uff0c\u57fa\u65bc\u7d71\u4e00\u7684\u9846\u7c92\u5ea6\uff0c\u6211\u5011\u555f\u7528\u76f4\u63a5\u8cc7\u6599\u50b3\u8f38\uff0c\u800c\u7121\u9700\u91cd\u65b0\u52a0\u5bc6\u548c\u6392\u7a0b\u56f0\u5883\u3002\u6211\u5011\u7684\u8a55\u4f30\u5efa\u7acb\u5728\u589e\u5f37\u7684 Gem5 \u548c\u9031\u671f\u7cbe\u78ba\u7684 NPU \u6a21\u64ec\u5668\u4e0a\u3002\u7d50\u679c\u986f\u793a\uff0c\u8207\u73fe\u6709\u5de5\u4f5c\u76f8\u6bd4\uff0cTensorTEE \u5c07\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u8a13\u7df4\u5de5\u4f5c\u8ca0\u8f09\u7684\u6548\u80fd\u63d0\u5347\u4e86 4.0 \u500d\uff0c\u800c\u4e14\u8207\u975e\u5b89\u5168\u8a13\u7df4\u76f8\u6bd4\uff0c\u50c5\u7522\u751f 2.1% \u7684\u958b\u92b7\uff0c\u70ba LLM \u8a13\u7df4\u63d0\u4f9b\u4e86\u5be6\u7528\u7684\u5b89\u5168\u6027\u4fdd\u8b49\u3002", "author": "Husheng Han et.al.", "authors": "Husheng Han, Xinyao Zheng, Yuanbo Wen, Yifan Hao, Erhu Feng, Ling Liang, Jianan Mu, Xiaqing Li, Tianyun Ma, Pengwei Jin, Xinkai Song, Zidong Du, Qi Guo, Xing Hu", "id": "2407.08903v1", "paper_url": "http://arxiv.org/abs/2407.08903v1", "repo": "null"}}