{"2407.11962": {"publish_time": "2024-07-16", "title": "Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling", "paper_summary": "This paper introduces Motion-oriented Compositional Neural Radiance Fields\n(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of\nmonocular human videos via novel non-rigid motion modeling approach. In the\ncontext of dynamic clothed humans, complex cloth dynamics generate non-rigid\nmotions that are intrinsically distinct from skeletal articulations and\ncritically important for the rendering quality. The conventional approach\nmodels non-rigid motions as spatial (3D) deviations in addition to skeletal\ntransformations. However, it is either time-consuming or challenging to achieve\noptimal quality due to its high learning complexity without a direct\nsupervision. To target this problem, we propose a novel approach of modeling\nnon-rigid motions as radiance residual fields to benefit from more direct color\nsupervision in the rendering and utilize the rigid radiance fields as a prior\nto reduce the complexity of the learning process. Our approach utilizes a\nsingle multiresolution hash encoding (MHE) to concurrently learn the canonical\nT-pose representation from rigid skeletal motions and the radiance residual\nfield for non-rigid motions. Additionally, to further improve both training\nefficiency and usability, we extend MoCo-NeRF to support simultaneous training\nof multiple subjects within a single framework, thanks to our effective design\nfor modeling non-rigid motions. This scalability is achieved through the\nintegration of a global MHE and learnable identity codes in addition to\nmultiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,\nclearly demonstrating state-of-the-art performance in both single- and\nmulti-subject settings. The code and model will be made publicly available at\nthe project page: https://stevejaehyeok.github.io/publications/moco-nerf.", "paper_summary_zh": "<paragraph>\u672c\u6587\u4ecb\u7ecd\u4e86\u9762\u5411\u8fd0\u52a8\u7684\u5408\u6210\u795e\u7ecf\u8f90\u5c04\u573a (MoCo-NeRF)\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u901a\u8fc7\u65b0\u9896\u7684\u975e\u521a\u6027\u8fd0\u52a8\u5efa\u6a21\u65b9\u6cd5\u6267\u884c\u5355\u773c\u4eba\u7c7b\u89c6\u9891\u7684\u81ea\u7531\u89c6\u70b9\u6e32\u67d3\u7684\u6846\u67b6\u3002\u5728\u52a8\u6001\u7740\u88c5\u7684\u4eba\u7c7b\u7684\u80cc\u666f\u4e0b\uff0c\u590d\u6742\u7684\u5e03\u6599\u52a8\u6001\u4f1a\u4ea7\u751f\u975e\u521a\u6027\u8fd0\u52a8\uff0c\u8fd9\u4e9b\u8fd0\u52a8\u672c\u8d28\u4e0a\u4e0d\u540c\u4e8e\u9aa8\u9abc\u5173\u8282\uff0c\u5e76\u4e14\u5bf9\u6e32\u67d3\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u975e\u521a\u6027\u8fd0\u52a8\u5efa\u6a21\u4e3a\u7a7a\u95f4 (3D) \u504f\u5dee\u4ee5\u53ca\u9aa8\u9abc\u53d8\u6362\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5176\u5b66\u4e60\u590d\u6742\u5ea6\u9ad8\u4e14\u6ca1\u6709\u76f4\u63a5\u76d1\u7763\uff0c\u56e0\u6b64\u8981\u8fbe\u5230\u6700\u4f73\u8d28\u91cf\u65e2\u8017\u65f6\u53c8\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5c06\u975e\u521a\u6027\u8fd0\u52a8\u5efa\u6a21\u4e3a\u8f90\u5c04\u6b8b\u5dee\u573a\uff0c\u4ee5\u53d7\u76ca\u4e8e\u6e32\u67d3\u4e2d\u66f4\u76f4\u63a5\u7684\u989c\u8272\u76d1\u7763\uff0c\u5e76\u5c06\u521a\u6027\u8f90\u5c04\u573a\u7528\u4f5c\u5148\u9a8c\u6765\u964d\u4f4e\u5b66\u4e60\u8fc7\u7a0b\u7684\u590d\u6742\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u5355\u4e00\u7684\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f16\u7801 (MHE) \u6765\u540c\u65f6\u4ece\u521a\u6027\u9aa8\u9abc\u8fd0\u52a8\u4e2d\u5b66\u4e60\u89c4\u8303\u7684 T \u59ff\u52bf\u8868\u793a\uff0c\u4ee5\u53ca\u7528\u4e8e\u975e\u521a\u6027\u8fd0\u52a8\u7684\u8f90\u5c04\u6b8b\u5dee\u573a\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u7528\u6027\uff0c\u6211\u4eec\u6269\u5c55\u4e86 MoCo-NeRF \u4ee5\u652f\u6301\u5728\u5355\u4e2a\u6846\u67b6\u5185\u540c\u65f6\u8bad\u7ec3\u591a\u4e2a\u4e3b\u4f53\uff0c\u8fd9\u8981\u5f52\u529f\u4e8e\u6211\u4eec\u7528\u4e8e\u5efa\u6a21\u975e\u521a\u6027\u8fd0\u52a8\u7684\u6709\u6548\u8bbe\u8ba1\u3002\u9664\u4e86\u591a\u4e2a\u5c40\u90e8 MHE \u4e4b\u5916\uff0c\u8fd9\u79cd\u53ef\u6269\u5c55\u6027\u662f\u901a\u8fc7\u96c6\u6210\u5168\u5c40 MHE \u548c\u53ef\u5b66\u4e60\u7684\u8eab\u4efd\u4ee3\u7801\u5b9e\u73b0\u7684\u3002\u6211\u4eec\u5728 ZJU-MoCap \u548c MonoCap \u4e0a\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u7ed3\u679c\uff0c\u6e05\u695a\u5730\u5c55\u793a\u4e86\u5728\u5355\u4e3b\u4f53\u548c\u591a\u4e3b\u4f53\u8bbe\u7f6e\u4e2d\u90fd\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728\u9879\u76ee\u9875\u9762\u516c\u5f00\uff1ahttps://stevejaehyeok.github.io/publications/moco-nerf\u3002</paragraph>", "author": "Jaehyeok Kim et.al.", "authors": "Jaehyeok Kim, Dongyoon Wee, Dan Xu", "id": "2407.11962v1", "paper_url": "http://arxiv.org/abs/2407.11962v1", "repo": "null"}}