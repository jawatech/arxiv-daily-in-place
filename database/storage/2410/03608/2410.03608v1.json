{"2410.03608": {"publish_time": "2024-10-04", "title": "TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation", "paper_summary": "Given the widespread adoption and usage of Large Language Models (LLMs), it\nis crucial to have flexible and interpretable evaluations of their\ninstruction-following ability. Preference judgments between model outputs have\nbecome the de facto evaluation standard, despite distilling complex,\nmulti-faceted preferences into a single ranking. Furthermore, as human\nannotation is slow and costly, LLMs are increasingly used to make these\njudgments, at the expense of reliability and interpretability. In this work, we\npropose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated,\ninterpretable evaluation protocol that structures evaluations with\nLLM-generated, instruction-specific checklists. We first show that, given an\ninstruction, LLMs can reliably produce high-quality, tailored evaluation\nchecklists that decompose the instruction into a series of YES/NO questions.\nEach question asks whether a candidate response meets a specific requirement of\nthe instruction. We demonstrate that using TICK leads to a significant increase\n(46.4% $\\to$ 52.2%) in the frequency of exact agreements between LLM judgements\nand human preferences, as compared to having an LLM directly score an output.\nWe then show that STICK (Self-TICK) can be used to improve generation quality\nacross multiple benchmarks via self-refinement and Best-of-N selection. STICK\nself-refinement on LiveBench reasoning tasks leads to an absolute gain of\n$+$7.8%, whilst Best-of-N selection with STICK attains $+$6.3% absolute\nimprovement on the real-world instruction dataset, WildBench. In light of this,\nstructured, multi-faceted self-improvement is shown to be a promising way to\nfurther advance LLM capabilities. Finally, by providing LLM-generated\nchecklists to human evaluators tasked with directly scoring LLM responses to\nWildBench instructions, we notably increase inter-annotator agreement (0.194\n$\\to$ 0.256).", "paper_summary_zh": "<paragraph>\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5e7f\u6cdb\u91c7\u7528\u548c\u4f7f\u7528\uff0c\u5bf9\u5b83\u4eec\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u8fdb\u884c\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u6a21\u578b\u8f93\u51fa\u4e4b\u95f4\u7684\u504f\u597d\u5224\u65ad\u5df2\u6210\u4e3a\u4e8b\u5b9e\u4e0a\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5c3d\u7ba1\u5c06\u590d\u6742\u3001\u591a\u65b9\u9762\u7684\u504f\u597d\u63d0\u70bc\u6210\u5355\u4e00\u6392\u540d\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u4eba\u5de5\u6ce8\u91ca\u7f13\u6162\u4e14\u6210\u672c\u9ad8\u6602\uff0cLLM \u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u505a\u51fa\u8fd9\u4e9b\u5224\u65ad\uff0c\u4f46\u727a\u7272\u4e86\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TICK\uff08\u6709\u9488\u5bf9\u6027\u7684\u6307\u4ee4\u8bc4\u4f30\u4e0e\u6e05\u5355\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5b83\u4f7f\u7528 LLM \u751f\u6210\u7684\u3001\u7279\u5b9a\u4e8e\u6307\u4ee4\u7684\u6e05\u5355\u6765\u6784\u5efa\u8bc4\u4f30\u3002\u6211\u4eec\u9996\u5148\u8868\u660e\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u6307\u4ee4\uff0cLLM \u53ef\u4ee5\u53ef\u9760\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u3001\u5b9a\u5236\u7684\u8bc4\u4f30\u6e05\u5355\uff0c\u5c06\u6307\u4ee4\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u662f/\u5426\u95ee\u9898\u3002\u6bcf\u4e2a\u95ee\u9898\u8be2\u95ee\u5019\u9009\u54cd\u5e94\u662f\u5426\u6ee1\u8db3\u6307\u4ee4\u7684\u7279\u5b9a\u8981\u6c42\u3002\u6211\u4eec\u8bc1\u660e\u4f7f\u7528 TICK \u5bfc\u81f4 LLM \u5224\u65ad\u548c\u4eba\u7c7b\u504f\u597d\u4e4b\u95f4\u5b8c\u5168\u4e00\u81f4\u7684\u9891\u7387\u663e\u7740\u589e\u52a0\uff0846.4% $ \\to $ 52.2%\uff09\uff0c\u4e0e\u8ba9 LLM \u76f4\u63a5\u5bf9\u8f93\u51fa\u8fdb\u884c\u8bc4\u5206\u76f8\u6bd4\u3002\u7136\u540e\u6211\u4eec\u8868\u660e\uff0cSTICK\uff08\u81ea TICK\uff09\u53ef\u4ee5\u901a\u8fc7\u81ea\u4f18\u5316\u548c N \u9009\u4f18\u6765\u63d0\u9ad8\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u751f\u6210\u8d28\u91cf\u3002LiveBench \u63a8\u7406\u4efb\u52a1\u4e0a\u7684 STICK \u81ea\u4f18\u5316\u5bfc\u81f4\u7edd\u5bf9\u589e\u76ca\u4e3a +$7.8%\uff0c\u800c STICK \u4e0a\u7684 N \u9009\u4f18\u5728\u73b0\u5b9e\u4e16\u754c\u6307\u4ee4\u6570\u636e\u96c6 WildBench \u4e0a\u83b7\u5f97\u4e86 +$6.3% \u7684\u7edd\u5bf9\u6539\u8fdb\u3002\u9274\u4e8e\u6b64\uff0c\u7ed3\u6784\u5316\u3001\u591a\u65b9\u9762\u7684\u81ea\u6211\u6539\u8fdb\u88ab\u8bc1\u660e\u662f\u8fdb\u4e00\u6b65\u63d0\u5347 LLM \u80fd\u529b\u7684\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5411\u8d1f\u8d23\u76f4\u63a5\u5bf9 LLM \u5bf9 WildBench \u6307\u4ee4\u7684\u54cd\u5e94\u8fdb\u884c\u8bc4\u5206\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\u63d0\u4f9b LLM \u751f\u6210\u7684\u6e05\u5355\uff0c\u6211\u4eec\u663e\u7740\u63d0\u9ad8\u4e86\u6ce8\u91ca\u8005\u95f4\u7684\u4e00\u81f4\u6027\uff080.194 $ \\to $ 0.256\uff09\u3002</paragraph>", "author": "Jonathan Cook et.al.", "authors": "Jonathan Cook, Tim Rockt\u00e4schel, Jakob Foerster, Dennis Aumiller, Alex Wang", "id": "2410.03608v1", "paper_url": "http://arxiv.org/abs/2410.03608v1", "repo": "null"}}