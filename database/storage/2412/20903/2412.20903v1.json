{"2412.20903": {"publish_time": "2024-12-30", "title": "WalkVLM:Aid Visually Impaired People Walking by Vision Language Model", "paper_summary": "Approximately 200 million individuals around the world suffer from varying\ndegrees of visual impairment, making it crucial to leverage AI technology to\noffer walking assistance for these people. With the recent progress of\nvision-language models (VLMs), employing VLMs to improve this field has emerged\nas a popular research topic. However, most existing methods are studied on\nself-built question-answering datasets, lacking a unified training and testing\nbenchmark for walk guidance. Moreover, in blind walking task, it is necessary\nto perform real-time streaming video parsing and generate concise yet\ninformative reminders, which poses a great challenge for VLMs that suffer from\nredundant responses and low inference efficiency. In this paper, we firstly\nrelease a diverse, extensive, and unbiased walking awareness dataset,\ncontaining 12k video-manual annotation pairs from Europe and Asia to provide a\nfair training and testing benchmark for blind walking task. Furthermore, a\nWalkVLM model is proposed, which employs chain of thought for hierarchical\nplanning to generate concise but informative reminders and utilizes\ntemporal-aware adaptive prediction to reduce the temporal redundancy of\nreminders. Finally, we have established a solid benchmark for blind walking\ntask and verified the advantages of WalkVLM in stream video processing for this\ntask compared to other VLMs. Our dataset and code will be released at anonymous\nlink https://walkvlm2024.github.io.", "paper_summary_zh": "\u5168\u7403\u7ea6\u6709 2 \u4ebf\u4eba\u60a3\u6709\u4e0d\u540c\u7a0b\u5ea6\u7684\u89c6\u89c9\u969c\u788d\uff0c\u56e0\u6b64\u5229\u7528\u4eba\u5de5\u667a\u80fd\u6280\u672f\u4e3a\u8fd9\u4e9b\u4eba\u63d0\u4f9b\u884c\u8d70\u8f85\u52a9\u81f3\u5173\u91cd\u8981\u3002\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4f7f\u7528 VLM \u6765\u6539\u8fdb\u8be5\u9886\u57df\u5df2\u6210\u4e3a\u4e00\u4e2a\u70ed\u95e8\u7684\u7814\u7a76\u8bfe\u9898\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u90fd\u662f\u5728\u81ea\u5efa\u7684\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7814\u7a76\u7684\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6b65\u884c\u6307\u5bfc\u8bad\u7ec3\u548c\u6d4b\u8bd5\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u5728\u76f2\u4eba\u884c\u8d70\u4efb\u52a1\u4e2d\uff0c\u6709\u5fc5\u8981\u6267\u884c\u5b9e\u65f6\u6d41\u89c6\u9891\u89e3\u6790\u5e76\u751f\u6210\u7b80\u6d01\u800c\u6709\u610f\u4e49\u7684\u63d0\u9192\uff0c\u8fd9\u5bf9\u906d\u53d7\u5197\u4f59\u54cd\u5e94\u548c\u4f4e\u63a8\u7406\u6548\u7387\u7684 VLM \u6784\u6210\u4e86\u5de8\u5927\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u53d1\u5e03\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u3001\u5e7f\u6cdb\u4e14\u65e0\u504f\u89c1\u7684\u6b65\u884c\u611f\u77e5\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u6765\u81ea\u6b27\u6d32\u548c\u4e9a\u6d32\u7684 12k \u4e2a\u89c6\u9891\u624b\u518c\u6ce8\u91ca\u5bf9\uff0c\u4ee5\u63d0\u4f9b\u76f2\u4eba\u6b65\u884c\u4efb\u52a1\u7684\u516c\u5e73\u8bad\u7ec3\u548c\u6d4b\u8bd5\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd WalkVLM \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u601d\u60f3\u94fe\u8fdb\u884c\u5206\u5c42\u89c4\u5212\u4ee5\u751f\u6210\u7b80\u6d01\u4f46\u6709\u610f\u4e49\u7684\u63d0\u9192\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u611f\u77e5\u81ea\u9002\u5e94\u9884\u6d4b\u6765\u51cf\u5c11\u63d0\u9192\u7684\u65f6\u95f4\u5197\u4f59\u3002\u6700\u540e\uff0c\u6211\u4eec\u4e3a\u76f2\u4eba\u884c\u8d70\u4efb\u52a1\u5efa\u7acb\u4e86\u4e00\u4e2a\u575a\u5b9e\u7684\u57fa\u51c6\uff0c\u5e76\u9a8c\u8bc1\u4e86 WalkVLM \u5728\u6d41\u89c6\u9891\u5904\u7406\u4e2d\u4e0e\u5176\u4ed6 VLM \u76f8\u6bd4\u7684\u4f18\u52bf\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5728\u533f\u540d\u94fe\u63a5 https://walkvlm2024.github.io \u4e0a\u53d1\u5e03\u3002", "author": "Zhiqiang Yuan et.al.", "authors": "Zhiqiang Yuan, Ting Zhang, Jiapei Zhang, Jie Zhou, Jinchao Zhang", "id": "2412.20903v1", "paper_url": "http://arxiv.org/abs/2412.20903v1", "repo": "null"}}