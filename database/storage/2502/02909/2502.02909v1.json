{"2502.02909": {"publish_time": "2025-02-05", "title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs", "paper_summary": "We propose SPARC, a lightweight continual learning framework for large\nlanguage models (LLMs) that enables efficient task adaptation through prompt\ntuning in a lower-dimensional space. By leveraging principal component analysis\n(PCA), we identify a compact subspace of the training data. Optimizing prompts\nin this lower-dimensional space enhances training efficiency, as it focuses\nupdates on the most relevant features while reducing computational overhead.\nFurthermore, since the model's internal structure remains unaltered, the\nextensive knowledge gained from pretraining is fully preserved, ensuring that\npreviously learned information is not compromised during adaptation. Our method\nachieves high knowledge retention in both task-incremental and\ndomain-incremental continual learning setups while fine-tuning only 0.04% of\nthe model's parameters. Additionally, by integrating LoRA, we enhance\nadaptability to computational constraints, allowing for a tradeoff between\naccuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate\nthat our PCA-based prompt tuning combined with LoRA maintains full knowledge\nretention while improving accuracy, utilizing only 1% of the model's\nparameters. These results establish our approach as a scalable and\nresource-efficient solution for continual learning in LLMs.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa SPARC\uff0c\u4e00\u7a2e\u9069\u7528\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8f15\u91cf\u7d1a\u6301\u7e8c\u5b78\u7fd2\u6846\u67b6\uff0c\u5b83\u80fd\u900f\u904e\u5728\u4f4e\u7dad\u5ea6\u7a7a\u9593\u4e2d\u8abf\u6574\u63d0\u793a\u4f86\u6709\u6548\u9032\u884c\u4efb\u52d9\u9069\u61c9\u3002\u900f\u904e\u5229\u7528\u4e3b\u6210\u5206\u5206\u6790 (PCA)\uff0c\u6211\u5011\u627e\u51fa\u8a13\u7df4\u8cc7\u6599\u7684\u7dca\u6e4a\u5b50\u7a7a\u9593\u3002\u5728\u9019\u500b\u4f4e\u7dad\u5ea6\u7a7a\u9593\u4e2d\u6700\u4f73\u5316\u63d0\u793a\u80fd\u589e\u5f37\u8a13\u7df4\u6548\u7387\uff0c\u56e0\u70ba\u5b83\u6703\u5c07\u66f4\u65b0\u96c6\u4e2d\u5728\u6700\u76f8\u95dc\u7684\u7279\u5fb5\u4e0a\uff0c\u540c\u6642\u6e1b\u5c11\u904b\u7b97\u8ca0\u64d4\u3002\u6b64\u5916\uff0c\u7531\u65bc\u6a21\u578b\u7684\u5167\u90e8\u7d50\u69cb\u4fdd\u6301\u4e0d\u8b8a\uff0c\u56e0\u6b64\u5f9e\u9810\u8a13\u7df4\u4e2d\u7372\u5f97\u7684\u8c50\u5bcc\u77e5\u8b58\u5f97\u4ee5\u5b8c\u5168\u4fdd\u7559\uff0c\u78ba\u4fdd\u5728\u9069\u61c9\u904e\u7a0b\u4e2d\u5148\u524d\u5b78\u7fd2\u7684\u8cc7\u8a0a\u4e0d\u6703\u53d7\u5230\u640d\u5bb3\u3002\u6211\u5011\u7684\u505a\u6cd5\u5728\u4efb\u52d9\u905e\u589e\u548c\u9818\u57df\u905e\u589e\u7684\u6301\u7e8c\u5b78\u7fd2\u8a2d\u5b9a\u4e2d\u90fd\u80fd\u9054\u6210\u9ad8\u77e5\u8b58\u4fdd\u7559\u7387\uff0c\u540c\u6642\u53ea\u5fae\u8abf\u6a21\u578b 0.04% \u7684\u53c3\u6578\u3002\u6b64\u5916\uff0c\u900f\u904e\u6574\u5408 LoRA\uff0c\u6211\u5011\u589e\u5f37\u4e86\u5c0d\u904b\u7b97\u9650\u5236\u7684\u9069\u61c9\u6027\uff0c\u5141\u8a31\u5728\u6e96\u78ba\u5ea6\u548c\u8a13\u7df4\u6210\u672c\u4e4b\u9593\u9032\u884c\u6b0a\u8861\u3002\u5728 SuperGLUE \u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u57fa\u65bc PCA \u7684\u63d0\u793a\u8abf\u6574\u7d50\u5408 LoRA\uff0c\u5728\u50c5\u4f7f\u7528\u6a21\u578b 1% \u7684\u53c3\u6578\u6642\uff0c\u80fd\u7dad\u6301\u5b8c\u5168\u7684\u77e5\u8b58\u4fdd\u7559\uff0c\u540c\u6642\u63d0\u9ad8\u6e96\u78ba\u5ea6\u3002\u9019\u4e9b\u7d50\u679c\u78ba\u7acb\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u4f5c\u70ba LLM \u4e2d\u6301\u7e8c\u5b78\u7fd2\u7684\u53ef\u64f4\u5145\u4e14\u8cc7\u6e90\u6709\u6548\u7387\u7684\u89e3\u6c7a\u65b9\u6848\u3002", "author": "Dinithi Jayasuriya et.al.", "authors": "Dinithi Jayasuriya, Sina Tayebati, Davide Ettori, Ranganath Krishnan, Amit Ranjan Trivedi", "id": "2502.02909v1", "paper_url": "http://arxiv.org/abs/2502.02909v1", "repo": "null"}}