{"2410.06606": {"publish_time": "2024-10-09", "title": "Dissecting Fine-Tuning Unlearning in Large Language Models", "paper_summary": "Fine-tuning-based unlearning methods prevail for preventing targeted harmful,\nsensitive, or copyrighted information within large language models while\npreserving overall capabilities. However, the true effectiveness of these\nmethods is unclear. In this paper, we delve into the limitations of\nfine-tuning-based unlearning through activation patching and parameter\nrestoration experiments. Our findings reveal that these methods alter the\nmodel's knowledge retrieval process, rather than genuinely erasing the\nproblematic knowledge embedded in the model parameters. Furthermore, behavioral\ntests demonstrate that the unlearning mechanisms inevitably impact the global\nbehavior of the models, affecting unrelated knowledge or capabilities. Our work\nadvocates the development of more resilient unlearning techniques for truly\nerasing knowledge. Our code is released at\nhttps://github.com/yihuaihong/Dissecting-FT-Unlearning.", "paper_summary_zh": "\u5fae\u8abf\u70ba\u57fa\u790e\u7684\u907a\u5fd8\u65b9\u6cd5\u666e\u904d\u7528\u65bc\u9632\u6b62\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\u76ee\u6a19\u6709\u5bb3\u3001\u654f\u611f\u6216\u53d7\u7248\u6b0a\u4fdd\u8b77\u7684\u8cc7\u8a0a\uff0c\u540c\u6642\u4fdd\u7559\u6574\u9ad4\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u7684\u771f\u6b63\u6709\u6548\u6027\u5c1a\u4e0d\u6e05\u695a\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u4e86\u57fa\u65bc\u5fae\u8abf\u7684\u907a\u5fd8\u7684\u9650\u5236\uff0c\u900f\u904e\u555f\u52d5\u7a0b\u5f0f\u4fee\u88dc\u548c\u53c3\u6578\u9084\u539f\u5be6\u9a57\u3002\u6211\u5011\u7684\u767c\u73fe\u63ed\u793a\u4e86\u9019\u4e9b\u65b9\u6cd5\u6703\u6539\u8b8a\u6a21\u578b\u7684\u77e5\u8b58\u64f7\u53d6\u904e\u7a0b\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u62b9\u9664\u5d4c\u5165\u5728\u6a21\u578b\u53c3\u6578\u4e2d\u7684\u554f\u984c\u77e5\u8b58\u3002\u6b64\u5916\uff0c\u884c\u70ba\u6e2c\u8a66\u8868\u660e\uff0c\u907a\u5fd8\u6a5f\u5236\u4e0d\u53ef\u907f\u514d\u5730\u6703\u5f71\u97ff\u6a21\u578b\u7684\u6574\u9ad4\u884c\u70ba\uff0c\u5f71\u97ff\u4e0d\u76f8\u95dc\u7684\u77e5\u8b58\u6216\u80fd\u529b\u3002\u6211\u5011\u7684\u7814\u7a76\u63d0\u5021\u958b\u767c\u66f4\u5177\u97cc\u6027\u7684\u907a\u5fd8\u6280\u8853\uff0c\u4ee5\u771f\u6b63\u62b9\u9664\u77e5\u8b58\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5df2\u767c\u5e03\u5728 https://github.com/yihuaihong/Dissecting-FT-Unlearning\u3002", "author": "Yihuai Hong et.al.", "authors": "Yihuai Hong, Yuelin Zou, Lijie Hu, Ziqian Zeng, Di Wang, Haiqin Yang", "id": "2410.06606v1", "paper_url": "http://arxiv.org/abs/2410.06606v1", "repo": "null"}}