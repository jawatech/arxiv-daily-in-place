{"2410.10343": {"publish_time": "2024-10-14", "title": "Locking Down the Finetuned LLMs Safety", "paper_summary": "Fine-tuning large language models (LLMs) on additional datasets is often\nnecessary to optimize them for specific downstream tasks. However, existing\nsafety alignment measures, which restrict harmful behavior during inference,\nare insufficient to mitigate safety risks during fine-tuning. Alarmingly,\nfine-tuning with just 10 toxic sentences can make models comply with harmful\ninstructions. We introduce SafetyLock, a novel alignment intervention method\nthat maintains robust safety post-fine-tuning through efficient and\ntransferable mechanisms. SafetyLock leverages our discovery that fine-tuned\nmodels retain similar safety-related activation representations to their base\nmodels. This insight enables us to extract what we term the Meta-SafetyLock, a\nset of safety bias directions representing key activation patterns associated\nwith safe responses in the original model. We can then apply these directions\nuniversally to fine-tuned models to enhance their safety. By searching for\nactivation directions across multiple token dimensions, SafetyLock achieves\nenhanced robustness and transferability. SafetyLock re-aligns fine-tuned models\nin under 0.01 seconds without additional computational cost. Our experiments\ndemonstrate that SafetyLock can reduce the harmful instruction response rate\nfrom 60% to below 1% in toxic fine-tuned models. It surpasses traditional\nmethods in both performance and efficiency, offering a scalable, non-invasive\nsolution for ensuring the safety of customized LLMs. Our analysis across\nvarious fine-tuning scenarios confirms SafetyLock's robustness, advocating its\nintegration into safety protocols for aligned LLMs. The code is released at\nhttps://github.com/zhu-minjun/SafetyLock.", "paper_summary_zh": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ee5\u7528\u4e8e\u5176\u4ed6\u6570\u636e\u96c6\u901a\u5e38\u6709\u5fc5\u8981\u9488\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u5bf9\u5176\u8fdb\u884c\u4f18\u5316\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5b89\u5168\u5bf9\u9f50\u63aa\u65bd\uff08\u5728\u63a8\u7406\u671f\u95f4\u9650\u5236\u6709\u5bb3\u884c\u4e3a\uff09\u4e0d\u8db3\u4ee5\u51cf\u8f7b\u5fae\u8c03\u671f\u95f4\u7684\u5b89\u5168\u98ce\u9669\u3002\u4ee4\u4eba\u62c5\u5fe7\u7684\u662f\uff0c\u4ec5\u4f7f\u7528 10 \u4e2a\u6709\u6bd2\u53e5\u5b50\u8fdb\u884c\u5fae\u8c03\u5c31\u80fd\u4f7f\u6a21\u578b\u9075\u5b88\u6709\u5bb3\u6307\u4ee4\u3002\u6211\u4eec\u5f15\u5165\u4e86 SafetyLock\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u9f50\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u4e14\u53ef\u8f6c\u79fb\u7684\u673a\u5236\u5728\u5fae\u8c03\u540e\u4fdd\u6301\u5f3a\u5927\u7684\u5b89\u5168\u6027\u3002SafetyLock \u5229\u7528\u4e86\u6211\u4eec\u53d1\u73b0\u5fae\u8c03\u6a21\u578b\u4fdd\u7559\u4e86\u4e0e\u5176\u57fa\u7840\u6a21\u578b\u7c7b\u4f3c\u7684\u5b89\u5168\u76f8\u5173\u6fc0\u6d3b\u8868\u793a\u3002\u8fd9\u4e00\u89c1\u89e3\u4f7f\u6211\u4eec\u80fd\u591f\u63d0\u53d6\u6211\u4eec\u79f0\u4e4b\u4e3a Meta-SafetyLock \u7684\u4e1c\u897f\uff0c\u8fd9\u662f\u4e00\u7ec4\u5b89\u5168\u504f\u5dee\u65b9\u5411\uff0c\u4ee3\u8868\u4e0e\u539f\u59cb\u6a21\u578b\u4e2d\u7684\u5b89\u5168\u54cd\u5e94\u76f8\u5173\u8054\u7684\u5173\u952e\u6fc0\u6d3b\u6a21\u5f0f\u3002\u7136\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u65b9\u5411\u666e\u904d\u5e94\u7528\u4e8e\u5fae\u8c03\u6a21\u578b\u4ee5\u589e\u5f3a\u5176\u5b89\u5168\u6027\u3002\u901a\u8fc7\u5728\u591a\u4e2a\u6807\u8bb0\u7ef4\u5ea6\u4e0a\u641c\u7d22\u6fc0\u6d3b\u65b9\u5411\uff0cSafetyLock \u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u53ef\u8f6c\u79fb\u6027\u3002SafetyLock \u5728\u4e0d\u5230 0.01 \u79d2\u7684\u65f6\u95f4\u5185\u91cd\u65b0\u5bf9\u9f50\u5fae\u8c03\u6a21\u578b\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u8ba1\u7b97\u6210\u672c\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSafetyLock \u53ef\u4ee5\u5c06\u6709\u5bb3\u6307\u4ee4\u54cd\u5e94\u7387\u4ece 60% \u964d\u4f4e\u5230\u6709\u6bd2\u5fae\u8c03\u6a21\u578b\u4e2d\u7684 1% \u4ee5\u4e0b\u3002\u5b83\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u786e\u4fdd\u5b9a\u5236 LLM \u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u975e\u4fb5\u5165\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u5bf9\u5404\u79cd\u5fae\u8c03\u573a\u666f\u7684\u5206\u6790\u8bc1\u5b9e\u4e86 SafetyLock \u7684\u7a33\u5065\u6027\uff0c\u63d0\u5021\u5c06\u5176\u96c6\u6210\u5230\u5bf9\u9f50 LLM \u7684\u5b89\u5168\u534f\u8bae\u4e2d\u3002\u4ee3\u7801\u5df2\u5728 https://github.com/zhu-minjun/SafetyLock \u4e2d\u53d1\u5e03\u3002", "author": "Minjun Zhu et.al.", "authors": "Minjun Zhu, Linyi Yang, Yifan Wei, Ningyu Zhang, Yue Zhang", "id": "2410.10343v1", "paper_url": "http://arxiv.org/abs/2410.10343v1", "repo": "https://github.com/zhu-minjun/safetylock"}}