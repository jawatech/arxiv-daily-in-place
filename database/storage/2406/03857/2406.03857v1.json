{"2406.03857": {"publish_time": "2024-06-06", "title": "MuJo: Multimodal Joint Feature Space Learning for Human Activity Recognition", "paper_summary": "Human Activity Recognition is a longstanding problem in AI with applications\nin a broad range of areas: from healthcare, sports and fitness, security, and\nhuman computer interaction to robotics. The performance of HAR in real-world\nsettings is strongly dependent on the type and quality of the input signal that\ncan be acquired. Given an unobstructed, high-quality camera view of a scene,\ncomputer vision systems, in particular in conjunction with foundational models\n(e.g., CLIP), can today fairly reliably distinguish complex activities. On the\nother hand, recognition using modalities such as wearable sensors (which are\noften more broadly available, e.g, in mobile phones and smartwatches) is a more\ndifficult problem, as the signals often contain less information and labeled\ntraining data is more difficult to acquire. In this work, we show how we can\nimprove HAR performance across different modalities using multimodal\ncontrastive pretraining. Our approach MuJo (Multimodal Joint Feature Space\nLearning), learns a multimodal joint feature space with video, language, pose,\nand IMU sensor data. The proposed approach combines contrastive and multitask\nlearning methods and analyzes different multitasking strategies for learning a\ncompact shared representation. A large dataset with parallel video, language,\npose, and sensor data points is also introduced to support the research, along\nwith an analysis of the robustness of the multimodal joint space for\nmodal-incomplete and low-resource data. On the MM-Fit dataset, our model\nachieves an impressive Macro F1-Score of up to 0.992 with only 2% of the train\ndata and 0.999 when using all available training data for classification tasks.\nMoreover, in the scenario where the MM-Fit dataset is unseen, we demonstrate a\ngeneralization performance of up to 0.638.", "paper_summary_zh": "\u4eba\u985e\u6d3b\u52d5\u8fa8\u8b58\u662f\u4eba\u5de5\u667a\u6167\u9818\u57df\u4e2d\u9577\u4e45\u5b58\u5728\u7684\u554f\u984c\uff0c\u5728\u5ee3\u6cdb\u7684\u9818\u57df\u4e2d\u90fd\u6709\u61c9\u7528\uff1a\u5f9e\u91ab\u7642\u4fdd\u5065\u3001\u904b\u52d5\u548c\u5065\u8eab\u3001\u5b89\u5168\u548c\u4eba\u6a5f\u4e92\u52d5\u5230\u6a5f\u5668\u4eba\u6280\u8853\u3002HAR \u5728\u771f\u5be6\u4e16\u754c\u4e2d\u7684\u8868\u73fe\u9ad8\u5ea6\u4f9d\u8cf4\u65bc\u8f38\u5165\u8a0a\u865f\u7684\u985e\u578b\u548c\u54c1\u8cea\uff0c\u800c\u8f38\u5165\u8a0a\u865f\u662f\u53ef\u4ee5\u88ab\u64f7\u53d6\u7684\u3002\u7d66\u5b9a\u4e00\u500b\u5834\u666f\u7684\u7121\u969c\u7919\u3001\u9ad8\u54c1\u8cea\u7684\u76f8\u6a5f\u8996\u89d2\uff0c\u96fb\u8166\u8996\u89ba\u7cfb\u7d71\uff0c\u7279\u5225\u662f\u7d50\u5408\u57fa\u790e\u6a21\u578b\uff08\u4f8b\u5982 CLIP\uff09\uff0c\u5982\u4eca\u53ef\u4ee5\u76f8\u7576\u53ef\u9760\u5730\u5340\u5206\u8907\u96dc\u7684\u6d3b\u52d5\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4f7f\u7528\u53ef\u7a7f\u6234\u5f0f\u611f\u6e2c\u5668\uff08\u4f8b\u5982\u5728\u884c\u52d5\u96fb\u8a71\u548c\u667a\u6167\u624b\u9336\u4e2d\u66f4\u5ee3\u6cdb\u53ef\u7528\u7684\uff09\u7b49\u65b9\u5f0f\u9032\u884c\u8fa8\u8b58\u662f\u4e00\u500b\u66f4\u56f0\u96e3\u7684\u554f\u984c\uff0c\u56e0\u70ba\u8a0a\u865f\u901a\u5e38\u5305\u542b\u8f03\u5c11\u8cc7\u8a0a\uff0c\u800c\u4e14\u6a19\u8a18\u8a13\u7df4\u8cc7\u6599\u66f4\u96e3\u4ee5\u53d6\u5f97\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u591a\u6a21\u614b\u5c0d\u6bd4\u9810\u8a13\u7df4\u4f86\u6539\u5584\u4e0d\u540c\u6a21\u614b\u7684 HAR \u6548\u80fd\u3002\u6211\u5011\u7684 MuJo\uff08\u591a\u6a21\u614b\u806f\u5408\u7279\u5fb5\u7a7a\u9593\u5b78\u7fd2\uff09\u65b9\u6cd5\uff0c\u5b78\u7fd2\u4e00\u500b\u5305\u542b\u5f71\u7247\u3001\u8a9e\u8a00\u3001\u59ff\u52e2\u548c IMU \u611f\u6e2c\u5668\u8cc7\u6599\u7684\u591a\u6a21\u614b\u806f\u5408\u7279\u5fb5\u7a7a\u9593\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7d50\u5408\u4e86\u5c0d\u6bd4\u548c\u591a\u4efb\u52d9\u5b78\u7fd2\u65b9\u6cd5\uff0c\u4e26\u5206\u6790\u4e86\u4e0d\u540c\u7684\u591a\u4efb\u52d9\u7b56\u7565\uff0c\u4ee5\u5b78\u7fd2\u4e00\u500b\u7dca\u6e4a\u7684\u5171\u4eab\u8868\u793a\u3002\u4e00\u500b\u5305\u542b\u5e73\u884c\u5f71\u7247\u3001\u8a9e\u8a00\u3001\u59ff\u52e2\u548c\u611f\u6e2c\u5668\u8cc7\u6599\u9ede\u7684\u5927\u578b\u8cc7\u6599\u96c6\u4e5f\u88ab\u5f15\u5165\uff0c\u4ee5\u652f\u63f4\u7814\u7a76\uff0c\u540c\u6642\u5206\u6790\u591a\u6a21\u614b\u806f\u5408\u7a7a\u9593\u5c0d\u6a21\u5f0f\u4e0d\u5b8c\u6574\u548c\u4f4e\u8cc7\u6e90\u8cc7\u6599\u7684\u7a69\u5065\u6027\u3002\u5728 MM-Fit \u8cc7\u6599\u96c6\u4e0a\uff0c\u6211\u5011\u7684\u6a21\u578b\u50c5\u4f7f\u7528 2% \u7684\u8a13\u7df4\u8cc7\u6599\u5c31\u9054\u5230\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684 0.992 \u5de8\u96c6 F1 \u5206\u6578\uff0c\u800c\u4f7f\u7528\u6240\u6709\u53ef\u7528\u7684\u8a13\u7df4\u8cc7\u6599\u9032\u884c\u5206\u985e\u4efb\u52d9\u6642\uff0c\u5247\u9054\u5230\u4e86 0.999\u3002\u6b64\u5916\uff0c\u5728 MM-Fit \u8cc7\u6599\u96c6\u672a\u898b\u7684\u60c5\u6cc1\u4e0b\uff0c\u6211\u5011\u5c55\u793a\u4e86\u9ad8\u9054 0.638 \u7684\u6cdb\u5316\u6548\u80fd\u3002", "author": "Stefan Gerd Fritsch et.al.", "authors": "Stefan Gerd Fritsch, Cennet Oguz, Vitor Fortes Rey, Lala Ray, Maximilian Kiefer-Emmanouilidis, Paul Lukowicz", "id": "2406.03857v1", "paper_url": "http://arxiv.org/abs/2406.03857v1", "repo": "null"}}