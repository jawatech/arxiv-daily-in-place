{"2405.20253": {"publish_time": "2024-05-30", "title": "Evaluating Large Language Model Biases in Persona-Steered Generation", "paper_summary": "The task of persona-steered text generation requires large language models\n(LLMs) to generate text that reflects the distribution of views that an\nindividual fitting a persona could have. People have multifaceted personas, but\nprior work on bias in LLM-generated opinions has only explored multiple-choice\nsettings or one-dimensional personas. We define an incongruous persona as a\npersona with multiple traits where one trait makes its other traits less likely\nin human survey data, e.g. political liberals who support increased military\nspending. We find that LLMs are 9.7% less steerable towards incongruous\npersonas than congruous ones, sometimes generating the stereotypical stance\nassociated with its demographic rather than the target stance. Models that we\nevaluate that are fine-tuned with Reinforcement Learning from Human Feedback\n(RLHF) are more steerable, especially towards stances associated with political\nliberals and women, but present significantly less diverse views of personas.\nWe also find variance in LLM steerability that cannot be predicted from\nmultiple-choice opinion evaluation. Our results show the importance of\nevaluating models in open-ended text generation, as it can surface new LLM\nopinion biases. Moreover, such a setup can shed light on our ability to steer\nmodels toward a richer and more diverse range of viewpoints.", "paper_summary_zh": "\u4eba\u683c\u5c0e\u5411\u6587\u672c\u751f\u6210\u7684\u4efb\u52d9\u9700\u8981\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7522\u751f\u53cd\u6620\u64c1\u6709\u7279\u5b9a\u4eba\u683c\u7684\u500b\u4eba\u89c0\u9ede\u5206\u4f48\u7684\u6587\u672c\u3002\u4eba\u5011\u6709\u591a\u9762\u6027\u7684\u4eba\u683c\uff0c\u4f46\u5148\u524d\u95dc\u65bc LLM \u751f\u6210\u610f\u898b\u4e2d\u504f\u898b\u7684\u7814\u7a76\u50c5\u63a2\u8a0e\u4e86\u591a\u91cd\u9078\u64c7\u8a2d\u5b9a\u6216\u4e00\u7dad\u4eba\u683c\u3002\u6211\u5011\u5c07\u4e0d\u4e00\u81f4\u7684\u4eba\u683c\u5b9a\u7fa9\u70ba\u5177\u6709\u591a\u91cd\u7279\u8cea\u7684\u4eba\u683c\uff0c\u5176\u4e2d\u4e00\u500b\u7279\u8cea\u4f7f\u5176\u5176\u4ed6\u7279\u8cea\u5728\u4eba\u985e\u8abf\u67e5\u6578\u64da\u4e2d\u4e0d\u592a\u53ef\u80fd\u51fa\u73fe\uff0c\u4f8b\u5982\u652f\u6301\u589e\u52a0\u8ecd\u8cbb\u958b\u652f\u7684\u653f\u6cbb\u81ea\u7531\u6d3e\u3002\u6211\u5011\u767c\u73fe\uff0cLLM \u5c0d\u4e0d\u4e00\u81f4\u4eba\u683c\u7684\u5f15\u5c0e\u529b\u6bd4\u4e00\u81f4\u4eba\u683c\u4f4e 9.7%\uff0c\u6709\u6642\u6703\u7522\u751f\u8207\u5176\u4eba\u53e3\u7d71\u8a08\u8cc7\u6599\u76f8\u95dc\u7684\u523b\u677f\u7acb\u5834\uff0c\u800c\u4e0d\u662f\u76ee\u6a19\u7acb\u5834\u3002\u6211\u5011\u8a55\u4f30\u7684\u6a21\u578b\u7d93\u904e\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u5fae\u8abf\u5f8c\uff0c\u66f4\u5177\u53ef\u5f15\u5c0e\u6027\uff0c\u7279\u5225\u662f\u5c0d\u8207\u653f\u6cbb\u81ea\u7531\u6d3e\u548c\u5973\u6027\u76f8\u95dc\u7684\u7acb\u5834\uff0c\u4f46\u5448\u73fe\u51fa\u660e\u986f\u8f03\u5c11\u6a23\u5316\u7684\u4eba\u683c\u89c0\u9ede\u3002\u6211\u5011\u9084\u767c\u73fe LLM \u53ef\u5f15\u5c0e\u6027\u4e2d\u7684\u5dee\u7570\u7121\u6cd5\u5f9e\u591a\u91cd\u9078\u64c7\u610f\u898b\u8a55\u4f30\u4e2d\u9810\u6e2c\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\u4e86\u5728\u958b\u653e\u5f0f\u6587\u672c\u751f\u6210\u4e2d\u8a55\u4f30\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u6d6e\u73fe\u65b0\u7684 LLM \u610f\u898b\u504f\u898b\u3002\u6b64\u5916\uff0c\u9019\u6a23\u7684\u8a2d\u5b9a\u53ef\u4ee5\u95e1\u660e\u6211\u5011\u5f15\u5c0e\u6a21\u578b\u671d\u5411\u66f4\u8c50\u5bcc\u3001\u66f4\u591a\u6a23\u5316\u7684\u89c0\u9ede\u7bc4\u570d\u7684\u80fd\u529b\u3002", "author": "Andy Liu et.al.", "authors": "Andy Liu, Mona Diab, Daniel Fried", "id": "2405.20253v1", "paper_url": "http://arxiv.org/abs/2405.20253v1", "repo": "https://github.com/andyjliu/persona-steered-generation-bias"}}