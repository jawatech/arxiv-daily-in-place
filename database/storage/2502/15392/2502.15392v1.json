{"2502.15392": {"publish_time": "2025-02-21", "title": "Chitrarth: Bridging Vision and Language for a Billion People", "paper_summary": "Recent multimodal foundation models are primarily trained on English or high\nresource European language data, which hinders their applicability to other\nmedium and low-resource languages. To address this limitation, we introduce\nChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model\n(VLM), specifically targeting the rich linguistic diversity and visual\nreasoning across 10 prominent Indian languages. Our model effectively\nintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)\nwith a vision module, primarily trained on multilingual image-text data.\nFurthermore, we also introduce BharatBench, a comprehensive framework for\nevaluating VLMs across various Indian languages, ultimately contributing to\nmore diverse and effective AI systems. Our model achieves SOTA results for\nbenchmarks across low resource languages while retaining its efficiency in\nEnglish. Through our research, we aim to set new benchmarks in\nmultilingual-multimodal capabilities, offering substantial improvements over\nexisting models and establishing a foundation to facilitate future advancements\nin this arena.", "paper_summary_zh": "\u6700\u8fd1\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5728\u82f1\u8bed\u6216\u9ad8\u8d44\u6e90\u7684\u6b27\u6d32\u8bed\u8a00\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5bf9\u5176\u4ed6\u4e2d\u7b49\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9002\u7528\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Chitrarth\uff08Chitra\uff1a\u56fe\u50cf\uff1bArtha\uff1a\u542b\u4e49\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u5bb9\u6027\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u4e13\u95e8\u9488\u5bf9 10 \u79cd\u4e3b\u8981\u7684\u5370\u5ea6\u8bed\u8a00\u7684\u4e30\u5bcc\u8bed\u8a00\u591a\u6837\u6027\u548c\u89c6\u89c9\u63a8\u7406\u3002\u6211\u4eec\u7684\u6a21\u578b\u6709\u6548\u5730\u96c6\u6210\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u89c6\u89c9\u6a21\u5757\uff0c\u4e3b\u8981\u5728\u591a\u8bed\u8a00\u56fe\u50cf\u6587\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86 BharatBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8de8\u5404\u79cd\u5370\u5ea6\u8bed\u8a00\u8bc4\u4f30 VLM \u7684\u7efc\u5408\u6846\u67b6\uff0c\u6700\u7ec8\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u591a\u6837\u5316\u548c\u6709\u6548\u7684 AI \u7cfb\u7edf\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86 SOTA \u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u5728\u82f1\u8bed\u4e2d\u7684\u6548\u7387\u3002\u901a\u8fc7\u6211\u4eec\u7684\u7814\u7a76\uff0c\u6211\u4eec\u65e8\u5728\u4e3a\u591a\u8bed\u8a00\u591a\u6a21\u6001\u80fd\u529b\u8bbe\u5b9a\u65b0\u7684\u57fa\u51c6\uff0c\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u5b9e\u8d28\u6027\u6539\u8fdb\uff0c\u5e76\u4e3a\u4fc3\u8fdb\u8be5\u9886\u57df\u672a\u6765\u7684\u8fdb\u6b65\u5960\u5b9a\u57fa\u7840\u3002", "author": "Shaharukh Khan et.al.", "authors": "Shaharukh Khan, Ayush Tarun, Abhinav Ravi, Ali Faraz, Akshat Patidar, Praveen Kumar Pokala, Anagha Bhangare, Raja Kolla, Chandra Khatri, Shubham Agarwal", "id": "2502.15392v1", "paper_url": "http://arxiv.org/abs/2502.15392v1", "repo": "null"}}