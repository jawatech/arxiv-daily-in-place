{"2407.10972": {"publish_time": "2024-07-15", "title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "paper_summary": "In the realm of vision models, the primary mode of representation is using\npixels to rasterize the visual world. Yet this is not always the best or unique\nway to represent visual content, especially for designers and artists who\ndepict the world using geometry primitives such as polygons. Vector graphics\n(VG), on the other hand, offer a textual representation of visual content,\nwhich can be more concise and powerful for content like cartoons or sketches.\nRecent studies have shown promising results on processing vector graphics with\ncapable Large Language Models (LLMs). However, such works focus solely on\nqualitative results, understanding, or a specific type of vector graphics. We\npropose VGBench, a comprehensive benchmark for LLMs on handling vector graphics\nthrough diverse aspects, including (a) both visual understanding and\ngeneration, (b) evaluation of various vector graphics formats, (c) diverse\nquestion types, (d) wide range of prompting techniques, (e) under multiple\nLLMs. Evaluating on our collected 4279 understanding and 5845 generation\nsamples, we find that LLMs show strong capability on both aspects while\nexhibiting less desirable performance on low-level formats (SVG). Both data and\nevaluation pipeline will be open-sourced at https://vgbench.github.io.", "paper_summary_zh": "\u5728\u8996\u89ba\u6a21\u578b\u9818\u57df\u4e2d\uff0c\u8868\u793a\u7684\u4e3b\u8981\u6a21\u5f0f\u662f\u4f7f\u7528\u50cf\u7d20\u4f86\u5149\u67f5\u5316\u8996\u89ba\u4e16\u754c\u3002\u7136\u800c\uff0c\u9019\u4e26\u975e\u7e3d\u662f\u8868\u793a\u8996\u89ba\u5167\u5bb9\u7684\u6700\u4f73\u6216\u552f\u4e00\u65b9\u5f0f\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u4f7f\u7528\u591a\u908a\u5f62\u7b49\u5e7e\u4f55\u5716\u5143\u63cf\u7e6a\u4e16\u754c\u7684\u8a2d\u8a08\u5e2b\u548c\u85dd\u8853\u5bb6\u800c\u8a00\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5411\u91cf\u5716\u5f62 (VG) \u63d0\u4f9b\u8996\u89ba\u5167\u5bb9\u7684\u6587\u5b57\u8868\u793a\uff0c\u5c0d\u65bc\u5361\u901a\u6216\u8349\u5716\u7b49\u5167\u5bb9\uff0c\u5b83\u53ef\u4ee5\u66f4\u7c21\u6f54\u6709\u529b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u986f\u793a\u4e86\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8655\u7406\u5411\u91cf\u5716\u5f62\u7684\u6709\u5e0c\u671b\u7684\u7d50\u679c\u3002\u7136\u800c\uff0c\u9019\u4e9b\u5de5\u4f5c\u50c5\u95dc\u6ce8\u5b9a\u6027\u7d50\u679c\u3001\u7406\u89e3\u6216\u7279\u5b9a\u985e\u578b\u7684\u5411\u91cf\u5716\u5f62\u3002\u6211\u5011\u63d0\u51fa VGBench\uff0c\u9019\u662f\u4e00\u500b\u7d9c\u5408\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30 LLM \u8655\u7406\u5411\u91cf\u5716\u5f62\u7684\u5404\u500b\u65b9\u9762\uff0c\u5305\u62ec (a) \u8996\u89ba\u7406\u89e3\u548c\u751f\u6210\uff0c(b) \u8a55\u4f30\u5404\u7a2e\u5411\u91cf\u5716\u5f62\u683c\u5f0f\uff0c(c) \u591a\u6a23\u5316\u7684\u554f\u984c\u985e\u578b\uff0c(d) \u5ee3\u6cdb\u7684\u63d0\u793a\u6280\u8853\uff0c(e) \u5728\u591a\u500b LLM \u4e0b\u3002\u5728\u6211\u5011\u6536\u96c6\u7684 4279 \u500b\u7406\u89e3\u548c 5845 \u500b\u751f\u6210\u7bc4\u4f8b\u4e2d\u9032\u884c\u8a55\u4f30\uff0c\u6211\u5011\u767c\u73fe LLM \u5728\u9019\u5169\u500b\u65b9\u9762\u90fd\u8868\u73fe\u51fa\u5f37\u5927\u7684\u80fd\u529b\uff0c\u540c\u6642\u5728\u4f4e\u968e\u683c\u5f0f (SVG) \u4e0a\u8868\u73fe\u51fa\u4e0d\u592a\u7406\u60f3\u7684\u6548\u80fd\u3002\u8cc7\u6599\u548c\u8a55\u4f30\u7ba1\u9053\u90fd\u5c07\u5728 https://vgbench.github.io/ \u958b\u6e90\u3002", "author": "Bocheng Zou et.al.", "authors": "Bocheng Zou, Mu Cai, Jianrui Zhang, Yong Jae Lee", "id": "2407.10972v1", "paper_url": "http://arxiv.org/abs/2407.10972v1", "repo": "https://github.com/vgbench/VGBench"}}