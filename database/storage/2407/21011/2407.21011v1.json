{"2407.21011": {"publish_time": "2024-07-30", "title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "paper_summary": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) have\ndemonstrated notable success in self-supervised representation learning across\nvarious tasks. However, the existing CLIP-like approaches often demand\nextensive GPU resources and prolonged training times due to the considerable\nsize of the model and dataset, making them poor for medical applications, in\nwhich large datasets are not always common. Meanwhile, the language model\nprompts are mainly manually derived from labels tied to images, potentially\noverlooking the richness of information within training samples. We introduce a\nnovel language-image Contrastive Learning method with an Efficient large\nlanguage model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of\nthe extensive pre-trained language and visual models. Furthermore, we present\nan efficient strategy for learning context-based prompts that mitigates the gap\nbetween informative clinical diagnostic data and simple class labels. Our\nmethod demonstrates state-of-the-art performance on multiple chest X-ray and\nmammography datasets compared with various baselines. The proposed parameter\nefficient framework can reduce the total trainable model size by 39% and reduce\nthe trainable language model to only 4% compared with the current BERT encoder.", "paper_summary_zh": "\u5c0d\u6bd4\u8a9e\u8a00\u5f71\u50cf\u9810\u8a13\u7df4 (CLIP) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u5c55\u73fe\u51fa\u5728\u5404\u9805\u4efb\u52d9\u4e2d\u4ee5\u81ea\u6211\u76e3\u7763\u8868\u5fb5\u5b78\u7fd2\u7372\u5f97\u986f\u8457\u6210\u529f\u7684\u6210\u679c\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 CLIP \u985e\u4f3c\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684 GPU \u8cc7\u6e90\u548c\u6f2b\u9577\u7684\u8a13\u7df4\u6642\u9593\uff0c\u56e0\u70ba\u6a21\u578b\u548c\u8cc7\u6599\u96c6\u7684\u898f\u6a21\u9f90\u5927\uff0c\u9019\u4f7f\u5f97\u5b83\u5011\u4e0d\u9069\u5408\u91ab\u7642\u61c9\u7528\uff0c\u56e0\u70ba\u91ab\u7642\u61c9\u7528\u4e2d\u4e26\u4e0d\u7e3d\u662f\u6703\u6709\u5927\u578b\u8cc7\u6599\u96c6\u3002\u540c\u6642\uff0c\u8a9e\u8a00\u6a21\u578b\u63d0\u793a\u4e3b\u8981\u4f86\u81ea\u8207\u5f71\u50cf\u76f8\u95dc\u7684\u6a19\u7c64\uff0c\u800c\u624b\u52d5\u884d\u751f\uff0c\u9019\u53ef\u80fd\u6703\u5ffd\u7565\u8a13\u7df4\u6a23\u672c\u4e2d\u8c50\u5bcc\u7684\u8cc7\u8a0a\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u8a9e\u8a00\u5f71\u50cf\u5c0d\u6bd4\u5b78\u7fd2\u65b9\u6cd5\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u500b\u9ad8\u6548\u7684\u5927\u8a9e\u8a00\u6a21\u578b\u548c\u63d0\u793a\u5fae\u8abf (CLEFT)\uff0c\u5b83\u5229\u7528\u4e86\u5ee3\u6cdb\u9810\u8a13\u7df4\u7684\u8a9e\u8a00\u548c\u8996\u89ba\u6a21\u578b\u7684\u512a\u52e2\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u5b78\u7fd2\u57fa\u65bc\u8108\u7d61\u63d0\u793a\u7684\u6709\u6548\u7b56\u7565\uff0c\u4ee5\u7e2e\u5c0f\u8cc7\u8a0a\u8c50\u5bcc\u7684\u81e8\u5e8a\u8a3a\u65b7\u8cc7\u6599\u548c\u7c21\u55ae\u985e\u5225\u6a19\u7c64\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u8207\u5404\u7a2e\u57fa\u6e96\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u591a\u500b\u80f8\u90e8 X \u5149\u548c\u4e73\u623f\u651d\u5f71\u8cc7\u6599\u96c6\u4e0a\u5c55\u73fe\u51fa\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u6240\u63d0\u51fa\u7684\u53c3\u6578\u6709\u6548\u67b6\u69cb\u53ef\u4ee5\u5c07\u7e3d\u9ad4\u53ef\u8a13\u7df4\u6a21\u578b\u5927\u5c0f\u6e1b\u5c11 39%\uff0c\u4e26\u5c07\u53ef\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b\u6e1b\u5c11\u5230\u50c5 4%\uff0c\u8207\u76ee\u524d\u7684 BERT \u7de8\u78bc\u5668\u76f8\u6bd4\u3002", "author": "Yuexi Du et.al.", "authors": "Yuexi Du, Brian Chang, Nicha C. Dvornek", "id": "2407.21011v1", "paper_url": "http://arxiv.org/abs/2407.21011v1", "repo": "https://github.com/xypb/cleft"}}