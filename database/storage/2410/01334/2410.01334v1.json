{"2410.01334": {"publish_time": "2024-10-02", "title": "Unveiling Language Skills under Circuits", "paper_summary": "The exploration of language skills in language models (LMs) has always been\none of the central goals in mechanistic interpretability. However, existing\ncircuit analyses often fall short in representing the full functional scope of\nthese models, primarily due to the exclusion of Feed-Forward layers.\nAdditionally, isolating the effect of a single language skill from a text,\nwhich inherently involves multiple entangled skills, poses a significant\nchallenge. To address these gaps, we introduce a novel concept, Memory Circuit,\na minimum unit that fully and independently manipulates the memory-reading\nfunctionality of a language model, and disentangle the transformer model\nprecisely into a circuit graph which is an ensemble of paths connecting\ndifferent memory circuits. Based on this disentanglement, we identify salient\ncircuit paths, named as skill paths, responsible for three crucial language\nskills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning\n(ICL) Skill, leveraging causal effect estimation through interventions and\ncounterfactuals. Our experiments on various datasets confirm the correspondence\nbetween our identified skill paths and language skills, and validate three\nlongstanding hypotheses: 1) Language skills are identifiable through circuit\ndissection; 2) Simple language skills reside in shallow layers, whereas complex\nlanguage skills are found in deeper layers; 3) Complex language skills are\nformed on top of simpler language skills. Our codes are available at:\nhttps://github.com/Zodiark-ch/Language-Skill-of-LLMs.", "paper_summary_zh": "<paragraph>\u5728\u8a9e\u8a00\u6a21\u578b (LM) \u4e2d\u63a2\u7d22\u8a9e\u8a00\u6280\u80fd\u4e00\u76f4\u662f\u6a5f\u68b0\u53ef\u89e3\u91cb\u6027\u7684\u6838\u5fc3\u76ee\u6a19\u4e4b\u4e00\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u96fb\u8def\u5206\u6790\u5f80\u5f80\u7121\u6cd5\u8868\u793a\u9019\u4e9b\u6a21\u578b\u7684\u5168\u90e8\u529f\u80fd\u7bc4\u570d\uff0c\u4e3b\u8981\u662f\u7531\u65bc\u6392\u9664\u4e86\u524d\u994b\u5c64\u3002\u6b64\u5916\uff0c\u5f9e\u6587\u672c\u4e2d\u5206\u96e2\u51fa\u55ae\u4e00\u8a9e\u8a00\u6280\u80fd\u7684\u5f71\u97ff\uff08\u9019\u672c\u8cea\u4e0a\u6d89\u53ca\u591a\u7a2e\u7cfe\u7e8f\u7684\u6280\u80fd\uff09\u69cb\u6210\u4e86\u4e00\u9805\u91cd\u5927\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u5dee\u8ddd\uff0c\u6211\u5011\u5f15\u5165\u4e86 Memory Circuit\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6982\u5ff5\uff0c\u5b83\u662f\u4e00\u500b\u6700\u5c0f\u55ae\u5143\uff0c\u53ef\u4ee5\u5b8c\u6574\u4e14\u7368\u7acb\u5730\u64cd\u4f5c\u8a9e\u8a00\u6a21\u578b\u7684\u8a18\u61b6\u9ad4\u8b80\u53d6\u529f\u80fd\uff0c\u4e26\u5c07 Transformer \u6a21\u578b\u7cbe\u78ba\u5730\u89e3\u958b\u6210\u4e00\u500b\u96fb\u8def\u5716\uff0c\u5b83\u662f\u4e00\u500b\u9023\u63a5\u4e0d\u540c\u8a18\u61b6\u9ad4\u96fb\u8def\u7684\u8def\u5f91\u96c6\u5408\u3002\u57fa\u65bc\u9019\u7a2e\u89e3\u958b\uff0c\u6211\u5011\u8b58\u5225\u51fa\u986f\u8457\u7684\u96fb\u8def\u8def\u5f91\uff0c\u7a31\u70ba\u6280\u80fd\u8def\u5f91\uff0c\u5b83\u8ca0\u8cac\u4e09\u9805\u95dc\u9375\u7684\u8a9e\u8a00\u6280\u80fd\uff0c\u5373\u524d\u4e00\u500b\u7b26\u865f\u6280\u80fd\u3001\u6b78\u7d0d\u6280\u80fd\u548c\u8a9e\u5883\u5b78\u7fd2 (ICL) \u6280\u80fd\uff0c\u5229\u7528\u56e0\u679c\u6548\u61c9\u4f30\u8a08\u901a\u904e\u5e72\u9810\u548c\u53cd\u4e8b\u5be6\u3002\u6211\u5011\u5728\u5404\u7a2e\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u8b49\u5be6\u4e86\u6211\u5011\u8b58\u5225\u51fa\u7684\u6280\u80fd\u8def\u5f91\u8207\u8a9e\u8a00\u6280\u80fd\u4e4b\u9593\u7684\u5c0d\u61c9\u95dc\u4fc2\uff0c\u4e26\u9a57\u8b49\u4e86\u4e09\u500b\u9577\u671f\u7684\u5047\u8a2d\uff1a1) \u8a9e\u8a00\u6280\u80fd\u53ef\u4ee5\u900f\u904e\u96fb\u8def\u89e3\u5256\u4f86\u8b58\u5225\uff1b2) \u7c21\u55ae\u7684\u8a9e\u8a00\u6280\u80fd\u5b58\u5728\u65bc\u6dfa\u5c64\u4e2d\uff0c\u800c\u8907\u96dc\u7684\u8a9e\u8a00\u6280\u80fd\u5247\u5b58\u5728\u65bc\u6df1\u5c64\u4e2d\uff1b3) \u8907\u96dc\u7684\u8a9e\u8a00\u6280\u80fd\u5efa\u7acb\u5728\u66f4\u7c21\u55ae\u7684\u8a9e\u8a00\u6280\u80fd\u4e4b\u4e0a\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/Zodiark-ch/Language-Skill-of-LLMs \u53d6\u5f97\u3002</paragraph>", "author": "Hang Chen et.al.", "authors": "Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang", "id": "2410.01334v1", "paper_url": "http://arxiv.org/abs/2410.01334v1", "repo": "null"}}