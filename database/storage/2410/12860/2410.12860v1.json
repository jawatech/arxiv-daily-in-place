{"2410.12860": {"publish_time": "2024-10-11", "title": "LLMD: A Large Language Model for Interpreting Longitudinal Medical Records", "paper_summary": "We introduce LLMD, a large language model designed to analyze a patient's\nmedical history based on their medical records. Along with domain knowledge,\nLLMD is trained on a large corpus of records collected over time and across\nfacilities, as well as tasks and labels that make nuanced connections among\nthem. This approach is critical to an accurate picture of patient health, and\nhas distinctive advantages over models trained on knowledge alone, unlabeled\nrecords, structured EHR data, or records from a single health system.\n  The recipe for LLMD continues pretraining a foundational model on both domain\nknowledge and the contents of millions of records. These span an average of 10\nyears of care and as many as 140 care sites per patient. LLMD is then\ninstruction fine-tuned on structuring and abstraction tasks. The former jointly\nidentify and normalize document metadata, provenance information, clinical\nnamed-entities, and ontology mappings, while the latter roll these into\nhigher-level representations, such a continuous era of time a patient was on a\nmedication. LLMD is deployed within a layered validation system that includes\ncontinual random audits and review by experts, e.g. based on uncertainty,\ndisease-specific rules, or use-case.\n  LLMD exhibits large gains over both more-powerful generalized models and\ndomain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state\nof the art accuracy on PubMedQA text responses, besting orders-of-magnitude\nlarger models. On production tasks, we show that LLMD significantly outperforms\nall other models evaluated, and among alternatives, large general purpose LLMs\nlike GPT-4o are more accurate than models emphasizing medical knowledge. We\nfind strong evidence that accuracy on today's medical benchmarks is not the\nmost significant factor when analyzing real-world patient data, an insight with\nimplications for future medical LLMs.'", "paper_summary_zh": "<paragraph>\u6211\u5011\u5f15\u5165\u4e86 LLMD\uff0c\u9019\u662f\u4e00\u500b\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u6839\u64da\u75c5\u6b77\u5206\u6790\u60a3\u8005\u7684\u75c5\u53f2\u3002\u9664\u4e86\u9818\u57df\u77e5\u8b58\u5916\uff0cLLMD \u9084\u63a5\u53d7\u4e86\u5927\u91cf\u96a8\u8457\u6642\u9593\u63a8\u79fb\u548c\u8de8\u8a2d\u65bd\u6536\u96c6\u7684\u8a18\u9304\u7684\u8a13\u7df4\uff0c\u4ee5\u53ca\u5728\u5b83\u5011\u4e4b\u9593\u5efa\u7acb\u7d30\u5fae\u806f\u7e6b\u7684\u4efb\u52d9\u548c\u6a19\u7c64\u3002\u9019\u7a2e\u65b9\u6cd5\u5c0d\u65bc\u6e96\u78ba\u63cf\u7e6a\u60a3\u8005\u5065\u5eb7\u72c0\u6cc1\u81f3\u95dc\u91cd\u8981\uff0c\u4e26\u4e14\u8207\u50c5\u63a5\u53d7\u77e5\u8b58\u8a13\u7df4\u7684\u6a21\u578b\u3001\u672a\u6a19\u8a18\u8a18\u9304\u3001\u7d50\u69cb\u5316\u7684 EHR \u6578\u64da\u6216\u4f86\u81ea\u55ae\u4e00\u5065\u5eb7\u7cfb\u7d71\u7684\u8a18\u9304\u76f8\u6bd4\uff0c\u5177\u6709\u986f\u8457\u512a\u52e2\u3002\nLLMD \u7684\u79d8\u8a23\u662f\u5c0d\u57fa\u790e\u6a21\u578b\u9032\u884c\u9810\u8a13\u7df4\uff0c\u65e2\u5305\u62ec\u9818\u57df\u77e5\u8b58\uff0c\u4e5f\u5305\u62ec\u6578\u767e\u842c\u689d\u8a18\u9304\u7684\u5167\u5bb9\u3002\u9019\u4e9b\u8a18\u9304\u5e73\u5747\u6db5\u84cb\u4e86\u6bcf\u4f4d\u60a3\u8005 10 \u5e74\u7684\u8b77\u7406\u6642\u9593\u548c\u591a\u9054 140 \u500b\u8b77\u7406\u5730\u9ede\u3002\u7136\u5f8c\u5c0d LLMD \u9032\u884c\u7d50\u69cb\u5316\u548c\u62bd\u8c61\u4efb\u52d9\u7684\u6307\u4ee4\u5fae\u8abf\u3002\u524d\u8005\u5171\u540c\u8b58\u5225\u548c\u6a19\u6e96\u5316\u6587\u6a94\u5143\u6578\u64da\u3001\u4f86\u6e90\u4fe1\u606f\u3001\u81e8\u5e8a\u547d\u540d\u5be6\u9ad4\u548c\u672c\u4f53\u6620\u5c04\uff0c\u800c\u5f8c\u8005\u5c07\u9019\u4e9b\u5167\u5bb9\u8f49\u63db\u70ba\u66f4\u9ad8\u7d1a\u5225\u7684\u8868\u793a\uff0c\u4f8b\u5982\u60a3\u8005\u670d\u85e5\u7684\u9023\u7e8c\u6642\u9593\u6bb5\u3002LLMD \u5728\u4e00\u500b\u5206\u5c64\u9a57\u8b49\u7cfb\u7d71\u4e2d\u90e8\u7f72\uff0c\u5176\u4e2d\u5305\u62ec\u6301\u7e8c\u7684\u96a8\u6a5f\u5be9\u6838\u548c\u5c08\u5bb6\u5be9\u67e5\uff0c\u4f8b\u5982\u57fa\u65bc\u4e0d\u78ba\u5b9a\u6027\u3001\u7279\u5b9a\u75be\u75c5\u898f\u5247\u6216\u7528\u4f8b\u3002\nLLMD \u5728\u529f\u80fd\u66f4\u5f37\u5927\u7684\u901a\u7528\u6a21\u578b\u548c\u7279\u5b9a\u9818\u57df\u6a21\u578b\u65b9\u9762\u90fd\u8868\u73fe\u51fa\u5de8\u5927\u7684\u512a\u52e2\u3002\u5728\u91ab\u5b78\u77e5\u8b58\u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0cLLMD-8B \u5728 PubMedQA \u6587\u672c\u97ff\u61c9\u65b9\u9762\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6e96\u78ba\u6027\uff0c\u512a\u65bc\u6578\u91cf\u7d1a\u66f4\u5927\u7684\u6a21\u578b\u3002\u5728\u751f\u7522\u4efb\u52d9\u4e2d\uff0c\u6211\u5011\u8868\u660e LLMD \u660e\u986f\u512a\u65bc\u6240\u6709\u5176\u4ed6\u8a55\u4f30\u6a21\u578b\uff0c\u4e26\u4e14\u5728\u66ff\u4ee3\u65b9\u6848\u4e2d\uff0c\u50cf GPT-4o \u9019\u6a23\u7684\u5927\u578b\u901a\u7528 LLM \u6bd4\u5f37\u8abf\u91ab\u5b78\u77e5\u8b58\u7684\u6a21\u578b\u66f4\u6e96\u78ba\u3002\u6211\u5011\u767c\u73fe\u5f37\u6709\u529b\u7684\u8b49\u64da\u8868\u660e\uff0c\u5728\u5206\u6790\u73fe\u5be6\u4e16\u754c\u7684\u60a3\u8005\u6578\u64da\u6642\uff0c\u7576\u4eca\u91ab\u5b78\u57fa\u6e96\u6e2c\u8a66\u7684\u6e96\u78ba\u6027\u4e26\u975e\u6700\u91cd\u8981\u7684\u56e0\u7d20\uff0c\u9019\u5c0d\u672a\u4f86\u7684\u91ab\u5b78 LLM \u4e5f\u6709\u5f71\u97ff\u3002</paragraph>", "author": "Robert Porter et.al.", "authors": "Robert Porter, Adam Diehl, Benjamin Pastel, J. Henry Hinnefeld, Lawson Nerenberg, Pye Maung, Sebastien Kerbrat, Gillian Hanson, Troy Astorino, Stephen J. Tarsa", "id": "2410.12860v1", "paper_url": "http://arxiv.org/abs/2410.12860v1", "repo": "null"}}