{"2405.19284": {"publish_time": "2024-05-29", "title": "Optimizing Foundation Model Inference on a Many-tiny-core Open-source RISC-V Platform", "paper_summary": "Transformer-based foundation models have become crucial for various domains,\nmost notably natural language processing (NLP) or computer vision (CV). These\nmodels are predominantly deployed on high-performance GPUs or hardwired\naccelerators with highly customized, proprietary instruction sets. Until now,\nlimited attention has been given to RISC-V-based general-purpose platforms. In\nour work, we present the first end-to-end inference results of transformer\nmodels on an open-source many-tiny-core RISC-V platform implementing\ndistributed Softmax primitives and leveraging ISA extensions for SIMD\nfloating-point operand streaming and instruction repetition, as well as\nspecialized DMA engines to minimize costly main memory accesses and to tolerate\ntheir latency. We focus on two foundational transformer topologies,\nencoder-only and decoder-only models. For encoder-only models, we demonstrate a\nspeedup of up to 12.8x between the most optimized implementation and the\nbaseline version. We reach over 79% FPU utilization and 294 GFLOPS/W,\noutperforming State-of-the-Art (SoA) accelerators by more than 2x utilizing the\nHW platform while achieving comparable throughput per computational unit. For\ndecoder-only topologies, we achieve 16.1x speedup in the Non-Autoregressive\n(NAR) mode and up to 35.6x speedup in the Autoregressive (AR) mode compared to\nthe baseline implementation. Compared to the best SoA dedicated accelerator, we\nachieve 2.04x higher FPU utilization.", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u7684\u57fa\u790e\u6a21\u578b\u5df2\u6210\u70ba\u5404\u7a2e\u9818\u57df\u7684\u95dc\u9375\uff0c\n\u6700\u8457\u540d\u7684\u662f\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u6216\u96fb\u8166\u8996\u89ba (CV)\u3002\u9019\u4e9b\n\u6a21\u578b\u4e3b\u8981\u90e8\u7f72\u5728\u9ad8\u6027\u80fd GPU \u6216\u786c\u63a5\u7dda\n\u52a0\u901f\u5668\u4e0a\uff0c\u5177\u6709\u9ad8\u5ea6\u5ba2\u88fd\u5316\u7684\u5c08\u6709\u6307\u4ee4\u96c6\u3002\u5230\u76ee\u524d\u70ba\u6b62\uff0c\n\u57fa\u65bc RISC-V \u7684\u901a\u7528\u5e73\u53f0\u53d7\u5230\u7684\u95dc\u6ce8\u6709\u9650\u3002\u5728\n\u6211\u5011\u7684\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 Transformer\n\u6a21\u578b\u5728\u958b\u6e90\u591a\u5fae\u6838\u5fc3 RISC-V \u5e73\u53f0\u4e0a\u7684\u7b2c\u4e00\u500b\u7aef\u5230\u7aef\u63a8\u8ad6\u7d50\u679c\uff0c\u5be6\u4f5c\n\u5206\u6563\u5f0f Softmax \u539f\u8a9e\u4e26\u5229\u7528 ISA \u5ef6\u4f38\u9032\u884c SIMD\n\u6d6e\u9ede\u904b\u7b97\u5143\u4e32\u6d41\u548c\u6307\u4ee4\u91cd\u8907\uff0c\u4ee5\u53ca\n\u5c08\u7528 DMA \u5f15\u64ce\u4ee5\u6700\u5c0f\u5316\u6602\u8cb4\u7684\u4e3b\u8a18\u61b6\u9ad4\u5b58\u53d6\u4e26\u5bb9\u5fcd\n\u5176\u5ef6\u9072\u3002\u6211\u5011\u5c08\u6ce8\u65bc\u5169\u500b\u57fa\u790e Transformer \u62d3\u64b2\uff0c\n\u50c5\u7de8\u78bc\u5668\u548c\u50c5\u89e3\u78bc\u5668\u6a21\u578b\u3002\u5c0d\u65bc\u50c5\u7de8\u78bc\u5668\u6a21\u578b\uff0c\u6211\u5011\u5c55\u793a\u4e86\n\u5728\u6700\u4f73\u5316\u5be6\u4f5c\u548c\u57fa\u7dda\u7248\u672c\u4e4b\u9593\u9ad8\u9054 12.8 \u500d\u7684\u52a0\u901f\u3002\u6211\u5011\u9054\u5230\u8d85\u904e 79% \u7684 FPU \u4f7f\u7528\u7387\u548c 294 GFLOPS/W\uff0c\n\u5728\u5229\u7528\nHW \u5e73\u53f0\u7684\u540c\u6642\uff0c\u5176\u6548\u80fd\u512a\u65bc\u6700\u5148\u9032 (SoA) \u52a0\u901f\u5668 2 \u500d\u4ee5\u4e0a\uff0c\u4e26\u5be6\u73fe\u6bcf\u8a08\u7b97\u55ae\u5143\u7684\u53ef\u6bd4\u541e\u5410\u91cf\u3002\u5c0d\u65bc\n\u50c5\u89e3\u78bc\u5668\u62d3\u64b2\uff0c\u6211\u5011\u5728\u975e\u81ea\u8ff4\u6b78\n(NAR) \u6a21\u5f0f\u4e2d\u5be6\u73fe\u4e86 16.1 \u500d\u7684\u52a0\u901f\uff0c\u5728\u81ea\u8ff4\u6b78\n(AR) \u6a21\u5f0f\u4e2d\u5be6\u73fe\u4e86\u9ad8\u9054 35.6 \u500d\u7684\u52a0\u901f\uff0c\u8207\n\u57fa\u7dda\u5be6\u4f5c\u76f8\u6bd4\u3002\u8207\u6700\u4f73 SoA \u5c08\u7528\u52a0\u901f\u5668\u76f8\u6bd4\uff0c\u6211\u5011\n\u5be6\u73fe\u4e86 2.04 \u500d\u66f4\u9ad8\u7684 FPU \u4f7f\u7528\u7387\u3002</paragraph>", "author": "Viviane Potocnik et.al.", "authors": "Viviane Potocnik, Luca Colagrande, Tim Fischer, Luca Bertaccini, Daniele Jahier Pagliari, Alessio Burrello, Luca Benini", "id": "2405.19284v1", "paper_url": "http://arxiv.org/abs/2405.19284v1", "repo": "null"}}