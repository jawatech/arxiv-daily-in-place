{"2410.06961": {"publish_time": "2024-10-09", "title": "Self-Boosting Large Language Models with Synthetic Preference Data", "paper_summary": "Through alignment with human preferences, Large Language Models (LLMs) have\nadvanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic\npreference data for model alignment. SynPO employs an iterative mechanism\nwherein a self-prompt generator creates diverse prompts, and a response\nimprover refines model responses progressively. This approach trains LLMs to\nautonomously learn the generative rewards for their own outputs and eliminates\nthe need for large-scale annotation of prompts and human preferences. After\nfour SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements\nin instruction-following abilities, achieving over 22.1% win rate improvements\non AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general\nperformance of LLMs on various tasks, validated by a 3.2 to 5.0 average score\nincrease on the well-recognized Open LLM leaderboard.", "paper_summary_zh": "\u900f\u904e\u8207\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u7522\u751f\u8aa0\u5be6\u3001\u7121\u5bb3\u4e14\u6709\u7528\u7684\u56de\u61c9\u65b9\u9762\u5df2\u53d6\u5f97\u986f\u8457\u9032\u5c55\u3002\u7136\u800c\uff0c\u6536\u96c6\u9ad8\u54c1\u8cea\u7684\u504f\u597d\u8cc7\u6599\u662f\u4e00\u500b\u8017\u8cbb\u8cc7\u6e90\u4e14\u9700\u8981\u5275\u9020\u529b\u7684\u904e\u7a0b\uff0c\u7279\u5225\u662f\u5c0d\u65bc LLM \u7684\u6301\u7e8c\u6539\u9032\u3002\u6211\u5011\u4ecb\u7d39 SynPO\uff0c\u4e00\u7a2e\u81ea\u6211\u63d0\u5347\u7684\u7bc4\u4f8b\uff0c\u5b83\u5229\u7528\u5408\u6210\u504f\u597d\u8cc7\u6599\u9032\u884c\u6a21\u578b\u5c0d\u9f4a\u3002SynPO \u63a1\u7528\u610f\u7fa9\u6a5f\u5236\uff0c\u5176\u4e2d\u81ea\u6211\u63d0\u793a\u7522\u751f\u5668\u6703\u7522\u751f\u4e0d\u540c\u7684\u63d0\u793a\uff0c\u800c\u56de\u61c9\u6539\u9032\u5668\u6703\u9010\u6b65\u6539\u5584\u6a21\u578b\u56de\u61c9\u3002\u9019\u7a2e\u65b9\u6cd5\u8a13\u7df4 LLM \u81ea\u4e3b\u5b78\u7fd2\u5176\u8f38\u51fa\u7522\u751f\u7684\u734e\u52f5\uff0c\u4e26\u6d88\u9664\u4e86\u5c0d\u63d0\u793a\u548c\u5927\u898f\u6a21\u6a19\u8a18\u4eba\u985e\u504f\u597d\u7684\u9700\u6c42\u3002\u5728\u56db\u6b21 SynPO \u8fed\u4ee3\u5f8c\uff0cLlama3-8B \u548c Mistral-7B \u5728\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\u4e0a\u8868\u73fe\u51fa\u986f\u8457\u7684\u63d0\u5347\uff0c\u5728 AlpacaEval 2.0 \u548c ArenaHard \u4e0a\u7684\u7372\u52dd\u7387\u63d0\u5347\u8d85\u904e 22.1%\u3002\u540c\u6642\uff0cSynPO \u63d0\u5347\u4e86 LLM \u5728\u5404\u7a2e\u4efb\u52d9\u4e0a\u7684\u6574\u9ad4\u8868\u73fe\uff0c\u9019\u7531\u516c\u8a8d\u7684 Open LLM \u6392\u884c\u699c\u4e0a\u5e73\u5747\u5206\u6578\u63d0\u5347 3.2 \u5230 5.0 \u5f97\u5230\u9a57\u8b49\u3002", "author": "Qingxiu Dong et.al.", "authors": "Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, Furu Wei", "id": "2410.06961v1", "paper_url": "http://arxiv.org/abs/2410.06961v1", "repo": "null"}}