{"2406.10085": {"publish_time": "2024-06-14", "title": "Enhancing Question Answering on Charts Through Effective Pre-training Tasks", "paper_summary": "To completely understand a document, the use of textual information is not\nenough. Understanding visual cues, such as layouts and charts, is also\nrequired. While the current state-of-the-art approaches for document\nunderstanding (both OCR-based and OCR-free) work well, a thorough analysis of\ntheir capabilities and limitations has not yet been performed. Therefore, in\nthis work, we addresses the limitation of current VisualQA models when applied\nto charts and plots. To investigate shortcomings of the state-of-the-art\nmodels, we conduct a comprehensive behavioral analysis, using ChartQA as a case\nstudy. Our findings indicate that existing models particularly underperform in\nanswering questions related to the chart's structural and visual context, as\nwell as numerical information. To address these issues, we propose three simple\npre-training tasks that enforce the existing model in terms of both\nstructural-visual knowledge, as well as its understanding of numerical\nquestions. We evaluate our pre-trained model (called MatCha-v2) on three chart\ndatasets - both extractive and abstractive question datasets - and observe that\nit achieves an average improvement of 1.7% over the baseline model.", "paper_summary_zh": "\u8981\u5b8c\u5168\u7406\u89e3\u6587\u4ef6\uff0c\u50c5\u4f7f\u7528\u6587\u5b57\u8cc7\u8a0a\u662f\u4e0d\u5920\u7684\u3002\u4e5f\u9700\u8981\u7406\u89e3\u8996\u89ba\u63d0\u793a\uff0c\u4f8b\u5982\u914d\u7f6e\u548c\u5716\u8868\u3002\u96d6\u7136\u7528\u65bc\u6587\u4ef6\u7406\u89e3\uff08\u57fa\u65bc OCR \u548c\u975e OCR\uff09\u7684\u76ee\u524d\u6700\u5148\u9032\u7684\u65b9\u6cd5\u904b\u4f5c\u826f\u597d\uff0c\u4f46\u5c1a\u672a\u5fb9\u5e95\u5206\u6790\u5b83\u5011\u7684\u80fd\u529b\u548c\u9650\u5236\u3002\u56e0\u6b64\uff0c\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u61c9\u7528\u65bc\u5716\u8868\u548c\u7e6a\u5716\u6642\u7684\u7576\u524d VisualQA \u6a21\u578b\u7684\u9650\u5236\u3002\u70ba\u4e86\u8abf\u67e5\u6700\u5148\u9032\u6a21\u578b\u7684\u7f3a\u9ede\uff0c\u6211\u5011\u4ee5 ChartQA \u70ba\u6848\u4f8b\u7814\u7a76\uff0c\u9032\u884c\u4e86\u5168\u9762\u7684\u884c\u70ba\u5206\u6790\u3002\u6211\u5011\u7684\u767c\u73fe\u8868\u660e\uff0c\u73fe\u6709\u6a21\u578b\u5728\u56de\u7b54\u8207\u5716\u8868\u7684\u7d50\u69cb\u548c\u8996\u89ba\u5167\u5bb9\u4ee5\u53ca\u6578\u503c\u8cc7\u8a0a\u76f8\u95dc\u7684\u554f\u984c\u6642\u8868\u73fe\u7279\u5225\u4e0d\u4f73\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e09\u9805\u7c21\u55ae\u7684\u9810\u8a13\u7df4\u4efb\u52d9\uff0c\u5728\u7d50\u69cb\u8996\u89ba\u77e5\u8b58\u548c\u5c0d\u6578\u503c\u554f\u984c\u7684\u7406\u89e3\u65b9\u9762\u5f37\u5236\u57f7\u884c\u73fe\u6709\u6a21\u578b\u3002\u6211\u5011\u5728\u4e09\u500b\u5716\u8868\u8cc7\u6599\u96c6\uff08\u8403\u53d6\u5f0f\u548c\u62bd\u8c61\u5f0f\u554f\u984c\u8cc7\u6599\u96c6\uff09\u4e0a\u8a55\u4f30\u6211\u5011\u7684\u9810\u8a13\u7df4\u6a21\u578b\uff08\u7a31\u70ba MatCha-v2\uff09\uff0c\u4e26\u89c0\u5bdf\u5230\u5b83\u6bd4\u57fa\u6e96\u6a21\u578b\u5e73\u5747\u63d0\u9ad8\u4e86 1.7%\u3002", "author": "Ashim Gupta et.al.", "authors": "Ashim Gupta, Vivek Gupta, Shuo Zhang, Yujie He, Ning Zhang, Shalin Shah", "id": "2406.10085v1", "paper_url": "http://arxiv.org/abs/2406.10085v1", "repo": "null"}}