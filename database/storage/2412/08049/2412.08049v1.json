{"2412.08049": {"publish_time": "2024-12-11", "title": "M2SE: A Multistage Multitask Instruction Tuning Strategy for Unified Sentiment and Emotion Analysis", "paper_summary": "Sentiment analysis and emotion recognition are crucial for applications such\nas human-computer interaction and depression detection. Traditional unimodal\nmethods often fail to capture the complexity of emotional expressions due to\nconflicting signals from different modalities. Current Multimodal Large\nLanguage Models (MLLMs) also face challenges in detecting subtle facial\nexpressions and addressing a wide range of emotion-related tasks. To tackle\nthese issues, we propose M2SE, a Multistage Multitask Sentiment and Emotion\nInstruction Tuning Strategy for general-purpose MLLMs. It employs a combined\napproach to train models on tasks such as multimodal sentiment analysis,\nemotion recognition, facial expression recognition, emotion reason inference,\nand emotion cause-pair extraction. We also introduce the Emotion Multitask\ndataset (EMT), a custom dataset that supports these five tasks. Our model,\nEmotion Universe (EmoVerse), is built on a basic MLLM framework without\nmodifications, yet it achieves substantial improvements across these tasks when\ntrained with the M2SE strategy. Extensive experiments demonstrate that EmoVerse\noutperforms existing methods, achieving state-of-the-art results in sentiment\nand emotion tasks. These results highlight the effectiveness of M2SE in\nenhancing multimodal emotion perception. The dataset and code are available at\nhttps://github.com/xiaoyaoxinyi/M2SE.", "paper_summary_zh": "\u60c5\u7dd2\u5206\u6790\u548c\u60c5\u7dd2\u8fa8\u8b58\u5c0d\u65bc\u4eba\u6a5f\u4e92\u52d5\u548c\u6182\u9b31\u75c7\u5075\u6e2c\u7b49\u61c9\u7528\u81f3\u95dc\u91cd\u8981\u3002\u50b3\u7d71\u7684\u55ae\u6a21\u614b\u65b9\u6cd5\u7531\u65bc\u4e0d\u540c\u6a21\u614b\u7684\u8a0a\u865f\u76f8\u4e92\u885d\u7a81\uff0c\u5e38\u5e38\u7121\u6cd5\u6355\u6349\u5230\u60c5\u7dd2\u8868\u9054\u7684\u8907\u96dc\u6027\u3002\u76ee\u524d\u7684\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u5728\u5075\u6e2c\u7d30\u5fae\u9762\u90e8\u8868\u60c5\u548c\u8655\u7406\u5ee3\u6cdb\u7684\u60c5\u7dd2\u76f8\u95dc\u4efb\u52d9\u65b9\u9762\u4e5f\u9762\u81e8\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 M2SE\uff0c\u4e00\u7a2e\u91dd\u5c0d\u901a\u7528 MLLM \u7684\u591a\u968e\u6bb5\u591a\u4efb\u52d9\u60c5\u7dd2\u548c\u60c5\u7dd2\u6307\u4ee4\u8abf\u6574\u7b56\u7565\u3002\u5b83\u63a1\u7528\u4e00\u7a2e\u7d50\u5408\u65b9\u6cd5\u4f86\u8a13\u7df4\u6a21\u578b\uff0c\u57f7\u884c\u591a\u6a21\u614b\u60c5\u7dd2\u5206\u6790\u3001\u60c5\u7dd2\u8fa8\u8b58\u3001\u9762\u90e8\u8868\u60c5\u8fa8\u8b58\u3001\u60c5\u7dd2\u539f\u56e0\u63a8\u8ad6\u548c\u60c5\u7dd2\u6210\u56e0\u5c0d\u8403\u53d6\u7b49\u4efb\u52d9\u3002\u6211\u5011\u4e5f\u5f15\u5165\u4e86\u60c5\u7dd2\u591a\u4efb\u52d9\u8cc7\u6599\u96c6 (EMT)\uff0c\u9019\u662f\u4e00\u500b\u652f\u63f4\u9019\u4e94\u9805\u4efb\u52d9\u7684\u5ba2\u88fd\u5316\u8cc7\u6599\u96c6\u3002\u6211\u5011\u7684\u6a21\u578b\u60c5\u7dd2\u5b87\u5b99 (EmoVerse) \u5efa\u7acb\u5728\u4e00\u500b\u57fa\u672c\u7684 MLLM \u67b6\u69cb\u4e0a\uff0c\u6c92\u6709\u4fee\u6539\uff0c\u4f46\u5728\u4f7f\u7528 M2SE \u7b56\u7565\u8a13\u7df4\u5f8c\uff0c\u5728\u9019\u4e9b\u4efb\u52d9\u4e2d\u90fd\u53d6\u5f97\u4e86\u986f\u8457\u7684\u9032\u6b65\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cEmoVerse \u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u5728\u60c5\u7dd2\u548c\u60c5\u611f\u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u7d50\u679c\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86 M2SE \u5728\u589e\u5f37\u591a\u6a21\u614b\u60c5\u7dd2\u611f\u77e5\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8cc7\u6599\u96c6\u548c\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/xiaoyaoxinyi/M2SE \u53d6\u5f97\u3002", "author": "Ao Li et.al.", "authors": "Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang", "id": "2412.08049v1", "paper_url": "http://arxiv.org/abs/2412.08049v1", "repo": "null"}}