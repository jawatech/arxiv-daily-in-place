{"2410.08964": {"publish_time": "2024-10-11", "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving", "paper_summary": "Large Language Models (LLMs) have achieved state-of-the-art performance\nacross numerous tasks. However, these advancements have predominantly benefited\n\"first-class\" languages such as English and Chinese, leaving many other\nlanguages underrepresented. This imbalance, while limiting broader\napplications, generates a natural preference ranking between languages,\noffering an opportunity to bootstrap the multilingual capabilities of LLM in a\nself-improving manner. Thus, we propose $\\textit{Language Imbalance Driven\nRewarding}$, where the inherent imbalance between dominant and non-dominant\nlanguages within LLMs is leveraged as a reward signal. Iterative DPO training\ndemonstrates that this approach not only enhances LLM performance in\nnon-dominant languages but also improves the dominant language's capacity,\nthereby yielding an iterative reward signal. Fine-tuning\nMeta-Llama-3-8B-Instruct over two iterations of this approach results in\ncontinuous improvements in multilingual performance across\ninstruction-following and arithmetic reasoning tasks, evidenced by an average\nimprovement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9%\naccuracy on the MGSM benchmark. This work serves as an initial exploration,\npaving the way for multilingual self-improvement of LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a31\u591a\u4efb\u52d9\u4e2d\u90fd\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u9019\u4e9b\u9032\u6b65\u4e3b\u8981\u4f7f\u82f1\u8a9e\u548c\u4e2d\u6587\u7b49\u300c\u4e00\u7d1a\u300d\u8a9e\u8a00\u53d7\u76ca\uff0c\u5c0e\u81f4\u8a31\u591a\u5176\u4ed6\u8a9e\u8a00\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u9019\u7a2e\u4e0d\u5e73\u8861\u9650\u5236\u4e86\u66f4\u5ee3\u6cdb\u7684\u61c9\u7528\uff0c\u540c\u6642\u5728\u8a9e\u8a00\u4e4b\u9593\u7522\u751f\u4e86\u81ea\u7136\u7684\u504f\u597d\u6392\u540d\uff0c\u63d0\u4f9b\u4e86\u4e00\u500b\u4ee5\u81ea\u6211\u63d0\u5347\u7684\u65b9\u5f0f\u5f15\u5c0e LLM \u591a\u8a9e\u8a00\u80fd\u529b\u7684\u6a5f\u6703\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u300c\u8a9e\u8a00\u4e0d\u5e73\u8861\u9a45\u52d5\u734e\u52f5\u300d\uff0c\u5176\u4e2d LLM \u4e2d\u4e3b\u5c0e\u8a9e\u8a00\u548c\u975e\u4e3b\u5c0e\u8a9e\u8a00\u4e4b\u9593\u7684\u56fa\u6709\u4e0d\u5e73\u8861\u88ab\u5229\u7528\u70ba\u734e\u52f5\u8a0a\u865f\u3002\u53cd\u8986\u7684 DPO \u8a13\u7df4\u8b49\u660e\uff0c\u9019\u7a2e\u65b9\u6cd5\u4e0d\u50c5\u589e\u5f37\u4e86 LLM \u5728\u975e\u4e3b\u5c0e\u8a9e\u8a00\u4e2d\u7684\u6548\u80fd\uff0c\u4e5f\u63d0\u5347\u4e86\u4e3b\u5c0e\u8a9e\u8a00\u7684\u80fd\u529b\uff0c\u5f9e\u800c\u7522\u751f\u53cd\u8986\u7684\u734e\u52f5\u8a0a\u865f\u3002\u91dd\u5c0d\u6b64\u65b9\u6cd5\u9032\u884c\u5169\u6b21\u53cd\u8986\u5fae\u8abf Meta-Llama-3-8B-Instruct\uff0c\u7d50\u679c\u986f\u793a\u591a\u8a9e\u8a00\u6548\u80fd\u6301\u7e8c\u6539\u5584\uff0c\u5305\u62ec\u9075\u5faa\u6307\u793a\u548c\u7b97\u8853\u63a8\u7406\u4efb\u52d9\uff0c\u9019\u5f9e X-AlpacaEval \u6392\u884c\u699c\u4e0a\u5e73\u5747\u63d0\u5347 7.46% \u7684\u7372\u52dd\u7387\u548c MGSM \u57fa\u6e96\u4e0a 13.9% \u7684\u6e96\u78ba\u5ea6\u4e2d\u5f97\u5230\u8b49\u660e\u3002\u9019\u9805\u5de5\u4f5c\u4f5c\u70ba\u4e00\u9805\u521d\u6b65\u63a2\u7d22\uff0c\u70ba LLM \u7684\u591a\u8a9e\u8a00\u81ea\u6211\u63d0\u5347\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Wen Yang et.al.", "authors": "Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang", "id": "2410.08964v1", "paper_url": "http://arxiv.org/abs/2410.08964v1", "repo": "null"}}