{"2410.06718": {"publish_time": "2024-10-09", "title": "MatMamba: A Matryoshka State Space Model", "paper_summary": "State Space Models (SSMs) like Mamba2 are a promising alternative to\nTransformers, with faster theoretical training and inference times --\nespecially for long context lengths. Recent work on Matryoshka Representation\nLearning -- and its application to Transformer backbones in works like\nMatFormer -- showed how to introduce nested granularities of smaller submodels\nin one universal elastic model. In this work, we present MatMamba: a state\nspace model which combines Matryoshka-style learning with Mamba2, by modifying\nthe block to contain nested dimensions to enable joint training and adaptive\ninference. MatMamba allows for efficient and adaptive deployment across various\nmodel sizes. We train a single large MatMamba model and are able to get a\nnumber of smaller nested models for free -- while maintaining or improving upon\nthe performance of a baseline smaller model trained from scratch. We train\nlanguage and image models at a variety of parameter sizes from 35M to 1.4B. Our\nresults on ImageNet and FineWeb show that MatMamba models scale comparably to\nTransformers, while having more efficient inference characteristics. This makes\nMatMamba a practically viable option for deploying large-scale models in an\nelastic way based on the available inference compute. Code and models are open\nsourced at \\url{https://github.com/ScaledFoundations/MatMamba}", "paper_summary_zh": "\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM)\uff0c\u4f8b\u5982 Mamba2\uff0c\u662f Transformer \u7684\u6709\u524d\u9014\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7406\u8ad6\u8a13\u7df4\u548c\u63a8\u8ad6\u6642\u9593\u66f4\u5feb\u2014\u2014\u7279\u5225\u662f\u5c0d\u65bc\u9577\u5167\u5bb9\u9577\u5ea6\u3002\u6700\u8fd1\u95dc\u65bc Matryoshka \u8868\u793a\u5b78\u7fd2\u7684\u5de5\u4f5c\u2014\u2014\u4ee5\u53ca\u5b83\u5728 MatFormer \u7b49\u4f5c\u54c1\u4e2d\u5c0d Transformer \u4e3b\u5e79\u7684\u61c9\u7528\u2014\u2014\u5c55\u793a\u4e86\u5982\u4f55\u5728\u4e00\u500b\u901a\u7528\u7684\u5f48\u6027\u6a21\u578b\u4e2d\u5f15\u5165\u8f03\u5c0f\u5b50\u6a21\u578b\u7684\u5d4c\u5957\u7c92\u5ea6\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 MatMamba\uff1a\u4e00\u7a2e\u72c0\u614b\u7a7a\u9593\u6a21\u578b\uff0c\u5b83\u901a\u904e\u4fee\u6539\u584a\u4ee5\u5305\u542b\u5d4c\u5957\u7dad\u5ea6\u4f86\u7d50\u5408 Matryoshka \u98a8\u683c\u7684\u5b78\u7fd2\u8207 Mamba2\uff0c\u4ee5\u5be6\u73fe\u806f\u5408\u8a13\u7df4\u548c\u81ea\u9069\u61c9\u63a8\u8ad6\u3002MatMamba \u5141\u8a31\u8de8\u5404\u7a2e\u6a21\u578b\u5927\u5c0f\u9032\u884c\u9ad8\u6548\u548c\u81ea\u9069\u61c9\u7684\u90e8\u7f72\u3002\u6211\u5011\u8a13\u7df4\u4e00\u500b\u5927\u578b MatMamba \u6a21\u578b\uff0c\u4e26\u4e14\u80fd\u5920\u514d\u8cbb\u7372\u5f97\u8a31\u591a\u8f03\u5c0f\u7684\u5d4c\u5957\u6a21\u578b\u2014\u2014\u540c\u6642\u4fdd\u6301\u6216\u6539\u9032\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u7684\u57fa\u6e96\u8f03\u5c0f\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u5011\u4ee5 35M \u5230 1.4B \u7684\u5404\u7a2e\u53c3\u6578\u5927\u5c0f\u8a13\u7df4\u8a9e\u8a00\u548c\u5716\u50cf\u6a21\u578b\u3002\u6211\u5011\u5728 ImageNet \u548c FineWeb \u4e0a\u7684\u7d50\u679c\u8868\u660e\uff0cMatMamba \u6a21\u578b\u53ef\u4ee5\u8207 Transformer \u9032\u884c\u6bd4\u8f03\uff0c\u540c\u6642\u5177\u6709\u66f4\u6709\u6548\u7684\u63a8\u8ad6\u7279\u6027\u3002\u9019\u4f7f\u5f97 MatMamba \u6210\u70ba\u4e00\u7a2e\u5be6\u7528\u7684\u53ef\u884c\u9078\u9805\uff0c\u53ef\u4ee5\u7528\u5f48\u6027\u65b9\u5f0f\u90e8\u7f72\u5927\u898f\u6a21\u6a21\u578b\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u53ef\u7528\u7684\u63a8\u8ad6\u8a08\u7b97\u3002\u4ee3\u78bc\u548c\u6a21\u578b\u5728 \\url{https://github.com/ScaledFoundations/MatMamba} \u958b\u6e90", "author": "Abhinav Shukla et.al.", "authors": "Abhinav Shukla, Sai Vemprala, Aditya Kusupati, Ashish Kapoor", "id": "2410.06718v1", "paper_url": "http://arxiv.org/abs/2410.06718v1", "repo": "https://github.com/scaledfoundations/matmamba"}}