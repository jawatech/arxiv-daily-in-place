{"2410.07820": {"publish_time": "2024-10-10", "title": "Mitigating Gender Bias in Code Large Language Models via Model Editing", "paper_summary": "In recent years, with the maturation of large language model (LLM) technology\nand the emergence of high-quality programming code datasets, researchers have\nbecome increasingly confident in addressing the challenges of program synthesis\nautomatically. However, since most of the training samples for LLMs are\nunscreened, it is inevitable that LLMs' performance may not align with\nreal-world scenarios, leading to the presence of social bias. To evaluate and\nquantify the gender bias in code LLMs, we propose a dataset named CodeGenBias\n(Gender Bias in the Code Generation) and an evaluation metric called FB-Score\n(Factual Bias Score) based on the actual gender distribution of correlative\nprofessions. With the help of CodeGenBias and FB-Score, we evaluate and analyze\nthe gender bias in eight mainstream Code LLMs. Previous work has demonstrated\nthat model editing methods that perform well in knowledge editing have the\npotential to mitigate social bias in LLMs. Therefore, we develop a model\nediting approach named MG-Editing (Multi-Granularity model Editing), which\nincludes the locating and editing phases. Our model editing method MG-Editing\ncan be applied at five different levels of model parameter granularity: full\nparameters level, layer level, module level, row level, and neuron level.\nExtensive experiments not only demonstrate that our MG-Editing can effectively\nmitigate the gender bias in code LLMs while maintaining their general code\ngeneration capabilities, but also showcase its excellent generalization. At the\nsame time, the experimental results show that, considering both the gender bias\nof the model and its general code generation capability, MG-Editing is most\neffective when applied at the row and neuron levels of granularity.", "paper_summary_zh": "<paragraph>\u8fd1\u5e74\u6765\uff0c\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u6280\u672f\u7684\u6210\u719f\n\u4ee5\u53ca\u9ad8\u8d28\u91cf\u7f16\u7a0b\u4ee3\u7801\u6570\u636e\u96c6\u7684\u51fa\u73b0\uff0c\u7814\u7a76\u4eba\u5458\u5bf9\n\u81ea\u52a8\u89e3\u51b3\u7a0b\u5e8f\u5408\u6210\u7684\u6311\u6218\u8d8a\u6765\u8d8a\u6709\u4fe1\u5fc3\u3002\u7136\u800c\uff0c\u7531\u4e8e LLM \u7684\u5927\u591a\u6570\u8bad\u7ec3\u6837\u672c\u662f\n\u672a\u7ecf\u7b5b\u9009\u7684\uff0c\u56e0\u6b64 LLM \u7684\u6027\u80fd\u4e0d\u53ef\u907f\u514d\u5730\u53ef\u80fd\u4e0e\n\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e0d\u4e00\u81f4\uff0c\u4ece\u800c\u5bfc\u81f4\u793e\u4f1a\u504f\u89c1\u7684\u5b58\u5728\u3002\u4e3a\u4e86\u8bc4\u4f30\u548c\n\u91cf\u5316\u4ee3\u7801 LLM \u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a CodeGenBias \u7684\u6570\u636e\u96c6\n\uff08\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff09\u548c\u4e00\u4e2a\u57fa\u4e8e\u76f8\u5173\u804c\u4e1a\u7684\u5b9e\u9645\u6027\u522b\u5206\u5e03\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u79f0\u4e3a FB-Score\n\uff08\u4e8b\u5b9e\u504f\u89c1\u5206\u6570\uff09\u3002\u5728 CodeGenBias \u548c FB-Score \u7684\u5e2e\u52a9\u4e0b\uff0c\u6211\u4eec\u8bc4\u4f30\u548c\u5206\u6790\n\u4e86\u516b\u4e2a\u4e3b\u6d41\u4ee3\u7801 LLM \u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u8bc1\u660e\n\u5728\u77e5\u8bc6\u7f16\u8f91\u4e2d\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5177\u6709\n\u51cf\u8f7b LLM \u4e2d\u793e\u4f1a\u504f\u89c1\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a MG-Editing \u7684\u6a21\u578b\n\u7f16\u8f91\u65b9\u6cd5\uff08\u591a\u7c92\u5ea6\u6a21\u578b\u7f16\u8f91\uff09\uff0c\u5176\u4e2d\u5305\u62ec\u5b9a\u4f4d\u548c\u7f16\u8f91\u9636\u6bb5\u3002\u6211\u4eec\u7684\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5 MG-Editing\n\u53ef\u4ee5\u5728\u6a21\u578b\u53c2\u6570\u7c92\u5ea6\u7684\u4e94\u4e2a\u4e0d\u540c\u7ea7\u522b\u4e0a\u5e94\u7528\uff1a\u5b8c\u6574\n\u53c2\u6570\u7ea7\u522b\u3001\u5c42\u7ea7\u522b\u3001\u6a21\u5757\u7ea7\u522b\u3001\u884c\u7ea7\u522b\u548c\u795e\u7ecf\u5143\u7ea7\u522b\u3002\n\u5927\u91cf\u7684\u5b9e\u9a8c\u4e0d\u4ec5\u8bc1\u660e\u4e86\u6211\u4eec\u7684 MG-Editing \u53ef\u4ee5\u6709\u6548\n\u51cf\u8f7b\u4ee3\u7801 LLM \u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u901a\u7528\u4ee3\u7801\n\u751f\u6210\u80fd\u529b\uff0c\u4f46\u4e5f\u5c55\u793a\u4e86\u5176\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u3002\u540c\u65f6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8003\u8651\u6a21\u578b\u7684\u6027\u522b\u504f\u89c1\n\u53ca\u5176\u901a\u7528\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0cMG-Editing \u5728\u884c\u548c\u795e\u7ecf\u5143\u7ea7\u522b\u5e94\u7528\u65f6\u6700\u6709\u6548\n\u7c92\u5ea6\u3002</paragraph>", "author": "Zhanyue Qin et.al.", "authors": "Zhanyue Qin, Haochuan Wang, Zecheng Wang, Deyuan Liu, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, Dianbo Sui", "id": "2410.07820v1", "paper_url": "http://arxiv.org/abs/2410.07820v1", "repo": "null"}}