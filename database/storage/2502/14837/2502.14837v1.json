{"2502.14837": {"publish_time": "2025-02-20", "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs", "paper_summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.", "paper_summary_zh": "\u591a\u982d\u6f5b\u5728\u6ce8\u610f\u529b (MLA) \u662f DeepSeek \u63d0\u51fa\u7684\u4e00\u7a2e\u5275\u65b0\u67b6\u69cb\uff0c\u65e8\u5728\u901a\u904e\u5c07\u9375\u503c (KV) \u5feb\u53d6\u5927\u5e45\u58d3\u7e2e\u6210\u6f5b\u5728\u5411\u91cf\uff0c\u78ba\u4fdd\u6709\u6548\u7387\u4e14\u7d93\u6fdf\u7684\u63a8\u8ad6\u3002\u8207 MLA \u76f8\u6bd4\uff0c\u63a1\u7528\u591a\u982d\u6ce8\u610f\u529b (MHA) \u53ca\u5176\u8b8a\u9ad4\uff08\u4f8b\u5982\u5206\u7d44\u67e5\u8a62\u6ce8\u610f\u529b (GQA)\uff09\u7684\u6a19\u6e96 LLM \u6703\u51fa\u73fe\u986f\u8457\u7684\u6210\u672c\u52a3\u52e2\u3002\u8b93\u8a13\u7df4\u5b8c\u5584\u7684 LLM\uff08\u4f8b\u5982 Llama\uff09\u80fd\u5920\u5feb\u901f\u9069\u61c9 MLA\uff0c\u800c\u7121\u9700\u5f9e\u982d\u958b\u59cb\u9810\u8a13\u7df4\uff0c\u9019\u65e2\u6709\u610f\u7fa9\u53c8\u5177\u6709\u6311\u6230\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u500b\u8cc7\u6599\u6709\u6548\u5fae\u8abf\u65b9\u6cd5\uff0c\u7528\u65bc\u5f9e MHA \u8f49\u63db\u5230 MLA (MHA2MLA)\uff0c\u5176\u4e2d\u5305\u542b\u5169\u500b\u95dc\u9375\u7d44\u6210\u90e8\u5206\uff1a\u5c0d\u65bc\u90e8\u5206 RoPE\uff0c\u6211\u5011\u5f9e\u67e5\u8a62\u548c\u9375\u7684\u7dad\u5ea6\u4e2d\u79fb\u9664\u5c0d\u6ce8\u610f\u529b\u5206\u6578\u8ca2\u737b\u8f03\u5c0f\u7684 RoPE\uff0c\u5c0d\u65bc\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u6211\u5011\u57fa\u65bc\u9375\u548c\u503c\u7684\u9810\u8a13\u7df4\u53c3\u6578\u5f15\u5165\u806f\u5408 SVD \u8fd1\u4f3c\u3002\u9019\u4e9b\u7d93\u904e\u4ed4\u7d30\u8a2d\u8a08\u7684\u7b56\u7565\u8b93 MHA2MLA \u80fd\u5920\u50c5\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206\u8cc7\u6599 (0.3% \u81f3 0.6%) \u4f86\u6062\u5fa9\u6548\u80fd\uff0c\u5927\u5e45\u964d\u4f4e\u63a8\u8ad6\u6210\u672c\uff0c\u540c\u6642\u8207\u58d3\u7e2e\u6280\u8853\uff08\u4f8b\u5982 KV \u5feb\u53d6\u91cf\u5316\uff09\u7121\u7e2b\u6574\u5408\u3002\u4f8b\u5982\uff0cLlama2-7B \u7684 KV \u5feb\u53d6\u5927\u5c0f\u6e1b\u5c11\u4e86 92.19%\uff0c\u800c LongBench \u6548\u80fd\u50c5\u4e0b\u964d\u4e86 0.5%\u3002", "author": "Tao Ji et.al.", "authors": "Tao Ji, Bin Guo, Yuanbin Wu, Qipeng Guo, Lixing Shen, Zhan Chen, Xipeng Qiu, Qi Zhang, Tao Gui", "id": "2502.14837v1", "paper_url": "http://arxiv.org/abs/2502.14837v1", "repo": "null"}}