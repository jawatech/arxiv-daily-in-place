{"2405.15319": {"publish_time": "2024-05-24", "title": "Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training", "paper_summary": "LLMs are computationally expensive to pre-train due to their large scale.\nModel growth emerges as a promising approach by leveraging smaller models to\naccelerate the training of larger ones. However, the viability of these model\ngrowth methods in efficient LLM pre-training remains underexplored. This work\nidentifies three critical $\\underline{\\textit{O}}$bstacles: ($\\textit{O}$1)\nlack of comprehensive evaluation, ($\\textit{O}$2) untested viability for\nscaling, and ($\\textit{O}$3) lack of empirical guidelines. To tackle\n$\\textit{O}$1, we summarize existing approaches into four atomic growth\noperators and systematically evaluate them in a standardized LLM pre-training\nsetting. Our findings reveal that a depthwise stacking operator, called\n$G_{\\text{stack}}$, exhibits remarkable acceleration in training, leading to\ndecreased loss and improved overall performance on eight standard NLP\nbenchmarks compared to strong baselines. Motivated by these promising results,\nwe conduct extensive experiments to delve deeper into $G_{\\text{stack}}$ to\naddress $\\textit{O}$2 and $\\textit{O}$3. For $\\textit{O}$2 (untested\nscalability), our study shows that $G_{\\text{stack}}$ is scalable and\nconsistently performs well, with experiments up to 7B LLMs after growth and\npre-training LLMs with 750B tokens. For example, compared to a conventionally\ntrained 7B model using 300B tokens, our $G_{\\text{stack}}$ model converges to\nthe same loss with 194B tokens, resulting in a 54.6\\% speedup. We further\naddress $\\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines\nto determine growth timing and growth factor for $G_{\\text{stack}}$, making it\npractical in general LLM pre-training. We also provide in-depth discussions and\ncomprehensive ablation studies of $G_{\\text{stack}}$. Our code and pre-trained\nmodel are available at\n$\\href{https://llm-stacking.github.io/}{https://llm-stacking.github.io/}$.", "paper_summary_zh": "<paragraph>\u7531\u65bc\u898f\u6a21\u9f90\u5927\uff0cLLM \u5728\u9810\u8a13\u7df4\u6642\u9700\u8981\u5927\u91cf\u7684\u8a08\u7b97\u6210\u672c\u3002\n\u6a21\u578b\u589e\u9577\u6210\u70ba\u4e86\u4e00\u7a2e\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u8f03\u5c0f\u7684\u6a21\u578b\u4f86\u52a0\u901f\u8f03\u5927\u6a21\u578b\u7684\u8a13\u7df4\u3002\n\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u589e\u9577\u65b9\u6cd5\u5728\u9ad8\u6548 LLM \u9810\u8a13\u7df4\u4e2d\u7684\u53ef\u884c\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\n\u9019\u9805\u5de5\u4f5c\u78ba\u5b9a\u4e86\u4e09\u500b\u95dc\u9375\u7684\u969c\u7919\uff1a($\\textit{O}$1) \u7f3a\u4e4f\u5168\u9762\u7684\u8a55\u4f30\uff0c($\\textit{O}$2) \u672a\u6e2c\u8a66\u7684\u64f4\u5c55\u53ef\u884c\u6027\uff0c\u4ee5\u53ca ($ \\textit{O}$3) \u7f3a\u4e4f\u7d93\u9a57\u6e96\u5247\u3002\n\u70ba\u4e86\u89e3\u6c7a $\\textit{O}$1\uff0c\u6211\u5011\u5c07\u73fe\u6709\u7684\u65b9\u6cd5\u7e3d\u7d50\u70ba\u56db\u500b\u539f\u5b50\u589e\u9577\u904b\u7b97\u5b50\uff0c\u4e26\u5728\u6a19\u6e96\u5316\u7684 LLM \u9810\u8a13\u7df4\u8a2d\u7f6e\u4e2d\u7cfb\u7d71\u5730\u8a55\u4f30\u5b83\u5011\u3002\n\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u4e00\u7a2e\u7a31\u70ba $G_{\\text{stack}}$ \u7684\u6df1\u5ea6\u5806\u758a\u904b\u7b97\u5b50\u5728\u8a13\u7df4\u4e2d\u8868\u73fe\u51fa\u986f\u8457\u7684\u52a0\u901f\uff0c\u8207\u5f37\u5927\u7684\u57fa\u6e96\u76f8\u6bd4\uff0c\u5728\u516b\u500b\u6a19\u6e96 NLP \u57fa\u6e96\u4e0a\u5c0e\u81f4\u640d\u5931\u964d\u4f4e\u548c\u6574\u9ad4\u6027\u80fd\u63d0\u5347\u3002\n\u53d7\u9019\u4e9b\u6709\u5e0c\u671b\u7684\u7d50\u679c\u7684\u555f\u767c\uff0c\u6211\u5011\u9032\u884c\u4e86\u5927\u91cf\u7684\u5be6\u9a57\uff0c\u6df1\u5165\u63a2\u8a0e $G_{\\text{stack}}$ \u4ee5\u89e3\u6c7a $\\textit{O}$2 \u548c $\\textit{O}$3\u3002\n\u5c0d\u65bc $\\textit{O}$2\uff08\u672a\u6e2c\u8a66\u7684\u53ef\u64f4\u5c55\u6027\uff09\uff0c\u6211\u5011\u7684\u7814\u7a76\u8868\u660e $G_{\\text{stack}}$ \u662f\u53ef\u64f4\u5c55\u7684\uff0c\u4e26\u4e14\u59cb\u7d42\u8868\u73fe\u826f\u597d\uff0c\u5728\u589e\u9577\u548c\u9810\u8a13\u7df4\u5177\u6709 750B \u500b\u4ee4\u724c\u7684 LLM \u4e4b\u5f8c\uff0c\u5be6\u9a57\u9ad8\u9054 7B LLM\u3002\n\u4f8b\u5982\uff0c\u8207\u4f7f\u7528 300B \u500b\u4ee4\u724c\u7684\u50b3\u7d71\u8a13\u7df4\u7684 7B \u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684 $G_{\\text{stack}}$ \u6a21\u578b\u4f7f\u7528 194B \u500b\u4ee4\u724c\u6536\u6582\u5230\u76f8\u540c\u7684\u640d\u5931\uff0c\u5f9e\u800c\u5c07\u901f\u5ea6\u63d0\u9ad8\u4e86 54.6%\u3002\n\u6211\u5011\u9032\u4e00\u6b65\u901a\u904e\u5f62\u5f0f\u5316\u6e96\u5247\u4f86\u78ba\u5b9a $G_{\\text{stack}}$ \u7684\u589e\u9577\u6642\u6a5f\u548c\u589e\u9577\u56e0\u5b50\uff0c\u5f9e\u800c\u89e3\u6c7a $\\textit{O}$3\uff08\u7f3a\u4e4f\u7d93\u9a57\u6e96\u5247\uff09\uff0c\u4f7f\u5176\u5728\u4e00\u822c\u7684 LLM \u9810\u8a13\u7df4\u4e2d\u5177\u6709\u5be6\u7528\u6027\u3002\n\u6211\u5011\u9084\u63d0\u4f9b\u4e86 $G_{\\text{stack}}$ \u7684\u6df1\u5165\u8a0e\u8ad6\u548c\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u3002\n\u6211\u5011\u7684\u4ee3\u78bc\u548c\u9810\u8a13\u7df4\u6a21\u578b\u53ef\u5728\n$\\href{https://llm-stacking.github.io/}{https://llm-stacking.github.io/}$ \u7372\u5f97\u3002</paragraph>", "author": "Wenyu Du et.al.", "authors": "Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, Jie Fu", "id": "2405.15319v1", "paper_url": "http://arxiv.org/abs/2405.15319v1", "repo": "null"}}