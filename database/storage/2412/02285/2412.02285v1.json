{"2412.02285": {"publish_time": "2024-12-03", "title": "GQWformer: A Quantum-based Transformer for Graph Representation Learning", "paper_summary": "Graph Transformers (GTs) have demonstrated significant advantages in graph\nrepresentation learning through their global attention mechanisms. However, the\nself-attention mechanism in GTs tends to neglect the inductive biases inherent\nin graph structures, making it chanllenging to effectively capture essential\nstructural information. To address this issue, we propose a novel approach that\nintegrate graph inductive bias into self-attention mechanisms by leveraging\nquantum technology for structural encoding. In this paper, we introduce the\nGraph Quantum Walk Transformer (GQWformer), a groundbreaking GNN framework that\nutilizes quantum walks on attributed graphs to generate node quantum states.\nThese quantum states encapsulate rich structural attributes and serve as\ninductive biases for the transformer, thereby enabling the generation of more\nmeaningful attention scores. By subsequently incorporating a recurrent neural\nnetwork, our design amplifies the model's ability to focus on both local and\nglobal information. We conducted comprehensive experiments across five publicly\navailable datasets to evaluate the effectiveness of our model. These results\nclearly indicate that GQWformer outperforms existing state-of-the-art graph\nclassification algorithms. These findings highlight the significant potential\nof integrating quantum computing methodologies with traditional GNNs to advance\nthe field of graph representation learning, providing a promising direction for\nfuture research and applications.", "paper_summary_zh": "\u5716\u5f62Transformer (GT) \u5df2\u900f\u904e\u5176\u5168\u7403\u6ce8\u610f\u529b\u6a5f\u5236\u8b49\u660e\u4e86\u5728\u5716\u5f62\u8868\u793a\u5b78\u7fd2\u4e2d\u7684\u986f\u8457\u512a\u52e2\u3002\u7136\u800c\uff0cGT \u4e2d\u7684\u81ea\u6211\u6ce8\u610f\u529b\u6a5f\u5236\u50be\u5411\u65bc\u5ffd\u7565\u5716\u5f62\u7d50\u69cb\u4e2d\u56fa\u6709\u7684\u6b78\u7d0d\u504f\u5dee\uff0c\u9019\u4f7f\u5f97\u6709\u6548\u64f7\u53d6\u5fc5\u8981\u7684\u7d50\u69cb\u8cc7\u8a0a\u8b8a\u5f97\u5177\u6709\u6311\u6230\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u900f\u904e\u5229\u7528\u91cf\u5b50\u6280\u8853\u9032\u884c\u7d50\u69cb\u7de8\u78bc\uff0c\u5c07\u5716\u5f62\u6b78\u7d0d\u504f\u5dee\u6574\u5408\u5230\u81ea\u6211\u6ce8\u610f\u529b\u6a5f\u5236\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u5716\u5f62\u91cf\u5b50\u6f2b\u6b65Transformer (GQWformer)\uff0c\u9019\u662f\u4e00\u500b\u7a81\u7834\u6027\u7684 GNN \u6846\u67b6\uff0c\u5b83\u5229\u7528\u5e36\u5c6c\u6027\u5716\u5f62\u4e0a\u7684\u91cf\u5b50\u6f2b\u6b65\u4f86\u7522\u751f\u7bc0\u9ede\u91cf\u5b50\u614b\u3002\u9019\u4e9b\u91cf\u5b50\u614b\u5305\u542b\u8c50\u5bcc\u7684\u7d50\u69cb\u5c6c\u6027\uff0c\u4e26\u4f5c\u70baTransformer\u7684\u6b78\u7d0d\u504f\u5dee\uff0c\u5f9e\u800c\u80fd\u5920\u7522\u751f\u66f4\u6709\u610f\u7fa9\u7684\u6ce8\u610f\u529b\u5206\u6578\u3002\u900f\u904e\u5f8c\u7e8c\u6574\u5408\u905e\u8ff4\u795e\u7d93\u7db2\u8def\uff0c\u6211\u5011\u7684\u8a2d\u8a08\u64f4\u5927\u4e86\u6a21\u578b\u95dc\u6ce8\u5c40\u90e8\u548c\u5168\u5c40\u8cc7\u8a0a\u7684\u80fd\u529b\u3002\u6211\u5011\u5728\u4e94\u500b\u516c\u958b\u53ef\u7528\u7684\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u4e86\u5168\u9762\u7684\u5be6\u9a57\uff0c\u4ee5\u8a55\u4f30\u6211\u5011\u6a21\u578b\u7684\u6709\u6548\u6027\u3002\u9019\u4e9b\u7d50\u679c\u6e05\u695a\u5730\u8868\u660e\uff0cGQWformer \u512a\u65bc\u73fe\u6709\u7684\u6700\u5148\u9032\u5716\u5f62\u5206\u985e\u6f14\u7b97\u6cd5\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u5c07\u91cf\u5b50\u904b\u7b97\u65b9\u6cd5\u8207\u50b3\u7d71 GNN \u6574\u5408\u4ee5\u63a8\u52d5\u5716\u5f62\u8868\u793a\u5b78\u7fd2\u9818\u57df\u7684\u5de8\u5927\u6f5b\u529b\uff0c\u70ba\u672a\u4f86\u7684\u7814\u7a76\u548c\u61c9\u7528\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "author": "Lei Yu et.al.", "authors": "Lei Yu, Hongyang Chen, Jingsong Lv, Linyao Yang", "id": "2412.02285v1", "paper_url": "http://arxiv.org/abs/2412.02285v1", "repo": "null"}}