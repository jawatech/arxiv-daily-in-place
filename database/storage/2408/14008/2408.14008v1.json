{"2408.14008": {"publish_time": "2024-08-26", "title": "LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models", "paper_summary": "The explosive growth of videos on streaming media platforms has underscored\nthe urgent need for effective video quality assessment (VQA) algorithms to\nmonitor and perceptually optimize the quality of streaming videos. However, VQA\nremains an extremely challenging task due to the diverse video content and the\ncomplex spatial and temporal distortions, thus necessitating more advanced\nmethods to address these issues. Nowadays, large multimodal models (LMMs), such\nas GPT-4V, have exhibited strong capabilities for various visual understanding\ntasks, motivating us to leverage the powerful multimodal representation ability\nof LMMs to solve the VQA task. Therefore, we propose the first Large\nMulti-Modal Video Quality Assessment (LMM-VQA) model, which introduces a novel\nspatiotemporal visual modeling strategy for quality-aware feature extraction.\nSpecifically, we first reformulate the quality regression problem into a\nquestion and answering (Q&A) task and construct Q&A prompts for VQA instruction\ntuning. Then, we design a spatiotemporal vision encoder to extract spatial and\ntemporal features to represent the quality characteristics of videos, which are\nsubsequently mapped into the language space by the spatiotemporal projector for\nmodality alignment. Finally, the aligned visual tokens and the quality-inquired\ntext tokens are aggregated as inputs for the large language model (LLM) to\ngenerate the quality score and level. Extensive experiments demonstrate that\nLMM-VQA achieves state-of-the-art performance across five VQA benchmarks,\nexhibiting an average improvement of $5\\%$ in generalization ability over\nexisting methods. Furthermore, due to the advanced design of the spatiotemporal\nencoder and projector, LMM-VQA also performs exceptionally well on general\nvideo understanding tasks, further validating its effectiveness. Our code will\nbe released at https://github.com/Sueqk/LMM-VQA.", "paper_summary_zh": "\u4e32\u6d41\u5a92\u9ad4\u5e73\u53f0\u4e0a\u5f71\u7247\u7684\u7206\u70b8\u6027\u6210\u9577\uff0c\u7a81\u986f\u51fa\u5c0d\u6709\u6548\u5f71\u7247\u54c1\u8cea\u8a55\u4f30 (VQA) \u6f14\u7b97\u6cd5\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4ee5\u76e3\u63a7\u548c\u611f\u77e5\u6700\u4f73\u5316\u4e32\u6d41\u5f71\u7247\u7684\u54c1\u8cea\u3002\u7136\u800c\uff0c\u7531\u65bc\u5f71\u7247\u5167\u5bb9\u591a\u6a23\u4e14\u7a7a\u9593\u548c\u6642\u9593\u626d\u66f2\u8907\u96dc\uff0c\u56e0\u6b64 VQA \u4ecd\u7136\u662f\u4e00\u9805\u6975\u5177\u6311\u6230\u6027\u7684\u4efb\u52d9\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9032\u968e\u7684\u65b9\u6cd5\u4f86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\u3002\u73fe\u4eca\uff0c\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM)\uff0c\u4f8b\u5982 GPT-4V\uff0c\u5df2\u5c55\u73fe\u51fa\u5c0d\u5404\u7a2e\u8996\u89ba\u7406\u89e3\u4efb\u52d9\u7684\u5f37\u5927\u529f\u80fd\uff0c\u4fc3\u4f7f\u6211\u5011\u5229\u7528 LMM \u5f37\u5927\u7684\u591a\u6a21\u614b\u8868\u793a\u80fd\u529b\u4f86\u89e3\u6c7a VQA \u4efb\u52d9\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u7b2c\u4e00\u500b\u5927\u578b\u591a\u6a21\u614b\u5f71\u7247\u54c1\u8cea\u8a55\u4f30 (LMM-VQA) \u6a21\u578b\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u6642\u7a7a\u8996\u89ba\u5efa\u6a21\u7b56\u7565\uff0c\u7528\u65bc\u54c1\u8cea\u611f\u77e5\u7279\u5fb5\u8403\u53d6\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u5c07\u54c1\u8cea\u56de\u6b78\u554f\u984c\u91cd\u65b0\u8868\u8ff0\u70ba\u554f\u7b54 (Q&A) \u4efb\u52d9\uff0c\u4e26\u5efa\u69cb Q&A \u63d0\u793a\u4ee5\u9032\u884c VQA \u6307\u4ee4\u8abf\u6574\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8a2d\u8a08\u4e00\u500b\u6642\u7a7a\u8996\u89ba\u7de8\u78bc\u5668\u4f86\u8403\u53d6\u7a7a\u9593\u548c\u6642\u9593\u7279\u5fb5\uff0c\u4ee5\u8868\u793a\u5f71\u7247\u7684\u54c1\u8cea\u7279\u5fb5\uff0c\u9019\u4e9b\u7279\u5fb5\u96a8\u5f8c\u7531\u6642\u7a7a\u6295\u5f71\u5100\u6620\u5c04\u5230\u8a9e\u8a00\u7a7a\u9593\u4ee5\u9032\u884c\u6a21\u614b\u5c0d\u9f4a\u3002\u6700\u5f8c\uff0c\u5c07\u5c0d\u9f4a\u7684\u8996\u89ba\u7b26\u865f\u548c\u54c1\u8cea\u63a2\u8a0e\u7684\u6587\u5b57\u7b26\u865f\u5f59\u96c6\u70ba\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8f38\u5165\uff0c\u4ee5\u7522\u751f\u54c1\u8cea\u5206\u6578\u548c\u7b49\u7d1a\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cLMM-VQA \u5728\u4e94\u500b VQA \u57fa\u6e96\u6e2c\u8a66\u4e2d\u9054\u5230\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u5e73\u5747\u63d0\u5347\u4e86 5%\u3002\u6b64\u5916\uff0c\u7531\u65bc\u6642\u7a7a\u7de8\u78bc\u5668\u548c\u6295\u5f71\u5100\u7684\u5148\u9032\u8a2d\u8a08\uff0cLMM-VQA \u5728\u4e00\u822c\u7684\u5f71\u7247\u7406\u89e3\u4efb\u52d9\u4e0a\u4e5f\u8868\u73fe\u5f97\u975e\u5e38\u597d\uff0c\u9032\u4e00\u6b65\u9a57\u8b49\u4e86\u5176\u6548\u80fd\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5c07\u5728 https://github.com/Sueqk/LMM-VQA \u767c\u5e03\u3002", "author": "Qihang Ge et.al.", "authors": "Qihang Ge, Wei Sun, Yu Zhang, Yunhao Li, Zhongpeng Ji, Fengyu Sun, Shangling Jui, Xiongkuo Min, Guangtao Zhai", "id": "2408.14008v1", "paper_url": "http://arxiv.org/abs/2408.14008v1", "repo": "null"}}