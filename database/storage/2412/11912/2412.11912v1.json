{"2412.11912": {"publish_time": "2024-12-16", "title": "CharacterBench: Benchmarking Character Customization of Large Language Models", "paper_summary": "Character-based dialogue (aka role-playing) enables users to freely customize\ncharacters for interaction, which often relies on LLMs, raising the need to\nevaluate LLMs' character customization capability. However, existing benchmarks\nfail to ensure a robust evaluation as they often only involve a single\ncharacter category or evaluate limited dimensions. Moreover, the sparsity of\ncharacter features in responses makes feature-focused generative evaluation\nboth ineffective and inefficient. To address these issues, we propose\nCharacterBench, the largest bilingual generative benchmark, with 22,859\nhuman-annotated samples covering 3,956 characters from 25 detailed character\ncategories. We define 11 dimensions of 6 aspects, classified as sparse and\ndense dimensions based on whether character features evaluated by specific\ndimensions manifest in each response. We enable effective and efficient\nevaluation by crafting tailored queries for each dimension to induce\ncharacters' responses related to specific dimensions. Further, we develop\nCharacterJudge model for cost-effective and stable evaluations. Experiments\nshow its superiority over SOTA automatic judges (e.g., GPT-4) and our\nbenchmark's potential to optimize LLMs' character customization. Our repository\nis at https://github.com/thu-coai/CharacterBench.", "paper_summary_zh": "\u57fa\u65bc\u5b57\u5143\u7684\u5c0d\u8a71\uff08\u53c8\u7a31\u89d2\u8272\u626e\u6f14\uff09\u5141\u8a31\u4f7f\u7528\u8005\u81ea\u7531\u81ea\u8a02\u89d2\u8272\u4ee5\u9032\u884c\u4e92\u52d5\uff0c\u9019\u901a\u5e38\u4ef0\u8cf4 LLM\uff0c\u4e26\u63d0\u5347\u4e86\u8a55\u4f30 LLM \u89d2\u8272\u81ea\u8a02\u529f\u80fd\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u6e96\u4e26\u672a\u78ba\u4fdd\u7a69\u5065\u7684\u8a55\u4f30\uff0c\u56e0\u70ba\u5b83\u5011\u901a\u5e38\u53ea\u6d89\u53ca\u55ae\u4e00\u7684\u89d2\u8272\u985e\u5225\u6216\u8a55\u4f30\u6709\u9650\u7684\u7dad\u5ea6\u3002\u6b64\u5916\uff0c\u56de\u61c9\u4e2d\u89d2\u8272\u7279\u5fb5\u7684\u7a00\u758f\u6027\u4f7f\u5f97\u4ee5\u7279\u5fb5\u70ba\u4e2d\u5fc3\u7684\u751f\u6210\u5f0f\u8a55\u4f30\u65e2\u7121\u6548\u53c8\u4f4e\u6548\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 CharacterBench\uff0c\u9019\u662f\u6700\u5927\u7684\u96d9\u8a9e\u751f\u6210\u5f0f\u57fa\u6e96\uff0c\u5305\u542b 22,859 \u500b\u7531\u4eba\u985e\u8a3b\u89e3\u7684\u7bc4\u4f8b\uff0c\u6db5\u84cb\u4f86\u81ea 25 \u500b\u8a73\u7d30\u89d2\u8272\u985e\u5225\u7684 3,956 \u500b\u89d2\u8272\u3002\u6211\u5011\u5b9a\u7fa9\u4e86 6 \u500b\u9762\u5411\u7684 11 \u500b\u7dad\u5ea6\uff0c\u6839\u64da\u7531\u7279\u5b9a\u7dad\u5ea6\u8a55\u4f30\u7684\u89d2\u8272\u7279\u5fb5\u662f\u5426\u5728\u6bcf\u500b\u56de\u61c9\u4e2d\u986f\u73fe\uff0c\u5c07\u5176\u5206\u985e\u70ba\u7a00\u758f\u548c\u7a20\u5bc6\u7dad\u5ea6\u3002\u6211\u5011\u900f\u904e\u70ba\u6bcf\u500b\u7dad\u5ea6\u5236\u5b9a\u5ba2\u88fd\u5316\u67e5\u8a62\u4f86\u8a98\u5c0e\u8207\u7279\u5b9a\u7dad\u5ea6\u76f8\u95dc\u7684\u89d2\u8272\u56de\u61c9\uff0c\u9032\u800c\u5be6\u73fe\u6709\u6548\u4e14\u9ad8\u6548\u7684\u8a55\u4f30\u3002\u6b64\u5916\uff0c\u6211\u5011\u958b\u767c\u4e86 CharacterJudge \u6a21\u578b\u4ee5\u9032\u884c\u5177\u6210\u672c\u6548\u76ca\u4e14\u7a69\u5b9a\u7684\u8a55\u4f30\u3002\u5be6\u9a57\u986f\u793a\u5176\u512a\u65bc SOTA \u81ea\u52d5\u8a55\u5206\u5668\uff08\u4f8b\u5982 GPT-4\uff09\uff0c\u800c\u6211\u5011\u7684\u57fa\u6e96\u6709\u6f5b\u529b\u6700\u4f73\u5316 LLM \u7684\u89d2\u8272\u81ea\u8a02\u3002\u6211\u5011\u7684\u5132\u5b58\u5eab\u4f4d\u65bc https://github.com/thu-coai/CharacterBench\u3002", "author": "Jinfeng Zhou et.al.", "authors": "Jinfeng Zhou, Yongkang Huang, Bosi Wen, Guanqun Bi, Yuxuan Chen, Pei Ke, Zhuang Chen, Xiyao Xiao, Libiao Peng, Kuntian Tang, Rongsheng Zhang, Le Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang", "id": "2412.11912v1", "paper_url": "http://arxiv.org/abs/2412.11912v1", "repo": "https://github.com/thu-coai/characterbench"}}