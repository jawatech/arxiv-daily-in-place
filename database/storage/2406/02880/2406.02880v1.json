{"2406.02880": {"publish_time": "2024-06-05", "title": "Controllable Talking Face Generation by Implicit Facial Keypoints Editing", "paper_summary": "Audio-driven talking face generation has garnered significant interest within\nthe domain of digital human research. Existing methods are encumbered by\nintricate model architectures that are intricately dependent on each other,\ncomplicating the process of re-editing image or video inputs. In this work, we\npresent ControlTalk, a talking face generation method to control face\nexpression deformation based on driven audio, which can construct the head pose\nand facial expression including lip motion for both single image or sequential\nvideo inputs in a unified manner. By utilizing a pre-trained video synthesis\nrenderer and proposing the lightweight adaptation, ControlTalk achieves precise\nand naturalistic lip synchronization while enabling quantitative control over\nmouth opening shape. Our experiments show that our method is superior to\nstate-of-the-art performance on widely used benchmarks, including HDTF and\nMEAD. The parameterized adaptation demonstrates remarkable generalization\ncapabilities, effectively handling expression deformation across same-ID and\ncross-ID scenarios, and extending its utility to out-of-domain portraits,\nregardless of languages.", "paper_summary_zh": "\u53d7\u97f3\u8a0a\u9a45\u52d5\u7684\u8aaa\u8a71\u4eba\u81c9\u751f\u6210\u5728\u6578\u4f4d\u4eba\u50cf\u7814\u7a76\u9818\u57df\u4e2d\u5099\u53d7\u95dc\u6ce8\u3002\u73fe\u6709\u65b9\u6cd5\u53d7\u5230\u8907\u96dc\u7684\u6a21\u578b\u67b6\u69cb\u6240\u7d2f\uff0c\u9019\u4e9b\u67b6\u69cb\u932f\u7d9c\u8907\u96dc\u5730\u76f8\u4e92\u4f9d\u8cf4\uff0c\u4f7f\u5f97\u91cd\u65b0\u7de8\u8f2f\u5f71\u50cf\u6216\u5f71\u7247\u8f38\u5165\u7684\u904e\u7a0b\u8b8a\u5f97\u8907\u96dc\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa ControlTalk\uff0c\u4e00\u7a2e\u57fa\u65bc\u53d7\u9a45\u52d5\u97f3\u8a0a\u63a7\u5236\u81c9\u90e8\u8868\u60c5\u8b8a\u5f62\u7684\u8aaa\u8a71\u4eba\u81c9\u751f\u6210\u65b9\u6cd5\uff0c\u5b83\u80fd\u5920\u4ee5\u7d71\u4e00\u7684\u65b9\u5f0f\u5efa\u69cb\u982d\u90e8\u59ff\u52e2\u548c\u81c9\u90e8\u8868\u60c5\uff0c\u5305\u62ec\u55ae\u4e00\u5f71\u50cf\u6216\u5e8f\u5217\u5f71\u7247\u8f38\u5165\u7684\u5507\u90e8\u52d5\u4f5c\u3002\u900f\u904e\u4f7f\u7528\u9810\u5148\u8a13\u7df4\u597d\u7684\u5f71\u7247\u5408\u6210\u6e32\u67d3\u5668\u4e26\u63d0\u51fa\u8f15\u91cf\u5316\u9069\u61c9\uff0cControlTalk \u80fd\u5920\u5728\u7cbe\u78ba\u4e14\u81ea\u7136\u5730\u9032\u884c\u5507\u90e8\u540c\u6b65\u7684\u540c\u6642\uff0c\u9084\u80fd\u5c0d\u5634\u5df4\u5f35\u958b\u7684\u5f62\u72c0\u9032\u884c\u91cf\u5316\u63a7\u5236\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u6e96\u4e0a\u512a\u65bc\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u5305\u62ec HDTF \u548c MEAD\u3002\u53c3\u6578\u5316\u9069\u61c9\u5c55\u793a\u51fa\u975e\u51e1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u5920\u6709\u6548\u5730\u8655\u7406\u540c ID \u548c\u8de8 ID \u5834\u666f\u4e2d\u7684\u8868\u60c5\u8b8a\u5f62\uff0c\u4e26\u5c07\u5176\u6548\u7528\u64f4\u5c55\u5230\u9818\u57df\u5916\u7684\u8096\u50cf\uff0c\u800c\u8207\u8a9e\u8a00\u7121\u95dc\u3002", "author": "Dong Zhao et.al.", "authors": "Dong Zhao, Jiaying Shi, Wenjun Li, Shudong Wang, Shenghui Xu, Zhaoming Pan", "id": "2406.02880v1", "paper_url": "http://arxiv.org/abs/2406.02880v1", "repo": "null"}}