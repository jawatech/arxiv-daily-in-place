{"2407.12164": {"publish_time": "2024-07-16", "title": "Subject-driven Text-to-Image Generation via Preference-based Reinforcement Learning", "paper_summary": "Text-to-image generative models have recently attracted considerable\ninterest, enabling the synthesis of high-quality images from textual prompts.\nHowever, these models often lack the capability to generate specific subjects\nfrom given reference images or to synthesize novel renditions under varying\nconditions. Methods like DreamBooth and Subject-driven Text-to-Image (SuTI)\nhave made significant progress in this area. Yet, both approaches primarily\nfocus on enhancing similarity to reference images and require expensive setups,\noften overlooking the need for efficient training and avoiding overfitting to\nthe reference images. In this work, we present the $\\lambda$-Harmonic reward\nfunction, which provides a reliable reward signal and enables early stopping\nfor faster training and effective regularization. By combining the\nBradley-Terry preference model, the $\\lambda$-Harmonic reward function also\nprovides preference labels for subject-driven generation tasks. We propose\nReward Preference Optimization (RPO), which offers a simpler setup (requiring\nonly $3\\%$ of the negative samples used by DreamBooth) and fewer gradient steps\nfor fine-tuning. Unlike most existing methods, our approach does not require\ntraining a text encoder or optimizing text embeddings and achieves text-image\nalignment by fine-tuning only the U-Net component. Empirically,\n$\\lambda$-Harmonic proves to be a reliable approach for model selection in\nsubject-driven generation tasks. Based on preference labels and early stopping\nvalidation from the $\\lambda$-Harmonic reward function, our algorithm achieves\na state-of-the-art CLIP-I score of 0.833 and a CLIP-T score of 0.314 on\nDreamBench.", "paper_summary_zh": "<paragraph>\u6587\u5b57\u5230\u5f71\u50cf\u751f\u6210\u6a21\u578b\u6700\u8fd1\u5f15\u8d77\u4e86\u76f8\u7576\u5927\u7684\u8208\u8da3\uff0c\u5b83\u80fd\u5f9e\u6587\u5b57\u63d0\u793a\u4e2d\u5408\u6210\u9ad8\u54c1\u8cea\u7684\u5f71\u50cf\u3002\n\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u5f9e\u7d66\u5b9a\u7684\u53c3\u8003\u5f71\u50cf\u4e2d\u751f\u6210\u7279\u5b9a\u4e3b\u984c\u6216\u5728\u4e0d\u540c\u689d\u4ef6\u4e0b\u5408\u6210\u65b0\u7248\u672c\u7684\u7684\u80fd\u529b\u3002\n\u50cf DreamBooth \u548c\u4e3b\u984c\u9a45\u52d5\u6587\u5b57\u5230\u5f71\u50cf (SuTI) \u7b49\u65b9\u6cd5\u5728\u6b64\u9818\u57df\u53d6\u5f97\u4e86\u91cd\u5927\u9032\u5c55\u3002\n\u7136\u800c\uff0c\u9019\u5169\u7a2e\u65b9\u6cd5\u4e3b\u8981\u5c08\u6ce8\u65bc\u589e\u5f37\u8207\u53c3\u8003\u5f71\u50cf\u7684\u76f8\u4f3c\u6027\uff0c\u4e26\u4e14\u9700\u8981\u6602\u8cb4\u7684\u8a2d\u5b9a\uff0c\u5e38\u5e38\u5ffd\u7565\u4e86\u5c0d\u6709\u6548\u7387\u8a13\u7df4\u548c\u907f\u514d\u904e\u5ea6\u64ec\u5408\u53c3\u8003\u5f71\u50cf\u7684\u9700\u6c42\u3002\n\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 $\\lambda$-Harmonic \u734e\u52f5\u51fd\u6578\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u9760\u7684\u734e\u52f5\u8a0a\u865f\uff0c\u4e26\u80fd\u9032\u884c\u65e9\u671f\u505c\u6b62\u4ee5\u9032\u884c\u66f4\u5feb\u901f\u7684\u8a13\u7df4\u548c\u6709\u6548\u7684\u6b63\u5247\u5316\u3002\n\u900f\u904e\u7d50\u5408 Bradley-Terry \u504f\u597d\u6a21\u578b\uff0c$\\lambda$-Harmonic \u734e\u52f5\u51fd\u6578\u4e5f\u70ba\u4e3b\u984c\u9a45\u52d5\u751f\u6210\u4efb\u52d9\u63d0\u4f9b\u504f\u597d\u6a19\u7c64\u3002\n\u6211\u5011\u63d0\u51fa\u4e86\u734e\u52f5\u504f\u597d\u6700\u4f73\u5316 (RPO)\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u500b\u66f4\u7c21\u55ae\u7684\u8a2d\u5b9a\uff08\u53ea\u9700\u8981 DreamBooth \u4f7f\u7528\u7684\u8ca0\u9762\u6a23\u672c\u7684 3%\uff09\uff0c\u4e26\u4e14\u9700\u8981\u66f4\u5c11\u7684\u68af\u5ea6\u6b65\u9a5f\u9032\u884c\u5fae\u8abf\u3002\n\u8207\u73fe\u6709\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u4e0d\u9700\u8981\u8a13\u7df4\u6587\u5b57\u7de8\u78bc\u5668\u6216\u6700\u4f73\u5316\u6587\u5b57\u5d4c\u5165\uff0c\u4e26\u50c5\u900f\u904e\u5fae\u8abf U-Net \u7d44\u4ef6\u4f86\u9054\u6210\u6587\u5b57\u5f71\u50cf\u5c0d\u9f4a\u3002\n\u6839\u64da\u7d93\u9a57\uff0c$\\lambda$-Harmonic \u88ab\u8b49\u660e\u662f\u4e3b\u984c\u9a45\u52d5\u751f\u6210\u4efb\u52d9\u4e2d\u9032\u884c\u6a21\u578b\u9078\u64c7\u7684\u53ef\u9760\u65b9\u6cd5\u3002\n\u57fa\u65bc $\\lambda$-Harmonic \u734e\u52f5\u51fd\u6578\u7684\u504f\u597d\u6a19\u7c64\u548c\u65e9\u671f\u505c\u6b62\u9a57\u8b49\uff0c\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u5728 DreamBench \u4e0a\u9054\u5230\u4e86\u6700\u5148\u9032\u7684 CLIP-I \u5206\u6578 0.833 \u548c CLIP-T \u5206\u6578 0.314\u3002</paragraph>", "author": "Yanting Miao et.al.", "authors": "Yanting Miao, William Loh, Suraj Kothawade, Pascal Poupart, Abdullah Rashwan, Yeqing Li", "id": "2407.12164v1", "paper_url": "http://arxiv.org/abs/2407.12164v1", "repo": "null"}}