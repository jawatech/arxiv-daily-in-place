{"2407.11802": {"publish_time": "2024-07-16", "title": "Invariant Consistency for Knowledge Distillation", "paper_summary": "Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nconsistent with those of the teacher. Our approach combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation of the data. Our results on\nCIFAR-100 demonstrate that ICD outperforms traditional KD techniques and\nsurpasses 13 state-of-the-art methods. In some cases, the student model even\nexceeds the teacher model in terms of accuracy. Furthermore, we successfully\ntransfer our method to other datasets, including Tiny ImageNet and STL-10. The\ncode will be made public soon.", "paper_summary_zh": "\u77e5\u8b58\u84b8\u993e (KD) \u6d89\u53ca\u5c07\u77e5\u8b58\u5f9e\u4e00\u500b\u795e\u7d93\u7db2\u8def\u8f49\u79fb\u5230\u53e6\u4e00\u500b\u795e\u7d93\u7db2\u8def\uff0c\u901a\u5e38\u5f9e\u4e00\u500b\u8f03\u5927\u3001\u8a13\u7df4\u826f\u597d\u7684\u6a21\u578b\uff08\u6559\u5e2b\uff09\u8f49\u79fb\u5230\u4e00\u500b\u8f03\u5c0f\u3001\u66f4\u6709\u6548\u7387\u7684\u6a21\u578b\uff08\u5b78\u751f\uff09\u3002\u50b3\u7d71\u7684 KD \u65b9\u6cd5\u5c07\u6559\u5e2b\u548c\u5b78\u751f\u7db2\u8def\u7684\u6a5f\u7387\u8f38\u51fa\u4e4b\u9593\u7684 Kullback-Leibler (KL) \u5dee\u7570\u6700\u5c0f\u5316\u3002\u7136\u800c\uff0c\u9019\u7a2e\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u6559\u5e2b\u7db2\u8def\u4e2d\u5d4c\u5165\u7684\u95dc\u9375\u7d50\u69cb\u77e5\u8b58\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e0d\u8b8a\u4e00\u81f4\u6027\u84b8\u993e (ICD)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u900f\u904e\u78ba\u4fdd\u5b78\u751f\u6a21\u578b\u7684\u8868\u5fb5\u8207\u6559\u5e2b\u7684\u8868\u5fb5\u4e00\u81f4\u4f86\u589e\u5f37 KD\u3002\u6211\u5011\u7684\u505a\u6cd5\u7d50\u5408\u4e86\u5c0d\u6bd4\u5b78\u7fd2\u8207\u660e\u78ba\u7684\u4e0d\u8b8a\u7f70\u5247\uff0c\u5f9e\u6559\u5e2b\u5c0d\u8cc7\u6599\u7684\u8868\u5fb5\u4e2d\u64f7\u53d6\u66f4\u591a\u8cc7\u8a0a\u3002\u6211\u5011\u5728 CIFAR-100 \u4e0a\u7684\u7d50\u679c\u8b49\u660e\uff0cICD \u512a\u65bc\u50b3\u7d71\u7684 KD \u6280\u8853\uff0c\u4e26\u8d85\u8d8a\u4e86 13 \u7a2e\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002\u5728\u67d0\u4e9b\u60c5\u6cc1\u4e0b\uff0c\u5b78\u751f\u6a21\u578b\u751a\u81f3\u5728\u6e96\u78ba\u5ea6\u65b9\u9762\u8d85\u904e\u4e86\u6559\u5e2b\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u6210\u529f\u5730\u5c07\u6211\u5011\u7684\u6a21\u578b\u8f49\u79fb\u5230\u5176\u4ed6\u8cc7\u6599\u96c6\uff0c\u5305\u62ec Tiny ImageNet \u548c STL-10\u3002\u7a0b\u5f0f\u78bc\u5c07\u5f88\u5feb\u516c\u958b\u3002", "author": "Nikolaos Giakoumoglou et.al.", "authors": "Nikolaos Giakoumoglou, Tania Stathaki", "id": "2407.11802v1", "paper_url": "http://arxiv.org/abs/2407.11802v1", "repo": "null"}}