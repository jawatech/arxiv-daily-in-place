{"2409.18346": {"publish_time": "2024-09-26", "title": "MultiClimate: Multimodal Stance Detection on Climate Change Videos", "paper_summary": "Climate change (CC) has attracted increasing attention in NLP in recent\nyears. However, detecting the stance on CC in multimodal data is understudied\nand remains challenging due to a lack of reliable datasets. To improve the\nunderstanding of public opinions and communication strategies, this paper\npresents MultiClimate, the first open-source manually-annotated stance\ndetection dataset with $100$ CC-related YouTube videos and $4,209$\nframe-transcript pairs. We deploy state-of-the-art vision and language models,\nas well as multimodal models for MultiClimate stance detection. Results show\nthat text-only BERT significantly outperforms image-only ResNet50 and ViT.\nCombining both modalities achieves state-of-the-art, $0.747$/$0.749$ in\naccuracy/F1. Our 100M-sized fusion models also beat CLIP and BLIP, as well as\nthe much larger 9B-sized multimodal IDEFICS and text-only Llama3 and Gemma2,\nindicating that multimodal stance detection remains challenging for large\nlanguage models. Our code, dataset, as well as supplementary materials, are\navailable at https://github.com/werywjw/MultiClimate.", "paper_summary_zh": "<paragraph>\u6c23\u5019\u8b8a\u9077 (CC) \u8fd1\u5e74\u4f86\u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u9818\u57df\u4e2d\u5099\u53d7\u95dc\u6ce8\u3002\u7136\u800c\uff0c\u5075\u6e2c\u591a\u6a21\u614b\u8cc7\u6599\u4e2d\u7684\u6c23\u5019\u8b8a\u9077\u7acb\u5834\u4ecd\u8655\u65bc\u7814\u7a76\u4e0d\u8db3\u7684\u72c0\u614b\uff0c\u4e14\u56e0\u7f3a\u4e4f\u53ef\u9760\u7684\u8cc7\u6599\u96c6\u800c\u6301\u7e8c\u9762\u81e8\u6311\u6230\u3002\u70ba\u4e86\u589e\u9032\u5c0d\u516c\u773e\u610f\u898b\u548c\u6e9d\u901a\u7b56\u7565\u7684\u4e86\u89e3\uff0c\u672c\u6587\u63d0\u51fa MultiClimate\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u958b\u653e\u539f\u59cb\u78bc\u7684\u624b\u52d5\u6a19\u8a3b\u7acb\u5834\u5075\u6e2c\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b 100 \u90e8\u8207\u6c23\u5019\u8b8a\u9077\u76f8\u95dc\u7684 YouTube \u5f71\u7247\u548c 4,209 \u5c0d\u5f71\u683c\u8f49\u9304\u3002\u6211\u5011\u90e8\u7f72\u4e86\u6700\u5148\u9032\u7684\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b\uff0c\u4ee5\u53ca\u591a\u6a21\u614b\u6a21\u578b\uff0c\u7528\u65bc MultiClimate \u7acb\u5834\u5075\u6e2c\u3002\u7d50\u679c\u986f\u793a\uff0c\u50c5\u4f7f\u7528\u6587\u5b57\u7684 BERT \u660e\u986f\u512a\u65bc\u50c5\u4f7f\u7528\u5f71\u50cf\u7684 ResNet50 \u548c ViT\u3002\u7d50\u5408\u5169\u7a2e\u6a21\u614b\u53ef\u9054\u6210\u6700\u5148\u9032\u7684\u6e96\u78ba\u5ea6/F1\uff0c\u5206\u5225\u70ba 0.747/0.749\u3002\u6211\u5011\u898f\u6a21\u9054 100M \u7684\u878d\u5408\u6a21\u578b\u4e5f\u52dd\u904e CLIP \u548c BLIP\uff0c\u4ee5\u53ca\u898f\u6a21\u66f4\u5927\u7684 9B \u591a\u6a21\u614b IDEFICS \u548c\u50c5\u4f7f\u7528\u6587\u5b57\u7684 Llama3 \u548c Gemma2\uff0c\u9019\u8868\u793a\u591a\u6a21\u614b\u7acb\u5834\u5075\u6e2c\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4f86\u8aaa\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u96c6\u4ee5\u53ca\u88dc\u5145\u8cc7\u6599\u53ef\u5728 https://github.com/werywjw/MultiClimate \u53d6\u5f97\u3002</paragraph>", "author": "Jiawen Wang et.al.", "authors": "Jiawen Wang, Longfei Zuo, Siyao Peng, Barbara Plank", "id": "2409.18346v1", "paper_url": "http://arxiv.org/abs/2409.18346v1", "repo": "https://github.com/werywjw/multiclimate"}}