{"2502.11896": {"publish_time": "2025-02-17", "title": "CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning", "paper_summary": "Reinforcement learning (RL) in continuous action spaces encounters persistent\nchallenges, such as inefficient exploration and convergence to suboptimal\nsolutions. To address these limitations, we propose CAMEL, a novel framework\nintegrating LLM-generated suboptimal policies into the RL training pipeline.\nCAMEL leverages dynamic action masking and an adaptive epsilon-masking\nmechanism to guide exploration during early training stages while gradually\nenabling agents to optimize policies independently. At the core of CAMEL lies\nthe integration of Python-executable suboptimal policies generated by LLMs\nbased on environment descriptions and task objectives. Although simplistic and\nhard-coded, these policies offer valuable initial guidance for RL agents. To\neffectively utilize these priors, CAMEL employs masking-aware optimization to\ndynamically constrain the action space based on LLM outputs. Additionally,\nepsilon-masking gradually reduces reliance on LLM-generated guidance, enabling\nagents to transition from constrained exploration to autonomous policy\nrefinement. Experimental validation on Gymnasium MuJoCo environments\ndemonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated\npolicies significantly improve sample efficiency, achieving performance\ncomparable to or surpassing expert masking baselines. For Walker2d-v4, where\nLLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust\nRL performance without notable degradation, highlighting the framework's\nadaptability across diverse tasks. While CAMEL shows promise in enhancing\nsample efficiency and mitigating convergence challenges, these issues remain\nopen for further research. Future work aims to generalize CAMEL to multimodal\nLLMs for broader observation-action spaces and automate policy evaluation,\nreducing human intervention and enhancing scalability in RL training pipelines.", "paper_summary_zh": "<paragraph>\u5728\u9023\u7e8c\u52d5\u4f5c\u7a7a\u9593\u4e2d\u7684\u5f37\u5316\u5b78\u7fd2 (RL) \u6703\u9047\u5230\u6301\u7e8c\u7684\u6311\u6230\uff0c\u4f8b\u5982\u63a2\u7d22\u6548\u7387\u4f4e\u843d\u548c\u6536\u6582\u81f3\u6b21\u4f73\u89e3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa CAMEL\uff0c\u4e00\u500b\u5c07 LLM \u751f\u6210\u7684\u6b21\u4f73\u7b56\u7565\u6574\u5408\u5230 RL \u8a13\u7df4\u7ba1\u7dda\u4e2d\u7684\u65b0\u6846\u67b6\u3002CAMEL \u900f\u904e\u52d5\u614b\u52d5\u4f5c\u906e\u7f69\u548c\u81ea\u9069\u61c9 epsilon \u906e\u7f69\u6a5f\u5236\u4f86\u5f15\u5c0e\u63a2\u7d22\uff0c\u540c\u6642\u9010\u6f38\u8b93\u4ee3\u7406\u7a0b\u5f0f\u80fd\u5920\u7368\u7acb\u6700\u4f73\u5316\u7b56\u7565\u3002CAMEL \u7684\u6838\u5fc3\u5728\u65bc\u6574\u5408\u7531 LLM \u751f\u6210\u7684 Python \u53ef\u57f7\u884c\u6b21\u4f73\u7b56\u7565\uff0c\u9019\u4e9b\u7b56\u7565\u57fa\u65bc\u74b0\u5883\u63cf\u8ff0\u548c\u4efb\u52d9\u76ee\u6a19\u3002\u5118\u7ba1\u9019\u4e9b\u7b56\u7565\u904e\u65bc\u7c21\u5316\u4e14\u786c\u7de8\u78bc\uff0c\u4f46\u5b83\u5011\u70ba RL \u4ee3\u7406\u7a0b\u5f0f\u63d0\u4f9b\u4e86\u6709\u50f9\u503c\u7684\u521d\u59cb\u6307\u5c0e\u3002\u70ba\u4e86\u6709\u6548\u5229\u7528\u9019\u4e9b\u5148\u9a57\u77e5\u8b58\uff0cCAMEL \u63a1\u7528\u906e\u7f69\u611f\u77e5\u6700\u4f73\u5316\u4f86\u6839\u64da LLM \u8f38\u51fa\u52d5\u614b\u9650\u5236\u52d5\u4f5c\u7a7a\u9593\u3002\u6b64\u5916\uff0cepsilon \u906e\u7f69\u9010\u6f38\u6e1b\u5c11\u5c0d LLM \u751f\u6210\u7684\u6307\u5c0e\u4f9d\u8cf4\uff0c\u8b93\u4ee3\u7406\u7a0b\u5f0f\u80fd\u5920\u5f9e\u53d7\u9650\u63a2\u7d22\u8f49\u63db\u70ba\u81ea\u4e3b\u7b56\u7565\u6539\u5584\u3002\u5728 Gymnasium MuJoCo \u74b0\u5883\u4e0a\u7684\u5be6\u9a57\u9a57\u8b49\u8b49\u660e\u4e86 CAMEL \u7684\u6709\u6548\u6027\u3002\u5728 Hopper-v4 \u548c Ant-v4 \u4e2d\uff0cLLM \u751f\u6210\u7684\u7b56\u7565\u986f\u8457\u63d0\u5347\u4e86\u6a23\u672c\u6548\u7387\uff0c\u9054\u5230\u4e86\u8207\u5c08\u5bb6\u906e\u7f69\u57fa\u6e96\u76f8\u8fd1\u6216\u8d85\u8d8a\u7684\u6548\u80fd\u3002\u5c0d\u65bc LLM \u96e3\u4ee5\u6e96\u78ba\u5efa\u6a21\u96d9\u8db3\u6b65\u614b\u52d5\u614b\u7684 Walker2d-v4\uff0cCAMEL \u7dad\u6301\u7a69\u5065\u7684 RL \u6548\u80fd\uff0c\u4e14\u6c92\u6709\u986f\u8457\u964d\u4f4e\uff0c\u7a81\u986f\u4e86\u8a72\u6846\u67b6\u5728\u4e0d\u540c\u4efb\u52d9\u4e2d\u7684\u9069\u61c9\u6027\u3002\u5118\u7ba1 CAMEL \u5728\u63d0\u5347\u6a23\u672c\u6548\u7387\u548c\u7de9\u89e3\u6536\u6582\u6311\u6230\u65b9\u9762\u986f\u793a\u51fa\u524d\u666f\uff0c\u4f46\u9019\u4e9b\u554f\u984c\u4ecd\u6709\u5f85\u9032\u4e00\u6b65\u7814\u7a76\u3002\u672a\u4f86\u7684\u7814\u7a76\u5de5\u4f5c\u65e8\u5728\u5c07 CAMEL \u63a8\u5ee3\u5230\u591a\u6a21\u614b LLM\uff0c\u4ee5\u6db5\u84cb\u66f4\u5ee3\u6cdb\u7684\u89c0\u5bdf\u52d5\u4f5c\u7a7a\u9593\uff0c\u4e26\u81ea\u52d5\u5316\u7b56\u7565\u8a55\u4f30\uff0c\u6e1b\u5c11\u4eba\u5de5\u4ecb\u5165\u4e26\u63d0\u5347 RL \u8a13\u7df4\u7ba1\u7dda\u7684\u53ef\u64f4\u5145\u6027\u3002</paragraph>", "author": "Yanxiao Zhao et.al.", "authors": "Yanxiao Zhao, Yangge Qian, Jingyang Shan, Xiaolin Qin", "id": "2502.11896v1", "paper_url": "http://arxiv.org/abs/2502.11896v1", "repo": "null"}}