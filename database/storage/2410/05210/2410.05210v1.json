{"2410.05210": {"publish_time": "2024-10-07", "title": "Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality", "paper_summary": "In this paper, we propose a new method to enhance compositional understanding\nin pre-trained vision and language models (VLMs) without sacrificing\nperformance in zero-shot multi-modal tasks. Traditional fine-tuning approaches\noften improve compositional reasoning at the cost of degrading multi-modal\ncapabilities, primarily due to the use of global hard negative (HN) loss, which\ncontrasts global representations of images and texts. This global HN loss\npushes HN texts that are highly similar to the original ones, damaging the\nmodel's multi-modal representations. To overcome this limitation, we propose\nFine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard\nnegative loss and selective calibrated regularization. These innovations\nprovide fine-grained negative supervision while preserving the model's\nrepresentational integrity. Our extensive evaluations across diverse benchmarks\nfor both compositionality and multi-modal tasks show that FSC-CLIP not only\nachieves compositionality on par with state-of-the-art models but also retains\nstrong multi-modal capabilities. Code is available at:\nhttps://github.com/ytaek-oh/fsc-clip.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\u4f86\u589e\u5f37\u9810\u8a13\u7df4\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b (VLM) \u4e2d\u7684\u7d44\u5408\u7406\u89e3\uff0c\u540c\u6642\u4e0d\u72a7\u7272\u96f6\u6b21\u5b78\u7fd2\u591a\u6a21\u5f0f\u4efb\u52d9\u7684\u6027\u80fd\u3002\u50b3\u7d71\u7684\u5fae\u8abf\u65b9\u6cd5\u901a\u5e38\u6703\u4ee5\u964d\u4f4e\u591a\u6a21\u5f0f\u80fd\u529b\u70ba\u4ee3\u50f9\u4f86\u6539\u9032\u7d44\u5408\u63a8\u7406\uff0c\u9019\u4e3b\u8981\u662f\u7531\u65bc\u4f7f\u7528\u4e86\u5168\u5c40\u786c\u8ca0\u4f8b (HN) \u640d\u5931\uff0c\u9019\u8207\u5716\u50cf\u548c\u6587\u672c\u7684\u5168\u5c40\u8868\u793a\u5f62\u6210\u5c0d\u6bd4\u3002\u6b64\u5168\u5c40 HN \u640d\u5931\u6703\u63a8\u52d5\u8207\u539f\u59cb\u6587\u672c\u9ad8\u5ea6\u76f8\u4f3c\u7684 HN \u6587\u672c\uff0c\u640d\u5bb3\u6a21\u578b\u7684\u591a\u6a21\u5f0f\u8868\u793a\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7d30\u7c92\u5ea6\u9078\u64c7\u6027\u6821\u6e96 CLIP (FSC-CLIP)\uff0c\u5b83\u6574\u5408\u4e86\u5c40\u90e8\u786c\u8ca0\u4f8b\u640d\u5931\u548c\u9078\u64c7\u6027\u6821\u6e96\u6b63\u5247\u5316\u3002\u9019\u4e9b\u5275\u65b0\u63d0\u4f9b\u4e86\u7d30\u7c92\u5ea6\u7684\u8ca0\u9762\u76e3\u7763\uff0c\u540c\u6642\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u8868\u793a\u5b8c\u6574\u6027\u3002\u6211\u5011\u5c0d\u7d44\u5408\u6027\u548c\u591a\u6a21\u5f0f\u4efb\u52d9\u7684\u5404\u7a2e\u57fa\u6e96\u9032\u884c\u7684\u5ee3\u6cdb\u8a55\u4f30\u8868\u660e\uff0cFSC-CLIP \u4e0d\u50c5\u5be6\u73fe\u4e86\u8207\u6700\u5148\u9032\u6a21\u578b\u76f8\u7576\u7684\u7d44\u5408\u6027\uff0c\u800c\u4e14\u9084\u4fdd\u7559\u4e86\u5f37\u5927\u7684\u591a\u6a21\u5f0f\u80fd\u529b\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u53d6\u5f97\uff1a\nhttps://github.com/ytaek-oh/fsc-clip\u3002</paragraph>", "author": "Youngtaek Oh et.al.", "authors": "Youngtaek Oh, Jae Won Cho, Dong-Jin Kim, In So Kweon, Junmo Kim", "id": "2410.05210v1", "paper_url": "http://arxiv.org/abs/2410.05210v1", "repo": "https://github.com/ytaek-oh/fsc-clip"}}