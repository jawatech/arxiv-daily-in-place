{"2407.07341": {"publish_time": "2024-07-10", "title": "MixSumm: Topic-based Data Augmentation using LLMs for Low-resource Extractive Text Summarization", "paper_summary": "Low-resource extractive text summarization is a vital but heavily\nunderexplored area of research. Prior literature either focuses on abstractive\ntext summarization or prompts a large language model (LLM) like GPT-3 directly\nto generate summaries. In this work, we propose MixSumm for low-resource\nextractive text summarization. Specifically, MixSumm prompts an open-source\nLLM, LLaMA-3-70b, to generate documents that mix information from multiple\ntopics as opposed to generating documents without mixup, and then trains a\nsummarization model on the generated dataset. We use ROUGE scores and L-Eval, a\nreference-free LLaMA-3-based evaluation method to measure the quality of\ngenerated summaries. We conduct extensive experiments on a challenging text\nsummarization benchmark comprising the TweetSumm, WikiHow, and ArXiv/PubMed\ndatasets and show that our LLM-based data augmentation framework outperforms\nrecent prompt-based approaches for low-resource extractive summarization.\nAdditionally, our results also demonstrate effective knowledge distillation\nfrom LLaMA-3-70b to a small BERT-based extractive summarizer.", "paper_summary_zh": "\u4f4e\u8cc7\u6e90\u8403\u53d6\u5f0f\u6587\u5b57\u6458\u8981\u662f\u4e00\u500b\u91cd\u8981\u4f46\u56b4\u91cd\u672a\u88ab\u63a2\u7d22\u7684\u7814\u7a76\u9818\u57df\u3002\u5148\u524d\u7684\u6587\u737b\u8981\u4e48\u5c08\u6ce8\u65bc\u6458\u8981\u5f0f\u6587\u5b57\u6458\u8981\uff0c\u8981\u4e48\u76f4\u63a5\u63d0\u793a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5982 GPT-3 \u4f86\u7522\u751f\u6458\u8981\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa MixSumm \u7528\u65bc\u4f4e\u8cc7\u6e90\u8403\u53d6\u5f0f\u6587\u5b57\u6458\u8981\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cMixSumm \u63d0\u793a\u958b\u6e90 LLM\u3001LLaMA-3-70b\uff0c\u7522\u751f\u6df7\u5408\u4f86\u81ea\u591a\u500b\u4e3b\u984c\u7684\u8cc7\u8a0a\u7684\u6587\u4ef6\uff0c\u800c\u4e0d\u662f\u7522\u751f\u6c92\u6709\u6df7\u6dc6\u7684\u6587\u4ef6\uff0c\u7136\u5f8c\u5728\u7522\u751f\u7684\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u6458\u8981\u6a21\u578b\u3002\u6211\u5011\u4f7f\u7528 ROUGE \u5206\u6578\u548c L-Eval\uff0c\u4e00\u7a2e\u57fa\u65bc LLaMA-3 \u7684\u7121\u53c3\u8003\u8a55\u4f30\u65b9\u6cd5\u4f86\u8861\u91cf\u7522\u751f\u6458\u8981\u7684\u54c1\u8cea\u3002\u6211\u5011\u5728\u4e00\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u6587\u5b57\u6458\u8981\u57fa\u6e96\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u5305\u62ec TweetSumm\u3001WikiHow \u548c ArXiv/PubMed \u8cc7\u6599\u96c6\uff0c\u4e26\u5c55\u793a\u6211\u5011\u7684\u57fa\u65bc LLM \u7684\u8cc7\u6599\u64f4\u5145\u67b6\u69cb\u512a\u65bc\u6700\u8fd1\u7684\u57fa\u65bc\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u4f4e\u8cc7\u6e90\u8403\u53d6\u5f0f\u6458\u8981\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7d50\u679c\u9084\u5c55\u793a\u4e86\u5f9e LLaMA-3-70b \u5230\u5c0f\u578b\u57fa\u65bc BERT \u7684\u8403\u53d6\u5f0f\u6458\u8981\u5668\u7684\u6709\u6548\u77e5\u8b58\u63d0\u7149\u3002", "author": "Gaurav Sahu et.al.", "authors": "Gaurav Sahu, Issam H. Laradji", "id": "2407.07341v1", "paper_url": "http://arxiv.org/abs/2407.07341v1", "repo": "null"}}