{"2501.13687": {"publish_time": "2025-01-23", "title": "Question Answering on Patient Medical Records with Private Fine-Tuned LLMs", "paper_summary": "Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop", "paper_summary_zh": "\u91ab\u7642\u4fdd\u5065\u7cfb\u7d71\u6301\u7e8c\u7522\u751f\u5927\u91cf\u7684\u96fb\u5b50\u5065\u5eb7\u7d00\u9304 (EHR)\uff0c\u901a\u5e38\u5132\u5b58\u5728\u5feb\u901f\u91ab\u7642\u4e92\u901a\u6027\u8cc7\u6e90 (FHIR) \u6a19\u6e96\u4e2d\u3002\u5118\u7ba1\u9019\u4e9b\u7d00\u9304\u4e2d\u5305\u542b\u8c50\u5bcc\u7684\u8cc7\u8a0a\uff0c\u4f46\u5176\u8907\u96dc\u6027\u548c\u9f90\u5927\u6578\u91cf\u8b93\u4f7f\u7528\u8005\u96e3\u4ee5\u64f7\u53d6\u548c\u8a6e\u91cb\u91cd\u8981\u7684\u5065\u5eb7\u898b\u89e3\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u63d0\u4f9b\u4e86\u89e3\u6c7a\u65b9\u6848\uff0c\u80fd\u5c0d\u91ab\u7642\u8cc7\u6599\u9032\u884c\u8a9e\u7fa9\u554f\u7b54 (QA)\uff0c\u8b93\u4f7f\u7528\u8005\u80fd\u66f4\u6709\u6548\u5730\u8207\u5176\u5065\u5eb7\u7d00\u9304\u4e92\u52d5\u3002\u7136\u800c\uff0c\u78ba\u4fdd\u96b1\u79c1\u548c\u76f8\u5bb9\u6027\u9700\u8981 LLM \u7684\u908a\u7de3\u548c\u79c1\u4eba\u90e8\u7f72\u3002\u672c\u6587\u63d0\u51fa\u4e86\u8a9e\u7fa9\u554f\u7b54\u7684\u65b0\u65b9\u6cd5\uff0c\u5148\u627e\u51fa\u8207\u4f7f\u7528\u8005\u67e5\u8a62\u6700\u76f8\u95dc\u7684 FHIR \u8cc7\u6e90 (\u4efb\u52d9 1)\uff0c\u7136\u5f8c\u6839\u64da\u9019\u4e9b\u8cc7\u6e90\u56de\u7b54\u67e5\u8a62 (\u4efb\u52d9 2)\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u79c1\u4eba\u4e3b\u6a5f\u3001\u5fae\u8abf LLM \u7684\u6548\u80fd\uff0c\u4e26\u6839\u64da GPT-4 \u548c GPT-4o \u7b49\u57fa\u6e96\u6a21\u578b\u8a55\u4f30\u5b83\u5011\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u5fae\u8abf LLM \u7684\u5927\u5c0f\u96d6\u7136\u5c0f 250 \u500d\uff0c\u4f46\u5728\u4efb\u52d9 1 \u7684 F1 \u5206\u6578\u4e0a\u512a\u65bc GPT-4 \u7cfb\u5217\u6a21\u578b 0.55%\uff0c\u5728\u4efb\u52d9 2 \u7684 Meteor \u4efb\u52d9\u4e2d\u512a\u65bc 42%\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a2\u8a0e\u4e86 LLM \u4f7f\u7528\u7684\u9ad8\u968e\u9762\u5411\uff0c\u5305\u62ec\u5faa\u5e8f\u5fae\u8abf\u3001\u6a21\u578b\u81ea\u6211\u8a55\u4f30\uff08\u81ea\u6200\u5f0f\u8a55\u4f30\uff09\u548c\u8a13\u7df4\u8cc7\u6599\u5927\u5c0f\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\u3002\u6a21\u578b\u548c\u8cc7\u6599\u96c6\u5728\u6b64\u8655\u63d0\u4f9b\uff1ahttps://huggingface.co/genloop", "author": "Sara Kothari et.al.", "authors": "Sara Kothari, Ayush Gupta", "id": "2501.13687v1", "paper_url": "http://arxiv.org/abs/2501.13687v1", "repo": "null"}}