{"2411.16318": {"publish_time": "2024-11-25", "title": "One Diffusion to Generate Them All", "paper_summary": "We introduce OneDiffusion, a versatile, large-scale diffusion model that\nseamlessly supports bidirectional image synthesis and understanding across\ndiverse tasks. It enables conditional generation from inputs such as text,\ndepth, pose, layout, and semantic maps, while also handling tasks like image\ndeblurring, upscaling, and reverse processes such as depth estimation and\nsegmentation. Additionally, OneDiffusion allows for multi-view generation,\ncamera pose estimation, and instant personalization using sequential image\ninputs. Our model takes a straightforward yet effective approach by treating\nall tasks as frame sequences with varying noise scales during training,\nallowing any frame to act as a conditioning image at inference time. Our\nunified training framework removes the need for specialized architectures,\nsupports scalable multi-task training, and adapts smoothly to any resolution,\nenhancing both generalization and scalability. Experimental results demonstrate\ncompetitive performance across tasks in both generation and prediction such as\ntext-to-image, multiview generation, ID preservation, depth estimation and\ncamera pose estimation despite relatively small training dataset. Our code and\ncheckpoint are freely available at https://github.com/lehduong/OneDiffusion", "paper_summary_zh": "<paragraph>\u6211\u5011\u63a8\u51fa OneDiffusion\uff0c\u9019\u662f\u4e00\u500b\u901a\u7528\u7684\u3001\u5927\u898f\u6a21\u7684\u64f4\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u7121\u7e2b\u5730\u652f\u63f4\u96d9\u5411\u5f71\u50cf\u5408\u6210\u548c\u7406\u89e3\uff0c\u4e26\u4e14\u9069\u7528\u65bc\u5404\u7a2e\u4efb\u52d9\u3002\u5b83\u80fd\u5920\u5f9e\u6587\u5b57\u3001\u6df1\u5ea6\u3001\u59ff\u52e2\u3001\u7248\u9762\u548c\u8a9e\u610f\u5716\u7b49\u8f38\u5165\u9032\u884c\u689d\u4ef6\u5f0f\u751f\u6210\uff0c\u540c\u6642\u4e5f\u80fd\u8655\u7406\u5f71\u50cf\u53bb\u6a21\u7cca\u3001\u5347\u983b\u548c\u53cd\u5411\u8655\u7406\uff0c\u4f8b\u5982\u6df1\u5ea6\u4f30\u8a08\u548c\u5206\u5272\u3002\u6b64\u5916\uff0cOneDiffusion \u9084\u5141\u8a31\u591a\u8996\u5716\u751f\u6210\u3001\u76f8\u6a5f\u59ff\u52e2\u4f30\u8a08\u548c\u4f7f\u7528\u9806\u5e8f\u5f71\u50cf\u8f38\u5165\u9032\u884c\u5373\u6642\u500b\u4eba\u5316\u3002\u6211\u5011\u7684\u6a21\u578b\u63a1\u7528\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c07\u6240\u6709\u4efb\u52d9\u8996\u70ba\u8a13\u7df4\u671f\u9593\u5177\u6709\u4e0d\u540c\u96dc\u8a0a\u6bd4\u4f8b\u7684\u5e40\u5e8f\u5217\uff0c\u5141\u8a31\u4efb\u4f55\u5e40\u5728\u63a8\u8ad6\u6642\u9593\u4f5c\u70ba\u689d\u4ef6\u5f71\u50cf\u3002\u6211\u5011\u7d71\u4e00\u7684\u8a13\u7df4\u6846\u67b6\u6d88\u9664\u4e86\u5c0d\u5c08\u7528\u67b6\u69cb\u7684\u9700\u6c42\uff0c\u652f\u63f4\u53ef\u64f4\u5145\u7684\u591a\u4efb\u52d9\u8a13\u7df4\uff0c\u4e26\u80fd\u9806\u5229\u9069\u61c9\u4efb\u4f55\u89e3\u6790\u5ea6\uff0c\u540c\u6642\u63d0\u5347\u6cdb\u5316\u6027\u548c\u53ef\u64f4\u5145\u6027\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u5118\u7ba1\u8a13\u7df4\u8cc7\u6599\u96c6\u76f8\u5c0d\u8f03\u5c0f\uff0c\u4f46\u5b83\u5728\u751f\u6210\u548c\u9810\u6e2c\u4efb\u52d9\uff08\u4f8b\u5982\u6587\u5b57\u8f49\u5f71\u50cf\u3001\u591a\u8996\u5716\u751f\u6210\u3001ID \u4fdd\u7559\u3001\u6df1\u5ea6\u4f30\u8a08\u548c\u76f8\u6a5f\u59ff\u52e2\u4f30\u8a08\uff09\u4e2d\u90fd\u5c55\u73fe\u51fa\u5177\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u6aa2\u67e5\u9ede\u53ef\u65bc https://github.com/lehduong/OneDiffusion \u514d\u8cbb\u53d6\u5f97</paragraph>", "author": "Duong H. Le et.al.", "authors": "Duong H. Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, Jiasen Lu", "id": "2411.16318v1", "paper_url": "http://arxiv.org/abs/2411.16318v1", "repo": "https://github.com/lehduong/onediffusion"}}