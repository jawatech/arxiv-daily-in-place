{"2406.09279": {"publish_time": "2024-06-13", "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback", "paper_summary": "Learning from preference feedback has emerged as an essential step for\nimproving the generation quality and performance of modern language models\n(LMs). Despite its widespread use, the way preference-based learning is applied\nvaries wildly, with differing data, learning algorithms, and evaluations used,\nmaking disentangling the impact of each aspect difficult. In this work, we\nidentify four core aspects of preference-based learning: preference data,\nlearning algorithm, reward model, and policy training prompts, systematically\ninvestigate the impact of these components on downstream model performance, and\nsuggest a recipe for strong learning for preference feedback. Our findings\nindicate that all aspects are important for performance, with better preference\ndata leading to the largest improvements, followed by the choice of learning\nalgorithm, the use of improved reward models, and finally the use of additional\nunlabeled prompts for policy training. Notably, PPO outperforms DPO by up to\n2.5% in math and 1.2% in general domains. High-quality preference data leads to\nimprovements of up to 8% in instruction following and truthfulness. Despite\nsignificant gains of up to 5% in mathematical evaluation when scaling up reward\nmodels, we surprisingly observe marginal improvements in other categories.\n  We publicly release the code used for training\n(https://github.com/hamishivi/EasyLM) and evaluating\n(https://github.com/allenai/open-instruct) our models, along with the models\nand datasets themselves\n(https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618).", "paper_summary_zh": "<paragraph>\u5f9e\u504f\u597d\u56de\u994b\u4e2d\u5b78\u7fd2\u5df2\u6210\u70ba\u6539\u5584\u73fe\u4ee3\u8a9e\u8a00\u6a21\u578b (LM) \u7684\u751f\u6210\u54c1\u8cea\u548c\u6548\u80fd\u7684\u5fc5\u8981\u6b65\u9a5f\u3002\u5118\u7ba1\u504f\u597d\u5f0f\u5b78\u7fd2\u5ee3\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u61c9\u7528\u65b9\u5f0f\u537b\u5927\u76f8\u9015\u5ead\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u8cc7\u6599\u3001\u5b78\u7fd2\u6f14\u7b97\u6cd5\u548c\u8a55\u91cf\uff0c\u4f7f\u5f97\u96e3\u4ee5\u91d0\u6e05\u5404\u500b\u9762\u5411\u7684\u5f71\u97ff\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u627e\u51fa\u504f\u597d\u5f0f\u5b78\u7fd2\u7684\u56db\u500b\u6838\u5fc3\u9762\u5411\uff1a\u504f\u597d\u8cc7\u6599\u3001\u5b78\u7fd2\u6f14\u7b97\u6cd5\u3001\u734e\u52f5\u6a21\u578b\u548c\u653f\u7b56\u8a13\u7df4\u63d0\u793a\uff0c\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u9019\u4e9b\u7d44\u6210\u90e8\u5206\u5c0d\u4e0b\u6e38\u6a21\u578b\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u4e26\u5efa\u8b70\u504f\u597d\u56de\u994b\u7684\u5f37\u5f0f\u5b78\u7fd2\u914d\u65b9\u3002\u6211\u5011\u7684\u767c\u73fe\u6307\u51fa\uff0c\u5404\u500b\u9762\u5411\u5c0d\u65bc\u6548\u80fd\u90fd\u81f3\u95dc\u91cd\u8981\uff0c\u5176\u4e2d\u4ee5\u66f4\u597d\u7684\u504f\u597d\u8cc7\u6599\u5e36\u4f86\u6700\u5927\u7684\u6539\u5584\uff0c\u5176\u6b21\u662f\u5b78\u7fd2\u6f14\u7b97\u6cd5\u7684\u9078\u64c7\u3001\u6539\u826f\u734e\u52f5\u6a21\u578b\u7684\u4f7f\u7528\uff0c\u6700\u5f8c\u662f\u984d\u5916\u672a\u6a19\u8a18\u63d0\u793a\u5728\u653f\u7b56\u8a13\u7df4\u4e2d\u7684\u4f7f\u7528\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cPPO \u5728\u6578\u5b78\u4e0a\u512a\u65bc DPO \u9054 2.5%\uff0c\u5728\u4e00\u822c\u9818\u57df\u4e2d\u512a\u65bc DPO \u9054 1.2%\u3002\u9ad8\u54c1\u8cea\u7684\u504f\u597d\u8cc7\u6599\u5728\u9075\u5faa\u8aaa\u660e\u548c\u771f\u5be6\u6027\u65b9\u9762\u5e36\u4f86\u9ad8\u9054 8% \u7684\u6539\u5584\u3002\u5118\u7ba1\u5728\u64f4\u5145\u734e\u52f5\u6a21\u578b\u6642\uff0c\u6578\u5b78\u8a55\u91cf\u7372\u5f97\u9ad8\u9054 5% \u7684\u986f\u8457\u63d0\u5347\uff0c\u4f46\u6211\u5011\u9a5a\u8a1d\u5730\u89c0\u5bdf\u5230\u5176\u4ed6\u985e\u5225\u7684\u6539\u5584\u5e45\u5ea6\u5f88\u5c0f\u3002\u6211\u5011\u516c\u958b\u767c\u5e03\u7528\u65bc\u8a13\u7df4 (https://github.com/hamishivi/EasyLM) \u548c\u8a55\u91cf (https://github.com/allenai/open-instruct) \u6211\u5011\u7684\u6a21\u578b\u7684\u7a0b\u5f0f\u78bc\uff0c\u4ee5\u53ca\u6a21\u578b\u548c\u8cc7\u6599\u96c6\u672c\u8eab (https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618)\u3002</paragraph>", "author": "Hamish Ivison et.al.", "authors": "Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi", "id": "2406.09279v1", "paper_url": "http://arxiv.org/abs/2406.09279v1", "repo": "https://github.com/hamishivi/easylm"}}