{"2409.14846": {"publish_time": "2024-09-23", "title": "A-VL: Adaptive Attention for Large Vision-Language Models", "paper_summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.", "paper_summary_zh": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u6574\u5408\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u5e94\u7528\u6f5c\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9700\u8981\u5927\u91cf\u7684\u8d44\u6e90\u3002\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6280\u672f\u53ef\u4ee5\u52a8\u6001\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u3002\u867d\u7136\u5f53\u524d\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u57fa\u4e8e Transformer \u7684\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u9700\u6c42\uff0c\u4f46\u5b83\u4eec\u5e76\u4e0d\u9002\u5408 LVLM\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0cLVLM \u4ece\u8fdc\u7a0b\u56fe\u50cf\u6807\u8bb0\u548c\u5c40\u90e8\u6587\u672c\u6807\u8bb0\u751f\u6210\u54cd\u5e94\uff0c\u5e76\u4e14\u4e0d\u540c\u7684\u6a21\u6001\u5177\u6709\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002\u8fd9\u4e00\u89c2\u5bdf\u542f\u53d1\u4e86\u6211\u4eec\u5206\u522b\u7ba1\u7406\u6bcf\u4e2a\u6a21\u6001\u7684\u6ce8\u610f\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5bf9\u4e8e\u89c6\u89c9\u8f93\u5165\uff0c\u6211\u4eec\u5b58\u50a8\u6f5c\u5728\u6709\u7528\u4fe1\u606f\u7684\u7f13\u5b58\uff0c\u4f46\u53ea\u8ba1\u7b97\u6700\u5173\u952e\u7684\u90e8\u5206\u3002\u5bf9\u4e8e\u8bed\u8a00\u8f93\u5165\uff0c\u6211\u4eec\u66f4\u5173\u5fc3\u5c40\u90e8\u4fe1\u606f\u3002\u57fa\u4e8e\u6211\u4eec\u5bf9\u89c6\u89c9\u8bed\u8a00\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u89c2\u5bdf\u548c\u5206\u6790\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 A-VL\uff0c\u8fd9\u662f\u4e00\u79cd\u9488\u5bf9 LVLM \u63a8\u7406\u91cf\u8eab\u5b9a\u5236\u7684\u5373\u63d2\u5373\u7528\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u3002\u5728\u4e09\u4e2a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u548c\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\u4e86\u6211\u4eec\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u65b9\u6cd5 A-VL \u5728\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u8d1f\u8f7d\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "author": "Junyang Zhang et.al.", "authors": "Junyang Zhang, Mu Yuan, Ruiguang Zhong, Puhan Luo, Huiyou Zhan, Ningkang Zhang, Chengchen Hu, Xiangyang Li", "id": "2409.14846v1", "paper_url": "http://arxiv.org/abs/2409.14846v1", "repo": "null"}}