{"2407.02211": {"publish_time": "2024-07-02", "title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning", "paper_summary": "Large language models (LLMs) have played a fundamental role in various\nnatural language processing tasks with powerful prompt techniques. However, in\nreal-world applications, there are often similar prompt components for repeated\nqueries, which causes significant computational burdens during inference.\nExisting prompt compression and direct fine-tuning methods aim to tackle these\nchallenges, yet they frequently struggle to strike an optimal balance between\ncost-efficiency and performance effectiveness, especially in complex tasks such\nas NL2Code. In this paper, we propose a novel method namely PromptIntern to\ninternalize the prompt knowledge into model parameters via progressive\nfine-tuning. Our method enables LLMs to emulate the human learning process for\na new task, where detailed templates and examples in a prompt are gradually\ninternalized and phased out progressively as the model grows accustomed to the\ntask. Extensive experiments demonstrate that our method reduces inference\ntokens over 90%, speedups inference by 4.2 times, and saves 88.3% monetary\ncost.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u626e\u6f14\u8457\u57fa\u672c\u7684\u89d2\u8272\uff0c\u4e26\u4f7f\u7528\u5f37\u5927\u7684\u63d0\u793a\u6280\u8853\u3002\u7136\u800c\uff0c\u5728\u5be6\u969b\u61c9\u7528\u4e2d\uff0c\u91cd\u8907\u7684\u67e5\u8a62\u901a\u5e38\u6709\u985e\u4f3c\u7684\u63d0\u793a\u7d44\u6210\uff0c\u9019\u6703\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u9020\u6210\u5927\u91cf\u7684\u904b\u7b97\u8ca0\u64d4\u3002\u73fe\u6709\u7684\u63d0\u793a\u58d3\u7e2e\u548c\u76f4\u63a5\u5fae\u8abf\u65b9\u6cd5\u65e8\u5728\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\uff0c\u4f46\u5b83\u5011\u7d93\u5e38\u96e3\u4ee5\u5728\u6210\u672c\u6548\u76ca\u548c\u6548\u80fd\u4e4b\u9593\u53d6\u5f97\u6700\u4f73\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5728 NL2Code \u7b49\u8907\u96dc\u4efb\u52d9\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba PromptIntern\uff0c\u901a\u904e\u6f38\u9032\u5f0f\u5fae\u8abf\u5c07\u63d0\u793a\u77e5\u8b58\u5167\u5316\u5230\u6a21\u578b\u53c3\u6578\u4e2d\u3002\u6211\u5011\u7684\u6a21\u578b\u8b93 LLM \u80fd\u6a21\u64ec\u4eba\u985e\u5b78\u7fd2\u65b0\u4efb\u52d9\u7684\u904e\u7a0b\uff0c\u5176\u4e2d\u63d0\u793a\u4e2d\u7684\u8a73\u7d30\u7bc4\u672c\u548c\u7bc4\u4f8b\u6703\u9010\u6f38\u5167\u5316\uff0c\u4e26\u96a8\u8457\u6a21\u578b\u9010\u6f38\u9069\u61c9\u4efb\u52d9\u800c\u9010\u6b65\u6dd8\u6c70\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c07\u63a8\u7406\u4ee3\u5e63\u6e1b\u5c11\u4e86 90% \u4ee5\u4e0a\uff0c\u5c07\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e86 4.2 \u500d\uff0c\u4e26\u7bc0\u7701\u4e86 88.3% \u7684\u91d1\u9322\u6210\u672c\u3002", "author": "Jiaru Zou et.al.", "authors": "Jiaru Zou, Mengyu Zhou, Tao Li, Shi Han, Dongmei Zhang", "id": "2407.02211v1", "paper_url": "http://arxiv.org/abs/2407.02211v1", "repo": "null"}}