{"2408.12292": {"publish_time": "2024-08-22", "title": "Towards Deconfounded Image-Text Matching with Causal Inference", "paper_summary": "Prior image-text matching methods have shown remarkable performance on many\nbenchmark datasets, but most of them overlook the bias in the dataset, which\nexists in intra-modal and inter-modal, and tend to learn the spurious\ncorrelations that extremely degrade the generalization ability of the model.\nFurthermore, these methods often incorporate biased external knowledge from\nlarge-scale datasets as prior knowledge into image-text matching model, which\nis inevitable to force model further learn biased associations. To address\nabove limitations, this paper firstly utilizes Structural Causal Models (SCMs)\nto illustrate how intra- and inter-modal confounders damage the image-text\nmatching. Then, we employ backdoor adjustment to propose an innovative\nDeconfounded Causal Inference Network (DCIN) for image-text matching task. DCIN\n(1) decomposes the intra- and inter-modal confounders and incorporates them\ninto the encoding stage of visual and textual features, effectively eliminating\nthe spurious correlations during image-text matching, and (2) uses causal\ninference to mitigate biases of external knowledge. Consequently, the model can\nlearn causality instead of spurious correlations caused by dataset bias.\nExtensive experiments on two well-known benchmark datasets, i.e., Flickr30K and\nMSCOCO, demonstrate the superiority of our proposed method.", "paper_summary_zh": "\u5148\u524d\u7684\u56fe\u50cf\u6587\u672c\u5339\u914d\u65b9\u6cd5\u5728\u8bb8\u591a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u663e\u7740\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u4e2d\u7684\u5927\u591a\u6570\u90fd\u5ffd\u7565\u4e86\u6570\u636e\u96c6\u4e2d\u7684\u504f\u5dee\uff0c\u8be5\u504f\u5dee\u5b58\u5728\u4e8e\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\uff0c\u5e76\u4e14\u503e\u5411\u4e8e\u5b66\u4e60\u6781\u5927\u5730\u964d\u4f4e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u865a\u5047\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5c06\u6765\u81ea\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u504f\u5dee\u5916\u90e8\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u7eb3\u5165\u56fe\u50cf\u6587\u672c\u5339\u914d\u6a21\u578b\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u8feb\u4f7f\u6a21\u578b\u8fdb\u4e00\u6b65\u5b66\u4e60\u504f\u5dee\u5173\u8054\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u9650\u5236\uff0c\u672c\u6587\u9996\u5148\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b (SCM) \u6765\u8bf4\u660e\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u6df7\u6742\u56e0\u7d20\u5982\u4f55\u635f\u5bb3\u56fe\u50cf\u6587\u672c\u5339\u914d\u3002\u7136\u540e\uff0c\u6211\u4eec\u91c7\u7528\u540e\u95e8\u8c03\u6574\u4e3a\u56fe\u50cf\u6587\u672c\u5339\u914d\u4efb\u52a1\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u53bb\u6df7\u6742\u56e0\u679c\u63a8\u7406\u7f51\u7edc (DCIN)\u3002DCIN (1) \u5206\u89e3\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u6df7\u6742\u56e0\u7d20\uff0c\u5e76\u5c06\u5b83\u4eec\u7eb3\u5165\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u7684\u7f16\u7801\u9636\u6bb5\uff0c\u6709\u6548\u6d88\u9664\u56fe\u50cf\u6587\u672c\u5339\u914d\u671f\u95f4\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u4ee5\u53ca (2) \u4f7f\u7528\u56e0\u679c\u63a8\u7406\u6765\u51cf\u8f7b\u5916\u90e8\u77e5\u8bc6\u7684\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u56e0\u679c\u5173\u7cfb\uff0c\u800c\u4e0d\u662f\u7531\u6570\u636e\u96c6\u504f\u5dee\u5f15\u8d77\u7684\u865a\u5047\u76f8\u5173\u6027\u3002\u5728\u4e24\u4e2a\u8457\u540d\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5373 Flickr30K \u548c MSCOCO\uff09\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "author": "Wenhui Li et.al.", "authors": "Wenhui Li, Xinqi Su, Dan Song, Lanjun Wang, Kun Zhang, An-An Liu", "id": "2408.12292v1", "paper_url": "http://arxiv.org/abs/2408.12292v1", "repo": "null"}}