{"2411.07611": {"publish_time": "2024-11-12", "title": "Multimodal Clinical Reasoning through Knowledge-augmented Rationale Generation", "paper_summary": "Clinical rationales play a pivotal role in accurate disease diagnosis;\nhowever, many models predominantly use discriminative methods and overlook the\nimportance of generating supportive rationales. Rationale distillation is a\nprocess that transfers knowledge from large language models (LLMs) to smaller\nlanguage models (SLMs), thereby enhancing the latter's ability to break down\ncomplex tasks. Despite its benefits, rationale distillation alone is inadequate\nfor addressing domain knowledge limitations in tasks requiring specialized\nexpertise, such as disease diagnosis. Effectively embedding domain knowledge in\nSLMs poses a significant challenge. While current LLMs are primarily geared\ntoward processing textual data, multimodal LLMs that incorporate time series\ndata, especially electronic health records (EHRs), are still evolving. To\ntackle these limitations, we introduce ClinRaGen, an SLM optimized for\nmultimodal rationale generation in disease diagnosis. ClinRaGen incorporates a\nunique knowledge-augmented attention mechanism to merge domain knowledge with\ntime series EHR data, utilizing a stepwise rationale distillation strategy to\nproduce both textual and time series-based clinical rationales. Our evaluations\nshow that ClinRaGen markedly improves the SLM's capability to interpret\nmultimodal EHR data and generate accurate clinical rationales, supporting more\nreliable disease diagnosis, advancing LLM applications in healthcare, and\nnarrowing the performance divide between LLMs and SLMs.", "paper_summary_zh": "<paragraph>\u81e8\u5e8a\u4f9d\u64da\u5728\u6e96\u78ba\u7684\u75be\u75c5\u8a3a\u65b7\u4e2d\u626e\u6f14\u8457\u95dc\u9375\u89d2\u8272\uff1b\n\u7136\u800c\uff0c\u8a31\u591a\u6a21\u578b\u4e3b\u8981\u4f7f\u7528\u5224\u5225\u5f0f\u65b9\u6cd5\uff0c\u800c\u5ffd\u7565\u4e86\u751f\u6210\u652f\u6301\u6027\u4f9d\u64da\u7684\u91cd\u8981\u6027\u3002\u4f9d\u64da\u8403\u53d6\u662f\u4e00\u7a2e\u5c07\u77e5\u8b58\u5f9e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8f49\u79fb\u5230\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM) \u7684\u904e\u7a0b\uff0c\u5f9e\u800c\u589e\u5f37\u5f8c\u8005\u5206\u89e3\u8907\u96dc\u4efb\u52d9\u7684\u80fd\u529b\u3002\u5118\u7ba1\u6709\u5176\u597d\u8655\uff0c\u4f46\u55ae\u7368\u7684\u4f9d\u64da\u8403\u53d6\u4e0d\u8db3\u4ee5\u89e3\u6c7a\u9700\u8981\u5c08\u696d\u77e5\u8b58\u7684\u4efb\u52d9\uff08\u4f8b\u5982\u75be\u75c5\u8a3a\u65b7\uff09\u4e2d\u7684\u9818\u57df\u77e5\u8b58\u9650\u5236\u3002\u6709\u6548\u5730\u5c07\u9818\u57df\u77e5\u8b58\u5d4c\u5165 SLM \u662f\u4e00\u500b\u91cd\u5927\u7684\u6311\u6230\u3002\u96d6\u7136\u76ee\u524d\u7684 LLM \u4e3b\u8981\u7528\u65bc\u8655\u7406\u6587\u672c\u8cc7\u6599\uff0c\u4f46\u6574\u5408\u6642\u9593\u5e8f\u5217\u8cc7\u6599\uff08\u7279\u5225\u662f\u96fb\u5b50\u5065\u5eb7\u8a18\u9304 (EHR)\uff09\u7684\u591a\u6a21\u614b LLM \u4ecd\u5728\u767c\u5c55\u4e2d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 ClinRaGen\uff0c\u4e00\u7a2e\u91dd\u5c0d\u75be\u75c5\u8a3a\u65b7\u4e2d\u591a\u6a21\u614b\u4f9d\u64da\u751f\u6210\u7684\u6700\u4f73\u5316 SLM\u3002ClinRaGen \u7d50\u5408\u4e86\u4e00\u500b\u7368\u7279\u7684\u77e5\u8b58\u589e\u5f37\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u5c07\u9818\u57df\u77e5\u8b58\u8207\u6642\u9593\u5e8f\u5217 EHR \u8cc7\u6599\u5408\u4f75\uff0c\u5229\u7528\u9010\u6b65\u7684\u4f9d\u64da\u8403\u53d6\u7b56\u7565\u4f86\u7522\u751f\u57fa\u65bc\u6587\u672c\u548c\u6642\u9593\u5e8f\u5217\u7684\u81e8\u5e8a\u4f9d\u64da\u3002\u6211\u5011\u7684\u8a55\u4f30\u8868\u660e\uff0cClinRaGen \u660e\u986f\u6539\u5584\u4e86 SLM \u89e3\u91cb\u591a\u6a21\u614b EHR \u8cc7\u6599\u548c\u751f\u6210\u6e96\u78ba\u81e8\u5e8a\u4f9d\u64da\u7684\u80fd\u529b\uff0c\u652f\u6301\u66f4\u53ef\u9760\u7684\u75be\u75c5\u8a3a\u65b7\uff0c\u63a8\u9032 LLM \u5728\u91ab\u7642\u4fdd\u5065\u4e2d\u7684\u61c9\u7528\uff0c\u4e26\u7e2e\u5c0f LLM \u548c SLM \u4e4b\u9593\u7684\u6548\u80fd\u5dee\u8ddd\u3002</paragraph>", "author": "Shuai Niu et.al.", "authors": "Shuai Niu, Jing Ma, Liang Bai, Zhihua Wang, Yida Xu, Yunya Song, Xian Yang", "id": "2411.07611v1", "paper_url": "http://arxiv.org/abs/2411.07611v1", "repo": "null"}}