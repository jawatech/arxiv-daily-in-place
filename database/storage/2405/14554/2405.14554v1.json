{"2405.14554": {"publish_time": "2024-05-23", "title": "UDKAG: Augmenting Large Vision-Language Models with Up-to-Date Knowledge", "paper_summary": "Large vision-language models (LVLMs) are ignorant of the up-to-date\nknowledge, such as LLaVA series, because they cannot be updated frequently due\nto the large amount of resources required, and therefore fail in many cases.\nFor example, if a LVLM was released on January 2024, and it wouldn't know the\ndetailed plot of the new movie Dune 2, which wasn't released until February\n2024. To solve the problem, a promising solution is to provide LVLMs with\nup-to-date knowledge via internet search during inference, i.e.,\ninternet-augmented generation (IAG), which is already integrated in some\nclosed-source commercial LVLMs such as GPT-4V. However, the specific mechanics\nunderpinning them remain a mystery. In this paper, we propose a plug-and-play\nframework, for augmenting existing LVLMs in handling visual question answering\n(VQA) about up-to-date knowledge, dubbed UDKAG. A hierarchical filtering model\nis trained to effectively and efficiently find the most helpful content from\nthe websites returned by a search engine to prompt LVLMs with up-to-date\nknowledge. To train the model and evaluate our framework's performance, we\npropose a pipeline to automatically generate news-related VQA samples to\nconstruct a dataset, dubbed UDK-VQA. A multi-model voting mechanism is\nintroduced to label the usefulness of website/content for VQA samples to\nconstruct the training set. Experimental results demonstrate the effectiveness\nof our framework, outperforming GPT-4V by about 25% in accuracy.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5c0d\u65bc\u6700\u65b0\u77e5\u8b58\u4e00\u7121\u6240\u77e5\uff0c\u4f8b\u5982 LLaVA \u7cfb\u5217\uff0c\u56e0\u70ba\u5b83\u5011\u7121\u6cd5\u983b\u7e41\u66f4\u65b0\uff0c\u56e0\u70ba\u9700\u8981\u5927\u91cf\u7684\u8cc7\u6e90\uff0c\u56e0\u6b64\u5728\u8a31\u591a\u60c5\u6cc1\u4e0b\u6703\u5931\u6557\u3002\u4f8b\u5982\uff0c\u5982\u679c LVLM \u5728 2024 \u5e74 1 \u6708\u767c\u5e03\uff0c\u5b83\u5c07\u4e0d\u77e5\u9053\u76f4\u5230 2024 \u5e74 2 \u6708\u624d\u767c\u5e03\u7684\u65b0\u96fb\u5f71\u300a\u6c99\u4e18 2\u300b\u7684\u8a73\u7d30\u60c5\u7bc0\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u4e00\u500b\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u662f\u900f\u904e\u7db2\u969b\u7db2\u8def\u641c\u5c0b\u63d0\u4f9b LVLMs \u6700\u65b0\u77e5\u8b58\uff0c\u4e5f\u5c31\u662f\u7db2\u8def\u589e\u5f37\u751f\u6210 (IAG)\uff0c\u9019\u5df2\u7d93\u6574\u5408\u5728\u4e00\u4e9b\u9589\u6e90\u5546\u696d LVLMs \u4e2d\uff0c\u4f8b\u5982 GPT-4V\u3002\u7136\u800c\uff0c\u652f\u6490\u5b83\u5011\u7684\u5177\u9ad4\u6a5f\u5236\u4ecd\u7136\u662f\u500b\u8b0e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5373\u63d2\u5373\u7528\u7684\u67b6\u69cb\uff0c\u7528\u65bc\u589e\u5f37\u73fe\u6709\u7684 LVLMs\uff0c\u4ee5\u8655\u7406\u6709\u95dc\u6700\u65b0\u77e5\u8b58\u7684\u8996\u89ba\u554f\u984c\u89e3\u7b54 (VQA)\uff0c\u7a31\u70ba UDKAG\u3002\u8a13\u7df4\u4e00\u500b\u968e\u5c64\u5f0f\u904e\u6ffe\u6a21\u578b\uff0c\u5f9e\u641c\u5c0b\u5f15\u64ce\u8fd4\u56de\u7684\u7db2\u7ad9\u4e2d\u6709\u6548\u7387\u5730\u627e\u5230\u6700\u6709\u7528\u7684\u5167\u5bb9\uff0c\u4ee5\u63d0\u793a LVLMs \u5177\u6709\u6700\u65b0\u77e5\u8b58\u3002\u70ba\u4e86\u8a13\u7df4\u6a21\u578b\u4e26\u8a55\u4f30\u6211\u5011\u67b6\u69cb\u7684\u6548\u80fd\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7ba1\u9053\uff0c\u81ea\u52d5\u7522\u751f\u8207\u65b0\u805e\u76f8\u95dc\u7684 VQA \u7bc4\u4f8b\uff0c\u4ee5\u5efa\u69cb\u4e00\u500b\u540d\u70ba UDK-VQA \u7684\u8cc7\u6599\u96c6\u3002\u5f15\u5165\u591a\u6a21\u578b\u6295\u7968\u6a5f\u5236\uff0c\u6a19\u8a18\u7db2\u7ad9/\u5167\u5bb9\u5c0d VQA \u7bc4\u4f8b\u7684\u6709\u7528\u6027\uff0c\u4ee5\u5efa\u69cb\u8a13\u7df4\u96c6\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6211\u5011\u67b6\u69cb\u7684\u6709\u6548\u6027\uff0c\u6e96\u78ba\u7387\u6bd4 GPT-4V \u9ad8\u51fa\u7d04 25%\u3002", "author": "Chuanhao Li et.al.", "authors": "Chuanhao Li, Zhen Li, Chenchen Jing, Shuo Liu, Wenqi Shao, Yuwei Wu, Ping Luo, Yu Qiao, Kaipeng Zhang", "id": "2405.14554v1", "paper_url": "http://arxiv.org/abs/2405.14554v1", "repo": "null"}}