{"2412.02114": {"publish_time": "2024-12-03", "title": "OmniCreator: Self-Supervised Unified Generation with Universal Editing", "paper_summary": "We introduce OmniCreator, a novel framework that can conduct text-prompted\nunified (image+video) generation as well as editing all in one place.\nOmniCreator acquires generative and universal editing capabilities in a\nself-supervised manner, taking original text-video pairs as conditions while\nutilizing the same video as a denoising target to learn the semantic\ncorrespondence between video and text. During inference, when presented with a\ntext prompt and a video, OmniCreator is capable of generating a target that is\nfaithful to both, achieving a universal editing effect that is unconstrained as\nopposed to existing editing work that primarily focuses on certain editing\ntypes or relies on additional controls (e.g., structural conditions, attention\nfeatures, or DDIM inversion). On the other hand, when presented with a text\nprompt only, OmniCreator becomes generative, producing high-quality video as a\nresult of the semantic correspondence learned. Importantly, we found that the\nsame capabilities extend to images as is, making OmniCreator a truly unified\nframework. Further, due to the lack of existing generative video editing\nbenchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the\nperformance of generative video editing models comprehensively. Extensive\nexperiments demonstrate that OmniCreator exhibits substantial superiority over\nall other models.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4ecb\u7d39 OmniCreator\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u5728\u4e00\u500b\u5730\u65b9\u57f7\u884c\u6587\u5b57\u63d0\u793a\u7684\u7d71\u4e00\uff08\u5f71\u50cf + \u5f71\u7247\uff09\u751f\u6210\u4ee5\u53ca\u7de8\u8f2f\u3002OmniCreator \u4ee5\u81ea\u76e3\u7763\u7684\u65b9\u5f0f\u7372\u5f97\u751f\u6210\u548c\u901a\u7528\u7de8\u8f2f\u529f\u80fd\uff0c\u5c07\u539f\u59cb\u6587\u5b57\u5f71\u7247\u5c0d\u4f5c\u70ba\u689d\u4ef6\uff0c\u540c\u6642\u5229\u7528\u76f8\u540c\u7684\u5f71\u7247\u4f5c\u70ba\u53bb\u566a\u76ee\u6a19\uff0c\u4ee5\u5b78\u7fd2\u5f71\u7247\u548c\u6587\u5b57\u4e4b\u9593\u7684\u8a9e\u7fa9\u5c0d\u61c9\u3002\u5728\u63a8\u7406\u671f\u9593\uff0c\u7576\u63d0\u4f9b\u6587\u5b57\u63d0\u793a\u548c\u5f71\u7247\u6642\uff0cOmniCreator \u80fd\u5920\u751f\u6210\u4e00\u500b\u5fe0\u65bc\u5169\u8005\u7684\u76ee\u6a19\uff0c\u5be6\u73fe\u901a\u7528\u7de8\u8f2f\u6548\u679c\uff0c\u4e0d\u53d7\u73fe\u6709\u7de8\u8f2f\u5de5\u4f5c\u7684\u9650\u5236\uff0c\u800c\u73fe\u6709\u7de8\u8f2f\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u67d0\u4e9b\u7de8\u8f2f\u985e\u578b\u6216\u4f9d\u8cf4\u984d\u5916\u7684\u63a7\u5236\uff08\u4f8b\u5982\u7d50\u69cb\u689d\u4ef6\u3001\u6ce8\u610f\u7279\u5fb5\u6216 DDIM \u53cd\u6f14\uff09\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u7576\u50c5\u63d0\u4f9b\u6587\u5b57\u63d0\u793a\u6642\uff0cOmniCreator \u6703\u8b8a\u5f97\u5177\u6709\u751f\u6210\u6027\uff0c\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u5f71\u7247\uff0c\u9019\u662f\u56e0\u70ba\u5b78\u7fd2\u5230\u7684\u8a9e\u7fa9\u5c0d\u61c9\u3002\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\u76f8\u540c\u7684\u6a5f\u80fd\u4e5f\u5ef6\u4f38\u5230\u5f71\u50cf\uff0c\u4f7f OmniCreator \u6210\u70ba\u4e00\u500b\u771f\u6b63\u7684\u7d71\u4e00\u6846\u67b6\u3002\u6b64\u5916\uff0c\u7531\u65bc\u7f3a\u4e4f\u73fe\u6709\u7684\u751f\u6210\u5f71\u7247\u7de8\u8f2f\u57fa\u6e96\uff0c\u6211\u5011\u5f15\u5165\u4e86 OmniBench-99 \u8cc7\u6599\u96c6\uff0c\u65e8\u5728\u5168\u9762\u8a55\u4f30\u751f\u6210\u5f71\u7247\u7de8\u8f2f\u6a21\u578b\u7684\u6548\u80fd\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0cOmniCreator \u5728\u6240\u6709\u5176\u4ed6\u6a21\u578b\u4e2d\u8868\u73fe\u51fa\u986f\u8457\u7684\u512a\u8d8a\u6027\u3002</paragraph>", "author": "Haodong Chen et.al.", "authors": "Haodong Chen, Lan Wang, Harry Yang, Ser-Nam Lim", "id": "2412.02114v1", "paper_url": "http://arxiv.org/abs/2412.02114v1", "repo": "null"}}