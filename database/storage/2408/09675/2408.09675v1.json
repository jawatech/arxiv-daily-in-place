{"2408.09675": {"publish_time": "2024-08-19", "title": "Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey", "paper_summary": "Reinforcement Learning (RL) is a potent tool for sequential decision-making\nand has achieved performance surpassing human capabilities across many\nchallenging real-world tasks. As the extension of RL in the multi-agent system\ndomain, multi-agent RL (MARL) not only need to learn the control policy but\nalso requires consideration regarding interactions with all other agents in the\nenvironment, mutual influences among different system components, and the\ndistribution of computational resources. This augments the complexity of\nalgorithmic design and poses higher requirements on computational resources.\nSimultaneously, simulators are crucial to obtain realistic data, which is the\nfundamentals of RL. In this paper, we first propose a series of metrics of\nsimulators and summarize the features of existing benchmarks. Second, to ease\ncomprehension, we recall the foundational knowledge and then synthesize the\nrecently advanced studies of MARL-related autonomous driving and intelligent\ntransportation systems. Specifically, we examine their environmental modeling,\nstate representation, perception units, and algorithm design. Conclusively, we\ndiscuss open challenges as well as prospects and opportunities. We hope this\npaper can help the researchers integrate MARL technologies and trigger more\ninsightful ideas toward the intelligent and autonomous driving.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u662f\u7528\u65bc\u9806\u5e8f\u6c7a\u7b56\u5236\u5b9a\u7684\u4e00\u7a2e\u5f37\u5927\u5de5\u5177\uff0c\u4e26\u5df2\u5728\u8a31\u591a\u5177\u6709\u6311\u6230\u6027\u7684\u73fe\u5be6\u4e16\u754c\u4efb\u52d9\u4e2d\u5be6\u73fe\u4e86\u8d85\u8d8a\u4eba\u985e\u80fd\u529b\u7684\u6548\u80fd\u3002\u4f5c\u70ba RL \u5728\u591a\u4e3b\u9ad4\u7cfb\u7d71\u9818\u57df\u7684\u5ef6\u4f38\uff0c\u591a\u4e3b\u9ad4 RL (MARL) \u4e0d\u50c5\u9700\u8981\u5b78\u7fd2\u63a7\u5236\u653f\u7b56\uff0c\u9084\u9700\u8981\u8003\u616e\u8207\u74b0\u5883\u4e2d\u6240\u6709\u5176\u4ed6\u4e3b\u9ad4\u7684\u4e92\u52d5\u3001\u4e0d\u540c\u7cfb\u7d71\u7d44\u4ef6\u4e4b\u9593\u7684\u76f8\u4e92\u5f71\u97ff\u4ee5\u53ca\u8a08\u7b97\u8cc7\u6e90\u7684\u5206\u914d\u3002\u9019\u589e\u52a0\u4e86\u6f14\u7b97\u6cd5\u8a2d\u8a08\u7684\u8907\u96dc\u6027\uff0c\u4e26\u5c0d\u8a08\u7b97\u8cc7\u6e90\u63d0\u51fa\u4e86\u66f4\u9ad8\u7684\u8981\u6c42\u3002\u540c\u6642\uff0c\u6a21\u64ec\u5668\u5c0d\u65bc\u53d6\u5f97\u73fe\u5be6\u8cc7\u6599\u81f3\u95dc\u91cd\u8981\uff0c\u800c\u9019\u662f RL \u7684\u57fa\u790e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6a21\u64ec\u5668\u7684\u6307\u6a19\uff0c\u4e26\u7e3d\u7d50\u4e86\u73fe\u6709\u57fa\u6e96\u7684\u7279\u5fb5\u3002\u5176\u6b21\uff0c\u70ba\u4e86\u4fbf\u65bc\u7406\u89e3\uff0c\u6211\u5011\u56de\u9867\u4e86\u57fa\u790e\u77e5\u8b58\uff0c\u7136\u5f8c\u7d9c\u5408\u4e86\u6700\u8fd1\u95dc\u65bc MARL \u76f8\u95dc\u81ea\u52d5\u99d5\u99db\u548c\u667a\u6167\u904b\u8f38\u7cfb\u7d71\u7684\u5148\u9032\u7814\u7a76\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5b83\u5011\u7684\u74b0\u5883\u5efa\u6a21\u3001\u72c0\u614b\u8868\u793a\u3001\u611f\u77e5\u55ae\u5143\u548c\u6f14\u7b97\u6cd5\u8a2d\u8a08\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u958b\u653e\u6027\u6311\u6230\u4ee5\u53ca\u524d\u666f\u548c\u6a5f\u9047\u3002\u6211\u5011\u5e0c\u671b\u672c\u6587\u80fd\u5e6b\u52a9\u7814\u7a76\u4eba\u54e1\u6574\u5408 MARL \u6280\u8853\uff0c\u4e26\u6fc0\u767c\u66f4\u591a\u95dc\u65bc\u667a\u6167\u548c\u81ea\u52d5\u99d5\u99db\u7684\u6df1\u5165\u898b\u89e3\u3002", "author": "Ruiqi Zhang et.al.", "authors": "Ruiqi Zhang, Jing Hou, Florian Walter, Shangding Gu, Jiayi Guan, Florian R\u00f6hrbein, Yali Du, Panpan Cai, Guang Chen, Alois Knoll", "id": "2408.09675v1", "paper_url": "http://arxiv.org/abs/2408.09675v1", "repo": "https://github.com/huawei-noah/SMARTS"}}