{"2501.11284": {"publish_time": "2025-01-20", "title": "RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?", "paper_summary": "Can scaling transform reasoning? In this work, we explore the untapped\npotential of scaling Long Chain-of-Thought (Long-CoT) data to 1000k samples,\npioneering the development of a slow-thinking model, RedStar. Through extensive\nexperiments with various LLMs and different sizes, we uncover the ingredients\nfor specialization and scale for Long-CoT training. Surprisingly, even smaller\nmodels show significant performance gains with limited data, revealing the\nsample efficiency of Long-CoT and the critical role of sample difficulty in the\nlearning process. Our findings demonstrate that Long-CoT reasoning can be\neffectively triggered with just a few thousand examples, while larger models\nachieve unparalleled improvements. We also introduce reinforcement learning\n(RL)-scale training as a promising direction for advancing slow-thinking\nsystems. RedStar shines across domains: on the MATH-Hard benchmark,\nRedStar-code-math boosts performance from 66.2\\% to 81.6\\%, and on the USA Math\nOlympiad (AIME), it solves 46.7\\% of problems using only 21k mixed-code-math\ndatasets. In multimodal tasks like GeoQA and MathVista-GEO, RedStar-Geo\nachieves competitive results with minimal Long-CoT data, outperforming other\nslow-thinking systems like QvQ-Preview. Compared to QwQ, RedStar strikes the\nperfect balance between reasoning and generalizability. Our work highlights\nthat, with careful tuning, scaling Long-CoT can unlock extraordinary reasoning\ncapabilities-even with limited dataset and set a new standard for slow-thinking\nmodels across diverse challenges. Our data and models are released at\nhttps://huggingface.co/RedStar-Reasoning.", "paper_summary_zh": "<paragraph>\u7e2e\u653e\u53ef\u4ee5\u8f49\u63db\u63a8\u7406\u55ce\uff1f\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u7d22\u5c07\u9577\u93c8\u601d\u8003\uff08Long-CoT\uff09\u8cc7\u6599\u7e2e\u653e\u5230 1000k \u7bc4\u4f8b\u7684\u672a\u958b\u767c\u6f5b\u529b\uff0c\u7387\u5148\u958b\u767c\u6162\u601d\u8003\u6a21\u578b RedStar\u3002\u900f\u904e\u4f7f\u7528\u5404\u7a2e LLM \u548c\u4e0d\u540c\u5927\u5c0f\u9032\u884c\u5ee3\u6cdb\u5be6\u9a57\uff0c\u6211\u5011\u63ed\u793a\u4e86 Long-CoT \u8a13\u7df4\u7684\u5c08\u696d\u5316\u548c\u898f\u6a21\u8981\u7d20\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u5373\u4f7f\u8f03\u5c0f\u7684\u6a21\u578b\u5728\u8cc7\u6599\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\u4e5f\u5c55\u73fe\u51fa\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\uff0c\u63ed\u793a\u4e86 Long-CoT \u7684\u7bc4\u4f8b\u6548\u7387\u548c\u7bc4\u4f8b\u96e3\u5ea6\u5728\u5b78\u7fd2\u904e\u7a0b\u4e2d\u626e\u6f14\u7684\u95dc\u9375\u89d2\u8272\u3002\u6211\u5011\u7684\u767c\u73fe\u8b49\u660e\uff0c\u53ea\u8981\u6709\u6578\u5343\u500b\u7bc4\u4f8b\uff0c\u5c31\u53ef\u4ee5\u6709\u6548\u89f8\u767c Long-CoT \u63a8\u7406\uff0c\u800c\u8f03\u5927\u7684\u6a21\u578b\u5247\u53ef\u7372\u5f97\u7121\u8207\u502b\u6bd4\u7684\u6539\u9032\u3002\u6211\u5011\u9084\u5c0e\u5165\u5f37\u5316\u5b78\u7fd2 (RL) \u898f\u6a21\u8a13\u7df4\uff0c\u4f5c\u70ba\u63a8\u9032\u6162\u601d\u8003\u7cfb\u7d71\u7684\u4e00\u500b\u6709\u524d\u9014\u7684\u65b9\u5411\u3002RedStar \u5728\u5404\u500b\u9818\u57df\u4e2d\u8868\u73fe\u51fa\u8272\uff1a\u5728 MATH-Hard \u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0cRedStar-code-math \u5c07\u6548\u80fd\u5f9e 66.2% \u63d0\u5347\u81f3 81.6%\uff0c\u800c\u5728\u7f8e\u570b\u6578\u5b78\u5967\u6797\u5339\u514b\uff08AIME\uff09\u4e2d\uff0c\u5b83\u50c5\u4f7f\u7528 21k \u500b\u6df7\u5408\u7a0b\u5f0f\u78bc\u6578\u5b78\u8cc7\u6599\u96c6\u5c31\u89e3\u6c7a\u4e86 46.7% \u7684\u554f\u984c\u3002\u5728 GeoQA \u548c MathVista-GEO \u7b49\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\uff0cRedStar-Geo \u5728 Long-CoT \u8cc7\u6599\u6700\u5c11\u7684\u60c5\u6cc1\u4e0b\u53d6\u5f97\u7af6\u722d\u529b\u7684\u7d50\u679c\uff0c\u512a\u65bc\u5176\u4ed6\u6162\u601d\u8003\u7cfb\u7d71\uff0c\u4f8b\u5982 QvQ-Preview\u3002\u8207 QwQ \u76f8\u6bd4\uff0cRedStar \u5728\u63a8\u7406\u548c\u6982\u62ec\u6027\u4e4b\u9593\u53d6\u5f97\u4e86\u5b8c\u7f8e\u7684\u5e73\u8861\u3002\u6211\u5011\u7684\u7814\u7a76\u91cd\u9ede\u5728\u65bc\uff0c\u900f\u904e\u4ed4\u7d30\u8abf\u6574\uff0c\u7e2e\u653e Long-CoT \u53ef\u4ee5\u89e3\u9396\u975e\u51e1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u8cc7\u6599\u96c6\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\uff0c\u4e5f\u80fd\u70ba\u5404\u7a2e\u6311\u6230\u8a2d\u5b9a\u6162\u601d\u8003\u6a21\u578b\u7684\u65b0\u6a19\u6e96\u3002\u6211\u5011\u7684\u8cc7\u6599\u548c\u6a21\u578b\u5df2\u65bc https://huggingface.co/RedStar-Reasoning \u767c\u5e03\u3002</paragraph>", "author": "Haotian Xu et.al.", "authors": "Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, Debing Zhang", "id": "2501.11284v1", "paper_url": "http://arxiv.org/abs/2501.11284v1", "repo": "null"}}