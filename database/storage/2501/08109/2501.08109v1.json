{"2501.08109": {"publish_time": "2025-01-14", "title": "Data-driven inventory management for new products: A warm-start and adjusted Dyna-$Q$ approach", "paper_summary": "In this paper, we propose a novel reinforcement learning algorithm for\ninventory management of newly launched products with no or limited historical\ndemand information. The algorithm follows the classic Dyna-$Q$ structure,\nbalancing the model-based and model-free approaches, while accelerating the\ntraining process of Dyna-$Q$ and mitigating the model discrepancy generated by\nthe model-based feedback. Warm-start information from the demand data of\nexisting similar products can be incorporated into the algorithm to further\nstabilize the early-stage training and reduce the variance of the estimated\noptimal policy. Our approach is validated through a case study of bakery\ninventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\\%\nreduction in average daily cost compared with $Q$-learning, and up to a 77.5\\%\nreduction in training time within the same horizon compared with classic\nDyna-$Q$. By incorporating the warm-start information, it can be found that the\nadjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and\nrelatively low shortage percentages among all the algorithms under a 30-day\ntesting.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6f14\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u6ca1\u6709\u6216\u53ea\u6709\u6709\u9650\u7684\u5386\u53f2\u9700\u6c42\u8d44\u8baf\u7684\u60c5\u51b5\u4e0b\u7ba1\u7406\u65b0\u63a8\u51fa\u7684\u4ea7\u54c1\u7684\u5e93\u5b58\u3002\u6b64\u6f14\u7b97\u6cd5\u9075\u5faa\u7ecf\u5178\u7684 Dyna-$Q$ \u7ed3\u6784\uff0c\u5e73\u8861\u57fa\u4e8e\u6a21\u578b\u548c\u65e0\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u52a0\u901f Dyna-$Q$ \u7684\u8bad\u7ec3\u7a0b\u5e8f\uff0c\u5e76\u51cf\u8f7b\u57fa\u4e8e\u6a21\u578b\u7684\u56de\u9988\u4ea7\u751f\u7684\u6a21\u578b\u5dee\u5f02\u3002\u53ef\u4ee5\u5c06\u73b0\u6709\u7c7b\u4f3c\u4ea7\u54c1\u7684\u9700\u6c42\u8d44\u6599\u7684\u70ed\u542f\u52a8\u8d44\u8baf\u7eb3\u5165\u6f14\u7b97\u6cd5\uff0c\u4ee5\u8fdb\u4e00\u6b65\u7a33\u5b9a\u65e9\u671f\u9636\u6bb5\u7684\u8bad\u7ec3\uff0c\u5e76\u51cf\u5c11\u4f30\u8ba1\u6700\u4f73\u653f\u7b56\u7684\u5dee\u5f02\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5df2\u900f\u8fc7\u5b9e\u9645\u8d44\u6599\u7684\u9eb5\u5305\u5e97\u5e93\u5b58\u7ba1\u7406\u6848\u4f8b\u7814\u7a76\u5f97\u5230\u9a8c\u8bc1\u3002\u8c03\u6574\u540e\u7684 Dyna-$Q$ \u663e\u793a\u51fa\u4e0e $Q$-learning \u76f8\u6bd4\uff0c\u5e73\u5747\u6bcf\u65e5\u6210\u672c\u964d\u4f4e\u4e86 23.7%\uff0c\u4e0e\u7ecf\u5178 Dyna-$Q$ \u76f8\u6bd4\uff0c\u5728\u76f8\u540c\u8303\u56f4\u5185\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e86 77.5%\u3002\u900f\u8fc7\u7eb3\u5165\u70ed\u542f\u52a8\u8d44\u8baf\uff0c\u53ef\u4ee5\u53d1\u73b0\u8c03\u6574\u540e\u7684 Dyna-$Q$ \u5728\u6240\u6709\u6f14\u7b97\u6cd5\u4e2d\u5177\u6709\u6700\u4f4e\u7684\u603b\u6210\u672c\u3001\u6700\u4f4e\u7684\u603b\u6210\u672c\u5dee\u5f02\uff0c\u4ee5\u53ca\u5728 30 \u5929\u6d4b\u8bd5\u4e2d\u7684\u76f8\u5bf9\u8f83\u4f4e\u7684\u77ed\u7f3a\u767e\u5206\u6bd4\u3002</paragraph>", "author": "Xinyu Qu et.al.", "authors": "Xinyu Qu, Longxiao Liu, Wenjie Huang", "id": "2501.08109v1", "paper_url": "http://arxiv.org/abs/2501.08109v1", "repo": "null"}}