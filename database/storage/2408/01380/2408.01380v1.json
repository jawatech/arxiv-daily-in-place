{"2408.01380": {"publish_time": "2024-08-02", "title": "Coalitions of Large Language Models Increase the Robustness of AI Agents", "paper_summary": "The emergence of Large Language Models (LLMs) have fundamentally altered the\nway we interact with digital systems and have led to the pursuit of LLM powered\nAI agents to assist in daily workflows. LLMs, whilst powerful and capable of\ndemonstrating some emergent properties, are not logical reasoners and often\nstruggle to perform well at all sub-tasks carried out by an AI agent to plan\nand execute a workflow. While existing studies tackle this lack of proficiency\nby generalised pretraining at a huge scale or by specialised fine-tuning for\ntool use, we assess if a system comprising of a coalition of pretrained LLMs,\neach exhibiting specialised performance at individual sub-tasks, can match the\nperformance of single model agents. The coalition of models approach showcases\nits potential for building robustness and reducing the operational costs of\nthese AI agents by leveraging traits exhibited by specific models. Our findings\ndemonstrate that fine-tuning can be mitigated by considering a coalition of\npretrained models and believe that this approach can be applied to other\nnon-agentic systems which utilise LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\u5f9e\u6839\u672c\u4e0a\u6539\u8b8a\u4e86\u6211\u5011\u8207\u6578\u4f4d\u7cfb\u7d71\u4e92\u52d5\u7684\u65b9\u5f0f\uff0c\u4e26\u5c0e\u81f4\u8ffd\u6c42\u7531 LLM \u9a45\u52d5\u7684 AI \u4ee3\u7406\uff0c\u4ee5\u5354\u52a9\u65e5\u5e38\u5de5\u4f5c\u6d41\u7a0b\u3002LLM \u96d6\u7136\u529f\u80fd\u5f37\u5927\uff0c\u4e26\u4e14\u80fd\u5920\u5c55\u793a\u4e00\u4e9b\u65b0\u8208\u5c6c\u6027\uff0c\u4f46\u4e26\u975e\u908f\u8f2f\u63a8\u7406\u8005\uff0c\u800c\u4e14\u901a\u5e38\u96e3\u4ee5\u57f7\u884c AI \u4ee3\u7406\u5728\u898f\u5283\u548c\u57f7\u884c\u5de5\u4f5c\u6d41\u7a0b\u6642\u57f7\u884c\u7684\u6240\u6709\u5b50\u4efb\u52d9\u3002\u96d6\u7136\u73fe\u6709\u7814\u7a76\u900f\u904e\u5927\u898f\u6a21\u7684\u5ee3\u6cdb\u9810\u8a13\u7df4\u6216\u91dd\u5c0d\u5de5\u5177\u4f7f\u7528\u9032\u884c\u5c08\u9580\u5fae\u8abf\u4f86\u89e3\u6c7a\u9019\u7a2e\u7f3a\u4e4f\u719f\u7df4\u5ea6\uff0c\u4f46\u6211\u5011\u8a55\u4f30\u7531\u9810\u8a13\u7df4 LLM \u806f\u76df\u7d44\u6210\u7684\u7cfb\u7d71\uff0c\u6bcf\u500b\u7cfb\u7d71\u5728\u500b\u5225\u5b50\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u5c08\u9580\u7684\u6548\u80fd\uff0c\u662f\u5426\u80fd\u8207\u55ae\u4e00\u6a21\u578b\u4ee3\u7406\u7684\u6548\u80fd\u76f8\u5339\u914d\u3002\u6a21\u578b\u806f\u76df\u65b9\u6cd5\u5c55\u793a\u4e86\u5176\u900f\u904e\u5229\u7528\u7279\u5b9a\u6a21\u578b\u6240\u8868\u73fe\u51fa\u7684\u7279\u8cea\u4f86\u5efa\u69cb\u7a69\u5065\u6027\u548c\u964d\u4f4e\u9019\u4e9b AI \u4ee3\u7406\u7684\u71df\u904b\u6210\u672c\u7684\u6f5b\u529b\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u900f\u904e\u8003\u616e\u9810\u8a13\u7df4\u6a21\u578b\u806f\u76df\uff0c\u53ef\u4ee5\u6e1b\u8f15\u5fae\u8abf\uff0c\u4e26\u76f8\u4fe1\u9019\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u61c9\u7528\u65bc\u5176\u4ed6\u4f7f\u7528 LLM \u7684\u975e\u4ee3\u7406\u7cfb\u7d71\u3002", "author": "Prattyush Mangal et.al.", "authors": "Prattyush Mangal, Carol Mak, Theo Kanakis, Timothy Donovan, Dave Braines, Edward Pyzer-Knapp", "id": "2408.01380v1", "paper_url": "http://arxiv.org/abs/2408.01380v1", "repo": "null"}}