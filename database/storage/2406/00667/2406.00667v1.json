{"2406.00667": {"publish_time": "2024-06-02", "title": "An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging", "paper_summary": "Recent developments in multimodal large language models (MLLMs) have spurred\nsignificant interest in their potential applications across various medical\nimaging domains. On the one hand, there is a temptation to use these generative\nmodels to synthesize realistic-looking medical image data, while on the other\nhand, the ability to identify synthetic image data in a pool of data is also\nsignificantly important. In this study, we explore the potential of the Gemini\n(\\textit{gemini-1.0-pro-vision-latest}) and GPT-4V (gpt-4-vision-preview)\nmodels for medical image analysis using two modalities of medical image data.\nUtilizing synthetic and real imaging data, both Gemini AI and GPT-4V are first\nused to classify real versus synthetic images, followed by an interpretation\nand analysis of the input images. Experimental results demonstrate that both\nGemini and GPT-4 could perform some interpretation of the input images. In this\nspecific experiment, Gemini was able to perform slightly better than the GPT-4V\non the classification task. In contrast, responses associated with GPT-4V were\nmostly generic in nature. Our early investigation presented in this work\nprovides insights into the potential of MLLMs to assist with the classification\nand interpretation of retinal fundoscopy and lung X-ray images. We also\nidentify key limitations associated with the early investigation study on MLLMs\nfor specialized tasks in medical image analysis.", "paper_summary_zh": "\u6700\u8fd1\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u53d1\u5c55\u6fc0\u53d1\u4e86\u4eba\u4eec\u5bf9\u5176\u5728\u5404\u79cd\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u6f5c\u5728\u5e94\u7528\u7684\u6d53\u539a\u5174\u8da3\u3002\u4e00\u65b9\u9762\uff0c\u4eba\u4eec\u5f88\u60f3\u4f7f\u7528\u8fd9\u4e9b\u751f\u6210\u6a21\u578b\u6765\u5408\u6210\u903c\u771f\u7684\u533b\u5b66\u5f71\u50cf\u6570\u636e\uff0c\u800c\u53e6\u4e00\u65b9\u9762\uff0c\u5728\u6570\u636e\u6c60\u4e2d\u8bc6\u522b\u5408\u6210\u5f71\u50cf\u6570\u636e\u7684\u80fd\u529b\u4e5f\u6781\u5176\u91cd\u8981\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63a2\u8ba8\u4e86 Gemini (\\textit{gemini-1.0-pro-vision-latest}) \u548c GPT-4V (gpt-4-vision-preview) \u6a21\u578b\u5728\u4f7f\u7528\u4e24\u79cd\u533b\u5b66\u5f71\u50cf\u6570\u636e\u6a21\u6001\u8fdb\u884c\u533b\u5b66\u5f71\u50cf\u5206\u6790\u65b9\u9762\u7684\u6f5c\u529b\u3002\u5229\u7528\u5408\u6210\u548c\u771f\u5b9e\u6210\u50cf\u6570\u636e\uff0cGemini AI \u548c GPT-4V \u9996\u5148\u7528\u4e8e\u5bf9\u771f\u5b9e\u56fe\u50cf\u548c\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0c\u7136\u540e\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u89e3\u91ca\u548c\u5206\u6790\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGemini \u548c GPT-4 \u90fd\u53ef\u4ee5\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u4e00\u4e9b\u89e3\u91ca\u3002\u5728\u8fd9\u4e2a\u5177\u4f53\u5b9e\u9a8c\u4e2d\uff0cGemini \u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u7565\u597d\u4e8e GPT-4V\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e0e GPT-4V \u76f8\u5173\u7684\u54cd\u5e94\u5927\u591a\u662f\u901a\u7528\u7684\u3002\u6211\u4eec\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\u63d0\u51fa\u7684\u65e9\u671f\u8c03\u67e5\u63d0\u4f9b\u4e86\u5173\u4e8e MLLM \u534f\u52a9\u5206\u7c7b\u548c\u89e3\u91ca\u89c6\u7f51\u819c\u773c\u5e95\u955c\u548c\u80ba\u90e8 X \u5c04\u7ebf\u56fe\u50cf\u7684\u6f5c\u529b\u7684\u89c1\u89e3\u3002\u6211\u4eec\u8fd8\u786e\u5b9a\u4e86\u4e0e\u9488\u5bf9\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u4e13\u95e8\u4efb\u52a1\u7684 MLLM \u65e9\u671f\u8c03\u67e5\u7814\u7a76\u76f8\u5173\u7684\u5173\u952e\u9650\u5236\u3002", "author": "Sulaiman Khan et.al.", "authors": "Sulaiman Khan, Md. Rafiul Biswas, Alina Murad, Hazrat Ali, Zubair Shah", "id": "2406.00667v1", "paper_url": "http://arxiv.org/abs/2406.00667v1", "repo": "null"}}