{"2411.10950": {"publish_time": "2024-11-17", "title": "Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava in Visual Question Answering", "paper_summary": "Understanding the mechanisms behind Large Language Models (LLMs) is crucial\nfor designing improved models and strategies. While recent studies have yielded\nvaluable insights into the mechanisms of textual LLMs, the mechanisms of\nMulti-modal Large Language Models (MLLMs) remain underexplored. In this paper,\nwe apply mechanistic interpretability methods to analyze the visual question\nanswering (VQA) mechanisms in the first MLLM, Llava. We compare the mechanisms\nbetween VQA and textual QA (TQA) in color answering tasks and find that: a) VQA\nexhibits a mechanism similar to the in-context learning mechanism observed in\nTQA; b) the visual features exhibit significant interpretability when\nprojecting the visual embeddings into the embedding space; and c) Llava\nenhances the existing capabilities of the corresponding textual LLM Vicuna\nduring visual instruction tuning. Based on these findings, we develop an\ninterpretability tool to help users and researchers identify important visual\nlocations for final predictions, aiding in the understanding of visual\nhallucination. Our method demonstrates faster and more effective results\ncompared to existing interpretability approaches. Code:\n\\url{https://github.com/zepingyu0512/llava-mechanism}", "paper_summary_zh": "\u4e86\u89e3\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80cc\u5f8c\u7684\u6a5f\u5236\u5c0d\u65bc\u8a2d\u8a08\u6539\u9032\u7684\u6a21\u578b\u548c\u7b56\u7565\u81f3\u95dc\u91cd\u8981\u3002\u96d6\u7136\u6700\u8fd1\u7684\u7814\u7a76\u5c0d\u6587\u672c LLM \u7684\u6a5f\u5236\u7522\u751f\u4e86\u6709\u50f9\u503c\u7684\u898b\u89e3\uff0c\u4f46\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u7684\u6a5f\u5236\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u61c9\u7528\u6a5f\u5236\u53ef\u89e3\u91cb\u6027\u65b9\u6cd5\u4f86\u5206\u6790\u7b2c\u4e00\u500b MLLM Llava \u4e2d\u7684\u8996\u89ba\u554f\u7b54 (VQA) \u6a5f\u5236\u3002\u6211\u5011\u6bd4\u8f03\u4e86 VQA \u548c\u6587\u672c QA (TQA) \u5728\u984f\u8272\u56de\u7b54\u4efb\u52d9\u4e2d\u7684\u6a5f\u5236\uff0c\u767c\u73fe\uff1aa) VQA \u8868\u73fe\u51fa\u8207\u5728 TQA \u4e2d\u89c0\u5bdf\u5230\u7684\u60c5\u5883\u5b78\u7fd2\u6a5f\u5236\u985e\u4f3c\u7684\u6a5f\u5236\uff1bb) \u5728\u5c07\u8996\u89ba\u5d4c\u5165\u6295\u5f71\u5230\u5d4c\u5165\u7a7a\u9593\u6642\uff0c\u8996\u89ba\u7279\u5fb5\u8868\u73fe\u51fa\u986f\u8457\u7684\u53ef\u89e3\u91cb\u6027\uff1bc) Llava \u5728\u8996\u89ba\u6307\u4ee4\u8abf\u6574\u671f\u9593\u589e\u5f37\u4e86\u76f8\u61c9\u6587\u672c LLM Vicuna \u7684\u73fe\u6709\u80fd\u529b\u3002\u6839\u64da\u9019\u4e9b\u767c\u73fe\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u53ef\u89e3\u91cb\u6027\u5de5\u5177\uff0c\u4ee5\u5e6b\u52a9\u7528\u6236\u548c\u7814\u7a76\u4eba\u54e1\u8b58\u5225\u6700\u7d42\u9810\u6e2c\u7684\u91cd\u8981\u8996\u89ba\u4f4d\u7f6e\uff0c\u5f9e\u800c\u6709\u52a9\u65bc\u7406\u89e3\u8996\u89ba\u5e7b\u89ba\u3002\u8207\u73fe\u6709\u7684\u53ef\u89e3\u91cb\u6027\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u5c55\u793a\u4e86\u66f4\u5feb\u901f\u3001\u66f4\u6709\u6548\u7387\u7684\u7d50\u679c\u3002\u4ee3\u78bc\uff1a\\url{https://github.com/zepingyu0512/llava-mechanism}", "author": "Zeping Yu et.al.", "authors": "Zeping Yu, Sophia Ananiadou", "id": "2411.10950v1", "paper_url": "http://arxiv.org/abs/2411.10950v1", "repo": "https://github.com/zepingyu0512/llava-mechanism"}}