{"2410.11451": {"publish_time": "2024-10-15", "title": "Tending Towards Stability: Convergence Challenges in Small Language Models", "paper_summary": "Increasing the number of parameters in language models is a common strategy\nto enhance their performance. However, smaller language models remain valuable\ndue to their lower operational costs. Despite their advantages, smaller models\nfrequently underperform compared to their larger counterparts, even when\nprovided with equivalent data and computational resources. Specifically, their\nperformance tends to degrade in the late pretraining phase. This is anecdotally\nattributed to their reduced representational capacity. Yet, the exact causes of\nthis performance degradation remain unclear. We use the Pythia model suite to\nanalyse the training dynamics that underlie this phenomenon. Across different\nmodel sizes, we investigate the convergence of the Attention and MLP\nactivations to their final state and examine how the effective rank of their\nparameters influences this process. We find that nearly all layers in larger\nmodels stabilise early in training - within the first 20% - whereas layers in\nsmaller models exhibit slower and less stable convergence, especially when\ntheir parameters have lower effective rank. By linking the convergence of\nlayers' activations to their parameters' effective rank, our analyses can guide\nfuture work to address inefficiencies in the learning dynamics of small models.", "paper_summary_zh": "\u589e\u52a0\u8a9e\u8a00\u6a21\u578b\u4e2d\u7684\u53c3\u6578\u6578\u91cf\u662f\u4e00\u7a2e\u589e\u5f37\u5176\u6548\u80fd\u7684\u5e38\u898b\u7b56\u7565\u3002\u7136\u800c\uff0c\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b\u7531\u65bc\u5176\u8f03\u4f4e\u7684\u904b\u4f5c\u6210\u672c\u800c\u4ecd\u7136\u6709\u50f9\u503c\u3002\u5118\u7ba1\u6709\u5176\u512a\u9ede\uff0c\u4f46\u8207\u5176\u8f03\u5927\u7684\u5c0d\u61c9\u6a21\u578b\u76f8\u6bd4\uff0c\u5c0f\u578b\u6a21\u578b\u7d93\u5e38\u8868\u73fe\u4e0d\u4f73\uff0c\u5373\u4f7f\u5728\u63d0\u4f9b\u76f8\u7576\u7684\u8cc7\u6599\u548c\u8a08\u7b97\u8cc7\u6e90\u6642\u4e5f\u662f\u5982\u6b64\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u5b83\u5011\u7684\u6548\u80fd\u5f80\u5f80\u6703\u5728\u5f8c\u671f\u7684\u9810\u8a13\u7df4\u968e\u6bb5\u4e0b\u964d\u3002\u9019\u901a\u5e38\u6b78\u56e0\u65bc\u5b83\u5011\u964d\u4f4e\u7684\u8868\u793a\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u7a2e\u6548\u80fd\u4e0b\u964d\u7684\u78ba\u5207\u539f\u56e0\u4ecd\u7136\u4e0d\u6e05\u695a\u3002\u6211\u5011\u4f7f\u7528 Pythia \u6a21\u578b\u5957\u4ef6\u4f86\u5206\u6790\u9019\u7a2e\u73fe\u8c61\u80cc\u5f8c\u7684\u8a13\u7df4\u52d5\u614b\u3002\u5728\u4e0d\u540c\u7684\u6a21\u578b\u5927\u5c0f\u4e4b\u9593\uff0c\u6211\u5011\u63a2\u8a0e\u6ce8\u610f\u529b\u548c MLP \u6fc0\u6d3b\u5c0d\u5176\u6700\u7d42\u72c0\u614b\u7684\u6536\u6582\uff0c\u4e26\u6aa2\u67e5\u5176\u53c3\u6578\u7684\u6709\u6548\u79e9\u5982\u4f55\u5f71\u97ff\u6b64\u904e\u7a0b\u3002\u6211\u5011\u767c\u73fe\uff0c\u8f03\u5927\u6a21\u578b\u4e2d\u7684\u5e7e\u4e4e\u6240\u6709\u5c64\u90fd\u5728\u8a13\u7df4\u7684\u65e9\u671f\uff08\u5728\u524d 20% \u5167\uff09\u7a69\u5b9a\u4e0b\u4f86\uff0c\u800c\u8f03\u5c0f\u6a21\u578b\u4e2d\u7684\u5c64\u5247\u8868\u73fe\u51fa\u8f03\u6162\u4e14\u4e0d\u7a69\u5b9a\u7684\u6536\u6582\uff0c\u7279\u5225\u662f\u5728\u5176\u53c3\u6578\u5177\u6709\u8f03\u4f4e\u6709\u6548\u79e9\u6642\u3002\u900f\u904e\u5c07\u5c64\u7684\u6fc0\u6d3b\u6536\u6582\u8207\u5176\u53c3\u6578\u7684\u6709\u6548\u79e9\u9023\u7d50\u8d77\u4f86\uff0c\u6211\u5011\u7684\u5206\u6790\u53ef\u4ee5\u5f15\u5c0e\u672a\u4f86\u7684\u7814\u7a76\u4f86\u89e3\u6c7a\u5c0f\u578b\u6a21\u578b\u5b78\u7fd2\u52d5\u614b\u4e2d\u7684\u4f4e\u6548\u7387\u3002", "author": "Richard Diehl Martinez et.al.", "authors": "Richard Diehl Martinez, Pietro Lesci, Paula Buttery", "id": "2410.11451v1", "paper_url": "http://arxiv.org/abs/2410.11451v1", "repo": "null"}}