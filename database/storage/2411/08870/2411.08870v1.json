{"2411.08870": {"publish_time": "2024-11-13", "title": "The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models", "paper_summary": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare ten\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting and supervised fine-tuning regimes for medical question-answering\n(QA). For instance, across all tasks and model pairs we consider in the 3-shot\nsetting, medical LLMs only outperform their base models in 22.7% of cases,\nreach a (statistical) tie in 36.8% of cases, and are significantly worse than\ntheir base models in the remaining 40.5% of cases. Our conclusions are based on\n(i) comparing each medical model head-to-head, directly against the\ncorresponding base model; (ii) optimizing the prompts for each model separately\nin zero-/few-shot prompting; and (iii) accounting for statistical uncertainty\nin comparisons. While these basic practices are not consistently adopted in the\nliterature, our ablations show that they substantially impact conclusions.\nMeanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs\ncan show performance improvements, but the benefits do not carry over to tasks\nbased on clinical notes. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u6709\u8a31\u591a\u7814\u7a76\u5c08\u9580\u958b\u767c\u91ab\u7642\u61c9\u7528\u57fa\u790e\u6a21\u578b\uff0c\u900f\u904e\u6301\u7e8c\u9810\u8a13\u7df4\u516c\u958b\u7684\u751f\u7269\u91ab\u5b78\u8a9e\u6599\u5eab\uff0c\u6539\u7de8\u901a\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\u3002\u9019\u4e9b\u7814\u7a76\u901a\u5e38\u8072\u7a31\u6b64\u985e\u9818\u57df\u81ea\u9069\u61c9\u9810\u8a13\u7df4 (DAPT) \u80fd\u63d0\u5347\u4e0b\u6e38\u91ab\u7642\u4efb\u52d9\u7684\u6548\u80fd\uff0c\u4f8b\u5982\u56de\u7b54\u91ab\u7642\u57f7\u7167\u8003\u8a66\u984c\u76ee\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6bd4\u8f03\u4e86\u5341\u500b\u516c\u958b\u7684\u300c\u91ab\u7642\u300dLLM \u548c\u5169\u500b VLM\uff0c\u4e26\u5c07\u5176\u8207\u5c0d\u61c9\u7684\u57fa\u672c\u6a21\u578b\u9032\u884c\u6bd4\u8f03\uff0c\u5f97\u51fa\u4e86\u4e0d\u540c\u7684\u7d50\u8ad6\uff1a\u6240\u6709\u91ab\u7642 VLM \u548c\u5e7e\u4e4e\u6240\u6709\u91ab\u7642 LLM \u90fd\u7121\u6cd5\u5728\u91ab\u7642\u554f\u984c\u89e3\u7b54 (QA) \u7684\u96f6\u6b21/\u5c0f\u6a23\u672c\u63d0\u793a\u548c\u76e3\u7763\u5fae\u8abf\u6a5f\u5236\u4e2d\u6301\u7e8c\u512a\u65bc\u5176\u57fa\u672c\u6a21\u578b\u3002\u4f8b\u5982\uff0c\u5728\u6211\u5011\u5728 3 \u6b21\u53d6\u6a23\u8a2d\u5b9a\u4e2d\u8003\u91cf\u7684\u6240\u6709\u4efb\u52d9\u548c\u6a21\u578b\u914d\u5c0d\u4e2d\uff0c\u91ab\u7642 LLM \u50c5\u5728 22.7% \u7684\u6848\u4f8b\u4e2d\u512a\u65bc\u5176\u57fa\u672c\u6a21\u578b\uff0c\u5728 36.8% \u7684\u6848\u4f8b\u4e2d\u9054\u5230\uff08\u7d71\u8a08\uff09\u5e73\u624b\uff0c\u800c\u5728\u5176\u9918 40.5% \u7684\u6848\u4f8b\u4e2d\u5247\u986f\u8457\u4f4e\u65bc\u5176\u57fa\u672c\u6a21\u578b\u3002\u6211\u5011\u7684\u7d50\u8ad6\u57fa\u65bc (i) \u5c07\u6bcf\u500b\u91ab\u7642\u6a21\u578b\u8207\u5c0d\u61c9\u7684\u57fa\u672c\u6a21\u578b\u9032\u884c\u4e00\u5c0d\u4e00\u6bd4\u8f03\uff1b(ii) \u5728\u96f6\u6b21/\u5c0f\u6a23\u672c\u63d0\u793a\u4e2d\u5206\u5225\u91dd\u5c0d\u6bcf\u500b\u6a21\u578b\u6700\u4f73\u5316\u63d0\u793a\uff1b\u4ee5\u53ca (iii) \u5728\u6bd4\u8f03\u4e2d\u8003\u91cf\u7d71\u8a08\u4e0d\u78ba\u5b9a\u6027\u3002\u5118\u7ba1\u9019\u4e9b\u57fa\u672c\u505a\u6cd5\u4e26\u672a\u5728\u6587\u737b\u4e2d\u4e00\u81f4\u63a1\u7528\uff0c\u4f46\u6211\u5011\u7684\u6d88\u878d\u7814\u7a76\u986f\u793a\uff0c\u5b83\u5011\u5c0d\u7d50\u8ad6\u6709\u91cd\u5927\u5f71\u97ff\u3002\u540c\u6642\uff0c\u6211\u5011\u767c\u73fe\uff0c\u5728\u91dd\u5c0d\u7279\u5b9a QA \u4efb\u52d9\u9032\u884c\u5fae\u8abf\u5f8c\uff0c\u91ab\u7642 LLM \u53ef\u4ee5\u5c55\u73fe\u6548\u80fd\u63d0\u5347\uff0c\u4f46\u9019\u4e9b\u597d\u8655\u4e26\u672a\u5ef6\u7e8c\u5230\u57fa\u65bc\u81e8\u5e8a\u7b46\u8a18\u7684\u4efb\u52d9\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u6700\u5148\u9032\u7684\u901a\u7528\u9818\u57df\u6a21\u578b\u53ef\u80fd\u5df2\u7d93\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u91ab\u7642\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e26\u63d0\u4f9b\u5efa\u8b70\u4ee5\u5f37\u5316\u672a\u4f86\u7814\u7a76\u7684\u7d50\u8ad6\u3002</paragraph>", "author": "Daniel P. Jeong et.al.", "authors": "Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst", "id": "2411.08870v1", "paper_url": "http://arxiv.org/abs/2411.08870v1", "repo": "null"}}