{"2501.04173": {"publish_time": "2025-01-07", "title": "Multimodal Multihop Source Retrieval for Web Question Answering", "paper_summary": "This work deals with the challenge of learning and reasoning over multi-modal\nmulti-hop question answering (QA). We propose a graph reasoning network based\non the semantic structure of the sentences to learn multi-source reasoning\npaths and find the supporting facts across both image and text modalities for\nanswering the question. In this paper, we investigate the importance of graph\nstructure for multi-modal multi-hop question answering. Our analysis is\ncentered on WebQA. We construct a strong baseline model, that finds relevant\nsources using a pairwise classification task. We establish that, with the\nproper use of feature representations from pre-trained models, graph structure\nhelps in improving multi-modal multi-hop question answering. We point out that\nboth graph structure and adjacency matrix are task-related prior knowledge, and\ngraph structure can be leveraged to improve the retrieval performance for the\ntask. Experiments and visualized analysis demonstrate that message propagation\nover graph networks or the entire graph structure can replace massive\nmultimodal transformers with token-wise cross-attention. We demonstrated the\napplicability of our method and show a performance gain of \\textbf{4.6$\\%$}\nretrieval F1score over the transformer baselines, despite being a very light\nmodel. We further demonstrated the applicability of our model to a large scale\nretrieval setting.", "paper_summary_zh": "\u672c\u7814\u7a76\u61c9\u5c0d\u591a\u6a21\u614b\u591a\u8df3\u554f\u984c\u89e3\u7b54 (QA) \u7684\u5b78\u7fd2\u548c\u63a8\u7406\u6311\u6230\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u53e5\u5b50\u8a9e\u7fa9\u7d50\u69cb\u7684\u5716\u63a8\u7406\u7db2\u8def\uff0c\u7528\u65bc\u5b78\u7fd2\u591a\u4f86\u6e90\u63a8\u7406\u8def\u5f91\uff0c\u4e26\u5728\u5f71\u50cf\u548c\u6587\u5b57\u6a21\u614b\u4e2d\u627e\u5230\u652f\u6490\u4e8b\u5be6\uff0c\u4ee5\u56de\u7b54\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5716\u7d50\u69cb\u5c0d\u65bc\u591a\u6a21\u614b\u591a\u8df3\u554f\u984c\u89e3\u7b54\u7684\u91cd\u8981\u6027\u3002\u6211\u5011\u7684\u5206\u6790\u96c6\u4e2d\u5728 WebQA\u3002\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u5f37\u5927\u7684\u57fa\u7dda\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u6210\u5c0d\u5206\u985e\u4efb\u52d9\u4f86\u627e\u51fa\u76f8\u95dc\u4f86\u6e90\u3002\u6211\u5011\u78ba\u7acb\u4e86\uff0c\u900f\u904e\u9069\u7576\u4f7f\u7528\u9810\u8a13\u7df4\u6a21\u578b\u4e2d\u7684\u7279\u5fb5\u8868\u793a\uff0c\u5716\u7d50\u69cb\u6709\u52a9\u65bc\u6539\u5584\u591a\u6a21\u614b\u591a\u8df3\u554f\u984c\u89e3\u7b54\u3002\u6211\u5011\u6307\u51fa\uff0c\u5716\u7d50\u69cb\u548c\u9130\u63a5\u77e9\u9663\u90fd\u662f\u8207\u4efb\u52d9\u76f8\u95dc\u7684\u5148\u9a57\u77e5\u8b58\uff0c\u4e26\u4e14\u5716\u7d50\u69cb\u53ef\u7528\u65bc\u6539\u5584\u4efb\u52d9\u7684\u6aa2\u7d22\u6548\u80fd\u3002\u5be6\u9a57\u548c\u8996\u89ba\u5316\u5206\u6790\u8868\u660e\uff0c\u5716\u7db2\u8def\u6216\u6574\u500b\u5716\u7d50\u69cb\u4e0a\u7684\u8a0a\u606f\u50b3\u64ad\u53ef\u4ee5\u53d6\u4ee3\u5177\u6709\u4ee4\u724c\u660e\u667a\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u5de8\u5927\u591a\u6a21\u614bTransformer\u3002\u6211\u5011\u5c55\u793a\u4e86\u6211\u5011\u65b9\u6cd5\u7684\u9069\u7528\u6027\uff0c\u4e26\u4e14\u5c55\u793a\u4e86\u6bd4Transformer\u57fa\u7dda\u9ad8\u51fa \\textbf{4.6$\\%$} \u7684\u6aa2\u7d22 F1 \u5206\u6578\uff0c\u5118\u7ba1\u5b83\u662f\u4e00\u500b\u975e\u5e38\u8f15\u91cf\u7d1a\u7684\u6a21\u578b\u3002\u6211\u5011\u9032\u4e00\u6b65\u5c55\u793a\u4e86\u6211\u5011\u7684\u6a21\u578b\u5c0d\u5927\u898f\u6a21\u6aa2\u7d22\u8a2d\u5b9a\u7684\u9069\u7528\u6027\u3002", "author": "Navya Yarrabelly et.al.", "authors": "Navya Yarrabelly, Saloni Mittal", "id": "2501.04173v1", "paper_url": "http://arxiv.org/abs/2501.04173v1", "repo": "null"}}