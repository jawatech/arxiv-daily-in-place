{"2501.13831": {"publish_time": "2025-01-23", "title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing", "paper_summary": "Large Language Models (LLMs) excel at rewriting tasks such as text style\ntransfer and grammatical error correction. While there is considerable overlap\nbetween the inputs and outputs in these tasks, the decoding cost still\nincreases with output length, regardless of the amount of overlap. By\nleveraging the overlap between the input and the output, Kaneko and Okazaki\n(2023) proposed model-agnostic edit span representations to compress the\nrewrites to save computation. They reported an output length reduction rate of\nnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,\nwe propose alternative edit phrase representations inspired by phrase-based\nstatistical machine translation. We systematically compare our phrasal\nrepresentations with their span representations. We apply the LLM rewriting\nmodel to the task of Automatic Speech Recognition (ASR) post editing and show\nthat our target-phrase-only edit representation has the best\nefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes\n50-60% of the WER gap between the edit span model and the full rewrite model\nwhile losing only 10-20% of the length reduction rate of the edit span model.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91cd\u5beb\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4f8b\u5982\u6587\u5b57\u98a8\u683c\u8f49\u63db\u548c\u8a9e\u6cd5\u932f\u8aa4\u66f4\u6b63\u3002\u96d6\u7136\u5728\u9019\u4e9b\u4efb\u52d9\u4e2d\u8f38\u5165\u548c\u8f38\u51fa\u4e4b\u9593\u6709\u76f8\u7576\u5927\u7684\u91cd\u758a\uff0c\u4f46\u89e3\u78bc\u6210\u672c\u4ecd\u6703\u96a8\u8457\u8f38\u51fa\u9577\u5ea6\u589e\u52a0\uff0c\u800c\u8207\u91cd\u758a\u91cf\u7121\u95dc\u3002\u900f\u904e\u5229\u7528\u8f38\u5165\u548c\u8f38\u51fa\u4e4b\u9593\u7684\u91cd\u758a\uff0cKaneko \u548c Okazaki (2023) \u63d0\u51fa\u8207\u6a21\u578b\u7121\u95dc\u7684\u7de8\u8f2f\u7bc4\u570d\u8868\u793a\u6cd5\uff0c\u4ee5\u58d3\u7e2e\u91cd\u5beb\u5167\u5bb9\u4ee5\u7bc0\u7701\u904b\u7b97\u3002\u4ed6\u5011\u5831\u544a\u8aaa\uff0c\u5728\u56db\u9805\u91cd\u5beb\u4efb\u52d9\u4e2d\uff0c\u8f38\u51fa\u9577\u5ea6\u6e1b\u5c11\u7387\u63a5\u8fd1 80%\uff0c\u4e14\u5c0d\u6e96\u78ba\u5ea6\u7684\u5f71\u97ff\u5f88\u5c0f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u53d7\u57fa\u65bc\u77ed\u8a9e\u7684\u7d71\u8a08\u6a5f\u5668\u7ffb\u8b6f\u555f\u767c\u7684\u66ff\u4ee3\u7de8\u8f2f\u77ed\u8a9e\u8868\u793a\u6cd5\u3002\u6211\u5011\u7cfb\u7d71\u6027\u5730\u5c07\u6211\u5011\u7684\u77ed\u8a9e\u8868\u793a\u6cd5\u8207\u4ed6\u5011\u7684\u7bc4\u570d\u8868\u793a\u6cd5\u9032\u884c\u6bd4\u8f03\u3002\u6211\u5011\u5c07 LLM \u91cd\u5beb\u6a21\u578b\u61c9\u7528\u65bc\u81ea\u52d5\u8a9e\u97f3\u8b58\u5225 (ASR) \u5f8c\u7de8\u8f2f\u4efb\u52d9\uff0c\u4e26\u5c55\u793a\u6211\u5011\u7684\u50c5\u76ee\u6a19\u77ed\u8a9e\u7de8\u8f2f\u8868\u793a\u6cd5\u5177\u6709\u6700\u4f73\u7684\u6548\u7387\u6e96\u78ba\u6027\u6b0a\u8861\u3002\u5728 LibriSpeech \u6e2c\u8a66\u96c6\u4e2d\uff0c\u6211\u5011\u7684\u6a21\u578b\u7e2e\u5c0f\u4e86\u7de8\u8f2f\u7bc4\u570d\u6a21\u578b\u548c\u5b8c\u6574\u91cd\u5beb\u6a21\u578b\u4e4b\u9593 50-60% \u7684 WER \u5dee\u8ddd\uff0c\u540c\u6642\u50c5\u640d\u5931\u4e86\u7de8\u8f2f\u7bc4\u570d\u6a21\u578b 10-20% \u7684\u9577\u5ea6\u6e1b\u5c11\u7387\u3002", "author": "Hao Zhang et.al.", "authors": "Hao Zhang, Felix Stahlberg, Shankar Kumar", "id": "2501.13831v1", "paper_url": "http://arxiv.org/abs/2501.13831v1", "repo": "null"}}