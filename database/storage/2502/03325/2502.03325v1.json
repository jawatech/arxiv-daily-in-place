{"2502.03325": {"publish_time": "2025-02-05", "title": "ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model", "paper_summary": "Recent advancements in large language models (LLMs) have led to significant\nsuccesses across various applications, where the most noticeable is to a series\nof emerging capabilities, particularly in the areas of In-Context Learning\n(ICL) and Chain-of-Thought (CoT). To better understand and control model\nperformance, many studies have begun investigating the underlying causes of\nthese phenomena and their impact on task outcomes. However, existing\nexplanatory frameworks predominantly focus on isolating and explaining ICL and\nCoT independently, leading to an incomplete understanding of their combined\ninfluence on model performance. To address this gap, we propose the Electronic\nCircuit Model (ECM), which provides a foundation for developing scalable,\nlearnable policies and improving the management of AI-generated content.\nSpecifically, ECM conceptualizes model behavior as an electronic circuit: ICL\nis represented as semantic magnetic field to providing an additional voltage\nfollowing Faraday's Law, while CoT is modeled as series resistors to constrain\nthe model output performance following Ohm's Law. Experimental results\ndemonstrate that the ECM effectively predicts and explains LLM performance\nacross a variety of prompting strategies. Furthermore, we apply ECM to advanced\nreasoning strategy optimization on a series of tasks, such as the International\nOlympiad in Informatics (IOI) and the International Mathematical Olympiad\n(IMO), achieving competitive performance that surpasses nearly 80% of top human\ncompetitors.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u9032\u5c55\u5df2\u5728\u5404\u7a2e\u61c9\u7528\u4e2d\u53d6\u5f97\u986f\u8457\u6210\u529f\uff0c\u5176\u4e2d\u6700\u986f\u8457\u7684\u662f\u4e00\u7cfb\u5217\u65b0\u8208\u80fd\u529b\uff0c\u7279\u5225\u662f\u5728\u60c5\u5883\u5b78\u7fd2 (ICL) \u548c\u601d\u7dad\u93c8 (CoT) \u9818\u57df\u3002\u70ba\u4e86\u66f4\u597d\u5730\u7406\u89e3\u548c\u63a7\u5236\u6a21\u578b\u6548\u80fd\uff0c\u8a31\u591a\u7814\u7a76\u5df2\u958b\u59cb\u63a2\u8a0e\u9019\u4e9b\u73fe\u8c61\u7684\u6839\u672c\u539f\u56e0\u53ca\u5176\u5c0d\u4efb\u52d9\u7d50\u679c\u7684\u5f71\u97ff\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u89e3\u91cb\u6027\u6846\u67b6\u4e3b\u8981\u8457\u91cd\u65bc\u7368\u7acb\u9694\u96e2\u548c\u89e3\u91cb ICL \u548c CoT\uff0c\u5c0e\u81f4\u5c0d\u5b83\u5011\u5c0d\u6a21\u578b\u6548\u80fd\u7684\u7d9c\u5408\u5f71\u97ff\u4e86\u89e3\u4e0d\u5b8c\u6574\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u96fb\u5b50\u96fb\u8def\u6a21\u578b (ECM)\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u500b\u958b\u767c\u53ef\u64f4\u5145\u3001\u53ef\u5b78\u7fd2\u653f\u7b56\u548c\u6539\u9032 AI \u751f\u6210\u5167\u5bb9\u7ba1\u7406\u7684\u57fa\u790e\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cECM \u5c07\u6a21\u578b\u884c\u70ba\u6982\u5ff5\u5316\u70ba\u96fb\u5b50\u96fb\u8def\uff1aICL \u88ab\u8868\u793a\u70ba\u8a9e\u7fa9\u78c1\u5834\uff0c\u6839\u64da\u6cd5\u62c9\u7b2c\u5b9a\u5f8b\u63d0\u4f9b\u984d\u5916\u96fb\u58d3\uff0c\u800c CoT \u5247\u5efa\u6a21\u70ba\u4e32\u806f\u96fb\u963b\uff0c\u4ee5\u6839\u64da\u6b50\u59c6\u5b9a\u5f8b\u7d04\u675f\u6a21\u578b\u8f38\u51fa\u6548\u80fd\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cECM \u6709\u6548\u9810\u6e2c\u548c\u89e3\u91cb\u4e86 LLM \u5728\u5404\u7a2e\u63d0\u793a\u7b56\u7565\u4e2d\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07 ECM \u61c9\u7528\u65bc\u4e00\u7cfb\u5217\u4efb\u52d9\u7684\u9ad8\u7d1a\u63a8\u7406\u7b56\u7565\u6700\u4f73\u5316\uff0c\u4f8b\u5982\u570b\u969b\u8cc7\u8a0a\u5967\u6797\u5339\u514b\u7af6\u8cfd (IOI) \u548c\u570b\u969b\u6578\u5b78\u5967\u6797\u5339\u514b\u7af6\u8cfd (IMO)\uff0c\u53d6\u5f97\u4e86\u512a\u65bc\u8fd1 80% \u9802\u5c16\u4eba\u985e\u7af6\u722d\u8005\u7684\u7af6\u722d\u6027\u6548\u80fd\u3002", "author": "Qiguang Chen et.al.", "authors": "Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiaqi Wang, Mengkang Hu, Zhi Chen, Wanxiang Che, Ting Liu", "id": "2502.03325v1", "paper_url": "http://arxiv.org/abs/2502.03325v1", "repo": "null"}}