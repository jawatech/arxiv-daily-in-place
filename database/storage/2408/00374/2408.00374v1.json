{"2408.00374": {"publish_time": "2024-08-01", "title": "Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving", "paper_summary": "Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.", "paper_summary_zh": "\u76ee\u524d\u95dc\u65bc\u8ecc\u8de1\u9810\u6e2c\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8cf4\u65bc\u81ea\u6211\u8eca\u8f1b\u4e0a\u642d\u8f09\u611f\u6e2c\u5668\u6240\u6536\u96c6\u7684\u8cc7\u6599\u3002\u96a8\u8457\u8eca\u806f\u7db2\u6280\u8853\u7684\u5feb\u901f\u767c\u5c55\uff0c\u4f8b\u5982\u8eca\u5c0d\u8eca (V2V) \u548c\u8eca\u5c0d\u57fa\u790e\u8a2d\u65bd (V2I) \u901a\u8a0a\uff0c\u4f86\u81ea\u5099\u7528\u8996\u89d2\u7684\u5bf6\u8cb4\u8cc7\u8a0a\u53ef\u900f\u904e\u7121\u7dda\u7db2\u8def\u5b58\u53d6\u3002\u6574\u5408\u4f86\u81ea\u5099\u7528\u8996\u89d2\u7684\u8cc7\u8a0a\u6709\u6f5b\u529b\u514b\u670d\u8207\u55ae\u4e00\u8996\u89d2\u76f8\u95dc\u7684\u56fa\u6709\u9650\u5236\uff0c\u4f8b\u5982\u906e\u64cb\u548c\u53d7\u9650\u8996\u91ce\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u9032 V2INet\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u8ecc\u8de1\u9810\u6e2c\u67b6\u69cb\uff0c\u65e8\u5728\u900f\u904e\u64f4\u5145\u73fe\u6709\u7684\u55ae\u4e00\u8996\u89d2\u6a21\u578b\u4f86\u5efa\u69cb\u591a\u8996\u89d2\u8cc7\u6599\u3002\u8207\u5148\u524d\u7684\u505a\u6cd5\u4e0d\u540c\uff0c\u5148\u524d\u7684\u505a\u6cd5\u662f\u624b\u52d5\u878d\u5408\u591a\u8996\u89d2\u8cc7\u6599\u6216\u5c07\u5176\u5236\u5b9a\u70ba\u4e00\u500b\u7368\u7acb\u7684\u8a13\u7df4\u968e\u6bb5\uff0c\u6211\u5011\u7684\u6a21\u578b\u652f\u63f4\u7aef\u5c0d\u7aef\u7684\u8a13\u7df4\uff0c\u63d0\u5347\u4e86\u5f48\u6027\u548c\u6548\u80fd\u3002\u6b64\u5916\uff0c\u9810\u6e2c\u7684\u591a\u6a21\u614b\u8ecc\u8de1\u7531\u4e8b\u5f8c\u5171\u5f62\u9810\u6e2c\u6a21\u7d44\u6821\u6b63\uff0c\u4ee5\u53d6\u5f97\u6709\u6548\u4e14\u7cbe\u78ba\u7684\u4fe1\u5fc3\u5340\u57df\u3002\u6211\u5011\u4f7f\u7528\u771f\u5be6\u4e16\u754c\u7684 V2I \u8cc7\u6599\u96c6 V2X-Seq \u8a55\u4f30\u4e86\u6574\u500b\u67b6\u69cb\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\uff0c\u4f7f\u7528\u55ae\u4e00 GPU \u6642\uff0c\u5728\u6700\u7d42\u4f4d\u79fb\u8aa4\u5dee (FDE) \u548c\u907a\u6f0f\u7387 (MR) \u65b9\u9762\u8868\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\u65bc\uff1a\\url{https://github.com/xichennn/V2I_trajectory_prediction}\u3002", "author": "Xi Chen et.al.", "authors": "Xi Chen, Rahul Bhadani, Larry Head", "id": "2408.00374v1", "paper_url": "http://arxiv.org/abs/2408.00374v1", "repo": "null"}}