{"2411.05361": {"publish_time": "2024-11-08", "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks", "paper_summary": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized\nhuman-machine interactions by seamlessly integrating various forms of data.\nDeveloping a universal spoken language model that comprehends a wide range of\nnatural language instructions is critical for bridging communication gaps and\nfacilitating more intuitive interactions. However, the absence of a\ncomprehensive evaluation benchmark poses a significant challenge. We present\nDynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive\nevaluation of instruction-based universal speech models. Building upon the\nfirst generation, this second version incorporates 125 new tasks contributed\ncollaboratively by the global research community, expanding the benchmark to a\ntotal of 180 tasks, making it the largest benchmark for speech and audio\nevaluation. While the first generation of Dynamic-SUPERB was limited to\nclassification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation\ncapabilities by introducing a wide array of novel and diverse tasks, including\nregression and sequence generation, across speech, music, and environmental\naudio. Evaluation results indicate that none of the models performed well\nuniversally. SALMONN-13B excelled in English ASR, while WavLLM demonstrated\nhigh accuracy in emotion recognition, but current models still require further\ninnovations to handle a broader range of tasks. We will soon open-source all\ntask data and the evaluation pipeline.", "paper_summary_zh": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u4f8b\u5982 Gemini \u548c ChatGPT\uff0c\u901a\u8fc7\u65e0\u7f1d\u96c6\u6210\u5404\u79cd\u5f62\u5f0f\u7684\u6570\u636e\uff0c\u5f7b\u5e95\u6539\u53d8\u4e86\u4eba\u673a\u4ea4\u4e92\u3002\u5f00\u53d1\u4e00\u4e2a\u7406\u89e3\u5e7f\u6cdb\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u901a\u7528\u53e3\u8bed\u8bed\u8a00\u6a21\u578b\u5bf9\u4e8e\u5f25\u5408\u6c9f\u901a\u9e3f\u6c9f\u548c\u4fc3\u8fdb\u66f4\u76f4\u89c2\u7684\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86 Dynamic-SUPERB 2 \u9636\u6bb5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u653e\u4e14\u4e0d\u65ad\u53d1\u5c55\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u5bf9\u57fa\u4e8e\u6307\u4ee4\u7684\u901a\u7528\u8bed\u97f3\u6a21\u578b\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002\u5728\u6b64\u7b2c\u4e00\u4ee3\u7684\u57fa\u7840\u4e0a\uff0c\u6b64\u7b2c\u4e8c\u7248\u7eb3\u5165\u4e86\u7531\u5168\u7403\u7814\u7a76\u754c\u534f\u4f5c\u8d21\u732e\u7684 125 \u9879\u65b0\u4efb\u52a1\uff0c\u5c06\u57fa\u51c6\u6269\u5c55\u5230\u603b\u5171 180 \u9879\u4efb\u52a1\uff0c\u4f7f\u5176\u6210\u4e3a\u8bed\u97f3\u548c\u97f3\u9891\u8bc4\u4f30\u4e2d\u6700\u5927\u7684\u57fa\u51c6\u3002\u867d\u7136\u7b2c\u4e00\u4ee3 Dynamic-SUPERB \u4ec5\u9650\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u4f46 Dynamic-SUPERB 2 \u9636\u6bb5\u901a\u8fc7\u5f15\u5165\u5e7f\u6cdb\u7684\u65b0\u9896\u4e14\u591a\u6837\u7684\u4efb\u52a1\uff0c\u5305\u62ec\u56de\u5f52\u548c\u5e8f\u5217\u751f\u6210\uff0c\u8de8\u8d8a\u8bed\u97f3\u3001\u97f3\u4e50\u548c\u73af\u5883\u97f3\u9891\uff0c\u6269\u5c55\u4e86\u5176\u8bc4\u4f30\u80fd\u529b\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6ca1\u6709\u4e00\u4e2a\u6a21\u578b\u5728\u6240\u6709\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002SALMONN-13B \u5728\u82f1\u8bed ASR \u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u800c WavLLM \u5728\u60c5\u7eea\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u4ecd\u9700\u8981\u8fdb\u4e00\u6b65\u521b\u65b0\u624d\u80fd\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u3002\u6211\u4eec\u5f88\u5feb\u5c06\u5f00\u6e90\u6240\u6709\u4efb\u52a1\u6570\u636e\u548c\u8bc4\u4f30\u7ba1\u9053\u3002", "author": "Chien-yu Huang et.al.", "authors": "Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Fabian Ritter-Gutierrez, Ming To Chuang, Kuan-Po Huang, Siddhant Arora, You-Kuan Lin, Eunjung Yeo, Kalvin Chang, Chung-Ming Chien, Kwanghee Choi, Cheng-Hsiu Hsieh, Yi-Cheng Lin, Chee-En Yu, I-Hsiang Chiu, Heitor R. Guimar\u00e3es, Jionghao Han, Tzu-Quan Lin, Tzu-Yuan Lin, Homu Chang, Ting-Wu Chang, Chun Wei Chen, Shou-Jen Chen, Yu-Hua Chen, Hsi-Chun Cheng, Kunal Dhawan, Jia-Lin Fang, Shi-Xin Fang, Kuan-Yu Fang Chiang, Chi An Fu, Hsien-Fu Hsiao, Ching Yu Hsu, Shao-Syuan Huang, Lee Chen Wei, Hsi-Che Lin, Hsuan-Hao Lin, Hsuan-Ting Lin, Jian-Ren Lin, Ting-Chun Liu, Li-Chun Lu, Tsung-Min Pai, Ankita Pasad, Shih-Yun Shan Kuan, Suwon Shon, Yuxun Tang, Yun-Shao Tsai, Jui-Chiang Wei, Tzu-Chieh Wei, Chengxi Wu, Dien-Ruei Wu, Chao-Han Huck Yang, Chieh-Chi Yang, Jia Qi Yip, Shao-Xiang Yuan, Vahid Noroozi, Zhehuai Chen, Haibin Wu, Karen Livescu, David Harwath, Shinji Watanabe, Hung-yi Lee", "id": "2411.05361v1", "paper_url": "http://arxiv.org/abs/2411.05361v1", "repo": "null"}}