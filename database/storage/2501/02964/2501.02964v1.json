{"2501.02964": {"publish_time": "2025-01-06", "title": "Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild", "paper_summary": "Complex visual reasoning remains a key challenge today. Typically, the\nchallenge is tackled using methodologies such as Chain of Thought (COT) and\nvisual instruction tuning. However, how to organically combine these two\nmethodologies for greater success remains unexplored. Also, issues like\nhallucinations and high training cost still need to be addressed. In this work,\nwe devise an innovative multi-round training and reasoning framework suitable\nfor lightweight Multimodal Large Language Models (MLLMs). Our self-questioning\napproach heuristically guides MLLMs to focus on visual clues relevant to the\ntarget problem, reducing hallucinations and enhancing the model's ability to\ndescribe fine-grained image details. This ultimately enables the model to\nperform well in complex visual reasoning and question-answering tasks. We have\nnamed this framework Socratic Questioning(SQ). To facilitate future research,\nwe create a multimodal mini-dataset named CapQA, which includes 1k images of\nfine-grained activities, for visual instruction tuning and evaluation, our\nproposed SQ method leads to a 31.2% improvement in the hallucination score. Our\nextensive experiments on various benchmarks demonstrate SQ's remarkable\ncapabilities in heuristic self-questioning, zero-shot visual reasoning and\nhallucination mitigation. Our model and code will be publicly available.", "paper_summary_zh": "\u8907\u96dc\u7684\u8996\u89ba\u63a8\u7406\u81f3\u4eca\u4ecd\u662f\u4e00\u9805\u95dc\u9375\u6311\u6230\u3002\u901a\u5e38\uff0c\u6b64\u6311\u6230\u662f\u900f\u904e\u601d\u60f3\u93c8 (COT) \u548c\u8996\u89ba\u6307\u4ee4\u8abf\u6574\u7b49\u65b9\u6cd5\u4f86\u89e3\u6c7a\u3002\u7136\u800c\uff0c\u5982\u4f55\u5c07\u9019\u5169\u7a2e\u65b9\u6cd5\u6709\u6a5f\u7d50\u5408\u4ee5\u7372\u5f97\u66f4\u5927\u7684\u6210\u529f\u4ecd\u672a\u88ab\u63a2\u8a0e\u3002\u6b64\u5916\uff0c\u5e7b\u89ba\u548c\u9ad8\u8a13\u7df4\u6210\u672c\u7b49\u554f\u984c\u4ecd\u9700\u8981\u89e3\u6c7a\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u5275\u65b0\u7684\u591a\u8f2a\u8a13\u7df4\u548c\u63a8\u7406\u6846\u67b6\uff0c\u9069\u7528\u65bc\u8f15\u91cf\u7d1a\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM)\u3002\u6211\u5011\u7684\u81ea\u554f\u81ea\u7b54\u65b9\u6cd5\u555f\u767c\u5f0f\u5730\u5f15\u5c0e MLLM \u5c08\u6ce8\u65bc\u8207\u76ee\u6a19\u554f\u984c\u76f8\u95dc\u7684\u8996\u89ba\u7dda\u7d22\uff0c\u6e1b\u5c11\u5e7b\u89ba\u4e26\u589e\u5f37\u6a21\u578b\u63cf\u8ff0\u7cbe\u7d30\u5f71\u50cf\u7d30\u7bc0\u7684\u80fd\u529b\u3002\u9019\u6700\u7d42\u4f7f\u6a21\u578b\u80fd\u5920\u5728\u8907\u96dc\u7684\u8996\u89ba\u63a8\u7406\u548c\u554f\u7b54\u4efb\u52d9\u4e2d\u8868\u73fe\u826f\u597d\u3002\u6211\u5011\u5c07\u6b64\u6846\u67b6\u547d\u540d\u70ba\u8607\u683c\u62c9\u5e95\u5f0f\u63d0\u554f (SQ)\u3002\u70ba\u4e86\u4fc3\u9032\u672a\u4f86\u7684\u7814\u7a76\uff0c\u6211\u5011\u5275\u5efa\u4e86\u4e00\u500b\u540d\u70ba CapQA \u7684\u591a\u6a21\u614b\u8ff7\u4f60\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b 1k \u5f35\u7cbe\u7d30\u6d3b\u52d5\u7684\u5f71\u50cf\uff0c\u7528\u65bc\u8996\u89ba\u6307\u4ee4\u8abf\u6574\u548c\u8a55\u4f30\uff0c\u6211\u5011\u63d0\u51fa\u7684 SQ \u65b9\u6cd5\u4f7f\u5e7b\u89ba\u8a55\u5206\u63d0\u9ad8\u4e86 31.2%\u3002\u6211\u5011\u5728\u5404\u7a2e\u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 SQ \u5728\u555f\u767c\u5f0f\u81ea\u554f\u81ea\u7b54\u3001\u96f6\u6b21\u8996\u89ba\u63a8\u7406\u548c\u5e7b\u89ba\u7de9\u89e3\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\u3002\u6211\u5011\u7684\u6a21\u578b\u548c\u7a0b\u5f0f\u78bc\u5c07\u516c\u958b\u63d0\u4f9b\u3002", "author": "Wanpeng Hu et.al.", "authors": "Wanpeng Hu, Haodi Liu, Lin Chen, Feng Zhou, Changming Xiao, Qi Yang, Changshui Zhang", "id": "2501.02964v1", "paper_url": "http://arxiv.org/abs/2501.02964v1", "repo": "https://github.com/aibee00/socraticquestioning"}}