{"2412.02684": {"publish_time": "2024-12-03", "title": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction", "paper_summary": "Generating animatable human avatars from a single image is essential for\nvarious digital human modeling applications. Existing 3D reconstruction methods\noften struggle to capture fine details in animatable models, while generative\napproaches for controllable animation, though avoiding explicit 3D modeling,\nsuffer from viewpoint inconsistencies in extreme poses and computational\ninefficiencies. In this paper, we address these challenges by leveraging the\npower of generative models to produce detailed multi-view canonical pose\nimages, which help resolve ambiguities in animatable human reconstruction. We\nthen propose a robust method for 3D reconstruction of inconsistent images,\nenabling real-time rendering during inference. Specifically, we adapt a\ntransformer-based video generation model to generate multi-view canonical pose\nimages and normal maps, pretraining on a large-scale video dataset to improve\ngeneralization. To handle view inconsistencies, we recast the reconstruction\nproblem as a 4D task and introduce an efficient 3D modeling approach using 4D\nGaussian Splatting. Experiments demonstrate that our method achieves\nphotorealistic, real-time animation of 3D human avatars from in-the-wild\nimages, showcasing its effectiveness and generalization capability.", "paper_summary_zh": "\u5f9e\u55ae\u4e00\u5f71\u50cf\u7522\u751f\u53ef\u52d5\u614b\u5316\u7684\u771f\u4eba\u5316\u8eab\u5c0d\u65bc\u5404\u7a2e\u6578\u4f4d\u4eba\u985e\u5efa\u6a21\u61c9\u7528\u4f86\u8aaa\u81f3\u95dc\u91cd\u8981\u3002\u73fe\u6709\u7684 3D \u91cd\u5efa\u65b9\u6cd5\u5728\u52d5\u614b\u5316\u6a21\u578b\u4e2d\u6355\u6349\u7cbe\u7d30\u7d30\u7bc0\u6642\u5f80\u5f80\u6703\u9047\u5230\u56f0\u96e3\uff0c\u800c\u7528\u65bc\u53ef\u63a7\u52d5\u614b\u5316\u7684\u751f\u6210\u65b9\u6cd5\u5118\u7ba1\u907f\u514d\u4e86\u660e\u78ba\u7684 3D \u5efa\u6a21\uff0c\u4f46\u5728\u6975\u7aef\u59ff\u52e2\u548c\u8a08\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u8996\u9ede\u4e0d\u4e00\u81f4\u7684\u554f\u984c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u529b\u91cf\u4f86\u7522\u751f\u8a73\u7d30\u7684\u591a\u8996\u5716\u6a19\u6e96\u59ff\u52e2\u5f71\u50cf\uff0c\u89e3\u6c7a\u52d5\u614b\u5316\u4eba\u985e\u91cd\u5efa\u4e2d\u7684\u6a21\u7cca\u6027\uff0c\u4f86\u89e3\u6c7a\u9019\u4e9b\u6311\u6230\u3002\u63a5\u8457\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7528\u65bc\u4e0d\u4e00\u81f4\u5f71\u50cf\u7684 3D \u91cd\u5efa\u7a69\u5065\u65b9\u6cd5\uff0c\u8b93\u63a8\u8ad6\u671f\u9593\u80fd\u5920\u5373\u6642\u6e32\u67d3\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8abf\u6574\u4e00\u500b\u57fa\u65bc\u8f49\u63db\u5668\u7684\u5f71\u7247\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u7522\u751f\u591a\u8996\u5716\u6a19\u6e96\u59ff\u52e2\u5f71\u50cf\u548c\u6cd5\u7dda\u8cbc\u5716\uff0c\u4e26\u5728\u4e00\u500b\u5927\u898f\u6a21\u7684\u5f71\u7247\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u9810\u8a13\u7df4\u4ee5\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002\u70ba\u4e86\u8655\u7406\u8996\u9ede\u4e0d\u4e00\u81f4\u7684\u554f\u984c\uff0c\u6211\u5011\u5c07\u91cd\u5efa\u554f\u984c\u91cd\u65b0\u5b9a\u7fa9\u70ba\u4e00\u500b 4D \u4efb\u52d9\uff0c\u4e26\u4f7f\u7528 4D \u9ad8\u65af\u5674\u7051\u6cd5\u5f15\u5165\u4e00\u500b\u9ad8\u6548\u7684 3D \u5efa\u6a21\u65b9\u6cd5\u3002\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u9019\u500b\u65b9\u6cd5\u53ef\u4ee5\u5f9e\u81ea\u7136\u5f71\u50cf\u4e2d\u5be6\u73fe 3D \u771f\u4eba\u5316\u8eab\u7684\u7167\u7247\u5beb\u5be6\u5373\u6642\u52d5\u756b\uff0c\u5c55\u793a\u4e86\u5b83\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "author": "Lingteng Qiu et.al.", "authors": "Lingteng Qiu, Shenhao Zhu, Qi Zuo, Xiaodong Gu, Yuan Dong, Junfei Zhang, Chao Xu, Zhe Li, Weihao Yuan, Liefeng Bo, Guanying Chen, Zilong Dong", "id": "2412.02684v1", "paper_url": "http://arxiv.org/abs/2412.02684v1", "repo": "null"}}