{"2405.06626": {"publish_time": "2024-05-10", "title": "Characterizing the Accuracy - Efficiency Trade-off of Low-rank Decomposition in Language Models", "paper_summary": "Large language models (LLMs) have emerged and presented their general\nproblem-solving capabilities with one model. However, the model size has\nincreased dramatically with billions of parameters to enable such broad\nproblem-solving capabilities. In addition, due to the dominance of\nmatrix-matrix and matrix-vector multiplications in LLMs, the compute-to-model\nsize ratio is significantly lower than that of CNNs. This shift pushes LLMs\nfrom a computation-bound regime to a memory-bound regime. Therefore, optimizing\nthe memory footprint and traffic is an important optimization direction for\nLLMs today.\n  Model compression methods such as quantization and parameter pruning have\nbeen actively explored for achieving the memory footprint and traffic\noptimization. However, the accuracy-efficiency trade-off of rank pruning for\nLLMs is not well-understood yet. Therefore, we characterize the\naccuracy-efficiency trade-off of a low-rank decomposition method, specifically\nTucker decomposition, on recent language models, including an open-source LLM,\nLlama 2.\n  We formalize the low-rank decomposition design space and show that the\ndecomposition design space is enormous (e.g., O($2^{37}$) for Llama2-7B). To\nnavigate such a vast design space, we formulate the design space and perform\nthorough case studies of accuracy-efficiency trade-offs using six widely used\nLLM benchmarks on BERT and Llama 2 models. Our results show that we can achieve\na 9\\% model size reduction with minimal accuracy drops, which range from 4\\%p\nto 10\\%p, depending on the difficulty of the benchmark, without any retraining\nto recover accuracy after decomposition. The results show that low-rank\ndecomposition can be a promising direction for LLM-based applications that\nrequire real-time service in scale (e.g., AI agent assist and real-time coding\nassistant), where the latency is as important as the model accuracy.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6d6e\u73fe\uff0c\u4e26\u4ee5\u55ae\u4e00\u6a21\u578b\u5c55\u793a\u5176\u4e00\u822c\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u3002\u7136\u800c\uff0c\u6a21\u578b\u5927\u5c0f\u5df2\u5927\u5e45\u589e\u52a0\uff0c\u64c1\u6709\u6578\u5341\u5104\u500b\u53c3\u6578\uff0c\u4ee5\u5be6\u73fe\u5982\u6b64\u5ee3\u6cdb\u7684\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7531\u65bc LLM \u4e2d\u77e9\u9663-\u77e9\u9663\u548c\u77e9\u9663-\u5411\u91cf\u4e58\u6cd5\u7684\u512a\u52e2\uff0c\u8a08\u7b97\u8207\u6a21\u578b\u5927\u5c0f\u7684\u6bd4\u7387\u986f\u8457\u4f4e\u65bc CNN\u3002\u6b64\u8f49\u8b8a\u5c07 LLM \u5f9e\u8a08\u7b97\u53d7\u9650\u7684\u6a21\u5f0f\u63a8\u5411\u8a18\u61b6\u53d7\u9650\u7684\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u512a\u5316\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u548c\u6d41\u91cf\u662f\u7576\u4eca LLM \u7684\u91cd\u8981\u512a\u5316\u65b9\u5411\u3002\n\u6a21\u578b\u58d3\u7e2e\u65b9\u6cd5\uff08\u4f8b\u5982\u91cf\u5316\u548c\u53c3\u6578\u526a\u679d\uff09\u5df2\u88ab\u7a4d\u6975\u63a2\u7d22\uff0c\u4ee5\u5be6\u73fe\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u548c\u6d41\u91cf\u512a\u5316\u3002\u7136\u800c\uff0cLLM \u7684\u79e9\u526a\u679d\u7684\u6e96\u78ba\u5ea6-\u6548\u7387\u6b0a\u8861\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u4f4e\u79e9\u5206\u89e3\u65b9\u6cd5\uff08\u7279\u5225\u662f Tucker \u5206\u89e3\uff09\u5728\u8fd1\u671f\u8a9e\u8a00\u6a21\u578b\uff08\u5305\u62ec\u958b\u6e90 LLM\uff0cLlama 2\uff09\u4e0a\u7684\u6e96\u78ba\u5ea6-\u6548\u7387\u6b0a\u8861\u3002\n\u6211\u5011\u5f62\u5f0f\u5316\u4e86\u4f4e\u79e9\u5206\u89e3\u8a2d\u8a08\u7a7a\u9593\uff0c\u4e26\u8868\u660e\u5206\u89e3\u8a2d\u8a08\u7a7a\u9593\u975e\u5e38\u9f90\u5927\uff08\u4f8b\u5982\uff0cLlama2-7B \u7684 O($2^{37}$)\u3002\u70ba\u4e86\u5c0e\u822a\u5982\u6b64\u5ee3\u95ca\u7684\u8a2d\u8a08\u7a7a\u9593\uff0c\u6211\u5011\u5236\u5b9a\u4e86\u8a2d\u8a08\u7a7a\u9593\uff0c\u4e26\u4f7f\u7528 BERT \u548c Llama 2 \u6a21\u578b\u4e0a\u7684\u516d\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684 LLM \u57fa\u6e96\uff0c\u5c0d\u6e96\u78ba\u5ea6-\u6548\u7387\u6b0a\u8861\u9032\u884c\u4e86\u5fb9\u5e95\u7684\u6848\u4f8b\u7814\u7a76\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u53ef\u4ee5\u5728\u4e0d\u91cd\u65b0\u8a13\u7df4\u4ee5\u5728\u5206\u89e3\u5f8c\u6062\u5fa9\u6e96\u78ba\u5ea6\u7684\u60c5\u6cc1\u4e0b\uff0c\u5be6\u73fe 9% \u7684\u6a21\u578b\u5927\u5c0f\u6e1b\u5c0f\uff0c\u6e96\u78ba\u5ea6\u4e0b\u964d\u6700\u5c0f\uff0c\u7bc4\u570d\u5f9e 4%p \u5230 10%p\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u57fa\u6e96\u7684\u96e3\u5ea6\u3002\u7d50\u679c\u8868\u660e\uff0c\u4f4e\u79e9\u5206\u89e3\u5c0d\u65bc\u9700\u8981\u5927\u898f\u6a21\u5373\u6642\u670d\u52d9\u7684\u57fa\u65bc LLM \u7684\u61c9\u7528\uff08\u4f8b\u5982\uff0cAI \u4ee3\u7406\u5354\u52a9\u548c\u5373\u6642\u7de8\u78bc\u52a9\u7406\uff09\u4f86\u8aaa\uff0c\u53ef\u80fd\u662f\u4e00\u500b\u6709\u524d\u9014\u7684\u65b9\u5411\uff0c\u5176\u4e2d\u5ef6\u9072\u8207\u6a21\u578b\u6e96\u78ba\u5ea6\u4e00\u6a23\u91cd\u8981\u3002", "author": "Chakshu Moar et.al.", "authors": "Chakshu Moar, Michael Pellauer, Hyoukjun Kwon", "id": "2405.06626v1", "paper_url": "http://arxiv.org/abs/2405.06626v1", "repo": "null"}}