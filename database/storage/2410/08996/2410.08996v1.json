{"2410.08996": {"publish_time": "2024-10-11", "title": "Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference", "paper_summary": "We test whether replacing crowdsource workers with LLMs to write Natural\nLanguage Inference (NLI) hypotheses similarly results in annotation artifacts.\nWe recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and\nMistral 7b, and train hypothesis-only classifiers to determine whether\nLLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI\ndatasets, BERT-based hypothesis-only classifiers achieve between 86-96%\naccuracy, indicating these datasets contain hypothesis-only artifacts. We also\nfind frequent \"give-aways\" in LLM-generated hypotheses, e.g. the phrase\n\"swimming in a pool\" appears in more than 10,000 contradictions generated by\nGPT-4. Our analysis provides empirical evidence that well-attested biases in\nNLI can persist in LLM-generated data.", "paper_summary_zh": "\u6211\u5011\u6e2c\u8a66\u4ee5 LLM \u53d6\u4ee3\u7fa4\u773e\u5916\u5305\u5de5\u4f5c\u8005\u4f86\u64b0\u5beb\u81ea\u7136\u8a9e\u8a00\u63a8\u7406 (NLI) \u5047\u8a2d\u662f\u5426\u4e5f\u6703\u7522\u751f\u8a3b\u89e3\u4eba\u5de5\u88fd\u54c1\u3002\u6211\u5011\u4f7f\u7528 GPT-4\u3001Llama-2 \u548c Mistral 7b \u91cd\u65b0\u5efa\u7acb\u4e00\u90e8\u5206\u53f2\u4e39\u4f5b NLI \u8a9e\u6599\u5eab\uff0c\u4e26\u8a13\u7df4\u50c5\u9650\u5047\u8a2d\u7684\u5206\u985e\u5668\uff0c\u4ee5\u78ba\u5b9a LLM \u5f15\u767c\u7684\u5047\u8a2d\u662f\u5426\u5305\u542b\u8a3b\u89e3\u4eba\u5de5\u88fd\u54c1\u3002\u5728\u6211\u5011\u7684 LLM \u5f15\u767c\u7684 NLI \u8cc7\u6599\u96c6\u4e0a\uff0c\u57fa\u65bc BERT \u7684\u50c5\u9650\u5047\u8a2d\u5206\u985e\u5668\u9054\u5230\u4e86 86-96% \u7684\u6e96\u78ba\u5ea6\uff0c\u8868\u793a\u9019\u4e9b\u8cc7\u6599\u96c6\u5305\u542b\u50c5\u9650\u5047\u8a2d\u7684\u4eba\u5de5\u88fd\u54c1\u3002\u6211\u5011\u4e5f\u5728 LLM \u751f\u6210\u7684\u5047\u8a2d\u4e2d\u767c\u73fe\u983b\u7e41\u7684\u300c\u63d0\u793a\u300d\uff0c\u4f8b\u5982\u77ed\u8a9e\u300c\u5728\u6cf3\u6c60\u4e2d\u6e38\u6cf3\u300d\u51fa\u73fe\u5728 GPT-4 \u751f\u6210\u7684 10,000 \u591a\u500b\u77db\u76fe\u4e2d\u3002\u6211\u5011\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u7d93\u9a57\u8b49\u64da\uff0c\u8b49\u660e NLI \u4e2d\u7d93\u904e\u5145\u5206\u9a57\u8b49\u7684\u504f\u5dee\u53ef\u80fd\u6703\u6301\u7e8c\u5b58\u5728\u65bc LLM \u751f\u6210\u7684\u8cc7\u6599\u4e2d\u3002", "author": "Grace Proebsting et.al.", "authors": "Grace Proebsting, Adam Poliak", "id": "2410.08996v1", "paper_url": "http://arxiv.org/abs/2410.08996v1", "repo": "null"}}