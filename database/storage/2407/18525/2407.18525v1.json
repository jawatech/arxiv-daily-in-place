{"2407.18525": {"publish_time": "2024-07-26", "title": "Is larger always better? Evaluating and prompting large language models for non-generative medical tasks", "paper_summary": "The use of Large Language Models (LLMs) in medicine is growing, but their\nability to handle both structured Electronic Health Record (EHR) data and\nunstructured clinical notes is not well-studied. This study benchmarks various\nmodels, including GPT-based LLMs, BERT-based models, and traditional clinical\npredictive models, for non-generative medical tasks utilizing renowned\ndatasets. We assessed 14 language models (9 GPT-based and 5 BERT-based) and 7\ntraditional predictive models using the MIMIC dataset (ICU patient records) and\nthe TJH dataset (early COVID-19 EHR data), focusing on tasks such as mortality\nand readmission prediction, disease hierarchy reconstruction, and biomedical\nsentence matching, comparing both zero-shot and finetuned performance. Results\nindicated that LLMs exhibited robust zero-shot predictive capabilities on\nstructured EHR data when using well-designed prompting strategies, frequently\nsurpassing traditional models. However, for unstructured medical texts, LLMs\ndid not outperform finetuned BERT models, which excelled in both supervised and\nunsupervised tasks. Consequently, while LLMs are effective for zero-shot\nlearning on structured data, finetuned BERT models are more suitable for\nunstructured texts, underscoring the importance of selecting models based on\nspecific task requirements and data characteristics to optimize the application\nof NLP technology in healthcare.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u91ab\u5b78\u4e2d\u7684\u61c9\u7528\u65e5\u76ca\u5ee3\u6cdb\uff0c\u4f46\u5b83\u5011\u540c\u6642\u8655\u7406\u7d50\u69cb\u5316\u96fb\u5b50\u75c5\u6b77 (EHR) \u8cc7\u6599\u548c\u975e\u7d50\u69cb\u5316\u81e8\u5e8a\u8a3b\u8a18\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u7814\u7a76\u91dd\u5c0d\u5404\u7a2e\u6a21\u578b\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u5305\u62ec\u57fa\u65bc GPT \u7684 LLM\u3001\u57fa\u65bc BERT \u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u50b3\u7d71\u7684\u81e8\u5e8a\u9810\u6e2c\u6a21\u578b\uff0c\u7528\u65bc\u5229\u7528\u8457\u540d\u8cc7\u6599\u96c6\u7684\u975e\u751f\u6210\u6027\u91ab\u7642\u4efb\u52d9\u3002\u6211\u5011\u4f7f\u7528 MIMIC \u8cc7\u6599\u96c6\uff08ICU \u75c5\u4eba\u8a18\u9304\uff09\u548c TJH \u8cc7\u6599\u96c6\uff08\u65e9\u671f COVID-19 EHR \u8cc7\u6599\uff09\u8a55\u4f30\u4e86 14 \u500b\u8a9e\u8a00\u6a21\u578b\uff089 \u500b\u57fa\u65bc GPT\uff0c5 \u500b\u57fa\u65bc BERT\uff09\u548c 7 \u500b\u50b3\u7d71\u9810\u6e2c\u6a21\u578b\uff0c\u91cd\u9ede\u95dc\u6ce8\u6b7b\u4ea1\u7387\u548c\u518d\u5165\u9662\u9810\u6e2c\u3001\u75be\u75c5\u5c64\u7d1a\u91cd\u5efa\u548c\u751f\u7269\u91ab\u5b78\u53e5\u5b50\u914d\u5c0d\u7b49\u4efb\u52d9\uff0c\u4e26\u6bd4\u8f03\u4e86\u96f6\u6b21\u5b78\u7fd2\u548c\u5fae\u8abf\u5f8c\u7684\u6548\u80fd\u3002\u7d50\u679c\u8868\u660e\uff0cLLM \u5728\u4f7f\u7528\u8a2d\u8a08\u826f\u597d\u7684\u63d0\u793a\u7b56\u7565\u6642\uff0c\u5c0d\u7d50\u69cb\u5316 EHR \u8cc7\u6599\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u96f6\u6b21\u5b78\u7fd2\u9810\u6e2c\u80fd\u529b\uff0c\u7d93\u5e38\u8d85\u8d8a\u50b3\u7d71\u6a21\u578b\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u975e\u7d50\u69cb\u5316\u7684\u91ab\u7642\u6587\u672c\uff0cLLM \u7684\u8868\u73fe\u4e0d\u5982\u5fae\u8abf\u5f8c\u7684 BERT \u6a21\u578b\uff0c\u5f8c\u8005\u5728\u76e3\u7763\u5f0f\u548c\u975e\u76e3\u7763\u5f0f\u4efb\u52d9\u4e2d\u90fd\u8868\u73fe\u51fa\u8272\u3002\u56e0\u6b64\uff0c\u5118\u7ba1 LLM \u5c0d\u65bc\u7d50\u69cb\u5316\u8cc7\u6599\u7684\u96f6\u6b21\u5b78\u7fd2\u5f88\u6709\u7528\uff0c\u4f46\u5fae\u8abf\u5f8c\u7684 BERT \u6a21\u578b\u66f4\u9069\u5408\u975e\u7d50\u69cb\u5316\u6587\u672c\uff0c\u9019\u5f37\u8abf\u4e86\u6839\u64da\u7279\u5b9a\u4efb\u52d9\u9700\u6c42\u548c\u8cc7\u6599\u7279\u6027\u9078\u64c7\u6a21\u578b\u4ee5\u512a\u5316\u91ab\u7642\u4fdd\u5065\u4e2d NLP \u6280\u8853\u61c9\u7528\u4e4b\u91cd\u8981\u6027\u3002", "author": "Yinghao Zhu et.al.", "authors": "Yinghao Zhu, Junyi Gao, Zixiang Wang, Weibin Liao, Xiaochen Zheng, Lifang Liang, Yasha Wang, Chengwei Pan, Ewen M. Harrison, Liantao Ma", "id": "2407.18525v1", "paper_url": "http://arxiv.org/abs/2407.18525v1", "repo": "https://github.com/yhzhu99/ehr-llm-benchmark"}}