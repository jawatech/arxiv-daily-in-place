{"2501.13669": {"publish_time": "2025-01-23", "title": "How to Complete Domain Tuning while Keeping General Ability in LLM: Adaptive Layer-wise and Element-wise Regularization", "paper_summary": "Large Language Models (LLMs) exhibit strong general-purpose language\ncapabilities. However, fine-tuning these models on domain-specific tasks often\nleads to catastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss to retain the\nparameter crucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10%-15% of the storage, highlighting the practical efficiency.\nThe code will be released.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u5f37\u5927\u7684\u901a\u7528\u8a9e\u8a00\u80fd\u529b\u3002\u7136\u800c\uff0c\u91dd\u5c0d\u7279\u5b9a\u9818\u57df\u4efb\u52d9\u5fae\u8abf\u9019\u4e9b\u6a21\u578b\u6642\uff0c\u5e38\u5e38\u6703\u5c0e\u81f4\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u6a21\u578b\u6703\u8986\u5beb\u6216\u907a\u5931\u9810\u8a13\u7df4\u671f\u9593\u7fd2\u5f97\u7684\u57fa\u672c\u77e5\u8b58\u3002\u9019\u7a2e\u73fe\u8c61\u5927\u5e45\u9650\u5236\u4e86 LLM \u7684\u5ee3\u6cdb\u9069\u7528\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u9805\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u65b9\u6cd5\uff0c\u7528\u65bc\u8a08\u7b97\u6a21\u578b\u53c3\u6578\u7684\u5143\u7d20\u7d1a\u91cd\u8981\u6027\uff0c\u9019\u4e9b\u53c3\u6578\u5c0d\u65bc\u5728\u5fae\u8abf\u671f\u9593\u4fdd\u7559\u4e00\u822c\u77e5\u8b58\u81f3\u95dc\u91cd\u8981\u3002\u6211\u5011\u7684\u505a\u6cd5\u63a1\u7528\u96d9\u76ee\u6a19\u512a\u5316\u7b56\u7565\uff1a(1) \u6b63\u5247\u5316\u640d\u5931\uff0c\u7528\u65bc\u4fdd\u7559\u5c0d\u4e00\u822c\u77e5\u8b58\u81f3\u95dc\u91cd\u8981\u7684\u53c3\u6578\uff1b(2) \u4ea4\u53c9\u71b5\u640d\u5931\uff0c\u7528\u65bc\u9069\u61c9\u7279\u5b9a\u9818\u57df\u7684\u4efb\u52d9\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5c64\u7d1a\u4fc2\u6578\uff0c\u7528\u65bc\u8003\u91cf\u4e0d\u540c\u5c64\u7684\u8b8a\u7570\u8ca2\u737b\uff0c\u4e26\u52d5\u614b\u5e73\u8861\u96d9\u76ee\u6a19\u512a\u5316\u3002\u4f7f\u7528 GPT-J \u548c LLaMA-3 \u5728\u79d1\u5b78\u3001\u91ab\u7642\u548c\u7269\u7406\u4efb\u52d9\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u6e1b\u8f15\u4e86\u707d\u96e3\u6027\u907a\u5fd8\uff0c\u540c\u6642\u589e\u5f37\u4e86\u6a21\u578b\u9069\u61c9\u6027\u3002\u8207\u4e4b\u524d\u7684\u505a\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u89e3\u6c7a\u65b9\u6848\u901f\u5ea6\u5feb\u4e86\u7d04 20 \u500d\uff0c\u800c\u4e14\u53ea\u9700\u8981 10%-15% \u7684\u5132\u5b58\u7a7a\u9593\uff0c\u7a81\u986f\u4e86\u5176\u5be6\u7528\u7684\u6548\u7387\u3002\u7a0b\u5f0f\u78bc\u5c07\u6703\u91cb\u51fa\u3002", "author": "Shezheng Song et.al.", "authors": "Shezheng Song, Hao Xu, Jun Ma, Shasha Li, Long Peng, Qian Wan, Xiaodong Liu, Jie Yu", "id": "2501.13669v1", "paper_url": "http://arxiv.org/abs/2501.13669v1", "repo": "null"}}