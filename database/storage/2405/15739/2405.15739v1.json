{"2405.15739": {"publish_time": "2024-05-24", "title": "Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias", "paper_summary": "Citation practices are crucial in shaping the structure of scientific\nknowledge, yet they are often influenced by contemporary norms and biases. The\nemergence of Large Language Models (LLMs) like GPT-4 introduces a new dynamic\nto these practices. Interestingly, the characteristics and potential biases of\nreferences recommended by LLMs that entirely rely on their parametric\nknowledge, and not on search or retrieval-augmented generation, remain\nunexplored. Here, we analyze these characteristics in an experiment using a\ndataset of 166 papers from AAAI, NeurIPS, ICML, and ICLR, published after\nGPT-4's knowledge cut-off date, encompassing 3,066 references in total. In our\nexperiment, GPT-4 was tasked with suggesting scholarly references for the\nanonymized in-text citations within these papers. Our findings reveal a\nremarkable similarity between human and LLM citation patterns, but with a more\npronounced high citation bias in GPT-4, which persists even after controlling\nfor publication year, title length, number of authors, and venue. Additionally,\nwe observe a large consistency between the characteristics of GPT-4's existing\nand non-existent generated references, indicating the model's internalization\nof citation patterns. By analyzing citation graphs, we show that the references\nrecommended by GPT-4 are embedded in the relevant citation context, suggesting\nan even deeper conceptual internalization of the citation networks. While LLMs\ncan aid in citation generation, they may also amplify existing biases and\nintroduce new ones, potentially skewing scientific knowledge dissemination. Our\nresults underscore the need for identifying the model's biases and for\ndeveloping balanced methods to interact with LLMs in general.", "paper_summary_zh": "<paragraph>\u5f15\u6587\u5be6\u52d9\u5c0d\u65bc\u5f62\u5851\u79d1\u5b78\u77e5\u8b58\u7684\u7d50\u69cb\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u53d7\u5230\u7576\u4ee3\u898f\u7bc4\u548c\u504f\u898b\u7684\u5f71\u97ff\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5982 GPT-4 \u7684\u51fa\u73fe\u70ba\u9019\u4e9b\u5be6\u52d9\u5e36\u4f86\u4e86\u65b0\u7684\u52d5\u614b\u3002\u6709\u8da3\u7684\u662f\uff0c\u5b8c\u5168\u4f9d\u8cf4\u53c3\u6578\u5316\u77e5\u8b58\u800c\u975e\u641c\u5c0b\u6216\u6aa2\u7d22\u589e\u5f37\u751f\u6210\uff0c\u7531 LLM \u63a8\u85a6\u7684\u53c3\u8003\u6587\u737b\u7684\u7279\u6027\u548c\u6f5b\u5728\u504f\u898b\u4ecd\u7136\u672a\u7d93\u63a2\u7a76\u3002\u5728\u6b64\uff0c\u6211\u5011\u4f7f\u7528\u4e00\u500b\u5305\u542b\u4f86\u81ea AAAI\u3001NeurIPS\u3001ICML \u548c ICLR \u7684 166 \u7bc7\u8ad6\u6587\u7684\u8cc7\u6599\u96c6\uff0c\u5728 GPT-4 \u7684\u77e5\u8b58\u622a\u6b62\u65e5\u671f\u5f8c\u767c\u5e03\uff0c\u7e3d\u8a08\u5305\u542b 3,066 \u7bc7\u53c3\u8003\u6587\u737b\uff0c\u5c0d\u9019\u4e9b\u7279\u6027\u9032\u884c\u5206\u6790\u3002\u5728\u6211\u5011\u7684\u5be6\u9a57\u4e2d\uff0cGPT-4 \u7684\u4efb\u52d9\u662f\u70ba\u9019\u4e9b\u8ad6\u6587\u4e2d\u7684\u533f\u540d\u5167\u6587\u5f15\u6587\u5efa\u8b70\u5b78\u8853\u53c3\u8003\u6587\u737b\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u4eba\u985e\u548c LLM \u7684\u5f15\u6587\u6a21\u5f0f\u6709\u986f\u8457\u7684\u76f8\u4f3c\u6027\uff0c\u4f46 GPT-4 \u7684\u9ad8\u5f15\u6587\u504f\u898b\u66f4\u70ba\u986f\u8457\uff0c\u5373\u4f7f\u5728\u63a7\u5236\u4e86\u51fa\u7248\u5e74\u4efd\u3001\u6a19\u984c\u9577\u5ea6\u3001\u4f5c\u8005\u6578\u91cf\u548c\u5834\u5730\u5f8c\uff0c\u9019\u7a2e\u60c5\u6cc1\u4ecd\u7136\u5b58\u5728\u3002\u6b64\u5916\uff0c\u6211\u5011\u89c0\u5bdf\u5230 GPT-4 \u73fe\u6709\u548c\u4e0d\u5b58\u5728\u7684\u751f\u6210\u53c3\u8003\u6587\u737b\u7684\u7279\u6027\u4e4b\u9593\u6709\u5f88\u5927\u7684\u76f8\u5bb9\u6027\uff0c\u9019\u8868\u660e\u6a21\u578b\u5167\u5316\u4e86\u5f15\u6587\u6a21\u5f0f\u3002\u900f\u904e\u5206\u6790\u5f15\u6587\u5716\uff0c\u6211\u5011\u986f\u793a GPT-4 \u63a8\u85a6\u7684\u53c3\u8003\u6587\u737b\u5d4c\u5165\u5728\u76f8\u95dc\u7684\u5f15\u6587\u8108\u7d61\u4e2d\uff0c\u9019\u8868\u660e\u5c0d\u5f15\u6587\u7db2\u8def\u6709\u66f4\u6df1\u5165\u7684\u6982\u5ff5\u5167\u5316\u3002\u96d6\u7136 LLM \u53ef\u4ee5\u5354\u52a9\u751f\u6210\u5f15\u6587\uff0c\u4f46\u5b83\u5011\u4e5f\u53ef\u80fd\u653e\u5927\u73fe\u6709\u7684\u504f\u898b\u4e26\u5f15\u5165\u65b0\u7684\u504f\u898b\uff0c\u6f5b\u5728\u5730\u626d\u66f2\u79d1\u5b78\u77e5\u8b58\u7684\u50b3\u64ad\u3002\u6211\u5011\u7684\u7d50\u679c\u5f37\u8abf\u4e86\u8b58\u5225\u6a21\u578b\u504f\u898b\u548c\u767c\u5c55\u5e73\u8861\u7684\u65b9\u6cd5\u4ee5\u8207 LLM \u9032\u884c\u4e92\u52d5\u7684\u9700\u6c42\u3002</paragraph>", "author": "Andres Algaba et.al.", "authors": "Andres Algaba, Carmen Mazijn, Vincent Holst, Floriano Tori, Sylvia Wenmackers, Vincent Ginis", "id": "2405.15739v1", "paper_url": "http://arxiv.org/abs/2405.15739v1", "repo": "null"}}