{"2405.16806": {"publish_time": "2024-05-27", "title": "Entity Alignment with Noisy Annotations from Large Language Models", "paper_summary": "Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying\nequivalent entity pairs. While existing methods heavily rely on human-generated\nlabels, it is prohibitively expensive to incorporate cross-domain experts for\nannotation in real-world scenarios. The advent of Large Language Models (LLMs)\npresents new avenues for automating EA with annotations, inspired by their\ncomprehensive capability to process semantic information. However, it is\nnontrivial to directly apply LLMs for EA since the annotation space in\nreal-world KGs is large. LLMs could also generate noisy labels that may mislead\nthe alignment. To this end, we propose a unified framework, LLM4EA, to\neffectively leverage LLMs for EA. Specifically, we design a novel active\nlearning policy to significantly reduce the annotation space by prioritizing\nthe most valuable entities based on the entire inter-KG and intra-KG structure.\nMoreover, we introduce an unsupervised label refiner to continuously enhance\nlabel accuracy through in-depth probabilistic reasoning. We iteratively\noptimize the policy based on the feedback from a base EA model. Extensive\nexperiments demonstrate the advantages of LLM4EA on four benchmark datasets in\nterms of effectiveness, robustness, and efficiency. Codes are available via\nhttps://github.com/chensyCN/llm4ea_official.", "paper_summary_zh": "\u5be6\u9ad4\u6bd4\u5c0d (EA) \u65e8\u5728\u900f\u904e\u8b58\u5225\u7b49\u6548\u7684\u5be6\u9ad4\u5c0d\u4f86\u5408\u4f75\u5169\u500b\u77e5\u8b58\u5716\u8b5c (KG)\u3002\u96d6\u7136\u73fe\u6709\u65b9\u6cd5\u6975\u5ea6\u4f9d\u8cf4\u4eba\u5de5\u7522\u751f\u7684\u6a19\u7c64\uff0c\u4f46\u5728\u73fe\u5be6\u4e16\u754c\u7684\u5834\u666f\u4e2d\uff0c\u8981\u7d0d\u5165\u8de8\u9818\u57df\u7684\u5c08\u5bb6\u9032\u884c\u8a3b\u89e3\u662f\u96e3\u4ee5\u8ca0\u64d4\u7684\u6210\u672c\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\u70ba\u81ea\u52d5\u5316 EA \u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f91\uff0c\u5176\u9748\u611f\u4f86\u81ea\u65bc LLM \u5168\u9762\u8655\u7406\u8a9e\u7fa9\u8cc7\u8a0a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u65bc\u73fe\u5be6\u4e16\u754c KG \u4e2d\u7684\u8a3b\u89e3\u7a7a\u9593\u5f88\u5927\uff0c\u56e0\u6b64\u76f4\u63a5\u61c9\u7528 LLM \u65bc EA \u4e26\u975e\u6613\u4e8b\u3002LLM \u4e5f\u6709\u53ef\u80fd\u7522\u751f\u96dc\u8a0a\u6a19\u7c64\uff0c\u9032\u800c\u8aa4\u5c0e\u6bd4\u5c0d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7d71\u4e00\u7684\u67b6\u69cb LLM4EA\uff0c\u4ee5\u6709\u6548\u5229\u7528 LLM \u9032\u884c EA\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u4e3b\u52d5\u5b78\u7fd2\u653f\u7b56\uff0c\u900f\u904e\u6839\u64da\u6574\u500b\u8de8 KG \u548c\u5167\u90e8 KG \u7d50\u69cb\uff0c\u512a\u5148\u8655\u7406\u6700\u6709\u50f9\u503c\u7684\u5be6\u9ad4\uff0c\u5f9e\u800c\u5927\u5e45\u6e1b\u5c11\u8a3b\u89e3\u7a7a\u9593\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u7121\u76e3\u7763\u6a19\u7c64\u7cbe\u7149\u5668\uff0c\u4ee5\u900f\u904e\u6df1\u5165\u7684\u6a5f\u7387\u63a8\u7406\u6301\u7e8c\u63d0\u5347\u6a19\u7c64\u7684\u6e96\u78ba\u6027\u3002\u6211\u5011\u6839\u64da\u57fa\u790e EA \u6a21\u578b\u7684\u56de\u994b\uff0c\u53cd\u8986\u6700\u4f73\u5316\u653f\u7b56\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86 LLM4EA \u5728\u56db\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u7684\u512a\u52e2\uff0c\u5305\u62ec\u5728\u6548\u80fd\u3001\u7a69\u5065\u6027\u548c\u6548\u7387\u65b9\u9762\u3002\u7a0b\u5f0f\u78bc\u53ef\u900f\u904e https://github.com/chensyCN/llm4ea_official \u53d6\u5f97\u3002", "author": "Shengyuan Chen et.al.", "authors": "Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Qing Li, Xiao Huang", "id": "2405.16806v2", "paper_url": "http://arxiv.org/abs/2405.16806v2", "repo": "null"}}