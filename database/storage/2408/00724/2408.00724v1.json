{"2408.00724": {"publish_time": "2024-08-01", "title": "An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models", "paper_summary": "The optimal training configurations of large language models (LLMs) with\nrespect to model sizes and compute budgets have been extensively studied. But\nhow to optimally configure LLMs during inference has not been explored in\nsufficient depth. We study compute-optimal inference: designing models and\ninference strategies that optimally trade off additional inference-time compute\nfor improved performance. As a first step towards understanding and designing\ncompute-optimal inference methods, we assessed the effectiveness and\ncomputational efficiency of multiple inference strategies such as Greedy\nSearch, Majority Voting, Best-of-N, Weighted Voting, and their variants on two\ndifferent Tree Search algorithms, involving different model sizes and\ncomputational budgets. We found that a smaller language model with a novel tree\nsearch algorithm typically achieves a Pareto-optimal trade-off. These results\nhighlight the potential benefits of deploying smaller models equipped with more\nsophisticated decoding algorithms in budget-constrained scenarios, e.g., on\nend-devices, to enhance problem-solving accuracy. For instance, we show that\nthe Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on\nMATH500 while using $2\\times$ less FLOPs. Our findings could potentially apply\nto any generation task with a well-defined measure of success.", "paper_summary_zh": "\u5c0d\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6700\u4f73\u8a13\u7df4\u7d44\u614b\uff0c\u7121\u8ad6\u662f\u6a21\u578b\u5927\u5c0f\u6216\u904b\u7b97\u9810\u7b97\uff0c\u90fd\u5df2\u5ee3\u6cdb\u7814\u7a76\u904e\u3002\u4f46\u5982\u4f55\u6700\u4f73\u7d44\u614b LLM \u5728\u63a8\u8ad6\u671f\u9593\u5c1a\u672a\u6df1\u5165\u63a2\u8a0e\u3002\u6211\u5011\u7814\u7a76\u8a08\u7b97\u6700\u4f73\u63a8\u8ad6\uff1a\u8a2d\u8a08\u6a21\u578b\u548c\u63a8\u8ad6\u7b56\u7565\uff0c\u6700\u4f73\u6298\u8877\u984d\u5916\u7684\u63a8\u8ad6\u6642\u9593\u8a08\u7b97\u4ee5\u63d0\u5347\u6548\u80fd\u3002\u4f5c\u70ba\u4e86\u89e3\u548c\u8a2d\u8a08\u8a08\u7b97\u6700\u4f73\u63a8\u8ad6\u65b9\u6cd5\u7684\u7b2c\u4e00\u6b65\uff0c\u6211\u5011\u8a55\u4f30\u591a\u7a2e\u63a8\u8ad6\u7b56\u7565\u7684\u6548\u80fd\u548c\u8a08\u7b97\u6548\u7387\uff0c\u4f8b\u5982\u8caa\u5a6a\u641c\u5c0b\u3001\u591a\u6578\u6c7a\u3001N \u4e2d\u6700\u4f73\u3001\u52a0\u6b0a\u6295\u7968\uff0c\u4ee5\u53ca\u5b83\u5011\u5728\u5169\u7a2e\u4e0d\u540c\u6a39\u72c0\u641c\u5c0b\u6f14\u7b97\u6cd5\u4e0a\u7684\u8b8a\u9ad4\uff0c\u6d89\u53ca\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u548c\u8a08\u7b97\u9810\u7b97\u3002\u6211\u5011\u767c\u73fe\uff0c\u5177\u5099\u65b0\u7a4e\u6a39\u72c0\u641c\u5c0b\u6f14\u7b97\u6cd5\u7684\u8f03\u5c0f\u8a9e\u8a00\u6a21\u578b\u901a\u5e38\u80fd\u9054\u6210\u5e15\u96f7\u6258\u6700\u4f73\u6298\u8877\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u5728\u9810\u7b97\u53d7\u9650\u7684\u60c5\u6cc1\u4e0b\uff0c\u90e8\u7f72\u914d\u5099\u66f4\u7cbe\u7dfb\u89e3\u78bc\u6f14\u7b97\u6cd5\u7684\u5c0f\u578b\u6a21\u578b\u7684\u6f5b\u5728\u597d\u8655\uff0c\u4f8b\u5982\u5728\u7d42\u7aef\u88dd\u7f6e\u4e0a\uff0c\u4ee5\u63d0\u5347\u554f\u984c\u89e3\u6c7a\u7684\u6e96\u78ba\u5ea6\u3002\u4f8b\u5982\uff0c\u6211\u5011\u986f\u793a Llemma-7B \u6a21\u578b\u5728 MATH500 \u4e0a\u80fd\u9054\u6210\u8207 Llemma-34B \u6a21\u578b\u7af6\u722d\u7684\u6e96\u78ba\u5ea6\uff0c\u540c\u6642\u4f7f\u7528\u5c11 $2\\times$ \u7684 FLOP\u3002\u6211\u5011\u7684\u767c\u73fe\u6f5b\u5728\u53ef\u61c9\u7528\u65bc\u4efb\u4f55\u5177\u6709\u660e\u78ba\u6210\u529f\u8861\u91cf\u6a19\u6e96\u7684\u7522\u751f\u4efb\u52d9\u3002", "author": "Yangzhen Wu et.al.", "authors": "Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang", "id": "2408.00724v1", "paper_url": "http://arxiv.org/abs/2408.00724v1", "repo": "null"}}