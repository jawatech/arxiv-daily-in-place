{"2410.22995": {"publish_time": "2024-10-30", "title": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning", "paper_summary": "Although previous research on large language models (LLMs) and large\nmulti-modal models (LMMs) has systematically explored mathematical\nproblem-solving (MPS) within visual contexts, the analysis of how these models\nprocess visual information during problem-solving remains insufficient. To\naddress this gap, we present VisAidMath, a benchmark for evaluating the MPS\nprocess related to visual information. We follow a rigorous data curation\npipeline involving both automated processes and manual annotations to ensure\ndata quality and reliability. Consequently, this benchmark includes 1,200\nchallenging problems from various mathematical branches, vision-aid\nformulations, and difficulty levels, collected from diverse sources such as\ntextbooks, examination papers, and Olympiad problems. Based on the proposed\nbenchmark, we conduct comprehensive evaluations on ten mainstream LLMs and\nLMMs, highlighting deficiencies in the visual-aided reasoning process. For\nexample, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning\ntask, even with a drop of 2 points when provided with golden visual aids.\nIn-depth analysis reveals that the main cause of deficiencies lies in\nhallucination regarding the implicit visual reasoning process, shedding light\non future research directions in the visual-aided MPS process.", "paper_summary_zh": "\u5118\u7ba1\u5148\u524d\u91dd\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u7684\u7814\u7a76\u5df2\u7cfb\u7d71\u6027\u5730\u63a2\u8a0e\u8996\u89ba\u60c5\u5883\u4e2d\u7684\u6578\u5b78\u554f\u984c\u89e3\u6c7a (MPS)\uff0c\u4f46\u5c0d\u65bc\u9019\u4e9b\u6a21\u578b\u5728\u554f\u984c\u89e3\u6c7a\u904e\u7a0b\u4e2d\u5982\u4f55\u8655\u7406\u8996\u89ba\u8cc7\u8a0a\u7684\u5206\u6790\u4ecd\u4e0d\u8db3\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa VisAidMath\uff0c\u4e00\u500b\u7528\u65bc\u8a55\u4f30\u8207\u8996\u89ba\u8cc7\u8a0a\u76f8\u95dc\u7684 MPS \u904e\u7a0b\u7684\u57fa\u6e96\u3002\u6211\u5011\u9075\u5faa\u56b4\u8b39\u7684\u8cc7\u6599\u6574\u7406\u6d41\u7a0b\uff0c\u6d89\u53ca\u81ea\u52d5\u5316\u8655\u7406\u548c\u624b\u52d5\u6a19\u8a3b\uff0c\u4ee5\u78ba\u4fdd\u8cc7\u6599\u54c1\u8cea\u548c\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u6b64\u57fa\u6e96\u5305\u542b 1,200 \u500b\u4f86\u81ea\u6559\u79d1\u66f8\u3001\u8003\u8a66\u5377\u548c\u5967\u6797\u5339\u4e9e\u7af6\u8cfd\u7b49\u4e0d\u540c\u4f86\u6e90\u7684\u5177\u6709\u6311\u6230\u6027\u7684\u554f\u984c\uff0c\u6db5\u84cb\u5404\u7a2e\u6578\u5b78\u5206\u652f\u3001\u8996\u89ba\u8f14\u52a9\u516c\u5f0f\u548c\u96e3\u5ea6\u7b49\u7d1a\u3002\u6839\u64da\u5efa\u8b70\u7684\u57fa\u6e96\uff0c\u6211\u5011\u5c0d\u5341\u500b\u4e3b\u6d41 LLM \u548c LMM \u9032\u884c\u5168\u9762\u8a55\u4f30\uff0c\u5f37\u8abf\u8996\u89ba\u8f14\u52a9\u63a8\u7406\u904e\u7a0b\u4e2d\u7684\u7f3a\u9677\u3002\u4f8b\u5982\uff0c\u5373\u4f7f\u5728\u63d0\u4f9b\u9ec3\u91d1\u8996\u89ba\u8f14\u52a9\u6642\u4e0b\u964d 2 \u500b\u767e\u5206\u9ede\uff0cGPT-4V \u5728\u8996\u89ba\u8f14\u52a9\u63a8\u7406\u4efb\u52d9\u4e2d\u50c5\u9054\u5230 45.33% \u7684\u6e96\u78ba\u5ea6\u3002\u6df1\u5165\u5206\u6790\u986f\u793a\uff0c\u7f3a\u9677\u7684\u4e3b\u8981\u539f\u56e0\u5728\u65bc\u5c0d\u96b1\u5f0f\u8996\u89ba\u63a8\u7406\u904e\u7a0b\u7684\u5e7b\u89ba\uff0c\u70ba\u8996\u89ba\u8f14\u52a9 MPS \u904e\u7a0b\u4e2d\u7684\u672a\u4f86\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u555f\u793a\u3002", "author": "Jingkun Ma et.al.", "authors": "Jingkun Ma, Runzhe Zhan, Derek F. Wong, Yang Li, Di Sun, Hou Pong Chan, Lidia S. Chao", "id": "2410.22995v1", "paper_url": "http://arxiv.org/abs/2410.22995v1", "repo": "null"}}