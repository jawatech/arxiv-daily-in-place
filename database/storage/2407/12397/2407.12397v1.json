{"2407.12397": {"publish_time": "2024-07-17", "title": "Mamba-PTQ: Outlier Channels in Recurrent Large Language Models", "paper_summary": "Modern recurrent layers are emerging as a promising path toward edge\ndeployment of foundation models, especially in the context of large language\nmodels (LLMs). Compressing the whole input sequence in a finite-dimensional\nrepresentation enables recurrent layers to model long-range dependencies while\nmaintaining a constant inference cost for each token and a fixed memory\nrequirement. However, the practical deployment of LLMs in resource-limited\nenvironments often requires further model compression, such as quantization and\npruning. While these techniques are well-established for attention-based\nmodels, their effects on recurrent layers remain underexplored.\n  In this preliminary work, we focus on post-training quantization for\nrecurrent LLMs and show that Mamba models exhibit the same pattern of outlier\nchannels observed in attention-based LLMs. We show that the reason for the\ndifficulty of quantizing SSMs is caused by activation outliers, similar to\nthose observed in transformer-based LLMs. We report baseline results for\npost-training quantization of Mamba that do not take into account the\nactivation outliers and suggest first steps for outlier-aware quantization.", "paper_summary_zh": "\u73fe\u4ee3\u905e\u8ff4\u5c64\u6b63\u6210\u70ba\u57fa\u790e\u6a21\u578b\u908a\u7de3\u90e8\u7f72\u7684\u6709\u524d\u666f\u9014\u5f91\uff0c\u7279\u5225\u662f\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u80cc\u666f\u4e0b\u3002\u5c07\u6574\u500b\u8f38\u5165\u5e8f\u5217\u58d3\u7e2e\u6210\u6709\u9650\u7dad\u8868\u793a\uff0c\u4f7f\u905e\u8ff4\u5c64\u80fd\u5920\u5efa\u6a21\u9577\u8ddd\u96e2\u4f9d\u8cf4\u95dc\u4fc2\uff0c\u540c\u6642\u70ba\u6bcf\u500b\u7b26\u865f\u548c\u56fa\u5b9a\u7684\u8a18\u61b6\u9ad4\u9700\u6c42\u7dad\u6301\u6046\u5b9a\u7684\u63a8\u8ad6\u6210\u672c\u3002\u7136\u800c\uff0c\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u74b0\u5883\u4e2d\u5be6\u969b\u90e8\u7f72 LLM \u901a\u5e38\u9700\u8981\u9032\u4e00\u6b65\u7684\u6a21\u578b\u58d3\u7e2e\uff0c\u4f8b\u5982\u91cf\u5316\u548c\u526a\u679d\u3002\u96d6\u7136\u9019\u4e9b\u6280\u8853\u5df2\u5728\u57fa\u65bc\u6ce8\u610f\u529b\u7684\u6a21\u578b\u4e2d\u5f97\u5230\u5145\u5206\u78ba\u7acb\uff0c\u4f46\u5b83\u5011\u5c0d\u905e\u8ff4\u5c64\u7684\u5f71\u97ff\u4ecd\u7136\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\n\u5728\u9019\u9805\u521d\u6b65\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u905e\u8ff4 LLM \u7684\u8a13\u7df4\u5f8c\u91cf\u5316\uff0c\u4e26\u5c55\u793a Mamba \u6a21\u578b\u5c55\u73fe\u51fa\u8207\u57fa\u65bc\u6ce8\u610f\u529b\u7684 LLM \u4e2d\u89c0\u5bdf\u5230\u7684\u7570\u5e38\u901a\u9053\u76f8\u540c\u7684\u6a21\u5f0f\u3002\u6211\u5011\u5c55\u793a\u4e86\u91cf\u5316 SSM \u7684\u96e3\u5ea6\u662f\u7531\u65bc\u6fc0\u6d3b\u7570\u5e38\u503c\u9020\u6210\u7684\uff0c\u9019\u8207\u5728\u57fa\u65bcTransformer\u7684 LLM \u4e2d\u89c0\u5bdf\u5230\u7684\u60c5\u6cc1\u985e\u4f3c\u3002\u6211\u5011\u5831\u544a\u4e86 Mamba \u8a13\u7df4\u5f8c\u91cf\u5316\u7684\u57fa\u6e96\u7d50\u679c\uff0c\u9019\u4e9b\u7d50\u679c\u6c92\u6709\u8003\u616e\u6fc0\u6d3b\u7570\u5e38\u503c\uff0c\u4e26\u5efa\u8b70\u4e86\u7570\u5e38\u611f\u77e5\u91cf\u5316\u7684\u7b2c\u4e00\u6b65\u3002", "author": "Alessandro Pierro et.al.", "authors": "Alessandro Pierro, Steven Abreu", "id": "2407.12397v1", "paper_url": "http://arxiv.org/abs/2407.12397v1", "repo": "null"}}