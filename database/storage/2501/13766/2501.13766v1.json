{"2501.13766": {"publish_time": "2025-01-23", "title": "UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level Mathematical Reasoning with Large Language Models", "paper_summary": "Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u6578\u5b78\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u9032\u5c55\uff0c\u9019\u51f8\u986f\u4e86\u5c0d\u5176\u80fd\u529b\u9032\u884c\u5168\u9762\u4e14\u516c\u5e73\u8a55\u4f30\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u6e96\u5f80\u5f80\u4e0d\u5920\uff0c\u8981\u4e48\u7f3a\u4e4f\u5c0d\u5927\u5b78\u7a0b\u5ea6\u6578\u5b78\u554f\u984c\u7684\u5ee3\u6cdb\u6db5\u84cb\uff0c\u8981\u4e48\u53ef\u80fd\u906d\u53d7\u6e2c\u8a66\u96c6\u6c61\u67d3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 UGMathBench\uff0c\u9019\u662f\u4e00\u500b\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u8a55\u4f30 LLM \u7684\u5927\u5b78\u7a0b\u5ea6\u6578\u5b78\u63a8\u7406\u7684\u591a\u6a23\u5316\u52d5\u614b\u57fa\u6e96\u3002UGMathBench \u5305\u542b 16 \u500b\u79d1\u76ee\u548c 111 \u500b\u4e3b\u984c\u7684 5,062 \u500b\u554f\u984c\uff0c\u5177\u6709 10 \u7a2e\u4e0d\u540c\u7684\u7b54\u6848\u985e\u578b\u3002\u6bcf\u500b\u554f\u984c\u5305\u62ec\u4e09\u500b\u96a8\u6a5f\u7248\u672c\uff0c\u4e26\u8a08\u5283\u96a8\u8457\u9818\u5148\u7684\u958b\u6e90 LLM \u5728 UGMathBench \u4e2d\u98fd\u548c\u800c\u767c\u5e03\u5176\u4ed6\u7248\u672c\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5169\u500b\u95dc\u9375\u6307\u6a19\uff1a\u6709\u6548\u6e96\u78ba\u7387 (EAcc)\uff0c\u5b83\u8861\u91cf\u5728\u6240\u6709\u4e09\u500b\u7248\u672c\u4e2d\u6b63\u78ba\u89e3\u6c7a\u554f\u984c\u7684\u767e\u5206\u6bd4\uff0c\u4ee5\u53ca\u63a8\u7406\u5dee\u8ddd\uff08$\\Delta$\uff09\uff0c\u5b83\u901a\u904e\u8a08\u7b97\u6240\u6709\u7248\u672c\u4e2d\u7684\u5e73\u5747\u6e96\u78ba\u7387\u548c EAcc \u4e4b\u9593\u7684\u5dee\u7570\u4f86\u8a55\u4f30\u63a8\u7406\u7a69\u5065\u6027\u3002\u6211\u5011\u5c0d 23 \u500b\u9818\u5148\u7684 LLM \u7684\u5ee3\u6cdb\u8a55\u4f30\u8868\u660e\uff0cOpenAI-o1-mini \u9054\u5230\u7684\u6700\u9ad8 EAcc \u70ba 56.3%\uff0c\u5728\u4e0d\u540c\u7684\u6a21\u578b\u4e2d\u89c0\u5bdf\u5230\u8f03\u5927\u7684 $\\Delta$ \u503c\u3002\u9019\u51f8\u986f\u4e86\u672a\u4f86\u7814\u7a76\u7684\u5fc5\u8981\u6027\uff0c\u65e8\u5728\u958b\u767c\u5177\u6709\u9ad8 EAcc \u548c $\\Delta = 0$ \u7684\u300c\u5927\u578b\u63a8\u7406\u6a21\u578b\u300d\u3002\u6211\u5011\u9810\u8a08 UGMathBench \u7684\u767c\u5e03\u53ca\u5176\u8a73\u7d30\u7684\u8a55\u4f30\u4ee3\u78bc\u5c07\u6210\u70ba\u63a8\u9032 LLM \u5728\u89e3\u6c7a\u6578\u5b78\u554f\u984c\u65b9\u9762\u7684\u767c\u5c55\u7684\u5bf6\u8cb4\u8cc7\u6e90\u3002", "author": "Xin Xu et.al.", "authors": "Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, Can Yang", "id": "2501.13766v1", "paper_url": "http://arxiv.org/abs/2501.13766v1", "repo": "null"}}