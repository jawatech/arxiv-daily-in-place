{"2502.12913": {"publish_time": "2025-02-18", "title": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning", "paper_summary": "Large Language Models (LLMs) fine-tuning technologies have achieved\nremarkable results. However, traditional LLM fine-tuning approaches face\nsignificant challenges: they require large Floating Point (FP) computation,\nraising privacy concerns when handling sensitive data, and are impractical for\nresource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT)\ntechniques reduce trainable parameters, their reliance on floating-point\narithmetic creates fundamental incompatibilities with edge hardware. In this\nwork, we introduce a novel framework for on-device LLM fine-tuning that\neliminates the need for floating-point operations in both inference and\ntraining, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer\nformat, which efficiently represents model parameters in integer format using\nshared exponents among parameter groups. When combined with LoRA-like adapters,\nthis enables fully integer-based fine-tuning that is both memory and compute\nefficient. We demonstrate that our approach achieves accuracy comparable to\nFP16-based fine-tuning while significantly reducing memory usage (50%).\nMoreover, compared to FP8, our method can reduce 5x power consumption and 11x\nchip area with same performance, making large-scale model adaptation feasible\non edge devices.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5fae\u8c03\u6280\u672f\u5df2\u53d6\u5f97\u663e\u8457\u6210\u679c\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684 LLM \u5fae\u8c03\u65b9\u6cd5\u9762\u4e34\u7740\u4e25\u5cfb\u7684\u6311\u6218\uff1a\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u6d6e\u70b9 (FP) \u8ba1\u7b97\uff0c\u5728\u5904\u7406\u654f\u611f\u6570\u636e\u65f6\u4f1a\u5f15\u53d1\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u4e14\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u800c\u8a00\u4e0d\u5207\u5b9e\u9645\u3002\u867d\u7136\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u6280\u672f\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u4f46\u5b83\u4eec\u5bf9\u6d6e\u70b9\u8fd0\u7b97\u7684\u4f9d\u8d56\u4e0e\u8fb9\u7f18\u786c\u4ef6\u4ea7\u751f\u4e86\u6839\u672c\u4e0a\u7684\u4e0d\u517c\u5bb9\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bbe\u5907\u4e0a LLM \u5fae\u8c03\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6d88\u9664\u4e86\u63a8\u7406\u548c\u8bad\u7ec3\u4e2d\u5bf9\u6d6e\u70b9\u8fd0\u7b97\u7684\u9700\u6c42\uff0c\u540d\u4e3a GSQ-Tuning\u3002\u5176\u6838\u5fc3\u662f\u7ec4\u5171\u4eab\u6307\u6570\u6574\u6570\u683c\u5f0f\uff0c\u8be5\u683c\u5f0f\u4f7f\u7528\u53c2\u6570\u7ec4\u4e4b\u95f4\u7684\u5171\u4eab\u6307\u6570\u4ee5\u6574\u6570\u683c\u5f0f\u6709\u6548\u5730\u8868\u793a\u6a21\u578b\u53c2\u6570\u3002\u5f53\u4e0e\u7c7b\u4f3c LoRA \u7684\u9002\u914d\u5668\u76f8\u7ed3\u5408\u65f6\uff0c\u8fd9\u5b9e\u73b0\u4e86\u5b8c\u5168\u57fa\u4e8e\u6574\u6570\u7684\u5fae\u8c03\uff0c\u65e2\u8282\u7701\u5185\u5b58\u53c8\u8282\u7701\u8ba1\u7b97\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u57fa\u4e8e FP16 \u7684\u5fae\u8c03\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\u91cf (50%)\u3002\u6b64\u5916\uff0c\u4e0e FP8 \u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u76f8\u540c\u7684\u6027\u80fd\u4e0b\u51cf\u5c11 5 \u500d\u7684\u529f\u8017\u548c 11 \u500d\u7684\u82af\u7247\u9762\u79ef\uff0c\u4ece\u800c\u4f7f\u5927\u89c4\u6a21\u6a21\u578b\u9002\u5e94\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6210\u4e3a\u53ef\u80fd\u3002", "author": "Sifan Zhou et.al.", "authors": "Sifan Zhou, Shuo Wang, Zhihang Yuan, Mingjia Shi, Yuzhang Shang, Dawei Yang", "id": "2502.12913v1", "paper_url": "http://arxiv.org/abs/2502.12913v1", "repo": "null"}}