{"2408.09856": {"publish_time": "2024-08-19", "title": "TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and Competition", "paper_summary": "While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have\neffectively addressed GPU memory constraints during fine-tuning, their\nperformance often falls short, especially in multidimensional task scenarios.\nTo address this issue, one straightforward solution is to introduce\ntask-specific LoRA modules as domain experts, leveraging the modeling of\nmultiple experts' capabilities and thus enhancing the general capability of\nmulti-task learning. Despite promising, these additional components often add\ncomplexity to the training and inference process, contravening the efficient\ncharacterization of PEFT designed for. Considering this, we introduce an\ninnovative PEFT method, TeamLoRA, consisting of a collaboration and competition\nmodule for experts, and thus achieving the right balance of effectiveness and\nefficiency: (i) For collaboration, a novel knowledge-sharing and -organizing\nmechanism is devised to appropriately reduce the scale of matrix operations,\nthereby boosting the training and inference speed. (ii) For competition, we\npropose leveraging a game-theoretic interaction mechanism for experts,\nencouraging experts to transfer their domain-specific knowledge while facing\ndiverse downstream tasks, and thus enhancing the performance. By doing so,\nTeamLoRA elegantly connects the experts as a \"Team\" with internal collaboration\nand competition, enabling a faster and more accurate PEFT paradigm for\nmulti-task learning. To validate the superiority of TeamLoRA, we curate a\ncomprehensive multi-task evaluation(CME) benchmark to thoroughly assess the\ncapability of multi-task learning. Experiments conducted on our CME and other\nbenchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project\nis available at https://github.com/Lin-Tianwei/TeamLoRA.", "paper_summary_zh": "<paragraph>\u96d6\u7136\u50cf LoRA \u9019\u6a23\u7684\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\u5728\u5fae\u8abf\u671f\u9593\u6709\u6548\u5730\u89e3\u6c7a\u4e86 GPU \u8a18\u61b6\u9ad4\u9650\u5236\uff0c\u4f46\u5176\u6548\u80fd\u901a\u5e38\u4e0d\u8db3\uff0c\u7279\u5225\u662f\u5728\u591a\u7dad\u5ea6\u4efb\u52d9\u60c5\u5883\u4e2d\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u4e00\u500b\u76f4\u63a5\u7684\u89e3\u6c7a\u65b9\u6848\u662f\u5f15\u5165\u7279\u5b9a\u65bc\u4efb\u52d9\u7684 LoRA \u6a21\u7d44\u4f5c\u70ba\u9818\u57df\u5c08\u5bb6\uff0c\u5229\u7528\u591a\u500b\u5c08\u5bb6\u7684\u80fd\u529b\u5efa\u6a21\uff0c\u5f9e\u800c\u589e\u5f37\u591a\u4efb\u52d9\u5b78\u7fd2\u7684\u4e00\u822c\u80fd\u529b\u3002\u5118\u7ba1\u6709\u524d\u666f\uff0c\u4f46\u9019\u4e9b\u984d\u5916\u7684\u7d44\u4ef6\u901a\u5e38\u6703\u589e\u52a0\u8a13\u7df4\u548c\u63a8\u8ad6\u904e\u7a0b\u7684\u8907\u96dc\u6027\uff0c\u9055\u53cd\u4e86 PEFT \u7684\u6709\u6548\u7279\u6027\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u5275\u65b0\u7684 PEFT \u65b9\u6cd5 TeamLoRA\uff0c\u5b83\u5305\u542b\u4e00\u500b\u5c08\u5bb6\u5354\u4f5c\u548c\u7af6\u722d\u6a21\u7d44\uff0c\u5f9e\u800c\u5be6\u73fe\u4e86\u6709\u6548\u6027\u548c\u6548\u7387\u7684\u9069\u7576\u5e73\u8861\uff1a(i) \u5c0d\u65bc\u5354\u4f5c\uff0c\u8a2d\u8a08\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u77e5\u8b58\u5171\u4eab\u548c\u7d44\u7e54\u6a5f\u5236\uff0c\u4ee5\u9069\u7576\u5730\u7e2e\u5c0f\u77e9\u9663\u904b\u7b97\u7684\u898f\u6a21\uff0c\u5f9e\u800c\u63d0\u9ad8\u8a13\u7df4\u548c\u63a8\u8ad6\u901f\u5ea6\u3002(ii) \u5c0d\u65bc\u7af6\u722d\uff0c\u6211\u5011\u5efa\u8b70\u5229\u7528\u535a\u5f08\u8ad6\u4e92\u52d5\u6a5f\u5236\u4f86\u6fc0\u52f5\u5c08\u5bb6\uff0c\u9f13\u52f5\u5c08\u5bb6\u5728\u9762\u5c0d\u4e0d\u540c\u7684\u4e0b\u6e38\u4efb\u52d9\u6642\u50b3\u905e\u5176\u7279\u5b9a\u9818\u57df\u7684\u77e5\u8b58\uff0c\u5f9e\u800c\u63d0\u5347\u6548\u80fd\u3002\u85c9\u7531\u9019\u9ebc\u505a\uff0cTeamLoRA \u512a\u96c5\u5730\u5c07\u5c08\u5bb6\u5011\u806f\u7e6b\u6210\u4e00\u500b\u5177\u6709\u5167\u90e8\u5354\u4f5c\u548c\u7af6\u722d\u7684\u300c\u5718\u968a\u300d\uff0c\u5be6\u73fe\u4e86\u66f4\u5feb\u901f\u3001\u66f4\u6e96\u78ba\u7684\u591a\u4efb\u52d9\u5b78\u7fd2 PEFT \u5178\u7bc4\u3002\u70ba\u4e86\u9a57\u8b49 TeamLoRA \u7684\u512a\u8d8a\u6027\uff0c\u6211\u5011\u7b56\u5283\u4e86\u4e00\u500b\u5168\u9762\u7684\u591a\u4efb\u52d9\u8a55\u4f30 (CME) \u57fa\u6e96\uff0c\u4ee5\u5fb9\u5e95\u8a55\u4f30\u591a\u4efb\u52d9\u5b78\u7fd2\u7684\u80fd\u529b\u3002\u5728\u6211\u5011\u7684 CME \u548c\u5176\u4ed6\u57fa\u6e96\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u8868\u660e\u4e86 TeamLoRA \u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002\u6211\u5011\u7684\u5c08\u6848\u53ef\u5728 https://github.com/Lin-Tianwei/TeamLoRA \u53d6\u5f97\u3002</paragraph>", "author": "Tianwei Lin et.al.", "authors": "Tianwei Lin, Jiang Liu, Wenqiao Zhang, Zhaocheng Li, Yang Dai, Haoyuan Li, Zhelun Yu, Wanggui He, Juncheng Li, Hao Jiang, Siliang Tang, Yueting Zhuang", "id": "2408.09856v1", "paper_url": "http://arxiv.org/abs/2408.09856v1", "repo": "https://github.com/lin-tianwei/teamlora"}}