{"2406.17888": {"publish_time": "2024-06-25", "title": "CTBench: A Comprehensive Benchmark for Evaluating Language Model Capabilities in Clinical Trial Design", "paper_summary": "CTBench is introduced as a benchmark to assess language models (LMs) in\naiding clinical study design. Given study-specific metadata, CTBench evaluates\nAI models' ability to determine the baseline features of a clinical trial (CT),\nwhich include demographic and relevant features collected at the trial's start\nfrom all participants. These baseline features, typically presented in CT\npublications (often as Table 1), are crucial for characterizing study cohorts\nand validating results. Baseline features, including confounders and\ncovariates, are also necessary for accurate treatment effect estimation in\nstudies involving observational data. CTBench consists of two datasets:\n\"CT-Repo,\" containing baseline features from 1,690 clinical trials sourced from\nclinicaltrials.gov, and \"CT-Pub,\" a subset of 100 trials with more\ncomprehensive baseline features gathered from relevant publications. Two\nLM-based evaluation methods are developed to compare the actual baseline\nfeature lists against LM-generated responses. \"ListMatch-LM\" and\n\"ListMatch-BERT\" use GPT-4o and BERT scores (at various thresholds),\nrespectively, for evaluation. To establish baseline results, advanced prompt\nengineering techniques using LLaMa3-70B-Instruct and GPT-4o in zero-shot and\nthree-shot learning settings are applied to generate potential baseline\nfeatures. The performance of GPT-4o as an evaluator is validated through\nhuman-in-the-loop evaluations on the CT-Pub dataset, where clinical experts\nconfirm matches between actual and LM-generated features. The results highlight\na promising direction with significant potential for improvement, positioning\nCTBench as a useful tool for advancing research on AI in CT design and\npotentially enhancing the efficacy and robustness of CTs.", "paper_summary_zh": "CTBench \u88ab\u5f15\u5165\u4f5c\u70ba\u4e00\u500b\u57fa\u6e96\uff0c\u7528\u65bc\u8a55\u4f30\u8a9e\u8a00\u6a21\u578b (LM) \u5728\u5e6b\u52a9\u81e8\u5e8a\u7814\u7a76\u8a2d\u8a08\u4e2d\u7684\u4f5c\u7528\u3002CTBench \u6703\u8a55\u4f30 AI \u6a21\u578b\u5728\u7d66\u5b9a\u7279\u5b9a\u7814\u7a76\u7684\u5143\u6578\u64da\u5f8c\uff0c\u6c7a\u5b9a\u81e8\u5e8a\u8a66\u9a57 (CT) \u57fa\u7dda\u7279\u5fb5\u7684\u80fd\u529b\uff0c\u5176\u4e2d\u5305\u62ec\u5728\u8a66\u9a57\u958b\u59cb\u6642\u5f9e\u6240\u6709\u53c3\u8207\u8005\u6536\u96c6\u7684\u4eba\u53e3\u7d71\u8a08\u548c\u76f8\u95dc\u7279\u5fb5\u3002\u9019\u4e9b\u57fa\u7dda\u7279\u5fb5\u901a\u5e38\u6703\u5728 CT \u51fa\u7248\u7269\u4e2d\u5448\u73fe\uff08\u901a\u5e38\u662f\u8868\u683c 1\uff09\uff0c\u5c0d\u65bc\u8868\u5fb5\u7814\u7a76\u7fa4\u7d44\u548c\u9a57\u8b49\u7d50\u679c\u81f3\u95dc\u91cd\u8981\u3002\u57fa\u7dda\u7279\u5fb5\uff08\u5305\u62ec\u6df7\u6dc6\u56e0\u5b50\u548c\u5354\u8b8a\u91cf\uff09\u5c0d\u65bc\u6e96\u78ba\u4f30\u8a08\u6d89\u53ca\u89c0\u5bdf\u6578\u64da\u7684\u7814\u7a76\u4e2d\u7684\u6cbb\u7642\u6548\u679c\u4e5f\u5f88\u6709\u5fc5\u8981\u3002CTBench \u5305\u542b\u5169\u500b\u6578\u64da\u96c6\uff1a\u300cCT-Repo\u300d\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea clinicaltrials.gov \u7684 1,690 \u500b\u81e8\u5e8a\u8a66\u9a57\u7684\u57fa\u7dda\u7279\u5fb5\uff0c\u4ee5\u53ca\u300cCT-Pub\u300d\uff0c\u4e00\u500b\u5305\u542b 100 \u500b\u8a66\u9a57\u7684\u5b50\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5f9e\u76f8\u95dc\u51fa\u7248\u7269\u6536\u96c6\u7684\u66f4\u5168\u9762\u7684\u57fa\u7dda\u7279\u5fb5\u3002\u958b\u767c\u4e86\u5169\u7a2e\u57fa\u65bc LM \u7684\u8a55\u4f30\u65b9\u6cd5\uff0c\u7528\u65bc\u6bd4\u8f03\u5be6\u969b\u7684\u57fa\u7dda\u7279\u5fb5\u5217\u8868\u548c LM \u751f\u6210\u7684\u56de\u61c9\u3002\u300cListMatch-LM\u300d\u548c\u300cListMatch-BERT\u300d\u5206\u5225\u4f7f\u7528 GPT-4o \u548c BERT \u5206\u6578\uff08\u5728\u5404\u7a2e\u95be\u503c\u4e0b\uff09\u9032\u884c\u8a55\u4f30\u3002\u70ba\u4e86\u5efa\u7acb\u57fa\u7dda\u7d50\u679c\uff0c\u5728\u96f6\u6b21\u5b78\u7fd2\u548c\u4e09\u6b21\u5b78\u7fd2\u8a2d\u7f6e\u4e2d\u4f7f\u7528 LLaMa3-70B-Instruct \u548c GPT-4o \u7684\u9032\u968e\u63d0\u793a\u5de5\u7a0b\u6280\u8853\uff0c\u7528\u65bc\u751f\u6210\u6f5b\u5728\u7684\u57fa\u7dda\u7279\u5fb5\u3002GPT-4o \u4f5c\u70ba\u8a55\u4f30\u5668\u7684\u6027\u80fd\u901a\u904e\u5728 CT-Pub \u6578\u64da\u96c6\u4e0a\u9032\u884c\u4eba\u6a5f\u4ea4\u4e92\u8a55\u4f30\u5f97\u5230\u9a57\u8b49\uff0c\u5728\u8a72\u8a55\u4f30\u4e2d\uff0c\u81e8\u5e8a\u5c08\u5bb6\u78ba\u8a8d\u5be6\u969b\u7279\u5fb5\u548c LM \u751f\u6210\u7684\u7279\u5fb5\u4e4b\u9593\u7684\u5339\u914d\u3002\u7d50\u679c\u7a81\u986f\u4e86\u4e00\u500b\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u5177\u6709\u986f\u8457\u7684\u6539\u9032\u6f5b\u529b\uff0c\u5c07 CTBench \u5b9a\u4f4d\u70ba\u4fc3\u9032 CT \u8a2d\u8a08\u4e2d AI \u7814\u7a76\u7684\u6709\u7528\u5de5\u5177\uff0c\u4e26\u6709\u53ef\u80fd\u63d0\u9ad8 CT \u7684\u529f\u6548\u548c\u7a69\u5065\u6027\u3002", "author": "Nafis Neehal et.al.", "authors": "Nafis Neehal, Bowen Wang, Shayom Debopadhaya, Soham Dan, Keerthiram Murugesan, Vibha Anand, Kristin P. Bennett", "id": "2406.17888v1", "paper_url": "http://arxiv.org/abs/2406.17888v1", "repo": "null"}}