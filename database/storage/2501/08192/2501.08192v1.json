{"2501.08192": {"publish_time": "2025-01-14", "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving", "paper_summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5ee3\u6cdb\u7528\u65bc\u5404\u7a2e\u61c9\u7528\u7a0b\u5f0f\u4e2d\uff0c\u4f46\u5176\u9f90\u5927\u7684\u904b\u7b97\u9700\u6c42\u5e36\u4f86\u56b4\u5cfb\u7684\u6311\u6230\uff0c\u7279\u5225\u662f\u5728 HBM \u983b\u5bec\u74f6\u9838\u548c\u88dd\u7f6e\u9593\u901a\u8a0a\u8ca0\u64d4\u65b9\u9762\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa PRESERVE\uff0c\u4e00\u500b\u5275\u65b0\u7684\u9810\u5148\u64f7\u53d6\u67b6\u69cb\uff0c\u65e8\u5728\u900f\u904e\u91cd\u758a\u6a21\u578b\u6b0a\u91cd\u7684\u8a18\u61b6\u9ad4\u8b80\u53d6\u548c KV \u5feb\u53d6\u8207\u96c6\u9ad4\u901a\u8a0a\u64cd\u4f5c\uff0c\u4f86\u6700\u4f73\u5316 LLM \u63a8\u8ad6\u3002\u900f\u904e\u5728\u5546\u7528 AI \u52a0\u901f\u5668\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u5c55\u793a\u4e86\u6700\u5148\u9032\u7684\u958b\u6e90 LLM \u53ef\u9054 1.6 \u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u5011\u57f7\u884c\u8a2d\u8a08\u7a7a\u9593\u63a2\u7d22\uff0c\u627e\u51fa\u5efa\u8b70\u65b9\u6cd5\u7684\u6700\u4f73\u786c\u9ad4\u914d\u7f6e\uff0c\u986f\u793a\u900f\u904e\u9078\u64c7\u6700\u4f73 L2 \u5feb\u53d6\u5927\u5c0f\uff0c\u6548\u80fd\u6bcf\u6210\u672c\u53ef\u9032\u4e00\u6b65\u63d0\u5347 1.25 \u500d\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a PRESERVE \u6709\u53ef\u80fd\u7de9\u89e3\u8a18\u61b6\u9ad4\u74f6\u9838\u548c\u901a\u8a0a\u8ca0\u64d4\uff0c\u63d0\u4f9b\u4e00\u500b\u89e3\u6c7a\u65b9\u6848\u4f86\u63d0\u5347 LLM \u63a8\u8ad6\u7cfb\u7d71\u7684\u6548\u80fd\u548c\u53ef\u64f4\u5145\u6027\u3002", "author": "Ahmet Caner Y\u00fcz\u00fcg\u00fcler et.al.", "authors": "Ahmet Caner Y\u00fcz\u00fcg\u00fcler, Jiawei Zhuang, Lukas Cavigelli", "id": "2501.08192v1", "paper_url": "http://arxiv.org/abs/2501.08192v1", "repo": "null"}}