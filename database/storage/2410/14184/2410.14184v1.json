{"2410.14184": {"publish_time": "2024-10-18", "title": "MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time", "paper_summary": "Large Language Models (LLMs) acquire extensive knowledge and remarkable\nabilities from extensive text corpora, making them powerful tools for various\napplications. To make LLMs more usable, aligning them with human preferences is\nessential. Existing alignment techniques, such as Reinforcement Learning from\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed\npredefined preferences directly within the model's parameters. These methods,\nhowever, often result in a static alignment that can not account for the\ndiversity of human preferences in practical applications. In response to this\nchallenge, we propose an effective method, \\textbf{MetaAlign}, which aims to\nhelp LLMs dynamically align with various explicit or implicit preferences\nspecified at inference time. Experimental results show that LLMs optimized on\nour meticulously constructed MetaAlign Dataset can effectively align with any\npreferences specified at the inference stage, validating the feasibility of\nMetaAlign. We hope that our work can provide some insights into the alignment\nof language models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5f9e\u5927\u91cf\u7684\u6587\u5b57\u8a9e\u6599\u5eab\u4e2d\u7372\u53d6\u5ee3\u6cdb\u7684\u77e5\u8b58\u548c\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u6210\u70ba\u5404\u7a2e\u61c9\u7528\u7a0b\u5e8f\u7684\u5f37\u5927\u5de5\u5177\u3002\u70ba\u4e86\u4f7f LLM \u66f4\u6613\u65bc\u4f7f\u7528\uff0c\u8b93\u5b83\u5011\u8207\u4eba\u985e\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u81f3\u95dc\u91cd\u8981\u3002\u73fe\u6709\u7684\u5c0d\u9f4a\u6280\u8853\uff0c\u4f8b\u5982\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u548c\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\uff0c\u901a\u5e38\u6703\u5c07\u9810\u5b9a\u7fa9\u7684\u504f\u597d\u76f4\u63a5\u5d4c\u5165\u6a21\u578b\u53c3\u6578\u4e2d\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u901a\u5e38\u6703\u5c0e\u81f4\u975c\u614b\u5c0d\u9f4a\uff0c\u7121\u6cd5\u8aaa\u660e\u5be6\u969b\u61c9\u7528\u4e2d\u4eba\u985e\u504f\u597d\u7684\u591a\u6a23\u6027\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5373 \\textbf{MetaAlign}\uff0c\u65e8\u5728\u5e6b\u52a9 LLM \u52d5\u614b\u5730\u8207\u5728\u63a8\u7406\u6642\u6307\u5b9a\u7684\u4e0d\u540c\u986f\u5f0f\u6216\u96b1\u5f0f\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5728\u6211\u5011\u7cbe\u5fc3\u69cb\u5efa\u7684 MetaAlign \u6578\u64da\u96c6\u4e0a\u512a\u5316\u7684 LLM \u53ef\u4ee5\u6709\u6548\u5730\u8207\u63a8\u7406\u968e\u6bb5\u6307\u5b9a\u7684\u4efb\u4f55\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u9a57\u8b49\u4e86 MetaAlign \u7684\u53ef\u884c\u6027\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684\u7814\u7a76\u6210\u679c\u80fd\u70ba\u8a9e\u8a00\u6a21\u578b\u7684\u5c0d\u9f4a\u63d0\u4f9b\u4e00\u4e9b\u898b\u89e3\u3002", "author": "Mozhi Zhang et.al.", "authors": "Mozhi Zhang, Pengyu Wang, Chenkun Tan, Mianqiu Huang, Dong Zhang, Yaqian Zhou, Xipeng Qiu", "id": "2410.14184v1", "paper_url": "http://arxiv.org/abs/2410.14184v1", "repo": "https://github.com/Jihuai-wpy/MetaAlign"}}