{"2410.11370": {"publish_time": "2024-10-15", "title": "Enhance Graph Alignment for Large Language Models", "paper_summary": "Graph-structured data is prevalent in the real world. Recently, due to the\npowerful emergent capabilities, Large Language Models (LLMs) have shown\npromising performance in modeling graphs. The key to effectively applying LLMs\non graphs is converting graph data into a format LLMs can comprehend.\nGraph-to-token approaches are popular in enabling LLMs to process graph\ninformation. They transform graphs into sequences of tokens and align them with\ntext tokens through instruction tuning, where self-supervised instruction\ntuning helps LLMs acquire general knowledge about graphs, and supervised\nfine-tuning specializes LLMs for the downstream tasks on graphs. Despite their\ninitial success, we find that existing methods have a misalignment between\nself-supervised tasks and supervised downstream tasks, resulting in negative\ntransfer from self-supervised fine-tuning to downstream tasks. To address these\nissues, we propose Graph Alignment Large Language Models (GALLM) to benefit\nfrom aligned task templates. In the self-supervised tuning stage, we introduce\na novel text matching task using templates aligned with downstream tasks. In\nthe task-specific tuning stage, we propose two category prompt methods that\nlearn supervision information from additional explanation with further aligned\ntemplates. Experimental evaluations on four datasets demonstrate substantial\nimprovements in supervised learning, multi-dataset generalizability, and\nparticularly in zero-shot capability, highlighting the model's potential as a\ngraph foundation model.", "paper_summary_zh": "\u5716\u5f62\u7d50\u69cb\u7684\u8cc7\u6599\u5728\u73fe\u5be6\u4e16\u754c\u4e2d\u5f88\u5e38\u898b\u3002\u6700\u8fd1\uff0c\u7531\u65bc\u5f37\u5927\u7684\u65b0\u8208\u80fd\u529b\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5716\u5f62\u5efa\u6a21\u65b9\u9762\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u6548\u80fd\u3002\u6709\u6548\u5c07 LLM \u61c9\u7528\u65bc\u5716\u5f62\u7684\u95dc\u9375\u662f\u5c07\u5716\u5f62\u8cc7\u6599\u8f49\u63db\u6210 LLM \u53ef\u4ee5\u7406\u89e3\u7684\u683c\u5f0f\u3002\u5716\u5f62\u5230\u6a19\u8a18\u7684\u65b9\u6cd5\u5f88\u6d41\u884c\uff0c\u8b93 LLM \u53ef\u4ee5\u8655\u7406\u5716\u5f62\u8cc7\u8a0a\u3002\u5b83\u5011\u5c07\u5716\u5f62\u8f49\u63db\u6210\u6a19\u8a18\u5e8f\u5217\uff0c\u4e26\u900f\u904e\u6307\u4ee4\u8abf\u6574\u8207\u6587\u5b57\u6a19\u8a18\u5c0d\u9f4a\uff0c\u5176\u4e2d\u81ea\u6211\u76e3\u7763\u7684\u6307\u4ee4\u8abf\u6574\u6709\u52a9\u65bc LLM \u7372\u5f97\u95dc\u65bc\u5716\u5f62\u7684\u5e38\u8b58\uff0c\u800c\u76e3\u7763\u5fae\u8abf\u5247\u5c08\u9580\u91dd\u5c0d\u5716\u5f62\u4e0a\u7684\u4e0b\u6e38\u4efb\u52d9\u8abf\u6574 LLM\u3002\u5118\u7ba1\u5b83\u5011\u6700\u521d\u5f88\u6210\u529f\uff0c\u6211\u5011\u767c\u73fe\u73fe\u6709\u65b9\u6cd5\u5728\u81ea\u6211\u76e3\u7763\u4efb\u52d9\u548c\u76e3\u7763\u4e0b\u6e38\u4efb\u52d9\u4e4b\u9593\u5b58\u5728\u932f\u4f4d\uff0c\u5c0e\u81f4\u81ea\u6211\u76e3\u7763\u5fae\u8abf\u5c0d\u4e0b\u6e38\u4efb\u52d9\u7522\u751f\u8ca0\u9762\u5f71\u97ff\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u5716\u5f62\u5c0d\u9f4a\u5927\u578b\u8a9e\u8a00\u6a21\u578b (GALLM) \u4ee5\u5f9e\u5c0d\u9f4a\u7684\u4efb\u52d9\u7bc4\u672c\u4e2d\u53d7\u76ca\u3002\u5728\u81ea\u6211\u76e3\u7763\u8abf\u6574\u968e\u6bb5\uff0c\u6211\u5011\u4f7f\u7528\u8207\u4e0b\u6e38\u4efb\u52d9\u5c0d\u9f4a\u7684\u7bc4\u672c\uff0c\u5f15\u5165\u4e00\u500b\u65b0\u7a4e\u7684\u6587\u5b57\u6bd4\u5c0d\u4efb\u52d9\u3002\u5728\u7279\u5b9a\u4efb\u52d9\u7684\u8abf\u6574\u968e\u6bb5\uff0c\u6211\u5011\u63d0\u51fa\u5169\u7a2e\u985e\u5225\u63d0\u793a\u65b9\u6cd5\uff0c\u5f9e\u9032\u4e00\u6b65\u5c0d\u9f4a\u7bc4\u672c\u7684\u984d\u5916\u8aaa\u660e\u4e2d\u5b78\u7fd2\u76e3\u7763\u8cc7\u8a0a\u3002\u5728\u56db\u500b\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u8a55\u4f30\u8b49\u660e\u4e86\u76e3\u7763\u5f0f\u5b78\u7fd2\u3001\u591a\u8cc7\u6599\u96c6\u7684\u6982\u62ec\u6027\uff0c\u7279\u5225\u662f\u5728\u96f6\u6b21\u5b78\u7fd2\u80fd\u529b\u65b9\u9762\u6709\u986f\u8457\u7684\u9032\u6b65\uff0c\u7a81\u986f\u4e86\u8a72\u6a21\u578b\u4f5c\u70ba\u5716\u5f62\u57fa\u790e\u6a21\u578b\u7684\u6f5b\u529b\u3002", "author": "Haitong Luo et.al.", "authors": "Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, Yujun Zhang", "id": "2410.11370v1", "paper_url": "http://arxiv.org/abs/2410.11370v1", "repo": "null"}}