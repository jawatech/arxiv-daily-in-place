{"2408.14776": {"publish_time": "2024-08-27", "title": "MROVSeg: Breaking the Resolution Curse of Vision-Language Models in Open-Vocabulary Semantic Segmentation", "paper_summary": "Open-vocabulary semantic segmentation aims to segment and recognize\nsemantically meaningful regions based on text-based descriptions during\ninference. A typical solution to address this task is to leverage powerful\nvision-language models (VLMs), such as CLIP, to bridge the gap between open-\nand close-vocabulary recognition. As VLMs are usually pretrained with\nlow-resolution images (e.g. $224\\times224$), most previous methods operate only\non downscaled images. We question this design as low resolution features often\nfail to preserve fine details. Although employing additional image backbones\nfor high-resolution inputs can mitigate this issue, it may also introduce\nsignificant computation overhead. Therefore, we propose MROVSeg, a\nmulti-resolution training framework for open-vocabulary semantic segmentation\nwith a single pretrained CLIP backbone, that uses sliding windows to slice the\nhigh-resolution input into uniform patches, each matching the input size of the\nwell-trained image encoder. Its key components include a Multi-Res Adapter,\nwhich restores the spatial geometry and grasps local-global correspondences\nacross patches by learnable convolutional and scale attention layers. To\nachieve accurate segmentation, we introduce Multi-grained Masked Attention\nscheme to aggregate multi-grained semantics by performing cross-attention\nbetween object queries and multi-resolution CLIP features within the region of\ninterests. Through comprehensive experiments, we demonstrate the superiority of\nMROVSeg on well-established open-vocabulary semantic segmentation benchmarks,\nparticularly for high-resolution inputs, establishing new standards for\nopen-vocabulary semantic segmentation.", "paper_summary_zh": "\u958b\u653e\u8a5e\u5f59\u8a9e\u610f\u5206\u5272\u65e8\u5728\u6839\u64da\u63a8\u7406\u671f\u9593\u7684\u6587\u5b57\u63cf\u8ff0\uff0c\u5206\u5272\u548c\u8fa8\u8b58\u8a9e\u610f\u6709\u610f\u7fa9\u7684\u5340\u57df\u3002\u89e3\u6c7a\u6b64\u4efb\u52d9\u7684\u5178\u578b\u65b9\u6cd5\u662f\u5229\u7528\u5f37\u5927\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\uff0c\u4f8b\u5982 CLIP\uff0c\u4f86\u5f4c\u5408\u958b\u653e\u548c\u5c01\u9589\u8a5e\u5f59\u8fa8\u8b58\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u7531\u65bc VLM \u901a\u5e38\u4f7f\u7528\u4f4e\u89e3\u6790\u5ea6\u5f71\u50cf (\u4f8b\u5982 $224\\times224$) \u9032\u884c\u9810\u8a13\u7df4\uff0c\u56e0\u6b64\u5927\u591a\u6578\u5148\u524d\u7684\u65b9\u6cd5\u50c5\u5728\u7e2e\u5c0f\u7684\u5f71\u50cf\u4e0a\u57f7\u884c\u3002\u6211\u5011\u8cea\u7591\u6b64\u8a2d\u8a08\uff0c\u56e0\u70ba\u4f4e\u89e3\u6790\u5ea6\u7279\u5fb5\u901a\u5e38\u7121\u6cd5\u4fdd\u7559\u7cbe\u7d30\u7684\u7d30\u7bc0\u3002\u5118\u7ba1\u63a1\u7528\u984d\u5916\u7684\u5f71\u50cf\u4e3b\u5e79\u9032\u884c\u9ad8\u89e3\u6790\u5ea6\u8f38\u5165\u53ef\u4ee5\u6e1b\u8f15\u6b64\u554f\u984c\uff0c\u4f46\u5b83\u4e5f\u53ef\u80fd\u6703\u5f15\u5165\u5927\u91cf\u7684\u904b\u7b97\u8ca0\u64d4\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa MROVSeg\uff0c\u9019\u662f\u4e00\u500b\u55ae\u4e00\u9810\u8a13\u7df4 CLIP \u4e3b\u5e79\u7684\u958b\u653e\u8a5e\u5f59\u8a9e\u610f\u5206\u5272\u591a\u89e3\u6790\u5ea6\u8a13\u7df4\u67b6\u69cb\uff0c\u5b83\u4f7f\u7528\u6ed1\u52d5\u8996\u7a97\u5c07\u9ad8\u89e3\u6790\u5ea6\u8f38\u5165\u5207\u6210\u5747\u52fb\u7684\u5340\u584a\uff0c\u6bcf\u500b\u5340\u584a\u90fd\u8207\u8a13\u7df4\u826f\u597d\u7684\u5f71\u50cf\u7de8\u78bc\u5668\u7684\u8f38\u5165\u5927\u5c0f\u76f8\u7b26\u3002\u5b83\u7684\u95dc\u9375\u7d44\u6210\u90e8\u5206\u5305\u62ec\u591a\u89e3\u6790\u5ea6\u9069\u914d\u5668\uff0c\u5b83\u900f\u904e\u53ef\u5b78\u7fd2\u7684\u5377\u7a4d\u548c\u7e2e\u653e\u6ce8\u610f\u5c64\uff0c\u6062\u5fa9\u7a7a\u9593\u5e7e\u4f55\u5f62\u72c0\u4e26\u638c\u63e1\u5340\u584a\u9593\u7684\u5c40\u90e8\u548c\u5168\u5c40\u5c0d\u61c9\u95dc\u4fc2\u3002\u70ba\u4e86\u9054\u6210\u7cbe\u78ba\u7684\u5206\u5272\uff0c\u6211\u5011\u5f15\u5165\u4e86\u591a\u7c92\u5ea6\u906e\u7f69\u6ce8\u610f\u6a5f\u5236\uff0c\u900f\u904e\u5728\u8208\u8da3\u5340\u57df\u5167\u57f7\u884c\u7269\u4ef6\u67e5\u8a62\u548c\u591a\u89e3\u6790\u5ea6 CLIP \u7279\u5fb5\u4e4b\u9593\u7684\u4ea4\u53c9\u6ce8\u610f\uff0c\u4f86\u5f59\u7e3d\u591a\u7c92\u5ea6\u8a9e\u610f\u3002\u900f\u904e\u5168\u9762\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86 MROVSeg \u5728\u5efa\u7acb\u826f\u597d\u7684\u958b\u653e\u8a5e\u5f59\u8a9e\u610f\u5206\u5272\u57fa\u6e96\u4e0a\u7684\u512a\u8d8a\u6027\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u9ad8\u89e3\u6790\u5ea6\u8f38\u5165\uff0c\u70ba\u958b\u653e\u8a5e\u5f59\u8a9e\u610f\u5206\u5272\u5efa\u7acb\u4e86\u65b0\u7684\u6a19\u6e96\u3002", "author": "Yuanbing Zhu et.al.", "authors": "Yuanbing Zhu, Bingke Zhu, Zhen Chen, Huan Xu, Ming Tang, Jinqiao Wang", "id": "2408.14776v1", "paper_url": "http://arxiv.org/abs/2408.14776v1", "repo": "null"}}