{"2501.13491": {"publish_time": "2025-01-23", "title": "RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles", "paper_summary": "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps://anonymous.4open.science/r/remember-B0B8/.", "paper_summary_zh": "\u6211\u5011\u5f15\u5165\u4e86\u81ea\u53c3\u7167\u56e0\u679c\u5faa\u74b0 (\u7c21\u7a31 RECALL) \u7684\u6982\u5ff5\uff0c\u9019\u662f\u4e00\u7a2e\u6a5f\u5236\uff0c\u53ef\u8b93\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7e5e\u904e\u55ae\u5411\u56e0\u679c\u95dc\u4fc2\u7684\u9650\u5236\uff0c\u800c\u55ae\u5411\u56e0\u679c\u95dc\u4fc2\u662f\u5c0e\u81f4\u9006\u8f49\u8a5b\u5492\u73fe\u8c61\u7684\u57fa\u790e\u3002\u7576 LLM \u53d7\u5230\u9806\u5e8f\u8cc7\u6599\u63d0\u793a\u6642\uff0c\u5b83\u7d93\u5e38\u7121\u6cd5\u56de\u61b6\u524d\u9762\u7684\u5167\u5bb9\u3002\u4f8b\u5982\uff0c\u7576\u6211\u5011\u8981\u6c42 LLM \u56de\u61b6\u7f8e\u570b\u570b\u6b4c\u4e2d\u300cO say does that star-spangled banner yet wave\u300d\u524d\u4e00\u53e5\u6642\uff0c\u5b83\u7d93\u5e38\u7121\u6cd5\u6b63\u78ba\u5730\u56de\u7b54\u300cGave proof through the night that our flag was still there\u300d\u2014\u2014\u9019\u662f\u56e0\u70ba\u9006\u8f49\u8a5b\u5492\u3002\u9019\u6703\u767c\u751f\uff0c\u662f\u56e0\u70ba\u50cf ChatGPT \u548c Llama \u9019\u6a23\u7684\u8a9e\u8a00\u6a21\u578b\u6703\u6839\u64da\u524d\u9762\u7684\u8a5e\u5f59\u7522\u751f\u6587\u5b57\uff0c\u9700\u8981\u4ee5\u4e00\u81f4\u7684\u8a5e\u5f59\u9806\u5e8f\u5b78\u7fd2\u548c\u8907\u88fd\u4e8b\u5be6\u3002\u96d6\u7136\u9006\u8f49\u8a5b\u5492\u901a\u5e38\u88ab\u8996\u70ba\u4e00\u7a2e\u9650\u5236\uff0c\u4f46\u6211\u5011\u63d0\u4f9b\u4e86\u53e6\u4e00\u7a2e\u89c0\u9ede\u7684\u8b49\u64da\uff1a\u5728\u5be6\u52d9\u4e0a\uff0c\u5b83\u4e26\u975e\u7e3d\u662f\u969c\u7919\u3002\u6211\u5011\u767c\u73fe RECALL \u662f\u7531\u6211\u5011\u6307\u5b9a\u7684\u5faa\u74b0\u8a5e\u5f59\u63a8\u52d5\u7684\uff0c\u5faa\u74b0\u8a5e\u5f59\u662f\u5c07\u8a13\u7df4\u8cc7\u6599\u7684\u4e0d\u540c\u90e8\u5206\u9023\u63a5\u8d77\u4f86\u7684\u5e8f\u5217\uff0c\u53ef\u5f9e\u5f8c\u7e8c\u8a5e\u5f59\u4e2d\u56de\u61b6\u524d\u9762\u7684\u8a5e\u5f59\u3002\u900f\u904e\u56b4\u8b39\u7684\u6a5f\u7387\u5f62\u5f0f\u5316\u548c\u53d7\u63a7\u5be6\u9a57\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5b83\u5011\u5f15\u767c\u7684\u5faa\u74b0\u5982\u4f55\u5f71\u97ff\u6a21\u578b\u8907\u88fd\u8cc7\u8a0a\u7684\u80fd\u529b\u3002\u70ba\u4e86\u4fc3\u9032\u91cd\u73fe\u6027\uff0c\u6211\u5011\u5728 https://anonymous.4open.science/r/remember-B0B8/ \u63d0\u4f9b\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u5be6\u9a57\u8a73\u7d30\u8cc7\u8a0a\u3002", "author": "Munachiso Nwadike et.al.", "authors": "Munachiso Nwadike, Zangir Iklassov, Toluwani Aremu, Tatsuya Hiraoka, Velibor Bojkovic, Benjamin Heinzerling, Hilal Alqaubeh, Martin Tak\u00e1\u010d, Kentaro Inui", "id": "2501.13491v1", "paper_url": "http://arxiv.org/abs/2501.13491v1", "repo": "null"}}