{"2501.12599": {"publish_time": "2025-01-22", "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs", "paper_summary": "Language model pretraining with next token prediction has proved effective\nfor scaling compute but is limited to the amount of available training data.\nScaling reinforcement learning (RL) unlocks a new axis for the continued\nimprovement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with\nrewards. However, prior published work has not produced competitive results. In\nlight of this, we report on the training practice of Kimi k1.5, our latest\nmulti-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling\nand improved policy optimization methods are key ingredients of our approach,\nwhich establishes a simplistic, effective RL framework without relying on more\ncomplex techniques such as Monte Carlo tree search, value functions, and\nprocess reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,\n96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching\nOpenAI's o1. Moreover, we present effective long2short methods that use\nlong-CoT techniques to improve short-CoT models, yielding state-of-the-art\nshort-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on\nLiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and\nClaude Sonnet 3.5 by a large margin (up to +550%).", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u52a0\u4e0a\u4e0b\u4e00\u500b\u7b26\u865f\u9810\u6e2c\u5df2\u8b49\u660e\u5c0d\u65bc\u64f4\u5145\u904b\u7b97\u6709\u6548\uff0c\u4f46\u53d7\u9650\u65bc\u53ef\u7528\u7684\u8a13\u7df4\u8cc7\u6599\u91cf\u3002\u64f4\u5145\u5f37\u5316\u5b78\u7fd2 (RL) \u70ba\u6301\u7e8c\u6539\u5584\u4eba\u5de5\u667a\u6167\u89e3\u9396\u4e86\u4e00\u500b\u65b0\u8ef8\uff0c\u627f\u8afe\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u80fd\u900f\u904e\u5b78\u7fd2\u63a2\u7d22\u734e\u52f5\u4f86\u64f4\u5145\u4ed6\u5011\u7684\u8a13\u7df4\u8cc7\u6599\u3002\u7136\u800c\uff0c\u4e4b\u524d\u767c\u5e03\u7684\u4f5c\u54c1\u5c1a\u672a\u7522\u751f\u6709\u7af6\u722d\u529b\u7684\u7d50\u679c\u3002\u6709\u9451\u65bc\u6b64\uff0c\u6211\u5011\u56de\u5831 Kimi k1.5 \u7684\u8a13\u7df4\u5be6\u52d9\uff0c\u9019\u662f\u6211\u5011\u6700\u65b0\u7684\u591a\u6a21\u614b LLM\uff0c\u4f7f\u7528 RL \u8a13\u7df4\uff0c\u5305\u62ec\u5176 RL \u8a13\u7df4\u6280\u5de7\u3001\u591a\u6a21\u614b\u8cc7\u6599\u98df\u8b5c\u548c\u57fa\u790e\u67b6\u69cb\u6700\u4f73\u5316\u3002\u9577\u8108\u7d61\u64f4\u5145\u548c\u6539\u5584\u7684\u7b56\u7565\u6700\u4f73\u5316\u65b9\u6cd5\u662f\u6211\u5011\u65b9\u6cd5\u4e2d\u7684\u95dc\u9375\u8981\u7d20\uff0c\u5efa\u7acb\u4e86\u4e00\u500b\u7c21\u6f54\u3001\u6709\u6548\u7684 RL \u67b6\u69cb\uff0c\u800c\u4e0d\u4f9d\u8cf4\u65bc\u66f4\u8907\u96dc\u7684\u6280\u5de7\uff0c\u4f8b\u5982\u8499\u5730\u5361\u7f85\u6a39\u72c0\u641c\u5c0b\u3001\u50f9\u503c\u51fd\u6578\u548c\u8655\u7406\u734e\u52f5\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u7684\u7cfb\u7d71\u5728\u591a\u500b\u57fa\u6e96\u548c\u6a21\u5f0f\u4e2d\u9054\u6210\u6700\u5148\u9032\u7684\u63a8\u7406\u6548\u80fd\u2014\u2014\u4f8b\u5982\uff0cAIME \u4e2d\u7684 77.5\u3001MATH 500 \u4e2d\u7684 96.2\u3001Codeforces \u4e2d\u7684 94 \u767e\u5206\u4f4d\u3001MathVista \u4e2d\u7684 74.9\u2014\u2014\u5339\u914d OpenAI \u7684 o1\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u6709\u6548\u7684 long2short \u65b9\u6cd5\uff0c\u4f7f\u7528 long-CoT \u6280\u5de7\u4f86\u6539\u5584 short-CoT \u6a21\u578b\uff0c\u7522\u751f\u6700\u5148\u9032\u7684 short-CoT \u63a8\u7406\u7d50\u679c\u2014\u2014\u4f8b\u5982\uff0cAIME \u4e2d\u7684 60.8\u3001MATH500 \u4e2d\u7684 94.6\u3001LiveCodeBench \u4e2d\u7684 47.3\u2014\u2014\u5927\u5e45\u8d85\u8d8a\u73fe\u6709\u7684 short-CoT \u6a21\u578b\uff0c\u4f8b\u5982 GPT-4o \u548c Claude Sonnet 3.5\uff08\u9ad8\u9054 +550%\uff09\u3002", "author": "Kimi Team et.al.", "authors": "Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang", "id": "2501.12599v1", "paper_url": "http://arxiv.org/abs/2501.12599v1", "repo": "null"}}