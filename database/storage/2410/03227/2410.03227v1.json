{"2410.03227": {"publish_time": "2024-10-04", "title": "ALR$^2$: A Retrieve-then-Reason Framework for Long-context Question Answering", "paper_summary": "The context window of large language models (LLMs) has been extended\nsignificantly in recent years. However, while the context length that the LLM\ncan process has grown, the capability of the model to accurately reason over\nthat context degrades noticeably. This occurs because modern LLMs often become\noverwhelmed by the vast amount of information in the context; when answering\nquestions, the model must identify and reason over relevant evidence sparsely\ndistributed throughout the text. To alleviate the challenge of long-context\nreasoning, we develop a retrieve-then-reason framework, enabling LLMs to reason\nover relevant evidence collected during an intermediate retrieval step. We find\nthat modern LLMs struggle to accurately retrieve relevant facts and instead,\noften hallucinate \"retrieved facts\", resulting in flawed reasoning and the\nproduction of incorrect answers. To address these issues, we introduce ALR$^2$,\na method that augments the long-context reasoning capability of LLMs via an\nexplicit two-stage procedure, i.e., aligning LLMs with the objectives of both\nretrieval and reasoning. We demonstrate the efficacy of ALR$^2$ for mitigating\nperformance degradation in long-context reasoning tasks. Through extensive\nexperiments on long-context QA benchmarks, we find our method to outperform\ncompetitive baselines by large margins, achieving at least 8.4 and 7.9 EM gains\non the long-context versions of HotpotQA and SQuAD datasets, respectively.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u4e0a\u4e0b\u6587\u8996\u7a97\u5df2\u986f\u8457\u64f4\u5c55\u3002\u7136\u800c\uff0c\u5118\u7ba1 LLM \u53ef\u8655\u7406\u7684\u4e0a\u4e0b\u6587\u9577\u5ea6\u5df2\u589e\u52a0\uff0c\u4f46\u6a21\u578b\u6e96\u78ba\u63a8\u7406\u8a72\u4e0a\u4e0b\u6587\u7684\u6548\u80fd\u537b\u660e\u986f\u4e0b\u964d\u3002\u9019\u662f\u56e0\u70ba\u73fe\u4ee3 LLM \u7d93\u5e38\u88ab\u5927\u91cf\u4e0a\u4e0b\u6587\u8cc7\u8a0a\u6df9\u6c92\uff1b\u5728\u56de\u7b54\u554f\u984c\u6642\uff0c\u6a21\u578b\u5fc5\u9808\u8b58\u5225\u4e26\u63a8\u8ad6\u5206\u6563\u5728\u6574\u500b\u6587\u672c\u4e2d\u7684\u76f8\u95dc\u8b49\u64da\u3002\u70ba\u4e86\u7de9\u89e3\u9577\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6311\u6230\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u5148\u64f7\u53d6\u518d\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4f7f LLM \u80fd\u5920\u5c0d\u5728\u4e2d\u9593\u64f7\u53d6\u6b65\u9a5f\u4e2d\u6536\u96c6\u5230\u7684\u76f8\u95dc\u8b49\u64da\u9032\u884c\u63a8\u7406\u3002\u6211\u5011\u767c\u73fe\uff0c\u73fe\u4ee3 LLM \u96e3\u4ee5\u6e96\u78ba\u64f7\u53d6\u76f8\u95dc\u4e8b\u5be6\uff0c\u800c\u7d93\u5e38\u6703\u51fa\u73fe\u300c\u64f7\u53d6\u4e8b\u5be6\u300d\u7684\u5e7b\u89ba\uff0c\u5c0e\u81f4\u63a8\u7406\u6709\u7f3a\u9677\u4e26\u7522\u751f\u4e0d\u6b63\u78ba\u7684\u7b54\u6848\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 ALR$^2$\uff0c\u9019\u662f\u4e00\u7a2e\u900f\u904e\u660e\u78ba\u7684\u5169\u968e\u6bb5\u7a0b\u5e8f\u4f86\u589e\u5f37 LLM \u9577\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5373\u8b93 LLM \u8207\u64f7\u53d6\u548c\u63a8\u7406\u7684\u76ee\u6a19\u4fdd\u6301\u4e00\u81f4\u3002\u6211\u5011\u5c55\u793a\u4e86 ALR$^2$ \u5728\u6e1b\u8f15\u9577\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52d9\u4e2d\u6548\u80fd\u4e0b\u964d\u65b9\u9762\u7684\u6548\u529b\u3002\u900f\u904e\u5c0d\u9577\u4e0a\u4e0b\u6587 QA \u57fa\u6e96\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u767c\u73fe\u6211\u5011\u7684\u6a21\u578b\u5927\u5e45\u512a\u65bc\u7af6\u722d\u57fa\u6e96\uff0c\u5206\u5225\u5728 HotpotQA \u548c SQuAD \u8cc7\u6599\u96c6\u7684\u9577\u4e0a\u4e0b\u6587\u7248\u672c\u4e2d\u7372\u5f97\u81f3\u5c11 8.4 \u548c 7.9 EM \u7684\u589e\u76ca\u3002", "author": "Huayang Li et.al.", "authors": "Huayang Li, Pat Verga, Priyanka Sen, Bowen Yang, Vijay Viswanathan, Patrick Lewis, Taro Watanabe, Yixuan Su", "id": "2410.03227v1", "paper_url": "http://arxiv.org/abs/2410.03227v1", "repo": "null"}}