{"2412.17739": {"publish_time": "2024-12-23", "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization", "paper_summary": "Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While existing works mainly\naddress RoPE's limitations within attention mechanism, this paper provides an\nanalysis across nearly all parts of LMs, uncovering their adverse effects on\nlength generalization for RoPE-based attention. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectral damage caused by: 1) linear layers and activation\nfunctions outside of attention; 2) insufficiently trained frequency components\nbrought by time-domain truncation. Building on our observations, we propose\nFourier Position Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs Fourier Series and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales show that, within varying context\nwindows, FoPE can maintain a more stable perplexity and a more consistent\naccuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several\nanalyses and ablations bring further support to our method and theoretical\nmodeling.", "paper_summary_zh": "\u501f\u7531\u6539\u826f\u65cb\u8f49\u4f4d\u7f6e\u5d4c\u5165 (RoPE)\uff0c\u9032\u800c\u5ef6\u4f38\u8a9e\u8a00\u6a21\u578b (LM) \u7684\u5167\u5bb9\u9577\u5ea6\uff0c\u5df2\u6210\u70ba\u4e00\u7a2e\u8da8\u52e2\u3002\u96d6\u7136\u73fe\u6709\u7814\u7a76\u4e3b\u8981\u63a2\u8a0e RoPE \u5728\u6ce8\u610f\u6a5f\u5236\u4e2d\u7684\u9650\u5236\uff0c\u4f46\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u500b\u5e7e\u4e4e\u6db5\u84cb LM \u6240\u6709\u90e8\u5206\u7684\u5206\u6790\uff0c\u63ed\u9732 RoPE \u57fa\u65bc\u6ce8\u610f\u7684\u9577\u5ea6\u6982\u5316\u5c0d\u5176\u7522\u751f\u7684\u4e0d\u5229\u5f71\u97ff\u3002\u6211\u5011\u4f7f\u7528\u96e2\u6563\u4fe1\u865f\u8655\u7406\u7406\u8ad6\uff0c\u8b49\u660e RoPE \u900f\u904e\u96b1\u542b\u5730\u9054\u6210\u975e\u5747\u52fb\u96e2\u6563\u5085\u7acb\u8449\u8f49\u63db\uff0c\u9032\u800c\u5be6\u73fe\u9031\u671f\u6027\u6ce8\u610f\u3002\u7136\u800c\uff0c\u9019\u7a2e\u9031\u671f\u6027\u53d7\u5230\u4ee5\u4e0b\u56e0\u7d20\u9020\u6210\u7684\u983b\u8b5c\u640d\u5bb3\u6240\u7834\u58de\uff1a1) \u6ce8\u610f\u4e4b\u5916\u7684\u7dda\u6027\u5c64\u548c\u6fc0\u6d3b\u51fd\u6578\uff1b2) \u6642\u57df\u622a\u65b7\u6240\u5e36\u4f86\u7684\u983b\u7387\u7d44\u6210\u8a13\u7df4\u4e0d\u8db3\u3002\u6839\u64da\u6211\u5011\u7684\u89c0\u5bdf\uff0c\u6211\u5011\u63d0\u51fa\u5085\u7acb\u8449\u4f4d\u7f6e\u5d4c\u5165 (FoPE)\uff0c\u5b83\u589e\u5f37\u4e86\u6ce8\u610f\u7684\u983b\u57df\u7279\u6027\uff0c\u4ee5\u6539\u5584\u5176\u9031\u671f\u6027\u5ef6\u4f38\u548c\u9577\u5ea6\u6982\u5316\u3002FoPE \u5efa\u69cb\u5085\u7acb\u8449\u7d1a\u6578\uff0c\u4e26\u5c07\u7834\u58de\u6027\u7684\u983b\u7387\u7d44\u6210\u96f6\u5316\uff0c\u589e\u52a0\u6a21\u578b\u5c0d\u983b\u8b5c\u640d\u5bb3\u7684\u7a69\u5065\u6027\u3002\u5728\u5404\u7a2e\u6a21\u578b\u898f\u6a21\u7684\u5be6\u9a57\u4e2d\u986f\u793a\uff0c\u5728\u4e0d\u540c\u7684\u5167\u5bb9\u8996\u7a97\u4e2d\uff0c\u8207 RoPE \u548c ALiBi \u76f8\u6bd4\uff0cFoPE \u80fd\u5920\u7dad\u6301\u66f4\u7a69\u5b9a\u7684\u56f0\u60d1\u5ea6\uff0c\u4ee5\u53ca\u5728\u91dd\u982d\u5927\u6d77\u4efb\u52d9\u4e2d\u66f4\u4e00\u81f4\u7684\u6e96\u78ba\u5ea6\u3002\u591a\u9805\u5206\u6790\u548c\u6d88\u878d\u9032\u4e00\u6b65\u652f\u6301\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u548c\u7406\u8ad6\u5efa\u6a21\u3002", "author": "Ermo Hua et.al.", "authors": "Ermo Hua, Che Jiang, Xingtai Lv, Kaiyan Zhang, Ning Ding, Youbang Sun, Biqing Qi, Yuchen Fan, Xue Kai Zhu, Bowen Zhou", "id": "2412.17739v1", "paper_url": "http://arxiv.org/abs/2412.17739v1", "repo": "null"}}