{"2408.12547": {"publish_time": "2024-08-22", "title": "Towards Evaluating and Building Versatile Large Language Models for Medicine", "paper_summary": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins.", "paper_summary_zh": "<paragraph>\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa MedS-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u6027\u80fd\u7684\u7efc\u5408\u57fa\u51c6\u3002\n\u4e0e\u4e13\u6ce8\u4e8e\u591a\u9879\u9009\u62e9\u95ee\u7b54\u7684\u73b0\u6709\u57fa\u51c6\u4e0d\u540c\uff0cMedS-Bench \u6db5\u76d6 11 \u9879\u9ad8\u7ea7\u4e34\u5e8a\u4efb\u52a1\uff0c\u5305\u62ec\u4e34\u5e8a\u62a5\u544a\u6458\u8981\u3001\u6cbb\u7597\u5efa\u8bae\u3001\u8bca\u65ad\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u533b\u5b66\u6982\u5ff5\u89e3\u91ca\u7b49\u3002\u6211\u4eec\u4f7f\u7528\u5c11\u6837\u672c\u63d0\u793a\u8bc4\u4f30\u4e86\u516d\u4e2a\u9886\u5148\u7684 LLM\uff0c\u4f8b\u5982 MEDITRON\u3001Mistral\u3001InternLM 2\u3001Llama 3\u3001GPT-4 \u548c Claude-3.5\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u6700\u590d\u6742\u7684\u6a21\u578b\u5728\u8fd9\u4e9b\u590d\u6742\u7684\u4efb\u52a1\u4e2d\u4e5f\u96be\u4ee5\u5e94\u5bf9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 MedS-Ins\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u7684\u5927\u89c4\u6a21\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u3002MedS-Ins \u5305\u542b 58 \u4e2a\u533b\u5b66\u8bed\u8a00\u8bed\u6599\u5e93\uff0c\u603b\u8ba1 122 \u4e2a\u4efb\u52a1\u4e2d\u7684 1350 \u4e07\u4e2a\u6837\u672c\u3002\u4e3a\u4e86\u5c55\u793a\u6570\u636e\u96c6\u7684\u6548\u7528\uff0c\u6211\u4eec\u901a\u8fc7\u5bf9\u8f7b\u91cf\u7ea7\u5f00\u6e90\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u6267\u884c\u6307\u4ee4\u5fae\u8c03\u6765\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u3002\u7531\u6b64\u4ea7\u751f\u7684\u6a21\u578b MMedIns-Llama 3 \u5728\u51e0\u4e4e\u6240\u6709\u4e34\u5e8a\u4efb\u52a1\u4e2d\u90fd\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u4e3a\u4e86\u4fc3\u8fdb LLM \u5728\u4e34\u5e8a\u6311\u6218\u4e2d\u7684\u5e94\u7528\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u6211\u4eec\u5df2\u7ecf\u8ba9 MedS-Ins \u6570\u636e\u96c6\u5b8c\u5168\u53ef\u4ee5\u8bbf\u95ee\uff0c\u5e76\u9080\u8bf7\u7814\u7a76\u754c\u4e3a\u5176\u6269\u5c55\u505a\u51fa\u8d21\u732e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5df2\u7ecf\u4e3a MedS-Bench \u63a8\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u6392\u884c\u699c\uff0c\u6211\u4eec\u8ba1\u5212\u5b9a\u671f\u66f4\u65b0\u6d4b\u8bd5\u96c6\u4ee5\u8ddf\u8e2a\u8fdb\u5ea6\u5e76\u589e\u5f3a\u901a\u7528 LLM \u5bf9\u533b\u5b66\u9886\u57df\u7684\u9002\u5e94\u6027\u3002\u6392\u884c\u699c\uff1ahttps://henrychur.github.io/MedS-Bench/\u3002Github\uff1ahttps://github.com/MAGIC-AI4Med/MedS-Ins\u3002</paragraph>", "author": "Chaoyi Wu et.al.", "authors": "Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, Weidi Xie", "id": "2408.12547v1", "paper_url": "http://arxiv.org/abs/2408.12547v1", "repo": "https://github.com/magic-ai4med/meds-ins"}}