{"2408.02416": {"publish_time": "2024-08-05", "title": "Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models", "paper_summary": "The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at \\url{https://github.com/liangzid/PromptExtractionEval}.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u53c3\u6578\u7684\u6025\u5287\u589e\u52a0\uff0c\u5c0e\u81f4\u4e86\u63d0\u793a\u7b26\u9032\u884c\u5fae\u8abf\u81ea\u7531\u4e0b\u6e38\u81ea\u8a02\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u5373\u4efb\u52d9\u63cf\u8ff0\u3002\u96d6\u7136\u9019\u4e9b\u57fa\u65bc\u63d0\u793a\u7b26\u7684\u670d\u52d9\uff08\u4f8b\u5982 OpenAI \u7684 GPT\uff09\u5728\u8a31\u591a\u4f01\u696d\u4e2d\u626e\u6f14\u8457\u91cd\u8981\u7684\u89d2\u8272\uff0c\u4f46\u5c0d\u65bc\u63d0\u793a\u7b26\u6d29\u6f0f\u7684\u64d4\u6182\u8207\u65e5\u4ff1\u589e\uff0c\u9019\u6703\u7834\u58de\u9019\u4e9b\u670d\u52d9\u7684\u667a\u6167\u8ca1\u7522\u6b0a\u4e26\u5c0e\u81f4\u4e0b\u6e38\u653b\u64ca\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5206\u6790\u4e86\u63d0\u793a\u7b26\u6d29\u6f0f\u7684\u5e95\u5c64\u6a5f\u5236\uff0c\u6211\u5011\u7a31\u4e4b\u70ba\u63d0\u793a\u7b26\u8a18\u61b6\uff0c\u4e26\u5236\u5b9a\u4e86\u76f8\u61c9\u7684\u9632\u79a6\u7b56\u7565\u3002\u901a\u904e\u63a2\u7d22\u63d0\u793a\u7b26\u63d0\u53d6\u4e2d\u7684\u898f\u6a21\u5b9a\u5f8b\uff0c\u6211\u5011\u5206\u6790\u4e86\u5f71\u97ff\u63d0\u793a\u7b26\u63d0\u53d6\u7684\u95dc\u9375\u5c6c\u6027\uff0c\u5305\u62ec\u6a21\u578b\u5927\u5c0f\u3001\u63d0\u793a\u7b26\u9577\u5ea6\u4ee5\u53ca\u63d0\u793a\u7b26\u985e\u578b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5169\u500b\u5047\u8a2d\u4f86\u89e3\u91cb LLM \u5982\u4f55\u516c\u958b\u5176\u63d0\u793a\u7b26\u3002\u7b2c\u4e00\u500b\u6b78\u56e0\u65bc\u56f0\u60d1\uff0c\u5373 LLM \u5c0d\u6587\u672c\u7684\u719f\u6089\u5ea6\uff0c\u800c\u7b2c\u4e8c\u500b\u5247\u57fa\u65bc\u6ce8\u610f\u529b\u77e9\u9663\u4e2d\u7684\u76f4\u63a5\u4ee3\u5e63\u8f49\u63db\u8def\u5f91\u3002\u70ba\u4e86\u9632\u79a6\u6b64\u985e\u5a01\u8105\uff0c\u6211\u5011\u7814\u7a76\u4e86\u5c0d\u9f4a\u662f\u5426\u6703\u7834\u58de\u63d0\u793a\u7b26\u7684\u63d0\u53d6\u3002\u6211\u5011\u767c\u73fe\u7576\u524d\u7684 LLM\uff0c\u5373\u4f7f\u662f\u90a3\u4e9b\u5177\u6709\u5b89\u5168\u5c0d\u9f4a\u529f\u80fd\uff08\u4f8b\u5982 GPT-4\uff09\u7684 LLM\uff0c\u4e5f\u6975\u6613\u53d7\u5230\u63d0\u793a\u7b26\u63d0\u53d6\u653b\u64ca\uff0c\u5373\u4f7f\u662f\u5728\u6700\u76f4\u63a5\u7684\u4f7f\u7528\u8005\u653b\u64ca\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u56e0\u6b64\uff0c\u6211\u5011\u6839\u64da\u6211\u5011\u7684\u767c\u73fe\u63d0\u51fa\u4e86\u5e7e\u7a2e\u9632\u79a6\u7b56\u7565\uff0c\u9019\u4e9b\u7b56\u7565\u5206\u5225\u70ba Llama2-7B \u548c GPT-3.5 \u7684\u63d0\u793a\u7b26\u63d0\u53d6\u7387\u964d\u4f4e\u4e86 83.8% \u548c 71.0%\u3002\u539f\u59cb\u78bc\u53ef\u65bc\\url{https://github.com/liangzid/PromptExtractionEval}\u53d6\u5f97\u3002</paragraph>", "author": "Zi Liang et.al.", "authors": "Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li", "id": "2408.02416v1", "paper_url": "http://arxiv.org/abs/2408.02416v1", "repo": "null"}}