{"2407.07080": {"publish_time": "2024-07-09", "title": "Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities", "paper_summary": "Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.", "paper_summary_zh": "\u5728\u5e0c\u4f2f\u4f86\u8a9e\u7b49\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u4e2d\u8a13\u7df4\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6703\u5e36\u4f86\u7368\u7279\u7684\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 DictaLM2.0 \u548c DictaLM2.0-Instruct\uff0c\u9019\u5169\u500b LLM \u662f\u5f9e Mistral \u6a21\u578b\u884d\u751f\u7684\uff0c\u4e26\u5728\u5305\u542b\u7d04 2,000 \u5104\u500b\u5e0c\u4f2f\u4f86\u8a9e\u548c\u82f1\u8a9e\u8a5e\u5f59\u7684\u9f90\u5927\u8a9e\u6599\u5eab\u4e2d\u8a13\u7df4\u3002\u5c07\u9810\u8a13\u7df4\u6a21\u578b\u9069\u61c9\u5230\u65b0\u8a9e\u8a00\u6d89\u53ca\u5c08\u696d\u6280\u8853\uff0c\u9019\u8207\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u6a21\u578b\u6216\u9032\u4e00\u6b65\u8a13\u7df4\u73fe\u6709\u6a21\u578b\uff08\u4f8b\u5982\u82f1\u8a9e\u7b49\u8cc7\u6e90\u8c50\u5bcc\u7684\u8a9e\u8a00\uff09\u6709\u986f\u8457\u4e0d\u540c\u3002\u6211\u5011\u6982\u8ff0\u4e86\u9019\u4e9b\u65b0\u7a4e\u7684\u8a13\u7df4\u65b9\u6cd5\uff0c\u6709\u52a9\u65bc\u6709\u6548\u5b78\u7fd2\u548c\u9069\u61c9\u5e0c\u4f2f\u4f86\u8a9e\u7684\u8a9e\u8a00\u7279\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u91dd\u5c0d\u5168\u9762\u7684\u6307\u5c0e\u8cc7\u6599\u96c6\u5fae\u8abf DictaLM2.0-Instruct\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u7279\u5b9a\u4efb\u52d9\u6307\u793a\u4e0a\u7684\u6548\u80fd\u3002\u70ba\u4e86\u56b4\u683c\u8a55\u4f30\u6211\u5011\u7684\u6a21\u578b\uff0c\u6211\u5011\u70ba\u5e0c\u4f2f\u4f86\u8a9e LLM \u8a55\u4f30\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u6e96\u7d44\uff0c\u6db5\u84cb\u4e86\u591a\u6a23\u5316\u7684\u4efb\u52d9\u96c6\uff0c\u5305\u62ec\u554f\u7b54\u3001\u60c5\u7dd2\u5206\u6790\u3001Winograd \u6a21\u5f0f\u6311\u6230\u3001\u7ffb\u8b6f\u548c\u6458\u8981\u3002\u6211\u5011\u7684\u7814\u7a76\u4e0d\u50c5\u89e3\u6c7a\u4e86\u5728\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u4e2d\u8a13\u7df4 LLM \u7684\u8907\u96dc\u6027\uff0c\u9084\u63d0\u51fa\u4e86\u53ef\u7528\u65bc\u5c07\u5176\u4ed6 LLM \u9069\u61c9\u5230\u5404\u7a2e\u975e\u82f1\u8a9e\u8a9e\u8a00\u7684\u67b6\u69cb\uff0c\u70ba\u591a\u8a9e\u8a00 NLP \u7684\u5ee3\u6cdb\u9818\u57df\u505a\u51fa\u8ca2\u737b\u3002", "author": "Shaltiel Shmidman et.al.", "authors": "Shaltiel Shmidman, Avi Shmidman, Amir DN Cohen, Moshe Koppel", "id": "2407.07080v1", "paper_url": "http://arxiv.org/abs/2407.07080v1", "repo": "null"}}