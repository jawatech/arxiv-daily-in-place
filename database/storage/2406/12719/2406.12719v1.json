{"2406.12719": {"publish_time": "2024-06-18", "title": "On the Robustness of Language Models for Tabular Question Answering", "paper_summary": "Large Language Models (LLMs), originally shown to ace various text\ncomprehension tasks have also remarkably been shown to tackle table\ncomprehension tasks without specific training. While previous research has\nexplored LLM capabilities with tabular dataset tasks, our study assesses the\ninfluence of $\\textit{in-context learning}$,$ \\textit{model scale}$,\n$\\textit{instruction tuning}$, and $\\textit{domain biases}$ on Tabular Question\nAnswering (TQA). We evaluate the robustness of LLMs on Wikipedia-based\n$\\textbf{WTQ}$ and financial report-based $\\textbf{TAT-QA}$ TQA datasets,\nfocusing on their ability to robustly interpret tabular data under various\naugmentations and perturbations. Our findings indicate that instructions\nsignificantly enhance performance, with recent models like Llama3 exhibiting\ngreater robustness over earlier versions. However, data contamination and\npractical reliability issues persist, especially with WTQ. We highlight the\nneed for improved methodologies, including structure-aware self-attention\nmechanisms and better handling of domain-specific tabular data, to develop more\nreliable LLMs for table comprehension.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u6700\u521d\u88ab\u8b49\u660e\u53ef\u4ee5\u61c9\u5c0d\u5404\u7a2e\u6587\u672c\u7406\u89e3\u4efb\u52d9\uff0c\u800c\u4e14\u9084\u7279\u5225\u88ab\u8b49\u660e\u53ef\u4ee5\u5728\u6c92\u6709\u7279\u5b9a\u8a13\u7df4\u7684\u60c5\u6cc1\u4e0b\u61c9\u5c0d\u8868\u683c\u7406\u89e3\u4efb\u52d9\u3002\u96d6\u7136\u5148\u524d\u7684\u7814\u7a76\u5df2\u7d93\u63a2\u7d22\u4e86 LLM \u5728\u8868\u683c\u8cc7\u6599\u96c6\u4efb\u52d9\u4e2d\u7684\u80fd\u529b\uff0c\u4f46\u6211\u5011\u7684\u7814\u7a76\u8a55\u4f30\u4e86\u300c\u60c5\u5883\u5b78\u7fd2\u300d\u3001\u300c\u6a21\u578b\u898f\u6a21\u300d\u3001\u300c\u6307\u4ee4\u8abf\u6574\u300d\u548c\u300c\u9818\u57df\u504f\u5dee\u300d\u5c0d\u8868\u683c\u554f\u984c\u89e3\u7b54 (TQA) \u7684\u5f71\u97ff\u3002\u6211\u5011\u8a55\u4f30\u4e86 LLM \u5728\u57fa\u65bc\u7dad\u57fa\u767e\u79d1\u7684\u300cWTQ\u300d\u548c\u57fa\u65bc\u8ca1\u52d9\u5831\u8868\u7684\u300cTAT-QA\u300dTQA \u8cc7\u6599\u96c6\u4e0a\u7684\u7a69\u5065\u6027\uff0c\u91cd\u9ede\u95dc\u6ce8\u5b83\u5011\u5728\u5404\u7a2e\u64f4\u5145\u548c\u64fe\u52d5\u4e0b\u7a69\u5065\u89e3\u91cb\u8868\u683c\u6578\u64da\u7684\u80fd\u529b\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u6307\u4ee4\u53ef\u4ee5\u986f\u8457\u63d0\u9ad8\u6548\u80fd\uff0c\u800c\u50cf Llama3 \u9019\u6a23\u7684\u6700\u65b0\u6a21\u578b\u6bd4\u65e9\u671f\u7248\u672c\u8868\u73fe\u51fa\u66f4\u5f37\u7684\u7a69\u5065\u6027\u3002\u7136\u800c\uff0c\u6578\u64da\u6c61\u67d3\u548c\u5be6\u969b\u53ef\u9760\u6027\u554f\u984c\u4f9d\u7136\u5b58\u5728\uff0c\u7279\u5225\u662f\u5728 WTQ \u4e2d\u3002\u6211\u5011\u5f37\u8abf\u9700\u8981\u6539\u9032\u65b9\u6cd5\u8ad6\uff0c\u5305\u62ec\u7d50\u69cb\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a5f\u5236\u548c\u5c0d\u7279\u5b9a\u9818\u57df\u8868\u683c\u6578\u64da\u7684\u66f4\u597d\u8655\u7406\uff0c\u4ee5\u958b\u767c\u51fa\u66f4\u53ef\u9760\u7684 LLM \u4f86\u9032\u884c\u8868\u683c\u7406\u89e3\u3002", "author": "Kushal Raj Bhandari et.al.", "authors": "Kushal Raj Bhandari, Sixue Xing, Soham Dan, Jianxi Gao", "id": "2406.12719v1", "paper_url": "http://arxiv.org/abs/2406.12719v1", "repo": "null"}}