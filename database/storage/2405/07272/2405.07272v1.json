{"2405.07272": {"publish_time": "2024-05-12", "title": "MAML MOT: Multiple Object Tracking based on Meta-Learning", "paper_summary": "With the advancement of video analysis technology, the multi-object tracking\n(MOT) problem in complex scenes involving pedestrians is gaining increasing\nimportance. This challenge primarily involves two key tasks: pedestrian\ndetection and re-identification. While significant progress has been achieved\nin pedestrian detection tasks in recent years, enhancing the effectiveness of\nre-identification tasks remains a persistent challenge. This difficulty arises\nfrom the large total number of pedestrian samples in multi-object tracking\ndatasets and the scarcity of individual instance samples. Motivated by recent\nrapid advancements in meta-learning techniques, we introduce MAML MOT, a\nmeta-learning-based training approach for multi-object tracking. This approach\nleverages the rapid learning capability of meta-learning to tackle the issue of\nsample scarcity in pedestrian re-identification tasks, aiming to improve the\nmodel's generalization performance and robustness. Experimental results\ndemonstrate that the proposed method achieves high accuracy on mainstream\ndatasets in the MOT Challenge. This offers new perspectives and solutions for\nresearch in the field of pedestrian multi-object tracking.", "paper_summary_zh": "\u968f\u7740\u5f71\u50cf\u5206\u6790\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u6d89\u53ca\u884c\u4eba\u7684\u590d\u6742\u573a\u666f\u4e2d\u7684\u591a\u76ee\u6807\u8ffd\u8e2a\n(MOT) \u95ee\u9898\u6b63\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u8fd9\u4e2a\u6311\u6218\u4e3b\u8981\u6d89\u53ca\u4e24\u9879\u5173\u952e\u4efb\u52a1\uff1a\u884c\u4eba\n\u68c0\u6d4b\u548c\u91cd\u65b0\u8bc6\u522b\u3002\u867d\u7136\u8fd1\u5e74\u6765\u884c\u4eba\u68c0\u6d4b\u4efb\u52a1\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u63d0\u9ad8\n\u91cd\u65b0\u8bc6\u522b\u4efb\u52a1\u7684\u6709\u6548\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u6311\u6218\u3002\u8fd9\u79cd\u56f0\u96be\u6e90\u4e8e\u591a\u76ee\u6807\u8ffd\u8e2a\n\u6570\u636e\u96c6\u4e2d\u7684\u884c\u4eba\u6837\u672c\u603b\u6570\u5e9e\u5927\uff0c\u800c\u5355\u4e2a\u5b9e\u4f8b\u6837\u672c\u7a00\u5c11\u3002\u53d7\u5143\u5b66\u4e60\u6280\u672f\n\u6700\u8fd1\u7684\u5feb\u901f\u53d1\u5c55\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86 MAML MOT\uff0c\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u7528\u4e8e\n\u591a\u76ee\u6807\u8ffd\u8e2a\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u5229\u7528\u5143\u5b66\u4e60\u7684\u5feb\u901f\u5b66\u4e60\u80fd\u529b\u6765\u89e3\u51b3\n\u884c\u4eba\u91cd\u65b0\u8bc6\u522b\u4efb\u52a1\u4e2d\u6837\u672c\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u548c\u7a33\u5065\n\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 MOT \u6311\u6218\u4e2d\u7684\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\n\u5f88\u9ad8\u7684\u51c6\u786e\u6027\u3002\u8fd9\u4e3a\u884c\u4eba\u591a\u76ee\u6807\u8ffd\u8e2a\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u89e3\u51b3\n\u65b9\u6848\u3002", "author": "Jiayi Chen et.al.", "authors": "Jiayi Chen, Chunhua Deng", "id": "2405.07272v1", "paper_url": "http://arxiv.org/abs/2405.07272v1", "repo": "null"}}