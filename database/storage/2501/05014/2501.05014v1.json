{"2501.05014": {"publish_time": "2025-01-09", "title": "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation", "paper_summary": "The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate\ncommunication with aerial robots. By integrating satellite imagery processing\nwith the Visual Language Model (VLM) and the powerful capabilities of GPT,\nUAV-VLA enables users to generate general flight paths-and-action plans through\nsimple text requests. This system leverages the rich contextual information\nprovided by satellite images, allowing for enhanced decision-making and mission\nplanning. The combination of visual analysis by VLM and natural language\nprocessing by GPT can provide the user with the path-and-action set, making\naerial operations more efficient and accessible. The newly developed method\nshowed the difference in the length of the created trajectory in 22% and the\nmean error in finding the objects of interest on a map in 34.22 m by Euclidean\ndistance in the K-Nearest Neighbors (KNN) approach.", "paper_summary_zh": "\u7121\u4eba\u6a5f-VLA\uff08\u8996\u89ba\u8a9e\u8a00\u52d5\u4f5c\uff09\u7cfb\u7d71\u662f\u4e00\u500b\u5de5\u5177\uff0c\u65e8\u5728\u4fc3\u9032\u8207\u7a7a\u4e2d\u6a5f\u5668\u4eba\u7684\u6e9d\u901a\u3002\u901a\u904e\u5c07\u885b\u661f\u5716\u50cf\u8655\u7406\u8207\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u548c GPT \u7684\u5f37\u5927\u529f\u80fd\u6574\u5408\u5728\u4e00\u8d77\uff0c\u7121\u4eba\u6a5f-VLA \u4f7f\u7528\u6236\u80fd\u5920\u901a\u904e\u7c21\u55ae\u7684\u6587\u5b57\u8acb\u6c42\u751f\u6210\u901a\u7528\u7684\u98db\u884c\u8def\u5f91\u548c\u884c\u52d5\u8a08\u756b\u3002\u6b64\u7cfb\u7d71\u5229\u7528\u885b\u661f\u5716\u50cf\u63d0\u4f9b\u7684\u8c50\u5bcc\u80cc\u666f\u8cc7\u8a0a\uff0c\u5f9e\u800c\u589e\u5f37\u6c7a\u7b56\u5236\u5b9a\u548c\u4efb\u52d9\u898f\u5283\u3002VLM \u7684\u8996\u89ba\u5206\u6790\u548c GPT \u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u76f8\u7d50\u5408\uff0c\u53ef\u4ee5\u70ba\u4f7f\u7528\u8005\u63d0\u4f9b\u8def\u5f91\u548c\u52d5\u4f5c\u96c6\uff0c\u4f7f\u7a7a\u4e2d\u4f5c\u696d\u66f4\u6709\u6548\u7387\u4e14\u66f4\u5bb9\u6613\u4f7f\u7528\u3002\u65b0\u958b\u767c\u7684\u65b9\u6cd5\u986f\u793a\u51fa\u5728 22% \u4e2d\u5efa\u7acb\u8ecc\u8de1\u7684\u9577\u5ea6\u5dee\u7570\uff0c\u4ee5\u53ca\u5728 K \u6700\u8fd1\u9130 (KNN) \u65b9\u6cd5\u4e2d\uff0c\u901a\u904e\u6b50\u5e7e\u91cc\u5f97\u8ddd\u96e2\u5728 34.22 \u516c\u5c3a\u4e2d\u627e\u5230\u5730\u5716\u4e0a\u611f\u8208\u8da3\u7269\u9ad4\u7684\u5e73\u5747\u8aa4\u5dee\u3002", "author": "Oleg Sautenkov et.al.", "authors": "Oleg Sautenkov, Yasheerah Yaqoot, Artem Lykov, Muhammad Ahsan Mustafa, Grik Tadevosyan, Aibek Akhmetkazy, Miguel Altamirano Cabrera, Mikhail Martynov, Sausar Karaf, Dzmitry Tsetserukou", "id": "2501.05014v1", "paper_url": "http://arxiv.org/abs/2501.05014v1", "repo": "null"}}