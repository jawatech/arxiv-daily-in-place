{"2410.07739": {"publish_time": "2024-10-10", "title": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture", "paper_summary": "Although many efforts have been made, it is still a challenge to balance the\ntraining budget, downstream performance, and the general capabilities of the\nLLMs in many applications. Training the whole model for downstream tasks is\nexpensive, and could easily result in catastrophic forgetting. By introducing\nparameter-efficient fine-tuning (PEFT), the training cost could be reduced, but\nit still suffers from forgetting, and limits the learning on the downstream\ntasks. To efficiently fine-tune the LLMs with less limitation to their\ndownstream performance while mitigating the forgetting of general capabilities,\nwe propose a novel mixture of expert (MoE) framework based on Soft LoRA and\nIdentity Mixture (SLIM), that allows dynamic routing between LoRA adapters and\nskipping connection, enables the suppression of forgetting. We adopt\nweight-yielding with sliding clustering for better out-of-domain distinguish to\nenhance the routing. We also propose to convert the mixture of low-rank\nadapters to the model merging formulation and introduce fast dynamic merging of\nLoRA adapters to keep the general capabilities of the base model. Extensive\nexperiments demonstrate that the proposed SLIM is comparable to the\nstate-of-the-art PEFT approaches on the downstream tasks while achieving the\nleading performance in mitigating catastrophic forgetting.", "paper_summary_zh": "\u5118\u7ba1\u5df2\u505a\u51fa\u8a31\u591a\u52aa\u529b\uff0c\u4f46\u5728\u8a31\u591a\u61c9\u7528\u4e2d\u5e73\u8861 LLM \u7684\u8a13\u7df4\u9810\u7b97\u3001\u4e0b\u6e38\u6548\u80fd\u548c\u4e00\u822c\u529f\u80fd\u4ecd\u662f\u4e00\u9805\u6311\u6230\u3002\u91dd\u5c0d\u4e0b\u6e38\u4efb\u52d9\u8a13\u7df4\u6574\u500b\u6a21\u578b\u5f88\u6602\u8cb4\uff0c\u800c\u4e14\u5f88\u5bb9\u6613\u5c0e\u81f4\u707d\u96e3\u6027\u907a\u5fd8\u3002\u900f\u904e\u5f15\u5165\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT)\uff0c\u8a13\u7df4\u6210\u672c\u53ef\u4ee5\u964d\u4f4e\uff0c\u4f46\u5b83\u4ecd\u7136\u6703\u907a\u5fd8\uff0c\u4e26\u9650\u5236\u4e0b\u6e38\u4efb\u52d9\u7684\u5b78\u7fd2\u3002\u70ba\u4e86\u6709\u6548\u5fae\u8abf LLM\uff0c\u540c\u6642\u6e1b\u5c11\u5c0d\u5176\u4e0b\u6e38\u6548\u80fd\u7684\u9650\u5236\uff0c\u4e26\u6e1b\u8f15\u4e00\u822c\u529f\u80fd\u7684\u907a\u5fd8\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc Soft LoRA \u548c\u8eab\u5206\u6df7\u5408 (SLIM) \u7684\u5c08\u5bb6\u6df7\u5408 (MoE) \u6846\u67b6\uff0c\u5141\u8a31\u5728 LoRA \u9069\u914d\u5668\u548c\u8df3\u63a5\u9023\u63a5\u4e4b\u9593\u9032\u884c\u52d5\u614b\u8def\u7531\uff0c\u4e26\u6291\u5236\u907a\u5fd8\u3002\u6211\u5011\u63a1\u7528\u5e36\u6709\u6ed1\u52d5\u805a\u985e\u7684\u6b0a\u91cd\u8b93\u6b65\uff0c\u4ee5\u7372\u5f97\u66f4\u597d\u7684\u7db2\u57df\u5916\u5340\u5206\uff0c\u4ee5\u589e\u5f37\u8def\u7531\u3002\u6211\u5011\u9084\u5efa\u8b70\u5c07\u4f4e\u968e\u9069\u914d\u5668\u7684\u6df7\u5408\u8f49\u63db\u70ba\u6a21\u578b\u5408\u4f75\u516c\u5f0f\uff0c\u4e26\u5f15\u5165 LoRA \u9069\u914d\u5668\u7684\u5feb\u901f\u52d5\u614b\u5408\u4f75\uff0c\u4ee5\u4fdd\u6301\u57fa\u790e\u6a21\u578b\u7684\u4e00\u822c\u529f\u80fd\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6240\u63d0\u51fa\u7684 SLIM \u5728\u4e0b\u6e38\u4efb\u52d9\u4e0a\u53ef\u8207\u6700\u5148\u9032\u7684 PEFT \u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u540c\u6642\u5728\u6e1b\u8f15\u707d\u96e3\u6027\u907a\u5fd8\u65b9\u9762\u53d6\u5f97\u9818\u5148\u7684\u6548\u80fd\u3002", "author": "Jiayi Han et.al.", "authors": "Jiayi Han, Liang Du, Hongwei Du, Xiangguo Zhou, Yiwen Wu, Weibo Zheng, Donghong Han", "id": "2410.07739v1", "paper_url": "http://arxiv.org/abs/2410.07739v1", "repo": "null"}}