{"2405.03688": {"publish_time": "2024-05-06", "title": "Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames", "paper_summary": "Adversarial information operations can destabilize societies by undermining\nfair elections, manipulating public opinions on policies, and promoting scams.\nDespite their widespread occurrence and potential impacts, our understanding of\ninfluence campaigns is limited by manual analysis of messages and subjective\ninterpretation of their observable behavior. In this paper, we explore whether\nthese limitations can be mitigated with large language models (LLMs), using\nGPT-3.5 as a case-study for coordinated campaign annotation. We first use\nGPT-3.5 to scrutinize 126 identified information operations spanning over a\ndecade. We utilize a number of metrics to quantify the close (if imperfect)\nagreement between LLM and ground truth descriptions. We next extract\ncoordinated campaigns from two large multilingual datasets from X (formerly\nTwitter) that respectively discuss the 2022 French election and 2023 Balikaran\nPhilippine-U.S. military exercise in 2023. For each coordinated campaign, we\nuse GPT-3.5 to analyze posts related to a specific concern and extract goals,\ntactics, and narrative frames, both before and after critical events (such as\nthe date of an election). While the GPT-3.5 sometimes disagrees with subjective\ninterpretation, its ability to summarize and interpret demonstrates LLMs'\npotential to extract higher-order indicators from text to provide a more\ncomplete picture of the information campaigns compared to previous methods.", "paper_summary_zh": "", "author": "Keith Burghardt et.al.", "authors": "Keith Burghardt,Kai Chen,Kristina Lerman", "id": "2405.03688v1", "paper_url": "http://arxiv.org/abs/2405.03688v1", "repo": "null"}}