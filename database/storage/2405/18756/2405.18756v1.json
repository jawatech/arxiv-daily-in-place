{"2405.18756": {"publish_time": "2024-05-29", "title": "Provable Contrastive Continual Learning", "paper_summary": "Continual learning requires learning incremental tasks with dynamic data\ndistributions. So far, it has been observed that employing a combination of\ncontrastive loss and distillation loss for training in continual learning\nyields strong performance. To the best of our knowledge, however, this\ncontrastive continual learning framework lacks convincing theoretical\nexplanations. In this work, we fill this gap by establishing theoretical\nperformance guarantees, which reveal how the performance of the model is\nbounded by training losses of previous tasks in the contrastive continual\nlearning framework. Our theoretical explanations further support the idea that\npre-training can benefit continual learning. Inspired by our theoretical\nanalysis of these guarantees, we propose a novel contrastive continual learning\nalgorithm called CILA, which uses adaptive distillation coefficients for\ndifferent tasks. These distillation coefficients are easily computed by the\nratio between average distillation losses and average contrastive losses from\nprevious tasks. Our method shows great improvement on standard benchmarks and\nachieves new state-of-the-art performance.", "paper_summary_zh": "\u6301\u7e8c\u5b78\u7fd2\u9700\u8981\u5b78\u7fd2\u5177\u6709\u52d5\u614b\u8cc7\u6599\u5206\u4f48\u7684\u589e\u91cf\u4efb\u52d9\u3002\u5230\u76ee\u524d\u70ba\u6b62\uff0c\u5df2\u7d93\u89c0\u5bdf\u5230\u63a1\u7528\u5c0d\u6bd4\u640d\u5931\u548c\u84b8\u993e\u640d\u5931\u7684\u7d44\u5408\u4f86\u8a13\u7df4\u6301\u7e8c\u5b78\u7fd2\u6703\u7522\u751f\u5f37\u5927\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5c31\u6211\u5011\u6240\u77e5\uff0c\u9019\u500b\u5c0d\u6bd4\u6301\u7e8c\u5b78\u7fd2\u67b6\u69cb\u7f3a\u4e4f\u4ee4\u4eba\u4fe1\u670d\u7684\u7406\u8ad6\u89e3\u91cb\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5efa\u7acb\u7406\u8ad6\u6548\u80fd\u4fdd\u8b49\u4f86\u586b\u88dc\u9019\u500b\u5dee\u8ddd\uff0c\u63ed\u793a\u6a21\u578b\u7684\u6548\u80fd\u5982\u4f55\u53d7\u5230\u5c0d\u6bd4\u6301\u7e8c\u5b78\u7fd2\u67b6\u69cb\u4e2d\u5148\u524d\u4efb\u52d9\u8a13\u7df4\u640d\u5931\u7684\u7d04\u675f\u3002\u6211\u5011\u7684\u7406\u8ad6\u89e3\u91cb\u9032\u4e00\u6b65\u652f\u6301\u4e86\u9810\u8a13\u7df4\u53ef\u4ee5\u4f7f\u6301\u7e8c\u5b78\u7fd2\u53d7\u76ca\u7684\u60f3\u6cd5\u3002\u53d7\u5230\u6211\u5011\u5c0d\u9019\u4e9b\u4fdd\u8b49\u7684\u7406\u8ad6\u5206\u6790\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba CILA \u7684\u65b0\u5c0d\u6bd4\u6301\u7e8c\u5b78\u7fd2\u6f14\u7b97\u6cd5\uff0c\u5b83\u4f7f\u7528\u81ea\u9069\u61c9\u84b8\u993e\u4fc2\u6578\u4f86\u8655\u7406\u4e0d\u540c\u7684\u4efb\u52d9\u3002\u9019\u4e9b\u84b8\u993e\u4fc2\u6578\u5f88\u5bb9\u6613\u900f\u904e\u5148\u524d\u4efb\u52d9\u7684\u5e73\u5747\u84b8\u993e\u640d\u5931\u548c\u5e73\u5747\u5c0d\u6bd4\u640d\u5931\u4e4b\u9593\u7684\u6bd4\u7387\u4f86\u8a08\u7b97\u3002\u6211\u5011\u7684\u6a21\u578b\u5728\u6a19\u6e96\u57fa\u6e96\u4e0a\u986f\u793a\u51fa\u6975\u5927\u7684\u9032\u6b65\uff0c\u4e26\u9054\u5230\u4e86\u65b0\u7684\u6700\u5148\u9032\u6548\u80fd\u3002", "author": "Yichen Wen et.al.", "authors": "Yichen Wen, Zhiquan Tan, Kaipeng Zheng, Chuanlong Xie, Weiran Huang", "id": "2405.18756v1", "paper_url": "http://arxiv.org/abs/2405.18756v1", "repo": "null"}}