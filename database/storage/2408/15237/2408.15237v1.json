{"2408.15237": {"publish_time": "2024-08-27", "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models", "paper_summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.", "paper_summary_zh": "\u7dda\u6027 RNN \u67b6\u69cb\uff0c\u4f8b\u5982 Mamba\uff0c\u5728\u8a9e\u8a00\u6a21\u578b\u4e2d\u53ef\u4ee5\u8207 Transformer \u6a21\u578b\u7af6\u722d\uff0c\u540c\u6642\u5177\u6709\u6709\u5229\u7684\u90e8\u7f72\u7279\u6027\u3002\u9451\u65bc\u91cd\u9ede\u5728\u8a13\u7df4\u5927\u578b Transformer \u6a21\u578b\uff0c\u6211\u5011\u8003\u616e\u5c07\u9019\u4e9b\u9810\u8a13\u7df4\u6a21\u578b\u8f49\u63db\u70ba\u90e8\u7f72\u7684\u6311\u6230\u3002\u6211\u5011\u8b49\u660e\u4e86\u900f\u904e\u91cd\u8907\u4f7f\u7528\u6ce8\u610f\u529b\u5c64\u7684\u7dda\u6027\u6295\u5f71\u6b0a\u91cd\uff0c\u5c07\u5927\u578b Transformer \u8403\u53d6\u5230\u7dda\u6027 RNN \u4e2d\u662f\u53ef\u884c\u7684\uff0c\u4e26\u5177\u5099\u5b78\u8853 GPU \u8cc7\u6e90\u3002\u6240\u7522\u751f\u7684\u6df7\u5408\u6a21\u578b\u5305\u542b\u56db\u5206\u4e4b\u4e00\u7684\u6ce8\u610f\u529b\u5c64\uff0c\u5728\u804a\u5929\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5be6\u73fe\u8207\u539f\u59cb Transformer \u76f8\u7576\u7684\u6548\u80fd\uff0c\u4e26\u512a\u65bc\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\u7684\u958b\u6e90\u6df7\u5408 Mamba \u6a21\u578b\uff0c\u4e14\u804a\u5929\u57fa\u6e96\u6e2c\u8a66\u548c\u4e00\u822c\u57fa\u6e96\u6e2c\u8a66\u4e2d\u90fd\u6709\u6578\u5146\u500b\u4ee3\u5e63\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u786c\u9ad4\u611f\u77e5\u63a8\u6e2c\u89e3\u78bc\u6f14\u7b97\u6cd5\uff0c\u53ef\u52a0\u901f Mamba \u548c\u6df7\u5408\u6a21\u578b\u7684\u63a8\u8ad6\u901f\u5ea6\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5982\u4f55\u900f\u904e\u6709\u9650\u7684\u904b\u7b97\u8cc7\u6e90\uff0c\u79fb\u9664\u8a31\u591a\u539f\u59cb\u7684\u6ce8\u610f\u529b\u5c64\uff0c\u4e26\u66f4\u6709\u6548\u7387\u5730\u5f9e\u7522\u751f\u7684\u6a21\u578b\u4e2d\u751f\u6210\u3002\u6211\u5011\u5f9e Llama3-8B-Instruct \u8403\u53d6\u51fa\u7684\u6548\u80fd\u6700\u4f73\u6a21\u578b\uff0c\u5728 AlpacaEval 2 \u4e2d\u5c0d\u4e0a GPT-4 \u548c MT-Bench \u4e2d\u7684\u7372\u52dd\u7387\u5206\u5225\u9054\u5230 29.61 \u548c 7.35\uff0c\u8d85\u8d8a\u4e86\u6700\u4f73\u7684\u6307\u4ee4\u8abf\u6574\u7dda\u6027 RNN \u6a21\u578b\u3002", "author": "Junxiong Wang et.al.", "authors": "Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao", "id": "2408.15237v1", "paper_url": "http://arxiv.org/abs/2408.15237v1", "repo": "https://github.com/jxiw/mambainllama"}}