{"2501.11651": {"publish_time": "2025-01-20", "title": "Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling", "paper_summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration and\nlearning from feedback, recent attempts yield only modest improvements in\ncomplex reasoning. In this paper, we present T1 to scale RL by encouraging\nexploration and understand inference scaling. We first initialize the LLM using\nsynthesized chain-of-thought data that integrates trial-and-error and\nself-verification. To scale RL training, we promote increased sampling\ndiversity through oversampling. We further employ an entropy bonus as an\nauxiliary loss, alongside a dynamic anchor for regularization to facilitate\nreward optimization. We demonstrate that T1 with open LLMs as its base exhibits\ninference scaling behavior and achieves superior performance on challenging\nmath reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model\noutperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and\nOmni-math-500. More importantly, we present a simple strategy to examine\ninference scaling, where increased inference budgets directly lead to T1's\nbetter performance without any additional verification. We will open-source the\nT1 models and the data used to train them at \\url{https://github.com/THUDM/T1}.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8907\u96dc\u63a8\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u73fe\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u65bc\u6a21\u4eff\u5b78\u7fd2\uff0c\u4e26\u4e14\u96e3\u4ee5\u5be6\u73fe\u6709\u6548\u7684\u6e2c\u8a66\u6642\u9593\u64f4\u5c55\u3002\u96d6\u7136\u5f37\u5316\u5b78\u7fd2\uff08RL\uff09\u6709\u671b\u5be6\u73fe\u81ea\u6211\u63a2\u7d22\u548c\u5f9e\u56de\u994b\u4e2d\u5b78\u7fd2\uff0c\u4f46\u6700\u8fd1\u7684\u5617\u8a66\u5728\u8907\u96dc\u63a8\u7406\u4e2d\u50c5\u7522\u751f\u4e86\u9069\u5ea6\u7684\u6539\u9032\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa T1 \u4f86\u64f4\u5c55 RL\uff0c\u4ee5\u9f13\u52f5\u63a2\u7d22\u4e26\u4e86\u89e3\u63a8\u7406\u64f4\u5c55\u3002\u6211\u5011\u9996\u5148\u4f7f\u7528\u7d9c\u5408\u7684\u601d\u7dad\u93c8\u6578\u64da\u521d\u59cb\u5316 LLM\uff0c\u8a72\u6578\u64da\u6574\u5408\u4e86\u8a66\u932f\u548c\u81ea\u6211\u9a57\u8b49\u3002\u70ba\u4e86\u64f4\u5c55 RL \u8a13\u7df4\uff0c\u6211\u5011\u901a\u904e\u904e\u5ea6\u63a1\u6a23\u4f86\u4fc3\u9032\u589e\u52a0\u63a1\u6a23\u591a\u6a23\u6027\u3002\u6211\u5011\u9032\u4e00\u6b65\u63a1\u7528\u71b5\u734e\u52f5\u4f5c\u70ba\u8f14\u52a9\u640d\u5931\uff0c\u4e26\u63a1\u7528\u52d5\u614b\u9328\u9ede\u9032\u884c\u6b63\u5247\u5316\uff0c\u4ee5\u4fc3\u9032\u734e\u52f5\u512a\u5316\u3002\u6211\u5011\u8b49\u660e\u4e86\u4ee5\u958b\u653e\u5f0f LLM \u70ba\u57fa\u790e\u7684 T1 \u8868\u73fe\u51fa\u63a8\u7406\u64f4\u5c55\u884c\u70ba\uff0c\u4e26\u5728\u5177\u6709\u6311\u6230\u6027\u7684\u6578\u5b78\u63a8\u7406\u57fa\u6e96\u4e0a\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u4ee5 Qwen2.5-32B \u4f5c\u70ba\u57fa\u790e\u6a21\u578b\u7684 T1 \u5728 MATH500\u3001AIME2024 \u548c Omni-math-500 \u4e0a\u512a\u65bc\u6700\u8fd1\u7684 Qwen QwQ-32B-Preview \u6a21\u578b\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7c21\u55ae\u7684\u7b56\u7565\u4f86\u6aa2\u67e5\u63a8\u7406\u64f4\u5c55\uff0c\u5176\u4e2d\u589e\u52a0\u7684\u63a8\u7406\u9810\u7b97\u76f4\u63a5\u5c0e\u81f4 T1 \u7684\u66f4\u597d\u6027\u80fd\uff0c\u800c\u7121\u9700\u4efb\u4f55\u984d\u5916\u7684\u9a57\u8b49\u3002\u6211\u5011\u5c07\u5728 https://github.com/THUDM/T1 \u958b\u6e90 T1 \u6a21\u578b\u548c\u7528\u65bc\u8a13\u7df4\u5b83\u5011\u7684\u6578\u64da\u3002", "author": "Zhenyu Hou et.al.", "authors": "Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, Yuxiao Dong", "id": "2501.11651v1", "paper_url": "http://arxiv.org/abs/2501.11651v1", "repo": "https://github.com/thudm/t1"}}