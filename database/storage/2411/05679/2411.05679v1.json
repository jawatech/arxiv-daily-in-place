{"2411.05679": {"publish_time": "2024-11-08", "title": "Tell What You Hear From What You See -- Video to Audio Generation Through Text", "paper_summary": "The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning.", "paper_summary_zh": "\u8996\u89ba\u548c\u97f3\u8a0a\u5834\u666f\u7684\u5167\u5bb9\u662f\u591a\u65b9\u9762\u7684\uff0c\u4f8b\u5982\u5f71\u7247\u53ef\u4ee5\u8207\u5404\u7a2e\u97f3\u8a0a\u914d\u5c0d\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u56e0\u6b64\uff0c\u5728\u5f71\u7247\u8f49\u97f3\u8a0a\u751f\u6210\u4efb\u52d9\u4e2d\uff0c\u5fc5\u9808\u5f15\u5165\u5c0e\u5f15\u65b9\u6cd5\u4f86\u63a7\u5236\u751f\u6210\u7684\u97f3\u8a0a\u3002\u96d6\u7136\u5f71\u7247\u8f49\u97f3\u8a0a\u751f\u6210\u662f\u4e00\u9805\u6210\u719f\u7684\u751f\u6210\u4efb\u52d9\uff0c\u4f46\u73fe\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9019\u7a2e\u53ef\u63a7\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa VATT\uff0c\u4e00\u500b\u591a\u6a21\u614b\u751f\u6210\u6846\u67b6\uff0c\u5b83\u4ee5\u5f71\u7247\u548c\u4e00\u500b\u9078\u7528\u7684\u6587\u5b57\u63d0\u793a\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u751f\u6210\u97f3\u8a0a\u548c\u97f3\u8a0a\u7684\u9078\u7528\u6587\u5b57\u63cf\u8ff0\u3002\u9019\u7a2e\u6846\u67b6\u6709\u5169\u500b\u512a\u9ede\uff1ai) \u5f71\u7247\u8f49\u97f3\u8a0a\u751f\u6210\u904e\u7a0b\u53ef\u4ee5\u900f\u904e\u6587\u5b57\u9032\u884c\u512a\u5316\u548c\u63a7\u5236\uff0c\u9019\u88dc\u5145\u4e86\u8996\u89ba\u8cc7\u8a0a\u7684\u80cc\u666f\uff0c\u4ee5\u53ca ii) \u9019\u500b\u6a21\u578b\u53ef\u4ee5\u900f\u904e\u751f\u6210\u97f3\u8a0a\u6a19\u984c\u4f86\u5efa\u8b70\u8981\u70ba\u5f71\u7247\u751f\u6210\u4ec0\u9ebc\u97f3\u8a0a\u3002VATT \u5305\u542b\u5169\u500b\u95dc\u9375\u6a21\u7d44\uff1aVATT \u8f49\u63db\u5668\uff0c\u4e00\u500b\u7d93\u904e\u5fae\u8abf\u7684 LLM\uff0c\u9069\u7528\u65bc\u6307\u4ee4\uff0c\u4e26\u5305\u542b\u4e00\u500b\u5c07\u5f71\u7247\u7279\u5fb5\u5c0d\u61c9\u5230 LLM \u5411\u91cf\u7a7a\u9593\u7684\u6295\u5f71\u5c64\uff1b\u4ee5\u53ca VATT \u97f3\u8a0a\uff0c\u4e00\u500b\u8f49\u63db\u5668\uff0c\u5b83\u4f7f\u7528\u53cd\u8986\u5e73\u884c\u89e3\u78bc\u5f9e\u8996\u89ba\u6846\u67b6\u548c\u9078\u7528\u7684\u6587\u5b57\u63d0\u793a\u751f\u6210\u97f3\u8a0a\u8a18\u865f\u3002\u97f3\u8a0a\u8a18\u865f\u7531\u9810\u8a13\u7df4\u7684\u795e\u7d93\u7de8\u89e3\u78bc\u5668\u8f49\u63db\u70ba\u6ce2\u5f62\u3002\u5be6\u9a57\u986f\u793a\uff0c\u7576 VATT \u8207\u73fe\u6709\u7684\u5f71\u7247\u8f49\u97f3\u8a0a\u751f\u6210\u65b9\u6cd5\u5728\u5ba2\u89c0\u6307\u6a19\u4e2d\u9032\u884c\u6bd4\u8f03\u6642\uff0c\u5b83\u5728\u6c92\u6709\u63d0\u4f9b\u97f3\u8a0a\u6a19\u984c\u6642\uff0c\u9054\u5230\u4e86\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\u3002\u7576\u97f3\u8a0a\u6a19\u984c\u4f5c\u70ba\u63d0\u793a\u63d0\u4f9b\u6642\uff0cVATT \u9054\u5230\u4e86\u66f4\u7cbe\u7dfb\u7684\u6548\u80fd\uff08\u6700\u4f4e KLD \u5206\u6578\u70ba 1.41\uff09\u3002\u6b64\u5916\uff0c\u4e3b\u89c0\u7814\u7a76\u986f\u793a\uff0cVATT \u97f3\u8a0a\u5df2\u88ab\u9078\u70ba\u6bd4\u73fe\u6709\u65b9\u6cd5\u751f\u6210\u7684\u97f3\u8a0a\u66f4\u4f73\u7684\u751f\u6210\u97f3\u8a0a\u3002VATT \u80fd\u5920\u900f\u904e\u6587\u5b57\u9032\u884c\u53ef\u63a7\u7684\u5f71\u7247\u8f49\u97f3\u8a0a\u751f\u6210\uff0c\u4e26\u900f\u904e\u97f3\u8a0a\u6a19\u984c\u70ba\u5f71\u7247\u5efa\u8b70\u6587\u5b57\u63d0\u793a\uff0c\u958b\u555f\u4e86\u65b0\u7684\u61c9\u7528\u7a0b\u5f0f\uff0c\u4f8b\u5982\u6587\u5b57\u5f15\u5c0e\u7684\u5f71\u7247\u8f49\u97f3\u8a0a\u751f\u6210\u548c\u5f71\u7247\u8f49\u97f3\u8a0a\u6a19\u984c\u3002", "author": "Xiulong Liu et.al.", "authors": "Xiulong Liu, Kun Su, Eli Shlizerman", "id": "2411.05679v1", "paper_url": "http://arxiv.org/abs/2411.05679v1", "repo": "null"}}