{"2501.05700": {"publish_time": "2025-01-10", "title": "Linguistic Entity Masking to Improve Cross-Lingual Representation of Multilingual Language Models for Low-Resource Languages", "paper_summary": "Multilingual Pre-trained Language models (multiPLMs), trained on the Masked\nLanguage Modelling (MLM) objective are commonly being used for cross-lingual\ntasks such as bitext mining. However, the performance of these models is still\nsuboptimal for low-resource languages (LRLs). To improve the language\nrepresentation of a given multiPLM, it is possible to further pre-train it.\nThis is known as continual pre-training. Previous research has shown that\ncontinual pre-training with MLM and subsequently with Translation Language\nModelling (TLM) improves the cross-lingual representation of multiPLMs.\nHowever, during masking, both MLM and TLM give equal weight to all tokens in\nthe input sequence, irrespective of the linguistic properties of the tokens. In\nthis paper, we introduce a novel masking strategy, Linguistic Entity Masking\n(LEM) to be used in the continual pre-training step to further improve the\ncross-lingual representations of existing multiPLMs. In contrast to MLM and\nTLM, LEM limits masking to the linguistic entity types nouns, verbs and named\nentities, which hold a higher prominence in a sentence. Secondly, we limit\nmasking to a single token within the linguistic entity span thus keeping more\ncontext, whereas, in MLM and TLM, tokens are masked randomly. We evaluate the\neffectiveness of LEM using three downstream tasks, namely bitext mining,\nparallel data curation and code-mixed sentiment analysis using three\nlow-resource language pairs English-Sinhala, English-Tamil, and Sinhala-Tamil.\nExperiment results show that continually pre-training a multiPLM with LEM\noutperforms a multiPLM continually pre-trained with MLM+TLM for all three\ntasks.", "paper_summary_zh": "\u591a\u8a9e\u8a00\u9810\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (multiPLM) \u5728\u906e\u7f69\u8a9e\u8a00\u6a21\u578b (MLM) \u76ee\u6a19\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u901a\u5e38\u7528\u65bc\u8de8\u8a9e\u8a00\u4efb\u52d9\uff0c\u4f8b\u5982\u96d9\u8a9e\u6587\u672c\u6316\u6398\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u7684\u6548\u80fd\u5c0d\u65bc\u4f4e\u8cc7\u6e90\u8a9e\u8a00 (LRL) \u4f86\u8aaa\u4ecd\u7136\u6b21\u4f73\u3002\u82e5\u8981\u6539\u5584\u7d66\u5b9a multiPLM \u7684\u8a9e\u8a00\u8868\u5fb5\uff0c\u53ef\u4ee5\u9032\u4e00\u6b65\u5c0d\u5176\u9032\u884c\u9810\u8a13\u7df4\u3002\u9019\u7a31\u70ba\u6301\u7e8c\u9810\u8a13\u7df4\u3002\u5148\u524d\u7684\u7814\u7a76\u986f\u793a\uff0c\u4f7f\u7528 MLM \u9032\u884c\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u7136\u5f8c\u4f7f\u7528\u7ffb\u8b6f\u8a9e\u8a00\u6a21\u578b (TLM) \u9032\u884c\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u53ef\u4ee5\u6539\u5584 multiPLM \u7684\u8de8\u8a9e\u8a00\u8868\u5fb5\u3002\u7136\u800c\uff0c\u5728\u906e\u7f69\u671f\u9593\uff0cMLM \u548c TLM \u90fd\u6703\u7d66\u4e88\u8f38\u5165\u5e8f\u5217\u4e2d\u7684\u6240\u6709\u7b26\u865f\u76f8\u7b49\u7684\u6b0a\u91cd\uff0c\u800c\u4e0d\u8003\u616e\u7b26\u865f\u7684\u8a9e\u8a00\u7279\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u65b0\u7684\u906e\u7f69\u7b56\u7565\uff0c\u8a9e\u8a00\u5be6\u9ad4\u906e\u7f69 (LEM)\uff0c\u7528\u65bc\u6301\u7e8c\u9810\u8a13\u7df4\u6b65\u9a5f\uff0c\u4ee5\u9032\u4e00\u6b65\u6539\u5584\u73fe\u6709 multiPLM \u7684\u8de8\u8a9e\u8a00\u8868\u5fb5\u3002\u8207 MLM \u548c TLM \u76f8\u6bd4\uff0cLEM \u5c07\u906e\u7f69\u9650\u5236\u5728\u8a9e\u8a00\u5be6\u9ad4\u985e\u578b\u540d\u8a5e\u3001\u52d5\u8a5e\u548c\u547d\u540d\u5be6\u9ad4\uff0c\u9019\u4e9b\u985e\u578b\u5728\u53e5\u5b50\u4e2d\u5177\u6709\u8f03\u9ad8\u7684\u91cd\u8981\u6027\u3002\u5176\u6b21\uff0c\u6211\u5011\u5c07\u906e\u7f69\u9650\u5236\u5728\u8a9e\u8a00\u5be6\u9ad4\u7bc4\u570d\u5167\u7684\u55ae\u4e00\u7b26\u865f\uff0c\u5f9e\u800c\u4fdd\u7559\u66f4\u591a\u5167\u5bb9\uff0c\u800c\u5728 MLM \u548c TLM \u4e2d\uff0c\u7b26\u865f\u6703\u96a8\u6a5f\u906e\u7f69\u3002\u6211\u5011\u4f7f\u7528\u4e09\u500b\u4e0b\u6e38\u4efb\u52d9\u8a55\u4f30 LEM \u7684\u6548\u80fd\uff0c\u5373\u96d9\u8a9e\u6587\u672c\u6316\u6398\u3001\u5e73\u884c\u8cc7\u6599\u6574\u7406\u548c\u4f7f\u7528\u4e09\u5c0d\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u82f1\u8a9e-\u50e7\u4f3d\u7f85\u8a9e\u3001\u82f1\u8a9e-\u6cf0\u7c73\u723e\u8a9e\u548c\u50e7\u4f3d\u7f85\u8a9e-\u6cf0\u7c73\u723e\u8a9e\u9032\u884c\u7684\u4ee3\u78bc\u6df7\u5408\u60c5\u7dd2\u5206\u6790\u3002\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u4f7f\u7528 LEM \u6301\u7e8c\u9810\u8a13\u7df4 multiPLM \u5728\u6240\u6709\u4e09\u500b\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u90fd\u512a\u65bc\u4f7f\u7528 MLM+TLM \u6301\u7e8c\u9810\u8a13\u7df4\u7684 multiPLM\u3002", "author": "Aloka Fernando et.al.", "authors": "Aloka Fernando, Surangika Ranathunga", "id": "2501.05700v1", "paper_url": "http://arxiv.org/abs/2501.05700v1", "repo": "null"}}