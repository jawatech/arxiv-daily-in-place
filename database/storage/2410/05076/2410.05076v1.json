{"2410.05076": {"publish_time": "2024-10-07", "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention", "paper_summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u63a8\u52d5\u5404\u7a2e NLP \u4efb\u52d9\u7684\u986f\u8457\u9032\u5c55\uff0c\u5176\u4e2d\u9577\u8a9e\u5883\u6a21\u578b\u5728\u8655\u7406\u64f4\u5145\u8f38\u5165\u65b9\u9762\u8d8a\u4f86\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0cTransformer \u67b6\u69cb\u6240\u9700\u7684\u64f4\u5145\u9375\u503c (KV) \u5feb\u53d6\u5927\u5c0f\u52a0\u5287\u4e86\u8a18\u61b6\u9ad4\u9650\u5236\uff0c\u7279\u5225\u662f\u5728\u89e3\u78bc\u968e\u6bb5\uff0c\u9020\u6210\u986f\u8457\u7684\u74f6\u9838\u3002\u73fe\u6709\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6a5f\u5236\u65e8\u5728\u89e3\u6c7a\u6b64\u74f6\u9838\uff0c\u4f46\u6709\u5169\u500b\u9650\u5236\uff1a(1) \u5b83\u5011\u901a\u5e38\u7121\u6cd5\u53ef\u9760\u5730\u8b58\u5225\u6700\u76f8\u95dc\u7684\u6ce8\u610f\u529b\u6b0a\u91cd\uff0c\u4ee5\u53ca (2) \u5b83\u5011\u5ffd\u7565\u4e86\u8de8\u9023\u7e8c Transformer \u5c64\u7684\u6b0a\u91cd\u9078\u64c7\u7a7a\u9593\u4e00\u81f4\u6027\uff0c\u9019\u53ef\u80fd\u5c0e\u81f4\u6b0a\u91cd\u9078\u64c7\u6548\u80fd\u4e0b\u964d\u548c\u5927\u91cf\u958b\u92b7\u3002\u672c\u6587\u4ecb\u7d39 TidalDecode\uff0c\u4e00\u7a2e\u900f\u904e\u4f4d\u7f6e\u6301\u7e8c\u7a00\u758f\u6ce8\u610f\u529b\u9032\u884c\u5feb\u901f\u4e14\u6e96\u78ba\u7684 LLM \u89e3\u78bc\u7684\u7c21\u55ae\u4f46\u6709\u6548\u7684\u6f14\u7b97\u6cd5\u548c\u7cfb\u7d71\u3002TidalDecode \u5229\u7528\u73fe\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u6240\u9078\u6b0a\u91cd\u7684\u7a7a\u9593\u4e00\u81f4\u6027\uff0c\u4e26\u5f15\u5165\u5e7e\u500b\u57f7\u884c\u5b8c\u6574\u6ce8\u610f\u529b\u7684\u6b0a\u91cd\u9078\u64c7\u5c64\uff0c\u4ee5\u8b58\u5225\u5177\u6709\u6700\u9ad8\u6ce8\u610f\u529b\u5206\u6578\u7684\u6b0a\u91cd\uff0c\u800c\u6240\u6709\u5176\u4ed6\u5c64\u5247\u5c0d\u9810\u5148\u9078\u53d6\u7684\u6b0a\u91cd\u57f7\u884c\u7a00\u758f\u6ce8\u610f\u529b\u3002\u6b64\u8a2d\u8a08\u4f7f TidalDecode \u80fd\u5920\u5927\u5e45\u6e1b\u5c11\u7a00\u758f\u6ce8\u610f\u529b\u7684\u6b0a\u91cd\u9078\u64c7\u958b\u92b7\uff0c\u800c\u4e0d\u6703\u72a7\u7272\u751f\u6210\u7d50\u679c\u7684\u54c1\u8cea\u3002\u5728\u5404\u7a2e LLM \u548c\u4efb\u52d9\u4e0a\u7684\u8a55\u4f30\u986f\u793a\uff0cTidalDecode \u8207\u5b8c\u6574\u6ce8\u610f\u529b\u65b9\u6cd5\u7684\u751f\u6210\u6548\u80fd\u975e\u5e38\u63a5\u8fd1\uff0c\u540c\u6642\u5c07 LLM \u89e3\u78bc\u5ef6\u9072\u6e1b\u5c11\u591a\u9054 2.1 \u500d\u3002", "author": "Lijie Yang et.al.", "authors": "Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia", "id": "2410.05076v1", "paper_url": "http://arxiv.org/abs/2410.05076v1", "repo": "null"}}