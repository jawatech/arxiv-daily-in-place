{"2412.04757": {"publish_time": "2024-12-06", "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free Dynamic Triangular Attention Pattern", "paper_summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.", "paper_summary_zh": "\u7576\u524d\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u4e8c\u6b21\u8a08\u7b97\u8907\u96dc\u5ea6\u4f7f\u5f97\u4f7f\u7528\u9577\u6587\u8108\u9032\u884c\u63a8\u8ad6\u7684\u6210\u672c\u9ad8\u5f97\u4ee4\u4eba\u671b\u800c\u537b\u6b65\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e00\u6311\u6230\uff0c\u5404\u7a2e\u65b9\u6cd5\u65e8\u5728\u4fdd\u7559\u6587\u8108\u4e2d\u7684\u95dc\u9375\u90e8\u5206\uff0c\u4ee5\u901a\u904e\u9375\u503c (KV) \u58d3\u7e2e\u6216\u7a00\u758f\u6ce8\u610f\u529b (SA) \u6700\u4f73\u903c\u8fd1\u5168\u6ce8\u610f\u529b (FA)\uff0c\u5f9e\u800c\u80fd\u5920\u4ee5\u4e32\u6d41\u65b9\u5f0f\u8655\u7406\u5e7e\u4e4e\u7121\u9650\u7684\u6587\u5b57\u9577\u5ea6\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u96e3\u4ee5\u9054\u5230\u8207 FA \u76f8\u7576\u7684\u6548\u80fd\u6c34\u6e96\uff0c\u7279\u5225\u662f\u5728\u6aa2\u7d22\u4efb\u52d9\u4e2d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c0d\u6ce8\u610f\u529b\u982d\u90e8\u6a21\u5f0f\u7684\u5206\u6790\u8868\u660e\uff0cLLM \u7684\u6ce8\u610f\u529b\u5206\u4f48\u986f\u793a\u51fa\u5f37\u70c8\u7684\u5c40\u90e8\u95dc\u806f\u6027\uff0c\u81ea\u7136\u5730\u53cd\u6620\u4e86\u8f38\u5165\u6587\u8108\u7684\u5340\u584a\u6a5f\u5236\u3002\u6211\u5011\u63d0\u51fa Ltri-LLM \u6846\u67b6\uff0c\u5b83\u5c07 KV \u5206\u6210\u5340\u584a\uff0c\u5c07\u5b83\u5011\u5132\u5b58\u5728\u96e2\u7dda\u7d22\u5f15\u4e2d\uff0c\u4e26\u5c07\u76f8\u95dc KV \u6aa2\u7d22\u5230\u8a18\u61b6\u9ad4\u4e2d\u4ee5\u4f9b\u5404\u7a2e\u67e5\u8a62\u4f7f\u7528\u3002\u5728\u6d41\u884c\u7684\u9577\u6587\u5b57\u57fa\u6e96\u6e2c\u8a66\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cLtri-LLM \u53ef\u4ee5\u5be6\u73fe\u63a5\u8fd1 FA \u7684\u6548\u80fd\uff0c\u540c\u6642\u4fdd\u6301\u9ad8\u6548\u7684\u57fa\u65bc\u4e32\u6d41\u7684\u63a8\u8ad6\u3002", "author": "Hongyin Tang et.al.", "authors": "Hongyin Tang, Di Xiu, Lanrui Wang, Xiurui Geng, Jingang Wang, Xunliang Cai", "id": "2412.04757v1", "paper_url": "http://arxiv.org/abs/2412.04757v1", "repo": "null"}}