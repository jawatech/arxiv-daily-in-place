{"2503.15485": {"publish_time": "2025-03-19", "title": "TULIP: Towards Unified Language-Image Pretraining", "paper_summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "paper_summary_zh": "<paragraph>\u5118\u7ba1\u985e\u4f3c CLIP \u548c SigLIP \u7684\u5716\u50cf-\u6587\u672c\u5c0d\u6bd4\u6a21\u578b\u8fd1\u671f\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u9019\u4e9b\u6a21\u578b\u5728\u9700\u8981\u9ad8\u4fdd\u771f\u5716\u50cf\u7406\u89e3\u7684\u4ee5\u8996\u89ba\u70ba\u4e2d\u5fc3\u7684\u4efb\u52d9\uff08\u4f8b\u5982\u8a08\u6578\u3001\u6df1\u5ea6\u4f30\u8a08\u548c\u7d30\u7c92\u5ea6\u7269\u9ad4\u8b58\u5225\uff09\u4e0a\u5e38\u5e38\u8868\u73fe\u4e0d\u4f73\u3002\u9019\u4e9b\u6a21\u578b\u901a\u904e\u57f7\u884c\u8a9e\u8a00\u5c0d\u9f4a\uff0c\u50be\u5411\u65bc\u512a\u5148\u8003\u616e\u9ad8\u7d1a\u8a9e\u7fa9\u800c\u975e\u8996\u89ba\u7406\u89e3\uff0c\u5f9e\u800c\u524a\u5f31\u4e86\u5b83\u5011\u7684\u5716\u50cf\u7406\u89e3\u80fd\u529b\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4ee5\u8996\u89ba\u70ba\u4e2d\u5fc3\u7684\u6a21\u578b\u64c5\u9577\u8655\u7406\u8996\u89ba\u4fe1\u606f\uff0c\u4f46\u5728\u7406\u89e3\u8a9e\u8a00\u65b9\u9762\u537b\u5b58\u5728\u56f0\u96e3\uff0c\u9650\u5236\u4e86\u5b83\u5011\u5728\u8a9e\u8a00\u9a45\u52d5\u4efb\u52d9\u4e2d\u7684\u9748\u6d3b\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a8\u51fa\u4e86 TULIP\uff0c\u9019\u662f\u4e00\u500b\u958b\u6e90\u7684\u3001\u53ef\u4ee5\u76f4\u63a5\u66ff\u4ee3\u73fe\u6709\u985e\u4f3c CLIP \u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6211\u5011\u7684\u65b9\u6cd5\u5229\u7528\u751f\u6210\u5f0f\u6578\u64da\u589e\u5f37\u3001\u589e\u5f37\u7684\u5716\u50cf-\u5716\u50cf\u548c\u6587\u672c-\u6587\u672c\u5c0d\u6bd4\u5b78\u7fd2\u4ee5\u53ca\u5716\u50cf/\u6587\u672c\u91cd\u5efa\u6b63\u5247\u5316\u4f86\u5b78\u7fd2\u7d30\u7c92\u5ea6\u7684\u8996\u89ba\u7279\u5fb5\uff0c\u540c\u6642\u4fdd\u7559\u5168\u5c40\u8a9e\u7fa9\u5c0d\u9f4a\u3002\u6211\u5011\u7684\u65b9\u6cd5\u64f4\u5c55\u5230\u8d85\u904e 1B \u500b\u53c3\u6578\uff0c\u5728\u591a\u500b\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5747\u512a\u65bc\u73fe\u6709\u7684\u6700\u5148\u9032 (SOTA) \u6a21\u578b\uff0c\u5728 ImageNet-1K \u4e0a\u5efa\u7acb\u4e86\u65b0\u7684 SOTA \u96f6\u6a23\u672c\u6027\u80fd\uff0c\u5728 RxRx1 \u7684\u7dda\u6027\u63a2\u6e2c\u5c11\u91cf\u6a23\u672c\u5206\u985e\u4e2d\uff0c\u76f8\u8f03\u65bc SigLIP \u63d0\u5347\u9ad8\u9054 $2\\times$\uff0c\u4e26\u6539\u9032\u4e86\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u5728 MMVP \u4e0a\u53d6\u5f97\u4e86\u6bd4 SigLIP \u9ad8 $3\\times$ \u4ee5\u4e0a\u7684\u5206\u6578\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc/\u6aa2\u67e5\u9ede\u53ef\u5728 https://tulip-berkeley.github.io \u7372\u5f97\u3002</paragraph>\n", "author": "Zineng Tang et.al.", "authors": "Zineng Tang, Long Lian, Seun Eisape, XuDong Wang, Roei Herzig, Adam Yala, Alane Suhr, Trevor Darrell, David M. Chan", "id": "2503.15485v1", "paper_url": "http://arxiv.org/abs/2503.15485v1", "repo": "null"}}