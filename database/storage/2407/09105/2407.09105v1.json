{"2407.09105": {"publish_time": "2024-07-12", "title": "Enhancing Training Efficiency Using Packing with Flash Attention", "paper_summary": "Padding is often used in tuning LLM models by adding special tokens to\nshorter training examples to match the length of the longest sequence in each\nbatch. While this ensures uniformity for batch processing, it introduces\ninefficiencies by including irrelevant padding tokens in the computation and\nwastes GPU resources. On the other hand, the Hugging Face SFT trainer offers\nthe option to use packing to combine multiple training examples up to the\nmaximum sequence length. This allows for maximal utilization of GPU resources.\nHowever, without proper masking of each packed training example, attention will\nnot be computed correctly when using SFT trainer. We enable and then analyse\npacking and Flash Attention with proper attention masking of each example and\nshow the benefits of this training paradigm.", "paper_summary_zh": "\u586b\u5145\u901a\u5e38\u7528\u65bc\u8abf\u6574 LLM \u6a21\u578b\uff0c\u65b9\u6cd5\u662f\u5728\u8f03\u77ed\u7684\u8a13\u7df4\u7bc4\u4f8b\u4e2d\u52a0\u5165\u7279\u6b8a\u7b26\u865f\uff0c\u4ee5\u7b26\u5408\u6bcf\u500b\u6279\u6b21\u4e2d\u5e8f\u5217\u7684\u6700\u9577\u9577\u5ea6\u3002\u96d6\u7136\u9019\u80fd\u78ba\u4fdd\u6279\u6b21\u8655\u7406\u7684\u4e00\u81f4\u6027\uff0c\u4f46\u5b83\u6703\u5728\u904b\u7b97\u4e2d\u52a0\u5165\u4e0d\u76f8\u95dc\u7684\u586b\u5145\u7b26\u865f\uff0c\u9020\u6210\u975e\u6548\u7387\uff0c\u4e26\u6d6a\u8cbb GPU \u8cc7\u6e90\u3002\u53e6\u4e00\u65b9\u9762\uff0cHugging Face SFT \u8a13\u7df4\u5668\u63d0\u4f9b\u4f7f\u7528\u5c01\u88dd\u9078\u9805\uff0c\u5c07\u591a\u500b\u8a13\u7df4\u7bc4\u4f8b\u7d44\u5408\u6210\u6700\u9577\u5e8f\u5217\u9577\u5ea6\u3002\u9019\u80fd\u8b93 GPU \u8cc7\u6e90\u5f97\u5230\u6700\u5927\u7684\u5229\u7528\u3002\u7136\u800c\uff0c\u82e5\u6c92\u6709\u9069\u7576\u5730\u906e\u853d\u6bcf\u500b\u5c01\u88dd\u7684\u8a13\u7df4\u7bc4\u4f8b\uff0c\u5728\u4f7f\u7528 SFT \u8a13\u7df4\u5668\u6642\uff0c\u6ce8\u610f\u529b\u5c07\u7121\u6cd5\u6b63\u78ba\u5730\u8a08\u7b97\u3002\u6211\u5011\u555f\u7528\u4e26\u5206\u6790\u5c01\u88dd\u548c\u9583\u5149\u6ce8\u610f\u529b\uff0c\u4e26\u9069\u7576\u5730\u906e\u853d\u6bcf\u500b\u7bc4\u4f8b\u7684\u6ce8\u610f\u529b\uff0c\u4e26\u5c55\u793a\u9019\u7a2e\u8a13\u7df4\u7bc4\u4f8b\u7684\u597d\u8655\u3002", "author": "Achintya Kundu et.al.", "authors": "Achintya Kundu, Rhui Dih Lee, Laura Wynter, Raghu Kiran Ganti", "id": "2407.09105v1", "paper_url": "http://arxiv.org/abs/2407.09105v1", "repo": "null"}}