{"2409.10011": {"publish_time": "2024-09-16", "title": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making", "paper_summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5927\u5e45\u63d0\u5347\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\uff0c\u4f46\u5b83\u5011\u5bb9\u6613\u7522\u751f\u4e0d\u6e96\u78ba\u6216\u4e0d\u53ef\u9760\u7684\u56de\u61c9\uff0c\u9019\u73fe\u8c61\u7a31\u70ba\u5e7b\u89ba\u3002\u5728\u5065\u5eb7\u548c\u91ab\u5b78\u7b49\u95dc\u9375\u9818\u57df\uff0c\u9019\u4e9b\u5e7b\u89ba\u53ef\u80fd\u6703\u9020\u6210\u56b4\u91cd\u7684\u98a8\u96aa\u3002\u672c\u8ad6\u6587\u4ecb\u7d39 HALO\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u65e8\u5728\u900f\u904e\u5c08\u6ce8\u65bc\u5075\u6e2c\u548c\u6e1b\u8f15\u5e7b\u89ba\uff0c\u4f86\u63d0\u5347\u91ab\u7642\u554f\u7b54 (QA) \u7cfb\u7d71\u7684\u6e96\u78ba\u6027\u548c\u53ef\u9760\u6027\u3002\u6211\u5011\u7684\u505a\u6cd5\u662f\u4f7f\u7528 LLM \u7522\u751f\u7d66\u5b9a\u67e5\u8a62\u7684\u591a\u500b\u8b8a\u9ad4\uff0c\u4e26\u5f9e\u5916\u90e8\u958b\u653e\u77e5\u8b58\u5eab\u4e2d\u64f7\u53d6\u76f8\u95dc\u8cc7\u8a0a\uff0c\u4ee5\u8c50\u5bcc\u5167\u5bb9\u3002\u6211\u5011\u5229\u7528\u6700\u5927\u908a\u969b\u76f8\u95dc\u6027\u8a55\u5206\u4f86\u512a\u5148\u8655\u7406\u64f7\u53d6\u7684\u5167\u5bb9\uff0c\u7136\u5f8c\u63d0\u4f9b\u7d66 LLM \u4ee5\u7522\u751f\u7b54\u6848\uff0c\u5f9e\u800c\u964d\u4f4e\u5e7b\u89ba\u7684\u98a8\u96aa\u3002LangChain \u7684\u6574\u5408\u9032\u4e00\u6b65\u7c21\u5316\u4e86\u9019\u500b\u6d41\u7a0b\uff0c\u5c0e\u81f4\u958b\u653e\u539f\u59cb\u78bc\u548c\u5546\u696d LLM\uff0c\u4f8b\u5982 Llama-3.1\uff08\u5f9e 44% \u5230 65%\uff09\u548c ChatGPT\uff08\u5f9e 56% \u5230 70%\uff09\u7684\u6e96\u78ba\u6027\u986f\u8457\u4e14\u7a69\u5065\u5730\u63d0\u5347\u3002\u9019\u500b\u67b6\u69cb\u5f37\u8abf\u4e86\u5728\u91ab\u7642\u554f\u7b54\u7cfb\u7d71\u4e2d\u89e3\u6c7a\u5e7b\u89ba\u7684\u91cd\u8981\u6027\uff0c\u6700\u7d42\u6539\u5584\u4e86\u81e8\u5e8a\u6c7a\u7b56\u5236\u5b9a\u548c\u60a3\u8005\u7167\u8b77\u3002\u958b\u653e\u539f\u59cb\u78bc HALO \u53ef\u5728\u4ee5\u4e0b\u7db2\u5740\u53d6\u5f97\uff1ahttps://github.com/ResponsibleAILab/HALO\u3002", "author": "Sumera Anjum et.al.", "authors": "Sumera Anjum, Hanzhi Zhang, Wenjun Zhou, Eun Jin Paek, Xiaopeng Zhao, Yunhe Feng", "id": "2409.10011v1", "paper_url": "http://arxiv.org/abs/2409.10011v1", "repo": "null"}}