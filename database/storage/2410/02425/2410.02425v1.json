{"2410.02425": {"publish_time": "2024-10-03", "title": "LLM-Pilot: Characterize and Optimize Performance of your LLM Inference Services", "paper_summary": "As Large Language Models (LLMs) are rapidly growing in popularity, LLM\ninference services must be able to serve requests from thousands of users while\nsatisfying performance requirements. The performance of an LLM inference\nservice is largely determined by the hardware onto which it is deployed, but\nunderstanding of which hardware will deliver on performance requirements\nremains challenging. In this work we present LLM-Pilot - a first-of-its-kind\nsystem for characterizing and predicting performance of LLM inference services.\nLLM-Pilot performs benchmarking of LLM inference services, under a realistic\nworkload, across a variety of GPUs, and optimizes the service configuration for\neach considered GPU to maximize performance. Finally, using this\ncharacterization data, LLM-Pilot learns a predictive model, which can be used\nto recommend the most cost-effective hardware for a previously unseen LLM.\nCompared to existing methods, LLM-Pilot can deliver on performance requirements\n33% more frequently, whilst reducing costs by 60% on average.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8fc5\u901f\u666e\u53ca\uff0cLLM \u63a8\u8ad6\u670d\u52d9\u5fc5\u9808\u80fd\u5920\u6eff\u8db3\u6548\u80fd\u9700\u6c42\uff0c\u540c\u6642\u8655\u7406\u4f86\u81ea\u6578\u5343\u540d\u4f7f\u7528\u8005\u7684\u8acb\u6c42\u3002LLM \u63a8\u8ad6\u670d\u52d9\u7684\u6548\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u6c7a\u65bc\u5176\u6240\u90e8\u7f72\u7684\u786c\u9ad4\uff0c\u4f46\u8981\u4e86\u89e3\u54ea\u7a2e\u786c\u9ad4\u80fd\u5920\u6eff\u8db3\u6548\u80fd\u9700\u6c42\u4ecd\u7136\u662f\u4e00\u9805\u6311\u6230\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 LLM-Pilot\uff0c\u9019\u662f\u4e00\u500b\u9996\u5275\u7684\u7cfb\u7d71\uff0c\u7528\u65bc\u63cf\u8ff0\u548c\u9810\u6e2c LLM \u63a8\u8ad6\u670d\u52d9\u7684\u6548\u80fd\u3002LLM-Pilot \u5728\u5be6\u969b\u7684\u5de5\u4f5c\u8ca0\u8f09\u4e0b\u5c0d LLM \u63a8\u8ad6\u670d\u52d9\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u4e26\u91dd\u5c0d\u6bcf\u7a2e\u8003\u91cf\u7684 GPU \u6700\u4f73\u5316\u670d\u52d9\u8a2d\u5b9a\uff0c\u4ee5\u6700\u5927\u5316\u6548\u80fd\u3002\u6700\u5f8c\uff0cLLM-Pilot \u4f7f\u7528\u6b64\u63cf\u8ff0\u8cc7\u6599\uff0c\u5b78\u7fd2\u4e00\u500b\u9810\u6e2c\u6a21\u578b\uff0c\u53ef\u7528\u65bc\u5efa\u8b70\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u786c\u9ad4\uff0c\u4ee5\u4f9b\u5148\u524d\u672a\u898b\u7684 LLM \u4f7f\u7528\u3002\u8207\u73fe\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cLLM-Pilot \u80fd\u5920\u66f4\u983b\u7e41\u5730\u6eff\u8db3\u6548\u80fd\u9700\u6c42\uff0c\u540c\u6642\u5e73\u5747\u964d\u4f4e 60% \u7684\u6210\u672c\u3002", "author": "Ma\u0142gorzata \u0141azuka et.al.", "authors": "Ma\u0142gorzata \u0141azuka, Andreea Anghel, Thomas Parnell", "id": "2410.02425v1", "paper_url": "http://arxiv.org/abs/2410.02425v1", "repo": "null"}}