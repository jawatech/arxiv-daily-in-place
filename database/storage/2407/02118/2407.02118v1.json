{"2407.02118": {"publish_time": "2024-07-02", "title": "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "paper_summary": "In recent years, Large Language Models (LLMs) have made significant strides\ntowards Artificial General Intelligence. However, training these models from\nscratch requires substantial computational resources and vast amounts of text\ndata. In this paper, we explore an alternative approach to constructing an LLM\nfor a new language by continually pretraining (CPT) from existing pretrained\nLLMs, instead of using randomly initialized parameters. Based on parallel\nexperiments on 40 model sizes ranging from 40M to 5B parameters, we find that\n1) CPT converges faster and saves significant resources in a scalable manner;\n2) CPT adheres to an extended scaling law derived from Hoffmann et al. (2022)\nwith a joint data-parameter scaling term; 3) The compute-optimal data-parameter\nallocation for CPT markedly differs based on our estimated scaling factors; 4)\nThe effectiveness of transfer at scale is influenced by training duration and\nlinguistic properties, while robust to data replaying, a method that\neffectively mitigates catastrophic forgetting in CPT. We hope our findings\nprovide deeper insights into the transferability of LLMs at scale for the\nresearch community.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4eba\u5de5\u901a\u7528\u667a\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u9032\u5c55\u3002\u7136\u800c\uff0c\u5f9e\u982d\u8a13\u7df4\u9019\u4e9b\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8a08\u7b97\u8cc7\u6e90\u548c\u5927\u91cf\u7684\u6587\u672c\u6578\u64da\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4e00\u7a2e\u66ff\u4ee3\u65b9\u6cd5\uff0c\u901a\u904e\u6301\u7e8c\u9810\u8a13\u7df4\uff08CPT\uff09\u73fe\u6709\u7684\u9810\u8a13\u7df4 LLM\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u96a8\u6a5f\u521d\u59cb\u5316\u7684\u53c3\u6578\uff0c\u4f86\u69cb\u5efa\u4e00\u7a2e\u65b0\u8a9e\u8a00\u7684 LLM\u3002\u57fa\u65bc\u5c0d 40 \u500b\u6a21\u578b\u5927\u5c0f\uff08\u7bc4\u570d\u5f9e 40M \u5230 5B \u53c3\u6578\uff09\u7684\u4e26\u884c\u5be6\u9a57\uff0c\u6211\u5011\u767c\u73fe 1) CPT \u6536\u6582\u5f97\u66f4\u5feb\uff0c\u4e26\u4ee5\u53ef\u64f4\u5c55\u7684\u65b9\u5f0f\u7bc0\u7701\u4e86\u5927\u91cf\u8cc7\u6e90\uff1b2) CPT \u9075\u5faa Hoffmann \u7b49\u4eba\uff082022\uff09\u63a8\u5c0e\u51fa\u7684\u64f4\u5c55\u7e2e\u653e\u5b9a\u5f8b\uff0c\u4e26\u5e36\u6709\u4e00\u500b\u806f\u5408\u6578\u64da\u53c3\u6578\u7e2e\u653e\u9805\uff1b3) \u6839\u64da\u6211\u5011\u4f30\u8a08\u7684\u7e2e\u653e\u56e0\u5b50\uff0cCPT \u7684\u8a08\u7b97\u6700\u4f73\u6578\u64da\u53c3\u6578\u5206\u914d\u986f\u8457\u4e0d\u540c\uff1b4) \u5927\u898f\u6a21\u8f49\u79fb\u7684\u6709\u6548\u6027\u53d7\u8a13\u7df4\u6301\u7e8c\u6642\u9593\u548c\u8a9e\u8a00\u5c6c\u6027\u7684\u5f71\u97ff\uff0c\u540c\u6642\u5c0d\u6578\u64da\u91cd\u653e\u5177\u6709\u9b6f\u68d2\u6027\uff0c\u9019\u662f\u4e00\u7a2e\u6709\u6548\u6e1b\u8f15 CPT \u4e2d\u707d\u96e3\u6027\u907a\u5fd8\u7684\u65b9\u6cd5\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u7684\u767c\u73fe\u80fd\u70ba\u7814\u7a76\u754c\u63d0\u4f9b\u5c0d\u5927\u898f\u6a21 LLM \u53ef\u8f49\u79fb\u6027\u7684\u66f4\u6df1\u5165\u898b\u89e3\u3002", "author": "Wenzhen Zheng et.al.", "authors": "Wenzhen Zheng, Wenbo Pan, Xu Xu, Libo Qin, Li Yue, Ming Zhou", "id": "2407.02118v1", "paper_url": "http://arxiv.org/abs/2407.02118v1", "repo": "null"}}