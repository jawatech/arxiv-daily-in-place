{"2405.21022": {"publish_time": "2024-05-31", "title": "You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet", "paper_summary": "Linear attention mechanisms have gained prominence in causal language models\ndue to their linear computational complexity and enhanced speed. However, the\ninherent decay mechanism in linear attention presents challenges when applied\nto multi-dimensional sequence modeling tasks, such as image processing and\nmulti-modal learning. In these scenarios, the utilization of sequential\nscanning to establish a global receptive field necessitates multiple scans for\nmulti-dimensional data, thereby leading to inefficiencies. This paper\nidentifies the inefficiency caused by a multiplicative linear recurrence and\nproposes an efficient alternative additive linear recurrence to avoid the\nissue, as it can handle multi-dimensional data within a single scan. We further\ndevelop an efficient multi-dimensional sequential modeling framework called\nLightNet based on the new recurrence. Moreover, we present two new\nmulti-dimensional linear relative positional encoding methods, MD-TPE and\nMD-LRPE to enhance the model's ability to discern positional information in\nmulti-dimensional scenarios. Our empirical evaluations across various tasks,\nincluding image classification, image generation, bidirectional language\nmodeling, and autoregressive language modeling, demonstrate the efficacy of\nLightNet, showcasing its potential as a versatile and efficient solution for\nmulti-dimensional sequential modeling.", "paper_summary_zh": "\u7dda\u6027\u6ce8\u610f\u529b\u6a5f\u5236\u7531\u65bc\u5176\u7dda\u6027\u8a08\u7b97\u8907\u96dc\u5ea6\u548c\u589e\u5f37\u7684\u901f\u5ea6\u800c\u5728\u56e0\u679c\u8a9e\u8a00\u6a21\u578b\u4e2d\u7372\u5f97\u4e86\u986f\u8457\u5730\u4f4d\u3002\u7136\u800c\uff0c\u7dda\u6027\u6ce8\u610f\u529b\u4e2d\u7684\u56fa\u6709\u8870\u6e1b\u6a5f\u5236\u5728\u61c9\u7528\u65bc\u591a\u7dad\u5e8f\u5217\u5efa\u6a21\u4efb\u52d9\uff08\u4f8b\u5982\u5716\u50cf\u8655\u7406\u548c\u591a\u6a21\u614b\u5b78\u7fd2\uff09\u6642\u6703\u5e36\u4f86\u6311\u6230\u3002\u5728\u9019\u4e9b\u5834\u666f\u4e2d\uff0c\u5229\u7528\u9806\u5e8f\u6383\u63cf\u4f86\u5efa\u7acb\u5168\u5c40\u611f\u53d7\u91ce\u9700\u8981\u5c0d\u591a\u7dad\u6578\u64da\u9032\u884c\u591a\u6b21\u6383\u63cf\uff0c\u5f9e\u800c\u5c0e\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u78ba\u5b9a\u4e86\u7531\u4e58\u6cd5\u7dda\u6027\u905e\u8ff4\u5f15\u8d77\u7684\u6548\u7387\u4f4e\u4e0b\uff0c\u4e26\u63d0\u51fa\u4e86\u4e00\u7a2e\u6709\u6548\u7684\u66ff\u4ee3\u52a0\u6cd5\u7dda\u6027\u905e\u8ff4\u4f86\u907f\u514d\u9019\u500b\u554f\u984c\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u5728\u4e00\u6b21\u6383\u63cf\u5167\u8655\u7406\u591a\u7dad\u6578\u64da\u3002\u6b64\u5916\uff0c\u6211\u5011\u9084\u57fa\u65bc\u65b0\u7684\u905e\u8ff4\u958b\u767c\u4e86\u4e00\u500b\u540d\u70ba LightNet \u7684\u9ad8\u6548\u591a\u7dad\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5169\u7a2e\u65b0\u7684\u591a\u7dad\u7dda\u6027\u76f8\u5c0d\u4f4d\u7f6e\u7de8\u78bc\u65b9\u6cd5\uff0cMD-TPE \u548c MD-LRPE\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u5728\u591a\u7dad\u5834\u666f\u4e2d\u8fa8\u5225\u4f4d\u7f6e\u4fe1\u606f\u7684\u80fd\u529b\u3002\u6211\u5011\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u7684\u5be6\u8b49\u8a55\u4f30\uff0c\u5305\u62ec\u5716\u50cf\u5206\u985e\u3001\u5716\u50cf\u751f\u6210\u3001\u96d9\u5411\u8a9e\u8a00\u5efa\u6a21\u548c\u81ea\u8ff4\u6b78\u8a9e\u8a00\u5efa\u6a21\uff0c\u8b49\u660e\u4e86 LightNet \u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u70ba\u591a\u7dad\u5e8f\u5217\u5efa\u6a21\u7684\u901a\u7528\u4e14\u9ad8\u6548\u89e3\u6c7a\u65b9\u6848\u7684\u6f5b\u529b\u3002", "author": "Zhen Qin et.al.", "authors": "Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong", "id": "2405.21022v1", "paper_url": "http://arxiv.org/abs/2405.21022v1", "repo": "null"}}