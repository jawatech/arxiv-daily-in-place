{"2411.08334": {"publish_time": "2024-11-13", "title": "Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval", "paper_summary": "Existing multimodal retrieval systems often rely on disjointed models for\nimage comprehension, such as object detectors and caption generators, leading\nto cumbersome implementations and training processes. To overcome this\nlimitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a\ntext retriever with the ability to understand multimodal queries via dynamic\nmodality interaction. Ret-XKnow leverages a partial convolution mechanism to\nfocus on visual information relevant to the given textual query, thereby\nenhancing multimodal query representations. To effectively learn multimodal\ninteraction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset\nautomatically constructed from visual dialogue datasets. Our dataset\nconstruction process ensures that the dialogues are transformed into suitable\ninformation retrieval tasks using a text retriever. We demonstrate that our\napproach not only significantly improves retrieval performance in zero-shot\nsettings but also achieves substantial improvements in fine-tuning scenarios.\nOur code is publicly available: https://github.com/yeongjoonJu/Ret_XKnow.", "paper_summary_zh": "\u73fe\u6709\u7684\u591a\u6a21\u614b\u6aa2\u7d22\u7cfb\u7d71\u901a\u5e38\u4f9d\u8cf4\u65bc\u4e0d\u9023\u8cab\u7684\u6a21\u578b\u4f86\u7406\u89e3\u5f71\u50cf\uff0c\u4f8b\u5982\u7269\u4ef6\u5075\u6e2c\u5668\u548c\u6a19\u984c\u7522\u751f\u5668\uff0c\u5c0e\u81f4\u5be6\u4f5c\u548c\u8a13\u7df4\u904e\u7a0b\u7e41\u7463\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7aef\u5230\u7aef\u7684\u6aa2\u7d22\u7cfb\u7d71 Ret-XKnow\uff0c\u8ce6\u4e88\u6587\u5b57\u6aa2\u7d22\u5668\u900f\u904e\u52d5\u614b\u6a21\u614b\u4e92\u52d5\u4f86\u7406\u89e3\u591a\u6a21\u614b\u67e5\u8a62\u7684\u80fd\u529b\u3002Ret-XKnow \u5229\u7528\u90e8\u5206\u5377\u7a4d\u6a5f\u5236\u4f86\u5c08\u6ce8\u65bc\u8207\u7d66\u5b9a\u6587\u5b57\u67e5\u8a62\u76f8\u95dc\u7684\u8996\u89ba\u8cc7\u8a0a\uff0c\u9032\u800c\u589e\u5f37\u591a\u6a21\u614b\u67e5\u8a62\u8868\u793a\u3002\u70ba\u4e86\u6709\u6548\u5730\u5b78\u7fd2\u591a\u6a21\u614b\u4e92\u52d5\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86\u8996\u89ba\u5c0d\u8a71\u5230\u6aa2\u7d22 (ViD2R) \u8cc7\u6599\u96c6\uff0c\u8a72\u8cc7\u6599\u96c6\u662f\u81ea\u52d5\u5f9e\u8996\u89ba\u5c0d\u8a71\u8cc7\u6599\u96c6\u4e2d\u5efa\u69cb\u7684\u3002\u6211\u5011\u7684\u8cc7\u6599\u96c6\u5efa\u69cb\u904e\u7a0b\u53ef\u78ba\u4fdd\u5c0d\u8a71\u4f7f\u7528\u6587\u5b57\u6aa2\u7d22\u5668\u8f49\u63db\u6210\u5408\u9069\u7684\u8cc7\u8a0a\u6aa2\u7d22\u4efb\u52d9\u3002\u6211\u5011\u8b49\u660e\u6211\u5011\u7684\u505a\u6cd5\u4e0d\u50c5\u986f\u8457\u6539\u5584\u4e86\u96f6\u6b21\u5b78\u7fd2\u8a2d\u5b9a\u4e2d\u7684\u6aa2\u7d22\u6548\u80fd\uff0c\u800c\u4e14\u5728\u5fae\u8abf\u5834\u666f\u4e2d\u4e5f\u7372\u5f97\u4e86\u986f\u8457\u7684\u6539\u5584\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5df2\u516c\u958b\uff1ahttps://github.com/yeongjoonJu/Ret_XKnow\u3002", "author": "Yeong-Joon Ju et.al.", "authors": "Yeong-Joon Ju, Ho-Joong Kim, Seong-Whan Lee", "id": "2411.08334v1", "paper_url": "http://arxiv.org/abs/2411.08334v1", "repo": "https://github.com/yeongjoonju/ret_xknow"}}