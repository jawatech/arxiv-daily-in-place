{"2504.16922": {"publish_time": "2025-04-23", "title": "Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light", "paper_summary": "Many sparse attention mechanisms such as Neighborhood Attention have\ntypically failed to consistently deliver speedup over the self attention\nbaseline. This is largely due to the level of complexity in attention\ninfrastructure, and the rapid evolution of AI hardware architecture. At the\nsame time, many state-of-the-art foundational models, particularly in computer\nvision, are heavily bound by attention, and need reliable sparsity to escape\nthe O(n^2) complexity. In this paper, we study a class of promising sparse\nattention mechanisms that focus on locality, and aim to develop a better\nanalytical model of their performance improvements. We first introduce\nGeneralized Neighborhood Attention (GNA), which can describe sliding window,\nstrided sliding window, and blocked attention. We then consider possible design\nchoices in implementing these approaches, and create a simulator that can\nprovide much more realistic speedup upper bounds for any given setting.\nFinally, we implement GNA on top of a state-of-the-art fused multi-headed\nattention (FMHA) kernel designed for the NVIDIA Blackwell architecture in\nCUTLASS. Our implementation can fully realize the maximum speedup theoretically\npossible in many perfectly block-sparse cases, and achieves an effective\nutilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA\nconfigurations into off-the-shelf generative models, such as Cosmos-7B,\nHunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end\nspeedup on B200 without any fine-tuning. We will open source our simulator and\nBlackwell kernels directly through the NATTEN project.", "paper_summary_zh": "<paragraph>\u8a31\u591a\u7a00\u758f\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4f8b\u5982Neighborhood Attention\uff0c\u901a\u5e38\u672a\u80fd\u6301\u7e8c\u63d0\u4f9b\u6bd4\u81ea\u6ce8\u610f\u529b\u57fa\u6e96\u66f4\u5feb\u7684\u901f\u5ea6\u3002\u9019\u4e3b\u8981\u662f\u7531\u65bc\u6ce8\u610f\u529b\u57fa\u790e\u67b6\u69cb\u7684\u8907\u96dc\u6027\uff0c\u4ee5\u53caAI\u786c\u9ad4\u67b6\u69cb\u7684\u5feb\u901f\u767c\u5c55\u3002\u540c\u6642\uff0c\u8a31\u591a\u6700\u5148\u9032\u7684\u57fa\u790e\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u96fb\u8166\u8996\u89ba\u9818\u57df\uff0c\u90fd\u53d7\u5230\u6ce8\u610f\u529b\u7684\u56b4\u91cd\u9650\u5236\uff0c\u9700\u8981\u53ef\u9760\u7684\u7a00\u758f\u6027\u4f86\u64fa\u812b O(n^2) \u7684\u8907\u96dc\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7814\u7a76\u4e86\u4e00\u985e\u5f88\u6709\u524d\u666f\u7684\u3001\u5c08\u6ce8\u65bc\u5c40\u90e8\u6027\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u65e8\u5728\u958b\u767c\u4e00\u500b\u66f4\u597d\u7684\u5206\u6790\u6a21\u578b\u4f86\u4e86\u89e3\u5b83\u5011\u7684\u6548\u80fd\u63d0\u5347\u3002\u6211\u5011\u9996\u5148\u4ecb\u7d39\u4e86\u5ee3\u7fa9\u9130\u57df\u6ce8\u610f\u529b\uff08GNA\uff09\uff0c\u5b83\u53ef\u4ee5\u63cf\u8ff0\u6ed1\u52d5\u7a97\u53e3\u3001\u8de8\u6b65\u6ed1\u52d5\u7a97\u53e3\u548c\u5206\u584a\u6ce8\u610f\u529b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u8003\u616e\u4e86\u5be6\u73fe\u9019\u4e9b\u65b9\u6cd5\u7684\u53ef\u80fd\u8a2d\u8a08\u9078\u64c7\uff0c\u4e26\u5275\u5efa\u4e86\u4e00\u500b\u6a21\u64ec\u5668\uff0c\u53ef\u4ee5\u70ba\u4efb\u4f55\u7d66\u5b9a\u8a2d\u7f6e\u63d0\u4f9b\u66f4\u5be6\u969b\u7684\u52a0\u901f\u4e0a\u9650\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5728 CUTLASS \u4e2d\u70ba NVIDIA Blackwell \u67b6\u69cb\u8a2d\u8a08\u7684\u6700\u5148\u9032\u7684\u878d\u5408\u591a\u982d\u6ce8\u610f\u529b\uff08FMHA\uff09\u6838\u5fc3\u4e4b\u4e0a\u5be6\u73fe\u4e86 GNA\u3002\u6211\u5011\u7684\u5be6\u73fe\u5728\u8a31\u591a\u5b8c\u7f8e\u584a\u7a00\u758f\u7684\u60c5\u6cc1\u4e0b\u53ef\u4ee5\u5b8c\u5168\u5be6\u73fe\u7406\u8ad6\u4e0a\u6700\u5927\u7684\u52a0\u901f\uff0c\u4e26\u5728 FP16 \u4e2d\u5be6\u73fe\u4e86 1.3 petaFLOPs/\u79d2\u7684\u6709\u6548\u5229\u7528\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u5c07\u5404\u7a2e GNA \u914d\u7f6e\u63d2\u5165\u73fe\u6210\u7684\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u4f8b\u5982 Cosmos-7B\u3001HunyuanVideo \u548c FLUX\uff0c\u4e26\u8868\u660e\u5b83\u53ef\u4ee5\u5728 B200 \u4e0a\u63d0\u4f9b 28% \u5230 46% \u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u800c\u7121\u9700\u4efb\u4f55\u5fae\u8abf\u3002\u6211\u5011\u5c07\u76f4\u63a5\u901a\u904e NATTEN \u5c08\u6848\u958b\u6e90\u6211\u5011\u7684\u6a21\u64ec\u5668\u548c Blackwell \u6838\u5fc3\u3002</paragraph>\n", "author": "Ali Hassani et.al.", "authors": "Ali Hassani, Fengzhe Zhou, Aditya Kane, Jiannan Huang, Chieh-Yun Chen, Min Shi, Steven Walton, Markus Hoehnerbach, Vijay Thakkar, Michael Isaev, Qinsheng Zhang, Bing Xu, Haicheng Wu, Wen-mei Hwu, Ming-Yu Liu, Humphrey Shi", "id": "2504.16922v1", "paper_url": "http://arxiv.org/abs/2504.16922v1", "repo": "null"}}