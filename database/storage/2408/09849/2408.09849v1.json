{"2408.09849": {"publish_time": "2024-08-19", "title": "Importance Weighting Can Help Large Language Models Self-Improve", "paper_summary": "Large language models (LLMs) have shown remarkable capability in numerous\ntasks and applications. However, fine-tuning LLMs using high-quality datasets\nunder external supervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed recently. The typical\nparadigm of LLM self-improvement involves training LLM on self-generated data,\npart of which may be detrimental and should be filtered out due to the unstable\ndata quality. While current works primarily employs filtering strategies based\non answer correctness, in this paper, we demonstrate that filtering out correct\nbut with high distribution shift extent (DSE) samples could also benefit the\nresults of self-improvement. Given that the actual sample distribution is\nusually inaccessible, we propose a new metric called DS weight to approximate\nDSE, inspired by the Importance Weighting methods. Consequently, we integrate\nDS weight with self-consistency to comprehensively filter the self-generated\nsamples and fine-tune the language model. Experiments show that with only a\ntiny valid set (up to 5\\% size of the training set) to compute DS weight, our\napproach can notably promote the reasoning ability of current LLM\nself-improvement methods. The resulting performance is on par with methods that\nrely on external supervision from pre-trained reward models.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a31\u591a\u4efb\u52d9\u548c\u61c9\u7528\u7a0b\u5f0f\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u4f7f\u7528\u5916\u90e8\u76e3\u7763\u5728\u9ad8\u54c1\u8cea\u8cc7\u6599\u96c6\u4e0a\u5fae\u8abf LLM \u4ecd\u662f\u96e3\u4ee5\u8ca0\u64d4\u7684\u6602\u8cb4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0cLLM \u81ea\u6211\u63d0\u5347\u65b9\u6cd5\u6700\u8fd1\u88ab\u71b1\u70c8\u5730\u958b\u767c\u3002LLM \u81ea\u6211\u63d0\u5347\u7684\u5178\u578b\u6a21\u5f0f\u5305\u62ec\u5728\u81ea\u6211\u7522\u751f\u7684\u8cc7\u6599\u4e0a\u8a13\u7df4 LLM\uff0c\u5176\u4e2d\u4e00\u90e8\u5206\u8cc7\u6599\u53ef\u80fd\u662f\u6709\u5bb3\u7684\uff0c\u4e26\u4e14\u7531\u65bc\u4e0d\u7a69\u5b9a\u7684\u8cc7\u6599\u54c1\u8cea\u800c\u61c9\u8a72\u88ab\u904e\u6ffe\u6389\u3002\u96d6\u7136\u76ee\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u63a1\u7528\u57fa\u65bc\u7b54\u6848\u6b63\u78ba\u6027\u7684\u904e\u6ffe\u7b56\u7565\uff0c\u4f46\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u904e\u6ffe\u6389\u6b63\u78ba\u4f46\u5177\u6709\u9ad8\u5206\u4f48\u8f49\u79fb\u7a0b\u5ea6 (DSE) \u7684\u6a23\u672c\u4e5f\u53ef\u4ee5\u4f7f\u81ea\u6211\u63d0\u5347\u7684\u7d50\u679c\u53d7\u76ca\u3002\u9451\u65bc\u5be6\u969b\u6a23\u672c\u5206\u4f48\u901a\u5e38\u7121\u6cd5\u53d6\u5f97\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7a31\u70ba DS \u6b0a\u91cd\u7684\u6307\u6a19\uff0c\u4ee5\u8fd1\u4f3c DSE\uff0c\u9748\u611f\u4f86\u81ea\u91cd\u8981\u6027\u52a0\u6b0a\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5c07 DS \u6b0a\u91cd\u8207\u81ea\u6211\u4e00\u81f4\u6027\u6574\u5408\uff0c\u4ee5\u5168\u9762\u904e\u6ffe\u81ea\u6211\u7522\u751f\u7684\u6a23\u672c\uff0c\u4e26\u5fae\u8abf\u8a9e\u8a00\u6a21\u578b\u3002\u5be6\u9a57\u8868\u660e\uff0c\u50c5\u4f7f\u7528\u4e00\u500b\u5fae\u5c0f\u7684\u6709\u6548\u96c6\u5408\uff08\u8a13\u7df4\u96c6\u5408\u5927\u5c0f\u7684 5%\uff09\u4f86\u8a08\u7b97 DS \u6b0a\u91cd\uff0c\u6211\u5011\u7684\u505a\u6cd5\u53ef\u4ee5\u986f\u8457\u63d0\u5347\u76ee\u524d LLM \u81ea\u6211\u63d0\u5347\u65b9\u6cd5\u7684\u63a8\u7406\u80fd\u529b\u3002\u7522\u751f\u7684\u6548\u80fd\u8207\u4f9d\u8cf4\u9810\u5148\u8a13\u7df4\u7684\u734e\u52f5\u6a21\u578b\u7684\u5916\u90e8\u76e3\u7763\u7684\u65b9\u6cd5\u76f8\u7576\u3002", "author": "Chunyang Jiang et.al.", "authors": "Chunyang Jiang, Chi-min Chan, Wei Xue, Qifeng Liu, Yike Guo", "id": "2408.09849v1", "paper_url": "http://arxiv.org/abs/2408.09849v1", "repo": "null"}}