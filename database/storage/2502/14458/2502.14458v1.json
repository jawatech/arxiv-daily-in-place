{"2502.14458": {"publish_time": "2025-02-20", "title": "Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing", "paper_summary": "We introduce Llamba, a family of efficient recurrent language models\ndistilled from Llama-3.x into the Mamba architecture. The series includes\nLlamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput\nand handle significantly larger batch sizes than Transformer-based models while\nmaintaining comparable benchmark performance. Furthermore, Llamba demonstrates\nthe effectiveness of cross-architecture distillation using MOHAWK (Bick et al.,\n2024), achieving these results with less than 0.1% of the training data\ntypically used for models of similar size. To take full advantage of their\nefficiency, we provide an optimized implementation of Llamba for\nresource-constrained devices such as smartphones and edge platforms, offering a\npractical and memory-efficient alternative to Transformers. Overall, Llamba\nimproves the tradeoff between speed, memory efficiency, and performance, making\nhigh-quality language models more accessible.", "paper_summary_zh": "\u6211\u5011\u63a8\u51fa Llamba\uff0c\u4e00\u7a2e\u9ad8\u6548\u7684\u905e\u8ff4\u8a9e\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u5f9e Llama-3.x \u8403\u53d6\u5230 Mamba \u67b6\u69cb\u4e2d\u3002\u8a72\u7cfb\u5217\u5305\u542b Llamba-1B\u3001Llamba-3B \u548c Llamba-8B\uff0c\u5b83\u5011\u6bd4\u57fa\u65bc Transformer \u7684\u6a21\u578b\u5be6\u73fe\u66f4\u9ad8\u7684\u63a8\u7406\u541e\u5410\u91cf\uff0c\u4e26\u8655\u7406\u986f\u8457\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f\uff0c\u540c\u6642\u4fdd\u6301\u53ef\u6bd4\u8f03\u7684\u57fa\u6e96\u6548\u80fd\u3002\u6b64\u5916\uff0cLlamba \u8b49\u660e\u4e86\u4f7f\u7528 MOHAWK\uff08Bick \u7b49\u4eba\uff0c2024 \u5e74\uff09\u9032\u884c\u8de8\u67b6\u69cb\u8403\u53d6\u7684\u6709\u6548\u6027\uff0c\u5728\u8a13\u7df4\u8cc7\u6599\u4e0d\u5230\u985e\u4f3c\u5927\u5c0f\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u7684 0.1% \u7684\u60c5\u6cc1\u4e0b\u5be6\u73fe\u4e86\u9019\u4e9b\u7d50\u679c\u3002\u70ba\u4e86\u5145\u5206\u5229\u7528\u5176\u6548\u7387\uff0c\u6211\u5011\u70ba Llamba \u63d0\u4f9b\u4e86\u91dd\u5c0d\u8cc7\u6e90\u53d7\u9650\u88dd\u7f6e\uff08\u4f8b\u5982\u667a\u6167\u578b\u624b\u6a5f\u548c\u908a\u7de3\u5e73\u53f0\uff09\u7684\u6700\u4f73\u5316\u5be6\u4f5c\uff0c\u63d0\u4f9b\u5be6\u7528\u4e14\u8a18\u61b6\u9ad4\u6548\u7387\u9ad8\u7684 Transformer \u66ff\u4ee3\u65b9\u6848\u3002\u7e3d\u9ad4\u800c\u8a00\uff0cLlamba \u6539\u5584\u4e86\u901f\u5ea6\u3001\u8a18\u61b6\u9ad4\u6548\u7387\u548c\u6548\u80fd\u4e4b\u9593\u7684\u6b0a\u8861\uff0c\u8b93\u9ad8\u54c1\u8cea\u8a9e\u8a00\u6a21\u578b\u66f4\u6613\u65bc\u53d6\u5f97\u3002", "author": "Aviv Bick et.al.", "authors": "Aviv Bick, Tobias Katsch, Nimit Sohoni, Arjun Desai, Albert Gu", "id": "2502.14458v1", "paper_url": "http://arxiv.org/abs/2502.14458v1", "repo": "null"}}