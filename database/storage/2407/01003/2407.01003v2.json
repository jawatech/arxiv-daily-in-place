{"2407.01003": {"publish_time": "2024-07-01", "title": "Embedded Prompt Tuning: Towards Enhanced Calibration of Pretrained Models for Medical Images", "paper_summary": "Foundation models pre-trained on large-scale data have been widely witnessed\nto achieve success in various natural imaging downstream tasks.\nParameter-efficient fine-tuning (PEFT) methods aim to adapt foundation models\nto new domains by updating only a small portion of parameters in order to\nreduce computational overhead. However, the effectiveness of these PEFT\nmethods, especially in cross-domain few-shot scenarios, e.g., medical image\nanalysis, has not been fully explored. In this work, we facilitate the study of\nthe performance of PEFT when adapting foundation models to medical image\nclassification tasks. Furthermore, to alleviate the limitations of prompt\nintroducing ways and approximation capabilities on Transformer architectures of\nmainstream prompt tuning methods, we propose the Embedded Prompt Tuning (EPT)\nmethod by embedding prompt tokens into the expanded channels. We also find that\nthere are anomalies in the feature space distribution of foundation models\nduring pre-training process, and prompt tuning can help mitigate this negative\nimpact. To explain this phenomenon, we also introduce a novel perspective to\nunderstand prompt tuning: Prompt tuning is a distribution calibrator. And we\nsupport it by analyzing patch-wise scaling and feature separation operations\ncontained in EPT. Our experiments show that EPT outperforms several\nstate-of-the-art fine-tuning methods by a significant margin on few-shot\nmedical image classification tasks, and completes the fine-tuning process\nwithin highly competitive time, indicating EPT is an effective PEFT method. The\nsource code is available at github.com/zuwenqiang/EPT.", "paper_summary_zh": "<paragraph>\u5728\u5927\u898f\u6a21\u8cc7\u6599\u4e0a\u9810\u5148\u8a13\u7df4\u7684\u57fa\u790e\u6a21\u578b\u5df2\u88ab\u5ee3\u6cdb\u8b49\u660e\n\u5728\u5404\u7a2e\u81ea\u7136\u5f71\u50cf\u4e0b\u6e38\u4efb\u52d9\u4e2d\u53d6\u5f97\u6210\u529f\u3002\n\u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\u65e8\u5728\u901a\u904e\u50c5\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u53c3\u6578\u4f86\u9069\u61c9\u57fa\u790e\u6a21\u578b\n\u5230\u65b0\u7db2\u57df\uff0c\u4ee5\u6e1b\u5c11\u904b\u7b97\u958b\u92b7\u3002\u7136\u800c\uff0c\u9019\u4e9b PEFT \u7684\u6709\u6548\u6027\n\u65b9\u6cd5\uff0c\u7279\u5225\u662f\u5728\u8de8\u7db2\u57df\u5c11\u6b21\u62cd\u651d\u5834\u666f\u4e2d\uff0c\u4f8b\u5982\u91ab\u5b78\u5f71\u50cf\n\u5206\u6790\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u8a0e\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u4fc3\u9032\u4e86\n\u7814\u7a76 PEFT \u5728\u9069\u61c9\u57fa\u790e\u6a21\u578b\u5230\u91ab\u5b78\u5f71\u50cf\u6642\u7684\u6548\u679c\n\u5206\u985e\u4efb\u52d9\u3002\u6b64\u5916\uff0c\u70ba\u4e86\u7de9\u89e3\u63d0\u793a\u7684\u9650\u5236\n\u5728\u4e3b\u6d41\u63d0\u793a\u8abf\u6574\u65b9\u6cd5\u7684 Transformer \u67b6\u69cb\u4e0a\u5f15\u5165\u65b9\u5f0f\u548c\u8fd1\u4f3c\u80fd\u529b\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u5d4c\u5165\u5f0f\u63d0\u793a\u8abf\u6574 (EPT)\n\u65b9\u6cd5\u662f\u5c07\u63d0\u793a\u4ee3\u5e63\u5d4c\u5165\u5230\u64f4\u5c55\u7684\u901a\u9053\u4e2d\u3002\u6211\u5011\u9084\u767c\u73fe\n\u5728\u9810\u8a13\u7df4\u904e\u7a0b\u4e2d\uff0c\u57fa\u790e\u6a21\u578b\u7684\u7279\u5fb5\u7a7a\u9593\u5206\u4f48\u5b58\u5728\u7570\u5e38\uff0c\u4e26\u4e14\u63d0\u793a\u8abf\u6574\u53ef\u4ee5\u5e6b\u52a9\u6e1b\u8f15\u9019\u7a2e\u8ca0\u9762\n\u5f71\u97ff\u3002\u70ba\u4e86\u89e3\u91cb\u9019\u7a2e\u73fe\u8c61\uff0c\u6211\u5011\u9084\u5f15\u5165\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u89c0\u9ede\u4f86\n\u4e86\u89e3\u63d0\u793a\u8abf\u6574\uff1a\u63d0\u793a\u8abf\u6574\u662f\u4e00\u500b\u5206\u4f48\u6821\u6e96\u5668\u3002\u6211\u5011\n\u901a\u904e\u5206\u6790 EPT \u4e2d\u5305\u542b\u7684\u88dc\u4e01\u5f0f\u7e2e\u653e\u548c\u7279\u5fb5\u5206\u96e2\u64cd\u4f5c\u4f86\u652f\u6301\u5b83\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cEPT \u5728\u5c11\u6b21\u62cd\u651d\u4e2d\u512a\u65bc\u5e7e\u7a2e\u6700\u5148\u9032\u7684\u5fae\u8abf\u65b9\u6cd5\n\u91ab\u5b78\u5f71\u50cf\u5206\u985e\u4efb\u52d9\u7684\u908a\u969b\uff0c\u4e26\u5728\u6975\u5177\u7af6\u722d\u529b\u7684\u6642\u9593\u5167\u5b8c\u6210\u5fae\u8abf\u904e\u7a0b\uff0c\u8868\u660e EPT \u662f\u4e00\u7a2e\u6709\u6548\u7684 PEFT \u65b9\u6cd5\u3002\n\u6e90\u4ee3\u78bc\u53ef\u5728 github.com/zuwenqiang/EPT \u4e2d\u7372\u5f97\u3002</paragraph>", "author": "Wenqiang Zu et.al.", "authors": "Wenqiang Zu, Shenghao Xie, Qing Zhao, Guoqi Li, Lei Ma", "id": "2407.01003v2", "paper_url": "http://arxiv.org/abs/2407.01003v2", "repo": "null"}}