{"2411.17685": {"publish_time": "2024-11-26", "title": "Attamba: Attending To Multi-Token States", "paper_summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.", "paper_summary_zh": "\u5728\u9884\u6d4b\u5e8f\u5217\u4e2d\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u65f6\uff0c\u9999\u8349\u8f6c\u6362\u5668\u4f1a\u8ba1\u7b97\u5bf9\u6240\u6709\u5148\u524d\u6807\u8bb0\u7684\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u5bfc\u81f4\u8ba1\u7b97\u4e0e\u5e8f\u5217\u957f\u5ea6\u7684\u4e8c\u6b21\u7f29\u653e\u3002\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5c06\u6574\u4e2a\u6807\u8bb0\u5e8f\u5217\u538b\u7f29\u6210\u4e00\u4e2a\u56fa\u5b9a\u7ef4\u5ea6\u7684\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u800c\u5176\u4ed6\u67b6\u6784\u901a\u8fc7\u4f4e\u79e9\u6295\u5f71\u6216\u5e8f\u5217\u4e0a\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5f0f\u5b9e\u73b0\u4e86\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Attamba\uff0c\u8fd9\u662f\u4e00\u79cd\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u538b\u7f29\u6807\u8bb0\u5757\u5e76\u5bf9\u8fd9\u4e9b\u538b\u7f29\u7684\u952e\u503c\u8868\u793a\u5e94\u7528\u6ce8\u610f\u529b\u7684\u65b0\u9896\u67b6\u6784\u3002\u6211\u4eec\u53d1\u73b0\u7528 SSM \u66ff\u6362\u53d8\u538b\u5668\u4e2d\u7684\u952e\u548c\u503c\u6295\u5f71\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\u5e76\u5b9e\u73b0\u7075\u6d3b\u7684\u6807\u8bb0\u5757\u5212\u5206\uff0c\u4ece\u800c\u4f7f\u5177\u6709\u7c7b\u4f3c KV \u7f13\u5b58\u548c\u6ce8\u610f\u529b\u5360\u7528\u7a7a\u95f4\u7684\u53d8\u538b\u5668\u7684\u56f0\u60d1\u5ea6\u63d0\u9ad8\u4e86 24%\uff0c\u800c KV \u7f13\u5b58\u548c\u6ce8\u610f\u529b FLOP \u5219\u5c0f\u7ea6 4 \u500d\uff0c\u4ee5\u5b9e\u73b0 5% \u7684\u56f0\u60d1\u5ea6\u6743\u8861\u3002Attamba \u53ef\u4ee5\u5728\u53ef\u53d8\u957f\u5ea6\u7684\u5757\u5e8f\u5217\u4e0a\u6267\u884c\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e8c\u6b21\u7f29\u653e\u548c\u7ebf\u6027\u7f29\u653e\u4e4b\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\uff0c\u63d0\u4f9b\u53ef\u9002\u5e94\u7684\u6548\u7387\u63d0\u5347\u3002", "author": "Yash Akhauri et.al.", "authors": "Yash Akhauri, Safeen Huda, Mohamed S. Abdelfattah", "id": "2411.17685v1", "paper_url": "http://arxiv.org/abs/2411.17685v1", "repo": "null"}}