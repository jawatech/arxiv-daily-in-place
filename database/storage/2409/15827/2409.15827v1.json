{"2409.15827": {"publish_time": "2024-09-24", "title": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability", "paper_summary": "As large language models (LLMs) become advance in their linguistic capacity,\nunderstanding how they capture aspects of language competence remains a\nsignificant challenge. This study therefore employs psycholinguistic paradigms,\nwhich are well-suited for probing deeper cognitive aspects of language\nprocessing, to explore neuron-level representations in language model across\nthree tasks: sound-shape association, sound-gender association, and implicit\ncausality. Our findings indicate that while GPT-2-XL struggles with the\nsound-shape task, it demonstrates human-like abilities in both sound-gender\nassociation and implicit causality. Targeted neuron ablation and activation\nmanipulation reveal a crucial relationship: when GPT-2-XL displays a linguistic\nability, specific neurons correspond to that competence; conversely, the\nabsence of such an ability indicates a lack of specialized neurons. This study\nis the first to utilize psycholinguistic experiments to investigate deep\nlanguage competence at the neuron level, providing a new level of granularity\nin model interpretability and insights into the internal mechanisms driving\nlanguage ability in transformer based LLMs.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8a9e\u8a00\u80fd\u529b\u4e0a\u7684\u9032\u6b65\uff0c\n\u4e86\u89e3\u5b83\u5011\u5982\u4f55\u6355\u6349\u8a9e\u8a00\u80fd\u529b\u7684\u5404\u500b\u9762\u5411\u4ecd\u7136\u662f\u4e00\u9805\u91cd\u5927\u7684\u6311\u6230\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u63a1\u7528\u5fc3\u7406\u8a9e\u8a00\u5b78\u7bc4\u4f8b\uff0c\n\u9019\u975e\u5e38\u9069\u5408\u63a2\u8a0e\u8a9e\u8a00\u8655\u7406\u7684\u66f4\u6df1\u5c64\u8a8d\u77e5\u9762\u5411\uff0c\u4ee5\u63a2\u7d22\u8a9e\u8a00\u6a21\u578b\u5728\u4ee5\u4e0b\u4e09\u9805\u4efb\u52d9\u4e2d\u7684\u795e\u7d93\u5143\u5c64\u7d1a\u8868\u5fb5\uff1a\u97f3\u5f62\u806f\u60f3\u3001\u97f3\u6027\u806f\u60f3\u548c\u5167\u96b1\u56e0\u679c\u95dc\u4fc2\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u5118\u7ba1 GPT-2-XL \u5728\u97f3\u5f62\u4efb\u52d9\u4e2d\u8868\u73fe\u4e0d\u4f73\uff0c\u4f46\u5728\u97f3\u6027\u806f\u60f3\u548c\u5167\u96b1\u56e0\u679c\u95dc\u4fc2\u4e2d\u90fd\u5c55\u73fe\u51fa\u985e\u4f3c\u4eba\u985e\u7684\u80fd\u529b\u3002\u91dd\u5c0d\u795e\u7d93\u5143\u7684\u6d88\u878d\u548c\u6d3b\u5316\u64cd\u4f5c\u63ed\u793a\u4e86\u4e00\u500b\u95dc\u9375\u95dc\u4fc2\uff1a\u7576 GPT-2-XL \u5c55\u73fe\u51fa\u8a9e\u8a00\u80fd\u529b\u6642\uff0c\u7279\u5b9a\u795e\u7d93\u5143\u6703\u5c0d\u61c9\u5230\u8a72\u80fd\u529b\uff1b\u53cd\u4e4b\uff0c\u7f3a\u4e4f\u9019\u7a2e\u80fd\u529b\u8868\u793a\u7f3a\u4e4f\u5c08\u9580\u7684\u795e\u7d93\u5143\u3002\u672c\u7814\u7a76\u9996\u6b21\u5229\u7528\u5fc3\u7406\u8a9e\u8a00\u5b78\u5be6\u9a57\u4f86\u63a2\u8a0e\u795e\u7d93\u5143\u5c64\u7d1a\u7684\u6df1\u5ea6\u8a9e\u8a00\u80fd\u529b\uff0c\u5728\u6a21\u578b\u53ef\u89e3\u91cb\u6027\u548c\u898b\u89e3\u65b9\u9762\u63d0\u4f9b\u4e86\u65b0\u7684\u7d30\u7dfb\u7a0b\u5ea6\uff0c\u6df1\u5165\u63a2\u8a0e\u4e86\u57fa\u65bc Transformer \u7684 LLM \u4e2d\u9a45\u52d5\u8a9e\u8a00\u80fd\u529b\u7684\u5167\u90e8\u6a5f\u5236\u3002", "author": "Xufeng Duan et.al.", "authors": "Xufeng Duan, Xinyu Zhou, Bei Xiao, Zhenguang G. Cai", "id": "2409.15827v1", "paper_url": "http://arxiv.org/abs/2409.15827v1", "repo": "null"}}