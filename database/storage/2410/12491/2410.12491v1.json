{"2410.12491": {"publish_time": "2024-10-16", "title": "Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL", "paper_summary": "Large language models (LLMs) trained with Reinforcement Learning from Human\nFeedback (RLHF) have demonstrated remarkable capabilities, but their underlying\nreward functions and decision-making processes remain opaque. This paper\nintroduces a novel approach to interpreting LLMs by applying inverse\nreinforcement learning (IRL) to recover their implicit reward functions. We\nconduct experiments on toxicity-aligned LLMs of varying sizes, extracting\nreward models that achieve up to 80.40% accuracy in predicting human\npreferences. Our analysis reveals key insights into the non-identifiability of\nreward functions, the relationship between model size and interpretability, and\npotential pitfalls in the RLHF process. We demonstrate that IRL-derived reward\nmodels can be used to fine-tune new LLMs, resulting in comparable or improved\nperformance on toxicity benchmarks. This work provides a new lens for\nunderstanding and improving LLM alignment, with implications for the\nresponsible development and deployment of these powerful systems.", "paper_summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7ecf\u8fc7\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u5176\u5e95\u5c42\u5956\u52b1\u51fd\u6570\u548c\u51b3\u7b56\u8fc7\u7a0b\u4ecd\u7136\u4e0d\u900f\u660e\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u901a\u8fc7\u5e94\u7528\u9006\u5411\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u6765\u89e3\u91ca LLM \u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u6062\u590d\u5176\u9690\u5f0f\u5956\u52b1\u51fd\u6570\u3002\u6211\u4eec\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684\u6bd2\u6027\u5bf9\u9f50 LLM \u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u63d0\u53d6\u4e86\u5956\u52b1\u6a21\u578b\uff0c\u5176\u5728\u9884\u6d4b\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe 80.40% \u7684\u51c6\u786e\u5ea6\u3002\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5956\u52b1\u51fd\u6570\u4e0d\u53ef\u8bc6\u522b\u7684\u5173\u952e\u89c1\u89e3\u3001\u6a21\u578b\u5927\u5c0f\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u4ee5\u53ca RLHF \u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u7f3a\u9677\u3002\u6211\u4eec\u8bc1\u660e\u4e86 IRL \u884d\u751f\u7684\u5956\u52b1\u6a21\u578b\u53ef\u7528\u4e8e\u5fae\u8c03\u65b0\u7684 LLM\uff0c\u4ece\u800c\u5728\u6bd2\u6027\u57fa\u51c6\u4e0a\u4ea7\u751f\u53ef\u6bd4\u6216\u6539\u8fdb\u7684\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb LLM \u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\uff0c\u5bf9\u8fd9\u4e9b\u5f3a\u5927\u7cfb\u7edf\u7684\u8d1f\u8d23\u4efb\u5f00\u53d1\u548c\u90e8\u7f72\u5177\u6709\u5f71\u54cd\u3002", "author": "Jared Joselowitz et.al.", "authors": "Jared Joselowitz, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo", "id": "2410.12491v1", "paper_url": "http://arxiv.org/abs/2410.12491v1", "repo": "null"}}