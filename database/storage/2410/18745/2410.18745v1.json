{"2410.18745": {"publish_time": "2024-10-24", "title": "Why Does the Effective Context Length of LLMs Fall Short?", "paper_summary": "Advancements in distributed training and efficient attention mechanisms have\nsignificantly expanded the context window sizes of large language models\n(LLMs). However, recent work reveals that the effective context lengths of\nopen-source LLMs often fall short, typically not exceeding half of their\ntraining lengths. In this work, we attribute this limitation to the left-skewed\nfrequency distribution of relative positions formed in LLMs pretraining and\npost-training stages, which impedes their ability to effectively gather distant\ninformation. To address this challenge, we introduce ShifTed Rotray position\nembeddING (STRING). STRING shifts well-trained positions to overwrite the\noriginal ineffective positions during inference, enhancing performance within\ntheir existing training lengths. Experimental results show that without\nadditional training, STRING dramatically improves the performance of the latest\nlarge-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on\npopular long-context benchmarks RULER and InfiniteBench, establishing new\nstate-of-the-art results for open-source LLMs. Compared to commercial models,\nLlama 3.1 70B with \\method even achieves better performance than GPT-4-128K and\nclearly surpasses Claude 2 and Kimi-chat.", "paper_summary_zh": "\u5206\u5e03\u5f0f\u8bad\u7ec3\u548c\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u7684\u8fdb\u6b65\u663e\u8457\u6269\u5c55\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\u3002\u7136\u800c\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5f00\u6e90 LLM \u7684\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u901a\u5e38\u4e0d\u8db3\uff0c\u901a\u5e38\u4e0d\u8d85\u8fc7\u5176\u8bad\u7ec3\u957f\u5ea6\u7684\u4e00\u534a\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5c06\u6b64\u9650\u5236\u5f52\u56e0\u4e8e LLM \u9884\u8bad\u7ec3\u548c\u8bad\u7ec3\u540e\u9636\u6bb5\u5f62\u6210\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7684\u5de6\u504f\u9891\u7387\u5206\u5e03\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u6709\u6548\u6536\u96c6\u8fdc\u8ddd\u79bb\u4fe1\u606f\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ShifTed Rotray \u4f4d\u7f6e\u5d4c\u5165 (STRING)\u3002STRING \u5728\u63a8\u7406\u671f\u95f4\u5c06\u8bad\u7ec3\u826f\u597d\u7684\u4f4d\u7f6e\u8f6c\u79fb\u5230\u8986\u76d6\u539f\u59cb\u65e0\u6548\u4f4d\u7f6e\uff0c\u4ece\u800c\u5728\u73b0\u6709\u8bad\u7ec3\u957f\u5ea6\u5185\u589e\u5f3a\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTRING \u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06 Llama3.1 70B \u548c Qwen2 72B \u7b49\u6700\u65b0\u7684\u5927\u89c4\u6a21\u6a21\u578b\u5728\u6d41\u884c\u7684\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6 RULER \u548c InfiniteBench \u4e0a\u7684\u6027\u80fd\u63d0\u9ad8\u4e86 10 \u5206\u4ee5\u4e0a\uff0c\u4e3a\u5f00\u6e90 LLM \u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u4e0e\u5546\u4e1a\u6a21\u578b\u76f8\u6bd4\uff0c\u91c7\u7528 \\method \u7684 Llama 3.1 70B \u751a\u81f3\u6bd4 GPT-4-128K \u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u660e\u663e\u8d85\u8d8a\u4e86 Claude 2 \u548c Kimi-chat\u3002", "author": "Chenxin An et.al.", "authors": "Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, Lingpeng Kong", "id": "2410.18745v1", "paper_url": "http://arxiv.org/abs/2410.18745v1", "repo": "null"}}