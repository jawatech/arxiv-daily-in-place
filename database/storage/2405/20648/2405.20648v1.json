{"2405.20648": {"publish_time": "2024-05-31", "title": "Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization", "paper_summary": "Video is an increasingly prominent and information-dense medium, yet it poses\nsubstantial challenges for language models. A typical video consists of a\nsequence of shorter segments, or shots, that collectively form a coherent\nnarrative. Each shot is analogous to a word in a sentence where multiple data\nstreams of information (such as visual and auditory data) must be processed\nsimultaneously. Comprehension of the entire video requires not only\nunderstanding the visual-audio information of each shot but also requires that\nthe model links the ideas between each shot to generate a larger,\nall-encompassing story. Despite significant progress in the field, current\nworks often overlook videos' more granular shot-by-shot semantic information.\nIn this project, we propose a family of efficient large language vision models\n(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By\nleveraging better pretraining and data collection strategies, we extend the\nabilities of existing small LLVMs from being able to understand a picture to\nbeing able to understand a sequence of frames. Specifically, we show that\nShotluck Holmes achieves better performance than state-of-the-art results on\nthe Shot2Story video captioning and summary task with significantly smaller and\nmore computationally efficient models.", "paper_summary_zh": "\u5f71\u7247\u662f\u4e00\u7a2e\u8d8a\u4f86\u8d8a\u986f\u8457\u4e14\u8cc7\u8a0a\u5bc6\u96c6\u7684\u5a92\u4ecb\uff0c\u4f46\u5b83\u5c0d\u8a9e\u8a00\u6a21\u578b\u537b\u69cb\u6210\u91cd\u5927\u7684\u6311\u6230\u3002\u4e00\u90e8\u5178\u578b\u7684\u5f71\u7247\u5305\u542b\u4e00\u7cfb\u5217\u8f03\u77ed\u7684\u7247\u6bb5\u6216\u93e1\u982d\uff0c\u5b83\u5011\u5171\u540c\u5f62\u6210\u4e00\u500b\u9023\u8cab\u7684\u6545\u4e8b\u3002\u6bcf\u500b\u93e1\u982d\u985e\u4f3c\u65bc\u53e5\u5b50\u4e2d\u7684\u55ae\u5b57\uff0c\u5176\u4e2d\u5fc5\u9808\u540c\u6642\u8655\u7406\u591a\u500b\u8cc7\u8a0a\u8cc7\u6599\u4e32\u6d41\uff08\u4f8b\u5982\u8996\u89ba\u548c\u807d\u89ba\u8cc7\u6599\uff09\u3002\u8981\u7406\u89e3\u6574\u90e8\u5f71\u7247\uff0c\u4e0d\u50c5\u9700\u8981\u7406\u89e3\u6bcf\u500b\u93e1\u982d\u7684\u8996\u89ba\u548c\u807d\u89ba\u8cc7\u8a0a\uff0c\u9084\u9700\u8981\u6a21\u578b\u9023\u7d50\u6bcf\u500b\u93e1\u982d\u4e4b\u9593\u7684\u60f3\u6cd5\uff0c\u4ee5\u7522\u751f\u4e00\u500b\u66f4\u5927\u4e14\u5305\u7f85\u842c\u8c61\u7684\u6545\u4e8b\u3002\u5118\u7ba1\u8a72\u9818\u57df\u6709\u986f\u8457\u7684\u9032\u5c55\uff0c\u4f46\u76ee\u524d\u7684\u8457\u4f5c\u7d93\u5e38\u5ffd\u7565\u5f71\u7247\u66f4\u7cbe\u7d30\u7684\u9010\u93e1\u982d\u8a9e\u7fa9\u8cc7\u8a0a\u3002\u5728\u9019\u500b\u5c08\u6848\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6709\u6548\u7387\u7684\u5927\u578b\u8a9e\u8a00\u8996\u89ba\u6a21\u578b (LLVM)\uff0c\u4ee5\u63d0\u5347\u5f71\u7247\u6458\u8981\u548c\u5b57\u5e55\uff0c\u7a31\u70ba Shotluck Holmes\u3002\u900f\u904e\u5229\u7528\u66f4\u597d\u7684\u9810\u8a13\u7df4\u548c\u8cc7\u6599\u6536\u96c6\u7b56\u7565\uff0c\u6211\u5011\u64f4\u5145\u4e86\u73fe\u6709\u5c0f\u578b LLVMs \u7684\u80fd\u529b\uff0c\u5f9e\u80fd\u5920\u7406\u89e3\u5716\u7247\u5230\u80fd\u5920\u7406\u89e3\u4e00\u7cfb\u5217\u7684\u5f71\u683c\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u8b49\u660e Shotluck Holmes \u5728 Shot2Story \u5f71\u7247\u5b57\u5e55\u548c\u6458\u8981\u4efb\u52d9\u4e0a\uff0c\u4ee5\u986f\u8457\u66f4\u5c0f\u4e14\u8a08\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u9054\u5230\u4e86\u6bd4\u6700\u5148\u9032\u7684\u7d50\u679c\u66f4\u597d\u7684\u6548\u80fd\u3002", "author": "Richard Luo et.al.", "authors": "Richard Luo, Austin Peng, Adithya Vasudev, Rishabh Jain", "id": "2405.20648v1", "paper_url": "http://arxiv.org/abs/2405.20648v1", "repo": "null"}}