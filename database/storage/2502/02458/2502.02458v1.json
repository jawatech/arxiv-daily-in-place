{"2502.02458": {"publish_time": "2025-02-04", "title": "SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency", "paper_summary": "Multimodal Large Language Models (MLLMs) mainly fall into two architectures,\neach involving a trade-off between training and inference efficiency: embedding\nspace alignment (e.g., LLaVA-1.5) is inefficient during inference, while\ncross-attention space alignment (e.g., Flamingo) is inefficient in training. In\nthis paper, we compare these two architectures and identify the key factors for\nbuilding efficient MLLMs. A primary difference between them lies in how\nattention is applied to visual tokens, particularly in their interactions with\neach other. To investigate whether attention among visual tokens is necessary,\nwe propose a new self-attention mechanism, NAAViT (\\textbf{N}o\n\\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which\neliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that\nattention among visual tokens is highly redundant. Based on these insights, we\nintroduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace\n\\textbf{A}lignment), a novel architecture that enhance both training and\ninference efficiency. SAISA directly aligns visual features with the input\nspaces of NAAViT self-attention blocks, reducing computational overhead in both\nself-attention blocks and feed-forward networks (FFNs). Using the same\nconfiguration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training\nbudget by 26\\%, while achieving superior performance in terms of accuracy.\nComprehensive ablation studies further validate the effectiveness of SAISA\nacross various LLMs and visual encoders. The code and model will be publicly\navailable at https://github.com/icip-cas/SAISA.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u4e3b\u8981\u5206\u4e3a\u4e24\u79cd\u67b6\u6784\uff0c\n\u6bcf\u79cd\u67b6\u6784\u90fd\u6d89\u53ca\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u5d4c\u5165\n\u7a7a\u95f4\u5bf9\u9f50\uff08\u4f8b\u5982\uff0cLLaVA-1.5\uff09\u5728\u63a8\u7406\u671f\u95f4\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\n\u4ea4\u53c9\u6ce8\u610f\u7a7a\u95f4\u5bf9\u9f50\uff08\u4f8b\u5982\uff0cFlamingo\uff09\u5728\u8bad\u7ec3\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002\u5728\n\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6bd4\u8f83\u4e86\u8fd9\u4e24\u79cd\u67b6\u6784\uff0c\u5e76\u786e\u5b9a\u4e86\u6784\u5efa\u9ad8\u6548 MLLM \u7684\u5173\u952e\u56e0\u7d20\u3002\u5b83\u4eec\u4e4b\u95f4\u7684\u4e3b\u8981\u533a\u522b\u5728\u4e8e\n\u6ce8\u610f\u5982\u4f55\u5e94\u7528\u4e8e\u89c6\u89c9\u6807\u8bb0\uff0c\u5c24\u5176\u662f\u5728\u5b83\u4eec\u76f8\u4e92\u4ea4\u4e92\u65f6\u3002\u4e3a\u4e86\u8c03\u67e5\u89c6\u89c9\u6807\u8bb0\u4e4b\u95f4\u7684\u6ce8\u610f\u662f\u5426\u5fc5\u8981\uff0c\n\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0cNAAViT\uff08\\textbf{N}o\n\\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens\uff09\uff0c\u5b83\u6d88\u9664\u4e86\u8fd9\u79cd\u7c7b\u578b\u7684\u6ce8\u610f\u529b\u3002\u6211\u4eec\u5728 LLaVA-1.5 \u4e0a\u7684\u8bd5\u70b9\u5b9e\u9a8c\u8868\u660e\n\u89c6\u89c9\u6807\u8bb0\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u9ad8\u5ea6\u5197\u4f59\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6211\u4eec\n\u5f15\u5165\u4e86 SAISA\uff08\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace\n\\textbf{A}lignment\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u548c\n\u63a8\u7406\u6548\u7387\u3002SAISA \u76f4\u63a5\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e NAAViT \u81ea\u6ce8\u610f\u529b\u5757\u7684\u8f93\u5165\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u81ea\u6ce8\u610f\u529b\u5757\u548c\u524d\u9988\u7f51\u7edc (FFN) \u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4f7f\u7528\u4e0e LLaVA-1.5 \u76f8\u540c\u7684\u914d\u7f6e\uff0cSAISA \u5c06\u63a8\u7406 FLOP \u51cf\u5c11\u4e86 66%\uff0c\u8bad\u7ec3\n\u9884\u7b97\u51cf\u5c11\u4e86 26%\uff0c\u540c\u65f6\u5728\u51c6\u786e\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\n\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86 SAISA \u5728\u5404\u79cd LLM \u548c\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728 https://github.com/icip-cas/SAISA \u4e0a\u516c\u5f00\u3002", "author": "Qianhao Yuan et.al.", "authors": "Qianhao Yuan, Yanjiang Liu, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun", "id": "2502.02458v1", "paper_url": "http://arxiv.org/abs/2502.02458v1", "repo": "null"}}