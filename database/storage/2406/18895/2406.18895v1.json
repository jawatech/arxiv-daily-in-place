{"2406.18895": {"publish_time": "2024-06-27", "title": "Can we teach language models to gloss endangered languages?", "paper_summary": "Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text can be desirable to reduce\nannotator effort and maintain consistency across annotated corpora. Prior\nresearch has explored a number of statistical and neural methods for\nautomatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with\nin-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art\nsupervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.", "paper_summary_zh": "\u8de8\u8a9e\u8a00\u4efb\u52d9\u4e2d\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u6210\u679c\uff0c\u5373\u4f7f\u662f\u5c0d\u65bc\u7f55\u898b\u6216\u7015\u5371\u8a9e\u8a00\uff0c\u56e0\u6b64\u81ea\u7136\u6703\u597d\u5947\u5b83\u5011\u662f\u5426\u80fd\u7528\u65bc\u7522\u751f IGT \u7684\u4efb\u52d9\u3002\u6211\u5011\u63a2\u8a0e LLM \u662f\u5426\u80fd\u6709\u6548\u57f7\u884c\u8de8\u884c\u8a3b\u91cb\u4efb\u52d9\uff0c\u900f\u904e\u60c5\u5883\u5b78\u7fd2\uff0c\u800c\u7121\u9700\u4efb\u4f55\u50b3\u7d71\u8a13\u7df4\u3002\u6211\u5011\u63d0\u51fa\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u65bc\u9078\u64c7\u7bc4\u4f8b\u4ee5\u63d0\u4f9b\u60c5\u5883\uff0c\u4e26\u89c0\u5bdf\u5230\u76ee\u6a19\u9078\u64c7\u80fd\u986f\u8457\u63d0\u5347\u6548\u80fd\u3002\u6211\u5011\u767c\u73fe\uff0c\u5118\u7ba1 LLM-based \u65b9\u6cd5\u4e0d\u9700\u8981\u4efb\u4f55\u8a13\u7df4\uff0c\u4f46\u4ecd\u52dd\u904e\u6a19\u6e96\u7684 transformer \u57fa\u6e96\u3002\u9019\u4e9b\u65b9\u6cd5\u5728\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u4ecd\u4e0d\u5982\u6700\u5148\u9032\u7684\u76e3\u7763\u5f0f\u7cfb\u7d71\uff0c\u4f46\u5c0d\u65bc NLP \u793e\u7fa4\u4ee5\u5916\u7684\u7814\u7a76\u4eba\u54e1\u4f86\u8aaa\u975e\u5e38\u5be6\u7528\uff0c\u4f7f\u7528\u6642\u6240\u9700\u7684\u52aa\u529b\u6975\u5c11\u3002", "author": "Michael Ginn et.al.", "authors": "Michael Ginn, Mans Hulden, Alexis Palmer", "id": "2406.18895v1", "paper_url": "http://arxiv.org/abs/2406.18895v1", "repo": "null"}}