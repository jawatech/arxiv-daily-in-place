{"2501.03747": {"publish_time": "2025-01-07", "title": "Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series", "paper_summary": "Recently, leveraging pre-trained Large Language Models (LLMs) for time series\n(TS) tasks has gained increasing attention, which involves activating and\nenhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities\nbased on token-level alignment but overlook LLMs' inherent strength on natural\nlanguage processing -- their deep understanding of linguistic logic and\nstructure rather than superficial embedding processing. We propose\nContext-Alignment, a new paradigm that aligns TS with a linguistic component in\nthe language environments familiar to LLMs to enable LLMs to contextualize and\ncomprehend TS data, thereby activating their capabilities. Specifically, such\ncontext-level alignment comprises structural alignment and logical alignment,\nwhich is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to\nTS-language multimodal inputs. Structural alignment utilizes dual-scale nodes\nto describe hierarchical structure in TS-language, enabling LLMs treat long TS\ndata as a whole linguistic component while preserving intrinsic token features.\nLogical alignment uses directed edges to guide logical relationships, ensuring\ncoherence in the contextual semantics. Demonstration examples prompt are\nemployed to construct Demonstration Examples based Context-Alignment (DECA)\nfollowing DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated\ninto various layers of pre-trained LLMs to improve awareness of logic and\nstructure, thereby enhancing performance. Extensive experiments show the\neffectiveness of DECA and the importance of Context-Alignment across tasks,\nparticularly in few-shot and zero-shot forecasting, confirming that\nContext-Alignment provide powerful prior knowledge on context.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u5229\u7528\u9884\u5148\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u6267\u884c\u65f6\u95f4\u5e8f\u5217 (TS) \u4efb\u52a1\u5907\u53d7\u5173\u6ce8\uff0c\u8fd9\u6d89\u53ca\u6fc0\u6d3b\u548c\u589e\u5f3a LLM \u7684\u529f\u80fd\u3002\u8bb8\u591a\u65b9\u6cd5\u65e8\u5728\u57fa\u4e8e\u6807\u8bb0\u7ea7\u522b\u7684\u5bf9\u9f50\u6765\u6fc0\u6d3b LLM \u7684\u529f\u80fd\uff0c\u4f46\u5ffd\u7565\u4e86 LLM \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u7684\u56fa\u6709\u4f18\u52bf\u2014\u2014\u5b83\u4eec\u5bf9\u8bed\u8a00\u903b\u8f91\u548c\u7ed3\u6784\u7684\u6df1\u523b\u7406\u89e3\uff0c\u800c\u4e0d\u662f\u8868\u9762\u7684\u5d4c\u5165\u5f0f\u5904\u7406\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u8bed\u5883\u5bf9\u9f50\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u8303\u4f8b\uff0c\u5b83\u5c06\u65f6\u95f4\u5e8f\u5217\u4e0e LLM \u719f\u6089\u7684\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u8bed\u8a00\u7ec4\u4ef6\u5bf9\u9f50\uff0c\u4f7f LLM \u80fd\u591f\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u8bed\u5883\u5316\u548c\u7406\u89e3\uff0c\u4ece\u800c\u6fc0\u6d3b\u5b83\u4eec\u7684\u529f\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u79cd\u8bed\u5883\u7ea7\u522b\u7684\u5bf9\u9f50\u5305\u62ec\u7ed3\u6784\u5bf9\u9f50\u548c\u903b\u8f91\u5bf9\u9f50\uff0c\u8fd9\u662f\u901a\u8fc7\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u8bed\u8a00\u591a\u6a21\u6001\u8f93\u5165\u7684\u53cc\u5c3a\u5ea6\u8bed\u5883\u5bf9\u9f50 GNN\uff08DSCA-GNN\uff09\u5b9e\u73b0\u7684\u3002\u7ed3\u6784\u5bf9\u9f50\u5229\u7528\u53cc\u5c3a\u5ea6\u8282\u70b9\u6765\u63cf\u8ff0\u65f6\u95f4\u5e8f\u5217\u8bed\u8a00\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4f7f LLM \u80fd\u591f\u5c06\u957f\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4f5c\u4e3a\u4e00\u4e2a\u6574\u4f53\u7684\u8bed\u8a00\u7ec4\u4ef6\u6765\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u7559\u56fa\u6709\u7684\u6807\u8bb0\u7279\u5f81\u3002\u903b\u8f91\u5bf9\u9f50\u4f7f\u7528\u6709\u5411\u8fb9\u6765\u6307\u5bfc\u903b\u8f91\u5173\u7cfb\uff0c\u786e\u4fdd\u8bed\u5883\u8bed\u4e49\u4e2d\u7684\u4e00\u81f4\u6027\u3002\u6f14\u793a\u793a\u4f8b\u63d0\u793a\u88ab\u7528\u6765\u6784\u5efa\u57fa\u4e8e\u8bed\u5883\u5bf9\u9f50\u7684\u6f14\u793a\u793a\u4f8b (DECA)\uff0c\u9075\u5faa DSCA-GNN \u6846\u67b6\u3002DECA \u53ef\u4ee5\u7075\u6d3b\u4e14\u91cd\u590d\u5730\u96c6\u6210\u5230\u9884\u8bad\u7ec3 LLM \u7684\u5404\u4e2a\u5c42\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u903b\u8f91\u548c\u7ed3\u6784\u7684\u8ba4\u8bc6\uff0c\u4ece\u800c\u589e\u5f3a\u6027\u80fd\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\u4e86 DECA \u7684\u6709\u6548\u6027\u4ee5\u53ca\u8bed\u5883\u5bf9\u9f50\u5728\u5404\u4e2a\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\uff0c\u8bc1\u5b9e\u4e86\u8bed\u5883\u5bf9\u9f50\u4e3a\u8bed\u5883\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5148\u9a8c\u77e5\u8bc6\u3002</paragraph>", "author": "Yuxiao Hu et.al.", "authors": "Yuxiao Hu, Qian Li, Dongxiao Zhang, Jinyue Yan, Yuntian Chen", "id": "2501.03747v1", "paper_url": "http://arxiv.org/abs/2501.03747v1", "repo": "null"}}