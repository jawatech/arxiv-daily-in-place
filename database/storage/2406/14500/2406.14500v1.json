{"2406.14500": {"publish_time": "2024-06-20", "title": "Improving Expert Radiology Report Summarization by Prompting Large Language Models with a Layperson Summary", "paper_summary": "Radiology report summarization (RRS) is crucial for patient care, requiring\nconcise \"Impressions\" from detailed \"Findings.\" This paper introduces a novel\nprompting strategy to enhance RRS by first generating a layperson summary. This\napproach normalizes key observations and simplifies complex information using\nnon-expert communication techniques inspired by doctor-patient interactions.\nCombined with few-shot in-context learning, this method improves the model's\nability to link general terms to specific findings. We evaluate this approach\non the MIMIC-CXR, CheXpert, and MIMIC-III datasets, benchmarking it against\n7B/8B parameter state-of-the-art open-source large language models (LLMs) like\nMeta-Llama-3-8B-Instruct. Our results demonstrate improvements in summarization\naccuracy and accessibility, particularly in out-of-domain tests, with\nimprovements as high as 5% for some metrics.", "paper_summary_zh": "\u653e\u5c04\u79d1\u62a5\u544a\u6458\u8981 (RRS) \u5bf9\u4e8e\u60a3\u8005\u62a4\u7406\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4ece\u8be6\u7ec6\u7684\u300c\u7ed3\u679c\u300d\u4e2d\u63d0\u53d6\u7b80\u6d01\u7684\u300c\u5370\u8c61\u300d\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u9996\u5148\u751f\u6210\u5916\u884c\u6458\u8981\u6765\u589e\u5f3a RRS\u3002\u6b64\u65b9\u6cd5\u4f7f\u7528\u53d7\u533b\u60a3\u4e92\u52a8\u542f\u53d1\u7684\u975e\u4e13\u5bb6\u6c9f\u901a\u6280\u5de7\uff0c\u5c06\u5173\u952e\u89c2\u5bdf\u7ed3\u679c\u6807\u51c6\u5316\u5e76\u7b80\u5316\u590d\u6742\u4fe1\u606f\u3002\u7ed3\u5408\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u6b64\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u5c06\u4e00\u822c\u672f\u8bed\u4e0e\u7279\u5b9a\u7ed3\u679c\u8054\u7cfb\u8d77\u6765\u7684\u80fd\u529b\u3002\u6211\u4eec\u5728 MIMIC-CXR\u3001CheXpert \u548c MIMIC-III \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8fd9\u79cd\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e Meta-Llama-3-8B-Instruct \u7b49 7B/8B \u53c2\u6570\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6458\u8981\u51c6\u786e\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u90fd\u6709\u6240\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u57df\u5916\u6d4b\u8bd5\u4e2d\uff0c\u67d0\u4e9b\u6307\u6807\u7684\u6539\u8fdb\u5e45\u5ea6\u9ad8\u8fbe 5%\u3002", "author": "Xingmeng Zhao et.al.", "authors": "Xingmeng Zhao, Tongnian Wang, Anthony Rios", "id": "2406.14500v1", "paper_url": "http://arxiv.org/abs/2406.14500v1", "repo": "null"}}