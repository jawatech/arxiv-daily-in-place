{"2409.02686": {"publish_time": "2024-09-04", "title": "Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs", "paper_summary": "Large Language Models (LLMs) have demonstrated remarkable efficiency in\ntackling various tasks based on human instructions, but recent studies reveal\nthat these models often fail to achieve satisfactory results on questions\ninvolving reasoning, such as mathematics or physics questions. This phenomenon\nis usually attributed to the uncertainty regarding whether these models could\ngenuinely comprehend the knowledge embedded in the text or merely learn to\nreplicate the token distribution without a true understanding of the content.\nIn this paper, we delve into this problem and aim to enhance the reasoning\ncapabilities of LLMs. First, we investigate if the model has genuine reasoning\ncapabilities by visualizing the text generation process at the attention and\nrepresentation level. Then, we formulate the reasoning process of LLMs into a\ncausal framework, which provides a formal explanation of the problems we\nobserve in the visualization. Finally, building upon this causal framework, we\npropose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient\nfine-tuning (PEFT) method to enhance the model's reasoning capabilities by\nencouraging the model to extract the general problem-solving skills and apply\nthese skills to different questions. Experiments show that our method\noutperforms the baseline consistently across multiple benchmarks, and with only\n1.2M tunable parameters, we achieve better or comparable results to other\nfine-tuning methods. This demonstrates the effectiveness and efficiency of our\nmethod in improving the overall accuracy and reliability of LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u8b49\u660e\u5728\u6839\u64da\u4eba\u985e\u6307\u793a\u8655\u7406\u5404\u7a2e\u4efb\u52d9\u65b9\u9762\u5177\u6709\u986f\u8457\u7684\u6548\u7387\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u9019\u4e9b\u6a21\u578b\u5728\u6d89\u53ca\u63a8\u7406\u7684\u554f\u984c\uff08\u4f8b\u5982\u6578\u5b78\u6216\u7269\u7406\u554f\u984c\uff09\u4e0a\u5e38\u5e38\u7121\u6cd5\u7372\u5f97\u4ee4\u4eba\u6eff\u610f\u7684\u7d50\u679c\u3002\u9019\u7a2e\u73fe\u8c61\u901a\u5e38\u6b78\u56e0\u65bc\u4e0d\u78ba\u5b9a\u6027\uff0c\u5373\u9019\u4e9b\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u771f\u6b63\u7406\u89e3\u5d4c\u5165\u5728\u6587\u672c\u4e2d\u7684\u77e5\u8b58\uff0c\u6216\u8005\u50c5\u50c5\u5b78\u6703\u8907\u88fd\u7b26\u865f\u5206\u4f48\u800c\u6c92\u6709\u771f\u6b63\u7406\u89e3\u5167\u5bb9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u9019\u500b\u554f\u984c\uff0c\u4e26\u65e8\u5728\u589e\u5f37 LLM \u7684\u63a8\u7406\u80fd\u529b\u3002\u9996\u5148\uff0c\u6211\u5011\u901a\u904e\u5728\u95dc\u6ce8\u548c\u8868\u793a\u5c64\u9762\u8996\u89ba\u5316\u6587\u672c\u751f\u6210\u904e\u7a0b\uff0c\u4f86\u8abf\u67e5\u6a21\u578b\u662f\u5426\u5177\u6709\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u3002\u7136\u5f8c\uff0c\u6211\u5011\u5c07 LLM \u7684\u63a8\u7406\u904e\u7a0b\u5236\u5b9a\u70ba\u56e0\u679c\u6846\u67b6\uff0c\u9019\u5c0d\u6211\u5011\u5728\u8996\u89ba\u5316\u4e2d\u89c0\u5bdf\u5230\u7684\u554f\u984c\u63d0\u4f9b\u4e86\u6b63\u5f0f\u89e3\u91cb\u3002\u6700\u5f8c\uff0c\u5728\u6b64\u56e0\u679c\u6846\u67b6\u7684\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u53bb\u6df7\u6dc6\u56e0\u679c\u9069\u61c9 (DCA)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u65b9\u6cd5\uff0c\u901a\u904e\u9f13\u52f5\u6a21\u578b\u63d0\u53d6\u4e00\u822c\u554f\u984c\u89e3\u6c7a\u6280\u80fd\u4e26\u5c07\u9019\u4e9b\u6280\u80fd\u61c9\u7528\u65bc\u4e0d\u540c\u7684\u554f\u984c\uff0c\u4f86\u589e\u5f37\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u5be6\u9a57\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u591a\u500b\u57fa\u6e96\u4e0a\u59cb\u7d42\u512a\u65bc\u57fa\u6e96\uff0c\u4e26\u4e14\u50c5\u4f7f\u7528 120 \u842c\u500b\u53ef\u8abf\u53c3\u6578\uff0c\u6211\u5011\u5c31\u7372\u5f97\u4e86\u6bd4\u5176\u4ed6\u5fae\u8abf\u65b9\u6cd5\u66f4\u597d\u6216\u76f8\u7576\u7684\u7d50\u679c\u3002\u9019\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8 LLM \u7684\u6574\u9ad4\u6e96\u78ba\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "author": "Ruoyu Wang et.al.", "authors": "Ruoyu Wang, Xiaoxuan Li, Lina Yao", "id": "2409.02686v1", "paper_url": "http://arxiv.org/abs/2409.02686v1", "repo": "null"}}