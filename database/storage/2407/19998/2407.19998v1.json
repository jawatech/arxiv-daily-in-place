{"2407.19998": {"publish_time": "2024-07-29", "title": "Do LLMs Really Adapt to Domains? An Ontology Learning Perspective", "paper_summary": "Large Language Models (LLMs) have demonstrated unprecedented prowess across\nvarious natural language processing tasks in various application domains.\nRecent studies show that LLMs can be leveraged to perform lexical semantic\ntasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).\nHowever, it has not effectively been verified whether their success is due to\ntheir ability to reason over unstructured or semi-structured data, or their\neffective learning of linguistic patterns and senses alone. This unresolved\nquestion is particularly crucial when dealing with domain-specific data, where\nthe lexical senses and their meaning can completely differ from what a LLM has\nlearned during its training stage. This paper investigates the following\nquestion: Do LLMs really adapt to domains and remain consistent in the\nextraction of structured knowledge, or do they only learn lexical senses\ninstead of reasoning? To answer this question and, we devise a controlled\nexperiment setup that uses WordNet to synthesize parallel corpora, with English\nand gibberish terms. We examine the differences in the outputs of LLMs for each\ncorpus in two OL tasks: relation extraction and taxonomy discovery. Empirical\nresults show that, while adapting to the gibberish corpora, off-the-shelf LLMs\ndo not consistently reason over semantic relationships between concepts, and\ninstead leverage senses and their frame. However, fine-tuning improves the\nperformance of LLMs on lexical semantic tasks even when the domain-specific\nterms are arbitrary and unseen during pre-training, hinting at the\napplicability of pre-trained LLMs for OL.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u61c9\u7528\u9818\u57df\u7684\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u524d\u6240\u672a\u6709\u7684\u5be6\u529b\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cLLM \u53ef\u7528\u65bc\u57f7\u884c\u8a5e\u5f59\u8a9e\u7fa9\u4efb\u52d9\uff0c\u4f8b\u5982\u77e5\u8b58\u5eab\u5b8c\u6210 (KBC) \u6216\u672c\u4f53\u5b78\u7fd2 (OL)\u3002\u7136\u800c\uff0c\u5c1a\u672a\u6709\u6548\u9a57\u8b49\u5176\u6210\u529f\u662f\u6b78\u56e0\u65bc\u5b83\u5011\u5c0d\u975e\u7d50\u69cb\u5316\u6216\u534a\u7d50\u69cb\u5316\u6578\u64da\u9032\u884c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u9084\u662f\u50c5\u6b78\u56e0\u65bc\u5b83\u5011\u5c0d\u8a9e\u8a00\u6a21\u5f0f\u548c\u610f\u7fa9\u7684\u6709\u6548\u5b78\u7fd2\u3002\u5728\u8655\u7406\u7279\u5b9a\u9818\u57df\u7684\u6578\u64da\u6642\uff0c\u9019\u500b\u672a\u89e3\u6c7a\u7684\u554f\u984c\u5c24\u5176\u95dc\u9375\uff0c\u5176\u4e2d\u8a5e\u5f59\u610f\u7fa9\u53ca\u5176\u542b\u7fa9\u53ef\u80fd\u8207 LLM \u5728\u8a13\u7df4\u968e\u6bb5\u6240\u5b78\u7fd2\u7684\u5167\u5bb9\u5b8c\u5168\u4e0d\u540c\u3002\u672c\u6587\u63a2\u8a0e\u4ee5\u4e0b\u554f\u984c\uff1aLLM \u662f\u5426\u771f\u7684\u9069\u61c9\u4e86\u9818\u57df\u4e26\u5728\u7d50\u69cb\u5316\u77e5\u8b58\u7684\u63d0\u53d6\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u9084\u662f\u5b83\u5011\u53ea\u5b78\u7fd2\u8a5e\u5f59\u610f\u7fa9\u800c\u4e0d\u662f\u63a8\u7406\uff1f\u70ba\u4e86\u56de\u7b54\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u53d7\u63a7\u5be6\u9a57\u8a2d\u7f6e\uff0c\u8a72\u8a2d\u7f6e\u4f7f\u7528 WordNet \u4f86\u5408\u6210\u5e73\u884c\u8a9e\u6599\u5eab\uff0c\u5176\u4e2d\u5305\u542b\u82f1\u8a9e\u548c\u80e1\u8a00\u4e82\u8a9e\u8853\u8a9e\u3002\u6211\u5011\u6aa2\u67e5\u4e86\u6bcf\u500b\u8a9e\u6599\u5eab\u4e2d LLM \u8f38\u51fa\u7684\u5dee\u7570\uff0c\u9019\u5728\u5169\u500b OL \u4efb\u52d9\u4e2d\uff1a\u95dc\u4fc2\u63d0\u53d6\u548c\u5206\u985e\u767c\u73fe\u3002\u5be6\u8b49\u7d50\u679c\u8868\u660e\uff0c\u5728\u9069\u61c9\u80e1\u8a00\u4e82\u8a9e\u8a9e\u6599\u5eab\u7684\u540c\u6642\uff0c\u73fe\u6210\u7684 LLM \u6c92\u6709\u5c0d\u6982\u5ff5\u4e4b\u9593\u7684\u8a9e\u7fa9\u95dc\u4fc2\u9032\u884c\u4e00\u81f4\u7684\u63a8\u7406\uff0c\u800c\u662f\u5229\u7528\u610f\u7fa9\u53ca\u5176\u6846\u67b6\u3002\u7136\u800c\uff0c\u5fae\u8abf\u63d0\u9ad8\u4e86 LLM \u5728\u8a5e\u5f59\u8a9e\u7fa9\u4efb\u52d9\u4e0a\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u9810\u8a13\u7df4\u671f\u9593\u7279\u5b9a\u9818\u57df\u7684\u8853\u8a9e\u662f\u4efb\u610f\u7684\u548c\u672a\u898b\u7684\uff0c\u9019\u6697\u793a\u4e86\u9810\u8a13\u7df4 LLM \u5c0d OL \u7684\u9069\u7528\u6027\u3002", "author": "Huu Tan Mai et.al.", "authors": "Huu Tan Mai, Cuong Xuan Chu, Heiko Paulheim", "id": "2407.19998v1", "paper_url": "http://arxiv.org/abs/2407.19998v1", "repo": "https://github.com/boschresearch/llm-vs-gibberish-ontologies"}}