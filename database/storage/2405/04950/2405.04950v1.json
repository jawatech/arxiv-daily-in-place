{"2405.04950": {"publish_time": "2024-05-08", "title": "VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context", "paper_summary": "Large Multimodal Models (LMMs) have achieved impressive success in visual\nunderstanding and reasoning, remarkably improving the performance of\nmathematical reasoning in a visual context. Yet, a challenging type of visual\nmath lies in the multimodal graph theory problem, which demands that LMMs\nunderstand the graphical structures accurately and perform multi-step reasoning\non the visual graph. Additionally, exploring multimodal graph theory problems\nwill lead to more effective strategies in fields like biology, transportation,\nand robotics planning. To step forward in this direction, we are the first to\ndesign a benchmark named VisionGraph, used to explore the capabilities of\nadvanced LMMs in solving multimodal graph theory problems. It encompasses eight\ncomplex graph problem tasks, from connectivity to shortest path problems.\nSubsequently, we present a Description-Program-Reasoning (DPR) chain to enhance\nthe logical accuracy of reasoning processes through graphical structure\ndescription generation and algorithm-aware multi-step reasoning. Our extensive\nstudy shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph\nreasoning; 2) All LMMs exhibit inferior perception accuracy for graphical\nstructures, whether in zero/few-shot settings or with supervised fine-tuning\n(SFT), which further affects problem-solving performance; 3) DPR significantly\nimproves the multi-step graph reasoning capabilities of LMMs and the GPT-4V\n(DPR) agent achieves SOTA performance.", "paper_summary_zh": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u77a9\u76ee\u7684\u6210\u529f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u73af\u5883\u4e2d\u6570\u5b66\u63a8\u7406\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u4e00\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u6570\u5b66\u5b58\u5728\u4e8e\u591a\u6a21\u6001\u56fe\u8bba\u95ee\u9898\u4e2d\uff0c\u5b83\u8981\u6c42 LMM \u51c6\u786e\u7406\u89e3\u56fe\u5f62\u7ed3\u6784\u5e76\u5728\u89c6\u89c9\u56fe\u4e0a\u6267\u884c\u591a\u6b65\u63a8\u7406\u3002\u6b64\u5916\uff0c\u63a2\u7d22\u591a\u6a21\u6001\u56fe\u8bba\u95ee\u9898\u5c06\u4e3a\u751f\u7269\u5b66\u3001\u4ea4\u901a\u8fd0\u8f93\u548c\u673a\u5668\u4eba\u89c4\u5212\u7b49\u9886\u57df\u5e26\u6765\u66f4\u6709\u6548\u7684\u7b56\u7565\u3002\u4e3a\u4e86\u671d\u8fd9\u4e2a\u65b9\u5411\u8fc8\u8fdb\uff0c\u6211\u4eec\u7387\u5148\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3a VisionGraph \u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u63a2\u7d22\u5148\u8fdb LMM \u5728\u89e3\u51b3\u591a\u6a21\u6001\u56fe\u8bba\u95ee\u9898\u4e2d\u7684\u80fd\u529b\u3002\u5b83\u5305\u542b\u516b\u4e2a\u590d\u6742\u7684\u56fe\u95ee\u9898\u4efb\u52a1\uff0c\u4ece\u8fde\u901a\u6027\u5230\u6700\u77ed\u8def\u5f84\u95ee\u9898\u3002\u968f\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u63cf\u8ff0-\u7a0b\u5e8f-\u63a8\u7406 (DPR) \u94fe\uff0c\u901a\u8fc7\u56fe\u5f62\u7ed3\u6784\u63cf\u8ff0\u751f\u6210\u548c\u7b97\u6cd5\u611f\u77e5\u7684\u591a\u6b65\u63a8\u7406\u6765\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u7684\u903b\u8f91\u51c6\u786e\u6027\u3002\u6211\u4eec\u7684\u5e7f\u6cdb\u7814\u7a76\u8868\u660e\uff1a1) GPT-4V \u5728\u591a\u6b65\u56fe\u63a8\u7406\u4e2d\u4f18\u4e8e Gemini Pro\uff1b2) \u6240\u6709 LMM \u5728\u56fe\u5f62\u7ed3\u6784\u7684\u611f\u77e5\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u65e0\u8bba\u662f\u5728\u96f6/\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u8fd8\u662f\u5728\u6709\u76d1\u7763\u5fae\u8c03 (SFT) \u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u8fdb\u4e00\u6b65\u5f71\u54cd\u4e86\u95ee\u9898\u6c42\u89e3\u6027\u80fd\uff1b3) DPR \u663e\u7740\u63d0\u9ad8\u4e86 LMM \u7684\u591a\u6b65\u56fe\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e14 GPT-4V (DPR) \u4ee3\u7406\u5b9e\u73b0\u4e86 SOTA \u6027\u80fd\u3002", "author": "Yunxin Li et.al.", "authors": "Yunxin Li, Baotian Hu, Haoyuan Shi, Wei Wang, Longyue Wang, Min Zhang", "id": "2405.04950v1", "paper_url": "http://arxiv.org/abs/2405.04950v1", "repo": "https://github.com/hitsz-tmg/visiongraph"}}