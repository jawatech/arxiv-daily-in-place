{"2408.12086": {"publish_time": "2024-08-22", "title": "Unlocking Attributes' Contribution to Successful Camouflage: A Combined Textual and VisualAnalysis Strategy", "paper_summary": "In the domain of Camouflaged Object Segmentation (COS), despite continuous\nimprovements in segmentation performance, the underlying mechanisms of\neffective camouflage remain poorly understood, akin to a black box. To address\nthis gap, we present the first comprehensive study to examine the impact of\ncamouflage attributes on the effectiveness of camouflage patterns, offering a\nquantitative framework for the evaluation of camouflage designs. To support\nthis analysis, we have compiled the first dataset comprising descriptions of\ncamouflaged objects and their attribute contributions, termed COD-Text And\nX-attributions (COD-TAX). Moreover, drawing inspiration from the hierarchical\nprocess by which humans process information: from high-level textual\ndescriptions of overarching scenarios, through mid-level summaries of local\nareas, to low-level pixel data for detailed analysis. We have developed a\nrobust framework that combines textual and visual information for the task of\nCOS, named Attribution CUe Modeling with Eye-fixation Network (ACUMEN). ACUMEN\ndemonstrates superior performance, outperforming nine leading methods across\nthree widely-used datasets. We conclude by highlighting key insights derived\nfrom the attributes identified in our study. Code:\nhttps://github.com/lyu-yx/ACUMEN.", "paper_summary_zh": "\u5728\u507d\u88dd\u7269\u9ad4\u5206\u5272 (COS) \u9818\u57df\u4e2d\uff0c\u5118\u7ba1\u5206\u5272\u6548\u80fd\u6301\u7e8c\u6539\u9032\uff0c\u4f46\u6709\u6548\u7684\u507d\u88dd\u80cc\u5f8c\u6a5f\u5236\u4ecd\u9bae\u70ba\u4eba\u77e5\uff0c\u5c31\u50cf\u500b\u9ed1\u76d2\u5b50\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7b2c\u4e00\u500b\u5168\u9762\u7684\u7814\u7a76\uff0c\u63a2\u8a0e\u507d\u88dd\u5c6c\u6027\u5c0d\u507d\u88dd\u6a23\u5f0f\u6709\u6548\u6027\u7684\u5f71\u97ff\uff0c\u4e26\u63d0\u4f9b\u4e00\u500b\u7528\u65bc\u8a55\u4f30\u507d\u88dd\u8a2d\u8a08\u7684\u91cf\u5316\u67b6\u69cb\u3002\u70ba\u4e86\u652f\u6301\u9019\u500b\u5206\u6790\uff0c\u6211\u5011\u7de8\u5236\u4e86\u7b2c\u4e00\u500b\u5305\u542b\u507d\u88dd\u7269\u9ad4\u63cf\u8ff0\u53ca\u5176\u5c6c\u6027\u8ca2\u737b\u7684\u8cc7\u6599\u96c6\uff0c\u7a31\u70ba COD-Text \u548c X-attributions (COD-TAX)\u3002\u6b64\u5916\uff0c\u5f9e\u4eba\u985e\u8655\u7406\u8cc7\u8a0a\u7684\u5206\u5c64\u904e\u7a0b\u6c72\u53d6\u9748\u611f\uff1a\u5f9e\u6982\u62ec\u5834\u666f\u7684\u9ad8\u968e\u6587\u5b57\u63cf\u8ff0\uff0c\u5230\u5340\u57df\u4e2d\u968e\u6458\u8981\uff0c\u518d\u5230\u7528\u65bc\u8a73\u7d30\u5206\u6790\u7684\u4f4e\u968e\u50cf\u7d20\u8cc7\u6599\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u7a69\u5065\u7684\u67b6\u69cb\uff0c\u7d50\u5408\u6587\u5b57\u548c\u8996\u89ba\u8cc7\u8a0a\uff0c\u7528\u65bc COS \u4efb\u52d9\uff0c\u7a31\u70ba Attribution CUe Modeling with Eye-fixation Network (ACUMEN)\u3002ACUMEN \u5c55\u73fe\u51fa\u512a\u7570\u7684\u6548\u80fd\uff0c\u5728\u4e09\u500b\u5ee3\u6cdb\u4f7f\u7528\u7684\u8cc7\u6599\u96c6\u4e0a\u52dd\u904e\u4e5d\u7a2e\u9818\u5148\u65b9\u6cd5\u3002\u6211\u5011\u6700\u5f8c\u5f37\u8abf\u4e86\u5f9e\u7814\u7a76\u4e2d\u8b58\u5225\u51fa\u7684\u5c6c\u6027\u4e2d\u5f97\u51fa\u7684\u95dc\u9375\u898b\u89e3\u3002\u7a0b\u5f0f\u78bc\uff1a\nhttps://github.com/lyu-yx/ACUMEN\u3002", "author": "Hong Zhang et.al.", "authors": "Hong Zhang, Yixuan Lyu, Qian Yu, Hanyang Liu, Huimin Ma, Ding Yuan, Yifan Yang", "id": "2408.12086v1", "paper_url": "http://arxiv.org/abs/2408.12086v1", "repo": "https://github.com/lyu-yx/acumen"}}