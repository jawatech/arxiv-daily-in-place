{"2408.07982": {"publish_time": "2024-08-15", "title": "Toward a Dialogue System Using a Large Language Model to Recognize User Emotions with a Camera", "paper_summary": "The performance of ChatGPT\\copyright{} and other LLMs has improved\ntremendously, and in online environments, they are increasingly likely to be\nused in a wide variety of situations, such as ChatBot on web pages, call center\noperations using voice interaction, and dialogue functions using agents. In the\noffline environment, multimodal dialogue functions are also being realized,\nsuch as guidance by Artificial Intelligence agents (AI agents) using tablet\nterminals and dialogue systems in the form of LLMs mounted on robots. In this\nmultimodal dialogue, mutual emotion recognition between the AI and the user\nwill become important. So far, there have been methods for expressing emotions\non the part of the AI agent or for recognizing them using textual or voice\ninformation of the user's utterances, but methods for AI agents to recognize\nemotions from the user's facial expressions have not been studied. In this\nstudy, we examined whether or not LLM-based AI agents can interact with users\naccording to their emotional states by capturing the user in dialogue with a\ncamera, recognizing emotions from facial expressions, and adding such emotion\ninformation to prompts. The results confirmed that AI agents can have\nconversations according to the emotional state for emotional states with\nrelatively high scores, such as Happy and Angry.", "paper_summary_zh": "ChatGPT\\copyright{} \u548c\u5176\u4ed6 LLM \u7684\u6548\u80fd\u5df2\u5927\u5e45\u63d0\u5347\uff0c\u5728\u7dda\u4e0a\u74b0\u5883\u4e2d\uff0c\u5b83\u5011\u6108\u4f86\u6108\u53ef\u80fd\u7528\u65bc\u5404\u7a2e\u60c5\u6cc1\uff0c\u4f8b\u5982\u7db2\u9801\u4e0a\u7684\u804a\u5929\u6a5f\u5668\u4eba\u3001\u4f7f\u7528\u8a9e\u97f3\u4e92\u52d5\u7684\u547c\u53eb\u4e2d\u5fc3\u4f5c\u696d\uff0c\u4ee5\u53ca\u4f7f\u7528\u4ee3\u7406\u4eba\u7684\u5c0d\u8a71\u529f\u80fd\u3002\u5728\u96e2\u7dda\u74b0\u5883\u4e2d\uff0c\u591a\u6a21\u614b\u5c0d\u8a71\u529f\u80fd\u4e5f\u6b63\u5728\u5be6\u73fe\uff0c\u4f8b\u5982\u4f7f\u7528\u5e73\u677f\u7d42\u7aef\u7684\u4eba\u5de5\u667a\u6167\u4ee3\u7406\u4eba (AI \u4ee3\u7406\u4eba) \u7684\u6307\u5c0e\uff0c\u4ee5\u53ca\u5b89\u88dd\u5728\u6a5f\u5668\u4eba\u4e0a\u7684 LLM \u5f62\u5f0f\u7684\u5c0d\u8a71\u7cfb\u7d71\u3002\u5728\u6b64\u591a\u6a21\u614b\u5c0d\u8a71\u4e2d\uff0cAI \u8207\u4f7f\u7528\u8005\u4e4b\u9593\u7684\u76f8\u4e92\u60c5\u7dd2\u8fa8\u8b58\u5c07\u8b8a\u5f97\u91cd\u8981\u3002\u5230\u76ee\u524d\u70ba\u6b62\uff0c\u5df2\u6709 AI \u4ee3\u7406\u4eba\u8868\u9054\u60c5\u7dd2\u6216\u4f7f\u7528\u4f7f\u7528\u8005\u7684\u8a71\u8a9e\u6587\u5b57\u6216\u8a9e\u97f3\u8cc7\u8a0a\u8fa8\u8b58\u60c5\u7dd2\u7684\u65b9\u6cd5\uff0c\u4f46 AI \u4ee3\u7406\u4eba\u5f9e\u4f7f\u7528\u8005\u7684\u9762\u90e8\u8868\u60c5\u8fa8\u8b58\u60c5\u7dd2\u7684\u65b9\u6cd5\u5c1a\u672a\u88ab\u7814\u7a76\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u57fa\u65bc LLM \u7684 AI \u4ee3\u7406\u4eba\u662f\u5426\u80fd\u900f\u904e\u76f8\u6a5f\u6355\u6349\u8207\u4f7f\u7528\u8005\u5c0d\u8a71\u3001\u5f9e\u9762\u90e8\u8868\u60c5\u8fa8\u8b58\u60c5\u7dd2\uff0c\u4e26\u5c07\u6b64\u985e\u60c5\u7dd2\u8cc7\u8a0a\u65b0\u589e\u81f3\u63d0\u793a\uff0c\u6839\u64da\u4f7f\u7528\u8005\u7684\u60c5\u7dd2\u72c0\u614b\u8207\u4f7f\u7528\u8005\u4e92\u52d5\u3002\u7d50\u679c\u8b49\u5be6\uff0c\u5c0d\u65bc\u60c5\u7dd2\u5206\u6578\u76f8\u5c0d\u8f03\u9ad8\u7684\u60c5\u7dd2\u72c0\u614b\uff0c\u4f8b\u5982\u5feb\u6a02\u548c\u751f\u6c23\uff0cAI \u4ee3\u7406\u4eba\u53ef\u4ee5\u6839\u64da\u60c5\u7dd2\u72c0\u614b\u9032\u884c\u5c0d\u8a71\u3002", "author": "Hiroki Tanioka et.al.", "authors": "Hiroki Tanioka, Tetsushi Ueta, Masahiko Sano", "id": "2408.07982v1", "paper_url": "http://arxiv.org/abs/2408.07982v1", "repo": "null"}}