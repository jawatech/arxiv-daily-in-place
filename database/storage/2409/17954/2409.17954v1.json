{"2409.17954": {"publish_time": "2024-09-26", "title": "Enhancing elusive clues in knowledge learning by contrasting attention of language models", "paper_summary": "Causal language models acquire vast amount of knowledge from general text\ncorpus during pretraining, but the efficiency of knowledge learning is known to\nbe unsatisfactory, especially when learning from knowledge-dense and\nsmall-sized corpora. The deficiency can come from long-distance dependencies\nwhich are hard to capture by language models, and overfitting to co-occurrence\npatterns and distracting clues in the training text. To address these issues,\nthe paper proposes a method to enhance knowledge learning during language model\npretraining, by enhancing elusive but important clues in text discovered by the\nlanguage model themselves. We found that larger language models pay more\nattention to non-obvious but important clues, which are often overlooked by\nsmaller language models. Therefore, we can identify these clues by contrasting\nthe attention weights of large and small language models. We use the identified\nclues as a guide to perform token-dropout data augmentation on the training\ntext, and observed a significant boost in both small and large models'\nperformance in fact memorization. This shows that the behavior contrast between\nmore and less-performant language models contains important clues for knowledge\nlearning, and it can be ``amplified\" for a straight-forward improvement in\nknowledge learning efficiency.", "paper_summary_zh": "\u56e0\u679c\u8a9e\u8a00\u6a21\u578b\u5728\u9810\u8a13\u7df4\u671f\u9593\u5f9e\u4e00\u822c\u6587\u5b57\u8a9e\u6599\u5eab\u4e2d\u7372\u53d6\u5927\u91cf\u7684\u77e5\u8b58\uff0c\u4f46\u5df2\u77e5\u77e5\u8b58\u5b78\u7fd2\u7684\u6548\u7387\u4e26\u4e0d\u4ee4\u4eba\u6eff\u610f\uff0c\u7279\u5225\u662f\u5728\u5f9e\u77e5\u8b58\u5bc6\u96c6\u4e14\u5c0f\u578b\u7684\u8a9e\u6599\u5eab\u4e2d\u5b78\u7fd2\u6642\u3002\u9019\u7a2e\u4e0d\u8db3\u53ef\u80fd\u4f86\u81ea\u9577\u8ddd\u96e2\u4f9d\u8cf4\u95dc\u4fc2\uff0c\u8a9e\u8a00\u6a21\u578b\u96e3\u4ee5\u6355\u6349\uff0c\u4ee5\u53ca\u904e\u5ea6\u64ec\u5408\u8a13\u7df4\u6587\u672c\u4e2d\u7684\u5171\u73fe\u6a21\u5f0f\u548c\u4ee4\u4eba\u5206\u5fc3\u7684\u7dda\u7d22\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b9\u6cd5\u4f86\u589e\u5f37\u8a9e\u8a00\u6a21\u578b\u9810\u8a13\u7df4\u671f\u9593\u7684\u77e5\u8b58\u5b78\u7fd2\uff0c\u65b9\u6cd5\u662f\u589e\u5f37\u8a9e\u8a00\u6a21\u578b\u672c\u8eab\u767c\u73fe\u7684\u6587\u672c\u4e2d\u96e3\u4ee5\u6349\u6478\u4f46\u91cd\u8981\u7684\u7dda\u7d22\u3002\u6211\u5011\u767c\u73fe\uff0c\u8f03\u5927\u7684\u8a9e\u8a00\u6a21\u578b\u6703\u66f4\u591a\u5730\u95dc\u6ce8\u4e0d\u986f\u773c\u4f46\u91cd\u8981\u7684\u7dda\u7d22\uff0c\u800c\u9019\u4e9b\u7dda\u7d22\u901a\u5e38\u6703\u88ab\u8f03\u5c0f\u7684\u8a9e\u8a00\u6a21\u578b\u6240\u5ffd\u7565\u3002\u56e0\u6b64\uff0c\u6211\u5011\u53ef\u4ee5\u901a\u904e\u5c0d\u6bd4\u5927\u5c0f\u8a9e\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u6b0a\u91cd\u4f86\u8b58\u5225\u9019\u4e9b\u7dda\u7d22\u3002\u6211\u5011\u4f7f\u7528\u8b58\u5225\u51fa\u7684\u7dda\u7d22\u4f5c\u70ba\u6307\u5357\uff0c\u5c0d\u8a13\u7df4\u6587\u672c\u57f7\u884c\u6b0a\u6756\u4e2d\u65b7\u6578\u64da\u64f4\u5145\uff0c\u4e26\u89c0\u5bdf\u5230\u5927\u5c0f\u6a21\u578b\u5728\u4e8b\u5be6\u8a18\u61b6\u4e2d\u7684\u6027\u80fd\u90fd\u6709\u986f\u8457\u63d0\u5347\u3002\u9019\u8868\u660e\uff0c\u6027\u80fd\u8f03\u9ad8\u548c\u8f03\u4f4e\u7684\u8a9e\u8a00\u6a21\u578b\u4e4b\u9593\u7684\u884c\u70ba\u5c0d\u6bd4\u5305\u542b\u4e86\u77e5\u8b58\u5b78\u7fd2\u7684\u91cd\u8981\u7dda\u7d22\uff0c\u4e26\u4e14\u53ef\u4ee5\u300c\u653e\u5927\u300d\u4ee5\u76f4\u63a5\u63d0\u9ad8\u77e5\u8b58\u5b78\u7fd2\u6548\u7387\u3002", "author": "Jian Gao et.al.", "authors": "Jian Gao, Xiao Zhang, Ji Wu, Miao Li", "id": "2409.17954v1", "paper_url": "http://arxiv.org/abs/2409.17954v1", "repo": "null"}}