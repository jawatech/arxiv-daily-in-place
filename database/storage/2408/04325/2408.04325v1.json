{"2408.04325": {"publish_time": "2024-08-08", "title": "HydraFormer: One Encoder For All Subsampling Rates", "paper_summary": "In automatic speech recognition, subsampling is essential for tackling\ndiverse scenarios. However, the inadequacy of a single subsampling rate to\naddress various real-world situations often necessitates training and deploying\nmultiple models, consequently increasing associated costs. To address this\nissue, we propose HydraFormer, comprising HydraSub, a Conformer-based encoder,\nand a BiTransformer-based decoder. HydraSub encompasses multiple branches, each\nrepresenting a distinct subsampling rate, allowing for the flexible selection\nof any branch during inference based on the specific use case. HydraFormer can\nefficiently manage different subsampling rates, significantly reducing training\nand deployment expenses. Experiments on AISHELL-1 and LibriSpeech datasets\nreveal that HydraFormer effectively adapts to various subsampling rates and\nlanguages while maintaining high recognition performance. Additionally,\nHydraFormer showcases exceptional stability, sustaining consistent performance\nunder various initialization conditions, and exhibits robust transferability by\nlearning from pretrained single subsampling rate automatic speech recognition\nmodels\\footnote{Model code and scripts:\nhttps://github.com/HydraFormer/hydraformer}.", "paper_summary_zh": "\u5728\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58\u4e2d\uff0c\u6b21\u62bd\u6a23\u5c0d\u65bc\u8655\u7406\u5404\u7a2e\u5834\u666f\u81f3\u95dc\u91cd\u8981\u3002\u7136\u800c\uff0c\u55ae\u4e00\u62bd\u6a23\u7387\u4e0d\u8db3\u4ee5\u61c9\u5c0d\u5404\u7a2e\u73fe\u5be6\u4e16\u754c\u7684\u72c0\u6cc1\uff0c\u901a\u5e38\u9700\u8981\u8a13\u7df4\u548c\u90e8\u7f72\u591a\u500b\u6a21\u578b\uff0c\u5f9e\u800c\u589e\u52a0\u76f8\u95dc\u6210\u672c\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 HydraFormer\uff0c\u5b83\u5305\u542b\u4e00\u500b\u57fa\u65bc Conformer \u7684\u7de8\u78bc\u5668 HydraSub \u548c\u4e00\u500b\u57fa\u65bc BiTransformer \u7684\u89e3\u78bc\u5668\u3002HydraSub \u6db5\u84cb\u591a\u500b\u5206\u652f\uff0c\u6bcf\u500b\u5206\u652f\u4ee3\u8868\u4e00\u500b\u4e0d\u540c\u7684\u62bd\u6a23\u7387\uff0c\u5141\u8a31\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u6839\u64da\u5177\u9ad4\u7528\u4f8b\u9748\u6d3b\u5730\u9078\u64c7\u4efb\u4f55\u5206\u652f\u3002HydraFormer \u53ef\u4ee5\u6709\u6548\u5730\u7ba1\u7406\u4e0d\u540c\u7684\u62bd\u6a23\u7387\uff0c\u986f\u8457\u964d\u4f4e\u8a13\u7df4\u548c\u90e8\u7f72\u8cbb\u7528\u3002\u5728 AISHELL-1 \u548c LibriSpeech \u6578\u64da\u96c6\u4e0a\u7684\u5be6\u9a57\u8868\u660e\uff0cHydraFormer \u53ef\u4ee5\u6709\u6548\u5730\u9069\u61c9\u5404\u7a2e\u62bd\u6a23\u7387\u548c\u8a9e\u8a00\uff0c\u540c\u6642\u4fdd\u6301\u8f03\u9ad8\u7684\u8fa8\u8b58\u6027\u80fd\u3002\u6b64\u5916\uff0cHydraFormer \u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u7a69\u5b9a\u6027\uff0c\u5728\u5404\u7a2e\u521d\u59cb\u5316\u689d\u4ef6\u4e0b\u90fd\u80fd\u4fdd\u6301\u4e00\u81f4\u7684\u6027\u80fd\uff0c\u4e26\u901a\u904e\u5f9e\u9810\u8a13\u7df4\u7684\u55ae\u62bd\u6a23\u7387\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58\u6a21\u578b\u4e2d\u5b78\u7fd2\uff0c\u5c55\u73fe\u51fa\u5f37\u5927\u7684\u53ef\u79fb\u690d\u6027\u3002", "author": "Yaoxun Xu et.al.", "authors": "Yaoxun Xu, Xingchen Song, Zhiyong Wu, Di Wu, Zhendong Peng, Binbin Zhang", "id": "2408.04325v1", "paper_url": "http://arxiv.org/abs/2408.04325v1", "repo": "https://github.com/hydraformer/hydraformer"}}