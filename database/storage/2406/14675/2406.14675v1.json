{"2406.14675": {"publish_time": "2024-06-20", "title": "This Looks Better than That: Better Interpretable Models with ProtoPNeXt", "paper_summary": "Prototypical-part models are a popular interpretable alternative to black-box\ndeep learning models for computer vision. However, they are difficult to train,\nwith high sensitivity to hyperparameter tuning, inhibiting their application to\nnew datasets and our understanding of which methods truly improve their\nperformance. To facilitate the careful study of prototypical-part networks\n(ProtoPNets), we create a new framework for integrating components of\nprototypical-part models -- ProtoPNeXt. Using ProtoPNeXt, we show that applying\nBayesian hyperparameter tuning and an angular prototype similarity metric to\nthe original ProtoPNet is sufficient to produce new state-of-the-art accuracy\nfor prototypical-part models on CUB-200 across multiple backbones. We further\ndeploy this framework to jointly optimize for accuracy and prototype\ninterpretability as measured by metrics included in ProtoPNeXt. Using the same\nresources, this produces models with substantially superior semantics and\nchanges in accuracy between +1.3% and -1.5%. The code and trained models will\nbe made publicly available upon publication.", "paper_summary_zh": "\u539f\u578b\u90e8\u5206\u6a21\u578b\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u79cd\u6d41\u884c\u7684\u53ef\u89e3\u91ca\u7684\u9ed1\u76d2\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5f88\u96be\u8bad\u7ec3\uff0c\u5bf9\u8d85\u53c2\u6570\u8c03\u6574\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u6291\u5236\u4e86\u5b83\u4eec\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u6211\u4eec\u5bf9\u771f\u6b63\u63d0\u9ad8\u5176\u6027\u80fd\u7684\u65b9\u6cd5\u7684\u7406\u89e3\u3002\u4e3a\u4e86\u4fc3\u8fdb\u5bf9\u539f\u578b\u90e8\u5206\u7f51\u7edc (ProtoPNets) \u7684\u4ed4\u7ec6\u7814\u7a76\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\u6765\u96c6\u6210\u539f\u578b\u90e8\u5206\u6a21\u578b\u7684\u7ec4\u4ef6\u2014\u2014ProtoPNeXt\u3002\u4f7f\u7528 ProtoPNeXt\uff0c\u6211\u4eec\u8868\u660e\u5bf9\u539f\u59cb ProtoPNet \u5e94\u7528\u8d1d\u53f6\u65af\u8d85\u53c2\u6570\u8c03\u6574\u548c\u89d2\u5ea6\u539f\u578b\u76f8\u4f3c\u6027\u5ea6\u91cf\u8db3\u4ee5\u5728\u591a\u4e2a\u9aa8\u5e72\u7f51\u4e0a\u4e3a CUB-200 \u4e0a\u7684\u539f\u578b\u90e8\u5206\u6a21\u578b\u4ea7\u751f\u65b0\u7684\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u90e8\u7f72\u6b64\u6846\u67b6\u4ee5\u6839\u636e ProtoPNeXt \u4e2d\u5305\u542b\u7684\u6307\u6807\u5171\u540c\u4f18\u5316\u51c6\u786e\u6027\u548c\u539f\u578b\u53ef\u89e3\u91ca\u6027\u3002\u4f7f\u7528\u76f8\u540c\u7684\u8d44\u6e90\uff0c\u8fd9\u4f1a\u4ea7\u751f\u5177\u6709\u660e\u663e\u4f18\u8d8a\u7684\u8bed\u4e49\u548c\u51c6\u786e\u6027\u53d8\u5316\u7684\u6a21\u578b\uff0c\u4ecb\u4e8e +1.3% \u548c -1.5% \u4e4b\u95f4\u3002\u4ee3\u7801\u548c\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5c06\u5728\u53d1\u5e03\u540e\u516c\u5f00\u3002", "author": "Frank Willard et.al.", "authors": "Frank Willard, Luke Moffett, Emmanuel Mokel, Jon Donnelly, Stark Guo, Julia Yang, Giyoung Kim, Alina Jade Barnett, Cynthia Rudin", "id": "2406.14675v1", "paper_url": "http://arxiv.org/abs/2406.14675v1", "repo": "null"}}