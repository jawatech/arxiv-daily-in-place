{"2407.12077": {"publish_time": "2024-07-16", "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression", "paper_summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 GoldFinch\uff0c\u4e00\u500b\u6df7\u5408\u7dda\u6027\u6ce8\u610f\u529b/\u8f49\u63db\u5668\u5e8f\u5217\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u4e00\u7a2e\u65b0\u6280\u8853\uff0c\u5728\u7dda\u6027\u6642\u9593\u548c\u7a7a\u9593\u4e2d\u6709\u6548\u751f\u6210\u4e00\u500b\u9ad8\u5ea6\u58d3\u7e2e\u4e14\u53ef\u91cd\u8907\u4f7f\u7528\u7684 KV \u5feb\u53d6\uff0c\u76f8\u5c0d\u65bc\u5e8f\u5217\u9577\u5ea6\u3002GoldFinch \u5c07\u6211\u5011\u65b0\u7684 GOLD \u8f49\u63db\u5668\u5806\u758a\u5728\u589e\u5f37\u7248\u672c\u7684 Finch (RWKV-6) \u67b6\u69cb\u4e4b\u4e0a\u3002\u6211\u5011\u8a13\u7df4\u4e86 Finch\u3001Llama \u548c GoldFinch \u67b6\u69cb\u4e2d\u9ad8\u9054 1.5B \u53c3\u6578\u985e\u5225\u6a21\u578b\uff0c\u4e26\u767c\u73fe\u76f8\u5c0d\u65bc Finch \u548c Llama\uff0c\u5efa\u6a21\u6027\u80fd\u986f\u8457\u63d0\u5347\u3002\u6211\u5011\u7684\u5feb\u53d6\u5927\u5c0f\u7bc0\u7701\u96a8\u8457\u6a21\u578b\u5c64\u6578\u7684\u589e\u52a0\u800c\u7dda\u6027\u589e\u52a0\uff0c\u6bd4\u50b3\u7d71\u8f49\u63db\u5668\u5feb\u53d6\u5c0f 756-2550 \u500d\uff0c\u5373\u4f7f\u5728\u6709\u9650\u7684\u786c\u9ad4\u4e0a\u4e5f\u80fd\u63a8\u8ad6\u6975\u5927\u7684\u4e0a\u4e0b\u6587\u9577\u5ea6\u3002\u5118\u7ba1\u81ea\u8ff4\u6b78\u751f\u6210\u5177\u6709 O(n) \u6642\u9593\u8907\u96dc\u5ea6\uff0c\u56e0\u70ba\u6ce8\u610f\u529b\uff0c\u4f46\u7531\u65bc\u4f7f\u7528\u905e\u8ff4\u795e\u7d93\u7db2\u8def (RNN) \u4f86\u751f\u6210\u6b64\u5feb\u53d6\uff0c\u63d0\u4ea4\u7684\u4e0a\u4e0b\u6587\u7684\u6574\u500b\u521d\u59cb\u5feb\u53d6\u72c0\u614b\u7684\u9810\u586b\u5145\u8a08\u7b97\u6bcf\u500b\u4ee4\u724c\u53ea\u9700 O(1) \u6642\u9593\u3002\u6211\u5011\u5728 Apache 2.0 \u8a31\u53ef\u4e0b\u91cb\u51fa\u8a13\u7df4\u5f8c\u7684\u6b0a\u91cd\u548c\u8a13\u7df4\u7a0b\u5f0f\u78bc\uff0c\u4f9b\u793e\u7fa4\u4f7f\u7528\u3002", "author": "Daniel Goldstein et.al.", "authors": "Daniel Goldstein, Fares Obeid, Eric Alcaide, Guangyu Song, Eugene Cheah", "id": "2407.12077v1", "paper_url": "http://arxiv.org/abs/2407.12077v1", "repo": "https://github.com/SmerkyG/GoldFinch-paper"}}