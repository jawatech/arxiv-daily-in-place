{"2408.16978": {"publish_time": "2024-08-30", "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer", "paper_summary": "Large Language Models (LLMs) with long context capabilities are integral to\ncomplex tasks in natural language processing and computational biology, such as\ntext generation and protein sequence analysis. However, training LLMs directly\non extremely long contexts demands considerable GPU resources and increased\nmemory, leading to higher costs and greater complexity. Alternative approaches\nthat introduce long context capabilities via downstream finetuning or\nadaptations impose significant design limitations. In this paper, we propose\nFully Pipelined Distributed Transformer (FPDT) for efficiently training\nlong-context LLMs with extreme hardware efficiency. For GPT and Llama models,\nwe achieve a 16x increase in sequence length that can be trained on the same\nhardware compared to current state-of-the-art solutions. With our dedicated\nsequence chunk pipeline design, we can now train 8B LLM with 2 million sequence\nlength on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed\nFPDT is agnostic to existing training techniques and is proven to work\nefficiently across different LLM models.", "paper_summary_zh": "\u5177\u5099\u9577\u8a9e\u5883\u80fd\u529b\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0d\u65bc\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u548c\u8a08\u7b97\u751f\u7269\u5b78\u4e2d\u7684\u8907\u96dc\u4efb\u52d9\u81f3\u95dc\u91cd\u8981\uff0c\u4f8b\u5982\u6587\u672c\u751f\u6210\u548c\u86cb\u767d\u8cea\u5e8f\u5217\u5206\u6790\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5728\u6975\u9577\u7684\u8a9e\u5883\u4e0a\u8a13\u7df4 LLM \u9700\u8981\u5927\u91cf\u7684 GPU \u8cc7\u6e90\u548c\u589e\u52a0\u7684\u8a18\u61b6\u9ad4\uff0c\u5c0e\u81f4\u66f4\u9ad8\u7684\u6210\u672c\u548c\u66f4\u5927\u7684\u8907\u96dc\u6027\u3002\u900f\u904e\u4e0b\u6e38\u5fae\u8abf\u6216\u9069\u61c9\u4f86\u5f15\u5165\u9577\u8a9e\u5883\u80fd\u529b\u7684\u66ff\u4ee3\u65b9\u6cd5\u6703\u9020\u6210\u91cd\u5927\u7684\u8a2d\u8a08\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u5168\u7ba1\u7dda\u5206\u6563\u5f0fTransformer (FPDT) \u4f86\u6709\u6548\u8a13\u7df4\u5177\u6709\u6975\u9ad8\u786c\u9ad4\u6548\u7387\u7684\u9577\u8a9e\u5883 LLM\u3002\u5c0d\u65bc GPT \u548c Llama \u6a21\u578b\uff0c\u6211\u5011\u5be6\u73fe\u4e86\u5e8f\u5217\u9577\u5ea6 16 \u500d\u7684\u589e\u9577\uff0c\u53ef\u4ee5\u5728\u8207\u7576\u524d\u6700\u5148\u9032\u7684\u89e3\u6c7a\u65b9\u6848\u76f8\u540c\u7684\u786c\u9ad4\u4e0a\u9032\u884c\u8a13\u7df4\u3002\u900f\u904e\u6211\u5011\u5c08\u7528\u7684\u5e8f\u5217\u5340\u584a\u7ba1\u7dda\u8a2d\u8a08\uff0c\u6211\u5011\u73fe\u5728\u53ef\u4ee5\u4f7f\u7528\u50c5 4 \u500b GPU \u8a13\u7df4\u5177\u6709 200 \u842c\u5e8f\u5217\u9577\u5ea6\u7684 8B LLM\uff0c\u540c\u6642\u9084\u7dad\u6301\u8d85\u904e 55% \u7684 MFU\u3002\u6211\u5011\u63d0\u51fa\u7684 FPDT \u8207\u73fe\u6709\u7684\u8a13\u7df4\u6280\u8853\u7121\u95dc\uff0c\u4e26\u4e14\u5df2\u8b49\u660e\u53ef\u4ee5\u5728\u4e0d\u540c\u7684 LLM \u6a21\u578b\u4e2d\u6709\u6548\u904b\u4f5c\u3002", "author": "Jinghan Yao et.al.", "authors": "Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, Aamir Shafi, Hari Subramoni, Dhabaleswar K. Panda", "id": "2408.16978v1", "paper_url": "http://arxiv.org/abs/2408.16978v1", "repo": "https://github.com/microsoft/DeepSpeed"}}