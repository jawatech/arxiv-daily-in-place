{"2408.13432": {"publish_time": "2024-08-24", "title": "Integrating Multi-Head Convolutional Encoders with Cross-Attention for Improved SPARQL Query Translation", "paper_summary": "The main task of the KGQA system (Knowledge Graph Question Answering) is to\nconvert user input questions into query syntax (such as SPARQL). With the rise\nof modern popular encoders and decoders like Transformer and ConvS2S, many\nscholars have shifted the research direction of SPARQL generation to the Neural\nMachine Translation (NMT) architecture or the generative AI field of\nText-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query\nsyntax as a language. It uses NMT-based translation models to translate natural\nlanguage questions into query syntax. Scholars use popular architectures\nequipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to\ntrain translation models for query syntax. To achieve better query results,\nthis paper improved the ConvS2S encoder and added multi-head attention from the\nTransformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the\nn-gram language model. The principle is to use convolutional layers to capture\nlocal hidden features in the input sequence with different receptive fields,\nusing multi-head attention to calculate dependencies between them. Ultimately,\nwe found that the translation model based on the Multi-Head Conv encoder\nachieved better performance than other encoders, obtaining 76.52\\% and 83.37\\%\nBLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0\ndatasets, respectively. Additionally, in the end-to-end system experiments on\nthe QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other\nKGQA systems, with Macro F1-measures reaching 52\\% and 66\\%, respectively.\nMoreover, the experimental results show that with limited computational\nresources, if one possesses an excellent encoder-decoder architecture and\ncross-attention, experts and scholars can achieve outstanding performance\nequivalent to large pre-trained models using only general embeddings.", "paper_summary_zh": "\u77e5\u8b58\u5716\u8868\u554f\u7b54\u7cfb\u7d71 (KGQA) \u7684\u4e3b\u8981\u4efb\u52d9\u662f\u5c07\u4f7f\u7528\u8005\u8f38\u5165\u7684\u554f\u984c\u8f49\u63db\u6210\u67e5\u8a62\u8a9e\u6cd5 (\u4f8b\u5982 SPARQL)\u3002\u96a8\u8457 Transformer \u548c ConvS2S \u7b49\u73fe\u4ee3\u6d41\u884c\u7de8\u78bc\u5668\u548c\u89e3\u78bc\u5668\u7684\u5d1b\u8d77\uff0c\u8a31\u591a\u5b78\u8005\u5df2\u5c07 SPARQL \u751f\u6210\u7684\u7814\u7a76\u65b9\u5411\u8f49\u79fb\u5230\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f (NMT) \u67b6\u69cb\u6216\u6587\u5b57\u8f49 SPARQL \u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u6167\u9818\u57df\u3002\u5728\u57fa\u65bc NMT \u7684\u554f\u7b54\u7cfb\u7d71\u4e2d\uff0c\u7cfb\u7d71\u5c07\u77e5\u8b58\u5eab\u67e5\u8a62\u8a9e\u6cd5\u8996\u70ba\u4e00\u7a2e\u8a9e\u8a00\u3002\u5b83\u4f7f\u7528\u57fa\u65bc NMT \u7684\u7ffb\u8b6f\u6a21\u578b\u5c07\u81ea\u7136\u8a9e\u8a00\u554f\u984c\u8f49\u63db\u6210\u67e5\u8a62\u8a9e\u6cd5\u3002\u5b78\u8005\u4f7f\u7528\u914d\u5099\u8de8\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u71b1\u9580\u67b6\u69cb\uff0c\u4f8b\u5982 Transformer\u3001ConvS2S \u548c BiLSTM\uff0c\u4f86\u8a13\u7df4\u67e5\u8a62\u8a9e\u6cd5\u7684\u7ffb\u8b6f\u6a21\u578b\u3002\u70ba\u4e86\u7372\u5f97\u66f4\u597d\u7684\u67e5\u8a62\u7d50\u679c\uff0c\u672c\u6587\u6539\u9032\u4e86 ConvS2S \u7de8\u78bc\u5668\uff0c\u4e26\u5f9e Transformer \u4e2d\u52a0\u5165\u591a\u982d\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc n-gram \u8a9e\u8a00\u6a21\u578b\u7684\u591a\u982d\u5377\u7a4d\u7de8\u78bc\u5668 (MHC \u7de8\u78bc\u5668)\u3002\u5176\u539f\u7406\u662f\u4f7f\u7528\u5377\u7a4d\u5c64\u4ee5\u4e0d\u540c\u7684\u611f\u53d7\u91ce\u64f7\u53d6\u8f38\u5165\u5e8f\u5217\u4e2d\u7684\u5c40\u90e8\u96b1\u85cf\u7279\u5fb5\uff0c\u4e26\u4f7f\u7528\u591a\u982d\u6ce8\u610f\u529b\u6a5f\u5236\u8a08\u7b97\u5b83\u5011\u4e4b\u9593\u7684\u4f9d\u8cf4\u95dc\u4fc2\u3002\u6700\u7d42\uff0c\u6211\u5011\u767c\u73fe\u57fa\u65bc\u591a\u982d\u5377\u7a4d\u7de8\u78bc\u5668\u7684\u7ffb\u8b6f\u6a21\u578b\u6bd4\u5176\u4ed6\u7de8\u78bc\u5668\u7372\u5f97\u4e86\u66f4\u597d\u7684\u6548\u80fd\uff0c\u5206\u5225\u5728 QALD-9 \u548c LC-QuAD-1.0 \u8cc7\u6599\u96c6\u4e0a\u7372\u5f97 76.52% \u548c 83.37% \u7684 BLEU-1\uff08\u96d9\u8a9e\u8a55\u4f30\u7814\u7a76\uff09\u5206\u6578\u3002\u6b64\u5916\uff0c\u5728 QALD-9 \u548c LC-QuAD-1.0 \u8cc7\u6599\u96c6\u7684\u7aef\u5230\u7aef\u7cfb\u7d71\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5728\u5176\u4ed6 KGQA \u7cfb\u7d71\u4e2d\u53d6\u5f97\u4e86\u9818\u5148\u7684\u7d50\u679c\uff0c\u5de8\u89c0 F1 \u6e2c\u91cf\u503c\u5206\u5225\u9054\u5230 52% \u548c 66%\u3002\u6b64\u5916\uff0c\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5982\u679c\u64c1\u6709\u51fa\u8272\u7684\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u67b6\u69cb\u548c\u8de8\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u5373\u4f7f\u5728\u904b\u7b97\u8cc7\u6e90\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\uff0c\u5c08\u5bb6\u548c\u5b78\u8005\u4ecd\u53ef\u4ee5\u4f7f\u7528\u4e00\u822c\u7684\u5d4c\u5165\u4f86\u7372\u5f97\u7b49\u540c\u65bc\u5927\u578b\u9810\u8a13\u7df4\u6a21\u578b\u7684\u5091\u51fa\u6548\u80fd\u3002", "author": "Yi-Hui Chen et.al.", "authors": "Yi-Hui Chen, Eric Jui-Lin Lu, Kwan-Ho Cheng", "id": "2408.13432v1", "paper_url": "http://arxiv.org/abs/2408.13432v1", "repo": "null"}}