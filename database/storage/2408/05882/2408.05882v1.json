{"2408.05882": {"publish_time": "2024-08-12", "title": "Creating Arabic LLM Prompts at Scale", "paper_summary": "The debut of chatGPT and BARD has popularized instruction following text\ngeneration using LLMs, where a user can interrogate an LLM using natural\nlanguage requests and obtain natural language answers that matches their\nrequests. Training LLMs to respond in this manner requires a large number of\nworked out examples of user requests (aka prompts) with corresponding gold\nresponses. In this paper, we introduce two methods for creating such prompts\nfor Arabic cheaply and quickly. The first methods entails automatically\ntranslating existing prompt datasets from English, such as PromptSource and\nSuper-NaturalInstructions, and then using machine translation quality\nestimation to retain high quality translations only. The second method involves\ncreating natural language prompts on top of existing Arabic NLP datasets. Using\nthese two methods we were able to create more than 67.4 million Arabic prompts\nthat cover a variety of tasks including summarization, headline generation,\ngrammar checking, open/closed question answering, creative writing, etc. We\nshow that fine tuning an open 7 billion parameter large language model, namely\nbase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter\ninstruction tuned model, namely Llama3 70B, in handling Arabic prompts.", "paper_summary_zh": "ChatGPT \u548c BARD \u7684\u9996\u6b21\u4eae\u76f8\u666e\u53ca\u4e86\u4f7f\u7528 LLM \u7684\u6307\u4ee4\u9075\u5faa\u6587\u672c\u751f\u6210\uff0c\u5176\u4e2d\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u8be2\u95ee LLM\uff0c\u5e76\u83b7\u5f97\u4e0e\u4ed6\u4eec\u7684\u8bf7\u6c42\u76f8\u5339\u914d\u7684\u81ea\u7136\u8bed\u8a00\u7b54\u6848\u3002\u8bad\u7ec3 LLM \u4ee5\u8fd9\u79cd\u65b9\u5f0f\u8fdb\u884c\u54cd\u5e94\u9700\u8981\u5927\u91cf\u5df2\u89e3\u51b3\u7684\u7528\u6237\u8bf7\u6c42\uff08\u53c8\u79f0\u63d0\u793a\uff09\u793a\u4f8b\uff0c\u5e76\u5e26\u6709\u76f8\u5e94\u7684\u9ec4\u91d1\u54cd\u5e94\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e24\u79cd\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5feb\u901f\u4e14\u5ec9\u4ef7\u5730\u4e3a\u963f\u62c9\u4f2f\u8bed\u521b\u5efa\u6b64\u7c7b\u63d0\u793a\u3002\u7b2c\u4e00\u4e2a\u65b9\u6cd5\u9700\u8981\u81ea\u52a8\u7ffb\u8bd1\u73b0\u6709\u7684\u63d0\u793a\u6570\u636e\u96c6\uff08\u4f8b\u5982 PromptSource \u548c Super-NaturalInstructions\uff09\u4ece\u82f1\u8bed\uff0c\u7136\u540e\u4f7f\u7528\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u4ec5\u4fdd\u7559\u9ad8\u8d28\u91cf\u7684\u7ffb\u8bd1\u3002\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u6d89\u53ca\u5728\u73b0\u6709\u7684\u963f\u62c9\u4f2f\u8bed NLP \u6570\u636e\u96c6\u4e4b\u4e0a\u521b\u5efa\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u3002\u4f7f\u7528\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u80fd\u591f\u521b\u5efa\u8d85\u8fc7 6740 \u4e07\u4e2a\u963f\u62c9\u4f2f\u8bed\u63d0\u793a\uff0c\u6db5\u76d6\u5404\u79cd\u4efb\u52a1\uff0c\u5305\u62ec\u6458\u8981\u3001\u6807\u9898\u751f\u6210\u3001\u8bed\u6cd5\u68c0\u67e5\u3001\u5f00\u653e/\u5c01\u95ed\u5f0f\u95ee\u9898\u89e3\u7b54\u3001\u521b\u610f\u5199\u4f5c\u7b49\u3002\u6211\u4eec\u8868\u660e\uff0c\u5bf9\u5f00\u653e\u7684 70 \u4ebf\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5373\u57fa\u7840 Qwen2 7B\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u63d0\u793a\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 700 \u4ebf\u53c2\u6570\u6307\u4ee4\u8c03\u6574\u6a21\u578b\uff08\u5373 Llama3 70B\uff09\u3002", "author": "Abdelrahman El-Sheikh et.al.", "authors": "Abdelrahman El-Sheikh, Ahmed Elmogtaba, Kareem Darwish, Muhammad Elmallah, Ashraf Elneima, Hassan Sawaf", "id": "2408.05882v1", "paper_url": "http://arxiv.org/abs/2408.05882v1", "repo": "null"}}