{"2406.16635": {"publish_time": "2024-06-24", "title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "paper_summary": "The high power consumption and latency-sensitive deployments of large\nlanguage models (LLMs) have motivated techniques like quantization and\nsparsity. Contextual sparsity, where the sparsity pattern is input-dependent,\nis crucial in LLMs because the permanent removal of attention heads or neurons\nfrom LLMs can significantly degrade accuracy. Prior work has attempted to model\ncontextual sparsity using neural networks trained to predict activation\nmagnitudes, which can be used to dynamically prune structures with low\npredicted activation magnitude. In this paper, we look beyond magnitude-based\npruning criteria to assess attention head and neuron importance in LLMs. We\ndeveloped a novel predictor called ShadowLLM, which can shadow the LLM behavior\nand enforce better sparsity patterns, resulting in over 15% improvement in\nend-to-end accuracy without increasing latency compared to previous methods.\nShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu\nframework. These enhancements are validated on models with up to 30 billion\nparameters. Our code is available at\n\\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u529f\u8017\u548c\u5ef6\u9072\u654f\u611f\u90e8\u7f72\u6fc0\u52f5\u4e86\u91cf\u5316\u548c\u7a00\u758f\u6027\u7b49\u6280\u8853\u3002\u8108\u7d61\u7a00\u758f\u6027\uff0c\u5176\u4e2d\u7a00\u758f\u6027\u6a21\u5f0f\u53d6\u6c7a\u65bc\u8f38\u5165\uff0c\u5728 LLM \u4e2d\u81f3\u95dc\u91cd\u8981\uff0c\u56e0\u70ba\u6c38\u4e45\u79fb\u9664 LLM \u7684\u6ce8\u610f\u529b\u982d\u6216\u795e\u7d93\u5143\u6703\u986f\u8457\u964d\u4f4e\u6e96\u78ba\u5ea6\u3002\u5148\u524d\u7684\u7814\u7a76\u5617\u8a66\u4f7f\u7528\u795e\u7d93\u7db2\u8def\u5efa\u6a21\u8108\u7d61\u7a00\u758f\u6027\uff0c\u8a72\u7db2\u8def\u7d93\u904e\u8a13\u7df4\u4f86\u9810\u6e2c\u6fc0\u6d3b\u5e45\u5ea6\uff0c\u53ef\u7528\u65bc\u52d5\u614b\u526a\u679d\u5177\u6709\u4f4e\u9810\u6e2c\u6fc0\u6d3b\u5e45\u5ea6\u7684\u7d50\u69cb\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8d85\u8d8a\u4e86\u57fa\u65bc\u5e45\u5ea6\u7684\u526a\u679d\u6a19\u6e96\u4f86\u8a55\u4f30 LLM \u4e2d\u7684\u6ce8\u610f\u529b\u982d\u548c\u795e\u7d93\u5143\u91cd\u8981\u6027\u3002\u6211\u5011\u958b\u767c\u4e86\u4e00\u7a2e\u7a31\u70ba ShadowLLM \u7684\u65b0\u7a4e\u9810\u6e2c\u5668\uff0c\u5b83\u53ef\u4ee5\u6a21\u64ec LLM \u884c\u70ba\u4e26\u5f37\u5236\u57f7\u884c\u66f4\u597d\u7684\u7a00\u758f\u6027\u6a21\u5f0f\uff0c\u8207\u4ee5\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7aef\u5230\u7aef\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86 15% \u4ee5\u4e0a\uff0c\u800c\u4e0d\u6703\u589e\u52a0\u5ef6\u9072\u3002ShadowLLM \u6bd4\u6700\u5148\u9032\u7684 DejaVu \u6846\u67b6\u5feb 20%\u3002\u9019\u4e9b\u589e\u5f37\u529f\u80fd\u5df2\u5728\u6700\u591a 300 \u5104\u500b\u53c3\u6578\u7684\u6a21\u578b\u4e0a\u5f97\u5230\u9a57\u8b49\u3002\u6211\u5011\u7684\u4ee3\u78bc\u53ef\u5728 \\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM} \u7372\u5f97\u3002", "author": "Yash Akhauri et.al.", "authors": "Yash Akhauri, Ahmed F AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M Rush, Safeen Huda, Mohamed S Abdelfattah", "id": "2406.16635v1", "paper_url": "http://arxiv.org/abs/2406.16635v1", "repo": "https://github.com/abdelfattah-lab/shadow_llm"}}