{"2502.09447": {"publish_time": "2025-02-13", "title": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations", "paper_summary": "Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.", "paper_summary_zh": "\u73fe\u6709\u7684\u8996\u89ba\u611f\u77e5\u7cfb\u7d71\u5c08\u6ce8\u65bc\u55ae\u8f2a\u5c0d\u8a71\u4e2d\u7684\u5340\u57df\u7d1a\u5206\u5272\uff0c\u4f9d\u8cf4\u65bc\u8907\u96dc\u4e14\u660e\u78ba\u7684\u67e5\u8a62\u6307\u4ee4\u3002\u6b64\u985e\u7cfb\u7d71\u7121\u6cd5\u5728\u50cf\u7d20\u7d1a\u5225\u63a8\u7406\u548c\u7406\u89e3\u5728\u4e92\u52d5\u4e2d\u4e0d\u65b7\u8b8a\u5316\u7684\u52d5\u614b\u4f7f\u7528\u8005\u610f\u5716\u3002\u6211\u5011\u7684\u7814\u7a76\u901a\u904e\u5f15\u5165\u4e00\u9805\u57fa\u65bc\u591a\u8f2a\u5c0d\u8a71\u7684\u50cf\u7d20\u7d1a\u63a8\u7406\u5206\u5272\uff08\u50cf\u7d20\u7d1a RS\uff09\u65b0\u4efb\u52d9\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u901a\u904e\u591a\u8f2a\u4e92\u52d5\u8ffd\u8e64\u4e0d\u65b7\u6f14\u8b8a\u7684\u4f7f\u7528\u8005\u610f\u5716\uff0c\u4ee5\u9032\u884c\u7cbe\u7d30\u5206\u5272\u3002\u70ba\u4e86\u5efa\u7acb\u6b64\u65b0\u4efb\u52d9\u7684\u57fa\u6e96\uff0c\u6211\u5011\u5efa\u7acb\u4e86\u4e00\u500b\u57fa\u65bc\u591a\u8f2a\u5c0d\u8a71\u7684\u50cf\u7d20\u7d1a\u63a8\u7406\u5206\u5272\u8cc7\u6599\u96c6\uff08PRIST\uff09\uff0c\u5176\u4e2d\u5305\u542b\u4f86\u81ea 8.3k \u591a\u8f2a\u5c0d\u8a71\u5834\u666f\u7684 24k \u500b\u8a9e\u53e5\uff0c\u4ee5\u53ca\u5206\u5272\u76ee\u6a19\u3002\u5728 PRIST \u7684\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u9032\u4e00\u6b65\u63d0\u51fa\u4e86 MIRAS\uff0c\u9019\u662f\u4e00\u500b\u591a\u8f2a\u4e92\u52d5\u63a8\u7406\u5206\u5272\u6846\u67b6\uff0c\u5b83\u5c07\u50cf\u7d20\u7d1a\u5206\u5272\u8207\u5f37\u5927\u7684\u591a\u8f2a\u5c0d\u8a71\u7406\u89e3\u6574\u5408\u5728\u4e00\u8d77\uff0c\u751f\u6210\u7b26\u5408\u4f7f\u7528\u8005\u610f\u5716\u7684\u50cf\u7d20\u7d1a\u89e3\u91cb\u3002PRIST \u8cc7\u6599\u96c6\u548c MIRSA \u6846\u67b6\u586b\u88dc\u4e86\u50cf\u7d20\u7d1a\u63a8\u7406\u5206\u5272\u7684\u7a7a\u767d\u3002\u5728 PRIST \u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u5206\u5272\u548c\u57fa\u65bc LLM \u7684\u63a8\u7406\u6307\u6a19\u65b9\u9762\u512a\u65bc\u76ee\u524d\u7684\u7279\u5b9a\u65bc\u5206\u5272\u7684\u57fa\u6e96\u3002\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u53ef\u5728 https://github.com/ccccai239/PixelRIST \u7372\u5f97\u3002", "author": "Dexian Cai et.al.", "authors": "Dexian Cai, Xiaocui Yang, Yongkang Liu, Daling Wang, Shi Feng, Yifei Zhang, Soujanya Poria", "id": "2502.09447v1", "paper_url": "http://arxiv.org/abs/2502.09447v1", "repo": "null"}}