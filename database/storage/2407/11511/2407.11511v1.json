{"2407.11511": {"publish_time": "2024-07-16", "title": "Reasoning with Large Language Models, a Survey", "paper_summary": "Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative \"System 1\" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong \"System 2\" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.", "paper_summary_zh": "<paragraph>\u5c07\u8a9e\u8a00\u6a21\u578b\u64f4\u5c55\u5230\u6578\u5341\u5104\u500b\u53c3\u6578\u958b\u555f\u4e86\u60c5\u5883\u5b78\u7fd2\u7684\u53ef\u80fd\u6027\uff0c\u5141\u8a31\u5c0d\u6a21\u578b\u672a\u7d93\u7279\u5225\u8a13\u7df4\u7684\u4efb\u52d9\u9032\u884c\u6307\u4ee4\u8abf\u6574\u548c\u5c11\u91cf\u5b78\u7fd2\u3002\u9019\u5728\u7ffb\u8b6f\u3001\u6458\u8981\u548c\u554f\u7b54\u7b49\u8a9e\u8a00\u4efb\u52d9\u4e0a\u5be6\u73fe\u4e86\u7a81\u7834\u6027\u7684\u8868\u73fe\u3002\u6b64\u5916\uff0c\u9664\u4e86\u9019\u4e9b\u806f\u60f3\u5f0f\u7684\u300c\u7cfb\u7d71 1\u300d\u4efb\u52d9\u4e4b\u5916\uff0c\u601d\u8003\u93c8\u63d0\u793a\u5b78\u7fd2\u7684\u6700\u65b0\u9032\u5c55\u5c55\u793a\u4e86\u5f37\u5927\u7684\u300c\u7cfb\u7d71 2\u300d\u63a8\u7406\u80fd\u529b\uff0c\u56de\u7b54\u4e86\u4eba\u5de5\u901a\u7528\u667a\u6167\u9818\u57df\u4e2d LLM \u662f\u5426\u53ef\u4ee5\u63a8\u7406\u7684\u554f\u984c\u3002\u8a72\u9818\u57df\u59cb\u65bc LLM \u662f\u5426\u80fd\u89e3\u6c7a\u5c0f\u5b78\u6578\u5b78\u6587\u5b57\u984c\u7684\u554f\u984c\u3002\u672c\u6587\u56de\u9867\u4e86 LLM \u63d0\u793a\u5f0f\u63a8\u7406\u5feb\u901f\u64f4\u5c55\u7684\u9818\u57df\u3002\u6211\u5011\u7684\u5206\u985e\u6cd5\u8b58\u5225\u4e86\u751f\u6210\u3001\u8a55\u4f30\u548c\u63a7\u5236\u591a\u6b65\u9a5f\u63a8\u7406\u7684\u4e0d\u540c\u65b9\u6cd5\u3002\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u4e86\u6838\u5fc3\u65b9\u6cd5\u548c\u958b\u653e\u6027\u554f\u984c\uff0c\u4e26\u63d0\u51fa\u4e86\u8fd1\u671f\u7814\u7a76\u8b70\u7a0b\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5f37\u8abf\u4e86\u63a8\u7406\u548c\u63d0\u793a\u5f0f\u5b78\u7fd2\u4e4b\u9593\u7684\u95dc\u4fc2\uff0c\u4e26\u8a0e\u8ad6\u4e86\u63a8\u7406\u3001\u5e8f\u5217\u6c7a\u7b56\u904e\u7a0b\u548c\u5f37\u5316\u5b78\u7fd2\u4e4b\u9593\u7684\u95dc\u4fc2\u3002\u6211\u5011\u767c\u73fe\uff0c\u901a\u904e\u660e\u667a\u5730\u4f7f\u7528\u63d0\u793a\uff0c\u63a8\u7406\u904e\u7a0b\u7684\u81ea\u5b8c\u5584\u3001\u81ea\u6211\u53cd\u601d\u548c\u4e00\u4e9b\u5143\u8a8d\u77e5\u80fd\u529b\u662f\u53ef\u80fd\u7684\u3002\u771f\u6b63\u7684\u81ea\u6211\u5b8c\u5584\u548c\u81ea\u6211\u63a8\u7406\uff0c\u5f9e\u4f7f\u7528 LLM \u63a8\u7406\u8f49\u8b8a\u70ba\u7531 LLM \u63a8\u7406\uff0c\u4ecd\u7136\u662f\u672a\u4f86\u7684\u7814\u7a76\u5de5\u4f5c\u3002</paragraph>", "author": "Aske Plaat et.al.", "authors": "Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back", "id": "2407.11511v1", "paper_url": "http://arxiv.org/abs/2407.11511v1", "repo": "null"}}