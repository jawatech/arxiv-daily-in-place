{"2405.02858": {"publish_time": "2024-05-05", "title": "Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation", "paper_summary": "Social media platforms such as Twitter, Reddit, and Sina Weibo play a crucial\nrole in global communication but often encounter strict regulations in\ngeopolitically sensitive regions. This situation has prompted users to\ningeniously modify their way of communicating, frequently resorting to coded\nlanguage in these regulated social media environments. This shift in\ncommunication is not merely a strategy to counteract regulation, but a vivid\nmanifestation of language evolution, demonstrating how language naturally\nevolves under societal and technological pressures. Studying the evolution of\nlanguage in regulated social media contexts is of significant importance for\nensuring freedom of speech, optimizing content moderation, and advancing\nlinguistic research. This paper proposes a multi-agent simulation framework\nusing Large Language Models (LLMs) to explore the evolution of user language in\nregulated social media environments. The framework employs LLM-driven agents:\nsupervisory agent who enforce dialogue supervision and participant agents who\nevolve their language strategies while engaging in conversation, simulating the\nevolution of communication styles under strict regulations aimed at evading\nsocial media regulation. The study evaluates the framework's effectiveness\nthrough a range of scenarios from abstract scenarios to real-world situations.\nKey findings indicate that LLMs are capable of simulating nuanced language\ndynamics and interactions in constrained settings, showing improvement in both\nevading supervision and information accuracy as evolution progresses.\nFurthermore, it was found that LLM agents adopt different strategies for\ndifferent scenarios.", "paper_summary_zh": "", "author": "Jinyu Cai et.al.", "authors": "Jinyu Cai,Jialong Li,Mingyue Zhang,Munan Li,Chen-Shu Wang,Kenji Tei", "id": "2405.02858v1", "paper_url": "http://arxiv.org/abs/2405.02858v1", "repo": "null"}}