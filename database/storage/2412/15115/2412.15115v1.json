{"2412.15115": {"publish_time": "2024-12-19", "title": "Qwen2.5 Technical Report", "paper_summary": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.", "paper_summary_zh": "<paragraph>\u5728\u9019\u4efd\u5831\u544a\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 Qwen2.5\uff0c\u9019\u662f\u4e00\u500b\u5168\u9762\u7684\u5927\u8a9e\u8a00\u6a21\u578b (LLM) \u7cfb\u5217\uff0c\u65e8\u5728\u6eff\u8db3\u4e0d\u540c\u7684\u9700\u6c42\u3002\u8207\u4e4b\u524d\u7684\u8fed\u4ee3\u76f8\u6bd4\uff0cQwen 2.5 \u5728\u9810\u8a13\u7df4\u548c\u5f8c\u8a13\u7df4\u968e\u6bb5\u90fd\u5f97\u5230\u4e86\u986f\u8457\u6539\u9032\u3002\u5728\u9810\u8a13\u7df4\u65b9\u9762\uff0c\u6211\u5011\u5c07\u9ad8\u54c1\u8cea\u7684\u9810\u8a13\u7df4\u6578\u64da\u96c6\u5f9e\u4e4b\u524d\u7684 7 \u5146\u500b\u7b26\u865f\u64f4\u5c55\u5230\u4e86 18 \u5146\u500b\u7b26\u865f\u3002\u9019\u70ba\u5e38\u8b58\u3001\u5c08\u696d\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5805\u5be6\u7684\u57fa\u790e\u3002\u5728\u5f8c\u8a13\u7df4\u65b9\u9762\uff0c\u6211\u5011\u5be6\u65bd\u4e86\u8907\u96dc\u7684\u76e3\u7763\u5fae\u8abf\uff0c\u6a23\u672c\u91cf\u8d85\u904e 100 \u842c\uff0c\u4ee5\u53ca\u591a\u968e\u6bb5\u5f37\u5316\u5b78\u7fd2\u3002\u5f8c\u8a13\u7df4\u6280\u8853\u589e\u5f37\u4e86\u4eba\u985e\u504f\u597d\uff0c\u4e26\u986f\u8457\u6539\u9032\u4e86\u9577\u6587\u672c\u751f\u6210\u3001\u7d50\u69cb\u5316\u6578\u64da\u5206\u6790\u548c\u6307\u4ee4\u9075\u5faa\u3002\u70ba\u4e86\u6709\u6548\u5730\u8655\u7406\u591a\u6a23\u5316\u7684\u7528\u4f8b\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u8c50\u5bcc\u5c3a\u5bf8\u7684 Qwen2.5 LLM \u7cfb\u5217\u3002\u958b\u653e\u6b0a\u91cd\u7522\u54c1\u5305\u62ec\u57fa\u790e\u6a21\u578b\u548c\u6307\u4ee4\u8abf\u6574\u6a21\u578b\uff0c\u4e26\u63d0\u4f9b\u91cf\u5316\u7248\u672c\u3002\u6b64\u5916\uff0c\u5c0d\u65bc\u8a17\u7ba1\u89e3\u6c7a\u65b9\u6848\uff0c\u5c08\u6709\u6a21\u578b\u76ee\u524d\u5305\u62ec\u5169\u500b\u6df7\u5408\u5c08\u5bb6 (MoE) \u8b8a\u9ad4\uff1aQwen2.5-Turbo \u548c Qwen2.5-Plus\uff0c\u5747\u53ef\u5728\u963f\u91cc\u96f2\u6a21\u578b\u5de5\u4f5c\u5ba4\u7372\u5f97\u3002Qwen2.5 \u5728\u8a55\u4f30\u8a9e\u8a00\u7406\u89e3\u3001\u63a8\u7406\u3001\u6578\u5b78\u3001\u7de8\u78bc\u3001\u4eba\u985e\u504f\u597d\u5c0d\u9f4a\u7b49\u65b9\u9762\u7684\u4e00\u7cfb\u5217\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5c55\u793a\u4e86\u4e00\u6d41\u7684\u6027\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u958b\u653e\u6b0a\u91cd\u65d7\u8266 Qwen2.5-72B-Instruct \u512a\u65bc\u8a31\u591a\u958b\u653e\u548c\u5c08\u6709\u6a21\u578b\uff0c\u4e26\u5c55\u793a\u4e86\u8207\u6700\u5148\u9032\u7684\u958b\u653e\u6b0a\u91cd\u6a21\u578b Llama-3-405B-Instruct \u7684\u7af6\u722d\u6027\u80fd\uff0c\u5f8c\u8005\u5927\u7d04\u5927 5 \u500d\u3002Qwen2.5-Turbo \u548c Qwen2.5-Plus \u5206\u5225\u63d0\u4f9b\u512a\u8d8a\u7684\u6210\u672c\u6548\u76ca\uff0c\u540c\u6642\u8207 GPT-4o-mini \u548c GPT-4o \u7af6\u722d\u3002\u6b64\u5916\uff0c\u4f5c\u70ba\u57fa\u790e\uff0cQwen2.5 \u6a21\u578b\u5728\u8a13\u7df4 Qwen2.5-Math\u3001Qwen2.5-Coder\u3001QwQ \u548c\u591a\u6a21\u614b\u6a21\u578b\u7b49\u5c08\u696d\u6a21\u578b\u65b9\u9762\u767c\u63ee\u4e86\u91cd\u8981\u4f5c\u7528\u3002</paragraph>", "author": "Qwen et.al.", "authors": "Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu", "id": "2412.15115v1", "paper_url": "http://arxiv.org/abs/2412.15115v1", "repo": "null"}}