{"2502.16459": {"publish_time": "2025-02-23", "title": "Deep learning approaches to surgical video segmentation and object detection: A Scoping Review", "paper_summary": "Introduction: Computer vision (CV) has had a transformative impact in\nbiomedical fields such as radiology, dermatology, and pathology. Its real-world\nadoption in surgical applications, however, remains limited. We review the\ncurrent state-of-the-art performance of deep learning (DL)-based CV models for\nsegmentation and object detection of anatomical structures in videos obtained\nduring surgical procedures.\n  Methods: We conducted a scoping review of studies on semantic segmentation\nand object detection of anatomical structures published between 2014 and 2024\nfrom 3 major databases - PubMed, Embase, and IEEE Xplore. The primary objective\nwas to evaluate the state-of-the-art performance of semantic segmentation in\nsurgical videos. Secondary objectives included examining DL models, progress\ntoward clinical applications, and the specific challenges with segmentation of\norgans/tissues in surgical videos.\n  Results: We identified 58 relevant published studies. These focused\npredominantly on procedures from general surgery [20(34.4%)], colorectal\nsurgery [9(15.5%)], and neurosurgery [8(13.8%)]. Cholecystectomy [14(24.1%)]\nand low anterior rectal resection [5(8.6%)] were the most common procedures\naddressed. Semantic segmentation [47(81%)] was the primary CV task. U-Net\n[14(24.1%)] and DeepLab [13(22.4%)] were the most widely used models. Larger\norgans such as the liver (Dice score: 0.88) had higher accuracy compared to\nsmaller structures such as nerves (Dice score: 0.49). Models demonstrated\nreal-time inference potential ranging from 5-298 frames-per-second (fps).\n  Conclusion: This review highlights the significant progress made in DL-based\nsemantic segmentation for surgical videos with real-time applicability,\nparticularly for larger organs. Addressing challenges with smaller structures,\ndata availability, and generalizability remains crucial for future\nadvancements.", "paper_summary_zh": "<paragraph>\u5f15\u8a00\uff1a\u96fb\u8166\u8996\u89ba (CV) \u5df2\u5728\u653e\u5c04\u79d1\u3001\u76ae\u819a\u79d1\u548c\u75c5\u7406\u5b78\u7b49\u751f\u7269\u91ab\u5b78\u9818\u57df\u7522\u751f\u8b8a\u9769\u6027\u5f71\u97ff\u3002\u7136\u800c\uff0c\u5b83\u5728\u5916\u79d1\u61c9\u7528\u4e2d\u7684\u5be6\u969b\u61c9\u7528\u4ecd\u7136\u6709\u9650\u3002\u6211\u5011\u56de\u9867\u4e86\u5728\u5916\u79d1\u624b\u8853\u904e\u7a0b\u4e2d\u7372\u5f97\u7684\u5f71\u7247\u4e2d\uff0c\u57fa\u65bc\u6df1\u5ea6\u5b78\u7fd2 (DL) \u7684 CV \u6a21\u578b\u5728\u89e3\u5256\u7d50\u69cb\u5206\u5272\u548c\u7269\u4ef6\u5075\u6e2c\u65b9\u9762\u7684\u73fe\u6709\u6700\u65b0\u6280\u8853\u8868\u73fe\u3002\n\u65b9\u6cd5\uff1a\u6211\u5011\u5c0d 2014 \u5e74\u81f3 2024 \u5e74\u9593\u767c\u8868\u7684\u89e3\u5256\u7d50\u69cb\u8a9e\u610f\u5206\u5272\u548c\u7269\u4ef6\u5075\u6e2c\u7814\u7a76\u9032\u884c\u4e86\u7bc4\u570d\u63a2\u8a0e\uff0c\u8cc7\u6599\u4f86\u81ea 3 \u500b\u4e3b\u8981\u8cc7\u6599\u5eab - PubMed\u3001Embase \u548c IEEE Xplore\u3002\u4e3b\u8981\u76ee\u6a19\u662f\u8a55\u4f30\u5916\u79d1\u5f71\u7247\u4e2d\u8a9e\u610f\u5206\u5272\u7684\u6700\u65b0\u6280\u8853\u8868\u73fe\u3002\u6b21\u8981\u76ee\u6a19\u5305\u62ec\u6aa2\u67e5 DL \u6a21\u578b\u3001\u671d\u5411\u81e8\u5e8a\u61c9\u7528\u9081\u9032\u7684\u9032\u5c55\uff0c\u4ee5\u53ca\u5916\u79d1\u5f71\u7247\u4e2d\u5668\u5b98/\u7d44\u7e54\u5206\u5272\u7684\u5177\u9ad4\u6311\u6230\u3002\n\u7d50\u679c\uff1a\u6211\u5011\u627e\u51fa 58 \u9805\u76f8\u95dc\u5df2\u767c\u8868\u7684\u7814\u7a76\u6240\u3002\u9019\u4e9b\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u666e\u901a\u5916\u79d1 [20(34.4%)], \u5927\u8178\u76f4\u8178\u5916\u79d1 [9(15.5%)] \u548c\u795e\u7d93\u5916\u79d1 [8(13.8%)] \u7684\u624b\u8853\u3002\u81bd\u56ca\u5207\u9664\u8853 [14(24.1%)] \u548c\u4f4e\u4f4d\u524d\u4f4d\u76f4\u8178\u5207\u9664\u8853 [5(8.6%)] \u662f\u6700\u5e38\u898b\u7684\u624b\u8853\u3002\u8a9e\u610f\u5206\u5272 [47(81%)] \u662f\u4e3b\u8981\u7684 CV \u4efb\u52d9\u3002U-Net [14(24.1%)] \u548c DeepLab [13(22.4%)] \u662f\u4f7f\u7528\u6700\u5ee3\u6cdb\u7684\u6a21\u578b\u3002\u8207\u795e\u7d93\u7b49\u8f03\u5c0f\u7684\u7d50\u69cb\u76f8\u6bd4\uff0c\u809d\u81df\u7b49\u8f03\u5927\u7684\u5668\u5b98\uff08\u9ab0\u5b50\u5206\u6578\uff1a0.88\uff09\u5177\u6709\u8f03\u9ad8\u7684\u6e96\u78ba\u5ea6\uff08\u9ab0\u5b50\u5206\u6578\uff1a0.49\uff09\u3002\u6a21\u578b\u5c55\u793a\u4e86\u5f9e\u6bcf\u79d2 5-298 \u5e40\uff08fps\uff09\u7684\u5373\u6642\u63a8\u8ad6\u6f5b\u529b\u3002\n\u7d50\u8ad6\uff1a\u9019\u7bc7\u56de\u9867\u5f37\u8abf\u4e86\u57fa\u65bc DL \u7684\u8a9e\u610f\u5206\u5272\u5728\u5916\u79d1\u5f71\u7247\u4e2d\u53d6\u5f97\u7684\u986f\u8457\u9032\u5c55\uff0c\u5177\u6709\u5373\u6642\u9069\u7528\u7684\u7279\u6027\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u8f03\u5927\u7684\u5668\u5b98\u3002\u89e3\u6c7a\u8f03\u5c0f\u7d50\u69cb\u3001\u8cc7\u6599\u53ef\u7528\u6027\u548c\u53ef\u6982\u62ec\u6027\u7684\u6311\u6230\u5c0d\u65bc\u672a\u4f86\u7684\u9032\u5c55\u4ecd\u7136\u81f3\u95dc\u91cd\u8981\u3002</paragraph>", "author": "Devanish N. Kamtam et.al.", "authors": "Devanish N. Kamtam, Joseph B. Shrager, Satya Deepya Malla, Nicole Lin, Juan J. Cardona, Jake J. Kim, Clarence Hu", "id": "2502.16459v1", "paper_url": "http://arxiv.org/abs/2502.16459v1", "repo": "null"}}