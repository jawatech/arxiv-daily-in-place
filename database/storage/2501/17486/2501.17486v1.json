{"2501.17486": {"publish_time": "2025-01-29", "title": "DINT Transformer", "paper_summary": "DIFF Transformer addresses the issue of irrelevant context interference by\nintroducing a differential attention mechanism that enhances the robustness of\nlocal attention. However, it has two critical limitations: the lack of global\ncontext modeling, which is essential for identifying globally significant\ntokens, and numerical instability due to the absence of strict row\nnormalization in the attention matrix. To overcome these challenges, we propose\nDINT Transformer, which extends DIFF Transformer by incorporating a\ndifferential-integral mechanism. By computing global importance scores and\nintegrating them into the attention matrix, DINT Transformer improves its\nability to capture global dependencies. Moreover, the unified parameter design\nenforces row-normalized attention matrices, improving numerical stability.\nExperimental results demonstrate that DINT Transformer excels in accuracy and\nrobustness across various practical applications, such as long-context language\nmodeling and key information retrieval. These results position DINT Transformer\nas a highly effective and promising architecture.", "paper_summary_zh": "DIFF Transformer \u900f\u904e\u5c0e\u5165\u5f37\u5316\u5c40\u90e8\u6ce8\u610f\u529b\u7a69\u5065\u6027\u7684\u5fae\u5206\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4f86\u89e3\u6c7a\u7121\u95dc\u80cc\u666f\u5e72\u64fe\u7684\u554f\u984c\u3002\u7136\u800c\uff0c\u5b83\u6709\u5169\u500b\u91cd\u5927\u7684\u9650\u5236\uff1a\u7f3a\u4e4f\u6574\u9ad4\u80cc\u666f\u5efa\u6a21\uff0c\u9019\u5c0d\u65bc\u8b58\u5225\u6574\u9ad4\u91cd\u8981\u7684\u7b26\u865f\u81f3\u95dc\u91cd\u8981\uff0c\u4ee5\u53ca\u7531\u65bc\u6ce8\u610f\u529b\u77e9\u9663\u4e2d\u7f3a\u4e4f\u56b4\u683c\u7684\u884c\u6b63\u898f\u5316\u800c\u5c0e\u81f4\u6578\u503c\u4e0d\u7a69\u5b9a\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 DINT Transformer\uff0c\u5b83\u900f\u904e\u7d0d\u5165\u5fae\u5206\u7a4d\u5206\u6a5f\u5236\u4f86\u64f4\u5145 DIFF Transformer\u3002\u900f\u904e\u8a08\u7b97\u6574\u9ad4\u91cd\u8981\u6027\u5206\u6578\u4e26\u5c07\u5b83\u5011\u6574\u5408\u5230\u6ce8\u610f\u529b\u77e9\u9663\u4e2d\uff0cDINT Transformer \u6539\u5584\u4e86\u5176\u6355\u6349\u6574\u9ad4\u4f9d\u8cf4\u6027\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7d71\u4e00\u53c3\u6578\u8a2d\u8a08\u5f37\u5236\u57f7\u884c\u884c\u6b63\u898f\u5316\u7684\u6ce8\u610f\u529b\u77e9\u9663\uff0c\u6539\u5584\u4e86\u6578\u503c\u7a69\u5b9a\u6027\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cDINT Transformer \u5728\u5404\u7a2e\u5be6\u969b\u61c9\u7528\u4e2d\u8868\u73fe\u51fa\u5353\u8d8a\u7684\u6e96\u78ba\u6027\u548c\u7a69\u5065\u6027\uff0c\u4f8b\u5982\u9577\u80cc\u666f\u8a9e\u8a00\u5efa\u6a21\u548c\u95dc\u9375\u8cc7\u8a0a\u6aa2\u7d22\u3002\u9019\u4e9b\u7d50\u679c\u5c07 DINT Transformer \u5b9a\u4f4d\u70ba\u4e00\u500b\u9ad8\u6548\u4e14\u6709\u524d\u666f\u7684\u67b6\u69cb\u3002", "author": "Yueyang Cang et.al.", "authors": "Yueyang Cang, Yuhang Liu, Xiaoteng Zhang, Erlu Zhao, Li Shi", "id": "2501.17486v1", "paper_url": "http://arxiv.org/abs/2501.17486v1", "repo": "null"}}