{"2412.07675": {"publish_time": "2024-12-10", "title": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text Rewriting", "paper_summary": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.", "paper_summary_zh": "\u5118\u7ba1 LLM \u56e0\u5176\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u8868\u73fe\u512a\u7570\u800c\u5ee3\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u904b\u7b97\u6210\u672c\u901a\u5e38\u5c0e\u81f4\u6f5b\u5728\u4f7f\u7528\u8005\u9078\u64c7\u9810\u8a13\u7df4\u5fae\u8abf\u7ba1\u7dda\u3002\u7136\u800c\uff0c\u624b\u52d5\u5efa\u69cb\u7684\u8cc7\u6599\u96c6\u4e2d\u666e\u904d\u5b58\u5728\u7684\u504f\u5dee\u6703\u5c0e\u81f4\u6a19\u8a18\u548c\u8a5e\u5f59\u4e4b\u9593\u7522\u751f\u865b\u5047\u7684\u76f8\u95dc\u6027\uff0c\u9020\u6210\u6240\u8b02\u7684\u6377\u5f91\uff0c\u4e26\u963b\u7919\u5fae\u8abf\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u73fe\u6709\u7684\u53bb\u504f\u5dee\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u65bc\u7279\u5b9a\u8cc7\u6599\u96c6\u504f\u5dee\u7684\u5148\u9a57\u77e5\u8b58\uff0c\u9019\u5728\u5148\u9a57\u4e0a\u5f88\u96e3\u7372\u5f97\u3002\u6211\u5011\u63d0\u51fa RAZOR\uff08\u91cd\u5beb\u548c\u96f6\u504f\u5dee\u6700\u4f73\u5316\u512a\u5316\uff09\uff0c\u4e00\u7a2e\u57fa\u65bc\u6587\u5b57\u91cd\u5beb\u7684\u65b0\u7a4e\u3001\u7121\u76e3\u7763\u548c\u4ee5\u8cc7\u6599\u70ba\u4e2d\u5fc3\u7684\u53bb\u504f\u5dee\u65b9\u6cd5\uff0c\u7528\u65bc\u6377\u5f91\u7de9\u89e3\u3002RAZR \u5229\u7528 LLM \u8fed\u4ee3\u91cd\u5beb\u6f5b\u5728\u6709\u504f\u5dee\u7684\u6587\u5b57\u7247\u6bb5\uff0c\u85c9\u7531\u5728\u7531\u6a19\u8a18\u7d71\u8a08\u548c\u4f4d\u7f6e\u8cc7\u8a0a\u5b9a\u7fa9\u7684\u6377\u5f91\u7a7a\u9593\u4e2d\u4ee5\u555f\u767c\u5f0f\u9078\u64c7\u7684\u66ff\u4ee3\u65b9\u6848\u53d6\u4ee3\u5b83\u5011\u3002\u6b64\u7a0b\u5e8f\u65e8\u5728\u4f7f\u8868\u9762\u6587\u5b57\u7279\u5fb5\u66f4\u7dca\u5bc6\u5730\u8207\u591a\u6a23\u5316\u7684\u6a19\u7c64\u5206\u4f48\u4fdd\u6301\u4e00\u81f4\uff0c\u5f9e\u800c\u4fc3\u9032\u771f\u6b63\u8a9e\u8a00\u6a21\u5f0f\u7684\u5b78\u7fd2\u3002\u8207\u7121\u76e3\u7763\u7684 SoTA \u6a21\u578b\u76f8\u6bd4\uff0c\u6839\u64da F1 \u5206\u6578\uff0cRAZR \u5728 FEVER \u4e0a\u63d0\u9ad8\u4e86 3.5%\uff0c\u5728 MNLI \u548c SNLI \u8cc7\u6599\u96c6\u4e0a\u63d0\u9ad8\u4e86 6.5%\u3002\u6b64\u5916\uff0cRAZR \u6709\u6548\u5730\u6e1b\u8f15\u4e86\u5df2\u77e5\u7684\u7279\u5b9a\u504f\u5dee\uff0c\u5c07\u8207\u504f\u5dee\u76f8\u95dc\u7684\u8853\u8a9e\u6e1b\u5c11\u4e86 x2\uff0c\u800c\u4e0d\u9700\u8981\u5148\u524d\u7684\u504f\u5dee\u8cc7\u8a0a\uff0c\u9019\u9805\u7d50\u679c\u8207\u5229\u7528\u5148\u9a57\u8cc7\u8a0a\u7684 SoTA \u6a21\u578b\u76f8\u7576\u3002\u6211\u5011\u7684\u7814\u7a76\u512a\u5148\u8003\u616e\u8cc7\u6599\u8655\u7406\u800c\u975e\u67b6\u69cb\u4fee\u6539\uff0c\u5f37\u8abf\u8cc7\u6599\u54c1\u8cea\u5728\u63d0\u5347\u6a21\u578b\u6548\u80fd\u548c\u516c\u5e73\u6027\u65b9\u9762\u7684\u95dc\u9375\u4f5c\u7528\u3002\u9019\u9805\u7814\u7a76\u6709\u52a9\u65bc\u958b\u767c\u66f4\u7a69\u5065\u7684\u53bb\u504f\u5dee\u65b9\u6cd5\u8a55\u4f30\u57fa\u6e96\uff0c\u65b9\u6cd5\u662f\u7d0d\u5165\u504f\u5dee\u6e1b\u5c11\u548c\u6574\u9ad4\u6a21\u578b\u6548\u80fd\u7684\u6307\u6a19\u3002", "author": "Shuo Yang et.al.", "authors": "Shuo Yang, Bardh Prenkaj, Gjergji Kasneci", "id": "2412.07675v1", "paper_url": "http://arxiv.org/abs/2412.07675v1", "repo": "null"}}