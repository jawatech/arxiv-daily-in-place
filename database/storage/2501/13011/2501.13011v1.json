{"2501.13011": {"publish_time": "2025-01-22", "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking", "paper_summary": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering.", "paper_summary_zh": "\u672a\u4f86\u9032\u968e\u7684 AI \u7cfb\u7d71\u53ef\u80fd\u900f\u904e\u4eba\u985e\u7121\u6cd5\u5145\u5206\u7406\u89e3\u4ee5\u5b89\u5168\u8a55\u4f30\u7684\u5f37\u5316\u5b78\u7fd2 (RL) \u4f86\u5b78\u7fd2\u8907\u96dc\u7684\u7b56\u7565\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u8a13\u7df4\u65b9\u6cd5\uff0c\u53ef\u907f\u514d\u4ee3\u7406\u5b78\u7fd2\u5373\u4f7f\u4eba\u985e\u7121\u6cd5\u5075\u6e2c\u5230\u884c\u70ba\u4e0d\u53d7\u6b61\u8fce\uff0c\u4e5f\u6703\u7372\u5f97\u9ad8\u734e\u52f5\u7684\u975e\u9810\u671f\u591a\u6b65\u9a5f\u8a08\u756b\uff08\u591a\u6b65\u9a5f\u300c\u734e\u52f5\u7834\u89e3\u300d\uff09\u3002\u9019\u7a2e\u65b9\u6cd5\uff0c\u8fd1\u8996\u512a\u5316\u642d\u914d\u9060\u8996\u734e\u52f5 (MONA)\uff0c\u662f\u900f\u904e\u7d50\u5408\u77ed\u8996\u512a\u5316\u8207\u9060\u898b\u734e\u52f5\u4f86\u904b\u4f5c\u3002\u6211\u5011\u793a\u7bc4 MONA \u53ef\u4ee5\u9632\u6b62\u4e00\u822c RL \u9020\u6210\u7684\u6b65\u9a5f\u734e\u52f5\u7834\u89e3\uff0c\u5373\u4f7f\u7121\u6cd5\u5075\u6e2c\u5230\u734e\u52f5\u7834\u89e3\uff0c\u4e14\u6c92\u6709\u4efb\u4f55\u4e00\u822c RL \u7121\u6cd5\u53d6\u5f97\u7684\u984d\u5916\u8cc7\u8a0a\u3002\u6211\u5011\u5728\u4e09\u7a2e\u8a2d\u5b9a\u4e2d\u4ee5\u7d93\u9a57\u7814\u7a76 MONA\uff0c\u5176\u4e2d\u5efa\u6a21\u4e86\u4e0d\u540c\u7684\u5931\u6e96\u5931\u6557\u6a21\u5f0f\uff0c\u5305\u62ec\u5177\u6709\u4ee3\u8868\u59d4\u6d3e\u76e3\u7763\u548c\u7de8\u78bc\u63a8\u7406\u7684 LLM \u7684 2 \u6b65\u9a5f\u74b0\u5883\uff0c\u4ee5\u53ca\u4ee3\u8868\u611f\u6e2c\u5668\u7834\u58de\u7684\u8f03\u9577\u6642\u7a0b\u7db2\u683c\u4e16\u754c\u74b0\u5883\u3002", "author": "Sebastian Farquhar et.al.", "authors": "Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah", "id": "2501.13011v1", "paper_url": "http://arxiv.org/abs/2501.13011v1", "repo": "null"}}