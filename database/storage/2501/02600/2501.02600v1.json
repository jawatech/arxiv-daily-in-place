{"2501.02600": {"publish_time": "2025-01-05", "title": "TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms", "paper_summary": "The rising demand for generative large language models (LLMs) poses\nchallenges for thermal and power management in cloud datacenters. Traditional\ntechniques often are inadequate for LLM inference due to the fine-grained,\nmillisecond-scale execution phases, each with distinct performance, thermal,\nand power profiles. Additionally, LLM inference workloads are sensitive to\nvarious configuration parameters (e.g., model parallelism, size, and\nquantization) that involve trade-offs between performance, temperature, power,\nand output quality. Moreover, clouds often co-locate SaaS and IaaS workloads,\neach with different levels of visibility and flexibility. We propose TAPAS, a\nthermal- and power-aware framework designed for LLM inference clusters in the\ncloud. TAPAS enhances cooling and power oversubscription capabilities, reducing\nthe total cost of ownership (TCO) while effectively handling emergencies (e.g.,\ncooling and power failures). The system leverages historical temperature and\npower data, along with the adaptability of SaaS workloads, to: (1) efficiently\nplace new GPU workload VMs within cooling and power constraints, (2) route LLM\ninference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load\nspikes and emergency situations. Our evaluation on a large GPU cluster\ndemonstrates significant reductions in thermal and power throttling events,\nboosting system efficiency.", "paper_summary_zh": "\u751f\u6210\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9700\u6c42\u4e0d\u65b7\u4e0a\u5347\uff0c\u5c0d\u96f2\u7aef\u8cc7\u6599\u4e2d\u5fc3\u7684\u71b1\u80fd\u548c\u96fb\u6e90\u7ba1\u7406\u63d0\u51fa\u4e86\u6311\u6230\u3002\u50b3\u7d71\u7684\u6280\u8853\u901a\u5e38\u4e0d\u8db3\u4ee5\u7528\u65bc LLM \u63a8\u8ad6\uff0c\u56e0\u70ba\u57f7\u884c\u968e\u6bb5\u662f\u7d30\u7dfb\u4e14\u4ee5\u6beb\u79d2\u70ba\u55ae\u4f4d\u7684\uff0c\u6bcf\u500b\u968e\u6bb5\u90fd\u6709\u4e0d\u540c\u7684\u6548\u80fd\u3001\u71b1\u80fd\u548c\u96fb\u6e90\u8a2d\u5b9a\u6a94\u3002\u6b64\u5916\uff0cLLM \u63a8\u8ad6\u5de5\u4f5c\u8ca0\u8f09\u5c0d\u5404\u7a2e\u7d44\u614b\u53c3\u6578\u5f88\u654f\u611f\uff08\u4f8b\u5982\uff0c\u6a21\u578b\u4e26\u884c\u3001\u5927\u5c0f\u548c\u91cf\u5316\uff09\uff0c\u9019\u4e9b\u53c3\u6578\u6d89\u53ca\u6548\u80fd\u3001\u6eab\u5ea6\u3001\u96fb\u6e90\u548c\u8f38\u51fa\u54c1\u8cea\u4e4b\u9593\u7684\u6b0a\u8861\u3002\u6b64\u5916\uff0c\u96f2\u7aef\u901a\u5e38\u6703\u5c07 SaaS \u548c IaaS \u5de5\u4f5c\u8ca0\u8f09\u914d\u7f6e\u5728\u540c\u4e00\u500b\u4f4d\u7f6e\uff0c\u800c\u6bcf\u500b\u5de5\u4f5c\u8ca0\u8f09\u7684\u80fd\u898b\u5ea6\u548c\u5f48\u6027\u90fd\u4e0d\u540c\u3002\u6211\u5011\u63d0\u51fa TAPAS\uff0c\u4e00\u500b\u5c08\u70ba\u96f2\u7aef\u4e2d\u7684 LLM \u63a8\u8ad6\u53e2\u96c6\u8a2d\u8a08\u7684\u71b1\u80fd\u548c\u96fb\u6e90\u611f\u77e5\u67b6\u69cb\u3002TAPAS \u589e\u5f37\u4e86\u51b7\u537b\u548c\u96fb\u6e90\u8d85\u984d\u9810\u8a02\u529f\u80fd\uff0c\u964d\u4f4e\u4e86\u7e3d\u6301\u6709\u6210\u672c (TCO)\uff0c\u540c\u6642\u6709\u6548\u5730\u8655\u7406\u7dca\u6025\u60c5\u6cc1\uff08\u4f8b\u5982\uff0c\u51b7\u537b\u548c\u96fb\u6e90\u6545\u969c\uff09\u3002\u7cfb\u7d71\u5229\u7528\u6b77\u53f2\u6eab\u5ea6\u548c\u96fb\u6e90\u8cc7\u6599\uff0c\u4ee5\u53ca SaaS \u5de5\u4f5c\u8ca0\u8f09\u7684\u9069\u61c9\u6027\uff0c\u4f86\uff1a(1) \u6709\u6548\u5730\u5c07\u65b0\u7684 GPU \u5de5\u4f5c\u8ca0\u8f09 VM \u653e\u7f6e\u5728\u51b7\u537b\u548c\u96fb\u6e90\u9650\u5236\u5167\uff0c(2) \u8def\u7531 LLM \u63a8\u8ad6\u8acb\u6c42\u5230\u5404\u500b SaaS VM\uff0c\u4ee5\u53ca (3) \u91cd\u65b0\u7d44\u614b SaaS VM \u4ee5\u7ba1\u7406\u8ca0\u8f09\u9ad8\u5cf0\u548c\u7dca\u6025\u60c5\u6cc1\u3002\u6211\u5011\u5728\u5927\u578b GPU \u53e2\u96c6\u4e0a\u7684\u8a55\u4f30\u986f\u793a\uff0c\u71b1\u80fd\u548c\u96fb\u6e90\u9650\u5236\u4e8b\u4ef6\u5927\u5e45\u6e1b\u5c11\uff0c\u63d0\u5347\u4e86\u7cfb\u7d71\u6548\u7387\u3002", "author": "Jovan Stojkovic et.al.", "authors": "Jovan Stojkovic, Chaojie Zhang, \u00cd\u00f1igo Goiri, Esha Choukse, Haoran Qiu, Rodrigo Fonseca, Josep Torrellas, Ricardo Bianchini", "id": "2501.02600v1", "paper_url": "http://arxiv.org/abs/2501.02600v1", "repo": "null"}}