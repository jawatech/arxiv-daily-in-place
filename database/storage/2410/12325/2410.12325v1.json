{"2410.12325": {"publish_time": "2024-10-16", "title": "Optimizing Low-Resource Language Model Training: Comprehensive Analysis of Multi-Epoch, Multi-Lingual, and Two-Stage Approaches", "paper_summary": "In this paper, we address the challenge of optimizing training setups for\nLarge Language Models (LLMs) of low-resource language with a limited amount of\ncorpus. Existing works adopt multi-epoch, multi-lingual, and two-stage training\nto utilize the limited target language corpus efficiently. However, there is\nstill a lack of understanding about the optimal hyperparameter setups for\ncombining these three approaches to train LLMs. We exhaustively explore\ntraining setups for low-resource language LLM, combining these three\napproaches, and found the following insights for efficiently reducing the cost\nof hyperparameter search: (1) As the amount of target language corpus\ndecreases, the optimal training approach shifts from monolingual single-stage\ntraining to multi-lingual two-stage training at a compute budget dependent\nthreshold. (2) The optimal model scale remains stable regardless of the amount\nof target language corpus, allowing the use of the compute-optimal scale of\nmonolingual training. (3) The optimal number of epochs can be extrapolated from\nsmaller-scale experiments to larger scale using our proposed model. Also, we\nprovide evidence that, in single-stage training, the target language validation\nloss follows a power law with respect to the target language ratio, with an\nexponent independent of the amount of data, model scale, and language pair.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4f7f\u7528\u6709\u9650\u8a9e\u6599\u5eab\uff0c\u91dd\u5c0d\u4f4e\u8cc7\u6e90\u8a9e\u8a00\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6700\u4f73\u5316\u8a13\u7df4\u8a2d\u5b9a\u7684\u6311\u6230\u3002\u73fe\u6709\u4f5c\u54c1\u63a1\u7528\u591a\u500b\u4e16\u4ee3\u3001\u591a\u8a9e\u8a00\u548c\u5169\u968e\u6bb5\u8a13\u7df4\uff0c\u4ee5\u6709\u6548\u5229\u7528\u6709\u9650\u7684\u76ee\u6a19\u8a9e\u8a00\u8a9e\u6599\u5eab\u3002\u7136\u800c\uff0c\u5c0d\u65bc\u7d50\u5408\u9019\u4e09\u7a2e\u65b9\u6cd5\u4f86\u8a13\u7df4 LLM \u7684\u6700\u4f73\u8d85\u53c3\u6578\u8a2d\u5b9a\uff0c\u76ee\u524d\u4ecd\u7f3a\u4e4f\u4e86\u89e3\u3002\u6211\u5011\u91dd\u5c0d\u4f4e\u8cc7\u6e90\u8a9e\u8a00 LLM \u7684\u8a13\u7df4\u8a2d\u5b9a\u9032\u884c\u4e86\u8a73\u76e1\u7684\u63a2\u8a0e\uff0c\u7d50\u5408\u4e86\u9019\u4e09\u7a2e\u65b9\u6cd5\uff0c\u4e26\u767c\u73fe\u4e86\u4ee5\u4e0b\u898b\u89e3\uff0c\u53ef\u6709\u6548\u6e1b\u5c11\u8d85\u53c3\u6578\u641c\u5c0b\u7684\u6210\u672c\uff1a(1) \u96a8\u8457\u76ee\u6a19\u8a9e\u8a00\u8a9e\u6599\u5eab\u7684\u6578\u91cf\u6e1b\u5c11\uff0c\u6700\u4f73\u8a13\u7df4\u65b9\u6cd5\u6703\u5f9e\u55ae\u8a9e\u55ae\u968e\u6bb5\u8a13\u7df4\u8f49\u79fb\u5230\u591a\u8a9e\u8a00\u5169\u968e\u6bb5\u8a13\u7df4\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u904b\u7b97\u9810\u7b97\u3002 (2) \u6700\u4f73\u6a21\u578b\u898f\u6a21\u6703\u4fdd\u6301\u7a69\u5b9a\uff0c\u8207\u76ee\u6a19\u8a9e\u8a00\u8a9e\u6599\u5eab\u7684\u6578\u91cf\u7121\u95dc\uff0c\u9019\u8b93\u6211\u5011\u53ef\u4ee5\u4f7f\u7528\u55ae\u8a9e\u8a13\u7df4\u7684\u904b\u7b97\u6700\u4f73\u898f\u6a21\u3002 (3) \u6700\u4f73\u4e16\u4ee3\u6578\u53ef\u4ee5\u4f7f\u7528\u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b\uff0c\u5f9e\u5c0f\u898f\u6a21\u5be6\u9a57\u5916\u63a8\u5230\u66f4\u5927\u898f\u6a21\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u8b49\u64da\uff0c\u5728\u55ae\u968e\u6bb5\u8a13\u7df4\u4e2d\uff0c\u76ee\u6a19\u8a9e\u8a00\u9a57\u8b49\u640d\u5931\u6703\u9075\u5faa\u76ee\u6a19\u8a9e\u8a00\u6bd4\u7387\u7684\u51aa\u5f8b\uff0c\u4e14\u6307\u6578\u8207\u8cc7\u6599\u91cf\u3001\u6a21\u578b\u898f\u6a21\u548c\u8a9e\u8a00\u914d\u5c0d\u7121\u95dc\u3002</paragraph>", "author": "Kosuke Akimoto et.al.", "authors": "Kosuke Akimoto, Masafumi Oyamada", "id": "2410.12325v1", "paper_url": "http://arxiv.org/abs/2410.12325v1", "repo": "null"}}