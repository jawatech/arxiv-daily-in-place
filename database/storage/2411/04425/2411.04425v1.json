{"2411.04425": {"publish_time": "2024-11-07", "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning", "paper_summary": "Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy.", "paper_summary_zh": "\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c0d\u65bc\u63d0\u5347\u5176\u5728\u7279\u5b9a\u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u81f3\u95dc\u91cd\u8981\uff0c\u4f46\u7531\u65bc\u5197\u9918\u6216\u7121\u8cc7\u8a0a\u8cc7\u6599\uff0c\u901a\u5e38\u6703\u8017\u8cbb\u5927\u91cf\u8cc7\u6e90\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u4f4e\u6548\u7387\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 DELIFT\uff08\u8cc7\u6599\u6709\u6548\u8a9e\u8a00\u6a21\u578b\u6307\u4ee4\u5fae\u8abf\uff09\uff0c\u4e00\u7a2e\u65b0\u6f14\u7b97\u6cd5\uff0c\u53ef\u7cfb\u7d71\u6027\u5730\u6700\u4f73\u5316\u5fae\u8abf\u4e09\u500b\u95dc\u9375\u968e\u6bb5\u7684\u8cc7\u6599\u9078\u53d6\uff1a(1) \u6307\u4ee4\u5fae\u8abf\u3001(2) \u4efb\u52d9\u7279\u5b9a\u5fae\u8abf\uff08\u4f8b\u5982\u63a8\u7406\u3001\u554f\u7b54\uff09\uff0c\u4ee5\u53ca (3) \u6301\u7e8c\u5fae\u8abf\uff08\u4f8b\u5982\u7d0d\u5165\u65b0\u8cc7\u6599\u7248\u672c\uff09\u3002\u8207\u73fe\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u73fe\u6709\u65b9\u6cd5\u5c08\u6ce8\u65bc\u55ae\u968e\u6bb5\u6700\u4f73\u5316\u6216\u4f9d\u8cf4\u65bc\u8a08\u7b97\u5bc6\u96c6\u7684\u68af\u5ea6\u8a08\u7b97\uff0cDELIFT \u5728\u6240\u6709\u968e\u6bb5\u90fd\u80fd\u6709\u6548\u904b\u4f5c\u3002\u6211\u5011\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u4e00\u500b\u6210\u5c0d\u6548\u7528\u6307\u6a19\uff0c\u7528\u65bc\u91cf\u5316\u8cc7\u6599\u7bc4\u4f8b\u5c0d\u65bc\u6539\u5584\u6a21\u578b\u5c0d\u5176\u4ed6\u7bc4\u4f8b\u7684\u56de\u61c9\u6709\u591a\u5927\u7684\u597d\u8655\uff0c\u6709\u6548\u5730\u8861\u91cf\u76f8\u5c0d\u65bc\u6a21\u578b\u7576\u524d\u529f\u80fd\u7684\u8cc7\u8a0a\u50f9\u503c\u3002\u900f\u904e\u5c0d\u6b64\u6307\u6a19\u61c9\u7528\u4e0d\u540c\u7684\u6b21\u6a21\u51fd\u6578\uff0cDELIFT \u9078\u53d6\u5728\u5fae\u8abf\u7684\u6240\u6709\u968e\u6bb5\u90fd\u6709\u7528\u7684\u591a\u6a23\u5316\u4e14\u6700\u4f73\u5b50\u96c6\u3002\u5728\u5404\u7a2e\u4efb\u52d9\u548c\u6a21\u578b\u898f\u6a21\u7684\u5be6\u9a57\u4e2d\u986f\u793a\uff0cDELIFT \u53ef\u4ee5\u5c07\u5fae\u8abf\u8cc7\u6599\u5927\u5c0f\u6e1b\u5c11\u591a\u9054 70%\uff0c\u540c\u6642\u4e0d\u5f71\u97ff\u6548\u80fd\uff0c\u63d0\u4f9b\u986f\u8457\u7684\u8a08\u7b97\u7bc0\u7701\uff0c\u4e26\u5728\u6548\u7387\u548c\u6548\u529b\u65b9\u9762\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002", "author": "Ishika Agarwal et.al.", "authors": "Ishika Agarwal, Krishna Killamsetty, Lucian Popa, Marina Danilevksy", "id": "2411.04425v1", "paper_url": "http://arxiv.org/abs/2411.04425v1", "repo": "https://github.com/agarwalishika/delift"}}