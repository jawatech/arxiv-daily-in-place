{"2502.14846": {"publish_time": "2025-02-20", "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation", "paper_summary": "Reasoning about images with rich text, such as charts and documents, is a\ncritical application of vision-language models (VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically create synthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-quality instruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-language instruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developing multimodal agents capable of acting in real-world\nenvironments.", "paper_summary_zh": "\u900f\u904e\u8c50\u5bcc\u6587\u5b57\uff08\u4f8b\u5982\u5716\u8868\u548c\u6587\u4ef6\uff09\u5c0d\u5f71\u50cf\u9032\u884c\u63a8\u7406\uff0c\u662f\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7684\u91cd\u8981\u61c9\u7528\u3002\u7136\u800c\uff0c\u7531\u65bc\u591a\u5143\u5316\u6587\u5b57\u8c50\u5bcc\u7684\u8996\u89ba\u8a9e\u8a00\u8cc7\u6599\u7a00\u5c11\uff0cVLM \u5728\u9019\u4e9b\u9818\u57df\u4e2d\u7d93\u5e38\u6703\u9047\u5230\u56f0\u96e3\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 CoSyn\uff0c\u4e00\u500b\u5229\u7528\u7d14\u6587\u5b57\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u7de8\u78bc\u80fd\u529b\u4f86\u81ea\u52d5\u5efa\u7acb\u5408\u6210\u6587\u5b57\u8c50\u5bcc\u591a\u6a21\u614b\u8cc7\u6599\u7684\u67b6\u69cb\u3002\u7d66\u5b9a\u63cf\u8ff0\u76ee\u6a19\u7db2\u57df\u7684\u8f38\u5165\u6587\u5b57\uff08\u4f8b\u5982\u300c\u71df\u990a\u6210\u5206\u6a19\u7c64\u300d\uff09\uff0cCoSyn \u6703\u63d0\u793a LLM \u7522\u751f\u7528\u65bc\u5408\u6210\u5f71\u50cf\u6e32\u67d3\u7684\u7a0b\u5f0f\u78bc\uff08Python\u3001HTML\u3001LaTeX \u7b49\uff09\u3002\u900f\u904e\u5c07\u5e95\u5c64\u7a0b\u5f0f\u78bc\u4f5c\u70ba\u5408\u6210\u5f71\u50cf\u7684\u6587\u5b57\u8868\u793a\uff0cCoSyn \u53ef\u4ee5\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\uff0c\u518d\u6b21\u4f9d\u8cf4\u7d14\u6587\u5b57 LLM\u3002\u4f7f\u7528 CoSyn\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u5305\u542b 40 \u842c\u5f35\u5f71\u50cf\u548c 270 \u842c\u5217\u8996\u89ba\u8a9e\u8a00\u6307\u4ee4\u8abf\u6574\u8cc7\u6599\u7684\u8cc7\u6599\u96c6\u3002\u5728\u4e03\u500b\u57fa\u6e96\u4e0a\u7684\u5168\u9762\u5be6\u9a57\u8b49\u660e\uff0c\u5728\u6211\u5011\u7684\u5408\u6210\u8cc7\u6599\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u5728\u7af6\u722d\u5c0d\u624b\u7684\u958b\u6e90\u6a21\u578b\uff08\u5305\u62ec Llama 3.2\uff09\u4e2d\u9054\u5230\u4e86\u6700\u5148\u9032\u7684\u6548\u80fd\uff0c\u4e26\u8d85\u8d8a\u4e86 GPT-4V \u548c Gemini 1.5 Flash \u7b49\u5c08\u6709\u6a21\u578b\u3002\u6b64\u5916\uff0cCoSyn \u53ef\u4ee5\u7522\u751f\u5408\u6210\u6307\u5411\u8cc7\u6599\uff0c\u8b93 VLM \u80fd\u5728\u8f38\u5165\u5f71\u50cf\u4e2d\u5efa\u7acb\u8cc7\u8a0a\u57fa\u790e\uff0c\u5c55\u793a\u5176\u5728\u958b\u767c\u80fd\u5920\u5728\u771f\u5be6\u4e16\u754c\u74b0\u5883\u4e2d\u904b\u4f5c\u7684\u591a\u6a21\u614b\u4ee3\u7406\u65b9\u9762\u7684\u6f5b\u529b\u3002", "author": "Yue Yang et.al.", "authors": "Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Christopher Clark", "id": "2502.14846v1", "paper_url": "http://arxiv.org/abs/2502.14846v1", "repo": "null"}}