{"2410.23510": {"publish_time": "2024-10-30", "title": "Tiny Transformers Excel at Sentence Compression", "paper_summary": "It is staggering that words of the English language, which are on average\nrepresented by 5--6 bytes of ASCII, require as much as 24 kilobytes when served\nto large language models. We show that there is room for more information in\nevery token embedding. We demonstrate that 1--3-layer transformers are capable\nof encoding and subsequently decoding standard English sentences into as little\nas a single 3-kilobyte token. Our work implies that even small networks can\nlearn to construct valid English sentences and suggests the possibility of\noptimising large language models by moving from sub-word token embeddings\ntowards larger fragments of text.", "paper_summary_zh": "\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u5e73\u5747\u7528 5 \u5230 6 \u500b ASCII \u4f4d\u5143\u7d44\u8868\u793a\u7684\u82f1\u6587\u55ae\u5b57\uff0c\u5728\u5927\u8a9e\u8a00\u6a21\u578b\u4e2d\u63d0\u4f9b\u6642\u9700\u8981\u591a\u9054 24 \u5343\u4f4d\u5143\u7d44\u3002\u6211\u5011\u5c55\u793a\u4e86\u6bcf\u500b\u6a19\u8a18\u5d4c\u5165\u4e2d\u9084\u6709\u66f4\u591a\u8cc7\u8a0a\u7684\u7a7a\u9593\u3002\u6211\u5011\u8b49\u660e\u4e86 1 \u5230 3 \u5c64\u7684\u8f49\u63db\u5668\u80fd\u5920\u5c07\u6a19\u6e96\u82f1\u6587\u53e5\u5b50\u7de8\u78bc\u4e26\u96a8\u5f8c\u89e3\u78bc\u6210\u50c5 3 \u5343\u4f4d\u5143\u7d44\u7684\u55ae\u4e00\u6a19\u8a18\u3002\u6211\u5011\u7684\u7814\u7a76\u610f\u5473\u8457\u5373\u4f7f\u662f\u5c0f\u7db2\u8def\u4e5f\u80fd\u5b78\u6703\u5efa\u69cb\u6709\u6548\u7684\u82f1\u6587\u53e5\u5b50\uff0c\u4e26\u5efa\u8b70\u900f\u904e\u5f9e\u5b50\u5b57\u8a5e\u6a19\u8a18\u5d4c\u5165\u8f49\u79fb\u5230\u66f4\u5927\u7684\u6587\u5b57\u7247\u6bb5\u4f86\u6700\u4f73\u5316\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u53ef\u80fd\u6027\u3002", "author": "Peter Belcak et.al.", "authors": "Peter Belcak, Roger Wattenhofer", "id": "2410.23510v1", "paper_url": "http://arxiv.org/abs/2410.23510v1", "repo": "null"}}