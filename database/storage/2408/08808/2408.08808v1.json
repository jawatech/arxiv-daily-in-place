{"2408.08808": {"publish_time": "2024-08-16", "title": "Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge", "paper_summary": "Large Language Models (LLMs) have revolutionized the landscape of machine\nlearning, yet current benchmarks often fall short in capturing the diverse\nbehavior of these models in real-world applications. A benchmark's usefulness\nis determined by its ability to clearly differentiate between models of varying\ncapabilities (separability) and closely align with human preferences. Existing\nframeworks like Alpaca-Eval 2.0 LC\n\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} and Arena-Hard v0.1\n\\cite{li2024crowdsourced} are limited by their focus on general-purpose queries\nand lack of diversity across domains such as law, medicine, and multilingual\ncontexts. In this paper, we address these limitations by introducing a novel\ndata pipeline that curates diverse, domain-specific evaluation sets tailored\nfor LLM-as-a-Judge frameworks. Our approach leverages a combination of manual\ncuration, semi-supervised learning to generate clusters, and stratified\nsampling to ensure balanced representation across a wide range of domains and\nlanguages. The resulting evaluation set, which includes 1573 samples across 14\ncategories, demonstrates high separability (84\\%) across ten top-ranked models,\nand agreement (84\\%) with Chatbot Arena and (0.915) Spearman correlation. The\nagreement values are 9\\% better than Arena Hard and 20\\% better than AlpacaEval\n2.0 LC, while the Spearman coefficient is 0.7 more than the next best\nbenchmark, showcasing a significant improvement in the usefulness of the\nbenchmark. We further provide an open-source evaluation tool that enables\nfine-grained analysis of model performance across user-defined categories,\noffering valuable insights for practitioners. This work contributes to the\nongoing effort to enhance the transparency, diversity, and effectiveness of LLM\nevaluation methodologies.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5fb9\u5e95\u6539\u8b8a\u4e86\u6a5f\u5668\u5b78\u7fd2\u7684\u683c\u5c40\uff0c\u4f46\u76ee\u524d\u7684\u57fa\u6e96\u7d93\u5e38\u7121\u6cd5\u6355\u6349\u9019\u4e9b\u6a21\u578b\u5728\u5be6\u969b\u61c9\u7528\u4e2d\u7684\u591a\u6a23\u5316\u884c\u70ba\u3002\u57fa\u6e96\u7684\u6548\u7528\u53d6\u6c7a\u65bc\u5176\u6e05\u695a\u5340\u5206\u4e0d\u540c\u80fd\u529b\u6a21\u578b\uff08\u53ef\u5206\u96e2\u6027\uff09\u4e26\u8207\u4eba\u985e\u504f\u597d\u7dca\u5bc6\u7d50\u5408\u7684\u80fd\u529b\u3002\u73fe\u6709\u7684\u6846\u67b6\uff08\u4f8b\u5982 Alpaca-Eval 2.0 LC\\cite{dubois2024lengthcontrolledalpacaevalsimpleway} \u548c Arena-Hard v0.1\\cite{li2024crowdsourced}\uff09\u53d7\u5230\u5176\u5c0d\u901a\u7528\u67e5\u8a62\u7684\u95dc\u6ce8\u4ee5\u53ca\u7f3a\u4e4f\u6cd5\u5f8b\u3001\u91ab\u5b78\u548c\u591a\u8a9e\u8a00\u74b0\u5883\u7b49\u9818\u57df\u591a\u6a23\u6027\u7684\u9650\u5236\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u901a\u904e\u5f15\u5165\u4e00\u7a2e\u65b0\u7a4e\u7684\u8cc7\u6599\u7ba1\u9053\u4f86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u8a72\u7ba1\u9053\u7b56\u5283\u4e86\u91dd\u5c0d LLM \u4f5c\u70ba\u8a55\u5be9\u6846\u67b6\u91cf\u8eab\u6253\u9020\u7684\u591a\u6a23\u5316\u3001\u7279\u5b9a\u65bc\u9818\u57df\u7684\u8a55\u4f30\u96c6\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u4e86\u624b\u52d5\u7b56\u5283\u3001\u534a\u76e3\u7763\u5f0f\u5b78\u7fd2\u4f86\u751f\u6210\u7fa4\u96c6\u4ee5\u53ca\u5206\u5c64\u62bd\u6a23\u7684\u7d44\u5408\uff0c\u4ee5\u78ba\u4fdd\u5728\u5ee3\u6cdb\u7684\u9818\u57df\u548c\u8a9e\u8a00\u4e2d\u5177\u6709\u5e73\u8861\u7684\u8868\u793a\u3002\u751f\u6210\u7684\u8a55\u4f30\u96c6\u5305\u542b 14 \u500b\u985e\u5225\u4e2d\u7684 1573 \u500b\u6a23\u672c\uff0c\u5c55\u793a\u4e86\u524d\u5341\u540d\u6a21\u578b\u4e4b\u9593\u7684\u9ad8\u53ef\u5206\u96e2\u6027 (84%)\uff0c\u4ee5\u53ca\u8207\u804a\u5929\u6a5f\u5668\u4eba\u7af6\u6280\u5834\u7684\u4e00\u81f4\u6027 (84%) \u548c (0.915) Spearman \u76f8\u95dc\u6027\u3002\u4e00\u81f4\u6027\u503c\u6bd4 Arena Hard \u9ad8 9%\uff0c\u6bd4 AlpacaEval 2.0 LC \u9ad8 20%\uff0c\u800c Spearman \u4fc2\u6578\u6bd4\u6b21\u4f73\u57fa\u6e96\u9ad8 0.7\uff0c\u986f\u793a\u57fa\u6e96\u7684\u6548\u7528\u6709\u986f\u8457\u63d0\u5347\u3002\u6211\u5011\u9032\u4e00\u6b65\u63d0\u4f9b\u4e86\u4e00\u500b\u958b\u6e90\u8a55\u4f30\u5de5\u5177\uff0c\u8a72\u5de5\u5177\u53ef\u4ee5\u5c0d\u4f7f\u7528\u8005\u5b9a\u7fa9\u985e\u5225\u4e2d\u7684\u6a21\u578b\u6548\u80fd\u9032\u884c\u7d30\u7dfb\u7684\u5206\u6790\uff0c\u70ba\u5be6\u52d9\u5de5\u4f5c\u8005\u63d0\u4f9b\u6709\u50f9\u503c\u7684\u898b\u89e3\u3002\u9019\u9805\u5de5\u4f5c\u6709\u52a9\u65bc\u6301\u7e8c\u52aa\u529b\u63d0\u9ad8 LLM \u8a55\u4f30\u65b9\u6cd5\u7684\u900f\u660e\u5ea6\u3001\u591a\u6a23\u6027\u548c\u6709\u6548\u6027\u3002", "author": "Ravi Raju et.al.", "authors": "Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, Urmish Thakkar", "id": "2408.08808v1", "paper_url": "http://arxiv.org/abs/2408.08808v1", "repo": "null"}}