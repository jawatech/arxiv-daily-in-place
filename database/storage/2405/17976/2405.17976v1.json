{"2405.17976": {"publish_time": "2024-05-28", "title": "Yuan 2.0-M32: Mixture of Experts with Attention Router", "paper_summary": "Yuan 2.0-M32, with a similar base architecture as Yuan-2.0 2B, uses a\nmixture-of-experts architecture with 32 experts of which 2 experts are active.\nA new router network, Attention Router, is proposed and adopted for a more\nefficient selection of experts, which boosts the accuracy of 3.8% compared to\nthe model with classical router network. Yuan 2.0-M32 is trained with 2000B\ntokens from scratch, and the training computation consumption is only 9.25% of\na dense model at the same parameter scale. Yuan 2.0-M32 demonstrates\ncompetitive capability on coding, math, and various domains of expertise, with\nonly 3.7B active parameters of 40B in total, and 7.4 GFlops forward computation\nper token, both of which are only 1/19 of Llama3-70B. Yuan 2.0-M32 surpass\nLlama3-70B on MATH and ARC-Challenge benchmark, with accuracy of 55.89 and 95.8\nrespectively. The models and source codes of Yuan 2.0-M32 are released at\nGithub.", "paper_summary_zh": "Yuan 2.0-M32\uff0c\u5176\u57fa\u672c\u67b6\u69cb\u8207 Yuan-2.0 2B \u76f8\u4f3c\uff0c\u4f7f\u7528\u6df7\u5408\u5c08\u5bb6\u67b6\u69cb\uff0c\u5176\u4e2d\u6709 32 \u4f4d\u5c08\u5bb6\uff0c\u5176\u4e2d 2 \u4f4d\u5c08\u5bb6\u8655\u65bc\u6d3b\u8e8d\u72c0\u614b\u3002\u63d0\u51fa\u4e26\u63a1\u7528\u4e86\u4e00\u500b\u65b0\u7684\u8def\u7531\u5668\u7db2\u8def\uff0c\u5373 Attention Router\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u9078\u64c7\u5c08\u5bb6\uff0c\u8207\u63a1\u7528\u7d93\u5178\u8def\u7531\u5668\u7db2\u8def\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u6e96\u78ba\u5ea6\u63d0\u5347\u4e86 3.8%\u3002Yuan 2.0-M32 \u4f7f\u7528 2000B \u7684\u4ee3\u5e63\u5f9e\u982d\u958b\u59cb\u8a13\u7df4\uff0c\u4e26\u4e14\u8a13\u7df4\u8a08\u7b97\u6d88\u8017\u50c5\u70ba\u540c\u53c3\u6578\u898f\u6a21\u4e0b\u7a20\u5bc6\u6a21\u578b\u7684 9.25%\u3002Yuan 2.0-M32 \u5728\u7de8\u78bc\u3001\u6578\u5b78\u548c\u5404\u7a2e\u5c08\u696d\u9818\u57df\u5c55\u793a\u4e86\u7af6\u722d\u529b\uff0c\u7e3d\u5171\u53ea\u6709 40B \u7684 3.7B \u500b\u6d3b\u8e8d\u53c3\u6578\uff0c\u4e26\u4e14\u6bcf\u500b\u4ee3\u5e63\u7684\u6b63\u5411\u8a08\u7b97\u70ba 7.4 GFlops\uff0c\u9019\u5169\u8005\u90fd\u53ea\u6709 Llama3-70B \u7684 1/19\u3002Yuan 2.0-M32 \u5728 MATH \u548c ARC-Challenge \u57fa\u6e96\u6e2c\u8a66\u4e2d\u8d85\u8d8a\u4e86 Llama3-70B\uff0c\u6e96\u78ba\u5ea6\u5206\u5225\u70ba 55.89 \u548c 95.8\u3002Yuan 2.0-M32 \u7684\u6a21\u578b\u548c\u6e90\u4ee3\u78bc\u5df2\u5728 Github \u4e0a\u767c\u5e03\u3002", "author": "Shaohua Wu et.al.", "authors": "Shaohua Wu, Jiangang Luo, Xi Chen, Lingjun Li, Xudong Zhao, Tong Yu, Chao Wang, Yue Wang, Fei Wang, Weixu Qiao, Houbo He, Zeru Zhang, Zeyu Sun, Junxiong Mao, Chong Shen", "id": "2405.17976v1", "paper_url": "http://arxiv.org/abs/2405.17976v1", "repo": "https://github.com/ieit-yuan/yuan2.0-m32"}}