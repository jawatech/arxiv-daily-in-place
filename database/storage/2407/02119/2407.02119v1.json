{"2407.02119": {"publish_time": "2024-07-02", "title": "Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning", "paper_summary": "Reinforcement learning with human feedback (RLHF), as a widely adopted\napproach in current large language model pipelines, is \\textit{bottlenecked by\nthe size of human preference data}. While traditional methods rely on offline\npreference dataset constructions, recent approaches have shifted towards online\nsettings, where a learner uses a small amount of labeled seed data and a large\npool of unlabeled prompts to iteratively construct new preference data through\nself-generated responses and high-quality reward/preference feedback. However,\nmost current online algorithms still focus on preference labeling during policy\nmodel updating with given feedback oracles, which incurs significant expert\nquery costs. \\textit{We are the first to explore cost-effective proxy reward\noracles construction strategies for further labeling preferences or rewards\nwith extremely limited labeled data and expert query budgets}. Our approach\nintroduces two key innovations: (1) on-policy query to avoid OOD and imbalance\nissues in seed data, and (2) active learning to select the most informative\ndata for preference queries. Using these methods, we train a evaluation model\nwith minimal expert-labeled data, which then effectively labels nine times more\npreference pairs for further RLHF training. For instance, our model using\nDirect Preference Optimization (DPO) gains around over 1% average improvement\non AlpacaEval2, MMLU-5shot and MMLU-0shot, with only 1.7K query cost. Our\nmethodology is orthogonal to other direct expert query-based strategies and\ntherefore might be integrated with them to further reduce query costs.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2\u642d\u914d\u4eba\u985e\u56de\u994b (RLHF) \u4f5c\u70ba\u7576\u524d\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7ba1\u7dda\u4e2d\u5ee3\u6cdb\u63a1\u7528\u7684\u65b9\u6cd5\uff0c\u53d7\u5230\u4eba\u985e\u504f\u597d\u8cc7\u6599\u898f\u6a21\u7684\u9650\u5236\u3002\u50b3\u7d71\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u7dda\u4e0b\u504f\u597d\u8cc7\u6599\u96c6\u5efa\u69cb\uff0c\u800c\u6700\u8fd1\u7684\u65b9\u6cd5\u5df2\u8f49\u5411\u7dda\u4e0a\u8a2d\u5b9a\uff0c\u5176\u4e2d\u5b78\u7fd2\u8005\u4f7f\u7528\u5c11\u91cf\u6a19\u7c64\u7a2e\u5b50\u8cc7\u6599\u548c\u5927\u91cf\u7684\u672a\u6a19\u7c64\u63d0\u793a\uff0c\u900f\u904e\u81ea\u6211\u7522\u751f\u7684\u56de\u61c9\u548c\u9ad8\u54c1\u8cea\u734e\u52f5/\u504f\u597d\u56de\u994b\uff0c\u53cd\u8986\u5efa\u69cb\u65b0\u7684\u504f\u597d\u8cc7\u6599\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u76ee\u524d\u7684\u7dda\u4e0a\u6f14\u7b97\u6cd5\u4ecd\u5c08\u6ce8\u65bc\u5728\u7d66\u5b9a\u56de\u994b\u795e\u8aed\u7684\u653f\u7b56\u6a21\u578b\u66f4\u65b0\u671f\u9593\u9032\u884c\u504f\u597d\u6a19\u7c64\uff0c\u9019\u6703\u7522\u751f\u5927\u91cf\u7684\u5c08\u5bb6\u67e5\u8a62\u6210\u672c\u3002\u6211\u5011\u7387\u5148\u63a2\u8a0e\u4e86\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u4ee3\u7406\u734e\u52f5\u795e\u8aed\u5efa\u69cb\u7b56\u7565\uff0c\u4ee5\u9032\u4e00\u6b65\u6a19\u8a18\u504f\u597d\u6216\u734e\u52f5\uff0c\u4e14\u6a19\u7c64\u8cc7\u6599\u548c\u5c08\u5bb6\u67e5\u8a62\u9810\u7b97\u6975\u70ba\u6709\u9650\u3002\u6211\u5011\u7684\u505a\u6cd5\u5f15\u5165\u4e86\u5169\u9805\u95dc\u9375\u5275\u65b0\uff1a(1) \u7b56\u7565\u67e5\u8a62\uff0c\u4ee5\u907f\u514d\u7a2e\u5b50\u8cc7\u6599\u4e2d\u7684 OOD \u548c\u4e0d\u5e73\u8861\u554f\u984c\uff0c\u4ee5\u53ca (2) \u4e3b\u52d5\u5b78\u7fd2\uff0c\u4ee5\u9078\u64c7\u6700\u5177\u8cc7\u8a0a\u6027\u7684\u8cc7\u6599\u9032\u884c\u504f\u597d\u67e5\u8a62\u3002\u4f7f\u7528\u9019\u4e9b\u65b9\u6cd5\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b\u8a55\u4f30\u6a21\u578b\uff0c\u5176\u4f7f\u7528\u6700\u5c11\u7684\u5c08\u5bb6\u6a19\u7c64\u8cc7\u6599\uff0c\u7136\u5f8c\u6709\u6548\u6a19\u8a18\u4e86\u591a\u9054\u4e5d\u500d\u7684\u504f\u597d\u5c0d\uff0c\u4ee5\u9032\u884c\u9032\u4e00\u6b65\u7684 RLHF \u8a13\u7df4\u3002\u4f8b\u5982\uff0c\u6211\u5011\u7684\u6a21\u578b\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO)\uff0c\u5728 AlpacaEval2\u3001MMLU-5shot \u548c MMLU-0shot \u4e0a\u7372\u5f97\u4e86\u8d85\u904e 1% \u7684\u5e73\u5747\u6539\u9032\uff0c\u67e5\u8a62\u6210\u672c\u50c5\u70ba 1.7K\u3002\u6211\u5011\u7684\u505a\u6cd5\u8207\u5176\u4ed6\u57fa\u65bc\u76f4\u63a5\u5c08\u5bb6\u67e5\u8a62\u7684\u7b56\u7565\u6b63\u4ea4\uff0c\u56e0\u6b64\u53ef\u4ee5\u8207\u9019\u4e9b\u7b56\u7565\u6574\u5408\uff0c\u4ee5\u9032\u4e00\u6b65\u964d\u4f4e\u67e5\u8a62\u6210\u672c\u3002", "author": "Yifang Chen et.al.", "authors": "Yifang Chen, Shuohang Wang, Ziyi Yang, Hiteshi Sharma, Nikos Karampatziakis, Donghan Yu, Kevin Jamieson, Simon Shaolei Du, Yelong Shen", "id": "2407.02119v1", "paper_url": "http://arxiv.org/abs/2407.02119v1", "repo": "null"}}