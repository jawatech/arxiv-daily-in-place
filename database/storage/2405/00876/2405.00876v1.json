{"2405.00876": {"publish_time": "2024-05-01", "title": "Beyond Human Vision: The Role of Large Vision Language Models in Microscope Image Analysis", "paper_summary": "Vision language models (VLMs) have recently emerged and gained the spotlight\nfor their ability to comprehend the dual modality of image and textual data.\nVLMs such as LLaVA, ChatGPT-4, and Gemini have recently shown impressive\nperformance on tasks such as natural image captioning, visual question\nanswering (VQA), and spatial reasoning. Additionally, a universal segmentation\nmodel by Meta AI, Segment Anything Model (SAM) shows unprecedented performance\nat isolating objects from unforeseen images. Since medical experts, biologists,\nand materials scientists routinely examine microscopy or medical images in\nconjunction with textual information in the form of captions, literature, or\nreports, and draw conclusions of great importance and merit, it is indubitably\nessential to test the performance of VLMs and foundation models such as SAM, on\nthese images. In this study, we charge ChatGPT, LLaVA, Gemini, and SAM with\nclassification, segmentation, counting, and VQA tasks on a variety of\nmicroscopy images. We observe that ChatGPT and Gemini are impressively able to\ncomprehend the visual features in microscopy images, while SAM is quite capable\nat isolating artefacts in a general sense. However, the performance is not\nclose to that of a domain expert - the models are readily encumbered by the\nintroduction of impurities, defects, artefact overlaps and diversity present in\nthe images.", "paper_summary_zh": "\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff08VLM\uff09\u6700\u8fd1\u6d6e\u51fa\u6aaf\u9762\uff0c\u4e26\u56e0\u5176\u7406\u89e3\u5716\u50cf\u548c\u6587\u5b57\u8cc7\u6599\u96d9\u91cd\u6a21\u5f0f\u7684\u80fd\u529b\u800c\u5099\u53d7\u95dc\u6ce8\u3002LLaVA\u3001ChatGPT-4 \u548c Gemini \u7b49 VLM \u6700\u8fd1\u5728\u81ea\u7136\u5716\u50cf\u6a19\u984c\u3001\u8996\u89ba\u554f\u984c\u56de\u7b54\uff08VQA\uff09\u548c\u7a7a\u9593\u63a8\u7406\u7b49\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u3002\u6b64\u5916\uff0cMeta AI \u7684\u901a\u7528\u5206\u5272\u6a21\u578b Segment Anything Model\uff08SAM\uff09\u5728\u5f9e\u672a\u898b\u904e\u7684\u5716\u50cf\u4e2d\u5206\u96e2\u7269\u9ad4\u65b9\u9762\u8868\u73fe\u51fa\u524d\u6240\u672a\u6709\u7684\u6548\u80fd\u3002\u7531\u65bc\u91ab\u5b78\u5c08\u5bb6\u3001\u751f\u7269\u5b78\u5bb6\u548c\u6750\u6599\u79d1\u5b78\u5bb6\u5e38\u898f\u6aa2\u67e5\u986f\u5fae\u93e1\u6216\u91ab\u5b78\u5716\u50cf\uff0c\u4e26\u7d50\u5408\u6a19\u984c\u3001\u6587\u737b\u6216\u5831\u544a\u7b49\u5f62\u5f0f\u7684\u6587\u5b57\u8cc7\u8a0a\uff0c\u4e26\u5f97\u51fa\u6975\u5176\u91cd\u8981\u4e14\u6709\u50f9\u503c\u7684\u7d50\u8ad6\uff0c\u56e0\u6b64\u7121\u5eb8\u7f6e\u7591\u5730\uff0c\u5728\u9019\u4e9b\u5716\u50cf\u4e0a\u6e2c\u8a66 VLM \u548c SAM \u7b49\u57fa\u790e\u6a21\u578b\u7684\u6548\u80fd\u81f3\u95dc\u91cd\u8981\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u8b93 ChatGPT\u3001LLaVA\u3001Gemini \u548c SAM \u57f7\u884c\u5404\u7a2e\u986f\u5fae\u93e1\u5716\u50cf\u7684\u5206\u985e\u3001\u5206\u5272\u3001\u8a08\u6578\u548c VQA \u4efb\u52d9\u3002\u6211\u5011\u89c0\u5bdf\u5230\uff0cChatGPT \u548c Gemini \u80fd\u5920\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u5730\u7406\u89e3\u986f\u5fae\u93e1\u5716\u50cf\u4e2d\u7684\u8996\u89ba\u7279\u5fb5\uff0c\u800c SAM \u5728\u5ee3\u7fa9\u4e0a\u76f8\u7576\u64c5\u9577\u5206\u96e2\u4eba\u5de5\u88fd\u54c1\u3002\u7136\u800c\uff0c\u6548\u80fd\u4e26\u672a\u9054\u5230\u9818\u57df\u5c08\u5bb6\u7684\u6c34\u6e96\u2014\u2014\u9019\u4e9b\u6a21\u578b\u5f88\u5bb9\u6613\u53d7\u5230\u5716\u50cf\u4e2d\u96dc\u8cea\u3001\u7f3a\u9677\u3001\u4eba\u5de5\u88fd\u54c1\u91cd\u758a\u548c\u591a\u6a23\u6027\u7684\u5f71\u97ff\u3002", "author": "Prateek Verma et.al.", "authors": "Prateek Verma, Minh-Hao Van, Xintao Wu", "id": "2405.00876v1", "paper_url": "http://arxiv.org/abs/2405.00876v1", "repo": "null"}}