{"2406.07971": {"publish_time": "2024-06-12", "title": "It Takes Two: On the Seamlessness between Reward and Policy Model in RLHF", "paper_summary": "Reinforcement Learning from Human Feedback (RLHF) involves training policy\nmodels (PMs) and reward models (RMs) to align language models with human\npreferences. Instead of focusing solely on PMs and RMs independently, we\npropose to examine their interactions during fine-tuning, introducing the\nconcept of seamlessness. Our study starts with observing the saturation\nphenomenon, where continual improvements in RM and PM do not translate into\nRLHF progress. Our analysis shows that RMs fail to assign proper scores to PM\nresponses, resulting in a 35% mismatch rate with human preferences,\nhighlighting a significant discrepancy between PM and RM. To measure\nseamlessness between PM and RM without human effort, we propose an automatic\nmetric, SEAM. SEAM quantifies the discrepancies between PM and RM judgments\ninduced by data samples. We validate the effectiveness of SEAM in data\nselection and model augmentation. Our experiments demonstrate that (1) using\nSEAM-filtered data for RL training improves RLHF performance by 4.5%, and (2)\nSEAM-guided model augmentation results in a 4% performance improvement over\nstandard augmentation methods.", "paper_summary_zh": "\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u6d89\u53ca\u8bad\u7ec3\u7b56\u7565\u6a21\u578b\uff08PM\uff09\u548c\u5956\u52b1\u6a21\u578b\uff08RM\uff09\uff0c\u4ee5\u4f7f\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u6211\u4eec\u4e0d\u53ea\u4e13\u6ce8\u4e8e\u72ec\u7acb\u7684 PM \u548c RM\uff0c\u8fd8\u5efa\u8bae\u5728\u5fae\u8c03\u671f\u95f4\u68c0\u67e5\u5b83\u4eec\u7684\u4ea4\u4e92\uff0c\u5f15\u5165\u65e0\u7f1d\u6027\u7684\u6982\u5ff5\u3002\u6211\u4eec\u7684\u7814\u7a76\u4ece\u89c2\u5bdf\u9971\u548c\u73b0\u8c61\u5f00\u59cb\uff0c\u5176\u4e2d RM \u548c PM \u7684\u6301\u7eed\u6539\u8fdb\u5e76\u4e0d\u80fd\u8f6c\u5316\u4e3a RLHF \u7684\u8fdb\u6b65\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0cRM \u65e0\u6cd5\u4e3a PM \u54cd\u5e94\u5206\u914d\u9002\u5f53\u7684\u5206\u6570\uff0c\u5bfc\u81f4\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u7b26\u7684\u6bd4\u7387\u8fbe\u5230 35%\uff0c\u7a81\u51fa\u4e86 PM \u548c RM \u4e4b\u95f4\u7684\u663e\u7740\u5dee\u5f02\u3002\u4e3a\u4e86\u5728\u6ca1\u6709\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u8861\u91cf PM \u548c RM \u4e4b\u95f4\u7684\u65e0\u7f1d\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u6307\u6807 SEAM\u3002SEAM \u91cf\u5316\u4e86\u7531\u6570\u636e\u6837\u672c\u5f15\u8d77\u7684 PM \u548c RM \u5224\u65ad\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6211\u4eec\u5728\u6570\u636e\u9009\u62e9\u548c\u6a21\u578b\u589e\u5f3a\u4e2d\u9a8c\u8bc1\u4e86 SEAM \u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\uff081\uff09\u4f7f\u7528 SEAM \u8fc7\u6ee4\u7684\u6570\u636e\u8fdb\u884c RL \u8bad\u7ec3\u53ef\u5c06 RLHF \u6027\u80fd\u63d0\u9ad8 4.5%\uff0c\uff082\uff09SEAM \u6307\u5bfc\u7684\u6a21\u578b\u589e\u5f3a\u6bd4\u6807\u51c6\u589e\u5f3a\u65b9\u6cd5\u63d0\u9ad8\u4e86 4% \u7684\u6027\u80fd\u3002", "author": "Taiming Lu et.al.", "authors": "Taiming Lu, Lingfeng Shen, Xinyu Yang, Weiting Tan, Beidi Chen, Huaxiu Yao", "id": "2406.07971v1", "paper_url": "http://arxiv.org/abs/2406.07971v1", "repo": "https://github.com/taiminglu/seamless"}}