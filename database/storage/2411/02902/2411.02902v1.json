{"2411.02902": {"publish_time": "2024-11-05", "title": "Membership Inference Attacks against Large Vision-Language Models", "paper_summary": "Large vision-language models (VLLMs) exhibit promising capabilities for\nprocessing multi-modal tasks across various application scenarios. However,\ntheir emergence also raises significant data security concerns, given the\npotential inclusion of sensitive information, such as private photos and\nmedical records, in their training datasets. Detecting inappropriately used\ndata in VLLMs remains a critical and unresolved issue, mainly due to the lack\nof standardized datasets and suitable methodologies. In this study, we\nintroduce the first membership inference attack (MIA) benchmark tailored for\nvarious VLLMs to facilitate training data detection. Then, we propose a novel\nMIA pipeline specifically designed for token-level image detection. Lastly, we\npresent a new metric called MaxR\\'enyi-K%, which is based on the confidence of\nthe model output and applies to both text and image data. We believe that our\nwork can deepen the understanding and methodology of MIAs in the context of\nVLLMs. Our code and datasets are available at\nhttps://github.com/LIONS-EPFL/VL-MIA.", "paper_summary_zh": "\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLLM) \u5728\u8655\u7406\u5404\u7a2e\u61c9\u7528\u5834\u666f\u7684\u591a\u6a21\u614b\u4efb\u52d9\u65b9\u9762\u8868\u73fe\u51fa\u6709\u524d\u666f\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u5011\u7684\u51fa\u73fe\u4e5f\u5f15\u767c\u4e86\u91cd\u5927\u7684\u8cc7\u6599\u5b89\u5168\u554f\u984c\uff0c\u56e0\u70ba\u5b83\u5011\u7684\u8a13\u7df4\u8cc7\u6599\u96c6\u4e2d\u53ef\u80fd\u6703\u5305\u542b\u654f\u611f\u8cc7\u8a0a\uff0c\u4f8b\u5982\u79c1\u4eba\u7167\u7247\u548c\u91ab\u7642\u8a18\u9304\u3002\u5075\u6e2c VLLM \u4e2d\u4e0d\u7576\u4f7f\u7528\u7684\u8cc7\u6599\u4ecd\u7136\u662f\u4e00\u500b\u95dc\u9375\u4e14\u5c1a\u672a\u89e3\u6c7a\u7684\u554f\u984c\uff0c\u4e3b\u8981\u662f\u7531\u65bc\u7f3a\u4e4f\u6a19\u6e96\u5316\u7684\u8cc7\u6599\u96c6\u548c\u9069\u7576\u7684\u65b9\u6cd5\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u7b2c\u4e00\u500b\u91dd\u5c0d\u5404\u7a2e VLLM \u91cf\u8eab\u6253\u9020\u7684\u6210\u54e1\u63a8\u8ad6\u653b\u64ca (MIA) \u57fa\u6e96\uff0c\u4ee5\u5229\u65bc\u8a13\u7df4\u8cc7\u6599\u5075\u6e2c\u3002\u7136\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5c08\u9580\u8a2d\u8a08\u7528\u65bc\u4ee4\u724c\u7d1a\u5225\u5f71\u50cf\u5075\u6e2c\u7684\u5168\u65b0 MIA \u7ba1\u7dda\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u540d\u70ba MaxR\\'enyi-K% \u7684\u65b0\u6307\u6a19\uff0c\u5b83\u57fa\u65bc\u6a21\u578b\u8f38\u51fa\u7684\u4fe1\u5fc3\uff0c\u4e26\u9069\u7528\u65bc\u6587\u5b57\u548c\u5f71\u50cf\u8cc7\u6599\u3002\u6211\u5011\u76f8\u4fe1\uff0c\u6211\u5011\u7684\u7814\u7a76\u53ef\u4ee5\u52a0\u6df1\u5c0d VLLM \u80cc\u666f\u4e0b MIA \u7684\u7406\u89e3\u548c\u65b9\u6cd5\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u548c\u8cc7\u6599\u96c6\u53ef\u5728 https://github.com/LIONS-EPFL/VL-MIA \u53d6\u5f97\u3002", "author": "Zhan Li et.al.", "authors": "Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, Volkan Cevher", "id": "2411.02902v1", "paper_url": "http://arxiv.org/abs/2411.02902v1", "repo": "https://github.com/lions-epfl/vl-mia"}}