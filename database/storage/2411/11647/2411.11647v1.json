{"2411.11647": {"publish_time": "2024-11-18", "title": "No-regret Exploration in Shuffle Private Reinforcement Learning", "paper_summary": "Differential privacy (DP) has recently been introduced into episodic\nreinforcement learning (RL) to formally address user privacy concerns in\npersonalized services. Previous work mainly focuses on two trust models of DP:\nthe central model, where a central agent is responsible for protecting users'\nsensitive data, and the (stronger) local model, where the protection occurs\ndirectly on the user side. However, they either require a trusted central agent\nor incur a significantly higher privacy cost, making it unsuitable for many\nscenarios. This work introduces a trust model stronger than the central model\nbut with a lower privacy cost than the local model, leveraging the emerging\n\\emph{shuffle} model of privacy. We present the first generic algorithm for\nepisodic RL under the shuffle model, where a trusted shuffler randomly permutes\na batch of users' data before sending it to the central agent. We then\ninstantiate the algorithm using our proposed shuffle Privatizer, relying on a\nshuffle private binary summation mechanism. Our analysis shows that the\nalgorithm achieves a near-optimal regret bound comparable to that of the\ncentralized model and significantly outperforms the local model in terms of\nprivacy cost.", "paper_summary_zh": "\u5dee\u5206\u9690\u79c1 (DP) \u6700\u8fd1\u5df2\u5f15\u5165\u5230\u60c5\u5883\u5f3a\u5316\u5b66\u4e60 (RL) \u4e2d\uff0c\u4ee5\u6b63\u5f0f\u89e3\u51b3\u4e2a\u6027\u5316\u670d\u52a1\u4e2d\u7684\u7528\u6237\u9690\u79c1\u95ee\u9898\u3002\u5148\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8 DP \u7684\u4e24\u79cd\u4fe1\u4efb\u6a21\u578b\uff1a\u4e2d\u5fc3\u6a21\u578b\uff0c\u5176\u4e2d\u4e00\u4e2a\u4e2d\u5fc3\u4ee3\u7406\u8d1f\u8d23\u4fdd\u62a4\u7528\u6237\u7684\u654f\u611f\u6570\u636e\uff0c\u4ee5\u53ca\uff08\u66f4\u5f3a\u7684\uff09\u672c\u5730\u6a21\u578b\uff0c\u5176\u4e2d\u4fdd\u62a4\u76f4\u63a5\u53d1\u751f\u5728\u7528\u6237\u7aef\u3002\u7136\u800c\uff0c\u5b83\u4eec\u8981\u4e48\u9700\u8981\u4e00\u4e2a\u53d7\u4fe1\u4efb\u7684\u4e2d\u5fc3\u4ee3\u7406\uff0c\u8981\u4e48\u4ea7\u751f\u660e\u663e\u66f4\u9ad8\u7684\u9690\u79c1\u6210\u672c\uff0c\u4f7f\u5176\u4e0d\u9002\u7528\u4e8e\u8bb8\u591a\u573a\u666f\u3002\u8fd9\u9879\u5de5\u4f5c\u5f15\u5165\u4e86\u4e00\u4e2a\u6bd4\u4e2d\u5fc3\u6a21\u578b\u66f4\u5f3a\u4f46\u9690\u79c1\u6210\u672c\u4f4e\u4e8e\u672c\u5730\u6a21\u578b\u7684\u4fe1\u4efb\u6a21\u578b\uff0c\u5229\u7528\u4e86\u65b0\u5174\u7684\u9690\u79c1\u201c\u6df7\u6d17\u201d\u6a21\u578b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u6df7\u6d17\u6a21\u578b\u4e0b\u60c5\u5883 RL \u7684\u7b2c\u4e00\u4e2a\u901a\u7528\u7b97\u6cd5\uff0c\u5176\u4e2d\u4e00\u4e2a\u53d7\u4fe1\u4efb\u7684\u6df7\u6d17\u5668\u5728\u5c06\u4e00\u6279\u7528\u6237\u6570\u636e\u53d1\u9001\u5230\u4e2d\u5fc3\u4ee3\u7406\u4e4b\u524d\u968f\u673a\u6392\u5217\u8fd9\u4e9b\u6570\u636e\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u6df7\u6d17\u79c1\u6709\u5316\u5668\u5b9e\u4f8b\u5316\u8be5\u7b97\u6cd5\uff0c\u4f9d\u9760\u6df7\u6d17\u79c1\u6709\u4e8c\u8fdb\u5236\u6c42\u548c\u673a\u5236\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u4e0e\u96c6\u4e2d\u5f0f\u6a21\u578b\u76f8\u5f53\u7684\u8fd1\u4f3c\u6700\u4f18\u540e\u6094\u754c\uff0c\u5e76\u4e14\u5728\u9690\u79c1\u6210\u672c\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u672c\u5730\u6a21\u578b\u3002", "author": "Shaojie Bai et.al.", "authors": "Shaojie Bai, Mohammad Sadegh Talebi, Chengcheng Zhao, Peng Cheng, Jiming Chen", "id": "2411.11647v1", "paper_url": "http://arxiv.org/abs/2411.11647v1", "repo": "null"}}