{"2410.22223": {"publish_time": "2024-10-29", "title": "MAPUNetR: A Hybrid Vision Transformer and U-Net Architecture for Efficient and Interpretable Medical Image Segmentation", "paper_summary": "Medical image segmentation is pivotal in healthcare, enhancing diagnostic\naccuracy, informing treatment strategies, and tracking disease progression.\nThis process allows clinicians to extract critical information from visual\ndata, enabling personalized patient care. However, developing neural networks\nfor segmentation remains challenging, especially when preserving image\nresolution, which is essential in detecting subtle details that influence\ndiagnoses. Moreover, the lack of transparency in these deep learning models has\nslowed their adoption in clinical practice. Efforts in model interpretability\nare increasingly focused on making these models' decision-making processes more\ntransparent. In this paper, we introduce MAPUNetR, a novel architecture that\nsynergizes the strengths of transformer models with the proven U-Net framework\nfor medical image segmentation. Our model addresses the resolution preservation\nchallenge and incorporates attention maps highlighting segmented regions,\nincreasing accuracy and interpretability. Evaluated on the BraTS 2020 dataset,\nMAPUNetR achieved a dice score of 0.88 and a dice coefficient of 0.92 on the\nISIC 2018 dataset. Our experiments show that the model maintains stable\nperformance and potential as a powerful tool for medical image segmentation in\nclinical practice.", "paper_summary_zh": "\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u5728\u91ab\u7642\u4fdd\u5065\u4e2d\u81f3\u95dc\u91cd\u8981\uff0c\u80fd\u63d0\u5347\u8a3a\u65b7\u6e96\u78ba\u5ea6\u3001\u63d0\u4f9b\u6cbb\u7642\u7b56\u7565\u8cc7\u8a0a\uff0c\u4e26\u8ffd\u8e64\u75be\u75c5\u9032\u7a0b\u3002\u6b64\u7a0b\u5e8f\u8b93\u81e8\u5e8a\u91ab\u751f\u80fd\u5f9e\u8996\u89ba\u8cc7\u6599\u4e2d\u8403\u53d6\u95dc\u9375\u8cc7\u8a0a\uff0c\u9032\u800c\u63d0\u4f9b\u500b\u4eba\u5316\u7684\u60a3\u8005\u7167\u8b77\u3002\u7136\u800c\uff0c\u958b\u767c\u7528\u65bc\u5206\u5272\u7684\u795e\u7d93\u7db2\u8def\u4ecd\u5177\u6311\u6230\u6027\uff0c\u7279\u5225\u662f\u5728\u4fdd\u7559\u5f71\u50cf\u89e3\u6790\u5ea6\u6642\uff0c\u9019\u5c0d\u65bc\u5075\u6e2c\u5f71\u97ff\u8a3a\u65b7\u7684\u7d30\u5fae\u7d30\u7bc0\u81f3\u95dc\u91cd\u8981\u3002\u6b64\u5916\uff0c\u9019\u4e9b\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5c0e\u81f4\u5176\u5728\u81e8\u5e8a\u5be6\u52d9\u4e2d\u7684\u63a1\u7528\u901f\u5ea6\u8b8a\u6162\u3002\u6a21\u578b\u53ef\u89e3\u91cb\u6027\u7684\u52aa\u529b\u8d8a\u4f86\u8d8a\u5c08\u6ce8\u65bc\u8b93\u9019\u4e9b\u6a21\u578b\u7684\u6c7a\u7b56\u904e\u7a0b\u66f4\u900f\u660e\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 MAPUNetR\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u7d50\u5408\u4e86Transformer\u6a21\u578b\u7684\u512a\u9ede\u548c\u5df2\u8b49\u5be6\u7684 U-Net \u6846\u67b6\uff0c\u7528\u65bc\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u3002\u6211\u5011\u7684\u6a21\u578b\u89e3\u6c7a\u4e86\u89e3\u6790\u5ea6\u4fdd\u7559\u7684\u6311\u6230\uff0c\u4e26\u7d50\u5408\u4e86\u7a81\u986f\u5206\u5272\u5340\u57df\u7684\u6ce8\u610f\u529b\u5716\uff0c\u63d0\u9ad8\u4e86\u6e96\u78ba\u5ea6\u548c\u53ef\u89e3\u91cb\u6027\u3002\u5728 BraTS 2020 \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a55\u4f30\uff0cMAPUNetR \u5728 ISIC 2018 \u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86 0.88 \u7684\u9ab0\u5b50\u4fc2\u6578\u548c 0.92 \u7684\u9ab0\u5b50\u7cfb\u6578\u3002\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0c\u8a72\u6a21\u578b\u5728\u81e8\u5e8a\u5be6\u52d9\u4e2d\u4f5c\u70ba\u91ab\u5b78\u5f71\u50cf\u5206\u5272\u7684\u5f37\u5927\u5de5\u5177\uff0c\u5177\u6709\u7a69\u5b9a\u7684\u6548\u80fd\u548c\u6f5b\u529b\u3002", "author": "Ovais Iqbal Shah et.al.", "authors": "Ovais Iqbal Shah, Danish Raza Rizvi, Aqib Nazir Mir", "id": "2410.22223v1", "paper_url": "http://arxiv.org/abs/2410.22223v1", "repo": "null"}}