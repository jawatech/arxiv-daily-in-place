{"2406.04692": {"publish_time": "2024-06-07", "title": "Mixture-of-Agents Enhances Large Language Model Capabilities", "paper_summary": "Recent advances in large language models (LLMs) demonstrate substantial\ncapabilities in natural language understanding and generation tasks. With the\ngrowing number of LLMs, how to harness the collective expertise of multiple\nLLMs is an exciting open direction. Toward this goal, we propose a new approach\nthat leverages the collective strengths of multiple LLMs through a\nMixture-of-Agents (MoA) methodology. In our approach, we construct a layered\nMoA architecture wherein each layer comprises multiple LLM agents. Each agent\ntakes all the outputs from agents in the previous layer as auxiliary\ninformation in generating its response. MoA models achieves state-of-art\nperformance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For\nexample, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by\na substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u9032\u5c55\u5728\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u548c\u751f\u6210\u4efb\u52d9\u4e2d\u5c55\u73fe\u4e86\u5f37\u5927\u7684\u80fd\u529b\u3002\u96a8\u8457 LLM \u6578\u91cf\u7684\u589e\u52a0\uff0c\u5982\u4f55\u5229\u7528\u591a\u500b LLM \u7684\u96c6\u9ad4\u5c08\u696d\u77e5\u8b58\u662f\u4e00\u500b\u4ee4\u4eba\u8208\u596e\u7684\u958b\u653e\u65b9\u5411\u3002\u70ba\u4e86\u5be6\u73fe\u9019\u4e00\u76ee\u6a19\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u901a\u904e\u6df7\u5408\u4ee3\u7406\uff08MoA\uff09\u65b9\u6cd5\u5229\u7528\u591a\u500b LLM \u7684\u96c6\u9ad4\u512a\u52e2\u3002\u5728\u6211\u5011\u7684\u505a\u6cd5\u4e2d\uff0c\u6211\u5011\u69cb\u5efa\u4e86\u4e00\u500b\u5206\u5c64\u7684 MoA \u67b6\u69cb\uff0c\u5176\u4e2d\u6bcf\u4e00\u5c64\u90fd\u5305\u542b\u591a\u500b LLM \u4ee3\u7406\u3002\u6bcf\u500b\u4ee3\u7406\u90fd\u5c07\u524d\u4e00\u5c64\u4ee3\u7406\u7684\u6240\u6709\u8f38\u51fa\u4f5c\u70ba\u8f14\u52a9\u4fe1\u606f\uff0c\u4ee5\u751f\u6210\u5176\u97ff\u61c9\u3002MoA \u6a21\u578b\u5728 AlpacaEval 2.0\u3001MT-Bench \u548c FLASK \u4e0a\u5be6\u73fe\u4e86\u6700\u5148\u9032\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86 GPT-4 Omni\u3002\u4f8b\u5982\uff0c\u6211\u5011\u50c5\u4f7f\u7528\u958b\u6e90 LLM \u7684 MoA \u4ee5\u5927\u5e45\u5dee\u8ddd\u9818\u5148 AlpacaEval 2.0\uff0c\u8207 GPT-4 Omni \u7684 57.5% \u76f8\u6bd4\uff0c\u9054\u5230 65.1% \u7684\u5206\u6578\u3002", "author": "Junlin Wang et.al.", "authors": "Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou", "id": "2406.04692v1", "paper_url": "http://arxiv.org/abs/2406.04692v1", "repo": "null"}}