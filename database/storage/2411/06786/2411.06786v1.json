{"2411.06786": {"publish_time": "2024-11-11", "title": "ScaleKD: Strong Vision Transformers Could Be Excellent Teachers", "paper_summary": "In this paper, we question if well pre-trained vision transformer (ViT)\nmodels could be used as teachers that exhibit scalable properties to advance\ncross architecture knowledge distillation (KD) research, in the context of\nusing large-scale datasets for evaluation. To make this possible, our analysis\nunderlines the importance of seeking effective strategies to align (1) feature\ncomputing paradigm differences, (2) model scale differences, and (3) knowledge\ndensity differences. By combining three coupled components namely cross\nattention projector, dual-view feature mimicking and teacher parameter\nperception tailored to address the above problems, we present a simple and\neffective KD method, called ScaleKD. Our method can train student backbones\nthat span across a variety of convolutional neural network (CNN), multi-layer\nperceptron (MLP), and ViT architectures on image classification datasets,\nachieving state-of-the-art distillation performance. For instance, taking a\nwell pre-trained Swin-L as the teacher model, our method gets\n75.15%|82.03%|84.16%|78.63%|81.96%|83.93%|83.80%|85.53% top-1 accuracies for\nMobileNet-V1|ResNet-50|ConvNeXt-T|Mixer-S/16|Mixer-B/16|ViT-S/16|Swin-T|ViT-B/16\nmodels trained on ImageNet-1K dataset from scratch, showing\n3.05%|3.39%|2.02%|4.61%|5.52%|4.03%|2.62%|3.73% absolute gains to the\nindividually trained counterparts. Intriguingly, when scaling up the size of\nteacher models or their pre-training datasets, our method showcases the desired\nscalable properties, bringing increasingly larger gains to student models. The\nstudent backbones trained by our method transfer well on downstream MS-COCO and\nADE20K datasets. More importantly, our method could be used as a more efficient\nalternative to the time-intensive pre-training paradigm for any target student\nmodel if a strong pre-trained ViT is available, reducing the amount of viewed\ntraining samples up to 195x.", "paper_summary_zh": "<paragraph>\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8d28\u7591\u7ecf\u8fc7\u826f\u597d\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8f6c\u6362\u5668 (ViT) \u6a21\u578b\u662f\u5426\u53ef\u7528\u4f5c\u6559\u5e08\uff0c\u5728\u8bc4\u4f30\u4e2d\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\uff0c\u5c55\u793a\u53ef\u6269\u5c55\u5c5e\u6027\u4ee5\u63a8\u8fdb\u8de8\u67b6\u6784\u77e5\u8bc6\u84b8\u998f (KD) \u7814\u7a76\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u7684\u5206\u6790\u5f3a\u8c03\u4e86\u5bfb\u6c42\u6709\u6548\u7b56\u7565\u4ee5\u5bf9\u9f50 (1) \u7279\u5f81\u8ba1\u7b97\u8303\u4f8b\u5dee\u5f02\u3001(2) \u6a21\u578b\u89c4\u6a21\u5dee\u5f02\u548c (3) \u77e5\u8bc6\u5bc6\u5ea6\u5dee\u5f02\u7684\u91cd\u8981\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u4e09\u4e2a\u8026\u5408\u7ec4\u4ef6\uff0c\u5373\u8de8\u6ce8\u610f\u529b\u6295\u5f71\u4eea\u3001\u53cc\u89c6\u56fe\u7279\u5f81\u6a21\u62df\u548c\u9488\u5bf9\u4e0a\u8ff0\u95ee\u9898\u91cf\u8eab\u5b9a\u5236\u7684\u6559\u5e08\u53c2\u6570\u611f\u77e5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684 KD \u65b9\u6cd5\uff0c\u79f0\u4e3a ScaleKD\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u8bad\u7ec3\u8de8\u8d8a\u5404\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN)\u3001\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u548c ViT \u67b6\u6784\u7684\u5b66\u751f\u4e3b\u5e72\u5728\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u84b8\u998f\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u4ee5\u7ecf\u8fc7\u826f\u597d\u9884\u8bad\u7ec3\u7684 Swin-L \u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86 75.15%|82.03%|84.16%|78.63%|81.96%|83.93%|83.80%|85.53% \u7684 top-1 \u51c6\u786e\u7387\uff0c\u7528\u4e8e\u5728 ImageNet-1K \u6570\u636e\u96c6\u4e0a\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u7684 MobileNet-V1|ResNet-50|ConvNeXt-T|Mixer-S/16|Mixer-B/16|ViT-S/16|Swin-T|ViT-B/16 \u6a21\u578b\uff0c\u663e\u793a\u51fa 3.05%|3.39%|2.02%|4.61%|5.52%|4.03%|2.62%|3.73% \u7684\u7edd\u5bf9\u589e\u76ca\uff0c\u9ad8\u4e8e\u5355\u72ec\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\u3002\u6709\u8da3\u7684\u662f\uff0c\u5f53\u6269\u5927\u6559\u5e08\u6a21\u578b\u6216\u5176\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u89c4\u6a21\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c55\u793a\u4e86\u6240\u9700\u7684\u53ef\u6269\u5c55\u5c5e\u6027\uff0c\u4e3a\u5b66\u751f\u6a21\u578b\u5e26\u6765\u4e86\u8d8a\u6765\u8d8a\u5927\u7684\u6536\u76ca\u3002\u7531\u6211\u4eec\u7684\u65b9\u6cd5\u8bad\u7ec3\u7684\u5b66\u751f\u4e3b\u5e72\u5f88\u597d\u5730\u8f6c\u79fb\u5230\u4e0b\u6e38 MS-COCO \u548c ADE20K \u6570\u636e\u96c6\u4e0a\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5982\u679c\u53ef\u4ee5\u4f7f\u7528\u7ecf\u8fc7\u826f\u597d\u9884\u8bad\u7ec3\u7684 ViT\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u7528\u4f5c\u4efb\u4f55\u76ee\u6807\u5b66\u751f\u6a21\u578b\u7684\u65f6\u95f4\u5bc6\u96c6\u578b\u9884\u8bad\u7ec3\u8303\u4f8b\u7684\u66f4\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4ece\u800c\u5c06\u67e5\u770b\u7684\u8bad\u7ec3\u6837\u672c\u6570\u91cf\u51cf\u5c11\u591a\u8fbe 195 \u500d\u3002</paragraph>", "author": "Jiawei Fan et.al.", "authors": "Jiawei Fan, Chao Li, Xiaolong Liu, Anbang Yao", "id": "2411.06786v1", "paper_url": "http://arxiv.org/abs/2411.06786v1", "repo": "https://github.com/deep-optimization/scalekd"}}