{"2408.14845": {"publish_time": "2024-08-27", "title": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark", "paper_summary": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live.", "paper_summary_zh": "\u6aa2\u6e2c\u975e\u88d4\u7f8e\u570b\u4eba\u767d\u8a71\u82f1\u8a9e (AAVE) \u4e2d\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u7684\u504f\u5dee\u5c0d\u65bc\u958b\u767c\u5305\u5bb9\u6027\u7684\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u7cfb\u7d71\u81f3\u95dc\u91cd\u8981\u3002\u70ba\u4e86\u89e3\u6c7a\u65b9\u8a00\u5f15\u8d77\u7684\u6548\u80fd\u5dee\u7570\uff0c\u6211\u5011\u5f15\u5165\u4e86 AAVENUE\uff08{AAVE} {N}atural Language {U}nderstanding {E}valuation\uff09\uff0c\u9019\u662f\u4e00\u500b\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728 AAVE \u548c\u6a19\u6e96\u7f8e\u5f0f\u82f1\u8a9e (SAE) \u7684 NLU \u4efb\u52d9\u4e0a\u6548\u80fd\u7684\u57fa\u6e96\u3002AAVENUE \u5efa\u7acb\u5728\u73fe\u6709\u57fa\u6e96\uff08\u5982 VALUE\uff09\u7684\u57fa\u790e\u4e0a\u4e26\u4e88\u4ee5\u64f4\u5145\uff0c\u7528\u66f4\u9748\u6d3b\u7684\u65b9\u6cd5\u53d6\u4ee3\u78ba\u5b9a\u6027\u7684\u53e5\u6cd5\u548c\u5f62\u614b\u8f49\u63db\uff0c\u4e26\u5229\u7528\u57fa\u65bc LLM \u7684\u7ffb\u8b6f\u548c\u5c11\u91cf\u63d0\u793a\uff0c\u6539\u5584\u4e86\u6211\u5011\u5728\u7ffb\u8b6f GLUE \u548c SuperGLUE \u57fa\u6e96\u4e2d\u7684\u95dc\u9375\u4efb\u52d9\u6642\u7684\u6548\u80fd\u8a55\u4f30\u6307\u6a19\u3002\u6211\u5011\u4f7f\u7528\u4e94\u7a2e\u6d41\u884c\u7684 LLM \u548c\u4e00\u7d44\u5168\u9762\u7684\u6307\u6a19\uff08\u5305\u62ec\u6d41\u66a2\u5ea6\u3001BARTScore\u3001\u54c1\u8cea\u3001\u9023\u8cab\u6027\u548c\u53ef\u7406\u89e3\u6027\uff09\u4f86\u6bd4\u8f03 AAVENUE \u548c VALUE \u7ffb\u8b6f\u3002\u6b64\u5916\uff0c\u6211\u5011\u62db\u52df\u4e86\u6d41\u5229\u7684 AAVE \u8aaa\u8a71\u8005\u4f86\u9a57\u8b49\u6211\u5011\u7ffb\u8b6f\u7684\u771f\u5be6\u6027\u3002\u6211\u5011\u7684\u8a55\u4f30\u986f\u793a\uff0cLLM \u5728 SAE \u4efb\u52d9\u4e0a\u7684\u8868\u73fe\u59cb\u7d42\u512a\u65bc AAVE \u7ffb\u8b6f\u7248\u672c\uff0c\u9019\u51f8\u986f\u4e86\u5167\u5728\u504f\u5dee\u4e26\u5f37\u8abf\u4e86\u5c0d\u66f4\u5177\u5305\u5bb9\u6027\u7684 NLP \u6a21\u578b\u7684\u9700\u6c42\u3002\u6211\u5011\u5df2\u5728 GitHub \u4e0a\u958b\u6e90\u4e86\u6211\u5011\u7684\u539f\u59cb\u78bc\uff0c\u4e26\u5efa\u7acb\u4e86\u4e00\u500b\u7db2\u7ad9\u4f86\u5c55\u793a\u6211\u5011\u7684\u4f5c\u54c1\uff0c\u7db2\u5740\u70ba https://aavenue.live\u3002", "author": "Abhay Gupta et.al.", "authors": "Abhay Gupta, Philip Meng, Ece Yurtseven, Sean O'Brien, Kevin Zhu", "id": "2408.14845v1", "paper_url": "http://arxiv.org/abs/2408.14845v1", "repo": "null"}}