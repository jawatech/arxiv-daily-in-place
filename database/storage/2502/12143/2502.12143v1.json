{"2502.12143": {"publish_time": "2025-02-17", "title": "Small Models Struggle to Learn from Strong Reasoners", "paper_summary": "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8907\u96dc\u63a8\u7406\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\uff0c\u4e14\u5c07\u5176\u63a8\u7406\u80fd\u529b\u63d0\u7149\u6210\u8f03\u5c0f\u7684\u6a21\u578b\u5df2\u5c55\u73fe\u524d\u666f\u3002\u7136\u800c\uff0c\u6211\u5011\u767c\u73fe\u4e86\u4e00\u500b\u6709\u8da3\u7684\u73fe\u8c61\uff0c\u6211\u5011\u7a31\u4e4b\u70ba\u5c0f\u578b\u6a21\u578b\u53ef\u5b78\u7fd2\u6027\u5dee\u8ddd\uff1a\u5c0f\u578b\u6a21\u578b\uff08\u53c3\u6578\u6578\u76ee \u2264 3B\uff09\u4e26\u975e\u7e3d\u80fd\u5f9e\u5927\u578b\u6a21\u578b\u7684\u9577\u93c8\u689d\u601d\u8003 (CoT) \u63a8\u7406\u6216\u63d0\u7149\u4e2d\u53d7\u76ca\u3002\u76f8\u53cd\u5730\uff0c\u7576\u91dd\u5c0d\u8f03\u77ed\u3001\u8f03\u7c21\u55ae\u7684\u63a8\u7406\u93c8\u9032\u884c\u5fae\u8abf\u6642\uff0c\u5b83\u5011\u7684\u8868\u73fe\u6703\u66f4\u597d\uff0c\u800c\u9019\u66f4\u7b26\u5408\u5176\u5167\u5728\u5b78\u7fd2\u80fd\u529b\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u6df7\u5408\u63d0\u7149\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u4f46\u6709\u6548\u7684\u7b56\u7565\uff0c\u900f\u904e\u7d50\u5408\u9577\u77ed CoT \u7bc4\u4f8b\u6216\u5f9e\u8f03\u5927\u53ca\u8f03\u5c0f\u6a21\u578b\u9032\u884c\u63a8\u7406\uff0c\u4f86\u5e73\u8861\u63a8\u7406\u7684\u8907\u96dc\u6027\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u50c5\u91dd\u5c0d\u4efb\u4e00\u8cc7\u6599\u9032\u884c\u8a13\u7df4\u76f8\u6bd4\uff0c\u6df7\u5408\u63d0\u7149\u986f\u8457\u6539\u5584\u4e86\u5c0f\u578b\u6a21\u578b\u7684\u63a8\u7406\u6548\u80fd\u3002\u9019\u4e9b\u767c\u73fe\u7a81\u986f\u4e86\u76f4\u63a5\u5f37\u6a21\u578b\u63d0\u7149\u7684\u9650\u5236\uff0c\u4e26\u5f37\u8abf\u4e86\u8abf\u6574\u63a8\u7406\u8907\u96dc\u6027\u4ee5\u6709\u6548\u8f49\u79fb\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "author": "Yuetai Li et.al.", "authors": "Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, Radha Poovendran", "id": "2502.12143v1", "paper_url": "http://arxiv.org/abs/2502.12143v1", "repo": "null"}}