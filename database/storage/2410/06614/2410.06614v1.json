{"2410.06614": {"publish_time": "2024-10-09", "title": "Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers", "paper_summary": "In this work we propose a novel joint training method for Visual Place\nRecognition (VPR), which simultaneously learns a global descriptor and a pair\nclassifier for re-ranking. The pair classifier can predict whether a given pair\nof images are from the same place or not. The network only comprises Vision\nTransformer components for both the encoder and the pair classifier, and both\ncomponents are trained using their respective class tokens. In existing VPR\nmethods, typically the network is initialized using pre-trained weights from a\ngeneric image dataset such as ImageNet. In this work we propose an alternative\npre-training strategy, by using Siamese Masked Image Modelling as a\npre-training task. We propose a Place-aware image sampling procedure from a\ncollection of large VPR datasets for pre-training our model, to learn visual\nfeatures tuned specifically for VPR. By re-using the Mask Image Modelling\nencoder and decoder weights in the second stage of training, Pair-VPR can\nachieve state-of-the-art VPR performance across five benchmark datasets with a\nViT-B encoder, along with further improvements in localization recall with\nlarger encoders. The Pair-VPR website is:\nhttps://csiro-robotics.github.io/Pair-VPR.", "paper_summary_zh": "\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u8996\u89ba\u5834\u666f\u8b58\u5225 (VPR) \u806f\u5408\u8a13\u7df4\u65b9\u6cd5\uff0c\u5b83\u540c\u6642\u5b78\u7fd2\u4e00\u500b\u5168\u5c40\u63cf\u8ff0\u7b26\u548c\u4e00\u500b\u7528\u65bc\u91cd\u65b0\u6392\u5e8f\u7684\u914d\u5c0d\u5206\u985e\u5668\u3002\u914d\u5c0d\u5206\u985e\u5668\u53ef\u4ee5\u9810\u6e2c\u7d66\u5b9a\u7684\u4e00\u5c0d\u5f71\u50cf\u662f\u5426\u4f86\u81ea\u540c\u4e00\u500b\u5834\u666f\u3002\u7db2\u8def\u53ea\u5305\u542b\u7528\u65bc\u7de8\u78bc\u5668\u548c\u914d\u5c0d\u5206\u985e\u5668\u7684\u8996\u89ba\u8f49\u63db\u5668\u7d44\u4ef6\uff0c\u4e26\u4e14\u5169\u500b\u7d44\u4ef6\u90fd\u4f7f\u7528\u5b83\u5011\u5404\u81ea\u7684\u985e\u5225\u4ee3\u78bc\u9032\u884c\u8a13\u7df4\u3002\u5728\u73fe\u6709\u7684 VPR \u65b9\u6cd5\u4e2d\uff0c\u7db2\u8def\u901a\u5e38\u4f7f\u7528\u4f86\u81ea\u901a\u7528\u5f71\u50cf\u8cc7\u6599\u96c6\uff08\u4f8b\u5982 ImageNet\uff09\u7684\u9810\u5148\u8a13\u7df4\u6b0a\u91cd\u9032\u884c\u521d\u59cb\u5316\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u66ff\u4ee3\u6027\u7684\u9810\u5148\u8a13\u7df4\u7b56\u7565\uff0c\u4f7f\u7528\u9023\u9ad4\u906e\u7f69\u5f71\u50cf\u5efa\u6a21\u4f5c\u70ba\u9810\u5148\u8a13\u7df4\u4efb\u52d9\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u4f86\u81ea\u5927\u578b VPR \u8cc7\u6599\u96c6\u96c6\u5408\u7684\u5834\u666f\u611f\u77e5\u5f71\u50cf\u53d6\u6a23\u7a0b\u5e8f\uff0c\u7528\u65bc\u9810\u5148\u8a13\u7df4\u6211\u5011\u7684\u6a21\u578b\uff0c\u4ee5\u5b78\u7fd2\u5c08\u9580\u91dd\u5c0d VPR \u8abf\u6574\u7684\u8996\u89ba\u7279\u5fb5\u3002\u901a\u904e\u5728\u8a13\u7df4\u7684\u7b2c\u4e8c\u968e\u6bb5\u91cd\u65b0\u4f7f\u7528\u906e\u7f69\u5f71\u50cf\u5efa\u6a21\u7de8\u78bc\u5668\u548c\u89e3\u78bc\u5668\u6b0a\u91cd\uff0cPair-VPR \u53ef\u4ee5\u4f7f\u7528 ViT-B \u7de8\u78bc\u5668\u5728\u4e94\u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u4e0a\u5be6\u73fe\u6700\u5148\u9032\u7684 VPR \u6548\u80fd\uff0c\u540c\u6642\u96a8\u8457\u7de8\u78bc\u5668\u8b8a\u5927\uff0c\u5b9a\u4f4d\u53ec\u56de\u7387\u9032\u4e00\u6b65\u63d0\u9ad8\u3002Pair-VPR \u7db2\u7ad9\u70ba\uff1a\nhttps://csiro-robotics.github.io/Pair-VPR\u3002", "author": "Stephen Hausler et.al.", "authors": "Stephen Hausler, Peyman Moghadam", "id": "2410.06614v1", "paper_url": "http://arxiv.org/abs/2410.06614v1", "repo": "null"}}