{"2407.02004": {"publish_time": "2024-07-02", "title": "SAVE: Segment Audio-Visual Easy way using Segment Anything Model", "paper_summary": "The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify\nand locate auditory elements within visual scenes by accurately predicting\nsegmentation masks at the pixel level. Achieving this involves comprehensively\nconsidering data and model aspects to address this task effectively. This study\npresents a lightweight approach, SAVE, which efficiently adapts the pre-trained\nsegment anything model (SAM) to the AVS task. By incorporating an image encoder\nadapter into the transformer blocks to better capture the distinct dataset\ninformation and proposing a residual audio encoder adapter to encode the audio\nfeatures as a sparse prompt, our proposed model achieves effective audio-visual\nfusion and interaction during the encoding stage. Our proposed method\naccelerates the training and inference speed by reducing the input resolution\nfrom 1024 to 256 pixels while achieving higher performance compared with the\nprevious SOTA. Extensive experimentation validates our approach, demonstrating\nthat our proposed model outperforms other SOTA methods significantly. Moreover,\nleveraging the pre-trained model on synthetic data enhances performance on real\nAVSBench data, achieving 84.59 mIoU on the S4 (V1S) subset and 70.28 mIoU on\nthe MS3 (V1M) set with only 256 pixels for input images. This increases up to\n86.16 mIoU on the S4 (V1S) and 70.83 mIoU on the MS3 (V1M) with inputs of 1024\npixels.", "paper_summary_zh": "\u8996\u89ba-\u807d\u89ba\u5206\u5272 (AVS) \u7684\u4e3b\u8981\u76ee\u6a19\u662f\u900f\u904e\u7cbe\u6e96\u9810\u6e2c\u50cf\u7d20\u5c64\u7d1a\u7684\u5206\u5272\u906e\u7f69\uff0c\u7cbe\u6e96\u8b58\u5225\u4e26\u5b9a\u4f4d\u8996\u89ba\u5834\u666f\u4e2d\u7684\u807d\u89ba\u5143\u7d20\u3002\u70ba\u4e86\u6709\u6548\u8655\u7406\u9019\u9805\u4efb\u52d9\uff0c\u9700\u8981\u5168\u9762\u8003\u91cf\u8cc7\u6599\u548c\u6a21\u578b\u9762\u5411\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u500b\u8f15\u91cf\u7d1a\u65b9\u6cd5 SAVE\uff0c\u5b83\u80fd\u6709\u6548\u5730\u5c07\u9810\u5148\u8a13\u7df4\u597d\u7684\u4efb\u4f55\u6a21\u578b\u5340\u6bb5 (SAM) \u8abf\u6574\u81f3 AVS \u4efb\u52d9\u3002\u900f\u904e\u5c07\u5f71\u50cf\u7de8\u78bc\u5668\u8f49\u63a5\u5668\u7d0d\u5165Transformer\u5340\u584a\uff0c\u4ee5\u66f4\u597d\u5730\u64f7\u53d6\u4e0d\u540c\u7684\u8cc7\u6599\u96c6\u8cc7\u8a0a\uff0c\u4e26\u63d0\u51fa\u4e00\u500b\u6b98\u5dee\u97f3\u8a0a\u7de8\u78bc\u5668\u8f49\u63a5\u5668\uff0c\u5c07\u97f3\u8a0a\u7279\u5fb5\u7de8\u78bc\u70ba\u4e00\u500b\u7a00\u758f\u63d0\u793a\uff0c\u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b\u5728\u7de8\u78bc\u968e\u6bb5\u5be6\u73fe\u4e86\u6709\u6548\u7684\u8996\u89ba-\u807d\u89ba\u878d\u5408\u548c\u4e92\u52d5\u3002\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u900f\u904e\u5c07\u8f38\u5165\u89e3\u6790\u5ea6\u5f9e 1024 \u964d\u81f3 256 \u50cf\u7d20\u4f86\u52a0\u901f\u8a13\u7df4\u548c\u63a8\u8ad6\u901f\u5ea6\uff0c\u540c\u6642\u8207\u5148\u524d\u7684 SOTA \u76f8\u6bd4\uff0c\u9054\u5230\u4e86\u66f4\u9ad8\u7684\u6548\u80fd\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u7684\u65b9\u6cd5\uff0c\u8b49\u660e\u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b\u986f\u8457\u512a\u65bc\u5176\u4ed6 SOTA \u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5728\u5408\u6210\u8cc7\u6599\u4e0a\u5229\u7528\u9810\u5148\u8a13\u7df4\u597d\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u63d0\u5347\u5728\u771f\u5be6 AVSBench \u8cc7\u6599\u4e0a\u7684\u6548\u80fd\uff0c\u5728 S4 (V1S) \u5b50\u96c6\u4e2d\u9054\u5230 84.59 mIoU\uff0c\u5728 MS3 (V1M) \u96c6\u5408\u4e2d\u9054\u5230 70.28 mIoU\uff0c\u8f38\u5165\u5f71\u50cf\u50c5\u6709 256 \u50cf\u7d20\u3002\u4f7f\u7528 1024 \u50cf\u7d20\u7684\u8f38\u5165\uff0c\u5728 S4 (V1S) \u4e0a\u589e\u52a0\u5230 86.16 mIoU\uff0c\u5728 MS3 (V1M) \u4e0a\u589e\u52a0\u5230 70.83 mIoU\u3002", "author": "Khanh-Binh Nguyen et.al.", "authors": "Khanh-Binh Nguyen, Chae Jung Park", "id": "2407.02004v1", "paper_url": "http://arxiv.org/abs/2407.02004v1", "repo": "null"}}