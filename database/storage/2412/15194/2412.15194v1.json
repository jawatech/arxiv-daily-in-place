{"2412.15194": {"publish_time": "2024-12-19", "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark", "paper_summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.", "paper_summary_zh": "<paragraph>\u591a\u91cd\u9078\u64c7\u984c (MCQ) \u8cc7\u6599\u96c6\uff0c\u4f8b\u5982 Massive Multitask Language Understanding (MMLU)\uff0c\u88ab\u5ee3\u6cdb\u7528\u65bc\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5e38\u8b58\u3001\u7406\u89e3\u548c\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u57fa\u6e96\u7684\u958b\u653e\u539f\u59cb\u78bc\u6027\u8cea\u548c LLM \u7684\u8a13\u7df4\u8cc7\u6599\u5ee3\u6cdb\u4f86\u6e90\u4e0d\u53ef\u907f\u514d\u5730\u5c0e\u81f4\u57fa\u6e96\u6c61\u67d3\uff0c\u5c0e\u81f4\u8a55\u4f30\u7d50\u679c\u4e0d\u53ef\u9760\u3002\u70ba\u4e86\u7de9\u89e3\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7121\u6c61\u67d3\u4e14\u66f4\u5177\u6311\u6230\u6027\u7684 MCQ \u57fa\u6e96\uff0c\u7a31\u70ba MMLU-CF\u3002\u6b64\u57fa\u6e96\u900f\u904e\u907f\u514d\u7121\u610f\u548c\u60e1\u610f\u7684\u8cc7\u6599\u5916\u6d29\uff0c\u91cd\u65b0\u8a55\u4f30 LLM \u5c0d\u4e16\u754c\u77e5\u8b58\u7684\u7406\u89e3\u3002\u70ba\u4e86\u907f\u514d\u7121\u610f\u7684\u8cc7\u6599\u5916\u6d29\uff0c\u6211\u5011\u5f9e\u66f4\u5ee3\u6cdb\u7684\u7db2\u57df\u4e2d\u53d6\u5f97\u8cc7\u6599\uff0c\u4e26\u8a2d\u8a08\u4e86\u4e09\u689d\u53bb\u6c59\u898f\u5247\u3002\u70ba\u4e86\u9632\u6b62\u60e1\u610f\u7684\u8cc7\u6599\u5916\u6d29\uff0c\u6211\u5011\u5c07\u57fa\u6e96\u5340\u5206\u70ba\u9a57\u8b49\u548c\u6e2c\u8a66\u96c6\uff0c\u5177\u6709\u76f8\u4f3c\u7684\u96e3\u5ea6\u548c\u4e3b\u984c\u5206\u4f48\u3002\u6e2c\u8a66\u96c6\u4fdd\u6301\u5c01\u9589\u539f\u59cb\u78bc\u4ee5\u78ba\u4fdd\u7d50\u679c\u53ef\u9760\uff0c\u800c\u9a57\u8b49\u96c6\u516c\u958b\u53ef\u7528\u4ee5\u4fc3\u9032\u900f\u660e\u5ea6\u548c\u5354\u52a9\u7368\u7acb\u9a57\u8b49\u3002\u6211\u5011\u5c0d\u4e3b\u6d41 LLM \u7684\u8a55\u4f30\u986f\u793a\uff0c\u5f37\u5927\u7684 GPT-4o \u5728\u6e2c\u8a66\u96c6\u4e0a\u50c5\u7372\u5f97 5 \u6b21\u5617\u8a66\u5f97\u5206 73.4% \u548c 0 \u6b21\u5617\u8a66\u5f97\u5206 71.9%\uff0c\u9019\u8868\u793a\u6211\u5011\u7684\u65b9\u6cd5\u5728\u5efa\u7acb\u66f4\u56b4\u8b39\u4e14\u7121\u6c61\u67d3\u7684\u8a55\u4f30\u6a19\u6e96\u4e0a\u662f\u6709\u6548\u7684\u3002GitHub \u5b58\u653e\u5eab\u53ef\u5728 https://github.com/microsoft/MMLU-CF \u53d6\u5f97\uff0c\u800c\u8cc7\u6599\u96c6\u8acb\u53c3\u95b1 https://huggingface.co/datasets/microsoft/MMLU-CF\u3002</paragraph>", "author": "Qihao Zhao et.al.", "authors": "Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei", "id": "2412.15194v1", "paper_url": "http://arxiv.org/abs/2412.15194v1", "repo": "null"}}