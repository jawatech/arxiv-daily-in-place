{"2411.19548": {"publish_time": "2024-11-29", "title": "ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration", "paper_summary": "Closed-loop simulation is crucial for end-to-end autonomous driving. Existing\nsensor simulation methods (e.g., NeRF and 3DGS) reconstruct driving scenes\nbased on conditions that closely mirror training data distributions. However,\nthese methods struggle with rendering novel trajectories, such as lane changes.\nRecent works have demonstrated that integrating world model knowledge\nalleviates these issues. Despite their efficiency, these approaches still\nencounter difficulties in the accurate representation of more complex\nmaneuvers, with multi-lane shifts being a notable example. Therefore, we\nintroduce ReconDreamer, which enhances driving scene reconstruction through\nincremental integration of world model knowledge. Specifically, DriveRestorer\nis proposed to mitigate artifacts via online restoration. This is complemented\nby a progressive data update strategy designed to ensure high-quality rendering\nfor more complex maneuvers. To the best of our knowledge, ReconDreamer is the\nfirst method to effectively render in large maneuvers. Experimental results\ndemonstrate that ReconDreamer outperforms Street Gaussians in the NTA-IoU,\nNTL-IoU, and FID, with relative improvements by 24.87%, 6.72%, and 29.97%.\nFurthermore, ReconDreamer surpasses DriveDreamer4D with PVG during large\nmaneuver rendering, as verified by a relative improvement of 195.87% in the\nNTA-IoU metric and a comprehensive user study.", "paper_summary_zh": "\u5c01\u9589\u8ff4\u8def\u6a21\u64ec\u5c0d\u65bc\u7aef\u5230\u7aef\u7684\u81ea\u52d5\u99d5\u99db\u81f3\u95dc\u91cd\u8981\u3002\u73fe\u6709\u7684\u611f\u6e2c\u5668\u6a21\u64ec\u65b9\u6cd5\uff08\u4f8b\u5982 NeRF \u548c 3DGS\uff09\u6839\u64da\u8207\u8a13\u7df4\u8cc7\u6599\u5206\u4f48\u9ad8\u5ea6\u76f8\u4f3c\u7684\u689d\u4ef6\u91cd\u5efa\u99d5\u99db\u5834\u666f\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u96e3\u4ee5\u5448\u73fe\u65b0\u7684\u8ecc\u8de1\uff0c\u4f8b\u5982\u8b8a\u63db\u8eca\u9053\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u6574\u5408\u4e16\u754c\u6a21\u578b\u77e5\u8b58\u53ef\u4ee5\u7de9\u89e3\u9019\u4e9b\u554f\u984c\u3002\u5118\u7ba1\u9019\u4e9b\u65b9\u6cd5\u5f88\u6709\u6548\u7387\uff0c\u4f46\u5728\u6e96\u78ba\u8868\u793a\u66f4\u8907\u96dc\u7684\u6a5f\u52d5\u65b9\u9762\u4ecd\u7136\u9047\u5230\u56f0\u96e3\uff0c\u5176\u4e2d\u591a\u8eca\u9053\u8b8a\u63db\u662f\u4e00\u500b\u986f\u8457\u7684\u4f8b\u5b50\u3002\u56e0\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 ReconDreamer\uff0c\u5b83\u900f\u904e\u9010\u6b65\u6574\u5408\u4e16\u754c\u6a21\u578b\u77e5\u8b58\u4f86\u589e\u5f37\u99d5\u99db\u5834\u666f\u91cd\u5efa\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u63d0\u51fa\u4e86 DriveRestorer \u4ee5\u900f\u904e\u7dda\u4e0a\u4fee\u5fa9\u4f86\u6e1b\u8f15\u4eba\u5de5\u88fd\u54c1\u3002\u9019\u7531\u9032\u6b65\u7684\u8cc7\u6599\u66f4\u65b0\u7b56\u7565\u6240\u88dc\u5145\uff0c\u65e8\u5728\u78ba\u4fdd\u66f4\u8907\u96dc\u6a5f\u52d5\u7684\u9ad8\u54c1\u8cea\u6e32\u67d3\u3002\u64da\u6211\u5011\u6240\u77e5\uff0cReconDreamer \u662f\u7b2c\u4e00\u500b\u6709\u6548\u5448\u73fe\u5927\u578b\u6a5f\u52d5\u7684\u65b9\u6cd5\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cReconDreamer \u5728 NTA-IoU\u3001NTL-IoU \u548c FID \u4e2d\u512a\u65bc Street Gaussians\uff0c\u76f8\u5c0d\u6539\u9032\u4e86 24.87%\u30016.72% \u548c 29.97%\u3002\u6b64\u5916\uff0cReconDreamer \u5728\u5927\u578b\u6a5f\u52d5\u6e32\u67d3\u671f\u9593\u4f7f\u7528 PVG \u8d85\u8d8a\u4e86 DriveDreamer4D\uff0c\u9019\u7531 NTA-IoU \u6307\u6a19\u4e2d 195.87% \u7684\u76f8\u5c0d\u6539\u9032\u548c\u5168\u9762\u7684\u4f7f\u7528\u8005\u7814\u7a76\u6240\u9a57\u8b49\u3002", "author": "Chaojun Ni et.al.", "authors": "Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, Yifei Zhan, Kun Zhan, Peng Jia, Xianpeng Lang, Xingang Wang, Wenjun Mei", "id": "2411.19548v1", "paper_url": "http://arxiv.org/abs/2411.19548v1", "repo": "null"}}