{"2412.12583": {"publish_time": "2024-12-17", "title": "Process-Supervised Reward Models for Clinical Note Generation: A Scalable Approach Guided by Domain Expertise", "paper_summary": "Process-supervised reward models (PRMs), which verify large language model\n(LLM) outputs step-by-step, have achieved significant success in mathematical\nand coding problems. However, their application to other domains remains\nlargely unexplored. In this work, we train a PRM to provide step-level reward\nsignals for clinical notes generated by LLMs from patient-doctor dialogues.\nGuided by real-world clinician expertise, we carefully designed step\ndefinitions for clinical notes and utilized Gemini-Pro 1.5 to automatically\ngenerate process supervision data at scale. Our proposed PRM, trained on the\nLLaMA-3.1 8B instruct model, demonstrated superior performance compared to\nGemini-Pro 1.5 and an outcome-supervised reward model (ORM) across two key\nevaluations: (1) the accuracy of selecting gold-reference samples from\nerror-containing samples, achieving 98.8% (versus 61.3% for ORM and 93.8% for\nGemini-Pro 1.5), and (2) the accuracy of selecting physician-preferred notes,\nachieving 56.2% (compared to 51.2% for ORM and 50.0% for Gemini-Pro 1.5).\nAdditionally, we conducted ablation studies to determine optimal loss functions\nand data selection strategies, along with physician reader studies to explore\npredictors of downstream Best-of-N performance. Our promising results suggest\nthe potential of PRMs to extend beyond the clinical domain, offering a scalable\nand effective solution for diverse generative tasks.", "paper_summary_zh": "<paragraph>\u904e\u7a0b\u76e3\u7763\u734e\u52f5\u6a21\u578b (PRM) \u6703\u9010\u6b65\u9a57\u8b49\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8f38\u51fa\uff0c\u5df2\u5728\u6578\u5b78\u548c\u7de8\u78bc\u554f\u984c\u4e2d\u53d6\u5f97\u91cd\u5927\u6210\u529f\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u5176\u4ed6\u9818\u57df\u7684\u61c9\u7528\u4ecd\u672a\u5ee3\u6cdb\u63a2\u8a0e\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u8a13\u7df4\u4e86\u4e00\u500b PRM\uff0c\u4ee5\u63d0\u4f9b LLM \u5f9e\u60a3\u8005\u8207\u91ab\u5e2b\u5c0d\u8a71\u4e2d\u7522\u751f\u7684\u81e8\u5e8a\u7b46\u8a18\u7684\u6b65\u9a5f\u5c64\u7d1a\u734e\u52f5\u8a0a\u865f\u3002\u5728\u771f\u5be6\u4e16\u754c\u7684\u81e8\u5e8a\u91ab\u5e2b\u5c08\u696d\u77e5\u8b58\u6307\u5c0e\u4e0b\uff0c\u6211\u5011\u4ed4\u7d30\u8a2d\u8a08\u4e86\u81e8\u5e8a\u7b46\u8a18\u7684\u6b65\u9a5f\u5b9a\u7fa9\uff0c\u4e26\u5229\u7528 Gemini-Pro 1.5 \u81ea\u52d5\u5927\u898f\u6a21\u7522\u751f\u904e\u7a0b\u76e3\u7763\u8cc7\u6599\u3002\u5728 LLaMA-3.1 8B \u6307\u4ee4\u6a21\u578b\u4e0a\u8a13\u7df4\u7684\u6211\u5011\u63d0\u51fa\u7684 PRM \u5728\u5169\u9805\u95dc\u9375\u8a55\u4f30\u4e2d\u8868\u73fe\u51fa\u512a\u65bc Gemini-Pro 1.5 \u548c\u7d50\u679c\u76e3\u7763\u734e\u52f5\u6a21\u578b (ORM) \u7684\u6548\u80fd\uff1a(1) \u5f9e\u542b\u932f\u8aa4\u7684\u7bc4\u4f8b\u4e2d\u9078\u53d6\u9ec3\u91d1\u53c3\u8003\u7bc4\u4f8b\u7684\u6e96\u78ba\u5ea6\uff0c\u9054\u5230 98.8%\uff08\u76f8\u8f03\u65bc ORM \u7684 61.3% \u548c Gemini-Pro 1.5 \u7684 93.8%\uff09\uff0c\u4ee5\u53ca (2) \u9078\u53d6\u91ab\u5e2b\u504f\u597d\u7684\u7b46\u8a18\u7684\u6e96\u78ba\u5ea6\uff0c\u9054\u5230 56.2%\uff08\u76f8\u8f03\u65bc ORM \u7684 51.2% \u548c Gemini-Pro 1.5 \u7684 50.0%\uff09\u3002\u6b64\u5916\uff0c\u6211\u5011\u9032\u884c\u4e86\u6d88\u878d\u7814\u7a76\u4ee5\u78ba\u5b9a\u6700\u4f73\u640d\u5931\u51fd\u6578\u548c\u8cc7\u6599\u9078\u53d6\u7b56\u7565\uff0c\u4e26\u8207\u91ab\u5e2b\u8b80\u8005\u7814\u7a76\u4e00\u8d77\u63a2\u8a0e\u4e0b\u6e38\u6700\u4f73 N \u6548\u80fd\u7684\u9810\u6e2c\u56e0\u5b50\u3002\u6211\u5011\u6709\u524d\u666f\u7684\u7d50\u679c\u8868\u660e PRM \u6709\u53ef\u80fd\u64f4\u5c55\u5230\u81e8\u5e8a\u9818\u57df\u4ee5\u5916\uff0c\u70ba\u5404\u7a2e\u751f\u6210\u4efb\u52d9\u63d0\u4f9b\u53ef\u64f4\u5145\u4e14\u6709\u6548\u7684\u89e3\u6c7a\u65b9\u6848\u3002</paragraph>", "author": "Hanyin Wang et.al.", "authors": "Hanyin Wang, Qiping Xu, Bolun Liu, Guleid Hussein, Hariprasad Korsapati, Mohamad El Labban, Kingsley Iheasirim, Mohamed Hassan, Gokhan Anil, Brian Bartlett, Jimeng Sun", "id": "2412.12583v1", "paper_url": "http://arxiv.org/abs/2412.12583v1", "repo": "https://github.com/hanyin88/prm-clinic"}}