{"2406.09760": {"publish_time": "2024-06-14", "title": "Bootstrapping Language Models with DPO Implicit Rewards", "paper_summary": "Human alignment in large language models (LLMs) is an active area of\nresearch. A recent groundbreaking work, direct preference optimization (DPO),\nhas greatly simplified the process from past work in reinforcement learning\nfrom human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO,\nafter training, provides an implicit reward model. In this work, we make a\nnovel observation that this implicit reward model can by itself be used in a\nbootstrapping fashion to further align the LLM. Our approach is to use the\nrewards from a current LLM model to construct a preference dataset, which is\nthen used in subsequent DPO rounds. We incorporate refinements that debias the\nlength of the responses and improve the quality of the preference dataset to\nfurther improve our approach. Our approach, named self-alignment with DPO\nImpliCit rEwards (DICE), shows great improvements in alignment and achieves\nsuperior performance than Gemini Pro on AlpacaEval 2, reaching 27.55%\nlength-controlled win rate against GPT-4 Turbo, but with only 8B parameters and\nno external feedback. Our code is available at https://github.com/sail-sg/dice.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u4eba\u985e\u5c0d\u9f4a\u662f\u4e00\u500b\u6d3b\u8e8d\u7684\u7814\u7a76\u9818\u57df\u3002\u6700\u8fd1\u4e00\u9805\u7a81\u7834\u6027\u7684\u5de5\u4f5c\uff0c\u76f4\u63a5\u504f\u597d\u512a\u5316 (DPO)\uff0c\u901a\u904e\u7e5e\u904e RLHF \u4e2d\u7684\u734e\u52f5\u5b78\u7fd2\u968e\u6bb5\uff0c\u6975\u5927\u5730\u7c21\u5316\u4e86\u4eba\u985e\u56de\u994b (RLHF) \u4e2d\u5f37\u5316\u5b78\u7fd2\u904e\u53bb\u5de5\u4f5c\u7684\u6d41\u7a0b\u3002DPO \u5728\u8a13\u7df4\u5f8c\u63d0\u4f9b\u4e86\u96b1\u542b\u7684\u734e\u52f5\u6a21\u578b\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u505a\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u89c0\u5bdf\uff0c\u5373\u9019\u500b\u96b1\u542b\u7684\u734e\u52f5\u6a21\u578b\u672c\u8eab\u53ef\u4ee5\u7528\u65bc\u81ea\u8209\u65b9\u5f0f\u9032\u4e00\u6b65\u5c0d\u9f4a LLM\u3002\u6211\u5011\u7684\u505a\u6cd5\u662f\u4f7f\u7528\u7576\u524d LLM \u6a21\u578b\u4e2d\u7684\u734e\u52f5\u4f86\u69cb\u5efa\u504f\u597d\u6578\u64da\u96c6\uff0c\u7136\u5f8c\u5728\u5f8c\u7e8c\u7684 DPO \u8f2a\u6b21\u4e2d\u4f7f\u7528\u8a72\u6578\u64da\u96c6\u3002\u6211\u5011\u7d0d\u5165\u4e86\u6539\u9032\uff0c\u4ee5\u6d88\u9664\u5c0d\u61c9\u7b54\u9577\u5ea6\u7684\u504f\u898b\u4e26\u63d0\u9ad8\u504f\u597d\u6578\u64da\u96c6\u7684\u8cea\u91cf\uff0c\u4ee5\u9032\u4e00\u6b65\u6539\u9032\u6211\u5011\u7684\u505a\u6cd5\u3002\u6211\u5011\u7684\u505a\u6cd5\uff0c\u7a31\u70ba\u4f7f\u7528 DPO \u96b1\u542b\u734e\u52f5\u7684\u81ea\u5c0d\u9f4a (DICE)\uff0c\u5728\u5c0d\u9f4a\u65b9\u9762\u986f\u793a\u51fa\u5de8\u5927\u7684\u6539\u9032\uff0c\u4e26\u4e14\u5728 AlpacaEval 2 \u4e0a\u9054\u5230\u4e86\u6bd4 Gemini Pro \u66f4\u512a\u8d8a\u7684\u6027\u80fd\uff0c\u5c0d\u6297 GPT-4 Turbo \u6642\u9054\u5230\u4e86 27.55% \u7684\u9577\u5ea6\u63a7\u5236\u7372\u52dd\u7387\uff0c\u4f46\u53ea\u6709 8B \u53c3\u6578\u548c\u6c92\u6709\u5916\u90e8\u56de\u994b\u3002\u6211\u5011\u7684\u4ee3\u78bc\u53ef\u5728 https://github.com/sail-sg/dice \u7372\u5f97\u3002", "author": "Changyu Chen et.al.", "authors": "Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, Min Lin", "id": "2406.09760v1", "paper_url": "http://arxiv.org/abs/2406.09760v1", "repo": "https://github.com/sail-sg/dice"}}