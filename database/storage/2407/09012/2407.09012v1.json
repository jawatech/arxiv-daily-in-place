{"2407.09012": {"publish_time": "2024-07-12", "title": "TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models", "paper_summary": "Pose-driven human-image animation diffusion models have shown remarkable\ncapabilities in realistic human video synthesis. Despite the promising results\nachieved by previous approaches, challenges persist in achieving temporally\nconsistent animation and ensuring robustness with off-the-shelf pose detectors.\nIn this paper, we present TCAN, a pose-driven human image animation method that\nis robust to erroneous poses and consistent over time. In contrast to previous\nmethods, we utilize the pre-trained ControlNet without fine-tuning to leverage\nits extensive pre-acquired knowledge from numerous pose-image-caption pairs. To\nkeep the ControlNet frozen, we adapt LoRA to the UNet layers, enabling the\nnetwork to align the latent space between the pose and appearance features.\nAdditionally, by introducing an additional temporal layer to the ControlNet, we\nenhance robustness against outliers of the pose detector. Through the analysis\nof attention maps over the temporal axis, we also designed a novel temperature\nmap leveraging pose information, allowing for a more static background.\nExtensive experiments demonstrate that the proposed method can achieve\npromising results in video synthesis tasks encompassing various poses, like\nchibi. Project Page: https://eccv2024tcan.github.io/", "paper_summary_zh": "<paragraph>\u4ee5\u59ff\u52e2\u70ba\u4e3b\u7684\u5716\u50cf\u52d5\u756b\u64f4\u6563\u6a21\u578b\u5728\u903c\u771f\u7684\u5f71\u7247\u5408\u6210\u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u5118\u7ba1\u5148\u524d\u7684\u6280\u8853\u5df2\u7372\u5f97\u53ef\u89c0\u7684\u6210\u679c\uff0c\u4f46\u4ecd\u9762\u81e8\u8457\u5728\u6642\u9593\u4e0a\u4fdd\u6301\u52d5\u756b\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u78ba\u4fdd\u8207\u73fe\u6210\u7684\u59ff\u52e2\u6aa2\u6e2c\u5668\u76f8\u5bb9\u7684\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 TCAN\uff0c\u9019\u662f\u4e00\u7a2e\u5c0d\u932f\u8aa4\u59ff\u52e2\u5177\u6709\u9b6f\u68d2\u6027\u4e14\u96a8\u8457\u6642\u9593\u4fdd\u6301\u4e00\u81f4\u7684\u59ff\u52e2\u9a45\u52d5\u4eba\u50cf\u52d5\u756b\u65b9\u6cd5\u3002\u8207\u5148\u524d\u7684\u6280\u8853\u4e0d\u540c\uff0c\u6211\u5011\u5229\u7528\u9810\u5148\u8a13\u7df4\u7684 ControlNet\uff0c\u800c\u7121\u9700\u5fae\u8abf\uff0c\u4ee5\u5229\u7528\u5176\u5f9e\u5927\u91cf\u59ff\u52e2-\u5716\u50cf-\u6a19\u984c\u5c0d\u4e2d\u7372\u5f97\u7684\u8c50\u5bcc\u9810\u5148\u7372\u53d6\u7684\u77e5\u8b58\u3002\u70ba\u4e86\u4f7f ControlNet \u4fdd\u6301\u51cd\u7d50\uff0c\u6211\u5011\u5c07 LoRA \u8abf\u6574\u5230 UNet \u5c64\uff0c\u4f7f\u7db2\u8def\u80fd\u5920\u5728\u59ff\u52e2\u548c\u5916\u89c0\u7279\u5fb5\u4e4b\u9593\u5c0d\u9f4a\u6f5b\u5728\u7a7a\u9593\u3002\u6b64\u5916\uff0c\u900f\u904e\u5728 ControlNet \u4e2d\u5f15\u5165\u4e00\u500b\u984d\u5916\u7684\u6642\u9593\u5c64\uff0c\u6211\u5011\u589e\u5f37\u4e86\u5c0d\u59ff\u52e2\u6aa2\u6e2c\u5668\u7570\u5e38\u503c\u7684\u9b6f\u68d2\u6027\u3002\u900f\u904e\u5c0d\u6642\u9593\u8ef8\u4e0a\u7684\u6ce8\u610f\u529b\u5716\u9032\u884c\u5206\u6790\uff0c\u6211\u5011\u9084\u8a2d\u8a08\u4e86\u4e00\u7a2e\u5229\u7528\u59ff\u52e2\u8cc7\u8a0a\u7684\u65b0\u578b\u6eab\u5ea6\u5716\uff0c\u5141\u8a31\u66f4\u975c\u614b\u7684\u80cc\u666f\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u6db5\u84cb\u5404\u7a2e\u59ff\u52e2\uff08\u5982 chibi\uff09\u7684\u5f71\u7247\u5408\u6210\u4efb\u52d9\u4e2d\u7372\u5f97\u6709\u5e0c\u671b\u7684\u7d50\u679c\u3002\u5c08\u6848\u9801\u9762\uff1ahttps://eccv2024tcan.github.io/</paragraph>", "author": "Jeongho Kim et.al.", "authors": "Jeongho Kim, Min-Jung Kim, Junsoo Lee, Jaegul Choo", "id": "2407.09012v1", "paper_url": "http://arxiv.org/abs/2407.09012v1", "repo": "null"}}