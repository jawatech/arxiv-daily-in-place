{"2408.10774": {"publish_time": "2024-08-20", "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models", "paper_summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u900f\u904e\u589e\u52a0\u6a21\u578b\u53c3\u6578\u7684\u898f\u6a21\uff0c\u63a8\u52d5\u4eba\u5de5\u667a\u6167\u7684\u9032\u5c55\uff0c\u9019\u986f\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u4e26\u5728\u5be6\u52d9\u4e0a\u89e3\u9396\u65b0\u7684\u529f\u80fd\u3002\u7136\u800c\uff0c\u5b83\u5011\u5728\u7279\u5b9a\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u901a\u5e38\u6703\u53d7\u5230\u9019\u4e9b\u4efb\u52d9\u7684\u77e5\u8b58\u754c\u7dda\u6240\u963b\u7919\u3002\u56e0\u6b64\uff0c\u5fae\u8abf\u6280\u8853\uff0c\u7279\u5225\u662f\u5ee3\u6cdb\u4f7f\u7528\u7684\u4f4e\u79e9\u9069\u61c9\uff08LoRA\uff09\u65b9\u6cd5\uff0c\u5df2\u88ab\u5f15\u5165\u4ee5\u64f4\u5c55\u9019\u4e9b\u4efb\u52d9\u7684\u754c\u7dda\uff0c\u800c LoRA \u537b\u6703\u56e0\u70ba\u5176\u5728\u9019\u4e9b\u4efb\u52d9\u4e0a\u7684\u6f5b\u5728\u904e\u5ea6\u64ec\u5408\u800c\u8868\u73fe\u4e0d\u4f73\u3002\u70ba\u4e86\u514b\u670d\u9019\u7a2e\u904e\u5ea6\u64ec\u5408\u4e26\u6539\u5584 LoRA \u7684\u6548\u80fd\uff0c\u6211\u5011\u63d0\u51fa\u5f48\u6027\u4f4e\u79e9\u9069\u61c9\uff08Flexora\uff09\u65b9\u6cd5\uff0c\u4ee5\u81ea\u52d5\u4e14\u5f48\u6027\u5730\u9078\u51fa\u6700\u91cd\u8981\u7684\u5c64\uff0c\u9700\u8981\u5fae\u8abf\u624d\u80fd\u5728\u4e0d\u540c\u7684\u4e0b\u6e38\u4efb\u52d9\u4e0a\u53d6\u5f97\u6700\u4f73\u6548\u80fd\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cFlexora \u9996\u5148\u5c07\u6b64\u5c64\u9078\u64c7\u554f\u984c\u8a2d\u5b9a\u70ba\u5b9a\u7fa9\u826f\u597d\u7684\u8d85\u53c3\u6578\u6700\u4f73\u5316\uff08HPO\uff09\u554f\u984c\uff0c\u7136\u5f8c\u4f7f\u7528\u5c55\u958b\u5fae\u5206\uff08UD\uff09\u65b9\u6cd5\u4f86\u89e3\u6c7a\u5b83\uff0c\u6700\u5f8c\u6839\u64da\u6700\u4f73\u5316\u7684\u8d85\u53c3\u6578\u9078\u51fa\u6700\u6709\u7528\u7684\u5c64\u3002\u6211\u5011\u5728\u8a31\u591a\u9810\u8a13\u7df4\u6a21\u578b\u548c\u81ea\u7136\u8a9e\u8a00\u4efb\u52d9\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u986f\u793a\uff0cFlexora \u80fd\u5920\u6301\u7e8c\u6539\u9032\u73fe\u6709\u7684\u57fa\u7dda\uff0c\u9019\u8868\u793a\u6211\u5011\u7684 Flexora \u5728\u5be6\u52d9\u4e0a\u662f\u6709\u6548\u7684\u3002\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u6709\u898b\u5730\u7684\u7406\u8ad6\u7d50\u679c\u548c\u8a31\u591a\u6d88\u878d\u7814\u7a76\uff0c\u4ee5\u63d0\u4f9b\u5c0d\u6211\u5011\u7684 Flexora \u7684\u5168\u9762\u7406\u89e3\u3002", "author": "Chenxing Wei et.al.", "authors": "Chenxing Wei, Yao Shu, Ying Tiffany He, Fei Richard Yu", "id": "2408.10774v2", "paper_url": "http://arxiv.org/abs/2408.10774v2", "repo": "null"}}