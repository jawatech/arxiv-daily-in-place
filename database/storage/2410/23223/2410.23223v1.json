{"2410.23223": {"publish_time": "2024-10-30", "title": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences", "paper_summary": "Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods.", "paper_summary_zh": "\u8a31\u591a\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u5305\u542b\u4eba\u985e\u56de\u994b\u7684\u5f37\u5316\u5b78\u7fd2 (RLHF)\uff0c\u4f9d\u8cf4\u65bc\u5e03\u840a\u5fb7\u5229-\u6cf0\u745e\u734e\u52f5\u5047\u8a2d\uff0c\u9019\u4e0d\u8db3\u4ee5\u6db5\u84cb\u4e00\u822c\u4eba\u985e\u504f\u597d\u7684\u5b8c\u6574\u7bc4\u570d\u3002\u70ba\u4e86\u9054\u6210\u8207\u4e00\u822c\u504f\u597d\u7684\u7a69\u5065\u5c0d\u9f4a\uff0c\u6211\u5011\u5c07\u5c0d\u9f4a\u554f\u984c\u5efa\u6a21\u70ba\u96d9\u4eba\u96f6\u548c\u904a\u6232\uff0c\u5176\u4e2d\u7d0d\u8a31\u5747\u8861\u7b56\u7565\u4fdd\u8b49\u5c0d\u6297\u4efb\u4f55\u7af6\u722d\u7b56\u7565\u6642\u6709 50% \u7684\u7372\u52dd\u7387\u3002\u7136\u800c\uff0c\u5148\u524d\u7528\u65bc\u5c0b\u627e\u7d0d\u8a31\u7b56\u7565\u7684\u6f14\u7b97\u6cd5\u6703\u767c\u6563\u6216\u6536\u6582\u81f3\u4fee\u6539\u5f8c\u904a\u6232\u4e2d\u7684\u7d0d\u8a31\u7b56\u7565\uff0c\u5373\u4f7f\u5728\u7c21\u55ae\u7684\u5408\u6210\u8a2d\u5b9a\u4e2d\u4e5f\u662f\u5982\u6b64\uff0c\u56e0\u6b64\u7121\u6cd5\u7dad\u6301\u5c0d\u6297\u6240\u6709\u5176\u4ed6\u7b56\u7565\u7684 50% \u7372\u52dd\u7387\u4fdd\u8b49\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5143\u6f14\u7b97\u6cd5\uff0c\u6536\u6582\u5143\u5c0d\u9f4a\u6f14\u7b97\u6cd5 (COMAL)\uff0c\u7528\u65bc\u8a9e\u8a00\u6a21\u578b\u5c0d\u9f4a\u8207\u4e00\u822c\u504f\u597d\uff0c\u9748\u611f\u4f86\u81ea\u65bc\u535a\u5f08\u8ad6\u4e2d\u7684\u6536\u6582\u6f14\u7b97\u6cd5\u3002\u7406\u8ad6\u4e0a\uff0c\u6211\u5011\u8b49\u660e\u6211\u5011\u7684\u5143\u6f14\u7b97\u6cd5\u5728\u6700\u5f8c\u4e00\u6b21\u8fed\u4ee3\u4e2d\u6536\u6582\u81f3\u7cbe\u78ba\u7684\u7d0d\u8a31\u7b56\u7565\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u5143\u6f14\u7b97\u6cd5\u5f88\u7c21\u55ae\uff0c\u53ef\u4ee5\u8207\u8a31\u591a\u73fe\u6709\u7684\u65b9\u6cd5\u6574\u5408\uff0c\u9019\u4e9b\u65b9\u6cd5\u662f\u70ba RLHF \u548c\u504f\u597d\u6700\u4f73\u5316\u6240\u8a2d\u8a08\u7684\uff0c\u800c\u4e14\u8b8a\u66f4\u5f88\u5c0f\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u5728\u8207\u73fe\u6709\u7684\u504f\u597d\u7b56\u7565\u6700\u4f73\u5316\u65b9\u6cd5\u7d50\u5408\u6642\u5f88\u6709\u6548\u3002", "author": "Yixin Liu et.al.", "authors": "Yixin Liu, Argyris Oikonomou, Weiqiang Zheng, Yang Cai, Arman Cohan", "id": "2410.23223v1", "paper_url": "http://arxiv.org/abs/2410.23223v1", "repo": "https://github.com/yale-nlp/comal"}}