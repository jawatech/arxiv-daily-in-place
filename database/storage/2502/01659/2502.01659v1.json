{"2502.01659": {"publish_time": "2025-01-31", "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques", "paper_summary": "Transformers have demonstrated great success in numerous domains including\nnatural language processing and bioinformatics. This success stems from the use\nof the attention mechanism by these models in order to represent and propagate\npairwise interactions between individual tokens of sequential data. However,\nthe primary limitation of this operation is its quadratic memory and time\ncomplexity in relation to the input's context length - the length of a sequence\nover which the interactions need to be captured. This significantly limits the\nlength of sequences that can be inferred upon by these models. Extensive\nresearch has been conducted to reduce the number of pairwise interactions to\nsub-quadratic in relation to the context length by introducing sparsity into\nthe attention mechanism through the development of sparse attention masks.\nHowever, efficient implementations that achieve \"true sparsity\" are lacking.\n  In this work, we address this issue by proposing a graph computing view of\nattention where tokens are perceived as nodes of the graph and the attention\nmask determines the edges of the graph. Using this view, we develop graph\nprocessing algorithms to implement the attention mechanism. Both theoretically\nand empirically, we demonstrate that our algorithms only perform the needed\ncomputations, i.e., they are work optimal. We also perform extensive\nexperimentation using popular attention masks to explore the impact of sparsity\non execution time and achievable context length. Our experiments demonstrate\nsignificant speedups in execution times compared to state-of-the-art attention\nimplementations such as FlashAttention for large sequence lengths. We also\ndemonstrate that our algorithms are able to achieve extremely long sequence\nlengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).", "paper_summary_zh": "Transformer\u5df2\u5728\u773e\u591a\u9818\u57df\u5c55\u73fe\u5176\u5353\u8d8a\u7684\u6210\u529f\uff0c\u5176\u4e2d\u5305\u62ec\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u548c\u751f\u7269\u8cc7\u8a0a\u5b78\u3002\u6b64\u9805\u6210\u529f\u6e90\u81ea\u9019\u4e9b\u6a21\u578b\u4f7f\u7528\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4ee5\u8868\u793a\u548c\u50b3\u64ad\u5e8f\u5217\u8cc7\u6599\u4e2d\u500b\u5225\u6a19\u8a18\u4e4b\u9593\u7684\u6210\u5c0d\u4e92\u52d5\u3002\u7136\u800c\uff0c\u6b64\u9805\u64cd\u4f5c\u7684\u4e3b\u8981\u9650\u5236\u5728\u65bc\u5176\u4e8c\u6b21\u8a18\u61b6\u9ad4\u548c\u6642\u9593\u8907\u96dc\u5ea6\uff0c\u8207\u8f38\u5165\u7684\u5167\u5bb9\u9577\u5ea6\u6709\u95dc\uff0c\u4e5f\u5c31\u662f\u9700\u8981\u64f7\u53d6\u4e92\u52d5\u7684\u5e8f\u5217\u9577\u5ea6\u3002\u9019\u5927\u5e45\u9650\u5236\u4e86\u9019\u4e9b\u6a21\u578b\u53ef\u4ee5\u63a8\u8ad6\u7684\u5e8f\u5217\u9577\u5ea6\u3002\u5df2\u9032\u884c\u5ee3\u6cdb\u7684\u7814\u7a76\uff0c\u4ee5\u5c07\u6210\u5c0d\u4e92\u52d5\u7684\u6578\u91cf\u6e1b\u5c11\u5230\u4e8c\u6b21\u4ee5\u4e0b\uff0c\u8207\u5167\u5bb9\u9577\u5ea6\u6709\u95dc\uff0c\u65b9\u6cd5\u662f\u900f\u904e\u7a00\u758f\u6ce8\u610f\u529b\u906e\u7f69\u5728\u6ce8\u610f\u529b\u6a5f\u5236\u4e2d\u5f15\u5165\u7a00\u758f\u6027\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u80fd\u9054\u6210\u300c\u771f\u5be6\u7a00\u758f\u6027\u300d\u7684\u9ad8\u6548\u7387\u5be6\u4f5c\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e\u63d0\u51fa\u6ce8\u610f\u529b\u7684\u5716\u5f62\u904b\u7b97\u89c0\u9ede\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u5176\u4e2d\u6a19\u8a18\u88ab\u8996\u70ba\u5716\u5f62\u7684\u7bc0\u9ede\uff0c\u800c\u6ce8\u610f\u529b\u906e\u7f69\u5247\u6c7a\u5b9a\u5716\u5f62\u7684\u908a\u7de3\u3002\u4f7f\u7528\u6b64\u89c0\u9ede\uff0c\u6211\u5011\u958b\u767c\u5716\u5f62\u8655\u7406\u6f14\u7b97\u6cd5\u4f86\u5be6\u4f5c\u6ce8\u610f\u529b\u6a5f\u5236\u3002\u5728\u7406\u8ad6\u4e0a\u548c\u7d93\u9a57\u4e0a\uff0c\u6211\u5011\u8b49\u660e\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u53ea\u57f7\u884c\u6240\u9700\u7684\u904b\u7b97\uff0c\u4e5f\u5c31\u662f\u8aaa\uff0c\u5b83\u5011\u662f\u5de5\u4f5c\u6700\u4f73\u5316\u7684\u3002\u6211\u5011\u4e5f\u4f7f\u7528\u71b1\u9580\u6ce8\u610f\u529b\u906e\u7f69\u9032\u884c\u5ee3\u6cdb\u5be6\u9a57\uff0c\u4ee5\u63a2\u8a0e\u7a00\u758f\u6027\u5c0d\u57f7\u884c\u6642\u9593\u548c\u53ef\u9054\u6210\u7684\u5167\u5bb9\u9577\u5ea6\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u6700\u65b0\u6ce8\u610f\u529b\u5be6\u4f5c\uff08\u4f8b\u5982 FlashAttention\uff09\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u5728\u57f7\u884c\u6642\u9593\u4e0a\u5927\u5e45\u52a0\u5feb\uff0c\u7279\u5225\u662f\u5728\u5e8f\u5217\u9577\u5ea6\u8f03\u5927\u7684\u60c5\u6cc1\u4e0b\u3002\u6211\u5011\u4e5f\u8b49\u660e\uff0c\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u80fd\u5920\u5728\u55ae\u4e00 NVIDIA A100 GPU\uff08SXM4 80GB\uff09\u4e0a\u9054\u6210\u6975\u9577\u7684\u5e8f\u5217\u9577\u5ea6\uff0c\u6700\u9ad8\u9054 1.6 \u5104\u3002", "author": "Nathaniel Tomczak et.al.", "authors": "Nathaniel Tomczak, Sanmukh Kuppannagari", "id": "2502.01659v1", "paper_url": "http://arxiv.org/abs/2502.01659v1", "repo": "null"}}