{"2408.12426": {"publish_time": "2024-08-22", "title": "Enhanced Infield Agriculture with Interpretable Machine Learning Approaches for Crop Classification", "paper_summary": "The increasing popularity of Artificial Intelligence in recent years has led\nto a surge in interest in image classification, especially in the agricultural\nsector. With the help of Computer Vision, Machine Learning, and Deep Learning,\nthe sector has undergone a significant transformation, leading to the\ndevelopment of new techniques for crop classification in the field. Despite the\nextensive research on various image classification techniques, most have\nlimitations such as low accuracy, limited use of data, and a lack of reporting\nmodel size and prediction. The most significant limitation of all is the need\nfor model explainability. This research evaluates four different approaches for\ncrop classification, namely traditional ML with handcrafted feature extraction\nmethods like SIFT, ORB, and Color Histogram; Custom Designed CNN and\nestablished DL architecture like AlexNet; transfer learning on five models\npre-trained using ImageNet such as EfficientNetV2, ResNet152V2, Xception,\nInception-ResNetV2, MobileNetV3; and cutting-edge foundation models like YOLOv8\nand DINOv2, a self-supervised Vision Transformer Model. All models performed\nwell, but Xception outperformed all of them in terms of generalization,\nachieving 98% accuracy on the test data, with a model size of 80.03 MB and a\nprediction time of 0.0633 seconds. A key aspect of this research was the\napplication of Explainable AI to provide the explainability of all the models.\nThis journal presents the explainability of Xception model with LIME, SHAP, and\nGradCAM, ensuring transparency and trustworthiness in the models' predictions.\nThis study highlights the importance of selecting the right model according to\ntask-specific needs. It also underscores the important role of explainability\nin deploying AI in agriculture, providing insightful information to help\nenhance AI-driven crop management strategies.", "paper_summary_zh": "<paragraph>\u8fd1\u5e74\u4f86\uff0c\u4eba\u5de5\u667a\u6167\u7684\u666e\u53ca\u7387\u4e0d\u65b7\u63d0\u9ad8\uff0c\u9032\u800c\u5c0e\u81f4\u5c0d\u5f71\u50cf\u5206\u985e\u7684\u8208\u8da3\u6fc0\u589e\uff0c\u7279\u5225\u662f\u5728\u8fb2\u696d\u9818\u57df\u3002\u5728\u96fb\u8166\u8996\u89ba\u3001\u6a5f\u5668\u5b78\u7fd2\u8207\u6df1\u5ea6\u5b78\u7fd2\u7684\u5e6b\u52a9\u4e0b\uff0c\u8a72\u9818\u57df\u5df2\u6b77\u7d93\u91cd\u5927\u8f49\u578b\uff0c\u9032\u800c\u958b\u767c\u51fa\u65b0\u7684\u7530\u9593\u4f5c\u7269\u5206\u985e\u6280\u8853\u3002\u5118\u7ba1\u5df2\u5c0d\u5404\u7a2e\u5f71\u50cf\u5206\u985e\u6280\u8853\u9032\u884c\u5ee3\u6cdb\u7814\u7a76\uff0c\u4f46\u5927\u591a\u6578\u6280\u8853\u90fd\u6709\u5176\u9650\u5236\uff0c\u4f8b\u5982\u6e96\u78ba\u5ea6\u4f4e\u3001\u8cc7\u6599\u4f7f\u7528\u53d7\u9650\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u5831\u544a\u6a21\u578b\u5927\u5c0f\u8207\u9810\u6e2c\u3002\u6700\u986f\u8457\u7684\u9650\u5236\u5728\u65bc\u9700\u8981\u6a21\u578b\u53ef\u89e3\u91cb\u6027\u3002\u672c\u7814\u7a76\u8a55\u4f30\u4e86\u56db\u7a2e\u4e0d\u540c\u7684\u4f5c\u7269\u5206\u985e\u65b9\u6cd5\uff0c\u5373\u63a1\u7528\u624b\u5de5\u7279\u5fb5\u8403\u53d6\u65b9\u6cd5\uff08\u4f8b\u5982 SIFT\u3001ORB \u548c\u8272\u5f69\u76f4\u65b9\u5716\uff09\u7684\u50b3\u7d71\u6a5f\u5668\u5b78\u7fd2\uff1b\u81ea\u8a02\u8a2d\u8a08\u7684 CNN \u548c\u65e2\u5b9a\u7684\u6df1\u5ea6\u5b78\u7fd2\u67b6\u69cb\uff08\u4f8b\u5982 AlexNet\uff09\uff1b\u4f7f\u7528 ImageNet \u9810\u5148\u8a13\u7df4\u7684\u4e94\u500b\u6a21\u578b\uff08\u4f8b\u5982 EfficientNetV2\u3001ResNet152V2\u3001Xception\u3001Inception-ResNetV2\u3001MobileNetV3\uff09\u9032\u884c\u9077\u79fb\u5b78\u7fd2\uff1b\u4ee5\u53ca\u5c16\u7aef\u7684\u57fa\u790e\u6a21\u578b\uff08\u4f8b\u5982 YOLOv8 \u548c DINOv2\uff0c\u4e00\u7a2e\u81ea\u6211\u76e3\u7763\u7684\u8996\u89baTransformer\u6a21\u578b\uff09\u3002\u6240\u6709\u6a21\u578b\u7684\u8868\u73fe\u90fd\u5f88\u597d\uff0c\u4f46 Xception \u5728\u6cdb\u5316\u6027\u65b9\u9762\u512a\u65bc\u6240\u6709\u6a21\u578b\uff0c\u5728\u6e2c\u8a66\u8cc7\u6599\u4e0a\u9054\u5230 98% \u7684\u6e96\u78ba\u5ea6\uff0c\u6a21\u578b\u5927\u5c0f\u70ba 80.03 MB\uff0c\u9810\u6e2c\u6642\u9593\u70ba 0.0633 \u79d2\u3002\u672c\u7814\u7a76\u7684\u4e00\u500b\u95dc\u9375\u9762\u5411\u662f\u61c9\u7528\u53ef\u89e3\u91cb AI \u4f86\u63d0\u4f9b\u6240\u6709\u6a21\u578b\u7684\u53ef\u89e3\u91cb\u6027\u3002\u672c\u671f\u520a\u4f7f\u7528 LIME\u3001SHAP \u548c GradCAM \u5448\u73fe Xception \u6a21\u578b\u7684\u53ef\u89e3\u91cb\u6027\uff0c\u78ba\u4fdd\u6a21\u578b\u9810\u6e2c\u7684\u900f\u660e\u5ea6\u8207\u53ef\u4fe1\u5ea6\u3002\u672c\u7814\u7a76\u5f37\u8abf\u6839\u64da\u7279\u5b9a\u4efb\u52d9\u9700\u6c42\u9078\u64c7\u6b63\u78ba\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002\u5b83\u4e5f\u5f37\u8abf\u53ef\u89e3\u91cb\u6027\u5728\u8fb2\u696d\u4e2d\u90e8\u7f72 AI \u6642\u6240\u626e\u6f14\u7684\u91cd\u8981\u89d2\u8272\uff0c\u63d0\u4f9b\u898b\u89e3\u8c50\u5bcc\u7684\u8cc7\u8a0a\u4f86\u5354\u52a9\u5f37\u5316 AI \u9a45\u52d5\u7684\u4f5c\u7269\u7ba1\u7406\u7b56\u7565\u3002</paragraph>", "author": "Sudi Murindanyi et.al.", "authors": "Sudi Murindanyi, Joyce Nakatumba-Nabende, Rahman Sanya, Rose Nakibuule, Andrew Katumba", "id": "2408.12426v1", "paper_url": "http://arxiv.org/abs/2408.12426v1", "repo": "null"}}