{"2412.18241": {"publish_time": "2024-12-24", "title": "An Automatic Graph Construction Framework based on Large Language Models for Recommendation", "paper_summary": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people.", "paper_summary_zh": "\u5716\u795e\u7d93\u7db2\u8def (GNN) \u5df2\u6210\u70ba\u6700\u5148\u9032\u7684\u65b9\u6cd5\uff0c\u53ef\u5f9e\u5716\u5f62\u7d50\u69cb\u5316\u8cc7\u6599\u4e2d\u5b78\u7fd2\u63a8\u85a6\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u57fa\u65bc GNN \u7684\u63a8\u85a6\u65b9\u6cd5\u5927\u591a\u5074\u91cd\u65bc\u9810\u5b9a\u7fa9\u5716\u5f62\u4e0a\u7684\u6a21\u578b\u7d50\u69cb\u548c\u5b78\u7fd2\u7b56\u7565\u7684\u6700\u4f73\u5316\uff0c\u5ffd\u7565\u4e86\u5716\u5f62\u5efa\u69cb\u968e\u6bb5\u7684\u91cd\u8981\u6027\u3002\u65e9\u671f\u5716\u5f62\u5efa\u69cb\u5de5\u4f5c\u901a\u5e38\u4f9d\u8cf4\u65bc\u7279\u5b9a\u898f\u5247\u6216\u7fa4\u773e\u5916\u5305\uff0c\u9019\u4e9b\u65b9\u6cd5\u904e\u65bc\u7c21\u5316\u6216\u904e\u65bc\u52de\u52d5\u5bc6\u96c6\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u958b\u59cb\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f86\u81ea\u52d5\u5316\u5716\u5f62\u5efa\u69cb\uff0c\u56e0\u70ba\u5b83\u5011\u5177\u6709\u8c50\u5bcc\u7684\u958b\u653e\u4e16\u754c\u77e5\u8b58\u548c\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u5b83\u5011\u901a\u5e38\u5b58\u5728\u5169\u500b\u9650\u5236\uff1a(1) \u5168\u57df\u6aa2\u8996\u7684\u4e0d\u53ef\u898b\u6027\uff08\u4f8b\u5982\uff0c\u5ffd\u7565\u4e0a\u4e0b\u6587\u8cc7\u8a0a\uff09\u548c (2) \u5efa\u69cb\u6548\u7387\u4f4e\u4e0b\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86 AutoGraph\uff0c\u4e00\u500b\u57fa\u65bc LLM \u7684\u81ea\u52d5\u5716\u5f62\u5efa\u69cb\u6846\u67b6\uff0c\u7528\u65bc\u63a8\u85a6\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528 LLM \u63a8\u65b7\u4f7f\u7528\u8005\u504f\u597d\u548c\u9805\u76ee\u77e5\u8b58\uff0c\u4e26\u5c07\u5176\u7de8\u78bc\u70ba\u8a9e\u7fa9\u5411\u91cf\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u63a1\u7528\u5411\u91cf\u91cf\u5316\u5f9e\u8a9e\u7fa9\u5411\u91cf\u4e2d\u63d0\u53d6\u6f5b\u5728\u56e0\u5b50\u3002\u7136\u5f8c\u5c07\u6f5b\u5728\u56e0\u5b50\u4f5c\u70ba\u984d\u5916\u7bc0\u9ede\u52a0\u5165\uff0c\u4ee5\u9023\u7d50\u4f7f\u7528\u8005/\u9805\u76ee\u7bc0\u9ede\uff0c\u5f9e\u800c\u5f62\u6210\u4e00\u500b\u5177\u6709\u6df1\u5165\u5168\u57df\u6aa2\u8996\u8a9e\u7fa9\u7684\u5716\u5f62\u3002\u6211\u5011\u9032\u4e00\u6b65\u8a2d\u8a08\u4e86\u57fa\u65bc\u5143\u8def\u5f91\u7684\u8a0a\u606f\u805a\u5408\uff0c\u4ee5\u6709\u6548\u805a\u5408\u8a9e\u7fa9\u548c\u5354\u4f5c\u8cc7\u8a0a\u3002\u8a72\u6846\u67b6\u8207\u6a21\u578b\u7121\u95dc\uff0c\u4e26\u8207\u4e0d\u540c\u7684\u4e3b\u5e79\u6a21\u578b\u76f8\u5bb9\u3002\u5728\u4e09\u500b\u771f\u5be6\u4e16\u754c\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\u4e86 AutoGraph \u8207\u73fe\u6709\u57fa\u6e96\u65b9\u6cd5\u76f8\u6bd4\u7684\u6548\u80fd\u548c\u6548\u7387\u3002\u6211\u5011\u5df2\u5728\u83ef\u70ba\u5ee3\u544a\u5e73\u53f0\u4e0a\u90e8\u7f72\u4e86 AutoGraph\uff0c\u4e26\u5728\u7dda\u4e0a A/B \u6e2c\u8a66\u4e2d\u7372\u5f97\u4e86 RPM \u63d0\u5347 2.69% \u548c eCPM \u63d0\u5347 7.31%\u3002\u76ee\u524d AutoGraph \u5df2\u88ab\u7528\u4f5c\u4e3b\u8981\u7684\u6d41\u91cf\u6a21\u578b\uff0c\u670d\u52d9\u65bc\u6578\u5104\u4eba\u3002", "author": "Rong Shan et.al.", "authors": "Rong Shan, Jianghao Lin, Chenxu Zhu, Bo Chen, Menghui Zhu, Kangning Zhang, Jieming Zhu, Ruiming Tang, Yong Yu, Weinan Zhang", "id": "2412.18241v1", "paper_url": "http://arxiv.org/abs/2412.18241v1", "repo": "https://github.com/lavieenrose365/autograph"}}