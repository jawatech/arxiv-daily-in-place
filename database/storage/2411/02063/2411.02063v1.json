{"2411.02063": {"publish_time": "2024-11-04", "title": "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "paper_summary": "Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer.", "paper_summary_zh": "\u540c\u6642\u63d0\u5347\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6548\u80fd\u548c\u6548\u7387\uff0c\u662f\u4e00\u9805\u91cd\u8981\u537b\u8271\u9245\u7684\u7814\u7a76\u76ee\u6a19\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u4f4e\u79e9\u9810\u8a13\u7df4\uff0c\u901a\u5e38\u88ab\u8996\u70ba\u6703\u5f71\u97ff\u6548\u80fd\u7684\u6709\u6548\u7387\u65b9\u6cd5\uff0c\u7576\u7cbe\u78ba\u9396\u5b9a\u53c3\u6578\u6642\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u64f4\u5145\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u50c5\u5c07\u4f4e\u7dad\u5ea6\u6a21\u7d44\u61c9\u7528\u65bc\u6ce8\u610f\u529b\u5c64\uff0c\u53ef\u4ee5\u89e3\u6c7a\u6b64\u554f\u984c\uff0c\u4e26\u63d0\u5347\u6548\u80fd\u548c\u6548\u7387\u3002\u6211\u5011\u5c07\u6b64\u7d50\u69cb\u7a31\u70ba\u4f4e\u7dad\u5ea6\u6295\u5f71\u6ce8\u610f\u529b (LPA)\uff0c\u4e26\u63d0\u4f9b\u8aaa\u660e\u6027\u5206\u6790\u3002\u900f\u904e\u5728 130M\u3001370M \u7684\u53c3\u6578\u898f\u6a21\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4e26\u64f4\u5145\u5230 3B\uff0c\u6211\u5011\u9a57\u8b49\u4e86 LPA \u7684\u6548\u80fd\u548c\u53ef\u64f4\u5145\u6027\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u8207\u539f\u59cb Transformer \u76f8\u6bd4\uff0cLPA \u6a21\u578b\u53ef\u4ee5\u7bc0\u7701\u591a\u9054 12.4% \u7684\u6642\u9593\uff0c\u540c\u6642\u5728\u6e2c\u8a66\u56f0\u60d1\u5ea6 (ppl) \u548c\u4e0b\u6e38\u4efb\u52d9\u4e2d\u7372\u5f97\u7d04 5% \u7684\u63d0\u5347\u3002", "author": "Xingtai Lv et.al.", "authors": "Xingtai Lv, Ning Ding, Kaiyan Zhang, Ermo Hua, Ganqu Cui, Bowen Zhou", "id": "2411.02063v1", "paper_url": "http://arxiv.org/abs/2411.02063v1", "repo": "null"}}