{"2410.10398": {"publish_time": "2024-10-14", "title": "FairMindSim: Alignment of Behavior, Emotion, and Belief in Humans and LLM Agents Amid Ethical Dilemmas", "paper_summary": "AI alignment is a pivotal issue concerning AI control and safety. It should\nconsider not only value-neutral human preferences but also moral and ethical\nconsiderations. In this study, we introduced FairMindSim, which simulates the\nmoral dilemma through a series of unfair scenarios. We used LLM agents to\nsimulate human behavior, ensuring alignment across various stages. To explore\nthe various socioeconomic motivations, which we refer to as beliefs, that drive\nboth humans and LLM agents as bystanders to intervene in unjust situations\ninvolving others, and how these beliefs interact to influence individual\nbehavior, we incorporated knowledge from relevant sociological fields and\nproposed the Belief-Reward Alignment Behavior Evolution Model (BREM) based on\nthe recursive reward model (RRM). Our findings indicate that, behaviorally,\nGPT-4o exhibits a stronger sense of social justice, while humans display a\nricher range of emotions. Additionally, we discussed the potential impact of\nemotions on behavior. This study provides a theoretical foundation for\napplications in aligning LLMs with altruistic values.", "paper_summary_zh": "AI \u5c0d\u9f4a\u662f\u95dc\u65bc AI \u63a7\u5236\u548c\u5b89\u5168\u7684\u4e00\u500b\u6838\u5fc3\u8b70\u984c\u3002\u5b83\u4e0d\u53ea\u61c9\u8003\u91cf\u50f9\u503c\u4e2d\u7acb\u7684\u4eba\u985e\u504f\u597d\uff0c\u4e5f\u61c9\u8003\u91cf\u9053\u5fb7\u548c\u502b\u7406\u8003\u91cf\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86 FairMindSim\uff0c\u5b83\u900f\u904e\u4e00\u7cfb\u5217\u4e0d\u516c\u5e73\u60c5\u5883\u6a21\u64ec\u9053\u5fb7\u5169\u96e3\u3002\u6211\u5011\u4f7f\u7528 LLM \u4ee3\u7406\u6a21\u64ec\u4eba\u985e\u884c\u70ba\uff0c\u78ba\u4fdd\u5728\u5404\u500b\u968e\u6bb5\u7684\u4e00\u81f4\u6027\u3002\u70ba\u4e86\u63a2\u7d22\u5404\u7a2e\u793e\u6703\u7d93\u6fdf\u52d5\u6a5f\uff08\u6211\u5011\u7a31\u4e4b\u70ba\u4fe1\u5ff5\uff09\uff0c\u9019\u4e9b\u52d5\u6a5f\u9a45\u4f7f\u4eba\u985e\u548c LLM \u4ee3\u7406\u4f5c\u70ba\u65c1\u89c0\u8005\u4ecb\u5165\u6d89\u53ca\u4ed6\u4eba\u7684\u4e0d\u516c\u6b63\u60c5\u6cc1\uff0c\u4ee5\u53ca\u9019\u4e9b\u4fe1\u5ff5\u5982\u4f55\u4e92\u52d5\u4ee5\u5f71\u97ff\u500b\u4eba\u884c\u70ba\uff0c\u6211\u5011\u7d0d\u5165\u4e86\u76f8\u95dc\u793e\u6703\u5b78\u9818\u57df\u7684\u77e5\u8b58\uff0c\u4e26\u6839\u64da\u905e\u8ff4\u734e\u52f5\u6a21\u578b (RRM) \u63d0\u51fa\u4fe1\u5ff5\u734e\u52f5\u5c0d\u9f4a\u884c\u70ba\u6f14\u5316\u6a21\u578b (BREM)\u3002\u6211\u5011\u7684\u767c\u73fe\u8868\u660e\uff0c\u5728\u884c\u70ba\u4e0a\uff0cGPT-4o \u8868\u73fe\u51fa\u66f4\u5f37\u70c8\u7684\u793e\u6703\u6b63\u7fa9\u611f\uff0c\u800c\u4eba\u985e\u5247\u8868\u73fe\u51fa\u66f4\u8c50\u5bcc\u7684\u60c5\u7dd2\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a0e\u8ad6\u4e86\u60c5\u7dd2\u5c0d\u884c\u70ba\u7684\u6f5b\u5728\u5f71\u97ff\u3002\u672c\u7814\u7a76\u70ba\u5c07 LLM \u8207\u5229\u4ed6\u50f9\u503c\u89c0\u5c0d\u9f4a\u7684\u61c9\u7528\u63d0\u4f9b\u4e86\u7406\u8ad6\u57fa\u790e\u3002", "author": "Yu Lei et.al.", "authors": "Yu Lei, Hao Liu, Chengxing Xie, Songjia Liu, Zhiyu Yin, Canyu chen, Guohao Li, Philip Torr, Zhen Wu", "id": "2410.10398v1", "paper_url": "http://arxiv.org/abs/2410.10398v1", "repo": "null"}}