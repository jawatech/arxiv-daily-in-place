{"2408.08670": {"publish_time": "2024-08-16", "title": "Adaptive Layer Selection for Efficient Vision Transformer Fine-Tuning", "paper_summary": "Recently, foundation models based on Vision Transformers (ViTs) have become\nwidely available. However, their fine-tuning process is highly\nresource-intensive, and it hinders their adoption in several edge or low-energy\napplications. To this end, in this paper we introduce an efficient fine-tuning\nmethod for ViTs called $\\textbf{ALaST}$ ($\\textit{Adaptive Layer Selection\nFine-Tuning for Vision Transformers}$) to speed up the fine-tuning process\nwhile reducing computational cost, memory load, and training time. Our approach\nis based on the observation that not all layers are equally critical during\nfine-tuning, and their importance varies depending on the current mini-batch.\nTherefore, at each fine-tuning step, we adaptively estimate the importance of\nall layers and we assign what we call ``compute budgets'' accordingly. Layers\nthat were allocated lower budgets are either trained with a reduced number of\ninput tokens or kept frozen. Freezing a layer reduces the computational cost\nand memory usage by preventing updates to its weights, while discarding tokens\nremoves redundant data, speeding up processing and reducing memory\nrequirements. We show that this adaptive compute allocation enables a\nnearly-optimal schedule for distributing computational resources across layers,\nresulting in substantial reductions in training time (up to 1.5x), FLOPs (up to\n2x), and memory load (up to 2x) compared to traditional full fine-tuning\napproaches. Additionally, it can be successfully combined with other\nparameter-efficient fine-tuning methods, such as LoRA.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\uff0c\u57fa\u4e8e\u89c6\u89c9\u8f6c\u6362\u5668 (ViT) \u7684\u57fa\u7840\u6a21\u578b\u5df2\u5e7f\u6cdb\u53ef\u7528\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u5fae\u8c03\u8fc7\u7a0b\u975e\u5e38\u8017\u8d39\u8d44\u6e90\uff0c\u5e76\u4e14\u963b\u788d\u4e86\u5b83\u4eec\u5728\u591a\u4e2a\u8fb9\u7f18\u6216\u4f4e\u80fd\u8017\u5e94\u7528\u4e2d\u7684\u91c7\u7528\u3002\u4e3a\u6b64\uff0c\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a $\\textbf{ALaST}$\uff08$\\textit{\u89c6\u89c9\u8f6c\u6362\u5668\u7684\u81ea\u9002\u5e94\u5c42\u9009\u62e9\u5fae\u8c03}$\uff09\u7684 ViT \u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3001\u5185\u5b58\u8d1f\u8f7d\u548c\u8bad\u7ec3\u65f6\u95f4\u7684\u8fc7\u7a0b\u4e2d\u52a0\u901f\u5fae\u8c03\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u57fa\u4e8e\u8fd9\u6837\u7684\u89c2\u5bdf\uff1a\u5e76\u975e\u6240\u6709\u5c42\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u90fd\u540c\u6837\u5173\u952e\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u91cd\u8981\u6027\u4f1a\u6839\u636e\u5f53\u524d\u7684\u5c0f\u6279\u91cf\u800c\u6709\u6240\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u5728\u6bcf\u4e2a\u5fae\u8c03\u6b65\u9aa4\u4e2d\uff0c\u6211\u4eec\u81ea\u9002\u5e94\u5730\u4f30\u8ba1\u6240\u6709\u5c42\u7684\u6743\u91cd\uff0c\u5e76\u76f8\u5e94\u5730\u5206\u914d\u6211\u4eec\u6240\u8c13\u7684 ``\u8ba1\u7b97\u9884\u7b97''\u3002\u5206\u914d\u8f83\u4f4e\u9884\u7b97\u7684\u5c42\u8981\u4e48\u4f7f\u7528\u51cf\u5c11\u6570\u91cf\u7684\u8f93\u5165\u6807\u8bb0\u8fdb\u884c\u8bad\u7ec3\uff0c\u8981\u4e48\u4fdd\u6301\u51bb\u7ed3\u3002\u51bb\u7ed3\u5c42\u901a\u8fc7\u9632\u6b62\u5176\u6743\u91cd\u7684\u66f4\u65b0\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u800c\u4e22\u5f03\u6807\u8bb0\u5219\u4f1a\u5220\u9664\u5197\u4f59\u6570\u636e\uff0c\u4ece\u800c\u52a0\u5feb\u5904\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002\u6211\u4eec\u8868\u660e\uff0c\u8fd9\u79cd\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d\u80fd\u591f\u4e3a\u8de8\u5c42\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u5236\u5b9a\u8fd1\u4e4e\u6700\u4f18\u7684\u8c03\u5ea6\uff0c\u4e0e\u4f20\u7edf\u7684\u5b8c\u5168\u5fae\u8c03\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53ef\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff08\u6700\u591a 1.5 \u500d\uff09\u3001FLOP\uff08\u6700\u591a 2 \u500d\uff09\u548c\u5185\u5b58\u8d1f\u8f7d\uff08\u6700\u591a 2 \u500d\uff09\u3002\u6b64\u5916\uff0c\u5b83\u53ef\u4ee5\u6210\u529f\u5730\u4e0e\u5176\u4ed6\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff08\u4f8b\u5982 LoRA\uff09\u7ed3\u5408\u4f7f\u7528\u3002</paragraph>", "author": "Alessio Devoto et.al.", "authors": "Alessio Devoto, Federico Alvetreti, Jary Pomponi, Paolo Di Lorenzo, Pasquale Minervini, Simone Scardapane", "id": "2408.08670v1", "paper_url": "http://arxiv.org/abs/2408.08670v1", "repo": "null"}}