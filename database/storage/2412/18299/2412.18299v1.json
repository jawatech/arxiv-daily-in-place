{"2412.18299": {"publish_time": "2024-12-24", "title": "M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models", "paper_summary": "With the widespread application of Large Language Models (LLMs) in the field\nof Natural Language Processing (NLP), enhancing their performance has become a\nresearch hotspot. This paper presents a novel multi-prompt ensemble decoding\napproach designed to bolster the generation quality of LLMs by leveraging the\naggregation of outcomes from multiple prompts. Given a unique input $X$, we\nsubmit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and\nderive probability distributions. For each token prediction, we calculate the\nensemble probability by averaging the $n$ probability distributions within the\nbatch, utilizing this aggregated probability to generate the token. This\ntechnique is dubbed Inner-Batch Ensemble. To facilitate efficient batch\ninference, we implement a Left-Padding strategy to maintain uniform input\nlengths across the n prompts. Through extensive experimentation on diverse NLP\ntasks, including machine translation, code generation, and text simplification,\nwe demonstrate the efficacy of our method in enhancing LLM performance. The\nresults show substantial improvements in BLEU scores, pass@$k$ rates, and LENS\nmetrics over conventional methods.", "paper_summary_zh": "\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u81ea\u7136\u8a9e\u8a00\u8655\u7406 (NLP) \u9818\u57df\u7684\u5ee3\u6cdb\u61c9\u7528\uff0c\u63d0\u5347\u5176\u6548\u80fd\u5df2\u6210\u70ba\u7814\u7a76\u71b1\u9ede\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u591a\u63d0\u793a\u96c6\u5408\u89e3\u78bc\u65b9\u6cd5\uff0c\u65e8\u5728\u900f\u904e\u5229\u7528\u591a\u500b\u63d0\u793a\u7d50\u679c\u7684\u805a\u5408\u4f86\u63d0\u5347 LLM \u7684\u751f\u6210\u54c1\u8cea\u3002\u7d66\u5b9a\u4e00\u500b\u7368\u7279\u7684\u8f38\u5165 $X$\uff0c\u6211\u5011\u6703\u4ee5\u6279\u6b21\u6a21\u5f0f\u5c07 $X$ \u7684 $n$ \u500b\u63d0\u793a\u8b8a\u5f62\u63d0\u4ea4\u7d66 LLM \u4ee5\u9032\u884c\u89e3\u78bc\u4e26\u63a8\u5c0e\u6a5f\u7387\u5206\u5e03\u3002\u5c0d\u65bc\u6bcf\u500b\u6a19\u8a18\u9810\u6e2c\uff0c\u6211\u5011\u6703\u8a08\u7b97\u6279\u6b21\u4e2d $n$ \u500b\u6a5f\u7387\u5206\u5e03\u7684\u5e73\u5747\u503c\u4f86\u8a08\u7b97\u96c6\u5408\u6a5f\u7387\uff0c\u4e26\u5229\u7528\u9019\u500b\u805a\u5408\u6a5f\u7387\u4f86\u751f\u6210\u6a19\u8a18\u3002\u6b64\u6280\u8853\u7a31\u70ba\u6279\u6b21\u5167\u96c6\u5408\u3002\u70ba\u4e86\u4fc3\u9032\u6709\u6548\u6279\u6b21\u63a8\u8ad6\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u4e00\u500b\u5de6\u908a\u88dc\u9f4a\u7b56\u7565\uff0c\u4ee5\u7dad\u8b77 $n$ \u500b\u63d0\u793a\u9593\u7684\u8f38\u5165\u9577\u5ea6\u4e00\u81f4\u3002\u900f\u904e\u5c0d\u5404\u7a2e NLP \u4efb\u52d9\uff08\u5305\u62ec\u6a5f\u5668\u7ffb\u8b6f\u3001\u7a0b\u5f0f\u78bc\u751f\u6210\u548c\u6587\u5b57\u7c21\u5316\uff09\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u63d0\u5347 LLM \u6548\u80fd\u65b9\u9762\u7684\u6548\u529b\u3002\u7d50\u679c\u986f\u793a\uff0c\u8207\u50b3\u7d71\u65b9\u6cd5\u76f8\u6bd4\uff0cBLEU \u5206\u6578\u3001pass@$k$ \u7387\u548c LENS \u6307\u6a19\u5747\u6709\u986f\u8457\u63d0\u5347\u3002", "author": "Jiaxin Guo et.al.", "authors": "Jiaxin Guo, Daimeng Wei, Yuanchang Luo, Shimin Tao, Hengchao Shang, Zongyao Li, Shaojun Li, Jinlong Yang, Zhanglin Wu, Zhiqiang Rao, Hao Yang", "id": "2412.18299v1", "paper_url": "http://arxiv.org/abs/2412.18299v1", "repo": "null"}}