{"2501.07888": {"publish_time": "2025-01-14", "title": "Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding", "paper_summary": "We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)\ndesigned for generating detailed and accurate video descriptions, while also\nexhibiting superior general video understanding capabilities. Tarsier2 achieves\nsignificant advancements through three key upgrades: (1) Scaling pre-training\ndata from 11M to 40M video-text pairs, enriching both volume and diversity; (2)\nPerforming fine-grained temporal alignment during supervised fine-tuning; (3)\nUsing model-based sampling to automatically construct preference data and\napplying DPO training for optimization. Extensive experiments show that\nTarsier2-7B consistently outperforms leading proprietary models, including\nGPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K\nbenchmark, Tarsier2-7B improves F1 by 2.8\\% over GPT-4o and 5.8\\% over\nGemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\\%\nperformance advantage over GPT-4o and +24.9\\% over Gemini-1.5-Pro. Tarsier2-7B\nalso sets new state-of-the-art results across 15 public benchmarks, spanning\ntasks such as video question-answering, video grounding, hallucination test,\nand embodied question-answering, demonstrating its versatility as a robust\ngeneralist vision-language model.", "paper_summary_zh": "<paragraph>\u6211\u5011\u4ecb\u7d39 Tarsier2\uff0c\u9019\u662f\u4e00\u7a2e\u6700\u5148\u9032\u7684\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLM)\uff0c\n\u5c08\u70ba\u7522\u751f\u8a73\u7d30\u4e14\u6e96\u78ba\u7684\u5f71\u7247\u8aaa\u660e\u800c\u8a2d\u8a08\uff0c\u540c\u6642\u9084\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u6574\u9ad4\u5f71\u7247\u7406\u89e3\u80fd\u529b\u3002Tarsier2 \u900f\u904e\u4e09\u9805\u95dc\u9375\u5347\u7d1a\u5be6\u73fe\u986f\u8457\u9032\u6b65\uff1a(1) \u5c07\u9810\u8a13\u7df4\n\u8cc7\u6599\u5f9e 11M \u64f4\u5145\u81f3 40M \u7684\u5f71\u7247\u6587\u5b57\u914d\u5c0d\uff0c\u8c50\u5bcc\u6578\u91cf\u548c\u591a\u6a23\u6027\uff1b(2)\n\u5728\u76e3\u7763\u5fae\u8abf\u671f\u9593\u57f7\u884c\u7d30\u7dfb\u7684\u6642\u9593\u5c0d\u9f4a\uff1b(3)\n\u4f7f\u7528\u57fa\u65bc\u6a21\u578b\u7684\u62bd\u6a23\u81ea\u52d5\u5efa\u69cb\u504f\u597d\u8cc7\u6599\uff0c\u4e26\u61c9\u7528 DPO \u8a13\u7df4\u9032\u884c\u6700\u4f73\u5316\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u986f\u793a\uff0c\nTarsier2-7B \u5728\u8a73\u7d30\u7684\u5f71\u7247\u8aaa\u660e\u4efb\u52d9\u4e2d\u59cb\u7d42\u512a\u65bc\u9818\u5148\u7684\u5c08\u6709\u6a21\u578b\uff0c\u5305\u62ec\nGPT-4o \u548c Gemini 1.5 Pro\u3002\u5728 DREAM-1K\n\u57fa\u6e96\u6e2c\u8a66\u4e2d\uff0cTarsier2-7B \u5c07 F1 \u63d0\u5347\u4e86 2.8%\uff0c\u512a\u65bc GPT-4o\uff0c\u4e26\u5c07\nGemini-1.5-Pro \u63d0\u5347\u4e86 5.8%\u3002\u5728\u4eba\u985e\u4e26\u6392\u8a55\u4f30\u4e2d\uff0cTarsier2-7B \u986f\u793a\u51fa\u6bd4\nGPT-4o \u9ad8\u51fa +8.6%\uff0c\u6bd4 Gemini-1.5-Pro \u9ad8\u51fa +24.9% \u7684\u6548\u80fd\u512a\u52e2\u3002Tarsier2-7B\n\u4e5f\u5275\u4e0b 15 \u500b\u516c\u958b\u57fa\u6e96\u6e2c\u8a66\u7684\u6700\u65b0\u6280\u8853\u6210\u679c\uff0c\u6db5\u84cb\u5f71\u7247\u554f\u7b54\u3001\u5f71\u7247\u63a5\u5730\u3001\u5e7b\u89ba\u6e2c\u8a66\uff0c\n\u4ee5\u53ca\u5177\u9ad4\u554f\u984c\u89e3\u7b54\u7b49\u4efb\u52d9\uff0c\u8b49\u660e\u5176\u4f5c\u70ba\u5f37\u5927\u901a\u624d\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u591a\u529f\u80fd\u6027\u3002</paragraph>", "author": "Liping Yuan et.al.", "authors": "Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, Yuan Lin", "id": "2501.07888v1", "paper_url": "http://arxiv.org/abs/2501.07888v1", "repo": "null"}}