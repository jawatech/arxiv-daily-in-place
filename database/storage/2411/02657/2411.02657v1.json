{"2411.02657": {"publish_time": "2024-11-04", "title": "Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge", "paper_summary": "Rare diseases present unique challenges in healthcare, often suffering from\ndelayed diagnosis and fragmented information landscapes. The scarcity of\nreliable knowledge in these conditions poses a distinct challenge for Large\nLanguage Models (LLMs) in supporting clinical management and delivering precise\npatient information underscoring the need for focused training on these 'zebra'\ncases. We present Zebra-Llama, a specialized context-aware language model with\nhigh precision Retrieval Augmented Generation (RAG) capability, focusing on\nEhlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000\nindividuals, exemplifies the complexities of rare diseases with its diverse\nsymptoms, multiple subtypes, and evolving diagnostic criteria. By implementing\na novel context-aware fine-tuning methodology trained on questions derived from\nmedical literature, patient experiences, and clinical resources, along with\nexpertly curated responses, Zebra-Llama demonstrates unprecedented capabilities\nin handling EDS-related queries. On a test set of real-world questions\ncollected from EDS patients and clinicians, medical experts evaluated the\nresponses generated by both models, revealing Zebra-Llama's substantial\nimprovements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.\n70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation\nreliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama\nnot only provides more accessible and reliable EDS information but also\nestablishes a framework for developing specialized AI solutions for other rare\nconditions. This work represents a crucial step towards democratizing\nexpert-level knowledge in rare disease management, potentially transforming how\nhealthcare providers and patients navigate the complex landscape of rare\ndiseases.", "paper_summary_zh": "<paragraph>\u7f55\u898b\u75be\u75c5\u5c0d\u91ab\u7642\u4fdd\u5065\u63d0\u51fa\u4e86\u7368\u7279\u7684\u6311\u6230\uff0c\u901a\u5e38\u6703\u5ef6\u8aa4\u8a3a\u65b7\u4e26\u9020\u6210\u8cc7\u8a0a\u5206\u6563\u7684\u554f\u984c\u3002\u9019\u4e9b\u75be\u75c5\u7f3a\u4e4f\u53ef\u9760\u7684\u77e5\u8b58\uff0c\u5c0d\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u652f\u6301\u81e8\u5e8a\u7ba1\u7406\u548c\u63d0\u4f9b\u7cbe\u78ba\u7684\u60a3\u8005\u8cc7\u8a0a\u65b9\u9762\u69cb\u6210\u4e86\u7368\u7279\u7684\u6311\u6230\uff0c\u9019\u51f8\u986f\u4e86\u91dd\u5c0d\u9019\u4e9b\u300c\u6591\u99ac\u300d\u75c5\u4f8b\u9032\u884c\u91cd\u9ede\u8a13\u7df4\u7684\u5fc5\u8981\u6027\u3002\u6211\u5011\u63d0\u51fa Zebra-Llama\uff0c\u9019\u662f\u4e00\u500b\u5c08\u9580\u7684\u8108\u7d61\u611f\u77e5\u8a9e\u8a00\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u7cbe\u78ba\u5ea6\u7684\u6aa2\u7d22\u64f4\u5145\u751f\u6210 (RAG) \u80fd\u529b\uff0c\u4e26\u4ee5 Ehlers-Danlos \u75c7\u5019\u7fa4 (EDS) \u4f5c\u70ba\u6211\u5011\u7684\u6848\u4f8b\u7814\u7a76\u3002EDS \u5f71\u97ff\u6bcf 5,000 \u4eba\u4e2d\u7684 1 \u4eba\uff0c\u5176\u75c7\u72c0\u591a\u6a23\u3001\u4e9e\u578b\u773e\u591a\u4e14\u8a3a\u65b7\u6a19\u6e96\u4e0d\u65b7\u6f14\u8b8a\uff0c\u9ad4\u73fe\u4e86\u7f55\u898b\u75be\u75c5\u7684\u8907\u96dc\u6027\u3002\u900f\u904e\u5be6\u65bd\u4e00\u7a2e\u65b0\u7a4e\u7684\u8108\u7d61\u611f\u77e5\u5fae\u8abf\u65b9\u6cd5\uff0c\u8a72\u65b9\u6cd5\u5728\u5f9e\u91ab\u5b78\u6587\u737b\u3001\u60a3\u8005\u7d93\u9a57\u548c\u81e8\u5e8a\u8cc7\u6e90\u4e2d\u884d\u751f\u7684\u554f\u984c\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u7d50\u5408\u5c08\u5bb6\u7b56\u5283\u7684\u56de\u61c9\uff0cZebra-Llama \u5728\u8655\u7406\u8207 EDS \u76f8\u95dc\u7684\u67e5\u8a62\u65b9\u9762\u5c55\u73fe\u4e86\u524d\u6240\u672a\u6709\u7684\u80fd\u529b\u3002\u5728\u5f9e EDS \u60a3\u8005\u548c\u81e8\u5e8a\u91ab\u751f\u6536\u96c6\u7684\u771f\u5be6\u554f\u984c\u6e2c\u8a66\u96c6\u4e2d\uff0c\u91ab\u5b78\u5c08\u5bb6\u8a55\u4f30\u4e86\u9019\u5169\u7a2e\u6a21\u578b\u7522\u751f\u7684\u56de\u61c9\uff0c\u7d50\u679c\u986f\u793a Zebra-Llama \u5728\u5fb9\u5e95\u6027 (77.5% \u5c0d 70.1%)\u3001\u6e96\u78ba\u6027 (83.0% \u5c0d 78.8%)\u3001\u6e05\u6670\u5ea6 (74.7% \u5c0d 72.0%) \u548c\u5f15\u7528\u53ef\u9760\u6027 (70.6% \u5c0d 52.3%) \u65b9\u9762\u90fd\u6bd4\u57fa\u790e\u6a21\u578b (Llama 3.1-8B-Instruct) \u6709\u986f\u8457\u7684\u9032\u6b65\u3002Zebra-Llama \u4ee5\u958b\u653e\u539f\u59cb\u78bc\u8cc7\u6e90\u7684\u5f62\u5f0f\u91cb\u51fa\uff0c\u4e0d\u50c5\u63d0\u4f9b\u4e86\u66f4\u6613\u65bc\u53d6\u5f97\u4e14\u53ef\u9760\u7684 EDS \u8cc7\u8a0a\uff0c\u9084\u5efa\u7acb\u4e86\u4e00\u500b\u958b\u767c\u5176\u4ed6\u7f55\u898b\u75be\u75c5\u5c08\u7528 AI \u89e3\u6c7a\u65b9\u6848\u7684\u67b6\u69cb\u3002\u9019\u9805\u5de5\u4f5c\u4ee3\u8868\u4e86\u5728\u7f55\u898b\u75be\u75c5\u7ba1\u7406\u4e2d\u5be6\u73fe\u5c08\u5bb6\u7d1a\u77e5\u8b58\u6c11\u4e3b\u5316\u7684\u95dc\u9375\u4e00\u6b65\uff0c\u6709\u53ef\u80fd\u6539\u8b8a\u91ab\u7642\u4fdd\u5065\u63d0\u4f9b\u8005\u548c\u60a3\u8005\u5728\u7f55\u898b\u75be\u75c5\u8907\u96dc\u9818\u57df\u4e2d\u7684\u61c9\u5c0d\u65b9\u5f0f\u3002</paragraph>", "author": "Karthik Soman et.al.", "authors": "Karthik Soman, Andrew Langdon, Catalina Villouta, Chinmay Agrawal, Lashaw Salta, Braian Peetoom, Gianmarco Bellucci, Orion J Buske", "id": "2411.02657v1", "paper_url": "http://arxiv.org/abs/2411.02657v1", "repo": "https://github.com/karthiksoman/zebra-Llama"}}