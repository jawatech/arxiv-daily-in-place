{"2406.12832": {"publish_time": "2024-06-18", "title": "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "paper_summary": "Low-rank adaptation (LoRA) has become the default approach to fine-tune large\nlanguage models (LLMs) due to its significant reduction in trainable\nparameters. However, trainable parameter demand for LoRA increases with\nincreasing model embedding dimensions, leading to high compute costs.\nAdditionally, its backward updates require storing high-dimensional\nintermediate activations and optimizer states, demanding high peak GPU memory.\nIn this paper, we introduce large model fine-tuning via spectrally decomposed\nlow-dimensional adaptation (LaMDA), a novel approach to fine-tuning large\nlanguage models, which leverages low-dimensional adaptation to achieve\nsignificant reductions in trainable parameters and peak GPU memory footprint.\nLaMDA freezes a first projection matrix (PMA) in the adaptation path while\nintroducing a low-dimensional trainable square matrix, resulting in substantial\nreductions in trainable parameters and peak GPU memory usage. LaMDA gradually\nfreezes a second projection matrix (PMB) during the early fine-tuning stages,\nreducing the compute cost associated with weight updates to enhance parameter\nefficiency further. We also present an enhancement, LaMDA++, incorporating a\n``lite-weight\" adaptive rank allocation for the LoRA path via normalized\nspectrum analysis of pre-trained model weights. We evaluate LaMDA/LaMDA++\nacross various tasks, including natural language understanding with the GLUE\nbenchmark, text summarization, natural language generation, and complex\nreasoning on different LLMs. Results show that LaMDA matches or surpasses the\nperformance of existing alternatives while requiring up to 17.7x fewer\nparameter updates and up to 1.32x lower peak GPU memory usage during\nfine-tuning. Code will be publicly available.", "paper_summary_zh": "\u4f4e\u968e\u6539\u7de8 (LoRA) \u5df2\u6210\u70ba\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9810\u8a2d\u65b9\u6cd5\uff0c\u56e0\u70ba\u5b83\u5927\u5e45\u6e1b\u5c11\u4e86\u53ef\u8a13\u7df4\u53c3\u6578\u3002\u7136\u800c\uff0cLoRA \u5c0d\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u9700\u6c42\u6703\u96a8\u8457\u6a21\u578b\u5d4c\u5165\u7dad\u5ea6\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5c0e\u81f4\u9ad8\u904b\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0c\u5176\u53cd\u5411\u66f4\u65b0\u9700\u8981\u5132\u5b58\u9ad8\u7dad\u4e2d\u9593\u6d3b\u5316\u548c\u6700\u4f73\u5316\u5668\u72c0\u614b\uff0c\u9700\u8981\u9ad8\u5cf0\u503c GPU \u8a18\u61b6\u9ad4\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u900f\u904e\u5149\u8b5c\u5206\u89e3\u4f4e\u7dad\u6539\u7de8 (LaMDA) \u9032\u884c\u5927\u578b\u6a21\u578b\u5fae\u8abf\uff0c\u9019\u662f\u4e00\u7a2e\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4f4e\u7dad\u6539\u7de8\u4f86\u5927\u5e45\u6e1b\u5c11\u53ef\u8a13\u7df4\u53c3\u6578\u548c\u5cf0\u503c GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002LaMDA \u51cd\u7d50\u4e86\u6539\u7de8\u8def\u5f91\u4e2d\u7684\u7b2c\u4e00\u500b\u6295\u5f71\u77e9\u9663 (PMA)\uff0c\u540c\u6642\u5f15\u5165\u4e86\u4e00\u500b\u4f4e\u7dad\u53ef\u8a13\u7df4\u65b9\u9663\uff0c\u5f9e\u800c\u5927\u5e45\u6e1b\u5c11\u4e86\u53ef\u8a13\u7df4\u53c3\u6578\u548c\u5cf0\u503c GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002LaMDA \u5728\u5fae\u8abf\u7684\u65e9\u671f\u968e\u6bb5\u9010\u6f38\u51cd\u7d50\u4e86\u7b2c\u4e8c\u500b\u6295\u5f71\u77e9\u9663 (PMB)\uff0c\u6e1b\u5c11\u4e86\u8207\u6b0a\u91cd\u66f4\u65b0\u76f8\u95dc\u7684\u904b\u7b97\u6210\u672c\uff0c\u9032\u4e00\u6b65\u63d0\u9ad8\u4e86\u53c3\u6578\u6548\u7387\u3002\u6211\u5011\u9084\u63d0\u51fa\u4e86\u4e00\u9805\u589e\u5f37\u529f\u80fd LaMDA++\uff0c\u5b83\u900f\u904e\u9810\u8a13\u7df4\u6a21\u578b\u6b0a\u91cd\u7684\u898f\u7bc4\u5316\u5149\u8b5c\u5206\u6790\uff0c\u70ba LoRA \u8def\u5f91\u7d0d\u5165\u4e86\u300c\u8f15\u91cf\u7d1a\u300d\u81ea\u9069\u61c9\u79e9\u5206\u914d\u3002\u6211\u5011\u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u8a55\u4f30\u4e86 LaMDA/LaMDA++\uff0c\u5305\u62ec\u4f7f\u7528 GLUE \u57fa\u6e96\u7684\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u3001\u6587\u5b57\u6458\u8981\u3001\u81ea\u7136\u8a9e\u8a00\u751f\u6210\u4ee5\u53ca\u5c0d\u4e0d\u540c LLM \u9032\u884c\u7684\u8907\u96dc\u63a8\u7406\u3002\u7d50\u679c\u986f\u793a\uff0cLaMDA \u5728\u9700\u8981\u8f03\u5c11\u591a\u9054 17.7 \u500d\u7684\u53c3\u6578\u66f4\u65b0\u548c\u5728\u5fae\u8abf\u671f\u9593\u964d\u4f4e\u591a\u9054 1.32 \u500d\u7684\u5cf0\u503c GPU \u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u7684\u540c\u6642\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u73fe\u6709\u66ff\u4ee3\u65b9\u6848\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u5c07\u516c\u958b\u63d0\u4f9b\u3002", "author": "Seyedarmin Azizi et.al.", "authors": "Seyedarmin Azizi, Souvik Kundu, Massoud Pedram", "id": "2406.12832v1", "paper_url": "http://arxiv.org/abs/2406.12832v1", "repo": "https://github.com/arminazizi98/lamda"}}