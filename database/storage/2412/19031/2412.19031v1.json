{"2412.19031": {"publish_time": "2024-12-26", "title": "Repository Structure-Aware Training Makes SLMs Better Issue Resolver", "paper_summary": "Language models have been applied to various software development tasks, but\nthe performance varies according to the scale of the models. Large Language\nModels (LLMs) outperform Small Language Models (SLMs) in complex tasks like\nrepository-level issue resolving, but raise concerns about privacy and cost. In\ncontrast, SLMs are more accessible but under-perform in complex tasks. In this\npaper, we introduce ReSAT (Repository Structure-Aware Training), construct\ntraining data based on a large number of issues and corresponding pull requests\nfrom open-source communities to enhance the model's understanding of repository\nstructure and issue resolving ability. We construct two types of training data:\n(1) localization training data, a multi-level progressive localization data to\nimprove code understanding and localization capability; (2) code edit training\ndata, which improves context-based code editing capability. The evaluation\nresults on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively\nenhances SLMs' issue-resolving and repository-level long-context understanding\ncapabilities.", "paper_summary_zh": "\u8a9e\u8a00\u6a21\u578b\u5df2\u61c9\u7528\u65bc\u5404\u7a2e\u8edf\u9ad4\u958b\u767c\u4efb\u52d9\uff0c\u4f46\u6548\u80fd\u6703\u6839\u64da\u6a21\u578b\u7684\u898f\u6a21\u800c\u6709\u6240\u4e0d\u540c\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8907\u96dc\u4efb\u52d9\u4e2d\u512a\u65bc\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM)\uff0c\u4f8b\u5982\u5132\u5b58\u5eab\u5c64\u7d1a\u554f\u984c\u89e3\u6c7a\uff0c\u4f46\u5f15\u767c\u4e86\u5c0d\u96b1\u79c1\u548c\u6210\u672c\u7684\u64d4\u6182\u3002\u76f8\u53cd\u5730\uff0cSLM \u66f4\u5bb9\u6613\u53d6\u5f97\uff0c\u4f46\u5728\u8907\u96dc\u4efb\u52d9\u4e2d\u7684\u8868\u73fe\u8f03\u5dee\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 ReSAT\uff08\u5132\u5b58\u5eab\u7d50\u69cb\u611f\u77e5\u8a13\u7df4\uff09\uff0c\u6839\u64da\u5927\u91cf\u554f\u984c\u548c\u4f86\u81ea\u958b\u6e90\u793e\u7fa4\u7684\u5c0d\u61c9\u62c9\u53d6\u8acb\u6c42\u5efa\u69cb\u8a13\u7df4\u8cc7\u6599\uff0c\u4ee5\u589e\u5f37\u6a21\u578b\u5c0d\u5132\u5b58\u5eab\u7d50\u69cb\u548c\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u7684\u7406\u89e3\u3002\u6211\u5011\u5efa\u69cb\u4e86\u5169\u7a2e\u985e\u578b\u7684\u8a13\u7df4\u8cc7\u6599\uff1a(1) \u672c\u5730\u5316\u8a13\u7df4\u8cc7\u6599\uff0c\u4e00\u7a2e\u591a\u5c64\u7d1a\u6f38\u9032\u5f0f\u672c\u5730\u5316\u8cc7\u6599\uff0c\u4ee5\u6539\u5584\u7a0b\u5f0f\u78bc\u7406\u89e3\u548c\u672c\u5730\u5316\u80fd\u529b\uff1b(2) \u7a0b\u5f0f\u78bc\u7de8\u8f2f\u8a13\u7df4\u8cc7\u6599\uff0c\u53ef\u6539\u5584\u57fa\u65bc\u5167\u5bb9\u7684\u7a0b\u5f0f\u78bc\u7de8\u8f2f\u80fd\u529b\u3002\u5728 SWE-Bench \u9a57\u8b49\u548c RepoQA \u4e0a\u7684\u8a55\u4f30\u7d50\u679c\u8b49\u660e\uff0cReSAT \u6709\u6548\u5730\u589e\u5f37\u4e86 SLM \u7684\u554f\u984c\u89e3\u6c7a\u548c\u5132\u5b58\u5eab\u5c64\u7d1a\u9577\u5167\u5bb9\u7406\u89e3\u80fd\u529b\u3002", "author": "Zexiong Ma et.al.", "authors": "Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Bing Xie", "id": "2412.19031v1", "paper_url": "http://arxiv.org/abs/2412.19031v1", "repo": "null"}}