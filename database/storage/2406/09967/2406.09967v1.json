{"2406.09967": {"publish_time": "2024-06-14", "title": "Bag of Lies: Robustness in Continuous Pre-training BERT", "paper_summary": "This study aims to acquire more insights into the continuous pre-training\nphase of BERT regarding entity knowledge, using the COVID-19 pandemic as a case\nstudy. Since the pandemic emerged after the last update of BERT's pre-training\ndata, the model has little to no entity knowledge about COVID-19. Using\ncontinuous pre-training, we control what entity knowledge is available to the\nmodel. We compare the baseline BERT model with the further pre-trained variants\non the fact-checking benchmark Check-COVID. To test the robustness of\ncontinuous pre-training, we experiment with several adversarial methods to\nmanipulate the input data, such as training on misinformation and shuffling the\nword order until the input becomes nonsensical. Surprisingly, our findings\nreveal that these methods do not degrade, and sometimes even improve, the\nmodel's downstream performance. This suggests that continuous pre-training of\nBERT is robust against misinformation. Furthermore, we are releasing a new\ndataset, consisting of original texts from academic publications in the\nLitCovid repository and their AI-generated false counterparts.", "paper_summary_zh": "\u672c\u7814\u7a76\u65e8\u5728\u900f\u904e\u4ee5 COVID-19 \u75ab\u60c5\u70ba\u6848\u4f8b\u7814\u7a76\uff0c\u9032\u4e00\u6b65\u4e86\u89e3 BERT \u6301\u7e8c\u9810\u8a13\u7df4\u968e\u6bb5\u4e2d\u95dc\u65bc\u5be6\u9ad4\u77e5\u8b58\u7684\u898b\u89e3\u3002\u7531\u65bc\u75ab\u60c5\u5728 BERT \u9810\u8a13\u7df4\u8cc7\u6599\u7684\u6700\u5f8c\u4e00\u6b21\u66f4\u65b0\u5f8c\u624d\u51fa\u73fe\uff0c\u56e0\u6b64\u8a72\u6a21\u578b\u5c0d\u65bc COVID-19 \u7684\u5be6\u9ad4\u77e5\u8b58\u5e7e\u4e4e\u6c92\u6709\u6216\u5b8c\u5168\u6c92\u6709\u3002\u900f\u904e\u6301\u7e8c\u9810\u8a13\u7df4\uff0c\u6211\u5011\u63a7\u5236\u6a21\u578b\u53ef\u53d6\u5f97\u7684\u5be6\u9ad4\u77e5\u8b58\u3002\u6211\u5011\u5728\u4e8b\u5be6\u67e5\u6838\u57fa\u6e96 Check-COVID \u4e0a\u6bd4\u8f03\u57fa\u6e96 BERT \u6a21\u578b\u8207\u9032\u4e00\u6b65\u9810\u8a13\u7df4\u7684\u8b8a\u9ad4\u3002\u70ba\u4e86\u6e2c\u8a66\u6301\u7e8c\u9810\u8a13\u7df4\u7684\u7a69\u5065\u6027\uff0c\u6211\u5011\u5617\u8a66\u4f7f\u7528\u591a\u7a2e\u5c0d\u6297\u65b9\u6cd5\u4f86\u64cd\u7e31\u8f38\u5165\u8cc7\u6599\uff0c\u4f8b\u5982\u91dd\u5c0d\u932f\u8aa4\u8cc7\u8a0a\u9032\u884c\u8a13\u7df4\uff0c\u4ee5\u53ca\u5c07\u5b57\u8a5e\u9806\u5e8f\u6253\u4e82\uff0c\u76f4\u5230\u8f38\u5165\u8b8a\u5f97\u6beb\u7121\u610f\u7fa9\u70ba\u6b62\u3002\u4ee4\u4eba\u9a5a\u8a1d\u7684\u662f\uff0c\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u9019\u4e9b\u65b9\u6cd5\u4e26\u4e0d\u6703\u964d\u4f4e\u6a21\u578b\u7684\u4e0b\u6e38\u6548\u80fd\uff0c\u6709\u6642\u751a\u81f3\u6703\u63d0\u5347\u6548\u80fd\u3002\u9019\u8868\u793a BERT \u7684\u6301\u7e8c\u9810\u8a13\u7df4\u5c0d\u65bc\u932f\u8aa4\u8cc7\u8a0a\u5177\u6709\u7a69\u5065\u6027\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u5e03\u4e86\u4e00\u500b\u65b0\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b LitCovid \u5b58\u653e\u5eab\u4e2d\u5b78\u8853\u51fa\u7248\u54c1\u7684\u539f\u59cb\u6587\u5b57\u53ca\u5176 AI \u751f\u6210\u7684\u932f\u8aa4\u5c0d\u61c9\u6587\u5b57\u3002", "author": "Ine Gevers et.al.", "authors": "Ine Gevers, Walter Daelemans", "id": "2406.09967v1", "paper_url": "http://arxiv.org/abs/2406.09967v1", "repo": "null"}}