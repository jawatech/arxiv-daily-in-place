{"2407.17060": {"publish_time": "2024-07-24", "title": "High Efficiency Image Compression for Large Visual-Language Models", "paper_summary": "In recent years, large visual language models (LVLMs) have shown impressive\nperformance and promising generalization capability in multi-modal tasks, thus\nreplacing humans as receivers of visual information in various application\nscenarios. In this paper, we pioneer to propose a variable bitrate image\ncompression framework consisting of a pre-editing module and an end-to-end\ncodec to achieve promising rate-accuracy performance for different LVLMs. In\nparticular, instead of optimizing an adaptive pre-editing network towards a\nparticular task or several representative tasks, we propose a new optimization\nstrategy tailored for LVLMs, which is designed based on the representation and\ndiscrimination capability with token-level distortion and rank. The pre-editing\nmodule and the variable bitrate end-to-end image codec are jointly trained by\nthe losses based on semantic tokens of the large model, which introduce\nenhanced generalization capability for various data and tasks. {Experimental\nresults demonstrate that the proposed framework could efficiently achieve much\nbetter rate-accuracy performance compared to the state-of-the-art coding\nstandard, Versatile Video Coding.} Meanwhile, experiments with multi-modal\ntasks have revealed the robustness and generalization capability of the\nproposed framework.", "paper_summary_zh": "\u8fd1\u5e74\u4f86\uff0c\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5728\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u80fd\u548c\u6709\u524d\u666f\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u5728\u5404\u7a2e\u61c9\u7528\u5834\u666f\u4e2d\u53d6\u4ee3\u4eba\u985e\u6210\u70ba\u8996\u89ba\u8cc7\u8a0a\u7684\u63a5\u6536\u8005\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u7387\u5148\u63d0\u51fa\u4e00\u500b\u53ef\u8b8a\u4f4d\u5143\u7387\u5f71\u50cf\u58d3\u7e2e\u67b6\u69cb\uff0c\u7531\u4e00\u500b\u9810\u7de8\u8f2f\u6a21\u7d44\u548c\u4e00\u500b\u7aef\u5c0d\u7aef\u7de8\u89e3\u78bc\u5668\u7d44\u6210\uff0c\u4ee5\u9054\u6210\u4e0d\u540c LVLMs \u7684\u6709\u524d\u666f\u901f\u7387\u6e96\u78ba\u5ea6\u6548\u80fd\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u6c92\u6709\u91dd\u5c0d\u7279\u5b9a\u4efb\u52d9\u6216\u5e7e\u500b\u4ee3\u8868\u6027\u4efb\u52d9\u4f86\u6700\u4f73\u5316\u81ea\u9069\u61c9\u9810\u7de8\u8f2f\u7db2\u8def\uff0c\u800c\u662f\u63d0\u51fa\u4e00\u500b\u5c08\u70ba LVLMs \u91cf\u8eab\u6253\u9020\u7684\u65b0\u6700\u4f73\u5316\u7b56\u7565\uff0c\u5176\u8a2d\u8a08\u57fa\u65bc\u5177\u6709\u4ee3\u5e63\u5c64\u7d1a\u5931\u771f\u548c\u7b49\u7d1a\u7684\u8868\u793a\u548c\u8fa8\u5225\u80fd\u529b\u3002\u9810\u7de8\u8f2f\u6a21\u7d44\u548c\u53ef\u8b8a\u4f4d\u5143\u7387\u7aef\u5c0d\u7aef\u5f71\u50cf\u7de8\u89e3\u78bc\u5668\u7531\u5927\u578b\u6a21\u578b\u7684\u8a9e\u610f\u4ee3\u5e63\u6240\u6839\u64da\u7684\u640d\u5931\u806f\u5408\u8a13\u7df4\uff0c\u9019\u70ba\u5404\u7a2e\u8cc7\u6599\u548c\u4efb\u52d9\u5f15\u5165\u4e86\u589e\u5f37\u7684\u6cdb\u5316\u80fd\u529b\u3002{\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u8207\u6700\u5148\u9032\u7684\u7de8\u78bc\u6a19\u6e96\u591a\u529f\u80fd\u8996\u8a0a\u7de8\u78bc\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u53ef\u4ee5\u6709\u6548\u5730\u9054\u6210\u66f4\u597d\u7684\u901f\u7387\u6e96\u78ba\u5ea6\u6548\u80fd\u3002}\u540c\u6642\uff0c\u591a\u6a21\u614b\u4efb\u52d9\u7684\u5be6\u9a57\u63ed\u793a\u4e86\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u7684\u7a69\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "author": "Binzhe Li et.al.", "authors": "Binzhe Li, Shurun Wang, Shiqi Wang, Yan Ye", "id": "2407.17060v1", "paper_url": "http://arxiv.org/abs/2407.17060v1", "repo": "null"}}