
### Medical
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-09-13**|**SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**|Paloma Rabaey et.al.|[2409.08936v1](http://arxiv.org/abs/2409.08936v1)|[link](https://github.com/prabaey/synsum)|
|**2024-09-13**|**A BERT-Based Summarization approach for depression detection**|Hossein Salahshoor Gavalan et.al.|[2409.08483v1](http://arxiv.org/abs/2409.08483v1)|null|
|**2024-09-12**|**Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**|Fuchen Zheng et.al.|[2409.07793v1](http://arxiv.org/abs/2409.07793v1)|[link](https://github.com/lzeeorno/lagrange-duality-and-cmaformer)|
|**2024-09-12**|**ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**|Fuchen Zheng et.al.|[2409.07779v1](http://arxiv.org/abs/2409.07779v1)|[link](https://github.com/lzeeorno/assnet)|
|**2024-09-11**|**SoK: Security and Privacy Risks of Medical AI**|Yuanhaur Chang et.al.|[2409.07415v1](http://arxiv.org/abs/2409.07415v1)|null|
|**2024-09-11**|**Federated Impression for Learning with Distributed Heterogeneous Data**|Sana Ayromlou et.al.|[2409.07351v1](http://arxiv.org/abs/2409.07351v1)|null|
|**2024-09-11**|**MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**|Praveen K Kanithi et.al.|[2409.07314v1](http://arxiv.org/abs/2409.07314v1)|null|
|**2024-09-11**|**Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**|Sheng Chen et.al.|[2409.07186v1](http://arxiv.org/abs/2409.07186v1)|null|
|**2024-09-11**|**CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**|Feiyang Jia et.al.|[2409.07092v1](http://arxiv.org/abs/2409.07092v1)|null|
|**2024-09-11**|**Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**|Daeun Kyung et.al.|[2409.07012v1](http://arxiv.org/abs/2409.07012v1)|null|
|**2024-09-11**|**Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**|Jianmei Jiang et.al.|[2409.06928v1](http://arxiv.org/abs/2409.06928v1)|[link](https://github.com/jjm1589/dstct)|
|**2024-09-10**|**Bifurcation Identification for Ultrasound-driven Robotic Cannulation**|Cecilia G. Morales et.al.|[2409.06817v1](http://arxiv.org/abs/2409.06817v1)|null|
|**2024-09-10**|**Personalized Federated Learning Techniques: Empirical Analysis**|Azal Ahmad Khan et.al.|[2409.06805v1](http://arxiv.org/abs/2409.06805v1)|null|
|**2024-09-10**|**Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**|Cristian Trout et.al.|[2409.06672v1](http://arxiv.org/abs/2409.06672v1)|null|
|**2024-09-10**|**EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**|Danli Shi et.al.|[2409.06644v2](http://arxiv.org/abs/2409.06644v2)|null|
|**2024-09-10**|**Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**|Zoe Hancox et.al.|[2409.06585v1](http://arxiv.org/abs/2409.06585v1)|[link](https://github.com/zoehancox/sparse_tgcnn)|
|**2024-09-10**|**MAGDA: Multi-agent guideline-driven diagnostic assistance**|David Bani-Harouni et.al.|[2409.06351v1](http://arxiv.org/abs/2409.06351v1)|null|
|**2024-09-10**|**Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**|Xin Zhang et.al.|[2409.06209v1](http://arxiv.org/abs/2409.06209v1)|[link](https://github.com/xinz0419/unisurv)|
|**2024-09-10**|**Can Large Language Models Unlock Novel Scientific Research Ideas?**|Sandeep Kumar et.al.|[2409.06185v1](http://arxiv.org/abs/2409.06185v1)|null|
|**2024-09-10**|**Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**|Georgios Chochlakis et.al.|[2409.06173v1](http://arxiv.org/abs/2409.06173v1)|[link](https://github.com/gchochla/cot-priors)|
|**2024-09-10**|**Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**|Dong Han et.al.|[2409.06147v1](http://arxiv.org/abs/2409.06147v1)|null|
|**2024-09-09**|**ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**|Zhaoyue Sun et.al.|[2409.05592v1](http://arxiv.org/abs/2409.05592v1)|null|
|**2024-09-09**|**Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**|Camilo Thorne et.al.|[2409.05486v1](http://arxiv.org/abs/2409.05486v1)|null|
|**2024-09-09**|**KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**|Yingshu Li et.al.|[2409.05370v1](http://arxiv.org/abs/2409.05370v1)|null|
|**2024-09-09**|**Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**|Javad Hassannataj Joloudari et.al.|[2409.07493v1](http://arxiv.org/abs/2409.07493v1)|null|
|**2024-09-09**|**Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**|Nirmalya Thakur et.al.|[2409.05292v2](http://arxiv.org/abs/2409.05292v2)|null|
|**2024-09-09**|**RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**|Quoc-Bao Nguyen-Le et.al.|[2409.05280v1](http://arxiv.org/abs/2409.05280v1)|[link](https://github.com/kyle-paul/RotCAtt-TransUNet-plusplus)|
|**2024-09-07**|**Activation Function Optimization Scheme for Image Classification**|Abdur Rahman et.al.|[2409.04915v1](http://arxiv.org/abs/2409.04915v1)|[link](https://github.com/abdurrahman1828/afos)|
|**2024-09-07**|**LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**|Yongxin Deng et.al.|[2409.04744v1](http://arxiv.org/abs/2409.04744v1)|null|
|**2024-09-07**|**NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**|Debaditya Shome et.al.|[2409.04723v1](http://arxiv.org/abs/2409.04723v1)|null|
|**2024-09-07**|**A Comprehensive Survey on Evidential Deep Learning and Its Applications**|Junyu Gao et.al.|[2409.04720v1](http://arxiv.org/abs/2409.04720v1)|[link](https://github.com/mengyuanchen21/awesome-evidential-deep-learning)|
|**2024-09-07**|**A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**|Cheng Wan et.al.|[2409.04704v1](http://arxiv.org/abs/2409.04704v1)|null|
|**2024-09-06**|**The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**|Gregory Szumel et.al.|[2409.04368v1](http://arxiv.org/abs/2409.04368v1)|null|
|**2024-09-06**|**CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**|William Knottenbelt et.al.|[2409.04290v1](http://arxiv.org/abs/2409.04290v1)|[link](https://github.com/knottwill/CoxKAN)|
|**2024-09-06**|**Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**|Daniel J. Tan et.al.|[2409.04224v1](http://arxiv.org/abs/2409.04224v1)|null|
|**2024-09-06**|**Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**|Yizhen Zheng et.al.|[2409.04481v1](http://arxiv.org/abs/2409.04481v1)|null|
|**2024-09-06**|**FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**|Kai Shu et.al.|[2409.03947v1](http://arxiv.org/abs/2409.03947v1)|null|
|**2024-09-05**|**A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**|Esther Lagemann et.al.|[2409.03933v1](http://arxiv.org/abs/2409.03933v1)|null|
|**2024-09-05**|**Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**|Yucong Zhang et.al.|[2409.03597v1](http://arxiv.org/abs/2409.03597v1)|null|
|**2024-09-05**|**Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**|Prerak Mody et.al.|[2409.03470v1](http://arxiv.org/abs/2409.03470v1)|[link](https://github.com/prerakmody/bayesuncertainty-error-correspondence)|
|**2024-09-05**|**Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**|Francisco de Arriba-Pérez et.al.|[2409.03375v1](http://arxiv.org/abs/2409.03375v1)|null|
|**2024-09-05**|**Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**|Juan A. Berrios Moya et.al.|[2409.03147v1](http://arxiv.org/abs/2409.03147v1)|null|
|**2024-09-04**|**MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**|Shehan Perera et.al.|[2409.03062v1](http://arxiv.org/abs/2409.03062v1)|[link](https://github.com/osupcvlab/mobileunetr)|
|**2024-09-04**|**Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**|Junyoung Park et.al.|[2409.02883v1](http://arxiv.org/abs/2409.02883v1)|null|
|**2024-09-04**|**Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**|Ramon Tavares et.al.|[2409.02681v1](http://arxiv.org/abs/2409.02681v1)|null|
|**2024-09-04**|**SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**|Wenwu Guo et.al.|[2409.02598v1](http://arxiv.org/abs/2409.02598v1)|[link](https://github.com/wenwucode/surgtrack)|
|**2024-09-04**|**Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**|Chih-Yuan Li et.al.|[2409.02530v1](http://arxiv.org/abs/2409.02530v1)|null|
|**2024-09-03**|**Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**|Deepak Raina et.al.|[2409.02337v1](http://arxiv.org/abs/2409.02337v1)|null|
|**2024-09-03**|**Action-Based ADHD Diagnosis in Video**|Yichun Li et.al.|[2409.02261v1](http://arxiv.org/abs/2409.02261v1)|null|
|**2024-09-03**|**A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**|Anna L. Trella et.al.|[2409.02069v1](http://arxiv.org/abs/2409.02069v1)|[link](https://github.com/StatisticalReinforcementLearningLab/oralytics-post-deployment-analysis)|
|**2024-09-03**|**TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**|Bobby Azad et.al.|[2409.02018v1](http://arxiv.org/abs/2409.02018v1)|null|
|**2024-09-03**|**A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**|Abdelmalek Mouazer et.al.|[2409.01903v1](http://arxiv.org/abs/2409.01903v1)|null|
|**2024-09-03**|**Training on the Benchmark Is Not All You Need**|Shiwen Ni et.al.|[2409.01790v1](http://arxiv.org/abs/2409.01790v1)|[link](https://github.com/nishiwen1214/Benchmark-leakage-detection)|
|**2024-09-03**|**Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**|Wenyang Hu et.al.|[2409.01676v1](http://arxiv.org/abs/2409.01676v1)|null|
|**2024-09-03**|**A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**|Zekang Yang et.al.|[2409.02145v1](http://arxiv.org/abs/2409.02145v1)|[link](https://github.com/yang-ze-kang/MOC)|
|**2024-09-03**|**A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**|Ruben D. Fonnegra et.al.|[2409.01596v1](http://arxiv.org/abs/2409.01596v1)|null|
|**2024-09-02**|**Kvasir-VQA: A Text-Image Pair GI Tract Dataset**|Sushant Gautam et.al.|[2409.01437v1](http://arxiv.org/abs/2409.01437v1)|[link](https://github.com/simula/Kvasir-VQA)|
|**2024-09-02**|**EEG-Language Modeling for Pathology Detection**|Sam Gijsen et.al.|[2409.07480v1](http://arxiv.org/abs/2409.07480v1)|null|
|**2024-09-02**|**SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**|Mevan Ekanayake et.al.|[2409.01013v1](http://arxiv.org/abs/2409.01013v1)|null|
|**2024-09-01**|**Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**|Sajib Acharjee Dip et.al.|[2409.00873v1](http://arxiv.org/abs/2409.00873v1)|null|
|**2024-09-01**|**Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**|Derian Boer et.al.|[2409.00861v1](http://arxiv.org/abs/2409.00861v1)|[link](https://github.com/kramerlab/4StepFocus)|
|**2024-09-01**|**Building FKG.in: a Knowledge Graph for Indian Food**|Saransh Kumar Gupta et.al.|[2409.00830v1](http://arxiv.org/abs/2409.00830v1)|null|
|**2024-09-01**|**AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**|Mahsa Khosravi et.al.|[2409.00735v1](http://arxiv.org/abs/2409.00735v1)|[link](https://github.com/scslabisu/aggym)|
|**2024-09-01**|**LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**|Zhaojie Fang et.al.|[2409.00726v1](http://arxiv.org/abs/2409.00726v1)|[link](https://github.com/Tinysqua/LPUWF-LDM)|
|**2024-09-01**|**BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**|Shams Nafisa Ali et.al.|[2409.00724v1](http://arxiv.org/abs/2409.00724v1)|[link](https://github.com/sani002/HS-Dataset)|
|**2024-09-01**|**Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**|Pragya Gupta et.al.|[2409.00718v1](http://arxiv.org/abs/2409.00718v1)|null|
|**2024-09-01**|**Curriculum Prompting Foundation Models for Medical Image Segmentation**|Xiuqi Zheng et.al.|[2409.00695v1](http://arxiv.org/abs/2409.00695v1)|[link](https://github.com/annazzz-zxq/curriculum-prompting)|
|**2024-08-31**|**Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**|Jacqueline Lammert et.al.|[2409.00544v1](http://arxiv.org/abs/2409.00544v1)|[link](https://github.com/LammertJ/RGT-Digital-Twin)|
|**2024-08-31**|**Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**|Georgios Ioannides et.al.|[2409.00391v1](http://arxiv.org/abs/2409.00391v1)|null|
|**2024-08-31**|**Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**|Mikhail Borisenkov et.al.|[2409.00310v1](http://arxiv.org/abs/2409.00310v1)|null|
|**2024-08-30**|**Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**|Antonio Rago et.al.|[2408.17401v1](http://arxiv.org/abs/2408.17401v1)|null|
|**2024-08-30**|**Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**|Yuhan Zheng et.al.|[2409.00163v1](http://arxiv.org/abs/2409.00163v1)|null|
|**2024-08-30**|**NDP: Next Distribution Prediction as a More Broad Target**|Junhao Ruan et.al.|[2408.17377v1](http://arxiv.org/abs/2408.17377v1)|null|
|**2024-08-30**|**Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**|Jutika Borah et.al.|[2408.17011v2](http://arxiv.org/abs/2408.17011v2)|null|
|**2024-08-29**|**A Survey for Large Language Models in Biomedicine**|Chong Wang et.al.|[2409.00133v1](http://arxiv.org/abs/2409.00133v1)|null|
|**2024-08-29**|**Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**|Yifei Chen et.al.|[2408.16343v1](http://arxiv.org/abs/2408.16343v1)|[link](https://github.com/justlfc03/mstnet)|
|**2024-08-29**|**Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**|Xia Jiang et.al.|[2408.16256v1](http://arxiv.org/abs/2408.16256v1)|null|
|**2024-08-29**|**M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**|Jonggwon Park et.al.|[2408.16213v1](http://arxiv.org/abs/2408.16213v1)|null|
|**2024-08-28**|**A Survey on Evaluation of Multimodal Large Language Models**|Jiaxing Huang et.al.|[2408.15769v1](http://arxiv.org/abs/2408.15769v1)|null|
|**2024-08-28**|**Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**|Yijun Zhou et.al.|[2408.15498v1](http://arxiv.org/abs/2408.15498v1)|null|
|**2024-08-27**|**What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**|Jana Schaich Borg et.al.|[2408.15354v1](http://arxiv.org/abs/2408.15354v1)|null|
|**2024-08-27**|**Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**|Weiyi Zhang et.al.|[2408.15217v1](http://arxiv.org/abs/2408.15217v1)|null|
|**2024-08-27**|**Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**|Daniil Filienko et.al.|[2409.00112v1](http://arxiv.org/abs/2409.00112v1)|null|
|**2024-08-27**|**Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**|Francesco Sovrano et.al.|[2408.15121v1](http://arxiv.org/abs/2408.15121v1)|null|
|**2024-08-27**|**MiWaves Reinforcement Learning Algorithm**|Susobhan Ghosh et.al.|[2408.15076v1](http://arxiv.org/abs/2408.15076v1)|[link](https://github.com/statisticalreinforcementlearninglab/miwaves_rl_service)|
|**2024-08-27**|**Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**|Yuqi Zhang et.al.|[2408.15032v1](http://arxiv.org/abs/2408.15032v1)|[link](https://github.com/yuqizhang-buaa/mamba2mil)|
|**2024-08-27**|**Sequence-aware Pre-training for Echocardiography Probe Guidance**|Haojun Jiang et.al.|[2408.15026v1](http://arxiv.org/abs/2408.15026v1)|null|
|**2024-08-27**|**Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**|Christos Theodoropoulos et.al.|[2408.15294v2](http://arxiv.org/abs/2408.15294v2)|null|
|**2024-08-27**|**Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**|Qiaoxin Li et.al.|[2408.14754v1](http://arxiv.org/abs/2408.14754v1)|null|
|**2024-08-27**|**Large Language Models for Disease Diagnosis: A Scoping Review**|Shuang Zhou et.al.|[2409.00097v1](http://arxiv.org/abs/2409.00097v1)|null|
|**2024-08-26**|**Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI**|Yukyeong Song et.al.|[2409.06723v1](http://arxiv.org/abs/2409.06723v1)|null|
|**2024-08-26**|**Improving Clinical Note Generation from Complex Doctor-Patient Conversation**|Yizhan Li et.al.|[2408.14568v1](http://arxiv.org/abs/2408.14568v1)|null|
|**2024-08-26**|**Temporal Ensemble Logic**|Guo-Qiang Zhang et.al.|[2408.14443v2](http://arxiv.org/abs/2408.14443v2)|null|
|**2024-08-26**|**MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**|Kuluhan Binici et.al.|[2408.14418v2](http://arxiv.org/abs/2408.14418v2)|null|
|**2024-08-26**|**Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**|Xiaoman Zhang et.al.|[2408.14397v1](http://arxiv.org/abs/2408.14397v1)|[link](https://github.com/rajpurkarlab/rexkg)|
|**2024-08-26**|**Foundation Models for Music: A Survey**|Yinghao Ma et.al.|[2408.14340v3](http://arxiv.org/abs/2408.14340v3)|[link](https://github.com/nicolaus625/fm4music)|
|**2024-08-26**|**Uncertainties of Latent Representations in Computer Vision**|Michael Kirchhof et.al.|[2408.14281v1](http://arxiv.org/abs/2408.14281v1)|null|
|**2024-08-26**|**Automatic Medical Report Generation: Methods and Applications**|Li Guo et.al.|[2408.13988v1](http://arxiv.org/abs/2408.13988v1)|null|
|**2024-08-25**|**Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**|Seyed Amir Ahmad Safavi-Naini et.al.|[2409.00084v2](http://arxiv.org/abs/2409.00084v2)|[link](https://github.com/sdamirsa/llm-vlm-in-gastroenterology)|
|**2024-08-25**|**PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**|Zifan Chen et.al.|[2408.13836v1](http://arxiv.org/abs/2408.13836v1)|null|

#### Abstracts
##### **SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records**
2409.08936v1 by Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester

We present the SynSUM benchmark, a synthetic dataset linking unstructured
clinical notes to structured background variables. The dataset consists of
10,000 artificial patient records containing tabular variables (like symptoms,
diagnoses and underlying conditions) and related notes describing the fictional
patient encounter in the domain of respiratory diseases. The tabular portion of
the data is generated through a Bayesian network, where both the causal
structure between the variables and the conditional probabilities are proposed
by an expert based on domain knowledge. We then prompt a large language model
(GPT-4o) to generate a clinical note related to this patient encounter,
describing the patient symptoms and additional context. The SynSUM dataset is
primarily designed to facilitate research on clinical information extraction in
the presence of tabular background variables, which can be linked through
domain knowledge to concepts of interest to be extracted from the text - the
symptoms, in the case of SynSUM. Secondary uses include research on the
automation of clinical reasoning over both tabular data and text, causal effect
estimation in the presence of tabular and/or textual confounders, and
multi-modal synthetic data generation. The dataset can be downloaded from
https://github.com/prabaey/SynSUM.

摘要：我們提出 SynSUM 基準，一個將非結構化臨床記錄連結到結構化背景變數的合成資料集。該資料集包含 10,000 個人工病歷，其中包含表格變數（例如症狀、診斷和潛在狀況）和相關記錄，描述了呼吸系統疾病領域中的虛構患者遭遇。資料的表格部分是透過貝氏網路產生的，其中變數之間的因果結構和條件機率都是由專家根據領域知識提出的。然後，我們提示一個大型語言模型 (GPT-4o) 產生與此患者遭遇相關的臨床記錄，描述患者症狀和額外背景。SynSUM 資料集主要是為了促進在表格背景變數存在的情況下進行臨床資訊萃取的研究，這些變數可以透過領域知識連結到從文本中萃取的目標概念 - 在 SynSUM 的案例中，是症狀。次要用途包括研究表格資料和文本的臨床推理自動化、在表格和/或文本混淆因子存在的情況下進行因果效應估計，以及多模式合成資料生成。此資料集可從 https://github.com/prabaey/SynSUM 下載。

##### **A BERT-Based Summarization approach for depression detection**
2409.08483v1 by Hossein Salahshoor Gavalan, Mohmmad Naim Rastgoo, Bahareh Nakisa

Depression is a globally prevalent mental disorder with potentially severe
repercussions if not addressed, especially in individuals with recurrent
episodes. Prior research has shown that early intervention has the potential to
mitigate or alleviate symptoms of depression. However, implementing such
interventions in a real-world setting may pose considerable challenges. A
promising strategy involves leveraging machine learning and artificial
intelligence to autonomously detect depression indicators from diverse data
sources. One of the most widely available and informative data sources is text,
which can reveal a person's mood, thoughts, and feelings. In this context,
virtual agents programmed to conduct interviews using clinically validated
questionnaires, such as those found in the DAIC-WOZ dataset, offer a robust
means for depression detection through linguistic analysis. Utilizing
BERT-based models, which are powerful and versatile yet use fewer resources
than contemporary large language models, to convert text into numerical
representations significantly enhances the precision of depression diagnosis.
These models adeptly capture complex semantic and syntactic nuances, improving
the detection accuracy of depressive symptoms. Given the inherent limitations
of these models concerning text length, our study proposes text summarization
as a preprocessing technique to diminish the length and intricacies of input
texts. Implementing this method within our uniquely developed framework for
feature extraction and classification yielded an F1-score of 0.67 on the test
set surpassing all prior benchmarks and 0.81 on the validation set exceeding
most previous results on the DAIC-WOZ dataset. Furthermore, we have devised a
depression lexicon to assess summary quality and relevance. This lexicon
constitutes a valuable asset for ongoing research in depression detection.

摘要：憂鬱症是一種全球普遍存在的心理疾病，如果不加以解決，可能會造成嚴重的後果，尤其是對有復發性發作的人。先前的研究表明，早期介入有可能減輕或緩解憂鬱症狀。然而，在現實環境中實施此類干預措施可能會帶來相當大的挑戰。一個有前途的策略包括利用機器學習和人工智慧，從不同的數據來源中自動檢測憂鬱症指標。最廣泛可用且最有資訊的數據來源之一是文字，它可以揭示一個人的情緒、想法和感受。在此脈絡中，使用臨床驗證問卷（例如在 DAIC-WOZ 資料集中找到的問卷）編寫程式進行訪談的虛擬代理人，提供了一種透過語言分析進行憂鬱症檢測的強大方法。利用 BERT 基礎模型（功能強大且用途廣泛，但使用的資源比當代大型語言模型少）將文字轉換為數值表示，可顯著提高憂鬱症診斷的準確性。這些模型巧妙地捕捉複雜的語義和句法細微差別，提高憂鬱症狀的檢測準確性。鑑於這些模型在文字長度方面存在固有缺陷，我們的研究提出文字摘要作為預處理技術，以減少輸入文字的長度和複雜性。在我們獨自開發的功能提取和分類框架中實施此方法，在測試集中產生 0.67 的 F1 分數，超越所有先前的基準，在驗證集中產生 0.81 的 F1 分數，超過 DAIC-WOZ 資料集上大多數先前的結果。此外，我們設計了一個憂鬱症詞彙表，用於評估摘要品質和相關性。此詞彙表構成憂鬱症檢測持續研究的寶貴資產。

##### **Lagrange Duality and Compound Multi-Attention Transformer for Semi-Supervised Medical Image Segmentation**
2409.07793v1 by Fuchen Zheng, Quanjun Li, Weixuan Li, Xuhang Chen, Yihang Dong, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a critical application of semantic segmentation
in healthcare, has seen significant advancements through specialized computer
vision techniques. While deep learning-based medical image segmentation is
essential for assisting in medical diagnosis, the lack of diverse training data
causes the long-tail problem. Moreover, most previous hybrid CNN-ViT
architectures have limited ability to combine various attentions in different
layers of the Convolutional Neural Network. To address these issues, we propose
a Lagrange Duality Consistency (LDC) Loss, integrated with Boundary-Aware
Contrastive Loss, as the overall training objective for semi-supervised
learning to mitigate the long-tail problem. Additionally, we introduce
CMAformer, a novel network that synergizes the strengths of ResUNet and
Transformer. The cross-attention block in CMAformer effectively integrates
spatial attention and channel attention for multi-scale feature fusion.
Overall, our results indicate that CMAformer, combined with the feature fusion
framework and the new consistency loss, demonstrates strong complementarity in
semi-supervised learning ensembles. We achieve state-of-the-art results on
multiple public medical image datasets. Example code are available at:
\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}.

摘要：醫學影像分割是語意分割在醫療保健領域中的一項重要應用，已透過專業的電腦視覺技術獲得顯著進展。雖然基於深度學習的醫學影像分割對於協助醫療診斷至關重要，但缺乏多樣化的訓練資料會導致長尾問題。此外，大多數先前的混合式 CNN-ViT 架構在結合卷積神經網路不同層中的各種注意力方面能力有限。為了解決這些問題，我們提出了一種拉格朗日對偶一致性 (LDC) 損失，並與邊界感知對比損失整合，作為半監督式學習的整體訓練目標，以減輕長尾問題。此外，我們介紹了 CMAformer，這是一個新穎的網路，它協同了 ResUNet 和 Transformer 的優點。CMAformer 中的交叉注意力區塊有效地整合了空間注意力和通道注意力，以進行多尺度特徵融合。總的來說，我們的結果表明，CMAformer 結合特徵融合架構和新的稠密損失，在半監督式學習集合中展現出強大的互補性。我們在多個公開醫學影像資料集上取得了最先進的成果。範例程式碼可在以下網址取得：\url{https://github.com/lzeeorno/Lagrange-Duality-and-CMAformer}。

##### **ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation**
2409.07779v1 by Fuchen Zheng, Xinyi Chen, Xuhang Chen, Haolun Li, Xiaojiao Guo, Guoheng Huang, Chi-Man Pun, Shoujun Zhou

Medical image segmentation, a crucial task in computer vision, facilitates
the automated delineation of anatomical structures and pathologies, supporting
clinicians in diagnosis, treatment planning, and disease monitoring. Notably,
transformers employing shifted window-based self-attention have demonstrated
exceptional performance. However, their reliance on local window attention
limits the fusion of local and global contextual information, crucial for
segmenting microtumors and miniature organs. To address this limitation, we
propose the Adaptive Semantic Segmentation Network (ASSNet), a transformer
architecture that effectively integrates local and global features for precise
medical image segmentation. ASSNet comprises a transformer-based U-shaped
encoder-decoder network. The encoder utilizes shifted window self-attention
across five resolutions to extract multi-scale features, which are then
propagated to the decoder through skip connections. We introduce an augmented
multi-layer perceptron within the encoder to explicitly model long-range
dependencies during feature extraction. Recognizing the constraints of
conventional symmetrical encoder-decoder designs, we propose an Adaptive
Feature Fusion (AFF) decoder to complement our encoder. This decoder
incorporates three key components: the Long Range Dependencies (LRD) block, the
Multi-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)
block. These components synergistically facilitate the effective fusion of
multi-scale features extracted by the decoder while capturing long-range
dependencies and refining object boundaries. Comprehensive experiments on
diverse medical image segmentation tasks, including multi-organ, liver tumor,
and bladder tumor segmentation, demonstrate that ASSNet achieves
state-of-the-art results. Code and models are available at:
\url{https://github.com/lzeeorno/ASSNet}.

摘要：<paragraph>醫學影像分割是電腦視覺中一項重要的任務，有助於自動描繪解剖結構和病理，協助臨床醫師進行診斷、治療計畫和疾病監控。值得注意的是，採用位移視窗自注意力機制的Transformer展現出非凡的效能。然而，它們依賴於區域視窗注意力，這限制了區域和全域脈絡資訊的融合，而這對於分割微小腫瘤和微型器官至關重要。為了解決這個限制，我們提出了自適應語義分割網路 (ASSNet)，這是一個Transformer架構，可以有效整合區域和全域特徵，以進行精確的醫學影像分割。ASSNet 包含一個基於Transformer的 U 型編碼器-解碼器網路。編碼器利用五個解析度的位移視窗自注意力來萃取多尺度特徵，然後透過跳躍連線將這些特徵傳播到解碼器。我們在編碼器中引入了擴增的多層感知器，以便在特徵萃取期間明確地建模長程依賴性。鑑於傳統對稱編碼器-解碼器設計的限制，我們提出了一個自適應特徵融合 (AFF) 解碼器來補充我們的編碼器。此解碼器包含三個關鍵組成部分：長程依賴性 (LRD) 區塊、多尺度特徵融合 (MFF) 區塊和自適應語義中心 (ASC) 區塊。這些組成部分相互配合，促成解碼器萃取的多尺度特徵有效融合，同時捕捉長程依賴性並微調物件邊界。在多器官、肝臟腫瘤和膀胱腫瘤分割等各種醫學影像分割任務上的全面實驗證明，ASSNet 達到了最先進的成果。程式碼和模型可於以下網址取得：\url{https://github.com/lzeeorno/ASSNet}。</paragraph>

##### **SoK: Security and Privacy Risks of Medical AI**
2409.07415v1 by Yuanhaur Chang, Han Liu, Evin Jaff, Chenyang Lu, Ning Zhang

The integration of technology and healthcare has ushered in a new era where
software systems, powered by artificial intelligence and machine learning, have
become essential components of medical products and services. While these
advancements hold great promise for enhancing patient care and healthcare
delivery efficiency, they also expose sensitive medical data and system
integrity to potential cyberattacks. This paper explores the security and
privacy threats posed by AI/ML applications in healthcare. Through a thorough
examination of existing research across a range of medical domains, we have
identified significant gaps in understanding the adversarial attacks targeting
medical AI systems. By outlining specific adversarial threat models for medical
settings and identifying vulnerable application domains, we lay the groundwork
for future research that investigates the security and resilience of AI-driven
medical systems. Through our analysis of different threat models and
feasibility studies on adversarial attacks in different medical domains, we
provide compelling insights into the pressing need for cybersecurity research
in the rapidly evolving field of AI healthcare technology.

摘要：科技與醫療的整合開啟了一個新紀元，由人工智慧和機器學習驅動的軟體系統已成為醫療產品和服務的必要組成部分。雖然這些進步對改善患者照護和醫療保健提供效率有很大的幫助，但它們也讓敏感的醫療資料和系統完整性面臨潛在的網路攻擊風險。本文探討了人工智慧/機器學習應用在醫療保健中帶來的安全性和隱私威脅。透過徹底檢視各項醫療領域現有的研究，我們發現了在了解針對醫療人工智慧系統的對抗性攻擊方面有顯著的差距。透過概述醫療環境的特定對抗性威脅模型並找出容易受攻擊的應用領域，我們為未來研究奠定基礎，探討人工智慧驅動醫療系統的安全性與復原力。透過分析不同的威脅模型和針對不同醫療領域的對抗性攻擊可行性研究，我們對人工智慧醫療保健技術快速發展領域中網路安全研究的迫切需求提供了令人信服的見解。

##### **Federated Impression for Learning with Distributed Heterogeneous Data**
2409.07351v1 by Sana Ayromlou, Atrin Arya, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li

Standard deep learning-based classification approaches may not always be
practical in real-world clinical applications, as they require a centralized
collection of all samples. Federated learning (FL) provides a paradigm that can
learn from distributed datasets across clients without requiring them to share
data, which can help mitigate privacy and data ownership issues. In FL,
sub-optimal convergence caused by data heterogeneity is common among data from
different health centers due to the variety in data collection protocols and
patient demographics across centers. Through experimentation in this study, we
show that data heterogeneity leads to the phenomenon of catastrophic forgetting
during local training. We propose FedImpres which alleviates catastrophic
forgetting by restoring synthetic data that represents the global information
as federated impression. To achieve this, we distill the global model resulting
from each communication round. Subsequently, we use the synthetic data
alongside the local data to enhance the generalization of local training.
Extensive experiments show that the proposed method achieves state-of-the-art
performance on both the BloodMNIST and Retina datasets, which contain label
imbalance and domain shift, with an improvement in classification accuracy of
up to 20%.

摘要：標準的深度學習分類方法在實際的臨床應用中可能並不總是實用的，因為它們需要集中收集所有樣本。聯邦學習 (FL) 提供了一個範例，可以在不讓客戶端分享數據的情況下從分布式數據集學習，這有助於減輕隱私和數據所有權問題。在 FL 中，由於不同醫療中心的數據收集協定和患者人口統計資料的差異，來自不同醫療中心的數據之間常見的數據異質性會導致次最佳收斂。透過本研究中的實驗，我們表明數據異質性會導致局部訓練期間發生災難性遺忘現象。我們提出 FedImpres，它透過還原表示全球資訊的合成資料作為聯邦印象來減輕災難性遺忘。為此，我們提煉出每一輪通訊所產生的全球模型。隨後，我們使用合成資料和本地資料來增強本地訓練的概括性。廣泛的實驗表明，所提出的方法在 BloodMNIST 和 Retina 數據集上都達到了最先進的效能，這些數據集包含標籤不平衡和領域轉移，分類準確度提高了 20%。

##### **MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications**
2409.07314v1 by Praveen K Kanithi, Clément Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan

The rapid development of Large Language Models (LLMs) for healthcare
applications has spurred calls for holistic evaluation beyond frequently-cited
benchmarks like USMLE, to better reflect real-world performance. While
real-world assessments are valuable indicators of utility, they often lag
behind the pace of LLM evolution, likely rendering findings obsolete upon
deployment. This temporal disconnect necessitates a comprehensive upfront
evaluation that can guide model selection for specific clinical applications.
We introduce MEDIC, a framework assessing LLMs across five critical dimensions
of clinical competence: medical reasoning, ethics and bias, data and language
understanding, in-context learning, and clinical safety. MEDIC features a novel
cross-examination framework quantifying LLM performance across areas like
coverage and hallucination detection, without requiring reference outputs. We
apply MEDIC to evaluate LLMs on medical question-answering, safety,
summarization, note generation, and other tasks. Our results show performance
disparities across model sizes, baseline vs medically finetuned models, and
have implications on model selection for applications requiring specific model
strengths, such as low hallucination or lower cost of inference. MEDIC's
multifaceted evaluation reveals these performance trade-offs, bridging the gap
between theoretical capabilities and practical implementation in healthcare
settings, ensuring that the most promising models are identified and adapted
for diverse healthcare applications.

摘要：大型語言模型 (LLM) 在醫療保健應用方面的快速發展，促使人們呼籲進行整體評估，超越經常引用的基準（例如 USMLE），以更好地反映實際效能。儘管實際評估是實用性的寶貴指標，但它們通常落後於 LLM 演化的速度，在部署後可能會使研究結果過時。這種時間上的脫節需要進行全面的前期評估，以指導特定臨床應用程式的模型選擇。我們引進 MEDIC，一個評估 LLM 跨越臨床能力的五個關鍵面向的架構：醫療推理、倫理和偏差、資料和語言理解、情境學習和臨床安全性。MEDIC 採用一種新穎的交互式檢查架構，量化 LLM 在涵蓋範圍和幻覺偵測等領域的效能，而不需要參考輸出。我們使用 MEDIC 來評估 LLM 在醫療問題解答、安全性、摘要、筆記產生和其他任務上的表現。我們的結果顯示，不同模型大小、基準與經過醫療微調的模型之間的效能差異，並對需要特定模型優勢的應用程式（例如低幻覺或較低的推論成本）的模型選擇產生影響。MEDIC 的多面向評估揭示了這些效能權衡，縮小了理論能力與醫療保健環境中的實際實作之間的差距，確保找出最有希望的模型，並針對不同的醫療保健應用程式進行調整。

##### **Enhancing Angular Resolution via Directionality Encoding and Geometric Constraints in Brain Diffusion Tensor Imaging**
2409.07186v1 by Sheng Chen, Zihao Tang, Mariano Cabezas, Xinyi Wang, Arkiev D'Souza, Michael Barnett, Fernando Calamante, Weidong Cai, Chenyu Wang

Diffusion-weighted imaging (DWI) is a type of Magnetic Resonance Imaging
(MRI) technique sensitised to the diffusivity of water molecules, offering the
capability to inspect tissue microstructures and is the only in-vivo method to
reconstruct white matter fiber tracts non-invasively. The DWI signal can be
analysed with the diffusion tensor imaging (DTI) model to estimate the
directionality of water diffusion within voxels. Several scalar metrics,
including axial diffusivity (AD), mean diffusivity (MD), radial diffusivity
(RD), and fractional anisotropy (FA), can be further derived from DTI to
quantitatively summarise the microstructural integrity of brain tissue. These
scalar metrics have played an important role in understanding the organisation
and health of brain tissue at a microscopic level in clinical studies. However,
reliable DTI metrics rely on DWI acquisitions with high gradient directions,
which often go beyond the commonly used clinical protocols. To enhance the
utility of clinically acquired DWI and save scanning time for robust DTI
analysis, this work proposes DirGeo-DTI, a deep learning-based method to
estimate reliable DTI metrics even from a set of DWIs acquired with the minimum
theoretical number (6) of gradient directions. DirGeo-DTI leverages directional
encoding and geometric constraints to facilitate the training process. Two
public DWI datasets were used for evaluation, demonstrating the effectiveness
of the proposed method. Extensive experimental results show that the proposed
method achieves the best performance compared to existing DTI enhancement
methods and potentially reveals further clinical insights with routine clinical
DWI scans.

摘要：擴散加權影像（DWI）是一種磁振造影（MRI）技術，對水分子擴散敏感，能檢測組織微結構，是唯一非侵入性重建白質纖維束的體內方法。DWI 訊號可用擴散張量影像（DTI）模型分析，以估計體素內水擴散的方向性。從 DTI 可進一步衍生出數個標量量測，包括軸向擴散率（AD）、平均擴散率（MD）、徑向擴散率（RD）和分數各向異性（FA），以量化總結腦組織的微結構完整性。這些標量量測在臨床研究中對於了解腦組織在微觀層面的組織和健康扮演重要角色。然而，可靠的 DTI 量測仰賴具有高梯度方向的 DWI 擷取，這通常超出臨床上常用的協定。為了提升臨床擷取 DWI 的效用，並為穩健的 DTI 分析節省掃描時間，本研究提出 DirGeo-DTI，一種基於深度學習的方法，即使從具備最小理論數量（6）的梯度方向擷取的 DWI 組也能估計可靠的 DTI 量測。DirGeo-DTI 利用方向編碼和幾何約束來促進訓練過程。使用兩個公開的 DWI 資料集進行評估，證明了所提出方法的有效性。廣泛的實驗結果顯示，與現有的 DTI 增強方法相比，所提出的方法取得最佳效能，並潛在揭露例行臨床 DWI 掃描的進一步臨床見解。

##### **CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer**
2409.07092v1 by Feiyang Jia, Zhineng Chen, Ziying Song, Lin Liu, Caiyan Jia

Super-resolution (SR) aims to enhance the quality of low-resolution images
and has been widely applied in medical imaging. We found that the design
principles of most existing methods are influenced by SR tasks based on
real-world images and do not take into account the significance of the
multi-level structure in pathological images, even if they can achieve
respectable objective metric evaluations. In this work, we delve into two
super-resolution working paradigms and propose a novel network called CWT-Net,
which leverages cross-scale image wavelet transform and Transformer
architecture. Our network consists of two branches: one dedicated to learning
super-resolution and the other to high-frequency wavelet features. To generate
high-resolution histopathology images, the Transformer module shares and fuses
features from both branches at various stages. Notably, we have designed a
specialized wavelet reconstruction module to effectively enhance the wavelet
domain features and enable the network to operate in different modes, allowing
for the introduction of additional relevant information from cross-scale
images. Our experimental results demonstrate that our model significantly
outperforms state-of-the-art methods in both performance and visualization
evaluations and can substantially boost the accuracy of image diagnostic
networks.

摘要：超解析度 (SR) 旨在提升低解析度影像的品質，並已廣泛應用於醫學影像。我們發現現有方法的大部分設計原則都受到基於真實影像的 SR 任務影響，而且即使它們能達到可觀的客觀指標評估，也不會考慮病理影像中多層級結構的重要性。在這項工作中，我們深入探討兩種超解析度工作範例，並提出一個名為 CWT-Net 的新型網路，它利用跨尺度影像小波轉換和 Transformer 架構。我們的網路包含兩個分支：一個專門用於學習超解析度，另一個則用於高頻小波特徵。為了產生高解析度組織病理學影像，Transformer 模組會在不同階段分享和融合來自兩個分支的特徵。值得注意的是，我們設計了一個專門的小波重建模組，以有效增強小波域特徵，並讓網路能夠在不同模式下運作，允許從跨尺度影像中引入其他相關資訊。我們的實驗結果證明，我們的模型在效能和視覺化評估方面都大幅優於現有技術，而且能大幅提升影像診斷網路的準確度。

##### **Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records**
2409.07012v1 by Daeun Kyung, Junu Kim, Tackeun Kim, Edward Choi

Chest X-ray imaging (CXR) is an important diagnostic tool used in hospitals
to assess patient conditions and monitor changes over time. Generative models,
specifically diffusion-based models, have shown promise in generating realistic
synthetic X-rays. However, these models mainly focus on conditional generation
using single-time-point data, i.e., typically CXRs taken at a specific time
with their corresponding reports, limiting their clinical utility, particularly
for capturing temporal changes. To address this limitation, we propose a novel
framework, EHRXDiff, which predicts future CXR images by integrating previous
CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc.
Our framework dynamically tracks and predicts disease progression based on a
latent diffusion model, conditioned on the previous CXR image and a history of
medical events. We comprehensively evaluate the performance of our framework
across three key aspects, including clinical consistency, demographic
consistency, and visual realism. We demonstrate that our framework generates
high-quality, realistic future images that capture potential temporal changes,
suggesting its potential for further development as a clinical simulation tool.
This could offer valuable insights for patient monitoring and treatment
planning in the medical field.

摘要：胸部 X 光影像（CXR）是一種重要的診斷工具，用於醫院評估病患狀況並監控其隨著時間的變化。生成模型，特別是基於擴散的模型，已在生成逼真的合成 X 光影像方面展現出潛力。然而，這些模型主要專注於使用單一時間點資料進行條件生成，即通常在特定時間點拍攝的 CXR 及其對應報告，這限制了其臨床效用，特別是對於捕捉時間變化。為了解決此限制，我們提出了一個新的框架 EHRXDiff，它透過整合先前的 CXR 與後續的醫療事件（例如處方、實驗室檢測等）來預測未來的 CXR 影像。我們的框架基於潛在擴散模型動態追蹤並預測疾病進展，條件取決於先前的 CXR 影像和醫療事件的歷史記錄。我們全面評估了我們框架在三個關鍵方面的效能，包括臨床一致性、人口統計一致性和視覺逼真度。我們證明我們的框架生成了高品質、逼真的未來影像，捕捉了潛在的時間變化，這表明其進一步發展為臨床模擬工具的潛力。這可以為醫療領域的病患監控和治療規劃提供有價值的見解。

##### **Intrapartum Ultrasound Image Segmentation of Pubic Symphysis and Fetal Head Using Dual Student-Teacher Framework with CNN-ViT Collaborative Learning**
2409.06928v1 by Jianmei Jiang, Huijin Wang, Jieyun Bai, Shun Long, Shuangping Chen, Victor M. Campello, Karim Lekadir

The segmentation of the pubic symphysis and fetal head (PSFH) constitutes a
pivotal step in monitoring labor progression and identifying potential delivery
complications. Despite the advances in deep learning, the lack of annotated
medical images hinders the training of segmentation. Traditional
semi-supervised learning approaches primarily utilize a unified network model
based on Convolutional Neural Networks (CNNs) and apply consistency
regularization to mitigate the reliance on extensive annotated data. However,
these methods often fall short in capturing the discriminative features of
unlabeled data and in delineating the long-range dependencies inherent in the
ambiguous boundaries of PSFH within ultrasound images. To address these
limitations, we introduce a novel framework, the Dual-Student and Teacher
Combining CNN and Transformer (DSTCT), which synergistically integrates the
capabilities of CNNs and Transformers. Our framework comprises a Vision
Transformer (ViT) as the teacher and two student mod ls one ViT and one CNN.
This dual-student setup enables mutual supervision through the generation of
both hard and soft pseudo-labels, with the consistency in their predictions
being refined by minimizing the classifier determinacy discrepancy. The teacher
model further reinforces learning within this architecture through the
imposition of consistency regularization constraints. To augment the
generalization abilities of our approach, we employ a blend of data and model
perturbation techniques. Comprehensive evaluations on the benchmark dataset of
the PSFH Segmentation Grand Challenge at MICCAI 2023 demonstrate our DSTCT
framework outperformed ten contemporary semi-supervised segmentation methods.
Code available at https://github.com/jjm1589/DSTCT.

摘要：恥骨聯合和胎頭（PSFH）的分割是監測產程進度和識別潛在分娩併發症的關鍵步驟。儘管深度學習取得進展，但標註醫學影像的缺乏阻礙了分割的訓練。傳統的半監督式學習方法主要利用基於卷積神經網路（CNN）的統一網路模型，並應用一致性正則化來減輕對大量標註數據的依賴。然而，這些方法通常無法捕捉未標註數據的區別性特徵，也無法描繪超音波影像中 PSFH 模糊邊界中固有的長程依賴性。為了解決這些限制，我們引入了一個新的框架，即雙學生和教師結合 CNN 和 Transformer（DSTCT），它協同整合了 CNN 和 Transformer 的功能。我們的框架包含一個視覺 Transformer（ViT）作為教師和兩個學生模型，一個 ViT 和一個 CNN。這種雙學生設置通過生成硬偽標籤和軟偽標籤實現相互監督，並通過最小化分類器確定性差異來優化其預測的一致性。教師模型通過施加一致性正則化約束進一步加強了此架構中的學習。為了增強我們方法的泛化能力，我們採用了數據和模型擾動技術的混合。在 MICCAI 2023 的 PSFH 分割大挑戰基準數據集上的綜合評估表明，我們的 DSTCT 框架優於十種當代半監督式分割方法。程式碼可在 https://github.com/jjm1589/DSTCT 取得。

##### **Bifurcation Identification for Ultrasound-driven Robotic Cannulation**
2409.06817v1 by Cecilia G. Morales, Dhruv Srikanth, Jack H. Good, Keith A. Dufendach, Artur Dubrawski

In trauma and critical care settings, rapid and precise intravascular access
is key to patients' survival. Our research aims at ensuring this access, even
when skilled medical personnel are not readily available. Vessel bifurcations
are anatomical landmarks that can guide the safe placement of catheters or
needles during medical procedures. Although ultrasound is advantageous in
navigating anatomical landmarks in emergency scenarios due to its portability
and safety, to our knowledge no existing algorithm can autonomously extract
vessel bifurcations using ultrasound images. This is primarily due to the
limited availability of ground truth data, in particular, data from live
subjects, needed for training and validating reliable models. Researchers often
resort to using data from anatomical phantoms or simulations. We introduce
BIFURC, Bifurcation Identification for Ultrasound-driven Robot Cannulation, a
novel algorithm that identifies vessel bifurcations and provides optimal needle
insertion sites for an autonomous robotic cannulation system. BIFURC integrates
expert knowledge with deep learning techniques to efficiently detect vessel
bifurcations within the femoral region and can be trained on a limited amount
of in-vivo data. We evaluated our algorithm using a medical phantom as well as
real-world experiments involving live pigs. In all cases, BIFURC consistently
identified bifurcation points and needle insertion locations in alignment with
those identified by expert clinicians.

摘要：在創傷和重症照護環境中，快速且精確的血管內通路是患者存活的關鍵。我們的研究旨在確保這種通路，即使在熟練的醫療人員無法立即獲得的情況下。血管分叉是解剖標誌，可以指導在醫療過程中安全放置導管或針頭。儘管超音波由於其可攜性和安全性而在緊急情況下導航解剖標誌具有優勢，但據我們所知，沒有現有演算法可以使用超音波影像自動提取血管分叉。這主要是由於地面實況資料的可用性有限，特別是來自活體受試者的資料，而這對於訓練和驗證可靠模型是必需的。研究人員經常求助於使用解剖模型或模擬的資料。我們引入了 BIFURC，即超音波驅動機器人插管的分叉識別，這是一種新穎的演算法，可以識別血管分叉，並為自動機器人插管系統提供最佳針頭插入位置。BIFURC 將專家知識與深度學習技術相結合，以有效檢測股骨區域內的血管分叉，並且可以在有限的體內資料上進行訓練。我們使用醫用模型以及涉及活體豬的真實世界實驗評估了我們的演算法。在所有情況下，BIFURC 都一致地識別出分叉點和針頭插入位置，與專家臨床醫生識別的位置一致。

##### **Personalized Federated Learning Techniques: Empirical Analysis**
2409.06805v1 by Azal Ahmad Khan, Ahmad Faraz Khan, Haider Ali, Ali Anwar

Personalized Federated Learning (pFL) holds immense promise for tailoring
machine learning models to individual users while preserving data privacy.
However, achieving optimal performance in pFL often requires a careful
balancing act between memory overhead costs and model accuracy. This paper
delves into the trade-offs inherent in pFL, offering valuable insights for
selecting the right algorithms for diverse real-world scenarios. We empirically
evaluate ten prominent pFL techniques across various datasets and data splits,
uncovering significant differences in their performance. Our study reveals
interesting insights into how pFL methods that utilize personalized (local)
aggregation exhibit the fastest convergence due to their efficiency in
communication and computation. Conversely, fine-tuning methods face limitations
in handling data heterogeneity and potential adversarial attacks while
multi-objective learning methods achieve higher accuracy at the cost of
additional training and resource consumption. Our study emphasizes the critical
role of communication efficiency in scaling pFL, demonstrating how it can
significantly affect resource usage in real-world deployments.

摘要：個人化聯合學習 (pFL) 在維護資料隱私的同時，為客製化機器學習模型給個別使用者帶來極大的希望。然而，要達成 pFL 的最佳效能，通常需要在記憶體開銷成本和模型準確度之間取得仔細的平衡。本文深入探討 pFL 中固有的權衡取捨，為在各種實際場景中選擇正確的演算法提供寶貴的見解。我們根據各種資料集和資料分割，對十種傑出的 pFL 技術進行實證評估，揭露其效能的顯著差異。我們的研究揭露了有趣的見解，說明利用個人化 (局部) 聚合的 pFL 方法，由於其在通訊和運算方面的效率，展現出最快的收斂速度。相反地，微調方法在處理資料異質性和潛在對抗攻擊方面面臨限制，而多目標學習方法則以額外的訓練和資源消耗為代價，達到更高的準確度。我們的研究強調了通訊效率在擴充 pFL 中的關鍵角色，展示它如何在實際部署中顯著影響資源使用。

##### **Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort**
2409.06672v1 by Cristian Trout

Many experts believe that AI systems will sooner or later pose uninsurable
risks, including existential risks. This creates an extreme judgment-proof
problem: few if any parties can be held accountable ex post in the event of
such a catastrophe. This paper proposes a novel solution: a
government-provided, mandatory indemnification program for AI developers. The
program uses risk-priced indemnity fees to induce socially optimal levels of
care. Risk-estimates are determined by surveying experts, including indemnified
developers. The Bayesian Truth Serum mechanism is employed to incent honest and
effortful responses. Compared to alternatives, this approach arguably better
leverages all private information, and provides a clearer signal to indemnified
developers regarding what risks they must mitigate to lower their fees. It's
recommended that collected fees be used to help fund the safety research
developers need, employing a fund matching mechanism (Quadratic Financing) to
induce an optimal supply of this public good. Under Quadratic Financing, safety
research projects would compete for private contributions from developers,
signaling how much each is to be supplemented with public funds.

摘要：許多專家相信 AI 系統遲早會造成無法承保的風險，包括生存風險。這會造成極端的無法追究責任問題：在發生此類災難時，幾乎沒有任何一方可以事後被追究責任。本文提出了一個創新的解決方案：政府提供的 AI 開發人員強制性補償計畫。該計畫使用風險定價的補償費用來誘使達到社會最適程度的照護。風險估計值是由調查專家（包括獲得補償的開發人員）來決定。貝氏真話血清機制被用來激勵誠實且努力的回應。與其他方法相比，這種方法可以說能更好地利用所有私人資訊，並向獲得補償的開發人員提供更明確的訊號，說明他們必須減輕哪些風險才能降低費用。建議將收取的費用用於資助安全研究開發人員所需的研究，並採用基金配對機制（二次方融資）來誘使提供這種公共財的最佳供應。在二次方融資下，安全研究計畫將競爭開發人員的私人捐款，並表示其中有多少將由公共資金補充。

##### **EyeCLIP: A visual-language foundation model for multi-modal ophthalmic image analysis**
2409.06644v2 by Danli Shi, Weiyi Zhang, Jiancheng Yang, Siyu Huang, Xiaolan Chen, Mayinuer Yusufu, Kai Jin, Shan Lin, Shunming Liu, Qing Zhang, Mingguang He

Early detection of eye diseases like glaucoma, macular degeneration, and
diabetic retinopathy is crucial for preventing vision loss. While artificial
intelligence (AI) foundation models hold significant promise for addressing
these challenges, existing ophthalmic foundation models primarily focus on a
single modality, whereas diagnosing eye diseases requires multiple modalities.
A critical yet often overlooked aspect is harnessing the multi-view information
across various modalities for the same patient. Additionally, due to the
long-tail nature of ophthalmic diseases, standard fully supervised or
unsupervised learning approaches often struggle. Therefore, it is essential to
integrate clinical text to capture a broader spectrum of diseases. We propose
EyeCLIP, a visual-language foundation model developed using over 2.77 million
multi-modal ophthalmology images with partial text data. To fully leverage the
large multi-modal unlabeled and labeled data, we introduced a pretraining
strategy that combines self-supervised reconstructions, multi-modal image
contrastive learning, and image-text contrastive learning to learn a shared
representation of multiple modalities. Through evaluation using 14 benchmark
datasets, EyeCLIP can be transferred to a wide range of downstream tasks
involving ocular and systemic diseases, achieving state-of-the-art performance
in disease classification, visual question answering, and cross-modal
retrieval. EyeCLIP represents a significant advancement over previous methods,
especially showcasing few-shot, even zero-shot capabilities in real-world
long-tail scenarios.

摘要：早期偵測青光眼、黃斑部病變和糖尿病視網膜病變等眼疾對於預防視力喪失至關重要。儘管人工智慧 (AI) 基礎模型在應對這些挑戰方面極具前景，但現有的眼科基礎模型主要關注於單一模式，而診斷眼疾需要多種模式。一個重要但經常被忽視的方面是利用同一患者不同模式的多視圖資訊。此外，由於眼科疾病的長尾性質，標準的全監督或無監督學習方法通常難以應付。因此，整合臨床文本以涵蓋更廣泛的疾病譜系至關重要。我們提出 EyeCLIP，這是一個視覺語言基礎模型，使用超過 277 萬張具有部分文字資料的多模式眼科影像開發而成。為了充分利用大量的多模式未標記和標記資料，我們引入了一種預訓練策略，結合了自我監督重建、多模式影像對比學習和影像文字對比學習，以學習多種模式的共享表徵。透過使用 14 個基準資料集進行評估，EyeCLIP 可以轉移到涉及眼部和全身疾病的廣泛下游任務，在疾病分類、視覺問題解答和跨模式檢索中實現最先進的效能。EyeCLIP 代表了對先前方法的重大進展，特別是在現實世界長尾場景中展示了小樣本，甚至零樣本的能力。

##### **Developing the Temporal Graph Convolutional Neural Network Model to Predict Hip Replacement using Electronic Health Records**
2409.06585v1 by Zoe Hancox, Sarah R. Kingsbury, Andrew Clegg, Philip G. Conaghan, Samuel D. Relton

Background: Hip replacement procedures improve patient lives by relieving
pain and restoring mobility. Predicting hip replacement in advance could reduce
pain by enabling timely interventions, prioritising individuals for surgery or
rehabilitation, and utilising physiotherapy to potentially delay the need for
joint replacement. This study predicts hip replacement a year in advance to
enhance quality of life and health service efficiency. Methods: Adapting
previous work using Temporal Graph Convolutional Neural Network (TG-CNN)
models, we construct temporal graphs from primary care medical event codes,
sourced from ResearchOne EHRs of 40-75-year-old patients, to predict hip
replacement risk. We match hip replacement cases to controls by age, sex, and
Index of Multiple Deprivation. The model, trained on 9,187 cases and 9,187
controls, predicts hip replacement one year in advance. We validate the model
on two unseen datasets, recalibrating for class imbalance. Additionally, we
conduct an ablation study and compare against four baseline models. Results:
Our best model predicts hip replacement risk one year in advance with an AUROC
of 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209),
achieving a calibration slope of 1.107 (95% CI: 1.074-1.139) after
recalibration. Conclusions: The TG-CNN model effectively predicts hip
replacement risk by identifying patterns in patient trajectories, potentially
improving understanding and management of hip-related conditions.

摘要：背景：髖關節置換手術可減輕疼痛並恢復行動能力，進而改善患者生活。預測髖關節置換手術有助於及時介入、優先安排個人進行手術或復健，並利用物理治療來延緩關節置換手術的必要性，進而減少疼痛。本研究預測一年後的髖關節置換手術，以提升生活品質和醫療服務效率。方法：採用時間圖形卷積神經網路 (TG-CNN) 模型改編先前的研究，我們從 ResearchOne EHR 40-75 歲患者的主要照護醫療事件代碼建構時間圖形，以預測髖關節置換手術風險。我們根據年齡、性別和多重剝奪指數，將髖關節置換手術病例與對照組進行配對。該模型針對 9,187 個病例和 9,187 個對照組進行訓練，預測一年後的髖關節置換手術。我們在兩個未見數據集驗證模型，並重新校準以解決類別不平衡問題。此外，我們進行消融研究，並與四個基準模型進行比較。結果：我們最佳的模型預測一年後的髖關節置換手術風險，AUROC 為 0.724 (95% CI：0.715-0.733)，AUPRC 為 0.185 (95% CI：0.160-0.209)，重新校準後校準斜率為 1.107 (95% CI：1.074-1.139)。結論：TG-CNN 模型可有效預測髖關節置換手術風險，方法是找出患者軌跡中的模式，進而潛在改善對髖關節相關疾病的了解和管理。

##### **MAGDA: Multi-agent guideline-driven diagnostic assistance**
2409.06351v1 by David Bani-Harouni, Nassir Navab, Matthias Keicher

In emergency departments, rural hospitals, or clinics in less developed
regions, clinicians often lack fast image analysis by trained radiologists,
which can have a detrimental effect on patients' healthcare. Large Language
Models (LLMs) have the potential to alleviate some pressure from these
clinicians by providing insights that can help them in their decision-making.
While these LLMs achieve high test results on medical exams showcasing their
great theoretical medical knowledge, they tend not to follow medical
guidelines. In this work, we introduce a new approach for zero-shot
guideline-driven decision support. We model a system of multiple LLM agents
augmented with a contrastive vision-language model that collaborate to reach a
patient diagnosis. After providing the agents with simple diagnostic
guidelines, they will synthesize prompts and screen the image for findings
following these guidelines. Finally, they provide understandable
chain-of-thought reasoning for their diagnosis, which is then self-refined to
consider inter-dependencies between diseases. As our method is zero-shot, it is
adaptable to settings with rare diseases, where training data is limited, but
expert-crafted disease descriptions are available. We evaluate our method on
two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing
performance improvement over existing zero-shot methods and generalizability to
rare diseases.

摘要：在急診室、鄉村醫院或欠發達地區的診所，臨床醫師常常缺乏受過訓練的放射科醫師進行快速的影像分析，這可能會對病患的醫療保健造成不利影響。大型語言模型 (LLM) 有潛力減輕這些臨床醫師的一些壓力，方法是提供見解，協助他們進行決策。儘管這些 LLM 在展示其豐富的理論醫學知識的醫學考試中獲得了很高的測試結果，但它們往往不遵循醫療指南。在這項工作中，我們介紹了一種新的零次學習指導方針驅動決策支援方法。我們模擬了一個多個 LLM 代理系統，並增強了一個對比視覺語言模型，該模型協作以達成病患診斷。在為代理提供簡單的診斷指南後，它們將綜合提示並根據這些指南篩選影像以找出發現。最後，它們為其診斷提供可以理解的思路推理，然後自我精進以考量疾病之間的相互依存關係。由於我們的模型是零次學習，因此它可以適應罕見疾病的設定，在這種設定中，訓練資料有限，但有專家製作的疾病描述可用。我們在兩個胸部 X 光片資料集，CheXpert 和 ChestX-ray 14 Longtail，評估我們的模型，展示了相較於現有的零次學習方法的效能提升，以及對罕見疾病的概括性。

##### **Adaptive Transformer Modelling of Density Function for Nonparametric Survival Analysis**
2409.06209v1 by Xin Zhang, Deval Mehta, Yanan Hu, Chao Zhu, David Darby, Zhen Yu, Daniel Merlo, Melissa Gresle, Anneke Van Der Walt, Helmut Butzkueven, Zongyuan Ge

Survival analysis holds a crucial role across diverse disciplines, such as
economics, engineering and healthcare. It empowers researchers to analyze both
time-invariant and time-varying data, encompassing phenomena like customer
churn, material degradation and various medical outcomes. Given the complexity
and heterogeneity of such data, recent endeavors have demonstrated successful
integration of deep learning methodologies to address limitations in
conventional statistical approaches. However, current methods typically involve
cluttered probability distribution function (PDF), have lower sensitivity in
censoring prediction, only model static datasets, or only rely on recurrent
neural networks for dynamic modelling. In this paper, we propose a novel
survival regression method capable of producing high-quality unimodal PDFs
without any prior distribution assumption, by optimizing novel
Margin-Mean-Variance loss and leveraging the flexibility of Transformer to
handle both temporal and non-temporal data, coined UniSurv. Extensive
experiments on several datasets demonstrate that UniSurv places a significantly
higher emphasis on censoring compared to other methods.

摘要：存活分析在經濟、工程和醫療保健等不同學科中扮演著至關重要的角色。它讓研究人員能夠分析時不變和時變數據，包含客戶流失、材料降解和各種醫療結果等現象。鑑於此類數據的複雜性和異質性，最近的努力已證明成功整合深度學習方法以解決傳統統計方法的限制。然而，目前的方法通常涉及雜亂的機率分佈函數 (PDF)，在審查預測中具有較低的敏感性，僅對靜態數據集進行建模，或僅依賴遞迴神經網路進行動態建模。在本文中，我們提出了一種新穎的存活迴歸方法，能夠在沒有任何先驗分佈假設的情況下產生高品質的單峰 PDF，藉由最佳化新穎的邊際平均值變異損失，並利用 Transformer 的靈活性來處理時間和非時間數據，稱為 UniSurv。在幾個數據集上的廣泛實驗證明，與其他方法相比，UniSurv 對審查的重視程度顯著提高。

##### **Can Large Language Models Unlock Novel Scientific Research Ideas?**
2409.06185v1 by Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal

"An idea is nothing more nor less than a new combination of old elements"
(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and
publicly available ChatGPT have marked a significant turning point in the
integration of Artificial Intelligence (AI) into people's everyday lives. This
study explores the capability of LLMs in generating novel research ideas based
on information from research papers. We conduct a thorough examination of 4
LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and
Physics). We found that the future research ideas generated by Claude-2 and
GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.
We also found that Claude-2 generates more diverse future research ideas than
GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the
novelty, relevancy, and feasibility of the generated future research ideas.
This investigation offers insights into the evolving role of LLMs in idea
generation, highlighting both its capability and limitations. Our work
contributes to the ongoing efforts in evaluating and utilizing language models
for generating future research ideas. We make our datasets and codes publicly
available.

摘要：「一個想法不過就是舊元素的新組合而已」
(Young, J.W.)。大型語言模型 (LLM) 和公開的 ChatGPT 廣泛採用，標誌著人工智能 (AI) 整合到人們日常生活中的重要轉折點。本研究探討了 LLM 在根據研究論文資訊產生新研究想法方面的能力。我們對五個領域（例如化學、電腦、經濟、醫學和物理）中的 4 個 LLM 進行了徹底檢查。我們發現 Claude-2 和 GPT-4 產生的未來研究想法比 GPT-3.5 和 Gemini 更符合作者的觀點。我們還發現，Claude-2 產生的未來研究想法比 GPT-4、GPT-3.5 和 Gemini 1.0 更為多樣化。我們進一步對產生的未來研究想法的新穎性、相關性和可行性進行了人工評估。本調查提供了對 LLM 在產生想法中不斷演變的角色的見解，突出了其能力和限制。我們的研究有助於評估和利用語言模型來產生未來研究想法的持續努力。我們公開提供我們的數據集和程式碼。

##### **Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks**
2409.06173v1 by Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan

In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the
dominant technique for performing natural language tasks, as it does not
require updating the model parameters with gradient-based methods. ICL promises
to "adapt" the LLM to perform the present task at a competitive or
state-of-the-art level at a fraction of the computational cost. ICL can be
augmented by incorporating the reasoning process to arrive at the final label
explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.
However, recent work has found that ICL relies mostly on the retrieval of task
priors and less so on "learning" to perform tasks, especially for complex
subjective domains like emotion and morality, where priors ossify posterior
predictions. In this work, we examine whether "enabling" reasoning also creates
the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors
that remain relatively unchanged despite the evidence in the prompt. We find
that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL
for larger language models. Code is avalaible at
https://github.com/gchochla/cot-priors.

摘要：大型語言模型 (LLM) 中的語境學習 (ICL) 已成為執行自然語言任務的主流技術，因為它不需要使用基於梯度的模型參數來更新。ICL 承諾以極低的計算成本「調整」LLM，以在競爭或最先進的層級執行當前任務。ICL 可以透過在提示中明確地納入推理過程來擴充，以得出最終標籤，這項技術稱為思考鏈 (CoT) 提示。然而，最近的研究發現，ICL 主要依賴於任務先驗的檢索，較少依賴於「學習」執行任務，特別是對於情緒和道德等複雜的主觀領域，其中先驗會僵化後驗預測。在這項研究中，我們探討「啟用」推理是否也會在 LLM 中產生相同的行為，其中 CoT 的格式會檢索推理先驗，儘管提示中的證據不同，但這些先驗仍然相對不變。我們發現，令人驚訝的是，對於較大的語言模型，CoT 確實會與 ICL 發生相同的後驗崩潰。程式碼可於 https://github.com/gchochla/cot-priors 取得。

##### **Multiclass Arrhythmia Classification using Smartwatch Photoplethysmography Signals Collected in Real-life Settings**
2409.06147v1 by Dong Han, Jihye Moon, Luís Roberto Mercado Díaz, Darren Chen, Devan Williams, Eric Y. Ding, Khanh-Van Tran, David D. McManus, Ki H. Chon

Most deep learning models of multiclass arrhythmia classification are tested
on fingertip photoplethysmographic (PPG) data, which has higher signal-to-noise
ratios compared to smartwatch-derived PPG, and the best reported sensitivity
value for premature atrial/ventricular contraction (PAC/PVC) detection is only
75%. To improve upon PAC/PVC detection sensitivity while maintaining high AF
detection, we use multi-modal data which incorporates 1D PPG, accelerometers,
and heart rate data as the inputs to a computationally efficient 1D
bi-directional Gated Recurrent Unit (1D-Bi-GRU) model to detect three
arrhythmia classes. We used motion-artifact prone smartwatch PPG data from the
NIH-funded Pulsewatch clinical trial. Our multimodal model tested on 72
subjects achieved an unprecedented 83% sensitivity for PAC/PVC detection while
maintaining a high accuracy of 97.31% for AF detection. These results
outperformed the best state-of-the-art model by 20.81% for PAC/PVC and 2.55%
for AF detection even while our model was computationally more efficient (14
times lighter and 2.7 faster).

摘要：大多數多類心律不整分類的深度學習模型都是在指尖光電容積描記法 (PPG) 資料上進行測試，與智慧手錶衍生的 PPG 相比，其訊號雜訊比更高，而對於期前心房/心室收縮 (PAC/PVC) 偵測所報告的最佳敏感度值僅為 75%。為了在維持高房顫偵測的同時提高 PAC/PVC 偵測敏感度，我們使用多模式資料，將 1D PPG、加速度計和心率資料作為計算效率高的 1D 雙向閘控遞迴單元 (1D-Bi-GRU) 模型的輸入，以偵測三類心律不整。我們使用了美國國家衛生研究院資助的 Pulsewatch 臨床試驗中的運動偽影易感智慧手錶 PPG 資料。我們在 72 名受試者身上測試的多模式模型，對於 PAC/PVC 偵測達到了前所未有的 83% 敏感度，同時對於房顫偵測維持了 97.31% 的高準確度。即使我們的模型在計算上更有效率（輕 14 倍，快 2.7 倍），這些結果仍比最先進的模型在 PAC/PVC 偵測上高出 20.81%，在房顫偵測上高出 2.55%。

##### **ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language**
2409.05592v1 by Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, Yulan He

Predicting unknown drug-drug interactions (DDIs) is crucial for improving
medication safety. Previous efforts in DDI prediction have typically focused on
binary classification or predicting DDI categories, with the absence of
explanatory insights that could enhance trust in these predictions. In this
work, we propose to generate natural language explanations for DDI predictions,
enabling the model to reveal the underlying pharmacodynamics and
pharmacokinetics mechanisms simultaneously as making the prediction. To do
this, we have collected DDI explanations from DDInter and DrugBank and
developed various models for extensive experiments and analysis. Our models can
provide accurate explanations for unknown DDIs between known drugs. This paper
contributes new tools to the field of DDI prediction and lays a solid
foundation for further research on generating explanations for DDI predictions.

摘要：預測未知的藥物交互作用 (DDI) 對於改善藥物安全至關重要。先前在 DDI 預測方面所做的努力通常集中於二元分類或預測 DDI 類別，而缺乏能夠增強這些預測的可信度的解釋性見解。在這項工作中，我們建議為 DDI 預測產生自然語言解釋，使模型能夠同時揭示藥效學和藥物動力學機制，並進行預測。為此，我們從 DDInter 和 DrugBank 收集了 DDI 解釋，並開發了各種模型進行廣泛的實驗和分析。我們的模型可以為已知藥物之間未知的 DDI 提供準確的解釋。本文為 DDI 預測領域貢獻了新的工具，並為進一步研究 DDI 預測的解釋生成奠定了堅實的基礎。

##### **Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models**
2409.05486v1 by Camilo Thorne, Christian Druckenbrodt, Kinga Szarkowska, Deepika Goyal, Pranita Marajan, Vijay Somanath, Corey Harper, Mao Yan, Tony Scerri

The quality and capabilities of large language models cannot be currently
fully assessed with automated, benchmark evaluations. Instead, human
evaluations that expand on traditional qualitative techniques from natural
language generation literature are required. One recent best-practice consists
in using A/B-testing frameworks, which capture preferences of human evaluators
for specific models. In this paper we describe a human evaluation experiment
focused on the biomedical domain (health, biology, chemistry/pharmacology)
carried out at Elsevier. In it a large but not massive (8.8B parameter)
decoder-only foundational transformer trained on a relatively small (135B
tokens) but highly curated collection of Elsevier datasets is compared to
OpenAI's GPT-3.5-turbo and Meta's foundational 7B parameter Llama 2 model
against multiple criteria. Results indicate -- even if IRR scores were
generally low -- a preference towards GPT-3.5-turbo, and hence towards models
that possess conversational abilities, are very large and were trained on very
large datasets. But at the same time, indicate that for less massive models
training on smaller but well-curated training sets can potentially give rise to
viable alternatives in the biomedical domain.

摘要：大型語言模型的品質和能力目前無法使用自動化基準評估來完全評估。相反，需要使用擴展自然語言生成文獻中傳統定性技術的人類評估。一個最近的最佳實務是使用 A/B 測試框架，該框架會擷取人類評估者對特定模型的偏好。在本文中，我們描述了專注於生物醫學領域（健康、生物學、化學/藥理學）的人類評估實驗，該實驗在 Elsevier 進行。其中，一個大型但並非龐大（8.8B 參數）僅解碼器基礎Transformer在相對較小（135B 令牌）但經過高度策展的 Elsevier 資料集上訓練，與 OpenAI 的 GPT-3.5-turbo 和 Meta 的基礎 7B 參數 Llama 2 模型進行比較，針對多個標準。結果表明——即使 IRR 分數普遍較低——偏好 GPT-3.5-turbo，因此偏好具備對話能力、非常龐大且在非常龐大的資料集上訓練的模型。但同時，表明對於較小規模的模型，在較小但經過良好策展的訓練集上訓練，有可能在生物醫學領域產生可行的替代方案。

##### **KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models**
2409.05370v1 by Yingshu Li, Zhanyu Wang, Yunyi Liu, Lei Wang, Lingqiao Liu, Luping Zhou

Harnessing the robust capabilities of Large Language Models (LLMs) for
narrative generation, logical reasoning, and common-sense knowledge
integration, this study delves into utilizing LLMs to enhance automated
radiology report generation (R2Gen). Despite the wealth of knowledge within
LLMs, efficiently triggering relevant knowledge within these large models for
specific tasks like R2Gen poses a critical research challenge. This paper
presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration
framework based on LLMs. Utilizing a frozen LLM to generate reports, the
framework integrates a knowledge graph to unlock chest disease-related
knowledge within the LLM to enhance the clinical utility of generated reports.
This is achieved by leveraging the knowledge graph to distill disease-related
features in a designed way. Since a radiology report encompasses both normal
and disease-related findings, the extracted graph-enhanced disease-related
features are integrated with regional image features, attending to both
aspects. We explore two fusion methods to automatically prioritize and select
the most relevant features. The fused features are employed by LLM to generate
reports that are more sensitive to diseases and of improved quality. Our
approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.

摘要：<paragraph>利用大型語言模型 (LLM) 強大的功能，進行敘事生成、邏輯推理和常識知識整合，本研究深入探討利用 LLM 來增強自動化放射報告生成 (R2Gen)。儘管 LLM 擁有豐富的知識，但要有效觸發這些大型模型中與特定任務（如 R2Gen）相關的知識，是一個重要的研究挑戰。本文提出了 KARGEN，一個基於 LLM 的知識增強自動化放射報告生成框架。利用凍結的 LLM 來生成報告，該框架整合了一個知識圖譜，以解鎖 LLM 中與胸部疾病相關的知識，以增強生成報告的臨床效用。這是透過利用知識圖譜以設計的方式提取與疾病相關的特徵來實現的。由於放射報告包含正常和疾病相關的發現，因此提取的圖形增強疾病相關特徵與區域影像特徵整合，兼顧兩個方面。我們探索了兩種融合方法，以自動優先排序和選擇最相關的特徵。融合的特徵由 LLM 使用，以生成對疾病更敏感且品質更高的報告。我們的做法在 MIMIC-CXR 和 IU-Xray 資料集上展示了有希望的結果。</paragraph>

##### **Complex Emotion Recognition System using basic emotions via Facial Expression, EEG, and ECG Signals: a review**
2409.07493v1 by Javad Hassannataj Joloudari, Mohammad Maftoun, Bahareh Nakisa, Roohallah Alizadehsani, Meisam Yadollahzadeh-Tabari

The Complex Emotion Recognition System (CERS) deciphers complex emotional
states by examining combinations of basic emotions expressed, their
interconnections, and the dynamic variations. Through the utilization of
advanced algorithms, CERS provides profound insights into emotional dynamics,
facilitating a nuanced understanding and customized responses. The attainment
of such a level of emotional recognition in machines necessitates the knowledge
distillation and the comprehension of novel concepts akin to human cognition.
The development of AI systems for discerning complex emotions poses a
substantial challenge with significant implications for affective computing.
Furthermore, obtaining a sizable dataset for CERS proves to be a daunting task
due to the intricacies involved in capturing subtle emotions, necessitating
specialized methods for data collection and processing. Incorporating
physiological signals such as Electrocardiogram (ECG) and Electroencephalogram
(EEG) can notably enhance CERS by furnishing valuable insights into the user's
emotional state, enhancing the quality of datasets, and fortifying system
dependability. A comprehensive literature review was conducted in this study to
assess the efficacy of machine learning, deep learning, and meta-learning
approaches in both basic and complex emotion recognition utilizing EEG, ECG
signals, and facial expression datasets. The chosen research papers offer
perspectives on potential applications, clinical implications, and results of
CERSs, with the objective of promoting their acceptance and integration into
clinical decision-making processes. This study highlights research gaps and
challenges in understanding CERSs, encouraging further investigation by
relevant studies and organizations. Lastly, the significance of meta-learning
approaches in improving CERS performance and guiding future research endeavors
is underscored.

摘要：複雜情緒辨識系統 (CERS) 透過檢驗表達的基本情緒組合、它們的相互連結，以及動態變化來解碼複雜的情緒狀態。透過使用進階演算法，CERS 提供了對情緒動態的深入見解，促進細緻的理解和客製化的回應。在機器中達成這種程度的情緒辨識需要知識提煉和理解類似於人類認知的新概念。發展用於辨別複雜情緒的人工智慧系統對情感運算來說是一個重大的挑戰，並具有重要的影響。此外，由於捕捉微妙情緒所涉及的複雜性，取得 CERS 的大量資料集被證明是一項艱鉅的任務，因此需要採用特殊的方法來收集和處理資料。納入生理訊號，例如心電圖 (ECG) 和腦電圖 (EEG)，可以透過提供對使用者情緒狀態的寶貴見解、提升資料集的品質以及強化系統的可靠性，來顯著增強 CERS。本研究進行了一項全面的文獻探討，以評估機器學習、深度學習和元學習方法在利用腦電圖、心電圖訊號和面部表情資料集進行基本和複雜情緒辨識方面的效能。所選的研究論文提供了關於 CERS 的潛在應用、臨床影響和結果的觀點，目的是促進它們被接受並整合到臨床決策制定過程中。本研究突出了理解 CERS 的研究差距和挑戰，鼓勵相關研究和組織進一步調查。最後，強調了元學習方法在改善 CERS 效能和指導未來研究工作中的重要性。

##### **Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis**
2409.05292v2 by Nirmalya Thakur

The world is currently experiencing an outbreak of mpox, which has been
declared a Public Health Emergency of International Concern by WHO. No prior
work related to social media mining has focused on the development of a dataset
of Instagram posts about the mpox outbreak. The work presented in this paper
aims to address this research gap and makes two scientific contributions to
this field. First, it presents a multilingual dataset of 60,127 Instagram posts
about mpox, published between July 23, 2022, and September 5, 2024. The
dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram
posts about mpox in 52 languages. For each of these posts, the Post ID, Post
Description, Date of publication, language, and translated version of the post
(translation to English was performed using the Google Translate API) are
presented as separate attributes in the dataset. After developing this dataset,
sentiment analysis, hate speech detection, and anxiety or stress detection were
performed. This process included classifying each post into (i) one of the
sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or
neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no
anxiety/stress detected. These results are presented as separate attributes in
the dataset. Second, this paper presents the results of performing sentiment
analysis, hate speech analysis, and anxiety or stress analysis. The variation
of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and
neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and
50.64%, respectively. In terms of hate speech detection, 95.75% of the posts
did not contain hate and the remaining 4.25% of the posts contained hate.
Finally, 72.05% of the posts did not indicate any anxiety/stress, and the
remaining 27.95% of the posts represented some form of anxiety/stress.

摘要：世界目前正在经历猴痘疫情，世界卫生组织已宣布猴痘疫情为国际关注的突发公共卫生事件。此前没有与社交媒体挖掘相关的研究集中于开发有关猴痘疫情的 Instagram 帖子的数据集。本文介绍的研究旨在解决这一研究空白，并对该领域做出两项科学贡献。首先，它提供了 60,127 条有关猴痘的 Instagram 帖子的多语言数据集，这些帖子发布于 2022 年 7 月 23 日至 2024 年 9 月 5 日之间。该数据集可在 https://dx.doi.org/10.21227/7fvc-y093 处获得，其中包含 52 种语言的有关猴痘的 Instagram 帖子。对于其中每篇帖子，帖子 ID、帖子描述、发布日期、语言和帖子的翻译版本（使用 Google 翻译 API 翻译成英文）作为单独的属性显示在数据集中。在开发此数据集后，进行了情感分析、仇恨言论检测以及焦虑或压力检测。此过程包括将每篇帖子分类为 (i) 情感类别之一，即恐惧、惊讶、快乐、悲伤、愤怒、厌恶或中立，(ii) 仇恨或非仇恨，以及 (iii) 检测到焦虑/压力或未检测到焦虑/压力。这些结果作为单独的属性显示在数据集中。其次，本文介绍了执行情感分析、仇恨言论分析和焦虑或压力分析的结果。观察到情感类别的变化——恐惧、惊讶、快乐、悲伤、愤怒、厌恶和中立分别为 27.95%、2.57%、8.69%、5.94%、2.69%、1.53% 和 50.64%。在仇恨言论检测方面，95.75% 的帖子不包含仇恨，其余 4.25% 的帖子包含仇恨。最后，72.05% 的帖子没有表现出任何焦虑/压力，其余 27.95% 的帖子代表某种形式的焦虑/压力。

##### **RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation**
2409.05280v1 by Quoc-Bao Nguyen-Le, Tuan-Hy Le, Anh-Triet Do, Quoc-Huy Trinh

Cardiovascular disease is a major global health concern, contributing
significantly to global mortality. Accurately segmenting cardiac medical
imaging data is crucial for reducing fatality rates associated with these
conditions. However, current state-of-the-art (SOTA) neural networks, including
CNN-based and Transformer-based approaches, face challenges in capturing both
inter-slice connections and intra-slice details, especially in datasets
featuring intricate, long-range details along the z-axis like coronary
arteries. Existing methods also struggle with differentiating non-cardiac
components from the myocardium, resulting in segmentation inaccuracies and the
"spraying" phenomenon. To address these issues, we introduce
RotCAtt-TransUNet++, a novel architecture designed for robust segmentation of
intricate cardiac structures. Our approach enhances global context modeling
through multiscale feature aggregation and nested skip connections in the
encoder. Transformer layers facilitate capturing intra-slice interactions,
while a rotatory attention mechanism handles inter-slice connectivity. A
channel-wise cross-attention gate integrates multiscale information and decoder
features, effectively bridging semantic gaps. Experimental results across
multiple datasets demonstrate superior performance over current methods,
achieving near-perfect annotation of coronary arteries and myocardium. Ablation
studies confirm that our rotatory attention mechanism significantly improves
segmentation accuracy by transforming embedded vectorized patches in semantic
dimensional space.

摘要：心血管疾病是全球主要的健康問題，對全球死亡率有顯著的影響。準確分割心臟醫學影像資料對於降低這些疾病相關的死亡率至關重要。然而，目前的先進神經網路，包括基於 CNN 和基於 Transformer 的方法，在擷取層間連接和層內細節方面面臨挑戰，特別是在具有沿著 z 軸的複雜、長程細節的資料集，例如冠狀動脈。現有方法也難以區分非心臟成分和心肌，導致分割不準確和「噴灑」現象。為了解決這些問題，我們引入了 RotCAtt-TransUNet++，一種專為複雜心臟結構的穩健分割而設計的新穎架構。我們的做法透過編碼器中的多尺度特徵聚合和巢狀跳躍連接增強了全局背景建模。Transformer 層促進擷取層內交互作用，而旋轉注意機制則處理層間連接。通道式交叉注意閘門整合了多尺度資訊和解碼器特徵，有效地彌合了語義差距。跨多個資料集的實驗結果證明了其優於目前方法的效能，實現了冠狀動脈和心肌的近乎完美的註解。消融研究證實，我們的旋轉注意機制透過轉換語義維度空間中的嵌入向量化補丁，顯著地提高了分割準確度。

##### **Activation Function Optimization Scheme for Image Classification**
2409.04915v1 by Abdur Rahman, Lu He, Haifeng Wang

Activation function has a significant impact on the dynamics, convergence,
and performance of deep neural networks. The search for a consistent and
high-performing activation function has always been a pursuit during deep
learning model development. Existing state-of-the-art activation functions are
manually designed with human expertise except for Swish. Swish was developed
using a reinforcement learning-based search strategy. In this study, we propose
an evolutionary approach for optimizing activation functions specifically for
image classification tasks, aiming to discover functions that outperform
current state-of-the-art options. Through this optimization framework, we
obtain a series of high-performing activation functions denoted as Exponential
Error Linear Unit (EELU). The developed activation functions are evaluated for
image classification tasks from two perspectives: (1) five state-of-the-art
neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and
Compact Convolutional Transformer which cover computationally heavy to light
neural networks, and (2) eight standard datasets, including CIFAR10,
Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15,
and TinyImageNet which cover from typical machine vision benchmark,
agricultural image applications to medical image applications. Finally, we
statistically investigate the generalization of the resultant activation
functions developed through the optimization scheme. With a Friedman test, we
conclude that the optimization scheme is able to generate activation functions
that outperform the existing standard ones in 92.8% cases among 28 different
cases studied, and $-x\cdot erf(e^{-x})$ is found to be the best activation
function for image classification generated by the optimization scheme.

摘要：<paragraph>激活函數對深度神經網路的動態、收斂和效能有顯著的影響。在深度學習模型開發過程中，一直致力於尋找一致且效能高的激活函數。現有的最先進激活函數，除了 Swish 之外，都是由人類專家手動設計的。Swish 是使用基於強化學習的搜尋策略開發的。在本研究中，我們提出了一種演化方法，專門針對圖像分類任務最佳化激活函數，旨在發現效能優於現有最先進選項的函數。透過這個最佳化架構，我們獲得了一系列效能高的激活函數，表示為指數誤差線性單元 (EELU)。已針對兩個觀點評估已開發的激活函數，用於圖像分類任務：(1) 五種最先進的神經網路架構，例如 ResNet50、AlexNet、VGG16、MobileNet 和 Compact Convolutional Transformer，涵蓋從計算量重的到輕量的網路，(2) 八個標準資料集，包括 CIFAR10、Imagenette、MNIST、Fashion MNIST、Beans、Colorectal Histology、CottonWeedID15 和 TinyImageNet，涵蓋從典型的機器視覺基準、農業影像應用到醫學影像應用。最後，我們統計調查了透過最佳化方案開發的結果激活函數的概化。透過 Friedman 檢定，我們得出結論，最佳化方案能夠產生在 28 個不同的研究案例中，有 92.8% 的案例效能優於現有標準函數，並且發現 $-x\cdot erf(e^{-x})$ 是最佳化方案產生的最佳影像分類激活函數。</paragraph>

##### **LMGT: Optimizing Exploration-Exploitation Balance in Reinforcement Learning through Language Model Guided Trade-offs**
2409.04744v1 by Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Wei Chu, Yinghui Xu

The uncertainty inherent in the environmental transition model of
Reinforcement Learning (RL) necessitates a careful balance between exploration
and exploitation to optimize the use of computational resources for accurately
estimating an agent's expected reward. Achieving balance in control systems is
particularly challenging in scenarios with sparse rewards. However, given the
extensive prior knowledge available for many environments, it is redundant to
begin learning from scratch in such settings. To address this, we introduce
\textbf{L}anguage \textbf{M}odel \textbf{G}uided \textbf{T}rade-offs (i.e.,
\textbf{LMGT}), a novel, sample-efficient framework that leverages the
comprehensive prior knowledge embedded in Large Language Models (LLMs) and
their adeptness at processing non-standard data forms, such as wiki tutorials.
LMGT proficiently manages the exploration-exploitation trade-off by employing
reward shifts guided by LLMs, which direct agents' exploration endeavors,
thereby improving sample efficiency. We have thoroughly tested LMGT across
various RL tasks and deployed it in industrial-grade RL recommendation systems,
where it consistently outperforms baseline methods. The results indicate that
our framework can significantly reduce the time cost required during the
training phase in RL.

摘要：在強化學習（RL）的環境轉換模型中，固有的不確定性需要在探索和利用之間取得仔細的平衡，以最佳化計算資源的使用，以精準估計代理預期的獎勵。在控制系統中取得平衡在獎勵稀疏的情況下特別具有挑戰性。然而，由於許多環境都有廣泛的先驗知識，因此在這種設定中從頭開始學習是多餘的。為了解決這個問題，我們引入了**L**anguage **M**odel **G**uided **T**rade-offs（即**LMGT**），這是一個新穎且樣本效率高的架構，它利用了大型語言模型（LLM）中嵌入的全面先驗知識，以及它們處理非標準數據形式（例如 wiki 教程）的靈活性。LMGT 透過採用由 LLM 引導的獎勵轉移來熟練地管理探索-利用權衡，指導代理的探索工作，從而提高樣本效率。我們已經徹底測試了 LMGT 在各種 RL 任務中的表現，並將其部署在工業級 RL 推薦系統中，在這些系統中，它始終優於基線方法。結果表明，我們的架構可以顯著減少 RL 訓練階段所需的時間成本。

##### **NapTune: Efficient Model Tuning for Mood Classification using Previous Night's Sleep Measures along with Wearable Time-series**
2409.04723v1 by Debaditya Shome, Nasim Montazeri Ghahjaverestan, Ali Etemad

Sleep is known to be a key factor in emotional regulation and overall mental
health. In this study, we explore the integration of sleep measures from the
previous night into wearable-based mood recognition. To this end, we propose
NapTune, a novel prompt-tuning framework that utilizes sleep-related measures
as additional inputs to a frozen pre-trained wearable time-series encoder by
adding and training lightweight prompt parameters to each Transformer layer.
Through rigorous empirical evaluation, we demonstrate that the inclusion of
sleep data using NapTune not only improves mood recognition performance across
different wearable time-series namely ECG, PPG, and EDA, but also makes it more
sample-efficient. Our method demonstrates significant improvements over the
best baselines and unimodal variants. Furthermore, we analyze the impact of
adding sleep-related measures on recognizing different moods as well as the
influence of individual sleep-related measures.

摘要：睡眠已知是情緒調節和整體心理健康中的關鍵因素。在本研究中，我們探討將前一晚的睡眠測量整合到可穿戴式情緒辨識中。為此，我們提出了 NapTune，這是一個新穎的提示調整框架，它利用與睡眠相關的測量作為凍結預訓練可穿戴時間序列編碼器的附加輸入，方法是將輕量級提示參數新增並訓練到每個 Transformer 層。透過嚴謹的經驗評估，我們證明使用 NapTune 納入睡眠數據不僅改善了不同可穿戴時間序列（即心電圖、光電容積描記和皮電活動）的情緒辨識效能，還讓它更具樣本效率。我們的模型證明了相較於最佳基線和單模態變異，有顯著的改善。此外，我們分析了新增與睡眠相關的測量對辨識不同情緒的影響，以及個別與睡眠相關的測量的影響。

##### **A Comprehensive Survey on Evidential Deep Learning and Its Applications**
2409.04720v1 by Junyu Gao, Mengyuan Chen, Liangyu Xiang, Changsheng Xu

Reliable uncertainty estimation has become a crucial requirement for the
industrial deployment of deep learning algorithms, particularly in high-risk
applications such as autonomous driving and medical diagnosis. However,
mainstream uncertainty estimation methods, based on deep ensembling or Bayesian
neural networks, generally impose substantial computational overhead. To
address this challenge, a novel paradigm called Evidential Deep Learning (EDL)
has emerged, providing reliable uncertainty estimation with minimal additional
computation in a single forward pass. This survey provides a comprehensive
overview of the current research on EDL, designed to offer readers a broad
introduction to the field without assuming prior knowledge. Specifically, we
first delve into the theoretical foundation of EDL, the subjective logic
theory, and discuss its distinctions from other uncertainty estimation
frameworks. We further present existing theoretical advancements in EDL from
four perspectives: reformulating the evidence collection process, improving
uncertainty estimation via OOD samples, delving into various training
strategies, and evidential regression networks. Thereafter, we elaborate on its
extensive applications across various machine learning paradigms and downstream
tasks. In the end, an outlook on future directions for better performances and
broader adoption of EDL is provided, highlighting potential research avenues.

摘要：可靠的不確定性估計已成為深度學習演算法產業部署的關鍵需求，特別是在高風險應用中，例如自動駕駛和醫療診斷。然而，基於深度集成或貝氏神經網路的主流不確定性估計方法通常會造成大量的計算負擔。為了應對這項挑戰，一種稱為證據深度學習 (EDL) 的新範例應運而生，它在單次前向傳遞中以最少的額外運算提供可靠的不確定性估計。這項調查對 EDL 的現有研究提供全面的概述，旨在為讀者提供該領域的廣泛介紹，而無需假設先備知識。具體來說，我們首先深入探討 EDL 的理論基礎，即主觀邏輯理論，並討論其與其他不確定性估計架構的區別。我們進一步從四個角度介紹 EDL 中現有的理論進展：重新制定證據收集過程、透過 OOD 樣本改善不確定性估計、深入探討各種訓練策略以及證據回歸網路。此後，我們詳細說明它在各種機器學習範例和下游任務中的廣泛應用。最後，我們提供了對未來方向的展望，以期獲得更好的效能和更廣泛地採用 EDL，並重點介紹潛在的研究途徑。

##### **A Multi-scenario Attention-based Generative Model for Personalized Blood Pressure Time Series Forecasting**
2409.04704v1 by Cheng Wan, Chenjie Xie, Longfei Liu, Dan Wu, Ye Li

Continuous blood pressure (BP) monitoring is essential for timely diagnosis
and intervention in critical care settings. However, BP varies significantly
across individuals, this inter-patient variability motivates the development of
personalized models tailored to each patient's physiology. In this work, we
propose a personalized BP forecasting model mainly using electrocardiogram
(ECG) and photoplethysmogram (PPG) signals. This time-series model incorporates
2D representation learning to capture complex physiological relationships.
Experiments are conducted on datasets collected from three diverse scenarios
with BP measurements from 60 subjects total. Results demonstrate that the model
achieves accurate and robust BP forecasts across scenarios within the
Association for the Advancement of Medical Instrumentation (AAMI) standard
criteria. This reliable early detection of abnormal fluctuations in BP is
crucial for at-risk patients undergoing surgery or intensive care. The proposed
model provides a valuable addition for continuous BP tracking to reduce
mortality and improve prognosis.

摘要：持續的血壓 (BP) 監控對於重症監護環境中的及時診斷和干預至關重要。然而，BP 因人而異，這種患者間變異性促使開發針對每位患者生理狀況量身打造的個人化模型。在這項工作中，我們提出了一種個人化 BP 預測模型，主要使用心電圖 (ECG) 和光電容積描記法 (PPG) 信號。此時間序列模型結合了 2D 表徵學習以捕捉複雜的生理關係。實驗是在從三種不同情境收集的資料集上進行，總共來自 60 位受試者的 BP 測量。結果表明，該模型在醫學儀器促進協會 (AAMI) 標準標準內實現了跨情境的準確且穩健的 BP 預測。對於接受手術或重症監護的高風險患者而言，這種對 BP 異常波動的可靠早期檢測至關重要。所提出的模型為持續 BP 追蹤提供了有價值的補充，以降低死亡率並改善預後。

##### **The Impact of Scanner Domain Shift on Deep Learning Performance in Medical Imaging: an Experimental Study**
2409.04368v1 by Gregory Szumel, Brian Guo, Darui Lu, Rongze Gui, Tingyu Wang, Nicholas Konz, Maciej A. Mazurowski

Purpose: Medical images acquired using different scanners and protocols can
differ substantially in their appearance. This phenomenon, scanner domain
shift, can result in a drop in the performance of deep neural networks which
are trained on data acquired by one scanner and tested on another. This
significant practical issue is well-acknowledged, however, no systematic study
of the issue is available across different modalities and diagnostic tasks.
Materials and Methods: In this paper, we present a broad experimental study
evaluating the impact of scanner domain shift on convolutional neural network
performance for different automated diagnostic tasks. We evaluate this
phenomenon in common radiological modalities, including X-ray, CT, and MRI.
Results: We find that network performance on data from a different scanner is
almost always worse than on same-scanner data, and we quantify the degree of
performance drop across different datasets. Notably, we find that this drop is
most severe for MRI, moderate for X-ray, and quite small for CT, on average,
which we attribute to the standardized nature of CT acquisition systems which
is not present in MRI or X-ray. We also study how injecting varying amounts of
target domain data into the training set, as well as adding noise to the
training data, helps with generalization. Conclusion: Our results provide
extensive experimental evidence and quantification of the extent of performance
drop caused by scanner domain shift in deep learning across different
modalities, with the goal of guiding the future development of robust deep
learning models for medical image analysis.

摘要：<paragraph>目的：使用不同掃描儀和協定取得的醫學影像，在影像外觀上可能會有顯著差異。這種現象稱為掃描儀領域偏移，可能會導致深度神經網路的效能下降，而這些網路是針對由一種掃描儀取得的資料進行訓練，並在另一種掃描儀上進行測試。這個重要的實際問題已獲得廣泛認可，但目前尚未針對不同形式和診斷任務進行系統性研究。材料和方法：在本文中，我們提出了一項廣泛的實驗研究，評估掃描儀領域偏移對不同自動化診斷任務的卷積神經網路效能的影響。我們在常見的放射學形式中評估這種現象，包括 X 光、電腦斷層掃描和磁振造影。結果：我們發現，來自不同掃描儀的資料在網路上的效能幾乎總是比來自相同掃描儀的資料差，我們量化了不同資料集效能下降的程度。值得注意的是，我們發現這種下降在磁振造影中最为嚴重，在 X 光中為中等，在電腦斷層掃描中相當小，平均而言，我們將其歸因於電腦斷層掃描取得系統的標準化性質，而磁振造影或 X 光中不存在這種性質。我們還研究了將不同數量的目標領域資料注入訓練集，以及向訓練資料加入雜訊，如何有助於泛化。結論：我們的結果提供了廣泛的實驗證據，並量化了深度學習中由掃描儀領域偏移造成的效能下降程度，目標是引導未來針對醫學影像分析的強健深度學習模型的發展。</paragraph>

##### **CoxKAN: Kolmogorov-Arnold Networks for Interpretable, High-Performance Survival Analysis**
2409.04290v1 by William Knottenbelt, Zeyu Gao, Rebecca Wray, Woody Zhidong Zhang, Jiashuai Liu, Mireia Crispin-Ortuzar

Survival analysis is a branch of statistics used for modeling the time until
a specific event occurs and is widely used in medicine, engineering, finance,
and many other fields. When choosing survival models, there is typically a
trade-off between performance and interpretability, where the highest
performance is achieved by black-box models based on deep learning. This is a
major problem in fields such as medicine where practitioners are reluctant to
blindly trust black-box models to make important patient decisions.
Kolmogorov-Arnold Networks (KANs) were recently proposed as an interpretable
and accurate alternative to multi-layer perceptrons (MLPs). We introduce
CoxKAN, a Cox proportional hazards Kolmogorov-Arnold Network for interpretable,
high-performance survival analysis. We evaluate the proposed CoxKAN on 4
synthetic datasets and 9 real medical datasets. The synthetic experiments
demonstrate that CoxKAN accurately recovers interpretable symbolic formulae for
the hazard function, and effectively performs automatic feature selection.
Evaluation on the 9 real datasets show that CoxKAN consistently outperforms the
Cox proportional hazards model and achieves performance that is superior or
comparable to that of tuned MLPs. Furthermore, we find that CoxKAN identifies
complex interactions between predictor variables that would be extremely
difficult to recognise using existing survival methods, and automatically finds
symbolic formulae which uncover the precise effect of important biomarkers on
patient risk.

摘要：生存分析是統計學的一個分支，用於建模特定事件發生的時間，並廣泛用於醫學、工程、金融和許多其他領域。在選擇生存模型時，通常在性能和可解釋性之間進行權衡，其中最高性能是由基於深度學習的黑盒模型實現的。這在醫學等領域是一個主要問題，因為從業者不願意盲目信任黑盒模型來做出重要的患者決策。Kolmogorov-阿諾德網絡 (KAN) 最近被提議作為多層感知器 (MLP) 的可解釋且準確的替代方案。我們引入了 CoxKAN，這是一個用於可解釋、高性能生存分析的 Cox 比例風險 Kolmogorov-Arnold 網絡。我們在 4 個合成數據集和 9 個真實醫療數據集上評估了所提出的 CoxKAN。合成實驗表明，CoxKAN 準確地恢復了風險函數的可解釋符號公式，並有效地執行自動特徵選擇。對 9 個真實數據集的評估表明，CoxKAN 始終優於 Cox 比例風險模型，並且達到了優於或與調整後的 MLP 相當的性能。此外，我們發現 CoxKAN 識別了預測變量之間的複雜交互作用，這些交互作用使用現有的生存方法極難識別，並自動找到揭示重要生物標誌物對患者風險的準確影響的符號公式。

##### **Advancing Multi-Organ Disease Care: A Hierarchical Multi-Agent Reinforcement Learning Framework**
2409.04224v1 by Daniel J. Tan, Qianyi Xu, Kay Choong See, Dilruk Perera, Mengling Feng

Multi-organ diseases present significant challenges due to their simultaneous
impact on multiple organ systems, necessitating complex and adaptive treatment
strategies. Despite recent advancements in AI-powered healthcare decision
support systems, existing solutions are limited to individual organ systems.
They often ignore the intricate dependencies between organ system and thereby
fails to provide holistic treatment recommendations that are useful in
practice. We propose a novel hierarchical multi-agent reinforcement learning
(HMARL) framework to address these challenges. This framework uses dedicated
agents for each organ system, and model dynamic through explicit inter-agent
communication channels, enabling coordinated treatment strategies across
organs. Furthermore, we introduce a dual-layer state representation technique
to contextualize patient conditions at various hierarchical levels, enhancing
the treatment accuracy and relevance. Through extensive qualitative and
quantitative evaluations in managing sepsis (a complex multi-organ disease),
our approach demonstrates its ability to learn effective treatment policies
that significantly improve patient survival rates. This framework marks a
substantial advancement in clinical decision support systems, pioneering a
comprehensive approach for multi-organ treatment recommendations.

摘要：多器官疾病由於同時影響多個器官系統，因此會帶來重大的挑戰，需要複雜且具有適應性的治療策略。儘管 AI 驅動的醫療保健決策支援系統最近有進展，但現有解決方案僅限於個別器官系統。它們常常忽略器官系統之間的複雜依賴性，因此無法提供實務上有用的整體治療建議。我們提出一個新穎的分層多智能體強化學習 (HMARL) 架構來解決這些挑戰。此架構為每個器官系統使用專用智能體，並透過明確的智能體間通訊管道建模動態，讓不同器官之間的治療策略能夠協調。此外，我們引入雙層狀態表示技術，在各種層級語境化病患狀況，以提升治療準確性和相關性。透過在敗血症（一種複雜的多器官疾病）管理中進行廣泛的定性和定量評估，我們的做法展示了它學習有效治療政策的能力，可顯著改善病患存活率。此架構標誌著臨床決策支援系統的一大進步，開創了多器官治療建議的全面性方法。

##### **Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials**
2409.04481v1 by Yizhen Zheng, Huan Yee Koh, Maddie Yang, Li Li, Lauren T. May, Geoffrey I. Webb, Shirui Pan, George Church

The integration of Large Language Models (LLMs) into the drug discovery and
development field marks a significant paradigm shift, offering novel
methodologies for understanding disease mechanisms, facilitating drug
discovery, and optimizing clinical trial processes. This review highlights the
expanding role of LLMs in revolutionizing various stages of the drug
development pipeline. We investigate how these advanced computational models
can uncover target-disease linkage, interpret complex biomedical data, enhance
drug molecule design, predict drug efficacy and safety profiles, and facilitate
clinical trial processes. Our paper aims to provide a comprehensive overview
for researchers and practitioners in computational biology, pharmacology, and
AI4Science by offering insights into the potential transformative impact of
LLMs on drug discovery and development.

摘要：大型語言模型（LLM）整合到藥物發現和開發領域標誌著重大的典範轉移，提供了解疾病機制、促進藥物發現和優化臨床試驗流程的新方法。本綜述重點介紹了 LLM 在革新藥物開發管線各個階段中日益重要的作用。我們探討了這些先進的計算模型如何揭示靶點疾病關聯性、解釋複雜的生物醫學數據、增強藥物分子設計、預測藥物療效和安全性，以及促進臨床試驗流程。我們的論文旨在為計算生物學、藥理學和 AI4Science 的研究人員和從業者提供全面的概述，深入了解 LLM 對藥物發現和開發的潛在變革性影響。

##### **FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes**
2409.03947v1 by Kai Shu, Yuzhuo Jia, Ziyang Zhang, Jiechao Gao

Automatic Medical Imaging Narrative generation aims to alleviate the workload
of radiologists by producing accurate clinical descriptions directly from
radiological images. However, the subtle visual nuances and domain-specific
terminology in medical images pose significant challenges compared to generic
image captioning tasks. Existing approaches often neglect the vital distinction
between normal and abnormal findings, leading to suboptimal performance. In
this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive
Partitioning Graph framework that addresses these limitations through
domain-adaptive learning. FODA-PG constructs a granular graphical
representation of radiological findings by separating disease-related
attributes into distinct "disease-specific" and "disease-free" categories based
on their clinical significance and location. This adaptive partitioning enables
our model to capture the nuanced differences between normal and pathological
states, mitigating the impact of data biases. By integrating this fine-grained
semantic knowledge into a powerful transformer-based architecture and providing
rigorous mathematical justifications for its effectiveness, FODA-PG generates
precise and clinically coherent reports with enhanced generalization
capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks
demonstrate the superiority of our approach over state-of-the-art methods,
highlighting the importance of domain adaptation in medical report generation.

摘要：自動醫學影像敘述生成旨在透過直接從放射影像產生精確的臨床描述，減輕放射科醫師的工作負擔。然而，與一般影像標題任務相比，醫學影像中的細微視覺差異和特定領域術語會帶來重大挑戰。現有方法常常忽略正常與異常發現之間的重要區別，導致次佳效能。在這項工作中，我們提出 FODA-PG，這是一個新穎的細粒度器官疾病自適應分割圖形架構，透過領域自適應學習來解決這些限制。FODA-PG 透過將疾病相關屬性依據其臨床重要性和位置分為不同的「特定疾病」和「無疾病」類別，來建構放射學發現的細粒度圖形表示。這種自適應分割使我們的模型能夠捕捉正常與病理狀態之間的細微差異，減輕資料偏差的影響。透過將這種細粒度語義知識整合到強大的基於轉換器的架構中，並提供其有效性的嚴謹數學證明，FODA-PG 能夠生成精確且臨床上連貫的報告，並具備增強的概括能力。在 IU-Xray 和 MIMIC-CXR 基準上的廣泛實驗證明了我們的方法優於最先進的方法，突顯了領域適應在醫學報告生成中的重要性。

##### **A deep learning approach to wall-shear stress quantification: From numerical training to zero-shot experimental application**
2409.03933v1 by Esther Lagemann, Julia Roeb, Steven L. Brunton, Christian Lagemann

The accurate quantification of wall-shear stress dynamics is of substantial
importance for various applications in fundamental and applied research,
spanning areas from human health to aircraft design and optimization. Despite
significant progress in experimental measurement techniques and post-processing
algorithms, temporally resolved wall-shear stress dynamics with adequate
spatial resolution and within a suitable spatial domain remain an elusive goal.
To address this gap, we introduce a deep learning architecture that ingests
wall-parallel velocity fields from the logarithmic layer of turbulent
wall-bounded flows and outputs the corresponding 2D wall-shear stress fields
with identical spatial resolution and domain size. From a physical perspective,
our framework acts as a surrogate model encapsulating the various mechanisms
through which highly energetic outer-layer flow structures influence the
governing wall-shear stress dynamics. The network is trained in a supervised
fashion on a unified dataset comprising direct numerical simulations of
statistically 1D turbulent channel and spatially developing turbulent boundary
layer flows at friction Reynolds numbers ranging from 390 to 1,500. We
demonstrate a zero-shot applicability to experimental velocity fields obtained
from Particle-Image Velocimetry measurements and verify the physical accuracy
of the wall-shear stress estimates with synchronized wall-shear stress
measurements using the Micro-Pillar Shear-Stress Sensor for Reynolds numbers up
to 2,000. In summary, the presented framework lays the groundwork for
extracting inaccessible experimental wall-shear stress information from readily
available velocity measurements and thus, facilitates advancements in a variety
of experimental applications.

摘要：<paragraph>準確量化壁面剪應力動態對於基礎和應用研究中的各種應用具有實質性的重要性，涵蓋從人類健康到飛機設計和優化的領域。儘管在實驗測量技術和後處理演算法方面取得了顯著進展，但時間解析壁面剪應力動態仍具有足夠的空間解析度和在合適的空間域中仍然是一個難以捉摸的目標。為了解決這個差距，我們引入了一個深度學習架構，它從湍流壁面約束流的對數層中攝取壁面平行速度場，並輸出相應的 2D 壁面剪應力場，具有相同的空間解析度和域大小。從物理角度來看，我們的框架充當一個代理模型，概括了高能量外層流結構影響控制壁面剪應力動態的各種機制。該網路以監督方式在一個統一的數據集上進行訓練，該數據集包含統計 1D 湍流通道的直接數值模擬和空間發展的湍流邊界層流，摩擦雷諾數範圍從 390 到 1,500。我們展示了對從粒子影像測速測量中獲得的實驗速度場的零次應用，並使用微柱剪應力感測器對雷諾數最高 2,000 的同步壁面剪應力測量驗證了壁面剪應力估計的物理準確性。總之，所提出的框架為從容易獲得的速度測量中提取無法獲得的實驗壁面剪應力資訊奠定了基礎，從而促進了各種實驗應用中的進展。</paragraph>

##### **Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis**
2409.03597v1 by Yucong Zhang, Xin Zou, Jinshan Yang, Wenjun Chen, Faya Liang, Ming Li

This paper presents the Multimodal Analyzing System for Laryngoscope (MASL),
a system that combines audio and video data to automatically extract key
segments and metrics from laryngeal videostroboscopic videos for clinical
assessment. MASL integrates glottis detection with keyword spotting to analyze
patient vocalizations and refine video highlights for better inspection of
vocal cord movements. The system includes a strobing video extraction module
that identifies frames by analyzing hue, saturation, and value fluctuations.
MASL also provides effective metrics for vocal cord paralysis detection,
employing a two-stage glottis segmentation process using U-Net followed by
diffusion-based refinement to reduce false positives. Instead of glottal area
waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis
masks, evaluating both left and right vocal cords to detect unilateral vocal
cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between
left and right paralysis. Ablation studies and experiments on public and
real-world datasets validate MASL's segmentation module and demonstrate its
ability to provide reliable metrics for UVFP diagnosis.

摘要：本文提出了喉镜多模态分析系统 (MASL)，
该系统结合音频和视频数据，自动从喉部视频频闪镜视频中提取关键
片段和指标，用于临床评估。MASL 将声门检测与关键词识别相结合，以分析
患者发声并细化视频重点，以便更好地检查声带运动。该系统包括一个频闪视频提取模块，
该模块通过分析色相、饱和度和值波动来识别帧。
MASL 还为声带麻痹检测提供了有效的指标，
采用两阶段声门分割过程，使用 U-Net，然后进行基于扩散的细化以减少误报。MASL 不使用声门面积波形，而是从声门掩模中估计前声门角波形 (AGAW)，评估左右声带以检测单侧声带麻痹 (UVFP)。通过比较 AGAW 方差，MASL 区分左右麻痹。消融研究和对公共和真实世界数据集的实验验证了 MASL 的分割模块，并证明了其提供可靠的 UVFP 诊断指标的能力。

##### **Improving Uncertainty-Error Correspondence in Deep Bayesian Medical Image Segmentation**
2409.03470v1 by Prerak Mody, Nicolas F. Chaves-de-Plaza, Chinmay Rao, Eleftheria Astrenidou, Mischa de Ridder, Nienke Hoekstra, Klaus Hildebrandt, Marius Staring

Increased usage of automated tools like deep learning in medical image
segmentation has alleviated the bottleneck of manual contouring. This has
shifted manual labour to quality assessment (QA) of automated contours which
involves detecting errors and correcting them. A potential solution to
semi-automated QA is to use deep Bayesian uncertainty to recommend potentially
erroneous regions, thus reducing time spent on error detection. Previous work
has investigated the correspondence between uncertainty and error, however, no
work has been done on improving the "utility" of Bayesian uncertainty maps such
that it is only present in inaccurate regions and not in the accurate ones. Our
work trains the FlipOut model with the Accuracy-vs-Uncertainty (AvU) loss which
promotes uncertainty to be present only in inaccurate regions. We apply this
method on datasets of two radiotherapy body sites, c.f. head-and-neck CT and
prostate MR scans. Uncertainty heatmaps (i.e. predictive entropy) are evaluated
against voxel inaccuracies using Receiver Operating Characteristic (ROC) and
Precision-Recall (PR) curves. Numerical results show that when compared to the
Bayesian baseline the proposed method successfully suppresses uncertainty for
accurate voxels, with similar presence of uncertainty for inaccurate voxels.
Code to reproduce experiments is available at
https://github.com/prerakmody/bayesuncertainty-error-correspondence

摘要：深度學習等自動化工具在醫學影像分割中使用率提升，減輕了手動輪廓描繪的瓶頸。這已將手動勞動轉移到自動輪廓的品質評估 (QA)，其中包含偵測錯誤並修正它們。半自動化 QA 的潛在解決方案是使用深度貝氏不確定性來建議潛在的錯誤區域，從而減少花費在錯誤偵測上的時間。先前的研究已調查不確定性和錯誤之間的對應關係，然而，尚未對改善貝氏不確定性地圖的「效用」進行研究，以使其僅出現在不準確區域，而不出現在準確區域。我們的研究使用準確度對抗不確定性 (AvU) 損失來訓練 FlipOut 模型，這會促使不確定性僅出現在不準確區域。我們將此方法應用於兩個放射治療部位的資料集，即頭頸部電腦斷層掃描和前列腺核磁共振掃描。使用接收器操作特性 (ROC) 和精確度召回率 (PR) 曲線，針對體素不準確性評估不確定性熱圖（即預測熵）。數值結果顯示，與貝氏基準相比，所提出的方法成功地抑制準確體素的不確定性，對於不準確體素的不確定性存在類似情況。可在 https://github.com/prerakmody/bayesuncertainty-error-correspondence 取得重現實驗的程式碼

##### **Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time**
2409.03375v1 by Francisco de Arriba-Pérez, Silvia García-Méndez

Based on official estimates, 50 million people worldwide are affected by
dementia, and this number increases by 10 million new patients every year.
Without a cure, clinical prognostication and early intervention represent the
most effective ways to delay its progression. To this end, Artificial
Intelligence and computational linguistics can be exploited for natural
language analysis, personalized assessment, monitoring, and treatment. However,
traditional approaches need more semantic knowledge management and
explicability capabilities. Moreover, using Large Language Models (LLMs) for
cognitive decline diagnosis is still scarce, even though these models represent
the most advanced way for clinical-patient communication using intelligent
systems. Consequently, we leverage an LLM using the latest Natural Language
Processing (NLP) techniques in a chatbot solution to provide interpretable
Machine Learning prediction of cognitive decline in real-time.
Linguistic-conceptual features are exploited for appropriate natural language
analysis. Through explainability, we aim to fight potential biases of the
models and improve their potential to help clinical workers in their diagnosis
decisions. More in detail, the proposed pipeline is composed of (i) data
extraction employing NLP-based prompt engineering; (ii) stream-based data
processing including feature engineering, analysis, and selection; (iii)
real-time classification; and (iv) the explainability dashboard to provide
visual and natural language descriptions of the prediction outcome.
Classification results exceed 80 % in all evaluation metrics, with a recall
value for the mental deterioration class about 85 %. To sum up, we contribute
with an affordable, flexible, non-invasive, personalized diagnostic system to
this work.

摘要：<paragraph>根據官方的估計，全球約有 5000 萬人罹患失智症，且這個數字每年增加 1000 萬名新患者。在沒有治癒方法的情況下，臨床預後和早期介入是延緩其惡化的最有效方法。為此，人工智慧和計算語言學可被用於自然語言分析、個人化評估、監控和治療。然而，傳統方法需要更多語義知識管理和可解釋性能力。此外，儘管這些模型代表了使用智慧系統進行臨床患者溝通的最先進方式，但將大型語言模型 (LLM) 用於認知能力下降診斷仍然很少見。因此，我們利用聊天機器人解決方案中使用最新自然語言處理 (NLP) 技術的 LLM，以提供對認知能力下降的機器學習預測。語言概念特徵被用於適當的自然語言分析。透過可解釋性，我們旨在消除模型的潛在偏差，並提高其在診斷決策中協助臨床工作者的潛力。更詳細地說，所提出的管道包括：(i) 使用基於 NLP 的提示工程進行資料萃取；(ii) 串流式資料處理，包括特徵工程、分析和選擇；(iii) 即時分類；以及 (iv) 可解釋性儀表板，以提供預測結果的可視化和自然語言描述。分類結果在所有評估指標中都超過 80%，心智退化類別的召回率約為 85%。總而言之，我們為這項工作貢獻了一個經濟實惠、靈活、非侵入性、個人化的診斷系統。</paragraph>

##### **Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced Diagnostic Models through Machine Learning**
2409.03147v1 by Juan A. Berrios Moya

The rapid global aging trend has led to an increase in dementia cases,
including Alzheimer's disease, underscoring the urgent need for early and
accurate diagnostic methods. Traditional diagnostic techniques, such as
cognitive tests, neuroimaging, and biomarker analysis, face significant
limitations in sensitivity, accessibility, and cost, particularly in the early
stages. This study explores the potential of machine learning (ML) as a
transformative approach to enhance early dementia detection by leveraging ML
models to analyze and integrate complex multimodal datasets, including
cognitive assessments, neuroimaging, and genetic information. A comprehensive
review of existing literature was conducted to evaluate various ML models,
including supervised learning, deep learning, and advanced techniques such as
ensemble learning and transformer models, assessing their accuracy,
interpretability, and potential for clinical integration. The findings indicate
that while ML models show significant promise in improving diagnostic precision
and enabling earlier interventions, challenges remain in their
generalizability, interpretability, and ethical deployment. This research
concludes by outlining future directions aimed at enhancing the clinical
utility of ML models in dementia detection, emphasizing interdisciplinary
collaboration and ethically sound frameworks to improve early detection and
intervention strategies for Alzheimer's disease and other forms of dementia.

摘要：全球人口快速老化趨勢導致失智症病例增加，包括阿茲海默症，突顯出早期且準確的診斷方法的迫切需求。傳統的診斷技術，例如認知測驗、神經影像和生物標記分析，在敏感性、可及性和成本方面面臨重大限制，特別是在早期階段。本研究探討機器學習 (ML) 作為一種變革性方法的潛力，通過利用 ML 模型分析和整合複雜的多模式數據集，包括認知評估、神經影像和遺傳信息，來增強早期失智症檢測。對現有文獻進行了全面回顧，以評估各種 ML 模型，包括監督學習、深度學習和先進技術，例如集成學習和Transformer模型，評估其準確性、可解釋性和臨床整合的潛力。研究結果表明，儘管 ML 模型在提高診斷精度和實現早期干預方面顯示出顯著的希望，但其可概化性、可解釋性和道德部署仍然存在挑戰。本研究最後概述了旨在增強 ML 模型在失智症檢測中的臨床效用的未來方向，強調跨學科合作和道德健全的框架，以改善阿茲海默症和其他形式失智症的早期檢測和干預策略。

##### **MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation**
2409.03062v1 by Shehan Perera, Yunus Erzurumlu, Deepak Gulati, Alper Yilmaz

Skin cancer segmentation poses a significant challenge in medical image
analysis. Numerous existing solutions, predominantly CNN-based, face issues
related to a lack of global contextual understanding. Alternatively, some
approaches resort to large-scale Transformer models to bridge the global
contextual gaps, but at the expense of model size and computational complexity.
Finally many Transformer based approaches rely primarily on CNN based decoders
overlooking the benefits of Transformer based decoding models. Recognizing
these limitations, we address the need efficient lightweight solutions by
introducing MobileUNETR, which aims to overcome the performance constraints
associated with both CNNs and Transformers while minimizing model size,
presenting a promising stride towards efficient image segmentation. MobileUNETR
has 3 main features. 1) MobileUNETR comprises of a lightweight hybrid
CNN-Transformer encoder to help balance local and global contextual feature
extraction in an efficient manner; 2) A novel hybrid decoder that
simultaneously utilizes low-level and global features at different resolutions
within the decoding stage for accurate mask generation; 3) surpassing large and
complex architectures, MobileUNETR achieves superior performance with 3 million
parameters and a computational complexity of 1.3 GFLOP resulting in 10x and 23x
reduction in parameters and FLOPS, respectively. Extensive experiments have
been conducted to validate the effectiveness of our proposed method on four
publicly available skin lesion segmentation datasets, including ISIC 2016, ISIC
2017, ISIC 2018, and PH2 datasets. The code will be publicly available at:
https://github.com/OSUPCVLab/MobileUNETR.git

摘要：皮膚癌分割在醫學影像分析中構成一項重大挑戰。現有許多解決方案（主要是基於 CNN）面臨缺乏整體背景理解的問題。或者，一些方法訴諸於大規模 Transformer 模型來彌合整體背景差距，但犧牲了模型大小和計算複雜度。最後，許多基於 Transformer 的方法主要依賴於基於 CNN 的解碼器，而忽視了基於 Transformer 的解碼模型的優點。認識到這些限制，我們通過引入 MobileUNETR 來解決對高效輕量級解決方案的需求，其目標是克服與 CNN 和 Transformer 相關的效能限制，同時最小化模型大小，為高效影像分割邁出有希望的一步。MobileUNETR 有 3 個主要特點。1) MobileUNETR 包含一個輕量級混合 CNN-Transformer 編碼器，以有效的方式幫助平衡局部和整體背景特徵提取；2) 一個新穎的混合解碼器，在解碼階段同時利用不同解析度下的低階和整體特徵，以進行精確的遮罩生成；3) 超越大型而複雜的架構，MobileUNETR 以 300 萬個參數和 1.3 GFLOP 的計算複雜度實現了卓越的效能，分別減少了 10 倍和 23 倍的參數和 FLOP。已經進行了廣泛的實驗，以驗證我們提出的方法在四個公開可用的皮膚病變分割資料集（包括 ISIC 2016、ISIC 2017、ISIC 2018 和 PH2 資料集）上的有效性。程式碼將公開於：https://github.com/OSUPCVLab/MobileUNETR.git

##### **Multi-stream deep learning framework to predict mild cognitive impairment with Rey Complex Figure Test**
2409.02883v1 by Junyoung Park, Eun Hyun Seo, Sunjun Kim, SangHak Yi, Kun Ho Lee, Sungho Won

Drawing tests like the Rey Complex Figure Test (RCFT) are widely used to
assess cognitive functions such as visuospatial skills and memory, making them
valuable tools for detecting mild cognitive impairment (MCI). Despite their
utility, existing predictive models based on these tests often suffer from
limitations like small sample sizes and lack of external validation, which
undermine their reliability. We developed a multi-stream deep learning
framework that integrates two distinct processing streams: a multi-head
self-attention based spatial stream using raw RCFT images and a scoring stream
employing a previously developed automated scoring system. Our model was
trained on data from 1,740 subjects in the Korean cohort and validated on an
external hospital dataset of 222 subjects from Korea. The proposed multi-stream
model demonstrated superior performance over baseline models (AUC = 0.872,
Accuracy = 0.781) in external validation. The integration of both spatial and
scoring streams enables the model to capture intricate visual details from the
raw images while also incorporating structured scoring data, which together
enhance its ability to detect subtle cognitive impairments. This dual approach
not only improves predictive accuracy but also increases the robustness of the
model, making it more reliable in diverse clinical settings. Our model has
practical implications for clinical settings, where it could serve as a
cost-effective tool for early MCI screening.

摘要：雷氏複雜圖形測驗 (RCFT) 等繪畫測驗廣泛用於評估視覺空間技能和記憶力等認知功能，使其成為檢測輕度認知障礙 (MCI) 的寶貴工具。儘管它們很有用，但基於這些測驗的現有預測模型通常會受到樣本量小和缺乏外部驗證等限制，這會損害其可靠性。我們開發了一個多串流深度學習框架，它整合了兩個不同的處理串流：一個基於多頭自注意力，使用原始 RCFT 影像的空間串流，以及一個採用先前開發的自動評分系統的評分串流。我們的模型在韓國群組中 1,740 名受試者的資料上進行訓練，並在來自韓國的 222 名受試者的外部醫院資料集上進行驗證。所提出的多串流模型在外部驗證中表現出優於基準模型的效能 (AUC = 0.872，準確率 = 0.781)。空間和評分串流的整合使模型能夠從原始影像擷取複雜的視覺細節，同時也能納入結構化的評分資料，這共同增強了它檢測細微認知障礙的能力。這種雙重方法不僅提高了預測準確性，也增加了模型的穩健性，使其在不同的臨床環境中更可靠。我們的模型對臨床環境有實際的意義，它可以在其中作為早期 MCI 篩檢的具成本效益的工具。

##### **Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon**
2409.02681v1 by Ramon Tavares

This study presents a comprehensive methodology for modeling and forecasting
the historical time series of fire spots detected by the AQUA_M-T satellite in
the Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network
(RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit
(GRU) architectures to predict monthly accumulations of daily detected fire
spots. A summary of the data revealed a consistent seasonality over time, with
annual maximum and minimum fire spot values tending to repeat at the same
periods each year. The primary objective is to verify whether the forecasts
capture this inherent seasonality through rigorous statistical analysis. The
methodology involved careful data preparation, model configuration, and
training using cross-validation with two seeds, ensuring that the data
generalizes well to the test and validation sets, and confirming the
convergence of the model parameters. The results indicate that the mixed LSTM
and GRU model offers improved accuracy in forecasting 12 months ahead,
demonstrating its effectiveness in capturing complex temporal patterns and
modeling the observed time series. This research significantly contributes to
the application of deep learning techniques in environmental monitoring,
specifically in fire spot forecasting. In addition to improving forecast
accuracy, the proposed approach highlights the potential for adaptation to
other time series forecasting challenges, opening new avenues for research and
development in machine learning and natural phenomenon prediction. Keywords:
Time Series Forecasting, Recurrent Neural Networks, Deep Learning.

摘要：本研究提出了一個全面的方法，用於建模和預測巴西亞馬遜地區由 AQUA_M-T 衛星偵測到的歷史火災點時間序列。該方法採用混合遞迴神經網路 (RNN) 模型，結合長短期記憶 (LSTM) 和門控遞迴單元 (GRU) 架構，以預測每日偵測火災點的月累計值。對資料的摘要顯示出隨著時間推移而出現的一致季節性，每年的年度最大和最小火災點值傾向於在同一時期重複出現。主要目標是透過嚴謹的統計分析驗證預測是否捕捉到這種固有的季節性。該方法涉及仔細的資料準備、模型配置，以及使用兩個種子的交叉驗證進行訓練，確保資料能很好地推廣到測試和驗證集，並確認模型參數的收斂性。結果表明，混合 LSTM 和 GRU 模型在預測 12 個月後提供了更高的準確度，證明了其在捕捉複雜時間模式和建模觀測時間序列方面的有效性。本研究顯著地促进了深度學習技術在環境監測中的應用，特別是在火災點預測方面。除了提高預測準確度外，所提出的方法還強調了適應其他時間序列預測挑戰的潛力，為機器學習和自然現象預測的研究和開發開闢了新的途徑。關鍵字：時間序列預測、遞迴神經網路、深度學習。

##### **SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments**
2409.02598v1 by Wenwu Guo, Jinlin Wu, Zhen Chen, Qingxiang Zhao, Miao Xu, Zhen Lei, Hongbin Liu

Vision-based surgical navigation has received increasing attention due to its
non-invasive, cost-effective, and flexible advantages. In particular, a
critical element of the vision-based navigation system is tracking surgical
instruments. Compared with 2D instrument tracking methods, 3D instrument
tracking has broader value in clinical practice, but is also more challenging
due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models
for 3D registration. To solve these challenges, we propose the SurgTrack, a
two-stage 3D instrument tracking method for CAD-free and robust real-world
applications. In the first registration stage, we incorporate an Instrument
Signed Distance Field (SDF) modeling the 3D representation of instruments,
achieving CAD-freed 3D registration. Due to this, we can obtain the location
and orientation of instruments in the 3D space by matching the video stream
with the registered SDF model. In the second tracking stage, we devise a
posture graph optimization module, leveraging the historical tracking results
of the posture memory pool to optimize the tracking results and improve the
occlusion robustness. Furthermore, we collect the Instrument3D dataset to
comprehensively evaluate the 3D tracking of surgical instruments. The extensive
experiments validate the superiority and scalability of our SurgTrack, by
outperforming the state-of-the-arts with a remarkable improvement. The code and
dataset are available at https://github.com/wenwucode/SurgTrack.

摘要：<paragraph>基於視覺的外科導航由於其非侵入性、成本效益和靈活性優勢而受到越來越多的關注。特別是，基於視覺的導航系統的一個關鍵元素是追蹤手術器械。與 2D 器械追蹤方法相比，3D 器械追蹤在臨床實務中具有更廣泛的價值，但由於紋理弱、遮擋和缺乏用於 3D 配準的電腦輔助設計 (CAD) 模型，因此也更具挑戰性。為了解決這些挑戰，我們提出 SurgTrack，一種適用於無 CAD 和穩健的真實世界應用程式的兩階段 3D 器械追蹤方法。在第一個配準階段，我們整合一個器械簽署距離場 (SDF)，對器械的 3D 表徵進行建模，實現無 CAD 的 3D 配準。因此，我們可以透過將視訊串流與已配準的 SDF 模型進行匹配，取得器械在 3D 空間中的位置和方向。在第二個追蹤階段，我們設計一個姿勢圖最佳化模組，利用姿勢記憶池的歷史追蹤結果來最佳化追蹤結果並改善遮擋的穩健性。此外，我們收集 Instrument3D 資料集，以全面評估手術器械的 3D 追蹤。廣泛的實驗驗證了我們 SurgTrack 的優越性和可擴充性，以顯著的改進優於現有技術。程式碼和資料集可在 https://github.com/wenwucode/SurgTrack 取得。</paragraph>

##### **Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models**
2409.02530v1 by Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of
kidney function in clinical practice. Although traditional equations and
Machine Learning (ML) models using clinical and laboratory data can estimate
eGFR, accurately predicting future eGFR levels remains a significant challenge
for nephrologists and ML researchers. Recent advances demonstrate that Large
Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust
foundation models for diverse applications. This study investigates the
potential of LMMs to predict future eGFR levels with a dataset consisting of
laboratory and clinical values from 50 patients. By integrating various
prompting techniques and ensembles of LMMs, our findings suggest that these
models, when combined with precise prompts and visual representations of eGFR
trajectories, offer predictive performance comparable to existing ML models.
This research extends the application of foundation models and suggests avenues
for future studies to harness these models in addressing complex medical
forecasting challenges.

摘要：估計的腎小球過濾率 (eGFR) 是臨床實務中腎臟功能的重要指標。雖然傳統方程式和使用臨床與實驗室資料的機器學習 (ML) 模型可以估計 eGFR，但準確預測未來 eGFR 水平仍然是腎臟科醫師和 ML 研究人員的一大挑戰。最近的研究進展顯示，大型語言模型 (LLM) 和大型多模態模型 (LMM) 可以作為各種應用程式的強健基礎模型。本研究探討 LMM 預測未來 eGFR 水平的潛力，其資料集包含 50 位病患的實驗室和臨床數值。透過整合各種提示技術和 LMM 的合奏，我們的研究結果顯示，這些模型在結合精確提示和 eGFR 軌跡的視覺化表示時，可提供與現有 ML 模型相近的預測效能。這項研究擴展了基礎模型的應用，並為未來研究利用這些模型來應對複雜的醫療預測挑戰提供了途徑。

##### **Coaching a Robotic Sonographer: Learning Robotic Ultrasound with Sparse Expert's Feedback**
2409.02337v1 by Deepak Raina, Mythra V. Balakuntala, Byung Wook Kim, Juan Wachs, Richard Voyles

Ultrasound is widely employed for clinical intervention and diagnosis, due to
its advantages of offering non-invasive, radiation-free, and real-time imaging.
However, the accessibility of this dexterous procedure is limited due to the
substantial training and expertise required of operators. The robotic
ultrasound (RUS) offers a viable solution to address this limitation;
nonetheless, achieving human-level proficiency remains challenging. Learning
from demonstrations (LfD) methods have been explored in RUS, which learns the
policy prior from a dataset of offline demonstrations to encode the mental
model of the expert sonographer. However, active engagement of experts, i.e.
Coaching, during the training of RUS has not been explored thus far. Coaching
is known for enhancing efficiency and performance in human training. This paper
proposes a coaching framework for RUS to amplify its performance. The framework
combines DRL (self-supervised practice) with sparse expert's feedback through
coaching. The DRL employs an off-policy Soft Actor-Critic (SAC) network, with a
reward based on image quality rating. The coaching by experts is modeled as a
Partially Observable Markov Decision Process (POMDP), which updates the policy
parameters based on the correction by the expert. The validation study on
phantoms showed that coaching increases the learning rate by $25\%$ and the
number of high-quality image acquisition by $74.5\%$.

摘要：超音波因其提供非侵入性、無輻射且即時影像的優點，而廣泛用於臨床介入和診斷。
然而，由於操作員需要大量的訓練和專業知識，限制了此靈活程序的可及性。機器人超音波 (RUS) 提供了一個可行的解決方案來解決此限制；
儘管如此，要達到人類等級的熟練度仍然具有挑戰性。學習示範 (LfD) 方法已在 RUS 中進行探討，它從離線示範的資料集學習先驗策略，以編碼專家超音波檢查員的心智模型。然而，迄今尚未探討專家在 RUS 訓練期間的積極參與，即指導。指導已知可以提高人類訓練的效率和績效。本文提出了一個 RUS 指導架構，以提升其績效。此架構結合了 DRL（自我監督實務）與透過指導提供的專家稀疏回饋。DRL 使用離線策略軟性動作-評論 (SAC) 網路，並根據影像品質評分給予獎勵。專家的指導被建模為部分可觀察馬可夫決策過程 (POMDP)，它根據專家的修正來更新策略參數。在模擬人體模型上的驗證研究顯示，指導將學習率提高了 $25\%$，高品質影像擷取數量提高了 $74.5\%$。

##### **Action-Based ADHD Diagnosis in Video**
2409.02261v1 by Yichun Li, Yuxing Yang, Syed Nohsen Naqvi

Attention Deficit Hyperactivity Disorder (ADHD) causes significant impairment
in various domains. Early diagnosis of ADHD and treatment could significantly
improve the quality of life and functioning. Recently, machine learning methods
have improved the accuracy and efficiency of the ADHD diagnosis process.
However, the cost of the equipment and trained staff required by the existing
methods are generally huge. Therefore, we introduce the video-based frame-level
action recognition network to ADHD diagnosis for the first time. We also record
a real multi-modal ADHD dataset and extract three action classes from the video
modality for ADHD diagnosis. The whole process data have been reported to
CNTW-NHS Foundation Trust, which would be reviewed by medical
consultants/professionals and will be made public in due course.

摘要：注意力缺陷過動症 (ADHD) 會在各種領域造成顯著的損害。提早診斷 ADHD 並接受治療可以大幅改善生活品質和功能。最近，機器學習方法已經提升了 ADHD 診斷程序的準確度和效率。然而，現有方法所需的設備和訓練有素的人員成本通常很高。因此，我們首次將基於影片的幀級動作辨識網路引入 ADHD 診斷。我們也記錄了一個真正的多模式 ADHD 資料集，並從影片模式中萃取出三個動作類別以進行 ADHD 診斷。整個流程的資料已經回報給 CNTW-NHS 基金會，將由醫療顧問/專業人士審查，並將適時公開。

##### **A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial**
2409.02069v1 by Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy

Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized individuals at risk for dental disease.
Oralytics incorporates an online reinforcement learning algorithm to determine
optimal times to deliver intervention prompts that encourage oral self-care
behaviors. We have deployed Oralytics in a registered clinical trial. The
deployment required careful design to manage challenges specific to the
clinical trials setting in the U.S. In this paper, we (1) highlight key design
decisions of the RL algorithm that address these challenges and (2) conduct a
re-sampling analysis to evaluate algorithm design decisions. A second phase
(randomized control trial) of Oralytics is planned to start in spring 2025.

摘要：牙科疾病是一種普遍的慢性疾病，與大量的經濟負擔、個人痛苦和增加的全身疾病風險有關。儘管普遍建議每天刷牙兩次，但由於健忘和脫離等因素，對建議的口腔自我保健行為的依從性仍然低於最佳水平。為了解決這個問題，我們開發了 Oralytics，一個 mHealth 介入系統，旨在補充臨床醫生提供的預防保健，以預防有牙科疾病風險的邊緣化個人。Oralytics 結合了一個在線強化學習演算法，以確定提供介入提示的最佳時間，這些提示鼓勵口腔自我保健行為。我們已在註冊的臨床試驗中部署了 Oralytics。該部署需要仔細的設計來管理美國臨床試驗設置中具體的挑戰。在本文中，我們（1）重點介紹了解決這些挑戰的 RL 演算法的關鍵設計決策，以及（2）進行重新抽樣分析以評估演算法設計決策。Oralytics 的第二階段（隨機對照試驗）計劃於 2025 年春季開始。

##### **TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation**
2409.02018v1 by Bobby Azad, Pourya Adibfar, Kaiqun Fu

In healthcare, medical image segmentation is crucial for accurate disease
diagnosis and the development of effective treatment strategies. Early
detection can significantly aid in managing diseases and potentially prevent
their progression. Machine learning, particularly deep convolutional neural
networks, has emerged as a promising approach to addressing segmentation
challenges. Traditional methods like U-Net use encoding blocks for local
representation modeling and decoding blocks to uncover semantic relationships.
However, these models often struggle with multi-scale objects exhibiting
significant variations in texture and shape, and they frequently fail to
capture long-range dependencies in the input data. Transformers designed for
sequence-to-sequence predictions have been proposed as alternatives, utilizing
global self-attention mechanisms. Yet, they can sometimes lack precise
localization due to insufficient granular details. To overcome these
limitations, we introduce TransDAE: a novel approach that reimagines the
self-attention mechanism to include both spatial and channel-wise associations
across the entire feature space, while maintaining computational efficiency.
Additionally, TransDAE enhances the skip connection pathway with an inter-scale
interaction module, promoting feature reuse and improving localization
accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on
the Synaps multi-organ dataset, even without relying on pre-trained weights.

摘要：在医疗保健领域，医学影像分割对于准确的疾病诊断和有效治疗策略的开发至关重要。早期检测可以极大地帮助控制疾病，并可能防止疾病进展。机器学习，尤其是深度卷积神经网络，已成为解决分割挑战的一种有前途的方法。U-Net 等传统方法使用编码块进行局部表示建模和解码块来揭示语义关系。然而，这些模型通常难以处理在纹理和形状上表现出显着变化的多尺度对象，并且它们经常无法捕获输入数据中的远程依赖关系。专为序列到序列预测而设计的 Transformer 已被提出作为替代方案，利用全局自注意力机制。然而，由于粒度细节不足，它们有时可能缺乏精确的定位。为了克服这些限制，我们引入了 TransDAE：一种新颖的方法，它重新构想了自注意力机制，以包含整个特征空间中的空间和通道关联，同时保持计算效率。此外，TransDAE 通过尺度间交互模块增强了跳跃连接路径，促进了特征重用并提高了定位精度。值得注意的是，即使不依赖预训练权重，TransDAE 在 Synaps 多器官数据集上也优于现有的最先进方法。

##### **A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management**
2409.01903v1 by Abdelmalek Mouazer, Sophie Dubois, Romain Léguillon, Nada Boudegzdame, Thibaud Levrard, Yoann Le Bars, Christian Simon, Brigitte Séroussi, Julien Grosjean, Romain Lelong, Catherine Letord, Stéfan Darmoni, Karima Sedki, Pierre Meneton, Rosy Tsopra, Hector Falcoff, Jean-Baptiste Lamy

Background: Medication review is a structured interview of the patient,
performed by the pharmacist and aimed at optimizing drug treatments. In
practice, medication review is a long and cognitively-demanding task that
requires specific knowledge. Clinical practice guidelines have been proposed,
but their application is tedious. Methods: We designed ABiMed, a clinical
decision support system for medication reviews, based on the implementation of
the STOPP/START v2 guidelines and on the visual presentation of aggregated drug
knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39
community pharmacists during a randomized simulation trial, each pharmacist
performing a medication review for two fictitious patients without ABiMed, and
two others with ABiMed. We recorded the problems identified by the pharmacists,
the interventions proposed, the response time, the perceived usability and the
comments. Pharmacists' medication reviews were compared to an expert-designed
gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant
drug-related problems during the medication review (p=1.1e-12) and proposed
better interventions (p=9.8e-9), without needing more time (p=0.56). The System
Usability Scale score is 82.7, which is ranked "excellent". In their comments,
pharmacists appreciated the visual aspect of ABiMed and its ability to compare
the current treatment with the proposed one. A multifactor analysis showed no
difference in the support offered by ABiMed according to the pharmacist's age
or sex, in terms of percentage of problems identified or quality of the
proposed interventions. Conclusions: The use of an intelligent and visual
clinical decision support system can help pharmacists when they perform
medication reviews. Our main perspective is the validation of the system in
clinical conditions.

摘要：<paragraph>背景：用藥審查是由藥師執行的一種結構化患者訪談，目的在於優化藥物治療。在實務上，用藥審查是一項冗長且認知需求高的任務，需要具備特定知識。雖然已提出臨床實務指引，但其應用很繁瑣。方法：我們根據 STOPP/START v2 指引的實作，並使用表格、圖表和花形符號視覺化呈現彙整的藥物知識，設計了一套用藥審查的臨床決策支援系統 ABiMed。我們在隨機模擬試驗中，讓 39 位社區藥師評估 ABiMed，每位藥師針對兩位虛構患者執行用藥審查，兩次沒有使用 ABiMed，兩次使用 ABiMed。我們記錄了藥師識別出的問題、建議的介入措施、反應時間、感知可用性和評論。將藥師的用藥審查與專家設計的金標準進行比較。結果：使用 ABiMed 後，藥師在用藥審查期間發現了多 1.6 倍相關的藥物相關問題（p=1.1e-12），並提出更好的介入措施（p=9.8e-9），而無需花費更多時間（p=0.56）。系統可用性評分為 82.7，被評為「優良」。在他們的評論中，藥師讚賞 ABiMed 的視覺化面向，以及它比較目前治療與建議治療的能力。多因素分析顯示，ABiMed 提供的支援在藥師的年齡或性別方面沒有差異，就識別出的問題百分比或建議介入措施的品質而言。結論：使用智慧且視覺化的臨床決策支援系統，可以協助藥師執行用藥審查。我們的觀點主要是驗證系統在臨床條件下的效度。</paragraph>

##### **Training on the Benchmark Is Not All You Need**
2409.01790v1 by Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang

The success of Large Language Models (LLMs) relies heavily on the huge amount
of pre-training data learned in the pre-training phase. The opacity of the
pre-training process and the training data causes the results of many benchmark
tests to become unreliable. If any model has been trained on a benchmark test
set, it can seriously hinder the health of the field. In order to automate and
efficiently test the capabilities of large language models, numerous mainstream
benchmarks adopt a multiple-choice format. As the swapping of the contents of
multiple-choice options does not affect the meaning of the question itself, we
propose a simple and effective data leakage detection method based on this
property. Specifically, we shuffle the contents of the options in the data to
generate the corresponding derived data sets, and then detect data leakage
based on the model's log probability distribution over the derived data sets.
If there is a maximum and outlier in the set of log probabilities, it indicates
that the data is leaked. Our method is able to work under black-box conditions
without access to model training data or weights, effectively identifying data
leakage from benchmark test sets in model pre-training data, including both
normal scenarios and complex scenarios where options may have been shuffled
intentionally or unintentionally. Through experiments based on two LLMs and
benchmark designs, we demonstrate the effectiveness of our method. In addition,
we evaluate the degree of data leakage of 31 mainstream open-source LLMs on
four benchmark datasets and give a ranking of the leaked LLMs for each
benchmark, and we find that the Qwen family of LLMs has the highest degree of
data leakage.

摘要：大型語言模型 (LLM) 的成功在很大程度上取決於預訓練階段中學習到的海量預訓練數據。預訓練過程和訓練數據的不透明性導致許多基準測試的結果變得不可靠。如果任何模型已在基準測試集中進行訓練，則可能會嚴重阻礙該領域的發展。為了自動化且有效地測試大型語言模型的能力，許多主流基準採用多選題格式。由於多選題選項內容的互換不影響問題本身的含義，因此我們提出了一種基於此屬性的簡單且有效的数据洩漏檢測方法。具體來說，我們將數據中選項的內容隨機排列以生成對應的派生數據集，然後根據模型在派生數據集上的對數概率分佈檢測數據洩漏。如果對數概率集中存在最大值和異常值，則表示數據已洩漏。我們的方法能夠在不訪問模型訓練數據或權重的黑盒條件下工作，有效地識別模型預訓練數據中基準測試集的數據洩漏，包括選項可能已有意或無意地被打亂的正常場景和複雜場景。通過基於兩個 LLM 和基準設計的實驗，我們證明了我們方法的有效性。此外，我們評估了 31 個主流開源 LLM 在四個基準數據集上的數據洩漏程度，並對每個基準的洩漏 LLM 進行了排名，我們發現 Qwen 家族的 LLM 具有最高的數據洩漏程度。

##### **Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring**
2409.01676v1 by Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink

Deriving health indicators of rotating machines is crucial for their
maintenance. However, this process is challenging for the prevalent adopted
intelligent methods since they may take the whole data distributions, not only
introducing noise interference but also lacking the explainability. To address
these issues, we propose a diffusion-based weakly-supervised approach for
deriving health indicators of rotating machines, enabling early fault detection
and continuous monitoring of condition evolution. This approach relies on a
classifier-free diffusion model trained using healthy samples and a few
anomalies. This model generates healthy samples. and by comparing the
differences between the original samples and the generated ones in the envelope
spectrum, we construct an anomaly map that clearly identifies faults. Health
indicators are then derived, which can explain the fault types and mitigate
noise interference. Comparative studies on two cases demonstrate that the
proposed method offers superior health monitoring effectiveness and robustness
compared to baseline models.

摘要：推導旋轉機器的健康指標對於其維護至關重要。然而，這個過程對普遍採用的智能方法來說具有挑戰性，因為它們可能會採用整個資料分佈，不僅會引入雜訊干擾，而且缺乏可解釋性。為了解決這些問題，我們提出了一種基於擴散的弱監督式方法，用於推導旋轉機器的健康指標，實現早期故障檢測和狀態演變的持續監控。這種方法依賴於使用健康樣本和一些異常值訓練的無分類器擴散模型。這個模型會產生健康樣本。並且通過比較封套譜中原始樣本和生成樣本之間的差異，我們構建了一個異常圖，可以清楚地識別故障。然後推導出健康指標，可以解釋故障類型並減輕雜訊干擾。對兩個案例的比較研究表明，與基準模型相比，所提出的方法提供了卓越的健康監控有效性和魯棒性。

##### **A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction**
2409.02145v1 by Zekang Yang, Hong Liu, Xiangdong Wang

Computer-aided cancer survival risk prediction plays an important role in the
timely treatment of patients. This is a challenging weakly supervised ordinal
regression task associated with multiple clinical factors involved such as
pathological images, genomic data and etc. In this paper, we propose a new
training method, multimodal object-level contrast learning, for cancer survival
risk prediction. First, we construct contrast learning pairs based on the
survival risk relationship among the samples in the training sample set. Then
we introduce the object-level contrast learning method to train the survival
risk predictor. We further extend it to the multimodal scenario by applying
cross-modal constrast. Considering the heterogeneity of pathological images and
genomics data, we construct a multimodal survival risk predictor employing
attention-based and self-normalizing based nerural network respectively.
Finally, the survival risk predictor trained by our proposed method outperforms
state-of-the-art methods on two public multimodal cancer datasets for survival
risk prediction.

摘要：電腦輔助癌症存活風險預測在病患的及時治療中扮演著重要的角色。這是一個困難的弱監督序數回歸任務，與多重臨床因素有關，例如病理圖像、基因組數據等。在本文中，我們提出了一種新的訓練方法，多模態物件層級對比學習，用於癌症存活風險預測。首先，我們根據訓練樣本集中樣本之間的存活風險關係建立對比學習對。接著，我們引入物件層級對比學習方法來訓練存活風險預測器。我們進一步將其延伸至多模態場景，透過應用跨模態對比。考量到病理圖像和基因體數據的異質性，我們分別採用基於注意力的和自標準化的神經網路來建構多模態存活風險預測器。最後，我們提出的方法所訓練的存活風險預測器在兩個公開的多模態癌症資料集上，在存活風險預測方面優於最先進的方法。

##### **A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models**
2409.01596v1 by Ruben D. Fonnegra, Maria Liliana Hernández, Juan C. Caicedo, Gloria M. Díaz

Contrast-enhancement pattern analysis is critical in breast magnetic
resonance imaging (MRI) to distinguish benign from probably malignant tumors.
However, contrast-enhanced image acquisitions are time-consuming and very
expensive. As an alternative to physical acquisition, this paper proposes a
comprehensive pipeline for the generation of accurate long-term (late)
contrast-enhanced breast MRI from the early counterpart. The proposed strategy
focuses on preserving the contrast agent pattern in the enhanced regions while
maintaining visual properties in the entire synthesized images. To that end, a
novel loss function that leverages the biological behavior of contrast agent
(CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed
to optimize a pixel-attention based generative model. In addition, unlike
traditional normalization and standardization methods, we developed a new
normalization strategy that maintains the contrast enhancement pattern across
the image sequences at multiple timestamps. This ensures the prevalence of the
CA pattern after image preprocessing, unlike conventional approaches.
Furthermore, in order to objectively evaluate the clinical quality of the
synthesized images, two metrics are also introduced to measure the differences
between the TI curves of enhanced regions of the acquired and synthesized
images. The experimental results showed that the proposed strategy generates
images that significantly outperform diagnostic quality in contrast-enhanced
regions while maintaining the spatial features of the entire image. This
results suggest a potential use of synthetic late enhanced images generated via
deep learning in clinical scenarios.

摘要：對比增強模式分析在乳房磁共振影像 (MRI) 中至關重要，可用於區分良性腫瘤和可能是惡性腫瘤。
然而，對比增強影像的擷取非常耗時且昂貴。作為物理擷取的替代方案，本文提出了一個全面的管道，用於從早期對應物生成準確的長期（晚期）對比增強乳房 MRI。所提出的策略著重於在增強區域中保留對比劑模式，同時在整個合成影像中維持視覺屬性。為此，提出了一種新穎的損失函數，利用對比劑 (CA) 在組織中的生物行為（由時間強度 (TI) 增強曲線給出），以最佳化基於像素注意力的生成模型。此外，與傳統的正規化和標準化方法不同，我們開發了一種新的正規化策略，可在多個時間戳的影像序列中維持對比增強模式。這確保了影像前處理後 CA 模式的普遍性，這與傳統方法不同。此外，為了客觀評估合成影像的臨床品質，還引入了兩個指標來測量擷取和合成影像的增強區域的 TI 曲線之間的差異。實驗結果顯示，所提出的策略產生的影像在對比增強區域中的診斷品質明顯優於其他影像，同時維持了整個影像的空間特徵。這些結果表明，在臨床場景中，通過深度學習生成的合成晚期增強影像具有潛在用途。

##### **Kvasir-VQA: A Text-Image Pair GI Tract Dataset**
2409.01437v1 by Sushant Gautam, Andrea Storås, Cise Midoglu, Steven A. Hicks, Vajira Thambawita, Pål Halvorsen, Michael A. Riegler

We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and
Kvasir-Instrument datasets, augmented with question-and-answer annotations to
facilitate advanced machine learning tasks in Gastrointestinal (GI)
diagnostics. This dataset comprises 6,500 annotated images spanning various GI
tract conditions and surgical instruments, and it supports multiple question
types including yes/no, choice, location, and numerical count. The dataset is
intended for applications such as image captioning, Visual Question Answering
(VQA), text-based generation of synthetic medical images, object detection, and
classification. Our experiments demonstrate the dataset's effectiveness in
training models for three selected tasks, showcasing significant applications
in medical image analysis and diagnostics. We also present evaluation metrics
for each task, highlighting the usability and versatility of our dataset. The
dataset and supporting artifacts are available at
https://datasets.simula.no/kvasir-vqa.

摘要：我們引進 Kvasir-VQA，一個由 HyperKvasir 和 Kvasir-Instrument 資料集衍生的延伸資料集，並加入問題與解答註解，以促進在胃腸 (GI) 診斷中的進階機器學習任務。此資料集包含 6,500 個註解影像，涵蓋各種 GI 道狀況和手術器械，並且支援包括是非題、選擇題、位置和數字計數等多種類型的問題。此資料集適用於影像標題、視覺問答 (VQA)、合成醫學影像的文字生成、物件偵測和分類等應用程式。我們的實驗證明此資料集在訓練三個選定任務的模型中具有成效，展示了在醫學影像分析和診斷中重要的應用。我們也為每個任務提供評估指標，突顯我們資料集的可用性和多功能性。此資料集和支援工件可於 https://datasets.simula.no/kvasir-vqa 取得。

##### **EEG-Language Modeling for Pathology Detection**
2409.07480v1 by Sam Gijsen, Kerstin Ritter

Multimodal language modeling constitutes a recent breakthrough which
leverages advances in large language models to pretrain capable multimodal
models. The integration of natural language during pretraining has been shown
to significantly improve learned representations, particularly in computer
vision. However, the efficacy of multimodal language modeling in the realm of
functional brain data, specifically for advancing pathology detection, remains
unexplored. This study pioneers EEG-language models trained on clinical reports
and 15000 EEGs. We extend methods for multimodal alignment to this novel domain
and investigate which textual information in reports is useful for training
EEG-language models. Our results indicate that models learn richer
representations from being exposed to a variety of report segments, including
the patient's clinical history, description of the EEG, and the physician's
interpretation. Compared to models exposed to narrower clinical text
information, we find such models to retrieve EEGs based on clinical reports
(and vice versa) with substantially higher accuracy. Yet, this is only observed
when using a contrastive learning approach. Particularly in regimes with few
annotations, we observe that representations of EEG-language models can
significantly improve pathology detection compared to those of EEG-only models,
as demonstrated by both zero-shot classification and linear probes. In sum,
these results highlight the potential of integrating brain activity data with
clinical text, suggesting that EEG-language models represent significant
progress for clinical applications.

摘要：多模态语言建模是一项最近的突破，它利用大型语言模型的进步来预训练有能力的多模态模型。事实证明，在预训练期间整合自然语言可以显著改善学习到的表征，特别是在计算机视觉中。然而，多模态语言建模在功能性脑数据领域中的功效，特别是对于推进病理检测，仍然未得到探索。本研究开创了在临床报告和 15000 个脑电图上训练的脑电图语言模型。我们将多模态对齐的方法扩展到这个新领域，并研究报告中的哪些文本信息对于训练脑电图语言模型是有用的。我们的结果表明，通过接触各种报告片段（包括患者的病史、脑电图描述和医生的解释），模型可以学习到更丰富的表征。与接触到较窄的临床文本信息的模型相比，我们发现此类模型可以根据临床报告（反之亦然）检索脑电图，其准确性大大提高。然而，只有在使用对比学习方法时才会观察到这一点。特别是在注释较少的方案中，我们观察到脑电图语言模型的表征可以显著改善病理检测，与仅脑电图模型相比，零样本分类和线性探针都证明了这一点。总之，这些结果突出了将脑活动数据与临床文本相结合的潜力，表明脑电图语言模型代表了临床应用的重大进展。

##### **SeCo-INR: Semantically Conditioned Implicit Neural Representations for Improved Medical Image Super-Resolution**
2409.01013v1 by Mevan Ekanayake, Zhifeng Chen, Gary Egan, Mehrtash Harandi, Zhaolin Chen

Implicit Neural Representations (INRs) have recently advanced the field of
deep learning due to their ability to learn continuous representations of
signals without the need for large training datasets. Although INR methods have
been studied for medical image super-resolution, their adaptability to
localized priors in medical images has not been extensively explored. Medical
images contain rich anatomical divisions that could provide valuable local
prior information to enhance the accuracy and robustness of INRs. In this work,
we propose a novel framework, referred to as the Semantically Conditioned INR
(SeCo-INR), that conditions an INR using local priors from a medical image,
enabling accurate model fitting and interpolation capabilities to achieve
super-resolution. Our framework learns a continuous representation of the
semantic segmentation features of a medical image and utilizes it to derive the
optimal INR for each semantic region of the image. We tested our framework
using several medical imaging modalities and achieved higher quantitative
scores and more realistic super-resolution outputs compared to state-of-the-art
methods.

摘要：隱式神經表徵 (INR) 近期由於其無需大量訓練資料集就能學習訊號的連續表徵的能力，而推動了深度學習領域的進展。儘管 INR 方法已被研究用於醫學影像超解析度，但其對於醫學影像中局部先驗的適應性尚未被廣泛探討。醫學影像包含豐富的解剖學區分，這些區分可以提供有價值的局部先驗資訊，以增強 INR 的準確性和穩健性。在這項工作中，我們提出了一個新穎的架構，稱為語義條件 INR (SeCo-INR)，它使用醫學影像中的局部先驗來調整 INR，實現準確的模型擬合和插值能力，以實現超解析度。我們的架構學習醫學影像的語意分割特徵的連續表徵，並利用它為影像的每個語意區域推導最佳 INR。我們使用多種醫學影像方式測試我們的架構，並與最先進的方法相比，達到了更高的量化評分和更逼真的超解析度輸出。

##### **Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation**
2409.00873v1 by Sajib Acharjee Dip, Kazi Hasan Ibn Arif, Uddip Acharjee Shuvo, Ishtiaque Ahmed Khan, Na Meng

In the realm of dermatology, the complexity of diagnosing skin conditions
manually necessitates the expertise of dermatologists. Accurate identification
of various skin ailments, ranging from cancer to inflammatory diseases, is
paramount. However, existing artificial intelligence (AI) models in dermatology
face challenges, particularly in accurately diagnosing diseases across diverse
skin tones, with a notable performance gap in darker skin. Additionally, the
scarcity of publicly available, unbiased datasets hampers the development of
inclusive AI diagnostic tools. To tackle the challenges in accurately
predicting skin conditions across diverse skin tones, we employ a
transfer-learning approach that capitalizes on the rich, transferable knowledge
from various image domains. Our method integrates multiple pre-trained models
from a wide range of sources, including general and specific medical images, to
improve the robustness and inclusiveness of the skin condition predictions. We
rigorously evaluated the effectiveness of these models using the Diverse
Dermatology Images (DDI) dataset, which uniquely encompasses both
underrepresented and common skin tones, making it an ideal benchmark for
assessing our approach. Among all methods, Med-ViT emerged as the top performer
due to its comprehensive feature representation learned from diverse image
sources. To further enhance performance, we conducted domain adaptation using
additional skin image datasets such as HAM10000. This adaptation significantly
improved model performance across all models.

摘要：<paragraph>在皮肤病学领域，人工诊断皮肤状况的复杂性需要皮肤科医师的专业知识。从癌症到炎症性疾病，对各种皮肤疾病的准确识别至关重要。然而，现有的皮肤病学人工智能 (AI) 模型面临挑战，尤其是在准确诊断不同肤色的疾病时，在较深的肤色上存在明显的性能差距。此外，公开可用的无偏数据集的稀缺性阻碍了包容性 AI 诊断工具的开发。为了应对准确预测不同肤色皮肤状况的挑战，我们采用了一种迁移学习方法，该方法利用了来自各种图像域的丰富可转移知识。我们的方法集成了来自广泛来源的多个预训练模型，包括一般和特定的医学图像，以提高皮肤状况预测的稳健性和包容性。我们使用 Diverse Dermatology Images (DDI) 数据集严格评估了这些模型的有效性，该数据集独特地包含了代表性不足和常见的肤色，使其成为评估我们方法的理想基准。在所有方法中，Med-ViT 由于其从各种图像来源中学到的综合特征表示而成为表现最好的方法。为了进一步提高性能，我们使用 HAM10000 等其他皮肤图像数据集进行了域适应。这种适应显着提高了所有模型的模型性能。</paragraph>

##### **Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering**
2409.00861v1 by Derian Boer, Fabian Koch, Stefan Kramer

Large Language Models (LLMs) frequently lack domain-specific knowledge and
even fine-tuned models tend to hallucinate. Hence, more reliable models that
can include external knowledge are needed. We present a pipeline, 4StepFocus,
and specifically a preprocessing step, that can substantially improve the
answers of LLMs. This is achieved by providing guided access to external
knowledge making use of the model's ability to capture relational context and
conduct rudimentary reasoning by themselves. The method narrows down
potentially correct answers by triplets-based searches in a semi-structured
knowledge base in a direct, traceable fashion, before switching to latent
representations for ranking those candidates based on unstructured data. This
distinguishes it from related methods that are purely based on latent
representations. 4StepFocus consists of the steps: 1) Triplet generation for
extraction of relational data by an LLM, 2) substitution of variables in those
triplets to narrow down answer candidates employing a knowledge graph, 3)
sorting remaining candidates with a vector similarity search involving
associated non-structured data, 4) reranking the best candidates by the LLM
with background data provided. Experiments on a medical, a product
recommendation, and an academic paper search test set demonstrate that this
approach is indeed a powerful augmentation. It not only adds relevant traceable
background information from information retrieval, but also improves
performance considerably in comparison to state-of-the-art methods. This paper
presents a novel, largely unexplored direction and therefore provides a wide
range of future work opportunities. Used source code is available at
https://github.com/kramerlab/4StepFocus.

摘要：大型語言模型 (LLM) 經常缺乏特定領域的知識，即使經過微調的模型也容易產生幻覺。因此，需要更多可靠的模型來納入外部知識。我們提出了一個流程 4StepFocus，特別是預處理步驟，可以大幅改善 LLM 的答案。這是透過提供受引導的外部知識存取，利用模型自行擷取關聯性脈絡和進行基本推理的能力來實現的。此方法透過在半結構化知識庫中進行基於三元組的搜尋，以直接且可追蹤的方式縮小潛在正確答案的範圍，然後再切換到潛在表徵，根據非結構化資料對這些候選答案進行排名。這與純粹基於潛在表徵的相關方法有所區別。4StepFocus 包含以下步驟：1) 由 LLM 進行三元組產生以擷取關聯資料，2) 在這些三元組中替換變數，以採用知識圖表縮小答案候選範圍，3) 使用涉及關聯非結構化資料的向量相似性搜尋對剩餘候選答案進行排序，4) 由 LLM 重新對最佳候選答案進行排名，並提供背景資料。在醫療、產品推薦和學術論文搜尋測試集中進行的實驗證明，這種方法確實是一種強大的擴充。它不僅增加了來自資訊檢索的相关可追蹤背景資訊，而且與最先進的方法相比，也大幅提升了效能。本文提出了一個新穎且鮮少探索的方向，因此提供了廣泛的未來工作機會。使用的原始碼可在 https://github.com/kramerlab/4StepFocus 取得。

##### **Building FKG.in: a Knowledge Graph for Indian Food**
2409.00830v1 by Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain

This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.

摘要：本文提出了一個知識工程和多語言語義推理技術的本体設計，用於建立一個自動化系統，以知識圖譜的形式吸收印度料理的烹飪資訊。重點在於設計智慧方法，以推導本体設計，並全面擷取關於食物、食譜、食材、烹飪特性，以及最重要的營養的知識，並擴大規模。我們在這個研討會論文中介紹了我們正在進行的工作，詳細描述了整理印度料理知識相關的挑戰，並提出了我們的高階本体設計。我們也提出了一種新的工作流程，它使用 AI、LLM 和語言技術，從公共領域的食譜部落格網站中整理資訊，以建立印度料理的知識圖譜。本文提出的知識整理方法是通用的，可以複製到任何領域。設計與應用無關，可用於 AI 驅動的智慧分析、建立個人化數位健康推薦系統，以及使用使用者資訊、食物生物化學、地理資訊、農業資訊等脈絡資訊，來補充印度料理的知識圖譜。

##### **AgGym: An agricultural biotic stress simulation environment for ultra-precision management planning**
2409.00735v1 by Mahsa Khosravi, Matthew Carroll, Kai Liang Tan, Liza Van der Laan, Joscif Raigne, Daren S. Mueller, Arti Singh, Aditya Balu, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar

Agricultural production requires careful management of inputs such as
fungicides, insecticides, and herbicides to ensure a successful crop that is
high-yielding, profitable, and of superior seed quality. Current
state-of-the-art field crop management relies on coarse-scale crop management
strategies, where entire fields are sprayed with pest and disease-controlling
chemicals, leading to increased cost and sub-optimal soil and crop management.
To overcome these challenges and optimize crop production, we utilize machine
learning tools within a virtual field environment to generate localized
management plans for farmers to manage biotic threats while maximizing profits.
Specifically, we present AgGym, a modular, crop and stress agnostic simulation
framework to model the spread of biotic stresses in a field and estimate yield
losses with and without chemical treatments. Our validation with real data
shows that AgGym can be customized with limited data to simulate yield outcomes
under various biotic stress conditions. We further demonstrate that deep
reinforcement learning (RL) policies can be trained using AgGym for designing
ultra-precise biotic stress mitigation strategies with potential to increase
yield recovery with less chemicals and lower cost. Our proposed framework
enables personalized decision support that can transform biotic stress
management from being schedule based and reactive to opportunistic and
prescriptive. We also release the AgGym software implementation as a community
resource and invite experts to contribute to this open-sourced publicly
available modular environment framework. The source code can be accessed at:
https://github.com/SCSLabISU/AgGym.

摘要：農業生產需要小心管理輸入，例如殺菌劑、殺蟲劑和除草劑，以確保作物成功、高產、有利可圖且具有優良的種子品質。目前最先進的田間作物管理依賴於粗略的作物管理策略，其中整個田地都噴灑了控制病蟲害的化學物質，導致成本增加和土壤和作物管理不佳。為了克服這些挑戰並優化作物生產，我們在虛擬田間環境中利用機器學習工具為農民生成局部管理計畫，以管理生物威脅並同時最大化利潤。具體來說，我們提出了 AgGym，一個模組化、作物和壓力不可知的模擬架構，用於模擬田間生物壓力的擴散，並估算有和沒有化學處理的產量損失。我們使用真實數據進行驗證，顯示 AgGym 可以使用有限的數據進行自訂，以模擬各種生物壓力條件下的產量結果。我們進一步證明，深度強化學習 (RL) 政策可以使用 AgGym 進行訓練，以設計超精確的生物壓力緩解策略，並有可能以更少的化學物質和更低的成本增加產量恢復。我們提出的架構啟用了個人化決策支援，可以將生物壓力管理從基於時間表和被動轉變為機會主義和規範性。我們還將 AgGym 軟體實作作為社區資源釋出，並邀請專家為這個開放原始碼且公開可用的模組化環境架構做出貢獻。可以在以下位置取得原始碼：https://github.com/SCSLabISU/AgGym。

##### **LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset**
2409.00726v1 by Zhaojie Fang, Xiao Yu, Guanyu Zhou, Ke Zhuang, Yifei Chen, Ruiquan Ge, Changmiao Wang, Gangyong Jia, Qing Wu, Juan Ye, Maimaiti Nuliqiman, Peifang Xu, Ahmed Elazab

Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise
identification of ocular diseases using sodium fluorescein, which can be
potentially harmful. Existing research has developed methods to generate UWF-FA
from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the
adverse reactions associated with injections. However, these methods have been
less effective in producing high-quality late-phase UWF-FA, particularly in
lesion areas and fine details. Two primary challenges hinder the generation of
high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and
early/late-phase UWF-FA datasets, and the need for realistic generation at
lesion sites and potential blood leakage regions. This study introduces an
improved latent diffusion model framework to generate high-quality late-phase
UWF-FA from limited paired UWF images. To address the challenges as mentioned
earlier, our approach employs a module utilizing Cross-temporal Regional
Difference Loss, which encourages the model to focus on the differences between
early and late phases. Additionally, we introduce a low-frequency enhanced
noise strategy in the diffusion forward process to improve the realism of
medical images. To further enhance the mapping capability of the variational
autoencoder module, especially with limited datasets, we implement a Gated
Convolutional Encoder to extract additional information from conditional
images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein
Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase
UWF-FA and achieves state-of-the-art results compared to other existing methods
when working with limited datasets. Our source code is available at:
https://github.com/Tinysqua/****.

摘要：超廣角螢光血管造影（UWF-FA）使用可能具有潛在危害的鈉螢光素，可精確識別眼疾。現有研究已開發出從超廣角掃描雷射眼科鏡（UWF-SLO）產生 UWF-FA 的方法，以減少與注射相關的不良反應。然而，這些方法在產生高品質的後期 UWF-FA 方面效果較差，特別是在病灶區域和精細細節方面。產生高品質後期 UWF-FA 面臨兩項主要挑戰：配對的 UWF-SLO 和早期/後期 UWF-FA 資料集稀少，以及需要在病灶部位和潛在出血區域進行逼真的產生。本研究引進一種改良的潛在擴散模型架構，從有限配對的 UWF 影像產生高品質的後期 UWF-FA。為了應對前面提到的挑戰，我們的方法採用一個模組，利用跨時間區域差異損失，鼓勵模型專注於早期和後期之間的差異。此外，我們在擴散前向過程中引進一種低頻增強雜訊策略，以改善醫學影像的真實性。為了進一步增強變異自動編碼器模組的對應能力，特別是在資料集有限的情況下，我們實作一個閘控卷積編碼器，從條件影像中萃取額外資訊。我們針對超廣角後期螢光血管造影（LPUWF-LDM）的潛在擴散模型有效重建後期 UWF-FA 中的精細細節，並在使用有限資料集時，與其他現有方法相比，達到最先進的結果。我們的原始碼可在以下網址取得：
https://github.com/Tinysqua/****。

##### **BUET Multi-disease Heart Sound Dataset: A Comprehensive Auscultation Dataset for Developing Computer-Aided Diagnostic Systems**
2409.00724v1 by Shams Nafisa Ali, Afia Zahin, Samiul Based Shuvo, Nusrat Binta Nizam, Shoyad Ibn Sabur Khan Nuhash, Sayeed Sajjad Razin, S. M. Sakeef Sani, Farihin Rahman, Nawshad Binta Nizam, Farhat Binte Azam, Rakib Hossen, Sumaiya Ohab, Nawsabah Noor, Taufiq Hasan

Cardiac auscultation, an integral tool in diagnosing cardiovascular diseases
(CVDs), often relies on the subjective interpretation of clinicians, presenting
a limitation in consistency and accuracy. Addressing this, we introduce the
BUET Multi-disease Heart Sound (BMD-HS) dataset - a comprehensive and
meticulously curated collection of heart sound recordings. This dataset,
encompassing 864 recordings across five distinct classes of common heart
sounds, represents a broad spectrum of valvular heart diseases, with a focus on
diagnostically challenging cases. The standout feature of the BMD-HS dataset is
its innovative multi-label annotation system, which captures a diverse range of
diseases and unique disease states. This system significantly enhances the
dataset's utility for developing advanced machine learning models in automated
heart sound classification and diagnosis. By bridging the gap between
traditional auscultation practices and contemporary data-driven diagnostic
methods, the BMD-HS dataset is poised to revolutionize CVD diagnosis and
management, providing an invaluable resource for the advancement of cardiac
health research. The dataset is publicly available at this link:
https://github.com/mHealthBuet/BMD-HS-Dataset.

摘要：心臟聽診是診斷心血管疾病 (CVD) 的一項整合工具，通常依賴於臨床醫師的主觀詮釋，在一致性和準確性方面存在限制。為了解決這個問題，我們引入了 BUET 多重疾病心音 (BMD-HS) 資料集，這是一個全面且經過精心策劃的心音錄音資料集。此資料集包含五種常見心音的 864 個錄音，代表了廣泛的心瓣膜疾病，重點在於診斷困難的病例。BMD-HS 資料集的突出特點是其創新的多標籤註解系統，它涵蓋了各種疾病和獨特的疾病狀態。這個系統顯著增強了資料集在開發自動心音分類和診斷中進階機器學習模型的效用。透過彌合傳統聽診實務與當代資料驅動診斷方法之間的差距，BMD-HS 資料集準備好革新心血管疾病的診斷和管理，為心臟健康研究的進展提供寶貴的資源。此資料集可透過以下連結公開取得：https://github.com/mHealthBuet/BMD-HS-Dataset。

##### **Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images**
2409.00718v1 by Pragya Gupta, Subhamoy Mandal, Debashree Guha, Debjani Chakraborty

Automatic diagnosis techniques have evolved to identify age-related macular
degeneration (AMD) by employing single modality Fundus images or optical
coherence tomography (OCT). To classify ocular diseases, fundus and OCT images
are the most crucial imaging modalities used in the clinical setting. Most deep
learning-based techniques are established on a single imaging modality, which
contemplates the ocular disorders to a specific extent and disregards other
modality that comprises exhaustive information among distinct imaging
modalities. This paper proposes a modality-specific multiscale color space
embedding integrated with the attention mechanism based on transfer learning
for classification (MCGAEc), which can efficiently extract the distinct
modality information at various scales using the distinct color spaces. In this
work, we first introduce the modality-specific multiscale color space encoder
model, which includes diverse feature representations by integrating distinct
characteristic color spaces on a multiscale into a unified framework. The
extracted features from the prior encoder module are incorporated with the
attention mechanism to extract the global features representation, which is
integrated with the prior extracted features and transferred to the random
forest classifier for the classification of AMD. To analyze the performance of
the proposed MCGAEc method, a publicly available multi-modality dataset from
Project Macula for AMD is utilized and compared with the existing models.

摘要：自動診斷技術已演進到能透過使用單一模式眼底影像或光學相干斷層掃描 (OCT) 來辨識年齡相關性黃斑部病變 (AMD)。為了分類眼疾，眼底和 OCT 影像是臨床環境中使用最關鍵的影像模式。大多數基於深度學習的技術建立在單一影像模式上，它在一定程度上考量了眼疾，卻忽略了其他模式，而其他模式包含了不同影像模式之間的詳盡資訊。本文提出了一種模式特定的多尺度色彩空間嵌入整合，並基於用於分類的轉移學習的注意力機制 (MCGAEc)，它能使用不同的色彩空間在不同的尺度上有效提取不同的模式資訊。在這項工作中，我們首先介紹了模式特定的多尺度色彩空間編碼器模型，它透過將不同的特徵色彩空間整合到多尺度中，來納入不同的特徵表徵到一個統一的架構中。從先前的編碼器模組中提取的特徵與注意力機制結合，以提取全域特徵表徵，它與先前提取的特徵整合，並轉移到隨機森林分類器，以進行 AMD 分類。為了分析所提出的 MCGAEc 方法的效能，我們利用了來自 Project Macula for AMD 的公開多模式資料集，並與現有模型進行比較。

##### **Curriculum Prompting Foundation Models for Medical Image Segmentation**
2409.00695v1 by Xiuqi Zheng, Yuhang Zhang, Haoran Zhang, Hongrui Liang, Xueqi Bao, Zhuqing Jiang, Qicheng Lao

Adapting large pre-trained foundation models, e.g., SAM, for medical image
segmentation remains a significant challenge. A crucial step involves the
formulation of a series of specialized prompts that incorporate specific
clinical instructions. Past works have been heavily reliant on a singular type
of prompt for each instance, necessitating manual input of an ideally correct
prompt, which is less efficient. To tackle this issue, we propose to utilize
prompts of different granularity, which are sourced from original images to
provide a broader scope of clinical insights. However, combining prompts of
varying types can pose a challenge due to potential conflicts. In response, we
have designed a coarse-to-fine mechanism, referred to as curriculum prompting,
that progressively integrates prompts of different types. Through extensive
experiments on three public medical datasets across various modalities, we
demonstrate the effectiveness of our proposed approach, which not only
automates the prompt generation process but also yields superior performance
compared to other SAM-based medical image segmentation methods. Code is
available at: https://github.com/AnnaZzz-zxq/Curriculum-Prompting.

摘要：調整大型預訓練基礎模型（例如 SAM）以進行醫學影像分割仍是一項重大挑戰。關鍵步驟涉及制定一系列包含特定臨床說明的專門提示。過去的工作在很大程度上依賴於每個例項的單一提示類型，這需要手動輸入理想的正確提示，效率較低。為了解決這個問題，我們建議利用不同粒度的提示，這些提示來自原始影像，以提供更廣泛的臨床見解。然而，由於潛在衝突，結合不同類型的提示可能會構成挑戰。為了解決這個問題，我們設計了一種由粗到細的機制，稱為課程提示，它逐步整合不同類型的提示。透過對各種模式下的三個公共醫學資料集進行廣泛的實驗，我們證明了我們提出的方法的有效性，它不僅自動化提示生成過程，而且與其他基於 SAM 的醫學影像分割方法相比，還產生了更好的效能。程式碼可在以下位置取得：https://github.com/AnnaZzz-zxq/Curriculum-Prompting。

##### **Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors**
2409.00544v1 by Jacqueline Lammert, Nicole Pfarr, Leonid Kuligin, Sonja Mathes, Tobias Dreyer, Luise Modersohn, Patrick Metzger, Dyke Ferber, Jakob Nikolas Kather, Daniel Truhn, Lisa Christine Adams, Keno Kyrill Bressem, Sebastian Lange, Kristina Schwamborn, Martin Boeker, Marion Kiechle, Ulrich A. Schatz, Holger Bronger, Maximilian Tschochohei

Rare gynecological tumors (RGTs) present major clinical challenges due to
their low incidence and heterogeneity. The lack of clear guidelines leads to
suboptimal management and poor prognosis. Molecular tumor boards accelerate
access to effective therapies by tailoring treatment based on biomarkers,
beyond cancer type. Unstructured data that requires manual curation hinders
efficient use of biomarker profiling for therapy matching. This study explores
the use of large language models (LLMs) to construct digital twins for
precision medicine in RGTs.
  Our proof-of-concept digital twin system integrates clinical and biomarker
data from institutional and published cases (n=21) and literature-derived data
(n=655 publications with n=404,265 patients) to create tailored treatment plans
for metastatic uterine carcinosarcoma, identifying options potentially missed
by traditional, single-source analysis. LLM-enabled digital twins efficiently
model individual patient trajectories. Shifting to a biology-based rather than
organ-based tumor definition enables personalized care that could advance RGT
management and thus enhance patient outcomes.

摘要：罕見婦科腫瘤 (RGT) 由於其低發生率和異質性，對臨床帶來重大挑戰。缺乏明確的指引導致次佳管理和不良預後。分子腫瘤委員會透過根據生物標記客製化治療，加速取得有效療法，超越癌症類型。需要手動整理的非結構化資料阻礙了生物標記分析在療法配對中的有效使用。本研究探討使用大型語言模型 (LLM) 為 RGT 的精準醫療建構數位雙胞胎。
我們的概念驗證數位雙胞胎系統整合了來自機構和已發表的案例 (n=21) 的臨床和生物標記資料，以及來自文獻的資料 (n=655 篇出版物，n=404,265 名患者)，為轉移性子宮肉瘤癌制定客製化治療計畫，找出傳統單一來源分析可能遺漏的選項。LLM 啟用的數位雙胞胎有效地模擬個別患者的軌跡。從基於器官的腫瘤定義轉變為基於生物學的定義，能實現個人化照護，進而提升 RGT 管理並改善患者預後。

##### **Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders**
2409.00391v1 by Georgios Ioannides, Adrian Kieback, Aman Chadha, Aaron Elkins

Speech-based depression detection poses significant challenges for automated
detection due to its unique manifestation across individuals and data scarcity.
Addressing these challenges, we introduce DAAMAudioCNNLSTM and
DAAMAudioTransformer, two parameter efficient and explainable models for audio
feature extraction and depression detection. DAAMAudioCNNLSTM features a novel
CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM),
focusing dynamically on informative speech segments. DAAMAudioTransformer,
leveraging a transformer encoder in place of the CNN-LSTM architecture,
incorporates the same DAAM module for enhanced attention and interpretability.
These approaches not only enhance detection robustness and interpretability but
also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro
score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the
DAIC-WOZ dataset, without reliance on supplementary information such as vowel
positions and speaker information during training/validation as in previous
approaches. Both models' significant explainability and efficiency in
leveraging speech signals for depression detection represent a leap towards
more reliable, clinically useful diagnostic tools, promising advancements in
speech and mental health care. To foster further research in this domain, we
make our code publicly available.

摘要：語音型憂鬱檢測對自動化檢測來說是一大挑戰，因為它在不同個體間的表現獨特，且資料稀少。為了應對這些挑戰，我們引入了 DAAMAudioCNNLSTM 和 DAAMAudioTransformer，這兩個參數有效且可解釋的模型，用於音訊特徵萃取和憂鬱檢測。DAAMAudioCNNLSTM 採用創新的 CNN-LSTM 架構，搭配多頭密度自適應注意力機制 (DAAM)，動態關注於有意義的語音區段。DAAMAudioTransformer 利用Transformer編碼器取代 CNN-LSTM 架構，並納入相同的 DAAM 模組，以增強注意力和可解釋性。這些方法不僅增強了檢測的穩健性和可解釋性，還達到了最先進的效能：DAAMAudioCNNLSTM 的 F1 巨觀分數為 0.702，DAAMAudioTransformer 在 DAIC-WOZ 資料集上的 F1 巨觀分數為 0.72，在訓練/驗證期間不依賴於輔助資訊，例如母音位置和說話者資訊，這與先前的做法不同。這兩個模型在利用語音訊號進行憂鬱檢測方面具有顯著的可解釋性和效率，代表著朝向更可靠、臨床上有用的診斷工具邁進了一大步，並預示著語音和心理保健的進步。為了促進這個領域的進一步研究，我們公開了我們的程式碼。

##### **Objective Features Extracted from Motor Activity Time Series for Food Addiction Analysis Using Machine Learning**
2409.00310v1 by Mikhail Borisenkov, Andrei Velichko, Maksim Belyaev, Dmitry Korzun, Tatyana Tserne, Larisa Bakutova, Denis Gubin

This study investigates machine learning algorithms to identify objective
features for diagnosing food addiction (FA) and assessing confirmed symptoms
(SC). Data were collected from 81 participants (mean age: 21.5 years, range:
18-61 years, women: 77.8%) whose FA and SC were measured using the Yale Food
Addiction Scale (YFAS). Participants provided demographic and anthropometric
data, completed the YFAS, the Zung Self-Rating Depression Scale, and the Dutch
Eating Behavior Questionnaire, and wore an actimeter on the non-dominant wrist
for a week to record motor activity. Analysis of the actimetric data identified
significant statistical and entropy-based features that accurately predicted FA
and SC using ML. The Matthews correlation coefficient (MCC) was the primary
metric. Activity-related features were more effective for FA prediction
(MCC=0.88) than rest-related features (MCC=0.68). For SC, activity segments
yielded MCC=0.47, rest segments MCC=0.38, and their combination MCC=0.51.
Significant correlations were also found between actimetric features related to
FA, emotional, and restrained eating behaviors, supporting the model's
validity. Our results support the concept of a human bionic suite composed of
IoT devices and ML sensors, which implements health digital assistance with
real-time monitoring and analysis of physiological indicators related to FA and
SC.

摘要：本研究調查機器學習演算法，以識別診斷食物成癮 (FA) 和評估已確認症狀 (SC) 的客觀特徵。資料來自 81 位參與者（平均年齡：21.5 歲，範圍：18-61 歲，女性：77.8%），其 FA 和 SC 是使用耶魯食物成癮量表 (YFAS) 測量的。參與者提供了人口統計和人類測量資料，完成了 YFAS、Zung 自我評量憂鬱量表和荷蘭飲食行為問卷，並在非慣用手腕上佩戴活動計一週以記錄運動活動。對活動計資料的分析識別出重要的統計和基於熵的特徵，這些特徵使用機器學習準確預測了 FA 和 SC。馬修斯相關係數 (MCC) 是主要指標。與休息相關的特徵（MCC=0.68）相比，與活動相關的特徵對於 FA 預測更有效（MCC=0.88）。對於 SC，活動區段產生的 MCC=0.47，休息區段 MCC=0.38，組合後 MCC=0.51。還發現與 FA、情緒和受限飲食行為相關的活動計特徵之間存在顯著相關性，這支持了模型的有效性。我們的結果支持由物聯網裝置和機器學習感測器組成的人體仿生套件的概念，該套件實作了健康數位協助，並對與 FA 和 SC 相關的生理指標進行即時監控和分析。

##### **Exploring the Effect of Explanation Content and Format on User Comprehension and Trust**
2408.17401v1 by Antonio Rago, Bence Palfi, Purin Sukpanichnant, Hannibal Nabli, Kavyesh Vivek, Olga Kostopoulou, James Kinross, Francesca Toni

In recent years, various methods have been introduced for explaining the
outputs of "black-box" AI models. However, it is not well understood whether
users actually comprehend and trust these explanations. In this paper, we focus
on explanations for a regression tool for assessing cancer risk and examine the
effect of the explanations' content and format on the user-centric metrics of
comprehension and trust. Regarding content, we experiment with two explanation
methods: the popular SHAP, based on game-theoretic notions and thus potentially
complex for everyday users to comprehend, and occlusion-1, based on feature
occlusion which may be more comprehensible. Regarding format, we present SHAP
explanations as charts (SC), as is conventional, and occlusion-1 explanations
as charts (OC) as well as text (OT), to which their simpler nature also lends
itself. The experiments amount to user studies questioning participants, with
two different levels of expertise (the general population and those with some
medical training), on their subjective and objective comprehension of and trust
in explanations for the outputs of the regression tool. In both studies we
found a clear preference in terms of subjective comprehension and trust for
occlusion-1 over SHAP explanations in general, when comparing based on content.
However, direct comparisons of explanations when controlling for format only
revealed evidence for OT over SC explanations in most cases, suggesting that
the dominance of occlusion-1 over SHAP explanations may be driven by a
preference for text over charts as explanations. Finally, we found no evidence
of a difference between the explanation types in terms of objective
comprehension. Thus overall, the choice of the content and format of
explanations needs careful attention, since in some contexts format, rather
than content, may play the critical role in improving user experience.

摘要：<paragraph>近年來，已經引進各種方法來解釋「黑箱」AI 模型的輸出。然而，目前並不清楚使用者是否實際理解和信任這些解釋。在本文中，我們專注於評估癌症風險的回歸工具的解釋，並探討解釋的內容和格式對以使用者為中心的理解和信任指標的影響。關於內容，我們實驗了兩種解釋方法：流行的 SHAP，基於博弈論概念，因此對於日常使用者來說可能很複雜，以及基於特徵遮蔽的 occlusion-1，可能更易於理解。關於格式，我們將 SHAP 解釋呈現為圖表 (SC)，這是慣例，而將 occlusion-1 解釋呈現為圖表 (OC) 以及文字 (OT)，其較為簡單的性質也適用於此。這些實驗等同於使用者研究，詢問參與者，具有兩種不同程度的專業知識（一般民眾和具備一些醫學訓練的人），他們對回歸工具輸出解釋的主觀和客觀理解和信任。在兩項研究中，我們發現，在基於內容進行比較時，一般來說，occlusion-1 優於 SHAP 解釋，在主觀理解和信任方面有明顯的偏好。然而，在僅控制格式的情況下直接比較解釋，在大多數情況下只顯示 OT 優於 SC 解釋的證據，這表明 occlusion-1 優於 SHAP 解釋的主導地位可能是由偏好文字而非圖表作為解釋所驅動的。最後，我們沒有發現解釋類型在客觀理解方面的差異證據。因此，總體而言，對解釋的內容和格式的選擇需要仔細注意，因為在某些情況下，格式而非內容，可能在改善使用者體驗方面發揮關鍵作用。</paragraph>

##### **Deep Neural Networks for Predicting Recurrence and Survival in Patients with Esophageal Cancer After Surgery**
2409.00163v1 by Yuhan Zheng, Jessie A Elliott, John V Reynolds, Sheraz R Markar, Bartłomiej W. Papież, ENSURE study group

Esophageal cancer is a major cause of cancer-related mortality
internationally, with high recurrence rates and poor survival even among
patients treated with curative-intent surgery. Investigating relevant
prognostic factors and predicting prognosis can enhance post-operative clinical
decision-making and potentially improve patients' outcomes. In this work, we
assessed prognostic factor identification and discriminative performances of
three models for Disease-Free Survival (DFS) and Overall Survival (OS) using a
large multicenter international dataset from ENSURE study. We first employed
Cox Proportional Hazards (CoxPH) model to assess the impact of each feature on
outcomes. Subsequently, we utilised CoxPH and two deep neural network
(DNN)-based models, DeepSurv and DeepHit, to predict DFS and OS. The
significant prognostic factors identified by our models were consistent with
clinical literature, with post-operative pathologic features showing higher
significance than clinical stage features. DeepSurv and DeepHit demonstrated
comparable discriminative accuracy to CoxPH, with DeepSurv slightly
outperforming in both DFS and OS prediction tasks, achieving C-index of 0.735
and 0.74, respectively. While these results suggested the potential of DNNs as
prognostic tools for improving predictive accuracy and providing personalised
guidance with respect to risk stratification, CoxPH still remains an adequately
good prediction model, with the data used in this study.

摘要：食道癌是國際間癌症相關死亡的主要原因，即使接受根治性手術的患者，復發率高且存活率低。探討相關預後因子並預測預後，可以增進術後臨床決策制定，並可能改善患者的結果。在這項工作中，我們評估了三種模型的預後因子識別和判別效能，分別針對無疾病存活期 (DFS) 和整體存活期 (OS)，使用 ENSURE 研究的大型多中心國際數據集。我們首先採用 Cox 比例風險 (CoxPH) 模型來評估每個特徵對結果的影響。隨後，我們利用 CoxPH 和兩個基於深度神經網路 (DNN) 的模型 DeepSurv 和 DeepHit 來預測 DFS 和 OS。我們的模型識別出的顯著預後因子與臨床文獻一致，術後病理特徵顯示出比臨床分期特徵更高的顯著性。DeepSurv 和 DeepHit 展現出與 CoxPH 相當的判別準確度，DeepSurv 在 DFS 和 OS 預測任務中表現略勝一籌，分別達到 C 指數 0.735 和 0.74。雖然這些結果表明 DNN 作為預後工具的潛力，可以提高預測準確度並針對風險分層提供個人化指導，但 CoxPH 仍然是足夠好的預測模型，使用本研究中的數據。

##### **NDP: Next Distribution Prediction as a More Broad Target**
2408.17377v1 by Junhao Ruan, Abudukeyumu Abudula, Xinyu Liu, Bei Li, Yinqiao Li, Chenglong Wang, Yuchun Fan, Yuan Ge, Tong Xiao, Jingbo Zhu

Large language models (LLMs) trained on next-token prediction (NTP) paradigm
have demonstrated powerful capabilities. However, the existing NTP paradigm
contains several limitations, particularly related to planned task
complications and error propagation during inference. In our work, we extend
the critique of NTP, highlighting its limitation also due to training with a
narrow objective: the prediction of a sub-optimal one-hot distribution. To
support this critique, we conducted a pre-experiment treating the output
distribution from powerful LLMs as efficient world data compression. By
evaluating the similarity between the $n$-gram distribution and the one-hot
distribution with LLMs, we observed that the $n$-gram distributions align more
closely with the output distribution of LLMs. Based on this insight, we
introduce Next Distribution Prediction (NDP), which uses $n$-gram distributions
to replace the one-hot targets, enhancing learning without extra online
training time. We conducted experiments across translation, general task,
language transfer, and medical domain adaptation. Compared to NTP, NDP can
achieve up to +2.97 COMET improvement in translation tasks, +0.61 average
improvement in general tasks, and incredible +10.75 average improvement in the
medical domain. This demonstrates the concrete benefits of addressing the
target narrowing problem, pointing to a new direction for future work on
improving NTP.

摘要：大型語言模型 (LLM) 根據下一個符號預測 (NTP) 範例進行訓練，已展現強大的功能。然而，現有的 NTP 範例包含了幾個限制，特別是與計畫任務複雜性和推論期間的錯誤傳播有關。在我們的研究中，我們擴展了 NTP 的批判，強調其限制也由於使用狹隘的目標進行訓練：預測次佳的一熱分佈。為了支持這項批判，我們進行了一個前置實驗，將強大 LLM 的輸出分佈視為有效的世界資料壓縮。透過評估 $n$-gram 分佈與 LLM 的一熱分佈之間的相似性，我們觀察到 $n$-gram 分佈與 LLM 的輸出分佈更為接近。基於這個見解，我們引入了下一個分佈預測 (NDP)，它使用 $n$-gram 分佈來取代一熱目標，在沒有額外線上訓練時間的情況下，加強學習。我們進行了翻譯、一般任務、語言轉移和醫學領域適應的實驗。與 NTP 相比，NDP 在翻譯任務中可以達到 +2.97 COMET 改進，在一般任務中平均改進 +0.61，在醫學領域中令人難以置信的平均改進 +10.75。這證明了解決目標收窄問題的具體好處，指出了改進 NTP 未來工作的全新方向。

##### **Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities**
2408.17011v2 by Jutika Borah, Kumaresh Sarmah, Hidam Kumarjit Singh

Imaging techniques such as Chest X-rays, whole slide images, and optical
coherence tomography serve as the initial screening and detection for a wide
variety of medical pulmonary and ophthalmic conditions respectively. This paper
investigates the intricacies of using pretrained deep convolutional neural
networks with transfer learning across diverse medical imaging datasets with
varying modalities for binary and multiclass classification. We conducted a
comprehensive performance analysis with ten network architectures and model
families each with pretraining and random initialization. Our finding showed
that the use of pretrained models as fixed feature extractors yields poor
performance irrespective of the datasets. Contrary, histopathology microscopy
whole slide images have better performance. It is also found that deeper and
more complex architectures did not necessarily result in the best performance.
This observation implies that the improvements in ImageNet are not parallel to
the medical imaging tasks. Within a medical domain, the performance of the
network architectures varies within model families with shifts in datasets.
This indicates that the performance of models within a specific modality may
not be conclusive for another modality within the same domain. This study
provides a deeper understanding of the applications of deep learning techniques
in medical imaging and highlights the impact of pretrained networks across
different medical imaging datasets under five different experimental settings.

摘要：影像技術，例如胸部 X 光、全切片影像和光學相干斷層掃描，分別作為各種醫學肺部和眼科疾病的初步篩檢和偵測。本文探討了使用預訓練深度卷積神經網路搭配遷移學習，橫跨不同醫療影像資料集，以進行二元和多類別分類的複雜性。我們對十種網路架構和模型系列進行了全面的效能分析，每個架構和系列都經過預訓練和隨機初始化。我們的發現顯示，將預訓練模型用作固定特徵萃取器會產生不佳的效能，與資料集無關。相反地，組織病理學顯微鏡全切片影像有較好的效能。我們也發現，較深且複雜的架構並非一定會產生最佳效能。此觀察結果意味著 ImageNet 的改良並未與醫療影像任務平行。在醫療領域內，網路架構的效能會隨著資料集的轉換而改變模型系列。這表示在特定模式中模型的效能可能無法決定在同一個領域中另一種模式的效能。本研究提供了對深度學習技術在醫療影像中的應用更深入的理解，並強調了預訓練網路在五種不同實驗設定下跨不同醫療影像資料集的影響。

##### **A Survey for Large Language Models in Biomedicine**
2409.00133v1 by Chong Wang, Mengyao Li, Junjun He, Zhongruo Wang, Erfan Darzi, Zan Chen, Jin Ye, Tianbin Li, Yanzhou Su, Jing Ke, Kaili Qu, Shuxin Li, Yi Yu, Pietro Liò, Tianyun Wang, Yu Guang Wang, Yiqing Shen

Recent breakthroughs in large language models (LLMs) offer unprecedented
natural language understanding and generation capabilities. However, existing
surveys on LLMs in biomedicine often focus on specific applications or model
architectures, lacking a comprehensive analysis that integrates the latest
advancements across various biomedical domains. This review, based on an
analysis of 484 publications sourced from databases including PubMed, Web of
Science, and arXiv, provides an in-depth examination of the current landscape,
applications, challenges, and prospects of LLMs in biomedicine, distinguishing
itself by focusing on the practical implications of these models in real-world
biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot
learning across a broad spectrum of biomedical tasks, including diagnostic
assistance, drug discovery, and personalized medicine, among others, with
insights drawn from 137 key studies. Then, we discuss adaptation strategies of
LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to
enhance their performance in specialized biomedical contexts where zero-shot
fails to achieve, such as medical question answering and efficient processing
of biomedical literature. Finally, we discuss the challenges that LLMs face in
the biomedicine domain including data privacy concerns, limited model
interpretability, issues with dataset quality, and ethics due to the sensitive
nature of biomedical data, the need for highly reliable model outputs, and the
ethical implications of deploying AI in healthcare. To address these
challenges, we also identify future research directions of LLM in biomedicine
including federated learning methods to preserve data privacy and integrating
explainable AI methodologies to enhance the transparency of LLMs.

摘要：大型語言模型 (LLM) 的最新突破提供了前所未有的自然語言理解和生成能力。然而，現有關於生物醫學中 LLM 的調查通常專注於特定應用或模型架構，缺乏整合各種生物醫學領域最新進展的全面分析。本綜述基於對來自 PubMed、Web of Science 和 arXiv 等數據庫的 484 篇出版物的分析，深入探討了生物醫學中 LLM 的當前現況、應用、挑戰和前景，其特點是關注這些模型在現實世界生物醫學背景中的實際應用。首先，我們探討了 LLM 在廣泛的生物醫學任務中的零次學習能力，包括診斷輔助、藥物發現和個性化醫療等，並從 137 項關鍵研究中汲取見解。然後，我們討論了 LLM 的適應策略，包括單模態和多模態 LLM 的微調方法，以增強它們在零次學習無法實現的專業生物醫學背景中的性能，例如醫療問題解答和生物醫學文獻的有效處理。最後，我們討論了 LLM 在生物醫學領域面臨的挑戰，包括數據隱私問題、模型可解釋性有限、數據集質量問題以及由於生物醫學數據的敏感性、對高度可靠模型輸出的需求以及在醫療保健中部署 AI 的倫理影響而產生的倫理問題。為了應對這些挑戰，我們還確定了生物醫學中 LLM 未來的研究方向，包括用於保護數據隱私的聯合學習方法以及整合可解釋 AI 方法以增強 LLM 的透明度。

##### **Toward Robust Early Detection of Alzheimer's Disease via an Integrated Multimodal Learning Approach**
2408.16343v1 by Yifei Chen, Shenghao Zhu, Zhaojie Fang, Chang Liu, Binfeng Zou, Yuhe Wang, Shuo Chang, Fan Jia, Feiwei Qin, Jin Fan, Yong Peng, Changmiao Wang

Alzheimer's Disease (AD) is a complex neurodegenerative disorder marked by
memory loss, executive dysfunction, and personality changes. Early diagnosis is
challenging due to subtle symptoms and varied presentations, often leading to
misdiagnosis with traditional unimodal diagnostic methods due to their limited
scope. This study introduces an advanced multimodal classification model that
integrates clinical, cognitive, neuroimaging, and EEG data to enhance
diagnostic accuracy. The model incorporates a feature tagger with a tabular
data coding architecture and utilizes the TimesBlock module to capture
intricate temporal patterns in Electroencephalograms (EEG) data. By employing
Cross-modal Attention Aggregation module, the model effectively fuses Magnetic
Resonance Imaging (MRI) spatial information with EEG temporal data,
significantly improving the distinction between AD, Mild Cognitive Impairment,
and Normal Cognition. Simultaneously, we have constructed the first AD
classification dataset that includes three modalities: EEG, MRI, and tabular
data. Our innovative approach aims to facilitate early diagnosis and
intervention, potentially slowing the progression of AD. The source code and
our private ADMC dataset are available at https://github.com/JustlfC03/MSTNet.

摘要：阿茲海默症 (AD) 是一種複雜的神經退化性疾病，特徵是記憶力喪失、執行功能障礙和人格改變。由於症狀微妙且表現形式多樣，早期診斷具有挑戰性，通常由於傳統單模態診斷方法的範圍有限而導致誤診。本研究引入了一個先進的多模態分類模型，它整合了臨床、認知、神經影像和腦電圖數據，以提高診斷準確性。該模型結合了一個具有表格數據編碼架構的特徵標籤器，並利用 TimesBlock 模組來捕捉腦電圖 (EEG) 數據中的複雜時間模式。通過採用跨模態注意力聚合模組，該模型有效地融合了磁共振成像 (MRI) 空間資訊和腦電圖時間數據，顯著改善了 AD、輕度認知障礙和正常認知之間的區別。同時，我們構建了第一個 AD 分類數據集，其中包含三種模態：腦電圖、磁共振成像和表格數據。我們的創新方法旨在促進早期診斷和干預，潛在地減緩 AD 的進展。原始碼和我們的私人 ADMC 數據集可在 https://github.com/JustlfC03/MSTNet 獲得。

##### **Coalitions of AI-based Methods Predict 15-Year Risks of Breast Cancer Metastasis Using Real-World Clinical Data with AUC up to 0.9**
2408.16256v1 by Xia Jiang, Yijun Zhou, Alan Wells, Adam Brufsky

Breast cancer is one of the two cancers responsible for the most deaths in
women, with about 42,000 deaths each year in the US. That there are over
300,000 breast cancers newly diagnosed each year suggests that only a fraction
of the cancers result in mortality. Thus, most of the women undergo seemingly
curative treatment for localized cancers, but a significant later succumb to
metastatic disease for which current treatments are only temporizing for the
vast majority. The current prognostic metrics are of little actionable value
for 4 of the 5 women seemingly cured after local treatment, and many women are
exposed to morbid and even mortal adjuvant therapies unnecessarily, with these
adjuvant therapies reducing metastatic recurrence by only a third. Thus, there
is a need for better prognostics to target aggressive treatment at those who
are likely to relapse and spare those who were actually cured. While there is a
plethora of molecular and tumor-marker assays in use and under-development to
detect recurrence early, these are time consuming, expensive and still often
un-validated as to actionable prognostic utility. A different approach would
use large data techniques to determine clinical and histopathological
parameters that would provide accurate prognostics using existing data. Herein,
we report on machine learning, together with grid search and Bayesian Networks
to develop algorithms that present a AUC of up to 0.9 in ROC analyses, using
only extant data. Such algorithms could be rapidly translated to clinical
management as they do not require testing beyond routine tumor evaluations.

摘要：乳癌是造成女性死亡人數最多的兩種癌症之一，每年約有 42,000 名女性死於乳癌。每年有超過 300,000 例乳癌新確診，這表示只有少部分癌症會導致死亡。因此，大多數女性接受局部癌症的根治性治療，但許多人後來仍會死於轉移性疾病，而目前的治療方法對絕大多數患者來說只是暫時的。目前的預後指標對於 5 名接受局部治療後看似治癒的女性中，有 4 名幾乎沒有實際價值，許多女性不必要地接受病態甚至致命的輔助療法，而這些輔助療法僅能將轉移性復發率降低三分之一。因此，需要更好的預後指標，才能針對那些可能復發的人進行積極治療，並避免那些實際上已經治癒的人接受治療。雖然有許多分子和腫瘤標記檢測方法正在使用和開發中，可以及早發現復發，但這些方法耗時、昂貴，而且作為可操作的預後工具，其效用仍然常常未經驗證。另一種方法會使用大量的資料技術，來確定臨床和組織病理學參數，並使用現有資料提供準確的預後指標。在此，我們報告了機器學習，以及網格搜尋和貝氏網路，用來開發演算法，在 ROC 分析中提供高達 0.9 的 AUC，僅使用現有資料。此類演算法可以快速轉換為臨床管理，因為它們不需要進行常規腫瘤評估以外的測試。

##### **M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation**
2408.16213v1 by Jonggwon Park, Soobum Kim, Byungmu Yoon, Jihun Hyun, Kyoyun Choi

The rapid evolution of artificial intelligence, especially in large language
models (LLMs), has significantly impacted various domains, including
healthcare. In chest X-ray (CXR) analysis, previous studies have employed LLMs,
but with limitations: either underutilizing the multi-tasking capabilities of
LLMs or lacking clinical accuracy. This paper presents M4CXR, a multi-modal LLM
designed to enhance CXR interpretation. The model is trained on a visual
instruction-following dataset that integrates various task-specific datasets in
a conversational format. As a result, the model supports multiple tasks such as
medical report generation (MRG), visual grounding, and visual question
answering (VQA). M4CXR achieves state-of-the-art clinical accuracy in MRG by
employing a chain-of-thought prompting strategy, in which it identifies
findings in CXR images and subsequently generates corresponding reports. The
model is adaptable to various MRG scenarios depending on the available inputs,
such as single-image, multi-image, and multi-study contexts. In addition to
MRG, M4CXR performs visual grounding at a level comparable to specialized
models and also demonstrates outstanding performance in VQA. Both quantitative
and qualitative assessments reveal M4CXR's versatility in MRG, visual
grounding, and VQA, while consistently maintaining clinical accuracy.

摘要：人工智慧的快速發展，特別是在大型語言模型 (LLM) 中，已對包括醫療保健在內的各個領域產生重大影響。在胸部 X 光 (CXR) 分析中，先前的研究已採用 LLM，但有其限制：不是未能充分利用 LLM 的多任務處理能力，就是缺乏臨床準確性。本文提出 M4CXR，一種多模態 LLM，旨在增強 CXR 解釋。該模型訓練於視覺指令遵循資料集，其中以對話格式整合各種特定任務資料集。因此，該模型支援多項任務，例如醫療報告產生 (MRG)、視覺基礎和視覺問題回答 (VQA)。M4CXR 透過採用思考鏈提示策略，在 MRG 中達成最先進的臨床準確性，其中它會識別 CXR 影像中的發現，並隨後產生對應的報告。該模型可根據可用輸入（例如單一影像、多重影像和多重研究脈絡）適應各種 MRG 情境。除了 MRG 之外，M4CXR 以與專門模型相當的層級執行視覺基礎，並在 VQA 中展現出色的效能。定量和定性評估均顯示出 M4CXR 在 MRG、視覺基礎和 VQA 中的多功能性，同時持續維持臨床準確性。

##### **A Survey on Evaluation of Multimodal Large Language Models**
2408.15769v1 by Jiaxing Huang, Jingyi Zhang

Multimodal Large Language Models (MLLMs) mimic human perception and reasoning
system by integrating powerful Large Language Models (LLMs) with various
modality encoders (e.g., vision, audio), positioning LLMs as the "brain" and
various modality encoders as sensory organs. This framework endows MLLMs with
human-like capabilities, and suggests a potential pathway towards achieving
artificial general intelligence (AGI). With the emergence of all-round MLLMs
like GPT-4V and Gemini, a multitude of evaluation methods have been developed
to assess their capabilities across different dimensions. This paper presents a
systematic and comprehensive review of MLLM evaluation methods, covering the
following key aspects: (1) the background of MLLMs and their evaluation; (2)
"what to evaluate" that reviews and categorizes existing MLLM evaluation tasks
based on the capabilities assessed, including general multimodal recognition,
perception, reasoning and trustworthiness, and domain-specific applications
such as socioeconomic, natural sciences and engineering, medical usage, AI
agent, remote sensing, video and audio processing, 3D point cloud analysis, and
others; (3) "where to evaluate" that summarizes MLLM evaluation benchmarks into
general and specific benchmarks; (4) "how to evaluate" that reviews and
illustrates MLLM evaluation steps and metrics; Our overarching goal is to
provide valuable insights for researchers in the field of MLLM evaluation,
thereby facilitating the development of more capable and reliable MLLMs. We
emphasize that evaluation should be regarded as a critical discipline,
essential for advancing the field of MLLMs.

摘要：多模態大型語言模型 (MLLM) 透過整合強大的大型語言模型 (LLM) 與各種模態編碼器（例如視覺、音訊），模擬人類的感知和推理系統，將 LLM 定位為「大腦」，而將各種模態編碼器定位為感官器官。此架構賦予 MLLM 類似人類的能力，並提出實現人工通用智慧 (AGI) 的潛在途徑。隨著 GPT-4V 和 Gemini 等全方位 MLLM 的出現，已經開發出多種評估方法來評估它們在不同維度上的能力。本文對 MLLM 評估方法進行了系統且全面的回顧，涵蓋以下幾個關鍵面向：(1) MLLM 及其評估的背景；(2)「要評估什麼」根據評估的能力，回顧並分類現有的 MLLM 評估任務，包括一般多模態辨識、感知、推理和可信度，以及特定領域的應用，例如社會經濟、自然科學和工程、醫療用途、AI 代理、遙測、影片和音訊處理、3D 點雲分析等；(3)「在哪裡評估」將 MLLM 評估基準總結為一般基準和特定基準；(4)「如何評估」回顧並說明 MLLM 評估步驟和指標。我們的首要目標是為 MLLM 評估領域的研究人員提供有價值的見解，從而促進更強大且可靠的 MLLM 的開發。我們強調評估應被視為一項關鍵的學科，對於推進 MLLM 領域至關重要。

##### **Deep Learning to Predict Late-Onset Breast Cancer Metastasis: the Single Hyperparameter Grid Search (SHGS) Strategy for Meta Tuning Concerning Deep Feed-forward Neural Network**
2408.15498v1 by Yijun Zhou, Om Arora-Jain, Xia Jiang

While machine learning has advanced in medicine, its widespread use in
clinical applications, especially in predicting breast cancer metastasis, is
still limited. We have been dedicated to constructing a DFNN model to predict
breast cancer metastasis n years in advance. However, the challenge lies in
efficiently identifying optimal hyperparameter values through grid search,
given the constraints of time and resources. Issues such as the infinite
possibilities for continuous hyperparameters like l1 and l2, as well as the
time-consuming and costly process, further complicate the task. To address
these challenges, we developed Single Hyperparameter Grid Search (SHGS)
strategy, serving as a preselection method before grid search. Our experiments
with SHGS applied to DFNN models for breast cancer metastasis prediction focus
on analyzing eight target hyperparameters: epochs, batch size, dropout, L1, L2,
learning rate, decay, and momentum. We created three figures, each depicting
the experiment results obtained from three LSM-I-10-Plus-year datasets. These
figures illustrate the relationship between model performance and the target
hyperparameter values. For each hyperparameter, we analyzed whether changes in
this hyperparameter would affect model performance, examined if there were
specific patterns, and explored how to choose values for the particular
hyperparameter. Our experimental findings reveal that the optimal value of a
hyperparameter is not only dependent on the dataset but is also significantly
influenced by the settings of other hyperparameters. Additionally, our
experiments suggested some reduced range of values for a target hyperparameter,
which may be helpful for low-budget grid search. This approach serves as a
prior experience and foundation for subsequent use of grid search to enhance
model performance.

摘要：儘管機器學習在醫學領域已有所進展，但其在臨床應用中的廣泛使用，特別是在預測乳癌轉移方面，仍有其限制。我們致力於建構 DFNN 模型，以預測乳癌轉移 n 年。然而，挑戰在於透過網格搜尋有效率地找出最佳超參數值，這受到時間和資源的限制。諸如 l1 和 l2 等連續超參數的可能性無窮，以及耗時且昂貴的過程等問題，更讓這項任務變得複雜。為了應對這些挑戰，我們開發了單一超參數網格搜尋 (SHGS) 策略，作為網格搜尋前的預選方法。我們針對乳癌轉移預測應用的 DFNN 模型進行 SHGS 實驗，重點分析八個目標超參數：epoch 次數、批次大小、中斷、L1、L2、學習率、衰減和動量。我們製作了三幅圖，每幅圖都描繪了從三個 LSM-I-10-Plus-year 資料集獲得的實驗結果。這些圖表說明了模型效能與目標超參數值之間的關係。對於每個超參數，我們分析了超參數的變動是否會影響模型效能，並檢視是否有特定模式，以及如何針對特定超參數選擇值。我們的實驗結果顯示，超參數的最佳值不僅取決於資料集，也受到其他超參數設定的顯著影響。此外，我們的實驗建議縮小目標超參數值的範圍，這可能有助於低預算的網格搜尋。此方法可作為後續使用網格搜尋以增強模型效能的先前經驗和基礎。

##### **What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users**
2408.15354v1 by Jana Schaich Borg, Hannah Read

Interest is growing in artificial empathy, but so is confusion about what
artificial empathy is or needs to be. This confusion makes it challenging to
navigate the technical and ethical issues that accompany empathic AI
development. Here, we outline a framework for thinking about empathic AI based
on the premise that different constellations of capabilities associated with
empathy are important for different empathic AI applications. We describe
distinctions of capabilities that we argue belong under the empathy umbrella,
and show how three medical empathic AI use cases require different sets of
these capabilities. We conclude by discussing why appreciation of the diverse
capabilities under the empathy umbrella is important for both AI creators and
users.

摘要：對人工同理心越來越感興趣，但對於人工同理心是什麼或需要什麼也越來越困惑。這種混淆使得難以解決伴隨同理心 AI 開發而來的技術和倫理問題。在此，我們概述了一個思考同理心 AI 的架構，其基於這樣一個前提：與同理心相關的不同能力組合對於不同的同理心 AI 應用很重要。我們描述了我們認為屬於同理心範疇的能力區別，並展示了三個醫療同理心 AI 使用案例需要這些能力的不同組合。我們最後討論了為什麼欣賞同理心範疇下的各種能力對於 AI 創造者和使用者都很重要的原因。

##### **Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance**
2408.15217v1 by Weiyi Zhang, Siyu Huang, Jiancheng Yang, Ruoyu Chen, Zongyuan Ge, Yingfeng Zheng, Danli Shi, Mingguang He

Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal
vascular dynamics and aiding in the diagnosis of eye diseases. However, its
invasive nature and less accessibility compared to Color Fundus (CF) images
pose significant challenges. Current CF to FFA translation methods are limited
to static generation. In this work, we pioneer dynamic FFA video generation
from static CF images. We introduce an autoregressive GAN for smooth,
memory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic
lesion changes in FFA regions, we design a knowledge mask based on clinical
experience. Leveraging this mask, our approach integrates innovative knowledge
mask-guided techniques, including knowledge-boosted attention, knowledge-aware
discriminators, and mask-enhanced patchNCE loss, aimed at refining generation
in critical areas and addressing the pixel misalignment challenge. Our method
achieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common
video generation approaches. Human assessment by an ophthalmologist confirms
its high generation quality. Notably, our knowledge mask surpasses supervised
lesion segmentation masks, offering a promising non-invasive alternative to
traditional FFA for research and clinical applications. The code is available
at https://github.com/Michi-3000/Fundus2Video.

摘要：眼底螢光血管攝影 (FFA) 是評估視網膜血管動力學和協助診斷眼疾的重要工具。然而，與彩色眼底 (CF) 影像相比，其侵入性較高且取得不易，因此造成重大挑戰。目前 CF 轉換成 FFA 的翻譯方法僅限於靜態產生。在這項工作中，我們率先從靜態 CF 影像產生動態 FFA 影片。我們引入一個自迴歸 GAN，以進行流暢且節省記憶體的逐幀 FFA 合成。為了加強對 FFA 區域中動態病灶變化的關注，我們根據臨床經驗設計了一個知識遮罩。透過利用這個遮罩，我們的做法整合了創新的知識遮罩引導技術，包括知識增強的注意力、知識感知的辨別器以及遮罩增強的 patchNCE 損失，旨在改善關鍵區域的生成並解決像素未對齊的挑戰。與其他常見的影片生成方法相比，我們的做法達到了最佳的 FVD 1503.21 和 PSNR 11.81。眼科醫師的人為評估證實了其生成品質很高。值得注意的是，我們的知識遮罩超越了有監督的病灶分割遮罩，為傳統 FFA 提供了一個有前途的非侵入性替代方案，可用於研究和臨床應用。程式碼可在 https://github.com/Michi-3000/Fundus2Video 取得。

##### **Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy**
2409.00112v1 by Daniil Filienko, Yinzhou Wang, Caroline El Jazmi, Serena Xie, Trevor Cohen, Martine De Cock, Weichao Yuwen

While Large Language Models (LLMs) are being quickly adapted to many domains,
including healthcare, their strengths and pitfalls remain under-explored. In
our study, we examine the effects of prompt engineering to guide Large Language
Models (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session
via text, particularly during the symptom identification and assessment phase
for personalized goal setting. We present evaluation results of the models'
performances by automatic metrics and experienced medical professionals. We
demonstrate that the models' capability to deliver protocolized therapy can be
improved with the proper use of prompt engineering methods, albeit with
limitations. To our knowledge, this study is among the first to assess the
effects of various prompting techniques in enhancing a generalist model's
ability to deliver psychotherapy, focusing on overall quality, consistency, and
empathy. Exploring LLMs' potential in delivering psychotherapy holds promise
with the current shortage of mental health professionals amid significant
needs, enhancing the potential utility of AI-based and AI-enhanced care
services.

摘要：隨著大型語言模型（LLM）快速適應於許多領域，包括醫療保健，它們的優勢和缺陷仍未得到充分探索。在我們的研究中，我們探討了提示工程在引導大型語言模型（LLM）通過文字提供問題解決療法（PST）環節的部分內容的效果，特別是在症狀識別和評估階段，用於個性化目標設定。我們通過自動化指標和經驗豐富的醫療專業人員展示了模型性能的評估結果。我們證明了模型提供程式化治療的能力可以使用提示工程方法適當使用來改進，儘管有局限性。據我們所知，這項研究是第一批評估各種提示技術對增強通才模型提供心理治療能力的影響的研究之一，重點關注整體品質、一致性和同理心。在心理健康專業人員嚴重短缺且需求巨大的情況下，探索 LLM 在提供心理治療方面的潛力很有前景，增強了基於 AI 和 AI 增強的護理服務的潛在效用。

##### **Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis**
2408.15121v1 by Francesco Sovrano, Michael Lognoul, Giulia Vilone

Significant investment and development have gone into integrating Artificial
Intelligence (AI) in medical and healthcare applications, leading to advanced
control systems in medical technology. However, the opacity of AI systems
raises concerns about essential characteristics needed in such sensitive
applications, like transparency and trustworthiness. Our study addresses these
concerns by investigating a process for selecting the most adequate Explainable
AI (XAI) methods to comply with the explanation requirements of key EU
regulations in the context of smart bioelectronics for medical devices. The
adopted methodology starts with categorising smart devices by their control
mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving
into their technology. Then, we analyse these regulations to define their
explainability requirements for the various devices and related goals.
Simultaneously, we classify XAI methods by their explanatory objectives. This
allows for matching legal explainability requirements with XAI explanatory
goals and determining the suitable XAI algorithms for achieving them. Our
findings provide a nuanced understanding of which XAI algorithms align better
with EU regulations for different types of medical devices. We demonstrate this
through practical case studies on different neural implants, from chronic
disease management to advanced prosthetics. This study fills a crucial gap in
aligning XAI applications in bioelectronics with stringent provisions of EU
regulations. It provides a practical framework for developers and researchers,
ensuring their AI innovations advance healthcare technology and adhere to legal
and ethical standards.

摘要：人工智慧（AI）在醫療和保健應用中投入了大量的投資和開發，進而導致醫療技術中的先進控制系統。然而，AI 系統的不透明性引發了對此類敏感應用中所需基本特性的擔憂，例如透明度和可信度。我們的研究透過調查一個程序來解決這些問題，用於選擇最充分的可解釋 AI（XAI）方法，以符合歐盟法規在醫療器材的智慧型生物電子學中的說明要求。採用的方法從透過其控制機制（開迴路、閉迴路和半閉迴路系統）對智慧型裝置進行分類，並深入探討其技術開始。然後，我們分析這些法規以定義其對各種裝置和相關目標的可解釋性要求。同時，我們透過其說明目標對 XAI 方法進行分類。這允許將法律可解釋性要求與 XAI 說明目標相匹配，並確定適當的 XAI 演算法來達成它們。我們的研究結果提供了對哪些 XAI 演算法更符合歐盟法規以適用於不同類型的醫療器材的細緻理解。我們透過不同神經植入物的實際案例研究來證明這一點，從慢性疾病管理到先進的義肢。這項研究填補了將生物電子學中的 XAI 應用與歐盟法規的嚴格規定相符的重要空白。它為開發人員和研究人員提供了一個實用的架構，確保其 AI 創新能促進醫療技術並遵守法律和道德標準。

##### **MiWaves Reinforcement Learning Algorithm**
2408.15076v1 by Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy

The escalating prevalence of cannabis use poses a significant public health
challenge globally. In the U.S., cannabis use is more prevalent among emerging
adults (EAs) (ages 18-25) than any other age group, with legalization in the
multiple states contributing to a public perception that cannabis is less risky
than in prior decades. To address this growing concern, we developed MiWaves, a
reinforcement learning (RL) algorithm designed to optimize the delivery of
personalized intervention prompts to reduce cannabis use among EAs. MiWaves
leverages domain expertise and prior data to tailor the likelihood of delivery
of intervention messages. This paper presents a comprehensive overview of the
algorithm's design, including key decisions and experimental outcomes. The
finalized MiWaves RL algorithm was deployed in a clinical trial from March to
May 2024.

摘要：大麻使用率不斷上升，對全球公共衛生構成重大挑戰。在美國，大麻使用率在年輕成年人（EA）（18-25 歲）中比任何其他年齡組都要普遍，多個州的合法化導致公眾認為大麻比過去幾十年風險較低。為了解決這個日益嚴重的問題，我們開發了 MiWaves，這是一種增強學習 (RL) 演算法，旨在優化個性化干預提示的傳遞，以減少 EA 中的大麻使用。MiWaves 利用領域專業知識和先前的數據來調整干預訊息傳遞的可能性。本文全面概述了演算法的設計，包括關鍵決策和實驗結果。最終的 MiWaves RL 演算法已於 2024 年 3 月至 5 月在臨床試驗中部署。

##### **Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology**
2408.15032v1 by Yuqi Zhang, Xiaoqian Zhang, Jiakai Wang, Yuancheng Yang, Taiying Peng, Chao Tong

Computational pathology (CPath) has significantly advanced the clinical
practice of pathology. Despite the progress made, Multiple Instance Learning
(MIL), a promising paradigm within CPath, continues to face challenges,
particularly related to incomplete information utilization. Existing
frameworks, such as those based on Convolutional Neural Networks (CNNs),
attention, and selective scan space state sequential model (SSM), lack
sufficient flexibility and scalability in fusing diverse features, and cannot
effectively fuse diverse features. Additionally, current approaches do not
adequately exploit order-related and order-independent features, resulting in
suboptimal utilization of sequence information. To address these limitations,
we propose a novel MIL framework called Mamba2MIL. Our framework utilizes the
state space duality model (SSD) to model long sequences of patches of whole
slide images (WSIs), which, combined with weighted feature selection, supports
the fusion processing of more branching features and can be extended according
to specific application needs. Moreover, we introduce a sequence transformation
method tailored to varying WSI sizes, which enhances sequence-independent
features while preserving local sequence information, thereby improving
sequence information utilization. Extensive experiments demonstrate that
Mamba2MIL surpasses state-of-the-art MIL methods. We conducted extensive
experiments across multiple datasets, achieving improvements in nearly all
performance metrics. Specifically, on the NSCLC dataset, Mamba2MIL achieves a
binary tumor classification AUC of 0.9533 and an accuracy of 0.8794. On the
BRACS dataset, it achieves a multiclass classification AUC of 0.7986 and an
accuracy of 0.4981. The code is available at
https://github.com/YuqiZhang-Buaa/Mamba2MIL.

摘要：<paragraph>計算病理學 (CPath) 已顯著提升病理學的臨床實務。儘管已有進展，作為 CPath 中一個有前途的範例，多重實例學習 (MIL) 持續面臨挑戰，特別是與不完整資訊使用有關。現有的架構，例如基於卷積神經網路 (CNN)、注意力和選擇性掃描空間狀態序列模型 (SSM) 的架構，在融合各種特徵時缺乏足夠的彈性和可擴充性，且無法有效融合各種特徵。此外，目前的作法並未充分利用與順序相關和與順序無關的特徵，導致序列資訊使用率不佳。為了解決這些限制，我們提出一個名為 Mamba2MIL 的新 MIL 架構。我們的架構利用狀態空間對偶模型 (SSD) 來建模全幻燈片影像 (WSI) 的長序列貼片，這與加權特徵選取結合使用，支援更多分支特徵的融合處理，且可根據特定應用需求進行延伸。此外，我們引入一種針對不同 WSI 大小量身打造的序列轉換方法，這增強了與序列無關的特徵，同時保留了局部序列資訊，進而改善序列資訊使用率。廣泛的實驗證明 Mamba2MIL 超越了最先進的 MIL 方法。我們在多個資料集上進行廣泛的實驗，在幾乎所有效能指標上均獲得改善。特別是在 NSCLC 資料集上，Mamba2MIL 達到 0.9533 的二元腫瘤分類 AUC 和 0.8794 的準確度。在 BRACS 資料集上，它達到 0.7986 的多類別分類 AUC 和 0.4981 的準確度。程式碼可在 https://github.com/YuqiZhang-Buaa/Mamba2MIL 取得。</paragraph>

##### **Sequence-aware Pre-training for Echocardiography Probe Guidance**
2408.15026v1 by Haojun Jiang, Zhenguo Sun, Yu Sun, Ning Jia, Meng Li, Shaqi Luo, Shiji Song, Gao Huang

Cardiac ultrasound probe guidance aims to help novices adjust the 6-DOF probe
pose to obtain high-quality sectional images. Cardiac ultrasound faces two
major challenges: (1) the inherently complex structure of the heart, and (2)
significant individual variations. Previous works have only learned the
population-averaged 2D and 3D structures of the heart rather than personalized
cardiac structural features, leading to a performance bottleneck. Clinically,
we observed that sonographers adjust their understanding of a patient's cardiac
structure based on prior scanning sequences, thereby modifying their scanning
strategies. Inspired by this, we propose a sequence-aware self-supervised
pre-training method. Specifically, our approach learns personalized 2D and 3D
cardiac structural features by predicting the masked-out images and actions in
a scanning sequence. We hypothesize that if the model can predict the missing
content it has acquired a good understanding of the personalized cardiac
structure. In the downstream probe guidance task, we also introduced a sequence
modeling approach that models individual cardiac structural information based
on the images and actions from historical scan data, enabling more accurate
navigation decisions. Experiments on a large-scale dataset with 1.36 million
samples demonstrated that our proposed sequence-aware paradigm can
significantly reduce navigation errors, with translation errors decreasing by
15.90% to 36.87% and rotation errors decreasing by 11.13% to 20.77%, compared
to state-of-the-art methods.

摘要：<paragraph>心臟超音波探頭引導旨在幫助新手調整 6-DOF 探頭姿勢，以取得高品質的斷面影像。心臟超音波面臨兩項主要挑戰：(1) 心臟結構複雜且固有，以及 (2) 個體差異顯著。先前的研究僅學習了整體平均的 2D 和 3D 心臟結構，而非個人化的解剖特徵，導致效能瓶頸。臨床上，我們觀察到超音波技師會根據先前的掃描序列調整他們對患者心臟結構的理解，進而修改他們的掃描策略。受到此啟發，我們提出一個具序列感知的自監督預訓練方法。具體來說，我們的做法透過預測掃描序列中遮罩的影像和動作，來學習個人化的 2D 和 3D 心臟解剖特徵。我們假設，如果模型可以預測遺漏的內容，它便對個人化的解剖結構有了良好的理解。在下游的探頭引導任務中，我們也導入一個序列建模方法，該方法根據歷史掃描資料中的影像和動作，模擬個別心臟解剖資訊，進而做出更精確的導航決策。在有 136 萬個樣本的大規模資料集上進行的實驗證明，我們提出的具序列感知的典範可以大幅減少導航錯誤，其中平移錯誤減少了 15.90% 至 36.87%，旋轉錯誤減少了 11.13% 至 20.77%，與最先進的方法相比。</paragraph>

##### **Evaluating the Predictive Features of Person-Centric Knowledge Graph Embeddings: Unfolding Ablation Studies**
2408.15294v2 by Christos Theodoropoulos, Natasha Mulligan, Joao Bettencourt-Silva

Developing novel predictive models with complex biomedical information is
challenging due to various idiosyncrasies related to heterogeneity,
standardization or sparseness of the data. We previously introduced a
person-centric ontology to organize information about individual patients, and
a representation learning framework to extract person-centric knowledge graphs
(PKGs) and to train Graph Neural Networks (GNNs). In this paper, we propose a
systematic approach to examine the results of GNN models trained with both
structured and unstructured information from the MIMIC-III dataset. Through
ablation studies on different clinical, demographic, and social data, we show
the robustness of this approach in identifying predictive features in PKGs for
the task of readmission prediction.

摘要：開發具有複雜生物醫學資訊的新穎預測模型，由於資料的異質性、標準化或稀疏性，因此具有挑戰性。我們先前介紹了一個以人為中心的本體，用於組織有關個別患者的資訊，以及一個表示學習架構，用於提取以人為中心的知識圖譜 (PKG) 和訓練圖形神經網路 (GNN)。在本文中，我們提出了一種系統性的方法來檢驗使用 MIMIC-III 資料集中的結構化和非結構化資訊訓練的 GNN 模型的結果。透過對不同的臨床、人口統計和社會資料進行消融研究，我們展示了這種方法在識別 PKG 中的預測特徵以進行再入院預測任務時的穩健性。

##### **Sequential-Scanning Dual-Energy CT Imaging Using High Temporal Resolution Image Reconstruction and Error-Compensated Material Basis Image Generation**
2408.14754v1 by Qiaoxin Li, Ruifeng Chen, Peng Wang, Guotao Quan, Yanfeng Du, Dong Liang, Yinsheng Li

Dual-energy computed tomography (DECT) has been widely used to obtain
quantitative elemental composition of imaged subjects for personalized and
precise medical diagnosis. Compared with DECT leveraging advanced X-ray source
and/or detector technologies, the use of the sequential-scanning data
acquisition scheme to implement DECT may make a broader impact on clinical
practice because this scheme requires no specialized hardware designs and can
be directly implemented into conventional CT systems. However, since the
concentration of iodinated contrast agent in the imaged subject varies over
time, sequentially scanned data sets acquired at two tube potentials are
temporally inconsistent. As existing material basis image reconstruction
approaches assume that the data sets acquired at two tube potentials are
temporally consistent, the violation of this assumption results in inaccurate
quantification of material concentration. In this work, we developed
sequential-scanning DECT imaging using high temporal resolution image
reconstruction and error-compensated material basis image generation,
ACCELERATION in short, to address the technical challenge induced by temporal
inconsistency of sequentially scanned data sets and improve quantification
accuracy of material concentration in sequential-scanning DECT. ACCELERATION
has been validated and evaluated using numerical simulation data sets generated
from clinical human subject exams and experimental human subject studies.
Results demonstrated the improvement of quantification accuracy and image
quality using ACCELERATION.

摘要：雙能量電腦斷層掃描 (DECT) 已廣泛用於取得影像化受試者的定量元素組成，以進行個人化且精確的醫療診斷。與使用進階 X 光源和/或偵測器技術的 DECT 相比，使用連續掃描資料擷取方案來實作 DECT 可能對臨床實務產生更廣泛的影響，因為此方案不需要專門的硬體設計，且可直接實作到傳統 CT 系統中。然而，由於影像化受試者中碘化對比劑的濃度會隨時間而變化，因此在兩個管電位下擷取的連續掃描資料集在時間上並不一致。由於現有的材料基礎影像重建方法假設在兩個管電位下擷取的資料集在時間上是一致的，因此違反此假設會導致材料濃度的量化不準確。在這項工作中，我們開發了使用高時間解析度影像重建和誤差補償材料基礎影像產生的連續掃描 DECT 影像，簡稱 ACCELERATION，以解決連續掃描資料集時間不一致所引發的技術挑戰，並改善連續掃描 DECT 中材料濃度的量化準確度。ACCELERATION 已使用從臨床人體受試者檢查和實驗人體受試者研究產生的數值模擬資料集進行驗證和評估。結果證明使用 ACCELERATION 可改善量化準確度和影像品質。

##### **Large Language Models for Disease Diagnosis: A Scoping Review**
2409.00097v1 by Shuang Zhou, Zidu Xu, Mian Zhang, Chunpu Xu, Yawen Guo, Zaifu Zhan, Sirui Ding, Jiashuo Wang, Kaishuai Xu, Yi Fang, Liqiao Xia, Jeremy Yeung, Daochen Zha, Mingquan Lin, Rui Zhang

Automatic disease diagnosis has become increasingly valuable in clinical
practice. The advent of large language models (LLMs) has catalyzed a paradigm
shift in artificial intelligence, with growing evidence supporting the efficacy
of LLMs in diagnostic tasks. Despite the growing attention in this field, many
critical research questions remain under-explored. For instance, what diseases
and LLM techniques have been investigated for diagnostic tasks? How can
suitable LLM techniques and evaluation methods be selected for clinical
decision-making? To answer these questions, we performed a comprehensive
analysis of LLM-based methods for disease diagnosis. This scoping review
examined the types of diseases, associated organ systems, relevant clinical
data, LLM techniques, and evaluation methods reported in existing studies.
Furthermore, we offered guidelines for data preprocessing and the selection of
appropriate LLM techniques and evaluation strategies for diagnostic tasks. We
also assessed the limitations of current research and delineated the challenges
and future directions in this research field. In summary, our review outlined a
blueprint for LLM-based disease diagnosis, helping to streamline and guide
future research endeavors.

摘要：自動疾病診斷在臨床實務中變得越來越有價值。大語言模型 (LLM) 的出現催化了人工智能的典範轉移，越來越多證據支持 LLM 在診斷任務中的效能。儘管這個領域受到越來越多的關注，但許多關鍵的研究問題仍未得到充分探討。例如，哪些疾病和 LLM 技術已被調查用於診斷任務？如何為臨床決策制定選擇合適的 LLM 技術和評估方法？為了回答這些問題，我們對基於 LLM 的疾病診斷方法進行了全面的分析。這項範圍探討回顧了現有研究中報告的疾病類型、相關器官系統、相關臨床資料、LLM 技術和評估方法。此外，我們提供了資料前處理和選擇適當的 LLM 技術和評估策略以進行診斷任務的指南。我們還評估了當前研究的限制，並描述了這個研究領域的挑戰和未來方向。總之，我們的回顧概述了基於 LLM 的疾病診斷藍圖，有助於簡化和指導未來的研究工作。

##### **Elementary School Students' and Teachers' Perceptions Towards Creative Mathematical Writing with Generative AI**
2409.06723v1 by Yukyeong Song, Jinhee Kim, Wanli Xing, Zifeng Liu, Chenglu Li, Hyunju Oh

While mathematical creative writing can potentially engage students in
expressing mathematical ideas in an imaginative way, some elementary school-age
students struggle in this process. Generative AI (GenAI) offers possibilities
for supporting creative writing activities, such as providing story generation.
However, the design of GenAI-powered learning technologies requires careful
consideration of the technology reception in the actual classrooms. This study
explores students' and teachers' perceptions of creative mathematical writing
with the developed GenAI-powered technology. The study adopted a qualitative
thematic analysis of the interviews, triangulated with open-ended survey
responses and classroom observation of 79 elementary school students, resulting
in six themes and 19 subthemes. This study contributes by investigating the
lived experience of GenAI-supported learning and the design considerations for
GenAI-powered learning technologies and instructions.

摘要：<paragraph>雖然數學創意寫作有可能讓學生以富有想像力的方式表達數學概念，但有些小學年齡的學生在這個過程中會遇到困難。生成式 AI (GenAI) 提供了支持創意寫作活動的可能性，例如提供故事生成。然而，GenAI 驅動的學習技術的設計需要仔細考慮技術在實際教室中的接受度。本研究探討了學生和教師對使用已開發的 GenAI 驅動技術進行創意數學寫作的看法。本研究採用了訪談的定性主題分析，並與 79 名小學生的開放式調查回應和教室觀察進行三角驗證，產生了六個主題和 19 個子主題。本研究通過調查 GenAI 支持的學習的真實體驗以及 GenAI 驅動的學習技術和說明的設計考量，做出了貢獻。</paragraph>

##### **Improving Clinical Note Generation from Complex Doctor-Patient Conversation**
2408.14568v1 by Yizhan Li, Sifan Wu, Christopher Smith, Thomas Lo, Bang Liu

Writing clinical notes and documenting medical exams is a critical task for
healthcare professionals, serving as a vital component of patient care
documentation. However, manually writing these notes is time-consuming and can
impact the amount of time clinicians can spend on direct patient interaction
and other tasks. Consequently, the development of automated clinical note
generation systems has emerged as a clinically meaningful area of research
within AI for health. In this paper, we present three key contributions to the
field of clinical note generation using large language models (LLMs). First, we
introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex
doctor-patient conversations paired with their full clinical notes. This
dataset, created and curated by medical experts with the help of modern neural
networks, provides a valuable resource for training and evaluating models in
clinical note generation tasks. Second, we propose the K-SOAP (Keyword,
Subjective, Objective, Assessment, and Plan) note format, which enhances
traditional SOAP~\cite{podder2023soap} (Subjective, Objective, Assessment, and
Plan) notes by adding a keyword section at the top, allowing for quick
identification of essential information. Third, we develop an automatic
pipeline to generate K-SOAP notes from doctor-patient conversations and
benchmark various modern LLMs using various metrics. Our results demonstrate
significant improvements in efficiency and performance compared to standard LLM
finetuning methods.

摘要：撰寫臨床筆記和記錄醫療檢查是醫療保健專業人員的一項重要任務，是患者照護文件中的重要組成部分。然而，手動撰寫這些筆記很耗時，並且會影響臨床醫生花在直接患者互動和其他任務上的時間。因此，自動化臨床筆記生成系統的開發已成為 AI 在健康領域中具有臨床意義的研究領域。在本文中，我們提出了使用大型語言模型 (LLM) 進行臨床筆記生成的領域的 3 項關鍵貢獻。首先，我們介紹了 CliniKnote，這是一個綜合性數據集，包含 1,200 個複雜的醫患對話及其完整的臨床筆記。此數據集由醫學專家在現代神經網路的幫助下創建和策劃，為臨床筆記生成任務中的模型訓練和評估提供了寶貴的資源。其次，我們提出了 K-SOAP（關鍵字、主觀、客觀、評估和計畫）筆記格式，它通過在頂部添加一個關鍵字部分來增強傳統的 SOAP~\cite{podder2023soap}（主觀、客觀、評估和計畫）筆記，以便快速識別基本資訊。第三，我們開發了一個自動化管道，從醫患對話中生成 K-SOAP 筆記，並使用各種指標對各種現代 LLM 進行基準測試。我們的結果表明，與標準 LLM 微調方法相比，效率和性能有了顯著的提升。

##### **Temporal Ensemble Logic**
2408.14443v2 by Guo-Qiang Zhang

We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal
logic for linear-time temporal reasoning. TEL includes primitive temporal
constructs such as ``always up to $t$ time later'' ($\Box_t$), ``sometimes
before $t$ time in the future'' ($\Diamond_t$), and ``$t$-time later''
$\varphi_t$. TEL has been motivated from the requirement for rigor and
reproducibility for cohort specification and discovery in clinical and
population health research, to fill a gap in formalizing temporal reasoning in
biomedicine. Existing logical frameworks such as linear temporal logic are too
restrictive to express temporal and sequential properties in biomedicine, or
too permissive in semantic constructs, such as in Halpern-Shoham logic, to
serve this purpose. In this paper, we first introduce TEL in a general set up,
with discrete and dense time as special cases. We then focus on the theoretical
development of discrete TEL on the temporal domain of positive integers
$\mathbb{N}^+$, denoted as ${\rm TEL}_{\mathbb{N}^+}$. ${\rm
TEL}_{\mathbb{N}^+}$ is strictly more expressive than the standard monadic
second order logic, characterized by B\"{u}chi automata. We present its formal
semantics, a proof system, and provide a proof for the undecidability of the
satisfiability of ${\rm TEL}_{\mathbb{N}^+}$. We also include initial results
on expressiveness and decidability fragments for ${\rm TEL}_{\mathbb{N}^+}$,
followed by application outlook and discussions.

摘要：<paragraph>我們介紹時間集合邏輯 (TEL)，一種用於線性時間時態推理的一階單子模態邏輯。TEL 包含原始時態結構，例如「始終在 $t$ 時間後」($\Box_t$)、「有時在未來 $t$ 時間前」($\Diamond_t$) 和「$t$ 時間後」$\varphi_t$。TEL 的動機是為了臨床和人口健康研究中群組規範和發現的嚴謹性和可再現性，以填補生物醫學中時態推理形式化的空白。現有的邏輯框架（例如線性時態邏輯）對於表達生物醫學中的時態和順序屬性過於嚴格，或者在語義結構上過於寬鬆（例如在 Halpern-Shoham 邏輯中），無法達到此目的。在本文中，我們首先在一般設置中介紹 TEL，其中離散時間和稠密時間為特殊情況。然後，我們專注於正整數時間域 $\mathbb{N}^+$ 上離散 TEL 的理論發展，表示為 ${\rm TEL}_{\mathbb{N}^+}$. ${\rm TEL}_{\mathbb{N}^+}$ 比標準單子二階邏輯更具表達力，其特徵在於 B\"{u}chi 自動機。我們展示其形式語義、證明系統，並提供 ${\rm TEL}_{\mathbb{N}^+}$ 可滿足性的不可判定性的證明。我們還包括 ${\rm TEL}_{\mathbb{N}^+}$ 的表達力和可判定片段的初步結果，然後是應用前景和討論。</paragraph>

##### **MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues**
2408.14418v2 by Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler

Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech
into text, yet the errors they introduce can significantly degrade the
performance of downstream tasks like summarization. This issue is particularly
pronounced in clinical dialogue summarization, a low-resource domain where
supervised data for fine-tuning is scarce, necessitating the use of ASR models
as black-box solutions. Employing conventional data augmentation for enhancing
the noise robustness of summarization models is not feasible either due to the
unavailability of sufficient medical dialogue audio recordings and
corresponding ASR transcripts. To address this challenge, we propose MEDSAGE,
an approach for generating synthetic samples for data augmentation using Large
Language Models (LLMs). Specifically, we leverage the in-context learning
capabilities of LLMs and instruct them to generate ASR-like errors based on a
few available medical dialogue examples with audio recordings. Experimental
results show that LLMs can effectively model ASR noise, and incorporating this
noisy data into the training process significantly improves the robustness and
accuracy of medical dialogue summarization systems. This approach addresses the
challenges of noisy ASR outputs in critical applications, offering a robust
solution to enhance the reliability of clinical dialogue summarization.

摘要：自動語音辨識 (ASR) 系統在將語音轉錄成文字方面至關重要，但它們所產生的錯誤可能會大幅降低摘要等下游任務的效能。這個問題在臨床對話摘要中特別明顯，這是一個低資源的領域，其中用於微調的監督資料很稀少，因此必須使用 ASR 模型作為黑盒解決方案。由於缺乏足夠的醫療對話音訊錄音和對應的 ASR 轉錄，採用傳統資料擴充來增強摘要模型的抗噪性也不可行。為了應對這個挑戰，我們提出了 MEDSAGE，這是一種使用大型語言模型 (LLM) 產生合成樣本進行資料擴充的方法。具體來說，我們利用 LLM 的情境學習能力，並指示它們根據少數帶有音訊錄音的可用醫療對話範例產生類似的 ASR 錯誤。實驗結果顯示，LLM 可以有效地模擬 ASR 雜訊，而將這些雜訊資料納入訓練過程中可以顯著提高醫療對話摘要系統的穩健性和準確性。這種方法應對了關鍵應用中 ASR 輸出雜訊的問題，提供了一個穩健的解決方案來增強臨床對話摘要的可靠性。

##### **Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs**
2408.14397v1 by Xiaoman Zhang, Julián N. Acosta, Hong-Yu Zhou, Pranav Rajpurkar

Recent advancements in artificial intelligence have significantly improved
the automatic generation of radiology reports. However, existing evaluation
methods fail to reveal the models' understanding of radiological images and
their capacity to achieve human-level granularity in descriptions. To bridge
this gap, we introduce a system, named ReXKG, which extracts structured
information from processed reports to construct a comprehensive radiology
knowledge graph. We then propose three metrics to evaluate the similarity of
nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs
(ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative
analysis of AI-generated and human-written radiology reports, assessing the
performance of both specialist and generalist models. Our study provides a
deeper understanding of the capabilities and limitations of current AI models
in radiology report generation, offering valuable insights for improving model
performance and clinical applicability.

摘要：近期人工智能的進展顯著改善了放射報告的自動生成。然而，現有的評估方法無法揭示模型對放射影像的理解，以及它們在描述中達到人類層級精細度的能力。為了彌補這個差距，我們引進一個名為 ReXKG 的系統，它從處理過的報告中萃取出結構化的資訊，以建構一個全面的放射知識圖譜。接著，我們提出三個指標來評估各種知識圖譜中節點的相似性 (ReXKG-NSC)、邊緣的分布 (ReXKG-AMS) 和子圖的涵蓋範圍 (ReXKG-SCS)。我們對 AI 生成的和人類撰寫的放射報告進行深入的比較分析，評估專家和通才模型的效能。我們的研究提供對目前 AI 模型在放射報告生成中的能力和限制更深入的理解，並提供有價值的見解來改善模型效能和臨床應用。

##### **Foundation Models for Music: A Survey**
2408.14340v3 by Yinghao Ma, Anders Øland, Anton Ragni, Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elona Shatri, Fabio Morreale, Ge Zhang, György Fazekas, Gus Xia, Huan Zhang, Ilaria Manco, Jiawen Huang, Julien Guinot, Liwei Lin, Luca Marinelli, Max W. Y. Lam, Megha Sharma, Qiuqiang Kong, Roger B. Dannenberg, Ruibin Yuan, Shangda Wu, Shih-Lun Wu, Shuqi Dai, Shun Lei, Shiyin Kang, Simon Dixon, Wenhu Chen, Wenhao Huang, Xingjian Du, Xingwei Qu, Xu Tan, Yizhi Li, Zeyue Tian, Zhiyong Wu, Zhizheng Wu, Ziyang Ma, Ziyu Wang

In recent years, foundation models (FMs) such as large language models (LLMs)
and latent diffusion models (LDMs) have profoundly impacted diverse sectors,
including music. This comprehensive review examines state-of-the-art (SOTA)
pre-trained models and foundation models in music, spanning from representation
learning, generative learning and multimodal learning. We first contextualise
the significance of music in various industries and trace the evolution of AI
in music. By delineating the modalities targeted by foundation models, we
discover many of the music representations are underexplored in FM development.
Then, emphasis is placed on the lack of versatility of previous methods on
diverse music applications, along with the potential of FMs in music
understanding, generation and medical application. By comprehensively exploring
the details of the model pre-training paradigm, architectural choices,
tokenisation, finetuning methodologies and controllability, we emphasise the
important topics that should have been well explored, like instruction tuning
and in-context learning, scaling law and emergent ability, as well as
long-sequence modelling etc. A dedicated section presents insights into music
agents, accompanied by a thorough analysis of datasets and evaluations
essential for pre-training and downstream tasks. Finally, by underscoring the
vital importance of ethical considerations, we advocate that following research
on FM for music should focus more on such issues as interpretability,
transparency, human responsibility, and copyright issues. The paper offers
insights into future challenges and trends on FMs for music, aiming to shape
the trajectory of human-AI collaboration in the music realm.

摘要：近年來，基礎模型 (FM)，例如大型語言模型 (LLM) 和潛在擴散模型 (LDM)，已對包括音樂在內的不同產業產生深遠影響。這篇全面性的評論探討了音樂領域中最先進 (SOTA) 的預訓練模型和基礎模型，涵蓋了表徵學習、生成式學習和多模態學習。我們首先將音樂在各產業的重要性脈絡化，並追溯 AI 在音樂中的演進。透過描繪基礎模型所針對的模態，我們發現許多音樂表徵在 FM 開發中尚未被充分探索。接著，我們強調先前方法在不同音樂應用中缺乏多樣性，以及 FM 在音樂理解、生成和醫療應用中的潛力。透過全面探討模型預訓練典範、架構選擇、標記化、微調方法和可控性的細節，我們強調了應深入探討的重要主題，例如指令微調和情境學習、規模定律和新興能力，以及長序列建模等。專門的章節提供了對音樂代理的見解，並附有對預訓練和下游任務至關重要的資料集和評估的深入分析。最後，透過強調倫理考量的至關重要性，我們主張後續關於音樂 FM 的研究應更專注於可解釋性、透明度、人類責任和版權問題等議題。本文提供了對音樂 FM 未來挑戰和趨勢的見解，旨在形塑人類與 AI 在音樂領域中合作的軌跡。

##### **Uncertainties of Latent Representations in Computer Vision**
2408.14281v1 by Michael Kirchhof

Uncertainty quantification is a key pillar of trustworthy machine learning.
It enables safe reactions under unsafe inputs, like predicting only when the
machine learning model detects sufficient evidence, discarding anomalous data,
or emitting warnings when an error is likely to be inbound. This is
particularly crucial in safety-critical areas like medical image classification
or self-driving cars. Despite the plethora of proposed uncertainty
quantification methods achieving increasingly higher scores on performance
benchmarks, uncertainty estimates are often shied away from in practice. Many
machine learning projects start from pretrained latent representations that
come without uncertainty estimates. Uncertainties would need to be trained by
practitioners on their own, which is notoriously difficult and
resource-intense.
  This thesis makes uncertainty estimates easily accessible by adding them to
the latent representation vectors of pretrained computer vision models. Besides
proposing approaches rooted in probability and decision theory, such as
Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both
theoretical and empirical questions. We show that these unobservable
uncertainties about unobservable latent representations are indeed provably
correct. We also provide an uncertainty-aware representation learning (URL)
benchmark to compare these unobservables against observable ground-truths.
Finally, we compile our findings to pretrain lightweight representation
uncertainties on large-scale computer vision models that transfer to unseen
datasets in a zero-shot manner.
  Our findings do not only advance the current theoretical understanding of
uncertainties over latent variables, but also facilitate the access to
uncertainty quantification for future researchers inside and outside the field,
enabling straightforward but trustworthy machine learning.

摘要：不確定量化是值得信賴機器學習的一大支柱。
它能讓機器學習模型在不安全的輸入下做出安全的反應，例如只在機器學習模型偵測到足夠證據時才進行預測、捨棄異常資料，或是在可能發生錯誤時發出警告。這在醫療影像分類或自駕車等安全關鍵領域中特別重要。儘管有許多已提出的不確定量化方法在效能基準上取得越來越高的分數，但在實務上卻常常迴避不確定性估計。許多機器學習專案從預訓練的潛在表徵開始，而這些表徵沒有不確定性估計。實務工作者需要自行訓練不確定性，這出了名的困難且耗費資源。
本論文透過將不確定性估計新增到預訓練電腦視覺模型的潛在表徵向量中，讓不確定性估計易於取得。除了提出植基於機率和決策理論的方法，例如蒙地卡羅資訊對比估計 (MCInfoNCE) 和損失預測之外，我們還深入探討理論和實證問題。我們證明這些關於不可觀察潛在表徵的不可觀察不確定性確實可以證明是正確的。我們還提供一個不確定性感知表徵學習 (URL) 基準，用來比較這些不可觀察的不確定性與可觀察的真實值。最後，我們將我們的發現彙整起來，在大型電腦視覺模型上預訓練輕量級表徵不確定性，並以零次學習的方式轉移到未見過的資料集。
我們的發現不僅提升了當前對潛在變數不確定性的理論理解，還促進了未來研究人員在該領域內外取得不確定量化，進而實現直接但值得信賴的機器學習。

##### **Automatic Medical Report Generation: Methods and Applications**
2408.13988v1 by Li Guo, Anas M. Tahir, Dong Zhang, Z. Jane Wang, Rabab K. Ward

The increasing demand for medical imaging has surpassed the capacity of
available radiologists, leading to diagnostic delays and potential
misdiagnoses. Artificial intelligence (AI) techniques, particularly in
automatic medical report generation (AMRG), offer a promising solution to this
dilemma. This review comprehensively examines AMRG methods from 2021 to 2024.
It (i) presents solutions to primary challenges in this field, (ii) explores
AMRG applications across various imaging modalities, (iii) introduces publicly
available datasets, (iv) outlines evaluation metrics, (v) identifies techniques
that significantly enhance model performance, and (vi) discusses unresolved
issues and potential future research directions. This paper aims to provide a
comprehensive understanding of the existing literature and inspire valuable
future research.

摘要：由於對醫學影像的需求日益增長，已經超過了現有放射科醫師的能力，導致診斷延誤和潛在的誤診。人工智慧 (AI) 技術，特別是在自動醫療報告生成 (AMRG) 方面，為此困境提供了有希望的解決方案。本篇評論全面探討了 2021 年至 2024 年的 AMRG 方法。它 (i) 提出解決此領域中主要挑戰的方案，(ii) 探討 AMRG 在各種影像模式中的應用，(iii) 介紹公開可用的資料集，(iv) 概述評估指標，(v) 找出顯著提升模型效能的技術，以及 (vi) 討論尚未解決的問題和潛在的未來研究方向。本文旨在提供對現有文獻的全面了解，並激發有價值的未來研究。

##### **Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models**
2409.00084v2 by Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush

Background and Aims: This study evaluates the medical reasoning performance
of large language models (LLMs) and vision language models (VLMs) in
gastroenterology.
  Methods: We used 300 gastroenterology board exam-style multiple-choice
questions, 138 of which contain images to systematically assess the impact of
model configurations and parameters and prompt engineering strategies utilizing
GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs
(versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0),
Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces
(web and API), computing environments (cloud and local), and model precisions
(with and without quantization). Finally, we assessed accuracy using a
semiautomated pipeline.
  Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet
(74.0%) achieved the highest accuracy, outperforming the top open-source
models: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%).
Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%)
performed best. The scores of the quantized models were comparable to those of
the full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM
performance on image-containing questions did not improve when the images were
provided and worsened when LLM-generated captions were provided. In contrast, a
10% increase in accuracy was observed when images were accompanied by
human-crafted image descriptions.
  Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in
medical reasoning, the integration of visual data remains a challenge for VLMs.
Effective deployment involves carefully determining optimal model
configurations, encouraging users to consider either the high performance of
proprietary models or the flexible adaptability of open-source models.

摘要：<paragraph>背景與目標：本研究評估大型語言模型 (LLM) 和視覺語言模型 (VLM) 在腸胃病學中的醫療推理表現。
方法：我們使用 300 個腸胃病學專科考試風格的多選題，其中 138 個包含影像，以系統性地評估模型配置和參數以及利用 GPT-3.5 的提示工程策略的影響。接下來，我們評估專有和開源 LLM（版本）的表現，包括 GPT（3.5、4、4o、4omini）、Claude（3、3.5）、Gemini（1.0）、Mistral、Llama（2、3、3.1）、Mixtral 和 Phi（3），跨不同介面（網路和 API）、運算環境（雲端和本地）和模型精確度（有和沒有量化）。最後，我們使用半自動化管道評估準確度。
結果：在專有模型中，GPT-4o（73.7%）和 Claude3.5-Sonnet（74.0%）達到最高準確度，優於頂尖的開源模型：Llama3.1-405b（64%）、Llama3.1-70b（58.3%）和 Mixtral-8x7b（54.3%）。在量化的開源模型中，6 位元量化的 Phi3-14b（48.7%）表現最佳。量化模型的分數與全精度模型 Llama2-7b、Llama2--13b 和 Gemma2-9b 相當。值得注意的是，當提供影像時，VLM 在包含影像的問題上的表現並未改善，而在提供 LLM 生成的標題時表現惡化。相反地，當影像附有人工製作的影像描述時，準確度觀察到增加了 10%。
結論：結論而言，雖然 LLM 在醫療推理中表現出強健的零次學習表現，但視覺資料的整合仍然是 VLM 的一項挑戰。有效的部署涉及仔細確定最佳模型配置，鼓勵使用者考慮專有模型的高效能或開源模型的靈活適應性。</paragraph>

##### **PropSAM: A Propagation-Based Model for Segmenting Any 3D Objects in Multi-Modal Medical Images**
2408.13836v1 by Zifan Chen, Xinyu Nan, Jiazheng Li, Jie Zhao, Haifeng Li, Zilin Lin, Haoshen Li, Heyun Chen, Yiting Liu, Bin Dong, Li Zhang, Lei Tang

Volumetric segmentation is crucial for medical imaging but is often
constrained by labor-intensive manual annotations and the need for
scenario-specific model training. Furthermore, existing general segmentation
models are inefficient due to their design and inferential approaches.
Addressing this clinical demand, we introduce PropSAM, a propagation-based
segmentation model that optimizes the use of 3D medical structure information.
PropSAM integrates a CNN-based UNet for intra-slice processing with a
Transformer-based module for inter-slice propagation, focusing on structural
and semantic continuities to enhance segmentation across various modalities.
Distinctively, PropSAM operates on a one-view prompt, such as a 2D bounding box
or sketch mask, unlike conventional models that require two-view prompts. It
has demonstrated superior performance, significantly improving the Dice
Similarity Coefficient (DSC) across 44 medical datasets and various imaging
modalities, outperforming models like MedSAM and SegVol with an average DSC
improvement of 18.1%. PropSAM also maintains stable predictions despite prompt
deviations and varying propagation configurations, confirmed by one-way ANOVA
tests with P>0.5985 and P>0.6131, respectively. Moreover, PropSAM's efficient
architecture enables faster inference speeds (Wilcoxon rank-sum test, P<0.001)
and reduces user interaction time by 37.8% compared to two-view prompt models.
Its ability to handle irregular and complex objects with robust performance
further demonstrates its potential in clinical settings, facilitating more
automated and reliable medical imaging analyses with minimal retraining.

摘要：體積分割對於醫學影像至關重要，但通常受到耗費大量人力的標註和特定場景模型訓練需求的限制。此外，現有的通用分割模型由於其設計和推論方法而效率低下。為了滿足這項臨床需求，我們引入了 PropSAM，這是一種基於傳播的分割模型，優化了 3D 醫學結構資訊的使用。PropSAM 整合了一個基於 CNN 的 UNet，用於切片內處理，以及一個基於 Transformer 的模組，用於切片間傳播，重點關注結構和語義連續性，以增強各種模式下的分割。與需要兩視提示的傳統模型不同，PropSAM 獨特地運作於單視提示上，例如 2D 邊界框或草圖遮罩。它已證明具有優異的效能，顯著改善了 44 個醫學資料集和各種影像模式下的骰子相似係數 (DSC)，優於 MedSAM 和 SegVol 等模型，平均 DSC 提升了 18.1%。儘管提示偏差和傳播配置不同，PropSAM 仍能維持穩定的預測，這已通過單向 ANOVA 測試得到證實，分別為 P>0.5985 和 P>0.6131。此外，PropSAM 的高效架構能實現更快的推論速度（Wilcoxon 等級和總和檢定，P<0.001），並將使用者互動時間減少了 37.8%，優於兩視提示模型。它在處理不規則和複雜物件時能展現出穩健的效能，進一步證明了其在臨床環境中的潛力，有助於以最少的重新訓練進行更自動化和可靠的醫學影像分析。

