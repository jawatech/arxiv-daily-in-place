{"2412.15523": {"publish_time": "2024-12-20", "title": "InstructOCR: Instruction Boosting Scene Text Spotting", "paper_summary": "In the field of scene text spotting, previous OCR methods primarily relied on\nimage encoders and pre-trained text information, but they often overlooked the\nadvantages of incorporating human language instructions. To address this gap,\nwe propose InstructOCR, an innovative instruction-based scene text spotting\nmodel that leverages human language instructions to enhance the understanding\nof text within images. Our framework employs both text and image encoders\nduring training and inference, along with instructions meticulously designed\nbased on text attributes. This approach enables the model to interpret text\nmore accurately and flexibly. Extensive experiments demonstrate the\neffectiveness of our model and we achieve state-of-the-art results on widely\nused benchmarks. Furthermore, the proposed framework can be seamlessly applied\nto scene text VQA tasks. By leveraging instruction strategies during\npre-training, the performance on downstream VQA tasks can be significantly\nimproved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on\nthe ST-VQA dataset. These experimental results provide insights into the\nbenefits of incorporating human language instructions for OCR-related tasks.", "paper_summary_zh": "\u5728\u5834\u666f\u6587\u5b57\u5075\u6e2c\u9818\u57df\uff0c\u5148\u524d\u7684 OCR \u65b9\u6cd5\u4e3b\u8981\u4f9d\u8cf4\u5f71\u50cf\u7de8\u78bc\u5668\u548c\u9810\u5148\u8a13\u7df4\u7684\u6587\u5b57\u8cc7\u8a0a\uff0c\u4f46\u5b83\u5011\u5e38\u5e38\u5ffd\u7565\u4e86\u7d50\u5408\u4eba\u985e\u8a9e\u8a00\u6307\u4ee4\u7684\u512a\u52e2\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa InstructOCR\uff0c\u4e00\u500b\u5275\u65b0\u7684\u57fa\u65bc\u6307\u4ee4\u7684\u5834\u666f\u6587\u5b57\u5075\u6e2c\u6a21\u578b\uff0c\u5229\u7528\u4eba\u985e\u8a9e\u8a00\u6307\u4ee4\u4f86\u589e\u5f37\u5c0d\u5f71\u50cf\u4e2d\u6587\u5b57\u7684\u7406\u89e3\u3002\u6211\u5011\u7684\u67b6\u69cb\u5728\u8a13\u7df4\u548c\u63a8\u8ad6\u671f\u9593\u540c\u6642\u63a1\u7528\u6587\u5b57\u548c\u5f71\u50cf\u7de8\u78bc\u5668\uff0c\u4e26\u6839\u64da\u6587\u5b57\u5c6c\u6027\u7cbe\u5fc3\u8a2d\u8a08\u6307\u4ee4\u3002\u9019\u7a2e\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u5920\u66f4\u6e96\u78ba\u548c\u9748\u6d3b\u5730\u89e3\u8b80\u6587\u5b57\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5728\u5ee3\u6cdb\u4f7f\u7528\u7684\u57fa\u6e96\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u9032\u7684\u7d50\u679c\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u69cb\u53ef\u4ee5\u7121\u7e2b\u5730\u61c9\u7528\u65bc\u5834\u666f\u6587\u5b57 VQA \u4efb\u52d9\u3002\u900f\u904e\u5728\u9810\u8a13\u7df4\u671f\u9593\u5229\u7528\u6307\u4ee4\u7b56\u7565\uff0c\u53ef\u4ee5\u986f\u8457\u63d0\u9ad8\u4e0b\u6e38 VQA \u4efb\u52d9\u7684\u6548\u80fd\uff0c\u5728 TextVQA \u8cc7\u6599\u96c6\u4e0a\u63d0\u5347 2.6%\uff0c\u5728 ST-VQA \u8cc7\u6599\u96c6\u4e0a\u63d0\u5347 2.1%\u3002\u9019\u4e9b\u5be6\u9a57\u7d50\u679c\u63d0\u4f9b\u4e86\u5c07\u4eba\u985e\u8a9e\u8a00\u6307\u4ee4\u7d0d\u5165 OCR \u76f8\u95dc\u4efb\u52d9\u7684\u512a\u9ede\u898b\u89e3\u3002", "author": "Chen Duan et.al.", "authors": "Chen Duan, Qianyi Jiang, Pei Fu, Jiamin Chen, Shengxi Li, Zining Wang, Shan Guo, Junfeng Luo", "id": "2412.15523v1", "paper_url": "http://arxiv.org/abs/2412.15523v1", "repo": "null"}}