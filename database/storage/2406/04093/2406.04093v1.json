{"2406.04093": {"publish_time": "2024-06-06", "title": "Scaling and evaluating sparse autoencoders", "paper_summary": "Sparse autoencoders provide a promising unsupervised approach for extracting\ninterpretable features from a language model by reconstructing activations from\na sparse bottleneck layer. Since language models learn many concepts,\nautoencoders need to be very large to recover all relevant features. However,\nstudying the properties of autoencoder scaling is difficult due to the need to\nbalance reconstruction and sparsity objectives and the presence of dead\nlatents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to\ndirectly control sparsity, simplifying tuning and improving the\nreconstruction-sparsity frontier. Additionally, we find modifications that\nresult in few dead latents, even at the largest scales we tried. Using these\ntechniques, we find clean scaling laws with respect to autoencoder size and\nsparsity. We also introduce several new metrics for evaluating feature quality\nbased on the recovery of hypothesized features, the explainability of\nactivation patterns, and the sparsity of downstream effects. These metrics all\ngenerally improve with autoencoder size. To demonstrate the scalability of our\napproach, we train a 16 million latent autoencoder on GPT-4 activations for 40\nbillion tokens. We release training code and autoencoders for open-source\nmodels, as well as a visualizer.", "paper_summary_zh": "\u7a00\u758f\u81ea\u52d5\u7de8\u78bc\u5668\u63d0\u4f9b\u4e86\u4e00\u7a2e\u6709\u524d\u9014\u7684\u975e\u76e3\u7763\u5f0f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u904e\u91cd\u5efa\u7a00\u758f\u74f6\u9838\u5c64\u7684\u6fc0\u6d3b\u4f86\u5f9e\u8a9e\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u53ef\u89e3\u91cb\u7279\u5fb5\u3002\u7531\u65bc\u8a9e\u8a00\u6a21\u578b\u5b78\u7fd2\u4e86\u5f88\u591a\u6982\u5ff5\uff0c\u56e0\u6b64\u81ea\u52d5\u7de8\u78bc\u5668\u9700\u8981\u975e\u5e38\u5927\u624d\u80fd\u6062\u5fa9\u6240\u6709\u76f8\u95dc\u7279\u5fb5\u3002\u7136\u800c\uff0c\u7531\u65bc\u9700\u8981\u5e73\u8861\u91cd\u5efa\u548c\u7a00\u758f\u6027\u76ee\u6a19\u4ee5\u53ca\u5b58\u5728\u6b7b\u6f5b\u5728\u8b8a\u91cf\uff0c\u56e0\u6b64\u7814\u7a76\u81ea\u52d5\u7de8\u78bc\u5668\u64f4\u5c55\u7684\u5c6c\u6027\u5f88\u56f0\u96e3\u3002\u6211\u5011\u5efa\u8b70\u4f7f\u7528 k \u7a00\u758f\u81ea\u52d5\u7de8\u78bc\u5668 [Makhzani \u548c Frey\uff0c2013] \u4f86\u76f4\u63a5\u63a7\u5236\u7a00\u758f\u6027\uff0c\u7c21\u5316\u8abf\u6574\u4e26\u6539\u5584\u91cd\u5efa\u7a00\u758f\u6027\u524d\u6cbf\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u5373\u4f7f\u5728\u6211\u5011\u5617\u8a66\u7684\u6700\u5927\u898f\u6a21\u4e0b\uff0c\u4fee\u6539\u4e5f\u6703\u5c0e\u81f4\u5f88\u5c11\u7684\u6b7b\u6f5b\u5728\u8b8a\u91cf\u3002\u4f7f\u7528\u9019\u4e9b\u6280\u8853\uff0c\u6211\u5011\u767c\u73fe\u4e86\u95dc\u65bc\u81ea\u52d5\u7de8\u78bc\u5668\u5927\u5c0f\u548c\u7a00\u758f\u6027\u7684\u6e05\u6670\u64f4\u5c55\u5b9a\u5f8b\u3002\u6211\u5011\u9084\u5f15\u5165\u4e86\u5e7e\u500b\u65b0\u7684\u6307\u6a19\uff0c\u7528\u65bc\u57fa\u65bc\u5047\u8a2d\u7279\u5fb5\u7684\u6062\u5fa9\u3001\u6fc0\u6d3b\u6a21\u5f0f\u7684\u53ef\u89e3\u91cb\u6027\u548c\u4e0b\u6e38\u5f71\u97ff\u7684\u7a00\u758f\u6027\u4f86\u8a55\u4f30\u7279\u5fb5\u8cea\u91cf\u3002\u9019\u4e9b\u6307\u6a19\u901a\u5e38\u96a8\u8457\u81ea\u52d5\u7de8\u78bc\u5668\u7684\u5927\u5c0f\u800c\u63d0\u9ad8\u3002\u70ba\u4e86\u8b49\u660e\u6211\u5011\u65b9\u6cd5\u7684\u53ef\u64f4\u5c55\u6027\uff0c\u6211\u5011\u5728 GPT-4 \u6fc0\u6d3b\u4e0a\u8a13\u7df4\u4e86\u4e00\u500b 1600 \u842c\u6f5b\u5728\u81ea\u52d5\u7de8\u78bc\u5668\uff0c\u7528\u65bc 400 \u5104\u500b\u7b26\u865f\u3002\u6211\u5011\u767c\u5e03\u4e86\u7528\u65bc\u958b\u6e90\u6a21\u578b\u7684\u8a13\u7df4\u4ee3\u78bc\u548c\u81ea\u52d5\u7de8\u78bc\u5668\uff0c\u4ee5\u53ca\u4e00\u500b\u53ef\u8996\u5316\u5668\u3002", "author": "Leo Gao et.al.", "authors": "Leo Gao, Tom Dupr\u00e9 la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, Jeffrey Wu", "id": "2406.04093v1", "paper_url": "http://arxiv.org/abs/2406.04093v1", "repo": "null"}}