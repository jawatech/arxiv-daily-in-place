{"2409.04183": {"publish_time": "2024-09-06", "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding", "paper_summary": "Programming languages possess rich semantic information such as data flow\nthat is represented by graphs and not available from the surface form of source\ncode. Recent code language models have scaled to billions of parameters, but\nmodel source code solely as text tokens while ignoring any other structural\ninformation. Conversely, models that do encode structural information of code\nmake modifications to the Transformer architecture, limiting their scale and\ncompatibility with pretrained LLMs. In this work, we take the best of both\nworlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph\nneural networks and cross-modal alignment technologies to inject the structural\ninformation of code into LLMs as an auxiliary task during finetuning. This\nframework is both model-agnostic and task-agnostic, as it can be applied to any\ncode LLM for any code downstream task, and requires the structural graph data\nonly at training time from a corpus unrelated to the finetuning data, while\nincurring no cost at inference time over the baseline LLM. Experiments on five\ncode tasks with four different baseline LLMs ranging in size from 350M to 8B\nvalidate the effectiveness of GALLa, demonstrating consistent improvement over\nthe baseline, even for powerful models such as LLaMA3.", "paper_summary_zh": "\u7a0b\u5f0f\u8a9e\u8a00\u64c1\u6709\u8c50\u5bcc\u7684\u8a9e\u610f\u8cc7\u8a0a\uff0c\u4f8b\u5982\u7531\u5716\u5f62\u8868\u793a\u4e14\u7121\u6cd5\u5f9e\u539f\u59cb\u78bc\u8868\u9762\u5f62\u5f0f\u53d6\u5f97\u7684\u8cc7\u6599\u6d41\u7a0b\u3002\u6700\u8fd1\u7684\u7a0b\u5f0f\u78bc\u8a9e\u8a00\u6a21\u578b\u5df2\u64f4\u5145\u81f3\u6578\u5341\u5104\u500b\u53c3\u6578\uff0c\u4f46\u6a21\u578b\u539f\u59cb\u78bc\u50c5\u4f5c\u70ba\u6587\u5b57\u7b26\u865f\uff0c\u800c\u5ffd\u7565\u4efb\u4f55\u5176\u4ed6\u7d50\u69cb\u8cc7\u8a0a\u3002\u53cd\u4e4b\uff0c\u7de8\u78bc\u7a0b\u5f0f\u78bc\u7d50\u69cb\u8cc7\u8a0a\u7684\u6a21\u578b\u6703\u4fee\u6539 Transformer \u67b6\u69cb\uff0c\u9650\u5236\u5176\u898f\u6a21\u548c\u8207\u9810\u5148\u8a13\u7df4\u7684 LLM \u7684\u76f8\u5bb9\u6027\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a1\u7528 GALLa\uff08\u5716\u5f62\u5c0d\u9f4a\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff09\u64f7\u53d6\u5169\u5168\u5176\u7f8e\u7684\u512a\u9ede\u3002GALLa \u5229\u7528\u5716\u5f62\u795e\u7d93\u7db2\u8def\u548c\u8de8\u6a21\u614b\u5c0d\u9f4a\u6280\u8853\uff0c\u5728\u5fae\u8abf\u671f\u9593\u5c07\u7a0b\u5f0f\u78bc\u7684\u7d50\u69cb\u8cc7\u8a0a\u6ce8\u5165 LLM \u4f5c\u70ba\u8f14\u52a9\u4efb\u52d9\u3002\u6b64\u67b6\u69cb\u540c\u6642\u4e0d\u4f9d\u8cf4\u6a21\u578b\u548c\u4efb\u52d9\uff0c\u56e0\u70ba\u5b83\u53ef\u4ee5\u61c9\u7528\u65bc\u4efb\u4f55\u7a0b\u5f0f\u78bc LLM \u7684\u4efb\u4f55\u7a0b\u5f0f\u78bc\u4e0b\u6e38\u4efb\u52d9\uff0c\u4e26\u4e14\u50c5\u5728\u8a13\u7df4\u671f\u9593\u5f9e\u8207\u5fae\u8abf\u8cc7\u6599\u7121\u95dc\u7684\u8a9e\u6599\u5eab\u53d6\u5f97\u7d50\u69cb\u5716\u5f62\u8cc7\u6599\uff0c\u540c\u6642\u5728\u63a8\u8ad6\u671f\u9593\u4e0d\u7522\u751f\u6bd4\u57fa\u6e96 LLM \u66f4\u9ad8\u7684\u6210\u672c\u3002\u5728\u4e94\u500b\u7a0b\u5f0f\u78bc\u4efb\u52d9\u4e2d\u9032\u884c\u5be6\u9a57\uff0c\u4f7f\u7528\u56db\u500b\u4e0d\u540c\u7684\u57fa\u6e96 LLM\uff0c\u898f\u6a21\u5f9e 350M \u5230 8B\uff0c\u9a57\u8b49 GALLa \u7684\u6709\u6548\u6027\uff0c\u8b49\u660e\u5373\u4f7f\u5c0d\u65bc LLaMA3 \u7b49\u5f37\u5927\u6a21\u578b\uff0c\u4e5f\u80fd\u6301\u7e8c\u512a\u65bc\u57fa\u6e96\u3002", "author": "Ziyin Zhang et.al.", "authors": "Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang", "id": "2409.04183v1", "paper_url": "http://arxiv.org/abs/2409.04183v1", "repo": "null"}}