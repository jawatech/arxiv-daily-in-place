{"2410.02642": {"publish_time": "2024-10-03", "title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers", "paper_summary": "Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.", "paper_summary_zh": "\u8cc7\u8a0a\u6aa2\u7d22 (IR) \u7cfb\u7d71\u5728\u73fe\u4ee3\u6578\u4f4d\u751f\u6d3b\u4e2d\u626e\u6f14\u4e86\u81f3\u95dc\u91cd\u8981\u7684\u89d2\u8272\uff0c\u4e26\u900f\u904e\u6aa2\u7d22\u589e\u5f37\u751f\u6210\uff0c\u5728\u751f\u6210\u5f0f AI \u7684\u65b0\u6642\u4ee3\u4e2d\u978f\u56fa\u4e86\u5176\u6301\u7e8c\u7684\u5be6\u7528\u6027\u3002\u5177\u5099\u5f37\u5927\u7684\u8a9e\u8a00\u8655\u7406\u80fd\u529b\u548c\u975e\u51e1\u7684\u591a\u529f\u80fd\u6027\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba IR \u7cfb\u7d71\u4e2d\u96f6\u6b21\u5b78\u7fd2\u91cd\u65b0\u6392\u5e8f\u7684\u71b1\u9580\u9078\u64c7\u3002\u5230\u76ee\u524d\u70ba\u6b62\uff0c\u57fa\u65bc LLM \u7684\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u4f9d\u8cf4\u65bc\u5f37\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u9019\u5c07\u5176\u4f7f\u7528\u9650\u5236\u5728\u5c08\u9580\u7684\u6216\u5f37\u5927\u7684\u5c08\u6709\u6a21\u578b\u4e2d\u3002\u6709\u9451\u65bc\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u7591\u554f\uff1a\u81ea\u8ff4\u6b78\u751f\u6210\u5c0d\u65bc LLM \u57f7\u884c\u91cd\u65b0\u6392\u5e8f\u4f86\u8aaa\u662f\u5426\u5fc5\u8981\u4e14\u6700\u4f73\uff1f\u6211\u5011\u5047\u8a2d LLM \u4e2d\u6709\u5927\u91cf\u8207\u91cd\u65b0\u6392\u5e8f\u76f8\u95dc\u7684\u8a0a\u865f\uff0c\u800c\u9019\u4e9b\u8a0a\u865f\u53ef\u80fd\u7121\u6cd5\u900f\u904e\u751f\u6210\u767c\u63ee\u5176\u5168\u90e8\u6f5b\u529b\u3002\u70ba\u4e86\u66f4\u76f4\u63a5\u5730\u5229\u7528\u9019\u4e9b\u8a0a\u865f\uff0c\u6211\u5011\u63d0\u51fa\u60c5\u5883\u5167\u91cd\u65b0\u6392\u5e8f (ICR)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u641c\u5c0b\u67e5\u8a62\u6240\u9020\u6210\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u8b8a\u5316\u4f86\u9032\u884c\u6e96\u78ba\u4e14\u6709\u6548\u7684\u91cd\u65b0\u6392\u5e8f\u3002\u70ba\u4e86\u6e1b\u8f15 LLM \u4e2d\u7684\u5167\u5728\u504f\u5dee\uff0c\u6211\u5011\u63d0\u51fa\u4f7f\u7528\u7121\u5167\u5bb9\u67e5\u8a62\u7684\u6821\u6b63\u65b9\u6cd5\u3002\u7531\u65bc\u6c92\u6709\u751f\u6210\uff0cICR \u53ea\u9700\u8981\u5169\u6b21 ($O(1)$) \u524d\u5411\u50b3\u905e\u4f86\u91cd\u65b0\u6392\u5e8f $N$ \u500b\u6587\u4ef6\uff0c\u4f7f\u5176\u6bd4\u9700\u8981\u81f3\u5c11 $O(N)$ \u6b21\u524d\u5411\u50b3\u905e\u7684\u751f\u6210\u5f0f\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u5927\u5e45\u63d0\u9ad8\u6548\u7387\u3002\u6211\u5011\u7684\u65b0\u7a4e\u8a2d\u8a08\u9084\u4f7f ICR \u80fd\u5920\u61c9\u7528\u65bc\u4efb\u4f55 LLM\uff0c\u800c\u7121\u9700\u5c08\u9580\u8a13\u7df4\uff0c\u540c\u6642\u4fdd\u8b49\u5f62\u6210\u826f\u597d\u7684\u6392\u540d\u3002\u5728\u6a19\u6e96\u55ae\u6b21\u8df3\u8e8d\u548c\u591a\u8df3\u8e8d\u8cc7\u8a0a\u6aa2\u7d22\u57fa\u6e96\u4e0a\u4f7f\u7528\u5169\u500b\u6d41\u884c\u7684\u958b\u653e\u6b0a\u91cd LLM \u9032\u884c\u7684\u5ee3\u6cdb\u5be6\u9a57\u8868\u660e\uff0cICR \u5728\u5be6\u52d9\u4e0a\u512a\u65bc RankGPT\uff0c\u540c\u6642\u5c07\u5ef6\u9072\u6642\u9593\u6e1b\u5c11\u4e86 60% \u4ee5\u4e0a\u3002\u900f\u904e\u8a73\u7d30\u5206\u6790\uff0c\u6211\u5011\u8868\u660e ICR \u7684\u6548\u80fd\u7279\u5225\u5f37\u65bc\u9700\u8981\u66f4\u8907\u96dc\u91cd\u65b0\u6392\u5e8f\u8a0a\u865f\u7684\u4efb\u52d9\u3002\u6211\u5011\u7684\u767c\u73fe\u547c\u7c72\u9032\u4e00\u6b65\u63a2\u7d22\u5728\u6587\u5b57\u751f\u6210\u4e4b\u5916\u5229\u7528\u958b\u653e\u6b0a\u91cd LLM \u7684\u65b0\u65b9\u6cd5\u3002", "author": "Shijie Chen et.al.", "authors": "Shijie Chen, Bernal Jim\u00e9nez Guti\u00e9rrez, Yu Su", "id": "2410.02642v1", "paper_url": "http://arxiv.org/abs/2410.02642v1", "repo": "null"}}