{"2406.02913": {"publish_time": "2024-06-05", "title": "Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity", "paper_summary": "Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning\nLarge Language Models using only forward passes. However, the application of ZO\nfine-tuning in memory-constrained settings such as mobile phones and laptops is\nstill challenging since full precision forward passes are infeasible. In this\nstudy, we address this limitation by integrating sparsity and quantization into\nZO fine-tuning of LLMs. Specifically, we investigate the feasibility of\nfine-tuning an extremely small subset of LLM parameters using ZO. This approach\nallows the majority of un-tuned parameters to be quantized to accommodate the\nconstraint of limited device memory. Our findings reveal that the pre-training\nprocess can identify a set of \"sensitive parameters\" that can guide the ZO\nfine-tuning of LLMs on downstream tasks. Our results demonstrate that\nfine-tuning 0.1% sensitive parameters in the LLM with ZO can outperform the\nfull ZO fine-tuning performance, while offering wall-clock time speedup.\nAdditionally, we show that ZO fine-tuning targeting these 0.1% sensitive\nparameters, combined with 4 bit quantization, enables efficient ZO fine-tuning\nof an Llama2-7B model on a GPU device with less than 8 GiB of memory and\nnotably reduced latency.", "paper_summary_zh": "\u96f6\u968e\u6700\u4f73\u5316 (ZO) \u662f\u4e00\u7a2e\u8a18\u61b6\u9ad4\u6548\u7387\u9ad8\u7684\u7b56\u7565\uff0c\u7528\u65bc\u50c5\u4f7f\u7528\u524d\u5411\u50b3\u905e\u4f86\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u3002\u7136\u800c\uff0cZO \u5fae\u8abf\u5728\u8a18\u61b6\u9ad4\u53d7\u9650\u7684\u8a2d\u5b9a\u4e2d\uff08\u4f8b\u5982\u884c\u52d5\u96fb\u8a71\u548c\u7b46\u96fb\uff09\u7684\u61c9\u7528\u4ecd\u7136\u5177\u6709\u6311\u6230\u6027\uff0c\u56e0\u70ba\u5168\u7cbe\u5ea6\u7684\u524d\u5411\u50b3\u905e\u4e0d\u53ef\u884c\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e\u5c07\u7a00\u758f\u6027\u548c\u91cf\u5316\u6574\u5408\u5230 LLM \u7684 ZO \u5fae\u8abf\u4e2d\u4f86\u89e3\u6c7a\u9019\u500b\u9650\u5236\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u4f7f\u7528 ZO \u5fae\u8abf LLM \u6975\u5c0f\u53c3\u6578\u5b50\u96c6\u7684\u53ef\u884c\u6027\u3002\u9019\u7a2e\u65b9\u6cd5\u5141\u8a31\u5c07\u5927\u591a\u6578\u672a\u8abf\u6574\u7684\u53c3\u6578\u91cf\u5316\uff0c\u4ee5\u9069\u61c9\u88dd\u7f6e\u8a18\u61b6\u9ad4\u6709\u9650\u7684\u9650\u5236\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0c\u9810\u8a13\u7df4\u904e\u7a0b\u53ef\u4ee5\u627e\u51fa\u4e00\u500b\u300c\u654f\u611f\u53c3\u6578\u300d\u96c6\u5408\uff0c\u7528\u65bc\u6307\u5c0e LLM \u5728\u4e0b\u6e38\u4efb\u52d9\u4e0a\u7684 ZO \u5fae\u8abf\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\uff0c\u4f7f\u7528 ZO \u5fae\u8abf LLM \u4e2d\u7684 0.1% \u654f\u611f\u53c3\u6578\u53ef\u4ee5\u512a\u65bc\u5b8c\u6574\u7684 ZO \u5fae\u8abf\u6548\u80fd\uff0c\u540c\u6642\u63d0\u4f9b\u7246\u4e0a\u6642\u9593\u52a0\u901f\u3002\u6b64\u5916\uff0c\u6211\u5011\u8868\u660e ZO \u5fae\u8abf\u91dd\u5c0d\u9019\u4e9b 0.1% \u654f\u611f\u53c3\u6578\uff0c\u7d50\u5408 4 \u4f4d\u5143\u91cf\u5316\uff0c\u53ef\u4ee5\u5728\u8a18\u61b6\u9ad4\u5c0f\u65bc 8 GiB \u7684 GPU \u88dd\u7f6e\u4e0a\u6709\u6548\u57f7\u884c Llama2-7B \u6a21\u578b\u7684 ZO \u5fae\u8abf\uff0c\u800c\u4e14\u5927\u5e45\u964d\u4f4e\u5ef6\u9072\u3002", "author": "Wentao Guo et.al.", "authors": "Wentao Guo, Jikai Long, Yimeng Zeng, Zirui Liu, Xinyu Yang, Yide Ran, Jacob R. Gardner, Osbert Bastani, Christopher De Sa, Xiaodong Yu, Beidi Chen, Zhaozhuo Xu", "id": "2406.02913v1", "paper_url": "http://arxiv.org/abs/2406.02913v1", "repo": "null"}}