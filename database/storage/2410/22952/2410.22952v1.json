{"2410.22952": {"publish_time": "2024-10-30", "title": "Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation", "paper_summary": "A common strategy for Parameter-Efficient Fine-Tuning (PEFT) of pre-trained\nVision Transformers (ViTs) involves adapting the model to downstream tasks by\nlearning a low-rank adaptation matrix. This matrix is decomposed into a product\nof down-projection and up-projection matrices, with the bottleneck\ndimensionality being crucial for reducing the number of learnable parameters,\nas exemplified by prevalent methods like LoRA and Adapter. However, these\nlow-rank strategies typically employ a fixed bottleneck dimensionality, which\nlimits their flexibility in handling layer-wise variations. To address this\nlimitation, we propose a novel PEFT approach inspired by Singular Value\nDecomposition (SVD) for representing the adaptation matrix. SVD decomposes a\nmatrix into the product of a left unitary matrix, a diagonal matrix of scaling\nvalues, and a right unitary matrix. We utilize Householder transformations to\nconstruct orthogonal matrices that efficiently mimic the unitary matrices,\nrequiring only a vector. The diagonal values are learned in a layer-wise\nmanner, allowing them to flexibly capture the unique properties of each layer.\nThis approach enables the generation of adaptation matrices with varying ranks\nacross different layers, providing greater flexibility in adapting pre-trained\nmodels. Experiments on standard downstream vision tasks demonstrate that our\nmethod achieves promising fine-tuning performance.", "paper_summary_zh": "\u5e38\u898b\u7684\u9810\u8a13\u7df4 Vision Transformers (ViTs) \u53c3\u6578\u6709\u6548\u5fae\u8abf (PEFT) \u7b56\u7565\u5305\u542b\u900f\u904e\u5b78\u7fd2\u4f4e\u968e\u9069\u61c9\u77e9\u9663\u4f86\u8abf\u6574\u6a21\u578b\u81f3\u4e0b\u6e38\u4efb\u52d9\u3002\u6b64\u77e9\u9663\u5206\u89e3\u70ba\u4e0b\u6295\u5f71\u548c\u4e0a\u6295\u5f71\u77e9\u9663\u7684\u4e58\u7a4d\uff0c\u5176\u4e2d\u74f6\u9838\u7dad\u5ea6\u5c0d\u65bc\u6e1b\u5c11\u53ef\u5b78\u7fd2\u53c3\u6578\u7684\u6578\u91cf\u81f3\u95dc\u91cd\u8981\uff0c\u4f8b\u5982 LoRA \u548c Adapter \u7b49\u666e\u904d\u65b9\u6cd5\u6240\u8209\u4f8b\u8aaa\u660e\u7684\u3002\u7136\u800c\uff0c\u9019\u4e9b\u4f4e\u968e\u7b56\u7565\u901a\u5e38\u63a1\u7528\u56fa\u5b9a\u7684\u74f6\u9838\u7dad\u5ea6\uff0c\u9019\u6703\u9650\u5236\u5b83\u5011\u5728\u8655\u7406\u9010\u5c64\u8b8a\u5316\u6642\u7684\u9748\u6d3b\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7684 PEFT \u65b9\u6cd5\uff0c\u9748\u611f\u4f86\u81ea\u5947\u7570\u503c\u5206\u89e3 (SVD) \u4ee5\u8868\u793a\u9069\u61c9\u77e9\u9663\u3002SVD \u5c07\u77e9\u9663\u5206\u89e3\u70ba\u5de6\u9149\u77e9\u9663\u3001\u7e2e\u653e\u503c\u7684\u5c0d\u89d2\u77e9\u9663\u548c\u53f3\u9149\u77e9\u9663\u7684\u4e58\u7a4d\u3002\u6211\u5011\u5229\u7528 Householder \u8f49\u63db\u4f86\u5efa\u69cb\u6b63\u4ea4\u77e9\u9663\uff0c\u4ee5\u6709\u6548\u5730\u6a21\u64ec\u9149\u77e9\u9663\uff0c\u53ea\u9700\u8981\u4e00\u500b\u5411\u91cf\u3002\u5c0d\u89d2\u503c\u662f\u4ee5\u9010\u5c64\u65b9\u5f0f\u5b78\u7fd2\u7684\uff0c\u8b93\u5b83\u5011\u80fd\u5920\u9748\u6d3b\u5730\u64f7\u53d6\u6bcf\u500b\u5c64\u7684\u7368\u7279\u5c6c\u6027\u3002\u6b64\u65b9\u6cd5\u80fd\u7522\u751f\u5177\u6709\u4e0d\u540c\u5c64\u9593\u4e0d\u540c\u79e9\u7684\u9069\u61c9\u77e9\u9663\uff0c\u5728\u8abf\u6574\u9810\u8a13\u7df4\u6a21\u578b\u6642\u63d0\u4f9b\u66f4\u5927\u7684\u9748\u6d3b\u6027\u3002\u5728\u6a19\u6e96\u4e0b\u6e38\u8996\u89ba\u4efb\u52d9\u4e0a\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u9054\u5230\u4e86\u4ee4\u4eba\u6eff\u610f\u7684\u5fae\u8abf\u6548\u80fd\u3002", "author": "Wei Dong et.al.", "authors": "Wei Dong, Yuan Sun, Yiting Yang, Xing Zhang, Zhijun Lin, Qingsen Yan, Haokui Zhang, Peng Wang, Yang Yang, Hengtao Shen", "id": "2410.22952v1", "paper_url": "http://arxiv.org/abs/2410.22952v1", "repo": "null"}}