{"2407.19807": {"publish_time": "2024-07-29", "title": "Cool-Fusion: Fuse Large Language Models without Training", "paper_summary": "We focus on the problem of fusing two or more heterogeneous large language\nmodels (LLMs) to facilitate their complementary strengths. One of the\nchallenges on model fusion is high computational load, i.e. to fine-tune or to\nalign vocabularies via combinatorial optimization. To this end, we propose\n\\emph{Cool-Fusion}, a simple yet effective approach that fuses the knowledge of\nheterogeneous source LLMs to leverage their complementary strengths.\n\\emph{Cool-Fusion} is the first method that does not require any type of\ntraining like the ensemble approaches. But unlike ensemble methods, it is\napplicable to any set of source LLMs that have different vocabularies. The\nbasic idea is to have each source LLM individually generate tokens until the\ntokens can be decoded into a text segment that ends at word boundaries common\nto all source LLMs. Then, the source LLMs jointly rerank the generated text\nsegment and select the best one, which is the fused text generation in one\nstep. Extensive experiments are conducted across a variety of benchmark\ndatasets. On \\emph{GSM8K}, \\emph{Cool-Fusion} increases accuracy from three\nstrong source LLMs by a significant 8\\%-17.8\\%.", "paper_summary_zh": "<paragraph>\u6211\u4eec\u4e13\u6ce8\u4e8e\u878d\u5408\u4e24\u4e2a\u6216\u591a\u4e2a\u5f02\u6784\u5927\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5\u4fc3\u8fdb\u5176\u4e92\u8865\u4f18\u52bf\u3002\u6a21\u578b\u878d\u5408\u9762\u4e34\u7684\u6311\u6218\u4e4b\u4e00\u662f\u9ad8\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u5373\u901a\u8fc7\u7ec4\u5408\u4f18\u5316\u5fae\u8c03\u6216\u5bf9\u9f50\u8bcd\u6c47\u8868\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201cCool-Fusion\u201d\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5b83\u878d\u5408\u4e86\u5f02\u6784\u6e90 LLM \u7684\u77e5\u8bc6\uff0c\u4ee5\u5229\u7528\u5176\u4e92\u8865\u4f18\u52bf\u3002\u201cCool-Fusion\u201d\u662f\u7b2c\u4e00\u79cd\u4e0d\u9700\u8981\u4efb\u4f55\u7c7b\u578b\u8bad\u7ec3\uff08\u5982\u96c6\u6210\u65b9\u6cd5\uff09\u7684\u65b9\u6cd5\u3002\u4f46\u4e0e\u96c6\u6210\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u9002\u7528\u4e8e\u4efb\u4f55\u5177\u6709\u4e0d\u540c\u8bcd\u6c47\u8868\u7684\u6e90 LLM \u96c6\u5408\u3002\u57fa\u672c\u601d\u60f3\u662f\u8ba9\u6bcf\u4e2a\u6e90 LLM \u5355\u72ec\u751f\u6210\u6807\u8bb0\uff0c\u76f4\u5230\u6807\u8bb0\u53ef\u4ee5\u89e3\u7801\u6210\u6587\u672c\u6bb5\u843d\uff0c\u8be5\u6bb5\u843d\u4ee5\u6240\u6709\u6e90 LLM \u5171\u6709\u7684\u5355\u8bcd\u8fb9\u754c\u7ed3\u675f\u3002\u7136\u540e\uff0c\u6e90 LLM \u8054\u5408\u91cd\u65b0\u5bf9\u751f\u6210\u7684\u6587\u672c\u6bb5\u843d\u8fdb\u884c\u6392\u540d\uff0c\u5e76\u9009\u62e9\u6700\u4f73\u6587\u672c\u6bb5\u843d\uff0c\u8fd9\u662f\u5355\u6b65\u878d\u5408\u6587\u672c\u751f\u6210\u3002\u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5728\u201cGSM8K\u201d\u4e0a\uff0c\u201cCool-Fusion\u201d\u5c06\u4e09\u4e2a\u5f3a\u5927\u7684\u6e90 LLM \u7684\u51c6\u786e\u6027\u63d0\u9ad8\u4e86\u663e\u8457\u7684 8%-17.8%\u3002</paragraph>", "author": "Cong Liu et.al.", "authors": "Cong Liu, Xiaojun Quan, Yan Pan, Liang Lin, Weigang Wu, Xu Chen", "id": "2407.19807v1", "paper_url": "http://arxiv.org/abs/2407.19807v1", "repo": "null"}}