{"2410.23066": {"publish_time": "2024-10-30", "title": "Don't Just Pay Attention, PLANT It: Transfer L2R Models to Fine-tune Attention in Extreme Multi-Label Text Classification", "paper_summary": "State-of-the-art Extreme Multi-Label Text Classification (XMTC) models rely\nheavily on multi-label attention layers to focus on key tokens in input text,\nbut obtaining optimal attention weights is challenging and resource-intensive.\nTo address this, we introduce PLANT -- Pretrained and Leveraged AtteNTion -- a\nnovel transfer learning strategy for fine-tuning XMTC decoders. PLANT surpasses\nexisting state-of-the-art methods across all metrics on mimicfull, mimicfifty,\nmimicfour, eurlex, and wikiten datasets. It particularly excels in few-shot\nscenarios, outperforming previous models specifically designed for few-shot\nscenarios by over 50 percentage points in F1 scores on mimicrare and by over 36\npercentage points on mimicfew, demonstrating its superior capability in\nhandling rare codes. PLANT also shows remarkable data efficiency in few-shot\nscenarios, achieving precision comparable to traditional models with\nsignificantly less data. These results are achieved through key technical\ninnovations: leveraging a pretrained Learning-to-Rank model as the planted\nattention layer, integrating mutual-information gain to enhance attention,\nintroducing an inattention mechanism, and implementing a stateful-decoder to\nmaintain context. Comprehensive ablation studies validate the importance of\nthese contributions in realizing the performance gains.", "paper_summary_zh": "\u6700\u5148\u8fdb\u7684\u6975\u7aef\u591a\u6a19\u7c64\u6587\u672c\u5206\u985e (XMTC) \u6a21\u578b\u9ad8\u5ea6\u4f9d\u8cf4\u591a\u6a19\u7c64\u6ce8\u610f\u529b\u5c64\uff0c\u4ee5\u95dc\u6ce8\u8f38\u5165\u6587\u672c\u4e2d\u7684\u95dc\u9375\u4ee3\u78bc\uff0c\u4f46\u53d6\u5f97\u6700\u4f73\u6ce8\u610f\u529b\u6b0a\u91cd\u5177\u6709\u6311\u6230\u6027\u4e14\u8017\u8cbb\u8cc7\u6e90\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 PLANT\uff08\u9810\u8a13\u7df4\u548c\u69d3\u687f\u6ce8\u610f\u529b\uff09\u2014\u2014\u4e00\u7a2e\u91dd\u5c0d\u5fae\u8abf XMTC \u89e3\u78bc\u5668\u7684\u65b0\u7a4e\u7684\u9077\u79fb\u5b78\u7fd2\u7b56\u7565\u3002PLANT \u5728 mimicfull\u3001mimicfifty\u3001mimicfour\u3001eurlex \u548c wikiten \u8cc7\u6599\u96c6\u7684\u6240\u6709\u6307\u6a19\u4e0a\u90fd\u8d85\u8d8a\u4e86\u73fe\u6709\u7684\u6700\u5148\u9032\u65b9\u6cd5\u3002\u5b83\u5728\u5c11\u6b21\u5617\u8a66\u7684\u5834\u666f\u4e2d\u7279\u5225\u51fa\u8272\uff0c\u5728 mimicrare \u4e0a\u7684 F1 \u5206\u6578\u6bd4\u5c08\u9580\u70ba\u5c11\u6b21\u5617\u8a66\u7684\u5834\u666f\u8a2d\u8a08\u7684\u5148\u524d\u6a21\u578b\u9ad8\u51fa 50 \u500b\u767e\u5206\u9ede\uff0c\u5728 mimicfew \u4e0a\u9ad8\u51fa 36 \u500b\u767e\u5206\u9ede\uff0c\u8b49\u660e\u4e86\u5176\u5728\u8655\u7406\u7f55\u898b\u4ee3\u78bc\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\u3002PLANT \u5728\u5c11\u6b21\u5617\u8a66\u7684\u5834\u666f\u4e2d\u4e5f\u5c55\u73fe\u4e86\u986f\u8457\u7684\u8cc7\u6599\u6548\u7387\uff0c\u9054\u5230\u4e86\u8207\u50b3\u7d71\u6a21\u578b\u76f8\u7576\u7684\u7cbe\u5ea6\uff0c\u800c\u8cc7\u6599\u537b\u5c11\u5f97\u591a\u3002\u9019\u4e9b\u6210\u679c\u662f\u901a\u904e\u95dc\u9375\u6280\u8853\u5275\u65b0\u5be6\u73fe\u7684\uff1a\u5229\u7528\u9810\u8a13\u7df4\u7684\u5b78\u7fd2\u6392\u540d\u6a21\u578b\u4f5c\u70ba\u690d\u5165\u7684\u6ce8\u610f\u529b\u5c64\uff0c\u6574\u5408\u4e92\u8cc7\u8a0a\u589e\u76ca\u4ee5\u589e\u5f37\u6ce8\u610f\u529b\uff0c\u5f15\u5165\u4e0d\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4e26\u5be6\u4f5c\u6709\u72c0\u614b\u89e3\u78bc\u5668\u4ee5\u7dad\u8b77\u4e0a\u4e0b\u6587\u3002\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u9a57\u8b49\u4e86\u9019\u4e9b\u8ca2\u737b\u5728\u5be6\u73fe\u6548\u80fd\u63d0\u5347\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "author": "Debjyoti Saharoy et.al.", "authors": "Debjyoti Saharoy, Javed A. Aslam, Virgil Pavlu", "id": "2410.23066v1", "paper_url": "http://arxiv.org/abs/2410.23066v1", "repo": "null"}}