{"2412.08054": {"publish_time": "2024-12-11", "title": "Federated In-Context LLM Agent Learning", "paper_summary": "Large Language Models (LLMs) have revolutionized intelligent services by\nenabling logical reasoning, tool use, and interaction with external systems as\nagents. The advancement of LLMs is frequently hindered by the scarcity of\nhigh-quality data, much of which is inherently sensitive. Federated learning\n(FL) offers a potential solution by facilitating the collaborative training of\ndistributed LLMs while safeguarding private data. However, FL frameworks face\nsignificant bandwidth and computational demands, along with challenges from\nheterogeneous data distributions. The emerging in-context learning capability\nof LLMs offers a promising approach by aggregating natural language rather than\nbulky model parameters. Yet, this method risks privacy leakage, as it\nnecessitates the collection and presentation of data samples from various\nclients during aggregation. In this paper, we propose a novel\nprivacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,\nwhich to our best knowledge for the first work unleashes the power of\nin-context learning to train diverse LLM agents through FL. In our design,\nknowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums\nGeneration (KCG) module are transmitted between clients and the server instead\nof model parameters in previous FL methods. Apart from that, an incredible\nRetrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU)\nmodule is designed and we incorporate the aggregated global knowledge\ncompendium as a teacher to teach LLM agents the usage of tools. We conducted\nextensive experiments and the results show that FICAL has competitive\nperformance compared to other SOTA baselines with a significant communication\ncost decrease of $\\mathbf{3.33\\times10^5}$ times.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u8b93\u4ee3\u7406\u4eba\u9032\u884c\u908f\u8f2f\u63a8\u7406\u3001\u4f7f\u7528\u5de5\u5177\u4ee5\u53ca\u8207\u5916\u90e8\u7cfb\u7d71\u4e92\u52d5\uff0c\u9032\u800c\u9769\u65b0\u4e86\u667a\u6167\u670d\u52d9\u3002LLM \u7684\u9032\u5c55\u7d93\u5e38\u53d7\u5230\u9ad8\u54c1\u8cea\u8cc7\u6599\u77ed\u7f3a\u7684\u963b\u7919\uff0c\u5176\u4e2d\u8a31\u591a\u8cc7\u6599\u672c\u8cea\u4e0a\u662f\u654f\u611f\u7684\u3002\u806f\u5408\u5b78\u7fd2 (FL) \u63d0\u4f9b\u4e86\u4e00\u500b\u6f5b\u5728\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u5b83\u4fc3\u9032\u4e86\u5206\u6563\u5f0f LLM \u7684\u5354\u4f5c\u8a13\u7df4\uff0c\u540c\u6642\u4fdd\u8b77\u4e86\u79c1\u4eba\u8cc7\u6599\u3002\u7136\u800c\uff0cFL \u6846\u67b6\u9762\u81e8\u8457\u986f\u8457\u7684\u983b\u5bec\u548c\u904b\u7b97\u9700\u6c42\uff0c\u4ee5\u53ca\u7570\u8cea\u8cc7\u6599\u5206\u4f48\u5e36\u4f86\u7684\u6311\u6230\u3002LLM \u65b0\u8208\u7684\u8a9e\u5883\u5b78\u7fd2\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u500b\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u5b83\u805a\u5408\u81ea\u7136\u8a9e\u8a00\uff0c\u800c\u4e0d\u662f\u9f90\u5927\u7684\u6a21\u578b\u53c3\u6578\u3002\u7136\u800c\uff0c\u6b64\u65b9\u6cd5\u6709\u96b1\u79c1\u5916\u6d29\u7684\u98a8\u96aa\uff0c\u56e0\u70ba\u5b83\u9700\u8981\u5728\u805a\u5408\u904e\u7a0b\u4e2d\u6536\u96c6\u548c\u5448\u73fe\u4f86\u81ea\u4e0d\u540c\u5ba2\u6236\u7aef\u7684\u8cc7\u6599\u6a23\u672c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u96b1\u79c1\u4fdd\u8b77\u806f\u5408\u8a9e\u5883 LLM \u4ee3\u7406\u5b78\u7fd2 (FICAL) \u6f14\u7b97\u6cd5\uff0c\u6839\u64da\u6211\u5011\u6240\u77e5\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u767c\u63ee\u8a9e\u5883\u5b78\u7fd2\u7684\u529b\u91cf\uff0c\u900f\u904e FL \u8a13\u7df4\u591a\u6a23\u5316\u7684 LLM \u4ee3\u7406\u3002\u5728\u6211\u5011\u7684\u8a2d\u8a08\u4e2d\uff0c\u7531\u65b0\u7a4e\u7684 LLM \u589e\u5f37\u7684\u77e5\u8b58\u5f59\u7de8\u751f\u6210 (KCG) \u6a21\u7d44\u7522\u751f\u7684\u77e5\u8b58\u5f59\u7de8\u5728\u5ba2\u6236\u7aef\u548c\u4f3a\u670d\u5668\u4e4b\u9593\u50b3\u8f38\uff0c\u800c\u4e0d\u662f\u5148\u524d FL \u65b9\u6cd5\u4e2d\u7684\u6a21\u578b\u53c3\u6578\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u57fa\u65bc\u4ee4\u4eba\u96e3\u4ee5\u7f6e\u4fe1\u7684\u6aa2\u7d22\u589e\u5f37\u751f\u6210 (RAG) \u7684\u5de5\u5177\u5b78\u7fd2\u548c\u5229\u7528 (TLU) \u6a21\u7d44\uff0c\u4e26\u5c07\u805a\u5408\u7684\u5168\u7403\u77e5\u8b58\u5f59\u7de8\u4f5c\u70ba\u8001\u5e2b\uff0c\u6559\u5c0e LLM \u4ee3\u7406\u5982\u4f55\u4f7f\u7528\u5de5\u5177\u3002\u6211\u5011\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u7d50\u679c\u986f\u793a FICAL \u8207\u5176\u4ed6 SOTA \u57fa\u6e96\u76f8\u6bd4\u5177\u6709\u7af6\u722d\u529b\u7684\u6548\u80fd\uff0c\u540c\u6642\u5c07\u901a\u8a0a\u6210\u672c\u5927\u5e45\u964d\u4f4e $\\mathbf{3.33\\times10^5}$ \u500d\u3002</paragraph>", "author": "Panlong Wu et.al.", "authors": "Panlong Wu, Kangshuo Li, Junbao Nan, Fangxin Wang", "id": "2412.08054v1", "paper_url": "http://arxiv.org/abs/2412.08054v1", "repo": "null"}}