{"2411.12580": {"publish_time": "2024-11-19", "title": "Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models", "paper_summary": "The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.", "paper_summary_zh": "\u8fd1\u51e0\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u5df2\u88ab\u8be6\u7ec6\u5730\u52fe\u52d2\u51fa\u6765\uff0c\u63d0\u4f9b\u4e86\u4e00\u5e45\u65e2\u5f15\u4eba\u5165\u80dc\u53c8\u76f8\u4e92\u77db\u76fe\u7684\u56fe\u666f\u3002\u4e00\u65b9\u9762\uff0cLLM \u5c55\u793a\u4e86\u89e3\u51b3\u95ee\u9898\u7684\u4e00\u822c\u80fd\u529b\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\uff0c\u5b83\u4eec\u8868\u73b0\u51fa\u4ee4\u4eba\u60ca\u8bb6\u7684\u63a8\u7406\u5dee\u8ddd\uff0c\u8ba9\u4eba\u5bf9\u5176\u6cdb\u5316\u7b56\u7565\u7684\u7a33\u5065\u6027\u4ea7\u751f\u6000\u7591\u3002LLM \u8bbe\u8ba1\u4e2d\u4f7f\u7528\u7684\u5927\u91cf\u6570\u636e\u4f7f\u6211\u4eec\u65e0\u6cd5\u5e94\u7528\u4f20\u7edf\u4e0a\u7528\u4e8e\u8861\u91cf\u6cdb\u5316\u7684\u65b9\u6cd5\uff1a\u8bad\u7ec3-\u6d4b\u8bd5\u96c6\u5206\u79bb\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u901a\u8fc7\u7814\u7a76 LLM \u4f9d\u8d56\u7684\u9884\u8bad\u7ec3\u6570\u636e\u6765\u7814\u7a76 LLM \u5728\u6267\u884c\u63a8\u7406\u4efb\u52a1\u65f6\u91c7\u7528\u54ea\u79cd\u6cdb\u5316\u7b56\u7565\u3002\u5bf9\u4e8e\u4e24\u4e2a\u4e0d\u540c\u5927\u5c0f\uff087B \u548c 35B\uff09\u7684\u6a21\u578b\u53ca\u5176 2.5B \u9884\u8bad\u7ec3\u6807\u8bb0\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u54ea\u4e9b\u6587\u6863\u5f71\u54cd\u4e86\u4e09\u4e2a\u7b80\u5355\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u6a21\u578b\u8f93\u51fa\uff0c\u5e76\u5c06\u6b64\u4e0e\u56de\u7b54\u4e8b\u5b9e\u95ee\u9898\u65f6\u6709\u5f71\u54cd\u529b\u7684\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u867d\u7136\u6a21\u578b\u5bf9\u6bcf\u4e2a\u4e8b\u5b9e\u95ee\u9898\u4f9d\u8d56\u4e8e\u5927\u90e8\u5206\u4e0d\u540c\u7684\u6570\u636e\u96c6\uff0c\u4f46\u4e00\u4e2a\u6587\u6863\u901a\u5e38\u5728\u540c\u4e00\u4efb\u52a1\u4e2d\u7684\u4e0d\u540c\u63a8\u7406\u95ee\u9898\u4e2d\u5177\u6709\u76f8\u4f3c\u7684\u5f71\u54cd\uff0c\u8868\u660e\u5b58\u5728\u7a0b\u5e8f\u77e5\u8bc6\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u53d1\u73b0\uff0c\u4e8b\u5b9e\u95ee\u9898\u7684\u7b54\u6848\u901a\u5e38\u51fa\u73b0\u5728\u6700\u6709\u5f71\u54cd\u529b\u7684\u6570\u636e\u4e2d\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u63a8\u7406\u95ee\u9898\uff0c\u7b54\u6848\u901a\u5e38\u4e0d\u4f1a\u663e\u793a\u4e3a\u9ad8\u5ea6\u6709\u5f71\u54cd\u529b\uff0c\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u7b54\u6848\u4e5f\u4e0d\u4f1a\u663e\u793a\u4e3a\u9ad8\u5ea6\u6709\u5f71\u54cd\u529b\u3002\u5f53\u6211\u4eec\u5bf9\u63a8\u7406\u95ee\u9898\u7684\u6392\u540d\u524d\u5217\u7684\u6587\u6863\u8fdb\u884c\u5b9a\u6027\u8868\u5f81\u65f6\uff0c\u6211\u4eec\u786e\u8ba4\u6709\u5f71\u54cd\u529b\u7684\u6587\u6863\u901a\u5e38\u5305\u542b\u7a0b\u5e8f\u77e5\u8bc6\uff0c\u4f8b\u5982\u6f14\u793a\u5982\u4f55\u4f7f\u7528\u516c\u5f0f\u6216\u4ee3\u7801\u83b7\u5f97\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u4f7f\u7528\u7684\u63a8\u7406\u65b9\u6cd5\u4e0d\u540c\u4e8e\u68c0\u7d22\uff0c\u66f4\u50cf\u662f\u4e00\u79cd\u53ef\u6cdb\u5316\u7684\u7b56\u7565\uff0c\u5b83\u4ece\u6267\u884c\u7c7b\u4f3c\u63a8\u7406\u5f62\u5f0f\u7684\u6587\u6863\u4e2d\u7efc\u5408\u7a0b\u5e8f\u77e5\u8bc6\u3002", "author": "Laura Ruis et.al.", "authors": "Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rockt\u00e4schel, Edward Grefenstette, Max Bartolo", "id": "2411.12580v1", "paper_url": "http://arxiv.org/abs/2411.12580v1", "repo": "null"}}