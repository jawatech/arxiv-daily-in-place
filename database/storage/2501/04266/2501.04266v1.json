{"2501.04266": {"publish_time": "2025-01-08", "title": "Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning", "paper_summary": "Scaling up Large Language Model(LLM) training involves fitting a tremendous\namount of training parameters across a limited number of workers. However,\nmethods like ZeRO-3 that drastically reduce GPU memory pressure often incur\nheavy communication to ensure global synchronization and consistency.\nEstablished efforts such as ZeRO++ use secondary partitions to avoid inter-node\ncommunications, given that intra-node GPU-GPU transfer generally has more\nbandwidth and lower latency than inter-node connections. However, as more\ncapable infrastructure like Frontier, equipped with AMD GPUs, emerged with\nimpressive computing capability, there is a need for investigations on the\nhardware topology and to develop targeted strategies to improve training\nefficiency. In this work, we propose a collection of communication and\noptimization strategies for ZeRO++ to reduce communication costs and improve\nmemory utilization. In this paper, we propose a 3-level hierarchical\npartitioning specifically for the current Top-1 supercomputing cluster,\nFrontier, which aims at leveraging various bandwidths across layers of\ncommunications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication\noverhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU\nwhen compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for\nup to 384 GCDs. To the best of our knowledge, our work is also the first effort\nto efficiently optimize LLM workloads on Frontier AMD GPUs.", "paper_summary_zh": "\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u8bad\u7ec3\u7684\u6269\u5c55\u6d89\u53ca\u5728\u6709\u9650\u6570\u91cf\u7684 worker \u4e2d\u62df\u5408\u5927\u91cf\u7684\u8bad\u7ec3\u53c2\u6570\u3002\u7136\u800c\uff0c\u50cf ZeRO-3 \u8fd9\u6837\u7684\u65b9\u6cd5\u5927\u5e45\u964d\u4f4e\u4e86 GPU \u5185\u5b58\u538b\u529b\uff0c\u901a\u5e38\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u901a\u4fe1\uff0c\u4ee5\u786e\u4fdd\u5168\u5c40\u540c\u6b65\u548c\u4e00\u81f4\u6027\u3002\u5df2\u5efa\u7acb\u7684\u52aa\u529b\uff0c\u4f8b\u5982 ZeRO++\uff0c\u4f7f\u7528\u8f85\u52a9\u5206\u533a\u6765\u907f\u514d\u8282\u70b9\u95f4\u901a\u4fe1\uff0c\u56e0\u4e3a\u8282\u70b9\u5185 GPU-GPU \u4f20\u8f93\u901a\u5e38\u6bd4\u8282\u70b9\u95f4\u8fde\u63a5\u5177\u6709\u66f4\u5927\u7684\u5e26\u5bbd\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002\u7136\u800c\uff0c\u968f\u7740\u914d\u5907 AMD GPU \u7684 Frontier \u7b49\u529f\u80fd\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u8bbe\u65bd\u7684\u51fa\u73b0\uff0c\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u9700\u8981\u5bf9\u786c\u4ef6\u62d3\u6251\u8fdb\u884c\u8c03\u67e5\u5e76\u5236\u5b9a\u6709\u9488\u5bf9\u6027\u7684\u7b56\u7565\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u7ec4\u7528\u4e8e ZeRO++ \u7684\u901a\u4fe1\u548c\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u5e76\u63d0\u9ad8\u5185\u5b58\u5229\u7528\u7387\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u9488\u5bf9\u5f53\u524d\u7684 Top-1 \u8d85\u7ea7\u8ba1\u7b97\u96c6\u7fa4 Frontier \u63d0\u51fa\u4e86\u4e00\u79cd 3 \u7ea7\u5206\u5c42\u5206\u533a\uff0c\u5176\u76ee\u7684\u662f\u5229\u7528\u8de8\u901a\u4fe1\u5c42\uff08GCD-GCD\u3001GPU-GPU \u548c\u8282\u70b9\u95f4\uff09\u7684\u5404\u79cd\u5e26\u5bbd\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002\u5bf9\u4e8e 20B GPT \u6a21\u578b\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u4e0e ZeRO++ \u76f8\u6bd4\uff0c\u6bcf\u4e2a GPU \u7684 TFLOPS \u589e\u52a0\u4e86 1.71 \u500d\uff0c\u6700\u591a\u53ef\u8fbe 384 \u4e2a GCD\uff0c\u5e76\u4e14\u5bf9\u4e8e\u6700\u591a 384 \u4e2a GCD\uff0c\u7f29\u653e\u6548\u7387\u4e3a 0.94\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e5f\u662f\u7b2c\u4e00\u4e2a\u5728 Frontier AMD GPU \u4e0a\u6709\u6548\u4f18\u5316 LLM \u5de5\u4f5c\u8d1f\u8f7d\u7684\u52aa\u529b\u3002", "author": "Lang Xu et.al.", "authors": "Lang Xu, Quentin Anthony, Jacob Hatef, Aamir Shafi, Hari Subramoni, Dhabaleswar K., Panda", "id": "2501.04266v1", "paper_url": "http://arxiv.org/abs/2501.04266v1", "repo": "null"}}