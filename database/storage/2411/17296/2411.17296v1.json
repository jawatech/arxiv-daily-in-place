{"2411.17296": {"publish_time": "2024-11-26", "title": "GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers", "paper_summary": "Graph Transformers (GTs) have demonstrated remarkable performance in\nincorporating various graph structure information, e.g., long-range structural\ndependency, into graph representation learning. However, self-attention -- the\ncore module of GTs -- preserves only low-frequency signals on graph features,\nretaining only homophilic patterns that capture similar features among the\nconnected nodes. Consequently, it has insufficient capacity in modeling complex\nnode label patterns, such as the opposite of homophilic patterns --\nheterophilic patterns. Some improved GTs deal with the problem by learning\npolynomial filters or performing self-attention over the first-order graph\nspectrum. However, these GTs either ignore rich information contained in the\nwhole spectrum or neglect higher-order spectrum information, resulting in\nlimited flexibility and frequency response in their spectral filters. To tackle\nthese challenges, we propose a novel GT network, namely Graph Fourier\nKolmogorov-Arnold Transformers (GrokFormer), to go beyond the self-attention in\nGTs. GrokFormer leverages learnable activation functions in order-$K$ graph\nspectrum through Fourier series modeling to i) learn eigenvalue-targeted filter\nfunctions producing learnable base that can capture a broad range of frequency\nsignals flexibly, and ii) extract first- and higher-order graph spectral\ninformation adaptively. In doing so, GrokFormer can effectively capture\nintricate patterns hidden across different orders and levels of frequency\nsignals, learning expressive, order-and-frequency-adaptive graph\nrepresentations. Comprehensive experiments conducted on 10 node classification\ndatasets across various domains, scales, and levels of graph heterophily, as\nwell as 5 graph classification datasets, demonstrate that GrokFormer\noutperforms state-of-the-art GTs and other advanced graph neural networks.", "paper_summary_zh": "\u5716\u5f62Transformer (GT) \u5728\u6574\u5408\u5404\u7a2e\u5716\u5f62\u7d50\u69cb\u8cc7\u8a0a\u6642\u5c55\u73fe\u51fa\u986f\u8457\u7684\u6548\u80fd\uff0c\u4f8b\u5982\u9577\u7a0b\u7d50\u69cb\u76f8\u4f9d\u6027\uff0c\u8f49\u5316\u70ba\u5716\u5f62\u8868\u5fb5\u5b78\u7fd2\u3002\u7136\u800c\uff0c\u81ea\u6ce8\u610f\u529b -- GT \u7684\u6838\u5fc3\u6a21\u7d44 -- \u50c5\u4fdd\u7559\u5716\u5f62\u7279\u5fb5\u4e2d\u7684\u4f4e\u983b\u8a0a\u865f\uff0c\u50c5\u4fdd\u7559\u6355\u6349\u9023\u63a5\u7bc0\u9ede\u9593\u76f8\u4f3c\u7279\u5fb5\u7684\u540c\u8cea\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u5b83\u5728\u5efa\u6a21\u8907\u96dc\u7684\u7bc0\u9ede\u6a19\u7c64\u6a21\u5f0f\u6642\u80fd\u529b\u4e0d\u8db3\uff0c\u4f8b\u5982\u540c\u8cea\u6a21\u5f0f\u7684\u76f8\u53cd\u6a21\u5f0f -- \u7570\u8cea\u6a21\u5f0f\u3002\u4e00\u4e9b\u6539\u826f\u7684 GT \u8655\u7406\u6b64\u554f\u984c\u7684\u65b9\u6cd5\u662f\u5b78\u7fd2\u591a\u9805\u5f0f\u6ffe\u6ce2\u5668\u6216\u5c0d\u4e00\u968e\u5716\u5f62\u983b\u8b5c\u57f7\u884c\u81ea\u6ce8\u610f\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b GT \u4e0d\u662f\u5ffd\u7565\u6574\u500b\u983b\u8b5c\u4e2d\u5305\u542b\u7684\u8c50\u5bcc\u8cc7\u8a0a\uff0c\u5c31\u662f\u5ffd\u7565\u9ad8\u968e\u983b\u8b5c\u8cc7\u8a0a\uff0c\u5c0e\u81f4\u5176\u983b\u8b5c\u6ffe\u6ce2\u5668\u7684\u9748\u6d3b\u6027\u8207\u983b\u7387\u97ff\u61c9\u53d7\u9650\u3002\u70ba\u4e86\u61c9\u5c0d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684 GT \u7db2\u8def\uff0c\u7a31\u70ba\u5716\u5f62\u5085\u7acb\u8449\u67ef\u723e\u83ab\u54e5\u6d1b\u592b-\u963f\u8afe\u5fb7Transformer (GrokFormer)\uff0c\u4ee5\u8d85\u8d8a GT \u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u3002GrokFormer \u5229\u7528\u53ef\u5b78\u7fd2\u7684\u6d3b\u5316\u51fd\u6578\u5728\u7b2c-$K$ \u968e\u5716\u5f62\u983b\u8b5c\u4e2d\u900f\u904e\u5085\u7acb\u8449\u7d1a\u6578\u5efa\u6a21\uff0c\u4ee5 i) \u5b78\u7fd2\u76ee\u6a19\u7279\u5fb5\u503c\u6ffe\u6ce2\u5668\u51fd\u6578\uff0c\u7522\u751f\u53ef\u5b78\u7fd2\u7684\u57fa\u5e95\uff0c\u80fd\u5920\u9748\u6d3b\u6355\u6349\u5ee3\u6cdb\u7684\u983b\u7387\u8a0a\u865f\uff0c\u4ee5\u53ca ii) \u81ea\u9069\u61c9\u5730\u63d0\u53d6\u4e00\u968e\u548c\u9ad8\u968e\u5716\u5f62\u983b\u8b5c\u8cc7\u8a0a\u3002\u900f\u904e\u9019\u7a2e\u65b9\u5f0f\uff0cGrokFormer \u53ef\u4ee5\u6709\u6548\u6355\u6349\u96b1\u85cf\u5728\u4e0d\u540c\u983b\u7387\u8a0a\u865f\u968e\u6b21\u548c\u5c64\u7d1a\u4e2d\u7684\u8907\u96dc\u6a21\u5f0f\uff0c\u5b78\u7fd2\u8868\u9054\u6027\u3001\u968e\u6b21\u548c\u983b\u7387\u81ea\u9069\u61c9\u7684\u5716\u5f62\u8868\u5fb5\u3002\u5728 10 \u500b\u6a6b\u8de8\u5404\u7a2e\u9818\u57df\u3001\u898f\u6a21\u548c\u5716\u5f62\u7570\u8cea\u6027\u5c64\u7d1a\u7684\u7bc0\u9ede\u5206\u985e\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u7684\u5168\u9762\u5be6\u9a57\uff0c\u4ee5\u53ca 5 \u500b\u5716\u5f62\u5206\u985e\u8cc7\u6599\u96c6\uff0c\u8b49\u660e GrokFormer \u512a\u65bc\u6700\u5148\u9032\u7684 GT \u548c\u5176\u4ed6\u5148\u9032\u7684\u5716\u5f62\u795e\u7d93\u7db2\u8def\u3002", "author": "Guoguo Ai et.al.", "authors": "Guoguo Ai, Guansong Pang, Hezhe Qiao, Yuan Gao, Hui Yan", "id": "2411.17296v1", "paper_url": "http://arxiv.org/abs/2411.17296v1", "repo": "https://github.com/GGA23/GrokFormer"}}