{"2412.20476": {"publish_time": "2024-12-29", "title": "Cut the Deadwood Out: Post-Training Model Purification with Selective Module Substitution", "paper_summary": "The success of DNNs often depends on training with large-scale datasets, but\nbuilding such datasets is both expensive and challenging. Consequently, public\ndatasets from open-source platforms like HuggingFace have become popular,\nposing significant risks of data poisoning attacks. Existing backdoor defenses\nin NLP primarily focus on identifying and removing poisoned samples; however,\npurifying a backdoored model with these sample-cleaning approaches typically\nrequires expensive retraining. Therefore, we propose Greedy Module Substitution\n(GMS), which identifies and substitutes ''deadwood'' modules (i.e., components\ncritical to backdoor pathways) in a backdoored model to purify it. Our method\nrelaxes the common dependency of prior model purification methods on clean\ndatasets or clean auxiliary models. When applied to RoBERTa-large under\nbackdoor attacks, GMS demonstrates strong effectiveness across various\nsettings, particularly against widely recognized challenging attacks like LWS,\nachieving a post-purification attack success rate (ASR) of 9.7% on SST-2\ncompared to 58.8% for the best baseline approach.", "paper_summary_zh": "DNN \u7684\u6210\u529f\u901a\u5e38\u53d6\u51b3\u4e8e\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u6784\u5efa\u6b64\u7c7b\u6570\u636e\u96c6\u65e2\u6602\u8d35\u53c8\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u6765\u81ea HuggingFace \u7b49\u5f00\u6e90\u5e73\u53f0\u7684\u516c\u5171\u6570\u636e\u96c6\u5df2\u53d8\u5f97\u6d41\u884c\uff0c\u5bf9\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u6784\u6210\u4e86\u91cd\u5927\u98ce\u9669\u3002\u73b0\u6709\u7684 NLP \u540e\u95e8\u9632\u5fa1\u4e3b\u8981\u96c6\u4e2d\u4e8e\u8bc6\u522b\u548c\u79fb\u9664\u4e2d\u6bd2\u6837\u672c\uff1b\u7136\u800c\uff0c\u4f7f\u7528\u8fd9\u4e9b\u6837\u672c\u6e05\u7406\u65b9\u6cd5\u51c0\u5316\u540e\u95e8\u6a21\u578b\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8d2a\u5a6a\u6a21\u5757\u66ff\u6362 (GMS)\uff0c\u5b83\u8bc6\u522b\u5e76\u66ff\u6362\u540e\u95e8\u6a21\u578b\u4e2d\u7684\u201c\u5e9f\u5f03\u201d\u6a21\u5757\uff08\u5373\u5bf9\u540e\u95e8\u8def\u5f84\u81f3\u5173\u91cd\u8981\u7684\u7ec4\u4ef6\uff09\u4ee5\u5bf9\u5176\u8fdb\u884c\u51c0\u5316\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6d88\u9664\u4e86\u5148\u524d\u6a21\u578b\u51c0\u5316\u65b9\u6cd5\u5bf9\u5e72\u51c0\u6570\u636e\u96c6\u6216\u5e72\u51c0\u8f85\u52a9\u6a21\u578b\u7684\u5e38\u89c1\u4f9d\u8d56\u6027\u3002\u5f53\u5e94\u7528\u4e8e RoBERTa-large \u53d7\u5230\u540e\u95e8\u653b\u51fb\u65f6\uff0cGMS \u5728\u5404\u79cd\u8bbe\u7f6e\u4e2d\u5c55\u793a\u4e86\u5f88\u5f3a\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9 LWS \u7b49\u516c\u8ba4\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u653b\u51fb\uff0c\u5728 SST-2 \u4e0a\u5b9e\u73b0\u4e86 9.7% \u7684\u51c0\u5316\u540e\u653b\u51fb\u6210\u529f\u7387 (ASR)\uff0c\u800c\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u4e3a 58.8%\u3002", "author": "Yao Tong et.al.", "authors": "Yao Tong, Weijun Li, Xuanli He, Haolan Zhan, Qiongkai Xu", "id": "2412.20476v1", "paper_url": "http://arxiv.org/abs/2412.20476v1", "repo": "null"}}