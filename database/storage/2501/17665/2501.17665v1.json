{"2501.17665": {"publish_time": "2025-01-29", "title": "Planning with Vision-Language Models and a Use Case in Robot-Assisted Teaching", "paper_summary": "Automating the generation of Planning Domain Definition Language (PDDL) with\nLarge Language Model (LLM) opens new research topic in AI planning,\nparticularly for complex real-world tasks. This paper introduces Image2PDDL, a\nnovel framework that leverages Vision-Language Models (VLMs) to automatically\nconvert images of initial states and descriptions of goal states into PDDL\nproblems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL\naddresses key challenges in bridging perceptual understanding with symbolic\nplanning, reducing the expertise required to create structured problem\ninstances, and improving scalability across tasks of varying complexity. We\nevaluate the framework on various domains, including standard planning domains\nlike blocksworld and sliding tile puzzles, using datasets with multiple\ndifficulty levels. Performance is assessed on syntax correctness, ensuring\ngrammar and executability, and content correctness, verifying accurate state\nrepresentation in generated PDDL problems. The proposed approach demonstrates\npromising results across diverse task complexities, suggesting its potential\nfor broader applications in AI planning. We will discuss a potential use case\nin robot-assisted teaching of students with Autism Spectrum Disorder.", "paper_summary_zh": "\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u81ea\u52d5\u5316\u898f\u5283\u9818\u57df\u5b9a\u7fa9\u8a9e\u8a00 (PDDL) \u7684\u751f\u6210\uff0c\u70ba AI \u898f\u5283\u958b\u555f\u4e86\u65b0\u7684\u7814\u7a76\u4e3b\u984c\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u8907\u96dc\u7684\u771f\u5be6\u4e16\u754c\u4efb\u52d9\u3002\u672c\u6587\u4ecb\u7d39 Image2PDDL\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u5c07\u521d\u59cb\u72c0\u614b\u7684\u5f71\u50cf\u548c\u76ee\u6a19\u72c0\u614b\u7684\u63cf\u8ff0\u81ea\u52d5\u8f49\u63db\u70ba PDDL \u554f\u984c\u3002\u900f\u904e\u63d0\u4f9b PDDL \u9818\u57df\u548c\u8996\u89ba\u8f38\u5165\uff0cImasge2PDDL \u61c9\u5c0d\u4e86\u5c07\u611f\u77e5\u7406\u89e3\u8207\u7b26\u865f\u898f\u5283\u7d50\u5408\u8d77\u4f86\u7684\u4e3b\u8981\u6311\u6230\uff0c\u6e1b\u5c11\u4e86\u5efa\u7acb\u7d50\u69cb\u5316\u554f\u984c\u5be6\u4f8b\u6240\u9700\u7684\u5c08\u696d\u77e5\u8b58\uff0c\u4e26\u63d0\u9ad8\u4e86\u8de8\u8d8a\u4e0d\u540c\u8907\u96dc\u6027\u4efb\u52d9\u7684\u53ef\u64f4\u5145\u6027\u3002\u6211\u5011\u5728\u5404\u7a2e\u9818\u57df\u5c0d\u8a72\u6846\u67b6\u9032\u884c\u8a55\u4f30\uff0c\u5305\u62ec\u6a19\u6e96\u898f\u5283\u9818\u57df\uff0c\u4f8b\u5982\u7a4d\u6728\u4e16\u754c\u548c\u6ed1\u52d5\u62fc\u5716\uff0c\u4f7f\u7528\u5177\u6709\u591a\u500b\u96e3\u5ea6\u7b49\u7d1a\u7684\u8cc7\u6599\u96c6\u3002\u6548\u80fd\u8a55\u4f30\u5305\u62ec\u8a9e\u6cd5\u6b63\u78ba\u6027\uff0c\u78ba\u4fdd\u8a9e\u6cd5\u548c\u53ef\u57f7\u884c\u6027\uff0c\u4ee5\u53ca\u5167\u5bb9\u6b63\u78ba\u6027\uff0c\u9a57\u8b49\u751f\u6210\u7684 PDDL \u554f\u984c\u4e2d\u7684\u6e96\u78ba\u72c0\u614b\u8868\u793a\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u4efb\u52d9\u8907\u96dc\u6027\u4e2d\u5c55\u793a\u4e86\u6709\u5e0c\u671b\u7684\u7d50\u679c\uff0c\u8868\u660e\u5176\u5728 AI \u898f\u5283\u4e2d\u5177\u6709\u66f4\u5ee3\u6cdb\u7684\u61c9\u7528\u6f5b\u529b\u3002\u6211\u5011\u5c07\u8a0e\u8ad6\u81ea\u9589\u75c7\u8b5c\u7cfb\u969c\u7919\u5b78\u751f\u7684\u6a5f\u5668\u4eba\u8f14\u52a9\u6559\u5b78\u4e2d\u7684\u6f5b\u5728\u7528\u4f8b\u3002", "author": "Xuzhe Dang et.al.", "authors": "Xuzhe Dang, Lada Kudl\u00e1\u010dkov\u00e1, Stefan Edelkamp", "id": "2501.17665v1", "paper_url": "http://arxiv.org/abs/2501.17665v1", "repo": "null"}}