{"2411.17458": {"publish_time": "2024-11-26", "title": "Spatially Visual Perception for End-to-End Robotic Learning", "paper_summary": "Recent advances in imitation learning have shown significant promise for\nrobotic control and embodied intelligence. However, achieving robust\ngeneralization across diverse mounted camera observations remains a critical\nchallenge. In this paper, we introduce a video-based spatial perception\nframework that leverages 3D spatial representations to address environmental\nvariability, with a focus on handling lighting changes. Our approach integrates\na novel image augmentation technique, AugBlender, with a state-of-the-art\nmonocular depth estimation model trained on internet-scale data. Together,\nthese components form a cohesive system designed to enhance robustness and\nadaptability in dynamic scenarios. Our results demonstrate that our approach\nsignificantly boosts the success rate across diverse camera exposures, where\nprevious models experience performance collapse. Our findings highlight the\npotential of video-based spatial perception models in advancing robustness for\nend-to-end robotic learning, paving the way for scalable, low-cost solutions in\nembodied intelligence.", "paper_summary_zh": "\u6700\u8fd1\u5728\u6a21\u4eff\u5b78\u7fd2\u4e0a\u7684\u9032\u5c55\u986f\u793a\u51fa\u5c0d\u6a5f\u5668\u4eba\u63a7\u5236\u548c\u5177\u8eab\u667a\u80fd\u7684\u91cd\u5927\u524d\u666f\u3002\u7136\u800c\uff0c\u5728\u4e0d\u540c\u7684\u5b89\u88dd\u76f8\u6a5f\u89c0\u6e2c\u4e2d\u5be6\u73fe\u7a69\u5065\u7684\u6cdb\u5316\u4ecd\u7136\u662f\u4e00\u500b\u95dc\u9375\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u57fa\u65bc\u8996\u983b\u7684\u7a7a\u9593\u611f\u77e5\u6846\u67b6\uff0c\u5229\u7528 3D \u7a7a\u9593\u8868\u793a\u4f86\u89e3\u6c7a\u74b0\u5883\u8b8a\u7570\u6027\uff0c\u91cd\u9ede\u662f\u8655\u7406\u5149\u7dda\u8b8a\u5316\u3002\u6211\u5011\u7684\u505a\u6cd5\u7d50\u5408\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u5716\u50cf\u64f4\u5145\u6280\u8853 AugBlender\uff0c\u4ee5\u53ca\u4e00\u500b\u5728\u7db2\u969b\u7db2\u8def\u898f\u6a21\u6578\u64da\u4e0a\u8a13\u7df4\u7684\u6700\u5148\u9032\u7684\u55ae\u773c\u6df1\u5ea6\u4f30\u8a08\u6a21\u578b\u3002\u9019\u4e9b\u7d44\u6210\u90e8\u5206\u5171\u540c\u5f62\u6210\u4e86\u4e00\u500b\u5167\u805a\u7684\u7cfb\u7d71\uff0c\u65e8\u5728\u589e\u5f37\u52d5\u614b\u5834\u666f\u4e2d\u7684\u7a69\u5065\u6027\u548c\u9069\u61c9\u6027\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u986f\u8457\u63d0\u9ad8\u4e86\u4e0d\u540c\u76f8\u6a5f\u66dd\u5149\u4e0b\u7684\u6210\u529f\u7387\uff0c\u800c\u5148\u524d\u7684\u6a21\u578b\u6703\u51fa\u73fe\u6548\u80fd\u5d29\u6f70\u3002\u6211\u5011\u7684\u767c\u73fe\u7a81\u986f\u4e86\u57fa\u65bc\u5f71\u7247\u7684\u7a7a\u9593\u611f\u77e5\u6a21\u578b\u5728\u63a8\u9032\u7aef\u5230\u7aef\u6a5f\u5668\u4eba\u5b78\u7fd2\u7684\u7a69\u5065\u6027\u4e2d\u7684\u6f5b\u529b\uff0c\u70ba\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u53ef\u64f4\u5145\u3001\u4f4e\u6210\u672c\u89e3\u6c7a\u65b9\u6848\u92ea\u5e73\u4e86\u9053\u8def\u3002", "author": "Travis Davies et.al.", "authors": "Travis Davies, Jiahuan Yan, Xiang Chen, Yu Tian, Yueting Zhuang, Yiqi Huang, Luhui Hu", "id": "2411.17458v1", "paper_url": "http://arxiv.org/abs/2411.17458v1", "repo": "null"}}