{"2411.02059": {"publish_time": "2024-11-04", "title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration", "paper_summary": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.", "paper_summary_zh": "<paragraph>GPT\u3001Claude\u3001LLaMA \u548c Qwen \u7b49\u6a21\u578b\u7684\u51fa\u73b0\u91cd\u5851\u4e86 AI\n\u5e94\u7528\u7a0b\u5e8f\uff0c\u4e3a\u5404\u884c\u5404\u4e1a\u5e26\u6765\u4e86\u5e7f\u9614\u7684\u65b0\u673a\u9047\u3002\u7136\u800c\uff0c\n\u5c3d\u7ba1\u8868\u683c\u6570\u636e\u5728\u4f17\u591a\u73b0\u5b9e\u4e16\u754c\u9886\u57df\u4e2d\u626e\u6f14\u7740\u57fa\u7840\u6027\u89d2\u8272\uff0c\u4f46\n\u5176\u96c6\u6210\u5374\u4ecd\u7136\u660e\u663e\u4e0d\u6210\u719f\u3002\n\u8fd9\u79cd\u5dee\u8ddd\u81f3\u5173\u91cd\u8981\uff0c\u4e3b\u8981\u6709\u4e09\u4e2a\u539f\u56e0\u3002\u9996\u5148\uff0c\u6570\u636e\u5e93\u6216\u6570\u636e\n\u4ed3\u5e93\u6570\u636e\u96c6\u6210\u5bf9\u4e8e\u9ad8\u7ea7\u5e94\u7528\u7a0b\u5e8f\u81f3\u5173\u91cd\u8981\uff1b\u5176\u6b21\uff0c\n\u5927\u91cf\u4e14\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5c1a\u672a\u5f00\u53d1\u7684\u8868\u683c\u6570\u636e\u8d44\u6e90\u4e3a\n\u5206\u6790\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u6f5c\u529b\uff1b\u7b2c\u4e09\uff0c\u5546\u4e1a\u667a\u80fd\u9886\u57df\u5c24\u5176\u9700\u8981\n\u8bb8\u591a\u5f53\u524d LLM \u53ef\u80fd\u96be\u4ee5\u63d0\u4f9b\u7684\u9002\u5e94\u6027\u5f3a\u3001\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002\n\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 TableGPT2\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ecf\u8fc7\u4e25\u683c\u9884\u8bad\u7ec3\u548c\n\u5fae\u8c03\u7684\u6a21\u578b\uff0c\u62e5\u6709\u8d85\u8fc7 593.8K \u5f20\u8868\u683c\u548c 2.36M \u4e2a\u9ad8\u8d28\u91cf\u7684\u67e5\u8be2-\u8868\u683c-\u8f93\u51fa\n\u5143\u7ec4\uff0c\u8fd9\u662f\u5148\u524d\u7814\u7a76\u4e2d\u524d\u6240\u672a\u6709\u7684\u8868\u683c\u76f8\u5173\u6570\u636e\u89c4\u6a21\u3002\u8fd9\u79cd\n\u5e7f\u6cdb\u7684\u8bad\u7ec3\u4f7f TableGPT2 \u80fd\u591f\u5728\u4ee5\u8868\u683c\u4e3a\u4e2d\u5fc3\u7684\n\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u901a\u7528\u8bed\u8a00\u548c\u7f16\u7801\u80fd\u529b\u3002\nTableGPT2 \u7684\u4e00\u9879\u5173\u952e\u521b\u65b0\u662f\u5176\u65b0\u9896\u7684\u8868\u683c\u7f16\u7801\u5668\uff0c\u4e13\u95e8\n\u8bbe\u8ba1\u7528\u4e8e\u6355\u83b7\u6a21\u5f0f\u7ea7\u522b\u548c\u5355\u5143\u683c\u7ea7\u522b\u4fe1\u606f\u3002\u6b64\u7f16\u7801\u5668\n\u589e\u5f3a\u4e86\u6a21\u578b\u5904\u7406\u6a21\u68f1\u4e24\u53ef\u7684\u67e5\u8be2\u3001\u7f3a\u5c11\u5217\u540d\u548c\n\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7a0b\u5e8f\u4e2d\u5e38\u89c1\u7684\u975e\u89c4\u5219\u8868\u683c\u7684\u80fd\u529b\u3002\n\u7c7b\u4f3c\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u79cd\u5f00\u521b\u6027\u65b9\u6cd5\u4e0e\n\u89e3\u7801\u5668\u96c6\u6210\uff0c\u5f62\u6210\u4e00\u4e2a\u5f3a\u5927\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u3002\n\u6211\u4eec\u76f8\u4fe1\u7ed3\u679c\u4ee4\u4eba\u4fe1\u670d\uff1a\u5728 23 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6307\u6807\u4e0a\uff0c\nTableGPT2 \u5728 7B \u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86 35.20% \u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\uff0c\u5728 72B \u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86 49.32% \u7684\u63d0\u5347\uff0c\u5e76\u4e14\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\n\u901a\u7528\u529f\u80fd\u3002</paragraph>", "author": "Aofeng Su et.al.", "authors": "Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, Haoze Li, Haoxuan Lan, Jiaming Tian, Jing Yuan, Junbo Zhao, Junlin Zhou, Kaizhe Shou, Liangyu Zha, Lin Long, Liyao Li, Pengzuo Wu, Qi Zhang, Qingyi Huang, Saisai Yang, Tao Zhang, Wentao Ye, Wufang Zhu, Xiaomeng Hu, Xijun Gu, Xinjie Sun, Xiang Li, Yuhang Yang, Zhiqing Xiao", "id": "2411.02059v1", "paper_url": "http://arxiv.org/abs/2411.02059v1", "repo": "null"}}