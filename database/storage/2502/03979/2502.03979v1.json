{"2502.03979": {"publish_time": "2025-02-06", "title": "Towards Unified Music Emotion Recognition across Dimensional and Categorical Models", "paper_summary": "One of the most significant challenges in Music Emotion Recognition (MER)\ncomes from the fact that emotion labels can be heterogeneous across datasets\nwith regard to the emotion representation, including categorical (e.g., happy,\nsad) versus dimensional labels (e.g., valence-arousal). In this paper, we\npresent a unified multitask learning framework that combines these two types of\nlabels and is thus able to be trained on multiple datasets. This framework uses\nan effective input representation that combines musical features (i.e., key and\nchords) and MERT embeddings. Moreover, knowledge distillation is employed to\ntransfer the knowledge of teacher models trained on individual datasets to a\nstudent model, enhancing its ability to generalize across multiple tasks. To\nvalidate our proposed framework, we conducted extensive experiments on a\nvariety of datasets, including MTG-Jamendo, DEAM, PMEmo, and EmoMusic.\nAccording to our experimental results, the inclusion of musical features,\nmultitask learning, and knowledge distillation significantly enhances\nperformance. In particular, our model outperforms the state-of-the-art models,\nincluding the best-performing model from the MediaEval 2021 competition on the\nMTG-Jamendo dataset. Our work makes a significant contribution to MER by\nallowing the combination of categorical and dimensional emotion labels in one\nunified framework, thus enabling training across datasets.", "paper_summary_zh": "\u97f3\u6a02\u60c5\u7dd2\u8fa8\u8b58 (MER) \u4e2d\u6700\u986f\u8457\u7684\u6311\u6230\u4e4b\u4e00\u662f\uff0c\u60c5\u7dd2\u6a19\u7c64\u5728\u8cc7\u6599\u96c6\u4e4b\u9593\u53ef\u80fd\u4e0d\u76e1\u76f8\u540c\uff0c\u9019\u53d6\u6c7a\u65bc\u60c5\u7dd2\u8868\u5fb5\uff0c\u5305\u62ec\u5206\u985e\u6a19\u7c64\uff08\u4f8b\u5982\uff1a\u5feb\u6a02\u3001\u60b2\u50b7\uff09\u8207\u7dad\u5ea6\u6a19\u7c64\uff08\u4f8b\u5982\uff1a\u6548\u50f9-\u559a\u9192\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7d71\u4e00\u7684\u591a\u4efb\u52d9\u5b78\u7fd2\u67b6\u69cb\uff0c\u7d50\u5408\u9019\u5169\u7a2e\u6a19\u7c64\u985e\u578b\uff0c\u56e0\u6b64\u80fd\u5920\u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a13\u7df4\u3002\u6b64\u67b6\u69cb\u4f7f\u7528\u4e00\u7a2e\u6709\u6548\u7684\u8f38\u5165\u8868\u5fb5\uff0c\u7d50\u5408\u97f3\u6a02\u7279\u5fb5\uff08\u5373\uff0c\u97f3\u8abf\u548c\u5f26\uff09\u548c MERT \u5d4c\u5165\u3002\u6b64\u5916\uff0c\u77e5\u8b58\u84b8\u993e\u7528\u65bc\u5c07\u5728\u500b\u5225\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4\u7684\u6559\u5e2b\u6a21\u578b\u7684\u77e5\u8b58\u50b3\u8f38\u7d66\u5b78\u751f\u6a21\u578b\uff0c\u589e\u5f37\u5176\u8de8\u591a\u500b\u4efb\u52d9\u9032\u884c\u6982\u62ec\u7684\u80fd\u529b\u3002\u70ba\u4e86\u9a57\u8b49\u6211\u5011\u63d0\u51fa\u7684\u67b6\u69cb\uff0c\u6211\u5011\u5c0d\u5404\u7a2e\u8cc7\u6599\u96c6\u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u5305\u62ec MTG-Jamendo\u3001DEAM\u3001PMEmo \u548c EmoMusic\u3002\u6839\u64da\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\uff0c\u97f3\u6a02\u7279\u5fb5\u3001\u591a\u4efb\u52d9\u5b78\u7fd2\u548c\u77e5\u8b58\u84b8\u993e\u7684\u52a0\u5165\u986f\u8457\u63d0\u5347\u4e86\u6548\u80fd\u3002\u7279\u5225\u662f\uff0c\u6211\u5011\u7684\u6a21\u578b\u512a\u65bc\u6700\u5148\u9032\u7684\u6a21\u578b\uff0c\u5305\u62ec MediaEval 2021 \u7af6\u8cfd\u4e2d\u5728 MTG-Jamendo \u8cc7\u6599\u96c6\u4e0a\u8868\u73fe\u6700\u4f73\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u7814\u7a76\u900f\u904e\u5141\u8a31\u5728\u4e00\u500b\u7d71\u4e00\u7684\u67b6\u69cb\u4e2d\u7d50\u5408\u5206\u985e\u548c\u7dad\u5ea6\u60c5\u7dd2\u6a19\u7c64\uff0c\u5c0d MER \u505a\u51fa\u91cd\u5927\u8ca2\u737b\uff0c\u5f9e\u800c\u80fd\u5920\u8de8\u8cc7\u6599\u96c6\u9032\u884c\u8a13\u7df4\u3002", "author": "Jaeyong Kang et.al.", "authors": "Jaeyong Kang, Dorien Herremans", "id": "2502.03979v1", "paper_url": "http://arxiv.org/abs/2502.03979v1", "repo": "null"}}