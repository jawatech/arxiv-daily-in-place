{"2411.10397": {"publish_time": "2024-11-15", "title": "Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning", "paper_summary": "Sparse Autoencoders (SAEs) are a promising approach for extracting neural\nnetwork representations by learning a sparse and overcomplete decomposition of\nthe network's internal activations. However, SAEs are traditionally trained\nconsidering only activation values and not the effect those activations have on\ndownstream computations. This limits the information available to learn\nfeatures, and biases the autoencoder towards neglecting features which are\nrepresented with small activation values but strongly influence model outputs.\nTo address this, we introduce Gradient SAEs (g-SAEs), which modify the\n$k$-sparse autoencoder architecture by augmenting the TopK activation function\nto rely on the gradients of the input activation when selecting the $k$\nelements. For a given sparsity level, g-SAEs produce reconstructions that are\nmore faithful to original network performance when propagated through the\nnetwork. Additionally, we find evidence that g-SAEs learn latents that are on\naverage more effective at steering models in arbitrary contexts. By considering\nthe downstream effects of activations, our approach leverages the dual nature\nof neural network features as both $\\textit{representations}$, retrospectively,\nand $\\textit{actions}$, prospectively. While previous methods have approached\nthe problem of feature discovery primarily focused on the former aspect, g-SAEs\nrepresent a step towards accounting for the latter as well.", "paper_summary_zh": "\u7a00\u758f\u81ea\u7de8\u78bc\u5668 (SAE) \u662f\u4e00\u7a2e\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u85c9\u7531\u5b78\u7fd2\u7db2\u8def\u5167\u90e8\u6d3b\u5316\u7684\u7a00\u758f\u4e14\u904e\u5ea6\u5b8c\u5099\u5206\u89e3\uff0c\u4f86\u8403\u53d6\u795e\u7d93\u7db2\u8def\u8868\u793a\u3002\u7136\u800c\uff0cSAE \u50b3\u7d71\u4e0a\u53ea\u8003\u616e\u6d3b\u5316\u503c\u9032\u884c\u8a13\u7df4\uff0c\u800c\u672a\u8003\u616e\u9019\u4e9b\u6d3b\u5316\u5c0d\u4e0b\u6e38\u904b\u7b97\u7684\u5f71\u97ff\u3002\u9019\u6703\u9650\u5236\u53ef\u7528\u65bc\u5b78\u7fd2\u7279\u5fb5\u7684\u8cc7\u8a0a\uff0c\u4e26\u4f7f\u81ea\u52d5\u7de8\u78bc\u5668\u504f\u5411\u5ffd\u7565\u4ee5\u5c0f\u6d3b\u5316\u503c\u8868\u793a\u4f46\u6703\u5f37\u70c8\u5f71\u97ff\u6a21\u578b\u8f38\u51fa\u7684\u7279\u5fb5\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u68af\u5ea6 SAE (g-SAE)\uff0c\u5b83\u900f\u904e\u64f4\u5145 TopK \u6d3b\u5316\u51fd\u6578\u4f86\u4fee\u6539 $k$-\u7a00\u758f\u81ea\u52d5\u7de8\u78bc\u5668\u67b6\u69cb\uff0c\u5728\u9078\u64c7 $k$ \u500b\u5143\u7d20\u6642\u4f9d\u8cf4\u8f38\u5165\u6d3b\u5316\u7684\u68af\u5ea6\u3002\u5c0d\u65bc\u7d66\u5b9a\u7684\u7a00\u758f\u5ea6\u7b49\u7d1a\uff0cg-SAE \u6703\u7522\u751f\u5728\u900f\u904e\u7db2\u8def\u50b3\u64ad\u6642\u66f4\u5fe0\u65bc\u539f\u59cb\u7db2\u8def\u6548\u80fd\u7684\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u8b49\u64da\u986f\u793a\uff0cg-SAE \u6703\u5b78\u7fd2\u5230\u5e73\u5747\u800c\u8a00\u66f4\u6709\u6548\u65bc\u5728\u4efb\u610f\u60c5\u5883\u4e2d\u5f15\u5c0e\u6a21\u578b\u7684\u6f5b\u5728\u8b8a\u6578\u3002\u900f\u904e\u8003\u616e\u6d3b\u5316\u7684\u4e0b\u6e38\u6548\u61c9\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u4e86\u795e\u7d93\u7db2\u8def\u7279\u5fb5\u4f5c\u70ba $\\textit{\u8868\u793a}$\uff08\u56de\u9867\u6027\uff09\u548c $\\textit{\u52d5\u4f5c}$\uff08\u524d\u77bb\u6027\uff09\u7684\u96d9\u91cd\u6027\u8cea\u3002\u96d6\u7136\u5148\u524d\u7684\u505a\u6cd5\u4e3b\u8981\u5c08\u6ce8\u65bc\u7279\u5fb5\u767c\u73fe\u554f\u984c\u7684\u524d\u8005\u9762\u5411\uff0c\u4f46 g-SAE \u4ee3\u8868\u4e86\u671d\u5f8c\u8005\u9081\u9032\u4e00\u6b65\u7684\u4f5c\u6cd5\u3002", "author": "Jeffrey Olmo et.al.", "authors": "Jeffrey Olmo, Jared Wilson, Max Forsey, Bryce Hepner, Thomas Vin Howe, David Wingate", "id": "2411.10397v1", "paper_url": "http://arxiv.org/abs/2411.10397v1", "repo": "null"}}