{"2412.15544": {"publish_time": "2024-12-20", "title": "VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving", "paper_summary": "In recent years, reinforcement learning (RL)-based methods for learning\ndriving policies have gained increasing attention in the autonomous driving\ncommunity and have achieved remarkable progress in various driving scenarios.\nHowever, traditional RL approaches rely on manually engineered rewards, which\nrequire extensive human effort and often lack generalizability. To address\nthese limitations, we propose \\textbf{VLM-RL}, a unified framework that\nintegrates pre-trained Vision-Language Models (VLMs) with RL to generate reward\nsignals using image observation and natural language goals. The core of VLM-RL\nis the contrasting language goal (CLG)-as-reward paradigm, which uses positive\nand negative language goals to generate semantic rewards. We further introduce\na hierarchical reward synthesis approach that combines CLG-based semantic\nrewards with vehicle state information, improving reward stability and offering\na more comprehensive reward signal. Additionally, a batch-processing technique\nis employed to optimize computational efficiency during training. Extensive\nexperiments in the CARLA simulator demonstrate that VLM-RL outperforms\nstate-of-the-art baselines, achieving a 10.5\\% reduction in collision rate, a\n104.6\\% increase in route completion rate, and robust generalization to unseen\ndriving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any\nstandard RL algorithms, potentially revolutionizing the existing RL paradigm\nthat relies on manual reward engineering and enabling continuous performance\nimprovements. The demo video and code can be accessed at:\nhttps://zilin-huang.github.io/VLM-RL-website.", "paper_summary_zh": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60 (RL) \u7684\u5b66\u4e60\u9a7e\u9a76\u7b56\u7565\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u793e\u533a\u4e2d\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u5e76\u5728\u5404\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684 RL \u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\uff0c\u8fd9\u9700\u8981\u5927\u91cf\u7684\u4eba\u529b\uff0c\u5e76\u4e14\u901a\u5e38\u7f3a\u4e4f\u6cdb\u5316\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 \\textbf{VLM-RL}\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5b83\u5c06\u9884\u5148\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u4e0e RL \u96c6\u6210\u5728\u4e00\u8d77\uff0c\u4ee5\u4f7f\u7528\u56fe\u50cf\u89c2\u5bdf\u548c\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u751f\u6210\u5956\u52b1\u4fe1\u53f7\u3002VLM-RL \u7684\u6838\u5fc3\u662f\u5bf9\u6bd4\u8bed\u8a00\u76ee\u6807 (CLG) \u4f5c\u4e3a\u5956\u52b1\u7684\u8303\u4f8b\uff0c\u5b83\u4f7f\u7528\u79ef\u6781\u548c\u6d88\u6781\u7684\u8bed\u8a00\u76ee\u6807\u6765\u751f\u6210\u8bed\u4e49\u5956\u52b1\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u5c42\u5956\u52b1\u5408\u6210\u65b9\u6cd5\uff0c\u5b83\u5c06\u57fa\u4e8e CLG \u7684\u8bed\u4e49\u5956\u52b1\u4e0e\u8f66\u8f86\u72b6\u6001\u4fe1\u606f\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5956\u52b1\u7a33\u5b9a\u6027\u5e76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5956\u52b1\u4fe1\u53f7\u3002\u6b64\u5916\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u91c7\u7528\u6279\u5904\u7406\u6280\u672f\u6765\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002\u5728 CARLA \u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVLM-RL \u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u51c6\uff0c\u78b0\u649e\u7387\u964d\u4f4e\u4e86 10.5%\uff0c\u8def\u7ebf\u5b8c\u6210\u7387\u63d0\u9ad8\u4e86 104.6%\uff0c\u5e76\u4e14\u5bf9\u770b\u4e0d\u89c1\u7684\u9a7e\u9a76\u573a\u666f\u5177\u6709\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0cVLM-RL \u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u51e0\u4e4e\u4efb\u4f55\u6807\u51c6 RL \u7b97\u6cd5\uff0c\u6709\u53ef\u80fd\u5f7b\u5e95\u6539\u53d8\u4f9d\u8d56\u4e8e\u624b\u52a8\u5956\u52b1\u5de5\u7a0b\u7684\u73b0\u6709 RL \u8303\u4f8b\uff0c\u5e76\u5b9e\u73b0\u6301\u7eed\u7684\u6027\u80fd\u6539\u8fdb\u3002\u53ef\u4ee5\u5728\u4ee5\u4e0b\u7f51\u5740\u8bbf\u95ee\u6f14\u793a\u89c6\u9891\u548c\u4ee3\u7801\uff1ahttps://zilin-huang.github.io/VLM-RL-website\u3002", "author": "Zilin Huang et.al.", "authors": "Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, Sikai Chen", "id": "2412.15544v1", "paper_url": "http://arxiv.org/abs/2412.15544v1", "repo": "null"}}