{"2502.15592": {"publish_time": "2025-02-21", "title": "Generalizing From Short to Long: Effective Data Synthesis for Long-Context Instruction Tuning", "paper_summary": "Long-context modelling for large language models (LLMs) has been a key area\nof recent research because many real world use cases require reasoning over\nlonger inputs such as documents. The focus of research into modelling long\ncontext has been on how to model position and there has been little\ninvestigation into other important aspects of language modelling such as\ninstruction tuning. Long context training examples are challenging and\nexpensive to create and use. In this paper, we investigate how to design\ninstruction data for the post-training phase of a long context pre-trained\nmodel: how much and what type of context is needed for optimal and efficient\npost-training. Our controlled study reveals that models instruction-tuned on\nshort contexts can effectively generalize to longer ones, while also\nidentifying other critical factors such as instruction difficulty and context\ncomposition. Based on these findings, we propose context synthesis, a novel\ndata synthesis framework that leverages off-the-shelf LLMs to generate extended\nbackground contexts for high-quality instruction-answer pairs. Experiment\nresults on the document-level benchmark (LongBench) demonstrate that our\nproposed approach outperforms previous instruction synthesis approaches and\ncomes close to the performance of human-annotated long-context instruction\ndata. The project will be available at:\nhttps://github.com/NJUNLP/context-synthesis.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9577\u8108\u7d61\u5efa\u6a21\u4e00\u76f4\u662f\u8fd1\u671f\u7814\u7a76\u7684\u91cd\u9ede\u9818\u57df\uff0c\u56e0\u70ba\u8a31\u591a\u5be6\u969b\u7684\u61c9\u7528\u6848\u4f8b\u9700\u8981\u5c0d\u8f03\u9577\u7684\u8f38\u5165\uff08\u4f8b\u5982\u6587\u4ef6\uff09\u9032\u884c\u63a8\u7406\u3002\u5c0d\u9577\u8108\u7d61\u5efa\u6a21\u7684\u7814\u7a76\u91cd\u9ede\u5728\u65bc\u5982\u4f55\u5efa\u6a21\u4f4d\u7f6e\uff0c\u800c\u5c0d\u8a9e\u8a00\u5efa\u6a21\u7684\u5176\u4ed6\u91cd\u8981\u9762\u5411\uff08\u4f8b\u5982\u6307\u4ee4\u5fae\u8abf\uff09\u5247\u9bae\u5c11\u63a2\u8a0e\u3002\u9577\u8108\u7d61\u8a13\u7df4\u7bc4\u4f8b\u7684\u5efa\u7acb\u548c\u4f7f\u7528\u5177\u6709\u6311\u6230\u6027\u4e14\u6210\u672c\u9ad8\u6602\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u5982\u4f55\u70ba\u9577\u8108\u7d61\u9810\u8a13\u7df4\u6a21\u578b\u7684\u5f8c\u8a13\u7df4\u968e\u6bb5\u8a2d\u8a08\u6307\u4ee4\u8cc7\u6599\uff1a\u9700\u8981\u591a\u5c11\u4ee5\u53ca\u54ea\u7a2e\u985e\u578b\u7684\u8108\u7d61\u624d\u80fd\u5be6\u73fe\u6700\u4f73\u4e14\u9ad8\u6548\u7684\u5f8c\u8a13\u7df4\u3002\u6211\u5011\u7684\u53d7\u63a7\u7814\u7a76\u986f\u793a\uff0c\u5728\u77ed\u8108\u7d61\u4e0a\u9032\u884c\u6307\u4ee4\u5fae\u8abf\u7684\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u63a8\u5ee3\u5230\u8f03\u9577\u7684\u8108\u7d61\uff0c\u540c\u6642\u4e5f\u80fd\u627e\u51fa\u5176\u4ed6\u95dc\u9375\u56e0\u7d20\uff0c\u4f8b\u5982\u6307\u4ee4\u96e3\u5ea6\u548c\u8108\u7d61\u7d44\u6210\u3002\u6839\u64da\u9019\u4e9b\u767c\u73fe\uff0c\u6211\u5011\u63d0\u51fa\u8108\u7d61\u5408\u6210\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u8cc7\u6599\u5408\u6210\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u73fe\u6210\u7684 LLM \u70ba\u9ad8\u54c1\u8cea\u7684\u6307\u4ee4\u56de\u7b54\u5c0d\u751f\u6210\u64f4\u5145\u7684\u80cc\u666f\u8108\u7d61\u3002\u6587\u4ef6\u7d1a\u57fa\u6e96\u6e2c\u8a66 (LongBench) \u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u512a\u65bc\u5148\u524d\u7684\u6307\u4ee4\u5408\u6210\u65b9\u6cd5\uff0c\u4e26\u4e14\u63a5\u8fd1\u4eba\u5de5\u6a19\u8a3b\u9577\u8108\u7d61\u6307\u4ee4\u8cc7\u6599\u7684\u6548\u80fd\u3002\u8a72\u5c08\u6848\u5c07\u65bc\u4ee5\u4e0b\u4f4d\u7f6e\u63d0\u4f9b\uff1a\nhttps://github.com/NJUNLP/context-synthesis\u3002", "author": "Wenhao Zhu et.al.", "authors": "Wenhao Zhu, Pinzhen Chen, Hanxu Hu, Shujian Huang, Fei Yuan, Jiajun Chen, Alexandra Birch", "id": "2502.15592v1", "paper_url": "http://arxiv.org/abs/2502.15592v1", "repo": "null"}}