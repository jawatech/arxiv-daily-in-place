{"2406.12644": {"publish_time": "2024-06-18", "title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models", "paper_summary": "Assessing the effectiveness of large language models (LLMs) in addressing\ndiverse tasks is essential for comprehending their strengths and weaknesses.\nConventional evaluation techniques typically apply a single prompting strategy\nuniformly across datasets, not considering the varying degrees of task\ncomplexity. We introduce the Hierarchical Prompting Taxonomy (HPT), a taxonomy\nthat employs a Hierarchical Prompt Framework (HPF) composed of five unique\nprompting strategies, arranged from the simplest to the most complex, to assess\nLLMs more precisely and to offer a clearer perspective. This taxonomy assigns a\nscore, called the Hierarchical Prompting Score (HP-Score), to datasets as well\nas LLMs based on the rules of the taxonomy, providing a nuanced understanding\nof their ability to solve diverse tasks and offering a universal measure of\ntask complexity. Additionally, we introduce the Adaptive Hierarchical Prompt\nframework, which automates the selection of appropriate prompting strategies\nfor each task. This study compares manual and adaptive hierarchical prompt\nframeworks using four instruction-tuned LLMs, namely Llama 3 8B, Phi 3 3.8B,\nMistral 7B, and Gemma 7B, across four datasets: BoolQ, CommonSenseQA (CSQA),\nIWSLT-2017 en-fr (IWSLT), and SamSum. Experiments demonstrate the effectiveness\nof HPT, providing a reliable way to compare different tasks and LLM\ncapabilities. This paper leads to the development of a universal evaluation\nmetric that can be used to evaluate both the complexity of the datasets and the\ncapabilities of LLMs. The implementation of both manual HPF and adaptive HPF is\npublicly available.", "paper_summary_zh": "<paragraph>\u8a55\u4f30\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u8655\u7406\u4e0d\u540c\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\u5c0d\u65bc\u7406\u89e3\u5176\u512a\u7f3a\u9ede\u81f3\u95dc\u91cd\u8981\u3002\u50b3\u7d71\u8a55\u4f30\u6280\u8853\u901a\u5e38\u5728\u6240\u6709\u8cc7\u6599\u96c6\u4e0a\u7d71\u4e00\u61c9\u7528\u55ae\u4e00\u63d0\u793a\u7b56\u7565\uff0c\u800c\u4e0d\u8003\u616e\u4efb\u52d9\u8907\u96dc\u5ea6\u7684\u4e0d\u540c\u7a0b\u5ea6\u3002\u6211\u5011\u5f15\u5165\u4e86\u5206\u5c64\u63d0\u793a\u5206\u985e\u6cd5 (HPT)\uff0c\u9019\u662f\u4e00\u7a2e\u5206\u985e\u6cd5\uff0c\u63a1\u7528\u7531\u4e94\u7a2e\u7368\u7279\u63d0\u793a\u7b56\u7565\u7d44\u6210\u7684\u5206\u5c64\u63d0\u793a\u6846\u67b6 (HPF)\uff0c\u5f9e\u6700\u7c21\u55ae\u5230\u6700\u8907\u96dc\uff0c\u4ee5\u66f4\u7cbe\u78ba\u5730\u8a55\u4f30 LLM \u4e26\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u89c0\u9ede\u3002\u6b64\u5206\u985e\u6cd5\u6839\u64da\u5206\u985e\u6cd5\u7684\u898f\u5247\uff0c\u70ba\u8cc7\u6599\u96c6\u548c LLM \u5206\u914d\u4e00\u500b\u7a31\u70ba\u5206\u5c64\u63d0\u793a\u5206\u6578 (HP \u5206\u6578) \u7684\u5206\u6578\uff0c\u63d0\u4f9b\u5c0d\u5176\u89e3\u6c7a\u4e0d\u540c\u4efb\u52d9\u7684\u80fd\u529b\u7684\u7d30\u7dfb\u7406\u89e3\uff0c\u4e26\u63d0\u4f9b\u4efb\u52d9\u8907\u96dc\u5ea6\u7684\u901a\u7528\u8861\u91cf\u6a19\u6e96\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u81ea\u9069\u61c9\u5206\u5c64\u63d0\u793a\u6846\u67b6\uff0c\u5b83\u81ea\u52d5\u5316\u4e86\u70ba\u6bcf\u500b\u4efb\u52d9\u9078\u64c7\u9069\u7576\u63d0\u793a\u7b56\u7565\u7684\u904e\u7a0b\u3002\u672c\u7814\u7a76\u4f7f\u7528\u56db\u500b\u6307\u4ee4\u8abf\u6574\u7684 LLM\uff08\u5373 Llama 3 8B\u3001Phi 3 3.8B\u3001Mistral 7B \u548c Gemma 7B\uff09\u5728\u56db\u500b\u8cc7\u6599\u96c6\uff08BoolQ\u3001\u5e38\u8b58\u554f\u7b54 (CSQA)\u3001IWSLT-2017 en-fr (IWSLT) \u548c SamSum\uff09\u4e0a\u6bd4\u8f03\u4e86\u624b\u52d5\u548c\u81ea\u9069\u61c9\u5206\u5c64\u63d0\u793a\u6846\u67b6\u3002\u5be6\u9a57\u8b49\u660e\u4e86 HPT \u7684\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u7a2e\u6bd4\u8f03\u4e0d\u540c\u4efb\u52d9\u548c LLM \u80fd\u529b\u7684\u53ef\u9760\u65b9\u6cd5\u3002\u672c\u6587\u5c0e\u81f4\u958b\u767c\u4e86\u4e00\u500b\u901a\u7528\u8a55\u4f30\u6307\u6a19\uff0c\u53ef\u7528\u65bc\u8a55\u4f30\u8cc7\u6599\u96c6\u7684\u8907\u96dc\u6027\u548c LLM \u7684\u80fd\u529b\u3002\u624b\u52d5 HPF \u548c\u81ea\u9069\u61c9 HPF \u7684\u5be6\u73fe\u5df2\u516c\u958b\u3002</paragraph>", "author": "Devichand Budagam et.al.", "authors": "Devichand Budagam, Sankalp KJ, Ashutosh Kumar, Vinija Jain, Aman Chadha", "id": "2406.12644v1", "paper_url": "http://arxiv.org/abs/2406.12644v1", "repo": "https://github.com/devichand579/HPT"}}