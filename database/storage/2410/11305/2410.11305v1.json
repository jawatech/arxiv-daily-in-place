{"2410.11305": {"publish_time": "2024-10-15", "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes", "paper_summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).", "paper_summary_zh": "\u91cf\u5316\u6280\u8853\u5df2\u88ab\u5ee3\u6cdb\u63a1\u7528\u4ee5\u52a0\u901f\u63a8\u7406\u4e26\u6e1b\u5c11\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u8a18\u61b6\u9ad4\u6d88\u8017\u3002\u96d6\u7136\u6fc0\u6d3b\u6b0a\u91cd\u806f\u5408\u91cf\u5316\u900f\u904e\u4f4e\u7cbe\u5ea6\u6838\u5fc3\u52a0\u901f\u63a8\u7406\u904e\u7a0b\uff0c\u4f46\u6211\u5011\u8b49\u660e\u5b83\u5728\u591a\u6b65\u9a5f\u63a8\u7406\u4efb\u52d9\u4e2d\u6703\u906d\u53d7\u56b4\u91cd\u7684\u6548\u80fd\u964d\u4f4e\uff0c\u5c0e\u81f4\u5b83\u7121\u6548\u3002\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7a31\u70ba QSPEC \u7684\u65b0\u91cf\u5316\u7bc4\u4f8b\uff0c\u5b83\u7121\u7e2b\u6574\u5408\u4e86\u5169\u7a2e\u4e92\u88dc\u7684\u91cf\u5316\u65b9\u6848\uff0c\u7528\u65bc\u63a8\u6e2c\u6027\u89e3\u78bc\u3002\u5229\u7528\u5e7e\u4e4e\u514d\u8cbb\u7684\u57f7\u884c\u5207\u63db\uff0cQSPEC \u8d77\u8349\u5177\u6709\u4f4e\u7cbe\u5ea6\u3001\u5feb\u901f\u6fc0\u6d3b\u6b0a\u91cd\u91cf\u5316\u7684\u4ee3\u78bc\uff0c\u4e26\u4f7f\u7528\u9ad8\u7cbe\u5ea6\u50c5\u6b0a\u91cd\u91cf\u5316\u9a57\u8b49\u5b83\u5011\uff0c\u6709\u6548\u5730\u7d50\u5408\u4e86\u5169\u7a2e\u91cf\u5316\u65b9\u6848\u7684\u512a\u9ede\u3002\u8207\u9ad8\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u76f8\u6bd4\uff0cQSPEC \u5be6\u8b49\u5730\u5c07\u4ee3\u78bc\u7522\u751f\u541e\u5410\u91cf\u63d0\u5347\u4e86 1.80 \u500d\uff0c\u4e14\u4e0d\u5f71\u97ff\u4efb\u4f55\u54c1\u8cea\uff0c\u4f7f\u5176\u6709\u5225\u65bc\u5176\u4ed6\u4f4e\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u3002\u9019\u7a2e\u589e\u5f37\u4e5f\u4e00\u81f4\u9069\u7528\u65bc\u5404\u7a2e\u670d\u52d9\u4efb\u52d9\u3001\u6a21\u578b\u5927\u5c0f\u3001\u91cf\u5316\u65b9\u6cd5\u548c\u6279\u6b21\u5927\u5c0f\u3002\u8207\u73fe\u6709\u7684\u63a8\u6e2c\u6027\u89e3\u78bc\u6280\u8853\u4e0d\u540c\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u6703\u91cd\u8907\u4f7f\u7528\u6b0a\u91cd\u548c KV \u5feb\u53d6\uff0c\u907f\u514d\u984d\u5916\u7684\u8a18\u61b6\u9ad4\u958b\u92b7\u3002\u6b64\u5916\uff0cQSPEC \u63d0\u4f9b\u5373\u63d2\u5373\u7528\u7684\u512a\u9ede\uff0c\u7121\u9700\u4efb\u4f55\u8a13\u7df4\u3002\u6211\u5011\u76f8\u4fe1 QSPEC \u70ba\u672a\u4f86\u90e8\u7f72\u9ad8\u4fdd\u771f\u91cf\u5316\u65b9\u6848\u5c55\u73fe\u4e86\u7368\u7279\u7684\u512a\u52e2\uff0c\u7279\u5225\u662f\u5728\u53d7\u8a18\u61b6\u9ad4\u9650\u5236\u7684\u5834\u666f\uff08\u4f8b\u5982\u908a\u7de3\u88dd\u7f6e\uff09\u4e2d\u3002", "author": "Juntao Zhao et.al.", "authors": "Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu", "id": "2410.11305v1", "paper_url": "http://arxiv.org/abs/2410.11305v1", "repo": "null"}}