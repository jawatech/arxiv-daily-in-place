{"2406.07522": {"publish_time": "2024-06-11", "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling", "paper_summary": "Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.", "paper_summary_zh": "\u6709\u6548\u5730\u5c0d\u5177\u6709\u7121\u9650\u4e0a\u4e0b\u6587\u9577\u5ea6\u7684\u5e8f\u5217\u9032\u884c\u5efa\u6a21\u4e00\u76f4\u662f\u4e00\u500b\u9577\u671f\u7684\u554f\u984c\u3002\u904e\u53bb\u7684\u5de5\u4f5c\u8981\u4e48\u8a08\u7b97\u8907\u96dc\u5ea6\u70ba\u4e8c\u6b21\u65b9\uff0c\u8981\u4e48\u5728\u9577\u5ea6\u6cdb\u5316\u4e0a\u5916\u63a8\u80fd\u529b\u6709\u9650\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86 Samba\uff0c\u9019\u662f\u4e00\u7a2e\u7c21\u55ae\u7684\u6df7\u5408\u67b6\u69cb\uff0c\u5b83\u9010\u5c64\u7d50\u5408\u4e86\u9078\u64c7\u6027\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM) Mamba \u8207\u6ed1\u52d5\u7a97\u53e3\u6ce8\u610f\u529b (SWA)\u3002Samba \u5c07\u7d66\u5b9a\u7684\u5e8f\u5217\u9078\u64c7\u6027\u5730\u58d3\u7e2e\u5230\u905e\u8ff4\u96b1\u85cf\u72c0\u614b\u4e2d\uff0c\u540c\u6642\u4ecd\u4fdd\u6301\u4f7f\u7528\u6ce8\u610f\u529b\u6a5f\u5236\u6e96\u78ba\u53ec\u56de\u8a18\u61b6\u7684\u80fd\u529b\u3002\u6211\u5011\u5c07 Samba \u64f4\u5c55\u5230 3.8B \u53c3\u6578\uff0c\u4e26\u4f7f\u7528 3.2T \u8a13\u7df4\u4ee4\u724c\uff0c\u4e26\u8868\u660e Samba \u5728\u57fa\u65bc\u7d14\u6ce8\u610f\u529b\u6216 SSM \u7684\u5404\u7a2e\u57fa\u6e96\u4e0a\u90fd\u660e\u986f\u512a\u65bc\u6700\u5148\u9032\u7684\u6a21\u578b\u3002\u5728 4K \u9577\u5ea6\u5e8f\u5217\u4e0a\u9032\u884c\u8a13\u7df4\u6642\uff0cSamba \u53ef\u4ee5\u6709\u6548\u5730\u5916\u63a8\u5230 256K \u4e0a\u4e0b\u6587\u9577\u5ea6\uff0c\u4e26\u5b8c\u7f8e\u5730\u53ec\u56de\u8a18\u61b6\uff0c\u4e26\u986f\u793a\u51fa\u5728\u9577\u9054 1M \u4e0a\u4e0b\u6587\u9577\u5ea6\u7bc4\u570d\u5167\u6539\u9032\u7684\u4ee4\u724c\u9810\u6e2c\u3002\u4f5c\u70ba\u4e00\u500b\u7dda\u6027\u6642\u9593\u5e8f\u5217\u6a21\u578b\uff0c\u8207\u5177\u6709\u5206\u7d44\u67e5\u8a62\u6ce8\u610f\u529b\u7684 Transformer \u76f8\u6bd4\uff0cSamba \u5728\u8655\u7406 128K \u9577\u5ea6\u7684\u7528\u6236\u63d0\u793a\u6642\u4eab\u6709\u9ad8 3.73 \u500d\u7684\u541e\u5410\u91cf\uff0c\u4e26\u4e14\u5728\u4f7f\u7528\u7121\u9650\u4e32\u6d41\u751f\u6210 64K \u4ee4\u724c\u6642\uff0c\u52a0\u901f\u4e86 3.64 \u500d\u3002Samba \u7684\u793a\u4f8b\u5be6\u4f5c\u5df2\u516c\u958b\u767c\u5e03\u5728 https://github.com/microsoft/Samba\u3002", "author": "Liliang Ren et.al.", "authors": "Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen", "id": "2406.07522v1", "paper_url": "http://arxiv.org/abs/2406.07522v1", "repo": "null"}}