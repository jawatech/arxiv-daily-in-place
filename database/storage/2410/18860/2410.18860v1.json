{"2410.18860": {"publish_time": "2024-10-24", "title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations", "paper_summary": "Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%).", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7d93\u5e38\u7522\u751f\u5e7b\u89ba\uff0c\u900f\u904e\u932f\u8aa4\u9673\u8ff0\u63d0\u4f9b\u7684\u8108\u7d61\u6216\u932f\u8aa4\u5730\u56de\u61b6\u5167\u90e8\u77e5\u8b58\u4f86\u7522\u751f\u4e0d\u5fe0\u5be6\u6216\u4e8b\u5be6\u4e0a\u4e0d\u6b63\u78ba\u7684\u8f38\u51fa\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u8b58\u5225\u51fa Transformer \u67b6\u69cb\u4e2d\u7279\u5b9a\u6ce8\u610f\u529b\u982d\uff0c\u7a31\u70ba\u6aa2\u7d22\u982d\uff0c\u8ca0\u8cac\u64f7\u53d6\u76f8\u95dc\u7684\u8108\u7d61\u8cc7\u8a0a\u3002\u6211\u5011\u5047\u8a2d\u906e\u853d\u9019\u4e9b\u6aa2\u7d22\u982d\u6703\u8a98\u767c\u5e7b\u89ba\uff0c\u4e26\u4e14\u5c0d\u6bd4\u57fa\u790e LLM \u548c\u906e\u853d LLM \u7684\u8f38\u51fa\u53ef\u4ee5\u6e1b\u5c11\u5e7b\u89ba\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u900f\u904e\u5c0d\u6bd4\u6aa2\u7d22\u982d\u9032\u884c\u89e3\u78bc (DeCoRe)\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u7121\u8a13\u7df4\u89e3\u78bc\u7b56\u7565\uff0c\u5b83\u6703\u653e\u5927\u8108\u7d61\u548c\u6a21\u578b\u53c3\u6578\u4e2d\u627e\u5230\u7684\u8cc7\u8a0a\u3002DeCoRe \u900f\u904e\u52d5\u614b\u5c0d\u6bd4\u57fa\u790e LLM \u548c\u906e\u853d LLM \u7684\u8f38\u51fa\uff0c\u4f7f\u7528\u689d\u4ef6\u71b5\u4f5c\u70ba\u6307\u5357\uff0c\u4f86\u6e1b\u8f15\u6f5b\u5728\u7684\u5e7b\u89ba\u53cd\u61c9\u3002\u6211\u5011\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u5be6\uff0cDeCoRe \u5728\u9700\u8981\u9ad8\u5ea6\u8108\u7d61\u5fe0\u5be6\u5ea6\u7684\u4efb\u52d9\u4e0a\u986f\u8457\u6539\u5584\u6548\u80fd\uff0c\u4f8b\u5982\u6458\u8981 (XSum \u63d0\u9ad8 18.6%)\u3001\u6307\u4ee4\u9075\u5faa (MemoTrap \u63d0\u9ad8 10.9%)\uff0c\u4ee5\u53ca\u958b\u653e\u5f0f\u554f\u7b54 (NQ-Open \u63d0\u9ad8 2.4% \u548c NQ-Swap \u63d0\u9ad8 5.5%)\u3002", "author": "Aryo Pradipta Gema et.al.", "authors": "Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare, Beatrice Alex, Pasquale Minervini, Amrutha Saseendran", "id": "2410.18860v1", "paper_url": "http://arxiv.org/abs/2410.18860v1", "repo": "https://github.com/aryopg/decore"}}