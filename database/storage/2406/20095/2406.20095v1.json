{"2406.20095": {"publish_time": "2024-06-28", "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy", "paper_summary": "Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5177\u5099\u5ee3\u6cdb\u7684\u4e16\u754c\u77e5\u8b58\u548c\u5f37\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u8655\u7406\u8de8\u9818\u57df\u7684\u4e0d\u540c\u4efb\u52d9\uff0c\u901a\u5e38\u900f\u904e\u5c07\u5b83\u5011\u8a2d\u5b9a\u70ba\u5c0d\u8a71\u5f0f\u6307\u4ee4\u56de\u61c9\u5c0d\u8a71\u4f86\u8655\u7406\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa LLaRA\uff1a\u5927\u578b\u8a9e\u8a00\u548c\u6a5f\u5668\u4eba\u52a9\u7406\uff0c\u4e00\u500b\u5c07\u6a5f\u5668\u4eba\u52d5\u4f5c\u7b56\u7565\u5236\u5b9a\u70ba\u5c0d\u8a71\u7684\u6846\u67b6\uff0c\u4e26\u5728\u4f7f\u7528\u88dc\u5145\u7b56\u7565\u5b78\u7fd2\u7684\u8f14\u52a9\u8cc7\u6599\u9032\u884c\u8a13\u7df4\u6642\u63d0\u4f9b\u6539\u9032\u7684\u56de\u61c9\u3002\u5177\u6709\u8996\u89ba\u8f38\u5165\u7684 LLM\uff0c\u5373\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM)\uff0c\u6709\u80fd\u529b\u5c07\u72c0\u614b\u8cc7\u8a0a\u8655\u7406\u70ba\u8996\u89ba\u6587\u5b57\u63d0\u793a\uff0c\u4e26\u4ee5\u6587\u5b57\u7522\u751f\u6700\u4f73\u7b56\u7565\u6c7a\u7b56\u3002\u70ba\u4e86\u8a13\u7df4\u6b64\u985e\u52d5\u4f5c\u7b56\u7565 VLM\uff0c\u6211\u5011\u9996\u5148\u5f15\u5165\u4e00\u500b\u81ea\u52d5\u5316\u7ba1\u9053\uff0c\u5f9e\u73fe\u6709\u7684\u884c\u70ba\u8907\u88fd\u8cc7\u6599\u4e2d\u7522\u751f\u591a\u6a23\u5316\u7684\u9ad8\u54c1\u8cea\u6a5f\u5668\u4eba\u6307\u4ee4\u8cc7\u6599\u3002\u91dd\u5c0d\u6a5f\u5668\u4eba\u4efb\u52d9\u91cf\u8eab\u6253\u9020\u7684\u5c0d\u8a71\u5f0f\u5236\u5b9a\u65b9\u5f0f\uff0c\u4f7f\u7528\u7531\u6b64\u7522\u751f\u7684\u8cc7\u6599\u96c6\u96c6\u5408\u5fae\u8abf\u7684 VLM\uff0c\u53ef\u4ee5\u7522\u751f\u6709\u610f\u7fa9\u7684\u6a5f\u5668\u4eba\u52d5\u4f5c\u7b56\u7565\u6c7a\u7b56\u3002\u6211\u5011\u5728\u591a\u500b\u6a21\u64ec\u548c\u771f\u5be6\u4e16\u754c\u74b0\u5883\u4e2d\u9032\u884c\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6240\u63d0\u51fa\u7684 LLaRA \u6846\u67b6\u7684\u6700\u65b0\u6280\u8853\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u96c6\u548c\u9810\u8a13\u7df4\u6a21\u578b\u53ef\u5728 https://github.com/LostXine/LLaRA \u53d6\u5f97\u3002", "author": "Xiang Li et.al.", "authors": "Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo", "id": "2406.20095v1", "paper_url": "http://arxiv.org/abs/2406.20095v1", "repo": "https://github.com/lostxine/llara"}}