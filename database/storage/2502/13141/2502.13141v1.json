{"2502.13141": {"publish_time": "2025-02-18", "title": "UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models", "paper_summary": "Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\nto generate harmful outputs. In this paper, departing from traditional deep\nlearning attack paradigms, we explore their intrinsic relationship and\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\nwe propose UniGuardian, the first unified defense mechanism designed to detect\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\nAdditionally, we introduce a single-forward strategy to optimize the detection\npipeline, enabling simultaneous attack detection and text generation within a\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\nefficiently identifies malicious prompts in LLMs.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5bb9\u6613\u53d7\u5230\u63d0\u793a\u6ce8\u5165\u3001\u5f8c\u9580\u653b\u64ca\u548c\u5c0d\u6297\u6027\u653b\u64ca\u7b49\u653b\u64ca\uff0c\u9019\u4e9b\u653b\u64ca\u6703\u64cd\u7e31\u63d0\u793a\u6216\u6a21\u578b\u4ee5\u7522\u751f\u6709\u5bb3\u7684\u8f38\u51fa\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u8df3\u812b\u50b3\u7d71\u6df1\u5ea6\u5b78\u7fd2\u653b\u64ca\u7bc4\u4f8b\uff0c\u63a2\u8a0e\u5b83\u5011\u7684\u5167\u5728\u95dc\u4fc2\uff0c\u4e26\u5c07\u5b83\u5011\u7d71\u7a31\u70ba\u63d0\u793a\u89f8\u767c\u653b\u64ca (PTA)\u3002\u9019\u5f15\u767c\u4e86\u4e00\u500b\u95dc\u9375\u554f\u984c\uff1a\u6211\u5011\u80fd\u78ba\u5b9a\u4e00\u500b\u63d0\u793a\u662f\u826f\u6027\u7684\u9084\u662f\u60e1\u610f\u7684\u55ce\uff1f\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 UniGuardian\uff0c\u9019\u662f\u4e00\u7a2e\u65e8\u5728\u5075\u6e2c LLM \u4e2d\u7684\u63d0\u793a\u6ce8\u5165\u3001\u5f8c\u9580\u653b\u64ca\u548c\u5c0d\u6297\u6027\u653b\u64ca\u7684\u7b2c\u4e00\u500b\u7d71\u4e00\u9632\u79a6\u6a5f\u5236\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u55ae\u4e00\u524d\u5411\u7b56\u7565\u4f86\u6700\u4f73\u5316\u5075\u6e2c\u7ba1\u9053\uff0c\u5728\u55ae\u4e00\u524d\u5411\u50b3\u905e\u4e2d\u540c\u6642\u9032\u884c\u653b\u64ca\u5075\u6e2c\u548c\u6587\u5b57\u751f\u6210\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u5be6\uff0cUniGuardian \u80fd\u6e96\u78ba\u4e14\u6709\u6548\u5730\u8b58\u5225 LLM \u4e2d\u7684\u60e1\u610f\u63d0\u793a\u3002", "author": "Huawei Lin et.al.", "authors": "Huawei Lin, Yingjie Lao, Tong Geng, Tan Yu, Weijie Zhao", "id": "2502.13141v1", "paper_url": "http://arxiv.org/abs/2502.13141v1", "repo": "null"}}