{"2411.15115": {"publish_time": "2024-11-22", "title": "VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement", "paper_summary": "Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of four stages: In (1) video evaluation, we detect\nmisalignments by generating fine-grained evaluation questions and answering\nthose questions with MLLM. In (2) refinement planning, we identify accurately\ngenerated objects and then create localized prompts to refine other areas in\nthe video. Next, in (3) region decomposition, we segment the correctly\ngenerated area using a combined grounding module. We regenerate the video by\nadjusting the misaligned regions while preserving the correct regions in (4)\nlocalized refinement. On two popular video generation benchmarks (EvalCrafter\nand T2V-CompBench), VideoRepair substantially outperforms recent baselines\nacross various text-video alignment metrics. We provide a comprehensive\nanalysis of VideoRepair components and qualitative examples.", "paper_summary_zh": "\u6700\u8fd1\u7684\u6587\u5b57\u8f49\u5f71\u7247 (T2V) \u64f4\u6563\u6a21\u578b\u5df2\u5c55\u793a\u51fa\u8de8\u5404\u7a2e\u9818\u57df\u7684\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u751f\u6210\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u7d93\u5e38\u6703\u7522\u751f\u8207\u6587\u5b57\u63d0\u793a\u4e0d\u4e00\u81f4\u7684\u5f71\u7247\uff0c\u7279\u5225\u662f\u5728\u63d0\u793a\u63cf\u8ff0\u5177\u6709\u591a\u500b\u7269\u4ef6\u548c\u5c6c\u6027\u7684\u8907\u96dc\u5834\u666f\u6642\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u5165\u4e86 VideoRepair\uff0c\u4e00\u7a2e\u65b0\u7a4e\u7684\u8207\u6a21\u578b\u7121\u95dc\u3001\u7121\u9700\u8a13\u7df4\u7684\u5f71\u7247\u7cbe\u7149\u67b6\u69cb\uff0c\u5b83\u6703\u81ea\u52d5\u8b58\u5225\u7d30\u5fae\u7684\u6587\u5b57\u5f71\u7247\u4e0d\u4e00\u81f4\u4e4b\u8655\uff0c\u4e26\u7522\u751f\u660e\u78ba\u7684\u7a7a\u9593\u548c\u6587\u5b57\u56de\u994b\uff0c\u8b93 T2V \u64f4\u6563\u6a21\u578b\u57f7\u884c\u6709\u91dd\u5c0d\u6027\u7684\u5c40\u90e8\u7cbe\u7149\u3002VideoRepair \u5305\u542b\u56db\u500b\u968e\u6bb5\uff1a\u5728 (1) \u5f71\u7247\u8a55\u4f30\u4e2d\uff0c\u6211\u5011\u900f\u904e\u7522\u751f\u7d30\u5fae\u7684\u8a55\u4f30\u554f\u984c\u4e26\u4f7f\u7528 MLLM \u56de\u7b54\u9019\u4e9b\u554f\u984c\u4f86\u5075\u6e2c\u4e0d\u4e00\u81f4\u4e4b\u8655\u3002\u5728 (2) \u7cbe\u7149\u898f\u5283\u4e2d\uff0c\u6211\u5011\u8b58\u5225\u6e96\u78ba\u751f\u6210\u7684\u7269\u4ef6\uff0c\u7136\u5f8c\u5efa\u7acb\u5c40\u90e8\u63d0\u793a\u4f86\u7cbe\u7149\u5f71\u7247\u4e2d\u7684\u5176\u4ed6\u5340\u57df\u3002\u63a5\u8457\uff0c\u5728 (3) \u5340\u57df\u5206\u89e3\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u7d50\u5408\u63a5\u5730\u6a21\u7d44\u4f86\u5340\u9694\u6b63\u78ba\u751f\u6210\u7684\u5340\u57df\u3002\u6211\u5011\u5728 (4) \u5c40\u90e8\u7cbe\u7149\u4e2d\u8abf\u6574\u4e0d\u4e00\u81f4\u7684\u5340\u57df\uff0c\u540c\u6642\u4fdd\u7559\u6b63\u78ba\u7684\u5340\u57df\uff0c\u85c9\u6b64\u91cd\u65b0\u7522\u751f\u5f71\u7247\u3002\u5728\u5169\u500b\u6d41\u884c\u7684\u5f71\u7247\u751f\u6210\u57fa\u6e96 (EvalCrafter \u548c T2V-CompBench) \u4e0a\uff0cVideoRepair \u5728\u5404\u7a2e\u6587\u5b57\u5f71\u7247\u5c0d\u9f4a\u6307\u6a19\u65b9\u9762\u90fd\u5927\u5e45\u512a\u65bc\u6700\u8fd1\u7684\u57fa\u6e96\u3002\u6211\u5011\u63d0\u4f9b\u4e86 VideoRepair \u7d44\u6210\u90e8\u5206\u7684\u5168\u9762\u5206\u6790\u548c\u5b9a\u6027\u7bc4\u4f8b\u3002", "author": "Daeun Lee et.al.", "authors": "Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal", "id": "2411.15115v1", "paper_url": "http://arxiv.org/abs/2411.15115v1", "repo": "null"}}