{"2410.16077": {"publish_time": "2024-10-21", "title": "CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian Product Routing in Mixture-of-Experts", "paper_summary": "Large language models (LLM) have been attracting much attention from the\ncommunity recently, due to their remarkable performance in all kinds of\ndownstream tasks. According to the well-known scaling law, scaling up a dense\nLLM enhances its capabilities, but also significantly increases the\ncomputational complexity. Mixture-of-Experts (MoE) models address that by\nallowing the model size to grow without substantially raising training or\ninference costs. Yet MoE models face challenges regarding knowledge sharing\namong experts, making their performance somehow sensitive to routing accuracy.\nTo tackle that, previous works introduced shared experts and combined their\noutputs with those of the top $K$ routed experts in an ``addition'' manner. In\nthis paper, inspired by collective matrix factorization to learn shared\nknowledge among data, we propose CartesianMoE, which implements more effective\nknowledge sharing among experts in more like a ``multiplication'' manner.\nExtensive experimental results indicate that CartesianMoE outperforms previous\nMoE models for building LLMs, in terms of both perplexity and downstream task\nperformance. And we also find that CartesianMoE achieves better expert routing\nrobustness.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8fd1\u671f\u5ee3\u53d7\u793e\u7fa4\u95dc\u6ce8\uff0c\u56e0\u70ba\u5b83\u5011\u5728\u5404\u7a2e\u4e0b\u6e38\u4efb\u52d9\u4e2d\u8868\u73fe\u5091\u51fa\u3002\u6839\u64da\u8457\u540d\u7684\u898f\u6a21\u5b9a\u5f8b\uff0c\u64f4\u5927\u5bc6\u96c6 LLM \u80fd\u589e\u5f37\u5176\u529f\u80fd\uff0c\u4f46\u4e5f\u5927\u5e45\u589e\u52a0\u904b\u7b97\u8907\u96dc\u5ea6\u3002\u5c08\u5bb6\u6df7\u5408 (MoE) \u6a21\u578b\u900f\u904e\u5141\u8a31\u6a21\u578b\u898f\u6a21\u64f4\u5927\u800c\u4e0d\u6703\u5927\u5e45\u589e\u52a0\u8a13\u7df4\u6216\u63a8\u8ad6\u6210\u672c\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\u7136\u800c\uff0cMoE \u6a21\u578b\u5728\u5c08\u5bb6\u4e4b\u9593\u7684\u77e5\u8b58\u5171\u4eab\u65b9\u9762\u9762\u81e8\u6311\u6230\uff0c\u4f7f\u5f97\u5b83\u5011\u7684\u6548\u80fd\u591a\u5c11\u6703\u53d7\u5230\u8def\u7531\u7cbe\u6e96\u5ea6\u7684\u5f71\u97ff\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u5148\u524d\u7684\u7814\u7a76\u5f15\u5165\u4e86\u5171\u4eab\u5c08\u5bb6\uff0c\u4e26\u4ee5\u300c\u52a0\u6cd5\u300d\u7684\u65b9\u5f0f\u5c07\u4ed6\u5011\u7684\u8f38\u51fa\u8207\u8def\u7531\u524d $K$ \u540d\u5c08\u5bb6\u7684\u8f38\u51fa\u7d50\u5408\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u53d7\u5230\u96c6\u9ad4\u77e9\u9663\u5206\u89e3\u7684\u555f\u767c\uff0c\u5f9e\u8cc7\u6599\u4e2d\u5b78\u7fd2\u5171\u4eab\u77e5\u8b58\uff0c\u4e26\u63d0\u51fa\u7b1b\u5361\u5152 MoE\uff0c\u5b83\u4ee5\u66f4\u50cf\u662f\u300c\u4e58\u6cd5\u300d\u7684\u65b9\u5f0f\u5728\u5c08\u5bb6\u4e4b\u9593\u5be6\u4f5c\u66f4\u6709\u6548\u7684\u77e5\u8b58\u5171\u4eab\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u7b1b\u5361\u5152 MoE \u5728\u56f0\u60d1\u5ea6\u548c\u4e0b\u6e38\u4efb\u52d9\u6548\u80fd\u65b9\u9762\u90fd\u512a\u65bc\u5148\u524d\u7684 MoE \u6a21\u578b\uff0c\u7528\u65bc\u5efa\u69cb LLM\u3002\u6211\u5011\u4e5f\u767c\u73fe\u7b1b\u5361\u5152 MoE \u9054\u5230\u66f4\u597d\u7684\u5c08\u5bb6\u8def\u7531\u7a69\u5065\u6027\u3002", "author": "Zhenpeng Su et.al.", "authors": "Zhenpeng Su, Xing Wu, Zijia Lin, Yizhe Xiong, Minxuan Lv, Guangyuan Ma, Hui Chen, Songlin Hu, Guiguang Ding", "id": "2410.16077v2", "paper_url": "http://arxiv.org/abs/2410.16077v2", "repo": "null"}}