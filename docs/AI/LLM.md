
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-06-20**|**Model Merging and Safety Alignment: One Bad Model Spoils the Bunch**|Hasan Abed Al Kader Hammoud et.al.|[2406.14563v1](http://arxiv.org/abs/2406.14563v1)|null|
|**2024-06-20**|**Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities**|Sachit Menon et.al.|[2406.14562v1](http://arxiv.org/abs/2406.14562v1)|null|
|**2024-06-20**|**How to Compute the Probability of a Word**|Tiago Pimentel et.al.|[2406.14561v1](http://arxiv.org/abs/2406.14561v1)|null|
|**2024-06-20**|**xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics**|Daniil Larionov et.al.|[2406.14553v1](http://arxiv.org/abs/2406.14553v1)|null|
|**2024-06-20**|**GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models**|Shilong Li et.al.|[2406.14550v1](http://arxiv.org/abs/2406.14550v1)|null|
|**2024-06-20**|**Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data**|Johannes Treutlein et.al.|[2406.14546v1](http://arxiv.org/abs/2406.14546v1)|null|
|**2024-06-20**|**Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems**|Đorđe Klisura et.al.|[2406.14545v1](http://arxiv.org/abs/2406.14545v1)|null|
|**2024-06-20**|**Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs**|Yuxuan Qiao et.al.|[2406.14544v1](http://arxiv.org/abs/2406.14544v1)|[link](https://github.com/sparksjoe/prism)|
|**2024-06-20**|**IRASim: Learning Interactive Real-Robot Action Simulators**|Fangqi Zhu et.al.|[2406.14540v1](http://arxiv.org/abs/2406.14540v1)|null|
|**2024-06-20**|**RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold**|Amrith Setlur et.al.|[2406.14532v1](http://arxiv.org/abs/2406.14532v1)|null|
|**2024-06-20**|**DeciMamba: Exploring the Length Extrapolation Potential of Mamba**|Assaf Ben-Kish et.al.|[2406.14528v1](http://arxiv.org/abs/2406.14528v1)|[link](https://github.com/assafbk/decimamba)|
|**2024-06-20**|**Fantastic Copyrighted Beasts and How (Not) to Generate Them**|Luxi He et.al.|[2406.14526v1](http://arxiv.org/abs/2406.14526v1)|null|
|**2024-06-20**|**PostMark: A Robust Blackbox Watermark for Large Language Models**|Yapei Chang et.al.|[2406.14517v1](http://arxiv.org/abs/2406.14517v1)|[link](https://github.com/lilakk/postmark)|
|**2024-06-20**|**Investigating Mysteries of CoT-Augmented Distillation**|Somin Wadhwa et.al.|[2406.14511v1](http://arxiv.org/abs/2406.14511v1)|null|
|**2024-06-20**|**V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data**|Rotem Shalev-Arkushin et.al.|[2406.14510v1](http://arxiv.org/abs/2406.14510v1)|null|
|**2024-06-20**|**Evidence of a log scaling law for political persuasion with large language models**|Kobi Hackenburg et.al.|[2406.14508v1](http://arxiv.org/abs/2406.14508v1)|[link](https://github.com/kobihackenburg/scaling-llm-persuasion)|
|**2024-06-20**|**On Newton's Method to Unlearn Neural Networks**|Nhung Bui et.al.|[2406.14507v1](http://arxiv.org/abs/2406.14507v1)|null|
|**2024-06-20**|**Translating Across Cultures: LLMs for Intralingual Cultural Adaptation**|Pushpdeep Singh et.al.|[2406.14504v1](http://arxiv.org/abs/2406.14504v1)|null|
|**2024-06-20**|**Overview of the CAIL 2023 Argument Mining Track**|Jingcong Liang et.al.|[2406.14503v1](http://arxiv.org/abs/2406.14503v1)|null|
|**2024-06-20**|**Improving Expert Radiology Report Summarization by Prompting Large Language Models with a Layperson Summary**|Xingmeng Zhao et.al.|[2406.14500v1](http://arxiv.org/abs/2406.14500v1)|null|
|**2024-06-20**|**LLaSA: Large Multimodal Agent for Human Activity Analysis Through Wearable Sensors**|Sheikh Asif Imran et.al.|[2406.14498v1](http://arxiv.org/abs/2406.14498v1)|[link](https://github.com/bashlab/llasa)|
|**2024-06-20**|**CodeRAG-Bench: Can Retrieval Augment Code Generation?**|Zora Zhiruo Wang et.al.|[2406.14497v1](http://arxiv.org/abs/2406.14497v1)|null|
|**2024-06-20**|**African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification**|Gregor Geigle et.al.|[2406.14496v1](http://arxiv.org/abs/2406.14496v1)|null|
|**2024-06-20**|**Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?**|Gregor Geigle et.al.|[2406.14492v1](http://arxiv.org/abs/2406.14492v1)|null|
|**2024-06-20**|**Instruction Pre-Training: Language Models are Supervised Multitask Learners**|Daixuan Cheng et.al.|[2406.14491v1](http://arxiv.org/abs/2406.14491v1)|[link](https://github.com/microsoft/lmops)|
|**2024-06-20**|**Revealing Vision-Language Integration in the Brain with Multimodal Networks**|Vighnesh Subramaniam et.al.|[2406.14481v1](http://arxiv.org/abs/2406.14481v1)|null|
|**2024-06-20**|**On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier**|Jiachen Jiang et.al.|[2406.14479v1](http://arxiv.org/abs/2406.14479v1)|null|
|**2024-06-20**|**SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset**|Josef Dai et.al.|[2406.14477v1](http://arxiv.org/abs/2406.14477v1)|[link](https://github.com/pku-alignment/safe-sora)|
|**2024-06-20**|**Data-Centric AI in the Age of Large Language Models**|Xinyi Xu et.al.|[2406.14473v1](http://arxiv.org/abs/2406.14473v1)|null|
|**2024-06-20**|**Fusion of Movement and Naive Predictions for Point Forecasting in Univariate Random Walks**|Cheng Zhang et.al.|[2406.14469v1](http://arxiv.org/abs/2406.14469v1)|null|
|**2024-06-20**|**Explicit and Implicit Large Language Model Personas Generate Opinions but Fail to Replicate Deeper Perceptions and Biases**|Salvatore Giorgi et.al.|[2406.14462v1](http://arxiv.org/abs/2406.14462v1)|null|
|**2024-06-20**|**Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models**|Shijie Han et.al.|[2406.14459v1](http://arxiv.org/abs/2406.14459v1)|null|
|**2024-06-20**|**Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue**|Huifang Du et.al.|[2406.14457v1](http://arxiv.org/abs/2406.14457v1)|null|
|**2024-06-20**|**APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking**|Can Jin et.al.|[2406.14449v1](http://arxiv.org/abs/2406.14449v1)|[link](https://github.com/jincan333/apeer)|
|**2024-06-20**|**Graph Representation Learning Strategies for Omics Data: A Case Study on Parkinson's Disease**|Elisa Gómez de Lope et.al.|[2406.14442v1](http://arxiv.org/abs/2406.14442v1)|null|
|**2024-06-20**|**Towards Truthful Multilingual Large Language Models: Benchmarking and Alignment Strategies**|Weihao Liu et.al.|[2406.14434v1](http://arxiv.org/abs/2406.14434v1)|null|
|**2024-06-20**|**CollaFuse: Collaborative Diffusion Models**|Simeon Allmendinger et.al.|[2406.14429v1](http://arxiv.org/abs/2406.14429v1)|[link](https://github.com/simeonallmendinger/collafuse)|
|**2024-06-20**|**SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages**|Gayane Ghazaryan et.al.|[2406.14425v1](http://arxiv.org/abs/2406.14425v1)|null|
|**2024-06-20**|**FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving**|Xiaohan Lin et.al.|[2406.14408v1](http://arxiv.org/abs/2406.14408v1)|null|
|**2024-06-20**|**Fair Streaming Feature Selection**|Zhangling Duan et.al.|[2406.14401v1](http://arxiv.org/abs/2406.14401v1)|null|
|**2024-06-20**|**SEC-QA: A Systematic Evaluation Corpus for Financial QA**|Viet Dac Lai et.al.|[2406.14394v1](http://arxiv.org/abs/2406.14394v1)|null|
|**2024-06-20**|**Jailbreaking as a Reward Misspecification Problem**|Zhihui Xie et.al.|[2406.14393v1](http://arxiv.org/abs/2406.14393v1)|null|
|**2024-06-20**|**Computation-Efficient Semi-Supervised Learning for ECG-based Cardiovascular Diseases Detection**|Rushuang Zhou et.al.|[2406.14377v1](http://arxiv.org/abs/2406.14377v1)|null|
|**2024-06-20**|**Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory**|Gordon Dai et.al.|[2406.14373v1](http://arxiv.org/abs/2406.14373v1)|null|
|**2024-06-20**|**PoseBench: Benchmarking the Robustness of Pose Estimation Models under Corruptions**|Sihan Ma et.al.|[2406.14367v1](http://arxiv.org/abs/2406.14367v1)|null|
|**2024-06-20**|**Robustness Analysis of AI Models in Critical Energy Systems**|Pantelis Dogoulis et.al.|[2406.14361v1](http://arxiv.org/abs/2406.14361v1)|null|
|**2024-06-20**|**The neural correlates of logical-mathematical symbol systems processing resemble that of spatial cognition more than natural language processing**|Yuannan Li et.al.|[2406.14358v1](http://arxiv.org/abs/2406.14358v1)|null|
|**2024-06-20**|**Automatic Labels are as Effective as Manual Labels in Biomedical Images Classification with Deep Learning**|Niccolò Marini et.al.|[2406.14351v1](http://arxiv.org/abs/2406.14351v1)|null|
|**2024-06-20**|**iWISDM: Assessing instruction following in multimodal models at scale**|Xiaoxuan Lei et.al.|[2406.14343v1](http://arxiv.org/abs/2406.14343v1)|null|
|**2024-06-20**|**Exploring Spatial Representations in the Historical Lake District Texts with LLM-based Relation Extraction**|Erum Haris et.al.|[2406.14336v1](http://arxiv.org/abs/2406.14336v1)|null|
|**2024-06-20**|**Self-supervised Interpretable Concept-based Models for Text Classification**|Francesco De Santis et.al.|[2406.14335v1](http://arxiv.org/abs/2406.14335v1)|null|
|**2024-06-20**|**medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs**|Mingyi Jia et.al.|[2406.14326v1](http://arxiv.org/abs/2406.14326v1)|null|
|**2024-06-20**|**Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning**|Lynn Chua et.al.|[2406.14322v1](http://arxiv.org/abs/2406.14322v1)|null|
|**2024-06-20**|**LiveMind: Low-latency Large Language Models with Simultaneous Inference**|Chuangtao Chen et.al.|[2406.14319v1](http://arxiv.org/abs/2406.14319v1)|[link](https://github.com/chuangtaochen-tum/livemind)|
|**2024-06-20**|**The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts**|Zhili Shen et.al.|[2406.14318v1](http://arxiv.org/abs/2406.14318v1)|null|
|**2024-06-20**|**Identifying User Goals from UI Trajectories**|Omri Berkovitch et.al.|[2406.14314v1](http://arxiv.org/abs/2406.14314v1)|null|
|**2024-06-20**|**Robust Few-shot Transfer Learning for Knowledge Base Question Answering with Unanswerable Questions**|Riya Sawhney et.al.|[2406.14313v1](http://arxiv.org/abs/2406.14313v1)|null|
|**2024-06-20**|**Infusing clinical knowledge into tokenisers for language models**|Abul Hasan et.al.|[2406.14312v1](http://arxiv.org/abs/2406.14312v1)|null|
|**2024-06-20**|**Cross-level Requirement Traceability: A Novel Approach Integrating Bag-of-Words and Word Embedding for Enhanced Similarity Functionality**|Baher Mohammad et.al.|[2406.14310v1](http://arxiv.org/abs/2406.14310v1)|null|
|**2024-06-20**|**QuST-LLM: Integrating Large Language Models for Comprehensive Spatial Transcriptomics Analysis**|Chao Hui Huang et.al.|[2406.14307v1](http://arxiv.org/abs/2406.14307v1)|null|
|**2024-06-20**|**AI in Space for Scientific Missions: Strategies for Minimizing Neural-Network Model Upload**|Jonah Ekelund et.al.|[2406.14297v1](http://arxiv.org/abs/2406.14297v1)|null|
|**2024-06-20**|**DASB -- Discrete Audio and Speech Benchmark**|Pooneh Mousavi et.al.|[2406.14294v1](http://arxiv.org/abs/2406.14294v1)|null|
|**2024-06-20**|**VAIYAKARANA : A Benchmark for Automatic Grammar Correction in Bangla**|Pramit Bhattacharyya et.al.|[2406.14284v1](http://arxiv.org/abs/2406.14284v1)|null|
|**2024-06-20**|**Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning**|Chaojie Wang et.al.|[2406.14283v1](http://arxiv.org/abs/2406.14283v1)|null|
|**2024-06-20**|**Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs**|Junjie Wang et.al.|[2406.14282v1](http://arxiv.org/abs/2406.14282v1)|null|
|**2024-06-20**|**FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and explainability**|Md Fahim Sikder et.al.|[2406.14281v1](http://arxiv.org/abs/2406.14281v1)|[link](https://github.com/fahim-sikder/fairx)|
|**2024-06-20**|**Augmenting Query and Passage for Retrieval-Augmented Generation using LLMs for Open-Domain Question Answering**|Minsang Kim et.al.|[2406.14277v1](http://arxiv.org/abs/2406.14277v1)|null|
|**2024-06-20**|**Step-Back Profiling: Distilling User History for Personalized Scientific Writing**|Xiangru Tang et.al.|[2406.14275v1](http://arxiv.org/abs/2406.14275v1)|[link](https://github.com/gersteinlab/step-back-profiling)|
|**2024-06-20**|**On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?**|Rochelle Choenni et.al.|[2406.14267v1](http://arxiv.org/abs/2406.14267v1)|null|
|**2024-06-20**|**Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries**|Anna Wróblewska et.al.|[2406.14266v1](http://arxiv.org/abs/2406.14266v1)|null|
|**2024-06-20**|**VeriFlow: Modeling Distributions for Neural Network Verification**|Faried Abu Zaid et.al.|[2406.14265v1](http://arxiv.org/abs/2406.14265v1)|null|
|**2024-06-20**|**CityNav: Language-Goal Aerial Navigation Dataset with Geographic Information**|Jungdae Lee et.al.|[2406.14240v1](http://arxiv.org/abs/2406.14240v1)|null|
|**2024-06-20**|**Enhancing robustness of data-driven SHM models: adversarial training with circle loss**|Xiangli Yang et.al.|[2406.14232v1](http://arxiv.org/abs/2406.14232v1)|null|
|**2024-06-20**|**Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing**|Han Jiang et.al.|[2406.14230v1](http://arxiv.org/abs/2406.14230v1)|null|
|**2024-06-20**|**EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms**|Siyu Yuan et.al.|[2406.14228v1](http://arxiv.org/abs/2406.14228v1)|null|
|**2024-06-20**|**REVEAL-IT: REinforcement learning with Visibility of Evolving Agent poLicy for InTerpretability**|Shuang Ao et.al.|[2406.14214v1](http://arxiv.org/abs/2406.14214v1)|[link](https://github.com/cruiseresearchgroup/reveal-it)|
|**2024-06-20**|**Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task**|Alsu Sagirova et.al.|[2406.14213v1](http://arxiv.org/abs/2406.14213v1)|null|
|**2024-06-20**|**SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots**|Weixing Wang et.al.|[2406.14208v1](http://arxiv.org/abs/2406.14208v1)|null|
|**2024-06-20**|**On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning**|Franz Nowak et.al.|[2406.14197v1](http://arxiv.org/abs/2406.14197v1)|null|
|**2024-06-20**|**VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language Model**|Jie Zhang et.al.|[2406.14194v1](http://arxiv.org/abs/2406.14194v1)|[link](https://github.com/xiangkui-cao/vlbiasbench)|
|**2024-06-20**|**Timo: Towards Better Temporal Reasoning for Language Models**|Zhaochen Su et.al.|[2406.14192v1](http://arxiv.org/abs/2406.14192v1)|[link](https://github.com/zhaochen0110/timo)|
|**2024-06-20**|**In Tree Structure Should Sentence Be Generated**|Yaguang Li et.al.|[2406.14189v1](http://arxiv.org/abs/2406.14189v1)|[link](https://github.com/arklyg/sentree)|
|**2024-06-20**|**Failure-Resilient Distributed Inference with Model Compression over Heterogeneous Edge Devices**|Li Wang et.al.|[2406.14185v1](http://arxiv.org/abs/2406.14185v1)|null|
|**2024-06-20**|**SimulSeamless: FBK at IWSLT 2024 Simultaneous Speech Translation**|Sara Papi et.al.|[2406.14177v1](http://arxiv.org/abs/2406.14177v1)|[link](https://github.com/hlt-mt/fbk-fairseq)|
|**2024-06-20**|**A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual Deepfake Detection**|Kyungbok Lee et.al.|[2406.14176v1](http://arxiv.org/abs/2406.14176v1)|null|
|**2024-06-20**|**Ranking LLMs by compression**|Peijia Guo et.al.|[2406.14171v1](http://arxiv.org/abs/2406.14171v1)|null|
|**2024-06-20**|**Definition generation for lexical semantic change detection**|Mariia Fedorova et.al.|[2406.14167v1](http://arxiv.org/abs/2406.14167v1)|null|
|**2024-06-20**|**A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning**|Panagiotis Kaliosis et.al.|[2406.14164v1](http://arxiv.org/abs/2406.14164v1)|[link](https://github.com/nlpaueb/dmmcs)|
|**2024-06-20**|**DIRAS: Efficient LLM-Assisted Annotation of Document Relevance in Retrieval Augmented Generation**|Jingwei Ni et.al.|[2406.14162v1](http://arxiv.org/abs/2406.14162v1)|null|
|**2024-06-20**|**Aligning Large Language Models with Diverse Political Viewpoints**|Dominik Stammbach et.al.|[2406.14155v1](http://arxiv.org/abs/2406.14155v1)|null|
|**2024-06-20**|**Finding Safety Neurons in Large Language Models**|Jianhui Chen et.al.|[2406.14144v1](http://arxiv.org/abs/2406.14144v1)|null|
|**2024-06-20**|**Online Learning of Weakly Coupled MDP Policies for Load Balancing and Auto Scaling**|S. R. Eshwar et.al.|[2406.14141v1](http://arxiv.org/abs/2406.14141v1)|null|
|**2024-06-20**|**MACAROON: Training Vision-Language Models To Be Your Engaged Partners**|Shujin Wu et.al.|[2406.14137v1](http://arxiv.org/abs/2406.14137v1)|null|
|**2024-06-20**|**Enhancing Monotonic Modeling with Spatio-Temporal Adaptive Awareness in Diverse Marketing**|Bin Li et.al.|[2406.14132v1](http://arxiv.org/abs/2406.14132v1)|null|
|**2024-06-20**|**Towards Event-oriented Long Video Understanding**|Yifan Du et.al.|[2406.14129v1](http://arxiv.org/abs/2406.14129v1)|[link](https://github.com/rucaibox/event-bench)|
|**2024-06-20**|**Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective**|Minsang Kim et.al.|[2406.14124v1](http://arxiv.org/abs/2406.14124v1)|null|
|**2024-06-20**|**EduQate: Generating Adaptive Curricula through RMABs in Education Settings**|Sidney Tio et.al.|[2406.14122v1](http://arxiv.org/abs/2406.14122v1)|null|
|**2024-06-20**|**An Investigation of Prompt Variations for Zero-shot LLM-based Rankers**|Shuoqi Sun et.al.|[2406.14117v1](http://arxiv.org/abs/2406.14117v1)|null|
|**2024-06-20**|**Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models**|Ziche Liu et.al.|[2406.14115v1](http://arxiv.org/abs/2406.14115v1)|null|
|**2024-06-20**|**EasyECR: A Library for Easy Implementation and Evaluation of Event Coreference Resolution Models**|Yuncong Li et.al.|[2406.14106v1](http://arxiv.org/abs/2406.14106v1)|null|

#### Abstracts
##### **Model Merging and Safety Alignment: One Bad Model Spoils the Bunch**
2406.14563v1 by Hasan Abed Al Kader Hammoud, Umberto Michieli, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem, Mete Ozay

Merging Large Language Models (LLMs) is a cost-effective technique for
combining multiple expert LLMs into a single versatile model, retaining the
expertise of the original ones. However, current approaches often overlook the
importance of safety alignment during merging, leading to highly misaligned
models. This work investigates the effects of model merging on alignment. We
evaluate several popular model merging techniques, demonstrating that existing
methods do not only transfer domain expertise but also propagate misalignment.
We propose a simple two-step approach to address this problem: (i) generating
synthetic safety and domain-specific data, and (ii) incorporating these
generated data into the optimization process of existing data-aware model
merging techniques. This allows us to treat alignment as a skill that can be
maximized in the resulting merged LLM. Our experiments illustrate the
effectiveness of integrating alignment-related data during merging, resulting
in models that excel in both domain expertise and alignment.

摘要：大型語言模型 (LLM) 合併是一種經濟實惠的技術，可將多個專家級 LLM 合併成一個用途廣泛的模型，同時保留原始模型的專業知識。然而，目前的做法通常忽略合併期間安全比對的重要性，導致模型高度失準。這項工作探討了模型合併對比對的影響。我們評估了幾種流行的模型合併技術，證明現有方法不僅會傳遞領域專業知識，還會傳播失準。我們提出了一個簡單的兩步驟方法來解決這個問題：(i) 生成合成安全和特定領域的數據，以及 (ii) 將這些生成的數據納入現有的數據感知模型合併技術的最佳化過程中。這讓我們可以將比對視為一種可以在合併後的 LLM 中最大化的技能。我們的實驗說明了在合併過程中整合與比對相關的數據的有效性，從而產生在領域專業知識和比對方面都表現出色的模型。

##### **Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities**
2406.14562v1 by Sachit Menon, Richard Zemel, Carl Vondrick

When presented with questions involving visual thinking, humans naturally
switch reasoning modalities, often forming mental images or drawing visual
aids. Large language models have shown promising results in arithmetic and
symbolic reasoning by expressing intermediate reasoning in text as a chain of
thought, yet struggle to extend this capability to answer text queries that are
easily solved by visual reasoning, even with extensive multimodal pretraining.
We introduce a simple method, whiteboard-of-thought prompting, to unlock the
visual reasoning capabilities of multimodal large language models across
modalities. Whiteboard-of-thought prompting provides multimodal large language
models with a metaphorical `whiteboard' to draw out reasoning steps as images,
then returns these images back to the model for further processing. We find
this can be accomplished with no demonstrations or specialized modules, instead
leveraging models' existing ability to write code with libraries such as
Matplotlib and Turtle. This simple approach shows state-of-the-art results on
four difficult natural language tasks that involve visual and spatial
reasoning. We identify multiple settings where GPT-4o using chain-of-thought
fails dramatically, including more than one where it achieves $0\%$ accuracy,
while whiteboard-of-thought enables up to $92\%$ accuracy in these same
settings. We present a detailed exploration of where the technique succeeds as
well as its sources of error.

摘要：當面臨涉及視覺思考的問題時，人類自然會轉換推理模式，通常會形成心像或繪製視覺輔助工具。大型語言模型在算術和符號推理方面表現出令人滿意的結果，透過將中間推理過程表述為文字，形成一連串的思考，但仍難以將此能力擴展到回答透過視覺推理就能輕鬆解決的文字查詢，即使經過廣泛的多模態預訓練。我們提出了一種簡單的方法，即白板思考提示，以解鎖跨模態多模態大型語言模型的視覺推理能力。白板思考提示為多模態大型語言模型提供了一個隱喻的「白板」，用於將推理步驟繪製成影像，然後將這些影像傳回模型以進行進一步處理。我們發現這可以在沒有示範或專業模組的情況下完成，而是利用模型現有的能力，透過 Matplotlib 和 Turtle 等函式庫來撰寫程式碼。這種簡單的方法在四項涉及視覺和空間推理的困難自然語言任務上展現了最先進的結果。我們發現 GPT-4o 使用思考鏈時，在多種設定下會大幅失敗，其中有超過一種設定下準確率達到 0%，而白板思考在這些相同的設定下，準確率可達 92%。我們詳細探討了該技術成功的地方以及其錯誤來源。

##### **How to Compute the Probability of a Word**
2406.14561v1 by Tiago Pimentel, Clara Meister

Language models (LMs) estimate the probability distribution over sequences of
natural language; these distributions are crucial for computing perplexity and
surprisal in linguistics research. While we are usually concerned with
measuring these values for words, most LMs operate over subwords. Despite
seemingly straightforward, accurately computing probabilities over one unit
given probabilities over the other requires care. Indeed, we show here that
many recent linguistic studies have been incorrectly computing these values.
This paper derives the correct methods for computing word probabilities,
highlighting issues when relying on language models that use beginning-of-word
(bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that
correcting the widespread bug in probability computations affects measured
outcomes in sentence comprehension and lexical optimisation analyses.

摘要：語言模型 (LM) 估計自然語言序列的機率分佈；這些分佈對於計算語言學研究中的困惑度和驚奇度至關重要。雖然我們通常關注測量字詞的這些值，但大多數 LM 都是針對子字詞運作。儘管看似簡單，但要根據一個單位的機率準確計算另一個單位的機率需要小心。事實上，我們在此顯示許多最近的語言學研究都錯誤地計算了這些值。本文推導出計算字詞機率的正確方法，並強調依賴使用字首標記 (bow) 標記化器的語言模型（例如 GPT 系列）時的問題。根據經驗，我們表明修正機率計算中廣泛存在的錯誤會影響句子理解和詞彙最佳化分析中測量的結果。

##### **xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics**
2406.14553v1 by Daniil Larionov, Mikhail Seleznyov, Vasiliy Viskov, Alexander Panchenko, Steffen Eger

State-of-the-art trainable machine translation evaluation metrics like xCOMET
achieve high correlation with human judgment but rely on large encoders (up to
10.7B parameters), making them computationally expensive and inaccessible to
researchers with limited resources. To address this issue, we investigate
whether the knowledge stored in these large encoders can be compressed while
maintaining quality. We employ distillation, quantization, and pruning
techniques to create efficient xCOMET alternatives and introduce a novel data
collection pipeline for efficient black-box distillation. Our experiments show
that, using quantization, xCOMET can be compressed up to three times with no
quality degradation. Additionally, through distillation, we create an
xCOMET-lite metric, which has only 2.6% of xCOMET-XXL parameters, but retains
92.1% of its quality. Besides, it surpasses strong small-scale metrics like
COMET-22 and BLEURT-20 on the WMT22 metrics challenge dataset by 6.4%, despite
using 50% fewer parameters. All code, dataset, and models are available online.

摘要：最先进的可训练机器翻译评估指标，如 xCOMET，与人类判断高度相关，但依赖于大型编码器（高达 10.7B 参数），这使得它们在计算上很昂贵，而且资源有限的研究人员无法使用。为了解决这个问题，我们调查了存储在这些大型编码器中的知识是否可以在保持质量的同时进行压缩。我们采用蒸馏、量化和剪枝技术来创建高效的 xCOMET 替代方案，并引入了一种新颖的数据收集管道，用于高效的黑盒蒸馏。我们的实验表明，使用量化，xCOMET 可以压缩到三倍，而不会降低质量。此外，通过蒸馏，我们创建了一个 xCOMET-lite 指标，它只有 xCOMET-XXL 参数的 2.6%，但保留了其 92.1% 的质量。此外，它在 WMT22 指标挑战数据集上超越了 COMET-22 和 BLEURT-20 等强大的小规模指标 6.4%，尽管使用较少 50% 的参数。所有代码、数据集和模型均在线提供。

##### **GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models**
2406.14550v1 by Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng

Long-context capabilities are essential for large language models (LLMs) to
tackle complex and long-input tasks. Despite numerous efforts made to optimize
LLMs for long contexts, challenges persist in robustly processing long inputs.
In this paper, we introduce GraphReader, a graph-based agent system designed to
handle long texts by structuring them into a graph and employing an agent to
explore this graph autonomously. Upon receiving a question, the agent first
undertakes a step-by-step analysis and devises a rational plan. It then invokes
a set of predefined functions to read node content and neighbors, facilitating
a coarse-to-fine exploration of the graph. Throughout the exploration, the
agent continuously records new insights and reflects on current circumstances
to optimize the process until it has gathered sufficient information to
generate an answer. Experimental results on the LV-Eval dataset reveal that
GraphReader, using a 4k context window, consistently outperforms GPT-4-128k
across context lengths from 16k to 256k by a large margin. Additionally, our
approach demonstrates superior performance on four challenging single-hop and
multi-hop benchmarks.

摘要：長語境能力對於大型語言模型 (LLM) 來說至關重要，可應對複雜且輸入長度較長的任務。儘管已針對 LLM 進行許多最佳化工作以應對長語境，但強健地處理長輸入仍存在挑戰。在本文中，我們介紹 GraphReader，一個基於圖表的代理系統，旨在透過將長文本結構化成一個圖表，並使用代理程式自主探索此圖表，來處理長文本。在收到問題後，代理程式首先進行逐步分析，並擬定一個合理計畫。然後，它會呼叫一組預定義的函式來讀取節點內容和鄰近節點，促進對圖表的粗略到精細探索。在整個探索過程中，代理程式會持續記錄新的見解，並反思當前情況，以最佳化處理程序，直到收集到足夠的資訊來產生答案。在 LV-Eval 資料集上的實驗結果顯示，GraphReader 使用 4k 語境視窗，在 16k 到 256k 的語境長度中，始終大幅優於 GPT-4-128k。此外，我們的做法在四個具有挑戰性的單跳和多跳基準測試中展現出卓越的效能。

##### **Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data**
2406.14546v1 by Johannes Treutlein, Dami Choi, Jan Betley, Cem Anil, Samuel Marks, Roger Baker Grosse, Owain Evans

One way to address safety risks from large language models (LLMs) is to
censor dangerous knowledge from their training data. While this removes the
explicit information, implicit information can remain scattered across various
training documents. Could an LLM infer the censored knowledge by piecing
together these implicit hints? As a step towards answering this question, we
study inductive out-of-context reasoning (OOCR), a type of generalization in
which LLMs infer latent information from evidence distributed across training
documents and apply it to downstream tasks without in-context learning. Using a
suite of five tasks, we demonstrate that frontier LLMs can perform inductive
OOCR. In one experiment we finetune an LLM on a corpus consisting only of
distances between an unknown city and other known cities. Remarkably, without
in-context examples or Chain of Thought, the LLM can verbalize that the unknown
city is Paris and use this fact to answer downstream questions. Further
experiments show that LLMs trained only on individual coin flip outcomes can
verbalize whether the coin is biased, and those trained only on pairs
$(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR
succeeds in a range of cases, we also show that it is unreliable, particularly
for smaller LLMs learning complex structures. Overall, the ability of LLMs to
"connect the dots" without explicit in-context learning poses a potential
obstacle to monitoring and controlling the knowledge acquired by LLMs.

摘要：解決大型語言模型 (LLM) 的安全風險的方法之一，就是從訓練資料中審查危險的知識。雖然這會移除明確的資訊，但隱含的資訊仍可能散落在各種訓練文件中。LLM 能否透過將這些隱含提示拼湊在一起，推論出經過審查的知識？為了回答這個問題，我們研究了歸納式非脈絡推理 (OOCR)，這是一種 LLM 從訓練文件中的證據中推論出潛在資訊，並將其應用於下游任務的概化類型，而無需進行脈絡學習。我們使用一套五個任務，證明了前沿 LLM 可以執行歸納式 OOCR。在一個實驗中，我們對 LLM 進行微調，使用僅包含未知城市與其他已知城市之間距離的語料庫。值得注意的是，LLM 在沒有脈絡範例或思考鏈的情況下，可以口述未知城市是巴黎，並利用這個事實來回答下游問題。進一步的實驗表明，僅在個別擲幣結果上訓練的 LLM 可以口述硬幣是否偏頗，而僅在成對 $(x,f(x))$ 上訓練的 LLM 可以說明 $f$ 的定義並計算逆函數。雖然 OOCR 在許多情況下都成功，但我們也表明它並不可靠，特別是對於學習複雜結構的較小 LLM。總的來說，LLM 在沒有明確脈絡學習的情況下「連接點」的能力，對監控和控制 LLM 獲得的知識構成潛在障礙。

##### **Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems**
2406.14545v1 by Đorđe Klisura, Anthony Rios

Relational databases are integral to modern information systems, serving as
the foundation for storing, querying, and managing data efficiently and
effectively. Advancements in large language modeling have led to the emergence
of text-to-SQL technologies, significantly enhancing the querying and
extracting of information from these databases and raising concerns about
privacy and security. Our research extracts the database schema elements
underlying a text-to-SQL model. Knowledge of the schema can make attacks such
as SQL injection easier. By asking specially crafted questions, we have
developed a zero-knowledge framework designed to probe various database schema
elements without knowledge of the database itself. The text-to-SQL models then
process these questions to produce an output that we use to uncover the
structure of the database schema. We apply it to specialized text-to-SQL models
fine-tuned on text-SQL pairs and generative language models used for SQL
generation. Overall, we can reconstruct the table names with an F1 of nearly
.75 for fine-tuned models and .96 for generative.

摘要：关系数据库是现代信息系统的组成部分，作为高效有效地存储、查询和管理数据的根基。大型语言模型的进步导致了文本到 SQL 技术的出现，显著地增强了从这些数据库中查询和提取信息的能力，并引发了对隐私和安全的担忧。我们的研究提取了文本到 SQL 模型中底层的数据库架构元素。架构知识可以使诸如 SQL 注入之类的攻击更容易。通过提出专门设计的问题，我们开发了一个零知识框架，旨在探测各种数据库架构元素，而无需了解数据库本身。然后，文本到 SQL 模型处理这些问题以产生我们用于揭示数据库架构结构的输出。我们将它应用于专门的文本到 SQL 模型，这些模型在文本-SQL 对和用于 SQL 生成的生成语言模型上进行了微调。总体而言，我们可以用 F1 接近 0.75 的微调模型和 0.96 的生成模型重建表名。

##### **Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs**
2406.14544v1 by Yuxuan Qiao, Haodong Duan, Xinyu Fang, Junming Yang, Lin Chen, Songyang Zhang, Jiaqi Wang, Dahua Lin, Kai Chen

Vision Language Models (VLMs) demonstrate remarkable proficiency in
addressing a wide array of visual questions, which requires strong perception
and reasoning faculties. Assessing these two competencies independently is
crucial for model refinement, despite the inherent difficulty due to the
intertwined nature of seeing and reasoning in existing VLMs. To tackle this
issue, we present Prism, an innovative framework designed to disentangle the
perception and reasoning processes involved in visual question solving. Prism
comprises two distinct stages: a perception stage that utilizes a VLM to
extract and articulate visual information in textual form, and a reasoning
stage that formulates responses based on the extracted visual information using
a Large Language Model (LLM). This modular design enables the systematic
comparison and assessment of both proprietary and open-source VLM for their
perception and reasoning strengths. Our analytical framework provides several
valuable insights, underscoring Prism's potential as a cost-effective solution
for vision-language tasks. By combining a streamlined VLM focused on perception
with a powerful LLM tailored for reasoning, Prism achieves superior results in
general vision-language tasks while substantially cutting down on training and
operational expenses. Quantitative evaluations show that Prism, when configured
with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on
par with VLMs $10 \times$ larger on the rigorous multimodal benchmark MMStar.
The project is released at: https://github.com/SparksJoe/Prism.

摘要：視覺語言模型 (VLM) 在回答各種視覺問題上展現出卓越的能力，這需要強大的感知和推理能力。儘管現有 VLM 中視覺和推理的本質上相互交織，導致難以獨立評估這兩種能力，但獨立評估這兩種能力對於模型改進至關重要。為了解決這個問題，我們提出了 Prism，一個創新的框架，旨在解開視覺問題求解中涉及的感知和推理過程。Prism 包含兩個不同的階段：一個感知階段，利用 VLM 以文字形式提取和表達視覺資訊，以及一個推理階段，使用大型語言模型 (LLM) 基於提取的視覺資訊制定回應。這種模組化設計可以系統地比較和評估專有和開源 VLM 的感知和推理優勢。我們的分析框架提供了幾個有價值的見解，強調了 Prism 作為視覺語言任務的經濟高效解決方案的潛力。Prism 結合了專注於感知的簡化 VLM 和專為推理量身打造的強大 LLM，在一般的視覺語言任務中取得了卓越的成果，同時大幅減少了訓練和營運費用。定量評估顯示，Prism 在與香草 2B LLaVA 和免費的 GPT-3.5 搭配使用時，在嚴格的多模態基準 MMStar 上的效能與大 $10 \times$ 的 VLM 相當。該專案已發布於：https://github.com/SparksJoe/Prism。

##### **IRASim: Learning Interactive Real-Robot Action Simulators**
2406.14540v1 by Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, Tao Kong

Scalable robot learning in the real world is limited by the cost and safety
issues of real robots. In addition, rolling out robot trajectories in the real
world can be time-consuming and labor-intensive. In this paper, we propose to
learn an interactive real-robot action simulator as an alternative. We
introduce a novel method, IRASim, which leverages the power of generative
models to generate extremely realistic videos of a robot arm that executes a
given action trajectory, starting from an initial given frame. To validate the
effectiveness of our method, we create a new benchmark, IRASim Benchmark, based
on three real-robot datasets and perform extensive experiments on the
benchmark. Results show that IRASim outperforms all the baseline methods and is
more preferable in human evaluations. We hope that IRASim can serve as an
effective and scalable approach to enhance robot learning in the real world. To
promote research for generative real-robot action simulators, we open-source
code, benchmark, and checkpoints at https: //gen-irasim.github.io.

摘要：現實世界的可擴充機器人學習受到實際機器人的成本和安全問題限制。此外，在現實世界中推出機器人軌跡可能耗時且費力。在本文中，我們建議學習互動式真實機器人動作模擬器作為替代方案。我們介紹了一種新方法 IRASim，它利用生成模型的力量來生成機器人手臂執行給定動作軌跡的極其逼真的影片，從給定的初始幀開始。為了驗證我們方法的有效性，我們基於三個真實機器人數據集創建了一個新的基準 IRASim Benchmark，並在基準上進行了廣泛的實驗。結果表明，IRASim 優於所有基準方法，並且在人類評估中更受青睞。我們希望 IRASim 能夠作為一種有效且可擴充的方法來增強現實世界中的機器人學習。為了促進生成式真實機器人動作模擬器的研究，我們在 https: //gen-irasim.github.io 公開原始碼、基準和檢查點。

##### **RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold**
2406.14532v1 by Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, Aviral Kumar

Training on model-generated synthetic data is a promising approach for
finetuning LLMs, but it remains unclear when it helps or hurts. In this paper,
we investigate this question for math reasoning via an empirical study,
followed by building a conceptual understanding of our observations. First, we
find that while the typical approach of finetuning a model on synthetic correct
or positive problem-solution pairs generated by capable models offers modest
performance gains, sampling more correct solutions from the finetuned learner
itself followed by subsequent fine-tuning on this self-generated data
$\textbf{doubles}$ the efficiency of the same synthetic problems. At the same
time, training on model-generated positives can amplify various spurious
correlations, resulting in flat or even inverse scaling trends as the amount of
data increases. Surprisingly, we find that several of these issues can be
addressed if we also utilize negative responses, i.e., model-generated
responses that are deemed incorrect by a final answer verifier. Crucially,
these negatives must be constructed such that the training can appropriately
recover the utility or advantage of each intermediate step in the negative
response. With this per-step scheme, we are able to attain consistent gains
over only positive data, attaining performance similar to amplifying the amount
of synthetic data by $\mathbf{8 \times}$. We show that training on per-step
negatives can help to unlearn spurious correlations in the positive data, and
is equivalent to advantage-weighted reinforcement learning (RL), implying that
it inherits robustness benefits of RL over imitating positive data alone.

摘要：<paragraph>利用模型生成合成資料進行訓練對於微調大型語言模型來說是一種很有前景的方法，但它在什麼時候有助益或有害仍然不清楚。在本文中，我們透過實證研究探討這個問題，並進一步建立對我們觀察結果的概念性理解。首先，我們發現，儘管在有能力的模型所產生的合成正確或正向問題解決配對上微調模型的典型方法提供了適度的效能提升，但從微調後的學習者本身取樣更多正確的解決方案，並在此自生資料上進行後續微調，會將相同合成問題的效率提升了一倍。同時，在模型產生的正向資料上進行訓練可能會放大各種虛假相關性，導致資料量增加時出現平坦甚至反向的擴充趨勢。令人驚訝的是，我們發現，如果我們也利用負面回應（即模型產生的、被最終答案驗證器視為不正確的回應），就能解決其中幾個問題。至關重要的是，這些負面回應必須建構得讓訓練能適當地恢復負面回應中每個中間步驟的效用或優勢。透過這個逐步驟的架構，我們能夠在僅有正向資料的基礎上持續獲得提升，達到類似於將合成資料量擴大 8 倍的效能。我們證明，在逐步驟的負面資料上進行訓練有助於取消正向資料中的虛假相關性，並且等同於優勢加權強化學習 (RL)，這表示它繼承了 RL 優於僅模仿正向資料的穩健性優勢。</paragraph>

##### **DeciMamba: Exploring the Length Extrapolation Potential of Mamba**
2406.14528v1 by Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, Raja Giryes

Long-range sequence processing poses a significant challenge for Transformers
due to their quadratic complexity in input length. A promising alternative is
Mamba, which demonstrates high performance and achieves Transformer-level
capabilities while requiring substantially fewer computational resources. In
this paper we explore the length-generalization capabilities of Mamba, which we
find to be relatively limited. Through a series of visualizations and analyses
we identify that the limitations arise from a restricted effective receptive
field, dictated by the sequence length used during training. To address this
constraint, we introduce DeciMamba, a context-extension method specifically
designed for Mamba. This mechanism, built on top of a hidden filtering
mechanism embedded within the S6 layer, enables the trained model to
extrapolate well even without additional training. Empirical experiments over
real-world long-range NLP tasks show that DeciMamba can extrapolate to context
lengths that are 25x times longer than the ones seen during training, and does
so without utilizing additional computational resources. We will release our
code and models.

摘要：長距離序列處理對於 Transformer 來說是一個重大的挑戰，因為它們的輸入長度二次複雜度。一個有前途的替代方案是 Mamba，它展示了高性能並達到了 Transformer 級別的能力，同時需要大幅減少的計算資源。在本文中，我們探討了 Mamba 的長度泛化能力，我們發現它相對有限。通過一系列的視覺化和分析，我們發現限制來自於受限的有效感受野，這是由訓練期間使用的序列長度決定的。為了解決這個限制，我們引入了 DeciMamba，這是一種專門為 Mamba 設計的上下文擴展方法。這種機制建立在嵌入在 S6 層中的隱藏過濾機制的基礎上，使訓練好的模型即使沒有額外的訓練也能很好地外推。在現實世界的長距離 NLP 任務上的經驗實驗表明，DeciMamba 可以外推到比訓練期間看到的長 25 倍的上下文長度，並且在不利用額外計算資源的情況下做到這一點。我們將發布我們的代碼和模型。

##### **Fantastic Copyrighted Beasts and How (Not) to Generate Them**
2406.14526v1 by Luxi He, Yangsibo Huang, Weijia Shi, Tinghao Xie, Haotian Liu, Yue Wang, Luke Zettlemoyer, Chiyuan Zhang, Danqi Chen, Peter Henderson

Recent studies show that image and video generation models can be prompted to
reproduce copyrighted content from their training data, raising serious legal
concerns around copyright infringement. Copyrighted characters, in particular,
pose a difficult challenge for image generation services, with at least one
lawsuit already awarding damages based on the generation of these characters.
Yet, little research has empirically examined this issue. We conduct a
systematic evaluation to fill this gap. First, we build CopyCat, an evaluation
suite consisting of diverse copyrighted characters and a novel evaluation
pipeline. Our evaluation considers both the detection of similarity to
copyrighted characters and generated image's consistency with user input. Our
evaluation systematically shows that both image and video generation models can
still generate characters even if characters' names are not explicitly
mentioned in the prompt, sometimes with only two generic keywords (e.g.,
prompting with "videogame, plumber" consistently generates Nintendo's Mario
character). We then introduce techniques to semi-automatically identify such
keywords or descriptions that trigger character generation. Using our
evaluation suite, we study runtime mitigation strategies, including both
existing methods and new strategies we propose. Our findings reveal that
commonly employed strategies, such as prompt rewriting in the DALL-E system,
are not sufficient as standalone guardrails. These strategies must be coupled
with other approaches, like negative prompting, to effectively reduce the
unintended generation of copyrighted characters. Our work provides empirical
grounding to the discussion of copyright mitigation strategies and offers
actionable insights for model deployers actively implementing them.

摘要：最近的研究顯示，影像和影片生成模型會受到提示，從其訓練資料中複製受版權保護的內容，這引發了有關版權侵權的嚴重法律問題。受版權保護的角色，特別對影像生成服務構成嚴峻挑戰，至少有一起訴訟已針對這些角色的生成判決賠償。然而，很少有研究對此問題進行實證檢視。我們進行系統性評估以填補此一空白。首先，我們建立 CopyCat，這是一個評估套件，包含各種受版權保護的角色和一個新穎的評估管道。我們的評估考量因素包括偵測與受版權保護角色的相似性，以及生成的影像是否符合使用者的輸入。我們的評估系統性地顯示，即使提示中未明確提到角色名稱，影像和影片生成模型仍可以生成角色，有時僅使用兩個通用關鍵字（例如，提示「電玩、水管工」會持續生成任天堂的瑪利歐角色）。然後，我們引入技術來半自動識別觸發角色生成的關鍵字或描述。使用我們的評估套件，我們研究執行時期的緩解策略，包括現有方法和我們提出的新策略。我們的研究結果顯示，常見的策略（例如 DALL-E 系統中的提示重寫）並不足以作為獨立的防護措施。這些策略必須與其他方法（例如負面提示）結合，才能有效減少受版權保護角色的意外生成。我們的研究為版權緩解策略的討論提供了實證基礎，並為積極實施這些策略的模型部署者提供可行的見解。

##### **PostMark: A Robust Blackbox Watermark for Large Language Models**
2406.14517v1 by Yapei Chang, Kalpesh Krishna, Amir Houmansadr, John Wieting, Mohit Iyyer

The most effective techniques to detect LLM-generated text rely on inserting
a detectable signature -- or watermark -- during the model's decoding process.
Most existing watermarking methods require access to the underlying LLM's
logits, which LLM API providers are loath to share due to fears of model
distillation. As such, these watermarks must be implemented independently by
each LLM provider. In this paper, we develop PostMark, a modular post-hoc
watermarking procedure in which an input-dependent set of words (determined via
a semantic embedding) is inserted into the text after the decoding process has
completed. Critically, PostMark does not require logit access, which means it
can be implemented by a third party. We also show that PostMark is more robust
to paraphrasing attacks than existing watermarking methods: our experiments
cover eight baseline algorithms, five base LLMs, and three datasets. Finally,
we evaluate the impact of PostMark on text quality using both automated and
human assessments, highlighting the trade-off between quality and robustness to
paraphrasing. We release our code, outputs, and annotations at
https://github.com/lilakk/PostMark.

摘要：最有效的偵測 LLM 生成的文字技術，是於模型解碼過程中插入可偵測的簽名或浮水印。
現有的浮水印方法大多需要存取 LLM 的底層 logit，而 LLM API 供應商基於對模型萃取的疑慮，不願意分享 logit。因此，這些浮水印必須由各個 LLM 供應商獨立實作。在本文中，我們開發了 PostMark，這是一個模組化的後設浮水印程序，其中一組與輸入相關的字詞（透過語意嵌入來決定）會在解碼程序完成後插入文字中。重要的是，PostMark 不需要存取 logit，這表示它可以由第三方實作。我們也證明 PostMark 比現有的浮水印方法更能抵抗同義詞改寫攻擊：我們的實驗涵蓋八個基準演算法、五個基礎 LLM 和三個資料集。最後，我們使用自動化和人工評估來評估 PostMark 對文字品質的影響，強調品質和對同義詞改寫的抵抗力之間的權衡。我們在 https://github.com/lilakk/PostMark 釋出我們的程式碼、輸出和註解。

##### **Investigating Mysteries of CoT-Augmented Distillation**
2406.14511v1 by Somin Wadhwa, Silvio Amir, Byron C. Wallace

Eliciting "chain of thought" (CoT) rationales -- sequences of token that
convey a "reasoning" process -- has been shown to consistently improve LLM
performance on tasks like question answering. More recent efforts have shown
that such rationales can also be used for model distillation: Including CoT
sequences (elicited from a large "teacher" model) in addition to target labels
when fine-tuning a small student model yields (often substantial) improvements.
In this work we ask: Why and how does this additional training signal help in
model distillation? We perform ablations to interrogate this, and report some
potentially surprising results. Specifically: (1) Placing CoT sequences after
labels (rather than before) realizes consistently better downstream performance
-- this means that no student "reasoning" is necessary at test time to realize
gains. (2) When rationales are appended in this way, they need not be coherent
reasoning sequences to yield improvements; performance increases are robust to
permutations of CoT tokens, for example. In fact, (3) a small number of key
tokens are sufficient to achieve improvements equivalent to those observed when
full rationales are used in model distillation.

摘要：<paragraph>引出“思维链”(CoT) 的基本原理——表达“推理”过程的标记序列——已被证明可以持续改善 LLM 在问答等任务上的表现。最近的研究表明，此类基本原理也可用于模型精馏：在微调小型学生模型时，除了目标标签外，还加入 CoT 序列（从大型“教师”模型中引出）会产生（通常是实质性的）改进。在这项工作中，我们提出以下问题：为什么以及如何这种额外的训练信号有助于模型精馏？我们执行消融以对此进行审问，并报告一些可能令人惊讶的结果。具体来说：(1) 将 CoT 序列置于标签之后（而不是之前）实现了始终如一地更好的下游性能——这意味着在测试时无需学生“推理”即可实现收益。(2) 当以这种方式附加基本原理时，它们不必是连贯的推理序列即可产生改进；例如，性能提升对 CoT 标记的排列是鲁棒的。(3) 事实上，少量关键标记足以实现与在模型精馏中使用完整基本原理时观察到的同等改进。</paragraph>

##### **V-LASIK: Consistent Glasses-Removal from Videos Using Synthetic Data**
2406.14510v1 by Rotem Shalev-Arkushin, Aharon Azulay, Tavi Halperin, Eitan Richardson, Amit H. Bermano, Ohad Fried

Diffusion-based generative models have recently shown remarkable image and
video editing capabilities. However, local video editing, particularly removal
of small attributes like glasses, remains a challenge. Existing methods either
alter the videos excessively, generate unrealistic artifacts, or fail to
perform the requested edit consistently throughout the video. In this work, we
focus on consistent and identity-preserving removal of glasses in videos, using
it as a case study for consistent local attribute removal in videos. Due to the
lack of paired data, we adopt a weakly supervised approach and generate
synthetic imperfect data, using an adjusted pretrained diffusion model. We show
that despite data imperfection, by learning from our generated data and
leveraging the prior of pretrained diffusion models, our model is able to
perform the desired edit consistently while preserving the original video
content. Furthermore, we exemplify the generalization ability of our method to
other local video editing tasks by applying it successfully to facial
sticker-removal. Our approach demonstrates significant improvement over
existing methods, showcasing the potential of leveraging synthetic data and
strong video priors for local video editing tasks.

摘要：基於擴散的生成模型最近展示了卓越的影像和影片編輯功能。然而，局部影片編輯，特別是移除眼鏡等小屬性，仍然是一項挑戰。現有方法會過度改變影片、產生不切實際的人工製品，或無法在整個影片中一致地執行要求的編輯。在這項工作中，我們專注於在影片中一致且保留身分地移除眼鏡，並將其作為影片中一致移除局部屬性的案例研究。由於缺乏配對資料，我們採用弱監督方法，並使用調整後的預訓練擴散模型來產生合成的不完美資料。我們展示了儘管資料不完美，但透過從我們產生的資料中學習並利用預訓練擴散模型的先驗，我們的模型能夠在保留原始影片內容的同時，一致地執行所需的編輯。此外，我們以成功應用於移除臉部貼紙來舉例說明我們的方法對其他局部影片編輯任務的泛化能力。我們的做法展示了利用合成資料和強大的影片先驗來進行局部影片編輯任務的潛力，大幅改善了現有方法。

##### **Evidence of a log scaling law for political persuasion with large language models**
2406.14508v1 by Kobi Hackenburg, Ben M. Tappin, Paul Röttger, Scott Hale, Jonathan Bright, Helen Margetts

Large language models can now generate political messages as persuasive as
those written by humans, raising concerns about how far this persuasiveness may
continue to increase with model size. Here, we generate 720 persuasive messages
on 10 U.S. political issues from 24 language models spanning several orders of
magnitude in size. We then deploy these messages in a large-scale randomized
survey experiment (N = 25,982) to estimate the persuasive capability of each
model. Our findings are twofold. First, we find evidence of a log scaling law:
model persuasiveness is characterized by sharply diminishing returns, such that
current frontier models are barely more persuasive than models smaller in size
by an order of magnitude or more. Second, mere task completion (coherence,
staying on topic) appears to account for larger models' persuasive advantage.
These findings suggest that further scaling model size will not much increase
the persuasiveness of static LLM-generated messages.

摘要：大型語言模型現在可以產生像人類寫的那樣具有說服力的政治訊息，這引發了人們對這種說服力隨著模型規模的增長可能會增加到何種程度的擔憂。在這裡，我們從 24 個語言模型中生成了 720 條關於 10 個美國政治問題的具有說服力的訊息，這些模型的大小跨越了幾個數量級。然後，我們在一個大規模的隨機調查實驗 (N = 25,982) 中部署這些訊息，以估計每個模型的說服能力。我們的發現有兩個方面。首先，我們發現了對數縮放定律的證據：模型說服力具有急劇遞減的回報，以至於當前的邊界模型比小一個數量級或更多的模型幾乎沒有說服力。其次，僅僅完成任務（連貫性，堅持主題）似乎可以解釋大型模型的說服力優勢。這些發現表明，進一步擴展模型規模不會顯著提高靜態 LLM 生成的訊息的說服力。

##### **On Newton's Method to Unlearn Neural Networks**
2406.14507v1 by Nhung Bui, Xinyang Lu, See-Kiong Ng, Bryan Kian Hsian Low

Machine unlearning facilitates personal data ownership, including the ``right
to be forgotten''. The proliferation of applications of \emph{neural networks}
(NNs) trained on users' personal data calls for the need to develop algorithms
to unlearn an NN. Since retraining is costly, efficiency is often achieved
through approximate unlearning which aims to unlearn a trained NN to be close
to the retrained one (in distribution). Though the Newton's method has been
used by previous works to approximately unlearn linear models, adapting it for
unlearning an NN often encounters degenerate Hessians that make computing the
Newton's update impossible. In this paper, we will first show that when coupled
with naive yet often effective solutions to mitigate the degeneracy issue for
unlearning, the Newton's method surprisingly suffers from catastrophic
forgetting. To overcome this difficulty, we revise the Newton's method to
include a theoretically justified regularizer and propose a cubic-regularized
Newton's method for unlearning an NN. The cubic regularizer comes with the
benefits of not requiring manual finetuning and affording a natural
interpretation. Empirical evaluation on several models and real-world datasets
shows that our method is more resilient to catastrophic forgetting and performs
better than the baselines, especially in sequential unlearning.

摘要：機器遺忘有助於個人資料所有權，包括「被遺忘權」。訓練於使用者個人資料的「神經網路」（NN）應用激增，因此需要開發演算法來遺忘 NN。由於重新訓練成本高昂，因此效率通常透過近似遺忘來達成，其目標是遺忘已訓練的 NN 以接近重新訓練的 NN（在分佈上）。儘管先前的研究已使用牛頓法來近似遺忘線性模型，但將其調整為遺忘 NN 時，通常會遇到簡併海森矩陣，導致無法計算牛頓更新。在本文中，我們將首先展示，當與用於減輕遺忘簡併問題的天真但通常有效的方法結合時，牛頓法會意外地遭受災難性遺忘。為了克服這個困難，我們修改牛頓法，加入理論上合理的正則化項，並提出一個三次正則化牛頓法來遺忘 NN。三次正則化項的好處是不需要人工微調，並提供自然的詮釋。對多個模型和真實世界資料集的經驗評估顯示，我們的模型對災難性遺忘更有韌性，並且表現優於基準，特別是在序列遺忘中。

##### **Translating Across Cultures: LLMs for Intralingual Cultural Adaptation**
2406.14504v1 by Pushpdeep Singh, Mayur Patidar, Lovekesh Vig

LLMs are increasingly being deployed for multilingual applications and have
demonstrated impressive translation capabilities between several low and high
resource languages. An aspect of translation that often gets overlooked is that
of cultural adaptation, or modifying source culture references to suit the
target culture. Cultural adaptation has applications across several creative
industries and requires intimate knowledge of source and target cultures during
translation. While specialized translation models still outperform LLMs on the
machine translation task when viewed from the lens of correctness, they are not
sensitive to cultural differences often requiring manual correction. LLMs on
the other hand have a rich reservoir of cultural knowledge embedded within its
parameters that can be potentially exploited for such applications. In this
paper we define the task of cultural adaptation and create an evaluation
framework to benchmark different models for this task. We evaluate the
performance of modern LLMs for cultural adaptation and analyze their cross
cultural knowledge while connecting related concepts across different cultures.
We also analyze possible issues with automatic adaptation including cultural
biases and stereotypes. We hope that this task will offer more insight into the
cultural understanding of LLMs and their creativity in cross-cultural
scenarios.

摘要：大型語言模型正越來越廣泛地部署於多語言應用程式中，並已展現出令人印象深刻的翻譯能力，支援多種低資源和高資源語言。翻譯中經常被忽略的一項面向是文化改編，或修改原始文化的參照以符合目標文化。文化改編在多個創意產業中都有應用，且在翻譯過程中需要對原始文化和目標文化有深入的了解。雖然從正確性的角度來看，專門的翻譯模型在機器翻譯任務上仍優於大型語言模型，但它們對文化差異不敏感，通常需要手動更正。另一方面，大型語言模型在其參數中蘊含豐富的文化知識儲備，可潛在用於此類應用程式。在本文中，我們定義文化改編任務，並建立一個評估架構來評量不同模型執行此任務的表現。我們評估現代大型語言模型在文化改編方面的表現，並分析它們的跨文化知識，同時連結不同文化中的相關概念。我們也分析自動改編可能出現的問題，包括文化偏見和刻板印象。我們希望此任務能更深入了解大型語言模型的文化理解，以及它們在跨文化情境中的創造力。

##### **Overview of the CAIL 2023 Argument Mining Track**
2406.14503v1 by Jingcong Liang, Junlong Wang, Xinyu Zhai, Yungui Zhuang, Yiyang Zheng, Xin Xu, Xiandong Ran, Xiaozheng Dong, Honghui Rong, Yanlun Liu, Hao Chen, Yuhan Wei, Donghai Li, Jiajie Peng, Xuanjing Huang, Chongde Shi, Yansong Feng, Yun Song, Zhongyu Wei

We give a detailed overview of the CAIL 2023 Argument Mining Track, one of
the Chinese AI and Law Challenge (CAIL) 2023 tracks. The main goal of the track
is to identify and extract interacting argument pairs in trial dialogs. It
mainly uses summarized judgment documents but can also refer to trial
recordings. The track consists of two stages, and we introduce the tasks
designed for each stage; we also extend the data from previous events into a
new dataset -- CAIL2023-ArgMine -- with annotated new cases from various causes
of action. We outline several submissions that achieve the best results,
including their methods for different stages. While all submissions rely on
language models, they have incorporated strategies that may benefit future work
in this field.

摘要：我們提供了 CAIL 2023 論證挖掘軌道的詳細概述，這是中國人工智能與法律挑戰 (CAIL) 2023 軌道之一。軌道的目標是識別和提取審判對話中的互動論證對。它主要使用總結的判決文件，但也可以參考審判記錄。軌道包含兩個階段，我們介紹了為每個階段設計的任務；我們還將以前活動的數據擴展到一個新的數據集——CAIL2023-ArgMine——其中包含來自各種訴因的註釋新案例。我們概述了取得最佳結果的幾個提交，包括它們在不同階段的方法。雖然所有提交都依賴於語言模型，但它們已納入可能有利於該領域未來工作的策略。

##### **Improving Expert Radiology Report Summarization by Prompting Large Language Models with a Layperson Summary**
2406.14500v1 by Xingmeng Zhao, Tongnian Wang, Anthony Rios

Radiology report summarization (RRS) is crucial for patient care, requiring
concise "Impressions" from detailed "Findings." This paper introduces a novel
prompting strategy to enhance RRS by first generating a layperson summary. This
approach normalizes key observations and simplifies complex information using
non-expert communication techniques inspired by doctor-patient interactions.
Combined with few-shot in-context learning, this method improves the model's
ability to link general terms to specific findings. We evaluate this approach
on the MIMIC-CXR, CheXpert, and MIMIC-III datasets, benchmarking it against
7B/8B parameter state-of-the-art open-source large language models (LLMs) like
Meta-Llama-3-8B-Instruct. Our results demonstrate improvements in summarization
accuracy and accessibility, particularly in out-of-domain tests, with
improvements as high as 5% for some metrics.

摘要：放射科报告摘要 (RRS) 对于患者护理至关重要，需要从详细的「结果」中提取简洁的「印象」。本文介绍了一种新颖的提示策略，通过首先生成外行摘要来增强 RRS。此方法使用受医患互动启发的非专家沟通技巧，将关键观察结果标准化并简化复杂信息。结合少样本上下文学习，此方法提高了模型将一般术语与特定结果联系起来的能力。我们在 MIMIC-CXR、CheXpert 和 MIMIC-III 数据集上评估了这种方法，并将其与 Meta-Llama-3-8B-Instruct 等 7B/8B 参数最先进的开源大语言模型 (LLM) 进行了基准测试。我们的结果表明，摘要准确性和可访问性都有所提高，尤其是在域外测试中，某些指标的改进幅度高达 5%。

##### **LLaSA: Large Multimodal Agent for Human Activity Analysis Through Wearable Sensors**
2406.14498v1 by Sheikh Asif Imran, Mohammad Nur Hossain Khan, Subrata Biswas, Bashima Islam

Integrating inertial measurement units (IMUs) with large language models
(LLMs) advances multimodal AI by enhancing human activity understanding. We
introduce SensorCaps, a dataset of 26,288 IMU-derived activity narrations, and
OpenSQA, an instruction-following dataset with 257,562 question-answer pairs.
Combining LIMU-BERT and Llama, we develop LLaSA, a Large Multimodal Agent
capable of interpreting and responding to activity and motion analysis queries.
Our evaluation demonstrates LLaSA's effectiveness in activity classification
and question answering, highlighting its potential in healthcare, sports
science, and human-computer interaction. These contributions advance
sensor-aware language models and open new research avenues. Our code repository
and datasets can be found on https://github.com/BASHLab/LLaSA.

摘要：整合慣性測量單元 (IMU) 與大型語言模型 (LLM) 透過加強人類活動理解來推進多模式 AI。我們推出 SensorCaps，一個包含 26,288 個 IMU 衍生活動敘述的資料集，以及 OpenSQA，一個包含 257,562 個問答對的指令遵循資料集。結合 LIMU-BERT 和 Llama，我們開發了 LLaSA，一個大型多模式代理，能夠解釋和回應活動和動作分析查詢。我們的評估證明了 LLaSA 在活動分類和問題解答方面的有效性，突顯了其在醫療保健、運動科學和人機互動方面的潛力。這些貢獻推動了感測器感知語言模型的發展，並開啟了新的研究途徑。我們的程式碼儲存庫和資料集可以在 https://github.com/BASHLab/LLaSA 找到。

##### **CodeRAG-Bench: Can Retrieval Augment Code Generation?**
2406.14497v1 by Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing Xie, Graham Neubig, Daniel Fried

While language models (LMs) have proven remarkably adept at generating code,
many programs are challenging for LMs to generate using their parametric
knowledge alone. Providing external contexts such as library documentation can
facilitate generating accurate and functional code. Despite the success of
retrieval-augmented generation (RAG) in various text-oriented tasks, its
potential for improving code generation remains under-explored. In this work,
we conduct a systematic, large-scale analysis by asking: in what scenarios can
retrieval benefit code generation models? and what challenges remain? We first
curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three
categories of code generation tasks, including basic programming, open-domain,
and repository-level problems. We aggregate documents from five sources for
models to retrieve contexts: competition solutions, online tutorials, library
documentation, StackOverflow posts, and GitHub repositories. We examine
top-performing models on CodeRAG-Bench by providing contexts retrieved from one
or multiple sources. While notable gains are made in final code generation by
retrieving high-quality contexts across various settings, our analysis reveals
room for improvement -- current retrievers still struggle to fetch useful
contexts especially with limited lexical overlap, and generators fail to
improve with limited context lengths or abilities to integrate additional
contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage
further development of advanced code-oriented RAG methods.

摘要：儘管語言模型 (LM) 已證明在生成程式碼方面非常熟練，但許多程式對 LM 而言，僅使用其參數化知識生成程式碼仍具挑戰性。提供外部內容（例如函式庫文件）有助於產生準確且具功能性的程式碼。儘管檢索增強生成 (RAG) 在各種以文字為導向的任務中獲得成功，但其改善程式碼生成的潛力仍有待探索。在這項工作中，我們透過提問進行系統性、大規模的分析：在哪些情況下檢索有助於程式碼生成模型？還有哪些挑戰？我們首先策劃了一個全面的評量基準 CodeRAG-Bench，其中包含三類程式碼生成任務，包括基本程式設計、開放領域和儲存庫級別問題。我們從五個來源彙總文件，供模型檢索內容：競賽解答、線上教學、函式庫文件、StackOverflow 文章和 GitHub 儲存庫。我們透過提供從一個或多個來源檢索的內容，檢視 CodeRAG-Bench 上效能最佳的模型。儘管透過在各種設定中檢索高品質內容，在最終程式碼生成方面取得顯著進展，但我們的分析顯示仍有進步空間——目前的檢索器仍難以擷取有用的內容，特別是在詞彙重疊有限的情況下，而且生成器無法在內容長度有限或整合額外內容的能力下進行改善。我們希望 CodeRAG-Bench 能作為一個有效的測試平台，以鼓勵進一步開發進階的程式碼導向 RAG 方法。

##### **African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification**
2406.14496v1 by Gregor Geigle, Radu Timofte, Goran Glavaš

Recent Large Vision-Language Models (LVLMs) demonstrate impressive abilities
on numerous image understanding and reasoning tasks. The task of fine-grained
object classification (e.g., distinction between \textit{animal species}),
however, has been probed insufficiently, despite its downstream importance. We
fill this evaluation gap by creating \texttt{FOCI} (\textbf{F}ine-grained
\textbf{O}bject \textbf{C}lass\textbf{I}fication), a difficult multiple-choice
benchmark for fine-grained object classification, from existing object
classification datasets: (1) multiple-choice avoids ambiguous answers
associated with casting classification as open-ended QA task; (2) we retain
classification difficulty by mining negative labels with a CLIP model.
\texttt{FOCI}\xspace complements five popular classification datasets with four
domain-specific subsets from ImageNet-21k. We benchmark 12 public LVLMs on
\texttt{FOCI} and show that it tests for a \textit{complementary skill} to
established image understanding and reasoning benchmarks. Crucially, CLIP
models exhibit dramatically better performance than LVLMs. Since the image
encoders of LVLMs come from these CLIP models, this points to inadequate
alignment for fine-grained object distinction between the encoder and the LLM
and warrants (pre)training data with more fine-grained annotation. We release
our code at \url{https://github.com/gregor-ge/FOCI-Benchmark}.

摘要：<paragraph>最近的大型视觉语言模型 (LVLMs) 在许多图像理解和推理任务上表现出惊人的能力。然而，细粒度对象分类（例如，区分“动物种类”）的任务，尽管其下游重要性，但尚未得到充分探究。我们通过创建 \texttt{FOCI}（\textbf{F}ine-grained \textbf{O}bject \textbf{C}lass\textbf{I}fication），一个困难的多项选择基准，用于从现有对象分类数据集进行细粒度对象分类，来填补这一评估空白：（1）多项选择避免了将分类表述为开放式问答任务所带来的模棱两可的答案；（2）我们通过使用 CLIP 模型挖掘负标签来保留分类难度。\texttt{FOCI}\xspace 用 ImageNet-21k 中的四个特定领域的子集补充了五个流行的分类数据集。我们在 \texttt{FOCI} 上对 12 个公共 LVLMs 进行了基准测试，并表明它测试了一项“补充技能”，以建立图像理解和推理基准。至关重要的是，CLIP 模型表现出比 LVLMs 显著更好的性能。由于 LVLMs 的图像编码器来自这些 CLIP 模型，这表明编码器和 LLM 之间在细粒度对象区分方面存在不充分的对齐，并保证使用更细粒度的注释进行（预）训练数据。我们在 \url{https://github.com/gregor-ge/FOCI-Benchmark} 上发布了我们的代码。</paragraph>

##### **Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?**
2406.14492v1 by Gregor Geigle, Radu Timofte, Goran Glavaš

Large vision-language models (LVLMs) have recently dramatically pushed the
state of the art in image captioning and many image understanding tasks (e.g.,
visual question answering). LVLMs, however, often \textit{hallucinate} and
produce captions that mention concepts that cannot be found in the image. These
hallucinations erode the trustworthiness of LVLMs and are arguably among the
main obstacles to their ubiquitous adoption. Recent work suggests that addition
of grounding objectives -- those that explicitly align image regions or objects
to text spans -- reduces the amount of LVLM hallucination. Although intuitive,
this claim is not empirically justified as the reduction effects have been
established, we argue, with flawed evaluation protocols that (i) rely on data
(i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure
hallucination via question answering rather than open-ended caption generation.
In this work, in contrast, we offer the first systematic analysis of the effect
of fine-grained object grounding on LVLM hallucination under an evaluation
protocol that more realistically captures LVLM hallucination in open
generation. Our extensive experiments over three backbone LLMs reveal that
grounding objectives have little to no effect on object hallucination in open
caption generation.

摘要：大型視覺語言模型 (LVLMs) 最近在圖像標題和許多圖像理解任務（例如視覺問題回答）方面大幅提升了技術水準。然而，LVLMs 經常「產生幻覺」，並產生提及圖像中找不到概念的標題。這些幻覺會侵蝕 LVLMs 的可信度，而且可以說是它們廣泛採用的主要障礙之一。最近的研究表明，加入接地目標（那些將圖像區域或物件明確對齊到文字跨度）可以減少 LVLMs 幻覺的數量。儘管直觀，但此說法並未得到經驗證實，因為我們認為，這種減少效果已透過有缺陷的評估協定建立，該協定 (i) 依賴於 LVLMs 訓練中廣泛使用的資料（例如 MSCOCO），以及 (ii) 透過問題回答而不是開放式標題產生來衡量幻覺。相反地，在這項工作中，我們首次提供對精細物件接地對 LVLMs 幻覺影響的系統分析，其評估協定更真實地捕捉了開放式產生中的 LVLMs 幻覺。我們針對三個主幹 LLM 進行的廣泛實驗顯示，接地目標對開放式標題產生中的物件幻覺幾乎沒有影響。

##### **Instruction Pre-Training: Language Models are Supervised Multitask Learners**
2406.14491v1 by Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, Furu Wei

Unsupervised multitask pre-training has been the critical method behind the
recent success of language models (LMs). However, supervised multitask learning
still holds significant promise, as scaling it in the post-training stage
trends towards better generalization. In this paper, we explore supervised
multitask pre-training by proposing Instruction Pre-Training, a framework that
scalably augments massive raw corpora with instruction-response pairs to
pre-train LMs. The instruction-response pairs are generated by an efficient
instruction synthesizer built on open-source models. In our experiments, we
synthesize 200M instruction-response pairs covering 40+ task categories to
verify the effectiveness of Instruction Pre-Training. In pre-training from
scratch, Instruction Pre-Training not only consistently enhances pre-trained
base models but also benefits more from further instruction tuning. In
continual pre-training, Instruction Pre-Training enables Llama3-8B to be
comparable to or even outperform Llama3-70B. Our model, code, and data are
available at https://github.com/microsoft/LMOps.

摘要：無監督多任務預訓練一直是語言模型 (LM) 近期成功的關鍵方法。然而，監督式多任務學習仍具有顯著的潛力，因為在後訓練階段擴展它趨向於更好的概括能力。在本文中，我們透過提出指令預訓練來探索監督式多任務預訓練，這是一個透過指令回應配對可擴充地增加大量原始語料庫以預訓練 LM 的架構。指令回應配對是由建立在開源模型上的有效指令合成器產生的。在我們的實驗中，我們合成了涵蓋 40 多個任務類別的 2 億個指令回應配對，以驗證指令預訓練的有效性。在從頭開始預訓練時，指令預訓練不僅持續增強預訓練的基本模型，而且還能從進一步的指令微調中受益更多。在持續預訓練中，指令預訓練使 Llama3-8B 能夠與 Llama3-70B 相媲美甚至表現得更好。我們的模型、程式碼和資料可在 https://github.com/microsoft/LMOps 取得。

##### **Revealing Vision-Language Integration in the Brain with Multimodal Networks**
2406.14481v1 by Vighnesh Subramaniam, Colin Conwell, Christopher Wang, Gabriel Kreiman, Boris Katz, Ignacio Cases, Andrei Barbu

We use (multi)modal deep neural networks (DNNs) to probe for sites of
multimodal integration in the human brain by predicting stereoencephalography
(SEEG) recordings taken while human subjects watched movies. We operationalize
sites of multimodal integration as regions where a multimodal vision-language
model predicts recordings better than unimodal language, unimodal vision, or
linearly-integrated language-vision models. Our target DNN models span
different architectures (e.g., convolutional networks and transformers) and
multimodal training techniques (e.g., cross-attention and contrastive
learning). As a key enabling step, we first demonstrate that trained vision and
language models systematically outperform their randomly initialized
counterparts in their ability to predict SEEG signals. We then compare unimodal
and multimodal models against one another. Because our target DNN models often
have different architectures, number of parameters, and training sets (possibly
obscuring those differences attributable to integration), we carry out a
controlled comparison of two models (SLIP and SimCLR), which keep all of these
attributes the same aside from input modality. Using this approach, we identify
a sizable number of neural sites (on average 141 out of 1090 total sites or
12.94%) and brain regions where multimodal integration seems to occur.
Additionally, we find that among the variants of multimodal training techniques
we assess, CLIP-style training is the best suited for downstream prediction of
the neural activity in these sites.

摘要：<paragraph>我們使用（多）模態深度神經網路（DNN）透過預測人類受試者觀看電影時所進行的立體腦電圖（SEEG）紀錄，來探測人類大腦中多模態整合的部位。我們將多模態整合的部位操作化為多模態視覺語言模型預測紀錄優於單模態語言、單模態視覺或線性整合語言視覺模型的區域。我們的目標 DNN 模型涵蓋不同的架構（例如卷積網路和Transformer）和多模態訓練技術（例如交叉注意力和對比學習）。作為一個關鍵的啟用步驟，我們首先證明受過訓練的視覺和語言模型在預測 SEEG 訊號的能力上系統性地優於它們隨機初始化的對應模型。然後，我們將單模態和多模態模型相互比較。由於我們的目標 DNN 模型通常有不同的架構、參數數量和訓練集（可能模糊了那些可歸因於整合的差異），我們對兩個模型（SLIP 和 SimCLR）進行受控比較，除了輸入模態之外，它們保持所有這些屬性相同。使用這種方法，我們識別出大量的神經部位（平均 1090 個總部位中的 141 個或 12.94%）和看似發生多模態整合的大腦區域。此外，我們發現，在我們評估的多模態訓練技術變體中，CLIP 風格的訓練最適合預測這些部位的神經活動。</paragraph>

##### **On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier**
2406.14479v1 by Jiachen Jiang, Jinxin Zhou, Zhihui Zhu

Analyzing the similarity of internal representations within and across
different models has been an important technique for understanding the behavior
of deep neural networks. Most existing methods for analyzing the similarity
between representations of high dimensions, such as those based on Canonical
Correlation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),
rely on statistical properties of the representations for a set of data points.
In this paper, we focus on transformer models and study the similarity of
representations between the hidden layers of individual transformers. In this
context, we show that a simple sample-wise cosine similarity metric is capable
of capturing the similarity and aligns with the complicated CKA. Our
experimental results on common transformers reveal that representations across
layers are positively correlated, albeit the similarity decreases when layers
are far apart. We then propose an aligned training approach to enhance the
similarity between internal representations, with trained models that enjoy the
following properties: (1) the last-layer classifier can be directly applied
right after any hidden layers, yielding intermediate layer accuracies much
higher than those under standard training, (2) the layer-wise accuracies
monotonically increase and reveal the minimal depth needed for the given task,
(3) when served as multi-exit models, they achieve on-par performance with
standard multi-exit architectures which consist of additional classifiers
designed for early exiting in shallow layers. To our knowledge, our work is the
first to show that one common classifier is sufficient for multi-exit models.
We conduct experiments on both vision and NLP tasks to demonstrate the
performance of the proposed aligned training.

摘要：<paragraph>分析不同模型內部表示的相似性一直是理解深度神經網路行為的重要技術。大多數現有的分析高維度表示相似性的方法，例如基於典範相關分析 (CCA) 和廣泛使用的中心核對齊 (CKA) 的方法，依賴於一組資料點表示的統計性質。在本文中，我們專注於Transformer模型，並研究個別Transformer隱藏層之間表示的相似性。在此背景下，我們展示了一個簡單的樣本級餘弦相似性指標能夠捕捉相似性並與複雜的 CKA 保持一致。我們在常見Transformer上的實驗結果表明，跨層表示呈正相關，儘管在層相距甚遠時相似性會降低。然後我們提出了一種對齊訓練方法來增強內部表示之間的相似性，訓練後的模型具有以下特性：(1) 最後一層分類器可以直接應用於任何隱藏層之後，產生比標準訓練下高的中間層準確度，(2) 層級準確度單調增加，並揭示給定任務所需的最小深度，(3) 當作為多出口模型時，它們實現了與標準多出口架構同等的效能，後者由額外的分類器組成，這些分類器設計用於在淺層中提前退出。據我們所知，我們的研究首次表明一個通用分類器對於多出口模型就足夠了。我們在視覺和 NLP 任務上進行了實驗，以展示所提出的對齊訓練的效能。</paragraph>

##### **SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset**
2406.14477v1 by Josef Dai, Tianle Chen, Xuyao Wang, Ziran Yang, Taiye Chen, Jiaming Ji, Yaodong Yang

To mitigate the risk of harmful outputs from large vision models (LVMs), we
introduce the SafeSora dataset to promote research on aligning text-to-video
generation with human values. This dataset encompasses human preferences in
text-to-video generation tasks along two primary dimensions: helpfulness and
harmlessness. To capture in-depth human preferences and facilitate structured
reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and
harmlessness into 12 sub-categories, serving as the basis for pilot
annotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique
videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations
labeled by humans. We further demonstrate the utility of the SafeSora dataset
through several applications, including training the text-video moderation
model and aligning LVMs with human preference by fine-tuning a prompt
augmentation module or the diffusion model. These applications highlight its
potential as the foundation for text-to-video alignment research, such as human
preference modeling and the development and validation of alignment algorithms.

摘要：為了降低大型視覺模型 (LVM) 有害產出的風險，我們引入了 SafeSora 資料集，以促進文本轉影片生成與人類價值觀一致性的研究。此資料集包含文本轉影片生成任務中的人類偏好，沿著兩個主要面向：有幫助性和無害性。為了捕捉深入的人類偏好並促進群眾工作者的結構化推理，我們將有幫助性細分為 4 個子面向，將無害性細分為 12 個子類別，作為試驗標註的基礎。SafeSora 資料集包含 14,711 個獨特提示、4 個不同的 LVM 生成的 57,333 個獨特影片，以及人類標註的 51,691 對偏好標註。我們進一步透過多項應用展示 SafeSora 資料集的效用，包括訓練文本影片審核模型，以及透過微調提示擴充模組或擴散模型，使 LVM 與人類偏好一致。這些應用突顯了其作為文本轉影片對齊研究基礎的潛力，例如人類偏好建模和對齊演算法的開發與驗證。

##### **Data-Centric AI in the Age of Large Language Models**
2406.14473v1 by Xinyi Xu, Zhaoxuan Wu, Rui Qiao, Arun Verma, Yao Shu, Jingtan Wang, Xinyuan Niu, Zhenfeng He, Jiangwei Chen, Zijian Zhou, Gregory Kang Ruey Lau, Hieu Dao, Lucas Agussurja, Rachael Hwee Ling Sim, Xiaoqiang Lin, Wenyang Hu, Zhongxiang Dai, Pang Wei Koh, Bryan Kian Hsiang Low

This position paper proposes a data-centric viewpoint of AI research,
focusing on large language models (LLMs). We start by making the key
observation that data is instrumental in the developmental (e.g., pretraining
and fine-tuning) and inferential stages (e.g., in-context learning) of LLMs,
and yet it receives disproportionally low attention from the research
community. We identify four specific scenarios centered around data, covering
data-centric benchmarks and data curation, data attribution, knowledge
transfer, and inference contextualization. In each scenario, we underscore the
importance of data, highlight promising research directions, and articulate the
potential impacts on the research community and, where applicable, the society
as a whole. For instance, we advocate for a suite of data-centric benchmarks
tailored to the scale and complexity of data for LLMs. These benchmarks can be
used to develop new data curation methods and document research efforts and
results, which can help promote openness and transparency in AI and LLM
research.

摘要：這篇立場文件提出以資料為中心的 AI 研究觀點，
專注於大型語言模型 (LLM)。我們首先提出關鍵
觀察，即資料是 LLM 的開發（例如，預訓練
和微調）和推理階段（例如，情境學習）中的工具，
然而它卻受到研究
社群不成比例的低關注。我們找出四個特定情境圍繞資料，涵蓋
以資料為中心的基準和資料策展、資料歸因、知識
轉移和推理情境化。在每個情境中，我們強調資料的重要性，強調有前途的研究方向，並闡明對研究社群的潛在影響，以及在適用的情況下，對整個社會的影響。例如，我們提倡一套以資料為中心的基準，針對 LLM 的資料規模和複雜性量身打造。這些基準可用於開發新的資料策展方法，並記錄研究工作和成果，這有助於促進 AI 和 LLM 研究的開放性和透明度。

##### **Fusion of Movement and Naive Predictions for Point Forecasting in Univariate Random Walks**
2406.14469v1 by Cheng Zhang

Traditional methods for point forecasting in univariate random walks often
fail to surpass naive benchmarks due to data unpredictability. This study
introduces a novel forecasting method that fuses movement prediction (binary
classification) with naive forecasts for accurate one-step-ahead point
forecasting. The method's efficacy is demonstrated through theoretical
analysis, simulations, and real-world data experiments. It reliably exceeds
naive forecasts with movement prediction accuracies as low as 0.55,
outperforming baseline models like ARIMA, linear regression, MLP, and LSTM
networks in forecasting the S\&P 500 index and Bitcoin prices. This method is
particularly advantageous when accurate point predictions are challenging but
accurate movement predictions are attainable, translating movement predictions
into point forecasts in random walk contexts.

摘要：傳統的單變數隨機漫步點預測方法由於資料不可預測性，通常無法超越樸素基準。本研究提出了一種新的預測方法，將移動預測（二元分類）與樸素預測融合，以進行準確的一步預測點預測。該方法的有效性透過理論分析、模擬和真實世界資料實驗得到證明。它可靠地超過了樸素預測，移動預測準確度低至 0.55，在預測標準普爾 500 指數和比特幣價格方面優於 ARIMA、線性回歸、MLP 和 LSTM 網路等基準模型。當準確的點預測具有挑戰性，但準確的移動預測可以實現時，此方法特別有優勢，將移動預測轉換為隨機漫步環境中的點預測。

##### **Explicit and Implicit Large Language Model Personas Generate Opinions but Fail to Replicate Deeper Perceptions and Biases**
2406.14462v1 by Salvatore Giorgi, Tingting Liu, Ankit Aich, Kelsey Isman, Garrick Sherman, Zachary Fried, João Sedoc, Lyle H. Ungar, Brenda Curtis

Large language models (LLMs) are increasingly being used in human-centered
social scientific tasks, such as data annotation, synthetic data creation, and
engaging in dialog. However, these tasks are highly subjective and dependent on
human factors, such as one's environment, attitudes, beliefs, and lived
experiences. Thus, employing LLMs (which do not have such human factors) in
these tasks may result in a lack of variation in data, failing to reflect the
diversity of human experiences. In this paper, we examine the role of prompting
LLMs with human-like personas and asking the models to answer as if they were a
specific human. This is done explicitly, with exact demographics, political
beliefs, and lived experiences, or implicitly via names prevalent in specific
populations. The LLM personas are then evaluated via (1) subjective annotation
task (e.g., detecting toxicity) and (2) a belief generation task, where both
tasks are known to vary across human factors. We examine the impact of explicit
vs. implicit personas and investigate which human factors LLMs recognize and
respond to. Results show that LLM personas show mixed results when reproducing
known human biases, but generate generally fail to demonstrate implicit biases.
We conclude that LLMs lack the intrinsic cognitive mechanisms of human thought,
while capturing the statistical patterns of how people speak, which may
restrict their effectiveness in complex social science applications.

摘要：大型語言模型 (LLM) 愈來愈常在以人為中心的社會科學任務中使用，例如資料標註、合成資料建立，以及參與對話。然而，這些任務高度主觀，且依賴於環境、態度、信念和生活經驗等人類因素。因此，在這些任務中採用 LLM（沒有這些人類因素）可能會導致資料缺乏變化，無法反映人類經驗的多樣性。在本文中，我們探討了提示 LLM 使用擬人化角色，並要求模型回答時假裝自己是特定人類。這是明確執行的，具有確切的人口統計、政治信仰和生活經驗，或透過特定族群中流行的名字隱含地執行。然後透過 (1) 主觀註解任務（例如，偵測毒性）和 (2) 信念產生任務來評估 LLM 角色，這兩個任務已知會因人類因素而有所不同。我們探討了明確角色與隱含角色的影響，並調查 LLM 識別和回應哪些人類因素。結果顯示，LLM 角色在複製已知的人類偏誤時表現出好壞參半的結果，但產生時通常無法表現出隱含偏誤。我們得出結論，LLM 缺乏人類思想的內在認知機制，同時擷取了人們說話的統計模式，這可能會限制它們在複雜社會科學應用中的效能。

##### **Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models**
2406.14459v1 by Shijie Han, Zhenyu Zhang, Andrei Arsene Simion

Language models like BERT excel at sentence classification tasks due to
extensive pre-training on general data, but their robustness to parameter
corruption is unexplored. To understand this better, we look at what happens if
a language model is "broken", in the sense that some of its parameters are
corrupted and then recovered by fine-tuning. Strategically corrupting BERT
variants at different levels, we find corrupted models struggle to fully
recover their original performance, with higher corruption causing more severe
degradation. Notably, bottom-layer corruption affecting fundamental linguistic
features is more detrimental than top-layer corruption. Our insights contribute
to understanding language model robustness and adaptability under adverse
conditions, informing strategies for developing resilient NLP systems against
parameter perturbations.

摘要：語言模型，例如 BERT，在句子分類任務中表現出色，因為在一般資料上進行了廣泛的預先訓練，但它們對參數損毀的穩健性尚未探索。為了更好地理解這一點，我們觀察如果語言模型「損毀」，即某些參數損毀，然後透過微調恢復，會發生什麼事。在不同層級策略性地損毀 BERT 變體，我們發現損毀的模型難以完全恢復其原始效能，損毀程度越高，效能下降越嚴重。值得注意的是，影響基本語言特徵的底層損毀比頂層損毀更具破壞性。我們的見解有助於了解語言模型在不利條件下的穩健性和適應性，並為開發能抵禦參數擾動的具韌性 NLP 系統提供策略。

##### **Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue**
2406.14457v1 by Huifang Du, Shuqin Li, Minghao Wu, Xuejing Feng, Yuan-Fang Li, Haofen Wang

Reinforcement learning (RL) is a powerful approach to enhance task-oriented
dialogue (TOD) systems. However, existing RL methods tend to mainly focus on
generation tasks, such as dialogue policy learning (DPL) or response generation
(RG), while neglecting dialogue state tracking (DST) for understanding. This
narrow focus limits the systems to achieve globally optimal performance by
overlooking the interdependence between understanding and generation.
Additionally, RL methods face challenges with sparse and delayed rewards, which
complicates training and optimization. To address these issues, we extend RL
into both understanding and generation tasks by introducing step-by-step
rewards throughout the token generation. The understanding reward increases as
more slots are correctly filled in DST, while the generation reward grows with
the accurate inclusion of user requests. Our approach provides a balanced
optimization aligned with task completion. Experimental results demonstrate
that our approach effectively enhances the performance of TOD systems and
achieves new state-of-the-art results on three widely used datasets, including
MultiWOZ2.0, MultiWOZ2.1, and In-Car. Our approach also shows superior few-shot
ability in low-resource settings compared to current models.

摘要：強化學習 (RL) 是增強任務導向對話 (TOD) 系統的強大方法。然而，現有的 RL 方法傾向於主要關注生成任務，例如對話策略學習 (DPL) 或回應生成 (RG)，同時忽略對話狀態追蹤 (DST) 以進行理解。這種狹隘的關注限制了系統通過忽視理解和生成之間的相互依賴性來實現全局最佳效能。此外，RL 方法面臨稀疏和延遲獎勵的挑戰，這使得訓練和最佳化變得複雜。為了解決這些問題，我們透過在整個代碼生成過程中引入逐步獎勵，將 RL 延伸到理解和生成任務中。隨著 DST 中正確填入更多時段，理解獎勵會增加，而生成獎勵則會隨著準確包含使用者要求而增加。我們的做法提供了一個與任務完成相一致的平衡最佳化。實驗結果表明，我們的方法有效地增強了 TOD 系統的效能，並在三個廣泛使用的資料集，包括 MultiWOZ2.0、MultiWOZ2.1 和 In-Car 上取得了新的最先進成果。與目前的模型相比，我們的做法在低資源設定中也展現出優異的少次學習能力。

##### **APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking**
2406.14449v1 by Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong, Sanguthevar Rajasekaran, Dimitris N. Metaxas

Large Language Models (LLMs) have significantly enhanced Information
Retrieval (IR) across various modules, such as reranking. Despite impressive
performance, current zero-shot relevance ranking with LLMs heavily relies on
human prompt engineering. Existing automatic prompt engineering algorithms
primarily focus on language modeling and classification tasks, leaving the
domain of IR, particularly reranking, underexplored. Directly applying current
prompt engineering algorithms to relevance ranking is challenging due to the
integration of query and long passage pairs in the input, where the ranking
complexity surpasses classification tasks. To reduce human effort and unlock
the potential of prompt optimization in reranking, we introduce a novel
automatic prompt engineering algorithm named APEER. APEER iteratively generates
refined prompts through feedback and preference optimization. Extensive
experiments with four LLMs and ten datasets demonstrate the substantial
performance improvement of APEER over existing state-of-the-art (SoTA) manual
prompts. Furthermore, we find that the prompts generated by APEER exhibit
better transferability across diverse tasks and LLMs. Code is available at
https://github.com/jincan333/APEER.

摘要：大型語言模型 (LLM) 已大幅提升資訊檢索 (IR) 在各種模組中的表現，例如重新排序。儘管有令人印象深刻的表現，但目前使用 LLM 的零次方相關性排序仍高度依賴於人工提示工程。現有的自動提示工程演算法主要專注於語言建模和分類任務，讓 IR 領域，特別是重新排序，處於未充分探索的狀態。由於輸入中整合了查詢和長篇段落的配對，因此直接將目前的提示工程演算法應用於相關性排序具有挑戰性，其中排序複雜度超越了分類任務。為了減少人工工作並發揮重新排序中提示最佳化的潛力，我們提出了一種名為 APEER 的新穎自動提示工程演算法。APEER 透過回饋和偏好最佳化，反覆產生精製的提示。使用四個 LLM 和十個資料集進行的廣泛實驗證明了 APEER 比現有的最先進 (SoTA) 手動提示有顯著的效能提升。此外，我們發現 APEER 產生的提示在不同的任務和 LLM 中表現出更好的可移植性。程式碼可在 https://github.com/jincan333/APEER 取得。

##### **Graph Representation Learning Strategies for Omics Data: A Case Study on Parkinson's Disease**
2406.14442v1 by Elisa Gómez de Lope, Saurabh Deshpande, Ramón Viñas Torné, Pietro Liò, Enrico Glaab, Stéphane P. A. Bordas

Omics data analysis is crucial for studying complex diseases, but its high
dimensionality and heterogeneity challenge classical statistical and machine
learning methods. Graph neural networks have emerged as promising alternatives,
yet the optimal strategies for their design and optimization in real-world
biomedical challenges remain unclear. This study evaluates various graph
representation learning models for case-control classification using
high-throughput biological data from Parkinson's disease and control samples.
We compare topologies derived from sample similarity networks and molecular
interaction networks, including protein-protein and metabolite-metabolite
interactions (PPI, MMI). Graph Convolutional Network (GCNs), Chebyshev spectral
graph convolution (ChebyNet), and Graph Attention Network (GAT), are evaluated
alongside advanced architectures like graph transformers, the graph U-net, and
simpler models like multilayer perceptron (MLP).
  These models are systematically applied to transcriptomics and metabolomics
data independently. Our comparative analysis highlights the benefits and
limitations of various architectures in extracting patterns from omics data,
paving the way for more accurate and interpretable models in biomedical
research.

摘要：組學資料分析對於研究複雜疾病至關重要，但其高維度和異質性挑戰了傳統的統計和機器學習方法。圖神經網路已成為有前途的替代方案，但針對現實世界生物醫學挑戰中其設計和最佳化策略仍不明確。本研究評估了各種圖形表示學習模型，以使用帕金森氏症和對照樣本的高通量生物資料進行病例對照分類。我們比較了從樣本相似性網路和分子交互網路（包括蛋白質-蛋白質和代謝物-代謝物交互（PPI、MMI））衍生的拓撲。圖形卷積網路（GCN）、切比雪夫譜圖卷積（ChebyNet）和圖形注意力網路（GAT）與先進架構（如圖形Transformer、圖形 U-net）和簡單模型（如多層感知器（MLP））一起進行評估。這些模型系統地獨立應用於轉錄組學和代謝組學資料。我們的比較分析突出了各種架構在從組學資料中提取模式方面的優缺點，為生物醫學研究中更準確和可解釋的模型鋪平了道路。

##### **Towards Truthful Multilingual Large Language Models: Benchmarking and Alignment Strategies**
2406.14434v1 by Weihao Liu, Ning Wu, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang

In the era of large language models (LLMs), building multilingual large
language models (MLLMs) that can serve users worldwide holds great
significance. However, existing research seldom focuses on the truthfulness of
MLLMs. Meanwhile, contemporary multilingual aligning technologies struggle to
balance massive languages and often exhibit serious truthfulness gaps across
different languages, especially those that differ greatly from English. In our
work, we construct a benchmark for truthfulness evaluation in multilingual
scenarios and explore the ways to align facts across languages to enhance the
truthfulness of MLLMs. Furthermore, we propose Fact-aware Multilingual
Selective Synergy (FaMSS) to optimize the data allocation across a large number
of languages and different data types. Experimental results demonstrate that
our approach can effectively reduce the multilingual representation disparity
and enhance the multilingual capabilities of LLMs.

摘要：在大型語言模型 (LLM) 時代，建立可服務全球使用者的多語言大型語言模型 (MLLM) 具有重大意義。然而，現有研究很少關注 MLLM 的真實性。同時，當代多語言對齊技術難以平衡海量語言，並且經常在不同語言之間表現出嚴重的真實性差距，特別是與英語差異很大的語言。在我們的研究中，我們構建了一個多語言場景中的真實性評估基準，並探討了跨語言對齊事實以增強 MLLM 真實性的方法。此外，我們提出了事實感知多語言選擇性協同 (FaMSS)，以優化大量語言和不同數據類型之間的數據分配。實驗結果表明，我們的做法可以有效減少多語言表示差異，並增強 LLM 的多語言能力。

##### **CollaFuse: Collaborative Diffusion Models**
2406.14429v1 by Simeon Allmendinger, Domenique Zipperling, Lukas Struppek, Niklas Kühl

In the landscape of generative artificial intelligence, diffusion-based
models have emerged as a promising method for generating synthetic images.
However, the application of diffusion models poses numerous challenges,
particularly concerning data availability, computational requirements, and
privacy. Traditional approaches to address these shortcomings, like federated
learning, often impose significant computational burdens on individual clients,
especially those with constrained resources. In response to these challenges,
we introduce a novel approach for distributed collaborative diffusion models
inspired by split learning. Our approach facilitates collaborative training of
diffusion models while alleviating client computational burdens during image
synthesis. This reduced computational burden is achieved by retaining data and
computationally inexpensive processes locally at each client while outsourcing
the computationally expensive processes to shared, more efficient server
resources. Through experiments on the common CelebA dataset, our approach
demonstrates enhanced privacy by reducing the necessity for sharing raw data.
These capabilities hold significant potential across various application areas,
including the design of edge computing solutions. Thus, our work advances
distributed machine learning by contributing to the evolution of collaborative
diffusion models.

摘要：在生成式人工智慧的領域中，基於擴散的模型已成為生成合成影像的潛力方法。然而，擴散模型的應用面臨許多挑戰，特別是關於資料可用性、運算需求和隱私。傳統上解決這些缺點的方法，例如聯合學習，通常會對個別用戶造成顯著的運算負擔，特別是那些資源受限的用戶。為了應對這些挑戰，我們引入了一種新的方法，用於分散協作擴散模型，其靈感來自分割學習。我們的做法促進了擴散模型的協作訓練，同時在影像合成過程中減輕了用戶的運算負擔。這種降低的運算負擔是透過在每個用戶端保留資料和運算成本低廉的處理程序，同時將運算成本昂貴的處理程序外包給共享的、更有效率的伺服器資源來實現的。透過在常見的 CelebA 資料集上進行實驗，我們的做法透過減少共享原始資料的必要性來展示增強的隱私。這些功能在各種應用領域中具有顯著的潛力，包括邊緣運算解決方案的設計。因此，我們的研究透過促進協作擴散模型的演進，推動了分散式機器學習。

##### **SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages**
2406.14425v1 by Gayane Ghazaryan, Erik Arakelyan, Pasquale Minervini, Isabelle Augenstein

Question Answering (QA) datasets have been instrumental in developing and
evaluating Large Language Model (LLM) capabilities. However, such datasets are
scarce for languages other than English due to the cost and difficulties of
collection and manual annotation. This means that producing novel models and
measuring the performance of multilingual LLMs in low-resource languages is
challenging. To mitigate this, we propose $\textbf{S}$yn$\textbf{DAR}$in, a
method for generating and validating QA datasets for low-resource languages. We
utilize parallel content mining to obtain $\textit{human-curated}$ paragraphs
between English and the target language. We use the English data as context to
$\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which
are automatically translated and further validated for quality. Combining these
with their designated non-English $\textit{human-curated}$ paragraphs form the
final QA dataset. The method allows to maintain the content quality, reduces
the likelihood of factual errors, and circumvents the need for costly
annotation. To test the method, we created a QA dataset with $1.2$K samples for
the Armenian language. The human evaluation shows that $98\%$ of the generated
English data maintains quality and diversity in the question types and topics,
while the translation validation pipeline can filter out $\sim70\%$ of data
with poor quality. We use the dataset to benchmark state-of-the-art LLMs,
showing their inability to achieve human accuracy with some model performances
closer to random chance. This shows that the generated dataset is non-trivial
and can be used to evaluate reasoning capabilities in low-resource language.

摘要：問答 (QA) 資料集對於開發和評估大型語言模型 (LLM) 能力發揮了至關重要的作用。然而，由於收集和手動標註的成本和難度，此類資料集對於英語以外的語言來說非常稀少。這表示製作新穎模型和衡量低資源語言中多語言 LLM 的效能具有挑戰性。為了減輕這種情況，我們提出了 $\textbf{S}$yn$\textbf{DAR}$in，一種用於為低資源語言產生和驗證 QA 資料集的方法。我們利用平行內容探勘來取得英語和目標語言之間的「人工策展」段落。我們使用英語資料作為背景來「產生」合成的多重選擇 (MC) 問題-答案對，這些對會自動翻譯並進一步驗證品質。將這些對與其指定的非英語「人工策展」段落結合，即可形成最終的 QA 資料集。此方法允許維持內容品質、降低事實錯誤的可能性，並規避昂貴標註的需求。為了測試此方法，我們為亞美尼亞語建立了一個包含 $1.2$K 個範例的 QA 資料集。人工評估顯示，98% 的產生式英語資料在問題類型和主題中維持了品質和多樣性，而翻譯驗證管線可以濾除約 70% 品質不佳的資料。我們使用此資料集來評量最先進的 LLM，顯示出它們無法達到人類準確度，有些模型效能接近隨機機會。這顯示產生的資料集並非微不足道，而且可以用於評估低資源語言中的推理能力。

##### **FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving**
2406.14408v1 by Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang

Formal verification (FV) has witnessed growing significance with current
emerging program synthesis by the evolving large language models (LLMs).
However, current formal verification mainly resorts to symbolic verifiers or
hand-craft rules, resulting in limitations for extensive and flexible
verification. On the other hand, formal languages for automated theorem
proving, such as Isabelle, as another line of rigorous verification, are
maintained with comprehensive rules and theorems. In this paper, we propose
FVEL, an interactive Formal Verification Environment with LLMs. Specifically,
FVEL transforms a given code to be verified into Isabelle, and then conducts
verification via neural automated theorem proving with an LLM. The joined
paradigm leverages the rigorous yet abundant formulated and organized rules in
Isabelle and is also convenient for introducing and adjusting cutting-edge
LLMs. To achieve this goal, we extract a large-scale FVELER3. The FVELER
dataset includes code dependencies and verification processes that are
formulated in Isabelle, containing 758 theories, 29,125 lemmas, and 200,646
proof steps in total with in-depth dependencies. We benchmark FVELER in the
FVEL environment by first fine-tuning LLMs with FVELER and then evaluating them
on Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned
Llama3- 8B solves 17.39% (69 -> 81) more problems, and Mistral-7B 12% (75 ->
84) more problems in SV-COMP. And the proportion of proof errors is reduced.
Project page: https://fveler.github.io/.

摘要：<paragraph>形式驗證 (FV) 已見證了當前大型語言模型 (LLM) 發展的程式合成日益重要。
然而，目前的正式驗證主要依賴於符號驗證器或手工規則，導致廣泛且靈活驗證的限制。另一方面，用於自動定理證明，例如 Isabelle 的形式語言，作為另一種嚴格驗證，使用全面的規則和定理來維護。在本文中，我們提出 FVEL，一個與 LLM 互動的形式驗證環境。具體來說，FVEL 將給定的要驗證的代碼轉換為 Isabelle，然後通過使用 LLM 的神經自動定理證明進行驗證。結合的範例利用了 Isabelle 中嚴謹而豐富的公式化和組織規則，並且便於引入和調整尖端的 LLM。為了實現這個目標，我們提取了一個大規模的 FVELER3。FVELER 資料集包括以 Isabelle 制定的代碼依賴性和驗證過程，總共包含 758 個理論、29,125 個引理和 200,646 個證明步驟以及深入的依賴性。我們在 FVEL 環境中使用 FVELER 對 FVELER 進行基準測試，首先使用 FVELER 對 LLM 進行微調，然後在 Code2Inv 和 SV-COMP 上對它們進行評估。結果表明，使用 FVELER 微調的 FVEL Llama3-8B 解決了 17.39%（69 -> 81）更多問題，而 Mistral-7B 12%（75 -> 84）更多問題在 SV-COMP 中。並且證明錯誤的比例減少了。專案頁面：https://fveler.github.io/。</paragraph>

##### **Fair Streaming Feature Selection**
2406.14401v1 by Zhangling Duan, Tianci Li, Xingyu Wu, Zhaolong Ling, Jingye Yang, Zhaohong Jia

Streaming feature selection techniques have become essential in processing
real-time data streams, as they facilitate the identification of the most
relevant attributes from continuously updating information. Despite their
performance, current algorithms to streaming feature selection frequently fall
short in managing biases and avoiding discrimination that could be perpetuated
by sensitive attributes, potentially leading to unfair outcomes in the
resulting models. To address this issue, we propose FairSFS, a novel algorithm
for Fair Streaming Feature Selection, to uphold fairness in the feature
selection process without compromising the ability to handle data in an online
manner. FairSFS adapts to incoming feature vectors by dynamically adjusting the
feature set and discerns the correlations between classification attributes and
sensitive attributes from this revised set, thereby forestalling the
propagation of sensitive data. Empirical evaluations show that FairSFS not only
maintains accuracy that is on par with leading streaming feature selection
methods and existing fair feature techniques but also significantly improves
fairness metrics.

摘要：串流特徵選取技術已成為處理即時資料串流的必要技術，因為它們能協助從持續更新的資訊中找出最相關的屬性。儘管這些串流特徵選取演算法效能良好，但它們在管理偏差和避免歧視方面常常不足，而這些偏差和歧視可能因敏感屬性而持續存在，進而可能導致結果模型出現不公平的結果。為了解決這個問題，我們提出 FairSFS，一種公平串流特徵選取的新演算法，在不影響線上資料處理能力的前提下，維護特徵選取過程的公平性。FairSFS 會根據輸入的特徵向量動態調整特徵集，並從這個已修改的集合中辨別分類屬性與敏感屬性之間的關聯性，從而防止敏感資料外洩。經驗評估顯示，FairSFS 不僅能維持與領先串流特徵選取方法和現有公平特徵技術同等的準確性，還能顯著改善公平性指標。

##### **SEC-QA: A Systematic Evaluation Corpus for Financial QA**
2406.14394v1 by Viet Dac Lai, Michael Krumdick, Charles Lovering, Varshini Reddy, Craig Schmidt, Chris Tanner

The financial domain frequently deals with large numbers of long documents
that are essential for daily operations. Significant effort is put towards
automating financial data analysis. However, a persistent challenge, not
limited to the finance domain, is the scarcity of datasets that accurately
reflect real-world tasks for model evaluation. Existing datasets are often
constrained by size, context, or relevance to practical applications. Moreover,
LLMs are currently trained on trillions of tokens of text, limiting access to
novel data or documents that models have not encountered during training for
unbiased evaluation. We propose SEC-QA, a continuous dataset generation
framework with two key features: 1) the semi-automatic generation of
Question-Answer (QA) pairs spanning multiple long context financial documents,
which better represent real-world financial scenarios; 2) the ability to
continually refresh the dataset using the most recent public document
collections, not yet ingested by LLMs. Our experiments show that current
retrieval augmented generation methods systematically fail to answer these
challenging multi-document questions. In response, we introduce a QA system
based on program-of-thought that improves the ability to perform complex
information retrieval and quantitative reasoning pipelines, thereby increasing
QA accuracy.

摘要：金融領域經常處理大量長篇文件，這些文件對於日常營運至關重要。自動化金融數據分析需要投入大量精力。然而，一個持續存在的挑戰，不限於金融領域，就是缺乏準確反映真實世界任務的資料集，以便進行模型評估。現有的資料集通常受到規模、內容或與實際應用相關性的限制。此外，大型語言模型目前是根據數兆個文本標記進行訓練，這限制了模型在訓練期間未曾遭遇過的新資料或文件的使用，以進行無偏評估。我們提出 SEC-QA，一個連續資料集生成架構，具有兩個主要特徵：1) 半自動生成跨越多個長篇背景金融文件的問答 (QA) 配對，這能更好地呈現真實世界的金融場景；2) 使用大型語言模型尚未吸收的最新公開文件集合，持續更新資料集的能力。我們的實驗顯示，目前的檢索擴充生成方法系統性地無法回答這些具有挑戰性的多文件問題。為了解決這個問題，我們引入了一個基於思考程式碼的 QA 系統，它提高了執行複雜資訊檢索和定量推理管線的能力，從而提高了 QA 的準確性。

##### **Jailbreaking as a Reward Misspecification Problem**
2406.14393v1 by Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong

The widespread adoption of large language models (LLMs) has raised concerns
about their safety and reliability, particularly regarding their vulnerability
to adversarial attacks. In this paper, we propose a novel perspective that
attributes this vulnerability to reward misspecification during the alignment
process. We introduce a metric ReGap to quantify the extent of reward
misspecification and demonstrate its effectiveness and robustness in detecting
harmful backdoor prompts. Building upon these insights, we present ReMiss, a
system for automated red teaming that generates adversarial prompts against
various target aligned LLMs. ReMiss achieves state-of-the-art attack success
rates on the AdvBench benchmark while preserving the human readability of the
generated prompts. Detailed analysis highlights the unique advantages brought
by the proposed reward misspecification objective compared to previous methods.

摘要：大型語言模型 (LLM) 的廣泛採用引發了對其安全性和可靠性的擔憂，特別是它們容易受到對抗性攻擊。在本文中，我們提出了一個新穎的觀點，將這種脆弱性歸因於對齊過程中獎勵規格錯誤。我們引入了一個指標 ReGap 來量化獎勵規格錯誤的程度，並證明了其在檢測有害後門提示方面的有效性和魯棒性。基於這些見解，我們提出了 ReMiss，一個用於自動紅隊的系統，它針對各種目標對齊的 LLM 生成對抗性提示。ReMiss 在 AdvBench 基準上實現了最先進的攻擊成功率，同時保持了生成提示的人類可讀性。詳細的分析突出了與先前方法相比，所提出的獎勵規格錯誤目標帶來的獨特優勢。

##### **Computation-Efficient Semi-Supervised Learning for ECG-based Cardiovascular Diseases Detection**
2406.14377v1 by Rushuang Zhou, Zijun Liu, Lei Clifton, David A. Clifton, Kannie W. Y. Chan, Yuan-Ting Zhang, Yining Dong

Label scarcity problem is the main challenge that hinders the wide
application of deep learning systems in automatic cardiovascular diseases
(CVDs) detection using electrocardiography (ECG). Tuning pre-trained models
alleviates this problem by transferring knowledge learned from large datasets
to downstream small datasets. However, bottlenecks in computational efficiency
and CVDs detection performance limit its clinical applications. It is difficult
to improve the detection performance without significantly sacrificing model
computational efficiency. Here, we propose a computation-efficient
semi-supervised learning paradigm (FastECG) for robust and
computation-efficient CVDs detection using ECG. It enables a robust adaptation
of pre-trained models on downstream datasets with limited supervision and high
computational efficiency. First, a random-deactivation technique is developed
to achieve robust and fast low-rank adaptation of pre-trained weights.
Subsequently, we propose a one-shot rank allocation module to determine the
optimal ranks for the update matrices of the pre-trained weights. Finally, a
lightweight semi-supervised learning pipeline is introduced to enhance model
performance by leveraging labeled and unlabeled data with high computational
efficiency. Extensive experiments on four downstream ECG datasets demonstrate
that FastECG not only outperforms the state-of-the-art methods in multi-label
CVDs detection but also consumes fewer GPU footprints, training time, and
parameter storage space. As such, this paradigm provides an effective solution
for achieving high computational efficiency and robust detection performance in
the clinical applications of pre-trained models under limited supervision.

摘要：標籤稀缺問題是阻礙深度學習系統在自動心血管疾病 (CVD) 使用心電圖 (ECG) 檢測中廣泛應用之主要挑戰。調整預訓練模型透過將從大型資料集學到的知識轉移到下游小型資料集，來緩解此問題。然而，運算效率和 CVD 檢測效能的瓶頸限制了其臨床應用。在不顯著犧牲模型運算效率的情況下，難以改善檢測效能。在此，我們提出一個運算效率的半監督學習範例 (FastECG)，用於使用 ECG 進行穩健且運算效率高的 CVD 檢測。它能讓預訓練模型在監督有限且運算效率高的下游資料集上進行穩健的調整。首先，開發出一種隨機停用技術，以達成預訓練權重的穩健且快速的低秩調整。接著，我們提出一個一次性秩配置模組，用於確定預訓練權重的更新矩陣之最佳秩。最後，引入一個輕量級的半監督學習管線，以利用標籤和未標籤資料，並在高運算效率下提升模型效能。在四個下游 ECG 資料集上的廣泛實驗證明，FastECG 不僅在多標籤 CVD 檢測中優於最先進的方法，而且消耗更少的 GPU 占用空間、訓練時間和參數儲存空間。因此，此範例提供了一個有效的解決方案，用於在監督有限的情況下，於預訓練模型的臨床應用中達成高運算效率和穩健的檢測效能。

##### **Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory**
2406.14373v1 by Gordon Dai, Weijia Zhang, Jinhan Li, Siqi Yang, Chidera Onochie lbe, Srihas Rao, Arthur Caetano, Misha Sra

The emergence of Large Language Models (LLMs) and advancements in Artificial
Intelligence (AI) offer an opportunity for computational social science
research at scale. Building upon prior explorations of LLM agent design, our
work introduces a simulated agent society where complex social relationships
dynamically form and evolve over time. Agents are imbued with psychological
drives and placed in a sandbox survival environment. We conduct an evaluation
of the agent society through the lens of Thomas Hobbes's seminal Social
Contract Theory (SCT). We analyze whether, as the theory postulates, agents
seek to escape a brutish "state of nature" by surrendering rights to an
absolute sovereign in exchange for order and security. Our experiments unveil
an alignment: Initially, agents engage in unrestrained conflict, mirroring
Hobbes's depiction of the state of nature. However, as the simulation
progresses, social contracts emerge, leading to the authorization of an
absolute sovereign and the establishment of a peaceful commonwealth founded on
mutual cooperation. This congruence between our LLM agent society's
evolutionary trajectory and Hobbes's theoretical account indicates LLMs'
capability to model intricate social dynamics and potentially replicate forces
that shape human societies. By enabling such insights into group behavior and
emergent societal phenomena, LLM-driven multi-agent simulations, while unable
to simulate all the nuances of human behavior, may hold potential for advancing
our understanding of social structures, group dynamics, and complex human
systems.

摘要：大型語言模型 (LLM) 的出現和人工智慧 (AI) 的進步，為大規模的計算社會科學研究提供了機會。在先前對 LLM 代理設計的探索基礎上，我們的研究引入了模擬代理社會，其中複雜的社會關係會隨著時間動態形成和演變。代理人被賦予了心理驅動力，並被置於沙盒生存環境中。我們透過湯瑪斯·霍布斯的開創性社會契約論 (SCT) 的觀點，對代理社會進行評估。我們分析了代理人是否如理論所假設的那樣，透過放棄權利以換取秩序和安全，來逃離殘酷的「自然狀態」。我們的實驗揭示了一致性：最初，代理人會參與不受約束的衝突，反映了霍布斯對自然狀態的描繪。然而，隨著模擬的進行，社會契約開始出現，導致授權絕對的主權者，並建立一個基於相互合作的和平聯邦。我們的 LLM 代理社會的演化軌跡與霍布斯的理論論述之間的這種一致性，表明 LLM 能夠模擬複雜的社會動態，並潛在地複製塑造人類社會的力量。透過提供對群體行為和新興社會現象的見解，LLM 驅動的多代理模擬雖然無法模擬人類行為的所有細微差別，但可能有助於增進我們對社會結構、群體動態和複雜人類系統的理解。

##### **PoseBench: Benchmarking the Robustness of Pose Estimation Models under Corruptions**
2406.14367v1 by Sihan Ma, Jing Zhang, Qiong Cao, Dacheng Tao

Pose estimation aims to accurately identify anatomical keypoints in humans
and animals using monocular images, which is crucial for various applications
such as human-machine interaction, embodied AI, and autonomous driving. While
current models show promising results, they are typically trained and tested on
clean data, potentially overlooking the corruption during real-world deployment
and thus posing safety risks in practical scenarios. To address this issue, we
introduce PoseBench, a comprehensive benchmark designed to evaluate the
robustness of pose estimation models against real-world corruption. We
evaluated 60 representative models, including top-down, bottom-up,
heatmap-based, regression-based, and classification-based methods, across three
datasets for human and animal pose estimation. Our evaluation involves 10 types
of corruption in four categories: 1) blur and noise, 2) compression and color
loss, 3) severe lighting, and 4) masks. Our findings reveal that
state-of-the-art models are vulnerable to common real-world corruptions and
exhibit distinct behaviors when tackling human and animal pose estimation
tasks. To improve model robustness, we delve into various design
considerations, including input resolution, pre-training datasets, backbone
capacity, post-processing, and data augmentations. We hope that our benchmark
will serve as a foundation for advancing research in robust pose estimation.
The benchmark and source code will be released at
https://xymsh.github.io/PoseBench

摘要：人體姿勢估計旨在使用單眼影像準確辨識人類和動物的解剖學關鍵點，這對於人機互動、具象 AI 和自動駕駛等各種應用至關重要。雖然目前的模型顯示出有希望的結果，但它們通常在乾淨的資料上訓練和測試，可能會忽略實際部署期間的損壞，從而對實際場景中的安全構成風險。為了解決這個問題，我們引入了 PoseBench，這是一個全面的基準測試，旨在評估姿勢估計模型對真實世界損壞的穩健性。我們評估了 60 個具有代表性的模型，包括自上而下、自下而上、基於熱圖、基於回歸和基於分類的方法，涵蓋了人類和動物姿勢估計的三個資料集。我們的評估涉及四類中的 10 種類型的損壞：1) 模糊和雜訊，2) 壓縮和色彩損失，3) 嚴重照明，以及 4) 遮罩。我們的研究結果表明，最先進的模型容易受到常見的真實世界損壞，並且在處理人類和動物姿勢估計任務時表現出不同的行為。為了提高模型的穩健性，我們深入探討了各種設計考量，包括輸入解析度、預訓練資料集、主幹容量、後處理和資料擴充。我們希望我們的基準測試將作為推進穩健姿勢估計研究的基礎。基準測試和原始碼將在 https://xymsh.github.io/PoseBench 發布

##### **Robustness Analysis of AI Models in Critical Energy Systems**
2406.14361v1 by Pantelis Dogoulis, Matthieu Jimenez, Salah Ghamizi, Maxime Cordy, Yves Le Traon

This paper analyzes the robustness of state-of-the-art AI-based models for
power grid operations under the $N-1$ security criterion. While these models
perform well in regular grid settings, our results highlight a significant loss
in accuracy following the disconnection of a line.%under this security
criterion. Using graph theory-based analysis, we demonstrate the impact of node
connectivity on this loss. Our findings emphasize the need for practical
scenario considerations in developing AI methodologies for critical
infrastructure.

摘要：本文分析了在 $N-1$ 安全準則下，用於電網運作的最新 AI 模型的穩健性。儘管這些模型在一般電網設定下表現良好，但我們的結果突顯出在斷開一條線路後，準確度會大幅下降。% 在此安全準則下，我們使用基於圖論的分析來證明節點連接性對此損失的影響。我們的發現強調了在為關鍵基礎設施開發 AI 方法時，需要考慮實際情況。

##### **The neural correlates of logical-mathematical symbol systems processing resemble that of spatial cognition more than natural language processing**
2406.14358v1 by Yuannan Li, Shan Xu, Jia Liu

The ability to manipulate logical-mathematical symbols (LMS), encompassing
tasks such as calculation, reasoning, and programming, is a cognitive skill
arguably unique to humans. Considering the relatively recent emergence of this
ability in human evolutionary history, it has been suggested that LMS
processing may build upon more fundamental cognitive systems, possibly through
neuronal recycling. Previous studies have pinpointed two primary candidates,
natural language processing and spatial cognition. Existing comparisons between
these domains largely relied on task-level comparison, which may be confounded
by task idiosyncrasy. The present study instead compared the neural correlates
at the domain level with both automated meta-analysis and synthesized maps
based on three representative LMS tasks, reasoning, calculation, and mental
programming. Our results revealed a more substantial cortical overlap between
LMS processing and spatial cognition, in contrast to language processing.
Furthermore, in regions activated by both spatial and language processing, the
multivariate activation pattern for LMS processing exhibited greater
multivariate similarity to spatial cognition than to language processing. A
hierarchical clustering analysis further indicated that typical LMS tasks were
indistinguishable from spatial cognition tasks at the neural level, suggesting
an inherent connection between these two cognitive processes. Taken together,
our findings support the hypothesis that spatial cognition is likely the basis
of LMS processing, which may shed light on the limitations of large language
models in logical reasoning, particularly those trained exclusively on textual
data without explicit emphasis on spatial content.

摘要：操縱邏輯數學符號 (LMS) 的能力，包含計算、推理和程式編寫等任務，是人類獨有的認知技能。考慮到這種能力在人類演化史上出現的時間相對較晚，有人提出 LMS 處理可能建立在更基本的認知系統之上，可能是透過神經元再利用。先前的研究已找出兩個主要候選者，自然語言處理和空間認知。這些領域之間的現有比較主要依賴於任務層級的比較，這可能會因任務的特性而產生混淆。本研究則比較了在領域層級上的神經相關性，同時採用自動化元分析和基於三個代表性 LMS 任務（推理、計算和心智程式編寫）的綜合地圖。我們的結果顯示，與語言處理相比，LMS 處理和空間認知之間存在更大量的皮質重疊。此外，在空間和語言處理都激活的區域中，LMS 處理的多變量激活模式與空間認知的多變量相似性大於與語言處理的多變量相似性。階層式聚類分析進一步指出，典型的 LMS 任務在神經層級上與空間認知任務無法區分，這表明這兩個認知過程之間存在內在聯繫。綜合來看，我們的發現支持了空間認知可能是 LMS 處理基礎的假設，這可能有助於闡明大型語言模型在邏輯推理上的限制，特別是那些僅在文本資料上訓練且未明確強調空間內容的模型。

##### **Automatic Labels are as Effective as Manual Labels in Biomedical Images Classification with Deep Learning**
2406.14351v1 by Niccolò Marini, Stefano Marchesin, Lluis Borras Ferris, Simon Püttmann, Marek Wodzinski, Riccardo Fratti, Damian Podareanu, Alessandro Caputo, Svetla Boytcheva, Simona Vatrano, Filippo Fraggetta, Iris Nagtegaal, Gianmaria Silvello, Manfredo Atzori, Henning Müller

The increasing availability of biomedical data is helping to design more
robust deep learning (DL) algorithms to analyze biomedical samples. Currently,
one of the main limitations to train DL algorithms to perform a specific task
is the need for medical experts to label data. Automatic methods to label data
exist, however automatic labels can be noisy and it is not completely clear
when automatic labels can be adopted to train DL models. This paper aims to
investigate under which circumstances automatic labels can be adopted to train
a DL model on the classification of Whole Slide Images (WSI). The analysis
involves multiple architectures, such as Convolutional Neural Networks (CNN)
and Vision Transformer (ViT), and over 10000 WSIs, collected from three use
cases: celiac disease, lung cancer and colon cancer, which one including
respectively binary, multiclass and multilabel data. The results allow
identifying 10% as the percentage of noisy labels that lead to train
competitive models for the classification of WSIs. Therefore, an algorithm
generating automatic labels needs to fit this criterion to be adopted. The
application of the Semantic Knowledge Extractor Tool (SKET) algorithm to
generate automatic labels leads to performance comparable to the one obtained
with manual labels, since it generates a percentage of noisy labels between
2-5%. Automatic labels are as effective as manual ones, reaching solid
performance comparable to the one obtained training models with manual labels.

摘要：隨著生物醫學資料的日益普及，有助於設計更穩健的深度學習 (DL) 演算法來分析生物醫學樣本。目前，訓練 DL 演算法執行特定任務的主要限制之一在於醫學專家標記資料的需求。標記資料的自動化方法確實存在，然而自動化標籤可能會產生雜訊，而且尚不清楚何時可以採用自動化標籤來訓練 DL 模型。本文旨在探討在何種情況下可以採用自動化標籤來訓練 DL 模型對全切片影像 (WSI) 進行分類。分析涉及多種架構，例如卷積神經網路 (CNN) 和視覺Transformer (ViT)，以及超過 10000 個 WSI，這些 WSI 來自三種使用案例：乳糜瀉、肺癌和結腸癌，其中一個分別包括二元、多類和多標籤資料。結果可以將產生雜訊標籤的比例確定為 10%，這將導致訓練出具有競爭力的 WSI 分類模型。因此，產生自動化標籤的演算法需要符合此準則才能被採用。將語義知識萃取工具 (SKET) 演算法應用於產生自動化標籤，其效能可與使用人工標籤獲得的效能相媲美，因為它產生的雜訊標籤比例在 2-5% 之間。自動化標籤與人工標籤一樣有效，可達到與使用人工標籤訓練模型所獲得的效能相當的穩健效能。

##### **iWISDM: Assessing instruction following in multimodal models at scale**
2406.14343v1 by Xiaoxuan Lei, Lucas Gomez, Hao Yuan Bai, Pouya Bashivan

The ability to perform complex tasks from detailed instructions is a key to
many remarkable achievements of our species. As humans, we are not only capable
of performing a wide variety of tasks but also very complex ones that may
entail hundreds or thousands of steps to complete. Large language models and
their more recent multimodal counterparts that integrate textual and visual
inputs have achieved unprecedented success in performing complex tasks. Yet,
most existing benchmarks are largely confined to single-modality inputs (either
text or vision), narrowing the scope of multimodal assessments, particularly
for instruction-following in multimodal contexts. To bridge this gap, we
introduce the instructed-Virtual VISual Decision Making (iWISDM) environment
engineered to generate a limitless array of vision-language tasks of varying
complexity. Using iWISDM, we compiled three distinct benchmarks of instruction
following visual tasks across varying complexity levels and evaluated several
newly developed multimodal models on these benchmarks. Our findings establish
iWISDM as a robust benchmark for assessing the instructional adherence of both
existing and emergent multimodal models and highlight a large gap between these
models' ability to precisely follow instructions with that of humans.

摘要：能夠根據詳細的說明執行複雜任務，是我們人類許多非凡成就的關鍵。作為人類，我們不僅能夠執行種類繁多的任務，還能執行非常複雜的任務，這些任務可能需要數百或數千個步驟才能完成。大型語言模型及其整合文字和視覺輸入的最新多模態對應物，在執行複雜任務方面取得了前所未有的成功。然而，現有的基準測試大多僅限於單一模態輸入（文字或視覺），這縮小了多模態評估的範圍，特別是針對多模態環境中的指令遵循。為了彌補這個差距，我們引入了指令式虛擬視覺決策制定 (iWISDM) 環境，旨在生成無數種類、複雜度各異的視覺語言任務。使用 iWISDM，我們編制了三個不同的基準測試，涵蓋了不同複雜程度的視覺任務指令遵循，並根據這些基準測試評估了幾個新開發的多模態模型。我們的研究結果確立了 iWISDM 成為評估現有和新興多模態模型指令遵循能力的強大基準測試，並強調了這些模型精確遵循指令的能力與人類之間的巨大差距。

##### **Exploring Spatial Representations in the Historical Lake District Texts with LLM-based Relation Extraction**
2406.14336v1 by Erum Haris, Anthony G. Cohn, John G. Stell

Navigating historical narratives poses a challenge in unveiling the spatial
intricacies of past landscapes. The proposed work addresses this challenge
within the context of the English Lake District, employing the Corpus of the
Lake District Writing. The method utilizes a generative pre-trained transformer
model to extract spatial relations from the textual descriptions in the corpus.
The study applies this large language model to understand the spatial
dimensions inherent in historical narratives comprehensively. The outcomes are
presented as semantic triples, capturing the nuanced connections between
entities and locations, and visualized as a network, offering a graphical
representation of the spatial narrative. The study contributes to a deeper
comprehension of the English Lake District's spatial tapestry and provides an
approach to uncovering spatial relations within diverse historical contexts.

摘要：探索歷史敘述對揭示過去景觀的空間複雜性構成挑戰。所提出的作品在英格蘭湖區的語境中探討這個挑戰，採用湖區寫作語料庫。此方法利用生成式預訓練轉換器模型從語料庫中的文字描述中萃取出空間關係。研究將這個大型語言模型應用於全面了解歷史敘述中固有的空間向度。成果以語義三元組呈現，捕捉實體與位置之間的細微連結，並視覺化為網路，提供空間敘述的圖形化表示。研究有助於更深入理解英格蘭湖區的空間掛毯，並提供一種在不同歷史脈絡中揭示空間關係的方法。

##### **Self-supervised Interpretable Concept-based Models for Text Classification**
2406.14335v1 by Francesco De Santis, Philippe Bich, Gabriele Ciravegna, Pietro Barbiero, Danilo Giordano, Tania Cerquitelli

Despite their success, Large-Language Models (LLMs) still face criticism as
their lack of interpretability limits their controllability and reliability.
Traditional post-hoc interpretation methods, based on attention and
gradient-based analysis, offer limited insight into the model's decision-making
processes. In the image field, Concept-based models have emerged as
explainable-by-design architectures, employing human-interpretable features as
intermediate representations. However, these methods have not been yet adapted
to textual data, mainly because they require expensive concept annotations,
which are impractical for real-world text data. This paper addresses this
challenge by proposing a self-supervised Interpretable Concept Embedding Models
(ICEMs). We leverage the generalization abilities of LLMs to predict the
concepts labels in a self-supervised way, while we deliver the final
predictions with an interpretable function. The results of our experiments show
that ICEMs can be trained in a self-supervised way achieving similar
performance to fully supervised concept-based models and end-to-end black-box
ones. Additionally, we show that our models are (i) interpretable, offering
meaningful logical explanations for their predictions; (ii) interactable,
allowing humans to modify intermediate predictions through concept
interventions; and (iii) controllable, guiding the LLMs' decoding process to
follow a required decision-making path.

摘要：儘管大型語言模型 (LLM) 已取得成功，但由於其缺乏可解釋性，仍面臨著批評，因為這會限制其可控性和可靠性。傳統的後設解釋方法，基於注意力和基於梯度的分析，僅能提供有限的洞察力，了解模型的決策制定過程。在影像領域，基於概念的模型已成為可解釋設計架構，採用人類可解釋的功能，作為中間表示。然而，這些方法尚未適應文本資料，主要是因為它們需要昂貴的概念註釋，這對於真實世界的文本資料來說是不切實際的。本文透過提出自我監督的可解釋概念嵌入模型 (ICEM) 來解決此挑戰。我們利用 LLM 的概化能力，以自我監督的方式預測概念標籤，同時我們提供可解釋函數的最終預測。我們的實驗結果顯示，ICEM 可以透過自我監督的方式訓練，達成與完全監督的基於概念的模型和端到端的黑盒模型相似的效能。此外，我們展示我們的模型是 (i) 可解釋的，提供有意義的邏輯解釋，說明其預測；(ii) 可互動的，允許人類透過概念介入，修改中間預測；以及 (iii) 可控的，引導 LLM 的解碼過程，遵循所需的決策制定路徑。

##### **medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs**
2406.14326v1 by Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang

Electronic Medical Records (EMRs), while integral to modern healthcare,
present challenges for clinical reasoning and diagnosis due to their complexity
and information redundancy. To address this, we proposed medIKAL (Integrating
Knowledge Graphs as Assistants of LLMs), a framework that combines Large
Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic
capabilities. medIKAL assigns weighted importance to entities in medical
records based on their type, enabling precise localization of candidate
diseases within KGs. It innovatively employs a residual network-like approach,
allowing initial diagnosis by the LLM to be merged into KG search results.
Through a path-based reranking algorithm and a fill-in-the-blank style prompt
template, it further refined the diagnostic process. We validated medIKAL's
effectiveness through extensive experiments on a newly introduced open-sourced
Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis
in real-world settings.

摘要：電子病歷 (EMR) 雖然是現代醫療保健不可或缺的一部分，但由於其複雜性和資訊冗餘，對臨床推理和診斷提出了挑戰。為了解決這個問題，我們提出了 medIKAL（將知識圖譜整合為 LLM 的助理），一個將大型語言模型 (LLM) 與知識圖譜 (KG) 結合的框架，以增強診斷能力。medIKAL 根據醫療記錄中實體的類型為其分配加權重要性，從而能夠精確定位 KG 中的候選疾病。它創新地採用了類似殘差網路的方法，允許 LLM 的初步診斷與 KG 搜尋結果合併。透過基於路徑的重新排序演算法和填空式提示範本，進一步優化了診斷過程。我們透過對新推出的開源中文 EMR 資料集進行廣泛的實驗，驗證了 medIKAL 的有效性，證明了其在現實世界中改善臨床診斷的潛力。

##### **Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning**
2406.14322v1 by Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Daogao Liu, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang

Large language models (LLMs) have emerged as powerful tools for tackling
complex tasks across diverse domains, but they also raise privacy concerns when
fine-tuned on sensitive data due to potential memorization. While differential
privacy (DP) offers a promising solution by ensuring models are `almost
indistinguishable' with or without any particular privacy unit, current
evaluations on LLMs mostly treat each example (text record) as the privacy
unit. This leads to uneven user privacy guarantees when contributions per user
vary. We therefore study user-level DP motivated by applications where it
necessary to ensure uniform privacy protection across users. We present a
systematic evaluation of user-level DP for LLM fine-tuning on natural language
generation tasks. Focusing on two mechanisms for achieving user-level DP
guarantees, Group Privacy and User-wise DP-SGD, we investigate design choices
like data selection strategies and parameter tuning for the best
privacy-utility tradeoff.

摘要：大型語言模型 (LLM) 已成為應對不同領域複雜任務的強大工具，但由於潛在的記憶化，在敏感數據上微調時也會引起隱私問題。雖然差分隱私 (DP) 通過確保模型在有或沒有任何特定隱私單元的條件下「幾乎無法區分」而提供了一個有希望的解決方案，但目前對 LLM 的評估大多將每個範例（文本記錄）視為隱私單元。當每個使用者的貢獻有所不同時，這將導致使用者隱私保證不均。因此，我們研究了使用者層級的 DP，其動機來自於必須確保所有使用者隱私保護一致的應用。我們對 LLM 微調在自然語言生成任務上的使用者層級 DP 進行了系統性評估。我們專注於實現使用者層級 DP 保證的兩種機制，群組隱私和使用者明智 DP-SGD，探討了資料選取策略和參數調整等設計選擇，以取得最佳的隱私實用權衡。

##### **LiveMind: Low-latency Large Language Models with Simultaneous Inference**
2406.14319v1 by Chuangtao Chen, Grace Li Zhang, Xunzhao Yin, Cheng Zhuo, Ulf Schlichtmann, Bing Li

In this paper, we introduce a novel low-latency inference framework for large
language models (LLMs) inference which enables LLMs to perform inferences with
incomplete prompts. By reallocating computational processes to prompt input
phase, we achieve a substantial reduction in latency, thereby significantly
enhancing the interactive experience for users of LLMs. The framework adeptly
manages the visibility of the streaming prompt to the model, allowing it to
infer from incomplete prompts or await additional prompts. Compared with
traditional inference methods that utilize complete prompts, our approach
demonstrates an average reduction of 59% in response latency on the MMLU-Pro
dataset, while maintaining comparable accuracy. Additionally, our framework
facilitates collaborative inference and output across different models. By
employing an LLM for inference and a small language model (SLM) for output, we
achieve an average 68% reduction in response latency, alongside a 5.5%
improvement in accuracy on the MMLU-Pro dataset compared with the SLM baseline.
For long prompts exceeding 20 sentences, the response latency can be reduced by
up to 93%.

摘要：在本文中，我们为大语言模型 (LLM) 推理引入了一个新颖的低延迟推理框架，该框架使 LLM 能够使用不完整的提示执行推理。通过将计算过程重新分配到提示输入阶段，我们大幅减少了延迟，从而显著增强了 LLM 用户的交互体验。该框架巧妙地管理模型对流式提示的可见性，允许它从不完整的提示中推断或等待其他提示。与利用完整提示的传统推理方法相比，我们的方法在 MMLU-Pro 数据集上将响应延迟平均减少了 59%，同时保持了可比的准确性。此外，我们的框架促进了不同模型之间的协作推理和输出。通过使用 LLM 进行推理和使用小型语言模型 (SLM) 进行输出，我们实现了响应延迟平均减少 68%，同时在 MMLU-Pro 数据集上将准确性提高了 5.5%，与 SLM 基线相比。对于超过 20 个句子的长提示，响应延迟可以减少多达 93%。

##### **The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts**
2406.14318v1 by Zhili Shen, Zihang Xi, Ying He, Wei Tong, Jingyu Hua, Sheng Zhong

The rapid adoption of online chatbots represents a significant advancement in
artificial intelligence. However, this convenience brings considerable privacy
concerns, as prompts can inadvertently contain sensitive information exposed to
large language models (LLMs). Limited by high computational costs, reduced task
usability, and excessive system modifications, previous works based on local
deployment, embedding perturbation, and homomorphic encryption are inapplicable
to online prompt-based LLM applications.
  To address these issues, this paper introduces Prompt Privacy Sanitizer
(i.e., ProSan), an end-to-end prompt privacy protection framework that can
produce anonymized prompts with contextual privacy removed while maintaining
task usability and human readability. It can also be seamlessly integrated into
the online LLM service pipeline. To achieve high usability and dynamic
anonymity, ProSan flexibly adjusts its protection targets and strength based on
the importance of the words and the privacy leakage risk of the prompts.
Additionally, ProSan is capable of adapting to diverse computational resource
conditions, ensuring privacy protection even for mobile devices with limited
computing power. Our experiments demonstrate that ProSan effectively removes
private information across various tasks, including question answering, text
summarization, and code generation, with minimal reduction in task performance.

摘要：網路聊天機器人的快速採用代表著人工智慧的重大進展。然而，這種便利性帶來了相當大的隱私問題，因為提示可能會不經意地包含暴露給大型語言模型 (LLM) 的敏感資訊。受限於高運算成本、降低任務可用性以及過度的系統修改，先前基於本地部署、嵌入式擾動和同態加密的技術並不適用於基於提示的線上 LLM 應用程式。
為了解決這些問題，本文介紹了 Prompt Privacy Sanitizer（即 ProSan），這是一個端到端的提示隱私保護架構，可以產生移除語境隱私的匿名提示，同時維持任務可用性和人類可讀性。它也可以無縫整合到線上 LLM 服務管道中。為了達成高可用性和動態匿名性，ProSan 會根據字詞的重要性以及提示的隱私洩漏風險，靈活地調整其保護目標和強度。此外，ProSan 能夠適應不同的運算資源條件，確保即使是運算能力有限的手機也能獲得隱私保護。我們的實驗證明，ProSan 在各種任務中有效地移除了私人資訊，包括問答、文字摘要和程式碼產生，同時將任務效能的降低降到最低。

##### **Identifying User Goals from UI Trajectories**
2406.14314v1 by Omri Berkovitch, Sapir Caduri, Noam Kahlon, Anatoly Efros, Avi Caciularu, Ido Dagan

Autonomous agents that interact with graphical user interfaces (GUIs) hold
significant potential for enhancing user experiences. To further improve these
experiences, agents need to be personalized and proactive. By effectively
comprehending user intentions through their actions and interactions with GUIs,
agents will be better positioned to achieve these goals. This paper introduces
the task of goal identification from observed UI trajectories, aiming to infer
the user's intended task based on their GUI interactions. We propose a novel
evaluation metric to assess whether two task descriptions are paraphrases
within a specific UI environment. By Leveraging the inverse relation with the
UI automation task, we utilized the Android-In-The-Wild and Mind2Web datasets
for our experiments. Using our metric and these datasets, we conducted several
experiments comparing the performance of humans and state-of-the-art models,
specifically GPT-4 and Gemini-1.5 Pro. Our results show that Gemini performs
better than GPT but still underperforms compared to humans, indicating
significant room for improvement.

摘要：自主代理與圖形使用者介面 (GUI) 互動具有提升使用者體驗的重大潛力。為了進一步改善這些體驗，代理人需要個性化且積極主動。透過有效理解使用者意圖，包括他們的動作與 GUI 互動，代理人將能更精確地達成這些目標。本文介紹從觀察到的 UI 軌跡中辨識目標的任務，目的是根據使用者的 GUI 互動推論其預期任務。我們提出一個創新的評估指標，用於評估在特定 UI 環境中，兩個任務描述是否為同義詞。透過利用與 UI 自動化任務的逆關係，我們在實驗中使用了 Android-In-The-Wild 和 Mind2Web 資料集。使用我們的指標和這些資料集，我們進行了多項實驗，比較人類與最先進模型的效能，特別是 GPT-4 和 Gemini-1.5 Pro。我們的結果顯示，Gemini 的效能優於 GPT，但與人類相比仍有不足，這表示有顯著的進步空間。

##### **Robust Few-shot Transfer Learning for Knowledge Base Question Answering with Unanswerable Questions**
2406.14313v1 by Riya Sawhney, Indrajit Bhattacharya, Mausam

Real-world KBQA applications require models that are (1) robust -- e.g., can
differentiate between answerable and unanswerable questions, and (2)
low-resource -- do not require large training data. Towards this goal, we
propose the novel task of few-shot transfer for KBQA with unanswerable
questions. We present FUn-FuSIC that extends the state-of-the-art (SoTA)
few-shot transfer model for answerable-only KBQA to handle unanswerability. It
iteratively prompts an LLM to generate logical forms for the question by
providing feedback using a diverse suite of syntactic, semantic and execution
guided checks, and adapts self-consistency to assess confidence of the LLM to
decide answerability. Experiments over newly constructed datasets show that
FUn-FuSIC outperforms suitable adaptations of the SoTA model for KBQA with
unanswerability, and the SoTA model for answerable-only few-shot-transfer KBQA.

摘要：現實世界的 KBQA 應用程式需要模型，這些模型 (1) 強健，例如，能夠區分可回答和不可回答的問題，以及 (2) 低資源，不需要大量的訓練資料。為了達成這個目標，我們提出了一個新的任務，即針對具有不可回答問題的 KBQA 進行小樣本轉移。我們提出了 FUn-FuSIC，它擴充了最先進 (SoTA) 的僅限於可回答 KBQA 的小樣本轉移模型，以處理不可回答性。它透過提供使用各種語法、語意和執行引導檢查的回饋，反覆提示 LLM 為問題產生邏輯形式，並調整自我一致性以評估 LLM 判斷可回答性的信心。在新建資料集上進行的實驗顯示，FUn-FuSIC 優於 SoTA 模型的適合改編，該模型適用於具有不可回答性的 KBQA，以及僅限於可回答的小樣本轉移 KBQA 的 SoTA 模型。

##### **Infusing clinical knowledge into tokenisers for language models**
2406.14312v1 by Abul Hasan, Jinge Wu, Quang Ngoc Nguyen, Salomé Andres, Imane Guellil, Huayu Zhang, Arlene Casey, Beatrice Alex, Bruce Guthrie, Honghan Wu

This study introduces a novel knowledge enhanced tokenisation mechanism,
K-Tokeniser, for clinical text processing. Technically, at initialisation
stage, K-Tokeniser populates global representations of tokens based on semantic
types of domain concepts (such as drugs or diseases) from either a domain
ontology like Unified Medical Language System or the training data of the task
related corpus. At training or inference stage, sentence level localised
context will be utilised for choosing the optimal global token representation
to realise the semantic-based tokenisation. To avoid pretraining using the new
tokeniser, an embedding initialisation approach is proposed to generate
representations for new tokens. Using three transformer-based language models,
a comprehensive set of experiments are conducted on four real-world datasets
for evaluating K-Tokeniser in a wide range of clinical text analytics tasks
including clinical concept and relation extraction, automated clinical coding,
clinical phenotype identification, and clinical research article
classification. Overall, our models demonstrate consistent improvements over
their counterparts in all tasks. In particular, substantial improvements are
observed in the automated clinical coding task with 13\% increase on Micro
$F_1$ score. Furthermore, K-Tokeniser also shows significant capacities in
facilitating quicker converge of language models. Specifically, using
K-Tokeniser, the language models would only require 50\% of the training data
to achieve the best performance of the baseline tokeniser using all training
data in the concept extraction task and less than 20\% of the data for the
automated coding task. It is worth mentioning that all these improvements
require no pre-training process, making the approach generalisable.

摘要：本研究引入一種新穎的知識增強標記化機制，K-Tokeniser，用於臨床文本處理。技術上，在初始化階段，K-Tokeniser 會根據來自領域概念（例如藥物或疾病）的語義類型，從統一醫學語言系統或任務相關語料庫的訓練資料中，填充標記的全局表示。在訓練或推論階段，句子級別的局部化上下文將被用於選擇最佳的全局標記表示，以實現基於語義的標記化。為了避免使用新的標記化器進行預訓練，提出了一種嵌入初始化方法，以產生新標記的表示。使用三種基於Transformer的語言模型，對四個真實世界數據集進行了一組全面的實驗，以評估 K-Tokeniser 在廣泛的臨床文本分析任務中的表現，包括臨床概念和關係提取、自動臨床編碼、臨床表型識別和臨床研究文章分類。總體而言，我們的模型在所有任務中都展示出比其對應模型更一致的改進。特別是，在自動臨床編碼任務中觀察到了顯著的改進，Micro $F_1$ 得分提高了 13%。此外，K-Tokeniser 還顯示出顯著的能力，可以促進語言模型更快的收斂。具體來說，使用 K-Tokeniser，語言模型只需要 50% 的訓練數據即可在概念提取任務中達到基線標記化器使用所有訓練數據的最佳性能，而自動編碼任務則不到 20% 的數據。值得一提的是，所有這些改進都不需要預訓練過程，這使得該方法具有普遍性。

##### **Cross-level Requirement Traceability: A Novel Approach Integrating Bag-of-Words and Word Embedding for Enhanced Similarity Functionality**
2406.14310v1 by Baher Mohammad, Riad Sonbol, Ghaida Rebdawi

Requirement traceability is the process of identifying the inter-dependencies
between requirements. It poses a significant challenge when conducted manually,
especially when dealing with requirements at various levels of abstraction. In
this work, we propose a novel approach to automate the task of linking
high-level business requirements with more technical system requirements. The
proposed approach begins by representing each requirement using a Bag of-Words
(BOW) model combined with the Term Frequency-Inverse Document Frequency
(TF-IDF) scoring function. Then, we suggested an enhanced cosine similarity
that uses recent advances in word embedding representation to correct
traditional cosine similarity function limitations. To evaluate the
effectiveness of our approach, we conducted experiments on three well-known
datasets: COEST, WARC(NFR), and WARC(FRS). The results demonstrate that our
approach significantly improves efficiency compared to existing methods. We
achieved better results with an increase of approximately 18.4% in one of the
datasets, as measured by the F2 score.

摘要：需求可追溯性是识别需求之间相互依赖关系的过程。当手动进行时，它会带来重大挑战，尤其是在处理不同抽象级别的需求时。在这项工作中，我们提出了一种新颖的方法来实现将高级业务需求与更多技术系统需求关联的任务自动化。所提出的方法首先使用词袋（BOW）模型结合词频-逆文档频率（TF-IDF）评分函数来表示每个需求。然后，我们提出了一种增强的余弦相似性，该相似性利用词嵌入表示中的最新进展来纠正传统的余弦相似性函数的局限性。为了评估我们方法的有效性，我们对三个众所周知的的数据集进行了实验：COEST、WARC(NFR)和WARC(FRS)。结果表明，与现有方法相比，我们的方法显着提高了效率。在其中一个数据集上，我们取得了更好的结果，F2 分数提高了大约 18.4%。

##### **QuST-LLM: Integrating Large Language Models for Comprehensive Spatial Transcriptomics Analysis**
2406.14307v1 by Chao Hui Huang

In this paper, we introduce QuST-LLM, an innovative extension of QuPath that
utilizes the capabilities of large language models (LLMs) to analyze and
interpret spatial transcriptomics (ST) data. This tool effectively simplifies
the intricate and high-dimensional nature of ST data by offering a
comprehensive workflow that includes data loading, region selection, gene
expression analysis, and functional annotation. QuST-LLM employs LLMs to
transform complex ST data into understandable and detailed biological
narratives based on gene ontology annotations, thereby significantly improving
the interpretability of ST data. Consequently, users can interact with their
own ST data using natural language. Hence, QuST-LLM provides researchers with a
potent functionality to unravel the spatial and functional complexities of
tissues, fostering novel insights and advancements in biomedical research.

摘要：在本文中，我們介紹 QuST-LLM，這是 QuPath 的創新擴充，它利用大型語言模型 (LLM) 的功能來分析和解釋空間轉錄組學 (ST) 數據。此工具有效簡化了 ST 數據的複雜性和高維度特性，提供了一個全面的工作流程，包括數據加載、區域選擇、基因表現分析和功能註釋。QuST-LLM 使用 LLM 將複雜的 ST 數據轉換為可理解且詳細的生物敘述，這些敘述基於基因本體註釋，從而顯著提高了 ST 數據的可解釋性。因此，使用者可以使用自然語言與自己的 ST 數據進行互動。因此，QuST-LLM 為研究人員提供了一項強大的功能，可以解開組織的空間和功能複雜性，促進生物醫學研究的新見解和進展。

##### **AI in Space for Scientific Missions: Strategies for Minimizing Neural-Network Model Upload**
2406.14297v1 by Jonah Ekelund, Ricardo Vinuesa, Yuri Khotyaintsev, Pierre Henri, Gian Luca Delzanno, Stefano Markidis

Artificial Intelligence (AI) has the potential to revolutionize space
exploration by delegating several spacecraft decisions to an onboard AI instead
of relying on ground control and predefined procedures. It is likely that there
will be an AI/ML Processing Unit onboard the spacecraft running an inference
engine. The neural-network will have pre-installed parameters that can be
updated onboard by uploading, by telecommands, parameters obtained by training
on the ground. However, satellite uplinks have limited bandwidth and
transmissions can be costly. Furthermore, a mission operating with a suboptimal
neural network will miss out on valuable scientific data. Smaller networks can
thereby decrease the uplink cost, while increasing the value of the scientific
data that is downloaded. In this work, we evaluate and discuss the use of
reduced-precision and bare-minimum neural networks to reduce the time for
upload. As an example of an AI use case, we focus on the NASA's Magnetosperic
MultiScale (MMS) mission. We show how an AI onboard could be used in the
Earth's magnetosphere to classify data to selectively downlink higher value
data or to recognize a region-of-interest to trigger a burst-mode, collecting
data at a high-rate. Using a simple filtering scheme and algorithm, we show how
the start and end of a region-of-interest can be detected in on a stream of
classifications. To provide the classifications, we use an established
Convolutional Neural Network (CNN) trained to an accuracy >94%. We also show
how the network can be reduced to a single linear layer and trained to the same
accuracy as the established CNN. Thereby, reducing the overall size of the
model by up to 98.9%. We further show how each network can be reduced by up to
75% of its original size, by using lower-precision formats to represent the
network parameters, with a change in accuracy of less than 0.6 percentage
points.

摘要：人工智慧 (AI) 有潛力透過將多項太空船決策委派給機載 AI，而非依賴地面控制和預先定義的程序，進而徹底改變太空探索。太空船上很可能會有一個搭載推理引擎的 AI/ML 處理單元。神經網路會預先安裝可透過上傳、遙控命令更新的參數，這些參數是透過地面訓練取得的。然而，衛星上行鏈路頻寬有限，傳輸可能很昂貴。此外，使用次佳神經網路執行的任務會錯失有價值的科學資料。較小的網路可以降低上行鏈路成本，同時增加下載的科學資料價值。在這項工作中，我們評估並討論使用降精度和極簡神經網路來縮短上傳時間。以 AI 使用案例為例，我們專注於 NASA 的磁層多尺度 (MMS) 任務。我們展示機載 AI 如何在行星磁層中用於分類資料，以選擇性地下傳較高價值的資料，或辨識感興趣區域以觸發爆發模式，以高頻率收集資料。我們使用簡單的篩選機制和演算法，展示如何在分類串流中偵測感興趣區域的開始和結束。為了提供分類，我們使用訓練至準確度 >94% 的已建立卷積神經網路 (CNN)。我們也展示如何將網路縮小為單一線性層，並訓練至與已建立 CNN 相同的準確度。藉此將模型的整體大小減少多達 98.9%。我們進一步展示如何透過使用低精度格式來表示網路參數，將每個網路縮小至其原始大小的 75%，而準確度變化不到 0.6 個百分點。

##### **DASB -- Discrete Audio and Speech Benchmark**
2406.14294v1 by Pooneh Mousavi, Luca Della Libera, Jarod Duret, Artem Ploujnikov, Cem Subakan, Mirco Ravanelli

Discrete audio tokens have recently gained considerable attention for their
potential to connect audio and language processing, enabling the creation of
modern multimodal large language models. Ideal audio tokens must effectively
preserve phonetic and semantic content along with paralinguistic information,
speaker identity, and other details. While several types of audio tokens have
been recently proposed, identifying the optimal tokenizer for various tasks is
challenging due to the inconsistent evaluation settings in existing studies. To
address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a
comprehensive leaderboard for benchmarking discrete audio tokens across a wide
range of discriminative tasks, including speech recognition, speaker
identification and verification, emotion recognition, keyword spotting, and
intent classification, as well as generative tasks such as speech enhancement,
separation, and text-to-speech. Our results show that, on average, semantic
tokens outperform compression tokens across most discriminative and generative
tasks. However, the performance gap between semantic tokens and standard
continuous representations remains substantial, highlighting the need for
further research in this field.

摘要：離散音訊代碼最近獲得相當多的關注，因為它們有潛力連結音訊和語言處理，進而建立現代多模態大型語言模型。理想的音訊代碼必須有效保留音標和語義內容，以及語言外的資訊、說話者身分和其他細節。雖然最近已提出數種類型的音訊代碼，但由於現有研究中的評估設定不一致，因此找出各種任務的最佳代碼化器具有挑戰性。為了解決這個問題，我們發布了離散音訊和語音基準 (DASB)，這是一個全面的排行榜，用於評量各種離散音訊代碼，涵蓋廣泛的區分任務，包括語音辨識、說話者識別和驗證、情緒辨識、關鍵字點選和意圖分類，以及產生任務，例如語音增強、分離和文字轉語音。我們的結果顯示，平均來說，語義代碼在大部分區分和產生任務中都優於壓縮代碼。然而，語義代碼和標準連續表示之間的效能差距仍然很大，這凸顯了進一步研究此領域的必要性。

##### **VAIYAKARANA : A Benchmark for Automatic Grammar Correction in Bangla**
2406.14284v1 by Pramit Bhattacharyya, Arnab Bhattacharya

Bangla (Bengali) is the fifth most spoken language globally and, yet, the
problem of automatic grammar correction in Bangla is still in its nascent
stage. This is mostly due to the need for a large corpus of grammatically
incorrect sentences, with their corresponding correct counterparts. The present
state-of-the-art techniques to curate a corpus for grammatically wrong
sentences involve random swapping, insertion and deletion of words.
However,these steps may not always generate grammatically wrong sentences in
Bangla. In this work, we propose a pragmatic approach to generate grammatically
wrong sentences in Bangla. We first categorize the different kinds of errors in
Bangla into 5 broad classes and 12 finer classes. We then use these to generate
grammatically wrong sentences systematically from a correct sentence. This
approach can generate a large number of wrong sentences and can, thus, mitigate
the challenge of lacking a large corpus for neural networks. We provide a
dataset, Vaiyakarana, consisting of 92,830 grammatically incorrect sentences as
well as 18,426 correct sentences. We also collected 619 human-generated
sentences from essays written by Bangla native speakers. This helped us to
understand errors that are more frequent. We evaluated our corpus against
neural models and LLMs and also benchmark it against human evaluators who are
native speakers of Bangla. Our analysis shows that native speakers are far more
accurate than state-of-the-art models to detect whether the sentence is
grammatically correct. Our methodology of generating erroneous sentences can be
applied for most other Indian languages as well.

摘要：孟加拉語（孟加拉語）是全球第五大語言，但孟加拉語的自動語法糾正問題仍處於起步階段。這主要是因為需要大量的語法錯誤句子及其相應的正確對應項。目前用於整理語法錯誤句子的最先進技術包括隨機交換、插入和刪除單詞。然而，這些步驟並不總能產生孟加拉語的語法錯誤句子。在這項工作中，我們提出了一種實用的方法來生成孟加拉語的語法錯誤句子。我們首先將孟加拉語的不同類型的錯誤分類為 5 個廣義類別和 12 個更精細的類別。然後，我們使用這些類別從正確的句子中系統地生成語法錯誤的句子。這種方法可以生成大量的錯誤句子，從而可以緩解神經網路缺乏大型語料庫的挑戰。我們提供了一個名為 Vaiyakarana 的數據集，其中包含 92,830 個語法錯誤句子和 18,426 個正確句子。我們還從孟加拉語母語人士撰寫的論文中收集了 619 個人工生成的句子。這幫助我們了解更常見的錯誤。我們根據神經模型和 LLM 評估了我們的語料庫，並根據孟加拉語母語的人類評估員對其進行了基準測試。我們的分析表明，母語人士比最先進的模型更準確地檢測句子是否語法正確。我們生成錯誤句子的方法也可以應用於大多數其他印度語言。

##### **Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning**
2406.14283v1 by Chaojie Wang, Yanchen Deng, Zhiyi Lv, Shuicheng Yan, An Bo

Large Language Models (LLMs) have demonstrated impressive capability in many
nature language tasks. However, the auto-regressive generation process makes
LLMs prone to produce errors, hallucinations and inconsistent statements when
performing multi-step reasoning. In this paper, we aim to alleviate the
pathology by introducing Q*, a general, versatile and agile framework for
guiding LLMs decoding process with deliberative planning. By learning a
plug-and-play Q-value model as heuristic function, our Q* can effectively guide
LLMs to select the most promising next step without fine-tuning LLMs for each
task, which avoids the significant computational overhead and potential risk of
performance degeneration on other tasks. Extensive experiments on GSM8K, MATH
and MBPP confirm the superiority of our method.

摘要：大型語言模型 (LLM) 已在許多自然語言任務中展現出驚人的能力。然而，自回歸生成過程使 LLM 在執行多步驟推理時易於產生錯誤、幻覺和不一致的陳述。在本文中，我們旨在透過引入 Q* 來緩解病理，Q* 是用於指導 LLM 解碼過程的通用、多功能且靈活的架構，並具備審慎規劃能力。透過學習即插即用的 Q 值模型作為啟發式函數，我們的 Q* 可以有效引導 LLM 選擇最有希望的下一步，而無需針對每個任務微調 LLM，這避免了顯著的計算開銷和在其他任務上效能下降的潛在風險。在 GSM8K、MATH 和 MBPP 上進行的廣泛實驗證實了我們方法的優越性。

##### **Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs**
2406.14282v1 by Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, Huajun Chen

Improving the performance of large language models (LLMs) in complex
question-answering (QA) scenarios has always been a research focal point.
Recent studies have attempted to enhance LLMs' performance by combining
step-wise planning with external retrieval. While effective for advanced models
like GPT-3.5, smaller LLMs face challenges in decomposing complex questions,
necessitating supervised fine-tuning. Previous work has relied on manual
annotation and knowledge distillation from teacher LLMs, which are
time-consuming and not accurate enough. In this paper, we introduce a novel
framework for enhancing LLMs' planning capabilities by using planning data
derived from knowledge graphs (KGs). LLMs fine-tuned with this data have
improved planning capabilities, better equipping them to handle complex QA
tasks that involve retrieval. Evaluations on multiple datasets, including our
newly proposed benchmark, highlight the effectiveness of our framework and the
benefits of KG-derived planning data.

摘要：<paragraph>改善大型語言模型 (LLM) 在複雜問答 (QA) 情境中的效能一直是研究重點。最近的研究嘗試透過結合逐步規劃與外部擷取來增強 LLM 的效能。雖然對於 GPT-3.5 等進階模型來說很有效，但較小的 LLM 在分解複雜問題時會面臨挑戰，因此需要監督微調。先前的研究仰賴人工標註和教師 LLM 的知識萃取，這耗時且不夠精確。在本文中，我們介紹一個創新的架構，透過使用從知識圖譜 (KG) 中衍生的規劃資料來增強 LLM 的規劃能力。使用此資料微調的 LLM 改善了規劃能力，讓它們更能處理涉及擷取的複雜 QA 任務。在多個資料集（包括我們新提出的基準）上的評估突顯了我們架構的有效性，以及 KG 衍生規劃資料的好處。</paragraph>

##### **FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and explainability**
2406.14281v1 by Md Fahim Sikder, Resmi Ramachandranpillai, Daniel de Leng, Fredrik Heintz

We present FairX, an open-source Python-based benchmarking tool designed for
the comprehensive analysis of models under the umbrella of fairness, utility,
and eXplainability (XAI). FairX enables users to train benchmarking
bias-removal models and evaluate their fairness using a wide array of fairness
metrics, data utility metrics, and generate explanations for model predictions,
all within a unified framework. Existing benchmarking tools do not have the way
to evaluate synthetic data generated from fair generative models, also they do
not have the support for training fair generative models either. In FairX, we
add fair generative models in the collection of our fair-model library
(pre-processing, in-processing, post-processing) and evaluation metrics for
evaluating the quality of synthetic fair data. This version of FairX supports
both tabular and image datasets. It also allows users to provide their own
custom datasets. The open-source FairX benchmarking package is publicly
available at https://github.com/fahim-sikder/FairX.

摘要：我們提出 FairX，一種基於 Python 的開放原始碼基準測試工具，專為在公平性、效用和可解釋性 (XAI) 的範疇下對模型進行全面分析而設計。FairX 使用戶能夠訓練基準測試的偏見移除模型，並使用廣泛的公平性指標、資料效用指標評估其公平性，並為模型預測產生解釋，所有這些都在一個統一的框架內進行。現有的基準測試工具無法評估由公平生成模型產生的合成資料，而且也不支援訓練公平生成模型。在 FairX 中，我們在公平模型庫 (預處理、處理中、後處理) 的集合中增加了公平生成模型，以及用於評估合成公平資料品質的評估指標。此版本的 FairX 支援表格和影像資料集。它也允許使用者提供自己的自訂資料集。開放原始碼 FairX 基準測試套件已公開於 https://github.com/fahim-sikder/FairX。

##### **Augmenting Query and Passage for Retrieval-Augmented Generation using LLMs for Open-Domain Question Answering**
2406.14277v1 by Minsang Kim, Cheoneum Park, Seungjun Baek

Retrieval-augmented generation (RAG) has received much attention for
Open-domain question-answering (ODQA) tasks as a means to compensate for the
parametric knowledge of large language models (LLMs). While previous approaches
focused on processing retrieved passages to remove irrelevant context, they
still rely heavily on the quality of retrieved passages which can degrade if
the question is ambiguous or complex. In this paper, we propose a simple yet
efficient method called question and passage augmentation via LLMs for
open-domain QA. Our method first decomposes the original questions into
multiple-step sub-questions. By augmenting the original question with detailed
sub-questions and planning, we are able to make the query more specific on what
needs to be retrieved, improving the retrieval performance. In addition, to
compensate for the case where the retrieved passages contain distracting
information or divided opinions, we augment the retrieved passages with
self-generated passages by LLMs to guide the answer extraction. Experimental
results show that the proposed scheme outperforms the previous state-of-the-art
and achieves significant performance gain over existing RAG methods.

摘要：檢索增強生成 (RAG) 已廣受關注，可用於開放領域問答 (ODQA) 任務，作為彌補大型語言模型 (LLM) 參數知識的一種方式。雖然先前的做法側重於處理檢索到的段落以移除無關的內容，但它們仍然高度依賴於檢索到的段落的品質，如果問題模稜兩可或複雜，品質可能會下降。在本文中，我們提出了一種簡單但有效的方法，稱為透過 LLM 進行問題與段落增強，以進行開放領域問答。我們的做法首先將原始問題分解成多步驟子問題。透過增強原始問題，加入詳細的子問題和規劃，我們能夠讓查詢更具體說明需要檢索的內容，進而提升檢索效能。此外，為了彌補檢索到的段落包含令人分心的資訊或意見分歧的情況，我們利用 LLM 自行產生的段落增強檢索到的段落，以引導答案萃取。實驗結果顯示，建議的架構優於先前的技術水準，並且在現有的 RAG 方法中獲得顯著的效能提升。

##### **Step-Back Profiling: Distilling User History for Personalized Scientific Writing**
2406.14275v1 by Xiangru Tang, Xingyao Zhang, Yanjun Shao, Jie Wu, Yilun Zhao, Arman Cohan, Ming Gong, Dongmei Zhang, Mark Gerstein

Large language models (LLMs) excel at a variety of natural language
processing tasks, yet they struggle to generate personalized content for
individuals, particularly in real-world scenarios like scientific writing.
Addressing this challenge, we introduce Step-Back Profiling to personalize LLMs
by distilling user history into concise profiles, including essential traits
and preferences of users. Regarding our experiments, we construct a
Personalized Scientific Writing (PSW) dataset to study multiuser
personalization. PSW requires the models to write scientific papers given
specialized author groups with diverse academic backgrounds. As for the
results, we demonstrate the effectiveness of capturing user characteristics via
Step-Back Profiling for collaborative writing. Moreover, our approach
outperforms the baselines by up to 3.6 points on the general personalization
benchmark (LaMP), including 7 personalization LLM tasks. Our extensive ablation
studies validate the contributions of different components in our method and
provide insights into our task definition. Our dataset and code are available
at \url{https://github.com/gersteinlab/step-back-profiling}.

摘要：大型語言模型 (LLM) 在各種自然語言處理任務中表現出色，但它們難以針對個人產生個人化內容，尤其是在科學寫作等真實世界的場景中。為了解決這個挑戰，我們引入了回溯分析，透過將使用者歷史精煉成簡潔的個人資料，包括使用者的基本特質和偏好，來讓 LLM 個人化。關於我們的實驗，我們建構了一個科學寫作個人化 (PSW) 資料集，以研究多使用者個人化。PSW 要求模型撰寫科學論文，給定具有不同學術背景的專業作者群組。至於結果，我們展示了透過回溯分析擷取使用者特徵，對於協作寫作的有效性。此外，我們的做法在一般個人化基準 (LaMP) 上，包括 7 項個人化 LLM 任務，比基準高出 3.6 分。我們廣泛的消融研究驗證了我們方法中不同組成部分的貢獻，並提供了對我們任務定義的見解。我們的資料集和程式碼可在 \url{https://github.com/gersteinlab/step-back-profiling} 取得。

##### **On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?**
2406.14267v1 by Rochelle Choenni, Sara Rajaee, Christof Monz, Ekaterina Shutova

While multilingual language models (MLMs) have been trained on 100+
languages, they are typically only evaluated across a handful of them due to a
lack of available test data in most languages. This is particularly problematic
when assessing MLM's potential for low-resource and unseen languages. In this
paper, we present an analysis of existing evaluation frameworks in multilingual
NLP, discuss their limitations, and propose several directions for more robust
and reliable evaluation practices. Furthermore, we empirically study to what
extent machine translation offers a {reliable alternative to human translation}
for large-scale evaluation of MLMs across a wide set of languages. We use a
SOTA translation model to translate test data from 4 tasks to 198 languages and
use them to evaluate three MLMs. We show that while the selected subsets of
high-resource test languages are generally sufficiently representative of a
wider range of high-resource languages, we tend to overestimate MLMs' ability
on low-resource languages. Finally, we show that simpler baselines can achieve
relatively strong performance without having benefited from large-scale
multilingual pretraining.

摘要：儘管多國語言模型 (MLM) 已針對 100 多種語言進行訓練，但由於大多數語言缺乏可用的測試資料，因此通常僅在少數語言中進行評估。在評估 MLM 對低資源和未知語言的潛力時，這尤其成問題。在本文中，我們分析了多國語言 NLP 中現有的評估架構，討論其限制，並提出多項方針以獲得更穩健且可靠的評估實務。此外，我們實證研究機器翻譯在多大程度上提供了一個 {可靠的人工翻譯替代方案}，用於跨廣泛語言集對 MLM 進行大規模評估。我們使用 SOTA 翻譯模型將 4 個任務的測試資料翻譯成 198 種語言，並使用它們來評估三個 MLM。我們表明，儘管所選的高資源測試語言子集通常足以代表更廣泛的高資源語言，但我們往往會高估 MLM 對低資源語言的能力。最後，我們表明，較簡單的基準線可以在沒有受益於大規模多國語言預訓練的情況下，獲得相對強大的效能。

##### **Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries**
2406.14266v1 by Anna Wróblewska, Marcel Witas, Kinga Frańczak, Arkadiusz Kniaź, Siew Ann Cheong, Tan Seng Chee, Janusz Hołyst, Marcin Paprzycki

Recently, multiple applications of machine learning have been introduced.
They include various possibilities arising when image analysis methods are
applied to, broadly understood, video streams. In this context, a novel tool,
developed for academic educators to enhance the teaching process by automating,
summarizing, and offering prompt feedback on conducting lectures, has been
developed. The implemented prototype utilizes machine learning-based techniques
to recognise selected didactic and behavioural teachers' features within
lecture video recordings.
  Specifically, users (teachers) can upload their lecture videos, which are
preprocessed and analysed using machine learning models. Next, users can view
summaries of recognized didactic features through interactive charts and
tables. Additionally, stored ML-based prediction results support comparisons
between lectures based on their didactic content. In the developed application
text-based models trained on lecture transcriptions, with enhancements to the
transcription quality, by adopting an automatic speech recognition solution are
applied. Furthermore, the system offers flexibility for (future) integration of
new/additional machine-learning models and software modules for image and video
analysis.

摘要：近期，機器學習的多種應用已陸續推出。
其中包括將影像分析方法應用於廣義的影片串流時出現的各種可能性。在此脈絡下，已開發出一種新工具，供學術教育者透過自動化、摘要和提供關於進行演講的即時回饋來增強教學過程。已實作的原型利用基於機器學習的技術，在演講影片錄製中辨識選定的教學和行為教師特徵。
具體來說，使用者（教師）可以上傳他們的演講影片，這些影片會使用機器學習模型進行預處理和分析。接著，使用者可以透過互動式圖表和表格檢視已辨識教學特徵的摘要。此外，儲存的基於 ML 的預測結果支援根據教學內容比較演講。在已開發的應用程式中，已套用在演講轉錄上訓練的基於文字的模型，並透過採用自動語音辨識解決方案來提升轉錄品質。此外，此系統提供彈性，可（未來）整合新的/額外的機器學習模型和用於影像和影片分析的軟體模組。

##### **VeriFlow: Modeling Distributions for Neural Network Verification**
2406.14265v1 by Faried Abu Zaid, Daniel Neider, Mustafa Yalçıner

Formal verification has emerged as a promising method to ensure the safety
and reliability of neural networks. Naively verifying a safety property amounts
to ensuring the safety of a neural network for the whole input space
irrespective of any training or test set. However, this also implies that the
safety of the neural network is checked even for inputs that do not occur in
the real-world and have no meaning at all, often resulting in spurious errors.
To tackle this shortcoming, we propose the VeriFlow architecture as a flow
based density model tailored to allow any verification approach to restrict its
search to the some data distribution of interest. We argue that our
architecture is particularly well suited for this purpose because of two major
properties. First, we show that the transformation and log-density function
that are defined by our model are piece-wise affine. Therefore, the model
allows the usage of verifiers based on SMT with linear arithmetic. Second,
upper density level sets (UDL) of the data distribution take the shape of an
$L^p$-ball in the latent space. As a consequence, representations of UDLs
specified by a given probability are effectively computable in latent space.
This allows for SMT and abstract interpretation approaches with fine-grained,
probabilistically interpretable, control regarding on how (a)typical the inputs
subject to verification are.

摘要：形式驗證已成為確保神經網路安全性和可靠性的有前途方法。天真地驗證安全屬性等於確保神經網路在整個輸入空間中的安全性，而不考慮任何訓練或測試集。然而，這也意味著即使對於在現實世界中不會出現且完全沒有意義的輸入，也會檢查神經網路的安全性，這通常會導致虛假錯誤。為了解決這個缺點，我們提出 VeriFlow 架構作為一個基於流的密度模型，專門用於允許任何驗證方法將其搜尋限制在感興趣的某些資料分佈上。我們認為我們的架構特別適合這個目的，原因有兩個主要特性。首先，我們展示由我們的模型定義的轉換和對數密度函數是分段仿射的。因此，該模型允許使用基於線性算術的 SMT 驗證器。其次，資料分佈的上密度層級集 (UDL) 在潛在空間中呈現出 $L^p$-球的形狀。因此，由給定機率指定的 UDL 表示在潛在空間中可以有效計算。這允許使用 SMT 和抽象詮釋方法，對輸入的 (a) 典型性進行精細、可機率詮釋的控制，以進行驗證。

##### **CityNav: Language-Goal Aerial Navigation Dataset with Geographic Information**
2406.14240v1 by Jungdae Lee, Taiki Miyanishi, Shuhei Kurita, Koya Sakamoto, Daichi Azuma, Yutaka Matsuo, Nakamasa Inoue

Vision-and-language navigation (VLN) aims to guide autonomous agents through
real-world environments by integrating visual and linguistic cues. While
substantial progress has been made in understanding these interactive
modalities in ground-level navigation, aerial navigation remains largely
underexplored. This is primarily due to the scarcity of resources suitable for
real-world, city-scale aerial navigation studies. To bridge this gap, we
introduce CityNav, a new dataset for language-goal aerial navigation using a 3D
point cloud representation from real-world cities. CityNav includes 32,637
natural language descriptions paired with human demonstration trajectories,
collected from participants via a new web-based 3D simulator developed for this
research. Each description specifies a navigation goal, leveraging the names
and locations of landmarks within real-world cities. We also provide baseline
models of navigation agents that incorporate an internal 2D spatial map
representing landmarks referenced in the descriptions. We benchmark the latest
aerial navigation baselines and our proposed model on the CityNav dataset. The
results using this dataset reveal the following key findings: (i) Our aerial
agent models trained on human demonstration trajectories outperform those
trained on shortest path trajectories, highlighting the importance of
human-driven navigation strategies; (ii) The integration of a 2D spatial map
significantly enhances navigation efficiency at city scale. Our dataset and
code are available at https://water-cookie.github.io/city-nav-proj/

摘要：視覺和語言導航 (VLN) 旨在透過整合視覺和語言提示，引導自主代理穿過真實世界的環境。雖然在理解地面導航中的這些互動模式方面已取得重大進展，但空中導航仍未得到充分探索。這主要是由於缺乏適合於真實世界、城市規模的空中導航研究的資源。為了彌補這一差距，我們引入了 CityNav，這是一個新的語言目標空中導航數據集，使用來自真實世界城市的 3D 點雲表示。CityNav 包含 32,637 個自然語言描述，並配有人類示範軌跡，這些軌跡是通過一個為此研究開發的新型網路 3D 模擬器從參與者那裡收集的。每個描述都指定了一個導航目標，利用真實世界城市中地標的名稱和位置。我們還提供了導航代理的基準模型，其中包含一個內部 2D 空間地圖，表示描述中引用的地標。我們在 CityNav 數據集上對最新的空中導航基準和我們提出的模型進行了基準測試。使用此數據集的結果揭示了以下關鍵發現：(i) 我們訓練於人類示範軌跡上的空中代理模型優於訓練於最短路徑軌跡上的模型，突出了人類驅動導航策略的重要性；(ii) 2D 空間地圖的整合顯著提高了城市規模的導航效率。我們的數據集和代碼可在 https://water-cookie.github.io/city-nav-proj/ 獲得。

##### **Enhancing robustness of data-driven SHM models: adversarial training with circle loss**
2406.14232v1 by Xiangli Yang, Xijie Deng, Hanwei Zhang, Yang Zou, Jianxi Yang

Structural health monitoring (SHM) is critical to safeguarding the safety and
reliability of aerospace, civil, and mechanical infrastructure. Machine
learning-based data-driven approaches have gained popularity in SHM due to
advancements in sensors and computational power. However, machine learning
models used in SHM are vulnerable to adversarial examples -- even small changes
in input can lead to different model outputs. This paper aims to address this
problem by discussing adversarial defenses in SHM. In this paper, we propose an
adversarial training method for defense, which uses circle loss to optimize the
distance between features in training to keep examples away from the decision
boundary. Through this simple yet effective constraint, our method demonstrates
substantial improvements in model robustness, surpassing existing defense
mechanisms.

摘要：結構健康監測 (SHM) 對保障航太、土木和機械基礎設施的安全和可靠性至關重要。機器學習為基礎的資料驅動方法由於感測器和計算能力的進步，在 SHM 中獲得普及。然而，用於 SHM 的機器學習模型容易受到對抗性範例的影響——輸入的微小變更甚至可能導致不同的模型輸出。本文旨在透過討論 SHM 中的對抗性防禦來解決此問題。在本文中，我們提出了一種用於防禦的對抗性訓練方法，該方法使用圓形損失來最佳化訓練中特徵之間的距離，以使範例遠離決策邊界。透過這個簡單但有效的約束，我們的模型展示了模型穩健性的顯著改善，超越了現有的防禦機制。

##### **Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing**
2406.14230v1 by Han Jiang, Xiaoyuan Yi, Zhihua Wei, Shu Wang, Xing Xie

Warning: this paper contains model outputs exhibiting unethical information.
Large Language Models (LLMs) have achieved significant breakthroughs, but their
generated unethical content poses potential risks. Measuring value alignment of
LLMs becomes crucial for their regulation and responsible deployment. Numerous
datasets have been constructed to assess social bias, toxicity, and ethics in
LLMs, but they suffer from evaluation chronoeffect, that is, as models rapidly
evolve, existing data becomes leaked or undemanding, overestimating
ever-developing LLMs. To tackle this problem, we propose GETA, a novel
generative evolving testing approach that dynamically probes the underlying
moral baselines of LLMs. Distinct from previous adaptive testing methods that
rely on static datasets with limited difficulty, GETA incorporates an
iteratively-updated item generator which infers each LLM's moral boundaries and
generates difficulty-tailored testing items, accurately reflecting the true
alignment extent. This process theoretically learns a joint distribution of
item and model response, with item difficulty and value conformity as latent
variables, where the generator co-evolves with the LLM, addressing
chronoeffect. We evaluate various popular LLMs with diverse capabilities and
demonstrate that GETA can create difficulty-matching testing items and more
accurately assess LLMs' values, better consistent with their performance on
unseen OOD and i.i.d. items, laying the groundwork for future evaluation
paradigms.

摘要：警告：本文包含展示不道德資訊的模型輸出。
大型語言模型 (LLM) 已取得重大突破，但它們產生的不道德內容構成潛在風險。衡量 LLM 的價值觀一致性對於其監管和負責任的部署至關重要。已經建構了大量資料集來評估 LLM 中的社會偏見、毒性和道德，但它們存在評估時間效應，也就是隨著模型快速演進，現有資料會洩露或變得不具挑戰性，高估持續發展的 LLM。為了解決這個問題，我們提出 GETA，一種新的生成式演化測試方法，動態探測 LLM 的基本道德基準。有別於依賴難度有限的靜態資料集的先前適應性測試方法，GETA 結合了一個反覆更新的項目產生器，它推論每個 LLM 的道德界限並產生量身打造的測試項目，準確反映真正的對齊程度。此過程理論上會學習項目和模型回應的聯合分佈，其中項目難度和價值符合性為潛在變數，產生器與 LLM 共同演化，解決時間效應。我們評估了各種具有不同功能的熱門 LLM，並證明 GETA 可以建立難度匹配的測試項目，更準確地評估 LLM 的價值觀，更符合它們在未見過 OOD 和 i.i.d. 項目上的表現，為未來的評估範例奠定基礎。

##### **EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms**
2406.14228v1 by Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, Deqing Yang

The rise of powerful large language models (LLMs) has spurred a new trend in
building LLM-based autonomous agents for solving complex tasks, especially
multi-agent systems. Despite the remarkable progress, we notice that existing
works are heavily dependent on human-designed frameworks, which greatly limits
the functional scope and scalability of agent systems. How to automatically
extend the specialized agent to multi-agent systems to improve task-solving
capability still remains a significant challenge. In this paper, we introduce
EvoAgent, a generic method to automatically extend expert agents to multi-agent
systems via the evolutionary algorithm, thereby improving the effectiveness of
LLM-based agents in solving tasks. Specifically, we consider the existing agent
frameworks as the initial individual and then apply a series of evolutionary
operators (e.g., mutation, crossover, selection, etc.) to generate multiple
agents with diverse agent settings. EvoAgent can be generalized to any
LLM-based agent framework, and can automatically extend the existing agent
framework to multi-agent systems without any extra human designs. Experimental
results across various tasks have shown that EvoAgent can automatically
generate multiple expert agents and significantly enhance the task-solving
capabilities of LLM-based agents.

摘要：強大的大型語言模型 (LLM) 的興起，刺激了建立基於 LLM 的自主代理來解決複雜任務的新趨勢，尤其是多代理系統。儘管取得了顯著進展，我們注意到現有作品嚴重依賴於人類設計的框架，這極大地限制了代理系統的功能範圍和可擴展性。如何自動將專用代理擴展到多代理系統以提高任務解決能力仍然是一個重大的挑戰。在本文中，我們介紹了 EvoAgent，這是一種通過演算法自動將專家代理擴展到多代理系統的通用方法，從而提高了基於 LLM 的代理在解決任務中的有效性。具體來說，我們將現有的代理框架視為初始個體，然後應用一系列演化運算子（例如，突變、交叉、選擇等）來生成具有不同代理設定的多個代理。EvoAgent 可以推廣到任何基於 LLM 的代理框架，並且可以自動將現有的代理框架擴展到多代理系統，而無需任何額外的人工設計。跨各種任務的實驗結果表明，EvoAgent 可以自動生成多個專家代理，並顯著增強基於 LLM 的代理的任務解決能力。

##### **REVEAL-IT: REinforcement learning with Visibility of Evolving Agent poLicy for InTerpretability**
2406.14214v1 by Shuang Ao, Simon Khan, Haris Aziz, Flora D. Salim

Understanding the agent's learning process, particularly the factors that
contribute to its success or failure post-training, is crucial for
comprehending the rationale behind the agent's decision-making process. Prior
methods clarify the learning process by creating a structural causal model
(SCM) or visually representing the distribution of value functions.
Nevertheless, these approaches have constraints as they exclusively function in
2D-environments or with uncomplicated transition dynamics. Understanding the
agent's learning process in complicated environments or tasks is more
challenging. In this paper, we propose REVEAL-IT, a novel framework for
explaining the learning process of an agent in complex environments. Initially,
we visualize the policy structure and the agent's learning process for various
training tasks. By visualizing these findings, we can understand how much a
particular training task or stage affects the agent's performance in test.
Then, a GNN-based explainer learns to highlight the most important section of
the policy, providing a more clear and robust explanation of the agent's
learning process. The experiments demonstrate that explanations derived from
this framework can effectively help in the optimization of the

摘要：了解代理的學習過程，特別是導致其在訓練後成功或失敗的因素，對於理解代理決策過程背後的原理至關重要。先前的研究方法透過建立結構性因果模型 (SCM) 或視覺化呈現價值函數的分布，來釐清學習過程。然而，這些方法有其限制，因為它們只能用於 2D 環境或具有簡單轉換動態的環境。在複雜的環境或任務中了解代理的學習過程更具挑戰性。在本文中，我們提出了 REVEAL-IT，這是一個用於解釋代理在複雜環境中學習過程的新框架。最初，我們將策略結構和代理在各種訓練任務中的學習過程視覺化。透過視覺化這些發現，我們可以了解特定訓練任務或階段如何影響代理在測試中的表現。接著，一個基於 GNN 的解釋器會學習強調策略中最重要的部分，提供更清晰且穩健的代理學習過程說明。實驗證明，從這個框架中衍生的說明可以有效地幫助最佳化

##### **Complexity of Symbolic Representation in Working Memory of Transformer Correlates with the Complexity of a Task**
2406.14213v1 by Alsu Sagirova, Mikhail Burtsev

Even though Transformers are extensively used for Natural Language Processing
tasks, especially for machine translation, they lack an explicit memory to
store key concepts of processed texts. This paper explores the properties of
the content of symbolic working memory added to the Transformer model decoder.
Such working memory enhances the quality of model predictions in machine
translation task and works as a neural-symbolic representation of information
that is important for the model to make correct translations. The study of
memory content revealed that translated text keywords are stored in the working
memory, pointing to the relevance of memory content to the processed text.
Also, the diversity of tokens and parts of speech stored in memory correlates
with the complexity of the corpora for machine translation task.

摘要：儘管 Transformer 廣泛用於自然語言處理任務，特別是機器翻譯，但它們缺乏明確的記憶體來儲存已處理文本的重要概念。本文探討了加入 Transformer 模型解碼器的符號工作記憶內容的特性。此類工作記憶增強了機器翻譯任務中模型預測的品質，並作為模型進行正確翻譯所需的重要資訊的神經符號表徵。對記憶體內容的研究顯示，已翻譯文本的關鍵字儲存在工作記憶中，指出記憶體內容與已處理文本之間的相關性。此外，儲存在記憶體中的標記和詞性種類的多樣性與機器翻譯任務語料庫的複雜性相關。

##### **SeCoKD: Aligning Large Language Models for In-Context Learning with Fewer Shots**
2406.14208v1 by Weixing Wang, Haojin Yang, Christoph Meinel

Previous studies have shown that demonstrations can significantly help Large
Language Models (LLMs ) perform better on the given tasks. However, this
so-called In-Context Learning ( ICL ) ability is very sensitive to the
presenting context, and often dozens of demonstrations are needed. In this
work, we investigate if we can reduce the shot number while still maintaining a
competitive performance. We present SeCoKD, a self-Knowledge Distillation ( KD
) training framework that aligns the student model with a heavily prompted
variation, thereby increasing the utilization of a single demonstration. We
experiment with the SeCoKD across three LLMs and six benchmarks focusing mainly
on reasoning tasks. Results show that our method outperforms the base model and
Supervised Fine-tuning ( SFT ), especially in zero-shot and one-shot settings
by 30% and 10%, respectively. Moreover, SeCoKD brings little negative artifacts
when evaluated on new tasks, which is more robust than Supervised Fine-tuning.

摘要：先前的研究表明，示範可以顯著幫助大型語言模型 (LLM) 在既定任務上表現得更好。然而，這種所謂的「情境中學習」(ICL) 能力對呈現情境非常敏感，而且通常需要數十個示範。在這項工作中，我們探討是否可以在維持競爭力表現的同時減少示範次數。我們提出 SeCoKD，一種自我知識蒸餾 (KD) 訓練架構，它將學生模型與大量提示的變體對齊，從而增加單一示範的利用率。我們使用 SeCoKD 針對三個 LLM 和六個基準進行實驗，主要專注於推理任務。結果表明，我們的方法在零次示範和一次示範設置中分別優於基礎模型和監督微調 (SFT) 30% 和 10%。此外，SeCoKD 在評估新任務時幾乎沒有帶來負面影響，這比監督微調更強大。

##### **On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning**
2406.14197v1 by Franz Nowak, Anej Svete, Alexandra Butoi, Ryan Cotterell

The performance of modern language models (LMs) has been improved by
chain-of-thought (CoT) reasoning, i.e., the process of generating intermediate
results that guide the model towards a final answer. A possible explanation for
this improvement is that CoT reasoning extends an LM's computational power, as
RNNs and transformers with additional scratch space are known to be Turing
complete. Comparing LMs to Turing machines, however, introduces a category
error - Turing machines decide language membership, whereas LMs define
distributions over strings. To bridge this gap, we formalize CoT reasoning in a
probabilistic setting. We present several results on the representational
capacity of recurrent and transformer LMs with CoT reasoning, showing that they
can represent the same family of distributions over strings as probabilistic
Turing machines.

摘要：現代語言模型 (LM) 的效能已透過思考鏈 (CoT) 推理獲得改善，亦即產生引導模型朝向最終答案的中間結果的過程。對這項改善的一種可能解釋是，CoT 推理擴展了 LM 的運算能力，因為已知具備額外暫存空間的 RNN 和轉換器是圖靈完備的。然而，將 LM 與圖靈機進行比較會造成類別錯誤 - 圖靈機決定語言成員資格，而 LM 定義字串上的分佈。為了彌合這個差距，我們在機率設定中形式化 CoT 推理。我們提出關於具有 CoT 推理的遞迴和轉換器 LM 的表徵能力的若干結果，顯示它們可以表徵與機率圖靈機相同的字串分佈家族。

##### **VLBiasBench: A Comprehensive Benchmark for Evaluating Bias in Large Vision-Language Model**
2406.14194v1 by Jie Zhang, Sibo Wang, Xiangkui Cao, Zheng Yuan, Shiguang Shan, Xilin Chen, Wen Gao

The emergence of Large Vision-Language Models (LVLMs) marks significant
strides towards achieving general artificial intelligence. However, these
advancements are tempered by the outputs that often reflect biases, a concern
not yet extensively investigated. Existing benchmarks are not sufficiently
comprehensive in evaluating biases due to their limited data scale, single
questioning format and narrow sources of bias. To address this problem, we
introduce VLBiasBench, a benchmark aimed at evaluating biases in LVLMs
comprehensively. In VLBiasBench, we construct a dataset encompassing nine
distinct categories of social biases, including age, disability status, gender,
nationality, physical appearance, race, religion, profession, social economic
status and two intersectional bias categories (race x gender, and race x social
economic status). To create a large-scale dataset, we use Stable Diffusion XL
model to generate 46,848 high-quality images, which are combined with different
questions to form 128,342 samples. These questions are categorized into open
and close ended types, fully considering the sources of bias and
comprehensively evaluating the biases of LVLM from multiple perspectives. We
subsequently conduct extensive evaluations on 15 open-source models as well as
one advanced closed-source model, providing some new insights into the biases
revealing from these models. Our benchmark is available at
https://github.com/Xiangkui-Cao/VLBiasBench.

摘要：大型視覺語言模型 (LVLMs) 的出現標誌著在實現通用人工智慧方面取得重大進展。然而，這些進展受到經常反映偏見的輸出所影響，而這是一個尚未廣泛研究的問題。現有的基準在評估偏見方面不夠全面，因為它們的數據規模有限、單一的詢問格式和偏見來源狹窄。為了解決這個問題，我們引入了 VLBiasBench，一個旨在全面評估 LVLMs 中偏見的基準。在 VLBiasBench 中，我們構建了一個包含九個不同的社會偏見類別的數據集，包括年齡、殘疾狀況、性別、國籍、外貌、種族、宗教、職業、社會經濟地位和兩個交叉偏見類別（種族 x 性別，以及種族 x 社會經濟地位）。為了創建一個大型數據集，我們使用 Stable Diffusion XL 模型生成了 46,848 張高品質圖像，並將它們與不同的問題相結合，形成了 128,342 個樣本。這些問題被分類為開放式和封閉式，充分考慮了偏見來源，並從多個角度全面評估了 LVLM 的偏見。隨後，我們對 15 個開源模型和一個高級閉源模型進行了廣泛的評估，對從這些模型中揭示的偏見提供了一些新的見解。我們的基準可在 https://github.com/Xiangkui-Cao/VLBiasBench 獲得。

##### **Timo: Towards Better Temporal Reasoning for Language Models**
2406.14192v1 by Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, Yu Cheng

Reasoning about time is essential for Large Language Models (LLMs) to
understand the world. Previous works focus on solving specific tasks, primarily
on time-sensitive question answering. While these methods have proven
effective, they cannot generalize to a wider spectrum of temporal reasoning
tasks. Therefore, we propose a crucial question: Can we build a universal
framework to handle a variety of temporal reasoning tasks? To that end, we
systematically study 38 temporal reasoning tasks. Based on the observation that
19 tasks are directly related to mathematics, we first leverage the available
mathematical dataset to set a solid foundation for temporal reasoning. However,
the in-depth study indicates that focusing solely on mathematical enhancement
falls short of addressing pure temporal reasoning tasks. To mitigate this
limitation, we propose a simple but effective self-critic temporal optimization
method to enhance the model's temporal reasoning capabilities without
sacrificing general task abilities. Finally, we develop Timo, a model designed
to excel in temporal reasoning at the 7B and 13B scales. Notably, Timo
outperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and
achieves the new state-of-the-art (SOTA) performance of comparable size.
Extensive experiments further validate our framework's effectiveness and its
generalization across diverse temporal tasks. The code is available at
https://github.com/zhaochen0110/Timo.

摘要：對於大型語言模型（LLM）來說，時間推理對於理解世界至關重要。先前的研究專注於解決特定任務，主要是時間敏感問題解答。雖然這些方法已被證明有效，但它們無法推廣到更廣泛的時間推理任務。因此，我們提出了一個關鍵問題：我們可以建立一個通用框架來處理各種時間推理任務嗎？為此，我們系統性地研究了 38 項時間推理任務。根據 19 項任務與數學直接相關的觀察，我們首先利用現有的數學資料集為時間推理奠定堅實的基礎。然而，深入研究表明，僅專注於數學強化並不足以解決純粹的時間推理任務。為了減輕這種限制，我們提出了一種簡單但有效的方法，即自我批判時間優化方法，以增強模型的時間推理能力，同時不犧牲一般任務能力。最後，我們開發了 Timo，這是一個旨在在 7B 和 13B 規模上優於時間推理的模型。值得注意的是，Timo 在平均準確度得分上比對應的 LLM 高出 10.0 和 7.6，並達到了同等規模的最新技術（SOTA）效能。廣泛的實驗進一步驗證了我們框架的有效性和其在不同時間任務中的泛化能力。程式碼可在 https://github.com/zhaochen0110/Timo 取得。

##### **In Tree Structure Should Sentence Be Generated**
2406.14189v1 by Yaguang Li, Xin Chen

Generative models reliant on sequential autoregression have been at the
forefront of language generation for an extensive period, particularly
following the introduction of widely acclaimed transformers. Despite its
excellent performance, there are always some issues that we face today. For
example, problems such as hallucinations and getting trapped in a logic loop
may occur. To enhance the performance of existing systems, this paper
introduces a new method for generating sequences in natural language, which
involves generating the targeted sentence in a tree-traversing order. The paper
includes an illustration of the theoretical basis and validity of the approach,
as well as a comparison of its fundamentals with the diffusion model in graphic
generation. Finally, a module called SenTree is introduced for generating an
approximating binary tree. It is already available at
https://github.com/arklyg/sentree. Additionally, a joint training framework
based on this approach is proposed, incorporating the intrinsics of generative
adversarial networks.

摘要：依賴順序自迴歸的生成模型一直處於語言生成的最前線，特別是在廣受好評的Transformer引入之後。儘管有出色的表現，但我們今天仍然面臨一些問題。例如，可能會出現幻覺和陷入邏輯迴圈等問題。為了增強現有系統的性能，本文介紹了一種生成自然語言序列的新方法，其中涉及以樹狀遍歷順序生成目標句子。本文說明了該方法的理論基礎和有效性，並將其基礎與圖形生成中的擴散模型進行了比較。最後，引入了一個名為 SenTree 的模組，用於生成近似二元樹。它已經可以在 https://github.com/arklyg/sentree 上獲得。此外，還提出了一個基於此方法的聯合訓練框架，其中包含生成對抗網路的內在特性。

##### **Failure-Resilient Distributed Inference with Model Compression over Heterogeneous Edge Devices**
2406.14185v1 by Li Wang, Liang Li, Lianming Xu, Xian Peng, Aiguo Fei

The distributed inference paradigm enables the computation workload to be
distributed across multiple devices, facilitating the implementations of deep
learning based intelligent services on extremely resource-constrained Internet
of Things (IoT) scenarios. Yet it raises great challenges to perform
complicated inference tasks relying on a cluster of IoT devices that are
heterogeneous in their computing/communication capacity and prone to crash or
timeout failures. In this paper, we present RoCoIn, a robust cooperative
inference mechanism for locally distributed execution of deep neural
network-based inference tasks over heterogeneous edge devices. It creates a set
of independent and compact student models that are learned from a large model
using knowledge distillation for distributed deployment. In particular, the
devices are strategically grouped to redundantly deploy and execute the same
student model such that the inference process is resilient to any local
failures, while a joint knowledge partition and student model assignment scheme
are designed to minimize the response latency of the distributed inference
system in the presence of devices with diverse capacities. Extensive
simulations are conducted to corroborate the superior performance of our RoCoIn
for distributed inference compared to several baselines, and the results
demonstrate its efficacy in timely inference and failure resiliency.

摘要：分布式推理範例使計算工作負載能夠在多個裝置上進行分配，進而促進在極度受資源限制的物聯網 (IoT) 場景中實施基於深度學習的智慧服務。然而，這對執行複雜的推理任務構成了極大的挑戰，這些任務依賴於運算/通訊容量異質且容易發生崩潰或逾時故障的物聯網裝置叢集。在本文中，我們提出了 RoCoIn，這是一種用於在異質邊緣裝置上對基於深度神經網路的推理任務進行本地化分散執行的穩健協作推理機制。它建立了一組獨立且緊湊的學生模型，這些模型是使用知識蒸餾從一個大型模型中學習而來的，用於分散式部署。特別是，這些裝置被策略性地分組，以便冗餘部署並執行相同的學生模型，使得推理程序能對任何本地故障產生韌性，同時設計了一種聯合知識分割和學生模型分配方案，以最小化在存在具有不同容量的裝置的情況下分散式推理系統的回應延遲。進行了廣泛的模擬，以證實我們的 RoCoIn 在分散式推理方面的優異效能，與多個基準相比，結果證明了其在及時推理和故障復原力方面的效能。

##### **SimulSeamless: FBK at IWSLT 2024 Simultaneous Speech Translation**
2406.14177v1 by Sara Papi, Marco Gaido, Matteo Negri, Luisa Bentivogli

This paper describes the FBK's participation in the Simultaneous Translation
Evaluation Campaign at IWSLT 2024. For this year's submission in the
speech-to-text translation (ST) sub-track, we propose SimulSeamless, which is
realized by combining AlignAtt and SeamlessM4T in its medium configuration. The
SeamlessM4T model is used "off-the-shelf" and its simultaneous inference is
enabled through the adoption of AlignAtt, a SimulST policy based on
cross-attention that can be applied without any retraining or adaptation of the
underlying model for the simultaneous task. We participated in all the Shared
Task languages (English->{German, Japanese, Chinese}, and Czech->English),
achieving acceptable or even better results compared to last year's
submissions. SimulSeamless, covering more than 143 source languages and 200
target languages, is released at: https://github.com/hlt-mt/FBK-fairseq/.

摘要：這篇論文描述了 FBK 在 IWSLT 2024 的同步翻譯評估活動中所扮演的角色。在今年的語音轉文字翻譯 (ST) 子軌道中，我們提出了 SimulSeamless，這是透過結合 AlignAtt 和中等組態的 SeamlessM4T 所實現的。SeamlessM4T 模型以「現成」的方式使用，並透過採用 AlignAtt 來啟用其同步推論，這是一種基於交叉注意力的 SimulST 政策，無需重新訓練或調整基礎模型即可應用於同步任務。我們參與了所有共享任務語言（英語->{德語、日語、中文}，以及捷克語->英語），與去年的提交相比，我們獲得了可接受甚至更好的結果。SimulSeamless 涵蓋超過 143 種原始語言和 200 種目標語言，已於以下位置發布：https://github.com/hlt-mt/FBK-fairseq/。

##### **A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual Deepfake Detection**
2406.14176v1 by Kyungbok Lee, You Zhang, Zhiyao Duan

This paper addresses the challenge of developing a robust audio-visual
deepfake detection model. In practical use cases, new generation algorithms are
continually emerging, and these algorithms are not encountered during the
development of detection methods. This calls for the generalization ability of
the method. Additionally, to ensure the credibility of detection methods, it is
beneficial for the model to interpret which cues from the video indicate it is
fake. Motivated by these considerations, we then propose a multi-stream fusion
approach with one-class learning as a representation-level regularization
technique. We study the generalization problem of audio-visual deepfake
detection by creating a new benchmark by extending and re-splitting the
existing FakeAVCeleb dataset. The benchmark contains four categories of fake
video(Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,
and unsynchronized video). The experimental results show that our approach
improves the model's detection of unseen attacks by an average of 7.31% across
four test sets, compared to the baseline model. Additionally, our proposed
framework offers interpretability, indicating which modality the model
identifies as fake.

摘要：本文探讨了开发稳健的音频视觉深度造假检测模型的挑战。在实际用例中，新一代算法不断涌现，并且在检测方法的开发过程中不会遇到这些算法。这需要该方法的泛化能力。此外，为了确保检测方法的可靠性，模型解释视频中的哪些线索表明它是假的是有益的。受这些考虑因素的启发，我们提出了一个多流融合方法，其中一类学习作为表示级正则化技术。我们通过扩展和重新分割现有的 FakeAVCeleb 数据集来创建新的基准，研究了音频视觉深度造假检测的泛化问题。该基准包含四类虚假视频（真实音频-虚假视觉、虚假音频-虚假视觉、虚假音频-真实视觉和不同步视频）。实验结果表明，与基线模型相比，我们的方法将模型对未见攻击的检测提高了平均 7.31%，分布在四个测试集中。此外，我们提出的框架提供了可解释性，指出模型将哪个模态识别为虚假。

##### **Ranking LLMs by compression**
2406.14171v1 by Peijia Guo, Ziguang Li, Haibo Hu, Chao Huang, Ming Li, Rui Zhang

We conceptualize the process of understanding as information compression, and
propose a method for ranking large language models (LLMs) based on lossless
data compression. We demonstrate the equivalence of compression length under
arithmetic coding with cumulative negative log probabilities when using a large
language model as a prior, that is, the pre-training phase of the model is
essentially the process of learning the optimal coding length. At the same
time, the evaluation metric compression ratio can be obtained without actual
compression, which greatly saves overhead. In this paper, we use five large
language models as priors for compression, then compare their performance on
challenging natural language processing tasks, including sentence completion,
question answering, and coreference resolution. Experimental results show that
compression ratio and model performance are positively correlated, so it can be
used as a general metric to evaluate large language models.

摘要：我們將理解的過程概念化為資訊壓縮，並提出一個基於無失真資料壓縮對大型語言模型 (LLM) 進行排序的方法。我們示範了當使用大型語言模型作為先驗時，算術編碼下的壓縮長度等於累積負對數機率，也就是說，模型的預訓練階段本質上是學習最佳編碼長度的過程。同時，評估指標壓縮率可以在不實際壓縮的情況下獲得，這大大節省了開銷。在本文中，我們使用五個大型語言模型作為壓縮的先驗，然後比較它們在具有挑戰性的自然語言處理任務上的表現，包括句子完成、問題解答和共指消解。實驗結果表明，壓縮率和模型效能呈正相關，因此可以用作評估大型語言模型的通用指標。

##### **Definition generation for lexical semantic change detection**
2406.14167v1 by Mariia Fedorova, Andrey Kutuzov, Yves Scherrer

We use contextualized word definitions generated by large language models as
semantic representations in the task of diachronic lexical semantic change
detection (LSCD). In short, generated definitions are used as `senses', and the
change score of a target word is retrieved by comparing their distributions in
two time periods under comparison. On the material of five datasets and three
languages, we show that generated definitions are indeed specific and general
enough to convey a signal sufficient to rank sets of words by the degree of
their semantic change over time. Our approach is on par with or outperforms
prior non-supervised sense-based LSCD methods. At the same time, it preserves
interpretability and allows to inspect the reasons behind a specific shift in
terms of discrete definitions-as-senses. This is another step in the direction
of explainable semantic change modeling.

摘要：我們使用大型語言模型產生的語境化單字定義，作為歷時詞彙語義變化偵測 (LSCD) 任務中的語義表示。簡而言之，產生的定義被用作「詞義」，而目標單字的變化分數則透過比較它們在兩個比較時間段中的分佈來取得。在五個資料集和三種語言的材料中，我們展示產生的定義的確足夠具體和一般，足以傳達一個訊號，根據詞彙隨著時間推移的語義變化程度對詞彙集進行排序。我們的做法與先前的非監督詞義基礎 LSCD 方法相當或表現得更好。同時，它保留了可解釋性，並且允許根據離散定義作為詞義來檢查特定轉變背後的原因。這是可解釋語義變化建模方向的另一項進展。

##### **A Data-Driven Guided Decoding Mechanism for Diagnostic Captioning**
2406.14164v1 by Panagiotis Kaliosis, John Pavlopoulos, Foivos Charalampakos, Georgios Moschovis, Ion Androutsopoulos

Diagnostic Captioning (DC) automatically generates a diagnostic text from one
or more medical images (e.g., X-rays, MRIs) of a patient. Treated as a draft,
the generated text may assist clinicians, by providing an initial estimation of
the patient's condition, speeding up and helping safeguard the diagnostic
process. The accuracy of a diagnostic text, however, strongly depends on how
well the key medical conditions depicted in the images are expressed. We
propose a new data-driven guided decoding method that incorporates medical
information, in the form of existing tags capturing key conditions of the
image(s), into the beam search of the diagnostic text generation process. We
evaluate the proposed method on two medical datasets using four DC systems that
range from generic image-to-text systems with CNN encoders and RNN decoders to
pre-trained Large Language Models. The latter can also be used in few- and
zero-shot learning scenarios. In most cases, the proposed mechanism improves
performance with respect to all evaluation measures. We provide an open-source
implementation of the proposed method at https://github.com/nlpaueb/dmmcs.

摘要：診斷標題 (DC) 會自動從一位或多位病患的醫療影像 (例如 X 光、磁振造影) 中產生一則診斷文字。產生的文字視為草稿，可協助臨床醫生提供病患狀況的初步估計，加速並協助保障診斷程序。然而，診斷文字的準確性高度仰賴影像中所描繪的主要醫療狀況如何表達。我們提出一個新的資料驅動引導解碼方法，該方法將醫療資訊以現有標籤的形式納入診斷文字產生過程的波束搜尋中，用以擷取影像的主要狀況。我們使用四個 DC 系統在兩個醫療資料集上評估所提出的方法，這些系統的範圍從具備 CNN 編碼器和 RNN 解碼器的通用影像轉文字系統到預先訓練的大型語言模型。後者也可在少樣本和零樣本學習情境中使用。在大部分情況下，所提出的機制在所有評量指標方面均提升了效能。我們在 https://github.com/nlpaueb/dmmcs 中提供了所提出方法的開源實作。

##### **DIRAS: Efficient LLM-Assisted Annotation of Document Relevance in Retrieval Augmented Generation**
2406.14162v1 by Jingwei Ni, Tobias Schimanski, Meihong Lin, Mrinmaya Sachan, Elliott Ash, Markus Leippold

Retrieval Augmented Generation (RAG) is widely employed to ground responses
to queries on domain-specific documents. But do RAG implementations leave out
important information or excessively include irrelevant information? To allay
these concerns, it is necessary to annotate domain-specific benchmarks to
evaluate information retrieval (IR) performance, as relevance definitions vary
across queries and domains. Furthermore, such benchmarks should be
cost-efficiently annotated to avoid annotation selection bias. In this paper,
we propose DIRAS (Domain-specific Information Retrieval Annotation with
Scalability), a manual-annotation-free schema that fine-tunes open-sourced LLMs
to annotate relevance labels with calibrated relevance probabilities. Extensive
evaluation shows that DIRAS fine-tuned models achieve GPT-4-level performance
on annotating and ranking unseen (query, document) pairs, and is helpful for
real-world RAG development.

摘要：檢索增強生成 (RAG) 被廣泛用於將回應建立在特定領域文件上的查詢上。但 RAG 實作是否遺漏了重要資訊或過度納入不相關的資訊？為了消除這些疑慮，有必要註解特定領域的基準，以評估資訊檢索 (IR) 的效能，因為相關性定義會因查詢和領域而異。此外，此類基準應以成本效益的方式註解，以避免註解選擇偏誤。在本文中，我們提出 DIRAS（可擴充性的特定領域資訊檢索註解），這是一個免手動註解的架構，可微調開源 LLM，以校準相關性機率來註解相關性標籤。廣泛的評估顯示，DIRAS 微調模型在註解和排名未見過的（查詢、文件）配對上，達到了 GPT-4 等級的效能，並有助於實際的 RAG 開發。

##### **Aligning Large Language Models with Diverse Political Viewpoints**
2406.14155v1 by Dominik Stammbach, Philine Widmer, Eunjung Cho, Caglar Gulcehre, Elliott Ash

Large language models such as ChatGPT often exhibit striking political
biases. If users query them about political information, they might take a
normative stance and reinforce such biases. To overcome this, we align LLMs
with diverse political viewpoints from 100,000 comments written by candidates
running for national parliament in Switzerland. Such aligned models are able to
generate more accurate political viewpoints from Swiss parties compared to
commercial models such as ChatGPT. We also propose a procedure to generate
balanced overviews from multiple viewpoints using such models.

摘要：大型語言模型，例如 ChatGPT，通常會表現出顯著的政治偏見。如果使用者向其查詢政治資訊，它們可能會採取規範立場並強化此類偏見。為了克服這個問題，我們將 LLM 與瑞士國會候選人撰寫的 100,000 則評論中的不同政治觀點保持一致。與 ChatGPT 等商業模型相比，這些對齊的模型能夠從瑞士政黨產生更準確的政治觀點。我們還提出了一個程序，使用這些模型從多個觀點生成平衡的概述。

##### **Finding Safety Neurons in Large Language Models**
2406.14144v1 by Jianhui Chen, Xiaozhi Wang, Zijun Yao, Yushi Bai, Lei Hou, Juanzi Li

Large language models (LLMs) excel in various capabilities but also pose
safety risks such as generating harmful content and misinformation, even after
safety alignment. In this paper, we explore the inner mechanisms of safety
alignment from the perspective of mechanistic interpretability, focusing on
identifying and analyzing safety neurons within LLMs that are responsible for
safety behaviors. We propose generation-time activation contrasting to locate
these neurons and dynamic activation patching to evaluate their causal effects.
Experiments on multiple recent LLMs show that: (1) Safety neurons are sparse
and effective. We can restore $90$% safety performance with intervention only
on about $5$% of all the neurons. (2) Safety neurons encode transferrable
mechanisms. They exhibit consistent effectiveness on different red-teaming
datasets. The finding of safety neurons also interprets "alignment tax". We
observe that the identified key neurons for safety and helpfulness
significantly overlap, but they require different activation patterns of the
shared neurons. Furthermore, we demonstrate an application of safety neurons in
detecting unsafe outputs before generation. Our findings may promote further
research on understanding LLM alignment. The source codes will be publicly
released to facilitate future research.

摘要：大型語言模型 (LLM) 在各種功能上表現出色，但也存在安全風險，例如即使在安全對齊後，也會產生有害內容和錯誤訊息。在本文中，我們從機制可解釋性的角度探討安全對齊的內部機制，重點在於識別和分析對 LLM 中安全行為負責的安全神經元。我們提出生成時間激活對比來定位這些神經元，並動態激活修補來評估它們的因果效應。對多個近期 LLM 的實驗表明：(1) 安全神經元稀疏且有效。我們僅對約 5% 的所有神經元進行干預，就能恢復 90% 的安全效能。(2) 安全神經元編碼可轉移的機制。它們在不同的紅隊資料集上表現出一致的有效性。安全神經元的發現也解釋了「對齊稅」。我們觀察到，安全和有用的已識別關鍵神經元有顯著重疊，但它們需要共享神經元的不同激活模式。此外，我們展示了安全神經元在生成之前檢測不安全輸出的應用。我們的發現可能會促進進一步研究，以了解 LLM 對齊。原始碼將公開發布，以利於未來的研究。

##### **Online Learning of Weakly Coupled MDP Policies for Load Balancing and Auto Scaling**
2406.14141v1 by S. R. Eshwar, Lucas Lopes Felipe, Alexandre Reiffers-Masson, Daniel Sadoc Menasché, Gugan Thoppe

Load balancing and auto scaling are at the core of scalable, contemporary
systems, addressing dynamic resource allocation and service rate adjustments in
response to workload changes. This paper introduces a novel model and
algorithms for tuning load balancers coupled with auto scalers, considering
bursty traffic arriving at finite queues. We begin by presenting the problem as
a weakly coupled Markov Decision Processes (MDP), solvable via a linear program
(LP). However, as the number of control variables of such LP grows
combinatorially, we introduce a more tractable relaxed LP formulation, and
extend it to tackle the problem of online parameter learning and policy
optimization using a two-timescale algorithm based on the LP Lagrangian.

摘要：負載平衡和自動調整大小是可擴展現代系統的核心，用於解決動態資源分配和服務速率調整以響應工作負載變更。本文介紹了一個新的模型和演算法，用於調整負載平衡器與自動調整大小器，考慮到突發流量到達有限佇列。我們首先將問題表述為一個弱耦合馬可夫決策過程 (MDP)，可透過線性規劃 (LP) 來解決。然而，由於此類 LP 的控制變數數量以組合方式增加，我們引入了更易於處理的放鬆 LP 公式，並將其擴充套件為解決在線參數學習和策略最佳化問題，使用基於 LP 拉格朗日乘數的雙時間尺度演算法。

##### **MACAROON: Training Vision-Language Models To Be Your Engaged Partners**
2406.14137v1 by Shujin Wu, Yi R. Fung, Sha Li, Yixin Wan, Kai-Wei Chang, Heng Ji

Large vision-language models (LVLMs), while proficient in following
instructions and responding to diverse questions, invariably generate detailed
responses even when questions are ambiguous or unanswerable, leading to
hallucinations and bias issues. Thus, it is essential for LVLMs to proactively
engage with humans to ask for clarifications or additional information for
better responses. In this study, we aim to shift LVLMs from passive answer
providers to proactive engaged partners. We begin by establishing a
three-tiered hierarchy for questions of invalid, ambiguous, and personalizable
nature to measure the proactive engagement capabilities of LVLMs. Utilizing
this hierarchy, we create PIE, (ProactIve Engagement Evaluation) through GPT-4o
and human annotators, consisting of 853 questions across six distinct,
fine-grained question types that are verified by human annotators and
accompanied with well-defined metrics. Our evaluations on \benchmark indicate
poor performance of existing LVLMs, with the best-performing open-weights model
only achieving an Aggregate Align Rate (AAR) of 0.28. In response, we introduce
MACAROON, self-iMaginAtion for ContrAstive pReference OptimizatiON, which
instructs LVLMs to autonomously generate contrastive response pairs for
unlabeled questions given the task description and human-crafted criteria.
Then, the self-imagined data is formatted for conditional reinforcement
learning. Experimental results show MACAROON effectively improves LVLMs'
capabilities to be proactively engaged (0.84 AAR) while maintaining comparable
performance on general tasks.

摘要：大型視覺語言模型 (LVLMs) 雖然擅長遵循指示並回答各種問題，但即使在問題模稜兩可或無法回答時，它們也會產生詳細的回應，導致出現幻覺和偏見問題。因此，LVLMs 主動與人類互動以尋求澄清或額外資訊以獲得更好的回應至關重要。在這項研究中，我們旨在將 LVLMs 從被動的答案提供者轉變為主動參與的合作夥伴。我們首先為無效、模稜兩可和可個人化的問題建立一個三層等級，以衡量 LVLMs 的主動參與能力。利用這個層級，我們透過 GPT-4o 和人工標記員建立 PIE（主動參與評估），包含六種不同、細緻的問題類型中的 853 個問題，這些問題已由人工標記員驗證並附上明確的指標。我們對 \benchmark 的評估表明現有 LVLMs 的效能不佳，效能最佳的開放權重模型僅達到 0.28 的總體對齊率 (AAR)。為了解決這個問題，我們引入了 MACAROON，一種用於對比性偏好最佳化的自我想像，它指示 LVLMs 在給定任務描述和人工建立的標準下，自主產生未標記問題的對比回應對。然後，將自我想像的資料格式化為條件增強學習。實驗結果顯示，MACAROON 有效地提升了 LVLMs 主動參與的能力（0.84 AAR），同時在一般任務上維持了相當的效能。

##### **Enhancing Monotonic Modeling with Spatio-Temporal Adaptive Awareness in Diverse Marketing**
2406.14132v1 by Bin Li, Jiayan Pei, Feiyang Xiao, Yifan Zhao, Zhixing Zhang, Diwei Liu, HengXu He, Jia Jia

In the mobile internet era, the Online Food Ordering Service (OFOS) emerges
as an integral component of inclusive finance owing to the convenience it
brings to people. OFOS platforms offer dynamic allocation incentives to users
and merchants through diverse marketing campaigns to encourage payments while
maintaining the platforms' budget efficiency. Despite significant progress, the
marketing domain continues to face two primary challenges: (i) how to allocate
a limited budget with greater efficiency, demanding precision in predicting
users' monotonic response (i.e. sensitivity) to incentives, and (ii) ensuring
spatio-temporal adaptability and robustness in diverse marketing campaigns
across different times and locations. To address these issues, we propose a
Constrained Monotonic Adaptive Network (CoMAN) method for spatio-temporal
perception within marketing pricing. Specifically, we capture spatio-temporal
preferences within attribute features through two foundational spatio-temporal
perception modules. To further enhance catching the user sensitivity
differentials to incentives across varied times and locations, we design
modules for learning spatio-temporal convexity and concavity as well as for
expressing sensitivity functions. CoMAN can achieve a more efficient allocation
of incentive investments during pricing, thus increasing the conversion rate
and orders while maintaining budget efficiency. Extensive offline and online
experimental results within our diverse marketing campaigns demonstrate the
effectiveness of the proposed approach while outperforming the monotonic
state-of-the-art method.

摘要：在行動網路時代，線上訂餐服務 (OFOS) 因其帶給人們的便利性而成為普惠金融中不可或缺的組成部分。OFOS 平台透過多樣化的行銷活動，向使用者和商家提供動態配置誘因，以鼓勵付款，同時維持平台的預算效率。儘管有顯著的進展，行銷領域仍持續面臨兩項主要挑戰：(i) 如何更有效率地分配有限的預算，要求精確預測使用者對誘因的單調反應（即敏感度），以及 (ii) 確保在不同時間和地點的多樣化行銷活動中具有時空適應性和穩健性。為了解決這些問題，我們提出了一個受限單調適應網路 (CoMAN) 方法，用於行銷定價中的時空感知。具體來說，我們透過兩個基礎時空感知模組，在屬性特徵中擷取時空偏好。為了進一步增強捕捉使用者對不同時間和地點的誘因敏感性差異，我們設計了用於學習時空凸性和凹性以及表示敏感度函數的模組。CoMAN 可以更有效地配置定價期間的誘因投資，從而提高轉換率和訂單量，同時維持預算效率。我們在多樣化的行銷活動中進行的廣泛離線和線上實驗結果證明了所提出的方法的有效性，同時優於單調的最新方法。

##### **Towards Event-oriented Long Video Understanding**
2406.14129v1 by Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen

With the rapid development of video Multimodal Large Language Models (MLLMs),
numerous benchmarks have been proposed to assess their video understanding
capability. However, due to the lack of rich events in the videos, these
datasets may suffer from the short-cut bias that the answers can be deduced
from a few frames, without the need to watch the entire video. To address this
issue, we introduce Event-Bench, an event-oriented long video understanding
benchmark built on existing datasets and human annotations. Event-Bench
includes six event-related tasks and 2,190 test instances to comprehensively
evaluate video event understanding ability. Additionally, we propose Video
Instruction Merging~(VIM), a cost-effective method that enhances video MLLMs
using merged, event-intensive video instructions, addressing the scarcity of
human-annotated, event-intensive data. Extensive experiments show that the
best-performing model, GPT-4o, achieves an overall accuracy of 53.33,
significantly outperforming the best open-source model by 41.42%. Leveraging an
effective instruction synthesis method and an adaptive model architecture, VIM
surpasses both state-of-the-art open-source models and GPT-4V on the
Event-Bench. All code, data, and models are publicly available at
https://github.com/RUCAIBox/Event-Bench.

摘要：隨著視訊多模態大型語言模型 (MLLM) 的快速發展，
已經提出許多基準來評估它們的視訊理解能力。
然而，由於影片中豐富事件的缺乏，這些
資料集可能因捷徑偏誤而受影響，即答案可以從幾幀中推論出來，而無需觀看整個影片。為了解決這個
問題，我們引入了 Event-Bench，這是一個建立在現有資料集和人工註解上的事件導向長影片理解基準。Event-Bench
包含六個與事件相關的任務和 2,190 個測試實例，以全面
評估影片事件理解能力。此外，我們提出了影片
指令合併~(VIM)，這是一種使用合併的、事件密集型影片指令來增強影片 MLLM 的經濟有效方法，解決了人工註解、事件密集型資料的稀缺性。廣泛的實驗表明，
表現最佳的模型 GPT-4o，達到了 53.33 的整體準確度，
顯著優於最佳開源模型 41.42%。利用有效的指令合成方法和自適應模型架構，VIM
超越了 Event-Bench 上的最新開源模型和 GPT-4V。所有程式碼、資料和模型都可以在
https://github.com/RUCAIBox/Event-Bench 公開取得。

##### **Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective**
2406.14124v1 by Minsang Kim, Seungjun Baek

Compute-efficient training of large language models (LLMs) has become an
important research problem. In this work, we consider data pruning as a method
of data-efficient training of LLMs, where we take a data compression view on
data pruning. We argue that the amount of information of a sample, or the
achievable compression on its description length, represents its sample
importance. The key idea is that, less informative samples are likely to
contain redundant information, and thus should be pruned first. We leverage
log-likelihood function of trained models as a surrogate to measure information
content of samples. Experiments reveal a surprising insight that
information-based pruning can enhance the generalization capability of the
model, improves upon language modeling and downstream tasks as compared to the
model trained on the entire dataset.

摘要：大型語言模型 (LLM) 的運算效率訓練已成為一項重要的研究問題。在本研究中，我們將資料修剪視為資料效率訓練 LLM 的一種方法，其中我們採用資料壓縮的觀點來看待資料修剪。我們認為樣本的資訊量，或其描述長度可達到的壓縮率，代表其樣本重要性。關鍵在於，資訊量較少的樣本很可能包含冗餘資訊，因此應優先予以修剪。我們利用訓練模型的對數似然函數作為衡量樣本資訊含量的替代方法。實驗揭露了一個令人驚訝的見解，即基於資訊的修剪可以增強模型的泛化能力，與在整個資料集上訓練的模型相比，它改善了語言建模和下游任務。

##### **EduQate: Generating Adaptive Curricula through RMABs in Education Settings**
2406.14122v1 by Sidney Tio, Dexun Li, Pradeep Varakantham

There has been significant interest in the development of personalized and
adaptive educational tools that cater to a student's individual learning
progress. A crucial aspect in developing such tools is in exploring how mastery
can be achieved across a diverse yet related range of content in an efficient
manner. While Reinforcement Learning and Multi-armed Bandits have shown promise
in educational settings, existing works often assume the independence of
learning content, neglecting the prevalent interdependencies between such
content. In response, we introduce Education Network Restless Multi-armed
Bandits (EdNetRMABs), utilizing a network to represent the relationships
between interdependent arms. Subsequently, we propose EduQate, a method
employing interdependency-aware Q-learning to make informed decisions on arm
selection at each time step. We establish the optimality guarantee of EduQate
and demonstrate its efficacy compared to baseline policies, using students
modeled from both synthetic and real-world data.

摘要：個人化和適應式教育工具的開發一直備受關注，這些工具能迎合學生的個別學習進度。開發此類工具的關鍵方面在於探索如何在廣泛且相關的內容中以有效的方式實現精熟。雖然強化學習和多重選擇法在教育環境中已展現出前景，但現有工作通常假設學習內容的獨立性，忽視了此類內容之間普遍存在的相互依賴性。為了解決此問題，我們引入了教育網路不穩定多重選擇法 (EdNetRMAB)，利用網路來表示相互依賴性手臂之間的關係。隨後，我們提出了 EduQate，這是一種使用具備相互依賴性感知的 Q 學習方法，在每個時間步驟對手臂選擇做出明智決策。我們建立了 EduQate 的最優性保證，並使用根據合成和真實世界資料建模的學生，證明了它與基準策略相比的功效。

##### **An Investigation of Prompt Variations for Zero-shot LLM-based Rankers**
2406.14117v1 by Shuoqi Sun, Shengyao Zhuang, Shuai Wang, Guido Zuccon

We provide a systematic understanding of the impact of specific components
and wordings used in prompts on the effectiveness of rankers based on zero-shot
Large Language Models (LLMs). Several zero-shot ranking methods based on LLMs
have recently been proposed. Among many aspects, methods differ across (1) the
ranking algorithm they implement, e.g., pointwise vs. listwise, (2) the
backbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording
used in prompts, e.g., the use or not of role-definition (role-playing) and the
actual words used to express this. It is currently unclear whether performance
differences are due to the underlying ranking algorithm, or because of spurious
factors such as better choice of words used in prompts. This confusion risks to
undermine future research. Through our large-scale experimentation and
analysis, we find that ranking algorithms do contribute to differences between
methods for zero-shot LLM ranking. However, so do the LLM backbones -- but even
more importantly, the choice of prompt components and wordings affect the
ranking. In fact, in our experiments, we find that, at times, these latter
elements have more impact on the ranker's effectiveness than the actual ranking
algorithms, and that differences among ranking methods become more blurred when
prompt variations are considered.

摘要：我們系統性地了解了提示中使用的特定組件和措辭對基於零次學習大型語言模型 (LLM) 的排名器的有效性的影響。最近已經提出了幾種基於 LLM 的零次學習排名方法。在許多方面，方法因 (1) 它們實作的排名演算法而異，例如逐點對比清單，(2) 使用的骨幹 LLM，例如 GPT3.5 對比 FLAN-T5，(3) 提示中使用的組件和措辭，例如是否使用角色定義 (角色扮演) 以及用於表達此角色定義的實際字詞。目前尚不清楚效能差異是歸因於基礎排名演算法，還是因為提示中使用的字詞選擇較佳等虛假因素。這種混淆有損害未來研究的風險。透過我們的大規模實驗和分析，我們發現排名演算法確實會造成零次學習 LLM 排名的不同方法之間的差異。然而，LLM 骨幹也是如此，但更重要的是，提示組件和措辭的選擇會影響排名。事實上，在我們的實驗中，我們發現後者有時對排名器的有效性影響大於實際排名演算法，並且當考慮提示變異時，排名方法之間的差異會變得更加模糊。

##### **Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models**
2406.14115v1 by Ziche Liu, Rui Ke, Feng Jiang, Haizhou Li

Data selection for fine-tuning Large Language Models (LLMs) aims to select a
high-quality subset from a given candidate dataset to train a Pending Fine-tune
Model (PFM) into a Selective-Enhanced Model (SEM). It can improve the model
performance and accelerate the training process. Although a few surveys have
investigated related works of data selection, there is a lack of comprehensive
comparison between existing methods due to their various experimental settings.
To address this issue, we first propose a three-stage scheme for data selection
and comprehensively review existing works according to this scheme. Then, we
design a unified comparing method with ratio-based efficiency indicators and
ranking-based feasibility indicators to overcome the difficulty of comparing
various models with diverse experimental settings. After an in-depth
comparative analysis, we find that the more targeted method with data-specific
and model-specific quality labels has higher efficiency, but the introduction
of additional noise information should be avoided when designing selection
algorithms. Finally, we summarize the trends in data selection and highlight
the short-term and long-term challenges to guide future research.

摘要：大型語言模型 (LLM) 的微調資料選取旨在從給定的候選資料集中選取高品質的子集，以將待微調模型 (PFM) 訓練成選擇性增強模型 (SEM)。它可以提升模型效能並加速訓練過程。儘管少數調查研究了與資料選取相關的工作，但由於現有方法的實驗設定不同，因此缺乏全面的比較。為了解決這個問題，我們首先提出了一個三階段資料選取方案，並根據此方案全面檢視現有工作。接著，我們設計了一個統一的比較方法，採用基於比率的效率指標和基於排名的可行性指標，以克服比較具有不同實驗設定的各種模型的難度。在深入的比較分析後，我們發現具有資料特定和模型特定品質標籤的目標方法效率較高，但在設計選取演算法時應避免引入額外的雜訊資訊。最後，我們總結了資料選取的趨勢，並強調了短期和長期挑戰，以引導未來的研究。

##### **EasyECR: A Library for Easy Implementation and Evaluation of Event Coreference Resolution Models**
2406.14106v1 by Yuncong Li, Tianhua Xu, Sheng-hua Zhong, Haiqin Yang

Event Coreference Resolution (ECR) is the task of clustering event mentions
that refer to the same real-world event. Despite significant advancements, ECR
research faces two main challenges: limited generalizability across domains due
to narrow dataset evaluations, and difficulties in comparing models within
diverse ECR pipelines. To address these issues, we develop EasyECR, the first
open-source library designed to standardize data structures and abstract ECR
pipelines for easy implementation and fair evaluation. More specifically,
EasyECR integrates seven representative pipelines and ten popular benchmark
datasets, enabling model evaluations on various datasets and promoting the
development of robust ECR pipelines. By conducting extensive evaluation via our
EasyECR, we find that, \lowercase\expandafter{\romannumeral1}) the
representative ECR pipelines cannot generalize across multiple datasets, hence
evaluating ECR pipelines on multiple datasets is necessary,
\lowercase\expandafter{\romannumeral2}) all models in ECR pipelines have a
great effect on pipeline performance, therefore, when one model in ECR
pipelines are compared, it is essential to ensure that the other models remain
consistent. Additionally, reproducing ECR results is not trivial, and the
developed library can help reduce this discrepancy. The experimental results
provide valuable baselines for future research.

摘要：事件共指解析（ECR）是將指涉相同真實世界事件的事件提及進行分群的任務。儘管有顯著的進展，ECR 研究面臨兩項主要挑戰：由於評估資料集狹窄，導致跨領域的通用性受限，以及在不同的 ECR 管線中比較模型的困難。為了解決這些問題，我們開發了 EasyECR，這是第一個開放原始碼函式庫，旨在標準化資料結構和抽象 ECR 管線，以便輕鬆實作和公平評估。更具體地說，EasyECR 整合了七個具代表性的管線和十個流行的基準資料集，讓模型能夠在各種資料集上進行評估，並促進穩健 ECR 管線的開發。透過 EasyECR 進行廣泛的評估，我們發現，\lowercase\expandafter{\romannumeral1}) 具代表性的 ECR 管線無法在多個資料集上進行概化，因此，在多個資料集上評估 ECR 管線是必要的，\lowercase\expandafter{\romannumeral2}) ECR 管線中的所有模型對管線效能都有很大的影響，因此，當比較 ECR 管線中的某個模型時，必須確保其他模型保持一致。此外，複製 ECR 結果並非易事，而開發的函式庫可以幫助減少這種差異。實驗結果為未來的研究提供了有價值的基準。

