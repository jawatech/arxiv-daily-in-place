{"2405.03594": {"publish_time": "2024-05-06", "title": "Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment", "paper_summary": "Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP), but their size creates computational bottlenecks. We introduce a novel\napproach to create accurate, sparse foundational versions of performant LLMs\nthat achieve full accuracy recovery for fine-tuning tasks at up to 70%\nsparsity. We achieve this for the LLaMA-2 7B model by combining the SparseGPT\none-shot pruning method and sparse pretraining of those models on a subset of\nthe SlimPajama dataset mixed with a Python subset of The Stack dataset. We\nexhibit training acceleration due to sparsity on Cerebras CS-3 chips that\nclosely matches theoretical scaling. In addition, we establish inference\nacceleration of up to 3x on CPUs by utilizing Neural Magic's DeepSparse engine\nand 1.7x on GPUs through Neural Magic's nm-vllm engine. The above gains are\nrealized via sparsity alone, thus enabling further gains through additional use\nof quantization. Specifically, we show a total speedup on CPUs for\nsparse-quantized LLaMA models of up to 8.6x. We demonstrate these results\nacross diverse, challenging tasks, including chat, instruction following, code\ngeneration, arithmetic reasoning, and summarization to prove their generality.\nThis work paves the way for rapidly creating smaller and faster LLMs without\nsacrificing accuracy.", "paper_summary_zh": "", "author": "Abhinav Agarwalla et.al.", "authors": "Abhinav Agarwalla,Abhay Gupta,Alexandre Marques,Shubhra Pandit,Michael Goin,Eldar Kurtic,Kevin Leong,Tuan Nguyen,Mahmoud Salem,Dan Alistarh,Sean Lie,Mark Kurtz", "id": "2405.03594v1", "paper_url": "http://arxiv.org/abs/2405.03594v1", "repo": "null"}}