{"2502.03699": {"publish_time": "2025-02-06", "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective", "paper_summary": "Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u900f\u904e\u5728\u63a8\u7406\u3001\u7de8\u78bc\u548c\u6e9d\u901a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5fb9\u5e95\u9769\u65b0\u4e86\u4eba\u5de5\u667a\u6167\uff0c\u4e26\u63a8\u52d5\u5404\u7522\u696d\u7684\u5275\u65b0\u3002\u5176\u771f\u6b63\u7684\u6f5b\u529b\u53d6\u6c7a\u65bc\u6709\u6548\u5c0d\u9f4a\uff0c\u4ee5\u78ba\u4fdd\u6b63\u78ba\u3001\u503c\u5f97\u4fe1\u8cf4\u4e14\u7b26\u5408\u9053\u5fb7\u7684\u884c\u70ba\uff0c\u4e26\u89e3\u6c7a\u932f\u8aa4\u8cc7\u8a0a\u3001\u5e7b\u89ba\u3001\u504f\u898b\u548c\u8aa4\u7528\u7b49\u6311\u6230\u3002\u96d6\u7136\u73fe\u6709\u7684\u57fa\u65bc\u5f37\u5316\u5b78\u7fd2 (RL) \u7684\u5c0d\u9f4a\u65b9\u6cd5\u51fa\u4e86\u540d\u7684\u8907\u96dc\uff0c\u4f46\u76f4\u63a5\u6700\u4f73\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u500b\u66f4\u7c21\u55ae\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u900f\u904e\u63a1\u7528\u65e2\u5b9a\u7684\u8cc7\u8a0a\u6aa2\u7d22 (IR) \u539f\u5247\uff0c\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7684 LLM \u5c0d\u9f4a\u76f4\u63a5\u6700\u4f73\u5316\u65b9\u6cd5\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u7cfb\u7d71\u5316\u7684\u67b6\u69cb\uff0c\u5c07 LLM \u5c0d\u9f4a\u548c IR \u65b9\u6cd5\u8ad6\u806f\u7e6b\u8d77\u4f86\uff0c\u5c07 LLM \u751f\u6210\u548c\u734e\u52f5\u6a21\u578b\u5c0d\u61c9\u5230 IR \u7684\u6aa2\u7d22\u5668\u91cd\u65b0\u6392\u5e8f\u7bc4\u4f8b\u3002\u5728\u6b64\u57fa\u790e\u4e0a\uff0c\u6211\u5011\u63d0\u51fa LLM \u5c0d\u9f4a\u4f5c\u70ba\u6aa2\u7d22\u5668\u504f\u597d\u6700\u4f73\u5316 (LarPO)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u5c0d\u9f4a\u65b9\u6cd5\uff0c\u53ef\u589e\u5f37\u6574\u9ad4\u5c0d\u9f4a\u54c1\u8cea\u3002\u5927\u91cf\u7684\u5be6\u9a57\u9a57\u8b49\u4e86 LarPO \u7684\u6709\u6548\u6027\uff0c\u5728 AlpacaEval2 \u548c MixEval-Hard \u5206\u5225\u5e73\u5747\u6539\u5584\u4e86 38.9% \u548c 13.7%\u3002\u6211\u5011\u7684\u7814\u7a76\u900f\u904e\u6574\u5408 IR \u57fa\u790e\uff0c\u70ba\u63a8\u9032 LLM \u5c0d\u9f4a\u958b\u95e2\u4e86\u65b0\u7684\u9014\u5f91\uff0c\u4e26\u70ba\u672a\u4f86\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "author": "Bowen Jin et.al.", "authors": "Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik", "id": "2502.03699v1", "paper_url": "http://arxiv.org/abs/2502.03699v1", "repo": "null"}}