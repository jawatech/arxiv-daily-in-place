{"2405.03138": {"publish_time": "2024-05-06", "title": "CRAFT: Extracting and Tuning Cultural Instructions from the Wild", "paper_summary": "Large language models (LLMs) have rapidly evolved as the foundation of\nvarious natural language processing (NLP) applications. Despite their wide use\ncases, their understanding of culturally-related concepts and reasoning remains\nlimited. Meantime, there is a significant need to enhance these models'\ncultural reasoning capabilities, especially concerning underrepresented\nregions. This paper introduces a novel pipeline for extracting high-quality,\nculturally-related instruction tuning datasets from vast unstructured corpora.\nWe utilize a self-instruction generation pipeline to identify cultural concepts\nand trigger instruction. By integrating with a general-purpose instruction\ntuning dataset, our model demonstrates enhanced capabilities in recognizing and\nunderstanding regional cultural nuances, thereby enhancing its reasoning\ncapabilities. We conduct experiments across three regions: Singapore, the\nPhilippines, and the United States, achieving performance improvement of up to\n6%. Our research opens new avenues for extracting cultural instruction tuning\nsets directly from unstructured data, setting a precedent for future\ninnovations in the field.", "paper_summary_zh": "", "author": "Bin Wang et.al.", "authors": "Bin Wang,Geyu Lin,Zhengyuan Liu,Chengwei Wei,Nancy F. Chen", "id": "2405.03138v1", "paper_url": "http://arxiv.org/abs/2405.03138v1", "repo": "null"}}