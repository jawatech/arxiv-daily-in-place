{"2410.15814": {"publish_time": "2024-10-21", "title": "Kaninfradet3D:A Road-side Camera-LiDAR Fusion 3D Perception Model based on Nonlinear Feature Extraction and Intrinsic Correlation", "paper_summary": "With the development of AI-assisted driving, numerous methods have emerged\nfor ego-vehicle 3D perception tasks, but there has been limited research on\nroadside perception. With its ability to provide a global view and a broader\nsensing range, the roadside perspective is worth developing. LiDAR provides\nprecise three-dimensional spatial information, while cameras offer semantic\ninformation. These two modalities are complementary in 3D detection. However,\nadding camera data does not increase accuracy in some studies since the\ninformation extraction and fusion procedure is not sufficiently reliable.\nRecently, Kolmogorov-Arnold Networks (KANs) have been proposed as replacements\nfor MLPs, which are better suited for high-dimensional, complex data. Both the\ncamera and the LiDAR provide high-dimensional information, and employing KANs\nshould enhance the extraction of valuable features to produce better fusion\noutcomes. This paper proposes Kaninfradet3D, which optimizes the feature\nextraction and fusion modules. To extract features from complex\nhigh-dimensional data, the model's encoder and fuser modules were improved\nusing KAN Layers. Cross-attention was applied to enhance feature fusion, and\nvisual comparisons verified that camera features were more evenly integrated.\nThis addressed the issue of camera features being abnormally concentrated,\nnegatively impacting fusion. Compared to the benchmark, our approach shows\nimprovements of +9.87 mAP and +10.64 mAP in the two viewpoints of the TUMTraf\nIntersection Dataset and an improvement of +1.40 mAP in the roadside end of the\nTUMTraf V2X Cooperative Perception Dataset. The results indicate that\nKaninfradet3D can effectively fuse features, demonstrating the potential of\napplying KANs in roadside perception tasks.", "paper_summary_zh": "<paragraph>\u96a8\u8457 AI \u8f14\u52a9\u99d5\u99db\u7684\u767c\u5c55\uff0c\u5df2\u51fa\u73fe\u8a31\u591a\u7528\u65bc\u81ea\u6211\u8eca\u8f1b 3D \u611f\u77e5\u4efb\u52d9\u7684\u65b9\u6cd5\uff0c\u4f46\u5c0d\u65bc\u8def\u5074\u611f\u77e5\u7684\u7814\u7a76\u537b\u5f88\u6709\u9650\u3002\u8def\u5074\u8996\u89d2\u7531\u65bc\u80fd\u63d0\u4f9b\u5168\u5c40\u8996\u89d2\u548c\u66f4\u5ee3\u6cdb\u7684\u611f\u6e2c\u7bc4\u570d\uff0c\u56e0\u6b64\u503c\u5f97\u767c\u5c55\u3002LiDAR \u53ef\u63d0\u4f9b\u7cbe\u78ba\u7684\u4e09\u7dad\u7a7a\u9593\u8cc7\u8a0a\uff0c\u800c\u76f8\u6a5f\u5247\u63d0\u4f9b\u8a9e\u610f\u8cc7\u8a0a\u3002\u9019\u5169\u7a2e\u65b9\u5f0f\u5728 3D \u5075\u6e2c\u4e2d\u662f\u4e92\u88dc\u7684\u3002\u7136\u800c\uff0c\u5728\u67d0\u4e9b\u7814\u7a76\u4e2d\uff0c\u65b0\u589e\u76f8\u6a5f\u8cc7\u6599\u4e26\u672a\u63d0\u9ad8\u6e96\u78ba\u5ea6\uff0c\u56e0\u70ba\u8cc7\u8a0a\u64f7\u53d6\u548c\u878d\u5408\u7a0b\u5e8f\u4e0d\u5920\u53ef\u9760\u3002\u6700\u8fd1\uff0cKolmogorov-Arnold \u7db2\u8def (KAN) \u5df2\u88ab\u63d0\u51fa\u4f5c\u70ba MLP \u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u66f4\u9069\u5408\u65bc\u9ad8\u7dad\u5ea6\u3001\u8907\u96dc\u7684\u8cc7\u6599\u3002\u76f8\u6a5f\u548c LiDAR \u90fd\u63d0\u4f9b\u9ad8\u7dad\u5ea6\u8cc7\u8a0a\uff0c\u800c\u63a1\u7528 KAN \u61c9\u53ef\u589e\u5f37\u6709\u50f9\u503c\u7279\u5fb5\u7684\u64f7\u53d6\uff0c\u4ee5\u7522\u751f\u66f4\u597d\u7684\u878d\u5408\u7d50\u679c\u3002\u672c\u6587\u63d0\u51fa Kaninfradet3D\uff0c\u5b83\u6700\u4f73\u5316\u4e86\u7279\u5fb5\u64f7\u53d6\u548c\u878d\u5408\u6a21\u7d44\u3002\u70ba\u4e86\u5f9e\u8907\u96dc\u7684\u9ad8\u7dad\u5ea6\u8cc7\u6599\u4e2d\u64f7\u53d6\u7279\u5fb5\uff0c\u4f7f\u7528 KAN \u5c64\u6539\u9032\u4e86\u6a21\u578b\u7684\u7de8\u78bc\u5668\u548c\u878d\u5408\u5668\u6a21\u7d44\u3002\u61c9\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u4f86\u589e\u5f37\u7279\u5fb5\u878d\u5408\uff0c\u800c\u8996\u89ba\u6bd4\u8f03\u9a57\u8b49\u4e86\u76f8\u6a5f\u7279\u5fb5\u66f4\u5747\u52fb\u5730\u6574\u5408\u3002\u9019\u89e3\u6c7a\u4e86\u76f8\u6a5f\u7279\u5fb5\u7570\u5e38\u96c6\u4e2d\u7684\u554f\u984c\uff0c\u5c0d\u878d\u5408\u7522\u751f\u8ca0\u9762\u5f71\u97ff\u3002\u8207\u57fa\u6e96\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728 TUMTraf \u4ea4\u53c9\u8def\u53e3\u8cc7\u6599\u96c6\u7684\u5169\u500b\u8996\u9ede\u4e2d\u986f\u793a\u51fa +9.87 mAP \u548c +10.64 mAP \u7684\u6539\u9032\uff0c\u4ee5\u53ca\u5728 TUMTraf V2X \u5408\u4f5c\u611f\u77e5\u8cc7\u6599\u96c6\u7684\u8def\u5074\u7aef\u6539\u9032\u4e86 +1.40 mAP\u3002\u7d50\u679c\u8868\u660e\uff0cKaninfradet3D \u53ef\u4ee5\u6709\u6548\u878d\u5408\u7279\u5fb5\uff0c\u5c55\u793a\u4e86\u5c07 KAN \u61c9\u7528\u65bc\u8def\u5074\u611f\u77e5\u4efb\u52d9\u7684\u6f5b\u529b\u3002</paragraph>", "author": "Pei Liu et.al.", "authors": "Pei Liu, Nanfang Zheng, Yiqun Li, Junlan Chen, Ziyuan Pu", "id": "2410.15814v1", "paper_url": "http://arxiv.org/abs/2410.15814v1", "repo": "null"}}