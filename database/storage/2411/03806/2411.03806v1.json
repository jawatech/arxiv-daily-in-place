{"2411.03806": {"publish_time": "2024-11-06", "title": "Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection", "paper_summary": "Natural Language Generation has been rapidly developing with the advent of\nlarge language models (LLMs). While their usage has sparked significant\nattention from the general public, it is important for readers to be aware when\na piece of text is LLM-generated. This has brought about the need for building\nmodels that enable automated LLM-generated text detection, with the aim of\nmitigating potential negative outcomes of such content. Existing LLM-generated\ndetectors show competitive performances in telling apart LLM-generated and\nhuman-written text, but this performance is likely to deteriorate when\nparaphrased texts are considered. In this study, we devise a new data\ncollection strategy to collect Human & LLM Paraphrase Collection (HLPC), a\nfirst-of-its-kind dataset that incorporates human-written texts and\nparaphrases, as well as LLM-generated texts and paraphrases. With the aim of\nunderstanding the effects of human-written paraphrases on the performance of\nstate-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark\ndetectors, we perform classification experiments that incorporate human-written\nparaphrases, watermarked and non-watermarked LLM-generated documents from GPT\nand OPT, and LLM-generated paraphrases from DIPPER and BART. The results show\nthat the inclusion of human-written paraphrases has a significant impact of\nLLM-generated detector performance, promoting TPR@1%FPR with a possible\ntrade-off of AUROC and accuracy.", "paper_summary_zh": "<paragraph>\u96a8\u8457\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73fe\uff0c\u81ea\u7136\u8a9e\u8a00\u751f\u6210\u6280\u8853\u7372\u5f97\u4e86\u5feb\u901f\u767c\u5c55\u3002\u5118\u7ba1\u5176\u4f7f\u7528\u5f15\u8d77\u4e86\u516c\u773e\u7684\u6975\u5927\u95dc\u6ce8\uff0c\u4f46\u8b80\u8005\u5728\u95b1\u8b80 LLM \u751f\u6210\u7684\u6587\u672c\u6642\uff0c\u4e86\u89e3\u5176\u4f86\u6e90\u975e\u5e38\u91cd\u8981\u3002\u9019\u4f7f\u5f97\u5efa\u7acb\u6a21\u578b\u4ee5\u5be6\u73fe\u81ea\u52d5 LLM \u751f\u6210\u7684\u6587\u672c\u6aa2\u6e2c\u8b8a\u5f97\u5341\u5206\u5fc5\u8981\uff0c\u76ee\u7684\u662f\u6e1b\u8f15\u6b64\u985e\u5167\u5bb9\u6f5b\u5728\u7684\u8ca0\u9762\u5f71\u97ff\u3002\u73fe\u6709\u7684 LLM \u751f\u6210\u7684\u6aa2\u6e2c\u5668\u5728\u5340\u5206 LLM \u751f\u6210\u7684\u6587\u672c\u548c\u4eba\u985e\u64b0\u5beb\u7684\u6587\u672c\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u4f46\u7576\u8003\u616e\u5230\u6539\u5beb\u7684\u6587\u672c\u6642\uff0c\u9019\u7a2e\u8868\u73fe\u53ef\u80fd\u6703\u4e0b\u964d\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u7a2e\u65b0\u7684\u6578\u64da\u6536\u96c6\u7b56\u7565\uff0c\u7528\u65bc\u6536\u96c6\u4eba\u985e\u548c LLM \u6539\u5beb\u8a9e\u6599\u5eab (HLPC)\uff0c\u9019\u662f\u4e00\u500b\u9996\u5275\u7684\u6578\u64da\u96c6\uff0c\u5305\u542b\u4eba\u985e\u64b0\u5beb\u7684\u6587\u672c\u548c\u6539\u5beb\uff0c\u4ee5\u53ca LLM \u751f\u6210\u7684\u6587\u672c\u548c\u6539\u5beb\u3002\u70ba\u4e86\u4e86\u89e3\u4eba\u985e\u64b0\u5beb\u7684\u6539\u5beb\u5c0d\u6700\u5148\u9032\u7684 LLM \u751f\u6210\u7684\u6587\u672c\u6aa2\u6e2c\u5668 OpenAI RoBERTa \u548c\u6c34\u5370\u6aa2\u6e2c\u5668\u7684\u6548\u80fd\u7684\u5f71\u97ff\uff0c\u6211\u5011\u9032\u884c\u4e86\u5206\u985e\u5be6\u9a57\uff0c\u5176\u4e2d\u5305\u542b\u4eba\u985e\u64b0\u5beb\u7684\u6539\u5beb\u3001\u5e36\u6c34\u5370\u548c\u4e0d\u5e36\u6c34\u5370\u7684 GPT \u548c OPT \u751f\u6210\u7684 LLM \u6587\u4ef6\uff0c\u4ee5\u53ca DIPPER \u548c BART \u751f\u6210\u7684 LLM \u6539\u5beb\u3002\u7d50\u679c\u8868\u660e\uff0c\u5305\u542b\u4eba\u985e\u64b0\u5beb\u7684\u6539\u5beb\u5c0d LLM \u751f\u6210\u7684\u6aa2\u6e2c\u5668\u6548\u80fd\u6709\u986f\u8457\u5f71\u97ff\uff0c\u4ee5\u53ef\u80fd\u7684 AUROC \u548c\u6e96\u78ba\u6027\u7684\u6b0a\u8861\u4f86\u63d0\u5347 TPR@1%FPR\u3002</paragraph>", "author": "Hiu Ting Lau et.al.", "authors": "Hiu Ting Lau, Arkaitz Zubiaga", "id": "2411.03806v1", "paper_url": "http://arxiv.org/abs/2411.03806v1", "repo": "null"}}