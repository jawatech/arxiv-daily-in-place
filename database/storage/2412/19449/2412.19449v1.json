{"2412.19449": {"publish_time": "2024-12-27", "title": "Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models", "paper_summary": "This study proposes a knowledge distillation algorithm based on large\nlanguage models and feature alignment, aiming to effectively transfer the\nknowledge of large pre-trained models into lightweight student models, thereby\nreducing computational costs while maintaining high model performance.\nDifferent from the traditional soft label distillation method, this method\nintroduces a multi-layer feature alignment strategy to deeply align the\nintermediate features and attention mechanisms of the teacher model and the\nstudent model, maximally retaining the semantic expression ability and context\nmodeling ability of the teacher model. In terms of method design, a multi-task\nloss function is constructed, including feature matching loss, attention\nalignment loss, and output distribution matching loss, to ensure multi-level\ninformation transfer through joint optimization. The experiments were\ncomprehensively evaluated on the GLUE data set and various natural language\nprocessing tasks. The results show that the proposed model performs very close\nto the state-of-the-art GPT-4 model in terms of evaluation indicators such as\nperplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline\nmodels such as DeBERTa, XLNet, and GPT-3, showing significant performance\nimprovements and computing efficiency advantages. Research results show that\nthe feature alignment distillation strategy is an effective model compression\nmethod that can significantly reduce computational overhead and storage\nrequirements while maintaining model capabilities. Future research can be\nfurther expanded in the directions of self-supervised learning, cross-modal\nfeature alignment, and multi-task transfer learning to provide more flexible\nand efficient solutions for the deployment and optimization of deep learning\nmodels.", "paper_summary_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u7279\u5f81\u5bf9\u9f50\u7684\u77e5\u8bc6\u84b8\u998f\u7b97\u6cd5\uff0c\u65e8\u5728\u6709\u6548\u5730\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u8f83\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u4e0e\u4f20\u7edf\u7684\u8f6f\u6807\u7b7e\u84b8\u998f\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u5c42\u7279\u5f81\u5bf9\u9f50\u7b56\u7565\uff0c\u4ee5\u6df1\u5ea6\u5bf9\u9f50\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u7684\u4e2d\u95f4\u7279\u5f81\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6700\u5927\u7a0b\u5ea6\u5730\u4fdd\u7559\u6559\u5e08\u6a21\u578b\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u548c\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002\u5728\u65b9\u6cd5\u8bbe\u8ba1\u65b9\u9762\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u635f\u5931\u51fd\u6570\uff0c\u5305\u62ec\u7279\u5f81\u5339\u914d\u635f\u5931\u3001\u6ce8\u610f\u529b\u5bf9\u9f50\u635f\u5931\u548c\u8f93\u51fa\u5206\u5e03\u5339\u914d\u635f\u5931\uff0c\u4ee5\u786e\u4fdd\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8fdb\u884c\u591a\u7ea7\u4fe1\u606f\u4f20\u9012\u3002\u5728 GLUE \u6570\u636e\u96c6\u548c\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u5bf9\u5b9e\u9a8c\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u56f0\u60d1\u5ea6\u3001BLEU\u3001ROUGE \u548c CER \u7b49\u8bc4\u4f30\u6307\u6807\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684 GPT-4 \u6a21\u578b\u975e\u5e38\u63a5\u8fd1\u3002\u540c\u65f6\uff0c\u5b83\u8fdc\u8fdc\u8d85\u8fc7\u4e86 DeBERTa\u3001XLNet \u548c GPT-3 \u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u663e\u793a\u51fa\u663e\u7740\u7684\u6027\u80fd\u6539\u8fdb\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7279\u5f81\u5bf9\u9f50\u84b8\u998f\u7b56\u7565\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u548c\u5b58\u50a8\u9700\u6c42\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u548c\u591a\u4efb\u52a1\u8fc1\u79fb\u5b66\u4e60\u7684\u65b9\u5411\u4e0a\u8fdb\u4e00\u6b65\u6269\u5c55\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u90e8\u7f72\u548c\u4f18\u5316\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "author": "Shuo Wang et.al.", "authors": "Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao", "id": "2412.19449v1", "paper_url": "http://arxiv.org/abs/2412.19449v1", "repo": "null"}}