{"2408.11312": {"publish_time": "2024-08-21", "title": "Swarm Intelligence in Geo-Localization: A Multi-Agent Large Vision-Language Model Collaborative Framework", "paper_summary": "Visual geo-localization demands in-depth knowledge and advanced reasoning\nskills to associate images with real-world geographic locations precisely. In\ngeneral, traditional methods based on data-matching are hindered by the\nimpracticality of storing adequate visual records of global landmarks.\nRecently, Large Vision-Language Models (LVLMs) have demonstrated the capability\nof geo-localization through Visual Question Answering (VQA), enabling a\nsolution that does not require external geo-tagged image records. However, the\nperformance of a single LVLM is still limited by its intrinsic knowledge and\nreasoning capabilities. Along this line, in this paper, we introduce a novel\nvisual geo-localization framework called \\name\\ that integrates the inherent\nknowledge of multiple LVLM agents via inter-agent communication to achieve\neffective geo-localization of images. Furthermore, our framework employs a\ndynamic learning strategy to optimize the communication patterns among agents,\nreducing unnecessary discussions among agents and improving the efficiency of\nthe framework. To validate the effectiveness of the proposed framework, we\nconstruct GeoGlobe, a novel dataset for visual geo-localization tasks.\nExtensive testing on the dataset demonstrates that our approach significantly\noutperforms state-of-the-art methods.", "paper_summary_zh": "\u8996\u89ba\u5730\u7406\u5b9a\u4f4d\u9700\u8981\u6df1\u5165\u7684\u77e5\u8b58\u548c\u5148\u9032\u7684\u63a8\u7406\u6280\u80fd\uff0c\u624d\u80fd\u7cbe\u78ba\u5730\u5c07\u5f71\u50cf\u8207\u771f\u5be6\u4e16\u754c\u7684\u5730\u7406\u4f4d\u7f6e\u806f\u7e6b\u8d77\u4f86\u3002\u4e00\u822c\u4f86\u8aaa\uff0c\u57fa\u65bc\u8cc7\u6599\u914d\u5c0d\u7684\u50b3\u7d71\u65b9\u6cd5\u53d7\u5230\u5132\u5b58\u5168\u7403\u5730\u6a19\u8db3\u5920\u8996\u89ba\u7d00\u9304\u7684\u4e0d\u5207\u5be6\u969b\u6027\u6240\u963b\u7919\u3002\u6700\u8fd1\uff0c\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5df2\u900f\u904e\u8996\u89ba\u554f\u7b54 (VQA) \u5c55\u793a\u4e86\u5730\u7406\u5b9a\u4f4d\u7684\u80fd\u529b\uff0c\u63d0\u4f9b\u4e00\u7a2e\u4e0d\u9700\u8981\u5916\u90e8\u5730\u7406\u6a19\u7c64\u5f71\u50cf\u7d00\u9304\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u7136\u800c\uff0c\u55ae\u4e00 LVLM \u7684\u6548\u80fd\u4ecd\u53d7\u5230\u5176\u5167\u5728\u77e5\u8b58\u548c\u63a8\u7406\u80fd\u529b\u7684\u9650\u5236\u3002\u5728\u9019\u689d\u8def\u7ebf\u4e0a\uff0c\u6211\u5011\u5728\u672c\u6587\u4e2d\u4ecb\u7d39\u4e86\u4e00\u500b\u540d\u70ba \\name\\ \u7684\u65b0\u8996\u89ba\u5730\u7406\u5b9a\u4f4d\u67b6\u69cb\uff0c\u5b83\u900f\u904e\u4ee3\u7406\u9593\u6e9d\u901a\u6574\u5408\u591a\u500b LVLM \u4ee3\u7406\u7684\u5167\u5728\u77e5\u8b58\uff0c\u4ee5\u9054\u6210\u5f71\u50cf\u7684\u6709\u6548\u5730\u7406\u5b9a\u4f4d\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u67b6\u69cb\u63a1\u7528\u52d5\u614b\u5b78\u7fd2\u7b56\u7565\u4f86\u512a\u5316\u4ee3\u7406\u4e4b\u9593\u7684\u6e9d\u901a\u6a21\u5f0f\uff0c\u6e1b\u5c11\u4ee3\u7406\u4e4b\u9593\u4e0d\u5fc5\u8981\u7684\u8a0e\u8ad6\uff0c\u4e26\u63d0\u9ad8\u67b6\u69cb\u7684\u6548\u7387\u3002\u70ba\u4e86\u9a57\u8b49\u6240\u63d0\u51fa\u67b6\u69cb\u7684\u6709\u6548\u6027\uff0c\u6211\u5011\u5efa\u69cb\u4e86 GeoGlobe\uff0c\u4e00\u500b\u7528\u65bc\u8996\u89ba\u5730\u7406\u5b9a\u4f4d\u4efb\u52d9\u7684\u65b0\u8cc7\u6599\u96c6\u3002\u5728\u8cc7\u6599\u96c6\u4e0a\u7684\u5ee3\u6cdb\u6e2c\u8a66\u8b49\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u986f\u8457\u512a\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002", "author": "Xiao Han et.al.", "authors": "Xiao Han, Chen Zhu, Xiangyu Zhao, Hengshu Zhu", "id": "2408.11312v1", "paper_url": "http://arxiv.org/abs/2408.11312v1", "repo": "null"}}