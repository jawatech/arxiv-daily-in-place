{"2410.22707": {"publish_time": "2024-10-30", "title": "Robotic State Recognition with Image-to-Text Retrieval Task of Pre-Trained Vision-Language Model and Black-Box Optimization", "paper_summary": "State recognition of the environment and objects, such as the open/closed\nstate of doors and the on/off of lights, is indispensable for robots that\nperform daily life support and security tasks. Until now, state recognition\nmethods have been based on training neural networks from manual annotations,\npreparing special sensors for the recognition, or manually programming to\nextract features from point clouds or raw images. In contrast, we propose a\nrobotic state recognition method using a pre-trained vision-language model,\nwhich is capable of Image-to-Text Retrieval (ITR) tasks. We prepare several\nkinds of language prompts in advance, calculate the similarity between these\nprompts and the current image by ITR, and perform state recognition. By\napplying the optimal weighting to each prompt using black-box optimization,\nstate recognition can be performed with higher accuracy. Experiments show that\nthis theory enables a variety of state recognitions by simply preparing\nmultiple prompts without retraining neural networks or manual programming. In\naddition, since only prompts and their weights need to be prepared for each\nrecognizer, there is no need to prepare multiple models, which facilitates\nresource management. It is possible to recognize the open/closed state of\ntransparent doors, the state of whether water is running or not from a faucet,\nand even the qualitative state of whether a kitchen is clean or not, which have\nbeen challenging so far, through language.", "paper_summary_zh": "\u6a5f\u5668\u4eba\u57f7\u884c\u65e5\u5e38\u751f\u6d3b\u652f\u63f4\u548c\u5b89\u5168\u4efb\u52d9\u6642\uff0c\u5fc5\u9808\u8fa8\u8b58\u74b0\u5883\u548c\u7269\u9ad4\u7684\u72c0\u614b\uff0c\u4f8b\u5982\u9580\u7684\u958b/\u95dc\u72c0\u614b\u548c\u71c8\u7684\u958b/\u95dc\u72c0\u614b\u3002\u5230\u76ee\u524d\u70ba\u6b62\uff0c\u72c0\u614b\u8fa8\u8b58\u65b9\u6cd5\u4e00\u76f4\u662f\u6839\u64da\u624b\u52d5\u8a3b\u89e3\u8a13\u7df4\u795e\u7d93\u7db2\u8def\u3001\u6e96\u5099\u7279\u6b8a\u611f\u6e2c\u5668\u9032\u884c\u8fa8\u8b58\uff0c\u6216\u624b\u52d5\u7de8\u5beb\u7a0b\u5f0f\u5f9e\u9ede\u96f2\u6216\u539f\u59cb\u5f71\u50cf\u4e2d\u64f7\u53d6\u7279\u5fb5\u3002\u76f8\u53cd\u5730\uff0c\u6211\u5011\u63d0\u51fa\u4f7f\u7528\u9810\u5148\u8a13\u7df4\u597d\u7684\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\u7684\u6a5f\u5668\u4eba\u72c0\u614b\u8fa8\u8b58\u65b9\u6cd5\uff0c\u8a72\u6a21\u578b\u80fd\u5920\u57f7\u884c\u5f71\u50cf\u5230\u6587\u5b57\u6aa2\u7d22 (ITR) \u4efb\u52d9\u3002\u6211\u5011\u4e8b\u5148\u6e96\u5099\u5e7e\u7a2e\u8a9e\u8a00\u63d0\u793a\uff0c\u900f\u904e ITR \u8a08\u7b97\u9019\u4e9b\u63d0\u793a\u548c\u7576\u524d\u5f71\u50cf\u4e4b\u9593\u7684\u76f8\u4f3c\u6027\uff0c\u4e26\u57f7\u884c\u72c0\u614b\u8fa8\u8b58\u3002\u900f\u904e\u4f7f\u7528\u9ed1\u76d2\u6700\u4f73\u5316\u5c0d\u6bcf\u500b\u63d0\u793a\u5957\u7528\u6700\u4f73\u6b0a\u91cd\uff0c\u53ef\u4ee5\u66f4\u6e96\u78ba\u5730\u57f7\u884c\u72c0\u614b\u8fa8\u8b58\u3002\u5be6\u9a57\u986f\u793a\uff0c\u6b64\u7406\u8ad6\u53ea\u9700\u6e96\u5099\u591a\u500b\u63d0\u793a\uff0c\u800c\u7121\u9700\u91cd\u65b0\u8a13\u7df4\u795e\u7d93\u7db2\u8def\u6216\u624b\u52d5\u7de8\u5beb\u7a0b\u5f0f\uff0c\u5373\u53ef\u9032\u884c\u5404\u7a2e\u72c0\u614b\u8fa8\u8b58\u3002\u6b64\u5916\uff0c\u7531\u65bc\u6bcf\u500b\u8fa8\u8b58\u5668\u53ea\u9700\u8981\u6e96\u5099\u63d0\u793a\u53ca\u5176\u6b0a\u91cd\uff0c\u56e0\u6b64\u7121\u9700\u6e96\u5099\u591a\u500b\u6a21\u578b\uff0c\u9019\u6709\u52a9\u65bc\u8cc7\u6e90\u7ba1\u7406\u3002\u53ef\u4ee5\u900f\u904e\u8a9e\u8a00\u8fa8\u8b58\u900f\u660e\u9580\u7684\u958b/\u95dc\u72c0\u614b\u3001\u6c34\u9f8d\u982d\u662f\u5426\u51fa\u6c34\uff0c\u751a\u81f3\u5eda\u623f\u662f\u5426\u4e7e\u6de8\u7684\u54c1\u8cea\u72c0\u614b\uff0c\u9019\u4e9b\u90fd\u662f\u5230\u76ee\u524d\u70ba\u6b62\u7684\u6311\u6230\u3002", "author": "Kento Kawaharazuka et.al.", "authors": "Kento Kawaharazuka, Yoshiki Obinata, Naoaki Kanazawa, Kei Okada, Masayuki Inaba", "id": "2410.22707v1", "paper_url": "http://arxiv.org/abs/2410.22707v1", "repo": "null"}}