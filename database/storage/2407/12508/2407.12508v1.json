{"2407.12508": {"publish_time": "2024-07-17", "title": "MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline", "paper_summary": "The rapid expansion of multimedia content has made accurately retrieving\nrelevant videos from large collections increasingly challenging. Recent\nadvancements in text-video retrieval have focused on cross-modal interactions,\nlarge-scale foundation model training, and probabilistic modeling, yet often\nneglect the crucial user perspective, leading to discrepancies between user\nqueries and the content retrieved. To address this, we introduce MERLIN\n(Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel,\ntraining-free pipeline that leverages Large Language Models (LLMs) for\niterative feedback learning. MERLIN refines query embeddings from a user\nperspective, enhancing alignment between queries and video content through a\ndynamic question answering process. Experimental results on datasets like\nMSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves\nRecall@1, outperforming existing systems and confirming the benefits of\nintegrating LLMs into multimodal retrieval systems for more responsive and\ncontext-aware multimedia retrieval.", "paper_summary_zh": "\u591a\u5a92\u9ad4\u5167\u5bb9\u7684\u5feb\u901f\u64f4\u5f35\uff0c\u4f7f\u5f97\u5f9e\u5927\u578b\u8cc7\u6599\u96c6\u4e2d\u6e96\u78ba\u5730\u6aa2\u7d22\u76f8\u95dc\u5f71\u7247\u8b8a\u5f97\u8d8a\u4f86\u8d8a\u5177\u6709\u6311\u6230\u6027\u3002\u6700\u8fd1\u5728\u6587\u5b57\u5f71\u7247\u6aa2\u7d22\u65b9\u9762\u7684\u9032\u5c55\uff0c\u4e00\u76f4\u5c08\u6ce8\u65bc\u8de8\u6a21\u614b\u4e92\u52d5\u3001\u5927\u898f\u6a21\u57fa\u790e\u6a21\u578b\u8a13\u7df4\u548c\u6a5f\u7387\u6a21\u578b\uff0c\u4f46\u537b\u5e38\u5e38\u5ffd\u7565\u95dc\u9375\u7684\u4f7f\u7528\u8005\u89c0\u9ede\uff0c\u5c0e\u81f4\u4f7f\u7528\u8005\u67e5\u8a62\u8207\u6aa2\u7d22\u7684\u5167\u5bb9\u4e4b\u9593\u51fa\u73fe\u5dee\u7570\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5f15\u9032\u4e86 MERLIN\uff08\u900f\u904e\u57fa\u65bc LLM \u7684\u53cd\u8986\u5f0f\u700f\u89bd\u9032\u884c\u591a\u6a21\u614b\u5167\u5d4c\u5f0f\u7cbe\u7149\uff09\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u3001\u7121\u9700\u8a13\u7df4\u7684\u7ba1\u7dda\uff0c\u53ef\u5229\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u9032\u884c\u53cd\u8986\u5f0f\u56de\u994b\u5b78\u7fd2\u3002MERLIN \u5f9e\u4f7f\u7528\u8005\u7684\u89c0\u9ede\u7cbe\u7149\u67e5\u8a62\u5167\u5d4c\uff0c\u900f\u904e\u52d5\u614b\u554f\u7b54\u7a0b\u5e8f\u589e\u5f37\u67e5\u8a62\u8207\u5f71\u7247\u5167\u5bb9\u4e4b\u9593\u7684\u4e00\u81f4\u6027\u3002\u5728 MSR-VTT\u3001MSVD \u548c ActivityNet \u7b49\u8cc7\u6599\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0cMERLIN \u5927\u5e45\u6539\u5584\u4e86 Recall@1\uff0c\u512a\u65bc\u73fe\u6709\u7684\u7cfb\u7d71\uff0c\u4e26\u78ba\u8a8d\u4e86\u5c07 LLM \u6574\u5408\u5230\u591a\u6a21\u614b\u6aa2\u7d22\u7cfb\u7d71\u4e2d\uff0c\u4ee5\u5be6\u73fe\u66f4\u5177\u56de\u61c9\u6027\u4e14\u5177\u5099\u8108\u7d61\u611f\u77e5\u7684\u591a\u5a92\u9ad4\u6aa2\u7d22\u7684\u512a\u9ede\u3002", "author": "Donghoon Han et.al.", "authors": "Donghoon Han, Eunhwan Park, Gisang Lee, Adam Lee, Nojun Kwak", "id": "2407.12508v1", "paper_url": "http://arxiv.org/abs/2407.12508v1", "repo": "null"}}