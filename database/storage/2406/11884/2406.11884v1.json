{"2406.11884": {"publish_time": "2024-06-13", "title": "Hierarchical Compression of Text-Rich Graphs via Large Language Models", "paper_summary": "Text-rich graphs, prevalent in data mining contexts like e-commerce and\nacademic graphs, consist of nodes with textual features linked by various\nrelations. Traditional graph machine learning models, such as Graph Neural\nNetworks (GNNs), excel in encoding the graph structural information, but have\nlimited capability in handling rich text on graph nodes. Large Language Models\n(LLMs), noted for their superior text understanding abilities, offer a solution\nfor processing the text in graphs but face integration challenges due to their\nlimitation for encoding graph structures and their computational complexities\nwhen dealing with extensive text in large neighborhoods of interconnected\nnodes. This paper introduces ``Hierarchical Compression'' (HiCom), a novel\nmethod to align the capabilities of LLMs with the structure of text-rich\ngraphs. HiCom processes text in a node's neighborhood in a structured manner by\norganizing the extensive textual information into a more manageable hierarchy\nand compressing node text step by step. Therefore, HiCom not only preserves the\ncontextual richness of the text but also addresses the computational challenges\nof LLMs, which presents an advancement in integrating the text processing power\nof LLMs with the structural complexities of text-rich graphs. Empirical results\nshow that HiCom can outperform both GNNs and LLM backbones for node\nclassification on e-commerce and citation graphs. HiCom is especially effective\nfor nodes from a dense region in a graph, where it achieves a 3.48% average\nperformance improvement on five datasets while being more efficient than LLM\nbackbones.", "paper_summary_zh": "<paragraph>\u5728\u8cc7\u6599\u63a2\u52d8\u60c5\u5883\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6587\u5b57\u8c50\u5bcc\u5716\u5f62\uff0c\u4f8b\u5982\u96fb\u5b50\u5546\u52d9\u548c\u5b78\u8853\u5716\u5f62\uff0c\u7531\u5177\u6709\u5404\u7a2e\u95dc\u806f\u7684\u6587\u5b57\u7279\u5fb5\u7bc0\u9ede\u7d44\u6210\u3002\u50b3\u7d71\u7684\u5716\u5f62\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\uff0c\u4f8b\u5982\u5716\u5f62\u795e\u7d93\u7db2\u8def (GNN)\uff0c\u64c5\u9577\u7de8\u78bc\u5716\u5f62\u7d50\u69cb\u8cc7\u8a0a\uff0c\u4f46\u5728\u8655\u7406\u5716\u5f62\u7bc0\u9ede\u4e0a\u7684\u8c50\u5bcc\u6587\u5b57\u6642\u80fd\u529b\u6709\u9650\u3002\u4ee5\u5176\u512a\u7570\u7684\u6587\u5b57\u7406\u89e3\u80fd\u529b\u800c\u805e\u540d\u7684\u5de8\u91cf\u8a9e\u8a00\u6a21\u578b (LLM) \u70ba\u8655\u7406\u5716\u5f62\u4e2d\u7684\u6587\u5b57\u63d0\u4f9b\u4e86\u89e3\u6c7a\u65b9\u6848\uff0c\u4f46\u7531\u65bc\u5176\u5728\u7de8\u78bc\u5716\u5f62\u7d50\u69cb\u65b9\u9762\u7684\u9650\u5236\u4ee5\u53ca\u5728\u8655\u7406\u76f8\u4e92\u9023\u63a5\u7bc0\u9ede\u7684\u5ee3\u6cdb\u6587\u5b57\u6642\u9762\u81e8\u7684\u8a08\u7b97\u8907\u96dc\u6027\uff0c\u800c\u9762\u81e8\u6574\u5408\u6311\u6230\u3002\u672c\u6587\u4ecb\u7d39\u4e86\u300c\u5206\u5c64\u58d3\u7e2e\u300d(HiCom)\uff0c\u9019\u662f\u4e00\u7a2e\u5c07 LLM \u7684\u80fd\u529b\u8207\u6587\u5b57\u8c50\u5bcc\u5716\u5f62\u7d50\u69cb\u76f8\u7d50\u5408\u7684\u65b0\u65b9\u6cd5\u3002HiCom \u4ee5\u7d50\u69cb\u5316\u7684\u65b9\u5f0f\u8655\u7406\u7bc0\u9ede\u9130\u57df\u4e2d\u7684\u6587\u5b57\uff0c\u65b9\u6cd5\u662f\u5c07\u5ee3\u6cdb\u7684\u6587\u5b57\u8cc7\u8a0a\u7d44\u7e54\u6210\u66f4\u6613\u65bc\u7ba1\u7406\u7684\u5c64\u7d1a\u4e26\u9010\u6b65\u58d3\u7e2e\u7bc0\u9ede\u6587\u5b57\u3002\u56e0\u6b64\uff0cHiCom \u4e0d\u50c5\u4fdd\u7559\u4e86\u6587\u5b57\u7684\u8a9e\u5883\u8c50\u5bcc\u6027\uff0c\u9084\u61c9\u5c0d\u4e86 LLM \u7684\u8a08\u7b97\u6311\u6230\uff0c\u9019\u4ee3\u8868\u4e86\u5c07 LLM \u7684\u6587\u5b57\u8655\u7406\u80fd\u529b\u8207\u6587\u5b57\u8c50\u5bcc\u5716\u5f62\u7684\u7d50\u69cb\u8907\u96dc\u6027\u76f8\u6574\u5408\u7684\u9032\u5c55\u3002\u5be6\u8b49\u7d50\u679c\u8868\u660e\uff0c\u5728\u96fb\u5b50\u5546\u52d9\u548c\u5f15\u6587\u5716\u5f62\u4e0a\u7684\u7bc0\u9ede\u5206\u985e\u4e2d\uff0cHiCom \u53ef\u4ee5\u512a\u65bc GNN \u548c LLM \u4e3b\u5e79\u3002HiCom \u5c0d\u65bc\u5716\u5f62\u4e2d\u5bc6\u96c6\u5340\u57df\u7684\u7bc0\u9ede\u7279\u5225\u6709\u6548\uff0c\u5728\u4e94\u500b\u8cc7\u6599\u96c6\u4e0a\u5be6\u73fe\u4e86\u5e73\u5747\u6548\u80fd\u63d0\u5347 3.48%\uff0c\u540c\u6642\u6bd4 LLM \u4e3b\u5e79\u66f4\u6709\u6548\u7387\u3002</paragraph>", "author": "Shichang Zhang et.al.", "authors": "Shichang Zhang, Da Zheng, Jiani Zhang, Qi Zhu, Xiang song, Soji Adeshina, Christos Faloutsos, George Karypis, Yizhou Sun", "id": "2406.11884v1", "paper_url": "http://arxiv.org/abs/2406.11884v1", "repo": "null"}}