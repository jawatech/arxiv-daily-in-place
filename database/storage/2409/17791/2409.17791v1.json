{"2409.17791": {"publish_time": "2024-09-26", "title": "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "paper_summary": "Recently, there has been significant interest in replacing the reward model\nin Reinforcement Learning with Human Feedback (RLHF) methods for Large Language\nModels (LLMs), such as Direct Preference Optimization (DPO) and its variants.\nThese approaches commonly use a binary cross-entropy mechanism on pairwise\nsamples, i.e., minimizing and maximizing the loss based on preferred or\ndis-preferred responses, respectively. However, while this training strategy\nomits the reward model, it also overlooks the varying preference degrees within\ndifferent responses. We hypothesize that this is a key factor hindering LLMs\nfrom sufficiently understanding human preferences. To address this problem, we\npropose a novel Self-supervised Preference Optimization (SPO) framework, which\nconstructs a self-supervised preference degree loss combined with the alignment\nloss, thereby helping LLMs improve their ability to understand the degree of\npreference. Extensive experiments are conducted on two widely used datasets of\ndifferent tasks. The results demonstrate that SPO can be seamlessly integrated\nwith existing preference optimization methods and significantly boost their\nperformance to achieve state-of-the-art performance. We also conduct detailed\nanalyses to offer comprehensive insights into SPO, which verifies its\neffectiveness. The code is available at https://github.com/lijian16/SPO.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u4eba\u4eec\u5bf9\u7528\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u65b9\u6cd5\u53d6\u4ee3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u4ea7\u751f\u4e86\u6d53\u539a\u7684\u5174\u8da3\uff0c\u4f8b\u5982\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u53ca\u5176\u53d8\u4f53\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u5bf9\u6210\u5bf9\u6837\u672c\u4f7f\u7528\u4e8c\u5143\u4ea4\u53c9\u71b5\u673a\u5236\uff0c\u5373\u6839\u636e\u504f\u597d\u6216\u975e\u504f\u597d\u54cd\u5e94\u5206\u522b\u6700\u5c0f\u5316\u548c\u6700\u5927\u5316\u635f\u5931\u3002\u7136\u800c\uff0c\u867d\u7136\u8fd9\u79cd\u8bad\u7ec3\u7b56\u7565\u7701\u7565\u4e86\u5956\u52b1\u6a21\u578b\uff0c\u4f46\u5b83\u4e5f\u5ffd\u7565\u4e86\u4e0d\u540c\u54cd\u5e94\u4e2d\u7684\u4e0d\u540c\u504f\u597d\u7a0b\u5ea6\u3002\u6211\u4eec\u5047\u8bbe\u8fd9\u662f\u963b\u788d LLM \u5145\u5206\u7406\u89e3\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u504f\u597d\u4f18\u5316\uff08SPO\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u76d1\u7763\u504f\u597d\u7a0b\u5ea6\u635f\u5931\uff0c\u5e76\u4e0e\u5bf9\u9f50\u635f\u5931\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u5e2e\u52a9 LLM \u63d0\u9ad8\u5176\u7406\u89e3\u504f\u597d\u7a0b\u5ea6\u7684\u80fd\u529b\u3002\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0cSPO \u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u663e\u8457\u63d0\u5347\u5176\u6027\u80fd\u4ee5\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u5206\u6790\uff0c\u4ee5\u63d0\u4f9b\u5bf9 SPO \u7684\u5168\u9762\u89c1\u89e3\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u4ee3\u7801\u53ef\u5728 https://github.com/lijian16/SPO \u83b7\u5f97\u3002", "author": "Jian Li et.al.", "authors": "Jian Li, Haojing Huang, Yujia Zhang, Pengfei Xu, Xi Chen, Rui Song, Lida Shi, Jingwen Wang, Hao Xu", "id": "2409.17791v1", "paper_url": "http://arxiv.org/abs/2409.17791v1", "repo": "https://github.com/lijian16/spo"}}