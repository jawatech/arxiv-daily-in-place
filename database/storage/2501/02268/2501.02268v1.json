{"2501.02268": {"publish_time": "2025-01-04", "title": "What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph", "paper_summary": "Recent Multimodal Large Language Models(MLLMs) often use a large number of\nvisual tokens to compensate their visual shortcoming, leading to excessive\ncomputation and obvious visual redundancy. In this paper, we investigate what\nkind of visual tokens are needed for MLLMs, and reveal that both foreground and\nbackground tokens are critical for MLLMs given the varying difficulties of\nexamples. Based on this observation, we propose a graph-based method towards\ntraining-free visual token pruning, termed G-Prune.In particular, G-Prune\nregards visual tokens as nodes, and construct their connections based on their\nsemantic similarities. Afterwards, the information flow is propagated via\nweighted links, and the most important tokens after iterations are kept for\nMLLMs, which can be front or background.To validate G-Prune, we apply it to a\nrecent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of\nbenchmarks.The experiment results show that G-Prune can greatly reduce\ncomputation overhead while retaining high performance on both coarse- and\nfine-grained tasks. For instance, G-Prune can reduce 63.57\\% FLOPs of\nLLaVA-NeXT on VQA2.0 and TextVQA with only 0.95\\% and 2.34\\% accuracy drops,\nrespectively.", "paper_summary_zh": "\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7ecf\u5e38\u4f7f\u7528\u5927\u91cf\u7684\u89c6\u89c9\u6807\u8bb0\u6765\u5f25\u8865\u5176\u89c6\u89c9\u4e0a\u7684\u7f3a\u70b9\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u7684\u8ba1\u7b97\u548c\u660e\u663e\u7684\u89c6\u89c9\u5197\u4f59\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8c03\u67e5\u4e86 MLLM \u9700\u8981\u54ea\u79cd\u89c6\u89c9\u6807\u8bb0\uff0c\u5e76\u63ed\u793a\u4e86\u9274\u4e8e\u793a\u4f8b\u7684\u96be\u5ea6\u4e0d\u540c\uff0c\u524d\u666f\u6807\u8bb0\u548c\u80cc\u666f\u6807\u8bb0\u5bf9\u4e8e MLLM \u90fd\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002\u57fa\u4e8e\u6b64\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u65e0\u8bad\u7ec3\u89c6\u89c9\u6807\u8bb0\u526a\u679d\u65b9\u6cd5\uff0c\u79f0\u4e3a G-Prune\u3002\u7279\u522b\u662f\uff0cG-Prune \u5c06\u89c6\u89c9\u6807\u8bb0\u89c6\u4e3a\u8282\u70b9\uff0c\u5e76\u6839\u636e\u5176\u8bed\u4e49\u76f8\u4f3c\u6027\u6784\u5efa\u5b83\u4eec\u7684\u8fde\u63a5\u3002\u4e4b\u540e\uff0c\u4fe1\u606f\u6d41\u901a\u8fc7\u52a0\u6743\u94fe\u63a5\u4f20\u64ad\uff0c\u5e76\u4e14\u5728\u8fed\u4ee3\u540e\u6700\u91cd\u8981\u7684\u6807\u8bb0\u4fdd\u7559\u7528\u4e8e MLLM\uff0c\u5b83\u53ef\u4ee5\u662f\u524d\u666f\u6216\u80cc\u666f\u3002\u4e3a\u4e86\u9a8c\u8bc1 G-Prune\uff0c\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e\u79f0\u4e3a LLaVA-NeXT \u7684\u6700\u65b0 MLLM\uff0c\u5e76\u5728\u4e00\u7ec4\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cG-Prune \u53ef\u4ee5\u6781\u5927\u5730\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u5728\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e0a\u4fdd\u6301\u9ad8\u6027\u80fd\u3002\u4f8b\u5982\uff0cG-Prune \u53ef\u4ee5\u5c06 LLaVA-NeXT \u5728 VQA2.0 \u548c TextVQA \u4e0a\u7684 FLOP \u51cf\u5c11 63.57%\uff0c\u800c\u51c6\u786e\u5ea6\u5206\u522b\u4ec5\u4e0b\u964d 0.95% \u548c 2.34%\u3002", "author": "Yutao Jiang et.al.", "authors": "Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, Yiyi Zhou", "id": "2501.02268v1", "paper_url": "http://arxiv.org/abs/2501.02268v1", "repo": "null"}}