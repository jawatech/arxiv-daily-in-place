{"2410.11469": {"publish_time": "2024-10-15", "title": "O-Edit: Orthogonal Subspace Editing for Language Model Sequential Editing", "paper_summary": "Large language models (LLMs) acquire knowledge during pre-training, but over\ntime, this knowledge may become incorrect or outdated, necessitating updates\nafter training. Knowledge editing techniques address this issue without the\nneed for costly re-training. However, most existing methods are designed for\nsingle edits, and as the number of edits increases, they often cause a decline\nin the model's overall performance, posing significant challenges for\nsequential editing. To overcome this, we propose Orthogonal Subspace Editing,\nO-Edit. This algorithm orthogonalizes the direction of each knowledge update,\nminimizing interference between successive updates and reducing the impact of\nnew updates on unrelated knowledge. Our approach does not require replaying\npreviously edited data and processes each edit knowledge on time. It can\nperform thousands of edits on mainstream LLMs, achieving an average performance\nimprovement that is 4.2 times better than existing methods while effectively\npreserving the model's performance on downstream tasks, all with minimal\nadditional parameter overhead.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9810\u8a13\u7df4\u671f\u9593\u6703\u7372\u53d6\u77e5\u8b58\uff0c\u4f46\u96a8\u8457\u6642\u9593\u63a8\u79fb\uff0c\u9019\u4e9b\u77e5\u8b58\u53ef\u80fd\u6703\u8b8a\u5f97\u4e0d\u6b63\u78ba\u6216\u904e\u6642\uff0c\u56e0\u6b64\u9700\u8981\u5728\u8a13\u7df4\u5f8c\u9032\u884c\u66f4\u65b0\u3002\u77e5\u8b58\u7de8\u8f2f\u6280\u8853\u89e3\u6c7a\u4e86\u9019\u500b\u554f\u984c\uff0c\u800c\u7121\u9700\u9032\u884c\u4ee3\u50f9\u9ad8\u6602\u7684\u91cd\u65b0\u8a13\u7df4\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u65b9\u6cd5\u5927\u591a\u662f\u91dd\u5c0d\u55ae\u4e00\u7de8\u8f2f\u800c\u8a2d\u8a08\u7684\uff0c\u96a8\u8457\u7de8\u8f2f\u6b21\u6578\u7684\u589e\u52a0\uff0c\u5b83\u5011\u901a\u5e38\u6703\u5c0e\u81f4\u6a21\u578b\u7684\u6574\u9ad4\u6548\u80fd\u4e0b\u964d\uff0c\u5c0d\u9806\u5e8f\u7de8\u8f2f\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u6b63\u4ea4\u5b50\u7a7a\u9593\u7de8\u8f2f\uff0c\u5373 O-Edit\u3002\u6b64\u6f14\u7b97\u6cd5\u6b63\u4ea4\u5316\u6bcf\u500b\u77e5\u8b58\u66f4\u65b0\u7684\u65b9\u5411\uff0c\u5c07\u9023\u7e8c\u66f4\u65b0\u4e4b\u9593\u7684\u5e72\u64fe\u6700\u5c0f\u5316\uff0c\u4e26\u6e1b\u5c11\u65b0\u66f4\u65b0\u5c0d\u4e0d\u76f8\u95dc\u77e5\u8b58\u7684\u5f71\u97ff\u3002\u6211\u5011\u7684\u505a\u6cd5\u4e0d\u9700\u8981\u91cd\u64ad\u5148\u524d\u7de8\u8f2f\u7684\u8cc7\u6599\uff0c\u4e26\u5373\u6642\u8655\u7406\u6bcf\u500b\u7de8\u8f2f\u77e5\u8b58\u3002\u5b83\u53ef\u4ee5\u5728\u4e3b\u6d41 LLM \u4e0a\u57f7\u884c\u6578\u5343\u6b21\u7de8\u8f2f\uff0c\u5be6\u73fe\u5e73\u5747\u6548\u80fd\u63d0\u5347\uff0c\u6bd4\u73fe\u6709\u65b9\u6cd5\u9ad8\u51fa 4.2 \u500d\uff0c\u540c\u6642\u6709\u6548\u5730\u4fdd\u7559\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52d9\u4e0a\u7684\u6548\u80fd\uff0c\u800c\u984d\u5916\u7684\u53c3\u6578\u958b\u92b7\u6975\u5c0f\u3002", "author": "Yuchen Cai et.al.", "authors": "Yuchen Cai, Ding Cao", "id": "2410.11469v1", "paper_url": "http://arxiv.org/abs/2410.11469v1", "repo": "null"}}