{"2406.07268": {"publish_time": "2024-06-11", "title": "Advancing Grounded Multimodal Named Entity Recognition via LLM-Based Reformulation and Box-Based Segmentation", "paper_summary": "Grounded Multimodal Named Entity Recognition (GMNER) task aims to identify\nnamed entities, entity types and their corresponding visual regions. GMNER task\nexhibits two challenging attributes: 1) The tenuous correlation between images\nand text on social media contributes to a notable proportion of named entities\nbeing ungroundable. 2) There exists a distinction between coarse-grained noun\nphrases used in similar tasks (e.g., phrase localization) and fine-grained\nnamed entities. In this paper, we propose RiVEG, a unified framework that\nreformulates GMNER into a joint MNER-VE-VG task by leveraging large language\nmodels (LLMs) as connecting bridges. This reformulation brings two benefits: 1)\nIt enables us to optimize the MNER module for optimal MNER performance and\neliminates the need to pre-extract region features using object detection\nmethods, thus naturally addressing the two major limitations of existing GMNER\nmethods. 2) The introduction of Entity Expansion Expression module and Visual\nEntailment (VE) module unifies Visual Grounding (VG) and Entity Grounding (EG).\nThis endows the proposed framework with unlimited data and model scalability.\nFurthermore, to address the potential ambiguity stemming from the\ncoarse-grained bounding box output in GMNER, we further construct the new\nSegmented Multimodal Named Entity Recognition (SMNER) task and corresponding\nTwitter-SMNER dataset aimed at generating fine-grained segmentation masks, and\nexperimentally demonstrate the feasibility and effectiveness of using box\nprompt-based Segment Anything Model (SAM) to empower any GMNER model with the\nability to accomplish the SMNER task. Extensive experiments demonstrate that\nRiVEG significantly outperforms SoTA methods on four datasets across the MNER,\nGMNER, and SMNER tasks.", "paper_summary_zh": "<paragraph>\u63a5\u5730\u591a\u6a21\u6001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (GMNER) \u4efb\u52a1\u65e8\u5728\u8bc6\u522b\u547d\u540d\u5b9e\u4f53\u3001\u5b9e\u4f53\u7c7b\u578b\u53ca\u5176\u5bf9\u5e94\u7684\u89c6\u89c9\u533a\u57df\u3002GMNER \u4efb\u52a1\u5c55\u73b0\u51fa\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5c5e\u6027\uff1a1) \u56fe\u50cf\u548c\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e4b\u95f4\u7684\u5fae\u5f31\u5173\u8054\u5bfc\u81f4\u76f8\u5f53\u4e00\u90e8\u5206\u547d\u540d\u5b9e\u4f53\u65e0\u6cd5\u63a5\u5730\u30022) \u5728\u7c7b\u4f3c\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u77ed\u8bed\u5b9a\u4f4d\uff09\u4e2d\u4f7f\u7528\u7684\u7c97\u7c92\u5ea6\u540d\u8bcd\u77ed\u8bed\u4e0e\u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u4e4b\u95f4\u5b58\u5728\u533a\u522b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 RiVEG\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4f5c\u4e3a\u8fde\u63a5\u6865\u6881\uff0c\u5c06 GMNER \u91cd\u65b0\u8868\u8ff0\u4e3a\u8054\u5408 MNER-VE-VG \u4efb\u52a1\u3002\u8fd9\u79cd\u91cd\u65b0\u8868\u8ff0\u5e26\u6765\u4e86\u4e24\u4e2a\u597d\u5904\uff1a1) \u5b83\u4f7f\u6211\u4eec\u80fd\u591f\u9488\u5bf9\u6700\u4f73 MNER \u6027\u80fd\u4f18\u5316 MNER \u6a21\u5757\uff0c\u5e76\u6d88\u9664\u4e86\u4f7f\u7528\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u9884\u5148\u63d0\u53d6\u533a\u57df\u7279\u5f81\u7684\u9700\u8981\uff0c\u4ece\u800c\u81ea\u7136\u5730\u89e3\u51b3\u4e86\u73b0\u6709 GMNER \u65b9\u6cd5\u7684\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\u30022) \u5b9e\u4f53\u6269\u5c55\u8868\u8fbe\u5f0f\u6a21\u5757\u548c\u89c6\u89c9\u8574\u6db5 (VE) \u6a21\u5757\u7684\u5f15\u5165\u7edf\u4e00\u4e86\u89c6\u89c9\u63a5\u5730 (VG) \u548c\u5b9e\u4f53\u63a5\u5730 (EG)\u3002\u8fd9\u8d4b\u4e88\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u65e0\u9650\u7684\u6570\u636e\u548c\u6a21\u578b\u53ef\u6269\u5c55\u6027\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u89e3\u51b3\u6e90\u81ea GMNER \u4e2d\u7c97\u7c92\u5ea6\u8fb9\u754c\u6846\u8f93\u51fa\u7684\u6f5c\u5728\u6b67\u4e49\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u6784\u5efa\u4e86\u65b0\u7684\u5206\u6bb5\u591a\u6a21\u6001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (SMNER) \u4efb\u52a1\u548c\u76f8\u5e94\u7684 Twitter-SMNER \u6570\u636e\u96c6\uff0c\u65e8\u5728\u751f\u6210\u7ec6\u7c92\u5ea6\u5206\u5272\u63a9\u7801\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u4f7f\u7528\u57fa\u4e8e\u6846\u63d0\u793a\u7684 Segment Anything Model (SAM) \u589e\u5f3a\u4efb\u4f55 GMNER \u6a21\u578b\u4ee5\u5b8c\u6210 SMNER \u4efb\u52a1\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRiVEG \u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684 MNER\u3001GMNER \u548c SMNER \u4efb\u52a1\u4e2d\u660e\u663e\u4f18\u4e8e SoTA \u65b9\u6cd5\u3002</paragraph>", "author": "Jinyuan Li et.al.", "authors": "Jinyuan Li, Ziyan Li, Han Li, Jianfei Yu, Rui Xia, Di Sun, Gang Pan", "id": "2406.07268v1", "paper_url": "http://arxiv.org/abs/2406.07268v1", "repo": "null"}}