{"2502.15681": {"publish_time": "2025-02-21", "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching", "paper_summary": "Sampling from diffusion models involves a slow iterative process that hinders\ntheir practical deployment, especially for interactive applications. To\naccelerate generation speed, recent approaches distill a multi-step diffusion\nmodel into a single-step student generator via variational score distillation,\nwhich matches the distribution of samples generated by the student to the\nteacher's distribution. However, these approaches use the reverse\nKullback-Leibler (KL) divergence for distribution matching which is known to be\nmode seeking. In this paper, we generalize the distribution matching approach\nusing a novel $f$-divergence minimization framework, termed $f$-distill, that\ncovers different divergences with different trade-offs in terms of mode\ncoverage and training variance. We derive the gradient of the $f$-divergence\nbetween the teacher and student distributions and show that it is expressed as\nthe product of their score differences and a weighting function determined by\ntheir density ratio. This weighting function naturally emphasizes samples with\nhigher density in the teacher distribution, when using a less mode-seeking\ndivergence. We observe that the popular variational score distillation approach\nusing the reverse-KL divergence is a special case within our framework.\nEmpirically, we demonstrate that alternative $f$-divergences, such as\nforward-KL and Jensen-Shannon divergences, outperform the current best\nvariational score distillation methods across image generation tasks. In\nparticular, when using Jensen-Shannon divergence, $f$-distill achieves current\nstate-of-the-art one-step generation performance on ImageNet64 and zero-shot\ntext-to-image generation on MS-COCO. Project page:\nhttps://research.nvidia.com/labs/genair/f-distill", "paper_summary_zh": "\u5f9e\u64f4\u6563\u6a21\u578b\u4e2d\u53d6\u6a23\u6d89\u53ca\u4e00\u500b\u7de9\u6162\u7684\u8fed\u4ee3\u904e\u7a0b\uff0c\u9019\u6703\u963b\u7919\u5b83\u5011\u7684\u5be6\u969b\u90e8\u7f72\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u4e92\u52d5\u5f0f\u61c9\u7528\u7a0b\u5f0f\u3002\u70ba\u4e86\u52a0\u901f\u751f\u6210\u901f\u5ea6\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u900f\u904e\u8b8a\u7570\u5206\u6578\u84b8\u993e\u5c07\u591a\u6b65\u9a5f\u64f4\u6563\u6a21\u578b\u63d0\u7149\u6210\u55ae\u6b65\u9a5f\u5b78\u751f\u751f\u6210\u5668\uff0c\u9019\u6703\u5c07\u5b78\u751f\u751f\u6210\u7684\u6a23\u672c\u5206\u4f48\u8207\u6559\u5e2b\u7684\u5206\u4f48\u76f8\u5339\u914d\u3002\u7136\u800c\uff0c\u9019\u4e9b\u65b9\u6cd5\u4f7f\u7528\u53cd\u5411 Kullback-Leibler (KL) \u6563\u5ea6\u9032\u884c\u5206\u4f48\u5339\u914d\uff0c\u5df2\u77e5\u9019\u6703\u5c0b\u6c42\u6a21\u5f0f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4f7f\u7528\u65b0\u7a4e\u7684 $f$-\u6563\u5ea6\u6700\u5c0f\u5316\u67b6\u69cb\uff0c\u7a31\u70ba $f$-distill\uff0c\u5c07\u5206\u4f48\u5339\u914d\u65b9\u6cd5\u6982\u62ec\u5316\uff0c\u5b83\u6db5\u84cb\u4e86\u5177\u6709\u4e0d\u540c\u6a21\u5f0f\u8986\u84cb\u7387\u548c\u8a13\u7df4\u8b8a\u7570\u6298\u8877\u7684\u4e0d\u540c\u6563\u5ea6\u3002\u6211\u5011\u63a8\u5c0e\u51fa\u6559\u5e2b\u548c\u5b78\u751f\u5206\u4f48\u4e4b\u9593\u7684 $f$-\u6563\u5ea6\u7684\u68af\u5ea6\uff0c\u4e26\u8868\u660e\u5b83\u8868\u793a\u70ba\u5b83\u5011\u7684\u5206\u6578\u5dee\u7684\u4e58\u7a4d\u548c\u7531\u5b83\u5011\u7684\u5bc6\u5ea6\u6bd4\u6c7a\u5b9a\u7684\u52a0\u6b0a\u51fd\u6578\u3002\u7576\u4f7f\u7528\u8f03\u5c11\u5c0b\u6c42\u6a21\u5f0f\u7684\u6563\u5ea6\u6642\uff0c\u6b64\u52a0\u6b0a\u51fd\u6578\u81ea\u7136\u6703\u5f37\u8abf\u6559\u5e2b\u5206\u4f48\u4e2d\u5bc6\u5ea6\u8f03\u9ad8\u7684\u6a23\u672c\u3002\u6211\u5011\u89c0\u5bdf\u5230\u4f7f\u7528\u53cd\u5411 KL \u6563\u5ea6\u7684\u6d41\u884c\u8b8a\u7570\u5206\u6578\u84b8\u993e\u65b9\u6cd5\u662f\u6211\u5011\u67b6\u69cb\u4e2d\u7684\u7279\u4f8b\u3002\u6839\u64da\u7d93\u9a57\uff0c\u6211\u5011\u8b49\u660e\u4e86\u66ff\u4ee3 $f$-\u6563\u5ea6\uff0c\u4f8b\u5982\u524d\u5411 KL \u548c Jensen-Shannon \u6563\u5ea6\uff0c\u5728\u5f71\u50cf\u751f\u6210\u4efb\u52d9\u4e2d\u512a\u65bc\u76ee\u524d\u6700\u597d\u7684\u8b8a\u7570\u5206\u6578\u84b8\u993e\u65b9\u6cd5\u3002\u7279\u5225\u662f\uff0c\u7576\u4f7f\u7528 Jensen-Shannon \u6563\u5ea6\u6642\uff0c$f$-distill \u5728 ImageNet64 \u4e0a\u5be6\u73fe\u4e86\u76ee\u524d\u6700\u5148\u9032\u7684\u55ae\u6b65\u751f\u6210\u6548\u80fd\uff0c\u4ee5\u53ca\u5728 MS-COCO \u4e0a\u5be6\u73fe\u4e86\u96f6\u6b21\u5b78\u7fd2\u6587\u5b57\u8f49\u5f71\u50cf\u751f\u6210\u3002\u5c08\u6848\u9801\u9762\uff1a\nhttps://research.nvidia.com/labs/genair/f-distill", "author": "Yilun Xu et.al.", "authors": "Yilun Xu, Weili Nie, Arash Vahdat", "id": "2502.15681v1", "paper_url": "http://arxiv.org/abs/2502.15681v1", "repo": "null"}}