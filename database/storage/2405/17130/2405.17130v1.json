{"2405.17130": {"publish_time": "2024-05-27", "title": "Exploiting the Layered Intrinsic Dimensionality of Deep Models for Practical Adversarial Training", "paper_summary": "Despite being a heavily researched topic, Adversarial Training (AT) is\nrarely, if ever, deployed in practical AI systems for two primary reasons: (i)\nthe gained robustness is frequently accompanied by a drop in generalization and\n(ii) generating adversarial examples (AEs) is computationally prohibitively\nexpensive. To address these limitations, we propose SMAAT, a new AT algorithm\nthat leverages the manifold conjecture, stating that off-manifold AEs lead to\nbetter robustness while on-manifold AEs result in better generalization.\nSpecifically, SMAAT aims at generating a higher proportion of off-manifold AEs\nby perturbing the intermediate deepnet layer with the lowest intrinsic\ndimension. This systematically results in better scalability compared to\nclassical AT as it reduces the PGD chains length required for generating the\nAEs. Additionally, our study provides, to the best of our knowledge, the first\nexplanation for the difference in the generalization and robustness trends\nbetween vision and language models, ie., AT results in a drop in generalization\nin vision models whereas, in encoder-based language models, generalization\neither improves or remains unchanged. We show that vision transformers and\ndecoder-based models tend to have low intrinsic dimensionality in the earlier\nlayers of the network (more off-manifold AEs), while encoder-based models have\nlow intrinsic dimensionality in the later layers. We demonstrate the efficacy\nof SMAAT; on several tasks, including robustifying (i) sentiment classifiers,\n(ii) safety filters in decoder-based models, and (iii) retrievers in RAG\nsetups. SMAAT requires only 25-33% of the GPU time compared to standard AT,\nwhile significantly improving robustness across all applications and\nmaintaining comparable generalization.", "paper_summary_zh": "\u5118\u7ba1\u5c0d\u6297\u8a13\u7df4 (AT) \u662f\u500b\u88ab\u5ee3\u6cdb\u7814\u7a76\u7684\u4e3b\u984c\uff0c\u4f46\u5be6\u969b\u4e0a\u5f88\u5c11\uff08\u751a\u81f3\u5f9e\u672a\uff09\u88ab\u90e8\u7f72\u5728\u5be6\u52d9 AI \u7cfb\u7d71\u4e2d\uff0c\u4e3b\u8981\u539f\u56e0\u6709\u5169\u500b\uff1a(i) \u7372\u5f97\u7684\u7a69\u5065\u6027\u7d93\u5e38\u4f34\u96a8\u8457\u6982\u62ec\u6027\u7684\u4e0b\u964d\uff0c\u4ee5\u53ca (ii) \u751f\u6210\u5c0d\u6297\u7bc4\u4f8b (AE) \u5728\u8a08\u7b97\u4e0a\u904e\u65bc\u6602\u8cb4\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86 SMAAT\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684 AT \u6f14\u7b97\u6cd5\uff0c\u5b83\u5229\u7528\u6d41\u5f62\u731c\u60f3\uff0c\u6307\u51fa\u6d41\u5f62\u5916\u7684 AE \u53ef\u5e36\u4f86\u66f4\u597d\u7684\u7a69\u5065\u6027\uff0c\u800c\u6d41\u5f62\u4e0a\u7684 AE \u5247\u53ef\u5e36\u4f86\u66f4\u597d\u7684\u6982\u62ec\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cSMAAT \u65e8\u5728\u900f\u904e\u64fe\u52d5\u5167\u5728\u7dad\u5ea6\u6700\u4f4e\u7684\u4e2d\u9593\u6df1\u5ea6\u7db2\u8def\u5c64\u4f86\u7522\u751f\u66f4\u9ad8\u6bd4\u4f8b\u7684\u6d41\u5f62\u5916 AE\u3002\u8207\u50b3\u7d71 AT \u76f8\u6bd4\uff0c\u9019\u7cfb\u7d71\u6027\u5730\u7522\u751f\u4e86\u66f4\u597d\u7684\u53ef\u64f4\u5145\u6027\uff0c\u56e0\u70ba\u5b83\u6e1b\u5c11\u4e86\u7522\u751f AE \u6240\u9700\u7684 PGD \u93c8\u9577\u3002\u6b64\u5916\uff0c\u6211\u5011\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u95dc\u65bc\u8996\u89ba\u548c\u8a9e\u8a00\u6a21\u578b\u4e4b\u9593\u7684\u6982\u62ec\u6027\u548c\u7a69\u5065\u6027\u8da8\u52e2\u5dee\u7570\u7684\u7b2c\u4e00\u500b\u89e3\u91cb\uff0c\u64da\u6211\u5011\u6240\u77e5\uff0c\u5373 AT \u5c0e\u81f4\u8996\u89ba\u6a21\u578b\u7684\u6982\u62ec\u6027\u4e0b\u964d\uff0c\u800c\u5728\u57fa\u65bc\u7de8\u78bc\u5668\u7684\u8a9e\u8a00\u6a21\u578b\u4e2d\uff0c\u6982\u62ec\u6027\u5247\u6703\u6539\u5584\u6216\u4fdd\u6301\u4e0d\u8b8a\u3002\u6211\u5011\u8868\u660e\uff0c\u8996\u89ba\u8b8a\u63db\u5668\u548c\u57fa\u65bc\u89e3\u78bc\u5668\u7684\u6a21\u578b\u5728\u7db2\u8def\u7684\u65e9\u671f\u5c64\u4e2d\u5f80\u5f80\u5177\u6709\u8f03\u4f4e\u7684\u5167\u5728\u7dad\u5ea6\uff08\u66f4\u591a\u6d41\u5f62\u5916 AE\uff09\uff0c\u800c\u57fa\u65bc\u7de8\u78bc\u5668\u7684\u6a21\u578b\u5728\u5f8c\u4f86\u7684\u5c64\u4e2d\u5177\u6709\u8f03\u4f4e\u7684\u5167\u5728\u7dad\u5ea6\u3002\u6211\u5011\u5c55\u793a\u4e86 SMAAT \u7684\u529f\u6548\uff1b\u5728\u591a\u9805\u4efb\u52d9\u4e2d\uff0c\u5305\u62ec\u5f37\u5316 (i) \u60c5\u7dd2\u5206\u985e\u5668\u3001(ii) \u57fa\u65bc\u89e3\u78bc\u5668\u7684\u6a21\u578b\u4e2d\u7684\u5b89\u5168\u7be9\u9078\u5668\uff0c\u4ee5\u53ca (iii) RAG \u8a2d\u5b9a\u4e2d\u7684\u6aa2\u7d22\u5668\u3002\u8207\u6a19\u6e96 AT \u76f8\u6bd4\uff0cSMAAT \u53ea\u9700\u8981 25-33% \u7684 GPU \u6642\u9593\uff0c\u540c\u6642\u986f\u8457\u63d0\u9ad8\u4e86\u6240\u6709\u61c9\u7528\u7a0b\u5f0f\u7684\u7a69\u5065\u6027\uff0c\u4e26\u7dad\u6301\u4e86\u76f8\u7576\u7684\u6982\u62ec\u6027\u3002", "author": "Enes Altinisik et.al.", "authors": "Enes Altinisik, Safa Messaoud, Husrev Taha Sencar, Hassan Sajjad, Sanjay Chawla", "id": "2405.17130v1", "paper_url": "http://arxiv.org/abs/2405.17130v1", "repo": "null"}}