{"2404.18271": {"publish_time": "2024-04-28", "title": "Parameter-Efficient Tuning Large Language Models for Graph Representation Learning", "paper_summary": "Text-rich graphs, which exhibit rich textual information on nodes and edges,\nare prevalent across a wide range of real-world business applications. Large\nLanguage Models (LLMs) have demonstrated remarkable abilities in understanding\ntext, which also introduced the potential for more expressive modeling in\ntext-rich graphs. Despite these capabilities, efficiently applying LLMs to\nrepresentation learning on graphs presents significant challenges. Recently,\nparameter-efficient fine-tuning methods for LLMs have enabled efficient new\ntask generalization with minimal time and memory consumption. Inspired by this,\nwe introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel\napproach for efficient graph representation learning with LLMs on text-rich\ngraphs. Specifically, we utilize a graph neural network (GNN) to encode\nstructural information from neighboring nodes into a graph prompt. This prompt\nis then inserted at the beginning of the text sequence. To improve the quality\nof graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting\nthe next token in the node text. Compared with existing joint GNN and LMs, our\nmethod directly generate the node embeddings from large language models with an\naffordable fine-tuning cost. We validate our approach through comprehensive\nexperiments conducted on 8 different text-rich graphs, observing an average\nimprovement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction\nevaluations. Our results demonstrate the efficacy and efficiency of our model,\nshowing that it can be smoothly integrated with various large language models,\nincluding OPT, LLaMA and Falcon.", "paper_summary_zh": "<paragraph>\u6587\u672c\u4e30\u5bcc\u7684\u56fe\u8868\u5728\u5e7f\u6cdb\u7684\u5b9e\u9645\u5546\u4e1a\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5b83\u4eec\u5728\u8282\u70b9\u548c\u8fb9\u4e0a\u5c55\u793a\u4e30\u5bcc\u7684\u6587\u672c\u4fe1\u606f\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u7406\u89e3\u6587\u672c\u65b9\u9762\u5c55\u73b0\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u8fd9\u4e5f\u4e3a\u5728\u6587\u672c\u4e30\u5bcc\u7684\u56fe\u8868\u4e2d\u8fdb\u884c\u66f4\u5177\u8868\u73b0\u529b\u7684\u5efa\u6a21\u5e26\u6765\u4e86\u53ef\u80fd\u6027\u3002\u5c3d\u7ba1\u6709\u8fd9\u4e9b\u80fd\u529b\uff0c\u4f46\u6709\u6548\u5730\u5c06 LLM \u5e94\u7528\u4e8e\u56fe\u8868\u4e0a\u7684\u8868\u793a\u5b66\u4e60\u4ecd\u7136\u9762\u4e34\u7740\u91cd\u5927\u6311\u6218\u3002\u6700\u8fd1\uff0c\u9488\u5bf9 LLM \u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5df2\u5b9e\u73b0\u9ad8\u6548\u7684\u65b0\u4efb\u52a1\u6cdb\u5316\uff0c\u4e14\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u6700\u5c0f\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u56fe\u611f\u77e5\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 - GPEFT\uff0c\u4e00\u79cd\u5728\u6587\u672c\u4e30\u5bcc\u7684\u56fe\u8868\u4e0a\u4f7f\u7528 LLM \u8fdb\u884c\u9ad8\u6548\u56fe\u8868\u793a\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u5c06\u6765\u81ea\u76f8\u90bb\u8282\u70b9\u7684\u7ed3\u6784\u4fe1\u606f\u7f16\u7801\u5230\u56fe\u63d0\u793a\u4e2d\u3002\u7136\u540e\u5c06\u6b64\u63d0\u793a\u63d2\u5165\u6587\u672c\u5e8f\u5217\u7684\u5f00\u5934\u3002\u4e3a\u4e86\u63d0\u9ad8\u56fe\u63d0\u793a\u7684\u8d28\u91cf\uff0c\u6211\u4eec\u9884\u8bad\u7ec3\u4e86 GNN \u4ee5\u5e2e\u52a9\u51bb\u7ed3\u7684 LLM \u9884\u6d4b\u8282\u70b9\u6587\u672c\u4e2d\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u3002\u4e0e\u73b0\u6709\u7684\u8054\u5408 GNN \u548c LM \u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u76f4\u63a5\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8282\u70b9\u5d4c\u5165\uff0c\u4e14\u5fae\u8c03\u6210\u672c\u4f4e\u5ec9\u3002\u6211\u4eec\u901a\u8fc7\u5728 8 \u4e2a\u4e0d\u540c\u7684\u6587\u672c\u4e30\u5bcc\u56fe\u8868\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u89c2\u5bdf\u5230\u5728\u94fe\u63a5\u9884\u6d4b\u8bc4\u4f30\u4e2d\uff0chit@1 \u548c\u5e73\u5747\u5012\u6570\u79e9 (MRR) \u5e73\u5747\u63d0\u9ad8\u4e86 2%\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u8868\u660e\u5b83\u53ef\u4ee5\u4e0e\u5404\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec OPT\u3001LLaMA \u548c Falcon\uff09\u5e73\u6ed1\u96c6\u6210\u3002</paragraph>", "author": "Qi Zhu et.al.", "authors": "Qi Zhu, Da Zheng, Xiang Song, Shichang Zhang, Bowen Jin, Yizhou Sun, George Karypis", "id": "2404.18271v1", "paper_url": "http://arxiv.org/abs/2404.18271v1", "repo": "null"}}