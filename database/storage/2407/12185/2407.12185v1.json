{"2407.12185": {"publish_time": "2024-07-16", "title": "Satisficing Exploration for Deep Reinforcement Learning", "paper_summary": "A default assumption in the design of reinforcement-learning algorithms is\nthat a decision-making agent always explores to learn optimal behavior. In\nsufficiently complex environments that approach the vastness and scale of the\nreal world, however, attaining optimal performance may in fact be an entirely\nintractable endeavor and an agent may seldom find itself in a position to\ncomplete the requisite exploration for identifying an optimal policy. Recent\nwork has leveraged tools from information theory to design agents that\ndeliberately forgo optimal solutions in favor of sufficiently-satisfying or\nsatisficing solutions, obtained through lossy compression. Notably, such agents\nmay employ fundamentally different exploratory decisions to learn satisficing\nbehaviors more efficiently than optimal ones that are more data intensive.\nWhile supported by a rigorous corroborating theory, the underlying algorithm\nrelies on model-based planning, drastically limiting the compatibility of these\nideas with function approximation and high-dimensional observations. In this\nwork, we remedy this issue by extending an agent that directly represents\nuncertainty over the optimal value function allowing it to both bypass the need\nfor model-based planning and to learn satisficing policies. We provide simple\nyet illustrative experiments that demonstrate how our algorithm enables deep\nreinforcement-learning agents to achieve satisficing behaviors. In keeping with\nprevious work on this setting for multi-armed bandits, we additionally find\nthat our algorithm is capable of synthesizing optimal behaviors, when feasible,\nmore efficiently than its non-information-theoretic counterpart.", "paper_summary_zh": "\u5728\u5f37\u5316\u5b78\u7fd2\u6f14\u7b97\u6cd5\u7684\u8a2d\u8a08\u4e2d\uff0c\u4e00\u500b\u9810\u8a2d\u7684\u5047\u8a2d\u662f\u6c7a\u7b56\u5236\u5b9a\u8005\u59cb\u7d42\u6703\u63a2\u7d22\u4ee5\u5b78\u7fd2\u6700\u4f73\u884c\u70ba\u3002\u7136\u800c\uff0c\u5728\u8db3\u5920\u8907\u96dc\u4e14\u63a5\u8fd1\u771f\u5be6\u4e16\u754c\u5ee3\u95ca\u6027\u548c\u898f\u6a21\u7684\u74b0\u5883\u4e2d\uff0c\u9054\u6210\u6700\u4f73\u6548\u80fd\u4e8b\u5be6\u4e0a\u53ef\u80fd\u662f\u4e00\u9805\u5b8c\u5168\u96e3\u4ee5\u89e3\u6c7a\u7684\u52aa\u529b\uff0c\u800c\u4e14\u4e00\u500b\u4ee3\u7406\u4eba\u53ef\u80fd\u5f88\u5c11\u767c\u73fe\u81ea\u5df1\u8655\u65bc\u4e00\u500b\u80fd\u5b8c\u6210\u8b58\u5225\u6700\u4f73\u7b56\u7565\u6240\u9700\u7684\u63a2\u7d22\u4f4d\u7f6e\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5229\u7528\u8cc7\u8a0a\u7406\u8ad6\u7684\u5de5\u5177\u4f86\u8a2d\u8a08\u4ee3\u7406\u4eba\uff0c\u9019\u4e9b\u4ee3\u7406\u4eba\u6545\u610f\u653e\u68c4\u6700\u4f73\u89e3\uff0c\u8f49\u800c\u63a1\u7528\u900f\u904e\u6709\u640d\u58d3\u7e2e\u5f97\u5230\u7684\u8db3\u5920\u4ee4\u4eba\u6eff\u610f\u6216\u4ee4\u4eba\u6eff\u610f\u7684\u89e3\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6b64\u985e\u4ee3\u7406\u4eba\u53ef\u80fd\u6703\u63a1\u7528\u6839\u672c\u4e0d\u540c\u7684\u63a2\u7d22\u6027\u6c7a\u7b56\uff0c\u4ee5\u6bd4\u8cc7\u6599\u5bc6\u96c6\u578b\u6700\u4f73\u89e3\u66f4\u6709\u6548\u7387\u5730\u5b78\u7fd2\u4ee4\u4eba\u6eff\u610f\u7684\u884c\u70ba\u3002\u96d6\u7136\u6709\u56b4\u8b39\u7684\u4f50\u8b49\u7406\u8ad6\u652f\u6301\uff0c\u4f46\u57fa\u790e\u6f14\u7b97\u6cd5\u4f9d\u8cf4\u65bc\u57fa\u65bc\u6a21\u578b\u7684\u898f\u5283\uff0c\u9019\u6975\u5927\u5730\u9650\u5236\u4e86\u9019\u4e9b\u60f3\u6cd5\u8207\u51fd\u6578\u903c\u8fd1\u548c\u9ad8\u7dad\u5ea6\u89c0\u6e2c\u7684\u76f8\u5bb9\u6027\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u900f\u904e\u64f4\u5145\u4e00\u500b\u4ee3\u7406\u4eba\u4f86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u8a72\u4ee3\u7406\u4eba\u76f4\u63a5\u8868\u793a\u5c0d\u6700\u4f73\u503c\u51fd\u6578\u7684\u4e0d\u78ba\u5b9a\u6027\uff0c\u4f7f\u5176\u65e2\u80fd\u7e5e\u904e\u5c0d\u57fa\u65bc\u6a21\u578b\u7684\u898f\u5283\u7684\u9700\u6c42\uff0c\u53c8\u80fd\u5b78\u7fd2\u4ee4\u4eba\u6eff\u610f\u7684\u7b56\u7565\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u7c21\u55ae\u4f46\u6709\u8aaa\u660e\u6027\u7684\u5be6\u9a57\uff0c\u5c55\u793a\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u5982\u4f55\u8b93\u6df1\u5ea6\u5f37\u5316\u5b78\u7fd2\u4ee3\u7406\u4eba\u5be6\u73fe\u4ee4\u4eba\u6eff\u610f\u7684\u884c\u70ba\u3002\u8207\u5148\u524d\u91dd\u5c0d\u591a\u81c2\u8001\u864e\u6a5f\u7684\u8a2d\u5b9a\u6240\u505a\u7684\u7814\u7a76\u4fdd\u6301\u4e00\u81f4\uff0c\u6211\u5011\u53e6\u5916\u767c\u73fe\u6211\u5011\u7684\u6f14\u7b97\u6cd5\u6709\u80fd\u529b\u5728\u53ef\u884c\u7684\u60c5\u6cc1\u4e0b\u6bd4\u5176\u975e\u8cc7\u8a0a\u7406\u8ad6\u5c0d\u61c9\u6f14\u7b97\u6cd5\u66f4\u6709\u6548\u7387\u5730\u7d9c\u5408\u6700\u4f73\u884c\u70ba\u3002", "author": "Dilip Arumugam et.al.", "authors": "Dilip Arumugam, Saurabh Kumar, Ramki Gummadi, Benjamin Van Roy", "id": "2407.12185v1", "paper_url": "http://arxiv.org/abs/2407.12185v1", "repo": "null"}}