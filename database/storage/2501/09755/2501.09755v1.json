{"2501.09755": {"publish_time": "2025-01-16", "title": "Learnings from Scaling Visual Tokenizers for Reconstruction and Generation", "paper_summary": "Visual tokenization via auto-encoding empowers state-of-the-art image and\nvideo generative models by compressing pixels into a latent space. Although\nscaling Transformer-based generators has been central to recent advances, the\ntokenizer component itself is rarely scaled, leaving open questions about how\nauto-encoder design choices influence both its objective of reconstruction and\ndownstream generative performance. Our work aims to conduct an exploration of\nscaling in auto-encoders to fill in this blank. To facilitate this exploration,\nwe replace the typical convolutional backbone with an enhanced Vision\nTransformer architecture for Tokenization (ViTok). We train ViTok on\nlarge-scale image and video datasets far exceeding ImageNet-1K, removing data\nconstraints on tokenizer scaling. We first study how scaling the auto-encoder\nbottleneck affects both reconstruction and generation -- and find that while it\nis highly correlated with reconstruction, its relationship with generation is\nmore complex. We next explored the effect of separately scaling the\nauto-encoders' encoder and decoder on reconstruction and generation\nperformance. Crucially, we find that scaling the encoder yields minimal gains\nfor either reconstruction or generation, while scaling the decoder boosts\nreconstruction but the benefits for generation are mixed. Building on our\nexploration, we design ViTok as a lightweight auto-encoder that achieves\ncompetitive performance with state-of-the-art auto-encoders on ImageNet-1K and\nCOCO reconstruction tasks (256p and 512p) while outperforming existing\nauto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x\nfewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates\ncompetitive performance on image generation for ImageNet-1K and sets new\nstate-of-the-art benchmarks for class-conditional video generation on UCF-101.", "paper_summary_zh": "<paragraph>\u900f\u904e\u81ea\u52d5\u7de8\u78bc\u9032\u884c\u8996\u89ba\u6a19\u8a18\u5316\uff0c\u80fd\u5c07\u50cf\u7d20\u58d3\u7e2e\u6210\u6f5b\u5728\u7a7a\u9593\uff0c\u9032\u800c\u589e\u5f37\u6700\u5148\u9032\u7684\u5f71\u50cf\u548c\u5f71\u7247\u751f\u6210\u6a21\u578b\u3002\u5118\u7ba1\u64f4\u5145\u57fa\u65bc Transformer \u7684\u751f\u6210\u5668\u5df2\u6210\u70ba\u8fd1\u671f\u9032\u5c55\u7684\u6838\u5fc3\uff0c\u4f46\u6a19\u8a18\u5316\u5143\u4ef6\u672c\u8eab\u537b\u5f88\u5c11\u88ab\u64f4\u5145\uff0c\u56e0\u6b64\u5c0d\u65bc\u81ea\u52d5\u7de8\u78bc\u5668\u8a2d\u8a08\u9078\u64c7\u5982\u4f55\u5f71\u97ff\u5176\u91cd\u5efa\u76ee\u6a19\u548c\u4e0b\u6e38\u751f\u6210\u6548\u80fd\uff0c\u4ecd\u6709\u5f85\u63a2\u8a0e\u3002\u6211\u5011\u7684\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u81ea\u52d5\u7de8\u78bc\u5668\u7684\u64f4\u5145\uff0c\u4ee5\u586b\u88dc\u9019\u9805\u7a7a\u767d\u3002\u70ba\u4e86\u4fc3\u9032\u6b64\u63a2\u7d22\uff0c\u6211\u5011\u5c07\u5178\u578b\u7684\u5377\u7a4d\u4e3b\u5e79\u66ff\u63db\u70ba\u589e\u5f37\u7684 Tokenization \u8996\u89ba Transformer \u67b6\u69cb (ViTok)\u3002\u6211\u5011\u5728\u9060\u9060\u8d85\u904e ImageNet-1K \u7684\u5927\u578b\u5f71\u50cf\u548c\u5f71\u7247\u8cc7\u6599\u96c6\u4e0a\u8a13\u7df4 ViTok\uff0c\u6d88\u9664\u4e86\u6a19\u8a18\u5316\u64f4\u5145\u7684\u8cc7\u6599\u9650\u5236\u3002\u6211\u5011\u9996\u5148\u7814\u7a76\u64f4\u5145\u81ea\u52d5\u7de8\u78bc\u5668\u74f6\u9838\u5982\u4f55\u5f71\u97ff\u91cd\u5efa\u548c\u751f\u6210\uff0c\u4e26\u767c\u73fe\u5118\u7ba1\u5b83\u8207\u91cd\u5efa\u9ad8\u5ea6\u76f8\u95dc\uff0c\u4f46\u8207\u751f\u6210\u4e4b\u9593\u7684\u95dc\u4fc2\u66f4\u70ba\u8907\u96dc\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u63a2\u8a0e\u4e86\u5206\u5225\u64f4\u5145\u81ea\u52d5\u7de8\u78bc\u5668\u7684\u7de8\u78bc\u5668\u548c\u89e3\u78bc\u5668\u5c0d\u91cd\u5efa\u548c\u751f\u6210\u6548\u80fd\u7684\u5f71\u97ff\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u6211\u5011\u767c\u73fe\u64f4\u5145\u7de8\u78bc\u5668\u5c0d\u91cd\u5efa\u6216\u751f\u6210\u800c\u8a00\u6536\u76ca\u751a\u5fae\uff0c\u800c\u64f4\u5145\u89e3\u78bc\u5668\u6703\u63d0\u5347\u91cd\u5efa\uff0c\u4f46\u5c0d\u751f\u6210\u7684\u76ca\u8655\u5247\u662f\u597d\u58de\u53c3\u534a\u3002\u6839\u64da\u6211\u5011\u7684\u63a2\u7d22\uff0c\u6211\u5011\u5c07 ViTok \u8a2d\u8a08\u70ba\u4e00\u500b\u8f15\u91cf\u7d1a\u81ea\u52d5\u7de8\u78bc\u5668\uff0c\u5728 ImageNet-1K \u548c COCO \u91cd\u5efa\u4efb\u52d9 (256p \u548c 512p) \u4e0a\uff0c\u90fd\u80fd\u9054\u5230\u8207\u6700\u5148\u9032\u81ea\u52d5\u7de8\u78bc\u5668\u76f8\u5ab2\u7f8e\u7684\u6548\u80fd\uff0c\u540c\u6642\u5728 UCF-101 \u7684 16 \u5e40 128p \u5f71\u7247\u91cd\u5efa\u4e0a\u512a\u65bc\u73fe\u6709\u7684\u81ea\u52d5\u7de8\u78bc\u5668\uff0c\u4e14 FLOP \u6578\u6e1b\u5c11 2-5 \u500d\u3002\u8207\u64f4\u6563 Transformer \u6574\u5408\u5f8c\uff0cViTok \u5728 ImageNet-1K \u7684\u5f71\u50cf\u751f\u6210\u4e0a\u5c55\u73fe\u4e86\u7af6\u722d\u529b\uff0c\u4e26\u5728 UCF-101 \u7684\u985e\u5225\u689d\u4ef6\u5f71\u7247\u751f\u6210\u4e0a\u5275\u4e0b\u65b0\u7684\u6700\u5148\u9032\u57fa\u6e96\u3002</paragraph>", "author": "Philippe Hansen-Estruch et.al.", "authors": "Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen", "id": "2501.09755v1", "paper_url": "http://arxiv.org/abs/2501.09755v1", "repo": "null"}}