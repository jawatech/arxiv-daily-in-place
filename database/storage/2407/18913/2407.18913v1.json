{"2407.18913": {"publish_time": "2024-07-26", "title": "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments", "paper_summary": "This work compares ways of extending Reinforcement Learning algorithms to\nPartially Observed Markov Decision Processes (POMDPs) with options. One view of\noptions is as temporally extended action, which can be realized as a memory\nthat allows the agent to retain historical information beyond the policy's\ncontext window. While option assignment could be handled using heuristics and\nhand-crafted objectives, learning temporally consistent options and associated\nsub-policies without explicit supervision is a challenge. Two algorithms, PPOEM\nand SOAP, are proposed and studied in depth to address this problem. PPOEM\napplies the forward-backward algorithm (for Hidden Markov Models) to optimize\nthe expected returns for an option-augmented policy. However, this learning\napproach is unstable during on-policy rollouts. It is also unsuited for\nlearning causal policies without the knowledge of future trajectories, since\noption assignments are optimized for offline sequences where the entire episode\nis available. As an alternative approach, SOAP evaluates the policy gradient\nfor an optimal option assignment. It extends the concept of the generalized\nadvantage estimation (GAE) to propagate option advantages through time, which\nis an analytical equivalent to performing temporal back-propagation of option\npolicy gradients. This option policy is only conditional on the history of the\nagent, not future actions. Evaluated against competing baselines, SOAP\nexhibited the most robust performance, correctly discovering options for POMDP\ncorridor environments, as well as on standard benchmarks including Atari and\nMuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The\nopen-sourced code is available at https://github.com/shuishida/SoapRL.", "paper_summary_zh": "<paragraph>\u9019\u9805\u5de5\u4f5c\u6bd4\u8f03\u4e86\u64f4\u5c55\u5f37 reinforcement learning \u6f14\u7b97\u6cd5\u5230\u5177\u6709\u9078\u9805\u7684 Partially Observed Markov Decision Processes (POMDPs) \u7684\u65b9\u5f0f\u3002\n\u9078\u9805\u7684\u4e00\u7a2e\u89c0\u9ede\u662f\u4f5c\u70ba\u6642\u9593\u5ef6\u4f38\u7684\u52d5\u4f5c\uff0c\u53ef\u4ee5\u5be6\u73fe\u70ba\u4e00\u500b\u8a18\u61b6\u9ad4\uff0c\u5141\u8a31\u4ee3\u7406\u4fdd\u7559\u8d85\u51fa\u653f\u7b56\u4e0a\u4e0b\u6587\u8996\u7a97\u7684\u6b77\u53f2\u8cc7\u8a0a\u3002\n\u96d6\u7136\u9078\u9805\u5206\u914d\u53ef\u4ee5\u4f7f\u7528\u555f\u767c\u6cd5\u548c\u624b\u5de5\u76ee\u6a19\u4f86\u8655\u7406\uff0c\u4f46\u5b78\u7fd2\u6642\u9593\u4e00\u81f4\u7684\u9078\u9805\u548c\u76f8\u95dc\u7684\u5b50\u653f\u7b56\u800c\u6c92\u6709\u660e\u78ba\u7684\u76e3\u7763\u662f\u4e00\u500b\u6311\u6230\u3002\n\u63d0\u51fa\u4e86\u5169\u7a2e\u6f14\u7b97\u6cd5\uff0cPPOEM \u548c SOAP\uff0c\u4e26\u6df1\u5165\u7814\u7a76\u4ee5\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\nPPOEM \u9069\u7528\u6b63\u5411\u5f8c\u5411\u6f14\u7b97\u6cd5\uff08\u5c0d\u65bc Hidden Markov \u6a21\u578b\uff09\u4f86\u6700\u4f73\u5316\u9078\u9805\u589e\u5f37\u653f\u7b56\u7684\u9810\u671f\u56de\u5831\u3002\n\u7136\u800c\uff0c\u9019\u7a2e\u5b78\u7fd2\u65b9\u6cd5\u5728\u57fa\u65bc\u653f\u7b56\u7684\u57f7\u884c\u671f\u9593\u662f\u4e0d\u7a69\u5b9a\u7684\u3002\n\u5b83\u4e5f\u4e0d\u9069\u5408\u5728\u4e0d\u77e5\u9053\u672a\u4f86\u8ecc\u8de1\u7684\u60c5\u6cc1\u4e0b\u5b78\u7fd2\u56e0\u679c\u653f\u7b56\uff0c\u56e0\u70ba\u9078\u9805\u5206\u914d\u662f\u91dd\u5c0d\u96e2\u7dda\u5e8f\u5217\u9032\u884c\u6700\u4f73\u5316\u7684\uff0c\u5176\u4e2d\u6574\u500b\u60c5\u7bc0\u662f\u53ef\u7528\u7684\u3002\n\u4f5c\u70ba\u4e00\u7a2e\u66ff\u4ee3\u65b9\u6cd5\uff0cSOAP \u8a55\u4f30\u6700\u4f73\u9078\u9805\u5206\u914d\u7684\u653f\u7b56\u68af\u5ea6\u3002\n\u5b83\u64f4\u5c55\u4e86\u5ee3\u7fa9\u512a\u52e2\u4f30\u8a08 (GAE) \u7684\u6982\u5ff5\uff0c\u4ee5\u96a8\u8457\u6642\u9593\u50b3\u64ad\u9078\u9805\u512a\u52e2\uff0c\u9019\u8207\u57f7\u884c\u9078\u9805\u653f\u7b56\u68af\u5ea6\u7684\u6642\u9593\u53cd\u5411\u50b3\u64ad\u7684\u5206\u6790\u7b49\u50f9\u3002\n\u9019\u500b\u9078\u9805\u653f\u7b56\u50c5\u53d6\u6c7a\u65bc\u4ee3\u7406\u7684\u6b77\u53f2\uff0c\u800c\u4e0d\u662f\u672a\u4f86\u7684\u52d5\u4f5c\u3002\n\u8207\u7af6\u722d\u57fa\u6e96\u9032\u884c\u8a55\u4f30\uff0cSOAP \u8868\u73fe\u51fa\u6700\u7a69\u5065\u7684\u6548\u80fd\uff0c\u6b63\u78ba\u5730\u767c\u73fe\u4e86 POMDP \u8d70\u5eca\u74b0\u5883\u7684\u9078\u9805\uff0c\u4ee5\u53ca\u5728\u5305\u62ec Atari \u548c MuJoCo \u5728\u5167\u7684\u6a19\u6e96\u57fa\u6e96\u4e0a\uff0c\u512a\u65bc PPOEM\uff0c\u4ee5\u53ca LSTM \u548c Option-Critic \u57fa\u6e96\u3002\n\u958b\u653e\u539f\u59cb\u78bc\u53ef\u4ee5\u5728 https://github.com/shuishida/SoapRL \u53d6\u5f97\u3002</paragraph>", "author": "Shu Ishida et.al.", "authors": "Shu Ishida, Jo\u00e3o F. Henriques", "id": "2407.18913v1", "paper_url": "http://arxiv.org/abs/2407.18913v1", "repo": "null"}}