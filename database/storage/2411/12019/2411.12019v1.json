{"2411.12019": {"publish_time": "2024-11-18", "title": "Regret-Free Reinforcement Learning for LTL Specifications", "paper_summary": "Reinforcement learning (RL) is a promising method to learn optimal control\npolicies for systems with unknown dynamics. In particular, synthesizing\ncontrollers for safety-critical systems based on high-level specifications,\nsuch as those expressed in temporal languages like linear temporal logic (LTL),\npresents a significant challenge in control systems research. Current RL-based\nmethods designed for LTL tasks typically offer only asymptotic guarantees,\nwhich provide no insight into the transient performance during the learning\nphase. While running an RL algorithm, it is crucial to assess how close we are\nto achieving optimal behavior if we stop learning.\n  In this paper, we present the first regret-free online algorithm for learning\na controller that addresses the general class of LTL specifications over Markov\ndecision processes (MDPs) with a finite set of states and actions. We begin by\nproposing a regret-free learning algorithm to solve infinite-horizon\nreach-avoid problems. For general LTL specifications, we show that the\nsynthesis problem can be reduced to a reach-avoid problem when the graph\nstructure is known. Additionally, we provide an algorithm for learning the\ngraph structure, assuming knowledge of a minimum transition probability, which\noperates independently of the main regret-free algorithm.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2 (RL) \u662f\u4e00\u7a2e\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5b78\u7fd2\u672a\u77e5\u52d5\u614b\u7cfb\u7d71\u7684\u6700\u4f73\u63a7\u5236\u7b56\u7565\u3002\u7279\u5225\u662f\uff0c\u57fa\u65bc\u9ad8\u968e\u898f\u7bc4\uff08\u4f8b\u5982\u7528\u7dda\u6027\u6642\u5e8f\u908f\u8f2f (LTL) \u7b49\u6642\u5e8f\u8a9e\u8a00\u8868\u9054\u7684\u898f\u7bc4\uff09\u70ba\u5b89\u5168\u95dc\u9375\u7cfb\u7d71\u5408\u6210\u63a7\u5236\u5668\uff0c\u9019\u5728\u63a7\u5236\u7cfb\u7d71\u7814\u7a76\u4e2d\u662f\u4e00\u500b\u91cd\u5927\u6311\u6230\u3002\u76ee\u524d\u7684\u57fa\u65bc RL \u7684 LTL \u4efb\u52d9\u65b9\u6cd5\u901a\u5e38\u50c5\u63d0\u4f9b\u6f38\u8fd1\u4fdd\u8b49\uff0c\u9019\u5728\u5b78\u7fd2\u968e\u6bb5\u6c92\u6709\u63d0\u4f9b\u66ab\u614b\u6548\u80fd\u7684\u898b\u89e3\u3002\u5728\u57f7\u884c RL \u6f14\u7b97\u6cd5\u6642\uff0c\u5982\u679c\u6211\u5011\u505c\u6b62\u5b78\u7fd2\uff0c\u8a55\u4f30\u6211\u5011\u8ddd\u96e2\u9054\u6210\u6700\u4f73\u884c\u70ba\u6709\u591a\u8fd1\u81f3\u95dc\u91cd\u8981\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7b2c\u4e00\u500b\u7121\u907a\u61be\u7dda\u4e0a\u6f14\u7b97\u6cd5\uff0c\u7528\u65bc\u5b78\u7fd2\u4e00\u500b\u63a7\u5236\u5668\uff0c\u8a72\u63a7\u5236\u5668\u89e3\u6c7a\u4e86\u99ac\u53ef\u592b\u6c7a\u7b56\u904e\u7a0b (MDP) \u4e0a\u7684\u4e00\u822c\u985e\u5225 LTL \u898f\u7bc4\uff0c\u5176\u4e2d\u5305\u542b\u6709\u9650\u7684\u72c0\u614b\u548c\u52d5\u4f5c\u96c6\u5408\u3002\u6211\u5011\u9996\u5148\u63d0\u51fa\u4e00\u500b\u7121\u907a\u61be\u5b78\u7fd2\u6f14\u7b97\u6cd5\u4f86\u89e3\u6c7a\u7121\u9650\u6642\u57df\u5230\u9054\u907f\u514d\u554f\u984c\u3002\u5c0d\u65bc\u4e00\u822c LTL \u898f\u7bc4\uff0c\u6211\u5011\u8868\u660e\u7576\u5716\u5f62\u7d50\u69cb\u5df2\u77e5\u6642\uff0c\u5408\u6210\u554f\u984c\u53ef\u4ee5\u7c21\u5316\u70ba\u5230\u9054\u907f\u514d\u554f\u984c\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u4f9b\u4e86\u4e00\u500b\u6f14\u7b97\u6cd5\u4f86\u5b78\u7fd2\u5716\u5f62\u7d50\u69cb\uff0c\u5047\u8a2d\u77e5\u9053\u6700\u5c0f\u8f49\u79fb\u6a5f\u7387\uff0c\u5b83\u7368\u7acb\u65bc\u4e3b\u8981\u7684\u7121\u907a\u61be\u6f14\u7b97\u6cd5\u904b\u4f5c\u3002", "author": "Rupak Majumdar et.al.", "authors": "Rupak Majumdar, Mahmoud Salamati, Sadegh Soudjani", "id": "2411.12019v1", "paper_url": "http://arxiv.org/abs/2411.12019v1", "repo": "null"}}