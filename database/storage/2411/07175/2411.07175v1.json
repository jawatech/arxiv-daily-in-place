{"2411.07175": {"publish_time": "2024-11-11", "title": "Continual Memorization of Factoids in Large Language Models", "paper_summary": "Large language models can absorb a massive amount of knowledge through\npretraining, but pretraining is inefficient for acquiring long-tailed or\nspecialized facts. Therefore, fine-tuning on specialized or new knowledge that\nreflects changes in the world has become popular, though it risks disrupting\nthe model's original capabilities. We study this fragility in the context of\ncontinual memorization, where the model is trained on a small set of long-tail\nfactoids (factual associations) and must retain these factoids after multiple\nstages of subsequent training on other datasets. Through extensive experiments,\nwe show that LLMs suffer from forgetting across a wide range of subsequent\ntasks, and simple replay techniques do not fully prevent forgetting, especially\nwhen the factoid datasets are trained in the later stages. We posit that there\nare two ways to alleviate forgetting: 1) protect the memorization process as\nthe model learns the factoids, or 2) reduce interference from training in later\nstages. With this insight, we develop an effective mitigation strategy: REMIX\n(Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic\ndata sampled from pretraining corpora or even randomly generated word sequences\nduring each stage, despite being unrelated to the memorized factoids in the\nfirst stage. REMIX can recover performance from severe forgetting, often\noutperforming replay-based methods that have access to the factoids from the\nfirst stage. We then analyze how REMIX alters the learning process and find\nthat successful forgetting prevention is associated with a pattern: the model\nstores factoids in earlier layers than usual and diversifies the set of layers\nthat store these factoids. The efficacy of REMIX invites further investigation\ninto the underlying dynamics of memorization and forgetting, opening exciting\npossibilities for future research.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u53ef\u900f\u904e\u9810\u8a13\u7df4\u5438\u6536\u5927\u91cf\u77e5\u8b58\uff0c\u4f46\u9810\u8a13\u7df4\u5c0d\u65bc\u7372\u53d6\u9577\u5c3e\u6216\u5c08\u696d\u77e5\u8b58\u800c\u8a00\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u91dd\u5c0d\u53cd\u6620\u4e16\u754c\u8b8a\u5316\u7684\u5c08\u696d\u6216\u65b0\u77e5\u8b58\u9032\u884c\u5fae\u8abf\u5df2\u8b8a\u5f97\u666e\u904d\uff0c\u5118\u7ba1\u9019\u6709\u7834\u58de\u6a21\u578b\u539f\u59cb\u529f\u80fd\u7684\u98a8\u96aa\u3002\u6211\u5011\u5728\u6301\u7e8c\u8a18\u61b6\u7684\u80cc\u666f\u4e0b\u7814\u7a76\u9019\u7a2e\u8106\u5f31\u6027\uff0c\u5176\u4e2d\u6a21\u578b\u5728\u5c11\u91cf\u9577\u5c3e\u4e8b\u5be6\uff08\u4e8b\u5be6\u95dc\u806f\uff09\u4e0a\u9032\u884c\u8a13\u7df4\uff0c\u4e26\u4e14\u5fc5\u9808\u5728\u5f8c\u7e8c\u5728\u5176\u4ed6\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u591a\u500b\u968e\u6bb5\u8a13\u7df4\u5f8c\u4fdd\u7559\u9019\u4e9b\u4e8b\u5be6\u3002\u900f\u904e\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u8868\u660e LLM \u5728\u5404\u7a2e\u5f8c\u7e8c\u4efb\u52d9\u4e2d\u6703\u767c\u751f\u907a\u5fd8\uff0c\u800c\u4e14\u7c21\u55ae\u7684\u91cd\u64ad\u6280\u8853\u4e26\u4e0d\u80fd\u5b8c\u5168\u9632\u6b62\u907a\u5fd8\uff0c\u7279\u5225\u662f\u5728\u5f8c\u7e8c\u968e\u6bb5\u8a13\u7df4\u4e8b\u5be6\u8cc7\u6599\u96c6\u6642\u3002\u6211\u5011\u5047\u8a2d\u6709\u5169\u7a2e\u65b9\u6cd5\u53ef\u4ee5\u6e1b\u8f15\u907a\u5fd8\uff1a1) \u5728\u6a21\u578b\u5b78\u7fd2\u4e8b\u5be6\u6642\u4fdd\u8b77\u8a18\u61b6\u904e\u7a0b\uff0c\u6216 2) \u6e1b\u5c11\u5f8c\u7e8c\u968e\u6bb5\u8a13\u7df4\u7684\u5e72\u64fe\u3002\u6709\u4e86\u9019\u500b\u898b\u89e3\uff0c\u6211\u5011\u5236\u5b9a\u4e86\u4e00\u500b\u6709\u6548\u7684\u7de9\u89e3\u7b56\u7565\uff1aREMIX\uff08\u96a8\u6a5f\u548c\u901a\u7528\u8cc7\u6599\u6df7\u5408\uff09\u3002REMIX \u900f\u904e\u5728\u6bcf\u500b\u968e\u6bb5\u6df7\u5408\u5f9e\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u4e2d\u53d6\u6a23\u7684\u901a\u7528\u8cc7\u6599\uff0c\u751a\u81f3\u96a8\u6a5f\u7522\u751f\u7684\u5b57\u8a5e\u5e8f\u5217\u4f86\u9632\u6b62\u907a\u5fd8\uff0c\u5118\u7ba1\u8207\u7b2c\u4e00\u968e\u6bb5\u4e2d\u8a18\u61b6\u7684\u4e8b\u5be6\u7121\u95dc\u3002REMIX \u53ef\u4ee5\u5f9e\u56b4\u91cd\u7684\u907a\u5fd8\u4e2d\u6062\u5fa9\u6548\u80fd\uff0c\u901a\u5e38\u512a\u65bc\u53ef\u4ee5\u5b58\u53d6\u7b2c\u4e00\u968e\u6bb5\u4e8b\u5be6\u7684\u57fa\u65bc\u91cd\u64ad\u7684\u65b9\u6cd5\u3002\u7136\u5f8c\u6211\u5011\u5206\u6790 REMIX \u5982\u4f55\u6539\u8b8a\u5b78\u7fd2\u904e\u7a0b\uff0c\u4e26\u767c\u73fe\u6210\u529f\u7684\u907a\u5fd8\u9810\u9632\u8207\u4e00\u500b\u6a21\u5f0f\u76f8\u95dc\uff1a\u6a21\u578b\u6bd4\u5e73\u5e38\u66f4\u65e9\u5c07\u4e8b\u5be6\u5132\u5b58\u5728\u8f03\u65e9\u7684\u5c64\u4e2d\uff0c\u4e26\u5c07\u5132\u5b58\u9019\u4e9b\u4e8b\u5be6\u7684\u5c64\u7d44\u591a\u6a23\u5316\u3002REMIX \u7684\u529f\u6548\u4fc3\u4f7f\u9032\u4e00\u6b65\u7814\u7a76\u8a18\u61b6\u548c\u907a\u5fd8\u7684\u57fa\u790e\u52d5\u614b\uff0c\u70ba\u672a\u4f86\u7684\u7814\u7a76\u958b\u555f\u4ee4\u4eba\u8208\u596e\u7684\u53ef\u80fd\u6027\u3002", "author": "Howard Chen et.al.", "authors": "Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, Danqi Chen", "id": "2411.07175v1", "paper_url": "http://arxiv.org/abs/2411.07175v1", "repo": "null"}}