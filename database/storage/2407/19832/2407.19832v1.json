{"2407.19832": {"publish_time": "2024-07-29", "title": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2", "paper_summary": "Multimodal Large Language Models (MLLMs) have attracted much attention due to\ntheir multifunctionality. However, traditional Transformer architectures incur\nsignificant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model that\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear extension and fast processing of long sequences. We replace the\nTransformer based backbone with a pre-trained Mamba-2 model and explore methods\nfor integrating 2D visual selective scanning mechanisms into multimodal\nlearning. We also try various visual encoders and Mamba-2 model variants. Our\nextensive experiments conducted in various multimodal benchmark tests have\ndemonstrated the competitive performance of ML-Mamba and highlighted the\npotential of state space models in multimodal tasks. The experimental results\nshow that: (1) ML-Mamba achieves performance comparable to state-of-the-art\nmethods such as TinyLaVA and MobileVLM v2 through its linear sequential\nmodeling, while also having faster inference speed; (2) ML-Mamba performs well\nin visual hallucinations and spatial relationship judgment in closed set\nbenchmark tests; (3) ML-Mamba achieves performance comparable to LLaVA while\nreducing the number of parameters by 40\\%.(4) Compared to the multimodal model\nusing the original Mamba model, the Mamba-2 based large-scale multimodal\nlanguage model has stronger inference performance and effectiveness.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM) \u56e0\u5176\u591a\u529f\u80fd\u6027\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684 Transformer \u67b6\u6784\u7531\u4e8e\u5176\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u6027\u800c\u4ea7\u751f\u5927\u91cf\u7684\u5f00\u9500\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ML-Mamba\uff0c\u8fd9\u662f\u4e00\u79cd\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u5229\u7528\u6700\u65b0\u4e14\u9ad8\u6548\u7684 Mamba-2 \u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002Mamba-2 \u4ee5\u5176\u7ebf\u6027\u6269\u5c55\u548c\u5bf9\u957f\u5e8f\u5217\u7684\u5feb\u901f\u5904\u7406\u800c\u95fb\u540d\u3002\u6211\u4eec\u7528\u9884\u5148\u8bad\u7ec3\u7684 Mamba-2 \u6a21\u578b\u66ff\u6362\u4e86\u57fa\u4e8e Transformer \u7684\u9aa8\u5e72\uff0c\u5e76\u63a2\u7d22\u5c06\u4e8c\u7ef4\u53ef\u89c6\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\u96c6\u6210\u5230\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u5c1d\u8bd5\u4e86\u5404\u79cd\u89c6\u89c9\u7f16\u7801\u5668\u548c Mamba-2 \u6a21\u578b\u53d8\u4f53\u3002\u6211\u4eec\u5728\u5404\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u5df2\u7ecf\u8bc1\u660e\u4e86 ML-Mamba \u7684\u7ade\u4e89\u6027\u80fd\uff0c\u5e76\u7a81\u51fa\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a(1) ML-Mamba \u901a\u8fc7\u5176\u7ebf\u6027\u987a\u5e8f\u5efa\u6a21\u5b9e\u73b0\u4e86\u4e0e TinyLaVA \u548c MobileVLM v2 \u7b49\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8fd8\u5177\u6709\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff1b(2) ML-Mamba \u5728\u5c01\u95ed\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5728\u89c6\u89c9\u5e7b\u89c9\u548c\u7a7a\u95f4\u5173\u7cfb\u5224\u65ad\u65b9\u9762\u8868\u73b0\u826f\u597d\uff1b(3) ML-Mamba \u5728\u5c06\u53c2\u6570\u6570\u91cf\u51cf\u5c11 40% \u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e LLaVA \u76f8\u5f53\u7684\u6027\u80fd\u3002(4) \u4e0e\u4f7f\u7528\u539f\u59cb Mamba \u6a21\u578b\u7684\u591a\u6a21\u6001\u6a21\u578b\u76f8\u6bd4\uff0c\u57fa\u4e8e Mamba-2 \u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u63a8\u7406\u6027\u80fd\u548c\u6709\u6548\u6027\u3002", "author": "Wenjun Huang et.al.", "authors": "Wenjun Huang, Jianguo Hu", "id": "2407.19832v1", "paper_url": "http://arxiv.org/abs/2407.19832v1", "repo": "null"}}