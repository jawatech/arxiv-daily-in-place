{"2412.05167": {"publish_time": "2024-12-06", "title": "Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models", "paper_summary": "Large Audio-Language Models (LALMs) have unclocked audio dialogue\ncapabilities, where audio dialogues are a direct exchange of spoken language\nbetween LALMs and humans. Recent advances, such as GPT-4o, have enabled LALMs\nin back-and-forth audio dialogues with humans. This progression not only\nunderscores the potential of LALMs but also broadens their applicability across\na wide range of practical scenarios supported by audio dialogues. However,\ngiven these advancements, a comprehensive benchmark to evaluate the performance\nof LALMs in the open-ended audio dialogue understanding remains absent\ncurrently. To address this gap, we propose an Audio Dialogue Understanding\nBenchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the\nopen-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills,\n9 multilingual languages, and 4 categories of ambiguity handling. Notably, we\nfirstly propose the evaluation of ambiguity handling in audio dialogues that\nexpresses different intentions beyond the same literal meaning of sentences,\ne.g., \"Really!?\" with different intonations. In summary, ADU-Bench includes\nover 20,000 open-ended audio dialogues for the assessment of LALMs. Through\nextensive experiments conducted on 13 LALMs, our analysis reveals that there is\nstill considerable room for improvement in the audio dialogue understanding\nabilities of existing LALMs. In particular, they struggle with mathematical\nsymbols and formulas, understanding human behavior such as roleplay,\ncomprehending multiple languages, and handling audio dialogue ambiguities from\ndifferent phonetic elements, such as intonations, pause positions, and\nhomophones.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u97f3\u8a0a\u6a21\u578b (LALM) \u5df2\u89e3\u9396\u97f3\u8a0a\u5c0d\u8a71\u80fd\u529b\uff0c\u5176\u4e2d\u97f3\u8a0a\u5c0d\u8a71\u662f LALM \u8207\u4eba\u985e\u4e4b\u9593\u53e3\u8a9e\u7684\u76f4\u63a5\u4ea4\u6d41\u3002\u6700\u8fd1\u7684\u9032\u5c55\uff0c\u4f8b\u5982 GPT-4o\uff0c\u5df2\u8b93 LALM \u80fd\u8207\u4eba\u985e\u9032\u884c\u4f86\u56de\u97f3\u8a0a\u5c0d\u8a71\u3002\u6b64\u9032\u5c55\u4e0d\u50c5\u5f37\u8abf LALM \u7684\u6f5b\u529b\uff0c\u4e5f\u64f4\u5c55\u5176\u5728\u97f3\u8a0a\u5c0d\u8a71\u652f\u63f4\u7684\u5404\u7a2e\u5be6\u969b\u5834\u666f\u4e2d\u7684\u9069\u7528\u6027\u3002\u7136\u800c\uff0c\u8003\u91cf\u9019\u4e9b\u9032\u5c55\uff0c\u76ee\u524d\u4ecd\u7f3a\u4e4f\u4e00\u500b\u5168\u9762\u7684\u57fa\u6e96\u4f86\u8a55\u4f30 LALM \u5728\u958b\u653e\u5f0f\u97f3\u8a0a\u5c0d\u8a71\u7406\u89e3\u4e2d\u7684\u8868\u73fe\u3002\u70ba\u4e86\u89e3\u6c7a\u6b64\u5dee\u8ddd\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u97f3\u8a0a\u5c0d\u8a71\u7406\u89e3\u57fa\u6e96 (ADU-Bench)\uff0c\u5176\u4e2d\u5305\u542b 4 \u500b\u57fa\u6e96\u8cc7\u6599\u96c6\u3002\u5b83\u5011\u8a55\u4f30 LALM \u5728 3 \u500b\u4e00\u822c\u5834\u666f\u300112 \u9805\u6280\u80fd\u30019 \u7a2e\u591a\u8a9e\u8a00\u548c 4 \u985e\u542b\u7cca\u6027\u8655\u7406\u4e2d\u7684\u958b\u653e\u5f0f\u97f3\u8a0a\u5c0d\u8a71\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u5011\u9996\u5148\u63d0\u51fa\u8a55\u4f30\u97f3\u8a0a\u5c0d\u8a71\u4e2d\u7684\u542b\u7cca\u6027\u8655\u7406\uff0c\u5176\u8868\u9054\u4e86\u8d85\u51fa\u53e5\u5b50\u76f8\u540c\u5b57\u9762\u610f\u7fa9\u7684\u4e0d\u540c\u610f\u5716\uff0c\u4f8b\u5982\u8a9e\u8abf\u4e0d\u540c\u7684\u300c\u771f\u7684\u55ce\uff1f\u300d\u3002\u7e3d\u4e4b\uff0cADU-Bench \u5305\u542b\u8d85\u904e 20,000 \u500b\u958b\u653e\u5f0f\u97f3\u8a0a\u5c0d\u8a71\uff0c\u7528\u65bc\u8a55\u4f30 LALM\u3002\u900f\u904e\u5c0d 13 \u500b LALM \u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u73fe\u6709 LALM \u7684\u97f3\u8a0a\u5c0d\u8a71\u7406\u89e3\u80fd\u529b\u4ecd\u6709\u5f88\u5927\u7684\u6539\u9032\u7a7a\u9593\u3002\u7279\u5225\u662f\uff0c\u4ed6\u5011\u5728\u6578\u5b78\u7b26\u865f\u548c\u516c\u5f0f\u3001\u7406\u89e3\u89d2\u8272\u626e\u6f14\u7b49\u4eba\u985e\u884c\u70ba\u3001\u7406\u89e3\u591a\u7a2e\u8a9e\u8a00\u4ee5\u53ca\u8655\u7406\u4f86\u81ea\u4e0d\u540c\u8a9e\u97f3\u5143\u7d20\uff08\u4f8b\u5982\u8a9e\u8abf\u3001\u505c\u9813\u4f4d\u7f6e\u548c\u540c\u97f3\u7570\u7fa9\u8a5e\uff09\u7684\u97f3\u8a0a\u5c0d\u8a71\u542b\u7cca\u6027\u65b9\u9762\u9047\u5230\u4e86\u56f0\u96e3\u3002", "author": "Kuofeng Gao et.al.", "authors": "Kuofeng Gao, Shu-Tao Xia, Ke Xu, Philip Torr, Jindong Gu", "id": "2412.05167v1", "paper_url": "http://arxiv.org/abs/2412.05167v1", "repo": "null"}}