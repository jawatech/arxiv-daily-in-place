{"2408.03092": {"publish_time": "2024-08-06", "title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement", "paper_summary": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous\nLLMs into one with all the capabilities. Ideally, any LLMs sharing the same\nbackbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)\nwith minor parameter changes or Pre-Trained (PT) with substantial parameter\nshifts. However, existing methods often manually assign the model importance,\nrendering them feasible only for LLMs with similar parameter alterations, such\nas multiple FT LLMs. The diverse parameter changed ranges between FT and PT\nLLMs pose challenges for current solutions in empirically determining the\noptimal combination. In this paper, we make a pioneering effort to broaden the\napplicability of merging techniques from FT to PT LLMs. We initially examine\nthe efficacy of current methods in merging FT and PT LLMs, discovering that\nthey struggle to deal with PT LLMs. Subsequently, we introduce an approach\nbased on WeIght DisENtanglement (WIDEN) to effectively extend the merging\nscope, which first disentangles model weights into magnitude and direction\ncomponents, and then performs adaptive fusion by considering their respective\ncontributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with\ninstruction-following skills) with Sailor (a PT LLM with multilingual\nabilities) across 7B and 14B model scales. Results reveal that: (1) existing\nsolutions usually fail when merging Sailor, either losing both abilities or\nonly retaining instruction-following skills; (2) WIDEN successfully injects the\nmultilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in\nSoutheast Asian languages, achieving enhancements in the fundamental\ncapabilities. In light of previous research, we also merge multiple 13B FT LLMs\nand observe that WIDEN achieves a balanced amalgamation of instruction\nfollowing, mathematical reasoning, and code generation skills.", "paper_summary_zh": "\u5408\u4f75\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u65e8\u5728\u5c07\u591a\u500b\u540c\u6e90 LLM \u6574\u5408\u70ba\u4e00\u500b\u5177\u5099\u6240\u6709\u529f\u80fd\u7684 LLM\u3002\u7406\u60f3\u60c5\u6cc1\u4e0b\uff0c\u4efb\u4f55\u5171\u4eab\u76f8\u540c\u4e3b\u5e79\u7684 LLM \u90fd\u61c9\u8a72\u662f\u53ef\u5408\u4f75\u7684\uff0c\u7121\u8ad6\u5b83\u5011\u662f\u5426\u7d93\u904e\u5fae\u8abf (FT) \u4ee5\u9032\u884c\u8f03\u5c0f\u7684\u53c3\u6578\u8b8a\u66f4\uff0c\u6216\u7d93\u904e\u9810\u8a13\u7df4 (PT) \u4ee5\u9032\u884c\u5927\u5e45\u5ea6\u7684\u53c3\u6578\u8f49\u79fb\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u6703\u624b\u52d5\u6307\u5b9a\u6a21\u578b\u91cd\u8981\u6027\uff0c\u4f7f\u5176\u50c5\u9069\u7528\u65bc\u53c3\u6578\u8b8a\u66f4\u76f8\u4f3c\u7684 LLM\uff0c\u4f8b\u5982\u591a\u500b FT LLM\u3002FT \u548c PT LLM \u4e4b\u9593\u4e0d\u540c\u7684\u53c3\u6578\u8b8a\u66f4\u7bc4\u570d\u5c0d\u7576\u524d\u89e3\u6c7a\u65b9\u6848\u5728\u7d93\u9a57\u4e0a\u78ba\u5b9a\u6700\u4f73\u7d44\u5408\u69cb\u6210\u6311\u6230\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u505a\u51fa\u4e86\u958b\u5275\u6027\u7684\u52aa\u529b\uff0c\u5c07\u5408\u4f75\u6280\u8853\u7684\u9069\u7528\u6027\u5f9e FT \u64f4\u5c55\u5230 PT LLM\u3002\u6211\u5011\u6700\u521d\u6aa2\u67e5\u4e86\u7576\u524d\u65b9\u6cd5\u5728\u5408\u4f75 FT \u548c PT LLM \u4e2d\u7684\u529f\u6548\uff0c\u767c\u73fe\u5b83\u5011\u96e3\u4ee5\u8655\u7406 PT LLM\u3002\u96a8\u5f8c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u57fa\u65bc\u6b0a\u91cd\u89e3\u958b (WIDEN) \u7684\u65b9\u6cd5\u4f86\u6709\u6548\u64f4\u5c55\u5408\u4f75\u7bc4\u570d\uff0c\u8a72\u65b9\u6cd5\u9996\u5148\u5c07\u6a21\u578b\u6b0a\u91cd\u89e3\u958b\u70ba\u5927\u5c0f\u548c\u65b9\u5411\u5206\u91cf\uff0c\u7136\u5f8c\u901a\u904e\u8003\u616e\u5b83\u5011\u5404\u81ea\u7684\u8ca2\u737b\u4f86\u57f7\u884c\u81ea\u9069\u61c9\u878d\u5408\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5c07 Qwen1.5-Chat\uff08\u5177\u6709\u6307\u4ee4\u9075\u5faa\u6280\u80fd\u7684 FT LLM\uff09\u8207 Sailor\uff08\u5177\u6709\u591a\u8a9e\u8a00\u80fd\u529b\u7684 PT LLM\uff09\u5408\u4f75\u5728 7B \u548c 14B \u6a21\u578b\u898f\u6a21\u4e2d\u3002\u7d50\u679c\u8868\u660e\uff1a(1) \u73fe\u6709\u7684\u89e3\u6c7a\u65b9\u6848\u5728\u5408\u4f75 Sailor \u6642\u901a\u5e38\u6703\u5931\u6557\uff0c\u8981\u9ebc\u5931\u53bb\u5169\u7a2e\u80fd\u529b\uff0c\u8981\u9ebc\u50c5\u4fdd\u7559\u6307\u4ee4\u9075\u5faa\u6280\u80fd\uff1b(2) WIDEN \u6210\u529f\u5730\u5c07 Sailor \u7684\u591a\u8a9e\u8a00\u80fd\u529b\u6ce8\u5165 Qwen1.5-Chat\uff0c\u4f7f\u5176\u7cbe\u901a\u6771\u5357\u4e9e\u8a9e\u8a00\uff0c\u4e26\u589e\u5f37\u4e86\u57fa\u672c\u80fd\u529b\u3002\u6839\u64da\u5148\u524d\u7684\u7814\u7a76\uff0c\u6211\u5011\u9084\u5408\u4f75\u4e86\u591a\u500b 13B FT LLM\uff0c\u4e26\u89c0\u5bdf\u5230 WIDEN \u9054\u5230\u4e86\u6307\u4ee4\u9075\u5faa\u3001\u6578\u5b78\u63a8\u7406\u548c\u7a0b\u5f0f\u78bc\u751f\u6210\u6280\u80fd\u7684\u5e73\u8861\u878d\u5408\u3002", "author": "Le Yu et.al.", "authors": "Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li", "id": "2408.03092v1", "paper_url": "http://arxiv.org/abs/2408.03092v1", "repo": "null"}}