{"2411.09273": {"publish_time": "2024-11-14", "title": "Cross-Modal Consistency in Multimodal Large Language Models", "paper_summary": "Recent developments in multimodal methodologies have marked the beginning of\nan exciting era for models adept at processing diverse data types, encompassing\ntext, audio, and visual content. Models like GPT-4V, which merge computer\nvision with advanced language processing, exhibit extraordinary proficiency in\nhandling intricate tasks that require a simultaneous understanding of both\ntextual and visual information. Prior research efforts have meticulously\nevaluated the efficacy of these Vision Large Language Models (VLLMs) in various\ndomains, including object detection, image captioning, and other related\nfields. However, existing analyses have often suffered from limitations,\nprimarily centering on the isolated evaluation of each modality's performance\nwhile neglecting to explore their intricate cross-modal interactions.\nSpecifically, the question of whether these models achieve the same level of\naccuracy when confronted with identical task instances across different\nmodalities remains unanswered. In this study, we take the initiative to delve\ninto the interaction and comparison among these modalities of interest by\nintroducing a novel concept termed cross-modal consistency. Furthermore, we\npropose a quantitative evaluation framework founded on this concept. Our\nexperimental findings, drawn from a curated collection of parallel\nvision-language datasets developed by us, unveil a pronounced inconsistency\nbetween the vision and language modalities within GPT-4V, despite its portrayal\nas a unified multimodal model. Our research yields insights into the\nappropriate utilization of such models and hints at potential avenues for\nenhancing their design.", "paper_summary_zh": "<paragraph>\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u6700\u65b0\u53d1\u5c55\u6807\u5fd7\u7740\u4e00\u4e2a\u4ee4\u4eba\u5174\u594b\u7684\u65b0\u65f6\u4ee3\uff0c\u5728\u8fd9\u4e2a\u65f6\u4ee3\uff0c\u6a21\u578b\u64c5\u957f\u5904\u7406\u5404\u79cd\u6570\u636e\u7c7b\u578b\uff0c\u5305\u62ec\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u89c9\u5185\u5bb9\u3002\u50cf GPT-4V \u8fd9\u6837\u7684\u6a21\u578b\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u9ad8\u7ea7\u8bed\u8a00\u5904\u7406\u76f8\u7ed3\u5408\uff0c\u5728\u5904\u7406\u9700\u8981\u540c\u65f6\u7406\u89e3\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u975e\u51e1\u7684\u719f\u7ec3\u5ea6\u3002\u5148\u524d\u7684\u7814\u7a76\u5de5\u4f5c\u5df2\u4ed4\u7ec6\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b (VLLM) \u5728\u5404\u4e2a\u9886\u57df\u7684\u529f\u6548\uff0c\u5305\u62ec\u5bf9\u8c61\u68c0\u6d4b\u3001\u56fe\u50cf\u5b57\u5e55\u548c\u5176\u4ed6\u76f8\u5173\u9886\u57df\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5206\u6790\u901a\u5e38\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u5b64\u7acb\u8bc4\u4f30\u6bcf\u4e2a\u6a21\u6001\u7684\u6027\u80fd\uff0c\u800c\u5ffd\u7565\u4e86\u63a2\u7d22\u5b83\u4eec\u590d\u6742\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u9762\u5bf9\u4e0d\u540c\u6a21\u6001\u7684\u76f8\u540c\u4efb\u52a1\u5b9e\u4f8b\u65f6\u662f\u5426\u8fbe\u5230\u76f8\u540c\u7ea7\u522b\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u4e2a\u95ee\u9898\u4ecd\u7136\u6ca1\u6709\u7b54\u6848\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4e3b\u52a8\u6df1\u5165\u7814\u7a76\u8fd9\u4e9b\u611f\u5174\u8da3\u7684\u6a21\u6001\u4e4b\u95f4\u7684\u4ea4\u4e92\u548c\u6bd4\u8f83\uff0c\u5f15\u5165\u4e86\u79f0\u4e3a\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u7684\u65b0\u6982\u5ff5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fd9\u4e00\u6982\u5ff5\u7684\u5b9a\u91cf\u8bc4\u4f30\u6846\u67b6\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u6765\u81ea\u6211\u4eec\u5f00\u53d1\u7684\u7ecf\u8fc7\u6574\u7406\u7684\u5e76\u884c\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86 GPT-4V \u4e2d\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u4e4b\u95f4\u660e\u663e\u7684\u5dee\u5f02\uff0c\u5c3d\u7ba1\u5b83\u88ab\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002\u6211\u4eec\u7684\u7814\u7a76\u5f97\u51fa\u4e86\u5bf9\u8fd9\u4e9b\u6a21\u578b\u7684\u9002\u5f53\u5229\u7528\u7684\u89c1\u89e3\uff0c\u5e76\u6697\u793a\u4e86\u589e\u5f3a\u5176\u8bbe\u8ba1\u7684\u6f5c\u5728\u9014\u5f84\u3002</paragraph>", "author": "Xiang Zhang et.al.", "authors": "Xiang Zhang, Senyu Li, Ning Shi, Bradley Hauer, Zijun Wu, Grzegorz Kondrak, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan", "id": "2411.09273v1", "paper_url": "http://arxiv.org/abs/2411.09273v1", "repo": "null"}}