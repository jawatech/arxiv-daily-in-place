{"2405.03952": {"publish_time": "2024-05-07", "title": "HAFFormer: A Hierarchical Attention-Free Framework for Alzheimer's Disease Detection From Spontaneous Speech", "paper_summary": "Automatically detecting Alzheimer's Disease (AD) from spontaneous speech\nplays an important role in its early diagnosis. Recent approaches highly rely\non the Transformer architectures due to its efficiency in modelling long-range\ncontext dependencies. However, the quadratic increase in computational\ncomplexity associated with self-attention and the length of audio poses a\nchallenge when deploying such models on edge devices. In this context, we\nconstruct a novel framework, namely Hierarchical Attention-Free Transformer\n(HAFFormer), to better deal with long speech for AD detection. Specifically, we\nemploy an attention-free module of Multi-Scale Depthwise Convolution to replace\nthe self-attention and thus avoid the expensive computation, and a GELU-based\nGated Linear Unit to replace the feedforward layer, aiming to automatically\nfilter out the redundant information. Moreover, we design a hierarchical\nstructure to force it to learn a variety of information grains, from the frame\nlevel to the dialogue level. By conducting extensive experiments on the\nADReSS-M dataset, the introduced HAFFormer can achieve competitive results\n(82.6% accuracy) with other recent work, but with significant computational\ncomplexity and model size reduction compared to the standard Transformer. This\nshows the efficiency of HAFFormer in dealing with long audio for AD detection.", "paper_summary_zh": "\u81ea\u52d5\u5f9e\u81ea\u767c\u6027\u8a9e\u8a00\u4e2d\u5075\u6e2c\u963f\u8332\u6d77\u9ed8\u75c7 (AD) \u5728\u5176\u65e9\u671f\u8a3a\u65b7\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\u3002\u7531\u65bc Transformer \u67b6\u69cb\u5728\u5efa\u6a21\u9577\u7a0b\u80cc\u666f\u4f9d\u8cf4\u95dc\u4fc2\u65b9\u9762\u7684\u6548\u7387\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u9ad8\u5ea6\u4f9d\u8cf4\u65bc Transformer \u67b6\u69cb\u3002\u7136\u800c\uff0c\u8207\u81ea\u6211\u6ce8\u610f\u548c\u97f3\u8a0a\u9577\u5ea6\u76f8\u95dc\u7684\u8a08\u7b97\u8907\u96dc\u5ea6\u4e8c\u6b21\u589e\u52a0\uff0c\u5728\u908a\u7de3\u88dd\u7f6e\u4e0a\u90e8\u7f72\u6b64\u985e\u6a21\u578b\u6642\u69cb\u6210\u4e86\u4e00\u9805\u6311\u6230\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u6211\u5011\u5efa\u69cb\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u67b6\u69cb\uff0c\u5373\u5206\u5c64\u7121\u6ce8\u610f\u529b Transformer (HAFFormer)\uff0c\u4ee5\u66f4\u597d\u5730\u8655\u7406 AD \u5075\u6e2c\u7684\u9577\u8a9e\u97f3\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63a1\u7528\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5377\u7a4d\u7684\u7121\u6ce8\u610f\u529b\u6a21\u7d44\u4f86\u53d6\u4ee3\u81ea\u6211\u6ce8\u610f\uff0c\u5f9e\u800c\u907f\u514d\u6602\u8cb4\u7684\u8a08\u7b97\uff0c\u4ee5\u53ca\u57fa\u65bc GELU \u7684\u9598\u63a7\u7dda\u6027\u55ae\u5143\u4f86\u53d6\u4ee3\u524d\u994b\u5c64\uff0c\u65e8\u5728\u81ea\u52d5\u904e\u6ffe\u6389\u5197\u9918\u8cc7\u8a0a\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a2d\u8a08\u4e86\u4e00\u500b\u5206\u5c64\u7d50\u69cb\u4f86\u5f37\u5236\u5b83\u5b78\u7fd2\u5404\u7a2e\u8cc7\u8a0a\u9846\u7c92\uff0c\u5f9e\u5e40\u7d1a\u5230\u5c0d\u8a71\u7d1a\u3002\u901a\u904e\u5728 ADReSS-M \u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u5f15\u5165\u7684 HAFFormer \u53ef\u4ee5\u5be6\u73fe\u8207\u5176\u4ed6\u8fd1\u671f\u5de5\u4f5c\u5177\u6709\u7af6\u722d\u529b\u7684\u7d50\u679c\uff0882.6%  accuracy\uff09\uff0c\u4f46\u8207\u6a19\u6e96 Transformer \u76f8\u6bd4\uff0c\u8a08\u7b97\u8907\u96dc\u5ea6\u548c\u6a21\u578b\u5927\u5c0f\u986f\u8457\u964d\u4f4e\u3002\u9019\u986f\u793a\u4e86 HAFFormer \u5728\u8655\u7406 AD \u5075\u6e2c\u7684\u9577\u97f3\u8a0a\u65b9\u9762\u7684\u6548\u7387\u3002", "author": "Zhongren Dong et.al.", "authors": "Zhongren Dong, Zixing Zhang, Weixiang Xu, Jing Han, Jianjun Ou, Bj\u00f6rn W. Schuller", "id": "2405.03952v1", "paper_url": "http://arxiv.org/abs/2405.03952v1", "repo": "null"}}