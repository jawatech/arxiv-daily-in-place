{"2410.18955": {"publish_time": "2024-10-24", "title": "BioMistral-NLU: Towards More Generalizable Medical Language Understanding through Instruction Tuning", "paper_summary": "Large language models (LLMs) such as ChatGPT are fine-tuned on large and\ndiverse instruction-following corpora, and can generalize to new tasks.\nHowever, those instruction-tuned LLMs often perform poorly in specialized\nmedical natural language understanding (NLU) tasks that require domain\nknowledge, granular text comprehension, and structured data extraction. To\nbridge the gap, we: (1) propose a unified prompting format for 7 important NLU\ntasks, % through span extraction and multi-choice question-answering (QA), (2)\ncurate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing\nopen-source medical NLU corpora, and (3) develop BioMistral-NLU, a\ngeneralizable medical NLU model, through fine-tuning BioMistral on\nMNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6\nimportant NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical\nLanguage Understanding Evaluation (BLUE) and Biomedical Language Understanding\nand Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLU\noutperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT\nand GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step\nover diverse NLU tasks enhance LLMs' generalizability across diverse medical\nNLU tasks. Our ablation experiments show that instruction-tuning on a wider\nvariety of tasks, even when the total number of training instances remains\nconstant, enhances downstream zero-shot generalization.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u4f8b\u5982 ChatGPT \u6703\u91dd\u5c0d\u5927\u91cf\u4e14\u591a\u6a23\u5316\u7684\u6307\u4ee4\u9075\u5faa\u8a9e\u6599\u5eab\u9032\u884c\u5fae\u8abf\uff0c\u4e26\u53ef\u6982\u62ec\u5230\u65b0\u7684\u4efb\u52d9\u3002\n\u7136\u800c\uff0c\u7d93\u904e\u6307\u4ee4\u5fae\u8abf\u7684 LLM \u5728\u9700\u8981\u9818\u57df\u77e5\u8b58\u3001\u7d30\u7dfb\u6587\u5b57\u7406\u89e3\u548c\u7d50\u69cb\u5316\u6578\u64da\u8403\u53d6\u7684\u5c08\u696d\u91ab\u7642\u81ea\u7136\u8a9e\u8a00\u7406\u89e3 (NLU) \u4efb\u52d9\u4e2d\u901a\u5e38\u8868\u73fe\u4e0d\u4f73\u3002\u70ba\u7e2e\u5c0f\u5dee\u8ddd\uff0c\u6211\u5011\uff1a(1) \u70ba 7 \u9805\u91cd\u8981\u7684 NLU \u4efb\u52d9\u63d0\u51fa\u7d71\u4e00\u7684\u63d0\u793a\u683c\u5f0f\uff0c\u900f\u904e\u5340\u9593\u8403\u53d6\u548c\u591a\u9078\u984c\u554f\u7b54 (QA)\uff0c(2) \u7b56\u5283\u6307\u4ee4\u5fae\u8abf\u8cc7\u6599\u96c6 MNLU-Instruct\uff0c\u5229\u7528\u73fe\u6709\u7684\u5404\u7a2e\u958b\u6e90\u91ab\u7642 NLU \u8a9e\u6599\u5eab\uff0c\u4ee5\u53ca (3) \u958b\u767c BioMistral-NLU\uff0c\u4e00\u7a2e\u53ef\u6982\u62ec\u7684\u91ab\u7642 NLU \u6a21\u578b\uff0c\u900f\u904e\u5728 MNLU-Instruct \u4e0a\u5c0d BioMistral \u9032\u884c\u5fae\u8abf\u3002\u6211\u5011\u5728\u96f6\u6b21\u5b78\u7fd2\u8a2d\u5b9a\u4e2d\u8a55\u4f30 BioMistral-NLU\uff0c\u6db5\u84cb 6 \u9805\u91cd\u8981\u7684 NLU \u4efb\u52d9\uff0c\u4f86\u81ea\u5169\u500b\u5ee3\u6cdb\u63a1\u7528\u7684\u91ab\u7642 NLU \u57fa\u6e96\uff1a\u751f\u7269\u91ab\u5b78\u8a9e\u8a00\u7406\u89e3\u8a55\u4f30 (BLUE) \u548c\u751f\u7269\u91ab\u5b78\u8a9e\u8a00\u7406\u89e3\u8207\u63a8\u7406\u57fa\u6e96 (BLURB)\u3002\u6211\u5011\u7684\u5be6\u9a57\u986f\u793a\uff0c\u6211\u5011\u7684 BioMistral-NLU \u512a\u65bc\u539f\u59cb\u7684 BioMistral\uff0c\u4ee5\u53ca\u5c08\u6709\u7684 LLM - ChatGPT \u548c GPT-4\u3002\u6211\u5011\u8207\u8cc7\u6599\u96c6\u7121\u95dc\u7684\u63d0\u793a\u7b56\u7565\u548c\u6307\u4ee4\u5fae\u8abf\u6b65\u9a5f\uff0c\u53ef\u589e\u5f37 LLM \u5728\u5404\u7a2e\u91ab\u7642 NLU \u4efb\u52d9\u4e2d\u7684\u6982\u62ec\u6027\u3002\u6211\u5011\u7684\u6d88\u878d\u5be6\u9a57\u986f\u793a\uff0c\u5373\u4f7f\u8a13\u7df4\u5be6\u4f8b\u7684\u7e3d\u6578\u4fdd\u6301\u4e0d\u8b8a\uff0c\u5728\u66f4\u591a\u6a23\u5316\u7684\u4efb\u52d9\u4e0a\u9032\u884c\u6307\u4ee4\u5fae\u8abf\uff0c\u4e5f\u80fd\u589e\u5f37\u4e0b\u6e38\u7684\u96f6\u6b21\u5b78\u7fd2\u6982\u62ec\u6027\u3002", "author": "Yujuan Velvin Fu et.al.", "authors": "Yujuan Velvin Fu, Giridhar Kaushik Ramachandran, Namu Park, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen", "id": "2410.18955v1", "paper_url": "http://arxiv.org/abs/2410.18955v1", "repo": "null"}}