{"2409.05395": {"publish_time": "2024-09-09", "title": "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "paper_summary": "This study explores replacing Transformers in Visual Language Models (VLMs)\nwith Mamba, a recent structured state space model (SSM) that demonstrates\npromising performance in sequence modeling. We test models up to 3B parameters\nunder controlled conditions, showing that Mamba-based VLMs outperforms\nTransformers-based VLMs in captioning, question answering, and reading\ncomprehension. However, we find that Transformers achieve greater performance\nin visual grounding and the performance gap widens with scale. We explore two\nhypotheses to explain this phenomenon: 1) the effect of task-agnostic visual\nencoding on the updates of the hidden states, and 2) the difficulty in\nperforming visual grounding from the perspective of in-context multimodal\nretrieval. Our results indicate that a task-aware encoding yields minimal\nperformance gains on grounding, however, Transformers significantly outperform\nMamba at in-context multimodal retrieval. Overall, Mamba shows promising\nperformance on tasks where the correct output relies on a summary of the image\nbut struggles when retrieval of explicit information from the context is\nrequired.", "paper_summary_zh": "\u9019\u9805\u7814\u7a76\u63a2\u8a0e\u7528 Mamba \u53d6\u4ee3\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u4e2d\u7684 Transformer\uff0cMamba \u662f\u4e00\u7a2e\u6700\u8fd1\u7684\u7d50\u69cb\u5316\u72c0\u614b\u7a7a\u9593\u6a21\u578b (SSM)\uff0c\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u5c55\u73fe\u51fa\u6975\u4f73\u7684\u6548\u80fd\u3002\u6211\u5011\u5728\u53d7\u63a7\u689d\u4ef6\u4e0b\u6e2c\u8a66\u9ad8\u9054 3B \u53c3\u6578\u7684\u6a21\u578b\uff0c\u986f\u793a\u57fa\u65bc Mamba \u7684 VLM \u5728\u6a19\u984c\u3001\u554f\u7b54\u548c\u95b1\u8b80\u7406\u89e3\u65b9\u9762\u90fd\u512a\u65bc\u57fa\u65bc Transformer \u7684 VLM\u3002\u7136\u800c\uff0c\u6211\u5011\u767c\u73fe Transformer \u5728\u8996\u89ba\u57fa\u790e\u4e0a\u7372\u5f97\u66f4\u597d\u7684\u6548\u80fd\uff0c\u800c\u4e14\u6548\u80fd\u5dee\u8ddd\u6703\u96a8\u8457\u898f\u6a21\u64f4\u5927\u800c\u64f4\u5927\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u5169\u500b\u5047\u8a2d\u4f86\u89e3\u91cb\u9019\u7a2e\u73fe\u8c61\uff1a1) \u8207\u4efb\u52d9\u7121\u95dc\u7684\u8996\u89ba\u7de8\u78bc\u5c0d\u96b1\u85cf\u72c0\u614b\u66f4\u65b0\u7684\u5f71\u97ff\uff0c\u4ee5\u53ca 2) \u5f9e\u8a9e\u5883\u591a\u6a21\u614b\u6aa2\u7d22\u7684\u89d2\u5ea6\u57f7\u884c\u8996\u89ba\u57fa\u790e\u7684\u56f0\u96e3\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0c\u4efb\u52d9\u611f\u77e5\u7de8\u78bc\u5728\u57fa\u790e\u4e0a\u7522\u751f\u6700\u5c0f\u7684\u6548\u80fd\u63d0\u5347\uff0c\u7136\u800c\uff0cTransformer \u5728\u8a9e\u5883\u591a\u6a21\u614b\u6aa2\u7d22\u65b9\u9762\u660e\u986f\u512a\u65bc Mamba\u3002\u7e3d\u9ad4\u800c\u8a00\uff0cMamba \u5728\u6b63\u78ba\u8f38\u51fa\u4f9d\u8cf4\u65bc\u5f71\u50cf\u6458\u8981\u7684\u4efb\u52d9\u4e0a\u5c55\u73fe\u51fa\u6975\u4f73\u7684\u6548\u80fd\uff0c\u4f46\u7576\u9700\u8981\u5f9e\u8a9e\u5883\u4e2d\u6aa2\u7d22\u660e\u78ba\u8cc7\u8a0a\u6642\uff0c\u5c31\u6703\u9047\u5230\u56f0\u96e3\u3002", "author": "Georgios Pantazopoulos et.al.", "authors": "Georgios Pantazopoulos, Malvina Nikandrou, Alessandro Suglia, Oliver Lemon, Arash Eshghi", "id": "2409.05395v1", "paper_url": "http://arxiv.org/abs/2409.05395v1", "repo": "null"}}