{"2410.12694": {"publish_time": "2024-10-16", "title": "VividMed: Vision Language Model with Versatile Visual Grounding for Medicine", "paper_summary": "Recent advancements in Vision Language Models (VLMs) have demonstrated\nremarkable promise in generating visually grounded responses. However, their\napplication in the medical domain is hindered by unique challenges. For\ninstance, most VLMs rely on a single method of visual grounding, whereas\ncomplex medical tasks demand more versatile approaches. Additionally, while\nmost VLMs process only 2D images, a large portion of medical images are 3D. The\nlack of medical data further compounds these obstacles. To address these\nchallenges, we present VividMed, a vision language model with versatile visual\ngrounding for medicine. Our model supports generating both semantic\nsegmentation masks and instance-level bounding boxes, and accommodates various\nimaging modalities, including both 2D and 3D data. We design a three-stage\ntraining procedure and an automatic data synthesis pipeline based on open\ndatasets and models. Besides visual grounding tasks, VividMed also excels in\nother common downstream tasks, including Visual Question Answering (VQA) and\nreport generation. Ablation studies empirically show that the integration of\nvisual grounding ability leads to improved performance on these tasks. Our code\nis publicly available at https://github.com/function2-llx/MMMM.", "paper_summary_zh": "\u6700\u8fd1\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u65b9\u9762\u7684\u8fdb\u6b65\u5df2\u5c55\u793a\u51fa\u5728\u751f\u6210\u89c6\u89c9\u57fa\u7840\u54cd\u5e94\u65b9\u9762\u7684\u975e\u51e1\u524d\u666f\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u53d7\u5230\u72ec\u7279\u7684\u6311\u6218\u7684\u963b\u788d\u3002\u4f8b\u5982\uff0c\u5927\u591a\u6570 VLM \u4f9d\u8d56\u4e8e\u4e00\u79cd\u89c6\u89c9\u57fa\u7840\u65b9\u6cd5\uff0c\u800c\u590d\u6742\u7684\u533b\u5b66\u4efb\u52a1\u9700\u8981\u66f4\u591a\u6837\u5316\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u867d\u7136\u5927\u591a\u6570 VLM \u53ea\u5904\u7406\u4e8c\u7ef4\u56fe\u50cf\uff0c\u4f46\u5927\u90e8\u5206\u533b\u5b66\u56fe\u50cf\u90fd\u662f\u4e09\u7ef4\u7684\u3002\u7f3a\u4e4f\u533b\u5b66\u6570\u636e\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e9b\u969c\u788d\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 VividMed\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u591a\u529f\u80fd\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u6211\u4eec\u7684\u6a21\u578b\u652f\u6301\u751f\u6210\u8bed\u4e49\u5206\u5272\u63a9\u6a21\u548c\u5b9e\u4f8b\u7ea7\u8fb9\u754c\u6846\uff0c\u5e76\u9002\u7528\u4e8e\u5404\u79cd\u6210\u50cf\u65b9\u5f0f\uff0c\u5305\u62ec\u4e8c\u7ef4\u548c\u4e09\u7ef4\u6570\u636e\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u8bad\u7ec3\u7a0b\u5e8f\u548c\u4e00\u4e2a\u57fa\u4e8e\u5f00\u653e\u6570\u636e\u96c6\u548c\u6a21\u578b\u7684\u81ea\u52a8\u6570\u636e\u5408\u6210\u7ba1\u9053\u3002\u9664\u4e86\u89c6\u89c9\u57fa\u7840\u4efb\u52a1\u4e4b\u5916\uff0cVividMed \u5728\u5176\u4ed6\u5e38\u89c1\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u89c6\u89c9\u95ee\u7b54 (VQA) \u548c\u62a5\u544a\u751f\u6210\u3002\u6d88\u878d\u7814\u7a76\u51ed\u7ecf\u9a8c\u8bc1\u660e\uff0c\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u7684\u6574\u5408\u5bfc\u81f4\u4e86\u8fd9\u4e9b\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5728 https://github.com/function2-llx/MMMM \u4e0a\u516c\u5f00\u63d0\u4f9b\u3002", "author": "Lingxiao Luo et.al.", "authors": "Lingxiao Luo, Bingda Tang, Xuanzhong Chen, Rong Han, Ting Chen", "id": "2410.12694v1", "paper_url": "http://arxiv.org/abs/2410.12694v1", "repo": "null"}}