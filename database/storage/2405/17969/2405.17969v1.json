{"2405.17969": {"publish_time": "2024-05-28", "title": "Knowledge Circuits in Pretrained Transformers", "paper_summary": "The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, has allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuit holds\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.", "paper_summary_zh": "\u73fe\u4ee3\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u7684\u5353\u8d8a\u80fd\u529b\u6e90\u65bc\u5176\u53c3\u6578\u4e2d\u7de8\u78bc\u7684\u9f90\u5927\u77e5\u8b58\u5eab\uff0c\u8b93\u5b83\u5011\u80fd\u5920\u611f\u77e5\u4e16\u754c\u4e26\u53c3\u8207\u63a8\u7406\u3002\u9019\u4e9b\u6a21\u578b\u5982\u4f55\u5132\u5b58\u77e5\u8b58\u7684\u5167\u90e8\u904b\u4f5c\u65b9\u5f0f\u4e00\u76f4\u662f\u7814\u7a76\u4eba\u54e1\u9ad8\u5ea6\u95dc\u6ce8\u548c\u7814\u7a76\u7684\u4e3b\u984c\u3002\u8fc4\u4eca\u70ba\u6b62\uff0c\u5927\u591a\u6578\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u9019\u4e9b\u6a21\u578b\u4e2d\u7684\u5b64\u7acb\u5143\u4ef6\uff0c\u4f8b\u5982\u591a\u5c64\u611f\u77e5\u5668\u548c\u6ce8\u610f\u529b\u982d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6df1\u5165\u63a2\u8a0e\u8a9e\u8a00\u6a21\u578b\u7684\u8a08\u7b97\u5716\uff0c\u4ee5\u63ed\u793a\u8868\u9054\u7279\u5b9a\u77e5\u8b58\u4e2d\u81f3\u95dc\u91cd\u8981\u7684\u77e5\u8b58\u96fb\u8def\u3002\u4f7f\u7528 GPT2 \u548c TinyLLAMA \u9032\u884c\u7684\u5be6\u9a57\u8b93\u6211\u5011\u80fd\u5920\u89c0\u5bdf\u67d0\u4e9b\u8cc7\u8a0a\u982d\u3001\u95dc\u4fc2\u982d\u548c\u591a\u5c64\u611f\u77e5\u5668\u5982\u4f55\u5728\u6a21\u578b\u4e2d\u5354\u540c\u7de8\u78bc\u77e5\u8b58\u3002\u6b64\u5916\uff0c\u6211\u5011\u8a55\u4f30\u4e86\u7576\u524d\u77e5\u8b58\u7de8\u8f2f\u6280\u8853\u5c0d\u9019\u4e9b\u77e5\u8b58\u96fb\u8def\u7684\u5f71\u97ff\uff0c\u5c0d\u9019\u4e9b\u7de8\u8f2f\u65b9\u6cd5\u7684\u529f\u80fd\u548c\u9650\u5236\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u898b\u89e3\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5229\u7528\u77e5\u8b58\u96fb\u8def\u5206\u6790\u548c\u8a6e\u91cb\u8a9e\u8a00\u6a21\u578b\u884c\u70ba\uff0c\u4f8b\u5982\u5e7b\u89ba\u548c\u60c5\u5883\u5b78\u7fd2\u3002\u6211\u5011\u76f8\u4fe1\u77e5\u8b58\u96fb\u8def\u6709\u6f5b\u529b\u4fc3\u9032\u6211\u5011\u5c0d Transformer \u7684\u7406\u89e3\uff0c\u4e26\u6307\u5c0e\u77e5\u8b58\u7de8\u8f2f\u7684\u6539\u9032\u8a2d\u8a08\u3002\u4ee3\u78bc\u548c\u8cc7\u6599\u53ef\u5728 https://github.com/zjunlp/KnowledgeCircuits \u4e2d\u53d6\u5f97\u3002", "author": "Yunzhi Yao et.al.", "authors": "Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen", "id": "2405.17969v1", "paper_url": "http://arxiv.org/abs/2405.17969v1", "repo": "https://github.com/zjunlp/knowledgecircuits"}}