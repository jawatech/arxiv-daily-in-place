{"2501.10054": {"publish_time": "2025-01-17", "title": "Accelerating Large Language Models through Partially Linear Feed-Forward Network", "paper_summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\ndeployment challenges due to their massive parameter counts. While existing\ncompression techniques like pruning can reduce model size, it leads to\nsignificant accuracy degradation under high compression ratios. We present a\nnovel perspective inspired by constant folding in compiler optimization. Our\napproach enables parameter reduction by treating activation functions in LLMs\nas linear functions.\n  However, recent LLMs use complex non-linear activations like GELU that\nprevent direct application of this technique. We propose TARDIS, which enables\noptimization of LLMs with non-linear activations by partially approximating\nthem with linear functions in frequently occurring input ranges. For outlier\ninputs, TARDIS employs an online predictor to dynamically fall back to original\ncomputations.\n  Our experiments demonstrate that TARDIS achieves 80% parameter reduction in\nfeed-forward networks, while significantly outperforming state-of-the-art\npruning methods Wanda and RIA with up to 65% higher accuracy. In practical\ndeployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup\nwhen integrated with the vLLM serving system, and 1.4x speedup with the widely\nadopted HuggingFace implementation, while incurring only a 10.9% accuracy\ntrade-off.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u793a\u4e86\u975e\u51e1\u7684\u80fd\u529b\uff0c\u4f46\u7531\u65bc\u5176\u9f90\u5927\u7684\u53c3\u6578\u6578\u91cf\u800c\u9762\u81e8\u90e8\u7f72\u6311\u6230\u3002\u96d6\u7136\u73fe\u6709\u7684\u58d3\u7e2e\u6280\u8853\uff08\u5982\u526a\u679d\uff09\u53ef\u4ee5\u7e2e\u5c0f\u6a21\u578b\u5927\u5c0f\uff0c\u4f46\u5b83\u6703\u5728\u9ad8\u58d3\u7e2e\u7387\u4e0b\u5c0e\u81f4\u986f\u8457\u7684\u6e96\u78ba\u6027\u4e0b\u964d\u3002\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u65b0\u7a4e\u7684\u89c0\u9ede\uff0c\u9748\u611f\u4f86\u81ea\u7de8\u8b6f\u5668\u512a\u5316\u4e2d\u7684\u5e38\u6578\u647a\u758a\u3002\u6211\u5011\u7684\u505a\u6cd5\u901a\u904e\u5c07 LLM \u4e2d\u7684\u6fc0\u6d3b\u51fd\u6578\u8996\u70ba\u7dda\u6027\u51fd\u6578\u4f86\u5be6\u73fe\u53c3\u6578\u6e1b\u5c11\u3002\n\u7136\u800c\uff0c\u6700\u8fd1\u7684 LLM \u4f7f\u7528\u8907\u96dc\u7684\u975e\u7dda\u6027\u6fc0\u6d3b\uff08\u5982 GELU\uff09\uff0c\u9019\u963b\u6b62\u4e86\u8a72\u6280\u8853\u7684\u76f4\u63a5\u61c9\u7528\u3002\u6211\u5011\u63d0\u51fa\u4e86 TARDIS\uff0c\u5b83\u901a\u904e\u5728\u983b\u7e41\u51fa\u73fe\u7684\u8f38\u5165\u7bc4\u570d\u5167\u90e8\u5206\u8fd1\u4f3c\u7dda\u6027\u51fd\u6578\u4f86\u5be6\u73fe\u5177\u6709\u975e\u7dda\u6027\u6fc0\u6d3b\u7684 LLM \u7684\u512a\u5316\u3002\u5c0d\u65bc\u7570\u5e38\u8f38\u5165\uff0cTARDIS \u4f7f\u7528\u7dda\u4e0a\u9810\u6e2c\u5668\u52d5\u614b\u56de\u9000\u5230\u539f\u59cb\u8a08\u7b97\u3002\n\u6211\u5011\u7684\u5be6\u9a57\u8868\u660e\uff0cTARDIS \u5728\u524d\u994b\u7db2\u8def\u4e2d\u5be6\u73fe\u4e86 80% \u7684\u53c3\u6578\u6e1b\u5c11\uff0c\u540c\u6642\u986f\u8457\u512a\u65bc\u6700\u5148\u9032\u7684\u526a\u679d\u65b9\u6cd5 Wanda \u548c RIA\uff0c\u6e96\u78ba\u5ea6\u63d0\u9ad8\u4e86 65%\u3002\u5728\u5c0d 7B \u6a21\u578b\u7684\u5be6\u969b\u90e8\u7f72\u4e2d\uff0cTARDIS \u8207 vLLM \u670d\u52d9\u7cfb\u7d71\u96c6\u6210\u6642\u5be6\u73fe\u4e86 1.6 \u500d\u7684\u7aef\u5230\u7aef\u63a8\u7406\u52a0\u901f\uff0c\u8207\u5ee3\u6cdb\u63a1\u7528\u7684 HuggingFace \u5be6\u73fe\u76f8\u6bd4\u52a0\u901f\u4e86 1.4 \u500d\uff0c\u540c\u6642\u50c5\u7522\u751f\u4e86 10.9% \u7684\u6e96\u78ba\u6027\u6298\u8877\u3002", "author": "Gansen Hu et.al.", "authors": "Gansen Hu, Zhaoguo Wang, Jinglin Wei, Wei Huang, Haibo Chen", "id": "2501.10054v1", "paper_url": "http://arxiv.org/abs/2501.10054v1", "repo": "null"}}