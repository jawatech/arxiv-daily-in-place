{"2410.20062": {"publish_time": "2024-10-26", "title": "Transforming Precision: A Comparative Analysis of Vision Transformers, CNNs, and Traditional ML for Knee Osteoarthritis Severity Diagnosis", "paper_summary": "Knee osteoarthritis(KO) is a degenerative joint disease that can cause severe\npain and impairment. With increased prevalence, precise diagnosis by medical\nimaging analytics is crucial for appropriate illness management. This research\ninvestigates a comparative analysis between traditional machine learning\ntechniques and new deep learning models for diagnosing KO severity from X-ray\npictures. This study does not introduce new architectural innovations but\nrather illuminates the robust applicability and comparative effectiveness of\npre-existing ViT models in a medical imaging context, specifically for KO\nseverity diagnosis. The insights garnered from this comparative analysis\nadvocate for the integration of advanced ViT models in clinical diagnostic\nworkflows, potentially revolutionizing the precision and reliability of KO\nassessments. This study does not introduce new architectural innovations but\nrather illuminates the robust applicability and comparative effectiveness of\npre-existing ViT models in a medical imaging context, specifically for KO\nseverity diagnosis. The insights garnered from this comparative analysis\nadvocate for the integration of advanced ViT models in clinical diagnostic\nworkflows, potentially revolutionizing the precision & reliability of KO\nassessments. The study utilizes an osteoarthritis dataset from the\nOsteoarthritis Initiative (OAI) comprising images with 5 severity categories\nand uneven class distribution. While classic machine learning models like\nGaussianNB and KNN struggle in feature extraction, Convolutional Neural\nNetworks such as Inception-V3, VGG-19 achieve better accuracy between 55-65% by\nlearning hierarchical visual patterns. However, Vision Transformer\narchitectures like Da-VIT, GCViT and MaxViT emerge as indisputable champions,\ndisplaying 66.14% accuracy, 0.703 precision, 0.614 recall, AUC exceeding 0.835\nthanks to self-attention processes.", "paper_summary_zh": "<paragraph>\u819d\u9aa8\u95dc\u7bc0\u708e (KO) \u662f\u4e00\u7a2e\u9000\u5316\u6027\u95dc\u7bc0\u75be\u75c5\uff0c\u53ef\u80fd\u5c0e\u81f4\u56b4\u91cd\u75bc\u75db\u548c\u529f\u80fd\u969c\u7919\u3002\u96a8\u8457\u60a3\u75c5\u7387\u4e0a\u5347\uff0c\u900f\u904e\u91ab\u5b78\u5f71\u50cf\u5206\u6790\u9032\u884c\u7cbe\u78ba\u8a3a\u65b7\u5c0d\u65bc\u9069\u7576\u7684\u75be\u75c5\u7ba1\u7406\u81f3\u95dc\u91cd\u8981\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u50b3\u7d71\u6a5f\u5668\u5b78\u7fd2\u6280\u8853\u548c\u65b0\u7684\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u5728\u5f9e X \u5149\u5f71\u50cf\u8a3a\u65b7 KO \u56b4\u91cd\u7a0b\u5ea6\u4e4b\u9593\u7684\u6bd4\u8f03\u5206\u6790\u3002\u672c\u7814\u7a76\u4e26\u672a\u63d0\u51fa\u65b0\u7684\u67b6\u69cb\u5275\u65b0\uff0c\u800c\u662f\u95e1\u660e\u4e86\u9810\u5148\u5b58\u5728\u7684 ViT \u6a21\u578b\u5728\u91ab\u5b78\u5f71\u50cf\u60c5\u5883\u4e2d\u7684\u5f37\u5927\u9069\u7528\u6027\u548c\u6bd4\u8f03\u6548\u80fd\uff0c\u7279\u5225\u662f\u91dd\u5c0d KO \u56b4\u91cd\u7a0b\u5ea6\u8a3a\u65b7\u3002\u5f9e\u6b64\u6bd4\u8f03\u5206\u6790\u4e2d\u7372\u5f97\u7684\u898b\u89e3\u4e3b\u5f35\u5c07\u5148\u9032\u7684 ViT \u6a21\u578b\u6574\u5408\u5230\u81e8\u5e8a\u8a3a\u65b7\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u9019\u53ef\u80fd\u6703\u5fb9\u5e95\u6539\u8b8a KO \u8a55\u4f30\u7684\u7cbe\u78ba\u5ea6\u548c\u53ef\u9760\u6027\u3002\u672c\u7814\u7a76\u4e26\u672a\u63d0\u51fa\u65b0\u7684\u67b6\u69cb\u5275\u65b0\uff0c\u800c\u662f\u95e1\u660e\u4e86\u9810\u5148\u5b58\u5728\u7684 ViT \u6a21\u578b\u5728\u91ab\u5b78\u5f71\u50cf\u60c5\u5883\u4e2d\u7684\u5f37\u5927\u9069\u7528\u6027\u548c\u6bd4\u8f03\u6548\u80fd\uff0c\u7279\u5225\u662f\u91dd\u5c0d KO \u56b4\u91cd\u7a0b\u5ea6\u8a3a\u65b7\u3002\u5f9e\u6b64\u6bd4\u8f03\u5206\u6790\u4e2d\u7372\u5f97\u7684\u898b\u89e3\u4e3b\u5f35\u5c07\u5148\u9032\u7684 ViT \u6a21\u578b\u6574\u5408\u5230\u81e8\u5e8a\u8a3a\u65b7\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u9019\u53ef\u80fd\u6703\u5fb9\u5e95\u6539\u8b8a KO \u8a55\u4f30\u7684\u7cbe\u78ba\u5ea6\u548c\u53ef\u9760\u6027\u3002\u672c\u7814\u7a76\u5229\u7528\u4e86\u4f86\u81ea\u9aa8\u95dc\u7bc0\u708e\u5021\u8b70\u7d44\u7e54 (OAI) \u7684\u9aa8\u95dc\u7bc0\u708e\u8cc7\u6599\u96c6\uff0c\u5176\u4e2d\u5305\u542b 5 \u500b\u56b4\u91cd\u7a0b\u5ea6\u985e\u5225\u548c\u4e0d\u5747\u52fb\u7684\u985e\u5225\u5206\u4f48\u7684\u5f71\u50cf\u3002\u96d6\u7136\u50cf GaussianNB \u548c KNN \u7b49\u7d93\u5178\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u5728\u7279\u5fb5\u8403\u53d6\u65b9\u9762\u9047\u5230\u4e86\u56f0\u96e3\uff0c\u4f46\u50cf Inception-V3\u3001VGG-19 \u7b49\u5377\u7a4d\u795e\u7d93\u7db2\u8def\u900f\u904e\u5b78\u7fd2\u968e\u5c64\u5f0f\u8996\u89ba\u6a21\u5f0f\u9054\u5230\u4e86 55-65% \u4e4b\u9593\u7684\u8f03\u4f73\u6e96\u78ba\u5ea6\u3002\u7136\u800c\uff0c\u50cf Da-VIT\u3001GCViT \u548c MaxViT \u7b49\u8996\u89ba\u8f49\u63db\u5668\u67b6\u69cb\u812b\u7a4e\u800c\u51fa\u6210\u70ba\u7121\u53ef\u722d\u8b70\u7684\u4f7c\u4f7c\u8005\uff0c\u7531\u65bc\u81ea\u6ce8\u610f\u529b\u7a0b\u5e8f\uff0c\u5b83\u5011\u8868\u73fe\u51fa 66.14% \u7684\u6e96\u78ba\u5ea6\u30010.703 \u7684\u7cbe\u78ba\u5ea6\u30010.614 \u7684\u53ec\u56de\u7387\u3001\u8d85\u904e 0.835 \u7684 AUC\u3002</paragraph>", "author": "Tasnim Sakib Apon et.al.", "authors": "Tasnim Sakib Apon, Md. Fahim-Ul-Islam, Nafiz Imtiaz Rafin, Joya Akter, Md. Golam Rabiul Alam", "id": "2410.20062v1", "paper_url": "http://arxiv.org/abs/2410.20062v1", "repo": "null"}}