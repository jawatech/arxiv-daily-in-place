{"2406.09401": {"publish_time": "2024-06-13", "title": "MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations", "paper_summary": "With the emergence of LLMs and their integration with other data modalities,\nmulti-modal 3D perception attracts more attention due to its connectivity to\nthe physical world and makes rapid progress. However, limited by existing\ndatasets, previous works mainly focus on understanding object properties or\ninter-object spatial relationships in a 3D scene. To tackle this problem, this\npaper builds the first largest ever multi-modal 3D scene dataset and benchmark\nwith hierarchical grounded language annotations, MMScan. It is constructed\nbased on a top-down logic, from region to object level, from a single target to\ninter-target relationships, covering holistic aspects of spatial and attribute\nunderstanding. The overall pipeline incorporates powerful VLMs via carefully\ndesigned prompts to initialize the annotations efficiently and further involve\nhumans' correction in the loop to ensure the annotations are natural, correct,\nand comprehensive. Built upon existing 3D scanning data, the resulting\nmulti-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects\nand 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding\nand question-answering benchmarks. We evaluate representative baselines on our\nbenchmarks, analyze their capabilities in different aspects, and showcase the\nkey problems to be addressed in the future. Furthermore, we use this\nhigh-quality dataset to train state-of-the-art 3D visual grounding and LLMs and\nobtain remarkable performance improvement both on existing benchmarks and\nin-the-wild evaluation. Codes, datasets, and benchmarks will be available at\nhttps://github.com/OpenRobotLab/EmbodiedScan.", "paper_summary_zh": "<paragraph>\u96a8\u8457 LLM \u7684\u51fa\u73fe\u53ca\u5176\u8207\u5176\u4ed6\u8cc7\u6599\u6a21\u5f0f\u7684\u6574\u5408\uff0c\u591a\u6a21\u5f0f 3D \u611f\u77e5\u56e0\u5176\u8207\u7269\u7406\u4e16\u754c\u7684\u9023\u901a\u6027\u800c\u5099\u53d7\u95dc\u6ce8\uff0c\u4e26\u53d6\u5f97\u5feb\u901f\u9032\u5c55\u3002\u7136\u800c\uff0c\u53d7\u9650\u65bc\u73fe\u6709\u8cc7\u6599\u96c6\uff0c\u5148\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u7406\u89e3 3D \u5834\u666f\u4e2d\u7684\u7269\u4ef6\u5c6c\u6027\u6216\u7269\u4ef6\u9593\u7684\u7a7a\u9593\u95dc\u4fc2\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u672c\u6587\u5efa\u69cb\u4e86\u53f2\u4e0a\u7b2c\u4e00\u500b\u6700\u5927\u7684\u591a\u6a21\u5f0f 3D \u5834\u666f\u8cc7\u6599\u96c6\u548c\u57fa\u6e96\uff0c\u4e26\u4f7f\u7528\u5206\u5c64\u7684\u57fa\u790e\u8a9e\u8a00\u8a3b\u89e3\uff0c\u5373 MMScan\u3002\u5b83\u662f\u6839\u64da\u7531\u4e0a\u800c\u4e0b\u7684\u908f\u8f2f\u5efa\u69cb\u7684\uff0c\u5f9e\u5340\u57df\u5230\u7269\u4ef6\u5c64\u7d1a\uff0c\u5f9e\u55ae\u4e00\u76ee\u6a19\u5230\u76ee\u6a19\u9593\u7684\u95dc\u4fc2\uff0c\u6db5\u84cb\u4e86\u7a7a\u9593\u548c\u5c6c\u6027\u7406\u89e3\u7684\u6574\u9ad4\u9762\u5411\u3002\u6574\u9ad4\u6d41\u7a0b\u900f\u904e\u7cbe\u5fc3\u8a2d\u8a08\u7684\u63d0\u793a\u6574\u5408\u5f37\u5927\u7684 VLM\uff0c\u4ee5\u6709\u6548\u5730\u521d\u59cb\u5316\u8a3b\u89e3\uff0c\u4e26\u9032\u4e00\u6b65\u8b93\u4eba\u985e\u53c3\u8207\u5faa\u74b0\u4e2d\u7684\u4fee\u6b63\uff0c\u4ee5\u78ba\u4fdd\u8a3b\u89e3\u81ea\u7136\u3001\u6b63\u78ba\u4e14\u5168\u9762\u3002\u5efa\u7acb\u5728\u73fe\u6709\u7684 3D \u6383\u63cf\u8cc7\u6599\u4e4b\u4e0a\uff0c\u6240\u7522\u751f\u7684\u591a\u6a21\u5f0f 3D \u8cc7\u6599\u96c6\u5305\u542b 109k \u500b\u7269\u4ef6\u548c 7.7k \u500b\u5340\u57df\u7684 1.4M \u500b\u5143\u8a3b\u89e3\u6a19\u984c\uff0c\u4ee5\u53ca\u8d85\u904e 3.04M \u500b\u7528\u65bc 3D \u8996\u89ba\u57fa\u790e\u548c\u554f\u7b54\u57fa\u51c6\u7684\u591a\u6a23\u5316\u6a23\u672c\u3002\u6211\u5011\u5728\u6211\u5011\u7684\u57fa\u6e96\u4e0a\u8a55\u4f30\u5177\u4ee3\u8868\u6027\u7684\u57fa\u6e96\uff0c\u5206\u6790\u5b83\u5011\u5728\u4e0d\u540c\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4e26\u5c55\u793a\u672a\u4f86\u9700\u8981\u89e3\u6c7a\u7684\u95dc\u9375\u554f\u984c\u3002\u6b64\u5916\uff0c\u6211\u5011\u4f7f\u7528\u9019\u500b\u9ad8\u54c1\u8cea\u7684\u8cc7\u6599\u96c6\u4f86\u8a13\u7df4\u6700\u5148\u9032\u7684 3D \u8996\u89ba\u57fa\u790e\u548c LLM\uff0c\u4e26\u5728\u73fe\u6709\u7684\u57fa\u6e96\u548c\u5be6\u969b\u8a55\u4f30\u4e2d\u7372\u5f97\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\u3002\u7a0b\u5f0f\u78bc\u3001\u8cc7\u6599\u96c6\u548c\u57fa\u6e96\u5c07\u5728 https://github.com/OpenRobotLab/EmbodiedScan \u63d0\u4f9b\u3002</paragraph>", "author": "Ruiyuan Lyu et.al.", "authors": "Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, Jiangmiao Pang", "id": "2406.09401v1", "paper_url": "http://arxiv.org/abs/2406.09401v1", "repo": "https://github.com/openrobotlab/embodiedscan"}}