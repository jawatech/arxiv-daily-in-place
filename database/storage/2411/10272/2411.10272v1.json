{"2411.10272": {"publish_time": "2024-11-15", "title": "Scaling Law for Post-training after Model Pruning", "paper_summary": "Large language models (LLMs) based on the Transformer architecture are widely\nemployed across various domains and tasks. However, their increasing size\nimposes significant hardware demands, limiting practical deployment. To\nmitigate this, model pruning techniques have been developed to create more\nefficient models while maintaining high performance. Despite this,\npost-training after pruning is crucial for performance recovery and can be\nresource-intensive. This paper investigates the post-training requirements of\npruned LLMs and introduces a scaling law to determine the optimal amount of\npost-training data. Post-training experiments with the Llama-3 and Qwen-2.5\nseries models, pruned using depth pruning, width pruning, and 2:4\nsemi-structured pruning, show that higher pruning ratios necessitate more\npost-training data for performance recovery, whereas larger LLMs require less.\nThe proposed scaling law predicts a model's loss based on its parameter counts\nbefore and after pruning, as well as the post-training token counts.\nFurthermore, we find that the scaling law established from smaller LLMs can be\nreliably extrapolated to larger LLMs. This work provides valuable insights into\nthe post-training of pruned LLMs and offers a practical scaling law for\noptimizing post-training data usage.", "paper_summary_zh": "<paragraph>\u57fa\u65bc Transformer \u67b6\u69cb\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5ee3\u6cdb\u7528\u65bc\u5404\u7a2e\u9818\u57df\u548c\u4efb\u52d9\u4e2d\u3002\u7136\u800c\uff0c\u5b83\u5011\u65e5\u76ca\u589e\u52a0\u7684\u5927\u5c0f\u6703\u5e36\u4f86\u986f\u8457\u7684\u786c\u9ad4\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5be6\u969b\u90e8\u7f72\u3002\u70ba\u4e86\u6e1b\u8f15\u9019\u500b\u554f\u984c\uff0c\u5df2\u7d93\u958b\u767c\u51fa\u6a21\u578b\u526a\u679d\u6280\u8853\uff0c\u4ee5\u5728\u7dad\u6301\u9ad8\u6027\u80fd\u7684\u540c\u6642\u5efa\u7acb\u66f4\u6709\u6548\u7387\u7684\u6a21\u578b\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u526a\u679d\u5f8c\u7684\u5f8c\u7e8c\u8a13\u7df4\u5c0d\u65bc\u6027\u80fd\u6062\u5fa9\u81f3\u95dc\u91cd\u8981\uff0c\u800c\u4e14\u53ef\u80fd\u6703\u6d88\u8017\u5927\u91cf\u8cc7\u6e90\u3002\u672c\u6587\u63a2\u8a0e\u4e86\u526a\u679d\u5f8c\u7684 LLM \u7684\u5f8c\u7e8c\u8a13\u7df4\u9700\u6c42\uff0c\u4e26\u5f15\u5165\u4e86\u4e00\u500b\u7e2e\u653e\u5b9a\u5f8b\u4f86\u78ba\u5b9a\u6700\u4f73\u7684\u5f8c\u7e8c\u8a13\u7df4\u8cc7\u6599\u91cf\u3002\u4f7f\u7528\u6df1\u5ea6\u526a\u679d\u3001\u5bec\u5ea6\u526a\u679d\u548c 2:4 \u534a\u7d50\u69cb\u5316\u526a\u679d\u9032\u884c\u526a\u679d\u7684 Llama-3 \u548c Qwen-2.5 \u7cfb\u5217\u6a21\u578b\u7684\u5f8c\u7e8c\u8a13\u7df4\u5be6\u9a57\u8868\u660e\uff0c\u8f03\u9ad8\u7684\u526a\u679d\u7387\u9700\u8981\u66f4\u591a\u5f8c\u7e8c\u8a13\u7df4\u8cc7\u6599\u624d\u80fd\u6062\u5fa9\u6027\u80fd\uff0c\u800c\u8f03\u5927\u7684 LLM \u5247\u9700\u8981\u8f03\u5c11\u3002\u6240\u63d0\u51fa\u7684\u7e2e\u653e\u5b9a\u5f8b\u6839\u64da\u6a21\u578b\u5728\u526a\u679d\u524d\u5f8c\u7684\u53c3\u6578\u6578\u91cf\u4ee5\u53ca\u5f8c\u7e8c\u8a13\u7df4\u7684 token \u6578\u91cf\u9810\u6e2c\u6a21\u578b\u7684\u640d\u5931\u3002\u6b64\u5916\uff0c\u6211\u5011\u767c\u73fe\u5f9e\u8f03\u5c0f\u7684 LLM \u5efa\u7acb\u7684\u7e2e\u653e\u5b9a\u5f8b\u53ef\u4ee5\u53ef\u9760\u5730\u5916\u63a8\u5230\u8f03\u5927\u7684 LLM\u3002\u9019\u9805\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5c0d\u526a\u679d\u5f8c LLM \u7684\u5f8c\u7e8c\u8a13\u7df4\u7684\u5bf6\u8cb4\u898b\u89e3\uff0c\u4e26\u70ba\u6700\u4f73\u5316\u5f8c\u7e8c\u8a13\u7df4\u8cc7\u6599\u4f7f\u7528\u63d0\u4f9b\u4e86\u5be6\u7528\u7684\u7e2e\u653e\u5b9a\u5f8b\u3002</paragraph>", "author": "Xiaodong Chen et.al.", "authors": "Xiaodong Chen, Yuxuan Hu, Jing Zhang, Xiaokang Zhang, Cuiping Li, Hong Chen", "id": "2411.10272v1", "paper_url": "http://arxiv.org/abs/2411.10272v1", "repo": "null"}}