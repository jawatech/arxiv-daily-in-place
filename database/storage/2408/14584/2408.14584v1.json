{"2408.14584": {"publish_time": "2024-08-26", "title": "DIAGen: Diverse Image Augmentation with Generative Models", "paper_summary": "Simple data augmentation techniques, such as rotations and flips, are widely\nused to enhance the generalization power of computer vision models. However,\nthese techniques often fail to modify high-level semantic attributes of a\nclass. To address this limitation, researchers have explored generative\naugmentation methods like the recently proposed DA-Fusion. Despite some\nprogress, the variations are still largely limited to textural changes, thus\nfalling short on aspects like varied viewpoints, environment, weather\nconditions, or even class-level semantic attributes (eg, variations in a dog's\nbreed). To overcome this challenge, we propose DIAGen, building upon DA-Fusion.\nFirst, we apply Gaussian noise to the embeddings of an object learned with\nTextual Inversion to diversify generations using a pre-trained diffusion\nmodel's knowledge. Second, we exploit the general knowledge of a text-to-text\ngenerative model to guide the image generation of the diffusion model with\nvaried class-specific prompts. Finally, we introduce a weighting mechanism to\nmitigate the impact of poorly generated samples. Experimental results across\nvarious datasets show that DIAGen not only enhances semantic diversity but also\nimproves the performance of subsequent classifiers. The advantages of DIAGen\nover standard augmentations and the DA-Fusion baseline are particularly\npronounced with out-of-distribution samples.", "paper_summary_zh": "\u7c21\u55ae\u7684\u8cc7\u6599\u64f4\u5145\u6280\u8853\uff0c\u4f8b\u5982\u65cb\u8f49\u548c\u7ffb\u8f49\uff0c\u5ee3\u6cdb\u7528\u65bc\u589e\u5f37\u96fb\u8166\u8996\u89ba\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6280\u8853\u901a\u5e38\u7121\u6cd5\u4fee\u6539\u985e\u5225\u7684\u9ad8\u968e\u8a9e\u7fa9\u5c6c\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u9650\u5236\uff0c\u7814\u7a76\u4eba\u54e1\u63a2\u7d22\u4e86\u751f\u6210\u64f4\u5145\u65b9\u6cd5\uff0c\u4f8b\u5982\u6700\u8fd1\u63d0\u51fa\u7684 DA-Fusion\u3002\u5118\u7ba1\u53d6\u5f97\u4e86\u4e00\u4e9b\u9032\u5c55\uff0c\u4f46\u8b8a\u7570\u4ecd\u7136\u5f88\u5927\u7a0b\u5ea6\u4e0a\u50c5\u9650\u65bc\u7d0b\u7406\u8b8a\u5316\uff0c\u56e0\u6b64\u5728\u8996\u89d2\u3001\u74b0\u5883\u3001\u5929\u6c23\u689d\u4ef6\u751a\u81f3\u985e\u5225\u7d1a\u8a9e\u7fa9\u5c6c\u6027\uff08\u4f8b\u5982\uff0c\u72d7\u54c1\u7a2e\u7684\u8b8a\u5316\uff09\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 DIAGen\uff0c\u4e26\u5efa\u7acb\u5728 DA-Fusion \u4e4b\u4e0a\u3002\u9996\u5148\uff0c\u6211\u5011\u5c07\u9ad8\u65af\u566a\u8072\u61c9\u7528\u65bc\u4f7f\u7528\u6587\u5b57\u53cd\u8f49\u5b78\u7fd2\u7684\u7269\u4ef6\u5d4c\u5165\uff0c\u4ee5\u4f7f\u7528\u9810\u5148\u8a13\u7df4\u7684\u64f4\u6563\u6a21\u578b\u7684\u77e5\u8b58\u4f86\u591a\u6a23\u5316\u751f\u6210\u3002\u5176\u6b21\uff0c\u6211\u5011\u5229\u7528\u6587\u5b57\u8f49\u6587\u5b57\u751f\u6210\u6a21\u578b\u7684\u4e00\u822c\u77e5\u8b58\u4f86\u6307\u5c0e\u64f4\u6563\u6a21\u578b\u7684\u5f71\u50cf\u751f\u6210\uff0c\u4e26\u4f7f\u7528\u4e0d\u540c\u7684\u985e\u5225\u7279\u5b9a\u63d0\u793a\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u500b\u52a0\u6b0a\u6a5f\u5236\u4f86\u6e1b\u8f15\u751f\u6210\u4e0d\u826f\u6a23\u672c\u7684\u5f71\u97ff\u3002\u8de8\u5404\u7a2e\u8cc7\u6599\u96c6\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cDIAGen \u4e0d\u50c5\u589e\u5f37\u4e86\u8a9e\u7fa9\u591a\u6a23\u6027\uff0c\u800c\u4e14\u9084\u6539\u5584\u4e86\u5f8c\u7e8c\u5206\u985e\u5668\u7684\u6027\u80fd\u3002DIAGen \u76f8\u5c0d\u65bc\u6a19\u6e96\u64f4\u5145\u548c DA-Fusion \u57fa\u7dda\u7684\u512a\u52e2\u5728\u5206\u4f48\u5916\u6a23\u672c\u4e2d\u5c24\u70ba\u660e\u986f\u3002", "author": "Tobias Lingenberg et.al.", "authors": "Tobias Lingenberg, Markus Reuter, Gopika Sudhakaran, Dominik Gojny, Stefan Roth, Simone Schaub-Meyer", "id": "2408.14584v1", "paper_url": "http://arxiv.org/abs/2408.14584v1", "repo": "null"}}