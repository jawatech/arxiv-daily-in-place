{"2405.09125": {"publish_time": "2024-05-15", "title": "HAAP: Vision-context Hierarchical Attention Autoregressive with Adaptive Permutation for Scene Text Recognition", "paper_summary": "Internal Language Model (LM)-based methods use permutation language modeling\n(PLM) to solve the error correction caused by conditional independence in\nexternal LM-based methods. However, random permutations of human interference\ncause fit oscillations in the model training, and Iterative Refinement (IR)\noperation to improve multimodal information decoupling also introduces\nadditional overhead. To address these issues, this paper proposes the\nHierarchical Attention autoregressive Model with Adaptive Permutation (HAAP) to\nenhance the location-context-image interaction capability, improving\nautoregressive generalization with internal LM. First, we propose Implicit\nPermutation Neurons (IPN) to generate adaptive attention masks to dynamically\nexploit token dependencies. The adaptive masks increase the diversity of\ntraining data and prevent model dependency on a specific order. It reduces the\ntraining overhead of PLM while avoiding training fit oscillations. Second, we\ndevelop Cross-modal Hierarchical Attention mechanism (CHA) to couple context\nand image features. This processing establishes rich positional semantic\ndependencies between context and image while avoiding IR. Extensive\nexperimental results show the proposed HAAP achieves state-of-the-art (SOTA)\nperformance in terms of accuracy, complexity, and latency on several datasets.", "paper_summary_zh": "\u57fa\u65bc\u5167\u90e8\u8a9e\u8a00\u6a21\u578b (LM) \u7684\u65b9\u6cd5\u4f7f\u7528\u7f6e\u63db\u8a9e\u8a00\u5efa\u6a21 (PLM) \u4f86\u89e3\u6c7a\u5916\u90e8 LM \u65b9\u6cd5\u4e2d\u689d\u4ef6\u7368\u7acb\u6027\u6240\u9020\u6210\u7684\u932f\u8aa4\u6821\u6b63\u3002\u7136\u800c\uff0c\u4eba\u985e\u5e72\u9810\u7684\u96a8\u6a5f\u7f6e\u63db\u6703\u5c0e\u81f4\u6a21\u578b\u8a13\u7df4\u4e2d\u7684\u64ec\u5408\u632f\u76ea\uff0c\u800c\u7528\u65bc\u6539\u5584\u591a\u6a21\u614b\u8cc7\u8a0a\u89e3\u8026\u7684\u8fed\u4ee3\u7cbe\u7149 (IR) \u64cd\u4f5c\u4e5f\u6703\u5f15\u5165\u984d\u5916\u7684\u958b\u92b7\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u672c\u6587\u63d0\u51fa\u5177\u6709\u9069\u61c9\u6027\u7f6e\u63db\u7684\u968e\u5c64\u5f0f\u6ce8\u610f\u529b\u81ea\u8ff4\u6b78\u6a21\u578b (HAAP)\uff0c\u4ee5\u589e\u5f37\u4f4d\u7f6e-\u8108\u7d61-\u5f71\u50cf\u4e92\u52d5\u80fd\u529b\uff0c\u6539\u5584\u5167\u90e8 LM \u7684\u81ea\u8ff4\u6b78\u6cdb\u5316\u3002\u9996\u5148\uff0c\u6211\u5011\u63d0\u51fa\u96b1\u5f0f\u7f6e\u63db\u795e\u7d93\u5143 (IPN) \u4f86\u7522\u751f\u9069\u61c9\u6027\u6ce8\u610f\u529b\u906e\u7f69\uff0c\u4ee5\u52d5\u614b\u5229\u7528\u7b26\u865f\u4f9d\u8cf4\u6027\u3002\u9069\u61c9\u6027\u906e\u7f69\u589e\u52a0\u4e86\u8a13\u7df4\u8cc7\u6599\u7684\u591a\u6a23\u6027\uff0c\u4e26\u9632\u6b62\u6a21\u578b\u4f9d\u8cf4\u65bc\u7279\u5b9a\u9806\u5e8f\u3002\u5b83\u6e1b\u5c11\u4e86 PLM \u7684\u8a13\u7df4\u958b\u92b7\uff0c\u540c\u6642\u907f\u514d\u4e86\u8a13\u7df4\u64ec\u5408\u632f\u76ea\u3002\u5176\u6b21\uff0c\u6211\u5011\u958b\u767c\u4e86\u8de8\u6a21\u614b\u968e\u5c64\u5f0f\u6ce8\u610f\u529b\u6a5f\u5236 (CHA) \u4f86\u7d50\u5408\u8108\u7d61\u548c\u5f71\u50cf\u7279\u5fb5\u3002\u6b64\u8655\u7406\u5efa\u7acb\u4e86\u8108\u7d61\u548c\u5f71\u50cf\u4e4b\u9593\u8c50\u5bcc\u7684\u4f4d\u7f6e\u8a9e\u7fa9\u4f9d\u8cf4\u6027\uff0c\u540c\u6642\u907f\u514d\u4e86 IR\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 HAAP \u5728\u591a\u500b\u8cc7\u6599\u96c6\u4e0a\u9054\u5230\u4e86\u6e96\u78ba\u5ea6\u3001\u8907\u96dc\u5ea6\u548c\u5ef6\u9072\u65b9\u9762\u7684\u6700\u65b0\u6280\u8853 (SOTA) \u6548\u80fd\u3002", "author": "Honghui Chen et.al.", "authors": "Honghui Chen, Yuhang Qiu, Jiabao Wang, Pingping Chen, Nam Ling", "id": "2405.09125v1", "paper_url": "http://arxiv.org/abs/2405.09125v1", "repo": "null"}}