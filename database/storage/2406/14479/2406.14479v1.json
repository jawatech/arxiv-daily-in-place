{"2406.14479": {"publish_time": "2024-06-20", "title": "On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier", "paper_summary": "Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.", "paper_summary_zh": "<paragraph>\u5206\u6790\u4e0d\u540c\u6a21\u578b\u5167\u90e8\u8868\u793a\u7684\u76f8\u4f3c\u6027\u4e00\u76f4\u662f\u7406\u89e3\u6df1\u5ea6\u795e\u7d93\u7db2\u8def\u884c\u70ba\u7684\u91cd\u8981\u6280\u8853\u3002\u5927\u591a\u6578\u73fe\u6709\u7684\u5206\u6790\u9ad8\u7dad\u5ea6\u8868\u793a\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u65bc\u5178\u7bc4\u76f8\u95dc\u5206\u6790 (CCA) \u548c\u5ee3\u6cdb\u4f7f\u7528\u7684\u4e2d\u5fc3\u6838\u5c0d\u9f4a (CKA) \u7684\u65b9\u6cd5\uff0c\u4f9d\u8cf4\u65bc\u4e00\u7d44\u8cc7\u6599\u9ede\u8868\u793a\u7684\u7d71\u8a08\u6027\u8cea\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bcTransformer\u6a21\u578b\uff0c\u4e26\u7814\u7a76\u500b\u5225Transformer\u96b1\u85cf\u5c64\u4e4b\u9593\u8868\u793a\u7684\u76f8\u4f3c\u6027\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4e00\u500b\u7c21\u55ae\u7684\u6a23\u672c\u7d1a\u9918\u5f26\u76f8\u4f3c\u6027\u6307\u6a19\u80fd\u5920\u6355\u6349\u76f8\u4f3c\u6027\u4e26\u8207\u8907\u96dc\u7684 CKA \u4fdd\u6301\u4e00\u81f4\u3002\u6211\u5011\u5728\u5e38\u898bTransformer\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8de8\u5c64\u8868\u793a\u5448\u6b63\u76f8\u95dc\uff0c\u5118\u7ba1\u5728\u5c64\u76f8\u8ddd\u751a\u9060\u6642\u76f8\u4f3c\u6027\u6703\u964d\u4f4e\u3002\u7136\u5f8c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u5c0d\u9f4a\u8a13\u7df4\u65b9\u6cd5\u4f86\u589e\u5f37\u5167\u90e8\u8868\u793a\u4e4b\u9593\u7684\u76f8\u4f3c\u6027\uff0c\u8a13\u7df4\u5f8c\u7684\u6a21\u578b\u5177\u6709\u4ee5\u4e0b\u7279\u6027\uff1a(1) \u6700\u5f8c\u4e00\u5c64\u5206\u985e\u5668\u53ef\u4ee5\u76f4\u63a5\u61c9\u7528\u65bc\u4efb\u4f55\u96b1\u85cf\u5c64\u4e4b\u5f8c\uff0c\u7522\u751f\u6bd4\u6a19\u6e96\u8a13\u7df4\u4e0b\u9ad8\u7684\u4e2d\u9593\u5c64\u6e96\u78ba\u5ea6\uff0c(2) \u5c64\u7d1a\u6e96\u78ba\u5ea6\u55ae\u8abf\u589e\u52a0\uff0c\u4e26\u63ed\u793a\u7d66\u5b9a\u4efb\u52d9\u6240\u9700\u7684\u6700\u5c0f\u6df1\u5ea6\uff0c(3) \u7576\u4f5c\u70ba\u591a\u51fa\u53e3\u6a21\u578b\u6642\uff0c\u5b83\u5011\u5be6\u73fe\u4e86\u8207\u6a19\u6e96\u591a\u51fa\u53e3\u67b6\u69cb\u540c\u7b49\u7684\u6548\u80fd\uff0c\u5f8c\u8005\u7531\u984d\u5916\u7684\u5206\u985e\u5668\u7d44\u6210\uff0c\u9019\u4e9b\u5206\u985e\u5668\u8a2d\u8a08\u7528\u65bc\u5728\u6dfa\u5c64\u4e2d\u63d0\u524d\u9000\u51fa\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u6211\u5011\u7684\u7814\u7a76\u9996\u6b21\u8868\u660e\u4e00\u500b\u901a\u7528\u5206\u985e\u5668\u5c0d\u65bc\u591a\u51fa\u53e3\u6a21\u578b\u5c31\u8db3\u5920\u4e86\u3002\u6211\u5011\u5728\u8996\u89ba\u548c NLP \u4efb\u52d9\u4e0a\u9032\u884c\u4e86\u5be6\u9a57\uff0c\u4ee5\u5c55\u793a\u6240\u63d0\u51fa\u7684\u5c0d\u9f4a\u8a13\u7df4\u7684\u6548\u80fd\u3002</paragraph>", "author": "Jiachen Jiang et.al.", "authors": "Jiachen Jiang, Jinxin Zhou, Zhihui Zhu", "id": "2406.14479v1", "paper_url": "http://arxiv.org/abs/2406.14479v1", "repo": "null"}}