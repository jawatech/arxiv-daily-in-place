{"2411.07176": {"publish_time": "2024-11-11", "title": "More Expressive Attention with Negative Weights", "paper_summary": "We propose a novel attention mechanism, named Cog Attention, that enables\nattention weights to be negative for enhanced expressiveness, which stems from\ntwo key factors: (1) Cog Attention can shift the token deletion and copying\nfunction from a static OV matrix to dynamic QK inner products, with the OV\nmatrix now focusing more on refinement or modification. The attention head can\nsimultaneously delete, copy, or retain tokens by assigning them negative,\npositive, or minimal attention weights, respectively. As a result, a single\nattention head becomes more flexible and expressive. (2) Cog Attention improves\nthe model's robustness against representational collapse, which can occur when\nearlier tokens are over-squashed into later positions, leading to homogeneous\nrepresentations. Negative weights reduce effective information paths from\nearlier to later tokens, helping to mitigate this issue. We develop\nTransformer-like models which use Cog Attention as attention modules, including\ndecoder-only models for language modeling and U-ViT diffusion models for image\ngeneration. Experiments show that models using Cog Attention exhibit superior\nperformance compared to those employing traditional softmax attention modules.\nOur approach suggests a promising research direction for rethinking and\nbreaking the entrenched constraints of traditional softmax attention, such as\nthe requirement for non-negative weights.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70ba Cog Attention \u7684\u65b0\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u5b83\u80fd\u8b93\u6ce8\u610f\u529b\u6b0a\u91cd\u70ba\u8ca0\u6578\u4ee5\u589e\u5f37\u8868\u9054\u529b\uff0c\u9019\u6e90\u65bc\u5169\u500b\u95dc\u9375\u56e0\u7d20\uff1a(1) Cog Attention \u53ef\u4ee5\u5c07\u7b26\u865f\u522a\u9664\u548c\u8907\u88fd\u529f\u80fd\u5f9e\u975c\u614b OV \u77e9\u9663\u8f49\u79fb\u5230\u52d5\u614b QK \u5167\u7a4d\uff0c\u800c OV \u77e9\u9663\u73fe\u5728\u66f4\u5c08\u6ce8\u65bc\u7cbe\u7149\u6216\u4fee\u6539\u3002\u6ce8\u610f\u529b\u982d\u90e8\u53ef\u4ee5\u540c\u6642\u522a\u9664\u3001\u8907\u88fd\u6216\u4fdd\u7559\u7b26\u865f\uff0c\u5206\u5225\u7d66\u5b83\u5011\u5206\u914d\u8ca0\u3001\u6b63\u6216\u6700\u5c0f\u7684\u6ce8\u610f\u529b\u6b0a\u91cd\u3002\u56e0\u6b64\uff0c\u55ae\u4e00\u6ce8\u610f\u529b\u982d\u90e8\u8b8a\u5f97\u66f4\u9748\u6d3b\u548c\u5bcc\u6709\u8868\u73fe\u529b\u3002(2) Cog Attention \u63d0\u9ad8\u4e86\u6a21\u578b\u5c0d\u8868\u5fb5\u5d29\u6f70\u7684\u7a69\u5065\u6027\uff0c\u9019\u7a2e\u60c5\u6cc1\u53ef\u80fd\u767c\u751f\u5728\u8f03\u65e9\u7684\u7b26\u865f\u904e\u5ea6\u58d3\u7e2e\u5230\u5f8c\u9762\u7684\u4f4d\u7f6e\u6642\uff0c\u5c0e\u81f4\u540c\u8cea\u8868\u5fb5\u3002\u8ca0\u6b0a\u91cd\u6e1b\u5c11\u4e86\u5f9e\u8f03\u65e9\u7b26\u865f\u5230\u8f03\u5f8c\u7b26\u865f\u7684\u6709\u6548\u8cc7\u8a0a\u8def\u5f91\uff0c\u6709\u52a9\u65bc\u7de9\u89e3\u9019\u500b\u554f\u984c\u3002\u6211\u5011\u958b\u767c\u4e86\u4f7f\u7528 Cog Attention \u4f5c\u70ba\u6ce8\u610f\u529b\u6a21\u7d44\u7684\u985e Transformer \u6a21\u578b\uff0c\u5305\u62ec\u7528\u65bc\u8a9e\u8a00\u5efa\u6a21\u7684\u50c5\u89e3\u78bc\u5668\u6a21\u578b\u548c\u7528\u65bc\u5f71\u50cf\u751f\u6210\u7684 U-ViT \u64f4\u6563\u6a21\u578b\u3002\u5be6\u9a57\u8868\u660e\uff0c\u4f7f\u7528 Cog Attention \u7684\u6a21\u578b\u8868\u73fe\u51fa\u512a\u65bc\u63a1\u7528\u50b3\u7d71 softmax \u6ce8\u610f\u529b\u6a21\u7d44\u7684\u6a21\u578b\u3002\u6211\u5011\u7684\u505a\u6cd5\u70ba\u91cd\u65b0\u601d\u8003\u548c\u6253\u7834\u50b3\u7d71 softmax \u6ce8\u610f\u529b\u7684\u6839\u6df1\u8482\u56fa\u7d04\u675f\uff08\u4f8b\u5982\u975e\u8ca0\u6b0a\u91cd\u7684\u8981\u6c42\uff09\u63d0\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002", "author": "Ang Lv et.al.", "authors": "Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Rui Yan", "id": "2411.07176v1", "paper_url": "http://arxiv.org/abs/2411.07176v1", "repo": "https://github.com/trestad/cogattn"}}