{"2212.11136": {"publish_time": "2022-12-16", "title": "It is not \"accuracy vs. explainability\" -- we need both for trustworthy AI systems", "paper_summary": "We are witnessing the emergence of an AI economy and society where AI\ntechnologies are increasingly impacting health care, business, transportation\nand many aspects of everyday life. Many successes have been reported where AI\nsystems even surpassed the accuracy of human experts. However, AI systems may\nproduce errors, can exhibit bias, may be sensitive to noise in the data, and\noften lack technical and judicial transparency resulting in reduction in trust\nand challenges in their adoption. These recent shortcomings and concerns have\nbeen documented in scientific but also in general press such as accidents with\nself driving cars, biases in healthcare, hiring and face recognition systems\nfor people of color, seemingly correct medical decisions later found to be made\ndue to wrong reasons etc. This resulted in emergence of many government and\nregulatory initiatives requiring trustworthy and ethical AI to provide accuracy\nand robustness, some form of explainability, human control and oversight,\nelimination of bias, judicial transparency and safety. The challenges in\ndelivery of trustworthy AI systems motivated intense research on explainable AI\nsystems (XAI). Aim of XAI is to provide human understandable information of how\nAI systems make their decisions. In this paper we first briefly summarize\ncurrent XAI work and then challenge the recent arguments of accuracy vs.\nexplainability for being mutually exclusive and being focused only on deep\nlearning. We then present our recommendations for the use of XAI in full\nlifecycle of high stakes trustworthy AI systems delivery, e.g. development,\nvalidation and certification, and trustworthy production and maintenance.", "paper_summary_zh": "", "author": "D. Petkovic et.al.", "authors": "D. Petkovic", "id": "2212.11136v2", "paper_url": "http://arxiv.org/abs/2212.11136v2", "repo": "null"}}