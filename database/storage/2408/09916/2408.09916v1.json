{"2408.09916": {"publish_time": "2024-08-19", "title": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction in Vision Language Models with VisEdit", "paper_summary": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs.", "paper_summary_zh": "\u6a21\u578b\u7f16\u8f91\u65e8\u5728\u66f4\u6b63\u5927\u578b\u6a21\u578b\u4e2d\u8fc7\u65f6\u6216\u9519\u8bef\u7684\u77e5\u8bc6\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u4ee3\u4ef7\u9ad8\u6602\u7684\u91cd\u65b0\u8bad\u7ec3\u3002\u6700\u8fd1\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u793a\u4e2d\u4e3b\u9898\u7684\u6700\u7ec8\u6807\u8bb0\u7684\u4e2d\u5c42\u8868\u793a\u5bf9\u4e8b\u5b9e\u9884\u6d4b\u6709\u5f88\u5927\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u6b64\u89c2\u5bdf\u7ed3\u679c\u5f00\u53d1\u4e86\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u7f16\u8f91\u6280\u672f\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b (VLLM)\uff0c\u89c6\u89c9\u8868\u793a\u5982\u4f55\u5f71\u54cd\u4ec5\u89e3\u7801\u5668\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u6587\u732e\u4e2d\u5c1a\u672a\u5e7f\u6cdb\u7814\u7a76 VLLM \u7684\u6a21\u578b\u7f16\u8f91\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u8d21\u732e\u5206\u914d\u548c\u566a\u58f0\u6270\u52a8\u65b9\u6cd5\u6765\u8861\u91cf\u89c6\u89c9\u8868\u793a\u5bf9\u6807\u8bb0\u9884\u6d4b\u7684\u8d21\u732e\u3002\u6211\u4eec\u7684\u5f52\u56e0\u5206\u6790\u8868\u660e\uff0c\u4e0e\u63d0\u793a\u9ad8\u5ea6\u76f8\u5173\u7684\u4e2d\u95f4\u5230\u540e\u5c42\u4e2d\u7684\u89c6\u89c9\u8868\u793a\u5bf9\u9884\u6d4b\u6709\u5f88\u5927\u8d21\u732e\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 VisEdit\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684 VLLM \u6a21\u578b\u7f16\u8f91\u5668\uff0c\u5b83\u901a\u8fc7\u7f16\u8f91\u5bf9\u7f16\u8f91\u63d0\u793a\u5f88\u91cd\u8981\u7684\u533a\u57df\u4e2d\u7684\u4e2d\u95f4\u89c6\u89c9\u8868\u793a\u6765\u6709\u6548\u5730\u66f4\u6b63\u77e5\u8bc6\u3002\u6211\u4eec\u4f7f\u7528\u591a\u4e2a VLLM \u4e3b\u5e72\u548c\u516c\u5171 VLLM \u7f16\u8f91\u57fa\u51c6\u6570\u636e\u96c6\u8bc4\u4f30\u4e86 VisEdit\u3002\u7ed3\u679c\u8868\u660e VisEdit \u4f18\u4e8e\u4ece\u73b0\u6709\u7684 LLM \u6700\u5148\u8fdb\u7f16\u8f91\u5668\u6539\u7f16\u7684\u5f3a\u5927\u57fa\u7ebf\u3002", "author": "Qizhou Chen et.al.", "authors": "Qizhou Chen, Taolin Zhang, Chengyu Wang, Xiaofeng He, Dakan Wang, Tingting Liu", "id": "2408.09916v1", "paper_url": "http://arxiv.org/abs/2408.09916v1", "repo": "null"}}