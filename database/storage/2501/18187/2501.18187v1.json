{"2501.18187": {"publish_time": "2025-01-30", "title": "In-Context Learning of Polynomial Kernel Regression in Transformers with GLU Layers", "paper_summary": "Transformer-based models have demonstrated remarkable ability in in-context\nlearning (ICL), where they can adapt to unseen tasks from a prompt with a few\nexamples, without requiring parameter updates. Recent research has provided\ninsight into how linear Transformers can perform ICL by implementing gradient\ndescent estimators. In particular, it has been shown that the optimal linear\nself-attention (LSA) mechanism can implement one step of gradient descent with\nrespect to a linear least-squares objective when trained on random linear\nregression tasks.\n  However, the theoretical understanding of ICL for nonlinear function classes\nremains limited. In this work, we address this gap by first showing that LSA is\ninherently restricted to solving linear least-squares objectives and thus, the\nsolutions in prior works cannot readily extend to nonlinear ICL tasks. To\novercome this limitation, drawing inspiration from modern architectures, we\nstudy a mechanism that combines LSA with GLU-like feed-forward layers and show\nthat this allows the model to perform one step of gradient descent on a\npolynomial kernel regression. Further, we characterize the scaling behavior of\nthe resulting Transformer model, highlighting the necessary model size to\neffectively handle quadratic ICL tasks. Our findings highlight the distinct\nroles of attention and feed-forward layers in nonlinear ICL and identify key\nchallenges when extending ICL to nonlinear function classes.", "paper_summary_zh": "\u57fa\u65bc Transformer \u7684\u6a21\u578b\u5df2\u5728\u60c5\u5883\u4e2d\u5b78\u7fd2 (ICL) \u4e2d\u5c55\u73fe\u51fa\u975e\u51e1\u7684\u80fd\u529b\uff0c\u5b83\u5011\u53ef\u4ee5\u5728\u63d0\u793a\u4e2d\u6839\u64da\u5e7e\u500b\u7bc4\u4f8b\u9069\u61c9\u672a\u898b\u904e\u7684\u5de5\u4f5c\uff0c\u800c\u7121\u9700\u53c3\u6578\u66f4\u65b0\u3002\u6700\u8fd1\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7dda\u6027 Transformer \u5982\u4f55\u900f\u904e\u5be6\u4f5c\u68af\u5ea6\u4e0b\u964d\u4f30\u8a08\u5668\u4f86\u57f7\u884c ICL \u7684\u898b\u89e3\u3002\u7279\u5225\u662f\uff0c\u5df2\u986f\u793a\u6700\u4f73\u7dda\u6027\u81ea\u6ce8\u610f\u529b (LSA) \u6a5f\u5236\u53ef\u4ee5\u5728\u96a8\u6a5f\u7dda\u6027\u56de\u6b78\u5de5\u4f5c\u4e0a\u8a13\u7df4\u6642\uff0c\u91dd\u5c0d\u7dda\u6027\u6700\u5c0f\u5e73\u65b9\u76ee\u6a19\u5be6\u4f5c\u4e00\u6b65\u68af\u5ea6\u4e0b\u964d\u3002\n\u7136\u800c\uff0c\u5c0d\u65bc\u975e\u7dda\u6027\u51fd\u6578\u985e\u5225\u7684 ICL \u7684\u7406\u8ad6\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u9996\u5148\u900f\u904e\u986f\u793a LSA \u672c\u8cea\u4e0a\u50c5\u9650\u65bc\u6c42\u89e3\u7dda\u6027\u6700\u5c0f\u5e73\u65b9\u76ee\u6a19\uff0c\u56e0\u6b64\u5148\u524d\u7684\u8457\u4f5c\u4e2d\u7684\u89e3\u6c7a\u65b9\u6848\u7121\u6cd5\u8f15\u6613\u5ef6\u4f38\u5230\u975e\u7dda\u6027 ICL \u5de5\u4f5c\uff0c\u4f86\u89e3\u6c7a\u9019\u500b\u5dee\u8ddd\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f9e\u73fe\u4ee3\u67b6\u69cb\u4e2d\u6c72\u53d6\u9748\u611f\uff0c\u7814\u7a76\u4e00\u7a2e\u5c07 LSA \u8207 GLU \u985e\u4f3c\u7684\u524d\u994b\u5c64\u7d50\u5408\u7684\u6a5f\u5236\uff0c\u4e26\u986f\u793a\u9019\u5141\u8a31\u6a21\u578b\u5728\u591a\u9805\u5f0f\u6838\u56de\u6b78\u4e0a\u57f7\u884c\u4e00\u6b65\u68af\u5ea6\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u7d50\u679c Transformer \u6a21\u578b\u7684\u7e2e\u653e\u884c\u70ba\uff0c\u5f37\u8abf\u6709\u6548\u8655\u7406\u4e8c\u6b21 ICL \u5de5\u4f5c\u6240\u9700\u7684\u6a21\u578b\u5927\u5c0f\u3002\u6211\u5011\u7684\u767c\u73fe\u7a81\u51fa\u4e86\u6ce8\u610f\u529b\u548c\u524d\u994b\u5c64\u5728\u975e\u7dda\u6027 ICL \u4e2d\u7684\u4e0d\u540c\u89d2\u8272\uff0c\u4e26\u5728\u5c07 ICL \u5ef6\u4f38\u5230\u975e\u7dda\u6027\u51fd\u6578\u985e\u5225\u6642\uff0c\u8b58\u5225\u51fa\u95dc\u9375\u6311\u6230\u3002", "author": "Haoyuan Sun et.al.", "authors": "Haoyuan Sun, Ali Jadbabaie, Navid Azizan", "id": "2501.18187v1", "paper_url": "http://arxiv.org/abs/2501.18187v1", "repo": "null"}}