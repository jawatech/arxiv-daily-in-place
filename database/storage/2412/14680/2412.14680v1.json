{"2412.14680": {"publish_time": "2024-12-19", "title": "A Light-Weight Framework for Open-Set Object Detection with Decoupled Feature Alignment in Joint Space", "paper_summary": "Open-set object detection (OSOD) is highly desirable for robotic manipulation\nin unstructured environments. However, existing OSOD methods often fail to meet\nthe requirements of robotic applications due to their high computational burden\nand complex deployment. To address this issue, this paper proposes a\nlight-weight framework called Decoupled OSOD (DOSOD), which is a practical and\nhighly efficient solution to support real-time OSOD tasks in robotic systems.\nSpecifically, DOSOD builds upon the YOLO-World pipeline by integrating a\nvision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)\nadaptor is developed to transform text embeddings extracted by the VLM into a\njoint space, within which the detector learns the region representations of\nclass-agnostic proposals. Cross-modality features are directly aligned in the\njoint space, avoiding the complex feature interactions and thereby improving\ncomputational efficiency. DOSOD operates like a traditional closed-set detector\nduring the testing phase, effectively bridging the gap between closed-set and\nopen-set detection. Compared to the baseline YOLO-World, the proposed DOSOD\nsignificantly enhances real-time performance while maintaining comparable\naccuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\\%$, compared to\n$26.2\\%$ for YOLO-World-v1-S and $22.7\\%$ for YOLO-World-v2-S, using similar\nbackbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is\n$57.1\\%$ higher than YOLO-World-v1-S and $29.6\\%$ higher than YOLO-World-v2-S.\nMeanwhile, we demonstrate that the DOSOD model facilitates the deployment of\nedge devices. The codes and models are publicly available at\nhttps://github.com/D-Robotics-AI-Lab/DOSOD.", "paper_summary_zh": "<paragraph>\u958b\u653e\u5f0f\u7269\u4ef6\u5075\u6e2c (OSOD) \u5c0d\u65bc\u975e\u7d50\u69cb\u5316\u74b0\u5883\u4e2d\u7684\u6a5f\u5668\u4eba\u64cd\u4f5c\u975e\u5e38\u9700\u8981\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 OSOD \u65b9\u6cd5\u7531\u65bc\u5176\u9ad8\u904b\u7b97\u8ca0\u64d4\u548c\u8907\u96dc\u7684\u90e8\u7f72\uff0c\u901a\u5e38\u7121\u6cd5\u6eff\u8db3\u6a5f\u5668\u4eba\u61c9\u7528\u7a0b\u5f0f\u7684\u9700\u6c42\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u500b\u7a31\u70ba\u89e3\u8026 OSOD (DOSOD) \u7684\u8f15\u91cf\u7d1a\u6846\u67b6\uff0c\u9019\u662f\u4e00\u500b\u5be6\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u7528\u65bc\u652f\u63f4\u6a5f\u5668\u4eba\u7cfb\u7d71\u4e2d\u7684\u5373\u6642 OSOD \u4efb\u52d9\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cDOSOD \u5efa\u7acb\u5728 YOLO-World \u7ba1\u7dda\u4e0a\uff0c\u900f\u904e\u5c07\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u8207\u5075\u6e2c\u5668\u6574\u5408\u5728\u4e00\u8d77\u3002\u958b\u767c\u4e86\u4e00\u500b\u591a\u5c64\u611f\u77e5\u5668 (MLP) \u9069\u914d\u5668\uff0c\u5c07 VLM \u63d0\u53d6\u7684\u6587\u5b57\u5d4c\u5165\u8f49\u63db\u6210\u4e00\u500b\u806f\u5408\u7a7a\u9593\uff0c\u5075\u6e2c\u5668\u5728\u5176\u4e2d\u5b78\u7fd2\u985e\u5225\u4e0d\u53ef\u77e5\u63d0\u6848\u7684\u5340\u57df\u8868\u793a\u3002\u8de8\u6a21\u614b\u7279\u5fb5\u76f4\u63a5\u5728\u806f\u5408\u7a7a\u9593\u4e2d\u5c0d\u9f4a\uff0c\u907f\u514d\u4e86\u8907\u96dc\u7684\u7279\u5fb5\u4ea4\u4e92\uff0c\u5f9e\u800c\u63d0\u9ad8\u4e86\u904b\u7b97\u6548\u7387\u3002DOSOD \u5728\u6e2c\u8a66\u968e\u6bb5\u5c31\u50cf\u50b3\u7d71\u7684\u5c01\u9589\u5f0f\u5075\u6e2c\u5668\u4e00\u6a23\u904b\u4f5c\uff0c\u6709\u6548\u5730\u7e2e\u5c0f\u4e86\u5c01\u9589\u5f0f\u548c\u958b\u653e\u5f0f\u5075\u6e2c\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u8207\u57fa\u6e96 YOLO-World \u76f8\u6bd4\uff0c\u63d0\u51fa\u7684 DOSOD \u5728\u4fdd\u6301\u53ef\u6bd4\u7cbe\u78ba\u5ea6\u7684\u540c\u6642\uff0c\u986f\u8457\u63d0\u5347\u4e86\u5373\u6642\u6548\u80fd\u3002\u8f15\u91cf\u7684 DOSOD-S \u6a21\u578b\u5728 LVIS minival \u8cc7\u6599\u96c6\u4e0a\u4f7f\u7528\u985e\u4f3c\u7684\u9aa8\u5e79\u7db2\u8def\uff0c\u9054\u5230\u4e86 26.7% \u7684\u56fa\u5b9a AP\uff0c\u800c YOLO-World-v1-S \u70ba 26.2%\uff0cYOLO-World-v2-S \u70ba 22.7%\u3002\u540c\u6642\uff0cDOSOD-S \u7684 FPS \u6bd4 YOLO-World-v1-S \u9ad8 57.1%\uff0c\u6bd4 YOLO-World-v2-S \u9ad8 29.6%\u3002\u540c\u6642\uff0c\u6211\u5011\u8b49\u660e\u4e86 DOSOD \u6a21\u578b\u6709\u52a9\u65bc\u908a\u7de3\u88dd\u7f6e\u7684\u90e8\u7f72\u3002\u7a0b\u5f0f\u78bc\u548c\u6a21\u578b\u5df2\u516c\u958b\u5728 https://github.com/D-Robotics-AI-Lab/DOSOD\u3002</paragraph>", "author": "Yonghao He et.al.", "authors": "Yonghao He, Hu Su, Haiyong Yu, Cong Yang, Wei Sui, Cong Wang, Song Liu", "id": "2412.14680v1", "paper_url": "http://arxiv.org/abs/2412.14680v1", "repo": "null"}}