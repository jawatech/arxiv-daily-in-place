{"2412.19155": {"publish_time": "2024-12-26", "title": "Referencing Where to Focus: Improving VisualGrounding with Referential Query", "paper_summary": "Visual Grounding aims to localize the referring object in an image given a\nnatural language expression. Recent advancements in DETR-based visual grounding\nmethods have attracted considerable attention, as they directly predict the\ncoordinates of the target object without relying on additional efforts, such as\npre-generated proposal candidates or pre-defined anchor boxes. However,\nexisting research primarily focuses on designing stronger multi-modal decoder,\nwhich typically generates learnable queries by random initialization or by\nusing linguistic embeddings. This vanilla query generation approach inevitably\nincreases the learning difficulty for the model, as it does not involve any\ntarget-related information at the beginning of decoding. Furthermore, they only\nuse the deepest image feature during the query learning process, overlooking\nthe importance of features from other levels. To address these issues, we\npropose a novel approach, called RefFormer. It consists of the query adaption\nmodule that can be seamlessly integrated into CLIP and generate the referential\nquery to provide the prior context for decoder, along with a task-specific\ndecoder. By incorporating the referential query into the decoder, we can\neffectively mitigate the learning difficulty of the decoder, and accurately\nconcentrate on the target object. Additionally, our proposed query adaption\nmodule can also act as an adapter, preserving the rich knowledge within CLIP\nwithout the need to tune the parameters of the backbone network. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmethod, outperforming state-of-the-art approaches on five visual grounding\nbenchmarks.", "paper_summary_zh": "\u8996\u89ba\u57fa\u790e\u65e8\u5728\u6839\u64da\u81ea\u7136\u8a9e\u8a00\u8868\u9054\u5f0f\u5728\u5716\u50cf\u4e2d\u5b9a\u4f4d\u53c3\u8003\u5c0d\u8c61\u3002\u57fa\u65bc DETR \u7684\u8996\u89ba\u57fa\u790e\u65b9\u6cd5\u7684\u6700\u65b0\u9032\u5c55\u5f15\u8d77\u4e86\u5ee3\u6cdb\u95dc\u6ce8\uff0c\u56e0\u70ba\u5b83\u5011\u76f4\u63a5\u9810\u6e2c\u76ee\u6a19\u5c0d\u8c61\u7684\u5750\u6a19\uff0c\u800c\u7121\u9700\u4f9d\u8cf4\u984d\u5916\u7684\u52aa\u529b\uff0c\u4f8b\u5982\u9810\u5148\u751f\u6210\u7684\u63d0\u8b70\u5019\u9078\u6216\u9810\u5148\u5b9a\u7fa9\u7684\u9328\u6846\u3002\u7136\u800c\uff0c\u73fe\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u65bc\u8a2d\u8a08\u66f4\u5f37\u5927\u7684\u591a\u6a21\u5f0f\u89e3\u78bc\u5668\uff0c\u5b83\u901a\u5e38\u901a\u904e\u96a8\u6a5f\u521d\u59cb\u5316\u6216\u4f7f\u7528\u8a9e\u8a00\u5d4c\u5165\u4f86\u751f\u6210\u53ef\u5b78\u7fd2\u67e5\u8a62\u3002\u9019\u7a2e\u9999\u8349\u67e5\u8a62\u751f\u6210\u65b9\u6cd5\u4e0d\u53ef\u907f\u514d\u5730\u589e\u52a0\u4e86\u6a21\u578b\u7684\u5b78\u7fd2\u96e3\u5ea6\uff0c\u56e0\u70ba\u5b83\u5728\u89e3\u78bc\u958b\u59cb\u6642\u4e0d\u6d89\u53ca\u4efb\u4f55\u8207\u76ee\u6a19\u76f8\u95dc\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5b83\u5011\u5728\u67e5\u8a62\u5b78\u7fd2\u904e\u7a0b\u4e2d\u53ea\u4f7f\u7528\u6700\u6df1\u7684\u5716\u50cf\u7279\u5fb5\uff0c\u5ffd\u8996\u4e86\u5176\u4ed6\u7d1a\u5225\u7279\u5fb5\u7684\u91cd\u8981\u6027\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u7a31\u70ba RefFormer\u3002\u5b83\u7531\u67e5\u8a62\u9069\u61c9\u6a21\u584a\u7d44\u6210\uff0c\u8a72\u6a21\u584a\u53ef\u4ee5\u7121\u7e2b\u96c6\u6210\u5230 CLIP \u4e2d\u4e26\u751f\u6210\u53c3\u8003\u67e5\u8a62\uff0c\u70ba\u89e3\u78bc\u5668\u63d0\u4f9b\u5148\u9a57\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53ca\u7279\u5b9a\u65bc\u4efb\u52d9\u7684\u89e3\u78bc\u5668\u3002\u901a\u904e\u5c07\u53c3\u8003\u67e5\u8a62\u7d0d\u5165\u89e3\u78bc\u5668\uff0c\u6211\u5011\u53ef\u4ee5\u6709\u6548\u5730\u964d\u4f4e\u89e3\u78bc\u5668\u7684\u5b78\u7fd2\u96e3\u5ea6\uff0c\u4e26\u6e96\u78ba\u5730\u96c6\u4e2d\u5728\u76ee\u6a19\u5c0d\u8c61\u4e0a\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u7684\u67e5\u8a62\u9069\u61c9\u6a21\u584a\u9084\u53ef\u4ee5\u4f5c\u70ba\u9069\u914d\u5668\uff0c\u5728\u7121\u9700\u8abf\u6574\u4e3b\u5e79\u7db2\u7d61\u53c3\u6578\u7684\u60c5\u6cc1\u4e0b\u4fdd\u7559 CLIP \u4e2d\u7684\u8c50\u5bcc\u77e5\u8b58\u3002\u5927\u91cf\u7684\u5be6\u9a57\u8b49\u660e\u4e86\u6211\u5011\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u5728\u4e94\u500b\u8996\u89ba\u57fa\u790e\u57fa\u6e96\u4e0a\u512a\u65bc\u6700\u5148\u9032\u7684\u65b9\u6cd5\u3002", "author": "Yabing Wang et.al.", "authors": "Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang", "id": "2412.19155v1", "paper_url": "http://arxiv.org/abs/2412.19155v1", "repo": "null"}}