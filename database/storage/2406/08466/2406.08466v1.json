{"2406.08466": {"publish_time": "2024-06-12", "title": "Scaling Laws in Linear Regression: Compute, Parameters, and Data", "paper_summary": "Empirically, large-scale deep learning models often satisfy a neural scaling\nlaw: the test error of the trained model improves polynomially as the model\nsize and data size grow. However, conventional wisdom suggests the test error\nconsists of approximation, bias, and variance errors, where the variance error\nincreases with model size. This disagrees with the general form of neural\nscaling laws, which predict that increasing model size monotonically improves\nperformance.\n  We study the theory of scaling laws in an infinite dimensional linear\nregression setup. Specifically, we consider a model with $M$ parameters as a\nlinear function of sketched covariates. The model is trained by one-pass\nstochastic gradient descent (SGD) using $N$ data. Assuming the optimal\nparameter satisfies a Gaussian prior and the data covariance matrix has a\npower-law spectrum of degree $a>1$, we show that the reducible part of the test\nerror is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which\nincreases with $M$, is dominated by the other errors due to the implicit\nregularization of SGD, thus disappearing from the bound. Our theory is\nconsistent with the empirical neural scaling laws and verified by numerical\nsimulation.", "paper_summary_zh": "\u7ecf\u9a8c\u4e0a\uff0c\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u6ee1\u8db3\u795e\u7ecf\u7f51\u7edc\u7f29\u653e\u5b9a\u5f8b\uff1a\u968f\u7740\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u5927\u5c0f\u7684\u589e\u957f\uff0c\u8bad\u7ec3\u6a21\u578b\u7684\u6d4b\u8bd5\u8bef\u5dee\u4ee5\u591a\u9879\u5f0f\u65b9\u5f0f\u5f97\u5230\u6539\u5584\u3002\u7136\u800c\uff0c\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u6d4b\u8bd5\u8bef\u5dee\u7531\u8fd1\u4f3c\u8bef\u5dee\u3001\u504f\u5dee\u8bef\u5dee\u548c\u65b9\u5dee\u8bef\u5dee\u7ec4\u6210\uff0c\u5176\u4e2d\u65b9\u5dee\u8bef\u5dee\u968f\u7740\u6a21\u578b\u5927\u5c0f\u7684\u589e\u52a0\u800c\u589e\u52a0\u3002\u8fd9\u4e0e\u795e\u7ecf\u7f51\u7edc\u7f29\u653e\u5b9a\u5f8b\u7684\u4e00\u822c\u5f62\u5f0f\u76f8\u77db\u76fe\uff0c\u540e\u8005\u9884\u6d4b\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u4f1a\u5355\u8c03\u5730\u63d0\u9ad8\u6027\u80fd\u3002\u6211\u4eec\u7814\u7a76\u4e86\u65e0\u9650\u7ef4\u7ebf\u6027\u56de\u5f52\u8bbe\u7f6e\u4e2d\u7684\u7f29\u653e\u5b9a\u5f8b\u7406\u8bba\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8003\u8651\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u5176\u4e2d $M$ \u4e2a\u53c2\u6570\u4f5c\u4e3a\u7d20\u63cf\u534f\u53d8\u91cf\u7684\u7ebf\u6027\u51fd\u6570\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u4f7f\u7528 $N$ \u4e2a\u6570\u636e\u7684\u4e00\u904d\u968f\u673a\u68af\u5ea6\u4e0b\u964d (SGD) \u8fdb\u884c\u8bad\u7ec3\u3002\u5047\u8bbe\u6700\u4f18\u53c2\u6570\u6ee1\u8db3\u9ad8\u65af\u5148\u9a8c\uff0c\u5e76\u4e14\u6570\u636e\u534f\u65b9\u5dee\u77e9\u9635\u5177\u6709 $a>1$ \u9636\u7684\u5e42\u5f8b\u8c31\uff0c\u6211\u4eec\u8868\u660e\u6d4b\u8bd5\u8bef\u5dee\u7684\u53ef\u7ea6\u90e8\u5206\u4e3a $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$\u3002\u65b9\u5dee\u8bef\u5dee\u968f\u7740 $M$ \u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u7531\u4e8e SGD \u7684\u9690\u5f0f\u6b63\u5219\u5316\uff0c\u5b83\u88ab\u5176\u4ed6\u8bef\u5dee\u6240\u652f\u914d\uff0c\u56e0\u6b64\u4ece\u754c\u9650\u4e2d\u6d88\u5931\u3002\u6211\u4eec\u7684\u7406\u8bba\u4e0e\u7ecf\u9a8c\u795e\u7ecf\u7f51\u7edc\u7f29\u653e\u5b9a\u5f8b\u4e00\u81f4\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u5f97\u5230\u9a8c\u8bc1\u3002", "author": "Licong Lin et.al.", "authors": "Licong Lin, Jingfeng Wu, Sham M. Kakade, Peter L. Bartlett, Jason D. Lee", "id": "2406.08466v1", "paper_url": "http://arxiv.org/abs/2406.08466v1", "repo": "null"}}