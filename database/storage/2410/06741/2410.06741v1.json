{"2410.06741": {"publish_time": "2024-10-09", "title": "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models", "paper_summary": "Multi-task learning (MTL) benefits the fine-tuning of large language models\n(LLMs) by providing a single model with improved performance and generalization\nability across tasks, presenting a resource-efficient alternative to developing\nseparate models for each task. Yet, existing MTL strategies for LLMs often fall\nshort by either being computationally intensive or failing to ensure\nsimultaneous task convergence. This paper presents CoBa, a new MTL approach\ndesigned to effectively manage task convergence balance with minimal\ncomputational overhead. Utilizing Relative Convergence Scores (RCS), Absolute\nConvergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically\nadjusts task weights during the training process, ensuring that the validation\nloss of all tasks progress towards convergence at an even pace while mitigating\nthe issue of individual task divergence. The results of our experiments\ninvolving three disparate datasets underscore that this approach not only\nfosters equilibrium in task improvement but enhances the LLMs' performance by\nup to 13% relative to the second-best baselines. Code is open-sourced at\nhttps://github.com/codefuse-ai/MFTCoder.", "paper_summary_zh": "\u591a\u4efb\u52d9\u5b78\u7fd2 (MTL) \u900f\u904e\u63d0\u4f9b\u55ae\u4e00\u6a21\u578b\uff0c\u9032\u800c\u6539\u5584\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5fae\u8abf\uff0c\u63d0\u5347\u4efb\u52d9\u9593\u7684\u6548\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u4f9b\u958b\u767c\u6bcf\u9805\u4efb\u52d9\u7684\u7368\u7acb\u6a21\u578b\u7684\u7701\u8cc7\u6e90\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u73fe\u6709\u7684 LLM MTL \u7b56\u7565\u901a\u5e38\u6703\u56e0\u904b\u7b97\u5bc6\u96c6\u6216\u7121\u6cd5\u78ba\u4fdd\u4efb\u52d9\u540c\u6642\u6536\u6582\u800c\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa CoBa\uff0c\u4e00\u7a2e\u65b0\u7684 MTL \u65b9\u6cd5\uff0c\u65e8\u5728\u6709\u6548\u7ba1\u7406\u4efb\u52d9\u6536\u6582\u5e73\u8861\uff0c\u4e14\u904b\u7b97\u958b\u92b7\u6700\u5c0f\u3002CoBa \u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u52d5\u614b\u8abf\u6574\u4efb\u52d9\u6b0a\u91cd\uff0c\u5229\u7528\u76f8\u5c0d\u6536\u6582\u5206\u6578 (RCS)\u3001\u7d55\u5c0d\u6536\u6582\u5206\u6578 (ACS) \u548c\u5206\u6b67\u56e0\u5b50 (DF)\uff0c\u78ba\u4fdd\u6240\u6709\u4efb\u52d9\u7684\u9a57\u8b49\u640d\u5931\u4ee5\u5747\u7b49\u7684\u6b65\u8abf\u671d\u5411\u6536\u6582\u9032\u884c\uff0c\u540c\u6642\u6e1b\u8f15\u500b\u5225\u4efb\u52d9\u5206\u6b67\u7684\u554f\u984c\u3002\u6211\u5011\u5728\u4e09\u500b\u4e0d\u540c\u7684\u8cc7\u6599\u96c6\u9032\u884c\u5be6\u9a57\u7684\u7d50\u679c\u5f37\u8abf\uff0c\u9019\u7a2e\u65b9\u6cd5\u4e0d\u50c5\u4fc3\u9032\u4efb\u52d9\u6539\u9032\u7684\u5e73\u8861\uff0c\u66f4\u5c07 LLM \u7684\u6548\u80fd\u63d0\u5347\u591a\u9054 13%\uff0c\u76f8\u8f03\u65bc\u7b2c\u4e8c\u4f73\u57fa\u6e96\u3002\u7a0b\u5f0f\u78bc\u5df2\u65bc https://github.com/codefuse-ai/MFTCoder \u958b\u6e90\u3002", "author": "Zi Gong et.al.", "authors": "Zi Gong, Hang Yu, Cong Liao, Bingchang Liu, Chaoyu Chen, Jianguo Li", "id": "2410.06741v1", "paper_url": "http://arxiv.org/abs/2410.06741v1", "repo": "https://github.com/codefuse-ai/mftcoder"}}