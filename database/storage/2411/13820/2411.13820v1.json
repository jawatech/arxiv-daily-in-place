{"2411.13820": {"publish_time": "2024-11-21", "title": "InstCache: A Predictive Cache for LLM Serving", "paper_summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u6b63\u5728\u6539\u8b8a\u4eba\u985e\u751f\u6d3b\u7684\u5404\u500b\u65b9\u9762\u3002\n\u7136\u800c\uff0c\u9019\u7a2e\u524d\u6240\u672a\u6709\u7684\u80fd\u529b\u662f\u4ee5\u5de8\u5927\u7684\u904b\u7b97\u5f37\u5ea6\u70ba\u4ee3\u50f9\u7684\uff0c\u9019\u610f\u5473\u8457\u5ef6\u9072\u6642\u9593\u9577\u548c\u80fd\u6e90\u6d88\u8017\u5927\u3002\u95dc\u9375\u503c\u5feb\u53d6\u548c\u8a9e\u7fa9\u5feb\u53d6\u5df2\u88ab\u63d0\u51fa\u4f5c\u70ba\u4e0a\u8ff0\u554f\u984c\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4f46\u7531\u65bc\u6bcf\u500b\u7b26\u865f\u6216\u6307\u4ee4\u5d4c\u5165\u7684\u8a18\u61b6\u9ad4\u6210\u672c\u5de8\u5927\uff0c\u9019\u5169\u7a2e\u65b9\u6cd5\u90fd\u53d7\u5230\u53ef\u64f4\u5145\u6027\u6709\u9650\u7684\u56f0\u64fe\u3002\u7531\u65bc\u5927\u591a\u6578\u6307\u4ee4\u90fd\u662f\u7c21\u77ed\u3001\u91cd\u8907\u4e14\u53ef\u7531 LLM \u9810\u6e2c\u7684\uff0c\u56e0\u6b64\u6211\u5011\u63d0\u51fa\u901a\u904e\u6307\u4ee4\u5c0d\u9f4a\u7684 LLM \u9810\u6e2c\u4f7f\u7528\u8005\u6307\u4ee4\uff0c\u4e26\u5c07\u5b83\u5011\u5132\u5b58\u5728\u4e00\u500b\u9810\u6e2c\u5feb\u53d6\u4e2d\uff0c\u7a31\u70ba InstCache\u3002\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u57fa\u65bc\u6307\u4ee4\u8ca0\u5c0d\u6578\u4f3c\u7136\u7684\u6307\u4ee4\u9810\u586b\u5145\u6f14\u7b97\u6cd5\uff0c\u6839\u64da\u547d\u4e2d\u7387\u78ba\u5b9a\u5feb\u53d6\u5927\u5c0f\u3002\u5efa\u8b70\u7684 InstCache \u4ee5\u96dc\u6e4a\u8868\u7684\u5f62\u5f0f\u6709\u6548\u5be6\u4f5c\uff0c\u5177\u6709\u6700\u5c0f\u7684\u67e5\u8a62\u5ef6\u9072\uff0c\u4ee5\u4fbf\u90e8\u7f72\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0cInstCache \u53ef\u4ee5\u5c0d LMSys \u8cc7\u6599\u96c6\u5be6\u73fe\u9ad8\u9054 51.34% \u7684\u547d\u4e2d\u7387\uff0c\u9019\u76f8\u7576\u65bc\u901f\u5ea6\u63d0\u5347 2 \u500d\uff0c\u800c\u8a18\u61b6\u9ad4\u6210\u672c\u50c5\u70ba 4.5GB\u3002", "author": "Longwei Zou et.al.", "authors": "Longwei Zou, Tingfeng Liu, Kai Chen, Jiangang Kong, Yangdong Deng", "id": "2411.13820v1", "paper_url": "http://arxiv.org/abs/2411.13820v1", "repo": "null"}}