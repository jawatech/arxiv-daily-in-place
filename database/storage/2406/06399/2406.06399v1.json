{"2406.06399": {"publish_time": "2024-06-10", "title": "Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue", "paper_summary": "We study the limitations of Large Language Models (LLMs) for the task of\nresponse generation in human-machine dialogue. Several techniques have been\nproposed in the literature for different dialogue types (e.g., Open-Domain).\nHowever, the evaluations of these techniques have been limited in terms of base\nLLMs, dialogue types and evaluation metrics. In this work, we extensively\nanalyze different LLM adaptation techniques when applied to different dialogue\ntypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue\ntypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.\nWe evaluate the performance of in-context learning and fine-tuning techniques\nacross datasets selected for each dialogue type. We assess the impact of\nincorporating external knowledge to ground the generation in both scenarios of\nRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent\nevaluation and explainability criteria for automatic metrics and human\nevaluation protocols. Our analysis shows that there is no universal\nbest-technique for adapting large language models as the efficacy of each\ntechnique depends on both the base LLM and the specific type of dialogue. Last\nbut not least, the assessment of the best adaptation technique should include\nhuman evaluation to avoid false expectations and outcomes derived from\nautomatic metrics.", "paper_summary_zh": "\u6211\u5011\u63a2\u8a0e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u4eba\u6a5f\u5c0d\u8a71\u4e2d\u56de\u61c9\u751f\u6210\u4efb\u52d9\u7684\u9650\u5236\u3002\u91dd\u5c0d\u4e0d\u540c\u7684\u5c0d\u8a71\u985e\u578b\uff08\u4f8b\u5982\uff0c\u958b\u653e\u9818\u57df\uff09\uff0c\u6587\u737b\u4e2d\u5df2\u63d0\u51fa\u591a\u7a2e\u6280\u8853\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6280\u8853\u7684\u8a55\u4f30\u5728\u57fa\u672c LLM\u3001\u5c0d\u8a71\u985e\u578b\u548c\u8a55\u4f30\u6307\u6a19\u65b9\u9762\u53d7\u5230\u9650\u5236\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5ee3\u6cdb\u5206\u6790\u4e86\u61c9\u7528\u65bc\u4e0d\u540c\u5c0d\u8a71\u985e\u578b\u7684\u4e0d\u540c LLM \u9069\u61c9\u6280\u8853\u3002\u6211\u5011\u9078\u64c7\u4e86\u5169\u500b\u57fa\u672c LLM\uff0cLlama-2 \u548c Mistral\uff0c\u4ee5\u53ca\u56db\u7a2e\u985e\u578b\u7684\u5c0d\u8a71\uff1a\u958b\u653e\u9818\u57df\u3001\u77e5\u8b58\u57fa\u790e\u3001\u4efb\u52d9\u5c0e\u5411\u548c\u554f\u7b54\u3002\u6211\u5011\u8a55\u4f30\u4e86\u91dd\u5c0d\u6bcf\u500b\u5c0d\u8a71\u985e\u578b\u6240\u9078\u8cc7\u6599\u96c6\u7684\u8a9e\u5883\u5b78\u7fd2\u548c\u5fae\u8abf\u6280\u8853\u7684\u6548\u80fd\u3002\u6211\u5011\u8a55\u4f30\u4e86\u5728\u6aa2\u7d22\u5f37\u5316\u751f\u6210 (RAG) \u548c\u9ec3\u91d1\u77e5\u8b58\u9019\u5169\u7a2e\u60c5\u6cc1\u4e0b\uff0c\u52a0\u5165\u5916\u90e8\u77e5\u8b58\u4ee5\u5960\u5b9a\u751f\u6210\u7684\u57fa\u790e\u7684\u5f71\u97ff\u3002\u6211\u5011\u63a1\u7528\u4e00\u81f4\u7684\u8a55\u4f30\u548c\u53ef\u89e3\u91cb\u6027\u6a19\u6e96\uff0c\u7528\u65bc\u81ea\u52d5\u6307\u6a19\u548c\u4eba\u985e\u8a55\u4f30\u5354\u5b9a\u3002\u6211\u5011\u7684\u5206\u6790\u986f\u793a\uff0c\u6c92\u6709\u901a\u7528\u7684\u6700\u4f73\u6280\u8853\u4f86\u9069\u61c9\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u56e0\u70ba\u6bcf\u7a2e\u6280\u8853\u7684\u6548\u80fd\u53d6\u6c7a\u65bc\u57fa\u672c LLM \u548c\u7279\u5b9a\u985e\u578b\u7684\u5c0d\u8a71\u3002\u6700\u5f8c\u4f46\u4e26\u975e\u6700\u4e0d\u91cd\u8981\u7684\u4e00\u9ede\u662f\uff0c\u6700\u4f73\u9069\u61c9\u6280\u8853\u7684\u8a55\u4f30\u61c9\u5305\u62ec\u4eba\u985e\u8a55\u4f30\uff0c\u4ee5\u907f\u514d\u4f86\u81ea\u81ea\u52d5\u6307\u6a19\u7684\u932f\u8aa4\u671f\u671b\u548c\u7d50\u679c\u3002", "author": "Simone Alghisi et.al.", "authors": "Simone Alghisi, Massimo Rizzoli, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi", "id": "2406.06399v1", "paper_url": "http://arxiv.org/abs/2406.06399v1", "repo": "null"}}