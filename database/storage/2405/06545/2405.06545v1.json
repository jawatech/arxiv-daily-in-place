{"2405.06545": {"publish_time": "2024-05-10", "title": "Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval", "paper_summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, although their susceptibility to hallucination poses\nsignificant challenges for their deployment in critical areas such as\nhealthcare. To address this issue, retrieving relevant facts from knowledge\ngraphs (KGs) is considered a promising method. Existing KG-augmented approaches\ntend to be resource-intensive, requiring multiple rounds of retrieval and\nverification for each factoid, which impedes their application in real-world\nscenarios.\n  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval\n(Re-KGR) to augment the factuality of LLMs' responses with less retrieval\nefforts in the medical field. Our approach leverages the attribution of\nnext-token predictive probability distributions across different tokens, and\nvarious model layers to primarily identify tokens with a high potential for\nhallucination, reducing verification rounds by refining knowledge triples\nassociated with these tokens. Moreover, we rectify inaccurate content using\nretrieved knowledge in the post-processing stage, which improves the\ntruthfulness of generated responses. Experimental results on a medical dataset\ndemonstrate that our approach can enhance the factual capability of LLMs across\nvarious foundational models as evidenced by the highest scores on truthfulness.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u500b\u9818\u57df\u5c55\u73fe\u51fa\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u5118\u7ba1\u5b83\u5011\u5bb9\u6613\u51fa\u73fe\u5e7b\u89ba\uff0c\u5c0d\u5176\u5728\u91ab\u7642\u4fdd\u5065\u7b49\u95dc\u9375\u9818\u57df\u7684\u90e8\u7f72\u69cb\u6210\u91cd\u5927\u6311\u6230\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u5f9e\u77e5\u8b58\u5716\u8b5c (KG) \u4e2d\u64f7\u53d6\u76f8\u95dc\u4e8b\u5be6\u88ab\u8a8d\u70ba\u662f\u4e00\u7a2e\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002\u73fe\u6709\u7684 KG \u589e\u5f37\u65b9\u6cd5\u5f80\u5f80\u9700\u8981\u5927\u91cf\u8cc7\u6e90\uff0c\u9700\u8981\u5c0d\u6bcf\u500b\u4e8b\u5be6\u9032\u884c\u591a\u8f2a\u6aa2\u7d22\u548c\u9a57\u8b49\uff0c\u9019\u963b\u7919\u4e86\u5b83\u5011\u5728\u5be6\u969b\u5834\u666f\u4e2d\u7684\u61c9\u7528\u3002\n\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u81ea\u4fee\u6b63\u589e\u5f37\u77e5\u8b58\u5716\u8b5c\u6aa2\u7d22 (Re-KGR)\uff0c\u4ee5\u6e1b\u5c11\u91ab\u7642\u9818\u57df\u4e2d LLM \u56de\u61c9\u7684\u4e8b\u5be6\u6027\u6aa2\u7d22\u5de5\u4f5c\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528\u4e86\u4e0d\u540c\u6a19\u8a18\u4e4b\u9593\u4e0b\u4e00\u500b\u6a19\u8a18\u9810\u6e2c\u6982\u7387\u5206\u4f48\u7684\u6b78\u56e0\uff0c\u4ee5\u53ca\u5404\u7a2e\u6a21\u578b\u5c64\uff0c\u4ee5\u4e3b\u8981\u8b58\u5225\u5177\u6709\u9ad8\u5e7b\u89ba\u6f5b\u529b\u7684\u6a19\u8a18\uff0c\u901a\u904e\u4fee\u6b63\u8207\u9019\u4e9b\u6a19\u8a18\u76f8\u95dc\u7684\u77e5\u8b58\u4e09\u5143\u7d44\u4f86\u6e1b\u5c11\u9a57\u8b49\u8f2a\u6b21\u3002\u6b64\u5916\uff0c\u6211\u5011\u5728\u5f8c\u8655\u7406\u968e\u6bb5\u4f7f\u7528\u6aa2\u7d22\u5230\u7684\u77e5\u8b58\u4f86\u4fee\u6b63\u4e0d\u6e96\u78ba\u7684\u5167\u5bb9\uff0c\u9019\u63d0\u9ad8\u4e86\u751f\u6210\u56de\u61c9\u7684\u771f\u5be6\u6027\u3002\u5728\u91ab\u7642\u6578\u64da\u96c6\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u53ef\u4ee5\u589e\u5f37 LLM \u5728\u5404\u7a2e\u57fa\u790e\u6a21\u578b\u4e0a\u7684\u4e8b\u5be6\u80fd\u529b\uff0c\u6700\u9ad8\u771f\u5be6\u6027\u5206\u6578\u8b49\u660e\u4e86\u9019\u4e00\u9ede\u3002", "author": "Mengjia Niu et.al.", "authors": "Mengjia Niu, Hao Li, Jie Shi, Hamed Haddadi, Fan Mo", "id": "2405.06545v1", "paper_url": "http://arxiv.org/abs/2405.06545v1", "repo": "null"}}