{"2501.14210": {"publish_time": "2025-01-24", "title": "PuzzleGPT: Emulating Human Puzzle-Solving Ability for Time and Location Prediction", "paper_summary": "The task of predicting time and location from images is challenging and\nrequires complex human-like puzzle-solving ability over different clues. In\nthis work, we formalize this ability into core skills and implement them using\ndifferent modules in an expert pipeline called PuzzleGPT. PuzzleGPT consists of\na perceiver to identify visual clues, a reasoner to deduce prediction\ncandidates, a combiner to combinatorially combine information from different\nclues, a web retriever to get external knowledge if the task can't be solved\nlocally, and a noise filter for robustness. This results in a zero-shot,\ninterpretable, and robust approach that records state-of-the-art performance on\ntwo datasets -- TARA and WikiTilo. PuzzleGPT outperforms large VLMs such as\nBLIP-2, InstructBLIP, LLaVA, and even GPT-4V, as well as automatically\ngenerated reasoning pipelines like VisProg, by at least 32% and 38%,\nrespectively. It even rivals or surpasses finetuned models.", "paper_summary_zh": "\u5f9e\u5f71\u50cf\u9810\u6e2c\u6642\u9593\u548c\u4f4d\u7f6e\u7684\u4efb\u52d9\u5177\u6709\u6311\u6230\u6027\uff0c\u9700\u8981\u5177\u5099\u4eba\u985e\u822c\u7684\u8907\u96dc\u89e3\u8b0e\u80fd\u529b\uff0c\u624d\u80fd\u5f9e\u4e0d\u540c\u7684\u7dda\u7d22\u4e2d\u89e3\u8b0e\u3002\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5c07\u9019\u7a2e\u80fd\u529b\u5f62\u5f0f\u5316\u70ba\u6838\u5fc3\u6280\u80fd\uff0c\u4e26\u4f7f\u7528\u5c08\u5bb6\u7ba1\u9053 PuzzleGPT \u4e2d\u7684\u4e0d\u540c\u6a21\u7d44\u4f86\u5be6\u4f5c\u9019\u4e9b\u6280\u80fd\u3002PuzzleGPT \u5305\u542b\u4e00\u500b\u611f\u77e5\u5668\uff0c\u7528\u65bc\u8b58\u5225\u8996\u89ba\u7dda\u7d22\uff1b\u4e00\u500b\u63a8\u7406\u5668\uff0c\u7528\u65bc\u63a8\u8ad6\u9810\u6e2c\u5019\u9078\u9805\uff1b\u4e00\u500b\u7d44\u5408\u5668\uff0c\u7528\u65bc\u7d44\u5408\u4f86\u81ea\u4e0d\u540c\u7dda\u7d22\u7684\u8cc7\u8a0a\uff1b\u4e00\u500b\u7db2\u8def\u6aa2\u7d22\u5668\uff0c\u7528\u65bc\u5728\u7121\u6cd5\u5728\u672c\u5730\u89e3\u6c7a\u4efb\u52d9\u6642\u53d6\u5f97\u5916\u90e8\u77e5\u8b58\uff1b\u4ee5\u53ca\u4e00\u500b\u96dc\u8a0a\u6ffe\u6ce2\u5668\uff0c\u7528\u65bc\u589e\u5f37\u7a69\u5065\u6027\u3002\u9019\u7522\u751f\u4e86\u4e00\u7a2e\u96f6\u6b21\u5b78\u7fd2\u3001\u53ef\u89e3\u91cb\u4e14\u7a69\u5065\u7684\u65b9\u6cd5\uff0c\u5728\u5169\u500b\u8cc7\u6599\u96c6\uff08TARA \u548c WikiTilo\uff09\u4e0a\u5275\u4e0b\u6700\u5148\u9032\u7684\u6548\u80fd\u3002PuzzleGPT \u7684\u8868\u73fe\u512a\u65bc\u5927\u578b VLM\uff0c\u4f8b\u5982 BLIP-2\u3001InstructBLIP\u3001LLaVA\uff0c\u751a\u81f3 GPT-4V\uff0c\u4ee5\u53ca\u81ea\u52d5\u7522\u751f\u7684\u63a8\u7406\u7ba1\u9053\uff08\u4f8b\u5982 VisProg\uff09\uff0c\u5206\u5225\u81f3\u5c11\u9ad8\u51fa 32% \u548c 38%\u3002\u5b83\u751a\u81f3\u8207\u5fae\u8abf\u6a21\u578b\u76f8\u6297\u8861\u6216\u8d85\u8d8a\u5fae\u8abf\u6a21\u578b\u3002", "author": "Hammad Ayyubi et.al.", "authors": "Hammad Ayyubi, Xuande Feng, Junzhang Liu, Xudong Lin, Zhecan Wang, Shih-Fu Chang", "id": "2501.14210v1", "paper_url": "http://arxiv.org/abs/2501.14210v1", "repo": "null"}}