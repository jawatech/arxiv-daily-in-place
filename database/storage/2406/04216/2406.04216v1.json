{"2406.04216": {"publish_time": "2024-06-06", "title": "What Do Language Models Learn in Context? The Structured Task Hypothesis", "paper_summary": "Large language models (LLMs) exhibit an intriguing ability to learn a novel\ntask from in-context examples presented in a demonstration, termed in-context\nlearning (ICL). Understandably, a swath of research has been dedicated to\nuncovering the theories underpinning ICL. One popular hypothesis explains ICL\nby task selection. LLMs identify the task based on the demonstration and\ngeneralize it to the prompt. Another popular hypothesis is that ICL is a form\nof meta-learning, i.e., the models learn a learning algorithm at pre-training\ntime and apply it to the demonstration. Finally, a third hypothesis argues that\nLLMs use the demonstration to select a composition of tasks learned during\npre-training to perform ICL. In this paper, we empirically explore these three\nhypotheses that explain LLMs' ability to learn in context with a suite of\nexperiments derived from common text classification tasks. We invalidate the\nfirst two hypotheses with counterexamples and provide evidence in support of\nthe last hypothesis. Our results suggest an LLM could learn a novel task in\ncontext via composing tasks learned during pre-training.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5c55\u73fe\u51fa\u5f9e\u793a\u7bc4\u4e2d\u4ee5\u8108\u7d61\u4e2d\u7684\u7bc4\u4f8b\u5b78\u7fd2\u65b0\u4efb\u52d9\u7684\u6709\u8da3\u80fd\u529b\uff0c\u7a31\u70ba\u8108\u7d61\u4e2d\u5b78\u7fd2 (ICL)\u3002\u53ef\u4ee5\u7406\u89e3\u7684\u662f\uff0c\u5927\u91cf\u7684\u7814\u7a76\u81f4\u529b\u65bc\u63ed\u793a\u652f\u6490 ICL \u7684\u7406\u8ad6\u3002\u4e00\u500b\u6d41\u884c\u7684\u5047\u8a2d\u901a\u904e\u4efb\u52d9\u9078\u64c7\u4f86\u89e3\u91cb ICL\u3002LLM \u6839\u64da\u793a\u7bc4\u8b58\u5225\u4efb\u52d9\u4e26\u5c07\u5176\u6982\u62ec\u70ba\u63d0\u793a\u3002\u53e6\u4e00\u500b\u6d41\u884c\u7684\u5047\u8a2d\u662f ICL \u662f\u5143\u5b78\u7fd2\u7684\u4e00\u7a2e\u5f62\u5f0f\uff0c\u5373\u6a21\u578b\u5728\u9810\u8a13\u7df4\u6642\u5b78\u7fd2\u5b78\u7fd2\u6f14\u7b97\u6cd5\u4e26\u5c07\u5176\u61c9\u7528\u65bc\u793a\u7bc4\u3002\u6700\u5f8c\uff0c\u7b2c\u4e09\u500b\u5047\u8a2d\u8a8d\u70ba LLM \u4f7f\u7528\u793a\u7bc4\u4f86\u9078\u64c7\u5728\u9810\u8a13\u7df4\u671f\u9593\u5b78\u7fd2\u7684\u4efb\u52d9\u7d44\u5408\u4ee5\u57f7\u884c ICL\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u6839\u64da\u5e38\u898b\u6587\u5b57\u5206\u985e\u4efb\u52d9\u884d\u751f\u7684\u4e00\u7cfb\u5217\u5be6\u9a57\uff0c\u5be6\u8b49\u63a2\u8a0e\u9019\u4e09\u500b\u89e3\u91cb LLM \u5728\u8108\u7d61\u4e2d\u5b78\u7fd2\u80fd\u529b\u7684\u5047\u8a2d\u3002\u6211\u5011\u4f7f\u7528\u53cd\u4f8b\u63a8\u7ffb\u524d\u5169\u500b\u5047\u8a2d\uff0c\u4e26\u63d0\u4f9b\u8b49\u64da\u652f\u6301\u6700\u5f8c\u4e00\u500b\u5047\u8a2d\u3002\u6211\u5011\u7684\u7d50\u679c\u8868\u660e\uff0cLLM \u53ef\u4ee5\u900f\u904e\u7d44\u5408\u9810\u8a13\u7df4\u671f\u9593\u5b78\u7fd2\u7684\u4efb\u52d9\uff0c\u5728\u8108\u7d61\u4e2d\u5b78\u7fd2\u65b0\u7684\u4efb\u52d9\u3002", "author": "Jiaoda Li et.al.", "authors": "Jiaoda Li, Yifan Hou, Mrinmaya Sachan, Ryan Cotterell", "id": "2406.04216v1", "paper_url": "http://arxiv.org/abs/2406.04216v1", "repo": "null"}}