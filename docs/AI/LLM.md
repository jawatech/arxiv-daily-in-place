
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-08-19**|**Demystifying the Communication Characteristics for Distributed Transformer Models**|Quentin Anthony et.al.|[2408.10197v1](http://arxiv.org/abs/2408.10197v1)|null|
|**2024-08-19**|**SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views**|Chao Xu et.al.|[2408.10195v1](http://arxiv.org/abs/2408.10195v1)|null|
|**2024-08-19**|**Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models**|Aviv Bick et.al.|[2408.10189v1](http://arxiv.org/abs/2408.10189v1)|null|
|**2024-08-19**|**LongVILA: Scaling Long-Context Visual Language Models for Long Videos**|Fuzhao Xue et.al.|[2408.10188v1](http://arxiv.org/abs/2408.10188v1)|[link](https://github.com/nvlabs/vila)|
|**2024-08-19**|**Imbalance-Aware Culvert-Sewer Defect Segmentation Using an Enhanced Feature Pyramid Network**|Rasha Alshawi et.al.|[2408.10181v1](http://arxiv.org/abs/2408.10181v1)|null|
|**2024-08-19**|**Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias in Facial Recognition**|Rafael M. Mamede et.al.|[2408.10175v1](http://arxiv.org/abs/2408.10175v1)|null|
|**2024-08-19**|**SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models**|Anke Tang et.al.|[2408.10174v1](http://arxiv.org/abs/2408.10174v1)|[link](https://github.com/tanganke/fusion_bench)|
|**2024-08-19**|**Customizing Language Models with Instance-wise LoRA for Sequential Recommendation**|Xiaoyu Kong et.al.|[2408.10159v1](http://arxiv.org/abs/2408.10159v1)|null|
|**2024-08-19**|**Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models**|Amey Hengle et.al.|[2408.10151v1](http://arxiv.org/abs/2408.10151v1)|null|
|**2024-08-19**|**In-Context Learning with Representations: Contextual Generalization of Trained Transformers**|Tong Yang et.al.|[2408.10147v1](http://arxiv.org/abs/2408.10147v1)|null|
|**2024-08-19**|**Instruction Finetuning for Leaderboard Generation from Empirical AI Research**|Salomon Kabongo et.al.|[2408.10141v1](http://arxiv.org/abs/2408.10141v1)|null|
|**2024-08-19**|**Rhyme-aware Chinese lyric generator based on GPT**|Yixiao Yuan et.al.|[2408.10130v1](http://arxiv.org/abs/2408.10130v1)|null|
|**2024-08-19**|**Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a Low-Resource Language**|Manjil Karki et.al.|[2408.10128v1](http://arxiv.org/abs/2408.10128v1)|null|
|**2024-08-19**|**Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**|Tianyu Zhang et.al.|[2408.10124v1](http://arxiv.org/abs/2408.10124v1)|[link](https://github.com/zhangtia16/molgraph-lardo)|
|**2024-08-19**|**Geometry Informed Tokenization of Molecules for Language Model Generation**|Xiner Li et.al.|[2408.10120v1](http://arxiv.org/abs/2408.10120v1)|null|
|**2024-08-19**|**Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data**|Tao Yang et.al.|[2408.10119v1](http://arxiv.org/abs/2408.10119v1)|null|
|**2024-08-19**|**GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization**|Ran Liu et.al.|[2408.10115v1](http://arxiv.org/abs/2408.10115v1)|[link](https://github.com/oswald1997/glimmer)|
|**2024-08-19**|**PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities**|Yuanjian Xu et.al.|[2408.10111v2](http://arxiv.org/abs/2408.10111v2)|null|
|**2024-08-19**|**Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments**|Heeyoung Lee et.al.|[2408.10107v1](http://arxiv.org/abs/2408.10107v1)|[link](https://github.com/hy18284/mixdiff)|
|**2024-08-19**|**Convert and Speak: Zero-shot Accent Conversion with Minimum Supervision**|Zhijun Jia et.al.|[2408.10096v1](http://arxiv.org/abs/2408.10096v1)|null|
|**2024-08-19**|**ARMADA: Attribute-Based Multimodal Data Augmentation**|Xiaomeng Jin et.al.|[2408.10086v1](http://arxiv.org/abs/2408.10086v1)|null|
|**2024-08-19**|**Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning**|Sriyash Poddar et.al.|[2408.10075v1](http://arxiv.org/abs/2408.10075v1)|null|
|**2024-08-19**|**Synthesis of Reward Machines for Multi-Agent Equilibrium Design (Full Version)**|Muhammad Najib et.al.|[2408.10074v1](http://arxiv.org/abs/2408.10074v1)|null|
|**2024-08-19**|**FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant**|Zhengchao Huang et.al.|[2408.10072v1](http://arxiv.org/abs/2408.10072v1)|null|
|**2024-08-19**|**Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with Texture Map-Based Weak Supervision**|Junho Moon et.al.|[2408.10060v1](http://arxiv.org/abs/2408.10060v1)|null|
|**2024-08-19**|**Privacy Checklist: Privacy Violation Detection Grounding on Contextual Integrity Theory**|Haoran Li et.al.|[2408.10053v1](http://arxiv.org/abs/2408.10053v1)|null|
|**2024-08-19**|**MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis**|Ruihui Hou et.al.|[2408.10039v1](http://arxiv.org/abs/2408.10039v1)|null|
|**2024-08-19**|**Towards a Knowledge Graph for Models and Algorithms in Applied Mathematics**|Björn Schembera et.al.|[2408.10003v1](http://arxiv.org/abs/2408.10003v1)|null|
|**2024-08-19**|**Edge-Cloud Collaborative Motion Planning for Autonomous Driving with Large Language Models**|Jiao Chen et.al.|[2408.09972v1](http://arxiv.org/abs/2408.09972v1)|null|
|**2024-08-19**|**Unsupervised Machine Learning Hybrid Approach Integrating Linear Programming in Loss Function: A Robust Optimization Technique**|Andrew Kiruluta et.al.|[2408.09967v1](http://arxiv.org/abs/2408.09967v1)|null|
|**2024-08-19**|**Contextual Importance and Utility in Python: New Functionality and Insights with the py-ciu Package**|Kary Främling et.al.|[2408.09957v1](http://arxiv.org/abs/2408.09957v1)|[link](https://github.com/karyframling/py-ciu)|
|**2024-08-19**|**Weakly Supervised Pretraining and Multi-Annotator Supervised Finetuning for Facial Wrinkle Detection**|Ik Jun Moon et.al.|[2408.09952v1](http://arxiv.org/abs/2408.09952v1)|null|
|**2024-08-19**|**Principle Driven Parameterized Fiber Model based on GPT-PINN Neural Network**|Yubin Zang et.al.|[2408.09951v1](http://arxiv.org/abs/2408.09951v1)|null|
|**2024-08-19**|**C${^2}$RL: Content and Context Representation Learning for Gloss-free Sign Language Translation and Retrieval**|Zhigang Chen et.al.|[2408.09949v1](http://arxiv.org/abs/2408.09949v1)|null|
|**2024-08-19**|**Caption-Driven Explorations: Aligning Image and Text Embeddings through Human-Inspired Foveated Vision**|Dario Zanca et.al.|[2408.09948v1](http://arxiv.org/abs/2408.09948v1)|null|
|**2024-08-19**|**Fiber Transmission Model with Parameterized Inputs based on GPT-PINN Neural Network**|Yubin Zang et.al.|[2408.09947v1](http://arxiv.org/abs/2408.09947v1)|null|
|**2024-08-19**|**Microscopic Analysis on LLM players via Social Deduction Game**|Byungjun Kim et.al.|[2408.09946v1](http://arxiv.org/abs/2408.09946v1)|null|
|**2024-08-19**|**Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating Adequacy, Fluency, and Elegance**|Andong Chen et.al.|[2408.09945v1](http://arxiv.org/abs/2408.09945v1)|null|
|**2024-08-19**|**SZU-AFS Antispoofing System for the ASVspoof 5 Challenge**|Yuxiong Xu et.al.|[2408.09933v1](http://arxiv.org/abs/2408.09933v1)|null|
|**2024-08-19**|**Attribution Analysis Meets Model Editing: Advancing Knowledge Correction in Vision Language Models with VisEdit**|Qizhou Chen et.al.|[2408.09916v1](http://arxiv.org/abs/2408.09916v1)|null|
|**2024-08-19**|**Active Learning for Identifying Disaster-Related Tweets: A Comparison with Keyword Filtering and Generic Fine-Tuning**|David Hanny et.al.|[2408.09914v1](http://arxiv.org/abs/2408.09914v1)|null|
|**2024-08-19**|**LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on Concept Discovery**|Weiji Kong et.al.|[2408.09899v1](http://arxiv.org/abs/2408.09899v1)|null|
|**2024-08-19**|**Performance Law of Large Language Models**|Chuhan Wu et.al.|[2408.09895v1](http://arxiv.org/abs/2408.09895v1)|null|
|**2024-08-19**|**Preoperative Rotator Cuff Tear Prediction from Shoulder Radiographs using a Convolutional Block Attention Module-Integrated Neural Network**|Chris Hyunchul Jo et.al.|[2408.09894v1](http://arxiv.org/abs/2408.09894v1)|null|
|**2024-08-19**|**Uncertainty Quantification of Pre-Trained and Fine-Tuned Surrogate Models using Conformal Prediction**|Vignesh Gopakumar et.al.|[2408.09881v1](http://arxiv.org/abs/2408.09881v1)|null|
|**2024-08-19**|**Docling Technical Report**|Christoph Auer et.al.|[2408.09869v1](http://arxiv.org/abs/2408.09869v1)|null|
|**2024-08-19**|**MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation**|Ching-Wen Yang et.al.|[2408.09865v1](http://arxiv.org/abs/2408.09865v1)|null|
|**2024-08-19**|**TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and Competition**|Tianwei Lin et.al.|[2408.09856v1](http://arxiv.org/abs/2408.09856v1)|[link](https://github.com/lin-tianwei/teamlora)|
|**2024-08-19**|**Self-Directed Turing Test for Large Language Models**|Weiqi Wu et.al.|[2408.09853v1](http://arxiv.org/abs/2408.09853v1)|null|
|**2024-08-19**|**Importance Weighting Can Help Large Language Models Self-Improve**|Chunyang Jiang et.al.|[2408.09849v1](http://arxiv.org/abs/2408.09849v1)|null|
|**2024-08-19**|**Continual Dialogue State Tracking via Reason-of-Select Distillation**|Yujie Feng et.al.|[2408.09846v1](http://arxiv.org/abs/2408.09846v1)|null|
|**2024-08-19**|**Segment-Anything Models Achieve Zero-shot Robustness in Autonomous Driving**|Jun Yan et.al.|[2408.09839v1](http://arxiv.org/abs/2408.09839v1)|[link](https://github.com/momo1986/robust_sam_iv)|
|**2024-08-19**|**Minor DPO reject penalty to increase training robustness**|Shiming Xie et.al.|[2408.09834v1](http://arxiv.org/abs/2408.09834v1)|null|
|**2024-08-19**|**CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models**|Linhao Yu et.al.|[2408.09819v1](http://arxiv.org/abs/2408.09819v1)|null|
|**2024-08-19**|**Contextual Dual Learning Algorithm with Listwise Distillation for Unbiased Learning to Rank**|Lulu Yu et.al.|[2408.09817v1](http://arxiv.org/abs/2408.09817v1)|null|
|**2024-08-19**|**World Models Increase Autonomy in Reinforcement Learning**|Zhao Yang et.al.|[2408.09807v2](http://arxiv.org/abs/2408.09807v2)|null|
|**2024-08-19**|**AutoML-guided Fusion of Entity and LLM-based representations**|Boshko Koloski et.al.|[2408.09794v1](http://arxiv.org/abs/2408.09794v1)|null|
|**2024-08-19**|**Anim-Director: A Large Multimodal Model Powered Agent for Controllable Animation Video Generation**|Yunxin Li et.al.|[2408.09787v1](http://arxiv.org/abs/2408.09787v1)|[link](https://github.com/hitsz-tmg/anim-director)|
|**2024-08-19**|**GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making**|Arsham Gholamzadeh Khoee et.al.|[2408.09785v1](http://arxiv.org/abs/2408.09785v1)|null|
|**2024-08-19**|**Summarizing long regulatory documents with a multi-step pipeline**|Mika Sie et.al.|[2408.09777v1](http://arxiv.org/abs/2408.09777v1)|null|
|**2024-08-19**|**Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?**|Shiyu Ni et.al.|[2408.09773v1](http://arxiv.org/abs/2408.09773v1)|null|
|**2024-08-19**|**Propagating the prior from shallow to deep with a pre-trained velocity-model Generative Transformer network**|Randy Harsuko et.al.|[2408.09767v1](http://arxiv.org/abs/2408.09767v1)|null|
|**2024-08-19**|**Event Stream based Human Action Recognition: A High-Definition Benchmark Dataset and Algorithms**|Xiao Wang et.al.|[2408.09764v1](http://arxiv.org/abs/2408.09764v1)|[link](https://github.com/event-ahu/celex-har)|
|**2024-08-19**|**Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning**|Jingyu Hu et.al.|[2408.09757v1](http://arxiv.org/abs/2408.09757v1)|null|
|**2024-08-19**|**Revisiting Reciprocal Recommender Systems: Metrics, Formulation, and Method**|Chen Yang et.al.|[2408.09748v1](http://arxiv.org/abs/2408.09748v1)|[link](https://github.com/rucaibox/crrs)|
|**2024-08-19**|**Enhanced Cascade Prostate Cancer Classifier in mp-MRI Utilizing Recall Feedback Adaptive Loss and Prior Knowledge-Based Feature Extraction**|Kun Luo et.al.|[2408.09746v1](http://arxiv.org/abs/2408.09746v1)|null|
|**2024-08-19**|**R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation**|Xiao Wang et.al.|[2408.09743v1](http://arxiv.org/abs/2408.09743v1)|[link](https://github.com/event-ahu/medical_image_analysis)|
|**2024-08-19**|**Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs**|Simon D Angus et.al.|[2408.09742v1](http://arxiv.org/abs/2408.09742v1)|null|
|**2024-08-19**|**Mutually-Aware Feature Learning for Few-Shot Object Counting**|Yerim Jeon et.al.|[2408.09734v1](http://arxiv.org/abs/2408.09734v1)|null|
|**2024-08-19**|**Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large Language Model Augmented Framework**|Jiandong Jin et.al.|[2408.09720v1](http://arxiv.org/abs/2408.09720v1)|[link](https://github.com/event-ahu/openpar)|
|**2024-08-19**|**SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing**|Pengjie Liu et.al.|[2408.09717v1](http://arxiv.org/abs/2408.09717v1)|null|
|**2024-08-19**|**HYDEN: Hyperbolic Density Representations for Medical Images and Reports**|Zhi Qiao et.al.|[2408.09715v2](http://arxiv.org/abs/2408.09715v2)|null|
|**2024-08-19**|**Partial-Multivariate Model for Forecasting**|Jaehoon Lee et.al.|[2408.09703v1](http://arxiv.org/abs/2408.09703v1)|null|
|**2024-08-19**|**Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering**|Ruofan Liang et.al.|[2408.09702v1](http://arxiv.org/abs/2408.09702v1)|null|
|**2024-08-19**|**Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer**|Mingda Li et.al.|[2408.09701v1](http://arxiv.org/abs/2408.09701v1)|null|
|**2024-08-19**|**Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation**|Yuyang Ye et.al.|[2408.09698v2](http://arxiv.org/abs/2408.09698v2)|null|
|**2024-08-19**|**LightWeather: Harnessing Absolute Positional Encoding to Efficient and Scalable Global Weather Forecasting**|Yisong Fu et.al.|[2408.09695v1](http://arxiv.org/abs/2408.09695v1)|null|
|**2024-08-19**|**Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts**|Jiaqing Liu et.al.|[2408.09688v1](http://arxiv.org/abs/2408.09688v1)|null|
|**2024-08-19**|**Simulating Field Experiments with Large Language Models**|Yaoyu Chen et.al.|[2408.09682v1](http://arxiv.org/abs/2408.09682v1)|null|
|**2024-08-19**|**MambaLoc: Efficient Camera Localisation via State Space Model**|Jialu Wang et.al.|[2408.09680v2](http://arxiv.org/abs/2408.09680v2)|null|
|**2024-08-19**|**Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey**|Ruiqi Zhang et.al.|[2408.09675v1](http://arxiv.org/abs/2408.09675v1)|[link](https://github.com/huawei-noah/SMARTS)|
|**2024-08-19**|**BLADE: Benchmarking Language Model Agents for Data-Driven Science**|Ken Gu et.al.|[2408.09667v1](http://arxiv.org/abs/2408.09667v1)|null|
|**2024-08-19**|**A Comparison of Large Language Model and Human Performance on Random Number Generation Tasks**|Rachel M. Harrison et.al.|[2408.09656v2](http://arxiv.org/abs/2408.09656v2)|null|
|**2024-08-19**|**Data-driven Conditional Instrumental Variables for Debiasing Recommender Systems**|Zhirong Huang et.al.|[2408.09651v1](http://arxiv.org/abs/2408.09651v1)|null|
|**2024-08-19**|**ExpoMamba: Exploiting Frequency SSM Blocks for Efficient and Effective Image Enhancement**|Eashan Adhikarla et.al.|[2408.09650v1](http://arxiv.org/abs/2408.09650v1)|[link](https://github.com/eashanadhikarla/expomamba)|
|**2024-08-19**|**Deep Learning-based Machine Condition Diagnosis using Short-time Fourier Transformation Variants**|Eduardo Jr Piedad et.al.|[2408.09649v1](http://arxiv.org/abs/2408.09649v1)|null|
|**2024-08-19**|**Debiased Contrastive Representation Learning for Mitigating Dual Biases in Recommender Systems**|Zhirong Huang et.al.|[2408.09646v1](http://arxiv.org/abs/2408.09646v1)|null|
|**2024-08-19**|**Exploring Wavelet Transformations for Deep Learning-based Machine Condition Diagnosis**|Eduardo Jr Piedad et.al.|[2408.09644v1](http://arxiv.org/abs/2408.09644v1)|null|
|**2024-08-19**|**Acquiring Bidirectionality via Large and Small Language Models**|Takumi Goto et.al.|[2408.09640v1](http://arxiv.org/abs/2408.09640v1)|null|
|**2024-08-19**|**How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments**|Yusuke Ide et.al.|[2408.09639v1](http://arxiv.org/abs/2408.09639v1)|null|
|**2024-08-19**|**Meta-Learning on Augmented Gene Expression Profiles for Enhanced Lung Cancer Detection**|Arya Hadizadeh Moghaddam et.al.|[2408.09635v1](http://arxiv.org/abs/2408.09635v1)|[link](https://github.com/aryahm1375/metagene)|
|**2024-08-19**|**MoDeGPT: Modular Decomposition for Large Language Model Compression**|Chi-Heng Lin et.al.|[2408.09632v2](http://arxiv.org/abs/2408.09632v2)|null|
|**2024-08-19**|**A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic Text Classification**|Claudio M. V. de Andrade et.al.|[2408.09629v1](http://arxiv.org/abs/2408.09629v1)|null|
|**2024-08-19**|**On the Foundations of Conflict-Driven Solving for Hybrid MKNF Knowledge Bases**|Riley Kinahan et.al.|[2408.09626v1](http://arxiv.org/abs/2408.09626v1)|null|
|**2024-08-19**|**Refining Packing and Shuffling Strategies for Enhanced Performance in Generative Language Models**|Yanbing Chen et.al.|[2408.09621v1](http://arxiv.org/abs/2408.09621v1)|null|
|**2024-08-18**|**Does Thought Require Sensory Grounding? From Pure Thinkers to Large Language Models**|David J. Chalmers et.al.|[2408.09605v1](http://arxiv.org/abs/2408.09605v1)|null|
|**2024-08-18**|**Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning**|Tiansheng Huang et.al.|[2408.09600v1](http://arxiv.org/abs/2408.09600v1)|null|
|**2024-08-18**|**Moonshine: Distilling Game Content Generators into Steerable Generative Models**|Yuhe Nie et.al.|[2408.09594v1](http://arxiv.org/abs/2408.09594v1)|null|
|**2024-08-18**|**PhysBERT: A Text Embedding Model for Physics Scientific Literature**|Thorsten Hellert et.al.|[2408.09574v1](http://arxiv.org/abs/2408.09574v1)|null|
|**2024-08-18**|**Say My Name: a Model's Bias Discovery Framework**|Massimiliano Ciranni et.al.|[2408.09570v1](http://arxiv.org/abs/2408.09570v1)|null|

#### Abstracts
##### **Demystifying the Communication Characteristics for Distributed Transformer Models**
2408.10197v1 by Quentin Anthony, Benjamin Michalowicz, Jacob Hatef, Lang Xu, Mustafa Abduljabbar, Aamir Shafi, Hari Subramoni, Dhabaleswar Panda

Deep learning (DL) models based on the transformer architecture have
revolutionized many DL applications such as large language models (LLMs),
vision transformers, audio generation, and time series prediction. Much of this
progress has been fueled by distributed training, yet distributed communication
remains a substantial bottleneck to training progress. This paper examines the
communication behavior of transformer models - that is, how different
parallelism schemes used in multi-node/multi-GPU DL Training communicate data
in the context of transformers. We use GPT-based language models as a case
study of the transformer architecture due to their ubiquity. We validate the
empirical results obtained from our communication logs using analytical models.
At a high level, our analysis reveals a need to optimize small message
point-to-point communication further, correlations between sequence length,
per-GPU throughput, model size, and optimizations used, and where to
potentially guide further optimizations in framework and HPC middleware design
and optimization.

摘要：基於Transformer架構的深度學習 (DL) 模型已徹底改革許多 DL 應用程式，例如大型語言模型 (LLM)、視覺Transformer、音訊產生和時間序列預測。這項進展在很大程度上是由於分散式訓練所推動，然而，分散式通訊仍然是訓練進展的一大瓶頸。本文探討Transformer模型的通訊行為，也就是在多節點/多 GPU DL 訓練中，使用於多重平行處理方案如何通訊Transformer中的資料。由於 GPT-based 語言模型普遍存在，因此我們使用它們作為Transformer架構的案例研究。我們使用分析模型驗證從通訊記錄中取得的經驗結果。在高層級中，我們的分析顯示需要進一步最佳化小型訊息點對點通訊、序列長度、每個 GPU 處理量、模型大小和所使用的最佳化之間的關聯性，以及在架構和 HPC 中間軟體設計和最佳化中潛在引導進一步最佳化的位置。

##### **SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views**
2408.10195v1 by Chao Xu, Ang Li, Linghao Chen, Yulin Liu, Ruoxi Shi, Hao Su, Minghua Liu

Open-world 3D generation has recently attracted considerable attention. While
many single-image-to-3D methods have yielded visually appealing outcomes, they
often lack sufficient controllability and tend to produce hallucinated regions
that may not align with users' expectations. In this paper, we explore an
important scenario in which the input consists of one or a few unposed 2D
images of a single object, with little or no overlap. We propose a novel
method, SpaRP, to reconstruct a 3D textured mesh and estimate the relative
camera poses for these sparse-view images. SpaRP distills knowledge from 2D
diffusion models and finetunes them to implicitly deduce the 3D spatial
relationships between the sparse views. The diffusion model is trained to
jointly predict surrogate representations for camera poses and multi-view
images of the object under known poses, integrating all information from the
input sparse views. These predictions are then leveraged to accomplish 3D
reconstruction and pose estimation, and the reconstructed 3D model can be used
to further refine the camera poses of input views. Through extensive
experiments on three datasets, we demonstrate that our method not only
significantly outperforms baseline methods in terms of 3D reconstruction
quality and pose prediction accuracy but also exhibits strong efficiency. It
requires only about 20 seconds to produce a textured mesh and camera poses for
the input views. Project page: https://chaoxu.xyz/sparp.

摘要：開放世界 3D 生成最近引起了相當大的關注。雖然許多單張圖片轉 3D 的方法產生了視覺上令人滿意的結果，但它們通常缺乏足夠的可控性，並且容易產生可能與使用者預期不符的幻覺區域。在本文中，我們探討了一個重要的場景，其中輸入包含一個或幾個未擺放的單一物體 2D 影像，幾乎沒有或沒有重疊。我們提出了一種新方法 SpaRP，用於重建 3D 紋理網格並估計這些稀疏視圖影像的相對相機姿勢。SpaRP 從 2D 擴散模型中提煉知識，並對它們進行微調，以隱式推論稀疏視圖之間的 3D 空間關係。擴散模型經過訓練，可以聯合預測已知姿勢下相機姿勢和物體多視圖影像的替代表示，整合輸入稀疏視圖中的所有資訊。然後利用這些預測來完成 3D 重建和姿勢估計，並且可以利用重建的 3D 模型進一步改善輸入視圖的相機姿勢。透過對三個資料集進行廣泛的實驗，我們證明了我們的方法不僅在 3D 重建品質和姿勢預測準確度方面明顯優於基準方法，而且還表現出很強的效率。它只需要大約 20 秒即可為輸入視圖產生紋理網格和相機姿勢。專案頁面：https://chaoxu.xyz/sparp。

##### **Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models**
2408.10189v1 by Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, Albert Gu

Transformer architectures have become a dominant paradigm for domains like
language modeling but suffer in many inference settings due to their
quadratic-time self-attention. Recently proposed subquadratic architectures,
such as Mamba, have shown promise, but have been pretrained with substantially
less computational resources than the strongest Transformer models. In this
work, we present a method that is able to distill a pretrained Transformer
architecture into alternative architectures such as state space models (SSMs).
The key idea to our approach is that we can view both Transformers and SSMs as
applying different forms of mixing matrices over the token sequences. We can
thus progressively distill the Transformer architecture by matching different
degrees of granularity in the SSM: first matching the mixing matrices
themselves, then the hidden units at each block, and finally the end-to-end
predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant
based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens and a hybrid
version (Hybrid Phi-Mamba) using 5B tokens. Despite using less than 1% of the
training data typically used to train models from scratch, Phi-Mamba boasts
substantially stronger performance compared to all past open-source
non-Transformer models. MOHAWK allows models like SSMs to leverage
computational resources invested in training Transformer-based architectures,
highlighting a new avenue for building such models.

摘要：變形器架構已成為語言模型等領域的主流範例，但由於其二次方時間自我注意，在許多推論設定中會遇到困難。最近提出的次二次方架構，例如 Mamba，已展現其前景，但預訓練時所用的運算資源遠低於最強大的變形器模型。在這項工作中，我們提出了一種方法，能夠將預訓練的變形器架構轉化為替代架構，例如狀態空間模型 (SSM)。我們方法的關鍵概念是，我們可以將變形器和 SSM 視為在令牌序列上套用不同形式的混合矩陣。因此，我們可以透過調整 SSM 中不同程度的詳細度來逐步轉化變形器架構：首先調整混合矩陣本身，然後調整每個區塊中的隱藏單元，最後調整端對端預測。我們的方法稱為 MOHAWK，能夠使用僅 3B 個令牌將基於 Phi-1.5 架構的 Mamba-2 變體（Phi-Mamba）轉化為混合版本（Hybrid Phi-Mamba），並使用 5B 個令牌。儘管使用少於從頭訓練模型通常使用的 1% 訓練資料，但與過去所有開源非變形器模型相比，Phi-Mamba 擁有顯著更強的效能。MOHAWK 允許像 SSM 這樣的模型利用投資於訓練基於變形器的架構的運算資源，突顯了建構此類模型的新途徑。

##### **LongVILA: Scaling Long-Context Visual Language Models for Long Videos**
2408.10188v1 by Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han

Long-context capability is critical for multi-modal foundation models. We
introduce LongVILA, a full-stack solution for long-context vision-language
models, including system, model training, and dataset development. On the
system side, we introduce the first Multi-Modal Sequence Parallelism (MM-SP)
system that enables long-context training and inference, enabling 2M context
length training on 256 GPUs. MM-SP is also efficient, being 2.1x - 5.7x faster
than Ring-Style Sequence Parallelism and 1.1x - 1.4x faster than Megatron-LM in
text-only settings. Moreover, it seamlessly integrates with Hugging Face
Transformers. For model training, we propose a five-stage pipeline comprising
alignment, pre-training, context extension, and long-short joint supervised
fine-tuning. Regarding datasets, we meticulously construct large-scale visual
language pre-training datasets and long video instruction-following datasets to
support our multi-stage training process. The full-stack solution extends the
feasible frame number of VILA by a factor of 128 (from 8 to 1024 frames) and
improves long video captioning score from 2.00 to 3.26 (1.6x), achieving 99.5%
accuracy in 1400-frames video (274k context length) needle in a haystack.
LongVILA-8B also demonstrates a consistent improvement in performance on long
videos within the VideoMME benchmark as the video frames increase.

摘要：長文本能力對於多模態基礎模型至關重要。我們推出 LongVILA，這是一個針對長文本視覺語言模型的完整解決方案，包括系統、模型訓練和資料集開發。在系統方面，我們引入了第一個多模態序列平行化 (MM-SP) 系統，它支援長文本訓練和推論，在 256 個 GPU 上進行 2M 文本長度訓練。MM-SP 也非常有效率，比 Ring-Style 序列平行化快 2.1 倍 - 5.7 倍，比 Megatron-LM 在純文字設定中快 1.1 倍 - 1.4 倍。此外，它與 Hugging Face Transformers 完美整合。對於模型訓練，我們提出了一個包含對齊、預訓練、文本延伸和長短聯合監督微調的五階段管道。關於資料集，我們精心構建了大規模的視覺語言預訓練資料集和長影片指令遵循資料集，以支援我們的多階段訓練流程。這個完整解決方案將 VILA 的可行幀數擴大了 128 倍（從 8 幀擴展到 1024 幀），並將長影片字幕評分從 2.00 提升到 3.26（1.6 倍），在 1400 幀影片（274k 文本長度）中達到 99.5% 的大海撈針準確度。LongVILA-8B 也證明了隨著影片幀數增加，在 VideoMME 基準中，長影片的效能持續提升。

##### **Imbalance-Aware Culvert-Sewer Defect Segmentation Using an Enhanced Feature Pyramid Network**
2408.10181v1 by Rasha Alshawi, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Kendall Niles, Ken Pathak, Steve Sloan

Imbalanced datasets are a significant challenge in real-world scenarios. They
lead to models that underperform on underrepresented classes, which is a
critical issue in infrastructure inspection. This paper introduces the Enhanced
Feature Pyramid Network (E-FPN), a deep learning model for the semantic
segmentation of culverts and sewer pipes within imbalanced datasets. The E-FPN
incorporates architectural innovations like sparsely connected blocks and
depth-wise separable convolutions to improve feature extraction and handle
object variations. To address dataset imbalance, the model employs strategies
like class decomposition and data augmentation. Experimental results on the
culvert-sewer defects dataset and a benchmark aerial semantic segmentation
drone dataset show that the E-FPN outperforms state-of-the-art methods,
achieving an average Intersection over Union (IoU) improvement of 13.8% and
27.2%, respectively. Additionally, class decomposition and data augmentation
together boost the model's performance by approximately 6.9% IoU. The proposed
E-FPN presents a promising solution for enhancing object segmentation in
challenging, multi-class real-world datasets, with potential applications
extending beyond culvert-sewer defect detection.

摘要：<paragraph>不平衡的資料集在真實世界的場景中是一個重大的挑戰。它們導致模型在代表性不足的類別上表現不佳，這在基礎設施檢查中是一個關鍵問題。本文介紹了增強特徵金字塔網路 (E-FPN)，這是一種用於不平衡資料集中涵洞和下水道管道的語義分割的深度學習模型。E-FPN 結合了稀疏連接區塊和深度可分離卷積等架構創新，以改進特徵提取和處理物件變異。為了解決資料集不平衡的問題，該模型採用了類別分解和資料擴充等策略。涵洞下水道缺陷資料集和基準航拍語義分割無人機資料集的實驗結果表明，E-FPN 優於最先進的方法，分別實現了平均聯合交集 (IoU) 提高了 13.8% 和 27.2%。此外，類別分解和資料擴充共同將模型的效能提升了大約 6.9% IoU。所提出的 E-FPN 為增強具有挑戰性、多類別真實世界資料集中物件分割提供了一個有希望的解決方案，其潛在應用範圍不僅限於涵洞下水道缺陷檢測。</paragraph>

##### **Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias in Facial Recognition**
2408.10175v1 by Rafael M. Mamede, Pedro C. Neto, Ana F. Sequeira

This study investigates the effects of occlusions on the fairness of face
recognition systems, particularly focusing on demographic biases. Using the
Racial Faces in the Wild (RFW) dataset and synthetically added realistic
occlusions, we evaluate their effect on the performance of face recognition
models trained on the BUPT-Balanced and BUPT-GlobalFace datasets. We note
increases in the dispersion of FMR, FNMR, and accuracy alongside decreases in
fairness according to Equilized Odds, Demographic Parity, STD of Accuracy, and
Fairness Discrepancy Rate. Additionally, we utilize a pixel attribution method
to understand the importance of occlusions in model predictions, proposing a
new metric, Face Occlusion Impact Ratio (FOIR), that quantifies the extent to
which occlusions affect model performance across different demographic groups.
Our results indicate that occlusions exacerbate existing demographic biases,
with models placing higher importance on occlusions in an unequal fashion,
particularly affecting African individuals more severely.

摘要：本研究调查遮挡对人脸识别系统公平性的影响，特别关注人口统计偏差。使用野外人脸种族（RFW）数据集和合成添加的真实遮挡，我们评估了它们对训练于 BUPT 平衡和 BUPT 全球人脸数据集的人脸识别模型性能的影响。我们注意到 FMR、FNMR 和准确度的分散性增加，同时根据均等几率、人口统计学平价、准确度标准差和公平性差异率，公平性下降。此外，我们利用像素归因方法来了解遮挡在模型预测中的重要性，提出了一种新的指标，即人脸遮挡影响率（FOIR），它量化了遮挡在不同人口统计组中影响模型性能的程度。我们的结果表明，遮挡加剧了现有人口统计偏差，模型对遮挡的重视程度不平等，尤其对非洲人影响更严重。

##### **SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models**
2408.10174v1 by Anke Tang, Li Shen, Yong Luo, Shuai Xie, Han Hu, Lefei Zhang, Bo Du, Dacheng Tao

Deep model training on extensive datasets is increasingly becoming
cost-prohibitive, prompting the widespread adoption of deep model fusion
techniques to leverage knowledge from pre-existing models. From simple weight
averaging to more sophisticated methods like AdaMerging, model fusion
effectively improves model performance and accelerates the development of new
models. However, potential interference between parameters of individual models
and the lack of interpretability in the fusion progress remain significant
challenges. Existing methods often try to resolve the parameter interference
issue by evaluating attributes of parameters, such as their magnitude or sign,
or by parameter pruning. In this study, we begin by examining the fine-tuning
of linear layers through the lens of subspace analysis and explicitly define
parameter interference as an optimization problem to shed light on this
subject. Subsequently, we introduce an innovative approach to model fusion
called zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, which
allows for the upscaling of source models into an MoE model without extra data
or further training. Our approach relies on the observation that fine-tuning
mostly keeps the important parts from the pre-training, but it uses less
significant or unused areas to adapt to new tasks. Also, the issue of parameter
interference, which is intrinsically intractable in the original parameter
space, can be managed by expanding the dimensions. We conduct extensive
experiments across diverse scenarios, such as image classification and text
generalization tasks, using full fine-tuning and LoRA fine-tuning, and we apply
our method to large language models (CLIP models, Flan-T5 models, and
Mistral-7B models), highlighting the adaptability and scalability of SMILE.
Code is available at https://github.com/tanganke/fusion_bench

摘要：<paragraph>在海量数据集上进行深度模型训练的成本越来越高，促使深度模型融合技术被广泛采用，以利用现有模型中的知识。从简单的权重平均到更复杂的方法（如 AdaMerging），模型融合有效地提高了模型性能，并加速了新模型的开发。然而，各个模型参数之间的潜在干扰和融合过程中的可解释性缺乏仍然是重大挑战。现有的方法通常试图通过评估参数的属性（例如它们的幅度或符号）或通过参数剪枝来解决参数干扰问题。在本研究中，我们首先通过子空间分析的视角来检查线性层的微调，并明确地将参数干扰定义为一个优化问题，以阐明这一主题。随后，我们引入了一种创新的模型融合方法，称为零样本稀疏低秩专家混合（SMILE）构建，它允许将源模型提升到 MoE 模型中，而无需额外的数据或进一步的训练。我们的方法依赖于这样一个观察：微调主要保留了预训练中的重要部分，但它使用不太重要或未使用的区域来适应新任务。此外，参数干扰问题在原始参数空间中本质上是难以处理的，可以通过扩展维度来管理。我们在图像分类和文本泛化任务等不同场景中进行了广泛的实验，使用完全微调和 LoRA 微调，并将我们的方法应用于大型语言模型（CLIP 模型、Flan-T5 模型和 Mistral-7B 模型），突出了 SMILE 的适应性和可扩展性。代码可在 https://github.com/tanganke/fusion_bench 获得</paragraph>

##### **Customizing Language Models with Instance-wise LoRA for Sequential Recommendation**
2408.10159v1 by Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, Xiangnan He

Sequential recommendation systems predict a user's next item of interest by
analyzing past interactions, aligning recommendations with individual
preferences. Leveraging the strengths of Large Language Models (LLMs) in
knowledge comprehension and reasoning, recent approaches have applied LLMs to
sequential recommendation through language generation paradigms. These methods
convert user behavior sequences into prompts for LLM fine-tuning, utilizing
Low-Rank Adaptation (LoRA) modules to refine recommendations. However, the
uniform application of LoRA across diverse user behaviors sometimes fails to
capture individual variability, leading to suboptimal performance and negative
transfer between disparate sequences. To address these challenges, we propose
Instance-wise LoRA (iLoRA), integrating LoRA with the Mixture of Experts (MoE)
framework. iLoRA creates a diverse array of experts, each capturing specific
aspects of user preferences, and introduces a sequence representation guided
gate function. This gate function processes historical interaction sequences to
generate enriched representations, guiding the gating network to output
customized expert participation weights. This tailored approach mitigates
negative transfer and dynamically adjusts to diverse behavior patterns.
Extensive experiments on three benchmark datasets demonstrate the effectiveness
of iLoRA, highlighting its superior performance compared to existing methods in
capturing user-specific preferences and improving recommendation accuracy.

摘要：序列推薦系統透過分析過往互動，比對推薦與個人偏好來預測使用者下一個感興趣的項目。利用大型語言模型 (LLM) 在知識理解和推理方面的優勢，最近的方法已透過語言生成範例將 LLM 應用於序列推薦。這些方法將使用者行為序列轉換為 LLM 微調提示，並利用低秩適應 (LoRA) 模組來改善推薦。然而，在不同的使用者行為中統一應用 LoRA 有時無法捕捉個別變異性，導致次佳效能和不同序列之間的負面轉移。為了應對這些挑戰，我們提出實例 LoRA (iLoRA)，將 LoRA 與專家混合 (MoE) 架構整合。iLoRA 建立了一系列不同的專家，每個專家都能捕捉使用者偏好的特定面向，並引入由序列表示引導的閘門函數。此閘門函數處理歷史互動序列以產生豐富的表示，引導閘控網路輸出客製化的專家參與權重。此量身打造的方法可減輕負面轉移，並動態調整至不同的行為模式。在三個基準資料集上進行的廣泛實驗證明了 iLoRA 的有效性，突顯出其在捕捉使用者特定偏好和改善推薦準確度方面優於現有方法的卓越效能。

##### **Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models**
2408.10151v1 by Amey Hengle, Prasoon Bajpai, Soham Dan, Tanmoy Chakraborty

While recent large language models (LLMs) demonstrate remarkable abilities in
responding to queries in diverse languages, their ability to handle long
multilingual contexts is unexplored. As such, a systematic evaluation of the
long-context capabilities of LLMs in multilingual settings is crucial,
specifically in the context of information retrieval. To address this gap, we
introduce the MultiLingual Needle-in-a-Haystack (MLNeedle) test, designed to
assess a model's ability to retrieve relevant information (the needle) from a
collection of multilingual distractor texts (the haystack). This test serves as
an extension of the multilingual question-answering task, encompassing both
monolingual and cross-lingual retrieval. We evaluate four state-of-the-art LLMs
on MLNeedle. Our findings reveal that model performance can vary significantly
with language and needle position. Specifically, we observe that model
performance is the lowest when the needle is (i) in a language outside the
English language family and (ii) located in the middle of the input context.
Furthermore, although some models claim a context size of $8k$ tokens or
greater, none demonstrate satisfactory cross-lingual retrieval performance as
the context length increases. Our analysis provides key insights into the
long-context behavior of LLMs in multilingual settings to guide future
evaluation protocols. To our knowledge, this is the first study to investigate
the multilingual long-context behavior of LLMs.

摘要：儘管最近的大型語言模型 (LLM) 在以各種語言回答查詢方面表現出非凡的能力，但它們處理長多語言語境的能耐尚未被探討。因此，在多語言環境中對 LLM 的長語境能力進行系統評估至關重要，特別是在資訊檢索的語境中。為了解決這個差距，我們引入了多語言大海撈針 (MLNeedle) 測試，旨在評估模型從多語言干擾文本（大海撈針）集合中檢索相關資訊（針）的能力。此測試作為多語言問答任務的延伸，包含單語和跨語言檢索。我們在 MLNeedle 上評估了四個最先進的 LLM。我們的研究結果顯示，模型效能會隨著語言和針的位置而有顯著差異。具體來說，我們觀察到模型效能最低的情況是當針（i）在英語語系之外的語言中，以及（ii）位於輸入語境中間時。此外，儘管有些模型聲稱語境大小為 8k 個 token 或更大，但隨著語境長度的增加，沒有任何模型表現出令人滿意的跨語言檢索效能。我們的分析提供了對多語言環境中 LLM 的長語境行為的重要見解，以指導未來的評估協定。據我們所知，這是第一個研究 LLM 的多語言長語境行為的研究。

##### **In-Context Learning with Representations: Contextual Generalization of Trained Transformers**
2408.10147v1 by Tong Yang, Yu Huang, Yingbin Liang, Yuejie Chi

In-context learning (ICL) refers to a remarkable capability of pretrained
large language models, which can learn a new task given a few examples during
inference. However, theoretical understanding of ICL is largely under-explored,
particularly whether transformers can be trained to generalize to unseen
examples in a prompt, which will require the model to acquire contextual
knowledge of the prompt for generalization. This paper investigates the
training dynamics of transformers by gradient descent through the lens of
non-linear regression tasks. The contextual generalization here can be attained
via learning the template function for each task in-context, where all template
functions lie in a linear space with $m$ basis functions. We analyze the
training dynamics of one-layer multi-head transformers to in-contextly predict
unlabeled inputs given partially labeled prompts, where the labels contain
Gaussian noise and the number of examples in each prompt are not sufficient to
determine the template. Under mild assumptions, we show that the training loss
for a one-layer multi-head transformer converges linearly to a global minimum.
Moreover, the transformer effectively learns to perform ridge regression over
the basis functions. To our knowledge, this study is the first provable
demonstration that transformers can learn contextual (i.e., template)
information to generalize to both unseen examples and tasks when prompts
contain only a small number of query-answer pairs.

摘要：情境學習 (ICL) 是一種預訓練大型語言模型的顯著功能，它可以在推理期間根據一些範例學習一項新任務。然而，對於 ICL 的理論理解在很大程度上尚未被探索，特別是Transformer是否可以接受訓練以概化提示中未見過的範例，這將要求模型習得提示的脈絡知識以進行概化。本文透過非線性回歸任務的透鏡，研究Transformer的梯度下降訓練動態。此處的脈絡概化可透過在情境中學習每個任務的範本函數來達成，其中所有範本函數都位於具有 $m$ 個基底函數的線性空間中。我們分析單層多頭Transformer的訓練動態，以在部分標記提示中預測未標記輸入，其中標籤包含高斯噪聲，且每個提示中的範例數量不足以確定範本。在溫和的假設下，我們表明單層多頭Transformer的訓練損失線性收斂到全局最小值。此外，Transformer有效地學習對基底函數執行嶺回歸。據我們所知，這項研究是第一個可證明Transformer可以學習脈絡（即範本）資訊，以在提示僅包含少數查詢答案配對時，概化到未見過的範例和任務。

##### **Instruction Finetuning for Leaderboard Generation from Empirical AI Research**
2408.10141v1 by Salomon Kabongo, Jennifer D'Souza

This study demonstrates the application of instruction finetuning of
pretrained Large Language Models (LLMs) to automate the generation of AI
research leaderboards, extracting (Task, Dataset, Metric, Score) quadruples
from articles. It aims to streamline the dissemination of advancements in AI
research by transitioning from traditional, manual community curation, or
otherwise taxonomy-constrained natural language inference (NLI) models, to an
automated, generative LLM-based approach. Utilizing the FLAN-T5 model, this
research enhances LLMs' adaptability and reliability in information extraction,
offering a novel method for structured knowledge representation.

摘要：本研究展示了預訓練大型語言模型 (LLM) 的指令微調應用，以自動化 AI 研究排行榜的生成，從文章中萃取 (任務、資料集、指標、分數) 四元組。其目標是透過從傳統的人工社群策展或受分類法約束的自然語言推論 (NLI) 模型，轉換為自動化、生成式 LLM 基礎方法，簡化 AI 研究進展的傳播。利用 FLAN-T5 模型，本研究增強了 LLM 在資訊萃取中的適應性和可靠性，提供了一種結構化知識表示的新方法。

##### **Rhyme-aware Chinese lyric generator based on GPT**
2408.10130v1 by Yixiao Yuan, Yangchen Huang, Yu Ma, Xinjin Li, Zhenglin Li, Yiming Shi, Huapeng Zhou

Neural language representation models such as GPT, pre-trained on large-scale
corpora, can effectively capture rich semantic patterns from plain text and be
fine-tuned to consistently improve natural language generation performance.
However, existing pre-trained language models used to generate lyrics rarely
consider rhyme information, which is crucial in lyrics. Using a pre-trained
model directly results in poor performance. To enhance the rhyming quality of
generated lyrics, we incorporate integrated rhyme information into our model,
thereby improving lyric generation performance.

摘要：神經語言表徵模型，例如 GPT，經過大規模語料庫預先訓練，能有效擷取純文字中的豐富語意模式，並進行微調以持續提升自然語言生成效能。
然而，現有的預先訓練語言模型用於產生歌詞時，很少考慮押韻資訊，而押韻資訊在歌詞中至關重要。直接使用預先訓練模型會導致效能不佳。為了提升生成歌詞的押韻品質，我們將整合的押韻資訊納入模型中，進而提升歌詞生成效能。

##### **Advancing Voice Cloning for Nepali: Leveraging Transfer Learning in a Low-Resource Language**
2408.10128v1 by Manjil Karki, Pratik Shakya, Sandesh Acharya, Ravi Pandit, Dinesh Gothe

Voice cloning is a prominent feature in personalized speech interfaces. A
neural vocal cloning system can mimic someone's voice using just a few audio
samples. Both speaker encoding and speaker adaptation are topics of research in
the field of voice cloning. Speaker adaptation relies on fine-tuning a
multi-speaker generative model, which involves training a separate model to
infer a new speaker embedding used for speaker encoding. Both methods can
achieve excellent performance, even with a small number of cloning audios, in
terms of the speech's naturalness and similarity to the original speaker.
Speaker encoding approaches are more appropriate for low-resource deployment
since they require significantly less memory and have a faster cloning time
than speaker adaption, which can offer slightly greater naturalness and
similarity. The main goal is to create a vocal cloning system that produces
audio output with a Nepali accent or that sounds like Nepali. For the further
advancement of TTS, the idea of transfer learning was effectively used to
address several issues that were encountered in the development of this system,
including the poor audio quality and the lack of available data.

摘要：語音複製是個人化語音介面中的一項顯著功能。神經語音複製系統僅使用少數音訊樣本就能模擬某人的聲音。在語音複製領域中，說話者編碼和說話者適應都是研究主題。說話者適應依賴於微調多說話者生成模型，其中涉及訓練一個單獨的模型來推斷用於說話者編碼的新說話者嵌入。這兩種方法都能在語音的自然性和與原始說話者的相似性方面取得極佳的效能，即使克隆音訊數量很少。說話者編碼方法更適合低資源配置，因為它們需要的記憶體顯著較少，而且克隆時間比說話者適應快，而說話者適應可以提供稍微更高的自然性和相似性。主要目標是建立一個產生具有尼泊爾口音或聽起來像尼泊爾語的音訊輸出的語音複製系統。為了進一步推進 TTS，轉移學習的想法被有效地用於解決這個系統開發中遇到的幾個問題，包括音訊品質差和可用資料不足。

##### **Molecular Graph Representation Learning Integrating Large Language Models with Domain-specific Small Models**
2408.10124v1 by Tianyu Zhang, Yuxiang Ren, Chengbin Hou, Hairong Lv, Xuegong Zhang

Molecular property prediction is a crucial foundation for drug discovery. In
recent years, pre-trained deep learning models have been widely applied to this
task. Some approaches that incorporate prior biological domain knowledge into
the pre-training framework have achieved impressive results. However, these
methods heavily rely on biochemical experts, and retrieving and summarizing
vast amounts of domain knowledge literature is both time-consuming and
expensive. Large Language Models (LLMs) have demonstrated remarkable
performance in understanding and efficiently providing general knowledge.
Nevertheless, they occasionally exhibit hallucinations and lack precision in
generating domain-specific knowledge. Conversely, Domain-specific Small Models
(DSMs) possess rich domain knowledge and can accurately calculate molecular
domain-related metrics. However, due to their limited model size and singular
functionality, they lack the breadth of knowledge necessary for comprehensive
representation learning. To leverage the advantages of both approaches in
molecular property prediction, we propose a novel Molecular Graph
representation learning framework that integrates Large language models and
Domain-specific small models (MolGraph-LarDo). Technically, we design a
two-stage prompt strategy where DSMs are introduced to calibrate the knowledge
provided by LLMs, enhancing the accuracy of domain-specific information and
thus enabling LLMs to generate more precise textual descriptions for molecular
samples. Subsequently, we employ a multi-modal alignment method to coordinate
various modalities, including molecular graphs and their corresponding
descriptive texts, to guide the pre-training of molecular representations.
Extensive experiments demonstrate the effectiveness of the proposed method.

摘要：分子特性預測是藥物發現的關鍵基礎。近年來，預訓練深度學習模型已廣泛應用於此任務。一些將先驗生物領域知識納入預訓練架構的方法已取得令人印象深刻的成果。然而，這些方法嚴重依賴於生物化學專家，並且檢索和總結大量的領域知識文獻既耗時又昂貴。大型語言模型 (LLM) 在理解和有效提供一般知識方面展示了卓越的性能。儘管如此，它們偶爾會出現幻覺，並且在生成特定領域的知識時缺乏精確性。相反，特定領域的小模型 (DSM) 擁有豐富的領域知識，並且可以準確計算與分子領域相關的指標。然而，由於它們有限的模型大小和單一功能，它們缺乏全面表示學習所需的知識廣度。為了在分子特性預測中利用這兩種方法的優點，我們提出了一個新穎的分子圖表示學習框架，它集成了大型語言模型和特定領域的小模型 (MolGraph-LarDo)。在技術上，我們設計了一個兩階段提示策略，其中引入 DSM 來校準 LLM 提供的知識，提高特定領域信息的準確性，從而使 LLM 能夠為分子樣本生成更精確的文本描述。隨後，我們採用多模態對齊方法來協調各種模態，包括分子圖及其對應的描述性文本，以指導分子表示的預訓練。廣泛的實驗證明了所提出方法的有效性。

##### **Geometry Informed Tokenization of Molecules for Language Model Generation**
2408.10120v1 by Xiner Li, Limei Wang, Youzhi Luo, Carl Edwards, Shurui Gui, Yuchao Lin, Heng Ji, Shuiwang Ji

We consider molecule generation in 3D space using language models (LMs),
which requires discrete tokenization of 3D molecular geometries. Although
tokenization of molecular graphs exists, that for 3D geometries is largely
unexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which
converts molecular geometries into $SE(3)$-invariant 1D discrete sequences.
Geo2Seq consists of canonical labeling and invariant spherical representation
steps, which together maintain geometric and atomic fidelity in a format
conducive to LMs. Our experiments show that, when coupled with Geo2Seq, various
LMs excel in molecular geometry generation, especially in controlled generation
tasks.

摘要：我們考慮使用語言模型 (LM) 在 3D 空間中生成分子，這需要對 3D 分子幾何結構進行離散的標記化。儘管存在分子圖的標記化，但對 3D 幾何結構的標記化在很大程度上尚未被探索。在此，我們嘗試通過提出 Geo2Seq 來彌合這一差距，該方法將分子幾何結構轉換為 $SE(3)$ 不變的 1D 離散序列。Geo2Seq 包含規範標籤和不變球面表示步驟，它們共同以有利於 LM 的格式保持幾何和原子保真度。我們的實驗表明，當與 Geo2Seq 結合使用時，各種 LM 在分子幾何生成方面表現出色，特別是在受控生成任務中。

##### **Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data**
2408.10119v1 by Tao Yang, Yangming Shi, Yunwen Huang, Feng Chen, Yin Zheng, Lei Zhang

Text-to-video (T2V) generation has gained significant attention due to its
wide applications to video generation, editing, enhancement and translation,
\etc. However, high-quality (HQ) video synthesis is extremely challenging
because of the diverse and complex motions existed in real world. Most existing
works struggle to address this problem by collecting large-scale HQ videos,
which are inaccessible to the community. In this work, we show that publicly
available limited and low-quality (LQ) data are sufficient to train a HQ video
generator without recaptioning or finetuning. We factorize the whole T2V
generation process into two steps: generating an image conditioned on a highly
descriptive caption, and synthesizing the video conditioned on the generated
image and a concise caption of motion details. Specifically, we present
\emph{Factorized-Dreamer}, a factorized spatiotemporal framework with several
critical designs for T2V generation, including an adapter to combine text and
image embeddings, a pixel-aware cross attention module to capture pixel-level
image information, a T5 text encoder to better understand motion description,
and a PredictNet to supervise optical flows. We further present a noise
schedule, which plays a key role in ensuring the quality and stability of video
generation. Our model lowers the requirements in detailed captions and HQ
videos, and can be directly trained on limited LQ datasets with noisy and brief
captions such as WebVid-10M, largely alleviating the cost to collect
large-scale HQ video-text pairs. Extensive experiments in a variety of T2V and
image-to-video generation tasks demonstrate the effectiveness of our proposed
Factorized-Dreamer. Our source codes are available at
\url{https://github.com/yangxy/Factorized-Dreamer/}.

摘要：<paragraph>文本转视频 (T2V) 生成因其在视频生成、编辑、增强和翻译等方面的广泛应用而备受关注，\等。然而，高质量 (HQ) 视频合成极具挑战性，因为现实世界中存在多样且复杂的运动。大多数现有作品通过收集社区无法获取的大规模 HQ 视频来解决此问题，但这很困难。在这项工作中，我们表明公开可用的有限且低质量 (LQ) 数据足以训练 HQ 视频生成器，而无需重新加标题或微调。我们将整个 T2V 生成过程分解为两个步骤：生成基于高度描述性标题的图像，并基于生成的图像和简洁的运动细节标题合成视频。具体来说，我们提出了\emph{Factorized-Dreamer}，这是一个分解的时空框架，具有几个关键设计用于 T2V 生成，包括一个将文本和图像嵌入相结合的适配器、一个像素感知交叉注意模块来捕获像素级图像信息、一个 T5 文本编码器以更好地理解运动描述，以及一个 PredictNet 来监督光流。我们进一步提出了一个噪声时间表，它在确保视频生成质量和稳定性方面发挥着关键作用。我们的模型降低了对详细标题和 HQ 视频的要求，并且可以直接在具有噪声和简短标题的有限 LQ 数据集（例如 WebVid-10M）上进行训练，从而大大降低了收集大规模 HQ 视频-文本对的成本。在各种 T2V 和图像到视频生成任务中的大量实验表明了我们提出的 Factorized-Dreamer 的有效性。我们的源代码可在\url{https://github.com/yangxy/Factorized-Dreamer/}获得。</paragraph>

##### **GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization**
2408.10115v1 by Ran Liu, Ming Liu, Min Yu, Jianguo Jiang, Gang Li, Dan Zhang, Jingyuan Li, Xiang Meng, Weiqing Huang

Pre-trained language models are increasingly being used in multi-document
summarization tasks. However, these models need large-scale corpora for
pre-training and are domain-dependent. Other non-neural unsupervised
summarization approaches mostly rely on key sentence extraction, which can lead
to information loss. To address these challenges, we propose a lightweight yet
effective unsupervised approach called GLIMMER: a Graph and LexIcal features
based unsupervised Multi-docuMEnt summaRization approach. It first constructs a
sentence graph from the source documents, then automatically identifies
semantic clusters by mining low-level features from raw texts, thereby
improving intra-cluster correlation and the fluency of generated sentences.
Finally, it summarizes clusters into natural sentences. Experiments conducted
on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach
outperforms existing unsupervised approaches. Furthermore, it surpasses
state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS
and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally,
human evaluations indicate that summaries generated by GLIMMER achieve high
readability and informativeness scores. Our code is available at
https://github.com/Oswald1997/GLIMMER.

摘要：预训练语言模型在多文件摘要任务中被越来越多地使用。然而，这些模型需要大规模语料库进行预训练，并且依赖于领域。其他非神经无监督摘要方法主要依赖于关键句子提取，这可能导致信息丢失。为了应对这些挑战，我们提出了一种轻量级但有效的无监督方法，称为 GLIMMER：一种基于图和词汇特征的无监督多文档摘要方法。它首先从源文档构建一个句子图，然后通过从原始文本中挖掘低级特征自动识别语义簇，从而提高簇内相关性和生成句子的流畅性。最后，它将簇总结为自然句子。在 Multi-News、Multi-XScience 和 DUC-2004 上进行的实验表明，我们的方法优于现有的无监督方法。此外，在零样本设置下，它在 ROUGE 得分方面超越了最先进的预训练多文档摘要模型（例如 PEGASUS 和 PRIMERA）。此外，人类评估表明，GLIMMER 生成的摘要获得了很高的可读性和信息性得分。我们的代码可在 https://github.com/Oswald1997/GLIMMER 获得。

##### **PLUTUS: A Well Pre-trained Large Unified Transformer can Unveil Financial Time Series Regularities**
2408.10111v2 by Yuanjian Xu, Anxian Liu, Jianing Hao, Zhenzhuo Li, Shichang Meng, Guang Zhang

Financial time series modeling is crucial for understanding and predicting
market behaviors but faces challenges such as non-linearity, non-stationarity,
and high noise levels. Traditional models struggle to capture complex patterns
due to these issues, compounded by limitations in computational resources and
model capacity. Inspired by the success of large language models in NLP, we
introduce $\textbf{PLUTUS}$, a $\textbf{P}$re-trained $\textbf{L}$arge
$\textbf{U}$nified $\textbf{T}$ransformer-based model that $\textbf{U}$nveils
regularities in financial time $\textbf{S}$eries. PLUTUS uses an invertible
embedding module with contrastive learning and autoencoder techniques to create
an approximate one-to-one mapping between raw data and patch embeddings.
TimeFormer, an attention based architecture, forms the core of PLUTUS,
effectively modeling high-noise time series. We incorporate a novel attention
mechanisms to capture features across both variable and temporal dimensions.
PLUTUS is pre-trained on an unprecedented dataset of 100 billion observations,
designed to thrive in noisy financial environments. To our knowledge, PLUTUS is
the first open-source, large-scale, pre-trained financial time series model
with over one billion parameters. It achieves state-of-the-art performance in
various tasks, demonstrating strong transferability and establishing a robust
foundational model for finance. Our research provides technical guidance for
pre-training financial time series data, setting a new standard in the field.

摘要：<paragraph>財務時間序列建模對於理解和預測市場行為至關重要，但面臨非線性、非平穩和高雜訊等級等挑戰。由於這些問題，傳統模型難以捕捉複雜模式，並受到計算資源和模型容量的限制。受大型語言模型在 NLP 中成功的啟發，我們引入了 $\textbf{PLUTUS}$，一個 $\textbf{P}$re-trained $\textbf{L}$arge $\textbf{U}$nified $\textbf{T}$ransformer-based 模型，它 $\textbf{U}$nveils 財務時間 $\textbf{S}$eries 中的規律性。PLUTUS 使用可逆嵌入模組，結合對比學習和自動編碼器技術，在原始資料和貼片嵌入之間建立近似的一對一對應。TimeFormer 是一個基於注意力的架構，構成 PLUTUS 的核心，有效地對高雜訊時間序列進行建模。我們納入了一種新穎的注意力機制，以捕捉變數和時間維度中的特徵。PLUTUS 在一個前所未有的 1000 億個觀測值的資料集上進行預訓練，旨在在嘈雜的金融環境中蓬勃發展。據我們所知，PLUTUS 是第一個開源、大規模、預訓練的財務時間序列模型，擁有超過 10 億個參數。它在各種任務中實現了最先進的效能，展示了強大的可轉移性，並為金融建立了一個強大的基礎模型。我們的研究為預訓練財務時間序列資料提供了技術指導，為該領域樹立了新的標準。</paragraph>

##### **Perturb-and-Compare Approach for Detecting Out-of-Distribution Samples in Constrained Access Environments**
2408.10107v1 by Heeyoung Lee, Hoyoon Byun, Changdae Oh, JinYeong Bak, Kyungwoo Song

Accessing machine learning models through remote APIs has been gaining
prevalence following the recent trend of scaling up model parameters for
increased performance. Even though these models exhibit remarkable ability,
detecting out-of-distribution (OOD) samples remains a crucial safety concern
for end users as these samples may induce unreliable outputs from the model. In
this work, we propose an OOD detection framework, MixDiff, that is applicable
even when the model's parameters or its activations are not accessible to the
end user. To bypass the access restriction, MixDiff applies an identical
input-level perturbation to a given target sample and a similar in-distribution
(ID) sample, then compares the relative difference in the model outputs of
these two samples. MixDiff is model-agnostic and compatible with existing
output-based OOD detection methods. We provide theoretical analysis to
illustrate MixDiff's effectiveness in discerning OOD samples that induce
overconfident outputs from the model and empirically demonstrate that MixDiff
consistently enhances the OOD detection performance on various datasets in
vision and text domains.

摘要：透過遠端 API 存取機器學習模型，在最近擴充模型參數以提升效能的趨勢下越來越普遍。儘管這些模型展現出卓越的能力，但偵測出配佈 (OOD) 樣本對於最終使用者來說仍是一項重要的安全問題，因為這些樣本可能會導致模型產生不可靠的輸出。在這項工作中，我們提出一個 OOD 偵測架構 MixDiff，即使模型的參數或其活化對於最終使用者而言無法存取，它也適用。為了繞過存取限制，MixDiff 對給定的目標樣本和類似的配佈內 (ID) 樣本套用相同的輸入層擾動，然後比較這兩個樣本在模型輸出中的相對差異。MixDiff 與模型無關，且與現有的基於輸出的 OOD 偵測方法相容。我們提供理論分析來說明 MixDiff 在辨別會導致模型產生過度自信輸出的 OOD 樣本方面的有效性，並根據經驗證明 MixDiff 在視覺和文字領域的各種資料集上持續提升 OOD 偵測效能。

##### **Convert and Speak: Zero-shot Accent Conversion with Minimum Supervision**
2408.10096v1 by Zhijun Jia, Huaying Xue, Xiulian Peng, Yan Lu

Low resource of parallel data is the key challenge of accent conversion(AC)
problem in which both the pronunciation units and prosody pattern need to be
converted. We propose a two-stage generative framework "convert-and-speak" in
which the conversion is only operated on the semantic token level and the
speech is synthesized conditioned on the converted semantic token with a speech
generative model in target accent domain. The decoupling design enables the
"speaking" module to use massive amount of target accent speech and relieves
the parallel data required for the "conversion" module. Conversion with the
bridge of semantic token also relieves the requirement for the data with text
transcriptions and unlocks the usage of language pre-training technology to
further efficiently reduce the need of parallel accent speech data. To reduce
the complexity and latency of "speaking", a single-stage AR generative model is
designed to achieve good quality as well as lower computation cost. Experiments
on Indian-English to general American-English conversion show that the proposed
framework achieves state-of-the-art performance in accent similarity, speech
quality, and speaker maintenance with only 15 minutes of weakly parallel data
which is not constrained to the same speaker. Extensive experimentation with
diverse accent types suggests that this framework possesses a high degree of
adaptability, making it readily scalable to accommodate other accents with
low-resource data. Audio samples are available at
https://www.microsoft.com/en-us/research/project/convert-and-speak-zero-shot-accent-conversion-with-minimumsupervision/.

摘要：平行資料資源不足是口音轉換 (AC) 問題的主要挑戰，在口音轉換問題中，發音單位和韻律模式都需要轉換。我們提出了一個兩階段生成架構「轉換和說話」，其中轉換僅在語意標記層級上執行，而語音則根據目標口音領域中語音生成模型轉換的語意標記進行合成。解耦設計使「說話」模組能夠使用大量的目標口音語音，並減輕「轉換」模組所需的平行資料。使用語意標記作為橋樑的轉換也減輕了對具有文字轉錄資料的要求，並解鎖了語言預訓練技術的使用，進一步有效地減少了對平行口音語音資料的需求。為了降低「說話」的複雜性和延遲，「單階段 AR 生成模型」被設計為既能達到良好的品質，又能降低運算成本。印度英語到一般美式英語轉換的實驗表明，所提出的架構在口音相似度、語音品質和說話者維護方面達到了最先進的效能，而僅需 15 分鐘的弱平行資料，且不受限於同一個說話者。使用不同口音類型的廣泛實驗表明，此架構具備高度的適應性，使其能夠輕鬆地擴展到使用低資源資料容納其他口音。音訊範例可在 https://www.microsoft.com/en-us/research/project/convert-and-speak-zero-shot-accent-conversion-with-minimumsupervision/ 取得。

##### **ARMADA: Attribute-Based Multimodal Data Augmentation**
2408.10086v1 by Xiaomeng Jin, Jeonghwan Kim, Yu Zhou, Kuan-Hao Huang, Te-Lin Wu, Nanyun Peng, Heng Ji

In Multimodal Language Models (MLMs), the cost of manually annotating
high-quality image-text pair data for fine-tuning and alignment is extremely
high. While existing multimodal data augmentation frameworks propose ways to
augment image-text pairs, they either suffer from semantic inconsistency
between texts and images, or generate unrealistic images, causing knowledge gap
with real world examples. To address these issues, we propose Attribute-based
Multimodal Data Augmentation (ARMADA), a novel multimodal data augmentation
method via knowledge-guided manipulation of visual attributes of the mentioned
entities. Specifically, we extract entities and their visual attributes from
the original text data, then search for alternative values for the visual
attributes under the guidance of knowledge bases (KBs) and large language
models (LLMs). We then utilize an image-editing model to edit the images with
the extracted attributes. ARMADA is a novel multimodal data generation
framework that: (i) extracts knowledge-grounded attributes from symbolic KBs
for semantically consistent yet distinctive image-text pair generation, (ii)
generates visually similar images of disparate categories using neighboring
entities in the KB hierarchy, and (iii) uses the commonsense knowledge of LLMs
to modulate auxiliary visual attributes such as backgrounds for more robust
representation of original entities. Our empirical results over four downstream
tasks demonstrate the efficacy of our framework to produce high-quality data
and enhance the model performance. This also highlights the need to leverage
external knowledge proxies for enhanced interpretability and real-world
grounding.

摘要：在多模態語言模型 (MLM) 中，人工標註高品質影像文字配對資料以進行微調和比對的成本極高。現有的多模態資料擴充架構雖然提出擴充影像文字配對的方法，但它們不是會造成文字和影像之間的語意不一致，就是會產生不切實際的影像，導致與真實世界的範例產生知識鴻溝。為了解決這些問題，我們提出基於屬性的多模態資料擴充 (ARMADA)，這是一種透過引導知識操作提及實體的視覺屬性，來進行多模態資料擴充的新方法。具體來說，我們從原始文字資料中萃取實體及其視覺屬性，然後在知識庫 (KB) 和大型語言模型 (LLM) 的指導下，為視覺屬性尋找替代值。接著，我們利用影像編輯模型來編輯具有萃取屬性的影像。ARMADA 是一個新穎的多模態資料產生架構，它：(i) 從符號 KB 中萃取基於知識的屬性，以產生語意一致但獨特的影像文字配對，(ii) 使用 KB 層級中的鄰近實體，產生視覺上相似的不同類別影像，以及 (iii) 使用 LLM 的常識知識來調整輔助視覺屬性，例如背景，以更穩健地表示原始實體。我們在四個下游任務中的實證結果，證明了我們的架構在產生高品質資料和增強模型效能方面的效力。這也突顯了利用外部知識代理來增強可解釋性和真實世界基礎的需求。

##### **Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning**
2408.10075v1 by Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, Natasha Jaques

Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for
aligning foundation models to human values and preferences. However, current
RLHF techniques cannot account for the naturally occurring differences in
individual human preferences across a diverse population. When these
differences arise, traditional RLHF frameworks simply average over them,
leading to inaccurate rewards and poor performance for individual subgroups. To
address the need for pluralistic alignment, we develop a class of multimodal
RLHF methods. Our proposed techniques are based on a latent variable
formulation - inferring a novel user-specific latent and learning reward models
and policies conditioned on this latent without additional user-specific data.
While conceptually simple, we show that in practice, this reward modeling
requires careful algorithmic considerations around model architecture and
reward scaling. To empirically validate our proposed technique, we first show
that it can provide a way to combat underspecification in simulated control
problems, inferring and optimizing user-specific reward functions. Next, we
conduct experiments on pluralistic language datasets representing diverse user
preferences and demonstrate improved reward function accuracy. We additionally
show the benefits of this probabilistic framework in terms of measuring
uncertainty, and actively learning user preferences. This work enables learning
from diverse populations of users with divergent preferences, an important
challenge that naturally occurs in problems from robot learning to foundation
model alignment.

摘要：人類回饋強化學習 (RLHF) 是一個強大的範例，可以將基礎模型與人類價值觀和偏好保持一致。然而，目前的 RLHF 技術無法說明不同族群中個別人類偏好的自然發生差異。當這些差異出現時，傳統的 RLHF 架構只會對它們進行平均，導致不準確的獎勵和個別子群的表現不佳。為了滿足多元化對齊的需求，我們開發了一類多模態 RLHF 方法。我們提出的技術基於潛在變數公式，推論出新的使用者特定潛在變數，並學習在沒有額外使用者特定資料的情況下，以此潛在變數為條件的獎勵模型和政策。雖然在概念上很簡單，但我們表明在實務上，這種獎勵建模需要仔細考量模型架構和獎勵縮放的演算法考量。為了實證驗證我們提出的技術，我們首先表明它可以提供一種方法來對抗模擬控制問題中的規格不足，推論和最佳化使用者特定的獎勵函數。接下來，我們對代表不同使用者偏好的多元語言資料集進行實驗，並展示改進的獎勵函數準確度。我們另外說明這種機率架構在衡量不確定性和主動學習使用者偏好方面的優點。這項工作能夠從具有不同偏好的不同使用者群體中學習，這是一個在機器人學習到基礎模型對齊問題中自然發生的重要挑戰。

##### **Synthesis of Reward Machines for Multi-Agent Equilibrium Design (Full Version)**
2408.10074v1 by Muhammad Najib, Giuseppe Perelli

Mechanism design is a well-established game-theoretic paradigm for designing
games to achieve desired outcomes. This paper addresses a closely related but
distinct concept, equilibrium design. Unlike mechanism design, the designer's
authority in equilibrium design is more constrained; she can only modify the
incentive structures in a given game to achieve certain outcomes without the
ability to create the game from scratch. We study the problem of equilibrium
design using dynamic incentive structures, known as reward machines. We use
weighted concurrent game structures for the game model, with goals (for the
players and the designer) defined as mean-payoff objectives. We show how reward
machines can be used to represent dynamic incentives that allocate rewards in a
manner that optimises the designer's goal. We also introduce the main decision
problem within our framework, the payoff improvement problem. This problem
essentially asks whether there exists a dynamic incentive (represented by some
reward machine) that can improve the designer's payoff by more than a given
threshold value. We present two variants of the problem: strong and weak. We
demonstrate that both can be solved in polynomial time using a Turing machine
equipped with an NP oracle. Furthermore, we also establish that these variants
are either NP-hard or coNP-hard. Finally, we show how to synthesise the
corresponding reward machine if it exists.

摘要：機制設計是一種完善的博弈論範例，用於設計遊戲以達成預期的結果。本文探討一個密切相關但不同的概念，均衡設計。與機制設計不同，設計者在均衡設計中的權限受到更多限制；她只能修改既有遊戲中的誘因結構以達成特定結果，而無法從頭開始建立遊戲。我們使用稱為獎勵機器的動態誘因結構來研究均衡設計問題。我們使用加權並行遊戲結構作為遊戲模型，目標（對於玩家和設計者）定義為平均收益目標。我們展示獎勵機器如何用於表示動態誘因，以最佳化設計者目標的方式分配獎勵。我們也在我們的架構中引入主要的決策問題，報酬改善問題。這個問題基本上在詢問是否存在一個動態誘因（由某個獎勵機器表示），可以將設計者的報酬改善超過給定的閾值。我們提出此問題的兩個變體：強式和弱式。我們證明這兩個變體都可以使用配備 NP oracle 的圖靈機在多項式時間內解決。此外，我們也確立這些變體不是 NP-hard 就是 coNP-hard。最後，我們展示如何合成對應的獎勵機器（如果存在）。

##### **FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant**
2408.10072v1 by Zhengchao Huang, Bin Xia, Zicheng Lin, Zhun Mou, Wenming Yang

The rapid advancement of deepfake technologies has sparked widespread public
concern, particularly as face forgery poses a serious threat to public
information security. However, the unknown and diverse forgery techniques,
varied facial features and complex environmental factors pose significant
challenges for face forgery analysis. Existing datasets lack descriptions of
these aspects, making it difficult for models to distinguish between real and
forged faces using only visual information amid various confounding factors. In
addition, existing methods do not yield user-friendly and explainable results,
complicating the understanding of the model's decision-making process. To
address these challenges, we introduce a novel Open-World Face Forgery Analysis
VQA (OW-FFA-VQA) task and the corresponding benchmark. To tackle this task, we
first establish a dataset featuring a diverse collection of real and forged
face images with essential descriptions and reliable forgery reasoning. Base on
this dataset, we introduce FFAA: Face Forgery Analysis Assistant, consisting of
a fine-tuned Multimodal Large Language Model (MLLM) and Multi-answer
Intelligent Decision System (MIDS). By integrating hypothetical prompts with
MIDS, the impact of fuzzy classification boundaries is effectively mitigated,
enhancing the model's robustness. Extensive experiments demonstrate that our
method not only provides user-friendly explainable results but also
significantly boosts accuracy and robustness compared to previous methods.

摘要：深度偽造技術的快速進展引發了廣泛的公眾關注，特別是當面部偽造對公共信息安全構成嚴重威脅時。然而，未知且多樣化的偽造技術、多變的面部特徵和複雜的環境因素對面部偽造分析構成重大挑戰。現有的數據集缺乏對這些方面的描述，這使得模型難以在各種混雜因素中僅使用視覺信息來區分真實面孔和偽造面孔。此外，現有方法無法產生用戶友好的可解釋結果，這使得理解模型的決策過程變得複雜。為了應對這些挑戰，我們引入了一項新穎的開放世界面部偽造分析 VQA（OW-FFA-VQA）任務和相應的基準。為了應對這項任務，我們首先建立了一個數據集，其中包含大量真實和偽造的面部圖像，並附有必要的描述和可靠的偽造推理。在此數據集的基礎上，我們引入了 FFAA：面部偽造分析助手，它由微調的多模態大型語言模型 (MLLM) 和多答案智能決策系統 (MIDS) 組成。通過將假設提示與 MIDS 集成，模糊分類邊界的影響得到有效緩解，從而增強了模型的魯棒性。大量的實驗表明，與之前的技術相比，我們的技術不僅提供了用戶友好的可解釋結果，而且還顯著提高了準確性和魯棒性。

##### **Facial Wrinkle Segmentation for Cosmetic Dermatology: Pretraining with Texture Map-Based Weak Supervision**
2408.10060v1 by Junho Moon, Haejun Chung, Ikbeom Jang

Facial wrinkle detection plays a crucial role in cosmetic dermatology.
Precise manual segmentation of facial wrinkles is challenging and
time-consuming, with inherent subjectivity leading to inconsistent results
among graders. To address this issue, we propose two solutions. First, we build
and release the first public facial wrinkle dataset, `FFHQ-Wrinkle', an
extension of the NVIDIA FFHQ dataset. This dataset includes 1,000 images with
human labels and 50,000 images with automatically generated weak labels. This
dataset can foster the research community to develop advanced wrinkle detection
algorithms. Second, we introduce a training strategy for U-Net-like
encoder-decoder models to detect wrinkles across the face automatically. Our
method employs a two-stage training strategy: texture map pretraining and
finetuning on human-labeled data. Initially, we pretrain models on a large
dataset with weak labels (N=50k) or masked texture maps generated through
computer vision techniques, without human intervention. Subsequently, we
finetune the models using human-labeled data (N=1k), which consists of manually
labeled wrinkle masks. During finetuning, the network inputs a combination of
RGB and masked texture maps, comprising four channels. We effectively combine
labels from multiple annotators to minimize subjectivity in manual labeling.
Our strategies demonstrate improved segmentation performance in facial wrinkle
segmentation both quantitatively and visually compared to existing pretraining
methods.

摘要：人臉皺紋偵測在美容皮膚科扮演著至關重要的角色。
精確手動分割人臉皺紋具有挑戰性且耗時，內在的主觀性導致評分員之間產生不一致的結果。為了解決這個問題，我們提出了兩個解決方案。首先，我們建立並發布了第一個公開人臉皺紋數據集，即「FFHQ-Wrinkle」，NVIDIA FFHQ 數據集的延伸。這個數據集包含 1,000 張具有真人標籤的圖像和 50,000 張具有自動生成弱標籤的圖像。這個數據集可以促進研究社群開發進階的皺紋偵測演算法。其次，我們針對 U-Net 型編碼器-解碼器模型引入了訓練策略，以自動偵測整張臉的皺紋。我們的技術採用了兩階段訓練策略：紋理貼圖預訓練和針對真人標籤資料進行微調。最初，我們在具有弱標籤（N=50k）的大型數據集或透過電腦視覺技術產生的遮罩紋理貼圖上預訓練模型，不進行人工干預。隨後，我們使用真人標籤資料（N=1k）對模型進行微調，這些資料包含手動標籤的皺紋遮罩。在微調過程中，網路輸入 RGB 和遮罩紋理貼圖的組合，包含四個通道。我們有效地結合了多個標記員的標籤，以最小化手動標籤中的主觀性。與現有的預訓練方法相比，我們的策略在人臉皺紋分割中展現了定量和視覺上改善的分割效能。

##### **Privacy Checklist: Privacy Violation Detection Grounding on Contextual Integrity Theory**
2408.10053v1 by Haoran Li, Wei Fan, Yulin Chen, Jiayang Cheng, Tianshu Chu, Xuebing Zhou, Peizhao Hu, Yangqiu Song

Privacy research has attracted wide attention as individuals worry that their
private data can be easily leaked during interactions with smart devices,
social platforms, and AI applications. Computer science researchers, on the
other hand, commonly study privacy issues through privacy attacks and defenses
on segmented fields. Privacy research is conducted on various sub-fields,
including Computer Vision (CV), Natural Language Processing (NLP), and Computer
Networks. Within each field, privacy has its own formulation. Though pioneering
works on attacks and defenses reveal sensitive privacy issues, they are
narrowly trapped and cannot fully cover people's actual privacy concerns.
Consequently, the research on general and human-centric privacy research
remains rather unexplored. In this paper, we formulate the privacy issue as a
reasoning problem rather than simple pattern matching. We ground on the
Contextual Integrity (CI) theory which posits that people's perceptions of
privacy are highly correlated with the corresponding social context. Based on
such an assumption, we develop the first comprehensive checklist that covers
social identities, private attributes, and existing privacy regulations. Unlike
prior works on CI that either cover limited expert annotated norms or model
incomplete social context, our proposed privacy checklist uses the whole Health
Insurance Portability and Accountability Act of 1996 (HIPAA) as an example, to
show that we can resort to large language models (LLMs) to completely cover the
HIPAA's regulations. Additionally, our checklist also gathers expert
annotations across multiple ontologies to determine private information
including but not limited to personally identifiable information (PII). We use
our preliminary results on the HIPAA to shed light on future context-centric
privacy research to cover more privacy regulations, social norms and standards.

摘要：隱私研究備受關注，因為個人擔心在與智慧裝置、社群平台和 AI 應用程式互動時，他們的私人資料可能會輕易外洩。另一方面，電腦科學研究人員通常透過分段領域的隱私攻擊和防禦來研究隱私問題。隱私研究涵蓋各種子領域，包括電腦視覺 (CV)、自然語言處理 (NLP) 和電腦網路。在每個領域中，隱私都有其自身的表述。儘管關於攻擊和防禦的開創性研究揭示了敏感的隱私問題，但它們卻受到嚴格限制，無法完全涵蓋人們的實際隱私問題。因此，關於一般且以人為中心的隱私研究仍未被充分探討。在本文中，我們將隱私問題表述為一個推理問題，而不是簡單的模式配對。我們以情境完整性 (CI) 理論為基礎，該理論假設人們對隱私的看法與對應的社會背景高度相關。基於這樣的假設，我們制定了第一份全面的清單，涵蓋社會身分、私人屬性和現有隱私法規。與先前僅涵蓋有限的專家註解規範或模擬不完整社會背景的 CI 研究不同，我們提出的隱私清單使用 1996 年健保可攜性和責任法案 (HIPAA) 作為範例，以表明我們可以採用大型語言模型 (LLM) 來完全涵蓋 HIPAA 的法規。此外，我們的清單還收集了跨多個本体的多位專家註解，以確定私人資訊，包括但不限於個人可辨識資訊 (PII)。我們使用我們在 HIPAA 上的初步結果，為未來的以情境為中心的隱私研究提供啟示，以涵蓋更多的隱私法規、社會規範和標準。

##### **MSDiagnosis: An EMR-based Dataset for Clinical Multi-Step Diagnosis**
2408.10039v1 by Ruihui Hou, Shencheng Chen, Yongqi Fan, Lifeng Zhu, Jing Sun, Jingping Liu, Tong Ruan

Clinical diagnosis is critical in medical practice, typically requiring a
continuous and evolving process that includes primary diagnosis, differential
diagnosis, and final diagnosis. However, most existing clinical diagnostic
tasks are single-step processes, which does not align with the complex
multi-step diagnostic procedures found in real-world clinical settings. In this
paper, we propose a multi-step diagnostic task and annotate a clinical
diagnostic dataset (MSDiagnosis). This dataset includes primary diagnosis,
differential diagnosis, and final diagnosis questions. Additionally, we propose
a novel and effective framework. This framework combines forward inference,
backward inference, reflection, and refinement, enabling the LLM to
self-evaluate and adjust its diagnostic results. To assess the effectiveness of
our proposed method, we design and conduct extensive experiments. The
experimental results demonstrate the effectiveness of the proposed method. We
also provide a comprehensive experimental analysis and suggest future research
directions for this task.

摘要：臨床診斷在醫療實務中至關重要，通常需要一個連續且不斷演進的過程，包括初步診斷、鑑別診斷和最終診斷。然而，現有的臨床診斷任務大多是單步驟的過程，與實際臨床環境中發現的複雜多步驟診斷程序不符。在本文中，我們提出了一項多步驟的診斷任務，並標註了一個臨床診斷資料集 (MSDiagnosis)。此資料集包含初步診斷、鑑別診斷和最終診斷的問題。此外，我們提出了一個新穎且有效的架構。此架構結合了前向推理、後向推理、反思和改善，使 LLM 能自我評估和調整其診斷結果。為了評估我們提出的方法的有效性，我們設計並進行了廣泛的實驗。實驗結果證明了所提出方法的有效性。我們還提供了全面的實驗分析，並建議了此任務的未來研究方向。

##### **Towards a Knowledge Graph for Models and Algorithms in Applied Mathematics**
2408.10003v1 by Björn Schembera, Frank Wübbeling, Hendrik Kleikamp, Burkhard Schmidt, Aurela Shehu, Marco Reidelbach, Christine Biedinger, Jochen Fiedler, Thomas Koprucki, Dorothea Iglezakis, Dominik Göddeke

Mathematical models and algorithms are an essential part of mathematical
research data, as they are epistemically grounding numerical data. In order to
represent models and algorithms as well as their relationship semantically to
make this research data FAIR, two previously distinct ontologies were merged
and extended, becoming a living knowledge graph. The link between the two
ontologies is established by introducing computational tasks, as they occur in
modeling, corresponding to algorithmic tasks. Moreover, controlled vocabularies
are incorporated and a new class, distinguishing base quantities from specific
use case quantities, was introduced. Also, both models and algorithms can now
be enriched with metadata. Subject-specific metadata is particularly relevant
here, such as the symmetry of a matrix or the linearity of a mathematical
model. This is the only way to express specific workflows with concrete models
and algorithms, as the feasible solution algorithm can only be determined if
the mathematical properties of a model are known. We demonstrate this using two
examples from different application areas of applied mathematics. In addition,
we have already integrated over 250 research assets from applied mathematics
into our knowledge graph.

摘要：數學模型和演算法是數學研究資料的必要部分，因為它們是認識論上基礎的數值資料。為了表示模型和演算法以及它們的語義關係，以使此研究資料 FAIR，兩個先前不同的本体被合併並擴充，成為一個活的知識圖譜。兩個本体之間的連結是透過引入運算任務來建立的，因為它們出現在建模中，對應於演算法任務。此外，受控詞彙被納入，並引入一個新的類別，區分基本量和特定用例量。此外，模型和演算法現在都可以用元資料豐富化。特定主題的元資料在此特別相關，例如矩陣的對稱性或數學模型的線性。這是使用具體模型和演算法表達特定工作流程的唯一方法，因為只有在已知模型的數學特性時，才能確定可行的解決演算法。我們使用應用數學不同應用領域的兩個範例來說明這一點。此外，我們已經將超過 250 個應用數學研究資產整合到我們的知識圖譜中。

##### **Edge-Cloud Collaborative Motion Planning for Autonomous Driving with Large Language Models**
2408.09972v1 by Jiao Chen, Suyan Dai, Fangfang Chen, Zuohong Lv, Jianhua Tang

Integrating large language models (LLMs) into autonomous driving enhances
personalization and adaptability in open-world scenarios. However, traditional
edge computing models still face significant challenges in processing complex
driving data, particularly regarding real-time performance and system
efficiency. To address these challenges, this study introduces EC-Drive, a
novel edge-cloud collaborative autonomous driving system with data drift
detection capabilities. EC-Drive utilizes drift detection algorithms to
selectively upload critical data, including new obstacles and traffic pattern
changes, to the cloud for processing by GPT-4, while routine data is
efficiently managed by smaller LLMs on edge devices. This approach not only
reduces inference latency but also improves system efficiency by optimizing
communication resource use. Experimental validation confirms the system's
robust processing capabilities and practical applicability in real-world
driving conditions, demonstrating the effectiveness of this edge-cloud
collaboration framework. Our data and system demonstration will be released at
https://sites.google.com/view/ec-drive.

摘要：將大型語言模型 (LLM) 整合至自動駕駛中，可在開放世界場景中增強個人化和適應性。然而，傳統的邊緣運算模型在處理複雜的駕駛資料時仍面臨重大挑戰，特別是在即時效能和系統效率方面。為了解決這些挑戰，本研究引入了 EC-Drive，這是一個具備資料漂移偵測功能的新穎邊緣雲協作自動駕駛系統。EC-Drive 使用漂移偵測演算法，選擇性上傳關鍵資料（包括新障礙物和交通模式變更）至雲端，由 GPT-4 進行處理，而例行資料則由邊緣裝置上較小的 LLM 有效管理。此方法不僅減少了推論延遲，也透過最佳化通訊資源使用來提升系統效率。實驗驗證確認了系統在真實世界駕駛條件下的強健處理能力和實用性，證明了此邊緣雲協作架構的有效性。我們的資料和系統示範將於 https://sites.google.com/view/ec-drive 發布。

##### **Unsupervised Machine Learning Hybrid Approach Integrating Linear Programming in Loss Function: A Robust Optimization Technique**
2408.09967v1 by Andrew Kiruluta, Andreas Lemos

This paper presents a novel hybrid approach that integrates linear
programming (LP) within the loss function of an unsupervised machine learning
model. By leveraging the strengths of both optimization techniques and machine
learning, this method introduces a robust framework for solving complex
optimization problems where traditional methods may fall short. The proposed
approach encapsulates the constraints and objectives of a linear programming
problem directly into the loss function, guiding the learning process to adhere
to these constraints while optimizing the desired outcomes. This technique not
only preserves the interpretability of linear programming but also benefits
from the flexibility and adaptability of machine learning, making it
particularly well-suited for unsupervised or semi-supervised learning
scenarios.

摘要：本文提出了一種創新的混合方法，將線性規劃 (LP) 整合到非監督機器學習模型的損失函數中。通過利用優化技術和機器學習的優勢，此方法引入了一個強大的框架，用於解決傳統方法可能無法解決的複雜優化問題。所提出的方法將線性規劃問題的約束和目標直接封裝到損失函數中，指導學習過程遵循這些約束，同時優化所需的結果。此技術不僅保留了線性規劃的可解釋性，而且還受益於機器學習的靈活性與適應性，使其特別適合非監督或半監督學習場景。

##### **Contextual Importance and Utility in Python: New Functionality and Insights with the py-ciu Package**
2408.09957v1 by Kary Främling

The availability of easy-to-use and reliable software implementations is
important for allowing researchers in academia and industry to test, assess and
take into use eXplainable AI (XAI) methods. This paper describes the
\texttt{py-ciu} Python implementation of the Contextual Importance and Utility
(CIU) model-agnostic, post-hoc explanation method and illustrates capabilities
of CIU that go beyond the current state-of-the-art that could be useful for XAI
practitioners in general.

摘要：易於使用且可靠的軟體實作對於讓學術界和業界的研究人員測試、評估和使用可解釋 AI (XAI) 方法非常重要。本文說明 Contextual Importance and Utility (CIU) 模型不可知、事後解釋方法的 \texttt{py-ciu} Python 實作，並說明 CIU 的功能，這些功能超越了目前對 XAI 從業人員整體而言可能很有用的最新技術。

##### **Weakly Supervised Pretraining and Multi-Annotator Supervised Finetuning for Facial Wrinkle Detection**
2408.09952v1 by Ik Jun Moon, Junho Moon, Ikbeom Jang

1. Research question: With the growing interest in skin diseases and skin
aesthetics, the ability to predict facial wrinkles is becoming increasingly
important. This study aims to evaluate whether a computational model,
convolutional neural networks (CNN), can be trained for automated facial
wrinkle segmentation. 2. Findings: Our study presents an effective technique
for integrating data from multiple annotators and illustrates that transfer
learning can enhance performance, resulting in dependable segmentation of
facial wrinkles. 3. Meaning: This approach automates intricate and
time-consuming tasks of wrinkle analysis with a deep learning framework. It
could be used to facilitate skin treatments and diagnostics.

摘要：1. 研究問題：隨著對皮膚疾病和皮膚美學的興趣日益濃厚，預測面部皺紋的能力變得越來越重要。本研究旨在評估是否可以訓練計算模型、卷積神經網路 (CNN) 進行自動化面部皺紋分割。2. 研究結果：我們的研究提出了一種整合多個註解者資料的有效技術，並說明遷移學習可以提升效能，進而可靠地分割面部皺紋。3. 意義：此方法自動化了深度學習架構中複雜且耗時的皺紋分析任務。它可用於促進皮膚治療和診斷。

##### **Principle Driven Parameterized Fiber Model based on GPT-PINN Neural Network**
2408.09951v1 by Yubin Zang, Boyu Hua, Zhenzhou Tang, Zhipeng Lin, Fangzheng Zhang, Simin Li, Zuxing Zhang, Hongwei Chen

In cater the need of Beyond 5G communications, large numbers of data driven
artificial intelligence based fiber models has been put forward as to utilize
artificial intelligence's regression ability to predict pulse evolution in
fiber transmission at a much faster speed compared with the traditional split
step Fourier method. In order to increase the physical interpretabiliy,
principle driven fiber models have been proposed which inserts the Nonlinear
Schodinger Equation into their loss functions. However, regardless of either
principle driven or data driven models, they need to be re-trained the whole
model under different transmission conditions. Unfortunately, this situation
can be unavoidable when conducting the fiber communication optimization work.
If the scale of different transmission conditions is large, then the whole
model needs to be retrained large numbers of time with relatively large scale
of parameters which may consume higher time costs. Computing efficiency will be
dragged down as well. In order to address this problem, we propose the
principle driven parameterized fiber model in this manuscript. This model
breaks down the predicted NLSE solution with respect to one set of transmission
condition into the linear combination of several eigen solutions which were
outputted by each pre-trained principle driven fiber model via the reduced
basis method. Therefore, the model can greatly alleviate the heavy burden of
re-training since only the linear combination coefficients need to be found
when changing the transmission condition. Not only strong physical
interpretability can the model posses, but also higher computing efficiency can
be obtained. Under the demonstration, the model's computational complexity is
0.0113% of split step Fourier method and 1% of the previously proposed
principle driven fiber model.

摘要：<paragraph>為了滿足 5G 通訊的需求，大量資料驅動的人工智慧光纖模型已被提出，以利用人工智慧的回歸能力來預測光纖傳輸中的脈衝演化，速度比傳統的分裂步驟傅立葉方法快得多。為了提高物理可解釋性，已經提出了原理驅動的光纖模型，將非線性薛丁格方程式插入其損失函數中。然而，無論是原理驅動模型還是資料驅動模型，都需要在不同的傳輸條件下重新訓練整個模型。不幸的是，在進行光纖通信優化工作時，這種情況是不可避免的。如果不同傳輸條件的規模很大，那麼整個模型需要使用相對大規模的參數進行大量重新訓練，這可能會消耗更高的時間成本。計算效率也會隨之下降。為了解決這個問題，我們在這個手稿中提出了原理驅動參數化光纖模型。此模型將關於一組傳輸條件的預測 NLSE 解決方案分解為幾個特徵解的線性組合，這些特徵解是由每個預先訓練的原理驅動光纖模型通過簡約基方法輸出的。因此，該模型可以大大減輕重新訓練的沉重負擔，因為在改變傳輸條件時只需要找到線性組合係數。該模型不僅可以擁有強大的物理可解釋性，還可以獲得更高的計算效率。在演示中，該模型的計算複雜度為分裂步驟傅立葉方法的 0.0113%，之前提出的原理驅動光纖模型的 1%。</paragraph>

##### **C${^2}$RL: Content and Context Representation Learning for Gloss-free Sign Language Translation and Retrieval**
2408.09949v1 by Zhigang Chen, Benjia Zhou, Yiqing Huang, Jun Wan, Yibo Hu, Hailin Shi, Yanyan Liang, Zhen Lei, Du Zhang

Sign Language Representation Learning (SLRL) is crucial for a range of sign
language-related downstream tasks such as Sign Language Translation (SLT) and
Sign Language Retrieval (SLRet). Recently, many gloss-based and gloss-free SLRL
methods have been proposed, showing promising performance. Among them, the
gloss-free approach shows promise for strong scalability without relying on
gloss annotations. However, it currently faces suboptimal solutions due to
challenges in encoding the intricate, context-sensitive characteristics of sign
language videos, mainly struggling to discern essential sign features using a
non-monotonic video-text alignment strategy. Therefore, we introduce an
innovative pretraining paradigm for gloss-free SLRL, called C${^2}$RL, in this
paper. Specifically, rather than merely incorporating a non-monotonic semantic
alignment of video and text to learn language-oriented sign features, we
emphasize two pivotal aspects of SLRL: Implicit Content Learning (ICL) and
Explicit Context Learning (ECL). ICL delves into the content of communication,
capturing the nuances, emphasis, timing, and rhythm of the signs. In contrast,
ECL focuses on understanding the contextual meaning of signs and converting
them into equivalent sentences. Despite its simplicity, extensive experiments
confirm that the joint optimization of ICL and ECL results in robust sign
language representation and significant performance gains in gloss-free SLT and
SLRet tasks. Notably, C${^2}$RL improves the BLEU-4 score by +5.3 on P14T,
+10.6 on CSL-daily, +6.2 on OpenASL, and +1.3 on How2Sign. It also boosts the
R@1 score by +8.3 on P14T, +14.4 on CSL-daily, and +5.9 on How2Sign.
Additionally, we set a new baseline for the OpenASL dataset in the SLRet task.

摘要：手語表徵學習 (SLRL) 對於一系列手語相關的下游任務至關重要，例如手語翻譯 (SLT) 和手語檢索 (SLRet)。最近，已經提出了許多基於手勢和不基於手勢的 SLRL 方法，表現出令人滿意的效能。其中，不基於手勢的方法顯示出強大的可擴充性，而不依賴手勢註解。然而，由於在編碼手語影片的複雜、對情境敏感的特徵時遇到挑戰，目前面臨次佳的解決方案，主要在於使用非單調的影片文字對齊策略來辨別必要的符號特徵。因此，我們在本文中介紹了一種創新的無手勢 SLRL 預訓練範例，稱為 C${^2}$RL。具體來說，我們不僅僅將影片和文字的非單調語義對齊整合起來以學習以語言為導向的手勢特徵，我們還強調了 SLRL 的兩個關鍵方面：隱含內容學習 (ICL) 和明確情境學習 (ECL)。ICL 深入探討溝通的內容，捕捉手勢的細微差別、強調、時機和節奏。相比之下，ECL 專注於理解手勢的上下文含義，並將其轉換為等效的句子。儘管其簡單性，但大量的實驗證實，ICL 和 ECL 的聯合最佳化會產生強健的手語表徵，並在無手勢 SLT 和 SLRet 任務中顯著提升效能。值得注意的是，C${^2}$RL 在 P14T 上將 BLEU-4 分數提高了 +5.3，在 CSL-daily 上提高了 +10.6，在 OpenASL 上提高了 +6.2，在 How2Sign 上提高了 +1.3。它還將 P14T 上的 R@1 分數提高了 +8.3，CSL-daily 上提高了 +14.4，How2Sign 上提高了 +5.9。此外，我們在 SLRet 任務中為 OpenASL 資料集設定了一個新的基準。

##### **Caption-Driven Explorations: Aligning Image and Text Embeddings through Human-Inspired Foveated Vision**
2408.09948v1 by Dario Zanca, Andrea Zugarini, Simon Dietz, Thomas R. Altstidl, Mark A. Turban Ndjeuha, Leo Schwinn, Bjoern Eskofier

Understanding human attention is crucial for vision science and AI. While
many models exist for free-viewing, less is known about task-driven image
exploration. To address this, we introduce CapMIT1003, a dataset with captions
and click-contingent image explorations, to study human attention during the
captioning task. We also present NevaClip, a zero-shot method for predicting
visual scanpaths by combining CLIP models with NeVA algorithms. NevaClip
generates fixations to align the representations of foveated visual stimuli and
captions. The simulated scanpaths outperform existing human attention models in
plausibility for captioning and free-viewing tasks. This research enhances the
understanding of human attention and advances scanpath prediction models.

摘要：了解人類的注意力對於視覺科學和 AI 至關重要。雖然有許多自由視覺模型，但對於任務驅動的影像探索卻知之甚少。為了解決這個問題，我們引入了 CapMIT1003，一個帶有標題和點擊相關影像探索的資料集，用於研究標題任務期間的人類注意力。我們還提出了 NevaClip，一種零次學習方法，透過將 CLIP 模型與 NeVA 演算法結合來預測視覺掃描路徑。NevaClip 會產生注視點，以對齊注視視覺刺激和標題的表徵。模擬掃描路徑在標題和自由視覺任務的合理性上優於現有的人類注意力模型。這項研究增進了對人類注意力的理解，並推動了掃描路徑預測模型的進步。

##### **Fiber Transmission Model with Parameterized Inputs based on GPT-PINN Neural Network**
2408.09947v1 by Yubin Zang, Boyu Hua, Zhipeng Lin, Fangzheng Zhang, Simin Li, Zuxing Zhang, Hongwei Chen

In this manuscript, a novelty principle driven fiber transmission model for
short-distance transmission with parameterized inputs is put forward. By taking
into the account of the previously proposed principle driven fiber model, the
reduced basis expansion method and transforming the parameterized inputs into
parameterized coefficients of the Nonlinear Schrodinger Equations, universal
solutions with respect to inputs corresponding to different bit rates can all
be obtained without the need of re-training the whole model. This model, once
adopted, can have prominent advantages in both computation efficiency and
physical background. Besides, this model can still be effectively trained
without the needs of transmitted signals collected in advance. Tasks of on-off
keying signals with bit rates ranging from 2Gbps to 50Gbps are adopted to
demonstrate the fidelity of the model.

摘要：在本文中，提出了一个以新穎原理驅動的纖維傳輸模型，用於帶有參數化輸入的短距離傳輸。通過考慮先前提出的原理驅動纖維模型、簡約基數擴展方法，並將參數化輸入轉換為非線性薛丁格方程的參數化係數，可以獲得關於不同比特率對應輸入的通用解，而無需重新訓練整個模型。一旦採用此模型，它在計算效率和物理背景方面都具有顯著的優勢。此外，此模型仍然可以有效地進行訓練，而無需預先收集傳輸信號。採用比特率從 2Gbps 到 50Gbps 的開關鍵控信號任務來證明模型的保真度。

##### **Microscopic Analysis on LLM players via Social Deduction Game**
2408.09946v1 by Byungjun Kim, Dayeon Seo, Bugeun Kim

Recent studies have begun developing autonomous game players for social
deduction games using large language models (LLMs). When building LLM players,
fine-grained evaluations are crucial for addressing weaknesses in game-playing
abilities. However, existing studies have often overlooked such assessments.
Specifically, we point out two issues with the evaluation methods employed.
First, game-playing abilities have typically been assessed through game-level
outcomes rather than specific event-level skills; Second, error analyses have
lacked structured methodologies. To address these issues, we propose an
approach utilizing a variant of the SpyFall game, named SpyGame. We conducted
an experiment with four LLMs, analyzing their gameplay behavior in SpyGame both
quantitatively and qualitatively. For the quantitative analysis, we introduced
eight metrics to resolve the first issue, revealing that these metrics are more
effective than existing ones for evaluating the two critical skills: intent
identification and camouflage. In the qualitative analysis, we performed
thematic analysis to resolve the second issue. This analysis identifies four
major categories that affect gameplay of LLMs. Additionally, we demonstrate how
these categories complement and support the findings from the quantitative
analysis.

摘要：最近的研究已經開始使用大型語言模型 (LLM) 為社交推理遊戲開發自動遊戲玩家。在建構 LLM 玩家時，細緻的評估對於解決遊戲玩法能力中的弱點至關重要。然而，現有的研究常常忽略了這種評估。具體來說，我們指出了評估方法中存在的兩個問題。首先，遊戲玩法能力通常透過遊戲層級的結果來評估，而不是具體的事件層級技能；其次，錯誤分析缺乏結構化的方法。為了解決這些問題，我們提出了一種利用 SpyFall 遊戲變體，名為 SpyGame 的方法。我們對四個 LLM 進行了一項實驗，定量和定性地分析了它們在 SpyGame 中的遊戲行為。對於定量分析，我們引入了八個指標來解決第一個問題，表明這些指標比現有的指標更有效地評估了兩個關鍵技能：意圖識別和偽裝。在定性分析中，我們進行了主題分析來解決第二個問題。此分析識別出影響 LLM 遊戲玩法的四個主要類別。此外，我們展示了這些類別如何補充和支持定量分析的結果。

##### **Benchmarking LLMs for Translating Classical Chinese Poetry:Evaluating Adequacy, Fluency, and Elegance**
2408.09945v1 by Andong Chen, Lianzhang Lou, Kehai Chen, Xuefeng Bai, Yang Xiang, Muyun Yang, Tiejun Zhao, Min Zhang

Large language models (LLMs) have shown remarkable performance in general
translation tasks. However, the increasing demand for high-quality translations
that are not only adequate but also fluent and elegant. To assess the extent to
which current LLMs can meet these demands, we introduce a suitable benchmark
for translating classical Chinese poetry into English. This task requires not
only adequacy in translating culturally and historically significant content
but also a strict adherence to linguistic fluency and poetic elegance. Our
study reveals that existing LLMs fall short of this task. To address these
issues, we propose RAT, a \textbf{R}etrieval-\textbf{A}ugmented machine
\textbf{T}ranslation method that enhances the translation process by
incorporating knowledge related to classical poetry. Additionally, we propose
an automatic evaluation metric based on GPT-4, which better assesses
translation quality in terms of adequacy, fluency, and elegance, overcoming the
limitations of traditional metrics. Our dataset and code will be made
available.

摘要：大型語言模型 (LLM) 在一般翻譯任務中展現了卓越的表現。然而，對於高品質翻譯的需求與日俱增，這些翻譯不僅要準確，還要流暢且優雅。為了評估目前 LLM 滿足這些需求的程度，我們針對將古典中文詩歌翻譯成英文這項任務，引進了一個合適的基準。這項任務不僅需要準確翻譯具有文化和歷史意義的內容，還必須嚴格遵守語言流暢性和詩歌優雅性。我們的研究顯示，現有的 LLM 無法勝任這項任務。為了解決這些問題，我們提出了 RAT，一種透過納入與古典詩歌相關的知識，來增強翻譯流程的「檢索輔助機器翻譯」方法。此外，我們提出了一種基於 GPT-4 的自動評估指標，它在充分性、流暢性和優雅性方面，能更好地評估翻譯品質，克服了傳統指標的限制。我們的資料集和程式碼將會公開。

##### **SZU-AFS Antispoofing System for the ASVspoof 5 Challenge**
2408.09933v1 by Yuxiong Xu, Jiafeng Zhong, Sengui Zheng, Zefeng Liu, Bin Li

This paper presents the SZU-AFS anti-spoofing system, designed for Track 1 of
the ASVspoof 5 Challenge under open conditions. The system is built with four
stages: selecting a baseline model, exploring effective data augmentation (DA)
methods for fine-tuning, applying a co-enhancement strategy based on gradient
norm aware minimization (GAM) for secondary fine-tuning, and fusing logits
scores from the two best-performing fine-tuned models. The system utilizes the
Wav2Vec2 front-end feature extractor and the AASIST back-end classifier as the
baseline model. During model fine-tuning, three distinct DA policies have been
investigated: single-DA, random-DA, and cascade-DA. Moreover, the employed
GAM-based co-enhancement strategy, designed to fine-tune the augmented model at
both data and optimizer levels, helps the Adam optimizer find flatter minima,
thereby boosting model generalization. Overall, the final fusion system
achieves a minDCF of 0.115 and an EER of 4.04% on the evaluation set.

摘要：本論文提出 SZU-AFS 防欺騙系統，專為 ASVspoof 5 挑戰的軌道 1 設計，在開放條件下。該系統分為四個階段：選擇基準模型、探索用於微調的有效資料擴充 (DA) 方法、應用基於梯度範數感知最小化 (GAM) 的共增強策略進行二次微調，以及融合來自兩個效能最佳微調模型的 logit 分數。該系統利用 Wav2Vec2 前端特徵萃取器和 AASIST 後端分類器作為基準模型。在模型微調期間，已探討了三種不同的 DA 策略：單一 DA、隨機 DA 和串聯 DA。此外，所採用的基於 GAM 的共增強策略，旨在同時在資料和最佳化器層級微調擴充模型，幫助 Adam 最佳化器找到較平坦的最小值，從而提升模型概化。整體而言，最終融合系統在評估集上達到 0.115 的 minDCF 和 4.04% 的 EER。

##### **Attribution Analysis Meets Model Editing: Advancing Knowledge Correction in Vision Language Models with VisEdit**
2408.09916v1 by Qizhou Chen, Taolin Zhang, Chengyu Wang, Xiaofeng He, Dakan Wang, Tingting Liu

Model editing aims to correct outdated or erroneous knowledge in large models
without costly retraining. Recent research discovered that the mid-layer
representation of the subject's final token in a prompt has a strong influence
on factual predictions, and developed Large Language Model (LLM) editing
techniques based on this observation. However, for Vision-LLMs (VLLMs), how
visual representations impact the predictions from a decoder-only language
model remains largely unexplored. To the best of our knowledge, model editing
for VLLMs has not been extensively studied in the literature. In this work, we
employ the contribution allocation and noise perturbation methods to measure
the contributions of visual representations for token predictions. Our
attribution analysis shows that visual representations in mid-to-later layers
that are highly relevant to the prompt contribute significantly to predictions.
Based on these insights, we propose VisEdit, a novel model editor for VLLMs
that effectively corrects knowledge by editing intermediate visual
representations in regions important to the edit prompt. We evaluated VisEdit
using multiple VLLM backbones and public VLLM editing benchmark datasets. The
results show the superiority of VisEdit over the strong baselines adapted from
existing state-of-the-art editors for LLMs.

摘要：模型编辑旨在更正大型模型中过时或错误的知识，而无需进行代价高昂的重新训练。最近的研究发现，提示中主题的最终标记的中层表示对事实预测有很大影响，并基于此观察结果开发了大语言模型 (LLM) 编辑技术。然而，对于视觉语言大模型 (VLLM)，视觉表示如何影响仅解码器语言模型的预测在很大程度上仍未得到探索。据我们所知，文献中尚未广泛研究 VLLM 的模型编辑。在这项工作中，我们采用贡献分配和噪声扰动方法来衡量视觉表示对标记预测的贡献。我们的归因分析表明，与提示高度相关的中间到后层中的视觉表示对预测有很大贡献。基于这些见解，我们提出了 VisEdit，这是一种新颖的 VLLM 模型编辑器，它通过编辑对编辑提示很重要的区域中的中间视觉表示来有效地更正知识。我们使用多个 VLLM 主干和公共 VLLM 编辑基准数据集评估了 VisEdit。结果表明 VisEdit 优于从现有的 LLM 最先进编辑器改编的强大基线。

##### **Active Learning for Identifying Disaster-Related Tweets: A Comparison with Keyword Filtering and Generic Fine-Tuning**
2408.09914v1 by David Hanny, Sebastian Schmidt, Bernd Resch

Information from social media can provide essential information for emergency
response during natural disasters in near real-time. However, it is difficult
to identify the disaster-related posts among the large amounts of unstructured
data available. Previous methods often use keyword filtering, topic modelling
or classification-based techniques to identify such posts. Active Learning (AL)
presents a promising sub-field of Machine Learning (ML) that has not been used
much in the field of text classification of social media content. This study
therefore investigates the potential of AL for identifying disaster-related
Tweets. We compare a keyword filtering approach, a RoBERTa model fine-tuned
with generic data from CrisisLex, a base RoBERTa model trained with AL and a
fine-tuned RoBERTa model trained with AL regarding classification performance.
For testing, data from CrisisLex and manually labelled data from the 2021 flood
in Germany and the 2023 Chile forest fires were considered. The results show
that generic fine-tuning combined with 10 rounds of AL outperformed all other
approaches. Consequently, a broadly applicable model for the identification of
disaster-related Tweets could be trained with very little labelling effort. The
model can be applied to use cases beyond this study and provides a useful tool
for further research in social media analysis.

摘要：社群媒體上的資訊能在近乎即時的情況下，提供自然災害期間緊急應變的重要資訊。然而，在大量的非結構化資料中找出與災害相關的貼文卻很困難。先前的做法通常會使用關鍵字過濾、主題模型或基於分類的技術來找出此類貼文。主動學習 (AL) 是機器學習 (ML) 中一個有前景的子領域，尚未廣泛用於社群媒體內容的文字分類領域。因此，本研究探討 AL 在找出與災害相關推文方面的潛力。我們比較了關鍵字過濾方法、使用 CrisisLex 中的通用資料微調的 RoBERTa 模型、使用 AL 訓練的基本 RoBERTa 模型，以及使用 AL 訓練的微調 RoBERTa 模型的分類效能。在測試方面，考量了 CrisisLex 中的資料，以及 2021 年德國洪災和 2023 年智利森林大火的手動標籤資料。結果顯示，通用微調結合 10 輪 AL 的表現優於其他所有方法。因此，可用非常少的標籤工作訓練出一個廣泛適用的模型來找出與災害相關的推文。此模型可應用於本研究以外的用例，並提供一個有用的工具，用於社群媒體分析的後續研究。

##### **LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on Concept Discovery**
2408.09899v1 by Weiji Kong, Xun Gong, Juan Wang

Explaining the decisions of Deep Neural Networks (DNNs) for medical images
has become increasingly important. Existing attribution methods have difficulty
explaining the meaning of pixels while existing concept-based methods are
limited by additional annotations or specific model structures that are
difficult to apply to ultrasound images. In this paper, we propose the Lesion
Concept Explainer (LCE) framework, which combines attribution methods with
concept-based methods. We introduce the Segment Anything Model (SAM),
fine-tuned on a large number of medical images, for concept discovery to enable
a meaningful explanation of ultrasound image DNNs. The proposed framework is
evaluated in terms of both faithfulness and understandability. We point out
deficiencies in the popular faithfulness evaluation metrics and propose a new
evaluation metric. Our evaluation of public and private breast ultrasound
datasets (BUSI and FG-US-B) shows that LCE performs well compared to
commonly-used explainability methods. Finally, we also validate that LCE can
consistently provide reliable explanations for more meaningful fine-grained
diagnostic tasks in breast ultrasound.

摘要：解釋深度神經網路 (DNN) 在醫學影像中的決策已變得越來越重要。現有的歸因方法難以解釋畫素的意義，而現有的基於概念的方法則受到額外註解或難以應用於超音波影像的特定模型結構限制。在本文中，我們提出病灶概念解釋器 (LCE) 架構，它結合了歸因方法與基於概念的方法。我們引入了在大量醫學影像上微調的「任何區段模型」(SAM)，用於概念發現，以實現超音波影像 DNN 的有意義解釋。所提出的架構在忠實度和可理解性方面都經過評估。我們指出了流行的忠實度評估指標中的缺陷，並提出了一個新的評估指標。我們對公共和私人乳房超音波資料集 (BUSI 和 FG-US-B) 的評估顯示，與常用的可解釋性方法相比，LCE 的表現良好。最後，我們還驗證了 LCE 能持續提供乳房超音波中更有意義的細粒度診斷任務的可靠解釋。

##### **Performance Law of Large Language Models**
2408.09895v1 by Chuhan Wu, Ruiming Tang

Guided by the belief of the scaling law, large language models (LLMs) have
achieved impressive performance in recent years. However, scaling law only
gives a qualitative estimation of loss, which is influenced by various factors
such as model architectures, data distributions, tokenizers, and computation
precision. Thus, estimating the real performance of LLMs with different
training settings rather than loss may be quite useful in practical
development. In this article, we present an empirical equation named
"Performance Law" to directly predict the MMLU score of an LLM, which is a
widely used metric to indicate the general capability of LLMs in real-world
conversations and applications. Based on only a few key hyperparameters of the
LLM architecture and the size of training data, we obtain a quite accurate MMLU
prediction of various LLMs with diverse sizes and architectures developed by
different organizations in different years. Performance law can be used to
guide the choice of LLM architecture and the effective allocation of
computational resources without extensive experiments.

摘要：在規模定律的信念指導下，大型語言模型 (LLM) 近年來已取得令人印象深刻的表現。然而，規模定律只對損失提供定性的估計，而損失受模型架構、資料分佈、分詞器和運算精度等各種因素影響。因此，在實際開發中，估計不同訓練設定下 LLM 的實際效能，而不是損失，可能相當有用。在本文中，我們提出一個名為「效能定律」的經驗方程式，用以直接預測 LLM 的 MMLU 分數，這是一個廣泛使用的指標，用於表示 LLM 在現實世界對話和應用中的整體能力。僅根據 LLM 架構的幾個關鍵超參數和訓練資料的大小，我們就能獲得各種 LLM 相當準確的 MMLU 預測，這些 LLM 具有不同的規模和架構，並由不同組織在不同年份開發。效能定律可用於指導 LLM 架構的選擇和有效分配運算資源，而無需進行大量實驗。

##### **Preoperative Rotator Cuff Tear Prediction from Shoulder Radiographs using a Convolutional Block Attention Module-Integrated Neural Network**
2408.09894v1 by Chris Hyunchul Jo, Jiwoong Yang, Byunghwan Jeon, Hackjoon Shim, Ikbeom Jang

Research question: We test whether a plane shoulder radiograph can be used
together with deep learning methods to identify patients with rotator cuff
tears as opposed to using an MRI in standard of care. Findings: By integrating
convolutional block attention modules into a deep neural network, our model
demonstrates high accuracy in detecting patients with rotator cuff tears,
achieving an average AUC of 0.889 and an accuracy of 0.831. Meaning: This study
validates the efficacy of our deep learning model to accurately detect rotation
cuff tears from radiographs, offering a viable pre-assessment or alternative to
more expensive imaging techniques such as MRI.

摘要：研究問題：我們測試平面肩部 X 光片是否可與深度學習方法結合使用，以識別旋轉肌袖撕裂的患者，而非在護理標準中使用 MRI。結果：透過將卷積區塊注意力模組整合到深度神經網路中，我們的模型在偵測旋轉肌袖撕裂的患者方面展現出高度準確性，達到平均 0.889 的 AUC 和 0.831 的準確度。意義：這項研究驗證了我們的深度學習模型從 X 光片中準確偵測旋轉肌袖撕裂的效能，提供了一個可行的預先評估或替代方案，以取代 MRI 等更昂貴的影像技術。

##### **Uncertainty Quantification of Pre-Trained and Fine-Tuned Surrogate Models using Conformal Prediction**
2408.09881v1 by Vignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, Daniel Giles, Matt Kusner, Marc Peter Deisenroth

Data-driven surrogate models have shown immense potential as quick,
inexpensive approximations to complex numerical and experimental modelling
tasks. However, most surrogate models characterising physical systems do not
quantify their uncertainty, rendering their predictions unreliable, and needing
further validation. Though Bayesian approximations offer some solace in
estimating the error associated with these models, they cannot provide they
cannot provide guarantees, and the quality of their inferences depends on the
availability of prior information and good approximations to posteriors for
complex problems. This is particularly pertinent to multi-variable or
spatio-temporal problems. Our work constructs and formalises a conformal
prediction framework that satisfies marginal coverage for spatio-temporal
predictions in a model-agnostic manner, requiring near-zero computational
costs. The paper provides an extensive empirical study of the application of
the framework to ascertain valid error bars that provide guaranteed coverage
across the surrogate model's domain of operation. The application scope of our
work extends across a large range of spatio-temporal models, ranging from
solving partial differential equations to weather forecasting. Through the
applications, the paper looks at providing statistically valid error bars for
deterministic models, as well as crafting guarantees to the error bars of
probabilistic models. The paper concludes with a viable conformal prediction
formalisation that provides guaranteed coverage of the surrogate model,
regardless of model architecture, and its training regime and is unbothered by
the curse of dimensionality.

摘要：<paragraph>資料驅動的代理模型已展現出極大的潛力，可用作複雜數值和實驗建模任務的快速且便宜的近似值。然而，大多數用於表徵物理系統的代理模型並未量化其不確定性，導致其預測不可靠，且需要進一步驗證。儘管貝氏近似法在估計與這些模型相關的誤差方面提供了一些慰藉，但它們無法提供擔保，且其推論的品質取決於先驗資訊的可用性，以及對複雜問題的後驗的良好近似值。這特別與多變量或時空問題有關。我們的研究建構並形式化了一個符合預測架構，該架構滿足時空預測的邊際覆蓋率，且與模型無關，且計算成本接近於零。本文提供了該架構應用的一個廣泛的經驗研究，以確定有效的誤差棒，在代理模型的操作域中提供有保證的覆蓋率。我們研究的應用範圍涵蓋了廣泛的時空模型，從求解偏微分方程到天氣預測。透過這些應用，本文著眼於為確定性模型提供統計上有效的誤差棒，以及為機率模型的誤差棒建立擔保。本文最後以一個可行的符合預測形式化作為結論，該形式化提供代理模型的有保證覆蓋率，不論模型架構和訓練機制為何，也不受維度災難的影響。</paragraph>

##### **Docling Technical Report**
2408.09869v1 by Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Nikolaos Livathinos, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Fabian Lindlbauer, Kasper Dinkla, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, Peter W. J. Staar

This technical report introduces Docling, an easy to use, self-contained,
MIT-licensed open-source package for PDF document conversion. It is powered by
state-of-the-art specialized AI models for layout analysis (DocLayNet) and
table structure recognition (TableFormer), and runs efficiently on commodity
hardware in a small resource budget. The code interface allows for easy
extensibility and addition of new features and models.

摘要：這份技術報告介紹了 Docling，一個易於使用、獨立、
MIT 授權的 PDF 文件轉換開源套件。它由
最先進的專用 AI 模型提供支援，用於版面分析 (DocLayNet) 和
表格結構辨識 (TableFormer)，並在小型資源預算中於商用
硬體上高效執行。程式碼介面允許輕鬆擴充和新增新功能和模型。

##### **MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation**
2408.09865v1 by Ching-Wen Yang, Che Wei Chen, Kun-da Wu, Hao Xu, Jui-Feng Yao, Hung-Yu Kao

Explainable Recommendation task is designed to receive a pair of user and
item and output explanations to justify why an item is recommended to a user.
Many models treat review-generation as a proxy of explainable recommendation.
Although they are able to generate fluent and grammatical sentences, they
suffer from generality and hallucination issues. We propose a personalized,
aspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), in which it
integrates aspect category as another input dimension to facilitate the
memorization of fine-grained aspect terms. Experiments on two real-world review
datasets in restaurant domain show that MAPLE outperforms the baseline
review-generation models in terms of text and feature diversity while
maintaining excellent coherence and factual relevance. We further treat MAPLE
as a retriever component in the retriever-reader framework and employ a
Large-Language Model (LLM) as the reader, showing that MAPLE's explanation
along with the LLM's comprehension ability leads to enriched and personalized
explanation as a result. We will release the code and data in this http upon
acceptance.

摘要：可解释性推荐任务旨在接收用户和商品对，并输出解释以证明向用户推荐商品的原因。许多模型将评论生成视为可解释性推荐的代理。虽然它们能够生成流畅且符合语法规则的句子，但它们存在普遍性和出现幻觉的问题。我们提出了一个名为多方面提示学习器 (MAPLE) 的个性化、方面控制模型，其中将方面类别作为另一个输入维度集成进来，以促进对细粒度方面术语的记忆。在餐厅领域的两个真实世界评论数据集上的实验表明，MAPLE 在文本和特征多样性方面优于基线评论生成模型，同时保持了出色的连贯性和事实相关性。我们进一步将 MAPLE 视为检索器-阅读器框架中的检索器组件，并使用大语言模型 (LLM) 作为阅读器，表明 MAPLE 的解释以及 LLM 的理解能力共同导致了丰富且个性化的解释。我们将在接受后在此 http 上发布代码和数据。

##### **TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and Competition**
2408.09856v1 by Tianwei Lin, Jiang Liu, Wenqiao Zhang, Zhaocheng Li, Yang Dai, Haoyuan Li, Zhelun Yu, Wanggui He, Juncheng Li, Hao Jiang, Siliang Tang, Yueting Zhuang

While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have
effectively addressed GPU memory constraints during fine-tuning, their
performance often falls short, especially in multidimensional task scenarios.
To address this issue, one straightforward solution is to introduce
task-specific LoRA modules as domain experts, leveraging the modeling of
multiple experts' capabilities and thus enhancing the general capability of
multi-task learning. Despite promising, these additional components often add
complexity to the training and inference process, contravening the efficient
characterization of PEFT designed for. Considering this, we introduce an
innovative PEFT method, TeamLoRA, consisting of a collaboration and competition
module for experts, and thus achieving the right balance of effectiveness and
efficiency: (i) For collaboration, a novel knowledge-sharing and -organizing
mechanism is devised to appropriately reduce the scale of matrix operations,
thereby boosting the training and inference speed. (ii) For competition, we
propose leveraging a game-theoretic interaction mechanism for experts,
encouraging experts to transfer their domain-specific knowledge while facing
diverse downstream tasks, and thus enhancing the performance. By doing so,
TeamLoRA elegantly connects the experts as a "Team" with internal collaboration
and competition, enabling a faster and more accurate PEFT paradigm for
multi-task learning. To validate the superiority of TeamLoRA, we curate a
comprehensive multi-task evaluation(CME) benchmark to thoroughly assess the
capability of multi-task learning. Experiments conducted on our CME and other
benchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project
is available at https://github.com/Lin-Tianwei/TeamLoRA.

摘要：<paragraph>雖然像 LoRA 這樣的參數有效微調 (PEFT) 方法在微調期間有效地解決了 GPU 記憶體限制，但其效能通常不足，特別是在多維度任務情境中。為了解決這個問題，一個直接的解決方案是引入特定於任務的 LoRA 模組作為領域專家，利用多個專家的能力建模，從而增強多任務學習的一般能力。儘管有前景，但這些額外的組件通常會增加訓練和推論過程的複雜性，違反了 PEFT 的有效特性。有鑑於此，我們引入了一種創新的 PEFT 方法 TeamLoRA，它包含一個專家協作和競爭模組，從而實現了有效性和效率的適當平衡：(i) 對於協作，設計了一種新穎的知識共享和組織機制，以適當地縮小矩陣運算的規模，從而提高訓練和推論速度。(ii) 對於競爭，我們建議利用博弈論互動機制來激勵專家，鼓勵專家在面對不同的下游任務時傳遞其特定領域的知識，從而提升效能。藉由這麼做，TeamLoRA 優雅地將專家們聯繫成一個具有內部協作和競爭的「團隊」，實現了更快速、更準確的多任務學習 PEFT 典範。為了驗證 TeamLoRA 的優越性，我們策劃了一個全面的多任務評估 (CME) 基準，以徹底評估多任務學習的能力。在我們的 CME 和其他基準上進行的實驗表明了 TeamLoRA 的有效性和效率。我們的專案可在 https://github.com/Lin-Tianwei/TeamLoRA 取得。</paragraph>

##### **Self-Directed Turing Test for Large Language Models**
2408.09853v1 by Weiqi Wu, Hongqiu Wu, Hai Zhao

The Turing test examines whether AIs can exhibit human-like behaviour in
natural language conversations. Traditional Turing tests adopt a rigid dialogue
format where each participant sends only one message each time and require
continuous human involvement to direct the entire interaction with the test
subject. This fails to reflect a natural conversational style and hinders the
evaluation of Large Language Models (LLMs) in complex and prolonged dialogues.
This paper proposes the Self-Directed Turing Test, which extends the original
test with a burst dialogue format, allowing more dynamic exchanges by multiple
consecutive messages. It further efficiently reduces human workload by having
the LLM self-direct the majority of the test process, iteratively generating
dialogues that simulate its interaction with humans. With the pseudo-dialogue
history, the model then engages in a shorter dialogue with a human, which is
paired with a human-human conversation on the same topic to be judged using
questionnaires. We introduce the X-Turn Pass-Rate metric to assess the human
likeness of LLMs across varying durations. While LLMs like GPT-4 initially
perform well, achieving pass rates of 51.9% and 38.9% during 3 turns and 10
turns of dialogues respectively, their performance drops as the dialogue
progresses, which underscores the difficulty in maintaining consistency in the
long term.

摘要：圖靈測試探討 AI 是否能在自然語言對話中表現出類似人類的行為。傳統的圖靈測試採用嚴格的對話格式，其中每個參與者每次只發送一條訊息，並且需要持續的人為介入來指導與受試者的整個互動。這無法反映自然的對話風格，並阻礙了在複雜且持久的對話中評估大型語言模型 (LLM)。本文提出了自導式圖靈測試，它通過爆發對話格式擴展了原始測試，允許通過多條連續訊息進行更動態的交流。它進一步通過讓 LLM 自我指導大部分測試過程來有效減少人類工作量，反覆生成模擬其與人類互動的對話。有了偽對話歷史，該模型隨後與人類進行較短的對話，該對話與同一個主題的人類對話配對，並使用問卷進行判斷。我們引入了 X-Turn 通過率指標來評估 LLM 在不同持續時間內的類人程度。雖然像 GPT-4 這樣的 LLM 最初表現良好，在 3 輪和 10 輪對話中分別達到 51.9% 和 38.9% 的通過率，但隨著對話的進行，它們的表現會下降，這突顯了長期保持一致性的難度。

##### **Importance Weighting Can Help Large Language Models Self-Improve**
2408.09849v1 by Chunyang Jiang, Chi-min Chan, Wei Xue, Qifeng Liu, Yike Guo

Large language models (LLMs) have shown remarkable capability in numerous
tasks and applications. However, fine-tuning LLMs using high-quality datasets
under external supervision remains prohibitively expensive. In response, LLM
self-improvement approaches have been vibrantly developed recently. The typical
paradigm of LLM self-improvement involves training LLM on self-generated data,
part of which may be detrimental and should be filtered out due to the unstable
data quality. While current works primarily employs filtering strategies based
on answer correctness, in this paper, we demonstrate that filtering out correct
but with high distribution shift extent (DSE) samples could also benefit the
results of self-improvement. Given that the actual sample distribution is
usually inaccessible, we propose a new metric called DS weight to approximate
DSE, inspired by the Importance Weighting methods. Consequently, we integrate
DS weight with self-consistency to comprehensively filter the self-generated
samples and fine-tune the language model. Experiments show that with only a
tiny valid set (up to 5\% size of the training set) to compute DS weight, our
approach can notably promote the reasoning ability of current LLM
self-improvement methods. The resulting performance is on par with methods that
rely on external supervision from pre-trained reward models.

摘要：大型語言模型 (LLM) 在許多任務和應用程式中展現出非凡的能力。然而，使用外部監督在高品質資料集上微調 LLM 仍是難以負擔的昂貴。為了解決這個問題，LLM 自我提升方法最近被熱烈地開發。LLM 自我提升的典型模式包括在自我產生的資料上訓練 LLM，其中一部分資料可能是有害的，並且由於不穩定的資料品質而應該被過濾掉。雖然目前的工作主要採用基於答案正確性的過濾策略，但在本文中，我們證明過濾掉正確但具有高分佈轉移程度 (DSE) 的樣本也可以使自我提升的結果受益。鑑於實際樣本分佈通常無法取得，我們提出一個稱為 DS 權重的指標，以近似 DSE，靈感來自重要性加權方法。因此，我們將 DS 權重與自我一致性整合，以全面過濾自我產生的樣本，並微調語言模型。實驗表明，僅使用一個微小的有效集合（訓練集合大小的 5%）來計算 DS 權重，我們的做法可以顯著提升目前 LLM 自我提升方法的推理能力。產生的效能與依賴預先訓練的獎勵模型的外部監督的方法相當。

##### **Continual Dialogue State Tracking via Reason-of-Select Distillation**
2408.09846v1 by Yujie Feng, Bo Liu, Xiaoyu Dong, Zexin Lu, Li-Ming Zhan, Xiao-Ming Wu, Albert Y. S. Lam

An ideal dialogue system requires continuous skill acquisition and adaptation
to new tasks while retaining prior knowledge. Dialogue State Tracking (DST),
vital in these systems, often involves learning new services and confronting
catastrophic forgetting, along with a critical capability loss termed the
"Value Selection Quandary." To address these challenges, we introduce the
Reason-of-Select (RoS) distillation method by enhancing smaller models with a
novel 'meta-reasoning' capability. Meta-reasoning employs an enhanced
multi-domain perspective, combining fragments of meta-knowledge from
domain-specific dialogues during continual learning. This transcends
traditional single-perspective reasoning. The domain bootstrapping process
enhances the model's ability to dissect intricate dialogues from multiple
possible values. Its domain-agnostic property aligns data distribution across
different domains, effectively mitigating forgetting. Additionally, two novel
improvements, "multi-value resolution" strategy and Semantic Contrastive
Reasoning Selection method, significantly enhance RoS by generating
DST-specific selection chains and mitigating hallucinations in teachers'
reasoning, ensuring effective and reliable knowledge transfer. Extensive
experiments validate the exceptional performance and robust generalization
capabilities of our method. The source code is provided for reproducibility.

摘要：理想的對話系統需要持續的技能習得和適應新任務，同時保留先前的知識。對話狀態追蹤 (DST) 在這些系統中至關重要，通常涉及學習新服務和應對災難性遺忘，以及稱為「值選擇困境」的關鍵能力損失。為了應對這些挑戰，我們引入了選擇原因 (RoS) 萃取方法，通過一種新穎的「元推理」能力增強較小的模型。元推理採用增強的多領域觀點，在持續學習期間結合來自特定領域對話的元知識片段。這超越了傳統的單一觀點推理。領域自舉過程增強了模型從多個可能值中剖析複雜對話的能力。其與領域無關的屬性使不同領域之間的數據分佈保持一致，有效減輕遺忘。此外，兩種新穎的改進，「多值解析」策略和語義對比推理選擇方法，通過生成 DST 特定的選擇鏈並減輕教師推理中的幻覺，顯著增強了 RoS，確保有效且可靠的知識傳遞。廣泛的實驗驗證了我們方法的出色性能和強大的泛化能力。提供源代碼以供重現。

##### **Segment-Anything Models Achieve Zero-shot Robustness in Autonomous Driving**
2408.09839v1 by Jun Yan, Pengyu Wang, Danni Wang, Weiquan Huang, Daniel Watzenig, Huilin Yin

Semantic segmentation is a significant perception task in autonomous driving.
It suffers from the risks of adversarial examples. In the past few years, deep
learning has gradually transitioned from convolutional neural network (CNN)
models with a relatively small number of parameters to foundation models with a
huge number of parameters. The segment-anything model (SAM) is a generalized
image segmentation framework that is capable of handling various types of
images and is able to recognize and segment arbitrary objects in an image
without the need to train on a specific object. It is a unified model that can
handle diverse downstream tasks, including semantic segmentation, object
detection, and tracking. In the task of semantic segmentation for autonomous
driving, it is significant to study the zero-shot adversarial robustness of
SAM. Therefore, we deliver a systematic empirical study on the robustness of
SAM without additional training. Based on the experimental results, the
zero-shot adversarial robustness of the SAM under the black-box corruptions and
white-box adversarial attacks is acceptable, even without the need for
additional training. The finding of this study is insightful in that the
gigantic model parameters and huge amounts of training data lead to the
phenomenon of emergence, which builds a guarantee of adversarial robustness.
SAM is a vision foundation model that can be regarded as an early prototype of
an artificial general intelligence (AGI) pipeline. In such a pipeline, a
unified model can handle diverse tasks. Therefore, this research not only
inspects the impact of vision foundation models on safe autonomous driving but
also provides a perspective on developing trustworthy AGI. The code is
available at: https://github.com/momo1986/robust_sam_iv.

摘要：語意分割是自動駕駛中一項重要的感知任務。
它會受到對抗範例的風險影響。在過去幾年，深度學習已逐漸從參數數量相對較少的卷積神經網路 (CNN) 模型轉變為參數數量龐大的基礎模型。區段任何事物模型 (SAM) 是一個通用的影像分割架構，它能夠處理各種類型的影像，並且能夠在影像中辨識和分割任意物件，而無需針對特定物件進行訓練。它是一個統一的模型，可以處理各種下游任務，包括語意分割、物件偵測和追蹤。在自動駕駛的語意分割任務中，研究 SAM 的零次學習對抗魯棒性非常重要。因此，我們對 SAM 的魯棒性進行了系統性的經驗研究，而無需額外訓練。根據實驗結果，即使無需額外訓練，SAM 在黑盒損壞和白盒對抗攻擊下的零次學習對抗魯棒性也是可以接受的。這項研究的發現很有見地，因為龐大的模型參數和大量的訓練資料會導致湧現現象，這建立了對抗魯棒性的保證。SAM 是視覺基礎模型，可視為人工通用智慧 (AGI) 管線的早期原型。在這樣的管線中，統一的模型可以處理各種任務。因此，這項研究不僅檢視了視覺基礎模型對安全自動駕駛的影響，還提供了開發可信賴 AGI 的觀點。程式碼可在 https://github.com/momo1986/robust_sam_iv 取得。

##### **Minor DPO reject penalty to increase training robustness**
2408.09834v1 by Shiming Xie, Hong Chen, Fred Yu, Zeye Sun, Xiuyu Wu, Yingfan Hu

Learning from human preference is a paradigm used in large-scale language
model (LLM) fine-tuning step to better align pretrained LLM to human preference
for downstream task. In the past it uses reinforcement learning from human
feedback (RLHF) algorithm to optimize the LLM policy to align with these
preferences and not to draft too far from the original model. Recently, Direct
Preference Optimization (DPO) has been proposed to solve the alignment problem
with a simplified RL-free method. Using preference pairs of chosen and reject
data, DPO models the relative log probability as implicit reward function and
optimize LLM policy using a simple binary cross entropy objective directly. DPO
is quite straight forward and easy to be understood. It perform efficiently and
well in most cases. In this article, we analyze the working mechanism of
$\beta$ in DPO, disclose its syntax difference between RL algorithm and DPO,
and understand the potential shortage brought by the DPO simplification. With
these insights, we propose MinorDPO, which is better aligned to the original RL
algorithm, and increase the stability of preference optimization process.

摘要：從人類偏好中學習是一種用於大規模語言模型 (LLM) 微調步驟的範例，以更好地將預訓練的 LLM 與人類偏好對齊以進行下游任務。過去它使用人類回饋（RLHF）演算法的強化學習來最佳化 LLM 政策，以與這些偏好對齊，並且不要與原始模型相差太遠。最近，已經提出直接偏好最佳化 (DPO) 來解決對齊問題，並採用簡化的無 RL 方法。使用所選和拒絕資料的偏好對，DPO 將相對對數機率建模為隱式獎勵函數，並使用簡單的二元交叉熵目標直接最佳化 LLM 政策。DPO 非常直接且易於理解。在大多數情況下，它的執行效率很高且表現良好。在本文中，我們分析 DPO 中 $\beta$ 的工作機制，揭示其在 RL 演算法和 DPO 之間的語法差異，並了解 DPO 簡化所帶來的潛在缺點。有了這些見解，我們提出了 MinorDPO，它與原始 RL 演算法更一致，並增加了偏好最佳化過程的穩定性。

##### **CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models**
2408.09819v1 by Linhao Yu, Yongqi Leng, Yufei Huang, Shang Wu, Haixin Liu, Xinmeng Ji, Jiahui Zhao, Jinwang Song, Tingting Cui, Xiaoqing Cheng, Tao Liu, Deyi Xiong

What a large language model (LLM) would respond in ethically relevant
context? In this paper, we curate a large benchmark CMoralEval for morality
evaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a
Chinese TV program discussing Chinese moral norms with stories from the society
and 2) a collection of Chinese moral anomies from various newspapers and
academic papers on morality. With these sources, we aim to create a moral
evaluation dataset characterized by diversity and authenticity. We develop a
morality taxonomy and a set of fundamental moral principles that are not only
rooted in traditional Chinese culture but also consistent with contemporary
societal norms. To facilitate efficient construction and annotation of
instances in CMoralEval, we establish a platform with AI-assisted instance
generation to streamline the annotation process. These help us curate
CMoralEval that encompasses both explicit moral scenarios (14,964 instances)
and moral dilemma scenarios (15,424 instances), each with instances from
different data sources. We conduct extensive experiments with CMoralEval to
examine a variety of Chinese LLMs. Experiment results demonstrate that
CMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly
available at \url{https://github.com/tjunlp-lab/CMoralEval}.

摘要：大型語言模型 (LLM) 在道德相關的語境中會如何回應？在本文中，我們策劃了一個大型基準 CMoralEval，用於評估中文 LLM 的道德標準。CMoralEval 的數據來源有兩個：1) 一個討論中國社會道德規範的中文電視節目，以及 2) 一些來自各種報紙和道德學術論文的中文道德規範異常現象。有了這些來源，我們希望創建一個以多樣性和真實性為特徵的道德評估數據集。我們制定了一個道德分類法和一套基本道德原則，這些原則不僅根植於傳統的中國文化，而且與當代社會規範一致。為了促進 CMoralEval 中實例的高效構建和註解，我們建立了一個具有 AI 輔助實例生成功能的平台，以簡化註解過程。這些幫助我們策劃了 CMoralEval，其中既包含明確的道德場景（14,964 個實例），又包含道德困境場景（15,424 個實例），每個實例都來自不同的數據源。我們使用 CMoralEval 進行了廣泛的實驗，以檢驗各種中文 LLM。實驗結果表明，CMoralEval 是中文 LLM 的一個具有挑戰性的基準。該數據集可在 \url{https://github.com/tjunlp-lab/CMoralEval} 公開獲得。

##### **Contextual Dual Learning Algorithm with Listwise Distillation for Unbiased Learning to Rank**
2408.09817v1 by Lulu Yu, Keping Bi, Shiyu Ni, Jiafeng Guo

Unbiased Learning to Rank (ULTR) aims to leverage biased implicit user
feedback (e.g., click) to optimize an unbiased ranking model. The effectiveness
of the existing ULTR methods has primarily been validated on synthetic
datasets. However, their performance on real-world click data remains unclear.
Recently, Baidu released a large publicly available dataset of their web search
logs. Subsequently, the NTCIR-17 ULTRE-2 task released a subset dataset
extracted from it. We conduct experiments on commonly used or effective ULTR
methods on this subset to determine whether they maintain their effectiveness.
In this paper, we propose a Contextual Dual Learning Algorithm with Listwise
Distillation (CDLA-LD) to simultaneously address both position bias and
contextual bias. We utilize a listwise-input ranking model to obtain
reconstructed feature vectors incorporating local contextual information and
employ the Dual Learning Algorithm (DLA) method to jointly train this ranking
model and a propensity model to address position bias. As this ranking model
learns the interaction information within the documents list of the training
set, to enhance the ranking model's generalization ability, we additionally
train a pointwise-input ranking model to learn the listwise-input ranking
model's capability for relevance judgment in a listwise manner. Extensive
experiments and analysis confirm the effectiveness of our approach.

摘要：無偏學習排名 (ULTR) 旨在利用有偏的隱式使用者回饋 (例如點擊) 來最佳化無偏的排名模型。現有 ULTR 方法的有效性主要已在合成資料集上得到驗證。然而，它們在真實世界點擊資料上的效能仍不明確。最近，百度發佈了一個大型公開可用的資料集，其中包含其網路搜尋記錄。隨後，NTCIR-17 ULTRE-2 任務發佈了一個從中提取的子集資料集。我們針對此子集執行常用或有效的 ULTR 方法的實驗，以確定它們是否維持其有效性。在本文中，我們提出一個具有清單式知識傳遞的脈絡雙重學習演算法 (CDLA-LD)，以同時處理位置偏誤和脈絡偏誤。我們利用清單式輸入排名模型，以取得包含局部脈絡資訊的重建特徵向量，並運用雙重學習演算法 (DLA) 方法來聯合訓練此排名模型和一個傾向模型，以處理位置偏誤。由於此排名模型會學習訓練集文件清單內的互動資訊，為了增強排名模型的概化能力，我們另外訓練一個點式輸入排名模型，以清單式方式學習清單式輸入排名模型在清單式相關性判斷的能力。廣泛的實驗和分析證實了我們方法的有效性。

##### **World Models Increase Autonomy in Reinforcement Learning**
2408.09807v2 by Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Edward S. Hu

Reinforcement learning (RL) is an appealing paradigm for training intelligent
agents, enabling policy acquisition from the agent's own autonomously acquired
experience. However, the training process of RL is far from automatic,
requiring extensive human effort to reset the agent and environments. To tackle
the challenging reset-free setting, we first demonstrate the superiority of
model-based (MB) RL methods in such setting, showing that a straightforward
adaptation of MBRL can outperform all the prior state-of-the-art methods while
requiring less supervision. We then identify limitations inherent to this
direct extension and propose a solution called model-based reset-free
(MoReFree) agent, which further enhances the performance. MoReFree adapts two
key mechanisms, exploration and policy learning, to handle reset-free tasks by
prioritizing task-relevant states. It exhibits superior data-efficiency across
various reset-free tasks without access to environmental reward or
demonstrations while significantly outperforming privileged baselines that
require supervision. Our findings suggest model-based methods hold significant
promise for reducing human effort in RL. Website:
https://sites.google.com/view/morefree

摘要：強化學習 (RL) 是訓練智慧型代理程式的一個誘人範例，它能讓代理程式從自己自主取得的經驗中獲取策略。然而，RL 的訓練過程遠非自動化，需要大量人力來重設代理程式和環境。為了應對具有挑戰性的無重設設定，我們首先展示了基於模型 (MB) 的 RL 方法在這種設定中的優越性，證明 MBRL 的直接改編可以優於所有先前的最先進方法，同時需要較少的監督。然後，我們找出此直接延伸固有的限制，並提出一個稱為基於模型的無重設 (MoReFree) 代理程式的解決方案，進一步提升效能。MoReFree 採用探索和策略學習這兩個關鍵機制，藉由優先處理與任務相關的狀態來處理無重設任務。它在各種無重設任務中展現出優異的資料效率，無需存取環境回饋或示範，同時顯著優於需要監督的特權基準。我們的發現表明，基於模型的方法對於減少 RL 中的人力具有顯著的潛力。網站：https://sites.google.com/view/morefree

##### **AutoML-guided Fusion of Entity and LLM-based representations**
2408.09794v1 by Boshko Koloski, Senja Pollak, Roberto Navigli, Blaž Škrlj

Large semantic knowledge bases are grounded in factual knowledge. However,
recent approaches to dense text representations (embeddings) do not efficiently
exploit these resources. Dense and robust representations of documents are
essential for effectively solving downstream classification and retrieval
tasks. This work demonstrates that injecting embedded information from
knowledge bases can augment the performance of contemporary Large Language
Model (LLM)-based representations for the task of text classification. Further,
by considering automated machine learning (AutoML) with the fused
representation space, we demonstrate it is possible to improve classification
accuracy even if we use low-dimensional projections of the original
representation space obtained via efficient matrix factorization. This result
shows that significantly faster classifiers can be achieved with minimal or no
loss in predictive performance, as demonstrated using five strong LLM baselines
on six diverse real-life datasets.

摘要：大型语义知识库以事实知识为基础。然而，最近对稠密文本表示（嵌入）的方法并没有有效利用这些资源。文档的稠密且稳健的表示对于有效解决下游分类和检索任务至关重要。这项工作表明，从知识库中注入嵌入式信息可以增强当代大型语言模型 (LLM) 在文本分类任务中的表示性能。此外，通过考虑具有融合表示空间的自动化机器学习 (AutoML)，我们证明即使我们使用通过高效矩阵分解获得的原始表示空间的低维投影，也能够提高分类准确性。这一结果表明，使用五个强大的 LLM 基线在六个不同的真实数据集上进行演示，可以以最小的或没有预测性能损失来实现明显更快的分类器。

##### **Anim-Director: A Large Multimodal Model Powered Agent for Controllable Animation Video Generation**
2408.09787v1 by Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, Min Zhang

Traditional animation generation methods depend on training generative models
with human-labelled data, entailing a sophisticated multi-stage pipeline that
demands substantial human effort and incurs high training costs. Due to limited
prompting plans, these methods typically produce brief, information-poor, and
context-incoherent animations. To overcome these limitations and automate the
animation process, we pioneer the introduction of large multimodal models
(LMMs) as the core processor to build an autonomous animation-making agent,
named Anim-Director. This agent mainly harnesses the advanced understanding and
reasoning capabilities of LMMs and generative AI tools to create animated
videos from concise narratives or simple instructions. Specifically, it
operates in three main stages: Firstly, the Anim-Director generates a coherent
storyline from user inputs, followed by a detailed director's script that
encompasses settings of character profiles and interior/exterior descriptions,
and context-coherent scene descriptions that include appearing characters,
interiors or exteriors, and scene events. Secondly, we employ LMMs with the
image generation tool to produce visual images of settings and scenes. These
images are designed to maintain visual consistency across different scenes
using a visual-language prompting method that combines scene descriptions and
images of the appearing character and setting. Thirdly, scene images serve as
the foundation for producing animated videos, with LMMs generating prompts to
guide this process. The whole process is notably autonomous without manual
intervention, as the LMMs interact seamlessly with generative tools to generate
prompts, evaluate visual quality, and select the best one to optimize the final
output.

摘要：傳統動畫生成方法依賴於訓練生成模型，並使用人工標記的資料，這需要一個複雜的多階段流程，需要大量的人力，並會產生高昂的訓練成本。由於提示規劃有限，這些方法通常會產生簡短、資訊不足且前後文不連貫的動畫。為了克服這些限制並自動化動畫製作過程，我們率先引進大型多模態模型 (LMM) 作為核心處理器，以建構一個自主的動畫製作代理，稱為 Anim-Director。此代理程式主要利用 LMM 和生成式 AI 工具的先進理解和推理能力，從簡潔的敘述或簡單的說明中建立動畫影片。具體來說，它的運作分為三個主要階段：首先，Anim-Director 從使用者的輸入中產生一個連貫的故事線，接著是一個詳細的導演腳本，其中包含角色設定和室內/室外描述，以及前後文連貫的場景描述，包括出現的角色、室內或室外，以及場景事件。其次，我們使用 LMM 和影像生成工具產生設定和場景的視覺影像。這些影像經過設計，可透過結合場景描述和出現角色和設定的影像，在不同的場景中維持視覺的一致性。第三，場景影像作為製作動畫影片的基礎，LMM 會產生提示來引導此過程。整個過程非常自主，無需人工介入，因為 LMM 會與生成式工具無縫互動，以產生提示、評估視覺品質，並選出最佳的提示來最佳化最終的輸出。

##### **GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making**
2408.09785v1 by Arsham Gholamzadeh Khoee, Yinan Yu, Robert Feldt, Andris Freimanis, Patrick Andersson, Dhasarathy Parthasarathy

Traditional methods for making software deployment decisions in the
automotive industry typically rely on manual analysis of tabular software test
data. These methods often lead to higher costs and delays in the software
release cycle due to their labor-intensive nature. Large Language Models (LLMs)
present a promising solution to these challenges. However, their application
generally demands multiple rounds of human-driven prompt engineering, which
limits their practical deployment, particularly for industrial end-users who
need reliable and efficient results. In this paper, we propose GoNoGo, an LLM
agent system designed to streamline automotive software deployment while
meeting both functional requirements and practical industrial constraints.
Unlike previous systems, GoNoGo is specifically tailored to address
domain-specific and risk-sensitive systems. We evaluate GoNoGo's performance
across different task difficulties using zero-shot and few-shot examples taken
from industrial practice. Our results show that GoNoGo achieves a 100% success
rate for tasks up to Level 2 difficulty with 3-shot examples, and maintains
high performance even for more complex tasks. We find that GoNoGo effectively
automates decision-making for simpler tasks, significantly reducing the need
for manual intervention. In summary, GoNoGo represents an efficient and
user-friendly LLM-based solution currently employed in our industrial partner's
company to assist with software release decision-making, supporting more
informed and timely decisions in the release process for risk-sensitive vehicle
systems.

摘要：傳統上，汽車產業在制定軟體部署決策時，通常仰賴人工分析表格化軟體測試資料。由於這些方法需要大量人力，因此經常導致軟體發布週期成本較高且延遲。大型語言模型 (LLM) 為這些挑戰提供了有前途的解決方案。然而，它們的應用通常需要多輪由人主導的提示工程，這限制了它們的實際部署，特別是對於需要可靠且有效率結果的產業端使用者。在本文中，我們提出了 GoNoGo，一種 LLM 代理系統，旨在簡化汽車軟體部署，同時滿足功能需求和實際產業限制。與先前的系統不同，GoNoGo 特別針對領域特定和風險敏感的系統而設計。我們使用取自產業實務的零次學習和少次學習範例，評估了 GoNoGo 在不同任務難度下的效能。我們的結果顯示，GoNoGo 在難度達 2 級的任務中，使用 3 次學習範例即可達到 100% 的成功率，即使在更複雜的任務中也能維持高效能。我們發現 GoNoGo 能有效自動化較簡單任務的決策制定，大幅減少人工介入的需要。總之，GoNoGo 代表了一種有效且使用者友善的 LLM 基礎解決方案，目前已在我們的產業合作夥伴公司中使用，協助進行軟體發布決策制定，在風險敏感的車輛系統發布過程中支援更明智且及時的決策。

##### **Summarizing long regulatory documents with a multi-step pipeline**
2408.09777v1 by Mika Sie, Ruby Beek, Michiel Bots, Sjaak Brinkkemper, Albert Gatt

Due to their length and complexity, long regulatory texts are challenging to
summarize. To address this, a multi-step extractive-abstractive architecture is
proposed to handle lengthy regulatory documents more effectively. In this
paper, we show that the effectiveness of a two-step architecture for
summarizing long regulatory texts varies significantly depending on the model
used. Specifically, the two-step architecture improves the performance of
decoder-only models. For abstractive encoder-decoder models with short context
lengths, the effectiveness of an extractive step varies, whereas for
long-context encoder-decoder models, the extractive step worsens their
performance. This research also highlights the challenges of evaluating
generated texts, as evidenced by the differing results from human and automated
evaluations. Most notably, human evaluations favoured language models
pretrained on legal text, while automated metrics rank general-purpose language
models higher. The results underscore the importance of selecting the
appropriate summarization strategy based on model architecture and context
length.

摘要：由於法規文字冗長且複雜，摘要起來極具挑戰性。為了解決這個問題，提出了一個多步驟的抽取式摘要架構，以更有效地處理冗長的規範文件。在本文中，我們展示了用於摘要冗長法規文字的兩步驟架構的有效性，會根據所使用的模型而有顯著差異。具體來說，兩步驟架構改善了僅解碼器模型的效能。對於具有短脈絡長度的抽象編碼器解碼器模型，抽取步驟的有效性有所不同，而對於長脈絡編碼器解碼器模型，抽取步驟會惡化其效能。這項研究也強調了評估生成文字的挑戰，這從人類和自動評估的不同結果中可以看出。最值得注意的是，人類評估偏好預先訓練於法律文字上的語言模型，而自動化指標則對通用語言模型給予較高的排名。這些結果強調了根據模型架構和脈絡長度選擇適當摘要策略的重要性。

##### **Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?**
2408.09773v1 by Shiyu Ni, Keping Bi, Lulu Yu, Jiafeng Guo

Large language models (LLMs) have been found to produce hallucinations when
the question exceeds their internal knowledge boundaries. A reliable model
should have a clear perception of its knowledge boundaries, providing correct
answers within its scope and refusing to answer when it lacks knowledge.
Existing research on LLMs' perception of their knowledge boundaries typically
uses either the probability of the generated tokens or the verbalized
confidence as the model's confidence in its response. However, these studies
overlook the differences and connections between the two. In this paper, we
conduct a comprehensive analysis and comparison of LLMs' probabilistic
perception and verbalized perception of their factual knowledge boundaries.
First, we investigate the pros and cons of these two perceptions. Then, we
study how they change under questions of varying frequencies. Finally, we
measure the correlation between LLMs' probabilistic confidence and verbalized
confidence. Experimental results show that 1) LLMs' probabilistic perception is
generally more accurate than verbalized perception but requires an in-domain
validation set to adjust the confidence threshold. 2) Both perceptions perform
better on less frequent questions. 3) It is challenging for LLMs to accurately
express their internal confidence in natural language.

摘要：大型語言模型 (LLM) 已被發現會在問題超出其內部知識界限時產生幻覺。可靠的模型應清楚了解其知識界限，在其範圍內提供正確答案，並在缺乏知識時拒絕回答。現有關於 LLM 對其知識界限的感知的研究通常使用生成詞彙的機率或口頭表達的信心作為模型對其回應的信心。然而，這些研究忽略了這兩者之間的差異和關聯。在本文中，我們對 LLM 對其事實知識界限的機率感知和口頭表達感知進行了全面的分析和比較。首先，我們探討這兩種感知的優缺點。然後，我們研究它們在不同頻率問題下的變化。最後，我們測量 LLM 的機率信心和口頭表達信心之間的相關性。實驗結果表明：1) LLM 的機率感知通常比口頭表達感知更準確，但需要一個領域內驗證集來調整信心閾值。2) 這兩種感知在頻率較低的問題上表現都較好。3) LLM 難以準確地用自然語言表達其內部信心。

##### **Propagating the prior from shallow to deep with a pre-trained velocity-model Generative Transformer network**
2408.09767v1 by Randy Harsuko, Shijun Cheng, Tariq Alkhalifah

Building subsurface velocity models is essential to our goals in utilizing
seismic data for Earth discovery and exploration, as well as monitoring. With
the dawn of machine learning, these velocity models (or, more precisely, their
distribution) can be stored accurately and efficiently in a generative model.
These stored velocity model distributions can be utilized to regularize or
quantify uncertainties in inverse problems, like full waveform inversion.
However, most generators, like normalizing flows or diffusion models, treat the
image (velocity model) uniformly, disregarding spatial dependencies and
resolution changes with respect to the observation locations. To address this
weakness, we introduce VelocityGPT, a novel implementation that utilizes
Transformer decoders trained autoregressively to generate a velocity model from
shallow subsurface to deep. Owing to the fact that seismic data are often
recorded on the Earth's surface, a top-down generator can utilize the inverted
information in the shallow as guidance (prior) to generating the deep. To
facilitate the implementation, we use an additional network to compress the
velocity model. We also inject prior information, like well or structure
(represented by a migration image) to generate the velocity model. Using
synthetic data, we demonstrate the effectiveness of VelocityGPT as a promising
approach in generative model applications for seismic velocity model building.

摘要：建立地下速度模型对于我们利用地震数据进行地球发现和勘探以及监测至关重要。随着机器学习的兴起，这些速度模型（或更准确地说，它们的分布）可以准确有效地存储在生成模型中。这些存储的速度模型分布可用于对反问题（如全波形反演）中的不确定性进行正则化或量化。然而，大多数生成器（如归一化流或扩散模型）将图像（速度模型）统一处理，而忽略了空间依赖性和相对于观测位置的分辨率变化。为了解决这个弱点，我们引入了 VelocityGPT，这是一种利用自回归训练的 Transformer 解码器来生成从浅层地下到深层的速度模型的新颖实现。由于地震数据通常记录在地表，因此自顶向下的生成器可以利用浅层中的反演信息作为生成深层信息的指导（先验）。为了促进实施，我们使用了一个额外的网络来压缩速度模型。我们还注入先验信息，如井或结构（由偏移图像表示）来生成速度模型。使用合成数据，我们展示了 VelocityGPT 作为地震速度模型构建生成模型应用中一种有前途的方法的有效性。

##### **Event Stream based Human Action Recognition: A High-Definition Benchmark Dataset and Algorithms**
2408.09764v1 by Xiao Wang, Shiao Wang, Pengpeng Shao, Bo Jiang, Lin Zhu, Yonghong Tian

Human Action Recognition (HAR) stands as a pivotal research domain in both
computer vision and artificial intelligence, with RGB cameras dominating as the
preferred tool for investigation and innovation in this field. However, in
real-world applications, RGB cameras encounter numerous challenges, including
light conditions, fast motion, and privacy concerns. Consequently, bio-inspired
event cameras have garnered increasing attention due to their advantages of low
energy consumption, high dynamic range, etc. Nevertheless, most existing
event-based HAR datasets are low resolution ($346 \times 260$). In this paper,
we propose a large-scale, high-definition ($1280 \times 800$) human action
recognition dataset based on the CeleX-V event camera, termed CeleX-HAR. It
encompasses 150 commonly occurring action categories, comprising a total of
124,625 video sequences. Various factors such as multi-view, illumination,
action speed, and occlusion are considered when recording these data. To build
a more comprehensive benchmark dataset, we report over 20 mainstream HAR models
for future works to compare. In addition, we also propose a novel Mamba vision
backbone network for event stream based HAR, termed EVMamba, which equips the
spatial plane multi-directional scanning and novel voxel temporal scanning
mechanism. By encoding and mining the spatio-temporal information of event
streams, our EVMamba has achieved favorable results across multiple datasets.
Both the dataset and source code will be released on
\url{https://github.com/Event-AHU/CeleX-HAR}

摘要：人體動作識別 (HAR) 是電腦視覺和人工智慧中一個重要的研究領域，而 RGB 相機作為首選工具，在這個領域的研究和創新中佔據主導地位。然而，在實際應用中，RGB 相機會遇到許多挑戰，包括光線條件、快速動作和隱私問題。因此，受生物啟發的事件相機由於具有低能耗、高動態範圍等優點而備受關注。儘管如此，現有的基於事件的 HAR 資料集大多是低解析度 ($346 \times 260$)。在本文中，我們提出了一個基於 CeleX-V 事件相機的大規模、高解析度 ($1280 \times 800$) 人體動作識別資料集，稱為 CeleX-HAR。它包含 150 個常見的動作類別，總共包含 124,625 個影片序列。在記錄這些數據時，考慮了多視角、光照、動作速度和遮擋等各種因素。為了建立一個更全面的基準資料集，我們報告了 20 多個主流 HAR 模型，供後續工作比較。此外，我們還提出了一個新的 Mamba 視覺主幹網路，用於基於事件串流的 HAR，稱為 EVMamba，它具備空間平面多向掃描和新穎的體素時間掃描機制。通過編碼和挖掘事件串流的時空資訊，我們的 EVMamba 在多個資料集上都取得了良好的結果。資料集和原始碼將在 \url{https://github.com/Event-AHU/CeleX-HAR} 上發布。

##### **Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning**
2408.09757v1 by Jingyu Hu, Weiru Liu, Mengnan Du

Recent studies highlight the effectiveness of using in-context learning (ICL)
to steer large language models (LLMs) in processing tabular data, a challenging
task given the structured nature of such data. Despite advancements in
performance, the fairness implications of these methods are less understood.
This study investigates how varying demonstrations within ICL prompts influence
the fairness outcomes of LLMs. Our findings reveal that deliberately including
minority group samples in prompts significantly boosts fairness without
sacrificing predictive accuracy. Further experiments demonstrate that the
proportion of minority to majority samples in demonstrations affects the
trade-off between fairness and prediction accuracy. Based on these insights, we
introduce a mitigation technique that employs clustering and evolutionary
strategies to curate a diverse and representative sample set from the training
data. This approach aims to enhance both predictive performance and fairness in
ICL applications. Experimental results validate that our proposed method
dramatically improves fairness across various metrics, showing its efficacy in
real-world scenarios.

摘要：最近的研究強調了在脈絡學習 (ICL) 中使用大型語言模型 (LLM) 來處理表格資料的效能，這是一個具有挑戰性的任務，因為此類資料具有結構化的性質。儘管效能有所進步，但這些方法的公平性影響卻鮮為人知。本研究探討了 ICL 提示中不同的示範如何影響 LLM 的公平性結果。我們的研究結果顯示，在提示中刻意包含少數群體樣本會顯著提升公平性，而不會犧牲預測準確性。進一步的實驗表明，示範中少數群體與多數群體樣本的比例會影響公平性和預測準確性之間的權衡。根據這些見解，我們引入了一種緩解技術，它採用聚類和演化策略從訓練資料中策劃一個多元且具代表性的樣本集。這種方法旨在提升 ICL 應用中的預測效能和公平性。實驗結果驗證了我們提出的方法在各種指標上大幅提升了公平性，顯示其在實際場景中的效能。

##### **Revisiting Reciprocal Recommender Systems: Metrics, Formulation, and Method**
2408.09748v1 by Chen Yang, Sunhao Dai, Yupeng Hou, Wayne Xin Zhao, Jun Xu, Yang Song, Hengshu Zhu

Reciprocal recommender systems~(RRS), conducting bilateral recommendations
between two involved parties, have gained increasing attention for enhancing
matching efficiency. However, the majority of existing methods in the
literature still reuse conventional ranking metrics to separately assess the
performance on each side of the recommendation process. These methods overlook
the fact that the ranking outcomes of both sides collectively influence the
effectiveness of the RRS, neglecting the necessity of a more holistic
evaluation and a capable systemic solution.
  In this paper, we systemically revisit the task of reciprocal recommendation,
by introducing the new metrics, formulation, and method. Firstly, we propose
five new evaluation metrics that comprehensively and accurately assess the
performance of RRS from three distinct perspectives: overall coverage,
bilateral stability, and balanced ranking. These metrics provide a more
holistic understanding of the system's effectiveness and enable a comprehensive
evaluation. Furthermore, we formulate the RRS from a causal perspective,
formulating recommendations as bilateral interventions, which can better model
the decoupled effects of potential influencing factors. By utilizing the
potential outcome framework, we further develop a model-agnostic causal
reciprocal recommendation method that considers the causal effects of
recommendations. Additionally, we introduce a reranking strategy to maximize
matching outcomes, as measured by the proposed metrics. Extensive experiments
on two real-world datasets from recruitment and dating scenarios demonstrate
the effectiveness of our proposed metrics and approach. The code and dataset
are available at: https://github.com/RUCAIBox/CRRS.

摘要：<paragraph>往復推薦系統~(RRS)在兩方參與者之間進行雙邊推薦，已獲得越來越多的關注，以提高匹配效率。然而，文獻中現有的方法大多數仍重複使用傳統的排名指標，以分別評估推薦過程中每一側的效能。這些方法忽視了雙方排名結果共同影響 RRS 有效性的事實，忽視了更全面評估和有能力的系統性解決方案的必要性。
在本文中，我們系統性地重新探討往復推薦的任務，引入了新的指標、公式和方法。首先，我們提出五個新的評估指標，從三個不同的角度全面準確地評估 RRS 的效能：整體覆蓋率、雙邊穩定性和平衡排名。這些指標提供了對系統效能更全面的理解，並能進行全面的評估。此外，我們從因果角度制定 RRS，將推薦制定為雙邊干預，這可以更好地模擬潛在影響因素的解耦效應。通過利用潛在結果框架，我們進一步開發了一個與模型無關的因果往復推薦方法，該方法考慮了推薦的因果效應。此外，我們引入了一個重新排名策略，以最大化匹配結果，如所提出的指標所測量的那樣。在招聘和約會場景的兩個真實世界數據集上進行的廣泛實驗證明了我們提出的指標和方法的有效性。程式碼和數據集可在以下網址獲得：https://github.com/RUCAIBox/CRRS。</paragraph>

##### **Enhanced Cascade Prostate Cancer Classifier in mp-MRI Utilizing Recall Feedback Adaptive Loss and Prior Knowledge-Based Feature Extraction**
2408.09746v1 by Kun Luo, Bowen Zheng, Shidong Lv, Jie Tao, Qiang Wei

Prostate cancer is the second most common cancer in males worldwide, and
mpMRI is commonly used for diagnosis. However, interpreting mpMRI is
challenging and requires expertise from radiologists. This highlights the
urgent need for automated grading in mpMRI. Existing studies lack integration
of clinical prior information and suffer from uneven training sample
distribution due to prevalence. Therefore, we propose a solution that
incorporates prior knowledge, addresses the issue of uneven medical sample
distribution, and maintains high interpretability in mpMRI. Firstly, we
introduce Prior Knowledge-Based Feature Extraction, which mathematically models
the PI-RADS criteria for prostate cancer as diagnostic information into model
training. Secondly, we propose Adaptive Recall Feedback Loss to address the
extremely imbalanced data problem. This method adjusts the training dynamically
based on accuracy and recall in the validation set, resulting in high accuracy
and recall simultaneously in the testing set.Thirdly, we design an Enhanced
Cascade Prostate Cancer Classifier that classifies prostate cancer into
different levels in an interpretable way, which refines the classification
results and helps with clinical intervention. Our method is validated through
experiments on the PI-CAI dataset and outperforms other methods with a more
balanced result in both accuracy and recall rate.

摘要：攝護腺癌是全球男性中第二常見的癌症，而多參數磁振造影（mpMRI）通常用於診斷。然而，mpMRI 的解讀具有挑戰性，需要放射科醫師的專業知識。這突顯了 mpMRI 自動分級的迫切需求。現有的研究缺乏臨床先驗資訊的整合，並且由於患病率而導致訓練樣本分佈不均。因此，我們提出了一個解決方案，它結合了先驗知識，解決了醫學樣本分佈不均的問題，並在 mpMRI 中保持了很高的可解釋性。首先，我們引入了基於先驗知識的特徵提取，它將 PI-RADS 攝護腺癌診斷資訊數學建模成模型訓練。其次，我們提出了自適應召回反饋損失來解決極度不平衡的資料問題。此方法根據驗證集中的準確度和召回率動態調整訓練，從而同時在測試集中獲得高準確度和召回率。第三，我們設計了一個增強型級聯攝護腺癌分類器，它以可解釋的方式將攝護腺癌分類為不同的等級，這可以改善分類結果並有助於臨床介入。我們的模型已通過 PI-CAI 資料集的實驗驗證，並且在準確度和召回率方面都優於其他方法，結果更為平衡。

##### **R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation**
2408.09743v1 by Xiao Wang, Yuehang Li, Fuling Wang, Shiao Wang, Chuanfu Li, Bo Jiang

Inspired by the tremendous success of Large Language Models (LLMs), existing
X-ray medical report generation methods attempt to leverage large models to
achieve better performance. They usually adopt a Transformer to extract the
visual features of a given X-ray image, and then, feed them into the LLM for
text generation. How to extract more effective information for the LLMs to help
them improve final results is an urgent problem that needs to be solved.
Additionally, the use of visual Transformer models also brings high
computational complexity. To address these issues, this paper proposes a novel
context-guided efficient X-ray medical report generation framework.
Specifically, we introduce the Mamba as the vision backbone with linear
complexity, and the performance obtained is comparable to that of the strong
Transformer model. More importantly, we perform context retrieval from the
training set for samples within each mini-batch during the training phase,
utilizing both positively and negatively related samples to enhance feature
representation and discriminative learning. Subsequently, we feed the vision
tokens, context information, and prompt statements to invoke the LLM for
generating high-quality medical reports. Extensive experiments on three X-ray
report generation datasets (i.e., IU-Xray, MIMIC-CXR, CheXpert Plus) fully
validated the effectiveness of our proposed model. The source code of this work
will be released on \url{https://github.com/Event-AHU/Medical_Image_Analysis}.

摘要：受到大型語言模型 (LLM) 巨大成功的啟發，現有的 X 光醫學報告生成方法嘗試利用大型模型來達成更好的效能。他們通常採用 Transformer 來擷取特定 X 光影像的視覺特徵，然後將其輸入 LLM 以進行文字生成。如何擷取更有效的資訊以供 LLM 使用，協助他們改善最終結果，是一個亟需解決的迫切問題。此外，視覺 Transformer 模型的使用也帶來了很高的運算複雜度。為了解決這些問題，本文提出了一個新穎的脈絡導引式高效 X 光醫學報告生成架構。具體來說，我們引入 Mamba 作為具有線性複雜度的視覺主幹，且獲得的效能與強大的 Transformer 模型相當。更重要的是，我們在訓練階段從訓練集中執行脈絡檢索，以取得每個小批次中的樣本，利用正相關和負相關樣本來增強特徵表徵和判別式學習。隨後，我們將視覺符號、脈絡資訊和提示陳述輸入 LLM，以生成高品質的醫學報告。在三個 X 光報告生成資料集（即 IU-Xray、MIMIC-CXR、CheXpert Plus）上進行的廣泛實驗，充分驗證了我們提出的模型的有效性。這項工作的原始碼將在 \url{https://github.com/Event-AHU/Medical_Image_Analysis} 上發布。

##### **Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs**
2408.09742v1 by Simon D Angus, Lachlan O'Neill

Detecting and quantifying issue framing in textual discourse - the
perspective one takes to a given topic (e.g. climate science vs. denialism,
misogyny vs. gender equality) - is highly valuable to a range of end-users from
social and political scientists to program evaluators and policy analysts.
However, conceptual framing is notoriously challenging for automated natural
language processing (NLP) methods since the words and phrases used by either
`side' of an issue are often held in common, with only subtle stylistic
flourishes separating their use. Here we develop and rigorously evaluate new
detection methods for issue framing and narrative analysis within large text
datasets. By introducing a novel application of next-token log probabilities
derived from generative large language models (LLMs) we show that issue framing
can be reliably and efficiently detected in large corpora with only a few
examples of either perspective on a given issue, a method we call `paired
completion'. Through 192 independent experiments over three novel, synthetic
datasets, we evaluate paired completion against prompt-based LLM methods and
labelled methods using traditional NLP and recent LLM contextual embeddings. We
additionally conduct a cost-based analysis to mark out the feasible set of
performant methods at production-level scales, and a model bias analysis.
Together, our work demonstrates a feasible path to scalable, accurate and
low-bias issue-framing in large corpora.

摘要：偵測並量化文本論述中的議題取向（即個人對特定主題的觀點，例如氣候科學與否認主義、厭惡女性與性別平等）對於廣泛的使用者而言極具價值，從社會與政治科學家到計畫評估者和政策分析師皆是如此。然而，概念取向對於自動化自然語言處理 (NLP) 方法來說出了名的具有挑戰性，因為議題「雙方」使用的字詞和片語通常是通用的，只有細微的文體變化區分其用法。在此，我們開發並嚴格評估大型文字資料集中的議題取向和敘述分析新偵測方法。透過引入衍生自生成式大型語言模型 (LLM) 的下一個代幣對數機率之新應用，我們證明議題取向可以在大型語料庫中透過對特定議題的任何觀點僅幾個範例，可靠且有效地偵測到，我們稱此方法為「配對完成」。透過三個新穎的合成資料集進行 192 個獨立實驗，我們評估配對完成與基於提示的 LLM 方法和使用傳統 NLP 和近期 LLM 上下文嵌入的標籤方法。我們另外進行成本效益分析，以標示出在生產等級規模上可行的高效方法組，以及模型偏誤分析。總而言之，我們的研究展示了一條可行的路徑，可在大語料庫中進行可擴充、準確且低偏誤的議題取向分析。

##### **Mutually-Aware Feature Learning for Few-Shot Object Counting**
2408.09734v1 by Yerim Jeon, Subeen Lee, Jihwan Kim, Jae-Pil Heo

Few-shot object counting has garnered significant attention for its
practicality as it aims to count target objects in a query image based on given
exemplars without the need for additional training. However, there is a
shortcoming in the prevailing extract-and-match approach: query and exemplar
features lack interaction during feature extraction since they are extracted
unaware of each other and later correlated based on similarity. This can lead
to insufficient target awareness of the extracted features, resulting in target
confusion in precisely identifying the actual target when multiple class
objects coexist. To address this limitation, we propose a novel framework,
Mutually-Aware FEAture learning(MAFEA), which encodes query and exemplar
features mutually aware of each other from the outset. By encouraging
interaction between query and exemplar features throughout the entire pipeline,
we can obtain target-aware features that are robust to a multi-category
scenario. Furthermore, we introduce a background token to effectively associate
the target region of query with exemplars and decouple its background region
from them. Our extensive experiments demonstrate that our model reaches a new
state-of-the-art performance on the two challenging benchmarks, FSCD-LVIS and
FSC-147, with a remarkably reduced degree of the target confusion problem.

摘要：小样本目标计数因其实用性而备受关注，因为它旨在根据给定的示例在查询图像中计数目标对象，而无需进行额外的训练。然而，流行的提取和匹配方法存在一个缺点：查询和示例特征在特征提取过程中缺乏交互，因为它们在彼此不知情的情况下被提取，并在之后根据相似性进行关联。这会导致提取的特征对目标的感知不足，从而在多个类别对象共存时导致在精确识别实际目标时出现目标混淆。为了解决这一限制，我们提出了一种新颖的框架，即相互感知特征学习 (MAFEA)，它从一开始就对查询和示例特征进行相互感知的编码。通过在整个管道中鼓励查询和示例特征之间的交互，我们可以获得对目标感知的特征，这些特征对多类别场景具有鲁棒性。此外，我们引入了一个背景令牌，以有效地将查询的目标区域与示例关联起来，并将其背景区域与示例分离。我们广泛的实验表明，我们的模型在两个具有挑战性的基准 FSCD-LVIS 和 FSC-147 上达到了新的最先进性能，并且目标混淆问题的程度显著降低。

##### **Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large Language Model Augmented Framework**
2408.09720v1 by Jiandong Jin, Xiao Wang, Qian Zhu, Haiyang Wang, Chenglong Li

Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in
human-centered research. However, existing datasets neglect different domains
(e.g., environments, times, populations, and data sources), only conducting
simple random splits, and the performance of these datasets has already
approached saturation. In the past five years, no large-scale dataset has been
opened to the public. To address this issue, this paper proposes a new
large-scale, cross-domain pedestrian attribute recognition dataset to fill the
data gap, termed MSP60K. It consists of 60,122 images and 57 attribute
annotations across eight scenarios. Synthetic degradation is also conducted to
further narrow the gap between the dataset and real-world challenging
scenarios. To establish a more rigorous benchmark, we evaluate 17
representative PAR models under both random and cross-domain split protocols on
our dataset. Additionally, we propose an innovative Large Language Model (LLM)
augmented PAR framework, named LLM-PAR. This framework processes pedestrian
images through a Vision Transformer (ViT) backbone to extract features and
introduces a multi-embedding query Transformer to learn partial-aware features
for attribute classification. Significantly, we enhance this framework with LLM
for ensemble learning and visual feature augmentation. Comprehensive
experiments across multiple PAR benchmark datasets have thoroughly validated
the efficacy of our proposed framework. The dataset and source code
accompanying this paper will be made publicly available at
\url{https://github.com/Event-AHU/OpenPAR}.

摘要：行人屬性辨識 (PAR) 是以人為中心的研究所中不可或缺的任務之一。然而，現有的資料集忽略了不同的領域（例如，環境、時間、族群和資料來源），只進行簡單的隨機分割，而這些資料集的效能已接近飽和。在過去的五年中，沒有大型資料集向公眾開放。為了解決這個問題，本文提出了一個新的、大規模的、跨領域的行人屬性辨識資料集，以填補資料差距，稱為 MSP60K。它包含 60,122 張影像和 57 個屬性註解，橫跨八個場景。合成降級也進行以進一步縮小資料集和現實世界挑戰場景之間的差距。為了建立更嚴謹的基準，我們在我們的資料集上，在隨機和跨領域分割協定的情況下，評估 17 個具代表性的 PAR 模型。此外，我們提出了一個創新的大型語言模型 (LLM) 增強 PAR 框架，稱為 LLM-PAR。此框架透過 Vision Transformer (ViT) 主幹處理行人影像以提取特徵，並引入多嵌入查詢 Transformer 來學習部分感知特徵以進行屬性分類。重要的是，我們透過 LLM 增強此框架以進行整體學習和視覺特徵增強。跨多個 PAR 基準資料集的綜合實驗已徹底驗證了我們提出的框架的效能。本文附帶的資料集和原始碼將在 \url{https://github.com/Event-AHU/OpenPAR} 公開。

##### **SEMDR: A Semantic-Aware Dual Encoder Model for Legal Judgment Prediction with Legal Clue Tracing**
2408.09717v1 by Pengjie Liu, Wang Zhang, Yulong Ding, Xuefeng Zhang, Shuang-Hua Yang

Legal Judgment Prediction (LJP) aims to form legal judgments based on the
criminal fact description. However, researchers struggle to classify confusing
criminal cases, such as robbery and theft, which requires LJP models to
distinguish the nuances between similar crimes. Existing methods usually design
handcrafted features to pick up necessary semantic legal clues to make more
accurate legal judgment predictions. In this paper, we propose a Semantic-Aware
Dual Encoder Model (SEMDR), which designs a novel legal clue tracing mechanism
to conduct fine-grained semantic reasoning between criminal facts and
instruments. Our legal clue tracing mechanism is built from three reasoning
levels: 1) Lexicon-Tracing, which aims to extract criminal facts from criminal
descriptions; 2) Sentence Representation Learning, which contrastively trains
language models to better represent confusing criminal facts; 3) Multi-Fact
Reasoning, which builds a reasons graph to propagate semantic clues among fact
nodes to capture the subtle difference among criminal facts. Our legal clue
tracing mechanism helps SEMDR achieve state-of-the-art on the CAIL2018 dataset
and shows its advance in few-shot scenarios. Our experiments show that SEMDR
has a strong ability to learn more uniform and distinguished representations
for criminal facts, which helps to make more accurate predictions on confusing
criminal cases and reduces the model uncertainty during making judgments. All
codes will be released via GitHub.

摘要：法律判決預測 (LJP) 旨在根據犯罪事實描述形成法律判決。然而，研究人員難以對搶劫和盜竊等令人困惑的刑事案件進行分類，這需要 LJP 模型區分類似犯罪之間的細微差別。現有方法通常設計手工特徵以獲取必要的語義法律線索，以做出更準確的法律判決預測。在本文中，我們提出了一個語義感知雙編碼器模型 (SEMDR)，它設計了一種新穎的法律線索追蹤機制，以在犯罪事實和工具之間進行細粒度的語義推理。我們的法律線索追蹤機制建立在三個推理層級之上：1) 詞彙追蹤，旨在從犯罪描述中提取犯罪事實；2) 句子表示學習，對比訓練語言模型以更好地表示令人困惑的犯罪事實；3) 多事實推理，構建一個原因圖，在事實節點之間傳播語義線索，以捕捉犯罪事實之間的細微差別。我們的法律線索追蹤機制幫助 SEMDR 在 CAIL2018 資料集上實現了最先進的技術，並展示了其在少鏡頭場景中的進步。我們的實驗表明，SEMDR 具有學習更統一和區別的犯罪事實表示的強大能力，這有助於對令人困惑的刑事案件做出更準確的預測，並在做出判決時減少模型的不確定性。所有代碼都將通過 GitHub 發布。

##### **HYDEN: Hyperbolic Density Representations for Medical Images and Reports**
2408.09715v2 by Zhi Qiao, Linbin Han, Xiantong Zhen, Jia-Hong Gao, Zhen Qian

In light of the inherent entailment relations between images and text,
hyperbolic point vector embeddings, leveraging the hierarchical modeling
advantages of hyperbolic space, have been utilized for visual semantic
representation learning. However, point vector embedding approaches fail to
address the issue of semantic uncertainty, where an image may have multiple
interpretations, and text may refer to different images, a phenomenon
particularly prevalent in the medical domain. Therefor, we propose
\textbf{HYDEN}, a novel hyperbolic density embedding based image-text
representation learning approach tailored for specific medical domain data.
This method integrates text-aware local features alongside global features from
images, mapping image-text features to density features in hyperbolic space via
using hyperbolic pseudo-Gaussian distributions. An encapsulation loss function
is employed to model the partial order relations between image-text density
distributions. Experimental results demonstrate the interpretability of our
approach and its superior performance compared to the baseline methods across
various zero-shot tasks and different datasets.

摘要：鉴于图像和文本之间固有的蕴涵关系，利用双曲空间的分层建模优势，双曲点向量嵌入已被用于视觉语义表示学习。然而，点向量嵌入方法未能解决语义不确定性问题，其中一个图像可能有多种解释，而文本可能指代不同的图像，这种现象在医学领域尤为普遍。因此，我们提出了\textbf{HYDEN}，这是一种新颖的基于双曲密度嵌入的图像-文本表示学习方法，专为特定医学领域数据量身定制。此方法将文本感知局部特征与图像中的全局特征相结合，通过使用双曲伪高斯分布将图像-文本特征映射到双曲空间中的密度特征。封装损失函数用于对图像-文本密度分布之间的偏序关系进行建模。实验结果证明了我们方法的可解释性，并且在各种零样本任务和不同数据集上，其性能优于基线方法。

##### **Partial-Multivariate Model for Forecasting**
2408.09703v1 by Jaehoon Lee, Hankook Lee, Sungik Choi, Sungjun Cho, Moontae Lee

When solving forecasting problems including multiple time-series features,
existing approaches often fall into two extreme categories, depending on
whether to utilize inter-feature information: univariate and
complete-multivariate models. Unlike univariate cases which ignore the
information, complete-multivariate models compute relationships among a
complete set of features. However, despite the potential advantage of
leveraging the additional information, complete-multivariate models sometimes
underperform univariate ones. Therefore, our research aims to explore a middle
ground between these two by introducing what we term Partial-Multivariate
models where a neural network captures only partial relationships, that is,
dependencies within subsets of all features. To this end, we propose PMformer,
a Transformer-based partial-multivariate model, with its training algorithm. We
demonstrate that PMformer outperforms various univariate and
complete-multivariate models, providing a theoretical rationale and empirical
analysis for its superiority. Additionally, by proposing an inference technique
for PMformer, the forecasting accuracy is further enhanced. Finally, we
highlight other advantages of PMformer: efficiency and robustness under missing
features.

摘要：在解決包含多個時序特徵的預測問題時，現有方法通常會根據是否利用特徵間資訊而分成兩個極端類別：單變數和完全多變數模型。與忽略資訊的單變數案例不同，完全多變數模型會計算一組完整特徵之間的關係。然而，儘管利用額外資訊具有潛在優勢，但完全多變數模型有時表現不如單變數模型。因此，我們的研究旨在透過引入我們稱之為部分多變數模型的中間點來探索這兩者之間的折衷，其中神經網路只擷取部分關係，即所有特徵子集內的依賴關係。為此，我們提出基於 Transformer 的部分多變數模型 PMformer，並提供其訓練演算法。我們證明 PMformer 優於各種單變數和完全多變數模型，並提供理論依據和經驗分析以證明其優越性。此外，透過提出 PMformer 的推論技術，進一步提升預測準確度。最後，我們強調 PMformer 的其他優點：在特徵缺失的情況下具有效率和穩健性。

##### **Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering**
2408.09702v1 by Ruofan Liang, Zan Gojcic, Merlin Nimier-David, David Acuna, Nandita Vijaykumar, Sanja Fidler, Zian Wang

The correct insertion of virtual objects in images of real-world scenes
requires a deep understanding of the scene's lighting, geometry and materials,
as well as the image formation process. While recent large-scale diffusion
models have shown strong generative and inpainting capabilities, we find that
current models do not sufficiently "understand" the scene shown in a single
picture to generate consistent lighting effects (shadows, bright reflections,
etc.) while preserving the identity and details of the composited object. We
propose using a personalized large diffusion model as guidance to a physically
based inverse rendering process. Our method recovers scene lighting and
tone-mapping parameters, allowing the photorealistic composition of arbitrary
virtual objects in single frames or videos of indoor or outdoor scenes. Our
physically based pipeline further enables automatic materials and tone-mapping
refinement.

摘要：要將虛擬物件正確插入真實場景的影像中，需要深入了解場景的燈光、幾何形狀和材質，以及影像形成過程。儘管最近的大型擴散模型展現出強大的生成和修復功能，我們發現目前的模型並不足以「理解」單張圖片中顯示的場景，無法產生一致的燈光效果（陰影、明亮的反光等），同時保留合成物件的身分和細節。我們建議使用個人化大型擴散模型作為指導，進行基於物理的反向渲染處理。我們的做法會還原場景的燈光和色調對應參數，讓虛擬物件能寫實地合成在室內或戶外場景的單一影像或影片中。我們的基於物理的管道進一步支援自動材質和色調對應的精煉。

##### **Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer**
2408.09701v1 by Mingda Li, Abhijit Mishra, Utkarsh Mujumdar

The use of Large Language Models (LLMs) for program code generation has
gained substantial attention, but their biases and limitations with non-English
prompts challenge global inclusivity. This paper investigates the complexities
of multilingual prompt-based code generation. Our evaluations of LLMs,
including CodeLLaMa and CodeGemma, reveal significant disparities in code
quality for non-English prompts; we also demonstrate the inadequacy of simple
approaches like prompt translation, bootstrapped data augmentation, and
fine-tuning. To address this, we propose a zero-shot cross-lingual approach
using a neural projection technique, integrating a cross-lingual encoder like
LASER artetxe2019massively to map multilingual embeddings from it into the
LLM's token space. This method requires training only on English data and
scales effectively to other languages. Results on a translated and
quality-checked MBPP dataset show substantial improvements in code quality.
This research promotes a more inclusive code generation landscape by empowering
LLMs with multilingual capabilities to support the diverse linguistic spectrum
in programming.

摘要：大型語言模型 (LLM) 用於程式碼產生已獲得大量關注，但其偏見和非英語提示的限制對全球包容性提出了挑戰。本文探討了多語言提示式程式碼產生的複雜性。我們對 LLM（包括 CodeLLaMa 和 CodeGemma）的評估顯示，非英語提示的程式碼品質存在顯著差異；我們還展示了提示翻譯、自舉資料擴充和微調等簡單方法的不足。為了解決這個問題，我們提出了一種使用神經投影技術的零次方跨語言方法，整合了一個跨語言編碼器（如 artetxe2019massively 的 LASER）將其多語言嵌入對應到 LLM 的標記空間。此方法僅需要使用英語資料進行訓練，並能有效擴充到其他語言。在經過翻譯和品質檢查的 MBPP 資料集上的結果顯示出程式碼品質有顯著提升。這項研究透過賦予 LLM 多語言能力來支援程式設計中多樣的語言範圍，促進了一個更具包容性的程式碼產生環境。

##### **Harnessing Multimodal Large Language Models for Multimodal Sequential Recommendation**
2408.09698v2 by Yuyang Ye, Zhi Zheng, Yishan Shen, Tianshu Wang, Hengruo Zhang, Peijun Zhu, Runlong Yu, Kai Zhang, Hui Xiong

Recent advances in Large Language Models (LLMs) have demonstrated significant
potential in the field of Recommendation Systems (RSs). Most existing studies
have focused on converting user behavior logs into textual prompts and
leveraging techniques such as prompt tuning to enable LLMs for recommendation
tasks. Meanwhile, research interest has recently grown in multimodal
recommendation systems that integrate data from images, text, and other sources
using modality fusion techniques. This introduces new challenges to the
existing LLM-based recommendation paradigm which relies solely on text modality
information. Moreover, although Multimodal Large Language Models (MLLMs)
capable of processing multi-modal inputs have emerged, how to equip MLLMs with
multi-modal recommendation capabilities remains largely unexplored. To this
end, in this paper, we propose the Multimodal Large Language Model-enhanced
Multimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic
user preference, we design a two-stage user preference summarization method.
Specifically, we first utilize an MLLM-based item-summarizer to extract image
feature given an item and convert the image into text. Then, we employ a
recurrent user preference summarization generation paradigm to capture the
dynamic changes in user preferences based on an LLM-based user-summarizer.
Finally, to enable the MLLM for multi-modal recommendation task, we propose to
fine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)
techniques. Extensive evaluations across various datasets validate the
effectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt
to the evolving dynamics of user preferences.

摘要：<paragraph>大型語言模型 (LLM) 的最新進展已證明在推薦系統 (RS) 領域具有顯著的潛力。現有的大部分研究都專注於將使用者行為記錄轉換為文字提示，並利用提示調整等技術，讓 LLM 能執行推薦任務。同時，最近的研究興趣已轉向多模態推薦系統，該系統使用模態融合技術整合來自影像、文字和其他來源的資料。這為現有的基於 LLM 的推薦範例帶來了新的挑戰，而該範例僅依賴文字模態資訊。此外，儘管已經出現能夠處理多模態輸入的多模態大型語言模型 (MLLM)，但如何讓 MLLM 具備多模態推薦功能在很大程度上仍未開發。為此，我們在本文中提出了多模態大型語言模型增強的多模態序列推薦 (MLLM-MSR) 模型。為了捕捉動態使用者偏好，我們設計了一個兩階段使用者偏好摘要方法。具體來說，我們首先利用基於 MLLM 的項目摘要器，根據項目提取影像特徵，並將影像轉換為文字。然後，我們採用遞迴使用者偏好摘要生成範例，根據基於 LLM 的使用者摘要器捕捉使用者偏好的動態變化。最後，為了讓 MLLM 能執行多模態推薦任務，我們建議使用監督微調 (SFT) 技術微調基於 MLLM 的推薦器。跨各種資料集的廣泛評估驗證了 MLLM-MSR 的有效性，展示了其捕捉和適應使用者偏好演變動態的卓越能力。</paragraph>

##### **LightWeather: Harnessing Absolute Positional Encoding to Efficient and Scalable Global Weather Forecasting**
2408.09695v1 by Yisong Fu, Fei Wang, Zezhi Shao, Chengqing Yu, Yujie Li, Zhao Chen, Zhulin An, Yongjun Xu

Recently, Transformers have gained traction in weather forecasting for their
capability to capture long-term spatial-temporal correlations. However, their
complex architectures result in large parameter counts and extended training
times, limiting their practical application and scalability to global-scale
forecasting. This paper aims to explore the key factor for accurate weather
forecasting and design more efficient solutions. Interestingly, our empirical
findings reveal that absolute positional encoding is what really works in
Transformer-based weather forecasting models, which can explicitly model the
spatial-temporal correlations even without attention mechanisms. We
theoretically prove that its effectiveness stems from the integration of
geographical coordinates and real-world time features, which are intrinsically
related to the dynamics of weather. Based on this, we propose LightWeather, a
lightweight and effective model for station-based global weather forecasting.
We employ absolute positional encoding and a simple MLP in place of other
components of Transformer. With under 30k parameters and less than one hour of
training time, LightWeather achieves state-of-the-art performance on global
weather datasets compared to other advanced DL methods. The results underscore
the superiority of integrating spatial-temporal knowledge over complex
architectures, providing novel insights for DL in weather forecasting.

摘要：<paragraph>最近，Transformer 在天气预报中获得了关注，因为它能够捕捉长期时空相关性。然而，它们复杂的架构导致参数计数庞大且训练时间延长，限制了它们在全球规模预报中的实际应用和可扩展性。本文旨在探索准确天气预报的关键因素并设计更有效的解决方案。有趣的是，我们的经验结果表明，绝对位置编码是基于 Transformer 的天气预报模型中真正起作用的因素，即使没有注意力机制，它也可以明确建模时空相关性。我们从理论上证明了其有效性源于地理坐标和现实世界时间特征的整合，这些特征与天气的动态本质上相关。基于此，我们提出了 LightWeather，这是一种用于基于站点的全球天气预报的轻量级且有效的模型。我们采用绝对位置编码和简单的 MLP 代替 Transformer 的其他组件。LightWeather 的参数不到 30k，训练时间不到一小时，与其他先进的 DL 方法相比，在全球天气数据集上实现了最先进的性能。结果强调了将时空知识集成到复杂架构中的优势，为天气预报中的 DL 提供了新的见解。</paragraph>

##### **Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts**
2408.09688v1 by Jiaqing Liu, Chong Deng, Qinglin Zhang, Qian Chen, Hai Yu, Wen Wang

Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and
various spoken language phenomena such as disfluencies, ungrammatical
sentences, and incomplete sentences, hence suffering from poor readability. To
improve readability, we propose a Contextualized Spoken-to-Written conversion
(CoS2W) task to address ASR and grammar errors and also transfer the informal
text into the formal style with content preserved, utilizing contexts and
auxiliary information. This task naturally matches the in-context learning
capabilities of Large Language Models (LLMs). To facilitate comprehensive
comparisons of various LLMs, we construct a document-level Spoken-to-Written
conversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study
the impact of different granularity levels on the CoS2W performance, and
propose methods to exploit contexts and auxiliary information to enhance the
outputs. Experimental results reveal that LLMs have the potential to excel in
the CoS2W task, particularly in grammaticality and formality, our methods
achieve effective understanding of contexts and auxiliary information by LLMs.
We further investigate the effectiveness of using LLMs as evaluators and find
that LLM evaluators show strong correlations with human evaluations on rankings
of faithfulness and formality, which validates the reliability of LLM
evaluators for the CoS2W task.

摘要：自動語音辨識 (ASR) 轉錄會出現辨識錯誤和各種口語現象，例如言語不流暢、不符合文法的句子和不完整的句子，因此可讀性很差。為了改善可讀性，我們提出語境化口語轉書寫轉換 (CoS2W) 任務來解決 ASR 和文法錯誤，並在保留內容的情況下將非正式文字轉換為正式風格，利用語境和輔助資訊。此任務自然符合大型語言模型 (LLM) 的語境學習能力。為了促進對各種 LLM 的全面比較，我們建構了 ASR 轉錄基準 (SWAB) 資料集的文檔級口語轉書寫轉換。使用 SWAB，我們研究了不同粒度層級對 CoS2W 效能的影響，並提出利用語境和輔助資訊來增強輸出的方法。實驗結果顯示，LLM 有潛力在 CoS2W 任務中表現出色，特別是在文法和正式性方面，我們的這些方法讓 LLM 有效理解語境和輔助資訊。我們進一步探討將 LLM 用作評估器的有效性，發現 LLM 評估器在忠實度和正式性的排名上與人類評估有很強的相關性，這驗證了 LLM 評估器在 CoS2W 任務中的可靠性。

##### **Simulating Field Experiments with Large Language Models**
2408.09682v1 by Yaoyu Chen, Yuheng Hu, Yingda Lu

Prevailing large language models (LLMs) are capable of human responses
simulation through its unprecedented content generation and reasoning
abilities. However, it is not clear whether and how to leverage LLMs to
simulate field experiments. In this paper, we propose and evaluate two
prompting strategies: the observer mode that allows a direct prediction on main
conclusions and the participant mode that simulates distributions of responses
from participants. Using this approach, we examine fifteen well cited field
experimental papers published in INFORMS and MISQ, finding encouraging
alignments between simulated experimental results and the actual results in
certain scenarios. We further identify topics of which LLMs underperform,
including gender difference and social norms related research. Additionally,
the automatic and standardized workflow proposed in this paper enables the
possibility of a large-scale screening of more papers with field experiments.
This paper pioneers the utilization of large language models (LLMs) for
simulating field experiments, presenting a significant extension to previous
work which focused solely on lab environments. By introducing two novel
prompting strategies, observer and participant modes, we demonstrate the
ability of LLMs to both predict outcomes and replicate participant responses
within complex field settings. Our findings indicate a promising alignment with
actual experimental results in certain scenarios, achieving a stimulation
accuracy of 66% in observer mode. This study expands the scope of potential
applications for LLMs and illustrates their utility in assisting researchers
prior to engaging in expensive field experiments. Moreover, it sheds light on
the boundaries of LLMs when used in simulating field experiments, serving as a
cautionary note for researchers considering the integration of LLMs into their
experimental toolkit.

摘要：<paragraph>現有的大型語言模型 (LLM) 可以透過前所未有的內容產生和推理能力模擬人類的回應。然而，目前尚不清楚是否以及如何利用 LLM 來模擬實地實驗。在本文中，我們提出並評估兩種提示策略：觀察者模式，允許對主要結論進行直接預測，以及參與者模式，模擬參與者的回應分佈。使用這種方法，我們檢視了 15 篇在 INFORMS 和 MISQ 發表且被廣泛引用的實地實驗論文，發現模擬實驗結果與特定情境中的實際結果之間存在令人鼓舞的一致性。我們進一步找出 LLM 表現不佳的主題，包括性別差異和與社會規範相關的研究。此外，本文提出的自動化和標準化工作流程讓大規模篩選更多包含實地實驗的論文成為可能。本文開創了利用大型語言模型 (LLM) 模擬實地實驗的方法，對先前僅專注於實驗室環境的研究進行了重大延伸。透過引入兩種新穎的提示策略（觀察者模式和參與者模式），我們展示了 LLM 在複雜的實地設定中預測結果和複製參與者回應的能力。我們的研究結果表明，在特定情境中與實際實驗結果有令人滿意的吻合度，在觀察者模式中達到 66% 的刺激準確度。這項研究擴大了 LLM 的潛在應用範圍，並說明了其在協助研究人員進行昂貴的實地實驗之前提供協助的效用。此外，它也闡明了 LLM 在用於模擬實地實驗時的界限，作為研究人員在考慮將 LLM 整合到其實驗工具包中時的一個警示。</paragraph>

##### **MambaLoc: Efficient Camera Localisation via State Space Model**
2408.09680v2 by Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni

Location information is pivotal for the automation and intelligence of
terminal devices and edge-cloud IoT systems, such as autonomous vehicles and
augmented reality. However, achieving reliable positioning across diverse IoT
applications remains challenging due to significant training costs and the
necessity of densely collected data. To tackle these issues, we have
innovatively applied the selective state space (SSM) model to visual
localization, introducing a new model named MambaLoc. The proposed model
demonstrates exceptional training efficiency by capitalizing on the SSM model's
strengths in efficient feature extraction, rapid computation, and memory
optimization, and it further ensures robustness in sparse data environments due
to its parameter sparsity. Additionally, we propose the Global Information
Selector (GIS), which leverages selective SSM to implicitly achieve the
efficient global feature extraction capabilities of Non-local Neural Networks.
This design leverages the computational efficiency of the SSM model alongside
the Non-local Neural Networks' capacity to capture long-range dependencies with
minimal layers. Consequently, the GIS enables effective global information
capture while significantly accelerating convergence. Our extensive
experimental validation using public indoor and outdoor datasets first
demonstrates our model's effectiveness, followed by evidence of its versatility
with various existing localization models. Our code and models are publicly
available to support further research and development in this area.

摘要：位置資訊對於自動化與智慧型終端裝置和邊緣雲端物聯網系統至關重要，例如自駕車和擴增實境。然而，由於顯著的訓練成本和密集收集資料的必要性，在不同的物聯網應用中實現可靠的定位仍然具有挑戰性。為了解決這些問題，我們創新地將選擇性狀態空間 (SSM) 模型應用於視覺定位，並引入了一個名為 MambaLoc 的新模型。所提出的模型透過利用 SSM 模型在有效特徵萃取、快速運算和記憶體最佳化方面的優勢，展現出卓越的訓練效率，並且由於其參數稀疏性，進一步確保在稀疏資料環境中的穩健性。此外，我們提出了全球資訊選擇器 (GIS)，它利用選擇性 SSM 來隱式實現非局部神經網路的有效全局特徵萃取能力。此設計利用 SSM 模型的運算效率，以及非局部神經網路在最少層數中擷取長程依賴關係的能力。因此，GIS 能夠有效擷取全局資訊，同時顯著加速收斂。我們使用公開的室內和室外資料集進行廣泛的實驗驗證，首先證明了我們模型的有效性，接著證明了它與各種現有定位模型的多功能性。我們的程式碼和模型公開提供，以支持此領域進一步的研究和開發。

##### **Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey**
2408.09675v1 by Ruiqi Zhang, Jing Hou, Florian Walter, Shangding Gu, Jiayi Guan, Florian Röhrbein, Yali Du, Panpan Cai, Guang Chen, Alois Knoll

Reinforcement Learning (RL) is a potent tool for sequential decision-making
and has achieved performance surpassing human capabilities across many
challenging real-world tasks. As the extension of RL in the multi-agent system
domain, multi-agent RL (MARL) not only need to learn the control policy but
also requires consideration regarding interactions with all other agents in the
environment, mutual influences among different system components, and the
distribution of computational resources. This augments the complexity of
algorithmic design and poses higher requirements on computational resources.
Simultaneously, simulators are crucial to obtain realistic data, which is the
fundamentals of RL. In this paper, we first propose a series of metrics of
simulators and summarize the features of existing benchmarks. Second, to ease
comprehension, we recall the foundational knowledge and then synthesize the
recently advanced studies of MARL-related autonomous driving and intelligent
transportation systems. Specifically, we examine their environmental modeling,
state representation, perception units, and algorithm design. Conclusively, we
discuss open challenges as well as prospects and opportunities. We hope this
paper can help the researchers integrate MARL technologies and trigger more
insightful ideas toward the intelligent and autonomous driving.

摘要：強化學習 (RL) 是用於順序決策制定的一種強大工具，並已在許多具有挑戰性的現實世界任務中實現了超越人類能力的效能。作為 RL 在多主體系統領域的延伸，多主體 RL (MARL) 不僅需要學習控制政策，還需要考慮與環境中所有其他主體的互動、不同系統組件之間的相互影響以及計算資源的分配。這增加了演算法設計的複雜性，並對計算資源提出了更高的要求。同時，模擬器對於取得現實資料至關重要，而這是 RL 的基礎。在本文中，我們首先提出了一系列模擬器的指標，並總結了現有基準的特徵。其次，為了便於理解，我們回顧了基礎知識，然後綜合了最近關於 MARL 相關自動駕駛和智慧運輸系統的先進研究。具體來說，我們探討了它們的環境建模、狀態表示、感知單元和演算法設計。最後，我們討論了開放性挑戰以及前景和機遇。我們希望本文能幫助研究人員整合 MARL 技術，並激發更多關於智慧和自動駕駛的深入見解。

##### **BLADE: Benchmarking Language Model Agents for Data-Driven Science**
2408.09667v1 by Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Yikun Zhang, Tianmai M. Zhang, Lanyi Zhu, Mike A. Merrill, Jeffrey Heer, Tim Althoff

Data-driven scientific discovery requires the iterative integration of
scientific domain knowledge, statistical expertise, and an understanding of
data semantics to make nuanced analytical decisions, e.g., about which
variables, transformations, and statistical models to consider. LM-based agents
equipped with planning, memory, and code execution capabilities have the
potential to support data-driven science. However, evaluating agents on such
open-ended tasks is challenging due to multiple valid approaches, partially
correct steps, and different ways to express the same decisions. To address
these challenges, we present BLADE, a benchmark to automatically evaluate
agents' multifaceted approaches to open-ended research questions. BLADE
consists of 12 datasets and research questions drawn from existing scientific
literature, with ground truth collected from independent analyses by expert
data scientists and researchers. To automatically evaluate agent responses, we
developed corresponding computational methods to match different
representations of analyses to this ground truth. Though language models
possess considerable world knowledge, our evaluation shows that they are often
limited to basic analyses. However, agents capable of interacting with the
underlying data demonstrate improved, but still non-optimal, diversity in their
analytical decision making. Our work enables the evaluation of agents for
data-driven science and provides researchers deeper insights into agents'
analysis approaches.

摘要：資料驅動的科學發現需要反覆整合科學領域知識、統計專業知識和資料語義理解，才能做出細微的分析決策，例如，要考慮哪些變數、轉換和統計模型。具備規劃、記憶和程式碼執行能力的 LM 基礎代理具有支援資料驅動科學的潛力。然而，由於有多種有效方法、部分正確的步驟和表達相同決策的不同方式，在這種開放式任務上評估代理具有挑戰性。為了應對這些挑戰，我們提出了 BLADE，一個自動評估代理對開放式研究問題的多方面方法的基準。BLADE 包含 12 個資料集和從現有科學文獻中提取的研究問題，其中基本事實由專家資料科學家和研究人員的獨立分析收集。為了自動評估代理回應，我們開發了對應的計算方法，以將分析的不同表示與此基本事實相匹配。儘管語言模型擁有大量的世界知識，但我們的評估表明，它們通常僅限於基本分析。然而，能夠與基礎資料互動的代理展示出其分析決策制定中改進的，但仍然不是最佳的多樣性。我們的研究能夠評估資料驅動科學的代理，並為研究人員提供對代理分析方法的更深入見解。

##### **A Comparison of Large Language Model and Human Performance on Random Number Generation Tasks**
2408.09656v2 by Rachel M. Harrison

Random Number Generation Tasks (RNGTs) are used in psychology for examining
how humans generate sequences devoid of predictable patterns. By adapting an
existing human RNGT for an LLM-compatible environment, this preliminary study
tests whether ChatGPT-3.5, a large language model (LLM) trained on
human-generated text, exhibits human-like cognitive biases when generating
random number sequences. Initial findings indicate that ChatGPT-3.5 more
effectively avoids repetitive and sequential patterns compared to humans, with
notably lower repeat frequencies and adjacent number frequencies. Continued
research into different models, parameters, and prompting methodologies will
deepen our understanding of how LLMs can more closely mimic human random
generation behaviors, while also broadening their applications in cognitive and
behavioral science research.

摘要：隨機數字產生任務 (RNGT) 用於心理學中，用於檢查人類如何產生沒有可預測模式的序列。通過將現有的 RNGT 適應到與 LLM 相容的環境中，這項初步研究測試了 ChatGPT-3.5，一個訓練於人類生成文本的大語言模型 (LLM)，在產生隨機數字序列時是否表現出類似人類的認知偏誤。初步結果表明，與人類相比，ChatGPT-3.5 更有效地避免了重複和順序模式，重複頻率和相鄰數字頻率明顯較低。持續研究不同的模型、參數和提示方法將加深我們對 LLM 如何更接近模擬人類隨機產生行為的理解，同時擴展它們在認知和行為科學研究中的應用。

##### **Data-driven Conditional Instrumental Variables for Debiasing Recommender Systems**
2408.09651v1 by Zhirong Huang, Shichao Zhang, Debo Cheng, Jiuyong Li, Lin Liu, Guangquan Lu

In recommender systems, latent variables can cause user-item interaction data
to deviate from true user preferences. This biased data is then used to train
recommendation models, further amplifying the bias and ultimately compromising
both recommendation accuracy and user satisfaction. Instrumental Variable (IV)
methods are effective tools for addressing the confounding bias introduced by
latent variables; however, identifying a valid IV is often challenging. To
overcome this issue, we propose a novel data-driven conditional IV (CIV)
debiasing method for recommender systems, called CIV4Rec. CIV4Rec automatically
generates valid CIVs and their corresponding conditioning sets directly from
interaction data, significantly reducing the complexity of IV selection while
effectively mitigating the confounding bias caused by latent variables in
recommender systems. Specifically, CIV4Rec leverages a variational autoencoder
(VAE) to generate the representations of the CIV and its conditional set from
interaction data, followed by the application of least squares to derive causal
representations for click prediction. Extensive experiments on two real-world
datasets, Movielens-10M and Douban-Movie, demonstrate that our CIV4Rec
successfully identifies valid CIVs, effectively reduces bias, and consequently
improves recommendation accuracy.

摘要：在推薦系統中，潛在變數會導致使用者與項目互動的資料偏離真正的使用者偏好。此有偏差的資料接著用於訓練推薦模型，進一步擴大偏差，並最終損害推薦準確度和使用者滿意度。工具變數 (IV) 方法是解決潛在變數引入的混淆偏差的有效工具；然而，找出有效的 IV 通常具有挑戰性。為了克服此問題，我們提出了一種新穎的資料驅動條件 IV (CIV) 去偏方法，用於推薦系統，稱為 CIV4Rec。CIV4Rec 自動從互動資料產生有效的 CIV 及其對應的條件集合，大幅降低 IV 選擇的複雜性，同時有效減輕推薦系統中潛在變數造成的混淆偏差。具體來說，CIV4Rec 採用變異自動編碼器 (VAE) 從互動資料產生 CIV 及其條件集合的表示，接著應用最小平方法導出用於點擊預測的因果表示。在兩個真實世界資料集 Movielens-10M 和 Douban-Movie 上進行的廣泛實驗證明，我們的 CIV4Rec 成功找出有效的 CIV，有效降低偏差，並因此提高推薦準確度。

##### **ExpoMamba: Exploiting Frequency SSM Blocks for Efficient and Effective Image Enhancement**
2408.09650v1 by Eashan Adhikarla, Kai Zhang, John Nicholson, Brian D. Davison

Low-light image enhancement remains a challenging task in computer vision,
with existing state-of-the-art models often limited by hardware constraints and
computational inefficiencies, particularly in handling high-resolution images.
Recent foundation models, such as transformers and diffusion models, despite
their efficacy in various domains, are limited in use on edge devices due to
their computational complexity and slow inference times. We introduce
ExpoMamba, a novel architecture that integrates components of the frequency
state space within a modified U-Net, offering a blend of efficiency and
effectiveness. This model is specifically optimized to address mixed exposure
challenges, a common issue in low-light image enhancement, while ensuring
computational efficiency. Our experiments demonstrate that ExpoMamba enhances
low-light images up to 2-3x faster than traditional models with an inference
time of 36.6 ms and achieves a PSNR improvement of approximately 15-20% over
competing models, making it highly suitable for real-time image processing
applications.

摘要：低光圖像增強在電腦視覺中仍是一項具有挑戰性的任務，現有的最先進模型通常受到硬體限制和計算效率低下的限制，特別是在處理高解析度圖像時。最近的基礎模型，例如Transformer和擴散模型，儘管在各個領域中都有效，但由於其計算複雜度和緩慢的推論時間，因此在邊緣裝置上的使用受到限制。我們引入了 ExpoMamba，這是一種新穎的架構，它將頻率狀態空間的組成部分整合到修改後的 U-Net 中，提供效率和效能的結合。此模型經過特別最佳化，以解決混合曝光挑戰，這是低光圖像增強中的常見問題，同時確保計算效率。我們的實驗表明，ExpoMamba 增強低光圖像的速度比傳統模型快 2-3 倍，推理時間為 36.6 毫秒，並且比競爭模型提高了約 15-20% 的 PSNR，使其非常適合於即時影像處理應用程式。

##### **Deep Learning-based Machine Condition Diagnosis using Short-time Fourier Transformation Variants**
2408.09649v1 by Eduardo Jr Piedad, Zherish Galvin Mayordo, Eduardo Prieto-Araujo, Oriol Gomis-Bellmunt

In motor condition diagnosis, electrical current signature serves as an
alternative feature to vibration-based sensor data, which is a more expensive
and invasive method. Machine learning (ML) techniques have been emerging in
diagnosing motor conditions using only motor phase current signals. This study
converts time-series motor current signals to time-frequency 2D plots using
Short-time Fourier Transform (STFT) methods. The motor current signal dataset
consists of 3,750 sample points with five classes - one healthy and four
synthetically-applied motor fault conditions, and with five loading conditions:
0, 25, 50, 75, and 100%. Five transformation methods are used on the dataset:
non-overlap and overlap STFTs, non-overlap and overlap realigned STFTs, and
synchrosqueezed STFT. Then, deep learning (DL) models based on the previous
Convolutional Neural Network (CNN) architecture are trained and validated from
generated plots of each method. The DL models of overlap-STFT, overlap R-STFT,
non-overlap STFT, non-overlap R-STFT, and synchrosqueezed-STFT performed
exceptionally with an average accuracy of 97.65, 96.03, 96.08, 96.32, and
88.27%, respectively. Four methods outperformed the previous best ML method
with 93.20% accuracy, while all five outperformed previous 2D-plot-based
methods with accuracy of 80.25, 74.80, and 82.80%, respectively, using the same
dataset, same DL architecture, and validation steps.

摘要：在电机状况诊断中，电流特征作为基于振动的传感器数据的替代特征，这是一种更昂贵且侵入性的方法。机器学习 (ML) 技术已在仅使用电机相电流信号诊断电机状况中出现。本研究使用短时傅里叶变换 (STFT) 方法将时序电机电流信号转换为时频 2D 图。电机电流信号数据集包含 3,750 个样本点，分为五类——一种健康状态和四种合成应用的电机故障状况，以及五种负载条件：0、25、50、75 和 100%。对数据集使用五种变换方法：非重叠和重叠 STFT、非重叠和重叠重新对齐 STFT 以及同步压缩 STFT。然后，基于先前的卷积神经网络 (CNN) 架构的深度学习 (DL) 模型根据每种方法生成的图进行训练和验证。重叠 STFT、重叠 R-STFT、非重叠 STFT、非重叠 R-STFT 和同步压缩 STFT 的 DL 模型表现出色，平均准确率分别为 97.65、96.03、96.08、96.32 和 88.27%。四种方法优于之前的最佳 ML 方法，准确率为 93.20%，而所有五种方法均优于之前的基于 2D 图的方法，使用相同的数据集、相同的 DL 架构和验证步骤，准确率分别为 80.25、74.80 和 82.80%。

##### **Debiased Contrastive Representation Learning for Mitigating Dual Biases in Recommender Systems**
2408.09646v1 by Zhirong Huang, Shichao Zhang, Debo Cheng, Jiuyong Li, Lin Liu, Guixian Zhang

In recommender systems, popularity and conformity biases undermine
recommender effectiveness by disproportionately favouring popular items,
leading to their over-representation in recommendation lists and causing an
unbalanced distribution of user-item historical data. We construct a causal
graph to address both biases and describe the abstract data generation
mechanism. Then, we use it as a guide to develop a novel Debiased Contrastive
Learning framework for Mitigating Dual Biases, called DCLMDB. In DCLMDB, both
popularity bias and conformity bias are handled in the model training process
by contrastive learning to ensure that user choices and recommended items are
not unduly influenced by conformity and popularity. Extensive experiments on
two real-world datasets, Movielens-10M and Netflix, show that DCLMDB can
effectively reduce the dual biases, as well as significantly enhance the
accuracy and diversity of recommendations.

摘要：在推薦系統中，流行度和從眾偏差會透過不成比例地偏好熱門項目來破壞推薦效果，導致熱門項目在推薦清單中過度呈現，造成使用者與項目歷史資料分佈不平衡。我們建構一個因果圖來處理這兩種偏差，並描述抽象的資料產生機制。然後，我們利用它作為指南來開發一個新穎的減輕雙重偏差的對比式學習框架，稱為 DCLMDB。在 DCLMDB 中，流行度偏差和從眾偏差都會在模型訓練過程中透過對比式學習來處理，以確保使用者選擇和推薦項目不會受到從眾和流行度的過度影響。在兩個真實世界資料集 Movielens-10M 和 Netflix 上進行的大量實驗顯示，DCLMDB 可以有效地減少雙重偏差，並顯著提高推薦的準確性和多樣性。

##### **Exploring Wavelet Transformations for Deep Learning-based Machine Condition Diagnosis**
2408.09644v1 by Eduardo Jr Piedad, Christian Ainsley Del Rosario, Eduardo Prieto-Araujo, Oriol Gomis-Bellmunt

Deep learning (DL) strategies have recently been utilized to diagnose motor
faults by simply analyzing motor phase current signals, offering a less costly
and non-intrusive alternative to vibration sensors. This research transforms
these time-series current signals into time-frequency 2D representations via
Wavelet Transform (WT). The dataset for motor current signals includes 3,750
data points across five categories: one representing normal conditions and four
representing artificially induced faults, each under five different load
conditions: 0, 25, 50, 75, and 100%. The study employs five WT-based
techniques: WT-Amor, WT-Bump, WT-Morse, WSST-Amor, and WSST-Bump. Subsequently,
five DL models adopting prior Convolutional Neural Network (CNN) architecture
were developed and tested using the transformed 2D plots from each method. The
DL models for WT-Amor, WT-Bump, and WT-Morse showed remarkable effectiveness
with peak model accuracy of 90.93, 89.20, and 93.73%, respectively, surpassing
previous 2D-image-based methods that recorded accuracy of 80.25, 74.80, and
82.80% respectively using the identical dataset and validation protocol.
Notably, the WT-Morse approach slightly exceeded the formerly highest ML
technique, achieving a 93.20% accuracy. However, the two WSST methods that
utilized synchrosqueezing techniques faced difficulty accurately classifying
motor faults. The performance of Wavelet-based deep learning methods offers a
compelling alternative for machine condition monitoring.

摘要：深度学习 (DL) 策略最近已被用来诊断电机故障，只需分析电机相电流信号，从而提供了一种成本更低、对振动传感器无损害的替代方案。本研究通过小波变换 (WT) 将这些时序电流信号转换为时频 2D 表示。电机电流信号的数据集包括五类 3,750 个数据点：一类表示正常情况，四类表示人为引起的故障，每类在五种不同的负载条件下：0、25、50、75 和 100%。本研究采用了五种基于 WT 的技术：WT-Amor、WT-Bump、WT-Morse、WSST-Amor 和 WSST-Bump。随后，开发并测试了五个采用先前卷积神经网络 (CNN) 架构的 DL 模型，使用每种方法转换后的 2D 图。WT-Amor、WT-Bump 和 WT-Morse 的 DL 模型显示出显着的有效性，峰值模型准确率分别为 90.93、89.20 和 93.73%，超过了使用相同数据集和验证协议记录的准确率分别为 80.25、74.80 和 82.80% 的基于 2D 图像的先前方法。值得注意的是，WT-Morse 方法略微超过了以前最高的 ML 技术，准确率达到 93.20%。然而，利用同步压缩技术的两种 WSST 方法在准确分类电机故障方面遇到了困难。基于小波的深度学习方法的性能为机器状态监测提供了一个有力的替代方案。

##### **Acquiring Bidirectionality via Large and Small Language Models**
2408.09640v1 by Takumi Goto, Hiroyoshi Nagao, Yuta Koreeda

Using token representation from bidirectional language models (LMs) such as
BERT is still a widely used approach for token-classification tasks. Even
though there exist much larger unidirectional LMs such as Llama-2, they are
rarely used to replace the token representation of bidirectional LMs. In this
work, we hypothesize that their lack of bidirectionality is keeping them
behind. To that end, we propose to newly train a small backward LM and
concatenate its representations to those of existing LM for downstream tasks.
Through experiments in named entity recognition, we demonstrate that
introducing backward model improves the benchmark performance more than 10
points. Furthermore, we show that the proposed method is especially effective
for rare domains and in few-shot learning settings.

摘要：使用來自雙向語言模型 (LM) 的標記表示法（例如 BERT）仍然是標記分類任務中廣泛使用的方法。儘管存在更大的單向 LM，例如 Llama-2，但它們很少用於替換雙向 LM 的標記表示法。在這項工作中，我們假設它們缺乏雙向性，讓它們落後。為此，我們提議重新訓練一個小的後向 LM，並將其表示與現有 LM 的表示串接，以用於下游任務。透過命名實體識別的實驗，我們證明了引入後向模型將基準效能提升了 10 個點以上。此外，我們表明所提出的方法對於罕見領域和少次學習設定特別有效。

##### **How to Make the Most of LLMs' Grammatical Knowledge for Acceptability Judgments**
2408.09639v1 by Yusuke Ide, Yuto Nishida, Miyu Oba, Yusuke Sakai, Justin Vasselli, Hidetaka Kamigaito, Taro Watanabe

The grammatical knowledge of language models (LMs) is often measured using a
benchmark of linguistic minimal pairs, where LMs are presented with a pair of
acceptable and unacceptable sentences and required to judge which is
acceptable. The existing dominant approach, however, naively calculates and
compares the probabilities of paired sentences using LMs. Additionally, large
language models (LLMs) have yet to be thoroughly examined in this field. We
thus investigate how to make the most of LLMs' grammatical knowledge to
comprehensively evaluate it. Through extensive experiments of nine judgment
methods in English and Chinese, we demonstrate that a probability readout
method, in-template LP, and a prompting-based method, Yes/No probability
computing, achieve particularly high performance, surpassing the conventional
approach. Our analysis reveals their different strengths, e.g., Yes/No
probability computing is robust against token-length bias, suggesting that they
harness different aspects of LLMs' grammatical knowledge. Consequently, we
recommend using diverse judgment methods to evaluate LLMs comprehensively.

摘要：語言模型 (LM) 的語法知識通常使用語言最小對的基準來衡量，其中 LM 會出現一對可接受和不可接受的句子，並要求判斷哪個可以接受。然而，現有的主要方法天真地計算和比較配對句子的機率，使用 LM。此外，大型語言模型 (LLM) 尚未徹底檢查此領域。因此，我們研究如何充分利用 LLM 的語法知識來全面評估它。透過對英語和中文的九種判斷方法進行廣泛的實驗，我們證明了機率讀出方法、範本內 LP 和基於提示的方法、是/否機率計算，達到了特別高的效能，超越了傳統的方法。我們的分析揭示了它們不同的優點，例如，是/否機率計算對於令牌長度偏差具有穩健性，表明它們利用了 LLM 語法知識的不同面向。因此，我們建議使用不同的判斷方法來全面評估 LLM。

##### **Meta-Learning on Augmented Gene Expression Profiles for Enhanced Lung Cancer Detection**
2408.09635v1 by Arya Hadizadeh Moghaddam, Mohsen Nayebi Kerdabadi, Cuncong Zhong, Zijun Yao

Gene expression profiles obtained through DNA microarray have proven
successful in providing critical information for cancer detection classifiers.
However, the limited number of samples in these datasets poses a challenge to
employ complex methodologies such as deep neural networks for sophisticated
analysis. To address this "small data" dilemma, Meta-Learning has been
introduced as a solution to enhance the optimization of machine learning models
by utilizing similar datasets, thereby facilitating a quicker adaptation to
target datasets without the requirement of sufficient samples. In this study,
we present a meta-learning-based approach for predicting lung cancer from gene
expression profiles. We apply this framework to well-established deep learning
methodologies and employ four distinct datasets for the meta-learning tasks,
where one as the target dataset and the rest as source datasets. Our approach
is evaluated against both traditional and deep learning methodologies, and the
results show the superior performance of meta-learning on augmented source data
compared to the baselines trained on single datasets. Moreover, we conduct the
comparative analysis between meta-learning and transfer learning methodologies
to highlight the efficiency of the proposed approach in addressing the
challenges associated with limited sample sizes. Finally, we incorporate the
explainability study to illustrate the distinctiveness of decisions made by
meta-learning.

摘要：透過 DNA 微陣列取得的基因表現特徵，已證明能成功提供癌症偵測分類器的關鍵資訊。然而，這些資料集中樣本數目有限，對採用深度神經網路等複雜方法進行精密的分析構成挑戰。為了解決這個「小資料」困境，元學習已被引入作為一種解決方案，透過使用類似的資料集來增強機器學習模型的最佳化，從而促進更快速地適應目標資料集，而無需足夠的樣本。在本研究中，我們提出一個基於元學習的方法，從基因表現特徵預測肺癌。我們將這個架構應用於既定的深度學習方法，並採用四個不同的資料集進行元學習任務，其中一個作為目標資料集，其餘作為來源資料集。我們的方法針對傳統和深度學習方法進行評估，結果顯示元學習在擴增來源資料上的表現優於在單一資料集上訓練的基準。此外，我們進行元學習和遷移學習方法之間的比較分析，以強調所提出的方法在解決與樣本大小有限相關的挑戰方面的效率。最後，我們納入可解釋性研究，以說明元學習所做決策的獨特性。

##### **MoDeGPT: Modular Decomposition for Large Language Model Compression**
2408.09632v2 by Chi-Heng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel, Shikhar Tuli, Yilin Shen, Hongxia Jin, Yen-Chang Hsu

Large Language Models (LLMs) have reshaped the landscape of artificial
intelligence by demonstrating exceptional performance across various tasks.
However, substantial computational requirements make their deployment
challenging on devices with limited resources. Recently, compression methods
using low-rank matrix techniques have shown promise, yet these often lead to
degraded accuracy or introduce significant overhead in parameters and inference
latency. This paper introduces \textbf{Mo}dular \textbf{De}composition
(MoDeGPT), a novel structured compression framework that does not need recovery
fine-tuning while resolving the above drawbacks. MoDeGPT partitions the
Transformer block into modules comprised of matrix pairs and reduces the hidden
dimensions via reconstructing the module-level outputs. MoDeGPT is developed
based on a theoretical framework that utilizes three well-established matrix
decomposition algorithms -- Nystr\"om approximation, CR decomposition, and SVD
-- and applies them to our redefined transformer modules. Our comprehensive
experiments show MoDeGPT, without backward propagation, matches or surpasses
previous structured compression methods that rely on gradient information, and
saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3
and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%
compression rates. Moreover, the compression can be done on a single GPU within
a few hours and increases the inference throughput by up to 46%.

摘要：大型語言模型 (LLM) 透過在各種任務中展現絕佳的效能，重塑了人工智慧的樣貌。然而，龐大的運算需求讓它們在資源受限的裝置上部署時面臨挑戰。近來，採用低秩矩陣技術的壓縮方法展現了前景，但這些方法通常會導致準確度降低，或在參數和推論延遲方面造成顯著的負擔。這篇論文介紹了模組化分解 (MoDeGPT)，這是一個創新的結構化壓縮架構，不需要恢復微調，同時解決了上述缺點。MoDeGPT 將 Transformer 區塊分割成包含矩陣對的模組，並透過重建模組層級的輸出，減少隱藏維度。MoDeGPT 是根據一個理論架構開發的，該架構利用了三種完善的矩陣分解演算法——Nystr\"om 近似、CR 分解和 SVD——並將它們應用於我們重新定義的 Transformer 模組。我們的綜合實驗顯示，MoDeGPT 在沒有反向傳播的情況下，比得上或超越了依賴梯度資訊的先前結構化壓縮方法，並在壓縮 13B 模型時節省了 98% 的運算成本。在 \textsc{Llama}-2/3 和 OPT 模型上，MoDeGPT 在零次學習的效能上維持在 90-95%，壓縮率為 25-30%。此外，壓縮可以在單一 GPU 上於數小時內完成，並將推論處理量提升多達 46%。

##### **A Strategy to Combine 1stGen Transformers and Open LLMs for Automatic Text Classification**
2408.09629v1 by Claudio M. V. de Andrade, Washington Cunha, Davi Reis, Adriana Silvina Pagano, Leonardo Rocha, Marcos André Gonçalves

Transformer models have achieved state-of-the-art results, with Large
Language Models (LLMs), an evolution of first-generation transformers (1stTR),
being considered the cutting edge in several NLP tasks. However, the literature
has yet to conclusively demonstrate that LLMs consistently outperform 1stTRs
across all NLP tasks. This study compares three 1stTRs (BERT, RoBERTa, and
BART) with two open LLMs (Llama 2 and Bloom) across 11 sentiment analysis
datasets. The results indicate that open LLMs may moderately outperform or
match 1stTRs in 8 out of 11 datasets but only when fine-tuned. Given this
substantial cost for only moderate gains, the practical applicability of these
models in cost-sensitive scenarios is questionable. In this context, a
confidence-based strategy that seamlessly integrates 1stTRs with open LLMs
based on prediction certainty is proposed. High-confidence documents are
classified by the more cost-effective 1stTRs, while uncertain cases are handled
by LLMs in zero-shot or few-shot modes, at a much lower cost than fine-tuned
versions. Experiments in sentiment analysis demonstrate that our solution not
only outperforms 1stTRs, zero-shot, and few-shot LLMs but also competes closely
with fine-tuned LLMs at a fraction of the cost.

摘要：Transformer模型已取得最先進的成果，大型語言模型 (LLM) 作為第一代Transformer (1stTR) 的演進，被認為是多項 NLP 任務的尖端技術。然而，文獻尚未確鑿地證明，LLM 在所有 NLP 任務中都持續優於 1stTR。本研究比較了三個 1stTR（BERT、RoBERTa 和 BART）與兩個開放式 LLM（Llama 2 和 Bloom），涵蓋 11 個情緒分析資料集。結果顯示，開放式 LLM 在 11 個資料集中有 8 個資料集可能適度優於或匹配 1stTR，但僅限於微調時。考量到微調僅能帶來適度的增益，但成本卻相當高，這些模型在成本敏感場景中的實際適用性令人質疑。在此脈絡下，提出了一種基於信心的策略，可根據預測確定性將 1stTR 與開放式 LLM 無縫整合。由較具成本效益的 1stTR 分類高信心文件，而由 LLM 在零次學習或少次學習模式中處理不確定的案例，成本遠低於微調版本。情緒分析的實驗證明，我們的解決方案不僅優於 1stTR、零次學習和少次學習 LLM，而且在成本僅為微調 LLM 的一小部分下，也能與微調 LLM 緊密競爭。

##### **On the Foundations of Conflict-Driven Solving for Hybrid MKNF Knowledge Bases**
2408.09626v1 by Riley Kinahan, Spencer Killen, Kevin Wan, Jia-Huai You

Hybrid MKNF Knowledge Bases (HMKNF-KBs) constitute a formalism for tightly
integrated reasoning over closed-world rules and open-world ontologies. This
approach allows for accurate modeling of real-world systems, which often rely
on both categorical and normative reasoning. Conflict-driven solving is the
leading approach for computationally hard problems, such as satisfiability
(SAT) and answer set programming (ASP), in which MKNF is rooted. This paper
investigates the theoretical underpinnings required for a conflict-driven
solver of HMKNF-KBs. The approach defines a set of completion and loop
formulas, whose satisfaction characterizes MKNF models. This forms the basis
for a set of nogoods, which in turn can be used as the backbone for a
conflict-driven solver.

摘要：混合 MKNF 知識庫 (HMKNF-KBs) 構成了一種形式主義，用於對封閉世界規則和開放世界本体進行緊密整合的推理。這種方法允許對現實世界系統進行準確建模，這些系統通常依賴於範疇推理和規範推理。衝突驅動求解是計算困難問題的主要方法，例如可滿足性 (SAT) 和答案集程式設計 (ASP)，MKNF 根植於其中。本文探討了 HMKNF-KB 的衝突驅動求解器所需的理論基礎。該方法定義了一組完備性和迴圈公式，其滿足度表徵了 MKNF 模型。這構成了禁止集的基礎，而禁止集反過來可以用作衝突驅動求解器的骨幹。

##### **Refining Packing and Shuffling Strategies for Enhanced Performance in Generative Language Models**
2408.09621v1 by Yanbing Chen, Ruilin Wang, Zihao Yang, Lavender Yao Jiang, Eric Karl Oermann

Packing and shuffling tokens is a common practice in training auto-regressive
language models (LMs) to prevent overfitting and improve efficiency. Typically
documents are concatenated to chunks of maximum sequence length (MSL) and then
shuffled. However setting the atom size, the length for each data chunk
accompanied by random shuffling, to MSL may lead to contextual incoherence due
to tokens from different documents being packed into the same chunk. An
alternative approach is to utilize padding, another common data packing
strategy, to avoid contextual incoherence by only including one document in
each shuffled chunk. To optimize both packing strategies (concatenation vs
padding), we investigated the optimal atom size for shuffling and compared
their performance and efficiency. We found that matching atom size to MSL
optimizes performance for both packing methods (concatenation and padding), and
padding yields lower final perplexity (higher performance) than concatenation
at the cost of more training steps and lower compute efficiency. This trade-off
informs the choice of packing methods in training language models.

摘要：打包和洗牌标记是训练自回归语言模型 (LM) 以防止过度拟合并提高效率的常见做法。通常，文件会连接成最大序列长度 (MSL) 的块，然后洗牌。然而，将原子大小（每个数据块的长度）伴随着随机洗牌设置为 MSL 可能会导致上下文不连贯，因为来自不同文件的标记被打包到同一个块中。另一种方法是利用填充（另一种常见的数据打包策略）来避免上下文不连贯，方法是在每个洗牌块中只包含一个文件。为了优化这两种打包策略（连接与填充），我们研究了洗牌的最佳原子大小，并比较了它们的性能和效率。我们发现，将原子大小与 MSL 匹配可以优化两种打包方法（连接和填充）的性能，并且填充比连接产生更低的最终困惑度（更高的性能），但代价是更多的训练步骤和更低的计算效率。这种权衡为训练语言模型中的打包方法选择提供了信息。

##### **Does Thought Require Sensory Grounding? From Pure Thinkers to Large Language Models**
2408.09605v1 by David J. Chalmers

Does the capacity to think require the capacity to sense? A lively debate on
this topic runs throughout the history of philosophy and now animates
discussions of artificial intelligence. I argue that in principle, there can be
pure thinkers: thinkers that lack the capacity to sense altogether. I also
argue for significant limitations in just what sort of thought is possible in
the absence of the capacity to sense. Regarding AI, I do not argue directly
that large language models can think or understand, but I rebut one important
argument (the argument from sensory grounding) that they cannot. I also use
recent results regarding language models to address the question of whether or
how sensory grounding enhances cognitive capacities.

摘要：思考的能力是否需要感知的能力？關於此主題的熱烈辯論貫穿哲學史，現在也激發了對人工智能的討論。我認為，原則上，可以有純粹的思想家：完全缺乏感知能力的思想家。我也主張，在缺乏感知能力的情況下，可能的想法種類僅有顯著的限制。關於人工智能，我並未直接主張大型語言模型可以思考或理解，但我反駁了一個重要的論點（感官基礎論點），認為它們不能。我也利用最近關於語言模型的結果來探討感官基礎是否或如何增強認知能力的問題。

##### **Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning**
2408.09600v1 by Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Josh Kimball, Ling Liu

Safety aligned Large Language Models (LLMs) are vulnerable to harmful
fine-tuning attacks \cite{qi2023fine}-- a few harmful data mixed in the
fine-tuning dataset can break the LLMs's safety alignment. Existing mitigation
strategies include alignment stage solutions \cite{huang2024vaccine,
rosati2024representation} and fine-tuning stage solutions
\cite{huang2024lazy,mukhoti2023fine}. However, our evaluation shows that both
categories of defenses fail \textit{when some specific training
hyper-parameters are chosen} -- a large learning rate or a large number of
training epochs in the fine-tuning stage can easily invalidate the defense,
which however, is necessary to guarantee finetune performance. To this end, we
propose Antidote, a post-fine-tuning stage solution, which remains
\textbf{\textit{agnostic to the training hyper-parameters in the fine-tuning
stage}}. Antidote relies on the philosophy that by removing the harmful
parameters, the harmful model can be recovered from the harmful behaviors,
regardless of how those harmful parameters are formed in the fine-tuning stage.
With this philosophy, we introduce a one-shot pruning stage after harmful
fine-tuning to remove the harmful weights that are responsible for the
generation of harmful content. Despite its embarrassing simplicity, empirical
results show that Antidote can reduce harmful score while maintaining accuracy
on downstream tasks.

摘要：<paragraph>經過安全調整的大型語言模型 (LLM) 容易受到有害的微調攻擊 \cite{qi2023fine}——微調資料集中混入少數有害資料便會破壞 LLM 的安全調整。現有的緩解策略包括調整階段解決方案 \cite{huang2024vaccine, rosati2024representation} 和微調階段解決方案 \cite{huang2024lazy, mukhoti2023fine}。然而，我們的評估顯示，兩類防禦措施都會失敗，\textit{在選擇某些特定訓練超參數時}——微調階段中較大的學習率或大量的訓練輪次很容易使防禦措施失效，然而這對於保證微調效能是必要的。為此，我們提出 Antidote，一種微調後階段解決方案，它仍然\textbf{\textit{與微調階段的訓練超參數無關}}。Antidote 依靠一種理念，即透過移除有害參數，可以從有害行為中恢復有害模型，而不管這些有害參數如何在微調階段形成。基於此理念，我們在有害微調後引入一個一次性剪枝階段，以移除對有害內容產生負面影響的有害權重。儘管其令人尷尬的簡潔性，但實證結果顯示，Antidote 可以降低有害分數，同時維持下游任務的準確性。</paragraph>

##### **Moonshine: Distilling Game Content Generators into Steerable Generative Models**
2408.09594v1 by Yuhe Nie, Michael Middleton, Tim Merino, Nidhushan Kanagaraja, Ashutosh Kumar, Zhan Zhuang, Julian Togelius

Procedural Content Generation via Machine Learning (PCGML) has enhanced game
content creation, yet challenges in controllability and limited training data
persist. This study addresses these issues by distilling a constructive PCG
algorithm into a controllable PCGML model. We first generate a large amount of
content with a constructive algorithm and label it using a Large Language Model
(LLM). We use these synthetic labels to condition two PCGML models for
content-specific generation, a diffusion model and the five-dollar model. This
neural network distillation process ensures that the generation aligns with the
original algorithm while introducing controllability through plain text. We
define this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering
an alternative to prevalent text-to-image multi-modal tasks. We compare our
distilled models with the baseline constructive algorithm. Our analysis of the
variety, accuracy, and quality of our generation demonstrates the efficacy of
distilling constructive methods into controllable text-conditioned PCGML
models.

摘要：透過機器學習的程序內容生成（PCGML）提升了遊戲內容的創作，但控制能力和有限的訓練資料仍然是挑戰。本研究透過將建構式 PCG 演算法提煉成可控的 PCGML 模型來解決這些問題。我們首先使用建構式演算法產生大量的內容，並使用大型語言模型（LLM）為其加上標籤。我們使用這些合成標籤來對應兩個 PCGML 模型進行內容特定生成，一個是擴散模型，另一個是五美元模型。這個神經網路提煉流程確保生成內容與原始演算法一致，同時透過純文字引入可控性。我們將這種文字條件式 PCGML 定義為文字轉遊戲地圖（T2M）任務，提供一個替代於流行文字轉圖片多模態任務的選擇。我們將提煉出的模型與基準建構式演算法進行比較。我們對生成內容的多樣性、準確性和品質的分析證明了將建構式方法提煉成可控文字條件式 PCGML 模型的有效性。

##### **PhysBERT: A Text Embedding Model for Physics Scientific Literature**
2408.09574v1 by Thorsten Hellert, João Montenegro, Andrea Pollastro

The specialized language and complex concepts in physics pose significant
challenges for information extraction through Natural Language Processing
(NLP). Central to effective NLP applications is the text embedding model, which
converts text into dense vector representations for efficient information
retrieval and semantic analysis. In this work, we introduce PhysBERT, the first
physics-specific text embedding model. Pre-trained on a curated corpus of 1.2
million arXiv physics papers and fine-tuned with supervised data, PhysBERT
outperforms leading general-purpose models on physics-specific tasks including
the effectiveness in fine-tuning for specific physics subdomains.

摘要：物理學中專業的語言和複雜的概念對透過自然語言處理 (NLP) 進行資訊萃取來說，構成重大的挑戰。文字嵌入模型對於有效的 NLP 應用程式至關重要，它能將文字轉換成稠密的向量表示，以進行有效率的資訊檢索和語意分析。在這項研究中，我們介紹 PhysBERT，這是第一個專門用於物理學的文字嵌入模型。PhysBERT 在 120 萬篇經過整理的 arXiv 物理論文語料庫上進行預先訓練，並使用監督式資料進行微調，在物理學特定任務上優於領先的通用模型，包括針對特定物理子領域進行微調的有效性。

##### **Say My Name: a Model's Bias Discovery Framework**
2408.09570v1 by Massimiliano Ciranni, Luca Molinaro, Carlo Alberto Barbano, Attilio Fiandrotti, Vittorio Murino, Vito Paolo Pastore, Enzo Tartaglione

In the last few years, due to the broad applicability of deep learning to
downstream tasks and end-to-end training capabilities, increasingly more
concerns about potential biases to specific, non-representative patterns have
been raised. Many works focusing on unsupervised debiasing usually leverage the
tendency of deep models to learn ``easier'' samples, for example by clustering
the latent space to obtain bias pseudo-labels. However, the interpretation of
such pseudo-labels is not trivial, especially for a non-expert end user, as it
does not provide semantic information about the bias features. To address this
issue, we introduce ``Say My Name'' (SaMyNa), the first tool to identify biases
within deep models semantically. Unlike existing methods, our approach focuses
on biases learned by the model. Our text-based pipeline enhances explainability
and supports debiasing efforts: applicable during either training or post-hoc
validation, our method can disentangle task-related information and proposes
itself as a tool to analyze biases. Evaluation on traditional benchmarks
demonstrates its effectiveness in detecting biases and even disclaiming them,
showcasing its broad applicability for model diagnosis.

摘要：在過去幾年中，由於深度學習廣泛適用於下游任務和端到端訓練功能，人們越來越關注潛在偏見對特定非代表性模式的影響。許多專注於無監督去偏的工作通常利用深度模型學習「較容易」樣本的趨勢，例如透過對潛在空間進行分群以取得偏偽標籤。然而，此類偽標籤的解釋並非易事，特別是對於非專家最終使用者而言，因為它沒有提供關於偏見特徵的語意資訊。為了解決這個問題，我們引入了「說出我的名字」(SaMyNa)，這是第一個以語意方式識別深度模型中偏見的工具。與現有方法不同，我們的做法專注於模型學習到的偏見。我們的基於文字的管道增強了解性並支援去偏工作：我們的做法適用於訓練期間或事後驗證，可以解開與任務相關的資訊，並建議將其用作分析偏見的工具。對傳統基準的評估證明了其在偵測偏見甚至否認偏見方面的有效性，展示了其在模型診斷方面的廣泛適用性。

