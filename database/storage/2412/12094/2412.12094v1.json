{"2412.12094": {"publish_time": "2024-12-16", "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator", "paper_summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u5404\u7a2e\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u4efb\u52d9\u4e2d\u8868\u73fe\u51fa\u8272\u7684\u6548\u80fd\u3002\u7136\u800c\uff0c\u5b83\u5011\u9f90\u5927\u7684\u898f\u6a21\u5e36\u4f86\u76f8\u7576\u5927\u7684\u6311\u6230\uff0c\u7279\u5225\u662f\u5728\u904b\u7b97\u9700\u6c42\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\uff0c\u56e0\u70ba\u5b83\u5011\u7684\u8907\u96dc\u5ea6\u70ba\u4e8c\u6b21\u65b9\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u4e86\u4e00\u500b\u95dc\u9375\u6a21\u5f0f\uff1a\u67d0\u4e9b\u770b\u4f3c\u6c92\u6709\u610f\u7fa9\u7684\u7279\u6b8a\u7b26\u865f\uff08\u4f8b\u5982\u5206\u9694\u7b26\u865f\uff09\u8207\u8a9e\u7fa9\u6709\u610f\u7fa9\u7684\u7b26\u865f\u76f8\u6bd4\uff0c\u5c0d\u6ce8\u610f\u529b\u5206\u6578\u7684\u8ca2\u737b\u4e0d\u6210\u6bd4\u4f8b\u3002\u6b64\u89c0\u5bdf\u7d50\u679c\u8868\u660e\uff0c\u9019\u4e9b\u5206\u9694\u7b26\u865f\u4e4b\u9593\u7684\u5340\u6bb5\u8cc7\u8a0a\u53ef\u4ee5\u6709\u6548\u5730\u58d3\u7e2e\u5230\u5206\u9694\u7b26\u865f\u672c\u8eab\uff0c\u800c\u4e0d\u6703\u9020\u6210\u986f\u8457\u7684\u8cc7\u8a0a\u907a\u5931\u3002\u6839\u64da\u6b64\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86 SepLLM\uff0c\u9019\u662f\u4e00\u500b\u5373\u63d2\u5373\u7528\u7684\u67b6\u69cb\uff0c\u900f\u904e\u58d3\u7e2e\u9019\u4e9b\u5340\u6bb5\u4e26\u6d88\u9664\u91cd\u8907\u7684\u7b26\u865f\u4f86\u52a0\u901f\u63a8\u7406\u3002\u6b64\u5916\uff0c\u6211\u5011\u5be6\u4f5c\u4e86\u7528\u65bc\u8a13\u7df4\u52a0\u901f\u7684\u6709\u6548\u6838\u5fc3\u3002\u8de8\u8d8a\u514d\u8a13\u7df4\u3001\u5f9e\u982d\u8a13\u7df4\u548c\u8a13\u7df4\u5f8c\u8a2d\u5b9a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\u4e86 SepLLM \u7684\u6548\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4f7f\u7528 Llama-3-8B \u4e3b\u5e79\uff0cSepLLM \u5728 GSM8K-CoT \u57fa\u6e96\u6e2c\u8a66\u4e2d\u5c07 KV \u5feb\u53d6\u6e1b\u5c11\u4e86 50% \u4ee5\u4e0a\uff0c\u540c\u6642\u7dad\u6301\u4e86\u76f8\u7576\u7684\u6548\u80fd\u3002\u6b64\u5916\uff0c\u5728\u4e32\u6d41\u8a2d\u5b9a\u4e2d\uff0cSepLLM \u6709\u6548\u5730\u8655\u7406\u591a\u9054 400 \u842c\u500b\u7b26\u865f\u6216\u66f4\u591a\u7b26\u865f\u7684\u5e8f\u5217\uff0c\u540c\u6642\u7dad\u6301\u4e00\u81f4\u7684\u8a9e\u8a00\u5efa\u6a21\u80fd\u529b\u3002", "author": "Guoxuan Chen et.al.", "authors": "Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang", "id": "2412.12094v1", "paper_url": "http://arxiv.org/abs/2412.12094v1", "repo": "null"}}