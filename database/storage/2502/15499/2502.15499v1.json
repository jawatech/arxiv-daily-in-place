{"2502.15499": {"publish_time": "2025-02-21", "title": "Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models", "paper_summary": "Training stability is a persistent challenge in the pre-training of large\nlanguage models (LLMs), particularly for architectures such as Post-Norm\nTransformers, which are prone to gradient explosion and dissipation. In this\npaper, we propose Scale-Distribution Decoupling (SDD), a novel approach that\nstabilizes training by explicitly decoupling the scale and distribution of the\nweight matrix in fully-connected layers. SDD applies a normalization mechanism\nto regulate activations and a learnable scaling vector to maintain\nwell-conditioned gradients, effectively preventing $\\textbf{gradient explosion\nand dissipation}$. This separation improves optimization efficiency,\nparticularly in deep networks, by ensuring stable gradient propagation.\nExperimental results demonstrate that our method stabilizes training across\nvarious LLM architectures and outperforms existing techniques in different\nnormalization configurations. Furthermore, the proposed method is lightweight\nand compatible with existing frameworks, making it a practical solution for\nstabilizing LLM training. Code is available at https://github.com/kaihemo/SDD.", "paper_summary_zh": "\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9810\u8a13\u7df4\u4e2d\uff0c\u8a13\u7df4\u7a69\u5b9a\u6027\u662f\u4e00\u500b\u6301\u7e8c\u7684\u6311\u6230\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u5bb9\u6613\u767c\u751f\u68af\u5ea6\u7206\u70b8\u548c\u8017\u6563\u7684\u67b6\u69cb\uff0c\u4f8b\u5982 Post-Norm Transformers\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u898f\u6a21\u5206\u4f48\u89e3\u8026 (SDD)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u900f\u904e\u660e\u78ba\u89e3\u8026\u5b8c\u5168\u9023\u63a5\u5c64\u4e2d\u6b0a\u91cd\u77e9\u9663\u7684\u898f\u6a21\u548c\u5206\u4f48\uff0c\u4f86\u7a69\u5b9a\u8a13\u7df4\u3002SDD \u61c9\u7528\u6b63\u898f\u5316\u6a5f\u5236\u4f86\u8abf\u7bc0\u6fc0\u6d3b\uff0c\u4e26\u4f7f\u7528\u53ef\u5b78\u7fd2\u7684\u7e2e\u653e\u5411\u91cf\u4f86\u7dad\u6301\u826f\u597d\u7684\u68af\u5ea6\uff0c\u6709\u6548\u9632\u6b62$\\textbf{\u68af\u5ea6\u7206\u70b8\u548c\u8017\u6563}$\u3002\u9019\u7a2e\u5206\u96e2\u900f\u904e\u78ba\u4fdd\u7a69\u5b9a\u7684\u68af\u5ea6\u50b3\u64ad\uff0c\u4f86\u6539\u5584\u6700\u4f73\u5316\u6548\u7387\uff0c\u7279\u5225\u662f\u5728\u6df1\u5ea6\u7db2\u8def\u4e2d\u3002\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u6211\u5011\u7684\u6a21\u578b\u5728\u5404\u7a2e LLM \u67b6\u69cb\u4e2d\u7a69\u5b9a\u8a13\u7df4\uff0c\u4e26\u4e14\u5728\u4e0d\u540c\u7684\u6b63\u898f\u5316\u914d\u7f6e\u4e2d\u512a\u65bc\u73fe\u6709\u6280\u8853\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8f15\u91cf\u4e14\u76f8\u5bb9\u65bc\u73fe\u6709\u67b6\u69cb\uff0c\u4f7f\u5176\u6210\u70ba\u7a69\u5b9a LLM \u8a13\u7df4\u7684\u5be6\u7528\u89e3\u6c7a\u65b9\u6848\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/kaihemo/SDD \u53d6\u5f97\u3002", "author": "Ya Wang et.al.", "authors": "Ya Wang, Zhijian Zhuo, Yutao Zeng, Xun Zhou, Jian Yang, Xiaoqing Li", "id": "2502.15499v1", "paper_url": "http://arxiv.org/abs/2502.15499v1", "repo": "null"}}