{"2407.01976": {"publish_time": "2024-07-02", "title": "A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding", "paper_summary": "Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. In particular,\nLayTextLLM projects each bounding box to a single embedding and interleaves it\nwith text, efficiently avoiding long sequence issues while leveraging\nautoregressive traits of LLMs. LayTextLLM not only streamlines the interaction\nof layout and textual data but also shows enhanced performance in Key\nInformation Extraction (KIE) and Visual Question Answering (VQA). Comprehensive\nbenchmark evaluations reveal significant improvements, with a 27.0% increase on\nKIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document\nunderstanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based\nLLMs on KIE tasks.", "paper_summary_zh": "\u6700\u8fd1\uff0c\u8a31\u591a\u7814\u7a76\u5df2\u7d93\u8b49\u660e\uff0c\u5c07\u50c5\u900f\u904e OCR \u884d\u751f\u7684\u6587\u5b57\u548c\u7a7a\u9593\u4f48\u5c40\u8207\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u6574\u5408\uff0c\u5c0d\u65bc\u6587\u4ef6\u7406\u89e3\u4efb\u52d9\u53ef\u80fd\u975e\u5e38\u6709\u6548\u3002\u7136\u800c\uff0c\u73fe\u6709\u6574\u5408\u7a7a\u9593\u4f48\u5c40\u8207\u6587\u5b57\u7684\u65b9\u6cd5\u6709\u5176\u9650\u5236\uff0c\u4f8b\u5982\u7522\u751f\u904e\u9577\u7684\u6587\u5b57\u5e8f\u5217\uff0c\u6216\u662f\u7121\u6cd5\u5145\u5206\u5229\u7528 LLM \u7684\u81ea\u8ff4\u6b78\u7279\u8cea\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e2d\u5f15\u5165\u4e86\u4ea4\u932f\u4f48\u5c40\u548c\u6587\u5b57\uff08LayTextLLM\uff09\uff0c\u7528\u65bc\u6587\u4ef6\u7406\u89e3\u3002\u7279\u5225\u662f\uff0cLayTextLLM \u5c07\u6bcf\u500b\u908a\u754c\u6846\u6295\u5f71\u5230\u55ae\u4e00\u5d4c\u5165\u4e2d\uff0c\u4e26\u5c07\u5176\u8207\u6587\u5b57\u4ea4\u932f\uff0c\u6709\u6548\u907f\u514d\u9577\u5e8f\u5217\u554f\u984c\uff0c\u540c\u6642\u5229\u7528 LLM \u7684\u81ea\u8ff4\u6b78\u7279\u8cea\u3002LayTextLLM \u4e0d\u50c5\u7c21\u5316\u4e86\u4f48\u5c40\u548c\u6587\u5b57\u8cc7\u6599\u7684\u4e92\u52d5\uff0c\u9084\u5c55\u73fe\u51fa\u5728\u95dc\u9375\u8cc7\u8a0a\u8403\u53d6 (KIE) \u548c\u8996\u89ba\u554f\u7b54 (VQA) \u4e2d\u7684\u6548\u80fd\u63d0\u5347\u3002\u5168\u9762\u7684\u57fa\u6e96\u8a55\u4f30\u986f\u793a\u51fa\u986f\u8457\u7684\u9032\u6b65\uff0c\u5728 KIE \u4efb\u52d9\u4e0a\u6bd4\u73fe\u6709\u6700\u5148\u9032\u7684\u6587\u4ef6\u7406\u89e3 MLLM \u63d0\u5347 27.0%\uff0c\u5728 VQA \u4efb\u52d9\u4e0a\u63d0\u5347 24.1%\uff0c\u4e26\u4e14\u5728 KIE \u4efb\u52d9\u4e0a\u6bd4\u5176\u4ed6 SOTA \u57fa\u65bc OCR \u7684 LLM \u63d0\u5347 15.5%\u3002", "author": "Jinghui Lu et.al.", "authors": "Jinghui Lu, Haiyang Yu, Yanjie Wang, Yongjie Ye, Jingqun Tang, Ziwei Yang, Binghong Wu, Qi Liu, Hao Feng, Han Wang, Hao Liu, Can Huang", "id": "2407.01976v1", "paper_url": "http://arxiv.org/abs/2407.01976v1", "repo": "null"}}