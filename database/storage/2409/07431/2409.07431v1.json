{"2409.07431": {"publish_time": "2024-09-11", "title": "Synthetic continued pretraining", "paper_summary": "Pretraining on large-scale, unstructured internet text has enabled language\nmodels to acquire a significant amount of world knowledge. However, this\nknowledge acquisition is data-inefficient -- to learn a given fact, models must\nbe trained on hundreds to thousands of diverse representations of it. This\nposes a challenge when adapting a pretrained model to a small corpus of\ndomain-specific documents, where each fact may appear rarely or only once. We\npropose to bridge this gap with synthetic continued pretraining: using the\nsmall domain-specific corpus to synthesize a large corpus more amenable to\nlearning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation\nalgorithm that extracts salient entities from the source documents and then\ngenerates diverse text by drawing connections between the sampled entities.\nSynthetic continued pretraining using EntiGraph enables a language model to\nanswer questions and follow generic instructions related to the source\ndocuments without access to them. If instead, the source documents are\navailable at inference time, we show that the knowledge acquired through our\napproach compounds with retrieval-augmented generation. To better understand\nthese results, we build a simple mathematical model of EntiGraph, and show how\nsynthetic data augmentation can \"rearrange\" knowledge to enable more\ndata-efficient learning.", "paper_summary_zh": "\u5728\u89c4\u6a21\u5e9e\u5927\u3001\u7ed3\u6784\u677e\u6563\u7684\u4e92\u8054\u7f51\u6587\u672c\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u83b7\u53d6\u5927\u91cf\u7684\u4e16\u754c\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u77e5\u8bc6\u83b7\u53d6\u6548\u7387\u4f4e\u4e0b\u2014\u2014\u4e3a\u4e86\u5b66\u4e60\u4e00\u4e2a\u7ed9\u5b9a\u7684\u4e8b\u5b9e\uff0c\u6a21\u578b\u5fc5\u987b\u63a5\u53d7\u6570\u767e\u5230\u6570\u5343\u4e2a\u4e0d\u540c\u8868\u793a\u5f62\u5f0f\u7684\u8bad\u7ec3\u3002\u5f53\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u5230\u4e00\u4e2a\u5305\u542b\u7279\u5b9a\u9886\u57df\u6587\u6863\u7684\u5c0f\u8bed\u6599\u5e93\u65f6\uff0c\u8fd9\u4f1a\u5e26\u6765\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u4e8b\u5b9e\u53ef\u80fd\u5f88\u5c11\u51fa\u73b0\u6216\u53ea\u51fa\u73b0\u4e00\u6b21\u3002\u6211\u4eec\u5efa\u8bae\u7528\u5408\u6210\u6301\u7eed\u9884\u8bad\u7ec3\u6765\u5f25\u5408\u7406\u8bba\u5dee\u8ddd\uff1a\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u7684\u5c0f\u8bed\u6599\u5e93\u6765\u5408\u6210\u4e00\u4e2a\u66f4\u5bb9\u6613\u5b66\u4e60\u7684\u5927\u8bed\u6599\u5e93\uff0c\u7136\u540e\u5bf9\u5408\u6210\u7684\u8bed\u6599\u5e93\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u3002\u6211\u4eec\u7528 EntiGraph \u5b9e\u4f8b\u5316\u4e86\u6b64\u63d0\u8bae\uff0c\u8fd9\u662f\u4e00\u79cd\u5408\u6210\u6570\u636e\u6269\u5145\u7b97\u6cd5\uff0c\u5b83\u4ece\u6e90\u6587\u6863\u4e2d\u63d0\u53d6\u663e\u8457\u5b9e\u4f53\uff0c\u7136\u540e\u901a\u8fc7\u7ed8\u5236\u62bd\u6837\u5b9e\u4f53\u4e4b\u95f4\u7684\u8054\u7cfb\u6765\u751f\u6210\u4e0d\u540c\u7684\u6587\u672c\u3002\u4f7f\u7528 EntiGraph \u8fdb\u884c\u5408\u6210\u6301\u7eed\u9884\u8bad\u7ec3\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u56de\u7b54\u95ee\u9898\u5e76\u9075\u5faa\u4e0e\u6e90\u6587\u6863\u76f8\u5173\u7684\u901a\u7528\u8bf4\u660e\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u5b83\u4eec\u3002\u5982\u679c\u76f8\u53cd\uff0c\u6e90\u6587\u6863\u5728\u63a8\u7406\u65f6\u53ef\u7528\uff0c\u6211\u4eec\u8868\u660e\u901a\u8fc7\u6211\u4eec\u7684\u65b9\u6cd5\u83b7\u5f97\u7684\u77e5\u8bc6\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u76f8\u7ed3\u5408\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u7ed3\u679c\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a EntiGraph \u7684\u7b80\u5355\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u6269\u5145\u5982\u4f55\u201c\u91cd\u65b0\u6392\u5217\u201d\u77e5\u8bc6\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7387\u7684\u6570\u636e\u5b66\u4e60\u3002", "author": "Zitong Yang et.al.", "authors": "Zitong Yang, Neil Band, Shuangping Li, Emmanuel Cand\u00e8s, Tatsunori Hashimoto", "id": "2409.07431v1", "paper_url": "http://arxiv.org/abs/2409.07431v1", "repo": "https://github.com/zitongyang/synthetic_continued_pretraining"}}