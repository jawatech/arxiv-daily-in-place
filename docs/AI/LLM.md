
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-12-04**|**Navigation World Models**|Amir Bar et.al.|[2412.03572v1](http://arxiv.org/abs/2412.03572v1)|null|
|**2024-12-04**|**The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control**|Ruili Feng et.al.|[2412.03568v1](http://arxiv.org/abs/2412.03568v1)|null|
|**2024-12-04**|**From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents**|Xinyi Mou et.al.|[2412.03563v1](http://arxiv.org/abs/2412.03563v1)|null|
|**2024-12-04**|**FLAIR: VLM with Fine-grained Language-informed Image Representations**|Rui Xiao et.al.|[2412.03561v1](http://arxiv.org/abs/2412.03561v1)|[link](https://github.com/explainableml/flair)|
|**2024-12-04**|**Best-of-N Jailbreaking**|John Hughes et.al.|[2412.03556v1](http://arxiv.org/abs/2412.03556v1)|null|
|**2024-12-04**|**Perception Tokens Enhance Visual Reasoning in Multimodal Language Models**|Mahtab Bigverdi et.al.|[2412.03548v1](http://arxiv.org/abs/2412.03548v1)|null|
|**2024-12-04**|**NODE-AdvGAN: Improving the transferability and perceptual similarity of adversarial examples by dynamic-system-driven adversarial generative model**|Xinheng Xie et.al.|[2412.03539v1](http://arxiv.org/abs/2412.03539v1)|null|
|**2024-12-04**|**Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models**|Natalie Mackraz et.al.|[2412.03537v1](http://arxiv.org/abs/2412.03537v1)|null|
|**2024-12-04**|**A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences**|Gabriel Lino Garcia et.al.|[2412.03531v1](http://arxiv.org/abs/2412.03531v1)|null|
|**2024-12-04**|**FANAL -- Financial Activity News Alerting Language Modeling Framework**|Urjitkumar Patel et.al.|[2412.03527v1](http://arxiv.org/abs/2412.03527v1)|null|
|**2024-12-04**|**Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos**|Hanxue Liang et.al.|[2412.03526v1](http://arxiv.org/abs/2412.03526v1)|null|
|**2024-12-04**|**You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?**|Dominic Lohr et.al.|[2412.03516v1](http://arxiv.org/abs/2412.03516v1)|null|
|**2024-12-04**|**KKLIP: Knowledge Distillation Exploiting K-means Clustering for Language-Image Pre-Training**|Kuei-Chun Kao et.al.|[2412.03513v1](http://arxiv.org/abs/2412.03513v1)|null|
|**2024-12-04**|**A Bidirectional Siamese Recurrent Neural Network for Accurate Gait Recognition Using Body Landmarks**|Proma Hossain Progga et.al.|[2412.03498v1](http://arxiv.org/abs/2412.03498v1)|null|
|**2024-12-04**|**Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective**|Neta Shaul et.al.|[2412.03487v1](http://arxiv.org/abs/2412.03487v1)|null|
|**2024-12-04**|**Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning**|Neale Ratzlaff et.al.|[2412.03467v1](http://arxiv.org/abs/2412.03467v1)|null|
|**2024-12-04**|**From Words to Workflows: Automating Business Processes**|Laura Minkova et.al.|[2412.03446v1](http://arxiv.org/abs/2412.03446v1)|null|
|**2024-12-04**|**PBP: Post-training Backdoor Purification for Malware Classifiers**|Dung Thuy Nguyen et.al.|[2412.03441v1](http://arxiv.org/abs/2412.03441v1)|[link](https://github.com/judydnguyen/pbp-backdoor-purification-official)|
|**2024-12-04**|**BIMCaP: BIM-based AI-supported LiDAR-Camera Pose Refinement**|Miguel Arturo Vega Torres et.al.|[2412.03434v1](http://arxiv.org/abs/2412.03434v1)|[link](https://github.com/migvega/bimcap)|
|**2024-12-04**|**Automated Test-Case Generation for REST APIs Using Model Inference Search Heuristic**|Clinton Cao et.al.|[2412.03420v1](http://arxiv.org/abs/2412.03420v1)|null|
|**2024-12-04**|**Benchmarking Pretrained Attention-based Models for Real-Time Recognition in Robot-Assisted Esophagectomy**|Ronald L. P. D. de Jong et.al.|[2412.03401v1](http://arxiv.org/abs/2412.03401v1)|null|
|**2024-12-04**|**RedStone: Curating General, Code, Math, and QA Data for Large Language Models**|Yaoyao Chang et.al.|[2412.03398v1](http://arxiv.org/abs/2412.03398v1)|null|
|**2024-12-04**|**Enhancing Supply Chain Visibility with Generative AI: An Exploratory Case Study on Relationship Prediction in Knowledge Graphs**|Ge Zheng et.al.|[2412.03390v1](http://arxiv.org/abs/2412.03390v1)|null|
|**2024-12-04**|**DiffStyleTTS: Diffusion-based Hierarchical Prosody Modeling for Text-to-Speech with Diverse and Controllable Styles**|Jiaxuan Liu et.al.|[2412.03388v1](http://arxiv.org/abs/2412.03388v1)|null|
|**2024-12-04**|**WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis**|Chengwei Hu et.al.|[2412.03359v1](http://arxiv.org/abs/2412.03359v1)|null|
|**2024-12-04**|**Intuitive Axial Augmentation Using Polar-Sine-Based Piecewise Distortion for Medical Slice-Wise Segmentation**|Yiqin Zhang et.al.|[2412.03352v1](http://arxiv.org/abs/2412.03352v1)|[link](https://github.com/mgamz/psbpd)|
|**2024-12-04**|**DIVE: Taming DINO for Subject-Driven Video Editing**|Yi Huang et.al.|[2412.03347v1](http://arxiv.org/abs/2412.03347v1)|null|
|**2024-12-04**|**Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning**|Long Mai et.al.|[2412.03343v1](http://arxiv.org/abs/2412.03343v1)|[link](https://github.com/mailong25/peft_diversity)|
|**2024-12-04**|**AI-Driven Day-to-Day Route Choice**|Leizhen Wang et.al.|[2412.03338v1](http://arxiv.org/abs/2412.03338v1)|null|
|**2024-12-04**|**Yankari: A Monolingual Yoruba Dataset**|Maro Akpobi et.al.|[2412.03334v1](http://arxiv.org/abs/2412.03334v1)|null|
|**2024-12-04**|**LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings**|Fred Philippy et.al.|[2412.03331v1](http://arxiv.org/abs/2412.03331v1)|null|
|**2024-12-04**|**Grounded Language Design for Lightweight Diagramming for Formal Methods**|Siddhartha Prasad et.al.|[2412.03310v1](http://arxiv.org/abs/2412.03310v1)|null|
|**2024-12-04**|**Contextual Data Integration for Bike-sharing Demand Prediction with Graph Neural Networks in Degraded Weather Conditions**|Romain Rochas et.al.|[2412.03307v1](http://arxiv.org/abs/2412.03307v1)|null|
|**2024-12-04**|**Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation**|Shivalika Singh et.al.|[2412.03304v1](http://arxiv.org/abs/2412.03304v1)|null|
|**2024-12-04**|**Integrating Generative AI into Art Therapy: A Technical Showcase**|Yannis Valentin Schmutz et.al.|[2412.03287v1](http://arxiv.org/abs/2412.03287v1)|[link](https://github.com/bfh-ami/sds24)|
|**2024-12-04**|**Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models**|Andreas MÃ¼ller et.al.|[2412.03283v1](http://arxiv.org/abs/2412.03283v1)|null|
|**2024-12-04**|**AntLM: Bridging Causal and Masked Language Models**|Xinru Yu et.al.|[2412.03275v1](http://arxiv.org/abs/2412.03275v1)|null|
|**2024-12-04**|**Intent-driven In-context Learning for Few-shot Dialogue State Tracking**|Zihao Yi et.al.|[2412.03270v1](http://arxiv.org/abs/2412.03270v1)|null|
|**2024-12-04**|**Alignment at Pre-training! Towards Native Alignment for Arabic LLMs**|Juhao Liang et.al.|[2412.03253v1](http://arxiv.org/abs/2412.03253v1)|[link](https://github.com/freedomintelligence/acegpt-v2)|
|**2024-12-04**|**AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning**|Yiwu Zhong et.al.|[2412.03248v1](http://arxiv.org/abs/2412.03248v1)|[link](https://github.com/lavi-lab/aim)|
|**2024-12-04**|**Benchmarking terminology building capabilities of ChatGPT on an English-Russian Fashion Corpus**|Anastasiia Bezobrazova et.al.|[2412.03242v1](http://arxiv.org/abs/2412.03242v1)|null|
|**2024-12-04**|**Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?**|Sravanti Addepalli et.al.|[2412.03235v1](http://arxiv.org/abs/2412.03235v1)|null|
|**2024-12-04**|**PERL: Pinyin Enhanced Rephrasing Language Model for Chinese ASR N-best Error Correction**|Junhong Liang et.al.|[2412.03230v1](http://arxiv.org/abs/2412.03230v1)|null|
|**2024-12-04**|**Linq-Embed-Mistral Technical Report**|Chanyeol Choi et.al.|[2412.03223v1](http://arxiv.org/abs/2412.03223v1)|null|
|**2024-12-04**|**ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression**|Guangda Liu et.al.|[2412.03213v1](http://arxiv.org/abs/2412.03213v1)|null|
|**2024-12-04**|**U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs**|Konstantin Chernyshev et.al.|[2412.03205v1](http://arxiv.org/abs/2412.03205v1)|null|
|**2024-12-04**|**Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for Traffic Prediction**|Ivan Kralj et.al.|[2412.03188v1](http://arxiv.org/abs/2412.03188v1)|null|
|**2024-12-04**|**Weighted-Reward Preference Optimization for Implicit Model Fusion**|Ziyi Yang et.al.|[2412.03187v1](http://arxiv.org/abs/2412.03187v1)|[link](https://github.com/SLIT-AI/WRPO)|
|**2024-12-04**|**Optimizing Dense Visual Predictions Through Multi-Task Coherence and Prioritization**|Maxime Fontana et.al.|[2412.03179v1](http://arxiv.org/abs/2412.03179v1)|null|
|**2024-12-04**|**Towards Understanding and Quantifying Uncertainty for Text-to-Image Generation**|Gianni Franchi et.al.|[2412.03178v1](http://arxiv.org/abs/2412.03178v1)|null|
|**2024-12-04**|**Automatic detection of diseases in Spanish clinical notes combining medical language models and ontologies**|Leon-Paul Schaub Torre et.al.|[2412.03176v1](http://arxiv.org/abs/2412.03176v1)|null|
|**2024-12-04**|**Physics-Informed Deep Inverse Operator Networks for Solving PDE Inverse Problems**|Sung Woong Cho et.al.|[2412.03161v1](http://arxiv.org/abs/2412.03161v1)|null|
|**2024-12-04**|**Byte BPE Tokenization as an Inverse string Homomorphism**|Saibo Geng et.al.|[2412.03160v1](http://arxiv.org/abs/2412.03160v1)|null|
|**2024-12-04**|**Testing Neural Network Verifiers: A Soundness Benchmark with Hidden Counterexamples**|Xingjian Zhou et.al.|[2412.03154v1](http://arxiv.org/abs/2412.03154v1)|[link](https://github.com/mvp-harry/soundnessbench)|
|**2024-12-04**|**Large Language Models show both individual and collective creativity comparable to humans**|Luning Sun et.al.|[2412.03151v1](http://arxiv.org/abs/2412.03151v1)|null|
|**2024-12-04**|**Fine-Grained Behavior Simulation with Role-Playing Large Language Model on Social Media**|Kun Li et.al.|[2412.03148v1](http://arxiv.org/abs/2412.03148v1)|[link](https://github.com/linkseed18612254945/finerob)|
|**2024-12-04**|**Robust Multi-bit Text Watermark with LLM-based Paraphrasers**|Xiaojun Xu et.al.|[2412.03123v1](http://arxiv.org/abs/2412.03123v1)|[link](https://github.com/xiaojunxu/multi-bit-text-watermark)|
|**2024-12-04**|**Experience-driven discovery of planning strategies**|Ruiqi He et.al.|[2412.03111v1](http://arxiv.org/abs/2412.03111v1)|null|
|**2024-12-04**|**CredID: Credible Multi-Bit Watermark for Large Language Models Identification**|Haoyu Jiang et.al.|[2412.03107v1](http://arxiv.org/abs/2412.03107v1)|null|
|**2024-12-04**|**ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning**|Zhe Xie et.al.|[2412.03104v1](http://arxiv.org/abs/2412.03104v1)|null|
|**2024-12-04**|**A surprisal oracle for when every layer counts**|Xudong Hong et.al.|[2412.03098v1](http://arxiv.org/abs/2412.03098v1)|[link](https://github.com/asayeed/activebaby)|
|**2024-12-04**|**TOOL-ED: Enhancing Empathetic Response Generation with the Tool Calling Capability of LLM**|Huiying Cao et.al.|[2412.03096v1](http://arxiv.org/abs/2412.03096v1)|null|
|**2024-12-04**|**Revolve: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization**|Peiyan Zhang et.al.|[2412.03092v1](http://arxiv.org/abs/2412.03092v1)|[link](https://github.com/peiyance/revolve)|
|**2024-12-04**|**ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error Correction**|Victor Junqiu Wei et.al.|[2412.03075v1](http://arxiv.org/abs/2412.03075v1)|null|
|**2024-12-04**|**Analytic Study of Text-Free Speech Synthesis for Raw Audio using a Self-Supervised Learning Model**|Joonyong Park et.al.|[2412.03074v1](http://arxiv.org/abs/2412.03074v1)|null|
|**2024-12-04**|**Preference-based opponent shaping in differentiable games**|Xinyu Qiao et.al.|[2412.03072v1](http://arxiv.org/abs/2412.03072v1)|null|
|**2024-12-04**|**UTSD: Unified Time Series Diffusion Model**|Xiangkai Ma et.al.|[2412.03068v1](http://arxiv.org/abs/2412.03068v1)|null|
|**2024-12-04**|**Point-GN: A Non-Parametric Network Using Gaussian Positional Encoding for Point Cloud Classification**|Marzieh Mohammadi et.al.|[2412.03056v1](http://arxiv.org/abs/2412.03056v1)|null|
|**2024-12-04**|**Less is More: A Stealthy and Efficient Adversarial Attack Method for DRL-based Autonomous Driving Policies**|Junchao Fan et.al.|[2412.03051v1](http://arxiv.org/abs/2412.03051v1)|null|
|**2024-12-04**|**MRNet: Multifaceted Resilient Networks for Medical Image-to-Image Translation**|Hyojeong Lee et.al.|[2412.03039v1](http://arxiv.org/abs/2412.03039v1)|null|
|**2024-12-04**|**MILLION: A General Multi-Objective Framework with Controllable Risk for Portfolio Management**|Liwei Deng et.al.|[2412.03038v1](http://arxiv.org/abs/2412.03038v1)|null|
|**2024-12-04**|**Specification Generation for Neural Networks in Systems**|Isha Chaudhary et.al.|[2412.03028v1](http://arxiv.org/abs/2412.03028v1)|null|
|**2024-12-04**|**Human Variability vs. Machine Consistency: A Linguistic Analysis of Texts Generated by Humans and Large Language Models**|Sergio E. Zanotto et.al.|[2412.03025v1](http://arxiv.org/abs/2412.03025v1)|null|
|**2024-12-04**|**PEMF-VVTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm**|Tianyu Chang et.al.|[2412.03021v1](http://arxiv.org/abs/2412.03021v1)|null|
|**2024-12-04**|**Human Multi-View Synthesis from a Single-View Model:Transferred Body and Face Representations**|Yu Feng et.al.|[2412.03011v1](http://arxiv.org/abs/2412.03011v1)|null|
|**2024-12-04**|**Advancing Conversational Psychotherapy: Integrating Privacy, Dual-Memory, and Domain Expertise with Large Language Models**|XiuYu Zhang et.al.|[2412.02987v1](http://arxiv.org/abs/2412.02987v1)|null|
|**2024-12-04**|**Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models**|Alex Havrilla et.al.|[2412.02980v1](http://arxiv.org/abs/2412.02980v1)|null|
|**2024-12-04**|**Theoretical limitations of multi-layer Transformer**|Lijie Chen et.al.|[2412.02975v1](http://arxiv.org/abs/2412.02975v1)|null|
|**2024-12-04**|**3D Interaction Geometric Pre-training for Molecular Relational Learning**|Namkyeong Lee et.al.|[2412.02957v1](http://arxiv.org/abs/2412.02957v1)|null|
|**2024-12-04**|**Curriculum-style Data Augmentation for LLM-based Metaphor Detection**|Kaidi Jia et.al.|[2412.02956v1](http://arxiv.org/abs/2412.02956v1)|null|
|**2024-12-04**|**Who Brings the Frisbee: Probing Hidden Hallucination Factors in Large Vision-Language Model via Causality Analysis**|Po-Hsuan Huang et.al.|[2412.02946v1](http://arxiv.org/abs/2412.02946v1)|null|
|**2024-12-04**|**STDCformer: A Transformer-Based Model with a Spatial-Temporal Causal De-Confounding Strategy for Crowd Flow Prediction**|Silu He et.al.|[2412.02942v1](http://arxiv.org/abs/2412.02942v1)|null|
|**2024-12-04**|**Dynamic Graph Neural Ordinary Differential Equation Network for Multi-modal Emotion Recognition in Conversation**|Yuntao Shou et.al.|[2412.02935v1](http://arxiv.org/abs/2412.02935v1)|null|
|**2024-12-04**|**Panoptic Diffusion Models: co-generation of images and segmentation maps**|Yinghan Long et.al.|[2412.02929v1](http://arxiv.org/abs/2412.02929v1)|null|
|**2024-12-04**|**Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data**|Soroush Omranpour et.al.|[2412.02919v1](http://arxiv.org/abs/2412.02919v1)|null|
|**2024-12-03**|**Single-Cell Omics Arena: A Benchmark Study for Large Language Models on Cell Type Annotation Using Single-Cell Data**|Junhao Liu et.al.|[2412.02915v1](http://arxiv.org/abs/2412.02915v1)|null|
|**2024-12-03**|**Does Few-Shot Learning Help LLM Performance in Code Synthesis?**|Derek Xu et.al.|[2412.02906v1](http://arxiv.org/abs/2412.02906v1)|null|
|**2024-12-03**|**Enhancing Trust in Large Language Models with Uncertainty-Aware Fine-Tuning**|Ranganath Krishnan et.al.|[2412.02904v1](http://arxiv.org/abs/2412.02904v1)|null|
|**2024-12-03**|**MLD-EA: Check and Complete Narrative Coherence by Introducing Emotions and Actions**|Jinming Zhang et.al.|[2412.02897v1](http://arxiv.org/abs/2412.02897v1)|null|
|**2024-12-03**|**Removing Spurious Correlation from Neural Network Interpretations**|Milad Fotouhi et.al.|[2412.02893v1](http://arxiv.org/abs/2412.02893v1)|null|
|**2024-12-03**|**TDD-Bench Verified: Can LLMs Generate Tests for Issues Before They Get Resolved?**|Toufique Ahmed et.al.|[2412.02883v1](http://arxiv.org/abs/2412.02883v1)|null|
|**2024-12-03**|**Modeling and Discovering Direct Causes for Predictive Models**|Yizuo Chen et.al.|[2412.02878v1](http://arxiv.org/abs/2412.02878v1)|null|
|**2024-12-03**|**Constrained Identifiability of Causal Effects**|Yizuo Chen et.al.|[2412.02869v1](http://arxiv.org/abs/2412.02869v1)|null|
|**2024-12-03**|**A Novel Compact LLM Framework for Local, High-Privacy EHR Data Applications**|Yixiang Qu et.al.|[2412.02868v1](http://arxiv.org/abs/2412.02868v1)|null|
|**2024-12-03**|**Unpaired Modality Translation for Pseudo Labeling of Histology Images**|Arthur Boschet et.al.|[2412.02858v1](http://arxiv.org/abs/2412.02858v1)|null|
|**2024-12-03**|**FLAME 3 Dataset: Unleashing the Power of Radiometric Thermal UAV Imagery for Wildfire Management**|Bryce Hopkins et.al.|[2412.02831v1](http://arxiv.org/abs/2412.02831v1)|null|
|**2024-12-03**|**RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models**|Hieu Tran et.al.|[2412.02830v1](http://arxiv.org/abs/2412.02830v1)|null|
|**2024-12-03**|**Minimization of Boolean Complexity in In-Context Concept Learning**|Leroy Z. Wang et.al.|[2412.02823v1](http://arxiv.org/abs/2412.02823v1)|null|
|**2024-12-03**|**CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels**|Lingxiao Wei et.al.|[2412.02819v1](http://arxiv.org/abs/2412.02819v1)|null|
|**2024-12-03**|**Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects**|Abdurrahman Zeybey et.al.|[2412.02803v1](http://arxiv.org/abs/2412.02803v1)|null|

#### Abstracts
##### **Navigation World Models**
2412.03572v1 by Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun

Navigation is a fundamental skill of agents with visual-motor capabilities.
We introduce a Navigation World Model (NWM), a controllable video generation
model that predicts future visual observations based on past observations and
navigation actions. To capture complex environment dynamics, NWM employs a
Conditional Diffusion Transformer (CDiT), trained on a diverse collection of
egocentric videos of both human and robotic agents, and scaled up to 1 billion
parameters. In familiar environments, NWM can plan navigation trajectories by
simulating them and evaluating whether they achieve the desired goal. Unlike
supervised navigation policies with fixed behavior, NWM can dynamically
incorporate constraints during planning. Experiments demonstrate its
effectiveness in planning trajectories from scratch or by ranking trajectories
sampled from an external policy. Furthermore, NWM leverages its learned visual
priors to imagine trajectories in unfamiliar environments from a single input
image, making it a flexible and powerful tool for next-generation navigation
systems.

æè¦ï¼å°èªæ¯å·åè¦è¦ºéåè½åçä»£çäººçä¸é åºæ¬æè½ã
æåå¼å¥å°èªä¸çæ¨¡å (NWM)ï¼éæ¯ä¸åå¯æ§å¶çå½±ççææ¨¡åï¼å®åºæ¼éå»çè§å¯åå°èªåä½ä¾é æ¸¬æªä¾çè¦è¦ºè§å¯ãçºäºææè¤éçç°å¢åæï¼NWM æ¡ç¨æ¢ä»¶æ´æ£è½æå¨ (CDiT)ï¼å®å¨äººé¡åæ©å¨äººä»£çäººçåç¨®ä»¥èªæçºä¸­å¿å½±ççéåä¸é²è¡è¨ç·´ï¼ä¸¦æ´å±å° 10 åååæ¸ãå¨çæçç°å¢ä¸­ï¼NWM å¯ä»¥ééæ¨¡æ¬å°èªè»è·¡ä¸¦è©ä¼°å®åæ¯å¦éææéç®æ¨ï¼ä¾è¦åå°èªè»è·¡ãèå·æåºå®è¡çºçç£ç£å¼å°èªç­ç¥ä¸åï¼NWM å¯ä»¥åæå°å°éå¶ç´å¥è¦åä¸­ãå¯¦é©è­æäºå®å¨å¾é ­éå§è¦åè»è·¡æå¾å¤é¨ç­ç¥ä¸­åæ¨£è»è·¡ä¸¦å°å¶é²è¡æåæ¹é¢çæææ§ãæ­¤å¤ï¼NWM å©ç¨å¶å­¸ç¿çè¦è¦ºåé©ï¼å¾å®ä¸è¼¸å¥å½±åæ³åå¨ä¸çæç°å¢ä¸­çè»è·¡ï¼éä½¿å¶æçºä¸ä¸ä»£å°èªç³»çµ±çéæ´»ä¸å¼·å¤§çå·¥å·ã

##### **The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control**
2412.03568v1 by Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, Hongyang Zhang

We present The Matrix, the first foundational realistic world simulator
capable of generating continuous 720p high-fidelity real-scene video streams
with real-time, responsive control in both first- and third-person
perspectives, enabling immersive exploration of richly dynamic environments.
Trained on limited supervised data from AAA games like Forza Horizon 5 and
Cyberpunk 2077, complemented by large-scale unsupervised footage from
real-world settings like Tokyo streets, The Matrix allows users to traverse
diverse terrains -- deserts, grasslands, water bodies, and urban landscapes --
in continuous, uncut hour-long sequences. Operating at 16 FPS, the system
supports real-time interactivity and demonstrates zero-shot generalization,
translating virtual game environments to real-world contexts where collecting
continuous movement data is often infeasible. For example, The Matrix can
simulate a BMW X3 driving through an office setting--an environment present in
neither gaming data nor real-world sources. This approach showcases the
potential of AAA game data to advance robust world models, bridging the gap
between simulations and real-world applications in scenarios with limited data.

æè¦ï¼æåå±ç¤ºäº The Matrixï¼éæ¯ç¬¬ä¸ååºç¤æ§çé¼çä¸çæ¨¡æ¬å¨
è½å¤ ç¢çé£çºç 720p é«ä¿çå¯¦å ´æ¯å½±çä¸²æµ
å¨ç¬¬ä¸äººç¨±åç¬¬ä¸äººç¨±
è¦è§ä¸­é²è¡å³æãéæçæ§å¶ï¼è®èº«æ­·å¶å¢çæ¢ç´¢è±å¯åæçç°å¢ã
ä½¿ç¨ä¾èª AAA éæ²ï¼ä¾å¦ Forza Horizon 5 å
Cyberpunk 2077ï¼çæéç£ç£æ¸æé²è¡è¨ç·´ï¼ä¸¦è¼ä»¥ä¾èª
æ±äº¬è¡é ­ç­çå¯¦ä¸çå ´æ¯çå¤§è¦æ¨¡ç¡ç£ç£çæ®µï¼The Matrix åè¨±ä½¿ç¨èç©¿è¶
åç¨®å°å½¢ââæ²æ¼ ãèåãæ°´é«ååå¸æ¯è§ââ
å¨é£çºãæªåªè¼¯çä¸å°æåºåä¸­ãç³»çµ±ä»¥ 16 FPS éä½
æ¯æ´å³æäºåï¼ä¸¦å±ç¤ºé¶æ¬¡å­¸ç¿æ³åï¼
å°èæ¬éæ²ç°å¢è½æçºçå¯¦ä¸ççç°å¢ï¼å¨å¶ä¸­æ¶é
é£çºçç§»åæ¸æéå¸¸ä¸å¯è¡ãä¾å¦ï¼The Matrix å¯ä»¥
æ¨¡æ¬ BMW X3 å¨è¾¦å¬å®¤ç°å¢ä¸­è¡é§ââä¸åæ¢ä¸å¨éæ²æ¸æä¸­ä¹ä¸å¨çå¯¦ä¸çä¾æºä¸­çç°å¢ãéç¨®æ¹æ³å±ç¤ºäº
AAA éæ²æ¸æå¨æ¨é²å¼·å¥ä¸çæ¨¡åæ¹é¢çæ½åï¼ç¸®å°äº
æ¨¡æ¬åçå¯¦ä¸çæç¨ä¹éçå·®è·ï¼å¨è³ææéçææ³ä¸ã

##### **From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents**
2412.03563v1 by Xinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jingcong Liang, Xinnong Zhang, Libo Sun, Jiayu Lin, Jie Zhou, Xuanjing Huang, Zhongyu Wei

Traditional sociological research often relies on human participation, which,
though effective, is expensive, challenging to scale, and with ethical
concerns. Recent advancements in large language models (LLMs) highlight their
potential to simulate human behavior, enabling the replication of individual
responses and facilitating studies on many interdisciplinary studies. In this
paper, we conduct a comprehensive survey of this field, illustrating the recent
progress in simulation driven by LLM-empowered agents. We categorize the
simulations into three types: (1) Individual Simulation, which mimics specific
individuals or demographic groups; (2) Scenario Simulation, where multiple
agents collaborate to achieve goals within specific contexts; and (3) Society
Simulation, which models interactions within agent societies to reflect the
complexity and variety of real-world dynamics. These simulations follow a
progression, ranging from detailed individual modeling to large-scale societal
phenomena. We provide a detailed discussion of each simulation type, including
the architecture or key components of the simulation, the classification of
objectives or scenarios and the evaluation method. Afterward, we summarize
commonly used datasets and benchmarks. Finally, we discuss the trends across
these three types of simulation. A repository for the related sources is at
{\url{https://github.com/FudanDISC/SocialAgent}}.

æè¦ï¼å³çµ±çç¤¾æå­¸ç ç©¶éå¸¸ä¾è³´æ¼äººé¡åèï¼åç®¡ææï¼ä½ä»£å¹é«æãé£ä»¥æ´å±ï¼ä¸æéå¾·ä¸ççæ®ãå¤§åèªè¨æ¨¡å (LLM) çææ°é²å±çªé¡¯äºå®åæ¨¡æ¬äººé¡è¡çºçæ½åï¼è½å¤ è¤è£½åå¥åæä¸¦ä¿é²è¨±å¤è·¨é åç ç©¶çç ç©¶ãå¨æ¬æä¸­ï¼æåå°éåé åé²è¡å¨é¢èª¿æ¥ï¼èªªæç± LLM è³¦è½çä»£çé©åçæ¨¡æ¬çææ°é²å±ãæåå°æ¨¡æ¬åé¡çºä¸ç¨®é¡åï¼(1) åé«æ¨¡æ¬ï¼æ¨¡æ¬ç¹å®åäººæäººå£ç¾¤é«ï¼(2) æå¢æ¨¡æ¬ï¼å¶ä¸­å¤åä»£çåä½å¨ç¹å®æå¢ä¸­éæç®æ¨ï¼(3) ç¤¾ææ¨¡æ¬ï¼æ¨¡æ¬ä»£çç¤¾æä¸­çäºåï¼ä»¥åæ ç¾å¯¦ä¸çåæçè¤éæ§åå¤æ¨£æ§ãéäºæ¨¡æ¬éµå¾ªä¸åé²ç¨ï¼å¾è©³ç´°çåé«å»ºæ¨¡å°å¤§åç¤¾æç¾è±¡ãæåå°æ¯ç¨®é¡åçæ¨¡æ¬é²è¡è©³ç´°è¨è«ï¼åæ¬æ¨¡æ¬çæ¶æ§æééµçµæé¨åãç®æ¨ææå¢çåé¡ä»¥åè©ä¼°æ¹æ³ãä¹å¾ï¼æåç¸½çµå¸¸ç¨è³æéååºæºãæå¾ï¼æåè¨è«éä¸ç¨®é¡åæ¨¡æ¬çè¶¨å¢ãç¸éè³æºçå­æ¾åº«ä½æ¼ {\url{https://github.com/FudanDISC/SocialAgent}}ã

##### **FLAIR: VLM with Fine-grained Language-informed Image Representations**
2412.03561v1 by Rui Xiao, Sanghwan Kim, Mariana-Iuliana Georgescu, Zeynep Akata, Stephan Alaniz

CLIP has shown impressive results in aligning images and texts at scale.
However, its ability to capture detailed visual features remains limited
because CLIP matches images and texts at a global level. To address this issue,
we propose FLAIR, Fine-grained Language-informed Image Representations, an
approach that utilizes long and detailed image descriptions to learn localized
image embeddings. By sampling diverse sub-captions that describe fine-grained
details about an image, we train our vision-language model to produce not only
global embeddings but also text-specific image representations. Our model
introduces text-conditioned attention pooling on top of local image tokens to
produce fine-grained image representations that excel at retrieving detailed
image content. We achieve state-of-the-art performance on both, existing
multimodal retrieval benchmarks, as well as, our newly introduced fine-grained
retrieval task which evaluates vision-language models' ability to retrieve
partial image content. Furthermore, our experiments demonstrate the
effectiveness of FLAIR trained on 30M image-text pairs in capturing
fine-grained visual information, including zero-shot semantic segmentation,
outperforming models trained on billions of pairs. Code is available at
https://github.com/ExplainableML/flair .

æè¦ï¼CLIP å¨å¤§è¦æ¨¡å½±ååæå­æ¯å°ä¸å±ç¾ä»¤äººå°è±¡æ·±å»çææã
ç¶èï¼å®æ·åè©³ç´°è¦è¦ºç¹å¾µçè½åä»ç¶æéï¼
å çº CLIP å¨å¨çå±¤ç´ä¸æ¯å°å½±ååæå­ãçºäºè§£æ±ºéååé¡ï¼
æåæåº FLAIRï¼ä¸ç¨®ç²¾ç´°åçèªè¨è³è¨å½±åè¡¨å¾µï¼ä¸ç¨®
å©ç¨é·èè©³ç´°çå½±åæè¿°ä¾å­¸ç¿ååå
å½±ååµå¥çæ¹æ³ãééåæ¨£æè¿°å½±åç²¾ç´°å
ç´°ç¯çå¤æ¨£åå­æ¨é¡ï¼æåè¨ç·´æåçè¦è¦ºèªè¨æ¨¡åï¼ä¸åç¢ç
æ´é«åµå¥ï¼ä¹ç¢çç¹å®æå­çå½±åè¡¨å¾µãæåçæ¨¡å
å¨ååå½±åæ¨è¨ä¸å¼é²æå­æ¢ä»¶æ³¨ææ± åï¼ä»¥
ç¢çç²¾ç´°åçå½±åè¡¨å¾µï¼éäºè¡¨å¾µæé·æ·åè©³ç´°
å½±åå§å®¹ãæåå¨ç¾æç
å¤æ¨¡ææª¢ç´¢åºæºä¸ä»¥åæåæ°æ¨åºçç²¾ç´°å
æª¢ç´¢ä»»åä¸éææåé²çè¡¨ç¾ï¼è©²ä»»åè©ä¼°è¦è¦ºèªè¨æ¨¡åæ·å
é¨åå½±åå§å®¹çè½åãæ­¤å¤ï¼æåçå¯¦é©è­æï¼å¨ 30M å½±åæå­å°ä¸è¨ç·´ç FLAIR å¨æ·å
ç²¾ç´°åçè¦è¦ºè³è¨ä¸å¾ææï¼åæ¬é¶æ¬¡å­¸ç¿èªæåå²ï¼
è¡¨ç¾åªæ¼å¨æ¸ååå°ä¸è¨ç·´çæ¨¡åãç¨å¼ç¢¼å¯å¨
https://github.com/ExplainableML/flair åå¾ã

##### **Best-of-N Jailbreaking**
2412.03556v1 by John Hughes, Sara Price, Aengus Lynch, Rylan Schaeffer, Fazl Barez, Sanmi Koyejo, Henry Sleight, Erik Jones, Ethan Perez, Mrinank Sharma

We introduce Best-of-N (BoN) Jailbreaking, a simple black-box algorithm that
jailbreaks frontier AI systems across modalities. BoN Jailbreaking works by
repeatedly sampling variations of a prompt with a combination of augmentations
- such as random shuffling or capitalization for textual prompts - until a
harmful response is elicited. We find that BoN Jailbreaking achieves high
attack success rates (ASRs) on closed-source language models, such as 89% on
GPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts.
Further, it is similarly effective at circumventing state-of-the-art
open-source defenses like circuit breakers. BoN also seamlessly extends to
other modalities: it jailbreaks vision language models (VLMs) such as GPT-4o
and audio language models (ALMs) like Gemini 1.5 Pro, using modality-specific
augmentations. BoN reliably improves when we sample more augmented prompts.
Across all modalities, ASR, as a function of the number of samples (N),
empirically follows power-law-like behavior for many orders of magnitude. BoN
Jailbreaking can also be composed with other black-box algorithms for even more
effective attacks - combining BoN with an optimized prefix attack achieves up
to a 35% increase in ASR. Overall, our work indicates that, despite their
capability, language models are sensitive to seemingly innocuous changes to
inputs, which attackers can exploit across modalities.

æè¦ï¼<paragraph>æåæ¨åº Best-of-N (BoN) è¶çï¼éæ¯ä¸ç¨®ç°¡å®çé»çå­æ¼ç®æ³ï¼å¯ä»¥è¶çè·¨æ¨¡æçéç AI ç³»çµ±ãBoN è¶çéééè¤æ½æ¨£æç¤ºçè®ç°ï¼ä¸¦çµåå¢å¼·åè½ï¼ä¾å¦é¨æ©æ´çæå°æå­æç¤ºé²è¡å¤§å°å¯«è®æï¼ï¼ç´å°å¼ç¼æå®³åæçºæ­¢ãæåç¼ç¾ BoN è¶çå¨å°éåå§ç¢¼èªè¨æ¨¡åä¸éå°å¾é«çæ»ææåç (ASR)ï¼ä¾å¦å¨æ½æ¨£ 10,000 åå¢å¼·æç¤ºæï¼GPT-4o ä¸çº 89%ï¼è Claude 3.5 Sonnet ä¸çº 78%ãæ­¤å¤ï¼å®å¨è¦é¿é»è·¯ä¸­æ·å¨ç­æåé²çéæºé²ç¦¦æ¹é¢ä¹åæ¨£ææãBoN ä¹è½ç¡ç¸«å»¶ä¼¸å°å¶ä»æ¨¡æï¼å®ééä½¿ç¨ç¹å®æ¼æ¨¡æçå¢å¼·åè½ï¼è¶çäºè¦è¦ºèªè¨æ¨¡å (VLM)ï¼ä¾å¦ GPT-4oï¼ä»¥åé³è¨èªè¨æ¨¡å (ALM)ï¼ä¾å¦ Gemini 1.5 Proãç¶æåæ½æ¨£æ´å¤å¢å¼·æç¤ºæï¼BoN æå¯é å°æåãå¨æææ¨¡æä¸­ï¼ASR ä½çºæ¨£æ¬æ¸ (N) çå½æ¸ï¼å¨è¨±å¤æ¸éç´ä¸é½ç¶é©æ§å°éµå¾ªåªå¾è¡çºãBoN è¶çä¹å¯ä»¥èå¶ä»é»çå­æ¼ç®æ³çµåï¼ä»¥é²è¡æ´ææçæ»æ - å° BoN èæä½³åå­é¦æ»æçµåï¼å¯å° ASR æåå¤é 35%ãç¸½é«èè¨ï¼æåçç ç©¶æåºï¼åç®¡èªè¨æ¨¡åå·æè½åï¼ä½å®åå°è¼¸å¥ççä¼¼ç¡å®³è®æ´å¾ææï¼èæ»æèå¯ä»¥è·¨æ¨¡æå©ç¨éä¸é»ã</paragraph>

##### **Perception Tokens Enhance Visual Reasoning in Multimodal Language Models**
2412.03548v1 by Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G. Shapiro, Ranjay Krishna

Multimodal language models (MLMs) still face challenges in fundamental visual
perception tasks where specialized models excel. Tasks requiring reasoning
about 3D structures benefit from depth estimation, and reasoning about 2D
object instances benefits from object detection. Yet, MLMs can not produce
intermediate depth or boxes to reason over. Finetuning MLMs on relevant data
doesn't generalize well and outsourcing computation to specialized vision tools
is too compute-intensive and memory-inefficient. To address this, we introduce
Perception Tokens, intrinsic image representations designed to assist reasoning
tasks where language is insufficient. Perception tokens act as auxiliary
reasoning tokens, akin to chain-of-thought prompts in language models. For
example, in a depth-related task, an MLM augmented with perception tokens can
reason by generating a depth map as tokens, enabling it to solve the problem
effectively. We propose AURORA, a training method that augments MLMs with
perception tokens for improved reasoning over visual inputs. AURORA leverages a
VQVAE to transform intermediate image representations, such as depth maps into
a tokenized format and bounding box tokens, which is then used in a multi-task
training framework. AURORA achieves notable improvements across counting
benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench,
outperforming finetuning approaches in generalization across datasets. It also
improves on relative depth: over +6% on BLINK. With perception tokens, AURORA
expands the scope of MLMs beyond language-based reasoning, paving the way for
more effective visual reasoning capabilities.

æè¦ï¼å¤æ¨¡æè¯­è¨æ¨¡å (MLM) å¨ä¸é¨æ¨¡åè¡¨ç°ä¼å¼çåºæ¬è§è§æç¥ä»»å¡ä¸­ä»ç¶é¢ä¸´ææãéè¦å¯¹ 3D ç»æè¿è¡æ¨ççä»»å¡åçäºæ·±åº¦ä¼°è®¡ï¼èå¯¹ 2D å¯¹è±¡å®ä¾è¿è¡æ¨çåçäºå¯¹è±¡æ£æµãç¶èï¼MLM æ æ³äº§çä¸­é´æ·±åº¦ææ¡æ¥è¿è¡æ¨çãéå¯¹ç¸å³æ°æ®å¾®è° MLM å¨æ³åè½åæ¹é¢è¡¨ç°ä¸ä½³ï¼èå°è®¡ç®å¤åç»ä¸é¨çè§è§å·¥å·åè¿äºè®¡ç®å¯éä¸åå­æçä½ä¸ãä¸ºäºè§£å³è¿ä¸ªé®é¢ï¼æä»¬å¼å¥äºæç¥æ è®°ï¼è¿æ¯æ¨å¨è¾å©è¯­è¨ä¸è¶³çæ¨çä»»å¡çåå¨å¾åè¡¨ç¤ºãæç¥æ è®°åå½è¾å©æ¨çæ è®°ï¼ç±»ä¼¼äºè¯­è¨æ¨¡åä¸­çææ³é¾æç¤ºãä¾å¦ï¼å¨ä¸æ·±åº¦ç¸å³çä»»å¡ä¸­ï¼ä½¿ç¨æç¥æ è®°å¢å¼ºåç MLM å¯ä»¥éè¿çææ·±åº¦å¾ä½ä¸ºæ è®°æ¥è¿è¡æ¨çï¼ä»èææå°è§£å³é®é¢ãæä»¬æåºäº AURORAï¼è¿æ¯ä¸ç§è®­ç»æ¹æ³ï¼å®ä½¿ç¨æç¥æ è®°å¢å¼º MLMï¼ä»¥æ¹è¿å¯¹è§è§è¾å¥çæ¨çãAURORA å©ç¨ VQVAE å°ä¸­é´å¾åè¡¨ç¤ºï¼ä¾å¦æ·±åº¦å¾ï¼è½¬æ¢ä¸ºæ è®°åæ ¼å¼åè¾¹çæ¡æ è®°ï¼ç¶åå¨å¤ä»»å¡è®­ç»æ¡æ¶ä¸­ä½¿ç¨ãAURORA å¨è®¡æ°åºåæµè¯ä¸­åå¾äºæ¾çæ¹è¿ï¼BLINK æé«äº +10.8%ï¼CVBench æé«äº +11.3%ï¼SEED-Bench æé«äº +8.3%ï¼å¨æ°æ®éä¸­çæ³åè½åæ¹é¢ä¼äºå¾®è°æ¹æ³ãå®è¿æ¹è¿äºç¸å¯¹æ·±åº¦ï¼BLINK æé«äº +6% ä»¥ä¸ãæäºæç¥æ è®°ï¼AURORA å° MLM çèå´æ©å±å°äºåºäºè¯­è¨çæ¨çä¹å¤ï¼ä¸ºæ´ææçè§è§æ¨çè½åéºå¹³äºéè·¯ã

##### **NODE-AdvGAN: Improving the transferability and perceptual similarity of adversarial examples by dynamic-system-driven adversarial generative model**
2412.03539v1 by Xinheng Xie, Yue Wu, Cuiyu He

Understanding adversarial examples is crucial for improving the model's
robustness, as they introduce imperceptible perturbations that deceive models.
Effective adversarial examples, therefore, offer the potential to train more
robust models by removing their singularities. We propose NODE-AdvGAN, a novel
approach that treats adversarial generation as a continuous process and employs
a Neural Ordinary Differential Equation (NODE) for simulating the dynamics of
the generator. By mimicking the iterative nature of traditional gradient-based
methods, NODE-AdvGAN generates smoother and more precise perturbations that
preserve high perceptual similarity when added to benign images. We also
propose a new training strategy, NODE-AdvGAN-T, which enhances transferability
in black-box attacks by effectively tuning noise parameters during training.
Experiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate more
effective adversarial examples that achieve higher attack success rates while
preserving better perceptual quality than traditional GAN-based methods.

æè¦ï¼äºè§£å°ææ§ç¯ä¾å°æ¼æåæ¨¡åçç©©å¥æ§è³ééè¦ï¼å çºå®åå¼å¥äºé£ä»¥å¯è¦ºçæ¾åï¼æ¬ºé¨äºæ¨¡åãå æ­¤ï¼ææçå°ææ§ç¯ä¾æä¾äºééç§»é¤å¶å¥ç°æ§ä¾è¨ç·´æ´ç©©å¥æ¨¡åçæ½åãæåæåºäº NODE-AdvGANï¼éæ¯ä¸ç¨®æ°ç©çæ¹æ³ï¼å°å°ææ§çæè¦çºä¸åé£çºçéç¨ï¼ä¸¦æ¡ç¨ç¥ç¶å¸¸å¾®åæ¹ç¨å¼ (NODE) ä¾æ¨¡æ¬çæå¨çåæãééæ¨¡ä»¿å³çµ±åºæ¼æ¢¯åº¦çè¿­ä»£æ¹æ³ï¼NODE-AdvGAN ç¢çæ´å¹³æ»ä¸æ´ç²¾ç¢ºçæ¾åï¼å¨æ·»å å°è¯æ§å½±åæï¼ä»è½ä¿æé«åº¦çæç¥ç¸ä¼¼æ§ãæåä¹æåºäºæ°çè¨ç·´ç­ç¥ NODE-AdvGAN-Tï¼å®ééå¨è¨ç·´æéææå°èª¿æ´éè¨åæ¸ï¼å¢å¼·äºé»çæ»æä¸­çå¯å³éæ§ãå¯¦é©è­æï¼NODE-AdvGAN å NODE-AdvGAN-T ç¢ççå°ææ§ç¯ä¾æ´ææï¼å¨ä¿ææ¯å³çµ±åºæ¼ GAN çæ¹æ³æ´å¥½çæç¥åè³ªçåæï¼éå°äºæ´é«çæ»ææåçã

##### **Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models**
2412.03537v1 by Natalie Mackraz, Nivedha Sivakumar, Samira Khorshidi, Krishna Patel, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff

Large language models (LLMs) are increasingly being adapted to achieve
task-specificity for deployment in real-world decision systems. Several
previous works have investigated the bias transfer hypothesis (BTH) by studying
the effect of the fine-tuning adaptation strategy on model fairness to find
that fairness in pre-trained masked language models have limited effect on the
fairness of models when adapted using fine-tuning. In this work, we expand the
study of BTH to causal models under prompt adaptations, as prompting is an
accessible, and compute-efficient way to deploy models in real-world systems.
In contrast to previous works, we establish that intrinsic biases in
pre-trained Mistral, Falcon and Llama models are strongly correlated (rho >=
0.94) with biases when the same models are zero- and few-shot prompted, using a
pronoun co-reference resolution task. Further, we find that bias transfer
remains strongly correlated even when LLMs are specifically prompted to exhibit
fair or biased behavior (rho >= 0.92), and few-shot length and stereotypical
composition are varied (rho >= 0.97). Our findings highlight the importance of
ensuring fairness in pre-trained LLMs, especially when they are later used to
perform downstream tasks via prompt adaptation.

æè¦ï¼å¤§åè¯­è¨æ¨¡å (LLM) æ­£è¶æ¥è¶å¤å°è¢«è°æ´ä¸ºå®ç°ä»»å¡ç¹å¼æ§ï¼ä»¥ä¾¿é¨ç½²å¨ç°å®ä¸ççå³ç­ç³»ç»ä¸­ãä¸äºä»¥åçä½åéè¿ç ç©¶å¾®è°è°æ´ç­ç¥å¯¹æ¨¡åå¬å¹³æ§çå½±åæ¥è°æ¥åå·®è½¬ç§»åè®¾ (BTH)ï¼åç°é¢åè®­ç»çæ©è½è¯­è¨æ¨¡åä¸­çå¬å¹³æ§å¯¹ä½¿ç¨å¾®è°è°æ´åçæ¨¡åå¬å¹³æ§å½±åæéãå¨è¿é¡¹å·¥ä½ä¸­ï¼æä»¬å° BTH çç ç©¶æ©å±å°æç¤ºè°æ´ä¸çå ææ¨¡åï¼å ä¸ºæç¤ºæ¯ä¸ç§å¯è®¿é®ä¸è®¡ç®é«æçæ¹æ³ï¼å¯ä»¥å°æ¨¡åé¨ç½²å°ç°å®ä¸çç³»ç»ä¸­ãä¸ä»¥åçä½åç¸åï¼æä»¬ç¡®å®é¢åè®­ç»ç MistralãFalcon å Llama æ¨¡åä¸­çåå¨åå·®ä¸ä½¿ç¨ä»£è¯å±æè§£æä»»å¡å¯¹ç¸åæ¨¡åè¿è¡é¶æ¬¡åå°æ¬¡æç¤ºæ¶çåå·®å¯åç¸å³ï¼rho >= 0.94ï¼ãæ­¤å¤ï¼æä»¬åç°ï¼å³ä½¿ LLM è¢«æç¡®æç¤ºè¡¨ç°åºå¬å¹³ææåå·®çè¡ä¸ºï¼rho >= 0.92ï¼ï¼å¹¶ä¸å°æ¬¡æç¤ºé¿åº¦åå»æ¿å°è±¡æåææä¸åï¼rho >= 0.97ï¼ï¼åå·®è½¬ç§»ä»ç¶å¯åç¸å³ãæä»¬çç ç©¶ç»æçªåºäºç¡®ä¿é¢åè®­ç»ç LLM å¬å¹³æ§çéè¦æ§ï¼å°¤å¶æ¯å¨å®ä»¬åæ¥éè¿æç¤ºè°æ´ç¨äºæ§è¡ä¸æ¸¸ä»»å¡æ¶ã

##### **A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences**
2412.03531v1 by Gabriel Lino Garcia, JoÃ£o Renato Ribeiro Manesco, Pedro Henrique Paiola, Lucas Miranda, Maria Paola de Salvo, JoÃ£o Paulo Papa

The rapid advancement of large language models (LLMs) has opened new
boundaries in the extraction and synthesis of medical knowledge, particularly
within evidence synthesis. This paper reviews the state-of-the-art applications
of LLMs in the biomedical domain, exploring their effectiveness in automating
complex tasks such as evidence synthesis and data extraction from a biomedical
corpus of documents. While LLMs demonstrate remarkable potential, significant
challenges remain, including issues related to hallucinations, contextual
understanding, and the ability to generalize across diverse medical tasks. We
highlight critical gaps in the current research literature, particularly the
need for unified benchmarks to standardize evaluations and ensure reliability
in real-world applications. In addition, we propose directions for future
research, emphasizing the integration of state-of-the-art techniques such as
retrieval-augmented generation (RAG) to enhance LLM performance in evidence
synthesis. By addressing these challenges and utilizing the strengths of LLMs,
we aim to improve access to medical literature and facilitate meaningful
discoveries in healthcare.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çå¿«éé²å±éåäºé«çç¥è­èååç¶åçæ°é åï¼ç¹å¥æ¯å¨è­æç¶åä¸­ãæ¬æåé¡§äº LLM å¨çç©é«å­¸é åçææ°æç¨ï¼æ¢è¨äºå®åå¨èªååè¤éä»»åï¼ä¾å¦å¾çç©é«å­¸æä»¶èªæåº«ä¸­é²è¡è­æç¶ååæ¸æèåï¼æ¹é¢çæè½ãåç®¡ LLM å±ç¾äºé¡¯èçæ½åï¼ä½ä»æéå¤§çææ°ï¼åæ¬èå¹»è¦ºãèçµ¡çè§£ä»¥åå¨ä¸åé«çä»»åä¸­æ¦æ¬çè½åç¸éçåé¡ãæåå¼·èª¿äºç¶åç ç©¶æç»ä¸­çééµå·®è·ï¼ç¹å¥æ¯éè¦çµ±ä¸çåºæºä¾æ¨æºåè©ä¼°ä¸¦ç¢ºä¿å¯¦éæç¨ä¸­çå¯é æ§ãæ­¤å¤ï¼æåæåºäºæªä¾ç ç©¶çæ¹åï¼å¼·èª¿æ´åæåé²çæè¡ï¼ä¾å¦æª¢ç´¢å¢å¼·çæ (RAG)ï¼ä»¥å¢å¼· LLM å¨è­æç¶åä¸­çè¡¨ç¾ãééè§£æ±ºéäºææ°ä¸¦å©ç¨ LLM çåªå¢ï¼æåæ¨å¨æ¹åå°é«å­¸æç»çåå¾ï¼ä¸¦ä¿é²é«çä¿å¥ä¸­çææç¾©ç¼ç¾ã

##### **FANAL -- Financial Activity News Alerting Language Modeling Framework**
2412.03527v1 by Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar, Hari Nalluri

In the rapidly evolving financial sector, the accurate and timely
interpretation of market news is essential for stakeholders needing to navigate
unpredictable events. This paper introduces FANAL (Financial Activity News
Alerting Language Modeling Framework), a specialized BERT-based framework
engineered for real-time financial event detection and analysis, categorizing
news into twelve distinct financial categories. FANAL leverages silver-labeled
data processed through XGBoost and employs advanced fine-tuning techniques,
alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned with
ORPO (Odds Ratio Preference Optimization) for superior class-wise probability
calibration and alignment with financial event relevance. We evaluate FANAL's
performance against leading large language models, including GPT-4o, Llama-3.1
8B, and Phi-3, demonstrating its superior accuracy and cost efficiency. This
framework sets a new standard for financial intelligence and responsiveness,
significantly outstripping existing models in both performance and
affordability.

æè¦ï¼å¨å¿«éè®åçéèé åä¸­ï¼æºç¢ºåæå°è§£è®å¸å ´æ°èå°æ¼éè¦æå°ä¸å¯é æ¸¬äºä»¶çå©çç¸éèè³ééè¦ãæ¬æä»ç´¹ FANALï¼éèæ´»åæ°èè­¦ç¤ºèªè¨å»ºæ¨¡æ¡æ¶ï¼ï¼éæ¯ä¸ååºæ¼ BERT çå°ç¨æ¡æ¶ï¼å°éè¨­è¨ç¨æ¼å¯¦æéèäºä»¶æª¢æ¸¬ååæï¼å°æ°èæ­¸é¡çºåäºåä¸åçéèé¡å¥ãFANAL å©ç¨éé XGBoost èççéæ¨ç±¤æ¸æï¼ä¸¦æ¡ç¨åé²çå¾®èª¿æè¡ï¼ä»¥å ORBERTï¼æ©çæ¯ BERTï¼ï¼éæ¯ä¸ç¨®æ°ç BERT è®é«ï¼ç¶é ORPOï¼æ©çæ¯åå¥½æä½³åï¼å¾®èª¿ï¼ä»¥å¯¦ç¾åªè¶çé¡å¥æ©çæ ¡æºåèéèäºä»¶ç¸éæ§çå°é½ãæåè©ä¼°äº FANAL èé åçå¤§èªè¨æ¨¡åçæè½ï¼åæ¬ GPT-4oãLlama-3.1 8B å Phi-3ï¼è­æäºå¶åªè¶çæºç¢ºæ§åææ¬æçãéåæ¡æ¶çºéèæå ±åé¿æè½åè¨­å®äºæ°çæ¨æºï¼å¨æè½åè² æè½åæ¹é¢é½é é è¶éç¾æçæ¨¡åã

##### **Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos**
2412.03526v1 by Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang

Recent advancements in static feed-forward scene reconstruction have
demonstrated significant progress in high-quality novel view synthesis.
However, these models often struggle with generalizability across diverse
environments and fail to effectively handle dynamic content. We present BTimer
(short for BulletTimer), the first motion-aware feed-forward model for
real-time reconstruction and novel view synthesis of dynamic scenes. Our
approach reconstructs the full scene in a 3D Gaussian Splatting representation
at a given target ('bullet') timestamp by aggregating information from all the
context frames. Such a formulation allows BTimer to gain scalability and
generalization by leveraging both static and dynamic scene datasets. Given a
casual monocular dynamic video, BTimer reconstructs a bullet-time scene within
150ms while reaching state-of-the-art performance on both static and dynamic
scene datasets, even compared with optimization-based approaches.

æè¦ï¼æè¿éæåé¥å ´æ¯éå»ºçé²å±å·²è­æå¨é«åè³ªçæ°è¦ååææ¹é¢åå¾éå¤§é²å±ã
ç¶èï¼éäºæ¨¡åéå¸¸é£ä»¥é©æåç¨®ç°å¢ï¼ä¸ç¡æ³ææèçåæå§å®¹ãæåæåº BTimerï¼BulletTimer çç¸®å¯«ï¼ï¼éæ¯ç¬¬ä¸åå·æåææç¥çåé¥æ¨¡åï¼ç¨æ¼åæå ´æ¯çå³æéå»ºåæ°è¦ååæãæåçåæ³æ¯ééå½æ´ææå§å®¹ç«é¢çè³è¨ï¼å¨çµ¦å®çç®æ¨ï¼ãå­å½ãï¼æéæ³è¨ä¸­ï¼ä»¥ 3D é«æ¯æ¿ºå°è¡¨ç¤ºæ³éå»ºæ´åå ´æ¯ãéç¨®å¬å¼åè® BTimer è½å¤ ééå©ç¨éæååæå ´æ¯è³æéä¾ç²å¾å¯æ´åæ§åæ¦æ¬æ§ãçµ¦å®ä¸åé¨æçå®ç¼åæå½±çï¼BTimer è½å¨ 150 æ¯«ç§å§éå»ºä¸åå­å½æéå ´æ¯ï¼åæå¨éæååæå ´æ¯è³æéä¸éå°æåé²çæè½ï¼å³ä½¿èåºæ¼æä½³åçåæ³ç¸æ¯ä¹æ¯å¦æ­¤ã

##### **You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?**
2412.03516v1 by Dominic Lohr, Hieke Keuning, Natalie Kiesler

Background: Feedback as one of the most influential factors for learning has
been subject to a great body of research. It plays a key role in the
development of educational technology systems and is traditionally rooted in
deterministic feedback defined by experts and their experience. However, with
the rise of generative AI and especially Large Language Models (LLMs), we
expect feedback as part of learning systems to transform, especially for the
context of programming. In the past, it was challenging to automate feedback
for learners of programming. LLMs may create new possibilities to provide
richer, and more individual feedback than ever before.
  Objectives: This paper aims to generate specific types of feedback for
introductory programming tasks using LLMs. We revisit existing feedback
taxonomies to capture the specifics of the generated feedback, such as
randomness, uncertainty, and degrees of variation.
  Methods: We iteratively designed prompts for the generation of specific
feedback types (as part of existing feedback taxonomies) in response to
authentic student programs. We then evaluated the generated output and
determined to what extent it reflected certain feedback types.
  Results and Conclusion: The present work provides a better understanding of
different feedback dimensions and characteristics. The results have
implications for future feedback research with regard to, for example, feedback
effects and learners' informational needs. It further provides a basis for the
development of new tools and learning systems for novice programmers including
feedback generated by AI.

æè¦ï¼**èæ¯ï¼**åé¥ä½çºå­¸ç¿ä¸­æå·å½±é¿åçå ç´ ä¹ä¸ï¼
ä¸ç´æ¯è¨±å¤ç ç©¶çä¸»é¡ãå®å¨æè²ç§æç³»çµ±çç¼å±ä¸­æ®æ¼èééµè§è²ï¼
ä¸¦ä¸å³çµ±ä¸æ ¹æ¤æ¼ç±å°å®¶åå¶ç¶é©å®ç¾©çç¢ºå®æ§åé¥ãç¶èï¼
é¨èçæå¼ AI åç¹å¥æ¯å¤§èªè¨æ¨¡å (LLM) çèèµ·ï¼æåé æåé¥
ä½çºå­¸ç¿ç³»çµ±çä¸é¨åæè½åï¼ç¹å¥æ¯å¨ç¨å¼è¨­è¨çèæ¯ä¸ãå¨éå»ï¼
èªååç¨å¼è¨­è¨å­¸ç¿èçåé¥æ¯ä¸é ææ°ãLLM å¯è½åµé æ°çå¯è½æ§ï¼
æä¾æ¯ä»¥å¾æ´è±å¯ãæ´åäººåçåé¥ã

**ç®æ¨ï¼**æ¬ææ¨å¨ä½¿ç¨ LLM çºå¥éç¨å¼è¨­è¨ä»»åç¢çç¹å®é¡åçåé¥ãæåéæ°å¯©è¦ç¾æçåé¥åé¡æ³ï¼
ä»¥æ·åæç¢çåé¥çå·é«ç¹å¾µï¼ä¾å¦é¨æ©æ§ãä¸ç¢ºå®æ§åè®ç°ç¨åº¦ã

**æ¹æ³ï¼**æååè¦è¨­è¨æç¤ºï¼ä»¥ç¢çç¹å®åé¥é¡åï¼ä½çºç¾æåé¥åé¡æ³çä¸é¨åï¼ï¼
ä»¥åæçå¯¦çå­¸çç¨å¼ãç¶å¾ï¼æåè©ä¼°ç¢ççè¼¸åºï¼
ä¸¦ç¢ºå®å®å¨å¤å¤§ç¨åº¦ä¸åæ äºæäºåé¥é¡åã

**çµæåçµè«ï¼**ç®åçç ç©¶æä¾äºå°ä¸ååé¥é¢ååç¹å¾µçæ´æ·±å¥äºè§£ãéäºçµæ
å°æªä¾çåé¥ç ç©¶å·æå½±é¿ï¼ä¾å¦åé¥ææåå­¸ç¿èçè³è¨éæ±ãå®é²ä¸æ­¥çº
éç¼æ°çå·¥å·åå­¸ç¿ç³»çµ±å¥ å®äºåºç¤ï¼éäºç³»çµ±åæ¬ç± AI çæçåé¥ï¼
é©ç¨æ¼æ°æç¨å¼è¨­è¨å¸«ã

##### **KKLIP: Knowledge Distillation Exploiting K-means Clustering for Language-Image Pre-Training**
2412.03513v1 by Kuei-Chun Kao

Recently, CLIP has emerged as a valuable model for aligning image and text
information in multi-modal scenarios. However, researchers have observed
limitations in the ability of CLIP's text and image encoders to extract
detailed knowledge from caption-image pairs. In response, this paper introduces
KKLIP, a novel approach designed to enhance the quality of CLIP by
incorporating a new knowledge distillation (KD) method derived from Llama 2.
Our method comprises three objectives: Text Embedding Distillation, Concept
Learning, and Contrastive Learning. Firstly, Text Embedding Distillation
involves training the KKLIP text encoder to emulate the teacher model, Llama 2.
Secondly, Concept Learning assigns a soft concept label to each caption-image
pair through offline k-means clustering of text information from Llama 2,
allowing KKLIP to learn from these soft concept labels. Finally, Contrastive
Learning harmonizes text and image embeddings. Our experimental results
demonstrate that KKLIP enhances the quality of both text and image encoders.

æè¦ï¼æè¿ï¼CLIP å·²æä¸ºä¸ç§æä»·å¼çæ¨¡åï¼ç¨äºå¨å¤æ¨¡æåºæ¯ä¸­å¯¹é½å¾ååææ¬ä¿¡æ¯ãç¶èï¼ç ç©¶äººåå·²ç»è§å¯å° CLIP çææ¬åå¾åç¼ç å¨ä»æ é¢å¾åå¯¹ä¸­æåè¯¦ç»ç¥è¯çè½åå­å¨å±éæ§ãå¯¹æ­¤ï¼æ¬æä»ç»äº KKLIPï¼è¿æ¯ä¸ç§æ°é¢çæ¹æ³ï¼æ¨å¨éè¿æ´åæºèª Llama 2 çæ°ç¥è¯è¸é¦ (KD) æ¹æ³æ¥å¢å¼º CLIP çè´¨éãæä»¬çæ¹æ³åæ¬ä¸ä¸ªç®æ ï¼ææ¬åµå¥è¸é¦ãæ¦å¿µå­¦ä¹ åå¯¹æ¯å­¦ä¹ ãé¦åï¼ææ¬åµå¥è¸é¦æ¶åè®­ç» KKLIP ææ¬ç¼ç å¨ä»¥æ¨¡ææå¸æ¨¡å Llama 2ãå¶æ¬¡ï¼æ¦å¿µå­¦ä¹ éè¿ç¦»çº¿å¯¹æ¥èª Llama 2 çææ¬ä¿¡æ¯è¿è¡ k åå¼èç±»ï¼ä¸ºæ¯ä¸ªæ é¢å¾åå¯¹åéä¸ä¸ªè½¯æ¦å¿µæ ç­¾ï¼ä»èåè®¸ KKLIP ä»è¿äºè½¯æ¦å¿µæ ç­¾ä¸­å­¦ä¹ ãæåï¼å¯¹æ¯å­¦ä¹ åè°ææ¬åå¾ååµå¥ãæä»¬çå®éªç»æè¡¨æï¼KKLIP å¢å¼ºäºææ¬åå¾åç¼ç å¨çè´¨éã

##### **A Bidirectional Siamese Recurrent Neural Network for Accurate Gait Recognition Using Body Landmarks**
2412.03498v1 by Proma Hossain Progga, Md. Jobayer Rahman, Swapnil Biswas, Md. Shakil Ahmed, Arif Reza Anwary, Swakkhar Shatabda

Gait recognition is a significant biometric technique for person
identification, particularly in scenarios where other physiological biometrics
are impractical or ineffective. In this paper, we address the challenges
associated with gait recognition and present a novel approach to improve its
accuracy and reliability. The proposed method leverages advanced techniques,
including sequential gait landmarks obtained through the Mediapipe pose
estimation model, Procrustes analysis for alignment, and a Siamese
biGRU-dualStack Neural Network architecture for capturing temporal
dependencies. Extensive experiments were conducted on large-scale cross-view
datasets to demonstrate the effectiveness of the approach, achieving high
recognition accuracy compared to other models. The model demonstrated
accuracies of 95.7%, 94.44%, 87.71%, and 86.6% on CASIA-B, SZU RGB-D, OU-MVLP,
and Gait3D datasets respectively. The results highlight the potential
applications of the proposed method in various practical domains, indicating
its significant contribution to the field of gait recognition.

æè¦ï¼æ­¥æè¾¨è­æ¯ä¸ç¨®éè¦ççç©ç¹å¾µæè¡ï¼ç¨æ¼åäººè­å¥ï¼ç¹å¥æ¯å¨å¶ä»çççç©ç¹å¾µä¸åå¯¦éæç¡æçææ³ä¸ãå¨æ¬æä¸­ï¼æåæ¢è¨äºèæ­¥æè¾¨è­ç¸éçææ°ï¼ä¸¦æåºäºä¸ç¨®æ°çæ¹æ³ä¾æé«å¶æºç¢ºæ§åå¯é æ§ãææåºçæ¹æ³å©ç¨äºåé²çæè¡ï¼åæ¬éé Mediapipe å§¿å¢ä¼°è¨æ¨¡åç²å¾çåºåæ­¥æå°æ¨ãç¨æ¼å°é½ç Procrustes åæï¼ä»¥åç¨æ¼æææéä¾è³´æ§ç Siamese biGRU-dualStack ç¥ç¶ç¶²è·¯æ¶æ§ãå¨å¤§åè·¨è¦åæ¸æéä¸é²è¡äºå»£æ³çå¯¦é©ï¼ä»¥è­æè©²æ¹æ³çæææ§ï¼èå¶ä»æ¨¡åç¸æ¯ï¼å¯¦ç¾äºå¾é«çè­å¥æºç¢ºçãè©²æ¨¡åå¨ CASIA-BãSZU RGB-DãOU-MVLP å Gait3D æ¸æéä¸åå¥å±ç¤ºäº 95.7%ã94.44%ã87.71% å 86.6% çæºç¢ºçãçµæçªåºäºææåºæ¹æ³å¨åç¨®å¯¦éé åçæ½å¨æç¨ï¼è¡¨æå¶å°æ­¥æè¾¨è­é åååºäºéå¤§è²¢ç»ã

##### **Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective**
2412.03487v1 by Neta Shaul, Itai Gat, Marton Havasi, Daniel Severo, Anuroop Sriram, Peter Holderrieth, Brian Karrer, Yaron Lipman, Ricky T. Q. Chen

The design space of discrete-space diffusion or flow generative models are
significantly less well-understood than their continuous-space counterparts,
with many works focusing only on a simple masked construction. In this work, we
aim to take a holistic approach to the construction of discrete generative
models based on continuous-time Markov chains, and for the first time, allow
the use of arbitrary discrete probability paths, or colloquially, corruption
processes. Through the lens of optimizing the symmetric kinetic energy, we
propose velocity formulas that can be applied to any given probability path,
completely decoupling the probability and velocity, and giving the user the
freedom to specify any desirable probability path based on expert knowledge
specific to the data domain. Furthermore, we find that a special construction
of mixture probability paths optimizes the symmetric kinetic energy for the
discrete case. We empirically validate the usefulness of this new design space
across multiple modalities: text generation, inorganic material generation, and
image generation. We find that we can outperform the mask construction even in
text with kinetic-optimal mixture paths, while we can make use of
domain-specific constructions of the probability path over the visual domain.

æè¦ï¼é¢æ£ç©ºéæ´æ£ææµåçææ¨¡åçè¨­è¨ç©ºéï¼èå¶é£çºç©ºéçå°æç©ç¸æ¯ï¼é¡¯èå°ä¸çæç­ï¼è¨±å¤ä½ååå°æ³¨æ¼ç°¡å®çé®ç½©æ§é ãå¨éé å·¥ä½ä¸­ï¼æåæ¨å¨æ¡ç¨æ´é«æ¹æ³ä¾å»ºæ§åºæ¼é£çºæéé¦¬å¯å¤«éçé¢æ£çææ¨¡åï¼ä¸¦é¦æ¬¡åè¨±ä½¿ç¨ä»»æé¢æ£æ©çè·¯å¾ï¼æéä¿å°èªªï¼æå£ç¨åºãééæä½³åå°ç¨±åè½çè§é»ï¼æåæåºå¯ä»¥æç¨æ¼ä»»ä½çµ¦å®æ©çè·¯å¾çéåº¦å¬å¼ï¼å®å¨è§£è¦æ©çåéåº¦ï¼ä¸¦è®ä½¿ç¨èè½å¤ æ ¹æç¹å®æ¼è³æé åçå°å®¶ç¥è­ï¼æå®ä»»ä½çæ³çæ©çè·¯å¾ãæ­¤å¤ï¼æåç¼ç¾æ··åæ©çè·¯å¾çç¹æ®æ§é ï¼ææä½³åé¢æ£ææ³çå°ç¨±åè½ãæåééå¤ç¨®æ¹å¼é©è­æ­¤æ°è¨­è¨ç©ºéçæç¨ï¼æå­çæãç¡æ©ææçæåå½±åçæãæåç¼ç¾ï¼å³ä½¿å¨å·æåè½æä½³æ··åè·¯å¾çæå­ä¸­ï¼æåä¹å¯ä»¥åªæ¼é®ç½©æ§é ï¼åææåå¯ä»¥å¨è¦è¦ºé åä¸­ä½¿ç¨æ©çè·¯å¾çç¹å®æ¼é åçæ§é ã

##### **Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning**
2412.03467v1 by Neale Ratzlaff, Man Luo, Xin Su, Vasudev Lal, Phillip Howard

Multimodal models typically combine a powerful large language model (LLM)
with a vision encoder and are then trained on multimodal data via instruction
tuning. While this process adapts LLMs to multimodal settings, it remains
unclear whether this adaptation compromises their original language reasoning
capabilities. In this work, we explore the effects of multimodal instruction
tuning on language reasoning performance. We focus on LLaVA, a leading
multimodal framework that integrates LLMs such as Vicuna or Mistral with the
CLIP vision encoder. We compare the performance of the original LLMs with their
multimodal-adapted counterparts across eight language reasoning tasks. Our
experiments yield several key insights. First, the impact of multimodal
learning varies between Vicuna and Mistral: we observe a degradation in
language reasoning for Mistral but improvements for Vicuna across most tasks.
Second, while multimodal instruction learning consistently degrades performance
on mathematical reasoning tasks (e.g., GSM8K), it enhances performance on
commonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that
a training-free model merging technique can effectively mitigate the language
reasoning degradation observed in multimodal-adapted Mistral and even improve
performance on visual tasks.

æè¦ï¼å¤æ¨¡ææ¨¡åéå¸¸å°å¼ºå¤§çå¤§åè¯­è¨æ¨¡å (LLM) ä¸è§è§ç¼ç å¨ç»åèµ·æ¥ï¼ç¶åéè¿æä»¤å¾®è°å¨å¤æ¨¡ææ°æ®ä¸è¿è¡è®­ç»ãè½ç¶æ­¤è¿ç¨ä½¿ LLM éåºå¤æ¨¡æè®¾ç½®ï¼ä½å°ä¸æ¸æ¥è¿ç§éåºæ¯å¦ä¼æå®³å¶åå§è¯­è¨æ¨çè½åãå¨è¿é¡¹å·¥ä½ä¸­ï¼æä»¬æ¢è®¨äºå¤æ¨¡ææä»¤å¾®è°å¯¹è¯­è¨æ¨çæ§è½çå½±åãæä»¬ä¸æ³¨äº LLaVAï¼è¿æ¯ä¸ä¸ªé¢åçå¤æ¨¡ææ¡æ¶ï¼å®å° Vicuna æ Mistral ç­ LLM ä¸ CLIP è§è§ç¼ç å¨éæå¨ä¸èµ·ãæä»¬æ¯è¾äºåå§ LLM ä¸å¶å¤æ¨¡æéåºå¯¹åºé¡¹å¨å«ä¸ªè¯­è¨æ¨çä»»å¡ä¸­çæ§è½ãæä»¬çå®éªäº§çäºå ä¸ªå³é®è§è§£ãé¦åï¼å¤æ¨¡æå­¦ä¹ çå½±åå¨ Vicuna å Mistral ä¹é´ææä¸åï¼æä»¬è§å¯å° Mistral çè¯­è¨æ¨çè½åä¸éï¼ä½å¤§å¤æ°ä»»å¡ä¸­ Vicuna çè¯­è¨æ¨çè½åé½æææé«ãå¶æ¬¡ï¼è½ç¶å¤æ¨¡ææä»¤å­¦ä¹ æç»­éä½æ°å­¦æ¨çä»»å¡ï¼ä¾å¦ GSM8Kï¼çæ§è½ï¼ä½å®å¢å¼ºäºå¸¸è¯æ¨çä»»å¡ï¼ä¾å¦ CommonsenseQAï¼çæ§è½ãæåï¼æä»¬è¯æäºä¸ç§æ è®­ç»æ¨¡ååå¹¶ææ¯å¯ä»¥ææç¼è§£å¨å¤æ¨¡æéåºç Mistral ä¸­è§å¯å°çè¯­è¨æ¨çéåï¼çè³å¯ä»¥æé«è§è§ä»»å¡çæ§è½ã

##### **From Words to Workflows: Automating Business Processes**
2412.03446v1 by Laura Minkova, Jessica LÃ³pez Espejel, Taki Eddine Toufik Djaidja, Walid Dahhane, El Hassane Ettifouri

As businesses increasingly rely on automation to streamline operations, the
limitations of Robotic Process Automation (RPA) have become apparent,
particularly its dependence on expert knowledge and inability to handle complex
decision-making tasks. Recent advancements in Artificial Intelligence (AI),
particularly Generative AI (GenAI) and Large Language Models (LLMs), have paved
the way for Intelligent Automation (IA), which integrates cognitive
capabilities to overcome the shortcomings of RPA. This paper introduces
Text2Workflow, a novel method that automatically generates workflows from
natural language user requests. Unlike traditional automation approaches,
Text2Workflow offers a generalized solution for automating any business
process, translating user inputs into a sequence of executable steps
represented in JavaScript Object Notation (JSON) format. Leveraging the
decision-making and instruction-following capabilities of LLMs, this method
provides a scalable, adaptable framework that enables users to visualize and
execute workflows with minimal manual intervention. This research outlines the
Text2Workflow methodology and its broader implications for automating complex
business processes.

æè¦ï¼<paragraph>é¨èä¼æ¥­æ¥çä¾è³´èªååä¾ç°¡åçéï¼æ©å¨äººæµç¨èªåå (RPA) çéå¶å·²è®å¾æé¡¯ï¼ç¹å¥æ¯å¶ä¾è³´å°å®¶ç¥è­ä»¥åç¡æ³èçè¤éæ±ºç­ä»»åãäººå·¥æºæ§ (AI) çææ°é²å±ï¼ç¹å¥æ¯çæå¼ AI (GenAI) åå¤§åèªè¨æ¨¡å (LLM)ï¼å·²çºæºæ§èªåå (IA) éªè·¯ï¼æ´åèªç¥è½åä»¥åæ RPA çç¼ºé»ãæ¬æä»ç´¹ Text2Workflowï¼ä¸ç¨®å¾èªç¶èªè¨ä½¿ç¨èè¦æ±ä¸­èªåç¢çå·¥ä½æµç¨çæ°æ¹æ³ãèå³çµ±çèªååæ¹æ³ä¸åï¼Text2Workflow æä¾äºä¸åéç¨çè§£æ±ºæ¹æ¡ï¼ç¨æ¼èªååä»»ä½æ¥­åæµç¨ï¼å°ä½¿ç¨èè¼¸å¥è½æçºä»¥ JavaScript ç©ä»¶è¡¨ç¤ºæ³ (JSON) æ ¼å¼è¡¨ç¤ºçå¯å·è¡æ­¥é©åºåãå©ç¨ LLM çæ±ºç­å¶å®åéµå¾ªæä»¤çè½åï¼æ­¤æ¹æ³æä¾äºä¸åå¯æ´åãå¯é©æçæ¡æ¶ï¼ä½¿ç¨æ¶è½å¤ ä»¥æå°ç äººå·¥å¹²é è¦è¦ºååå·è¡å·¥ä½æµç¨ãæ¬ç ç©¶æ¦è¿°äº Text2Workflow æ¹æ³åå¶å°èªååè¤éæ¥­åæµç¨çæ´å»£æ³å½±é¿ã</paragraph>

##### **PBP: Post-training Backdoor Purification for Malware Classifiers**
2412.03441v1 by Dung Thuy Nguyen, Ngoc N. Tran, Taylor T. Johnson, Kevin Leach

In recent years, the rise of machine learning (ML) in cybersecurity has
brought new challenges, including the increasing threat of backdoor poisoning
attacks on ML malware classifiers. For instance, adversaries could inject
malicious samples into public malware repositories, contaminating the training
data and potentially misclassifying malware by the ML model. Current
countermeasures predominantly focus on detecting poisoned samples by leveraging
disagreements within the outputs of a diverse set of ensemble models on
training data points. However, these methods are not suitable for scenarios
where Machine Learning-as-a-Service (MLaaS) is used or when users aim to remove
backdoors from a model after it has been trained. Addressing this scenario, we
introduce PBP, a post-training defense for malware classifiers that mitigates
various types of backdoor embeddings without assuming any specific backdoor
embedding mechanism. Our method exploits the influence of backdoor attacks on
the activation distribution of neural networks, independent of the
trigger-embedding method. In the presence of a backdoor attack, the activation
distribution of each layer is distorted into a mixture of distributions. By
regulating the statistics of the batch normalization layers, we can guide a
backdoored model to perform similarly to a clean one. Our method demonstrates
substantial advantages over several state-of-the-art methods, as evidenced by
experiments on two datasets, two types of backdoor methods, and various attack
configurations. Notably, our approach requires only a small portion of the
training data -- only 1\% -- to purify the backdoor and reduce the attack
success rate from 100\% to almost 0\%, a 100-fold improvement over the baseline
methods. Our code is available at
\url{https://github.com/judydnguyen/pbp-backdoor-purification-official}.

æè¦ï¼è¿å¹´ä¾ï¼æ©å¨å­¸ç¿ (ML) å¨ç¶²è·¯å®å¨çå´èµ·å¸¶ä¾äºæ°çææ°ï¼åæ¬éå° ML æ¡æè»é«åé¡å¨è¶ä¾è¶å´éçå¾éææ¯æ»æå¨èãä¾å¦ï¼æ»æèå¯ä»¥å°æ¡ææ¨£æ¬æ³¨å¥å¬å±æ¡æè»é«å²å­åº«ï¼æ±¡æè¨ç·´è³æï¼ä¸¦å¯è½å°è´ ML æ¨¡åé¯èª¤åé¡æ¡æè»é«ãç®åçå°ç­ä¸»è¦éä¸­æ¼ééå©ç¨ä¸çµå¤åéææ¨¡åå¨è¨ç·´è³æé»ä¸è¼¸åºçåæ­§ä¾åµæ¸¬ä¸­æ¯æ¨£æ¬ãç¶èï¼éäºæ¹æ³ä¸é©ç¨æ¼ä½¿ç¨æ©å¨å­¸ç¿å³æå (MLaaS) æä½¿ç¨èå¨è¨ç·´æ¨¡åå¾å¸æå¾æ¨¡åä¸­ç§»é¤å¾éçå ´æ¯ãçºäºèçéç¨®å ´æ¯ï¼æåå¼å¥äº PBPï¼éæ¯ä¸ç¨®éå°æ¡æè»é«åé¡å¨çè¨ç·´å¾é²ç¦¦æ©å¶ï¼å®è½æ¸è¼åç¨®é¡åçå¾éåµå¥ï¼èç¡éåè¨­ä»»ä½ç¹å®å¾éåµå¥æ©å¶ãæåçæ¹æ³å©ç¨å¾éæ»æå°ç¥ç¶ç¶²è·¯åç¨åä½çå½±é¿ï¼èè§¸ç¼åµå¥æ¹æ³ç¡éãå¨å¾éæ»æçææ³ä¸ï¼æ¯ä¸å±¤çåç¨åä½ææ­æ²ææ··ååä½ãééèª¿ç¯æ¹æ¬¡æ¨æºåå±¤ççµ±è¨è³æï¼æåå¯ä»¥å¼å°å¾éæ¨¡åå·è¡é¡ä¼¼æ¼ä¹¾æ·¨æ¨¡åçä»»åãæåçæ¨¡åå±ç¾åºåªæ¼å¤ç¨®æåé²æ¹æ³çé¡¯èåªå¢ï¼éå¨å©åè³æéãå©ç¨®å¾éæ¹æ³ååç¨®æ»æéç½®çå¯¦é©ä¸­å¾å°è­æãå¼å¾æ³¨æçæ¯ï¼æåçæ¨¡ååéè¦ä¸å°é¨åè¨ç·´è³æ (å 1%) å°±è½æ·¨åå¾éï¼ä¸¦å°æ»ææåçå¾ 100% éä½å°å¹¾ä¹ 0%ï¼æ¯åºæºæ¹æ³æåäº 100 åãæåçç¨å¼ç¢¼å¯æ¼ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/judydnguyen/pbp-backdoor-purification-official}ã

##### **BIMCaP: BIM-based AI-supported LiDAR-Camera Pose Refinement**
2412.03434v1 by Miguel Arturo Vega Torres, Anna Ribic, Borja GarcÃ­a de Soto, AndrÃ© Borrmann

This paper introduces BIMCaP, a novel method to integrate mobile 3D sparse
LiDAR data and camera measurements with pre-existing building information
models (BIMs), enhancing fast and accurate indoor mapping with affordable
sensors. BIMCaP refines sensor poses by leveraging a 3D BIM and employing a
bundle adjustment technique to align real-world measurements with the model.
Experiments using real-world open-access data show that BIMCaP achieves
superior accuracy, reducing translational error by over 4 cm compared to
current state-of-the-art methods. This advancement enhances the accuracy and
cost-effectiveness of 3D mapping methodologies like SLAM. BIMCaP's improvements
benefit various fields, including construction site management and emergency
response, by providing up-to-date, aligned digital maps for better
decision-making and productivity. Link to the repository:
https://github.com/MigVega/BIMCaP

æè¦ï¼éç¯è«æä»ç´¹äº BIMCaPï¼ä¸ç¨®æ°ç©çæ¹æ³ï¼ç¨æ¼æ´åè¡å 3D ç¨ç LiDAR è³æåç¸æ©æ¸¬éï¼ä»¥åé åå­å¨çå»ºç¯è³è¨æ¨¡å (BIM)ï¼èç±ç¶æ¿å¯¦æ çææ¸¬å¨å å¼·å¿«éä¸ç²¾æºçå®¤å§å»ºæ§ãBIMCaP ééå©ç¨ 3D BIMï¼ä¸¦æ¡ç¨æèª¿æ´æè¡ï¼å°çå¯¦ä¸çæ¸¬éèæ¨¡åå°é½ï¼é²èåªåææ¸¬å¨å§¿å¢ãä½¿ç¨çå¯¦ä¸çéæ¾åç¨è³æçå¯¦é©é¡¯ç¤ºï¼èç®åæåé²çæ¹æ³ç¸æ¯ï¼BIMCaP éå°æ´é«çç²¾ç¢ºåº¦ï¼å°å¹³ç§»èª¤å·®æ¸å°è¶é 4 å¬åãæ­¤é é²å±æåäº 3D å»ºæ§æ¹æ³ï¼ä¾å¦ SLAMï¼çç²¾ç¢ºåº¦åææ¬æçãBIMCaP çæ¹é²ä½¿åç¨®é ååçï¼åæ¬å»ºç¯å·¥å°ç®¡çåç·æ¥æè®ï¼èç±æä¾ææ°çå°é½æ¸ä½å°åï¼ä»¥å©æ¼æ´å¥½çæ±ºç­å¶å®åçç¢åãå²å­åº«é£çµï¼https://github.com/MigVega/BIMCaP

##### **Automated Test-Case Generation for REST APIs Using Model Inference Search Heuristic**
2412.03420v1 by Clinton Cao, Annibale Panichella, Sicco Verwer

The rising popularity of the microservice architectural style has led to a
growing demand for automated testing approaches tailored to these systems.
EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to
automatically generate test cases for microservices' REST APIs. One limitation
of these EAs is the use of unit-level search heuristics, such as branch
distances, which focus on fine-grained code coverage and may not effectively
capture the complex, interconnected behaviors characteristic of system-level
testing. To address this limitation, we propose a new search heuristic (MISH)
that uses real-time automaton learning to guide the test case generation
process. We capture the sequential call patterns exhibited by a test case by
learning an automaton from the stream of log events outputted by different
microservices within the same system. Therefore, MISH learns a representation
of the systemwide behavior, allowing us to define the fitness of a test case
based on the path it traverses within the inferred automaton. We empirically
evaluate MISH's effectiveness on six real-world benchmark microservice
applications and compare it against a state-of-the-art technique, MOSA, for
testing REST APIs. Our evaluation shows promising results for using MISH to
guide the automated test case generation within EvoMaster.

æè¦ï¼å¾®æåæ¶æ§é¢¨æ ¼æ¥çæ®åï¼å°è´å°éå°éäºç³»çµ±éèº«æé çèªååæ¸¬è©¦æ¹æ³çéæ±ä¸æ·å¢é·ãEvoMaster æ¯ä¸æ¬¾æåé²çå·¥å·ï¼å®ä½¿ç¨æ¼ç®æ³ (EA) èªåç¢çå¾®æå REST API çæ¸¬è©¦æ¡ä¾ãéäº EA çä¸åéå¶æ¯ä½¿ç¨å®åå±¤ç´çæå°åç¼æ³ï¼ä¾å¦åæ¯è·é¢ï¼éç¨®æ¹æ³å°æ³¨æ¼ç²¾ç´°çç¨å¼ç¢¼è¦èçï¼å¯è½ç¡æ³ææææç³»çµ±å±¤ç´æ¸¬è©¦ç¹æçè¤éãç¸äºéè¯çè¡çºãçºäºè§£æ±ºéåéå¶ï¼æåæåºä¸åæ°çæå°åç¼æ³ (MISH)ï¼å®ä½¿ç¨å³æèªåæ©å­¸ç¿ä¾å¼å°æ¸¬è©¦æ¡ä¾ç¢çç¨åºãæåééå¾åä¸åç³»çµ±ä¸­ä¸åå¾®æåè¼¸åºçæ¥èªäºä»¶ä¸²æµå­¸ç¿èªåæ©ï¼ä¾æææ¸¬è©¦æ¡ä¾å±ç¾çé åºå¼å«æ¨¡å¼ãå æ­¤ï¼MISH æå­¸ç¿ç³»çµ±ç¯åè¡çºçè¡¨ç¤ºï¼è®æåå¯ä»¥æ ¹ææ¸¬è©¦æ¡ä¾å¨æ¨è«çèªåæ©ä¸­æèµ°è¨ªçè·¯å¾ä¾å®ç¾©å¶é©æåº¦ãæåå°å­åçå¯¦ä¸ççåºæºå¾®æåæç¨ç¨å¼å¯¦è­è©ä¼°äº MISH çæææ§ï¼ä¸¦å°å¶èæåé²ç REST API æ¸¬è©¦æè¡ MOSA é²è¡æ¯è¼ãæåçè©ä¼°é¡¯ç¤ºï¼ä½¿ç¨ MISH ä¾å¼å° EvoMaster ä¸­çèªååæ¸¬è©¦æ¡ä¾ç¢çï¼å·æä»¤äººæ¯å¥®ççµæã

##### **Benchmarking Pretrained Attention-based Models for Real-Time Recognition in Robot-Assisted Esophagectomy**
2412.03401v1 by Ronald L. P. D. de Jong, Yasmina al Khalil, Tim J. M. Jaspers, Romy C. van Jaarsveld, Gino M. Kuiper, Yiping Li, Richard van Hillegersberg, Jelle P. Ruurda, Marcel Breeuwer, Fons van der Sommen

Esophageal cancer is among the most common types of cancer worldwide. It is
traditionally treated using open esophagectomy, but in recent years,
robot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a
promising alternative. However, robot-assisted surgery can be challenging for
novice surgeons, as they often suffer from a loss of spatial orientation.
Computer-aided anatomy recognition holds promise for improving surgical
navigation, but research in this area remains limited. In this study, we
developed a comprehensive dataset for semantic segmentation in RAMIE, featuring
the largest collection of vital anatomical structures and surgical instruments
to date. Handling this diverse set of classes presents challenges, including
class imbalance and the recognition of complex structures such as nerves. This
study aims to understand the challenges and limitations of current
state-of-the-art algorithms on this novel dataset and problem. Therefore, we
benchmarked eight real-time deep learning models using two pretraining
datasets. We assessed both traditional and attention-based networks,
hypothesizing that attention-based networks better capture global patterns and
address challenges such as occlusion caused by blood or other tissues. The
benchmark includes our RAMIE dataset and the publicly available CholecSeg8k
dataset, enabling a thorough assessment of surgical segmentation tasks. Our
findings indicate that pretraining on ADE20k, a dataset for semantic
segmentation, is more effective than pretraining on ImageNet. Furthermore,
attention-based models outperform traditional convolutional neural networks,
with SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former
additionally excelling in average symmetric surface distance.

æè¦ï¼é£éçæ¯å¨çæå¸¸è¦çççé¡åä¹ä¸ãå³çµ±ä¸ä½¿ç¨éè¸é£éåé¤è¡é²è¡æ²»çï¼ä½è¿å¹´ä¾ï¼æ©å¨äººè¼å©å¾®åµé£éåé¤è¡ (RAMIE) å·²æçºä¸ç¨®æåæ¯çæ¿ä»£æ¹æ¡ãç¶èï¼å°æ¼æ°æå¤ç§é«çä¾èªªï¼æ©å¨äººè¼å©æè¡å¯è½å·æææ°æ§ï¼å çºä»åç¶å¸¸æåªå¤±ç©ºéå®ä½è½åãé»è¦è¼å©è§£åè­å¥æææ¹åæè¡å°èªï¼ä½éæ¹é¢çç ç©¶ä»ç¶æéãå¨éé ç ç©¶ä¸­ï¼æåçº RAMIE ä¸­çèªç¾©åå²éç¼äºä¸åå¨é¢çæ¸æéï¼å¶ä¸­åå«è¿ä»çºæ­¢æå¤§çéè¦è§£åçµæ§åæè¡å¨æ¢°éåãèçéä¸çµä¸åçé¡å¥æå¸¶ä¾ææ°ï¼åæ¬é¡å¥ä¸å¹³è¡¡ä»¥åå°ç¥ç¶ç­è¤éçµæ§çè­å¥ãæ¬ç ç©¶æ¨å¨äºè§£ç¶åæåé²æ¼ç®æ³å¨éåæ°æ¸æéååé¡ä¸çææ°åéå¶ãå æ­¤ï¼æåä½¿ç¨å©åé è¨ç·´æ¸æéå°å«åå¯¦ææ·±åº¦å­¸ç¿æ¨¡åé²è¡äºåºæºæ¸¬è©¦ãæåè©ä¼°äºå³çµ±ååºæ¼æ³¨æåçç¶²è·¯ï¼åè¨­åºæ¼æ³¨æåçç¶²è·¯å¯ä»¥æ´å¥½å°ææå¨å±æ¨¡å¼ï¼ä¸¦è§£æ±ºå è¡æ¶²æå¶ä»çµç¹å¼èµ·çé®æç­ææ°ãåºæºæ¸¬è©¦åæ¬æåç RAMIE æ¸æéåå¬éç CholecSeg8k æ¸æéï¼å¯ä»¥å°å¤ç§åå²ä»»åé²è¡å¨é¢è©ä¼°ãæåçç ç©¶çµæè¡¨æï¼å¨ ADE20kï¼ä¸åç¨æ¼èªç¾©åå²çæ¸æéï¼ä¸é²è¡é è¨ç·´æ¯å¨ ImageNet ä¸é²è¡é è¨ç·´æ´ææãæ­¤å¤ï¼åºæ¼æ³¨æåçæ¨¡ååªæ¼å³çµ±çå·ç©ç¥ç¶ç¶²è·¯ï¼å¶ä¸­ SegNeXt å Mask2Former ç²å¾äºæ´é«ç Dice åæ¸ï¼è Mask2Former å¨å¹³åå°ç¨±è¡¨é¢è·é¢æ¹é¢ä¹è¡¨ç¾åºè²ã

##### **RedStone: Curating General, Code, Math, and QA Data for Large Language Models**
2412.03398v1 by Yaoyao Chang, Lei Cui, Li Dong, Shaohan Huang, Yangyu Huang, Yupan Huang, Scarlett Li, Tengchao Lv, Shuming Ma, Qinzheng Sun, Wenhui Wang, Furu Wei, Ying Xin, Mao Yang, Qiufeng Yin, Xingxing Zhang

Pre-training Large Language Models (LLMs) on high-quality, meticulously
curated datasets is widely recognized as critical for enhancing their
performance and generalization capabilities. This study explores the untapped
potential of Common Crawl as a comprehensive and flexible resource for
pre-training LLMs, addressing both general-purpose language understanding and
specialized domain knowledge. We introduce RedStone, an innovative and scalable
pipeline engineered to extract and process data from Common Crawl, facilitating
the creation of extensive and varied pre-training datasets. Unlike traditional
datasets, which often require expensive curation and domain-specific expertise,
RedStone leverages the breadth of Common Crawl to deliver datasets tailored to
a wide array of domains. In this work, we exemplify its capability by
constructing pre-training datasets across multiple fields, including general
language understanding, code, mathematics, and question-answering tasks. The
flexibility of RedStone allows for easy adaptation to other specialized
domains, significantly lowering the barrier to creating valuable
domain-specific datasets. Our findings demonstrate that Common Crawl, when
harnessed through effective pipelines like RedStone, can serve as a rich,
renewable source of pre-training data, unlocking new avenues for domain
adaptation and knowledge discovery in LLMs. This work also underscores the
importance of innovative data acquisition strategies and highlights the role of
web-scale data as a powerful resource in the continued evolution of LLMs.
RedStone code and data samples will be publicly available at
\url{https://aka.ms/redstone}.

æè¦ï¼<paragraph>å¨é«åè³ªãç¶éä»ç´°æ´ççè³æéä¸é åè¨ç·´å¤§åèªè¨æ¨¡å (LLM) è¢«å»£æ³èªçºå°æ¼æåå¶æè½åæ¦åè½åè³ééè¦ãæ¬ç ç©¶æ¢è¨äº Common Crawl ä½çºä¸åå¨é¢ä¸å½æ§çè³æºï¼å¨é åè¨ç·´ LLM æ¹é¢çæªéç¼æ½åï¼åæè§£æ±ºä¸è¬ç¨éçèªè¨çè§£åå°æ¥­é åç¥è­ãæåä»ç´¹äº RedStoneï¼éæ¯ä¸ååµæ°ä¸å¯æ´åçç®¡éï¼è¨­è¨ç¨æ¼å¾ Common Crawl æ·ååèçè³æï¼ä¿é²å»ºç«å»£æ³ä¸å¤æ¨£çé åè¨ç·´è³æéãèå³çµ±è³æéä¸åï¼å³çµ±è³æééå¸¸éè¦æè²´çæ´çåç¹å®é åçå°æ¥­ç¥è­ï¼RedStone å©ç¨ Common Crawl çå»£åº¦ä¾æä¾é©ååç¨®é åçè³æéãå¨éé å·¥ä½ä¸­ï¼æåééå»ºæ§è·¨å¤åé åçé åè¨ç·´è³æéä¾è­æå¶è½åï¼åæ¬ä¸è¬èªè¨çè§£ãç¨å¼ç¢¼ãæ¸å­¸ååç­ä»»åãRedStone çå½æ§åè¨±è¼é¬é©æå¶ä»å°æ¥­é åï¼å¤§å¹éä½å»ºç«æå¹å¼çç¹å®é åè³æéçéæª»ãæåçç ç©¶çµæè­æï¼ç¶ééå RedStone éæ¨£çææç®¡éå©ç¨ Common Crawl æï¼å®å¯ä»¥ç¨ä½è±å¯ãå¯åççé åè¨ç·´è³æä¾æºï¼çº LLM ä¸­çé åé©æåç¥è­ç¼ç¾éåæ°éå¾ãéé å·¥ä½ä¹å¼·èª¿äºåµæ°è³ææ·åç­ç¥çéè¦æ§ï¼ä¸¦çªé¡¯äºç¶²è·¯è¦æ¨¡è³æå¨ LLM æçºæ¼é²ä¸­ä½çºå¼·å¤§è³æºçè§è²ãRedStone ç¨å¼ç¢¼åè³æç¯ä¾å°å¨ \url{https://aka.ms/redstone} å¬éæä¾ã</paragraph>

##### **Enhancing Supply Chain Visibility with Generative AI: An Exploratory Case Study on Relationship Prediction in Knowledge Graphs**
2412.03390v1 by Ge Zheng, Alexandra Brintrup

A key stumbling block in effective supply chain risk management for companies
and policymakers is a lack of visibility on interdependent supply network
relationships. Relationship prediction, also called link prediction is an
emergent area of supply chain surveillance research that aims to increase the
visibility of supply chains using data-driven techniques. Existing methods have
been successful for predicting relationships but struggle to extract the
context in which these relationships are embedded - such as the products being
supplied or locations they are supplied from. Lack of context prevents
practitioners from distinguishing transactional relations from established
supply chain relations, hindering accurate estimations of risk. In this work,
we develop a new Generative Artificial Intelligence (Gen AI) enhanced machine
learning framework that leverages pre-trained language models as embedding
models combined with machine learning models to predict supply chain
relationships within knowledge graphs. By integrating Generative AI techniques,
our approach captures the nuanced semantic relationships between entities,
thereby improving supply chain visibility and facilitating more precise risk
management. Using data from a real case study, we show that GenAI-enhanced link
prediction surpasses all benchmarks, and demonstrate how GenAI models can be
explored and effectively used in supply chain risk management.

æè¦ï¼ä¾æéé¢¨éªç®¡çä¸­çä¸åééµéç¤å¨æ¼ä¼æ¥­åæ¿ç­å¶å®èç¼ºä¹å°ç¸äºä¾å­ä¾æç¶²è·¯éä¿çè½è¦åº¦ãéä¿é æ¸¬ï¼ä¹ç¨±çºé£çµé æ¸¬ï¼æ¯ä¾æéç£æ§ç ç©¶ä¸­ä¸åæ°èé åï¼æ¨å¨ä½¿ç¨è³æé©åæè¡æé«ä¾æéçè½è¦åº¦ãç¾ææ¹æ³å·²æåé æ¸¬éä¿ï¼ä½é£ä»¥æåéäºéä¿æåµå¥çèæ¯ï¼ä¾å¦æä¾æçç¢åæä¾æå°é»ãç¼ºä¹èæ¯æå¦¨ç¤å¾æ¥­èååäº¤æéä¿åæ¢å®çä¾æééä¿ï¼é²èé»ç¤é¢¨éªçæºç¢ºè©ä¼°ãå¨éé å·¥ä½ä¸­ï¼æåéç¼äºä¸åæ°ççæå¼äººå·¥æºæ§ (Gen AI) å¢å¼·æ©å¨å­¸ç¿æ¶æ§ï¼å®å©ç¨é åè¨ç·´çèªè¨æ¨¡åä½çºåµå¥æ¨¡åï¼ä¸¦çµåæ©å¨å­¸ç¿æ¨¡åä¾é æ¸¬ç¥è­åè­ä¸­çä¾æééä¿ãééæ´åçæå¼ AI æè¡ï¼æåçåæ³ææå°å¯¦é«ä¹éç´°å¾®çèªç¾©éä¿ï¼å¾èæé«ä¾æéè½è¦åº¦ä¸¦ä¿é²æ´ç²¾ç¢ºçé¢¨éªç®¡çãä½¿ç¨ä¾èªçå¯¦æ¡ä¾ç ç©¶çè³æï¼æåè­æ GenAI å¢å¼·é£çµé æ¸¬åªæ¼ææåºæºï¼ä¸¦å±ç¤ºå¦ä½æ¢ç´¢åææå°å¨ä¾æéé¢¨éªç®¡çä¸­ä½¿ç¨ GenAI æ¨¡åã

##### **DiffStyleTTS: Diffusion-based Hierarchical Prosody Modeling for Text-to-Speech with Diverse and Controllable Styles**
2412.03388v1 by Jiaxuan Liu, Zhaoci Liu, Yajun Hu, Yingying Gao, Shilei Zhang, Zhenhua Ling

Human speech exhibits rich and flexible prosodic variations. To address the
one-to-many mapping problem from text to prosody in a reasonable and flexible
manner, we propose DiffStyleTTS, a multi-speaker acoustic model based on a
conditional diffusion module and an improved classifier-free guidance, which
hierarchically models speech prosodic features, and controls different prosodic
styles to guide prosody prediction. Experiments show that our method
outperforms all baselines in naturalness and achieves superior synthesis speed
compared to three diffusion-based baselines. Additionally, by adjusting the
guiding scale, DiffStyleTTS effectively controls the guidance intensity of the
synthetic prosody.

æè¦ï¼äººé¡çèªé³å±ç¾åºè±å¯ä¸éæ´»çé»å¾è®åãçºäºä»¥åçä¸éæ´»çæ¹å¼è§£æ±ºæå­å°é»å¾çä¸å°å¤å°æåé¡ï¼æåæåº DiffStyleTTSï¼éæ¯ä¸ååºæ¼æ¢ä»¶æ´æ£æ¨¡çµåæ¹é²çç¡åé¡å¨å¼å°çå¤éèªªè©±èè²å­¸æ¨¡åï¼å®ä»¥éå±¤æ¹å¼å»ºæ§èªé³é»å¾ç¹å¾µï¼ä¸¦æ§å¶ä¸åçé»å¾é¢¨æ ¼ä»¥å¼å°é»å¾é æ¸¬ãå¯¦é©é¡¯ç¤ºï¼èèªç¶åº¦ä¸­çææåºæºç¸æ¯ï¼æåçæ¨¡åè¡¨ç¾åªç°ï¼ä¸¦ä¸èä¸ååºæ¼æ´æ£çåºæºç¸æ¯ï¼éå°äºæ´ä½³çåæéåº¦ãæ­¤å¤ï¼ééèª¿æ´å¼å°æ¯ä¾ï¼DiffStyleTTS è½ææå°æ§å¶åæé»å¾çå¼å°å¼·åº¦ã

##### **WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems Through Game-Based Analysis**
2412.03359v1 by Chengwei Hu, Jianhui Zheng, Yancheng He, Hangyu Guo, Junguang Jiang, Han Zhu, Kai Sun, Yuning Jiang, Wenbo Su, Bo Zheng

Recent advancements in autonomous multi-agent systems (MAS) based on large
language models (LLMs) have enhanced the application scenarios and improved the
capability of LLMs to handle complex tasks. Despite demonstrating
effectiveness, existing studies still evidently struggle to evaluate, analysis,
and reproducibility of LLM-based MAS. In this paper, to facilitate the research
on LLM-based MAS, we introduce an open, scalable, and real-time updated
platform for accessing and analyzing the LLM-based MAS based on the games Who
is Spy?" (WiS). Our platform is featured with three main worths: (1) a unified
model evaluate interface that supports models available on Hugging Face; (2)
real-time updated leaderboard for model evaluation; (3) a comprehensive
evaluation covering game-winning rates, attacking, defense strategies, and
reasoning of LLMs. To rigorously test WiS, we conduct extensive experiments
coverage of various open- and closed-source LLMs, we find that different agents
exhibit distinct and intriguing behaviors in the game. The experimental results
demonstrate the effectiveness and efficiency of our platform in evaluating
LLM-based MAS. Our platform and its documentation are publicly available at
\url{https://whoisspy.ai/}

æè¦ï¼<paragraph>åºæ¼å¤§åèªè¨æ¨¡å (LLM) çèªä¸»å¤æºè½é«ç³»çµ± (MAS) è¿æçé²å±ï¼æ´å±äºæç¨å ´æ¯ï¼ä¸¦æåäº LLM èçè¤éä»»åçè½åãåç®¡è­æäºå¶æææ§ï¼ä½ç¾æç ç©¶ä»æé¡¯é£ä»¥è©ä¼°ãåæåéç¾åºæ¼ LLM ç MASãå¨æ¬æä¸­ï¼çºäºä¿é²å°åºæ¼ LLM ç MAS çç ç©¶ï¼æåä»ç´¹äºä¸åéæ¾ãå¯æ´åä¸å³ææ´æ°çå¹³å°ï¼ç¨æ¼å­åååæåºæ¼éæ²ãèª°æ¯éè«ï¼ã(WiS) çåºæ¼ LLM ç MASãæåçå¹³å°å·æä¸åä¸»è¦å¹å¼ï¼(1) çµ±ä¸çæ¨¡åè©ä¼°ä»é¢ï¼æ¯æ´ Hugging Face ä¸å¯ç¨çæ¨¡åï¼(2) å³ææ´æ°çæ¨¡åè©ä¼°æè¡æ¦ï¼(3) å¨é¢çè©ä¼°ï¼æ¶µèéæ²ç²åçãæ»æãé²ç¦¦ç­ç¥å LLM çæ¨çãçºäºå´æ ¼æ¸¬è©¦ WiSï¼æåå°åç¨®éæºåéæº LLM é²è¡å»£æ³çå¯¦é©ï¼æåç¼ç¾ä¸åçæºè½é«å¨éæ²ä¸­è¡¨ç¾åºæªç¶ä¸åä¸æè¶£çè¡çºãå¯¦é©çµæè­æäºæåçå¹³å°å¨è©ä¼°åºæ¼ LLM ç MAS æçæææ§åæçãæåçå¹³å°åå¶æä»¶å¨ \url{https://whoisspy.ai/} å¬éæä¾</paragraph>

##### **Intuitive Axial Augmentation Using Polar-Sine-Based Piecewise Distortion for Medical Slice-Wise Segmentation**
2412.03352v1 by Yiqin Zhang, Qingkui Chen, Chen Huang, Zhengjie Zhang, Meiling Chen, Zhibing Fu

Most data-driven models for medical image analysis rely on universal
augmentations to improve performance. Experimental evidence has confirmed their
effectiveness, but the unclear mechanism underlying them poses a barrier to the
widespread acceptance and trust in such methods within the medical community.
We revisit and acknowledge the unique characteristics of medical images apart
from traditional digital images, and consequently, proposed a medical-specific
augmentation algorithm that is more elastic and aligns well with radiology scan
procedure. The method performs piecewise affine with sinusoidal distorted ray
according to radius on polar coordinates, thus simulating uncertain postures of
human lying flat on the scanning table. Our method could generate human
visceral distribution without affecting the fundamental relative position on
axial plane. Two non-adaptive algorithms, namely Meta-based Scan Table Removal
and Similarity-Guided Parameter Search, are introduced to bolster robustness of
our augmentation method. Experiments show our method improves accuracy across
multiple famous segmentation frameworks without requiring more data samples.
Our preview code is available in: https://github.com/MGAMZ/PSBPD.

æè¦ï¼å¤§å¤æ¸ç¨æ¼é«å­¸å½±ååæçè³æé©åæ¨¡åä»°è³´éç¨æ´ååè½ä¾æåæè½ãå¯¦é©è­æå·²è­å¯¦å¶æææ§ï¼ä½å¶èå¾ä¸æç¢ºçæ©å¶å°é«å­¸çå»£æ³æ¥ååä¿¡ä»»æ­¤é¡æ¹æ³æ§æé»ç¤ãæåéæ°æª¢è¦ä¸¦æ¿èªé«å­¸å½±åèå³çµ±æ¸ä½å½±åçç¨ç¹ç¹æ§ï¼å æ­¤æåºæ´å·å½æ§ä¸èæ¾å°ç·ææç¨åºå¯åéåçé«å­¸ç¹å®æ´åæ¼ç®æ³ãè©²æ¹æ³æ ¹ææ¥µåº§æ¨ä¸çåå¾å·è¡æ­£å¼¦æ­æ²å°ç·çéæ®µä»¿å°ï¼å¾èæ¨¡æ¬äººå¹³èººå¨ææå°ä¸æçä¸ç¢ºå®å§¿å¢ãæåçæ¹æ³å¯ä»¥å¨ä¸å½±é¿è»¸åå¹³é¢ä¸åºæ¬ç¸å°ä½ç½®çææ³ä¸çæäººé«å§èåä½ãå¼å¥äºå©ç¨®éèªé©ææ¼ç®æ³ï¼å³åºæ¼ Meta çææå°ç§»é¤åç¸ä¼¼æ§å°å¼åæ¸æå°ï¼ä»¥å å¼·æåæ´åæ¹æ³çç©©å¥æ§ãå¯¦é©è¡¨æï¼æåçæ¼ç®æ³å¨ä¸éè¦æ´å¤è³ææ¨£æ¬çææ³ä¸ï¼å°±è½æåå¤åèååå²æ¶æ§çæºç¢ºæ§ãæåçé è¦½ç¨å¼ç¢¼å¯å¨ https://github.com/MGAMZ/PSBPD ä¸­åå¾ã

##### **DIVE: Taming DINO for Subject-Driven Video Editing**
2412.03347v1 by Yi Huang, Wei Xiong, He Zhang, Chaoqi Chen, Jianzhuang Liu, Mingfu Yan, Shifeng Chen

Building on the success of diffusion models in image generation and editing,
video editing has recently gained substantial attention. However, maintaining
temporal consistency and motion alignment still remains challenging. To address
these issues, this paper proposes DINO-guided Video Editing (DIVE), a framework
designed to facilitate subject-driven editing in source videos conditioned on
either target text prompts or reference images with specific identities. The
core of DIVE lies in leveraging the powerful semantic features extracted from a
pretrained DINOv2 model as implicit correspondences to guide the editing
process. Specifically, to ensure temporal motion consistency, DIVE employs DINO
features to align with the motion trajectory of the source video. Extensive
experiments on diverse real-world videos demonstrate that our framework can
achieve high-quality editing results with robust motion consistency,
highlighting the potential of DINO to contribute to video editing. For precise
subject editing, DIVE incorporates the DINO features of reference images into a
pretrained text-to-image model to learn Low-Rank Adaptations (LoRAs),
effectively registering the target subject's identity. Project page:
https://dino-video-editing.github.io

æè¦ï¼å»ºç«å¨æ´æ£æ¨¡åå¨å½±åçæåç·¨è¼¯ä¸çæåï¼å½±çç·¨è¼¯æè¿ç²å¾äºå¤§éçéæ³¨ãç¶èï¼ç¶­ææéä¸è´æ§ååä½å°é½ä»ç¶å·æææ°æ§ãçºäºè§£æ±ºéäºåé¡ï¼æ¬ææåºäº DINO å¼å°çå½±çç·¨è¼¯ (DIVE)ï¼ä¸åæ¨å¨ä¿é²ä»¥ä¸»é¡çºå°åçç·¨è¼¯ï¼å¨ä¾æºå½±çä¸­ä»¥ç®æ¨æå­æç¤ºæå·æç¹å®èº«åçåèå½±åçºæ¢ä»¶ãDIVE çæ ¸å¿å¨æ¼å©ç¨å¾é è¨ç·´ç DINOv2 æ¨¡åä¸­æåçå¼·å¤§èªç¾©ç¹å¾µï¼ä½çºé±å«å°æä¾å¼å°ç·¨è¼¯éç¨ãå·é«ä¾èªªï¼çºäºç¢ºä¿æéåä½ä¸è´æ§ï¼DIVE ä½¿ç¨ DINO ç¹å¾µèä¾æºå½±ççåä½è»è·¡å°é½ãå¨åç¨®çå¯¦ä¸çå½±çä¸çå¤§éå¯¦é©è¡¨æï¼æåçæ¡æ¶å¯ä»¥å¨å¼·å¤§çåä½ä¸è´æ§ä¸å¯¦ç¾é«åè³ªçç·¨è¼¯çµæï¼çªé¡¯äº DINO å°å½±çç·¨è¼¯çæ½å¨è²¢ç»ãå°æ¼ç²¾ç¢ºçä¸»é¡ç·¨è¼¯ï¼DIVE å°åèå½±åç DINO ç¹å¾µæ´åå°é è¨ç·´çæå­è½å½±åæ¨¡åä¸­ï¼ä»¥å­¸ç¿ä½ç§©é©æ (LoRAs)ï¼ææè¨»åç®æ¨ä¸»é¡çèº«åãå°æ¡é é¢ï¼https://dino-video-editing.github.io

##### **Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning**
2412.03343v1 by Long Mai, Julie Carson-Berndsen

While Large Language Models (LLMs) have made significant strides in
replicating human-like abilities, there are concerns about a reduction in the
linguistic diversity of their outputs. This results in the homogenization of
viewpoints and perspectives, as well as the underrepresentation of specific
demographic groups. Although several fine-tuning and prompting techniques have
been suggested to tackle the issue, they are often tailored to specific tasks
or come with a substantial increase in computational cost and latency. This
makes them challenging to apply to applications that demand very low latency,
such as chatbots and virtual assistants. We propose Possibility Exploration
Fine-Tuning (PEFT), a task-agnostic framework that enhances the text diversity
of LLMs without increasing latency or computational cost. Given the same
prompt, models fine-tuned with PEFT can simultaneously generate multiple
diverse responses, each corresponding with a controllable possibility number.
Experiments on dialogue and story generation tasks demonstrate that PEFT
significantly enhances the diversity of LLM outputs, as evidenced by lower
similarity between candidate responses. Since PEFT emphasizes semantic
diversity over lexical diversity, it can also notably reduce demographic bias
in dialogue systems. The implementations and datasets are available in our
repository: https://github.com/mailong25/peft_diversity

æè¦ï¼éç¶å¤§åèªè¨æ¨¡å (LLM) å·²å¨è¤è£½é¡ä¼¼äººé¡çè½åæ¹é¢åå¾éå¤§é²å±ï¼ä½äººåææå¶è¼¸åºçèªè¨å¤æ¨£æ§æä¸éãéå°è´è§é»åè§å¿µçåè³ªåï¼ä»¥åç¹å®äººå£ç¾¤é«çä»£è¡¨æ§ä¸è¶³ãåç®¡å·²æåºå¹¾ç¨®å¾®èª¿åæç¤ºæè¡ä¾è§£æ±ºæ­¤åé¡ï¼ä½å®åéå¸¸éå°ç¹å®ä»»åé²è¡èª¿æ´ï¼æä¼´é¨èè¨ç®ææ¬åå»¶é²çå¤§å¹å¢å ãéä½¿å¾å®åé£ä»¥æç¨æ¼éè¦éå¸¸ä½å»¶é²çæç¨ç¨å¼ï¼ä¾å¦èå¤©æ©å¨äººåèæ¬å©çãæåæåºå¯è½æ§æ¢ç´¢å¾®èª¿ (PEFT)ï¼éæ¯ä¸åèä»»åç¡éçæ¶æ§ï¼å®å¯ä»¥å¢å¼· LLM çæå­å¤æ¨£æ§ï¼èä¸æå¢å å»¶é²æè¨ç®ææ¬ãå¨çµ¦å®ç¸åçæç¤ºä¸ï¼ä½¿ç¨ PEFT å¾®èª¿çæ¨¡åå¯ä»¥åæç¢çå¤ç¨®ä¸åçåæï¼æ¯ååæé½å°æä¸åå¯æ§çå¯è½æ§æ¸å­ãå°è©±åæäºçæä»»åçå¯¦é©è¡¨æï¼PEFT å¯ä»¥é¡¯èæé« LLM è¼¸åºçå¤æ¨£æ§ï¼åé¸åæä¹éçç¸ä¼¼æ§è¼ä½ï¼å³å¯è­æéä¸é»ãç±æ¼ PEFT å¼·èª¿èªç¾©å¤æ¨£æ§èéè©å½å¤æ¨£æ§ï¼å æ­¤å®ä¹å¯ä»¥é¡¯èæ¸å°å°è©±ç³»çµ±ä¸­çäººå£çµ±è¨åè¦ãå¯¦ä½åè³æéå¯å¨æåçå²å­åº«ä¸­åå¾ï¼https://github.com/mailong25/peft_diversity

##### **AI-Driven Day-to-Day Route Choice**
2412.03338v1 by Leizhen Wang, Peibo Duan, Zhengbing He, Cheng Lyu, Xin Chen, Nan Zheng, Li Yao, Zhenliang Ma

Understanding travelers' route choices can help policymakers devise optimal
operational and planning strategies for both normal and abnormal circumstances.
However, existing choice modeling methods often rely on predefined assumptions
and struggle to capture the dynamic and adaptive nature of travel behavior.
Recently, Large Language Models (LLMs) have emerged as a promising alternative,
demonstrating remarkable ability to replicate human-like behaviors across
various fields. Despite this potential, their capacity to accurately simulate
human route choice behavior in transportation contexts remains doubtful. To
satisfy this curiosity, this paper investigates the potential of LLMs for route
choice modeling by introducing an LLM-empowered agent, "LLMTraveler." This
agent integrates an LLM as its core, equipped with a memory system that learns
from past experiences and makes decisions by balancing retrieved data and
personality traits. The study systematically evaluates the LLMTraveler's
ability to replicate human-like decision-making through two stages: (1)
analyzing its route-switching behavior in single origin-destination (OD) pair
congestion game scenarios, where it demonstrates patterns align with laboratory
data but are not fully explained by traditional models, and (2) testing its
capacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar
and Willumsen (OW) network, producing results comparable to Multinomial Logit
(MNL) and Reinforcement Learning (RL) models. These experiments demonstrate
that the framework can partially replicate human-like decision-making in route
choice while providing natural language explanations for its decisions. This
capability offers valuable insights for transportation policymaking, such as
simulating traveler responses to new policies or changes in the network.

æè¦ï¼<paragraph>äºè§£æå®¢çè·¯ç·é¸ææå©æ¼æ¿ç­å¶å®èçºæ­£å¸¸åç°å¸¸ææ³å¶å®æä½³çéåè¦åç­ç¥ãç¶èï¼ç¾æçé¸ææ¨¡åæ¹æ³éå¸¸ä¾è³´æ¼é å®ç¾©çåè¨­ï¼ä¸¦ä¸é£ä»¥æææéè¡çºçåæåé©ææ§ãæè¿ï¼å¤§åèªè¨æ¨¡å (LLM) å·²æçºä¸ç¨®æå¸æçæ¿ä»£æ¹æ¡ï¼å±ç¤ºäºå¨åç¨®é åè¤è£½é¡äººè¡çºçéå¡è½åãåç®¡æéç¨®æ½åï¼å®åå¨éè¼¸ç°å¢ä¸­æºç¢ºæ¨¡æ¬äººé¡è·¯ç·é¸æè¡çºçè½åä»ç¶ä»¤äººæ·çãçºäºæ»¿è¶³éç¨®å¥½å¥å¿ï¼æ¬æééå¼å¥ LLM è³¦è½çä»£çãLLMTravelerãä¾æ¢è¨ LLM å¨è·¯ç·é¸æå»ºæ¨¡ä¸­çæ½åãæ­¤ä»£çæ´å LLM ä½çºå¶æ ¸å¿ï¼éåä¸åè¨æ¶ç³»çµ±ï¼å¾éå»çç¶é©ä¸­å­¸ç¿ï¼ä¸¦ééå¹³è¡¡æª¢ç´¢çæ¸æåäººæ ¼ç¹è³ªä¾ååºæ±ºç­ãè©²ç ç©¶ç³»çµ±å°è©ä¼°äº LLMTraveler ééå©åéæ®µè¤è£½é¡äººæ±ºç­çè½åï¼(1) åæå¶å¨å®ä¸èµ·é»-ç®çå° (OD) å°æå¡åå¼å ´æ¯ä¸­çè·¯ç·åæè¡çºï¼å®å±ç¤ºçæ¨¡å¼èå¯¦é©å®¤æ¸æä¸è´ï¼ä½ç¡æ³å®å¨ç±å³çµ±æ¨¡åè§£éï¼ä»¥å (2) æ¸¬è©¦å¶å¨ Ortuzar å Willumsen (OW) ç¶²è·¯ä¸å°æ¥å¾©ä¸æ¥ (DTD) é©ææ§å­¸ç¿è¡çºé²è¡å»ºæ¨¡çè½åï¼ç¢çççµæèå¤é å¼ Logit (MNL) åå¼·åå­¸ç¿ (RL) æ¨¡åç¸ç¶ãéäºå¯¦é©è¡¨æï¼è©²æ¡æ¶å¯ä»¥é¨åè¤è£½é¡äººè·¯ç·é¸æä¸­çæ±ºç­ï¼åæçºå¶æ±ºç­æä¾èªç¶èªè¨è§£éãéç¨®è½åçºäº¤éæ¿ç­å¶å®æä¾äºæå¹å¼çè¦è§£ï¼ä¾å¦æ¨¡æ¬æå®¢å°æ°æ¿ç­æç¶²è·¯è®åçåæã</paragraph>

##### **Yankari: A Monolingual Yoruba Dataset**
2412.03334v1 by Maro Akpobi

This paper presents Yankari, a large-scale monolingual dataset for the Yoruba
language, aimed at addressing the critical gap in Natural Language Processing
(NLP) resources for this important West African language. Despite being spoken
by over 30 million people, Yoruba has been severely underrepresented in NLP
research and applications. We detail our methodology for creating this dataset,
which includes careful source selection, automated quality control, and
rigorous data cleaning processes. The Yankari dataset comprises 51,407
documents from 13 diverse sources, totaling over 30 million tokens. Our
approach focuses on ethical data collection practices, avoiding problematic
sources and addressing issues prevalent in existing datasets. We provide
thorough automated evaluations of the dataset, demonstrating its quality
compared to existing resources. The Yankari dataset represents a significant
advancement in Yoruba language resources, providing a foundation for developing
more accurate NLP models, supporting comparative linguistic studies, and
contributing to the digital accessibility of the Yoruba language.

æè¦ï¼éç¯è«ææåºäº Yankariï¼ä¸åéå°ç´é­¯å·´èªçå¤§åå®èªè³æéï¼æ¨å¨è§£æ±ºéåéè¦çè¥¿éèªè¨å¨èªç¶èªè¨èç (NLP) è³æºä¸­çééµå·®è·ãåç®¡è¶é 3000 è¬äººä½¿ç¨ç´é­¯å·´èªï¼ä½å®å¨ NLP ç ç©¶åæç¨ä¸­å´éä¸è¶³ãæåè©³ç´°èªªæäºå»ºç«éåè³æéçæ¹æ³ï¼å¶ä¸­åæ¬ä»ç´°çä¾æºé¸æãèªååçåè³ªæ§ç®¡åå´è¬¹çè³ææ¸çç¨åºãYankari è³æéåå«ä¾èª 13 åä¸åä¾æºç 51,407 ä»½æä»¶ï¼ç¸½è¨è¶é 3000 è¬åè©å½ãæåçåæ³èéæ¼ç¬¦åéå¾·çè³ææ¶éå¯¦åï¼é¿åæåé¡çä¾æºä¸¦è§£æ±ºç¾æè³æéä¸­æ®éå­å¨çåé¡ãæåæä¾è³æéçå¾¹åºèªååè©ä¼°ï¼è­æå¶åè³ªåªæ¼ç¾æè³æºãYankari è³æéä»£è¡¨ç´é­¯å·´èªè³æºçéå¤§é²å±ï¼çºéç¼æ´ç²¾ç¢ºç NLP æ¨¡åãæ¯æ´æ¯è¼èªè¨å­¸ç ç©¶åä¿é²ç´é­¯å·´èªçæ¸ä½å¯åæ§å¥ å®åºç¤ã

##### **LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings**
2412.03331v1 by Fred Philippy, Siwen Guo, Jacques Klein, TegawendÃ© F. BissyandÃ©

Sentence embedding models play a key role in various Natural Language
Processing tasks, such as in Topic Modeling, Document Clustering and
Recommendation Systems. However, these models rely heavily on parallel data,
which can be scarce for many low-resource languages, including Luxembourgish.
This scarcity results in suboptimal performance of monolingual and
cross-lingual sentence embedding models for these languages. To address this
issue, we compile a relatively small but high-quality human-generated
cross-lingual parallel dataset to train \tool, an enhanced sentence embedding
model for Luxembourgish with strong cross-lingual capabilities. Additionally,
we present evidence suggesting that including low-resource languages in
parallel training datasets can be more advantageous for other low-resource
languages than relying solely on high-resource language pairs. Furthermore,
recognizing the lack of sentence embedding benchmarks for low-resource
languages, we create a paraphrase detection benchmark specifically for
Luxembourgish, aiming to partially fill this gap and promote further research.

æè¦ï¼å¥å­åµå¥æ¨¡åå¨åç¨®èªç¶èªè¨èçä»»åä¸­æ®æ¼èééµè§è²ï¼ä¾å¦ä¸»é¡å»ºæ¨¡ãæä»¶åç¾¤åæ¨è¦ç³»çµ±ãç¶èï¼éäºæ¨¡åé«åº¦ä¾è³´å¹³è¡è³æï¼èéå°æ¼è¨±å¤ä½è³æºèªè¨ä¾èªªå¯è½å¾ç¨å°ï¼åæ¬ç§æ£®å ¡èªãéç¨®ç¨å°æ§å°è´äºéäºèªè¨çå®èªåè·¨èªè¨å¥å­åµå¥æ¨¡åçæ¬¡åªæ§è½ãçºäºè§£æ±ºéååé¡ï¼æåç·¨å¶äºä¸åç¸å°è¼å°ä½é«åè³ªçäººé¡çæçè·¨èªè¨å¹³è¡è³æéï¼ä»¥è¨ç·´ \toolï¼ä¸åå·æå¼·å¤§è·¨èªè¨è½åçç§æ£®å ¡èªå¢å¼·å¥å­åµå¥æ¨¡åãæ­¤å¤ï¼æåæåºè­æè¡¨æï¼å¨å¹³è¡è¨ç·´è³æéä¸­åå«ä½è³æºèªè¨å°å¶ä»ä½è³æºèªè¨ä¾èªªå¯è½æ¯åä¾è³´æ¼é«è³æºèªè¨å°æ´æå©ãæ­¤å¤ï¼èªè­å°ä½è³æºèªè¨ç¼ºä¹å¥å­åµå¥åºæºï¼æåå°éçºç§æ£®å ¡èªåµå»ºäºä¸ååç¾©è©åµæ¸¬åºæºï¼æ¨å¨é¨åå¡«è£éä¸ç©ºç½ä¸¦ä¿é²é²ä¸æ­¥çç ç©¶ã

##### **Grounded Language Design for Lightweight Diagramming for Formal Methods**
2412.03310v1 by Siddhartha Prasad, Ben Greenman, Tim Nelson, Shriram Krishnamurthi

Model finding, as embodied by SAT solvers and similar tools, is used widely,
both in embedding settings and as a tool in its own right. For instance, tools
like Alloy target SAT to enable users to incrementally define, explore, verify,
and diagnose sophisticated specifications for a large number of complex
systems.
  These tools critically include a visualizer that lets users graphically
explore these generated models. As we show, however, default visualizers, which
know nothing about the domain, are unhelpful and even actively violate
presentational and cognitive principles. At the other extreme, full-blown
visualizations require significant effort as well as knowledge a specifier
might not possess; they can also exhibit bad failure modes (including silent
failure). Instead, we need a language to capture essential domain information
for lightweight diagramming. We ground our language design in both the
cognitive science literature on diagrams and on a large number of example
custom visualizations. This identifies the key elements of lightweight
diagrams. We distill these into a small set of orthogonal primitives. We extend
an Alloy-like tool to support these primitives. We evaluate the effectiveness
of the produced diagrams, finding them good for reasoning. We then compare this
against many other drawing languages and tools to show that this work defines a
new niche that is lightweight, effective, and driven by sound principles.

æè¦ï¼æ¨¡åå°æ¾ï¼ä¾å¦ SAT è§£æ±ºå¨åé¡ä¼¼å·¥å·ï¼è¢«å»£æ³ä½¿ç¨ï¼
æ¢ç¨æ¼åµå¥å¼è¨­å®ï¼ä¹ä½çºä¸ç¨®ç¨ç«çå·¥å·ãä¾å¦ï¼
Alloy ç­å·¥å·ä»¥ SAT çºç®æ¨ï¼ä½¿ç¨æ¶è½å¤ éæ­¥å®ç¾©ãæ¢ç´¢ãé©è­ï¼
ä¸¦è¨ºæ·å¤§éè¤éç³»çµ±çé«ç´è¦æ ¼ã
éäºå·¥å·è³ééè¦çæ¯åæ¬ä¸åå¯è¦åå¨ï¼è®ç¨æ¶ä»¥åå½¢æ¹å¼
æ¢ç´¢éäºçæçæ¨¡åãç¶èï¼æåå±ç¤ºäºé»èªçå¯è¦åå¨ï¼
å°é åä¸ç¡æç¥ï¼æ²æå¹«å©ï¼çè³ä¸»åéå
å±ç¤ºåèªç¥ååãå¨å¦ä¸åæ¥µç«¯ï¼å¨é¢ç
å¯è¦åéè¦å¤§éçç²¾åä»¥åèªªæèå¯è½ä¸å·åçç¥è­ï¼å®åä¹å¯ä»¥è¡¨ç¾åºä¸è¯çæéæ¨¡å¼ï¼åæ¬éé»
æéï¼ãç¸åï¼æåéè¦ä¸ç¨®èªè¨ä¾ææåºæ¬é åä¿¡æ¯
ç¨æ¼è¼éç´åè¡¨ãæåå°æåçèªè¨è¨­è¨å»ºç«å¨
éæ¼åè¡¨çèªç¥ç§å­¸æç»åå¤§éç¤ºä¾
èªå®ç¾©å¯è¦åãéç¢ºå®äºè¼éç´
åè¡¨çééµåç´ ãæåå°éäºæçæä¸çµå°çæ­£äº¤åºåãæåæ´å±
ä¸åé¡ä¼¼ Alloy çå·¥å·ä¾æ¯æéäºåºåãæåè©ä¼°ç¢ççåè¡¨çæææ§ï¼ç¼ç¾å®åå°æ¨çæçãç¶å¾æåæ¯è¼é
èè¨±å¤å¶ä»ç¹ªåèªè¨åå·¥å·ï¼ä»¥è¡¨æéé å·¥ä½å®ç¾©äºä¸å
æ°çå©åºï¼å®è¼éãææä¸ç±åççååé©åã

##### **Contextual Data Integration for Bike-sharing Demand Prediction with Graph Neural Networks in Degraded Weather Conditions**
2412.03307v1 by Romain Rochas, Angelo Furno, Nour-Eddin El Faouzi

Demand for bike sharing is impacted by various factors, such as weather
conditions, events, and the availability of other transportation modes. This
impact remains elusive due to the complex interdependence of these factors or
locationrelated user behavior variations. It is also not clear which factor is
additional information which are not already contained in the historical
demand. Intermodal dependencies between bike-sharing and other modes are also
underexplored, and the value of this information has not been studied in
degraded situations. The proposed study analyzes the impact of adding
contextual data, such as weather, time embedding, and road traffic flow, to
predict bike-sharing Origin-Destination (OD) flows in atypical weather
situations Our study highlights a mild relationship between prediction quality
of bike-sharing demand and road traffic flow, while the introduced time
embedding allows outperforming state-of-the-art results, particularly in the
case of degraded weather conditions. Including weather data as an additional
input further improves our model with respect to the basic ST-ED-RMGC
prediction model by reducing of more than 20% the prediction error in degraded
weather condition.

æè¦ï¼èªè¡è»å±äº«çéæ±æåå°åç¨®å ç´ çå½±é¿ï¼ä¾å¦å¤©æ°£çæ³ãæ´»ååå¶å®äº¤éæ¹å¼çå¯ç¨æ§ãç±æ¼éäºå ç´ çè¤éç¸äºä¾è³´æ§æèä½ç½®ç¸éçä½¿ç¨èè¡çºè®åï¼éç¨®å½±é¿ä»ç¶é£ä»¥ææ¸ãç®åä¹ä¸æ¸æ¥åªåå ç´ æ¯æ­·å²éæ±ä¸­å°æªåå«çéå è³è¨ãèªè¡è»å±äº«èå¶å®æ¹å¼ä¹éçè·¨æ¨¡å¼ä¾è³´æ§ä¹å°æªååæ¢è¨ï¼èä¸å¨æ¡å£ææ³ä¸ï¼æ­¤è³è¨çå¹å¼å°æªç²å¾ç ç©¶ãææåºçç ç©¶åæå å¥èæ¯è³æï¼ä¾å¦å¤©æ°£ãæéåµå¥åéè·¯äº¤éæµéï¼å°é æ¸¬éå¸åå¤©æ°£çæ³ä¸çèªè¡è»å±äº«èµ·è¿é» (OD) æµéçå½±é¿ãæåçç ç©¶å¼·èª¿èªè¡è»å±äº«éæ±é æ¸¬åè³ªèéè·¯äº¤éæµéä¹éçè¼å¾®éä¿ï¼èæå¼å¥çæéåµå¥åè¨±è¶è¶æåé²ççµæï¼ç¹å¥æ¯å¨æ¡å£å¤©æ°£æ¢ä»¶çææ³ä¸ãå°å¤©æ°£è³æä½çºéå è¼¸å¥ï¼é²ä¸æ­¥æ¹åæåçæ¨¡åï¼ç¸è¼æ¼åºæ¬ç ST-ED-RMGC é æ¸¬æ¨¡åï¼å¨æ¡å£å¤©æ°£æ¢ä»¶ä¸å°é æ¸¬èª¤å·®éä½è¶é 20%ã

##### **Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation**
2412.03304v1 by Shivalika Singh, Angelika Romanou, ClÃ©mentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, Sara Hooker

Cultural biases in multilingual datasets pose significant challenges for
their effectiveness as global benchmarks. These biases stem not only from
language but also from the cultural knowledge required to interpret questions,
reducing the practical utility of translated datasets like MMLU. Furthermore,
translation often introduces artifacts that can distort the meaning or clarity
of questions in the target language. A common practice in multilingual
evaluation is to rely on machine-translated evaluation sets, but simply
translating a dataset is insufficient to address these challenges. In this
work, we trace the impact of both of these issues on multilingual evaluations
and ensuing model performances. Our large-scale evaluation of state-of-the-art
open and proprietary models illustrates that progress on MMLU depends heavily
on learning Western-centric concepts, with 28% of all questions requiring
culturally sensitive knowledge. Moreover, for questions requiring geographic
knowledge, an astounding 84.9% focus on either North American or European
regions. Rankings of model evaluations change depending on whether they are
evaluated on the full portion or the subset of questions annotated as
culturally sensitive, showing the distortion to model rankings when blindly
relying on translated MMLU. We release Global-MMLU, an improved MMLU with
evaluation coverage across 42 languages -- with improved overall quality by
engaging with compensated professional and community annotators to verify
translation quality while also rigorously evaluating cultural biases present in
the original dataset. This comprehensive Global-MMLU set also includes
designated subsets labeled as culturally sensitive and culturally agnostic to
allow for more holistic, complete evaluation.

æè¦ï¼å¤èªç³»è³æéä¸­çæååè¦å°å¶ä½çºå¨çåºæºçæææ§æ§æéå¤§ææ°ãéäºåè¦ä¸åæºèªèªè¨ï¼ä¹æºèªè©®éåé¡æéçæåç¥è­ï¼éä½äº MMLU ç­ç¿»è­¯è³æéçå¯¦éæç¨ãæ­¤å¤ï¼ç¿»è­¯éå¸¸æå¼å¥äººå·¥è£½åï¼å¯è½æ­æ²ç®æ¨èªè¨ä¸­åé¡çæç¾©ææ¸æ°åº¦ãå¤èªç³»è©ä¼°ä¸­çä¸ç¨®å¸¸è¦åæ³æ¯ä¾è³´æ©å¨ç¿»è­¯çè©ä¼°éï¼ä½åç¿»è­¯è³æéä¸è¶³ä»¥æå°éäºææ°ãå¨éé å·¥ä½ä¸­ï¼æåè¿½è¹¤äºéå©ååé¡å°å¤èªç³»è©ä¼°åå¾çºæ¨¡åæè½çå½±é¿ãæåå°æåé²çéæ¾åå°ææ¨¡åé²è¡å¤§è¦æ¨¡è©ä¼°ï¼èªªæ MMLU çé²å±å¨å¾å¤§ç¨åº¦ä¸åæ±ºæ¼å­¸ç¿ä»¥è¥¿æ¹çºä¸­å¿çè§å¿µï¼å¶ä¸­ 28% çåé¡éè¦æåææçç¥è­ãæ­¤å¤ï¼å°æ¼éè¦å°çç¥è­çåé¡ï¼é©äººç 84.9% å°æ³¨æ¼åç¾ææ­æ´²å°åãæ¨¡åè©ä¼°çæåææ ¹æå®åæ¯å¨å¨é¨é¨åéæ¯æ¨è¨»çºæåææçå­éåé¡ä¸é²è¡è©ä¼°èæ¹è®ï¼é¡¯ç¤ºåºå¨ç²ç®ä¾è³´ç¿»è­¯ç MMLU æå°æ¨¡åæåçæ­æ²ãæåç¼å¸äº Global-MMLUï¼éæ¯ä¸åæ¹é²ç MMLUï¼å¶è©ä¼°æ¶µè 42 ç¨®èªè¨ââééèç²å¾å ±é¬çå°æ¥­åç¤¾ç¾¤è¨»è§£å¡åä½ï¼é©è­ç¿»è­¯åè³ªï¼åæå´æ ¼è©ä¼°åå§è³æéä¸­å­å¨çæååè¦ï¼å¾èæé«æ´é«åè³ªãéåå¨é¢ç Global-MMLU ééåæ¬æ¨ç¤ºçºæåææåæåä¸å¯ç¥è«çæå®å­éï¼ä»¥é²è¡æ´å¨é¢ãå®æ´çè©ä¼°ã

##### **Integrating Generative AI into Art Therapy: A Technical Showcase**
2412.03287v1 by Yannis Valentin Schmutz, Tetiana Kravchenko, Souhir Ben Souissi, Mascha Kurpicz-Briki

This paper explores the integration of generative AI into the field of art
therapy. Leveraging proven text-to-image models, we introduce a novel technical
design to complement art therapy. The resulting AI-based tools shall enable
patients to refine and customize their creative work, opening up new avenues of
expression and accessibility. Using three illustrative examples, we demonstrate
potential outputs of our solution and evaluate them qualitatively. Furthermore,
we discuss the current limitations and ethical considerations associated with
this integration and provide an outlook into future research efforts. Our
implementations are publicly available at https://github.com/BFH-AMI/sds24.

æè¦ï¼æ¬è«ææ¢è¨çæå¼ AI èèè¡æ²»çé åçæ´åãå©ç¨ç¶éé©è­çæå­è½ååæ¨¡åï¼æåå¼é²äºä¸ç¨®æ°ç©çæè¡è¨­è¨ï¼ä½çºèè¡æ²»ççè£åãæç¢ççåºæ¼ AI çå·¥å·å°è½è®æ£èç²¾é²ä¸¦å®¢è£½åä»åçåµä½ï¼éåè¡¨éååå¾ç®¡éçæ°éå¾ãæåä½¿ç¨ä¸åèªªææ§ç¯ä¾ï¼å±ç¤ºæåè§£æ±ºæ¹æ¡çæ½å¨ç¢åºï¼ä¸¦å°å¶é²è¡å®æ§è©ä¼°ãæ­¤å¤ï¼æåè¨è«èæ­¤æ´åç¸éçç¾æéå¶åå«çèéï¼ä¸¦æä¾æªä¾ç ç©¶å·¥ä½çå±æãæåçå¯¦ä½å·²å¬éæ¼ https://github.com/BFH-AMI/sds24ã

##### **Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models**
2412.03283v1 by Andreas MÃ¼ller, Denis Lukovnikov, Jonas Thietke, Asja Fischer, Erwin Quiring

Integrating watermarking into the generation process of latent diffusion
models (LDMs) simplifies detection and attribution of generated content.
Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel
class of watermarking techniques that are easy to implement and highly robust
against various perturbations. However, our work demonstrates a fundamental
security vulnerability of semantic watermarks. We show that attackers can
leverage unrelated models, even with different latent spaces and architectures
(UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically,
we design two watermark forgery attacks. The first imprints a targeted
watermark into real images by manipulating the latent representation of an
arbitrary image in an unrelated LDM to get closer to the latent representation
of a watermarked image. We also show that this technique can be used for
watermark removal. The second attack generates new images with the target
watermark by inverting a watermarked image and re-generating it with an
arbitrary prompt. Both attacks just need a single reference image with the
target watermark. Overall, our findings question the applicability of semantic
watermarks by revealing that attackers can easily forge or remove these
watermarks under realistic conditions.

æè¦ï¼å°æµ®æ°´å°æ´åå°æ½å¨æ´æ£æ¨¡å (LDM) ççæéç¨ä¸­ï¼ç°¡åäºçæå§å®¹çåµæ¸¬åæ­¸å ãèªç¾©æµ®æ°´å°ï¼ä¾å¦æ¨¹çç°åé«æ¯é°å½±ï¼ä»£è¡¨äºä¸ç¨®æ°ç©çæµ®æ°´å°æè¡ï¼ææ¼å¯¦ä½ä¸å°åç¨®æ¾åå·æé«åº¦çç©©å¥æ§ãç¶èï¼æåçç ç©¶å±ç¤ºäºèªç¾©æµ®æ°´å°çåºæ¬å®å¨æ§æ¼æ´ãæåå±ç¤ºæ»æèå¯ä»¥å©ç¨ä¸ç¸éçæ¨¡åï¼å³ä½¿å®åå·æä¸åçæ½å¨ç©ºéåæ¶æ§ (UNet è DiT)ï¼ä¾å·è¡å¼·å¤§ä¸é¼ççå½é æ»æãå·é«ä¾èªªï¼æåè¨­è¨äºå©ç¨®æµ®æ°´å°å½é æ»æãç¬¬ä¸åééå¨ä¸ç¸éç LDM ä¸­æç¸±ä»»æå½±åçæ½å¨è¡¨ç¤ºï¼å°ç®æ¨æµ®æ°´å°å°è¨å°çå¯¦å½±åä¸­ï¼ä»¥æ´æ¥è¿æ°´å°å½±åçæ½å¨è¡¨ç¤ºãæåä¹å±ç¤ºäºæ­¤æè¡å¯è¢«ç¨æ¼æµ®æ°´å°ç§»é¤ãç¬¬äºåæ»æééåè½æ°´å°å½±åä¸¦ä½¿ç¨ä»»ææç¤ºéæ°çæå®ï¼ä¾çæå·æç®æ¨æµ®æ°´å°çæ°å½±åãå©ç¨®æ»æé½åªéè¦ä¸åå·æç®æ¨æµ®æ°´å°çåèå½±åãç¸½çä¾èªªï¼æåçç¼ç¾è³ªçäºèªç¾©æµ®æ°´å°çé©ç¨æ§ï¼æ­ç¤ºäºæ»æèå¯ä»¥å¨ç¾å¯¦æ¢ä»¶ä¸è¼é¬å½é æç§»é¤éäºæµ®æ°´å°ã

##### **AntLM: Bridging Causal and Masked Language Models**
2412.03275v1 by Xinru Yu, Bin Guo, Shiwei Luo, Jie Wang, Tao Ji, Yuanbin Wu

Causal Language Modeling (CLM) and Masked Language Modeling (MLM) are two
mainstream learning paradigms based on Transformer networks, specifically the
Decoder-only and Encoder-only architectures. The strengths of each paradigm in
downstream tasks have shown a mix of advantages and disadvantages. In the past
BabyLM Challenge 2023, although the MLM paradigm achieved the best average
performance, the CLM paradigm demonstrated significantly faster convergence
rates. For the BabyLM Challenge 2024, we propose a novel language modeling
paradigm named $\textbf{AntLM}$, which integrates both CLM and MLM to leverage
the advantages of these two classic paradigms. We chose the strict-small track
and conducted experiments on two foundation models: BabyLlama, representing
CLM, and LTG-BERT, representing MLM. During the training process for specific
foundation models, we alternate between applying CLM or MLM training objectives
and causal or bidirectional attention masks. Experimental results show that
combining the two pretraining objectives leverages their strengths, enhancing
overall training performance. Under the same epochs, $AntLM_{BabyLlama}$
improves Macro-average by 1%, and $AntLM_{LTG-BERT}$ achieves a 2.2% increase
over the baselines.

æè¦ï¼å æè¯­è¨æ¨¡å (CLM) åé®è½è¯­è¨æ¨¡å (MLM) æ¯åºäº Transformer ç½ç»çä¸¤ç§ä¸»æµå­¦ä¹ èä¾ï¼ç¹å«æ¯ä»è§£ç å¨åä»ç¼ç å¨æ¶æãæ¯ç§èä¾å¨ä¸æ¸¸ä»»å¡ä¸­çä¼å¿é½è¡¨ç°åºä¼å¿åå£å¿çç»åãå¨è¿å»ç BabyLM Challenge 2023 ä¸­ï¼å°½ç®¡ MLM èä¾åå¾äºæä½³çå¹³åæ§è½ï¼ä½ CLM èä¾å´è¡¨ç°åºææ¾æ´å¿«çæ¶æéåº¦ãå¯¹äº BabyLM Challenge 2024ï¼æä»¬æåºäºä¸ç§åä¸º $\textbf{AntLM}$ çæ°è¯­è¨å»ºæ¨¡èä¾ï¼å®æ´åäº CLM å MLM ä»¥å©ç¨è¿ä¸¤ç§ç»å¸èä¾çä¼å¿ãæä»¬éæ©äºä¸¥æ ¼çå°åè½¨éï¼å¹¶å¨ä¸¤ä¸ªåºç¡æ¨¡åä¸è¿è¡äºå®éªï¼ä»£è¡¨ CLM ç BabyLlama åä»£è¡¨ MLM ç LTG-BERTãå¨ç¹å®åºç¡æ¨¡åçè®­ç»è¿ç¨ä¸­ï¼æä»¬å¨åºç¨ CLM æ MLM è®­ç»ç®æ åå ææååæ³¨æåæ©ç ä¹é´äº¤æ¿è¿è¡ãå®éªç»æè¡¨æï¼ç»åè¿ä¸¤ä¸ªé¢è®­ç»ç®æ å¯ä»¥å©ç¨å®ä»¬çä¼å¿ï¼ä»èå¢å¼ºæ´ä½è®­ç»æ§è½ãå¨ç¸åç epoch ä¸ï¼$AntLM_{BabyLlama}$ å°å®å¹³åæé«äº 1%ï¼è $AntLM_{LTG-BERT}$ æ¯åºçº¿æé«äº 2.2%ã

##### **Intent-driven In-context Learning for Few-shot Dialogue State Tracking**
2412.03270v1 by Zihao Yi, Zhe Xu, Ying Shen

Dialogue state tracking (DST) plays an essential role in task-oriented
dialogue systems. However, user's input may contain implicit information,
posing significant challenges for DST tasks. Additionally, DST data includes
complex information, which not only contains a large amount of noise unrelated
to the current turn, but also makes constructing DST datasets expensive. To
address these challenges, we introduce Intent-driven In-context Learning for
Few-shot DST (IDIC-DST). By extracting user's intent, we propose an
Intent-driven Dialogue Information Augmentation module to augment the dialogue
information, which can track dialogue states more effectively. Moreover, we
mask noisy information from DST data and rewrite user's input in the
Intent-driven Examples Retrieval module, where we retrieve similar examples. We
then utilize a pre-trained large language model to update the dialogue state
using the augmented dialogue information and examples. Experimental results
demonstrate that IDIC-DST achieves state-of-the-art performance in few-shot
settings on MultiWOZ 2.1 and MultiWOZ 2.4 datasets.

æè¦ï¼å°è©±çæè¿½è¹¤ (DST) å¨ä»»åå°åå°è©±ç³»çµ±ä¸­æ®æ¼èéè¦çè§è²ãç¶èï¼ä½¿ç¨èçè¼¸å¥å¯è½åå«é±å«è³è¨ï¼å° DST ä»»åé æéå¤§çææ°ãæ­¤å¤ï¼DST è³æåå«è¤éçè³è¨ï¼ä¸ååå«å¤§éèç®åè¼ªæ¬¡ç¡éçéè¨ï¼ä¹è®å»ºæ§ DST è³æéè®å¾æè²´ãçºäºæå°éäºææ°ï¼æåå¼å¥äºæåé©åçèçµ¡ä¸­å­¸ç¿ï¼ç¨æ¼å°é DST (IDIC-DST)ãééæåä½¿ç¨èçæåï¼æåæåºäºä¸åæåé©åçå°è©±è³è¨æ´åæ¨¡çµä¾æ´åå°è©±è³è¨ï¼å¯ä»¥æ´ææå°è¿½è¹¤å°è©±çæãæ­¤å¤ï¼æåé®è½ DST è³æä¸­çéè¨è³è¨ï¼ä¸¦å¨æåé©åçç¯ä¾æ·åæ¨¡çµä¸­æ¹å¯«ä½¿ç¨èçè¼¸å¥ï¼å¨å¶ä¸­æåæ·åé¡ä¼¼çç¯ä¾ãæ¥èï¼æåå©ç¨é åè¨ç·´çå¤§èªè¨æ¨¡åï¼ä½¿ç¨æ´åçå°è©±è³è¨åç¯ä¾ä¾æ´æ°å°è©±çæãå¯¦é©çµæé¡¯ç¤ºï¼IDIC-DST å¨ MultiWOZ 2.1 å MultiWOZ 2.4 è³æéä¸çå°éè¨­å®ä¸­ï¼éå°äºæåé²çæè½ã

##### **Alignment at Pre-training! Towards Native Alignment for Arabic LLMs**
2412.03253v1 by Juhao Liang, Zhenyang Cai, Jianqing Zhu, Huang Huang, Kewei Zong, Bang An, Mosen Alharthi, Juncai He, Lian Zhang, Haizhou Li, Benyou Wang, Jinchao Xu

The alignment of large language models (LLMs) is critical for developing
effective and safe language models. Traditional approaches focus on aligning
models during the instruction tuning or reinforcement learning stages, referred
to in this paper as `post alignment'. We argue that alignment during the
pre-training phase, which we term `native alignment', warrants investigation.
Native alignment aims to prevent unaligned content from the beginning, rather
than relying on post-hoc processing. This approach leverages extensively
aligned pre-training data to enhance the effectiveness and usability of
pre-trained models. Our study specifically explores the application of native
alignment in the context of Arabic LLMs. We conduct comprehensive experiments
and ablation studies to evaluate the impact of native alignment on model
performance and alignment stability. Additionally, we release open-source
Arabic LLMs that demonstrate state-of-the-art performance on various
benchmarks, providing significant benefits to the Arabic LLM community.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼çå°é½å°æ¼éç¼ææä¸å®å¨çèªè¨æ¨¡åè³ééè¦ãå³çµ±æ¹æ³èéæ¼å¨æä»¤å¾®èª¿æå¼·åå­¸ç¿éæ®µå°é½æ¨¡åï¼æ¬æç¨±ä¹çºãå¾å°é½ããæåèªçºï¼å¨é è¨ç·´éæ®µé²è¡å°é½ï¼æåç¨±ä¹çºãåçå°é½ãï¼å¼å¾æ¢è¨ãåçå°é½æ¨å¨å¾ä¸éå§å°±é²æ­¢æªå°é½çå§å®¹ï¼èä¸æ¯ä¾è³´äºå¾èçãéç¨®æ¹æ³ååå©ç¨ç¶éå¤§éå°é½çé è¨ç·´è³æï¼ä»¥å¢å¼·é è¨ç·´æ¨¡åçæææ§åå¯ç¨æ§ãæåçç ç©¶ç¹å¥æ¢è¨äºåçå°é½å¨é¿æä¼¯èª LLM ä¸­çæç¨ãæåé²è¡äºå¨é¢çå¯¦é©åæ¶èç ç©¶ï¼ä»¥è©ä¼°åçå°é½å°æ¨¡åæè½åå°é½ç©©å®æ§çå½±é¿ãæ­¤å¤ï¼æåç¼å¸äºéæ¾åå§ç¢¼çé¿æä¼¯èª LLMï¼éäº LLM å¨åç¨®åºæºæ¸¬è©¦ä¸­è¡¨ç¾åºæåé²çæè½ï¼çºé¿æä¼¯èª LLM ç¤¾ç¾¤å¸¶ä¾é¡¯èçå¥½èã

##### **AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning**
2412.03248v1 by Yiwu Zhong, Zhuoming Liu, Yin Li, Liwei Wang

Large language models (LLMs) have enabled the creation of multi-modal LLMs
that exhibit strong comprehension of visual data such as images and videos.
However, these models usually rely on extensive visual tokens from visual
encoders, leading to high computational demands, which limits their
applicability in resource-constrained environments and for long-context tasks.
In this work, we propose a training-free adaptive inference method for
multi-modal LLMs that can accommodate a broad range of efficiency requirements
with a minimum performance drop. Our method consists of a) iterative token
merging based on embedding similarity before LLMs, and b) progressive token
pruning within LLM layers based on multi-modal importance. With a minimalist
design, our method can be applied to both video and image LLMs. Extensive
experiments on diverse video and image benchmarks demonstrate that, our method
substantially reduces computation load (e.g., a $\textbf{7-fold}$ reduction in
FLOPs) while preserving the performance of video and image LLMs. Further, under
a similar computational cost, our method outperforms the state-of-the-art
methods in long video understanding (e.g., $\textbf{+4.6}$ on MLVU).
Additionally, our in-depth analysis provides insights into token redundancy and
LLM layer behaviors, offering guidance for future research in designing
efficient multi-modal LLMs. Our code will be available at
https://github.com/LaVi-Lab/AIM.

æè¦ï¼<paragraph>å¤§åèªè¨æ¨¡å (LLM) å·²è½å»ºç«å¤æ¨¡æ LLMï¼è½å°è¦è¦ºè³æï¼ä¾å¦å½±ååå½±çï¼å±ç¾å¼·å¤§ççè§£åã
ç¶èï¼éäºæ¨¡åéå¸¸ä»°è³´è¦è¦ºç·¨ç¢¼å¨çå»£æ³è¦è¦ºç¬¦èï¼å°è´é«éç®éæ±ï¼éå¶å¶å¨è³æºåéç°å¢åé·èçµ¡ä»»åä¸­çæç¨ã
å¨éé å·¥ä½ä¸­ï¼æåçºå¤æ¨¡æ LLM æåºä¸ååè¨ç·´çèªé©ææ¨è«æ¹æ³ï¼è½ä»¥æå°çæè½ä¸éä¾é©æå»£æ³çæçéæ±ãæåçåæ³åæ¬ a) å¨ LLM ä¹åæ ¹æåµå¥ç¸ä¼¼æ§é²è¡åè¦ç¬¦èåä½µï¼ä»¥å b) æ ¹æå¤æ¨¡æéè¦æ§å¨ LLM å±¤ä¸­é²è¡æ¼¸é²ç¬¦èä¿®åªãæåçåæ³æ¡ç¨æ¥µç°¡è¨­è¨ï¼å¯æç¨æ¼å½±çåå½±å LLMãå¨åç¨®å½±çåå½±ååºæºä¸çå»£æ³å¯¦é©è­æï¼æåçåæ³å¤§å¹éä½éç®è² è¼ï¼ä¾å¦ï¼FLOP æ¸å°äº $\textbf{7 å}$ï¼ï¼åæä¿çå½±çåå½±å LLM çæè½ãæ­¤å¤ï¼å¨é¡ä¼¼çéç®ææ¬ä¸ï¼æåçåæ³å¨é·å½±ççè§£æ¹é¢åªæ¼ç¾ææè¡ï¼ä¾å¦ï¼å¨ MLVU ä¸ $\textbf{+4.6}$ï¼ã
æ­¤å¤ï¼æåçæ·±å¥åææä¾äºç¬¦èåé¤å LLM å±¤è¡çºçè¦è§£ï¼çºæªä¾è¨­è¨é«æå¤æ¨¡æ LLM çç ç©¶æä¾æå°ãæåçç¨å¼ç¢¼å°æ¼ https://github.com/LaVi-Lab/AIM ä¸æä¾ã</paragraph>

##### **Benchmarking terminology building capabilities of ChatGPT on an English-Russian Fashion Corpus**
2412.03242v1 by Anastasiia Bezobrazova, Miriam Seghiri, Constantin Orasan

This paper compares the accuracy of the terms extracted using SketchEngine,
TBXTools and ChatGPT. In addition, it evaluates the quality of the definitions
produced by ChatGPT for these terms. The research is carried out on a
comparable corpus of fashion magazines written in English and Russian collected
from the web. A gold standard for the fashion terminology was also developed by
identifying web pages that can be harvested automatically and contain
definitions of terms from the fashion domain in English and Russian. This gold
standard was used to evaluate the quality of the extracted terms and of the
definitions produced. Our evaluation shows that TBXTools and SketchEngine,
while capable of high recall, suffer from reduced precision as the number of
terms increases, which affects their overall performance. Conversely, ChatGPT
demonstrates superior performance, maintaining or improving precision as more
terms are considered. Analysis of the definitions produced by ChatGPT for 60
commonly used terms in English and Russian shows that ChatGPT maintains a
reasonable level of accuracy and fidelity across languages, but sometimes the
definitions in both languages miss crucial specifics and include unnecessary
deviations. Our research reveals that no single tool excels universally; each
has strengths suited to particular aspects of terminology extraction and
application.

æè¦ï¼æ¬ææ¯è¼äºä½¿ç¨ SketchEngineãTBXTools å ChatGPT æåè¡èªçæºç¢ºæ§ãæ­¤å¤ï¼å®éè©ä¼°äº ChatGPT çºéäºè¡èªç¢ççå®ç¾©çåè³ªãç ç©¶æ¯å¨å¾ç¶²è·¯æ¶éçä»¥è±èªåä¿èªå¯«æçæå°éèªçåé¡èªæåº«ä¸é²è¡çãéééè­å¥å¯ä»¥èªåæ·åä¸¦åå«è±èªåä¿èªæå°é åè¡èªå®ç¾©çç¶²é ï¼å¶å®äºæå°è¡èªçé»éæ¨æºãéåé»éæ¨æºç¨æ¼è©ä¼°æåçè¡èªåç¢ççå®ç¾©çåè³ªãæåçè©ä¼°é¡¯ç¤ºï¼TBXTools å SketchEngine éç¶å·æå¾é«çå¬åçï¼ä½é¨èè¡èªæ¸éçå¢å ï¼å®åçæºç¢ºåº¦æéä½ï¼éæå½±é¿å®åçæ´é«æè½ãç¸åï¼ChatGPT è¡¨ç¾åºåªç°çæè½ï¼å¨èæ®æ´å¤è¡èªæï¼å®è½ç¶­æææé«æºç¢ºåº¦ãå° ChatGPT çºè±èªåä¿èªä¸­ 60 åå¸¸ç¨è¡èªç¢ççå®ç¾©é²è¡åæï¼çµæé¡¯ç¤º ChatGPT å¨ä¸åèªè¨ä¹éç¶­æåççæºç¢ºåº¦åä¿çåº¦ï¼ä½ææå©ç¨®èªè¨çå®ç¾©é½æéºæ¼ééµçå·é«è³è¨ï¼ä¸¦åå«ä¸å¿è¦çåå·®ãæåçç ç©¶è¡¨æï¼æ²æå®ä¸çå·¥å·è½æ®éååºï¼æ¯åå·¥å·é½æé©åè¡èªæååæç¨ç¹å®æ¹é¢çåªå¢ã

##### **Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?**
2412.03235v1 by Sravanti Addepalli, Yerram Varun, Arun Suggala, Karthikeyan Shanmugam, Prateek Jain

Large Language Models (LLMs) are known to be susceptible to crafted
adversarial attacks or jailbreaks that lead to the generation of objectionable
content despite being aligned to human preferences using safety fine-tuning
methods. While the large dimensionality of input token space makes it
inevitable to find adversarial prompts that can jailbreak these models, we aim
to evaluate whether safety fine-tuned LLMs are safe against natural prompts
which are semantically related to toxic seed prompts that elicit safe responses
after alignment. We surprisingly find that popular aligned LLMs such as GPT-4
can be compromised using naive prompts that are NOT even crafted with an
objective of jailbreaking the model. Furthermore, we empirically show that
given a seed prompt that elicits a toxic response from an unaligned model, one
can systematically generate several semantically related natural prompts that
can jailbreak aligned LLMs. Towards this, we propose a method of Response
Guided Question Augmentation (ReG-QA) to evaluate the generalization of safety
aligned LLMs to natural prompts, that first generates several toxic answers
given a seed question using an unaligned LLM (Q to A), and further leverages an
LLM to generate questions that are likely to produce these answers (A to Q). We
interestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to
producing natural jailbreak questions from unsafe content (without denial) and
can thus be used for the latter (A to Q) step. We obtain attack success rates
that are comparable to/ better than leading adversarial attack methods on the
JailbreakBench leaderboard, while being significantly more stable against
defenses such as Smooth-LLM and Synonym Substitution, which are effective
against existing all attacks on the leaderboard.

æè¦ï¼å¤§åè¯­è¨æ¨¡å (LLM) å·²ç¥å®¹æåå°ç²¾å¿è®¾è®¡çå¯¹ææ§æ»å»æè¶ç±æ»å»ï¼å°½ç®¡ä½¿ç¨å®å¨å¾®è°æ¹æ³ä¸äººç±»åå¥½ä¿æä¸è´ï¼ä½è¿äºæ»å»æè¶ç±æ»å»ä¼å¯¼è´çæä»¤äººåæçåå®¹ãè½ç¶è¾å¥æ è®°ç©ºé´çå¤§ç»´åº¦ä½¿å¾æ¾å°å¯ä»¥è¶ç±è¿äºæ¨¡åçå¯¹ææ§æç¤ºä¸å¯é¿åï¼ä½æä»¬çç®æ æ¯è¯ä¼°ç»è¿å®å¨å¾®è°ç LLM æ¯å¦å¯ä»¥é²æ­¢ä¸è¯­ä¹ç¸å³çèªç¶æç¤ºï¼è¿äºæç¤ºä¸å¼åå¯¹é½åå®å¨ååºçææ¯ç§å­æç¤ºç¸å³ãæä»¬æè®¶å°åç°ï¼è¯¸å¦ GPT-4 ç­æµè¡çå¯¹é½ LLM å¯ä»¥ä½¿ç¨å¤©ççæç¤ºæ¥ç ´åï¼çè³è¿äºæç¤ºå¹¶ä¸æ¯ä¸ºäºè¶ç±æ¨¡åèè®¾è®¡çãæ­¤å¤ï¼æä»¬å­ç»éªè¡¨æï¼ç»å®ä¸ä¸ªä»æªå¯¹é½çæ¨¡åå¼ååºææ¯ååºçç§å­æç¤ºï¼äººä»¬å¯ä»¥ç³»ç»å°çæå ä¸ªè¯­ä¹ç¸å³çèªç¶æç¤ºï¼è¿äºæç¤ºå¯ä»¥è¶ç±å¯¹é½ç LLMãä¸ºæ­¤ï¼æä»¬æåºäºä¸ç§ååºå¼å¯¼é®é¢å¢å¼º (ReG-QA) æ¹æ³æ¥è¯ä¼°å®å¨å¯¹é½ç LLM å¯¹èªç¶æç¤ºçæ³åï¼è¯¥æ¹æ³é¦åä½¿ç¨æªå¯¹é½ç LLM (Q å° A) ç»å®ä¸ä¸ªç§å­é®é¢çæå ä¸ªææ¯ç­æ¡ï¼å¹¶è¿ä¸æ­¥å©ç¨ LLM çæå¯è½äº§çè¿äºç­æ¡çé®é¢ (A å° Q)ãæä»¬æè¶£å°åç°ï¼è¯¸å¦ GPT-4o ç­ç»è¿å®å¨å¾®è°ç LLM å®¹æä»ä¸å®å¨åå®¹ï¼æ²¡æå¦è®¤ï¼ä¸­äº§çèªç¶çè¶ç±é®é¢ï¼å æ­¤å¯ä»¥ç¨äºåèï¼A å° Qï¼æ­¥éª¤ãæä»¬è·å¾çæ»å»æåçä¸ JailbreakBench æè¡æ¦ä¸çé¢åå¯¹ææ§æ»å»æ¹æ³ç¸å½/æ´å¥½ï¼åæ¶å¯¹å¹³æ» LLM ååä¹è¯æ¿æ¢ç­é²å¾¡æªæ½çç¨³å®æ§ææ¾æ´é«ï¼èè¿äºé²å¾¡æªæ½å¯¹æè¡æ¦ä¸çææç°ææ»å»é½æ¯ææçã

##### **PERL: Pinyin Enhanced Rephrasing Language Model for Chinese ASR N-best Error Correction**
2412.03230v1 by Junhong Liang

ASR correction methods have predominantly focused on general datasets and
have not effectively utilized Pinyin information, unique to the Chinese
language. In this study, we address this gap by proposing a Pinyin Enhanced
Rephrasing Language Model (PERL), specifically designed for N-best correction
scenarios. Additionally, we implement a length predictor module to address the
variable-length problem. We conduct experiments on the Aishell-1 dataset and
our newly proposed DoAD dataset. The results show that our approach outperforms
baseline methods, achieving a 29.11% reduction in Character Error Rate (CER) on
Aishell-1 and around 70% CER reduction on domain-specific datasets.
Furthermore, our approach leverages Pinyin similarity at the token level,
providing an advantage over baselines and leading to superior performance.

æè¦ï¼èªé³è¾¨è­æ ¡æ­£æ¹æ³ä¸»è¦éä¸­å¨ä¸è¬è³æéï¼ä¸å°æªææå©ç¨ä¸­æç¹æçæ³¨é³ç¬¦èè³è¨ãå¨æ¬ç ç©¶ä¸­ï¼æåééæåºå°éè¨­è¨ç¨æ¼ N-best æ ¡æ­£æå¢çæ³¨é³ç¬¦èå¢å¼·æ¹è¿°èªè¨æ¨¡å (PERL) ä¾è§£æ±ºæ­¤å·®è·ãæ­¤å¤ï¼æåå¯¦ä½é·åº¦é æ¸¬æ¨¡çµä¾è§£æ±ºé·åº¦è®ç°çåé¡ãæåå¨ Aishell-1 è³æéåæåæ°æåºç DoAD è³æéä¸é²è¡å¯¦é©ãçµæé¡¯ç¤ºï¼æåçåæ³åªæ¼åºç·æ¹æ³ï¼å¨ Aishell-1 ä¸å°å­åé¯èª¤ç (CER) éä½äº 29.11%ï¼å¨ç¹å®é åçè³æéä¸å° CER éä½äºç´ 70%ãæ­¤å¤ï¼æåçåæ³å¨ç¬¦èå±¤ç´ä¸­å©ç¨æ³¨é³ç¬¦èç¸ä¼¼æ§ï¼ç¸è¼æ¼åºç·æ¹æ³å·æåªå¢ï¼ä¸¦å¸¶ä¾æ´åªç°çæè½ã

##### **Linq-Embed-Mistral Technical Report**
2412.03223v1 by Chanyeol Choi, Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung Cho, Jy-yong Sohn

This report explores the enhancement of text retrieval performance using
advanced data refinement techniques. We develop
Linq-Embed-Mistral\footnote{\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}}
by building on the E5-mistral and Mistral-7B-v0.1 models, focusing on
sophisticated data crafting, data filtering, and negative mining methods, which
are highly tailored to each task, applied to both existing benchmark dataset
and highly tailored synthetic dataset generated via large language models
(LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024),
achieving an average score of 68.2 across 56 datasets, and ranks 1st among all
models for retrieval tasks on the MTEB leaderboard with a performance score of
60.2. This performance underscores its superior capability in enhancing search
precision and reliability. Our contributions include advanced data refinement
methods that significantly improve model performance on benchmark and synthetic
datasets, techniques for homogeneous task ordering and mixed task fine-tuning
to enhance model generalization and stability, and a streamlined evaluation
process using 4-bit precision and a light retrieval evaluation set, which
accelerates validation without sacrificing accuracy.

æè¦ï¼æ¬å ±åæ¢è¨ä½¿ç¨é²éè³æç²¾çæè¡ä¾æåæå­æª¢ç´¢æè½ãæåå¨ E5-mistral å Mistral-7B-v0.1 æ¨¡åçåºç¤ä¸éç¼ Linq-Embed-Mistral\footnote{\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}}ï¼å°æ³¨æ¼ç²¾å¯çè³æè£½ä½ãè³æç¯©é¸åè² é¢æææ¹æ³ï¼éäºæ¹æ³éå°æ¯åä»»åéèº«æé ï¼æç¨æ¼ç¾æçåºæºè³æéåééå¤§åèªè¨æ¨¡å (LLM) çæçéèº«æé åæè³æéãæªè³ 2024 å¹´ 5 æ 29 æ¥ï¼Linq-Embed-Mistral å¨ MTEB åºæºä¸­è¡¨ç¾åºè²ï¼å¨ 56 åè³æéä¸­åå¾å¹³å 68.2 åï¼å¨ MTEB æè¡æ¦ä¸ï¼æª¢ç´¢ä»»åçæææ¨¡åä¸­æåç¬¬ 1ï¼æè½åæ¸çº 60.2ãæ­¤æè½çªé¡¯å¶å¨æåæå°ç²¾æºåº¦åå¯é åº¦æ¹é¢çåªç°è½åãæåçè²¢ç»åæ¬é²éè³æç²¾çæ¹æ³ï¼å¯å¤§å¹æååºæºååæè³æéä¸çæ¨¡åæè½ãåè³ªä»»åæåºåæ··åä»»åå¾®èª¿æè¡ï¼ä»¥æåæ¨¡åæ¦ååç©©å®æ§ï¼ä»¥åä½¿ç¨ 4 ä½åç²¾æºåº¦åè¼éæª¢ç´¢è©ä¼°éçç°¡åè©ä¼°æµç¨ï¼éè½å¨ä¸ç§ç²ç²¾æºåº¦çææ³ä¸å éé©è­ã

##### **ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression**
2412.03213v1 by Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, Minyi Guo

Large Language Models (LLMs) have been widely deployed in a variety of
applications, and the context length is rapidly increasing to handle tasks such
as long-document QA and complex logical reasoning. However, long context poses
significant challenges for inference efficiency, including high memory costs of
key-value (KV) cache and increased latency due to extensive memory accesses.
Recent works have proposed compressing KV cache to approximate computation, but
these methods either evict tokens permanently, never recalling them for later
inference, or recall previous tokens at the granularity of pages divided by
textual positions. Both approaches degrade the model accuracy and output
quality. To achieve efficient and accurate recallable KV cache compression, we
introduce ClusterKV, which recalls tokens at the granularity of semantic
clusters. We design and implement efficient algorithms and systems for
clustering, selection, indexing and caching. Experiment results show that
ClusterKV attains negligible accuracy loss across various tasks with 32k
context lengths, using only a 1k to 2k KV cache budget, and achieves up to a
2$\times$ speedup in latency and a 2.5$\times$ improvement in decoding
throughput. Compared to SoTA recallable KV compression methods, ClusterKV
demonstrates higher model accuracy and output quality, while maintaining or
exceeding inference efficiency.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å»£æ³é¨ç½²æ¼åç¨®æç¨ç¨å¼ä¸­ï¼ä¸èæ¯é·åº¦è¿éå¢å ï¼ä»¥èçé·æä»¶åç­åè¤ééè¼¯æ¨çç­ä»»åãç¶èï¼é·èæ¯å°æ¨è«æçæ§æéå¤§ææ°ï¼åæ¬éµå¼ (KV) å¿«åçé«è¨æ¶é«ææ¬ï¼ä»¥åç±æ¼å¤§éè¨æ¶é«å­åèå¢å çå»¶é²ãæè¿çç ç©¶å·²æåºå£ç¸® KV å¿«åä»¥è¿ä¼¼è¨ç®ï¼ä½éäºæ¹æ³ææ°¸ä¹é©éæ¬æï¼æ°¸é ä¸æå¨å¾çºæ¨è«ä¸­æåå®åï¼æå¨ç±æå­ä½ç½®ååçé é¢ç²åº¦ä¸­æåååçæ¬æãéå©ç¨®æ¹æ³é½æéä½æ¨¡åæºç¢ºåº¦åè¼¸åºåè³ªãçºäºéæææä¸æºç¢ºçå¯æå KV å¿«åå£ç¸®ï¼æåå¼å¥äº ClusterKVï¼å®å¨èªç¾©å¢éçç²åº¦ä¸­æåæ¬æãæåè¨­è¨ä¸¦å¯¦ä½äºç¨æ¼å¢éãé¸æãç´¢å¼åå¿«åçæææ¼ç®æ³åç³»çµ±ãå¯¦é©çµæé¡¯ç¤ºï¼ClusterKV å¨åç¨®ä»»åä¸­ç²å¾æ¥µå°çæºç¢ºåº¦æå¤±ï¼èæ¯é·åº¦çº 32kï¼åä½¿ç¨ 1k å° 2k ç KV å¿«åé ç®ï¼ä¸¦å¨å»¶é²ä¸­å¯¦ç¾äºé«é 2 åçå éï¼ä»¥åå¨è§£ç¢¼èçéä¸­å¯¦ç¾äº 2.5 åçæåãè SoTA å¯æå KV å£ç¸®æ¹æ³ç¸æ¯ï¼ClusterKV å±ç¤ºäºæ´é«çæ¨¡åæºç¢ºåº¦åè¼¸åºåè³ªï¼åæç¶­ææè¶è¶æ¨è«æçã

##### **U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs**
2412.03205v1 by Konstantin Chernyshev, Vitaliy Polshkov, Ekaterina Artemova, Alex Myasnikov, Vlad Stepanov, Alexei Miasnikov, Sergei Tilga

The current evaluation of mathematical skills in LLMs is limited, as existing
benchmarks are either relatively small, primarily focus on elementary and
high-school problems, or lack diversity in topics. Additionally, the inclusion
of visual elements in tasks remains largely under-explored.
  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100
unpublished open-ended university-level problems sourced from teaching
materials. It is balanced across six core subjects, with 20% of multimodal
problems. Given the open-ended nature of U-MATH problems, we employ an LLM to
judge the correctness of generated solutions. To this end, we release
$\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.
  The evaluation of general domain, math-specific, and multimodal LLMs
highlights the challenges presented by U-MATH. Our findings reveal that LLMs
achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%
on visual problems. The solution assessment proves challenging for LLMs, with
the best LLM judge having an F1-score of 80% on $\mu$-MATH.

æè¦ï¼ç¾æè©ä¼° LLM æ¸å­¸æè½çæ¨æºæéï¼å çºç¾æåºæºè¦ä¸è¦æ¨¡è¼å°ãä¸»è¦èéæ¼å°å­¸åé«ä¸­åé¡ï¼è¦ä¸ç¼ºä¹ä¸»é¡å¤åæ§ãæ­¤å¤ï¼ä»»åä¸­åå«è¦è¦ºåç´ çé¨åä»æªå¾å°ååæ¢è¨ã
çºäºè§£æ±ºéäºå·®è·ï¼æåå¼å¥äº U-MATHï¼éæ¯ä¸åç±æå­¸ææä¸­èéç 1,100 åæªç¼è¡¨çéæ¾å¼å¤§å­¸ç¨åº¦åé¡çæ°åºæºãå®æ¶µèå­åæ ¸å¿ç§ç®ï¼20% çºå¤æ¨¡æåé¡ãéæ¼ U-MATH åé¡çéæ¾å¼æ§è³ªï¼æåæ¡ç¨ LLM ä¾å¤æ·æç¢çè§£æ³çæ­£ç¢ºæ§ãçºæ­¤ï¼æåç¼å¸äº $\mu$-MATHï¼ä¸åç¨æ¼è©ä¼° LLM å¤æ·è§£é¡è½åçè³æéã
å°ä¸è¬é åãç¹å®æ¸å­¸åå¤æ¨¡æ LLM çè©ä¼°çªé¡¯äº U-MATH æå¸¶ä¾çææ°ãæåçç ç©¶çµæé¡¯ç¤ºï¼LLM å¨åºæ¼æå­çä»»åä¸åéå° 63% çæé«æºç¢ºåº¦ï¼å¨è¦è¦ºåé¡ä¸çè³æ´ä½ï¼åªæ 45%ãè§£é¡è©ä¼°å° LLM ä¾èªªå·æææ°æ§ï¼æä½³ LLM è©å¯©å¨ $\mu$-MATH ä¸ç F1 åæ¸çº 80%ã

##### **Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for Traffic Prediction**
2412.03188v1 by Ivan Kralj, Lodovico Giaretta, Gordan JeÅ¾iÄ, Ivana Podnar Å½arko, Å arÅ«nas Girdzijauskas

In smart mobility, large networks of geographically distributed sensors
produce vast amounts of high-frequency spatio-temporal data that must be
processed in real time to avoid major disruptions. Traditional centralized
approaches are increasingly unsuitable to this task, as they struggle to scale
with expanding sensor networks, and reliability issues in central components
can easily affect the whole deployment. To address these challenges, we explore
and adapt semi-decentralized training techniques for Spatio-Temporal Graph
Neural Networks (ST-GNNs) in smart mobility domain. We implement a simulation
framework where sensors are grouped by proximity into multiple cloudlets, each
handling a subgraph of the traffic graph, fetching node features from other
cloudlets to train its own local ST-GNN model, and exchanging model updates
with other cloudlets to ensure consistency, enhancing scalability and removing
reliance on a centralized aggregator. We perform extensive comparative
evaluation of four different ST-GNN training setups -- centralized, traditional
FL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the
METR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed
predictions. Experimental results show that semi-decentralized setups are
comparable to centralized approaches in performance metrics, while offering
advantages in terms of scalability and fault tolerance. In addition, we
highlight often overlooked issues in existing literature for distributed
ST-GNNs, such as the variation in model performance across different
geographical areas due to region-specific traffic patterns, and the significant
communication overhead and computational costs that arise from the large
receptive field of GNNs, leading to substantial data transfers and increased
computation of partial embeddings.

æè¦ï¼<paragraph>å¨æºæ§è¡åé åä¸­ï¼ç±å°çåæ£ææ¸¬å¨çµæçé¾å¤§ç¶²è·¯æç¢çå¤§éçæç©ºé«é »çè³æï¼éäºè³æå¿é å³æèçï¼æè½é¿åéå¤§ä¸­æ·ãå³çµ±çéä¸­å¼æ¹æ³æä¾æä¸é©åéé ä»»åï¼å çºå®åå¨æ´å±ææ¸¬å¨ç¶²è·¯æé£ä»¥æ´åï¼èä¸­å¤®çµä»¶çå¯é æ§åé¡ä¹å¯è½è¼æå½±é¿æ´åé¨ç½²ãçºäºæå°éäºææ°ï¼æåæ¢ç´¢ä¸¦èª¿æ´äºæºæ§è¡åé åä¸­æç©ºåå½¢ç¥ç¶ç¶²è·¯ (ST-GNN) çååæ£å¼è¨ç·´æè¡ãæåå¯¦ä½äºä¸åæ¨¡æ¬æ¶æ§ï¼å¶ä¸­ææ¸¬å¨æä¾ææ¥è¿æ§åçµå°å¤åé²ç«¯ä¸­ï¼æ¯åé²ç«¯èçäº¤éåå½¢çä¸åå­åï¼å¾å¶ä»é²ç«¯æ·åç¯é»ç¹å¾µä¾è¨ç·´å¶èªå·±çå±é¨ ST-GNN æ¨¡åï¼ä¸¦èå¶ä»é²ç«¯äº¤ææ¨¡åæ´æ°ä»¥ç¢ºä¿ä¸è´æ§ï¼é²èæåæ´åæ§ä¸¦æ¶é¤å°éä¸­å¼å½ç¸½å¨çä¾è³´ãæåå°åç¨®ä¸åç ST-GNN è¨ç·´è¨­å®é²è¡å»£æ³çæ¯è¼è©ä¼°ï¼åæ¬éä¸­å¼ãå³çµ± FLãç¡ä¼ºæå¨ FL åå«å¦å­¸ç¿ï¼éäºè©ä¼°æ¯å¨å¤§è¦æ¨¡äº¤éè³æéï¼METR-LA å PeMS-BAY è³æéï¼ä¸é²è¡ï¼ç¨æ¼ç­æãä¸­æåé·æè»è¼éåº¦é æ¸¬ãå¯¦é©çµæé¡¯ç¤ºï¼ååæ£å¼è¨­å®å¨æè½ææ¨ä¸å¯èéä¸­å¼æ¹æ³ç¸æä¸¦è«ï¼åæå¨æ´åæ§åå®¹é¯æ§æ¹é¢å·æåªå¢ãæ­¤å¤ï¼æåå¼·èª¿äºç¾æåæ£å¼ ST-GNN æç»ä¸­ç¶å¸¸è¢«å¿½ç¥çåé¡ï¼ä¾å¦ç±æ¼ååç¹å®äº¤éæ¨¡å¼èå°è´ä¸åå°çååçæ¨¡åæè½å·®ç°ï¼ä»¥åç±æ¼ GNN çå¤§æåéèç¢ççé¡¯èéè¨éé·åéç®ææ¬ï¼å°è´å¤§éè³æå³è¼¸åå±é¨åµå¥éç®å¢å ã</paragraph>

##### **Weighted-Reward Preference Optimization for Implicit Model Fusion**
2412.03187v1 by Ziyi Yang, Fanqi Wan, Longguang Zhong, Tianyuan Shi, Xiaojun Quan

While fusing heterogeneous open-source LLMs with varying architectures and
sizes can potentially integrate the strengths of different models, existing
fusion methods face significant challenges, such as vocabulary alignment and
merging distribution matrices. These procedures are not only complex but also
prone to introducing noise and errors. In this paper, we propose an implicit
fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages
preference optimization between the source LLMs and the target LLM to transfer
their capabilities effectively. WRPO eliminates the need for vocabulary
alignment and matrix fusion and can be efficiently scaled to accommodate
various LLMs. To address distributional deviations between the source and
target LLMs, WRPO introduces a progressive adaptation strategy that gradually
shifts reliance on preferred examples from the target LLM to the source LLMs.
Extensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks
demonstrate that WRPO consistently outperforms existing knowledge fusion
methods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct
as the target model, WRPO achieves a length-controlled win rate of 55.9%
against GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against
GPT-4-0314 on Arena-Hard. Our code is available at
\url{https://github.com/SLIT-AI/WRPO}.

æè¦ï¼<paragraph>éç¶èåç°è³ªéæ¾åå§ç¢¼ LLMï¼å¶æ¶æ§åè¦æ¨¡åç°ï¼ææ´åä¸åæ¨¡ååªå¢çæ½åï¼ç¾æçèåæ¹æ³å»é¢è¨è«¸å¤ææ°ï¼ä¾å¦è©å½æ¯å°ååä½µåä½ç©é£ãéäºç¨åºä¸åè¤éï¼éå®¹æå¼å¥éè¨åé¯èª¤ãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®é±å¼èåæ¹æ³ï¼å³å æ¬çåµåå¥½æä½³å (WRPO)ï¼å®å©ç¨åå§ LLM åç®æ¨ LLM ä¹éçåå¥½æä½³åä¾ææè½ç§»å®åçè½åãWRPO æ¶é¤äºè©å½æ¯å°åç©é£èåçéè¦ï¼ä¸¦ä¸å¯ä»¥æææ´å±ä»¥å®¹ç´åç¨® LLMãçºäºè§£æ±ºåå§åç®æ¨ LLM ä¹éçåéåå·®ï¼WRPO å¼å¥äºä¸ç¨®æ¼¸é²é©æç­ç¥ï¼éæ¼¸å°å°ç®æ¨ LLM çåå¥½ç¯ä¾çä¾è³´è½ç§»å°åå§ LLMãå¨ MT-BenchãAlpacaEval-2 å Arena-Hard åºæºä¸çå»£æ³å¯¦é©è¡¨æï¼WRPO æçºåªæ¼ç¾æçç¥è­èåæ¹æ³ååç¨®å¾®èª¿åºæºãç¶æç¨æ¼ LLaMA3-8B-Instruct ä½çºç®æ¨æ¨¡åæï¼WRPO å¨ AlpacaEval-2 ä¸å° GPT-4-Preview-1106 éå°äº 55.9% çé·åº¦æ§å¶ç²åçï¼å¨ Arena-Hard ä¸å° GPT-4-0314 éå°äº 46.2% çç²åçãæåçç¨å¼ç¢¼å¯å¨ä»¥ä¸ç¶²ååå¾ï¼\url{https://github.com/SLIT-AI/WRPO}ã</paragraph>

##### **Optimizing Dense Visual Predictions Through Multi-Task Coherence and Prioritization**
2412.03179v1 by Maxime Fontana, Michael Spratling, Miaojing Shi

Multi-Task Learning (MTL) involves the concurrent training of multiple tasks,
offering notable advantages for dense prediction tasks in computer vision. MTL
not only reduces training and inference time as opposed to having multiple
single-task models, but also enhances task accuracy through the interaction of
multiple tasks. However, existing methods face limitations. They often rely on
suboptimal cross-task interactions, resulting in task-specific predictions with
poor geometric and predictive coherence. In addition, many approaches use
inadequate loss weighting strategies, which do not address the inherent
variability in task evolution during training. To overcome these challenges, we
propose an advanced MTL model specifically designed for dense vision tasks. Our
model leverages state-of-the-art vision transformers with task-specific
decoders. To enhance cross-task coherence, we introduce a trace-back method
that improves both cross-task geometric and predictive features. Furthermore,
we present a novel dynamic task balancing approach that projects task losses
onto a common scale and prioritizes more challenging tasks during training.
Extensive experiments demonstrate the superiority of our method, establishing
new state-of-the-art performance across two benchmark datasets. The code is
available at:https://github.com/Klodivio355/MT-CP

æè¦ï¼å¤ä»»åå­¸ç¿ (MTL) æ¶åå¤åä»»åçä¸¦ç¼è¨ç·´ï¼çºé»è¦è¦è¦ºä¸­çå¯éé æ¸¬ä»»åæä¾äºé¡¯èåªå¢ãMTL ä¸åæ¸å°äºè¨ç·´åæ¨çæéï¼èææå¤åå®ä»»åæ¨¡åç¸æ¯ï¼éééå¤åä»»åçäº¤äºå¢å¼·äºä»»åæºç¢ºæ§ãç¶èï¼ç¾ææ¹æ³é¢è¨éå¶ãå®åéå¸¸ä¾è³´æ¼æ¬¡åªçè·¨ä»»åäº¤äºï¼å°è´ä»»åç¹å®çé æ¸¬å·æè¼å·®çå¹¾ä½åé æ¸¬ä¸è´æ§ãæ­¤å¤ï¼è¨±å¤æ¹æ³ä½¿ç¨ä¸ååçæå¤±å æ¬ç­ç¥ï¼éç¡æ³è§£æ±ºè¨ç·´éç¨ä¸­ä»»åæ¼åçåºæè®ç°æ§ãçºäºåæéäºææ°ï¼æåæåºäºä¸åå°éçºå¯éè¦è¦ºä»»åè¨­è¨çé«ç´ MTL æ¨¡åãæåçæ¨¡åå©ç¨äºæåé²çè¦è¦ºè®æå¨åä»»åç¹å®çè§£ç¢¼å¨ãçºäºå¢å¼·è·¨ä»»åä¸è´æ§ï¼æåå¼å¥äºä¸ç¨®è¿½æº¯æ¹æ³ï¼å®æ¹é²äºè·¨ä»»åå¹¾ä½åé æ¸¬ç¹å¾µãæ­¤å¤ï¼æåæåºäºä¸ç¨®æ°ç©çåæä»»åå¹³è¡¡æ¹æ³ï¼å®å°ä»»åæå¤±æå½±å°ä¸åå¬å±å°ºåº¦ä¸ï¼ä¸¦å¨è¨ç·´éç¨ä¸­åªåèæ®æ´å·ææ°æ§çä»»åãå»£æ³çå¯¦é©è­æäºæåæ¹æ³çåªè¶æ§ï¼å¨å©ååºæºæ¸æéä¸å»ºç«äºæ°çæåé²æ§è½ãä»£ç¢¼å¯å¨ä»¥ä¸ä½ç½®ç²å¾ï¼https://github.com/Klodivio355/MT-CP

##### **Towards Understanding and Quantifying Uncertainty for Text-to-Image Generation**
2412.03178v1 by Gianni Franchi, Dat Nguyen Trong, Nacim Belkhir, Guoxuan Xia, Andrea Pilzer

Uncertainty quantification in text-to-image (T2I) generative models is
crucial for understanding model behavior and improving output reliability. In
this paper, we are the first to quantify and evaluate the uncertainty of T2I
models with respect to the prompt. Alongside adapting existing approaches
designed to measure uncertainty in the image space, we also introduce
Prompt-based UNCertainty Estimation for T2I models (PUNC), a novel method
leveraging Large Vision-Language Models (LVLMs) to better address uncertainties
arising from the semantics of the prompt and generated images. PUNC utilizes a
LVLM to caption a generated image, and then compares the caption with the
original prompt in the more semantically meaningful text space. PUNC also
enables the disentanglement of both aleatoric and epistemic uncertainties via
precision and recall, which image-space approaches are unable to do. Extensive
experiments demonstrate that PUNC outperforms state-of-the-art uncertainty
estimation techniques across various settings. Uncertainty quantification in
text-to-image generation models can be used on various applications including
bias detection, copyright protection, and OOD detection. We also introduce a
comprehensive dataset of text prompts and generation pairs to foster further
research in uncertainty quantification for generative models. Our findings
illustrate that PUNC not only achieves competitive performance but also enables
novel applications in evaluating and improving the trustworthiness of
text-to-image models.

æè¦ï¼å¨ææ¬å°å¾å (T2I) çææ¨¡åä¸­éåä¸ç¡®å®æ§å¯¹äºçè§£æ¨¡åè¡ä¸ºåæé«è¾åºå¯é æ§è³å³éè¦ãå¨æ¬æä¸­ï¼æä»¬é¦æ¬¡éååè¯ä¼°äº T2I æ¨¡åç¸å¯¹äºæç¤ºçä¸ç¡®å®æ§ãé¤äºè°æ´æ¨å¨æµéå¾åç©ºé´ä¸­ä¸ç¡®å®æ§çç°ææ¹æ³å¤ï¼æä»¬è¿å¼å¥äºåºäºæç¤ºçä¸ç¡®å®æ§ä¼°è®¡ç¨äº T2I æ¨¡å (PUNC)ï¼è¿æ¯ä¸ç§å©ç¨å¤§åè§è§è¯­è¨æ¨¡å (LVLMs) æ¥æ´å¥½å°è§£å³æºèªæç¤ºè¯­ä¹åçæå¾åçä¸ç¡®å®æ§ãPUNC å©ç¨ LVLM ä¸ºçæçå¾åæ·»å æ é¢ï¼ç¶åå¨è¯­ä¹ä¸æ´ææä¹çææ¬ç©ºé´ä¸­å°æ é¢ä¸åå§æç¤ºè¿è¡æ¯è¾ãPUNC è¿è½å¤éè¿ç²¾åº¦åå¬åçæ¥è§£å¼å¶ç¶ä¸ç¡®å®æ§åè®¤ç¥ä¸ç¡®å®æ§ï¼è¿æ¯å¾åç©ºé´æ¹æ³æ æ³åå°çãå¤§éçå®éªè¡¨æï¼PUNC å¨åç§è®¾ç½®ä¸é½ä¼äºæåè¿çä¸ç¡®å®æ§ä¼°è®¡ææ¯ãææ¬å°å¾åçææ¨¡åä¸­çä¸ç¡®å®æ§éåå¯ç¨äºåç§åºç¨ï¼åæ¬åå·®æ£æµãçæä¿æ¤å OOD æ£æµãæä»¬è¿å¼å¥äºä¸ä¸ªåå«ææ¬æç¤ºåçæå¯¹çç»¼åæ°æ®éï¼ä»¥ä¿è¿å¯¹çææ¨¡åä¸ç¡®å®æ§éåçè¿ä¸æ­¥ç ç©¶ãæä»¬çç ç©¶ç»æè¡¨æï¼PUNC ä¸ä»å®ç°äºæç«äºåçæ§è½ï¼èä¸è¿è½å¤å¨è¯ä¼°åæé«ææ¬å°å¾åæ¨¡åçå¯ä¿¡åº¦æ¹é¢å®ç°æ°åºç¨ã

##### **Automatic detection of diseases in Spanish clinical notes combining medical language models and ontologies**
2412.03176v1 by Leon-Paul Schaub Torre, Pelayo Quiros, Helena Garcia Mieres

In this paper we present a hybrid method for the automatic detection of
dermatological pathologies in medical reports. We use a large language model
combined with medical ontologies to predict, given a first appointment or
follow-up medical report, the pathology a person may suffer from. The results
show that teaching the model to learn the type, severity and location on the
body of a dermatological pathology, as well as in which order it has to learn
these three features, significantly increases its accuracy. The article
presents the demonstration of state-of-the-art results for classification of
medical texts with a precision of 0.84, micro and macro F1-score of 0.82 and
0.75, and makes both the method and the data set used available to the
community.

æè¦ï¼å¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ··åæ¹æ³ï¼ç¨æ¼èªåæª¢æ¸¬é«çå ±åä¸­çç®èççãæåä½¿ç¨å¤§åèªè¨æ¨¡åçµåé«å­¸æ¬ä½ï¼é æ¸¬çµ¦å®åè¨ºæå¾çºé«çå ±åï¼ä¸åäººå¯è½æ£æçççãçµæè¡¨æï¼æææ¨¡åå­¸ç¿ç®èçççé¡åãå´éç¨åº¦åèº«é«ä½ç½®ï¼ä»¥åæä½ç¨®é åºå­¸ç¿éä¸åç¹å¾µï¼å¯ä»¥é¡¯èæé«å¶æºç¢ºæ§ãæ¬æå±ç¤ºäºé«å­¸ææ¬åé¡çææ°çµæï¼ç²¾ç¢ºåº¦çº 0.84ï¼å¾®è§åå·¨è§ F1 åæ¸çº 0.82 å 0.75ï¼ä¸¦å°æ¹æ³åæ¸æéæä¾çµ¦ç¤¾åã

##### **Physics-Informed Deep Inverse Operator Networks for Solving PDE Inverse Problems**
2412.03161v1 by Sung Woong Cho, Hwijae Son

Inverse problems involving partial differential equations (PDEs) can be seen
as discovering a mapping from measurement data to unknown quantities, often
framed within an operator learning approach. However, existing methods
typically rely on large amounts of labeled training data, which is impractical
for most real-world applications. Moreover, these supervised models may fail to
capture the underlying physical principles accurately. To address these
limitations, we propose a novel architecture called Physics-Informed Deep
Inverse Operator Networks (PI-DIONs), which can learn the solution operator of
PDE-based inverse problems without labeled training data. We extend the
stability estimates established in the inverse problem literature to the
operator learning framework, thereby providing a robust theoretical foundation
for our method. These estimates guarantee that the proposed model, trained on a
finite sample and grid, generalizes effectively across the entire domain and
function space. Extensive experiments are conducted to demonstrate that
PI-DIONs can effectively and accurately learn the solution operators of the
inverse problems without the need for labeled data.

æè¦ï¼æ¶ååå¾®åæ¹ç¨ (PDE) çååé¡å¯ä»¥è¦çºå¾æ¸¬éæ¸æä¸­æ¾åºå°ææªç¥æ¸çå°æéä¿ï¼éå¸¸æå¨éç®å­å­¸ç¿æ¹æ³ä¸­å»ºæ§ãä¸éï¼ç¾ææ¹æ³éå¸¸ä»°è³´å¤§éæ¨ç±¤è¨ç·´è³æï¼éå¨å¤æ¸çå¯¦ä¸ççæç¨ä¸­ä¸¦ä¸åå¯¦éãæ­¤å¤ï¼éäºç£ç£å¼æ¨¡åå¯è½ç¡æ³ç²¾æºææå°åºå±¤çç©çåçãçºäºè§£æ±ºéäºéå¶ï¼æåæåºäºä¸ç¨®ç¨±çºç©çè¨æ¯æ·±åº¦åéç®å­ç¶²è·¯ (PI-DION) çæ°æ¶æ§ï¼å®å¯ä»¥å¨æ²ææ¨ç±¤è¨ç·´è³æçææ³ä¸å­¸ç¿åºæ¼ PDE çååé¡çè§£éç®å­ãæåå°ååé¡æç»ä¸­å·²å»ºç«çç©©å®æ§ä¼°è¨å¼å»¶ä¼¸å°éç®å­å­¸ç¿æ¶æ§ä¸­ï¼é²èçºæåçæ¨¡åæä¾ç©©å¥ççè«åºç¤ãéäºä¼°è¨å¼ä¿è­äºææåºçæ¨¡åå¨æéæ¨£æ¬åç¶²æ ¼ä¸è¨ç·´å¾ï¼å¯ä»¥å¨æ´åç¶²ååå½æ¸ç©ºéä¸­ææå°é²è¡æ¦åãæåé²è¡äºå¤§éçå¯¦é©ï¼ä»¥è­æ PI-DION å¯ä»¥ææä¸ç²¾æºå°å­¸ç¿ååé¡çè§£éç®å­ï¼èä¸ä¸éè¦æ¨ç±¤è³æã

##### **Byte BPE Tokenization as an Inverse string Homomorphism**
2412.03160v1 by Saibo Geng, Sankalp Gambhir, Chris Wendler, Robert West

Tokenization is an important preprocessing step in the training and inference
of large language models (LLMs). While there has been extensive research on the
expressive power of the neural achitectures used in LLMs, the impact of
tokenization has not been well understood. In this work, we demonstrate that
tokenization, irrespective of the algorithm used, acts as an inverse
homomorphism between strings and tokens. This suggests that the character space
of the source language and the token space of the tokenized language are
homomorphic, preserving the structural properties of the source language.
Additionally, we explore the concept of proper tokenization, which refers to an
unambiguous tokenization returned from the tokenizer. Our analysis reveals that
the expressiveness of neural architectures in recognizing context-free
languages is not affected by tokenization.

æè¦ï¼åè©æ¯è¨ç·´åæ¨è«å¤§åèªè¨æ¨¡å (LLM) ä¸­ä¸åéè¦çé èçæ­¥é©ãéç¶å°æ¼ LLM ä¸­ä½¿ç¨çç¥ç¶æ¶æ§çè¡¨ç¾åå·²æå»£æ³çç ç©¶ï¼ä½åè©çå½±é¿å°æªè¢«ååçè§£ãå¨éé å·¥ä½ä¸­ï¼æåè­æäºåè©ï¼ç¡è«ä½¿ç¨ä½ç¨®æ¼ç®æ³ï¼é½åç¶å­ä¸²åç¬¦èä¹éçååæãéè¡¨æåå§èªè¨çå­åç©ºéååè©èªè¨çç¬¦èç©ºéæ¯åæçï¼ä¿çäºåå§èªè¨ççµæ§ç¹æ§ãæ­¤å¤ï¼æåæ¢è¨äºé©ç¶åè©çæ¦å¿µï¼éæçæ¯å¾åè©å¨è¿åçæç¢ºåè©ãæåçåæè¡¨æï¼ç¥ç¶æ¶æ§å¨è­å¥ä¸ä¸æç¡éèªè¨ä¸­çè¡¨ç¾åä¸ååè©çå½±é¿ã

##### **Testing Neural Network Verifiers: A Soundness Benchmark with Hidden Counterexamples**
2412.03154v1 by Xingjian Zhou, Hongji Xu, Andy Xu, Zhouxing Shi, Cho-Jui Hsieh, Huan Zhang

In recent years, many neural network (NN) verifiers have been developed to
formally verify certain properties of neural networks such as robustness.
Although many benchmarks have been constructed to evaluate the performance of
NN verifiers, they typically lack a ground-truth for hard instances where no
current verifier can verify and no counterexample can be found, which makes it
difficult to check the soundness of a new verifier if it claims to verify hard
instances which no other verifier can do. We propose to develop a soundness
benchmark for NN verification. Our benchmark contains instances with
deliberately inserted counterexamples while we also try to hide the
counterexamples from regular adversarial attacks which can be used for finding
counterexamples. We design a training method to produce neural networks with
such hidden counterexamples. Our benchmark aims to be used for testing the
soundness of NN verifiers and identifying falsely claimed verifiability when it
is known that hidden counterexamples exist. We systematically construct our
benchmark and generate instances across diverse model architectures, activation
functions, input sizes, and perturbation radii. We demonstrate that our
benchmark successfully identifies bugs in state-of-the-art NN verifiers, as
well as synthetic bugs, providing a crucial step toward enhancing the
reliability of testing NN verifiers. Our code is available at
https://github.com/MVP-Harry/SoundnessBench and our benchmark is available at
https://huggingface.co/datasets/SoundnessBench/SoundnessBench.

æè¦ï¼è¿å¹´æ¥ï¼è®¸å¤ç¥ç»ç½ç» (NN) éªè¯å¨è¢«å¼ååºæ¥ï¼ä»¥æ­£å¼éªè¯ç¥ç»ç½ç»çæäºå±æ§ï¼ä¾å¦é²æ£æ§ãè½ç¶å·²ç»æå»ºäºè®¸å¤åºåæ¥è¯ä¼° NN éªè¯å¨çæ§è½ï¼ä½å®ä»¬éå¸¸ç¼ºä¹é¾ä»¥éªè¯çå®ä¾çåºæ¬äºå®ï¼èå½åæ²¡æéªè¯å¨å¯ä»¥éªè¯å¹¶ä¸æ¾ä¸å°åä¾ï¼è¿ä½¿å¾å¦æå£°ç§°éªè¯å¶ä»éªè¯å¨æ æ³éªè¯çå°é¾å®ä¾ï¼åé¾ä»¥æ£æ¥æ°éªè¯å¨çå¥å¨æ§ãæä»¬å»ºè®®ä¸º NN éªè¯å¼åä¸ä¸ªå¥å¨æ§åºåãæä»¬çåºååå«æææå¥åä¾çå®ä¾ï¼èæä»¬ä¹å°è¯å°åä¾éèå¨å¸¸è§å¯¹ææ§æ»å»ä¸­ï¼è¯¥æ»å»å¯ç¨äºæ¥æ¾åä¾ãæä»¬è®¾è®¡äºä¸ç§è®­ç»æ¹æ³æ¥çæå·ææ­¤ç±»éèåä¾çç¥ç»ç½ç»ãæä»¬çåºåæ¨å¨ç¨äºæµè¯ NN éªè¯å¨çå¥å¨æ§ï¼å¹¶å¨å·²ç¥å­å¨éèåä¾çæåµä¸è¯å«éè¯¯å£°ç§°çå¯éªè¯æ§ãæä»¬ç³»ç»å°æå»ºåºåï¼å¹¶å¨ä¸åçæ¨¡åæ¶æãæ¿æ´»å½æ°ãè¾å¥å¤§å°åæ°å¨åå¾ä¸­çæå®ä¾ãæä»¬è¯æäºæä»¬çåºåæåå°è¯å«äºæåè¿ç NN éªè¯å¨ä¸­çéè¯¯ï¼ä»¥ååæçéè¯¯ï¼ä¸ºæé« NN éªè¯å¨æµè¯çå¯é æ§æä¾äºè³å³éè¦çä¸æ­¥ãæä»¬çä»£ç å¯å¨ https://github.com/MVP-Harry/SoundnessBench è·å¾ï¼æä»¬çåºåå¯å¨ https://huggingface.co/datasets/SoundnessBench/SoundnessBench è·å¾ã

##### **Large Language Models show both individual and collective creativity comparable to humans**
2412.03151v1 by Luning Sun, Yuzhuo Yuan, Yuan Yao, Yanyan Li, Hao Zhang, Xing Xie, Xiting Wang, Fang Luo, David Stillwell

Artificial intelligence has, so far, largely automated routine tasks, but
what does it mean for the future of work if Large Language Models (LLMs) show
creativity comparable to humans? To measure the creativity of LLMs
holistically, the current study uses 13 creative tasks spanning three domains.
We benchmark the LLMs against individual humans, and also take a novel approach
by comparing them to the collective creativity of groups of humans. We find
that the best LLMs (Claude and GPT-4) rank in the 52nd percentile against
humans, and overall LLMs excel in divergent thinking and problem solving but
lag in creative writing. When questioned 10 times, an LLM's collective
creativity is equivalent to 8-10 humans. When more responses are requested, two
additional responses of LLMs equal one extra human. Ultimately, LLMs, when
optimally applied, may compete with a small group of humans in the future of
work.

æè¦ï¼ç®åçºæ­¢ï¼äººå·¥æºæ§å·²ç¶èªååäºå¤§éçä¾è¡å·¥ä½ï¼ä½å¦æå¤§åèªè¨æ¨¡å (LLM) å±ç¾åºèäººé¡ç¸ç¶çåµé åï¼éå°å·¥ä½çæªä¾èè¨ä»£è¡¨ä»éº¼æç¾©ï¼çºäºå¨é¢è¡¡é LLM çåµé åï¼ç®åçç ç©¶ä½¿ç¨äºæ¶µèä¸åé åç 13 é åµé åä»»åãæåä»¥åå¥äººé¡çºåºæºä¾è©é LLMï¼ä¸¦æ¡ç¨åµæ°çæ¹æ³ï¼å°å®åèäººé¡ç¾¤é«çéé«åµé åé²è¡æ¯è¼ãæåç¼ç¾ï¼æä½³ç LLMï¼Claude å GPT-4ï¼å¨èäººé¡çæ¯è¼ä¸­æåç¬¬ 52 åç¾åä½ï¼æ´é«èè¨ï¼LLM å¨ç¼æ£æ§æèååé¡è§£æ±ºæ¹é¢è¡¨ç¾åºè²ï¼ä½å¨åµæå¯«ä½æ¹é¢åè½å¾ãç¶è¢«è©¢å 10 æ¬¡æï¼LLM çéé«åµé åç­æ¼ 8-10 åäººé¡ãç¶è¦æ±æä¾æ´å¤åææï¼LLM çå©åé¡å¤åæç­æ¼ä¸åé¡å¤çäººé¡ãæçµï¼å¨æä½³æç¨æï¼LLM å¯è½å¨æªä¾çè·å ´ä¸­èä¸å°ç¾¤äººé¡ç«¶ç­ã

##### **Fine-Grained Behavior Simulation with Role-Playing Large Language Model on Social Media**
2412.03148v1 by Kun Li, Chenwei Dai, Wei Zhou, Songlin Hu

Large language models (LLMs) have demonstrated impressive capabilities in
role-playing tasks. However, there is limited research on whether LLMs can
accurately simulate user behavior in real-world scenarios, such as social
media. This requires models to effectively analyze a user's history and
simulate their role. In this paper, we introduce \textbf{FineRob}, a novel
fine-grained behavior simulation dataset. We collect the complete behavioral
history of 1,866 distinct users across three social media platforms. Each
behavior is decomposed into three fine-grained elements: object, type, and
content, resulting in 78.6k QA records. Based on FineRob, we identify two
dominant reasoning patterns in LLMs' behavior simulation processes and propose
the \textbf{OM-CoT} fine-tuning method to enhance the capability. Through
comprehensive experiments, we conduct an in-depth analysis of key factors of
behavior simulation and also demonstrate the effectiveness of OM-CoT
approach\footnote{Code and dataset are available at
\url{https://github.com/linkseed18612254945/FineRob}}

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å·²å¨è§è²æ®æ¼ä»»åä¸­å±ç¤ºäºä»¤äººå°è±¡æ·±å»çè½åãç¶èï¼éæ¼ LLM æ¯å¦å¯ä»¥å¨ç¾å¯¦ä¸çå ´æ¯ä¸­æºç¢ºæ¨¡æ¬ä½¿ç¨èè¡çºï¼ä¾å¦ç¤¾äº¤åªé«ï¼çç ç©¶æéãééè¦æ¨¡åææåæä½¿ç¨èçæ­·å²è¨éä¸¦æ¨¡æ¬å¶è§è²ãå¨æ¬æä¸­ï¼æåä»ç´¹äº FineRobï¼éæ¯ä¸åæ°ç©çç´°ç²åº¦è¡çºæ¨¡æ¬æ¸æéãæåæ¶éäº 1,866 åä¸åä½¿ç¨èå¨ä¸åç¤¾äº¤åªé«å¹³å°ä¸çå®æ´è¡çºæ­·å²è¨éãæ¯åè¡çºè¢«åè§£çºä¸åç´°ç²åº¦åç´ ï¼å°è±¡ãé¡ååå§å®¹ï¼ç¢çäº 78.6k ååç­è¨éãåºæ¼ FineRobï¼æåå¨ LLM çè¡çºæ¨¡æ¬éç¨ä¸­è­å¥äºå©ç¨®ä¸»è¦çæ¨çæ¨¡å¼ï¼ä¸¦æåºäº OM-CoT å¾®èª¿æ¹æ³ä¾å¢å¼·è½åãééå¨é¢çå¯¦é©ï¼æåå°è¡çºæ¨¡æ¬çééµå ç´ é²è¡äºæ·±å¥åæï¼ä¸¦å±ç¤ºäº OM-CoT æ¹æ³çæææ§ã

##### **Robust Multi-bit Text Watermark with LLM-based Paraphrasers**
2412.03123v1 by Xiaojun Xu, Jinghan Jia, Yuanshun Yao, Yang Liu, Hang Li

We propose an imperceptible multi-bit text watermark embedded by paraphrasing
with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave
differently so that their paraphrasing difference reflected in the text
semantics can be identified by a trained decoder. To embed our multi-bit
watermark, we use two paraphrasers alternatively to encode the pre-defined
binary code at the sentence level. Then we use a text classifier as the decoder
to decode each bit of the watermark. Through extensive experiments, we show
that our watermarks can achieve over 99.99\% detection AUC with small (1.1B)
text paraphrasers while keeping the semantic information of the original
sentence. More importantly, our pipeline is robust under word substitution and
sentence paraphrasing perturbations and generalizes well to
out-of-distributional data. We also show the stealthiness of our watermark with
LLM-based evaluation. We open-source the code:
https://github.com/xiaojunxu/multi-bit-text-watermark.

æè¦ï¼æåæåºä¸åç± LLM éç¾©åµå¥çé£ä»¥å¯è¦ºçå¤ä½åæå­æµ®æ°´å°ãæåå¾®èª¿äºä¸å° LLM éç¾©å¨ï¼å®åè¢«è¨­è¨æè¡¨ç¾ä¸åï¼éæ¨£å®åçéç¾©å·®ç°åæ å¨æå­èªæä¸­ï¼å°±è½è¢«ä¸åè¨ç·´éçè§£ç¢¼å¨è­å¥ãçºäºåµå¥æåçå¤ä½åæµ®æ°´å°ï¼æåäº¤æ¿ä½¿ç¨å©åéç¾©å¨ï¼å¨å¥å­å±¤ç´ç·¨ç¢¼é å®ç¾©çäºé²ä½ç¢¼ãç¶å¾æåä½¿ç¨æå­åé¡å¨ä½çºè§£ç¢¼å¨ï¼è§£ç¢¼æµ®æ°´å°çæ¯ä¸æ¯ç¹ãééå»£æ³çå¯¦é©ï¼æåè­ææåçæµ®æ°´å°å¯ä»¥éå°è¶é 99.99% çåµæ¸¬ AUCï¼æå­éç¾©å¨å¾å°ï¼1.1Bï¼ï¼åæä¿çåå§å¥å­çèªæè³è¨ãæ´éè¦çæ¯ï¼æåçç®¡éå¨è©å½æ¿æåå¥å­éç¾©æ¾åä¸å·æç©©å¥æ§ï¼ä¸¦ä¸å¯ä»¥å¾å¥½å°æ¦æ¬å°åå¸å¤è³æãæåéå±ç¤ºäºæåæµ®æ°´å°çé±å¯æ§ï¼ä¸¦ä½¿ç¨åºæ¼ LLM çè©ä¼°ãæåéæ¾åå§ç¢¼ï¼
https://github.com/xiaojunxu/multi-bit-text-watermarkã

##### **Experience-driven discovery of planning strategies**
2412.03111v1 by Ruiqi He, Falk Lieder

One explanation for how people can plan efficiently despite limited cognitive
resources is that we possess a set of adaptive planning strategies and know
when and how to use them. But how are these strategies acquired? While previous
research has studied how individuals learn to choose among existing strategies,
little is known about the process of forming new planning strategies. In this
work, we propose that new planning strategies are discovered through
metacognitive reinforcement learning. To test this, we designed a novel
experiment to investigate the discovery of new planning strategies. We then
present metacognitive reinforcement learning models and demonstrate their
capability for strategy discovery as well as show that they provide a better
explanation of human strategy discovery than alternative learning mechanisms.
However, when fitted to human data, these models exhibit a slower discovery
rate than humans, leaving room for improvement.

æè¦ï¼äººååç®¡èªç¥è³æºæéï¼å»è½ææçå°è¦åï¼å¶ä¸­ä¸åè§£éæ¯æåææä¸çµé©ææ§è¦åç­ç¥ï¼ä¸¦ç¥éä½æä»¥åå¦ä½ä½¿ç¨å®åãä½éäºç­ç¥æ¯å¦ä½ç¿å¾çå¢ï¼éç¶ååçç ç©¶æ¢è¨äºåäººå¦ä½å­¸ç¿å¨æ¢æç­ç¥ä¸­ååºé¸æï¼ä½å°æ¼å½¢ææ°è¦åç­ç¥çéç¨æç¥çå°ãå¨éé ç ç©¶ä¸­ï¼æåæåºæ°çè¦åç­ç¥æ¯ééåèªç¥å¼·åå­¸ç¿èç¼ç¾çãçºäºæ¸¬è©¦éä¸é»ï¼æåè¨­è¨äºä¸é æ°ç©çå¯¦é©ä¾ç ç©¶æ°è¦åç­ç¥çç¼ç¾ãæ¥èï¼æåæåºåèªç¥å¼·åå­¸ç¿æ¨¡åï¼ä¸¦å±ç¤ºå®åç¼ç¾ç­ç¥çè½åï¼ä»¥åå®åæ¯å¶ä»å­¸ç¿æ©å¶æ´è½è§£éäººé¡çç­ç¥ç¼ç¾ãç¶èï¼ç¶éäºæ¨¡åå¥ç¨æ¼äººé¡è³ææï¼å®åè¡¨ç¾åºæ¯äººé¡æ´æ¢çç¼ç¾çï¼å æ­¤æé²æ­¥çç©ºéã

##### **CredID: Credible Multi-Bit Watermark for Large Language Models Identification**
2412.03107v1 by Haoyu Jiang, Xuhong Wang, Ping Yi, Shanzhe Lei, Yilun Lin

Large Language Models (LLMs) are widely used in complex natural language
processing tasks but raise privacy and security concerns due to the lack of
identity recognition. This paper proposes a multi-party credible watermarking
framework (CredID) involving a trusted third party (TTP) and multiple LLM
vendors to address these issues. In the watermark embedding stage, vendors
request a seed from the TTP to generate watermarked text without sending the
user's prompt. In the extraction stage, the TTP coordinates each vendor to
extract and verify the watermark from the text. This provides a credible
watermarking scheme while preserving vendor privacy. Furthermore, current
watermarking algorithms struggle with text quality, information capacity, and
robustness, making it challenging to meet the diverse identification needs of
LLMs. Thus, we propose a novel multi-bit watermarking algorithm and an
open-source toolkit to facilitate research. Experiments show our CredID
enhances watermark credibility and efficiency without compromising text
quality. Additionally, we successfully utilized this framework to achieve
highly accurate identification among multiple LLM vendors.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å»£æ³ç¨æ¼è¤éçèªç¶èªè¨èçä»»åï¼ä½ç±æ¼ç¼ºä¹èº«åè­å¥ï¼å æ­¤å¼ç¼äºé±ç§åå®å¨åé¡ãæ¬ææåºäºä¸åå¤æ¹å¯ä¿¡æµ®æ°´å°æ¶æ§ (CredID)ï¼å¶ä¸­æ¶åä¸ååä¿¡ä»»çç¬¬ä¸æ¹ (TTP) åå¤å LLM ä¾æåï¼ä»¥è§£æ±ºéäºåé¡ãå¨æµ®æ°´å°åµå¥éæ®µï¼ä¾æåæå TTP è¦æ±ä¸åç¨®å­ï¼ä»¥ç¢çæµ®æ°´å°æå­ï¼èä¸ç¨å³éä½¿ç¨èçæç¤ºãå¨èåéæ®µï¼TTP æåèª¿æ¯åä¾æåï¼ä»¥å¾æå­ä¸­èååé©è­æµ®æ°´å°ãéæä¾äºä¸åå¯ä¿¡çæµ®æ°´å°æ¶æ§ï¼åæä¿è­·ä¾æåçé±ç§ãæ­¤å¤ï¼ç®åçæµ®æ°´å°æ¼ç®æ³å¨æå­åè³ªãè³è¨å®¹éåç©©å¥æ§æ¹é¢é½é¢è¨ææ°ï¼éä½¿å¾é£ä»¥æ»¿è¶³ LLM å¤æ¨£åçè­å¥éæ±ãå æ­¤ï¼æåæåºäºä¸ç¨®æ°ç©çå¤ä½åæµ®æ°´å°æ¼ç®æ³åä¸åéæºå·¥å·åï¼ä»¥å©æ¼ç ç©¶ãå¯¦é©é¡¯ç¤ºï¼æåç CredID å¢å¼·äºæµ®æ°´å°çå¯ä¿¡åº¦åæçï¼åæä¸æå®³æå­åè³ªãæ­¤å¤ï¼æåæåå°å©ç¨éåæ¶æ§ï¼å¨å¤å LLM ä¾æåä¹ééå°äºé«åº¦æºç¢ºçè­å¥ã

##### **ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning**
2412.03104v1 by Zhe Xie, Zeyan Li, Xiao He, Longlong Xu, Xidao Wen, Tieying Zhang, Jianjun Chen, Rui Shi, Dan Pei

Understanding time series is crucial for its application in real-world
scenarios. Recently, large language models (LLMs) have been increasingly
applied to time series tasks, leveraging their strong language capabilities to
enhance various applications. However, research on multimodal LLMs (MLLMs) for
time series understanding and reasoning remains limited, primarily due to the
scarcity of high-quality datasets that align time series with textual
information. This paper introduces ChatTS, a novel MLLM designed for time
series analysis. ChatTS treats time series as a modality, similar to how vision
MLLMs process images, enabling it to perform both understanding and reasoning
with time series. To address the scarcity of training data, we propose an
attribute-based method for generating synthetic time series with detailed
attribute descriptions. We further introduce Time Series Evol-Instruct, a novel
approach that generates diverse time series Q&As, enhancing the model's
reasoning capabilities. To the best of our knowledge, ChatTS is the first MLLM
that takes multivariate time series as input, which is fine-tuned exclusively
on synthetic datasets. We evaluate its performance using benchmark datasets
with real-world data, including six alignment tasks and four reasoning tasks.
Our results show that ChatTS significantly outperforms existing vision-based
MLLMs (e.g., GPT-4o) and text/agent-based LLMs, achieving a 46.0% improvement
in alignment tasks and a 25.8% improvement in reasoning tasks.

æè¦ï¼<paragraph>äºè§£æéåºåå°æ¼å¶å¨ç¾å¯¦ä¸çä¸­çæç¨è³ééè¦ãæè¿ï¼å¤§åèªè¨æ¨¡å (LLM) å·²è¶ä¾è¶å¤å°æç¨æ¼æéåºåä»»åï¼å©ç¨å¶å¼·å¤§çèªè¨è½åä¾å¢å¼·åç¨®æç¨ãç¶èï¼éå°æéåºåçè§£åæ¨ççå¤æ¨¡æ LLM (MLLM) çç ç©¶ä»ç¶æéï¼éä¸»è¦æ¯å çºç¼ºä¹å°æéåºåèææ¬ä¿¡æ¯å°é½çé«åè³ªæ¸æéãæ¬æä»ç´¹äº ChatTSï¼éæ¯ä¸ç¨®å°çºæéåºååæè¨­è¨çæ°å MLLMãChatTS å°æéåºåè¦çºä¸ç¨®æ¨¡æï¼é¡ä¼¼æ¼è¦è¦º MLLM èçååçæ¹å¼ï¼ä½¿å¶è½å¤ å°æéåºåé²è¡çè§£åæ¨çãçºäºè§£æ±ºè¨ç·´æ¸æçç¨ç¼ºæ§ï¼æåæåºäºä¸ç¨®åºæ¼å±¬æ§çæ¹æ³ï¼ç¨æ¼çæå·æè©³ç´°å±¬æ§æè¿°çåææéåºåãæåé²ä¸æ­¥å¼å¥äºæéåºå Evol-Instructï¼éæ¯ä¸ç¨®çæå¤æ¨£åæéåºååç­çæ°æ¹æ³ï¼å¢å¼·äºæ¨¡åçæ¨çè½åãææåæç¥ï¼ChatTS æ¯ç¬¬ä¸åå°å¤è®éæéåºåä½çºè¼¸å¥ç MLLMï¼å®å°ééå°åææ¸æéé²è¡å¾®èª¿ãæåä½¿ç¨åå«çå¯¦ä¸çæ¸æçåºæºæ¸æéè©ä¼°å¶æ§è½ï¼åæ¬å­åå°é½ä»»ååååæ¨çä»»åãæåççµæè¡¨æï¼ChatTS æé¡¯åªæ¼ç¾æçåºæ¼è¦è¦ºç MLLMï¼ä¾å¦ GPT-4oï¼ååºæ¼ææ¬/ä»£çç LLMï¼å¨å°é½ä»»åä¸­æ¹é²äº 46.0%ï¼å¨æ¨çä»»åä¸­æ¹é²äº 25.8%ã</paragraph>

##### **A surprisal oracle for when every layer counts**
2412.03098v1 by Xudong Hong, Sharid LoÃ¡iciga, Asad Sayeed

Active Curriculum Language Modeling (ACLM; Hong et al., 2023) is a learner
directed approach to training a language model. We proposed the original
version of this process in our submission to the BabyLM 2023 task, and now we
propose an updated ACLM process for the BabyLM 2024 task. ACLM involves an
iteratively- and dynamically-constructed curriculum informed over the training
process by a model of uncertainty; other training items that are similarly
uncertain to a least certain candidate item are prioritized. Our new process
improves the similarity model so that it is more dynamic, and we run ACLM over
the most successful model from the BabyLM 2023 task: ELC-BERT (Charpentier and
Samuel, 2023). We find that while our models underperform on fine-grained
grammatical inferences, they outperform the BabyLM 2024 official base-lines on
common-sense and world-knowledge tasks. We make our code available at https:
//github.com/asayeed/ActiveBaby.

æè¦ï¼ä¸»åå¼èª²ç¨èªè¨æ¨¡å (ACLMï¼Hong ç­äººï¼2023) æ¯ä¸ç¨®ç±å­¸ç¿èå¼å°ï¼ç¨æ¼è¨ç·´èªè¨æ¨¡åçæ¹æ³ãæåå¨æäº¤çµ¦ BabyLM 2023 ä»»åææåºäºéåæµç¨çåå§çæ¬ï¼ç¾å¨æåçº BabyLM 2024 ä»»åæåºä¸åæ´æ°ç ACLM æµç¨ãACLM æ¶åä¸ååè¦ä¸åæå»ºæ§çèª²ç¨ï¼ä¸¦å¨è¨ç·´éç¨ä¸­ç±ä¸åä¸ç¢ºå®æ§æ¨¡åæä¾è³è¨ï¼å¶ä»è¨ç·´é ç®èè³å°ä¸åæä¸ç¢ºå®çåé¸é ç®ç¸ä¼¼çä¸ç¢ºå®é ç®æè¢«åªåèçãæåçæµç¨æ¹é²äºç¸ä¼¼æ§æ¨¡åï¼ä½¿å¶æ´å·åææ§ï¼æåå¨ BabyLM 2023 ä»»åä¸­ææåçæ¨¡åï¼ELC-BERT (Charpentier å Samuelï¼2023) ä¸å·è¡ ACLMãæåç¼ç¾ï¼åç®¡æåçæ¨¡åå¨ç´°å¾®çèªæ³æ¨è«ä¸è¡¨ç¾ä¸ä½³ï¼ä½å®åå¨å¸¸è­åä¸çç¥è­ä»»åä¸åªæ¼ BabyLM 2024 å®æ¹åºæºç·ãæåå¨ https://github.com/asayeed/ActiveBaby ä¸æä¾æåçç¨å¼ç¢¼ã

##### **TOOL-ED: Enhancing Empathetic Response Generation with the Tool Calling Capability of LLM**
2412.03096v1 by Huiying Cao, Yiqun Zhang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang

Empathetic conversation is a crucial characteristic in daily conversations
between individuals. Nowadays, Large Language models (LLMs) have shown
outstanding performance in generating empathetic responses. Knowledge bases
like COMET can assist LLMs in mitigating illusions and enhancing the
understanding of users' intentions and emotions. However, models remain heavily
reliant on fixed knowledge bases and unrestricted incorporation of external
knowledge can introduce noise. Tool learning is a flexible end-to-end approach
that assists LLMs in handling complex problems. In this paper, we propose
Emotional Knowledge Tool Calling (EKTC) framework, which encapsulates the
commonsense knowledge bases as empathetic tools, enabling LLMs to integrate
external knowledge flexibly through tool calling. In order to adapt the models
to the new task, we construct a novel dataset TOOL-ED based on the
EMPATHETICMPATHETIC DIALOGUE (ED) dataset. We validate EKTC on the ED dataset,
and the experimental results demonstrate that our framework can enhance the
ability of LLMs to generate empathetic responses effectively.

æè¦ï¼åçå¿å°è©±æ¯åäººæ¥å¸¸å°è©±ä¸­è³ééè¦çç¹è³ªãå¦ä»ï¼å¤§åèªè¨æ¨¡å (LLM) å¨ç¢çåçå¿åææ¹é¢è¡¨ç¾ååºãå COMET éæ¨£çç¥è­åº«å¯ä»¥åå© LLM æ¸è¼é¯è¦ºï¼ä¸¦å¢å¼·å°ä½¿ç¨èæååæç·ççè§£ãç¶èï¼æ¨¡åä¾èé«åº¦ä¾è³´åºå®ç¥è­åº«ï¼èå¤é¨ç¥è­çä¸åéç´å¥å¯è½æå¼å¥éè¨ãå·¥å·å­¸ç¿æ¯ä¸ç¨®éæ´»çç«¯å°ç«¯æ¹æ³ï¼å¯åå© LLM èçè¤éçåé¡ãå¨æ¬æä¸­ï¼æåæåºæç·ç¥è­å·¥å·å¼å« (EKTC) æ¶æ§ï¼å®å°å¸¸è­ç¥è­åº«å°è£çºåçå¿å·¥å·ï¼è® LLM è½å¤ ééå·¥å·å¼å«éæ´»å°æ´åå¤é¨ç¥è­ãçºäºè®æ¨¡åé©ææ°ä»»åï¼æåæ ¹æåçå¿å°è©± (ED) è³æéå»ºæ§äºä¸åæ°ç©çè³æé TOOL-EDãæåå¨ ED è³æéä¸é©è­ EKTCï¼èå¯¦é©çµæè­ææåçæ¶æ§å¯ä»¥ææå¢å¼· LLM ç¢çåçå¿åæçè½åã

##### **Revolve: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization**
2412.03092v1 by Peiyan Zhang, Haibo Jin, Leyang Hu, Xinnuo Li, Liying Kang, Man Luo, Yangqiu Song, Haohan Wang

Recent advancements in large language models (LLMs) have significantly
enhanced the ability of LLM-based systems to perform complex tasks through
natural language processing and tool interaction. However, optimizing these
LLM-based systems for specific tasks remains challenging, often requiring
manual interventions like prompt engineering and hyperparameter tuning.
Existing automatic optimization methods, such as textual feedback-based
techniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to
using immediate derivatives in traditional numerical gradient descent. However,
relying solely on such feedback can be limited when the adjustments made in
response to this feedback are either too small or fluctuate irregularly,
potentially slowing down or even stalling the optimization process. To overcome
these challenges, more adaptive methods are needed, especially in situations
where the system's response is evolving slowly or unpredictably. In this paper,
we introduce REVOLVE, an optimization method that tracks how "R"esponses
"EVOLVE" across iterations in LLM systems. By focusing on the evolution of
responses over time, REVOLVE enables more stable and effective optimization by
making thoughtful, progressive adjustments at each step. Experimental results
demonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8%
improvement in prompt optimization, a 20.72% gain in solution refinement, and a
29.17% increase in code optimization. Additionally, REVOLVE converges in fewer
iterations, resulting in significant computational savings. These advantages
highlight its adaptability and efficiency, positioning REVOLVE as a valuable
tool for optimizing LLM-based systems and accelerating the development of
next-generation AI technologies. Code is available at:
https://github.com/Peiyance/REVOLVE.

æè¦ï¼<paragraph>å¤§åèªè¨æ¨¡å (LLM) çææ°é²å±é¡¯èæåäº LLM åºæ¼ç³»çµ±ééèªç¶èªè¨èçåå·¥å·äºåå·è¡è¤éä»»åçè½åãç¶èï¼éå°ç¹å®ä»»åæä½³åéäº LLM åºæ¼ç³»çµ±ä»ç¶å·æææ°æ§ï¼éå¸¸éè¦æåä»å¥ï¼ä¾å¦æç¤ºå·¥ç¨åè¶åæ¸èª¿æ´ãç¾æçèªåæä½³åæ¹æ³ï¼ä¾å¦åºæ¼æå­åé¥çæè¡ (ä¾å¦ TextGrad)ï¼å¾åæ¼éæ³¨ç«å³åé¥ï¼é¡ä¼¼æ¼å¨å³çµ±æ¸å¼æ¢¯åº¦ä¸éä¸­ä½¿ç¨ç«å³å°æ¸ãç¶èï¼åä¾è³´æ­¤é¡åé¥å¯è½æåå°éå¶ï¼ç¶æ ¹ææ­¤åé¥é²è¡çèª¿æ´éå°æä¸è¦åæ³¢åæï¼å¯è½ææ¸æ¢çè³åæ­¢æä½³åç¨åºãçºäºåæéäºææ°ï¼éè¦æ´å¤é©ææ§æ¹æ³ï¼ç¹å¥æ¯å¨ç³»çµ±åæç·©æ¢æé£ä»¥é æ¸¬çææ³ä¸ãå¨æ¬æä¸­ï¼æåä»ç´¹ REVOLVEï¼éæ¯ä¸ç¨®æä½³åæ¹æ³ï¼å®è¿½è¹¤ LLM ç³»çµ±ä¸­ãRãesponses å¦ä½å¨è¿­ä»£ä¸­ãEVOLVEããéééæ³¨åæé¨æéçæ¼è®ï¼REVOLVE è½å¤ å¨æ¯åæ­¥é©é²è¡æ·±æçæ®çæ¼¸é²å¼èª¿æ´ï¼é²èå¯¦ç¾æ´ç©©å®ä¸ææçæä½³åãå¯¦é©çµæè­æ REVOLVE åªæ¼ç«¶ç­åºæºï¼å¨æç¤ºæä½³åæ¹é¢æåäº 7.8%ï¼å¨è§£æ±ºæ¹æ¡ç²¾çæ¹é¢æåäº 20.72%ï¼å¨ç¨å¼ç¢¼æä½³åæ¹é¢æåäº 29.17%ãæ­¤å¤ï¼REVOLVE å¨è¼å°è¿­ä»£ä¸­æ¶æï¼å¾èç¯çäºå¤§éçéç®ãéäºåªé»çªé¡¯äºå®çé©ææ§åæçï¼å° REVOLVE å®ä½çºæä½³å LLM åºæ¼ç³»çµ±åå éä¸ä¸ä»£ AI æè¡ç¼å±çå¯¶è²´å·¥å·ãç¨å¼ç¢¼å¯å¨ä»¥ä¸ä½ç½®åå¾ï¼https://github.com/Peiyance/REVOLVEã</paragraph>

##### **ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error Correction**
2412.03075v1 by Victor Junqiu Wei, Weicheng Wang, Di Jiang, Yuanfeng Song, Lu Wang

Automatic speech Recognition (ASR) is a fundamental and important task in the
field of speech and natural language processing. It is an inherent building
block in many applications such as voice assistant, speech translation, etc.
Despite the advancement of ASR technologies in recent years, it is still
inevitable for modern ASR systems to have a substantial number of erroneous
recognition due to environmental noise, ambiguity, etc. Therefore, the error
correction in ASR is crucial.
  Motivated by this, this paper studies ASR error correction in the Chinese
language, which is one of the most popular languages and enjoys a large number
of users in the world. We first create a benchmark dataset named \emph{ASR-EC}
that contains a wide spectrum of ASR errors generated by industry-grade ASR
systems. To the best of our knowledge, it is the first Chinese ASR error
correction benchmark. Then, inspired by the recent advances in \emph{large
language models (LLMs)}, we investigate how to harness the power of LLMs to
correct ASR errors. We apply LLMs to ASR error correction in three paradigms.
The first paradigm is prompting, which is further categorized as zero-shot,
few-shot, and multi-step. The second paradigm is finetuning, which finetunes
LLMs with ASR error correction data. The third paradigm is multi-modal
augmentation, which collectively utilizes the audio and ASR transcripts for
error correction. Extensive experiments reveal that prompting is not effective
for ASR error correction. Finetuning is effective only for a portion of LLMs.
Multi-modal augmentation is the most effective method for error correction and
achieves state-of-the-art performance.

æè¦ï¼<paragraph>èªåèªé³è¾¨è­ (ASR) æ¯èªé³èèªç¶èªè¨èçé åä¸­çä¸é åºæ¬ä¸éè¦çä»»åãå®æ¯è¨±å¤æç¨ç¨å¼ä¸­åºæççµæé¨åï¼ä¾å¦èªé³å©çãèªé³ç¿»è­¯ç­ãåç®¡è¿å¹´ä¾ ASR æè¡é²æ­¥ï¼ä½ç¾ä»£ ASR ç³»çµ±ä»é£åæå ç°å¢åªé³ãæ­§ç¾©ç­å ç´ ç¢çå¤§éé¯èª¤è¾¨è­ãå æ­¤ï¼ASR ä¸­çé¯èª¤æ ¡æ­£è³ééè¦ã
åæ­¤åç¼ï¼æ¬æç ç©¶äºä¸­æ ASR é¯èª¤æ ¡æ­£ï¼ä¸­ææ¯ææµè¡çèªè¨ä¹ä¸ï¼å¨å¨çææå¤§éçä½¿ç¨èãæåé¦åå»ºç«äºä¸ååçº \emph{ASR-EC} çåºæºè³æéï¼å¶ä¸­åå«ç±ç¢æ¥­ç´ ASR ç³»çµ±ç¢ççåç¨® ASR é¯èª¤ãææåæç¥ï¼éæ¯ç¬¬ä¸åä¸­æ ASR é¯èª¤æ ¡æ­£åºæºãæ¥èï¼åå° \emph{å¤§åèªè¨æ¨¡å (LLM)} è¿æé²å±çåç¼ï¼æåæ¢è¨å¦ä½å©ç¨ LLM çåéä¾æ ¡æ­£ ASR é¯èª¤ãæåå° LLM æç¨æ¼ ASR é¯èª¤æ ¡æ­£çä¸ç¨®ç¯ä¾ãç¬¬ä¸åç¯ä¾æ¯æç¤ºï¼é²ä¸æ­¥åé¡çºé¶æ¬¡å­¸ç¿ãå°æ¬¡å­¸ç¿åå¤æ­¥é©ãç¬¬äºåç¯ä¾æ¯å¾®èª¿ï¼ä½¿ç¨ ASR é¯èª¤æ ¡æ­£è³æå¾®èª¿ LLMãç¬¬ä¸åç¯ä¾æ¯å¤æ¨¡å¼æ´åï¼å±åå©ç¨é³è¨å ASR è½éé²è¡é¯èª¤æ ¡æ­£ãå¤§éçå¯¦é©é¡¯ç¤ºï¼æç¤ºå°æ¼ ASR é¯èª¤æ ¡æ­£ç¡æãå¾®èª¿åå°é¨å LLM ææãå¤æ¨¡å¼æ´åæ¯é¯èª¤æ ¡æ­£æææçæ¹æ³ï¼ä¸¦éå°äºæåé²çæè½ã</paragraph>

##### **Analytic Study of Text-Free Speech Synthesis for Raw Audio using a Self-Supervised Learning Model**
2412.03074v1 by Joonyong Park, Daisuke Saito, Nobuaki Minematsu

We examine the text-free speech representations of raw audio obtained from a
self-supervised learning (SSL) model by analyzing the synthesized speech using
the SSL representations instead of conventional text representations. Since raw
audio does not have paired speech representations as transcribed texts do,
obtaining speech representations from unpaired speech is crucial for augmenting
available datasets for speech synthesis. Specifically, the proposed speech
synthesis is conducted using discrete symbol representations from the SSL model
in comparison with text representations, and analytical examinations of the
synthesized speech have been carried out. The results empirically show that
using text representations is advantageous for preserving semantic information,
while using discrete symbol representations is superior for preserving acoustic
content, including prosodic and intonational information.

æè¦ï¼æåééåæåæèªé³ï¼ä½¿ç¨ SSL è¡¨ç¤ºæ³èéå³çµ±æå­è¡¨ç¤ºæ³ï¼ä¾æª¢è¦å¾èªç£ç£å¼å­¸ç¿ (SSL) æ¨¡ååå¾çç¡æå­èªé³è¡¨ç¤ºæ³ãç±æ¼åå§é³è¨æ²æåè½éæå­é£æ¨£çéå°èªé³è¡¨ç¤ºæ³ï¼å æ­¤å¾æªéå°çèªé³ä¸­åå¾èªé³è¡¨ç¤ºæ³å°æ¼æ´åèªé³åæçå¯ç¨è³æéè³ééè¦ãå·é«ä¾èªªï¼å»ºè­°çèªé³åææ¯ä½¿ç¨ä¾èª SSL æ¨¡åçé¢æ£ç¬¦èè¡¨ç¤ºæ³ï¼ä¸¦èæå­è¡¨ç¤ºæ³é²è¡æ¯è¼ï¼ä¸¦å°åæèªé³é²è¡åææª¢è¦ãçµæç¶é©æ§å°é¡¯ç¤ºï¼ä½¿ç¨æå­è¡¨ç¤ºæ³æå©æ¼ä¿çèªæè³è¨ï¼èä½¿ç¨é¢æ£ç¬¦èè¡¨ç¤ºæ³ååªæ¼ä¿çé³é¿å§å®¹ï¼åæ¬é»å¾åèªèª¿è³è¨ã

##### **Preference-based opponent shaping in differentiable games**
2412.03072v1 by Xinyu Qiao, Yudong Hu, Congying Han, Weiyan Wu, Tiande Guo

Strategy learning in game environments with multi-agent is a challenging
problem. Since each agent's reward is determined by the joint strategy, a
greedy learning strategy that aims to maximize its own reward may fall into a
local optimum. Recent studies have proposed the opponent modeling and shaping
methods for game environments. These methods enhance the efficiency of strategy
learning by modeling the strategies and updating processes of other agents.
However, these methods often rely on simple predictions of opponent strategy
changes. Due to the lack of modeling behavioral preferences such as cooperation
and competition, they are usually applicable only to predefined scenarios and
lack generalization capabilities. In this paper, we propose a novel
Preference-based Opponent Shaping (PBOS) method to enhance the strategy
learning process by shaping agents' preferences towards cooperation. We
introduce the preference parameter, which is incorporated into the agent's loss
function, thus allowing the agent to directly consider the opponent's loss
function when updating the strategy. We update the preference parameters
concurrently with strategy learning to ensure that agents can adapt to any
cooperative or competitive game environment. Through a series of experiments,
we verify the performance of PBOS algorithm in a variety of differentiable
games. The experimental results show that the PBOS algorithm can guide the
agent to learn the appropriate preference parameters, so as to achieve better
reward distribution in multiple game environments.

æè¦ï¼å¨å·æå¤æºè½é«çéæ²ç°å¢ä¸­é²è¡ç­ç¥å­¸ç¿æ¯ä¸åå·æææ°æ§çåé¡ãç±æ¼æ¯åæºè½é«ççåµæ¯ç±è¯åç­ç¥æ±ºå®çï¼å æ­¤æ¨å¨æå¤§åèªèº«çåµçè²ªå©ªå­¸ç¿ç­ç¥å¯è½æé·å¥å±é¨æåªãæè¿çç ç©¶æåºäºå°æå»ºæ¨¡åå¡é éæ²ç°å¢çæ¹æ³ãéäºæ¹æ³ééå°å¶ä»æºè½é«çç­ç¥åæ´æ°éç¨é²è¡å»ºæ¨¡ï¼æé«äºç­ç¥å­¸ç¿çæçãç¶èï¼éäºæ¹æ³éå¸¸ä¾è³´æ¼å°å°æç­ç¥è®åçç°¡å®é æ¸¬ãç±æ¼ç¼ºä¹å°åä½åç«¶ç­ç­è¡çºåå¥½çå»ºæ¨¡ï¼å®åéå¸¸åé©ç¨æ¼é å®ç¾©çå ´æ¯ï¼ä¸¦ä¸ç¼ºä¹æ³åè½åãå¨æ¬æä¸­ï¼æåæåºäºä¸ç¨®æ°çåºæ¼åå¥½çå°æå¡é  (PBOS) æ¹æ³ï¼ééå¡é æºè½é«å°åä½çåå¥½ä¾å¢å¼·ç­ç¥å­¸ç¿éç¨ãæåå¼å¥äºåå¥½åæ¸ï¼è©²åæ¸è¢«ç´å¥æºè½é«çæå¤±å½æ¸ä¸­ï¼å¾èåè¨±æºè½é«å¨æ´æ°ç­ç¥æç´æ¥èæ®å°æçæå¤±å½æ¸ãæåèç­ç¥å­¸ç¿åææ´æ°åå¥½åæ¸ï¼ä»¥ç¢ºä¿æºè½é«å¯ä»¥é©æä»»ä½åä½æç«¶ç­çéæ²ç°å¢ãééä¸ç³»åå¯¦é©ï¼æåé©è­äº PBOS æ¼ç®æ³å¨åç¨®å¯å¾®åéæ²ä¸­çæ§è½ãå¯¦é©çµæè¡¨æï¼PBOS æ¼ç®æ³å¯ä»¥å¼å°æºè½é«å­¸ç¿é©ç¶çåå¥½åæ¸ï¼å¾èå¨å¤åéæ²ç°å¢ä¸­å¯¦ç¾æ´å¥½ççåµåéã

##### **UTSD: Unified Time Series Diffusion Model**
2412.03068v1 by Xiangkai Ma, Xiaobin Hong, Wenzhong Li, Sanglu Lu

Transformer-based architectures have achieved unprecedented success in time
series analysis. However, facing the challenge of across-domain modeling,
existing studies utilize statistical prior as prompt engineering fails under
the huge distribution shift among various domains. In this paper, a Unified
Time Series Diffusion (UTSD) model is established for the first time to model
the multi-domain probability distribution, utilizing the powerful probability
distribution modeling ability of Diffusion. Unlike the autoregressive models
that capture the conditional probabilities of the prediction horizon to the
historical sequence, we use a diffusion denoising process to model the mixture
distribution of the cross-domain data and generate the prediction sequence for
the target domain directly utilizing conditional sampling. The proposed UTSD
contains three pivotal designs: (1) The condition network captures the
multi-scale fluctuation patterns from the observation sequence, which are
utilized as context representations to guide the denoising network to generate
the prediction sequence; (2) Adapter-based fine-tuning strategy, the
multi-domain universal representation learned in the pretraining stage is
utilized for downstream tasks in target domains; (3) The diffusion and
denoising process on the actual sequence space, combined with the improved
classifier free guidance as the conditional generation strategy, greatly
improves the stability and accuracy of the downstream task. We conduct
extensive experiments on mainstream benchmarks, and the pre-trained UTSD
outperforms existing foundation models on all data domains, exhibiting superior
zero-shot generalization ability. After training from scratch, UTSD achieves
comparable performance against domain-specific proprietary models. The
empirical results validate the potential of UTSD as a time series foundational
model.

æè¦ï¼<paragraph>åºæ¼ Transformer çæ¶æ§å¨æéåºååæä¸­ç²å¾äºåææªæçæåãç¶èï¼é¢å°è·¨åå»ºæ¨¡çææ°ï¼ç¾æçç ç©¶å©ç¨çµ±è¨åé©ä½çºæç¤ºå·¥ç¨ï¼å¨åç¨®åä¹éçå·¨å¤§åä½è½ç§»ä¸æå¤±æãå¨æ¬æä¸­ï¼é¦æ¬¡å»ºç«äºçµ±ä¸æéåºåæ´æ£ (UTSD) æ¨¡åä¾å°å¤åæ©çåä½é²è¡å»ºæ¨¡ï¼å©ç¨æ´æ£çå¼·å¤§æ©çåä½å»ºæ¨¡è½åãèææé æ¸¬ç¯åå°æ­·å²åºåæ¢ä»¶æ©ççèªè¿´æ­¸æ¨¡åä¸åï¼æåä½¿ç¨æ´æ£å»åªç¨åºå°è·¨åè³æçæ··ååä½é²è¡å»ºæ¨¡ï¼ä¸¦ç´æ¥å©ç¨æ¢ä»¶æ½æ¨£çºç®æ¨åç¢çé æ¸¬åºåãææåºç UTSD åå«ä¸åééµè¨­è¨ï¼(1) æ¢ä»¶ç¶²è·¯å¾è§å¯åºåä¸­ææå¤å°ºåº¦æ³¢åæ¨¡å¼ï¼éäºæ¨¡å¼è¢«ç¨ä½ä¸ä¸æè¡¨ç¤ºï¼ä»¥å¼å°å»åªç¶²è·¯ç¢çé æ¸¬åºåï¼(2) åºæ¼é©éå¨çå¾®èª¿ç­ç¥ï¼å¨é è¨ç·´éæ®µå­¸ç¿çå¤åéç¨è¡¨ç¤ºç¨æ¼ç®æ¨åçä¸æ¸¸ä»»åï¼(3) å¨å¯¦éåºåç©ºéä¸çæ´æ£åå»åªç¨åºï¼çµåä½çºæ¢ä»¶çæç­ç¥çæ¹è¯åé¡å¨èªç±å¼å°ï¼æ¥µå¤§å°æé«äºä¸æ¸¸ä»»åçç©©å®æ§åæºç¢ºæ§ãæåå¨ä¸»æµåºæºä¸é²è¡äºå»£æ³çå¯¦é©ï¼é åè¨ç·´ç UTSD å¨ææè³æåä¸é½åªæ¼ç¾æçåºç¤æ¨¡åï¼å±ç¾åºåè¶çé¶æ¬¡æ¹æ³åè½åãå¾é ­éå§è¨ç·´å¾ï¼UTSD å¨èç¹å®æ¼åçå°ææ¨¡åç¸æ¯æï¼éå°äºç¸ç¶çæè½ãå¯¦è­çµæé©è­äº UTSD ä½çºæéåºååºç¤æ¨¡åçæ½åã</paragraph>

##### **Point-GN: A Non-Parametric Network Using Gaussian Positional Encoding for Point Cloud Classification**
2412.03056v1 by Marzieh Mohammadi, Amir Salarpour

This paper introduces Point-GN, a novel non-parametric network for efficient
and accurate 3D point cloud classification. Unlike conventional deep learning
models that rely on a large number of trainable parameters, Point-GN leverages
non-learnable components-specifically, Farthest Point Sampling (FPS), k-Nearest
Neighbors (k-NN), and Gaussian Positional Encoding (GPE)-to extract both local
and global geometric features. This design eliminates the need for additional
training while maintaining high performance, making Point-GN particularly
suited for real-time, resource-constrained applications. We evaluate Point-GN
on two benchmark datasets, ModelNet40 and ScanObjectNN, achieving
classification accuracies of 85.29% and 85.89%, respectively, while
significantly reducing computational complexity. Point-GN outperforms existing
non-parametric methods and matches the performance of fully trained models, all
with zero learnable parameters. Our results demonstrate that Point-GN is a
promising solution for 3D point cloud classification in practical, real-time
environments.

æè¦ï¼æ¬æä»ç´¹ Point-GNï¼ä¸ç¨®ç¨æ¼é«æä¸æºç¢ºç 3D é»é²åé¡çæ°åéåæ¸ç¶²è·¯ãèä¾è³´å¤§éå¯è¨ç·´åæ¸çå³çµ±æ·±åº¦å­¸ç¿æ¨¡åä¸åï¼Point-GN å©ç¨ä¸å¯å­¸ç¿çåä»¶ï¼å·é«ä¾èªªï¼æé é»åæ¨£ (FPS)ãk æè¿é° (k-NN) åé«æ¯ä½ç½®ç·¨ç¢¼ (GPE)ï¼ä¾æåå±é¨åå¨å±å¹¾ä½ç¹å¾µãæ­¤è¨­è¨æ¶é¤äºé¡å¤è¨ç·´çéæ±ï¼åæç¶­æé«æ§è½ï¼ä½¿ Point-GN ç¹å¥é©åæ¼å³æãè³æºåéçæç¨ç¨å¼ãæåå¨å©ååºæºè³æé ModelNet40 å ScanObjectNN ä¸è©ä¼° Point-GNï¼åå¥éå° 85.29% å 85.89% çåé¡æºç¢ºåº¦ï¼åæå¤§å¹éä½éç®è¤éåº¦ãPoint-GN åªæ¼ç¾æçéåæ¸æ¹æ³ï¼ä¸¦èå®å¨è¨ç·´æ¨¡åçæ§è½ç¸å¹éï¼ææéäºé½ç¡éå¯å­¸ç¿åæ¸ãæåççµæè¡¨æï¼Point-GN æ¯å¨å¯¦éå³æç°å¢ä¸­é²è¡ 3D é»é²åé¡çæåéçè§£æ±ºæ¹æ¡ã

##### **Less is More: A Stealthy and Efficient Adversarial Attack Method for DRL-based Autonomous Driving Policies**
2412.03051v1 by Junchao Fan, Xuyang Lei, Xiaolin Chang, Jelena MiÅ¡iÄ, Vojislav B. MiÅ¡iÄ

Despite significant advancements in deep reinforcement learning (DRL)-based
autonomous driving policies, these policies still exhibit vulnerability to
adversarial attacks. This vulnerability poses a formidable challenge to the
practical deployment of these policies in autonomous driving. Designing
effective adversarial attacks is an indispensable prerequisite for enhancing
the robustness of these policies. In view of this, we present a novel stealthy
and efficient adversarial attack method for DRL-based autonomous driving
policies. Specifically, we introduce a DRL-based adversary designed to trigger
safety violations (e.g., collisions) by injecting adversarial samples at
critical moments. We model the attack as a mixed-integer optimization problem
and formulate it as a Markov decision process. Then, we train the adversary to
learn the optimal policy for attacking at critical moments without domain
knowledge. Furthermore, we introduce attack-related information and a
trajectory clipping method to enhance the learning capability of the adversary.
Finally, we validate our method in an unprotected left-turn scenario across
different traffic densities. The experimental results show that our method
achieves more than 90% collision rate within three attacks in most cases.
Furthermore, our method achieves more than 130% improvement in attack
efficiency compared to the unlimited attack method.

æè¦ï¼åç®¡å¨åºæ¼æ·±åº¦å¼·åå­¸ç¿ (DRL) çèªåé§é§æ¿ç­æ¹é¢åå¾äºé¡¯èé²å±ï¼ä½éäºæ¿ç­ä»ç¶å®¹æåå°å°ææ§æ»æãéç¨®èå¼±æ§å°å¨èªåé§é§ä¸­å¯¦éé¨ç½²éäºæ¿ç­æ§æäºå·¨å¤§çææ°ãè¨­è¨ææçå°ææ§æ»ææ¯å¢å¼·éäºæ¿ç­ç©©å¥æ§çå¿è¦åæ±ºæ¢ä»¶ãæéæ¼æ­¤ï¼æåæåºäºä¸ç¨®éå°åºæ¼ DRL çèªåé§é§æ¿ç­çæ°åé±è½ä¸ææçå°ææ§æ»ææ¹æ³ãå·é«ä¾èªªï¼æåå¼å¥äºä¸ååºæ¼ DRL çå°æï¼æ¨å¨ééå¨ééµæå»æ³¨å¥å°ææ§æ¨£æ¬ä¾è§¸ç¼å®å¨éè¦ï¼ä¾å¦ç¢°æï¼ãæåå°æ»æå»ºæ¨¡çºä¸åæ··åæ´æ¸åªååé¡ï¼ä¸¦å°å¶è¡¨è¿°çºé¦¬å¯å¤«æ±ºç­éç¨ãç¶å¾ï¼æåè¨ç·´å°æå¨æ²æé åç¥è­çææ³ä¸å­¸ç¿å¨ééµæå»æ»æçæä½³ç­ç¥ãæ­¤å¤ï¼æåå¼å¥äºæ»æç¸éä¿¡æ¯åè»è·¡åªè¼¯æ¹æ³ï¼ä»¥å¢å¼·å°æçå­¸ç¿è½åãæå¾ï¼æåå¨ä¸åçäº¤éå¯åº¦ä¸ï¼å¨ä¸åä¸åä¿è­·çå·¦è½å ´æ¯ä¸­é©è­äºæåçæ¨¡åãå¯¦é©çµæè¡¨æï¼å¨å¤§å¤æ¸ææ³ä¸ï¼æåçæ¨¡åå¨ä¸æ¬¡æ»æä¸­å¯¦ç¾äºè¶é 90% çç¢°æçãæ­¤å¤ï¼èç¡éå¶æ»ææ¹æ³ç¸æ¯ï¼æåçæ¨¡åå¨æ»ææçæ¹é¢å¯¦ç¾äºè¶é 130% çæ¹é²ã

##### **MRNet: Multifaceted Resilient Networks for Medical Image-to-Image Translation**
2412.03039v1 by Hyojeong Lee, Youngwan Jo, Inpyo Hong, Sanghyun Park

We propose a Multifaceted Resilient Network(MRNet), a novel architecture
developed for medical image-to-image translation that outperforms
state-of-the-art methods in MRI-to-CT and MRI-to-MRI conversion. MRNet
leverages the Segment Anything Model (SAM) to exploit frequency-based features
to build a powerful method for advanced medical image transformation. The
architecture extracts comprehensive multiscale features from diverse datasets
using a powerful SAM image encoder and performs resolution-aware feature fusion
that consistently integrates U-Net encoder outputs with SAM-derived features.
This fusion optimizes the traditional U-Net skip connection while leveraging
transformer-based contextual analysis. The translation is complemented by an
innovative dual-mask configuration incorporating dynamic attention patterns and
a specialized loss function designed to address regional mapping mismatches,
preserving both the gross anatomy and tissue details. Extensive validation
studies have shown that MRNet outperforms state-of-the-art architectures,
particularly in maintaining anatomical fidelity and minimizing translation
artifacts.

æè¦ï¼æåæåºä¸åå¤æ¹é¢çå½æ§ç¶²è·¯ (MRNet)ï¼éæ¯ä¸ååµæ°çæ¶æ§ï¼
éç¼ç¨æ¼é«å­¸å½±åè½å½±åçç¿»è­¯ï¼å¶åªæ¼ MRI è½ CT å MRI è½ MRI è½æçææ°æ¹æ³ãMRNet
å©ç¨ Segment Anything Model (SAM) ä¾å©ç¨åºæ¼é »ççç¹å¾µï¼ä»¥å»ºç«ä¸ç¨®å¼·å¤§çæ¹æ³ï¼ç¨æ¼åé²çé«å­¸å½±åè½æãæ­¤
æ¶æ§ä½¿ç¨å¼·å¤§ç SAM å½±åç·¨ç¢¼å¨å¾ä¸åçè³æéæåå¨é¢çå¤å°ºåº¦ç¹å¾µï¼ä¸¦å·è¡è§£æåº¦æç¥ç¹å¾µèåï¼æçºå° U-Net ç·¨ç¢¼å¨è¼¸åºè SAM è¡ççç¹å¾µæ´åå¨ä¸èµ·ã
æ­¤èåæä½³åå³çµ±ç U-Net è·³èºé£æ¥ï¼åæå©ç¨åºæ¼Transformerçä¸ä¸æåæãç¿»è­¯ç±ä¸ååµæ°çéé®ç½©éç½®è£åï¼å®çµåäºåææ³¨ææ¨¡å¼åä¸åå°éçæå¤±å½æ¸ï¼æ¨å¨è§£æ±ºååå°æä¸å¹éçåé¡ï¼åæä¿çäºæ´é«è§£åçµæ§åçµç¹ç´°ç¯ãå»£æ³çé©è­ç ç©¶é¡¯ç¤ºï¼MRNet åªæ¼æåé²çæ¶æ§ï¼ç¹å¥æ¯å¨ç¶­æè§£åä¿çåº¦åæå°åè½æå½å½±æ¹é¢ã

##### **MILLION: A General Multi-Objective Framework with Controllable Risk for Portfolio Management**
2412.03038v1 by Liwei Deng, Tianfu Wang, Yan Zhao, Kai Zheng

Portfolio management is an important yet challenging task in AI for FinTech,
which aims to allocate investors' budgets among different assets to balance the
risk and return of an investment. In this study, we propose a general
Multi-objectIve framework with controLLable rIsk for pOrtfolio maNagement
(MILLION), which consists of two main phases, i.e., return-related maximization
and risk control. Specifically, in the return-related maximization phase, we
introduce two auxiliary objectives, i.e., return rate prediction, and return
rate ranking, combined with portfolio optimization to remit the overfitting
problem and improve the generalization of the trained model to future markets.
Subsequently, in the risk control phase, we propose two methods, i.e.,
portfolio interpolation and portfolio improvement, to achieve fine-grained risk
control and fast risk adaption to a user-specified risk level. For the
portfolio interpolation method, we theoretically prove that the risk can be
perfectly controlled if the to-be-set risk level is in a proper interval. In
addition, we also show that the return rate of the adjusted portfolio after
portfolio interpolation is no less than that of the min-variance optimization,
as long as the model in the reward maximization phase is effective.
Furthermore, the portfolio improvement method can achieve greater return rates
while keeping the same risk level compared to portfolio interpolation.
Extensive experiments are conducted on three real-world datasets. The results
demonstrate the effectiveness and efficiency of the proposed framework.

æè¦ï¼æè³çµåç®¡çæ¯éèç§æä¸­äººå·¥æºè½çä¸é éè¦ä¸å·æææ°æ§çä»»åï¼å¶ç®çæ¯å¨ä¸åçè³ç¢ä¹éåéæè³èçé ç®ï¼ä»¥å¹³è¡¡æè³çé¢¨éªåå ±é¬ãå¨æ¬ç ç©¶ä¸­ï¼æåæåºäºä¸åå·æå¯æ§é¢¨éªçå¤ç®æ¨æè³çµåç®¡çæ¡æ¶ (MILLION)ï¼å®åå«å©åä¸»è¦éæ®µï¼å³å ±é¬ç¸éæå¤§ååé¢¨éªæ§å¶ãå·é«ä¾èªªï¼å¨å ±é¬ç¸éæå¤§åéæ®µï¼æåå¼å¥äºå©åè¼å©ç®æ¨ï¼å³å ±é¬çé æ¸¬åå ±é¬çæåï¼ä¸¦çµåæè³çµåæä½³åä¾è§£æ±ºéåº¦æ¬ååé¡ï¼ä¸¦æé«è¨ç·´æ¨¡åå°æªä¾å¸å ´çæ³åè½åãé¨å¾ï¼å¨é¢¨éªæ§å¶éæ®µï¼æåæåºäºå©ç¨®æ¹æ³ï¼å³æè³çµåæå¼åæè³çµåæ¹åï¼ä»¥å¯¦ç¾ç´°ç²åº¦çé¢¨éªæ§å¶åå¿«éé¢¨éªé©æä½¿ç¨èæå®çé¢¨éªæ°´æºãå°æ¼æè³çµåæå¼æ¹æ³ï¼æåå¾çè«ä¸è­æï¼å¦æå¾è¨­å®çé¢¨éªæ°´æºå¨é©ç¶çåéå§ï¼åé¢¨éªå¯ä»¥å¾å°å®ç¾çæ§å¶ãæ­¤å¤ï¼æåéè¡¨æï¼åªè¦å ±é¬æå¤§åéæ®µä¸­çæ¨¡åæ¯ææçï¼é£éº¼æè³çµåæå¼å¾èª¿æ´å¾çæè³çµåçå ±é¬çä¸æä½æ¼æå°è®ç°æ¸æä½³åãæ­¤å¤ï¼èæè³çµåæå¼ç¸æ¯ï¼æè³çµåæ¹åæ¹æ³å¯ä»¥å¨ä¿æç¸åé¢¨éªæ°´æºçåæå¯¦ç¾æ´é«çå ±é¬çãå¨ä¸åçå¯¦ä¸çè³æéä¸é²è¡äºå»£æ³çå¯¦é©ãçµæè­æäºææåºçæ¡æ¶çæææ§åæçã

##### **Specification Generation for Neural Networks in Systems**
2412.03028v1 by Isha Chaudhary, Shuyi Lin, Cheng Tan, Gagandeep Singh

Specifications - precise mathematical representations of correct
domain-specific behaviors - are crucial to guarantee the trustworthiness of
computer systems. With the increasing development of neural networks as
computer system components, specifications gain more importance as they can be
used to regulate the behaviors of these black-box models. Traditionally,
specifications are designed by domain experts based on their intuition of
correct behavior. However, this is labor-intensive and hence not a scalable
approach as computer system applications diversify. We hypothesize that the
traditional (aka reference) algorithms that neural networks replace for higher
performance can act as effective proxies for correct behaviors of the models,
when available. This is because they have been used and tested for long enough
to encode several aspects of the trustworthy/correct behaviors in the
underlying domain. Driven by our hypothesis, we develop a novel automated
framework, SpecTRA to generate specifications for neural networks using
references. We formulate specification generation as an optimization problem
and solve it with observations of reference behaviors. SpecTRA clusters similar
observations into compact specifications. We present specifications generated
by SpecTRA for neural networks in adaptive bit rate and congestion control
algorithms. Our specifications show evidence of being correct and matching
intuition. Moreover, we use our specifications to show several unknown
vulnerabilities of the SOTA models for computer systems.

æè¦ï¼è¦ç¯ - æ­£ç¢ºçç¹å®é åè¡çºçç²¾ç¢ºæ¸å­¸è¡¨ç¤º - å°ä¿è­é»è¦ç³»çµ±çå¼å¾ä¿¡è³´æ§è³ééè¦ãé¨èç¥ç¶ç¶²è·¯ä½çºé»è¦ç³»çµ±çµä»¶çç¼å±è¶ä¾è¶å»£æ³ï¼è¦ç¯è®å¾è¶ä¾è¶éè¦ï¼å çºå®åå¯ç¨æ¼è¦ç¯éäºé»çæ¨¡åçè¡çºãå³çµ±ä¸ï¼è¦ç¯æ¯ç±é åå°å®¶æ ¹æä»åå°æ­£ç¢ºè¡çºçç´è¦ºè¨­è¨çãç¶èï¼ééè¦å¤§éçäººåï¼å æ­¤é¨èé»è¦ç³»çµ±æç¨ç¨å¼å¤ååï¼éä¸¦éä¸åå¯æ´åçæ¹æ³ãæååè¨­ç¥ç¶ç¶²è·¯ç¨ä»¥æ¿ä»£ä»¥ç²å¾æ´é«æè½çå³çµ±ï¼åç¨±åèï¼æ¼ç®æ³ï¼å¨æéè¦æå¯ä»¥ä½çºæ¨¡åæ­£ç¢ºè¡çºçææä»£çãéæ¯å çºå®åå·²ç¶è¢«ä½¿ç¨åæ¸¬è©¦äºè¶³å¤ é·çæéï¼è¶³ä»¥ç·¨ç¢¼å¯ä¿¡/æ­£ç¢ºè¡çºçå¹¾åé¢åå¨åºç¤é åä¸­ãå¨æåçåè¨­é©ä½¿ä¸ï¼æåéç¼äºä¸åæ°ç©çèªååæ¡æ¶ SpecTRAï¼ä½¿ç¨åèçºç¥ç¶ç¶²è·¯ç¢çè¦ç¯ãæåå°è¦ç¯ç¢çè¡¨è¿°çºä¸åæä½³ååé¡ï¼ä¸¦ééè§å¯åèè¡çºä¾è§£æ±ºå®ãSpecTRA å°é¡ä¼¼çè§å¯çµæåç¾¤çºç²¾ç°¡çè¦ç¯ãæåæä¾ SpecTRA çºèªé©æä½åçåå£å¡æ§å¶æ¼ç®æ³ä¸­çç¥ç¶ç¶²è·¯ç¢ççè¦ç¯ãæåçè¦ç¯é¡¯ç¤ºææ­£ç¢ºä¸ç¬¦åç´è¦ºçè­æãæ­¤å¤ï¼æåä½¿ç¨æåçè¦ç¯ä¾é¡¯ç¤ºé»è¦ç³»çµ±ç SOTA æ¨¡åçå¹¾åæªç¥æ¼æ´ã

##### **Human Variability vs. Machine Consistency: A Linguistic Analysis of Texts Generated by Humans and Large Language Models**
2412.03025v1 by Sergio E. Zanotto, Segun Aroyehun

The rapid advancements in large language models (LLMs) have significantly
improved their ability to generate natural language, making texts generated by
LLMs increasingly indistinguishable from human-written texts. Recent research
has predominantly focused on using LLMs to classify text as either
human-written or machine-generated. In our study, we adopt a different approach
by profiling texts spanning four domains based on 250 distinct linguistic
features. We select the M4 dataset from the Subtask B of SemEval 2024 Task 8.
We automatically calculate various linguistic features with the LFTK tool and
additionally measure the average syntactic depth, semantic similarity, and
emotional content for each document. We then apply a two-dimensional PCA
reduction to all the calculated features. Our analyses reveal significant
differences between human-written texts and those generated by LLMs,
particularly in the variability of these features, which we find to be
considerably higher in human-written texts. This discrepancy is especially
evident in text genres with less rigid linguistic style constraints. Our
findings indicate that humans write texts that are less cognitively demanding,
with higher semantic content, and richer emotional content compared to texts
generated by LLMs. These insights underscore the need for incorporating
meaningful linguistic features to enhance the understanding of textual outputs
of LLMs.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) çå¿«éé²å±é¡¯èæåäºå®åçæèªç¶èªè¨çè½åï¼è® LLM çæçæå­è¶ä¾è¶é£ä»¥èäººé¡å¯«çæå­ååãæè¿çç ç©¶ä¸»è¦éä¸­æ¼ä½¿ç¨ LLM å°æå­åé¡çºäººé¡å¯«çææ©å¨çæçãå¨æåçç ç©¶ä¸­ï¼æåæ¡ç¨ä¸åçæ¹æ³ï¼æ ¹æ 250 åä¸åçèªè¨ç¹å¾µå°è·¨è¶ååé åçæå­é²è¡åæãæåå¾ SemEval 2024 ä»»å 8 çå­ä»»å B ä¸­é¸å M4 è³æéãæåä½¿ç¨ LFTK å·¥å·èªåè¨ç®åç¨®èªè¨ç¹å¾µï¼ä¸¦å¦å¤æ¸¬éæ¯åæä»¶çå¹³åå¥æ³æ·±åº¦ãèªç¾©ç¸ä¼¼åº¦åæç·å§å®¹ãç¶å¾ï¼æåå°ææè¨ç®åºçç¹å¾µæç¨äºç¶­ PCA éç¶­ãæåçåææ­ç¤ºäºäººé¡å¯«çæå­å LLM çæçæå­ä¹éçé¡¯èå·®ç°ï¼ç¹å¥æ¯å¨éäºç¹å¾µçå¯è®æ§æ¹é¢ï¼æåç¼ç¾äººé¡å¯«çæå­çå¯è®æ§é¡¯èè¼é«ãéç¨®å·®ç°å¨èªè¨é¢¨æ ¼ç´æè¼å°çæå­é¡åä¸­å°¤å¶æé¡¯ãæåçç ç©¶çµæè¡¨æï¼äººé¡å¯«çæå­èªç¥è¦æ±è¼ä½ï¼èªç¾©å§å®¹è¼é«ï¼è LLM çæçæå­ç¸æ¯ï¼æç·å§å®¹ä¹æ´è±å¯ãéäºè¦è§£å¼·èª¿äºç´å¥ææç¾©çèªè¨ç¹å¾µä»¥å¢å¼·å° LLM ææ¬è¼¸åºççè§£çå¿è¦æ§ã

##### **PEMF-VVTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm**
2412.03021v1 by Tianyu Chang, Xiaohao Chen. Zhichao Wei, Xuanpu Zhang, Qing-Guo Chen, Weihua Luo, Xun Yang

Video Virtual Try-on aims to fluently transfer the garment image to a
semantically aligned try-on area in the source person video. Previous methods
leveraged the inpainting mask to remove the original garment in the source
video, thus achieving accurate garment transfer on simple model videos.
However, when these methods are applied to realistic video data with more
complex scene changes and posture movements, the overly large and incoherent
agnostic masks will destroy the essential spatial-temporal information of the
original video, thereby inhibiting the fidelity and coherence of the try-on
video. To alleviate this problem, %avoid the inherent deficiencies of
mask-based try-on paradigm, we propose a novel point-enhanced mask-free video
virtual try-on framework (PEMF-VVTO). Specifically, we first leverage the
pre-trained mask-based try-on model to construct large-scale paired training
data (pseudo-person samples). Training on these mask-free data enables our
model to perceive the original spatial-temporal information while realizing
accurate garment transfer. Then, based on the pre-acquired sparse frame-cloth
and frame-frame point alignments, we design the point-enhanced spatial
attention (PSA) and point-enhanced temporal attention (PTA) to further improve
the try-on accuracy and video coherence of the mask-free model. Concretely, PSA
explicitly guides the garment transfer to desirable locations through the
sparse semantic alignments of video frames and cloth. PTA exploits the temporal
attention on sparse point correspondences to enhance the smoothness of
generated videos. Extensive qualitative and quantitative experiments clearly
illustrate that our PEMF-VVTO can generate more natural and coherent try-on
videos than existing state-of-the-art methods.

æè¦ï¼<paragraph>å½±çèæ¬è©¦ç©¿æ¨å¨æµæ¢å°å°æé£¾å½±åå³è¼¸å°ä¾æºäººç©å½±çä¸­èªç¾©å°é½çè©¦ç©¿ååãååçåæ³å©ç¨å¡«è²é®ç½©ç§»é¤ä¾æºå½±çä¸­çåå§æé£¾ï¼é²èæ¼ç°¡å®çæ¨¡åå½±çä¸å¯¦ç¾ç²¾æºçæé£¾å³è¼¸ãç¶èï¼ç¶éäºåæ³å¥ç¨æ¼å ´æ¯è®æåå§¿å¢åä½æ´è¤éçå¯«å¯¦å½±çè³ææï¼éæ¼é¾å¤§ä¸ä¸é£è²«çéç¹å®é®ç½©æç ´å£åå§å½±çä¸­éè¦çæç©ºè³è¨ï¼é²èæå¶è©¦ç©¿å½±ççä¿çåº¦åé£è²«æ§ãçºäºæ¸è¼éååé¡ï¼é¿ååºæ¼é®ç½©çè©¦ç©¿ç¯ä¾ä¸­åºæçç¼ºé»ï¼æåæåºä¸åæ°ç©çé»å¢å¼·ç¡é®ç½©å½±çèæ¬è©¦ç©¿æ¶æ§ (PEMF-VVTO)ãå·é«ä¾èªªï¼æåé¦åå©ç¨é åè¨ç·´çåºæ¼é®ç½©çè©¦ç©¿æ¨¡åå»ºç«å¤§è¦æ¨¡éå°è¨ç·´è³æï¼æ¬äººæ¨£æ¬ï¼ãå¨éäºç¡é®ç½©è³æä¸é²è¡è¨ç·´ï¼è½è®æåçæ¨¡åæç¥åå§æç©ºè³è¨ï¼åæå¯¦ç¾ç²¾æºçæé£¾å³è¼¸ãç¶å¾ï¼æ ¹æé ååå¾çç¨çå¹å¸æåå¹å¹é»å°é½ï¼æåè¨­è¨é»å¢å¼·ç©ºéæ³¨æå (PSA) åé»å¢å¼·æéæ³¨æå (PTA)ï¼é²ä¸æ­¥æåç¡é®ç½©æ¨¡åçè©¦ç©¿ç²¾æºåº¦åå½±çé£è²«æ§ãå·é«èè¨ï¼PSA ééå½±çå¹åå¸æçç¨çèªç¾©å°é½ï¼æç¢ºå¼å°æé£¾å³è¼¸å°çæ³ä½ç½®ãPTA å©ç¨ç¨çé»å°æçæåºæ³¨æåï¼æåçæå½±ççæµæ¢åº¦ãå»£æ³çå®æ§åå®éå¯¦é©æ¸æ¥é¡¯ç¤ºï¼æåç PEMF-VVTO è½ç¢çæ¯ç¾ææåé²æ¹æ³æ´èªç¶ä¸é£è²«çè©¦ç©¿å½±çã</paragraph>

##### **Human Multi-View Synthesis from a Single-View Model:Transferred Body and Face Representations**
2412.03011v1 by Yu Feng, Shunsi Zhang, Jian Shu, Hanfeng Zhao, Guoliang Pang, Chi Zhang, Hao Wang

Generating multi-view human images from a single view is a complex and
significant challenge. Although recent advancements in multi-view object
generation have shown impressive results with diffusion models, novel view
synthesis for humans remains constrained by the limited availability of 3D
human datasets. Consequently, many existing models struggle to produce
realistic human body shapes or capture fine-grained facial details accurately.
To address these issues, we propose an innovative framework that leverages
transferred body and facial representations for multi-view human synthesis.
Specifically, we use a single-view model pretrained on a large-scale human
dataset to develop a multi-view body representation, aiming to extend the 2D
knowledge of the single-view model to a multi-view diffusion model.
Additionally, to enhance the model's detail restoration capability, we
integrate transferred multimodal facial features into our trained human
diffusion model. Experimental evaluations on benchmark datasets demonstrate
that our approach outperforms the current state-of-the-art methods, achieving
superior performance in multi-view human synthesis.

æè¦ï¼å¾å®ä¸è¦è§çæå¤è¦è§çäººé¡å½±åæ¯ä¸é è¤éä¸éå¤§çææ°ãåç®¡æè¿å¨å¤è¦è§ç©ä»¶çææ¹é¢çé²å±å·²å¨æ´æ£æ¨¡åä¸­å±ç¾ä»¤äººå°è±¡æ·±å»çææï¼ä½äººé¡çæ°è¦è§åæä»åå° 3D äººé¡è³æéæéçå¯ç¨æ§æéå¶ãå æ­¤ï¼è¨±å¤ç¾ææ¨¡åé£ä»¥ç¢çé¼ççäººé«å½¢çææºç¢ºææç´°ç·»çé¢é¨ç´°ç¯ãçºäºè§£æ±ºéäºåé¡ï¼æåæåºäºä¸ååµæ°çæ¶æ§ï¼è©²æ¶æ§å©ç¨è½ç§»çèº«é«åé¢é¨è¡¨ç¤ºä¾é²è¡å¤è¦è§äººé¡åæãå·é«ä¾èªªï¼æåä½¿ç¨å¨å¤§åäººé¡è³æéä¸é åè¨ç·´çå®è¦è§æ¨¡åä¾éç¼å¤è¦è§èº«é«è¡¨ç¤ºï¼æ¨å¨å°å®è¦è§æ¨¡åç 2D ç¥è­æ´å±å°å¤è¦è§æ´æ£æ¨¡åãæ­¤å¤ï¼çºäºå¢å¼·æ¨¡åçç´°ç¯éåè½åï¼æåå°è½ç§»çå¤æ¨¡æé¢é¨ç¹å¾µæ´åå°æåè¨ç·´çäººé¡æ´æ£æ¨¡åä¸­ãå¨åºæºè³æéä¸çå¯¦é©è©ä¼°è¡¨æï¼æåçåæ³åªæ¼ç®åçææ°æ¹æ³ï¼å¨å¤è¦è§äººé¡åæä¸­åå¾äºåè¶çæ§è½ã

##### **Advancing Conversational Psychotherapy: Integrating Privacy, Dual-Memory, and Domain Expertise with Large Language Models**
2412.02987v1 by XiuYu Zhang, Zening Luo

Mental health has increasingly become a global issue that reveals the
limitations of traditional conversational psychotherapy, constrained by
location, time, expense, and privacy concerns. In response to these challenges,
we introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed
to democratize access to psychotherapy. SoulSpeak improves upon the
capabilities of standard LLM-enabled chatbots by incorporating a novel
dual-memory component that combines short-term and long-term context via
Retrieval Augmented Generation (RAG) to offer personalized responses while
ensuring the preservation of user privacy and intimacy through a dedicated
privacy module. In addition, it leverages a counseling chat dataset of
therapist-client interactions and various prompting techniques to align the
generated responses with psychotherapeutic methods. We introduce two fine-tuned
BERT models to evaluate the system against existing LLMs and human therapists:
the Conversational Psychotherapy Preference Model (CPPM) to simulate human
preference among responses and another to assess response relevance to user
input. CPPM is useful for training and evaluating psychotherapy-focused
language models independent from SoulSpeak, helping with the constrained
resources available for psychotherapy. Furthermore, the effectiveness of the
dual-memory component and the robustness of the privacy module are also
examined. Our findings highlight the potential and challenge of enhancing
mental health care by offering an alternative that combines the expertise of
traditional therapy with the advantages of LLMs, providing a promising way to
address the accessibility and personalization gap in current mental health
services.

æè¦ï¼å¿çå¥åº·å·²æ¥çæçºå¨çåé¡ï¼æ´é²äºå³çµ±å°è©±å¼å¿çæ²»ççéå¶ï¼åå°å°é»ãæéãè²»ç¨åé±ç§åé¡çç´æãçºäºæå°éäºææ°ï¼æåæ¨åºäº SoulSpeakï¼éæ¯ä¸æ¬¾å¤§åèªè¨æ¨¡å (LLM) åç¨çèå¤©æ©å¨äººï¼æ¨å¨å¯¦ç¾å¿çæ²»ççæ°ä¸»åãSoulSpeak ééçµåç­æåé·æèªå¢çåµæ°ééè¨æ¶çµä»¶ï¼ééæª¢ç´¢å¢å¼·çæ (RAG) ä¾æ¹é²æ¨æº LLM åç¨èå¤©æ©å¨äººçè½åï¼ä»¥æä¾åæ§åçåæï¼åæééå°ç¨çé±ç§æ¨¡çµç¢ºä¿ä½¿ç¨èé±ç§åè¦ªå¯æ§çä¿è­·ãæ­¤å¤ï¼å®å©ç¨æ²»çå¸«èå®¢æ¶äºåçè«®è©¢èå¤©è³æéååç¨®æç¤ºæè¡ï¼ä½¿çæçåæèå¿çæ²»çæ¹æ³ä¿æä¸è´ãæåå¼å¥äºå©åå¾®èª¿ç BERT æ¨¡åï¼ä»¥éå°ç¾æç LLM åäººé¡æ²»çå¸«è©ä¼°ç³»çµ±ï¼å°è©±å¼å¿çæ²»çåå¥½æ¨¡å (CPPM) ç¨æ¼æ¨¡æ¬äººé¡å°åæçåå¥½ï¼å¦ä¸åç¨æ¼è©ä¼°å°ä½¿ç¨èè¼¸å¥çåæç¸éæ§ãCPPM å¯ç¨æ¼è¨ç·´åè©ä¼°èå¿çæ²»ççºéé»çèªè¨æ¨¡åï¼èè SoulSpeak ç¡éï¼æå©æ¼æå°å¿çæ²»ççåéè³æºãæ­¤å¤ï¼éæª¢é©äºééè¨æ¶çµä»¶çæææ§åé±ç§æ¨¡çµçç©©å¥æ§ãæåçç ç©¶çµæçªåºäºééæä¾çµåå³çµ±çæ³å°æ¥­ç¥è­å LLM åªå¢çæ¿ä»£æ¹æ¡ä¾å¢å¼·å¿çä¿å¥çæ½ååææ°ï¼çºè§£æ±ºç¶åå¿çå¥åº·æåçå¯åæ§ååäººåå·®è·æä¾äºä¸åæå¸æçæ¹æ³ã

##### **Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models**
2412.02980v1 by Alex Havrilla, Andrew Dai, Laura O'Mahony, Koen Oostermeijer, Vera Zisler, Alon Albalak, Fabrizio Milo, Sharath Chandra Raparthy, Kanishk Gandhi, Baber Abbasi, Duy Phung, Maia Iyer, Dakota Mahan, Chase Blagden, Srishti Gureja, Mohammed Hamdy, Wen-Ding Li, Giovanni Paolini, Pawan Sasanka Ammanamanchi, Elliot Meyerson

Synthetic data generation with Large Language Models is a promising paradigm
for augmenting natural data over a nearly infinite range of tasks. Given this
variety, direct comparisons among synthetic data generation algorithms are
scarce, making it difficult to understand where improvement comes from and what
bottlenecks exist. We propose to evaluate algorithms via the makeup of
synthetic data generated by each algorithm in terms of data quality, diversity,
and complexity. We choose these three characteristics for their significance in
open-ended processes and the impact each has on the capabilities of downstream
models. We find quality to be essential for in-distribution model
generalization, diversity to be essential for out-of-distribution
generalization, and complexity to be beneficial for both. Further, we emphasize
the existence of Quality-Diversity trade-offs in training data and the
downstream effects on model performance. We then examine the effect of various
components in the synthetic data pipeline on each data characteristic. This
examination allows us to taxonomize and compare synthetic data generation
algorithms through the components they utilize and the resulting effects on
data QDC composition. This analysis extends into a discussion on the importance
of balancing QDC in synthetic data for efficient reinforcement learning and
self-improvement algorithms. Analogous to the QD trade-offs in training data,
often there exist trade-offs between model output quality and output diversity
which impact the composition of synthetic data. We observe that many models are
currently evaluated and optimized only for output quality, thereby limiting
output diversity and the potential for self-improvement. We argue that
balancing these trade-offs is essential to the development of future
self-improvement algorithms and highlight a number of works making progress in
this direction.

æè¦ï¼<paragraph>ä½¿ç¨å¤§åèªè¨æ¨¡åé²è¡åæè³æçæå°æ¼å¨å¹¾ä¹ç¡éç¯åçä»»åä¸­æ´åèªç¶è³æèè¨æ¯ä¸ç¨®æåéçç¯ä¾ãéæ¼æ­¤ç¨®å¤æ¨£æ§ï¼åæè³æçææ¼ç®æ³ä¹éçç´æ¥æ¯è¼ç¸ç¶ç¨å°ï¼éä½¿å¾é£ä»¥çè§£é²æ­¥çä¾æºä»¥åç¶é ¸å¨åªè£¡ãæåæè­°ééæ¯ç¨®æ¼ç®æ³æç¢ççåæè³æççµæï¼å¨è³æåè³ªãå¤æ¨£æ§åè¤éæ§æ¹é¢è©ä¼°æ¼ç®æ³ãæåé¸æéä¸é ç¹å¾µï¼æ¯å çºå®åå¨éæ¾å¼æµç¨ä¸­å·æéè¦æç¾©ï¼èä¸æ¯é ç¹å¾µé½æå°ä¸æ¸¸æ¨¡åçè½åç¢çå½±é¿ãæåç¼ç¾åè³ªå°æ¼åéä¸­æ¨¡åçæ¦åè³ééè¦ï¼å¤æ¨£æ§å°æ¼åéå¤æ¦åè³ééè¦ï¼èè¤éæ§å°æ¼å©èé½æå©ãæ­¤å¤ï¼æåå¼·èª¿å¨è¨ç·´è³æä¸­å­å¨åè³ªå¤æ¨£æ§æ¬è¡¡ï¼ä»¥åå°æ¨¡åæè½çä¸æ¸¸å½±é¿ãç¶å¾ï¼æåæª¢æ¥åæè³æç®¡éä¸­åç¨®çµæå°æ¯åè³æç¹å¾µçå½±é¿ãæ­¤é æª¢æ¥è®æåè½å¤ ééå®åæå©ç¨ççµæåå°è³æ QDC çµæçå½±é¿ï¼å°åæè³æçææ¼ç®æ³é²è¡åé¡åæ¯è¼ãæ­¤åæå»¶ä¼¸è³è¨è«å¨åæè³æä¸­å¹³è¡¡ QDC ä»¥å¯¦ç¾ææå¼·åå­¸ç¿åèªææ¹åæ¼ç®æ³çéè¦æ§ãé¡ä¼¼æ¼è¨ç·´è³æä¸­ç QD æ¬è¡¡ï¼æ¨¡åè¼¸åºåè³ªåè¼¸åºå¤æ¨£æ§ä¹ééå¸¸å­å¨æ¬è¡¡ï¼éæå½±é¿åæè³æççµæãæåè§å¯å°ï¼ç®åè¨±å¤æ¨¡ååéå°è¼¸åºåè³ªé²è¡è©ä¼°åæä½³åï¼å¾èéå¶äºè¼¸åºå¤æ¨£æ§ä»¥åèªææ¹åçæ½åãæåèªçºï¼å¹³è¡¡éäºæ¬è¡¡å°æ¼æªä¾èªææ¹åæ¼ç®æ³çç¼å±è³ééè¦ï¼ä¸¦å¼·èª¿è¨±å¤å¨éåæ¹åä¸åå¾é²å±çä½åã</paragraph>

##### **Theoretical limitations of multi-layer Transformer**
2412.02975v1 by Lijie Chen, Binghui Peng, Hongxun Wu

Transformers, especially the decoder-only variants, are the backbone of most
modern large language models; yet we do not have much understanding of their
expressive power except for the simple $1$-layer case.
  Due to the difficulty of analyzing multi-layer models, all previous work
relies on unproven complexity conjectures to show limitations for multi-layer
Transformers. In this work, we prove the first $\textit{unconditional}$ lower
bound against multi-layer decoder-only transformers. For any constant $L$, we
prove that any $L$-layer decoder-only transformer needs a polynomial model
dimension ($n^{\Omega(1)}$) to perform sequential composition of $L$ functions
over an input of $n$ tokens.
  As a consequence, our results give: (1) the first depth-width trade-off for
multi-layer transformers, exhibiting that the $L$-step composition task is
exponentially harder for $L$-layer models compared to $(L+1)$-layer ones; (2)
an unconditional separation between encoder and decoder, exhibiting a hard task
for decoders that can be solved by an exponentially shallower and smaller
encoder; (3) a provable advantage of chain-of-thought, exhibiting a task that
becomes exponentially easier with chain-of-thought.
  On the technical side, we propose the multi-party $\textit{autoregressive}$
$\textit{communication}$ $\textit{model}$ that captures the computation of a
decoder-only Transformer. We also introduce a new proof technique that finds a
certain $\textit{indistinguishable}$ $\textit{decomposition}$ of all possible
inputs iteratively for proving lower bounds in this model. We believe our new
communication model and proof technique will be helpful to further understand
the computational power of transformers.

æè¦ï¼<paragraph>Transformerï¼å°¤å¶æ¯åè§£ç¢¼å¨è®é«ï¼æ¯å¤§å¤æ¸ç¾ä»£å¤§åèªè¨æ¨¡åçéª¨å¹¹ï¼ç¶èï¼é¤äºç°¡å®ç $1$ å±¤æ¡ä¾ä¹å¤ï¼æåå°å®åçè¡¨éè½åä¸¦æ²æå¤ªå¤äºè§£ã
ç±æ¼åæå¤å±¤æ¨¡åçé£åº¦ï¼ææååçç ç©¶é½ä¾è³´æ¼æªç¶è­å¯¦çè¤éæ§çæ³ä¾é¡¯ç¤ºå¤å±¤ Transformer çéå¶ãå¨éé ç ç©¶ä¸­ï¼æåè­æäºå°æ¼åè§£ç¢¼å¨å¤å±¤ Transformer çç¬¬ä¸å$\textit{ç¡æ¢ä»¶}$ä¸çãå°æ¼ä»»ä½å¸¸æ¸ $L$ï¼æåè­æä»»ä½ $L$ å±¤åè§£ç¢¼å¨ Transformer éè¦å¤é å¼æ¨¡åç¶­åº¦ ($n^{\Omega(1)}$) ä¾å·è¡ $n$ å token è¼¸å¥ç $L$ åå½æ¸çåºåçµæã
å æ­¤ï¼æåççµæçµ¦åºï¼(1) å¤å±¤ Transformer çç¬¬ä¸åæ·±åº¦å¯¬åº¦æ¬è¡¡ï¼å±ç¤ºäºå°æ¼ $L$ å±¤æ¨¡åèè¨ï¼$L$ æ­¥é©çµæä»»åæ¯ $(L+1)$ å±¤æ¨¡åé£ä»¥ææ¸ç´ï¼(2) ç·¨ç¢¼å¨åè§£ç¢¼å¨ä¹éçç¡æ¢ä»¶åé¢ï¼å±ç¤ºäºè§£ç¢¼å¨çä¸åå°é£ä»»åï¼èéåä»»åå¯ä»¥ééææ¸ç´æ´æ·ºåæ´å°çç·¨ç¢¼å¨ä¾è§£æ±ºï¼(3) ææ³éçå¯è­æåªå¢ï¼å±ç¤ºäºä¸åé¨èææ³éèè®å¾ææ¸ç´æ´ç°¡å®çä»»åã
å¨æè¡æ¹é¢ï¼æåæåºäºå¤æ¹$\textit{èªè¿´æ­¸}$$\textit{éä¿¡}$$\textit{æ¨¡å}$ï¼å®æç²äºåè§£ç¢¼å¨ Transformer çè¨ç®ãæåéå¼å¥äºä¸ç¨®æ°çè­ææè¡ï¼è©²æè¡åè¦å°æ¾ææå¯è½è¼¸å¥çæå$\textit{ä¸å¯åå}$$\textit{åè§£}$ï¼ä»¥è­ææ­¤æ¨¡åä¸­çä¸çãæåç¸ä¿¡æåæ°çéä¿¡æ¨¡ååè­ææè¡æå©æ¼é²ä¸æ­¥äºè§£ Transformer çè¨ç®è½åã</paragraph>

##### **3D Interaction Geometric Pre-training for Molecular Relational Learning**
2412.02957v1 by Namkyeong Lee, Yunhak Oh, Heewoong Noh, Gyoung S. Na, Minkai Xu, Hanchen Wang, Tianfan Fu, Chanyoung Park

Molecular Relational Learning (MRL) is a rapidly growing field that focuses
on understanding the interaction dynamics between molecules, which is crucial
for applications ranging from catalyst engineering to drug discovery. Despite
recent progress, earlier MRL approaches are limited to using only the 2D
topological structure of molecules, as obtaining the 3D interaction geometry
remains prohibitively expensive. This paper introduces a novel 3D geometric
pre-training strategy for MRL (3DMRL) that incorporates a 3D virtual
interaction environment, overcoming the limitations of costly traditional
quantum mechanical calculation methods. With the constructed 3D virtual
interaction environment, 3DMRL trains 2D MRL model to learn the overall 3D
geometric information of molecular interaction through contrastive learning.
Moreover, fine-grained interaction between molecules is learned through force
prediction loss, which is crucial in understanding the wide range of molecular
interaction processes. Extensive experiments on various tasks using real-world
datasets, including out-of-distribution and extrapolation scenarios,
demonstrate the effectiveness of 3DMRL, showing up to a 24.93\% improvement in
performance across 40 tasks.

æè¦ï¼åå­éä¿å­¸ç¿ (MRL) æ¯å¿«éæé·çé åï¼å°æ³¨æ¼äºè§£åå­ä¹éçäºååæï¼éå°æ¼å¾å¬ååå·¥ç¨å°è¥ç©ç¼ç¾ç­æç¨è³ééè¦ãåç®¡æè¿æé²å±ï¼ä½æ©æç MRL æ¹æ³åéæ¼ä½¿ç¨åå­ç 2D ææ²çµæ§ï¼å çºç²å 3D äº¤äºå¹¾ä½çµæ§ä»ç¶éå¸¸æè²´ãæ¬æä»ç´¹äºä¸ç¨®æ°ç©ç 3D å¹¾ä½é è¨ç·´ç­ç¥ï¼ç¨æ¼ MRL (3DMRL)ï¼å®çµåäºä¸å 3D èæ¬äº¤äºç°å¢ï¼åæäºæè²´çå³çµ±éå­åå­¸è¨ç®æ¹æ³çéå¶ãå©ç¨æ§å»ºç 3D èæ¬äº¤äºç°å¢ï¼3DMRL è¨ç·´ 2D MRL æ¨¡åééå°æ¯å­¸ç¿ä¾å­¸ç¿åå­äº¤äºçæ´é« 3D å¹¾ä½è³è¨ãæ­¤å¤ï¼ééåé æ¸¬æå¤±å­¸ç¿åå­ä¹éçç´°ç²åº¦äº¤äºï¼éå°æ¼çè§£å»£æ³çåå­äº¤äºéç¨è³ééè¦ãå¨ä½¿ç¨çå¯¦ä¸çè³æéçåç¨®ä»»åä¸é²è¡çå»£æ³å¯¦é©ï¼åæ¬åä½å¤åå¤æ¨å ´æ¯ï¼è­æäº 3DMRL çæææ§ï¼é¡¯ç¤ºå¨ 40 åä»»åä¸­çæè½æåäº 24.93%ã

##### **Curriculum-style Data Augmentation for LLM-based Metaphor Detection**
2412.02956v1 by Kaidi Jia, Yanxia Wu, Rongsheng Li

Recently, utilizing large language models (LLMs) for metaphor detection has
achieved promising results. However, these methods heavily rely on the
capabilities of closed-source LLMs, which come with relatively high inference
costs and latency. To address this, we propose a method for metaphor detection
by fine-tuning open-source LLMs, effectively reducing inference costs and
latency with a single inference step. Furthermore, metaphor detection suffers
from a severe data scarcity problem, which hinders effective fine-tuning of
LLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA).
Specifically, before fine-tuning, we evaluate the training data to identify
correctly predicted instances for fine-tuning, while incorrectly predicted
instances are used as seed data for data augmentation. This approach enables
the model to quickly learn simpler knowledge and progressively acquire more
complex knowledge, thereby improving performance incrementally. Experimental
results demonstrate that our method achieves state-of-the-art performance
across all baselines. Additionally, we provide detailed ablation studies to
validate the effectiveness of CDA.

æè¦ï¼<paragraph>æè¿ï¼å©ç¨å¤§åèªè¨æ¨¡å (LLM) é²è¡é±å»åµæ¸¬å·²ç²å¾ä»¤äººæ»¿æççµæãç¶èï¼éäºæ¹æ³å´éä¾è³´éæº LLM çåè½ï¼èéæå¸¶ä¾ç¸å°é«çæ¨è«ææ¬åå»¶é²ãçºäºè§£æ±ºéååé¡ï¼æåæåºäºä¸ç¨®ééå¾®èª¿éæº LLM ä¾é²è¡é±å»åµæ¸¬çæ¹æ³ï¼ææå°æ¸å°äºå®ä¸æ¨è«æ­¥é©çæ¨è«ææ¬åå»¶é²ãæ­¤å¤ï¼é±å»åµæ¸¬æé­åå´éçè³æç¨å°åé¡ï¼éæé»ç¤ LLM çææå¾®èª¿ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äºèª²ç¨å¼è³ææ´å (CDA)ãå·é«ä¾èªªï¼å¨å¾®èª¿ä¹åï¼æåæè©ä¼°è¨ç·´è³æä»¥æ¾åºæ­£ç¢ºé æ¸¬çå¾®èª¿å¯¦ä¾ï¼èé¯èª¤é æ¸¬çå¯¦ä¾åç¨ä½è³ææ´åçç¨®å­è³æãéç¨®æ¹æ³è®æ¨¡åè½å¤ å¿«éå­¸ç¿è¼ç°¡å®çç¥è­ï¼ä¸¦éæ­¥ç¿å¾è¼è¤éçç¥è­ï¼å¾èéæ­¥æåæè½ãå¯¦é©çµæè­æï¼æåçæ¨¡åå¨ææåºæºä¸­é½éå°äºæåé²çæè½ãæ­¤å¤ï¼æåæä¾äºè©³ç´°çæ¶èç ç©¶ï¼ä»¥é©è­ CDA çæææ§ã</paragraph>

##### **Who Brings the Frisbee: Probing Hidden Hallucination Factors in Large Vision-Language Model via Causality Analysis**
2412.02946v1 by Po-Hsuan Huang, Jeng-Lin Li, Chin-Po Chen, Ming-Ching Chang, Wei-Chao Chen

Recent advancements in large vision-language models (LVLM) have significantly
enhanced their ability to comprehend visual inputs alongside natural language.
However, a major challenge in their real-world application is hallucination,
where LVLMs generate non-existent visual elements, eroding user trust. The
underlying mechanism driving this multimodal hallucination is poorly
understood. Minimal research has illuminated whether contexts such as sky,
tree, or grass field involve the LVLM in hallucinating a frisbee. We
hypothesize that hidden factors, such as objects, contexts, and semantic
foreground-background structures, induce hallucination. This study proposes a
novel causal approach: a hallucination probing system to identify these hidden
factors. By analyzing the causality between images, text prompts, and network
saliency, we systematically explore interventions to block these factors. Our
experimental findings show that a straightforward technique based on our
analysis can significantly reduce hallucinations. Additionally, our analyses
indicate the potential to edit network internals to minimize hallucinated
outputs.

æè¦ï¼å¤§åè¦è¦ºèªè¨æ¨¡å (LVLM) çææ°é²å±é¡¯èå¢å¼·äºå®åçè§£èªç¶èªè¨åè¦è¦ºè¼¸å¥çè½åãç¶èï¼å¨ç¾å¯¦ä¸çæç¨ä¸­çä¸åéå¤§ææ°æ¯å¹»è¦ºï¼å¶ä¸­ LVLM ç¢çä¸å­å¨çè¦è¦ºåç´ ï¼ä¾µèäºä½¿ç¨èçä¿¡ä»»ãé©åéç¨®å¤æ¨¡æå¹»è¦ºçåºå±¤æ©å¶å°æªè¢«ååçè§£ãå¾å°æç ç©¶æ¢è¨å¤©ç©ºãæ¨¹æ¨æèå°ç­èæ¯æ¯å¦æè® LVLM ç¢çé£ç¤çå¹»è¦ºãæååè¨­ç©é«ãèæ¯åèªç¾©åæ¯èæ¯çµæ§ç­é±èå ç´ æèªç¼å¹»è¦ºãæ¬ç ç©¶æåºäºä¸ç¨®æ°çå ææ¹æ³ï¼ä¸ç¨®å¹»è¦ºæ¢æ¸¬ç³»çµ±ï¼ç¨æ¼è­å¥éäºé±èå ç´ ãééåæå½±åãæå­æç¤ºåç¶²è·¯é¡¯èæ§ä¹éçå æéä¿ï¼æåç³»çµ±æ§å°æ¢ç´¢äºé»æ­¢éäºå ç´ çå¹²é æªæ½ãæåçå¯¦é©çµæè¡¨æï¼åºæ¼æååæçä¸ç¨®ç´æ¥æè¡å¯ä»¥é¡¯èæ¸å°å¹»è¦ºãæ­¤å¤ï¼æåçåæè¡¨æææ½åç·¨è¼¯ç¶²è·¯å§é¨çµæ§ä»¥æå°åç¢çå¹»è¦ºçè¼¸åºã

##### **STDCformer: A Transformer-Based Model with a Spatial-Temporal Causal De-Confounding Strategy for Crowd Flow Prediction**
2412.02942v1 by Silu He, Peng Shen, Pingzhen Xu, Qinyao Luo, Haifeng Li

Existing works typically treat spatial-temporal prediction as the task of
learning a function $F$ to transform historical observations to future
observations. We further decompose this cross-time transformation into three
processes: (1) Encoding ($E$): learning the intrinsic representation of
observations, (2) Cross-Time Mapping ($M$): transforming past representations
into future representations, and (3) Decoding ($D$): reconstructing future
observations from the future representations. From this perspective,
spatial-temporal prediction can be viewed as learning $F = E \cdot M \cdot D$,
which includes learning the space transformations $\left\{{E},{D}\right\}$
between the observation space and the hidden representation space, as well as
the spatial-temporal mapping $M$ from future states to past states within the
representation space. This leads to two key questions: \textbf{Q1: What kind of
representation space allows for mapping the past to the future? Q2: How to
achieve map the past to the future within the representation space?} To address
Q1, we propose a Spatial-Temporal Backdoor Adjustment strategy, which learns a
Spatial-Temporal De-Confounded (STDC) representation space and estimates the
de-confounding causal effect of historical data on future data. This causal
relationship we captured serves as the foundation for subsequent
spatial-temporal mapping. To address Q2, we design a Spatial-Temporal Embedding
(STE) that fuses the information of temporal and spatial confounders, capturing
the intrinsic spatial-temporal characteristics of the representations.
Additionally, we introduce a Cross-Time Attention mechanism, which queries the
attention between the future and the past to guide spatial-temporal mapping.

æè¦ï¼<paragraph>ç¾æçä½åéå¸¸å°æç©ºé æ¸¬è¦çºå­¸ç¿å½æ¸ $F$ çä»»åï¼ä»¥å°æ­·å²è§æ¸¬å¼è½æçºæªä¾è§æ¸¬å¼ãæåé²ä¸æ­¥å°éç¨®è·¨æéè½æåè§£çºä¸åéç¨ï¼(1) ç·¨ç¢¼ ($E$)ï¼å­¸ç¿è§æ¸¬å¼çå§å¨è¡¨ç¤ºï¼(2) è·¨æéæ å° ($M$)ï¼å°éå»çè¡¨ç¤ºè½æçºæªä¾çè¡¨ç¤ºï¼ä»¥å (3) è§£ç¢¼ ($D$)ï¼å¾æªä¾çè¡¨ç¤ºéå»ºæªä¾çè§æ¸¬å¼ãå¾éåè§åº¦ä¾çï¼æç©ºé æ¸¬å¯ä»¥è¦çºå­¸ç¿ $F = E \cdot M \cdot D$ï¼å¶ä¸­åæ¬å­¸ç¿è§æ¸¬ç©ºéåé±èè¡¨ç¤ºç©ºéä¹éçç©ºéè½æ $\left\{{E},{D}\right\}$ï¼ä»¥åè¡¨ç¤ºç©ºéå§å¾æªä¾çæå°éå»çæçæç©ºæ å° $M$ãéå°è´äºå©åééµåé¡ï¼\textbf{Q1ï¼åªç¨®é¡åçè¡¨ç¤ºç©ºéåè¨±å°éå»æ å°å°æªä¾ï¼Q2ï¼å¦ä½å¨è¡¨ç¤ºç©ºéå§å°éå»æ å°å°æªä¾ï¼} çºäºåç­ Q1ï¼æåæåºäºæç©ºå¾éèª¿æ´ç­ç¥ï¼è©²ç­ç¥å­¸ç¿æç©ºå»æ··æ· (STDC) è¡¨ç¤ºç©ºéï¼ä¸¦ä¼°è¨æ­·å²æ¸æå°æªä¾æ¸æçå»æ··æ·å æææãæåææå°çéç¨®å æéä¿ä½çºå¾çºæç©ºæ å°çåºç¤ãçºäºåç­ Q2ï¼æåè¨­è¨äºä¸åæç©ºåµå¥ (STE)ï¼å®èåäºæéåç©ºéæ··æ·å ç´ çä¿¡æ¯ï¼ææäºè¡¨ç¤ºçå§å¨æç©ºç¹å¾µãæ­¤å¤ï¼æåå¼å¥äºä¸åè·¨æéæ³¨æåæ©å¶ï¼å®æ¥è©¢æªä¾åéå»ä¹éçæ³¨æåä»¥æå°æç©ºæ å°ã</paragraph>

##### **Dynamic Graph Neural Ordinary Differential Equation Network for Multi-modal Emotion Recognition in Conversation**
2412.02935v1 by Yuntao Shou, Tao Meng, Wei Ai, Keqin Li

Multimodal emotion recognition in conversation (MERC) refers to identifying
and classifying human emotional states by combining data from multiple
different modalities (e.g., audio, images, text, video, etc.). Most existing
multimodal emotion recognition methods use GCN to improve performance, but
existing GCN methods are prone to overfitting and cannot capture the temporal
dependency of the speaker's emotions. To address the above problems, we propose
a Dynamic Graph Neural Ordinary Differential Equation Network (DGODE) for MERC,
which combines the dynamic changes of emotions to capture the temporal
dependency of speakers' emotions, and effectively alleviates the overfitting
problem of GCNs. Technically, the key idea of DGODE is to utilize an adaptive
mixhop mechanism to improve the generalization ability of GCNs and use the
graph ODE evolution network to characterize the continuous dynamics of node
representations over time and capture temporal dependencies. Extensive
experiments on two publicly available multimodal emotion recognition datasets
demonstrate that the proposed DGODE model has superior performance compared to
various baselines. Furthermore, the proposed DGODE can also alleviate the
over-smoothing problem, thereby enabling the construction of a deep GCN
network.

æè¦ï¼å¤æ¨¡æå¯¹è¯ææè¯å«ï¼MERCï¼æ¯æéè¿ç»åæ¥èªå¤ç§ä¸åæ¨¡æï¼ä¾å¦é³é¢ãå¾åãææ¬ãè§é¢ç­ï¼çæ°æ®æ¥è¯å«ååç±»äººç±»ææç¶æãå¤§å¤æ°ç°æçå¤æ¨¡æææè¯å«æ¹æ³ä½¿ç¨ GCN æ¥æé«æ§è½ï¼ä½ç°æç GCN æ¹æ³å®¹æè¿æåï¼å¹¶ä¸æ æ³ææè¯´è¯äººææçæ¶é´ä¾èµæ§ãä¸ºäºè§£å³ä¸è¿°é®é¢ï¼æä»¬æåºäºä¸ç§ç¨äº MERC çå¨æå¾ç¥ç»å¸¸å¾®åæ¹ç¨ç½ç» (DGODE)ï¼å®ç»åäºææçå¨æååæ¥ææè¯´è¯äººææçæ¶é´ä¾èµæ§ï¼å¹¶ææå°ç¼è§£äº GCN çè¿æåé®é¢ãä»ææ¯ä¸è®²ï¼DGODE çå³é®ææ³æ¯å©ç¨èªéåºæ··åæºå¶æ¥æé« GCN çæ³åè½åï¼å¹¶ä½¿ç¨å¾ ODE æ¼åç½ç»æ¥è¡¨å¾èç¹è¡¨ç¤ºéæ¶é´ååçè¿ç»­å¨æå¹¶æææ¶é´ä¾èµæ§ãå¨ä¸¤ä¸ªå¬å¼çå¤æ¨¡æææè¯å«æ°æ®éä¸è¿è¡çå¹¿æ³å®éªè¡¨æï¼ææåºç DGODE æ¨¡åä¸åç§åºçº¿ç¸æ¯å·æä¼è¶çæ§è½ãæ­¤å¤ï¼ææåºç DGODE è¿å¯ä»¥ç¼è§£è¿åº¦å¹³æ»é®é¢ï¼ä»èè½å¤æå»ºæ·±åº¦ GCN ç½ç»ã

##### **Panoptic Diffusion Models: co-generation of images and segmentation maps**
2412.02929v1 by Yinghan Long, Kaushik Roy

Recently, diffusion models have demonstrated impressive capabilities in
text-guided and image-conditioned image generation. However, existing diffusion
models cannot simultaneously generate a segmentation map of objects and a
corresponding image from the prompt. Previous attempts either generate
segmentation maps based on the images or provide maps as input conditions to
control image generation, limiting their functionality to given inputs.
Incorporating an inherent understanding of the scene layouts can improve the
creativity and realism of diffusion models. To address this limitation, we
present Panoptic Diffusion Model (PDM), the first model designed to generate
both images and panoptic segmentation maps concurrently. PDM bridges the gap
between image and text by constructing segmentation layouts that provide
detailed, built-in guidance throughout the generation process. This ensures the
inclusion of categories mentioned in text prompts and enriches the diversity of
segments within the background. We demonstrate the effectiveness of PDM across
two architectures: a unified diffusion transformer and a two-stream transformer
with a pretrained backbone. To facilitate co-generation with fewer sampling
steps, we incorporate a fast diffusion solver into PDM. Additionally, when
ground-truth maps are available, PDM can function as a text-guided
image-to-image generation model. Finally, we propose a novel metric for
evaluating the quality of generated maps and show that PDM achieves
state-of-the-art results in image generation with implicit scene control.

æè¦ï¼<paragraph>è¿æï¼æ©æ£æ¨¡åå¨ææ¬å¼å¯¼åå¾åæ¡ä»¶å¾åçææ¹é¢å±ç¤ºäºä»¤äººå°è±¡æ·±å»çè½åãç¶èï¼ç°æçæ©æ£æ¨¡åæ æ³åæ¶ä»æç¤ºçæå¯¹è±¡çåå²å¾åå¯¹åºçå¾åãååçå°è¯è¦ä¹åºäºå¾åçæåå²å¾ï¼è¦ä¹å°åå²å¾ä½ä¸ºè¾å¥æ¡ä»¶æ¥æ§å¶å¾åçæï¼ä»èå°å®ä»¬çåè½éå¶å¨ç»å®çè¾å¥ä¸ã
èå¥å¯¹åºæ¯å¸å±çåå¨çè§£å¯ä»¥æé«æ©æ£æ¨¡åçåé ååçå®æãä¸ºäºè§£å³è¿ä¸éå¶ï¼æä»¬æåºäºå¨æ¯æ©æ£æ¨¡åï¼PDMï¼ï¼è¿æ¯ç¬¬ä¸ä¸ªæ¨å¨åæ¶çæå¾ååå¨æ¯åå²å¾çæ¨¡åãPDM éè¿æå»ºåå²å¸å±æ¥å¼¥åå¾ååææ¬ä¹é´çå·®è·ï¼è¿äºå¸å±å¨æ´ä¸ªçæè¿ç¨ä¸­æä¾äºè¯¦ç»çåç½®æå¯¼ãè¿ç¡®ä¿äºåå«ææ¬æç¤ºä¸­æå°çç±»å«ï¼å¹¶ä¸°å¯äºèæ¯ä¸­çæ®µçå¤æ ·æ§ãæä»¬å¨ä¸¤ç§æ¶æä¸­å±ç¤ºäº PDM çæææ§ï¼ä¸ä¸ªç»ä¸çæ©æ£è½¬æ¢å¨åä¸ä¸ªå·æé¢è®­ç»ä¸»å¹²ç½ç»çä¸¤æµè½¬æ¢å¨ãä¸ºäºä¿è¿ä½¿ç¨æ´å°çéæ ·æ­¥éª¤è¿è¡å±çæï¼æä»¬å¨ PDM ä¸­å å¥äºä¸ä¸ªå¿«éæ©æ£æ±è§£å¨ãæ­¤å¤ï¼å½æ ground-truth å¾æ¶ï¼PDM å¯ä»¥ç¨ä½ææ¬å¼å¯¼çå¾åå°å¾åçææ¨¡åãæåï¼æä»¬æåºäºä¸ç§æ°é¢çææ æ¥è¯ä¼°çæå¾çè´¨éï¼å¹¶è¡¨æ PDM å¨å¾åçæä¸­å®ç°äºæåè¿çç»æï¼å¹¶å·æéå¼åºæ¯æ§å¶ã</paragraph>

##### **Higher Order Transformers: Efficient Attention Mechanism for Tensor Structured Data**
2412.02919v1 by Soroush Omranpour, Guillaume Rabusseau, Reihaneh Rabbany

Transformers are now ubiquitous for sequence modeling tasks, but their
extension to multi-dimensional data remains a challenge due to the quadratic
cost of the attention mechanism. In this paper, we propose Higher-Order
Transformers (HOT), a novel architecture designed to efficiently process data
with more than two axes, i.e. higher-order tensors. To address the
computational challenges associated with high-order tensor attention, we
introduce a novel Kronecker factorized attention mechanism that reduces the
attention cost to quadratic in each axis' dimension, rather than quadratic in
the total size of the input tensor. To further enhance efficiency, HOT
leverages kernelized attention, reducing the complexity to linear. This
strategy maintains the model's expressiveness while enabling scalable attention
computation. We validate the effectiveness of HOT on two high-dimensional
tasks, including multivariate time series forecasting, and 3D medical image
classification. Experimental results demonstrate that HOT achieves competitive
performance while significantly improving computational efficiency, showcasing
its potential for tackling a wide range of complex, multi-dimensional data.

æè¦ï¼è®å½¢éåç¾å¨æ®éç¨æ¼åºåå»ºæ¨¡ä»»åï¼ä½ç±æ¼æ³¨æåæ©å¶çäºæ¬¡æ¹ææ¬ï¼å®åæ´å±å°å¤ç¶­æ¸æä»ç¶æ¯ä¸åææ°ãå¨æ¬æä¸­ï¼æåæåºäºé«éè®å½¢éå (HOT)ï¼éæ¯ä¸ç¨®æ°ç©çæ¶æ§ï¼æ¨å¨ææèçå·æå©åä»¥ä¸è»¸ç·çæ¸æï¼å³é«éå¼µéãçºäºæå°èé«éå¼µéæ³¨æåç¸éçè¨ç®ææ°ï¼æåå¼å¥äºä¸ç¨®æ°ç©çåç¾å§ååè§£æ³¨æåæ©å¶ï¼è©²æ©å¶å°æ³¨æåææ¬éä½å°æ¯åè»¸ç·ç¶­åº¦çäºæ¬¡æ¹ï¼èä¸æ¯è¼¸å¥å¼µéçç¸½å¤§å°çäºæ¬¡æ¹ãçºäºé²ä¸æ­¥æé«æçï¼HOT å©ç¨æ ¸åæ³¨æåï¼å°è¤éåº¦éä½å°ç·æ§ãæ­¤ç­ç¥ä¿æäºæ¨¡åçè¡¨ç¾åï¼åæå¯¦ç¾äºå¯æ´å±çæ³¨æåè¨ç®ãæåå¨å©åé«ç¶­ä»»åä¸é©è­äº HOT çæææ§ï¼åæ¬å¤åæéåºåé æ¸¬å 3D é«å­¸å½±ååé¡ãå¯¦é©çµæè¡¨æï¼HOT å¨é¡¯èæé«è¨ç®æççåæå¯¦ç¾äºç«¶ç­åçæè½ï¼å±ç¤ºäºå¶æå°åç¨®è¤éçå¤ç¶­æ¸æçæ½åã

##### **Single-Cell Omics Arena: A Benchmark Study for Large Language Models on Cell Type Annotation Using Single-Cell Data**
2412.02915v1 by Junhao Liu, Siwei Xu, Lei Zhang, Jing Zhang

Over the past decade, the revolution in single-cell sequencing has enabled
the simultaneous molecular profiling of various modalities across thousands of
individual cells, allowing scientists to investigate the diverse functions of
complex tissues and uncover underlying disease mechanisms. Among all the
analytical steps, assigning individual cells to specific types is fundamental
for understanding cellular heterogeneity. However, this process is usually
labor-intensive and requires extensive expert knowledge. Recent advances in
large language models (LLMs) have demonstrated their ability to efficiently
process and synthesize vast corpora of text to automatically extract essential
biological knowledge, such as marker genes, potentially promoting more
efficient and automated cell type annotations. To thoroughly evaluate the
capability of modern instruction-tuned LLMs in automating the cell type
identification process, we introduce SOAR, a comprehensive benchmarking study
of LLMs for cell type annotation tasks in single-cell genomics. Specifically,
we assess the performance of 8 instruction-tuned LLMs across 11 datasets,
spanning multiple cell types and species. Our study explores the potential of
LLMs to accurately classify and annotate cell types in single-cell RNA
sequencing (scRNA-seq) data, while extending their application to multiomics
data through cross-modality translation. Additionally, we evaluate the
effectiveness of chain-of-thought (CoT) prompting techniques in generating
detailed biological insights during the annotation process. The results
demonstrate that LLMs can provide robust interpretations of single-cell data
without requiring additional fine-tuning, advancing the automation of cell type
annotation in genomics research.

æè¦ï¼<paragraph>å¨éå»åå¹´ä¸­ï¼å®ç´°èå®åºçé©å½æ§ç¼å±å·²ç¶è½å¤ 
åæå°æ¸ååå®ä¸ç´°èé²è¡åç¨®æ¨¡å¼çåå­åæï¼è®ç§å­¸å®¶å¾ä»¥ç ç©¶
è¤éçµç¹çå¤ååè½ä¸¦æ­ç¤ºæ½å¨çç¾çæ©å¶ãå¨ææåææ­¥é©ä¸­ï¼
å°å®ä¸ç´°èåéå°ç¹å®é¡åå°æ¼äºè§£ç´°èç°è³ªæ§è³ééè¦ãç¶èï¼
éåéç¨éå¸¸éè¦å¤§éäººåï¼ä¸¦ä¸éè¦å»£æ³çå°æ¥­ç¥è­ãå¤§åèªè¨
æ¨¡å (LLM) çææ°é²å±å·²è­æå®åè½å¤ ææèçåç¶åå¤§éçæå­
èªæåº«ï¼ä»¥èªåæååºæ¬ççç©ç¥è­ï¼ä¾å¦æ¨è¨åºå ï¼æ½å¨å°ä¿é²æ´
ææçä¸èªååçç´°èé¡åè¨»è§£ãçºäºå¾¹åºè©ä¼°ç¾ä»£æä»¤èª¿æ´ LLM
å¨èªååç´°èé¡åè­å¥éç¨ä¸­çè½åï¼æåå¼å¥äº SOARï¼éæ¯ä¸é 
éå°å®ç´°èåºå çµå­¸ä¸­ç´°èé¡åè¨»è§£ä»»åç LLM å¨é¢åºæºç ç©¶ãå·é«
ä¾èªªï¼æåè©ä¼°äº 8 åæä»¤èª¿æ´ LLM å¨ 11 åæ¸æéä¸­çæ§è½ï¼æ¶µè
å¤ç¨®ç´°èé¡ååç©ç¨®ãæåçç ç©¶æ¢è¨äº LLM å¨å®ç´°è RNA å®åº
(scRNA-seq) è³æä¸­æºç¢ºåé¡åè¨»è§£ç´°èé¡åçæ½åï¼åæééè·¨æ¨¡
æè½è­¯å°å¶æç¨æ´å±å°å¤çµå­¸è³æãæ­¤å¤ï¼æåè©ä¼°äºæèé (CoT)
æç¤ºæè¡å¨è¨»è§£éç¨ä¸­ç¢çè©³ç´°çç©è¦è§£çæææ§ãçµæè¡¨æï¼LLM
å¯ä»¥å°å®ç´°èè³ææä¾å¼·å¥çè©®éï¼èä¸éè¦é¡å¤çå¾®èª¿ï¼æ¨åäº
åºå çµå­¸ç ç©¶ä¸­ç´°èé¡åè¨»è§£çèªååã</paragraph>

##### **Does Few-Shot Learning Help LLM Performance in Code Synthesis?**
2412.02906v1 by Derek Xu, Tong Xie, Botao Xia, Haoyu Li, Yunsheng Bai, Yizhou Sun, Wei Wang

Large language models (LLMs) have made significant strides at code generation
through improved model design, training, and chain-of-thought. However,
prompt-level optimizations remain an important yet under-explored aspect of
LLMs for coding. This work focuses on the few-shot examples present in most
code generation prompts, offering a systematic study on whether few-shot
examples improve LLM's coding capabilities, which few-shot examples have the
largest impact, and how to select impactful examples. Our work offers 2
approaches for selecting few-shot examples, a model-free method,
CODEEXEMPLAR-FREE, and a model-based method, CODEEXEMPLAR-BASED. The 2 methods
offer a trade-off between improved performance and reliance on training data
and interpretability. Both methods significantly improve CodeLlama's coding
ability across the popular HumanEval+ coding benchmark. In summary, our work
provides valuable insights into how to pick few-shot examples in code
generation prompts to improve LLM code generation capabilities.

æè¦ï¼å¤§åèªè¨æ¨¡åï¼LLMï¼ééæ¹åæ¨¡åè¨­è¨ãè¨ç·´åæèéï¼å¨ç¨å¼ç¢¼çææ¹é¢åå¾éå¤§é²å±ãç¶èï¼æç¤ºå±¤ç´æä½³åä»ç¶æ¯ LLM ç·¨ç¢¼çéè¦ä½å°æªååæ¢ç´¢çæ¹é¢ãéé å·¥ä½éé»å¨æ¼å¤§å¤æ¸ç¨å¼ç¢¼çææç¤ºä¸­åºç¾çå°æ¸ç¯ä¾ï¼æä¾ä¸é ç³»çµ±æ§ç ç©¶ï¼æ¢è¨å°æ¸ç¯ä¾æ¯å¦è½æå LLM çç·¨ç¢¼è½åï¼åªäºå°æ¸ç¯ä¾å½±é¿æå¤§ï¼ä»¥åå¦ä½é¸ææå½±é¿åçç¯ä¾ãæåçç ç©¶æä¾ 2 ç¨®é¸æå°æ¸ç¯ä¾çæ¹æ³ï¼ä¸ç¨®æ¯ç¡æ¨¡åæ¹æ³ï¼CODEEXEMPLAR-FREEï¼ä¸ç¨®æ¯åºæ¼æ¨¡åçæ¹æ³ï¼CODEEXEMPLAR-BASEDãé 2 ç¨®æ¹æ³å¨æ¹åæè½åä¾è³´è¨ç·´è³æåå¯è§£éæ§ä¹éåå¾æ¬è¡¡ãéå©ç¨®æ¹æ³é½å¤§å¹æå CodeLlama å¨ç±é HumanEval+ ç·¨ç¢¼åºæºä¸­çç·¨ç¢¼è½åãç¸½ä¹ï¼æåçç ç©¶æä¾æå¹å¼çè¦è§£ï¼èªªæå¦ä½å¨ç¨å¼ç¢¼çææç¤ºä¸­æé¸å°æ¸ç¯ä¾ï¼ä»¥æå LLM ç¨å¼ç¢¼çæè½åã

##### **Enhancing Trust in Large Language Models with Uncertainty-Aware Fine-Tuning**
2412.02904v1 by Ranganath Krishnan, Piyush Khanna, Omesh Tickoo

Large language models (LLMs) have revolutionized the field of natural
language processing with their impressive reasoning and question-answering
capabilities. However, these models are sometimes prone to generating
credible-sounding but incorrect information, a phenomenon known as LLM
hallucinations. Reliable uncertainty estimation in LLMs is essential for
fostering trust in their generated responses and serves as a critical tool for
the detection and prevention of erroneous or hallucinated outputs. To achieve
reliable and well-calibrated uncertainty quantification in open-ended and
free-form natural language generation, we propose an uncertainty-aware
fine-tuning approach for LLMs. This approach enhances the model's ability to
provide reliable uncertainty estimates without compromising accuracy, thereby
guiding them to produce more trustworthy responses. We introduce a novel
uncertainty-aware causal language modeling loss function, grounded in the
principles of decision theory. Through rigorous evaluation on multiple
free-form question-answering datasets and models, we demonstrate that our
uncertainty-aware fine-tuning approach yields better calibrated uncertainty
estimates in natural language generation tasks than fine-tuning with the
standard causal language modeling loss. Furthermore, the experimental results
show that the proposed method significantly improves the model's ability to
detect hallucinations and identify out-of-domain prompts.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) ä»¥å¶ä»¤äººå°è±¡æ·±å»çæ¨çååç­è½åï¼å¾¹åºæ¹è®äºèªç¶èªè¨èçé åãç¶èï¼éäºæ¨¡åææå®¹æç¢çè½èµ·ä¾å¯ä¿¡ä½é¯èª¤çè³è¨ï¼éç¨®ç¾è±¡ç¨±çº LLM å¹»è¦ºãLLM ä¸­å¯é çä¸ç¢ºå®æ§ä¼°è¨å°æ¼å»ºç«å°å¶çæåæçä¿¡ä»»è³ééè¦ï¼ä¸¦ä½çºæª¢æ¸¬åé é²é¯èª¤æå¹»è¦ºè¼¸åºçééµå·¥å·ãçºäºå¨éæ¾å¼åèªç±å½¢å¼çèªç¶èªè¨çæä¸­å¯¦ç¾å¯é ä¸æ ¡æºè¯å¥½çä¸ç¢ºå®æ§éåï¼æåæåºäºä¸ç¨®éå° LLM çä¸ç¢ºå®æ§æç¥å¾®èª¿æ¹æ³ãéç¨®æ¹æ³å¢å¼·äºæ¨¡åæä¾å¯é ä¸ç¢ºå®æ§ä¼°è¨çè½åï¼åæä¸å½±é¿æºç¢ºæ§ï¼å¾èå¼å°å®åç¢çæ´å¯ä¿¡çåæãæåå¼å¥äºä¸ç¨®æ°ç©çä¸ç¢ºå®æ§æç¥å æèªè¨å»ºæ¨¡æå¤±å½æ¸ï¼å®åºæ¼æ±ºç­çè«çåçãééå°å¤åèªç±å½¢å¼åç­è³æéåæ¨¡åé²è¡å´æ ¼è©ä¼°ï¼æåè­æäºæåçä¸ç¢ºå®æ§æç¥å¾®èª¿æ¹æ³å¨èªç¶èªè¨çæä»»åä¸­ç¢çäºæ¯ä½¿ç¨æ¨æºå æèªè¨å»ºæ¨¡æå¤±é²è¡å¾®èª¿æ´å¥½çæ ¡æºä¸ç¢ºå®æ§ä¼°è¨ãæ­¤å¤ï¼å¯¦é©çµæè¡¨æï¼ææåºçæ¹æ³é¡¯èæé«äºæ¨¡åæª¢æ¸¬å¹»è¦ºåè­å¥åå¤æç¤ºçè½åã

##### **MLD-EA: Check and Complete Narrative Coherence by Introducing Emotions and Actions**
2412.02897v1 by Jinming Zhang, Yunfei Long

Narrative understanding and story generation are critical challenges in
natural language processing (NLP), with much of the existing research focused
on summarization and question-answering tasks. While previous studies have
explored predicting plot endings and generating extended narratives, they often
neglect the logical coherence within stories, leaving a significant gap in the
field. To address this, we introduce the Missing Logic Detector by Emotion and
Action (MLD-EA) model, which leverages large language models (LLMs) to identify
narrative gaps and generate coherent sentences that integrate seamlessly with
the story's emotional and logical flow. The experimental results demonstrate
that the MLD-EA model enhances narrative understanding and story generation,
highlighting LLMs' potential as effective logic checkers in story writing with
logical coherence and emotional consistency. This work fills a gap in NLP
research and advances border goals of creating more sophisticated and reliable
story-generation systems.

æè¦ï¼æäºçè§£åæäºçææ¯èªç¶èªè¨èç (NLP) ä¸­çéå¤§ææ°ï¼ç¾æè¨±å¤ç ç©¶å°æ³¨æ¼æè¦ååç­ä»»åãåç®¡éå¾çç ç©¶å·²æ¢è¨é æ¸¬æç¯çµå±åçæå»¶ä¼¸æäºï¼ä½å®åå¸¸å¸¸å¿½ç¥æäºä¸­çéè¼¯ä¸è´æ§ï¼å¨éåé åä¸­çä¸ä¸åé¡¯èçç¼ºå£ãçºäºè§£æ±ºéååé¡ï¼æåå¼å¥äºæç·ååä½çéºå¤±éè¼¯åµæ¸¬å¨ (MLD-EA) æ¨¡åï¼å®å©ç¨å¤§åèªè¨æ¨¡å (LLM) ä¾è­å¥æäºç¼ºå£ï¼ä¸¦çæèæäºçæç·åéè¼¯æµæ¢æ´åçé£è²«å¥å­ãå¯¦é©çµæè­æï¼MLD-EA æ¨¡åå¢å¼·äºæäºçè§£åæäºçæï¼çªé¡¯äº LLM ä½çºæäºå¯«ä½ä¸­ææéè¼¯æª¢æ¥å¨çæ½åï¼å·åéè¼¯ä¸è´æ§åæç·ä¸è´æ§ãéé å·¥ä½å¡«è£äº NLP ç ç©¶ä¸­çç¼ºå£ï¼ä¸¦æ¨åäºåµé æ´ç²¾ç·»ä¸å¯é çæäºçæç³»çµ±çéçç®æ¨ã

##### **Removing Spurious Correlation from Neural Network Interpretations**
2412.02893v1 by Milad Fotouhi, Mohammad Taha Bahadori, Oluwaseyi Feyisetan, Payman Arabshahi, David Heckerman

The existing algorithms for identification of neurons responsible for
undesired and harmful behaviors do not consider the effects of confounders such
as topic of the conversation. In this work, we show that confounders can create
spurious correlations and propose a new causal mediation approach that controls
the impact of the topic. In experiments with two large language models, we
study the localization hypothesis and show that adjusting for the effect of
conversation topic, toxicity becomes less localized.

æè¦ï¼ç¾æçç¨æ¼è­å¥å°ä¸è¯åæå®³è¡çºè² è²¬çç¥ç¶åæ¼ç®æ³ä¸¦æªèæ®æ··æ·å ç´ çå½±é¿ï¼ä¾å¦å°è©±ä¸»é¡ãå¨éé å·¥ä½ä¸­ï¼æåè¡¨ææ··æ·å ç´ æç¢çèåçç¸éæ§ï¼ä¸¦æåºäºä¸ç¨®æ°çå æä¸­ä»æ¹æ³ä¾æ§å¶ä¸»é¡çå½±é¿ãå¨ä½¿ç¨å©åå¤§åèªè¨æ¨¡åé²è¡çå¯¦é©ä¸­ï¼æåç ç©¶äºå±é¨ååèªªï¼ä¸¦è¡¨æèª¿æ´å°è©±ä¸»é¡çå½±é¿å¾ï¼æ¯æ§æéä½å±é¨åã

##### **TDD-Bench Verified: Can LLMs Generate Tests for Issues Before They Get Resolved?**
2412.02883v1 by Toufique Ahmed, Martin Hirzel, Rangeet Pan, Avraham Shinnar, Saurabh Sinha

Test-driven development (TDD) is the practice of writing tests first and
coding later, and the proponents of TDD expound its numerous benefits. For
instance, given an issue on a source code repository, tests can clarify the
desired behavior among stake-holders before anyone writes code for the
agreed-upon fix. Although there has been a lot of work on automated test
generation for the practice "write code first, test later", there has been
little such automation for TDD. Ideally, tests for TDD should be fail-to-pass
(i.e., fail before the issue is resolved and pass after) and have good adequacy
with respect to covering the code changed during issue resolution. This paper
introduces TDD-Bench Verified, a high-quality benchmark suite of 449 issues
mined from real-world GitHub code repositories. The benchmark's evaluation
harness runs only relevant tests in isolation for simple yet accurate coverage
measurements, and the benchmark's dataset is filtered both by human judges and
by execution in the harness. This paper also presents Auto-TDD, an LLM-based
solution that takes as input an issue description and a codebase (prior to
issue resolution) and returns as output a test that can be used to validate the
changes made for resolving the issue. Our evaluation shows that Auto-TDD yields
a better fail-to-pass rate than the strongest prior work while also yielding
high coverage adequacy. Overall, we hope that this work helps make developers
more productive at resolving issues while simultaneously leading to more robust
fixes.

æè¦ï¼æ¸¬è©¦é©åéç¼ (TDD) æ¯ä¸ç¨®åæ°å¯«æ¸¬è©¦åç·¨å¯«ç¨å¼ç¢¼çåæ³ï¼è TDD çæ¯æèåé¡è¿°äºå®çè¨±å¤å¥½èãä¾å¦ï¼çµ¦å®åå§ç¨å¼ç¢¼å²å­åº«ä¸­çåé¡ï¼æ¸¬è©¦å¯ä»¥å¨ä»»ä½äººçºåå®çä¿®æ­£ç¨å¼æ°å¯«ç¨å¼ç¢¼ä¹åï¼éæ¸å©å®³éä¿äººä¹éæéè¡çºãåç®¡å¨ãåæ°å¯«ç¨å¼ç¢¼ï¼å¾æ¸¬è©¦ãçåæ³ä¸­ï¼å·²ç¶æè¨±å¤éæ¼èªååæ¸¬è©¦ç¢ççå·¥ä½ï¼ä½ TDD çéç¨®èªååå»å¾å°ãçæ³ææ³ä¸ï¼TDD çæ¸¬è©¦æè©²æ¯å¤±æå°ééï¼å³å¨åé¡è§£æ±ºä¹åå¤±æï¼ä¹å¾ééï¼ï¼ä¸¦ä¸å¨æ¶µèåé¡è§£æ±ºæéè®æ´çç¨å¼ç¢¼æ¹é¢å·æè¯å¥½çååæ§ãæ¬æä»ç´¹äº TDD-Bench Verifiedï¼éæ¯ä¸åå¾çå¯¦ä¸çç GitHub ç¨å¼ç¢¼å²å­åº«ä¸­ææåºç 449 ååé¡çé«åè³ªåºæºæ¸¬è©¦å¥ä»¶ãåºæºæ¸¬è©¦çè©ä¼°å·¥å·åéå°ç°¡å®ä½æºç¢ºçæ¶µèç¯åæ¸¬éï¼å¨éé¢ç°å¢ä¸­å·è¡ç¸éæ¸¬è©¦ï¼ä¸¦ä¸åºæºæ¸¬è©¦çè³æéæ¯ç±äººå·¥è©å¯©åå¨å·¥å·ä¸­å·è¡æç¯©é¸åºä¾çãæ¬æéæåºäº Auto-TDDï¼éæ¯ä¸ååºæ¼ LLM çè§£æ±ºæ¹æ¡ï¼å®ä»¥åé¡æè¿°åç¨å¼ç¢¼åº«ï¼å¨åé¡è§£æ±ºä¹åï¼ä½çºè¼¸å¥ï¼ä¸¦åå³ä¸åå¯ä»¥ç¨ä¾é©è­çºäºè§£æ±ºåé¡èé²è¡çè®æ´çæ¸¬è©¦ãæåçè©ä¼°é¡¯ç¤ºï¼Auto-TDD ç¢çæ¯æå¼·çåæå·¥ä½æ´å¥½çå¤±æå°ééçï¼åæä¹ç¢çäºå¾é«çæ¶µèçãç¸½çä¾èªªï¼æåå¸æéé å·¥ä½æå©æ¼è®éç¼äººå¡å¨è§£æ±ºåé¡ææ´ææçï¼åæä¹è½å¸¶ä¾æ´å¼·å¥çä¿®æ­£ã

##### **Modeling and Discovering Direct Causes for Predictive Models**
2412.02878v1 by Yizuo Chen, Amit Bhatia

We introduce a causal modeling framework that captures the input-output
behavior of predictive models (e.g., machine learning models) by representing
it using causal graphs. The framework enables us to define and identify
features that directly cause the predictions, which has broad implications for
data collection and model evaluation. We show two assumptions under which the
direct causes can be discovered from data, one of which further simplifies the
discovery process. In addition to providing sound and complete algorithms, we
propose an optimization technique based on an independence rule that can be
integrated with the algorithms to speed up the discovery process both
theoretically and empirically.

æè¦ï¼æåå¼å¥ä¸åå ææ¨¡åæ¶æ§ï¼å®ééä½¿ç¨å æåè¡¨ï¼ä¾ææé æ¸¬æ¨¡åï¼ä¾å¦æ©å¨å­¸ç¿æ¨¡åï¼çè¼¸å¥è¼¸åºè¡çºãéåæ¶æ§è®æåè½å¤ å®ç¾©åæ¾åºç´æ¥å°è´é æ¸¬çåè½ï¼éå°æ¼è³ææ¶éåæ¨¡åè©ä¼°æå»£æ³çå½±é¿ãæåå±ç¤ºäºå©ååè¨­ï¼å¨éäºåè¨­ä¸ï¼å¯ä»¥ç´æ¥å¾è³æä¸­æ¾åºåå ï¼å¶ä¸­ä¸åé²ä¸æ­¥ç°¡åäºç¼ç¾çéç¨ãé¤äºæä¾å¥å¨ä¸å®æ´çæ¼ç®æ³ä¹å¤ï¼æåéæåºä¸ååºæ¼ç¨ç«æ§è¦åçæä½³åæè¡ï¼å®å¯ä»¥èæ¼ç®æ³æ´åï¼å¨çè«ä¸åç¶é©ä¸å éç¼ç¾çéç¨ã

##### **Constrained Identifiability of Causal Effects**
2412.02869v1 by Yizuo Chen, Adnan Darwiche

We study the identification of causal effects in the presence of different
types of constraints (e.g., logical constraints) in addition to the causal
graph. These constraints impose restrictions on the models (parameterizations)
induced by the causal graph, reducing the set of models considered by the
identifiability problem. We formalize the notion of constrained
identifiability, which takes a set of constraints as another input to the
classical definition of identifiability. We then introduce a framework for
testing constrained identifiability by employing tractable Arithmetic Circuits
(ACs), which enables us to accommodate constraints systematically. We show that
this AC-based approach is at least as complete as existing algorithms (e.g.,
do-calculus) for testing classical identifiability, which only assumes the
constraint of strict positivity. We use examples to demonstrate the
effectiveness of this AC-based approach by showing that unidentifiable causal
effects may become identifiable under different types of constraints.

æè¦ï¼æåç ç©¶å¨å æåä¹å¤ï¼å­å¨ä¸åé¡åç´æï¼ä¾å¦éè¼¯ç´æï¼çææ³ä¸ï¼å æææçè­å¥ãéäºç´æå°å æåæå¼ç¼çæ¨¡åï¼åæ¸åï¼æ½å éå¶ï¼æ¸å°è­å¥åé¡æèæ®çæ¨¡åéãæåå°åç´æè­å¥çæ¦å¿µå½¢å¼åï¼éå°ä¸çµç´æä½çºå¯è­å¥æ§çç¶å¸å®ç¾©çå¦ä¸åè¼¸å¥ãç¶å¾ï¼æåå¼å¥ä¸åæ¸¬è©¦åç´æè­å¥çæ¡æ¶ï¼ééä½¿ç¨ææ¼èççç®è¡é»è·¯ (AC) ä¾å¯¦ç¾ï¼éä½¿æåè½å¤ ç³»çµ±å°é©æç´æãæåè¡¨æï¼éç¨®åºæ¼ AC çæ¹æ³è³å°èç¾æçæ¼ç®æ³ï¼ä¾å¦ï¼do æ¼ç®ï¼ä¸æ¨£å®æ´ï¼ç¨æ¼æ¸¬è©¦ç¶å¸å¯è­å¥æ§ï¼éååè¨­å´æ ¼æ­£å®çç´æãæåä½¿ç¨ç¯ä¾ä¾è­æåºæ¼ AC çæ¹æ³çæææ§ï¼æ¹æ³æ¯è¡¨æä¸å¯è­å¥çå æææå¨ä¸åé¡åçç´æä¸å¯è½è®å¾å¯è­å¥ã

##### **A Novel Compact LLM Framework for Local, High-Privacy EHR Data Applications**
2412.02868v1 by Yixiang Qu, Yifan Dai, Shilin Yu, Pradham Tanikella, Travis Schrank, Trevor Hackman, Didong Li, Di Wu

Large Language Models (LLMs) have shown impressive capabilities in natural
language processing, yet their use in sensitive domains like healthcare,
particularly with Electronic Health Records (EHR), faces significant challenges
due to privacy concerns and limited computational resources. This paper
presents a compact LLM framework designed for local deployment in settings with
strict privacy requirements and limited access to high-performance GPUs. We
introduce a novel preprocessing technique that uses information extraction
methods, e.g., regular expressions, to filter and emphasize critical
information in clinical notes, enhancing the performance of smaller LLMs on EHR
data. Our framework is evaluated using zero-shot and few-shot learning
paradigms on both private and publicly available (MIMIC-IV) datasets, and we
also compare its performance with fine-tuned LLMs on the MIMIC-IV dataset. The
results demonstrate that our preprocessing approach significantly boosts the
prediction accuracy of smaller LLMs, making them suitable for high-privacy,
resource-constrained applications. This study offers valuable insights into
optimizing LLM performance for sensitive, data-intensive tasks while addressing
computational and privacy limitations.

æè¦ï¼å¤§åèªè¨æ¨¡å (LLM) å¨èªç¶èªè¨èçæ¹é¢å±ç¾åºä»¤äººå°è±¡æ·±å»çè½åï¼ç¶èå®åå¨é«çä¿å¥ç­ææé åçä½¿ç¨ï¼ç¹å¥æ¯é»å­å¥åº·ç´é (EHR)ï¼ç±æ¼é±ç§åé¡åæéçéç®è³æºèé¢è¨éå¤§ææ°ãæ¬ææåºäºä¸åç·æ¹ç LLM æ¡æ¶ï¼æ¨å¨å¨å·æå´æ ¼é±ç§è¦æ±åæéä½¿ç¨é«æ§è½ GPU çç°å¢ä¸­é²è¡æ¬å°é¨ç½²ãæåå¼å¥äºä¸ç¨®æ°ç©çé èçæè¡ï¼å®ä½¿ç¨è³è¨èåæ¹æ³ï¼ä¾å¦æ­£è¦è¡¨ç¤ºæ³ï¼ä¾éæ¿¾åå¼·èª¿è¨åºç­è¨ä¸­çééµè³è¨ï¼å¢å¼·è¼å° LLM å¨ EHR è³æä¸çæè½ãæåçæ¡æ¶ä½¿ç¨é¶æ¬¡å­¸ç¿åå°æ¬¡å­¸ç¿ç¯ä¾å¨ç§äººåå¬éå¯ç¨ç (MIMIC-IV) è³æéä¸é²è¡è©ä¼°ï¼æåä¹æ¯è¼å®å¨ MIMIC-IV è³æéä¸èå¾®èª¿ LLM çæè½ãçµæè¡¨æï¼æåçé èçæ¹æ³é¡¯èæåäºè¼å° LLM çé æ¸¬æºç¢ºåº¦ï¼ä½¿å¶é©ç¨æ¼é«åº¦é±ç§ãè³æºåéçæç¨ç¨å¼ãéé ç ç©¶æä¾äºå¯¶è²´çè¦è§£ï¼ç¨æ¼æä½³å LLM æè½ä»¥æå°ææãè³æå¯éåä»»åï¼åæè§£æ±ºéç®åé±ç§éå¶ã

##### **Unpaired Modality Translation for Pseudo Labeling of Histology Images**
2412.02858v1 by Arthur Boschet, Armand Collin, Nishka Katoch, Julien Cohen-Adad

The segmentation of histological images is critical for various biomedical
applications, yet the lack of annotated data presents a significant challenge.
We propose a microscopy pseudo labeling pipeline utilizing unsupervised image
translation to address this issue. Our method generates pseudo labels by
translating between labeled and unlabeled domains without requiring prior
annotation in the target domain. We evaluate two pseudo labeling strategies
across three image domains increasingly dissimilar from the labeled data,
demonstrating their effectiveness. Notably, our method achieves a mean Dice
score of $0.736 \pm 0.005$ on a SEM dataset using the tutoring path, which
involves training a segmentation model on synthetic data created by translating
the labeled dataset (TEM) to the target modality (SEM). This approach aims to
accelerate the annotation process by providing high-quality pseudo labels as a
starting point for manual refinement.

æè¦ï¼çµç¹åçå½±åçåå²å°æ¼åç¨®çç©é«å­¸æç¨è³ééè¦ï¼ç¶èç¼ºä¹è¨»è§£è³æå»æ¯ä¸åéå¤§çææ°ã
æåæåºä¸åå©ç¨éç£ç£å¼å½±åè½æçé¡¯å¾®é¡å½æ¨ç±¤èçæµç¨ä¾è§£æ±ºéååé¡ãæåçæè¡ééå¨æ¨ç±¤åæªæ¨ç±¤ç¶²åéé²è¡è½æä¾ç¢çå½æ¨ç±¤ï¼èä¸éè¦å¨ç®æ¨ç¶²åä¸­é²è¡äºåè¨»è§£ãæåå¨ä¸åèæ¨ç±¤è³æè¶ä¾è¶ä¸é¡ä¼¼çå½±åç¶²åä¸­è©ä¼°äºå©ç¨®å½æ¨ç±¤ç­ç¥ï¼ä¸¦è­æäºå®åçæææ§ãå¼å¾æ³¨æçæ¯ï¼æåçæè¡å¨ä½¿ç¨è¼å°è·¯å¾ç SEM è³æéä¸éå°äºå¹³å Dice åæ¸ $0.736 \pm 0.005$ï¼éåæ¬å¨åæè³æä¸è¨ç·´åå²æ¨¡åï¼è©²åæè³ææ¯ééå°æ¨ç±¤è³æé (TEM) è½æå°ç®æ¨æ¨¡å¼ (SEM) èå»ºç«çãæ­¤æ¹æ³æ¨å¨ééæä¾é«åè³ªçå½æ¨ç±¤ä½çºäººå·¥ç²¾ä¿®çèµ·é»ï¼ä¾å éè¨»è§£ç¨åºã

##### **FLAME 3 Dataset: Unleashing the Power of Radiometric Thermal UAV Imagery for Wildfire Management**
2412.02831v1 by Bryce Hopkins, Leo ONeill, Michael Marinaccio, Eric Rowell, Russell Parsons, Sarah Flanary, Irtija Nazim, Carl Seielstad, Fatemeh Afghah

The increasing accessibility of radiometric thermal imaging sensors for
unmanned aerial vehicles (UAVs) offers significant potential for advancing
AI-driven aerial wildfire management. Radiometric imaging provides per-pixel
temperature estimates, a valuable improvement over non-radiometric data that
requires irradiance measurements to be converted into visible images using RGB
color palettes. Despite its benefits, this technology has been underutilized
largely due to a lack of available data for researchers. This study addresses
this gap by introducing methods for collecting and processing synchronized
visual spectrum and radiometric thermal imagery using UAVs at prescribed fires.
The included imagery processing pipeline drastically simplifies and partially
automates each step from data collection to neural network input. Further, we
present the FLAME 3 dataset, the first comprehensive collection of side-by-side
visual spectrum and radiometric thermal imagery of wildland fires. Building on
our previous FLAME 1 and FLAME 2 datasets, FLAME 3 includes radiometric thermal
Tag Image File Format (TIFFs) and nadir thermal plots, providing a new data
type and collection method. This dataset aims to spur a new generation of
machine learning models utilizing radiometric thermal imagery, potentially
trivializing tasks such as aerial wildfire detection, segmentation, and
assessment. A single-burn subset of FLAME 3 for computer vision applications is
available on Kaggle with the full 6 burn set available to readers upon request.

æè¦ï¼ç¡äººæ©ï¼UAVï¼çæ¾å°ç±å½±åææ¸¬å¨è¶ä¾è¶æ®åï¼çºæ¨é²äººå·¥æºæ§é©åçç©ºä¸­éç«ç®¡çæä¾äºå·¨å¤§çæ½åãæ¾å°ç±å½±åæä¾æ¯ååç´ çæº«åº¦ä¼°è¨å¼ï¼éæ¯éæ¾å°ç±æ¸æçå¯¶è²´æ¹é²ï¼å¾èéè¦å°è¼»ç§åº¦æ¸¬éå¼è½æçºä½¿ç¨ RGB èª¿è²æ¿çå¯è¦å½±åãåç®¡æéäºåªé»ï¼ä½ç±æ¼ç ç©¶äººå¡ç¼ºä¹å¯ç¨çæ¸æï¼éé æè¡çä½¿ç¨çä¸ç´åä½ãæ¬ç ç©¶ééä»ç´¹ä½¿ç¨ç¡äººæ©å¨è¦å®çç«ç½ä¸­æ¶éåèçåæ­¥å¯è¦åè­åæ¾å°ç±å½±åçæ¹æ³ä¾è§£æ±ºéåå·®è·ãæåå«çå½±åèçç®¡éå¤§å¹ç°¡åä¸¦é¨åèªååå¾æ¸ææ¶éå°ç¥ç¶ç¶²è·¯è¼¸å¥çæ¯ä¸åæ­¥é©ãæ­¤å¤ï¼æåå±ç¤ºäº FLAME 3 è³æéï¼éæ¯ç¬¬ä¸åä¸¦ææ¶ééçç«ç½çå¯è¦åè­åæ¾å°ç±å½±åçç¶åè³æéãFLAME 3 å»ºæ§å¨æåä¹åç FLAME 1 å FLAME 2 è³æéä¹ä¸ï¼åå«æ¾å°ç±æ¨ç±¤å½±åæªæ¡æ ¼å¼ (TIFF) åå¤©åºç±åå¡ï¼æä¾æ°çè³æé¡ååæ¶éæ¹æ³ãæ­¤è³æéæ¨å¨æ¿ç¼æ°ä¸ä»£æ©å¨å­¸ç¿æ¨¡åï¼å©ç¨æ¾å°ç±å½±åï¼æ½å¨å°ç°¡åç©ºä¸­éç«åµæ¸¬ãåå²åè©ä¼°ç­ä»»åãFLAME 3 çå®æ¬¡ççå­éå¯ä¾é»è¦è¦è¦ºæç¨ç¨å¼å¨ Kaggle ä¸ä½¿ç¨ï¼èå®æ´ç 6 æ¬¡ççè¨­å®å¯æè®èè¦æ±æä¾ã

##### **RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models**
2412.02830v1 by Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, Hong Yu

This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a
versatile extension to the mutual reasoning framework (rStar), aimed at
enhancing reasoning accuracy and factual integrity across large language models
(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical
reasoning. RARE incorporates two innovative actions within the Monte Carlo Tree
Search (MCTS) framework: A6, which generates search queries based on the
initial problem statement, performs information retrieval using those queries,
and augments reasoning with the retrieved data to formulate the final answer;
and A7, which leverages information retrieval specifically for generated
sub-questions and re-answers these sub-questions with the relevant contextual
information. Additionally, a Retrieval-Augmented Factuality Scorer is proposed
to replace the original discriminator, prioritizing reasoning paths that meet
high standards of factuality. Experimental results with LLaMA 3.1 show that
RARE enables open-source LLMs to achieve competitive performance with top
open-source models like GPT-4 and GPT-4o. This research establishes RARE as a
scalable solution for improving LLMs in domains where logical coherence and
factual integrity are critical.

æè¦ï¼éé å·¥ä½ä»ç´¹äº RAREï¼æª¢ç´¢å¢å¼·æ¨çå¼·åï¼ï¼éæ¯å°ç¸äºæ¨çæ¶æ§ (rStar) çå¤åè½æ´åï¼æ¨å¨å¢å¼·å¤§åèªè¨æ¨¡å (LLM) å¨å¸¸è­åé«å­¸æ¨çç­è¤éãç¥è­å¯éåä»»åçæ¨çæºç¢ºæ§åäºå¯¦å®æ´æ§ãRARE å¨èå°å¡ç¾æ¨¹æå° (MCTS) æ¶æ§ä¸­åå«å©ååµæ°çåä½ï¼A6ï¼å®æ ¹æåå§åé¡é³è¿°ç¢çæå°æ¥è©¢ï¼ä½¿ç¨éäºæ¥è©¢å·è¡è³è¨æª¢ç´¢ï¼ä¸¦ä½¿ç¨æª¢ç´¢å°çè³ææ´åæ¨çä»¥å¶å®æçµç­æ¡ï¼ä»¥å A7ï¼å®ç¹å¥éå°çæçå­åé¡å©ç¨è³è¨æª¢ç´¢ï¼ä¸¦ä½¿ç¨ç¸éçä¸ä¸æè³è¨éæ°åç­éäºå­åé¡ãæ­¤å¤ï¼éæåºäºæª¢ç´¢å¢å¼·äºå¯¦è©åå¨ä¾åä»£åå§çå¤å¥å¨ï¼åªåèæ®ç¬¦åé«äºå¯¦æ¨æºçæ¨çè·¯å¾ãè LLaMA 3.1 çå¯¦é©çµæé¡¯ç¤ºï¼RARE è½è®éæº LLM éå°è GPT-4 å GPT-4o ç­é å°éæºæ¨¡åç«¶ç­çæè½ãéé ç ç©¶å° RARE å»ºç«çºä¸åå¯æ´åçè§£æ±ºæ¹æ¡ï¼ç¨æ¼æ¹åå¨éè¼¯é£è²«æ§åäºå¯¦å®æ´æ§è³ééè¦çé åä¸­ç LLMã

##### **Minimization of Boolean Complexity in In-Context Concept Learning**
2412.02823v1 by Leroy Z. Wang, R. Thomas McCoy, Shane Steinert-Threlkeld

What factors contribute to the relative success and corresponding
difficulties of in-context learning for Large Language Models (LLMs)? Drawing
on insights from the literature on human concept learning, we test LLMs on
carefully designed concept learning tasks, and show that task performance
highly correlates with the Boolean complexity of the concept. This suggests
that in-context learning exhibits a learning bias for simplicity in a way
similar to humans.

æè¦ï¼åªäºå ç´ ä¿æäºå¤§åèªè¨æ¨¡å (LLM) çæå¢å­¸ç¿çç¸å°æååç¸æçé£åº¦ï¼å©ç¨äººé¡æ¦å¿µå­¸ç¿æç»ä¸­çè¦è§£ï¼æåå¨ç²¾å¿è¨­è¨çæ¦å¿µå­¸ç¿ä»»åä¸­æ¸¬è©¦äº LLMï¼ä¸¦è¡¨æä»»åè¡¨ç¾èæ¦å¿µçå¸æè¤éåº¦é«åº¦ç¸éãéè¡¨ææå¢å­¸ç¿è¡¨ç¾åºé¡ä¼¼æ¼äººé¡çç°¡åå­¸ç¿åè¦ã

##### **CNNSum: Exploring Long-Conext Summarization with Large Language Models in Chinese Novels**
2412.02819v1 by Lingxiao Wei, He Yan, Xiangju Lu, Junmin Zhu, Jun Wang, Wei Zhang

Large Language Models (LLMs) have been well-researched in many long-context
tasks. However, due to high annotation costs, high-quality long-context summary
datasets for training or evaluation are scarce, limiting further research. In
this work, we introduce CNNSum, a new multi-scale Chinese long-context novel
summarization benchmark, including four subsets, length covering
16k\textasciitilde128k, 695 samples in total, the annotations are human-driven.
We evaluate commercial and open-source models on CNNSum and conduct a detailed
analysis. Based on the observations, we further conduct fine-tuning exploration
with short-context summary data. In our study: (1) GPT-4o underperformed, due
to excessive subjective commentary. (2) Currently, long-context summarization
mainly relies on memory ability, small LLMs with stable longer context lengths
are the most cost-effective. Using long data concatenated from short-context
summaries makes a significant improvement. (3) Prompt templates may cause a
large performance gap but can be mitigated through fine-tuning. (4) Fine-tuned
Chat or Instruction versions may harm the Base model and further fine-tuning
cannot bridge performance gap. (5) while models with RoPE base scaling exhibit
strong extrapolation potential, their performance may vary significantly when
combined with other interpolation methods and need careful selection. (6)
CNNSum provides more reliable and insightful evaluation results than other
benchmarks. We release CNNSum to advance research in this field.

æè¦ï¼<paragraph>å¤§åèªè¨æ¨¡å (LLM) å·²å¨è¨±å¤é·èªå¢ä»»åä¸­ç²å¾ååç ç©¶ãç¶èï¼ç±æ¼æ¨è¨»ææ¬é«æï¼ç¨æ¼è¨ç·´æè©ä¼°çé«åè³ªé·èªå¢æè¦è³æéç¨å°ï¼éå¶äºé²ä¸æ­¥çç ç©¶ãå¨éé å·¥ä½ä¸­ï¼æåä»ç´¹äº CNNSumï¼ä¸åæ°çå¤å°ºåº¦ä¸­æé·èªå¢å°èªªæè¦åºæºï¼åæ¬ååå­éï¼é·åº¦æ¶µè 16k\textasciitilde128kï¼ç¸½å± 695 åæ¨£æ¬ï¼æ¨è¨»æ¯ç±äººå·¥é©åçãæåè©ä¼°äº CNNSum ä¸çåæ¥­åéæºæ¨¡åï¼ä¸¦é²è¡äºè©³ç´°çåæãæ ¹æè§å¯çµæï¼æåé²ä¸æ­¥ä½¿ç¨ç­èªå¢æè¦è³æé²è¡å¾®èª¿æ¢ç´¢ãå¨æåçç ç©¶ä¸­ï¼(1) GPT-4o è¡¨ç¾ä¸ä½³ï¼å çºéåº¦çä¸»è§è©è«ã(2) ç®åï¼é·èªå¢æè¦ä¸»è¦ä¾è³´è¨æ¶è½åï¼å·æç©©å®è¼é·èªå¢é·åº¦çå°å LLM æå·ææ¬æçãä½¿ç¨å¾ç­èªå¢æè¦ä¸²æ¥èæçé·è³æå¯ä»¥é¡¯èæåã(3) æç¤ºç¯æ¬å¯è½æå°è´å¾å¤§çæè½å·®è·ï¼ä½å¯ä»¥ééå¾®èª¿ä¾æ¸è¼ã(4) å¾®èª¿çèå¤©ææä»¤çæ¬å¯è½ææå®³åºç¤æ¨¡åï¼é²ä¸æ­¥çå¾®èª¿ç¡æ³å½åæè½å·®è·ã(5) éç¶å·æ RoPE åºç¤ç¸®æ¾çæ¨¡åå±ç¾åºå¼·å¤§çå¤æ¨æ½åï¼ä½å®åèå¶ä»å§ææ¹æ³çµåä½¿ç¨æï¼æè½å¯è½æé¡¯èè®åï¼éè¦ä»ç´°é¸æã(6) CNNSum æä¾æ¯å¶ä»åºæºæ´å¯é ä¸æè¦å°çè©ä¼°çµæãæåç¼å¸ CNNSum ä»¥æ¨åæ­¤é åçç ç©¶ã</paragraph>

##### **Gaussian Splatting Under Attack: Investigating Adversarial Noise in 3D Objects**
2412.02803v1 by Abdurrahman Zeybey, Mehmet Ergezer, Tommy Nguyen

3D Gaussian Splatting has advanced radiance field reconstruction, enabling
high-quality view synthesis and fast rendering in 3D modeling. While
adversarial attacks on object detection models are well-studied for 2D images,
their impact on 3D models remains underexplored. This work introduces the
Masked Iterative Fast Gradient Sign Method (M-IFGSM), designed to generate
adversarial noise targeting the CLIP vision-language model. M-IFGSM
specifically alters the object of interest by focusing perturbations on masked
regions, degrading the performance of CLIP's zero-shot object detection
capability when applied to 3D models. Using eight objects from the Common
Objects 3D (CO3D) dataset, we demonstrate that our method effectively reduces
the accuracy and confidence of the model, with adversarial noise being nearly
imperceptible to human observers. The top-1 accuracy in original model renders
drops from 95.4\% to 12.5\% for train images and from 91.2\% to 35.4\% for test
images, with confidence levels reflecting this shift from true classification
to misclassification, underscoring the risks of adversarial attacks on 3D
models in applications such as autonomous driving, robotics, and surveillance.
The significance of this research lies in its potential to expose
vulnerabilities in modern 3D vision models, including radiance fields,
prompting the development of more robust defenses and security measures in
critical real-world applications.

æè¦ï¼<paragraph>3D é«æ¯æ£å°æåäºè¼»ç§å ´éå»ºï¼å¯¦ç¾äº 3D å»ºæ¨¡ä¸­çé«åè³ªè¦ååæåå¿«éæ¸²æãåç®¡å°ç©é«æª¢æ¸¬æ¨¡åçå°ææ»æå¨ 2D ååä¸­å¾å°äºå¾å¥½çç ç©¶ï¼ä½å®åå° 3D æ¨¡åçå½±é¿ä»æªå¾å°ååæ¢ç´¢ãéé å·¥ä½å¼å¥äºèçè¿­ä»£å¿«éæ¢¯åº¦ç¬¦èæ¹æ³ (M-IFGSM)ï¼æ¨å¨ç¢çéå° CLIP è¦è¦ºèªè¨æ¨¡åçå°æåªè²ãM-IFGSM ééå°æ¾åéä¸­å¨èçååä¾å°éæ¹è®æèè¶£çç©é«ï¼å¾èéä½ CLIP é¶æ¬¡ç©é«æª¢æ¸¬è½åå¨æç¨æ¼ 3D æ¨¡åæçæ§è½ãä½¿ç¨ Common Objects 3D (CO3D) æ¸æéä¸­çå«åç©é«ï¼æåè­æäºæåçæ¹æ³ææå°éä½äºæ¨¡åçæºç¢ºæ§åç½®ä¿¡åº¦ï¼èå°æåªè²å°äººé¡è§å¯èä¾èªªå¹¾ä¹æ¯ä¸å¯å¯è¦ºçãåå§æ¨¡åæ¸²æä¸­çå 1 æºç¢ºçå¾è¨ç·´ååç 95.4% éè³ 12.5%ï¼å¾æ¸¬è©¦ååç 91.2% éè³ 35.4%ï¼ç½®ä¿¡åº¦åæ äºéç¨®å¾æ­£ç¢ºåé¡å°é¯èª¤åé¡çè½è®ï¼å¼·èª¿äºå°ææ»æå° 3D çé¢¨éªå¨èªåé§é§ãæ©å¨äººåç£æ§ç­æç¨ä¸­çæ¨¡åãéé ç ç©¶çæç¾©å¨æ¼å®æå¯è½æ­ç¤ºç¾ä»£ 3D è¦è¦ºæ¨¡åï¼åæ¬è¼»ç§å ´ï¼ä¸­çæ¼æ´ï¼ä¿ä½¿å¨ééµçç¾å¯¦ä¸çæç¨ä¸­éç¼æ´å¼·å¤§çé²ç¦¦åå®å¨æªæ½ã</paragraph>

