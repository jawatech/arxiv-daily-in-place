{"2411.10414": {"publish_time": "2024-11-15", "title": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations", "paper_summary": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 Llama Guard 3 Vision\uff0c\u9019\u662f\u4e00\u7a2e\u57fa\u65bc\u591a\u6a21\u614b LLM \u7684\u591a\u6a21\u614b\u9632\u8b77\u63aa\u65bd\uff0c\u7528\u65bc\u6d89\u53ca\u5f71\u50cf\u7406\u89e3\u7684\u4eba\u5de5\u667a\u6167\u5c0d\u8a71\uff1a\u5b83\u53ef\u7528\u65bc\u4fdd\u8b77\u591a\u6a21\u614b LLM \u8f38\u5165\uff08\u63d0\u793a\u5206\u985e\uff09\u548c\u8f38\u51fa\uff08\u56de\u61c9\u5206\u985e\uff09\u7684\u5167\u5bb9\u3002\u8207\u4e4b\u524d\u7684\u7d14\u6587\u5b57 Llama Guard \u7248\u672c\uff08Inan et al., 2023; Llama Team, 2024b,a\uff09\u4e0d\u540c\uff0c\u5b83\u7279\u5225\u8a2d\u8a08\u7528\u65bc\u652f\u63f4\u5f71\u50cf\u63a8\u7406\u7528\u4f8b\uff0c\u4e26\u91dd\u5c0d\u5075\u6e2c\u6709\u5bb3\u7684\u591a\u6a21\u614b\uff08\u6587\u5b57\u548c\u5f71\u50cf\uff09\u63d0\u793a\u548c\u5c0d\u9019\u4e9b\u63d0\u793a\u7684\u6587\u5b57\u56de\u61c9\u9032\u884c\u6700\u4f73\u5316\u3002Llama Guard 3 Vision \u5728 Llama 3.2-Vision \u4e0a\u9032\u884c\u5fae\u8abf\uff0c\u4e26\u4f7f\u7528 MLCommons \u5206\u985e\u6cd5\u5728\u5167\u90e8\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5c55\u73fe\u5f37\u52c1\u6548\u80fd\u3002\u6211\u5011\u4e5f\u6e2c\u8a66\u5b83\u5c0d\u6297\u653b\u64ca\u7684\u7a69\u5065\u6027\u3002\u6211\u5011\u76f8\u4fe1 Llama Guard 3 Vision \u53ef\u4ee5\u4f5c\u70ba\u4e00\u500b\u826f\u597d\u7684\u8d77\u9ede\uff0c\u70ba\u5177\u5099\u591a\u6a21\u614b\u529f\u80fd\u7684\u4eba\u5de5\u667a\u6167\u5c0d\u8a71\u5efa\u7f6e\u66f4\u5f37\u5927\u4e14\u7a69\u5065\u7684\u5167\u5bb9\u5be9\u6838\u5de5\u5177\u3002", "author": "Jianfeng Chi et.al.", "authors": "Jianfeng Chi, Ujjwal Karn, Hongyuan Zhan, Eric Smith, Javier Rando, Yiming Zhang, Kate Plawiak, Zacharie Delpierre Coudert, Kartikeya Upasani, Mahesh Pasupuleti", "id": "2411.10414v1", "paper_url": "http://arxiv.org/abs/2411.10414v1", "repo": "null"}}