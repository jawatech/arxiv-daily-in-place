{"2406.10083": {"publish_time": "2024-06-14", "title": "On the Evaluation of Speech Foundation Models for Spoken Language Understanding", "paper_summary": "The Spoken Language Understanding Evaluation (SLUE) suite of benchmark tasks\nwas recently introduced to address the need for open resources and benchmarking\nof complex spoken language understanding (SLU) tasks, including both\nclassification and sequence generation tasks, on natural speech. The benchmark\nhas demonstrated preliminary success in using pre-trained speech foundation\nmodels (SFM) for these SLU tasks. However, the community still lacks a\nfine-grained understanding of the comparative utility of different SFMs.\nInspired by this, we ask: which SFMs offer the most benefits for these complex\nSLU tasks, and what is the most effective approach for incorporating these\nSFMs? To answer this, we perform an extensive evaluation of multiple supervised\nand self-supervised SFMs using several evaluation protocols: (i) frozen SFMs\nwith a lightweight prediction head, (ii) frozen SFMs with a complex prediction\nhead, and (iii) fine-tuned SFMs with a lightweight prediction head. Although\nthe supervised SFMs are pre-trained on much more speech recognition data (with\nlabels), they do not always outperform self-supervised SFMs; the latter tend to\nperform at least as well as, and sometimes better than, supervised SFMs,\nespecially on the sequence generation tasks in SLUE. While there is no\nuniversally optimal way of incorporating SFMs, the complex prediction head\ngives the best performance for most tasks, although it increases the inference\ntime. We also introduce an open-source toolkit and performance leaderboard,\nSLUE-PERB, for these tasks and modeling strategies.", "paper_summary_zh": "\u53e3\u8a9e\u8a9e\u8a00\u7406\u89e3\u8a55\u4f30 (SLUE) \u57fa\u6e96\u4efb\u52d9\u5957\u4ef6\u6700\u8fd1\u88ab\u63d0\u51fa\uff0c\u4ee5\u6eff\u8db3\u958b\u653e\u8cc7\u6e90\u548c\u8907\u96dc\u53e3\u8a9e\u8a9e\u8a00\u7406\u89e3 (SLU) \u4efb\u52d9\u7684\u57fa\u6e96\u6e2c\u8a66\u9700\u6c42\uff0c\u5305\u62ec\u81ea\u7136\u8a9e\u97f3\u4e2d\u7684\u5206\u985e\u548c\u5e8f\u5217\u751f\u6210\u4efb\u52d9\u3002\u8a72\u57fa\u6e96\u6e2c\u8a66\u5df2\u8b49\u660e\u5728\u4f7f\u7528\u9810\u8a13\u7df4\u8a9e\u97f3\u57fa\u790e\u6a21\u578b (SFM) \u57f7\u884c\u9019\u4e9b SLU \u4efb\u52d9\u6642\u53d6\u5f97\u4e86\u521d\u6b65\u6210\u529f\u3002\u7136\u800c\uff0c\u793e\u7fa4\u4ecd\u7136\u7f3a\u4e4f\u5c0d\u4e0d\u540c SFM \u6bd4\u8f03\u6548\u7528\u7684\u6df1\u5165\u4e86\u89e3\u3002\u53d7\u6b64\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4ee5\u4e0b\u554f\u984c\uff1a\u54ea\u4e9b SFM \u70ba\u9019\u4e9b\u8907\u96dc\u7684 SLU \u4efb\u52d9\u63d0\u4f9b\u4e86\u6700\u5927\u7684\u597d\u8655\uff0c\u4ee5\u53ca\u6574\u5408\u9019\u4e9b SFM \u6700\u6709\u6548\u7684\u65b9\u6cd5\u662f\u4ec0\u9ebc\uff1f\u70ba\u4e86\u56de\u7b54\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u4f7f\u7528\u591a\u500b\u8a55\u4f30\u5354\u5b9a\u5c0d\u591a\u500b\u76e3\u7763\u5f0f\u548c\u81ea\u76e3\u7763\u5f0f SFM \u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u8a55\u4f30\uff1a(i) \u5177\u6709\u8f15\u91cf\u7d1a\u9810\u6e2c\u982d\u7684\u51cd\u7d50\u5f0f SFM\u3001(ii) \u5177\u6709\u8907\u96dc\u9810\u6e2c\u982d\u7684\u51cd\u7d50\u5f0f SFM\uff0c\u4ee5\u53ca (iii) \u5177\u6709\u8f15\u91cf\u7d1a\u9810\u6e2c\u982d\u7684\u5fae\u8abf\u5f0f SFM\u3002\u5118\u7ba1\u76e3\u7763\u5f0f SFM \u5728\u66f4\u591a\u8a9e\u97f3\u8fa8\u8b58\u8cc7\u6599 (\u5e36\u6a19\u7c64) \u4e0a\u9032\u884c\u9810\u8a13\u7df4\uff0c\u4f46\u5b83\u5011\u4e26\u4e0d\u7e3d\u662f\u512a\u65bc\u81ea\u76e3\u7763\u5f0f SFM\uff1b\u5f8c\u8005\u5f80\u5f80\u8868\u73fe\u5f97\u81f3\u5c11\u548c\u76e3\u7763\u5f0f SFM \u4e00\u6a23\u597d\uff0c\u6709\u6642\u751a\u81f3\u66f4\u597d\uff0c\u7279\u5225\u662f\u5728 SLUE \u4e2d\u7684\u5e8f\u5217\u751f\u6210\u4efb\u52d9\u4e0a\u3002\u96d6\u7136\u6c92\u6709\u666e\u904d\u6700\u4f73\u7684\u6574\u5408 SFM \u65b9\u6cd5\uff0c\u4f46\u8907\u96dc\u7684\u9810\u6e2c\u982d\u5728\u5927\u591a\u6578\u4efb\u52d9\u4e2d\u8868\u73fe\u6700\u4f73\uff0c\u5118\u7ba1\u5b83\u589e\u52a0\u4e86\u63a8\u7406\u6642\u9593\u3002\u6211\u5011\u9084\u70ba\u9019\u4e9b\u4efb\u52d9\u548c\u5efa\u6a21\u7b56\u7565\u5f15\u5165\u4e86\u958b\u6e90\u5de5\u5177\u5305\u548c\u6548\u80fd\u6392\u884c\u699c SLUE-PERB\u3002", "author": "Siddhant Arora et.al.", "authors": "Siddhant Arora, Ankita Pasad, Chung-Ming Chien, Jionghao Han, Roshan Sharma, Jee-weon Jung, Hira Dhamyal, William Chen, Suwon Shon, Hung-yi Lee, Karen Livescu, Shinji Watanabe", "id": "2406.10083v1", "paper_url": "http://arxiv.org/abs/2406.10083v1", "repo": "null"}}