{"2501.01028": {"publish_time": "2025-01-02", "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model", "paper_summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.", "paper_summary_zh": "\u968f\u7740\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u76db\u884c\uff0c\u5d4c\u5165\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u5173\u952e\u3002\u5c3d\u7ba1\u901a\u7528\u5d4c\u5165\u6a21\u578b\u7684\u6570\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u4f46\u5148\u524d\u7684\u7814\u7a76\u5e38\u5e38\u5ffd\u89c6\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u7684\u5173\u952e\u4f5c\u7528\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86 KaLM-Embedding\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u7528\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\uff0c\u5b83\u5229\u7528\u4e86\u5927\u91cf\u66f4\u5e72\u51c0\u3001\u66f4\u591a\u6837\u5316\u548c\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u8bad\u7ec3\u6570\u636e\u3002\u6211\u4eec\u7684\u6a21\u578b\u5df2\u7ecf\u8fc7\u5173\u952e\u6280\u672f\u7684\u57f9\u8bad\uff0c\u4e8b\u5b9e\u8bc1\u660e\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\uff1a(1) \u57fa\u4e8e\u89d2\u8272\u7684\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u521b\u5efa\u4ece LLM \u4e2d\u63d0\u53d6\u7684\u591a\u6837\u5316\u793a\u4f8b\uff0c(2) \u6392\u540d\u4e00\u81f4\u6027\u8fc7\u6ee4\uff0c\u7528\u4e8e\u53bb\u9664\u4fe1\u606f\u91cf\u8f83\u5c11\u7684\u6837\u672c\uff0c\u4ee5\u53ca (3) \u534a\u540c\u8d28\u4efb\u52a1\u6279\u91cf\u91c7\u6837\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u6211\u4eec\u91c7\u7528 Qwen2-0.5B \u4f5c\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6446\u8131\u4e86\u4f20\u7edf\u7684 BERT \u7c7b\u67b6\u6784\uff0c\u4fc3\u8fdb\u4e86\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5bf9\u901a\u7528\u5d4c\u5165\u4efb\u52a1\u7684\u9002\u5e94\u3002\u5bf9\u591a\u4e2a\u8bed\u8a00\u7684 MTEB \u57fa\u51c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u4f18\u4e8e\u5176\u4ed6\u5177\u6709\u53ef\u6bd4\u5927\u5c0f\u7684\u6a21\u578b\uff0c\u4e3a\u5177\u6709 <1B \u53c2\u6570\u7684\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "author": "Xinshuo Hu et.al.", "authors": "Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, Min Zhang", "id": "2501.01028v1", "paper_url": "http://arxiv.org/abs/2501.01028v1", "repo": "https://github.com/HITsz-TMG/KaLM-Embedding"}}