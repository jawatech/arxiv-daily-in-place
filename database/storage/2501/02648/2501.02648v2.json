{"2501.02648": {"publish_time": "2025-01-05", "title": "Representation Learning of Lab Values via Masked AutoEncoder", "paper_summary": "Accurate imputation of missing laboratory values in electronic health records\n(EHRs) is critical to enable robust clinical predictions and reduce biases in\nAI systems in healthcare. Existing methods, such as variational autoencoders\n(VAEs) and decision tree-based approaches such as XGBoost, struggle to model\nthe complex temporal and contextual dependencies in EHR data, mainly in\nunderrepresented groups. In this work, we propose Lab-MAE, a novel\ntransformer-based masked autoencoder framework that leverages self-supervised\nlearning for the imputation of continuous sequential lab values. Lab-MAE\nintroduces a structured encoding scheme that jointly models laboratory test\nvalues and their corresponding timestamps, enabling explicit capturing temporal\ndependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that\nLab-MAE significantly outperforms the state-of-the-art baselines such as\nXGBoost across multiple metrics, including root mean square error (RMSE),\nR-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves\nequitable performance across demographic groups of patients, advancing fairness\nin clinical predictions. We further investigate the role of follow-up\nlaboratory values as potential shortcut features, revealing Lab-MAE's\nrobustness in scenarios where such data is unavailable. The findings suggest\nthat our transformer-based architecture, adapted to the characteristics of the\nEHR data, offers a foundation model for more accurate and fair clinical\nimputation models. In addition, we measure and compare the carbon footprint of\nLab-MAE with the baseline XGBoost model, highlighting its environmental\nrequirements.", "paper_summary_zh": "<paragraph>\u6e96\u78ba\u4f30\u7b97\u96fb\u5b50\u5065\u5eb7\u8a18\u9304 (EHR) \u4e2d\u907a\u5931\u7684\u5be6\u9a57\u5ba4\u503c\u5c0d\u65bc\u555f\u7528\u7a69\u5065\u7684\u81e8\u5e8a\u9810\u6e2c\u548c\u6e1b\u5c11\u91ab\u7642\u4fdd\u5065\u4e2d AI \u7cfb\u7d71\u7684\u504f\u5dee\u81f3\u95dc\u91cd\u8981\u3002\u73fe\u6709\u65b9\u6cd5\uff08\u4f8b\u5982\u8b8a\u7570\u81ea\u52d5\u7de8\u78bc\u5668 (VAE) \u548c\u57fa\u65bc\u6c7a\u7b56\u6a39\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982 XGBoost\uff09\u96e3\u4ee5\u5efa\u6a21 EHR \u8cc7\u6599\u4e2d\u8907\u96dc\u7684\u6642\u9593\u548c\u4e0a\u4e0b\u6587\u4f9d\u8cf4\u6027\uff0c\u7279\u5225\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7fa4\u7d44\u4e2d\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa Lab-MAE\uff0c\u4e00\u500b\u65b0\u7a4e\u7684\u57fa\u65bc Transformer \u7684\u906e\u7f69\u81ea\u52d5\u7de8\u78bc\u5668\u6846\u67b6\uff0c\u5b83\u5229\u7528\u81ea\u6211\u76e3\u7763\u5b78\u7fd2\u4f86\u4f30\u7b97\u9023\u7e8c\u9806\u5e8f\u5be6\u9a57\u5ba4\u503c\u3002Lab-MAE \u5f15\u5165\u4e86\u4e00\u500b\u7d50\u69cb\u5316\u7de8\u78bc\u65b9\u6848\uff0c\u5b83\u806f\u5408\u5efa\u6a21\u5be6\u9a57\u5ba4\u6e2c\u8a66\u503c\u53ca\u5176\u5c0d\u61c9\u7684\u6642\u9593\u6233\uff0c\u5f9e\u800c\u80fd\u5920\u660e\u78ba\u6355\u6349\u6642\u9593\u4f9d\u8cf4\u6027\u3002\u5728 MIMIC-IV \u8cc7\u6599\u96c6\u4e0a\u7684\u7d93\u9a57\u8a55\u4f30\u8868\u660e\uff0cLab-MAE \u5728\u5305\u62ec\u5747\u65b9\u6839\u8aa4\u5dee (RMSE)\u3001R \u5e73\u65b9 (R2) \u548c Wasserstein \u8ddd\u96e2 (WD) \u5728\u5167\u7684\u591a\u9805\u6307\u6a19\u4e0a\u986f\u8457\u512a\u65bc XGBoost \u7b49\u6700\u5148\u9032\u7684\u57fa\u6e96\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cLab-MAE \u5728\u60a3\u8005\u7684\u4eba\u53e3\u7d71\u8a08\u7fa4\u7d44\u4e2d\u53d6\u5f97\u4e86\u516c\u5e73\u7684\u8868\u73fe\uff0c\u5f9e\u800c\u63d0\u5347\u4e86\u81e8\u5e8a\u9810\u6e2c\u4e2d\u7684\u516c\u5e73\u6027\u3002\u6211\u5011\u9032\u4e00\u6b65\u7814\u7a76\u4e86\u5f8c\u7e8c\u5be6\u9a57\u5ba4\u503c\u4f5c\u70ba\u6f5b\u5728\u6377\u5f91\u7279\u5fb5\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u4e86 Lab-MAE \u5728\u6b64\u985e\u8cc7\u6599\u4e0d\u53ef\u7528\u7684\u60c5\u6cc1\u4e0b\u7684\u7a69\u5065\u6027\u3002\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u6211\u5011\u57fa\u65bc Transformer \u7684\u67b6\u69cb\uff08\u8abf\u6574\u70ba EHR \u8cc7\u6599\u7684\u7279\u5fb5\uff09\u70ba\u66f4\u6e96\u78ba\u548c\u516c\u5e73\u7684\u81e8\u5e8a\u4f30\u7b97\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u500b\u57fa\u790e\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u6e2c\u91cf\u4e26\u6bd4\u8f03\u4e86 Lab-MAE \u8207\u57fa\u6e96 XGBoost \u6a21\u578b\u7684\u78b3\u8db3\u8de1\uff0c\u7a81\u51fa\u4e86\u5176\u74b0\u5883\u9700\u6c42\u3002</paragraph>", "author": "David Restrepo et.al.", "authors": "David Restrepo, Chenwei Wu, Yueran Jia, Jaden K. Sun, Jack Gallifant, Catherine G. Bielick, Yugang Jia, Leo A. Celi", "id": "2501.02648v2", "paper_url": "http://arxiv.org/abs/2501.02648v2", "repo": "null"}}