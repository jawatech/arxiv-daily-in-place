{"2501.12067": {"publish_time": "2025-01-21", "title": "EDoRA: Efficient Weight-Decomposed Low-Rank Adaptation via Singular Value Decomposition", "paper_summary": "Parameter-efficient fine-tuning methods, such as LoRA, reduces the number of\ntrainable parameters. However, they often suffer from scalability issues and\ndifferences between their learning pattern and full fine-tuning. To overcome\nthese limitations, we propose Efficient Weight-Decomposed Low-Rank Adaptation\n(EDoRA): a novel PEFT method that decomposes pre-trained weights into magnitude\nand directional components. By freezing low-rank matrices, initializing them by\nsingular value decomposition, and introducing a small trainable matrix between\nthem, EDoRA achieves substantial reduction in trainable parameters while\nmaintaining learning capacity. Experimental results on the GLUE benchmark\ndemonstrate that EDoRA achieves competitive or superior performance compared to\nstate-of-the-art methods, such as LoRA and DoRA, with up to 30x fewer trainable\nparameters. This makes EDoRA a highly efficient solution for adapting LLMs to\ndiverse tasks under memory-constrained settings. Code is available at\nhttps://github.com/Hamid-Nasiri/EDoRA .", "paper_summary_zh": "\u53c3\u6578\u9ad8\u6548\u5fae\u8abf\u65b9\u6cd5\uff08\u4f8b\u5982 LoRA\uff09\u6e1b\u5c11\u4e86\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u6578\u91cf\u3002\u7136\u800c\uff0c\u5b83\u5011\u7d93\u5e38\u6703\u9047\u5230\u53ef\u64f4\u5145\u6027\u554f\u984c\uff0c\u800c\u4e14\u5b83\u5011\u7684\u5b78\u7fd2\u6a21\u5f0f\u8207\u5b8c\u5168\u5fae\u8abf\u4e4b\u9593\u5b58\u5728\u5dee\u7570\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u9ad8\u6548\u6b0a\u91cd\u5206\u89e3\u4f4e\u79e9\u9069\u61c9 (EDoRA)\uff1a\u4e00\u7a2e\u5c07\u9810\u8a13\u7df4\u6b0a\u91cd\u5206\u89e3\u70ba\u5e45\u5ea6\u548c\u65b9\u5411\u5206\u91cf\u7684\u5168\u65b0 PEFT \u65b9\u6cd5\u3002\u900f\u904e\u51cd\u7d50\u4f4e\u79e9\u77e9\u9663\u3001\u4f7f\u7528\u5947\u7570\u503c\u5206\u89e3\u5c0d\u5b83\u5011\u9032\u884c\u521d\u59cb\u5316\uff0c\u4e26\u5728\u5b83\u5011\u4e4b\u9593\u5f15\u5165\u4e00\u500b\u5c0f\u7684\u53ef\u8a13\u7df4\u77e9\u9663\uff0cEDoRA \u5be6\u73fe\u4e86\u53ef\u8a13\u7df4\u53c3\u6578\u7684\u5927\u5e45\u6e1b\u5c11\uff0c\u540c\u6642\u7dad\u6301\u4e86\u5b78\u7fd2\u80fd\u529b\u3002GLUE \u57fa\u6e96\u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u8b49\u660e\uff0c\u8207 LoRA \u548c DoRA \u7b49\u6700\u5148\u9032\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cEDoRA \u9054\u5230\u4e86\u5177\u6709\u7af6\u722d\u529b\u6216\u66f4\u512a\u7570\u7684\u6548\u80fd\uff0c\u53ef\u8a13\u7df4\u53c3\u6578\u6e1b\u5c11\u4e86\u591a\u9054 30 \u500d\u3002\u9019\u4f7f\u5f97 EDoRA \u6210\u70ba\u5728\u8a18\u61b6\u9ad4\u53d7\u9650\u8a2d\u5b9a\u4e0b\u5c07 LLM \u9069\u61c9\u5230\u5404\u7a2e\u4efb\u52d9\u7684\u9ad8\u6548\u89e3\u6c7a\u65b9\u6848\u3002\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/Hamid-Nasiri/EDoRA \u53d6\u5f97\u3002", "author": "Hamid Nasiri et.al.", "authors": "Hamid Nasiri, Peter Garraghan", "id": "2501.12067v1", "paper_url": "http://arxiv.org/abs/2501.12067v1", "repo": "https://github.com/hamid-nasiri/edora"}}