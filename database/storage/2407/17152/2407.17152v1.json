{"2407.17152": {"publish_time": "2024-07-24", "title": "XMeCap: Meme Caption Generation with Sub-Image Adaptability", "paper_summary": "Humor, deeply rooted in societal meanings and cultural details, poses a\nunique challenge for machines. While advances have been made in natural\nlanguage processing, real-world humor often thrives in a multi-modal context,\nencapsulated distinctively by memes. This paper poses a particular emphasis on\nthe impact of multi-images on meme captioning. After that, we introduce the\n\\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning\nand reinforcement learning based on an innovative reward model, which factors\nin both global and local similarities between visuals and text. Our results,\nbenchmarked against contemporary models, manifest a marked improvement in\ncaption generation for both single-image and multi-image memes, as well as\ndifferent meme categories. \\textsc{XMeCap} achieves an average evaluation score\nof 75.85 for single-image memes and 66.32 for multi-image memes, outperforming\nthe best baseline by 3.71\\% and 4.82\\%, respectively. This research not only\nestablishes a new frontier in meme-related studies but also underscores the\npotential of machines in understanding and generating humor in a multi-modal\nsetting.", "paper_summary_zh": "\u5e7d\u9ed8\u6df1\u6df1\u690d\u6839\u65bc\u793e\u6703\u610f\u7fa9\u548c\u6587\u5316\u7d30\u7bc0\u4e2d\uff0c\u5c0d\u6a5f\u5668\u4f86\u8aaa\u662f\u4e00\u500b\u7368\u7279\u7684\u6311\u6230\u3002\u5118\u7ba1\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u9032\u5c55\uff0c\u4f46\u73fe\u5be6\u4e16\u754c\u7684\u5e7d\u9ed8\u901a\u5e38\u5728\u591a\u6a21\u614b\u8a9e\u5883\u4e2d\u84ec\u52c3\u767c\u5c55\uff0c\u4e26\u7531\u6a21\u56e0\u7368\u7279\u5730\u6982\u62ec\u3002\u672c\u6587\u7279\u5225\u5f37\u8abf\u591a\u5716\u50cf\u5c0d\u6a21\u56e0\u6a19\u984c\u7684\u5f71\u97ff\u3002\u5728\u90a3\u4e4b\u5f8c\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\\textsc{XMeCap}\u6846\u67b6\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7a4e\u7684\u65b9\u6cd5\uff0c\u63a1\u7528\u57fa\u65bc\u5275\u65b0\u734e\u52f5\u6a21\u578b\u7684\u76e3\u7763\u5fae\u8abf\u548c\u5f37\u5316\u5b78\u7fd2\uff0c\u8a72\u6a21\u578b\u8003\u616e\u4e86\u8996\u89ba\u548c\u6587\u672c\u4e4b\u9593\u7684\u5168\u5c40\u548c\u5c40\u90e8\u76f8\u4f3c\u6027\u3002\u6211\u5011\u7684\u7d50\u679c\u8207\u7576\u4ee3\u6a21\u578b\u9032\u884c\u4e86\u57fa\u6e96\u6e2c\u8a66\uff0c\u8868\u660e\u55ae\u5716\u50cf\u548c\u591a\u5716\u50cf\u6a21\u56e0\u4ee5\u53ca\u4e0d\u540c\u6a21\u56e0\u985e\u5225\u7684\u6a19\u984c\u751f\u6210\u90fd\u6709\u986f\u8457\u6539\u9032\u3002\\textsc{XMeCap}\u5c0d\u55ae\u5716\u50cf\u6a21\u56e0\u7684\u5e73\u5747\u8a55\u5206\u70ba75.85\uff0c\u5c0d\u591a\u5716\u50cf\u6a21\u56e0\u7684\u5e73\u5747\u8a55\u5206\u70ba66.32\uff0c\u5206\u5225\u512a\u65bc\u6700\u4f73\u57fa\u7dda3.71%\u548c4.82%\u3002\u9019\u9805\u7814\u7a76\u4e0d\u50c5\u5728\u6a21\u56e0\u76f8\u95dc\u7814\u7a76\u4e2d\u958b\u95e2\u4e86\u4e00\u500b\u65b0\u9818\u57df\uff0c\u800c\u4e14\u5f37\u8abf\u4e86\u6a5f\u5668\u5728\u591a\u6a21\u614b\u74b0\u5883\u4e2d\u7406\u89e3\u548c\u7522\u751f\u5e7d\u9ed8\u7684\u6f5b\u529b\u3002", "author": "Yuyan Chen et.al.", "authors": "Yuyan Chen, Songzhou Yan, Zhihong Zhu, Zhixu Li, Yanghua Xiao", "id": "2407.17152v1", "paper_url": "http://arxiv.org/abs/2407.17152v1", "repo": "null"}}