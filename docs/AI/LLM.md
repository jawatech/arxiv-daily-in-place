
### LLM
|Publish Date|Title|Authors|Homepage|Code|
| :---: | :---: | :---: | :---: | :---: |
|**2024-12-05**|**PaintScene4D: Consistent 4D Scene Generation from Text Prompts**|Vinayak Gupta et.al.|[2412.04471v1](http://arxiv.org/abs/2412.04471v1)|null|
|**2024-12-05**|**QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos**|Sharath Girish et.al.|[2412.04469v1](http://arxiv.org/abs/2412.04469v1)|null|
|**2024-12-05**|**VisionZip: Longer is Better but Not Necessary in Vision Language Models**|Senqiao Yang et.al.|[2412.04467v1](http://arxiv.org/abs/2412.04467v1)|null|
|**2024-12-05**|**Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection**|Enshen Zhou et.al.|[2412.04455v1](http://arxiv.org/abs/2412.04455v1)|null|
|**2024-12-05**|**Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction**|Yiheng Xu et.al.|[2412.04454v1](http://arxiv.org/abs/2412.04454v1)|null|
|**2024-12-05**|**p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay**|Jun Zhang et.al.|[2412.04449v1](http://arxiv.org/abs/2412.04449v1)|null|
|**2024-12-05**|**EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios**|Lu Qiu et.al.|[2412.04447v1](http://arxiv.org/abs/2412.04447v1)|null|
|**2024-12-05**|**Moto: Latent Motion Token as the Bridging Language for Robot Manipulation**|Yi Chen et.al.|[2412.04445v1](http://arxiv.org/abs/2412.04445v1)|null|
|**2024-12-05**|**CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing**|Yen-Ju Lu et.al.|[2412.04425v1](http://arxiv.org/abs/2412.04425v1)|null|
|**2024-12-05**|**Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion**|Jiuhai Chen et.al.|[2412.04424v1](http://arxiv.org/abs/2412.04424v1)|null|
|**2024-12-05**|**FedDUAL: A Dual-Strategy with Adaptive Loss and Dynamic Aggregation for Mitigating Data Heterogeneity in Federated Learning**|Pranab Sahoo et.al.|[2412.04416v1](http://arxiv.org/abs/2412.04416v1)|null|
|**2024-12-05**|**Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation**|Xuying Li et.al.|[2412.04415v1](http://arxiv.org/abs/2412.04415v1)|null|
|**2024-12-05**|**Establishing Task Scaling Laws via Compute-Efficient Model Ladders**|Akshita Bhagia et.al.|[2412.04403v1](http://arxiv.org/abs/2412.04403v1)|null|
|**2024-12-05**|**Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction**|Yuanhui Huang et.al.|[2412.04384v1](http://arxiv.org/abs/2412.04384v1)|null|
|**2024-12-05**|**Discriminative Fine-tuning of LVLMs**|Yassine Ouali et.al.|[2412.04378v1](http://arxiv.org/abs/2412.04378v1)|null|
|**2024-12-05**|**Machine Theory of Mind for Autonomous Cyber-Defence**|Luke Swaby et.al.|[2412.04367v1](http://arxiv.org/abs/2412.04367v1)|null|
|**2024-12-05**|**BhashaVerse : Translation Ecosystem for Indian Subcontinent Languages**|Vandan Mujadia et.al.|[2412.04351v1](http://arxiv.org/abs/2412.04351v1)|null|
|**2024-12-05**|**RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse**|Zhouyingcheng Liao et.al.|[2412.04343v1](http://arxiv.org/abs/2412.04343v1)|null|
|**2024-12-05**|**Retrieval-Augmented Machine Translation with Unstructured Knowledge**|Jiaan Wang et.al.|[2412.04342v1](http://arxiv.org/abs/2412.04342v1)|null|
|**2024-12-05**|**Action Mapping for Reinforcement Learning in Continuous Environments with Constraints**|Mirco Theile et.al.|[2412.04327v1](http://arxiv.org/abs/2412.04327v1)|null|
|**2024-12-05**|**The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation**|Fredrik Carlsson et.al.|[2412.04318v1](http://arxiv.org/abs/2412.04318v1)|null|
|**2024-12-05**|**Densing Law of LLMs**|Chaojun Xiao et.al.|[2412.04315v1](http://arxiv.org/abs/2412.04315v1)|null|
|**2024-12-05**|**ALMA: Alignment with Minimal Annotation**|Michihiro Yasunaga et.al.|[2412.04305v1](http://arxiv.org/abs/2412.04305v1)|null|
|**2024-12-05**|**T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts**|Ziwei Huang et.al.|[2412.04300v1](http://arxiv.org/abs/2412.04300v1)|null|
|**2024-12-05**|**SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model**|Zhenglin Huang et.al.|[2412.04292v1](http://arxiv.org/abs/2412.04292v1)|null|
|**2024-12-05**|**Evolutionary Pre-Prompt Optimization for Mathematical Reasoning**|Mathurin Videau et.al.|[2412.04291v1](http://arxiv.org/abs/2412.04291v1)|null|
|**2024-12-05**|**Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic**|Zaid Alyafeai et.al.|[2412.04277v1](http://arxiv.org/abs/2412.04277v1)|null|
|**2024-12-05**|**PoTable: Programming Standardly on Table-based Reasoning Like a Human Analyst**|Qingyang Mao et.al.|[2412.04272v1](http://arxiv.org/abs/2412.04272v1)|null|
|**2024-12-05**|**Representation Purification for End-to-End Speech Translation**|Chengwei Zhang et.al.|[2412.04266v1](http://arxiv.org/abs/2412.04266v1)|null|
|**2024-12-05**|**Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier**|John Dang et.al.|[2412.04261v1](http://arxiv.org/abs/2412.04261v1)|null|
|**2024-12-05**|**Enhancing Whole Slide Image Classification through Supervised Contrastive Domain Adaptation**|Ilán Carretero et.al.|[2412.04260v1](http://arxiv.org/abs/2412.04260v1)|null|
|**2024-12-05**|**CLINICSUM: Utilizing Language Models for Generating Clinical Summaries from Patient-Doctor Conversations**|Subash Neupane et.al.|[2412.04254v1](http://arxiv.org/abs/2412.04254v1)|null|
|**2024-12-05**|**A History of Philosophy in Colombia through Topic Modelling**|Juan R. Loaiza et.al.|[2412.04236v1](http://arxiv.org/abs/2412.04236v1)|null|
|**2024-12-05**|**Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots**|Maria Paola Priola et.al.|[2412.04235v1](http://arxiv.org/abs/2412.04235v1)|null|
|**2024-12-05**|**DEIM: DETR with Improved Matching for Fast Convergence**|Shihua Huang et.al.|[2412.04234v1](http://arxiv.org/abs/2412.04234v1)|null|
|**2024-12-05**|**Customize Segment Anything Model for Multi-Modal Semantic Segmentation with Mixture of LoRA Experts**|Chenyang Zhu et.al.|[2412.04220v1](http://arxiv.org/abs/2412.04220v1)|null|
|**2024-12-05**|**A Context-aware Framework for Translation-mediated Conversations**|José Pombal et.al.|[2412.04205v1](http://arxiv.org/abs/2412.04205v1)|null|
|**2024-12-05**|**AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in Dialectal Arabic**|Nathaniel R. Robinson et.al.|[2412.04193v1](http://arxiv.org/abs/2412.04193v1)|null|
|**2024-12-05**|**Directed Structural Adaptation to Overcome Statistical Conflicts and Enable Continual Learning**|Zeki Doruk Erden et.al.|[2412.04190v1](http://arxiv.org/abs/2412.04190v1)|null|
|**2024-12-05**|**Leveraging Large Language Models to Generate Course-specific Semantically Annotated Learning Objects**|Dominic Lohr et.al.|[2412.04185v1](http://arxiv.org/abs/2412.04185v1)|null|
|**2024-12-05**|**Bench-CoE: a Framework for Collaboration of Experts from Benchmark**|Yuanshuai Wang et.al.|[2412.04167v1](http://arxiv.org/abs/2412.04167v1)|null|
|**2024-12-05**|**If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs**|Muhammad Khalifa et.al.|[2412.04144v1](http://arxiv.org/abs/2412.04144v1)|null|
|**2024-12-05**|**Methodology for Online Estimation of Rheological Parameters in Polymer Melts Using Deep Learning and Microfluidics**|Juan Sandubete-López et.al.|[2412.04142v1](http://arxiv.org/abs/2412.04142v1)|null|
|**2024-12-05**|**Reducing Tool Hallucination via Reliability Alignment**|Hongshen Xu et.al.|[2412.04141v1](http://arxiv.org/abs/2412.04141v1)|null|
|**2024-12-05**|**Understanding Memorization in Generative Models via Sharpness in Probability Landscapes**|Dongjae Jeon et.al.|[2412.04140v1](http://arxiv.org/abs/2412.04140v1)|null|
|**2024-12-05**|**Monet: Mixture of Monosemantic Experts for Transformers**|Jungwoo Park et.al.|[2412.04139v1](http://arxiv.org/abs/2412.04139v1)|null|
|**2024-12-05**|**Text Change Detection in Multilingual Documents Using Image Comparison**|Doyoung Park et.al.|[2412.04137v1](http://arxiv.org/abs/2412.04137v1)|null|
|**2024-12-05**|**DeepFEA: Deep Learning for Prediction of Transient Finite Element Analysis Solutions**|Georgios Triantafyllou et.al.|[2412.04121v1](http://arxiv.org/abs/2412.04121v1)|null|
|**2024-12-05**|**GRAF: Graph Retrieval Augmented by Facts for Legal Question Answering**|Cristian-George Crăciun et.al.|[2412.04119v1](http://arxiv.org/abs/2412.04119v1)|null|
|**2024-12-05**|**Thermal and RGB Images Work Better Together in Wind Turbine Damage Detection**|Serhii Svystun et.al.|[2412.04114v1](http://arxiv.org/abs/2412.04114v1)|null|
|**2024-12-05**|**Enhancing Mathematical Reasoning in LLMs with Background Operators**|Jiajun Chen et.al.|[2412.04110v1](http://arxiv.org/abs/2412.04110v1)|null|
|**2024-12-05**|**Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models**|Yuhao Wang et.al.|[2412.04107v1](http://arxiv.org/abs/2412.04107v1)|null|
|**2024-12-05**|**Practical Considerations for Agentic LLM Systems**|Chris Sypherd et.al.|[2412.04093v1](http://arxiv.org/abs/2412.04093v1)|null|
|**2024-12-05**|**GEITje 7B Ultra: A Conversational Model for Dutch**|Bram Vanroy et.al.|[2412.04092v1](http://arxiv.org/abs/2412.04092v1)|null|
|**2024-12-05**|**BodyMetric: Evaluating the Realism of HumanBodies in Text-to-Image Generation**|Nefeli Andreou et.al.|[2412.04086v1](http://arxiv.org/abs/2412.04086v1)|null|
|**2024-12-05**|**Federated Learning in Mobile Networks: A Comprehensive Case Study on Traffic Forecasting**|Nikolaos Pavlidis et.al.|[2412.04081v1](http://arxiv.org/abs/2412.04081v1)|null|
|**2024-12-05**|**Does your model understand genes? A benchmark of gene properties for biological and text models**|Yoav Kan-Tor et.al.|[2412.04075v1](http://arxiv.org/abs/2412.04075v1)|null|
|**2024-12-05**|**ProtDAT: A Unified Framework for Protein Sequence Design from Any Protein Text Description**|Xiao-Yu Guo et.al.|[2412.04069v1](http://arxiv.org/abs/2412.04069v1)|null|
|**2024-12-05**|**Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning**|Amnon Bleich et.al.|[2412.04067v1](http://arxiv.org/abs/2412.04067v1)|null|
|**2024-12-05**|**Graph Neural Networks Need Cluster-Normalize-Activate Modules**|Arseny Skryagin et.al.|[2412.04064v1](http://arxiv.org/abs/2412.04064v1)|null|
|**2024-12-05**|**ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality**|Yefei He et.al.|[2412.04062v1](http://arxiv.org/abs/2412.04062v1)|null|
|**2024-12-05**|**Expanding Deep Learning-based Sensing Systems with Multi-Source Knowledge Transfer**|Gaole Dai et.al.|[2412.04060v1](http://arxiv.org/abs/2412.04060v1)|null|
|**2024-12-05**|**From Code to Play: Benchmarking Program Search for Games Using Large Language Models**|Manuel Eberhardinger et.al.|[2412.04057v1](http://arxiv.org/abs/2412.04057v1)|null|
|**2024-12-05**|**Hostility Detection in UK Politics: A Dataset on Online Abuse Targeting MPs**|Mugdha Pandya et.al.|[2412.04046v1](http://arxiv.org/abs/2412.04046v1)|null|
|**2024-12-05**|**INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations**|Yongming Zhu et.al.|[2412.04037v1](http://arxiv.org/abs/2412.04037v1)|null|
|**2024-12-05**|**SocialMind: LLM-based Proactive AR Social Assistive System with Human-like Perception for In-situ Live Interactions**|Bufang Yang et.al.|[2412.04036v1](http://arxiv.org/abs/2412.04036v1)|null|
|**2024-12-05**|**M$^{3}$D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level Information Extraction**|Jiang Liu et.al.|[2412.04026v1](http://arxiv.org/abs/2412.04026v1)|null|
|**2024-12-05**|**Exploring the Influence of Label Aggregation on Minority Voices: Implications for Dataset Bias and Model Training**|Mugdha Pandya et.al.|[2412.04025v1](http://arxiv.org/abs/2412.04025v1)|null|
|**2024-12-05**|**Deep-Unrolling Multidimensional Harmonic Retrieval Algorithms on Neuromorphic Hardware**|Vlad C. Andrei et.al.|[2412.04008v1](http://arxiv.org/abs/2412.04008v1)|null|
|**2024-12-05**|**Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement**|Lingfeng Ming et.al.|[2412.04003v1](http://arxiv.org/abs/2412.04003v1)|null|
|**2024-12-05**|**MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM**|Changcheng Li et.al.|[2412.03987v1](http://arxiv.org/abs/2412.03987v1)|null|
|**2024-12-05**|**Exploring Fully Convolutional Networks for the Segmentation of Hyperspectral Imaging Applied to Advanced Driver Assistance Systems**|Jon Gutiérrez-Zaballa et.al.|[2412.03982v1](http://arxiv.org/abs/2412.03982v1)|null|
|**2024-12-05**|**A Data-Driven Framework for Discovering Fractional Differential Equations in Complex Systems**|Xiangnan Yu et.al.|[2412.03970v1](http://arxiv.org/abs/2412.03970v1)|null|
|**2024-12-05**|**Demonstration Selection for In-Context Learning via Reinforcement Learning**|Xubin Wang et.al.|[2412.03966v1](http://arxiv.org/abs/2412.03966v1)|null|
|**2024-12-05**|**Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation**|Hao Yang et.al.|[2412.03944v1](http://arxiv.org/abs/2412.03944v1)|null|
|**2024-12-05**|**Enhancing and Accelerating Diffusion-Based Inverse Problem Solving through Measurements Optimization**|Tianyu Chen et.al.|[2412.03941v1](http://arxiv.org/abs/2412.03941v1)|null|
|**2024-12-05**|**InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models**|Yifan Lu et.al.|[2412.03934v1](http://arxiv.org/abs/2412.03934v1)|null|
|**2024-12-05**|**Exploring AI Text Generation, Retrieval-Augmented Generation, and Detection Technologies: a Comprehensive Overview**|Fnu Neha et.al.|[2412.03933v1](http://arxiv.org/abs/2412.03933v1)|null|
|**2024-12-05**|**MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model**|Yunhe Pang et.al.|[2412.03930v1](http://arxiv.org/abs/2412.03930v1)|null|
|**2024-12-05**|**MT3DNet: Multi-Task learning Network for 3D Surgical Scene Reconstruction**|Mithun Parab et.al.|[2412.03928v1](http://arxiv.org/abs/2412.03928v1)|null|
|**2024-12-05**|**A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios**|Xiachong Feng et.al.|[2412.03920v1](http://arxiv.org/abs/2412.03920v1)|null|
|**2024-12-05**|**Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair**|Qiong Feng et.al.|[2412.03905v1](http://arxiv.org/abs/2412.03905v1)|null|
|**2024-12-05**|**MISR: Measuring Instrumental Self-Reasoning in Frontier Models**|Kai Fronsdal et.al.|[2412.03904v1](http://arxiv.org/abs/2412.03904v1)|null|
|**2024-12-05**|**A Noise is Worth Diffusion Guidance**|Donghoon Ahn et.al.|[2412.03895v1](http://arxiv.org/abs/2412.03895v1)|null|
|**2024-12-05**|**Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models**|Swarnava Sinha Roy et.al.|[2412.03886v1](http://arxiv.org/abs/2412.03886v1)|null|
|**2024-12-05**|**A Unified Framework for Evaluating the Effectiveness and Enhancing the Transparency of Explainable AI Methods in Real-World Applications**|Md. Ariful Islam et.al.|[2412.03884v1](http://arxiv.org/abs/2412.03884v1)|null|
|**2024-12-05**|**Weak-to-Strong Generalization Through the Data-Centric Lens**|Changho Shin et.al.|[2412.03881v1](http://arxiv.org/abs/2412.03881v1)|null|
|**2024-12-05**|**AyutthayaAlpha: A Thai-Latin Script Transliteration Transformer**|Davor Lauc et.al.|[2412.03877v1](http://arxiv.org/abs/2412.03877v1)|null|
|**2024-12-05**|**Fine-Grained Sentiment Analysis of Electric Vehicle User Reviews: A Bidirectional LSTM Approach to Capturing Emotional Intensity in Chinese Text**|Shuhao Chen et.al.|[2412.03873v1](http://arxiv.org/abs/2412.03873v1)|null|
|**2024-12-05**|**How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs in E-Learning Environments?**|Patrick Ocheja et.al.|[2412.03856v1](http://arxiv.org/abs/2412.03856v1)|null|
|**2024-12-05**|**Automated LaTeX Code Generation from Handwritten Math Expressions Using Vision Transformer**|Jayaprakash Sundararaj et.al.|[2412.03853v1](http://arxiv.org/abs/2412.03853v1)|null|
|**2024-12-05**|**FedMetaMed: Federated Meta-Learning for Personalized Medication in Distributed Healthcare Systems**|Jiechao Gao et.al.|[2412.03851v1](http://arxiv.org/abs/2412.03851v1)|null|
|**2024-12-05**|**Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration**|Shiwen Ni et.al.|[2412.03847v1](http://arxiv.org/abs/2412.03847v1)|null|
|**2024-12-05**|**HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting**|Jingyu Lin et.al.|[2412.03844v1](http://arxiv.org/abs/2412.03844v1)|null|
|**2024-12-05**|**LL-ICM: Image Compression for Low-level Machine Vision via Large Vision-Language Model**|Yuan Xue et.al.|[2412.03841v1](http://arxiv.org/abs/2412.03841v1)|null|
|**2024-12-05**|**Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries**|Abul Ehtesham et.al.|[2412.03837v1](http://arxiv.org/abs/2412.03837v1)|null|
|**2024-12-05**|**Towards Data Governance of Frontier AI Models**|Jason Hausenloy et.al.|[2412.03824v1](http://arxiv.org/abs/2412.03824v1)|null|
|**2024-12-05**|**Beyond the Binary: Capturing Diverse Preferences With Reward Regularization**|Vishakh Padmakumar et.al.|[2412.03822v1](http://arxiv.org/abs/2412.03822v1)|null|
|**2024-12-05**|**Detecting Redundant Health Survey Questions Using Language-agnostic BERT Sentence Embedding (LaBSE)**|Sunghoon Kang et.al.|[2412.03817v1](http://arxiv.org/abs/2412.03817v1)|null|
|**2024-12-05**|**Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software Repository-Related Question Answering**|Samuel Abedu et.al.|[2412.03815v1](http://arxiv.org/abs/2412.03815v1)|null|

#### Abstracts
##### **PaintScene4D: Consistent 4D Scene Generation from Text Prompts**
2412.04471v1 by Vinayak Gupta, Yunze Man, Yu-Xiong Wang

Recent advances in diffusion models have revolutionized 2D and 3D content
creation, yet generating photorealistic dynamic 4D scenes remains a significant
challenge. Existing dynamic 4D generation methods typically rely on distilling
knowledge from pre-trained 3D generative models, often fine-tuned on synthetic
object datasets. Consequently, the resulting scenes tend to be object-centric
and lack photorealism. While text-to-video models can generate more realistic
scenes with motion, they often struggle with spatial understanding and provide
limited control over camera viewpoints during rendering. To address these
limitations, we present PaintScene4D, a novel text-to-4D scene generation
framework that departs from conventional multi-view generative models in favor
of a streamlined architecture that harnesses video generative models trained on
diverse real-world datasets. Our method first generates a reference video using
a video generation model, and then employs a strategic camera array selection
for rendering. We apply a progressive warping and inpainting technique to
ensure both spatial and temporal consistency across multiple viewpoints.
Finally, we optimize multi-view images using a dynamic renderer, enabling
flexible camera control based on user preferences. Adopting a training-free
architecture, our PaintScene4D efficiently produces realistic 4D scenes that
can be viewed from arbitrary trajectories. The code will be made publicly
available. Our project page is at https://paintscene4d.github.io/

摘要：最近在扩散模型方面的进展彻底改变了 2D 和 3D 内容创作，但生成逼真的动态 4D 场景仍然是一项重大挑战。现有的动态 4D 生成方法通常依赖于从预先训练的 3D 生成模型中提取知识，通常针对合成对象数据集进行微调。因此，生成的场景往往以对象为中心，缺乏逼真性。虽然文本到视频模型可以生成更逼真的场景并带有动态效果，但它们通常难以理解空间，并且在渲染期间对摄像机视点控制有限。为了解决这些限制，我们提出了 PaintScene4D，这是一种新颖的文本到 4D 场景生成框架，它摒弃了传统的多分辨率生成模型，转而采用一种简化的架构，该架构利用在各种真实世界数据集上训练的视频生成模型。我们的方法首先使用视频生成模型生成参考视频，然后采用策略性摄像机阵列选择进行渲染。我们应用渐进式扭曲和修复技术以确保多个视点之间的空间和时间一致性。最后，我们使用动态渲染器优化多分辨率图像，从而根据用户偏好实现灵活的摄像机控制。我们的 PaintScene4D 采用无训练架构，可以高效地生成逼真的 4D 场景，这些场景可以从任意轨迹中查看。代码将公开发布。我们的项目页面位于 https://paintscene4d.github.io/

##### **QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos**
2412.04469v1 by Sharath Girish, Tianye Li, Amrita Mazumdar, Abhinav Shrivastava, David Luebke, Shalini De Mello

Online free-viewpoint video (FVV) streaming is a challenging problem, which
is relatively under-explored. It requires incremental on-the-fly updates to a
volumetric representation, fast training and rendering to satisfy real-time
constraints and a small memory footprint for efficient transmission. If
achieved, it can enhance user experience by enabling novel applications, e.g.,
3D video conferencing and live volumetric video broadcast, among others. In
this work, we propose a novel framework for QUantized and Efficient ENcoding
(QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly
learns Gaussian attribute residuals between consecutive frames at each
time-step without imposing any structural constraints on them, allowing for
high quality reconstruction and generalizability. To efficiently store the
residuals, we further propose a quantization-sparsity framework, which contains
a learned latent-decoder for effectively quantizing attribute residuals other
than Gaussian positions and a learned gating module to sparsify position
residuals. We propose to use the Gaussian viewspace gradient difference vector
as a signal to separate the static and dynamic content of the scene. It acts as
a guide for effective sparsity learning and speeds up training. On diverse FVV
benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all
metrics. Notably, for several highly dynamic scenes, it reduces the model size
to just 0.7 MB per frame while training in under 5 sec and rendering at 350
FPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen

摘要：<paragraph>線上自由視角影片 (FVV) 串流是個具有挑戰性的問題，且相對來說較少被探討。它需要對體積表示進行增量即時更新、快速訓練和渲染以滿足即時限制，以及一個小型記憶體佔用空間以利於有效傳輸。如果達成，它可以透過啟用新穎應用程式（例如 3D 視訊會議和現場體積影片廣播等）來提升使用者體驗。在這項工作中，我們提出一個新穎的框架，稱為量化與有效編碼 (QUEEN)，以使用 3D 高斯潑濺 (3D-GS) 串流 FVV。QUEEN 直接學習每個時間步長中連續幀之間的高斯屬性殘差，而不會對它們施加任何結構限制，從而實現高品質重建和泛化性。為了有效儲存殘差，我們進一步提出一個量化稀疏性框架，其中包含一個學習的潛在解碼器，用於有效量化高斯位置以外的屬性殘差，以及一個學習的閘控模組，用於稀疏化位置殘差。我們建議使用高斯視角空間梯度差向量作為一個訊號，以分離場景中的靜態和動態內容。它作為一個有效稀疏性學習指南，並加快訓練速度。在不同的 FVV 基準上，QUEEN 在所有指標上都優於最先進的線上 FVV 方法。值得注意的是，對於幾個高度動態的場景，它將模型大小減小到每幀僅 0.7 MB，同時在不到 5 秒的時間內完成訓練，並以 350 FPS 進行渲染。專案網站位於 https://research.nvidia.com/labs/amri/projects/queen</paragraph>

##### **VisionZip: Longer is Better but Not Necessary in Vision Language Models**
2412.04467v1 by Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, Jiaya Jia

Recent advancements in vision-language models have enhanced performance by
increasing the length of visual tokens, making them much longer than text
tokens and significantly raising computational costs. However, we observe that
the visual tokens generated by popular vision encoders, such as CLIP and
SigLIP, contain significant redundancy. To address this, we introduce
VisionZip, a simple yet effective method that selects a set of informative
tokens for input to the language model, reducing visual token redundancy and
improving efficiency while maintaining model performance. The proposed
VisionZip can be widely applied to image and video understanding tasks and is
well-suited for multi-turn dialogues in real-world scenarios, where previous
methods tend to underperform. Experimental results show that VisionZip
outperforms the previous state-of-the-art method by at least 5% performance
gains across nearly all settings. Moreover, our method significantly enhances
model inference speed, improving the prefilling time by 8x and enabling the
LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while
achieving better results. Furthermore, we analyze the causes of this redundancy
and encourage the community to focus on extracting better visual features
rather than merely increasing token length. Our code is available at
https://github.com/dvlab-research/VisionZip .

摘要：最近在視覺語言模型上的進展，透過增加視覺標記的長度，讓它們遠比文字標記長，大幅提升了效能，但也顯著提高了運算成本。不過，我們觀察到由熱門視覺編碼器產生的視覺標記，例如 CLIP 和 SigLIP，包含大量的冗餘。為了解決這個問題，我們引進 VisionZip，這是一種簡單但有效的方法，它會選取一組具資訊性的標記作為語言模型的輸入，減少視覺標記的冗餘並提升效率，同時維持模型效能。建議的 VisionZip 可廣泛應用於影像和影片理解任務，也很適合在現實世界的場景中進行多輪對話，而先前的做法往往表現不佳。實驗結果顯示，VisionZip 在幾乎所有設定中都優於先前的最新技術，效能提升至少 5%。此外，我們的做法大幅提升了模型推論速度，將預填時間縮短 8 倍，並讓 LLaVA-Next 13B 模型的推論速度比 LLaVA-Next 7B 模型更快，同時獲得更好的結果。此外，我們分析了這種冗餘的原因，並鼓勵社群專注於萃取更好的視覺特徵，而不是只增加標記長度。我們的程式碼可在 https://github.com/dvlab-research/VisionZip 取得。

##### **Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection**
2412.04455v1 by Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, He Wang

Automatic detection and prevention of open-set failures are crucial in
closed-loop robotic systems. Recent studies often struggle to simultaneously
identify unexpected failures reactively after they occur and prevent
foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a
novel paradigm leveraging the vision-language model (VLM) for both open-set
reactive and proactive failure detection. The core of our method is to
formulate both tasks as a unified set of spatio-temporal constraint
satisfaction problems and use VLM-generated code to evaluate them for real-time
monitoring. To enhance the accuracy and efficiency of monitoring, we further
introduce constraint elements that abstract constraint-related entities or
their parts into compact geometric elements. This approach offers greater
generality, simplifies tracking, and facilitates constraint-aware visual
programming by leveraging these elements as visual prompts. Experiments show
that CaM achieves a 28.7% higher success rate and reduces execution time by
31.8% under severe disturbances compared to baselines across three simulators
and a real-world setting. Moreover, CaM can be integrated with open-loop
control policies to form closed-loop systems, enabling long-horizon tasks in
cluttered scenes with dynamic environments.

摘要：在閉迴路機器人系統中，自動偵測和預防開放式故障至關重要。最近的研究通常難以在故障發生後同時主動識別意外故障並預防可預見的故障。為此，我們提出 Code-as-Monitor (CaM)，一種新的範例，利用視覺語言模型 (VLM) 進行開放式反應式和主動式故障偵測。我們方法的核心是將這兩個任務制定為一組統一的時空約束滿足問題，並使用 VLM 生成的程式碼對它們進行評估以進行即時監控。為了提高監控的準確性和效率，我們進一步引入了約束元素，將與約束相關的實體或其部分抽象化為緊湊的幾何元素。這種方法提供了更大的通用性，簡化了追蹤，並透過將這些元素作為視覺提示，促进了約束感知視覺程式設計。實驗表明，與三個模擬器和一個真實世界設定中的基準相比，CaM 在嚴重干擾下實現了高出 28.7% 的成功率，並將執行時間減少了 31.8%。此外，CaM 可以與開迴路控制策略整合以形成閉迴路系統，從而在具有動態環境的混亂場景中執行長期任務。

##### **Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction**
2412.04454v1 by Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, Caiming Xiong

Graphical User Interfaces (GUIs) are critical to human-computer interaction,
yet automating GUI tasks remains challenging due to the complexity and
variability of visual environments. Existing approaches often rely on textual
representations of GUIs, which introduce limitations in generalization,
efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure
vision-based framework for autonomous GUI agents that operates across various
platforms. Our approach leverages image-based observations, and grounding
instructions in natural language to visual elements, and employs a consistent
action space to ensure cross-platform generalization. To address the
limitations of previous work, we integrate explicit planning and reasoning
within the model, enhancing its ability to autonomously navigate and interact
with complex digital environments. We construct a large-scale dataset of GUI
agent trajectories, incorporating multimodal reasoning and grounding, and
employ a two-stage training pipeline that first focuses on general GUI
grounding, followed by planning and reasoning. Through comprehensive
experiments, we demonstrate that Aguvis surpasses previous state-of-the-art
methods in both offline and real-world online scenarios, achieving, to our
knowledge, the first fully autonomous pure vision GUI agent capable of
performing tasks independently without collaboration with external
closed-source models. We open-sourced all datasets, models, and training
recipes to facilitate future research at https://aguvis-project.github.io/.

摘要：圖形使用者介面 (GUI) 對人機互動至關重要，
但由於視覺環境的複雜性和變異性，自動化 GUI 任務仍然具有挑戰性。現有方法通常依賴於 GUI 的文字表示，這會在概括、效率和可擴充性方面造成限制。在本文中，我們介紹 Aguvis，一個統一的純視覺化框架，適用於跨各種平台的自主 GUI 代理。我們的做法利用基於影像的觀察，並將自然語言的基礎指令轉換為視覺元素，並採用一致的動作空間來確保跨平台概括。為了解決先前工作的限制，我們在模型中整合了明確的規劃和推理，增強了其自主導航和與複雜數位環境互動的能力。我們建立了一個 GUI 代理軌跡的大規模資料集，結合了多模態推理和基礎，並採用一個兩階段的訓練管道，首先專注於一般的 GUI 基礎，然後進行規劃和推理。透過全面的實驗，我們證明 Aguvis 在離線和真實世界的線上情境中都超越了先前的最先進方法，據我們所知，這是第一個完全自主的純視覺 GUI 代理，能夠獨立執行任務，而無需與外部閉源模型合作。我們開放了所有資料集、模型和訓練範例，以促進未來在 https://aguvis-project.github.io/ 上的研究。

##### **p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay**
2412.04449v1 by Jun Zhang, Desen Meng, Ji Qi, Zhenpeng Huang, Tao Wu, Limin Wang

Despite the remarkable performance of multimodal large language models
(MLLMs) across diverse tasks, the substantial training and inference costs
impede their advancement. The majority of computation stems from the
overwhelming volume of vision tokens processed by the transformer decoder. In
this paper, we propose to build efficient MLLMs by leveraging the
Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects
essential vision tokens to process while skipping redundant ones. However,
integrating MoD into MLLMs is non-trivial. To address the challenges of
training and inference stability as well as limited training data, we adapt the
MoD module with two novel designs: tanh-gated weight normalization (TanhNorm)
and symmetric token reweighting (STRing). Moreover, we observe that vision
tokens exhibit higher redundancy in deeper layer and thus design a progressive
ratio decay (PRD) strategy, which gradually reduces the token retention ratio
layer by layer, employing a shifted cosine schedule. This crucial design fully
unleashes the potential of MoD, significantly boosting the efficiency and
performance of our models. To validate the effectiveness of our approach, we
conduct extensive experiments with two baseline models across 14 benchmarks.
Our model, p-MoD, matches or even surpasses the performance of the baseline
models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and
77.7% GPU hours during training.

摘要：儘管多模態大型語言模型 (MLLM) 在各種任務中表現出色，但龐大的訓練和推論成本阻礙了它們的進步。大部分運算源自Transformer解碼器處理的龐大視覺符號量。在本文中，我們提出透過利用深度混合 (MoD) 機制來建立高效的 MLLM，其中每個Transformer解碼器層會選擇必要的視覺符號來處理，同時略過多餘的符號。然而，將 MoD 整合到 MLLM 並不容易。為了應對訓練和推論穩定性以及訓練資料有限的挑戰，我們透過兩種創新設計來調整 MoD 模組：tanh 閘控權重正規化 (TanhNorm) 和對稱符號重新加權 (STRing)。此外，我們觀察到視覺符號在較深的層中表現出更高的冗餘，因此設計了一個漸進比率衰減 (PRD) 策略，它會逐層逐漸降低符號保留率，並採用偏移餘弦時程。這個關鍵設計完全釋放了 MoD 的潛力，顯著提升了我們模型的效率和效能。為了驗證我們方法的有效性，我們在 14 個基準上對兩個基準模型進行了廣泛的實驗。我們的模型 p-MoD 在推論期間僅有 55.6% 的 TFLOP 和 53.8% 的 KV 快取儲存，以及訓練期間 77.7% 的 GPU 小時，就能夠達到或甚至超越基準模型的效能。

##### **EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios**
2412.04447v1 by Lu Qiu, Yuying Ge, Yi Chen, Yixiao Ge, Ying Shan, Xihui Liu

The advent of Multimodal Large Language Models, leveraging the power of Large
Language Models, has recently demonstrated superior multimodal understanding
and reasoning abilities, heralding a new era for artificial general
intelligence. However, achieving AGI necessitates more than just comprehension
and reasoning. A crucial capability required is effective planning in diverse
scenarios, which involves making reasonable decisions based on complex
environments to solve real-world problems. Despite its importance, the planning
abilities of current MLLMs in varied scenarios remain underexplored. In this
paper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmark
designed to assess the planning capabilities of MLLMs across a wide range of
real-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4
major domains and 24 detailed scenarios, closely aligned with human daily life.
EgoPlan-Bench2 is constructed through a semi-automatic process utilizing
egocentric videos, complemented by manual verification. Grounded in a
first-person perspective, it mirrors the way humans approach problem-solving in
everyday life. We evaluate 21 competitive MLLMs and provide an in-depth
analysis of their limitations, revealing that they face significant challenges
in real-world planning. To further improve the planning proficiency of current
MLLMs, we propose a training-free approach using multimodal Chain-of-Thought
(CoT) prompting through investigating the effectiveness of various multimodal
prompts in complex planning. Our approach enhances the performance of GPT-4V by
10.24 on EgoPlan-Bench2 without additional training. Our work not only sheds
light on the current limitations of MLLMs in planning, but also provides
insights for future enhancements in this critical area. We have made data and
code available at https://qiulu66.github.io/egoplanbench2/.

摘要：多模态大型语言模型的出现，利用了大型语言模型的力量，最近展示了卓越的多模态理解和推理能力，预示着人工智能新时代的到来。然而，实现 AGI 不仅仅需要理解和推理。一个至关重要的能力是有效规划各种场景，这涉及在复杂环境中做出合理的决策来解决现实世界中的问题。尽管其重要性，当前 MLLM 在各种场景中的规划能力仍然未得到充分探索。在本文中，我们介绍了 EgoPlan-Bench2，这是一个严格且全面的基准，旨在评估 MLLM 在广泛的现实世界场景中的规划能力。EgoPlan-Bench2 涵盖了跨越 4 个主要领域和 24 个详细场景的日常任务，与人类日常生活紧密相关。EgoPlan-Bench2 是通过利用自我中心视频的半自动过程构建的，并辅以人工验证。基于第一人称视角，它反映了人类在日常生活中解决问题的方式。我们评估了 21 个有竞争力的 MLLM，并深入分析了它们的局限性，揭示了它们在现实世界规划中面临着重大挑战。为了进一步提高当前 MLLM 的规划能力，我们提出了一种使用多模态思想链 (CoT) 提示的无训练方法，通过调查各种多模态提示在复杂规划中的有效性。我们的方法通过多模态 Chain-of-Thought (CoT) 提示将 GPT-4V 的性能提高了 10.24，而无需额外训练。我们的工作不仅揭示了 MLLM 在规划方面的当前局限性，还为这一关键领域的未来增强提供了见解。我们已在 https://qiulu66.github.io/egoplanbench2/ 提供数据和代码。

##### **Moto: Latent Motion Token as the Bridging Language for Robot Manipulation**
2412.04445v1 by Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, Xihui Liu

Recent developments in Large Language Models pre-trained on extensive corpora
have shown significant success in various natural language processing tasks
with minimal fine-tuning. This success offers new promise for robotics, which
has long been constrained by the high cost of action-labeled data. We ask:
given the abundant video data containing interaction-related knowledge
available as a rich "corpus", can a similar generative pre-training approach be
effectively applied to enhance robot learning? The key challenge is to identify
an effective representation for autoregressive pre-training that benefits robot
manipulation tasks. Inspired by the way humans learn new skills through
observing dynamic environments, we propose that effective robotic learning
should emphasize motion-related knowledge, which is closely tied to low-level
actions and is hardware-agnostic, facilitating the transfer of learned motions
to actual robot actions. To this end, we introduce Moto, which converts video
content into latent Motion Token sequences by a Latent Motion Tokenizer,
learning a bridging "language" of motion from videos in an unsupervised manner.
We pre-train Moto-GPT through motion token autoregression, enabling it to
capture diverse visual motion knowledge. After pre-training, Moto-GPT
demonstrates the promising ability to produce semantically interpretable motion
tokens, predict plausible motion trajectories, and assess trajectory
rationality through output likelihood. To transfer learned motion priors to
real robot actions, we implement a co-fine-tuning strategy that seamlessly
bridges latent motion token prediction and real robot control. Extensive
experiments show that the fine-tuned Moto-GPT exhibits superior robustness and
efficiency on robot manipulation benchmarks, underscoring its effectiveness in
transferring knowledge from video data to downstream visual manipulation tasks.

摘要：<paragraph>大型语言模型在大量语料库上进行预训练的最新进展
在各种自然语言处理任务中表现出显著的成功，
只需进行最少的微调。这一成功为机器人技术带来了新的希望，
机器人技术长期以来一直受到动作标记数据的高成本的制约。我们问：
鉴于包含交互相关知识的丰富视频数据
可用作丰富的“语料库”，类似的生成式预训练方法可以
有效地应用于增强机器人学习吗？关键的挑战是识别
一个有效的表示，用于自回归预训练，该表示有利于机器人
操作任务。受人类通过
观察动态环境学习新技能的方式的启发，我们提出有效的机器人学习
应该强调与运动相关的知识，这与低级
动作密切相关，并且与硬件无关，从而促进学习动作的转移
到实际的机器人动作。为此，我们引入了 Moto，它将视频
内容通过潜在运动标记化器转换为潜在运动标记序列，
以无监督的方式从视频中学习运动的桥接“语言”。
我们通过运动标记自回归对 Moto-GPT 进行预训练，使其能够
捕捉不同的视觉运动知识。预训练后，Moto-GPT
展示了生成语义可解释运动标记、预测合理的运动轨迹，以及评估轨迹
通过输出可能性进行合理性。为了将学习到的运动先验转移到
真正的机器人动作，我们实施了协同微调策略，该策略无缝
桥接潜在运动标记预测和真正的机器人控制。广泛的
实验表明，微调后的 Moto-GPT 在机器人操作基准测试中表现出卓越的鲁棒性和
效率，强调了其从视频数据到下游视觉操纵任务转移知识的有效性。</paragraph>

##### **CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing**
2412.04425v1 by Yen-Ju Lu, Jing Liu, Thomas Thebaud, Laureano Moro-Velazquez, Ariya Rastrow, Najim Dehak, Jesus Villalba

We introduce Condition-Aware Self-Supervised Learning Representation
(CA-SSLR), a generalist conditioning model broadly applicable to various
speech-processing tasks. Compared to standard fine-tuning methods that optimize
for downstream models, CA-SSLR integrates language and speaker embeddings from
earlier layers, making the SSL model aware of the current language and speaker
context. This approach reduces the reliance on input audio features while
preserving the integrity of the base SSLR. CA-SSLR improves the model's
capabilities and demonstrates its generality on unseen tasks with minimal
task-specific tuning. Our method employs linear modulation to dynamically
adjust internal representations, enabling fine-grained adaptability without
significantly altering the original model behavior. Experiments show that
CA-SSLR reduces the number of trainable parameters, mitigates overfitting, and
excels in under-resourced and unseen tasks. Specifically, CA-SSLR achieves a
10% relative reduction in LID errors, a 37% improvement in ASR CER on the
ML-SUPERB benchmark, and a 27% decrease in SV EER on VoxCeleb-1, demonstrating
its effectiveness.

摘要：我們引入了條件感知自我監督學習表示 (CA-SSLR)，這是一個通才條件模型，廣泛適用於各種語音處理任務。與針對下游模型進行優化的標準微調方法相比，CA-SSLR 整合了來自早期層的語言和說話者嵌入，使 SSL 模型感知當前的語言和說話者背景。此方法減少了對輸入音訊特徵的依賴，同時保留了基礎 SSLR 的完整性。CA-SSLR 改善了模型的能力，並在未見任務上展示了其通用性，且任務特定調整最少。我們的模型採用線性調製來動態調整內部表示，實現細粒度的適應性，而不會顯著改變原始模型行為。實驗表明，CA-SSLR 減少了可訓練參數的數量，減輕了過度擬合，並在資源不足和未見任務中表現出色。具體來說，CA-SSLR 在 LID 錯誤中實現了 10% 的相對減少，在 ML-SUPERB 基準上 ASR CER 提高了 37%，在 VoxCeleb-1 上 SV EER 減少了 27%，證明了其有效性。

##### **Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion**
2412.04424v1 by Jiuhai Chen, Jianwei Yang, Haiping Wu, Dianqi Li, Jianfeng Gao, Tianyi Zhou, Bin Xiao

We present Florence-VL, a new family of multimodal large language models
(MLLMs) with enriched visual representations produced by Florence-2, a
generative vision foundation model. Unlike the widely used CLIP-style vision
transformer trained by contrastive learning, Florence-2 can capture different
levels and aspects of visual features, which are more versatile to be adapted
to diverse downstream tasks. We propose a novel feature-fusion architecture and
an innovative training recipe that effectively integrates Florence-2's visual
features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we
propose "depth-breath fusion (DBFusion)" to fuse the visual features extracted
from different depths and under multiple prompts. Our model training is
composed of end-to-end pretraining of the whole model followed by finetuning of
the projection layer and the LLM, on a carefully designed recipe of diverse
open-source datasets that include high-quality image captions and
instruction-tuning pairs. Our quantitative analysis and visualization of
Florence-VL's visual features show its advantages over popular vision encoders
on vision-language alignment, where the enriched depth and breath play
important roles. Florence-VL achieves significant improvements over existing
state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks
covering general VQA, perception, hallucination, OCR, Chart,
knowledge-intensive understanding, etc. To facilitate future research, our
models and the complete training recipe are open-sourced.
https://github.com/JiuhaiChen/Florence-VL

摘要：<paragraph>我們提出了 Florence-VL，一個新的多模態大型語言模型 (MLLM) 家族，其豐富的視覺表徵是由 Florence-2，一個生成式視覺基礎模型所產生。與廣泛使用的 CLIP 風格視覺Transformer不同，由對比學習訓練而得，Florence-2 能擷取視覺特徵的不同層級和面向，更能適應多樣的下游任務。我們提出一個新穎的特徵融合架構和一個創新的訓練配方，有效地將 Florence-2 的視覺特徵整合到預訓練的 LLM，例如 Phi 3.5 和 LLama 3。特別是，我們提出「深度廣度融合 (DBFusion)」來融合從不同深度和多個提示中萃取的視覺特徵。我們的模型訓練由整個模型的端到端預訓練組成，接著是投影層和 LLM 的微調，使用精心設計的各種開源資料集配方，其中包括高品質的圖片標題和指令調整對。我們對 Florence-VL 視覺特徵的定量分析和視覺化顯示出它優於流行的視覺編碼器在視覺語言對齊上的優勢，其中豐富的深度和廣度扮演了重要的角色。Florence-VL 在現有的最先進 MLLM 上，在涵蓋一般 VQA、感知、幻覺、OCR、圖表、知識密集理解等的各種多模態和以視覺為中心的基準上，都取得了顯著的進步。為了促進未來的研究，我們的模型和完整的訓練配方都是開源的。
https://github.com/JiuhaiChen/Florence-VL</paragraph>

##### **FedDUAL: A Dual-Strategy with Adaptive Loss and Dynamic Aggregation for Mitigating Data Heterogeneity in Federated Learning**
2412.04416v1 by Pranab Sahoo, Ashutosh Tripathi, Sriparna Saha, Samrat Mondal

Federated Learning (FL) marks a transformative approach to distributed model
training by combining locally optimized models from various clients into a
unified global model. While FL preserves data privacy by eliminating
centralized storage, it encounters significant challenges such as performance
degradation, slower convergence, and reduced robustness of the global model due
to the heterogeneity in client data distributions. Among the various forms of
data heterogeneity, label skew emerges as a particularly formidable and
prevalent issue, especially in domains such as image classification. To address
these challenges, we begin with comprehensive experiments to pinpoint the
underlying issues in the FL training process. Based on our findings, we then
introduce an innovative dual-strategy approach designed to effectively resolve
these issues. First, we introduce an adaptive loss function for client-side
training, meticulously crafted to preserve previously acquired knowledge while
maintaining an optimal equilibrium between local optimization and global model
coherence. Secondly, we develop a dynamic aggregation strategy for aggregating
client models at the server. This approach adapts to each client's unique
learning patterns, effectively addressing the challenges of diverse data across
the network. Our comprehensive evaluation, conducted across three diverse
real-world datasets, coupled with theoretical convergence guarantees,
demonstrates the superior efficacy of our method compared to several
established state-of-the-art approaches.

摘要：聯邦學習 (FL) 標誌著一種變革性的分布式模型訓練方法，它將來自不同用戶端的局部最佳化模型組合成一個統一的全局模型。雖然 FL 透過消除集中式儲存來保護資料隱私，但它會遇到一些重大挑戰，例如效能下降、收斂速度較慢，以及由於用戶端資料分佈的異質性而導致全局模型的穩健性降低。在各種形式的資料異質性中，標籤偏差成為一個特別難以應付且普遍存在的問題，特別是在影像分類等領域。為了應對這些挑戰，我們從全面的實驗開始，以找出 FL 訓練過程中潛在的問題。根據我們的發現，我們隨後提出了一種創新的雙重策略方法，旨在有效解決這些問題。首先，我們為用戶端訓練引進一個自適應損失函數，經過精心設計，可在維持局部最佳化與全局模型一致性之間的最佳平衡，同時保留先前獲得的知識。其次，我們開發了一種動態聚合策略，用於在伺服器上聚合用戶端模型。這種方法會適應每個用戶端獨特的學習模式，有效應對網路中資料多樣化的挑戰。我們的全面評估是在三個不同的真實世界資料集上進行的，再加上理論收斂保證，證明了我們的方法與幾種已建立的最新方法相比具有更優異的功效。

##### **Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation**
2412.04415v1 by Xuying Li, Zhuo Li, Yuji Kosuga, Yasuhiro Yoshida, Victor Bian

AI agents, powered by large language models (LLMs), have transformed
human-computer interactions by enabling seamless, natural, and context-aware
communication. While these advancements offer immense utility, they also
inherit and amplify inherent safety risks such as bias, fairness,
hallucinations, privacy breaches, and a lack of transparency. This paper
investigates a critical vulnerability: adversarial attacks targeting the LLM
core within AI agents. Specifically, we test the hypothesis that a deceptively
simple adversarial prefix, such as \textit{Ignore the document}, can compel
LLMs to produce dangerous or unintended outputs by bypassing their contextual
safeguards. Through experimentation, we demonstrate a high attack success rate
(ASR), revealing the fragility of existing LLM defenses. These findings
emphasize the urgent need for robust, multi-layered security measures tailored
to mitigate vulnerabilities at the LLM level and within broader agent-based
architectures.

摘要：由大型語言模型 (LLM) 驅動的 AI 代理，透過啟用無縫、自然且具脈絡感知的溝通，已經轉變了人機互動。雖然這些進展提供了極大的效用，但它們也繼承並擴大了固有的安全風險，例如偏見、公平性、幻覺、隱私洩露和缺乏透明度。本文探討了一個關鍵的漏洞：針對 AI 代理內部 LLM 核心的對抗性攻擊。具體來說，我們測試了一個假設，即一個具有欺騙性的簡單對抗性前綴，例如「忽略文件」，可以迫使 LLM 繞過其上下文安全防護，產生危險或意外的輸出。透過實驗，我們證明了很高的攻擊成功率 (ASR)，揭示了現有 LLM 防禦的脆弱性。這些發現強調了迫切需要針對 LLM 層級和更廣泛的基於代理的架構量身打造強健的多層安全措施，以減輕漏洞。

##### **Establishing Task Scaling Laws via Compute-Efficient Model Ladders**
2412.04403v1 by Akshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, Hannaneh Hajishirzi

We develop task scaling laws and model ladders to predict the individual task
performance of pretrained language models (LMs) in the overtrained setting.
Standard power laws for language modeling loss cannot accurately model task
performance. Therefore, we leverage a two-step prediction approach: first use
model and data size to predict a task-specific loss, and then use this task
loss to predict task performance. We train a set of small-scale "ladder"
models, collect data points to fit the parameterized functions of the two
prediction steps, and make predictions for two target models: a 7B model
trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder
models only costs 1% of the compute used for the target models. On four
multiple-choice tasks written in ranked classification format, we can predict
the accuracy of both target models within 2 points of absolute error. We have
higher prediction error on four other tasks (average absolute error 6.9) and
find that these are often tasks with higher variance in task metrics. We also
find that using less compute to train fewer ladder models tends to deteriorate
predictions. Finally, we empirically show that our design choices and the
two-step approach lead to superior performance in establishing scaling laws.

摘要：<paragraph>我們開發任務擴充法則和模型階梯，以預測預訓練語言模型 (LM) 在過度訓練設定中的個別任務執行效能。
語言模型損失的標準冪次法則無法精準模擬任務效能。因此，我們利用兩步驟預測方法：首先使用模型和資料大小預測特定任務的損失，然後使用此任務損失預測任務效能。我們訓練一組小規模「階梯」模型，收集資料點以符合兩個預測步驟的參數化函數，並對兩個目標模型進行預測：一個訓練至 4T 令牌的 7B 模型和一個訓練至 5T 令牌的 13B 模型。訓練階梯模型僅耗費目標模型運算的 1%。在四個以排名分類格式撰寫的多重選擇任務中，我們可以預測兩個目標模型的準確度，絕對誤差在 2 點以內。我們在其他四個任務上具有較高的預測誤差（平均絕對誤差 6.9），並發現這些任務通常是任務指標變異較高的任務。我們還發現，使用較少的運算訓練較少的階梯模型往往會導致預測惡化。最後，我們透過經驗證明，我們的設計選擇和兩步驟方法有助於在建立擴充法則時獲得優異的效能。</paragraph>

##### **Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction**
2412.04384v1 by Yuanhui Huang, Amonnut Thammatadatrakoon, Wenzhao Zheng, Yunpeng Zhang, Dalong Du, Jiwen Lu

3D semantic occupancy prediction is an important task for robust
vision-centric autonomous driving, which predicts fine-grained geometry and
semantics of the surrounding scene. Most existing methods leverage dense
grid-based scene representations, overlooking the spatial sparsity of the
driving scenes. Although 3D semantic Gaussian serves as an object-centric
sparse alternative, most of the Gaussians still describe the empty region with
low efficiency. To address this, we propose a probabilistic Gaussian
superposition model which interprets each Gaussian as a probability
distribution of its neighborhood being occupied and conforms to probabilistic
multiplication to derive the overall geometry. Furthermore, we adopt the exact
Gaussian mixture model for semantics calculation to avoid unnecessary
overlapping of Gaussians. To effectively initialize Gaussians in non-empty
region, we design a distribution-based initialization module which learns the
pixel-aligned occupancy distribution instead of the depth of surfaces. We
conduct extensive experiments on nuScenes and KITTI-360 datasets and our
GaussianFormer-2 achieves state-of-the-art performance with high efficiency.
Code: https://github.com/huang-yh/GaussianFormer.

摘要：3D 語意佔用預測是健全視覺中心自動駕駛的一項重要任務，它預測了周圍場景的精細幾何形狀和語意。現有的大多數方法利用密集的基於網格的場景表示，忽略了駕駛場景的空間稀疏性。雖然 3D 語意高斯函數作為以物件為中心的稀疏替代方案，但大多數高斯函數仍以低效率描述空區域。為了解決此問題，我們提出了一個機率高斯疊加模型，該模型將每個高斯函數解釋為其鄰域被佔用的機率分佈，並符合機率乘法以推導整體幾何形狀。此外，我們採用精確高斯混合模型進行語意計算，以避免高斯函數不必要的重疊。為了有效地初始化非空區域中的高斯函數，我們設計了一個基於分佈的初始化模組，該模組學習像素對齊的佔用分佈，而不是表面的深度。我們對 nuScenes 和 KITTI-360 資料集進行了廣泛的實驗，我們的 GaussianFormer-2 以高效率實現了最先進的效能。程式碼：https://github.com/huang-yh/GaussianFormer。

##### **Discriminative Fine-tuning of LVLMs**
2412.04378v1 by Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Georgios Tzimiropoulos, Brais Martinez

Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the
de facto approach for discriminative vision-language representation learning.
However, these models have limited language understanding, often exhibiting a
"bag of words" behavior. At the same time, Large Vision-Language Models
(LVLMs), which combine vision encoders with LLMs, have been shown capable of
detailed vision-language reasoning, yet their autoregressive nature renders
them less suitable for discriminative tasks.
  In this work, we propose to combine "the best of both worlds": a new training
approach for discriminative fine-tuning of LVLMs that results in strong
discriminative and compositional capabilities. Essentially, our approach
converts a generative LVLM into a discriminative one, unlocking its capability
for powerful image-text discrimination combined with enhanced language
understanding.
  Our contributions include: (1) A carefully designed training/optimization
framework that utilizes image-text pairs of variable length and granularity for
training the model with both contrastive and next-token prediction losses. This
is accompanied by ablation studies that justify the necessity of our
framework's components. (2) A parameter-efficient adaptation method using a
combination of soft prompting and LoRA adapters. (3) Significant improvements
over state-of-the-art CLIP-like models of similar size, including standard
image-text retrieval benchmarks and notable gains in compositionality.

摘要：對比訓練的視覺語言模型 (VLM)，例如 CLIP，已成為判別式視覺語言表示學習的事實上方法。
然而，這些模型的語言理解力有限，經常表現出「一籃子詞彙」的行為。同時，結合視覺編碼器與 LLM 的大型視覺語言模型 (LVLM) 已被證明有能力進行詳細的視覺語言推理，但它們的自動迴歸性質讓它們不太適合判別式任務。
在這項工作中，我們提議結合「兩全其美」：一種新的訓練方法，用於 LVLMs 的判別式微調，可產生強大的判別式和組合能力。基本上，我們的方法將生成式 LVLM 轉換為判別式 LVLM，發揮其功能，以強大的影像文字判別結合增強的語言理解力。
我們的貢獻包括：(1) 精心設計的訓練/最佳化架構，利用變長且粒度化的影像文字對，使用對比和下一個符號預測損失訓練模型。這附帶了消融研究，證明了我們架構組成的必要性。(2) 使用軟提示和 LoRA 適配器的參數有效適應方法。(3) 與類似大小的最新 CLIP 類似模型相比有顯著的改進，包括標準影像文字檢索基準，以及在組合性方面有顯著的進步。

##### **Machine Theory of Mind for Autonomous Cyber-Defence**
2412.04367v1 by Luke Swaby, Matthew Stewart, Daniel Harrold, Chris Willis, Gregory Palmer

Intelligent autonomous agents hold much potential for the domain of
cyber-security. However, due to many state-of-the-art approaches relying on
uninterpretable black-box models, there is growing demand for methods that
offer stakeholders clear and actionable insights into their latent beliefs and
motivations. To address this, we evaluate Theory of Mind (ToM) approaches for
Autonomous Cyber Operations. Upon learning a robust prior, ToM models can
predict an agent's goals, behaviours, and contextual beliefs given only a
handful of past behaviour observations. In this paper, we introduce a novel
Graph Neural Network (GNN)-based ToM architecture tailored for cyber-defence,
Graph-In, Graph-Out (GIGO)-ToM, which can accurately predict both the targets
and attack trajectories of adversarial cyber agents over arbitrary computer
network topologies. To evaluate the latter, we propose a novel extension of the
Wasserstein distance for measuring the similarity of graph-based probability
distributions. Whereas the standard Wasserstein distance lacks a fixed
reference scale, we introduce a graph-theoretic normalization factor that
enables a standardized comparison between networks of different sizes. We
furnish this metric, which we term the Network Transport Distance (NTD), with a
weighting function that emphasizes predictions according to custom node
features, allowing network operators to explore arbitrary strategic
considerations. Benchmarked against a Graph-In, Dense-Out (GIDO)-ToM
architecture in an abstract cyber-defence environment, our empirical
evaluations show that GIGO-ToM can accurately predict the goals and behaviours
of various unseen cyber-attacking agents across a range of network topologies,
as well as learn embeddings that can effectively characterize their policies.

摘要：<paragraph>智能自主代理在网络安全领域拥有巨大潜力。然而，由于许多最先进的方法依赖于不可解释的黑盒模型，因此对能够为利益相关者提供对其潜在信念和动机清晰且可操作的见解的方法的需求越来越大。为了解决这个问题，我们评估了自主网络操作的心智理论 (ToM) 方法。在学习到可靠的先验后，ToM 模型仅根据少数过去的的行为观察，就能预测代理的目标、行为和背景信念。在本文中，我们引入了一种新颖的基于图神经网络 (GNN) 的 ToM 架构，专门用于网络防御，即 Graph-In, Graph-Out (GIGO)-ToM，它可以准确地预测对抗性网络代理在任意计算机网络拓扑上的目标和攻击轨迹。为了评估后者，我们提出了 Wasserstein 距离的一种新颖扩展，用于测量基于图的概率分布的相似性。由于标准 Wasserstein 距离缺乏固定的参考尺度，我们引入了图论归一化因子，可以对不同大小的网络进行标准化比较。我们为该指标（我们称之为网络传输距离 (NTD)）提供了一个加权函数，该函数根据自定义节点特征强调预测，允许网络运营商探索任意的战略考虑。在抽象的网络防御环境中，与 Graph-In, Dense-Out (GIDO)-ToM 架构进行基准测试，我们的经验评估表明，GIGO-ToM 可以准确地预测各种未见网络攻击代理在各种网络拓扑中的目标和行为，并且可以学习有效表征其策略的嵌入。</paragraph>

##### **BhashaVerse : Translation Ecosystem for Indian Subcontinent Languages**
2412.04351v1 by Vandan Mujadia, Dipti Misra Sharma

This paper focuses on developing translation models and related applications
for 36 Indian languages, including Assamese, Awadhi, Bengali, Bhojpuri, Braj,
Bodo, Dogri, English, Konkani, Gondi, Gujarati, Hindi, Hinglish, Ho, Kannada,
Kangri, Kashmiri (Arabic and Devanagari), Khasi, Mizo, Magahi, Maithili,
Malayalam, Marathi, Manipuri (Bengali and Meitei), Nepali, Oriya, Punjabi,
Sanskrit, Santali, Sinhala, Sindhi (Arabic and Devanagari), Tamil, Tulu,
Telugu, and Urdu. Achieving this requires parallel and other types of corpora
for all 36 * 36 language pairs, addressing challenges like script variations,
phonetic differences, and syntactic diversity. For instance, languages like
Kashmiri and Sindhi, which use multiple scripts, demand script normalization
for alignment, while low-resource languages such as Khasi and Santali require
synthetic data augmentation to ensure sufficient coverage and quality.
  To address these challenges, this work proposes strategies for corpus
creation by leveraging existing resources, developing parallel datasets,
generating domain-specific corpora, and utilizing synthetic data techniques.
Additionally, it evaluates machine translation across various dimensions,
including standard and discourse-level translation, domain-specific
translation, reference-based and reference-free evaluation, error analysis, and
automatic post-editing. By integrating these elements, the study establishes a
comprehensive framework to improve machine translation quality and enable
better cross-lingual communication in India's linguistically diverse ecosystem.

摘要：<paragraph>這篇論文專注於開發翻譯模型和相關應用程式，對象為 36 種印度語言，包括阿薩姆語、阿瓦德語、孟加拉語、博傑普爾語、布拉傑語、博多語、多格拉語、英語、孔卡尼語、貢德語、古吉拉特語、印地語、印度式英語、霍語、卡納達語、康格里語、克什米爾語（阿拉伯語和天城文）、卡西語、米佐語、馬加希語、邁蒂利語、馬拉雅拉姆語、馬拉地語、曼尼普爾語（孟加拉語和梅泰語）、尼泊爾語、奧里亞語、旁遮普語、梵語、桑塔利語、僧伽羅語、信德語（阿拉伯語和天城文）、泰米爾語、圖盧語、泰盧固語和烏爾都語。要達成這個目標，需要 36 * 36 種語言配對的平行語料庫和其他類型的語料庫，並解決腳本差異、語音差異和句法多樣性等挑戰。例如，使用多種腳本的語言（如克什米爾語和信德語）需要腳本標準化才能進行比對，而卡西語和桑塔利語等低資源語言則需要合成資料擴充，以確保足夠的涵蓋範圍和品質。為了應對這些挑戰，這項工作提出透過利用現有資源、開發平行資料集、產生特定領域的語料庫和使用合成資料技術來建立語料庫的策略。此外，它評估機器翻譯的各種面向，包括標準和語篇層級翻譯、特定領域翻譯、基於參考和無參考評估、錯誤分析和自動後編輯。透過整合這些元素，這項研究建立了一個全面的架構，以提升機器翻譯品質，並在印度語言多樣性的生態系統中實現更好的跨語言溝通。</paragraph>

##### **RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse**
2412.04343v1 by Zhouyingcheng Liao, Mingyuan Zhang, Wenjia Wang, Lei Yang, Taku Komura

While motion generation has made substantial progress, its practical
application remains constrained by dataset diversity and scale, limiting its
ability to handle out-of-distribution scenarios. To address this, we propose a
simple and effective baseline, RMD, which enhances the generalization of motion
generation through retrieval-augmented techniques. Unlike previous
retrieval-based methods, RMD requires no additional training and offers three
key advantages: (1) the external retrieval database can be flexibly replaced;
(2) body parts from the motion database can be reused, with an LLM facilitating
splitting and recombination; and (3) a pre-trained motion diffusion model
serves as a prior to improve the quality of motions obtained through retrieval
and direct combination. Without any training, RMD achieves state-of-the-art
performance, with notable advantages on out-of-distribution data.

摘要：儘管動作生成取得重大進展，其實際應用仍受限於資料集的多樣性和規模，這限制了其處理分布外場景的能力。為了解決這個問題，我們提出了一個簡單且有效的基準 RMD，它透過檢索增強技術增強動作生成的泛化能力。與先前的基於檢索的方法不同，RMD 不需要額外的訓練，並提供三個關鍵優勢：(1) 外部檢索資料庫可以靈活替換；(2) 動作資料庫中的身體部位可以重複使用，LLM 促進了拆分和重組；(3) 預先訓練好的動作擴散模型作為先驗，以提高透過檢索和直接組合獲得的動作品質。在沒有任何訓練的情況下，RMD 達到了最先進的效能，在分布外資料上具有顯著優勢。

##### **Retrieval-Augmented Machine Translation with Unstructured Knowledge**
2412.04342v1 by Jiaan Wang, Fandong Meng, Yingxue Zhang, Jie Zhou

Retrieval-augmented generation (RAG) introduces additional information to
enhance large language models (LLMs). In machine translation (MT), previous
work typically retrieves in-context examples from paired MT corpora, or
domain-specific knowledge from knowledge graphs, to enhance models' MT ability.
However, a large amount of world knowledge is organized in unstructured
documents, and might not be fully paired across different languages. In this
paper, we study retrieval-augmented MT using unstructured documents.
Specifically, we build RAGtrans, the first benchmark to train and evaluate
LLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples
collected via GPT-4o and human translators. Besides, documents from different
languages are also provided to supply the knowledge to these samples. Based on
RAGtrans, we further propose a multi-task training method to teach LLMs how to
use information from multilingual documents during their translation. The
method uses existing multilingual corpora to create auxiliary training
objectives without additional labeling requirements. Extensive experiments show
that the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.

摘要：檢索增強產生 (RAG) 會引入額外資訊，以增強大型語言模型 (LLM)。在機器翻譯 (MT) 中，先前的作業通常會從配對的 MT 語料庫中檢索情境範例，或從知識圖表中檢索特定領域的知識，以增強模型的 MT 能力。然而，大量的世界知識都是以非結構化文件組織，而且可能無法完全配對到不同的語言中。在本文中，我們研究使用非結構化文件進行檢索增強 MT。具體來說，我們建立了 RAGtrans，這是第一個用於訓練和評估 LLM 的檢索增強 MT 能力的基準。RAGtrans 包含透過 GPT-4o 和人工翻譯人員收集的 79K 個 MT 範例。此外，也提供了不同語言的文件，以提供這些範例的知識。根據 RAGtrans，我們進一步提出了一個多任務訓練方法，以教導 LLM 如何在翻譯過程中使用多語言文件的資訊。該方法使用現有的多語言語料庫建立輔助訓練目標，而無需額外的標記需求。廣泛的實驗顯示，該方法將 LLM 的 BLEU 分數提高了 1.58-3.09，COMET 分數提高了 1.00-2.03。

##### **Action Mapping for Reinforcement Learning in Continuous Environments with Constraints**
2412.04327v1 by Mirco Theile, Lukas Dirnberger, Raphael Trumpp, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli

Deep reinforcement learning (DRL) has had success across various domains, but
applying it to environments with constraints remains challenging due to poor
sample efficiency and slow convergence. Recent literature explored
incorporating model knowledge to mitigate these problems, particularly through
the use of models that assess the feasibility of proposed actions. However,
integrating feasibility models efficiently into DRL pipelines in environments
with continuous action spaces is non-trivial. We propose a novel DRL training
strategy utilizing action mapping that leverages feasibility models to
streamline the learning process. By decoupling the learning of feasible actions
from policy optimization, action mapping allows DRL agents to focus on
selecting the optimal action from a reduced feasible action set. We demonstrate
through experiments that action mapping significantly improves training
performance in constrained environments with continuous action spaces,
especially with imperfect feasibility models.

摘要：深度強化學習 (DRL) 在各種領域都獲得成功，但由於取樣效率低和收斂速度慢，將其應用於受限環境中仍然具有挑戰性。最近的文獻探討了整合模型知識以減輕這些問題，特別是透過使用評估提議動作可行性的模型。然而，在具有連續動作空間的環境中，將可行性模型有效地整合到 DRL 管線中並不容易。我們提出了一種新穎的 DRL 訓練策略，利用動作對應，該策略利用可行性模型來簡化學習過程。透過將可行動作的學習與策略最佳化分離，動作對應允許 DRL 代理專注於從減少的可行動作集中選擇最佳動作。我們透過實驗證明，動作對應顯著改善了具有連續動作空間的受限環境中的訓練效能，特別是在可行性模型不完美的情況下。

##### **The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation**
2412.04318v1 by Fredrik Carlsson, Fangyu Liu, Daniel Ward, Murathan Kurfali, Joakim Nivre

This paper introduces the counter-intuitive generalization results of
overfitting pre-trained large language models (LLMs) on very small datasets. In
the setting of open-ended text generation, it is well-documented that LLMs tend
to generate repetitive and dull sequences, a phenomenon that is especially
apparent when generating using greedy decoding. This issue persists even with
state-of-the-art LLMs containing billions of parameters, trained via next-token
prediction on large datasets. We find that by further fine-tuning these models
to achieve a near-zero training loss on a small set of samples -- a process we
refer to as hyperfitting -- the long-sequence generative capabilities are
greatly enhanced. Greedy decoding with these Hyperfitted models even outperform
Top-P sampling over long-sequences, both in terms of diversity and human
preferences. This phenomenon extends to LLMs of various sizes, different
domains, and even autoregressive image generation. We further find this
phenomena to be distinctly different from that of Grokking and double descent.
Surprisingly, our experiments indicate that hyperfitted models rarely fall into
repeating sequences they were trained on, and even explicitly blocking these
sequences results in high-quality output. All hyperfitted models produce
extremely low-entropy predictions, often allocating nearly all probability to a
single token.

摘要：本文介绍了在大数据集上过度拟合预训练大型语言模型 (LLM) 的反直觉泛化结果。在开放式文本生成设置中，有充分的文献表明，LLM 倾向于生成重复且枯燥的序列，这种现象在使用贪婪解码生成时尤其明显。即使是包含数十亿个参数、通过对大型数据集进行下一个标记预测训练的最先进的 LLM，也会持续出现此问题。我们发现，通过进一步微调这些模型以在少量样本上实现接近于零的训练损失——我们称之为超拟合——可以极大地增强长序列生成能力。使用这些超拟合模型进行贪婪解码，即使在多样性和人类偏好方面，也优于对长序列进行 Top-P 采样。这种现象延伸到各种规模的 LLM、不同的域，甚至自回归图像生成。我们进一步发现，这种现象与 Grokking 和双重下降截然不同。令人惊讶的是，我们的实验表明，超拟合模型很少陷入其接受训练的重复序列中，即使明确阻止这些序列也会产生高质量的输出。所有超拟合模型都会产生极低熵的预测，通常将几乎所有概率都分配给单个标记。

##### **Densing Law of LLMs**
2412.04315v1 by Chaojun Xiao, Jie Cai, Weilin Zhao, Guoyang Zeng, Xu Han, Zhiyuan Liu, Maosong Sun

Large Language Models (LLMs) have emerged as a milestone in artificial
intelligence, and their performance can improve as the model size increases.
However, this scaling brings great challenges to training and inference
efficiency, particularly for deploying LLMs in resource-constrained
environments, and the scaling trend is becoming increasingly unsustainable.
This paper introduces the concept of ``\textit{capacity density}'' as a new
metric to evaluate the quality of the LLMs across different scales and
describes the trend of LLMs in terms of both effectiveness and efficiency. To
calculate the capacity density of a given target LLM, we first introduce a set
of reference models and develop a scaling law to predict the downstream
performance of these reference models based on their parameter sizes. We then
define the \textit{effective parameter size} of the target LLM as the parameter
size required by a reference model to achieve equivalent performance, and
formalize the capacity density as the ratio of the effective parameter size to
the actual parameter size of the target LLM. Capacity density provides a
unified framework for assessing both model effectiveness and efficiency. Our
further analysis of recent open-source base LLMs reveals an empirical law (the
densing law)that the capacity density of LLMs grows exponentially over time.
More specifically, using some widely used benchmarks for evaluation, the
capacity density of LLMs doubles approximately every three months. The law
provides new perspectives to guide future LLM development, emphasizing the
importance of improving capacity density to achieve optimal results with
minimal computational overhead.

摘要：大型語言模型 (LLM) 已成為人工智慧的里程碑，且其效能會隨著模型大小的增加而提升。
然而，這種擴充為訓練和推論效率帶來極大的挑戰，特別是在資源受限的環境中部署 LLM，且擴充趨勢正變得愈來愈不可持續。
本文介紹了「容量密度」的概念，作為評估不同規模 LLM 品質的新指標，並根據效能和效率描述 LLM 的趨勢。
若要計算特定目標 LLM 的容量密度，我們首先會引入一組參考模型，並根據其參數規模，制定一個擴充定律來預測這些參考模型的下游效能。
接著，我們將目標 LLM 的「有效參數規模」定義為參考模型達成等效效能所需的參數規模，並將容量密度形式化為有效參數規模與目標 LLM 實際參數規模的比率。
容量密度提供了一個統一的架構，用於評估模型的效能和效率。
我們進一步分析近期開源的基本 LLM，揭露了一項經驗法則（密度定律），指出 LLM 的容量密度會隨著時間呈指數成長。
更具體來說，使用一些廣泛用於評估的基準，LLM 的容量密度大約每三個月就會增加一倍。
此定律為未來的 LLM 發展提供了新的觀點，強調提升容量密度以在最小的運算負擔下達成最佳結果的重要性。

##### **ALMA: Alignment with Minimal Annotation**
2412.04305v1 by Michihiro Yasunaga, Leonid Shamis, Chunting Zhou, Andrew Cohen, Jason Weston, Luke Zettlemoyer, Marjan Ghazvininejad

Recent approaches to large language model (LLM) alignment typically require
millions of human annotations or rely on external aligned models for synthetic
data generation. This paper introduces ALMA: Alignment with Minimal Annotation,
demonstrating that effective alignment can be achieved using only 9,000 labeled
examples -- less than 1% of conventional approaches. ALMA generates large
amounts of high-quality synthetic alignment data through new techniques:
diverse prompt synthesis via few-shot learning, diverse response generation
with multiple model checkpoints, and judge (reward model) enhancement through
score aggregation and self-distillation. Using only a pretrained Llama3 base
model, 5,000 SFT examples, and 4,000 judge annotations, ALMA achieves
performance close to Llama3-Instruct across diverse alignment benchmarks (e.g.,
0.1% difference on AlpacaEval 2.0 score). These results are achieved with a
multi-round, self-bootstrapped data synthesis and training recipe that
continues to improve for 10 rounds, surpassing the typical 3-round ceiling of
previous methods. These results suggest that base models already possess
sufficient knowledge for effective alignment, and that synthetic data
generation methods can expose it.

摘要：近期針對大型語言模型 (LLM) 對齊的方法通常需要數百萬筆人工註解，或依賴外部對齊模型來產生合成資料。本文介紹 ALMA：使用最少註解對齊，證明僅使用 9,000 個標記範例就能達成有效對齊，少於傳統方法的 1%。ALMA 透過新技術產生大量高品質的合成對齊資料：透過少量學習進行多元提示合成、使用多個模型檢查點進行多元回應產生，以及透過分數彙總和自我蒸餾來提升評判 (獎勵模型)。ALMA 僅使用預先訓練的 Llama3 基礎模型、5,000 個 SFT 範例和 4,000 個評判註解，就能在不同的對齊基準上達成接近 Llama3-Instruct 的效能 (例如，AlpacaEval 2.0 分數相差 0.1%)。這些結果是透過多輪、自我引導的資料合成和訓練方法達成，持續改善 10 輪，超越先前方法典型的 3 輪上限。這些結果顯示，基礎模型已具備足夠知識來進行有效對齊，而合成資料產生方法可以揭露這些知識。

##### **T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts**
2412.04300v1 by Ziwei Huang, Wanggui He, Quanyu Long, Yandi Wang, Haoyuan Li, Zhelun Yu, Fangxun Shu, Long Chen, Hao Jiang, Leilei Gan

Evaluating the quality of synthesized images remains a significant challenge
in the development of text-to-image (T2I) generation. Most existing studies in
this area primarily focus on evaluating text-image alignment, image quality,
and object composition capabilities, with comparatively fewer studies
addressing the evaluation of the factuality of T2I models, particularly when
the concepts involved are knowledge-intensive. To mitigate this gap, we present
T2I-FactualBench in this work - the largest benchmark to date in terms of the
number of concepts and prompts specifically designed to evaluate the factuality
of knowledge-intensive concept generation. T2I-FactualBench consists of a
three-tiered knowledge-intensive text-to-image generation framework, ranging
from the basic memorization of individual knowledge concepts to the more
complex composition of multiple knowledge concepts. We further introduce a
multi-round visual question answering (VQA) based evaluation framework to
assess the factuality of three-tiered knowledge-intensive text-to-image
generation tasks. Experiments on T2I-FactualBench indicate that current
state-of-the-art (SOTA) T2I models still leave significant room for
improvement.

摘要：評估合成影像品質在文字轉影像 (T2I) 生成發展中仍是一項重大挑戰。現有大多數研究主要專注於評估文字影像對齊、影像品質和物體組成能力，較少研究探討 T2I 模型的事實性評估，特別是在所涉及概念需要大量知識時。為了彌補這個差距，我們在這項工作中提出 T2I-FactualBench，這是目前在概念和提示數量方面最大的基準，專門用於評估知識密集型概念生成的真實性。T2I-FactualBench 包含一個三層級的知識密集型文字轉影像生成架構，從基本記憶個別知識概念到更複雜的多個知識概念組成。我們進一步引入一個基於多輪視覺問答 (VQA) 的評估架構，以評估三層級知識密集型文字轉影像生成任務的事實性。在 T2I-FactualBench 上的實驗表明，目前的最新技術 (SOTA) T2I 模型仍有很大的改進空間。

##### **SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model**
2412.04292v1 by Zhenglin Huang, Jinwei Hu, Xiangtai Li, Yiwei He, Xingyu Zhao, Bei Peng, Baoyuan Wu, Xiaowei Huang, Guangliang Cheng

The rapid advancement of generative models in creating highly realistic
images poses substantial risks for misinformation dissemination. For instance,
a synthetic image, when shared on social media, can mislead extensive audiences
and erode trust in digital content, resulting in severe repercussions. Despite
some progress, academia has not yet created a large and diversified deepfake
detection dataset for social media, nor has it devised an effective solution to
address this issue. In this paper, we introduce the Social media Image
Detection dataSet (SID-Set), which offers three key advantages: (1) extensive
volume, featuring 300K AI-generated/tampered and authentic images with
comprehensive annotations, (2) broad diversity, encompassing fully synthetic
and tampered images across various classes, and (3) elevated realism, with
images that are predominantly indistinguishable from genuine ones through mere
visual inspection. Furthermore, leveraging the exceptional capabilities of
large multimodal models, we propose a new image deepfake detection,
localization, and explanation framework, named SIDA (Social media Image
Detection, localization, and explanation Assistant). SIDA not only discerns the
authenticity of images, but also delineates tampered regions through mask
prediction and provides textual explanations of the model's judgment criteria.
Compared with state-of-the-art deepfake detection models on SID-Set and other
benchmarks, extensive experiments demonstrate that SIDA achieves superior
performance among diversified settings. The code, model, and dataset will be
released.

摘要：生成模型在創造高度逼真的圖像方面快速進展，對錯誤訊息傳播構成重大風險。例如，合成影像在社群媒體上分享時，可能會誤導廣大受眾，並侵蝕對數位內容的信任，造成嚴重的後果。儘管有些進展，學術界尚未針對社群媒體建立大型且多樣化的深度偽造偵測資料集，也沒有提出有效的解決方案來解決此問題。在本文中，我們介紹了社群媒體影像偵測資料集 (SID-Set)，它提供了三個主要優點：(1) 龐大的數量，包含 300K 個 AI 生成的/竄改的和真實的影像，並附有全面的註解，(2) 廣泛的多樣性，涵蓋各種類別的完全合成和竄改影像，以及 (3) 提升的真實性，影像主要透過單純的視覺檢查與真品難以區分。此外，利用大型多模態模型的卓越功能，我們提出了一個新的影像深度偽造偵測、定位和說明架構，稱為 SIDA (社群媒體影像偵測、定位和說明助理)。SIDA 不僅可以辨別影像的真實性，還可以透過遮罩預測描繪竄改區域，並提供模型判斷準則的文字說明。與 SID-Set 和其他基準上的最新深度偽造偵測模型相比，廣泛的實驗證明 SIDA 在多樣化的設定中取得了卓越的效能。程式碼、模型和資料集將會釋出。

##### **Evolutionary Pre-Prompt Optimization for Mathematical Reasoning**
2412.04291v1 by Mathurin Videau, Alessandro Leite, Marc Schoenauer, Olivier Teytaud

Recent advancements have highlighted that large language models (LLMs), when
given a small set of task-specific examples, demonstrate remarkable
proficiency, a capability that extends to complex reasoning tasks. In
particular, the combination of few-shot learning with the chain-of-thought
(CoT) approach has been pivotal in steering models towards more logically
consistent conclusions. This paper explores the optimization of example
selection for designing effective CoT pre-prompts and shows that the choice of
the optimization algorithm, typically in favor of comparison-based methods such
as evolutionary computation, significantly enhances efficacy and feasibility.
Specifically, thanks to a limited exploitative and overfitted optimization,
Evolutionary Pre-Prompt Optimization (EPPO) brings an improvement over the
naive few-shot approach exceeding 10 absolute points in exact match scores on
benchmark datasets such as GSM8k and MathQA. These gains are consistent across
various contexts and are further amplified when integrated with
self-consistency (SC)

摘要：近期進展表明，大型語言模型 (LLM) 在給予少量特定任務範例時，能展現出卓越的熟練度，此能力延伸至複雜的推理任務。特別是，將少量學習與思考鏈 (CoT) 方法相結合，對於引導模型朝向更合乎邏輯一致的結論至關重要。本文探討了範例選取的最佳化，用於設計有效的 CoT 預提示，並顯示最佳化演算法的選擇（通常偏好基於比較的演算法，例如演化運算）顯著增強了效能和可行性。具體來說，由於有限的開發和過度擬合最佳化，演化預提示最佳化 (EPPO) 在基準資料集（例如 GSM8k 和 MathQA）上的完全匹配分數中，比天真的少量學習方法提升了超過 10 個絕對點。這些增益在各種情境中是一致的，並且在與自我一致性 (SC) 整合後進一步擴大。

##### **Arabic Stable LM: Adapting Stable LM 2 1.6B to Arabic**
2412.04277v1 by Zaid Alyafeai, Michael Pieler, Hannah Teufel, Jonathan Tow, Marco Bellagente, Duy Phung, Nikhil Pinnaparaju, Reshinth Adithyan, Paulo Rocha, Maksym Zhuravinskyi, Carlos Riquelme

Large Language Models (LLMs) have shown impressive results in multiple
domains of natural language processing (NLP) but are mainly focused on the
English language. Recently, more LLMs have incorporated a larger proportion of
multilingual text to represent low-resource languages. In Arabic NLP, several
Arabic-centric LLMs have shown remarkable results on multiple benchmarks in the
past two years. However, most Arabic LLMs have more than 7 billion parameters,
which increases their hardware requirements and inference latency, when
compared to smaller LLMs. This paper introduces Arabic Stable LM 1.6B in a base
and chat version as a small but powerful Arabic-centric LLM. Our Arabic Stable
LM 1.6B chat model achieves impressive results on several benchmarks beating
multiple models with up to 8x the parameters. In addition, we show the benefit
of mixing in synthetic instruction tuning data by augmenting our fine-tuning
data with a large synthetic dialogue dataset.

摘要：大型語言模型 (LLM) 在自然語言處理 (NLP) 的多個領域中展現令人印象深刻的成果，但主要著重於英語。最近，更多 LLM 融入了更大比例的多語言文字，以呈現低資源語言。在阿拉伯語 NLP 中，幾個以阿拉伯語為中心的 LLM 在過去兩年中，在多個基準測試中展現卓越的成果。然而，大多數阿拉伯語 LLM 擁有超過 70 億個參數，與較小的 LLM 相比，這會增加其硬體需求和推論延遲。本文介紹了基礎版和聊天版的阿拉伯語 Stable LM 1.6B，作為一個小巧但強大的以阿拉伯語為中心的 LLM。我們的阿拉伯語 Stable LM 1.6B 聊天模型在幾個基準測試中獲得令人印象深刻的成果，擊敗多個參數多達 8 倍的模型。此外，我們展示了混合合成指令調整資料的好處，方法是用大型合成對話資料集擴充我們的微調資料。

##### **PoTable: Programming Standardly on Table-based Reasoning Like a Human Analyst**
2412.04272v1 by Qingyang Mao, Qi Liu, Zhi Li, Mingyue Cheng, Zheng Zhang, Rui Li

Table-based reasoning has garnered substantial research interest,
particularly in its integration with Large Language Model (LLM) which has
revolutionized the general reasoning paradigm. Numerous LLM-based studies
introduce symbolic tools (e.g., databases, Python) as assistants to extend
human-like abilities in structured table understanding and complex arithmetic
computations. However, these studies can be improved better in simulating human
cognitive behavior when using symbolic tools, as they still suffer from
limitations of non-standard logical splits and constrained operation pools. In
this study, we propose PoTable as a novel table-based reasoning method that
simulates a human tabular analyst, which integrates a Python interpreter as the
real-time executor accompanied by an LLM-based operation planner and code
generator. Specifically, PoTable follows a human-like logical stage split and
extends the operation pool into an open-world space without any constraints.
Through planning and executing in each distinct stage, PoTable standardly
completes the entire reasoning process and produces superior reasoning results
along with highly accurate, steply commented and completely executable
programs. Accordingly, the effectiveness and explainability of PoTable are
fully demonstrated. Extensive experiments over three evaluation datasets from
two public benchmarks on two backbones show the outstanding performance of our
approach. In particular, GPT-based PoTable achieves over 4% higher absolute
accuracy than runner-ups on all evaluation datasets.

摘要：基於表格的推理已引起大量的研究興趣，特別是它與大型語言模型 (LLM) 的整合，這已徹底改變了通用的推理範例。許多基於 LLM 的研究引入符號工具（例如資料庫、Python）作為助理，以擴展人類在結構化表格理解和複雜算術計算中類似的能力。然而，這些研究可以在模擬人類在使用符號工具時的認知行為方面得到更好的改進，因為它們仍然受到非標準邏輯分割和受限操作池的限制。在這項研究中，我們提出 PoTable 作為一種新穎的基於表格的推理方法，它模擬人類表格分析師，它整合了一個 Python 解釋器作為實時執行器，並附帶一個基於 LLM 的操作規劃器和程式碼生成器。具體來說，PoTable 遵循類似人類的邏輯階段分割，並將操作池擴展到一個沒有任何約束的開放世界空間。透過在每個不同的階段進行規劃和執行，PoTable 標準地完成整個推理過程，並產生優異的推理結果，以及高度準確、逐步註解且完全可執行的程式。因此，PoTable 的有效性和可解釋性得到了充分的證明。在兩個主幹上來自兩個公開基準測試的三個評估資料集上的大量實驗顯示了我們方法的傑出性能。特別是，基於 GPT 的 PoTable 在所有評估資料集上比第二名高出 4% 以上的絕對準確度。

##### **Representation Purification for End-to-End Speech Translation**
2412.04266v1 by Chengwei Zhang, Yue Zhou, Rui Zhao, Yidong Chen, Xiaodong Shi

Speech-to-text translation (ST) is a cross-modal task that involves
converting spoken language into text in a different language. Previous research
primarily focused on enhancing speech translation by facilitating knowledge
transfer from machine translation, exploring various methods to bridge the gap
between speech and text modalities. Despite substantial progress made, factors
in speech that are not relevant to translation content, such as timbre and
rhythm, often limit the efficiency of knowledge transfer. In this paper, we
conceptualize speech representation as a combination of content-agnostic and
content-relevant factors. We examine the impact of content-agnostic factors on
translation performance through preliminary experiments and observe a
significant performance deterioration when content-agnostic perturbations are
introduced to speech signals. To address this issue, we propose a
\textbf{S}peech \textbf{R}epresentation \textbf{P}urification with
\textbf{S}upervision \textbf{E}nhancement (SRPSE) framework, which excludes the
content-agnostic components within speech representations to mitigate their
negative impact on ST. Experiments on MuST-C and CoVoST-2 datasets demonstrate
that SRPSE significantly improves translation performance across all
translation directions in three settings and achieves preeminent performance
under a \textit{transcript-free} setting.

摘要：語音轉文字翻譯 (ST) 是一個跨模態任務，涉及將口說語言轉換為不同語言的文字。先前的研究主要集中於透過促進機器翻譯的知識轉移來增強語音翻譯，探索各種方法來彌合語音和文字模態之間的差距。儘管取得了實質進展，但語音中與翻譯內容無關的因素（例如音色和節奏）通常會限制知識轉移的效率。在本文中，我們將語音表徵概念化為內容不可知和與內容相關的因素的組合。我們透過初步實驗檢查內容不可知因素對翻譯效能的影響，並觀察到當內容不可知的擾動被引入語音訊號時，效能會顯著下降。為了解決這個問題，我們提出了一個語音表徵淨化與監督增強 (SRPSE) 框架，它排除了語音表徵中的內容不可知組成部分，以減輕它們對 ST 的負面影響。在 MuST-C 和 CoVoST-2 資料集上的實驗證明，SRPSE 在所有翻譯方向上顯著改善了翻譯效能，並在「無轉錄」設定下取得卓越的效能。

##### **Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier**
2412.04261v1 by John Dang, Shivalika Singh, Daniel D'souza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, Sara Hooker

We introduce the Aya Expanse model family, a new generation of 8B and 32B
parameter multilingual language models, aiming to address the critical
challenge of developing highly performant multilingual models that match or
surpass the capabilities of monolingual models. By leveraging several years of
research at Cohere For AI and Cohere, including advancements in data arbitrage,
multilingual preference training, and model merging, Aya Expanse sets a new
state-of-the-art in multilingual performance. Our evaluations on the
Arena-Hard-Auto dataset, translated into 23 languages, demonstrate that Aya
Expanse 8B and 32B outperform leading open-weight models in their respective
parameter classes, including Gemma 2, Qwen 2.5, and Llama 3.1, achieving up to
a 76.6% win-rate. Notably, Aya Expanse 32B outperforms Llama 3.1 70B, a model
with twice as many parameters, achieving a 54.0% win-rate. In this short
technical report, we present extended evaluation results for the Aya Expanse
model family and release their open-weights, together with a new multilingual
evaluation dataset m-ArenaHard.

摘要：我們推出了 Aya Expanse 模型家族，這是新一代的 8B 和 32B 參數多語言語言模型，旨在解決開發高度效能多語言模型的關鍵挑戰，這些模型與單語言模型相匹配或超越其能力。透過利用 Cohere For AI 和 Cohere 多年來的研究，包括資料套利、多語言偏好訓練和模型合併的進展，Aya Expanse 在多語言效能方面樹立了新的技術領先地位。我們對已翻譯成 23 種語言的 Arena-Hard-Auto 資料集進行的評估表明，Aya Expanse 8B 和 32B 在各自的參數類別中優於領先的開放權重模型，包括 Gemma 2、Qwen 2.5 和 Llama 3.1，達到高達 76.6% 的獲勝率。值得注意的是，Aya Expanse 32B 優於 Llama 3.1 70B，後者是一個具有兩倍參數的模型，達到 54.0% 的獲勝率。在這份簡短的技術報告中，我們展示了 Aya Expanse 模型家族的延伸評估結果，並發布了它們的開放權重，以及新的多語言評估資料集 m-ArenaHard。

##### **Enhancing Whole Slide Image Classification through Supervised Contrastive Domain Adaptation**
2412.04260v1 by Ilán Carretero, Pablo Meseguer, Rocío del Amor, Valery Naranjo

Domain shift in the field of histopathological imaging is a common phenomenon
due to the intra- and inter-hospital variability of staining and digitization
protocols. The implementation of robust models, capable of creating generalized
domains, represents a need to be solved. In this work, a new domain adaptation
method to deal with the variability between histopathological images from
multiple centers is presented. In particular, our method adds a training
constraint to the supervised contrastive learning approach to achieve domain
adaptation and improve inter-class separability. Experiments performed on
domain adaptation and classification of whole-slide images of six skin cancer
subtypes from two centers demonstrate the method's usefulness. The results
reflect superior performance compared to not using domain adaptation after
feature extraction or staining normalization.

摘要：組織病理影像領域中的域偏移是一種常見現象，這是由於染色和數位化協定的院內和院際變異所致。實作強健的模型，能夠建立廣義的域，代表著一種亟待解決的需求。在這項工作中，提出了一種新的域適應方法來處理來自多個中心的組織病理影像之間的變異性。具體來說，我們的做法是為監督對比學習方法新增一個訓練約束，以達成域適應並改善類間可分離性。在兩個中心對六種皮膚癌亞型的全切片影像進行域適應和分類的實驗中，證明了該方法的實用性。結果顯示，與在特徵萃取或染色標準化後不使用域適應相比，表現優異。

##### **CLINICSUM: Utilizing Language Models for Generating Clinical Summaries from Patient-Doctor Conversations**
2412.04254v1 by Subash Neupane, Himanshu Tripathi, Shaswata Mitra, Sean Bozorgzad, Sudip Mittal, Shahram Rahimi, Amin Amirlatifi

This paper presents ClinicSum, a novel framework designed to automatically
generate clinical summaries from patient-doctor conversations. It utilizes a
two-module architecture: a retrieval-based filtering module that extracts
Subjective, Objective, Assessment, and Plan (SOAP) information from
conversation transcripts, and an inference module powered by fine-tuned
Pre-trained Language Models (PLMs), which leverage the extracted SOAP data to
generate abstracted clinical summaries. To fine-tune the PLM, we created a
training dataset of consisting 1,473 conversations-summaries pair by
consolidating two publicly available datasets, FigShare and MTS-Dialog, with
ground truth summaries validated by Subject Matter Experts (SMEs). ClinicSum's
effectiveness is evaluated through both automatic metrics (e.g., ROUGE,
BERTScore) and expert human assessments. Results show that ClinicSum
outperforms state-of-the-art PLMs, demonstrating superior precision, recall,
and F-1 scores in automatic evaluations and receiving high preference from SMEs
in human assessment, making it a robust solution for automated clinical
summarization.

摘要：本文介紹 ClinicSum，這是一個新穎的架構，旨在自動從病患與醫師的對話中產生臨床摘要。它利用一個雙模組架構：一個基於檢索的過濾模組，從對話轉錄中萃取主觀、客觀、評估和計畫 (SOAP) 資訊，以及一個由微調過之預先訓練語言模型 (PLM) 提供動力的推論模組，它利用萃取的 SOAP 資料產生摘要的臨床摘要。為了微調 PLM，我們建立了一個訓練資料集，其中包含 1,473 組對話摘要，透過合併兩個公開可用的資料集 FigShare 和 MTS-Dialog，以及由主題專家 (SME) 驗證的真實摘要。ClinicSum 的效能透過自動評量指標 (例如 ROUGE、BERTScore) 和專家人類評估進行評量。結果顯示 ClinicSum 勝過現有最先進的 PLM，在自動評量中展現出優異的精確度、召回率和 F-1 分數，並在人類評估中獲得 SME 的高度偏好，使其成為自動臨床摘要的強健解決方案。

##### **A History of Philosophy in Colombia through Topic Modelling**
2412.04236v1 by Juan R. Loaiza, Miguel González-Duque

Data-driven approaches to philosophy have emerged as a valuable tool for
studying the history of the discipline. However, most studies in this area have
focused on a limited number of journals from specific regions and subfields. We
expand the scope of this research by applying dynamic topic modelling
techniques to explore the history of philosophy in Colombia and Latin America.
Our study examines the Colombian philosophy journal Ideas y Valores, founded in
1951 and currently one of the most influential academic philosophy journals in
the region. By analyzing the evolution of topics across the journal's history,
we identify various trends and specific dynamics in philosophical discourse
within the Colombian and Latin American context. Our findings reveal that the
most prominent topics are value theory (including ethics, political philosophy,
and aesthetics), epistemology, and the philosophy of science. We also trace the
evolution of articles focusing on the historical and interpretive aspects of
philosophical texts, and we note a notable emphasis on German philosophers such
as Kant, Husserl, and Hegel on various topics throughout the journal's
lifetime. Additionally, we investigate whether articles with a historical focus
have decreased over time due to editorial pressures. Our analysis suggests no
significant decline in such articles. Finally, we propose ideas for extending
this research to other Latin American journals and suggest improvements for
natural language processing workflows in non-English languages.

摘要：<paragraph>資料驅動的哲學方法已成為研究該學科歷史的寶貴工具。然而，此領域的大多數研究都集中在特定區域和子領域的少量期刊上。我們透過應用動態主題建模技術來探索哥倫比亞和拉丁美洲的哲學歷史，以擴展這項研究的範圍。我們的研究探討了創立於 1951 年的哥倫比亞哲學期刊《Ideas y Valores》，它目前是該地區最具影響力的學術哲學期刊之一。透過分析期刊歷史中主題的演變，我們找出哥倫比亞和拉丁美洲背景中哲學論述中的各種趨勢和具體動態。我們的研究結果顯示，最突出的主題是價值理論（包括倫理學、政治哲學和美學）、認識論和科學哲學。我們也追溯了專注於哲學文本的歷史和詮釋面向的文章的演變，並注意到在期刊的整個生命週期中，對康德、胡塞爾和黑格爾等德國哲學家的各種主題都有顯著的重視。此外，我們調查了具有歷史焦點的文章是否因編輯壓力而隨著時間推移而減少。我們的分析表明，此類文章並未顯著減少。最後，我們提出了將這項研究擴展到其他拉丁美洲期刊的想法，並建議改善非英語語言的自然語言處理工作流程。</paragraph>

##### **Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots**
2412.04235v1 by Maria Paola Priola

I combine detection and mitigation techniques to addresses hallucinations in
Large Language Models (LLMs). Mitigation is achieved in a question-answering
Retrieval-Augmented Generation (RAG) framework while detection is obtained by
introducing the Negative Missing Information Scoring System (NMISS), which
accounts for contextual relevance in responses. While RAG mitigates
hallucinations by grounding answers in external data, NMISS refines the
evaluation by identifying cases where traditional metrics incorrectly flag
contextually accurate responses as hallucinations. I use Italian health news
articles as context to evaluate LLM performance. Results show that Gemma2 and
GPT-4 outperform the other models, with GPT-4 producing answers closely aligned
with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral
benefit significantly from NMISS, highlighting their ability to provide richer
contextual information. This combined approach offers new insights into the
reduction and more accurate assessment of hallucinations in LLMs, with
applications in real-world healthcare tasks and other domains.

摘要：我結合偵測和緩解技術來處理大型語言模型 (LLM) 中的幻覺。緩解是在問答檢索擴增生成 (RAG) 架構中實現的，而偵測則是透過導入負向遺失資訊評分系統 (NMISS) 獲得，該系統考量回應中的脈絡相關性。RAG 透過將答案建立在外部資料中來緩解幻覺，而 NMISS 則透過找出傳統指標錯誤地將脈絡上正確的回應標示為幻覺的情況來改善評估。我使用義大利健康新聞文章作為脈絡來評估 LLM 表現。結果顯示 Gemma2 和 GPT-4 優於其他模型，其中 GPT-4 產生的答案與參考回應最為接近。中階模型，例如 Llama2、Llama3 和 Mistral 從 NMISS 中受益匪淺，突顯出它們提供更豐富脈絡資訊的能力。這種結合方法提供了對 LLM 中幻覺的減少和更準確評估的新見解，並應用於真實世界的醫療保健任務和其他領域。

##### **DEIM: DETR with Improved Matching for Fast Convergence**
2412.04234v1 by Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, Xi Shen

We introduce DEIM, an innovative and efficient training framework designed to
accelerate convergence in real-time object detection with Transformer-based
architectures (DETR). To mitigate the sparse supervision inherent in one-to-one
(O2O) matching in DETR models, DEIM employs a Dense O2O matching strategy. This
approach increases the number of positive samples per image by incorporating
additional targets, using standard data augmentation techniques. While Dense
O2O matching speeds up convergence, it also introduces numerous low-quality
matches that could affect performance. To address this, we propose the
Matchability-Aware Loss (MAL), a novel loss function that optimizes matches
across various quality levels, enhancing the effectiveness of Dense O2O.
Extensive experiments on the COCO dataset validate the efficacy of DEIM. When
integrated with RT-DETR and D-FINE, it consistently boosts performance while
reducing training time by 50%. Notably, paired with RT-DETRv2, DEIM achieves
53.2% AP in a single day of training on an NVIDIA 4090 GPU. Additionally,
DEIM-trained real-time models outperform leading real-time object detectors,
with DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7% and 56.5% AP at 124 and 78
FPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We
believe DEIM sets a new baseline for advancements in real-time object
detection. Our code and pre-trained models are available at
https://github.com/ShihuaHuang95/DEIM.

摘要：<paragraph>我們推出 DEIM，一個創新且高效的訓練架構，旨在加速基於 Transformer 的架構（DETR）的即時物件偵測中的收斂。為了減輕 DETR 模型中的一對一 (O2O) 匹配中固有的稀疏監督，DEIM 採用密集 O2O 匹配策略。此方法透過使用標準資料擴充技術，納入額外的目標，來增加每張影像中的正樣本數量。雖然密集 O2O 匹配加速了收斂，但它也引入了許多可能影響效能的低品質匹配。為了解決這個問題，我們提出了匹配感知損失 (MAL)，一種針對各種品質層級最佳化匹配的新損失函數，以增強密集 O2O 的效能。在 COCO 資料集上的廣泛實驗驗證了 DEIM 的效能。當與 RT-DETR 和 D-FINE 整合時，它持續提升效能，同時將訓練時間縮短 50%。值得注意的是，搭配 RT-DETRv2，DEIM 在 NVIDIA 4090 GPU 上訓練一天即可達到 53.2% 的 AP。此外，DEIM 訓練的即時模型優於領先的即時物件偵測器，其中 DEIM-D-FINE-L 和 DEIM-D-FINE-X 在 NVIDIA T4 GPU 上分別以 124 和 78 FPS 達到 54.7% 和 56.5% 的 AP，而無需額外資料。我們相信 DEIM 為即時物件偵測的進展設定了新的基準。我們的程式碼和預訓練模型可在 https://github.com/ShihuaHuang95/DEIM 取得。</paragraph>

##### **Customize Segment Anything Model for Multi-Modal Semantic Segmentation with Mixture of LoRA Experts**
2412.04220v1 by Chenyang Zhu, Bin Xiao, Lin Shi, Shoukun Xu, Xu Zheng

The recent Segment Anything Model (SAM) represents a significant breakthrough
in scaling segmentation models, delivering strong performance across various
downstream applications in the RGB modality. However, directly applying SAM to
emerging visual modalities, such as depth and event data results in suboptimal
performance in multi-modal segmentation tasks. In this paper, we make the first
attempt to adapt SAM for multi-modal semantic segmentation by proposing a
Mixture of Low-Rank Adaptation Experts (MoE-LoRA) tailored for different input
visual modalities. By training only the MoE-LoRA layers while keeping SAM's
weights frozen, SAM's strong generalization and segmentation capabilities can
be preserved for downstream tasks. Specifically, to address cross-modal
inconsistencies, we propose a novel MoE routing strategy that adaptively
generates weighted features across modalities, enhancing multi-modal feature
integration. Additionally, we incorporate multi-scale feature extraction and
fusion by adapting SAM's segmentation head and introducing an auxiliary
segmentation head to combine multi-scale features for improved segmentation
performance effectively. Extensive experiments were conducted on three
multi-modal benchmarks: DELIVER, MUSES, and MCubeS. The results consistently
demonstrate that the proposed method significantly outperforms state-of-the-art
approaches across diverse scenarios. Notably, under the particularly
challenging condition of missing modalities, our approach exhibits a
substantial performance gain, achieving an improvement of 32.15% compared to
existing methods.

摘要：最近的 Segment Anything Model (SAM) 代表了分割模型的縮放上的一項重大突破，在 RGB 模式下的各種下游應用中提供了強大的效能。然而，將 SAM 直接應用於新興的視覺模式，例如深度和事件資料，會導致多模式分割任務中的次最佳效能。在本文中，我們首次嘗試通過提出針對不同輸入視覺模式量身定做的低秩適應專家混合模型 (MoE-LoRA) 來適應 SAM 以進行多模式語義分割。通過僅訓練 MoE-LoRA 層，同時保持 SAM 的權重凍結，可以為下游任務保留 SAM 強大的泛化和分割能力。具體來說，為了解決跨模式不一致性，我們提出了一種新穎的 MoE 路由策略，該策略自適應地生成跨模式的加權特徵，增強了多模式特徵整合。此外，我們通過適應 SAM 的分割頭並引入輔助分割頭來結合多尺度特徵，以有效地提高分割效能，從而整合多尺度特徵提取和融合。在三個多模式基準測試：DELIVER、MUSES 和 MCubeS 上進行了廣泛的實驗。結果一致地表明，所提出的方法在各種場景中都明顯優於最先進的方法。值得注意的是，在特別具有挑戰性的模式缺失條件下，我們的做法表現出顯著的效能提升，與現有方法相比，提升了 32.15%。

##### **A Context-aware Framework for Translation-mediated Conversations**
2412.04205v1 by José Pombal, Sweta Agrawal, Patrick Fernandes, Emmanouil Zaranis, André F. T. Martins

Effective communication is fundamental to any interaction, yet challenges
arise when participants do not share a common language. Automatic translation
systems offer a powerful solution to bridge language barriers in such
scenarios, but they introduce errors that can lead to misunderstandings and
conversation breakdown. A key issue is that current systems fail to incorporate
the rich contextual information necessary to resolve ambiguities and omitted
details, resulting in literal, inappropriate, or misaligned translations. In
this work, we present a framework to improve large language model-based
translation systems by incorporating contextual information in bilingual
conversational settings. During training, we leverage context-augmented
parallel data, which allows the model to generate translations sensitive to
conversational history. During inference, we perform quality-aware decoding
with context-aware metrics to select the optimal translation from a pool of
candidates. We validate both components of our framework on two task-oriented
domains: customer chat and user-assistant interaction. Across both settings,
our framework consistently results in better translations than state-of-the-art
systems like GPT-4o and TowerInstruct, as measured by multiple automatic
translation quality metrics on several language pairs. We also show that the
resulting model leverages context in an intended and interpretable way,
improving consistency between the conveyed message and the generated
translations.

摘要：有效的溝通對於任何互動都至關重要，然而，當參與者不共享共同語言時，挑戰就會出現。自動翻譯系統提供了一個強大的解決方案，可以在這種情況下消除語言障礙，但它們會引入錯誤，從而導致誤解和對話中斷。一個關鍵問題是當前系統未能納入解決歧義和省略細節所需的豐富上下文資訊，導致翻譯過於生硬、不恰當或不一致。在這項工作中，我們提出了一個框架，通過在雙語對話場景中納入上下文資訊來改進基於大型語言模型的翻譯系統。在訓練期間，我們利用上下文增強平行數據，這允許模型生成對話歷史敏感的翻譯。在推理過程中，我們使用上下文感知指標執行品質感知解碼，從候選池中選擇最佳翻譯。我們在兩個面向任務的領域（客戶聊天和使用者助理互動）驗證了我們框架的兩個組成部分。在兩種設定中，我們的框架始終產生比最先進的系統（如 GPT-4o 和 TowerInstruct）更好的翻譯，這是通過多種自動翻譯品質指標在多種語言對上測量的。我們還表明，生成的模型以預期和可解釋的方式利用上下文，改進了傳達訊息和生成翻譯之間的一致性。

##### **AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in Dialectal Arabic**
2412.04193v1 by Nathaniel R. Robinson, Shahd Abdelmoneim, Kelly Marchisio, Sebastian Ruder

Dialectal Arabic (DA) varieties are under-served by language technologies,
particularly large language models (LLMs). This trend threatens to exacerbate
existing social inequalities and limits language modeling applications, yet the
research community lacks operationalized LLM performance measurements in DA. We
present a method that comprehensively evaluates LLM fidelity, understanding,
quality, and diglossia in modeling DA. We evaluate nine LLMs in eight DA
varieties across these four dimensions and provide best practice
recommendations. Our evaluation suggests that LLMs do not produce DA as well as
they understand it, but does not suggest deterioration in quality when they do.
Further analysis suggests that current post-training can degrade DA
capabilities, that few-shot examples can overcome this and other LLM
deficiencies, and that otherwise no measurable features of input text correlate
well with LLM DA performance.

摘要：方言阿拉伯語 (DA) 品種缺乏語言技術的支援，特別是大語言模型 (LLM)。這種趨勢可能會加劇現有的社會不平等，並限制語言建模應用，但研究社群缺乏在 DA 中操作 LLM 效能測量的能力。我們提出了一種方法，全面評估 LLM 在建模 DA 中的保真度、理解力、品質和雙語。我們在八種 DA 品種中評估了九個 LLM，並針對這四個面向提供最佳實務建議。我們的評估表明，LLM 產生的 DA 品質不如理解力，但並沒有顯示出品質下降。進一步的分析表明，目前的訓練後處理可能會降低 DA 的能力，少量的範例可以克服這個和其他 LLM 的缺陷，而輸入文字的其他可測量特徵與 LLM 的 DA 效能並無顯著相關性。

##### **Directed Structural Adaptation to Overcome Statistical Conflicts and Enable Continual Learning**
2412.04190v1 by Zeki Doruk Erden, Boi Faltings

Adaptive networks today rely on overparameterized fixed topologies that
cannot break through the statistical conflicts they encounter in the data they
are exposed to, and are prone to "catastrophic forgetting" as the network
attempts to reuse the existing structures to learn new task. We propose a
structural adaptation method, DIRAD, that can complexify as needed and in a
directed manner without being limited by statistical conflicts within a
dataset. We then extend this method and present the PREVAL framework, designed
to prevent "catastrophic forgetting" in continual learning by detection of new
data and assigning encountered data to suitable models adapted to process them,
without needing task labels anywhere in the workflow. We show the reliability
of the DIRAD in growing a network with high performance and orders-of-magnitude
simpler than fixed topology networks; and demonstrate the proof-of-concept
operation of PREVAL, in which continual adaptation to new tasks is observed
while being able to detect and discern previously-encountered tasks.

摘要：當今的自適應網路依賴於過度參數化的固定拓撲，無法突破在所接觸資料中遇到的統計衝突，且在網路嘗試重複使用現有結構來學習新任務時，容易發生「災難性遺忘」。我們提出結構適應方法 DIRAD，它可以在需要時以定向方式複雜化，而不會受到資料集中統計衝突的限制。然後，我們擴充此方法並提出 PREVAL 架構，旨在透過偵測新資料，並將遭遇的資料分配給適合處理它們的模型，在持續學習中防止「災難性遺忘」，而工作流程中任何地方都不需要任務標籤。我們展示了 DIRAD 在建立效能高且比固定拓撲網路簡單好幾個數量級的網路中的可靠性；並展示 PREVAL 的概念驗證操作，其中觀察到持續適應新任務，同時能夠偵測和辨別先前遭遇的任務。

##### **Leveraging Large Language Models to Generate Course-specific Semantically Annotated Learning Objects**
2412.04185v1 by Dominic Lohr, Marc Berges, Abhishek Chugh, Michael Kohlhase, Dennis Müller

Background: Over the past few decades, the process and methodology of
automated question generation (AQG) have undergone significant transformations.
Recent progress in generative natural language models has opened up new
potential in the generation of educational content.
  Objectives: This paper explores the potential of large language models (LLMs)
for generating computer science questions that are sufficiently annotated for
automatic learner model updates, are fully situated in the context of a
particular course, and address the cognitive dimension understand.
  Methods: Unlike previous attempts that might use basic methods like ChatGPT,
our approach involves more targeted strategies such as retrieval-augmented
generation (RAG) to produce contextually relevant and pedagogically meaningful
learning objects.
  Results and Conclusions: Our results show that generating structural,
semantic annotations works well. However, this success was not reflected in the
case of relational annotations. The quality of the generated questions often
did not meet educational standards, highlighting that although LLMs can
contribute to the pool of learning materials, their current level of
performance requires significant human intervention to refine and validate the
generated content.

摘要：背景：在過去的幾十年中，自動化問題生成 (AQG) 的流程和方法論已經歷了重大的轉變。生成式自然語言模型的最新進展為教育內容的生成開啟了新的潛力。
目標：本文探討了大型語言模型 (LLM) 的潛力，可用於生成足夠註解的電腦科學問題，以進行自動學習者模型更新，完全位於特定課程的背景中，並解決認知維度的理解。
方法：與可能使用 ChatGPT 等基本方法的先前嘗試不同，我們的做法涉及更具針對性的策略，例如檢索增強生成 (RAG)，以產生與上下文相關且具有教學意義的學習對象。
結果和結論：我們的結果表明，生成結構化、語義註解效果很好。然而，這種成功並沒有反映在關係註解的情況中。生成問題的品質通常不符合教育標準，這凸顯了儘管 LLM 可以為學習材料庫做出貢獻，但它們目前的效能水平需要大量人工干預來改善和驗證所生成的內容。

##### **Bench-CoE: a Framework for Collaboration of Experts from Benchmark**
2412.04167v1 by Yuanshuai Wang, Xingjian Zhang, Jinkun Zhao, Siwei Wen, Peilin Feng, Shuhao Liao, Lei Huang, Wenjun Wu

Large Language Models (LLMs) are key technologies driving intelligent systems
to handle multiple tasks. To meet the demands of various tasks, an increasing
number of LLMs-driven experts with diverse capabilities have been developed,
accompanied by corresponding benchmarks to evaluate their performance. This
paper proposes the Bench-CoE framework, which enables Collaboration of Experts
(CoE) by effectively leveraging benchmark evaluations to achieve optimal
performance across various tasks. Bench-CoE includes a set of expert models, a
router for assigning tasks to corresponding experts, and a benchmark dataset
for training the router. Moreover, we formulate Query-Level and Subject-Level
approaches based on our framework, and analyze the merits and drawbacks of
these two approaches. Finally, we conduct a series of experiments with vary
data distributions on both language and multimodal tasks to validate that our
proposed Bench-CoE outperforms any single model in terms of overall
performance. We hope this method serves as a baseline for further research in
this area. The code is available at
\url{https://github.com/ZhangXJ199/Bench-CoE}.

摘要：大型語言模型 (LLM) 是驅動智慧系統處理多項任務的關鍵技術。為了滿足各種任務的需求，已經開發出越來越多的具備多樣化功能的 LLM 驅動專家，並伴隨著對應的基準來評估其效能。本文提出了 Bench-CoE 框架，透過有效利用基準評估來達成專家協作 (CoE)，以在各種任務中達成最佳效能。Bench-CoE 包含一組專家模型、一個將任務分配給對應專家的路由器，以及一個用於訓練路由器的基準資料集。此外，我們根據我們的框架制定了查詢層級和主題層級的方法，並分析這兩種方法的優缺點。最後，我們對語言和多模態任務中的各種資料分佈進行了一系列實驗，以驗證我們提出的 Bench-CoE 在整體效能方面優於任何單一模型。我們希望此方法能作為此領域進一步研究的基準。程式碼可在
\url{https://github.com/ZhangXJ199/Bench-CoE} 取得。

##### **If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs**
2412.04144v1 by Muhammad Khalifa, Yi-Chern Tan, Arash Ahmadian, Tom Hosking, Honglak Lee, Lu Wang, Ahmet Üstün, Tom Sherborne, Matthias Gallé

Model merging has shown great promise at combining expert models, but the
benefit of merging is unclear when merging ``generalist'' models trained on
many tasks. We explore merging in the context of large ($\sim100$B) models, by
\textit{recycling} checkpoints that exhibit tradeoffs among different tasks.
Such checkpoints are often created in the process of developing a frontier
model, and many suboptimal ones are usually discarded. Given a pool of model
checkpoints obtained from different training runs (e.g., different stages,
objectives, hyperparameters, and data mixtures), which naturally show tradeoffs
across different language capabilities (e.g., instruction following vs. code
generation), we investigate whether merging can recycle such suboptimal models
into a Pareto-optimal one. Our optimization algorithm tunes the weight of each
checkpoint in a linear combination, resulting in a Pareto-optimal models that
outperforms both individual models and merge-based baselines. Further analysis
shows that good merges tend to include almost all checkpoints with with
non-zero weights, indicating that even seemingly bad initial checkpoints can
contribute to good final merges.

摘要：模型合并已在结合专家模型方面显示出巨大的前景，但当合并训练了许多任务的“通才”模型时，合并的好处尚不清楚。我们通过\textit{回收}在不同任务之间表现出权衡的检查点，探索在大（约 100B）模型的上下文中进行合并。此类检查点通常在开发前沿模型的过程中创建，许多次优检查点通常会被丢弃。给定从不同训练运行（例如，不同的阶段、目标、超参数和数据混合）获得的一组模型检查点，它们自然会显示不同语言能力之间的权衡（例如，指令遵循与代码生成），我们调查合并是否可以将此类次优模型回收成帕累托最优模型。我们的优化算法调整线性组合中每个检查点的权重，从而生成帕累托最优模型，该模型优于单个模型和基于合并的基准。进一步的分析表明，良好的合并往往包括几乎所有具有非零权重的检查点，这表明即使看似糟糕的初始检查点也可以促成良好的最终合并。

##### **Methodology for Online Estimation of Rheological Parameters in Polymer Melts Using Deep Learning and Microfluidics**
2412.04142v1 by Juan Sandubete-López, José L. Risco-Martín, Alexander H. McMillan, Eva Besada-Portas

Microfluidic devices are increasingly used in biological and chemical
experiments due to their cost-effectiveness for rheological estimation in
fluids. However, these devices often face challenges in terms of accuracy,
size, and cost. This study presents a methodology, integrating deep learning,
modeling and simulation to enhance the design of microfluidic systems, used to
develop an innovative approach for viscosity measurement of polymer melts. We
use synthetic data generated from the simulations to train a deep learning
model, which then identifies rheological parameters of polymer melts from
pressure drop and flow rate measurements in a microfluidic circuit, enabling
online estimation of fluid properties. By improving the accuracy and
flexibility of microfluidic rheological estimation, our methodology accelerates
the design and testing of microfluidic devices, reducing reliance on physical
prototypes, and offering significant contributions to the field.

摘要：微流體裝置由於其在流體流變估算中的成本效益，在生物和化學實驗中使用越來越廣泛。然而，這些裝置通常在準確性、尺寸和成本方面面臨挑戰。本研究提出了一種方法，整合深度學習、建模和模擬，以增強微流體系統的設計，用於開發聚合物熔體粘度測量的創新方法。我們使用從模擬中產生的合成數據來訓練深度學習模型，然後從微流體電路中的壓降和流速測量中識別聚合物熔體的流變參數，從而實現流體特性的在線估算。通過提高微流體流變估算的準確性和靈活性，我們的技術加速了微流體裝置的設計和測試，減少了對物理樣品的依賴，並為該領域做出了重大貢獻。

##### **Reducing Tool Hallucination via Reliability Alignment**
2412.04141v1 by Hongshen Xu, Su Zhu, Zihan Wang, Hang Zheng, Da Ma, Ruisheng Cao, Shuai Fan, Lu Chen, Kai Yu

Large Language Models (LLMs) have extended their capabilities beyond language
generation to interact with external systems through tool calling, offering
powerful potential for real-world applications. However, the phenomenon of tool
hallucinations, which occur when models improperly select or misuse tools,
presents critical challenges that can lead to flawed task execution and
increased operational costs. This paper investigates the concept of reliable
tool calling and highlights the necessity of addressing tool hallucinations. We
systematically categorize tool hallucinations into two main types: tool
selection hallucination and tool usage hallucination. To mitigate these issues,
we propose a reliability-focused alignment framework that enhances the model's
ability to accurately assess tool relevance and usage. By proposing a suite of
evaluation metrics and evaluating on StableToolBench, we further demonstrate
the effectiveness of our framework in mitigating tool hallucination and
improving the overall system reliability of LLM tool calling.

摘要：大型語言模型 (LLM) 已將其功能從語言生成擴展到透過工具呼叫與外部系統互動，為真實世界的應用提供了強大的潛力。然而，工具幻覺的現象，即模型不適當地選擇或誤用工具時發生，會造成重大的挑戰，可能導致任務執行有缺陷和營運成本增加。本文探討可靠工具呼叫的概念，並強調解決工具幻覺的必要性。我們系統性地將工具幻覺分類為兩種主要類型：工具選擇幻覺和工具使用幻覺。為了減輕這些問題，我們提出一個以可靠性為中心的對齊架構，以增強模型準確評估工具相關性和使用情況的能力。透過提出評估指標組並在 StableToolBench 上進行評估，我們進一步證明了我們的架構在減輕工具幻覺和改善 LLM 工具呼叫的整體系統可靠性方面的有效性。

##### **Understanding Memorization in Generative Models via Sharpness in Probability Landscapes**
2412.04140v1 by Dongjae Jeon, Dueun Kim, Albert No

In this paper, we introduce a geometric framework to analyze memorization in
diffusion models using the eigenvalues of the Hessian of the log probability
density. We propose that memorization arises from isolated points in the
learned probability distribution, characterized by sharpness in the probability
landscape, as indicated by large negative eigenvalues of the Hessian. Through
experiments on various datasets, we demonstrate that these eigenvalues
effectively detect and quantify memorization. Our approach provides a clear
understanding of memorization in diffusion models and lays the groundwork for
developing strategies to ensure secure and reliable generative models

摘要：在本文中，我們引入一個幾何框架，使用對數機率密度的 Hessian 矩陣特徵值來分析擴散模型中的記憶。我們提出記憶化來自於學習到的機率分佈中的孤立點，其特徵是機率景觀中的銳利度，如 Hessian 矩陣的大負特徵值所示。透過在各種資料集上的實驗，我們證明這些特徵值可以有效地偵測和量化記憶化。我們的做法提供對擴散模型中記憶化的清晰理解，並奠定制定策略以確保安全和可靠的生成模型的基礎

##### **Monet: Mixture of Monosemantic Experts for Transformers**
2412.04139v1 by Jungwoo Park, Young Jin Ahn, Kee-Eung Kim, Jaewoo Kang

Understanding the internal computations of large language models (LLMs) is
crucial for aligning them with human values and preventing undesirable
behaviors like toxic content generation. However, mechanistic interpretability
is hindered by polysemanticity -- where individual neurons respond to multiple,
unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to
disentangle these features through sparse dictionary learning, they have
compromised LLM performance due to reliance on post-hoc reconstruction loss. To
address this issue, we introduce Mixture of Monosemantic Experts for
Transformers (Monet) architecture, which incorporates sparse dictionary
learning directly into end-to-end Mixture-of-Experts pretraining. Our novel
expert decomposition method enables scaling the expert count to 262,144 per
layer while total parameters scale proportionally to the square root of the
number of experts. Our analyses demonstrate mutual exclusivity of knowledge
across experts and showcase the parametric knowledge encapsulated within
individual experts. Moreover, Monet allows knowledge manipulation over domains,
languages, and toxicity mitigation without degrading general performance. Our
pursuit of transparent LLMs highlights the potential of scaling expert counts
to enhance} mechanistic interpretability and directly resect the internal
knowledge to fundamentally adjust} model behavior. The source code and
pretrained checkpoints are available at https://github.com/dmis-lab/Monet.

摘要：了解大型语言模型 (LLM) 的内部计算对于将它们与人类价值观保持一致并防止不良行为（例如生成有毒内容）至关重要。然而，多义性阻碍了机械可解释性——其中单个神经元对多个不相关的概念做出反应。虽然稀疏自动编码器 (SAE) 已尝试通过稀疏字典学习来解开这些特征，但它们由于依赖于事后重建损失而损害了 LLM 性能。为了解决这个问题，我们引入了用于 Transformer（Monet）架构的单语义专家混合，它将稀疏字典学习直接纳入端到端专家混合预训练中。我们新颖的专家分解方法使专家数量能够扩展到每层 262,144 个，而总参数与专家数量的平方根成正比。我们的分析证明了专家之间知识的互斥性，并展示了单个专家中封装的参数知识。此外，Monet 允许跨领域、语言和毒性缓解操作知识，而不会降低整体性能。我们追求透明的 LLM 突出了扩展专家数量以增强机械可解释性的潜力，并直接调整内部知识以从根本上调整模型行为。源代码和预训练检查点可在 https://github.com/dmis-lab/Monet 获得。

##### **Text Change Detection in Multilingual Documents Using Image Comparison**
2412.04137v1 by Doyoung Park, Naresh Reddy Yarram, Sunjin Kim, Minkyu Kim, Seongho Cho, Taehee Lee

Document comparison typically relies on optical character recognition (OCR)
as its core technology. However, OCR requires the selection of appropriate
language models for each document and the performance of multilingual or hybrid
models remains limited. To overcome these challenges, we propose text change
detection (TCD) using an image comparison model tailored for multilingual
documents. Unlike OCR-based approaches, our method employs word-level text
image-to-image comparison to detect changes. Our model generates bidirectional
change segmentation maps between the source and target documents. To enhance
performance without requiring explicit text alignment or scaling preprocessing,
we employ correlations among multi-scale attention features. We also construct
a benchmark dataset comprising actual printed and scanned word pairs in various
languages to evaluate our model. We validate our approach using our benchmark
dataset and public benchmarks Distorted Document Images and the LRDE Document
Binarization Dataset. We compare our model against state-of-the-art semantic
segmentation and change detection models, as well as to conventional OCR-based
models.

摘要：文件比較通常依賴光學字元辨識 (OCR) 作為其核心技術。然而，OCR 需要為每個文件選擇適當的語言模型，而多語言或混合模型的效能仍然有限。為了克服這些挑戰，我們提出使用針對多語言文件量身打造的影像比較模型來進行文字變更偵測 (TCD)。與基於 OCR 的方法不同，我們的模型採用文字層級的文字影像對影像比較來偵測變更。我們的模型在原始文件和目標文件之間產生雙向變更分割圖。為了在不需要明確文字對齊或縮放前處理的情況下提升效能，我們採用多尺度注意特徵之間的關聯性。我們還建構了一個基準資料集，其中包含各種語言的實際印刷和掃描文字對，以評估我們的模型。我們使用我們的基準資料集和公開基準 Distorted Document Images 和 LRDE Document Binarization Dataset 來驗證我們的做法。我們將我們的模型與最先進的語意分割和變更偵測模型，以及傳統的基於 OCR 的模型進行比較。

##### **DeepFEA: Deep Learning for Prediction of Transient Finite Element Analysis Solutions**
2412.04121v1 by Georgios Triantafyllou, Panagiotis G. Kalozoumis, George Dimas, Dimitris K. Iakovidis

Finite Element Analysis (FEA) is a powerful but computationally intensive
method for simulating physical phenomena. Recent advancements in machine
learning have led to surrogate models capable of accelerating FEA. Yet there
are still limitations in developing surrogates of transient FEA models that can
simultaneously predict the solutions for both nodes and elements with
applicability on both the 2D and 3D domains. Motivated by this research gap,
this study proposes DeepFEA, a deep learning-based framework that leverages a
multilayer Convolutional Long Short-Term Memory (ConvLSTM) network branching
into two parallel convolutional neural networks to predict the solutions for
both nodes and elements of FEA models. The proposed network is optimized using
a novel adaptive learning algorithm, called Node-Element Loss Optimization
(NELO). NELO minimizes the error occurring at both branches of the network
enabling the prediction of solutions for transient FEA simulations. The
experimental evaluation of DeepFEA is performed on three datasets in the
context of structural mechanics, generated to serve as publicly available
reference datasets. The results show that DeepFEA can achieve less than 3%
normalized mean and root mean squared error for 2D and 3D simulation scenarios,
and inference times that are two orders of magnitude faster than FEA. In
contrast, relevant state-of-the-art methods face challenges with
multi-dimensional output and dynamic input prediction. Furthermore, DeepFEA's
robustness was demonstrated in a real-life biomedical scenario, confirming its
suitability for accurate and efficient predictions of FEA simulations.

摘要：有限元分析 (FEA) 是一種強大但計算密集的方法，用於模擬物理現象。機器學習的最新進展導致了能夠加速 FEA 的代理模型。然而，仍存在開發瞬態 FEA 模型代理的限制，這些模型可以同時預測 2D 和 3D 域中節點和單元的解。受這一研究差距的啟發，本研究提出了 DeepFEA，這是一個基於深度學習的框架，利用多層卷積長短期記憶 (ConvLSTM) 網路分支到兩個並行的卷積神經網路，以預測 FEA 模型的節點和單元解。所提出的網路使用一種稱為節點元素損失優化 (NELO) 的新型自適應學習演算法進行優化。NELO 最小化網路兩個分支發生的錯誤，從而能夠預測瞬態 FEA 模擬的解。DeepFEA 的實驗評估是在結構力學的背景下對三個資料集進行的，這些資料集生成的目的是作為公開可用的參考資料集。結果表明，DeepFEA 可以實現 2D 和 3D 模擬場景的歸一化平均值和均方根誤差小於 3%，並且推理時間比 FEA 快兩個數量級。相比之下，相關的最新方法面臨多維輸出和動態輸入預測的挑戰。此外，DeepFEA 的穩健性在真實的生物醫學場景中得到證明，證實了其適用於準確有效地預測 FEA 模擬。

##### **GRAF: Graph Retrieval Augmented by Facts for Legal Question Answering**
2412.04119v1 by Cristian-George Crăciun, Răzvan-Alexandru Smădu, Dumitru-Clementin Cercel, Mihaela-Claudia Cercel

Pre-trained Language Models (PLMs) have shown remarkable performances in
recent years, setting a new paradigm for NLP research and industry. The legal
domain has received some attention from the NLP community partly due to its
textual nature. Some tasks from this domain are represented by
question-answering (QA) tasks. This work explores the legal domain
Multiple-Choice QA (MCQA) for a low-resource language. The contribution of this
work is multi-fold. We first introduce JuRO, the first openly available
Romanian legal MCQA dataset, comprising three different examinations and a
number of 10,836 total questions. Along with this dataset, we introduce CROL,
an organized corpus of laws that has a total of 93 distinct documents with
their modifications from 763 time spans, that we leveraged in this work for
Information Retrieval (IR) techniques. Moreover, we are the first to propose
Law-RoG, a Knowledge Graph (KG) for the Romanian language, and this KG is
derived from the aforementioned corpus. Lastly, we propose a novel approach for
MCQA, Graph Retrieval Augmented by Facts (GRAF), which achieves competitive
results with generally accepted SOTA methods and even exceeds them in most
settings.

摘要：<paragraph>預訓練語言模型 (PLM) 在近年來展現出卓越的效能，為自然語言處理的研究和產業樹立了新的典範。法律領域因為其文本性質而受到自然語言處理社群的部分關注。此領域中的一些任務由問答 (QA) 任務表示。這項工作探索了低資源語言的法律領域多重選擇問答 (MCQA)。這項工作的貢獻是多方面的。我們首先介紹 JuRO，這是第一個公開的羅馬尼亞法律 MCQA 資料集，包含三次不同的考試和總共 10,836 個問題。除了這個資料集之外，我們還介紹了 CROL，這是一個有組織的法律語料庫，總共有 93 個不同的文件，包含了來自 763 個時間區間的修改，我們在這個工作中利用它來進行資訊檢索 (IR) 技術。此外，我們是第一個提出 Law-RoG 的人，這是一個羅馬尼亞語的知識圖譜 (KG)，而這個 KG 是從上述語料庫衍生的。最後，我們提出了一個新的多重選擇問答方法，由事實增強的圖形檢索 (GRAF)，它在一般公認的 SOTA 方法中獲得了有競爭力的結果，甚至在大多數設定中都超越了它們。</paragraph>

##### **Thermal and RGB Images Work Better Together in Wind Turbine Damage Detection**
2412.04114v1 by Serhii Svystun, Oleksandr Melnychenko, Pavlo Radiuk, Oleg Savenko, Anatoliy Sachenko, Andrii Lysyi

The inspection of wind turbine blades (WTBs) is crucial for ensuring their
structural integrity and operational efficiency. Traditional inspection methods
can be dangerous and inefficient, prompting the use of unmanned aerial vehicles
(UAVs) that access hard-to-reach areas and capture high-resolution imagery. In
this study, we address the challenge of enhancing defect detection on WTBs by
integrating thermal and RGB images obtained from UAVs. We propose a
multispectral image composition method that combines thermal and RGB imagery
through spatial coordinate transformation, key point detection, binary
descriptor creation, and weighted image overlay. Using a benchmark dataset of
WTB images annotated for defects, we evaluated several state-of-the-art object
detection models. Our results show that composite images significantly improve
defect detection efficiency. Specifically, the YOLOv8 model's accuracy
increased from 91% to 95%, precision from 89% to 94%, recall from 85% to 92%,
and F1-score from 87% to 93%. The number of false positives decreased from 6 to
3, and missed defects reduced from 5 to 2. These findings demonstrate that
integrating thermal and RGB imagery enhances defect detection on WTBs,
contributing to improved maintenance and reliability.

摘要：風力渦輪機葉片 (WTB) 的檢查對於確保其結構完整性和運作效率至關重要。傳統的檢查方法可能既危險又低效，促使使用無人機 (UAV) 來進入難以到達的地區並擷取高解析度影像。在本研究中，我們透過整合從無人機取得的熱影像和 RGB 影像來解決增強 WTB 缺陷檢測的挑戰。我們提出了一種多光譜影像合成方法，透過空間座標轉換、關鍵點偵測、二進位描述子建立和加權影像疊加來結合熱影像和 RGB 影像。使用標記有缺陷的 WTB 影像基準資料集，我們評估了幾種最先進的物件偵測模型。我們的結果顯示，合成影像顯著提升了缺陷檢測效率。具體來說，YOLOv8 模型的準確度從 91% 提升到 95%，精確度從 89% 提升到 94%，召回率從 85% 提升到 92%，F1 分數從 87% 提升到 93%。假正例的數量從 6 個減少到 3 個，遺漏的缺陷從 5 個減少到 2 個。這些發現證明了整合熱影像和 RGB 影像可以增強 WTB 上的缺陷檢測，有助於改善維護和可靠性。

##### **Enhancing Mathematical Reasoning in LLMs with Background Operators**
2412.04110v1 by Jiajun Chen, Yik-Cheung Tam

We propose utilizing background operators for mathematical reasoning in large
language models (LLMs). To achieve this, we define a set of fundamental
mathematical predicates as the basic building blocks. For each mathematical
problem, we develop a Prolog solution that includes problem-specific predicates
and intermediate predicates derived from these background operators, ensuring
that each solution adheres to the defined operator set. We introduce the
MATH-Prolog corpus, which is derived from the counting and probability
categories of the MATH corpus. For efficient data augmentation, we apply K-fold
cross-validated self-training. This method incrementally generates new Prolog
solutions for each fold, incorporating those verified as correct into the
training set throughout the model training process. Our experimental results
demonstrate that 5-fold crossvalidated self-training effectively identifies
new, accurate Prolog solutions, achieving an accuracy of 84.6% on the
cross-validated set, and 84.8% on the test set during fine-tuning the
Meta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new
solutions with fully computable inference steps for previously unseen problems.
Additionally, incorporating the background mathematical predicates into the
prompt enhances solution coverage.

摘要：我們建議利用背景運算子在大型語言模型 (LLM) 中進行數學推理。為此，我們將一組基本數學謂詞定義為基本構建區塊。對於每個數學問題，我們開發一個 Prolog 解決方案，其中包含特定於問題的謂詞和從這些背景運算子衍生的中間謂詞，確保每個解決方案都遵守定義的運算子集。我們引入了 MATH-Prolog 語料庫，它來自 MATH 語料庫的計數和機率類別。對於有效資料擴充，我們應用 K 折交叉驗證自我訓練。此方法為每個折疊遞增產生新的 Prolog 解決方案，將驗證為正確的解決方案納入整個模型訓練過程中的訓練集。我們的實驗結果表明，5 折交叉驗證自我訓練有效地識別出新的、準確的 Prolog 解決方案，在微調 Meta-Llama-3.1-8B-Instruct 模型時，在交叉驗證集上達到 84.6% 的準確率，在測試集上達到 84.8%。這種方法成功地為以前未見的問題揭示了具有完全可計算推理步驟的新解決方案。此外，將背景數學謂詞納入提示中會增強解決方案涵蓋範圍。

##### **Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models**
2412.04107v1 by Yuhao Wang, Junwei Pan, Xiangyu Zhao, Pengyue Jia, Wanyu Wang, Yuan Wang, Yue Liu, Dapeng Liu, Jie Jiang

Sequential recommendation (SR) aims to model the sequential dependencies in
users' historical interactions to better capture their evolving interests.
However, existing SR approaches primarily rely on collaborative data, which
leads to limitations such as the cold-start problem and sub-optimal
performance. Meanwhile, despite the success of large language models (LLMs),
their application in industrial recommender systems is hindered by high
inference latency, inability to capture all distribution statistics, and
catastrophic forgetting. To this end, we propose a novel Pre-train, Align, and
Disentangle (PAD) paradigm to empower recommendation models with LLMs.
Specifically, we first pre-train both the SR and LLM models to get
collaborative and textual embeddings. Next, a characteristic
recommendation-anchored alignment loss is proposed using multi-kernel maximum
mean discrepancy with Gaussian kernels. Finally, a triple-experts architecture,
consisting aligned and modality-specific experts with disentangled embeddings,
is fine-tuned in a frequency-aware manner. Experiments conducted on three
public datasets demonstrate the effectiveness of PAD, showing significant
improvements and compatibility with various SR backbone models, especially on
cold items. The implementation code and datasets will be publicly available.

摘要：序列推薦 (SR) 的目標是模擬使用者歷史互動中的順序依賴性，以更好地捕捉他們不斷變化的興趣。
然而，現有的 SR 方法主要依賴協作資料，這會導致冷啟動問題和次佳效能等限制。同時，儘管大型語言模型 (LLM) 很成功，但它們在產業推薦系統中的應用受到高推論延遲、無法捕捉所有分佈統計資料和災難性遺忘的阻礙。為此，我們提出一個新的預訓練、對齊和解開 (PAD) 範例，以 LLM 賦能推薦模型。
具體來說，我們首先預訓練 SR 和 LLM 模型，以取得協作和文字嵌入。接下來，建議使用具有高斯核的多核最大平均差異，提出一個特徵推薦錨定對齊損失。最後，一個由對齊和具有解開嵌入的特定模式專家組成的三重專家架構，以頻率感知的方式進行微調。在三個公開資料集上進行的實驗證明了 PAD 的有效性，顯示出顯著的改進，並與各種 SR 主幹模型相容，特別是在冷門項目上。實作程式碼和資料集將公開提供。

##### **Practical Considerations for Agentic LLM Systems**
2412.04093v1 by Chris Sypherd, Vaishak Belle

As the strength of Large Language Models (LLMs) has grown over recent years,
so too has interest in their use as the underlying models for autonomous
agents. Although LLMs demonstrate emergent abilities and broad expertise across
natural language domains, their inherent unpredictability makes the
implementation of LLM agents challenging, resulting in a gap between related
research and the real-world implementation of such systems. To bridge this gap,
this paper frames actionable insights and considerations from the research
community in the context of established application paradigms to enable the
construction and facilitate the informed deployment of robust LLM agents.
Namely, we position relevant research findings into four broad
categories--Planning, Memory, Tools, and Control Flow--based on common
practices in application-focused literature and highlight practical
considerations to make when designing agentic LLMs for real-world applications,
such as handling stochasticity and managing resources efficiently. While we do
not conduct empirical evaluations, we do provide the necessary background for
discussing critical aspects of agentic LLM designs, both in academia and
industry.

摘要：隨著大型語言模型 (LLM) 的實力在近年來大幅提升，
將其用作自主代理人的基礎模型也備受關注。儘管 LLM 在自然語言領域展現出新興的能力和廣泛的專業知識，但其固有的不可預測性使得 LLM 代理人的實作充滿挑戰，導致相關研究與此類系統的實際實作之間出現差距。為了彌補這個差距，本文在既定的應用範例脈絡中，架構出可行的見解和研究社群的考量，以利建構和促進穩健 LLM 代理人的明智部署。具體來說，我們根據應用為重點的文獻中的常見做法，將相關的研究發現定位在四個廣泛的類別中，分別為規劃、記憶、工具和控制流程，並重點說明在為實際應用設計代理 LLM 時應考量的實務考量，例如處理隨機性和有效管理資源。儘管我們未進行經驗評估，但我們確實提供了必要的背景，以便討論代理 LLM 設計中的關鍵面向，無論是在學術界或業界皆然。

##### **GEITje 7B Ultra: A Conversational Model for Dutch**
2412.04092v1 by Bram Vanroy

Language models have rapidly evolved, predominantly focusing on English while
often neglecting extensive pretraining in other languages. This approach has
required initiatives to adapt powerful, English-centric models to other
linguistic contexts through finetuning. For Dutch, such a recent endeavour is
``GEITje'' a model originally derived from the English-based Mistral 7B.
Building on this fundamental work, the current research extends the
capabilities of GEITje by supervised finetuning on newly created high-quality
synthetic conversational datasets, along with an additional preference
alignment procedure on a synthetic feedback dataset. Both the developed models
and the created datasets are openly available.

摘要：語言模型快速演進，主要專注於英語，但經常忽略在其他語言進行廣泛的預訓練。此方法需要透過微調，將強大且以英語為中心的模型調整至其他語言環境。對於荷蘭語，最近的一項努力是 ``GEITje''，一個最初源自以英語為基礎的 Mistral 7B 的模型。在此基礎工作上，目前的研究透過在最新建立的高品質合成對話資料集上進行監督式微調，以及在合成回饋資料集上進行額外的偏好比對程序，來擴展 GEITje 的功能。開發的模型和建立的資料集皆為開放取得。

##### **BodyMetric: Evaluating the Realism of HumanBodies in Text-to-Image Generation**
2412.04086v1 by Nefeli Andreou, Varsha Vivek, Ying Wang, Alex Vorobiov, Tiffany Deng, Raja Bala, Larry Davis, Betty Mohler Tesch

Accurately generating images of human bodies from text remains a challenging
problem for state of the art text-to-image models. Commonly observed
body-related artifacts include extra or missing limbs, unrealistic poses,
blurred body parts, etc. Currently, evaluation of such artifacts relies heavily
on time-consuming human judgments, limiting the ability to benchmark models at
scale. We address this by proposing BodyMetric, a learnable metric that
predicts body realism in images. BodyMetric is trained on realism labels and
multi-modal signals including 3D body representations inferred from the input
image, and textual descriptions. In order to facilitate this approach, we
design an annotation pipeline to collect expert ratings on human body realism
leading to a new dataset for this task, namely, BodyRealism. Ablation studies
support our architectural choices for BodyMetric and the importance of
leveraging a 3D human body prior in capturing body-related artifacts in 2D
images. In comparison to concurrent metrics which evaluate general user
preference in images, BodyMetric specifically reflects body-related artifacts.
We demonstrate the utility of BodyMetric through applications that were
previously infeasible at scale. In particular, we use BodyMetric to benchmark
the generation ability of text-to-image models to produce realistic human
bodies. We also demonstrate the effectiveness of BodyMetric in ranking
generated images based on the predicted realism scores.

摘要：<paragraph>從文字準確生成人體影像對現今的文字轉影像模型來說仍是一項具有挑戰性的問題。常見的身體相關人工製品包括多餘或缺少的肢體、不切實際的姿勢、模糊的身體部位等。目前，此類人工製品的評估仰賴耗時的個人判斷，限制了大規模評估模型的能力。我們提出 BodyMetric 來解決這個問題，這是一個可學習的指標，可以預測影像中人體的真實性。BodyMetric 是在真實性標籤和多模式訊號上訓練的，包括從輸入影像推論出的 3D 身體表示和文字描述。為了促進此方法，我們設計了一個註解管道來收集人體真實性的專家評分，從而產生一個新的資料集，也就是 BodyRealism。消融研究支持我們在 BodyMetric 中的架構選擇，以及在捕捉 2D 影像中與身體相關的人工製品時，利用 3D 人體先驗的重要性。與評估影像中一般使用者偏好的同時指標相比，BodyMetric 特別反映了與身體相關的人工製品。我們透過以前無法大規模執行的應用程式，展示了 BodyMetric 的效用。特別是，我們使用 BodyMetric 來評估文字轉影像模型產生逼真人體的能力。我們也展示了 BodyMetric 在根據預測的真實性分數對生成影像進行排名的有效性。</paragraph>

##### **Federated Learning in Mobile Networks: A Comprehensive Case Study on Traffic Forecasting**
2412.04081v1 by Nikolaos Pavlidis, Vasileios Perifanis, Selim F. Yilmaz, Francesc Wilhelmi, Marco Miozzo, Pavlos S. Efraimidis, Remous-Aris Koutsiamanis, Pavol Mulinka, Paolo Dini

The increasing demand for efficient resource allocation in mobile networks
has catalyzed the exploration of innovative solutions that could enhance the
task of real-time cellular traffic prediction. Under these circumstances,
federated learning (FL) stands out as a distributed and privacy-preserving
solution to foster collaboration among different sites, thus enabling
responsive near-the-edge solutions. In this paper, we comprehensively study the
potential benefits of FL in telecommunications through a case study on
federated traffic forecasting using real-world data from base stations (BSs) in
Barcelona (Spain). Our study encompasses relevant aspects within the federated
experience, including model aggregation techniques, outlier management, the
impact of individual clients, personalized learning, and the integration of
exogenous sources of data. The performed evaluation is based on both prediction
accuracy and sustainability, thus showcasing the environmental impact of
employed FL algorithms in various settings. The findings from our study
highlight FL as a promising and robust solution for mobile traffic prediction,
emphasizing its twin merits as a privacy-conscious and environmentally
sustainable approach, while also demonstrating its capability to overcome data
heterogeneity and ensure high-quality predictions, marking a significant stride
towards its integration in mobile traffic management systems.

摘要：隨著行動網路中對資源有效配置需求的增加，促使探索創新的解決方案，以增強即時行動通訊流量預測的任務。在這些情況下，聯合學習 (FL) 成為一種分散且保護隱私的解決方案，以促進不同站點之間的協作，從而實現近邊緣的回應式解決方案。在本文中，我們透過一個案例研究全面探討了 FL 在電信中的潛在優點，該案例研究使用巴塞隆納 (西班牙) 基地台 (BS) 的真實世界資料進行聯合流量預測。我們的研究涵蓋了聯合體驗中的相關方面，包括模型聚合技術、異常值管理、個別用戶的影響、個人化學習以及外生資料來源的整合。執行評估的依據是預測準確度和永續性，從而展示了在各種設定中所使用的 FL 演算法的環境影響。我們研究的發現強調了 FL 作為行動流量預測的強大且穩健的解決方案，強調其作為注重隱私和環境永續方法的雙重優點，同時也展示了它克服資料異質性並確保高品質預測的能力，標誌著朝著將其整合至行動流量管理系統邁出了重要一步。

##### **Does your model understand genes? A benchmark of gene properties for biological and text models**
2412.04075v1 by Yoav Kan-Tor, Michael Morris Danziger, Eden Zohar, Matan Ninio, Yishai Shimoni

The application of deep learning methods, particularly foundation models, in
biological research has surged in recent years. These models can be text-based
or trained on underlying biological data, especially omics data of various
types. However, comparing the performance of these models consistently has
proven to be a challenge due to differences in training data and downstream
tasks. To tackle this problem, we developed an architecture-agnostic
benchmarking approach that, instead of evaluating the models directly,
leverages entity representation vectors from each model and trains simple
predictive models for each benchmarking task. This ensures that all types of
models are evaluated using the same input and output types. Here we focus on
gene properties collected from professionally curated bioinformatics databases.
These gene properties are categorized into five major groups: genomic
properties, regulatory functions, localization, biological processes, and
protein properties. Overall, we define hundreds of tasks based on these
databases, which include binary, multi-label, and multi-class classification
tasks. We apply these benchmark tasks to evaluate expression-based models,
large language models, protein language models, DNA-based models, and
traditional baselines. Our findings suggest that text-based models and protein
language models generally outperform expression-based models in genomic
properties and regulatory functions tasks, whereas expression-based models
demonstrate superior performance in localization tasks. These results should
aid in the development of more informed artificial intelligence strategies for
biological understanding and therapeutic discovery. To ensure the
reproducibility and transparency of our findings, we have made the source code
and benchmark data publicly accessible for further investigation and expansion
at github.com/BiomedSciAI/gene-benchmark.

摘要：<paragraph>近年来，深度学习方法，尤其是基础模型，在生物研究中的应用激增。这些模型可以基于文本，也可以在底层生物数据上进行训练，尤其是各种类型的组学数据。然而，由于训练数据和下游任务的差异，对这些模型的性能进行一致的比较已被证明是一项挑战。为了解决这个问题，我们开发了一种与架构无关的基准测试方法，该方法不是直接评估模型，而是利用每个模型中的实体表示向量并为每个基准测试任务训练简单的预测模型。这确保了使用相同的输入和输出类型来评估所有类型的模型。在这里，我们重点关注从专业策划的生物信息学数据库中收集的基因特性。这些基因特性被分为五类：基因组特性、调控功能、定位、生物过程和蛋白质特性。总体而言，我们基于这些数据库定义了数百个任务，其中包括二元、多标签和多类分类任务。我们将这些基准任务应用于基于表达的模型、大型语言模型、蛋白质语言模型、基于 DNA 的模型和传统基线。我们的研究结果表明，基于文本的模型和蛋白质语言模型通常在基因组特性和调控功能任务中优于基于表达的模型，而基于表达的模型在定位任务中表现出优越的性能。这些结果应有助于开发更明智的人工智能策略，以用于生物学理解和治疗发现。为了确保我们研究结果的可重复性和透明性，我们已将源代码和基准数据公开，以便在 github.com/BiomedSciAI/gene-benchmark 上进行进一步调查和扩展。</paragraph>

##### **ProtDAT: A Unified Framework for Protein Sequence Design from Any Protein Text Description**
2412.04069v1 by Xiao-Yu Guo, Yi-Fan Li, Yuan Liu, Xiaoyong Pan, Hong-Bin Shen

Protein design has become a critical method in advancing significant
potential for various applications such as drug development and enzyme
engineering. However, protein design methods utilizing large language models
with solely pretraining and fine-tuning struggle to capture relationships in
multi-modal protein data. To address this, we propose ProtDAT, a de novo
fine-grained framework capable of designing proteins from any descriptive
protein text input. ProtDAT builds upon the inherent characteristics of protein
data to unify sequences and text as a cohesive whole rather than separate
entities. It leverages an innovative multi-modal cross-attention, integrating
protein sequences and textual information for a foundational level and seamless
integration. Experimental results demonstrate that ProtDAT achieves the
state-of-the-art performance in protein sequence generation, excelling in
rationality, functionality, structural similarity, and validity. On 20,000
text-sequence pairs from Swiss-Prot, it improves pLDDT by 6%, TM-score by 0.26,
and reduces RMSD by 1.2 {\AA}, highlighting its potential to advance protein
design.

摘要：蛋白設計已成為推進各種應用（如藥物開發和酵素工程）的重大潛力的關鍵方法。然而，利用大型語言模型、僅使用預訓練和微調的蛋白設計方法難以捕捉多模式蛋白數據中的關係。為了解決這個問題，我們提出了 ProtDAT，一個能夠根據任何描述性蛋白文本輸入設計蛋白質的全新細粒度框架。ProtDAT 建構在蛋白質數據的內在特性之上，將序列和文本統一為一個整體，而不是分開的實體。它利用創新的多模式交叉注意力，整合蛋白質序列和文本資訊，作為基礎層級和無縫整合。實驗結果表明，ProtDAT 在蛋白質序列生成中實現了最先進的效能，在合理性、功能性、結構相似性和有效性方面表現出色。在 Swiss-Prot 中的 20,000 個文本序列對中，它將 pLDDT 提高了 6%，TM 分數提高了 0.26，並將 RMSD 降低了 1.2 {\AA}，突顯了它在推進蛋白質設計方面的潛力。

##### **Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning**
2412.04067v1 by Amnon Bleich, Antje Linnemann, Bjoern H. Diem, Tim OF Conrad

Recent advances in deep learning and natural language generation have
significantly improved image captioning, enabling automated, human-like
descriptions for visual content. In this work, we apply these captioning
techniques to generate clinician-like interpretations of ECG data. This study
leverages existing ECG datasets accompanied by free-text reports authored by
healthcare professionals (HCPs) as training data. These reports, while often
inconsistent, provide a valuable foundation for automated learning. We
introduce an encoder-decoder-based method that uses these reports to train
models to generate detailed descriptions of ECG episodes. This represents a
significant advancement in ECG analysis automation, with potential applications
in zero-shot classification and automated clinical decision support.
  The model is tested on various datasets, including both 1- and 12-lead ECGs.
It significantly outperforms the state-of-the-art reference model by Qiu et
al., achieving a METEOR score of 55.53% compared to 24.51% achieved by the
reference model. Furthermore, several key design choices are discussed,
providing a comprehensive overview of current challenges and innovations in
this domain.
  The source codes for this research are publicly available in our Git
repository https://git.zib.de/ableich/ecg-comment-generation-public

摘要：深度學習和自然語言生成技術的最新進展顯著改善了影像標題，能為視覺內容提供自動化的人類語言描述。在這項工作中，我們將這些標題技術應用於產生類似臨床醫師對心電圖資料的詮釋。這項研究利用既有的心電圖資料集，並附上由醫療保健專業人員 (HCP) 撰寫的自由文字報告作為訓練資料。這些報告雖然常常不一致，但為自動化學習提供了有價值的基礎。我們引入了一個編碼器-解碼器方法，使用這些報告來訓練模型，以產生心電圖事件的詳細描述。這代表心電圖分析自動化的重大進展，在零次學習分類和自動化臨床決策支援中具有潛在應用。此模型在各種資料集上進行測試，包括 1 導程和 12 導程心電圖。它明顯優於邱等人的現有最佳參考模型，與參考模型達成的 24.51% 相比，達到了 55.53% 的 METEOR 分數。此外，討論了幾個關鍵的設計選擇，提供了對這個領域中當前挑戰和創新的全面概述。此研究的原始程式碼在我們的 Git 儲存庫 https://git.zib.de/ableich/ecg-comment-generation-public 中公開。

##### **Graph Neural Networks Need Cluster-Normalize-Activate Modules**
2412.04064v1 by Arseny Skryagin, Felix Divo, Mohammad Amin Ali, Devendra Singh Dhami, Kristian Kersting

Graph Neural Networks (GNNs) are non-Euclidean deep learning models for
graph-structured data. Despite their successful and diverse applications,
oversmoothing prohibits deep architectures due to node features converging to a
single fixed point. This severely limits their potential to solve complex
tasks. To counteract this tendency, we propose a plug-and-play module
consisting of three steps: Cluster-Normalize-Activate (CNA). By applying CNA
modules, GNNs search and form super nodes in each layer, which are normalized
and activated individually. We demonstrate in node classification and property
prediction tasks that CNA significantly improves the accuracy over the
state-of-the-art. Particularly, CNA reaches 94.18% and 95.75% accuracy on Cora
and CiteSeer, respectively. It further benefits GNNs in regression tasks as
well, reducing the mean squared error compared to all baselines. At the same
time, GNNs with CNA require substantially fewer learnable parameters than
competing architectures.

摘要：圖形神經網路 (GNN) 是非歐幾里得深度學習模型，用於圖形結構化資料。儘管它們有成功且多元的應用，過度平滑會因為節點特徵收斂到單一固定點而禁止深度架構。這嚴重限制了它們解決複雜任務的潛力。為了對抗這種趨勢，我們提出了一個包含三個步驟的即插即用模組：群集-正規化-啟動 (CNA)。透過套用 CNA 模組，GNN 會在每一層搜尋並形成超級節點，這些超級節點會個別正規化並啟動。我們在節點分類和屬性預測任務中展示，CNA 大幅提升了最先進技術的準確度。特別是，CNA 分別在 Cora 和 CiteSeer 上達到了 94.18% 和 95.75% 的準確度。它也進一步讓 GNN 受益於回歸任務，與所有基線相比減少了平均平方誤差。同時，具有 CNA 的 GNN 所需的可學習參數比競爭架構少得多。

##### **ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality**
2412.04062v1 by Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou, Kaipeng Zhang, Bohan Zhuang

In this paper, we propose ZipAR, a training-free, plug-and-play parallel
decoding framework for accelerating auto-regressive (AR) visual generation. The
motivation stems from the observation that images exhibit local structures, and
spatially distant regions tend to have minimal interdependence. Given a
partially decoded set of visual tokens, in addition to the original next-token
prediction scheme in the row dimension, the tokens corresponding to spatially
adjacent regions in the column dimension can be decoded in parallel, enabling
the ``next-set prediction'' paradigm. By decoding multiple tokens
simultaneously in a single forward pass, the number of forward passes required
to generate an image is significantly reduced, resulting in a substantial
improvement in generation efficiency. Experiments demonstrate that ZipAR can
reduce the number of model forward passes by up to 91% on the Emu3-Gen model
without requiring any additional retraining.

摘要：在本文中，我們提出 ZipAR，一個無需訓練、即插即用的平行
解碼框架，用於加速自迴歸 (AR) 視覺生成。這個
動機源自於觀察到影像展現出局部結構，且
空間上相距較遠的區域往往有最小的相互依賴性。給定一個
部分解碼的視覺符號集，除了在列維度中原始的下一符號
預測方案外，對應於列維度中空間相鄰區域的符號可以平行解碼，從而
實現「下一組預測」範例。透過在單一前向傳遞中同時解碼多個符號，
生成影像所需的前向傳遞次數大幅減少，進而大幅
提升生成效率。實驗證明 ZipAR 可以將 Emu3-Gen 模型的模型前向傳遞次數
減少多達 91%，且不需要任何額外的重新訓練。

##### **Expanding Deep Learning-based Sensing Systems with Multi-Source Knowledge Transfer**
2412.04060v1 by Gaole Dai, Huatao Xu, Rui Tan, Mo Li

Expanding the existing sensing systems to provide high-quality deep learning
models for more domains, such as new users or environments, is challenged by
the limited labeled data and the data and device heterogeneities. While
knowledge distillation methods could overcome label scarcity and device
heterogeneity, they assume the teachers are fully reliable and overlook the
data heterogeneity, which prevents the direct adoption of existing models. To
address this problem, this paper proposes an efficient knowledge transfer
framework, HaKT, to expand sensing systems. It first selects multiple
high-quality models from the system at a low cost and then fuses their
knowledge by assigning sample-wise weights to their predictions. Later, the
fused knowledge is selectively injected into the customized models for new
domains based on the knowledge quality. Extensive experiments on different
tasks, modalities, and settings show that HaKT outperforms stat-of-the-art
baselines by at most 16.5% accuracy and saves up to 39% communication traffic.

摘要：擴展現有感測系統，以提供更多領域（例如新使用者或環境）的高品質深度學習模型，受到標籤資料有限以及資料和裝置異質性的挑戰。雖然知識蒸餾方法可以克服標籤稀少性和裝置異質性，但它們假設教師完全可靠，並忽略資料異質性，這會妨礙直接採用現有模型。為了解決這個問題，本文提出了一個有效的知識傳輸架構 HaKT，以擴展感測系統。它首先從系統中低成本地選擇多個高品質模型，然後透過為其預測指派樣本權重來融合其知識。稍後，基於知識品質，將融合的知識選擇性地注入到新領域的客製化模型中。在不同任務、方式和設定上的大量實驗表明，HaKT 的準確度比最先進的基準高出 16.5%，並節省了高達 39% 的通訊流量。

##### **From Code to Play: Benchmarking Program Search for Games Using Large Language Models**
2412.04057v1 by Manuel Eberhardinger, James Goodman, Alexander Dockhorn, Diego Perez-Liebana, Raluca D. Gaina, Duygu Çakmak, Setareh Maghsudi, Simon Lucas

Large language models (LLMs) have shown impressive capabilities in generating
program code, opening exciting opportunities for applying program synthesis to
games. In this work, we explore the potential of LLMs to directly synthesize
usable code for a wide range of gaming applications, focusing on two
programming languages, Python and Java. We use an evolutionary hill-climbing
algorithm, where the mutations and seeds of the initial programs are controlled
by LLMs. For Python, the framework covers various game-related tasks, including
five miniature versions of Atari games, ten levels of Baba is You, an
environment inspired by Asteroids, and a maze generation task. For Java, the
framework contains 12 games from the TAG tabletop games framework. Across 29
tasks, we evaluated 12 language models for Python and 8 for Java. Our findings
suggest that the performance of LLMs depends more on the task than on model
size. While larger models generate more executable programs, these do not
always result in higher-quality solutions but are much more expensive. No model
has a clear advantage, although on any specific task, one model may be better.
Trying many models on a problem and using the best results across them is more
reliable than using just one.

摘要：大型語言模型 (LLM) 在產生程式碼方面展現了令人印象深刻的能力，為將程式合成應用於遊戲開啟了令人興奮的機會。在這項工作中，我們探討了 LLM 直接合成可用於各種遊戲應用程式的程式碼的潛力，重點放在兩種程式語言：Python 和 Java。我們使用一種演化式的爬山演算法，其中初始程式的突變和種子由 LLM 控制。對於 Python，這個架構涵蓋了各種與遊戲相關的任務，包括五個微型版本的 Atari 遊戲、十個 Baba is You 的關卡、一個受小行星啟發的環境，以及一個迷宮生成任務。對於 Java，這個架構包含了來自 TAG 桌上遊戲架構的 12 個遊戲。在 29 個任務中，我們評估了 12 個針對 Python 的語言模型和 8 個針對 Java 的語言模型。我們的發現表明，LLM 的效能取決於任務，而非模型大小。雖然較大的模型會產生更多可執行的程式，但這些程式並非總是能產生更高品質的解決方案，而且成本更高。沒有任何模型具有明顯的優勢，儘管在任何特定任務中，一個模型可能表現得更好。在一個問題上嘗試多個模型，並使用它們的最佳結果，比只使用一個模型更可靠。

##### **Hostility Detection in UK Politics: A Dataset on Online Abuse Targeting MPs**
2412.04046v1 by Mugdha Pandya, Mali Jin, Kalina Bontcheva, Diana Maynard

Numerous politicians use social media platforms, particularly X, to engage
with their constituents. This interaction allows constituents to pose questions
and offer feedback but also exposes politicians to a barrage of hostile
responses, especially given the anonymity afforded by social media. They are
typically targeted in relation to their governmental role, but the comments
also tend to attack their personal identity. This can discredit politicians and
reduce public trust in the government. It can also incite anger and disrespect,
leading to offline harm and violence. While numerous models exist for detecting
hostility in general, they lack the specificity required for political
contexts. Furthermore, addressing hostility towards politicians demands
tailored approaches due to the distinct language and issues inherent to each
country (e.g., Brexit for the UK). To bridge this gap, we construct a dataset
of 3,320 English tweets spanning a two-year period manually annotated for
hostility towards UK MPs. Our dataset also captures the targeted identity
characteristics (race, gender, religion, none) in hostile tweets. We perform
linguistic and topical analyses to delve into the unique content of the UK
political data. Finally, we evaluate the performance of pre-trained language
models and large language models on binary hostility detection and multi-class
targeted identity type classification tasks. Our study offers valuable data and
insights for future research on the prevalence and nature of politics-related
hostility specific to the UK.

摘要：許多政治人物使用社群媒體平台，特別是 X，與其選民互動。這種互動讓選民得以提出問題並提供回饋，但也讓政治人物暴露在大量敵意的回應中，特別是考量到社群媒體提供的匿名性。他們通常會因其政府角色而成為攻擊目標，但這些評論也傾向於攻擊他們的個人身分。這可能會讓政治人物失去信譽，並降低民眾對政府的信任。它也可能煽動憤怒和不尊重，導致線下傷害和暴力。雖然有許多模型存在可用於偵測一般性的敵意，但它們缺乏政治脈絡所需的特殊性。此外，針對政治人物的敵意需要量身打造的方法，因為每個國家都有其獨特的語言和問題（例如英國的脫歐）。為了彌補這個差距，我們建立了一個包含 3,320 則英文推文的資料集，涵蓋兩年的時間，並針對針對英國國會議員的敵意進行手動註解。我們的資料集也會擷取敵意推文中針對的身分特徵（種族、性別、宗教、無）。我們執行語言和主題分析，以深入探討英國政治資料的獨特內容。最後，我們評估預先訓練的語言模型和大型語言模型在二元敵意偵測和多類別目標身分類型分類任務上的表現。我們的研究為未來針對英國政治相關敵意的盛行和性質的研究提供了有價值的資料和見解。

##### **INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations**
2412.04037v1 by Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Hu, Shuang Liang, Zhipeng Ge

Imagine having a conversation with a socially intelligent agent. It can
attentively listen to your words and offer visual and linguistic feedback
promptly. This seamless interaction allows for multiple rounds of conversation
to flow smoothly and naturally. In pursuit of actualizing it, we propose INFP,
a novel audio-driven head generation framework for dyadic interaction. Unlike
previous head generation works that only focus on single-sided communication,
or require manual role assignment and explicit role switching, our model drives
the agent portrait dynamically alternates between speaking and listening state,
guided by the input dyadic audio. Specifically, INFP comprises a Motion-Based
Head Imitation stage and an Audio-Guided Motion Generation stage. The first
stage learns to project facial communicative behaviors from real-life
conversation videos into a low-dimensional motion latent space, and use the
motion latent codes to animate a static image. The second stage learns the
mapping from the input dyadic audio to motion latent codes through denoising,
leading to the audio-driven head generation in interactive scenarios. To
facilitate this line of research, we introduce DyConv, a large scale dataset of
rich dyadic conversations collected from the Internet. Extensive experiments
and visualizations demonstrate superior performance and effectiveness of our
method. Project Page: https://grisoon.github.io/INFP/.

摘要：想像一下與一個社交智能代理對話。它可以專注聆聽你的話語，並迅速提供視覺和語言回饋。這種無縫的互動允許多輪對話順暢且自然地進行。為了實現它，我們提出了 INFP，一個用於雙人互動的新穎音訊驅動頭部生成架構。與先前僅專注於單邊溝通或需要手動角色分配和明確角色切換的頭部生成作品不同，我們的模型驅動代理肖像在說話和聆聽狀態之間動態交替，由輸入的雙人音訊引導。具體來說，INFP 包含一個基於動作的頭部模仿階段和一個音訊引導的動作生成階段。第一個階段學習將現實生活對話影片中的臉部溝通行為投射到低維動作潛在空間中，並使用動作潛在代碼為靜態影像製作動畫。第二個階段透過去噪學習從輸入的雙人音訊對應到動作潛在代碼，從而導致互動場景中的音訊驅動頭部生成。為了促進這條研究線，我們引入了 DyConv，一個從網際網路收集的大規模豐富雙人對話資料集。廣泛的實驗和視覺化證明了我們方法的優異效能和有效性。專案頁面：https://grisoon.github.io/INFP/。

##### **SocialMind: LLM-based Proactive AR Social Assistive System with Human-like Perception for In-situ Live Interactions**
2412.04036v1 by Bufang Yang, Yunqi Guo, Lilin Xu, Zhenyu Yan, Hongkai Chen, Guoliang Xing, Xiaofan Jiang

Social interactions are fundamental to human life. The recent emergence of
large language models (LLMs)-based virtual assistants has demonstrated their
potential to revolutionize human interactions and lifestyles. However, existing
assistive systems mainly provide reactive services to individual users, rather
than offering in-situ assistance during live social interactions with
conversational partners. In this study, we introduce SocialMind, the first
LLM-based proactive AR social assistive system that provides users with in-situ
social assistance. SocialMind employs human-like perception leveraging
multi-modal sensors to extract both verbal and nonverbal cues, social factors,
and implicit personas, incorporating these social cues into LLM reasoning for
social suggestion generation. Additionally, SocialMind employs a multi-tier
collaborative generation strategy and proactive update mechanism to display
social suggestions on Augmented Reality (AR) glasses, ensuring that suggestions
are timely provided to users without disrupting the natural flow of
conversation. Evaluations on three public datasets and a user study with 20
participants show that SocialMind achieves 38.3% higher engagement compared to
baselines, and 95% of participants are willing to use SocialMind in their live
social interactions.

摘要：社交互動是人類生活的基礎。最近出現的基於大型語言模型 (LLM) 的虛擬助理已經證明了它們徹底改變人類互動和生活方式的潛力。然而，現有的輔助系統主要為個別使用者提供反應式服務，而不是在與對話夥伴進行現場社交互動時提供現場協助。在這個研究中，我們介紹了 SocialMind，這是第一個基於 LLM 的主動式 AR 社交輔助系統，它為使用者提供現場社交協助。SocialMind 採用類人感知，利用多模式感測器來提取言語和非語言線索、社會因素和隱含角色，將這些社交線索納入 LLM 推理以產生社交建議。此外，SocialMind 採用多層協作生成策略和主動更新機制，在擴增實境 (AR) 眼鏡上顯示社交建議，確保在不中斷對話自然流暢的情況下及時向使用者提供建議。在三個公開資料集和一項有 20 位參與者的使用者研究中進行的評估顯示，與基線相比，SocialMind 的參與度高出 38.3%，而 95% 的參與者願意在他們的現場社交互動中使用 SocialMind。

##### **M$^{3}$D: A Multimodal, Multilingual and Multitask Dataset for Grounded Document-level Information Extraction**
2412.04026v1 by Jiang Liu, Bobo Li, Xinran Yang, Na Yang, Hao Fei, Mingyao Zhang, Fei Li, Donghong Ji

Multimodal information extraction (IE) tasks have attracted increasing
attention because many studies have shown that multimodal information benefits
text information extraction. However, existing multimodal IE datasets mainly
focus on sentence-level image-facilitated IE in English text, and pay little
attention to video-based multimodal IE and fine-grained visual grounding.
Therefore, in order to promote the development of multimodal IE, we constructed
a multimodal multilingual multitask dataset, named M$^{3}$D, which has the
following features: (1) It contains paired document-level text and video to
enrich multimodal information; (2) It supports two widely-used languages,
namely English and Chinese; (3) It includes more multimodal IE tasks such as
entity recognition, entity chain extraction, relation extraction and visual
grounding. In addition, our dataset introduces an unexplored theme, i.e.,
biography, enriching the domains of multimodal IE resources. To establish a
benchmark for our dataset, we propose an innovative hierarchical multimodal IE
model. This model effectively leverages and integrates multimodal information
through a Denoised Feature Fusion Module (DFFM). Furthermore, in non-ideal
scenarios, modal information is often incomplete. Thus, we designed a Missing
Modality Construction Module (MMCM) to alleviate the issues caused by missing
modalities. Our model achieved an average performance of 53.80% and 53.77% on
four tasks in English and Chinese datasets, respectively, which set a
reasonable standard for subsequent research. In addition, we conducted more
analytical experiments to verify the effectiveness of our proposed module. We
believe that our work can promote the development of the field of multimodal
IE.

摘要：多模態資訊萃取 (IE) 任務吸引了越來越多的關注，因為許多研究顯示多模態資訊有益於文字資訊萃取。然而，現有的多模態 IE 資料集主要專注於英文文字中基於影像的句子層級 IE，而較少關注基於影片的多模態 IE 和細粒度的視覺基礎。因此，為了促進多模態 IE 的發展，我們構建了一個名為 M$^{3}$D 的多模態多語言多任務資料集，它具有以下特點：(1) 它包含配對的文件層級文字和影片，以豐富多模態資訊；(2) 它支援兩種廣泛使用的語言，即英文和中文；(3) 它包含更多多模態 IE 任務，例如實體辨識、實體鏈萃取、關係萃取和視覺基礎。此外，我們的資料集引入了一個未探索的主題，即傳記，豐富了多模態 IE 資源的領域。為了建立我們資料集的基準，我們提出了一個創新的階層式多模態 IE 模型。此模型透過去噪特徵融合模組 (DFFM) 有效地利用和整合多模態資訊。此外，在非理想情況下，模式資訊通常是不完整的。因此，我們設計了一個遺失模態建構模組 (MMCM) 來緩解遺失模態所造成的問題。我們的模型在英文和中文資料集的四項任務上分別達到了 53.80% 和 53.77% 的平均效能，為後續研究設定了一個合理的標準。此外，我們進行了更多分析實驗，以驗證我們提出的模組的有效性。我們相信我們的研究成果可以促進多模態 IE 領域的發展。

##### **Exploring the Influence of Label Aggregation on Minority Voices: Implications for Dataset Bias and Model Training**
2412.04025v1 by Mugdha Pandya, Nafise Sadat Moosavi, Diana Maynard

Resolving disagreement in manual annotation typically consists of removing
unreliable annotators and using a label aggregation strategy such as majority
vote or expert opinion to resolve disagreement. These may have the side-effect
of silencing or under-representing minority but equally valid opinions. In this
paper, we study the impact of standard label aggregation strategies on minority
opinion representation in sexism detection. We investigate the quality and
value of minority annotations, and then examine their effect on the class
distributions in gold labels, as well as how this affects the behaviour of
models trained on the resulting datasets. Finally, we discuss the potential
biases introduced by each method and how they can be amplified by the models.

摘要：解決手動註解中的分歧通常包括移除不可靠的註解者，並使用標籤彙總策略（例如多數決或專家意見）來解決分歧。這些可能會產生使少數派意見沉默或代表性不足的副作用，但這些意見同樣有效。在本文中，我們研究了標準標籤彙總策略對性別歧視檢測中少數派意見代表的影響。我們調查了少數派註解的品質和價值，然後檢視它們對黃金標籤中的類別分佈的影響，以及這如何影響在所得資料集上訓練的模型的行為。最後，我們討論了每種方法引入的潛在偏差，以及這些偏差如何被模型放大。

##### **Deep-Unrolling Multidimensional Harmonic Retrieval Algorithms on Neuromorphic Hardware**
2412.04008v1 by Vlad C. Andrei, Alexandru P. Drăguţoiu, Gabriel Béna, Mahmoud Akl, Yin Li, Matthias Lohrmann, Ullrich J. Mönich, Holger Boche

This paper explores the potential of conversion-based neuromorphic algorithms
for highly accurate and energy-efficient single-snapshot multidimensional
harmonic retrieval (MHR). By casting the MHR problem as a sparse recovery
problem, we devise the currently proposed, deep-unrolling-based Structured
Learned Iterative Shrinkage and Thresholding (S-LISTA) algorithm to solve it
efficiently using complex-valued convolutional neural networks with
complex-valued activations, which are trained using a supervised regression
objective. Afterward, a novel method for converting the complex-valued
convolutional layers and activations into spiking neural networks (SNNs) is
developed. At the heart of this method lies the recently proposed Few Spikes
(FS) conversion, which is extended by modifying the neuron model's parameters
and internal dynamics to account for the inherent coupling between real and
imaginary parts in complex-valued computations. Finally, the converted SNNs are
mapped onto the SpiNNaker2 neuromorphic board, and a comparison in terms of
estimation accuracy and power efficiency between the original CNNs deployed on
an NVIDIA Jetson Xavier and the SNNs is being conducted. The measurement
results show that the converted SNNs achieve almost five-fold power efficiency
at moderate performance loss compared to the original CNNs.

摘要：本文探討了基於轉換的神經形態演算法在高準確度和節能單次快照多維諧波擷取 (MHR) 中的潛力。透過將 MHR 問題轉換為稀疏恢復問題，我們設計了目前提出的，基於深度展開的結構化學習迭代收縮和閾值 (S-LISTA) 演算法，使用具有複數值激活的複數值卷積神經網路有效地解決它，這些網路使用監督式回歸目標進行訓練。之後，開發出一種將複數值卷積層和激活轉換為脈衝神經網路 (SNN) 的新方法。此方法的核心在於最近提出的少脈衝 (FS) 轉換，透過修改神經元模型的參數和內部動態來擴充，以說明複數值運算中實部和虛部之間的內在耦合。最後，將轉換後的 SNN 對映到 SpiNNaker2 神經形態電路板上，並針對部署在 NVIDIA Jetson Xavier 上的原始 CNN 和 SNN 進行估計準確度和電力效率方面的比較。測量結果顯示，與原始 CNN 相比，轉換後的 SNN 在適度的效能損失下，達到了幾乎五倍的電力效率。

##### **Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement**
2412.04003v1 by Lingfeng Ming, Bo Zeng, Chenyang Lyu, Tianqi Shi, Yu Zhao, Xue Yang, Yefeng Liu, Yiyu Wang, Linlong Xu, Yangyang Liu, Xiaohu Zhao, Hao Wang, Heng Liu, Hao Zhou, Huifeng Yin, Zifu Shang, Haijun Li, Longyue Wang, Weihua Luo, Kaifu Zhang

Large Language Models (LLMs) have achieved remarkable progress in recent
years; however, their excellent performance is still largely limited to major
world languages, primarily English. Many LLMs continue to face challenges with
multilingual tasks, especially when it comes to low-resource languages. To
address this issue, we introduced Marco-LLM: Massive multilingual training for
cross-lingual enhancement LLM. We have collected a substantial amount of
multilingual data for several low-resource languages and conducted extensive
continual pre-training using the Qwen2 models. This effort has resulted in a
multilingual LLM named Marco-LLM. Through comprehensive evaluations on various
multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA
and many others, Marco-LLM has demonstrated substantial improvements over
state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements
in any-to-any machine translation tasks, showing the effectiveness of our
multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not
only perform exceptionally well in multilingual tasks, including low-resource
languages, but also maintain strong performance in English and other major
languages, closing the performance gap between high- and low-resource language
capabilities. By bridging languages, this effort demonstrates our dedication to
ensuring LLMs work accurately across various languages.

摘要：大型語言模型（LLM）近年來取得顯著進展；然而，其卓越的效能仍然主要侷限於主要世界語言，特別是英語。許多 LLM 持續面臨多語言任務的挑戰，尤其是在低資源語言方面。為了解決這個問題，我們引入了 Marco-LLM：用於跨語言增強 LLM 的大型多語言訓練。我們收集了大量來自多種低資源語言的多語言資料，並使用 Qwen2 模型進行廣泛的持續預訓練。這項工作產生了一個名為 Marco-LLM 的多語言 LLM。透過在各種多語言基準上進行全面評估，包括 MMMLU、AGIEval、Belebele、Flores-200、XCOPA 以及許多其他基準，Marco-LLM 已證明比現有的 LLM 有顯著的進步。此外，Marco-LLM 在任何到任何機器翻譯任務中都獲得了顯著的增強，顯示了我們多語言 LLM 的有效性。Marco-LLM 是一個創新的多語言 LLM，不僅在多語言任務中表現出色，包括低資源語言，而且在英語和其他主要語言中也能保持強勁的效能，縮小了高資源和低資源語言能力之間的效能差距。透過建立語言橋梁，這項工作證明了我們致力於確保 LLM 能準確地跨越各種語言運作。

##### **MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM**
2412.03987v1 by Changcheng Li, Xiangyu Wang, Qiuju Chen, Xiren Zhou, Huanhuan Chen

Large language models (LLMs) have shown limitations in tasks requiring
complex logical reasoning and multi-step problem-solving. To address these
challenges, researchers have employed carefully designed prompts and
flowcharts, simulating human cognitive processes to enhance LLM performance,
such as the Chain of Thought approach. In this paper, we introduce MTMT
(Multi-thinking Modes Tree), a novel method that interacts with LLMs to
construct a thought tree, simulating various advanced cognitive processes,
including but not limited to association, counterfactual thinking, task
decomposition, and comparison. By breaking down the original complex task into
simpler sub-questions, MTMT facilitates easier problem-solving for LLMs,
enabling more effective utilization of the latent knowledge within LLMs. We
evaluate the performance of MTMT under different parameter configurations,
using GPT-4o mini as the base model. Our results demonstrate that integrating
multiple modes of thinking significantly enhances the ability of LLMs to handle
complex tasks.

摘要：大型語言模型 (LLM) 在需要複雜邏輯推理和多步驟問題解決能力的任務中表現出限制。為了應對這些挑戰，研究人員採用精心設計的提示和流程圖，模擬人類認知過程以增強 LLM 的效能，例如思考鏈方法。在本文中，我們介紹了 MTMT（多思考模式樹），這是一種與 LLM 互動以建構思考樹的新方法，模擬各種先進的認知過程，包括但不限於聯想、反事實思考、任務分解和比較。通過將原本複雜的任務分解成更簡單的子問題，MTMT 促進 LLM 更容易解決問題，讓 LLM 能更有效地利用其潛在知識。我們使用 GPT-4o mini 作為基礎模型，評估 MTMT 在不同參數配置下的效能。我們的結果表明，整合多種思考模式顯著增強了 LLM 處理複雜任務的能力。

##### **Exploring Fully Convolutional Networks for the Segmentation of Hyperspectral Imaging Applied to Advanced Driver Assistance Systems**
2412.03982v1 by Jon Gutiérrez-Zaballa, Koldo Basterretxea, Javier Echanobe, M. Victoria Martínez, Inés del Campo

Advanced Driver Assistance Systems (ADAS) are designed with the main purpose
of increasing the safety and comfort of vehicle occupants. Most of current
computer vision-based ADAS perform detection and tracking tasks quite
successfully under regular conditions, but are not completely reliable,
particularly under adverse weather and changing lighting conditions, neither in
complex situations with many overlapping objects. In this work we explore the
use of hyperspectral imaging (HSI) in ADAS on the assumption that the distinct
near infrared (NIR) spectral reflectances of different materials can help to
better separate the objects in a driving scene. In particular, this paper
describes some experimental results of the application of fully convolutional
networks (FCN) to the image segmentation of HSI for ADAS applications. More
specifically, our aim is to investigate to what extent the spatial features
codified by convolutional filters can be helpful to improve the performance of
HSI segmentation systems. With that aim, we use the HSI-Drive v1.1 dataset,
which provides a set of labelled images recorded in real driving conditions
with a small-size snapshot NIR-HSI camera. Finally, we analyze the
implementability of such a HSI segmentation system by prototyping the developed
FCN model together with the necessary hyperspectral cube preprocessing stage
and characterizing its performance on an MPSoC.

摘要：進階駕駛輔助系統 (ADAS) 的主要設計目的是要提升車輛乘客的安全與舒適度。目前大多數基於電腦視覺的 ADAS 在一般情況下都能相當順利地執行偵測與追蹤任務，但並非完全可靠，特別是在惡劣的天氣和光線變化條件下，以及許多重疊物體的複雜情況中。在這項工作中，我們探討在 ADAS 中使用高光譜影像 (HSI)，假設不同材料的近紅外線 (NIR) 光譜反射率可以協助更有效地區分駕駛場景中的物體。特別是，本文描述了將完全卷積網路 (FCN) 應用於 HSI 影像分割以用於 ADAS 應用的一些實驗結果。更具體地說，我們的目標是探討卷積濾波器編碼的空間特徵在多大程度上可以協助提升 HSI 分割系統的效能。基於此目標，我們使用 HSI-Drive v1.1 資料集，其提供一組在實際駕駛條件下使用小型快照 NIR-HSI 相機所記錄的標籤影像。最後，我們透過將已開發的 FCN 模型與必要的超光譜立方體前處理階段結合來建構原型，並在 MPSoC 上分析此類 HSI 分割系統的可實作性，並描述其效能。

##### **A Data-Driven Framework for Discovering Fractional Differential Equations in Complex Systems**
2412.03970v1 by Xiangnan Yu, Hao Xu, Zhiping Mao, HongGuang Sun, Yong Zhang, Dongxiao Zhang, Yuntian Chen

In complex physical systems, conventional differential equations often fall
short in capturing non-local and memory effects, as they are limited to local
dynamics and integer-order interactions. This study introduces a stepwise
data-driven framework for discovering fractional differential equations (FDEs)
directly from data. FDEs, known for their capacity to model non-local dynamics
with fewer parameters than integer-order derivatives, can represent complex
systems with long-range interactions. Our framework applies deep neural
networks as surrogate models for denoising and reconstructing sparse and noisy
observations while using Gaussian-Jacobi quadrature to handle the challenges
posed by singularities in fractional derivatives. To optimize both the sparse
coefficients and fractional order, we employ an alternating optimization
approach that combines sparse regression with global optimization techniques.
We validate the framework across various datasets, including synthetic
anomalous diffusion data, experimental data on the creep behavior of frozen
soils, and single-particle trajectories modeled by L\'{e}vy motion. Results
demonstrate the framework's robustness in identifying the structure of FDEs
across diverse noise levels and its capacity to capture integer-order dynamics,
offering a flexible approach for modeling memory effects in complex systems.

摘要：在複雜的物理系統中，傳統的微分方程式通常無法捕捉非局部和記憶效應，因為它們僅限於局部動力學和整階交互作用。本研究引入了一個逐步資料驅動的架構，用於直接從資料中發現分數微分方程式 (FDE)。FDE 以其使用比整階導數更少的參數來建模非局部動力學的能力而聞名，可以用具有長程交互作用的複雜系統表示。我們的架構將深度神經網路應用於替代模型，用於去噪和重建稀疏和有噪聲的觀測，同時使用高斯-雅可比正交法來處理分數導數中奇異性帶來的挑戰。為了最佳化稀疏係數和分數階，我們採用了交替最佳化方法，結合了稀疏迴歸和全局最佳化技術。我們在各種資料集上驗證了該架構，包括合成異常擴散資料、凍土蠕變行為的實驗資料以及由 L\'{e}vy 運動建模的單粒子軌跡。結果證明了該架構在識別不同噪聲級別下的 FDE 結構方面的穩健性，以及其捕捉整階動力學的能力，為建模複雜系統中的記憶效應提供了一種靈活的方法。

##### **Demonstration Selection for In-Context Learning via Reinforcement Learning**
2412.03966v1 by Xubin Wang, Jianfei Wu, Yichen Yuan, Mingzhe Li, Deyu Cai, Weijia Jia

Diversity in demonstration selection is crucial for enhancing model
generalization, as it enables a broader coverage of structures and concepts.
However, constructing an appropriate set of demonstrations has remained a focal
point of research. This paper presents the Relevance-Diversity Enhanced
Selection (RDES), an innovative approach that leverages reinforcement learning
to optimize the selection of diverse reference demonstrations for text
classification tasks using Large Language Models (LLMs), especially in few-shot
prompting scenarios. RDES employs a Q-learning framework to dynamically
identify demonstrations that maximize both diversity and relevance to the
classification objective by calculating a diversity score based on label
distribution among selected demonstrations. This method ensures a balanced
representation of reference data, leading to improved classification accuracy.
Through extensive experiments on four benchmark datasets and involving 12
closed-source and open-source LLMs, we demonstrate that RDES significantly
enhances classification accuracy compared to ten established baselines.
Furthermore, we investigate the incorporation of Chain-of-Thought (CoT)
reasoning in the reasoning process, which further enhances the model's
predictive performance. The results underscore the potential of reinforcement
learning to facilitate adaptive demonstration selection and deepen the
understanding of classification challenges.

摘要：示範選擇的多樣性對於增強模型概化至關重要，因為它能更廣泛地涵蓋結構和概念。然而，建構一組適當的示範仍然是研究的重點。本文提出了相關性多樣性增強選擇 (RDES)，這是一種創新的方法，它利用強化學習來最佳化選擇用於文本分類任務的不同參考示範，特別是在少次提示場景中，並使用大型語言模型 (LLM)。RDES 使用 Q 學習架構來動態識別示範，這些示範能最大化多樣性和與分類目標相關性，方法是根據已選擇示範中的標籤分佈計算多樣性分數。此方法能確保參考資料的均衡表示，進而提升分類準確度。透過在四個基準資料集上進行廣泛的實驗，並涉及 12 個閉源和開源 LLM，我們證明 RDES 與十個已建立的基準相比，顯著提升了分類準確度。此外，我們探討了在推理過程中納入思考鏈 (CoT) 推理，這進一步提升了模型的預測效能。結果強調了強化學習促進適應性示範選擇的潛力，並加深了對分類挑戰的理解。

##### **Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation**
2412.03944v1 by Hao Yang, Qianghua Zhao, Lei Li

Chain-of-Thought prompting has significantly enhanced the reasoning
capabilities of large language models, with numerous studies exploring factors
influencing its performance. However, the underlying mechanisms remain poorly
understood. To further demystify the operational principles, this work examines
three key aspects: decoding, projection, and activation, aiming to elucidate
the changes that occur within models when employing Chainof-Thought. Our
findings reveal that LLMs effectively imitate exemplar formats while
integrating them with their understanding of the question, exhibiting
fluctuations in token logits during generation but ultimately producing a more
concentrated logits distribution, and activating a broader set of neurons in
the final layers, indicating more extensive knowledge retrieval compared to
standard prompts. Our code and data will be publicly avialable when the paper
is accepted.

摘要：鏈式思考提示顯著增強了大型語言模型的推理能力，許多研究探討了影響其效能的因素。然而，其底層機制仍鮮為人知。為了進一步揭開運作原理的神秘面紗，本研究探討了三個關鍵面向：解碼、投影和啟動，旨在闡明採用鏈式思考時模型中發生的變化。我們的研究結果顯示，大型語言模型有效地模仿範例格式，同時將其與對問題的理解整合在一起，在產生過程中展現出標記邏輯的波動，但最終產生出更集中的邏輯分佈，並在最後的層級中啟動更廣泛的神經元組，這表示與標準提示相比，知識擷取更為廣泛。我們的程式碼和資料將在論文被接受時公開。

##### **Enhancing and Accelerating Diffusion-Based Inverse Problem Solving through Measurements Optimization**
2412.03941v1 by Tianyu Chen, Zhendong Wang, Mingyuan Zhou

Diffusion models have recently demonstrated notable success in solving
inverse problems. However, current diffusion model-based solutions typically
require a large number of function evaluations (NFEs) to generate high-quality
images conditioned on measurements, as they incorporate only limited
information at each step. To accelerate the diffusion-based inverse
problem-solving process, we introduce \textbf{M}easurements
\textbf{O}ptimization (MO), a more efficient plug-and-play module for
integrating measurement information at each step of the inverse problem-solving
process. This method is comprehensively evaluated across eight diverse linear
and nonlinear tasks on the FFHQ and ImageNet datasets. By using MO, we
establish state-of-the-art (SOTA) performance across multiple tasks, with key
advantages: (1) it operates with no more than 100 NFEs, with phase retrieval on
ImageNet being the sole exception; (2) it achieves SOTA or near-SOTA results
even at low NFE counts; and (3) it can be seamlessly integrated into existing
diffusion model-based solutions for inverse problems, such as DPS
\cite{chung2022diffusion} and Red-diff \cite{mardani2023variational}. For
example, DPS-MO attains a peak signal-to-noise ratio (PSNR) of 28.71 dB on the
FFHQ 256 dataset for high dynamic range imaging, setting a new SOTA benchmark
with only 100 NFEs, whereas current methods require between 1000 and 4000 NFEs
for comparable performance.

摘要：<paragraph>擴散模型最近在解決逆問題方面展現出顯著的成功。然而，當前基於擴散模型的解決方案通常需要大量的函數評估（NFE）才能生成符合測量條件的高品質影像，因為它們在每個步驟中僅納入有限的資訊。為了加速基於擴散的逆問題求解程序，我們引入了「測量最佳化」（MO），這是一個更有效率的即插即用模組，用於在逆問題求解程序的每個步驟中整合測量資訊。此方法在 FFHQ 和 ImageNet 資料集上的八項不同的線性和非線性任務中進行了全面的評估。透過使用 MO，我們在多項任務中建立了最先進（SOTA）的效能，具備以下主要優勢：(1) 它運作時不超過 100 個 NFE，ImageNet 上的相位擷取是唯一的例外；(2) 即使在 NFE 數量較低的情況下，它也能達成 SOTA 或接近 SOTA 的結果；(3) 它可以無縫整合到現有的基於擴散模型的逆問題解決方案中，例如 DPS \cite{chung2022diffusion} 和 Red-diff \cite{mardani2023variational}。例如，DPS-MO 在 FFHQ 256 資料集上對於高動態範圍影像達到了 28.71 dB 的峰值信噪比（PSNR），僅使用 100 個 NFE 就設定了新的 SOTA 基準，而當前的方法需要介於 1000 到 4000 個 NFE 才能達到相近的效能。</paragraph>

##### **InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models**
2412.03934v1 by Yifan Lu, Xuanchi Ren, Jiawei Yang, Tianchang Shen, Zhangjie Wu, Jun Gao, Yue Wang, Siheng Chen, Mike Chen, Sanja Fidler, Jiahui Huang

We present InfiniCube, a scalable method for generating unbounded dynamic 3D
driving scenes with high fidelity and controllability. Previous methods for
scene generation either suffer from limited scales or lack geometric and
appearance consistency along generated sequences. In contrast, we leverage the
recent advancements in scalable 3D representation and video models to achieve
large dynamic scene generation that allows flexible controls through HD maps,
vehicle bounding boxes, and text descriptions. First, we construct a
map-conditioned sparse-voxel-based 3D generative model to unleash its power for
unbounded voxel world generation. Then, we re-purpose a video model and ground
it on the voxel world through a set of carefully designed pixel-aligned
guidance buffers, synthesizing a consistent appearance. Finally, we propose a
fast feed-forward approach that employs both voxel and pixel branches to lift
the dynamic videos to dynamic 3D Gaussians with controllable objects. Our
method can generate controllable and realistic 3D driving scenes, and extensive
experiments validate the effectiveness and superiority of our model.

摘要：我們提出 InfiniCube，一種可擴充的方法，用於生成具有高保真度和可控性的無界動態 3D 駕駛場景。用於場景生成的先前方法會受限於規模或缺乏沿著生成序列的幾何和外觀一致性。相反，我們利用可擴充 3D 表示和影片模型的最新進展，以實現大型動態場景生成，並透過 HD 地圖、車輛邊界框和文字描述允許靈活控制。首先，我們建構一個地圖條件化的稀疏體素基礎 3D 生成模型，以釋放其用於無界體素世界生成的強大功能。然後，我們重新調整影片模型，並透過一組精心設計的像素對齊引導緩衝區將其建立在體素世界中，合成一致的外觀。最後，我們提出一個快速的饋前傳方法，採用體素和像素分支，將動態影片提升到具有可控物件的動態 3D 高斯。我們的模型可以生成可控且逼真的 3D 駕駛場景，且廣泛的實驗驗證了我們模型的有效性和優越性。

##### **Exploring AI Text Generation, Retrieval-Augmented Generation, and Detection Technologies: a Comprehensive Overview**
2412.03933v1 by Fnu Neha, Deepshikha Bhati, Deepak Kumar Shukla, Angela Guercio, Ben Ward

The rapid development of Artificial Intelligence (AI) has led to the creation
of powerful text generation models, such as large language models (LLMs), which
are widely used for diverse applications. However, concerns surrounding
AI-generated content, including issues of originality, bias, misinformation,
and accountability, have become increasingly prominent. This paper offers a
comprehensive overview of AI text generators (AITGs), focusing on their
evolution, capabilities, and ethical implications. This paper also introduces
Retrieval-Augmented Generation (RAG), a recent approach that improves the
contextual relevance and accuracy of text generation by integrating dynamic
information retrieval. RAG addresses key limitations of traditional models,
including their reliance on static knowledge and potential inaccuracies in
handling real-world data. Additionally, the paper reviews detection tools that
help differentiate AI-generated text from human-written content and discusses
the ethical challenges these technologies pose. The paper explores future
directions for improving detection accuracy, supporting ethical AI development,
and increasing accessibility. The paper contributes to a more responsible and
reliable use of AI in content creation through these discussions.

摘要：人工智慧 (AI) 的快速發展，促成了強大的文字生成模型的建立，例如大型語言模型 (LLM)，廣泛用於各種應用程式中。然而，圍繞著 AI 生成的內容，包括原創性、偏見、錯誤資訊和責任歸屬等問題的疑慮日益受到重視。本文提供了一份關於 AI 文字生成器 (AITG) 的全面概述，重點在它們的演進、能力和倫理影響。本文也介紹了檢索增強生成 (RAG)，一種近期方法，透過整合動態資訊檢索來改善文字生成的脈絡相關性和準確性。RAG 解決了傳統模型的主要限制，包括它們依賴於靜態知識，以及在處理真實世界資料時潛在的不準確性。此外，本文回顧了偵測工具，這些工具有助於區分 AI 生成的文字和人類撰寫的內容，並探討了這些技術帶來的倫理挑戰。本文探討了改進偵測準確性、支援倫理 AI 發展和增加可及性的未來方向。本文透過這些討論，為在內容創作中更負責任和可靠地使用 AI 做出了貢獻。

##### **MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model**
2412.03930v1 by Yunhe Pang, Bo Chen, Fanjin Zhang, Yanghui Rao, Jie Tang

The rapid growth of academic publications has exacerbated the issue of author
name ambiguity in online digital libraries. Despite advances in name
disambiguation algorithms, cumulative errors continue to undermine the
reliability of academic systems. It is estimated that over 10% paper-author
assignments are rectified when constructing the million-scale WhoIsWho
benchmark. Existing endeavors to detect incorrect assignments are either
semantic-based or graph-based approaches, which fall short of making full use
of the rich text attributes of papers and implicit structural features defined
via the co-occurrence of paper attributes. To this end, this paper introduces a
structure-enhanced language model that combines key structural features from
graph-based methods with fine-grained semantic features from rich paper
attributes to detect incorrect assignments. The proposed model is trained with
a highly effective multi-modal multi-turn instruction tuning framework, which
incorporates task-guided instruction tuning, text-attribute modality, and
structural modality. Experimental results demonstrate that our model
outperforms previous approaches, achieving top performance on the leaderboard
of KDD Cup 2024. Our code has been publicly available.

摘要：學術出版品的快速成長，加劇了線上數位圖書館中作者姓名歧義的問題。儘管姓名消歧演算法有進展，累積的錯誤仍持續破壞學術系統的可靠性。據估計，在建構百萬規模的 WhoIsWho 基準時，超過 10% 的論文作者指派被修正。現有的偵測不正確指派的努力，不是基於語意的，就是基於圖的，無法充分利用論文豐富的文字屬性和透過論文屬性共現定義的隱含結構特徵。為此，本文介紹了一個結構增強語言模型，將基於圖的方法中的關鍵結構特徵與豐富論文屬性中的細粒度語義特徵相結合，以偵測不正確的指派。所提出的模型使用一個高效的多模態多輪指令微調架構進行訓練，其中包含任務導向的指令微調、文字屬性模態和結構模態。實驗結果證明，我們的模型優於先前的模型，在 KDD Cup 2024 的排行榜上取得最佳效能。我們的程式碼已公開。

##### **MT3DNet: Multi-Task learning Network for 3D Surgical Scene Reconstruction**
2412.03928v1 by Mithun Parab, Pranay Lendave, Jiyoung Kim, Thi Quynh Dan Nguyen, Palash Ingle

In image-assisted minimally invasive surgeries (MIS), understanding surgical
scenes is vital for real-time feedback to surgeons, skill evaluation, and
improving outcomes through collaborative human-robot procedures. Within this
context, the challenge lies in accurately detecting, segmenting, and estimating
the depth of surgical scenes depicted in high-resolution images, while
simultaneously reconstructing the scene in 3D and providing segmentation of
surgical instruments along with detection labels for each instrument. To
address this challenge, a novel Multi-Task Learning (MTL) network is proposed
for performing these tasks concurrently. A key aspect of this approach involves
overcoming the optimization hurdles associated with handling multiple tasks
concurrently by integrating a Adversarial Weight Update into the MTL framework,
the proposed MTL model achieves 3D reconstruction through the integration of
segmentation, depth estimation, and object detection, thereby enhancing the
understanding of surgical scenes, which marks a significant advancement
compared to existing studies that lack 3D capabilities. Comprehensive
experiments on the EndoVis2018 benchmark dataset underscore the adeptness of
the model in efficiently addressing all three tasks, demonstrating the efficacy
of the proposed techniques.

摘要：<paragraph>在影像輔助微創手術 (MIS) 中，了解手術場景對於外科醫師的即時反饋、技能評估以及透過人機協作程序改善結果至關重要。在此背景下，挑戰在於準確地偵測、分割和估計高解析度影像中描繪的手術場景深度，同時以 3D 重建場景並提供手術器械的分割，以及每個器械的偵測標籤。為了應對此挑戰，提出了一種新穎的多任務學習 (MTL) 網路，用於同時執行這些任務。此方法的一個關鍵方面涉及透過將對抗權重更新整合到 MTL 架構中，克服處理多個任務同時進行相關的最佳化障礙，所提出的 MTL 模型透過整合分割、深度估計和物件偵測來達成 3D 重建，從而增進對手術場景的理解，這與缺乏 3D 功能的現有研究相比，標誌著顯著的進步。在 EndoVis2018 基準資料集上進行的全面實驗強調了該模型在有效處理所有三項任務方面的熟練度，證明了所提出技術的功效。</paragraph>

##### **A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios**
2412.03920v1 by Xiachong Feng, Longxu Dou, Ella Li, Qinghao Wang, Haochuan Wang, Yu Guo, Chang Ma, Lingpeng Kong

Game-theoretic scenarios have become pivotal in evaluating the social
intelligence of Large Language Model (LLM)-based social agents. While numerous
studies have explored these agents in such settings, there is a lack of a
comprehensive survey summarizing the current progress. To address this gap, we
systematically review existing research on LLM-based social agents within
game-theoretic scenarios. Our survey organizes the findings into three core
components: Game Framework, Social Agent, and Evaluation Protocol. The game
framework encompasses diverse game scenarios, ranging from choice-focusing to
communication-focusing games. The social agent part explores agents'
preferences, beliefs, and reasoning abilities. The evaluation protocol covers
both game-agnostic and game-specific metrics for assessing agent performance.
By reflecting on the current research and identifying future research
directions, this survey provides insights to advance the development and
evaluation of social agents in game-theoretic scenarios.

摘要：博弈论场景已成为评估大型语言模型 (LLM) 为基础的社交代理的社交智能的关键。尽管许多研究已在该场景中探索这些代理，但缺乏一项总结当前进展的综合调查。为了解决这一差距，我们系统性地回顾了博弈论场景中基于 LLM 的社交代理的现有研究。我们的调查将研究结果组织成三个核心组成部分：博弈框架、社交代理和评估协议。博弈框架包含各种博弈场景，从选择聚焦到沟通聚焦的博弈。社交代理部分探索了代理的偏好、信念和推理能力。评估协议涵盖了用于评估代理性能的游戏无关和特定游戏指标。通过反思当前的研究并确定未来的研究方向，本调查为在博弈论场景中推进社交代理的开发和评估提供了见解。

##### **Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair**
2412.03905v1 by Qiong Feng, Xiaotian Ma, Jiayi Sheng, Ziyuan Feng, Wei Song, Peng Liang

LLMs have garnered considerable attention for their potential to streamline
Automated Program Repair (APR). LLM-based approaches can either insert the
correct code or directly generate patches when provided with buggy methods.
However, most of LLM-based APR methods rely on a single type of software
information, without fully leveraging different software artifacts. Despite
this, many LLM-based approaches do not explore which specific types of
information best assist in APR. Addressing this gap is crucial for advancing
LLM-based APR techniques. We propose DEVLoRe to use issue content (description
and message) and stack error traces to localize buggy methods, then rely on
debug information in buggy methods and issue content and stack error to
localize buggy lines and generate plausible patches which can pass all unit
tests. The results show that while issue content is particularly effective in
assisting LLMs with fault localization and program repair, different types of
software artifacts complement each other. By incorporating different artifacts,
DEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy
methods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0
dataset, respectively. This outperforms current state-of-the-art APR methods.
The source code and experimental results of this work for replication are
available at https://github.com/XYZboom/DEVLoRe.

摘要：大型語言模型 (LLM) 因其簡化自動程式修復 (APR) 的潛力而備受關注。基於 LLM 的方法可以在提供有缺陷的方法時插入正確的程式碼或直接產生修補程式。然而，大多數基於 LLM 的 APR 方法依賴於單一類型的軟體資訊，而沒有充分利用不同的軟體人工製品。儘管如此，許多基於 LLM 的方法並未探討哪些特定類型的資訊最有助於 APR。解決這個差距對於推進基於 LLM 的 APR 技術至關重要。我們提出 DEVLoRe 使用問題內容（說明和訊息）和堆疊錯誤追蹤來定位有缺陷的方法，然後依賴於有缺陷方法中的偵錯資訊以及問題內容和堆疊錯誤來定位有缺陷的行並產生合理的修補程式，這些修補程式可以通過所有單元測試。結果表明，雖然問題內容在協助 LLM 進行故障定位和程式修復方面特別有效，但不同類型的軟體人工製品可以互補。透過整合不同的人工製品，DEVLoRe 成功定位了 49.3% 和 47.6% 的單一和非單一有缺陷方法，並分別為 Defects4J v2.0 資料集產生了 56.0% 和 14.5% 的合理修補程式。這優於目前最先進的 APR 方法。這項工作的原始碼和實驗結果可於 https://github.com/XYZboom/DEVLoRe 取得，以供複製。

##### **MISR: Measuring Instrumental Self-Reasoning in Frontier Models**
2412.03904v1 by Kai Fronsdal, David Lindner

We propose a suite of tasks to evaluate the instrumental self-reasoning
ability of large language model (LLM) agents. Instrumental self-reasoning
ability could improve adaptability and enable self-modification, but it could
also pose significant risks, such as enabling deceptive alignment. Prior work
has only evaluated self-reasoning in non-agentic settings or in limited
domains. In this paper, we propose evaluations for instrumental self-reasoning
ability in agentic tasks in a wide range of scenarios, including
self-modification, knowledge seeking, and opaque self-reasoning. We evaluate
agents built using state-of-the-art LLMs, including commercial and open source
systems. We find that instrumental self-reasoning ability emerges only in the
most capable frontier models and that it is highly context-dependent. No model
passes the the most difficult versions of our evaluations, hence our evaluation
can be used to measure increases in instrumental self-reasoning ability in
future models. We open-source our evaluations at
https://github.com/kaifronsdal/Self-Reasoning-Evals.

摘要：我們提出了一系列任務，用於評估大型語言模型 (LLM) 代理的工具性自我推理能力。工具性自我推理能力可以提高適應性並實現自我修改，但它也可能帶來重大風險，例如實現欺騙性對齊。先前的研究僅在非代理設定或有限的領域中評估自我推理。在本文中，我們提出了在各種場景中對代理任務中的工具性自我推理能力進行評估，包括自我修改、知識尋求和不透明的自我推理。我們評估了使用最先進的 LLM 建構的代理，包括商業和開源系統。我們發現工具性自我推理能力僅出現在最有能力的前沿模型中，並且它高度依賴於上下文。沒有模型通過我們評估的最困難版本，因此我們的評估可用於衡量未來模型中工具性自我推理能力的提升。我們在 https://github.com/kaifronsdal/Self-Reasoning-Evals 上開放我們的評估。

##### **A Noise is Worth Diffusion Guidance**
2412.03895v1 by Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Jaewon Min, Minjae Kim, Wooseok Jang, Hyoungwon Cho, Sayak Paul, SeonHwa Kim, Eunju Cha, Kyong Hwan Jin, Seungryong Kim

Diffusion models excel in generating high-quality images. However, current
diffusion models struggle to produce reliable images without guidance methods,
such as classifier-free guidance (CFG). Are guidance methods truly necessary?
Observing that noise obtained via diffusion inversion can reconstruct
high-quality images without guidance, we focus on the initial noise of the
denoising pipeline. By mapping Gaussian noise to `guidance-free noise', we
uncover that small low-magnitude low-frequency components significantly enhance
the denoising process, removing the need for guidance and thus improving both
inference throughput and memory. Expanding on this, we propose \ours, a novel
method that replaces guidance methods with a single refinement of the initial
noise. This refined noise enables high-quality image generation without
guidance, within the same diffusion pipeline. Our noise-refining model
leverages efficient noise-space learning, achieving rapid convergence and
strong performance with just 50K text-image pairs. We validate its
effectiveness across diverse metrics and analyze how refined noise can
eliminate the need for guidance. See our project page:
https://cvlab-kaist.github.io/NoiseRefine/.

摘要：擴散模型在生成高品質影像方面表現出色。然而，目前的擴散模型在沒有引導方法的情況下，難以產生可靠的影像，例如無分類器引導 (CFG)。引導方法真的有必要嗎？我們觀察到透過擴散反演取得的雜訊可以重建高品質影像而無需引導，我們將重點放在去噪程序的初始雜訊上。藉由將高斯雜訊對應到「無引導雜訊」，我們發現幅度小、頻率低的微小元件會顯著提升去噪程序，消除了對引導的需求，進而提升推論處理量和記憶體。在此基礎上，我們提出 \ours，一種創新的方法，以單一初始雜訊精煉取代引導方法。這種精煉的雜訊可以在相同的擴散程序中，無需引導就能產生高品質影像。我們的雜訊精煉模型利用有效率的雜訊空間學習，僅使用 50K 組文字影像配對，就能快速收斂並展現強大的效能。我們透過各種指標驗證其效能，並分析精煉雜訊如何消除對引導的需求。請參閱我們的專案頁面：
https://cvlab-kaist.github.io/NoiseRefine/。

##### **Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models**
2412.03886v1 by Swarnava Sinha Roy, Ayan Kundu

Integrated Gradients is a well-known technique for explaining deep learning
models. It calculates feature importance scores by employing a gradient based
approach computing gradients of the model output with respect to input features
and accumulating them along a linear path. While this works well for continuous
features spaces, it may not be the most optimal way to deal with discrete
spaces like word embeddings. For interpreting LLMs (Large Language Models),
there exists a need for a non-linear path where intermediate points, whose
gradients are to be computed, lie close to actual words in the embedding space.
In this paper, we propose a method called Uniform Discretized Integrated
Gradients (UDIG) based on a new interpolation strategy where we choose a
favorable nonlinear path for computing attribution scores suitable for
predictive language models. We evaluate our method on two types of NLP tasks-
Sentiment Classification and Question Answering against three metrics viz Log
odds, Comprehensiveness and Sufficiency. For sentiment classification, we have
used the SST2, IMDb and Rotten Tomatoes datasets for benchmarking and for
Question Answering, we have used the fine-tuned BERT model on SQuAD dataset.
Our approach outperforms the existing methods in almost all the metrics.

摘要：整合梯度是解釋深度學習模型的一種著名技術。它透過採用基於梯度的策略，計算特徵重要性分數，計算模型輸出相對於輸入特徵的梯度，並沿著線性路徑累積它們。雖然這對於連續特徵空間來說很有效，但對於處理離散空間（例如字詞嵌入）來說，可能不是最佳方法。對於解釋 LLM（大型語言模型），需要一個非線性路徑，其中要計算其梯度的中間點位於嵌入空間中的實際字詞附近。在本文中，我們提出了一種稱為均勻離散整合梯度 (UDIG) 的方法，它基於一種新的插值策略，在其中我們選擇一條有利的非線性路徑來計算適合預測語言模型的歸因分數。我們根據三種指標（即對數機率、全面性和充分性）對兩種 NLP 任務類型（情緒分類和問答）評估我們的模型。對於情緒分類，我們使用 SST2、IMDb 和爛番茄數據集進行基準測試，對於問答，我們在 SQuAD 數據集上使用微調後的 BERT 模型。我們的模型在幾乎所有指標上都優於現有方法。

##### **A Unified Framework for Evaluating the Effectiveness and Enhancing the Transparency of Explainable AI Methods in Real-World Applications**
2412.03884v1 by Md. Ariful Islam, M. F. Mridha, Md Abrar Jahin, Nilanjan Dey

The rapid advancement of deep learning has resulted in substantial
advancements in AI-driven applications; however, the "black box" characteristic
of these models frequently constrains their interpretability, transparency, and
reliability. Explainable artificial intelligence (XAI) seeks to elucidate AI
decision-making processes, guaranteeing that explanations faithfully represent
the model's rationale and correspond with human comprehension. Despite
comprehensive research in XAI, a significant gap persists in standardized
procedures for assessing the efficacy and transparency of XAI techniques across
many real-world applications. This study presents a unified XAI evaluation
framework incorporating extensive quantitative and qualitative criteria to
systematically evaluate the correctness, interpretability, robustness,
fairness, and completeness of explanations generated by AI models. The
framework prioritizes user-centric and domain-specific adaptations, hence
improving the usability and reliability of AI models in essential domains. To
address deficiencies in existing evaluation processes, we suggest defined
benchmarks and a systematic evaluation pipeline that includes data loading,
explanation development, and thorough method assessment. The suggested
framework's relevance and variety are evidenced by case studies in healthcare,
finance, agriculture, and autonomous systems. These provide a solid basis for
the equitable and dependable assessment of XAI methodologies. This paradigm
enhances XAI research by offering a systematic, flexible, and pragmatic method
to guarantee transparency and accountability in AI systems across many
real-world contexts.

摘要：深度學習的快速發展已為 AI 驅動應用程式帶來實質的進展；然而，這些模型的「黑盒子」特性經常限制其可解釋性、透明度和可靠性。可解釋人工智慧 (XAI) 旨在闡明 AI 決策流程，保證解釋忠實地呈現模型的依據，並與人類理解相符。儘管 XAI 的研究很全面，但評估 XAI 技術在許多實際應用中效能和透明度的標準化程序仍存在顯著差距。本研究提出一個統一的 XAI 評估架構，結合廣泛的量化和定性標準，以系統性地評估 AI 模型產生的解釋的正確性、可解釋性、穩健性、公平性和完整性。該架構優先考慮以使用者為中心和特定領域的調整，因此改善了 AI 模型在基本領域中的可用性和可靠性。為了解決現有評估流程的不足，我們建議定義基準和一個系統性的評估管道，其中包括資料載入、解釋開發和徹底的方法評估。建議的架構在醫療保健、金融、農業和自駕系統中的案例研究證明了其相關性和多樣性。這些為 XAI 方法的公平且可靠評估提供了穩固的基礎。此範例透過在許多實際情況下提供一種系統性、靈活且務實的方法來保證 AI 系統的透明度和問責性，進而增強 XAI 研究。

##### **Weak-to-Strong Generalization Through the Data-Centric Lens**
2412.03881v1 by Changho Shin, John Cooper, Frederic Sala

The weak-to-strong generalization phenomenon is the driver for important
machine learning applications including highly data-efficient learning and,
most recently, performing superalignment. While decades of research have
resulted in numerous algorithms that produce strong empirical performance,
understanding what aspects of data enable weak-to-strong generalization has
been understudied. We propose a simple data-centric mechanism that
characterizes weak-to-strong generalization: the overlap density. Intuitively,
generalization tracks the number of points that contain overlaps, i.e., both
easy patterns (learnable by a weak model) and challenging patterns (only
learnable by a stronger model), as with such points, weak predictions can be
used to learn challenging patterns by stronger models. We provide a practical
overlap detection algorithm to find such points in datasets and leverage them
to learn, among multiple sources of data, which to query when seeking to
maximize overlap density and thereby enhance weak-to-strong generalization. We
present a theoretical result showing that the generalization benefit is a
function of the overlap density and a regret bound for our data selection
algorithm. Empirically, we validate the mechanism and the overlap detection
algorithm on a wide array of settings.

摘要：弱到強的泛化現象是重要的機器學習應用程式的驅動力，包括高度資料有效率的學習，以及最近執行的超級對齊。雖然數十年的研究產生了許多產生強大經驗效能的演算法，但了解資料的哪些面向能讓弱到強的泛化成為可能，卻鮮少有人研究。我們提出一個簡單的以資料為中心的機制，其特徵在於弱到強的泛化：重疊密度。直覺上，泛化追蹤包含重疊的點的數量，亦即簡單的模式（弱模型可學習）和具挑戰性的模式（只有較強的模型可學習），就像這樣的點，弱預測可用於學習較強模型的具挑戰性模式。我們提供一個實用的重疊偵測演算法，在資料集中找出這樣的點，並利用它們來學習，在多個資料來源中，找出在尋求最大化重疊密度並因此增強弱到強泛化的過程中要查詢的資料。我們提出一個理論結果，顯示泛化效益是重疊密度和資料選擇演算法的後悔邊界的函數。經驗上，我們在廣泛的設定中驗證了機制和重疊偵測演算法。

##### **AyutthayaAlpha: A Thai-Latin Script Transliteration Transformer**
2412.03877v1 by Davor Lauc, Attapol Rutherford, Weerin Wongwarawipatr

This study introduces AyutthayaAlpha, an advanced transformer-based machine
learning model designed for the transliteration of Thai proper names into Latin
script. Our system achieves state-of-the-art performance with 82.32%
first-token accuracy and 95.24% first-three-token accuracy, while maintaining a
low character error rate of 0.0047. The complexity of Thai phonology, including
tonal features and vowel length distinctions, presents significant challenges
for accurate transliteration, which we address through a novel two-model
approach: AyutthayaAlpha-Small, based on the ByT5 architecture, and
AyutthayaAlpha-VerySmall, a computationally efficient variant that unexpectedly
outperforms its larger counterpart. Our research combines linguistic rules with
deep learning, training on a carefully curated dataset of 1.2 million
Thai-Latin name pairs, augmented through strategic upsampling to 2.7 million
examples. Extensive evaluations against existing transliteration methods and
human expert benchmarks demonstrate that AyutthayaAlpha not only achieves
superior accuracy but also effectively captures personal and cultural
preferences in name romanization. The system's practical applications extend to
cross-lingual information retrieval, international data standardization, and
identity verification systems, with particular relevance for government
databases, academic institutions, and global business operations. This work
represents a significant advance in bridging linguistic gaps between Thai and
Latin scripts, while respecting the cultural and personal dimensions of name
transliteration.

摘要：本研究介紹了 AyutthayaAlpha，一種先進的基於 Transformer 的機器學習模型，專為將泰語專有名詞音譯成拉丁字母而設計。我們的系統達到了最先進的性能，首個符號準確度為 82.32%，前三個符號準確度為 95.24%，同時將字元錯誤率保持在 0.0047 的低水準。泰語音韻的複雜性，包括聲調特徵和元音長度區別，對準確音譯提出了重大挑戰，我們通過一種新穎的雙模型方法來解決這個問題：基於 ByT5 架構的 AyutthayaAlpha-Small，以及計算效率高的變體 AyutthayaAlpha-VerySmall，出乎意料地優於其較大的對應模型。我們的研究結合了語言規則和深度學習，在一個經過仔細策劃的包含 120 萬個泰語-拉丁語姓名對的資料集上進行訓練，並通過策略性上採樣擴充到 270 萬個範例。針對現有音譯方法和人類專家基準進行的廣泛評估表明，AyutthayaAlpha 不僅實現了更高的準確性，而且還有效地捕捉到了姓名羅馬化中的個人和文化偏好。該系統的實際應用延伸到跨語言資訊檢索、國際資料標準化和身分驗證系統，特別與政府資料庫、學術機構和全球業務運作相關。這項工作代表了在泰語和拉丁語字母之間彌合語言差距方面的一項重大進展，同時尊重姓名音譯的文化和個人面向。

##### **Fine-Grained Sentiment Analysis of Electric Vehicle User Reviews: A Bidirectional LSTM Approach to Capturing Emotional Intensity in Chinese Text**
2412.03873v1 by Shuhao Chen, Chengyi Tu

The rapid expansion of the electric vehicle (EV) industry has highlighted the
importance of user feedback in improving product design and charging
infrastructure. Traditional sentiment analysis methods often oversimplify the
complexity of user emotions, limiting their effectiveness in capturing nuanced
sentiments and emotional intensities. This study proposes a Bidirectional Long
Short-Term Memory (Bi-LSTM) network-based sentiment scoring model to analyze
user reviews of EV charging infrastructure. By assigning sentiment scores
ranging from 0 to 5, the model provides a fine-grained understanding of
emotional expression. Leveraging a dataset of 43,678 reviews from PC Auto, the
study employs rigorous data cleaning and preprocessing, including tokenization
and stop word removal, to optimize input for deep learning. The Bi-LSTM model
demonstrates significant improvements over traditional approaches like SnowNLP
across key evaluation metrics, including Mean Squared Error (MSE), Mean
Absolute Error (MAE), and Explained Variance Score (EVS). These results
highlight the model's superior capability to capture nuanced sentiment
dynamics, offering valuable insights for targeted product and service
enhancements in the EV ecosystem.

摘要：電動車 (EV) 產業的快速擴張突顯了使用者回饋在改善產品設計和充電基礎設施的重要性。傳統的情緒分析方法通常過度簡化使用者情緒的複雜性，限制了它們在捕捉細微情緒和情緒強度方面的效能。本研究提出一個基於雙向長短期記憶 (Bi-LSTM) 網路的評分模型，用於分析使用者對電動車充電基礎設施的評論。透過分配從 0 到 5 的情緒分數，該模型提供對情緒表達的細微理解。利用來自 PC Auto 的 43,678 則評論的資料集，本研究採用嚴謹的資料清理和預處理，包括標記化和移除停止詞，以最佳化深度學習的輸入。Bi-LSTM 模型在關鍵評估指標（包括平均平方誤差 (MSE)、平均絕對誤差 (MAE) 和解釋變異數 (EVS)）上，展示出優於 SnowNLP 等傳統方法的顯著改進。這些結果突顯了該模型捕捉細微情緒動態的卓越能力，為電動車生態系統中的目標產品和服務增強提供了有價值的見解。

##### **How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs in E-Learning Environments?**
2412.03856v1 by Patrick Ocheja, Brendan Flanagan, Yiling Dai, Hiroaki Ogata

E-learning environments are increasingly harnessing large language models
(LLMs) like GPT-3.5 and GPT-4 for tailored educational support. This study
introduces an approach that integrates dynamic knowledge graphs with LLMs to
offer nuanced student assistance. By evaluating past and ongoing student
interactions, the system identifies and appends the most salient learning
context to prompts directed at the LLM. Central to this method is the knowledge
graph's role in assessing a student's comprehension of topic prerequisites.
Depending on the categorized understanding (good, average, or poor), the LLM
adjusts its guidance, offering advanced assistance, foundational reviews, or
in-depth prerequisite explanations, respectively. Preliminary findings suggest
students could benefit from this tiered support, achieving enhanced
comprehension and improved task outcomes. However, several issues related to
potential errors arising from LLMs were identified, which can potentially
mislead students. This highlights the need for human intervention to mitigate
these risks. This research aims to advance AI-driven personalized learning
while acknowledging the limitations and potential pitfalls, thus guiding future
research in technology and data-driven education.

摘要：電子學習環境正日益利用大型語言模型 (LLM)，例如 GPT-3.5 和 GPT-4，提供量身打造的教育支援。本研究提出了一種方法，將動態知識圖與 LLM 整合，提供細緻入微的學生協助。系統會評估過去和正在進行的學生互動，找出並附加最顯著的學習脈絡，以提示 LLM。此方法的核心在於知識圖在評估學生對主題先備知識的理解程度方面所扮演的角色。LLM 會根據分類後的理解程度（良好、普通或差）調整其指導，分別提供進階協助、基礎回顧或深入的先備知識說明。初步發現表明，學生可以受益於這種分層支援，達到增強的理解力和改善的任務成果。然而，已找出與 LLM 產生的潛在錯誤相關的幾個問題，這些錯誤可能會誤導學生。這突顯了人類介入以降低這些風險的必要性。本研究旨在推進 AI 驅動的個人化學習，同時承認限制和潛在的陷阱，從而指導未來在技術和資料驅動教育方面的研究。

##### **Automated LaTeX Code Generation from Handwritten Math Expressions Using Vision Transformer**
2412.03853v1 by Jayaprakash Sundararaj, Akhil Vyas, Benjamin Gonzalez-Maldonado

Converting mathematical expressions into LaTeX is challenging. In this paper,
we explore using newer transformer based architectures for addressing the
problem of converting handwritten/digital mathematical expression images into
equivalent LaTeX code. We use the current state of the art CNN encoder and RNN
decoder as a baseline for our experiments. We also investigate improvements to
CNN-RNN architecture by replacing the CNN encoder with the ResNet50 model. Our
experiments show that transformer architectures achieve a higher overall
accuracy and BLEU scores along with lower Levenschtein scores compared to the
baseline CNN/RNN architecture with room to achieve even better results with
appropriate fine-tuning of model parameters.

摘要：將數學表達式轉換為 LaTeX 是一項挑戰。在本文中，
我們探討使用更新的Transformer架構來解決將手寫/數位數學表達式影像轉換為
等效 LaTeX 程式碼的問題。我們使用目前最先進的 CNN 編碼器和 RNN
解碼器作為我們實驗的基準。我們還研究了透過將 CNN 編碼器替換為 ResNet50 模型來改善
CNN-RNN 架構。我們的實驗顯示，與具有適當微調模型參數的基準 CNN/RNN 架構相比，Transformer架構可獲得更高的整體
準確度和 BLEU 分數以及較低的 Levenschtein 分數，並有空間獲得更好的結果。

##### **FedMetaMed: Federated Meta-Learning for Personalized Medication in Distributed Healthcare Systems**
2412.03851v1 by Jiechao Gao, Yuangang Li

Personalized medication aims to tailor healthcare to individual patient
characteristics. However, the heterogeneity of patient data across healthcare
systems presents significant challenges to achieving accurate and effective
personalized treatments. Ethical concerns further complicate the aggregation of
large volumes of data from diverse institutions. Federated Learning (FL) offers
a promising decentralized solution by enabling collaborative model training
through the exchange of client models rather than raw data, thus preserving
privacy. However, existing FL methods often suffer from retrogression during
server aggregation, leading to a decline in model performance in real-world
medical FL settings. To address data variability in distributed healthcare
systems, we introduce Federated Meta-Learning for Personalized Medication
(FedMetaMed), which combines federated learning and meta-learning to create
models that adapt to diverse patient data across healthcare systems. The
FedMetaMed framework aims to produce superior personalized models for
individual clients by addressing these limitations. Specifically, we introduce
Cumulative Fourier Aggregation (CFA) at the server to improve stability and
effectiveness in global knowledge aggregation. CFA achieves this by gradually
integrating client models from low to high frequencies. At the client level, we
implement a Collaborative Transfer Optimization (CTO) strategy with a
three-step process - Retrieve, Reciprocate, and Refine - to enhance the
personalized local model through seamless global knowledge transfer.
Experiments on real-world medical imaging datasets demonstrate that FedMetaMed
outperforms state-of-the-art FL methods, showing superior generalization even
on out-of-distribution cohorts.

摘要：個人化醫療旨在針對個別患者特徵調整醫療保健。然而，醫療系統中患者資料的異質性對達成準確且有效的個人化治療帶來重大挑戰。倫理問題進一步使來自不同機構的大量資料的彙總複雜化。聯邦學習 (FL) 提供了一種有前景的分散式解決方案，透過交換客戶模型而非原始資料來實現協作模型訓練，從而保護隱私。然而，現有的 FL 方法在伺服器彙總期間經常遭受退化，導致實際醫療 FL 設定中的模型效能下降。為了解決分散式醫療系統中的資料變異性，我們引入了個人化藥物聯邦元學習 (FedMetaMed)，它結合了聯邦學習和元學習來建立模型，以適應醫療系統中不同的患者資料。FedMetaMed 框架旨在透過解決這些限制，為個別客戶產生優越的個人化模型。具體來說，我們在伺服器端引入了累積傅立葉彙總 (CFA)，以改善全球知識彙總的穩定性和有效性。CFA 透過逐步整合從低頻率到高頻率的客戶模型來實現這一點。在客戶端層級，我們實施了一種協作傳輸最佳化 (CTO) 策略，採用三步驟流程 - 擷取、回饋和精煉 - 透過無縫的全球知識傳輸來增強個人化本地模型。在實際醫療影像資料集上的實驗表明，FedMetaMed 優於最先進的 FL 方法，即使在非分佈群組中也展現出優越的泛化性。

##### **Educational-Psychological Dialogue Robot Based on Multi-Agent Collaboration**
2412.03847v1 by Shiwen Ni, Min Yang

Intelligent dialogue systems are increasingly used in modern education and
psychological counseling fields, but most existing systems are limited to a
single domain, cannot deal with both educational and psychological issues, and
often lack accuracy and professionalism when dealing with complex issues. To
address these problems, this paper proposes an intelligent dialog system that
combines educational and psychological counseling functions. The system
consists of multiple AI agent, including security detection agent, intent
identification agent, educational LLM agent, and psychological LLM agent, which
work in concert to ensure the provision of accurate educational knowledge Q\&A
and psychological support services. Specifically, the system recognizes
user-input intentions through an intention classification model and invokes a
retrieval-enhanced educational grand model and a psychological grand model
fine-tuned with psychological data in order to provide professional educational
advice and psychological support.

摘要：智慧型對話系統在現代教育和心理諮商領域的使用日益廣泛，但現有系統大多侷限於單一領域，無法同時處理教育和心理議題，且在處理複雜問題時往往缺乏準確性和專業性。針對這些問題，本文提出一個結合教育和心理諮商功能的智慧型對話系統。系統由多個 AI 代理組成，包括安全偵測代理、意圖識別代理、教育 LLM 代理和心理 LLM 代理，共同協作以確保提供準確的教育知識問答和心理支持服務。具體而言，系統透過意圖分類模型識別使用者輸入的意圖，並呼叫以心理資料微調的檢索增強式教育大模型和心理大模型，以提供專業的教育建議和心理支持。

##### **HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting**
2412.03844v1 by Jingyu Lin, Jiaqi Gu, Lubin Fan, Bojian Wu, Yujing Lou, Renjie Chen, Ligang Liu, Jieping Ye

Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS)
in scenes featuring transient objects is challenging. We propose a novel hybrid
representation, termed as HybridGS, using 2D Gaussians for transient objects
per image and maintaining traditional 3D Gaussians for the whole static scenes.
Note that, the 3DGS itself is better suited for modeling static scenes that
assume multi-view consistency, but the transient objects appear occasionally
and do not adhere to the assumption, thus we model them as planar objects from
a single view, represented with 2D Gaussians. Our novel representation
decomposes the scene from the perspective of fundamental viewpoint consistency,
making it more reasonable. Additionally, we present a novel multi-view
regulated supervision method for 3DGS that leverages information from
co-visible regions, further enhancing the distinctions between the transients
and statics. Then, we propose a straightforward yet effective multi-stage
training strategy to ensure robust training and high-quality view synthesis
across various settings. Experiments on benchmark datasets show our
state-of-the-art performance of novel view synthesis in both indoor and outdoor
scenes, even in the presence of distracting elements.

摘要：在包含瞬態物體的場景中產生高品質的新視圖渲染 3D 高斯潑濺 (3DGS) 具有挑戰性。我們提出一個新穎的混合表示，稱為 HybridGS，使用 2D 高斯函數表示每個影像中的瞬態物體，並維持傳統的 3D 高斯函數表示整個靜態場景。請注意，3DGS 本身更適合於假設多視圖一致性的靜態場景建模，但瞬態物體偶爾出現且不符合此假設，因此我們將它們建模為來自單一視圖的平面物體，並使用 2D 高斯函數表示。我們的新穎表示從基本視點一致性的角度分解場景，使其更合理。此外，我們提出一個新穎的多視圖調節監督方法，用於 3DGS，該方法利用來自共視區域的資訊，進一步增強瞬態和靜態之間的區別。然後，我們提出一個簡單但有效的多分段訓練策略，以確保穩健的訓練和在各種設定中進行高品質的視圖合成。基準資料集上的實驗顯示，我們在室內和室外場景中進行新視圖合成的最先進效能，即使在存在干擾元素的情況下也是如此。

##### **LL-ICM: Image Compression for Low-level Machine Vision via Large Vision-Language Model**
2412.03841v1 by Yuan Xue, Qi Zhang, Chuanmin Jia, Shiqi Wang

Image Compression for Machines (ICM) aims to compress images for machine
vision tasks rather than human viewing. Current works predominantly concentrate
on high-level tasks like object detection and semantic segmentation. However,
the quality of original images is usually not guaranteed in the real world,
leading to even worse perceptual quality or downstream task performance after
compression. Low-level (LL) machine vision models, like image restoration
models, can help improve such quality, and thereby their compression
requirements should also be considered. In this paper, we propose a pioneered
ICM framework for LL machine vision tasks, namely LL-ICM. By jointly optimizing
compression and LL tasks, the proposed LL-ICM not only enriches its encoding
ability in generalizing to versatile LL tasks but also optimizes the processing
ability of down-stream LL task models, achieving mutual adaptation for image
codecs and LL task models. Furthermore, we integrate large-scale
vision-language models into the LL-ICM framework to generate more universal and
distortion-robust feature embeddings for LL vision tasks. Therefore, one LL-ICM
codec can generalize to multiple tasks. We establish a solid benchmark to
evaluate LL-ICM, which includes extensive objective experiments by using both
full and no-reference image quality assessments. Experimental results show that
LL-ICM can achieve 22.65% BD-rate reductions over the state-of-the-art methods.

摘要：機器影像壓縮 (ICM) 旨在壓縮影像以進行機器視覺任務，而非人類觀看。目前的著作主要集中於高階任務，例如物體偵測與語意分割。然而，在真實世界中，原始影像的品質通常無法保證，導致壓縮後產生更差的感知品質或下游任務效能。低階 (LL) 機器視覺模型，例如影像修復模型，有助於提升品質，因此也應考量其壓縮需求。在本文中，我們提出一個針對 LL 機器視覺任務的先驅 ICM 架構，即 LL-ICM。透過聯合最佳化壓縮與 LL 任務，所提出的 LL-ICM 不僅豐富其編碼能力以概化到多樣的 LL 任務，同時也最佳化下游 LL 任務模型的處理能力，達成影像編解碼器與 LL 任務模型的相互適應。此外，我們將大規模視覺語言模型整合到 LL-ICM 架構中，以產生更通用的且抗失真的特徵嵌入，適用於 LL 視覺任務。因此，一個 LL-ICM 編解碼器可以概化到多項任務。我們建立一個穩固的基準來評估 LL-ICM，其中包含廣泛的客觀實驗，同時使用完整和無參考影像品質評估。實驗結果顯示，與目前最先進的方法相比，LL-ICM 可減少 22.65% 的 BD 速率。

##### **Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries**
2412.03837v1 by Abul Ehtesham, Saket Kumar, Aditi Singh, Tala Talaei Khoei

Generative AI is reshaping the media landscape, enabling unprecedented
capabilities in video creation, personalization, and scalability. This paper
presents a comprehensive SWOT analysis of Metas Movie Gen, a cutting-edge
generative AI foundation model designed to produce 1080p HD videos with
synchronized audio from simple text prompts. We explore its strengths,
including high-resolution video generation, precise editing, and seamless audio
integration, which make it a transformative tool across industries such as
filmmaking, advertising, and education. However, the analysis also addresses
limitations, such as constraints on video length and potential biases in
generated content, which pose challenges for broader adoption. In addition, we
examine the evolving regulatory and ethical considerations surrounding
generative AI, focusing on issues like content authenticity, cultural
representation, and responsible use. Through comparative insights with leading
models like DALL-E and Google Imagen, this paper highlights Movie Gens unique
features, such as video personalization and multimodal synthesis, while
identifying opportunities for innovation and areas requiring further research.
Our findings provide actionable insights for stakeholders, emphasizing both the
opportunities and challenges of deploying generative AI in media production.
This work aims to guide future advancements in generative AI, ensuring
scalability, quality, and ethical integrity in this rapidly evolving field.

摘要：生成式 AI 正在重塑媒體版圖，在影片製作、個人化和可擴充性方面提供了前所未有的功能。本文全面分析了 Meta 的 Movie Gen，這是一個尖端的生成式 AI 基礎模型，旨在根據簡單的文字提示製作 1080p HD 影片，並配有同步音訊。我們探討了它的優勢，包括高解析度影片生成、精確編輯和無縫音訊整合，這讓它成為電影製作、廣告和教育等產業的變革性工具。然而，分析也探討了限制，例如影片長度限制和生成內容中的潛在偏差，這些限制對更廣泛的採用構成挑戰。此外，我們探討了圍繞生成式 AI 的不斷演進的法規和倫理考量，重點關注內容真實性、文化表徵和負責任使用等議題。透過與 DALL-E 和 Google Imagen 等領先模型的比較見解，本文突出了 Movie Gen 的獨特功能，例如影片個人化和多模態合成，同時找出創新機會和需要進一步研究的領域。我們的發現為利害關係人提供了可行的見解，強調了在媒體製作中部署生成式 AI 的機會和挑戰。這項工作旨在引導生成式 AI 的未來進展，確保在這個快速演進的領域中實現可擴充性、品質和倫理誠信。

##### **Towards Data Governance of Frontier AI Models**
2412.03824v1 by Jason Hausenloy, Duncan McClements, Madhavendra Thakur

Data is essential to train and fine-tune today's frontier artificial
intelligence (AI) models and to develop future ones. To date, academic, legal,
and regulatory work has primarily addressed how data can directly harm
consumers and creators, such as through privacy breaches, copyright
infringements, and bias and discrimination. Our work, instead, focuses on the
comparatively neglected question of how data can enable new governance
capacities for frontier AI models. This approach for "frontier data governance"
opens up new avenues for monitoring and mitigating risks from advanced AI
models, particularly as they scale and acquire specific dangerous capabilities.
Still, frontier data governance faces challenges that stem from the fundamental
properties of data itself: data is non-rival, often non-excludable, easily
replicable, and increasingly synthesizable. Despite these inherent
difficulties, we propose a set of policy mechanisms targeting key actors along
the data supply chain, including data producers, aggregators, model developers,
and data vendors. We provide a brief overview of 15 governance mechanisms, of
which we centrally introduce five, underexplored policy recommendations. These
include developing canary tokens to detect unauthorized use for producers;
(automated) data filtering to remove malicious content for pre-training and
post-training datasets; mandatory dataset reporting requirements for developers
and vendors; improved security for datasets and data generation algorithms; and
know-your-customer requirements for vendors. By considering data not just as a
source of potential harm, but as a critical governance lever, this work aims to
equip policymakers with a new tool for the governance and regulation of
frontier AI models.

摘要：<paragraph>資料對於訓練和微調現今最前沿的人工智慧 (AI) 模型，以及開發未來的模型至關重要。迄今為止，學術、法律和法規工作主要探討資料如何直接損害消費者和創作者，例如透過隱私權外洩、侵犯著作權以及偏見和歧視。然而，我們的研究專注於一個相對被忽略的問題：資料如何為最前沿的 AI 模型啟用新的治理能力。這種「最前沿資料治理」方法開啟了新的途徑，用於監控和降低先進 AI 模型的風險，特別是在這些模型擴展規模並獲取特定危險能力時。儘管如此，最前沿資料治理仍面臨著源自資料本身基本屬性的挑戰：資料是非競爭性的、通常是非排他性的、容易複製的，而且越來越容易合成。儘管存在這些固有的困難，我們提出了一組政策機制，針對資料供應鏈中的關鍵參與者，包括資料生產者、彙總者、模型開發者和資料供應商。我們簡要概述了 15 項治理機制，其中我們重點介紹了五項未被充分探討的政策建議。這些建議包括：為生產者開發金絲雀代幣以偵測未經授權的使用；(自動) 資料過濾以移除預訓練和後訓練資料集中的惡意內容；強制性資料集報告要求，適用於開發者和供應商；改善資料集和資料生成演算法的安全性；以及供應商的認識客戶要求。透過將資料視為不僅是潛在危害的來源，也是重要的治理槓桿，這項研究旨在為政策制定者提供一個新的工具，用於最前沿 AI 模型的治理和法規。</paragraph>

##### **Beyond the Binary: Capturing Diverse Preferences With Reward Regularization**
2412.03822v1 by Vishakh Padmakumar, Chuanyang Jin, Hannah Rose Kirk, He He

Large language models (LLMs) are increasingly deployed via public-facing
interfaces to interact with millions of users, each with diverse preferences.
Despite this, preference tuning of LLMs predominantly relies on reward models
trained using binary judgments where annotators select the preferred choice out
of pairs of model outputs. In this work, we argue that this reliance on binary
choices does not capture the broader, aggregate preferences of the target user
in real-world tasks. We propose a taxonomy that identifies two dimensions of
subjectivity where different users disagree on the preferred output-namely, the
Plurality of Responses to Prompts, where prompts allow for multiple correct
answers, and the Indistinguishability of Responses, where candidate outputs are
paraphrases of each other. We show that reward models correlate weakly with
user preferences in these cases. As a first step to address this issue, we
introduce a simple yet effective method that augments existing binary
preference datasets with synthetic preference judgments to estimate potential
user disagreement. Incorporating these via a margin term as a form of
regularization during model training yields predictions that better align with
the aggregate user preferences.

摘要：大型語言模型 (LLM) 透過面向公眾的介面進行部署，與數百萬使用者互動，而每位使用者的偏好皆不相同。儘管如此，LLM 的偏好調整主要依賴於獎勵模型，而獎勵模型則是使用二元判斷進行訓練，其中註解者從模型輸出的成對選項中選出偏好的選項。在這項工作中，我們主張依賴二元選擇無法捕捉到目標使用者在實際任務中的廣泛、綜合偏好。我們提出了一個分類法，其中找出兩個主觀性向度，在這些向度中，不同的使用者對於偏好的輸出意見不一，即對提示的多元回應，其中提示允許多個正確答案，以及回應的難以區分，其中候選輸出彼此互為同義反覆。我們顯示，在這些情況下，獎勵模型與使用者偏好的相關性較弱。作為解決此問題的第一步，我們引入一種簡單卻有效的方法，該方法使用合成偏好判斷來擴充現有的二元偏好資料集，以估計潛在的使用者分歧。將這些透過邊際項納入模型訓練中的一種正規化形式，可產生更符合綜合使用者偏好的預測。

##### **Detecting Redundant Health Survey Questions Using Language-agnostic BERT Sentence Embedding (LaBSE)**
2412.03817v1 by Sunghoon Kang, Hyeoneui Kim, Hyewon Park, Ricky Taira

The goal of this work was to compute the semantic similarity among publicly
available health survey questions in order to facilitate the standardization of
survey-based Person-Generated Health Data (PGHD). We compiled various health
survey questions authored in both English and Korean from the NIH CDE
Repository, PROMIS, Korean public health agencies, and academic publications.
Questions were drawn from various health lifelog domains. A randomized question
pairing scheme was used to generate a Semantic Text Similarity (STS) dataset
consisting of 1758 question pairs. Similarity scores between each question pair
were assigned by two human experts. The tagged dataset was then used to build
three classifiers featuring: Bag-of-Words, SBERT with BERT-based embeddings,
and SBRET with LaBSE embeddings. The algorithms were evaluated using
traditional contingency statistics. Among the three algorithms, SBERT-LaBSE
demonstrated the highest performance in assessing question similarity across
both languages, achieving an Area Under the Receiver Operating Characteristic
(ROC) and Precision-Recall Curves of over 0.99. Additionally, it proved
effective in identifying cross-lingual semantic similarities.The SBERT-LaBSE
algorithm excelled at aligning semantically equivalent sentences across both
languages but encountered challenges in capturing subtle nuances and
maintaining computational efficiency. Future research should focus on testing
with larger multilingual datasets and on calibrating and normalizing scores
across the health lifelog domains to improve consistency. This study introduces
the SBERT-LaBSE algorithm for calculating semantic similarity across two
languages, showing it outperforms BERT-based models and the Bag of Words
approach, highlighting its potential to improve semantic interoperability of
survey-based PGHD across language barriers.

摘要：本研究旨在計算公開健康調查問卷之間的語義相似性，以便促進基於調查的個人生成健康數據 (PGHD) 的標準化。我們從 NIH CDE 儲存庫、PROMIS、韓國公共衛生機構和學術出版物中編譯了各種以英語和韓語撰寫的健康調查問卷。問題來自於各種健康生活日誌領域。隨機問題配對方案用於生成語義文本相似度 (STS) 資料集，其中包含 1758 個問題配對。每個問題配對之間的相似度分數由兩位人類專家指定。然後使用標記資料集來建立三個分類器，其特點是：詞袋、基於 BERT 的嵌入的 SBERT 和基於 LaBSE 嵌入的 SBRET。使用傳統的或然率統計來評估演算法。在三個演算法中，SBERT-LaBSE 在評估兩種語言之間的問題相似性方面表現出最高的效能，在接收器操作特徵 (ROC) 下方區域和精確度召回曲線達到 0.99 以上。此外，它被證明在識別跨語言語義相似性方面有效。SBERT-LaBSE 演算法擅長對齊兩種語言中語義等價的句子，但在捕捉細微差別和維持運算效率方面遇到挑戰。未來的研究應重點測試更大的多語言資料集，並校準和標準化健康生活日誌領域的分數以提高一致性。本研究引入了 SBERT-LaBSE 演算法，用於計算兩種語言之間的語義相似性，表明它優於基於 BERT 的模型和詞袋法，強調了它在跨語言障礙改善基於調查的 PGHD 的語義互操作性的潛力。

##### **Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software Repository-Related Question Answering**
2412.03815v1 by Samuel Abedu, SayedHassan Khatoonabadi, Emad Shihab

Software repositories contain valuable information for gaining insights into
their development process. However, extracting insights from these repository
data is time-consuming and requires technical expertise. While software
engineering chatbots have been developed to facilitate natural language
interactions with repositories, they struggle with understanding natural
language and accurately retrieving relevant data. This study aims to improve
the accuracy of LLM-based chatbots in answering repository-related questions by
augmenting them with knowledge graphs. We achieve this in a two-step approach;
(1) constructing a knowledge graph from the repository data and (2) synergizing
the knowledge graph with LLM to allow for the natural language questions and
answers. We curated a set of 20 questions with different complexities and
evaluated our approach on five popular open-source projects. Our approach
achieved an accuracy of 65%. We further investigated the limitations and
identified six key issues, with the majority relating to the reasoning
capability of the LLM. We experimented with a few-shot chain-of-thought
prompting to determine if it could enhance our approach. This technique
improved the overall accuracy to 84%. Our findings demonstrate the synergy
between LLMs and knowledge graphs as a viable solution for making repository
data accessible to both technical and non-technical stakeholders.

摘要：軟體儲存庫包含有價值的資訊，可深入了解其開發流程。然而，從這些儲存庫資料中擷取見解既耗時又需要技術專業知識。儘管已開發出軟體工程聊天機器人來促進與儲存庫的自然語言互動，但它們在理解自然語言和準確擷取相關資料方面仍有困難。本研究旨在透過知識圖譜擴充 LLM 基礎聊天機器人，以提高其回答儲存庫相關問題的準確性。我們採用兩步驟方法來達成此目標：(1) 從儲存庫資料建構知識圖譜，以及 (2) 將知識圖譜與 LLM 結合，以允許自然語言問題和答案。我們策劃了一組 20 個具有不同複雜度的問題，並針對五個熱門的開源專案評估我們的做法。我們的做法達到了 65% 的準確度。我們進一步探討了限制，並找出六個關鍵問題，其中大部分與 LLM 的推理能力有關。我們實驗了少次數的思考鏈提示，以確定它是否可以增強我們的做法。此技術將整體準確度提高到 84%。我們的研究結果證明了 LLM 和知識圖譜之間的協同效應，作為讓技術和非技術利害關係人能夠存取儲存庫資料的可行解決方案。

