{"2502.14553": {"publish_time": "2025-02-20", "title": "Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling", "paper_summary": "Bytes form the basis of the digital world and thus are a promising building\nblock for multimodal foundation models. Recently, Byte Language Models (BLMs)\nhave emerged to overcome tokenization, yet the excessive length of bytestreams\nrequires new architectural paradigms. Therefore, we present the Multiscale Byte\nLanguage Model (MBLM), a model-agnostic hierarchical decoder stack that allows\ntraining with context windows of $5$M bytes on single GPU in full model\nprecision. We thoroughly examine MBLM's performance with Transformer and Mamba\nblocks on both unimodal and multimodal tasks. Our experiments demonstrate that\nhybrid architectures are efficient in handling extremely long byte sequences\nduring training while achieving near-linear generational efficiency. To the\nbest of our knowledge, we present the first evaluation of BLMs on visual Q\\&A\ntasks and find that, despite serializing images and the absence of an encoder,\na MBLM with pure next token prediction can match custom CNN-LSTM architectures\nwith designated classification heads. We show that MBLMs exhibit strong\nadaptability in integrating diverse data representations, including pixel and\nimage filestream bytes, underlining their potential toward omnimodal foundation\nmodels. Source code is publicly available at:\nhttps://github.com/ai4sd/multiscale-byte-lm", "paper_summary_zh": "\u4f4d\u5143\u7d44\u69cb\u6210\u6578\u4f4d\u4e16\u754c\u7684\u57fa\u790e\uff0c\u56e0\u6b64\u662f\u591a\u6a21\u614b\u57fa\u790e\u6a21\u578b\u7684\u4e00\u500b\u6709\u524d\u9014\u7684\u5efa\u69cb\u6a21\u7d44\u3002\u6700\u8fd1\uff0c\u4f4d\u5143\u7d44\u8a9e\u8a00\u6a21\u578b (BLM) \u5df2\u61c9\u904b\u800c\u751f\uff0c\u4ee5\u514b\u670d\u6a19\u8a18\u5316\uff0c\u4f46\u4f4d\u5143\u7d44\u4e32\u6d41\u7684\u904e\u9577\u9700\u8981\u65b0\u7684\u67b6\u69cb\u7bc4\u4f8b\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u591a\u5c3a\u5ea6\u4f4d\u5143\u7d44\u8a9e\u8a00\u6a21\u578b (MBLM)\uff0c\u9019\u662f\u4e00\u500b\u8207\u6a21\u578b\u7121\u95dc\u7684\u5206\u5c64\u89e3\u78bc\u5668\u5806\u758a\uff0c\u5141\u8a31\u5728\u55ae\u4e00 GPU \u4e0a\u4ee5\u5b8c\u6574\u7684\u6a21\u578b\u7cbe\u5ea6\u8a13\u7df4 500 \u842c\u4f4d\u5143\u7d44\u7684\u5167\u5bb9\u8996\u7a97\u3002\u6211\u5011\u5fb9\u5e95\u6aa2\u9a57\u4e86 MBLM \u5728\u55ae\u6a21\u614b\u548c\u591a\u6a21\u614b\u4efb\u52d9\u4e0a\u4f7f\u7528 Transformer \u548c Mamba \u5340\u584a\u7684\u6548\u80fd\u3002\u6211\u5011\u7684\u5be6\u9a57\u8b49\u660e\uff0c\u6df7\u5408\u67b6\u69cb\u5728\u8655\u7406\u8a13\u7df4\u671f\u9593\u6975\u9577\u7684\u4f4d\u5143\u7d44\u5e8f\u5217\u6642\u5f88\u6709\u6548\u7387\uff0c\u540c\u6642\u9054\u5230\u8fd1\u4e4e\u7dda\u6027\u7684\u751f\u6210\u6548\u7387\u3002\u64da\u6211\u5011\u6240\u77e5\uff0c\u6211\u5011\u63d0\u51fa\u5728\u8996\u89ba\u554f\u7b54\u4efb\u52d9\u4e0a\u5c0d BLM \u7684\u9996\u6b21\u8a55\u4f30\uff0c\u4e26\u767c\u73fe\uff0c\u5118\u7ba1\u5e8f\u5217\u5316\u5f71\u50cf\u4e14\u6c92\u6709\u7de8\u78bc\u5668\uff0c\u4f46\u5177\u6709\u7d14\u7cb9\u4e0b\u4e00\u500b\u6a19\u8a18\u9810\u6e2c\u7684 MBLM \u53ef\u4ee5\u5339\u914d\u5177\u6709\u6307\u5b9a\u5206\u985e\u6a19\u982d\u7684\u5ba2\u88fd\u5316 CNN-LSTM \u67b6\u69cb\u3002\u6211\u5011\u8868\u660e\uff0cMBLM \u5728\u6574\u5408\u5404\u7a2e\u8cc7\u6599\u8868\u793a\u5f62\u5f0f\u65b9\u9762\u8868\u73fe\u51fa\u5f37\u5927\u7684\u9069\u61c9\u6027\uff0c\u5305\u62ec\u50cf\u7d20\u548c\u5f71\u50cf\u6a94\u6848\u4e32\u6d41\u4f4d\u5143\u7d44\uff0c\u5f37\u8abf\u5b83\u5011\u671d\u5411\u5168\u6a21\u614b\u57fa\u790e\u6a21\u578b\u7684\u6f5b\u529b\u3002\u539f\u59cb\u78bc\u5df2\u516c\u958b\u65bc\uff1a\nhttps://github.com/ai4sd/multiscale-byte-lm", "author": "Eric Egli et.al.", "authors": "Eric Egli, Matteo Manica, Jannis Born", "id": "2502.14553v1", "paper_url": "http://arxiv.org/abs/2502.14553v1", "repo": "null"}}