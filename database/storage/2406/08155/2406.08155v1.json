{"2406.08155": {"publish_time": "2024-06-12", "title": "Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark", "paper_summary": "Large Language Models~(LLMs) have become foundational in the realm of natural\nlanguage processing, demonstrating performance improvements as model sizes\nincrease. The Mixture-of-Experts~(MoE) approach offers a promising way to scale\nLLMs more efficiently by using fewer computational FLOPs through sparse\nactivation. However, it suffers from significant memory overheads,\nnecessitating model compression techniques. Post-training quantization, a\npopular method for model compression, proves less effective when directly\napplied to MoE models due to MoE's overlooked inherent sparsity. This paper\nexplores several MoE structure-aware quantization heuristics, ranging from\ncoarse to fine granularity, from MoE block to individual linear weight. Our\ninvestigations reveal critical principles: different MoE structures (i.e.,\nblocks, experts, linear layers) require varying numbers of weight bits for\neffective and efficient quantization. Conclusions are supported by extensive\nbenchmarking across two representative MoE models and six tasks. We further\nintroduce novel enhancements to more accurately identify the most critical\nweights in MoE quantization that necessitate higher bit allocations, including\nthe linear weight outlier scorer and MoE block scorer. Additionally, subsequent\nexperiments validate our findings in the context of both weight and activation\nquantization.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u6210\u70ba\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u9818\u57df\u7684\u57fa\u790e\uff0c\u96a8\u8457\u6a21\u578b\u898f\u6a21\u7684\u589e\u52a0\uff0c\u6548\u80fd\u8868\u73fe\u4e5f\u6709\u6240\u63d0\u5347\u3002\u6df7\u5408\u5c08\u5bb6 (MoE) \u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u7a2e\u66f4\u6709\u6548\u7387\u7684 LLM \u64f4\u5145\u65b9\u5f0f\uff0c\u900f\u904e\u7a00\u758f\u6fc0\u6d3b\u4f7f\u7528\u8f03\u5c11\u7684\u904b\u7b97\u6d6e\u9ede\u904b\u7b97 (FLOP)\u3002\u7136\u800c\uff0c\u5b83\u6703\u9020\u6210\u986f\u8457\u7684\u8a18\u61b6\u9ad4\u958b\u92b7\uff0c\u56e0\u6b64\u9700\u8981\u6a21\u578b\u58d3\u7e2e\u6280\u8853\u3002\u8a13\u7df4\u5f8c\u91cf\u5316\u662f\u6a21\u578b\u58d3\u7e2e\u7684\u71b1\u9580\u65b9\u6cd5\uff0c\u4f46\u7531\u65bc\u5ffd\u7565\u4e86 MoE \u7684\u5167\u5728\u7a00\u758f\u6027\uff0c\u76f4\u63a5\u61c9\u7528\u65bc MoE \u6a21\u578b\u6642\u6548\u679c\u8f03\u5dee\u3002\u672c\u6587\u63a2\u8a0e\u4e86\u591a\u7a2e\u8003\u91cf MoE \u7d50\u69cb\u7684\u91cf\u5316\u555f\u767c\u5f0f\u65b9\u6cd5\uff0c\u7bc4\u570d\u5f9e\u7c97\u7565\u5230\u7cbe\u7d30\u7c92\u5ea6\uff0c\u5f9e MoE \u5340\u584a\u5230\u500b\u5225\u7dda\u6027\u6b0a\u91cd\u3002\u6211\u5011\u7684\u8abf\u67e5\u63ed\u9732\u4e86\u91cd\u8981\u7684\u539f\u5247\uff1a\u4e0d\u540c\u7684 MoE \u7d50\u69cb\uff08\u5373\u5340\u584a\u3001\u5c08\u5bb6\u3001\u7dda\u6027\u5c64\uff09\u9700\u8981\u4e0d\u540c\u6578\u91cf\u7684\u6b0a\u91cd\u4f4d\u5143\uff0c\u624d\u80fd\u9032\u884c\u6709\u6548\u4e14\u9ad8\u6548\u7684\u91cf\u5316\u3002\u7d50\u8ad6\u7372\u5f97\u4e86\u5169\u500b\u5177\u4ee3\u8868\u6027\u7684 MoE \u6a21\u578b\u548c\u516d\u9805\u4efb\u52d9\u7684\u5ee3\u6cdb\u57fa\u6e96\u6e2c\u8a66\u652f\u6301\u3002\u6211\u5011\u9032\u4e00\u6b65\u5f15\u5165\u4e86\u65b0\u7a4e\u7684\u5f37\u5316\u529f\u80fd\uff0c\u4ee5\u66f4\u6e96\u78ba\u5730\u8b58\u5225 MoE \u91cf\u5316\u4e2d\u9700\u8981\u8f03\u9ad8\u4f4d\u5143\u914d\u7f6e\u7684\u6700\u95dc\u9375\u6b0a\u91cd\uff0c\u5305\u62ec\u7dda\u6027\u6b0a\u91cd\u7570\u5e38\u503c\u8a55\u5206\u5668\u548c MoE \u5340\u584a\u8a55\u5206\u5668\u3002\u6b64\u5916\uff0c\u5f8c\u7e8c\u5be6\u9a57\u9a57\u8b49\u4e86\u6211\u5011\u7684\u767c\u73fe\uff0c\u540c\u6642\u8003\u91cf\u4e86\u6b0a\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\u3002", "author": "Pingzhi Li et.al.", "authors": "Pingzhi Li, Xiaolong Jin, Yu Cheng, Tianlong Chen", "id": "2406.08155v1", "paper_url": "http://arxiv.org/abs/2406.08155v1", "repo": "null"}}