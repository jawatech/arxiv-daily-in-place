{"2407.10747": {"publish_time": "2024-07-15", "title": "Codebook LLMs: Adapting Political Science Codebooks for LLM Use and Adapting LLMs to Follow Codebooks", "paper_summary": "Codebooks -- documents that operationalize constructs and outline annotation\nprocedures -- are used almost universally by social scientists when coding\nunstructured political texts. Recently, to reduce manual annotation costs,\npolitical scientists have looked to generative large language models (LLMs) to\nlabel and analyze text data. However, previous work using LLMs for\nclassification has implicitly relied on the universal label assumption --\ncorrect classification of documents is possible using only a class label or\nminimal definition and the information that the LLM inductively learns during\nits pre-training. In contrast, we argue that political scientists who care\nabout valid measurement should instead make a codebook-construct label\nassumption -- an LLM should follow the definition and exclusion criteria of a\nconstruct/label provided in a codebook. In this work, we collect and curate\nthree political science datasets and their original codebooks and conduct a set\nof experiments to understand whether LLMs comply with codebook instructions,\nwhether rewriting codebooks improves performance, and whether\ninstruction-tuning LLMs on codebook-document-label tuples improves performance\nover zero-shot classification. Using Mistral 7B Instruct as our LLM, we find\nre-structuring the original codebooks gives modest gains in zero-shot\nperformance but the model still struggles to comply with the constraints of the\ncodebooks. Optimistically, instruction-tuning Mistral on one of our datasets\ngives significant gains over zero-shot inference (0.76 versus 0.53 micro F1).\nWe hope our conceptualization of the codebook-specific task, assumptions, and\ninstruction-tuning pipeline as well our semi-structured LLM codebook format\nwill help political scientists readily adapt to the LLM era.", "paper_summary_zh": "<paragraph>\u7de8\u78bc\u624b\u518a\u2014\u2014\u5c07\u7d50\u69cb\u5316\u6982\u5ff5\u5177\u9ad4\u5316\u4e26\u6982\u8ff0\u6a19\u8a3b\u7a0b\u5e8f\u7684\u6587\u4ef6\u2014\u2014\u5728\u793e\u6703\u79d1\u5b78\u5bb6\u5c0d\u975e\u7d50\u69cb\u5316\u653f\u6cbb\u6587\u672c\u9032\u884c\u7de8\u78bc\u6642\u5e7e\u4e4e\u666e\u904d\u4f7f\u7528\u3002\u6700\u8fd1\uff0c\u70ba\u4e86\u964d\u4f4e\u624b\u52d5\u6a19\u8a3b\u6210\u672c\uff0c\u653f\u6cbb\u5b78\u5bb6\u958b\u59cb\u95dc\u6ce8\u751f\u6210\u5f0f\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff0c\u4ee5\u6a19\u8a18\u548c\u5206\u6790\u6587\u672c\u6578\u64da\u3002\u7136\u800c\uff0c\u5148\u524d\u4f7f\u7528 LLM \u9032\u884c\u5206\u985e\u7684\u5de5\u4f5c\u96b1\u542b\u5730\u4f9d\u8cf4\u65bc\u901a\u7528\u6a19\u7c64\u5047\u8a2d\u2014\u2014\u50c5\u4f7f\u7528\u985e\u5225\u6a19\u7c64\u6216\u6700\u5c0f\u5b9a\u7fa9\u4ee5\u53ca LLM \u5728\u9810\u8a13\u7df4\u671f\u9593\u6b78\u7d0d\u5b78\u7fd2\u7684\u4fe1\u606f\u5373\u53ef\u6b63\u78ba\u5c0d\u6587\u4ef6\u9032\u884c\u5206\u985e\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u5011\u8a8d\u70ba\u91cd\u8996\u6709\u6548\u6e2c\u91cf\u7684\u653f\u6cbb\u5b78\u5bb6\u61c9\u8a72\u6539\u70ba\u505a\u51fa\u7de8\u78bc\u624b\u518a-\u7d50\u69cb\u6a19\u7c64\u5047\u8a2d\u2014\u2014LLM \u61c9\u9075\u5faa\u7de8\u78bc\u624b\u518a\u4e2d\u63d0\u4f9b\u7684\u7d50\u69cb/\u6a19\u7c64\u7684\u5b9a\u7fa9\u548c\u6392\u9664\u6a19\u6e96\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u6536\u96c6\u4e26\u6574\u7406\u4e86\u4e09\u500b\u653f\u6cbb\u79d1\u5b78\u6578\u64da\u96c6\u53ca\u5176\u539f\u59cb\u7de8\u78bc\u624b\u518a\uff0c\u4e26\u9032\u884c\u4e86\u4e00\u7cfb\u5217\u5be6\u9a57\uff0c\u4ee5\u4e86\u89e3 LLM \u662f\u5426\u7b26\u5408\u7de8\u78bc\u624b\u518a\u8aaa\u660e\u3001\u91cd\u5beb\u7de8\u78bc\u624b\u518a\u662f\u5426\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4ee5\u53ca\u5728\u7de8\u78bc\u624b\u518a-\u6587\u4ef6-\u6a19\u7c64\u5143\u7d44\u4e0a\u5c0d LLM \u9032\u884c\u6307\u4ee4\u5fae\u8abf\u662f\u5426\u80fd\u63d0\u5347\u96f6\u6b21\u5206\u985e\u7684\u6027\u80fd\u3002\u4f7f\u7528 Mistral 7B Instruct \u4f5c\u70ba\u6211\u5011\u7684 LLM\uff0c\u6211\u5011\u767c\u73fe\u91cd\u65b0\u67b6\u69cb\u539f\u59cb\u7de8\u78bc\u624b\u518a\u5728\u96f6\u6b21\u5206\u985e\u6027\u80fd\u65b9\u9762\u5e36\u4f86\u4e86\u9069\u5ea6\u7684\u63d0\u5347\uff0c\u4f46\u6a21\u578b\u4ecd\u7136\u96e3\u4ee5\u7b26\u5408\u7de8\u78bc\u624b\u518a\u7684\u7d04\u675f\u3002\u6a02\u89c0\u5730\u8aaa\uff0c\u5728\u6211\u5011\u7684\u4e00\u500b\u6578\u64da\u96c6\u4e0a\u5c0d Mistral \u9032\u884c\u6307\u4ee4\u5fae\u8abf\u5728\u96f6\u6b21\u63a8\u8ad6\u4e0a\u5e36\u4f86\u4e86\u986f\u8457\u7684\u63d0\u5347\uff080.76 \u5c0d\u6bd4 0.53 \u5fae F1\uff09\u3002\u6211\u5011\u5e0c\u671b\u6211\u5011\u5c0d\u7de8\u78bc\u624b\u518a\u7279\u5b9a\u4efb\u52d9\u3001\u5047\u8a2d\u548c\u6307\u4ee4\u5fae\u8abf\u7ba1\u7dda\u7684\u6982\u5ff5\u5316\u4ee5\u53ca\u6211\u5011\u534a\u7d50\u69cb\u5316\u7684 LLM \u7de8\u78bc\u624b\u518a\u683c\u5f0f\u5c07\u5e6b\u52a9\u653f\u6cbb\u5b78\u5bb6\u8f15\u6613\u9069\u61c9 LLM \u6642\u4ee3\u3002</paragraph>", "author": "Andrew Halterman et.al.", "authors": "Andrew Halterman, Katherine A. Keith", "id": "2407.10747v1", "paper_url": "http://arxiv.org/abs/2407.10747v1", "repo": "null"}}