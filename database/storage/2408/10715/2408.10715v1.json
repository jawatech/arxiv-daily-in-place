{"2408.10715": {"publish_time": "2024-08-20", "title": "Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology", "paper_summary": "Generating physician letters is a time-consuming task in daily clinical\npractice. This study investigates local fine-tuning of large language models\n(LLMs), specifically LLaMA models, for physician letter generation in a\nprivacy-preserving manner within the field of radiation oncology. Our findings\ndemonstrate that base LLaMA models, without fine-tuning, are inadequate for\neffectively generating physician letters. The QLoRA algorithm provides an\nefficient method for local intra-institutional fine-tuning of LLMs with limited\ncomputational resources (i.e., a single 48 GB GPU workstation within the\nhospital). The fine-tuned LLM successfully learns radiation oncology-specific\ninformation and generates physician letters in an institution-specific style.\nROUGE scores of the generated summary reports highlight the superiority of the\n8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician\nevaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has\nlimited capacity to generate content beyond the provided input data, it\nsuccessfully generates salutations, diagnoses and treatment histories,\nrecommendations for further treatment, and planned schedules. Overall, clinical\nbenefit was rated highly by the clinical experts (average score of 3.44 on a\n4-point scale). With careful physician review and correction, automated\nLLM-based physician letter generation has significant practical value.", "paper_summary_zh": "<paragraph>\u5728\u65e5\u5e38\u81e8\u5e8a\u5be6\u52d9\u4e2d\uff0c\u751f\u6210\u91ab\u5e2b\u4fe1\u51fd\u662f\u4e00\u9805\u8017\u6642\u7684\u4efb\u52d9\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5c40\u90e8\u5fae\u8abf\uff0c\u7279\u5225\u662f LLaMA \u6a21\u578b\uff0c\u5728\u653e\u5c04\u816b\u7624\u5b78\u9818\u57df\u4e2d\u4ee5\u96b1\u79c1\u4fdd\u8b77\u7684\u65b9\u5f0f\u751f\u6210\u91ab\u5e2b\u4fe1\u51fd\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u8868\u660e\uff0c\u57fa\u790e LLaMA \u6a21\u578b\u5728\u6c92\u6709\u5fae\u8abf\u7684\u60c5\u6cc1\u4e0b\uff0c\u4e0d\u8db3\u4ee5\u6709\u6548\u751f\u6210\u91ab\u5e2b\u4fe1\u51fd\u3002QLoRA \u6f14\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u6709\u9650\u7684\u904b\u7b97\u8cc7\u6e90\uff08\u5373\u91ab\u9662\u5167\u55ae\u4e00 48 GB GPU \u5de5\u4f5c\u7ad9\uff09\u4e0b\uff0c\u9032\u884c LLM \u7684\u5c40\u90e8\u9662\u5167\u5fae\u8abf\u3002\u5fae\u8abf\u5f8c\u7684 LLM \u6210\u529f\u5b78\u7fd2\u4e86\u653e\u5c04\u816b\u7624\u5b78\u7684\u7279\u5b9a\u8cc7\u8a0a\uff0c\u4e26\u4ee5\u7279\u5b9a\u65bc\u6a5f\u69cb\u7684\u98a8\u683c\u751f\u6210\u91ab\u5e2b\u4fe1\u51fd\u3002\u751f\u6210\u7684\u6458\u8981\u5831\u544a\u7684 ROUGE \u5206\u6578\u7a81\u986f\u4e86 8B LLaMA-3 \u6a21\u578b\u512a\u65bc 13B LLaMA-2 \u6a21\u578b\u3002\u9032\u4e00\u6b65\u7684\u591a\u7dad\u91ab\u5e2b\u8a55\u4f30\u986f\u793a\uff0c\u5118\u7ba1\u5fae\u8abf\u5f8c\u7684 LLaMA-3 \u6a21\u578b\u751f\u6210\u8d85\u51fa\u63d0\u4f9b\u8f38\u5165\u8cc7\u6599\u7684\u5167\u5bb9\u7684\u80fd\u529b\u6709\u9650\uff0c\u4f46\u5b83\u6210\u529f\u5730\u751f\u6210\u4e86\u554f\u5019\u8a9e\u3001\u8a3a\u65b7\u548c\u6cbb\u7642\u75c5\u53f2\u3001\u9032\u4e00\u6b65\u6cbb\u7642\u5efa\u8b70\u548c\u8a08\u756b\u884c\u7a0b\u3002\u6574\u9ad4\u800c\u8a00\uff0c\u81e8\u5e8a\u5c08\u5bb6\u5c0d\u81e8\u5e8a\u6548\u76ca\u7684\u8a55\u5206\u5f88\u9ad8\uff08\u5728 4 \u5206\u5236\u4e2d\u5e73\u5747\u5f97\u5206\u70ba 3.44\uff09\u3002\u900f\u904e\u4ed4\u7d30\u7684\u91ab\u5e2b\u5be9\u67e5\u548c\u66f4\u6b63\uff0c\u57fa\u65bc LLM \u7684\u81ea\u52d5\u5316\u91ab\u5e2b\u4fe1\u51fd\u751f\u6210\u5177\u6709\u986f\u8457\u7684\u5be6\u7528\u50f9\u503c\u3002</paragraph>", "author": "Yihao Hou et.al.", "authors": "Yihao Hou, Christoph Bert, Ahmed Gomaa, Godehard Lahmer, Daniel Hoefler, Thomas Weissmann, Raphaela Voigt, Philipp Schubert, Charlotte Schmitter, Alina Depardon, Sabine Semrau, Andreas Maier, Rainer Fietkau, Yixing Huang, Florian Putz", "id": "2408.10715v1", "paper_url": "http://arxiv.org/abs/2408.10715v1", "repo": "null"}}