{"2409.20424": {"publish_time": "2024-09-30", "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "paper_summary": "Recent advances in Vision-Language Models (VLMs) and the scarcity of\nhigh-quality multi-modal alignment data have inspired numerous researches on\nsynthetic VLM data generation. The conventional norm in VLM data construction\nuses a mixture of specialists in caption and OCR, or stronger VLM APIs and\nexpensive human annotation. In this paper, we present World to Code (W2C), a\nmeticulously curated multi-modal data construction pipeline that organizes the\nfinal generation output into a Python code format. The pipeline leverages the\nVLM itself to extract cross-modal information via different prompts and filter\nthe generated outputs again via a consistency filtering strategy. Experiments\nhave demonstrated the high quality of W2C by improving various existing visual\nquestion answering and visual grounding benchmarks across different VLMs.\nFurther analysis also demonstrates that the new code parsing ability of VLMs\npresents better cross-modal equivalence than the commonly used detail caption\nability. Our code is available at\nhttps://github.com/foundation-multimodal-models/World2Code.", "paper_summary_zh": "\u8fd1\u671f\u5728\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u7684\u9032\u5c55\u4ee5\u53ca\u9ad8\u54c1\u8cea\u591a\u6a21\u614b\u5c0d\u9f4a\u8cc7\u6599\u7684\u7a00\u7f3a\uff0c\u6fc0\u767c\u4e86\u8a31\u591a\u95dc\u65bc\u5408\u6210 VLM \u8cc7\u6599\u7522\u751f\u7684\u7814\u7a76\u3002VLM \u8cc7\u6599\u5efa\u69cb\u4e2d\u7684\u50b3\u7d71\u898f\u7bc4\u4f7f\u7528\u6a19\u984c\u548c OCR \u5c08\u5bb6\uff0c\u6216\u66f4\u5f37\u5927\u7684 VLM API \u548c\u6602\u8cb4\u7684\u4eba\u5de5\u6a19\u8a3b\u7684\u6df7\u5408\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e16\u754c\u5230\u7a0b\u5f0f\u78bc (W2C)\uff0c\u4e00\u500b\u7cbe\u5fc3\u7b56\u5283\u7684\u591a\u6a21\u614b\u8cc7\u6599\u5efa\u69cb\u7ba1\u9053\uff0c\u5b83\u5c07\u6700\u7d42\u7522\u751f\u7684\u8f38\u51fa\u7d44\u7e54\u6210 Python \u7a0b\u5f0f\u78bc\u683c\u5f0f\u3002\u8a72\u7ba1\u9053\u5229\u7528 VLM \u672c\u8eab\u900f\u904e\u4e0d\u540c\u7684\u63d0\u793a\u4f86\u8403\u53d6\u8de8\u6a21\u614b\u8cc7\u8a0a\uff0c\u4e26\u900f\u904e\u4e00\u81f4\u6027\u904e\u6ffe\u7b56\u7565\u518d\u6b21\u904e\u6ffe\u7522\u751f\u7684\u8f38\u51fa\u3002\u5be6\u9a57\u5df2\u900f\u904e\u6539\u5584\u4e0d\u540c VLM \u4e2d\u7684\u5404\u7a2e\u73fe\u6709\u8996\u89ba\u554f\u7b54\u548c\u8996\u89ba\u57fa\u790e\u57fa\u6e96\u4f86\u8b49\u660e W2C \u7684\u9ad8\u54c1\u8cea\u3002\u9032\u4e00\u6b65\u7684\u5206\u6790\u4e5f\u8b49\u660e VLM \u7684\u65b0\u7a0b\u5f0f\u78bc\u89e3\u6790\u80fd\u529b\u5448\u73fe\u51fa\u6bd4\u4e00\u822c\u4f7f\u7528\u7684\u8a73\u7d30\u6a19\u984c\u80fd\u529b\u66f4\u597d\u7684\u8de8\u6a21\u614b\u7b49\u50f9\u6027\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/foundation-multimodal-models/World2Code \u53d6\u5f97\u3002", "author": "Jiacong Wang et.al.", "authors": "Jiacong Wang, Bohong Wu, Haiyong Jiang, Xun Zhou, Xin Xiao, Haoyuan Guo, Jun Xiao", "id": "2409.20424v1", "paper_url": "http://arxiv.org/abs/2409.20424v1", "repo": "null"}}