{"2405.05803": {"publish_time": "2024-05-09", "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference", "paper_summary": "Multimodal large language models (MLLMs) demand considerable computations for\ninference due to the extensive parameters and the additional input tokens\nneeded for visual information representation. Herein, we introduce Visual\nTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid\ninference. Our approach is inspired by two intriguing phenomena we have\nobserved: (1) the attention sink phenomenon that is prevalent in LLMs also\npersists in MLLMs, suggesting that initial tokens and nearest tokens receive\nthe majority of attention, while middle vision tokens garner minimal attention\nin deep layers; (2) the presence of information migration, which implies that\nvisual information is transferred to subsequent text tokens within the first\nfew layers of MLLMs. As per our findings, we conclude that vision tokens are\nnot necessary in the deep layers of MLLMs. Thus, we strategically withdraw them\nat a certain layer, enabling only text tokens to engage in subsequent layers.\nTo pinpoint the ideal layer for vision tokens withdrawal, we initially analyze\na limited set of tiny datasets and choose the first layer that meets the\nKullback-Leibler divergence criterion. Our VTW approach can cut computational\noverhead by over 40\\% across diverse multimodal tasks while maintaining\nperformance. Our code is released at https://github.com/lzhxmu/VTW.", "paper_summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7531\u4e8e\u5e7f\u6cdb\u7684\u53c2\u6570\u548c\u89c6\u89c9\u4fe1\u606f\u8868\u793a\u6240\u9700\u7684\u9644\u52a0\u8f93\u5165\u6807\u8bb0\uff0c\u56e0\u6b64\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u624d\u80fd\u8fdb\u884c\u63a8\u7406\u3002\u5728\u6b64\uff0c\u6211\u4eec\u4ecb\u7ecd\u89c6\u89c9\u6807\u8bb0\u64a4\u56de (VTW)\uff0c\u8fd9\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u5347 MLLM \u4ee5\u8fdb\u884c\u5feb\u901f\u63a8\u7406\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u53d7\u5230\u6211\u4eec\u89c2\u5bdf\u5230\u7684\u4e24\u4e2a\u6709\u8da3\u7684\u73b0\u8c61\u7684\u542f\u53d1\uff1a(1) \u5728 LLM \u4e2d\u666e\u904d\u5b58\u5728\u7684\u6ce8\u610f\u529b\u6c47\u96c6\u73b0\u8c61\u4e5f\u5b58\u5728\u4e8e MLLM \u4e2d\uff0c\u8868\u660e\u521d\u59cb\u6807\u8bb0\u548c\u6700\u8fd1\u6807\u8bb0\u63a5\u6536\u4e86\u5927\u90e8\u5206\u6ce8\u610f\u529b\uff0c\u800c\u4e2d\u95f4\u89c6\u89c9\u6807\u8bb0\u5728\u6df1\u5c42\u4e2d\u83b7\u5f97\u7684\u6ce8\u610f\u529b\u6700\u5c11\uff1b(2) \u4fe1\u606f\u8fc1\u79fb\u7684\u5b58\u5728\uff0c\u8fd9\u610f\u5473\u7740\u89c6\u89c9\u4fe1\u606f\u5728 MLLM \u7684\u524d\u51e0\u5c42\u5185\u88ab\u4f20\u8f93\u5230\u540e\u7eed\u6587\u672c\u6807\u8bb0\u3002\u6839\u636e\u6211\u4eec\u7684\u53d1\u73b0\uff0c\u6211\u4eec\u5f97\u51fa\u7ed3\u8bba\uff0c\u89c6\u89c9\u6807\u8bb0\u5728 MLLM \u7684\u6df1\u5c42\u4e2d\u662f\u4e0d\u5fc5\u8981\u7684\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5728\u67d0\u4e00\u5c42\u5bf9\u5176\u8fdb\u884c\u6218\u7565\u6027\u64a4\u56de\uff0c\u4ec5\u5141\u8bb8\u6587\u672c\u6807\u8bb0\u53c2\u4e0e\u540e\u7eed\u5c42\u3002\u4e3a\u4e86\u7cbe\u786e\u5b9a\u4f4d\u89c6\u89c9\u6807\u8bb0\u64a4\u56de\u7684\u7406\u60f3\u5c42\uff0c\u6211\u4eec\u6700\u521d\u5206\u6790\u4e86\u4e00\u7ec4\u6709\u9650\u7684\u5c0f\u578b\u6570\u636e\u96c6\uff0c\u5e76\u9009\u62e9\u4e86\u6ee1\u8db3 Kullback-Leibler \u6563\u5ea6\u51c6\u5219\u7684\u7b2c\u4e00\u5c42\u3002\u6211\u4eec\u7684 VTW \u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5728\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5c06\u8ba1\u7b97\u5f00\u9500\u51cf\u5c11 40% \u4ee5\u4e0a\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u5728 https://github.com/lzhxmu/VTW \u4e2d\u53d1\u5e03\u3002", "author": "Zhihang Lin et.al.", "authors": "Zhihang Lin, Mingbao Lin, Luxi Lin, Rongrong Ji", "id": "2405.05803v1", "paper_url": "http://arxiv.org/abs/2405.05803v1", "repo": "https://github.com/lzhxmu/vtw"}}