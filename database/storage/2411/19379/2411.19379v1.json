{"2411.19379": {"publish_time": "2024-11-28", "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs", "paper_summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.", "paper_summary_zh": "\u7d50\u5408\u4e86\u6ce8\u610f\u529b\u5c64\u7684\u8a9e\u8a00\u5efa\u6a21\u529f\u80fd\u8207\u905e\u8ff4\u5c64\uff08\u4f8b\u5982\u72c0\u614b\u7a7a\u9593\u6a21\u578b\uff09\u7684\u6548\u7387\u7684\u6df7\u5408\u6a21\u578b\u5728\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u670d\u52d9\u4e2d\u5be6\u52d9\u4e0a\u652f\u63f4\u9577\u8108\u7d61\u65b9\u9762\u7372\u5f97\u4e86\u9032\u5c55\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u7684\u7368\u7279\u5c6c\u6027\u8b93\u4f7f\u7528\u88dc\u5145\u6548\u7387\u6700\u4f73\u5316\u8b8a\u5f97\u8907\u96dc\uff0c\u4f8b\u5982\u8df3\u904e\u8acb\u6c42\u9593\u91cd\u8907\u904b\u7b97\u7684\u524d\u7db4\u5feb\u53d6\u3002\u6700\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u5011\u5c0d\u905e\u8ff4\u5c64\u4f7f\u7528\u5c31\u5730\u72c0\u614b\u66f4\u65b0\uff0c\u6392\u9664\u56de\u6efe\u90e8\u5206\u5e8f\u5217\u91cd\u758a\u7684\u5feb\u53d6\u689d\u76ee\uff0c\u800c\u53ea\u5f37\u5236\u5b8c\u5168\u6bd4\u5c0d\u7684\u5feb\u53d6\u547d\u4e2d\uff1b\u5176\u7d50\u679c\u662f\u6bcf\u500b\u5e8f\u5217\u7522\u751f\u5927\u91cf\uff08\u5927\u578b\uff09\u5feb\u53d6\u689d\u76ee\uff0c\u5176\u4e2d\u5927\u90e8\u5206\u7522\u751f\u7684\u91cd\u8907\u4f7f\u7528\u6a5f\u6703\u6700\u5c11\u3002\u6211\u5011\u63d0\u51fa Marconi\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u652f\u63f4\u4f7f\u7528\u6df7\u5408 LLM \u7684\u6709\u6548\u524d\u7db4\u5feb\u53d6\u7684\u7cfb\u7d71\u3002Marconi \u7684\u95dc\u9375\u5728\u65bc\u5176\u65b0\u7a4e\u7684\u63a5\u7d0d\u548c\u9a45\u9010\u653f\u7b56\uff0c\u9019\u4e9b\u653f\u7b56\u4e0d\u50c5\u6839\u64da\u6700\u8fd1\u4f7f\u7528\u60c5\u6cc1\uff0c\u9084\u6839\u64da (1) \u4e0d\u540c\u547d\u4e2d\u60c5\u6cc1\u5206\u985e\u4e2d\u5b83\u5011\u7684\u91cd\u8907\u4f7f\u7528\u53ef\u80fd\u6027\u7684\u9810\u6e2c\uff0c\u4ee5\u53ca (2) \u76f8\u5c0d\u65bc\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\uff0c\u547d\u4e2d\u63d0\u4f9b\u7684\u904b\u7b97\u7bc0\u7701\uff0c\u66f4\u5be9\u614e\u5730\u8a55\u4f30\u6f5b\u5728\u5feb\u53d6\u689d\u76ee\u3002\u5728\u4e0d\u540c\u7684\u5de5\u4f5c\u8ca0\u8f09\u548c\u6df7\u5408\u6a21\u578b\u4e2d\uff0c\u8207\u6700\u5148\u9032\u7684\u524d\u7db4\u5feb\u53d6\u7cfb\u7d71\u76f8\u6bd4\uff0cMarconi \u9054\u5230\u9ad8\u9054 34.4 \u500d\u7684 token \u547d\u4e2d\u7387\uff0871.1% \u6216\u4f4e 617 \u6beb\u79d2\u7684 TTFT\uff09\u3002", "author": "Rui Pan et.al.", "authors": "Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Ravi Netravali, Yida Wang", "id": "2411.19379v1", "paper_url": "http://arxiv.org/abs/2411.19379v1", "repo": "null"}}