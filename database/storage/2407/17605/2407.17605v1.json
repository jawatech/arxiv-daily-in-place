{"2407.17605": {"publish_time": "2024-07-24", "title": "Coupling Speech Encoders with Downstream Text Models", "paper_summary": "We present a modular approach to building cascade speech translation (AST)\nmodels that guarantees that the resulting model performs no worse than the\n1-best cascade baseline while preserving state-of-the-art speech recognition\n(ASR) and text translation (MT) performance for a given task. Our novel\ncontribution is the use of an ``exporter'' layer that is trained under L2-loss\nto ensure a strong match between ASR embeddings and the MT token embeddings for\nthe 1-best sequence. The ``exporter'' output embeddings are fed directly to the\nMT model in lieu of 1-best token embeddings, thus guaranteeing that the\nresulting model performs no worse than the 1-best cascade baseline, while\nallowing back-propagation gradient to flow from the MT model into the ASR\ncomponents. The matched-embeddings cascade architecture provide a significant\nimprovement over its 1-best counterpart in scenarios where incremental training\nof the MT model is not an option and yet we seek to improve quality by\nleveraging (speech, transcription, translated transcription) data provided with\nthe AST task. The gain disappears when the MT model is incrementally trained on\nthe parallel text data available with the AST task. The approach holds promise\nfor other scenarios that seek to couple ASR encoders and immutable text models,\nsuch at large language models (LLM).", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u5efa\u69cb\u4e32\u806f\u8a9e\u97f3\u7ffb\u8b6f (AST) \u6a21\u578b\u7684\u6a21\u7d44\u5316\u65b9\u6cd5\uff0c\u53ef\u78ba\u4fdd\u7522\u751f\u7684\u6a21\u578b\u57f7\u884c\u6548\u80fd\u4e0d\u905c\u65bc 1-best \u4e32\u806f\u57fa\u6e96\uff0c\u540c\u6642\u4fdd\u7559\u6700\u5148\u9032\u7684\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u548c\u6587\u5b57\u7ffb\u8b6f (MT) \u6548\u80fd\uff0c\u4ee5\u57f7\u884c\u7279\u5b9a\u4efb\u52d9\u3002\u6211\u5011\u7684\u5275\u65b0\u8ca2\u737b\u662f\u4f7f\u7528\u4e00\u500b\u5728 L2 \u640d\u5931\u4e0b\u8a13\u7df4\u7684\u300c\u532f\u51fa\u5c64\u300d\uff0c\u4ee5\u78ba\u4fdd ASR \u5d4c\u5165\u8207 MT \u4ee3\u5e63\u5d4c\u5165\u4e4b\u9593\u6709\u5f37\u70c8\u7684\u5339\u914d\uff0c\u4ee5\u4f9b 1-best \u9806\u5e8f\u4f7f\u7528\u3002\u5c07\u300c\u532f\u51fa\u5c64\u300d\u8f38\u51fa\u5d4c\u5165\u76f4\u63a5\u63d0\u4f9b\u7d66 MT \u6a21\u578b\uff0c\u4ee5\u53d6\u4ee3 1-best \u4ee3\u5e63\u5d4c\u5165\uff0c\u5f9e\u800c\u78ba\u4fdd\u7522\u751f\u7684\u6a21\u578b\u57f7\u884c\u6548\u80fd\u4e0d\u905c\u65bc 1-best \u4e32\u806f\u57fa\u6e96\uff0c\u540c\u6642\u5141\u8a31\u53cd\u5411\u50b3\u64ad\u68af\u5ea6\u5f9e MT \u6a21\u578b\u6d41\u5165 ASR \u7d44\u4ef6\u3002\u5339\u914d\u5d4c\u5165\u4e32\u806f\u67b6\u69cb\u5728\u5176 1-best \u5c0d\u61c9\u67b6\u69cb\u4e2d\u63d0\u4f9b\u986f\u8457\u7684\u6539\u9032\uff0c\u5728\u7121\u6cd5\u9078\u64c7 MT \u6a21\u578b\u589e\u91cf\u8a13\u7df4\u7684\u60c5\u6cc1\u4e0b\uff0c\u6211\u5011\u4ecd\u5c0b\u6c42\u900f\u904e\u5229\u7528 AST \u4efb\u52d9\u63d0\u4f9b\u7684\uff08\u8a9e\u97f3\u3001\u8f49\u9304\u3001\u7ffb\u8b6f\u8f49\u9304\uff09\u8cc7\u6599\u4f86\u63d0\u5347\u54c1\u8cea\u3002\u7576 MT \u6a21\u578b\u6839\u64da AST \u4efb\u52d9\u63d0\u4f9b\u7684\u5e73\u884c\u6587\u5b57\u8cc7\u6599\u9032\u884c\u589e\u91cf\u8a13\u7df4\u6642\uff0c\u589e\u76ca\u5c31\u6703\u6d88\u5931\u3002\u9019\u7a2e\u65b9\u6cd5\u5c0d\u65bc\u5c0b\u6c42\u5c07 ASR \u7de8\u78bc\u5668\u548c\u4e0d\u53ef\u8b8a\u6587\u5b57\u6a21\u578b\uff08\u4f8b\u5982\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\uff09\u7d50\u5408\u7684\u5176\u4ed6\u5834\u666f\u5f88\u6709\u524d\u666f\u3002", "author": "Ciprian Chelba et.al.", "authors": "Ciprian Chelba, Johan Schalkwyk", "id": "2407.17605v1", "paper_url": "http://arxiv.org/abs/2407.17605v1", "repo": "null"}}