{"2502.04576": {"publish_time": "2025-02-07", "title": "Self-Regulation and Requesting Interventions", "paper_summary": "Human intelligence involves metacognitive abilities like self-regulation,\nrecognizing limitations, and seeking assistance only when needed. While LLM\nAgents excel in many domains, they often lack this awareness. Overconfident\nagents risk catastrophic failures, while those that seek help excessively\nhinder efficiency. A key challenge is enabling agents with a limited\nintervention budget $C$ is to decide when to request assistance. In this paper,\nwe propose an offline framework that trains a \"helper\" policy to request\ninterventions, such as more powerful models or test-time compute, by combining\nLLM-based process reward models (PRMs) with tabular reinforcement learning.\nUsing state transitions collected offline, we score optimal intervention timing\nwith PRMs and train the helper model on these labeled trajectories. This\noffline approach significantly reduces costly intervention calls during\ntraining. Furthermore, the integration of PRMs with tabular RL enhances\nrobustness to off-policy data while avoiding the inefficiencies of deep RL. We\nempirically find that our method delivers optimal helper behavior.", "paper_summary_zh": "\u4eba\u985e\u667a\u6167\u5305\u542b\u50cf\u81ea\u6211\u8abf\u7bc0\u3001\u8a8d\u77e5\u9650\u5236\u548c\u50c5\u5728\u9700\u8981\u6642\u5c0b\u6c42\u5354\u52a9\u7b49\u5143\u8a8d\u77e5\u80fd\u529b\u3002\u96d6\u7136 LLM \u4ee3\u7406\u5728\u8a31\u591a\u9818\u57df\u8868\u73fe\u512a\u7570\uff0c\u4f46\u4ed6\u5011\u5e38\u5e38\u7f3a\u4e4f\u9019\u7a2e\u610f\u8b58\u3002\u904e\u5ea6\u81ea\u4fe1\u7684\u4ee3\u7406\u6703\u5192\u8457\u707d\u96e3\u6027\u5931\u6557\u7684\u98a8\u96aa\uff0c\u800c\u904e\u5ea6\u5c0b\u6c42\u5e6b\u52a9\u7684\u4ee3\u7406\u6703\u963b\u7919\u6548\u7387\u3002\u4e00\u500b\u95dc\u9375\u6311\u6230\u662f\u8b93\u5177\u6709\u6709\u9650\u5e72\u9810\u9810\u7b97 $C$ \u7684\u4ee3\u7406\u6c7a\u5b9a\u4f55\u6642\u8acb\u6c42\u5354\u52a9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u96e2\u7dda\u6846\u67b6\uff0c\u8a13\u7df4\u300c\u5e6b\u52a9\u8005\u300d\u653f\u7b56\u4f86\u8acb\u6c42\u5e72\u9810\uff0c\u4f8b\u5982\u66f4\u5f37\u5927\u7684\u6a21\u578b\u6216\u6e2c\u8a66\u6642\u9593\u8a08\u7b97\uff0c\u65b9\u6cd5\u662f\u5c07\u57fa\u65bc LLM \u7684\u904e\u7a0b\u734e\u52f5\u6a21\u578b (PRM) \u8207\u8868\u683c\u5f37\u5316\u5b78\u7fd2\u76f8\u7d50\u5408\u3002\u4f7f\u7528\u96e2\u7dda\u6536\u96c6\u7684\u72c0\u614b\u8f49\u63db\uff0c\u6211\u5011\u4f7f\u7528 PRM \u8a55\u5206\u6700\u4f73\u5e72\u9810\u6642\u6a5f\uff0c\u4e26\u5728\u9019\u4e9b\u6a19\u8a18\u8ecc\u8de1\u4e0a\u8a13\u7df4\u5e6b\u52a9\u8005\u6a21\u578b\u3002\u9019\u7a2e\u96e2\u7dda\u65b9\u6cd5\u986f\u8457\u6e1b\u5c11\u4e86\u8a13\u7df4\u671f\u9593\u4ee3\u50f9\u9ad8\u6602\u7684\u5e72\u9810\u547c\u53eb\u3002\u6b64\u5916\uff0cPRM \u8207\u8868\u683c RL \u7684\u6574\u5408\u589e\u5f37\u4e86\u5c0d\u975e\u7b56\u7565\u6578\u64da\u7684\u7a69\u5065\u6027\uff0c\u540c\u6642\u907f\u514d\u4e86\u6df1\u5ea6 RL \u7684\u4f4e\u6548\u7387\u3002\u6211\u5011\u6191\u7d93\u9a57\u767c\u73fe\uff0c\u6211\u5011\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u5e6b\u52a9\u8005\u884c\u70ba\u3002", "author": "So Yeon Min et.al.", "authors": "So Yeon Min, Yue Wu, Jimin Sun, Max Kaufmann, Fahim Tajwar, Yonatan Bisk, Ruslan Salakhutdinov", "id": "2502.04576v1", "paper_url": "http://arxiv.org/abs/2502.04576v1", "repo": "null"}}