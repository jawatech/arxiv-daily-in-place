{"2406.04984": {"publish_time": "2024-06-07", "title": "MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter", "paper_summary": "Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large\nLanguage Models (LLMs) under limited resources. However, the fine-tuning\nperformance with PEFT on complex, knowledge-intensive tasks is limited due to\nthe constrained model capacity, which originates from the limited number of\nadditional trainable parameters. To overcome this limitation, we introduce a\nnovel mechanism that fine-tunes LLMs with adapters of larger size yet\nmemory-efficient. This is achieved by leveraging the inherent activation\nsparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger\ncapacity of Central Processing Unit (CPU) memory compared to Graphics\nProcessing Unit (GPU). We store and update the parameters of larger adapters on\nthe CPU. Moreover, we employ a Mixture of Experts (MoE)-like architecture to\nmitigate unnecessary CPU computations and reduce the communication volume\nbetween the GPU and CPU. This is particularly beneficial over the limited\nbandwidth of PCI Express (PCIe). Our method can achieve fine-tuning results\ncomparable to those obtained with larger memory capacities, even when operating\nunder more limited resources such as a 24GB memory single GPU setup, with\nacceptable loss in training efficiency. Our codes are available at\nhttps://github.com/CURRENTF/MEFT.", "paper_summary_zh": "\u53c3\u6578\u9ad8\u6548\u5fae\u8abf (PEFT) \u53ef\u5728\u6709\u9650\u8cc7\u6e90\u4e0b\uff0c\u5354\u52a9\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u7136\u800c\uff0c\u7531\u65bc\u6a21\u578b\u5bb9\u91cf\u53d7\u9650\uff0c\u4e14\u6e90\u81ea\u65bc\u6709\u9650\u7684\u53ef\u8a13\u7df4\u53c3\u6578\u6578\u91cf\uff0c\u56e0\u6b64 PEFT \u5728\u8907\u96dc\u3001\u77e5\u8b58\u5bc6\u96c6\u578b\u4efb\u52d9\u4e0a\u7684\u5fae\u8abf\u6548\u80fd\u53d7\u5230\u9650\u5236\u3002\u70ba\u4e86\u514b\u670d\u9019\u500b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u6a5f\u5236\uff0c\u4f7f\u7528\u8f03\u5927\u4f46\u8a18\u61b6\u9ad4\u6548\u7387\u9ad8\u7684\u9069\u914d\u5668\u4f86\u5fae\u8abf LLM\u3002\u9019\u662f\u900f\u904e\u5229\u7528 LLM \u7684\u524d\u994b\u7db2\u8def (FFN) \u4e2d\u56fa\u6709\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u4e26\u5229\u7528\u4e2d\u592e\u8655\u7406\u55ae\u5143 (CPU) \u8a18\u61b6\u9ad4\u6bd4\u5716\u5f62\u8655\u7406\u55ae\u5143 (GPU) \u66f4\u5927\u7684\u5bb9\u91cf\u4f86\u5be6\u73fe\u3002\u6211\u5011\u5728 CPU \u4e0a\u5132\u5b58\u548c\u66f4\u65b0\u8f03\u5927\u9069\u914d\u5668\u7684\u53c3\u6578\u3002\u6b64\u5916\uff0c\u6211\u5011\u63a1\u7528\u985e\u4f3c\u6df7\u5408\u5c08\u5bb6 (MoE) \u7684\u67b6\u69cb\uff0c\u4ee5\u6e1b\u8f15\u4e0d\u5fc5\u8981\u7684 CPU \u8a08\u7b97\uff0c\u4e26\u6e1b\u5c11 GPU \u548c CPU \u4e4b\u9593\u7684\u901a\u8a0a\u91cf\u3002\u9019\u5c0d\u65bc PCI Express (PCIe) \u7684\u6709\u9650\u983b\u5bec\u7279\u5225\u6709\u76ca\u3002\u6211\u5011\u7684\u6a21\u578b\u5373\u4f7f\u5728\u8f03\u6709\u9650\u7684\u8cc7\u6e90\uff08\u4f8b\u5982 24GB \u8a18\u61b6\u9ad4\u55ae GPU \u8a2d\u5b9a\uff09\u4e0b\u904b\u4f5c\uff0c\u4e5f\u80fd\u9054\u5230\u8207\u8f03\u5927\u8a18\u61b6\u9ad4\u5bb9\u91cf\u6240\u53d6\u5f97\u7684\u5fae\u8abf\u7d50\u679c\u76f8\u7576\u7684\u8868\u73fe\uff0c\u540c\u6642\u5728\u8a13\u7df4\u6548\u7387\u4e0a\u53ef\u63a5\u53d7\u7684\u640d\u5931\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728 https://github.com/CURRENTF/MEFT \u53d6\u5f97\u3002", "author": "Jitai Hao et.al.", "authors": "Jitai Hao, WeiWei Sun, Xin Xin, Qi Meng, Zhumin Chen, Pengjie Ren, Zhaochun Ren", "id": "2406.04984v1", "paper_url": "http://arxiv.org/abs/2406.04984v1", "repo": "https://github.com/currentf/meft"}}