{"2407.05868": {"publish_time": "2024-07-08", "title": "KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions", "paper_summary": "Recent studies have demonstrated that large language models (LLMs) are\nsusceptible to being misled by false premise questions (FPQs), leading to\nerrors in factual knowledge, know as factuality hallucination. Existing\nbenchmarks that assess this vulnerability primarily rely on manual\nconstruction, resulting in limited scale and lack of scalability. In this work,\nwe introduce an automated, scalable pipeline to create FPQs based on knowledge\ngraphs (KGs). The first step is modifying true triplets extracted from KGs to\ncreate false premises. Subsequently, utilizing the state-of-the-art\ncapabilities of GPTs, we generate semantically rich FPQs. Based on the proposed\nmethod, we present a comprehensive benchmark, the Knowledge Graph-based False\nPremise Questions (KG-FPQ), which contains approximately 178k FPQs across three\nknowledge domains, at six levels of confusability, and in two task formats.\nUsing KG-FPQ, we conduct extensive evaluations on several representative LLMs\nand provide valuable insights. The KG-FPQ dataset and code are available\nat~https://github.com/yanxuzhu/KG-FPQ.", "paper_summary_zh": "\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5bb9\u6613\u88ab\u9519\u8bef\u524d\u63d0\u95ee\u9898 (FPQ) \u8bef\u5bfc\uff0c\u4ece\u800c\u5bfc\u81f4\u4e8b\u5b9e\u77e5\u8bc6\u9519\u8bef\uff0c\u5373\u4e8b\u5b9e\u5e7b\u89c9\u3002\u7528\u4e8e\u8bc4\u4f30\u6b64\u6f0f\u6d1e\u7684\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u4e8e\u624b\u52a8\u6784\u5efa\uff0c\u5bfc\u81f4\u89c4\u6a21\u6709\u9650\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31 (KG) \u521b\u5efa FPQ \u7684\u81ea\u52a8\u5316\u53ef\u6269\u5c55\u7ba1\u9053\u3002\u7b2c\u4e00\u6b65\u662f\u4fee\u6539\u4ece KG \u4e2d\u63d0\u53d6\u7684\u771f\u4e09\u5143\u7ec4\u4ee5\u521b\u5efa\u9519\u8bef\u524d\u63d0\u3002\u968f\u540e\uff0c\u5229\u7528 GPT \u7684\u6700\u5148\u8fdb\u529f\u80fd\uff0c\u6211\u4eec\u751f\u6210\u4e86\u8bed\u4e49\u4e30\u5bcc\u7684 FPQ\u3002\u57fa\u4e8e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u5373\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u9519\u8bef\u524d\u63d0\u95ee\u9898 (KG-FPQ)\uff0c\u5b83\u5305\u542b\u5927\u7ea6 178k \u4e2a FPQ\uff0c\u6db5\u76d6\u4e09\u4e2a\u77e5\u8bc6\u57df\uff0c\u516d\u4e2a\u6df7\u6dc6\u7ea7\u522b\u548c\u4e24\u79cd\u4efb\u52a1\u683c\u5f0f\u3002\u4f7f\u7528 KG-FPQ\uff0c\u6211\u4eec\u5bf9\u51e0\u4e2a\u6709\u4ee3\u8868\u6027\u7684 LLM \u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002KG-FPQ \u6570\u636e\u96c6\u548c\u4ee3\u7801\u53ef\u5728~https://github.com/yanxuzhu/KG-FPQ \u83b7\u5f97\u3002", "author": "Yanxu Zhu et.al.", "authors": "Yanxu Zhu, Jinlin Xiao, Yuhang Wang, Jitao Sang", "id": "2407.05868v1", "paper_url": "http://arxiv.org/abs/2407.05868v1", "repo": "https://github.com/yanxuzhu/kg-fpq"}}