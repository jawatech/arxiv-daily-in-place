{"2412.09871": {"publish_time": "2024-12-13", "title": "Byte Latent Transformer: Patches Scale Better Than Tokens", "paper_summary": "We introduce the Byte Latent Transformer (BLT), a new byte-level LLM\narchitecture that, for the first time, matches tokenization-based LLM\nperformance at scale with significant improvements in inference efficiency and\nrobustness. BLT encodes bytes into dynamically sized patches, which serve as\nthe primary units of computation. Patches are segmented based on the entropy of\nthe next byte, allocating more compute and model capacity where increased data\ncomplexity demands it. We present the first FLOP controlled scaling study of\nbyte-level models up to 8B parameters and 4T training bytes. Our results\ndemonstrate the feasibility of scaling models trained on raw bytes without a\nfixed vocabulary. Both training and inference efficiency improve due to\ndynamically selecting long patches when data is predictable, along with\nqualitative improvements on reasoning and long tail generalization. Overall,\nfor fixed inference costs, BLT shows significantly better scaling than\ntokenization-based models, by simultaneously growing both patch and model size.", "paper_summary_zh": "\u6211\u5011\u4ecb\u7d39 Byte Latent Transformer (BLT)\uff0c\u9019\u662f\u4e00\u7a2e\u65b0\u7684\u4f4d\u5143\u7d44\u7d1a\u5225 LLM \u67b6\u69cb\uff0c\u5b83\u9996\u6b21\u4ee5\u986f\u8457\u63d0\u5347\u7684\u63a8\u8ad6\u6548\u7387\u548c\u7a69\u5065\u6027\uff0c\u5339\u914d\u4e86\u57fa\u65bc\u6a19\u8a18\u5316\u7684 LLM \u7684\u898f\u6a21\u5316\u6548\u80fd\u3002BLT \u5c07\u4f4d\u5143\u7d44\u7de8\u78bc\u6210\u52d5\u614b\u5927\u5c0f\u7684\u5340\u584a\uff0c\u4f5c\u70ba\u904b\u7b97\u7684\u4e3b\u8981\u55ae\u4f4d\u3002\u5340\u584a\u6839\u64da\u4e0b\u4e00\u500b\u4f4d\u5143\u7d44\u7684\u71b5\u9032\u884c\u5206\u5272\uff0c\u5728\u589e\u52a0\u7684\u8cc7\u6599\u8907\u96dc\u5ea6\u9700\u8981\u6642\uff0c\u5206\u914d\u66f4\u591a\u904b\u7b97\u548c\u6a21\u578b\u5bb9\u91cf\u3002\u6211\u5011\u63d0\u4f9b\u4e86\u7b2c\u4e00\u500b FLOP \u63a7\u5236\u7684\u4f4d\u5143\u7d44\u7d1a\u5225\u6a21\u578b\u64f4\u5145\u7814\u7a76\uff0c\u53c3\u6578\u9ad8\u9054 8B\uff0c\u8a13\u7df4\u4f4d\u5143\u7d44\u9ad8\u9054 4T\u3002\u6211\u5011\u7684\u7d50\u679c\u8b49\u660e\u4e86\u5728\u6c92\u6709\u56fa\u5b9a\u8a5e\u5f59\u8868\u7684\u60c5\u6cc1\u4e0b\uff0c\u64f4\u5145\u5728\u539f\u59cb\u4f4d\u5143\u7d44\u4e0a\u8a13\u7df4\u7684\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002\u7531\u65bc\u5728\u8cc7\u6599\u53ef\u9810\u6e2c\u6642\u52d5\u614b\u9078\u64c7\u9577\u5340\u584a\uff0c\u4ee5\u53ca\u5728\u63a8\u7406\u548c\u9577\u5c3e\u6982\u62ec\u4e0a\u7684\u8cea\u5316\u6539\u9032\uff0c\u8a13\u7df4\u548c\u63a8\u8ad6\u6548\u7387\u90fd\u6709\u6240\u63d0\u5347\u3002\u7e3d\u9ad4\u800c\u8a00\uff0c\u5c0d\u65bc\u56fa\u5b9a\u7684\u63a8\u8ad6\u6210\u672c\uff0cBLT \u900f\u904e\u540c\u6642\u589e\u52a0\u5340\u584a\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u5c55\u73fe\u51fa\u6bd4\u57fa\u65bc\u6a19\u8a18\u5316\u7684\u6a21\u578b\u986f\u8457\u66f4\u597d\u7684\u64f4\u5145\u3002", "author": "Artidoro Pagnoni et.al.", "authors": "Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, Srinivasan Iyer", "id": "2412.09871v1", "paper_url": "http://arxiv.org/abs/2412.09871v1", "repo": "https://github.com/facebookresearch/blt"}}