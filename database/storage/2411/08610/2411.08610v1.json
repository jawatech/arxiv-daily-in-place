{"2411.08610": {"publish_time": "2024-11-13", "title": "Dynamic Subset Tuning: Expanding the Operational Range of Parameter-Efficient Training for Large Language Models", "paper_summary": "We propose a novel parameter-efficient training (PET) method for large\nlanguage models that adapts models to downstream tasks by optimizing a small\nsubset of the existing model parameters. Unlike prior methods, this subset is\nnot fixed in location but rather which parameters are modified evolves over the\ncourse of training. This dynamic parameter selection can yield good performance\nwith many fewer parameters than extant methods. Our method enables a seamless\nscaling of the subset size across an arbitrary proportion of the total model\nsize, while popular PET approaches like prompt tuning and LoRA cover only a\nsmall part of this spectrum. We match or outperform prompt tuning and LoRA in\nmost cases on a variety of NLP tasks (MT, QA, GSM8K, SuperGLUE) for a given\nparameter budget across different model families and sizes.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e00\u500b\u65b0\u7a4e\u7684\u53c3\u6578\u6709\u6548\u8a13\u7df4 (PET) \u65b9\u6cd5\uff0c\u7528\u65bc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u8a72\u65b9\u6cd5\u900f\u904e\u6700\u4f73\u5316\u73fe\u6709\u6a21\u578b\u53c3\u6578\u7684\u4e00\u500b\u5c0f\u90e8\u5206\uff0c\u4f86\u8abf\u6574\u6a21\u578b\u4ee5\u57f7\u884c\u4e0b\u6e38\u4efb\u52d9\u3002\u8207\u5148\u524d\u7684\u505a\u6cd5\u4e0d\u540c\uff0c\u9019\u500b\u5b50\u96c6\u4e26\u672a\u56fa\u5b9a\u5728\u4f4d\u7f6e\u4e0a\uff0c\u800c\u662f\u4fee\u6539\u54ea\u4e9b\u53c3\u6578\u6703\u96a8\u8457\u8a13\u7df4\u904e\u7a0b\u800c\u6f14\u8b8a\u3002\u9019\u7a2e\u52d5\u614b\u53c3\u6578\u9078\u53d6\u53ef\u4ee5\u7522\u751f\u826f\u597d\u7684\u6548\u80fd\uff0c\u800c\u53c3\u6578\u6bd4\u73fe\u6709\u65b9\u6cd5\u5c11\u5f88\u591a\u3002\u6211\u5011\u7684\u505a\u6cd5\u80fd\u8b93\u5b50\u96c6\u5927\u5c0f\u5728\u7e3d\u6a21\u578b\u5927\u5c0f\u7684\u4efb\u610f\u6bd4\u4f8b\u4e2d\u7121\u7e2b\u7e2e\u653e\uff0c\u800c\u50cf\u63d0\u793a\u8abf\u6574\u548c LoRA \u4e4b\u985e\u7684\u71b1\u9580 PET \u65b9\u6cd5\u53ea\u6db5\u84cb\u4e86\u9019\u500b\u7bc4\u570d\u7684\u4e00\u5c0f\u90e8\u5206\u3002\u5728\u4e0d\u540c\u7684\u6a21\u578b\u7cfb\u5217\u548c\u5927\u5c0f\u7684\u5404\u7a2e NLP \u4efb\u52d9 (MT\u3001QA\u3001GSM8K\u3001SuperGLUE) \u4e2d\uff0c\u6211\u5011\u5728\u7d66\u5b9a\u7684\u53c3\u6578\u9810\u7b97\u4e0b\uff0c\u8207\u63d0\u793a\u8abf\u6574\u548c LoRA \u76f8\u5339\u914d\u6216\u8868\u73fe\u5f97\u66f4\u597d\u3002", "author": "Felix Stahlberg et.al.", "authors": "Felix Stahlberg, Jared Lichtarge, Shankar Kumar", "id": "2411.08610v1", "paper_url": "http://arxiv.org/abs/2411.08610v1", "repo": "null"}}