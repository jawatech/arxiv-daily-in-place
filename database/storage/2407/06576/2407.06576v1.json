{"2407.06576": {"publish_time": "2024-07-09", "title": "Virtual Personas for Language Models via an Anthology of Backstories", "paper_summary": "Large language models (LLMs) are trained from vast repositories of text\nauthored by millions of distinct authors, reflecting an enormous diversity of\nhuman traits. While these models bear the potential to be used as\napproximations of human subjects in behavioral studies, prior efforts have been\nlimited in steering model responses to match individual human users. In this\nwork, we introduce \"Anthology\", a method for conditioning LLMs to particular\nvirtual personas by harnessing open-ended life narratives, which we refer to as\n\"backstories.\" We show that our methodology enhances the consistency and\nreliability of experimental outcomes while ensuring better representation of\ndiverse sub-populations. Across three nationally representative human surveys\nconducted as part of Pew Research Center's American Trends Panel (ATP), we\ndemonstrate that Anthology achieves up to 18% improvement in matching the\nresponse distributions of human respondents and 27% improvement in consistency\nmetrics. Our code and generated backstories are available at\nhttps://github.com/CannyLab/anthology.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u63a5\u53d7\u7531\u6578\u767e\u842c\u4f4d\u4e0d\u540c\u4f5c\u8005\u64b0\u5beb\u7684\u9f90\u5927\u6587\u5b57\u8cc7\u6599\u5eab\u8a13\u7df4\uff0c\u53cd\u6620\u4e86\u4eba\u985e\u7279\u8cea\u7684\u6975\u5927\u5dee\u7570\u3002\u96d6\u7136\u9019\u4e9b\u6a21\u578b\u6709\u6f5b\u529b\u7528\u65bc\u884c\u70ba\u7814\u7a76\u4e2d\u4eba\u985e\u53d7\u8a66\u8005\u7684\u8fd1\u4f3c\u503c\uff0c\u4f46\u5148\u524d\u7684\u52aa\u529b\u5728\u5f15\u5c0e\u6a21\u578b\u53cd\u61c9\u4ee5\u7b26\u5408\u500b\u5225\u4eba\u985e\u4f7f\u7528\u8005\u65b9\u9762\u53d7\u5230\u9650\u5236\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5f15\u5165\u4e86\u300c\u9078\u96c6\u300d\uff0c\u4e00\u7a2e\u901a\u904e\u5229\u7528\u958b\u653e\u5f0f\u4eba\u751f\u6558\u8ff0\uff08\u6211\u5011\u7a31\u4e4b\u70ba\u300c\u80cc\u666f\u6545\u4e8b\u300d\uff09\u4f86\u8abf\u6574 LLM \u4ee5\u9069\u61c9\u7279\u5b9a\u865b\u64ec\u89d2\u8272\u7684\u65b9\u6cd5\u3002\u6211\u5011\u8868\u660e\uff0c\u6211\u5011\u7684\u6280\u8853\u589e\u5f37\u4e86\u5be6\u9a57\u7d50\u679c\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\uff0c\u540c\u6642\u78ba\u4fdd\u4e86\u5c0d\u4e0d\u540c\u4e9e\u7fa4\u7684\u66f4\u597d\u8868\u73fe\u3002\u5728\u76ae\u5c24\u7814\u7a76\u4e2d\u5fc3\u7f8e\u570b\u8da8\u52e2\u5c0f\u7d44 (ATP) \u7684\u4e00\u74b0\u4e2d\u9032\u884c\u7684\u4e09\u9805\u5168\u570b\u4ee3\u8868\u6027\u4eba\u985e\u8abf\u67e5\u4e2d\uff0c\u6211\u5011\u8b49\u660e\u9078\u96c6\u5728\u5339\u914d\u4eba\u985e\u53d7\u8a2a\u8005\u7684\u53cd\u61c9\u5206\u4f48\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u9054 18% \u7684\u6539\u9032\uff0c\u5728\u4e00\u81f4\u6027\u6307\u6a19\u65b9\u9762\u53d6\u5f97\u4e86 27% \u7684\u6539\u9032\u3002\u6211\u5011\u7684\u4ee3\u78bc\u548c\u751f\u6210\u7684\u80cc\u666f\u6545\u4e8b\u53ef\u5728 https://github.com/CannyLab/anthology \u7372\u5f97\u3002", "author": "Suhong Moon et.al.", "authors": "Suhong Moon, Marwa Abdulhai, Minwoo Kang, Joseph Suh, Widyadewi Soedarmadji, Eran Kohen Behar, David M. Chan", "id": "2407.06576v1", "paper_url": "http://arxiv.org/abs/2407.06576v1", "repo": "https://github.com/cannylab/anthology"}}