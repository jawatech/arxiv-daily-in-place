{"2502.06773": {"publish_time": "2025-02-10", "title": "On the Emergence of Thinking in LLMs I: Searching for the Right Intuition", "paper_summary": "Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.", "paper_summary_zh": "<paragraph>\u6700\u8fd1\u7684\u4eba\u5de5\u667a\u6167\u9032\u5c55\uff0c\u4f8b\u5982 OpenAI \u7684\u65b0\u6a21\u578b\uff0c\u6b63\u5728\u5c07 LLM \u8f49\u8b8a\u70ba LRM\uff08\u5927\u578b\u63a8\u7406\u6a21\u578b\uff09\uff0c\u5b83\u5011\u5728\u63a8\u7406\u904e\u7a0b\u4e2d\u57f7\u884c\u63a8\u7406\uff0c\u82b1\u8cbb\u984d\u5916\u7684\u6642\u9593\u4e26\u8a08\u7b97\u4ee5\u7372\u5f97\u66f4\u9ad8\u54c1\u8cea\u7684\u8f38\u51fa\u3002\u6211\u5011\u7684\u76ee\u6a19\u662f\u63ed\u793a\u8a13\u7df4 LRM \u7684\u6f14\u7b97\u6cd5\u67b6\u69cb\u3002\u81ea\u4e00\u81f4\u6027\u3001PRM \u548c AlphaZero \u7b49\u65b9\u6cd5\u5efa\u8b70\u5c07\u63a8\u7406\u8996\u70ba\u5f15\u5c0e\u5f0f\u641c\u5c0b\u3002\u6211\u5011\u554f\uff1a\u4ec0\u9ebc\u662f\u6700\u7c21\u55ae\u3001\u6700\u5177\u64f4\u5145\u6027\u7684\u65b9\u6cd5\u4f86\u555f\u7528 LLM \u4e2d\u7684\u641c\u5c0b\uff1f\n  \u6211\u5011\u63d0\u51fa\u4e00\u500b\u7a31\u70ba\u900f\u904e\u81ea\u73a9\u5f37\u5316\u5b78\u7fd2 (RLSP) \u7684\u8a13\u7df4\u5f8c\u67b6\u69cb\u3002RLSP \u6d89\u53ca\u4e09\u500b\u6b65\u9a5f\uff1a(1) \u4f7f\u7528\u63a8\u7406\u904e\u7a0b\u7684\u4eba\u985e\u6216\u5408\u6210\u793a\u7bc4\u9032\u884c\u76e3\u7763\u5fae\u8abf\uff0c(2) \u4f7f\u7528\u63a2\u7d22\u734e\u52f5\u8a0a\u865f\u4f86\u9f13\u52f5\u591a\u6a23\u5316\u4e14\u6709\u6548\u7684\u63a8\u7406\u884c\u70ba\uff0c\u4ee5\u53ca (3) \u4f7f\u7528\u7d50\u679c\u9a57\u8b49\u5668\u9032\u884c RL \u8a13\u7df4\uff0c\u4ee5\u78ba\u4fdd\u6b63\u78ba\u6027\uff0c\u540c\u6642\u9632\u6b62\u734e\u52f5\u7834\u89e3\u3002\u6211\u5011\u7684\u95dc\u9375\u5275\u65b0\u662f\u5728 PPO \u8a13\u7df4\u671f\u9593\u89e3\u8026\u63a2\u7d22\u548c\u6b63\u78ba\u6027\u8a0a\u865f\uff0c\u4e26\u4ed4\u7d30\u5e73\u8861\u5b83\u5011\u4ee5\u63d0\u9ad8\u6548\u80fd\u548c\u6548\u7387\u3002\n  \u6578\u5b78\u9818\u57df\u7684\u5be6\u8b49\u7814\u7a76\u8868\u660e\uff0cRLSP \u6539\u5584\u4e86\u63a8\u7406\u3002\u5728 Llama-3.1-8B-Instruct \u6a21\u578b\u4e0a\uff0cRLSP \u53ef\u4ee5\u5c07 MATH-500 \u6e2c\u8a66\u96c6\u7684\u6548\u80fd\u63d0\u5347 23%\uff1b\u5728 AIME 2024 \u6578\u5b78\u984c\u76ee\u4e0a\uff0c\u7531\u65bc RLSP\uff0cQwen2.5-32B-Instruct \u6539\u5584\u4e86 10%\u3002\u7136\u800c\uff0c\u9019\u9805\u5de5\u4f5c\u66f4\u91cd\u8981\u7684\u767c\u73fe\u662f\uff0c\u5373\u4f7f\u4f7f\u7528\u6700\u7c21\u55ae\u7684\u63a2\u7d22\u734e\u52f5\u4f86\u9f13\u52f5\u6a21\u578b\u63a1\u53d6\u66f4\u591a\u4e2d\u9593\u6b65\u9a5f\uff0c\u4f7f\u7528 RLSP \u8a13\u7df4\u7684\u6a21\u578b\u4e5f\u986f\u793a\u51fa\u5e7e\u7a2e\u65b0\u8208\u884c\u70ba\uff0c\u4f8b\u5982\u56de\u6eaf\u3001\u63a2\u7d22\u60f3\u6cd5\u548c\u9a57\u8b49\u3002\u9019\u4e9b\u767c\u73fe\u8b49\u660e\u4e86 RLSP \u67b6\u69cb\u53ef\u80fd\u8db3\u4ee5\u5728\u64f4\u5145\u6642\u8b93 LLM \u51fa\u73fe\u8907\u96dc\u7684\u63a8\u7406\u80fd\u529b\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7406\u8ad6\u4f86\u8aaa\u660e\u70ba\u4ec0\u9ebc RLSP \u641c\u5c0b\u7b56\u7565\u66f4\u9069\u5408 LLM\uff0c\u9748\u611f\u4f86\u81ea\u4e00\u500b\u986f\u8457\u7684\u7d50\u679c\uff0c\u8a72\u7d50\u679c\u6307\u51fa CoT \u53ef\u8b49\u660e\u5730\u589e\u52a0\u4e86 LLM \u7684\u8a08\u7b97\u80fd\u529b\uff0c\u96a8\u8457 CoT \u4e2d\u6b65\u9a5f\u6578\u7684\u589e\u52a0\u800c\u589e\u9577\\cite{li2024chain,merrill2023expresssive}\u3002</paragraph>", "author": "Guanghao Ye et.al.", "authors": "Guanghao Ye, Khiem Duc Pham, Xinzhi Zhang, Sivakanth Gopi, Baolin Peng, Beibin Li, Janardhan Kulkarni, Huseyin A. Inan", "id": "2502.06773v1", "paper_url": "http://arxiv.org/abs/2502.06773v1", "repo": "null"}}