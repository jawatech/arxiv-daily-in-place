{"2409.04992": {"publish_time": "2024-09-08", "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference", "paper_summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5ee3\u6cdb\u4f7f\u7528\u6a19\u8a8c\u8457\u751f\u6210\u5f0f AI \u7684\u4e00\u500b\u91cd\u8981\u91cc\u7a0b\u7891\u3002\u7136\u800c\uff0c\u96e2\u7dda LLM \u63a8\u8ad6\u4e2d\u4e0d\u65b7\u589e\u52a0\u7684\u5167\u5bb9\u9577\u5ea6\u548c\u6279\u6b21\u5927\u5c0f\u6703\u63d0\u5347\u5feb\u53d6\u503c (KV) \u7684\u8a18\u61b6\u9ad4\u9700\u6c42\uff0c\u9019\u5c0d GPU VRAM \u9020\u6210\u6975\u5927\u7684\u8ca0\u64d4\uff0c\u7279\u5225\u662f\u5728\u8cc7\u6e90\u53d7\u9650\u7684\u60c5\u6cc1\u4e0b\uff08\u4f8b\u5982\uff0c\u908a\u7de3\u904b\u7b97\u548c\u500b\u4eba\u88dd\u7f6e\uff09\u3002\u5e7e\u500b\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u89e3\u6c7a\u65b9\u6848\u5229\u7528\u4e3b\u6a5f\u8a18\u61b6\u9ad4\u6216 SSD \u4f86\u964d\u4f4e\u96e2\u7dda\u63a8\u8ad6\u5834\u666f\u7684\u5132\u5b58\u6210\u672c\u4e26\u6539\u5584\u541e\u5410\u91cf\u3002\u7136\u800c\uff0c\u7531\u65bc\u6709\u9650\u7684 PCIe \u983b\u5bec\uff0c\u5b83\u5011\u6703\u53d7\u5230\u5bc6\u96c6 KV \u5feb\u53d6\u5b58\u53d6\u6240\u9020\u6210\u7684\u986f\u8457\u6548\u80fd\u640d\u5931\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa InstInfer\uff0c\u4e00\u500b\u65b0\u7a4e\u7684 LLM \u63a8\u8ad6\u7cfb\u7d71\uff0c\u5b83\u5c07\u6548\u80fd\u6700\u95dc\u9375\u7684\u904b\u7b97\uff08\u5373\u89e3\u78bc\u968e\u6bb5\u7684\u6ce8\u610f\u529b\uff09\u548c\u8cc7\u6599\uff08\u5373 KV \u5feb\u53d6\uff09\u90e8\u5206\u5378\u8f09\u5230\u904b\u7b97\u5132\u5b58\u78c1\u789f\u6a5f (CSD)\uff0c\u9019\u5c07\u9f90\u5927\u7684 KV \u50b3\u8f38\u958b\u92b7\u964d\u5230\u6700\u4f4e\u3002InstInfer \u8a2d\u8a08\u4e86\u4e00\u500b\u5c08\u7528\u7684\u5feb\u9583\u8a18\u61b6\u9ad4\u611f\u77e5\u7684\u5132\u5b58\u5167\u6ce8\u610f\u529b\u5f15\u64ce\uff0c\u5177\u5099 KV \u5feb\u53d6\u7ba1\u7406\u6a5f\u5236\uff0c\u4ee5\u5229\u7528 CSD \u7684\u9ad8\u5167\u90e8\u983b\u5bec\uff0c\u800c\u4e0d\u662f\u53d7\u5230 PCIe \u983b\u5bec\u7684\u9650\u5236\u3002GPU \u548c CSD \u4e4b\u9593\u6700\u4f73\u5316\u7684 P2P \u50b3\u8f38\u9032\u4e00\u6b65\u964d\u4f4e\u4e86\u8cc7\u6599\u9077\u79fb\u958b\u92b7\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5c0d\u65bc\u4f7f\u7528 NVIDIA A6000 GPU \u7684 13B \u6a21\u578b\uff0c\u8207\u73fe\u6709\u7684\u57fa\u65bc SSD \u7684\u89e3\u6c7a\u65b9\u6848\uff08\u4f8b\u5982 FlexGen\uff09\u76f8\u6bd4\uff0cInstInfer \u5c07\u9577\u5e8f\u5217\u63a8\u8ad6\u7684\u541e\u5410\u91cf\u63d0\u5347\u4e86 11.1 \u500d\u3002", "author": "Xiurui Pan et.al.", "authors": "Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei Luo, Xiaolin Wang, Jie Zhang", "id": "2409.04992v1", "paper_url": "http://arxiv.org/abs/2409.04992v1", "repo": "null"}}