{"2502.12130": {"publish_time": "2025-02-17", "title": "Scaling Autonomous Agents via Automatic Reward Modeling And Planning", "paper_summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of text-generation tasks. However, LLMs still struggle with problems\nrequiring multi-step decision-making and environmental feedback, such as online\nshopping, scientific reasoning, and mathematical problem-solving. Unlike pure\ntext data, collecting large-scale decision-making data is challenging.\nMoreover, many powerful LLMs are only accessible through APIs, which hinders\ntheir fine-tuning for agent tasks due to cost and complexity. To address LLM\nagents' limitations, we propose a framework that can automatically learn a\nreward model from the environment without human annotations. This model can be\nused to evaluate the action trajectories of LLM agents and provide heuristics\nfor task planning. Specifically, our approach involves employing one LLM-based\nagent to navigate an environment randomly, generating diverse action\ntrajectories. Subsequently, a separate LLM is leveraged to assign a task intent\nand synthesize a negative response alongside the correct response for each\ntrajectory. These triplets (task intent, positive response, and negative\nresponse) are then utilized as training data to optimize a reward model capable\nof scoring action trajectories. The effectiveness and generalizability of our\nframework are demonstrated through evaluations conducted on different agent\nbenchmarks. In conclusion, our proposed framework represents a significant\nadvancement in enhancing LLM agents' decision-making capabilities. By\nautomating the learning of reward models, we overcome the challenges of data\nscarcity and API limitations, potentially revolutionizing the application of\nLLMs in complex and interactive environments. This research paves the way for\nmore sophisticated AI agents capable of tackling a wide range of real-world\nproblems requiring multi-step decision-making.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5df2\u5728\u5404\u7a2e\u6587\u5b57\u751f\u6210\u4efb\u52d9\u4e2d\u5c55\u793a\u51fa\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0cLLM \u4ecd\u7136\u5728\u9700\u8981\u591a\u6b65\u9a5f\u6c7a\u7b56\u5236\u5b9a\u548c\u74b0\u5883\u56de\u994b\u7684\u554f\u984c\u4e0a\u82e6\u82e6\u6399\u624e\uff0c\u4f8b\u5982\u7db2\u4e0a\u8cfc\u7269\u3001\u79d1\u5b78\u63a8\u7406\u548c\u6578\u5b78\u554f\u984c\u6c42\u89e3\u3002\u8207\u7d14\u6587\u672c\u6578\u64da\u4e0d\u540c\uff0c\u6536\u96c6\u5927\u898f\u6a21\u6c7a\u7b56\u5236\u5b9a\u6578\u64da\u5177\u6709\u6311\u6230\u6027\u3002\u6b64\u5916\uff0c\u8a31\u591a\u5f37\u5927\u7684 LLM \u53ea\u80fd\u901a\u904e API \u8a2a\u554f\uff0c\u9019\u7531\u65bc\u6210\u672c\u548c\u8907\u96dc\u6027\u800c\u963b\u7919\u4e86\u5b83\u5011\u5c0d\u4ee3\u7406\u4efb\u52d9\u7684\u5fae\u8abf\u3002\u70ba\u4e86\u89e3\u6c7a LLM \u4ee3\u7406\u7684\u5c40\u9650\u6027\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u6846\u67b6\uff0c\u8a72\u6846\u67b6\u53ef\u4ee5\u5f9e\u74b0\u5883\u4e2d\u81ea\u52d5\u5b78\u7fd2\u734e\u52f5\u6a21\u578b\uff0c\u800c\u7121\u9700\u4eba\u5de5\u8a3b\u91cb\u3002\u6b64\u6a21\u578b\u53ef\u7528\u4e8e\u8a55\u4f30 LLM \u4ee3\u7406\u7684\u52d5\u4f5c\u8ecc\u8de1\u4e26\u70ba\u4efb\u52d9\u898f\u5283\u63d0\u4f9b\u555f\u767c\u5f0f\u65b9\u6cd5\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u6d89\u53ca\u4f7f\u7528\u4e00\u500b\u57fa\u65bc LLM \u7684\u4ee3\u7406\u96a8\u6a5f\u5c0e\u822a\u74b0\u5883\uff0c\u751f\u6210\u4e0d\u540c\u7684\u52d5\u4f5c\u8ecc\u8de1\u3002\u96a8\u5f8c\uff0c\u5229\u7528\u4e00\u500b\u55ae\u7368\u7684 LLM \u70ba\u6bcf\u500b\u8ecc\u8de1\u5206\u914d\u4efb\u52d9\u610f\u5716\u4e26\u5408\u6210\u4e00\u500b\u8ca0\u9762\u97ff\u61c9\u4ee5\u53ca\u6b63\u78ba\u7684\u97ff\u61c9\u3002\u7136\u5f8c\u5c07\u9019\u4e9b\u4e09\u5143\u7d44\uff08\u4efb\u52d9\u610f\u5716\u3001\u6b63\u9762\u97ff\u61c9\u548c\u8ca0\u9762\u97ff\u61c9\uff09\u7528\u4f5c\u8a13\u7df4\u6578\u64da\uff0c\u4ee5\u512a\u5316\u80fd\u5920\u8a55\u5206\u52d5\u4f5c\u8ecc\u8de1\u7684\u734e\u52f5\u6a21\u578b\u3002\u6211\u5011\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u666e\u904d\u6027\u901a\u904e\u5728\u4e0d\u540c\u4ee3\u7406\u57fa\u6e96\u4e0a\u9032\u884c\u7684\u8a55\u4f30\u5f97\u5230\u8b49\u660e\u3002\u7e3d\u4e4b\uff0c\u6211\u5011\u63d0\u51fa\u7684\u6846\u67b6\u4ee3\u8868\u4e86\u52a0\u5f37 LLM \u4ee3\u7406\u6c7a\u7b56\u80fd\u529b\u7684\u91cd\u5927\u9032\u6b65\u3002\u901a\u904e\u81ea\u52d5\u5316\u734e\u52f5\u6a21\u578b\u7684\u5b78\u7fd2\uff0c\u6211\u5011\u514b\u670d\u4e86\u6578\u64da\u7a00\u7f3a\u548c API \u9650\u5236\u7684\u6311\u6230\uff0c\u6709\u53ef\u80fd\u5fb9\u5e95\u6539\u8b8a LLM \u5728\u8907\u96dc\u548c\u4e92\u52d5\u74b0\u5883\u4e2d\u7684\u61c9\u7528\u3002\u9019\u9805\u7814\u7a76\u70ba\u66f4\u8907\u96dc\u7684 AI \u4ee3\u7406\u92ea\u5e73\u4e86\u9053\u8def\uff0c\u9019\u4e9b\u4ee3\u7406\u80fd\u5920\u89e3\u6c7a\u9700\u8981\u591a\u6b65\u9a5f\u6c7a\u7b56\u5236\u5b9a\u7684\u5927\u91cf\u73fe\u5be6\u4e16\u754c\u554f\u984c\u3002", "author": "Zhenfang Chen et.al.", "authors": "Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan", "id": "2502.12130v1", "paper_url": "http://arxiv.org/abs/2502.12130v1", "repo": "null"}}