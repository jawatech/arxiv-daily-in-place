{"2405.17942": {"publish_time": "2024-05-28", "title": "Self-supervised Pre-training for Transferable Multi-modal Perception", "paper_summary": "In autonomous driving, multi-modal perception models leveraging inputs from\nmultiple sensors exhibit strong robustness in degraded environments. However,\nthese models face challenges in efficiently and effectively transferring\nlearned representations across different modalities and tasks. This paper\npresents NeRF-Supervised Masked Auto Encoder (NS-MAE), a self-supervised\npre-training paradigm for transferable multi-modal representation learning.\nNS-MAE is designed to provide pre-trained model initializations for efficient\nand high-performance fine-tuning. Our approach uses masked multi-modal\nreconstruction in neural radiance fields (NeRF), training the model to\nreconstruct missing or corrupted input data across multiple modalities.\nSpecifically, multi-modal embeddings are extracted from corrupted LiDAR point\nclouds and images, conditioned on specific view directions and locations. These\nembeddings are then rendered into projected multi-modal feature maps using\nneural rendering techniques. The original multi-modal signals serve as\nreconstruction targets for the rendered feature maps, facilitating\nself-supervised representation learning. Extensive experiments demonstrate the\npromising transferability of NS-MAE representations across diverse multi-modal\nand single-modal perception models. This transferability is evaluated on\nvarious 3D perception downstream tasks, such as 3D object detection and BEV map\nsegmentation, using different amounts of fine-tuning labeled data. Our code\nwill be released to support the community.", "paper_summary_zh": "\u5728\u81ea\u52d5\u99d5\u99db\u4e2d\uff0c\u591a\u6a21\u614b\u611f\u77e5\u6a21\u578b\u5229\u7528\u4f86\u81ea\u591a\u500b\u611f\u6e2c\u5668\u7684\u8f38\u5165\uff0c\u5728\u60e1\u52a3\u7684\u74b0\u5883\u4e2d\u8868\u73fe\u51fa\u5f37\u5927\u7684\u9b6f\u68d2\u6027\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u5728\u6709\u6548\u7387\u4e14\u6709\u6548\u5730\u5c07\u5b78\u7fd2\u5230\u7684\u8868\u5fb5\u8f49\u79fb\u5230\u4e0d\u540c\u7684\u6a21\u614b\u548c\u4efb\u52d9\u4e2d\u6642\u9762\u81e8\u6311\u6230\u3002\u672c\u6587\u63d0\u51fa\u795e\u7d93\u8f3b\u5c04\u5834 (NeRF) \u76e3\u7763\u906e\u7f69\u81ea\u52d5\u7de8\u78bc\u5668 (NS-MAE)\uff0c\u9019\u662f\u4e00\u7a2e\u7528\u65bc\u53ef\u8f49\u79fb\u591a\u6a21\u614b\u8868\u5fb5\u5b78\u7fd2\u7684\u81ea\u76e3\u7763\u9810\u8a13\u7df4\u7bc4\u4f8b\u3002NS-MAE \u88ab\u8a2d\u8a08\u70ba\u63d0\u4f9b\u9810\u8a13\u7df4\u6a21\u578b\u521d\u59cb\u5316\uff0c\u4ee5\u9032\u884c\u6709\u6548\u7387\u4e14\u9ad8\u6027\u80fd\u7684\u5fae\u8abf\u3002\u6211\u5011\u7684\u505a\u6cd5\u4f7f\u7528\u795e\u7d93\u8f3b\u5c04\u5834 (NeRF) \u4e2d\u7684\u906e\u7f69\u591a\u6a21\u614b\u91cd\u5efa\uff0c\u8a13\u7df4\u6a21\u578b\u4ee5\u91cd\u5efa\u8de8\u591a\u500b\u6a21\u614b\u7684\u907a\u5931\u6216\u640d\u58de\u8f38\u5165\u8cc7\u6599\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u591a\u6a21\u614b\u5d4c\u5165\u5f9e\u640d\u58de\u7684 LiDAR \u9ede\u96f2\u548c\u5f71\u50cf\u4e2d\u63d0\u53d6\uff0c\u4f9d\u64da\u7279\u5b9a\u7684\u8996\u5716\u65b9\u5411\u548c\u4f4d\u7f6e\u70ba\u689d\u4ef6\u3002\u7136\u5f8c\u4f7f\u7528\u795e\u7d93\u6e32\u67d3\u6280\u8853\u5c07\u9019\u4e9b\u5d4c\u5165\u6e32\u67d3\u5230\u6295\u5f71\u7684\u591a\u6a21\u614b\u7279\u5fb5\u5716\u4e2d\u3002\u539f\u59cb\u7684\u591a\u6a21\u614b\u8a0a\u865f\u7528\u4f5c\u6e32\u67d3\u7279\u5fb5\u5716\u7684\u91cd\u5efa\u76ee\u6a19\uff0c\u4fc3\u9032\u81ea\u76e3\u7763\u8868\u5fb5\u5b78\u7fd2\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8b49\u660e\u4e86 NS-MAE \u8868\u5fb5\u5728\u5404\u7a2e\u591a\u6a21\u614b\u548c\u55ae\u6a21\u614b\u611f\u77e5\u6a21\u578b\u4e2d\u5177\u6709\u826f\u597d\u7684\u53ef\u8f49\u79fb\u6027\u3002\u9019\u7a2e\u53ef\u8f49\u79fb\u6027\u5728\u5404\u7a2e 3D \u611f\u77e5\u4e0b\u6e38\u4efb\u52d9\u4e2d\u9032\u884c\u8a55\u4f30\uff0c\u4f8b\u5982 3D \u7269\u4ef6\u5075\u6e2c\u548c BEV \u5730\u5716\u5206\u5272\uff0c\u4f7f\u7528\u4e0d\u540c\u6578\u91cf\u7684\u5fae\u8abf\u6a19\u7c64\u8cc7\u6599\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5c07\u767c\u5e03\u4ee5\u652f\u63f4\u793e\u7fa4\u3002", "author": "Xiaohao Xu et.al.", "authors": "Xiaohao Xu, Tianyi Zhang, Jinrong Yang, Matthew Johnson-Roberson, Xiaonan Huang", "id": "2405.17942v1", "paper_url": "http://arxiv.org/abs/2405.17942v1", "repo": "null"}}