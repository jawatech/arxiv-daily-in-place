{"2410.17621": {"publish_time": "2024-10-23", "title": "Process Supervision-Guided Policy Optimization for Code Generation", "paper_summary": "Reinforcement Learning (RL) with unit test feedback has enhanced large\nlanguage models (LLMs) code generation, but relies on sparse rewards provided\nonly after complete code evaluation, limiting learning efficiency and\nincremental improvements. When generated code fails all unit tests, no learning\nsignal is received, hindering progress on complex tasks. To address this, we\npropose a Process Reward Model (PRM) that delivers dense, line-level feedback\non code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs\nand integrating them into the RL framework, finding that using PRMs both as\ndense rewards and for value function initialization significantly boosts\nperformance. Our approach increases our in-house LLM's pass rate from 28.2% to\n29.8% on LiveCodeBench and from 31.8% to 35.8% on our internal benchmark. Our\nexperimental results highlight the effectiveness of PRMs in enhancing RL-driven\ncode generation, especially for long-horizon scenarios.", "paper_summary_zh": "\u5f37\u5316\u5b78\u7fd2\uff08RL\uff09\u642d\u914d\u55ae\u5143\u6e2c\u8a66\u56de\u994b\u5df2\u589e\u5f37\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7a0b\u5f0f\u78bc\u751f\u6210\uff0c\u4f46\u4f9d\u8cf4\u65bc\u50c5\u5728\u5b8c\u6574\u7a0b\u5f0f\u78bc\u8a55\u4f30\u5f8c\u63d0\u4f9b\u7684\u7a00\u758f\u734e\u52f5\uff0c\u9019\u9650\u5236\u4e86\u5b78\u7fd2\u6548\u7387\u548c\u6f38\u9032\u5f0f\u6539\u9032\u3002\u7576\u7522\u751f\u7684\u7a0b\u5f0f\u78bc\u672a\u901a\u904e\u6240\u6709\u55ae\u5143\u6e2c\u8a66\u6642\uff0c\u4e0d\u6703\u6536\u5230\u4efb\u4f55\u5b78\u7fd2\u8a0a\u865f\uff0c\u9019\u963b\u7919\u4e86\u8907\u96dc\u4efb\u52d9\u7684\u9032\u5ea6\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u8655\u7406\u734e\u52f5\u6a21\u578b\uff08PRM\uff09\uff0c\u5b83\u5728\u7522\u751f\u7a0b\u5f0f\u78bc\u6642\u63d0\u4f9b\u5bc6\u96c6\u7684\u3001\u9010\u884c\u56de\u994b\uff0c\u6a21\u64ec\u4eba\u985e\u7a0b\u5f0f\u78bc\u7684\u7cbe\u7149\u4e26\u63d0\u4f9b\u5373\u6642\u6307\u5c0e\u3002\u6211\u5011\u63a2\u8a0e\u4e86\u5404\u7a2e\u8a13\u7df4 PRM \u548c\u5c07\u5176\u6574\u5408\u5230 RL \u67b6\u69cb\u4e2d\u7684\u7b56\u7565\uff0c\u767c\u73fe\u5c07 PRM \u7528\u4f5c\u5bc6\u96c6\u734e\u52f5\u548c\u7528\u65bc\u503c\u51fd\u6578\u521d\u59cb\u5316\u90fd\u80fd\u986f\u8457\u63d0\u5347\u6548\u80fd\u3002\u6211\u5011\u7684\u505a\u6cd5\u5c07\u6211\u5011\u5167\u90e8\u7684 LLM \u5728 LiveCodeBench \u4e0a\u7684\u901a\u904e\u7387\u5f9e 28.2% \u63d0\u5347\u81f3 29.8%\uff0c\u5728\u6211\u5011\u7684\u5167\u90e8\u57fa\u6e96\u4e0a\u5f9e 31.8% \u63d0\u5347\u81f3 35.8%\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u7a81\u986f\u4e86 PRM \u5728\u589e\u5f37 RL \u9a45\u52d5\u7684\u7a0b\u5f0f\u78bc\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7279\u5225\u662f\u5728\u9577\u6642\u7a0b\u60c5\u5883\u4e2d\u3002", "author": "Ning Dai et.al.", "authors": "Ning Dai, Zheng Wu, Renjie Zheng, Ziyun Wei, Wenlei Shi, Xing Jin, Guanlin Liu, Chen Dun, Liang Huang, Lin Yan", "id": "2410.17621v1", "paper_url": "http://arxiv.org/abs/2410.17621v1", "repo": "null"}}