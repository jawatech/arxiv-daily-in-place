{"2408.05093": {"publish_time": "2024-08-09", "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models", "paper_summary": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u81ea\u63a8\u51fa\u4ee5\u4f86\u5df2\u5f15\u8d77\u5ee3\u6cdb\u95dc\u6ce8\uff0c\u4e26\u5728\u5404\u500b\u5b78\u8853\u548c\u7522\u696d\u9818\u57df\u4e2d\u627e\u5230\u61c9\u7528\u3002\u7136\u800c\uff0c\u9019\u4e9b\u6a21\u578b\u5e38\u5e38\u6703\u51fa\u73fe\u300c\u5e7b\u89ba\u554f\u984c\u300d\uff0c\u5176\u8f38\u51fa\u5167\u5bb9\u96d6\u7136\u5728\u6587\u6cd5\u548c\u908f\u8f2f\u4e0a\u9023\u8cab\uff0c\u4f46\u7f3a\u4e4f\u4e8b\u5be6\u4f9d\u64da\u6216\u5b8c\u5168\u662f\u634f\u9020\u7684\u3002\u6700\u8fd1\u767c\u73fe\u4e26\u5ee3\u6cdb\u8a0e\u8ad6\u7684\u4e00\u500b\u7279\u5225\u4ee4\u4eba\u4e0d\u5b89\u7684\u554f\u984c\u662f\u6578\u503c\u6bd4\u8f03\u932f\u8aa4\uff0c\u5176\u4e2d\u591a\u500b LLM \u932f\u8aa4\u5730\u63a8\u8ad6\u51fa\u300c9.11$>$9.9\u300d\u3002\u6211\u5011\u767c\u73fe LLM \u751f\u6210\u7b54\u6848\u548c\u63a8\u7406\u7684\u9806\u5e8f\u6703\u5f71\u97ff\u5176\u4e00\u81f4\u6027\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u7576 LLM \u5148\u751f\u6210\u7b54\u6848\uff0c\u7136\u5f8c\u63d0\u4f9b\u63a8\u7406\uff0c\u8207\u5148\u751f\u6210\u63a8\u7406\u904e\u7a0b\uff0c\u7136\u5f8c\u751f\u6210\u7d50\u8ad6\u6642\uff0c\u7d50\u679c\u6703\u6709\u986f\u8457\u5dee\u7570\u3002\u53d7\u6b64\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7684\u57fa\u6e96\u65b9\u6cd5\u4f86\u8a55\u4f30 LLM \u7684\u4e00\u81f4\u6027\uff1a\u6bd4\u8f03\u901a\u904e\u9019\u5169\u7a2e\u4e0d\u540c\u65b9\u6cd5\u751f\u6210\u7684\u56de\u61c9\u3002\u6b64\u57fa\u6e96\u6709\u6548\u5730\u8b58\u5225\u51fa LLM \u7de8\u9020\u7b54\u6848\u4e26\u96a8\u5f8c\u751f\u6210\u7406\u7531\u7684\u5be6\u4f8b\u3002\u6b64\u5916\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7a4e\u4e14\u76f4\u63a5\u7684\u63d0\u793a\u7b56\u7565\uff0c\u65e8\u5728\u6e1b\u8f15\u6b64\u554f\u984c\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u8207\u76f4\u63a5\u63d0\u554f\u76f8\u6bd4\uff0c\u6b64\u7b56\u7565\u6539\u5584\u4e86\u5404\u7a2e LLM \u7684\u6027\u80fd\u3002\u9019\u9805\u5de5\u4f5c\u4e0d\u50c5\u63ed\u793a\u4e86 LLM \u7684\u4e00\u500b\u95dc\u9375\u7f3a\u9677\uff0c\u9084\u63d0\u4f9b\u4e86\u4e00\u500b\u5be6\u7528\u7684\u89e3\u6c7a\u65b9\u6848\u4f86\u589e\u5f37\u5176\u53ef\u9760\u6027\u3002", "author": "Zikai Xie et.al.", "authors": "Zikai Xie", "id": "2408.05093v1", "paper_url": "http://arxiv.org/abs/2408.05093v1", "repo": "https://github.com/xiezikai/reflexiveprompting"}}