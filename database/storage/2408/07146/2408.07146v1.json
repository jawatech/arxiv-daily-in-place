{"2408.07146": {"publish_time": "2024-08-13", "title": "Vision Language Model for Interpretable and Fine-grained Detection of Safety Compliance in Diverse Workplaces", "paper_summary": "Workplace accidents due to personal protective equipment (PPE) non-compliance\nraise serious safety concerns and lead to legal liabilities, financial\npenalties, and reputational damage. While object detection models have shown\nthe capability to address this issue by identifying safety items, most existing\nmodels, such as YOLO, Faster R-CNN, and SSD, are limited in verifying the\nfine-grained attributes of PPE across diverse workplace scenarios. Vision\nlanguage models (VLMs) are gaining traction for detection tasks by leveraging\nthe synergy between visual and textual information, offering a promising\nsolution to traditional object detection limitations in PPE recognition.\nNonetheless, VLMs face challenges in consistently verifying PPE attributes due\nto the complexity and variability of workplace environments, requiring them to\ninterpret context-specific language and visual cues simultaneously. We\nintroduce Clip2Safety, an interpretable detection framework for diverse\nworkplace safety compliance, which comprises four main modules: scene\nrecognition, the visual prompt, safety items detection, and fine-grained\nverification. The scene recognition identifies the current scenario to\ndetermine the necessary safety gear. The visual prompt formulates the specific\nvisual prompts needed for the detection process. The safety items detection\nidentifies whether the required safety gear is being worn according to the\nspecified scenario. Lastly, the fine-grained verification assesses whether the\nworn safety equipment meets the fine-grained attribute requirements. We conduct\nreal-world case studies across six different scenarios. The results show that\nClip2Safety not only demonstrates an accuracy improvement over state-of-the-art\nquestion-answering based VLMs but also achieves inference times two hundred\ntimes faster.", "paper_summary_zh": "<paragraph>\u7531\u65bc\u500b\u4eba\u9632\u8b77\u8a2d\u5099 (PPE) \u672a\u9075\u5b88\u898f\u5b9a\u800c\u5c0e\u81f4\u7684\u5de5\u4f5c\u5834\u6240\u4e8b\u6545\n\u5f15\u767c\u56b4\u91cd\u7684\u5b89\u5168\u7591\u616e\uff0c\u4e26\u5c0e\u81f4\u6cd5\u5f8b\u8cac\u4efb\u3001\u8ca1\u52d9\u7f70\u6b3e\u548c\u8072\u8b7d\u53d7\u640d\u3002\u96d6\u7136\u7269\u4ef6\u5075\u6e2c\u6a21\u578b\u5df2\u5c55\u73fe\u51fa\u900f\u904e\u8b58\u5225\u5b89\u5168\u7269\u54c1\u4f86\u89e3\u6c7a\u6b64\u554f\u984c\u7684\u80fd\u529b\uff0c\u4f46\u5927\u591a\u6578\u73fe\u6709\u7684\u6a21\u578b\uff0c\u4f8b\u5982 YOLO\u3001Faster R-CNN \u548c SSD\uff0c\u5728\u9a57\u8b49\u4e0d\u540c\u5de5\u4f5c\u5834\u6240\u5834\u666f\u4e2d PPE \u7684\u7d30\u5fae\u5c6c\u6027\u65b9\u9762\u53d7\u5230\u9650\u5236\u3002\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (VLM) \u900f\u904e\u5229\u7528\u8996\u89ba\u548c\u6587\u672c\u8cc7\u8a0a\u4e4b\u9593\u7684\u5354\u540c\u4f5c\u7528\uff0c\u5728\u5075\u6e2c\u4efb\u52d9\u4e2d\u7372\u5f97\u95dc\u6ce8\uff0c\u70ba PPE \u8fa8\u8b58\u4e2d\u50b3\u7d71\u7684\u7269\u4ef6\u5075\u6e2c\u9650\u5236\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u89e3\u6c7a\u65b9\u6848\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u7531\u65bc\u5de5\u4f5c\u5834\u6240\u74b0\u5883\u7684\u8907\u96dc\u6027\u548c\u8b8a\u7570\u6027\uff0cVLM \u5728\u6301\u7e8c\u9a57\u8b49 PPE \u5c6c\u6027\u6642\u9762\u81e8\u6311\u6230\uff0c\u9700\u8981\u5b83\u5011\u540c\u6642\u89e3\u91cb\u7279\u5b9a\u65bc\u8108\u7d61\u7684\u8a9e\u8a00\u548c\u8996\u89ba\u7dda\u7d22\u3002\u6211\u5011\u5f15\u5165\u4e86 Clip2Safety\uff0c\u9019\u662f\u4e00\u500b\u53ef\u89e3\u91cb\u7684\u5075\u6e2c\u67b6\u69cb\uff0c\u7528\u65bc\u4e0d\u540c\u7684\u5de5\u4f5c\u5834\u6240\u5b89\u5168\u5408\u898f\u6027\uff0c\u5b83\u5305\u542b\u56db\u500b\u4e3b\u8981\u6a21\u7d44\uff1a\u5834\u666f\u8fa8\u8b58\u3001\u8996\u89ba\u63d0\u793a\u3001\u5b89\u5168\u7269\u54c1\u5075\u6e2c\u548c\u7d30\u5fae\u9a57\u8b49\u3002\u5834\u666f\u8fa8\u8b58\u8b58\u5225\u7576\u524d\u5834\u666f\u4ee5\u78ba\u5b9a\u5fc5\u8981\u7684\u5b89\u5168\u88dd\u5099\u3002\u8996\u89ba\u63d0\u793a\u5236\u5b9a\u5075\u6e2c\u904e\u7a0b\u4e2d\u6240\u9700\u7684\u7279\u5b9a\u8996\u89ba\u63d0\u793a\u3002\u5b89\u5168\u7269\u54c1\u5075\u6e2c\u8b58\u5225\u662f\u5426\u6839\u64da\u6307\u5b9a\u5834\u666f\u914d\u6234\u6240\u9700\u7684\u9632\u8b77\u88dd\u5099\u3002\u6700\u5f8c\uff0c\u7d30\u5fae\u9a57\u8b49\u8a55\u4f30\u914d\u6234\u7684\u5b89\u5168\u88dd\u5099\u662f\u5426\u7b26\u5408\u7d30\u5fae\u7684\u5c6c\u6027\u8981\u6c42\u3002\u6211\u5011\u5728\u516d\u7a2e\u4e0d\u540c\u7684\u5834\u666f\u4e2d\u9032\u884c\u4e86\u771f\u5be6\u4e16\u754c\u7684\u6848\u4f8b\u7814\u7a76\u3002\u7d50\u679c\u8868\u660e\uff0cClip2Safety \u4e0d\u50c5\u8b49\u660e\u4e86\u6bd4\u6700\u5148\u9032\u7684\u57fa\u65bc\u554f\u7b54\u7684 VLM \u6e96\u78ba\u6027\u6709\u6240\u63d0\u5347\uff0c\u800c\u4e14\u63a8\u7406\u6642\u9593\u4e5f\u5feb\u4e86\u5169\u767e\u500d\u3002</paragraph>", "author": "Zhiling Chen et.al.", "authors": "Zhiling Chen, Hanning Chen, Mohsen Imani, Ruimin Chen, Farhad Imani", "id": "2408.07146v1", "paper_url": "http://arxiv.org/abs/2408.07146v1", "repo": "null"}}