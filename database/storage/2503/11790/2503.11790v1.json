{"2503.11790": {"publish_time": "2025-03-14", "title": "Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs", "paper_summary": "Human reasoning relies on constructing and manipulating mental\nmodels-simplified internal representations of situations that we use to\nunderstand and solve problems. Conceptual diagrams (for example, sketches drawn\nby humans to aid reasoning) externalize these mental models, abstracting\nirrelevant details to efficiently capture relational and spatial information.\nIn contrast, Large Language Models (LLMs) and Large Multimodal Models (LMMs)\npredominantly reason through textual representations, limiting their\neffectiveness in complex multi-step combinatorial and planning tasks. In this\npaper, we propose a zero-shot fully automatic framework that enables LMMs to\nreason through multiple chains of self-generated intermediate conceptual\ndiagrams, significantly enhancing their combinatorial planning capabilities.\nOur approach does not require any human initialization beyond a natural\nlanguage description of the task. It integrates both textual and diagrammatic\nreasoning within an optimized graph-of-thought inference framework, enhanced by\nbeam search and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves GPT-4o's performance (for\nexample, from 35.5% to 90.2% in Blocksworld). On more difficult planning\ndomains with solution depths up to 40, our approach outperforms even the\no1-preview reasoning model (for example, over 13% improvement in Parking).\nThese results highlight the value of conceptual diagrams as a complementary\nreasoning medium in LMMs.", "paper_summary_zh": "<paragraph>\u4eba\u985e\u7684\u63a8\u7406\u4f9d\u8cf4\u65bc\u69cb\u5efa\u548c\u64cd\u7e31\u5fc3\u667a\u6a21\u578b\u2014\u2014\u6211\u5011\u7528\u4f86\u7406\u89e3\u548c\u89e3\u6c7a\u554f\u984c\u7684\u60c5\u5883\u7c21\u5316\u5167\u90e8\u8868\u5fb5\u3002\u6982\u5ff5\u5716\uff08\u4f8b\u5982\uff0c\u4eba\u985e\u70ba\u4e86\u8f14\u52a9\u63a8\u7406\u800c\u7e6a\u88fd\u7684\u8349\u5716\uff09\u5c07\u9019\u4e9b\u5fc3\u667a\u6a21\u578b\u5916\u90e8\u5316\uff0c\u62bd\u8c61\u51fa\u7121\u95dc\u7684\u7d30\u7bc0\uff0c\u4ee5\u6709\u6548\u5730\u6355\u6349\u95dc\u4fc2\u548c\u7a7a\u9593\u8cc7\u8a0a\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u548c\u5927\u578b\u591a\u6a21\u614b\u6a21\u578b (LMM) \u4e3b\u8981\u900f\u904e\u6587\u672c\u8868\u5fb5\u9032\u884c\u63a8\u7406\uff0c\u9019\u9650\u5236\u4e86\u5b83\u5011\u5728\u8907\u96dc\u7684\u591a\u6b65\u9a5f\u7d44\u5408\u548c\u898f\u5283\u4efb\u52d9\u4e2d\u7684\u6709\u6548\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u96f6\u6a23\u672c\u5168\u81ea\u52d5\u6846\u67b6\uff0c\u4f7f LMM \u80fd\u5920\u900f\u904e\u591a\u500b\u81ea\u751f\u6210\u7684\u4e2d\u9593\u6982\u5ff5\u5716\u93c8\u9032\u884c\u63a8\u7406\uff0c\u5f9e\u800c\u986f\u8457\u589e\u5f37\u5176\u7d44\u5408\u898f\u5283\u80fd\u529b\u3002\u9664\u4e86\u4efb\u52d9\u7684\u81ea\u7136\u8a9e\u8a00\u63cf\u8ff0\u5916\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u4e0d\u9700\u8981\u4efb\u4f55\u4eba\u5de5\u521d\u59cb\u5316\u3002\u5b83\u5728\u4e00\u500b\u512a\u5316\u7684\u601d\u7dad\u5716\u63a8\u7406\u6846\u67b6\u5167\u6574\u5408\u4e86\u6587\u672c\u548c\u5716\u8868\u63a8\u7406\uff0c\u4e26\u900f\u904e\u96c6\u675f\u641c\u5c0b\u548c\u6df1\u5ea6\u56de\u6eaf\u9032\u884c\u4e86\u589e\u5f37\u3002\u5728\u591a\u500b\u5177\u6709\u6311\u6230\u6027\u7684 PDDL \u898f\u5283\u9818\u57df\u9032\u884c\u8a55\u4f30\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u5927\u5e45\u63d0\u9ad8\u4e86 GPT-4o \u7684\u6548\u80fd\uff08\u4f8b\u5982\uff0c\u5728 Blocksworld \u4e2d\u5f9e 35.5% \u63d0\u9ad8\u5230 90.2%\uff09\u3002\u5728\u89e3\u6c7a\u65b9\u6848\u6df1\u5ea6\u9ad8\u9054 40 \u7684\u66f4\u56f0\u96e3\u7684\u898f\u5283\u9818\u57df\uff0c\u6211\u5011\u7684\u65b9\u6cd5\u751a\u81f3\u512a\u65bc o1-preview \u63a8\u7406\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u5728 Parking \u4e2d\u63d0\u9ad8\u4e86 13% \u4ee5\u4e0a\uff09\u3002\u9019\u4e9b\u7d50\u679c\u7a81\u986f\u4e86\u6982\u5ff5\u5716\u4f5c\u70ba LMM \u4e2d\u88dc\u5145\u63a8\u7406\u5a92\u4ecb\u7684\u50f9\u503c\u3002</paragraph>\n", "author": "Nasim Borazjanizadeh et.al.", "authors": "Nasim Borazjanizadeh, Roei Herzig, Eduard Oks, Trevor Darrell, Rogerio Feris, Leonid Karlinsky", "id": "2503.11790v1", "paper_url": "http://arxiv.org/abs/2503.11790v1", "repo": "null"}}