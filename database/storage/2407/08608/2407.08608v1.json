{"2407.08608": {"publish_time": "2024-07-11", "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision", "paper_summary": "Attention, as a core layer of the ubiquitous Transformer architecture, is the\nbottleneck for large language models and long-context applications.\nFlashAttention elaborated an approach to speed up attention on GPUs through\nminimizing memory reads/writes. However, it has yet to take advantage of new\ncapabilities present in recent hardware, with FlashAttention-2 achieving only\n35% utilization on the H100 GPU. We develop three main techniques to speed up\nattention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to\n(1) overlap overall computation and data movement via warp-specialization and\n(2) interleave block-wise matmul and softmax operations, and (3) block\nquantization and incoherent processing that leverages hardware support for FP8\nlow-precision. We demonstrate that our method, FlashAttention-3, achieves\nspeedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s\n(75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate\nthat FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a\nbaseline FP8 attention.", "paper_summary_zh": "\u6ce8\u610f\u529b\uff0c\u4f5c\u70ba\u7121\u8655\u4e0d\u5728\u7684 Transformer \u67b6\u69cb\u7684\u6838\u5fc3\u5c64\uff0c\u662f\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u548c\u9577\u8a9e\u5883\u61c9\u7528\u7a0b\u5f0f\u7684\u74f6\u9838\u3002\nFlashAttention \u900f\u904e\u6700\u5c0f\u5316\u8a18\u61b6\u9ad4\u8b80\u53d6/\u5beb\u5165\uff0c\u95e1\u8ff0\u4e86\u4e00\u7a2e\u5728 GPU \u4e0a\u52a0\u901f\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u5b83\u5c1a\u672a\u5229\u7528\u6700\u65b0\u786c\u9ad4\u4e2d\u5b58\u5728\u7684\u65b0\u529f\u80fd\uff0c\u800c FlashAttention-2 \u5728 H100 GPU \u4e0a\u50c5\u9054\u5230 35% \u7684\u5229\u7528\u7387\u3002\u6211\u5011\u958b\u767c\u4e86\u4e09\u7a2e\u4e3b\u8981\u6280\u8853\u4f86\u52a0\u901f Hopper GPU \u4e0a\u7684\u6ce8\u610f\u529b\uff1a\u5229\u7528\u5f35\u91cf\u6838\u5fc3\u7684\u7570\u6b65\u6027\u548c TMA \u4f86 (1) \u900f\u904e\u626d\u66f2\u5c08\u9580\u5316\u91cd\u758a\u6574\u9ad4\u904b\u7b97\u548c\u8cc7\u6599\u79fb\u52d5\uff0c\u4ee5\u53ca (2) \u4ea4\u932f\u5340\u584a\u77e9\u9663\u4e58\u6cd5\u548c softmax \u904b\u7b97\uff0c\u4ee5\u53ca (3) \u5229\u7528\u786c\u9ad4\u5c0d FP8 \u4f4e\u7cbe\u5ea6\u7684\u652f\u63f4\u4f86\u9032\u884c\u5340\u584a\u91cf\u5316\u548c\u975e\u76f8\u5e72\u8655\u7406\u3002\u6211\u5011\u8b49\u660e\u6211\u5011\u7684 FlashAttention-3 \u65b9\u6cd5\uff0c\u5728 H100 GPU \u4e0a\u900f\u904e FP16 \u9054\u5230 1.5-2.0 \u500d\u7684\u52a0\u901f\uff0c\u6700\u9ad8\u53ef\u9054 740 TFLOP/s\uff0875% \u5229\u7528\u7387\uff09\uff0c\u800c\u900f\u904e FP8 \u63a5\u8fd1 1.2 PFLOP/s\u3002\u6211\u5011\u9a57\u8b49 FP8 FlashAttention-3 \u6bd4\u57fa\u7dda FP8 \u6ce8\u610f\u529b\u7372\u5f97\u4f4e 2.6 \u500d\u7684\u6578\u503c\u8aa4\u5dee\u3002", "author": "Jay Shah et.al.", "authors": "Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao", "id": "2407.08608v1", "paper_url": "http://arxiv.org/abs/2407.08608v1", "repo": "null"}}