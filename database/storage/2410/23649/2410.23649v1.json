{"2410.23649": {"publish_time": "2024-10-31", "title": "Deep Convolutional Neural Networks on Multiclass Classification of Three-Dimensional Brain Images for Parkinson's Disease Stage Prediction", "paper_summary": "Parkinson's disease (PD), a degenerative disorder of the central nervous\nsystem, is commonly diagnosed using functional medical imaging techniques such\nas single-photon emission computed tomography (SPECT). In this study, we\nutilized two SPECT data sets (n = 634 and n = 202) from different hospitals to\ndevelop a model capable of accurately predicting PD stages, a multiclass\nclassification task. We used the entire three-dimensional (3D) brain images as\ninput and experimented with various model architectures. Initially, we treated\nthe 3D images as sequences of two-dimensional (2D) slices and fed them\nsequentially into 2D convolutional neural network (CNN) models pretrained on\nImageNet, averaging the outputs to obtain the final predicted stage. We also\napplied 3D CNN models pretrained on Kinetics-400. Additionally, we incorporated\nan attention mechanism to account for the varying importance of different\nslices in the prediction process. To further enhance model efficacy and\nrobustness, we simultaneously trained the two data sets using weight sharing, a\ntechnique known as cotraining. Our results demonstrated that 2D models\npretrained on ImageNet outperformed 3D models pretrained on Kinetics-400, and\nmodels utilizing the attention mechanism outperformed both 2D and 3D models.\nThe cotraining technique proved effective in improving model performance when\nthe cotraining data sets were sufficiently large.", "paper_summary_zh": "\u5e15\u91d1\u68ee\u6c0f\u75c7 (PD) \u662f\u4e00\u7a2e\u4e2d\u6a1e\u795e\u7d93\u7cfb\u7d71\u9000\u5316\u6027\u75be\u75c5\uff0c\u901a\u5e38\u4f7f\u7528\u529f\u80fd\u6027\u91ab\u5b78\u5f71\u50cf\u6280\u8853\uff0c\u4f8b\u5982\u55ae\u5149\u5b50\u767c\u5c04\u65b7\u5c64\u6383\u63cf (SPECT) \u4f86\u8a3a\u65b7\u3002\u5728\u9019\u9805\u7814\u7a76\u4e2d\uff0c\u6211\u5011\u5229\u7528\u4f86\u81ea\u4e0d\u540c\u91ab\u9662\u7684\u5169\u500b SPECT \u8cc7\u6599\u96c6 (n = 634 \u548c n = 202) \u4f86\u958b\u767c\u4e00\u500b\u6a21\u578b\uff0c\u80fd\u5920\u6e96\u78ba\u9810\u6e2c PD \u5206\u671f\uff0c\u9019\u662f\u4e00\u500b\u591a\u985e\u5225\u5206\u985e\u4efb\u52d9\u3002\u6211\u5011\u4f7f\u7528\u6574\u500b\u4e09\u7dad (3D) \u5927\u8166\u5f71\u50cf\u4f5c\u70ba\u8f38\u5165\uff0c\u4e26\u5617\u8a66\u4f7f\u7528\u5404\u7a2e\u6a21\u578b\u67b6\u69cb\u3002\u6700\u521d\uff0c\u6211\u5011\u5c07 3D \u5f71\u50cf\u8996\u70ba\u4e8c\u7dad (2D) \u5207\u7247\u7684\u5e8f\u5217\uff0c\u4e26\u5c07\u5b83\u5011\u4f9d\u5e8f\u8f38\u5165\u5230\u9810\u5148\u5728 ImageNet \u4e0a\u8a13\u7df4\u904e\u7684 2D \u5377\u7a4d\u795e\u7d93\u7db2\u8def (CNN) \u6a21\u578b\u4e2d\uff0c\u53d6\u5e73\u5747\u8f38\u51fa\u503c\u4f86\u53d6\u5f97\u6700\u7d42\u9810\u6e2c\u7684\u671f\u5225\u3002\u6211\u5011\u4e5f\u61c9\u7528\u9810\u5148\u5728 Kinetics-400 \u4e0a\u8a13\u7df4\u904e\u7684 3D CNN \u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u5011\u7d0d\u5165\u4e00\u500b\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u4ee5\u8003\u91cf\u4e0d\u540c\u5207\u7247\u5728\u9810\u6e2c\u904e\u7a0b\u4e2d\u7684\u91cd\u8981\u6027\u5dee\u7570\u3002\u70ba\u4e86\u9032\u4e00\u6b65\u589e\u5f37\u6a21\u578b\u7684\u6548\u80fd\u548c\u7a69\u5065\u6027\uff0c\u6211\u5011\u4f7f\u7528\u6b0a\u91cd\u5171\u4eab\u540c\u6642\u8a13\u7df4\u5169\u500b\u8cc7\u6599\u96c6\uff0c\u9019\u662f\u4e00\u7a2e\u7a31\u70ba\u5171\u540c\u8a13\u7df4\u7684\u6280\u8853\u3002\u6211\u5011\u7684\u7d50\u679c\u986f\u793a\uff0c\u9810\u5148\u5728 ImageNet \u4e0a\u8a13\u7df4\u904e\u7684 2D \u6a21\u578b\u512a\u65bc\u9810\u5148\u5728 Kinetics-400 \u4e0a\u8a13\u7df4\u904e\u7684 3D \u6a21\u578b\uff0c\u800c\u4f7f\u7528\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u6a21\u578b\u5247\u512a\u65bc 2D \u548c 3D \u6a21\u578b\u3002\u7576\u5171\u540c\u8a13\u7df4\u7684\u8cc7\u6599\u96c6\u5920\u5927\u7684\u6642\u5019\uff0c\u5171\u540c\u8a13\u7df4\u6280\u8853\u5df2\u88ab\u8b49\u660e\u80fd\u6709\u6548\u6539\u5584\u6a21\u578b\u6548\u80fd\u3002", "author": "Guan-Hua Huang et.al.", "authors": "Guan-Hua Huang, Wan-Chen Lai, Tai-Been Chen, Chien-Chin Hsu, Huei-Yung Chen, Yi-Chen Wu, Li-Ren Yeh", "id": "2410.23649v1", "paper_url": "http://arxiv.org/abs/2410.23649v1", "repo": "null"}}