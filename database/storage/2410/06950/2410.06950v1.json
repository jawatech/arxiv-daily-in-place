{"2410.06950": {"publish_time": "2024-10-09", "title": "Faithful Interpretation for Graph Neural Networks", "paper_summary": "Currently, attention mechanisms have garnered increasing attention in Graph\nNeural Networks (GNNs), such as Graph Attention Networks (GATs) and Graph\nTransformers (GTs). It is not only due to the commendable boost in performance\nthey offer but also its capacity to provide a more lucid rationale for model\nbehaviors, which are often viewed as inscrutable. However, Attention-based GNNs\nhave demonstrated instability in interpretability when subjected to various\nsources of perturbations during both training and testing phases, including\nfactors like additional edges or nodes. In this paper, we propose a solution to\nthis problem by introducing a novel notion called Faithful Graph\nAttention-based Interpretation (FGAI). In particular, FGAI has four crucial\nproperties regarding stability and sensitivity to interpretation and final\noutput distribution. Built upon this notion, we propose an efficient\nmethodology for obtaining FGAI, which can be viewed as an ad hoc modification\nto the canonical Attention-based GNNs. To validate our proposed solution, we\nintroduce two novel metrics tailored for graph interpretation assessment.\nExperimental results demonstrate that FGAI exhibits superior stability and\npreserves the interpretability of attention under various forms of\nperturbations and randomness, which makes FGAI a more faithful and reliable\nexplanation tool.", "paper_summary_zh": "\u76ee\u524d\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5728\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u4e2d\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u4f8b\u5982\u56fe\u6ce8\u610f\u529b\u7f51\u7edc (GAT) \u548c\u56fe Transformer (GT)\u3002\u8fd9\u4e0d\u4ec5\u662f\u56e0\u4e3a\u5b83\u4eec\u63d0\u4f9b\u7684\u53ef\u559c\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fd8\u56e0\u4e3a\u5b83\u4eec\u80fd\u591f\u4e3a\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u7406\u7531\uff0c\u800c\u8fd9\u4e9b\u884c\u4e3a\u901a\u5e38\u88ab\u89c6\u4e3a\u6df1\u4e0d\u53ef\u6d4b\u7684\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684 GNN \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u53d7\u5230\u5404\u79cd\u6270\u52a8\u6e90\uff08\u5305\u62ec\u9644\u52a0\u8fb9\u6216\u8282\u70b9\u7b49\u56e0\u7d20\uff09\u65f6\uff0c\u5176\u53ef\u89e3\u91ca\u6027\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u79f0\u4e3a\u5fe0\u5b9e\u56fe\u6ce8\u610f\u529b\u89e3\u91ca (FGAI) \u7684\u65b0\u6982\u5ff5\u6765\u63d0\u51fa\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u65b9\u6848\u3002\u5177\u4f53\u6765\u8bf4\uff0cFGAI \u5177\u6709\u56db\u4e2a\u5173\u4e8e\u7a33\u5b9a\u6027\u548c\u5bf9\u89e3\u91ca\u548c\u6700\u7ec8\u8f93\u51fa\u5206\u5e03\u7684\u654f\u611f\u6027\u7684\u5173\u952e\u5c5e\u6027\u3002\u5728\u6b64\u6982\u5ff5\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u83b7\u53d6 FGAI \u7684\u6709\u6548\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u770b\u4f5c\u662f\u5bf9\u89c4\u8303\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684 GNN \u7684\u7279\u8bbe\u4fee\u6539\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u9488\u5bf9\u56fe\u89e3\u91ca\u8bc4\u4f30\u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u6307\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFGAI \u8868\u73b0\u51fa\u5353\u8d8a\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u5404\u79cd\u5f62\u5f0f\u7684\u6270\u52a8\u548c\u968f\u673a\u6027\u4e0b\u4fdd\u6301\u4e86\u6ce8\u610f\u529b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u4f7f\u5f97 FGAI \u6210\u4e3a\u4e00\u79cd\u66f4\u5fe0\u5b9e\u3001\u66f4\u53ef\u9760\u7684\u89e3\u91ca\u5de5\u5177\u3002", "author": "Lijie Hu et.al.", "authors": "Lijie Hu, Tianhao Huang, Lu Yu, Wanyu Lin, Tianhang Zheng, Di Wang", "id": "2410.06950v1", "paper_url": "http://arxiv.org/abs/2410.06950v1", "repo": "null"}}