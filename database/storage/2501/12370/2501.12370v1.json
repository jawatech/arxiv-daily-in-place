{"2501.12370": {"publish_time": "2025-01-21", "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models", "paper_summary": "Scaling the capacity of language models has consistently proven to be a\nreliable approach for improving performance and unlocking new capabilities.\nCapacity can be primarily defined by two dimensions: the number of model\nparameters and the compute per example. While scaling typically involves\nincreasing both, the precise interplay between these factors and their combined\ncontribution to overall capacity remains not fully understood. We explore this\nrelationship in the context of sparse Mixture-of-Expert models (MoEs), which\nallow scaling the number of parameters without proportionally increasing the\nFLOPs per example. We investigate how varying the sparsity level, i.e., the\nratio of non-active to total parameters, affects model performance in terms of\nboth pretraining and downstream performance. We find that under different\nconstraints (e.g. parameter size and total training compute), there is an\noptimal level of sparsity that improves both training efficiency and model\nperformance. These results provide a better understanding of the impact of\nsparsity in scaling laws for MoEs and complement existing works in this area,\noffering insights for designing more efficient architectures.", "paper_summary_zh": "\u64f4\u5c55\u8a9e\u8a00\u6a21\u578b\u7684\u5bb9\u91cf\u4e00\u76f4\u88ab\u8b49\u660e\u662f\u6539\u5584\u6548\u80fd\u4e26\u89e3\u9396\u65b0\u529f\u80fd\u7684\u53ef\u9760\u65b9\u6cd5\u3002\u5bb9\u91cf\u4e3b\u8981\u53ef\u4ee5\u7531\u5169\u500b\u9762\u5411\u5b9a\u7fa9\uff1a\u6a21\u578b\u53c3\u6578\u6578\u91cf\u548c\u6bcf\u500b\u7bc4\u4f8b\u7684\u904b\u7b97\u3002\u96d6\u7136\u64f4\u5c55\u901a\u5e38\u5305\u542b\u589e\u52a0\u5169\u8005\uff0c\u4f46\u9019\u4e9b\u56e0\u7d20\u4e4b\u9593\u7684\u7cbe\u78ba\u4ea4\u4e92\u4f5c\u7528\u53ca\u5176\u5c0d\u6574\u9ad4\u5bb9\u91cf\u7684\u5171\u540c\u8ca2\u737b\u4ecd\u672a\u5b8c\u5168\u4e86\u89e3\u3002\u6211\u5011\u5728\u7a00\u758f\u6df7\u5408\u5c08\u5bb6\u6a21\u578b (MoE) \u7684\u80cc\u666f\u4e0b\u63a2\u8a0e\u9019\u7a2e\u95dc\u4fc2\uff0c\u5b83\u5141\u8a31\u64f4\u5c55\u53c3\u6578\u6578\u91cf\uff0c\u800c\u4e0d\u6703\u6210\u6bd4\u4f8b\u5730\u589e\u52a0\u6bcf\u500b\u7bc4\u4f8b\u7684 FLOP\u3002\u6211\u5011\u7814\u7a76\u6539\u8b8a\u7a00\u758f\u7a0b\u5ea6\uff08\u5373\u975e\u6d3b\u52d5\u53c3\u6578\u8207\u7e3d\u53c3\u6578\u7684\u6bd4\u7387\uff09\u5982\u4f55\u5f71\u97ff\u6a21\u578b\u5728\u9810\u8a13\u7df4\u548c\u4e0b\u6e38\u6548\u80fd\u65b9\u9762\u7684\u6548\u80fd\u3002\u6211\u5011\u767c\u73fe\uff0c\u5728\u4e0d\u540c\u7684\u9650\u5236\u689d\u4ef6\uff08\u4f8b\u5982\u53c3\u6578\u5927\u5c0f\u548c\u7e3d\u8a13\u7df4\u904b\u7b97\uff09\u4e0b\uff0c\u5b58\u5728\u6700\u4f73\u7a00\u758f\u7a0b\u5ea6\uff0c\u53ef\u540c\u6642\u6539\u5584\u8a13\u7df4\u6548\u7387\u548c\u6a21\u578b\u6548\u80fd\u3002\u9019\u4e9b\u7d50\u679c\u8b93\u6211\u5011\u5c0d\u7a00\u758f\u6027\u5728 MoE \u7684\u64f4\u5c55\u5b9a\u5f8b\u4e2d\u7684\u5f71\u97ff\u6709\u66f4\u597d\u7684\u4e86\u89e3\uff0c\u4e26\u88dc\u5145\u4e86\u9019\u65b9\u9762\u7684\u73fe\u6709\u5de5\u4f5c\uff0c\u63d0\u4f9b\u4e86\u8a2d\u8a08\u66f4\u6709\u6548\u7387\u7684\u67b6\u69cb\u7684\u898b\u89e3\u3002", "author": "Samira Abnar et.al.", "authors": "Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed Elnouby Ali, Josh Susskind, Vimal Thilak", "id": "2501.12370v1", "paper_url": "http://arxiv.org/abs/2501.12370v1", "repo": "null"}}