{"2410.15885": {"publish_time": "2024-10-21", "title": "How to Build a Pre-trained Multimodal model for Simultaneously Chatting and Decision-making?", "paper_summary": "Existing large pre-trained models typically map text input to text output in\nan end-to-end manner, such as ChatGPT, or map a segment of text input to a\nhierarchy of action decisions, such as OpenVLA. However, humans can\nsimultaneously generate text and actions when receiving specific input signals.\nFor example, a driver can make precise driving decisions while conversing with\na friend in the passenger seat. Motivated by this observation, we consider the\nfollowing question in this work: is it possible to construct a pre-trained\nmodel that can provide both language interaction and precise decision-making\ncapabilities in dynamic open scenarios. We provide a definitive answer to this\nquestion by developing a new model architecture termed Visual Language Action\nmodel for Chatting and Decision Making (VLA4CD), and further demonstrating its\nperformance in challenging autonomous driving tasks. Specifically, we leverage\nLoRA to fine-tune a pre-trained LLM with data of multiple modalities covering\nlanguage, visual, and action. Unlike the existing LoRA operations used for LLM\nfine-tuning, we have designed new computational modules and training cost\nfunctions for VLA4CD. These designs enable VLA4CD to provide continuous-valued\naction decisions while outputting text responses. In contrast, existing LLMs\ncan only output text responses, and current VLA models can only output action\ndecisions. Moreover, these VLA models handle action data by discretizing and\nthen tokenizing the discretized actions, a method unsuitable for complex\ndecision-making tasks involving high-dimensional continuous-valued action\nvectors, such as autonomous driving. The experimental results on CARLA validate\nthat: (1) our proposed model construction method is effective; (2) compared to\nthe SOTA VLA model, VLA4CD can provide more accurate real-time decision-making\nwhile retaining the text interaction capability inherent to LLMs.", "paper_summary_zh": "\u73fe\u6709\u7684\u5927\u578b\u9810\u8a13\u7df4\u6a21\u578b\u901a\u5e38\u6703\u4ee5\u7aef\u5c0d\u7aef\u7684\u65b9\u5f0f\u5c07\u6587\u5b57\u8f38\u5165\u5c0d\u61c9\u5230\u6587\u5b57\u8f38\u51fa\uff0c\u4f8b\u5982 ChatGPT\uff0c\u6216\u5c07\u4e00\u6bb5\u6587\u5b57\u8f38\u5165\u5c0d\u61c9\u5230\u4e00\u9023\u4e32\u52d5\u4f5c\u6c7a\u7b56\u7684\u968e\u5c64\uff0c\u4f8b\u5982 OpenVLA\u3002\u7136\u800c\uff0c\u4eba\u985e\u5728\u6536\u5230\u7279\u5b9a\u7684\u8f38\u5165\u8a0a\u865f\u6642\uff0c\u53ef\u4ee5\u540c\u6642\u7522\u751f\u6587\u5b57\u548c\u52d5\u4f5c\u3002\u4f8b\u5982\uff0c\u4e00\u540d\u99d5\u99db\u5728\u8207\u5750\u5728\u526f\u99d5\u99db\u5ea7\u7684\u670b\u53cb\u4ea4\u8ac7\u6642\uff0c\u53ef\u4ee5\u505a\u51fa\u7cbe\u78ba\u7684\u99d5\u99db\u6c7a\u7b56\u3002\u53d7\u5230\u6b64\u9805\u89c0\u5bdf\u7684\u555f\u767c\uff0c\u6211\u5011\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\u601d\u8003\u4ee5\u4e0b\u554f\u984c\uff1a\u662f\u5426\u53ef\u4ee5\u5efa\u69cb\u4e00\u500b\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u5728\u52d5\u614b\u958b\u653e\u5834\u666f\u4e2d\u540c\u6642\u63d0\u4f9b\u8a9e\u8a00\u4e92\u52d5\u548c\u7cbe\u78ba\u6c7a\u7b56\u7684\u80fd\u529b\u3002\u6211\u5011\u900f\u904e\u958b\u767c\u4e00\u7a2e\u7a31\u70ba\u804a\u5929\u548c\u6c7a\u7b56\u88fd\u4f5c\u7684\u8996\u89ba\u8a9e\u8a00\u52d5\u4f5c\u6a21\u578b\uff08VLA4CD\uff09\u7684\u65b0\u6a21\u578b\u67b6\u69cb\uff0c\u4e26\u9032\u4e00\u6b65\u5c55\u793a\u5176\u5728\u5177\u6709\u6311\u6230\u6027\u7684\u81ea\u52d5\u99d5\u99db\u4efb\u52d9\u4e2d\u7684\u6548\u80fd\uff0c\u5c0d\u6b64\u554f\u984c\u7d66\u51fa\u660e\u78ba\u7684\u7b54\u6848\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5229\u7528 LoRA \u5fae\u8abf\u4e00\u500b\u9810\u8a13\u7df4\u7684 LLM\uff0c\u5176\u8cc7\u6599\u5305\u542b\u8a9e\u8a00\u3001\u8996\u89ba\u548c\u52d5\u4f5c\u7b49\u591a\u7a2e\u6a21\u5f0f\u3002\u8207\u7528\u65bc LLM \u5fae\u8abf\u7684\u73fe\u6709 LoRA \u64cd\u4f5c\u4e0d\u540c\uff0c\u6211\u5011\u70ba VLA4CD \u8a2d\u8a08\u4e86\u65b0\u7684\u904b\u7b97\u6a21\u7d44\u548c\u8a13\u7df4\u6210\u672c\u51fd\u6578\u3002\u9019\u4e9b\u8a2d\u8a08\u4f7f VLA4CD \u80fd\u5920\u5728\u8f38\u51fa\u6587\u5b57\u56de\u61c9\u7684\u540c\u6642\uff0c\u63d0\u4f9b\u9023\u7e8c\u503c\u52d5\u4f5c\u6c7a\u7b56\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u73fe\u6709\u7684 LLM \u53ea\u80fd\u8f38\u51fa\u6587\u5b57\u56de\u61c9\uff0c\u800c\u73fe\u6709\u7684 VLA \u6a21\u578b\u53ea\u80fd\u8f38\u51fa\u52d5\u4f5c\u6c7a\u7b56\u3002\u6b64\u5916\uff0c\u9019\u4e9b VLA \u6a21\u578b\u900f\u904e\u5c07\u96e2\u6563\u5316\u52d5\u4f5c\u96e2\u6563\u5316\u4e26\u9032\u884c\u6a19\u8a18\u5316\u4f86\u8655\u7406\u52d5\u4f5c\u8cc7\u6599\uff0c\u9019\u7a2e\u65b9\u6cd5\u4e0d\u9069\u5408\u6d89\u53ca\u9ad8\u7dad\u9023\u7e8c\u503c\u52d5\u4f5c\u5411\u91cf\u7684\u8907\u96dc\u6c7a\u7b56\u4efb\u52d9\uff0c\u4f8b\u5982\u81ea\u52d5\u99d5\u99db\u3002CARLA \u4e0a\u7684\u5be6\u9a57\u7d50\u679c\u9a57\u8b49\u4e86\uff1a(1) \u6211\u5011\u63d0\u51fa\u7684\u6a21\u578b\u5efa\u69cb\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff1b(2) \u8207 SOTA VLA \u6a21\u578b\u76f8\u6bd4\uff0cVLA4CD \u80fd\u5920\u5728\u4fdd\u6709 LLM \u56fa\u6709\u7684\u6587\u5b57\u4e92\u52d5\u80fd\u529b\u7684\u540c\u6642\uff0c\u63d0\u4f9b\u66f4\u6e96\u78ba\u7684\u5373\u6642\u6c7a\u7b56\u3002", "author": "Zuojin Tang et.al.", "authors": "Zuojin Tang, Bin Hu, Chenyang Zhao, De Ma, Gang Pan, Bin Liu", "id": "2410.15885v1", "paper_url": "http://arxiv.org/abs/2410.15885v1", "repo": "null"}}