{"2501.08686": {"publish_time": "2025-01-15", "title": "Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching", "paper_summary": "Traditional similarity-based schema matching methods are incapable of\nresolving semantic ambiguities and conflicts in domain-specific complex mapping\nscenarios due to missing commonsense and domain-specific knowledge. The\nhallucination problem of large language models (LLMs) also makes it challenging\nfor LLM-based schema matching to address the above issues. Therefore, we\npropose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema\nMatching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces\nnovel vector-based, graph traversal-based, and query-based graph retrievals, as\nwell as a hybrid approach and ranking schemes that identify the most relevant\nsubgraphs from external large knowledge graphs (KGs). We showcase that KG-based\nretrieval-augmented LLMs are capable of generating more accurate results for\ncomplex matching cases without any re-training. Our experimental results show\nthat KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g.,\nJellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the\nMIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the\npre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and\n21.97% in terms of precision and F1 score on the Synthea dataset, respectively.\nThe results also demonstrate that our approach is more efficient in end-to-end\nschema matching, and scales to retrieve from large KGs. Our case studies on the\ndataset from the real-world schema matching scenario exhibit that the\nhallucination problem of LLMs for schema matching is well mitigated by our\nsolution.", "paper_summary_zh": "\u50b3\u7d71\u57fa\u65bc\u76f8\u4f3c\u5ea6\u7684\u6a21\u5f0f\u6bd4\u5c0d\u65b9\u6cd5\u7121\u6cd5\u89e3\u6c7a\u7279\u5b9a\u9818\u57df\u8907\u96dc\u6bd4\u5c0d\u5834\u666f\u4e2d\u7684\u8a9e\u610f\u6a21\u7cca\u6027\u548c\u885d\u7a81\uff0c\u9019\u662f\u56e0\u70ba\u7f3a\u4e4f\u5e38\u8b58\u548c\u7279\u5b9a\u9818\u57df\u77e5\u8b58\u3002\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u5e7b\u89ba\u554f\u984c\u4e5f\u4f7f\u5f97\u57fa\u65bc LLM \u7684\u6a21\u5f0f\u6bd4\u5c0d\u96e3\u4ee5\u89e3\u6c7a\u4e0a\u8ff0\u554f\u984c\u3002\u56e0\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u77e5\u8b58\u5716\u8b5c\u7684\u6aa2\u7d22\u589e\u5f37\u751f\u6210\u6a21\u578b\uff0c\u7528\u65bc\u6a21\u5f0f\u6bd4\u5c0d\uff0c\u7a31\u70ba KG-RAG4SM\u3002\u5177\u9ad4\u800c\u8a00\uff0cKG-RAG4SM \u5f15\u5165\u4e86\u57fa\u65bc\u5411\u91cf\u7684\u3001\u57fa\u65bc\u5716\u5f62\u904d\u6b77\u7684\u548c\u57fa\u65bc\u67e5\u8a62\u7684\u5716\u5f62\u6aa2\u7d22\uff0c\u4ee5\u53ca\u4e00\u7a2e\u6df7\u5408\u65b9\u6cd5\u548c\u6392\u540d\u65b9\u6848\uff0c\u9019\u4e9b\u65b9\u6848\u5f9e\u5916\u90e8\u5927\u578b\u77e5\u8b58\u5716\u8b5c (KG) \u4e2d\u8b58\u5225\u6700\u76f8\u95dc\u7684\u5b50\u5716\u3002\u6211\u5011\u5c55\u793a\u4e86\u57fa\u65bc KG \u7684\u6aa2\u7d22\u589e\u5f37 LLM \u80fd\u5920\u5728\u4e0d\u9032\u884c\u4efb\u4f55\u91cd\u65b0\u8a13\u7df4\u7684\u60c5\u6cc1\u4e0b\u70ba\u8907\u96dc\u7684\u6bd4\u5c0d\u6848\u4f8b\u751f\u6210\u66f4\u6e96\u78ba\u7684\u7d50\u679c\u3002\u6211\u5011\u7684\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5728 MIMIC \u8cc7\u6599\u96c6\u4e0a\uff0cKG-RAG4SM \u5728\u6e96\u78ba\u5ea6\u548c F1 \u5206\u6578\u65b9\u9762\u5206\u5225\u6bd4\u57fa\u65bc LLM \u7684\u6700\u65b0 (SOTA) \u65b9\u6cd5 (\u4f8b\u5982 Jellyfish-8B) \u9ad8\u51fa 35.89% \u548c 30.50%\uff1b\u5177\u6709 GPT-4o-mini \u7684 KG-RAG4SM \u5728\u6e96\u78ba\u5ea6\u548c F1 \u5206\u6578\u65b9\u9762\u5206\u5225\u6bd4\u57fa\u65bc\u9810\u5148\u8a13\u7df4\u8a9e\u8a00\u6a21\u578b (PLM) \u7684 SOTA \u65b9\u6cd5 (\u4f8b\u5982 SMAT) \u9ad8\u51fa 69.20% \u548c 21.97% \u5728 Synthea \u8cc7\u6599\u96c6\u4e0a\u3002\u7d50\u679c\u9084\u8868\u660e\uff0c\u6211\u5011\u7684\u505a\u6cd5\u5728\u7aef\u5230\u7aef\u6a21\u5f0f\u6bd4\u5c0d\u4e2d\u66f4\u6709\u6548\u7387\uff0c\u4e26\u4e14\u53ef\u4ee5\u64f4\u5c55\u5230\u5f9e\u5927\u578b KG \u4e2d\u6aa2\u7d22\u3002\u6211\u5011\u5c0d\u4f86\u81ea\u73fe\u5be6\u4e16\u754c\u6a21\u5f0f\u6bd4\u5c0d\u5834\u666f\u7684\u8cc7\u6599\u96c6\u9032\u884c\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u6211\u5011\u7684\u89e3\u6c7a\u65b9\u6848\u5f88\u597d\u5730\u7de9\u89e3\u4e86 LLM \u5728\u6a21\u5f0f\u6bd4\u5c0d\u4e2d\u7684\u5e7b\u89ba\u554f\u984c\u3002", "author": "Chuangtao Ma et.al.", "authors": "Chuangtao Ma, Sriom Chakrabarti, Arijit Khan, B\u00e1lint Moln\u00e1r", "id": "2501.08686v1", "paper_url": "http://arxiv.org/abs/2501.08686v1", "repo": "null"}}