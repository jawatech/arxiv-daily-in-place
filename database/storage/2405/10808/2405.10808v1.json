{"2405.10808": {"publish_time": "2024-05-17", "title": "ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios", "paper_summary": "Active learning is designed to minimize annotation efforts by prioritizing\ninstances that most enhance learning. However, many active learning strategies\nstruggle with a 'cold start' problem, needing substantial initial data to be\neffective. This limitation often reduces their utility for pre-trained models,\nwhich already perform well in few-shot scenarios. To address this, we introduce\nActiveLLM, a novel active learning approach that leverages large language\nmodels such as GPT-4, Llama 3, and Mistral Large for selecting instances. We\ndemonstrate that ActiveLLM significantly enhances the classification\nperformance of BERT classifiers in few-shot scenarios, outperforming both\ntraditional active learning methods and the few-shot learning method SetFit.\nAdditionally, ActiveLLM can be extended to non-few-shot scenarios, allowing for\niterative selections. In this way, ActiveLLM can even help other active\nlearning strategies to overcome their cold start problem. Our results suggest\nthat ActiveLLM offers a promising solution for improving model performance\nacross various learning setups.", "paper_summary_zh": "\u4e3b\u52a8\u5b66\u4e60\u65e8\u5728\u901a\u8fc7\u4f18\u5148\u8003\u8651\u6700\u80fd\u4fc3\u8fdb\u5b66\u4e60\u7684\u5b9e\u4f8b\u6765\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u3002\u7136\u800c\uff0c\u8bb8\u591a\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u90fd\u9762\u4e34\u7740\u201c\u51b7\u542f\u52a8\u201d\u95ee\u9898\uff0c\u9700\u8981\u5927\u91cf\u521d\u59cb\u6570\u636e\u624d\u80fd\u6709\u6548\u3002\u6b64\u9650\u5236\u901a\u5e38\u4f1a\u964d\u4f4e\u5176\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6548\u7528\uff0c\u800c\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e2d\u5df2\u7ecf\u8868\u73b0\u826f\u597d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 ActiveLLM\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u4f8b\u5982 GPT-4\u3001Llama 3 \u548c Mistral Large\uff09\u6765\u9009\u62e9\u5b9e\u4f8b\u3002\u6211\u4eec\u8bc1\u660e ActiveLLM \u5728\u5c0f\u6837\u672c\u573a\u666f\u4e2d\u663e\u8457\u589e\u5f3a\u4e86 BERT \u5206\u7c7b\u5668\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u548c\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u65b9\u6cd5 SetFit\u3002\u6b64\u5916\uff0cActiveLLM \u53ef\u4ee5\u6269\u5c55\u5230\u975e\u5c0f\u6837\u672c\u573a\u666f\uff0c\u5141\u8bb8\u8fdb\u884c\u8fed\u4ee3\u9009\u62e9\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0cActiveLLM \u751a\u81f3\u53ef\u4ee5\u5e2e\u52a9\u5176\u4ed6\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u514b\u670d\u5176\u51b7\u542f\u52a8\u95ee\u9898\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cActiveLLM \u4e3a\u6539\u5584\u5404\u79cd\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "author": "Markus Bayer et.al.", "authors": "Markus Bayer, Christian Reuter", "id": "2405.10808v1", "paper_url": "http://arxiv.org/abs/2405.10808v1", "repo": "null"}}