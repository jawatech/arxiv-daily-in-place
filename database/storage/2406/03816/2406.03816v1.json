{"2406.03816": {"publish_time": "2024-06-06", "title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search", "paper_summary": "Recent methodologies in LLM self-training mostly rely on LLM generating\nresponses and filtering those with correct output answers as training data.\nThis approach often yields a low-quality fine-tuning training set (e.g.,\nincorrect plans or intermediate reasoning). In this paper, we develop a\nreinforced self-training approach, called ReST-MCTS*, based on integrating\nprocess reward guidance with tree search MCTS* for collecting higher-quality\nreasoning traces as well as per-step value to train policy and reward models.\nReST-MCTS* circumvents the per-step manual annotation typically used to train\nprocess rewards by tree-search-based reinforcement learning: Given oracle final\ncorrect answers, ReST-MCTS* is able to infer the correct process rewards by\nestimating the probability this step can help lead to the correct answer. These\ninferred rewards serve dual purposes: they act as value targets for further\nrefining the process reward model and also facilitate the selection of\nhigh-quality traces for policy model self-training. We first show that the\ntree-search policy in ReST-MCTS* achieves higher accuracy compared with prior\nLLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same\nsearch budget. We then show that by using traces searched by this tree-search\npolicy as training data, we can continuously enhance the three language models\nfor multiple iterations, and outperform other self-training algorithms such as\nReST$^\\text{EM}$ and Self-Rewarding LM.", "paper_summary_zh": "\u6700\u8fd1 LLM \u81ea\u6211\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e LLM \u751f\u6210\u54cd\u5e94\uff0c\u5e76\u8fc7\u6ee4\u90a3\u4e9b\u5177\u6709\u6b63\u786e\u8f93\u51fa\u7b54\u6848\u7684\u54cd\u5e94\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u5e38\u4f1a\u4ea7\u751f\u4f4e\u8d28\u91cf\u7684\u5fae\u8c03\u8bad\u7ec3\u96c6\uff08\u4f8b\u5982\uff0c\u4e0d\u6b63\u786e\u7684\u8ba1\u5212\u6216\u4e2d\u95f4\u63a8\u7406\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u79f0\u4e3a ReST-MCTS* \u7684\u5f3a\u5316\u81ea\u6211\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5c06\u8fc7\u7a0b\u5956\u52b1\u6307\u5bfc\u4e0e\u6811\u641c\u7d22 MCTS* \u76f8\u7ed3\u5408\uff0c\u4ee5\u6536\u96c6\u66f4\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u8f68\u8ff9\u4ee5\u53ca\u7528\u4e8e\u8bad\u7ec3\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\u7684\u6bcf\u6b65\u4ef7\u503c\u3002ReST-MCTS* \u901a\u8fc7\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u7ed5\u8fc7\u4e86\u901a\u5e38\u7528\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u7684\u6bcf\u6b65\u4eba\u5de5\u6ce8\u91ca\uff1a\u7ed9\u5b9a oracle \u6700\u7ec8\u6b63\u786e\u7b54\u6848\uff0cReST-MCTS* \u80fd\u591f\u901a\u8fc7\u4f30\u8ba1\u6b64\u6b65\u9aa4\u6709\u52a9\u4e8e\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u7684\u6982\u7387\u6765\u63a8\u65ad\u6b63\u786e\u7684\u8fc7\u7a0b\u5956\u52b1\u3002\u8fd9\u4e9b\u63a8\u65ad\u51fa\u7684\u5956\u52b1\u5177\u6709\u53cc\u91cd\u76ee\u7684\uff1a\u5b83\u4eec\u5145\u5f53\u8fdb\u4e00\u6b65\u4f18\u5316\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7684\u4ef7\u503c\u76ee\u6807\uff0c\u5e76\u4e14\u8fd8\u6709\u52a9\u4e8e\u4e3a\u7b56\u7565\u6a21\u578b\u81ea\u6211\u8bad\u7ec3\u9009\u62e9\u9ad8\u8d28\u91cf\u7684\u8f68\u8ff9\u3002\u6211\u4eec\u9996\u5148\u8868\u660e\uff0c\u4e0e\u5148\u524d\u7684 LLM \u63a8\u7406\u57fa\u51c6\uff08\u5982 Best-of-N \u548c Tree-of-Thought\uff09\u76f8\u6bd4\uff0cReST-MCTS* \u4e2d\u7684\u6811\u641c\u7d22\u7b56\u7565\u5728\u76f8\u540c\u7684\u641c\u7d22\u9884\u7b97\u5185\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u7136\u540e\u6211\u4eec\u8868\u660e\uff0c\u901a\u8fc7\u4f7f\u7528\u6b64\u6811\u641c\u7d22\u7b56\u7565\u641c\u7d22\u7684\u8f68\u8ff9\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u6301\u7eed\u589e\u5f3a\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u4ee5\u8fdb\u884c\u591a\u6b21\u8fed\u4ee3\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6\u81ea\u6211\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5982 ReST$^\\text{EM}$ \u548c Self-Rewarding LM\u3002", "author": "Dan Zhang et.al.", "authors": "Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, Jie Tang", "id": "2406.03816v1", "paper_url": "http://arxiv.org/abs/2406.03816v1", "repo": "null"}}