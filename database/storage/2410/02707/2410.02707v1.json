{"2410.02707": {"publish_time": "2024-10-03", "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations", "paper_summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7d93\u5e38\u6703\u7522\u751f\u932f\u8aa4\uff0c\u5305\u62ec\u4e8b\u5be6\u4e0d\u6b63\u78ba\u3001\u504f\u898b\u548c\u63a8\u7406\u5931\u6557\uff0c\u9019\u4e9b\u932f\u8aa4\u7d71\u7a31\u70ba\u300c\u5e7b\u89ba\u300d\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cLLM \u7684\u5167\u90e8\u72c0\u614b\u6703\u7de8\u78bc\u6709\u95dc\u5176\u8f38\u51fa\u771f\u5be6\u6027\u7684\u8cc7\u8a0a\uff0c\u800c\u4e14\u9019\u4e9b\u8cc7\u8a0a\u53ef\u7528\u65bc\u5075\u6e2c\u932f\u8aa4\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86 LLM \u7684\u5167\u90e8\u8868\u793a\u7de8\u78bc\u4e86\u6bd4\u5148\u524d\u8a8d\u77e5\u9084\u8981\u591a\u5f97\u591a\u7684\u771f\u5be6\u6027\u8cc7\u8a0a\u3002\u6211\u5011\u9996\u5148\u767c\u73fe\u771f\u5be6\u6027\u8cc7\u8a0a\u96c6\u4e2d\u5728\u7279\u5b9a\u4ee3\u78bc\u4e2d\uff0c\u800c\u4e14\u5229\u7528\u6b64\u7279\u6027\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u932f\u8aa4\u5075\u6e2c\u6548\u80fd\u3002\u7136\u800c\uff0c\u6211\u5011\u767c\u73fe\u6b64\u985e\u932f\u8aa4\u5075\u6e2c\u5668\u7121\u6cd5\u5728\u8cc7\u6599\u96c6\u4e4b\u9593\u5ee3\u6cdb\u61c9\u7528\uff0c\u9019\u8868\u793a\u8207\u5148\u524d\u7684\u8aaa\u6cd5\u76f8\u53cd\uff0c\u771f\u5be6\u6027\u7de8\u78bc\u4e26\u975e\u666e\u904d\u5b58\u5728\uff0c\u800c\u662f\u591a\u9762\u5411\u7684\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u5c55\u793a\u4e86\u5167\u90e8\u8868\u793a\u4e5f\u53ef\u4ee5\u7528\u65bc\u9810\u6e2c\u6a21\u578b\u53ef\u80fd\u7522\u751f\u7684\u932f\u8aa4\u985e\u578b\uff0c\u6709\u52a9\u65bc\u5236\u5b9a\u5ba2\u88fd\u5316\u7684\u7de9\u89e3\u7b56\u7565\u3002\u6700\u5f8c\uff0c\u6211\u5011\u63ed\u9732\u4e86 LLM \u5167\u90e8\u7de8\u78bc\u8207\u5916\u90e8\u884c\u70ba\u4e4b\u9593\u7684\u5dee\u7570\uff1a\u5b83\u5011\u53ef\u80fd\u6703\u7de8\u78bc\u6b63\u78ba\u7b54\u6848\uff0c\u4f46\u537b\u6301\u7e8c\u7522\u751f\u4e0d\u6b63\u78ba\u7684\u7b54\u6848\u3002\u7d9c\u5408\u800c\u8a00\uff0c\u9019\u4e9b\u898b\u89e3\u52a0\u6df1\u4e86\u6211\u5011\u5f9e\u6a21\u578b\u5167\u90e8\u89c0\u9ede\u4e86\u89e3 LLM \u932f\u8aa4\u7684\u65b9\u5f0f\uff0c\u9019\u6709\u52a9\u65bc\u5f15\u5c0e\u672a\u4f86\u589e\u5f37\u932f\u8aa4\u5206\u6790\u548c\u7de9\u89e3\u63aa\u65bd\u7684\u7814\u7a76\u3002", "author": "Hadas Orgad et.al.", "authors": "Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov", "id": "2410.02707v1", "paper_url": "http://arxiv.org/abs/2410.02707v1", "repo": "https://github.com/technion-cs-nlp/llmsknow"}}