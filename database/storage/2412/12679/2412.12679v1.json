{"2412.12679": {"publish_time": "2024-12-17", "title": "Detecting Document-level Paraphrased Machine Generated Content: Mimicking Human Writing Style and Involving Discourse Features", "paper_summary": "The availability of high-quality APIs for Large Language Models (LLMs) has\nfacilitated the widespread creation of Machine-Generated Content (MGC), posing\nchallenges such as academic plagiarism and the spread of misinformation.\nExisting MGC detectors often focus solely on surface-level information,\noverlooking implicit and structural features. This makes them susceptible to\ndeception by surface-level sentence patterns, particularly for longer texts and\nin texts that have been subsequently paraphrased.\n  To overcome these challenges, we introduce novel methodologies and datasets.\nBesides the publicly available dataset Plagbench, we developed the paraphrased\nLong-Form Question and Answer (paraLFQA) and paraphrased Writing Prompts\n(paraWP) datasets using GPT and DIPPER, a discourse paraphrasing tool, by\nextending artifacts from their original versions. To address the challenge of\ndetecting highly similar paraphrased texts, we propose MhBART, an\nencoder-decoder model designed to emulate human writing style while\nincorporating a novel difference score mechanism. This model outperforms strong\nclassifier baselines and identifies deceptive sentence patterns. To better\ncapture the structure of longer texts at document level, we propose\nDTransformer, a model that integrates discourse analysis through PDTB\npreprocessing to encode structural features. It results in substantial\nperformance gains across both datasets -- 15.5\\% absolute improvement on\nparaLFQA, 4\\% absolute improvement on paraWP, and 1.5\\% absolute improvement on\nM4 compared to SOTA approaches.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u9ad8\u54c1\u8cea API \u7684\u53ef\u7528\u6027\u4fc3\u9032\u4e86\u6a5f\u5668\u751f\u6210\u5167\u5bb9 (MGC) \u7684\u5ee3\u6cdb\u5275\u5efa\uff0c\u9019\u5c0d\u5b78\u8853\u527d\u7aca\u548c\u932f\u8aa4\u8a0a\u606f\u7684\u6563\u5e03\u69cb\u6210\u4e86\u6311\u6230\u3002\u73fe\u6709\u7684 MGC \u5075\u6e2c\u5668\u901a\u5e38\u53ea\u5c08\u6ce8\u65bc\u8868\u9762\u5c64\u7d1a\u7684\u8cc7\u8a0a\uff0c\u5ffd\u7565\u4e86\u96b1\u542b\u548c\u7d50\u69cb\u7279\u5fb5\u3002\u9019\u4f7f\u5f97\u5b83\u5011\u5bb9\u6613\u88ab\u8868\u9762\u5c64\u7d1a\u7684\u53e5\u5b50\u6a21\u5f0f\u6240\u6b3a\u9a19\uff0c\u7279\u5225\u662f\u5c0d\u65bc\u8f03\u9577\u7684\u6587\u5b57\u548c\u7d93\u904e\u91cd\u65b0\u6539\u5beb\u7684\u6587\u5b57\u3002\n\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u6311\u6230\uff0c\u6211\u5011\u5f15\u5165\u4e86\u5275\u65b0\u7684\u65b9\u6cd5\u548c\u8cc7\u6599\u96c6\u3002\u9664\u4e86\u516c\u958b\u7684\u8cc7\u6599\u96c6 Plagbench \u4e4b\u5916\uff0c\u6211\u5011\u9084\u4f7f\u7528\u4e86 GPT \u548c DIPPER\uff08\u4e00\u7a2e\u8a9e\u7bc7\u6539\u5beb\u5de5\u5177\uff09\u958b\u767c\u4e86\u6539\u5beb\u7684\u9577\u7bc7\u554f\u7b54 (paraLFQA) \u548c\u6539\u5beb\u7684\u5beb\u4f5c\u63d0\u793a (paraWP) \u8cc7\u6599\u96c6\uff0c\u65b9\u6cd5\u662f\u64f4\u5145\u5176\u539f\u59cb\u7248\u672c\u7684\u6210\u54c1\u3002\u70ba\u4e86\u61c9\u5c0d\u5075\u6e2c\u9ad8\u5ea6\u76f8\u4f3c\u6539\u5beb\u6587\u5b57\u7684\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 MhBART\uff0c\u9019\u662f\u4e00\u7a2e\u7de8\u78bc\u5668-\u89e3\u78bc\u5668\u6a21\u578b\uff0c\u65e8\u5728\u6a21\u64ec\u4eba\u985e\u7684\u5beb\u4f5c\u98a8\u683c\uff0c\u540c\u6642\u7d0d\u5165\u4e00\u7a2e\u65b0\u7a4e\u7684\u5dee\u7570\u5206\u6578\u6a5f\u5236\u3002\u6b64\u6a21\u578b\u512a\u65bc\u5f37\u5927\u7684\u5206\u985e\u5668\u57fa\u6e96\uff0c\u4e26\u8b58\u5225\u51fa\u5177\u6709\u6b3a\u9a19\u6027\u7684\u53e5\u5b50\u6a21\u5f0f\u3002\u70ba\u4e86\u66f4\u597d\u5730\u64f7\u53d6\u6587\u4ef6\u5c64\u7d1a\u8f03\u9577\u6587\u5b57\u7684\u7d50\u69cb\uff0c\u6211\u5011\u63d0\u51fa\u4e86 DTransformer\uff0c\u9019\u662f\u4e00\u7a2e\u900f\u904e PDTB \u9810\u8655\u7406\u6574\u5408\u8a9e\u7bc7\u5206\u6790\u4ee5\u7de8\u78bc\u7d50\u69cb\u7279\u5fb5\u7684\u6a21\u578b\u3002\u8207 SOTA \u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5728\u5169\u500b\u8cc7\u6599\u96c6\u4e0a\u90fd\u7522\u751f\u4e86\u986f\u8457\u7684\u6548\u80fd\u63d0\u5347\u2014\u2014\u5728 paraLFQA \u4e0a\u63d0\u5347\u4e86 15.5% \u7684\u7d55\u5c0d\u503c\uff0c\u5728 paraWP \u4e0a\u63d0\u5347\u4e86 4% \u7684\u7d55\u5c0d\u503c\uff0c\u5728 M4 \u4e0a\u63d0\u5347\u4e86 1.5% \u7684\u7d55\u5c0d\u503c\u3002", "author": "Yupei Li et.al.", "authors": "Yupei Li, Manuel Milling, Lucia Specia, Bj\u00f6rn W. Schuller", "id": "2412.12679v1", "paper_url": "http://arxiv.org/abs/2412.12679v1", "repo": "null"}}