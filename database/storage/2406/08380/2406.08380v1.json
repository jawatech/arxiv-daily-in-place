{"2406.08380": {"publish_time": "2024-06-12", "title": "Towards Unsupervised Speech Recognition Without Pronunciation Models", "paper_summary": "Recent advancements in supervised automatic speech recognition (ASR) have\nachieved remarkable performance, largely due to the growing availability of\nlarge transcribed speech corpora. However, most languages lack sufficient\npaired speech and text data to effectively train these systems. In this\narticle, we tackle the challenge of developing ASR systems without paired\nspeech and text corpora by proposing the removal of reliance on a phoneme\nlexicon. We explore a new research direction: word-level unsupervised ASR.\nUsing a curated speech corpus containing only high-frequency English words, our\nsystem achieves a word error rate of nearly 20% without parallel transcripts or\noracle word boundaries. Furthermore, we experimentally demonstrate that an\nunsupervised speech recognizer can emerge from joint speech-to-speech and\ntext-to-text masked token-infilling. This innovative model surpasses the\nperformance of previous unsupervised ASR models trained with direct\ndistribution matching.", "paper_summary_zh": "\u6700\u8fd1\u5728\u76e3\u7763\u5f0f\u81ea\u52d5\u8a9e\u97f3\u8fa8\u8b58 (ASR) \u65b9\u9762\u7684\u9032\u5c55\u5df2\u53d6\u5f97\u986f\u8457\u7684\u6210\u6548\uff0c\u9019\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u8981\u6b78\u529f\u65bc\u5927\u91cf\u8f49\u9304\u8a9e\u97f3\u8a9e\u6599\u5eab\u7684\u65e5\u76ca\u666e\u53ca\u3002\u7136\u800c\uff0c\u5927\u591a\u6578\u8a9e\u8a00\u90fd\u7f3a\u4e4f\u8db3\u5920\u7684\u914d\u5c0d\u8a9e\u97f3\u548c\u6587\u5b57\u8cc7\u6599\uff0c\u7121\u6cd5\u6709\u6548\u5730\u8a13\u7df4\u9019\u4e9b\u7cfb\u7d71\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u900f\u904e\u63d0\u8b70\u4e0d\u518d\u4f9d\u8cf4\u97f3\u7d20\u8a5e\u5f59\u8868\uff0c\u4f86\u61c9\u5c0d\u5728\u6c92\u6709\u914d\u5c0d\u8a9e\u97f3\u548c\u6587\u5b57\u8a9e\u6599\u5eab\u7684\u60c5\u6cc1\u4e0b\u958b\u767c ASR \u7cfb\u7d71\u7684\u6311\u6230\u3002\u6211\u5011\u63a2\u7d22\u4e86\u4e00\u500b\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a\u5b57\u5143\u7d1a\u5225\u7684\u975e\u76e3\u7763\u5f0f ASR\u3002\u6211\u5011\u7684\u7cfb\u7d71\u4f7f\u7528\u4e00\u500b\u7d93\u904e\u6574\u7406\u7684\u8a9e\u6599\u5eab\uff0c\u5176\u4e2d\u50c5\u5305\u542b\u9ad8\u983b\u7387\u7684\u82f1\u6587\u55ae\u5b57\uff0c\u5728\u6c92\u6709\u5e73\u884c\u8f49\u9304\u6216\u795e\u8aed\u5b57\u5143\u908a\u754c\u7684\u689d\u4ef6\u4e0b\uff0c\u5373\u53ef\u9054\u5230\u8fd1 20% \u7684\u5b57\u5143\u932f\u8aa4\u7387\u3002\u6b64\u5916\uff0c\u6211\u5011\u900f\u904e\u5be6\u9a57\u8b49\u660e\uff0c\u4e00\u500b\u975e\u76e3\u7763\u5f0f\u8a9e\u97f3\u8fa8\u8b58\u5668\u53ef\u4ee5\u5f9e\u806f\u5408\u8a9e\u97f3\u5230\u8a9e\u97f3\u548c\u6587\u5b57\u5230\u6587\u5b57\u7684\u906e\u853d\u7b26\u865f\u586b\u5165\u4e2d\u51fa\u73fe\u3002\u9019\u500b\u5275\u65b0\u7684\u6a21\u578b\u8d85\u8d8a\u4e86\u4ee5\u524d\u4f7f\u7528\u76f4\u63a5\u5206\u4f48\u5339\u914d\u8a13\u7df4\u7684\u975e\u76e3\u7763\u5f0f ASR \u6a21\u578b\u7684\u6548\u80fd\u3002", "author": "Junrui Ni et.al.", "authors": "Junrui Ni, Liming Wang, Yang Zhang, Kaizhi Qian, Heting Gao, Mark Hasegawa-Johnson, Chang D. Yoo", "id": "2406.08380v1", "paper_url": "http://arxiv.org/abs/2406.08380v1", "repo": "null"}}