{"2502.04643": {"publish_time": "2025-02-07", "title": "Confidence Elicitation: A New Attack Vector for Large Language Models", "paper_summary": "A fundamental issue in deep learning has been adversarial robustness. As\nthese systems have scaled, such issues have persisted. Currently, large\nlanguage models (LLMs) with billions of parameters suffer from adversarial\nattacks just like their earlier, smaller counterparts. However, the threat\nmodels have changed. Previously, having gray-box access, where input embeddings\nor output logits/probabilities were visible to the user, might have been\nreasonable. However, with the introduction of closed-source models, no\ninformation about the model is available apart from the generated output. This\nmeans that current black-box attacks can only utilize the final prediction to\ndetect if an attack is successful. In this work, we investigate and demonstrate\nthe potential of attack guidance, akin to using output probabilities, while\nhaving only black-box access in a classification setting. This is achieved\nthrough the ability to elicit confidence from the model. We empirically show\nthat the elicited confidence is calibrated and not hallucinated for current\nLLMs. By minimizing the elicited confidence, we can therefore increase the\nlikelihood of misclassification. Our new proposed paradigm demonstrates\npromising state-of-the-art results on three datasets across two models\n(LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique\nto existing hard-label black-box attack methods that introduce word-level\nsubstitutions.", "paper_summary_zh": "\u6df1\u5ea6\u5b78\u7fd2\u4e2d\u7684\u57fa\u672c\u554f\u984c\u4e00\u76f4\u662f\u5c0d\u6297\u9b6f\u68d2\u6027\u3002\u96a8\u8457\u9019\u4e9b\u7cfb\u7d71\u7684\u64f4\u5c55\uff0c\u9019\u4e9b\u554f\u984c\u4ecd\u7136\u5b58\u5728\u3002\u76ee\u524d\uff0c\u64c1\u6709\u6578\u5341\u5104\u500b\u53c3\u6578\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8207\u5176\u65e9\u671f\u8f03\u5c0f\u7684\u5c0d\u61c9\u6a21\u578b\u4e00\u6a23\uff0c\u6703\u53d7\u5230\u5c0d\u6297\u6027\u653b\u64ca\u3002\u7136\u800c\uff0c\u5a01\u8105\u6a21\u578b\u5df2\u7d93\u6539\u8b8a\u3002\u4ee5\u524d\uff0c\u5177\u6709\u7070\u76d2\u5b58\u53d6\u6b0a\u9650\uff0c\u5176\u4e2d\u4f7f\u7528\u8005\u53ef\u4ee5\u770b\u5230\u8f38\u5165\u5d4c\u5165\u6216\u8f38\u51fa logit/\u6a5f\u7387\uff0c\u53ef\u80fd\u662f\u5408\u7406\u7684\u3002\u7136\u800c\uff0c\u96a8\u8457\u5c01\u9589\u539f\u59cb\u78bc\u6a21\u578b\u7684\u5f15\u5165\uff0c\u9664\u4e86\u7522\u751f\u7684\u8f38\u51fa\u4e4b\u5916\uff0c\u6c92\u6709\u4efb\u4f55\u95dc\u65bc\u6a21\u578b\u7684\u8cc7\u8a0a\u53ef\u7528\u3002\u9019\u8868\u793a\u76ee\u524d\u7684 black-box \u653b\u64ca\u53ea\u80fd\u5229\u7528\u6700\u7d42\u9810\u6e2c\u4f86\u5075\u6e2c\u653b\u64ca\u662f\u5426\u6210\u529f\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63a2\u8a0e\u4e26\u5c55\u793a\u4e86\u653b\u64ca\u6307\u5c0e\u7684\u6f5b\u529b\uff0c\u985e\u4f3c\u65bc\u4f7f\u7528\u8f38\u51fa\u6a5f\u7387\uff0c\u540c\u6642\u5728\u5206\u985e\u8a2d\u5b9a\u4e2d\u53ea\u6709 black-box \u5b58\u53d6\u6b0a\u9650\u3002\u9019\u662f\u900f\u904e\u5f15\u767c\u6a21\u578b\u4fe1\u5fc3\u7684\u80fd\u529b\u4f86\u5be6\u73fe\u7684\u3002\u6211\u5011\u7d93\u9a57\u6027\u5730\u5c55\u793a\u4e86\u5f15\u767c\u7684\u4fe1\u5fc3\u662f\u7d93\u904e\u6821\u6e96\u7684\uff0c\u800c\u4e0d\u662f\u5c0d\u76ee\u524d LLM \u7522\u751f\u7684\u5e7b\u89ba\u3002\u900f\u904e\u6700\u5c0f\u5316\u5f15\u767c\u7684\u4fe1\u5fc3\uff0c\u6211\u5011\u56e0\u6b64\u53ef\u4ee5\u589e\u52a0\u8aa4\u5206\u985e\u7684\u53ef\u80fd\u6027\u3002\u6211\u5011\u65b0\u63d0\u51fa\u7684\u7bc4\u4f8b\u5728\u6bd4\u8f03\u6211\u5011\u7684\u6280\u8853\u8207\u73fe\u6709\u7684\u786c\u6a19\u7c64 black-box \u653b\u64ca\u65b9\u6cd5\uff08\u5f15\u5165\u5b57\u5143\u5c64\u7d1a\u66ff\u63db\uff09\u6642\uff0c\u5728\u5169\u500b\u6a21\u578b\uff08LLaMA-3-8B-Instruct \u548c Mistral-7B-Instruct-V0.3\uff09\u7684 3 \u500b\u8cc7\u6599\u96c6\u4e0a\u5c55\u793a\u4e86\u6709\u524d\u9014\u7684\u6700\u65b0\u7d50\u679c\u3002", "author": "Brian Formento et.al.", "authors": "Brian Formento, Chuan Sheng Foo, See-Kiong Ng", "id": "2502.04643v1", "paper_url": "http://arxiv.org/abs/2502.04643v1", "repo": "null"}}