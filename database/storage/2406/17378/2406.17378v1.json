{"2406.17378": {"publish_time": "2024-06-25", "title": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens", "paper_summary": "Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nembedding LLMs, the obtained text embedding will be able to be aligned with the\nkey tokens in the input text. We first fully analyze this phenomenon on eight\nembedding LLMs and show that this phenomenon is universal and is not affected\nby model architecture, training strategy, and embedding method. With a deeper\nanalysis, we then find that the main change in embedding space between the\nembedding LLMs and their original generative LLMs is in the first principal\ncomponent. By adjusting the first principal component, we can align text\nembedding with the key tokens. Finally, we give several examples to demonstrate\nthe vast application potential of this finding: (1) we propose a simple and\npractical sparse retrieval method based on the aligned tokens, which can\nachieve 80\\% of the dense retrieval effect of the same model while reducing the\ncomputation significantly; (2) we show that our findings provide a fresh\nperspective to help understand fuzzy concepts (e.g., semantic relatedness vs.\nsemantic similarity) and emerging technologies (e.g., instruction-following\nembedding) in this field.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u6587\u5b57\u5d4c\u5165\u5728\u8cc7\u8a0a\u6aa2\u7d22\u3001\u8a9e\u7fa9\u6587\u5b57\u76f8\u4f3c\u5ea6\u7b49\u4efb\u52d9\u4e2d\u7372\u5f97\u4e86\u6975\u4f73\u7684\u7d50\u679c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c55\u793a\u4e86\u4e00\u500b\u6709\u8da3\u7684\u767c\u73fe\uff1a\u7576\u5c07\u6587\u5b57\u8f38\u5165\u5d4c\u5165\u5f0f LLM \u6642\uff0c\u7372\u5f97\u7684\u6587\u5b57\u5d4c\u5165\u5c07\u80fd\u5920\u8207\u8f38\u5165\u6587\u5b57\u4e2d\u7684\u95dc\u9375\u6a19\u8a18\u5c0d\u9f4a\u3002\u6211\u5011\u9996\u5148\u5728\u516b\u500b\u5d4c\u5165\u5f0f LLM \u4e0a\u5168\u9762\u5206\u6790\u4e86\u6b64\u73fe\u8c61\uff0c\u4e26\u8868\u660e\u6b64\u73fe\u8c61\u662f\u666e\u904d\u7684\uff0c\u4e0d\u53d7\u6a21\u578b\u67b6\u69cb\u3001\u8a13\u7df4\u7b56\u7565\u548c\u5d4c\u5165\u65b9\u6cd5\u7684\u5f71\u97ff\u3002\u900f\u904e\u66f4\u6df1\u5165\u7684\u5206\u6790\uff0c\u6211\u5011\u767c\u73fe\u5d4c\u5165\u5f0f LLM \u8207\u5176\u539f\u59cb\u751f\u6210\u5f0f LLM \u4e4b\u9593\u5d4c\u5165\u7a7a\u9593\u7684\u4e3b\u8981\u8b8a\u5316\u5728\u65bc\u7b2c\u4e00\u4e3b\u6210\u5206\u3002\u900f\u904e\u8abf\u6574\u7b2c\u4e00\u4e3b\u6210\u5206\uff0c\u6211\u5011\u53ef\u4ee5\u5c07\u6587\u5b57\u5d4c\u5165\u8207\u95dc\u9375\u6a19\u8a18\u5c0d\u9f4a\u3002\u6700\u5f8c\uff0c\u6211\u5011\u8209\u4e86\u5e7e\u500b\u4f8b\u5b50\u4f86\u8aaa\u660e\u6b64\u767c\u73fe\u7684\u5ee3\u6cdb\u61c9\u7528\u6f5b\u529b\uff1a(1) \u6211\u5011\u63d0\u51fa\u4e00\u500b\u57fa\u65bc\u5c0d\u9f4a\u6a19\u8a18\u7684\u7c21\u55ae\u4e14\u5be6\u7528\u7684\u7a00\u758f\u6aa2\u7d22\u65b9\u6cd5\uff0c\u5b83\u53ef\u4ee5\u5728\u986f\u8457\u964d\u4f4e\u904b\u7b97\u91cf\u7684\u540c\u6642\uff0c\u9054\u5230\u76f8\u540c\u6a21\u578b 80% \u7684\u7a20\u5bc6\u6aa2\u7d22\u6548\u679c\uff1b(2) \u6211\u5011\u5c55\u793a\u6211\u5011\u7684\u767c\u73fe\u63d0\u4f9b\u4e86\u4e00\u500b\u65b0\u7684\u89c0\u9ede\uff0c\u6709\u52a9\u65bc\u7406\u89e3\u6b64\u9818\u57df\u4e2d\u7684\u6a21\u7cca\u6982\u5ff5\uff08\u4f8b\u5982\uff0c\u8a9e\u7fa9\u76f8\u95dc\u6027\u8207\u8a9e\u7fa9\u76f8\u4f3c\u6027\uff09\u548c\u65b0\u8208\u6280\u8853\uff08\u4f8b\u5982\uff0c\u9075\u5faa\u6307\u4ee4\u7684\u5d4c\u5165\uff09\u3002", "author": "Zhijie Nie et.al.", "authors": "Zhijie Nie, Richong Zhang, Zhanyu Wu", "id": "2406.17378v1", "paper_url": "http://arxiv.org/abs/2406.17378v1", "repo": "null"}}