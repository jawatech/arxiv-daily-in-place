{"2406.17415": {"publish_time": "2024-06-25", "title": "Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels", "paper_summary": "We present a simple variable quantization approach that quantizes different\nlayers of a large language model (LLM) at different bit levels. Specifically,\nwe quantize the most important layers to higher bit precision and less\nimportant layers to lower bits to achieve floating point quantization levels.\nWe propose two effective strategies to measure the importance of layers within\nLLMs: the first measures the importance of a layer based on how different its\noutput embeddings are from the input embeddings (the higher the better); the\nsecond estimates the importance of a layer using the number of layer weights\nthat are much larger than average (the smaller the better). We show that\nquantizing different layers at varying bits according to our importance scores\nresults in minimal performance drop with a far more compressed model size.\nFinally, we present several practical key takeaways from our variable\nlayer-wise quantization experiments: (a) LLM performance under variable\nquantization remains close to the original model until 25-50% of layers are\nmoved in lower quantization using our proposed ordering but only until 5-10% if\nmoved using no specific ordering; (b) Quantizing LLMs to lower bits performs\nsubstantially better than pruning unless extreme quantization (2-bit) is used;\nand (c) Layer-wise quantization to lower bits works better in the case of\nlarger LLMs with more layers compared to smaller LLMs with fewer layers. The\ncode used to run the experiments is available at:\nhttps://github.com/RazvanDu/LayerwiseQuant.", "paper_summary_zh": "<paragraph>\u6211\u5011\u63d0\u51fa\u4e00\u500b\u7c21\u55ae\u7684\u8b8a\u6578\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u4e0d\u540c\u7684\u4f4d\u5143\u7d1a\u5225\u91cf\u5316\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u7684\u4e0d\u540c\u5c64\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u5c07\u6700\u91cd\u8981\u7684\u5c64\u91cf\u5316\u70ba\u66f4\u9ad8\u7684\u4f4d\u5143\u7cbe\u5ea6\uff0c\u800c\u5c07\u8f03\u4e0d\u91cd\u8981\u7684\u5c64\u91cf\u5316\u70ba\u8f03\u4f4e\u7684\u4f4d\u5143\uff0c\u4ee5\u5be6\u73fe\u6d6e\u9ede\u91cf\u5316\u7d1a\u5225\u3002\u6211\u5011\u63d0\u51fa\u4e86\u5169\u7a2e\u6709\u6548\u7684\u7b56\u7565\u4f86\u8861\u91cf LLM \u5167\u5c64\u7684\u91cd\u8981\u6027\uff1a\u7b2c\u4e00\u500b\u7b56\u7565\u6839\u64da\u5c64\u7684\u8f38\u51fa\u5d4c\u5165\u8207\u8f38\u5165\u5d4c\u5165\u7684\u5dee\u7570\u4f86\u8861\u91cf\u5c64\u7684\u91cd\u8981\u6027\uff08\u5dee\u7570\u8d8a\u5927\u8d8a\u597d\uff09\uff1b\u7b2c\u4e8c\u500b\u7b56\u7565\u4f7f\u7528\u9060\u5927\u65bc\u5e73\u5747\u503c\u7684\u5c64\u6b0a\u91cd\u7684\u6578\u91cf\u4f86\u4f30\u8a08\u5c64\u7684\u91cd\u8981\u6027\uff08\u8d8a\u5c0f\u8d8a\u597d\uff09\u3002\u6211\u5011\u8868\u660e\uff0c\u6839\u64da\u6211\u5011\u7684\u91cd\u8981\u6027\u5206\u6578\uff0c\u4ee5\u4e0d\u540c\u7684\u4f4d\u5143\u91cf\u5316\u4e0d\u540c\u7684\u5c64\uff0c\u6703\u5c0e\u81f4\u6548\u80fd\u4e0b\u964d\u6975\u5c0f\uff0c\u4f46\u6a21\u578b\u5927\u5c0f\u537b\u5927\u5e45\u58d3\u7e2e\u3002\u6700\u5f8c\uff0c\u6211\u5011\u5f9e\u53ef\u8b8a\u5c64\u7d1a\u91cf\u5316\u5be6\u9a57\u4e2d\u63d0\u51fa\u5e7e\u500b\u5be6\u7528\u7684\u95dc\u9375\u7d50\u8ad6\uff1a(a) \u5728\u53ef\u8b8a\u91cf\u5316\u4e0b\uff0cLLM \u6548\u80fd\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\uff0c\u76f4\u5230 25-50% \u7684\u5c64\u4f7f\u7528\u6211\u5011\u5efa\u8b70\u7684\u6392\u5e8f\u79fb\u81f3\u8f03\u4f4e\u91cf\u5316\uff0c\u4f46\u5982\u679c\u4f7f\u7528\u6c92\u6709\u7279\u5b9a\u6392\u5e8f\uff0c\u5247\u53ea\u6703\u63a5\u8fd1 5-10%\uff1b(b) \u5c07 LLM \u91cf\u5316\u70ba\u8f03\u4f4e\u7684\u4f4d\u5143\u6bd4\u4fee\u526a\u57f7\u884c\u5f97\u66f4\u597d\uff0c\u9664\u975e\u4f7f\u7528\u6975\u7aef\u91cf\u5316 (2 \u4f4d\u5143)\uff1b(c) \u8207\u5177\u6709\u8f03\u5c11\u5c64\u7684\u5c0f\u578b LLM \u76f8\u6bd4\uff0c\u5c64\u7d1a\u91cf\u5316\u5230\u8f03\u4f4e\u4f4d\u5143\u5728\u5177\u6709\u8f03\u591a\u5c64\u7684\u5927\u578b LLM \u4e2d\u6548\u679c\u66f4\u597d\u3002\u7528\u65bc\u57f7\u884c\u5be6\u9a57\u7684\u7a0b\u5f0f\u78bc\u53ef\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u53d6\u5f97\uff1ahttps://github.com/RazvanDu/LayerwiseQuant\u3002</paragraph>", "author": "Razvan-Gabriel Dumitru et.al.", "authors": "Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu", "id": "2406.17415v2", "paper_url": "http://arxiv.org/abs/2406.17415v2", "repo": "null"}}