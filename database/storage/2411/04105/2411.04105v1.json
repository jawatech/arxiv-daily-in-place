{"2411.04105": {"publish_time": "2024-11-06", "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis", "paper_summary": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u5728\u9700\u8981\u898f\u5283\u548c\u63a8\u7406\u7684\u4efb\u52d9\u4e0a\u8868\u73fe\u51fa\u8272\u3002\u53d7\u6b64\u555f\u767c\uff0c\u6211\u5011\u7814\u7a76\u4e86\u652f\u6490\u7db2\u8def\u57f7\u884c\u8907\u96dc\u908f\u8f2f\u63a8\u7406\u80fd\u529b\u7684\u5167\u90e8\u6a5f\u5236\u3002\u6211\u5011\u9996\u5148\u69cb\u5efa\u4e86\u4e00\u500b\u5408\u6210\u547d\u984c\u908f\u8f2f\u554f\u984c\uff0c\u4f5c\u70ba\u7db2\u8def\u8a13\u7df4\u548c\u8a55\u4f30\u7684\u5177\u9ad4\u6e2c\u8a66\u5e73\u53f0\u3002\u81f3\u95dc\u91cd\u8981\u7684\u662f\uff0c\u9019\u500b\u554f\u984c\u9700\u8981\u975e\u5e73\u51e1\u7684\u898f\u5283\u624d\u80fd\u89e3\u6c7a\uff0c\u4f46\u6211\u5011\u53ef\u4ee5\u8a13\u7df4\u4e00\u500b\u5c0f\u578bTransformer\u4f86\u5be6\u73fe\u5b8c\u7f8e\u7684\u6e96\u78ba\u6027\u3002\u5efa\u7acb\u5728\u6211\u5011\u7684\u8a2d\u7f6e\u4e4b\u4e0a\uff0c\u6211\u5011\u63a5\u8457\u63a2\u8a0e\u4e00\u500b\u5f9e\u982d\u8a13\u7df4\u7684\u4e09\u5c64Transformer\u5982\u4f55\u89e3\u6c7a\u9019\u500b\u554f\u984c\u3002\u6211\u5011\u80fd\u5920\u8b58\u5225\u7db2\u8def\u4e2d\u67d0\u4e9b\u300c\u898f\u5283\u300d\u548c\u300c\u63a8\u7406\u300d\u96fb\u8def\uff0c\u5b83\u5011\u9700\u8981\u6ce8\u610f\u529b\u5340\u584a\u4e4b\u9593\u7684\u5408\u4f5c\u4f86\u5be6\u73fe\u6240\u9700\u7684\u908f\u8f2f\u3002\u70ba\u4e86\u64f4\u5c55\u6211\u5011\u7684\u767c\u73fe\uff0c\u6211\u5011\u63a5\u8457\u7814\u7a76\u4e00\u500b\u66f4\u5927\u7684\u6a21\u578b\uff0cMistral 7B\u3002\u4f7f\u7528\u6fc0\u6d3b\u4fee\u88dc\uff0c\u6211\u5011\u63cf\u8ff0\u4e86\u5728\u89e3\u6c7a\u6211\u5011\u7684\u908f\u8f2f\u554f\u984c\u4e2d\u81f3\u95dc\u91cd\u8981\u7684\u5167\u90e8\u7d44\u4ef6\u3002\u7e3d\u7684\u4f86\u8aaa\uff0c\u6211\u5011\u7684\u7814\u7a76\u7cfb\u7d71\u5730\u63ed\u793a\u4e86\u5c0f\u578b\u548c\u5927\u578bTransformer\u7684\u65b0\u65b9\u9762\uff0c\u4e26\u7e7c\u7e8c\u7814\u7a76\u5b83\u5011\u662f\u5982\u4f55\u898f\u5283\u548c\u63a8\u7406\u7684\u3002", "author": "Guan Zhe Hong et.al.", "authors": "Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Rina Panigrahy", "id": "2411.04105v1", "paper_url": "http://arxiv.org/abs/2411.04105v1", "repo": "null"}}