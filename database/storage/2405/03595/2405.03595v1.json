{"2405.03595": {"publish_time": "2024-05-06", "title": "GREEN: Generative Radiology Report Evaluation and Error Notation", "paper_summary": "Evaluating radiology reports is a challenging problem as factual correctness\nis extremely important due to the need for accurate medical communication about\nmedical images. Existing automatic evaluation metrics either suffer from\nfailing to consider factual correctness (e.g., BLEU and ROUGE) or are limited\nin their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we\nintroduce GREEN (Generative Radiology Report Evaluation and Error Notation), a\nradiology report generation metric that leverages the natural language\nunderstanding of language models to identify and explain clinically significant\nerrors in candidate reports, both quantitatively and qualitatively. Compared to\ncurrent metrics, GREEN offers: 1) a score aligned with expert preferences, 2)\nhuman interpretable explanations of clinically significant errors, enabling\nfeedback loops with end-users, and 3) a lightweight open-source method that\nreaches the performance of commercial counterparts. We validate our GREEN\nmetric by comparing it to GPT-4, as well as to error counts of 6 experts and\npreferences of 2 experts. Our method demonstrates not only higher correlation\nwith expert error counts, but simultaneously higher alignment with expert\npreferences when compared to previous approaches.\"", "paper_summary_zh": "\u653e\u5c04\u5b78\u5831\u544a\u8a55\u4f30\u662f\u4e00\u500b\u5177\u6709\u6311\u6230\u6027\u7684\u554f\u984c\uff0c\u56e0\u70ba\u4e8b\u5be6\u6b63\u78ba\u6027\u7531\u65bc\u6e96\u78ba\u7684\u91ab\u7642\u5f71\u50cf\u91ab\u5b78\u6e9d\u901a\u9700\u6c42\u800c\u6975\u70ba\u91cd\u8981\u3002\u73fe\u6709\u7684\u81ea\u52d5\u8a55\u4f30\u6307\u6a19\u6703\u56e0\u70ba\u672a\u80fd\u8003\u616e\u4e8b\u5be6\u6b63\u78ba\u6027\uff08\u4f8b\u5982\uff0cBLEU \u548c ROUGE\uff09\u800c\u6709\u6240\u7f3a\u5931\uff0c\u6216\u5728\u53ef\u89e3\u91cb\u6027\u4e0a\u53d7\u5230\u9650\u5236\uff08\u4f8b\u5982\uff0cF1CheXpert \u548c F1RadGraph\uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39 GREEN\uff08\u751f\u6210\u5f0f\u653e\u5c04\u5b78\u5831\u544a\u8a55\u4f30\u548c\u932f\u8aa4\u6a19\u8a18\uff09\uff0c\u9019\u662f\u4e00\u500b\u653e\u5c04\u5b78\u5831\u544a\u751f\u6210\u6307\u6a19\uff0c\u5b83\u5229\u7528\u8a9e\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8a9e\u8a00\u7406\u89e3\u4f86\u8b58\u5225\u548c\u89e3\u91cb\u5019\u9078\u5831\u544a\u4e2d\u5728\u81e8\u5e8a\u4e0a\u986f\u8457\u7684\u932f\u8aa4\uff0c\u7121\u8ad6\u5728\u91cf\u5316\u6216\u8cea\u5316\u4e0a\u3002\u8207\u76ee\u524d\u7684\u6307\u6a19\u76f8\u6bd4\uff0cGREEN \u63d0\u4f9b\uff1a1) \u8207\u5c08\u5bb6\u504f\u597d\u4e00\u81f4\u7684\u5206\u6578\uff0c2) \u5c0d\u81e8\u5e8a\u4e0a\u986f\u8457\u7684\u932f\u8aa4\u9032\u884c\u4eba\u985e\u53ef\u89e3\u91cb\u7684\u8aaa\u660e\uff0c\u8b93\u6700\u7d42\u4f7f\u7528\u8005\u80fd\u5920\u9032\u884c\u53cd\u994b\u8ff4\u8def\uff0c\u4ee5\u53ca 3) \u4e00\u7a2e\u8f15\u91cf\u5316\u7684\u958b\u6e90\u65b9\u6cd5\uff0c\u53ef\u9054\u5230\u5546\u696d\u5c0d\u61c9\u7522\u54c1\u7684\u6548\u80fd\u3002\u6211\u5011\u900f\u904e\u5c07 GREEN \u6307\u6a19\u8207 GPT-4 \u9032\u884c\u6bd4\u8f03\uff0c\u4ee5\u53ca\u8207 6 \u4f4d\u5c08\u5bb6\u7684\u932f\u8aa4\u8a08\u6578\u548c 2 \u4f4d\u5c08\u5bb6\u7684\u504f\u597d\u9032\u884c\u6bd4\u8f03\uff0c\u4f86\u9a57\u8b49\u6211\u5011\u7684 GREEN \u6307\u6a19\u3002\u8207\u5148\u524d\u7684\u505a\u6cd5\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u505a\u6cd5\u4e0d\u50c5\u986f\u793a\u51fa\u8207\u5c08\u5bb6\u932f\u8aa4\u8a08\u6578\u6709\u66f4\u9ad8\u7684\u76f8\u95dc\u6027\uff0c\u540c\u6642\u4e5f\u8207\u5c08\u5bb6\u504f\u597d\u6709\u66f4\u9ad8\u7684\u543b\u5408\u6027\u3002", "author": "Sophie Ostmeier et.al.", "authors": "Sophie Ostmeier, Justin Xu, Zhihong Chen, Maya Varma, Louis Blankemeier, Christian Bluethgen, Arne Edward Michalson, Michael Moseley, Curtis Langlotz, Akshay S Chaudhari, Jean-Benoit Delbrouck", "id": "2405.03595v1", "paper_url": "http://arxiv.org/abs/2405.03595v1", "repo": "null"}}