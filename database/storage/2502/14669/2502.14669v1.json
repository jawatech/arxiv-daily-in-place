{"2502.14669": {"publish_time": "2025-02-20", "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO", "paper_summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nlanguage processing, yet they often struggle with tasks requiring genuine\nvisual spatial reasoning. In this paper, we introduce a novel two-stage\ntraining framework designed to equip standard LLMs with visual reasoning\nabilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT)\non a curated dataset of tokenized maze representations to teach the model to\npredict step-by-step movement commands. Next, we apply Group Relative Policy\nOptimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted\nreward function to refine the model's sequential decision-making and encourage\nemergent chain-of-thought behaviors. Experimental results on synthetically\ngenerated mazes show that while a baseline model fails to navigate the maze,\nthe SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning\nboosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more\nrobust and self-corrective reasoning, highlighting the potential of our\napproach to bridge the gap between language models and visual spatial tasks.\nThese findings offer promising implications for applications in robotics,\nautonomous navigation, and other domains that require integrated visual and\nsequential reasoning.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8a9e\u8a00\u8655\u7406\u65b9\u9762\u5c55\u73fe\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u5011\u7d93\u5e38\u96e3\u4ee5\u61c9\u4ed8\u9700\u8981\u771f\u6b63\u8996\u89ba\u7a7a\u9593\u63a8\u7406\u7684\u4efb\u52d9\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u5169\u968e\u6bb5\u8a13\u7df4\u67b6\u69cb\uff0c\u65e8\u5728\u70ba\u6a19\u6e96 LLM \u63d0\u4f9b\u8ff7\u5bae\u5c0e\u822a\u7684\u8996\u89ba\u63a8\u7406\u80fd\u529b\u3002\u9996\u5148\uff0c\u6211\u5011\u5728\u6a19\u8a18\u5316\u8ff7\u5bae\u8868\u793a\u7684\u7b56\u5c55\u8cc7\u6599\u96c6\u4e0a\u5229\u7528\u76e3\u7763\u5fae\u8abf\uff08SFT\uff09\u4f86\u6559\u5c0e\u6a21\u578b\u9810\u6e2c\u9010\u6b65\u79fb\u52d5\u6307\u4ee4\u3002\u63a5\u4e0b\u4f86\uff0c\u6211\u5011\u4f7f\u7528 DeepSeekR1 \u4e2d\u4f7f\u7528\u7684\u6280\u8853\uff0c\u5373\u7fa4\u9ad4\u76f8\u5c0d\u7b56\u7565\u6700\u4f73\u5316\uff08GRPO\uff09\uff0c\u4e26\u642d\u914d\u7cbe\u5fc3\u8a2d\u8a08\u7684\u734e\u52f5\u51fd\u6578\u4f86\u512a\u5316\u6a21\u578b\u7684\u9806\u5e8f\u6c7a\u7b56\u5236\u5b9a\uff0c\u4e26\u9f13\u52f5\u51fa\u73fe\u9023\u8cab\u7684\u601d\u8003\u884c\u70ba\u3002\u5728\u5408\u6210\u7522\u751f\u7684\u8ff7\u5bae\u4e0a\u9032\u884c\u7684\u5be6\u9a57\u7d50\u679c\u986f\u793a\uff0c\u96d6\u7136\u57fa\u6e96\u6a21\u578b\u7121\u6cd5\u5c0e\u822a\u8ff7\u5bae\uff0c\u4f46\u7d93\u904e SFT \u8a13\u7df4\u7684\u6a21\u578b\u9054\u5230 86% \u7684\u6e96\u78ba\u5ea6\uff0c\u800c\u9032\u4e00\u6b65\u7684 GRPO \u5fae\u8abf\u5c07\u6e96\u78ba\u5ea6\u63d0\u5347\u81f3 93%\u3002\u5b9a\u6027\u5206\u6790\u986f\u793a\uff0cGRPO \u4fc3\u9032\u66f4\u5f37\u5065\u4e14\u81ea\u6211\u4fee\u6b63\u7684\u63a8\u7406\uff0c\u51f8\u986f\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u5f4c\u5408\u8a9e\u8a00\u6a21\u578b\u8207\u8996\u89ba\u7a7a\u9593\u4efb\u52d9\u4e4b\u9593\u5dee\u8ddd\u7684\u6f5b\u529b\u3002\u9019\u4e9b\u767c\u73fe\u70ba\u6a5f\u5668\u4eba\u3001\u81ea\u4e3b\u5c0e\u822a\u548c\u5176\u4ed6\u9700\u8981\u6574\u5408\u8996\u89ba\u548c\u9806\u5e8f\u63a8\u7406\u7684\u9818\u57df\u7684\u61c9\u7528\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u555f\u793a\u3002", "author": "Alan Dao et.al.", "authors": "Alan Dao, Dinh Bach Vu", "id": "2502.14669v1", "paper_url": "http://arxiv.org/abs/2502.14669v1", "repo": "null"}}