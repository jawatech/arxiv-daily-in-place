{"2409.08596": {"publish_time": "2024-09-13", "title": "Large Language Model Can Transcribe Speech in Multi-Talker Scenarios with Versatile Instructions", "paper_summary": "Recent advancements in large language models (LLMs) have revolutionized\nvarious domains, bringing significant progress and new opportunities. Despite\nprogress in speech-related tasks, LLMs have not been sufficiently explored in\nmulti-talker scenarios. In this work, we present a pioneering effort to\ninvestigate the capability of LLMs in transcribing speech in multi-talker\nenvironments, following versatile instructions related to multi-talker\nautomatic speech recognition (ASR), target talker ASR, and ASR based on\nspecific talker attributes such as sex, occurrence order, language, and keyword\nspoken. Our approach utilizes WavLM and Whisper encoder to extract\nmulti-faceted speech representations that are sensitive to speaker\ncharacteristics and semantic context. These representations are then fed into\nan LLM fine-tuned using LoRA, enabling the capabilities for speech\ncomprehension and transcription. Comprehensive experiments reveal the promising\nperformance of our proposed system, MT-LLM, in cocktail party scenarios,\nhighlighting the potential of LLM to handle speech-related tasks based on user\ninstructions in such complex settings.", "paper_summary_zh": "\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fd1\u671f\u9032\u5c55\u5fb9\u5e95\u6539\u8b8a\u4e86\u5404\u500b\u9818\u57df\uff0c\u5e36\u4f86\u986f\u8457\u7684\u9032\u5c55\u548c\u65b0\u6a5f\u9047\u3002\u5118\u7ba1\u5728\u8207\u8a9e\u97f3\u76f8\u95dc\u7684\u4efb\u52d9\u4e2d\u53d6\u5f97\u9032\u5c55\uff0c\u4f46 LLM \u5c1a\u672a\u5728\u591a\u8aaa\u8a71\u8005\u5834\u666f\u4e2d\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5728\u9019\u9805\u5de5\u4f5c\u4e2d\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u9805\u958b\u5275\u6027\u7684\u52aa\u529b\uff0c\u4ee5\u7814\u7a76 LLM \u5728\u591a\u8aaa\u8a71\u8005\u74b0\u5883\u4e2d\u8f49\u9304\u8a9e\u97f3\u7684\u80fd\u529b\uff0c\u9075\u5faa\u8207\u591a\u8aaa\u8a71\u8005\u81ea\u52d5\u8a9e\u97f3\u8b58\u5225 (ASR)\u3001\u76ee\u6a19\u8aaa\u8a71\u8005 ASR \u548c\u57fa\u65bc\u7279\u5b9a\u8aaa\u8a71\u8005\u5c6c\u6027\u7684 ASR\uff08\u4f8b\u5982\u6027\u5225\u3001\u51fa\u73fe\u9806\u5e8f\u3001\u8a9e\u8a00\u548c\u95dc\u9375\u5b57\uff09\u76f8\u95dc\u7684\u591a\u529f\u80fd\u8aaa\u660e\u767c\u8a00\u3002\u6211\u5011\u7684\u505a\u6cd5\u5229\u7528 WavLM \u548c Whisper \u7de8\u78bc\u5668\u4f86\u63d0\u53d6\u5c0d\u8aaa\u8a71\u8005\u7279\u5fb5\u548c\u8a9e\u7fa9\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u591a\u65b9\u9762\u8a9e\u97f3\u8868\u793a\u3002\u7136\u5f8c\u5c07\u9019\u4e9b\u8868\u793a\u8f38\u5165\u5230\u4f7f\u7528 LoRA \u5fae\u8abf\u7684 LLM \u4e2d\uff0c\u5f9e\u800c\u5be6\u73fe\u8a9e\u97f3\u7406\u89e3\u548c\u8f49\u9304\u529f\u80fd\u3002\u5168\u9762\u7684\u5be6\u9a57\u63ed\u793a\u4e86\u6211\u5011\u63d0\u51fa\u7684\u7cfb\u7d71 MT-LLM \u5728\u96de\u5c3e\u9152\u6703\u5834\u666f\u4e2d\u7684\u51fa\u8272\u6027\u80fd\uff0c\u7a81\u51fa\u4e86 LLM \u5728\u6b64\u985e\u8907\u96dc\u8a2d\u7f6e\u4e2d\u6839\u64da\u7528\u6236\u8aaa\u660e\u8655\u7406\u8207\u8a9e\u97f3\u76f8\u95dc\u4efb\u52d9\u7684\u6f5b\u529b\u3002", "author": "Lingwei Meng et.al.", "authors": "Lingwei Meng, Shujie Hu, Jiawen Kang, Zhaoqing Li, Yuejiao Wang, Wenxuan Wu, Xixin Wu, Xunying Liu, Helen Meng", "id": "2409.08596v1", "paper_url": "http://arxiv.org/abs/2409.08596v1", "repo": "null"}}