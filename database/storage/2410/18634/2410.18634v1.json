{"2410.18634": {"publish_time": "2024-10-24", "title": "Little Giants: Synthesizing High-Quality Embedding Data at Scale", "paper_summary": "Synthetic data generation has become an increasingly popular way of training\nmodels without the need for large, manually labeled datasets. For tasks like\ntext embedding, synthetic data offers diverse and scalable training examples,\nsignificantly reducing the cost of human annotation. However, most current\napproaches rely heavily on proprietary models like GPT-4, which are expensive\nand inefficient for generating large-scale embedding data. In this paper, we\nintroduce SPEED, a framework that aligns open-source small models (8B) to\nefficiently generate large-scale synthetic embedding data. Through supervised\nfine-tuning, preference optimization, and self-improvement, SPEED enables small\nopen-source models to produce high-quality data. Remarkably, SPEED uses only\nless than 1/10 of the GPT API calls, outperforming the state-of-the-art\nembedding model E5_mistral when both are trained solely on their synthetic\ndata. Using this efficient generator, we conduct a comprehensive study on how\nvarious factors within the alignment pipeline impact data quality and reveal\nthe scaling law for synthetic embedding data.", "paper_summary_zh": "\u5408\u6210\u8cc7\u6599\u751f\u6210\u5df2\u6210\u70ba\u4e00\u7a2e\u65e5\u76ca\u6d41\u884c\u7684\u8a13\u7df4\u6a21\u578b\u65b9\u5f0f\uff0c\u7121\u9700\u5927\u91cf\u624b\u52d5\u6a19\u8a18\u7684\u8cc7\u6599\u96c6\u3002\u5c0d\u65bc\u6587\u672c\u5d4c\u5165\u7b49\u4efb\u52d9\uff0c\u5408\u6210\u8cc7\u6599\u63d0\u4f9b\u4e86\u591a\u6a23\u4e14\u53ef\u64f4\u5c55\u7684\u8a13\u7df4\u7bc4\u4f8b\uff0c\u986f\u8457\u964d\u4f4e\u4e86\u4eba\u5de5\u6a19\u8a3b\u7684\u6210\u672c\u3002\u7136\u800c\uff0c\u76ee\u524d\u5927\u591a\u6578\u65b9\u6cd5\u90fd\u56b4\u91cd\u4f9d\u8cf4\u65bc\u5c08\u6709\u6a21\u578b\uff0c\u4f8b\u5982 GPT-4\uff0c\u800c\u9019\u4e9b\u6a21\u578b\u5728\u751f\u6210\u5927\u898f\u6a21\u5d4c\u5165\u8cc7\u6599\u65b9\u9762\u65e2\u6602\u8cb4\u53c8\u4f4e\u6548\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u4ecb\u7d39\u4e86 SPEED\uff0c\u4e00\u500b\u5c07\u958b\u6e90\u5c0f\u578b\u6a21\u578b\uff088B\uff09\u5c0d\u9f4a\u4ee5\u6709\u6548\u751f\u6210\u5927\u898f\u6a21\u5408\u6210\u5d4c\u5165\u8cc7\u6599\u7684\u6846\u67b6\u3002\u901a\u904e\u76e3\u7763\u5fae\u8abf\u3001\u504f\u597d\u6700\u4f73\u5316\u548c\u81ea\u6211\u6539\u9032\uff0cSPEED \u4f7f\u5c0f\u578b\u958b\u6e90\u6a21\u578b\u80fd\u5920\u7522\u751f\u9ad8\u54c1\u8cea\u8cc7\u6599\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSPEED \u53ea\u4f7f\u7528\u4e86\u4e0d\u5230\u5341\u5206\u4e4b\u4e00\u7684 GPT API \u547c\u53eb\uff0c\u5728\u50c5\u4f7f\u7528\u5176\u5408\u6210\u8cc7\u6599\u9032\u884c\u8a13\u7df4\u6642\uff0c\u5176\u8868\u73fe\u512a\u65bc\u6700\u5148\u9032\u7684\u5d4c\u5165\u6a21\u578b E5_mistral\u3002\u4f7f\u7528\u9019\u500b\u9ad8\u6548\u7684\u751f\u6210\u5668\uff0c\u6211\u5011\u5c0d\u5c0d\u9f4a\u7ba1\u9053\u4e2d\u7684\u5404\u7a2e\u56e0\u7d20\u5982\u4f55\u5f71\u97ff\u8cc7\u6599\u54c1\u8cea\u9032\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u4e26\u63ed\u793a\u4e86\u5408\u6210\u5d4c\u5165\u8cc7\u6599\u7684\u64f4\u5145\u5b9a\u5f8b\u3002", "author": "Haonan Chen et.al.", "authors": "Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou", "id": "2410.18634v1", "paper_url": "http://arxiv.org/abs/2410.18634v1", "repo": "null"}}