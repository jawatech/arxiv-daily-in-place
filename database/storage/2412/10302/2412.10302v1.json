{"2412.10302": {"publish_time": "2024-12-13", "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding", "paper_summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.", "paper_summary_zh": "\u6211\u5011\u63d0\u51fa\u4e86 DeepSeek-VL2\uff0c\u9019\u662f\u4e00\u500b\u5148\u9032\u7684\u5927\u578b\u6df7\u5408\u5c08\u5bb6 (MoE) \u7cfb\u5217\u8996\u89ba\u8a9e\u8a00\u6a21\u578b\uff0c\u900f\u904e\u5169\u9805\u4e3b\u8981\u5347\u7d1a\uff0c\u5927\u5e45\u6539\u9032\u5176\u524d\u8eab DeepSeek-VL\u3002\u5c0d\u65bc\u8996\u89ba\u5143\u4ef6\uff0c\u6211\u5011\u7d50\u5408\u52d5\u614b\u62fc\u8cbc\u8996\u89ba\u7de8\u78bc\u7b56\u7565\uff0c\u65e8\u5728\u8655\u7406\u5177\u6709\u4e0d\u540c\u9577\u5bec\u6bd4\u7684\u9ad8\u89e3\u6790\u5ea6\u5f71\u50cf\u3002\u5c0d\u65bc\u8a9e\u8a00\u5143\u4ef6\uff0c\u6211\u5011\u5229\u7528\u5177\u5099\u591a\u982d\u6f5b\u5728\u6ce8\u610f\u529b\u6a5f\u5236\u7684 DeepSeekMoE \u6a21\u578b\uff0c\u5c07\u9375\u503c\u5feb\u53d6\u58d3\u7e2e\u6210\u6f5b\u5728\u5411\u91cf\uff0c\u4ee5\u5be6\u73fe\u6709\u6548\u7387\u7684\u63a8\u8ad6\u548c\u9ad8\u901a\u91cf\u3002\u5728\u6539\u826f\u7684\u8996\u89ba\u8a9e\u8a00\u8cc7\u6599\u96c6\u4e0a\u9032\u884c\u8a13\u7df4\uff0cDeepSeek-VL2 \u5728\u5404\u7a2e\u4efb\u52d9\u4e2d\u5c55\u73fe\u51fa\u512a\u7570\u7684\u80fd\u529b\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u65bc\u8996\u89ba\u554f\u984c\u89e3\u7b54\u3001\u5149\u5b78\u5b57\u5143\u8fa8\u8b58\u3001\u6587\u4ef6/\u8868\u683c/\u5716\u8868\u7406\u89e3\u548c\u8996\u89ba\u57fa\u790e\u3002\u6211\u5011\u7684\u6a21\u578b\u7cfb\u5217\u7531\u4e09\u7a2e\u8b8a\u9ad4\u7d44\u6210\uff1aDeepSeek-VL2-Tiny\u3001DeepSeek-VL2-Small \u548c DeepSeek-VL2\uff0c\u5206\u5225\u5177\u6709 1.0B\u30012.8B \u548c 4.5B \u500b\u5df2\u555f\u7528\u7684\u53c3\u6578\u3002\u8207\u73fe\u6709\u7684\u958b\u6e90\u5bc6\u96c6\u548c\u57fa\u65bc MoE \u7684\u6a21\u578b\u76f8\u6bd4\uff0cDeepSeek-VL2 \u4ee5\u76f8\u540c\u6216\u66f4\u5c11\u7684\u5df2\u555f\u7528\u53c3\u6578\uff0c\u53d6\u5f97\u5177\u7af6\u722d\u529b\u6216\u6700\u5148\u9032\u7684\u6548\u80fd\u3002\u7a0b\u5f0f\u78bc\u548c\u9810\u8a13\u7df4\u6a21\u578b\u53ef\u5728 https://github.com/deepseek-ai/DeepSeek-VL2 \u516c\u958b\u53d6\u5f97\u3002", "author": "Zhiyu Wu et.al.", "authors": "Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, Chong Ruan", "id": "2412.10302v1", "paper_url": "http://arxiv.org/abs/2412.10302v1", "repo": "https://github.com/deepseek-ai/deepseek-vl2"}}