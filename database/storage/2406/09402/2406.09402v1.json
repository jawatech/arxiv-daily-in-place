{"2406.09402": {"publish_time": "2024-06-13", "title": "Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion", "paper_summary": "This paper proposes Instruct 4D-to-4D that achieves 4D awareness and\nspatial-temporal consistency for 2D diffusion models to generate high-quality\ninstruction-guided dynamic scene editing results. Traditional applications of\n2D diffusion models in dynamic scene editing often result in inconsistency,\nprimarily due to their inherent frame-by-frame editing methodology. Addressing\nthe complexities of extending instruction-guided editing to 4D, our key insight\nis to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems:\nachieving temporal consistency in video editing and applying these edits to the\npseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P)\nmodel with an anchor-aware attention module for batch processing and consistent\nediting. Additionally, we integrate optical flow-guided appearance propagation\nin a sliding window fashion for more precise frame-to-frame editing and\nincorporate depth-based projection to manage the extensive data of pseudo-3D\nscenes, followed by iterative editing to achieve convergence. We extensively\nevaluate our approach in various scenes and editing instructions, and\ndemonstrate that it achieves spatially and temporally consistent editing\nresults, with significantly enhanced detail and sharpness over the prior art.\nNotably, Instruct 4D-to-4D is general and applicable to both monocular and\nchallenging multi-camera scenes. Code and more results are available at\nimmortalco.github.io/Instruct-4D-to-4D.", "paper_summary_zh": "\u672c\u8ad6\u6587\u63d0\u51fa Instruct 4D-to-4D\uff0c\u5b83\u70ba 2D \u64f4\u6563\u6a21\u578b\u5be6\u73fe\u4e86 4D \u610f\u8b58\u548c\u6642\u7a7a\u4e00\u81f4\u6027\uff0c\u4ee5\u7522\u751f\u9ad8\u54c1\u8cea\u7684\u6307\u793a\u5f15\u5c0e\u52d5\u614b\u5834\u666f\u7de8\u8f2f\u7d50\u679c\u30022D \u64f4\u6563\u6a21\u578b\u5728\u52d5\u614b\u5834\u666f\u7de8\u8f2f\u4e2d\u7684\u50b3\u7d71\u61c9\u7528\u901a\u5e38\u6703\u5c0e\u81f4\u4e0d\u4e00\u81f4\uff0c\u4e3b\u8981\u662f\u7531\u65bc\u5b83\u5011\u56fa\u6709\u7684\u9010\u5e40\u7de8\u8f2f\u65b9\u6cd5\u3002\u70ba\u4e86\u89e3\u6c7a\u5c07\u6307\u793a\u5f15\u5c0e\u7de8\u8f2f\u64f4\u5c55\u5230 4D \u7684\u8907\u96dc\u6027\uff0c\u6211\u5011\u7684\u95dc\u9375\u898b\u89e3\u662f\u5c07 4D \u5834\u666f\u8996\u70ba\u507d 3D \u5834\u666f\uff0c\u5206\u89e3\u70ba\u5169\u500b\u5b50\u554f\u984c\uff1a\u5728\u5f71\u7247\u7de8\u8f2f\u4e2d\u5be6\u73fe\u6642\u9593\u4e00\u81f4\u6027\uff0c\u4e26\u5c07\u9019\u4e9b\u7de8\u8f2f\u61c9\u7528\u65bc\u507d 3D \u5834\u666f\u3002\u7dca\u63a5\u8457\uff0c\u6211\u5011\u9996\u5148\u4f7f\u7528\u9328\u9ede\u611f\u77e5\u6ce8\u610f\u529b\u6a21\u7d44\u589e\u5f37 Instruct-Pix2Pix (IP2P) \u6a21\u578b\uff0c\u4ee5\u9032\u884c\u6279\u6b21\u8655\u7406\u548c\u4e00\u81f4\u7de8\u8f2f\u3002\u6b64\u5916\uff0c\u6211\u5011\u4ee5\u6ed1\u52d5\u8996\u7a97\u7684\u65b9\u5f0f\u6574\u5408\u5149\u6d41\u5f15\u5c0e\u7684\u5916\u89c0\u50b3\u64ad\uff0c\u4ee5\u9032\u884c\u66f4\u7cbe\u78ba\u7684\u9010\u5e40\u7de8\u8f2f\uff0c\u4e26\u7d50\u5408\u57fa\u65bc\u6df1\u5ea6\u6295\u5f71\u4f86\u7ba1\u7406\u507d 3D \u5834\u666f\u7684\u5ee3\u6cdb\u8cc7\u6599\uff0c\u63a5\u8457\u9032\u884c\u53cd\u8986\u7de8\u8f2f\u4ee5\u5be6\u73fe\u6536\u6582\u3002\u6211\u5011\u5ee3\u6cdb\u8a55\u4f30\u4e86\u6211\u5011\u7684\u65b9\u6cd5\u5728\u5404\u7a2e\u5834\u666f\u548c\u7de8\u8f2f\u6307\u793a\u4e2d\u7684\u8868\u73fe\uff0c\u4e26\u8b49\u660e\u5b83\u5be6\u73fe\u4e86\u6642\u7a7a\u4e00\u81f4\u7684\u7de8\u8f2f\u7d50\u679c\uff0c\u8207\u5148\u524d\u7684\u6280\u8853\u76f8\u6bd4\uff0c\u7d30\u7bc0\u548c\u6e05\u6670\u5ea6\u986f\u8457\u63d0\u5347\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cInstruct 4D-to-4D \u5177\u6709\u901a\u7528\u6027\uff0c\u9069\u7528\u65bc\u55ae\u773c\u548c\u5177\u6709\u6311\u6230\u6027\u7684\u591a\u93e1\u982d\u5834\u666f\u3002\u7a0b\u5f0f\u78bc\u548c\u66f4\u591a\u7d50\u679c\u53ef\u5728 immortalco.github.io/Instruct-4D-to-4D \u53d6\u5f97\u3002", "author": "Linzhan Mou et.al.", "authors": "Linzhan Mou, Jun-Kun Chen, Yu-Xiong Wang", "id": "2406.09402v1", "paper_url": "http://arxiv.org/abs/2406.09402v1", "repo": "null"}}