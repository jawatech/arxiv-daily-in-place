{"2504.15280": {"publish_time": "2025-04-21", "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs", "paper_summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.", "paper_summary_zh": "\u591a\u8996\u89d2\u7406\u89e3\uff0c\u5373\u6574\u5408\u4e0d\u540c\u8996\u89d2\u7684\u8996\u89ba\u4fe1\u606f\u4ee5\u6709\u6548\u5c0e\u822a\u3001\u64cd\u4f5c\u548c\u7406\u89e3 3D \u5834\u666f\u7684\u80fd\u529b\uff0c\u662f\u591a\u6a21\u614b\u5927\u578b\u8a9e\u8a00\u6a21\u578b (MLLM) \u4f5c\u70ba\u5177\u8eab\u4ee3\u7406\u7684\u4e00\u9805\u57fa\u672c\u6311\u6230\u3002\u96d6\u7136\u6700\u8fd1\u7684 MLLM \u5728\u9ad8\u968e\u63a8\u7406\u548c\u898f\u5283\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u9032\u5c55\uff0c\u4f46\u5728\u9762\u5c0d\u591a\u8996\u89d2\u5e7e\u4f55\u4e00\u81f4\u6027\u548c\u8de8\u8996\u89d2\u5c0d\u61c9\u95dc\u4fc2\u6642\uff0c\u5b83\u5011\u5e38\u5e38\u8868\u73fe\u4e0d\u4f73\u3002\u70ba\u4e86\u5168\u9762\u8a55\u4f30 MLLM \u5728\u591a\u8996\u89d2\u5834\u666f\u63a8\u7406\u4e2d\u7684\u6311\u6230\uff0c\u6211\u5011\u63d0\u51fa\u4e86 All-Angles Bench\uff0c\u9019\u662f\u4e00\u500b\u5305\u542b 90 \u500b\u4e0d\u540c\u771f\u5be6\u4e16\u754c\u5834\u666f\u3001\u8d85\u904e 2,100 \u5c0d\u7d93\u4eba\u5de5\u4ed4\u7d30\u6a19\u8a3b\u7684\u591a\u8996\u89d2\u554f\u7b54\u7684\u57fa\u6e96\u6e2c\u8a66\u3002\u6211\u5011\u7684\u516d\u9805\u4efb\u52d9\uff08\u8a08\u6578\u3001\u5c6c\u6027\u8b58\u5225\u3001\u76f8\u5c0d\u8ddd\u96e2\u3001\u76f8\u5c0d\u65b9\u5411\u3001\u7269\u9ad4\u64cd\u4f5c\u548c\u76f8\u6a5f\u59ff\u614b\u4f30\u8a08\uff09\u5c08\u9580\u6e2c\u8a66\u6a21\u578b\u7684\u5e7e\u4f55\u5c0d\u61c9\u95dc\u4fc2\u4ee5\u53ca\u8de8\u8996\u89d2\u4e00\u81f4\u6027\u5c0d\u9f4a\u4fe1\u606f\u7684\u80fd\u529b\u3002\u6211\u5011\u5c0d\u5305\u62ec Gemini-2.0-Flash\u3001Claude-3.7-Sonnet \u548c GPT-4o \u5728\u5167\u7684 27 \u500b\u4ee3\u8868\u6027 MLLM \u9032\u884c\u4e86\u5ee3\u6cdb\u7684\u5be6\u9a57\uff0c\u4e26\u8207\u4eba\u985e\u8a55\u4f30\u8005\u9032\u884c\u4e86\u57fa\u6e96\u6e2c\u8a66\uff0c\u7d50\u679c\u986f\u793a\u51fa\u5de8\u5927\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u9019\u8868\u660e\u76ee\u524d\u7684 MLLM \u8ddd\u96e2\u4eba\u985e\u6c34\u5e73\u7684\u719f\u7df4\u7a0b\u5ea6\u9084\u5f88\u9060\u3002\u901a\u904e\u6df1\u5165\u5206\u6790\uff0c\u6211\u5011\u767c\u73fe MLLM \u5728\u5169\u500b\u65b9\u9762\u8868\u73fe\u5c24\u5176\u4e0d\u4f73\uff1a(1) \u90e8\u5206\u906e\u64cb\u8996\u5716\u7684\u8de8\u8996\u89d2\u5c0d\u61c9\u95dc\u4fc2\uff1b(2) \u5efa\u7acb\u7c97\u7565\u7684\u76f8\u6a5f\u59ff\u614b\u3002\u9019\u4e9b\u767c\u73fe\u51f8\u986f\u4e86\u7279\u5b9a\u9818\u57df\u6539\u9032\u6216\u6a21\u7d44\u7684\u5fc5\u8981\u6027\uff0c\u9019\u4e9b\u6539\u9032\u6216\u6a21\u7d44\u5d4c\u5165\u4e86\u66f4\u5f37\u7684\u591a\u8996\u89d2\u611f\u77e5\u80fd\u529b\u3002\u6211\u5011\u76f8\u4fe1\uff0c\u6211\u5011\u7684 All-Angles Bench \u63d0\u4f9b\u4e86\u5bf6\u8cb4\u7684\u898b\u89e3\uff0c\u6709\u52a9\u65bc\u7e2e\u5c0f MLLM \u8207\u4eba\u985e\u6c34\u5e73\u7684\u591a\u8996\u89d2\u7406\u89e3\u4e4b\u9593\u7684\u5dee\u8ddd\u3002\u8a72\u5c08\u6848\u548c\u57fa\u6e96\u6e2c\u8a66\u5df2\u516c\u958b\u767c\u5e03\u65bc https://danielchyeh.github.io/All-Angles-Bench/\u3002\n", "author": "Chun-Hsiao Yeh et.al.", "authors": "Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Rouyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, Yi Ma", "id": "2504.15280v1", "paper_url": "http://arxiv.org/abs/2504.15280v1", "repo": "null"}}