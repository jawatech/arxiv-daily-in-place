{"2410.01686": {"publish_time": "2024-10-02", "title": "Positional Attention: Out-of-Distribution Generalization and Expressivity for Neural Algorithmic Reasoning", "paper_summary": "There has been a growing interest in the ability of neural networks to solve\nalgorithmic tasks, such as arithmetic, summary statistics, and sorting. While\nstate-of-the-art models like Transformers have demonstrated good generalization\nperformance on in-distribution tasks, their out-of-distribution (OOD)\nperformance is poor when trained end-to-end. In this paper, we focus on value\ngeneralization, a common instance of OOD generalization where the test\ndistribution has the same input sequence length as the training distribution,\nbut the value ranges in the training and test distributions do not necessarily\noverlap. To address this issue, we propose that using fixed positional\nencodings to determine attention weights-referred to as positional\nattention-enhances empirical OOD performance while maintaining expressivity. We\nsupport our claim about expressivity by proving that Transformers with\npositional attention can effectively simulate parallel algorithms.", "paper_summary_zh": "\u5c0d\u65bc\u795e\u7d93\u7db2\u8def\u89e3\u6c7a\u6f14\u7b97\u6cd5\u4efb\u52d9\uff0c\u4f8b\u5982\u7b97\u8853\u3001\u6458\u8981\u7d71\u8a08\u548c\u6392\u5e8f\u7684\u80fd\u529b\uff0c\u4eba\u5011\u8d8a\u4f86\u8d8a\u611f\u8208\u8da3\u3002\u96d6\u7136\u50cf Transformer \u9019\u6a23\u7684\u6700\u5148\u9032\u6a21\u578b\u5df2\u5728\u5206\u4f48\u5167\u4efb\u52d9\u4e0a\u5c55\u793a\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6548\u80fd\uff0c\u4f46\u7576\u7aef\u5c0d\u7aef\u8a13\u7df4\u6642\uff0c\u5b83\u5011\u7684\u5206\u4f48\u5916 (OOD) \u6548\u80fd\u5f88\u5dee\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u503c\u6cdb\u5316\uff0c\u9019\u662f OOD \u6cdb\u5316\u7684\u5e38\u898b\u7bc4\u4f8b\uff0c\u5176\u4e2d\u6e2c\u8a66\u5206\u4f48\u5177\u6709\u8207\u8a13\u7df4\u5206\u4f48\u76f8\u540c\u7684\u8f38\u5165\u5e8f\u5217\u9577\u5ea6\uff0c\u4f46\u8a13\u7df4\u548c\u6e2c\u8a66\u5206\u4f48\u4e2d\u7684\u503c\u7bc4\u570d\u4e0d\u4e00\u5b9a\u91cd\u758a\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u56fa\u5b9a\u7684\u4f4d\u7f6e\u7de8\u78bc\u4f86\u78ba\u5b9a\u6ce8\u610f\u529b\u6b0a\u91cd\uff0c\u7a31\u70ba\u4f4d\u7f6e\u6ce8\u610f\u529b\uff0c\u5b83\u53ef\u4ee5\u5728\u4fdd\u6301\u8868\u9054\u529b\u7684\u540c\u6642\u589e\u5f37\u7d93\u9a57\u6027\u7684 OOD \u6548\u80fd\u3002\u6211\u5011\u900f\u904e\u8b49\u660e\u5177\u6709\u4f4d\u7f6e\u6ce8\u610f\u529b\u7684 Transformer \u53ef\u4ee5\u6709\u6548\u6a21\u64ec\u5e73\u884c\u6f14\u7b97\u6cd5\uff0c\u4f86\u652f\u6301\u6211\u5011\u5c0d\u8868\u9054\u529b\u7684\u4e3b\u5f35\u3002", "author": "Artur Back de Luca et.al.", "authors": "Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veli\u010dkovi\u0107, Kimon Fountoulakis", "id": "2410.01686v1", "paper_url": "http://arxiv.org/abs/2410.01686v1", "repo": "https://github.com/opallab/positional_attention"}}