{"2406.15259": {"publish_time": "2024-06-21", "title": "V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and Suggestions", "paper_summary": "NL2VIS (natural language to visualization) is a promising and recent research\narea that involves interpreting natural language queries and translating them\ninto visualizations that accurately represent the underlying data. As we\nnavigate the era of big data, NL2VIS holds considerable application potential\nsince it greatly facilitates data exploration by non-expert users. Following\nthe increasingly widespread usage of generative AI in NL2VIS applications, in\nthis paper we present V-RECS, the first LLM-based Visual Recommender augmented\nwith explanations(E), captioning(C), and suggestions(S) for further data\nexploration. V-RECS' visualization narratives facilitate both response\nverification and data exploration by non-expert users. Furthermore, our\nproposed solution mitigates computational, controllability, and cost issues\nassociated with using powerful LLMs by leveraging a methodology to effectively\nfine-tune small models. To generate insightful visualization narratives, we use\nChain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify\nand generate the logical steps to produce a correct answer. Since CoT is\nreported to perform poorly with small LLMs, we adopted a strategy in which a\nlarge LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to\nfine-tune a small model, Llama-2-7B, which plays the role of a Student.\nExtensive experiments-based on a framework for the quantitative evaluation of\nAI-based visualizations and on manual assessment by a group of\nparticipants-show that V-RECS achieves performance scores comparable to GPT-4,\nat a much lower cost. The efficacy of the V-RECS teacher-student paradigm is\nalso demonstrated by the fact that the un-tuned Llama fails to perform the task\nin the vast majority of test cases. We release V-RECS for the visualization\ncommunity to assist visualization designers throughout the entire visualization\ngeneration process.", "paper_summary_zh": "<paragraph>NL2VIS\uff08\u81ea\u7136\u8a9e\u8a00\u8f49\u8996\u89ba\u5316\uff09\u662f\u4e00\u500b\u6709\u524d\u9014\u4e14\u65b0\u8208\u7684\u7814\u7a76\u9818\u57df\uff0c\u6d89\u53ca\u89e3\u8b80\u81ea\u7136\u8a9e\u8a00\u67e5\u8a62\uff0c\u4e26\u5c07\u5176\u8f49\u63db\u6210\u7cbe\u78ba\u5448\u73fe\u5e95\u5c64\u8cc7\u6599\u7684\u8996\u89ba\u5316\u3002\u96a8\u8457\u6211\u5011\u9032\u5165\u5927\u6578\u64da\u6642\u4ee3\uff0cNL2VIS \u64c1\u6709\u76f8\u7576\u5927\u7684\u61c9\u7528\u6f5b\u529b\uff0c\u56e0\u70ba\u5b83\u6975\u5927\u5730\u4fc3\u9032\u4e86\u975e\u5c08\u5bb6\u4f7f\u7528\u8005\u63a2\u7d22\u8cc7\u6599\u3002\u96a8\u8457\u751f\u6210\u5f0f AI \u5728 NL2VIS \u61c9\u7528\u4e2d\u8d8a\u4f86\u8d8a\u5ee3\u6cdb\u5730\u4f7f\u7528\uff0c\u6211\u5011\u5728\u9019\u7bc7\u8ad6\u6587\u4e2d\u63d0\u51fa\u4e86 V-RECS\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u57fa\u65bc LLM \u7684\u8996\u89ba\u63a8\u85a6\u7cfb\u7d71\uff0c\u4e26\u64f4\u5145\u4e86\u89e3\u91cb (E)\u3001\u6a19\u984c (C) \u548c\u5efa\u8b70 (S) \u4ee5\u9032\u4e00\u6b65\u63a2\u7d22\u8cc7\u6599\u3002V-RECS \u7684\u8996\u89ba\u5316\u6558\u8ff0\u6709\u52a9\u65bc\u975e\u5c08\u5bb6\u4f7f\u7528\u8005\u9a57\u8b49\u56de\u61c9\u548c\u63a2\u7d22\u8cc7\u6599\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u7684\u89e3\u6c7a\u65b9\u6848\u900f\u904e\u63a1\u7528\u4e00\u7a2e\u65b9\u6cd5\u4f86\u6709\u6548\u5fae\u8abf\u5c0f\u578b\u6a21\u578b\uff0c\u5f9e\u800c\u6e1b\u8f15\u8207\u4f7f\u7528\u5f37\u5927 LLM \u76f8\u95dc\u7684\u904b\u7b97\u3001\u53ef\u63a7\u6027\u548c\u6210\u672c\u554f\u984c\u3002\u70ba\u4e86\u7522\u751f\u6709\u898b\u5730\u7684\u8996\u89ba\u5316\u6558\u8ff0\uff0c\u6211\u5011\u4f7f\u7528\u601d\u60f3\u93c8 (CoT)\uff0c\u9019\u662f\u4e00\u7a2e\u63d0\u793a\u5de5\u7a0b\u6280\u8853\uff0c\u53ef\u5e6b\u52a9 LLM \u8b58\u5225\u548c\u7522\u751f\u7522\u751f\u6b63\u78ba\u7b54\u6848\u7684\u908f\u8f2f\u6b65\u9a5f\u3002\u7531\u65bc\u64da\u5831\u5c0e\uff0cCoT \u5728\u5c0f\u578b LLM \u4e2d\u8868\u73fe\u4e0d\u4f73\uff0c\u56e0\u6b64\u6211\u5011\u63a1\u7528\u4e86\u4e00\u7a2e\u7b56\u7565\uff0c\u5176\u4e2d\u5927\u578b LLM (GPT-4) \u626e\u6f14\u8001\u5e2b\u7684\u89d2\u8272\uff0c\u751f\u6210\u57fa\u65bc CoT \u7684\u6307\u4ee4\u4f86\u5fae\u8abf\u5c0f\u578b\u6a21\u578b Llama-2-7B\uff0c\u5f8c\u8005\u626e\u6f14\u5b78\u751f\u7684\u89d2\u8272\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u57fa\u65bc\u4e00\u500b\u7528\u65bc\u8a55\u4f30\u57fa\u65bc AI \u7684\u8996\u89ba\u5316\u7684\u91cf\u5316\u6846\u67b6\uff0c\u4ee5\u53ca\u4e00\u7d44\u53c3\u8207\u8005\u7684\u624b\u52d5\u8a55\u4f30\uff0c\u8868\u660e V-RECS \u9054\u5230\u4e86\u8207 GPT-4 \u76f8\u7576\u7684\u6548\u80fd\u5206\u6578\uff0c\u4f46\u6210\u672c\u537b\u4f4e\u5f97\u591a\u3002V-RECS \u5e2b\u751f\u7bc4\u5f0f\u7684\u6709\u6548\u6027\u4e5f\u7531\u4ee5\u4e0b\u4e8b\u5be6\u8b49\u660e\uff1a\u672a\u8abf\u6574\u7684 Llama \u5728\u7d55\u5927\u591a\u6578\u6e2c\u8a66\u6848\u4f8b\u4e2d\u7121\u6cd5\u57f7\u884c\u4efb\u52d9\u3002\u6211\u5011\u91cb\u51fa V-RECS \u7d66\u8996\u89ba\u5316\u793e\u7fa4\uff0c\u4ee5\u5354\u52a9\u8996\u89ba\u5316\u8a2d\u8a08\u5e2b\u5b8c\u6210\u6574\u500b\u8996\u89ba\u5316\u751f\u6210\u904e\u7a0b\u3002</paragraph>", "author": "Luca Podo et.al.", "authors": "Luca Podo, Marco Angelini, Paola Velardi", "id": "2406.15259v1", "paper_url": "http://arxiv.org/abs/2406.15259v1", "repo": "null"}}