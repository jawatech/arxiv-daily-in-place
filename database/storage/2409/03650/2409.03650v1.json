{"2409.03650": {"publish_time": "2024-09-05", "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization", "paper_summary": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.", "paper_summary_zh": "\u4eba\u985e\u56de\u994b\u5f37\u5316\u5b78\u7fd2 (RLHF) \u662f\u4e00\u7a2e\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c07\u8a9e\u8a00\u6a21\u578b\u8abf\u6574\u5230\u4eba\u985e\u504f\u597d\u3002RLHF \u7684\u6838\u5fc3\u662f\u5b78\u7fd2\u4e00\u500b\u734e\u52f5\u51fd\u6578\u4f86\u8a55\u5206\u4eba\u985e\u504f\u597d\u3002\u5b78\u7fd2\u734e\u52f5\u6a21\u578b\u7684\u5169\u7a2e\u4e3b\u8981\u65b9\u6cd5\u70ba 1) \u8a13\u7df4\u4e00\u500b\u660e\u78ba\u734e\u52f5\u6a21\u578b (EXRM)\uff0c\u5982\u540c RLHF \u4e2d\uff0c2) \u4f7f\u7528\u5f9e\u504f\u597d\u6578\u64da\u4e2d\u5b78\u7fd2\u5230\u7684\u96b1\u542b\u734e\u52f5\uff0c\u900f\u904e\u76f4\u63a5\u504f\u597d\u6700\u4f73\u5316 (DPO) \u7b49\u65b9\u6cd5\u3002\u5148\u524d\u7684\u7814\u7a76\u986f\u793a\uff0cDPO \u7684\u96b1\u542b\u734e\u52f5\u6a21\u578b (\u8868\u793a\u70ba DPORM) \u53ef\u5728\u6975\u9650\u4e2d\u8fd1\u4f3c EXRM\u3002DPORM \u7684\u6548\u80fd\u76f4\u63a5\u6697\u793a\u5b78\u7fd2\u5230\u7684\u653f\u7b56\u7684\u6700\u4f73\u6027\uff0c\u4e14\u5c0d LLM \u8abf\u6574\u65b9\u6cd5\uff08\u5305\u62ec\u53cd\u8986 DPO\uff09\u4e5f\u6709\u5be6\u969b\u7684\u610f\u7fa9\u3002\u7136\u800c\uff0c\u5c1a\u4e0d\u6e05\u695a DPORM \u5728\u7d93\u9a57\u4e0a\u6709\u591a\u7b26\u5408 EXRM \u7684\u6548\u80fd\u3002\u672c\u7814\u7a76\u63a2\u8a0e\u5340\u5206 DPORM \u548c EXRM \u7684\u504f\u597d\u548c\u62d2\u7d55\u7b54\u6848\u7684\u6e96\u78ba\u6027\u3002\u6211\u5011\u7684\u767c\u73fe\u986f\u793a\uff0c\u5118\u7ba1 DPORM \u7b26\u5408\u8a13\u7df4\u8cc7\u6599\u96c6\u7684\u7a0b\u5ea6\u76f8\u7576\uff0c\u4f46\u5176\u6cdb\u5316\u6548\u679c\u4e0d\u5982 EXRM\uff0c\u7279\u5225\u662f\u5728\u9a57\u8b49\u8cc7\u6599\u96c6\u5305\u542b\u5206\u914d\u8f49\u79fb\u6642\u3002\u5728\u4e94\u500b\u975e\u5206\u914d\u8a2d\u5b9a\u4e2d\uff0cDPORM \u7684\u6e96\u78ba\u6027\u5e73\u5747\u4e0b\u964d 3%\uff0c\u6700\u5927\u4e0b\u964d 7%\u3002\u9019\u4e9b\u767c\u73fe\u5f37\u8abf DPORM \u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e26\u8b49\u5be6\u4e86\u5728\u53cd\u8986 DPO \u65b9\u6cd5\u4e2d\u6574\u5408\u660e\u78ba\u734e\u52f5\u6a21\u578b\u3002", "author": "Yong Lin et.al.", "authors": "Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang", "id": "2409.03650v1", "paper_url": "http://arxiv.org/abs/2409.03650v1", "repo": "null"}}