{"2409.20048": {"publish_time": "2024-09-30", "title": "Depression detection in social media posts using transformer-based models and auxiliary features", "paper_summary": "The detection of depression in social media posts is crucial due to the\nincreasing prevalence of mental health issues. Traditional machine learning\nalgorithms often fail to capture intricate textual patterns, limiting their\neffectiveness in identifying depression. Existing studies have explored various\napproaches to this problem but often fall short in terms of accuracy and\nrobustness. To address these limitations, this research proposes a neural\nnetwork architecture leveraging transformer-based models combined with metadata\nand linguistic markers. The study employs DistilBERT, extracting information\nfrom the last four layers of the transformer, applying learned weights, and\naveraging them to create a rich representation of the input text. This\nrepresentation, augmented by metadata and linguistic markers, enhances the\nmodel's comprehension of each post. Dropout layers prevent overfitting, and a\nMultilayer Perceptron (MLP) is used for final classification. Data augmentation\ntechniques, inspired by the Easy Data Augmentation (EDA) methods, are also\nemployed to improve model performance. Using BERT, random insertion and\nsubstitution of phrases generate additional training data, focusing on\nbalancing the dataset by augmenting underrepresented classes. The proposed\nmodel achieves weighted Precision, Recall, and F1-scores of 84.26%, 84.18%, and\n84.15%, respectively. The augmentation techniques significantly enhance model\nperformance, increasing the weighted F1-score from 72.59% to 84.15%.", "paper_summary_zh": "\u7531\u65bc\u5fc3\u7406\u5065\u5eb7\u554f\u984c\u7684\u76db\u884c\uff0c\u5728\u793e\u7fa4\u5a92\u9ad4\u8cbc\u6587\u4e2d\u5075\u6e2c\u6182\u9b31\u75c7\u81f3\u95dc\u91cd\u8981\u3002\u50b3\u7d71\u6a5f\u5668\u5b78\u7fd2\u6f14\u7b97\u6cd5\u7d93\u5e38\u7121\u6cd5\u6355\u6349\u8907\u96dc\u7684\u6587\u5b57\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5b83\u5011\u5728\u8fa8\u8b58\u6182\u9b31\u75c7\u4e0a\u7684\u6548\u80fd\u3002\u73fe\u6709\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u5404\u7a2e\u89e3\u6c7a\u6b64\u554f\u984c\u7684\u65b9\u6cd5\uff0c\u4f46\u5f80\u5f80\u5728\u6e96\u78ba\u5ea6\u548c\u7a69\u5065\u6027\u65b9\u9762\u6709\u6240\u4e0d\u8db3\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u9650\u5236\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7a2e\u795e\u7d93\u7db2\u8def\u67b6\u69cb\uff0c\u5b83\u5229\u7528\u4e86\u57fa\u65bc\u8f49\u63db\u5668\u7684\u6a21\u578b\uff0c\u4e26\u7d50\u5408\u4e86\u5143\u8cc7\u6599\u548c\u8a9e\u8a00\u6a19\u8a18\u3002\u672c\u7814\u7a76\u63a1\u7528 DistilBERT\uff0c\u5f9e\u8f49\u63db\u5668\u7684\u6700\u5f8c\u56db\u5c64\u63d0\u53d6\u8cc7\u8a0a\uff0c\u5957\u7528\u5b78\u7fd2\u5230\u7684\u6b0a\u91cd\uff0c\u4e26\u5c07\u5b83\u5011\u5e73\u5747\u4ee5\u5efa\u7acb\u8f38\u5165\u6587\u5b57\u7684\u8c50\u5bcc\u8868\u5fb5\u3002\u6b64\u8868\u5fb5\u7531\u5143\u8cc7\u6599\u548c\u8a9e\u8a00\u6a19\u8a18\u589e\u5f37\uff0c\u589e\u5f37\u4e86\u6a21\u578b\u5c0d\u6bcf\u5247\u8cbc\u6587\u7684\u7406\u89e3\u3002\u4e2d\u65b7\u5c64\u9632\u6b62\u904e\u5ea6\u64ec\u5408\uff0c\u4e26\u4f7f\u7528\u591a\u5c64\u611f\u77e5\u5668 (MLP) \u9032\u884c\u6700\u7d42\u5206\u985e\u3002\u8cc7\u6599\u64f4\u5145\u6280\u8853\uff08\u53d7\u7c21\u6613\u8cc7\u6599\u64f4\u5145 (EDA) \u65b9\u6cd5\u555f\u767c\uff09\u4e5f\u7528\u65bc\u6539\u5584\u6a21\u578b\u6548\u80fd\u3002\u4f7f\u7528 BERT\uff0c\u96a8\u6a5f\u63d2\u5165\u548c\u66ff\u63db\u7247\u8a9e\u6703\u7522\u751f\u984d\u5916\u7684\u8a13\u7df4\u8cc7\u6599\uff0c\u91cd\u9ede\u5728\u900f\u904e\u64f4\u5145\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u985e\u5225\u4f86\u5e73\u8861\u8cc7\u6599\u96c6\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5206\u5225\u9054\u5230\u4e86\u52a0\u6b0a\u6e96\u78ba\u7387\u3001\u53ec\u56de\u7387\u548c F1 \u5206\u6578 84.26%\u300184.18% \u548c 84.15%\u3002\u64f4\u5145\u6280\u8853\u986f\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6548\u80fd\uff0c\u5c07\u52a0\u6b0a F1 \u5206\u6578\u5f9e 72.59% \u63d0\u5347\u81f3 84.15%\u3002", "author": "Marios Kerasiotis et.al.", "authors": "Marios Kerasiotis, Loukas Ilias, Dimitris Askounis", "id": "2409.20048v1", "paper_url": "http://arxiv.org/abs/2409.20048v1", "repo": "null"}}