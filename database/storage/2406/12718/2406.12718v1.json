{"2406.12718": {"publish_time": "2024-06-18", "title": "AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention", "paper_summary": "Despite their great success across various multimodal tasks, Large\nVision-Language Models (LVLMs) are facing a prevalent problem with object\nhallucinations, where the generated textual responses are inconsistent with\nground-truth objects in the given image. This paper investigates various LVLMs\nand pinpoints attention deficiency toward discriminative local image features\nas one root cause of object hallucinations. Specifically, LVLMs predominantly\nattend to prompt-independent global image features, while failing to capture\nprompt-relevant local features, consequently undermining the visual grounding\ncapacity of LVLMs and leading to hallucinations. To this end, we propose\nAssembly of Global and Local Attention (AGLA), a training-free and\nplug-and-play approach that mitigates object hallucinations by exploring an\nensemble of global features for response generation and local features for\nvisual discrimination simultaneously. Our approach exhibits an image-prompt\nmatching scheme that captures prompt-relevant local features from images,\nleading to an augmented view of the input image where prompt-relevant content\nis reserved while irrelevant distractions are masked. With the augmented view,\na calibrated decoding distribution can be derived by integrating generative\nglobal features from the original image and discriminative local features from\nthe augmented image. Extensive experiments show that AGLA consistently\nmitigates object hallucinations and enhances general perception capability for\nLVLMs across various discriminative and generative benchmarks. Our code will be\nreleased at https://github.com/Lackel/AGLA.", "paper_summary_zh": "\u5118\u7ba1\u5927\u578b\u8996\u89ba\u8a9e\u8a00\u6a21\u578b (LVLMs) \u5728\u5404\u7a2e\u591a\u6a21\u614b\u4efb\u52d9\u4e2d\u53d6\u5f97\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5b83\u5011\u6b63\u9762\u81e8\u4e00\u500b\u666e\u904d\u7684\u554f\u984c\uff0c\u5373\u7269\u4ef6\u5e7b\u89ba\uff0c\u5176\u4e2d\u7522\u751f\u7684\u6587\u5b57\u56de\u61c9\u8207\u7d66\u5b9a\u5f71\u50cf\u4e2d\u7684\u771f\u5be6\u7269\u4ef6\u4e0d\u4e00\u81f4\u3002\u672c\u6587\u63a2\u8a0e\u4e86\u5404\u7a2e LVLMs\uff0c\u4e26\u6307\u51fa\u5c0d\u5340\u8fa8\u6027\u5c40\u90e8\u5f71\u50cf\u7279\u5fb5\u7684\u6ce8\u610f\u529b\u4e0d\u8db3\u662f\u7269\u4ef6\u5e7b\u89ba\u7684\u4e00\u500b\u6839\u672c\u539f\u56e0\u3002\u5177\u9ad4\u4f86\u8aaa\uff0cLVLMs \u4e3b\u8981\u95dc\u6ce8\u8207\u63d0\u793a\u7121\u95dc\u7684\u6574\u9ad4\u5f71\u50cf\u7279\u5fb5\uff0c\u800c\u7121\u6cd5\u64f7\u53d6\u8207\u63d0\u793a\u76f8\u95dc\u7684\u5c40\u90e8\u7279\u5fb5\uff0c\u56e0\u6b64\u640d\u5bb3\u4e86 LVLMs \u7684\u8996\u89ba\u57fa\u790e\u80fd\u529b\uff0c\u4e26\u5c0e\u81f4\u5e7b\u89ba\u3002\u70ba\u6b64\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u6574\u9ad4\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u7d44\u88dd (AGLA)\uff0c\u9019\u662f\u4e00\u7a2e\u514d\u8a13\u7df4\u4e14\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u900f\u904e\u540c\u6642\u63a2\u7d22\u6574\u9ad4\u7279\u5fb5\u4ee5\u7522\u751f\u56de\u61c9\u548c\u5c40\u90e8\u7279\u5fb5\u4ee5\u9032\u884c\u8996\u89ba\u5340\u8fa8\uff0c\u4f86\u6e1b\u8f15\u7269\u4ef6\u5e7b\u89ba\u3002\u6211\u5011\u7684\u505a\u6cd5\u5c55\u793a\u4e86\u4e00\u500b\u5f71\u50cf\u63d0\u793a\u6bd4\u5c0d\u6a5f\u5236\uff0c\u5f9e\u5f71\u50cf\u4e2d\u64f7\u53d6\u8207\u63d0\u793a\u76f8\u95dc\u7684\u5c40\u90e8\u7279\u5fb5\uff0c\u5f9e\u800c\u5c0e\u81f4\u8f38\u5165\u5f71\u50cf\u7684\u64f4\u589e\u6aa2\u8996\uff0c\u5176\u4e2d\u4fdd\u7559\u8207\u63d0\u793a\u76f8\u95dc\u7684\u5167\u5bb9\uff0c\u540c\u6642\u906e\u853d\u7121\u95dc\u7684\u5e72\u64fe\u3002\u900f\u904e\u64f4\u589e\u6aa2\u8996\uff0c\u53ef\u4ee5\u900f\u904e\u6574\u5408\u4f86\u81ea\u539f\u59cb\u5f71\u50cf\u7684\u751f\u6210\u6574\u9ad4\u7279\u5fb5\u548c\u4f86\u81ea\u64f4\u589e\u5f71\u50cf\u7684\u5340\u8fa8\u6027\u5c40\u90e8\u7279\u5fb5\uff0c\u4f86\u884d\u751f\u6821\u6e96\u7684\u89e3\u78bc\u5206\u4f48\u3002\u5ee3\u6cdb\u7684\u5be6\u9a57\u8868\u660e\uff0cAGLA \u6301\u7e8c\u6e1b\u8f15\u7269\u4ef6\u5e7b\u89ba\uff0c\u4e26\u589e\u5f37 LVLMs \u5728\u5404\u7a2e\u5340\u8fa8\u6027\u548c\u751f\u6210\u57fa\u6e96\u4e2d\u7684\u6574\u9ad4\u611f\u77e5\u80fd\u529b\u3002\u6211\u5011\u7684\u7a0b\u5f0f\u78bc\u5c07\u5728 https://github.com/Lackel/AGLA \u767c\u5e03\u3002", "author": "Wenbin An et.al.", "authors": "Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, QianYing Wang, Guang Dai, Ping Chen, Shijian Lu", "id": "2406.12718v1", "paper_url": "http://arxiv.org/abs/2406.12718v1", "repo": "null"}}