{"2411.11707": {"publish_time": "2024-11-18", "title": "FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models", "paper_summary": "By adapting Large Language Models (LLMs) to domain-specific tasks or\nenriching them with domain-specific knowledge, we can fully harness the\ncapabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous\nmutual enhancement between the server's LLM and the downstream clients' Small\nLanguage Models (SLMs). To address this, we propose FedCoLLM, a novel and\nparameter-efficient federated framework designed for co-tuning LLMs and SLMs.\nThis approach is aimed at adaptively transferring server-side LLMs knowledge to\nclients' SLMs while simultaneously enriching the LLMs with domain insights from\nthe clients. To accomplish this, FedCoLLM utilizes lightweight adapters in\nconjunction with SLMs, facilitating knowledge exchange between server and\nclients in a manner that respects data privacy while also minimizing\ncomputational and communication overhead. Our evaluation of FedCoLLM, utilizing\nvarious public LLMs and SLMs across a range of NLP text generation tasks,\nreveals that the performance of clients' SLMs experiences notable improvements\nwith the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM\nachieves comparable performance to that obtained through direct fine-tuning on\nclients' data.", "paper_summary_zh": "\u900f\u904e\u5c07\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u8abf\u6574\u5230\u7279\u5b9a\u9818\u57df\u4efb\u52d9\u6216\u8c50\u5bcc\u5b83\u5011\u7684\u7279\u5b9a\u9818\u57df\u77e5\u8b58\uff0c\u6211\u5011\u53ef\u4ee5\u5145\u5206\u5229\u7528 LLM \u7684\u529f\u80fd\u3002\u5118\u7ba1\u5982\u6b64\uff0c\u4f3a\u670d\u5668\u7684 LLM \u548c\u4e0b\u6e38\u5ba2\u6236\u7aef\u7684\u5c0f\u578b\u8a9e\u8a00\u6a21\u578b (SLM) \u4e4b\u9593\uff0c\u5728\u540c\u6642\u76f8\u4e92\u589e\u5f37\u65b9\u9762\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u500b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa\u4e86 FedCoLLM\uff0c\u4e00\u500b\u65b0\u7a4e\u4e14\u53c3\u6578\u6548\u7387\u9ad8\u7684\u806f\u5408\u6846\u67b6\uff0c\u5c08\u9580\u7528\u65bc\u5171\u540c\u8abf\u6574 LLM \u548c SLM\u3002\u6b64\u65b9\u6cd5\u65e8\u5728\u81ea\u9069\u61c9\u5730\u5c07\u4f3a\u670d\u5668\u7aef\u7684 LLM \u77e5\u8b58\u50b3\u8f38\u5230\u5ba2\u6236\u7aef\u7684 SLM\uff0c\u540c\u6642\u8c50\u5bcc LLM \u5c0d\u5ba2\u6236\u7aef\u9818\u57df\u898b\u89e3\u3002\u70ba\u4e86\u9054\u6210\u6b64\u76ee\u7684\uff0cFedCoLLM \u5229\u7528\u8f15\u91cf\u7d1a\u9069\u914d\u5668\u8207 SLM \u7d50\u5408\uff0c\u4fc3\u9032\u4f3a\u670d\u5668\u548c\u5ba2\u6236\u7aef\u4e4b\u9593\u7684\u77e5\u8b58\u4ea4\u6d41\uff0c\u540c\u6642\u5c0a\u91cd\u8cc7\u6599\u96b1\u79c1\uff0c\u4e26\u5c07\u904b\u7b97\u548c\u901a\u8a0a\u958b\u92b7\u964d\u5230\u6700\u4f4e\u3002\u6211\u5011\u5229\u7528\u5404\u7a2e\u516c\u5171 LLM \u548c SLM \u8a55\u4f30 FedCoLLM\uff0c\u6db5\u84cb\u4e00\u7cfb\u5217 NLP \u6587\u5b57\u751f\u6210\u4efb\u52d9\uff0c\u7d50\u679c\u986f\u793a\u5ba2\u6236\u7aef\u7684 SLM \u5728 LLM \u7684\u5354\u52a9\u4e0b\uff0c\u5176\u6548\u80fd\u6709\u986f\u8457\u63d0\u5347\u3002\u540c\u6642\uff0c\u900f\u904e FedCoLLM \u589e\u5f37\u7684 LLM\uff0c\u53ef\u9054\u6210\u8207\u5728\u5ba2\u6236\u7aef\u8cc7\u6599\u4e0a\u9032\u884c\u76f4\u63a5\u5fae\u8abf\u6240\u7372\u5f97\u7684\u6548\u80fd\u76f8\u7576\u3002", "author": "Tao Fan et.al.", "authors": "Tao Fan, Yan Kang, Guoqiang Ma, Lixin Fan, Kai Chen, Qiang Yang", "id": "2411.11707v1", "paper_url": "http://arxiv.org/abs/2411.11707v1", "repo": "null"}}