{"2501.01702": {"publish_time": "2025-01-03", "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning", "paper_summary": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research.", "paper_summary_zh": "<paragraph>\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u57fa\u790e\u4ee3\u7406\u5df2\u8b49\u660e\u5b83\u5011\u5177\u5099\u57f7\u884c\u4eba\u985e\u822c\u8907\u96dc\u4efb\u52d9\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u958b\u6e90 LLM \u8207 GPT \u7cfb\u5217\u7b49\u5546\u696d\u6a21\u578b\u4e4b\u9593\u4ecd\u5b58\u5728\u5f88\u5927\u5dee\u8ddd\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c08\u6ce8\u65bc\u900f\u904e\u6307\u4ee4\u8abf\u6574\u4f86\u6539\u5584 LLM \u7684\u4ee3\u7406\u6982\u5316\u80fd\u529b\u3002\u6211\u5011\u9996\u5148\u89c0\u5bdf\u5230\u73fe\u6709\u7684\u4ee3\u7406\u8a13\u7df4\u8a9e\u6599\u5eab\u5728\u4fdd\u7559\u8a55\u4f30\u96c6\u4e2d\u5c55\u73fe\u51fa\u4ee4\u4eba\u6eff\u610f\u7684\u7d50\u679c\uff0c\u4f46\u7121\u6cd5\u6982\u5316\u5230\u4fdd\u7559\u96c6\u4e2d\u3002\u9019\u4e9b\u4ee3\u7406\u8abf\u6574\u5de5\u4f5c\u9762\u81e8\u56b4\u91cd\u7684\u683c\u5f0f\u5316\u932f\u8aa4\uff0c\u800c\u4e14\u7d93\u5e38\u9577\u6642\u9593\u9677\u5165\u76f8\u540c\u7684\u932f\u8aa4\u4e2d\u3002\u6211\u5011\u5206\u6790\u51fa\u6982\u5316\u80fd\u529b\u5dee\u4f86\u81ea\u65bc\u904e\u5ea6\u64ec\u5408\u5230\u5e7e\u500b\u624b\u52d5\u4ee3\u7406\u74b0\u5883\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u5c0d\u65b0\u60c5\u6cc1\u7684\u9069\u61c9\u80fd\u529b\u3002\u5b83\u5011\u8207\u932f\u8aa4\u7684\u884c\u52d5\u6b65\u9a5f\u4f5c\u9b25\u722d\uff0c\u7121\u6cd5\u5f9e\u7d93\u9a57\u4e2d\u5b78\u7fd2\uff0c\u53ea\u80fd\u8a18\u4f4f\u73fe\u6709\u7684\u89c0\u5bdf\u884c\u52d5\u95dc\u4fc2\u3002\u53d7\u6b64\u898b\u89e3\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u7528\u65bc\u4ee3\u7406\u8abf\u6574\u7684\u65b0\u578b AgentRefine \u6846\u67b6\u3002\u6838\u5fc3\u601d\u60f3\u662f\u8b93\u6a21\u578b\u80fd\u5920\u900f\u904e\u5728\u8ecc\u8de1\u4e2d\u89c0\u5bdf\u4f86\u5b78\u7fd2\u7cfe\u6b63\u5176\u932f\u8aa4\u3002\u5177\u9ad4\u4f86\u8aaa\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u500b\u4ee3\u7406\u5408\u6210\u6846\u67b6\uff0c\u4ee5\u6db5\u84cb\u5404\u7a2e\u74b0\u5883\u548c\u4efb\u52d9\uff0c\u4e26\u63d0\u793a\u5f37\u5927\u7684 LLM \u6839\u64da\u74b0\u5883\u53cd\u994b\u8abf\u6574\u5176\u932f\u8aa4\u52d5\u4f5c\u3002AgentRefine \u5728\u5404\u7a2e\u4ee3\u7406\u4efb\u52d9\u4e0a\u7684\u6982\u5316\u80fd\u529b\u65b9\u9762\u986f\u8457\u512a\u65bc\u6700\u5148\u9032\u7684\u4ee3\u7406\u8abf\u6574\u5de5\u4f5c\u3002\u5b83\u5728\u9762\u5c0d\u64fe\u52d5\u6642\u4e5f\u5177\u6709\u66f4\u597d\u7684\u9b6f\u68d2\u6027\uff0c\u4e26\u4e14\u53ef\u4ee5\u5728\u63a8\u7406\u4e2d\u7522\u751f\u591a\u6a23\u5316\u7684\u601d\u8003\u3002\u6211\u5011\u7684\u7814\u7a76\u7d50\u679c\u5efa\u7acb\u4e86\u4ee3\u7406\u6982\u5316\u548c\u81ea\u6211\u7cbe\u9032\u4e4b\u9593\u7684\u95dc\u806f\uff0c\u4e26\u70ba\u672a\u4f86\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7bc4\u4f8b\u3002</paragraph>", "author": "Dayuan Fu et.al.", "authors": "Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu", "id": "2501.01702v1", "paper_url": "http://arxiv.org/abs/2501.01702v1", "repo": "null"}}