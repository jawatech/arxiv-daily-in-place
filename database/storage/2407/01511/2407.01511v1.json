{"2407.01511": {"publish_time": "2024-07-01", "title": "CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents", "paper_summary": "The development of autonomous agents increasingly relies on Multimodal\nLanguage Models (MLMs) to perform tasks described in natural language with GUI\nenvironments, such as websites, desktop computers, or mobile phones. Existing\nbenchmarks for MLM agents in interactive environments are limited by their\nfocus on a single environment, lack of detailed and generalized evaluation\nmethods, and the complexities of constructing tasks and evaluators. To overcome\nthese limitations, we introduce Crab, the first agent benchmark framework\ndesigned to support cross-environment tasks, incorporating a graph-based\nfine-grained evaluation method and an efficient mechanism for task and\nevaluator construction. Our framework supports multiple devices and can be\neasily extended to any environment with a Python interface. Leveraging Crab, we\ndeveloped a cross-platform Crab Benchmark-v0 comprising 100 tasks in computer\ndesktop and mobile phone environments. We evaluated four advanced MLMs using\ndifferent single and multi-agent system configurations on this benchmark. The\nexperimental results demonstrate that the single agent with GPT-4o achieves the\nbest completion ratio of 35.26%. All framework code, agent code, and task\ndatasets are publicly available at https://github.com/camel-ai/crab.", "paper_summary_zh": "\u81ea\u4e3b\u4ee3\u7406\u7684\u958b\u767c\u8d8a\u4f86\u8d8a\u4f9d\u8cf4\u591a\u6a21\u614b\u8a9e\u8a00\u6a21\u578b (MLM)\uff0c\u4ee5\u5728\u5177\u6709 GUI \u74b0\u5883\uff08\u4f8b\u5982\u7db2\u7ad9\u3001\u684c\u4e0a\u578b\u96fb\u8166\u6216\u624b\u6a5f\uff09\u7684\u81ea\u7136\u8a9e\u8a00\u4e2d\u57f7\u884c\u4efb\u52d9\u3002\u73fe\u6709\u7684\u4e92\u52d5\u74b0\u5883\u4e2d MLM \u4ee3\u7406\u7684\u57fa\u6e96\u53d7\u5230\u4ee5\u4e0b\u9650\u5236\uff1a\u5b83\u5011\u5c08\u6ce8\u65bc\u55ae\u4e00\u74b0\u5883\u3001\u7f3a\u4e4f\u8a73\u7d30\u4e14\u901a\u7528\u7684\u8a55\u4f30\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5efa\u69cb\u4efb\u52d9\u548c\u8a55\u4f30\u5668\u7684\u8907\u96dc\u6027\u3002\u70ba\u4e86\u514b\u670d\u9019\u4e9b\u9650\u5236\uff0c\u6211\u5011\u5f15\u5165\u4e86 Crab\uff0c\u9019\u662f\u7b2c\u4e00\u500b\u4ee3\u7406\u57fa\u6e96\u67b6\u69cb\uff0c\u65e8\u5728\u652f\u63f4\u8de8\u74b0\u5883\u4efb\u52d9\uff0c\u4e26\u7d50\u5408\u4e86\u57fa\u65bc\u5716\u5f62\u7684\u7d30\u7c92\u5ea6\u8a55\u4f30\u65b9\u6cd5\u548c\u4efb\u52d9\u8207\u8a55\u4f30\u5668\u5efa\u69cb\u7684\u6709\u6548\u6a5f\u5236\u3002\u6211\u5011\u7684\u67b6\u69cb\u652f\u63f4\u591a\u7a2e\u88dd\u7f6e\uff0c\u4e26\u4e14\u53ef\u4ee5\u8f15\u9b06\u5730\u64f4\u5145\u5230\u4efb\u4f55\u5177\u6709 Python \u4ecb\u9762\u7684\u74b0\u5883\u3002\u5229\u7528 Crab\uff0c\u6211\u5011\u958b\u767c\u4e86\u4e00\u500b\u8de8\u5e73\u53f0\u7684 Crab Benchmark-v0\uff0c\u5176\u4e2d\u5305\u542b\u96fb\u8166\u684c\u4e0a\u578b\u96fb\u8166\u548c\u624b\u6a5f\u74b0\u5883\u4e2d\u7684 100 \u500b\u4efb\u52d9\u3002\u6211\u5011\u4f7f\u7528\u4e0d\u540c\u7684\u55ae\u4e00\u548c\u591a\u4ee3\u7406\u7cfb\u7d71\u914d\u7f6e\uff0c\u5728\u9019\u500b\u57fa\u6e96\u4e0a\u8a55\u4f30\u4e86\u56db\u7a2e\u5148\u9032\u7684 MLM\u3002\u5be6\u9a57\u7d50\u679c\u8868\u660e\uff0c\u5177\u6709 GPT-4o \u7684\u55ae\u4e00\u4ee3\u7406\u5be6\u73fe\u4e86 35.26% \u7684\u6700\u4f73\u5b8c\u6210\u7387\u3002\u6240\u6709\u67b6\u69cb\u7a0b\u5f0f\u78bc\u3001\u4ee3\u7406\u7a0b\u5f0f\u78bc\u548c\u4efb\u52d9\u8cc7\u6599\u96c6\u90fd\u516c\u958b\u65bc https://github.com/camel-ai/crab\u3002", "author": "Tianqi Xu et.al.", "authors": "Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, Philip Torr, Bernard Ghanem, Guohao Li", "id": "2407.01511v1", "paper_url": "http://arxiv.org/abs/2407.01511v1", "repo": "https://github.com/camel-ai/crab"}}