{"2410.10819": {"publish_time": "2024-10-14", "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads", "paper_summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.", "paper_summary_zh": "\u90e8\u7f72\u9577\u8a9e\u5883\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM) \u662f\u5fc5\u8981\u7684\uff0c\u4f46\u6703\u5e36\u4f86\n\u986f\u8457\u7684\u8a08\u7b97\u548c\u8a18\u61b6\u9ad4\u6311\u6230\u3002\u5feb\u53d6\u6240\u6709\u95dc\u6ce8\u982d\u4e0a\u7684\u6240\u6709\u9375\u503c (KV)\n\u72c0\u614b\u6703\u6d88\u8017\u5927\u91cf\u7684\u8a18\u61b6\u9ad4\u3002\u73fe\u6709\u7684 KV \u5feb\u53d6\u4fee\u526a\u65b9\u6cd5\u6703\u640d\u5bb3 LLM \u7684\u9577\u8a9e\u5883\u529f\u80fd\u6216\n\u50c5\u63d0\u4f9b\u6709\u9650\u7684\u6548\u7387\u63d0\u5347\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u767c\u73fe\u53ea\u6709\u5c11\u6578\u7684\u95dc\u6ce8\u982d\uff0c\u53c8\u7a31\u6aa2\u7d22\u982d\uff0c\u5c0d\n\u8655\u7406\u9577\u8a9e\u5883\u81f3\u95dc\u91cd\u8981\uff0c\u4e26\u4e14\u9700\u8981\u6240\u6709\u4ee3\u5e63\u7684\u5b8c\u5168\u95dc\u6ce8\u3002\u76f8\u53cd\uff0c\u6240\u6709\u5176\u4ed6\u4e3b\u8981\u95dc\u6ce8\u6700\u8fd1\u4ee3\u5e63\u548c\u6ce8\u610f\u529b\u7684\u982d\n\u63a5\u6536\u5668\u2014\u2014\u7a31\u70ba\u4e32\u6d41\u982d\u2014\u2014\u4e0d\u9700\u8981\u5b8c\u5168\u95dc\u6ce8\u3002\u57fa\u65bc\n\u9019\u500b\u898b\u89e3\uff0c\u6211\u5011\u5f15\u5165\u4e86 DuoAttention\uff0c\u4e00\u500b\u53ea\u5c0d\u6aa2\u7d22\u982d\u61c9\u7528\u5b8c\u6574\nKV \u5feb\u53d6\u7684\u67b6\u69cb\uff0c\u540c\u6642\u5c0d\u4e32\u6d41\u982d\u4f7f\u7528\u8f15\u91cf\u7d1a\u3001\u9577\u5ea6\u56fa\u5b9a\u7684 KV\n\u5feb\u53d6\uff0c\u9019\u6e1b\u5c11\u4e86 LLM \u7684\u89e3\u78bc\u548c\u9810\u586b\u5145\n\u8a18\u61b6\u9ad4\u548c\u5ef6\u9072\uff0c\u540c\u6642\u4e0d\u640d\u5bb3\u5176\u9577\u8a9e\u5883\u80fd\u529b\u3002\nDuoAttention \u4f7f\u7528\u4e00\u7a2e\u57fa\u65bc\u512a\u5316\u7684\u8f15\u91cf\u7d1a\u6f14\u7b97\u6cd5\uff0c\u4e26\u4f7f\u7528\u5408\u6210\n\u8cc7\u6599\u6e96\u78ba\u8b58\u5225\u6aa2\u7d22\u982d\u3002\u6211\u5011\u7684\u6280\u8853\u986f\u8457\u6e1b\u5c11\u4e86\nMHA \u7684\u9577\u8a9e\u5883\u63a8\u8ad6\u8a18\u61b6\u9ad4\uff0c\u6700\u591a\u53ef\u9054 2.55 \u500d\uff0cGQA \u6a21\u578b\u53ef\u9054 1.67 \u500d\n\u540c\u6642\u5c07\u89e3\u78bc\u901f\u5ea6\u63d0\u9ad8\u4e86 2.18 \u500d\u548c 1.50 \u500d\uff0c\u4e26\u5c07\nMHA \u548c GQA \u6a21\u578b\u7684\u9810\u586b\u5145\u901f\u5ea6\u5206\u5225\u63d0\u9ad8\u4e86 1.73 \u500d\u548c 1.63 \u500d\uff0c\u8207\n\u5b8c\u5168\u95dc\u6ce8\u76f8\u6bd4\uff0c\u6e96\u78ba\u5ea6\u640d\u5931\u6700\u5c0f\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u7d50\u5408\n\u91cf\u5316\uff0cDuoAttention \u53ef\u4ee5\u8b93 Llama-3-8B \u5728\u55ae\u500b A100 GPU \u4e0a\u89e3\u78bc 330 \u842c\u500b\u8a9e\u5883\u9577\u5ea6\u3002\u7a0b\u5f0f\u78bc\u63d0\u4f9b\u5728\nhttps://github.com/mit-han-lab/duo-attention\u3002", "author": "Guangxuan Xiao et.al.", "authors": "Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han", "id": "2410.10819v1", "paper_url": "http://arxiv.org/abs/2410.10819v1", "repo": "https://github.com/mit-han-lab/duo-attention"}}