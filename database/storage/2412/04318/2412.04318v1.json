{"2412.04318": {"publish_time": "2024-12-05", "title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation", "paper_summary": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.", "paper_summary_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728\u5927\u6570\u636e\u96c6\u4e0a\u8fc7\u5ea6\u62df\u5408\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u53cd\u76f4\u89c9\u6cdb\u5316\u7ed3\u679c\u3002\u5728\u5f00\u653e\u5f0f\u6587\u672c\u751f\u6210\u8bbe\u7f6e\u4e2d\uff0c\u6709\u5145\u5206\u7684\u6587\u732e\u8868\u660e\uff0cLLM \u503e\u5411\u4e8e\u751f\u6210\u91cd\u590d\u4e14\u67af\u71e5\u7684\u5e8f\u5217\uff0c\u8fd9\u79cd\u73b0\u8c61\u5728\u4f7f\u7528\u8d2a\u5a6a\u89e3\u7801\u751f\u6210\u65f6\u5c24\u5176\u660e\u663e\u3002\u5373\u4f7f\u662f\u5305\u542b\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\u3001\u901a\u8fc7\u5bf9\u5927\u578b\u6570\u636e\u96c6\u8fdb\u884c\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u7684 LLM\uff0c\u4e5f\u4f1a\u6301\u7eed\u51fa\u73b0\u6b64\u95ee\u9898\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u901a\u8fc7\u8fdb\u4e00\u6b65\u5fae\u8c03\u8fd9\u4e9b\u6a21\u578b\u4ee5\u5728\u5c11\u91cf\u6837\u672c\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u4e8e\u96f6\u7684\u8bad\u7ec3\u635f\u5931\u2014\u2014\u6211\u4eec\u79f0\u4e4b\u4e3a\u8d85\u62df\u5408\u2014\u2014\u53ef\u4ee5\u6781\u5927\u5730\u589e\u5f3a\u957f\u5e8f\u5217\u751f\u6210\u80fd\u529b\u3002\u4f7f\u7528\u8fd9\u4e9b\u8d85\u62df\u5408\u6a21\u578b\u8fdb\u884c\u8d2a\u5a6a\u89e3\u7801\uff0c\u5373\u4f7f\u5728\u591a\u6837\u6027\u548c\u4eba\u7c7b\u504f\u597d\u65b9\u9762\uff0c\u4e5f\u4f18\u4e8e\u5bf9\u957f\u5e8f\u5217\u8fdb\u884c Top-P \u91c7\u6837\u3002\u8fd9\u79cd\u73b0\u8c61\u5ef6\u4f38\u5230\u5404\u79cd\u89c4\u6a21\u7684 LLM\u3001\u4e0d\u540c\u7684\u57df\uff0c\u751a\u81f3\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u53d1\u73b0\uff0c\u8fd9\u79cd\u73b0\u8c61\u4e0e Grokking \u548c\u53cc\u91cd\u4e0b\u964d\u622a\u7136\u4e0d\u540c\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8d85\u62df\u5408\u6a21\u578b\u5f88\u5c11\u9677\u5165\u5176\u63a5\u53d7\u8bad\u7ec3\u7684\u91cd\u590d\u5e8f\u5217\u4e2d\uff0c\u5373\u4f7f\u660e\u786e\u963b\u6b62\u8fd9\u4e9b\u5e8f\u5217\u4e5f\u4f1a\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\u3002\u6240\u6709\u8d85\u62df\u5408\u6a21\u578b\u90fd\u4f1a\u4ea7\u751f\u6781\u4f4e\u71b5\u7684\u9884\u6d4b\uff0c\u901a\u5e38\u5c06\u51e0\u4e4e\u6240\u6709\u6982\u7387\u90fd\u5206\u914d\u7ed9\u5355\u4e2a\u6807\u8bb0\u3002", "author": "Fredrik Carlsson et.al.", "authors": "Fredrik Carlsson, Fangyu Liu, Daniel Ward, Murathan Kurfali, Joakim Nivre", "id": "2412.04318v1", "paper_url": "http://arxiv.org/abs/2412.04318v1", "repo": "null"}}