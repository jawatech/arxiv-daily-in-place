{"2405.16877": {"publish_time": "2024-05-27", "title": "Are Self-Attentions Effective for Time Series Forecasting?", "paper_summary": "Time series forecasting is crucial for applications across multiple domains\nand various scenarios. Although Transformer models have dramatically shifted\nthe landscape of forecasting, their effectiveness remains debated. Recent\nfindings have indicated that simpler linear models might outperform complex\nTransformer-based approaches, highlighting the potential for more streamlined\narchitectures. In this paper, we shift focus from the overall architecture of\nthe Transformer to the effectiveness of self-attentions for time series\nforecasting. To this end, we introduce a new architecture, Cross-Attention-only\nTime Series transformer (CATS), that rethinks the traditional Transformer\nframework by eliminating self-attention and leveraging cross-attention\nmechanisms instead. By establishing future horizon-dependent parameters as\nqueries and enhanced parameter sharing, our model not only improves long-term\nforecasting accuracy but also reduces the number of parameters and memory\nusage. Extensive experiment across various datasets demonstrates that our model\nachieves superior performance with the lowest mean squared error and uses fewer\nparameters compared to existing models.", "paper_summary_zh": "\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u5c0d\u8de8\u591a\u500b\u9818\u57df\u548c\u5404\u7a2e\u60c5\u5883\u7684\u61c9\u7528\u81f3\u95dc\u91cd\u8981\u3002\u96d6\u7136 Transformer \u6a21\u578b\u5df2\u5927\u5e45\u6539\u8b8a\u9810\u6e2c\u7684\u683c\u5c40\uff0c\u4f46\u5176\u6709\u6548\u6027\u4ecd\u6709\u722d\u8b70\u3002\u6700\u8fd1\u7684\u767c\u73fe\u986f\u793a\uff0c\u8f03\u7c21\u55ae\u7684\u7dda\u6027\u6a21\u578b\u53ef\u80fd\u512a\u65bc\u8907\u96dc\u7684\u57fa\u65bc Transformer \u7684\u65b9\u6cd5\uff0c\u7a81\u986f\u4e86\u66f4\u7cbe\u7c21\u67b6\u69cb\u7684\u6f5b\u529b\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5011\u5c07\u7126\u9ede\u5f9e Transformer \u7684\u6574\u9ad4\u67b6\u69cb\u8f49\u79fb\u5230\u81ea\u6ce8\u610f\u529b\u5728\u6642\u9593\u5e8f\u5217\u9810\u6e2c\u4e2d\u7684\u6709\u6548\u6027\u3002\u70ba\u6b64\uff0c\u6211\u5011\u5f15\u5165\u4e86\u4e00\u7a2e\u65b0\u7684\u67b6\u69cb\uff0c\u7a31\u70ba\u50c5\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6642\u9593\u5e8f\u5217 Transformer (CATS)\uff0c\u5b83\u91cd\u65b0\u601d\u8003\u50b3\u7d71\u7684 Transformer \u6846\u67b6\uff0c\u65b9\u6cd5\u662f\u6d88\u9664\u81ea\u6ce8\u610f\u529b\u4e26\u6539\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u6a5f\u5236\u3002\u900f\u904e\u5c07\u672a\u4f86\u6642\u9593\u7bc4\u570d\u4f9d\u8cf4\u53c3\u6578\u8a2d\u5b9a\u70ba\u67e5\u8a62\u4e26\u589e\u5f37\u53c3\u6578\u5171\u4eab\uff0c\u6211\u5011\u7684\u6a21\u578b\u4e0d\u50c5\u63d0\u9ad8\u4e86\u9577\u671f\u9810\u6e2c\u7684\u6e96\u78ba\u6027\uff0c\u800c\u4e14\u9084\u6e1b\u5c11\u4e86\u53c3\u6578\u548c\u8a18\u61b6\u9ad4\u4f7f\u7528\u91cf\u3002\u8de8\u5404\u7a2e\u8cc7\u6599\u96c6\u7684\u5ee3\u6cdb\u5be6\u9a57\u8b49\u660e\uff0c\u8207\u73fe\u6709\u6a21\u578b\u76f8\u6bd4\uff0c\u6211\u5011\u7684\u6a21\u578b\u4ee5\u6700\u4f4e\u7684\u5747\u65b9\u8aa4\u5dee\u5be6\u73fe\u4e86\u5353\u8d8a\u7684\u6548\u80fd\uff0c\u4e26\u4e14\u4f7f\u7528\u4e86\u8f03\u5c11\u7684\u53c3\u6578\u3002", "author": "Dongbin Kim et.al.", "authors": "Dongbin Kim, Jinseong Park, Jaewook Lee, Hoki Kim", "id": "2405.16877v1", "paper_url": "http://arxiv.org/abs/2405.16877v1", "repo": "null"}}