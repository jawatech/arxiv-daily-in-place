{"2407.17678": {"publish_time": "2024-07-25", "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding among Attention Heads", "paper_summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.", "paper_summary_zh": "\u73fe\u6709\u7684 LLM \u8a13\u7df4\u548c\u63a8\u7406\u67b6\u69cb\u5728\u63d0\u5347\u7a00\u758f\u6027\u6548\u7387\u7684\u540c\u6642\uff0c\u96e3\u4ee5\u7dad\u6301\u8108\u7d61\u548c\u6a21\u578b\u67b6\u69cb\u7684\u5b8c\u6574\u6027\u3002\u53d7\u5230\u8cc7\u6599\u5eab\u5206\u7247\u6982\u5ff5\u548c\u6ce8\u610f\u529b\u5728\u52a0\u901f\u5668\u4e0a\u5e73\u884c\u65bc\u982d\u90e8\u7684\u555f\u767c\uff0c\u6211\u5011\u63d0\u51fa\u7a00\u758f\u5206\u7247 (S2) \u6ce8\u610f\u529b\uff0c\u9019\u662f\u4e00\u7a2e\u6ce8\u610f\u529b\u6f14\u7b97\u6cd5\uff0c\u5b83\u5206\u914d\u7570\u8cea\u8108\u7d61\u5206\u5272\uff0c\u4f9b\u4e0d\u540c\u7684\u6ce8\u610f\u529b\u982d\u90e8\u9032\u884c\u5206\u800c\u6cbb\u4e4b\u3002S2-Attention \u5f37\u5236\u6bcf\u500b\u6ce8\u610f\u529b\u982d\u90e8\u53ea\u95dc\u6ce8\u8108\u7d61\u5206\u5272\uff0c\u9075\u5faa\u5206\u6b65\u7a00\u758f\u6027\u6a21\u5f0f\uff0c\u540c\u6642\u5c07\u5b8c\u6574\u8108\u7d61\u4fdd\u7559\u70ba\u6240\u6709\u5206\u7247\u7684\u806f\u96c6\u3002\u7531\u65bc\u6ce8\u610f\u529b\u982d\u90e8\u5728\u7368\u7acb\u7684\u57f7\u884c\u7dd2\u5340\u584a\u4e2d\u8655\u7406\uff0c\u56e0\u6b64\u6bcf\u500b\u982d\u90e8\u7684\u8108\u7d61\u7c21\u5316\u53ef\u4ee5\u7522\u751f\u7aef\u5230\u7aef\u7684\u52a0\u901f\u548c\u8a18\u61b6\u9ad4\u6e1b\u5c11\u3002\u5728\u63a8\u7406\u6642\uff0c\u4f7f\u7528 S2-Attention \u8a13\u7df4\u7684 LLM \u53ef\u4ee5\u5c07 KV \u5feb\u53d6\u7c21\u5316\u8996\u70ba\u514d\u8cbb\u9910\u9ede\uff0c\u540c\u6642\u4fdd\u8b49\u6a21\u578b\u54c1\u8cea\u4fdd\u6301\u4e0d\u8b8a\u3002\u5728\u5be6\u9a57\u4e2d\uff0c\u6211\u5011\u5c55\u793a S2-Attention \u53ef\u4ee5\u63d0\u4f9b\u9ad8\u9054 (1) 25.3 \u500d\u7684 FlashAttention-2 \u7246\u9762\u6642\u8108\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u5f9e\u800c\u5c07\u7aef\u5230\u7aef\u8a13\u7df4\u6642\u9593\u6e1b\u5c11 6 \u500d\uff0c\u5c07\u63a8\u7406\u5ef6\u9072\u6e1b\u5c11 10 \u500d\uff0c(2) \u8207\u9810\u8a2d\u6ce8\u610f\u529b\u76f8\u6bd4\uff0c\u6a21\u578b\u8a13\u7df4\u54c1\u8cea\u76f8\u7576\uff0c(3) \u5728 32K \u8108\u7d61\u8996\u7a97\u4e2d\u5b8c\u7f8e\u7121\u7f3a\u7684\u91dd\u982d\u6aa2\u7d22\u6e96\u78ba\u5ea6\u3002\u5728\u6f14\u7b97\u6cd5\u4e4b\u4e0a\uff0c\u6211\u5011\u5efa\u69cb\u4e86 DKernel\uff0c\u9019\u662f\u4e00\u500b LLM \u8a13\u7df4\u548c\u63a8\u7406\u6838\u5fc3\u51fd\u5f0f\u5eab\uff0c\u5141\u8a31\u4f7f\u7528\u8005\u70ba\u81ea\u5df1\u7684\u6a21\u578b\u81ea\u8a02\u7a00\u758f\u6027\u6a21\u5f0f\u3002\u6211\u5011\u958b\u653e\u539f\u59cb\u78bc DKernel\uff0c\u4e26\u4f7f\u5176\u8207 Megatron\u3001Pytorch \u548c vLLM \u76f8\u5bb9\u3002", "author": "Xihui Lin et.al.", "authors": "Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Xia Song", "id": "2407.17678v1", "paper_url": "http://arxiv.org/abs/2407.17678v1", "repo": "null"}}