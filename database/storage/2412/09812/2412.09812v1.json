{"2412.09812": {"publish_time": "2024-12-13", "title": "ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic LayerReplace and Selective Rank Compression", "paper_summary": "Offsite-tuning is a privacy-preserving method for tuning large language\nmodels (LLMs) by sharing a lossy compressed emulator from the LLM owners with\ndata owners for downstream task tuning. This approach protects the privacy of\nboth the model and data owners. However, current offsite tuning methods often\nsuffer from adaptation degradation, high computational costs, and limited\nprotection strength due to uniformly dropping LLM layers or relying on\nexpensive knowledge distillation. To address these issues, we propose ScaleOT,\na novel privacy-utility-scalable offsite-tuning framework that effectively\nbalances privacy and utility. ScaleOT introduces a novel layerwise lossy\ncompression algorithm that uses reinforcement learning to obtain the importance\nof each layer. It employs lightweight networks, termed harmonizers, to replace\nthe raw LLM layers. By combining important original LLM layers and harmonizers\nin different ratios, ScaleOT generates emulators tailored for optimal\nperformance with various model scales for enhanced privacy protection.\nAdditionally, we present a rank reduction method to further compress the\noriginal LLM layers, significantly enhancing privacy with negligible impact on\nutility. Comprehensive experiments show that ScaleOT can achieve nearly\nlossless offsite tuning performance compared with full fine-tuning while\nobtaining better model privacy.", "paper_summary_zh": "\u5834\u5916\u5fae\u8abf\u662f\u4e00\u7a2e\u96b1\u79c1\u4fdd\u8b77\u65b9\u6cd5\uff0c\u7528\u65bc\u900f\u904e LLM \u6240\u6709\u8005\u8207\u8cc7\u6599\u6240\u6709\u8005\u5206\u4eab\u6709\u640d\u5931\u58d3\u7e2e\u7684 LLM \u6a21\u64ec\u5668\uff0c\u4ee5\u9032\u884c\u4e0b\u6e38\u4efb\u52d9\u5fae\u8abf\uff0c\u9032\u800c\u5fae\u8abf\u5927\u578b\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u6b64\u65b9\u6cd5\u4fdd\u8b77\u6a21\u578b\u548c\u8cc7\u6599\u6240\u6709\u8005\u7684\u96b1\u79c1\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u5834\u5916\u5fae\u8abf\u65b9\u6cd5\u901a\u5e38\u6703\u56e0\u70ba\u9069\u61c9\u6027\u964d\u4f4e\u3001\u9ad8\u904b\u7b97\u6210\u672c\u548c\u6709\u9650\u7684\u4fdd\u8b77\u5f37\u5ea6\u800c\u53d7\u82e6\uff0c\u9019\u662f\u56e0\u70ba\u5747\u52fb\u5730\u6368\u68c4 LLM \u5c64\u6216\u4f9d\u8cf4\u6602\u8cb4\u7684\u77e5\u8b58\u8403\u53d6\u3002\u70ba\u4e86\u89e3\u6c7a\u9019\u4e9b\u554f\u984c\uff0c\u6211\u5011\u63d0\u51fa ScaleOT\uff0c\u9019\u662f\u4e00\u500b\u65b0\u7a4e\u7684\u96b1\u79c1\u516c\u7528\u4e8b\u696d\u53ef\u64f4\u5145\u5834\u5916\u5fae\u8abf\u67b6\u69cb\uff0c\u80fd\u6709\u6548\u5e73\u8861\u96b1\u79c1\u548c\u516c\u7528\u4e8b\u696d\u3002ScaleOT \u4ecb\u7d39\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u9010\u5c64\u6709\u640d\u5931\u58d3\u7e2e\u6f14\u7b97\u6cd5\uff0c\u8a72\u6f14\u7b97\u6cd5\u4f7f\u7528\u5f37\u5316\u5b78\u7fd2\u4f86\u53d6\u5f97\u6bcf\u4e00\u5c64\u7684\u91cd\u8981\u6027\u3002\u5b83\u63a1\u7528\u7a31\u70ba\u8abf\u548c\u5668\u7684\u8f15\u91cf\u7d1a\u7db2\u8def\uff0c\u4ee5\u53d6\u4ee3\u539f\u59cb\u7684 LLM \u5c64\u3002\u900f\u904e\u4ee5\u4e0d\u540c\u7684\u6bd4\u4f8b\u7d50\u5408\u91cd\u8981\u7684\u539f\u59cb LLM \u5c64\u548c\u8abf\u548c\u5668\uff0cScaleOT \u80fd\u7522\u751f\u91dd\u5c0d\u6700\u4f73\u6548\u80fd\u91cf\u8eab\u6253\u9020\u7684\u6a21\u64ec\u5668\uff0c\u4e26\u5177\u5099\u5404\u7a2e\u6a21\u578b\u898f\u6a21\u4ee5\u589e\u5f37\u96b1\u79c1\u4fdd\u8b77\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e00\u500b\u79e9\u6b21\u7c21\u7d04\u6cd5\uff0c\u4ee5\u9032\u4e00\u6b65\u58d3\u7e2e\u539f\u59cb\u7684 LLM \u5c64\uff0c\u5927\u5e45\u63d0\u5347\u96b1\u79c1\uff0c\u4e14\u5c0d\u516c\u7528\u4e8b\u696d\u7684\u5f71\u97ff\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8a08\u3002\u5168\u9762\u7684\u5be6\u9a57\u986f\u793a\uff0c\u8207\u5b8c\u5168\u5fae\u8abf\u76f8\u6bd4\uff0cScaleOT \u80fd\u5920\u9054\u6210\u8fd1\u4e4e\u7121\u640d\u5931\u7684\u5834\u5916\u5fae\u8abf\u6548\u80fd\uff0c\u540c\u6642\u9084\u80fd\u53d6\u5f97\u66f4\u597d\u7684\u6a21\u578b\u96b1\u79c1\u3002", "author": "Kai Yao et.al.", "authors": "Kai Yao, Zhaorui Tan, Tiandi Ye, Lichun Li, Yuan Zhao, Wenyan Liu, Wei Wang, Jianke Zhu", "id": "2412.09812v1", "paper_url": "http://arxiv.org/abs/2412.09812v1", "repo": "null"}}