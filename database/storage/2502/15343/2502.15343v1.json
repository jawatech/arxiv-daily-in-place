{"2502.15343": {"publish_time": "2025-02-21", "title": "Tokenization is Sensitive to Language Variation", "paper_summary": "Variation in language is ubiquitous and often systematically linked to\nregional, social, and contextual factors. Tokenizers split texts into smaller\nunits and might behave differently for less common linguistic forms. This might\naffect downstream LLM performance differently on two types of tasks: Tasks\nwhere the model should be robust to language variation (e.g., for semantic\ntasks like NLI, labels do not depend on whether a text uses British or American\nspelling) and tasks where the model should be sensitive to language variation\n(e.g., for form-based tasks like authorship verification, labels depend on\nwhether a text uses British or American spelling). We pre-train BERT base\nmodels for the popular Byte-Pair Encoding algorithm to investigate how key\nalgorithmic design choices impact downstream models' performances: fitting\ncorpus, pre-tokenizer and vocabulary size. We find that the best tokenizer\nvaries on the two task types -- with the pre-tokenizer having the biggest\nimpact on performance. Further, we introduce a new approach to estimate\ntokenizer impact on downstream LLM performance, showing significant improvement\nover techniques like R\\'enyi efficiency. We encourage more work on language\nvariation and its relation to tokenizers and thus LLM performance.", "paper_summary_zh": "\u8a9e\u8a00\u8b8a\u7570\u7121\u6240\u4e0d\u5728\uff0c\u4e14\u7d93\u5e38\u8207\u5340\u57df\u3001\u793e\u6703\u548c\u60c5\u5883\u56e0\u7d20\u7cfb\u7d71\u6027\u5730\u9023\u7d50\u3002\u6a19\u8a18\u5316\u5668\u5c07\u6587\u5b57\u5206\u5272\u6210\u8f03\u5c0f\u7684\u55ae\u4f4d\uff0c\u5c0d\u65bc\u8f03\u4e0d\u5e38\u898b\u7684\u8a9e\u8a00\u5f62\u5f0f\uff0c\u5176\u884c\u70ba\u53ef\u80fd\u6709\u6240\u4e0d\u540c\u3002\u9019\u53ef\u80fd\u6703\u5c0d\u5169\u7a2e\u4efb\u52d9\u7684\u4e0b\u6e38 LLM \u6548\u80fd\u7522\u751f\u4e0d\u540c\u7684\u5f71\u97ff\uff1a\u6a21\u578b\u61c9\u8a72\u5c0d\u8a9e\u8a00\u8b8a\u7570\u5177\u6709\u7a69\u5065\u6027\u7684\u4efb\u52d9\uff08\u4f8b\u5982\uff0c\u5c0d\u65bc NLI \u7b49\u8a9e\u7fa9\u4efb\u52d9\uff0c\u6a19\u7c64\u4e0d\u53d6\u6c7a\u65bc\u6587\u5b57\u63a1\u7528\u82f1\u5f0f\u6216\u7f8e\u5f0f\u62fc\u5beb\uff09\u4ee5\u53ca\u6a21\u578b\u61c9\u8a72\u5c0d\u8a9e\u8a00\u8b8a\u7570\u654f\u611f\u7684\u4efb\u52d9\uff08\u4f8b\u5982\uff0c\u5c0d\u65bc\u4f5c\u8005\u9a57\u8b49\u7b49\u57fa\u65bc\u5f62\u5f0f\u7684\u4efb\u52d9\uff0c\u6a19\u7c64\u53d6\u6c7a\u65bc\u6587\u5b57\u63a1\u7528\u82f1\u5f0f\u6216\u7f8e\u5f0f\u62fc\u5beb\uff09\u3002\u6211\u5011\u91dd\u5c0d\u71b1\u9580\u7684 Byte-Pair \u7de8\u78bc\u6f14\u7b97\u6cd5\u9810\u5148\u8a13\u7df4 BERT \u57fa\u790e\u6a21\u578b\uff0c\u4ee5\u63a2\u8a0e\u95dc\u9375\u6f14\u7b97\u6cd5\u8a2d\u8a08\u9078\u9805\u5982\u4f55\u5f71\u97ff\u4e0b\u6e38\u6a21\u578b\u7684\u6548\u80fd\uff1a\u64ec\u5408\u8a9e\u6599\u5eab\u3001\u9810\u5148\u6a19\u8a18\u5316\u5668\u548c\u8a5e\u5f59\u91cf\u5927\u5c0f\u3002\u6211\u5011\u767c\u73fe\u6700\u4f73\u6a19\u8a18\u5316\u5668\u5728\u5169\u7a2e\u4efb\u52d9\u985e\u578b\u4e0a\u6709\u6240\u4e0d\u540c\uff0c\u5176\u4e2d\u9810\u5148\u6a19\u8a18\u5316\u5668\u5c0d\u6548\u80fd\u7684\u5f71\u97ff\u6700\u5927\u3002\u6b64\u5916\uff0c\u6211\u5011\u63d0\u51fa\u4e86\u4e00\u7a2e\u4f30\u8a08\u6a19\u8a18\u5316\u5668\u5c0d\u4e0b\u6e38 LLM \u6548\u80fd\u5f71\u97ff\u7684\u65b0\u65b9\u6cd5\uff0c\u986f\u793a\u51fa\u6bd4 R\\'enyi \u6548\u7387\u7b49\u6280\u8853\u6709\u986f\u8457\u7684\u9032\u6b65\u3002\u6211\u5011\u9f13\u52f5\u5c0d\u8a9e\u8a00\u8b8a\u7570\u53ca\u5176\u8207\u6a19\u8a18\u5316\u5668\u548c LLM \u6548\u80fd\u7684\u95dc\u4fc2\u9032\u884c\u66f4\u591a\u7814\u7a76\u3002", "author": "Anna Wegmann et.al.", "authors": "Anna Wegmann, Dong Nguyen, David Jurgens", "id": "2502.15343v1", "paper_url": "http://arxiv.org/abs/2502.15343v1", "repo": "null"}}